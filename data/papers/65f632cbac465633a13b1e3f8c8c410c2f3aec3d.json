{"title": "Stackelberg MADDPG: Learning Emergent Behaviors via Information Asymmetry in Competitive Games", "abstract": "Using competitive multi-agent reinforcement learning (MARL) methods to solve physically grounded problems, such as robust control and interactive manipulation tasks, has become more popular in the robotics community. However, the asymmetric nature of these tasks makes the generation of sophisticated policies challenging. Indeed, the asymmetry in the environment may implicitly or explicitly provide an advantage to a subset of agents which could, in turn, lead to a low quality equilibrium. This paper proposes a novel game-theoretic MARL algorithm, Stackelberg Multi-Agent Deep Deterministic Policy Gradient (ST-MADDPG), which formulates a two-player MARL problem as a Stackelberg game with one player as the \u2018leader\u2019 and the other as the \u2018follower\u2019 in a hierarchical interaction structure wherein the leader has an information advantage: the leader in ST-MADDPG updates using its total policy gradient, meaning it differentiates through the local best response of the follower. In a simple competitive robotics environment, we show that the leader learns a better policy by exploiting this information advantage and is able to either dominate the game or alleviate the native disadvantage from the game environment. In two practical robotic problems, ST-MADDPG allows the leader to learn more sophisticated and complex strategies that work well even against an unseen strong opponent.", "year": 2021, "ssId": "65f632cbac465633a13b1e3f8c8c410c2f3aec3d", "arXivId": null, "link": null, "openAccess": false, "authors": ["Boling Yang", "Liyuan Zheng", "L. Ratliff", "Byron Boots"]}