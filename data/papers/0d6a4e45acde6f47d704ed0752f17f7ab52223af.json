{"title": "Ask Your Humans: Using Human Instructions to Improve Generalization in Reinforcement Learning", "abstract": "Complex, multi-task problems have proven to be difficult to solve efficiently in a sparse-reward reinforcement learning setting. In order to be sample efficient, multi-task learning requires reuse and sharing of low-level policies. To facilitate the automatic decomposition of hierarchical tasks, we propose the use of step-by-step human demonstrations in the form of natural language instructions and action trajectories. We introduce a dataset of such demonstrations in a crafting-based grid world. Our model consists of a high-level language generator and low-level policy, conditioned on language. We find that human demonstrations help solve the most complex tasks. We also find that incorporating natural language allows the model to generalize to unseen tasks in a zero-shot setting and to learn quickly from a few demonstrations. Generalization is not only reflected in the actions of the agent, but also in the generated natural language instructions in unseen tasks. Our approach also gives our trained agent interpretable behaviors because it is able to generate a sequence of high-level descriptions of its actions.", "year": 2020, "ssId": "0d6a4e45acde6f47d704ed0752f17f7ab52223af", "arXivId": "2011.00517", "link": "https://arxiv.org/pdf/2011.00517.pdf", "openAccess": true, "authors": ["Valerie Chen", "A. Gupta", "Kenneth Marino"]}