{"title": "Why do you think that? Exploring faithful sentence\u2013level rationales without supervision", "abstract": "Evaluating the trustworthiness of a model\u2019s prediction is essential for differentiating between \u2018right for the right reasons\u2019 and \u2018right for the wrong reasons\u2019. Identifying textual spans that determine the target label, known as faithful rationales, usually relies on pipeline approaches or reinforcement learning. However, such methods either require supervision and thus costly annotation of the rationales or employ non-differentiable models. We propose a differentiable training\u2013framework to create models which output faithful rationales on a sentence level, by solely applying supervision on the target task. To achieve this, our model solves the task based on each rationale individually and learns to assign high scores to those which solved the task best. Our evaluation on three different datasets shows competitive results compared to a standard BERT blackbox while exceeding a pipeline counterpart\u2019s performance in two cases. We further exploit the transparent decision\u2013making process of these models to prefer selecting the correct rationales by applying direct supervision, thereby boosting the performance on the rationale\u2013level.", "year": 2020, "ssId": "2ce3428ba8777c723b9b12e9f8eaeb2c87a5a793", "arXivId": "2010.03384", "link": "https://arxiv.org/pdf/2010.03384.pdf", "openAccess": true, "authors": ["Max Glockner", "Ivan Habernal", "Iryna Gurevych"]}