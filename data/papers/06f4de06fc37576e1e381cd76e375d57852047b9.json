{"title": "Improving Robustness of Neural Machine Translation with Multi-task Learning", "abstract": "While neural machine translation (NMT) achieves remarkable performance on clean, in-domain text, performance is known to degrade drastically when facing text which is full of typos, grammatical errors and other varieties of noise. In this work, we propose a multi-task learning algorithm for transformer-based MT systems that is more resilient to this noise. We describe our submission to the WMT 2019 Robustness shared task based on this method. Our model achieves a BLEU score of 32.8 on the shared task French to English dataset, which is 7.1 BLEU points higher than the baseline vanilla transformer trained with clean text.", "year": 2019, "ssId": "06f4de06fc37576e1e381cd76e375d57852047b9", "arXivId": null, "link": null, "openAccess": false, "authors": ["Shuyan Zhou", "Xiangkai Zeng", "Yingqi Zhou", "Antonios Anastasopoulos", "Graham Neubig"]}