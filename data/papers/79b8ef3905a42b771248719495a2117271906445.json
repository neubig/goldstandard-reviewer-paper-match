{"title": "Carbon Emissions and Large Neural Network Training", "abstract": "David Patterson 1 , 2 , Joseph Gonzalez 2 , Quoc Le 1 , Chen Liang 1 , Lluis-Miquel Munguia 1 , Daniel Rothchild 2 , David So 1 , Maud Texier 1 , and Jeff Dean 1  {davidpatterson, qvl, crazydonkey, llmunguia, davidso, maudt, jeff}@google.com, {pattrsn, jegonzal, drothchild}@berkeley.edu  Abstract: The computation demand for machine learning (ML) has grown rapidly recently, which comes with a number of costs. Estimating the energy cost helps measure its environmental impact and finding greener strategies, yet it is challenging without detailed information . We calculate the energy use and carbon footprint of several recent large models\u2014 T5 , Meena , GShard , Switch Transformer , and GPT-3 \u2014and refine earlier estimates for the neural architecture search that found Evolved Transformer . We highlight the following opportunities to improve energy efficiency and CO 2  equivalent emissions ( CO 2 e ): \u25cf Large but sparsely activated DNNs can consume <1/10th the energy of large, dense DNNs without sacrificing accuracy despite using as many or even more parameters. \u25cf Geographic location matters for ML workload scheduling since the fraction of carbon-free energy and resulting CO 2 e vary ~5X-10X, even within the same country and the same organization. We are now optimizing where and when large models are trained. \u25cf Specific datacenter infrastructure matters, as Cloud datacenters can be ~1.4-2X more energy efficient than typical datacenters, and the ML-oriented accelerators inside them can be ~2-5X more effective than off-the-shelf systems. Remarkably, the choice of DNN, datacenter, and processor can reduce the carbon footprint up to ~100-1000X. These large factors also make retroactive estimates of energy cost difficult. To avoid miscalculations, we believe ML papers requiring large computational resources should make energy consumption and CO 2 e explicit when practical. We are working to be more transparent about energy use and CO 2 e in our future research. To help reduce the carbon footprint of ML, we believe energy usage and CO 2 e should be a key metric in evaluating models, and we are collaborating with MLPerf developers to include energy usage during training and inference in this industry standard benchmark.", "year": 2021, "ssId": "79b8ef3905a42b771248719495a2117271906445", "arXivId": "2104.10350", "link": "https://arxiv.org/pdf/2104.10350.pdf", "openAccess": true, "authors": ["David A. Patterson", "Joseph Gonzalez", "Quoc V. Le", "Chen Liang", "Llu\u00eds-Miquel Mungu\u00eda", "D. Rothchild", "David R. So", "Maud Texier", "J. Dean"]}