{"title": "Transformer-GAN: Symbolic music generation using a learned loss", "abstract": "The conventional approach to symbolic music generation uses the Transformer, an autoregressive model that is commonly trained by minimizing the negative log-likelihood (NLL) of the observed sequence. The quality of samples from these models tends to degrade significantly for long sequences, a phenomenon attributed to exposure bias. However, we are able to detect these failures with classifiers trained to distinguish between real and sampled sequences, an observation that motivates our exploration of adversarial losses to complement the NLL objective. We use a pre-trained SpanBERT model for the discriminator of the GAN, which in our experiments helped with training stability. We demonstrate via human evaluations and a new discriminative metric that music generated by our approach outperforms a baseline trained with likelihood maximization, the state-of-the-art Music Transformer, and other GANs used for sequence generation. 57% of people prefer music generated via our approach while 43% prefer Music Transformer.", "year": 2020, "ssId": "7e122cc1a62e2f30951e14b91811896e1866dd7c", "arXivId": null, "link": null, "openAccess": false, "authors": ["Aashiq Muhamed", "Liang Li", "Xingjian Shi", "Suri Yaddanapudi", "Wayne Chi", "Dylan Jackson", "Rahul Suresh", "Zachary Chase Lipton", "Alexander J. Smola"]}