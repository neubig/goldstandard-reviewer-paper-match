{"title": "Combating Deep Reinforcement Learning \u2019 s Sisyphean Curse with Reinforcement Learning", "abstract": "To use deep reinforcement learning in the wild, we might hope for an agent that can avoid catastrophic mistakes. Unfortunately, even in simple environments, the popular deep Q-network (DQN) algorithm is doomed by a Sisyphean curse. Owing to the use of function approximation, these agents may eventually forget experiences as they become exceedingly unlikely under a new policy. Consequently, for as long as they continue to train, DQNs may periodically repeat avoidable catastrophic mistakes. In this paper, we learn a reward shaping that accelerates learning and guards oscillating policies against repeated catastrophes. First, we demonstrate unacceptable performance of DQNs on two toy problems. We then introduce intrinsic fear, a new method that mitigates these problems by avoiding dangerous states. Our approach incorporates a second model trained via supervised learning to predict the probability of catastrophe within a short number of steps. This score then acts to penalize the Q-learning objective. Equipped with intrinsic fear, our DQNs solve the toy environments and improve on the Atari games Seaquest, Asteroids, and Freeway.", "year": 2017, "ssId": "683bbb665bdaea8688834e97559d63842242ee1f", "arXivId": null, "link": null, "openAccess": false, "authors": ["Zachary Chase Lipton", "Abhishek Kumar", "Jianfeng Gao", "Lihong Li", "Li Deng zlipton"]}