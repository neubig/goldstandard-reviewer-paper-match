{"title": "Meta Back-translation", "abstract": "Back-translation (Sennrich et al., 2016) is an effective strategy to improve the performance of Neural Machine Translation (NMT) by generating pseudo-parallel data. However, several recent works have found that better translation quality of the pseudo-parallel data does not necessarily lead to better final translation models, while lower-quality but more diverse data often yields stronger results (Edunov et al., 2018). In this paper, we propose a novel method to generate pseudo-parallel data from a pre-trained back-translation model. Our method is a meta-learning algorithm which adapts a pre-trained back-translation model so that the pseudoparallel data it generates would train a forward-translation model to do well on a validation set. In our evaluations in both the standard datasets WMT En-De\u201914 and WMT En-Fr\u201914, as well as a multilingual translation setting, our method leads to significant improvements over strong baselines.1", "year": 2021, "ssId": "fcdac45272543b4f8b8eaa59d66044d1b7018494", "arXivId": "2102.07847", "link": "https://arxiv.org/pdf/2102.07847.pdf", "openAccess": true, "authors": ["Hieu Pham", "Xinyi Wang", "Yiming Yang", "Graham Neubig"]}