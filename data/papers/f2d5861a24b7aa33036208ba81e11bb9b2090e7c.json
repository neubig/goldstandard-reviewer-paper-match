{"title": "Improving the Diversity of Unsupervised Paraphrasing with Embedding Outputs", "abstract": "We present a novel technique for zero-shot paraphrase generation. The key contribution is an end-to-end multilingual paraphrasing model that is trained using translated parallel corpora to generate paraphrases into \u201cmeaning spaces\u201d \u2013 replacing the final softmax layer with word embeddings. This architectural modification, plus a training procedure that incorporates an autoencoding objective, enables effective parameter sharing across languages for more fluent monolingual rewriting, and facilitates fluency and diversity in the generated outputs. Our continuous-output paraphrase generation models outperform zero-shot paraphrasing baselines when evaluated on two languages using a battery of computational metrics as well as in human assessment.", "year": 2021, "ssId": "f2d5861a24b7aa33036208ba81e11bb9b2090e7c", "arXivId": "2110.13231", "link": "https://arxiv.org/pdf/2110.13231.pdf", "openAccess": true, "authors": ["M. Jegadeesan", "Sachin Kumar", "J. Wieting", "Yulia Tsvetkov"]}