{"title": "LaMDA: Language Models for Dialog Applications", "abstract": "We present LaMDA: Language Models for Dialog Applications. LaMDA is a family of Transformerbased neural language models specialized for dialog, which have up to 137B parameters and are pre-trained on 1.56T words of public dialog data and web text. While model scaling alone can improve quality, it shows less improvements on safety and factual grounding. We demonstrate that fine-tuning with annotated data and enabling the model to consult external knowledge sources can lead to significant improvements towards the two key challenges of safety and factual grounding. The first challenge, safety, involves ensuring that the model\u2019s responses are consistent with a set of human values, such as preventing harmful suggestions and unfair bias. We quantify safety using a metric based on an illustrative set of human values, and we find that filtering candidate responses using a LaMDA classifier fine-tuned with a small amount of crowdworker-annotated data offers a promising approach to improving model safety. The second challenge, factual grounding, involves enabling the model to consult external knowledge sources, such as an information retrieval system, a language translator, and a calculator. We quantify factuality using a groundedness metric, and we find that our approach enables the model to generate responses grounded in known sources, rather than responses that merely sound plausible. Finally, we explore the use of LaMDA in the domains of education and content recommendations, and analyze their helpfulness and role consistency. \u2217Work done while at Google. ar X iv :2 20 1. 08 23 9v 3 [ cs .C L ] 1 0 Fe b 20 22 Figure 1: Impact of model pre-training alone vs. with fine-tuning in LaMDA on dialog quality (left), and safety and factual grounding (right). The quality metric (SSI) corresponds to sensibleness, specificity, and interestingness. See Section 4 for more details on these metrics.", "year": 2022, "ssId": "b3848d32f7294ec708627897833c4097eb4d8778", "arXivId": "2201.08239", "link": "https://arxiv.org/pdf/2201.08239.pdf", "openAccess": true, "authors": ["Romal Thoppilan", "Daniel De Freitas", "Jamie Hall", "Noam M. Shazeer", "Apoorv Kulshreshtha", "Heng-Tze Cheng", "Alicia Jin", "Taylor Bos", "Leslie Baker", "Yu Du", "Yaguang Li", "Hongrae Lee", "Huaixiu Zheng", "Amin Ghafouri", "Marcelo Menegali", "Yanping Huang", "M. Krikun", "Dmitry Lepikhin", "James Qin", "Dehao Chen", "Yuanzhong Xu", "Zhifeng Chen", "Adam Roberts", "Maarten Bosma", "Yanqi Zhou", "Chung-Ching Chang", "I. Krivokon", "W. Rusch", "Marc Pickett", "K. Meier-Hellstern", "M. Morris", "Tulsee Doshi", "Renelito Delos Santos", "Toju Duke", "J. S\u00f8raker", "Ben Zevenbergen", "Vinodkumar Prabhakaran", "Mark Diaz", "B. Hutchinson", "Kristen Olson", "Alejandra Molina", "Erin Hoffman-John", "Josh Lee", "Lora Aroyo", "Ravindran Rajakumar", "Alena Butryna", "Matthew Lamm", "V. Kuzmina", "Joseph Fenton", "Aaron Cohen", "R. Bernstein", "R. Kurzweil", "Blaise Aguera-Arcas", "Claire Cui", "Marian Croak", "Ed Chi", "Quoc Le"]}