{"title": "Deep long short-term memory adaptive beamforming networks for multichannel robust speech recognition", "abstract": "Far-field speech recognition in noisy and reverberant conditions remains a challenging problem despite recent deep learning breakthroughs. This problem is commonly addressed by acquiring a speech signal from multiple microphones and performing beamforming over them. In this paper, we propose to use a recurrent neural network with long short-term memory (LSTM) architecture to adaptively estimate real-time beamforming filter coefficients to cope with non-stationary environmental noise and dynamic nature of source and microphones positions which results in a set of timevarying room impulse responses. The LSTM adaptive beamformer is jointly trained with a deep LSTM acoustic model to predict senone labels. Further, we use hidden units in the deep LSTM acoustic model to assist in predicting the beamforming filter coefficients. The proposed system achieves 7.97% absolute gain over baseline systems with no beamforming on CHiME-3 real evaluation set.", "year": 2017, "ssId": "5b2c5eeea9ac8f26908b9dfc8fd0a2d0e7aa5bb1", "arXivId": "1711.08016", "link": "https://arxiv.org/pdf/1711.08016.pdf", "openAccess": true, "authors": ["Zhong Meng", "Shinji Watanabe", "J. Hershey", "Hakan Erdogan"]}