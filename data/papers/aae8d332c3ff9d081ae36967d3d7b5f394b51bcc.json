{"title": "Self-Training with Differentiable Teacher", "abstract": "Self-training achieves enormous success in various semi-supervised and weakly-supervised learning tasks. The method can be interpreted as a teacher-student framework, where the teacher generates pseudo-labels, and the student makes predictions. The two models are updated alternatingly. However, such a straightforward alternating update rule leads to training instability. This is because a small change in the teacher may result in a significant change in the student. To address this issue, we propose DRIFT, short for differentiable self-training, that treats teacherstudent as a Stackelberg game. In this game, a leader is always in a more advantageous position than a follower. In self-training, the student contributes to the prediction performance, and the teacher controls the training process by generating pseudo-labels. Therefore, we treat the student as the leader and the teacher as the follower. The leader procures its advantage by acknowledging the follower\u2019s strategy, which involves differentiable pseudo-labels and differentiable sample weights. Consequently, the leader-follower interaction can be effectively captured via Stackelberg gradient, obtained by differentiating the follower\u2019s strategy. Experimental results on semiand weakly-supervised classification and named entity recognition tasks show that our model outperforms existing approaches by large margins.", "year": 2021, "ssId": "aae8d332c3ff9d081ae36967d3d7b5f394b51bcc", "arXivId": "2109.07049", "link": "https://arxiv.org/pdf/2109.07049.pdf", "openAccess": true, "authors": ["Simiao Zuo", "Yue Yu", "Chen Liang", "Haoming Jiang", "Siawpeng Er", "Chao Zhang", "T. Zhao", "H. Zha"]}