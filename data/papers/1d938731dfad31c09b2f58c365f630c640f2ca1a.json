{"title": "ATM: An Uncertainty-aware Active Self-training Framework for Label-efficient Text Classification", "abstract": "Despite the great success of pre-trained language models (LMs) in many natural language processing (NLP) tasks, they require excessive labeled data for fine-tuning to achieve satisfactory performance. To enhance the label efficiency, researchers have resorted to active learning (AL), while the potential of unlabeled data is ignored by most of the prior work. To unleash the power of unlabeled data for better label efficiency and model performance, we develop ATM, a new framework that leverages self-training to exploit unlabeled data and is agnostic to the specific AL algorithm, serving as a plug-in module to improve existing AL methods. Specifically, the unlabeled data with high uncertainty is exposed to the oracle for annotations while those with low uncertainty are leveraged for self-training. To alleviate the label noise propagation issue in self-training, we design a simple and effective momentumbased memory bank to dynamically aggregate the model predictions from preceding rounds. By extensive experiments, we demonstrate that ATM outperforms the strongest active learning and self-training baselines and improves the label efficiency by 51.9% on average.", "year": 2021, "ssId": "1d938731dfad31c09b2f58c365f630c640f2ca1a", "arXivId": "2112.08787", "link": "https://arxiv.org/pdf/2112.08787.pdf", "openAccess": true, "authors": ["Yue Yu", "Lingkai Kong", "Jieyu Zhang", "Rongzhi Zhang", "Chao Zhang"]}