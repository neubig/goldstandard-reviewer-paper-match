{"title": "Joint Speech Recognition and Audio Captioning", "abstract": "Speech samples recorded in both indoor and outdoor environments are often contaminated with secondary audio sources. Most endto-end monaural speech recognition systems either remove these background sounds using speech enhancement or train noise-robust models. For better model interpretability and holistic understanding, we aim to bring together the growing field of automated audio captioning (AAC) and the thoroughly studied automatic speech recognition (ASR). The goal of AAC is to generate natural language descriptions of contents in audio samples. We propose several approaches for end-to-end joint modeling of ASR and AAC tasks and demonstrate their advantages over traditional approaches, which model these tasks independently. A major hurdle in evaluating our proposed approach is the lack of labeled audio datasets with both speech transcriptions and audio captions. Therefore we also create a multi-task dataset by mixing the clean speech Wall Street Journal corpus with multiple levels of background noises chosen from the AudioCaps dataset. We also perform extensive experimental evaluation and show improvements of our proposed methods as compared to existing state-of-the-art ASR and AAC methods.", "year": 2022, "ssId": "5f8d2da91a6c4b9dd079ccb2706c31bda14ef320", "arXivId": "2202.01405", "link": "https://arxiv.org/pdf/2202.01405.pdf", "openAccess": true, "authors": ["Chaitanya Narisetty", "E. Tsunoo", "Xuankai Chang", "Yosuke Kashiwagi", "Michael Hentschel", "Shinji Watanabe"]}