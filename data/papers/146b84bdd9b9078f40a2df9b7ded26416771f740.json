{"title": "Inverse Risk-Sensitive Reinforcement Learning", "abstract": "This work addresses the problem of inverse reinforcement learning in Markov decision processes where the decision-making agent is risk-sensitive. In particular, a risk-sensitive reinforcement learning algorithm with convergence guarantees that makes use of coherent risk metrics and models of human decision-making which have their origins in behavioral psychology and economics is presented. The risk-sensitive reinforcement learning algorithm provides the theoretical underpinning for a gradient-based inverse reinforcement learning algorithm that seeks to minimize a loss function defined on the observed behavior. It is shown that the gradient of the loss function with respect to the model parameters is well defined and computable via a contraction map argument. Evaluation of the proposed technique is performed on a Grid World example, a canonical benchmark problem.", "year": 2017, "ssId": "146b84bdd9b9078f40a2df9b7ded26416771f740", "arXivId": null, "link": null, "openAccess": false, "authors": ["L. Ratliff", "Eric V. Mazumdar"]}