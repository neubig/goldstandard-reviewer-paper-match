{"title": "Input-Tuning: Adapting Unfamiliar Inputs to Frozen Pretrained Models", "abstract": "Recently the prompt-tuning paradigm has attracted significant attention. By only tuning continuous prompts with a frozen pretrained language model (PLM), prompt-tuning takes a step towards deploying a shared frozen PLM to serve numerous downstream tasks. Although prompt-tuning shows good performance on certain natural language understanding (NLU) tasks, its effectiveness on natural language generation (NLG) tasks is still underexplored. In this paper, we argue that one of the factors hindering the development of prompt-tuning on NLG tasks is the unfamiliar inputs (i.e., inputs are linguistically different from the pretraining corpus). For example, our preliminary exploration reveals a large performance gap between prompt-tuning and fine-tuning when unfamiliar inputs occur frequently in NLG tasks. This motivates us to propose input-tuning, which fine-tunes both the continuous prompts and the input representations, leading to a more effective way to adapt unfamiliar inputs to frozen PLMs. Our proposed input-tuning is conceptually simple and empirically powerful. Experimental results on seven NLG tasks demonstrate that input-tuning is significantly and consistently better than prompt-tuning. Furthermore, on three of these tasks, input-tuning can achieve a comparable or even better performance than fine-tuning.", "year": 2022, "ssId": "2e4cdd36d77b9d814638fc2cd6c703535cb1d2f7", "arXivId": "2203.03131", "link": "https://arxiv.org/pdf/2203.03131.pdf", "openAccess": true, "authors": ["Shengnan An", "Yifei Li", "Zeqi Lin", "Qian Liu", "Bei Chen", "Qiang Fu", "Weizhu Chen", "Nanning Zheng", "Jian-Guang Lou"]}