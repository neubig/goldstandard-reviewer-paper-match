{"title": "Learning Optimal Conditional Priors For Disentangled Representations", "abstract": "A large part of the literature on learning disentangled representations focuses on variational autoencoders (VAEs). Recent developments demonstrate that disentanglement cannot be obtained in a fully unsupervised setting without inductive biases on models and data. As such, Khemakhem et al., AISTATS 2020, suggest employing a factorized prior distribution over the latent variables that is conditionally dependent on auxiliary observed variables complementing input observations. While this is a remarkable advancement toward model identifiability, the learned conditional prior only focuses on sufficiency, giving no guarantees on a minimal representation. Motivated by information theoretic principles, we propose a novel VAE-based generative model with theoretical guarantees on disentanglement. Our proposed model learns a sufficient and compact - thus optimal - conditional prior, which serves as regularization for the latent space. Experimental results indicate superior performance with respect to state-of-the-art methods, according to several established metrics proposed in the literature on disentanglement.", "year": 2020, "ssId": "48b18bf5c9cad0e4c36b2d885f380c5c637e1a09", "arXivId": "2010.09360", "link": "https://arxiv.org/pdf/2010.09360.pdf", "openAccess": true, "authors": ["Graziano Mita", "M. Filippone", "P. Michiardi"]}