{"title": "Augmenting Pre-trained Language Models with QA-Memory for Open-Domain Question Answering", "abstract": "Existing state-of-the-art methods for open-domain question-answering (ODQA) gener-ally used a open book approach, in which information is retrieved from a large text corpus or knowledge base (KB), and then rea-soned with to produce an answer. A recent alternative is to retrieve from a collection of previously-generated question-answer pairs. This has several practical advantages, including being more memory- and compute-ef\ufb01cient. Question-answer pairs are also appealing in that they seem to be an intermediate between text and KB triples: like KB triples, they usually concisely express a single rela-tionship, but like text, they have good coverage. We describe a new QA system which aug-ments a text-to-text model with a large memory of question-answer pairs, and a new pre-training task for the latent step of question retrieval. The pre-training task substantially sim-pli\ufb01es training, and greatly improves performance on smaller QA benchmarks. Unlike prior systems of this sort, our QA system can also answer multi-hop questions that do not explicitly appear in the collection of stored question-answer pairs.", "year": 2022, "ssId": "190865e2c3d4908ff20bf9a31f5a2773d6fec5cb", "arXivId": "2204.04581", "link": "https://arxiv.org/pdf/2204.04581.pdf", "openAccess": true, "authors": ["Wenhu Chen", "Pat Verga", "M. D. Jong", "J. Wieting", "W. Cohen"]}