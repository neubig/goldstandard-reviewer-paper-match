{"title": "Multi-task learning for historical text normalization: Size matters", "abstract": "Historical text normalization suffers from small datasets that exhibit high variance, and previous work has shown that multi-task learning can be used to leverage data from related problems in order to obtain more robust models. Previous work has been limited to datasets from a specific language and a specific historical period, and it is not clear whether results generalize. It therefore remains an open problem, when historical text normalization benefits from multi-task learning. We explore the benefits of multi-task learning across 10 different datasets, representing different languages and periods. Our main finding\u2014contrary to what has been observed for other NLP tasks\u2014is that multi-task learning mainly works when target task data is very scarce.", "year": 2018, "ssId": "2a39a4f2d18e376ef8a6e2f45416e7b87957481e", "arXivId": null, "link": null, "openAccess": false, "authors": ["Marcel Bollmann", "Anders S\u00f8gaard", "Joachim Bingel"]}