{"title": "A Set of Recommendations for Assessing Human-Machine Parity in Language Translation", "abstract": "The quality of machine translation has increased remarkably over the past years, to the \ndegree that it was found to be indistinguishable from professional human translation in \na number of empirical investigations. We reassess Hassan et al.\u2019s 2018 investigation into \nChinese to English news translation, showing that the finding of human\u2013machine parity was \nowed to weaknesses in the evaluation design\u2014which is currently considered best practice in \nthe field. We show that the professional human translations contained significantly fewer \nerrors, and that perceived quality in human evaluation depends on the choice of raters, the \navailability of linguistic context, and the creation of reference translations. Our results call \nfor revisiting current best practices to assess strong machine translation systems in general \nand human\u2013machine parity in particular, for which we offer a set of recommendations based \non our empirical findings.", "year": 2020, "ssId": "407eacc5ade80b54126c300b57b81f4b4f411487", "arXivId": "2004.01694", "link": "https://arxiv.org/pdf/2004.01694.pdf", "openAccess": true, "authors": ["Samuel L\u00e4ubli", "Sheila Castilho", "Graham Neubig", "Rico Sennrich", "Qinlan Shen", "Antonio Toral"]}