{"title": "Towards Neural Diarization for Unlimited Numbers of Speakers Using Global and Local Attractors", "abstract": "Attractor-based end-to-end diarization is achieving comparable accuracy to the carefully tuned conventional clustering-based methods on challenging datasets. However, the main drawback is that it cannot deal with the case where the number of speakers is larger than the one observed during training. This is because its speaker counting relies on supervised learning. In this work, we introduce an unsupervised clustering process embedded in the attractor-based end-to-end diarization. We first split a sequence of frame-wise embeddings into short subsequences and then perform attractor-based diarization for each subsequence. Given subsequence-wise diarization results, inter-subsequence speaker correspondence is obtained by unsupervised clustering of the vectors computed from the attractors from all the subsequences. This makes it possible to produce diarization results of a large number of speakers for the whole recording even if the number of output speakers for each subsequence is limited. Experimental results showed that our method could produce accurate diarization results of an unseen number of speakers. Our method achieved 11.84 %, 28.33 %, and 19.49 % on the CALLHOME, DI-HARD II, and DIHARD III datasets, respectively, each of which is better than the conventional end-to-end diarization methods.", "year": 2021, "ssId": "6f173939f6defe3ebae8fb12f19349ba96b7b5c4", "arXivId": "2107.01545", "link": "https://arxiv.org/pdf/2107.01545.pdf", "openAccess": true, "authors": ["Shota Horiguchi", "Shinji Watanabe", "Paola Garc\u00eda", "Yawen Xue", "Yuki Takashima", "Y. Kawaguchi"]}