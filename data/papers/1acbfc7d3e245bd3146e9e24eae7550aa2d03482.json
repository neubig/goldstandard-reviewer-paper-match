{"title": "Theoretical Understanding of Batch-normalization: A Markov Chain Perspective", "abstract": "Batch-normalization (BN) is a key component to effectively train deep neural networks. Empirical evidence has shown that without BN, the training process is prone to unstabilities. This is however not well understood from a theoretical point of view. Leveraging tools from Markov chain theory, we show that BN has a direct effect on the rank of the pre-activation matrices of a neural network. Specifically, while deep networks without BN exhibit rank collapse and poor training performance, networks equipped with BN have a higher rank. In an extensive set of experiments on standard neural network architectures and datasets, we show that the latter quantity is a good predictor for the optimization speed of training.", "year": 2020, "ssId": "1acbfc7d3e245bd3146e9e24eae7550aa2d03482", "arXivId": "2003.01652", "link": "https://arxiv.org/pdf/2003.01652.pdf", "openAccess": true, "authors": ["Hadi Daneshmand", "J. Kohler", "F. Bach", "T. Hofmann", "Aur\u00e9lien Lucchi"]}