{"title": "Modeling Attrition in Recommender Systems with Departing Bandits", "abstract": "Traditionally, when recommender systems are formalized as multi-armed bandits, the policy of the recommender system in\ufb02uences the rewards accrued, but not the length of interac- tion. However, in real-world systems, dissatis\ufb01ed users may depart (and never come back). In this work, we propose a novel multi-armed bandit setup that captures such policy-dependent horizons. Our setup consists of a \ufb01nite set of user types , and multiple arms with Bernoulli payoffs. Each (user type, arm) tuple corresponds to an (unknown) reward proba- bility. Each user\u2019s type is initially unknown and can only be inferred through their response to recommendations. More- over, if a user is dissatis\ufb01ed with their recommendation, they might depart the system. We \ufb01rst address the case where all users share the same type, demonstrating that a recent UCB-based algorithm is optimal. We then move forward to the more challenging case, where users are divided among two types. While naive approaches cannot handle this setting, we provide an ef\ufb01cient learning algorithm that achieves \u02dc O ( \u221a T ) regret, where T is the number of users.", "year": 2022, "ssId": "3c37b9ec2ff1828877575acc600b73c3bcde138f", "arXivId": "2203.13423", "link": "https://arxiv.org/pdf/2203.13423.pdf", "openAccess": true, "authors": ["Omer Ben-Porat", "Lee Cohen", "Liu Leqi", "Zachary Chase Lipton", "Y. Mansour"]}