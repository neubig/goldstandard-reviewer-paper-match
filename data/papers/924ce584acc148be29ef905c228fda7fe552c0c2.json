{"title": "Efficient inference and learning in a large knowledge base", "abstract": "One important challenge for probabilistic logics is reasoning with very large knowledge bases (KBs) of imperfect information, such as those produced by modern web-scale information extraction systems. One scalability problem shared by many probabilistic logics is that answering queries involves \u201cgrounding\u201d the query\u2014i.e., mapping it to a propositional representation\u2014and the size of a \u201cgrounding\u201d grows with database size. To address this bottleneck, we present a first-order probabilistic language called ProPPR in which approximate \u201clocal groundings\u201d can be constructed in time independent of database size. Technically, ProPPR is an extension to stochastic logic programs that is biased towards short derivations; it is also closely related to an earlier relational learning algorithm called the path ranking algorithm. We show that the problem of constructing proofs for this logic is related to computation of personalized PageRank on a linearized version of the proof space, and based on this connection, we develop a provably-correct approximate grounding scheme, based on the PageRank\u2013Nibble algorithm. Building on this, we develop a fast and easily-parallelized weight-learning algorithm for ProPPR. In our experiments, we show that learning for ProPPR is orders of magnitude faster than learning for Markov logic networks; that allowing mutual recursion (joint learning) in KB inference leads to improvements in performance; and that ProPPR can learn weights for a mutually recursive program with hundreds of clauses defining scores of interrelated predicates over a KB containing one million entities.", "year": 2014, "ssId": "924ce584acc148be29ef905c228fda7fe552c0c2", "arXivId": "1404.3301", "link": "https://arxiv.org/pdf/1404.3301.pdf", "openAccess": true, "authors": ["William Yang Wang", "Kathryn Mazaitis", "N. Lao", "William W. Cohen"]}