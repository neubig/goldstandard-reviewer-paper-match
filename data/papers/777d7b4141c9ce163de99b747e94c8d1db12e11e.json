{"title": "Interpretable Privacy for Deep Learning Inference", "abstract": "In order to receive machine learning services from a cloud-based service provider, consumers usually send their entire raw data (e.g. an entire image). However, this models reveals much more information to the service provider than what is actually necessary for the execution of the service. This work shows that, in many cases, only a small portion of the input is required for the service provider to offer an accurate prediction. Discovering this subset is one of the main objectives of this paper. We formulate this problem as a gradient-based perturbation maximization method that discovers this subset in the input feature space with respect to the decision making of the prediction model used by the provider. After identifying the essential subset, our framework, Cloak, suppresses the rest of the features in the consumer\u2019s input and only sends the essential ones to the cloud. As such, the service provider can use those features to return an accurate prediction and also to improve its service, while at the same time the privacy of the consumer is better protected. We also demonstrate in our experiments that by removing the extra features, the post-hoc fairness of the classifier is improved as well.", "year": 2020, "ssId": "777d7b4141c9ce163de99b747e94c8d1db12e11e", "arXivId": null, "link": null, "openAccess": false, "authors": ["FatemehSadat Mireshghallah", "Mohammadkazem Taram", "A. Jalali", "Ahmed T. Elthakeb", "D. Tullsen", "H. Esmaeilzadeh"]}