{"title": "Reinforcement Learning under Drift", "abstract": "We propose algorithms with state-of-the-art \\emph{dynamic regret} bounds for un-discounted reinforcement learning under drifting non-stationarity, where both the reward functions and state transition distributions are allowed to evolve over time. Our main contributions are: 1) A tuned Sliding Window Upper-Confidence bound for Reinforcement Learning with Confidence-Widening (\\texttt{SWUCRL2-CW}) algorithm, which attains low dynamic regret bounds against the optimal non-stationary policy in various cases. 2) The Bandit-over-Reinforcement Learning (\\texttt{BORL}) framework that further permits us to enjoy these dynamic regret bounds in a parameter-free manner.", "year": 2019, "ssId": "930445d9cda71d6ff857e69aa5bb4b1bef7d31e5", "arXivId": "1906.02922", "link": "https://arxiv.org/pdf/1906.02922.pdf", "openAccess": true, "authors": ["Wang Chi Cheung", "D. Simchi-Levi", "Ruihao Zhu"]}