{"title": "Deep Bayesian Active Learning for Natural Language Processing: Results of a Large-Scale Empirical Study", "abstract": "Several recent papers investigate Active Learning (AL) for mitigating the data dependence of deep learning for natural language processing. However, the applicability of AL to real-world problems remains an open question. While in supervised learning, practitioners can try many different methods, evaluating each against a validation set before selecting a model, AL affords no such luxury. Over the course of one AL run, an agent annotates its dataset exhausting its labeling budget. Thus, given a new task, we have no opportunity to compare models and acquisition functions. This paper provides a large-scale empirical study of deep active learning, addressing multiple tasks and, for each, multiple datasets, multiple models, and a full suite of acquisition functions. We find that across all settings, Bayesian active learning by disagreement, using uncertainty estimates provided either by Dropout or Bayes-by-Backprop significantly improves over i.i.d. baselines and usually outperforms classic uncertainty sampling.", "year": 2018, "ssId": "838fbfd9066dbbac6c10059c5b183046fb1cd9d1", "arXivId": "1808.05697", "link": "https://arxiv.org/pdf/1808.05697.pdf", "openAccess": true, "authors": ["Aditya Siddhant", "Zachary Chase Lipton"]}