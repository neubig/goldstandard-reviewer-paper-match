{"title": "Combating Reinforcement Learning's Sisyphean Curse with Intrinsic Fear", "abstract": "To use deep reinforcement learning in the wild, we might hope for an agent that can avoid catastrophic mistakes. Unfortunately, even in simple environments, the popular deep Q-network (DQN) algorithm is doomed by a Sisyphean curse. Owing to the use of function approximation, these agents may eventually forget experiences as they become exceedingly unlikely under a new policy. Consequently, for as long as they continue to train, DQNs may periodically repeat avoidable catastrophic mistakes. In this paper, we learn a reward shaping that accelerates learning and guards oscillating policies against repeated catastrophes. First, we demonstrate unacceptable performance of DQNs on two toy problems. We then introduce intrinsic fear, a new method that mitigates these problems by avoiding dangerous states. Our approach incorporates a second model trained via supervised learning to predict the probability of catastrophe within a short number of steps. This score then acts to penalize the Q-learning objective. Equipped with intrinsic fear, our DQNs solve the toy environments and improve on the Atari games Seaquest, Asteroids, and Freeway.", "year": 2016, "ssId": "616cc6826066184a8c77c3f2562e4e891ce42911", "arXivId": "1611.01211", "link": "https://arxiv.org/pdf/1611.01211.pdf", "openAccess": true, "authors": ["Zachary Chase Lipton", "Jianfeng Gao", "Lihong Li", "Jianshu Chen", "L. Deng"]}