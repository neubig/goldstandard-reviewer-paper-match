{"title": "A Study of All-Convolutional Encoders for Connectionist Temporal Classification", "abstract": "Connectionist temporal classification (CTC) is a popular sequence prediction approach for automatic speech recognition that is typically used with models based on recurrent neural networks (RNNs). We explore whether deep convolutional neural networks (CNNs) can be used effectively instead of RNNs as the \u201cencoder\u201d in CTC. CNNs lack an explicit representation of the entire sequence, but have the advantage that they are much faster to train. We present an exploration of CNN s as encoders for CTC models, in the context of character-based (lexicon-free) automatic speech recognition. In particular, we explore a range of one-dimensional convolutionallayers, which are particularly efficient. We compare the performance of our CNN-based models against typical RNN-based models in terms of training time, decoding time, model size and word error rate (WER) on the Switchboard Eva12000 corpus. We find that our CNN-based models are close in performance to LSTMs, while not matching them, and are much faster to train and decode.", "year": 2017, "ssId": "57b972ebe314cfe8e57fd6b9f9239123eb70e979", "arXivId": "1710.10398", "link": "https://arxiv.org/pdf/1710.10398.pdf", "openAccess": true, "authors": ["Kalpesh Krishna", "Liang Lu", "Kevin Gimpel", "Karen Livescu"]}