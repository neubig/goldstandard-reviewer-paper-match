{"title": "Crossmodal Clustered Contrastive Learning: Grounding of Spoken Language to Gesture", "abstract": "Crossmodal grounding is a key technical challenge when generating relevant and well-timed gestures from spoken language. Often, the same gesture can accompany semantically different spoken language phrases which makes crossmodal grounding especially challenging. For example, a gesture (semi-circular with both hands) could co-occur with semantically different phrases \u201dentire bottom row\u201d (referring to a physical point) and \u201dmolecules expand and decay\u201d (referring to a scientific phenomena). In this paper, we introduce a self-supervised approach to learn representations better suited to such many-to-one grounding relationships between spoken language and gestures. As part of this approach, we propose a new contrastive loss function, Crossmodal Cluster NCE, that guides the model to learn spoken language representations which are consistent with the similarities in the gesture space. This gesture-aware space can help us generate more relevant gestures given language as input. We demonstrate the effectiveness of our approach on a publicly available dataset through quantitative and qualitative evaluations. Our proposed methodology significantly outperforms prior approaches for gestures-language grounding. Link to code: https://github.com/dondongwon/CC_NCE_GENEA.", "year": 2021, "ssId": "e03d9684a19c8f8e29ee97b347d4f1e280a88e44", "arXivId": null, "link": null, "openAccess": false, "authors": ["Dongwon Lee", "Chaitanya Ahuja"]}