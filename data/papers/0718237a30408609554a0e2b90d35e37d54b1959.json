{"title": "Entropy-Based Subword Mining with an Application to Word Embeddings", "abstract": "Recent literature has shown a wide variety of benefits to mapping traditional onehot representations of words and phrases to lower-dimensional real-valued vectors known as word embeddings. Traditionally, most word embedding algorithms treat each word as the finest meaningful semantic granularity and perform embedding by learning distinct embedding vectors for each word. Contrary to this line of thought, technical domains such as scientific and medical literature compose words from subword structures such as prefixes, suffixes, and root-words as well as compound words. Treating individual words as the finest-granularity unit discards meaningful shared semantic structure between words sharing substructures. This not only leads to poor embeddings for text corpora that have longtail distributions, but also heuristic methods for handling out-of-vocabulary words. In this paper we propose SubwordMine, an entropybased subword mining algorithm that is fast, unsupervised, and fully data-driven. We show that this allows for great cross-domain performance in identifying semantically meaningful subwords. We then investigate utilizing the mined subwords within the FastText embedding model and compare performance of the learned representations in a downstream language modeling task.", "year": 2018, "ssId": "0718237a30408609554a0e2b90d35e37d54b1959", "arXivId": null, "link": null, "openAccess": false, "authors": ["Ahmed El-Kishky", "Frank F. Xu", "Aston Zhang", "Stephen Macke", "Jiawei Han"]}