{"title": "Multilingual Multimodal Pre-training for Zero-Shot Cross-Lingual Transfer of Vision-Language Models", "abstract": "This paper studies zero-shot cross-lingual transfer of vision-language models. Specifically, we focus on multilingual text-to-video search and propose a Transformer-based model that learns contextual multilingual multimodal embeddings. Under a zero-shot setting, we empirically demonstrate that performance degrades significantly when we query the multilingual text-video model with non-English sentences. To address this problem, we introduce a multilingual multimodal pre-training strategy, and collect a new multilingual instructional video dataset (Multi-HowTo100M) for pre-training. Experiments on VTT show that our method significantly improves video search in non-English languages without additional annotations. Furthermore, when multilingual annotations are available, our method outperforms recent baselines by a large margin in multilingual text-to-video search on VTT and VATEX; as well as in multilingual text-to-image search on Multi30K. Our model and Multi-HowTo100M is available at http://github.com/berniebear/Multi-HT100M.", "year": 2021, "ssId": "25efc17ba82ba4af29f2e03868de74e1ea66d025", "arXivId": "2103.08849", "link": "https://arxiv.org/pdf/2103.08849.pdf", "openAccess": true, "authors": ["Po-Yao Huang", "Mandela Patrick", "Junjie Hu", "Graham Neubig", "Florian Metze", "A. Hauptmann"]}