{"title": "Progressive Transfer Learning", "abstract": "Model fine-tuning is a widely used transfer learning approach in person Re-identification (ReID) applications, which fine-tuning a pre-trained feature extraction model into the target scenario instead of training a model from scratch. It is challenging due to the significant variations inside the target scenario, e.g., different camera viewpoint, illumination changes, and occlusion. These variations result in a gap between each mini-batch\u2019s distribution and the whole dataset\u2019s distribution when using mini-batch training. In this paper, we study model fine-tuning from the perspective of the aggregation and utilization of the dataset\u2019s global information when using mini-batch training. Specifically, we introduce a novel network structure called Batch-related Convolutional Cell (BConv-Cell), which progressively collects the dataset\u2019s global information into a latent state and uses it to rectify the extracted feature. Based on BConv-Cells, we further proposed the Progressive Transfer Learning (PTL) method to facilitate the model fine-tuning process by jointly optimizing BConv-Cells and the pre-trained ReID model. Empirical experiments show that our proposal can greatly improve the ReID model\u2019s performance on MSMT17, Market-1501, CUHK03, and DukeMTMC-reID datasets. Moreover, we extend our proposal to the general image classification task. The experiments in several image classification benchmark datasets demonstrate that our proposal can significantly improve baseline models\u2019 performance. The code has been released at https://github.com/ZJULearning/PTL", "year": 2020, "ssId": "01a21d74fb7414404851872f23cdca42243ab6a8", "arXivId": "1908.02492", "link": "https://arxiv.org/pdf/1908.02492.pdf", "openAccess": true, "authors": ["Zhengxu Yu", "Dong Shen", "Zhongming Jin", "Jianqiang Huang", "Deng Cai", "Xiansheng Hua"]}