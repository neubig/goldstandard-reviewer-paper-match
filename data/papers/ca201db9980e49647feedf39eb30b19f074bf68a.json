{"title": "SPGISpeech: 5, 000 hours of transcribed financial audio for fully formatted end-to-end speech recognition", "abstract": "In the English speech-to-text (STT) machine learning task, acoustic models are conventionally trained on uncased Latin characters, and any necessary orthography (such as capitalization, punctuation, and denormalization of non-standard words) is imputed by separate post-processing models. This adds complexity and limits performance, as many formatting tasks benefit from semantic information present in the acoustic signal but absent in transcription. Here we propose a new STT task: endto-end neural transcription with fully formatted text for target labels. We present baseline Conformer-based models trained on a corpus of 5,000 hours of professionally transcribed earnings calls, achieving a CER of 1.7. As a contribution to the STT research community, we release the corpus free for noncommercial use.", "year": 2021, "ssId": "ca201db9980e49647feedf39eb30b19f074bf68a", "arXivId": "2104.02014", "link": "https://arxiv.org/pdf/2104.02014.pdf", "openAccess": true, "authors": ["Patrick K. O\u2019Neill", "Vitaly Lavrukhin", "Somshubra Majumdar", "V. Noroozi", "Yuekai Zhang", "O. Kuchaiev", "J. Balam", "Yuliya Dovzhenko", "Keenan Freyberg", "Michael D. Shulman", "Boris Ginsburg", "Shinji Watanabe", "G. Kucsko"]}