{"title": "Shallow pooling for sparse labels", "abstract": "Recent years have seen enormous gains in core information retrieval tasks, including document and passage ranking. Datasets and leaderboards, and in particular the MS MARCO datasets, illustrate the dramatic improvements achieved by modern neural rankers. When compared with traditional information retrieval test collections, such as those developed by TREC, the MS MARCO datasets employ substantially more queries \u2014 thousands vs. dozens \u2013 with substantially fewer known relevant items per query \u2014 often just one. For example, 94% of the nearly seven thousand queries in the MS MARCO passage ranking development set have only a single known relevant passage, and no query has more than four. Given the sparsity of these relevance labels, the MS MARCO leaderboards track improvements with mean reciprocal rank (MRR). In essence, a relevant item is treated as the \u201cright answer\u201d, with rankers scored on their ability to place this item as high in the ranking as possible. In working with these sparse labels, we have observed that the top items returned by a ranker often appear superior to judged relevant items. Others have reported the same observation. To test this observation, we employed crowdsourced workers to make preference judgments between the top item returned by a modern neural ranking stack and a judged relevant item for the nearly seven thousand queries in the passage ranking development set. The results support our observation. If we imagine a perfect ranker under MRR, with a score of 1 on all queries, our preference judgments indicate that a searcher would prefer the top result from a modern neural ranking stack more frequently than the top result from the imaginary perfect ranker, making our neural ranker \u201cbetter than perfect\u201d. To understand the implications for the leaderboard, we pooled the top document from available runs near the top of the passage ranking leaderboard for over 500 queries. We employed crowdsourced workers to make preference judgments over these pools and re-evaluated the runs. Our results support our concerns that current MS MARCO datasets may no longer be able to recognize genuine improvements in rankers. In future, if rankers are measured against a single \u201cright answer\u201d, this answer should be the best answer or most preferred answer, and maintained with ongoing judgments. Since only the best answer is required, this ongoing maintenance might be performed with shallow pooling. When a previously unjudged document is surfaced as the top item in a ranking, it can directly compared with the previous known best answer. 1 ar X iv :2 10 9. 00 06 2v 1 [ cs .I R ] 3 1 A ug 2 02 1", "year": 2021, "ssId": "6e2e7df21a5b5457ea4167133a40bc729028250d", "arXivId": "2109.00062", "link": "https://arxiv.org/pdf/2109.00062.pdf", "openAccess": true, "authors": ["Negar Arabzadeh", "A. Vtyurina", "Xinyi Yan", "C. Clarke"]}