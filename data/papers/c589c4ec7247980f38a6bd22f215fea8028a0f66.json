{"title": "Re-Examining Human Annotations for Interpretable NLP", "abstract": "Explanation methods in Interpretable NLP often explain the model\u2019s decision by extracting evidence (rationale) from the input texts supporting the decision. Benchmark datasets for rationales have been released to evaluate how good the rationale is. The ground truth rationales in these datasets are of- ten human annotations obtained via crowd-sourced websites. Valuable as these datasets are, the details on how those human annotations are obtained are often not clearly speci\ufb01ed. We conduct comprehensive controlled experiments using crowd- sourced websites on two widely used datasets in Interpretable NLP to understand how those unsaid details can affect the annotation results. Speci\ufb01cally, we compare the annotation results obtained from recruiting workers satisfying different levels of quali\ufb01cation. We also provide high-quality workers with different instructions for completing the same under- lying tasks. Our results reveal that the annotation quality is highly subject to the workers\u2019 quali\ufb01cation, and workers can be guided to provide certain annotations by the instructions. We further show that speci\ufb01c explanation methods perform better when evaluated using the ground truth rationales obtained by particular instructions. Based on these observations, we highlight the importance of providing complete details of the annotation process and call for careful interpretation of any experiment results obtained using those annotations.", "year": 2022, "ssId": "c589c4ec7247980f38a6bd22f215fea8028a0f66", "arXivId": "2204.04580", "link": "https://arxiv.org/pdf/2204.04580.pdf", "openAccess": true, "authors": ["David C. Chiang", "Hung-yi Lee"]}