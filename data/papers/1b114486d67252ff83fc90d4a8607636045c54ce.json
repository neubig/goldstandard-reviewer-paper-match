{"title": "Poster: Learning to Mine Parallel Natural Language/Source Code Corpora from Stack Overflow", "abstract": "For tasks like code synthesis from natural language, code retrieval, and code summarization, data-driven models have shown great promise. However, creating these models requires parallel data between natural language (NL) and code with fine-grained alignments. Stack Overflow (SO) is a promising source to create such a data set but existing heuristic methods are limited both in their coverage and the correctness of the NL-code pairs obtained. In this paper, we propose a method to mine high-quality aligned data from SO by training a classifier using two sets of features: hand-crafted features considering the structure of the extracted snippets, and correspondence features obtained by training a neural network model to capture the correlation between NL and code. Experiments using Python and Java as test beds show that the proposed method greatly expands coverage and accuracy over existing mining methods, even when using only a small number of labeled examples.", "year": 2018, "ssId": "1b114486d67252ff83fc90d4a8607636045c54ce", "arXivId": null, "link": null, "openAccess": false, "authors": ["Pengcheng Yin", "Bowen Deng", "Edgar Chen", "Bogdan Vasilescu", "Graham Neubig"]}