{"title": "A Unified Theory of Decentralized SGD with Changing Topology and Local Updates", "abstract": "Decentralized stochastic optimization methods have gained a lot of attention recently, mainly because of their cheap per iteration cost, data locality, and their communication-efficiency. In this paper we introduce a unified convergence analysis that covers a large variety of decentralized SGD methods which so far have required different intuitions, have different applications, and which have been developed separately in various communities. \nOur algorithmic framework covers local SGD updates and synchronous and pairwise gossip updates on adaptive network topology. We derive universal convergence rates for smooth (convex and non-convex) problems and the rates interpolate between the heterogeneous (non-identically distributed data) and iid-data settings, recovering linear convergence rates in many special cases, for instance for over-parametrized models. Our proofs rely on weak assumptions (typically improving over prior work in several aspects) and recover (and improve) the best known complexity results for a host of important scenarios, such as for instance coorperative SGD and federated averaging (local SGD).", "year": 2020, "ssId": "c933fed82e7b5cbf7230f0f970b69590b40f86a1", "arXivId": "2003.10422", "link": "https://arxiv.org/pdf/2003.10422.pdf", "openAccess": true, "authors": ["Anastasia Koloskova", "Nicolas Loizou", "Sadra Boreiri", "Martin Jaggi", "Sebastian U. Stich"]}