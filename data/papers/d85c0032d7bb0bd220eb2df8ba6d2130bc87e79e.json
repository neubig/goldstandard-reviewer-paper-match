{"title": "Semi-Supervised Training with Pseudo-Labeling for End-to-End Neural Diarization", "abstract": "In this paper, we present a semi-supervised training technique using pseudo-labeling for end-to-end neural diarization (EEND). The EEND system has shown promising performance compared with traditional clustering-based methods, especially in the case of overlapping speech. However, to get a welltuned model, EEND requires labeled data for all the joint speech activities of every speaker at each time frame in a recording. In this paper, we explore a pseudo-labeling approach that employs unlabeled data. First, we propose an iterative pseudolabel method for EEND, which trains the model using unlabeled data of a target condition. Then, we also propose a committeebased training method to improve the performance of EEND. To evaluate our proposed method, we conduct the experiments of model adaptation using labeled and unlabeled data. Experimental results on the CALLHOME dataset show that our proposed pseudo-label achieved a 37.4% relative diarization error rate reduction compared to a seed model. Moreover, we analyzed the results of semi-supervised adaptation with pseudo-labeling. We also show the effectiveness of our approach on the third DIHARD dataset.", "year": 2021, "ssId": "d85c0032d7bb0bd220eb2df8ba6d2130bc87e79e", "arXivId": "2106.04764", "link": "https://arxiv.org/pdf/2106.04764.pdf", "openAccess": true, "authors": ["Yuki Takashima", "Yusuke Fujita", "Shota Horiguchi", "Shinji Watanabe", "Paola Garc'ia", "Kenji Nagamatsu"]}