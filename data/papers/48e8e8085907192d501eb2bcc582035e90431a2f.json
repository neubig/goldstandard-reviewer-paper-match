{"title": "Multi-Task Cross-Lingual Sequence Tagging from Scratch", "abstract": "We present a deep hierarchical recurrent neural network for sequence tagging. Given a sequence of words, our model employs deep gated recurrent units on both character and word levels to encode morphology and context information, and applies a conditional random field layer to predict the tags. Our model is task independent, language independent, and feature engineering free. We further extend our model to multi-task and cross-lingual joint training by sharing the architecture and parameters. Our model achieves state-of-the-art results in multiple languages on several benchmark tasks including POS tagging, chunking, and NER. We also demonstrate that multi-task and cross-lingual joint training can improve the performance in various cases.", "year": 2016, "ssId": "48e8e8085907192d501eb2bcc582035e90431a2f", "arXivId": "1603.06270", "link": "https://arxiv.org/pdf/1603.06270.pdf", "openAccess": true, "authors": ["Zhilin Yang", "R. Salakhutdinov", "William W. Cohen"]}