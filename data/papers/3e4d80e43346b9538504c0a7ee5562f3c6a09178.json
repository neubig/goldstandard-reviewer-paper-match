{"title": "Incentives in the Dark: Multi-armed Bandits for Evolving Users with Unknown Type", "abstract": "Design of incentives or recommendations to users is becoming more common as platform providers continually emerge. We propose a multi-armed bandit approach to the problem in which users types are unknown a priori and evolve dynamically in time. Unlike the traditional bandit setting, observed rewards are generated by a single Markov process. We demonstrate via an illustrative example that blindly applying the traditional bandit algorithms results in very poor performance as measured by regret. We introduce two variants of classical bandit algorithms, upper confidence bound (UCB) and epsilon-greedy, for which we provide theoretical bounds on the regret. We conduct a number of simulation-based experiments to show how the algorithms perform in comparison to traditional UCB and epsilon-greedy algorithms as well as reinforcement learning (Q-learning).", "year": 2018, "ssId": "3e4d80e43346b9538504c0a7ee5562f3c6a09178", "arXivId": null, "link": null, "openAccess": false, "authors": ["L. Ratliff", "S. Sekar", "Liyuan Zheng", "Tanner Fiez"]}