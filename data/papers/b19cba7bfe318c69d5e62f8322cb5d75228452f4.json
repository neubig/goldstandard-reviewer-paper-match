{"title": "Compacter: Efficient Low-Rank Hypercomplex Adapter Layers", "abstract": "Adapting large-scale pretrained language models to downstream tasks via fine-tuning is the standard method for achieving state-of-the-art performance on NLP benchmarks. However, fine-tuning all weights of models with millions or billions of parameters is sample-inefficient, unstable in low-resource settings, and wasteful as it requires storing a separate copy of the model for each task. Recent work has developed parameter-efficient fine-tuning methods, but these approaches either still require a relatively large number of parameters or underperform standard fine-tuning. In this work, we propose COMPACTER, a method for fine-tuning large-scale language models with a better trade-off between task performance and the number of trainable parameters than prior work. COMPACTER accomplishes this by building on top of ideas from adapters, low-rank optimization, and parameterized hypercomplex multiplication layers. Specifically, COMPACTER inserts task-specific weight matrices into a pretrained model\u2019s weights, which are computed efficiently as a sum of Kronecker products between shared \u201cslow\u201d weights and \u201cfast\u201d rank-one matrices defined per COMPACTER layer. By only training 0.047% of a pretrained model\u2019s parameters, COMPACTER performs on par with standard fine-tuning on GLUE and outperforms standard fine-tuning on SuperGLUE and low-resource settings. Our code is publicly available at https://github.com/rabeehk/compacter.", "year": 2021, "ssId": "b19cba7bfe318c69d5e62f8322cb5d75228452f4", "arXivId": "2106.04647", "link": "https://arxiv.org/pdf/2106.04647.pdf", "openAccess": true, "authors": ["Rabeeh Karimi Mahabadi", "James Henderson", "Sebastian Ruder"]}