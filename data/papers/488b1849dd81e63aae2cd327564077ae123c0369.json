{"title": "Distributed Methods with Absolute Compression and Error Compensation", "abstract": "Distributed optimization methods are often applied to solving huge-scale problems like training neural networks with millions and even billions of parameters. In such applications, commu-nicating full vectors, e.g., (stochastic) gradients, iterates, is prohibitively expensive, especially when the number of workers is large. Communication compression is a powerful approach to al-leviating this issue, and, in particular, methods with biased compression and error compensation are extremely popular due to their practical e\ufb03ciency. Sahu et al. (2021) [29] propose a new analysis of Error Compensated SGD ( EC-SGD ) for the class of absolute compression operators showing that in a certain sense, this class contains optimal compressors for EC-SGD . However, the analysis was conducted only under the so-called ( M, \u03c3 2 )-bounded noise assumption. In this paper, we generalize the analysis of EC-SGD with absolute compression to the arbitrary sampling strategy and propose the \ufb01rst analysis of EC-LSVRG [9] with absolute compression for (strongly) convex problems. Our rates improve upon the previously known ones in this setting. Our theoretical \ufb01ndings are corroborated by several numerical experiments.", "year": 2022, "ssId": "488b1849dd81e63aae2cd327564077ae123c0369", "arXivId": "2203.02383", "link": "https://arxiv.org/pdf/2203.02383.pdf", "openAccess": true, "authors": ["Marina Danilova", "Eduard A. Gorbunov"]}