{"title": "Explaining Neural Scaling Laws", "abstract": "The test loss of well-trained neural networks often follows precise power-law scaling relations with either the size of the training dataset or the number of parameters in the network. We propose a theory that explains and connects these scaling laws. We identify variance-limited and resolution-limited scaling behavior for both dataset and model size, for a total of four scaling regimes. The variance-limited scaling follows simply from the existence of a well-behaved infinite data or infinite width limit, while the resolution-limited regime can be explained by positing that models are effectively resolving a smooth data manifold. In the large width limit, this can be equivalently obtained from the spectrum of certain kernels, and we present evidence that large width and large dataset resolution-limited scaling exponents are related by a duality. We exhibit all four scaling regimes in the controlled setting of large random feature and pretrained models and test the predictions empirically on a range of standard architectures and datasets. We also observe several empirical relationships between datasets and scaling exponents: super-classing image tasks does not change exponents, while changing input distribution (via changing datasets or adding noise) has a strong effect. We further explore the effect of architecture aspect ratio on scaling exponents. 1 Scaling Laws for Neural Networks For a large variety of models and datasets, neural network performance has been empirically observed to scale as a power-law with model size and dataset size [1\u20134]. We would like to understand why these power laws emerge, and what features of the data and models determine the values of the power-law exponents. Since these exponents determine how quickly performance improves with more data and larger models, they are of great importance when considering whether to scale up existing models. In this work, we present a theoretical framework for explaining scaling laws in trained neural networks. We identify four related scaling regimes with respect to the number of model parameters P and the dataset size D. With respect to each of D, P , there is both a resolution-limited regime and a variance-limited regime. Variance-Limited Regime In the limit of infinite data or an arbitrarily wide model, some aspects of neural network training simplify. Specifically, if we fix one of D,P and study scaling with respect to the other parameter as it becomes arbitrarily large, then the loss scales as 1/x, i.e. as a power-law with exponent 1, with x = D or \u221a P \u221d width in deep networks and x = D or P in linear models. In essence, this variance-limited regime is amenable to analysis because model predictions can be series expanded in either inverse width or inverse dataset size. To demonstrate these variance-limited scalings, it is sufficient to argue that the infinite data or width limit exists and is smooth; this guarantees that an expansion in simple integer powers exists. \u2217Authors listed alphabetically \u2020A portion of work completed during an internship at Google. 1 ar X iv :2 10 2. 06 70 1v 1 [ cs .L G ] 1 2 Fe b 20 21 101 102 103 104 Dataset size (D) 10 8 10 6 10 4 10 2 100 Lo ss Lo ss ( ) Variance-limited : Theory D = 1 D: 1.02 D: 1.01 D: 1.00 (MSE) D: 0.98 (CNN) D: 1.01 D: 1.02 D: 1.10 D: 1.01 103 104 Dataset size (D) 10 1 100 Lo ss Resolution-limited D: 0.26 D: 0.37 D: 0.40 D: 0.58 101 102 Width 10 1 100 Lo ss Resolution-limited W: 0.46 W: 0.34 W: 0.62 W: 0.40 102 103 104 Width 10 5 10 4 10 3 10 2 10 1 100 Lo ss Lo ss ( ) Variance-limited : Theory W = 1 W: 0.98 (MSE, ERF) W: 1.03 W: 1.02 (ERF) W: 1.01 W: 1.00 W: 1.03 (ERF) Teacher-Student CIFAR-10 CIFAR-100 SVHN FashionMNIST MNIST Figure 1: Four scaling regimes Here we exhibit the four regimes we focus on in this work. (top-left, bottomright) Variance-limited scaling of under-parameterized models with dataset size and over-parameterized models with number of parameters (width) exhibit universal scaling (\u03b1D = \u03b1W = 1) independent of the architecture or underlying dataset. (top-right, bottom-left) Resolution-limited over-parameterized models with dataset or under-parameterized models with model size exhibit scaling with exponents that depend on the details of the data distribution. These four regimes are also found in random feature (Figure 3) and pretrained models (see supplement). Resolution-Limited Regime In this regime, one of D or P is effectively infinite, and we study scaling as the other parameter increases. In this case, a variety of works have empirically observed power-law scalings 1/x, typically with 0 < \u03b1 < 1 for both x = P or D. We can provide a very general argument for power-law scalings if we assume that trained models map the data into a d-dimensional data manifold. The key idea is then that additional data (in the infinite model-size limit) or added model parameters (in the infinite data limit) are used by the model to carve up the data manifold into smaller components. The model then makes independent predictions in each component of the data manifold in order to optimize the training loss. If the underlying data varies continuously on the manifold, then the size of the sub-regions into which we can divide the manifold (rather than the number of regions) determines the model\u2019s loss. To shrink the size of the sub-regions by a factor of 2 requires increasing the parameter count or dataset size by a factor of 2, and so the inverse of the scaling exponent will be proportional to the intrinsic dimension d of the data manifold, so that \u03b1 \u221d 1/d. A visualization of this successively better approximation with dataset size is shown in Figure 2 for models trained to predict data generated by a random fully-connected network. Explicit Realization These regimes can be realized in linear models, and this includes linearized versions of neural networks via the large width limit. In these limits, we can solve for the test error directly in terms of the feature covariance (kernel). The scaling of the test loss then follows from the asymptotic decay of the spectrum of the covariance matrix. Furthermore, well-known theorems provide bounds on the spectra associated with continuous kernels on a d-dimensional manifold. Since otherwise generic kernels saturate these bounds, we find a tight connection between the dimension of the data manifold, kernel spectra, and", "year": 2021, "ssId": "6b2b5d3d9a2ca4bc4fbd81551a62370be2fbff1b", "arXivId": "2102.06701", "link": "https://arxiv.org/pdf/2102.06701.pdf", "openAccess": true, "authors": ["Yasaman Bahri", "Ethan Dyer", "J. Kaplan", "Jaehoon Lee", "Utkarsh Sharma"]}