{"title": "Sentence-T5: Scalable Sentence Encoders from Pre-trained Text-to-Text Models", "abstract": "We provide the first exploration of text-to-text transformers (T5) sentence embeddings. Sentence embeddings are broadly useful for language processing tasks. While T5 achieves impressive performance on language tasks cast as sequence-to-sequence mapping problems, it is unclear how to produce sentence embeddings from encoder-decoder models. We investigate three methods for extracting T5 sentence embeddings: two utilize only the T5 encoder and one uses the full T5 encoder-decoder model. Our encoder-only models outperforms BERTbased sentence embeddings on both transfer tasks and semantic textual similarity (STS). Our encoder-decoder method achieves further improvement on STS. Scaling up T5 from millions to billions of parameters is found to produce consistent improvements on downstream tasks. Finally, we introduce a two-stage contrastive learning approach that achieves a new state-of-art on STS using sentence embeddings, outperforming both Sentence BERT (Reimers and Gurevych, 2019) and SimCSE (Gao et al., 2021).", "year": 2021, "ssId": "dbe87b171bfb789e1d22a047aeeee69105e6fd02", "arXivId": "2108.08877", "link": "https://arxiv.org/pdf/2108.08877.pdf", "openAccess": true, "authors": ["Jianmo Ni", "Gustavo Hern'andez 'Abrego", "Noah Constant", "Ji Ma", "Keith B. Hall", "Daniel Matthew Cer", "Yinfei Yang"]}