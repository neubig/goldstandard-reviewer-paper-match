{"title": "Improving Diversity of Neural Text Generation via Inverse Probability Weighting", "abstract": "The neural network based text generation suffers from the text degeneration issue such as repetition. Although top-k sampling and nucleus sampling outperform beam search based decoding methods, they only focus on truncating the \u201ctail\u201d of the distribution and do not address the \u201chead\u201d part, which we show might contain tedious or even repetitive candidates with high probability that lead to repetition loops. They also do not fully address the issue that human text does not always favor high probability words. To explore improved diversity for text generation, we propose a heuristic sampling method inspired by inverse probability weighting. We propose to use interquartile range of the predicted distribution to determine the \u201chead\u201d part, then permutate and rescale the \u201chead\u201d with inverse probability. This aims at decreasing the probability for the tedious and possibly repetitive candidates with higher probability, and increasing the probability for the rational but more surprising candidates with lower probability. The proposed algorithm provides a controllable variation on the predicted distribution which enhances diversity without compromising rationality of the distribution. We use pre-trained language model to compare our algorithm with nucleus sampling. Results show that our algorithm can effectively increase the diversity of generated samples while achieving close resemblance to human text.", "year": 2021, "ssId": "981152cb3f1e11c9cee6af2275b57ef79c621934", "arXivId": "2103.07649", "link": "https://arxiv.org/pdf/2103.07649.pdf", "openAccess": true, "authors": ["Xinran Zhang", "Maosong Sun", "Jiafeng Liu", "Xiaobing Li"]}