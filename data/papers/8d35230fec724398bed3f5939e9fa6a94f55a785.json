{"title": "Differential Privacy and Machine Learning: a Survey and Review", "abstract": "The objective of machine learning is to extract useful information from data, while privacy is preserved by concealing information. Thus it seems hard to reconcile these competing interests. However, they frequently must be balanced when mining sensitive data. For example, medical research represents an important application where it is necessary both to extract useful information and protect patient privacy. One way to resolve the conflict is to extract general characteristics of whole populations without disclosing the private information of individuals. \nIn this paper, we consider differential privacy, one of the most popular and powerful definitions of privacy. We explore the interplay between machine learning and differential privacy, namely privacy-preserving machine learning algorithms and learning-based data release mechanisms. We also describe some theoretical results that address what can be learned differentially privately and upper bounds of loss functions for differentially private algorithms. \nFinally, we present some open questions, including how to incorporate public data, how to deal with missing data in private datasets, and whether, as the number of observed samples grows arbitrarily large, differentially private machine learning algorithms can be achieved at no cost to utility as compared to corresponding non-differentially private algorithms.", "year": 2014, "ssId": "8d35230fec724398bed3f5939e9fa6a94f55a785", "arXivId": "1412.7584", "link": "https://arxiv.org/pdf/1412.7584.pdf", "openAccess": true, "authors": ["Zhanglong Ji", "Zachary Chase Lipton", "C. Elkan"]}