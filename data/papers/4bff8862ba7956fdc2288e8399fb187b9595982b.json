{"title": "UvA-DARE ( Digital Academic Repository ) Overview of BioCreative II gene mention recognition", "abstract": "Nineteen teams presented results for the Gene Mention Task at the BioCreative II Workshop. In this task participants designed systems to identify substrings in sentences corresponding to gene name mentions. A variety of different methods were used and the results varied with a highest achieved F1 score of 0.8721. Here we present brief descriptions of all the methods used and a statistical analysis of the results. We also demonstrate that, by combining the results from all submissions, an F score of 0.9066 is feasible, and furthermore that the best result makes use of the lowest scoring submissions. Published: 01 September 2008 Genome Biology 2008, 9(Suppl 2):S2 doi: 10.1186/gb-2008-9-S2-S2 The electronic version of this article is the complete one and can be found online at http://genomebiology.com/2008/9/S2/S2 Genome Biology 2008, 9(Suppl 2):S2 http://genomebiology.com/2008/9/S2/S2 Genome Biology 2008, Volume 9, Suppl 2, Article S2 Smith et al. S2.2 Background Finding gene names in scientific text is both important and difficult. It is important because it is needed for tasks such as document retrieval, information extraction, summarization, and automated text mining, reasoning, and discovery. Technically, finding gene names in text is a kind of named entity recognition (NER) similar to the tasks of finding person names and company names in newspaper text [1]. However, a combination of characteristics, some of which are common to other domains, makes gene names particularly difficult to recognize automatically. \u2022 Millions of gene names are used. \u2022 New names are created continuously. \u2022 Authors usually do not use proposed standardized names, which means that the name used depends on preference. \u2022 Gene names naturally co-occur with other types, such as cell names, that have similar morphology, and even similar context. \u2022 Expert readers may disagree on which parts of text correspond to a gene name. \u2022 Unlike companies and individuals, genes are not defined unambiguously. A gene name may refer to a specified sequence of DNA base pairs, but that sequence may vary in nonspecific ways, as a result of polymorphism, multiple alleles, translocation, and cross-species analogs. All of these things make gene name finding a unique and persistent problem. An alternative approach to finding gene names in text is to decide upon the actual gene database identifiers that are referenced in a sentence. This is the goal of the gene normalization task [2]. While success in gene normalization to some degree eliminates the need to find explicit gene mentions, it will probably never be the case that gene normalization is more easily achieved. Therefore, the need to find gene mentions will probably continue into the future. Task description BioCreative is a 'challenge evaluation' (competition or contest), in which participants are given well defined text-mining or information extraction tasks in the biological domain. Participants are given a common training corpus, and a period of time to develop systems to carry out the task. At a specified time the participants are then given a test corpus, previously unseen, and a short period of time in which to apply their systems and return the results to the organizers for evaluation. All submissions are then evaluated according to numerical criteria, specified in advance. The results are then returned to the participants and subsequently made public in a workshop and coordinated publication. The first BioCreative challenge was carried out in 2003 (with a workshop in 2004) and consisted of a gene mention task, a gene normalization task and a functional annotation task. The current BioCreative challenge took place in 2006 and the workshop in April of 2007. There were three tasks in 'BioCreative II', called the gene mention (GM), gene normalization (GN) and protein-protein interaction (PPI) tasks. The BioCreative II GM task builds on the similar task from BioCreative I [3]. The training corpus for the current task consists mainly of the training and testing corpora (text collections) from the previous task, and the testing corpus for the current task consists of an additional 5,000 sentences that were held 'in reserve' from the previous task. In the time since the previous challenge, the corpus was reviewed for consistency using a combined automated and manual process. In the previous task, participants were asked to identify gene mentions by giving a range of tokens in the pretokenized sentences of the corpus. In the current corpus, tokenization is not provided; instead participants are asked to identify a gene mention in a sentence by giving its start and end characters. As before, the training set consists of a set of sentences, and for each sentence a set of gene mentions (GENE annotations). Each 'official' GENE annotation in a sentence may optionally have alternate boundaries that are judged by human annotators to be essentially equivalent references (ALTGENE annotations). Every string identified by a run is considered either a true positive or a false positive. If the string matches a GENE or ALTGENE in the humanly annotated corpus, it is counted as a true positive with the exception that only one true positive is permitted per gene given in the corpus. If none of the annotations of a gene given in the corpus match a string nominated by a run, then the gene is counted as a false negative. A run is scored by counting the true positives (TP), false positives (FP), and false negatives (FN). Let T = TP + FN denote the total number of genes in the corpus, and let P = TP + FP denote the total number of nominated gene mentions by a run. The evaluation is based on the performance measures p (precision), r (recall), and their harmonic average F: Different applications may favor a different weighting between precision and recall, but this is beyond the scope of our analysis. We assume this simple form of F score in all of our analysis. Despite being called a 'challenge evaluation', competition, or contest, there are several reasons to view the results differently. As is pointed out repeatedly in the TREC workshop [4], the 'absolute value of effectiveness measure is not meaningp TP P", "year": 2008, "ssId": "4bff8862ba7956fdc2288e8399fb187b9595982b", "arXivId": null, "link": null, "openAccess": false, "authors": ["Lawrence H. Smith", "L. Tanabe", "R. Ando", "Cheng-Ju Kuo", "I. Chung", "Chun-Nan Hsu", "Yu-Shi Lin", "Roman Klinger", "C. Friedrich", "K. Ganchev", "Manabu Torii", "Hongfang Liu", "B. Haddow", "C. Struble", "R. Povinelli", "Andreas Vlachos", "W. Baumgartner", "L. Hunter", "B. Carpenter", "Richard Tzong-Han Tsai", "Hong-Jie Dai", "F. Liu", "Yifei Chen", "Chengjie Sun", "S. Katrenko", "P. Adriaans", "C. Blaschke", "Rafael Torres", "M. Neves", "Preslav Nakov", "A. Divoli", "M. Ma\u00f1a-L\u00f3pez", "J. Mata", "W. Wilbur"]}