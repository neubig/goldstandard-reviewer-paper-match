{"title": "When Curation Becomes Creation: Algorithms, Microcontent, and the Vanishing Distinction between Platforms and Creators", "abstract": "Ever since social activity on the Internet began migrating from the wilds of the open web to the walled gardens erected by so-called platforms (think Myspace, Facebook, Twitter, YouTube, or TikTok), debates have raged about the responsibilities that these platforms ought to bear. And yet, despite intense scrutiny from the news media and grassroots movements of outraged users, platforms continue to operate, from a legal standpoint, on the friendliest terms. You might say that today\u2019s platforms enjoy a \u201chave your cake, eat it too, and here\u2019s a side of ice cream\u201d deal. They simultaneously benefit from: (1) broad discretion to organize (and censor) content however they choose; (2) powerful algorithms for curating a practically limitless supply of user-posted microcontent according to whatever ends they wish; and (3) absolution from almost any liability associated with that content. This favorable regulatory environment results from the current legal framework, which distinguishes between intermediaries (e.g., platforms) and content providers. This distinction is ill-adapted to the modern social media landscape, where platforms deploy powerful data-driven algorithms (so-called AI) to play an increasingly active role in shaping what people see and where users supply disconnected bits of raw content (tweets, photos, etc.) as fodder. Specifically, under Section 230 of the Telecommunications Act of 1996, \u201cinteractive computer services\u201d are shielded from liability for information produced by \u201cinformation content providers.\u201d While this provision was originally intended to protect telecommunications companies and Internet service providers from liability for content that merely passed through their plumbing [2], the designation now shelters services such as Facebook, Twitter, and YouTube, which actively shape user experiences. Excepting obligations to take down specific categories of content (e.g., child pornography and copyright violations), today\u2019s platforms have license to monetize whatever content they like, moderate if and when it aligns with their corporate objectives, and curate their content however they wish.", "year": 2021, "ssId": "04a94c15fec43e7563d58be697246a0dd6c57021", "arXivId": "2107.00441", "link": "https://arxiv.org/pdf/2107.00441.pdf", "openAccess": true, "authors": ["Liu Leqi", "Dylan Hadfield-Menell", "Zachary Chase Lipton"]}