{"title": "Privacy-Preserving Graph Convolutional Networks for Text Classification", "abstract": "Graph convolutional networks (GCNs) are a 001 powerful architecture for representation learn002 ing on documents that naturally occur as 003 graphs, e.g., citation or social networks. How004 ever, sensitive personal information, such as 005 documents with people\u2019s profiles or relation006 ships as edges, are prone to privacy leaks, 007 as the trained model might reveal the orig008 inal input. Although differential privacy 009 (DP) offers a well-founded privacy-preserving 010 framework, GCNs pose theoretical and prac011 tical challenges due to their training specifics. 012 We address these challenges by adapting 013 differentially-private gradient-based training 014 to GCNs and conduct experiments using two 015 optimizers on five NLP datasets in two lan016 guages. We propose a simple yet efficient 017 method based on random graph splits that not 018 only improves the baseline privacy bounds by 019 a factor of 2.7 while retaining competitive F1 020 scores, but also provides strong privacy guar021 antees of \u03b5 = 1.0. We show that, under certain 022 modeling choices, privacy-preserving GCNs 023 perform up to 90% of their non-private vari024 ants, while formally guaranteeing strong pri025 vacy measures. 026", "year": 2021, "ssId": "80b92f762e116d4513da27792822897ca3915247", "arXivId": "2102.09604", "link": "https://arxiv.org/pdf/2102.09604.pdf", "openAccess": true, "authors": ["Timour Igamberdiev", "Ivan Habernal"]}