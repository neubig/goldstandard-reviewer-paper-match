{"title": "Gradient Conjectures for Strategic Multi-Agent Learning", "abstract": "We introduce a general framework for gradient-based learning that incorporates opponent behavior in continuous, general-sum games. In contrast to much of the work on learning in games, which primarily analyzes agents that myopically update a strategy under the belief opposing players repeat the last joint action, we study agents that model the dynamic behavior of opponents and optimize a surrogate cost. The surrogate cost functions embed conjectures, which anticipate the dynamic behavior of opponents. We show that agents with heterogeneous conjectures can result in a number of game-theoretic outcomes including Nash, Stackelberg, and general conjectural variations equilibrium. We review the suitability of each equilibrium concept for implicit and gradient conjectures and analyze the limiting outcomes of conjectural learning. Moreover, we demonstrate our framework generalizes a number of learning rules from recent years.", "year": 2019, "ssId": "f78e5aaf34cc1e4874490e9155c640b73c630021", "arXivId": null, "link": null, "openAccess": false, "authors": ["Benjamin J. Chasnov", "Tanner Fiez", "L. Ratliff"]}