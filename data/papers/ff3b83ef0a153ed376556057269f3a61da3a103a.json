{"title": "Towards Automatic Instrumentation by Learning to Separate Parts in Symbolic Multitrack Music", "abstract": "Modern keyboards allow a musician to play multiple instruments at the same time by assigning zones\u2014fixed pitch ranges of the keyboard\u2014to different instruments. In this paper, we aim to further extend this idea and examine the feasibility of automatic instrumentation\u2014dynamically assigning instruments to notes in solo music during performance. In addition to the online, real-time-capable setting for performative use cases, automatic instrumentation can also find applications in assistive composing tools in an offline setting. Due to the lack of paired data of original solo music and their full arrangements, we approach automatic instrumentation by learning to separate parts (e.g., voices, instruments and tracks) from their mixture in symbolic multitrack music, assuming that the mixture is to be played on a keyboard. We frame the task of part separation as a sequential multi-class classification problem and adopt machine learning to map sequences of notes into sequences of part labels. To examine the effectiveness of our proposed models, we conduct a comprehensive empirical evaluation over four diverse datasets of different genres and ensembles\u2014Bach chorales, string quartets, game music and pop music. Our experiments show that the proposed models outperform various baselines. We also demonstrate the potential for our proposed models to produce alternative convincing instrumentations for an existing arrangement by separating its mixture into parts. All source code and audio samples can be found at https://salu133445.github.io/arranger/.", "year": 2021, "ssId": "ff3b83ef0a153ed376556057269f3a61da3a103a", "arXivId": "2107.05916", "link": "https://arxiv.org/pdf/2107.05916.pdf", "openAccess": true, "authors": ["Hao-Wen Dong", "Chris Donahue", "Taylor Berg-Kirkpatrick", "Julian McAuley"]}