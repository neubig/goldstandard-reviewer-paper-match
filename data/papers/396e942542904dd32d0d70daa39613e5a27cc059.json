{"title": "Stacked Graphical Learning", "abstract": "Collective classification predicts class labels simultaneously for a group of related instances, rather than predicting a class for each instance separately. The existing collective classification methods are usually expensive due to the iterative inference in graphical models and their learning procedures based on iterative optimization. When the dataset is large, the cost of maintaining large graphs or related instances in memory becomes a problem as well. Stacked graphical learning has been proposed for collective classification with efficient inference. However, the memory and time cost of standard stacked graphical learning is still expensive since it requires cross-validation-like predictions to be constructed during training. In this paper, we integrate recently-developed single-pass online learning with stacked learning, to save training time and to handle large streaming datasets with minimal memory overhead. Experimentally we will show that online stacked graphical learning gives accurate results on eleven sample problems from three domains, with less time and memory cost.", "year": 2006, "ssId": "396e942542904dd32d0d70daa39613e5a27cc059", "arXivId": null, "link": null, "openAccess": false, "authors": ["Zhenzhen Kou", "Vitor R. Carvalho", "William W. Cohen"]}