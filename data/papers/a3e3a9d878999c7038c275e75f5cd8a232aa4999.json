{"title": "SUPERB-SG: Enhanced Speech processing Universal PERformance Benchmark for Semantic and Generative Capabilities", "abstract": "Transfer learning has proven to be crucial in advancing the state of speech and natural language processing research in recent years. In speech, a model pre-trained by self-supervised learning transfers remarkably well on multiple tasks. However, the lack of a consistent evaluation methodology is limiting towards a holistic understanding of the efficacy of such models. SUPERB was a step towards introducing a common benchmark to evaluate pretrained models across various speech tasks. In this paper, we introduce SUPERB-SG, a new benchmark focused on evaluating the semantic and generative capabilities of pre-trained models by increasing task diversity and difficulty over SUPERB. We use a lightweight methodology to test the robustness of representations learned by pre-trained models under shifts in data domain and quality across different types of tasks. It entails freezing pretrained model parameters, only using simple task-specific trainable heads. The goal is to be inclusive of all researchers, and encourage efficient use of computational resources. We also show that the task diversity of SUPERB-SG coupled with limited task supervision is an effective recipe for evaluating the generalizability of model representation. \u030aEqual contribution.", "year": 2022, "ssId": "a3e3a9d878999c7038c275e75f5cd8a232aa4999", "arXivId": "2203.06849", "link": "https://arxiv.org/pdf/2203.06849.pdf", "openAccess": true, "authors": ["Hsiang-Sheng Tsai", "Heng-Jui Chang", "Wen-Chin Huang", "Zili Huang", "Kushal Lakhotia", "Shu-wen Yang", "Shuyan Dong", "Andy T. Liu", "Cheng-I Lai", "Jiatong Shi", "Xuankai Chang", "Phil Hall", "Hsuan-Jui Chen", "Shang-Wen Li", "Shinji Watanabe", "Abdel-rahman Mohamed", "Hung-yi Lee"]}