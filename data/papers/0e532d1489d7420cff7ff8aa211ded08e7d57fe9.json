{"title": "Generalization Ability of Online Strongly Convex Learning Algorithms", "abstract": "Online learning, in contrast to batch learning, occurs in a sequence of rounds. At the beginning of a round, an example is presented to the learning algorithm, the learning algorithm uses its current hypothesis to label the example, and then the learning algorithm is presented with the correct label and the hypothesis is updated. It is a different learning paradigm than batch learning where we are given all of our data at once, and we aim to construct a single optimal hypothesis using the entire data set. We hope that the resulting hypothesis will generalize well to unseen data. In online learning, our goal is to minimize the total loss along the entire sequence of training examples and we generate a new hypothesis with nearly every training example. Online learning can be motivated from situations where it is not feasible or desirable to utilize a batch learning approach. Examples could be situations where there is a huge amount of data where storing and learning from all of it is computationally unfeasible, or perhaps when the distribution generating the data is changing i.e. during the sequence more than one hypothesis is generating the data. Recently, statistical learning machinery has been used to analyze this paradigm. In [1] the generalization ability of convex functions was analyzed and [2] extends this work by investigating online algorithms with strongly convex loss functions. This analysis can be motivated due to the fact that there exists a large number of optimization problems in machine learning that are strongly convex. For instance all problems that use a log-loss or square-loss loss function or those who use a convex loss function, that is not necessarily strongly convex, and use L2 regularization or another strongly convex regularizer. The latter case describes the SVM problem which uses a convex loss function (hinge loss) with L2 regularization. This paper will discuss [2] and examine the paper through the lens of our CS 598 course. It will also discuss the main application of this paper, which is bounding the convergence rate of the Pegasos algorithm [4], with high probability.", "year": 2013, "ssId": "0e532d1489d7420cff7ff8aa211ded08e7d57fe9", "arXivId": null, "link": null, "openAccess": false, "authors": ["J. Wieting"]}