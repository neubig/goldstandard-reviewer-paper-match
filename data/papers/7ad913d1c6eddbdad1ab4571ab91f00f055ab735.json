{"title": "SRU++: Pioneering Fast Recurrence with Attention for Speech Recognition", "abstract": "The Transformer architecture has been well adopted as a dominant architecture in most sequence transduction tasks including automatic speech recognition (ASR), since its attention mechanism excels in capturing long-range dependencies. While models built solely upon attention can be better parallelized than regular RNN, a novel network architecture, SRU++, was recently proposed. By combining the fast recurrence and attention mechanism, SRU++ exhibits strong capability in sequence modeling and achieves near-state-ofthe-art results in various language modeling and machine translation tasks with improved compute efficiency. In this work, we present the advantages of applying SRU++ in ASR tasks by comparing with Conformer across multiple ASR benchmarks and study how the benefits can be generalized to long-form speech inputs. On the popular LibriSpeech benchmark, our SRU++ model achieves 2.0% / 4.7% WER on test-clean / test-other, showing competitive performances compared with the state-of-the-art Conformer encoder under the same set-up. Specifically, SRU++ can surpass Conformer on long-form speech input with a large margin, based on our analysis.", "year": 2021, "ssId": "7ad913d1c6eddbdad1ab4571ab91f00f055ab735", "arXivId": "2110.05571", "link": "https://arxiv.org/pdf/2110.05571.pdf", "openAccess": true, "authors": ["Jing Pan", "Tao Lei", "Kwangyoun Kim", "Kyu J. Han", "Shinji Watanabe"]}