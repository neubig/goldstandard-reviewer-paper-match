{"title": "Sequence Transduction with Graph-based Supervision", "abstract": "The recurrent neural network transducer (RNN-T) objective plays a major role in building today\u2019s best automatic speech recognition (ASR) systems for production. Similarly to the connectionist temporal classi\ufb01cation (CTC) objective, the RNN-T loss uses speci\ufb01c rules that de\ufb01ne how a set of alignments is generated to form a lattice for the full-sum training. However, it is yet largely unknown if these rules are optimal and do lead to the best possible ASR results. In this work, we present a new transducer objective function that generalizes the RNN-T loss to accept a graph representation of the labels, thus providing a \ufb02exible and ef\ufb01cient framework to manipulate training lattices, e.g., for studying different transition rules, implementing different transducer losses, or restricting alignments. We demonstrate that transducer-based ASR with CTC-like lattice achieves better results compared to standard RNN-T, while also ensuring a strictly monotonic alignment, which will allow better optimization of the decoding procedure. For example, the proposed CTC-like transducer achieves an improvement of 4.8% on the test-other condition of LibriSpeech relative to an equivalent RNN-T based system.", "year": 2021, "ssId": "6f69fcacdf53a811ef18c5e9ac8ec58035dc43fc", "arXivId": "2111.01272", "link": "https://arxiv.org/pdf/2111.01272.pdf", "openAccess": true, "authors": ["Niko Moritz", "Takaaki Hori", "Shinji Watanabe", "Jonathan Le Roux"]}