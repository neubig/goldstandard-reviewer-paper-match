{"title": "When differential privacy meets NLP: The devil is in the detail", "abstract": "Differential privacy provides a formal approach to privacy of individuals. Applications of differential privacy in various scenarios, such as protecting users\u2019 original utterances, must satisfy certain mathematical properties. Our contribution is a formal analysis of ADePT, a differentially private auto-encoder for text rewriting (Krishna et al, 2021). ADePT achieves promising results on downstream tasks while providing tight privacy guarantees. Our proof reveals that ADePT is not differentially private, thus rendering the experimental results unsubstantiated. We also quantify the impact of the error in its private mechanism, showing that the true sensitivity is higher by at least factor 6 in an optimistic case of a very small encoder\u2019s dimension and that the amount of utterances that are not privatized could easily reach 100% of the entire dataset. Our intention is neither to criticize the authors, nor the peer-reviewing process, but rather point out that if differential privacy applications in NLP rely on formal guarantees, these should be outlined in full and put under detailed scrutiny.", "year": 2021, "ssId": "2fb44f1317bc51a1e011a5a44d817ad9104e29e8", "arXivId": "2109.03175", "link": "https://arxiv.org/pdf/2109.03175.pdf", "openAccess": true, "authors": ["Ivan Habernal"]}