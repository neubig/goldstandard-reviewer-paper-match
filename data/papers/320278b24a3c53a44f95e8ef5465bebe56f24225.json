{"title": "A Joint Model for Pause Prediction and Dependency Parsing using Latent Variables The", "abstract": "Prediction of prosodic information from text is a basic technology used in a number of speech-related applications. In particular, pauses prediction is used in speech synthesis to allow for more natural prosodic boundaries [1]. While early studies in pause prediction only relied on lexical information such as POS tags or punctuation [1, 2], recent works have shown that using syntactic structure helps achieve better accuracy [3]. This work focuses on the use of dependency structure for pause prediction. Our method is inspired by recent work by Honnibal and Johnson [4] on the related, but quite different task, of disfluency detection. In this work, they propose a model that jointly performs dependency parsing and disfluency detection, and demonstrate that a joint model that considers these two tasks improves over solving these two tasks individually. Thus, in this paper, we propose a method for joint dependency parsing and pause prediction. One of the most widely used methods for dependency parsing is the transition-based method based on the shift-reduce algorithm [5]. As shown in the black text of Figure 1, and explained in detail in Section 2, the shift-reduce method builds a dependency tree expressing the syntactic structure of the sentence by performing a series of \u201cshift\u201d and \u201creduce\u201d actions, and if the correct action sequence is chosen, the correct dependency tree will be created. A classifier to choose the correct answer is trained from syntactically annotated data. In our proposed model, we further expand the action set of the shift-reduce algorithm by adding actions that predict pauses, as shown in the bold, red text in Figure 1, and is described in detail in Section 3. This presents more difficulties in training, however, as it is necessary to have data that is annotated with both pauses and syntactic information, which cannot be obtained in large quantities. To solve this problem, we treat the pauses as latent variables, allowing our parsing model to be trained on data fully annotated with syntax and pauses as well as data only annotated with syntactic trees. In the experiments described in Section 4, we find that the proposed model exceeds all baselines, and that the proposed latent variable allows for effective use of data not explicitly annotated with pauses, resulting in an 11.6% absolute improvement in pause F -measure.", "year": 2016, "ssId": "320278b24a3c53a44f95e8ef5465bebe56f24225", "arXivId": null, "link": null, "openAccess": false, "authors": ["T. Nguyen", "Graham Neubig", "Hiroyuki Shindo", "S. Sakti", "T. Toda", "Satoshi Nakamura"]}