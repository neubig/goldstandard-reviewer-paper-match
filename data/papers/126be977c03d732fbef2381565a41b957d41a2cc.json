{"title": "Discourse-Level Language Understanding with Deep Learning", "abstract": "Title of thesis: Discourse-Level Language Understanding with Deep Learning Mohit Iyyer, Doctor of Philosophy, 2017 Thesis directed by: Professor Jordan Boyd-Graber Computer Science, University of Maryland Professor Hal Daum\u00e9 III Computer Science, University of Maryland Designing computational models that can understand language at a human level is a foundational goal in the field of natural language processing (NLP). Given a sentence, machines are capable of translating it into many different languages, generating a corresponding syntactic parse tree, marking words that refer to people or places, and much more. These tasks are solved by statistical machine learning algorithms, which leverage patterns in large datasets to build predictive models. Many recent advances in NLP are due to deep learning models (parameterized as neural networks), which bypass user-specified features in favor of building representations of language directly from the text. Despite many deep learning-fueled advances at the word and sentence level, however, computers still struggle to understand high-level discourse structure in language, or the way in which authors combine and order different units of text (e.g., sentences, paragraphs, chapters) to express a coherent message or narrative. Part of the reason is data-related, as there are few existing datasets for contextual language-based problems, and some tasks are too complex to be framed as supervised learning problems; for the latter type, we must either resort to unsupervised learning or devise training objectives that simulate the supervised setting. Another reason is architectural: neural networks designed for sentencelevel tasks require additional functionality, interpretability, and efficiency to operate at the discourse level. In this thesis, I design deep learning architectures for three NLP tasks that require integrating information across high-level linguistic context: question answering, fictional relationship understanding, and comic book narrative modeling. While these tasks are very different from each other on the surface, I show that similar neural network modules can be used in each case to form contextual representations. I conclude by discussing potential avenues for future research that seeks to understand increasingly large and complex context. DISCOURSE-LEVEL LANGUAGE UNDERSTANDING WITH DEEP LEARNING", "year": 2017, "ssId": "126be977c03d732fbef2381565a41b957d41a2cc", "arXivId": null, "link": null, "openAccess": false, "authors": ["Mohit Iyyer"]}