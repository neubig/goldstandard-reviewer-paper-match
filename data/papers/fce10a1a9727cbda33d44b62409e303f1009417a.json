{"title": "What Do Recurrent Neural Network Grammars Learn About Syntax?", "abstract": "Recurrent neural network grammars (RNNG) are a recently proposed probablistic generative modeling family for natural language. They show state-of-the-art language modeling and parsing performance. We investigate what information they learn, from a linguistic perspective, through various ablations to the model and the data, and by augmenting the model with an attention mechanism (GA-RNNG) to enable closer inspection. We find that explicit modeling of composition is crucial for achieving the best performance. Through the attention mechanism, we find that headedness plays a central role in phrasal representation (with the model\u2019s latent attention largely agreeing with predictions made by hand-crafted head rules, albeit with some important differences). By training grammars without nonterminal labels, we find that phrasal representations depend minimally on nonterminals, providing support for the endocentricity hypothesis.", "year": 2016, "ssId": "fce10a1a9727cbda33d44b62409e303f1009417a", "arXivId": "1611.05774", "link": "https://arxiv.org/pdf/1611.05774.pdf", "openAccess": true, "authors": ["Noah A. Smith", "Chris Dyer", "Miguel Ballesteros", "Graham Neubig", "Lingpeng Kong", "A. Kuncoro"]}