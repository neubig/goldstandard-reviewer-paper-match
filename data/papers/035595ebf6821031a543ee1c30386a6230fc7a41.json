{"title": "Online End-To-End Neural Diarization with Speaker-Tracing Buffer", "abstract": "This paper proposes a novel online speaker diarization algorithm based on a fully supervised self-attention mechanism (SA-EEND). Online diarization inherently presents a speaker\u2019s permutation problem due to the possibility to assign speaker regions incorrectly across the recording. To circumvent this inconsistency, we proposed a speaker-tracing buffer mechanism that selects several input frames representing the speaker permutation information from previous chunks and stores them in a buffer. These buffered frames are stacked with the input frames in the current chunk and fed into a self-attention network. Our method ensures consistent diarization outputs across the buffer and the current chunk by checking the correlation between their corresponding outputs. Additionally, we trained SA-EEND with variable chunk-sizes to mitigate the mismatch between training and inference introduced by the speaker-tracing buffer mechanism. Experimental results, including online SA-EEND and variable chunk-size, achieved DERs of 12.54 % for CALLHOME and 20.77 % for CSJ with 1.4 s actual latency.", "year": 2020, "ssId": "035595ebf6821031a543ee1c30386a6230fc7a41", "arXivId": "2006.02616", "link": "https://arxiv.org/pdf/2006.02616.pdf", "openAccess": true, "authors": ["Yawen Xue", "Shota Horiguchi", "Yusuke Fujita", "Shinji Watanabe", "Kenji Nagamatsu"]}