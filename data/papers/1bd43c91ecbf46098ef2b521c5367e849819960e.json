{"title": "Dynamic Data Selection and Weighting for Iterative Back-Translation", "abstract": "Back-translation has proven to be an effective method to utilize monolingual data in neural machine translation (NMT), and iteratively conducting back-translation can further improve the model performance. Selecting which monolingual data to back-translate is crucial, as we require that the resulting synthetic data are of high quality \\textit{and} reflect the target domain. To achieve these two goals, data selection and weighting strategies have been proposed, with a common practice being to select samples close to the target domain but also dissimilar to the average general-domain text. In this paper, we provide insights into this commonly used approach and generalize it to a dynamic curriculum learning strategy, which is applied to iterative back-translation models. In addition, we propose weighting strategies based on both the current quality of the sentence and its improvement over the previous iteration. We evaluate our models on domain adaptation, low-resource, and high-resource MT settings and on two language pairs. Experimental results demonstrate that our methods achieve improvements of up to 1.8 BLEU points over competitive baselines.", "year": 2020, "ssId": "1bd43c91ecbf46098ef2b521c5367e849819960e", "arXivId": "2004.03672", "link": "https://arxiv.org/pdf/2004.03672.pdf", "openAccess": true, "authors": ["Zi-Yi Dou", "Antonios Anastasopoulos", "Graham Neubig"]}