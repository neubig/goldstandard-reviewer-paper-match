{"title": "Improving Neural Model Performance through Natural Language Feedback on Their Explanations", "abstract": "A class of explainable NLP models for reasoning tasks support their decisions by generating free-form or structured explanations, but what happens when these supporting structures contain errors? Our goal is to allow users to interactively correct explanation structures through natural language feedback. We introduce MERCURIEan interactive system that refines its explanations for a given reasoning task by getting human feedback in natural language. Our approach generates graphs that have 40% fewer inconsistencies as compared with the off-the-shelf system. Further, simply appending the corrected explanation structures to the output leads to a gain of 1.2 points on accuracy on defeasible reasoning across all three domains.1", "year": 2021, "ssId": "f5ca46585818771e64ee9449c930748fbee35cba", "arXivId": "2104.08765", "link": "https://arxiv.org/pdf/2104.08765.pdf", "openAccess": true, "authors": ["Aman Madaan", "Niket Tandon", "Dheeraj Rajagopal", "Yiming Yang", "Peter Clark", "Keisuke Sakaguchi", "E. Hovy"]}