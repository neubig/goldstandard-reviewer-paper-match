{"title": "Speech Representation Learning Combining Conformer CPC with Deep Cluster for the ZeroSpeech Challenge 2021", "abstract": "We present a system for the Zero Resource Speech Challenge 2021, which combines a Contrastive Predictive Coding (CPC) with deep cluster. In deep cluster, we first prepare pseudo-labels obtained by clustering the outputs of a CPC network with kmeans. Then, we train an additional autoregressive model to classify the previously obtained pseudo-labels in a supervised manner. Phoneme discriminative representation is achieved by executing the second-round clustering with the outputs of the final layer of the autoregressive model. We show that replacing a Transformer layer with a Conformer layer leads to a further gain in a lexical metric. Experimental results show that a relative improvement of 35% in a phonetic metric, 1.5% in the lexical metric, and 2.3% in a syntactic metric are achieved compared to a baseline method of CPC-small which is trained on LibriSpeech 460h data. We achieve top results in this challenge with the syntactic metric.", "year": 2021, "ssId": "63d99a61e798d7cb714f336a8d581ae2b75672ee", "arXivId": "2107.05899", "link": "https://arxiv.org/pdf/2107.05899.pdf", "openAccess": true, "authors": ["Takashi Maekaku", "Xuankai Chang", "Yuya Fujita", "Li-Wei Chen", "Shinji Watanabe", "Alexander I. Rudnicky"]}