{"title": "Semi-supervised learning for biomedical information extraction", "abstract": "s Seen 434 76.32 95.4 84.8 Unseen 195 34.54 73.63 47.02 Overall 629 63.43 90.89 74.72 Full papers Seen 801 74.78 94.48 83.48 Unseen 1,179 53.6 86.58 66.21 Overall 1,980 62.17 90.25 73.62 Table 2.6: Evaluation of the CRF+syntax system trained on the automatically annotated abstracts and evaluated on the abstracts and the full papers dataset. found in the dictionary from FlyBase were not found in the abstracts tagged to generate the training data. The CRF+syntax system uses a complex but more general representation of the context based on the features extracted from the output of the syntactic parser, namely the lemmas, the part-of-speech tags and the grammatical relationships, while the HMM-based system uses a simple morphological rule-based classifier. Also, the former system takes the two previous labels into account, while the latter only the previous one. Therefore, it is expected that the CRF+syntax system has superior performance on unseen genes. This difference between the two systems is more pronounced when evaluating on full papers (65.03% versus 46.66% respectively) than on abstracts (47.02% versus 44.98% respectively). This can be attributed to the fact that the training data used is generated from abstracts, and when evaluating on full papers the change in genre can be handled more effectively by the CRF+RASP system due to its more complex feature set. On the other hand, the simpler HMM-based one is likely to perform better on seen genes, whose effective recognition does not rely on complex features. In order to assess the contribution of the syntactic features, we trained the CRF+syntax system excluding the features extracted from the GR output of RASP, i.e. the last four features in Table 2.4. The resulting performance of the system was 60.25%/91.84%/72.77% (Precision/Recall/F-score) on full papers, which was lower than the one achieved with these features included (73.62% F-score). As expected, this is mainly due to the lower performance on unseen gene names (62.22% vs 66.21%). Apart from being affected by noise in parsing which reduces their effect, the syntactic features are likely to be useful relatively rarely, since in many cases simpler features suffice. For example, in the sample sentence of Figure 2.1 the link between the gene name \u201cUAS-D-mib\u201d and the rather strongly indicative token \u201ctransgene\u201d is also obtained by adding the latter as the token following the former. However in the sentence \u201cEndogenous CED-4 is normally localized. . . \u201d, the feature that \u201cCED-4\u201d is the subject of the verb \u201clocalize\u201d in passive voice is crucial in recognizing the former as a gene name. 30 2.7. USING ANNOTATION FROM THE USERS On both abstracts and full papers, the performance of the HMM-based system is superior on seen genes while the CRF+syntax system performs better on unseen genes. Therefore, it was natural to attempt to combine the strengths of these two systems. In particular, since the HMM-based system is performing very well on seen gene names, for each sentence we check whether it has recognized any gene names unseen in the training data (potential unseen precision errors) or if it considered as ordinary English words any tokens not seen as such in the training data (potential unseen recall errors). If either of these is true, then we pass the sentence to the CRF+syntax system, which has better performance on unseen gene names. Such a strategy is expected to trade some of the performance of the seen gene names of the HMM-based system for improved performance on the unseen gene names by using the predictions of the CRF+syntax system. This occurs because seen and unseen gene names may appear in the same sentence and choosing the predictions of the latter system could result in more errors on the seen gene names. This strategy is likely to improve the performance on datasets where there are more unseen gene names, since the CRF+syntax system is substantially better than the HMM-based one on them. Using this strategy the overall F-score increased to 75.26% on the full paper corpus which contains a lot of unseen gene names (57% of the total gene names). Out of the 1,220 sentences of the corpus, 759 were passed on to the CRF+syntax system for tagging. For the manually annotated abstracts, 185 out of 600 sentences were passed to the CRF+syntax system and the overall performance was 80.21%, which is lower than the one achieved by the HMM-based system alone (81.86%). This is expected since the majority of gene names (69%) are seen in the training data and the performance of the CRF+syntax system on the unseen data is better than the HMM-based one only by a small margin (47.02 vs 44.98 in F-score respectively). Overall, the performances on abstracts were better than on full papers, which can be attributed to the fact that the training data used consists of abstracts only. Annotating full papers automatically, as performed in Section 2.2 for abstracts, is unlikely to provide us with training data of adequate quality given the more complex and variable language used in them. 2.7 Using annotation from the users In this section we attempt to improve the performance of the BioNER systems described in the previous sections by incorporating user feedback. While the performances already achieved are reasonable, we want to explore how they could be improved further, especially on full papers, in which the performance was lower than on abstracts. In the context of the FlySlip project the HMM-based BioNER system described in Section 2.2 was deployed http://www.wiki.cl.cam.ac.uk/rowiki/NaturalLanguage/FlySlip CHAPTER 2. BIOMEDICAL NAMED ENTITY RECOGNITION 31 Figure 2.2: Screenshot of the interface used to collect the feedback from the users. as part of a pipeline which was used to preprocess full papers. They were then curated by FlyBase staff using a specialized curation interface which was developed under a usercentered approach and was shown to improve curation performance (Karamanis et al., 2007, 2008). This process presented us with an opportunity to obtain feedback from the users. For each article, the curators were asked to give feedback on the annotation performed by the HMM-based BioNER system. A screenshot of the interface appears in Figure 2.2. The tokens to be corrected were chosen according to the uncertainty of the system, which was estimated as the conditional entropy of the decision on the current token given the decision on the previous one. These tokens were chosen independently and no attempt was made to identify sequences of tokens to be corrected by the users. Each chosen token was highlighted and the curators had to confirm whether it was a gene name (by clicking on the \u201cMark as Gene\u201d button) or not. They were not requested to distinguish between tokens that were gene names on their own and tokens being part of multi-token gene names. If needed, they could also modify the boundaries of the gene name by selecting the appropriate textual string. 32 2.7. USING ANNOTATION FROM THE USERS CRF+syntax HMM R P F R P F seen 79.52% 91.2% 84.96% 92.54% 87.57% 89.99% unseen 55.45% 92.53% 69.34% 37.94% 63.52% 47.5% overall 67.02% 91.96% 77.53% 64.19% 78.46% 70.61% Table 2.7: Evaluation of the performance using user feedback on full papers In the experiments described below, we collected data from 28 curated papers (other than those included in the full paper gold standard). The curators provided feedback on 677 tokens, resulting in 549 sentences containing 1,668 gene names and 19,449 tokens in total. This feedback was requested once the curators had finished the curation of the paper. The data collected contained noise, since the curators were asked to make decisions on particular tokens, without correcting any other errors found in the sentences containing these tokens. As a consequence, errors that were not related to the token in question were not corrected. We add these sentences to the automatically annotated training data described in Section 2.2. The purpose is to provide the systems with training data from the exact domain they would be applied to, which is the full papers rather than their abstracts. We re-trained the HMM-based and the CRF+syntax BioNER systems and we evaluated their performance on the full paper corpus described in Section 2.5. As the results in Table 2.7 demonstrate, the improvements achieved by adding the sentences from the full papers are substantial. The performance gains for the CRF+syntax and the HMMbased systems were 3.91% and 2.47% in F-score respectively. This is due not only to the increased coverage of gene names (48.1% were now seen in the training data, compared to 40.5%), but also to the fact that contexts and features useful in recognizing gene names in full papers that had not been encountered in the abstracts were now used. For example, a source of errors for the CRF+syntax system was that identifiers of image panels in parentheses were commonly mistaken for gene names, especially when they followed a gene name, since in abstracts, the pattern \u201cgene name (token)\u201d is a very strong indication that the token in parentheses is in fact a gene name as well. By adding data from the full papers to the training material, such errors were avoided. As in Section 2.6.4, we observe again that the performance of the HMM-based NER system is better on seen gene names than that of the CRF+syntax system. Therefore we combine them, keeping the tagging provided by the former for sentences containing only seen gene names and the tagging of the latter for the rest. The performance achieved was 68.64% / 89.94% / 77.86% (Recall / Precision / F-score), or 0.33% better in F-score than the (re-trained) CRF+syntax system on its own. The improvement obtained by combining the systems is smaller than it was in the experiments of Section 2.6 when the feedback CHAPTER 2. BIOMEDICAL NAMED ENTITY RECOGNITION 33 from the users was not used (1.64%). This is due to the fact that the CRF+syntax system took ", "year": 2010, "ssId": "e5a5888966be6b5f9c0e8a82facd604086a1ee4c", "arXivId": null, "link": null, "openAccess": false, "authors": ["Andreas Vlachos"]}