{"title": "Generalizing and Hybridizing Count-based and Neural Language Models", "abstract": "Language models (LMs) are statistical models that calculate probabilities over sequences of words or other discrete symbols. Currently two major paradigms for language modeling exist: count-based n-gram models, which have advantages of scalability and test-time speed, and neural LMs, which often achieve superior modeling performance. We demonstrate how both varieties of models can be unified in a single modeling framework that defines a set of probability distributions over the vocabulary of words, and then dynamically calculates mixture weights over these distributions. This formulation allows us to create novel hybrid models that combine the desirable features of count-based and neural LMs, and experiments demonstrate the advantages of these approaches.", "year": 2016, "ssId": "95cedaeb3178a4671703a05171a144e6b964a819", "arXivId": "1606.00499", "link": "https://arxiv.org/pdf/1606.00499.pdf", "openAccess": true, "authors": ["Graham Neubig", "Chris Dyer"]}