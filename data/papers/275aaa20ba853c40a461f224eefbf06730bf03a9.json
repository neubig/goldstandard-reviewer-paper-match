{"title": "Escape saddle points by a simple gradient-descent based algorithm", "abstract": "Escaping saddle points is a central research topic in nonconvex optimization. In this paper, we propose a simple gradient-based algorithm such that for a smooth function f : Rn ! R, it outputs an \u270f-approximate second-order stationary point in \u00d5(log n/\u270f1.75) iterations. Compared to the previous state-of-the-art algorithms by Jin et al. with \u00d5(log n/\u270f2) or \u00d5(log n/\u270f1.75) iterations, our algorithm is polynomially better in terms of log n and matches their complexities in terms of 1/\u270f. For the stochastic setting, our algorithm outputs an \u270f-approximate second-order stationary point in \u00d5(log n/\u270f4) iterations. Technically, our main contribution is an idea of implementing a robust Hessian power method using only gradients, which can find negative curvature near saddle points and achieve the polynomial speedup in log n compared to the perturbed gradient descent methods. Finally, we also perform numerical experiments that support our results.", "year": 2021, "ssId": "275aaa20ba853c40a461f224eefbf06730bf03a9", "arXivId": "2111.14069", "link": "https://arxiv.org/pdf/2111.14069.pdf", "openAccess": true, "authors": ["Chenyi Zhang", "Tongyang Li"]}