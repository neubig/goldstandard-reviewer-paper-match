{"title": "Margin-space integration of MPE loss via differencing of MMI functionals for generalized error-weighted discriminative training", "abstract": "Abstract Using the central observation that margin-based weightedclassi\ufb01cation error (modeled using Minimum Phone Error(MPE)) corresponds to the derivative with respect to the mar-gin term of margin-based hinge loss (modeled using MaximumMutual Information (MMI)), this article subsumes and extendsmargin-based MPE and MMI within a broader framework inwhich the objective function is an integral of MPE loss over arange of margin values. Applying the Fundamental Theorem ofCalculus,thisintegraliseasilyevaluatedusing\ufb01nitedifferencesof MMI functionals; lattice-based training using the new crite-rion can then be carried out using differences of MMI gradi-ents. Experimental results comparing the new framework withmargin-based MMI, MCE and MPE on the Corpus of Sponta-neous Japanese and the MIT OpenCourseWare/MIT-World cor-pus are presented. 1. Introduction The \ufb01eld of discriminative training for speech recognition haswitnessed considerable activity in recent years. The appeal ofminimizingphoneorworderrorratherthanstringerrorhasmo-tivated a transition from well-known string-level methods suchas MMI and MCE [1][2] to error-weighted approaches, such asMPE [3][4]. More recently, there has been a surge in proposalsfor\u201clargemargin\u201dapproachestohiddenMarkovmodel(HMM)design, such as the \u201clarge-margin HMM\u201d [5], \u201csoft margin es-timation\u201d [6], and incrementally shifted MCE loss [7]. Sha andSaul [8] made the important proposal that a \ufb01ne-grained er-ror measure, such as the Hamming distance between candidaterecognition strings, be itself directly incorporated into the mar-gin term for HMM-based learning. It turns out that introducinga margin term that multiplies \ufb01ne-grained error can easily bebrought to MMI, MCE and MPE based HMM training as well,simply by adding margin-scaled local frame/phone/word errorto lattice arc log-likelihoods during Forward-Backward com-putation [9][10][11]. This approach links the original use ofmargin in the context of machine learning (e.g. Support VectorMachines (SVMs)) with margin in the context of \u201ctried-and-tested\u201d frameworks for large-scale discriminative training withwell-understood methods for HMM optimization on large-scaleASR tasks. Bene\ufb01ts to performance for large-scale tasks havebeen reported for the use of margin in MMI and MPE, thoughit appears the relative gains are larger for MMI than for MPE[10][11].Aiming at leveraging the bene\ufb01ts of margin use within thecontextofMPE-styleerror-weightedHMMtraining,thisarticlepresents a uni\ufb01cation of margin-based MMI and MPE trainingbased on a novel concept:", "year": 2009, "ssId": "38bd034e6a0589bf1132d3e8c79818b271377290", "arXivId": null, "link": null, "openAccess": false, "authors": ["E. McDermott", "Shinji Watanabe", "A. Nakamura"]}