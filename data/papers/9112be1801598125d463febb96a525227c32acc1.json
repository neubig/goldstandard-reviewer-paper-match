{"title": "Differentiable Weighted Finite-State Transducers", "abstract": "We introduce a framework for automatic differentiation with weighted finite-state transducers (WFSTs) allowing them to be used dynamically at training time. Through the separation of graphs from operations on graphs, this framework enables the exploration of new structured loss functions which in turn eases the encoding of prior knowledge into learning algorithms. We show how the framework can combine pruning and back-off in transition models with various sequence-level loss functions. We also show how to learn over the latent decomposition of phrases into word pieces. Finally, to demonstrate that WFSTs can be used in the interior of a deep neural network, we propose a convolutional WFST layer which maps lower-level representations to higher-level representations and can be used as a drop-in replacement for a traditional convolution. We validate these algorithms with experiments in handwriting recognition and speech recognition.", "year": 2020, "ssId": "9112be1801598125d463febb96a525227c32acc1", "arXivId": "2010.01003", "link": "https://arxiv.org/pdf/2010.01003.pdf", "openAccess": true, "authors": ["Awni Y. Hannun", "Vineel Pratap", "Jacob Kahn", "Wei-Ning Hsu"]}