{"title": "Speech Processing for Digital Home Assistants: Combining signal processing with deep-learning techniques", "abstract": "Once a popular theme of futuristic science fiction or far-fetched technology forecasts, digital home assistants with a spoken language interface have become a ubiquitous commodity today. This success has been made possible by major advancements in signal processing and machine learning for so-called far-field speech recognition, where the commands are spoken at a distance from the sound-capturing device. The challenges encountered are quite unique and different from many other use cases of automatic speech recognition (ASR). The purpose of this article is to describe, in a way that is amenable to the nonspecialist, the key speech processing algorithms that enable reliable, fully hands-free speech interaction with digital home assistants. These technologies include multichannel acoustic echo cancellation (MAEC), microphone array processing and dereverberation techniques for signal enhancement, reliable wake-up word and end-of-interaction detection, and high-quality speech synthesis as well as sophisticated statistical models for speech and language, learned from large amounts of heterogeneous training data. In all of these fields, deep learning (DL) has played a critical role.", "year": 2019, "ssId": "e6602786132e040e02df93f729f737f65a116677", "arXivId": null, "link": null, "openAccess": false, "authors": ["R. Haeb-Umbach", "Shinji Watanabe", "T. Nakatani", "M. Bacchiani", "Bj\u00f6rn Hoffmeister", "M. Seltzer", "H. Zen", "M. Souden"]}