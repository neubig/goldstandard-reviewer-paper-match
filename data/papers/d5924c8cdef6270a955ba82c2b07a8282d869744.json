{"title": "No Gestures Left Behind: Learning Relationships between Spoken Language and Freeform Gestures", "abstract": "We study relationships between spoken language and co-speech gestures in context of two key challenges. First, distributions of text and gestures are inherently skewed making it important to model the long tail. Second, gesture predictions are made at a subword level, making it important to learn relationships between language and acoustic cues. We introduce AISLe, which combines adversarial learning with importance sampling to strike a balance between precision and coverage. We propose the use of a multimodal multiscale attention block to perform subword alignment without the need of explicit alignment between language and acoustic cues. Finally, to empirically study the importance of language in this task, we extend the dataset proposed in Ahuja et al. (2020) with automatically extracted transcripts for audio signals. We substantiate the effectiveness of our approach through large-scale quantitative and user studies, which show that our proposed methodology significantly outperforms previous state-of-the-art approaches for gesture generation. Link to code, data and videos: https://github.com/chahuja/aisle", "year": 2020, "ssId": "d5924c8cdef6270a955ba82c2b07a8282d869744", "arXivId": null, "link": null, "openAccess": false, "authors": ["Chaitanya Ahuja", "Dong Won Lee", "Ryo Ishii", "Louis-Philippe Morency"]}