{"title": "An Interface for Annotating Science Questions", "abstract": "Recent work introduces the AI2 Reasoning Challenge (ARC) and the associated ARC dataset that partitions open domain, complex science questions into an Easy Set and a Challenge Set. That work includes an analysis of 100 questions with respect to the types of knowledge and reasoning required to answer them. However, it does not include clear definitions of these types, nor does it offer information about the quality of the labels or the annotation process used. In this paper, we introduce a novel interface for human annotation of science question-answer pairs with their respective knowledge and reasoning types, in order that the classification of new questions may be improved. We build on the classification schema proposed by prior work on the ARC dataset, and evaluate the effectiveness of our interface with a preliminary study involving 10 participants.", "year": 2018, "ssId": "192fc995631e3443bb7f291a971089bd06e61017", "arXivId": null, "link": null, "openAccess": false, "authors": ["Michael Boratko", "Harshit Padigela", "Divyendra Mikkilineni", "Pritish Yuvraj", "R. Das", "A. McCallum", "Maria Chang", "Achille Fokoue", "Pavan Kapanipathi", "Nicholas Mattei", "Ryan Musa", "Kartik Talamadupula", "M. Witbrock"]}