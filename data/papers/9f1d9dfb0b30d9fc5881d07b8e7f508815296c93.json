{"title": "Self-Training Tree Substitution Grammars for Domain Adaptation", "abstract": "Parsing is the process of inferring the syntactic structure of a sentence, based on a model of syntax that specifies which sentences are possible or likely. The field of statistical parsing concerns itself with learning probabilistic syntactic models from corpora. Ideally, it should be possible to parse any grammatical sentence of any natural language. Because different languages have wildly different syntactic structures (not to mention lexical items), this will require multiple parsers, one for each language that we might want to parse. For this reason, it is important to develop syntactic models and learning algorithms which generalize well to new languages. Even within a language, there is considerable variation between domains; for example, children\u2019s fiction will likely exhibit different syntactic and lexical properties than articles from the Wall Street Journal. One approach to dealing with these differences between domains would be to train a new parser for each new domain encountered. However, statistical parsers are typically trained in a supervised fashion on hand-annotated treebank data such as the Penn Treebank. It is expensive and time-consuming to produce this hand-annotated data, making it impractical to build a new treebank for every new domain. Thus, it is important to develop other methods for training parsers that perform well across domains. One way of ensuring that a parser will be able to handle new domains is to make that parser less domain-specific to begin with. For instance, if a parser trained on the Wall Street Journal performs equally well on children\u2019s fiction, then there is no need to learn a separate parser for children\u2019s fiction. One way of building a less domain-specific parser would be to train the parsing model on data from multiple domains; then, the parser would be more likely to identify the features that are constant across domains, instead of overfitting to a single genre of text. It is also important to design parsing models and learning techniques which are less likely to overfit in general. To summarize, a good parsing model should be generalizable; that is, it should be applicable to multiple languages, as well as multiple domains within each language. We identify two different senses in which a parser can be generalizable. The first sense is that the model should be able to learn equally well when trained on different datasets. That is, the model should not contain features that are specially tailored for a particular dataset or language. We refer to this sense as model generalizability. The second sense is that the model, after being trained on data from one domain, should be able to parse other datasets as well. This is important because it is impractical to train a new parser every time we wish to parse a new genre of text. Of course, any model will perform best on data that is similar to that which it was trained on; however, we can seek to improve a parser\u2019s generalizability in this sense by ensuring that we do not overfit on a single domain during training. We refer to this sense as parser generalizability.", "year": 2012, "ssId": "9f1d9dfb0b30d9fc5881d07b8e7f508815296c93", "arXivId": null, "link": null, "openAccess": false, "authors": ["Darcey Riley"]}