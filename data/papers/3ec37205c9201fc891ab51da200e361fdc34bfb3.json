{"title": "A Comparative Study of Word Embeddings for Reading Comprehension", "abstract": "The focus of past machine learning research for Reading Comprehension tasks has been primarily on the design of novel deep learning architectures. Here we show that seemingly minor choices made on (1) the use of pre-trained word embeddings, and (2) the representation of out-of-vocabulary tokens at test time, can turn out to have a larger impact than architectural choices on the final performance. We systematically explore several options for these choices, and provide recommendations to researchers working in this area.", "year": 2017, "ssId": "3ec37205c9201fc891ab51da200e361fdc34bfb3", "arXivId": "1703.00993", "link": "https://arxiv.org/pdf/1703.00993.pdf", "openAccess": true, "authors": ["Bhuwan Dhingra", "Hanxiao Liu", "R. Salakhutdinov", "William W. Cohen"]}