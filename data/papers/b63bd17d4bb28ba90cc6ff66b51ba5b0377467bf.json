{"title": "Minimum word error training of long short-term memory recurrent neural network language models for speech recognition", "abstract": "This paper describes minimum word error (MWE) training of recurrent neural network language models (RNNLMs) for speech recognition. RNNLMs are usually trained to minimize a cross entropy of estimated word probabilities against the correct word sequence, which corresponds to maximum likelihood criterion. However, this training does not necessarily maximize a performance measure in a target task, i.e. it does not minimize word error rate (WER) explicitly in speech recognition. To solve such a problem, several discriminative training methods have already been proposed for n-gram language models, but those for RNNLMs have not sufficiently investigated. In this paper, we propose a MWE training method for RNNLMs, and report significant WER reductions when we applied the MWE method to a standard Elman-type RNNLM and a more advanced model, a Long Short-Term Memory (LSTM) RNNLM. We also present efficient MWE training with N-best lists on Graphics Processing Units (GPUs).", "year": 2016, "ssId": "b63bd17d4bb28ba90cc6ff66b51ba5b0377467bf", "arXivId": null, "link": null, "openAccess": false, "authors": ["Takaaki Hori", "Chiori Hori", "Shinji Watanabe", "J. Hershey"]}