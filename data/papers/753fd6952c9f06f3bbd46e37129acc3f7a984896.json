{"title": "Hindsight: Posterior-guided training of retrievers for improved open-ended generation", "abstract": "Many text generation systems benefit from using a retriever to retrieve passages from a textual knowledge corpus (e.g., Wikipedia) and providing these passages as additional context to the generator. For open-ended generation tasks (like generating informative utterances in conversations) many varied passages may be equally relevant and we find that existing methods that jointly train the retriever and generator underperform: the retriever may not find relevant passages even amongst the top-10 and the generator may hence not learn a preference to ground its generated output in them. We propose using an additional guide retriever that is allowed to use the target output and \u201cin hindsight\u201d retrieve relevant passages during training. We model the guide retriever after the posterior distribution Q of passages given the input and the target output and train it jointly with the standard retriever and the generator by maximizing the evidence lower bound (ELBo) in expectation over Q. For informative conversations from the Wizard of Wikipedia dataset, with posterior-guided training, the retriever finds passages with higher relevance in the top-10 (23% relative improvement), the generator\u2019s responses are more grounded in the retrieved passage (19% relative improvement) and the end-to-end system produces better overall output (6.4% relative improvement).", "year": 2021, "ssId": "753fd6952c9f06f3bbd46e37129acc3f7a984896", "arXivId": "2110.07752", "link": "https://arxiv.org/pdf/2110.07752.pdf", "openAccess": true, "authors": ["Ashwin Paranjape", "O. Khattab", "Christopher Potts", "M. Zaharia", "Christopher D. Manning"]}