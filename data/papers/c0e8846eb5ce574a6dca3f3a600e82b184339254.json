{"title": "Inferring Rewards from Language in Context", "abstract": "In classic instruction following, language like \u201cI\u2019d like the JetBlue \ufb02ight\u201d maps to actions (e.g., selecting that \ufb02ight). However, language also conveys information about a user\u2019s underlying reward function (e.g., a general preference for JetBlue), which can allow a model to carry out desirable actions in new contexts. We present a model that infers rewards from language pragmatically: reasoning about how speakers choose utterances not only to elicit desired actions, but also to reveal information about their preferences. On a new interactive \ufb02ight\u2013booking task with natural language, our model more accurately infers rewards and predicts optimal actions in unseen environments, in comparison to past work that \ufb01rst maps language to actions (instruction following) and then maps actions to rewards (inverse reinforcement learning).", "year": 2022, "ssId": "c0e8846eb5ce574a6dca3f3a600e82b184339254", "arXivId": "2204.02515", "link": "https://arxiv.org/pdf/2204.02515.pdf", "openAccess": true, "authors": ["Jessy Lin", "Daniel Fried", "D. Klein", "A. Dragan"]}