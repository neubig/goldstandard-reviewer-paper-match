{"title": "Reachability of Optimal Convergence Rate Estimates for High-Order Numerical Convex Optimization Methods", "abstract": "The Monteiro\u2013Svaiter accelerated hybrid proximal extragradient method (2013) with one step of Newton\u2019s method used at every iteration for the approximate solution of an auxiliary problem is considered. The Monteiro\u2013Svaiter method is optimal (with respect to the number of gradient and Hessian evaluations for the optimized function) for sufficiently smooth convex optimization problems in the class of methods using only the gradient and Hessian of the optimized function. An optimal tensor method involving higher derivatives is proposed by replacing Newton\u2019s step with a step of Yu.E. Nesterov\u2019s recently proposed tensor method (2018) and by using a special generalization of the step size selection condition in the outer accelerated proximal extragradient method. This tensor method with derivatives up to the third order inclusive is found fairly practical, since the complexity of its iteration is comparable with that of Newton\u2019s one. Thus, a constructive solution is obtained for Nesterov\u2019s problem (2018) of closing the gap between tight lower and overstated upper bounds for the convergence rate of existing tensor methods of order $$p\\; \\geqslant \\;3$$.", "year": 2019, "ssId": "09ec8d8e2251e079abb0e109979f33ee120211fa", "arXivId": null, "link": null, "openAccess": false, "authors": ["A. Gasnikov", "Eduard A. Gorbunov", "D. Kovalev", "A. A. M. Mokhammed", "E. Chernousova"]}