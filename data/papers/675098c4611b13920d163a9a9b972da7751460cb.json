{"title": "Efficient learning for spoken language understanding tasks with word embedding based pre-training", "abstract": "Spoken language understanding (SLU) tasks such as goal estimation and intention identi\ufb01cation from user\u2019s commands are essential components in spoken dialog systems. In recent years, neural network approaches have shown great success in various SLU tasks. However, one major dif\ufb01culty of SLU is that the annotation of collected data can be expensive. Often this results in insuf\ufb01cient data being available for a task. The performance of a neural network trained in low resource conditions is usually inferior because of over-training. To improve the performance, this paper investigates the use of unsupervised training methods with large-scale corpora based on word embedding and latent topic models to pre-train the SLU networks. In order to capture long-term characteristics over the entire dialog, we propose a novel Recurrent Neural Network (RNN) architecture. The proposed RNN uses two sub-networks to model the different time scales represented by word and turn sequences. The combination of pre-training and RNN gives us a 18% relative error reduction compared to a baseline system.", "year": 2015, "ssId": "675098c4611b13920d163a9a9b972da7751460cb", "arXivId": null, "link": null, "openAccess": false, "authors": ["Yi Luan", "Shinji Watanabe", "B. Harsham"]}