{"title": "Beyond Theory and Data in Preference Modeling: Bringing Humans into the Loop", "abstract": "Many mathematical frameworks aim at modeling human preferences, employing a number of methods including utility functions, qualitative preference statements, constraint optimization, and logic formalisms. The choice of one model over another is usually based on the assumption that it can accurately describe the preferences of humans or other subjects/processes in the considered setting and is computationally tractable. Verification of these preference models often leverages some form of real life or domain specific data; demonstrating the models can predict the series of choices observed in the past. We argue that this is not enough: to evaluate a preference model, humans must be brought into the loop. Human experiments in controlled environments are needed to avoid common pitfalls associated with exclusively using prior data including introducing bias in the attempt to clean the data, mistaking correlation for causality, or testing data in a context that is different from the one where the data were produced. Human experiments need to be done carefully and we advocate a multi-disciplinary research environment that includes experimental psychologists and AI researchers. We argue that experiments should be used to validate models. We detail the design of an experiment in order to highlight some of the significant computational, conceptual, ethical, mathematical, psychological, and statistical hurdles to testing whether decision makers' preferences are consistent with a particular mathematical model of preferences.", "year": 2015, "ssId": "808a9c9dece4c21be50f41e6caf50101f2b24b47", "arXivId": null, "link": null, "openAccess": false, "authors": ["T. E. Allen", "Muye Chen", "J. Goldsmith", "Nicholas Mattei", "Anna Popova", "Michel Regenwetter", "F. Rossi", "Christopher E. Zwilling"]}