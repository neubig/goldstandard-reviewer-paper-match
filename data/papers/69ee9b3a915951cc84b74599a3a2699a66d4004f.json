{"title": "CLIPort: What and Where Pathways for Robotic Manipulation", "abstract": "How can we imbue robots with the ability to manipulate objects pre1 cisely but also to reason about them in terms of abstract concepts? Recent works 2 in manipulation have shown that end-to-end networks can learn dexterous skills 3 that require precise spatial reasoning, but these methods often fail to generalize to 4 new goals or quickly learn transferable concepts. In parallel, there has been great 5 progress in learning generalizable semantic representations for vision and lan6 guage by training on large-scale internet data, however these representations lack 7 the spatial understanding necessary for fine-grained manipulation. To this end, we 8 propose a framework that combines the best of both worlds: a two-stream archi9 tecture with semantic (what) and spatial (where) pathways for vision-based ma10 nipulation. Specifically, we present CLIPORT, a language-conditioned imitation11 learning agent that combines the broad semantic understanding of CLIP [1] with 12 the spatial precision of Transporter [2]. Our end-to-end framework is capable of 13 solving a variety of language-specified tabletop tasks from packing unseen objects 14 to folding cloths, all without any explicit representations of object poses, instance 15 segmentations, history, symbolic states, or syntactic structures. Experiments in 16 simulation and hardware show that our approach is data efficient in few-shot set17 tings and generalizes effectively to seen and unseen semantic concepts. We even 18 learn one multi-task policy for 10 simulated and 9 real-world tasks that shows 19 better or comparable performance to single-task models. 20", "year": 2021, "ssId": "69ee9b3a915951cc84b74599a3a2699a66d4004f", "arXivId": "2109.12098", "link": "https://arxiv.org/pdf/2109.12098.pdf", "openAccess": true, "authors": ["Mohit Shridhar", "Lucas Manuelli", "D. Fox"]}