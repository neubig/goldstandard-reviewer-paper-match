{"title": "Constrained Upper Confidence Reinforcement Learning with Known Dynamics", "abstract": "Constrained Markov Decision Processes are a class of stochastic decision problems in which the decision maker must select a policy that satisfies auxiliary cost constraints. This paper extends upper confidence reinforcement learning for settings in which the reward function and the constraints, described by cost functions, are unknown a priori but the transition kernel is known. Such a setting is well-motivated by a number of applications including exploration of unknown, potentially unsafe, environments. The algorithm, C-UCRL, is shown to have sub-linear regret (O(T 3 4 \u221a log(T/\u03b4))) with respect to the reward while satisfying the constraints even while learning with probability 1\u2212 \u03b4. An illustrative example is provided.", "year": 2020, "ssId": "af6c6e66fe0a9ba19c304665e01db1c5a5fba1e4", "arXivId": null, "link": null, "openAccess": false, "authors": ["Liyuan Zheng", "L. Ratliff", "M. Zeilinger"]}