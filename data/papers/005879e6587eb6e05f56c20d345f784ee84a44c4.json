{"title": "Comparing Top-Down and Bottom-Up Neural Generative Dependency Models", "abstract": "Recurrent neural network grammars generate sentences using phrase-structure syntax and perform very well on both parsing and language modeling. To explore whether generative dependency models are similarly effective, we propose two new generative models of dependency syntax. Both models use recurrent neural nets to avoid making explicit independence assumptions, but they differ in the order used to construct the trees: one builds the tree bottom-up and the other top-down, which profoundly changes the estimation problem faced by the learner. We evaluate the two models on three typologically different languages: English, Arabic, and Japanese. While both generative models improve parsing performance over a discriminative baseline, they are significantly less effective than non-syntactic LSTM language models. Surprisingly, little difference between the construction orders is observed for either parsing or language modeling.", "year": 2019, "ssId": "005879e6587eb6e05f56c20d345f784ee84a44c4", "arXivId": null, "link": null, "openAccess": false, "authors": ["Austin Matthews", "Graham Neubig", "Chris Dyer"]}