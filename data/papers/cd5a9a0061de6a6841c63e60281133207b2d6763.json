{"title": "Learning to Describe Phrases with Local and Global Contexts", "abstract": "When reading a text, it is common to become stuck on unfamiliar words and phrases, such as polysemous words with novel senses, rarely used idioms, internet slang, or emerging entities. If we humans cannot figure out the meaning of those expressions from the immediate local context, we consult dictionaries for definitions or search documents or the web to find other global context to help in interpretation. Can machines help us do this work? Which type of context is more important for machines to solve the problem? To answer these questions, we undertake a task of describing a given phrase in natural language based on its local and global contexts. To solve this task, we propose a neural description model that consists of two context encoders and a description decoder. In contrast to the existing methods for non-standard English explanation [Ni+ 2017] and definition generation [Noraset+ 2017; Gadetsky+ 2018], our model appropriately takes important clues from both local and global contexts. Experimental results on three existing datasets (including WordNet, Oxford and Urban Dictionaries) and a dataset newly created from Wikipedia demonstrate the effectiveness of our method over previous work.", "year": 2018, "ssId": "cd5a9a0061de6a6841c63e60281133207b2d6763", "arXivId": "1811.00266", "link": "https://arxiv.org/pdf/1811.00266.pdf", "openAccess": true, "authors": ["Shonosuke Ishiwatari", "Hiroaki Hayashi", "Naoki Yoshinaga", "Graham Neubig", "M. Toyoda", "M. Kitsuregawa"]}