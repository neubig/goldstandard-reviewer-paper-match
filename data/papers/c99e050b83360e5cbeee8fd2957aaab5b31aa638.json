{"title": "Calibration, Entropy Rates, and Memory in Language Models", "abstract": "Building accurate language models that capture meaningful long-term dependencies is a core challenge in natural language processing. Towards this end, we present a calibration-based approach to measure long-term discrepancies between a generative sequence model and the true distribution, and use these discrepancies to improve the model. Empirically, we show that state-of-the-art language models, including LSTMs and Transformers, are \\emph{miscalibrated}: the entropy rates of their generations drift dramatically upward over time. We then provide provable methods to mitigate this phenomenon. Furthermore, we show how this calibration-based approach can also be used to measure the amount of memory that language models use for prediction.", "year": 2019, "ssId": "c99e050b83360e5cbeee8fd2957aaab5b31aa638", "arXivId": "1906.05664", "link": "https://arxiv.org/pdf/1906.05664.pdf", "openAccess": true, "authors": ["M. Braverman", "Xinyi Chen", "S. Kakade", "Karthik Narasimhan", "Cyril Zhang", "Yi Zhang"]}