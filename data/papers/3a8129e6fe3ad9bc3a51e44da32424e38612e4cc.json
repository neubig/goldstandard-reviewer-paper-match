{"title": "TensorLog: A Differentiable Deductive Database", "abstract": "Large knowledge bases (KBs) are useful in many tasks, but it is unclear how to integrate this sort of knowledge into \"deep\" gradient-based learning systems. To address this problem, we describe a probabilistic deductive database, called TensorLog, in which reasoning uses a differentiable process. In TensorLog, each clause in a logical theory is first converted into certain type of factor graph. Then, for each type of query to the factor graph, the message-passing steps required to perform belief propagation (BP) are \"unrolled\" into a function, which is differentiable. We show that these functions can be composed recursively to perform inference in non-trivial logical theories containing multiple interrelated clauses and predicates. Both compilation and inference in TensorLog are efficient: compilation is linear in theory size and proof depth, and inference is linear in database size and the number of message-passing steps used in BP. We also present experimental results with TensorLog and discuss its relationship to other first-order probabilistic logics.", "year": 2016, "ssId": "3a8129e6fe3ad9bc3a51e44da32424e38612e4cc", "arXivId": "1605.06523", "link": "https://arxiv.org/pdf/1605.06523.pdf", "openAccess": true, "authors": ["William W. Cohen"]}