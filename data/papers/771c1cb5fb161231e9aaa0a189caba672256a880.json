{"title": "Using Morphological Knowledge in Open-Vocabulary Neural Language Models", "abstract": "Languages with productive morphology pose problems for language models that generate words from a fixed vocabulary. Although character-based models allow any possible word type to be generated, they are linguistically na\u00efve: they must discover that words exist and are delimited by spaces\u2014basic linguistic facts that are built in to the structure of word-based models. We introduce an open-vocabulary language model that incorporates more sophisticated linguistic knowledge by predicting words using a mixture of three generative processes: (1) by generating words as a sequence of characters, (2) by directly generating full word forms, and (3) by generating words as a sequence of morphemes that are combined using a hand-written morphological analyzer. Experiments on Finnish, Turkish, and Russian show that our model outperforms character sequence models and other strong baselines on intrinsic and extrinsic measures. Furthermore, we show that our model learns to exploit morphological knowledge encoded in the analyzer, and, as a byproduct, it can perform effective unsupervised morphological disambiguation.", "year": 2018, "ssId": "771c1cb5fb161231e9aaa0a189caba672256a880", "arXivId": null, "link": null, "openAccess": false, "authors": ["Austin Matthews", "Graham Neubig", "Chris Dyer"]}