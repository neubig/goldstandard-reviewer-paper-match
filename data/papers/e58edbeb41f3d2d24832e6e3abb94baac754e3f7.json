{"title": "Re-evaluating Evaluation in Text Summarization", "abstract": "Automated evaluation metrics as a stand-in for manual evaluation are an essential part of the development of text-generation tasks such as text summarization. However, while the field has progressed, our standard metrics have not -- for nearly 20 years ROUGE has been the standard evaluation in most summarization papers. In this paper, we make an attempt to re-evaluate the evaluation method for text summarization: assessing the reliability of automatic metrics using top-scoring system outputs, both abstractive and extractive, on recently popular datasets for both system-level and summary-level evaluation settings. We find that conclusions about evaluation metrics on older datasets do not necessarily hold on modern datasets and systems.", "year": 2020, "ssId": "e58edbeb41f3d2d24832e6e3abb94baac754e3f7", "arXivId": "2010.07100", "link": "https://arxiv.org/pdf/2010.07100.pdf", "openAccess": true, "authors": ["Manik Bhandari", "Pranav Narayan Gour", "A. Ashfaq", "P. Liu", "Graham Neubig"]}