{"title": "Supertagging With LSTMs", "abstract": "In this paper we present new state-of-the-art performance on CCG supertagging and parsing. Our model outperforms existing approaches by an absolute gain of 1.5%. We analyze the performance of several neural models and demonstrate that while feed-forward architectures can compete with bidirectional LSTMs on POS tagging, models that encode the complete sentence are necessary for the long range syntactic information encoded in supertags.", "year": 2016, "ssId": "0f5bb9ae0c060b349597c0b2582bf271a5a2156a", "arXivId": null, "link": null, "openAccess": false, "authors": ["Ashish Vaswani", "Yonatan Bisk", "Kenji Sagae", "Ryan Musa"]}