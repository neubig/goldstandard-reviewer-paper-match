{"title": "Towards Zero-Label Language Learning", "abstract": "This paper explores zero-label learning in Natural Language Processing (NLP), whereby no human-annotated data is used anywhere during training and models are trained purely on synthetic data. At the core of our framework is a novel approach for better leveraging the powerful pretrained language models. Specifically, inspired by the recent success of fewshot inference on GPT-3, we present a training data creation procedure named Unsupervised Data Generation (UDG), which leverages fewshot prompts to synthesize high-quality training data without real human annotations. Our method enables zero-label learning as we train task-specific models solely on the synthetic data, yet we achieve better or comparable results from strong baseline models trained on human-labeled data. Furthermore, when mixed with labeled data, our approach serves as a highly effective data augmentation procedure, achieving new state-of-the-art results on the SuperGLUE benchmark1.", "year": 2021, "ssId": "c2a79e2a65b721d4de5f6d4806323174b9f8f393", "arXivId": "2109.09193", "link": "https://arxiv.org/pdf/2109.09193.pdf", "openAccess": true, "authors": ["Zirui Wang", "Adams Wei Yu", "Orhan Firat", "Yuan Cao"]}