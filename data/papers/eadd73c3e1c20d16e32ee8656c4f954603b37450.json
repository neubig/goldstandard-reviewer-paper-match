{"title": "Vision-based Detection of Acoustic Timed Events: a Case Study on Clarinet Note Onsets", "abstract": "Acoustic events often have a visual counterpart. Knowledge of visual information can aid the understanding of complex auditory scenes, even when only a stereo mixdown is available in the audio domain, \\eg identifying which musicians are playing in large musical ensembles. In this paper, we consider a vision-based approach to note onset detection. As a case study we focus on challenging, real-world clarinetist videos and carry out preliminary experiments on a 3D convolutional neural network based on multiple streams and purposely avoiding temporal pooling. We release an audiovisual dataset with 4.5 hours of clarinetist videos together with cleaned annotations which include about 36,000 onsets and the coordinates for a number of salient points and regions of interest. By performing several training trials on our dataset, we learned that the problem is challenging. We found that the CNN model is highly sensitive to the optimization algorithm and hyper-parameters, and that treating the problem as binary classification may prevent the joint optimization of precision and recall. To encourage further research, we publicly share our dataset, annotations and all models and detail which issues we came across during our preliminary experiments.", "year": 2017, "ssId": "eadd73c3e1c20d16e32ee8656c4f954603b37450", "arXivId": "1706.09556", "link": "https://arxiv.org/pdf/1706.09556.pdf", "openAccess": true, "authors": ["A. Bazzica", "J. V. Gemert", "Cynthia C. S. Liem", "A. Hanjalic"]}