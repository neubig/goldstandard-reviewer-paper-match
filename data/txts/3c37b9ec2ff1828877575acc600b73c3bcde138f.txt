Modeling Attrition in Recommender Systems with Departing Bandits
Omer Ben-Porat*1, Lee Cohen*1, Liu Leqi*2, Zachary C. Lipton2, Yishay Mansour1,3
1 Blavatnik School of Computer Science, Tel-Aviv University 2 Machine Learning Department, Carnegie Mellon University
3 Google Research

arXiv:2203.13423v1 [cs.LG] 25 Mar 2022

Abstract
Traditionally, when recommender systems are formalized as multi-armed bandits, the policy of the recommender system inﬂuences the rewards accrued, but not the length of interaction. However, in real-world systems, dissatisﬁed users may depart (and never come back). In this work, we propose a novel multi-armed bandit setup that captures such policydependent horizons. Our setup consists of a ﬁnite set of user types, and multiple arms with Bernoulli payoffs. Each (user type, arm) tuple corresponds to an (unknown) reward probability. Each user’s type is initially unknown and can only be inferred through their response to recommendations. Moreover, if a user is dissatisﬁed with their recommendation, they might depart the system. We ﬁrst address the case where all users share the same type, demonstrating that a recent UCBbased algorithm is optimal. We then move forward to the more challenging case, where users are divided among two tpyrpoevsid. eWahnileefﬁnaciiveentalpeparronainchgeaslgcoanrinthomt htahnadtlaecthhiiesvseesttOi˜n(g√, wTe) regret, where T is the number of users.
1 Introduction
At the heart of online services spanning such diverse industries as media consumption, dating, ﬁnancial products, and more, recommendation systems (RSs) drive personalized experiences by making curation decisions informed by each user’s past history of interactions. While in practice, these systems employ diverse statistical heuristics, much of our theoretical understanding of them comes via stylized formulations within the multi-armed bandits (MABs) framework. While MABs abstract away from many aspects of real-world systems they allow us to extract crisp insights by formalizing fundamental tradeoffs, such as that between exploration and exploitation that all RSs must face (Joseph et al. 2016; Liu and Ho 2018; Patil et al. 2020; Ron, Ben-Porat, and Shalit 2021). As applies to RSs, exploitation consists of continuing to recommend items (or categories of items) that have been observed to yield high rewards in the past, while exploration consists of recommending items (or categories of items) about which the RS is uncertain but that could potentially yield even higher rewards.
In traditional formalizations of RSs as MABs, the recommender’s decisions affect only the rewards obtained. How-
*Equal contribution, alphabetical order.

ever, real-life recommenders face a dynamic that potentially alters the exploration-exploitation tradeoff: Dissatisﬁed users have the option to depart the system, never to return. Thus, recommendations in the service of exploration not only impact instantaneous rewards but also risk driving away users and therefore can inﬂuence long-term cumulative rewards by shortening trajectories of interactions.
In this work, we propose departing bandits which augment conventional MABs by incorporating these policydependent horizons. To motivate our setup, we consider the following example: An RS for recommending blog articles must choose at each time among two categories of articles, e.g., economics and sports. Upon a user’s arrival, the RS recommends articles sequentially. After each recommendation, the user decides whether to “click” the article and continue to the next recommendation, or to “not click” and may leave the system. Crucially, the user interacts with the system for a random number of rounds. The user’s departure probability depends on their satisfaction from the recommended item, which in turn depends on the user’s unknown type. A user’s type encodes their preferences (hence the probability of clicking) on the two topics (economics and sports).
When model parameters are given, in contrast to traditional MABs where the optimal policy is to play the best ﬁxed arm, departing bandits require more careful analysis to derive an optimal planning strategy. Such planning is a local problem, in the sense that it is solved for each user. Since the user type is never known explicitly (the recommender must update its beliefs over the user types after each interaction), ﬁnding an optimal recommendation policy requires solving a speciﬁc partially observable MDP (POMDP) where the user type constitutes the (unobserved) state (more details in Section 5.1). When the model parameters are unknown, we deal with a learning problem that is global, in the sense that the recommender (learner) is learning for a stream of users instead of a particular user.
We begin with a formal deﬁnition of departing bandits in Section 2, and demonstrate that any ﬁxed-arm policy is prone to suffer linear regret. In Section 3, we establish the UCB-based learning framework used in later sections. We itniosntan4t,iawtehetrheiswferasmheowworthkawt iitthacahsieinvgesleO˜u(s√erTty) preegirnetSfeocrT being the number of users. We then move to the more challenging case with two user types and two recommenda-

tion categories in Section 5. To analyze the planning problem, we effectively reduce the search space for the optimal policy by using a closed-form of the expected return orifthamnythreactoamchmieevnedserO˜p(o√licTy). Trehgerseet riensuthltiss ssuegttgiensgt. aFninaalglloy-, we show an efﬁcient optimal planning algorithm for multiple user types and two recommendation categories (Appendix A) and describe a scheme to construct semi-synthetic problem instances for this setting using real-world datasets (Appendix B).

1.1 Related Work

MABs have been studied extensively by the online

learning community (Cesa-Bianchi and Lugosi 2006;

Bubeck, Cesa-Bianchi et al. 2012). The contextual bandit

literature augments the MAB setup with context-dependent

rewards (Abbasi-Yadkori, Pál, and Szepesvári 2011;

Slivkins 2019; Mahadik et al. 2020; Korda, Szörényi, and Li

2016; Lattimore and Szepesvári 2020). In contextual ban-

dits, the learner observes a context before they make a

decision, and the reward depends on the context. Another

line of related work considers the dynamics that emerge

when users act strategically (Kremer, Mansour, and Perry

2014;

Mansour, Slivkins, and Syrgkanis

2015;

Cohen and Mansour

2019;

Bahar, Smorodinsky, and Tennenholtz 2016; Bahar et al.

2020). In that line of work, users arriving at the system

receive a recommendation but act strategically: They can

follow the recommendation or choose a different action.

This modeling motivates the development of incentive-

compatible mechanisms as solutions. In our work, however,

the users are modeled in a stochastic (but not strategic)

manner. Users may leave the system if they are dissatisﬁed

with recommendations, and this departure follows a ﬁxed

(but possibly unknown) stochastic model.

The departing bandits problem has two important fea-

tures: Policy-dependent horizons, and multiple user types

that can be interpreted as unknown states. Existing MAB

works (Azar, Lazaric, and Brunskill 2013; Cao et al. 2020)

have addresses these phenomena separately but we know of

no work that integrates the two in a single framework. In

particular, while Azar, Lazaric, and Brunskill (2013) study

the setting with multiple user types, they focus on a ﬁxed

horizon setting. Additionally, while Cao et al. (2020) deal

with departure probabilities and policy-dependent interac-

tion times for a single user type, they do not consider the

possibility of multiple underlying user types.

The planning part of our problem falls under

the framework of using Markov Decision Pro-

cesses for modeling recommender-user dynam-

ics (Shani, Heckerman, and Brafman 2005). Speciﬁcally,

our problem works with partially observable user states

which have also been seen in many recent bandits variants

(Pike-Burke and Grünewälder 2019; Leqi et al. 2020).

Unlike these prior works that focus on interactions with a

single user, departing bandits consider a stream of users

each of which has an (unknown) type selected among a

ﬁnite set of user types.

More broadly, our RS learning problem falls under the domain of reinforcement learning (RL). Existing RL literature that considers departing users in RSs include Zhao et al. (2020b); Lu and Yang (2016); Zhao et al. (2020a). While Zhao et al. (2020b) handle users of a single type that depart the RS within a bounded number of interactions, our work deals with multiple user types. In contrast to Zhao et al. (2020a), we consider an online setting and provide regret guarantees that do not require bounded horizon. Finally, Lu and Yang (2016) use POMDPs to model user departure and focus on approximating the value function. They conduct an experimental analysis on historical data, while we devise an online learning algorithm with theoretical guarantees.
2 Departing Bandits
We propose a new online problem, called departing bandits, where the goal is to ﬁnd the optimal recommendation algorithm for users of (unknown) types, and where the length of the interactions depends on the algorithm itself. Formally, the departing bandits problem is deﬁned by a tuple [M ], [K], q, P, , where M is the number of user types, K is the number of categories, q ∈ [0, 1]M speciﬁes a prior distribution over types, and P ∈ (0, 1)K×M and ∈ (0, 1)K×M are the click-probability and the departure-probability matrices, respectively.1
There are T users who arrive sequentially at the RS. At every episode, a new user t ∈ [T ] arrives with a type type(t). We let q denote the prior distribution over the user types, i.e., type(t) ∼ q. Each user of type x clicks on a recommended category a with probability Pa,x. In other words, each click follows a Bernoulli distribution with parameter Pa,x. Whenever the user clicks, she stays for another iteration, and when the user does not click (no-click), she departs with probability a,x (and stays with probability 1 − a,x). Each user t interacts with the RS (the learner) until she departs.
We proceed to describe the user-RS interaction protocol. In every iteration j of user t, the learner recommends a category a ∈ [K] to user t. The user clicks on it with probability Pa,type(t). If the user clicks, the learner receives a reward of rt,j(a) = 1.2 If the user does not click, the learner receives no reward (i.e., rt,j(a) = 0), and user t departs with probability a,type(t). We assume that the learner knows the value of a constant ǫ > 0 such that maxa,x Pa,x ≤ 1 − ǫ (i.e., ǫ does not depend on T ). When user t departs, she does not interact with the learner anymore (and the learner moves on to the next user t + 1). For convenience, the departing bandits problem protocol is summarized in Algorithm 1.
Having described the protocol, we move on to the goals of the learner. Without loss of generality, we assume that the online learner’s recommendations are made based on a
1We denote by [n] the set {1, . . . , n}. 2We formalize the reward as is standard in the online learning literature, from the perspective of the learner. However, deﬁning the reward from the user perspective by, e.g., considering her utility as the number of clicks she gives or the number of articles she reads induces the same model.

Algorithm 1: The Departing Bandits Protocol

Input: number of types M , number of categories K, and number of users (episodes) T Hidden Parameters: types prior q, click-probability P, and

departure-probability

1: for episode t ← 1, . . . , T do

2: a new user with type type(t) ∼ q arrives

3: j ← 1, depart ← f alse

4: while depart is f alse do

5:

the learner picks a category a ∈ [K]

6:

with probability Pa,x, user t clicks on a and

rt,j(a) ← 1; otherwise, rt,j(a) ← 0

7:

if rt,j(a) = 0 then

8:

with probability a,x: depart ← true and user t

departs

9:

the learner observes rt,j(a) and depart

10:

if depart is f alse then

11:

j ←j+1

policy π, which is a mapping from the history of previous interactions (with that user) to recommendation categories.

For each user (episode) t ∈ [T ], the learner selects a

policy πt that recommends category πt,j ∈ [K] at every iteration j ∈ [N πt (t)], where N πt(t) denotes the episode

length (i.e., total number of iterations policy πt interacts with user t until she departs).3 The return of a policy π, denoted by V π is the cumulative reward the learner obtains

when executing the policy π until the user departs. Put dif-

ferently, the return of π from user t is the random variable

Vπ =

N π (t) j=1

rt,j

(πt,j

).

We denote by π∗ an optimal policy, namely a policy that maximizes the expected return, π∗ = arg maxπ E[V π]. Similarly, we denote by V ∗ the optimal return, i.e., V ∗ = V π∗.

We highlight two algorithmic tasks. The ﬁrst is the planning task, in which the goal is to ﬁnd an optimal policy π∗, given P, , q. The second is the online learning task. We consider settings where the learner knows the number of categories, K, the number of types, M , and the number of users, T , but has no prior knowledge regarding P, or q. In the online learning task, the value of the learner’s algorithm is the
sum of the returns obtained from all the users, namely

T

T N π(t)

V πt =

rt,j (πt,j ).

t=1

t=1 j=1

The performance of the leaner is compared to that of the best policy, formally deﬁned by the regret for T episodes,

T

RT = T · E[V π∗ ] − V πt .

(1)

t=1

The learner’s goal is to minimize the expected regret E[RT ].

Category 1 Category 2
Prior

Type x
P1,x = 0.5 P2,x = 0.4 qx = 0.4

Type y
P1,y = 0.28 P2,y = 0.39 qy = 0.6

Table 1: The departing bandits instance in Section 2.1.

2.1 Example

The motivation for the following example is two-fold. First,

to get the reader acquainted with our notations; and second,

to show why ﬁxed-arm policies are inferior in our setting.

Consider a problem instance with two user types (M = 2),

which we call x and y for convenience. There are two cat-

egories (K = 2), and given no-click the departure is de-

terministic, i.e., a,τ = 1 for every category a ∈ [K] and

type τ ∈ [M ]. That is, every user leaves immediately if she

does not click. Furthermore, let the click-probability P ma-

trix and the user type prior distribution q be as in Table 1.

Looking at P and q, we see that Category 1 is better for

Type x, while Category 2 is better for type y. Notice that

without any additional information, a user is more likely

to be type y. Given the prior distribution, recommending

Category 1 in the ﬁrst round yields an expected reward of

qxP1,x + qyP1,y = 0.368. Similarly, recommending Cat-

egory 2 in the ﬁrst round results in an expected reward

of 0.394. Consequently, if we recommend myopically, i.e.,

without considering the user type, always recommending

Category 2 is better than always recommending Category

1.

Let πa denote the ﬁxed-arm policy that always selects a

single category a. Using the tools we derive in Section 5 and

in particular Theorem 5.3, we can compute the expected re-

turns of π1 and π2, E[V π1] and E[V π2]. Additionally, using

results from Section 5.2, we can show that the optimal policy for the planning task, π∗, recommends Category 2 until

iteration 7, and then recommends Category 1 for the rest of

the iterations until the user departs.

Using simple calculations, we see that E[V π∗ ] −

E[V π1 ] > 0.0169 and E[V π∗ ] − E[V π2 ] > 1.22 × 10−5;

hence, the expected return of the optimal policy is greater

than the returns of both ﬁxed-arm policies by a constant. As a result, if the learner only uses ﬁxed-arm policies (πa

for every a ∈ [K]), she suffers linear expected regret, i.e.,

E[RT ] = T · E[V π∗ ] −

T t=1

E[V

πa ]

=

Ω(T

).

3 UCB Policy for Sub-exponential Returns
In this section, we introduce the learning framework used in the paper and provide a general regret guarantee for it.
In standard MAB problems, at each t ∈ [T ] the learner picks a single arm and receives a single sub-Gaussian reward. In contrast, in departing bandits, at each t ∈ [T ] the learner receives a return V π, which is the cumulative reward of that policy. The return V π depends on the policy π not only through the obtained rewards at each iteration

3We limit the discussion to deterministic policies solely; this is w.l.o.g. (see Subsection 5.1 for further details).

but also through the total number of iterations (trajectory length). Such returns are not necessarily sub-Gaussian. Consequently, we cannot use standard MAB algorithms as they usually rely on concentration bounds for sub-Gaussian rewards. Furthermore, as we have shown in Section 2.1, in departing bandits ﬁxed-arm policies can suffer linear regret (in terms of the number of users), which suggests considering a more expressive set of policies. This in turn yields another disadvantage for using MAB algorithms for departing bandits, as their regret is linear in the number of arms (categories) K.
As we show later in Sections 4 and 5, for some natural instances of the departing bandits problem, the return from each user is sub-exponential (Deﬁnition 3.1). Algorithm 2, which we propose below, receives a set of policies Π as input, along with other parameters that we describe shortly. The algorithm is a restatement of the UCB-Hybrid Algorithm from Jia, Shi, and Shen (2021), with two modiﬁcations: (1) The input includes a set of policies rather than a set of actions/categories, and accordingly, the conﬁdence bound updates are based on return samples (denoted by Vˆ π) rather than reward samples. (2) There are two global parameters (τ˜ and η) instead of two local parameters per action. If the return from each policy in Π is sub-exponential, Algorithm 2 not only handles sub-exponential returns, but also comes with the following guarantee: Its expected value is close to the value of the best policy in Π.

3.1 Sub-exponential Returns
For convenience, we state here the deﬁnition of subexponential random variables (Eldar and Kutyniok 2012).
Deﬁnition 3.1. We say that a random variable X is subexponential with parameters (τ 2, b) if for every γ such that |γ| < 1/b,

E[exp(γ(X − E[X]))] ≤ exp( γ2τ 2 ). 2

In addition, for every (τ 2, b)-sub-exponential random variables, there exist constants C1, C2 > 0 such that the above is equivalent to each of the following properties:

1. Tails: ∀v ≥ 0 : Pr[|X| > v] ≤ exp(1 − Cv1 ). 2. Moments: ∀p ≥ 1 : (E[|X|p])1/p ≤ C2p.

Let Π be a set of policies with the following property:

There exist τ˜, η such that the return of every policy π ∈ Π

is

(τ 2, b)-sub-exponential with

τ˜

≥

τ

and

η

≥

b2 τ2

.

The

fol-

lowing Algorithm 2 receives as input a set of policies Π with

the associated parameters, τ˜ and η. Similarly to the UCB al-

gorithm, it maintains an upper conﬁdence bound U for each

policy, and balances between exploration and exploitation.

Theorem 3.2 below shows that Algorithm 2 always gets a

value similar to that of the best policy in Π up to an additive

factor of O˜ |Π| T +|Π| . The theorem follows directly

from Theorem 3 from Jia, Shi, and Shen (2021) by having policies as arms and returns as rewards.

Theorem 3.2. Let Π be a set of policies with the associated parameters τ˜, η. Let π1, . . . , πT be the policies Algorithm 2

Algorithm 2: UCB-based algorithm with hybrid radii: UCBHybrid (Jia, Shi, and Shen 2021)

1: Input: set of policies Π, number of users T , τ˜, η

2: Initialize: ∀π ∈ Π : U0(π) ← ∞, n(π) = 0

3: for user t ← 1, . . . , T do

4: Execute πt such that πt ∈ arg maxπ∈Π Ut−1(π) and

receive return Vˆ πt[n(πt)] ←

N πt j=1

(t)

rt,j

(πt,j

)

5: n(πt) ← n(πt) + 1

6: if n(πt) < 8η ln T then

7:

Update Ut(πt) = ni=(π1t) Vˆ πt [i] + 8√η·τ˜ ln T

n(πt )

n(πt )

8: else 9: Update Ut(πt) = ni=(πn1t()πVtˆ)πt [i] + 8τn˜2(πlnt)T

selects. It holds that





T

E max T · V π − V πt  = O( π∈Π t=1

|Π| T log T +|Π| log T ).

There are two challenges in leveraging Theorem 3.2. The
ﬁrst challenge is crucial: Notice that Theorem 3.2 does not
imply that Algorithm 2 has a low regret; its only guarantee is w.r.t. the policies in Π received as an input. As the number of policies is inﬁnite, our success will depend on our ability to characterize a “good” set of policies Π. The second challenge is technical: Even if we ﬁnd such Π, we still need to characterize the associated τ˜ and η. This is precisely what we do in Section 4 and 5.

4 Single User Type
In this section, we focus on the special case of a single user type, i.e., M = 1. For notational convenience, since we only discuss single-type users, we associate each category a ∈ [K] with its two unique parameters Pa := Pa,1, a := a,1 and refer to them as scalars rather than vectors. In addition, We use the notation Na for the random variable representing the number of iterations until a random user departs after being recommended by πa, the ﬁxed-arm policy that recommends category a in each iteration.
To derive a regret bound for single-type users, we use two
main lemmas: Lemma 4.1, which shows the optimal policy
is ﬁxed, and Lemma 4.3, which shows that returns of ﬁxed-
arm policies are sub-exponential and calculate their corre-
sponding parameters. These lemmas allow us to use Algopriothlimcie2s,waintdh daepriovleicayO˜se(t√ΠT )thraetgcreotnbtaoiunnsda.lFl othrebrﬁexveidty-,awrme relegate all the proofs to the Appendix.
To show that there exists a category a∗ ∈ [K] for which πa∗ is optimal, we rely on the assumption that all the users have the same type (hence we drop the type subscripts t), and as a result the rewards of each category a ∈ [K] have an expectation that depends on a single parameter, namely E[r(a)] = Pa. Such a category a∗ ∈ [K] does not necessarily have the maximal click-probability nor the minimal
departure-probability, but rather an optimal combination of
the two (in a way, this is similar to the knapsack problem,

where we want to maximize the reward while having as little weight as possible). We formalize it in the following lemma.
Lemma 4.1. A policy πa∗ is optimal if

a∗ ∈ arg max Pa . a∈[K] a(1 − Pa)

As a consequence of this lemma, the planning problem
for single-type users is trivial—the solution is a ﬁxed-arm policy πa∗ given in the lemma. However, without access to the model parameters, identifying πa∗ requires learning. We proceed with a simple observation regarding the random
number of iterations obtained by executing a ﬁxed-arm pol-
icy. The observation would later help us show that the return
of any ﬁxed-arm policy is sub-exponential.

Observation 4.2. For every a ∈ [K] and every a > 0, the random variable Na follows a geometric distribution with success probability parameter a[1 − Pa] ∈ (0, 1 − ǫ].

Using Observation 4.2 and previously known results (stated as Lemma D.3 in the appendix), we show that Na is sub-exponential for all a ∈ [K]. Notice that return realizations are always upper bounded by the trajectory length;
this implies that returns are also sub-exponential. However,
to use the regret bound of Algorithm 2, we need information regarding the parameters (τa2, ba) for every policy πa. We provide this information in the following Lemma 4.3.

Lemma 4.3. For each category a ∈ [K], the centred random variable V πa − E[V πa ] is sub-exponential with parameters (τa2, ba), such that

τa = ba = −

8e

.

ln(1 − a(1 − Pa))

Proof sketch. We rely on the equivalence between the subex-
ponentiality of a random variable and the bounds on its
moments (Property 2 in Deﬁnition 3.1). We bound the expectation of the return V πa, and use Minkowski’s and Jensen’s inequalities to show in Lemma D.2 that E[|V πa − E[V πa ]|p])1/p is upper bounded by −4/ ln(1 − a(1 − Pa)) for every a ∈ [K] and p ≥ 1. Finally, we apply a normalization trick and bound the Taylor series of E[exp(γ(V πa − E[V πa ]))] to obtain the result.

An immediate consequence of Lemma 4.3 is that the pa-

rameters

τ˜

=

8e/

ln(

1 1−ǫ

)

and

η

=

1

are

valid

upper

bounds

for τa and ba/τa2 for each a ∈ [K] (I.e., ∀a ∈ [K] : τ˜ ≥ τa

and η ≥ b2a/τa2). We can now derive a regret bound using

Algorithm 2 and Theorem 3.2.

Theorem 4.4. For single-type users (M = 1), running Al-

gorithm 2 with Π = {πa : a ∈ [K]} and τ˜ =

8e
1

,η =1

ln( 1−ǫ )

achieves an expected regret of at most

E[RT ] = O( KT log T + K log T ).

5 Two User Types and Two Categories
In this section, we consider cases with two user types (M = 2), two categories (K = 2) and departure-probability a,τ = 1 for every category a ∈ [K] and type τ ∈ [M ]. Even in

this relatively simpliﬁed setting, where users leave after the ﬁrst “no-click”, planning is essential. To see this, notice that the event of a user clicking on a certain category provides additional information about the user, which can be used to tailor better recommendations; hence, algorithms that do not take this into account may suffer a linear regret. In fact, this is not just a matter of the learning algorithm at hand, but rather a failure of all ﬁxed-arm policies; there are instances where all ﬁxed-arm policies yield high regret w.r.t. the baseline deﬁned in Equation (1). Indeed, this is what the example in Section 2.1 showcases. Such an observation suggests that studying the optimal planning problem is vital.
In Section 5.1, we introduce the partially observable MDP formulation of departing bandits along with notion of beliefcategory walk. We use this notion to provide a closed-form formula for policies’ expected return, which we use extensively later on. Next, in Section 5.2 we characterize the optimal policy, and show that we can compute it in constant time relying on the closed-form formula. This is striking, as generally computing optimal POMDP policies is computationally intractable since, e.g., the space of policies grows exponentially with the horizon. Conceptually, we show that there exists an optimal policy that depends on a belief threshold: It recommends one category until the posterior belief of one type, which is monotonically increasing, crosses the threshold, and then it recommends the other category. Finally, in Section 5.3 we leverage all the previously obtained results to derive a small set of threshold policies of size O(ln T ) woriethmc3o.r2r,esthpiosnrdeisnugltsiumbp-elixepsoaneO˜n(t√ialTp)arraemgreette.rs. Due to The-
5.1 Efﬁcient Planning
To recap, we aim to ﬁnd the optimal policy when the clickprobability matrix and the prior over user types are known. Namely, given an instance in the form of P, q , our goal is to efﬁciently ﬁnd the optimal policy.
For planning purposes, the problem can be modeled by an episodic POMDP, S, [K], O, Tr, P, Ω, q, O . A set of states, S = [M ] ∪ {⊥} that comprises all types [M ], along with a designated absorbing state ⊥ suggesting that the user departed (and the episode terminated). [K] is the set of the actions (categories). O = {stay, depart} is the set of possible observations. The transition and observation functions, Tr : S × [K] → S and Ω : S × [K] → O (respectively) satisfy Tr(⊥ |i, a) = Ω(depart|i, a) = 1 − Pi,a and Tr(i|i, a) = Ω(stay|i, a) = Pi,a for every type i ∈ [M ] and action a ∈ [K]. Finally, P is the expected reward matrix, and q is the initial state distribution over the M types.
When there are two user types and two categories, the click-probability matrix is given by Table 2 where we note that the prior on the types holds qy = 1 − qx, thus can be represented by a single parameter qx.
Remark 5.1. Without loss of generality, we assume that P1,x ≥ P2,x, P1,y, P2,y since one could always permute the matrix to obtain such a structure.
Since the return and number of iterations for the same policy is independent of the user index, we drop the subscript t in the rest of this subsection and use .

Category 1 Category 2
Prior

Type x
P1,x P2,x qx

Type y
P1,y P2,y qy = 1 − qx

Table 2: Click probabilities for two user types and two categories.

As is well-known in the POMDP litera-

ture (Kaelbling, Littman, and Cassandra 1998), the optimal policy π∗ and its expected return are functions of belief

states that represent the probability of the state at each time.

In our setting, the states are the user types. We denote by bj the belief that the state is (type) x at iteration j. Similarly,

1 − bj is the belief that the state is (type) y at iteration j. Needless to say, once the state ⊥ is reached, the belief over

the type states [M ] is irrelevant, as users do not come back.

Nevertheless, we neglect this case as our analysis does not

make use it.

We now describe how to compute the belief. At iteration

j = 1, the belief state is set to be b1 = P (state = x) = qx. At iteration j > 1, upon receiving a positive reward rj = 1, the belief is updated from bj−1 ∈ [0, 1] to

bj(bj−1, a, 1) =

bj−1 · Pa,x

, (2)

bj−1 · Pa,x + Pa,y(1 − bj−1)

where we note that in the event of no-click, the current user

departs the system, i.e., we move to the absorbing state ⊥. For any policy π : [0, 1] → {1, 2} that maps a belief to a

category, its expected return satisﬁes the Bellman equation,

E[V π(b)] = bPπ(b),x + (1 − b)Pπ(b),y ·

(1 + E[V π(b′(b, π(b), 1))]).

To better characterize the expected return, we introduce the

following notion of belief-category walk.

Deﬁnition 5.2 (Belief-category walk). Let π : [0, 1] → {1, 2} be any policy. The sequence

b1, a1 = π(b1), b2, a2 = π(b2), . . .

is called the belief-category walk. Namely, it is the induced walk of belief updates and categories chosen by π, given all the rewards are positive (rj = 1 for every j ∈ N).

Notice that every policy induces a single, well-deﬁned

and deterministic belief-category walk (recall that we as-
sume departure-probabilities satisfy a,τ = 1 for every a ∈ [K], τ ∈ [M ]). Moreover, given any policy π, the trajectory of every user recommended by π is fully characterized by belief-category walk clipped at bNπ(t), aNπ(t).
In what follows, we derive a closed-form expression for
the expected return as a function of b, the categories chosen

by the policy, and the click-probability matrix.

Theorem 5.3. For every policy π and an initial belief b ∈ [0, 1], the expected return is given by

∞

E[V π(b)] =

b

·

Pm1,i
1,x

·

Pm2,i
2,x

+

(1

−

b)Pm 1,y1,i

·

Pm2,i
2,y

,

i=1

where m1,i := |{aj = 1, j ≤ i}| and m2,i := |{aj = 2, j ≤ i}| are calculated based on the belief-category walk
b1, a1, b2, a2, . . . induced by π.

5.2 Characterizing the Optimal Policy
Using Theorem 5.3, we show that the planning problem can be solved in O(1). To arrive at this conclusion, we perform a case analysis over the following three structures of the clickprobability matrix P:
• Dominant Row, where P1,y ≥ P2,y;
• Dominant Column, where P2,x ≥ P2,y > P1,y;
• Dominant Diagonal, where P1,x ≥ P2,y > P1,y, P2,x.
Crucially, any matrix P takes exactly one of the three structures. Further, since P is known in the planning problem, identifying the structure at hand takes O(1) time. Using this structure partition, we characterize the optimal policy.
Dominant Row We start by considering the simplest structure, in which the Category 1 is preferred by both types of users: Since P1,y ≥ P2,y and P1,x ≥ P2,x, P1,y, P2,y (Remark 5.1), there exists a dominant row, i.e., Category 1.
Lemma 5.4. For any instance such that P has a dominant row a, the ﬁxed policy πa is an optimal policy.
As expected, if Category 1 is dominant then the policy that always recommends Category 1 is optimal.
Dominant Column In the second structure we consider the case where there is no dominant row, and that the column of type x is dominant, i.e., P1,x ≥ P2,x ≥ P2,y > P1,y. In such a case, which is also the one described in the example in Section 2.1, it is unclear what the optimal policy would be since none of the categories dominates the other.
Surprisingly, we show that the optimal policy can be of only one form: Recommend Category 2 for some time steps (possibly zero) and then always recommend Category 1. To identify when to switch from Category 2 to Category 1, one only needs to compare four expected returns.
Theorem 5.5. For any instance such that P has a dominant column, one of the following four policies is optimal:
π1, π2, π2:⌊N∗⌋, π2:⌈N∗⌉,
where N ∗ = N ∗(P, q) is a constant, and π2:⌊N∗⌋ (π2:⌈N∗⌉) stands for recommending Category 2 until iteration ⌊N ∗⌋ (⌈N ∗⌉) and then switching to Category 1.
The intuition behind the theorem is as follows. If the prior tends towards type y, we might start with recommending Category 2 (which users of type y are more likely to click on). But after several iterations, and as long as the user stays, the posterior belief b increases since P2,x > P2,y (recall Equation (2)). Consequently, since type x becomes more probable, and since P1,x ≥ P2,x, the optimal policy recommends the best category for this type, i.e., Category 1. For the exact expression of N ∗, we refer the reader to Appendix E.3.
Using Theorem 5.3, we can compute the expected return for each of the four policies in O(1), showing that we can ﬁnd the optimal policy when P has a column in O(1).

Dominant Diagonal In the last structure, we consider the case where there is no dominant row (i.e., P2,y > P1,y) nor a dominant column (i.e., P2,y > P2,x). At ﬁrst glance, this case is more complex than the previous two, since none of the categories and none of the types dominates the other one. However, we uncover that the optimal policy can be either always recommending Category 1 or always recommending Category 2. Theorem 5.6 summarizes this result.
Theorem 5.6. For any instance such that P has a dominant diagonal, either π1 or π2 is optimal.
With the full characterization of the optimal policy derived in this section (for all the three structures), we have shown that the optimal policy can be computed in O(1).

5.3 Learning: UCB-based regret bound
In this section, we move from the planning task to the learning one. Building on the results of previous sections, we know that there must exist a threshold policy—a policy whose belief-category walk has a ﬁnite preﬁx of one category, and an inﬁnite sufﬁx with the other category—which is optimal. However, there can still be inﬁnitely many such policies. To address this problem, we ﬁrst show how to reduce the search space for approximately optimal policies with negligible additive factor to a set of |Π| = O(ln(T )) policies. Then, we derive the parameters τ˜ and η required for Algorithm 2. As an immediate consequence, we get a sublinear regret algorithm for this setting. We begin with deﬁning threshold policies.

Deﬁnition 5.7 (Threshold Policy). A policy π is called an (a, h)-threshold policy if there exists an number h ∈ N∪{0} in π’s belief-category walk such that
• π recommends category a in iterations j ≤ h, and • π recommends category a′ in iterations j > h,
for a, a′ ∈ {1, 2} and a = a′.
For instance, the policy π1 that always recommends Category 1 is the (2, 0)-threshold policy, as it recommends Category 2 until the zero’th iteration (i.e., never recommends
Category 2) and then Category 1 eternally. Furthermore, the policy π2:⌊N∗⌋ introduced in Theorem 5.5 is the (2, ⌊N ∗⌋)threshold policy.
Next, recall that the chance of departure in every iteration is greater or equal to ǫ, since we assume maxa,τ Pa,τ ≤ 1 − ǫ. Consequently, the probability that a user will stay beyond H iterations is exponentially decreasing with H. We could use high-probability arguments to claim that it sufﬁces to focus on the ﬁrst H iterations, but without further insights this would yield Ω(2H ) candidates for the optimal policy. Instead, we exploit our insights about threshold policies.
Let ΠH be the set of all (a, h)-threshold policies for a ∈ {1, 2} and h ∈ [H] ∪ {0}. Clearly, |ΠH | = 2H + 2. Lemma 5.8 shows that the return obtained by the best policy in ΠH is not worse than that of the optimal policy π∗ by a negligible factor.

Lemma 5.8. For every H ∈ N, it holds that

E V π∗ − max V π
π∈ΠH

1 ≤ 2O(H) .

Before we describe how to apply Algorithm 2, we need to show that returns of all the policies in ΠH are subexponential. In Lemma 5.9, we show that V π is (τ 2, b)-subexponential for every threshold policy π ∈ ΠH , and provide bounds for both τ and b2/τ 2.

Lemma 5.9. Let τ˜ =

8e
1

and η = 1. For every

ln( 1−ǫ )

threshold policy π ∈ ΠH , the centred random variable

V π −E[V π] is (τ 2, b)-sub-exponential with (τ 2, b) satisfying

τ˜ ≥ τ and η ≥ b2/τ 2.

We are ready to wrap up our solution for the learning task
proposed in this section. Let H = Θ(ln T ), ΠH be the set of threshold policies characterized before, and let τ˜ and η be
constants as deﬁned in Lemma 5.9.

Theorem 5.10. Applying Algorithm 2 with ΠH , T, τ˜, η on the class of two-types two-categories instances considered
in this section always yields an expected regret of √
E[RT ] ≤ O( T ln T ).

Proof. It holds that





T

E[RT ] = E T V π∗ − V πt 

t=1

=E

T V π∗ − max T V π
π∈ΠH





T

+ E  max T V π − V πt 
π∈ΠH t=1

(∗) T

√

≤ 2O(H) + O( HT log T + H log T ) = O( T ln T ),

where (∗) follows from Theorem 3.2 and Lemma 5.8. Finally, setting H = Θ(ln T ) yields the desired result.

6 Conclusions and Discussion
This paper introduces a MAB model in which the recommender system inﬂuences both the rewards accrued and the length of interaction. We dealt with two classes of problems: A single user type with general departure probabilities (Section 4) and the two user types, two categories where each user departs after her ﬁrst no-click (Section 5). For each problem class, we started with analyzing the planning task, then characterized a small set of candidates for the optimal policy, and then applied Algorithm 2 to achieve sublinear regret.
In the appendix, we also consider a third class of problems: Two categories, multiple user types (M ≥ 2) where user departs with their ﬁrst no-click. We use the closed-form expected return derived in Theorem 5.3 to show how to use dynamic programming to ﬁnd approximately optimal planning policies. We formulate the problem of ﬁnding an optimal policy for a ﬁnite horizon H in a recursive manner. Particularly, we show how to ﬁnd a /1 2O(H) additive approximation in run-time of O(H2). Unfortunately, this approach cannot assist us in the learning task. Dynamic programming relies on skipping sub-optimal solutions to sub-problems (shorter horizons in our case), but this happens on the ﬂy; thus, we cannot a-priori deﬁne a small set of candidates

like what Algorithm 2 requires. More broadly, we could use this dynamic programming approach for more than two categories, namely for K ≥ 2, but then the run-time becomes O(HK ).
There are several interesting future directions. First, achieving low regret for the setup in Section 5 with K ≥ 2. We suspect that this class of problems could enjoy a solution similar to ours, where candidates for optimal policies are mixing two categories solely. Second, achieving low regret for the setup in Section 5 with uncertain departure (i.e., = 1). Our approach fails in such a case since we cannot use belief-category walks; these are no longer deterministic. Consequently, the closed-form formula is much more complex and optimal planning becomes more intricate. These two challenges are left open for future work.
Acknowledgement
LL is generously supported by an Open Philanthropy AI Fellowship. LC is supported by Ariane de Rothschild Women Doctoral Program. ZL thanks the Block Center for Technology and Society; Amazon AI; PwC USA via the Digital Transformation and Innovation Center; and the NSF: Fair AI Award IIS2040929 for supporting ACMI lab’s research on the responsible use of machine learning. This project has received funding from the European Research Council (ERC) under the European Union’s Horizon 2020 research and innovation program (grant agreement No. 882396), by the Israel Science Foundation (grant number 993/17), Tel Aviv University Center for AI and Data Science (TAD), and the Yandex Initiative for Machine Learning at Tel Aviv University.
References
Abbasi-Yadkori, Y.; Pál, D.; and Szepesvári, C. 2011. Improved algorithms for linear stochastic bandits. Advances in neural information processing systems, 24: 2312–2320.
Azar, M. G.; Lazaric, A.; and Brunskill, E. 2013. Sequential transfer in multi-armed bandit with ﬁnite set of models. In Proceedings of the 26th International Conference on Neural Information Processing Systems-Volume 2, 2220–2228.
Bahar, G.; Ben-Porat, O.; Leyton-Brown, K.; and Tennenholtz, M. 2020. Fiduciary bandits. In International Conference on Machine Learning, 518–527. PMLR.
Bahar, G.; Smorodinsky, R.; and Tennenholtz, M. 2016. Economic Recommendation Systems: One Page Abstract. In Proceedings of the 2016 ACM Conference on Economics and Computation, EC ’16, 757–757. New York, NY, USA: ACM. ISBN 978-1-4503-3936-0.
Bubeck, S.; Cesa-Bianchi, N.; et al. 2012. Regret analysis of stochastic and nonstochastic multi-armed bandit problems. Foundations and Trends® in Machine Learning, 5(1): 1–122.
Cao, J.; Sun, W.; Shen, Z.-J. M.; and Ettl, M. 2020. FatigueAware Bandits for Dependent Click Models. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 34, 3341–3348.

Cesa-Bianchi, N.; and Lugosi, G. 2006. Prediction, learning, and games. Cambridge Univ Press.
Cohen, L.; and Mansour, Y. 2019. Optimal Algorithm for Bayesian Incentive-Compatible. In ACM Conf. on Economics and Computation (EC).
Eldar, Y.; and Kutyniok, G. 2012. Compressed Sensing: Theory and Applications. ISBN 978-1107005587.
Harper, F. M.; and Konstan, J. A. 2015. The MovieLens Datasets: History and Context. ACM Trans. Interact. Intell. Syst.
Hillar, C.; and Wibisono, A. 2018. Maximum entropy distributions on graphs. arXiv:1301.3321.
Jia, H.; Shi, C.; and Shen, S. 2021. Multi-armed Bandit with Sub-exponential Rewards. Operations Research Letters.
Joseph, M.; Kearns, M.; Morgenstern, J.; and Roth, A. 2016. Fairness in learning: Classic and contextual bandits. arXiv preprint arXiv:1605.07139.
Kaelbling, L. P.; Littman, M. L.; and Cassandra, A. R. 1998. Planning and acting in partially observable stochastic domains. Artiﬁcial intelligence, 101(1-2): 99–134.
Korda, N.; Szörényi, B.; and Li, S. 2016. Distributed Clustering of Linear Bandits in Peer to Peer Networks. In Proceedings of the 33rd International Conference on International Conference on Machine Learning.
Kremer, I.; Mansour, Y.; and Perry, M. 2014. Implementing the wisdom of the crowd. Journal of Political Economy, 122: 988–1012.
Lattimore, T.; and Szepesvári, C. 2020. Bandit algorithms. Cambridge University Press.
Leqi, L.; Kilinc-Karzan, F.; Lipton, Z. C.; and Montgomery, A. L. 2020. Rebounding Bandits for Modeling Satiation Effects. arXiv preprint arXiv:2011.06741.
Liu, Y.; and Ho, C.-J. 2018. Incentivizing high quality user contributions: New arm generation in bandit learning. In Thirty-Second AAAI Conference on Artiﬁcial Intelligence.
Lu, Z.; and Yang, Q. 2016. Partially Observable Markov Decision Process for Recommender Systems. CoRR, abs/1608.07793.
Mahadik, K.; Wu, Q.; Li, S.; and Sabne, A. 2020. Fast Distributed Bandits for Online Recommendation Systems.
Mansour, Y.; Slivkins, A.; and Syrgkanis, V. 2015. Bayesian Incentive-Compatible Bandit Exploration. In ACM Conf. on Economics and Computation (EC).
Patil, V.; Ghalme, G.; Nair, V.; and Narahari, Y. 2020. Achieving fairness in the stochastic multi-armed bandit problem. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 34, 5379–5386.
Pike-Burke, C.; and Grünewälder, S. 2019. Recovering bandits. arXiv preprint arXiv:1910.14354.
Ron, T.; Ben-Porat, O.; and Shalit, U. 2021. Corporate Social Responsibility via Multi-Armed Bandits. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, 26–40.
Shani, G.; Heckerman, D.; and Brafman, R. I. 2005. An MDP-Based Recommender System. Journal of Machine Learning Research, 6(43): 1265–1295.

Slivkins, A. 2019. Introduction to multi-armed bandits. arXiv preprint arXiv:1904.07272.
Zhao, X.; Zheng, X.; Yang, X.; Liu, X.; and Tang, J. 2020a. Jointly Learning to Recommend and Advertise. In KDD ’20: The 26th ACM SIGKDD Conference on Knowledge Discovery and Data Mining.
Zhao, Y.; Zhou, Y.; Ou, M.; Xu, H.; and Li, N. 2020b. Maximizing Cumulative User Engagement in Sequential Recommendation: An Online Optimization Perspective. In Gupta, R.; Liu, Y.; Tang, J.; and Prakash, B. A., eds., KDD ’20: The 26th ACM SIGKDD Conference on Knowledge Discovery and Data Mining.

A Extension: Planning Beyond Two User Types

In this section, we treat the planning task with two categories (K = 2) but potentially many types (i.e., M ≥ 2). For convenience, we formalize the results in this section in terms of M = 2, but the results are readily extendable for the more general 2×M case.

We derive an almost-optimal planning policy via dynamic programming, and then explain why it cannot be used for learning as

we did in the previous section. For reasons that will become apparent later on, we deﬁne by VHπ as the return of a policy π until the H’s iteration. Using
Theorem 5.3, we have that

H

E[VHπ(b)] =

b

·

Pm1,i
1,x

·

Pm2,i
2,x

+

(1

−

b)Pm 1,y1,i

·

Pm2,i
2,y

,

i=1

where m1,i := |{aj = 1, j ≤ i}| and m2,i := |{aj = 2, j ≤ i}| are calculated based on the belief-category walk b1, a1, b2, a2, . . . induced by π. Further, let π˜∗ denote the policy maximizing VH .
Notice that there is a bijection from H−iterations policies to (m1,i, m2,i)Hi=1; hence, we can ﬁnd π˜∗ by ﬁnding the arg max of the expression on the right-hand-side of the above equation, in terms of (m1,i, m2,i)Hi=1. Formally, we want to solve the integer linear programming (ILP),

H

maximize

b

·

Pm1,i
1,x

·

Pm2,i
2,x

+

(1

−

b)Pm 1,y1,i

·

Pm2,i
2,y

i=1

i

subject to ma,i = za,l for a ∈ {1, 2}, i ∈ [H],

(3)

l=1
za,i ∈ {0, 1} for a ∈ {1, 2}, i ∈ [H], z1,i + z2,i = 1 for i ∈ [H].

Despite that this problem involves integer programming, we can solve it using dynamic programming in O H2 runtime.
Notice that the optimization is over a subset of binary variables (z1,i, z2,i)Hi=1. Let ZH be the set of feasible solutions of the ILP, and similarly let Zh denote set of preﬁxes of length h ≤ H of ZH .
For any h ∈ [H] and z ∈ Zh, deﬁne

h

Dh(z) d=ef

b

·

Pm1,i
1,x

·

Pm2,i
2,x

+

(1

−

b)Pm 1,y1,i

·

Pm2,i
2,y

,

i=1

where ma,i =

i l=1

za,l

for

j

∈

{1,

2},

i

∈

[h]

as

in

the

ILP.

Consequently, solving the ILP is equivalent to maximizing DH over the domain ZH .

Next, for any h ∈ [H] and two integers c1, c2 such that c1 + c2 = h, deﬁne

D˜ h(c1, c2) =def max Dh(z).

(4)

z∈Z h ,

m1,h (z)=c1

m2,h (z)=c2

Under this construction, maxc1,c2 D˜ H (c1, c2) over c1, c2 such that c1 + c2 = H is precisely the value of the ILP. Reformulating Equation (4) for h > 1,

D˜ h(c1,c2)= max D˜ h−1(c1 −z1,c2 −z2)+α(c1,c2) , z1 ,z2 ∈{0,1} z1 +z2 =1

where α(m1, m2) d=ef b · xm 1 1 · xm 2 2 + (1 − b)y1m1 · y2m2 . For every h, there are only h + 1 possible values D˜ h can take: All the ways of dividing h into non-negative integers c1 and c2; therefore, having computed D˜ h−1 for all h feasible inputs, we can compute D˜ h(c1, c2) in O(h). Consequently, computing maxc1,c2 D˜ H (c1, c2), which is precisely the value of the ILP in (3), takes O(H2) run-time. Moreover, the policy π˜∗ can be found using backtracking. We remark that an argument similar to

Lemma

5.8

implies

that

E[V

π∗

−

V

π˜∗ ]

≤

1 2O(H)

;

hence,

π˜∗

is

almost

optimal.

To ﬁnalize this section, we remark that this approach could also work for K > 2 categories. Naively, for a ﬁnite horizon H,

there are KH possible policies. The dynamic programming procedure explain above makes the search operate in run-time of

O(HK ). The run-time, exponential in the number of categories but polynomial in the horizon, is feasible when the number of

categories is small.

B Experimental Evaluation
For general real-world datasets, we propose a scheme to construct semi-synthetic problem instances with many arms and many user types, using rating data sets with multiple ratings per user. We exemplify our scheme on the MovieLens Dataset Harper and Konstan (2015). As a pre-processing step, we set movie genres to be the categories of interest, select a subset of categories |A| of size k (e.g., sci-ﬁ, drama, and comedy), and select the number of user types, m. Remove any user who has not provided a rating for at least one movie from each category a ∈ A. When running the algorithm, randomly draw users from the data, and given a recommended category a, suggest them a random movie which they have rated, and set their click probability to 1 − r, where r ∈ [0, 1] is their normalized rating of the suggested movie.

C UCB Policy for Sub-exponential Returns

An important tool for analyzing sub-exponential random variables is Bernstein’s Inequality, which is a concentration inequality for sub-exponential random variables (see, e.g., Jia, Shi, and Shen (2021)). Being a major component of the regret analysis for Algorithm 2, we state it here for convenience.

Lemma C.1. v ≥ 0:

(Bernstein’s Inequality) Let a random variable X be sub-exponential with parameters (τ 2, b). Then for every

Pr[|X − E[X]| ≥ v] ≤

2

exp(−

v2 2τ 2

)

2

e

x

p(−

v 2b

)

v

≤

τ2 b

.

else

D Single User Type: Proofs from Section 4

To simplify the proofs, we use the following notation: For a ﬁxed-arm policy πa, we use Vjπa to denote its return from iteration j until the user departs. Namely,

N πa

Vjπa =

Pa

i=j

Throughout this section, we will use the following Observation.

Observation D.1. For every policy π and iteration j,

E[Vjπ] = Pπj (1 + E[Vjπ+1]) + (1 − πj )(1 − Pπj )E[Vjπ+1] = E[Vjπ+1](1 − πj (1 − Pπj )) + Pπj .

Lemma 4.1. A policy πa∗ is optimal if

a∗ ∈ arg max Pa . a∈[K] a(1 − Pa)

Proof. First, recall that every POMDP has an optimal Markovian policy which is deterministic (we refer the reader to Sec-
tion 5.1 for full formulation of the problem as POMDP). Having independent rewards and a single state implies that there exists µ∗ ∈ N such that E[Vj∗] = µ∗ for every j ∈ N (similarly to standard MAB problems, there exists a ﬁxed-arm policy which is optimal). Assume by contradiction that the optimal policy πa∗ holds

a∗ ∈/ arg max Pa . a∈[k] a(1 − Pa)

Now, notice that

E[V πa′ ] = E[V1πa′ ] = E[V2πa′ ](1 − a′ (1 − Pa′ )) + Pa′

Solving the recurrence relation and summing the geometric series we get

∞ πa′

j

Pa′

E[V ] = Pa′ (1 − a′ (1 − Pa′ )) = a′ (1 − Pa′ ) .

j=0

Finally, yields that any ﬁxed-armed policy, πa′ such that

a∗ ∈/ arg max Pa , a∈[k] a(1 − Pa)

a′ ∈ arg max Pa a∈[k] a(1 − Pa)

holds E[V πa′ ] > E[V πa∗ ], a contradiction to the optimality of πa∗ .

Lemma D.2. For each a ∈ [k], the centered random return V πa − E[V πa] is sub-exponential with parameter C2 = −4/ ln(1 − a(1 − Pa)).
In order to show that returns of ﬁxed-arm policies are sub-exponential random variables, we ﬁrst show that the number of iterations of users recommended by ﬁxed-arm policies is also a sub-exponential. For this purpose, we state here a lemma that implies that every geometric r.v. is a sub-exponential r.v.. The proof of the next lemma appears, e.g., in Hillar and Wibisono (2018) (Lemma 4.3).
Lemma D.3. Let X be a geometric random variable with parameter r ∈ (0, 1), so that:

Pr[X = x] = (1 − r)x−1 r, x ∈ N.

Then X satisﬁes Property (2) from Deﬁnition 3.1. Namely, X is sub-exponential with parameter C2 = −2/ ln(1 − r). Formally, ∀p ≥ 0 : (E[|X|p])1/p ≤ − 2 p. ln(1 − r)

The lemma above and Observation 4.2 allow us to deduce that the variables Na are sub-exponential in the ﬁrst part of the following Corollary (the case in which a = 0 follows immediately from deﬁnition.). The second part of the lemma follows
directly from the equivalences between Properties (2) and (1) in Deﬁnition 3.1.

Corollary D.4. For each a ∈ [K], the number of iterations a user recommended by πa stays within the system, Na, is sub-

exponential with parameter C2a = −2/ ln(1 − a(1 − Pa)). In addition, there exist constants C1a > 0 for every a ∈ [K] such

that

v

∀a ∈ [K], v ≥ 0 : Pr[|Na| > v] ≤ exp(1 − Ca ).

1

The next Proposition D.5 is used for the proof of Lemma D.2.

Proposition D.5. For every a ∈ [K],

|E[V πa ]| ≤

−2

ln(1 − a(1 − Pa))

Proof. First, notice that

(1 − a(1 − Pa)) ln(1 − a(1 − Pa)) > (1 − a(1 − Pa)) −a(1 − Pa) = −a(1 − Pa) > −2a(1 − Pa), 1 − a(1 − Pa)

where

the

ﬁrst

inequality

is

due

to

x 1+x

≤

ln(1

+

x)

for

every

x

≥

−1.

Rearranging,

1 − a(1 − Pa) <

−2 .

(5)

a(1 − Pa)

ln(1 − a(1 − Pa))

For each user, the realization of V πa is less or equal to the realization of Na − 1 for the same user (as users provide negative feedback in their last iteration); hence,

|E[V πa ]| = E[V πa ] ≤ E[Na] − 1 = 1 − 1 = 1 − a(1 − Pa) <

−2 .

a(1 − Pa)

a(1 − Pa)

ln(1 − a(1 − Pa))

We proceed by showing that returns of ﬁx-armed policies satisfy Property (1) from Deﬁnition 3.1.
Lemma D.2. For each a ∈ [k], the centered random return V πa − E[V πa] is sub-exponential with parameter C2 = −4/ ln(1 − a(1 − Pa)).

Proof. We use Property (1) from Deﬁnition 3.1 to derive that V πa is also sub-exponential. This is true since the tails of V πa satisfy that for all v ≥ 0,
Pr[|V πa| > v] ≤ Pr[|Na| > v + 1] ≤ Pr[|Na| > v] ≤(1) exp(1 − Cv1 ),
where the ﬁrst inequality follows since |Na| > v + 1 is a necessary condition for |V πa | > v, and the last inequality follows from Corollary D.4. Along with Deﬁnition 3.1, we conclude that

E[|V πa |p]1/p ≤ −2/ ln(1 − a(1 − Pa))p.

(6)

Now, applying Minkowski’s inequality and then Jensen’s inequality (as f (z) = zp, g(z) = |z| are convex for every p ≥ 1) we get
(E[|V πa − E[V πa ]|p])1/p ≤ E[|V πa |p]1/p + E[E[|V πa |]p]1/p ≤ E[|V πa |p]1/p + |E[V πa ]|.

Using Proposition D.5 and Inequality (6), we get

E[|V πa |p]1/p + |E[V πa ]| ≤

−2

+ 1 −1≤

−4

ln(1 − a(1 − Pa)) a(1 − Pa)

ln(1 − a(1 − Pa))

Hence V πa − E[V πa] is sub-exponential with parameter C2 = −4/ ln(1 − a(1 − Pa)).

Lemma 4.3. For each category a ∈ [K], the centred random variable V πa − E[V πa ] is sub-exponential with parameters

(τa2, ba), such that

τa = ba = −

8e

.

ln(1 − a(1 − Pa))

Proof. Throughout this proof, we will use the sub-exponential norm, || · ||ψ1 , which is deﬁned as

(E[|Z |p ])1/p

||Z||ψ1 = sup
p≥1

p

.

Let

V πa − E[V πa ] X = ||V πa − E[V πa ]||ψ ,

y = γ · ||V πa − E[V πa ]||ψ1 .

1

We have that V πa − E[V πa ]
||X||ψ1 = || ||V πa − E[V πa ]||ψ1 ||ψ1 = 1. (7)

Let

γ

be

such

that

|γ|

<

1/ba

=

−

l

n(

1−

a (1− 8e

Pa

)

)

.

From

Lemma

D.2

we

conclude

that

|γ| =

y

≤ − ln(1 − a(1 − Pa)) = 1 ·

1;

||V πa − E[V πa ]||ψ1

8e

2e ||V πa − E[V πa ]||ψ1

hence, |y| < 21e .

Summing the geometric series, we get

∞
(e|y|)p =

e2y2

< 2e2y2

(8)

p=2 1 − e|y|

In addition, notice that yX = γ(V πa − E[V πa]). Next, from the Taylor series of exp(·) we have

E[exp(γ(V πa − E[V πa]))] = E[exp(yX)] = 1 + yE[x] + ∞ ypE[Xp] . p=2 p!

Combining the fact that E[X] = 0 and (7) to the above,

∞ ypE[Xp]

∞ yppp

1 + yE[x] +

p! ≤ 1 + p! .

p=2

p=2

By

applying

p!

≥

(

p e

)p

and

(8),

we

get

1 + ∞ yppp! p ≤ 1 + ∞ (e|y|)p ≤ 1 + 2e2y2 ≤ exp(2e2y2) = exp(2e2(γ · ||V πa − E[V πa ]||ψ1 )2),

p=2

p=2

where the last inequality is due to 1 + x ≤ ex. Note that ||V πa − E[V πa ]||ψ1 ≤ − ln(1−a(41−Pa)) )2). Ultimately,

E[exp(γ(V πa − E[V πa ]))] ≤ exp 2e2γ2(−

4

)2 = exp 1 γ2(−

8e

)2 .

ln(1 − a(1 − Pa))

2

ln(1 − a(1 − Pa))

This concludes the proof of the lemma.

E Two User Types and Two Categories: Proofs from Section 5

E.1 Planning when K = 2 Theorem 5.3. For every policy π and an initial belief b ∈ [0, 1], the expected return is given by

∞

E[V π(b)] =

b

·

Pm1,i
1,x

·

Pm2,i
2,x

+

(1

−

b)Pm 1,y1,i

·

Pm2,i
2,y

,

i=1

where m1,i := |{aj = 1, j ≤ i}| and m2,i := |{aj = 2, j ≤ i}| are calculated based on the belief-category walk b1, a1, b2, a2, . . . induced by π.

Proof.

Let

βiπ (b)

:=

b

·

P m1,i
1,x

·

P m2,i
2,x

+

(1

−

b)P1,y m1,i

·

P2,y m2,i .

We

will

prove

that

for

every

policy

π

and

every

belief

b, we have that E[VHπa(b)] =

H i=1

βiπ

(b)

by

a

backward

induction

over

H

.

For the base case, consider H = 1. We have that

E[V1π (b1 )]

=

b1

· Pa1,x

+

(1

−

b)Pa1 ,y

=

b

·

P m1,1
1,x

·

P m2,1
2,x

+

(1

−

b)P1,y m1,1

·

P m2,1
2,y

=

β1π (b)

as ma,1 = I[a1 = a].

For the inductive step, assume that E[VHπ−1(b)] =

H i=1

βiπ (b)

for

every

b

∈

[0,

1].

Indeed,

H−1 i=1

βiπ

(b)

for

every

b

∈

[0, 1].

We

need

to

show

that

E[VHπ (b)]

=

E[VHπ(b1)] = β1π(b1)(1 + E[VHπ−1(b′(b1, a1, liked))]) = β1π(b1)(1 + E[VHπ−1(b2)])
H−1
= β1π(b1)(1 + βiπ(b2))
i=2
H
= βiπ(b1),
i=1

where the second to last equality is due to the induction hypothesis and the assumption that π is a deterministic stationary policy.

The proof completes by realizing that E[V π(b)] = limH→∞ E[VHπ(b)] = limH→∞ is ﬁnite and has positive summands.

H i=1

βiπ (b)

=

∞ i=1

βiπ (b),

since

the

sum

E.2 Dominant Row (DR) Lemma 5.4. For any instance such that P has a dominant row a, the ﬁxed policy πa is an optimal policy.

Proof. We will show that for every iteration j, no matter what were the previous topic recommendations were, selecting topic
1 rather than topic 2 can only increase the value. Let π be a stationary policy such that π(bj) = 2. Changing it into a policy πj that is equivalent to π for all iterations but
iteration j + 1 in which it recommends topic 1 can only improve the value.
Since P1,x, P2,x, P1,y, P2,y ≥ 0, P1,x − P2,x ≥ 0, b, 1 − b ≥ 0 and this structure satisﬁes P2,y − P1,y ≤ 0, we get that for every m¯ 1,j, m¯ 2,j, n1,j, n2,j ∈ N and for every b,

b

·

Pm¯ 1,j +n1,j
1,x

·

Pm¯ 2,j
2,x

+

n2,j

(P1,

x

−

P2,x)

≥

(1

−

b)P1m¯,y1,j +n1,j

·

Pm¯ 2,j
2,y

+n2,j

(P2,

y

−

P1,y );

thus,

b

·

Pm¯ 1,j +1+n1,j
1,x

·

Pm¯ 2,j +n2,j
2,x

+ (1 − b)Pm 1¯,y1,j +1+n1,j

·

Pm2,j
2,y

+n2,j

≥

b

·

Pm¯ 1,j
1,x

+n1,j

·

Pm¯ 2,j +1+n2,j
2,x

+

(1

−

b)Pm 1¯,y1,j +n1,j

·

Pm¯ 2,j
2,y

+1+n2,j

.

Hence for every time step j + 1, choosing topic 1 instead of topic 2 leads to increased value of each of the summation element

b

·

Pm1,i
1,x

·

Pm2,i
2,x

+ (1 − b)Pm 1,y1,i

·

Pm2,i
2,y

such

that

m1,i

=

m¯ 1,j

+ n1,j

≥

m¯ 1,j

and

m2,i

=

m¯ 2,j

+ n2,j

≥

m¯ 2,j.

We

deduce

that

E[V πj (b)] ≥ E[V π(b)].

E.3 Dominant Column (DC)

Before proving the main theorem (Theorem 5.5), we prove two auxiliary lemmas.

Lemma E.1. For P with a DC structure, if a policy π is optimal then it recommends topic 1 for all iteration j′ ≥ j + 1 such that

∞ m1,i m2,i

∞ 1 − b P2,y − P1,y P2,x m1,i m2,i

P1,x P2,x >

b · P1,x − P2,x · P2,y P1,y P2,y .

(9)

i=j+1

i=j+1

Proof. First, assume by contradiction that there exists an optimal policy π that recommends topic 2 in iteration j + 1 such that (9) holds.
Let πj be the policy that is equivalent to π but recommend topic 1 instead of topic 2 in iteration j + 1. Since π and πj recommends the same topic until iteration j, along with the optimality of π, we have

E[V πj (b)] − E[V π(b)] = E[Vjπ+j1(b)] − E[Vjπ+1(b)] ≤ 0.

Expending the above equation,

∞
b · P1m,xπ1,i+1 · Pm 2,xπ2,i−1 + (1 − b)Pm 1,yπ1,i+1 · Pm 2,yπ2,i−1 −
i=j+1

∞
b · Pm 1,xπ1,i · Pm 2,xπ2,i + (1 − b)Pm 1,yπ1,i · Pm 2,yπ2,i
i=j+1

≤0

∞

mπ

mπ P1,x

∞

mπ

mπ

P1,y

b

·

P 1,i
1,x

·

P 2,i
2,x

(

P2,x

− 1)

≤

(1

−

b)P1,y1,i

·

P2,

2,i
y

(1

−

P2,y

)

i=j+1

i=j+1

b(P1,x − P2,x) ∞

mπ

mπ

(1 − b)(P2,y − P1,y) ∞ mπ

mπ

P2,x

·P1,x1,i

·

P 2,i
2,x

≤

P2,y

P 1,i
1,y

·

P 2,i
2,y

i=j+1

i=j+1

∞ mπ

mπ

1 − b P2,x P2,y − P1,y ∞ mπ

mπ

P 1,i
1,x

·

P 2,i
2,x

≤

b

· P2,y · P1,x − P2,x

P 1,i
1,y

·

P2,

2,i
y

,

i=j+1

i=j+1

which is a contradiction to (9). For the second part of the lemma, assume that condition (9) holds for some iteration j + 1 ∈ N and some optimal policy π;
hence, π(b, mπ1,j , mπ2,j ) = 1 and we have mπ1,j+1 = mπ1,j + 1 and mπ2,j+1 = mπ2,j . Exploiting this fact, we have that

∞ i=j+2

P P mπ1,i
1,x

mπ2,i 2,x

∞
=
i=j+1

P P mπ1,i+1
1,x

mπ2,i 2,x

∞
= P1,x
i=j+1

P P mπ1,i
1,x

mπ2,i 2,x

> (9),

implying

∞ 1 − b P2,y − P1,y P2,x mπ mπ

P1,x

b

·

P1,x

− P2,x

·

P2,y

P 1,i
1,y

P 2,i
2,y

i=j+1

∞ 1 − b P2,y − P1,y P2,x mπ mπ

>(P1,x ≥ P1,y)P1,y

b

·

P1,x

− P2,x

·

P2,y

P 1,i
1,y

P 2,i
2,y

i=j+1

∞ 1 − b P2,y − P1,y P2,x mπ +1 mπ

=

b

·

P1,x

− P2,x

·

P2,y

P 1,i
1,y

P 2,i
2,y

i=j+1

∞ 1 − b P2,y − P1,y P2,x mπ mπ

=

b

·

P1,x

− P2,x

·

P2,y

P 1,i
1,y

P 2,i
2,y

.

i=j+2

An immediate consequence of Lemma E.1 is the following corollary. Corollary E.2. For any DC-structured P and every belief b ∈ [0, 1], the optimal policy π ﬁrst recommends topic 2 for at most

N mπ

1 − b P2,y − P1,y P2,x N mπ

argminN

P 2,i
2,x

>

b

· P1,x − P2,x · P2,y

P 2,i
2,y

i=1

i=1

times, and then recommends topic 1 permanently. In addition, N ∈ N since P2,x > P2,y.

Theorem 5.5. For any instance such that P has a dominant column, one of the following four policies is optimal:

π1, π2, π2:⌊N∗⌋, π2:⌈N∗⌉,
where N ∗ = N ∗(P, q) is a constant, and π2:⌊N∗⌋ (π2:⌈N∗⌉) stands for recommending Category 2 until iteration ⌊N ∗⌋ (⌈N ∗⌉) and then switching to Category 1.

Proof. Due to Theorem 5.3 and Corollary E.2, we can write the expected value of a policy as a function of N when P has a DC structure:

∞

E[V πN (b)] =

b

·

Pm1,i
1,x

·

Pm2,i
2,x

+

(1

−

b)Pm 1,y1,i

·

Pm2,i
2,y

i=1

N

∞

= b · Pi2,x + (1 − b)Pi2,y +

b · PN2,x · Pi1−,xN + (1 − b)PN2,y · Pi1−,yN

i=1

i=N +1

P2,x(PN2,x − 1)

P2,y(PN2,y − 1)

∞

N

i

∞

N

i

= b · P2,x − 1 + (1 − b) · P2,y − 1 + b · P2,x · P1,x + (1 − b)P2,y · P1,y

i=1

i=1

= b · P2,x(PN2,x − 1) + (1 − b) · P2,y(PN2,y − 1) + b · PN · P1,x + (1 − b)PN P1,y

P2,x − 1

P2,y − 1

2,x 1 − P1,x

2,y 1 − P1,y

= PN · b P2,x + P1,x + PN (1 − b) P2,y + P1,y + bP2,x + (1 − b)P2,y . (10)

2,x P2,x − 1 1 − P1,x

2,y

P2,y − 1 1 − P1,y 1 − P2,x 1 − P2,y

Equation (10) could be cast as c1 ·PN2,x +c2PN2,y +c3(P2,x, P2,y) for positive c1, negative c2 and positive c3. Let f : R ← R be the continuous function such that f (N ) = c1 · PN2,x + c2PN2,y + c3(P2,x, P2,y).
We take the derivative w.r.t. N to ﬁnd the saddle point of f :

ddN f = c1 · ln P2,x · PN2,x + c2 ln P2,y · PN2,y = 0,

which suggests the saddle point of f is

N˜ = ln

− c2 ln P2,y
c1 ln P2,x

.

ln P2,x
P2,y

Next, set N ∗ d=ef max{0, N˜ }. Since f has a single saddle point and for every n ∈ N it holds that f (N ) = E[V πN (b)], to determine the optimal policy, one only needs to compare the value E[V πN (b)] at the boundary points (N = 0, N = ∞) and at the closest integers to the saddle point (N = ⌊N ∗⌋, N = ⌈N ∗⌉).

E.4 Dominant Diagonal (SD) Theorem 5.6. For any instance such that P has a dominant diagonal, either π1 or π2 is optimal.

Proof. We prove the following claim by a backward induction over the number of iterations remaining: For every k = H − 1, . . . 1 it holds that for every policy π and belief b,

E[Vkπ(b)] ≤ max{E[Vkπ1 (b)], E[Vkπ2 (b)]}.

First, we notice that when k = H − 1, the only possible policies are π1 and π2. For k = H − 2, we prove the statement by contradiction. There are only two ways to selects topics when k = H − 2:

π1′ :H = (π1:H−2, 1 , 2 ) and π1′′:H = (π1:H−2, 2 , 1 ).

H−1 H

H−1 H

Let m1 and m2 denote the number of times π has played topic 1 and 2 till time H − 2, inclusive. Assume that the policy π′ is optimal. In particular, it holds that E[Vkπ1] ≤ E[Vkπ′ ] and E[Vkπ2 ] ≤ E[Vkπ′]. We get

bPm 1,x1 Pm 2,x2 P1,x(P1,x − P2,x) ≤ Pm 1,y1 Pm 2,y2 (1 − b)P1,y(P2,y − P1,y),

(11)

and

Pm 1,y1 Pm 2,y2 (1 − b)(P2,y − P1,y)(1 + P2,y) ≤ bPm 1,x1 Pm 2,x2 (P1,x − P2,x)(1 + P2,x).

(12)

As both sides of (11) and (12) are positive, the multiplication of their left hand sides is smaller than the multiplication of their right hand sides, i.e.,
bPm 1,x1 Pm 2,x2 P1,x(P1,x − P2,x)Pm 1,y1 Pm 2,y2 (1 − b)(P2,y − P1,y)(1 + P2,y) ≤ Pm 1,y1 Pm 2,y2 (1 − b)P1,y(P2,y − P1,y)bPm 1,x1 Pm 2,x2 (P1,x − P2,x)(1 + P2,x)
Dividing both sides by bPm 1,x1 Pm 2,x2 (P1,x − P2,x)Pm 1,y1 Pm 2,y2 (1 − b)(P2,y − P1,y) > 0, we obtain

P1,x(1 + P2,y) ≤ P1,y(1 + P2,x),

which is a contradiction as P1,x > P1,y and 1 + P2,y > 1 + P2,x. Now, assume that the policy π′′ is optimal. In particular, it holds that E[Vkπ1 ] ≤ E[Vkπ′′ ] and E[Vkπ2 ] ≤ E[Vkπ′′ ]. We get

Pm 1,x1 Pm 2,x2 b(P1,x − P2,x)(1 + P1,x) ≤ Pm 1,y1 Pm 2,y2 (1 − b)(1 + P1,y)(P2,y − P1,y),

(13)

and

Pm 1,y1 Pm 2,y2 (1 − b)P2,y(P2,y − P1,y) ≤ Pm 1,x1 Pm 2,x2 bP2,x(P1,x − P2,x).

(14)

As both sides of (13) and (14) are positive, the multiplication of their left hand sides is smaller than the multiplication of their right hand sides,
Pm 1,x1 Pm 2,x2 b(P1,x − P2,x)(1 + P1,x)Pm 1,y1 Pm 2,y2 (1 − b)P2,y(P2,y − P1,y) ≤ Pm 1,y1 Pm 2,y2 (1 − b)(1 + P1,y)(P2,y − P1,y)Pm 1,x1 Pm 2,x2 bP2,x(P1,x − P2,x).
Dividing both sides by Pm 1,x1 Pm 2,x2 b(P1,x − P2,x)Pm 1,y1 Pm 2,y2 (1 − b)(P2,y − P1,y) > 0, we obtain

P2,y(1 + P1,x) ≤ P2,x(1 + P1,y),

which is again, a contradiction as P2,x < P2,y and 1 + P1,y < 1 + P1,x. For H ≥ 3, we prove the statement by contradiction. Suppose not, i.e., the optimal policy π switch recommended topic at
least once. Let t denote the time step where π switch for the last time. We ﬁrst consider the case where π has switched from topic 2 to topic 1 at time t. More speciﬁcally, we have

π1:H = (π1:t−2, 2 , 1 , 1, . . . , 1, 1 ).
πt−1 πt πt+1:H−1 πH

Consider another policy π˜ (that behaves the same as π except at time step t − 1) deﬁned as

π˜1:H = (π1:t−2, 2 , 2 , 1, . . . , 1, 1 ).
πt−1 πt πt+1:H−1 πH

Let m1 and m2 denote the number of times π has recommended topic 1 and 2 till (and include) time t − 1. Since π is optimal, we have the difference between the value of π and π˜ to be non-negative, i.e.,

H−t+1

E[VHπ] − E[VHπ˜ ] =

bPm 1,x1+i−1Pm 2,x2+1(P1,x − P2,x) + (1 − b)Pm 1,y1+i−1Pm 2,y2+1(P1,y − P2,y) ≥ 0,

i=1

(15)

where the difference is induced by the discrepancy of the two policies from time step t to H. Consider another policy π′ (that behaves the same as π except at time step H) deﬁned as

π1′ :H = (π1:t−2, 2 , 1 , 1, . . . , 1, 2 ).
πt−1 πt πt+1:H−1 πH

Since π is optimal, we have the difference between the value of π and π′ to be non-negative, i.e.,

E[VHπ] > E[VHπ′ ] ⇒ bPm 1,x1+H−tPm 2,x2 (P1,x − P2,x) > (1 − b)Pm 1,y1+H−tPm 2,y2 (P2,y − P1,y),
where the difference is induced by the discrepancy of the two policies from time step H. Multiplying both sides by P1,y > 0, we get
P1,ybPm 1,x1+H−tPm 2,x2 (P1,x − P2,x) > (1 − b)P1m,y1+H−t+1Pm 2,y2 (P2,y − P1,y).
Using PP11,,xy > 1, and P1,ybPm 1,x1+H−tPm 2,x2 (P1,x − P2,x) > 0,

bPm 1,x1+H−t+1Pm 2,x2 (P1,x − P2,x) > (1 − b)P1m,y1+H−t+1Pm 2,y2 (P2,y − P1,y);

hence,

bPm 1,x1+H−t+1Pm 2,x2 (P1,x − P2,x) + (1 − b)P1m,y1+H−t+1Pm 2,y2 (P1,y − P2,y) ≥ 0.

(16)

Next, we construct a new policy πnew that outperforms π. We let πnew to be the policy deﬁned as below

π1ne:Hw = (π1:t−2, 1 , 1 , 1, . . . , 1, 1 ).
πt−1 πt πt+1:H−1 πH

The value difference between πnew and π (caused by the discrepancy of the two policies from time t − 1 to H) is

H−t+1

E[VHπnew ] − E[VHπ] =

bP1m,x1+i−1Pm 2,x2 (P1,x − P2,x) + (1 − b)Pm 1,y1+i−1Pm 2,y2 (P1,y − P2,y)

i=1

+ bPm 1,x1+H−t+1Pm 2,x2 (P1,x − P2,x) + (1 − b)Pm 1,y1+H−t+1Pm 2,y2 (P1,y − P2,y)

H−t+1

>

bP1m,x1+i−1Pm 2,x2+1(P1,x − P2,x) + (1 − b)Pm 1,y1+i−1Pm 2,y2+1(P1,y − P2,y)

i=1

≥ 0,

where the ﬁrst inequality is true because P2,x < P2,y, P1,x − P2,x > 0 and P1,y − P2,y < 0, therefore for every 1 ≤ i ≤ H −t+1
bPm 1,x1+i−1Pm 2,x2 (P1,x − P2,x)(1 − P2,x) > 0 > (1 − b)Pm 1,y1+i−1Pm 2,y2 (P2,y − P1,y)(P2,y − 1)
along with (16). The second inequality follows from (15). Thus, we have successfully ﬁnd another policy π1ne:Hw that differs from π and achieves a higher value, which is a contradiction.
next, we consider the case where π has switched from topic 1 to topic 2 at time t, i.e.,

π1:H = (π1:t−2, 1 , 2 , 2, . . . , 2, 2 ).
πt−1 πt πt+1:H−1 πH

Consider another policy π˜ (that behaves the same as π except at time step t) deﬁned as

π˜1:H = (π1:t−2, 1 , 1 , 2, . . . , 2, 2 ).
πt−1 πt πt+1:H−1 πH

Since π is optimal, we have the difference between the value of π and π˜ to be non-negative, i.e.,

H−t+1

E[VHπ] − E[VHπ˜ ] =

bPm 1,x1+1P2m,x2+i−1(P2,x − P1,x) + (1 − b)Pm 1,y1+1Pm 2,y2+i−1(P2,y − P1,y) ≥ 0,

i=1

(17)

where the difference follows from the discrepancy between the two policies from time step t to H. Consider another policy π′ (that behaves the same as π except at time step H) deﬁned as
π1′ :H = (π1:t−2, 1 , 2 , 2, . . . , 2, 1 ).
πt−1 πt πt+1:H−1 πH

Since π is optimal, we have the difference between the value of π and π′ to be non-negative, i.e.,

E[VHπ] > E[VHπ′ ] ⇒ (1 − b)Pm 1,y1 P2m,y2+H−t(P2,y − P1,y) ≥ bPm 1,x1 Pm 2,x2+H−t(P1,x − P2,x), where the difference is induced by the discrepancy of the two policies from time step H. Multiplying both sides by P2,x > 0,

P2,x(1 − b)Pm 1,y1 P2m,y2+H−t(P2,y − P1,y) ≥ bPm 1,x1 P2m,x2+H−t+1(P1,x − P2,x).

Using P2,x(1 − b)Pm 1,y1 Pm 2,y2+H−t(P2,y − P1,y) > 0 and PP22,,xy ≥ 1, we get

(1 − b)Pm 1,y1 P2m,y2+H−t+1(P2,y − P1,y) ≥ bPm 1,x1 P2m,x2+H−t+1(P1,x − P2,x);

hence,

bPm 1,x1 P2m,x2+H−t+1(P2,x − P1,x) + (1 − b)Pm 1,y1 P2m,y2+H−t+1(P2,y − P1,y) ≥ 0.

(18)

Again, we will construct a new policy πnew that outperforms π. We let πnew to be the policy deﬁned as below
π1ne:Hw = (π1:t−2, 2 , 2 , 2, . . . , 2, 1 ).
πt−1 πt πt+1:H−1 πH

Now, the value difference between πnew and π (caused by the discrepancy of the two policies from time t − 1 to H) is

H−t+1
E[VHπnew ] − E[VHπ] =
i=1

bPm 1,x1 Pm 2,x2+i−1(P2,x − P1,x) + (1 − b)Pm 1,y1 P2m,y2+i−1(P2,y − P1,y)

+ bPm 1,x1 Pm 2,x2+H−t+1(P2,x − P1,x) + (1 − b)Pm 1,y1 Pm 2,y2+i−1(P2,y − P1,y)

H−t+1

>

bPm 1,x1+1Pm 2,x2+i−1(P2,x − P1,x) + (1 − b)Pm 1,y1+1P2m,y2+i−1(P2,y − P1,y)

i=1

≥ 0,

where the ﬁrst inequality is true because P1,y < P1,x, P2,x − P1,x < 0 and P2,y − P1,y > 0 and (18), and the second from (17). Similarly, we have successfully ﬁnd another policy π1ne:Hw that differs from π and achieves a higher value, which is a contradiction.
We have covered all cases, so the inductive argument holds. This concludes the proof of the theorem.

E.5 UCB-based regret bound

Lemma 5.9.

Let τ˜

=

8e ln( 1

)

and η

=

1. For every threshold policy π

∈

ΠH , the centred random variable V π − E[V π] is

1−ǫ

(τ 2, b)-sub-exponential with (τ 2, b) satisfying τ˜ ≥ τ and η ≥ b2/τ 2.

Proof. Let γ be such that

|γ| < − ln(1 − ǫ) ≤ min {− ln(1 − a,i(1 − Pa,i)) } = min {− ln(Pa,i) }.

8e

a∈{1,2},i∈{x,y}

8e

a∈{1,2},i∈{x,y}

8e

Next, we have that

E[exp(γ(V π − E[V π]))] ≤

E[exp(γ(V πa − E[V πa ]))] type(t) ∈ arg max Pa,i] · Pr[type(t) ∈ arg max Pa,i] ≤

a∈{1,2}

i∈[1,2]

i∈[1,2]

Where V¯ πa is the return for the instance Finally, from Lemma 4.3 we get

max {E[exp(γ(V¯ πa − E[V¯ πa ]))]}
a∈{1,2}
[1], [2], q, P¯ ,¯ such that for every a ∈ {1, 2}: P¯ a,1 = maxi∈{x,y} Pa,i and a,1 = 1.

max {E[exp(γ(V¯ πa − E[V¯ πa]))]} ≤ max exp((− 8e )2 γ2 ) = max exp((− 8e )2 γ2 ).

a∈{1,2}

a∈{1,2}

ln(P¯ a,1) 2

a∈{1,2},i∈{x,y}

ln(Pa,i) 2

Choosing completes the proof as

τ = b = max − 8e a∈{1,2},i∈{x,y} ln(Pa,i)

8e

8e

τ2

max −

≤−

= τ˜

a∈{1,2},i∈{x,y} ln(Pa,i)

ln(1 − ǫ)

and

b2 = 1 = η.

Lemma 5.8. For every H ∈ N, it holds that

E V π∗ − max V π
π∈ΠH

1 ≤ 2O(H) .

Proof. Recall that V π =

Nπ j=1

rj

(πj

),

where

we

drop

the

dependence

on

the

user

index

for

readability.

Formulating

differently,

for any H ∈ N it holds that

H

∞

V π = Ij≤Nπ · rj (πj ) +

Ij≤Nπ · rj (πj ).

j=1

j=H+1

Using the same representation for V π′ and taking expectation, we get that

E V π − V π′







H

H

∞

≤ E  Ij≤Nπ · rj (πj ) − Ij≤Nπ′ · rj (πj′ ) + E 

Ij≤Nπ · rj (πj )

j=1

j=1

j=H+1


∞


∞

≤ 0+E

Ij≤Nπ · rj (πj ) =

Pr (j ≤ N π) rj(πj )

j=H+1

j=H+1

∞

∞

≤

(1 − ǫ)j(1 − ǫ) = (1 − ǫ)H+2 (1 − ǫ)j

j=H+1

j=0

≤ (1 − ǫ)H 1 ≤ e−ǫH = 1 .

ǫ

ǫ

2O(H)

