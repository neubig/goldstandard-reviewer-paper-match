arXiv:2104.08977v2 [cs.LG] 29 Jun 2021

Oﬀ-Policy Risk Assessment in Contextual Bandits
Audrey Huang1, Liu Leqi1, Zachary C. Lipton1, and Kamyar Azizzadenesheli2
audreyh@andrew.cmu.edu,leqil@cs.cmu.edu,zlipton@cmu.edu,kamyar@purdue.edu 1Machine Learning Department, Carnegie Mellon University 2Department of Computer Science, Purdue University
Abstract
Even when unable to run experiments, practitioners can evaluate prospective policies, using previously logged data. However, while the bandits literature has adopted a diverse set of objectives, most research on oﬀ-policy evaluation to date focuses on the expected reward. In this paper, we introduce Lipschitz risk functionals, a broad class of objectives that subsumes conditional value-at-risk (CVaR), variance, mean-variance, many distorted risks, and CPT risks, among others. We propose Oﬀ-Policy Risk Assessment (OPRA), a framework that ﬁrst estimates a target policy’s CDF and then generates plugin estimates for any collection of Lipschitz risks, providing ﬁnite sample guarantees that hold simultaneously over the entire class. We instantiate OPRA with both importance sampling and doubly robust estimators. Our primary theoretical contributions are (i) the ﬁrst uniform concentration inequalities for both CDF estimators in contextual ba√ndits and (ii) error bounds on our Lipschitz risk estimates, which all converge at a rate of O(1/ n).
1 Introduction
Many practical tasks, including medical treatment [66] and content recommendation [45] are commonly modeled within the contextual bandits framework. In the online setting, an agent observes a context at each step and chooses among the available actions. The agent then receives a contextdependent reward corresponding to the action taken, but cannot observe the rewards corresponding to alternative actions. In a healthcare setting, the observed context might be a vector capturing vital signs, lab tests, and other available data, while the action space might consist of the available treatments. The reward to optimize could be a measure of patient health or treatment response.
While contextual bandits research has traditionally focused on the expected reward, stakeholders often care about other risk functionals (parameters of the reward distribution) that express real-world desiderata or have desirable statistical properties. For example, investors assess mutual funds via the Sharpe ratio, which normalizes returns by their variance [59]. Related works in reinforcement learning (RL) have sought to estimate the variance of returns [56, 64] and to optimize the mean return under variance constraints [47]. In safety-critical and ﬁnancial applications, researchers often measure the conditional value-at-risk (CVaR), which captures the expected return among the lower α quantile of outcomes [55, 39]. In an emerging line of RL works, researchers have explored other risk functionals, including cumulative prospect weighting [30], distortion risk measures [17], and exponential utility functions [21].
1

In many real-world problems otherwise suited to the contextual bandits framework, experimentation turns out to be prohibitively expensive or unethical. In such settings, we might hope to evaluate prospective policies using the data collected under a previous policy. Formally, this problem is called oﬀ-policy evaluation, and our goal is to evaluate the performance of a target policy π using data collected under a behavior policy β. While most existing research focuses on estimating the expected value of the returns [27, 26], one recent paper evaluates the variance of returns [13].
In this paper, we propose practical methods and the ﬁrst sample complexity guarantees for oﬀ-policy risk evaluation, addressing a diverse set of objectives of interest to researchers and practitioners. Towards this end, we introduce Lipschitz risk functionals which encompass all objectives for which the risk (i) depends only on the CDF of rewards; and (ii) is Lipschitz with respect to changes in the CDF (as assessed via the sup norm). We prove that for bounded rewards, this class subsumes many risk functionals of practical interest, including variance, mean-variance, conditional value-at-risk, and cumulative prospect weighting, among others.
Thus, given accurate estimates of the CDF of rewards under π, we can accurately estimate Lipschitz risks. Moreover, (sup norm) error bounds on our CDF estimates imply error bounds on the corresponding plugin estimates for any Lipschitz risks. The key remaining step is to establish ﬁnite sample guarantees on the error in estimating the target policy’s CDF of rewards. Our analysis centers on an importance sampling estimator (Section 5.1), and a variance-reduced doubly robust estimator (Section 5.3). We derive √ﬁnite sample concentrations for both CDF estimators, showing that they achieve the desired O(1/ n) rates, where n is the sample size. Moreover, the estimati√on error for any Lipschitz risk is scales with its Lipschitz constant, and similarly converges as O(1/ n).
We assemble these results into an algorithm called OPRA (Algorithm 1) that outputs a comprehensive risk assessment for a target policy π, using any set of Lipschitz risk functionals. Notably, because all risk estimates share the same underlying CDF estimate, our error guarantees hold simultaneously for all estimated risk functionals in the set, regardless of the cardinality (Section 6). Finally, we present experiments that demonstrate the practical applicability our estimators.
2 Related Work
The study of risk functionals and risk-aware algorithms is core to the decision making literature [4, 55, 41, 58, 1, 52, 36]. In supervised learning, [62, 16] study generalization properties under the CVaR risk functional, while [43] study a variety of risk functionals. [40] considers the uniform convergence of L risks that are induced by CDF-dependent weighting functions and generalize CVaR and cumulative prospect theory (CPT) inspired risks [44]. Uniform convergence of worst-case risks deﬁned by f -divergences, which is a generalization of CVaR, is studied in [25]. A recent line of research on developing the pointwise concentration of CVaR can be found in [9, 68, 51].
In the bandit literature, many works address regret minimization problems using risk functionals; popular examples include the CVaR, value-at-risk, and mean-variance [11, 56, 71, 77]. [65] studies optimistic UCB exploration for optimizing CVaR while [14, 7] study Thompson sampling, and [38, 10] study regret minimization for linear combinations of the mean and CVaR. Using the CPT risk functional, [30] considers regret minimization in both K-armed bandits and linear contextual bandits. [70, 49] tackle the problem of black-box function optimization under diﬀerent risk functionals.
2

In oﬀ-policy evaluation, we face an additional challenge due to the discrepancy between the data distribution and that induced by the target policy. Importance sampling (IS) estimators are among the most prominent methods for dealing with distribution shift [2, 34, 60]. Doubly robust (DR) estimators [54, 6] leverage (possibly misspeciﬁed) models to achieve lower variance without sacriﬁcing consistency. These estimators have been adapted for oﬀ-policy evaluation in multi-armed bandits [46, 69, 13], contextual bandits [27, 26, 75], and Markov decision processes [35, 67].
In addition, distributional reinforcement learning methods have gained traction in recent years. These methods, introduced by [8], work with the full distribution of returns and were subsequently improved upon by [17, 18]. Notably, [17] uses the learned distribution to optimize a number of risk functionals, including the CVaR and other distorted risk functionals. [39] leverages a similar method to learn the distribution of returns, but uses optimistic distribution-dependent exploration to optimize the CVaR in MDPs. Along similar lines, [65] considers CVaR regret minimization using UCB exploration in the multi-armed bandit setting, and uses an empirical estimate of the reward distribution for each arm in order to evaluate the CVaR.
For empirical CDF estimation, the seminal work of [28] provides an approximation-theoretic concentration bound which was later tightened by [48]. [3] provides concentration bounds on probability estimates of arbitrary but structured measurable sets. Later, the works of [72, 29] systematically improved Alexander’s inequality. For more details on concentrations of probability estimates, we refer readers to [23].
After deriving our key results, we learned of a prior independent (but then unpublished) work [12] that also employs importance sampling to estimate CDFs for the purpose of providing oﬀ-policy estimates for parameters of the reward distribution. However, they do not establish uniform concentration of their estimates or formally relate the parameter and CDF errors, leaving open questions concerning the convergence (both asymptotically and in ﬁnite samples) of the parameter estimates. Our work formulates both importance sampling and variance-reduced doubly robust estimators and provides the ﬁrst uniform ﬁnite sample concentration bounds for both types of CDF and risk estimates.

3 Problem Setting

We denote contexts by X and the corresponding context space by X . Similarly, we denote actions by

A and the corresponding action space by A. We study the contextual bandit problem characterized

by a ﬁxed probability measure over context space X , and a reward function that maps from tuples

of contexts and actions to rewards: R : X × A → R. In the oﬀ-policy setting, we have access to a

dataset D generated using a behavior policy β that interacts with the environment for n rounds

as follows: at each round, a new context X is drawn and then the policy β chooses an action

A ∼ β(·|X). The environment then reveals the reward R ∼ R(·|X, A) for only the chosen action A.

Running

this

process

for

n

steps

generates

a

dataset

D

:=

{

xi

,

ai

,

ri

}

n i=1

.

In

the

oﬀ-policy

evaluation

setting, our goal is to evaluate the performance of a target policy π, using only a dataset D.

Next, we can express our sample space in terms of the contexts, actions, and rewards: Ω = (X × A × R). Let (Ω, F, Pβ) be the probability space induced by the behavior policy β, and (Ω, F, P) the probability space induced by the target policy π. We assume that P is absolutely continuous with respect to Pβ. For any context x and action a, the importance weight expresses the ratio

3

between the two densities w(ω) = w(a, x) = βπ((aa||xx)) , and the maximum weight wmax = supa,x w(a, x) is simply the supremum taken over all contexts and actions. Further, let w2 = EPβ w(A, X)2 denote the exponential of the second order Rényi divergence. Note that by deﬁnition, w2 ≤ wmax, and in practice, we often have w2 wmax.
Finally, we introduce some notation for describing CDFs: For any t ∈ R, let F (t) = EP[1{R≤t}] denote the CDF under the target policy; further, let G(t; X, A) = EP[1{R≤t}|X, A] = EPβ [1{R≤t}|X, A]
denote the CDF of rewards conditioned on a context X and action A, which is independent of
the policy. Lastly, for any t ∈ R, we denote the variance by σ2(t; X, A) = VP 1{R≤t}|X, A = VPβ 1{R≤t}|X, A .
4 Lipschitz Risk Functionals
We now introduce Lipschitz risk functionals, a novel class of objectives for which absolute diﬀerences in the risk are bounded by sup norm diﬀerences in the CDF of rewards. After formally deﬁning the class, we provide an in-depth review of common risk functionals and their relationship to the CDF of rewards. When possible, we derive the associated Lipschitz constants, when rewards are bounded on support [0, D], relegating all proofs to Appendix A.
4.1 Deﬁning the Lipschitz Risk Functionals
The Lipschitz risk functionals are a subset of the broader family of law-invariant risk functionals. Formally, let Z ∈ L∞(Ω, FZ, PZ) denote a real-valued random variable that admits a CDF FZ ∈ L∞(R, B(R)). A risk functional ρ is a mapping from a space of random variables to the space of real numbers ρ : L∞(Ω, FZ, PZ) → R. Any risk functional ρ is said to be law-invariant if ρ(Z) depends only on the distribution of Z [42].
Deﬁnition 4.1 (Law-Invariant Risk Functional). A risk functional ρ : L∞(Ω, F, P) → R, is lawinvariant if for any pair of random variables Z and Z , FZ = FZ =⇒ ρ(Z) = ρ(Z ).
When clear from the context, we sometimes abuse notation by writing ρ(FZ) in place of ρ(Z). In general, it may not be practical to estimate risk functionals that are not law invariant from data [5]. Thus focusing on law-invariant risks is only mildly restrictive.
We can now formally deﬁne the Lipschitz risk functionals:
Deﬁnition 4.2 (Lipschitz Risk Functional). A law invariant risk functional ρ is L-Lipschitz if for any pair of CDFs FZ and FZ and some L ∈ (0, ∞), it satisﬁes
|ρ(FZ ) − ρ(FZ )| ≤ L FZ − FZ ∞.
A risk functional is L-Lipschitz if, for any two random variables Z, Z , its value is upper bounded by the sup-norm of the diﬀerence between their corresponding CDFs. The signiﬁcance of this Lipschitzness property in the contextual bandit setting is that, given a high conﬁdence bound on the error of the estimated CDF of rewards for a policy π, we can obtain a high conﬁdence bound on its evaluation under any L-Lipschitz law-invariant risk functional on the distribution of rewards.
4

4.2 Overview of Common Risk Functionals (and their Lipschitzness)
We now brieﬂy describe some popular classes of risk functionals and their axiomatic deﬁnitions. When possible, we derive their associated Lipschitz constants.
First, we enumerate a set of prominent axioms explored in the current literature [4, 57]. Consider a pair of random variables Z and Z , we have the following axioms:
1. Monotonicity: ρ(Z) ≤ ρ(Z ) whenever Z ≤ Z .
2. Subadditivity: ρ(Z + Z ) ≤ ρ(Z) + ρ(Z ).
3. Additivity: ρ(Z + Z ) = ρ(Z) + ρ(Z ) if Z and Z are co-monotonic random variables (i.e., there exists a random variable Y and weakly increasing functions f, g such that Z = f (Y ) and Z = g(Y )).
4. Translation invariance: ρ(Z + c) = ρ(Z) + c, ∀c ∈ R.
5. Positive homogeneity: ρ(tZ) = tρ(Z) for t > 0.
6. Bounded above by the maximum cost, i.e., ρ(Z) ≤ max(Z).
7. Bounded below by the mean cost, i.e., ρ(Z) ≥ E[Z].
From this set of axioms, one can deﬁne a class of risk functionals by choosing the subset best suited to the problem at hand.
Coherent Risk Functionals. The set of risk functionals that satisfy monotonicity (Axiom 1), subadditivity (Axiom 2), translation invariance (Axiom 4), and positive homogeneity (Axiom 5), and positive homogeneity (see Appendix A), constitute the coherent risk functionals [4, 20]. Further, if a law-invariant coherent risk functional additionally satisﬁes Additivity (Axiom 3), it is said to be a spectral risk functional [37, 1].
While not all coherent risk functionals are law-invariant, nearly all of those commonly addressed in the literature are. Examples include expected value, conditional value-at-risk (CVaR), entropic valueat-risk, and mean semideviation [14, 65, 63, 58]. Others include the Wang transform function [73] and the proportional hazard (PH) risk functional [76].
Distorted Risk Functionals. When the random variable Z is required to be non-negative, lawinvariant coherent risk functionals are examples of the more general class of law-invariant distorted risk functionals [22, 73, 74, 5]. For Z ≥ 0, a distorted risk functional has the following form
∞
ρ(FZ ) = g(1 − FZ (t))dt,
0
where the distortion function g : [0, 1] → [0, 1] is an increasing function with g(0) = 0 and g(1) = 1. Distorted risk functionals are coherent if and only if g is concave [76]. For example, when g(s) = min{ 1−sα , 1} for s ∈ [0, 1] and α ∈ (0, 1), CVaR at level α is recovered. When g is the identity map, the distorted risk functional is the expected value. The Wang risk functional at level α [73] is recovered when g(s) = F (F −1(s) − F −1(α)), and the proportional hazard risk functional can by
5

obtained by setting g(s) = sα for α < 1. Not all distorted risk functionals are coherent. For example,
setting g(s) = 1{s≥1−α} recovers the value-at-risk (VaR), which is not coherent.
Distorted risk functionals have many desirable theoretical properties. They are translation invariant (Axiom 4) and positive homogeneous (Axiom 5), and are deﬁned utilizing (Axiom 6) and (Axiom 7) [76]. They satisfy Axiom 7 if and only if g(s) ≥ s ∀s ∈ [0, 1] [76], and are subadditive (Axiom 2) if and only if g is concave, which preserves second order stochastic dominance [73]. In addition, all distorted risk functionals preserve stochastic ﬁrst order dominance [76].
Lemma 4.1 (Lipschitzness of Coherent and Distorted Risk Functionals). On the space of random variables with support in [0, D], the distorted risk functional of any DL -Lipschitz distortion function g : [0, 1] → [0, 1], i.e., |g(t) − g(t )| ≤ DL |t − t |, is a L-Lipschitz risk functional.
Remark 4.1 (Expected Value and CVaR). Both expected value and CVaR are examples of distorted risk functionals. Then using Lemma 4.1, on the space of random variables with support in [0, D], the expected value risk functional is D-Lipschitz because g is the identity and thus 1-Lipschitz. On the same space, the risk functional CVaRα is Dα -Lipschitz because g is α1 -Lipschitz.

Cumulative Prospect Theory (CPT) Risk Functionals. CPT risks [52] take the form:

+∞

+∞

ρ(FZ ) =

g+ 1 − Fu+(Z)(t) dt −

g− 1 − Fu−(Z)(t) dt,

0

0

where g+, g−: [0, 1] → [0, 1], g+/−(0) = 0, and g+/−(1) = 1. The functions u+, u− : R → R+ are continuous, with u+(z) = 0 when z ≥ c and u−(z) = 0 when z < c for some constant c ∈ R. Importantly, the CPT functional handles gains and losses separately. The functions u+, u− compare the random variable Z to a baseline c, and the distortion g+ is applied to “gains” (when Z ≥ c), while g− is applied to “losses” (when Z < c).
Note that the distortions g+, g− may not necessarily be monotone functions. As a result, the distortion functionals can be seen as a special case of the CPT functional when Z is nonnegative, c = 0, and g is an increasing function. Appropriate choices of g+, g− can again be used to recover risk functionals such as the CVaR and VaR. We make note of the fact that, in the CPT literature, g+/− is chosen to necessarily have a ﬁxed point where g+/−(s) = s for some s ∈ (0, 1), although we do not make this assumption here.
In general, due to the general form of g+/− and the separate consideration of losses and gains, the CPT-inspired risk functional may not satisfy any of the deﬁned axioms. However, additional assumptions on the distortions g+ and g− may allow certain axioms to be satisﬁed. For example, if the random variable has nonnegative support and the threshold c is set to be 0 so that only gains are observed, and g+ is additionally increasing, we recover the distorted risk functionals with axioms speciﬁed above. If g+ is additionally concave, then we recover the coherent risk functionals.

Lemma 4.2 (Lipschitzness of CPT Functional). On the space of random variables with support in [0, D], if the CPT distortion functions g+ and g− are both DL -Lipschitz, then the CPT risk functionals is L-Lipschitz.

6

Other Risk Functionals. The variance, mean-variance, and many other popular risks do not ﬁt easily into the aforementioned classes, but are nevertheless law-invariant. For example, for a nonnegative random variable Z, the variance is deﬁned as ρ(FZ ) = 2 0∞ t(1−FZ(t))dt− 0∞(1 − FZ (t))dt 2 . Moreover, the variance and mean-variance are both L-Lipschitz.

Lemma 4.3 (Lipschitzness of Variance). On the space of random variables with support in [0, D], variance is a 3D2-Lipschitz risk functional.

A number of recent papers have addressed risk functionals expressed as weighted combinations of others, e.g., mean-variance [56]. Other papers have optimized constrained objectives, such as expected reward constrained by variance or CVaR below a certain threshold [15, 53]. When expressed as Lagrangians, these objectives can also be expressed as weighted combinations of the risk functionals involved. We extend the Lipschitzness property to risk functionals of this form:

Lemma 4.4 (Lipschitzness of Weighted Sum of Risk Functionals). Let ρ be a weighted sum of risk

functionals ρ1, ..., ρK that are L1, ..., LK-Lipschitz, respectively, with weights λ1, ...., λK > 0, i.e.,

ρ(Z) =

K k=1

λk

ρk

(Z

).

Then

ρ

is

k λkLk-Lipschitz.

Remark 4.2. Note that mean-variance is given by ρ(Z) = E[Z] + λV(Z) for some λ > 0. Then, using Lemma 4.4, we immediately obtain that mean-variance is (1 + 3λD2)-Lipschitz for bounded
random variables.

Though we have provided many examples of Lipschitz risk functionals in this section, it is worth noting that there are a number of risk functionals that do not satisfy the Lipschitzness property, such as the value-at-risk (VaR). For the sake of brevity, we omit consideration of such risk functionals in this paper, and outline future avenues of research on this topic in the discussion.

5 Oﬀ-Policy CDF Estimation
This section describes our method for high-conﬁdence oﬀ-policy estimation of F , the CDF of returns under the policy π. The key challenge in estimating F is that the reward samples are observed only for actions taken by the behavior policy β. To overcome this limitation, one intuitive solution is to reweight the observed samples according to their importance sampling (IS) weight (Section 5.1). However, IS estimators are known to suﬀer from high variance. To mitigate this, we deﬁne the ﬁrst doubly robust CDF estimator (Section 5.3).
5.1 CDF Estimation with Importance Sampling (IS)
Given an oﬀ-policy dataset D = {xi, ai, ri}ni=1, we deﬁne the following nonparametric IS-based estimator for the empirical CDF,
FIS(t) := n1 n w (ai, xi) 1{ri≤t}, (1) i=1
where w(a, x) = βπ((aa||xx)) are the importance weights. The IS estimator is pointwise-unbiased, with variance given below:

7

Lemma 5.1. The IS estimator (1) is unbiased and its variance is
VPβ FIS(t) = n1 EPβ w(A, X)2σ2(t; X, A) + n1 VPβ EPβ [w(A, X)G(t; X, A)|X] 1
+ n EPβ VPβ [w(A, X)G(t; X, A)|X]

The expression for variance is broken down into three terms. The ﬁrst term represents randomness in the rewards. The second term represents variance due to the randomness over contexts X. The ﬁnal term is the penalty arising from using importance sampling, and is proportional to the importance sampling weights w and the true CDF of conditional rewards G. The variance contributed by the third term can be large when the weights w have a wide range, which occurs when β assigns extremely small probabilities to actions where π assigns high probability.

Due to the use of importance sampling weights, the estimated CDF FIS(t) may be greater than 1 for some t, even though a valid CDF must be in the interval [0, 1] for all t. To mitigate this problem, a weighted importance sampling (WIS) estimator can be used, which normalizes each importance weight by the sum of importance weights:

FWIS(t) =

1

n
w(a , x )1 ,

n w(aj, xj)

i i {ri≤t}

j=1

i=1

which [12] shows is a biased but uniformly consistent estimator. Another option is the clipped estimator IS-Clip (2), which simply limits the estimator to the unit interval:

FIS-clip(t) := min{FIS(t), 1}

(2)

Although FIS-clip has lower variance than the IS estimator, it is potentially biased.
However, given ﬁnite samples, we can bound with high conﬁdence the sup-norm error between FIS-clip and F , in Theorem 5.1 below (proof in Appendix B.1.2):
Theorem 5.1. Given n samples drawn from Pβ, for the IS estimator FIS(t), we have

Pβ FIS-clip − F ∞ ≤ εIS1 := 8wnm2 ax log(4/δ) ≥ 1 − δ. (3)

or, based on w2, we obtain a Bernstein-style bound,

Pβ FIS-clip − F ∞ ≤ εIS2 := 4wmax lnog(4/δ) + 2 2w2 long(4/δ) ≥ 1 − δ (4)

When w2 wmax, we observe that inequality (4) is more favorable than inequality (3). Th√eorem 5.1 demonstrates that the FIS-clip uniformly converges to the true CDF at a rate of O(1/ n), with the uniform consistency of FIS-clip as an immediate consequence. To the best of our knowledge, it is the ﬁrst DKW-style concentration inequality on the importance sampling estimator CDF estimator in oﬀ-policy evaluation. The bound has explicit constants and subsumes the classical DKW inequality.
8

5.2 Model-Based CDF Estimation

As we have shown previously, IS estimators can suﬀer from high variance, which can be limiting in practice. However, in many practical applications, we may have access to a model G(t; X, A) of the conditional distribution G(t; X, A), which can be used in estimation with very low variance. In many cases, practitioners may have a model of G from expert studies or from a simulator, or can form a regression estimate of G from logged data. One simple model-based estimator can then be obtained using the direct method, which simply employs the model G for each observed context:

1n

FDM(t) = n G(t; xi, π), where G(t; xi, π) = π(a|xi)G(t; xi, a). (5)

i=1

a

Because the DM estimator FDM does not use importance weights, it can have signiﬁcantly lower variance than the IS and DR estimators. In general, however, the DI estimator is biased, and its error ﬂows directly from error in the model G (full derivations of the bias and variance are given in Lemma D.3 of Appendix D). The magnitude and distribution of bias over the context and action space is diﬃcult to characterize. In practice, G is often estimated or modeled agnostic to the target policy, and hence may not be well-approximated in areas that are important for π. If G is an accurate model of the conditional reward distribution, however, then FDM is a good approximation of F .

5.3 Doubly Robust (DR) CDF Estimation
We now deﬁne a doubly robust (DR) CDF estimator that takes advantage of both importance sampling and models G to obtain the best characteristics of both types of estimation. In particular, the DR estimator is unbiased, but has potentially signiﬁcant reduction in variance. The DR estimator for the empirical CDF is deﬁned to be
FDR(t) := n1 n w(ai, xi) 1{ri≤t} − G(t; xi, ai) + G(t; xi, π), (6) i=1
where G(t; x, π) = EPβ G(t; x, A)|x . Informally, the DR estimator takes the model G as a baseline, using the available data to apply a correction. While G alone may be biased, the DR estimator is an unbiased estimator of F , and can have reduced variance compared to the IS estimator:
Lemma 5.2. The DR estimator (6) is unbiased and its variance is
VPβ FDR(t) = n1 EPβ w(A, X)2σ2(t; X, A) + n1 VPβ EPβ [w(A, X)G(t; X, a)|X] 1
+ n EPβ VPβ w(A, X) G(t; X, A) − G(t; X, A) |X
The variance reduction advantage of the DR estimator becomes apparent from a direct comparison of the three terms in the IS estimator variance (Lemma 5.1) and the DR estimator variance (Lemma 5.2). The ﬁrst and second terms, which capture the variance in rewards and contexts, are identical. The third term, which represents the importance sampling penalty, is proportional to G − G in the DR estimator, but proportional to G in the IS estimator. When this diﬀerence G − G is smaller than G,

9

which is often the case in practice, the third term has reduced variance in the DR estimator. The magnitude of variance reduction is greater when the weights w have a large range, which is precisely when large variance can become problematic in importance sampling.
Remark 5.1 (Double Robustness). Although we consider the setting where the behavior policy β is known, when the behavior policy is unknown and needs to be estimated, the estimator FDR is consistent when either G is consistent or the policy estimator is consistent. This is where the name “doubly robust" comes from. We demonstrate and discuss this fact further in Appendix D.
Although the DR estimator FDR has desirable reductions in variance, given ﬁnite samples, it is not guaranteed to be a valid CDF. Like the IS estimator, the DR estimator may be greater than 1 for some t due to the use of importance weighting. However, it may also be negative at some t as a consequence of the subtracted term in (6). As an additional consequence of this term, the DR estimator is not guaranteed to be a monotone function. As a result, in order to use the DR CDF estimate for risk estimation, we must transform FDR into a monotone function bounded in [0, 1]. Examples of such transformations include isotonic approximation [61] and monotone Lp approximation [19].
For our analysis, however, we consider a simple monotone transformation that involves an accumulation function, which does not allow the CDF to decrease, followed by a clipping to [0, 1]:

FM-DR(t) = Clip max FDR(t ), 0, 1 ,

(7)

t ≤t

which is a uniformly consistent estimator, as the following concentration guarantee shows:

Theorem 5.2. The monotone transformation of the DR estimator FM-DR(t) satisﬁes





Pβ  FM-DR − F ∞ ≤ DR := 72wnm2 ax log 8nδ1/2  ≥ 1 − δ. (8)

The purpose of Theorem 5.2 is to show the dependence of the error on the importance weights wmax and on t√he ﬁnite sample size n. Using the M-DR estimator, we again recover a sample complexity of O (1/ n). The proof is given in Appendix B.2.2. Note that (8) does not depend on the error G − G, which is the term responsible for variance reduction, as given in Lemma D.5. A tighter bound for the DR estimator, which incorporates the error G − G, remains an open problem, and we leave this to future work. We demonstrate empirically in Section 7 that, in practice, we do achieve faster convergence and smaller empirical conﬁdence intervals with the M-DR estimator.
6 Oﬀ-Policy Risk Assessment
Given any law-invariant risk functional ρ and CDF estimator F , we can estimate the value of the risk functional as ρ := ρ(F ). However, the estimator ρ may be biased even if F is unbiased. For Lipschitz risk functionals introduced in Section 4, we can obtain their ﬁnite sample error bounds, using the error bound of the CDF estimator. Further, a set of risk functionals of interest can be evaluated using the same estimated CDF, which suggests that the error bound of the CDF gives error bounds on the risk estimators that hold simultaneously.
10

Theorem 6.1 utilizes our error bound of the estimated CDF to derive error bounds for estimators of a set of Lipschitz risk functionals. As we showed in Section 4, most if not all commonly studied risk functionals satisfy the property of Lipschitzness, showing our result’s wide applicability.
Theorem 6.1. Given a set of Lipschitz risk functionals {ρp}Pp=1 with Lipschitz constants {Lp}Pp=1, and a CDF estimator F , such that F − F ∞ ≤ with probability at least 1 − δ, we have with probability at least 1 − δ that for all p ∈ {1, . . . P },
ρp(F ) − ρp(F ) ≤ Lp .
Thus, one powerful property of risk estimation using the estimated CDF approach is that, given a high-probability error bound on the CDF estimator, the corresponding error bounds on estimates of all Lipschitz risk functionals of interest hold simultaneously with the same probability. Further, because the error of the IS CDF es√timator IS (Theorem 5.1) and DR CDF estimator DR (Theorem 5.2) converge at a rate of O(1/ n), Th√eorem 6.1 shows that the error of all Lipschitz risk functional estimators shrink at a rate of O(1/ n). Thus, ρp(F ) are consistent risk functional estimators.
Putting these results together, we now provide an algorithm, called OPRA (Algorithm 1), which given an oﬀ-policy contextual bandit dataset and a set of Lipschitz risk functionals of interest, outputs for each risk functional an estimate of its value and a conﬁdence bound. The algorithm ﬁrst uses a valid CDF estimator, e.g., the clipped IS estimator (2) or monotonized DR estimator (7), to form F with sup-norm error . OPRA then evaluates each Lp-Lipschitz risk functional ρp on F to obtain ρp, along with its upper and lower conﬁdence bound ρp ± Lp .
Algorithm 1: Oﬀ-Policy Risk Assessment (OPRA) Input: Dataset D, policy π, probability δ, models G, Lipschitz risk functionals {ρp}Pp=1 with Lipschitz constants {Lp}Pp=1.
1 Estimate the CDF using a valid CDF estimator F ; 2 Compute the corresponding CDF estimation error such that P( F − F ∞ < ) ≥ 1 − δ; 3 for p = 1 . . . P do 4 Estimate ρp = ρp(F ); 5 end
Output: Estimates with errors {ρp ± Lp }Pp=1.
OPRA can be used to obtain a full risk assessment of any given policy, using the input Lipschitz risk functionals of interest, which can include the popularly used mean, variance, and CVaR. As demonstrated in Theorem 6.1, the error guarantee on the risk estimators holds simultaneously for all P risk functionals with probability at least 1 − δ. Importantly, OPRA also demonstrates the computational eﬃciency of the distribution-centric risk estimation approach proposed in this paper. For a given π, the CDF only needs to be estimated once, and can be used repetitively to estimate the value of the risk functionals. Further, the error of the risk estimators are determined by the known error of the CDF estimator, multiplied by the known Lipschitz constants.
Remark 6.1 (Estimation of Risk Functionals That Are Not L-Lipschitz). We have focused our discussion on the estimation of L-Lipschitz risk functionals due their generalizability and ﬂexibility, and because we can characterize the rate at which the error decreases. Any law-invariant risk
11

functional can actually be estimated using the CDF estimate, although the error or conﬁdence of the estimate may have to be determined on a case-by-case basis for each risk functional of interest. Further, the rate at which the error converges may not necessarily be known.
Remark 6.2 (Risk Functionals Estimation When Behavioral Policy Is Unknown). Although we work with known behavioral policy β in this paper, previous works on oﬀ-policy evaluation have considered the case where the behavioral policy is unknown. In such cases, an estimate or model of the policy, called β, is instead used in CDF and risk estimation. In Appendix D we extend the bias, variance, error bound results to this setting.
7 Empirical Studies
In this section, we give empirical evidence for the eﬀectiveness of the doubly robust (DR) CDF and risk estimates, in comparison to the importance sampling (IS), weighted importance sampling (WIS), and direct method (DM) estimates. Further, we demonstrate the convergence of the CDF and risk estimation error in terms of the number of samples.
Setup. Following [27, 26, 75], we obtain our oﬀ-policy contextual bandit datasets by transforming classiﬁcation datasets. The contexts are the provided features, and the actions correspond to the possible class labels. To obtain the evaluation policy π, we use the output probabilities of a trained logistic regression classiﬁer. The behavior policy is deﬁned as β = απ + (1 − α)πUNIF, where πUNIF is a uniform policy over the actions, for some α ∈ (0, 1]. We apply this process to the PageBlocks and OptDigits datasets [24], which have dimensions d and actions k using α = 0.1 (Figure 1). When models G are used (for DM, DR estimators), as in [27], the dataset is divided into two splits, with each of the two splits used to calculate G via regression, which is then used with the other split to calculate the estimator. The two results are averaged to produce the ﬁnal estimators. We provide further details and extensive evaluations in Appendix E.
CDF Estimation. We evaluate the error F − F ∞ of our CDF estimat√ors against sample size for two UCI datasets (Figure 1). The IS and DR exhibit the expected O (1/ n) rate of convergence in error previously derived in Theorems 5.1 and 5.2, respectively. We note that the WIS estimator, while biased, performs as well as the IS estimator if not better. In the PageBlocks dataset (Figure 1, left), the regression model for G is relatively well-speciﬁed as exempliﬁed by the relatively low error of the DM estimator, though it has high variance for low samples sizes. The DR estimator leverages this model to outperform all other estimators for all sample sizes, without suﬀering the drawbacks of the DM estimator. It takes an order of magnitude less data to reach the same error compared to the IS and WIS estimators. In contrast, the regression model is less well-speciﬁed in the OptDigits dataset for lower sample sizes (Figure 1, right), and consequently, the DR estimator cannot perform as well as the IS and WIS estimators for small n. This trend reverses as data increases and the model improves, with the DR estimator outperforming the IS estimators.
Estimation of Risk Functionals. Figure 2 shows the mean, variance, and CVaR0.5 estimates, which are obtained by evaluating each risk functional on the CDF estimators for the OptDigits dataset. Here, the estimates are plotted against the true value (dashed line) to make the variance reduction eﬀect of the DR estimators more apparent. The DM estimator, which appeared to have
12

PageBlocks: n=5472, k=5, d=10
0.100 0.010

OptDigits: n=5620, k=10, d=64 DR WIS IS DM

error

0.001 100

n 1000

100

n 1000

Figure 1: The error of the CDF estimators as a function of sample size n, for (left) the PageBlocks dataset and (right) the OptDigits dataset. Shaded area is the 95% quantile over 500 runs.

0.70

0.3

0.26

Mean

true

CVaR 0.5 Variance

0.25

DR

0.55

WIS

IS

0.1

DM

0.40 1000 2000 3000 4000 5000

n 0.0 1000 2000 3000 4000 5000

0.23 1000 2000 3000 4000 5000

Figure 2: Estimated mean, CVaR0.5, and variance for the OptDigits dataset, compared to their true values (black). Shaded area is the standard deviation over 500 runs.

competitive performance in the CDF error plot, has relatively high risk estimate error, which occurs because the DM CDF may be poorly approximated in areas that are important for risk functional estimation. The IS, WIS, and DR risk estimates converge quickly to the true value as n increases, and as expected, their relative behavior echoes the trends in Figure 1 as a consequence of our distributional approach. The DR estimator has slightly worse performance for small samples sizes due to the poor speciﬁcation of the model, but soon exhibits the desired variance reduction for n > 1000.

8 Discussion
In this paper, we have developed a distribution-centric method for high conﬁdence oﬀ-policy estimation of risk functionals. Our method relies on ﬁrst estimating the CDF and its conﬁdence band, then estimating risk functionals by evaluating on the estimated CDF. We have deﬁned several estimators for the CDF, including an importance sampling and doubly robust estimator which takes advantage of side information to reduce variance. For L-Lipschitz risk functionals, which we show many classes of risks fall under, the concentration of the estimated risk can be derived from the conﬁdence band on the CDF.

13

From a theoretical point of view, our paper provides the ﬁrst ﬁnite sample concentration inequalities for a number of diﬀerent CDF and risk estimators, which are widely applicable to recent distributional reinforcement learning settings, which learn the CDF of returns and are capable of optimizing diﬀerent risk functionals [17, 39]. Of these estimators, the doubly robust estimator is a novel contribution and has not yet been deﬁned or analyzed for distributions in the literature. From a practical standpoint, our method can be used to comprehensively evaluate the behavior of a target policy before deployment using a wide range of risk functionals–a contribution that is especially important in real-world applications.
Our work also raises several open questions and avenues of future work, which we discuss below.
Error Bound for the Doubly Robust CDF Estimator. Although we presented a sampledependent bound, we believe that it can be improved because our current bound does not take into account the double robustness of the estimator. We obtain a looser bound for the DR estimator compared to the IS estimator, even though we show the DR estimator has reduced pointwise variance. Obtaining an improved error bound dependent on the term G − G, which is the error of the given model compared to the true conditional CDF, is one important direction of future work.
Monotone Transformation of the CDF Estimate. As we have demonstrated for the importance sampling and doubly robust estimators, CDF estimation faces a unique problem in that the estimate may not be a valid CDF. Estimates of the expected value, for example, are not subject to any such constraints. We have shown how methods such as clipping and monotone transformation can be applied to FIS or FDR to mitigate this problem.
However, it is important to note that there are, in fact, several options for how the clipping and monotone transformation is applied. For example, instead of applying these transformations after averaging the n samples to form the estimator, another option is to apply the transformation to each individual sample, and then average the transformed results. Applying the monotone transformation to each sample before averaging may potentially increase bias while reducing variance, which may be desirable in certain applications.
In this paper we proposed a simple method (7) for clipping and transforming the CDF estimate, but diﬀerent forms of monotone regression [61, 19], which may potentially provide a better monotone approximation of the CDF estimate. As of yet, the best method of transforming estimates into valid CDFs is not yet clear. Extensive theoretical and empirical evaluation of such estimator options is another important avenue of future work.
CDF and Risk Estimation in MDPs. Previous work in oﬀ-policy evaluation for expected value has developed a doubly robust estimator for the MDP setting [35]. Following this, another avenue of future work will aim to extend our results for the contextual bandit setting to CDF and risk estimation in the Markov Decision Process (MDP) and nonstationary settings. We believe this is especially relevant in relation to recent advances in distributional reinforcement learning, that aims to learn the distribution of returns in MDPs [17].
CDF and Risk Estimation with Unknown Behavioral Policy. Along similar lines, another direction of future work lies in in-depth analysis of the importance sampling and doubly robust
14

estimators when the behavioral policy is unknown and must be estimated from the data. Previous works have shown that under mild consistency assumptions, using estimates of importance weight asymptotically provides a better estimator of policy evaluate [31, 33, 32]. Whether similar properties hold under CDF and risk estimation remains to be seen. Risk Error Bounds Without Lipschitzness. Finally, though we provide concentration bounds for a large number of risk functionals under the L-Lipschitz property, a number of other risk functionals, such as the inverse quantile function, do not satisfy this property. The conﬁdence band on the CDF can still be used to calculate a conﬁdence interval on the risk, but it is not clear if and how quickly the conﬁdence interval shrinks with more samples. This motivates the following open question, which we plan to study in future work: can concentration inequalities for risk functional estimates be derived if and only if they are Lipschitz, in a more general sense?
Acknowledgements
The authors thank Siva Balakrishnan for inspiring discussions, and Roberto Imbuzeiro Oliveira for his lecture notes. Liu Leqi is generously supported by an Open Philanthropy AI Fellowship. Zachary Lipton thanks Amazon AI, Salesforce Research, the Block Center, the PwC Center, Abridge, UPMC, the NSF, DARPA, and SEI for supporting ACMI lab’s research on robust and socially aligned machine learning.
15

References
[1] Carlo Acerbi. Spectral measures of risk: A coherent representation of subjective risk aversion. Journal of Banking & Finance, 26(7):1505–1518, 2002.
[2] V. M. Aleksandrov, V. I. Sysoyev, and V. V. Shemeneva. Stochastic optimization. Engineering Cybernetics, 5(11-16):229–256, 1968.
[3] Kenneth S Alexander. Probability inequalities for empirical processes and a law of the iterated logarithm. The Annals of Probability, pages 1041–1067, 1984.
[4] Philippe Artzner, Freddy Delbaen, Jean-Marc Eber, and David Heath. Coherent measures of risk. Mathematical ﬁnance, 9(3):203–228, 1999.
[5] Alejandro Balbás, José Garrido, and Silvia Mayoral. Properties of distortion risk measures. Methodology and Computing in Applied Probability, 11(3):385–399, 2009.
[6] Heejung Bang and James M Robins. Doubly robust estimation in missing data and causal inference models. Biometrics, 61(4):962–973, 2005.
[7] Dorian Baudry, Romain Gautron, Emilie Kaufmann, and Odalric-Ambryn Maillard. Thompson sampling for cvar bandits. arXiv preprint arXiv:2012.05754, 2020.
[8] Marc G Bellemare, Will Dabney, and Rémi Munos. A distributional perspective on reinforcement learning. In International Conference on Machine Learning, pages 449–458. PMLR, 2017.
[9] David B Brown. Large deviations bounds for estimating conditional value-at-risk. Operations Research Letters, 35(6):722–730, 2007.
[10] Sébastien Bubeck, Rémi Munos, Gilles Stoltz, and Csaba Szepesvári. X-armed bandits. Journal of Machine Learning Research, 12(5), 2011.
[11] Asaf Cassel, Shie Mannor, and Assaf Zeevi. A general framework for bandit problems beyond cumulative objectives, 2020.
[12] Yash Chandak, Scott Niekum, Bruno Castro da Silva, Erik Learned-Miller, Emma Brunskill, and Philip S. Thomas. Universal oﬀ-policy evaluation, 2021.
[13] Yash Chandak, Shiv Shankar, and Philip S. Thomas. High-conﬁdence oﬀ-policy (or counterfactual) variance estimation, 2021.
[14] Joel QL Chang, Qiuyu Zhu, and Vincent YF Tan. Risk-constrained thompson sampling for cvar bandits. arXiv preprint arXiv:2011.08046, 2020.
[15] Yinlam Chow, Mohammad Ghavamzadeh, Lucas Janson, and Marco Pavone. Risk-constrained reinforcement learning with percentile risk criteria. The Journal of Machine Learning Research, 18(1):6070–6120, 2017.
[16] Sebastian Curi, Kﬁr Levy, Stefanie Jegelka, Andreas Krause, et al. Adaptive sampling for stochastic risk-averse learning. arXiv preprint arXiv:1910.12511, 2019.
16

[17] Will Dabney, Georg Ostrovski, David Silver, and Rémi Munos. Implicit quantile networks for distributional reinforcement learning. In International conference on machine learning, pages 1096–1105. PMLR, 2018.
[18] Will Dabney, Mark Rowland, Marc Bellemare, and Rémi Munos. Distributional reinforcement learning with quantile regression. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 32, 2018.
[19] Richard B Darst and Robert Huotari. Best l1-approximation of bounded, approximately continuous functions on [0, 1] by nondecreasing functions. Journal of approximation theory, 43(2):178–189, 1985.
[20] Freddy Delbaen. Coherent risk measures on general probability spaces. In Advances in ﬁnance and stochastics, pages 1–37. Springer, 2002.
[21] Eric V Denardo, Haechurl Park, and Uriel G Rothblum. Risk-sensitive and risk-neutral multiarmed bandits. Mathematics of Operations Research, 32(2):374–394, 2007.
[22] Dieter Denneberg. Distorted probabilities and insurance premiums. Methods of Operations Research, 63(3):3–5, 1990.
[23] Luc Devroye, László Györﬁ, and Gábor Lugosi. A probabilistic theory of pattern recognition, volume 31. Springer Science & Business Media, 2013.
[24] Dheeru Dua and Casey Graﬀ. UCI machine learning repository, 2017.
[25] John Duchi and Hongseok Namkoong. Learning models with uniform performance via distributionally robust optimization. Annals of Statistics, 2018.
[26] Miroslav Dudík, Dumitru Erhan, John Langford, Lihong Li, et al. Doubly robust policy evaluation and optimization. Statistical Science, 29(4):485–511, 2014.
[27] Miroslav Dudik, John Langford, and Lihong Li. Doubly robust policy evaluation and learning, 2011.
[28] Aryeh Dvoretzky, Jack Kiefer, and Jacob Wolfowitz. Asymptotic minimax character of the sample distribution function and of the classical multinomial estimator. The Annals of Mathematical Statistics, pages 642–669, 1956.
[29] Peter Gänssler and Winfried Stute. Empirical processes: a survey of results for independent and identically distributed random variables. The Annals of Probability, pages 193–243, 1979.
[30] Aditya Gopalan, LA Prashanth, Michael Fu, and Steve Marcus. Weighted bandits or: How bandits learn distorted values that are not expected. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 31, 2017.
[31] Jinyong Hahn. On the role of the propensity score in eﬃcient semiparametric estimation of average treatment eﬀects. Econometrica, pages 315–331, 1998.
[32] James Heckman, Hidehiko Ichimura, Jeﬀrey Smith, and Petra Todd. Characterizing selection bias using experimental data. Econometrica, pages 1017–1098, 1998.
17

[33] Keisuke Hirano, Guido W Imbens, and Geert Ridder. Eﬃcient estimation of average treatment eﬀects using the estimated propensity score. Econometrica, 71(4):1161–1189, 2003.
[34] Daniel G Horvitz and Donovan J Thompson. A generalization of sampling without replacement from a ﬁnite universe. Journal of the American statistical Association, 47(260):663–685, 1952.
[35] Nan Jiang and Lihong Li. Doubly robust oﬀ-policy value evaluation for reinforcement learning. In International Conference on Machine Learning, pages 652–661. PMLR, 2016.
[36] Cheng Jie, LA Prashanth, Michael Fu, Steve Marcus, and Csaba Szepesvári. Stochastic optimization in a cumulative prospect theory framework. IEEE Transactions on Automatic Control, 63(9):2867–2882, 2018.
[37] Elyès Jouini, Walter Schachermayer, and Nizar Touzi. Law invariant risk measures have the fatou property. In Advances in mathematical economics, pages 49–71. Springer, 2006.
[38] Anmol Kagrecha, Jayakrishnan Nair, and Krishna Jagannathan. Distribution oblivious, risk-aware algorithms for multi-armed bandits with unbounded rewards. arXiv preprint arXiv:1906.00569, 2019.
[39] Ramtin Keramati, Christoph Dann, Alex Tamkin, and Emma Brunskill. Being optimistic to be conservative: Quickly learning a cvar policy. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 34, pages 4436–4443, 2020.
[40] Justin Khim, Liu Leqi, Adarsh Prasad, and Pradeep Ravikumar. Uniform convergence of rank-weighted learning. In International Conference on Machine Learning, pages 5254–5263. PMLR, 2020.
[41] Pavlo A Krokhmal. Higher moment coherent risk measures. Taylor & Francis, 2007.
[42] Shigeo Kusuoka. On law invariant coherent risk measures. In Advances in mathematical economics, pages 83–95. Springer, 2001.
[43] Jaeho Lee, Sejun Park, and Jinwoo Shin. Learning bounds for risk-sensitive learning. arXiv preprint arXiv:2006.08138, 2020.
[44] Liu Leqi, Adarsh Prasad, and Pradeep Ravikumar. On human-aligned risk minimization. In Advances in Neural Information Processing Systems, 2019.
[45] Lihong Li, Wei Chu, John Langford, and Robert E Schapire. A contextual-bandit approach to personalized news article recommendation. In Proceedings of the 19th international conference on World wide web, pages 661–670, 2010.
[46] Lihong Li, Rémi Munos, and Csaba Szepesvári. Toward minimax oﬀ-policy value estimation. In Artiﬁcial Intelligence and Statistics, pages 608–616. PMLR, 2015.
[47] Shie Mannor and John Tsitsiklis. Mean-variance optimization in markov decision processes. arXiv preprint arXiv:1104.5601, 2011.
[48] Pascal Massart. The tight constant in the dvoretzky-kiefer-wolfowitz inequality. The annals of Probability, pages 1269–1283, 1990.
18

[49] Rémi Munos. From Bandits to Monte-Carlo Tree Search: The Optimistic Principle Applied to Optimization and Planning. Now Foundations and Trends, 2014.
[50] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825–2830, 2011.
[51] LA Prashanth, Krishna Jagannathan, and Ravi Kolla. Concentration bounds for cvar estimation: The cases of light-tailed and heavy-tailed distributions. In International Conference on Machine Learning, pages 5577–5586. PMLR, 2020.
[52] LA Prashanth, Cheng Jie, Michael Fu, Steve Marcus, and Csaba Szepesvári. Cumulative prospect theory meets reinforcement learning: Prediction and control. In International Conference on Machine Learning, pages 1406–1415. PMLR, 2016.
[53] A Prashanth L and Michael Fu. Risk-sensitive reinforcement learning: A constrained optimization viewpoint. arXiv e-prints, pages arXiv–1810, 2018.
[54] James M Robins and Andrea Rotnitzky. Semiparametric eﬃciency in multivariate regression models with missing data. Journal of the American Statistical Association, 90(429):122–129, 1995.
[55] R Tyrrell Rockafellar, Stanislav Uryasev, et al. Optimization of conditional value-at-risk. Journal of risk, 2:21–42, 2000.
[56] Amir Sani, Alessandro Lazaric, and Rémi Munos. Risk-aversion in multi-armed bandits. arXiv preprint arXiv:1301.1936, 2013.
[57] Ekaterina N Sereda, Eﬁm M Bronshtein, Svetozar T Rachev, Frank J Fabozzi, Wei Sun, and Stoyan V Stoyanov. Distortion risk measures in portfolio optimization. In Handbook of portfolio construction, pages 649–673. Springer, 2010.
[58] Alexander Shapiro, Darinka Dentcheva, and Andrzej Ruszczyński. Lectures on stochastic programming: modeling and theory. SIAM, 2014.
[59] William F Sharpe. Mutual fund performance. The Journal of business, 39(1):119–138, 1966.
[60] Hidetoshi Shimodaira. Improving predictive inference under covariate shift by weighting the log-likelihood function. Journal of statistical planning and inference, 90(2):227–244, 2000.
[61] PW Smith and JJ Swetits. Best approximation by monotone functions. Journal of approximation theory, 49(4):398–403, 1987.
[62] Tasuku Soma and Yuichi Yoshida. Statistical learning with conditional value at risk. arXiv preprint arXiv:2002.05826, 2020.
[63] Aviv Tamar, Yinlam Chow, Mohammad Ghavamzadeh, and Shie Mannor. Policy gradient for coherent risk measures. In Proceedings of the 28th International Conference on Neural Information Processing Systems-Volume 1, pages 1468–1476, 2015.
19

[64] Aviv Tamar, Dotan Di Castro, and Shie Mannor. Learning the variance of the reward-to-go. The Journal of Machine Learning Research, 17(1):361–396, 2016.
[65] Alex Tamkin, Ramtin Keramati, Christoph Dann, and Emma Brunskill. Distributionally-aware exploration for cvar bandits. In NeurIPS 2019 Workshop on Safety and Robustness on Decision Making, 2019.
[66] Ambuj Tewari and Susan A Murphy. From ads to interventions: Contextual bandits in mobile health. In Mobile Health, pages 495–517. Springer, 2017.
[67] Philip Thomas and Emma Brunskill. Data-eﬃcient oﬀ-policy policy evaluation for reinforcement learning. In International Conference on Machine Learning, pages 2139–2148. PMLR, 2016.
[68] Philip Thomas and Erik Learned-Miller. Concentration inequalities for conditional value at risk. In International Conference on Machine Learning, pages 6225–6233. PMLR, 2019.
[69] Philip Thomas, Georgios Theocharous, and Mohammad Ghavamzadeh. High-conﬁdence oﬀpolicy evaluation. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 29, 2015.
[70] Léonard Torossian, Aurélien Garivier, and Victor Picheny. X -armed bandits: Optimizing quantiles, cvar and other risks. In Asian Conference on Machine Learning, pages 252–267. PMLR, 2019.
[71] Sattar Vakili and Qing Zhao. Mean-variance and value at risk in multi-armed bandit problems. In 2015 53rd Annual Allerton Conference on Communication, Control, and Computing (Allerton), pages 1330–1335. IEEE, 2015.
[72] Vladimir Vapnik. Estimation of dependences based on empirical data. Springer Science & Business Media, 2006.
[73] Shaun Wang. Premium calculation by transforming the layer premium density. ASTIN Bulletin: The Journal of the IAA, 26(1):71–92, 1996.
[74] Shaun S Wang, Virginia R Young, and Harry H Panjer. Axiomatic characterization of insurance prices. Insurance: Mathematics and economics, 21(2):173–183, 1997.
[75] Yu-Xiang Wang, Alekh Agarwal, and Miroslav Dudık. Optimal and adaptive oﬀ-policy evaluation in contextual bandits. In International Conference on Machine Learning, pages 3589–3597. PMLR, 2017.
[76] Julia L Wirch and Mary R Hardy. Distortion risk measures: Coherence and stochastic dominance. In International congress on insurance: Mathematics and economics, pages 15–17, 2001.
[77] Alexander Zimin, Rasmus Ibsen-Jensen, and Krishnendu Chatterjee. Generalized risk-aversion in stochastic multi-armed bandits. arXiv preprint arXiv:1405.0833, 2014.
20

Contents (Appendix)

A Proofs for Risk Functionals (Section 4)

22

B Proofs for CDF Estimation (Section 5)

24

B.1 Importance Sampling (IS) Estimators (Section 5.1) . . . . . . . . . . . . . . . . . . . 24

B.1.1 Proof: Bias and Variance of IS CDF Estimate . . . . . . . . . . . . . . . . . . 24

B.1.2 Proof: Error Bound of IS CDF Estimate . . . . . . . . . . . . . . . . . . . . . 24

B.2 Doubly Robust (DR) Estimators (Section 5.3) . . . . . . . . . . . . . . . . . . . . . . 31

B.2.1 Proof: Bias and Variance of DR CDF Estimate . . . . . . . . . . . . . . . . . 31

B.2.2 Proof: Error Bound of DR CDF Estimate . . . . . . . . . . . . . . . . . . . . 31

C Proofs for Risk Functional Estimation (Section 6)

42

D Risk Estimation with Unknown Behavior Policy

43

D.1 Bias and Variance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43

D.2 CDF and Risk Estimate Error Bounds . . . . . . . . . . . . . . . . . . . . . . . . . . 46

E Additional Experiments

49

21

A Proofs for Risk Functionals (Section 4)

Proof of Lemma 4.1.

D

|ρ (FZ ) − ρ (FZ ) | =

g (1 − FZ(t)) − g (1 − FZ (t)) dt

0

D

≤ |g (1 − FZ(t)) − g (1 − FZ (t))| dt
0

DL ≤ 0 D |FZ (t) − FZ (t)| dt

≤ L max |FZ (t) − FZ (t)| ,
t

where the second to last step uses the L/D-Lipschitzness of ρ.

Proof of Lemma 4.2. Using the deﬁnition of the CDF, note that on the bounded support of [0, D] the CPT functional can be rewritten as

D

D

ρ(FZ ) = g+ PZ u+(Z) > t dt − g− PZ u−(Z) > t dt.

0

0

Then,

ρ(Z) − ρ(Z ) =

D

D

g+ PZ u+(Z) > t dt − g− PZ u−(Z) > t dt

0

0

D

D

− g+ PZ u+(Z ) > t dt − g− PZ u−(Z ) > t dt

0

0

D

D

≤

g+ PZ u+(Z) > t dt − g+ PZ u+(Z ) > t dt

0

0

D

D

+

g− PZ u−(Z) > t dt − g− PZ u−(Z ) > t dt

0

0

≤ L D PZ u+(Z) > t − PZ u+(Z ) > t dt D0

+ L D PZ u−(Z) > t − PZ u−(Z ) > t dt D0

LD ≤ D 0 PZ (Z > t) − PZ Z > t dt

LD + D 0 PZ (Z > t) − PZ Z > t dt

LD = 2 D 0 |FZ (t) − FZ (t)| dt

≤ 2L max |FZ (t) − FZ (t)|
t

22

Proof of Lemma 4.3. For the variance of any random variable Z with bounded support [0, D], we have

V(Z) = E(Z2) − E(Z)2.

Note that by the deﬁnition of expectation,

E(Z2) =

D2
1 − FZ2 (t2)dt2
t2=0

Then using dt2 = 2tdt and the fact that P(Z2 ≥ t2) = P(Z ≥ t) since t is nonnegative, with this change of variables we have

D
E(Z2) = 2 t (1 − FZ(t)) dt.
t=0

This gives us the following expression for variance:

D
V(Z) = 2 t(1 − FZ(t))dt −
0

D

2

(1 − FZ(t))dt

0

Next, consider a pair of random variables Z and Z with FZ and FZ as their CDF respectively. Therefore,

V(Z) − V(Z )

D
≤ 2 t(FZ (t) − FZ (t))dt +
0

D

2

(1 − FZ(t))dt −

0

D

2

(1 − FZ (t))dt

0

≤ D2 FZ (t) − FZ
≤ D2 FZ (t) − FZ ≤ D2 FZ (t) − FZ

D

D

∞ + (FZ (t) − FZ (t))dt

(1 − FZ(t))dt +

0

0

D

∞ + 2D (FZ (t) − FZ (t))dt

0

∞ + 2D2 FZ (t) − FZ ∞

D
(1 − FZ (t))dt
0

= 3D2 FZ − FZ ∞

Proof of Lemma 4.4. The proof of this lemma follows directly from the deﬁnition of Lipschitzness:

K

K

λkρk(Z) − λkρk(Z )

k=1

k=1

K
≤ λk ρk(Z) − ρk(Z )
k=1 K
≤ FZ − FZ ∞ λkLk.
k=1

23

B Proofs for CDF Estimation (Section 5)

B.1 Importance Sampling (IS) Estimators (Section 5.1)

B.1.1 Proof: Bias and Variance of IS CDF Estimate

Proof of Lemma 5.1. We take the expectation of the IS estimator (1) with respect to Pβ. Then for any t ∈ R,

EPβ [FIS(t)] = EPβ

n1 n w(Ai, Xi)1{Ri≤t} i=1

= EPβ EPβ βπ((AA||XX)) EPβ 1{R≤t}|X, A

= EP w(A, X)1{R≤t}

= F (t).

Recall that G(t; X, A) = E[1{R≤t}|X, A}]. The variance of the IS estimator is derived using:

VPβ FIS(t) = n1 VPβ w(A, X)1{R≤t} = n1 EPβ w(A, X)2VPβ 1{R≤t}|A, X + n1 VPβ w(A, X)EPβ 1{R≤t}|A, X
= n1 EPβ w(A, X)2σ2(t; X, A) + n1 VPβ [w(A, X)G(t; X, A)] = n1 EPβ w(A, X)2σ2(t; X, A) + n1 VPβ EPβ [w(A, X)G(t; X, A)|X]
1 + n EPβ VPβ [w(A, X)G(t; X, A)|X]
where the second equality uses the law of total variance conditioned on actions A and contexts X, and the third equality uses the deﬁnitions of σ2 and G. The last equality is another application of the law of total variance conditioning on the context X.

B.1.2 Proof: Error Bound of IS CDF Estimate

Proof Theorem 5.1. Deﬁne the following function class:
F(n) := f (r) := 1 1{r≤t} : ∀t ∈ R; ∀r ∈ Q,
n Note that this is a countable set. Using this deﬁnition, we have

∈ {−1, +1}

sup FIS(t) − F (t) = sup

t∈R

f ∈F(n)

Using this equality, for λ > 0, we have:

n
w(Ai, Xi)f (Ri) − EPβ [w(Ai, Xi)f (Ri)]
i

EPβ exp λ sup FIS(t) − F (t)
t∈R

24

n

= EPβ exp λ sup
f ∈F(n)

w(Ai, Xi)f (Ri) − EPβ [w(Ai, Xi)f (Ri)]
i

n

= EPβ exp λ sup EPβ

w(Ai, Xi)f (Ri) − w(Xi, Ai)f (Ri)

f ∈F(n)

i

n

≤ EPβ exp λ sup EPβ
f ∈F(n)

w(Ai, Xi)f (Ri) − w(Xi, Ai)f (Ri)
i

n

≤ EPβ exp λEPβ sup
f ∈F(n)

w(Ai, Xi)f (Ri) − w(Xi, Ai)f (Ri)
i

n

≤ EPβ exp λ sup
f ∈F(n)

w(Ai, Xi)f (Ri) − w(Xi, Ai)f (Ri)
i

n

= EPβ,R exp λ sup
f ∈F(n)

ξi(w(Ai, Xi)f (Ri) − w(Xi, Ai)f (Ri))
i

n

≤ EPβ,R exp 2λ sup
f ∈F(n)

ξiw(Ai, Xi)f (Ri)
i

n

= EPβ,R sup exp 2λ
f ∈F(n)

ξiw(Ai, Xi)f (Ri)
i

{Xi, Ai, Ri}ni {Xi, Ai, Ri}ni {Xi, Ai, Ri}ni

with R a Rademacher measure on a set of Rademacher random variable {ξi} a Rademacher random variable.

Next, permute the indices i such that R1 ≤ . . . Ri . . . ≤ Rn.

Consider

a

function

f (r)

=

1 n

1{r≤t}.

For such a function,

n i

ξiw(Ai,

Xi)f (Ri)

is

equal

to

• 0 if t < mini{Ri}ni ,

•

1 n

j i

w(Ai, Xi)ξi

when

Rj

≤

t

<

Rj+1

for

a

j

∈

{1, . . . , n

−

1},

•

1 n

n i

w(Ai, Xi)ξi

otherwise.

Then,

sup exp 2λ
f ∈F(n)

n

ξiw(Ai, Xi)f (Ri)

i

2λ

= max exp

,j

n

j
w(Ai, Xi)ξi
i

j

2λ

= max exp

j

n

1 w(Ai, Xi)ξi

{

j i

w(Ai,Xi)ξi≥0}

i

+ exp

2λ j − n w(Ai, Xi)ξi
i

1{

j i

w(Ai,Xi)ξi<0}

25

= max
j

2λ j exp
ni + max
j

w(Ai, Xi)ξi 2λ
exp − n

1{

j i

w(Ai,Xi)ξi≥0}

j
w(Ai, Xi)ξi 1{
i

j i

w(Ai,Xi)ξi<0}

Which gives us the inequality

EPβ exp λ sup FIS(t) − F (t)
t∈R

≤ 2EPβ,R max exp
j

2λ j n w(Ai, Xi)ξi
i

1{

j i

w(Ai,Xi)ξi≥0}

(9)

Now we are left to bound the right hand side of (9). Using Lemma B.1, for the right hand side of the (9) we have,

EPβ,R exp

2λ j

max nj

w(Ai, Xi)ξi

i

1{maxj

j i

w(Ai,Xi)ξi≥0}

2λ j

= Pβ{max jn

w(Ai, Xi)ξi ≥ 0}

i

∞

2λ j

+ λ exp(λt)P{max

w(Ai, Xi)ξi ≥ t}dt

0

j ni

2λ j

≤ Pβ{max jn

w(Ai, Xi)ξi ≥ 0}

i

∞

2λ

+ 2λ exp(λt)P{

w(Ai, Xi)ξi ≥ t}dt

(10)

0

ni

Note that similarly we have,

EPβ,R exp

2λ n w(Ai, Xi)ξi
i

1{ i w(Ai,Xi)ξi≥0}

2λ

∞

2λ

=Pβ {

w(Ai, Xi)ξi ≥ 0} + λ exp(λt)P{

w(Ai, Xi)ξi ≥ t}dt

(11)

ni

0

ni

Putting these two statements, i.e., (10), and (11) together, and applying the result of Lemma B.2, we have,

EPβ,R exp

2λ j

max jn

w(Ai, Xi)ξi

i

1{

j i

w(Ai,Xi)ξi≥0}

26

2λ j

≤ Pβ{max jn

w(Ai, Xi)ξi ≥ 0}

i

+ 2EPβ,R exp

2λ n w(Ai, Xi)ξi
i

1{ i w(Ai,Xi)ξi≥0}

2λ − 2Pβ{ n w(Ai, Xi)ξi ≥ 0}
i

≤ 2EPβ,R exp

2λ n w(Ai, Xi)ξi
i

1{ i w(Ai,Xi)ξi≥0}

≤ 2EPβ,R exp

2λ n w(Ai, Xi)ξi
i

Note

that

2 n

w

(

A

i

,

Xi

)

ξ

i

is

a

mean

zero

random

variable

with

values

in

[− n2 wmax,

n2 wmax].

Therefore,

it is a sub-Gaussian random variable with sub-Gaussian constant as

2 n

2 wm2 ax.

Using this, we have,

2 n

i w(Ai, Xi)ξi is n4 wm2 ax sub-Gaussian random variable. Therefore, we have,

EPβ,R exp

2λ j

max jn

w(Ai, Xi)ξi

i

1{

j i

w(Ai,Xi)ξi≥0}

2λ ≤ 2EPβ,R exp n ≤ 2 exp λ2 n2 wm2 ax

w(Ai, Xi)ξi
i

Putting this with the (9), we have

EPβ exp λ sup FIS(t) − F (t)
t∈R
Using Markov inequality we have

2λ j

≤ 2EPβ,R

exp

max jn

w(Ai, Xi)ξi

i

≤ 4 exp λ2 n2 wm2 ax

1{

j i

w(Ai,Xi)ξi≥0}

Pβ sup FIS(t) − F (t) ≥
t∈R

= Pβ exp λ sup FIS(t) − F (t)
t∈R
≤ 4 exp λ2 n2 wm2 ax exp(−λ )
= 4 exp λ2 n2 wm2 ax − λ

≥ exp(λ )

27

This holds for any choice of λ > 0, resulting in

Pβ sup FIS(t) − F (t) ≥
t∈R

≤ inf 4 exp λ2 2 w2 − λ

λ>0

n max

= 4 exp

−n 2 8wm2 ax

Using this, we have

Pβ sup FIS(t) − F (t) ≤
t∈R

8wm2 ax log 4

n

δ

≥ 1 − δ.

Bernstein

style:

To bound this EPβ,R

exp

2λ n

i w(Ai, Xi)ξi

now we use Bernstein’s. As

discussed, the random variable w(Ai, Xi)ξi is in [−wmax, wmax]. However, if we look at its variance,

we have EPβ,R

w

(A

i

,

Xi

)

2

ξ

2 i

= EPβ,R w(Ai, Xi)2

which is the second order Rényi divergence

d(P||Pβ). Therefore, for 0 < λ < 2wnmax , we have

EPβ,R exp

2λ n w(Ai, Xi)ξi
i

Using the Markov inequality, we have,

= EPβ,R exp
i

2λ n w(Ai, Xi)ξi

≤ exp
i

λ2 4d(Pn|2|Pβ )

2

1

−

λ

2 n

wmax

= exp

nλ2 4d(Pn|2|Pβ)

2

1

−

λ

2 n

wmax

Pβ sup FIS(t) − F (t) ≥
t∈R
Setting λ = 2wmnax +n 4d(Pn|2|Pβ) , we have,

= 4 exp

nλ2 4d(Pn|2|Pβ) − λ

2

1

−

λ

2 n

wmax

Pβ sup FIS(t) − F (t) ≥
t∈R





−2

≤ 4 exp 



2 n2 wmax + n 4d(Pn|2|Pβ)

−n 2 = 4 exp
4wmax + 8d(P||Pβ)

which results in,

28



4wmax

log(

4 δ

)

Pβ sup FIS(t) − F (t) ≤

+2

t∈R

n

 2d(P||Pβ) log( 4δ )  ≥ 1 − δ.
n

Finally, we note that since supt |FIS-clip(t) − F (t)| ≤ supt |FIS(t) − F (t)|, the above results for FIS also hold for FIS-clip.

Auxiliary Lemmas
Lemma B.1. For any random variable X, with probability measure P, we have
∞
E exp(λX)1{X≥0} = P{X ≥ 0} + λ exp(λt)P{X ≥ t}dt.
0

Proof. for any random variable X, with probability measure P, we have

E exp(λX)1{X≥0} = E

X
exp(0) + λ exp(λt)dt 1{X≥0}
0

X
= E 1{X≥0} exp(0) + E 1{X≥0}λ exp(λt)1{X≥0}dt
0

X
= P{X ≥ 0} + E λ exp(λt)1{X≥0}dt
0

∞

= P{X ≥ 0} + λ exp(λt)P{X ≥ t}dt.

(12)

0

Lemma B.2. For γ > 0, we have,

j

n

Pβ max w(Ai, Xi)ξi ≥ γ ≤ 2Pβ

w(Ai, Xi)ξi ≥ γ

(13)

j

i

i

Proof. Consider events Ej := { these deﬁnitions, we have,

j i

w(Ai,

Xi)ξi

≥

γ,

l i

w(Ai

,

Xi

)ξi

<

γ, ∀l

<

j}

with

E0

:=

∅.

Using

j

{max w(Ai, Xi)ξi ≥ γ} ⊂ Ej

j

i

j

Also,





Ej
j

{ w(Ai, Xi)ξi ≥ 0} ⊂ { w(Ai, Xi)ξi ≥ γ}

i>j

i

29

Also note that





1 Pβ  w(Ai, Xi)ξi ≥ 0 ≥ 2
i>j

since this quantity is mean zero and symmetric. Also note that the event independent of Ej.
Using these, we have,

i>j w(Ai, Xi)ξi is

 Pβ Ej







{ w(Ai, Xi)ξi ≥ 0} = Pβ [Ej] Pβ { w(Ai, Xi)ξi ≥ 0} ≥ Pβ [Ej] i>j i>j 2

As a result we have,





Pβ

w(Ai, Xi)ξi ≥ γ ≥ Pβ  Ej { w(Ai, Xi)ξi ≥ 0}

i

j

i>j





which concludes the statement.

= Pβ Ej
j
≥ Pβ [Ej] j2
Pβ {maxj ≥

{ w(Ai, Xi)ξi ≥ 0}
i>j

j i

w(Ai,

Xi)ξi

≥

γ}

2

30

B.2 Doubly Robust (DR) Estimators (Section 5.3)
B.2.1 Proof: Bias and Variance of DR CDF Estimate Proof of Lemma 5.2. The expectation of the DR estimator (22) is as follows:

EPβ FDR(t) = EPβ w(A, X)1{R≤t} + EPβ G(t; X, π) − w(A, X)G(t; X, A)

= F (t) + EPβ = F (t) + EPβ = F (t).

G(t; X, π) − EPβ [w(A, X)G(t; X, A)|X] G(t; X, π) − G(t; X, π)

Next, we derive the variance.
VPβ FDR(t) = n1 VPβ w(A, X) 1{R≤t} − G(t; X, A) + G(t; X, π)
= n1 EPβ w(A, X)2σ2(t; X, A) 1
+ n VPβ w(A, X) G(t; X, A) − G(t; X, A) + G(t; X, π) = n1 EPβ w(A, X)2σ2(t; X, A) + n1 VPβ EPβ [w(A, X)G(t; X, A)|X]
1 + n EPβ VPβ w(A, X) G(t; X, A) − G(t; X, A) |X
The ﬁrst equality follows from applying the law of total variance, noting that the variance VPβ G(t; X, A)|X, A = 0, and using the deﬁnitions of G and σ2. The second equality again applies the law of total variance.

B.2.2 Proof: Error Bound of DR CDF Estimate Proof of Theorem 5.2. Recall that the DR estimator FDR(t) is deﬁned as
FDR(t) = n1 n w(ai, xi) 1{ri≤t} − G(t; xi, ai) + G(t; xi, π) i=1
where G(t; x, π) = a π(a|x)G(t; x, a). We can decompose the error of the DR estimator as:

EPβ sup |FDR(t) − F (t)|
t

= EPβ sup
t

n1 n w(Ai, Xi) 1{Ri≤t} − G(t; Xi, Ai) + G(t; Xi, π) i=1

− F (t)

≤ EPβ sup
t

n1 n w(Ai, Xi)1{Ri≤t} − F (t) + n1 n G(t; Xi, π) − w(Ai, Xi)G(t; Xi, Ai)

i=1

i=1

≤ E sup 1 n w(A , X )1

1n

− F (t) + sup

G(t; X , π) − w(A , X )G(t; X , A ) .

Pβ t n

i i {Ri≤t}

tn

i

ii

ii

i=1

i=1

31

We have already bounded the ﬁrst term in Theorem 5.1, and Lemma B.3 bounds the second term. Then in total, we have


Pβ sup FDR(t) − F (t) ≥
t

8wm2 ax log 4 +

n

δ



32wm2 ax log (2n)1/2  ≤ 2δ

n

wmaxδ

Simplifying,





Pβ sup FDR(t) − F (t) ≥ 72wm2 ax log 4n1/2  ≤ 2δ (14)

t

n

δ

which gives us our error bound for the DR estimator FDR.
As mentioned previously, however, FDR may not be monotone, and in practice we must use a monotone transformation of the estimator. Consider a monotone transformation M of FDR that is a simple accumulation function, e.g. ∀t,

M FDR(t) = max FDR(t )
t ≤t

Now we want to bound the error between the monotonized estimate M FDR(t) and F . Using our error bound in (14), let = 72wnm 2 ax log 8nδ1/2 . Then with probability at least 1 − δ, for all t ∈ R,

max |FDR(t) − F (t)| ≤ .
t
On this event, ∀t there exists some t ≤ t for which max FDR(t ) − F (t) = FDR(t ) − F (t)
t ≤t
Using the fact that F is monotone thus F (t ) ≤ F (t), when FDR(t ) ≥ F (t) we have FDR(t ) − F (t) ≤ FDR(t ) − F (t ) ≤
Similarly, when FDR(t ) ≤ F (t), F (t) − FDR(t ) ≤ F (t) − FDR(t) ≤
Putting these two inequalities together, we have max M FDR (t) − F (t) ≤ .
t

The theorem statement, which applies to the clipped monotone transformation, follows from the fact

that

max min M FDR (t), 1 − F (t) ≤ max M FDR (t) − F (t) .

t

t

32

Lemma B.3. Let G(t; x, a) be a valid conditional CDF for all x ∈ X , a ∈ A, and let w : A × X → R be the importance sampling weights. Then for δ ∈ (0, 1],



1n

1n

Pβ sutp n G(t; Xi, π) − n w(Ai, Xi)G(t; Xi, Ai) ≥

i=1

i=1



32wm2 ax log (2n)1/2  ≤ δ.

n

wmaxδ

where G(t; x, π) = EP[G(t; x, A)|x].

Proof. Since G is a valid CDF, we apply Lemma B.4 to G. Consider a function of the form
ζ(t; s1, ..., sm) = m1 m 1{si≤t} j=1
The function ζ can be seen as a stepwise CDF function, where each step is 1/m and occurs at points {sj }mj=1.
Lemma B.4 approximates G using such 1/m-stepwise CDFs. For each context x and action a, let s1x,a, ..., smx,a ∈ Qm be the points chosen according to the deterministic procedure in Lemma B.4, such that the following inequality holds:

sup G(t; x, a) − ζ t; {sj }m

1 ≤.

(15)

t

x,a j=1

2m

Next, consider the class of functions G(m) := ζ(s1, ..., sm) := 1 m

m
1{sj≤t} : ∀t ∈ R,
j=1

∈ {−1, +1}; {sj}mj=1 ∈ Qm

Note that, ζ is a subset of the function class G(m), e.g. ζ t; {sjx,a}mj=1 ∈ G(m).

1n

1n

sup tn

w(Ai, Xi)G(t; Xi, Ai) − n

G(t; Xi, π)

i=1

i=1

1n

1n

= sup tn

w(Ai, Xi)G(t; Xi, Ai) − n

EP G(t; Xi, A)|Xi

i=1

i=1

1n

1n

= sup tn

w(Ai, Xi)G(t; Xi, Ai) − n

EPβ w(Xi, A)G(t; Xi, A)|Xi

i=1

i=1

1n

j

m

1n

j

m

1

≤ sup tn

w(Ai, Xi)ζ

t; {sXi,Ai }j=1

− n

EPβ w(A, Xi)ζ t; {sXi,A}j=1

Xi

+ m

i=1

i=1

≤ sup
ζ∈G(m)

1n

j

m

1n

n w(Ai, Xi)ζ({sXi,Ai }j=1) − n EPβ

i=1

i=1

w(A, Xi)ζ({sjXi,A}mj=1) Xi

1 +
m

33

where the second line uses the deﬁnition of G(t; Xi, π), the third line uses a change of measure through the importance sampling weight w, the fourth line uses (B.2.2), and the last line uses the fact that, conditioned on {sjx,a}mj=1, the function ζ is a member of G(m).
We can now upper bound the RHS. Going forward, we refer to ζ({sjX,A}mj=1) as ζ(X, A) for short. Then for λ > 0 we have:

EPβ exp

λ sup
ζ∈G(m)

1n

1n

n w(Ai, Xi)ζ(Xi, Ai) − n EPβ w(A, Xi)ζ(Xi, A) Xi

i=1

i=1

= EPβ exp

λ sup
ζ∈G(m)

1n

n

n EPβ w(Ai, Xi)ζ(Xi, Ai) − w(Ai, Xi)ζ(Xi, Ai) {Xi, Ai}i=1

i=1

1n

n

≤ EPβ exp λEPβ ζ∈sGu(pm) n i=1 w(Ai, Xi)ζ(Xi, Ai) − w(Ai, Xi)ζ(Xi, Ai) {Xi, Ai}i=1

1n ≤ EPβ exp λ ζ∈sGu(pm) n i=1 w(Ai, Xi)ζ(Xi, Ai) − w(Ai, Xi)ζ(Xi, Ai)

1n ≤ EPβ,R exp 2λ ζ∈sGu(pm) n i=1 ξiw(Ai, Xi)ζ(Xi, Ai)

= EPβ,R

sup exp
ζ∈G(m)

1n 2λ n ξiw(Ai, Xi)ζ(Xi, Ai)
i=1







mn

= EPβ,R stu, p exp 2λ nm j=1 i=1 ξiw(Ai, Xi)1{sjXi,Ai ≤t}

(16)

where {A }ni are the ghost variables, the second to last inequality uses symmetrization (Lemma B.5), and the last line uses the deﬁnition of ζ(Xi, Ai) = ζ(s1Xi,Ai, ..., smXi,Ai).

Now, for each j, permute the indices i such that sjX ,A ≤ ... ≤ sjX ,A ≤ ... ≤ sjX ,A .

j(1) j(1)

j(i) j(i)

j(n) j(n)

Then, for a given j, consider the function

n

1 ξj(i)w(Aj(i), Xj(i)) {sj

≤t},

i=1

Xj (i) ,Aj (i)

which equals

1. 0 if t < sjX ,A , j(1) j(1)

2.

k i=1

w(Aj(i),

Xj(i))ξj(i)

if

there

exists

k

∈

{1, ..., n − 1}

such

that

sjX

,A

≤t≤

j(k) j(k)

s , j
Xj (k)+1 ,Aj (k)+1

3.

n i=1

w(Aj(i), Xj(i))ξj(i)

otherwise.

34

Then the RHS of (16) equals







mn

EPβ,R stu, p exp 2λ nm j=1 i=1 ξiw(Ai, Xi)1{sjXi,Ai ≤t}







mk

=

EPβ ,R

max
k,

exp

2λ

nm

ξj(i)w(Aj(i), Xj(i))

j=1 i=1

k

≤ EPβ,R

max exp
j,k,

2λ n

ξj(i)w(Aj(i), Xj(i)) .

i=1

Further, we have that

k

max exp 2λ

j,k,

n

ξj(i)w(Aj(i), Xj(i))

i=1

2λ k

= max exp

w(A , X )ξ

1

j,k

n

j(i) j(i) j(i) {

i

+ exp

2λ −
n

k
w(Aj(i), Xj(i))ξj(i)
i

2λ k

≤ 2 max exp

w(A , X )ξ

1

j,k

n

j(i) j(i) j(i) {

i

k i

w(Aj (i) ,Xj (i) )ξj (i) ≥0}

1{

k i

w(aj (i) ,xj (i) )ξj (i) <0}

. k
i

w(Aj (i) ,Xj (i) )ξj (i) ≥0}

Putting it together, we have that

1n

1n

EPβ exp λ ζ∈sGu(pm) n i=1 w(Ai, Xi)ζ(Xi, Ai) − n i=1 EA∼π(·|Xi) [ζ(Xi, A)|Xi]

≤ 2EPβ,R max exp
j,k

2λ k

n

w(Aj(i), Xj(i))ξj(i)

i

1{

k i

w(Aj (i) ,Xj (i) )ξj (i) ≥0}

(17)

Now we are left to bound the RHS of (17). Using Lemma B.1,

EPβ,R max exp
j,k

2λ k

n

w(Aj(i), Xj(i))ξj(i)

i

1{maxk

k i

w(Aj (i) ,Xj (i) )ξj (i) ≥0}

≤ Pβ

2λ k

max kn

w(Aj(i), Xj(i))ξj(i) ≥ 0

i

+ 2λ
j

∞

2λ n

exp(λt)P
0

n w(Aj(i), Xj(i))ξj(i) ≥ t dt.
i=1

Similarly, for any j, we have

EPβ,R exp

2λ n

n

w(Aj(i), Xj(i))ξj(i)

i

1{

k i

w(Aj (i) ,Xj (i) )ξj (i) ≥0}

35

= Pβ

2λ n n w(Aj(i), Xj(i))ξj(i) ≥ 0
i

∞
+ λ exp(λt)P
0

2λ n n w(Aj(i), Xj(i))ξj(i) ≥ t dt
i=1

Putting these two together, we have

EPβ,R max exp
j,k

2λ k

n

w(Aj(i), Xj(i))ξj(i)

i

1{

k i

w(Aj (i) ,Xj (i) )ξj (i) ≥0}

≤ Pβ
j

2λ k

max kn

w(Aj(i), Xj(i))ξj(i) ≥ 0

i

− 2 Pβ
j

2λ n n w(Aj(i), Xj(i))ξj(i) ≥ 0
i

+ 2 EPβ,R exp
j

2λ n

n

w(Aj(i), Xj(i))ξj(i)

i

1{

n i

w(Aj (i) ,Xj (i) )ξj (i) ≥0}

≤ 2 EPβ,R exp
j

2λ n

n

w(Aj(i), Xj(i))ξj(i)

i

1{

n i

w(Aj (i) ,Xj (i) )ξj (i) ≥0}

≤ 2mEPβ,R exp

2λ n

n

w(Aj(i), Xj(i))ξj(i)

i

≤ 2m exp 2λ2wm2 ax n

where the last inequality uses the fact that ξ is a Rademacher random variable, and w(A, X) ≤ wmax. Finally, using Markov’s inequality,

1n

1

Pβ sutp n G(t; Xi, π) − w(Ai, Xi)G(t; Xi, Ai) ≥ + m

i=1

1n

1n

≤ Pβ exp λ sutp n w(Ai, Xi)ζ(Xi, Ai) − n EPβ [ζ(Xi, A)|Xi]

i=1

i=1

≤ 4m exp 2λ2wm2 ax − λ n

≥ exp(λ )

Because this holds for any λ > 0, we can minimize the RHS over λ:

1n

1

Pβ sup

G(t; Xi, π) − w(Ai, Xi)G(t; Xi, Ai) ≥ +

≤ inf 4m exp 2λ2wm2 ax − λ

t n i=1

m

λ>0

n

−n = 4m exp 8w2 .
max

Then we have 1n
Pβ sutp n G(t; Xi, π) − w(Ai, Xi)G(t; Xi, Ai) ≥
i=1

8wm2 ax log 4m + 1 ≤ δ.

n

δm

36

Setting m = n/8wm2 ax gives the theorem statement:
 1n
Pβ sutp n G(t; Xi, π) − w(Ai, Xi)G(t; Xi, Ai) ≥
i=1



32wm2 ax log (2n)1/2  ≤ δ.

n

wmaxδ

Auxiliary Lemmas
Lemma B.4. For any ζ, a non-decreasing function with support [0, D], there exists m points s1....sm ∈ Qm such that for a function of the form,

ζ(t; s1, ..., sm) = m1 m 1{sj≤t}, j=1
the following inequality holds: 1
ζ − ζ ∞ ≤ 2m .

∀t ∈ R

Proof of Lemma B.4. Uniformly partition the interval [0, D] to m partitions, with partition points

{ Dj }mj=0. We construct the set {sj}mj=1 using the following procedure. For any j ∈ {1, . . . , m} and the

corresponding

partition

point

jD−1 ,

let

sj

∈Q

be

a

point

such

that

either

limt→sj

ζ(t) =

j−1 m

+

1 2m

−

or

limt→sj

ζ(t) =

j−1 m

+

1 2m

(e.g.,

as

illustrated

in

Figure

3).

Then

for

any

t,

ζ (t)

is

21m -close

to

ζ (t).

+

$

$

1

2/# 1/#
ℝ

Figure 3: Approximating monotonic function ζ with ζ.

Lemma B.5. For the function class G deﬁned in Appendix B.2.2, we have for any λ > 0 that

1n EPβ exp λ ζ∈sGu(pm) n i=1 w(Ai, Xi)ζ(Xi, Ai) − w(Ai, Xi)ζ(Xi, Ai)

≤ EPβ,R

sup exp
ζ∈G(m)

1n 2λ n ξiw(Ai, Xi)ζ(Xi, Ai)
i=1

37

where contexts and actions X, A, A ∼ Pβ, and Rademacher random variables ξi ∼ R.

Proof. For each i = 1, ..., n, and let ξi be i.i.d. Rademacher random variables. Set

A+ = Ai, if ξi = 1

i

Ai, if ξi = −1

A− = Ai, if ξi = 1

i

Ai, if ξi = −1

We

have

that,

conditioned

on

Xi,

(A

+ i

,

A−i

)

=d

(Ai, Ai).

Then

1n EPβ exp λ ζ∈sGu(pm) n i=1 w(Ai, Xi)ζ(Xi, Ai) − w(Ai, Xi)ζ(Xi, Ai)

1n

+

+

−

−

= EPβ exp λ ζ∈sGu(pm) n i=1 w(Ai , Xi)ζ(Xi, Ai ) − w(Ai , Xi)ζ(Xi, Ai )

1n = EPβ,R exp λ ζ∈sGu(pm) n i=1 ξi w(Ai, Xi)ζ(Xi, Ai) − w(Ai, Xi)ζ(Xi, Ai)

Our last step is to bound the last line of the above display.

1 = EPβ,R exp λ ζ∈sGu(pm) 2

2n

2n

n ξiw(Ai, Xi)ζ(Xi, Ai) − n ξiw(Ai, Xi)ζ(Xi, Ai)

i=1

i=1

1

2n

≤ 2 EPβ,R exp λ ζ∈sGu(pm) n i=1 ξiw(Ai, Xi)ζ(Xi, Ai)

1

2n

+ 2 EPβ,R exp λ ζ∈sGu(pm) n i=1 (−ξi)w(Ai, Xi)ζ(Xi, Ai)

2n = EPβ,R exp λ ζ∈sGu(pm) n i=1 ξiw(Ai, Xi)ζ(Xi, Ai)

Lemma B.6. Let G(t; X, π) = EP[1{R≤t}|X] be the conditional CDF of returns for all x ∈ X . Then
for δ ∈ (0, 1],

1n

32 (2n)1/2

Pβ sutp n G(t; Xi, π) − F (t) ≥ n log δ ≤ δ.

i=1

Proof. Since G is a valid CDF, we apply Lemma B.4 to G. Consider a function of the form
ζ(t; s1, ..., sm) = m1 m 1{si≤t} j=1

38

The function ζ can be seen as a stepwise CDF function, where each step is 1/m and occurs at points {sj }mj=1.

Lemma

B.4

approximates

G

using

such

1/m-stepwise

CDFs.

For

each

context

x,

let

s1x

,

...,

s

m x

∈

Qm

be the points chosen according to the deterministic procedure in Lemma B.4, such that the following

inequality holds:

sup G(t; x, π) − ζ t; {sj }m

1 ≤.

(18)

t

x j=1

2m

Next, consider the class of functions G(m) := ζ(s1, ..., sm) := 1 m

m
1{sj≤t} : ∀t ∈ R,
j=1

∈ {−1, +1}; {sj}mj=1 ∈ Qm

Note that, ζ is a subset of the function class G(m), e.g. ζ Then our problem becomes

t

;

{

sjx

}

m j=1

∈ G(m).

1n

sup tn

G(t; Xi, π) − F (t)

i=1

1n

= sup tn

G(t; Xi, π) − EPβ

i=1

1n G(t; X, π)
n i=1

1n

≤ sup

ζ

t n i=1

t; {sjXi }mj=1

− EPβ

1n ζ
n i=1

t; {sjXi }mj=1

1n

≤ sup

ζ

ζ∈G(m) n i=1

{sjXi }mj=1

− EPβ

1n ζ
n i=1

{sjXi }mj=1

1 +
m 1 + m

We can now upper bound the RHS. Going forward, we refer to ζ({sjX }mj=1) as ζ(X) for short. Then for λ > 0 we have:

EPβ exp

λ sup
ζ∈G(m)

1n n ζ(Xi) − EPβ
i=1

1n n ζ(Xi)
i=1

= EPβ exp

λ sup
ζ∈G(m)

1n n EPβ
i=1

ζ(Xi) − ζ(Xi) {Xi}ni=1

1n

n

≤ EPβ exp λEPβ ζ∈sGu(pm) n i=1 ζ(Xi) − ζ(Xi) {Xi}i=1

1n ≤ EPβ exp λ ζ∈sGu(pm) n i=1 ζ(Xi) − ζ(Xi, Ai)

39

1n = EPβ,R exp λ ζ∈sGu(pm) n i=1 ξi ζ(Xi) − ζ(Xi)

1n ≤ EPβ,R exp 2λ ζ∈sGu(pm) n i=1 ξiζ(Xi)

= EPβ,R

sup exp
ζ∈G(m)

1n 2λ n ξiζ(Xi)
i=1







mn

= EPβ,R stu, p exp 2λ nm j=1 i=1 ξi1{sjXi ≤t}

(19)

where {X }ni are the ghost variables, and the last line uses the deﬁnition of ζ(Xi) = ζ(s1Xi, ..., smXi).

Now, for each j, permute the indices i such that sjX ≤ ... ≤ sjX ≤ ... ≤ sjX . Then, for a given

j(1)

j(i)

j(n)

j, consider the function
n

1 ξj(i) {sj ≤t},

i=1

Xj(i)

which equals

1. 0 if t < sjX , j(1)

2.

k i=1

ξj(i)

if

there

exists

k

∈

{1, ..., n

−

1}

such

that

sjX

≤ t ≤ sjX

,

j(k)

j(k)+1

3.

n i=1

ξj(i)

otherwise.

Then the RHS of (19) equals







mn

EPβ,R stu, p exp 2λ nm j=1 i=1 ξi1{sjXi ≤t}







mk

=

EPβ ,R

max
k,

exp

2λ

nm

ξj(i)

j=1 i=1

k

≤ EPβ,R

max exp
j,k,

2λ n

ξj(i) .

i=1

Further, we have that

max exp
j,k,
= max
j,k

2λ n
exp

k

ξj(i)

i=1

2λ k

n

ξj(i)

i

1{ ki ξj(i)≥0} + exp

2λ k

− n

ξj(i)

i

1{

k i

ξj (i) <0}

40

≤ 2 max exp
j,k

2λ k

n

ξj(i)

i

Putting it together, we have that

1 . {

k i

ξj (i) ≥0}

1n

1n

EPβ exp λ ζ∈sGu(pm) n i=1 ζ(Xi) − EPβ n i=1 ζ(Xi)

≤ 2EPβ,R max exp
j,k

2λ k

n

ξj(i)

i

1{

k i

ξj (i) ≥0}

Now we are left to bound the RHS of (20). Using Lemma B.1,

EPβ,R max exp
j,k

2λ k

n

ξj(i)

i

1{maxk

k i

ξj (i) ≥0}

≤ Pβ

2λ k

max kn

ξj(i) ≥ 0

i

+ 2λ
j

∞

2λ n

exp(λt)P
0

n ξj(i) ≥ t dt.
i=1

Similarly, for any j, we have

EPβ,R exp

2λ n

n

ξj(i)

i

1{

k i

ξj (i) ≥0}

= Pβ

2λ n n w(Aj(i), Xj(i))ξj(i) ≥ 0
i

∞
+ λ exp(λt)P
0

2λ n n ξj(i) ≥ t dt
i=1

Putting these two together, we have

EPβ,R max exp
j,k

2λ k

n

ξj(i)

i

1{

k i

ξj (i) ≥0}

≤ Pβ
j

2λ k

max kn

ξj(i) ≥ 0

i

− 2 Pβ
j

2λ n n ξj(i) ≥ 0
i

+ 2 EPβ,R exp
j

2λ n

n

ξj(i)

i

1{

n i

ξj (i) ≥0}

≤ 2 EPβ,R exp
j

2λ n

n

ξj(i)

i

1{

n i

ξj (i) ≥0}

≤ 2mEPβ,R exp 2λ2
≤ 2m exp n

2λ n

n

ξj(i)

i

(20)

41

where the last inequality uses the fact that ξ is a Rademacher random variable. Finally, using Markov’s inequality,

1n

1

Pβ sutp n G(t; Xi, π) − F (t) ≥ + m

i=1

1n

1n

≤ Pβ exp λ sutp n ζ(Xi) − EPβ n ζ(Xi)

i=1

i=1

2λ2

≤ 4m exp

−λ

n

≥ exp(λ )

Because this holds for any λ > 0, we can minimize the RHS over λ:

1n

1

2λ2

Pβ sup n

G(t; Xi, π) − F (t) ≥ + ≤ inf 4m exp

−λ

m

λ>0

n

t

i=1

−n

= 4m exp

.

8

Then we have

1n Pβ sutp n G(t; Xi, π) − F (t) ≥
i=1

8 4m 1 log + ≤ δ.
n δm

Setting m = n/8 gives the theorem statement:

1n

32 (2n)1/2

Pβ sutp n G(t; Xi, π) − F (t) ≥ n log δ ≤ δ.

i=1

C Proofs for Risk Functional Estimation (Section 6)
Proof of Theorem 6.1. By the deﬁnition of L-Lipschitz risk functionals, for the CDFs F and F ,
|ρ(F ) − ρ(F )| ≤ L F − F ∞ ≤L
with probability at least 1 − δ, where the last line uses the fact that F is -close to F with probability at least 1 − δ.

42

D Risk Estimation with Unknown Behavior Policy
We begin this section with a consideration of estimators when the behavior policy is unknown, and must be modeled or estimated, which we call β. We ﬁrst deﬁne the IS, DR, and DI estimators using β, then derive their bias and variance expressions. To diﬀerentiate between the estimator that use β and the estimators that use β, we call the latter F while continuing to call the former F .
The proofs of bias and variance begins with derivations for the DR estimator with estimated policy, from which the bias and variance of the remaining estimators can be derived as special cases. Let β be the estimated behavior policy, and let w(a, x) := π(a|x) be the importance weight with
β(a|x)
estimated policy. Then the importance sampling (IS) estimator is given by
FIS(t) := n1 n w(ai, xi)1{ri≤t} (21) i=1
Then doubly robust (DR) estimator is:
FDR(t) := n1 n w(ai, xi) 1{ri≤t} − G(t; xi, ai) + G(t; xi, π) (22) i=1
And the direct method (DI) estimator is still deﬁned to be 1n
FDI(t) := n G(t; xi, π) (23)
i=1
Note that the direct estimator does not depend on the behavior policy, and thus we continue to call it FDI.

D.1 Bias and Variance
Next, we analyze the bias and variance of these estimators. Deﬁne ∆(a, x, t) to be the additive error between G and the model G, and deﬁne δ(x, a) to be the multiplicative error of the estimate β, that is:
∆(t; x, a) := G(t; x, a) − G(t; x, a),
δ(x, a) := 1 − β(a|x)/β(a|x).

Note that when β is known or β = β for all x, a, δ(x, a) = 0, The bias of the IS estimator then given in Lemma D.1, in terms of δ and the conditional reward distribution G.

Lemma D.1 (Bias and Variance of IS Estimator with β.). The expectation of the IS estimator is

EPβ [FIS(t)] = F (t) + EP[δ(A, X)G(t; X, π)]

When β(a|x) = β(a|x) for all a, x, the IS estimator is unbiased and EPβ [FIS(t)] = F (t). Further, the variance is

VPβ [FIS(t)] = n1 EP (1 − δ(A, X))2 σ2(t; X, A) + n1 VP [EP [(1 − δ(A, X))G(t; X, A)|X]]

1

+ n EP VPβ [w(A, X)G(t; X, A)|X]

(24)

43

The expression for variance is broken down into three terms. The ﬁrst represents randomness in the rewards, and the second represents variance from the aleatoric uncertainty due to randomness over contexts X. The ﬁnal term represents variance arising from using importance sampling, and is proportional to the true CDF of conditional rewards G.

The following lemma, similarly, derives the bias and variance for the DR estimator:

Lemma D.2 (Bias and Variance of DR Estimator with β.). The pointwise expectation of the DR estimator is
EPβ [FDR(t)] = F (t) + EP[δ(X, A)∆(t; X, A)]
Further, when there is perfect knowledge of the behavior policy β, e.g. βˆ(a|x) = β(a|x) for all a, x, the DR estimator is unbiased and
EPβ [FDR(t)] = F (t)
The variance of the doubly robust estimator is given by

VPβ [FDR(t)] = n1 EP (1 − δ(A, X))2 σ2(t; X, A) + n1 VP [EP [δ(A, X)∆(t; X, A) + G(t; X, A)|X]]

1

+ n EP VPβ [w(A, X)∆(t; X, A)|X]

(25)

Because the DR estimator takes advantage of both policy and reward estimates, it is unbiased whenever either the estimated policy or estimated reward is unbiased. Further, when we have access to the true behavior policy β and w = w, it retains the unbiasedness of the IS estimator.
Compared to the IS estimator, the DR estimator may also have pointwise reduced variance. When the variances of the IS estimator (24) and the DR estimator (25) are compared, the ﬁrst term is identical, and the middle term is of similar magnitude because the randomness in contexts X is endemic. The third term is the primary diﬀerence. For the IS estimator, it is proportional to G, but for the DR estimator, it is proportional to the error ∆ between the estimated conditional CDF G and the true G. Thus, this term can be much larger in the IS estimator when w is large and the error ∆ is smaller than G. This demonstrates that the DR estimator retains the low bias of the IS estimator, but has the advantage of reduced variance.
Next, Lemma D.3 gives the bias and variance of the DI estimator, which is directly related to the bias and variance of the conditional distribution model G.
Lemma D.3 (Bias of DI Estimator with β.). The bias is

Ex,a∼β,r[FDI(t)] = F (t) + EP[∆]

and the variance is

1 V[FDI(t)] = n VP G(t; X, π) + ∆].

While the DI estimator has lower variance than both the IS and DR estimators, it suﬀers from potentially high bias from G. Unlike the other two estimators, it is biased even when β is a perfect estimate of β, which in practice is undesirable. Though the DI estimator has low bias when G is a good model of the condition reward distribution, it is often much easier to form accurate models of β than of G.

44

Proofs: Bias and Variance
We begin by proving the bias and variance expressions of the DR estimator with β. The bias and variance of the other estimators can be derived as special cases, which we show later.

Proof of Lemma D.2. First, we take the expectation of the DR estimator (22) with respect to Pβ:

EPβ F (t) = EPβ βπ((AA||XX)) 1{R ≤ t} + EPβ

π(A|X ) − π(A|X) G(t; X, A)
β(A|X) a

= EP β(A|X) 1{R ≤ t} + EP
β (A|X )

β (A|X ) − 1 G(t; X, A)
β (A|X )

= F (t) + EP

β(A|X) − 1 1{R ≤ t} + EP
β (A|X )

β (A|X ) − 1 G(t; X, A)
β (A|X )

= F (t) + EP

β (A|X ) −1
β (A|X )

G(t; X, A) − G(t; X, A)

= F (t) + EP δ(A, X)∆(t; X, A)

When β = β for all a, x, we have δ = 0, giving the unbiasedness of the estimator.

Starting from the second line of the proof of variance for the DR estimator (Appendix B.2.1), we have

VPβ FDR(t) = n1 VPβ w(A, X) 1{R≤t} − G(t; X, A) + G(t; X, π)

= n1 EPβ w(A, X)2σ2(t; X, A)

1 + n VPβ w(A, X) G(t; X, A) − G(t; X, A) + G(t; X, π)



2



1

β(A, X)

= n EP  β(A, X)

σ2(t; X, A)

1 + n VPβ EPβ w(A, X) G(t; X, A) − G(t; X, A)
1 + n EPβ VPβ w(A, X) G(t; X, A) − G(t; X, A) = n1 EP (1 − δ(A, X))2 σ2(t; X, A)
1 + n VP [EP [δ(A, X)∆(t; X, A) + G(t; X, A)|X]]
1 + n EP VPβ [w(A, X)∆(t; X, A)|X]

+ G(t; X, π)|X |X

the second line uses a change of measure in the ﬁrst term, and the law of total variance conditioned on the context X. The third line follows again from change of measure and substituting in the deﬁnition of δ and ∆.
45

Lemma D.1 is derived from Lemma D.2 using the fact that the IS estimator is a special case of the DR estimator with G = 0.
Lemma D.3 is derived from Lemma D.2 by using β → ∞ which means w = 0, e.g. importance weighting is not used, and δ = 1.

D.2 CDF and Risk Estimate Error Bounds
Theorem D.1 generalizes the CDF error bounds established for the IS and DR estimators with known behavior policy to the case where β is estimated, given an additional high-probability guarantee on the quality of β.
Theorem D.1. For the IS or DR CDF estimator F that uses estimated weights w(a, x) = π(a|x)/β(a, x), given an estimate β that is β-close to the true behavior policy β, that is
sup |β(a|x) − β(a|x)| ≤ β,
a,x
we have with probability at least 1 − δ that

where

is either IS or

Pβ sup F (t) − F (t) ≤ + c β ≥ 1 − δ
t∈R
−1
= DR depending the choice of F , and c = wmax infa,x β(a|x) .

Similarly, for L-Lipschitz risk functionals, the general error bound given in Theorem 6.1 can be extended to the case of β by adding the additional error term from the policy estimation.

Corollary D.1. For the IS or DR CDF estimator F that uses estimated weights w(a, x) = π(a|x)/β(a, x), given an estimate β that is β-close to the true behavior policy β, we have with probability at least 1 − δ that
ρ(F ) − ρ(F ) ≤ L ( + c β)
−1
where c = wmax infa,x β(a|x) .

Note that the error contributed by policy estimation, c β, is primarily dependent upon two factors.
First, the quality of β estimation determines the magnitude of β; a poor estimate naturally leads to a higher value of this constant. Second, c is a problem-dependent constant proportional to the maximum importance weight wmax and the minimum probability of the estimated behavior policy infa,x β(a|x). If infa,x β(a|x) is particularly small, the error bound is also large. This reﬂects the fact that CDF estimation can be diﬃcult when the behavior policy places low probability in some area of the context and action space.

Remark D.1. When actions and contexts are discrete, and β is estimated using empirical averages,
standard concentrations for the mean of a random variable can be used to determine β. If β is estimated using regression, depending on the estimator β can also be determined from concentration inequalities.

v2

46

Proofs: Error Bounds The proof of these results is given below.

Proof of Theorem D.1. We can decompose the error F − F as:

sup |F (t) − F (t)| ≤ sup |F (t) − F (t)| + |F (t) − F (t)|

t

t

≤ sup |F (t) − F (t)| + sup |F (t) − F (t)|

t

t

Theorem 5.1 gives a bound for the ﬁrst term, and the bound for the second term bound is given in Lemma D.4 for the IS estimator, and in Lemma D.5 for the DR estimator.

Proof of Corollary D.1. This result follows directly from applying the general risk estimation error bound in Theorem 6.1 to the error from Theorem D.1.

The intermediary lemmas are deﬁned and proved below:
Lemma D.4. Suppose that |β(a|x) − β(a|x)| ≤ β for all a, x with probability at least 1 − δ. Then with probability at least 1 − δ,

−1
where c = wmax infa,x β(a|x) .

sup |FIS(t) − FIS(t)| ≤ c β
t

Proof. We can bound the LHS of the lemma statement as follows.

sup |FIS(t) − FIS(t)| = sup n1 n (w(ai, xi) − w(ai, xi))1{ri≤t}

t

t

i=1

1n = sup
t n i=1

π(ai|xi) − π(ai|xi) 1{r ≤t}
β(ai|xi) β(ai|xi) i

1 n π(ai|xi) π(ai|xi)

≤

−

n i=1 β(ai|xi) β(ai|xi)

1n

β(ai|xi)

≤ wmax

1−

n i=1

β(ai|xi)

1 n β(ai|xi) − β(ai|xi)

= wmax

n i=1

β(ai|xi)

≤ wmax

−1 1 n

inf β(a|x)

β(ai|xi) − β(ai|xi)

a,x

n

i=1

−1

≤ wmax inf β(a|x)

β

a,x

where the last line follows from using the assumption that |β(a|x) − β(a|x)| ≤ β for all a, x.

47

Lemma D.5. Suppose that |β(a|x) − β(a|x)| ≤ β for all a, x with probability at least 1 − δ. Then with probability at least 1 − δ,

sup |FDR(t) − FDR(t)| ≤ c β
t
−1
where c = wmax infa,x β(a|x) .

Proof. We can bound the LHS of the lemma statement as follows. Using the deﬁnitions of the DR estimators,

sup |FDR(t) − FDR(t)| = sup n1 n (w(ai, xi) − w(ai, xi)) 1{ri≤t} − G(t; xi, ai)

t

t

i=1

1n = sup
t n i=1

π(ai|xi) − π(ai|xi) β(ai|xi) β(ai|xi)

1{ri≤t} − G(t; xi, ai)

1 n π(ai|xi) π(ai|xi)

≤

−

n i=1 β(ai|xi) β(ai|xi)

1n

β(ai|xi)

≤ wmax

1−

n i=1

β(ai|xi)

1 n β(ai|xi) − β(ai|xi)

= wmax

n i=1

β(ai|xi)

≤ wmax

−1 1 n

inf β(a|x)

β(ai|xi) − β(ai|xi)

a,x

n

i=1

−1

≤ wmax inf β(a|x)

β

a,x

where the last line uses the assumption that |β(a|x) − β(a|x)| ≤ β for all a, x.

48

E Additional Experiments
Implementation Details. Following [27, 26, 75], we obtain our oﬀ-policy contextual bandit datasets by transforming classiﬁcation datasets. The contexts are the provided features, and the actions correspond to the possible class labels. To obtain the evaluation policy π, we use the output probabilities of a trained logistic regression classiﬁer [50]. The behavior policy is deﬁned as β = απ + (1 − α)πUNIF, where πUNIF is a uniform policy over the actions, for some α ∈ (0, 1]. Each dataset is generated by drawing actions for each context according to the probabilities of β, and the deterministic reward is 1 if the action matches the ground truth label, and 0 otherwise.
We apply this process to the set of 9 UCI datasets [24] used in [27, 26, 75], which each have diﬀering dimensions d, actions k, and sample size n. Models G must be constructed for the DM and DR estimators. As in [27], the dataset is divided into two splits, with each of the two splits used to estimate G, which is then used with the other split to calculate the estimator. The two results are averaged to produce the ﬁnal estimators. In order to estimate G, we discretize the reward support into t ∈ [0, 1], and train a logistic regression classiﬁer [50] for each action a and each t, with regularization parameter C = 1 and tolerance 0.0001. The code to reproduce these experiments is provided in the supplementary. On a CPU, they take roughly half a day of compute in total.
Relationship With α. We plot the error over the range of α, which controls the mismatch between the behavioral policy β and the target policy π and is thus proportional to wmax, for the PageBlocks dataset (also in Figure 1). The CDF error is shown in Figure 4 and the mean squared error (MSE) for the mean, CVaR 0.5, and variance risk functionals are shown in Figure 5.
The DR estimator exhibits lower error than any other estimator, and signiﬁcantly lower variance than the IS and WIS estimators, across the range of α. This is particularly obvious in the region where α is small, which is where importance weights can become larger and the IS-based estimators are prone to higher variance. Note that the CVaR0.5 MSE is close to 0 for all estimators.

error

0.018

DR

0.015

WIS IS

0.012

DM

0.010

0.007

0.005

0.003

0.000

0.0

0.2

0.4

0.6

0.8

1.0

Figure 4: Sup-norm CDF error over α for PageBlocks. Shaded region shows one empirical standard deviation.

49

MSE

Mean

CVaR 0.5

Variance

3e-04

6e-33

8e-06

7e-06

3e-04

5e-33

6e-06

DR

2e-04

4e-33

5e-06

WIS

2e-04

3e-33

4e-06

IS DM

1e-04

2e-33

3e-06

2e-06

5e-05

1e-33

1e-06

0e+00

0e+00

0e+00

0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0

Figure 5: Mean squared error (MSE) over α for diﬀerent risk functionals evaluated in the PageBlocks dataset. Shaded region shows one empirical standard deviation.

Evaluation Over UCI Datasets. We display the sup-norm error of the estimated CDF and the mean-squared error (MSE) of estimated risk functionals (mean, CVaR0.5, and variance) for the 9 UCI datasets below. Here, α = 0.5 is ﬁxed. All plots are shown over 500 repetitions, with error bars omitted for readability but similar to those shown in Figure 1.
The general trends reﬂect analysis presented in Section 7. As expected of our distribution-based approach, trends in CDF estimation performance are r√eﬂected in risk estimation performance. Both the DR and IS estimators exhibit the expected O(1/ n) error convergence across the estimation tasks. Generally, the DR estimator does as well as if not better than the other estimators; where the model is diﬃcult to specify well, the DR estimator may suﬀer slightly in performance in the low sample regime, but always outperforms the other estimators as the number of samples n increases.

50

Error

Error

Error

0.100 6 × 10 2 4 × 10 2 3 × 10 2 2 × 10 2
0.010100
0.100 6 × 10 2 4 × 10 2 3 × 10 2 2 × 10 2

Ecoli : n=336, k=8, d=8

CDF

0.010

Mean

0.010

CVaR 0.5

0.001

Variance

0.001

DR

WIS

IS

0.000

DM

MSE

0.001

n 2 × 102 CDF

100 0.010

n 2 × 102

100

2 × 102

100

Yeast: n=1484, k=10, d=8

Mean

00..000010

CVaR 0.5

0.001

2 × 102
Variance

MSE

DR WIS IS DM
0.001 0.000

0.010100 5 ×01.00502

2 × 102 n3 × 1024 × 102 6 × 102

100

CDF

0.005

Glass: n=214, k=6, d=10 2×102 n3×1024×102 6×102

100

Mean

0.010

0.001

2 × 102 3 × 1024 × 102 6 × 102
CVaR 0.5

100 0.001

2 × 102 3 × 1024 × 102 6 × 102
Variance

4 × 10 2

MSE

4 × 10 3 3 × 10 3
2 × 10 3

6 × 10 4

DR

4 × 10 4

WIS IS

3 × 10 4

DM

3 × 10 2

2 × 10 4

n 0.025100

0.100

CDF

0.001100 0.010

n

100

PageBlocks: n=5472, k=5, d=10

Mean

0.010

CVaR 0.5

0.001

0.000100 0.001

Variance

0.001 0.010

MSE

DR

0.000

WIS

IS

DM

100

n 1000

100

n 1000

100

1000

100

1000

OptDigits: n=5620, k=10, d=64

CDF

Mean

CVaR 0.5

Variance

0.010

0.001

0.100

0.010

DR WIS

MSE

0.000

IS

0.001

0.001

DM

0.010

100

n 1000

100

n 1000

100

1000

100

1000

Error

Error

51

Error

Error

0.100 0.010
100 0.100
0.010 100
0.100
0.010 100
0.100
0.010 100

CDF
n1000 CDF

MSE

0.010 0.001
100
0.010 0.001

MSE

n 1000 CDF

100 0.010 0.001

MSE

n 1000 CDF

100 0.010

MSE

0.001

n 2 × 102 3 × 102 4 × 102

100

Letters: n=20000, k=26, d=16

Mean

CVaR 0.5

0.001

0.010 0.000

0.001

n1000

100

1000

100

PenDigits: n=10992, k=10, d=16

Mean

CVaR 0.5

0.010

0.001

0.001

0.000

n 1000

100

1000

100

SatImage: n=6435, k=6, d=36

Mean

CVaR 0.5

0.001

0.010

0.000 0.001

n 1000

100

1000

100

Vehicle: n=813, k=4, d=18

Mean

CVaR 0.5

0.001

0.010

0.001

n 2 × 102 3 × 102 4 × 102

100

0.000

2 × 102 3 × 102 4 × 102

100

Variance DR WIS IS DM
1000
Variance DR WIS IS DM
1000
Variance DR WIS IS DM
1000
Variance DR WIS IS DM
2 × 102 3 × 102 4 × 102

Error

Error

52

