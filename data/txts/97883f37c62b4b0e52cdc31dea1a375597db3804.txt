arXiv:1801.06519v2 [cs.CV] 16 Mar 2018

Piggyback: Adapting a Single Network to Multiple Tasks by Learning to Mask Weights
Arun Mallya, Dillon Davis, Svetlana Lazebnik
University of Illinois at Urbana-Champaign
Abstract. This work presents a method for adapting a single, ﬁxed deep neural network to multiple tasks without aﬀecting performance on already learned tasks. By building upon ideas from network quantization and pruning, we learn binary masks that “piggyback” on an existing network, or are applied to unmodiﬁed weights of that network to provide good performance on a new task. These masks are learned in an end-toend diﬀerentiable fashion, and incur a low overhead of 1 bit per network parameter, per task. Even though the underlying network is ﬁxed, the ability to mask individual weights allows for the learning of a large number of ﬁlters. We show performance comparable to dedicated ﬁne-tuned networks for a variety of classiﬁcation tasks, including those with large domain shifts from the initial task (ImageNet), and a variety of network architectures. Unlike prior work, we do not suﬀer from catastrophic forgetting or competition between tasks, and our performance is agnostic to task ordering. Code available at https://github.com/arunmallya/piggyback.
Keywords: Incremental Learning, Binary Networks.
1 Introduction
The most popular method used in prior work for training a deep network for a new task or dataset is ﬁne-tuning an established pre-trained model, such as the VGG-16 [1] trained on ImageNet classiﬁcation [2]. A major drawback of ﬁnetuning is the phenomenon of “catastrophic forgetting” [3], by which performance on the old task degrades signiﬁcantly as the new task is learned, necessitating one to store specialized models for each task or dataset. For achieving progress towards continual learning [4,5], we need better methods for augmenting capabilities of an existing network while avoiding catastrophic forgetting and requiring as few additional parameters as possible.
Prior methods for avoiding catastrophic forgetting, such as Learning without Forgetting (LwF) [6] and Elastic Weight Consolidation (EWC) [4], maintain performance on older tasks through proxy losses and regularization terms while modifying network weights. Another recent work, PackNet [7], adopts a diﬀerent route of iteratively pruning unimportant weights and ﬁne-tuning them for learning new tasks. As a result of pruning and weight modiﬁcations, a binary parameter usage mask is produced by PackNet. We question whether the weights

2

Dense filter (W ) of pretrained backbone network

Binary mask (m ) for Task K
⊙ Elementwise Masking

Thresholding Function e.g. Binarizer
Real-valued mask weights (m r ) for Task K

Effective filter for Task K

Eval Time Behavior

Train Time Behavior

Fig. 1: Overview of our method for learning piggyback masks for ﬁxed backbone networks. During training, we maintain a set of real-valued weights mr which are passed through a thresholding function to obtain binary-valued masks m. These masks are applied to the weights W of the backbone network in an elementwise fashion, keeping individual weights active, or masked out. The gradients obtained through backpropagation of the task-speciﬁc loss are used to update the realvalued mask weights. After training, the real-valued mask weights are discarded and only the thresholded mask is retained, giving one network mask per task.
of a network have to be changed at all to learn a new task, or whether we can get by with just selectively masking, or setting certain weights to 0, while keeping the rest of the weights the same as before. Based on this idea, we propose a novel approach in which we learn to mask weights of an existing “backbone” network for obtaining good performance on a new task, as shown in Figure 1. Binary masks that take values in {0, 1} are learned in an end-to-end diﬀerentiable fashion while optimizing for the task at hand. These masks are elementwise applied to backbone weights, allowing us to learn a large range of diﬀerent ﬁlters, even with ﬁxed weights. We ﬁnd that a well-initialized backbone network is crucial for good performance and that the popular ImageNet pre-trained network generalizes to multiple new tasks. After training for a new task, we obtain a per-task binary mask that simply “piggybacks” onto the backbone network.
Our experiments conducted on image classiﬁcation, and presented in Section 4, show that this proposed method obtains performance similar to using a separate network per task, for a variety of datasets considered in prior work [7] such as CUBS birds [8], Stanford cars [9], Oxford ﬂowers [10], as well datasets with a signiﬁcant departure from the natural image domain of the ImageNet dataset such as WikiArt paintings [11] and human sketches [12]. We demonstrate the applicability of our method to multiple network architectures including VGG-16 [1], ResNets [13,14], and DenseNets [15]. Section 5 tries to oﬀer some insight into the workings of the proposed method, and analyzes design choices that aﬀect performance. As presented in Section 6, we also obtain performance

3
competitive with the best methods [16] on the Visual Decathlon challenge [17] while using the least amount of additional parameters. Finally, we show that our method can be used to train a fully convolutional network for semantic segmentation starting from a classiﬁcation backbone.
2 Related Work
While multiple prior works [18,19,20] have explored multi-task training, wherein data of all tasks is available at the time of training, we consider the setting in which new tasks are available sequentially, a more realistic and challenging scenario. Prior work under this setting is based on Learning without Forgetting (LwF) [5,6,21] and Elastic Weight Consolidation (EWC) [4,22]. LwF uses initial network responses on new data as regularization targets during new task training, while EWC imposes a smooth penalty on changing weights deemed to be important to prior tasks. An issue with these methods is that it is not possible to determine the change in performance on prior tasks beforehand since all weights of the network are allowed to be modiﬁed to varying degrees. PackNet [7] avoids this issue by identifying weights important for prior tasks through network pruning, and keeping the important weights ﬁxed after training for a particular task. Additional information is stored per weight parameter of the network to indicate which tasks it is used by. However, for each of these methods, performance begins to drop as many tasks are added to the network. In the case of LwF, a large domain shift for a new task causes signiﬁcant drop in prior task performance [6]. For PackNet, performance on a task drops as it is added later to the network due to the lack of available free parameters, and the total number of tasks that can be added is ultimately limited due to the ﬁxed size of the network [7].
Our proposed method does not change weights of the initial backbone network and learns a diﬀerent mask per task. As a result, it is agnostic to task ordering and the addition of a task does not aﬀect performance on any other task. Further, an unlimited number of tasks can piggyback onto a backbone network by learning a new mask. The parameter usage masks in PackNet were obtained as a by-product of network pruning [23], but we learn appropriate masks based on the task at hand. This idea of masking is related to PathNet [24], which learns selective routing through neurons using evolutionary strategies. We achieve similar behavior through an end-to-end diﬀerentiable method, which is less computationally demanding. The learning of separate masks per task decouples the learning of multiple tasks, freeing us from having to choose hyperparameters such as batch mixing ratios [20], pruning ratios [7], and cost weighting [6].
Similar to our proposed method, another set of methods adds new tasks by learning additional task-speciﬁc parameters. For a new task, Progressive Neural Networks [25] duplicates the base architecture while adding lateral connections to layers of the existing network. The newly added parameters are optimized for the new task, while keeping old weights ﬁxed. This method incurs a large overhead as the network is replicated for the number of tasks added. The method of Residual Adapters [17] develops on the observation that linearly parameterizing

4
a convolutional ﬁlter bank of a network is the same as adding an additional per-task convolutional layer to the network. The most recent Deep Adaptation Networks (DAN) [16] allows for learning new ﬁlters that are linear combinations of existing ﬁlters. Similar to these methods, we enable the learning of new pertask ﬁlters. However, these new ﬁlters are constrained to be masked versions of existing ﬁlters. Our learned binary masks incur an overhead of 1 bit per network parameter, smaller than all of the prior work. Further, we do not ﬁnd it necessary to learn task-speciﬁc layer biases and batch normalization parameters.
Our method for training binary masks is based on the technique introduced by Courbariaux et al. [26,27] for the training of a neural network with binaryvalued weights from scratch. The authors maintain a set of real-valued weights that are passed through a binarizer function during the forward pass. Gradients are computed with respect to the binarized weights during the backward pass through the application of the chain rule, and the real-valued weights are updated using the gradients computed for the binarized versions. In [26], the authors argue that even though the gradients computed in this manner are noisy, they eﬀectively serve as a regularizer and quantization errors cancel out over multiple iterations. Subsequent work including [28,29] has extended this idea to ternaryvalued weights. Unlike these works, we do not train a quantized network from scratch but instead learn quantized masks that are applied to ﬁxed, real-valued ﬁlter weights. Work on sparsifying dense neural networks, speciﬁcally [30], has used the idea of masked weight matrices. However, only their weight matrix was trainable and their mask values were a ﬁxed function of the magnitude of the weight matrix and not explicitly trainable. In contrast, we treat the weight matrix of the backbone network as a ﬁxed constant, and our proposed approach combines the key ideas from these two areas of network binarization and masked weight matrices to learn piggyback masks for new tasks.
3 Approach
The key idea behind our method is to learn to selectively mask the ﬁxed weights of a base network, so as to improve performance on a new task. We achieve this by maintaining a set of real-valued weights that are passed through a deterministic thresholding function to obtain binary masks, that are then applied to existing weights. By updating the real-valued weights through backpropagation, we hope to learn binary masks appropriate for the task at hand. This process is illustrated in Figure 1. By learning diﬀerent binary-valued {0, 1} masks per task, which are element-wise applied to network parameters, we can re-use the same underlying base network for multiple tasks, with minimal overhead. Even though we do not modify the weights of the network, a large number of diﬀerent ﬁlters can be obtained through masking. For example, a dense weight vector such as [0.1, 0.9, −0.5, 1] can give rise to ﬁlters such as [0.1, 0, 0, 1], [0, 0.9, −0.5, 0], and [0, 0.9, −0.5, 1] after binary masking. In practice, we begin with a network such as the VGG-16 or ResNet-50 pre-trained on the ImageNet classiﬁcation task as our base network, referred to as the backbone network, and associate a

5

real-valued mask variable with each weight parameter of all the convolutional and fully-connected layers. By combining techniques used in network binarization [26,27] and pruning [30], we train these mask variables to learn the task at hand in an end-to-end fashion, as described in detail below. The choice of the initialization of the backbone network is crucial for obtaining good performance, and is further analyzed in Section 5.1.

For simplicity, we describe the mask learning procedure using the example of

a fully-connected layer, but this idea can easily be extended to a convolutional

layer as well. Consider a simple fully-connected layer in a neural network. Let

the input and output vectors be denoted by x = (x1, x2, · · · , xm)T of size m × 1, and y = (y1, y2, · · · , yn)T of size n × 1, respectively. Let the weight matrix of

the layer be W = [w]ji of size n × m. The input-output relationship is then

given by y = Wx, or yj =

m i=1

wji

·

xi.

The

bias

term

is

ignored

for

ease

of notation. Let δv denote the partial derivative of the error function E with

respect to the variable v. The backpropagation equation for the weights W of

this fully-connected layer is given by

δwji ∂E = ∂E · ∂yj (1)

∂wji

∂yj

∂wji

= δyj · xi

(2)

∴ δW ∂E = δy · xT ,

(3)

∂w ji

where δy = (δy1, δy2, · · · , δyn)T is of size n × 1.
Our modiﬁed fully-connected layer associates a matrix of real-valued mask weights mr = [mr]ji with every weight matrix W, of the same size as W (n×m), as indicated by the rightmost ﬁlter in Figure 1. We obtain thresholded mask matrices m = [m]ji by passing the real-valued mask weight matrices mr through a hard binary thresholding function given by

1, if mrji ≥ τ

mji =

,

(4)

0, otherwise

where τ is a selected threshold. The binary-valued matrix m activates or switches

oﬀ contents of W depending on whether a particular value mji is 0 or 1. The

layer’s input-output relationship is given by the equation y = (W m) x, or

yj =

m i=1

wji

·mji

·xi,

where

indicates elementwise multiplication or masking.

As mentioned previously, we set the weights W of our modiﬁed layer to those

from the same architecture pre-trained on a task such as ImageNet classiﬁcation.

We treat the weights W as ﬁxed constants throughout, while only training the

real-valued mask weights mr. The backpropagation equation for the thresholded

6

mask weights m of this fully-connected layer is given by

δmji ∂E = ∂E · ∂yj (5)

∂mji

∂yj

∂mji

= δyj · wji · xi

(6)

∴ δm ∂E = (δy · xT ) W.

(7)

∂m ji

Even though the hard thresholding function is non-diﬀerentiable, the gradients of the thresholded mask values m serve as a noisy estimator of the gradients of the real-valued mask weights mr, and can even serve as a regularizer, as shown in prior work [26,27]. We thus update the real-valued mask weights mr using gradients computed for m, the thresholded mask values. After adding a new ﬁnal classiﬁcation layer for the new task, the entire system can be trained in an end-to-end diﬀerentiable manner. In our experiments, we did not train per-task biases as prior work [7] showed that this does not have any signiﬁcant impact on performance. We also did not train per-task batch-normalization parameters for simplicity. Section 5.3 analyzes the beneﬁt of training per-task batchnorm parameters, especially for tasks with large domain shifts.
After training a mask for a given task, we no longer require the real-valued mask weights. They are discarded, and only the thresholded masks associated with the backbone network layers are stored. A typical neural network parameter is represented using a 32-bit ﬂoat value (including in our PyTorch implementation). A binary mask only requires 1 extra bit per parameter, leading to an approximate per-task overhead of 1/32 or 3.12% of the backbone network size.
Practical optimization details. From Eq. 7, we observe that |δm|, |δmr| ∝ |W|. The magnitude of pre-trained weights varies across layers of a network, and as a result, the mask gradients would also have diﬀerent magnitudes at diﬀerent layers. This relationship requires us to be careful about the manner in which we initialize and train mask weights mr. There are two possible approaches: 1) Initialize mr with values proportional to the weight matrix W of the corresponding layer. In this case, the ratio |δmr|/|mr| will be similar across layers, and a constant learning rate can be used for all layers. 2) Initialize mr with a constant value, such as 0.01, for all layers. This would require a separate learning rate per layer, due to the scaling of the mask gradient by the layer weight magnitude. While using SGD, scaling gradients obtained at each layer by a factor of 1/avg(|W|), while using a constant learning rate, has the same eﬀect as layer-dependent learning rates. Alternatively, one could use adaptive optimizers such as Adam, which would learn appropriate scaling factors.
The second initialization approach combined with the Adam optimizer produced the best results, with a consistent gain in accuracy by ∼ 2% compared to the alternatives. We initialized the real-valued weights with a value of 1e-2 with a binarizer threshold (τ , in Equation 4) of 5e-3 in all our experiments. Randomly initializing the real-valued mask weights such that the thresholded binary masks had an equal number of 0s and 1s did not give very good performance. Ensuring

7
that all thresholded mask values were 1 provides the same network initialization as that of the baseline methods.
We also tried learning ternary masks {−1, 0, 1} by using a modiﬁed version of Equation 4 with two cut-oﬀ thresholds, but did not achieve results that were signiﬁcantly diﬀerent from those obtained with binary masks. As a result, we only focus on results obtained with binary masks in the rest of this work.
4 Experiments and Results
We consider a wide variety of datasets, statistics of which are summarized in Table 1, to evaluate our proposed method. Similar to PackNet [7], we evaluate our method on two large-scale datasets, the ImageNet object classiﬁcation dataset [2] and the Places365 scene classiﬁcation dataset [31], each of which has over a million images, as well as the CUBS [8], Stanford Cars [9], and Flowers [10] ﬁne-grained classiﬁcation datasets. Further, we include two more datasets with signiﬁcant domain shifts from the natural images of ImageNet, the WikiArt Artists classiﬁcation dataset, created from the WikiArt dataset [11], and the Sketch classifcation dataset [12]. The former includes a wide genre of painting styles, as shown in Figure 2a, while the latter includes black-and-white sketches drawn by humans, as shown in Figure 2b. For all these datasets, we use networks with an input image size of 224 × 224 px.

Dataset
ImageNet [2] Places365 [31] CUBS [8] Stanford Cars [9] Flowers [10] WikiArt [11] Sketch [12]

#Train
1,281,144 1,803,460
5,994 8,144 2,040 42,129 16,000

#Eval
50,000 36,500 5,794 8,041 6,149 10,628 4,000

#Classes
1,000 365 200 196 102 195 250

Table 1: Summary of datasets used.

(a) WikiArt

(b) Sketch

Fig. 2: Datasets unlike ImageNet.

Table 2 reports the errors obtained on ﬁne-grained classiﬁcation tasks by learning binary-valued piggyback masks for a VGG-16 network pre-trained on ImageNet classiﬁcation. The ﬁrst baseline considered is Classiﬁer Only, which only trains a linear classiﬁer using fc7 features extracted from the pre-trained VGG-16 network. This is a commonly used method that has low overhead as all layers except for the last classiﬁcation layer are re-used amongst tasks. The second and more powerful baseline is Individual Networks, which ﬁnetunes a separate network per task. We also compare our method to the recently introduced PackNet [7] method, which adds multiple tasks to a network through iterative pruning and re-training. We train all methods for 30 epochs. We train the piggyback and classiﬁer only, using the Adam optimizer with an initial learning rate of 1e-4, which is decayed by a factor of 10 after 15 epochs. We found

8
SGDm with an initial learning rate of 1e-3 to work better for the individual VGG network baseline. For PackNet, we used a 50% pruned initial network trained with SGDm with an initial learning rate of 1e-3 using the same decay scheme as before. We prune the network by 75% and re-train for 15 epochs with a learning rate of 1e-4 after each new task is added. All errors are averaged over 3 independent runs.

Dataset
ImageNet
CUBS Stanford Cars Flowers WikiArt Sketch # Models (Size)

Classiﬁer Only
28.42 (9.61) 36.49 54.66 20.01 49.53 58.53
1 (537 MB)

PackNet [7]

↓

↑

29.33 (9.99) 22.30 29.69 15.81 21.66 10.33 10.25 32.80 31.48 28.62 24.88

1 (587 MB)

Piggyback (ours)
28.42 (9.61) 20.99 11.87 7.19 29.91 22.70
1 (621 MB)

Individual Networks
28.42 (9.61) 21.30 12.49 7.35 29.84 23.54
6 (3,222 MB)

Table 2: Errors obtained by starting from an ImageNet-trained VGG-16 network and then using various methods to learn new ﬁne-grained classiﬁcation tasks. PackNet performance is sensitive to order of task addition, while the rest, including our proposed method, are agnostic. ↓ and ↑ indicate that tasks were added in the CUBS → Sketch, and Sketch → CUBS order, resp. Values in parentheses are top-5 errors, rest are top-1 errors.

As seen in Table 2, training individual networks per task clearly provides a huge beneﬁt over the classiﬁer only baseline for all tasks. PackNet signiﬁcantly improves over the classiﬁer only baseline, but begins to suﬀer when more than 3 tasks are added to a single network. As PackNet is sensitive to the ordering of tasks, we try two settings - adding tasks in order from CUBS to Sketch (top to bottom in Table 2), and the reverse. The order of new task addition has a large impact on the performance of PackNet, with errors increasing by 4-7% as the addition of a task is delayed from ﬁrst to last (ﬁfth). The error on ImageNet is also higher in the case of PackNet, due to initial network pruning. By training binary piggyback masks, we are able to obtain errors slightly lower than the individual network case. We believe that this is due to the regularization eﬀect caused by the constrained ﬁlter modiﬁcation allowed by our method. Due to the learning of independent masks per task, the obtained performance is agnostic to the ordering of new tasks, albeit at a slightly higher storage overhead as compared to PackNet. The number of weights switched oﬀ varies per layer and by dataset depending on its similarity to the ImageNet dataset. This eﬀect is further examined in Section 5.2.
While the results above were obtained by adding multiple smaller ﬁne-grained classiﬁcation tasks to a network, the next set of results in Table 3 examines the eﬀect of adding a large-scale dataset, the Places365 scene classiﬁcation task with 1.8M images, to a network. Here, instead of the Classiﬁer Only baseline,

9

Dataset ImageNet Places365 # Models (Size)

Jointly Trained Network∗
33.49 (12.25) 45.98 (15.59)
1 (537 MB)

PackNet [7]
29.33 (9.99) 46.64 (15.92)
1 (554 MB)

Piggyback (ours)
28.42 (9.61) 46.71 (16.18)
1 (554 MB)

Individual Networks
28.42 (9.61) 46.35 (16.14)∗
2 (1,074 MB)

Table 3: Adding a large-scale dataset to an ImageNet-trained VGG-16 network. Values in parentheses are top-5 errors, rest are top-1 errors. ∗ indicates models
downloaded from https://github.com/CSAILVision/places365, trained by [31].

we compare against the Jointly Trained Network of [31], in which a single network is simultaneously trained for both tasks. Both PackNet and Piggyback were trained for 20 epochs on Places365. Once again, we are able to achieve close to best-case performance on the Places365 task, obtaining top-1 errors within 0.36% of the individual network, even though the baselines were trained for 6090 epochs [31]. The performance is comparable to PackNet, and for the case of adding just one task, both incur a similar overhead.
The previous results were obtained using the large VGG-16 network, and it is not immediately obvious whether the piggyback method would work for much deeper networks that have batch normalization layers. Masking out ﬁlter weights can change the average magnitude of activations, requiring changes to batchnorm parameters. We present results obtained with a VGG-16 network with batch normalization layers, the ResNet-50, and DenseNet-121 networks in Table 4. We observe that the method can be applied without any changes to these network architectures with batchnorm, residual, and skip connections. In the presented results, we do not learn task-speciﬁc batchnorm parameters. We however notice that the deeper a network gets, the larger the gap between the performance of piggyback and individual networks. For the VGG-16 architecture, piggyback can often do as well as or better than individual models, but for the ResNet and DenseNet architectures, the gap is ∼2%. In Section 5.3 we show that learning task-speciﬁc batchnorm parameters in the case of datasets that exhibit a large domain shift, such as WikiArt, for which the performance gap is 4-5% (as seen in Table 4), helps further close the gap.

5 Analysis
5.1 Does Initialization Matter?
Here, we analyze the importance of the initialization of the backbone network. It is well known that training a large network such as the VGG-16 from scratch on a small dataset such as CUBS, or Flowers leads to poor performance, and the most popular approach is to ﬁne-tune a network pre-trained on the ImageNet classiﬁcation task. It is not obvious whether initialization is just as important for the piggyback method. Table 5 presents the errors obtained by training piggyback

10

Dataset
ImageNet CUBS Stanford Cars Flowers WikiArt Sketch # Models (Size)
ImageNet CUBS Stanford Cars Flowers WikiArt Sketch # Models (Size)
ImageNet CUBS Stanford Cars Flowers WikiArt Sketch # Models (Size)

Classiﬁer PackNet [7]

Only

↓

↑

26.63 (8.49) 33.88 51.62 19.38 48.05 59.96

VGG-16 BN 27.18 (8.69)
20.21 23.82 14.05 17.60 7.82 7.85 30.21 29.59 25.47 23.53

1 (537 MB) 1 (587 MB)

ResNet-50

23.84

24.29

(7.13)

(7.18)

29.97 19.59 28.62

47.20 13.89 19.99

14.01

6.96 9.45

44.40 30.60 29.69

49.14 23.83 21.30

1 (94 MB) 1 (103 MB)

DenseNet-121

25.56

25.60

(8.02)

(7.89)

26.55 19.26 30.36

43.19 15.35 22.09

16.56

8.94 8.46

45.08 33.66 30.81

46.88 25.35 21.08

1 (28 MB) 1 (31 MB)

Piggyback (ours)
26.63 (8.49) 18.37 9.87 4.84 27.50 21.41 1 (621 MB)
23.84 (7.13) 18.41 10.38 5.23 28.67 20.09 1 (109 MB)
25.56 (8.02) 19.50 10.87 5.31 29.56 20.30 1 (33 MB)

Individual Networks
26.63 (8.49) 19.57 9.41 4.55 26.68 21.92 6 (3,222 MB)
23.84 (7.13) 17.17 8.17 3.44 24.40 19.22 6 (564 MB)
25.56 (8.02) 18.08 8.64 3.49 23.59 19.48 6 (168 MB)

Table 4: Results on other network architectures. Values in parentheses are top-5 errors, rest are top-1 errors. ↑ and ↓ indicate order of task addition for PackNet.

masks for tasks using the ResNet-50 as the backbone network, but with diﬀerent initializations. We consider 3 diﬀerent initializations: 1) a network trained on the ImageNet classiﬁcation task, the popular initialization for ﬁne-tuning, 2) a network trained from scratch on the Places365 scene classiﬁcation task, a dataset larger than ImageNet (1.8 M v/s 1.3 M images), but with fewer classes (365 v/s 1000), and lastly 3) a randomly initialized network.
We observe in Table 5 that initialization does indeed matter, with the ImageNetinitialized network outperforming both the Places365 and randomly initialized network on all tasks. In fact, by training a piggyback mask for the Places365 dataset on an ImageNet-initialized backbone network, we obtain an accuracy very similar to a network trained from scratch on the Places365 dataset. The ImageNet dataset is very diverse, with classes ranging from animals, to plants, cars and other inanimate objects, whereas the Places365 dataset is solely devoted to the classiﬁcation of scenes such as beaches, bedrooms, restaurants, etc. As a result, the features of the ImageNet-trained network serve as a very general

11
and ﬂexible initialization A very interesting observation is that even a randomly initialized network obtains non-trivial accuracies on all datasets. This indicates the learning a mask is indeed a powerful technique of utilizing ﬁxed ﬁlters and weights for adapting a network to a new task.

Dataset
CUBS Stanford Cars Flowers WikiArt Sketch
ImageNet
Places365

Pre-training/Initialization ImageNet Places365 Random

18.41 10.38 5.23 28.67 20.09 23.84 (7.13) 45.17 (15.12)

28.50 13.70 10.92 31.24 23.17 32.56 (11.92) 45.39 (15.05)

66.24 77.79 71.17 64.74 43.75 71.48 (46.73) 60.41 (28.94)

Table 5: Errors obtained by piggyback masks for the ResNet-50 backbone network with diﬀerent initializations. Errors in parentheses are top-5 errors, the rest are top-1 errors.

5.2 Learned sparsity and its distribution across network layers
Table 6 reports the total sparsity, or the number of mask values set to 0 in a binary piggyback mask learned for the corresponding choice of dataset and network architecture. This measures the amount of change that is required to be made to the backbone network, or the deviation from the ImageNet pretrained initialization, in order to obtain good performance on a given dataset. We note that the amount of sparsity obtained on ﬁne-grained datasets seems to be proportional to the errors obtained by the Classiﬁer Only method on the respective datasets. The easiest Flowers dataset requires the least number of changes, or a sparsity of 4.51%, while the harder WikiArt dataset leads to a 34.14% sparsity for a VGG-16 network mask. Across network architectures, we observe a similar pattern of sparsity based on the diﬃculty of the tasks. The sparsity obtained is also a function of the magnitude of the real-valued mask initialization and threshold used for the binarization (See Equation 4), with a higher threshold leading to higher sparsity. The numbers in Table 6 were obtained using our default settings of a binarizer threshold of 5e-3 and a uniform real-valued mask initialization of 1e-2.
We observe that a Places365-initialized network requires more changes as compared to an ImageNet-initialized network (refer to the ResNet-50 column of Table 6). This once again indicates that features learned on ImageNet are more diverse and serve as better initialization than those learned on Places365.
Figure 3 shows the sparsity obtained per layer of the ImageNet pre-trained VGG-16 network, for three datasets considered. While the total amount of spar-

% zero'ed out weights % zero'ed out weights % zero'ed out weights

12

2.5

CUBS

2.0

1.5

1.0

0.5

0.0

Zero'ed out weights per VGG-16 layer

0.16

Flowers

0.14

20

0.12

0.10

15

0.08

0.06

10

0.04

5

0.02

0.00

0

WikiArt

fc7 fc6 conv5_3 conv5_2 conv5_1 conv4_3 conv4_2 conv4_1 conv3_3 conv3_2 conv3_1 conv2_2 conv2_1 conv1_2 conv1_1 fc7 fc6 conv5_3 conv5_2 conv5_1 conv4_3 conv4_2 conv4_1 conv3_3 conv3_2 conv3_1 conv2_2 conv2_1 conv1_2 conv1_1 fc7 fc6 conv5_3 conv5_2 conv5_1 conv4_3 conv4_2 conv4_1 conv3_3 conv3_2 conv3_1 conv2_2 conv2_1 conv1_2 conv1_1

Fig. 3: Percentage of weights masked out per ImageNet pre-trained VGG-16 layer. Datasets similar to ImageNet share a lot of the lower layers, and require fewer changes. The number of masked out weights increases with depth of layer.
sity obtained per dataset is diﬀerent, we observe a consistent pattern of sparsity across the layers. In general, the number of changes increases with depth of the network layer. For datasets similar to ImageNet, such as CUBS, and Flowers, we observe that the low-level features (conv1-conv3) are mostly re-used without any major changes. WikiArt, which has a signiﬁcant domain shift from ImageNet, requires some changes in the low-level features. All tasks seem to require changes to the mid-level (conv4-conv5) and high-level features (fc6-fc7) in order to learn new task-speciﬁc features. Similar behavior was also observed for the deeper ResNet and DenseNet networks.

Dataset
CUBS Stanford Cars Flowers WikiArt Sketch ImageNet Places365

VGG-16
14.09% 17.03% 4.51% 34.14% 27.23%
– 43.47%

VGG-16 BN
13.24% 16.70% 4.52% 33.01% 26.05%
– –

ResNet-50 ImNet-init. Places-init.

12.21% 15.65% 4.48% 30.47% 23.04%
– 37.99%

15.22% 17.72% 6.45% 30.04% 24.23% 37.59%
–

DenseNet-121
12.01% 15.80% 5.28% 29.11% 22.24%
– –

Table 6: Percentage of zeroed out weights after training a binary mask for the respective network architectures and datasets.

Dataset
WikiArt Sketch
WikiArt Sketch

Piggyback (ours) Fixed BN Trained BN

ResNet-50

28.67

25.92

20.09

19.82

DenseNet-121

29.56

25.90

20.30

20.12

Individual Network
24.40 19.22
23.59 19.48

Table 7: Eﬀect of task-speciﬁc batch normalization layers on the top-1 error.

13
5.3 Handling large input domain shifts
In Table 4, we observe that WikiArt, which has a large domain shift from the ImageNet dataset on which the backbone network was trained on, has a larger gap in performance (4–5%) between the piggyback and individual network methods, especially for the deeper ResNet and DenseNet networks. Those numbers are duplicated in the Piggyback - Fixed BN and Individual Network columns of Table 7. We suspect that keeping batchnorm parameters ﬁxed while training the piggyback masks might be a reason for the gap in performance, as the domain shift is likely to cause a larger discrepancy between the ideal batchnorm parameter values and those inherited from ImageNet, the eﬀect of which is cascaded through the large number of layers. We performed these experiments again, but while updating batchnorm parameters, and report the results in the Piggyback - Trained BN column of Table 7. The top-1 error on WikiArt reduces from 28.67% to 25.92% for the ResNet-50 network, and from 29.56% to 25.90% for the DenseNet-121 network if the batchnorm parameters are allowed to update. For the Sketch dataset, training separate batchnorm parameters leads to a small decrease in error. Task-speciﬁc batchnorm parameters thus help improve performance, while causing a small increase of ∼1 MB in the storage overhead for both networks considered.

6 Results on Visual Decathlon & Semantic Segmentation
We also evaluate our proposed method on the newly introduced Visual Decathlon challenge [17] consisting of 10 classiﬁcation tasks. While the images of this task are of a lower resolution (72 × 72 px), they contain a wide variety of tasks such as pedestrian, digit, aircraft, and action classiﬁcation, making it perfect for testing the generalization abilities of our method. Evaluation on this challenge reports per-task accuracies, and assigns a cumulative score with a maximum value of 10,000 (1,000 per task) based on the per-task accuracies. The goal is to learn models for maximizing the total score over the 10 tasks while using the least number of parameters. Complete details about the challenge settings, evaluation, and datasets used can be found at http://www.robots.ox.ac.uk/~vgg/decathlon/.

Method
Scratch [17] Feature [17] Finetune [17] Res. Adapt. [17] Res. Adapt. (J) [17] DAN [16]
Piggyback (Ours)

#par
10 1 10 2 2 2.17
1.28

ImNet.
59.87 59.67 59.87 59.67 59.23 57.74
57.69

Airc.
57.1 23.31 60.34 56.68 63.73 64.12
65.29

C100
75.73 63.11 82.12 81.2 81.31 80.07
79.87

DPed
91.2 80.33 92.82 93.88 93.3 91.3
96.99

DTD
37.77 45.37 55.53 50.85 57.02 56.54
57.45

GTSR
96.55 68.16 97.53 97.05 97.47 98.46
97.27

Flwr
56.3 73.69 81.41 66.24 83.43 86.05
79.09

Oglt
88.74 58.79 87.69 89.62 89.82 89.67
87.63

SVHN
96.63 43.54 96.55 96.13 96.17 96.77
97.24

UCF
43.27 26.8 51.2 47.45 50.28 49.38
47.48

Mean
70.32 54.28 76.51 73.88 77.17 77.01
76.60

Score
1625 544 2500 2118 2643 2851
2838

Table 8: Top-1 accuracies obtained on the Visual Decathlon online test set.

Table 8 reports the results obtained on the online test set of the challenge. Consistent with prior work [17,16], we use a Wide Residual Network [14] with a depth of 28, widening factor of 4, and a stride of 2 in the ﬁrst convolutional layer

14

of each block. We use the 64 × 64 px ImageNet-trained network of [16] as our

backbone network, and train piggyback masks for the remaining 9 datasets. We

train for a total of 60 epochs per dataset, with learning rate decay by a factor

of 10 after 45 epochs. The base learning rate for ﬁnal classiﬁer layer which uses

SGDm was chosen from {1e-2, 1e-3} using cross-validation over the validation

set. Adam with a base learning rate of 1e-4 was used for updating the real-valued

piggyback masks. Data augmentation by random cropping, horizontal ﬂipping,

and resizing the entire image was chosen based on cross-validation.

As observed in Table 8, our method obtains performance competitive with

the state-of-the-art, while using the least amount of additional parameters over

a single network. Assuming that the base network uses 32-bit parameters, it

accounts for a parameter cost of 32n bits, where n is the number of parameters. A

binary mask per dataset requires n bits, leading to a total cost of approximately

(32n + 9n) = 41n bits, or a parameter ratio of (41/32) = 1.28, as reported.

The results presented in Section 4 only

required a single fully connected layer to

be added on top of the backbone network. Our method can also be extended to cases

classifier

deconv classifier

Finetuning from scratch

where more than one layers are added and

fc7

fc7

trained from scratch on top of a backbone network, as shown in Figure 4. We

VGG-16

VGG-16

Piggyback masking

tested our method on the task of pixelwise segmentation using the basic Fully Convolutional Network architecture [32]

conv1_1 Classification

conv1_1 Segmentation

which has fully connected layer followed by a deconvolutional layer of stride 32. We trained our networks on the 21-class PASCAL 2011 + SBD dataset, using the oﬃ-

Fig. 4: Mixed training of layers using ﬁnetuning from scratch and piggyback masking.

cial splits provided by [33] for 15 epochs.

Using the VGG-16 ﬁnetuned network, we obtain a mean IOU of 61.081. Using

the piggyback method, we obtain a competitive mean IOU of 61.41. Instead of

replicating the whole VGG-16 network of ∼500 MB, we only need an overhead

of 17 MB for masking the backbone network and 7.5 MB for the newly added

layers. These results show that the proposed method does not face any issues

due to mixed training schemes and that piggyback masking is a competitive

alternative to full-network ﬁnetuning.

7 Conclusions
We have presented a novel method for utilizing the ﬁxed weights of a network for obtaining good performance on a new task, empirically showing that the proposed method works for multiple datasets and network architectures. We hope that the piggyback method will be useful in practical scenarios where new
1 This is lower than the 63.6 mIOU obtained by [32] owing to diﬀerences in the Caﬀe and PyTorch VGG-16 initializations, as documented at https://goo.gl/quvmm2.

15
skills need to be learned on a deployed device without having to modify existing weights or download a new large network. The re-usability of the backbone network and learned masks should help simplify and scale the learning of a new task across large numbers of potential users and devices. One drawback of our current method is that there is no scope for added tasks to beneﬁt from each other. Only the features learned for the initial task, such as the ImageNet pretraining, are re-used and adapted for new tasks. Apart from addressing this issue, another interesting area for future work is the extension to tasks such as object detection that require specialized layers, and expanding existing layers with more capacity as dictated by the task and accuracy targets.
References
1. Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale image recognition. CoRR abs/1409.1556 (2014)
2. Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., Berg, A.C., Fei-Fei, L.: ImageNet Large Scale Visual Recognition Challenge. IJCV (2015)
3. French, R.M.: Catastrophic forgetting in connectionist networks. Trends in cognitive sciences 3(4) (1999) 128–135
4. Kirkpatrick, J., Pascanu, R., Rabinowitz, N., Veness, J., Desjardins, G., Rusu, A.A., Milan, K., Quan, J., Ramalho, T., Grabska-Barwinska, A., et al.: Overcoming catastrophic forgetting in neural networks. PNAS (2017)
5. Rannen, A., Aljundi, R., Blaschko, M.B., Tuytelaars, T.: Encoder based lifelong learning. In: ICCV. (2017)
6. Li, Z., Hoiem, D.: Learning without forgetting. In: ECCV. (2016) 7. Mallya, A., Lazebnik, S.: PackNet: Adding multiple tasks to a single network by
iterative pruning. arXiv:1711.05769 (2017) 8. Wah, C., Branson, S., Welinder, P., Perona, P., Belongie, S.: The Caltech-UCSD
Birds-200-2011 Dataset. Technical Report CNS-TR-2011-001, California Institute of Technology (2011) 9. Krause, J., Stark, M., Deng, J., Fei-Fei, L.: 3d object representations for ﬁnegrained categorization. In: CVPRW. (2013) 10. Nilsback, M.E., Zisserman, A.: Automated ﬂower classiﬁcation over a large number of classes. In: ICCVGIP. (2008) 11. Saleh, B., Elgammal, A.: Large-scale classiﬁcation of ﬁne-art paintings: Learning the right metric on the right feature. In: ICDMW. (2015) 12. Eitz, M., Hays, J., Alexa, M.: How do humans sketch objects? In: SIGGRAPH. (2012) 13. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: CVPR. (2016) 14. Zagoruyko, S., Komodakis, N.: Wide residual networks. In: BMVC. (2016) 15. Huang, G., Liu, Z., van der Maaten, L., Weinberger, K.Q.: Densely connected convolutional networks. In: CVPR. (2017) 16. Rosenfeld, A., Tsotsos, J.K.: Incremental learning through deep adaptation. arXiv:1705.04228 (2017) 17. Rebuﬃ, S.A., Bilen, H., Vedaldi, A.: Learning multiple visual domains with residual adapters. In: NIPS. (2017)

16
18. Bilen, H., Vedaldi, A.: Integrated perception with recurrent multi-task neural networks. In: NIPS. (2016)
19. Caruana, R.: Multitask learning. In: Learning to learn. (1998) 20. Kokkinos, I.: Ubernet: Training a universal convolutional neural network for low-,
mid-, and high-level vision using diverse datasets and limited memory. In: CVPR. (2017) 21. Shmelkov, K., Schmid, C., Alahari, K.: Incremental learning of object detectors without catastrophic forgetting. In: ICCV. (2017) 22. Lee, S.W., Kim, J.H., Ha, J.W., Zhang, B.T.: Overcoming catastrophic forgetting by incremental moment matching. In: NIPS. (2017) 23. Han, S., Pool, J., Tran, J., Dally, W.: Learning both weights and connections for eﬃcient neural network. In: NIPS. (2015) 24. Fernando, C., Banarse, D., Blundell, C., Zwols, Y., Ha, D., Rusu, A.A., Pritzel, A., Wierstra, D.: PathNet: Evolution channels gradient descent in super neural networks. arXiv:1701.08734 (2017) 25. Rusu, A.A., Rabinowitz, N.C., Desjardins, G., Soyer, H., Kirkpatrick, J., Kavukcuoglu, K., Pascanu, R., Hadsell, R.: Progressive neural networks. arXiv:1606.04671 (2016) 26. Courbariaux, M., Bengio, Y., David, J.P.: Binaryconnect: Training deep neural networks with binary weights during propagations. In: NIPS. (2015) 27. Hubara, I., Courbariaux, M., Soudry, D., El-Yaniv, R., Bengio, Y.: Binarized neural networks. In: NIPS. (2016) 28. Li, F., Zhang, B., Liu, B.: Ternary weight networks. arXiv:1605.04711 (2016) 29. Zhu, C., Han, S., Mao, H., Dally, W.J.: Trained ternary quantization. (2017) 30. Guo, Y., Yao, A., Chen, Y.: Dynamic network surgery for eﬃcient dnns. In: NIPS. (2016) 31. Zhou, B., Lapedriza, A., Khosla, A., Oliva, A., Torralba, A.: Places: A 10 million image database for scene recognition. TPAMI (2017) 32. Long, J., Shelhamer, E., Darrell, T.: Fully convolutional networks for semantic segmentation. In: CVPR. (2015) 33. BerekeleyVision: Segmentation data splits. https://github.com/shelhamer/fcn. berkeleyvision.org/tree/master/data/pascal Accessed: 2018-03-11.

