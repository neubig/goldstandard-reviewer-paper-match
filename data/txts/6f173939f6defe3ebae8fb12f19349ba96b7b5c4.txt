TOWARDS NEURAL DIARIZATION FOR UNLIMITED NUMBERS OF SPEAKERS USING GLOBAL AND LOCAL ATTRACTORS
Shota Horiguchi1, Shinji Watanabe2, Paola Garc´ıa3, Yawen Xue1, Yuki Takashima1, Yohei Kawaguchi1
1Hitachi, Ltd., Japan 2Carnegie Mellon University, USA 3Johns Hopkins University, USA

arXiv:2107.01545v2 [eess.AS] 23 Sep 2021

ABSTRACT
Attractor-based end-to-end diarization is achieving comparable accuracy to the carefully tuned conventional clustering-based methods on challenging datasets. However, the main drawback is that it cannot deal with the case where the number of speakers is larger than the one observed during training. This is because its speaker counting relies on supervised learning. In this work, we introduce an unsupervised clustering process embedded in the attractor-based end-toend diarization. We ﬁrst split a sequence of frame-wise embeddings into short subsequences and then perform attractor-based diarization for each subsequence. Given subsequence-wise diarization results, inter-subsequence speaker correspondence is obtained by unsupervised clustering of the vectors computed from the attractors from all the subsequences. This makes it possible to produce diarization results of a large number of speakers for the whole recording even if the number of output speakers for each subsequence is limited. Experimental results showed that our method could produce accurate diarization results of an unseen number of speakers. Our method achieved 11.84 %, 28.33 %, and 19.49 % on the CALLHOME, DIHARD II, and DIHARD III datasets, respectively, each of which is better than the conventional end-to-end diarization methods. Index Terms: speaker diarization, EEND, EDA, attractor, clustering
1. INTRODUCTION
Analyzing who is speaking when in a multi-talker conversation plays an essential role in many speech-related applications. This task is called speaker diarization, and in recent years there have been extensive efforts toward accurate speaker diarization in various domains [1]. Although the conventional methods based on clustering of speaker embeddings are still powerful baselines [2], end-to-end methods are almost reaching their performance (see the results of the DIHARD III challenge [3]), thanks largely to their way of handling overlapped speech.
While some end-to-end methods are reaching the accuracy of conventional clustering-based approaches that utilize speaker embeddings, they still have difﬁculty in correctly estimating the number of speakers. For clustering-based methods, the number of speakers is determined in the clustering step, making the methods ﬂexible. On the other hand, some end-to-end methods ﬁx the number of speakers [4, 5, 6]. This assumption does not matter in some ﬁxed-domain scenarios (e.g., two-speaker telephone conversations [7], doctor-patient conversations [8], or four-speaker dinner parties [9]), but becomes a crucial problem in the harder scenarios [3]. There are a few recent methods that can handle a ﬂexible number of speakers [10, 11, 12], but since they are trained in a fully supervised manner, they still face the problem of not being able to produce a larger number of speakers

than that observed during training. A solution to the mismatch in the number of speakers has not been established yet.
To tackle this problem, this paper introduces a clustering step into the attractor-based end-to-end neural diarization (EEND-EDA) [10, 13, 14]. We ﬁrst clarify that the maximum number of speakers that can be estimated stems from the attractor calculation part; the embedding extraction part based on stacked Transformer encoders is well generalized to an unseen number of speakers. To extend EEND-EDA to an unlimited number of speakers, we focus on the fact that the number of speakers who speak in a short period is sufﬁciently small [15, 16]. Therefore, given a sequence of frame-wise embeddings, we ﬁrst split them into short subsequences and calculate speaker-wise local attractors and speech activities based on them for each subsequence. Then, the system ﬁnds inter-subsequence attractor correspondence by clustering the vectors converted from the local attractors. The number of clusters, i.e., the number of speakers, is automatically decided by eigenvalue analysis of the afﬁnity matrix calculated based on the local attractors. Because the number of speakers observed in each subsequence can be regarded as small, we can still use the attractor calculation part. We show that the proposed method can produce diarization results for a large number of speakers different from the number of speakers in the training data. We also show that it achieves comparable performance to conventional methods including carefully tuned challenge submissions.

2. EEND-EDA AND ITS LIMITATION

2.1. Brief introduction to EEND-EDA

EEND-EDA [10, 14] is an end-to-end trainable diarization model
that can treat a ﬂexible number of speakers’ overlapping speech.
Given a T -length sequence of F -dimensional acoustic features (xt)Tt=1, EEND-EDA ﬁrst converts them into the same length of D-dimensional embeddings (et)Tt=1 using stacked Transformer encoders without positional encoding. To calculate speaker-wise ﬂexible number of attractors as ∈ (−1, 1)D s, where s is the speaker index, the embeddings are passed through the encoder-
decoder-based attractor calculation module (EDA):

a1, a2, · · · = EDA (e1, . . . , eT ) .

(1)

In this paper, we refer to these attractors calculated from a whole recording as global attractors. We stop the attractor calculation if the attractor existence probability

zˆs = σ wTas + b ∈ (0, 1) (s = 1, 2, . . . )

(2)

is below 0.5, where σ (·) is the sigmoid function and w ∈ RD and b ∈ R are the trainable weights and bias of a fully connected layer,

Fig. 1: Visualization of frame-wise embeddings of simulated 5-speaker mixtures (top) and 6-speaker mixtures (bottom). The embeddings were extracted using the model trained on {1,2,3,4}-speaker mixtures. The color of each dot corresponds to the speaker identity. Overlapped frames are excluded from the visualization.

Subseq#1 Embeddings
Posteriors Diarization loss Labels SSppkk31

Acoustic features

.........

Embeddings

Stacked Transformer encoders

.........

Subseq#2

Split into subsequences

Subseq#L

Local EDA

Local attractors

query

key & value

Transformer decoder

Converted local attractors

Local EDA

Local EDA

Local attractors
query

key &

...

value

Local attractors

query

key & value

Diarization loss
Spk3

Transformer decoder
Converted local attractors

Diarization loss
Spk1 Spk2 Spk3

Transformer decoder
Converted local attractors

Speaker assignment

... ...
... ...

...

Pairwise

...

loss

...

...

...

Affinity matrix

...

Fig. 2: EEND based on local attractors.

respectively. During training, we know the oracle number of speakers S so that S + 1 attractors are calculated to compute the attractor existence loss Lexist:

1 S+1

1 (s ∈ {1, . . . , S})

Lexist = S + 1 H (zs, zˆs) , zs = 0 (s = S + 1) ,

s=1

(3)

where H (zs, zˆs) is the binary cross entropy determined as H (zs, zˆs) = −zs log zˆs − (1 − zs) log (1 − zˆs) . (4)
The estimation of speaker s’s speech activity at t is calculated as the dot product of the corresponding embedding and attractor, as

yˆt,s = σ eTt as ∈ (0, 1) .

(5)

The posterior yˆt,s is optimized using the following diarization loss:

1

TS

Ldiar = T S (φ1,...,aφrTg)∈mΦi(n1,...,T ) t=1 s=1 H (yφt,s, yˆt,s) , (6)

where Φ (1, . . . , T ) is a set of all the permutations of (1, . . . , T ) and yt,s ∈ {0, 1} is the ground-truth label of speaker s’s speech activity at t. The total loss Lglobal is the weighted sum of the diarization loss and attractor existence loss:

Lglobal = Ldiar + αLexist,

(7)

where α is the weighting parameter; we set α = 1 in this paper. Note that the loss Lexist is used to update only w and b in (2), which was found to contribute to improve the performance [14], while the original paper [10] used it to update the parameters of the Transformer encoders and LSTM encoder-decoder as well.

2.2. Limitation of EEND-EDA
EEND-EDA can handle recordings of a ﬂexible number of speakers, but it is empirically known that the number of output speakers is capped by that in the training dataset (see Section 5.2.1). Even if it is adapted to datasets that contain more speakers than four (e.g.,

DIHARD [17, 3]), it is hard to produce diarization results of more than four speakers.
Figure 1 shows the t-SNE visualization of the frame-wise embeddings et using ﬁve- and six-speaker mixtures. Here, EEND-EDA was trained on mixtures that each contain four speakers at most. We can clearly see that the speakers are well-separated in the embedding space even when the input mixture contains a larger number of speakers than the training data. This indicates that the number of output speakers is limited owing to the EDA operation in (1), despite the speaker separation capabilities of the EEND embedding vectors et. In the next section, we explain how this problem is solved in the proposed method.
3. PROPOSED METHOD: NEURAL DIARIZATION BASED ON GLOBAL AND LOCAL ATTRACTORS
3.1. Overview
One possible way to ease this limitation is to train the model using mixtures that contain large numbers of speakers [12]. However, it is difﬁcult to increase the number of speakers inexhaustibly. This is because the EEND’s training procedure depends on a permutation-free objective [18, 19], which takes O T S2 + O S3 operation even with the optimal mapping loss [20]. It is also a problem that EEND uses a ﬁxed length of chunks (e.g. 500 frames) for efﬁcient batch processing during training, and it is rare to contain a large number of speakers within such a chunk even if the whole recording can. Moreover, we do not know how many speakers would be sufﬁcient for training. Therefore, we have to treat the problem without using mixtures with a large number of speakers.
In this paper, we assume that the number of speakers in a short period is small [15, 16]. We apply EDA for each short subsequence to calculate attractors called local attractors, and then cluster the local attractors to ﬁnd inter-subsequence speaker correspondence. Even though the number of speakers for each subsequence is limited, the total number of speakers can be larger than the limitation.

3.2. Training

The schematic diagram of the proposed local-attractor-based di-
arization is shown in Figure 2. As introduced in Section 2.1, given the embeddings (et)Tt=1 =: E, which are output from the stacked Transformer encoders, we ﬁrst split them into mul-
L
tiple short length L subsequences (et)ttl=tl−1+1 l=1, where 0 = t0 < · · · < tL = T . From the l-th sequence, we calculate local attractors a(1l), . . . , a(Sll) =: Al ∈ (−1, 1)D×Sl by using (1), where Sl is the number of speakers active during tl−1 + 1 ≤ t ≤ tl. Note that the local attractors are calculated only for active speakers during each subsequence; thus, even if the S speakers appeared during T frames, the number of speakers in each sequence may be smaller than S, i.e., 0 ≤ Sl ≤ S. The diarization loss L(dlia)r and attractor existence loss L(exl)ist are calculated for each subsequence by using (6) and (3), respectively.
With the two losses, we can estimate the number of speakers and
speech activities for that number of speakers for each subsequence.
Here, the problem is how to ﬁnd whether a pair of attractors from two
subsequences correspond to the same speaker or different speakers.
To optimize the attractor distribution, we deﬁne the training objec-
tive based on the contrastive loss.
Because the local attractors are optimized to minimize the di-
arization error, we convert them to be suitable for clustering. Given
local attractors and frame-wise embeddings for each subsequence,
we ﬁrst convert the attractors using a Transformer decoder:

b(1l), . . . , b(Sll) = TransformerDecoder (Al, E) . (8)

Here, the local attractors Al are queries and the embeddings E are

keys and values input to the Transformer decoder. With the con-

verted local attractors Bl := b(1l), . . . , b(Sll) ∈ RD×Sl , the con-

trastive loss is calculated on the converted vectors from all the sub-

sequences B = [bi]i := [B1, . . . , BL] ∈ RD×S∗ , where S∗ :=

L l=1

Sl.

When

we

calculate

the

subsequence-wise

diarization

loss

L(dlia)r using (6), we ﬁnd the optimal mapping between the estimated

and ground-truth speakers, i.e., we know whether the i-th and j-th

local attractors (or converted local attractors) correspond to the same

speaker. Thus, the pairwise loss can be calculated for each pair of

the converted local attractors as follows:

1

Lpair =

S2cicj (rij (1 − sim (bi, bj )) +

i,j ∈{1,...,S ∗ }

(1 − rij ) [sim (bi, bj ) − δ]+ , (9)
where ci (cj) is the number of attractors that correspond to the i-th (j-th) attractor’s speaker, sim (bi, bj) := bTi bj is the cosine
bi bj similarity between bi and bj, rij is the indicator that takes 1 if bi and bj correspond to the same speaker and 0 otherwise, and [·]+ is the hinge function. This pairwise loss aims to make the angle between converted attractors of the same speaker be zero and those of different speakers be at least arccos δ apart. In this paper, δ is set to 0.5 during pretraining and to 0 during adaptation. Note that this loss is highly inﬂuenced by the instance segmentation in computer vision [21, 22]. The operation of grouping pixel-wise embeddings calculated from a single image into instances is very similar to the current problem of grouping multiple local attractors from a single input into speaker identities. X-vectors or frame-wise embeddings (e.g., (et)Tt=1) cannot be divided by speakers because natural conversations include overlaps. However, each local attractor here corresponds to one speaker so that each local attractor can be hardly

assigned to one of the clusters. During the training, the following loss is used instead of (7) to
optimize the diarization error within each subsequence and the distribution of the local attractors across subsequences:

1L Llocal = L
l=1

L(dlia)r + αL(exl)ist

+ γLpair,

(10)

where γ is the weighting parameter, which is set to 1 in this paper. We found that the model training that fully relies on the local-
attractor-based loss (10) resulted in slow and unstable convergence. To make use of global consistency, we also use the loss that utilizes both local and global attractors, deﬁned as

Lboth = Llocal + Lglobal.

(11)

3.3. Inference

In the inference phase, we ﬁrst estimate the number of speakers Sˆl ∈ Z≥0 using attractor existence probabilities in (2), and then estimate speech activities yˆl ∈ (0, 1)Sˆl×(tl−tl−1) from each subsequence by using (5). We then apply unsupervised clustering for the converted local attractors from all the subsequences. The crucial problem here is how to deﬁne the clustering parameters, e.g., the number of clusters or threshold values.
One approach consists of the following processes: 1) construct an afﬁnity matrix, 2) calculate its graph Laplacian, 3) conduct eigenvalue decomposition, and 4) estimate the number of speakers based on the maximum eigengap. Because the afﬁnity matrix calculated from speaker embeddings often contains unreliable values, it is important to remove noises from it. For example, Gaussian blur was applied to smooth the afﬁnity matrix calculated from d-vectors extracted using sliding window [23]. For our local-attractor-based method, however, smoothing cannot be used because attractors are calculated not only for each subsequence but also for each speaker within a subsequence. In [24], p nearest neighbor binarization was applied to the afﬁnity matrix to remove unreliable values. The value of p is selected automatically, and empirically dozens of nearest neighbors are used. In our case, however, this is not suitable because local attractors are calculated every ﬁve seconds in this paper, so the number of vectors to be clustered is extremely insufﬁcient.
Let us consider why the noise inhibits the accurate approximation of the number of clusters in the eigengap-based estimation. One reason is that the eigenvalues of a graph Laplacian are obtained without considering the size of clusters; thus, noises produce a lot of tiny clusters. To penalize more on small clusters, we directly use an afﬁnity matrix instead of its graph Laplacian. Given the afﬁnity matrix R = (rij ) ∈ [−1, 1]S∗×S∗ , where rij = sim (bi, bj ), we apply matrix decomposition as

R = V λ1 . . .

V −1,

(12)

λS∗

where V

∈

S ∗ ×S ∗
R

is the eigenvectors and λ1

> · · · > λS∗

are the

eigenvalues of R. Because R is positive-semideﬁnite, the eigenval-

ues are non-negative, and they indicate the size of each cluster where

each local attractor is softly assigned. The number of clusters can be

estimated by using the eigenratio instead of eigengap, as

Sˆ = min λs+1 .

(13)

1≤s≤S∗−1 λs

In this paper, we use the hinge function in (9). We also know that the attractors from the same subsequence must be assigned to dif-

ferent clusters. Therefore, we use the modiﬁed afﬁnity matrix R = rij ∈ [0, 1]S∗×S∗ instead of R, deﬁned as

1 (i = j)


(bi and bj are from

rij =

the same subsequence) , (14)

 

1

1−δ

[sim

(bi,

bj )

−

δ]+

(otherwise)

where 1 (cond) is an indicator function that returns 1 if cond is true

and 0 otherwise. We then apply matrix decomposition as in (12)

and obtain eigenvalues λ1 ≥ · · · ≥ λS∗ . Indeed, R is no longer

positive-semideﬁnite but its eigenvalues are still good indicators of

the size of clusters. Because the eigenvalues indicate the size of clus-

ters, we only use those of not less than one to estimate the number of speakers Sˆ as

Sˆ = min λs+1 . 1≤s≤S∗−1 λs
λs ≥1

(15)

Even if we force the afﬁnity value between a pair of local attractors from the same subsequence to be zero in R , the local attractors may belong to the same cluster; thus, the number of estimated speakers can be less than the maximum number of speakers from one of the subsequences. Therefore, we update the estimation of the number of speakers by

Sˆ ← max Sˆ , max Sl .

(16)

1≤l≤L

After the number of speakers is estimated, we apply a clustering method to the local attractors. We know that the attractors from the same subsequence have to be assigned to different clusters, so it is effective to use cannot-link constraints for clustering. One possible choice is to use COP-Kmeans clustering [25], which is used in EEND-vector clustering [26, 27], but this has a difﬁculty in the case where cannot-link constraints have to be satisﬁed; the algorithm sometimes results in no solution. Thus, we used the CLC-Kmeans algorithm [28] instead for stable convergence.
The model that is trained using (11) also has outputs based on the global branch. They are still useful when the number of clusters is small because the model is trained in a fully supervised manner. Therefore, we propose using global- and local-attractor-based inference depending on the estimated number of speakers. We ﬁrst estimate the number of speakers by using (2) calculated from global attractors. In this paper, we trained the model using {1,2,3,4}-speaker mixtures; thus, if the number of estimated speakers is less than four, we use inference based on global attractors explained in Section 2.1. If the estimated number of speakers is equal to or larger than four, we use inference based on local attractors explained in this section. We call this inference switching strategy.

4. RELATED WORKS
4.1. Speaker diarization
The clustering-based methods generally consist of the following: speech activity detection (SAD), speaker embedding extraction, clustering of the embeddings, and optional overlap assignment. The SAD is often replaced by the oracle speech segments, but the remaining parts are actively being studied, e.g., investigation of better architectures for speaker embedding extractors [29], better clustering methods [2, 30, 31], and better overlap assignment methods [32]. An important property of the clustering-based methods is that they do not limit the number of speakers that can be estimated during inference because the results are obtained by unsupervised

clustering. On the other hand, neural-network-based diarization methods
are rapidly emerging to replace the conventional clustering-based methods, but they still have a limitation. For example, personal VAD [33] and VoiceFilter-Lite [34] assume that the target speaker’s d-vector is available during inference, so they are not suitable for speaker-independent diarization. Target-speaker voice activity detection [5] and the initial models of end-to-end neural diarization (EEND) [4] ﬁx the output number of speakers, so they are not suitable for diarization of unknown numbers of speakers. The recurrent selective attention network (RSAN) [35] or some extensions of EEND [10, 11] can deal with ﬂexible numbers of speakers. However, EEND-based models empirically limit the number of output speakers by the number of speakers in the training datasets. It is unclear whether the RSAN can deal with the number of speakers limitation because only the speaker counting accuracy on matched conditions (zero, one, or two speakers) has been reported. EEND as post-processing [36] tackled the problem of the limitation of the number of speakers by taking advantage of both clustering-based methods and end-to-end methods through the utilization of EEND to reﬁne the results from clustering, but a single model solution for this problem is still awaited.
The most relevant work is the recently proposed EEND-vector clustering, which incorporates EEND and speaker embeddings [26, 27], but it differs from ours in a few key ways. One is that EEND-vector clustering relies on the speaker embedding dictionary, which requires speaker identities across recordings in the training set. Such information is accessible in the simulated mixtures created from single-speaker recordings (e.g., NIST SRE, Librispeech [37], VoxCeleb [38]), but is not always available in real conversation datasets (e.g., DIHARD [17, 3]). Our method only utilizes the speaker information within each recording so that it can use such datasets for training. Another difference is that EEND-vector clustering calculates speaker embeddings for each block, and it has been reported in [27] that the block size needs to be somewhat long (e.g. 30 s) to obtain reliable speaker embeddings. However, since the number of speakers that can appear in a block is limited by the network architecture, increasing the block size also limits the number of speakers that can appear in the ﬁnal diarization results. In contrast, in our method, we process a sequence of acoustic features by means of a stacked Transformer encoder before splitting them into subsequences. Therefore, the frame-wise embeddings et can capture global context, and thus, we can use a shorter size of subsequence (5 s in this paper) than EEND-vector clustering.
4.2. Efforts to produce results for larger number of speakers than that observed during training
Supervised speech processing methods sometimes suffer from the number of speakers mismatch between training and inference, especially when more speakers appear during inference than during training. Some neural-network-based speech separation methods [19, 39, 40, 41, 42] limit the number of outputs by their network architecture, and thus, there is no way to deal with the mismatch. Even if the method itself is designed not to limit the number of speakers, there is rarely experimental evidence to show how the model actually works under mismatched conditions [43, 44]. In terms of speaker diarization, some EEND-based models [10, 11] can deal with ﬂexible numbers of speakers, but the mismatch of the number of speakers remains an open question. In this subsection, we introduce two successful speech processing approaches for the mismatch.
The ﬁrst approach is one-and-rest permutation invariant training

(OR-PIT) [45], which aims to split a mixture into the one-speaker waveform and the mixture of the rest of the speakers as residual output. Even though the model is trained only on two- and three-speaker mixtures, the experimental results demonstrated that it worked well on four-speaker mixtures. To adopt the one-vs-rest approach, it is necessary to decide the residual output. In the context of speech separation, this can be easily determined by waveforms or timefrequency masks. However, in the context of diarization, we cannot determine such residual output because we cannot assume the maximum number of sources for each frame; thus, it cannot be used for diarization.
The second approach is to introduce unsupervised clustering into the decoding step. Deep clustering paper [18] reported that timefrequency-bin-wise embeddings somewhat worked for three-speaker mixtures even if the model was only trained on two-speaker mixtures. However, the number of speakers was assumed to be given and it is unclear whether we can share the same clustering parameters for matched and mismatched conditions.
5. EXPERIMENTS
5.1. Settings
To train the proposed model, we created simulated multi-talker recordings using NIST SRE and Switchboard corpora following [4]. The average silence duration β was varied to get a similar overlap ratio for each number of speakers, as shown in Table 1a. For training, we created {1,2,3,4}-speaker simulated mixtures. In addition, we created 5- and 6-speaker mixtures for evaluation to show that the proposed method can deal with a larger number of speakers than that observed during training. See [4] for the detailed protocol.
We also used real recordings summarized in Table 1b for evaluation. For CALLHOME, we used Part 1 for model adaptation and Part 2 for testing. For DIHARD II and III, we used the development set for adaptation and the evaluation set for testing.
The original EEND-EDA [10] was ﬁrstly trained on 2-speaker mixtures and then ﬁnetuned on {1,2,3,4}-speaker mixtures. We found that training with subsequences from scratch resulted in poor model performance, so we ﬁrst trained the model using 2-speaker mixtures with global attractors for 100 epochs and then ﬁnetuned it on {1,2,3,4}-speaker mixtures with the proposed method for another 50 epochs. In the real dataset evaluations, we adapted the model for an additional 100 epochs on each adaptation set. The Adam optimizer [46] was used during training, with the Noam scheduler [47] with 100,000 warm-up steps for simulation-dataset-based training and a ﬁxed learning rate of 1 × 10−5 for adaptation.
As Transformer encoders, we used four-stacked encoders, which align with the experimental settings in the EEND-EDA papers [10, 14]. We also used six-stacked encoders with eight attention heads following the setting in the EEND-vector clustering paper [27]. As input features for the encoders, we used 345-dimensional log-mel ﬁlterbank-based acoustic features obtained every 100 ms following [4, 10]. During training, the length of a sequence was set to 50 s, i.e., L = 500, and the length of a subsequence was set to 5 s, i.e., tl = 50l for l ∈ {0, . . . , 10}. During inference, each whole recording was processed at once and the length of each subsequence was set to 5 s. To obtain high-resolution results for the DIHARD datasets, we used acoustic features extracted every 50 ms.
For evaluation metrics, we used diarization error rates (DERs) and Jaccard error rates (JERs). Following prior studies [4, 10], we allowed the collar tolerance of 0.25 s in the evaluations using the simulated datasets and the CALLHOME dataset, while we did not

Table 1: Dataset to train and test our diarization models. (a) Simulated datasets

Dataset

#Spk #Mixtures β Overlap ratio (%)

Train Sim1spk 1

100,000 2

0.0

Sim2spk 2

100,000 2

34.1

Sim3spk 3

100,000 5

34.2

Sim4spk 4

100,000 9

31.5

Test

Sim1spk 1

500

2

0.0

Sim2spk 2

500

2

34.4

Sim3spk 3

500

5

34.7

Sim4spk 4

500

9

32.0

Sim5spk 5

500

13

30.7

Sim6spk 6

500

17

29.9

Dataset Adaptation
Test

(b) Real datasets

CALLHOME [7] DIHARD II [17] DIHARD III [3]
CALLHOME [7] DIHARD II [17] DIHARD III [3]

Split
Part 1 dev dev
Part 2 eval eval

#Spk
2–7 1–10 1–10
2–6 1–9 1–9

#Mixtures
249 192 254
250 194 259

Overlap ratio (%)
17.0 9.8 10.7
16.7 8.9 9.2

allow such collar in the evaluation of the DIHARD datasets. Note that we did not exclude overlapped speech from the evaluation.
5.2. Results
5.2.1. Simulated data
To show that the proposed method can deal with an unseen number of speakers, we ﬁrst evaluated our model on the simulated datasets. The results are shown in Table 2. As an x-vector clustering baseline, we used the Kaldi recipe 1, which resulted in poor performance (ﬁrst row). EEND-EDA performed well in matched conditions but DERs were rapidly degraded in mismatched conditions (second row). We also show the results when a maximum of four attractors were used i.e., ﬁfth and later attractors were ignored (third row). DERs on the four-, ﬁve-, and six-speaker mixtures were improved by limiting the number of attractors (third row). These results indicate that the ﬁfth and subsequent attractors are of no use even if EDA estimates that the number of speakers is larger than four.
The proposed method trained using Llocal in (10) performed better in mismatched conditions (fourth row), and the combined use of Lglobal and Llocal in (11) further improved the DERs in both matched and mismatched conditions (ﬁfth row). However, local-attractorbased inference did not perform well in matched conditions, especially when the number of speakers was small (e.g., one or two). This is because a small error in the estimated number of speakers (e.g., ±1) can signiﬁcantly degrade the DER. By using global- and localattractor-based estimation depending on the number of estimated speakers, the proposed method performed well in both matched and mismatched conditions (sixth row). Increasing the number of Transformer encoders further improved the DERs in matched conditions, but the DERs in mismatched conditions were slightly degraded (seventh row). This may be because the Transformers were overtrained to distinguish the seen number of speakers. For comparison, we also show the results of the model that is trained on mixtures of at most ﬁve speakers in the last row, which is drawn from [14]. It performed
1https://github.com/kaldi-asr/kaldi/tree/master/ egs/callhome_diarization/v2

Table 2: DERs (%) on the simulated datasets. Note that ﬁve- and six-speaker mixtures were not included in the training set. The best scores are bolded and the second best are underlined. #Blocks: the number of Transformer encoder blocks. Switch: use of global- and local-attractor-based inference depending on the estimated number of speakers.

#Speakers

matched

mismatched

#Blocks 1 2 3 4 5 6

X-vector clustering

N/A 37.42 7.74 11.46 22.45 31.00 38.62

EEND-EDA [10, 14]

4 0.15 3.19 6.60 9.26 23.11 34.97

EEND-EDA [10, 14] †

4 0.15 3.19 6.60 8.68 22.43 33.28

Proposed (Llocal)

4 8.85 12.71 10.31 11.14 14.11 19.36

Proposed (Llocal + Lglobal)

4 2.84 10.21 7.54 9.08 12.40 18.03

Proposed (Llocal + Lglobal, switch) 4 0.25 3.53 6.79 8.98 12.44 17.98

Proposed (Llocal + Lglobal, switch) 6 0.09 3.54 5.74 6.79 12.51 20.42

EEND-EDA [10, 14] ‡

4 0.36 3.65 7.70 9.97 11.95 22.59

† At most four attractors were used. ‡ Trained on Sim{1,2,3,4,5}spk. At most ﬁve attractors were used.

Table 3: Confusion matrices for speaker counting.

(a) EEND-EDA [10, 14]

(b) Proposed (Llocal+Lglobal, switch, #Blocks = 4)

Ref. #Speakers 123456
1 500 0 0 0 0 0 2 0 482 0 0 0 0 3 0 17 435 5 1 0 4 0 1 65 447 224 139 5 0 0 0 48 268 337 6 0 0 0 0 7 24 7+ 0 0 0 0 0 0

Ref. #Speakers 123456
1 498 0 0 0 0 0 2 2 474 0 0 0 0 3 0 25 451 17 2 1 4 0 1 33 412 78 30 5 0 0 10 62 361 183 6 0 0 6 7 47 229 7+ 0 0 0 2 12 57

Pred. #Speakers Pred. #Speakers

well on Sim5spk because it used ﬁve-speaker mixtures for training, but the DER sharply fell when six-speaker mixtures were input. The proposed method achieved a comparative performance on Sim5spk even though it did not see ﬁve-speaker mixtures during training, and it also outperformed EEND-EDA on Sim6spk.
The confusion matrices for speaker counting are shown in Table 3. We can clearly see that the proposed method could estimate the number of speakers in mismatched conditions with higher accuracies than the conventional EEND-EDA. Indeed, EEND-EDA sometimes estimated the number of speakers as more than four, but considering the results in Table 2, the ﬁfth and sixth attractors did not represent the ﬁfth and sixth speakers.

5.2.2. CALLHOME
We also evaluated our method on the CALLHOME dataset. As comparison methods, the state-of-the-art x-vector-based method (VBx) [2] and several EEND-based methods that can deal with a ﬂexible number of speakers [48, 10, 14, 27] were adopted. Note that VBx used the oracle speech segments while EEND-based methods estimated speech activities from the input audio.
Table 4 shows the number-of-speakers-wise DERs. Our method achieved 11.92 % and 11.84 % DERs by using four- and six-stacked Transformer encoders as a backbone, respectively, which were better than the conventional methods. Compared with EEND-EDA, the proposed method improved the DERs, especially when the number of speakers was large.

Table 4: DERs (%) on CALLHOME dataset. The switching strategy was used for the proposed method.

Method

#Blocks 2

VBx [2] †

N/A 9.44

SC-EEND [48]

4 9.57

EEND-EDA [10, 14]

4 7.83

EEND-vector clust. [27]

6 7.94

Proposed (Llocal + Lglobal, switch) 4 6.94

Proposed (Llocal + Lglobal, switch) 6 7.11

† Oracle speech segments were used.

#Speakers

345

13.89 14.00 12.29 11.93 11.42 11.88

16.05 21.14 17.59 16.38 14.49 14.37

13.87 31.07 27.66 21.21 29.76 25.95

6
24.73 37.06 37.17 23.10 24.09 21.95

All
13.28 15.75 13.65 12.49 11.92 11.84

Table 5: DERs / JERs (%) on DIHARD II dataset.

Method
VBx + overlap handling [49] EEND-EDA [10, 14] Proposed (Llocal + Lglobal, switch) Proposed (Llocal + Lglobal, switch)

#Blocks
N/A 4 4 6

#Speakers

≤4

≥5

21.34 / 43.00 39.85 / 57.40 22.09 / 40.70 47.66 / 71.49 22.24 / 40.47 44.92 / 69.34 21.40 / 37.87 43.62 / 68.09

All
27.11 / 49.07 30.07 / 53.69 29.31 / 52.64 28.33 / 50.62

Table 6: DERs / JERs (%) on DIHARD III dataset.

Method
VBx + overlap handling [50] EEND-EDA [10, 14] Proposed (Llocal + Lglobal, switch) Proposed (Llocal + Lglobal, switch)

#Blocks
N/A 4 4 6

#Speakers

≤4

≥5

16.38 / 29.43 42.51 / 53.47 15.55 / 27.23 48.30 / 71.76 14.39 / 25.85 44.32 / 68.06 13.64 / 24.60 43.67 / 69.33

All
21.47 / 37.83 21.94 / 42.79 20.23 / 40.60 19.49 / 40.23

5.2.3. DIHARD II & III
Finally, we evaluated our method on the DIHARD II and III datasets, which contain mixtures of at most nine speakers. We used VBxbased systems by BUT [49] and the Hitachi-JHU team [50] as clustering-based baselines. Because they are challenge submissions, each of them is carefully tuned to concatenate multiple modules including speech activity detection, speech dereverberation, x-vector extraction, probabilistic linear discriminant analysis scoring, and overlap detection and assignment. EEND-EDA was also used as a comparison method.
Tables 5 and 6 show DERs and JERs on the DIHARD II and III datasets, respectively. The proposed method improved DERs especially when the number of speakers was larger than four compared to EEND-EDA, and achieved 28.33 % DER on the DIHARD II dataset and 19.49 % DER on the DIHARD III dataset with sixstacked Transformer encoders. These DERs are close to those of carefully tuned challenge submissions, and in particular even better on the DIHARD III dataset.

6. CONCLUSION
In this paper, we proposed a neural diarization method based on global and local attractors. An input sequence of acoustic features is ﬁrst converted into a sequence of frame-wise embeddings using stacked Transformer encoders and then divided into short subsequences. We calculate the diarization results based on speaker-wise local attractors for each subsequence, followed by unsupervised clustering based on the local attractors to ﬁnd the optimal correspondence between subsequences. The experimental results demonstrated the effectiveness of our method, especially when the number of speakers is large. Future work will include an online extension of the proposed method using online clustering methods.

7. REFERENCES
[1] Tae Jin Park, Naoyuki Kanda, Dimitrios Dimitriadis, Kyu J. Han, Shinji Watanabe, and Shrikanth Narayanan, “A review of speaker diarization: Recent advances with deep learning,” arXiv:2101.09624, 2021.
[2] Federico Landini, Ja´n Profant, Mireia Diez, and Luka´sˇ Burget, “Bayesian HMM clustering of x-vector sequences (VBx) in speaker diarization: Theory, implementation and analysis on standard tasks,” Computer Speech & Language, vol. 71, pp. 101254, 2022.
[3] Neville Ryant, Prachi Singh, Venkat Krishnamohan, Rajat Varma, Kenneth Church, Christopher Cieri, Jun Du, Sriram Ganapathy, and Mark Liberman, “The third DIHARD diarization challenge,” in INTERSPEECH, 2021, pp. 3570–3574.
[4] Yusuke Fujita, Naoyuki Kanda, Shota Horiguchi, Yawen Xue, Kenji Nagamatsu, and Shinji Watanabe, “End-to-end neural speaker diarization with self-attention,” in ASRU, 2019, pp. 296–303.
[5] Ivan Medennikov, Maxim Korenevsky, Tatiana Prisyach, Yuri Khokhlov, Mariya Korenevskaya, Ivan Sorokin, Tatiana Timofeeva, Anton Mitrofanov, Andrei Andrusenko, Ivan Podluzhny, Aleksandr Laptev, and Aleksei Romanenko, “Target-speaker voice activity detection: a novel approach for multi-speaker diarization in a dinner party scenario,” in INTERSPEECH, 2020, pp. 274–278.
[6] Yi Chieh Liu, Eunjung Han, Chul Lee, and Andreas Stolcke, “End-to-end neural diarization: From transformer to conformer,” in INTERSPEECH, 2021, pp. 3081–3085.
[7] “2000 NIST Speaker Recognition Evaluation,” https:// catalog.ldc.upenn.edu/LDC2001S97.
[8] Laurent El Shafey, Hagen Soltau, and Izhak Shafran, “Joint speech recognition and speaker diarization via sequence transduction,” in INTERSPEECH, 2019, pp. 396–400.
[9] Shinji Watanabe, Michael Mandel, Jon Barker, Emmanuel Vincent, Ashish Arora, Xuankai Chang, Sanjeev Khudanpur, Vimal Manohar, Daniel Povey, Desh Raj, David Snyder, Aswin Shanmugam Subramanian, Jan Trmal, Bar Ben Yair, Christoph Boeddeker, Zhaoheng Ni, Yusuke Fujita, Shota Horiguchi, Naoyuki Kanda, Takuya Yoshioka, and Neville Ryant, “CHiME-6 Challenge: Tackling multispeaker speech recognition for unsegmented recordings,” in CHiME-6, 2020.
[10] Shota Horiguchi, Yusuke Fujita, Shinji Wananabe, Yawen Xue, and Kenji Nagamatsu, “End-to-end speaker diarization for an unknown number of speakers with encoder-decoder based attractors,” in INTERSPEECH, 2020, pp. 269–273.
[11] Yuki Takashima, Yusuke Fujita, Shinji Watanabe, Shota Horiguchi, Paola Garc´ıa, and Kenji Nagamatsu, “End-to-end speaker diarization conditioned on speech activity and overlap detection,” in SLT, 2021, pp. 849–856.
[12] Soumi Maiti, Hakan Erdogan, Kevin Wilson, Scott Wisdom, Shinji Watanabe, and John R. Hershey, “End-to-end diarization for variable number of speakers with local-global networks and discriminative speaker embeddings,” in ICASSP, 2021, pp. 7183–7187.
[13] Eunjung Han, Chul Lee, and Andreas Stolcke, “BW-EDAEEND: Streaming end-to-end neural speaker diarization for a variable number of speakers,” in ICASSP, 2021, pp. 7193– 7197.

[14] Shota Horiguchi, Yusuke Fujita, Shinji Watanabe, Yawen Xue, and Paola Garc´ıa, “Encoder-decoder based attractor calculation for end-to-end neural diarization,” arXiv:2106.10654, 2021.
[15] Takuya Yoshioka, Igor Abramovski, Cem Aksoylar, Zhuo Chen, Moshe David, Dimitrios Dimitriadis, Yifan Gong, Ilya Gurvich, Xuedong Huang, Yan Huang, Aviv Hurvitz, Li Jiang, Sharon Koubi, Eyal Krupka, Ido Leichter, Changliang Liu, Partha Parthasarathy, Alon Vinnikov, Lingfeng Wu, Xiong Xiao, Wayne Xiong, Huaming Wang, Zhenghao Wang, Jun Zhang, Yong Zhao, and Tianyan Zhou, “Advances in online audio-visual meeting transcription,” in ASRU, 2019, pp. 276– 283.
[16] Zhuo Chen, Takuya Yoshioka, Liang Lu, Tianyan Zhou, Zhong Meng, Yi Luo, Jian Wu, Xiong Xiao, and Jinyu Li, “Continuous speech separation: Dataset and analysis,” in ICASSP, 2020, pp. 7284–7288.
[17] Neville Ryant, Kenneth Church, Christopher Cieri, Alejandrina Cristia, Jun Du, Sriram Ganapathy, and Mark Liberman, “The Second DIHARD Diarization Challenge: Dataset, task, and baselines,” in INTERSPEECH, 2019, pp. 978–982.
[18] John R. Hershey, Zhuo Chen, Jonathan Le Roux, and Shinji Watanabe, “Deep clustering: Discriminative embeddings for segmentation and separation,” in ICASSP, 2016, pp. 31–35.
[19] Dong Yu, Morten Kolbæk, Zheng-Hua Tan, and Jesper Jensen, “Permutation invariant training of deep models for speakerindependent multi-talker speech separation,” in ICASSP, 2017, pp. 241–245.
[20] Qingjian Lin, Tingle Li, Lin Yang, Junjie Wang, and Ming Li, “Optimal mapping loss: A faster loss for end-to-end speaker diarization,” in Odyssey, 2020, pp. 125–131.
[21] Alireza Fathi, Zbigniew Wojna, Vivek Rathod, Peng Wang, Hyun Oh Song, Sergio Guadarrama, and Kevin P Murphy, “Semantic instance segmentation via deep metric learning,” arXiv:1703.10277, 2017.
[22] Shu Kong and Charless C Fowlkes, “Recurrent pixel embedding for instance grouping,” in CVPR, 2018, pp. 9018–9028.
[23] Quan Wang, Carlton Downey, Li Wan, Phlip Andrew Mansﬁeld, and Ignacio Lopez Moreno, “Speaker diarization with LSTM,” in ICASSP, 2018, pp. 5239–5243.
[24] Tae Jin Park, Kyu J. Han, Manoj Kumar, and Shrikanth Narayanan, “Auto-tuning spectral clustering for speaker diarization using normalized maximum eigengap,” IEEE Signal Processing Letters, vol. 27, pp. 381–385, 2020.
[25] Kiri Wagstaff, Claire Cardie, Seth Rogers, Stefan Schroedl, et al., “Constrained k-means clustering with background knowledge,” in ICML, 2001, pp. 577–584.
[26] Keisuke Kinoshita, Marc Delcroix, and Naohiro Tawara, “Integrating end-to-end neural and clustering-based diarization: Getting the best of both worlds,” in ICASSP, 2021, pp. 7198– 7202.
[27] Keisuke Kinoshita, Marc Delcroix, and Naohiro Tawara, “Advances in integration of end-to-end neural and clustering-based diarization for real conversational speech,” in INTERSPEECH, 2021, pp. 3565–3569.
[28] Yan Yang, Tonny Rutayisire, Chao Lin, Tianrui Li, and Fei Teng, “An improved Cop-Kmeans clustering for solving constraint violation based on MapReduce framework,” Fundamenta Informaticae, vol. 29, no. 4, pp. 301–318, 2013.

[29] Tianyan Zhou, Yong Zhao, and Jian Wu, “ResNeXt and Res2Net structures for speaker veriﬁcation,” in SLT, 2021, pp. 301–307.
[30] Aonan Zhang, Quan Wang, Zhenyao Zhu, John Paisley, and Chong Wang, “Fully supervised speaker diarization,” in ICASSP, 2019, pp. 6301–6305.
[31] Qiujia Li, Florian L. Kreyssig, Chao Zhang, and Philip C. Woodland, “Discriminative neural clustering for speaker diarisation,” in SLT, 2021, pp. 574–581.
[32] Latane´ Bullock, Herve´ Bredin, and Leibny Paola GarciaPerera, “Overlap-aware diarization: Resegmentation using neural end-to-end overlapped speech detection,” in ICASSP, 2020, pp. 7114–7118.
[33] Shaojin Ding, Quan Wang, Shuo-yiin Chang, Li Wan, and Ignacio Lopez Moreno, “Personal VAD: Speaker-conditioned voice activity detection,” in Odyssey, 2020, pp. 433–439.
[34] Quan Wang, Ignacio Lopez Moreno, Mert Saglam, Kevin Wilson, Alan Chiao, Renjie Liu, Yanzhang He, Wei Li, Jason Pelecanos, Marily Nika, and Alexander Gruenstein, “VoiceFilterLite: Streaming targeted voice separation for on-device speech recognition,” in INTERSPEECH, 2020, pp. 2677–2681.
[35] Keisuke Kinoshita, Marc Delcroix, Shoko Araki, and Tomohiro Nakatani, “Tackling real noisy reverberant meetings with all-neural source separation, counting, and diarization system,” in ICASSP, 2020, pp. 381–385.
[36] Shota Horiguchi, Paola Garcia, Yusuke Fujita, Shinji Watanabe, and Kenji Nagamatsu, “End-to-end speaker diarization as post-processing,” in ICASSP, 2021, pp. 7188–7192.
[37] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur, “LibriSpeech: An ASR corpus based on public domain audio books,” in ICASSP, 2015, pp. 5206–5210.
[38] Arsha Nagrani, Joon Son Chung, Weidi Xie, and Andrew Zisserman, “VoxCeleb: Large-scale speaker veriﬁcation in the wild,” Computer Speech & Language, vol. 60, pp. 101027, 2020.
[39] Zhong-Qiu Wang, Johathan Le Roux, and John R Hershey, “Alternative objective functions for deep clustering,” in ICASSP, 2018, pp. 686–690.
[40] Yi Luo and Nima Mesgarani, “TasNet: time-domain audio separation network for real-time, single-channel speech separation,” in ICASSP, 2018, pp. 696–700.
[41] Yi Luo and Nima Mesgarani, “Conv-TasNet: Surpassing ideal time–frequency magnitude masking for speech separation,” IEEE/ACM TASLP, vol. 27, no. 8, pp. 1256–1266, 2019.
[42] Eliya Nachmani, Yossi Adi, and Lior Wolf, “Voice separation with an unknown number of multiple speakers,” in ICML, 2020, pp. 7164–7175.
[43] Zhuo Chen, Yi Luo, and Nima Mesgarani, “Deep attractor network for single-microphone speaker separation,” in ICASSP, 2017, pp. 246–250.
[44] Neil Zeghidour and David Grangier, “Wavesplit: End-to-end speech separation by speaker clustering,” IEEE TASLP, vol. 29, pp. 2840–2849, 2021.
[45] Naoya Takahashi, Sudarsanam Parthasaarathy, Nabarun Goswami, and Yuki Mitsufuji, “Recursive speech separation for unknown number of speakers,” in INTERSPEECH, 2019, pp. 1348–1352.

[46] Diederik P. Kingma and Jimmy Ba, “Adam: A method for stochastic optimization,” in ICLR, 2015.
[47] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz Kaiser, and Illia Polosukhin, “Attention is all you need,” in NeurIPS, 2017, pp. 5998–6008.
[48] Yusuke Fujita, Shinji Watanabe, Shota Horiguchi, Yawen Xue, Jing Shi, and Kenji Nagamatsu, “Neural speaker diarization with speaker-wise chain rule,” arXiv:2006.01796, 2020.
[49] Federico Landini, Shuai Wang, Mireia Diez, Luka´sˇ Burget, Pavel Mateˇjka, Kateˇrina Zˇ mol´ıkova´, Ladislav Mosˇner, Anna Silnova, Oldˇrich Plchot, Ondˇrej Novotny`, Hossein Zeinali, and Johan Rohdin, “BUT system for the Second DIHARD Speech Diarization Challenge,” in ICASSP, 2020, pp. 6529–6533.
[50] Shota Horiguchi, Nelson Yalta, Paola Garcia, Yuki Takashima, Yawen Xue, Desh Raj, Zili Huang, Yusuke Fujita, Shinji Watanabe, and Sanjeev Khudanpur, “The Hitachi-JHU DIHARD III system: Competitive end-to-end neural diarization and x-vector clustering systems combined by DOVER-Lap,” in DIHARD III, 2021.

