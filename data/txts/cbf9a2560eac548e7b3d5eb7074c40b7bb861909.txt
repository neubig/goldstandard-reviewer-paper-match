END-TO-END SPEAKER DIARIZATION CONDITIONED ON SPEECH ACTIVITY AND OVERLAP DETECTION
Yuki Takashima1, Yusuke Fujita1, Shinji Watanabe2, Shota Horiguchi1, Paola Garc´ıa2, Kenji Nagamatsu1
1 Hitachi, Ltd. Research & Development Group, Japan 2 Center for Language and Speech Processing, Johns Hopkins University, USA

arXiv:2106.04078v1 [eess.AS] 8 Jun 2021

ABSTRACT
In this paper, we present a conditional multitask learning method for end-to-end neural speaker diarization (EEND). The EEND system has shown promising performance compared with traditional clustering-based methods, especially in the case of overlapping speech. In this paper, to further improve the performance of the EEND system, we propose a novel multitask learning framework that solves speaker diarization and a desired subtask while explicitly considering the task dependency. We optimize speaker diarization conditioned on speech activity and overlap detection that are subtasks of speaker diarization, based on the probabilistic chain rule. Experimental results show that our proposed method can leverage a subtask to effectively model speaker diarization, and outperforms conventional EEND systems in terms of diarization error rate.
Index Terms— speaker diarization, multitask learning, chain rule, neural network, end-to-end
1. INTRODUCTION
Speaker diarization is the process of partitioning a speech recording into homogeneous segments associated with each speaker. This process is an essential part of multi-speaker audio applications such as generating transcriptions from meetings [1, 2]. Recent studies [3, 4] have shown that accurate diarization improves the performance of automatic speech recognition (ASR). Therefore, more advanced speaker diarization is required.
The traditional speaker diarization approach is a clusteringbased method that relies on multiple steps: speech activity detection (SAD), speech segmentation, feature extraction, and clustering. SAD is the process to ﬁlter out the non-speech parts from an input speech. Speech regions are then split into multiple speaker-homogeneous segments, and frame-level speaker embeddings are extracted. The recent progress on deep learning has made it possible to compute better speaker representation such as x-vectors [5] and d-vectors [6]. Once, the embeddings are obtained, a clustering method is needed. The methods commonly used are agglomerative hierarchical

clustering (AHC) [7], k-means clustering [8], spectral clustering [9, 10], and afﬁnity propagation [11]. Recently, neural network-based clustering has been explored [12]. Although clustering-based methods performed well, they are not optimized to directly minimize diarization errors because the clustering is an unsupervised method. To directly minimize diarization errors in a supervised manner, clustering-free methods have been studied [13, 14, 15].
End-to-end neural diarization (EEND) [14, 15] is a promising direction for speaker diarization. EEND uses a single neural network that maps a multi-speaker audio to joint speech activities of multiple speakers. In contrast to most of the clustering-based methods, EEND handles overlapping speech without using any external module. Most recently, EEND has been extended to handle a ﬂexible number of speakers [16, 17]. Fujita et al. [16] proposed speakerwise conditional EEND (SC-EEND) that produces a speech activity of one speaker conditioned by speech activities of previously estimated speakers. However, the performance of SC-EEND is not enough for real recordings of conversation due to the low performance of SAD. To tackle this problem, we employ a multitask learning approach that optimizes not only speaker diarization but its subtask, such as SAD and overlap detection (OD).
In this paper, we propose a conditional multitask learning framework with the subtask of speaker diarization for SC-EEND. Multitask learning [18, 19] is a learning strategy where a model simultaneously executes multiple tasks for one input data. In this work, we utilize easier subtasks of speaker diarization such as SAD and OD as additional tasks. It is expected that these subtasks have useful information to solve speaker diarization. Moreover, there is an explicit dependency between speaker diarization and the subtasks. Therefore, we propose the subtask-ﬁrst speaker diarization model that executes the subtask and speaker diarization successively, similar to the coarse-to-ﬁne approach on computer vision [20].
To take the order between tasks into account, we employ a conditional parallel mapping [21] that models the relevance between multiple output sequences explicitly via the probabilistic chain rule. We model speaker diarization conditioned on a subtask via conditional parallel mapping as shown in

SAD/OD

Diarization

SAD/OD

Diarization

Inputs
(a) Traditional approach

Inputs
(b) Proposed approach

Fig. 1. Multitask learning approaches.

Fig. 1, where both tasks are optimized simultaneously. Our proposed method imitates the process of speaker diarization, which allows the model to learn effective dependency between tasks to improve the performance of speaker diarization.
In experiments, we perform three instances of our proposed approach: SAD-ﬁrst SC-EEND, OD-ﬁrst SC-EEND, and SAD-OD-ﬁrst SC-EEND. SAD-ﬁrst SC-EEND and ODﬁrst SC-EEND utilize SAD and OD as subtasks, respectively. SAD-OD-ﬁrst SC-EEND solves each task one by one in the order of SAD, OD, and speaker diarization, conditioned on the previous task. It is an important property that multiple tasks can be combined. The experimental results on CALLHOME [22] and simulated mixture datasets reveal that our proposed method achieves improvement over the conventional SC-EEND method.
The rest of this paper is organized as follows: In Section 2, related works is described. In Section 3, our proposed method is described. In Section 4, the experimental data are evaluated, and the ﬁnal section is devoted to our conclusions.
2. RELATED WORK
Speaker diarization has attracted attention because it can be used to boost the performance of ASR [23]. Motivated by the CHiME Challenges [24, 25] and the DIHARD Challenges [26, 27], several researchers have worked on developing more advanced speaker diarization system. Lin et al. proposed a long short-term memory (LSTM)-based similarity measurement for the clustering-based speaker diarization. Moreover, speech activity estimation based on neural networks has been proposed [28, 29] that directly produce the speech activity form the acoustic feature. Kinoshita et al. proposed all-neural model that jointly solves speaker diarization, source separation, and source counting and demonstrated the performance on real meeting scenarios. On the other hand, the EEND [15] has acquired signiﬁcant interest as it can handle overlapping speech. Various extensions of EEND had been proposed. In [16, 17], the authors show experimental analyses and proposals for increasing the number of speakers. In [30], online speaker diarization based on EEND has been investigated using a speaker-tracing buffer. In this paper, we focus on SC-EEND [16] because it is highly expandable.
Under the scope of our research, multitask learning in-

tends to leverage the useful information contained in multiple tasks to improve the generalization performance of those tasks. Especially, there are some multitask learning approaches in which explicitly leverage the hierarchical relationship between tasks [31, 32]. Sanh et al. [33] proposed a hierarchical multitask learning model focused on semantic tasks, which describes information ﬂow from the lower level tasks to more complex tasks in the single neural network. Li et al. [34] proposed a joint multitask learning framework that leverages the correlation between intent prediction and slot ﬁlling. In the ﬁeld of ASR, some recognized symbols have been used for multitask learning [35, 36]. Kubo et al. [36] proposed a joint phoneme-grapheme model that simultaneously converts a speech signal to phonemes and graphemes. These methods consider an internal representation shared between tasks; however, the task-dependency cannot be considered. In this paper, we investigate how to incorporate the relationship between the target task and its subtask into the model via the conditional chain mapping [21].
3. PROPOSED METHOD
In this section, we describe subtask-ﬁrst SC-EEND as an extension of SC-EEND [16] shown in Fig. 2(a). We formulate speaker diarization as a probabilistic model and then introduce the multitask mechanism using the probabilistic chain rule. During training, our proposed method optimizes not only the diarization loss but the subtask loss by conditioning the speaker diarization on the subtask. In this work, we employ SAD and OD as the subtasks.
3.1. Subtask-ﬁrst SC-EEND
Given a T -length time sequence of F -dimensional acoustic features as a matrix X ∈ RF ×T and a set of speech activities {ys | s ∈ {1, . . . , S}} for speaker index s and the number of speakers S, speaker diarization is formulated as follows:
yˆ1, . . . , yˆS = arg max P (y1, . . . , yS | X), (1)
y1 ,...,yS
where ys ∈ {0, 1}1×T is a vector representing a time sequence of speech activity for speaker index s. Eq. (1) means that the most likely speaker activity results are estimated from the joint probability of P (y1, . . . , yS | X). In this paper, we extend Eq. (1) by introducing K subtasks and formulate a multitask problem of speaker diarization and the subtasks as
yˆ1, . . . , yˆS, uˆ1, . . . , uˆK = arg max P (y1, . . . , yS, u1, . . . , uK | X), (2)
y1,...,yS ,u1,...,uK
where uk = {0, 1}1×T denotes an output for k-th subtask. The joint probability in Eq. (2) can be transformed by using

Activity

𝒚෥1

𝒚෥2

𝒚෥𝑆

Activity

𝒖෥1

𝒖෥𝐾

𝒚෥1

𝒚෥𝑆

Output

𝒛1
Diarization 𝐇1 𝐇1 LSTM

𝒛2

𝒛𝑆

Diarization 𝐇2 𝐇2 LSTM

Diarization

𝐇𝑆−1

𝐇𝑆 LSTM

Output

𝒗1

𝒗𝐾

𝒛1

𝒛𝑆

1st subtask 𝐇1
𝐇1 LSTM

Kth subtask

𝐇𝐾

𝐇𝐾−1

𝐇𝐾

LSTM

Diarization

Diarization

𝐇𝐾+1 LSTM

𝐇𝐾+𝑆 𝐇𝐾+1 𝐇𝐾+𝑆−1
LSTM

E
Transformer Encoder

E
Transformer Encoder

Acoustic

X

feature

(a) SC-EEND

Acoustic

X

feature

(b) Subtask-ﬁrst SC-EEND

Fig. 2. Overview of the conventional SC-EEND and proposed subtask-ﬁrst SC-EEND.

the probabilistic chain rule as follows:

P (y1, . . . , yS, u1, . . . , uK | X)

(3)

K

= P (uk | u1, . . . , uk−1, X)

k=1

subtask distribution

S

× P (ys | y1, . . . , ys−1, u1, . . . , uK , X) (4)

s=1

speaker activity distribution

Thus, we can factorize the joint probability by considering the dependency of the subtask and speaker activities. The following explanations provide an actual form of subtask distribution P (uk | u1, . . . , uk−1, X) and speaker activity distribution P (ys | y1, . . . , ys−1, u1, . . . , uK , X), respectively.
Note that the order of y1, . . . , yS, u1, . . . , uK is not unique, and it can be reordered. This paper pre-deﬁne the part of the order (ﬁrst subtask followed by speech activities) to realize subtask-ﬁrst speaker diarization.
In the proposed method, we model both subtask and speaker activity probability distribution in Eq. (4) as stateful neural network functions. Each subtask and speaker activity distribution is represented as follows:

vk = NN(Sku)b(X, u˜k−1),

(5)

zs = NNDiar(X, y˜s−1),

(6)

where NN(Sku)b(·, ·) is a neural network that outputs a probability vk ∈ (0, 1)1×T of the k-th subtask given an input X and the estimation of (k − 1)-th subtask u˜k−1. NNDiar(·, ·) is a
speaker-wise conditional neural network that outputs a probability zs ∈ (0, 1)1×T given X and the estimated (s − 1)-th speaker’s speech activity y˜s−1. To calculate the estimation of k-th subtask activity u˜k from the neural network output vk := [vk,t]Tt=1, we simply threshold each element in vk as

u˜k = [I(vk,t > 0.5)]Tt=1 ,

(7)

where I(cond ) is an indicator function that takes 1 if the con-
dition cond is true and 0 otherwise. The s-th speaker’s speech activity y˜s is estimated from zs := [zs,t]Tt=1 in the same manner as

y˜s = [I(zs,t > 0.5)]Tt=1 .

(8)

Note that the condition for the ﬁrst subtask network u˜0 is a zero matrix 0(1×T ) and that used to extract the ﬁrst speaker is the estimation of the last, i.e. K-th, subtask: y˜0 := u˜K .
In this paper, we adopt SAD and OD as subtasks. The
reference labels for SAD and OD are uniquely determined by those of diarization ys∗ := [ys∗,t]Tt=1 ∈ {0, 1}1×T as follows:

 max(y∗

, . . . , y∗

)T

 u∗ :=

1,t

S,t t=1

T

k
I

S s=1

ys∗,t

>

1

t=1

(if k-th subtask is SAD) .
(if k-th subtask is OD)
(9)

Therefore, we do not need additional labeling to carry out the proposed multitask learning approach. The detailed implementation of the proposed model is described in the following section.

3.2. Model design
Figure 2(b) shows an overview of the proposed method. In order to model the task relationship, we employ a conditional chain mapping [21] that explicitly models the relevance between multiple output sequences with the probabilistic chain rule. In [21], its effectiveness has been proven on speech separation and multi-speaker speech recognition where each of the output sequences corresponds to the speech or the text of each individual speaker. In this work, we regard the SAD/OD activities and speech activities of each speaker as an additional sequence along the subtask k and speaker s direction.

Our model consists of three parts: shared encoder, conditional
chain module, and task-speciﬁc decoder.
For the encoder part, similar to SC-EEND [16], we use
four stacked Transformer encoder blocks [37], each of which
consists of a multi-head self-attention layer, a position-wise
feed-forward layer, and residual connections. In this step, the input sequence of acoustic features X ∈ RF ×T is mapped to a sequence of D-dimensional embeddings E ∈ RD×T . This encoder is shared among NN(Sku)b(·, ·) and NNDiar(·, ·) in Eqs. (5) and (6).
For the conditional chain module, similar to [21], we use a uni-directional LSTM, whose hidden representation at l-th step Hl ∈ RD×T is calculated one by one as

Hl =
LSTM([E, f (u˜l−1)], Hl−1) LSTM([E, f (y˜l−K−1)], Hl−1)

(1 ≤ l ≤ K) ,
(K + 1 ≤ l ≤ K + S) (10)

where f (·) is a linear projection that maps a scalar to a D-dimensional vector for each row of the input vector. LSTM(·, ·) maps a 2D-dimensional vector to a Ddimensional vector while keeping a D-dimensional memory cell for each column of the input matrix. The initial hidden representation H0 is a zero matrix 0(D×T ).
For the decoder part, the output probabilities vk and zs in Eqs. (5) and (6) are computed as follows:

vk = σ(gS(ku)b(Hk)),

(11)

zs = σ(gDiar(Hs+K )),

(12)

where gS(ku)b(·) and gDiar(·) are linear projections for the k-th subtask and speaker diarization, respectively. These functions map a D-dimensional vector to a scalar for each column of the input matrix. σ(·) is a sigmoid activation function that
produces an output probability.
Our proposed model is optimized minimizing the losses
for the subtask and speaker diarization. The subtask loss between the neural network output vk and reference label u∗k determined in Eq. (9) is computed as follows:

1K

LSub =

BCE(vk, u∗k),

(13)

T

k=1

where BCE(·, ·) is an element-wise binary cross-entropy function followed by the summation of all elements. For speaker diarization, the loss between the neural network output zs and reference label ys∗ is calculated as

1

S

LPIT =

min

BCE(zs, yφ∗ ), (14)

ST φ∈perm(S)

s

s=1

where perm(S) is a set of all possible permutations of a sequence (1, . . . , S), and yφ∗s indicates the s-th reference label

Table 1. Statistics of training/adaptation/test sets.

Traning sets Simulated-2spk Simulated-vspk Adaptation sets CALLHOME-2spk CALLHOME-vspk Test sets CALLHOME-2spk CALLHOME-vspk

Num. spk
2 1-4
2 2-7
2 2-6

Num. of mixtures
100,000 100,000
155 249
148 250

Avg. dur. (sec)
88.6 130.0
74.0 125.8
72.1 123.2

Overlap ratio (%)
34.1 29.7
14.0 17.0
13.0 16.7

after the permutation φ. This loss is referred as permutationinvariant training (PIT) loss [38]. Finally, the loss of our proposed model is written as: L = LSub+LPIT. During training, we use the teacher-forcing (TF) technique [39] to boost the performance by exploiting the ground-truth labels, i.e., u˜k−1 and y˜s−1 in Eqs. (5) and (6) are replaced by u∗k−1 and ys∗−1, respectively. However, the order of the speakers cannot be determined in advance because it is determined during training. To alleviate this problem, we examine the two-stage PIT loss computation strategy [16]. This technique ﬁrstly estimates speech activities of all speakers. Then, we obtain an optimal permutation by calculating the PIT loss using Eq. (14). In the next step, network outputs are produced again using groundtruth labels associated with the optimal permutation. We also use a stop sequence condition to handle variable number of speakers as in the original SC-EEND paper [16].
4. EXPERIMENTAL RESULTS
4.1. Conditions
The proposed method was evaluated for both two-speaker and variable-speaker audio mixtures. We prepared a simulated training set based on [15]. We also prepared real adaptation/test sets from CALLHOME [22]. The statistics of the datasets are listed in Table 1. For the CALLHOME2spk and CALLHOME-vspk sets, we employed identical sets as the ones given in Kaldi CALLHOME diarization v2 recipe1, to ensure a fair comparison with the x-vector clustering method [40]. The recipe uses AHC with probabilistic linear discriminant analysis (PLDA) scoring scheme. In this case, time-delay neural network-based speech activity detection (TDNNSAD) was used. The number of clusters was ﬁxed to two for the two-speaker experiments. On the other hand, to estimate the variable number of speakers the PLDA scores were used.
We also compared the proposed method with two conventional EEND-based systems: SA-EEND system and SCEEND system. Additionally, we also evaluated the traditional multitask learning method to verify the effectiveness of the
1https://github.com/kaldi-asr/kaldi/tree/master/egs/callhome diarization

Table 2. Detailed DERs (%) evaluated on CALLHOME2spk. DER is composed of Misses (MI), False alarms (FA), and Confusion errors (CF). The SD errors are composed of Misses (MI) and False alarms (FA) errors.

Method Clustering-based i-vector x-vector EEND-based SA-EEND SC-EEND SC-EEND w/ subtask SAD-multitask SAD-ﬁrst OD-multitask OD-ﬁrst SAD-OD-multitask SAD-OD-ﬁrst

DER
12.10 11.53
10.32 9.39
9.20 8.81 9.16 9.09 9.06 8.53

DER breakdown MI FA CF
7.74 0.54 3.82 7.74 0.54 3.25
5.66 3.25 1.40 4.96 2.73 1.70
4.89 2.61 1.70 4.11 2.96 1.74 4.59 2.52 2.05 5.25 1.86 1.98 4.90 2.35 1.80 4.22 2.33 1.98

SAD MI FA
1.4 0.5 1.4 0.5
3.0 0.5 2.2 0.4
2.2 0.4 1.4 0.8 2.2 0.5 2.3 0.4 2.3 0.5 1.6 0.7

proposed conditional connections. A multitask model (Multitask SC-EEND) has a task-speciﬁc LSTM without the conditional connection between two tasks, unlike the subtask-ﬁrst SC-EEND model as shown in Fig. 1 (a). Moreover, we investigated an application of our proposed method on a three-task scenario where SAD and OD and speaker diarization are solved in this order. For the EEND-based systems, including the proposed system, the input features were 23-dimensional log-Mel-ﬁlterbanks with a 25 ms frame length and 10 ms frame shift. For the two-speaker experiments, each feature was concatenated with those from the previous seven frames and subsequent seven frames. After subsampling the concatenated features by a factor of ten. We used four encoder blocks with 256 attention units containing four heads. For variable number of speaker experiments, we used a concatenation length of 29 and a subsampling ratio of 20 which are twice larger than that of two-speaker experiments. We used four encoder blocks and with 384 attention units containing six heads.
We used diarization error rate (DER) as evaluation metric. A 250 ms collar was employed at the start and end of each segment. Note that we included errors in overlapped segments and SAD-related errors for the DER calculation, whereas most works in literature did not evaluate such errors.

4.2. Results and discussion
4.2.1. Experiments on ﬁxed two-speaker models
First, we conﬁrmed the behavior of the model trained on two-speaker mixtures as shown in Table 2. As observed, DERs on EEND-based systems outperformed the conventional clustering-based methods. The main reason is that EEND can handle overlapping speech.

Table 3. DERs (%) on CALLHOME-vspk.

Method Clustering-based x-vector EEND-based SC-EEND SAD-ﬁrst SC-EEND OD-ﬁrst SC-EEND SAD-OD-ﬁrst SC-EEND

DER
19.01
15.57 15.36 16.37 15.32

Second, we observed the performance of the models using SAD as the subtask. SAD-ﬁrst SC-EEND achieved a 6.2% relative improvement over SC-EEND. Compared with the traditional multitask model, we observed a 4.2% relative improvement. Another important observation is that our proposed method signiﬁcantly reduced the MI errors in SAD errors compared with traditional multitask learning. Surprisingly, our models achieved comparable performance with clustering-based methods that have a SAD module trained separately. This result indicates that our proposed method can improve the subtask’s performance and can leverage it to the subsequent task effectively via the conditional mechanism.
Next, we discuss the performance of models using OD as the subtask. The OD-ﬁrst SC-EEND outperformed both SCEEND and traditional multitask models, and showed slightly worse performance than SAD-ﬁrst SC-EEND. On the other hand, the FA errors in DER breakdown were signiﬁcantly reduced. Considering the result, we hypothesized that the overlap information helped the model prevent overproducing the overlap speech activity in single-speaker segments.
Furthermore, we describe the results on the multiple subtask scenario with SAD and OD. As shown in the table, SADOD-ﬁrst SC-EEND achieved the lowest DER among the evaluated methods. By utilizing two subtasks, SAD-OD-ﬁrst SCEEND showed 3.18% and 6.16% relative DER improvements over SAD-ﬁrst and OD-ﬁrst approaches, respectively. Similar to SAD-ﬁrst SC-EEND, we observed a signiﬁcant reduction in SAD errors. The results also showed that FA errors in DER breakdown were signiﬁcantly reduced compared with SADﬁrst SC-EEND owing to the conditioning on OD. These results indicate that conditioning speaker diarization on the subtasks contributes the signiﬁcant performance improvement. Note that an increase of the number of parameters is less than 0.01% for the SAD-OD-ﬁrst model associated with Eq. (11) whereas about 17% for the multitask model. This means that our proposed method is an efﬁcient strategy to model the relationship between tasks.

4.2.2. Experiments on variable number of speakers
DERs on variable number of speakers for CALLHOME test set are shown in Table 3. For the adaptation of SAD-ﬁrst and SAD-OD-ﬁrst models, we dropped the SAD subtask losses at

randomly selected frames with a ratio of 0.7 and multiplied the losses by 0.1, to prevent overﬁtting. For the inference using the SAD-ﬁrst model, we used the outputs of the SAD subtask network to determine non-speech frames regardless of the diarization outputs. According to the table, SAD-ﬁrst SCEEND achieved 15.36%, which corresponds to 19.2% and 1.35% relative DER improvements over the conventional xvector clustering method and SC-EEND. Moreover, SADOD-ﬁrst SC-EEND reached 15.32% DER which is the best performance among the evaluated methods. These results indicate that our proposed SAD-ﬁrst approach is also effective in a variable-speaker setting. However, OD-ﬁrst SC-EEND did not outperform the conventional SC-EEND although it outperformed the conventional x-vector clustering method. It is assumed that a more careful training strategy is needed such as the scheduled learning because OD is a harder task than SAD. We will investigate a more elaborated strategy to tacke this problem in the future.
Table 4 shows the detailed DER results of the bestproposed system result (SAD-OD-ﬁrst SC-EEND) and the conventional SC-EEND in CALLHOME-vspk (Table 3) for each number of speakers. SAD-OD-ﬁrst SC-EEND is better than the conventional SC-EEND in most cases except for the four-speaker case. This result indicates that our proposed approach is robust to the increase in the number of speakers.
We also analyze the accuracy of speaker counting. The results are shown in Table 5. The proposed method achieved better speaker counting accuracy than the x-vector+AHC method; however, it was still difﬁcult to handle more than four speakers. One of the reasons is that the CALLHOME dataset consists of an imbalance number of speakers. Therefore, it is needed to solve this problem using several training strategies such as generating the simulated data and applying the loss weighting. Moreover, the conventional SC-EEND achieved slightly higher speaker counting accuracy than SAD-OD-ﬁrst SC-EEND. This indicates that SAD-OD-ﬁrst SC-EEND focuses to solve diarization accurately for the small number of speakers, as shown in Table 4.
Finally, we evaluated our proposed method with comparison to other systems as shown in Table 6. In this comparison, we only evaluated single-speaker regions. For this purpose, we used oracle SAD and OD information as the conditions, and ﬁltered out non-speech frames of the estimated diarization result using the oracle SAD information. Although our proposed method could not achieve state-of-the-art performance, it outperformed the system of Zhang et al. [13]. Compared to McCree’s system [41], our proposed method has an advantage that the system can be constructed as a single neural network without a complex implementation. Our proposed method achieved competitive performance with other systems which also suggests that it can use external subtask information via the subtask-ﬁrst model.

Table 4. Detailed DERs (%) associated with each number of speaker on CALLHOME-vspk.

Num. of speakers

Model

23 4 5 6

SC-EEND

9.0 14.4 19.1 34.6 39.5

SAD-OD-ﬁrst SC-EEND 8.0 13.5 23.1 30.0 35.2

Table 5. Speaker counting results on variable-speaker CALLHOME. SAD-OD-ﬁrst SC-EEND-based and SC-EENDbased models were trained with PIT+TF.

(a) x-vector+AHC (Acc: 54.6%)

(b) Proposed (Acc: 75.6%)

Reference

Estimated 2 3 456 2 84 62 2 0 0 3 18 51 5 0 0 4 2 12 6 0 0 5 0 4 100 6 0 1 200

Estimated 2 3 456 2 129 19 0 0 0 3 15 54 5 0 0 4 2 12 6 0 0 5 0 2 300 6 0 1 200

(c) SC-EEND (Acc: 77.6%)
Estimated 2 3 456 2 129 19 0 0 0 3 9 60 5 0 0 4 1 14 5 0 0 5 0 1 400 6 0 1 200

5. CONCLUSION
In this paper, we proposed an end-to-end speaker diarization conditioned on speciﬁc subtasks. Our proposed model performs not only speaker diarization but also SAD/OD based on the probabilistic chain rule. In our experiments, we conﬁrmed that our proposed subtask-ﬁrst SC-EEND improves the DER on the two-speaker CALLHOME dataset compared with EEND-based systems with the traditional multitask learning. Furthermore, our proposed SAD-ﬁrst approaches showed the robustness to increasing the number of speakers.
In the future, we will investigate the combination with other tasks. Moreover, we will explore the variable number of speaker and other difﬁcult scenarios.

Table 6. DERs (%) on CALLHOME-vspk with oracle SAD. Overlap segments were excluded from the DER calculation. Note that the evaluation set used in the proposed method was different from other systems. We used a random subset of CALLHOME, while other systems used the whole CALLHOME evaluation set.

Method

DER

McCree et al. [41]

7.1

SAD-OD-ﬁrst SC-EEND 7.4

Zhang et al. [13]

7.6

6. REFERENCES
[1] Sue E Tranter and Douglas A Reynolds, “An overview of automatic speaker diarization systems,” IEEE Trans. on ASLP, vol. 14, no. 5, pp. 1557–1565, 2006.
[2] Xavier Anguera Miro´, Simon Bozonnet, Nicholas W. D. Evans, Corinne Fredouille, Gerald Friedland, and Oriol Vinyals, “Speaker diarization: A review of recent research,” IEEE Trans. on ASLP, vol. 20, no. 2, pp. 356– 370, 2012.
[3] Naoyuki Kanda, Christoph Bo¨ddeker, Jens Heitkaemper, Yusuke Fujita, Shota Horiguchi, Kenji Nagamatsu, and Reinhold Haeb-Umbach, “Guided source separation meets a strong ASR backend: Hitachi/Paderborn University joint investigation for dinner party ASR,” in Proc. ISCA Interspeech, 2019, pp. 1248–1252.
[4] Catalin Zorila, Christoph Bo¨ddeker, Rama Doddipatla, and Reinhold Haeb-Umbach, “An investigation into the effectiveness of enhancement in ASR training and test for CHiME-5 dinner party transcription,” in Proc. IEEE Automatic Speech Recognition and Understanding (ASRU), 2019, pp. 47–53.
[5] David Snyder, Daniel Garcia-Romero, Gregory Sell, Daniel Povey, and Sanjeev Khudanpur, “X-vectors: Robust DNN embeddings for speaker recognition,” in Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2018, pp. 5329–5333.
[6] Li Wan, Quan Wang, Alan Papir, and Ignacio LopezMoreno, “Generalized end-to-end loss for speaker veriﬁcation,” in Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2018, pp. 4879–4883.
[7] Gregory Sell and Daniel Garcia-Romero, “Speaker diarization with PLDA i-vector scoring and unsupervised calibration,” in Proc. IEEE Spoken Language Technology Workshop (SLT), 2014, pp. 413–417.
[8] Dimitrios Dimitriadis and Petr Fousek, “Developing online speaker diarization system,” in Proc. ISCA Interspeech, 2017, pp. 2739–2743.
[9] Quan Wang, Carlton Downey, Li Wan, Philip Andrew Mansﬁeld, and Ignacio Lopez-Moreno, “Speaker diarization with LSTM,” in Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2018, pp. 5239–5243.
[10] Qingjian Lin, Ruiqing Yin, Ming Li, Herve´ Bredin, and Claude Barras, “LSTM based similarity measurement with spectral clustering for speaker diarization,” in Proc. ISCA Interspeech, 2019, pp. 366–370.

[11] Brendan J J. Frey and Delbert Dueck, “Clustering by passing messages between data points,” Science, vol. 315, no. 5814, pp. 972–976, 2007.
[12] Qiujia Li, Florian L Kreyssig, Chao Zhang, and Philip C Woodland, “Discriminative neural clustering for speaker diarisation,” arXiv preprint arXiv:1910.09703, 2019.
[13] Aonan Zhang, Quan Wang, Zhenyao Zhu, John W. Paisley, and Chong Wang, “Fully supervised speaker diarization,” in Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2019, pp. 6301–6305.
[14] Yusuke Fujita, Naoyuki Kanda, Shota Horiguchi, Kenji Nagamatsu, and Shinji Watanabe, “End-to-end neural speaker diarization with permutation-free objectives,” in Proc. ISCA Interspeech, 2019, pp. 4300–4304.
[15] Yusuke Fujita, Naoyuki Kanda, Shota Horiguchi, Yawen Xue, Kenji Nagamatsu, and Shinji Watanabe, “End-toend neural speaker diarization with self-attention,” in Proc. IEEE Automatic Speech Recognition and Understanding (ASRU), 2019, pp. 296–303.
[16] Yusuke Fujita, Shinji Watanabe, Shota Horiguchi, Yawen Xue, Jing Shi, and Kenji Nagamatsu, “Neural speaker diarization with speaker-wise chain rule,” arXiv preprint arXiv:2006.01796, 2020.
[17] Shota Horiguchi, Yusuke Fujita, Shinji Watanabe, Yawen Xue, and Kenji Nagamatsu, “End-to-end speaker diarization for an unknown number of speakers with encoder-decoder based attractors,” Proc. ISCA Interspeech, pp. 269–273, 2020.
[18] Rich Caruana, “Multitask learning,” Machine Learning, vol. 28, pp. 41–75, 1997.
[19] Yu Zhang and Qiang Yang, “An overview of multi-task learning,” National Science Review, vol. 5, no. 1, pp. 30–43, 2018.
[20] Moshe Bar, “A cortical mechanism for triggering topdown facilitation in visual object recognition,” Journal of Cognitive Neuroscience, vol. 15, pp. 600–609, 2003.
[21] Jing Shi, Xuankai Chang, Pengcheng Guo, Shinji Watanabe, Yusuke Fujita, Jiaming Xu, Bo Xu, and Lei Xie, “Sequence to multi-sequence learning via conditional chain mapping for mixture signals,” arXiv preprint arXiv:2006.14150, 2020.
[22] “2000 NIST speaker recognition evaluation,” https://catalog.ldc.upenn.edu/LDC2001S97.

[23] Naoyuki Kanda, Shota Horiguchi, Ryoichi Takashima, Yusuke Fujita, Kenji Nagamatsu, and Shinji Watanabe, “Auxiliary interference speaker loss for target-speaker speech recognition,” in Proc. ISCA Interspeech, 2019, pp. 236–240.
[24] Jon Barker, Shinji Watanabe, Emmanuel Vincent, and Jan Trmal, “The ﬁfth ‘CHiME’ speech separation and recognition challenge: Dataset, task and baselines,” in Proc. ISCA Interspeech, 2018, pp. 1561–1565.
[25] Shinji Watanabe, Michael Mandel, Jon Barker, and Emmanuel Vincent, “CHiME-6 challenge: Tackling multispeaker speech recognition for unsegmented recordings,” arXiv preprint arXiv:2004.09249, 2020.
[26] Neville Ryant, Kenneth Church, Christopher Cieri, Alejandrina Cristia, Jun Du, Sriram Ganapathy, and Mark Liberman, “First DIHARD challenge evaluation plan,” 2018, tech. Rep., 2018.
[27] Neville Ryant, Kenneth Church, Christopher Cieri, Alejandrina Cristia, Jun Du, Sriram Ganapathy, and Mark Liberman, “The second DIHARD diarization challenge: Dataset, task, and baselines,” in Proc. ISCA Interspeech, 2019, pp. 978–982.
[28] Shaojin Ding, Quan Wang, Shuo-yiin Chang, Li Wan, and Ignacio Lopez Moreno, “Personal VAD: Speakerconditioned voice activity detection,” arXiv preprint arXiv:1908.04284, 2019.
[29] Ivan Medennikov, Maxim Korenevsky, Tatiana Prisyach, Yuri Khokhlov, Mariya Korenevskaya, Ivan Sorokin, Tatiana Timofeeva, Anton Mitrofanov, Andrei Andrusenko, Ivan Podluzhny, et al., “Target-speaker voice activity detection: a novel approach for multispeaker diarization in a dinner party scenario,” in Proc. ISCA Interspeech, 2020, (to appear).
[30] Yawen Xue, Shota Horiguchi, Yusuke Fujita, Shinji Watanabe, and Kenji Nagamatsu, “Online end-to-end neural diarization with speaker-tracing buffer,” arXiv preprint arXiv:2006.02616, 2020.
[31] Yu Zhang, Ying Wei, and Qiang Yang, “Learning to multitask,” in Proc. NeurIPS, 2018, pp. 5776–5787.
[32] Jianping Fan, Tianyi Zhao, Zhenzhong Kuang, Yu Zheng, Ji Zhang, Jun Yu, and Jinye Peng, “HDMTL: Hierarchical deep multi-task learning for large-scale visual recognition,” IEEE Trans. Image Processing, vol. 26, no. 4, pp. 1923–1938, 2017.
[33] Victor Sanh, Thomas Wolf, and Sebastian Ruder, “A hierarchical multi-task approach for learning embeddings from semantic tasks,” in Proc. Association for the Advancement of Artiﬁcial Intelligence (AAAI), 2019, pp. 6949–6956.

[34] Changliang Li, Cunliang Kong, and Yan Zhao, “A joint multi-task learning framework for spoken language understanding,” in Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2018, pp. 6054–6058.
[35] Dongpeng Chen, Brian Mak, Cheung-Chi Leung, and Sunil Sivadas, “Joint acoustic modeling of triphones and trigraphemes by multi-task learning deep neural networks for low-resource speech recognition,” in Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2014, pp. 5592–5596.
[36] Yotaro Kubo and Michiel Bacchiani, “Joint phonemegrapheme model for end-to-end speech recognition,” in Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2020, pp. 6119–6123.
[37] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin, “Attention is all you need,” in Proc. NIPS, 2017, pp. 5998–6008.
[38] Dong Yu, Morten Kolbæk, Zheng-Hua Tan, and Jesper Jensen, “Permutation invariant training of deep models for speaker-independent multi-talker speech separation,” in Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2017, pp. 241–245.
[39] R. J. Williams and D. Zipser, “A learning algorithm for continually running fully recurrent networks,” Neural Computation, vol. 1, no. 2, pp. 270–280, 1989.
[40] Gregory Sell, David Snyder, Alan McCree, Daniel Garcia-Romero, Jesu´s Villalba, Matthew Maciejewski, Vimal Manohar, Najim Dehak, Daniel Povey, Shinji Watanabe, and Sanjeev Khudanpur, “Diarization is hard: Some experiences and lessons learned for the JHU team in the inaugural DIHARD challenge,” in Proc. ISCA Interspeech, 2018, pp. 2808–2812.
[41] Alan McCree, Gregory Sell, and Daniel GarciaRomero, “Speaker diarization using leave-one-out gaussian PLDA clustering of DNN embeddings,” in Proc. ISCA Interspeech, 2019, pp. 381–385.

