Underspecification in Machine Learning

arXiv:2011.03395v2 [cs.LG] 24 Nov 2020

Underspeciﬁcation Presents Challenges for Credibility in Modern Machine Learning

Alexander D’Amour∗ Katherine Heller∗ Dan Moldovan∗ Ben Adlam Babak Alipanahi Alex Beutel Christina Chen Jonathan Deaton Jacob Eisenstein Matthew D. Hoﬀman Farhad Hormozdiari Neil Houlsby Shaobo Hou Ghassen Jerfel Alan Karthikesalingam Mario Lucic Yian Ma Cory McLean Diana Mincu Akinori Mitani Andrea Montanari Zachary Nado Vivek Natarajan Christopher Nielson† Thomas F. Osborne† Rajiv Raman Kim Ramasamy Rory Sayres Jessica Schrouﬀ Martin Seneviratne Shannon Sequeira Harini Suresh Victor Veitch Max Vladymyrov Xuezhi Wang Kellie Webster Steve Yadlowsky Taedong Yun Xiaohua Zhai D. Sculley

alexdamour@google.com kheller@google.com mdan@google.com adlam@google.com babaka@google.com
alexbeutel@google.com christinium@google.com
jdeaton@google.com jeisenstein@google.com mhoffman@google.com
fhormoz@google.com neilhoulsby@google.com
shaobohou@google.com ghassen@google.com
alankarthi@google.com lucic@google.com yianma@ucsd.edu cym@google.com
dmincu@google.com amitani@google.com montanari@stanford.edu
znado@google.com natviv@google.com christopher.nielson@va.gov thomas.osborne@va.gov
drrrn@snmail.org kim@aravind.org
sayres@google.com schrouff@google.com martsen@google.com
shnnn@google.com hsuresh@mit.edu
victorveitch@google.com mxv@google.com
xuezhiw@google.com websterk@google.com yadlowsky@google.com
tedyun@google.com xzhai@google.com
dsculley@google.com

Editor:

∗. These authors contributed equally to this work. †. This paper represents the views of the authors, and not of the VA.

1

A. D’Amour, K. Heller, D. Moldovan, et al
Abstract
ML models often exhibit unexpectedly poor behavior when they are deployed in real-world domains. We identify underspeciﬁcation as a key reason for these failures. An ML pipeline is underspeciﬁed when it can return many predictors with equivalently strong held-out performance in the training domain. Underspeciﬁcation is common in modern ML pipelines, such as those based on deep learning. Predictors returned by underspeciﬁed pipelines are often treated as equivalent based on their training domain performance, but we show here that such predictors can behave very diﬀerently in deployment domains. This ambiguity can lead to instability and poor model behavior in practice, and is a distinct failure mode from previously identiﬁed issues arising from structural mismatch between training and deployment domains. We show that this problem appears in a wide variety of practical ML pipelines, using examples from computer vision, medical imaging, natural language processing, clinical risk prediction based on electronic health records, and medical genomics. Our results show the need to explicitly account for underspeciﬁcation in modeling pipelines that are intended for real-world deployment in any domain. Keywords: distribution shift, spurious correlation, fairness, identiﬁability, computer vision, natural language processing, medical imaging, electronic health records, genomics
1. Introduction
In many applications of machine learning (ML), a trained model is required to not only predict well in the training domain, but also encode some essential structure of the underlying system. In some domains, such as medical diagnostics, the required structure corresponds to causal phenomena that remain invariant under intervention. In other domains, such as natural language processing, the required structure is determined by the details of the application (e.g., the requirements in question answering, where world knowledge is important, may be diﬀerent from those in translation, where isolating semantic knowledge is desirable). These requirements for encoded structure have practical consequences: they determine whether the model will generalize as expected in deployment scenarios. These requirements often determine whether a predictor is credible, that is, whether it can be trusted in practice.
Unfortunately, standard ML pipelines are poorly set up for satisfying these requirements. Standard ML pipelines are built around a training task that is characterized by a model speciﬁcation, a training dataset, and an independent and identically distributed (iid) evaluation procedure; that is, a procedure that validates a predictor’s expected predictive performance on data drawn from the training distribution. Importantly, the evaluations in this pipeline are agnostic to the particular inductive biases encoded by the trained model. While this paradigm has enabled transformational progress in a number of problem areas, its blind spots are now becoming more salient. In particular, concerns regarding “spurious correlations” and “shortcut learning” in trained models are now widespread (e.g., Geirhos et al., 2020; Arjovsky et al., 2019).
The purpose of this paper is to explore this gap, and how it can arise in practical ML pipelines. A common explanation is simply that, in many situations, there is a fundamental conﬂict between iid performance and desirable behavior in deployment. For example, this occurs when there are diﬀerences in causal structure between training and deployment domains, or when the data collection mechanism imposes a selection bias. In such cases, the iid-optimal predictors must necessarily incorporate spurious associations (Caruana et al., 2015; Arjovsky et al., 2019; Ilyas et al., 2019). This is intuitive: a predictor trained in a setting that is structurally misaligned with the application will reﬂect this mismatch.
However, this is not the whole story. Informally, in this structural-conﬂict view, we would expect that two identically trained predictors would show the same defects in deployment. The observation of this paper is that this structural-conﬂict view does not adequately capture the challenges of deploying ML models in practice. Instead, predictors trained to the same level of iid generalization will often show widely divergent behavior when applied to real-world settings.
2

Underspecification in Machine Learning
We identify the root cause of this behavior as underspeciﬁcation in ML pipelines. In general, the solution to a problem is underspeciﬁed if there are many distinct solutions that solve the problem equivalently. For example, the solution to an underdetermined system of linear equations (i.e., more unknowns than linearly independent equations) is underspeciﬁed, with an equivalence class of solutions given by a linear subspace of the variables. In the context of ML, we say an ML pipeline is underspeciﬁed if there are many distinct ways (e.g., diﬀerent weight conﬁgurations) for the model to achieve equivalent held-out performance on iid data, even if the model speciﬁcation and training data are held constant. Underspeciﬁcation is well-documented in the ML literature, and is a core idea in deep ensembles, double descent, Bayesian deep learning, and loss landscape analysis (Lakshminarayanan et al., 2017; Fort et al., 2019; Belkin et al., 2018; Nakkiran et al., 2020). However, its implications for the gap between iid and application-speciﬁc generalization are neglected.
Here, we make two main claims about the role of underspeciﬁcation in modern machine learning. The ﬁrst claim is that underspeciﬁcation in ML pipelines is a key obstacle to reliably training models that behave as expected in deployment. Speciﬁcally, when a training pipeline must choose between many predictors that yield near-optimal iid performance, if the pipeline is only sensitive to iid performance, it will return an arbitrarily chosen predictor from this class. Thus, even if there exists an iid-optimal predictor that encodes the right structure, we cannot guarantee that such a model will be returned when the pipeline is underspeciﬁed. We demonstrate this issue in several examples that incorporate simple models: one simulated, one theoretical, and one a real empirical example from medical genomics. In these examples, we show how, in practice, underspeciﬁcation manifests as sensitivity to arbitrary choices that keep iid performance ﬁxed, but can have substantial eﬀects on performance in a new domain, such as in model deployment.
The second claim is that underspeciﬁcation is ubiquitous in modern applications of ML, and has substantial practical implications. We support this claim with an empirical study, in which we apply a simple experimental protocol across production-grade deep learning pipelines in computer vision, medical imaging, natural language processing (NLP), and electronic health record (EHR) based prediction. The protocol is designed to detect underspeciﬁcation by showing that a predictor’s performance on stress tests—empirical evaluations that probe the model’s inductive biases on practically relevant dimensions—is sensitive to arbitrary, iid-performance-preserving choices, such as the choice of random seed. A key point is that the stress tests induce variation between predictors’ behavior, not simply a uniform degradation of performance. This variation distinguishes underspeciﬁcation-induced failure from the more familiar case of structural-change induced failure. We ﬁnd evidence of underspeciﬁcation in all applications, with downstream eﬀects on robustness, fairness, and causal grounding.
Together, our ﬁndings indicate that underspeciﬁcation can, and does, degrade the credibility of ML predictors in applications, even in settings where the prediction problem is well-aligned with the goals of an application. The direct implication of our ﬁndings is that substantive real-world behavior of ML predictors can be determined in unpredictable ways by choices that are made for convenience, such as initialization schemes or step size schedules chosen for trainability—even when these choices do not aﬀect iid performance. More broadly, our results suggest a need to explicitly test models for required behaviors in all cases where these requirements are not directly guaranteed by iid evaluations. Finally, these results suggest a need for training and evaluation techniques tailored to address underspeciﬁcation, such as ﬂexible methods to constrain ML pipelines toward the credible inductive biases for each speciﬁc application. Interestingly, our ﬁndings suggest that enforcing these credible inductive biases need not compromise iid performance.
Organization The paper is organized as follows. We present some core concepts and review relevant literature in Section 2. We present a set of examples of underspeciﬁcation in simple, analytically tractable models as a warm-up in Section 3. We then present a set of four deep learning case studies in Sections 5–8. We close with a discussion in Section 9.
3

A. D’Amour, K. Heller, D. Moldovan, et al
Overall, our strategy in this paper is to provide a broad range of examples of underspeciﬁcation in a variety of modeling pipelines. Readers may not ﬁnd it necessary to peruse every example to appreciate our argument, but diﬀerent readers may ﬁnd diﬀerent domains to be more familiar. As such, the paper is organized such that readers can take away most of the argument from understanding one example from Section 3 and one case study from Sections 5–8. However, we believe there is beneﬁt to presenting all of these examples under the single banner of underspeciﬁcation, so we include them all in the main text.
2. Preliminaries and Related Work
2.1 Underspeciﬁcation
We consider a supervised learning setting, where the goal is to obtain a predictor f : X → Y that maps inputs x (e.g., images, text) to labels y. We say a model is speciﬁed by a function class F from which a predictor f (x) will be chosen. An ML pipeline takes in training data D drawn from a training distribution P and produces a trained model, or predictor, f (x) from F. Usually, the pipeline selects f ∈ F by approximately minimizing the predictive risk on the training distribution RP(f ) := E(X,Y )∼P[ (f (X), Y )]. Regardless of the method used to obtain a predictor f , we assume that the pipeline validates that f achieves low expected risk on the training distribution P by evaluating its predictions on an independent and identically distributed test set D , e.g., a hold-out set selected completely at random. This validation translates to a behavioral guarantee, or contract (Jacovi et al., 2020), about the model’s aggregate performance on future data drawn from P.
We say that an ML pipeline is underspeciﬁed if there are many predictors f that a pipeline could return with similar predictive risk. We denote this set of risk-equivalent near-optimal predictors F∗ ⊂ F. However, underspeciﬁcation creates diﬃculties when the predictors in F∗ encode substantially diﬀerent inductive biases that result in diﬀerent generalization behavior on distributions that diﬀer from P. When this is true, even when F∗ contains a predictor with credible inductive biases, a pipeline may return a diﬀerent predictor because it cannot distinguish between them.
The ML literature has studied various notions of underspeciﬁcation before. In the deep learning literature speciﬁcally, much of the discussion has focused on the shape of the loss landscape E(X,Y )∼P[ (f (X), Y )], and of the geometry of non-unique risk minimizers, including discussions of wide or narrow optima (see, e.g. Chaudhari et al., 2019), and connectivity between global modes in the context of model averaging (Izmailov et al., 2018; Fort et al., 2019; Wilson and Izmailov, 2020) and network pruning (Frankle et al., 2020). Underspeciﬁcation also plays a role in recent analyses of overparametrization in theoretical and real deep learning models (Belkin et al., 2018; Mei and Montanari, 2019; Nakkiran et al., 2020). Here, underspeciﬁcation is a direct consequence of having more degrees of freedom than datapoints. Our work here complements these eﬀorts in two ways: ﬁrst, our goal is to understand how underspeciﬁcation relates to inductive biases that could enable generalization beyond the training distribution P; and secondly, the primary object that we study is practical ML pipelines rather than the loss landscape itself. This latter distinction is important for our empirical investigation, where the pipelines that we analyze incorporate a number of standard tricks, such as early stopping, which are ubiquitous in ML as it is applied to real problems, but diﬃcult to fully incorporate into theoretical analysis. However, we note that these questions are clearly connected, and in Section 3, we motivate underspeciﬁcation using a similar approach to this previous literature.
Our treatment of underspeciﬁcation is more closely related to work on “Rashomon sets” (Fisher et al., 2019; Semenova et al., 2019), “predictive multiplicity” (Marx et al., 2019), and methods that seek our risk-equivalent predictors that are “right for the right reasons” (Ross et al., 2017). These lines of work similarly note that a single learning problem speciﬁcation can admit many near-optimal solutions, and that these solutions may have very diﬀerent properties along axes such
4

Underspecification in Machine Learning
as interpretability or fairness. Our work here is complementary: we provide concrete examples of how such equivalence classes manifest empirically in common machine learning practice.
2.2 Shortcuts, Spurious Correlations, and Structural vs Underspeciﬁed Failure Modes
Many explorations of the failures of ML pipelines that optimize for iid generalization focus on cases where there is an explicit tension between iid generalization and encoding credible inductive biases. We call these structural failure modes, because they are often diagnosed as a misalignment between the predictor learned by empirical risk minimization and the causal structure of the desired predictor (Schölkopf, 2019; Arjovsky et al., 2019). In these scenarios, a predictor with credible inductive biases cannot achieve optimal iid generalization in the training distribution, because there are so-called “spurious” features in that are strongly associated with the label in the training data, but are not associated with the label in some practically important settings.
Some well-known examples of this case have been reported in medical applications of ML, where the training inputs often include markers of a doctor’s diagnostic judgment (Oakden-Rayner et al., 2020). For example, Winkler et al. (2019) report on a CNN model used to diagnose skin lesions, which exhibited strong reliance on surgical ink markings around skin lesions that doctors had deemed to be cancerous. Because the judgment that went into the ink markings may have used information not available in the image itself, an iid-optimal predictor would need to incorporate this feature, but these markings would not be expected to be present in deployment, where the predictor would itself be part of the workﬂow for making a diagnostic judgment. In this context, Peters et al. (2016); Heinze-Deml et al. (2018); Arjovsky et al. (2019); Magliacane et al. (2018) propose approaches to overcome this structural bias, often by using data collected in multiple environments to identify causal invariances.
While structural failure modes are important when they arise, they do not cover all cases where predictors trained to minimize predictive risk encode poor inductive biases. In many settings where ML excels, the structural issues identiﬁed above are not present. For example, it’s known in many perception problems that suﬃcient information exists in the relevant features of the input alone to recover the label with high certainty. Instead, we argue that it is often the case that there is simply not enough information in the training distribution to distinguish between these inductive biases and spurious relationships: making the connection to causal reasoning, this underspeciﬁed failure mode corresponds to a lack of positivity, not a structural defect in the learning problem. Geirhos et al. (2020) connects this idea to the notion of “shortcut learning”. They point out that there may be many predictors that generalize well in iid settings, but only some that align with the intended solution tot he prediction problem. In addition, they also note (as we do) that some seemingly arbitrary aspects of ML pipelines, such as the optimization procedure, can make certain inductive biases easier for a pipeline to represent, and note the need for future investigation in this area. We agree with these points, and we oﬀer additional empirical support to this argument. Furthermore, we show that even pipelines that are identical up to their random seed can produce predictors that encode distinct shortcuts, emphasizing the relevance of underspeciﬁcation. We also emphasize that these problems are far-reaching across ML applications.
2.3 Stress Tests and Credibility
Our core claims revolve around how underspeciﬁcation creates ambiguity in the encoded structure of a predictor, which, in turn, aﬀect the predictor’s credibility. In particular, we are interested in behavior that is not tested by iid evaluations, but has observable implications in practically important situations. To this end, we follow the framework presented in Jacovi et al. (2020), and focus on inductive biases that can be expressed in terms of a contract, or an explicit statement of expected predictor behavior, that can be falsiﬁed concretely by stress tests, or evaluations that probe a predictor by observing its outputs on speciﬁcally designed inputs.
5

A. D’Amour, K. Heller, D. Moldovan, et al
Importantly, stress tests probe a broader set of contracts than iid evaluations. Stress tests are becoming a key part of standards of evidence in a number of applied domains, including medicine (Collins et al., 2015; Liu et al., 2020a; Rivera et al., 2020), economics (Mullainathan and Spiess, 2017; Athey, 2017), public policy (Kleinberg et al., 2015), and epidemiology (Hoﬀmann et al., 2019). In many settings where stress tests have been proposed in the ML literature, they have often uncovered cases where models fail to generalize as required for direct real-world application. Our aim is to show that underspeciﬁcation can play a role in these failures.
Here, we review three types of stress tests that we consider in this paper, and make connections to existing literature where they have been applied.
Stratiﬁed Performance Evaluations Stratiﬁed evaluations (i.e., subgroup analyses) test whether a predictor f encodes inductive biases that yield similar performance across diﬀerent strata of a dataset. We choose a particular feature A and stratify a standard test dataset D into strata Da = {(xi, yi) : Ai = a}. A performance metric can then be calculated and compared across diﬀerent values of a.
Stratiﬁed evaluations have been presented in the literature on fairness in machine learning, where examples are stratiﬁed by socially salient characteristics like skin type (Buolamwini and Gebru, 2018); the ML for healthcare literature (Obermeyer et al., 2019; Oakden-Rayner et al., 2020), where examples are stratiﬁed by subpopulations; and the natural language processing and computer vision literatures where examples are stratiﬁed by topic or notions of diﬃculty (Hendrycks et al., 2019; Zellers et al., 2018).
Shifted Performance Evaluations Shifted performance evaluations test whether the average performance of a predictor f generalizes when the test distribution diﬀers in a speciﬁc way from the training distribution. Speciﬁcally, these tests deﬁne a new data distribution P = P from which to draw the test dataset D , then evaluate a performance metric with respect to this shifted dataset.
There are several strategies for generating P , which test diﬀerent properties of f . For example, to test whether f exhibits invariance to a particular transformation T (x) of the input, one can deﬁne P to be the distribution of the variables (T (x), y) when (x, y) are drawn from the training distribution PD (e.g., noising of images in ImageNet-C (Hendrycks and Dietterich, 2019)). One can also deﬁne PD less formally, for example by changing the data scraping protocol used to collect the test dataset (e.g., ObjectNet (Barbu et al., 2019)), or changing the instrument used to collect data.
Shifted performance evaluations form the backbone of empirical evaluations in the literature on robust machine learning and task adaptation (e.g., Hendrycks and Dietterich, 2019; Wang et al., 2019; Djolonga et al., 2020; Taori et al., 2020). Shifted evaluations are also required in some reporting standards, including those for medical applications of AI (Collins et al., 2015; Liu et al., 2020a; Rivera et al., 2020).
Contrastive Evaluations Shifted evaluations that measure aggregate performance can be useful for diagnosing the existence of poor inductive biases, but the aggregation involved can obscure more ﬁne-grained patterns. Contrastive evaluations can support localized analysis of particular inductive biases. Speciﬁcally, contrastive evaluations are performed on the example, rather than distribution level, and check whether a particular modiﬁcation of the input x causes the output of the model to change in unexpected ways. Formally, a contrastive evaluation makes use of a dataset of matched sets C = {zi}|iC=|1, where each matched set zi consists of a base input xi that is modiﬁed by a set of transformations T , zi = (Tj(xi))Tj∈T . In contrastive evaluations, metrics are computed with respect to matched sets, and can include, for example, measures of similarity or ordering among the examples in the matched set. For instance, if it is assumed that each transformation in T should be label-preserving, then a measurement of disagreement within the matched sets can reveal a poor inductive bias.
Contrastive evaluations are common in the ML fairness literature, e.g., to assess counterfactual notions of fairness (Garg et al., 2019; Kusner et al., 2017). They are also increasingly common as
6

Underspecification in Machine Learning

robustness or debugging checks in the natural language processing literature (Ribeiro et al., 2020; Kaushik et al., 2020).

3. Warm-Up: Underspeciﬁcation in Simple Models
To build intuition for how underspeciﬁcation manifests in practice, we demonstrate its consequences in three relatively simple models before moving on to study production-scale deep neural networks. In particular, we examine three underspeciﬁed models in three diﬀerent settings: (1) a simple parametric model for an epidemic in a simulated setting; (2) a shallow random feature model in the theoretical inﬁnitely wide limit; and (3) a linear model in a real-world medical genomics setting, where such models are currently state-of-the-art. In each case, we show that underspeciﬁcation is an obstacle to learning a predictor with the required inductive bias.

3.1 Underspeciﬁcation in a Simple Epidemiological Model
One core task in infectious disease epidemiology is forecasting the trajectory of an epidemic. Dynamical models are often used for this task. Here, we consider a simple simulated setting where the data is generated exactly from this model; thus, unlike a real setting where model misspeciﬁcation is a primary concern, the only challenge here is to recover the true parameters of the generating process, which would enable an accurate forecast. We show that even in this simpliﬁed setting, underspeciﬁcation can derail the forecasting task.
Speciﬁcally, we consider the simple Susceptible-Infected-Recovered (SIR) model that is often used as the basis of epidemic forecasting models in infectious disease epidemiology. This model is speciﬁed in terms of the rates at which the number of susceptible (S), infected (I), and recovered (R) individuals in a population of size N , change over time:

dS

I

= −β

S,

dt

N

dI

I

I

=− +β

S,

dt D

N

dR I =.
dt D

In this model, the parameter β represents the transmission rate of the disease from the infected to susceptible populations, and the parameter D represents the average duration that an infected individual remains infectious.
To simulate the forecasting task, we generate a full trajectory from this model for a full time-course T , but learn the parameters (β, D) from data of observed infections up to some time Tobs < T by minimizing squared-error loss on predicted infections at each timepoint using gradient descent (susceptible and recovered are usually not observed). Importantly, during the early stages of an epidemic, when Tobs is small, the parameters of the model are underspeciﬁed by this training task. This is because, at this stage, the number of susceptible is approximately constant at the total population size (N ), and the number of infections grows approximately exponentially at rate β − 1/D. The data only determine this rate. Thus, there are many pairs of parameter values (β, D) that describe the exponentially growing timeseries of infections equivalently.
However, when used to forecast the trajectory of the epidemic past Tobs, these parameters yield very diﬀerent predictions. In Figure 1(a), we show two predicted trajectories of infections corresponding to two parameter sets (β, D). Despite ﬁtting the observed data identically, these models predict peak infection numbers, for example, that are orders of magnitude apart.
Because the training objective cannot distinguish between parameter sets (β, D) that yield equivalent growth rates β − 1/D, arbitrary choices in the learning process determine which set of observation-equivalent parameters are returned by the learning algorithm. In Figure 1(c), we show that by changing the point D0 at which the parameter D is initialized in the least-squares minimization procedure, we obtain a wide variety of predicted trajectories from the model. In addition, the particular distribution used to draw D0 (Figure 1(b)) has a substantial inﬂuence on the distribution of predicted trajectories.

7

A. D’Amour, K. Heller, D. Moldovan, et al

Number of Infected Number of Infected

20000 15000

D0=28 D0=7 data points

10000

5000

0 0 40 80T1im20e1(d6a0y2s0)0 240 280

Gamma Normal
D0

20000

D0 Gamma

15000

D0 Normal data points

10000

5000

0 0 40 80T1im20e 1(d6a0y2s0) 0 240 280

Figure 1: Underspeciﬁcation in a simple epidemiological model. A training pipeline that only minimizes predictive risk on early stages of the epidemic leaves key parameters underspeciﬁed, making key behaviors of the model sensitive to arbitrary training choices. Because many parameter values are equivalently compatible with ﬁtting data from early in the epidemic, the trajectory returned by a given training run depends on where it was initialized, and diﬀerent initialization distributions result in diﬀerent distributions of predicted trajectories.

In realistic epidemiological models that have been used to inform policy, underspeciﬁcation is dealt with by testing models in forecasting scenarios (i.e., stress testing), and constraining the problem with domain knowledge and external data, for example about viral dynamics in patients (informing D) and contact patterns in the population (informing β) (see, e.g. Flaxman et al., 2020).
3.2 Theoretical Analysis of Underspeciﬁcation in a Random Feature Model
Underspeciﬁcation is also a natural consequence of overparameterization, which is a key property of many modern neural network models: when there are more parameters than datapoints, the learning problem is inherently underspeciﬁed. Much recent work has shown that this underspeciﬁcation has interesting regularizing eﬀects on iid generalization, but there has been little focus on its impact on how models behave on other distributions. Here, we show that we can recover the eﬀect of underspeciﬁcation on out-of-distribution generalization in an asymptotic analysis of a simple random feature model, which is often used as a model system for neural networks in the inﬁnitely wide regime.
We consider for simplicity a regression problem: we are given data {(xi, yi)}i≤n, with xi ∈ Rd vector of covariates and yi ∈ R a response. As a tractable and yet mathematically rich setting, we use the random features model of Neal (1996) and Rahimi and Recht (2008). This is a one-hidden-layer neural network with random ﬁrst layer weights W and learned second layer weights θ. We learn a predictor fW : Rd → R of the form
fW (x) = θTσ(W x).
Here, W ∈ RN×d is a random matrix with rows wi ∈ Rd, 1 ≤ i ≤ N that are not optimized and deﬁne the featurization map x → σ(W x). We take (wi)i≤N to be iid and uniforml√y random with wi 2 = 1. We consider data (xi, yi), where xi are uniformly random with xi 2 = d and a linear target yi = f∗(xi) = β0Txi.
We analyze this model in a setting where both the number of datapoints n and the neurons N both tend toward inﬁnity with a ﬁxed overparameterization ratio N/n. For N/n < 1, we learn the second layer weights using least squares. For N/n ≥ 1 there exists choices of the parameters θ that perfectly interpolate the data fτ (xi) = yi for all i ≤ n. We choose the minimum 2-norm interpolant (which is the model selected by GD when θ is initialized at 0):
minimize θ
subject to fτ (xi) = yi for all i.

8

Underspecification in Machine Learning
We analyze the predictive risk of the predictor fW on two test distributions, P, which matches the training distribution, and P∆, which is perturbed in a speciﬁc way that we describe below. For a given distribution Q, we deﬁne the prediction risk as the mean squared error for the random feature model derived from W and for a test point sampled from Q:
R(W , Q) = E(X,Y )∼Q(Y − θˆ(W )σ(W X))2.
This risk depends implicitly on the training data through θˆ, but we suppress this dependence. Building on the work of Mei and Montanari (2019) we can determine the precise asymptotics of
the risk under certain distribution shifts in the limit n, N, d → ∞ with ﬁxed ratios n/d, N/n. We provide detailed derivations in Appendix E, as well as characterizations of other quantities such as the sensitivity of the prediction function fW to the choice of W .
In this limit, any two independent random choices W1 and W2 induce trained predictors fW1 and fW2 that have indistinguishable in-distribution error R(Wi, P). However, given this value of the risk, the prediction function fW1 (x) and fW2 (x) are nearly as orthogonal as they can be, and this leads to very diﬀerent test errors on certain shifted distributions P∆.
Speciﬁcally, we deﬁne P∆ in terms of an adversarial mean shift. We consider test inputs xtest = x0 + x, where x is an independent sample from the training distribution, but x0 is a constant mean-shift deﬁned with respect to a ﬁxed set of random feature weights W0. We denote this shifted distribution with P∆,W0 . For a given W0, a shift x0 can be chosen such that (1) it has small norm (||x0|| < ∆ ||x||), (2) it leaves the risk of an independently sampled W mostly unchanged (R(W , P∆,W0 ) ≈ R(W , Ptrain)), but (3) it drastically increases the risk of W0 (R(W0, P∆,W0 ) > R(W0, Ptrain)). In Figure 2 we plot the risks R(W , P∆,W0 ) and R(W0, P∆,W0 ) normalized by the iid test risk R(W , Ptrain) as a function of the overparameterization ratio for two diﬀerent data dimensionalities. The upper curves correspond to the risk for the model against which the shift was chosen adversarially, producing a 3-fold increase in risk. Lower curves correspond to the risk for the same distributional shift for the independent model, resulting in very little risk inﬂation.
These results show that any predictor selected by min-norm interpolation is vulnerable to shifts along a certain direction, while many other models with equivalent risk are not vulnerable to the same shift. The particular shift itself depends on a random set of choices made during model training. Here, we argue that similar dynamics are at play in many modern ML pipelines, under distribution shﬁts that reveal practically important model properties.
3.3 Underspeciﬁcation in a Linear Polygenic Risk Score Model
Polygenic risk scores (PRS) in medical genomics leverage patient genetic information (genotype) to predict clinically relevant characteristics (phenotype). Typically, they are linear models built on categorical features that represent genetic variants. PRS have shown great success in some settings (Khera et al., 2018), but face diﬃculties when applied to new patient populations (Martin et al., 2017; Duncan et al., 2019; Berg et al., 2019).
We show that underspeciﬁcation plays a role in this diﬃculty with generalization. Speciﬁcally, we show that there is a non-trivial set of predictors F∗ that have near-optimal performance in the training domain, but transfer very diﬀerently to a new population. Thus, a modeling pipeline based on iid performance alone cannot reliably return a predictor that transfers well.
To construct distinct, near-optimal predictors, we exploit a core ambiguity in PRS, namely, that many genetic variants that are used as features are nearly collinear. This collinearity makes it diﬃcult to distinguish causal and correlated-but-noncausal variants (Slatkin, 2008). A common approach to this problem is to partition variants into clusters of highly-correlated variants and to only include one representative of each cluster in the PRS (e.g., International Schizophrenia Consortium et al., 2009; CARDIoGRAMplusC4D Consortium et al., 2013). Usually, standard heuristics are applied to choose clusters and cluster representatives as a pre-processing step (e.g., “LD clumping”, Purcell et al., 2007).
9

A. D’Amour, K. Heller, D. Moldovan, et al

Rshift/R

4.5

n/d=4

4.0

n/d=6

3.5

3.0

2.5

2.0

1.5

1.0

0

1

2

3

4

5

N/n

Figure 2: Random feature models with identical in-distribution risk show distinct risks
under mean shift. Expected risk (averaging over random features W0, W ) of predictors fW0 , fW under a W0-adversarial mean-shift at diﬀerent levels of overparameterization (N/n) and sample sizeto-parameter ratio (n/d). Upper curves: Normalized risk EW0 R(W0; PW0,∆)/EW R(W ; P) of the adversarially targeted predictor fW0 . Lower curves: Normalized risk EW ,W0 R(W ; PW0,∆)/EW R(W ; P) of a predictor fW deﬁned with independently draw random weights W . Here the input dimension is d = 80, N is the number of neurons, and n the number of samples. We use ReLU activations; the
ground truth is linear with β0 2 = 1. Circles are empirical results obtained by averaging over 50 realizations. Continuous lines correspond to the analytical predictions detailed in the supplement.

Importantly, because of the high correlation of features within clusters, the choice of cluster representative leaves the iid risk of the predictor largely unchanged. Thus, distinct PRS predictors that incorporate diﬀerent cluster representatives can be treated as members of the risk-minimizing set F∗. However, this choice has strong consequences for model generalization.
To demonstrate this eﬀect, we examine how feature selection inﬂuences behavior in a stress test that simulates transfer of PRS across populations. Using data from the UK Biobank (Sudlow et al., 2015), we examine how a PRS predicting a particular continuous phenotype called the intraocular pressure (IOP) transfers from a predominantly British training population to “non-British” test population (see Appendix D for deﬁnitions). We construct an ensemble of 1000 PRS predictors that sample diﬀerent representatives from each feature cluster, including one that applies a standard heuristic from the popular tool PLINK (Purcell et al., 2007).
The three plots on the left side of Figure 3 conﬁrm that each predictor with distinct features attains comparable performance in the training set and iid test set, with the standard heuristic (red dots) slightly outperforming random representative selection. However, on the shifted “non-British” test data, we see far wider variation in performance, and the standard heuristic fares no better than the rest of the ensemble. More generally, performance on the British test set is only weakly associated with performance on the “non-British” set (Spearman ρ = 0.135; 95% CI 0.070-0.20; Figure 3, right).

10

Underspecification in Machine Learning
Figure 3: Underspeciﬁcation in linear models in medical genomics. (Left) Performance of a PRS model using genetic features in the British training set, the British evaluation set, and the “nonBritish” evaluation set, as measured by the normalized mean squared error (MSE divided by the true variance, lower is better). Each dot represents a PRS predictor (using both genomic and demographic features); large red dots are PRS predictors using the “index” variants of the clusters of correlated features selected by PLINK. Gray lines represent the baseline models using only demographic information. (Right) Comparison of model performance (NMSE) in British and “non-British” eval sets, given the same set of genomic features (Spearman ρ = 0.135; 95% CI 0.070-0.20).
Thus, because the model is underspeciﬁed, this PRS training pipeline cannot reliably return a predictor that transfers as required between populations, despite some models in F∗ having acceptable transfer performance. For full details of this experiment and additional background information, see Appendix D.
4. Underspeciﬁcation in Deep Learning Models
Underspeciﬁcation is present in a wide range of modern deep learning pipelines, and poses an obstacle to reliably learning predictors that encode credible inductive biases. We show this empirically in three domains: computer vision (including both basic research and medical imaging), natural language processing, and clinical risk prediction using electronic health records. In each case, we use a simple experimental protocol to show that these modeling pipelines admit a non-trivial set F∗ of near-optimal predictors, and that diﬀerent models in F∗ encode diﬀerent inductive biases that result in diﬀerent generalization behavior.
Similarly to our approach in Section 3, our protocol approaches underspeciﬁcation constructively by instantiating a set of predictors from the near-optimal set F∗, and then probing them to show that they encode diﬀerent inductive biases. However, for deep models, it is diﬃcult to specify predictors in this set analytically. Instead, we construct an ensemble of predictors from a given model by perturbing small parts of the ML pipeline (e.g., the random seed used in training, or the recurrent unit in an RNN), and retraining the model several times. When there is a non-trivial set F∗, such small perturbations are often enough to push the pipeline to return a diﬀerent choice f ∈ F∗. This strategy does not yield an exhaustive exploration of F∗; rather, it is a conservative indicator of which predictor properties are well-constrained and which are underspeciﬁed by the modeling pipeline.
Once we obtain an ensemble, we make several measurements. First, we empirically conﬁrm that the models in the ensemble have near-equivalent iid performance, and can thus be considered to be members of F∗. Secondly, we evaluate the ensemble on one or more application-speciﬁc stress tests that probe whether the predictors encode appropriate inductive biases for the application (see Section 2.3). Variability in stress test performance provides evidence that the modeling pipeline is underspeciﬁed along a practically important dimension.
11

A. D’Amour, K. Heller, D. Moldovan, et al
The experimental protocol we use to probe underspeciﬁcation is closely related to uncertainty quantiﬁcation approaches based on deep ensembles (e.g., Lakshminarayanan et al., 2017; Dusenberry et al., 2020). In particular, by averaging across many randomly perturbed predictors from a single modeling pipline, deep ensembles have been shown to be eﬀective tools for detecting out-of-distribution inputs, and correspondingly for tamping down the conﬁdence of predictions for such inputs (Snoek et al., 2019). Our experimental strategy and the deep ensembles approach can be framed as probing a notion of model stability resulting from perturbations to the model, even when the data are held constant (Yu et al., 2013).
To establish that observed variability in stress test performance is a genuine indicator of underspeciﬁcation, we evaluate three properties.
• First, we consider the magnitude of the variation, either relative to iid performance (when they are on the same scale), or relative to external benchmarks, such as comparisons between ML pipelines with featuring diﬀerent model architectures.
• Secondly, when sample size permits, we consider unpredictability of the variation from iid performance. Even if the observed diﬀerences in iid performance in our ensemble is small, if stress test performance tracks closely with iid performance, this would suggest that our characterization of F∗ is too permissive. We assess this with the Spearman rank correlation between the iid validation metric and the stress test metric.
• Finally, we establish that the variation in stress tests indicates systematic diﬀerences between the predictors in the ensemble. Often, the magnitude of variation in stress test performance alone will be enough to establish systematicness. However, in some cases we supplement with a mixture of quantitative and qualitative analyses of stress test outputs to illustrate that the diﬀerences between models does align with important dimensions of the application.
In all cases that we consider, we ﬁnd evidence that important inductive biases are underspeciﬁed. In some cases the evidence is obvious, while in others it is more subtle, owing in part to the conservative nature of our exploration of F∗. Our results interact with a number of research areas in each of the ﬁelds that we consider, so we close each case study with a short application-speciﬁc discussion.
5. Case Studies in Computer Vision
Computer vision is one of the ﬂagship application areas in which deep learning on large-scale training sets has advanced the state of the art. Here, we focus on an image classiﬁcation task, speciﬁcally on the ImageNet validation set (Deng et al., 2009). We examine two models: the ResNet-50 model (He et al., 2016) trained in ImageNet, and a ResNet-101x3 Big Transfer (BiT) model (Kolesnikov et al., 2019) pre-trained on the JFT-300M dataset (Sun et al., 2017) and ﬁne-tuned on ImageNet. The former is a standard baseline in image classiﬁcation. The latter is scaled-up ResNet designed for transfer learning, which attains state-of-the-art, or near state-of-the-art, on many image classiﬁcation benchmarks, including ImageNet.
A key challenge in computer vision is robustness under distribution shift. It has been welldocumented that many deep computer vision models suﬀer from brittleness under distribution shifts that humans do not ﬁnd challenging (Goodfellow et al., 2016; Hendrycks and Dietterich, 2019; Barbu et al., 2019). This brittleness has raised questions about deployments open-world high-stakes applications. This has given rise to an active literature on robustness in image classiﬁcation (see, e.g., Taori et al., 2020; Djolonga et al., 2020). Recent work has connected lack of robustness to computer vision models’ encoding counterintuitive inductive biases (Ilyas et al., 2019; Geirhos et al., 2019; Yin et al., 2019; Wang et al., 2020).
Here, we show concretely that the models we study are underspeciﬁed in ways that are important for robustness to distribution shift. We apply our experimental protocol to show that there is
12

Underspecification in Machine Learning
substantial ambiguity in how image classiﬁcation models will perform under distribution shift, even when their iid performance is held ﬁxed. Speciﬁcally, we construct ensembles of the ResNet-50 and BiT models to stress test: we train 50 ResNet-50 models on ImageNet using identical pipelines that diﬀer only in their random seed, 30 BiT models that are initialized at the same JFT-300M-trained checkpoint, and diﬀer only in their ﬁne-tuning seed and initialization distributions (10 runs each of zero, uniform, and Gaussian initializations). On the ImageNet validation set, the ResNet-50 predictors achieve a 75.9% ± 0.11 top-1 accuracy, while the BiT models achieve a 86.2% ± 0.09 top-1 accuracy.
We evaluate these predictor ensembles on two stress tests that have been proposed in the image classiﬁcation robustness literature: ImageNet-C (Hendrycks and Dietterich, 2019) and ObjectNet (Barbu et al., 2019). ImageNet-C is a benchmark dataset that replicates the ImageNet validation set, but applies synthetic but realistic corruptions to the images, such as pixelation or simulated snow, at varying levels of intensity. ObjectNet is a crowdsourced benchmark dataset designed to cover a set of classes included in the ImageNet validation set, but to vary the settings and conﬁgurations in which these objects are observed. Both stress tests have been used as prime examples of the lack of human-like robustness in deep image classiﬁcation models.
5.1 ImageNet-C
We show results from the evaluation on several ImageNet-C tasks in Figure 4. The tasks we show here incorporate corruptions at their highest intensity levels (level 5 in the benchmark). In the ﬁgure, we highlight variability in the accuracy across predictors in the ensemble, relative to the variability in accuracy on the standard iid test set. For both the ResNet-50 and BiT models, variation on some ImageNet-C tasks is an order of magnitude larger than variation in iid performance. Furthermore, within this ensemble, there is weak sample correlation between performance on the iid test set and performance on each benchmark stress test, and performance between tasks (all 95% CI’s for Pearson correlation using n = 50 and n = 30 contain zero, see Figure 5). We report full results on model accuracies and ensemble standard deviations in Table 1.
5.2 ObjectNet
We also evaluate these ensembles along more “natural” shifts in the ObjectNet test set. Here, we compare the variability in model performance on the ObjectNet test set to a subset of the standard ImageNet test set with the 113 classes that appear in ObjectNet. The results of this evaluation are in Table 1. The relative variability in accuracy on the ObjectNet stress test is larger that the variability seen in the standard test set (standard deviation is 2x for ResNet-50 and 5x for BiT), although the diﬀerence in magnitude is not as striking as in the ImageNet-C case. There is also a slightly stronger relationship between standard test accuracy and test accuracy on ObjectNet (Spearman ρ 0.22 (−0.06, 0.47) for ResNet-50, 0.47 (0.13, 71) for BiT).
Nonetheless, the variability in accuracy suggests that some predictors in the ensembles are systematically better or worse at making predictions on the ObjectNet test set. We quantify this with p-values from a one-sided permutation test, which we interpret as descriptive statistics. Speciﬁcally, we compare the variability in model performance on the ObjectNet test set with variability that would be expected if prediction errors were randomly distributed between predictors. The variability of predictor accuracies on ObjectNet is large compared to this baseline (p = 0.002 for ResNet-50 and p = 0.000 for BiT). On the other hand, the variability between predictor accuracies on the standard ImageNet test set are more typical of what would be observed if errors were randomly distributed (p = 0.203 for ResNet-50 and p = 0.474 for BiT). In addition, the predictors in our ensembles disagree far more often on the ObjectNet test set than they do in the ImageNet test set, whether or not we consider the subset of the ImageNet test set examples that have classes that appear in ObjectNet (Table 2).
13

A. D’Amour, K. Heller, D. Moldovan, et al

Dataset
ResNet-50 BiT

ImageNet
0.759 (0.001) 0.862 (0.001)

pixelate
0.197 (0.024) 0.555 (0.008)

contrast
0.091 (0.008) 0.462 (0.019)

motion blur
0.100 (0.007) 0.515 (0.008)

brightness
0.607 (0.003) 0.723 (0.002)

ObjectNet
0.259 (0.002) 0.520 (0.005)

Table 1: Accuracies of ensemble members on stress tests. Ensemble mean (standard deviations) of accuracy proportions on ResNet-50 and BiT models.

Dataset
ResNet-50 BiT

ImageNet
0.160 (0.001) 0.064 (0.004)

ImageNet (subset)
0.245 (0.005) 0.094 (0.006)

ObjectNet
0.509 (0.003) 0.253 (0.012)

Table 2: Ensemble disagreement proportions for ImageNet vs ObjectNet models. Average disagreement between pairs of predictors in the ResNet and BiT ensembles. The “subset” test set only includes classes that also appear in the ObjectNet test set. Models show substantially more disagreement on the ObjectNet test set.

5.3 Conclusions
These results indicate that the inductive biases that are relevant to making predictions in the presence of these corruptions are so weakly diﬀerentiated by the iid prediction task that changing random seeds in training can cause the pipeline to return predictors with substantially diﬀerent stress test performance. The fact that underspeciﬁcation persists in the BiT models is particularly notable, because simultaneously scaling up data and model size has been shown to improve performance across a wide range of robustness stress tests, aligning closely with how much these models improve performance on iid evaluations (Djolonga et al., 2020; Taori et al., 2020; Hendrycks et al., 2020). Our results here suggest that underspeciﬁcation remains an issue even for these models; potentially, as models are scaled up, underspeciﬁed dimensions may account for a larger proportion of the “headroom” available for improving out-of-distribution model performance.
6. Case Studies in Medical Imaging
Medical imaging is one of the primary high-stakes domains where deep image classiﬁcation models are directly applicable. In this section, we examine underspeciﬁcation in two medical imaging models designed for real-world deployment. The ﬁrst classiﬁes images of patient retinas, while the second classiﬁes clinical images of patient skin. We show that these models are underspeciﬁed along dimensions that are practically important for deployment. These results conﬁrm the need for explicitly testing and monitoring ML models in settings that accurately represent the deployment domain, as codiﬁed in recent best practices (Collins et al., 2015; Kelly et al., 2019; Rivera et al., 2020; Liu et al., 2020a).
6.1 Ophthalmological Imaging
Deep learning models have shown great promise in the ophthalmological domain (Gulshan et al., 2016; Ting et al., 2017). Here, we consider one such model trained to predict diabetic retinopathy (DR) and referable diabetic macular edema (DME) from retinal fundus images. The model employs an Inception-V4 backbone (Szegedy et al., 2017) pre-trained on ImageNet, and ﬁne-tuned using de-identiﬁed retrospective fundus images from EyePACS in the United States and from eye hospitals in India. Dataset and model architecture details are similar to those in (Krause et al., 2018).
A key use case for these models is to augment human clinical expertise in underserved settings, where doctor capacity may be stretched thin. As such, generalization to images taken by a range of

14

Underspecification in Machine Learning
Figure 4: Image classiﬁcation model performance on stress tests is sensitive to random initialization in ways that are not apparent in iid evaluation. (Top Left) Parallel axis plot showing variation in accuracy between identical, randomly initialized ResNet 50 models on several ImageNet-C tasks at corruption strength 5. Each line corresponds to a particular model in the ensemble; each each parallel axis shows deviation from the ensemble mean in accuracy, scaled by the standard deviation of accuracies on the “clean” ImageNet test set. On some tasks, variation in performance is orders of magnitude larger than on the standard test set. (Right) Example image from the standard ImageNet test set, with corrupted versions from the ImageNet-C benchmark.
Figure 5: Performance on ImageNet-C stress tests is unpredictable from standard test performance. Spearman rank correlations of predictor performance, calculated from random initialization predictor ensembles. (Left) Correlations from 50 retrainings of a ResNet-50 model on ImageNet. (Right) Correlations from 30 ImageNet ﬁne-tunings of a ResNet-101x3 model pre-trained on the JFT300M dataset.
15

A. D’Amour, K. Heller, D. Moldovan, et al
Figure 6: Stress test performance varies across identically trained medical imaging models. Points connected by lines represent metrics from the same model, evaluated on an iid test set (bold) and stress tests. Each axis shows deviations from the ensemble mean, divided by the standard deviation for that metric in the standard iid test set. These models diﬀer only in random initialization at the ﬁne-tuning stage. (Top Left) Variation in AUC between identical diabetic retinopathy classiﬁcation models when evaluated on images from diﬀerent camera types. Camera type 5 is a camera type that was not encountered during training. (Bottom Left) Variation in accuracy between identical skin condition classiﬁcation models when evaluated on diﬀerent skin types. (Right) Example images from the original test set (left) and the stress test set (right). Some images are cropped to match the aspect ratio.
Figure 7: Identically trained retinal imaging models show systematically diﬀerent behavior on stress tests. Calibration plots for two diabetic retinopathy classiﬁers (orange and blue) that diﬀer only in random seed at ﬁne-tuning. Calibration characteristics of the models are nearly identical for each in-distribution camera type 1–4, but are qualitatively diﬀerent for the held-out camera type 5. Error bars are ±2 standard errors.
cameras, including those deployed at diﬀerent locations and clinical settings, is essential for system usability (Beede et al., 2020).
Here, we show that the performance of predictors produced by this model is sensitive to underspeciﬁcation. Speciﬁcally, we construct an ensemble of 10 models that diﬀer only in random initialization at the ﬁne-tuning stage. We evaluate these models on stress tests predicting DR using camera type images not encountered during training.
The results are shown in Figure 6. Measuring accuracy in terms of AUC, variability in AUC on the held-out camera type is larger than that in the standard test set, both in aggregate, and
16

Underspecification in Machine Learning
compared to most strata of camera types in the training set. To establish that this larger variability is not easily explained away by diﬀerences in sample size, we conduct a two-sample z-test comparing the AUC standard deviation in the held-out camera test set (n = 287) against the AUC standard deviation in the standard test set (n = 3712) using jackknife standard errors, obtaining a z-value of 2.47 and a one-sided p-value of 0.007. In addition, models in the ensemble diﬀer systematically in ways that are not revealed by performance in the standard test set. For example, in Figure 7, we show calibration plots of two models from the ensemble computed across camera types. The models have similar calibration curves for the cameras encountered during training, but have markedly diﬀerent calibration curves for the held-out camera type. This suggests that these predictors process images in systematically diﬀerent ways that only become apparent when evaluated on the held-out camera type.
6.2 Dermatological Imaging
Deep learning based image classiﬁcation models have also been explored for applications in dermatology (Esteva et al., 2017). Here, we examine a model proposed in Liu et al. (2020b) that is trained to classify skin conditions from clinical skin images. As in Section 6.1, this model incorporates an ImageNet–pre-trained Inception-V4 backbone followed by ﬁne-tuning.
In this setting, one key concern is that the model may have variable performance across skin types, especially when these skin types are diﬀerently represented in the training data. Given the social salience of skin type, this concern is aligned with broader concerns about ensuring that machine learning does not amplify existing healthcare disparities (Adamson and Smith, 2018). In dermatology in particular, diﬀerences between the presentation of skin conditions across skin types has been linked to disparities in care (Adelekun et al., 2020).
Here, we show that model performance across skin types is sensitive to underspeciﬁcation. Speciﬁcally, we construct an ensemble of 10 models with randomly initialized ﬁne-tuning layer weights. We then evaluate the models on a stress test that stratiﬁes the test set by skin type on the Fitzpatrick scale (Fitzpatrick, 1975) and measures Top-1 accuracy within each slice.
The results are shown at the bottom of Figure 6. Compared to overall test accuracy, there is larger variation in test accuracy within skin type strata across models, particularly in skin types II and IV, which form substantial portions (n = 437, or 10.7%, and n = 798, or 19.6%, respectively) of the test data. Based on this test set, some models in this ensemble would be judged to have higher discrepancies across skin types than others, even though they were all produced by an identical training pipeline.
Because the sample sizes in each skin type stratum diﬀer substantially, we use a permutation test to explore the extent to which the larger variation in some subgroups can be accounted for by sampling noise. In particular, the larger variation within some strata could be explained by either sampling noise driven by smaller sample sizes, or by systematic diﬀerences between predictors that are revealed when they are evaluated on inputs whose distribution departs from the overall iid test set. This test shuﬄes the skin type indicators across examples in the test set, then calculates the variance of the accuracy across these random strata. We compute one-sided p-values with respect to this null distribution and interpret them as exploratory descriptive statistics. The key question is whether the larger variability in some strata, particularly skin types II and IV, can be explained away by sampling noise alone. (Our expectation is that skin type III is both large enough and similar enough to the iid test set that its accuracy variance should be similar to the overall variance, and the sample size for skin type V is so small that a reliable characterization would be diﬃcult.) Here, we ﬁnd that the variation in accuracy in skin types III and V are easily explained by sampling noise, as expected (p = 0.54, n = 2619; p = 0.42, n = 109). Meanwhile the variation in skin type II is largely consistent with sampling noise (p = 0.29, n = 437), but the variation in skin type IV seems to be more systematic (p = 0.03, n = 798). These results are exploratory, but they suggest a need to pay special attention to this dimension of underspeciﬁcation in ML models for dermatology.
17

A. D’Amour, K. Heller, D. Moldovan, et al
6.3 Conclusions
Overall, the vignettes in this section demonstrate that underspeciﬁcation can introduce complications for deploying ML, even in application areas where it has the potential to highly beneﬁcial. In particular, these results suggest that one cannot expect ML models to automatically generalize to new clinical settings or populations, because the inductive biases that would enable such generalization are underspeciﬁed. This conﬁrms the need to tailor and test models for the clinical settings and population in which they will be deployed. While current strategies exist to mitigate these concerns, addressing underspeciﬁcation, and generalization issues more generally, could reduce a number of points of friction at the point of care (Beede et al., 2020).
7. Case Study in Natural Language Processing
Deep learning models play a major role in modern natural language processing (NLP). In particular, large-scale Transformer models (Vaswani et al., 2017) trained on massive unlabeled text corpora have become a core component of many NLP pipelines (Devlin et al., 2019). For many applications, a successful recipe is to “pretrain” by applying a masked language modeling objective to a large generic unlabeled corpus, and then ﬁne-tune using labeled data from a task of interest, sometimes no more than a few hundred examples (e.g., Howard and Ruder, 2018; Peters et al., 2018) This workﬂows has yielded strong results across a wide range of tasks in natural language processing, including machine translation, question answering, summarization, sequence labeling, and more. As a result, a number of NLP products are built on top of publicly released pretrained checkpoints of language models such as BERT (Devlin et al., 2019).
However, recent work has shown that NLP systems built with this pattern often rely on “shortcuts” (Geirhos et al., 2020), which may be based on spurious phenomena in the training data (McCoy et al., 2019b). Shortcut learning presents a number of diﬃculties in natural language processing: failure to satisfy intuitive invariances, such as invariance to typographical errors or seemingly irrelevant word substitutions (Ribeiro et al., 2020); ambiguity in measuring progress in language understanding (Zellers et al., 2019); and reliance on stereotypical associations with race and gender (Caliskan et al., 2017; Rudinger et al., 2018; Zhao et al., 2018; De-Arteaga et al., 2019).
In this section, we show that underspeciﬁcation plays a role in shortcut learning in the pretrain/ﬁnetune approach to NLP, in both stages. In particular, we show that reliance on speciﬁc shortcuts can vary substantially between predictors that diﬀer only in their random seed at ﬁne-tuning or pretraining time. Following our experimental protocol, we perform this case study with an ensemble of predictors obtained from identical training pipelines that diﬀer only in the speciﬁc random seed used at pretraining and/or ﬁne-tuning time. Speciﬁcally, we train 5 instances of the BERT “large-cased” language model Devlin et al. (2019), using the same Wikipedia and BookCorpus data that was used to train the public checkpoints. This model has 340 million parameters, and is the largest BERT model with publicly released pretraining checkpoints. For tasks that require ﬁne-tuning, we ﬁne-tune each of the ﬁve checkpoints 20 times using diﬀerent random seeds.
In each case, we evaluate the ensemble of models on stress tests designed to probe for speciﬁc shortcuts, focusing on shortcuts based on stereotypical correlations, and ﬁnd evidence of underspeciﬁcation along this dimension in both pretraining and ﬁne-tuning. As in the other cases we study here, these results suggest that shortcut learning is not enforced by model architectures, but can be a symptom of ambiguity in model speciﬁcation.
Underspeciﬁcation has a wider range of implications in NLP. In the supplement, we connect our results to instability that has previously been reported on stress tests designed to diagnose “cheating” on Natural Language Inference tasks (McCoy et al., 2019b; Naik et al., 2018). Using the same protocol, we replicate the results (McCoy et al., 2019a; Dodge et al., 2020; Zhou et al., 2020), and extend them to show sensitivity to the pretraining random seed. We also explore how underspeciﬁcation aﬀects inductive biases in static word embeddings.
18

Underspecification in Machine Learning
7.1 Gendered Correlations in Downstream Tasks
We begin by examining gender-based shortcuts on two previously proposed benchmarks: a semantic textual similarity (STS) task and a pronoun resolution task.
7.1.1 Semantic textual similarity (STS)
In the STS task, a predictor takes in two sentences as input and scores their similarity. We obtain predictors for this task by ﬁne-tuning BERT checkpoints on the STS-B benchmark (Cer et al., 2017), which is part of the GLUE suite of benchmarks for representation learning in NLP (Wang et al., 2018). Our ensemble of predictors achieves consistent accuracy, measured in terms of correlation with human-provided similarity scores, ranging from 0.87 to 0.90. This matches reported results from Devlin et al. (2019), although better correlations have subsequently been obtained by pretraining on larger datasets (Liu et al., 2019; Lan et al., 2019; Yang et al., 2019).
To measure reliance on gendered correlations in the STS task, we use a set of challenge templates proposed by Webster et al. (2020): we create a set of triples in which the noun phrase in a given sentence is replaced by a profession, “a man”, or “a woman”, e.g., “a doctor/woman/man is walking.” The model’s gender association for each profession is quantiﬁed by the similarity delta between pairs from this triple, e.g.,
sim(“a woman is walking”, “a doctor is walking”) − sim(“a man is walking”, “a doctor is walking”).
A model that does not learn a gendered correlation for a given profession will have an expected similarity delta of zero. We are particularly interested in the extent to which the similarity delta for each profession correlates with the percentage of women actually employed in that profession, as measured by U.S. Bureau of Labor Statistics (BLS; Rudinger et al., 2018).
7.1.2 Pronoun resolution
In the pronoun resolution task, the input is a sentence with a pronoun that could refer to one of two possible antecedents, and the predictor must determine which of the antecedents is the correct one. We obtain predictors for this task by ﬁne-tuning BERT checkpoints on the OntoNotes dataset (Hovy et al., 2006). Our ensemble of predictors achieves accuracy ranging from 0.960 to 0.965.
To measure gendered correlations on the pronoun resolution task, we use the challenge templates proposed by Rudinger et al. (2018). In these templates, there is a gendered pronoun with two possible antecedents, one of which is a profession. The linguistic cues in the template are suﬃcient to indicate the correct antecedent, but models may instead learn to rely on the correlation between gender and profession. In this case, the similarity delta is the diﬀerence in predictive probability for the profession depending on the gender of the pronoun.
7.1.3 Gender correlations and underspecification
We ﬁnd signiﬁcant variation in the extent to which the models in our ensemble incorporate gendered correlations. For example, in Figure 8 (Left), we contrast the behavior of two predictors (which diﬀer only in pretraining and ﬁne-tuning seed) on the STS task. Here, the slope of the line is a proxy for the predictor’s reliance on gender. One ﬁne-tuning run shows strong correlation with BLS statistics about gender and occupations in the United States, while another shows a much weaker relationship. For an aggregate view, Figures 8 (Center) and (Right) show these correlations in the STS and coreference tasks across all predictors in our ensemble, with predictors produced from diﬀerent pretrainings indicated by diﬀerent markers. These plots show three important patterns:
1. There is a large spread in correlation with BLS statistics: on the STS task, correlations range from 0.3 to 0.7; on the pronoun resolution task, the range is 0.26 to 0.51. As a point of
19

A. D’Amour, K. Heller, D. Moldovan, et al

Semantic text similarity (STS) Test Accuracy Gender Correlation
Pronoun resolution Test Accuracy Gender Correlation

F (p-value)
5.66 (4e-04) 9.66 (1e-06)
48.98 (3e-22) 7.91 (2e-05)

Spearman ρ (95% CI)
— 0.21 (-0.00, 0.40)
— 0.08 (-0.13, 0.28)

Table 3: Summary statistics for structure of variation on gendered shortcut stress tests. For each dataset, we measure the accuracy of 100 predictors, corresponding to 20 randomly initialized ﬁne-tunings from 5 randomly initialized pretrained BERT checkpoints. Models are ﬁne-tuned on the STS-B and OntoNotes training sets, respectively. The F statistic quantiﬁes how systematic diﬀerences are between pretrainings using the ratio of within-pretraining variance to between-pretraining variance in the accuracy statistics. p-values are reported to give a sense of scale, but not for inferential purposes; it is unlikely that assumptions for a valid F -test are met. F -values of this magnitude are consistent with systematic between-group variation. The Spearman ρ statistic quantiﬁes how ranked performance on the ﬁne-tuning task correlates with the stress test metric of gender correlation.

comparison, prior work on gender shortcuts in pronoun resolution found correlations ranging between 0.31 and 0.55 for diﬀerent types of models (Rudinger et al., 2018).
2. There is a weak relationship between test accuracy performance and gendered correlation (STS-B: Spearman ρ = 0.21; 95% CI = (0.00, 0.39), Pronoun resolution: Spearman ρ = 0.08; 95% CI = (−0.13, 0.29)). This indicates that learning accurate predictors does not require learning strong gendered correlations.
3. Third, the encoding of spurious correlations is sensitive to the random seed at pretraining, and not just ﬁne-tuning. Especially in the pronoun resolution task, (Figure 8(Right)) predictors produced by diﬀerent pretraining seeds cluster together, tending to show substantially weaker or stronger gender correlations.
In Table 3, we numerically summarize the variance with respect to pretraining and ﬁne-tuning using an F statistic — the ratio of between-pretraining to within-pretraining variance. The pretraining seed has an eﬀect on both the main ﬁne-tuning task and the stress test, but the small correlation between the ﬁne-tuning tasks and stress test metrics suggests that this random seed aﬀects these metrics independently.
To better understand the diﬀerences between predictors in our ensemble, we analyze the structure in how similarity scores produced by the predictors in our ensemble deviate from the ensemble mean. Here, we ﬁnd that the main axis of variation aligns, at least at its extremes, with diﬀerences in how predictors represent sterotypical associations between profession and gender. Speciﬁcally, we perform principal components analysis (PCA) over similarity score produced by 20 ﬁne-tunings of a single BERT checkpoint. We plot the ﬁrst principal components, which contains 22% of the variation in score deviations, against BLS female participation percentages in Figure 9. Notably, examples in the region where the ﬁrst principal component values are strongly negative include some of the strongest gender imbalances. The right side of Figure 9 shows some of these examples (marked in red on the scatterplots), along with the predicted similarities from models that have strongly negative or strongly positive loadings on this principal axis. The similarity scores between these models are clearly divergent, with the positive-loading models encoding a sterotypical contradiction between gender and profession—that is, a contradiction between ‘man’ and ‘receptionist’ or ‘nurse’; or a contradiction between ‘woman’ and ‘mechanic’, ‘carpenter’, and ‘doctor’—that the negative-loading models do not.

20

Underspecification in Machine Learning
Figure 8: Reliance on gendered correlations is aﬀected by random initialization. (Left) The gap in similarity for female and male template sentences is correlated with the gender statistics of the occupation, shown in two randomly-initialized ﬁne-tunes. (Right) Pretraining initialization signiﬁcantly aﬀects the distribution of gender biases encoded at the ﬁne-tuning stage.
Figure 9: The ﬁrst principal axis of model disagreement predicts diﬀerences in handling stereotypes. The ﬁrst principal component of BERT models ﬁne-tuned for STS-B, against the % female participation of a profession in the BLS data. The top panel shows examples with a male subject (e.g., “a man”) and the bottom panel shows examples with a female subject. The region to the far left (below −1) shows that the second principal component encodes apparent gender contradictions: ‘man’ partnered with a female-dominated profession (top) or ‘woman’ partnered with a male-dominated profession (bottom). On the right, examples marked with red points in the left panels are shown, along with their BLS percentages in parentheses, and predicted similarities from the predictors with the most negative and positive loadings in the ﬁrst principal component.
21

A. D’Amour, K. Heller, D. Moldovan, et al
Figure 10: Diﬀerent pretraining seeds produce diﬀerent steretypical associations. Results across ﬁve identically trained BERT Large (Cased) pretraining checkpoints on StereoSet (Nadeem et al., 2020). The ICAT score combines a language model (LM) score measuring “sensibility” and a stereotype score measuring correlations of language model predictions with known stereotypes. A leaderboard featuring canonical pretrainings is available at https://stereoset.mit.edu/.
7.2 Stereotypical Associations in Pretrained Language Models Underspeciﬁcation in supervised NLP systems can occur at both the ﬁne-tuning and pretraining stages. In the previous section, we gave suggestive evidence that underspeciﬁcation allows identically pretrained BERT checkpoints to encode substantively diﬀerent inductive biases. Here, we examine pretraining underspeciﬁcation more directly, considering again its impact on reliance on stereotypical shortcuts. Speciﬁcally, we examine the performance of our ensemble of ﬁve BERT checkpoints on the StereoSet benchmark (Nadeem et al., 2020).
StereoSet is a set of stress tests designed to directly assess how the predictions of pretrained language models correlate with well-known social stereotypes. Speciﬁcally, the test inputs are spans of text with sentences or words masked out, and the task is to score a set of choices for the missing piece of text. The choice set contains one non-sensical option, and two sensical options, one of which conforms to a stereotype, and the other of which does not. The benchmark probes stereotypes along the axes of gender, profession, race, and religion. Models are scored based on both whether they are able to exclude the non-sensical option (LM Score) and whether they consistently choose the option that conforms with the stereotype (Stereotype Score). These scores are averaged together to produce an Idealized Context Association Test (ICAT) score, which can be applied to any language model.
In Figure 10, we show the results of evaluating our ﬁve BERT checkpoints, which diﬀer only in random seed, across all StereoSet metrics. The variation across checkpoints is large. The range of overall ICAT score between the our identical checkpoints is 3.35. For context, this range is larger than the gap between the top six models on the public leaderboard,1, which diﬀer in size, architecture, and training data (GPT-2 (small), XLNet (large), GPT-2 (medium), BERT (base), GPT-2 (large), BERT (large)). On the disaggregated metrics, the score range between checkpoints is narrower on the LM score (sensible vs. non-sensible sentence completions) than on the Stereotype score (consistent vs. inconsistent with social stereotypes). This is consistent with underspeciﬁcation, as the LM score is more closely aligned to the training task. Interestingly, score ranges are also lower on overall metrics compared to by-demographic metrics, suggesting that even when model performance looks stable in aggregate, checkpoints can encode diﬀerent social stereotypes.
7.3 Spurious Correlations in Natural Language Inference Underspeciﬁcation also aﬀects more general inductive biases that align with some notions of “semantic understanding” in NLP systems. One task that probes such notions is natural language inference (NLI). The NLI task is to classify sentence pairs (called the premise and hypothesis) into one of the following semantic relations: entailment (the hypothesis is true whenever the premise is), contradiction (the hypothesis is false when the premise is true), and neutral (Bowman et al., 2015).
1. https://stereoset.mit.edu retrieved October 28, 2020.
22

Underspecification in Machine Learning
Typically, language models are ﬁne-tuned for this task on labeled datasets such as the MultiNLI training set (Williams et al., 2018). While test set performance on benchmark NLI datasets approaches human agreement (Wang et al., 2018), it has been shown that there are shortcuts to achieving high performance on many NLI datasets (McCoy et al., 2019b; Zellers et al., 2018, 2019). In particular, on stress tests that are designed to probe semantic inductive biases more directly these models are still far below human performance.
Notably, previous work has shown that performance on these stronger stress tests has also been shown to be unstable with respect to the ﬁne-tuning seed (Zhou et al., 2020; McCoy et al., 2019a; Dodge et al., 2020). We interpret this to be a symptom of underspeciﬁcation. Here, we replicate and extend this prior work by assessing sensitivity to both ﬁne-tuning and, for the ﬁrst time, pretraining. Here we use the same ﬁve pre-trained BERT Large cased checkpoints, and ﬁne-tune each on the MultiNLI training set (Williams et al., 2018) 20 times. Across all pre-trainings and ﬁne-tunings, accuracy on the standard MNLI matched and unmatched test sets are in tightly constrained ranges of (83.4% − 84.4%) and (83.8% − 84.7%), respectively.2
We evaluate our ensemble of predictors on the HANS stress test (McCoy et al., 2019b) and the StressTest suite from Naik et al. (2018). The HANS Stress Tests are constructed by identifying spurious correlations in the training data — for example, that entailed pairs tend to have high lexical overlap — and then generating a test set such that the spurious correlations no longer hold. The Naik et al. (2018) stress tests are constructed by perturbing examples, for example by introducing spelling errors or meaningless expressions (“and true is true”).
We again ﬁnd strong evidence that the extent to which a trained model relies on shortcuts is underspeciﬁed, as demonstrated by sensitivity to the choice of random seed at both ﬁne-tuning and pre-training time. Here, we report several broad trends of variation on these stress tests: ﬁrst, the magnitude of the variation is large; second, the variation is also sensitive to the ﬁne-tuning seed, replicating Zhou et al. (2020); third, the variation is also sensitive to the pre-training seed; fourth, the variation is diﬃcult to predict based on performance on the standard MNLI validation sets; and ﬁnally, the variation on diﬀerent stress tests tends to be weakly correlated.
Figure 11 shows our full set of results, broken down by pre-training seed. These plots show evidence of the inﬂuence of the pre-training seed; for many tests, there appear to be systematic diﬀerences in performance from ﬁne-tunings based on checkpoints that were pre-trained with diﬀerent seeds. We report one numerical measurement of these diﬀerences with F statistics in Table 4, where the ratio of between-group variance to within-group variance is generally quite large. Table 4 also reports Spearman rank correlations between stress test accuracies and accuracy on the MNLI matched validation set. The rank correlation is typically small, suggesting that the variation in stress test accuracy is largely orthogonal to validation set accuracy enforced by the training pipeline. Finally, in Figure 12, we show that the correlation between stress tests performance is also typically small (with the exception of some pairs of stress tests meant to test the same inductive bias), suggesting that the space of underspeciﬁed inductive biases spans many dimensions.
7.4 Conclusions
There is increasing concern about whether natural language processing systems are learning general linguistic principles, or whether they are simply learning to use surface-level shortcuts (e.g., Bender and Koller, 2020; Linzen, 2020). Particularly worrying are shortcuts that reinforce societal biases around protected attributes such as gender (e.g., Webster et al., 2020). The results in this section replicate prior ﬁndings that highly-parametrized NLP models do learn spurious correlations and shortcuts. However, this reliance is underspeciﬁed by the model architecture, learning algorithm, and training data: merely changing the random seed can induce large variation in the extent to which spurious correlations are learned. Furthermore, this variation is demonstrated in both pretraining
2. The “matched” and “unmatched” conditions refer to whether the test data is drawn from the same genre of text as the training set.
23

A. D’Amour, K. Heller, D. Moldovan, et al
Figure 11: Predictor performance on NLI stress tests varies both within and between pretraining checkpoints. Each point corresponds to a ﬁne-tuning of a pre-trained BERT checkpoint on the MNLI training set, with pre-training distinguished on the x-axis. All pre-trainings and ﬁne-tunings diﬀer only in random seed at their respective training stages. Performance on HANS (McCoy et al., 2019b) is shown in the top left; remaining results are from the StressTest suite (Naik et al., 2018). Red bars show a 95% CI around for the mean accuracy within each pre-training. The tests in the bottom group of panels were also explored in Zhou et al. (2020) across ﬁne-tunings from the public BERT large cased checkpoint (Devlin et al., 2019); for these, we also plot the mean +/1.96 standard deviations interval, using values reported in Zhou et al. (2020). The magnitude of variation is substantially larger on most stress tests than the MNLI test sets (< 1% on both MNLI matched and unmatched). There is also substantial variation between some pretrained checkpoints, even after ﬁne-tuning.
24

Underspecification in Machine Learning

Dataset
MNLI, matched MNLI, mismatched
Naik et al. (2018) stress tests Antonym, matched Antonym, mismatched Length Mismatch, matched Length Mismatch, mismatched Negation, matched Negation, mismatched Spelling Error, matched Spelling Error, mismatched Word Overlap, matched Word Overlap, mismatched Numerical Reasoning
HANS (McCoy et al., 2019b)

F (p-value)
1.71 (2E-01) 20.18 (5E-12)

15.46 7.32 4.83 5.61 19.62 18.21 25.11 14.65 9.99 9.13 12.02

(9E-10) (4E-05) (1E-03) (4E-04) (8E-12) (4E-11) (3E-14) (2E-09) (9E-07) (3E-06) (6E-08)

4.95 (1E-03)

Spearman ρ (95% CI)
— 0.11 (-0.10, 0.31)

0.05 0.01 0.33 -0.03 0.17 0.09 0.40 0.43 0.08 -0.07 0.18

(-0.16, (-0.20, ( 0.13, (-0.24, (-0.04, (-0.12, ( 0.21, ( 0.24, (-0.13, (-0.27, (-0.03,

0.26) 0.21) 0.50) 0.18) 0.36) 0.29) 0.56) 0.58) 0.28) 0.14) 0.38)

0.07 (-0.14, 0.27)

Table 4: Summary statistics for structure of variation in predictor accuracy across NLI stress tests. For each dataset, we measure the accuracy of 100 predictors, corresponding to 20 randomly initialized ﬁne-tunings from 5 randomly initialized pretrained BERT checkpoints. All models are ﬁne-tuned on the MNLI training set, and validated on the MNLI matched test set (Williams et al., 2018). The F statistic quantiﬁes how systematic diﬀerences are between pretrainings. Speciﬁcally, it is the ratio of within-pretraining variance to between-pretraining variance in the accuracy statistics. p-values are reported to give a sense of scale, but not for inferential purposes; it is unlikely that assumptions for a valid F -test are met. The Spearman ρ statistic quantiﬁes how ranked performance on the MNLI matched test set correlates with ranked performance on each stress test. For most stress tests, there is only a weak relationship, such that choosing models based on test performance alone would not yield the best models on stress test performance.

25

A. D’Amour, K. Heller, D. Moldovan, et al
Figure 12: Predictor performance across stress tests are typically weakly correlated. Spearman correlation coeﬃcients of 100 predictor accuracies from 20 ﬁne-tunings of ﬁve pretrained BERT checkpoints.
and ﬁne-tuning, indicating that pretraining alone can “bake in” more or less robustness. This implies that individual stress test results should be viewed as statements about individual model checkpoints, and not about architectures or learning algorithms. More general comparisons require the evaluation of multiple random seeds.
8. Case Study in Clinical Predictions from Electronic Health Records
The rise of Electronic Health Record (EHR) systems has created an opportunity for building predictive ML models for diagnosis and prognosis (e.g. Ambrosino et al. (1995); Brisimi et al. (2019); Feng et al. (2019)). In this section, we focus on one such model that uses a Recurrent Neural Network (RNN) architecture with EHR data to predict acute kidney injury (AKI) during hospital admissions (Tomašev et al., 2019a). AKI is a common complication in hospitalized patients and is associated with increased morbidity, mortality, and healthcare costs (Khwaja, 2012). Early intervention can improve outcomes in AKI (National Institute for Health and Care Excellence (NICE), 2019), which has driven eﬀorts to predict it in advance using machine learning. Tomašev et al. (2019a) achieve state-of-the-art performance, detecting the onset of AKI up to 48 hours in advance with an accuracy of 55.8% across all episodes and 90.2% for episodes associated with dialysis administration.
Despite this strong discriminative performance, there have been questions raised about the associations being learned by this model and whether they conform with our understanding of physiology(Kellum and Bihorac, 2019). Speciﬁcally, for some applications, it is desirable to disentangle physiological signals from operational factors related to the delivery of healthcare, both of which appear in EHR data. As an example, the value of a lab test may be considered a physiological signal; however the timing of that same test may be considered an operational one (e.g. due to staﬃng constraints during the night or timing of ward rounds). Given the fact that operational signals may
26

Underspecification in Machine Learning
be institution-speciﬁc and are likely to change over time, understanding to what extent a model relies on diﬀerent signals can help practitioners determine whether the model meets their speciﬁc generalization requirements (Futoma et al., 2020).
Here, we show that underspeciﬁcation makes the answer to this question ambiguous. Speciﬁcally, we apply our experimental protocol to the Tomašev et al. (2019a) AKI model which predicts the continuous risk (every 6 hours) of AKI in a 48h lookahead time window (see Supplement for details).
8.1 Data, Predictor Ensemble, and Metrics
The pipeline and data used in this study are described in detail in Tomašev et al. (2019a). Brieﬂy, the data consists of de-identiﬁed EHRs from 703,782 patients across multiple sites in the United States collected at the US Department of Veterans Aﬀairs3 between 2011 and 2015. Records include structured data elements such as medications, labs, vital signs, diagnosis codes etc, aggregated in six hour time buckets (time of day 1: 12am-6am, 2: 6am-12pm, 3: 12pm-6pm, 4: 6pm-12am). In addition, precautions beyond standard de-identiﬁcation have been taken to safeguard patient privacy: free text notes and rare diagnoses have been excluded; many feature names have been obfuscated; feature values have been jittered; and all patient records are time-shifted, respecting relative temporal relationships for individual patients. Therefore, this dataset is only intended for methodological exploration.
The model consists of embedding layers followed by a 3 layer-stacked RNN before a ﬁnal dense layer for prediction of AKI across multiple time horizons. Our analyses focus on predictions with a 48h lookahead horizon, which have been showcased in the original work for their clinical actionability. To examine underspeciﬁcation, we construct a model ensemble by training the model from 5 random seeds for each of three RNN cell types: Simple Recursive Units (SRU, Lei et al. (2018)), Long Short-Term Memory (LSTM, Hochreiter and Schmidhuber (1997)) or Update Gate RNN (UGRNN, Collins et al. (2017)). This yields an ensemble of 15 model instances in total.
The primary metric that we use to evaluate predictive performance is normalized area under the precision-recall curve (PRAUC) (Boyd et al., 2012), evaluated across all patient-timepoints where the model makes a prediction. This is a PRAUC metric that is normalized for prevalence of the positive label (in this case, AKI events). Our ensemble of predictors achieves tightly constrained normalized PRAUC values between 34.59 and 36.61.
8.2 Reliance on Operational Signals
We evaluate these predictors on stress tests designed to probe the sensitivity to speciﬁc operational signals in the data: the timing and number of labs recorded in the EHR4. In this dataset, the prevalence of AKI is largely the same across diﬀerent times of day (see Table 1 of Supplement). However, AKI is diagnosed based on lab tests, 5, and there are clear temporal patterns in how tests are ordered. For most patients, creatinine is measured in the morning as part of a ‘routine’, comprehensive panel of lab tests. Meanwhile, patients requiring closer monitoring may have creatinine samples taken at additional times, often ordered as part of an ‘acute’, limited panel (usually, the basic metabolic panel6). Thus, both the time of day that a test is ordered, and the panel of tests that accompany a given measurement may be considered primarily as operational factors correlated with AKI risk.
3. Disclaimer: Please note that the views presented in this manuscript are that of the authors and not that of the Department of the Veterans Aﬀairs.
4. Neither of these factors are purely operational—there is known variation in kidney function across the day and the values of accompanying lab tests carry valuable information about patient physiology. However, we use these here as approximations for an operational perturbation.
5. speciﬁcally a comparison of past and current values of creatinine (Khwaja, 2012) 6. This panel samples Creatinine, Sodium, Potassium, Urea Nitrogen, CO2, Chloride and Glucose.
27

A. D’Amour, K. Heller, D. Moldovan, et al
We test for reliance on this signal by applying two interventions to the test data that modify (1) the time of day of all features (aggregated in 6h buckets) and (2) the selection of lab tests. The ﬁrst intervention shifts the patient timeseries by a ﬁxed oﬀset, while the second intervention additionally removes all blood tests that are not directly relevant to the diagnosis of AKI. We hypothesize that if the predictor encodes physiological signals rather than these operational cues, the predictions would be invariant to these interventions. More importantly, if the model’s reliance on these operational signals is underspeciﬁed, we would expect the behavior of the predictors in our ensemble to respond diﬀerently to these modiﬁed inputs.
We begin by examining overall performance on this shifted test set across our ensemble. In Figure 13, we show that performance on the intervened data is both worse and more widely dispersed than in the standard test set, especially when both interventions are applied. This shows that the model incorporates time of day and lab content signals, and that the extent to which it relies on these signals is sensitive to both the recurrent unit and random initialization.
Figure 13: Variability in performance from ensemble of RNN models processing electronic health records (EHR). Model sensitivity to time of day and lab perturbations. The x-axis denotes the evaluation set: “Test” is the original test set; “Shift” is the test set with time shifts applied; “Shift + Labs” applies the time shift and subsets lab orders to only include the basic metabolic panel CHEM-7. The y-axis represents the normalized PRAUC, and each set of dots joined by a line represents a model instance.
The variation in performance reﬂects systematically diﬀerent inductive biases encoded by the predictors in the ensemble. We examine this directly by measuring how individual model predictions change under the timeshift and lab intervensions. Here, we focus on two trained LSTM models that diﬀer only in their random seeds, and examine patient-timepoints at which creatinine measurements were taken. In Figure 14(Right), we show distributions of predicted risk on the original patienttimepoints observed in the “early morning” (12am-6am) time range, and proportional changes to these risks when the timeshift and lab interventions were applied. Both predictors exhibit substantial changes in predicted risk under both interventions, but the second predictor is far more sensitive to these changes than the ﬁrst, with the predicted risks taking on substantially diﬀerent distributions depending on the time range to which the observation is shifted.
These shifts in risk are consequential for decision-making and can result in AKI episodes being predicted tardily or missed. In Figure 15, we illustrate the number of patient-timepoints where the changed risk score crosses each model’s calibrated decision threshold. In addition to substantial diﬀerences in the number of ﬂipped decisions, we also show that most of these ﬂipped decisions occur at diﬀerent patient-timepoints across models.
8.3 Conclusions Our results here suggest that predictors produced by this model tend to rely on the pattern of lab orders in a substantial way, but the extent of this reliance is underspeciﬁed. Depending on how stable
28

Underspecification in Machine Learning

Figure 14: Variability in AKI risk predictions between two LSTM models processing

electronic health records (EHR). Histograms showing showing risk predictions from two models,

and changes induced by time of day and lab perturbations. Histograms show counts of patient-

timepoints where creatinine measurements were taken in the early morning (12am-6am). LSTM

1 and 5 diﬀer only in random seed. “Test” shows histogram of risk predicted in original test data.

“Shift”

and

“Shift

+

Labs”

show

histograms

of

proportional

changes

(in

%)

Perturbed−Baseline Baseline

induced

by the time-shift perturbation and the combined time-shift and lab perturbation, respectively.

this signal is in the deployment context, this may or may not present challenges. However, this result also shows that the reliance on this signal is not enforced by the model speciﬁcation of training data, suggesting that the reliance on lab ordering patterns could be modulated by simply adding constraints to the training procedure, and without sacriﬁcing iid performance. In the Supplement, we show one such preliminary result, where a model trained with the timestamp feature completely ablated was able to achieve identical iid predictive performance. This is compatible with previous ﬁndings that inputting medical/domain relational knowledge has led to better out of domain behaviour Nestor et al. (2019), performance Popescu and Khalilia (2011); Choi et al. (2017); Tomašev et al. (2019a,b) and interpretability Panigutti et al. (2020) of ML models.
9. Discussion: Implications for ML Practice
Our results show that underspeciﬁcation is a key failure mode for machine leaning models to encode generalizable inductive biases. We have used between-predictor variation in stress test performance as an observable signature of underspeciﬁcation. This failure mode is distinct from generalization failures due to structural mismatch between training and deployment domains. We have seen that underspeciﬁcation is ubiquitious in practical machine learning pipelines across many domains. Indeed, thanks to underspeciﬁcation, substantively important aspects of the decisions are determined by arbitrary choices such as the random seed used for parameter initialization. We close with a discussion of some of the implications of the study, which broadly suggest a need to ﬁnd better interfaces for domain knowledge in ML pipelines.
First, we note that the methodology in this study underestimates the impact of underspeciﬁcation: our goal was to detect rather than fully characterize underspeciﬁcation, and in most examples, we only explored underspeciﬁcation through the subtle variation that can result from modifying random seeds in training. However, modern deep learning pipelines incorporate a wide variety of ad hoc

29

A. D’Amour, K. Heller, D. Moldovan, et al
Figure 15: Variability in AKI predictions between two LSTM models processing electronic health records (EHR). Counts (color-coded) of decisions being ﬂipped due to the stress tests, from the LSTM 1 and LSTM 5 models, as well as the proportions of those ﬂipped decision intersecting between the two models (in %). Rows represent the time of day in the original test set, while columns represent the time of day these samples were shifted to. LSTM 1 and 5 diﬀer only in random seed. “Shift” represents the ﬂipped decisions (both positive to negative and negative to positive) between the predictions on the test set and the predictions after time-shift perturbation. “Shift + Labs” represents the same information for the combined time-shift and labs perturbation.
practices, each of which may carry its own “implicit regularization”, which in turn can translate into substantive inductive biases about how diﬀerent features contribute to the behavior of predictors. These include the particular scheme used for initialization; conventions for parameterization; choice of optimization algorithm; conventions for representing data; and choices of batch size, learning rate, and other hyperparameters, all of which may interact with the infrastructure available for training and serving models (Hooker, 2020). We conjecture that many combinations of these choices would reveal a far larger risk-preserving set of predictors F∗, a conjecture that has been partially borne out by concurrent work (Wenzel et al., 2020). However, we believe that there would be value in more systematically mapping out the set of iid-equivalent predictors that a pipeline could return as a true measurement of the uncertainty entailed by underspeciﬁcation. Current eﬀorts to design more eﬀective methods for exploring loss landscapes (Fort et al., 2019; Garipov et al., 2018) could play an important role here, and there are opportunities to import ideas from the sensitivity analysis and partial identiﬁcation subﬁelds in causal inference and inverse problems.
Second, our ﬁndings underscore the need to thoroughly test models on application-speciﬁc tasks, and in particular to check that the performance on these tasks is stable. The extreme complexity of modern ML models ensures that some aspect of the model will almost certainly be underspeciﬁed; thus, the challenge is to ensure that this underspeciﬁcation does not jeopardize the inductive biases that are required by an application. In this vein, designing stress tests that are well matched to applied requirements, and that provide good “coverage" of potential failure modes is a major challenge that requires incorporating domain knowledge. This can be particularly challenging, given our results show that there is often low correlation between performance on distinct stress tests when iid performance is held constant, and the fact that many applications will have ﬁne-grained requirements that require more customized stress testing. For example, within the medical risk prediction domain, the dimensions that a model is required to generalize across (e.g., temporal,
30

Underspecification in Machine Learning
demographic, operational, etc.) will depend on the details of the deployment and the goals of the practitioners (Futoma et al., 2020). For this reason, developing best practices for building stress tests that crisply represent requirements, rather than standardizing on speciﬁc benchmarks, may be an eﬀective approach. This approach has gained traction in the NLP subﬁled, where several papers now discuss the process by which stress tests datasets should iterate continuously (Zellers et al., 2019), and new systems for developing customized stress tests have been proposed (Ribeiro et al., 2020; Kaushik et al., 2020).
Third, our results suggest some ways forward for training models with credible inductive biases. By deﬁnition, underspeciﬁcation can be resolved by specifying additional criteria for selecting predictors from the equivalence class of near-optimal predictors F∗. Importantly, this suggests a departure from a popular strategy of improving iid performance of models by marginalizing across F∗ (Wilson and Izmailov, 2020). Here, because it is known that some predictors in F∗ generalize poorly in new domains, simply averaging them together is not guaranteed to produce better results on stress tests than carefully choosing a speciﬁc predictor from the equivalence class (see Appendix A for some examples). Of course, these approaches can be reconciled if the marginalization is restricted to models that satisfy required constraints. The challenge of specifying selection criteria or constraints on F∗, however, remains an active area of research. Because they are meant to enforce applicationspeciﬁc requirements, such criteria or constraints must also be application-speciﬁc, presenting a challenge for the development of general methods. Although some general-purpose heuristics have been proposed (Bengio, 2017), proposals for expressing application-speciﬁc requirements with ﬂexible but uniﬁed frameworks may be a promising middle ground solution. Causal DAGs (Schölkopf, 2019) and explanations (Ross et al., 2017) are both promising candidates for such frameworks.
Finally, when the main hurdle is underspeciﬁcation, adding constraints should not result in a tradeoﬀ between learning better inductive biases and generalizing well in iid settings. Some recent work in several places comports with this conjecture: in the robustness literature Raghunathan et al. (2020) show that robustness / accuracy tradeoﬀs need not be fundamental; in the NLP literature, Webster et al. (2020) show that reliance on gendered correlations can be reduced in BERT-derived models little-to-no tradeoﬀ (Webster et al., 2020); and in Appendix C, we show a similar preliminary result from our EHR example. These results suggest that designing application-speciﬁc regularization schemes (e.g., that bias the pipeline toward predictors that approximately respect causal structure) may be a promising direction for incorporating domain expertise without compromising the powerful prediction abilities of modern ML models.
Acknowledgements
This research has been conducted using the UK Biobank Resource under Application Number 17643. We would also like to thank our partners - EyePACS in the United States and Aravind Eye Hospital and Sankara Nethralaya in India for providing the datasets used to train the models for predicting diabetic retinopathy from fundus images. We also appreciate the advice of our DeepMind collaborator Dr. Nenad Tomasev, Prof. Finale Doshi-Velez and the wider Google Health Research UK team led by Dr. Alan Karthikesalingam.
References
Adewole S Adamson and Avery Smith. Machine learning and health care disparities in dermatology. JAMA dermatology, 154(11):1247–1248, 2018.
Ademide Adelekun, Ginikanwa Onyekaba, and Jules B Lipoﬀ. Skin color in dermatology textbooks: An updated evaluation and analysis. Journal of the American Academy of Dermatology, 2020.
31

A. D’Amour, K. Heller, D. Moldovan, et al
R Ambrosino, B G Buchanan, G F Cooper, and M J Fine. The use of misclassiﬁcation costs to learn rule-based decision support models for cost-eﬀective hospital admission strategies. Proceedings. Symposium on Computer Applications in Medical Care, pages 304–8, 1995. ISSN 01954210. URL http://www.ncbi.nlm.nih.gov/pubmed/8563290http://www.pubmedcentral.nih. gov/articlerender.fcgi?artid=PMC2579104.
Martin Arjovsky, Léon Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk minimization. arXiv preprint arXiv:1907.02893, 2019.
Susan Athey. Beyond prediction: Using big data for policy problems. Science, 355(6324):483–485, 2017.
Marzieh Babaeianjelodar, Stephen Lorenz, Josh Gordon, Jeanna Matthews, and Evan Freitag. Quantifying gender bias in diﬀerent corpora. In Companion Proceedings of the Web Conference 2020, pages 752–759, 2020.
Andrei Barbu, David Mayo, Julian Alverio, William Luo, Christopher Wang, Dan Gutfreund, Josh Tenenbaum, and Boris Katz. Objectnet: A large-scale bias-controlled dataset for pushing the limits of object recognition models. In Advances in Neural Information Processing Systems, pages 9448–9458, 2019.
Emma Beede, Elizabeth Baylor, Fred Hersch, Anna Iurchenko, Lauren Wilcox, Paisan Ruamviboonsuk, and Laura M Vardoulakis. A human-centered evaluation of a deep learning system deployed in clinics for the detection of diabetic retinopathy. In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems, pages 1–12, 2020.
Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling modern machine learning and the bias-variance trade-oﬀ. arXiv preprint arXiv:1812.11118, 2018.
Emily M. Bender and Alexander Koller. Climbing towards NLU: On meaning, form, and understanding in the age of data. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5185–5198, Online, July 2020. Association for Computational Linguistics. doi: 10. 18653/v1/2020.acl-main.463. URL https://www.aclweb.org/anthology/2020.acl-main.463.
Yoshua Bengio. The consciousness prior. arXiv preprint arXiv:1709.08568, 2017.
Jeremy J Berg, Arbel Harpak, Nasa Sinnott-Armstrong, Anja Moltke Joergensen, Hakhamanesh Mostafavi, Yair Field, Evan August Boyle, Xinjun Zhang, Fernando Racimo, Jonathan K Pritchard, and Graham Coop. Reduced signal for polygenic adaptation of height in UK biobank. Elife, 8, March 2019.
Tolga Bolukbasi, Kai-Wei Chang, James Y Zou, Venkatesh Saligrama, and Adam T Kalai. Man is to computer programmer as woman is to homemaker? debiasing word embeddings. In Advances in neural information processing systems, pages 4349–4357, 2016.
Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. A large annotated corpus for learning natural language inference. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 632–642, Lisbon, Portugal, September 2015. Association for Computational Linguistics. doi: 10.18653/v1/D15-1075. URL https: //www.aclweb.org/anthology/D15-1075.
Kendrick Boyd, Vítor Santos Costa, Jesse Davis, and C. David Page. Unachievable region in precision-recall space and its eﬀect on empirical evaluation. In Proceedings of the 29th International Conference on Machine Learning, ICML 2012, volume 1, pages 639–646, 2012. ISBN 9781450312851.
32

Underspecification in Machine Learning
Theodora S. Brisimi, Tingting Xu, Taiyao Wang, Wuyang Dai, and Ioannis Ch Paschalidis. Predicting diabetes-related hospitalizations based on electronic health records. Statistical Methods in Medical Research, 28(12):3667–3682, dec 2019. ISSN 14770334. doi: 10.1177/0962280218810911.
Joy Buolamwini and Timnit Gebru. Gender shades: Intersectional accuracy disparities in commercial gender classiﬁcation. In Conference on fairness, accountability and transparency, pages 77–91, 2018.
Aylin Caliskan, Joanna J Bryson, and Arvind Narayanan. Semantics derived automatically from language corpora contain human-like biases. Science, 356(6334):183–186, 2017.
CARDIoGRAMplusC4D Consortium, Panos Deloukas, Stavroula Kanoni, Christina Willenborg, Martin Farrall, Themistocles L Assimes, John R Thompson, Erik Ingelsson, Danish Saleheen, Jeanette Erdmann, Benjamin A Goldstein, Kathleen Stirrups, Inke R König, Jean-Baptiste Cazier, Asa Johansson, Alistair S Hall, Jong-Young Lee, Cristen J Willer, John C Chambers, Tõnu Esko, Lasse Folkersen, Anuj Goel, Elin Grundberg, Aki S Havulinna, Weang K Ho, Jemma C Hopewell, Niclas Eriksson, Marcus E Kleber, Kati Kristiansson, Per Lundmark, Leo-Pekka Lyytikäinen, Suzanne Rafelt, Dmitry Shungin, Rona J Strawbridge, Gudmar Thorleifsson, Emmi Tikkanen, Natalie Van Zuydam, Benjamin F Voight, Lindsay L Waite, Weihua Zhang, Andreas Ziegler, Devin Absher, David Altshuler, Anthony J Balmforth, Inês Barroso, Peter S Braund, Christof Burgdorf, Simone Claudi-Boehm, David Cox, Maria Dimitriou, Ron Do, DIAGRAM Consortium, CARDIOGENICS Consortium, Alex S F Doney, Noureddine El Mokhtari, Per Eriksson, Krista Fischer, Pierre Fontanillas, Anders Franco-Cereceda, Bruna Gigante, Leif Groop, Stefan Gustafsson, Jörg Hager, Göran Hallmans, Bok-Ghee Han, Sarah E Hunt, Hyun M Kang, Thomas Illig, Thorsten Kessler, Joshua W Knowles, Genovefa Kolovou, Johanna Kuusisto, Claudia Langenberg, Cordelia Langford, Karin Leander, Marja-Liisa Lokki, Anders Lundmark, Mark I McCarthy, Christa Meisinger, Olle Melander, Evelin Mihailov, Seraya Maouche, Andrew D Morris, Martina Müller-Nurasyid, MuTHER Consortium, Kjell Nikus, John F Peden, N William Rayner, Asif Rasheed, Silke Rosinger, Diana Rubin, Moritz P Rumpf, Arne Schäfer, Mohan Sivananthan, Ci Song, Alexandre F R Stewart, Sian-Tsung Tan, Gudmundur Thorgeirsson, C Ellen van der Schoot, Peter J Wagner, Wellcome Trust Case Control Consortium, George A Wells, Philipp S Wild, Tsun-Po Yang, Philippe Amouyel, Dominique Arveiler, Hanneke Basart, Michael Boehnke, Eric Boerwinkle, Paolo Brambilla, Francois Cambien, Adrienne L Cupples, Ulf de Faire, Abbas Dehghan, Patrick Diemert, Stephen E Epstein, Alun Evans, Marco M Ferrario, Jean Ferrières, Dominique Gauguier, Alan S Go, Alison H Goodall, Villi Gudnason, Stanley L Hazen, Hilma Holm, Carlos Iribarren, Yangsoo Jang, Mika Kähönen, Frank Kee, Hyo-Soo Kim, Norman Klopp, Wolfgang Koenig, Wolfgang Kratzer, Kari Kuulasmaa, Markku Laakso, Reijo Laaksonen, JiYoung Lee, Lars Lind, Willem H Ouwehand, Sarah Parish, Jeong E Park, Nancy L Pedersen, Annette Peters, Thomas Quertermous, Daniel J Rader, Veikko Salomaa, Eric Schadt, Svati H Shah, Juha Sinisalo, Klaus Stark, Kari Stefansson, David-Alexandre Trégouët, Jarmo Virtamo, Lars Wallentin, Nicholas Wareham, Martina E Zimmermann, Markku S Nieminen, Christian Hengstenberg, Manjinder S Sandhu, Tomi Pastinen, Ann-Christine Syvänen, G Kees Hovingh, George Dedoussis, Paul W Franks, Terho Lehtimäki, Andres Metspalu, Pierre A Zalloua, Agneta Siegbahn, Stefan Schreiber, Samuli Ripatti, Stefan S Blankenberg, Markus Perola, Robert Clarke, Bernhard O Boehm, Christopher O’Donnell, Muredach P Reilly, Winfried März, Rory Collins, Sekar Kathiresan, Anders Hamsten, Jaspal S Kooner, Unnur Thorsteinsdottir, John Danesh, Colin N A Palmer, Robert Roberts, Hugh Watkins, Heribert Schunkert, and Nilesh J Samani. Large-scale association analysis identiﬁes new risk loci for coronary artery disease. Nat. Genet., 45(1):25–33, January 2013.
Rich Caruana, Yin Lou, Johannes Gehrke, Paul Koch, Marc Sturm, and Noemie Elhadad. Intelligible Models for HealthCare. In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining - KDD ’15, pages 1721–1730, 2015. ISBN 9781450336642.
33

A. D’Amour, K. Heller, D. Moldovan, et al
doi: 10.1145/2783258.2788613. URL http://dx.doi.org/10.1145/2783258.2788613http://dl. acm.org/citation.cfm?doid=2783258.2788613.
Daniel Cer, Mona Diab, Eneko Agirre, Iñigo Lopez-Gazpio, and Lucia Specia. SemEval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation. In Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017), pages 1–14, Vancouver, Canada, August 2017. Association for Computational Linguistics. doi: 10.18653/v1/S17-2001. URL https://www.aclweb.org/anthology/S17-2001.
Pratik Chaudhari, Anna Choromanska, Stefano Soatto, Yann LeCun, Carlo Baldassi, Christian Borgs, Jennifer Chayes, Levent Sagun, and Riccardo Zecchina. Entropy-sgd: Biasing gradient descent into wide valleys. Journal of Statistical Mechanics: Theory and Experiment, 2019(12):124018, 2019.
Lenaic Chizat, Edouard Oyallon, and Francis Bach. On lazy training in diﬀerentiable programming. 2019.
Edward Choi, Mohammad Taha Bahadori, Le Song, Walter F Stewart, and Jimeng Sun. GRAM: Graph-based Attention Model for Healthcare Representation Learning. 2017. doi: 10.1145/3097983. 3098126. URL http://dx.doi.org/10.1145/3097983.3098126.
Gary S Collins, Johannes B Reitsma, Douglas G Altman, and Karel GM Moons. Transparent reporting of a multivariable prediction model for individual prognosis or diagnosis (tripod): the tripod statement. British Journal of Surgery, 102(3):148–158, 2015.
Jasmine Collins, Jascha Sohl-Dickstein, and David Sussillo. Capacity and trainability in recurrent neural networks. In 5th International Conference on Learning Representations, ICLR 2017 Conference Track Proceedings, 2017.
Maria De-Arteaga, Alexey Romanov, Hanna Wallach, Jennifer Chayes, Christian Borgs, Alexandra Chouldechova, Sahin Geyik, Krishnaram Kenthapadi, and Adam Tauman Kalai. Bias in bios: A case study of semantic representation bias in a high-stakes setting. In Proceedings of the Conference on Fairness, Accountability, and Transparency, pages 120–128, 2019.
J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A Large-Scale Hierarchical Image Database. In CVPR09, 2009.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171–4186, 2019.
Josip Djolonga, Jessica Yung, Michael Tschannen, Rob Romijnders, Lucas Beyer, Alexander Kolesnikov, Joan Puigcerver, Matthias Minderer, Alexander D’Amour, Dan Moldovan, et al. On robustness and transferability of convolutional neural networks. arXiv preprint arXiv:2007.08558, 2020.
Jesse Dodge, Gabriel Ilharco, Roy Schwartz, Ali Farhadi, Hannaneh Hajishirzi, and Noah Smith. Fine-tuning pretrained language models: Weight initializations, data orders, and early stopping. arXiv preprint arXiv:2002.06305, 2020.
L Duncan, H Shen, B Gelaye, J Meijsen, K Ressler, M Feldman, R Peterson, and B Domingue. Analysis of polygenic risk score usage and performance in diverse human populations. Nat. Commun., 10(1):3328, July 2019.
34

Underspecification in Machine Learning
Michael W Dusenberry, Dustin Tran, Edward Choi, Jonas Kemp, Jeremy Nixon, Ghassen Jerfel, Katherine Heller, and Andrew M Dai. Analyzing the role of model uncertainty for electronic health records. In Proceedings of the ACM Conference on Health, Inference, and Learning, pages 204–213, 2020.
Andre Esteva, Brett Kuprel, Roberto A Novoa, Justin Ko, Susan M Swetter, Helen M Blau, and Sebastian Thrun. Dermatologist-level classiﬁcation of skin cancer with deep neural networks. Nature, 542(7639):115–118, 2017.
Chenchen Feng, David Le, and Allison B. McCoy. Using Electronic Health Records to Identify Adverse Drug Events in Ambulatory Care: A Systematic Review. Applied Clinical Informatics, 10 (1):123–128, 2019. ISSN 18690327. doi: 10.1055/s-0039-1677738.
Aaron Fisher, Cynthia Rudin, and Francesca Dominici. All models are wrong, but many are useful: Learning a variable’s importance by studying an entire class of prediction models simultaneously. Journal of Machine Learning Research, 20(177):1–81, 2019.
TB Fitzpatrick. Sun and skin. Journal de Medecine Esthetique, 2:33–34, 1975.
Seth Flaxman, Swapnil Mishra, Axel Gandy, H Juliette T Unwin, Thomas A Mellan, Helen Coupland, Charles Whittaker, Harrison Zhu, Tresnia Berah, Jeﬀrey W Eaton, et al. Estimating the eﬀects of non-pharmaceutical interventions on covid-19 in europe. Nature, 584(7820):257–261, 2020.
Stanislav Fort, Huiyi Hu, and Balaji Lakshminarayanan. Deep ensembles: A loss landscape perspective. arXiv preprint arXiv:1912.02757, 2019.
Jonathan Frankle, Gintare Karolina Dziugaite, Daniel M Roy, and Michael Carbin. Linear mode connectivity and the lottery ticket hypothesis. In Proceedings of the 37th International Conference on Machine Learning, 2020.
Joseph Futoma, Morgan Simons, Trishan Panch, Finale Doshi-Velez, and Leo Anthony Celi. The myth of generalisability in clinical research and machine learning in health care. The Lancet Digital Health, 2(9):e489 – e492, 2020. ISSN 2589-7500. doi: https://doi.org/10.1016/S2589-7500(20)30186-2. URL http://www.sciencedirect.com/science/article/pii/S2589750020301862.
Sahaj Garg, Vincent Perot, Nicole Limtiaco, Ankur Taly, Ed H Chi, and Alex Beutel. Counterfactual fairness in text classiﬁcation through robustness. In Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society, pages 219–226, 2019.
Timur Garipov, Pavel Izmailov, Dmitrii Podoprikhin, Dmitry P Vetrov, and Andrew G Wilson. Loss surfaces, mode connectivity, and fast ensembling of dnns. In Advances in Neural Information Processing Systems, pages 8789–8798, 2018.
Robert Geirhos, Patricia Rubisch, Claudio Michaelis, Matthias Bethge, Felix A. Wichmann, and Wieland Brendel. Imagenet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=Bygh9j09KX.
Robert Geirhos, Jörn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel, Wieland Brendel, Matthias Bethge, and Felix A Wichmann. Shortcut learning in deep neural networks. arXiv preprint arXiv:2004.07780, 2020.
Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. Deep learning, volume 1. MIT press Cambridge, 2016.
35

A. D’Amour, K. Heller, D. Moldovan, et al
Varun Gulshan, Lily Peng, Marc Coram, Martin C Stumpe, Derek Wu, Arunachalam Narayanaswamy, Subhashini Venugopalan, Kasumi Widner, Tom Madams, Jorge Cuadros, et al. Development and validation of a deep learning algorithm for detection of diabetic retinopathy in retinal fundus photographs. Jama, 316(22):2402–2410, 2016.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770–778, 2016.
Christina Heinze-Deml, Jonas Peters, and Nicolai Meinshausen. Invariant causal prediction for nonlinear models. Journal of Causal Inference, 6(2), 2018.
Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=HJz6tiCqYm.
Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. Natural adversarial examples. arXiv preprint arXiv:1907.07174, 2019.
Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, et al. The many faces of robustness: A critical analysis of out-of-distribution generalization. arXiv preprint arXiv:2006.16241, 2020.
Sepp Hochreiter and Jürgen Schmidhuber. Long Short-Term Memory. Neural Computation, 9(8):1735– 1780, 1997. URL http://www7.informatik.tu-muenchen.de/{~}hochreithttp://www.idsia. ch/{~}juergen.
Wolfgang Hoﬀmann, Ute Latza, Sebastian E Baumeister, Martin Brünger, Nina Buttmann-Schweiger, Juliane Hardt, Verena Hoﬀmann, André Karch, Adrian Richter, Carsten Oliver Schmidt, et al. Guidelines and recommendations for ensuring good epidemiological practice (gep): a guideline developed by the german society for epidemiology. European journal of epidemiology, 34(3):301–317, 2019.
Sara Hooker. The hardware lottery. arXiv preprint arXiv:2009.06489, 2020.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance Ramshaw, and Ralph Weischedel. OntoNotes: The 90% solution. In Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume: Short Papers, pages 57–60, New York City, USA, June 2006. Association for Computational Linguistics. URL https://www.aclweb.org/anthology/N06-2015.
Jeremy Howard and Sebastian Ruder. Universal language model ﬁne-tuning for text classiﬁcation. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 328–339, 2018.
Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Logan Engstrom, Brandon Tran, and Aleksander Madry. Adversarial examples are not bugs, they are features. In Advances in Neural Information Processing Systems, pages 125–136, 2019.
International Schizophrenia Consortium, Shaun M Purcell, Naomi R Wray, Jennifer L Stone, Peter M Visscher, Michael C O’Donovan, Patrick F Sullivan, and Pamela Sklar. Common polygenic variation contributes to risk of schizophrenia and bipolar disorder. Nature, 460(7256):748–752, August 2009.
Pavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry Vetrov, and Andrew Gordon Wilson. Averaging weights leads to wider optima and better generalization. arXiv preprint arXiv:1803.05407, 2018.
36

Underspecification in Machine Learning
Alon Jacovi, Ana Marasović, Tim Miller, and Yoav Goldberg. Formalizing trust in artiﬁcial intelligence: Prerequisites, causes and goals of human trust in ai. arXiv preprint arXiv:2010.07487, 2020.
Divyansh Kaushik, Eduard Hovy, and Zachary Lipton. Learning the diﬀerence that makes a diﬀerence with counterfactually-augmented data. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=Sklgs0NFvr.
John A Kellum and Azra Bihorac. Artiﬁcial intelligence to predict aki: is it a breakthrough? Nature Reviews Nephrology, pages 1–2, 2019.
Christopher J Kelly, Alan Karthikesalingam, Mustafa Suleyman, Greg Corrado, and Dominic King. Key challenges for delivering clinical impact with artiﬁcial intelligence. BMC medicine, 17(1):195, 2019.
Amit V Khera, Mark Chaﬃn, Krishna G Aragam, Mary E Haas, Carolina Roselli, Seung Hoan Choi, Pradeep Natarajan, Eric S Lander, Steven A Lubitz, Patrick T Ellinor, and Sekar Kathiresan. Genome-wide polygenic scores for common diseases identify individuals with risk equivalent to monogenic mutations. Nat. Genet., 50(9):1219–1224, September 2018.
Arif Khwaja. KDIGO clinical practice guidelines for acute kidney injury. Nephron - Clinical Practice, 120(4), oct 2012. ISSN 16602110. doi: 10.1159/000339789.
Jon Kleinberg, Jens Ludwig, Sendhil Mullainathan, and Ziad Obermeyer. Prediction policy problems. American Economic Review, 105(5):491–95, 2015.
Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly, and Neil Houlsby. Large scale learning of general visual representations for transfer. arXiv preprint arXiv:1912.11370, 2019.
Jonathan Krause, Varun Gulshan, Ehsan Rahimy, Peter Karth, Kasumi Widner, Greg S Corrado, Lily Peng, and Dale R Webster. Grader variability and the importance of reference standards for evaluating machine learning models for diabetic retinopathy. Ophthalmology, 125(8):1264–1272, 2018.
Matt J Kusner, Joshua Loftus, Chris Russell, and Ricardo Silva. Counterfactual fairness. In Advances in neural information processing systems, pages 4066–4076, 2017.
Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive uncertainty estimation using deep ensembles. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems 30, pages 6402–6413. Curran Associates, Inc., 2017. URL http://papers.nips.cc/paper/ 7219-simple-and-scalable-predictive-uncertainty-estimation-using-deep-ensembles. pdf.
Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. Albert: A lite bert for self-supervised learning of language representations. arXiv preprint arXiv:1909.11942, 2019.
Olivier Ledoit and Sandrine Péché. Eigenvectors of some large sample covariance matrix ensembles. Probability Theory and Related Fields, 151(1-2):233–264, 2011.
Tao Lei, Yu Zhang, Sida I. Wang, Hui Dai, and Yoav Artzi. Simple recurrent units for highly parallelizable recurrence. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, EMzhou 2018, pages 4470–4481. Association for Computational Linguistics, sep 2018. ISBN 9781948087841. doi: 10.18653/v1/d18-1477. URL http://arxiv.org/abs/1709. 02755.
37

A. D’Amour, K. Heller, D. Moldovan, et al
Tal Linzen. How can we accelerate progress towards human-like linguistic generalization? In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5210–5217, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020. acl-main.465. URL https://www.aclweb.org/anthology/2020.acl-main.465.
Xiaoxuan Liu, Samantha Cruz Rivera, David Moher, Melanie J Calvert, and Alastair K Denniston. Reporting guidelines for clinical trial reports for interventions involving artiﬁcial intelligence: the consort-ai extension. bmj, 370, 2020a.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019.
Yuan Liu, Ayush Jain, Clara Eng, David H Way, Kang Lee, Peggy Bui, Kimberly Kanada, Guilherme de Oliveira Marinho, Jessica Gallegos, Sara Gabriele, et al. A deep learning system for diﬀerential diagnosis of skin diseases. Nature Medicine, pages 1–9, 2020b.
Sara Magliacane, Thijs van Ommen, Tom Claassen, Stephan Bongers, Philip Versteeg, and Joris M Mooij. Domain adaptation by using causal inference to predict invariant conditional distributions. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems 31 (NeurIPS2018), pages 10869–10879. Curran Associates, Inc., 2018. URL http://papers.nips.cc/paper/ 8282-domain-adaptation-by-using-causal-inference-to-predict-invariant-conditional-distributions. pdf.
Alicia R Martin, Christopher R Gignoux, Raymond K Walters, Genevieve L Wojcik, Benjamin M Neale, Simon Gravel, Mark J Daly, Carlos D Bustamante, and Eimear E Kenny. Human demographic history impacts genetic risk prediction across diverse populations. Am. J. Hum. Genet., 100(4):635–649, April 2017.
Alicia R Martin, Masahiro Kanai, Yoichiro Kamatani, Yukinori Okada, Benjamin M Neale, and Mark J Daly. Clinical use of current polygenic risk scores may exacerbate health disparities. Nat. Genet., 51(4):584–591, April 2019.
Charles T Marx, Flavio du Pin Calmon, and Berk Ustun. Predictive multiplicity in classiﬁcation. arXiv preprint arXiv:1909.06677, 2019.
R Thomas McCoy, Junghyun Min, and Tal Linzen. Berts of a feather do not generalize together: Large variability in generalization across models with similar test set performance. arXiv preprint arXiv:1911.02969, 2019a.
R Thomas McCoy, Ellie Pavlick, and Tal Linzen. Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference. arXiv preprint arXiv:1902.01007, 2019b.
Song Mei and Andrea Montanari. The generalization error of random features regression: Precise asymptotics and double descent curve. arXiv:1908.05355, 2019.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeﬀrey Dean. Eﬃcient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781, 2013.
Joannella Morales, Danielle Welter, Emily H Bowler, Maria Cerezo, Laura W Harris, Aoife C McMahon, Peggy Hall, Heather A Junkins, Annalisa Milano, Emma Hastings, Cinzia Malangone, Annalisa Buniello, Tony Burdett, Paul Flicek, Helen Parkinson, Fiona Cunningham, Lucia A Hindorﬀ, and Jacqueline A L MacArthur. A standardized framework for representation of ancestry data in genomics studies, with application to the NHGRI-EBI GWAS catalog. Genome Biol., 19 (1):21, February 2018.
38

Underspecification in Machine Learning
Sendhil Mullainathan and Jann Spiess. Machine learning: an applied econometric approach. Journal of Economic Perspectives, 31(2):87–106, 2017.
Moin Nadeem, Anna Bethke, and Siva Reddy. Stereoset: Measuring stereotypical bias in pretrained language models. arXiv preprint arXiv:2004.09456, 2020.
Aakanksha Naik, Abhilasha Ravichander, Norman Sadeh, Carolyn Rose, and Graham Neubig. Stress test evaluation for natural language inference. arXiv preprint arXiv:1806.00692, 2018.
Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya Sutskever. Deep double descent: Where bigger models and more data hurt. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=B1g5sA4twr.
National Institute for Health and Care Excellence (NICE). Acute kidney injury: prevention, detection and management. NICE Guideline NG148, 2019.
Radford M Neal. Priors for inﬁnite networks. In Bayesian Learning for Neural Networks, pages 29–53. Springer, 1996.
Anna C Need and David B Goldstein. Next generation disparities in human genomics: concerns and remedies. Trends Genet., 25(11):489–494, November 2009.
Bret Nestor, Matthew B. A. McDermott, Willie Boag, Gabriela Berner, Tristan Naumann, Michael C Hughes, Anna Goldenberg, and Marzyeh Ghassemi. Feature Robustness in Non-stationary Health Records: Caveats to Deployable Model Performance in Common Clinical Machine Learning Tasks. Proceedings of Machine Learning Research, 106:1–23, 2019. URL https://mimic.physionet.org/ mimicdata/carevue/http://arxiv.org/abs/1908.00690.
Luke Oakden-Rayner, Jared Dunnmon, Gustavo Carneiro, and Christopher Ré. Hidden stratiﬁcation causes clinically meaningful failures in machine learning for medical imaging. In Proceedings of the ACM Conference on Health, Inference, and Learning, pages 151–159, 2020.
Ziad Obermeyer, Brian Powers, Christine Vogeli, and Sendhil Mullainathan. Dissecting racial bias in an algorithm used to manage the health of populations. Science, 366(6464):447–453, oct 2019. ISSN 10959203. doi: 10.1126/science.aax2342.
Cecilia Panigutti, Alan Perotti, and Dino Pedreschi. Doctor XAI An ontology-based approach to blackbox sequential data classiﬁcation explanations. In FAT* 2020 - Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency, pages 629–639, 2020. ISBN 9781450369367. doi: 10.1145/3351095.3372855. URL https://doi.org/10.1145/3351095.3372855.
Jonas Peters, Peter Bühlmann, and Nicolai Meinshausen. Causal inference by using invariant prediction: identiﬁcation and conﬁdence intervals. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 78(5):947–1012, 2016. doi: 10.1111/rssb.12167. URL https://rss. onlinelibrary.wiley.com/doi/abs/10.1111/rssb.12167.
Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. Deep contextualized word representations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 2227–2237, 2018.
Alice B Popejoy and Stephanie M Fullerton. Genomics is failing on diversity. Nature, 538(7624): 161–164, October 2016.
Mihail Popescu and Mohammad Khalilia. Improving disease prediction using ICD-9 ontological features. In IEEE International Conference on Fuzzy Systems, pages 1805–1809, 2011. ISBN 9781424473175. doi: 10.1109/FUZZY.2011.6007410.
39

A. D’Amour, K. Heller, D. Moldovan, et al
Alkes L Price, Nick J Patterson, Robert M Plenge, Michael E Weinblatt, Nancy A Shadick, and David Reich. Principal components analysis corrects for stratiﬁcation in genome-wide association studies. Nat. Genet., 38(8):904–909, August 2006.
Shaun Purcell, Benjamin Neale, Kathe Todd-Brown, Lori Thomas, Manuel A R Ferreira, David Bender, Julian Maller, Pamela Sklar, Paul I W de Bakker, Mark J Daly, and Pak C Sham. PLINK: a tool set for whole-genome association and population-based linkage analyses. Am. J. Hum. Genet., 81(3):559–575, September 2007.
Aditi Raghunathan, Sang Michael Xie, Fanny Yang, John Duchi, and Percy Liang. Understanding and mitigating the tradeoﬀ between robustness and accuracy. arXiv preprint arXiv:2002.10716, 2020.
Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In Advances in neural information processing systems, pages 1177–1184, 2008.
Marco Tulio Ribeiro, Tongshuang Wu, Carlos Guestrin, and Sameer Singh. Beyond accuracy: Behavioral testing of NLP models with CheckList. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4902–4912, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.442. URL https://www.aclweb. org/anthology/2020.acl-main.442.
Samantha Cruz Rivera, Xiaoxuan Liu, An-Wen Chan, Alastair K Denniston, and Melanie J Calvert. Guidelines for clinical trial protocols for interventions involving artiﬁcial intelligence: the spirit-ai extension. bmj, 370, 2020.
Andrew Slavin Ross, Michael C Hughes, and Finale Doshi-Velez. Right for the right reasons: training diﬀerentiable models by constraining their explanations. In Proceedings of the 26th International Joint Conference on Artiﬁcial Intelligence, pages 2662–2670. AAAI Press, 2017.
Rachel Rudinger, Jason Naradowsky, Brian Leonard, and Benjamin Van Durme. Gender bias in coreference resolution. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 8–14, 2018.
Bernhard Schölkopf. Causality for machine learning. arXiv preprint arXiv:1911.10500, 2019.
Lesia Semenova, Cynthia Rudin, and Ronald Parr. A study in rashomon curves and volumes: A new perspective on generalization and model simplicity in machine learning. arXiv preprint arXiv:1908.01755, 2019.
Montgomery Slatkin. Linkage disequilibrium — understanding the evolutionary past and mapping the medical future. Nature Reviews Genetics, 9:477–485, 2008.
Jasper Snoek, Yaniv Ovadia, Emily Fertig, Balaji Lakshminarayanan, Sebastian Nowozin, D Sculley, Joshua Dillon, Jie Ren, and Zachary Nado. Can you trust your model’s uncertainty? evaluating predictive uncertainty under dataset shift. In Advances in Neural Information Processing Systems, pages 13969–13980, 2019.
Cathie Sudlow, John Gallacher, Naomi Allen, Valerie Beral, Paul Burton, John Danesh, Paul Downey, Paul Elliott, Jane Green, Martin Landray, Bette Liu, Paul Matthews, Giok Ong, Jill Pell, Alan Silman, Alan Young, Tim Sprosen, Tim Peakman, and Rory Collins. UK biobank: an open access resource for identifying the causes of a wide range of complex diseases of middle and old age. PLoS Med., 12(3):e1001779, March 2015.
40

Underspecification in Machine Learning
Chen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhinav Gupta. Revisiting unreasonable eﬀectiveness of data in deep learning era. In Proceedings of the IEEE international conference on computer vision, pages 843–852, 2017.
Christian Szegedy, Sergey Ioﬀe, Vincent Vanhoucke, and Alexander A Alemi. Inception-v4, inceptionresnet and the impact of residual connections on learning. In Thirty-ﬁrst AAAI conference on artiﬁcial intelligence, 2017.
Rohan Taori, Achal Dave, Vaishaal Shankar, Nicholas Carlini, Benjamin Recht, and Ludwig Schmidt. Measuring robustness to natural distribution shifts in image classiﬁcation. arXiv preprint arXiv:2007.00644, 2020.
Daniel Shu Wei Ting, Carol Yim-Lui Cheung, Gilbert Lim, Gavin Siew Wei Tan, Nguyen D Quang, Alfred Gan, Haslina Hamzah, Renata Garcia-Franco, Ian Yew San Yeo, Shu Yen Lee, et al. Development and validation of a deep learning system for diabetic retinopathy and related eye diseases using retinal images from multiethnic populations with diabetes. Jama, 318(22):2211–2223, 2017.
Nenad Tomašev, Xavier Glorot, Jack W Rae, Michal Zielinski, Harry Askham, Andre Saraiva, Anne Mottram, Clemens Meyer, Suman Ravuri, Ivan Protsyuk, Alistair Connell, Cían O Hughes, Alan Karthikesalingam, Julien Cornebise, Hugh Montgomery, Geraint Rees, Chris Laing, Clifton R Baker, Kelly Peterson, Ruth Reeves, Demis Hassabis, Dominic King, Mustafa Suleyman, Trevor Back, Christopher Nielson, Joseph R Ledsam, and Shakir Mohamed. A clinically applicable approach to continuous prediction of future acute kidney injury. Nature, 572(7767):116–119, aug 2019a. ISSN 0028-0836. doi: 10.1038/s41586-019-1390-1.
Nenad Tomašev, Xavier Glorot, Jack W. Rae, Michal Zielinski, Harry Askham, Andre Saraiva, Anne Mottram, Clemens Meyer, Suman Ravuri, Ivan Protsyuk, Alistair Connell, Cian O. Hugues, Alan Kathikesalingam, Julien Cornebise, Hugh Montgomery, Geraint Rees, Chris Laing, Clifton R. Baker, Kelly Peterson, Ruth Reeves, Demis Hassabis, Dominic King, Mustafa Suleyman, Trevor Back, Christopher Nielson, Joseph R. Ledsam, and Shakir Mohamed. Developing Deep Learning Continuous Risk Models for Early Adverse Event Prediction in Electronic Health Records: an AKI Case Study. PROTOCOL available at Protocol Exchange, version 1, jul 2019b. doi: 10.21203/RS. 2.10083/V1.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems, pages 5998–6008, 2017.
Bjarni J Vilhjálmsson, Jian Yang, Hilary K Finucane, Alexander Gusev, Sara Lindström, Stephan Ripke, Giulio Genovese, Po-Ru Loh, Gaurav Bhatia, Ron Do, Tristan Hayeck, Hong-Hee Won, Schizophrenia Working Group of the Psychiatric Genomics Consortium, Discovery, Biology, and Risk of Inherited Variants in Breast Cancer (DRIVE) study, Sekar Kathiresan, Michele Pato, Carlos Pato, Rulla Tamimi, Eli Stahl, Noah Zaitlen, Bogdan Pasaniuc, Gillian Belbin, Eimear E Kenny, Mikkel H Schierup, Philip De Jager, Nikolaos A Patsopoulos, Steve McCarroll, Mark Daly, Shaun Purcell, Daniel Chasman, Benjamin Neale, Michael Goddard, Peter M Visscher, Peter Kraft, Nick Patterson, and Alkes L Price. Modeling linkage disequilibrium increases accuracy of polygenic risk scores. Am. J. Hum. Genet., 97(4):576–592, October 2015.
Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. Glue: A multi-task benchmark and analysis platform for natural language understanding. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 353–355, 2018.
41

A. D’Amour, K. Heller, D. Moldovan, et al
Haohan Wang, Songwei Ge, Zachary Lipton, and Eric P Xing. Learning robust global representations by penalizing local predictive power. In Advances in Neural Information Processing Systems, pages 10506–10518, 2019.
Haohan Wang, Xindi Wu, Zeyi Huang, and Eric P Xing. High-frequency component helps explain the generalization of convolutional neural networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8684–8694, 2020.
Kellie Webster, Xuezhi Wang, Ian Tenney, Alex Beutel, Emily Pitler, Ellie Pavlick, Jilin Chen, and Slav Petrov. Measuring and reducing gendered correlations in pre-trained models. arXiv preprint arXiv:2010.06032, 2020.
Florian Wenzel, Jasper Snoek, Dustin Tran, and Rodolphe Jenatton. Hyperparameter ensembles for robustness and uncertainty quantiﬁcation. arXiv preprint arXiv:2006.13570, 2020.
Adina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage challenge corpus for sentence understanding through inference. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 1112–1122, 2018.
Andrew Gordon Wilson and Pavel Izmailov. Bayesian deep learning and a probabilistic perspective of generalization. arXiv preprint arXiv:2002.08791, 2020.
Julia K Winkler, Christine Fink, Ferdinand Toberer, Alexander Enk, Teresa Deinlein, Rainer HofmannWellenhof, Luc Thomas, Aimilios Lallas, Andreas Blum, Wilhelm Stolz, et al. Association between surgical skin markings in dermoscopic images and diagnostic performance of a deep learning convolutional neural network for melanoma recognition. JAMA dermatology, 155(10):1135–1141, 2019.
Naomi R Wray, Michael E Goddard, and Peter M Visscher. Prediction of individual genetic risk to disease from genome-wide association studies. Genome Res., 17(10):1520–1528, October 2007.
Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le. Xlnet: Generalized autoregressive pretraining for language understanding. In Advances in neural information processing systems, pages 5753–5763, 2019.
Dong Yin, Raphael Gontijo Lopes, Jon Shlens, Ekin Dogus Cubuk, and Justin Gilmer. A fourier perspective on model robustness in computer vision. In Advances in Neural Information Processing Systems, pages 13255–13265, 2019.
Bin Yu et al. Stability. Bernoulli, 19(4):1484–1500, 2013.
Rowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin Choi. Swag: A large-scale adversarial dataset for grounded commonsense inference. arXiv preprint arXiv:1808.05326, 2018.
Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine really ﬁnish your sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4791–4800, 2019.
Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Ordonez, and Kai-Wei Chang. Gender bias in coreference resolution: Evaluation and debiasing methods. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 15–20, 2018.
Xiang Zhou, Yixin Nie, Hao Tan, and Mohit Bansal. The curse of performance instability in analysis datasets: Consequences, source, and suggestions. arXiv preprint arXiv:2004.13606, 2020.
42

Underspecification in Machine Learning
Appendix A. Computer Vision: Marginaliztaion versus Model Selection
Figure 16: Comparison of performance of the “best” ensemble member from an ensemble of 50 ResNet-50 predictor (dashed) against the average performance from averaging the predictions of diﬀerently-sized subsets of the ensemble (solid). ImageNet Test is an iid evaluation; the other three panels show stress tests. See main text for full description. Iid performance on ImageNet improves as ensemble size increases, and this is associated with correlated improvements in stress tests. The larger the variability in stress test performance within the ensemble, the larger the ensemble needs to be to out-perform the best single ensemble member. In some cases, the ensemble average never out-performs the best single model.
In the discussion in the main text, we argue that marginalization may not be the best response to underspeciﬁcation when the goal is to obtain predictors that encode the “right” structure for a given application. We suggest instead that model selection may be a more fruitful approaach here. This is because, by the nature of underspeciﬁcation, some predictors returned by the pipeline will exhibit worse behavior in deployment domains than others, so averaging them together does not guarantee that the ensemble average will out-perform the best member. Notably, this represents a departure from the argument made in favor of marginalization for improving iid performance: in the training domain, all of the models in the near-optimal equivalence class F∗ (recall this deﬁnition from Section 2 of the main text) contain a “right” answer for iid generalization, so one would expect that averaging them could only lead to improvements.
In this section, we provide some empirical support for this argument. Broadly, there is an interplay between iid performance and performance on stress tests that can make marginalization beneﬁcial, but when there is large variability in stress test performance across an ensemble, selecting the best single model can out-perform large ensemble averages.
In Figure 16, we show a comparison between performance of individual ensemble members and ensemble averages on several test sets. We calculate these metrics with respect to the ensemble of 50 ResNet-50 models used to produce the result in the main text. The dashed line shows the performance of the best model from this ensemble, while the solid line shows the average performance from marginalizing across diﬀerently-sized subsets of models in this ensemble. The ImageNet test set is the iid evaluation, while the other test sets are from the ImageNet-C and ObjectNet benchmarks. As expected, performance on the ImageNet test set improves substantially as more ensemble members are averaged together. This translates to correlated performance improvements on stress test benchmarks, which is a well-known phenomenon in the image robustness literature (see, e.g. Taori et al., 2020; Djolonga et al., 2020). Interestingly, however, it takes marginalizing across a larger subset of models to surpass the performance of the best predictor on stress tests compared to the iid evaluation. In particular, the higher the variance of performance across the ensemble, the more predictors need to be averaged to beat the surpass single model. In the case of the pixealate task, the full ensemble of 50 models is never able to surpass the best single model.
43

A. D’Amour, K. Heller, D. Moldovan, et al

Appendix B. Natural Language Processing: Analysis of Static Embeddings

In the main text, we showed that underspeciﬁcation plays a key role in shortcut learning in BERTbased NLP models. However, highly parameterized models pre-date this approach, and here we provide a supplementary analysis suggesting that underspeciﬁcation is also present in static word embeddings such as word2vec Mikolov et al. (2013). Here, we examine stereotypical associations with respect to demographic attributes like race, gender, and age, which have been studied in the past (Bolukbasi et al., 2016).
We train twenty diﬀerent 500-dimensional word2vec models Mikolov et al. (2013) on large news and wikipedia datasets using the demo-train-big-model-v1.sh script from the canonical word2vec repository,7 varying only the random seeds. These models obtain very consistent performance on a word analogy task, scoring between 76.2% and 76.7%.
As a stress test, we apply the Word Embedding Association Test, which quantiﬁes the extent to which these associations are encoded by a given set of embeddings Caliskan et al. (2017). Speciﬁcally, the WEAT score measures the relative similarity of two sets of target words (e.g., types of ﬂowers, types of insects) to two sets of attribute words (e.g., pleasant words, unpleansant words). Let X and Y be the sets of target words, and A and B the sets of attribute words. The test statistic is then:

s(X, Y, A, B) = s(x, A, B) − s(y, A, B)

x∈X

y∈Y

where

s(w, A, B) = meana∈Acos(w, a) − meanb∈Bcos(w, b)

and cos(a, b) is the cosine distance between two word vectors. This score is then normalized by the standard deviation of s(w, A, B) for all w in X ∪ Y . If the score is closer to zero, the relative similarity diﬀerence (i.e., bias) is considered to be smaller. Caliskan et al. (2017) provide a number of wordsets to probe biases along socially salient axes of gender, race, disability, and age.
For each model and test, we compute statistical signiﬁcance using a permutation test that compares the observed score against the score obtained under a random shuﬄing of target words. As shown in Figure 17, we ﬁnd strong and consistent gender associations, but observe substantial variation on the three tests related to associations with race: in many cases, whether an association is statistically signiﬁcant depends on the random seed. Finally, we note that the particular axes along which we observe sensitivity depends on the dataset used to train the embeddings, and could vary on embeddings trained on other corpora Babaeianjelodar et al. (2020).

Appendix C. Clinical Prediction with EHR: Additional Details and Supplementary Ablation Experiment
This section provides additional details and results for the analysis of the model in Tomašev et al. (2019a) performed in the main text. In particular, we provide some descriptive statistics regarding AKI prevalence, and additional summaries of model performance across diﬀerent time slices and dataset shifts.
C.1 Lab Order Patterns and Time of Day
In the main text, we investigate how reliant predictors can be on signals related to the timing and composition of lab tests. Here, we show some descriptive statistics for how these tests tend to be distributed in time, and some patterns that emerge as a result.
7. Canonical codebase is https://code.google.com/archive/p/word2vec/; a GitHub export of this repository is available at https://github.com/tmikolov/word2vec.

44

Underspecification in Machine Learning

Figure 17: Static word embeddings also show evidence of stereotype-aligned underspeciﬁcation. Word Embedding Association Test (WEAT) scores across twenty word2vec models. Each group corresponds to a speciﬁc association test, and each bar corresponds to the score on the test for a speciﬁc word2vec model.

Table 5 shows patterns of AKI prevalence and creatinine sampling. Even though AKI prevalence is largely constant across time windows, creatinine is sampled far more frequently in between 12am and 6am. Thus, when creatinine samples are taken in other time windows, the prevalence of AKI conditional on that sample being taken is higher.
Figure 18 shows the distributions of number of labs taken at diﬀerent times of day. The distribution of labs in the ﬁrst two time buckets is clearly distinct from the distributions in the second two time buckets.
Table 5: Patterns of creatinine sampling can induce a spurious relationship between time of day and AKI. Prevalence of AKI is stable across times of day in the test set (test set prevalence is 2.269%), but creatinine samples are taken more frequently in the ﬁrst two time buckets. As a result, conditional on a sample being taken, AKI prevalence is higher in the latter two time buckets.

Metric Prevalence of AKI (%) Creatinine samples Creatinine samples (%) Prevalence of AKI (%) in creatinine samples

12am-6am
2.242 332743
3.570 6.236

6am-12pm
2.153 320068
3.433 5.425

12pm-6pm
2.287 89379 0.959 8.824

6pm-12am
2.396 67765 0.727 9.154

C.2 Details of Predictor Performance on Intervened Data
Here, we perform a stratiﬁed evaluation of model performance across diﬀerent time buckets in the standard and intervened test data. This analysis is repeated for each of the 15 models trained, and acoss the Shift and Shift+Labs intevened datasets described in the main text. Table 6 displays model performance on each model instance for the test set as well as for time windows where creatinine samples were taken.

45

A. D’Amour, K. Heller, D. Moldovan, et al
Figure 18: Distribution of number of lab values observed on average across 100000 time steps (random sample in the test set), per time of day. Each violin plot represent a time of day while the y-axis represents the number of lab values observed.
46

Table 6: Model sensitivity per time of day: Normalized PRAUC on the test set, per time of day, and per time of day for creatinine samples for each model instance when the data is not perturbed (‘Test’), when time of day is shifted (‘Shift’) and when time of day is shifted and only CHEM-7 labs are considered for creatinine samples (‘Shift+Labs’). ‘Diﬀ.’ refers to the maximum diﬀerence in value between instances.

Underspecification in Machine Learning 47

cell seed Test 12am - 6am 6am - 12pm 12pm - 6pm 6pm - 12am Creatinine samples 12am - 6am 6am - 12pm 12pm - 6pm 6pm - 12am Shift Population Test 12am - 6am 6am - 12pm 12pm - 6pm 6pm - 12am Creatinine samples 12am - 6am 6am - 12pm 12pm - 6pm 6pm - 12am Shift+Labs Population Test 12am - 6am 6am - 12pm 12pm - 6pm 6pm - 12am Creatinine samples 12am - 6am 6am - 12pm 12pm - 6pm 6pm - 12am

1
34.69 34.26 36.51 37.22
36.51 34.4
42.68 41.81
34.93
33.75 33.55 35.72 36.49
36.15 33.72 41.81
41
33.66
32.5 32.22 34.45 35.27
34.32 32.58 40.05 38.91

2
34.42 34
36.26 37
36.32 34.53
42.5 41.86
34.9
33.63 33.54 35.78 36.45
36.1 33.97 41.98 41.27
33.86
32.62 32.44 34.71 35.43
34.55 33.03 40.37 39.43

SRU 3
34.64 34.14 36.43 37.17
36.66 34.24 42.68 41.72
34.74
33.67 33.32 35.57 36.28
36.22 33.64 41.99
41.1
33.46
32.44 32.09 34.24 34.96
34.58 32.65
40.1 38.87

4
34.99 34.32 36.67 37.41
37.06 34.75 42.34 42.17
35.08
33.79 33.74 35.94 36.71
36.66 34.06 41.68 41.49
33.99
32.74 32.55 34.85 35.68
35.03 33.1
40.13 39.53

5
34.74 34.26 36.55 37.24
36.57 34.55 42.23 41.91
34.93
33.52 33.62 35.88 36.45
36.12 33.97 41.55 41.13
33.76
32.29 32.43 34.78 35.34
34.43 32.91 39.95 39.18

1
34.61 33.98 36.13 36.97
36.52 34.61 42.17 41.87
35.08
34.06 33.7
35.79 36.52
36.18 34.44 41.73 41.27
34.08
32.94 32.76 34.82 35.57
34.56 33.5
39.97 39.33

UGRNN

2

3

4

34.88 34.39 36.57 37.34

34.46 34.05 36.24 36.93

34.86 34.3
36.57 37.32

36.92 34.85 42.49 42.52

36.31 34.37 41.64 41.05

36.82 34.59 42.44
41.7

35.46 35.11 35.53

34.51 33.89 36.22 36.99

34.05 33.7
35.92 36.54

34.55 34
36.28 37.04

36.72 34.5 42.2
42.11

36.12 34.05 41.03
40.6

36.63 34.39 42.27 41.31

34.77 34.22 34.67

33.79 33.23
35.5 36.33

33.19 32.84
35 35.64

33.72 33.07 35.39 36.26

35.7 33.85 40.91 40.63

35.03 33.37 39.59 39.13

35.45 33.65 40.85 39.68

5
34.62 34.01 36.25 36.99
36.55 34.58 42.25 41.75
35.13
34.09 33.62 35.97 36.61
36.24 34.23 41.95 41.36
34.29
33.18 32.77 35.13 35.86
34.64 33.45 40.35
39.7

1
35.43 34.86 37.05 37.78
37.43 35.42
43 42.58
35.97
35.03 34.44 36.74 37.45
37.2 35.03 42.77 42.11
35.16
34.25 33.68 35.89 36.62
36.05 34.29
41.1 40.42

LSTM

2

3

4

35.55 35.06 37.28 38.03

35.42 34.91 37.07 37.82

35.63 35.12 37.35 38.07

37.64 35.52
43.2 42.79

37.5 35.28 42.93 42.86

37.57 35.48 43.77 42.84

36.16 35.93 36.21

35.11 34.62 36.97 37.68

34.82 34.51 36.71 37.41

35.16 34.73 37.03 37.69

37.43 35.14 42.87 42.33

37.12 34.97 42.67 42.43

37.41 35.15 43.51 42.51

35.18 34.93 35.21

34.05 33.72 35.98 36.69

33.81 33.52 35.69 36.42

34.2 33.73 36.01 36.66

35.99 34.26 40.85 40.23

35.69 34.03 40.83 40.35

36.08 34.16 41.81 40.79

5
35.51 34.94 37.11 37.89
37.48 35.41 43.38 43.19
35.82
34.85 34.3
36.56 37.32
37.2 34.97 42.95
42.7
34.76
33.72 33.34 35.49 36.27
35.73 34.28 41.03 40.68

Diﬀ.
1.21 1.14 1.22 1.14
1.33 1.27 2.13 2.14
1.47
1.64 1.4
1.45 1.41
1.33 1.51 2.48
2.1
1.74
1.96 1.64 1.77 1.73
1.76 1.71 2.23 1.92

A. D’Amour, K. Heller, D. Moldovan, et al
When perturbing the correlation between AKI label, time of day and number of labs on a per patient basis, we observe a decrease in model performance, as well as a widening of the performance bounds (reported in the main text). This widening of performance bounds displays a diﬀerential eﬀect of the shifts on the diﬀerent models of the ensemble, both on the individual patient timepoints risk (Figures 19, 20, 21) and on the model’s decisions (Table 7).

Figure 19: Variability in AKI risk predictions from ensemble of RNN models processing

electronic health records (EHR). Histograms showing showing risk predictions from two models,

and changes induced by time of day and lab perturbations. Histograms show counts of patient-

timepoints where creatinine measurements were taken in the morning (6am-12pm). LSTM 1 and 5

diﬀer only in random seed. “Test” shows histogram of risk predicted in original test data. “Shift”

and

“Shift

+

Labs”

show

histograms

of

proportional

changes

(in

%)

Perturbed−Baseline Baseline

induced

by

the

time-shift perturbation and the combined time-shift and lab perturbation, respectively.

Figure 20: Same as Figure 19 for the afternoon (12pm-6pm). 48

Underspecification in Machine Learning

Figure 21: Same as Figure 19 for the evening (6pm-12am).

Table 7: Flipped decisions under time-shift and lab order composition interventions depend on random seed. Each cell is number of patient-timepoints at which decisions changed when the time range feature and lab order composition were changed, for patient timepoints with creatinine measured. “+ to -” indicates a change from the “at risk of AKI in next 48 hrs" to “not at risk”; “- to +” indicates the opposite change. Model 1 and model 2 are LSTM models that diﬀer only in random seed. Overlap indicates the number of patient-timepoint ﬂips shared between the two models. The number of ﬂips in each direction changes as a function of random seed, and the patient-timepoints that ﬂip are largely disjoint between random seeds.

Original
E. Morning (12am-6am)
Morning (6am-12pm)
Afternoon (12pm-6pm)
Night (6pm-12am)

Shifted
model 1 model 2 overlap model 1 model 2 overlap model 1 model 2 overlap model 1 model 2 overlap

E. Morning (12am-6am)

+ to - - to +

1417

251

1560

285

570

31

1202

321

1368

281

483

42

588

221

738

191

255

32

422

199

520

124

189

35

Morning (6am-12pm)

+ to - - to +

1378

456

1425

553

459

70

1279

188

1437

193

508

26

509

318

555

271

209

67

423

349

542

233

188

73

Afternoon (12pm-6pm)

+ to - - to +

1400

394

1206

764

423

103

1616

278

1694

297

700

49

601

161

607

109

258

23

410

223

417

181

159

41

Night (6pm-12am)

+ to - - to +

1700

396

1663

499

646

76

1523

215

1395

366

585

50

767

150

708

189

294

33

441

124

468

95

186

19

C.3 Preliminary Ablation Experiment
Finally, to test the hypothesis that our results point to the possibility of modifying the signals a predictor uses to make its predictions without aﬀecting iid performance, we perform an experiment

49

A. D’Amour, K. Heller, D. Moldovan, et al
where we ablate the timestamp feature entirely while training a predictor. In particular, we rerun the pipeline with an LSTM architecture. This simple ablation leads to a test set population performance similar to the rest of our ensemble of predictors where that feature was included (normalized PRAUC of 0.368, compared to a range of 0.346 to 0.366).
In addition, there is evidence that underspeciﬁcation here results from a collinearity between features, similar to that discussed in the Genomics example in the main text. In particular, this ablated model can predict time of day with an accuracy of 85% using an auxiliary head (without backpropagation). These results suggest that the signal related to time of day is present through diﬀerent correlations that the training pipeline is unable to pull apart.
Appendix D. Genomics: Full Experimental Details
In this section we provide full details of the random featurization experiment using linear models in genomic medicine, along with a brief overview of the relevant research areas.
D.1 Background
In genetics research, a genome-wide association study (GWAS ) is an observational study of a large group of individuals to identify genetic variants (or genotypes) associated with a particular trait (phenotype) of interest. One application of GWAS results is for construction of a polygenic risk score (PRS ) Wray et al. (2007); International Schizophrenia Consortium et al. (2009) for the phenotype for each individual, generally deﬁned as a weighted sum of the associated genotypes, where the weights are derived from the GWAS. One crucial factor to consider in this construction is that genetic variants are not independent and may contain highly correlated pairs due to a phenomenon called linkage disequilibrium (LD) Slatkin (2008). The most common way to correct for LD is to partition the associated variants into clusters of highly-correlated variants and to only include one representative of each cluster for the PRS (e.g. International Schizophrenia Consortium et al. (2009); CARDIoGRAMplusC4D Consortium et al. (2013)). There are other more advanced methods (e.g. Bayesian modeling of LD Vilhjálmsson et al. (2015)) which will not be discussed here.
While PRS show potential for identifying high-risk individuals for certain common diseases (e.g. Khera et al. (2018)) when derived and tested within one ancestry group (mostly European), recent work has shown that the prediction accuracy of PRS from one ancestry group does not necessarily generalize to other ancestry groups Martin et al. (2017); Duncan et al. (2019); Berg et al. (2019). When combined with the fact that more than three quarters of individuals in widely-used GWAS are of European ancestry Morales et al. (2018) (while representing less than a quarter of global population), this has raised scientiﬁc and ethical concerns about the clinical use of PRS and GWAS in the community Martin et al. (2019); Need and Goldstein (2009); Popejoy and Fullerton (2016).
D.2 Methods
In this work we investigate the issue of generalizability of PRS from a slightly diﬀerent angle. Instead of focusing on the loss of predictive accuracy of a PRS when transferred to a diﬀerent ancestry group, we investigate the sensitivity of the PRS to the choice of genotypes used in the derivation of the score, when evaluated within the same ancestry group versus outside of the group. Our phenotype of interest is the intraocular pressure (IOP ), a continuous phenotype representing the ﬂuid pressure inside the eye. This metric is an important aspect in the evaluation of risk of eye diseases such as glaucoma. We aim to predict IOP of individuals with their demographic information (age, sex, and BMI) and their genomic variants only, using the UK Biobank dataset Sudlow et al. (2015), a large, de-identiﬁed biobank study in the United Kingdom.
We ﬁrst performed a GWAS on IOP and identiﬁed 4,054 genetic variants signiﬁcantly associated with IOP distributed over 16 human chromosomes. We partitioned the variants into 129 clusters,
50

Underspecification in Machine Learning

Table 8: Distribution of IOP associated variants. 129 variant clusters are distributed over 16 chromosomes.

chrom
chr1 chr2 chr3 chr4 chr5 chr6 chr7 chr8

clusters
19 10 10 13 1 9 15 11

chrom
chr9 chr11 chr13 chr14 chr16 chr17 chr20 chr22

clusters
10 16 2 2 2 4 2 3

where variants in the same cluster are highly-correlated and the ones in diﬀerent clusters are relatively less correlated, and constructed the set of “index variants”, consisting of the best representative of each cluster. We identiﬁed and clustered the IOP-associated variants with PLINK v1.9 Purcell et al. (2007), a standard tool in population genetics, using the --clump command. We used 5 × 10−8 for the index variant p-value threshold, 5 × 10−6 for the p-value threshold for the rest of the associated variants, 0.5 for the r2 threshold, and 250 kb for the clumping radius. Table 8 summarizes the distribution of IOP-associated variant clusters over chromosomes.
After identifying the 129 clusters of variants, we created 1,000 sets of variants, each set consisting of 129 variants, exactly one variant in each cluster. The ﬁrst of those sets is the set of index variants identiﬁed by PLINK. We then sampled 999 sets of cluster representatives by sampling one variant in each cluster uniformly at random. Each of these 1,000 sets deﬁnes a set of 129 genomic features to be used in our regression models.
For training and evaluation we partitioned the UK Biobank population into British and “nonBritish” individuals, and then we randomly partitioned the British individuals into British training and evaluation set. We leave the “non-British” individuals out of training and use them solely for evaluation. We measured the “British-ness” of an individual by the distance from the coordinate-wise median of the self-reported British individuals, in the 10-dimensional vector space of the top 10 principal components (PCs) of genetic variation Price et al. (2006). Individuals whose z-scored distance from the coordinate-wise median are no greater than 4 in this PC space, are considered British. We then randomly partitioned 91,971 British individuals deﬁned as above into a British training set (82,309 individuals) and a British evaluation set (9,662 individuals). All remaining “non-British” set (14,898 individuals) was used for evaluation.
We trained linear regression models for predicting IOP with (a) demographics and a set of 129 genomic features (one of the 1,000 sets created above) and (b) demographic features only, using the British training set. We used L2 regularization whose strength was determined by 10-fold cross validation in the training set.
We observed drastically increased sensitivity (Figure 3, left, in the main text) for the genomic models (blue dots) in the “non-British” evaluation set compared to the British evaluation set or the training set. Genomic models’ margin of improvement from the baseline demographic model (gray line) is also decreased in the “non-British” evaluation set. In the “non-British” evaluation set we still see in general some improvement over the baseline, but the margins are highly variable. We also observed that the performance in British and “non-British” evaluation sets are mostly uncorrelated (r = 0.131) given the same set of genomic features (Figure 3, middle, in the main text).
Another interesting point is that the model using the original index variants (red dot) outperforms models with other choices of cluster representative in the British evaluation set, but not in the

51

A. D’Amour, K. Heller, D. Moldovan, et al

"non-British" evaluation set. This implies the cluster representatives chosen in the training domain are not always the best representatives outside of the British ancestry group.
In summary, we investigated the sensitivity to the choice of features (variants) representing clusters of highly correlated features in the context of genomics in this section. We observed that the prediction errors become highly sensitive when the evaluation domain is distinct from the training domain, in addition to being higher in magnitude. As previously discussed the robust generalization of PRS in underrepresented ancestry groups is one of the major open questions for its real-world application in clinical settings.

Appendix E. Random Feature Model: Complete Theoretical Analysis
In this section we presents deﬁnitions and asymptotic formulas for the random features model that is presented in Section 3 of the main text. Before focusing on the random features model, we will describe the general setting and deﬁne some quantities of interest.
Our general focus is to investigate the validity of two main hypotheses that arose from our empirical case studies: (i) The model learnt depends strongly on the arbitrary or random choices in the training procedure; (ii) As a consequence, for most choices of the training procedure, there exist test distributions that are close to the train distribution and have much higher test error, while the test error on the same test distribution is unchanged for other choices of the training procedure.

E.1 General Deﬁnitions
We consider for simplicity a regression problem: we are given data {(xi, yi)}i≤n, with xi ∈ Rd vector of covariates and yi ∈ R a response. We learn a model fτ : Rd → R, where τ captures the arbitrary choices in the training procedure, such as initialization, stepsize schedule, and so on. Also, to be concrete, we consider square loss and hence the test error reads

R(τ, P ) test := Etest{(y − fτ (x))2} .

(1)

Our deﬁnitions are easily generalized to other loss functions. The notation emphasizes that the test error is computed with respect to a distribution Ptest that is not necessarily the same as the training one (which will be denoted simply by P). The classical in-distribution test error reads R(τ, P).
As a ﬁrst question, we want to investigate to what extent the model fτ (x) is dependent on the arbitrary choice of τ , in particular when this is random. In order to explore this point, we deﬁne the model sensitivity as

S(τ1, τ2; Ptest) := Etest [fτ1 (x) − fτ2 (x)]2 . (2)

We next want to explore the eﬀect to this sensitivity on the out-of-distribution test error. In particular, we want to understand whether the out-of-distribution error can increase signiﬁcantly, even when the in-distribution error does not change much. Normally, the out-of-distribution risk is deﬁned by constructing a suitable neighborhood of the train distribution P, call it N (P), and letting Rshift(τ0) := supPtest∈N (P) R(τ0; Ptest).
Here we extend this classical deﬁnition, as to incorporate the constraint that the distribution shift should not damage the model constructed with an average choice of τ :

Rshift(τ0; δ) := sup R(τ0; P ) test : Eτ R(τ ; P ) test ≤ δ .

(3)

Ptest∈N (P)

52

Underspecification in Machine Learning

E.2 Random Featurization Maps
A broad class of overparametrized models is obtained by constructing a featurization map φτ : Rd → RN . We then ﬁt a model that is linear in φτ (x), e.g. via min-norm interpolation

minimize θ 2 ,

(4)

subject to y = Φτ (X)θ .

(5)

(Other procedures make sense as well.) Here Φτ (X) ∈ Rn×N is the matrix whose i-th row is φτ (xi). The corresponding estimator is denoted by θˆτ , and the predictive model is fτ (x) = θˆτ , φτ (x) . It is useful to consider a couple of examples.
Example 1. Imagine training a highly overparametrized neural network using SGD. Let F ( · ; w) : Rd → R be the input-output relation of the network. In the lazy training regime, this is well approximated by its ﬁrst-order Taylor expansion around the initialization w0 (Chizat et al., 2019). Namely F (x; w) ≈ F (x; w0)+ ∇wF (x; w0), w−w0 . If the initialization is symmetric, we can further neglect the zero-th order term, and, by letting θ = w − w0, we obtain F (x; w) ≈ ∇wF (x; w0), θ . We can therefore identify τ = (w0) and φτ (x) = ∇wF (x; w0).
Example 2. Imagine xi ∈ Rd represents the degree of activity of d biological mechanism in patient i. We do not have access to xi, and instead we observe the expression levels of N0 d genes, which are given by u0,i = W0xi + z0,i, where W0 ∈ RN0×d and z0,i are unexplained eﬀects. We do not ﬁt a model that uses the whole vector u0,i, and instead select by a clustering procedure τ a subset of N genes, hence obtaining a vector of features ui = Wτ xi + zi ∈ RN . In this case we identify the featurization map with the random map φτ (xi) := Wτ xi + zi.
As a mathematically rich and yet tractable model, we consider the random features model of Rahimi and Recht (2008), whereby

φτ (x) = σ(W x) , W ∈ RN×d .

(6)

Here σ : R → R is an activation function: it is understood that this applied to vectors entrywise.

Further, W is a matrix of ﬁrst-layer weights which are drawn randomly and are not optimized over.

We will draw the rows (wi)i≤N independently with wi ∼ Unif(Sd−1(1)). (Here and below Sd−1(r) is

the sphere of radius r in d dimensions.) We identify the arbitrary training choice with the choice of

this ﬁrst-layer weights τ = W .

√

We assume an extremely simple data distribution, namely xi ∼ Unif(Sd−1( d)) and yi = f∗(xi) =

β0, xi . Note that f∗ L2 = β0 2.

We will derive exact characterizations of the sensitivity and risk under shifted test distribution in

the proportional asymptotics N, n, d → ∞, with

N → ψ1 , n → ψ2 , (7)

d

d

for some ψ1, ψ2 ∈ (0, ∞). In what follows we will assume to to be given sequences of triples (N, n, d)

which without loss of generality we can think to be indexed by d. When we write d → ∞, it is

understood that N, n → ∞ as well , with Eq. (7) holding. Finally, we assume limd→∞

β0

2 2

=

r2

∈

(0, ∞).

E.3 Random features model: Risk
We begin by recalling some results and notations from Mei and Montanari (2019). We refer to the original paper for formal statements.

53

A. D’Amour, K. Heller, D. Moldovan, et al

In this section we consider the random features model under a slightly more general setting than the one introduced above. Namely, we allow for Gaussian noise yi = f∗(xi) + εi, εi ∼ N(0, s2), and perform ridge regression with a positive regularization parameter λ > 0:

θˆ(W , X, y; λ) := arg min n1 n yi − θ, σ(W xi) 2 + Ndλ θ 22 , (8)

θ∈RN

i=1

In the following we will omit all arguments except W , and write θˆ(W ) := θˆ(W , X, y; λ). By specializing our general deﬁnition, the risk per realization of the ﬁrst layer weights W is given by

R(W , P ) test = R(τ, P ) test := Etest [y − θˆ(W ), σ(W x) ]2 .

(9)

The activation function is assumed to be σ ∈ L2(R, γ), with γ the Gaussian measure. Such an activation function is characterized via its projection onto constant and linear functions

µ0 := E{σ(G)} , µ1 := E{Gσ(G)} , µ2∗ := E{σ(G)2} − µ20 − µ21 .

(10)

In particular, we deﬁne the following ratio

ζ ≡ µ1 .

(11)

µ∗

Let ν1, ν2 : C+ → C be analytic functions such that, for (ξ) > C a large enough constant ν1(ξ), ν2(ξ)

satisfy

ζ 2 ν2

−1

ν1 = ψ1 − ξ − ν2 − 1 − ζ2ν1ν2 ,

ζ 2 ν1

−1

(12)

ν2 = ψ2 − ξ − ν1 − 1 − ζ2ν1ν2 .

We then let

χ ≡ ν1(i(ψ1ψ2λ)1/2) · ν2(i(ψ1ψ2λ)1/2),

(13)

λ

λ = µ2 .

(14)

∗

Finally, deﬁne

E0(ζ, ψ1, ψ2, λ) ≡ − χ5ζ6 + 3χ4ζ4 + (ψ1ψ2 − ψ2 − ψ1 + 1)χ3ζ6 − 2χ3ζ4 − 3χ3ζ2
+ (ψ1 + ψ2 − 3ψ1ψ2 + 1)χ2ζ4 + 2χ2ζ2 + χ2 + 3ψ1ψ2χζ2 − ψ1ψ2 ,
E1(ζ, ψ1, ψ2, λ) ≡ ψ2χ3ζ4 − ψ2χ2ζ2 + ψ1ψ2χζ2 − ψ1ψ2 ,
E2(ζ, ψ1, ψ2, λ) ≡ χ5ζ6 − 3χ4ζ4 + (ψ1 − 1)χ3ζ6 + 2χ3ζ4 + 3χ3ζ2 + (−ψ1 − 1)χ2ζ4 − 2χ2ζ2 − χ2 . (15)
We then have, from (Mei and Montanari, 2019, Theorem 1), the following characterization of the in-distribution risk8

R(W , P) = r2 E1(ζ, ψ1, ψ2, λ) + s2 E2(ζ, ψ1, ψ2, λ) + oP(1) .

(16)

E0(ζ, ψ1, ψ2, λ)

E0(ζ, ψ1, ψ2, λ)

Here the oP (1) term depends on the realization of W , and is such that E|oP(1)| → 0 as N, n, d → ∞.

Remark 1 Notice that the right-hand side of Eq. (16) is independent of W . Hence we see that the in-distribution error is (for large N, n, d) essentially the same for most choices of W .

54

Underspecification in Machine Learning

2.5

n/d=4

n/d=6

n/d=8

2.0

S1,2/R

1.5

1.0

0.5

0.00

1

2

3

4

5

N/n

Figure 22: Random features model trained via min-norm least squares: sensitivity to the initial condition, normalized by the risk. Here the input dimension is d = 40, N is the number of neurons, and n the number of samples. We use ReLU activations; the ground truth is linear with β0 2 = 1. Circles are empirical results obtained by averaging over 50 realizations. Continuous lines correspond to the analytical prediction of Eq. (34).

E.4 Random features model: Sensitivity to random featurization Let W1, W2 be two realizations of the ﬁrst-layer weights. We can decompose

σ(W x) = µ0 + µ1W x + µ∗z⊥ ,

(17)

where, under P, we have E{z⊥(z⊥)T} = I + cn11T + ∆, ∆ op = oP(1), and E{(W x)z⊥} = 0. We therefore have (writing θˆi := θˆ(Wi))

S(W1, W2) = E ( θˆ(W1), σ(W1x) − θˆ(W2), σ(W2x) )2 = µ21E ( θˆ(W1), W1x − θˆ(W2), W2x )2 + µ2∗E ( θˆ(W1) − θˆ(W2), z⊥ )2 = µ21EX,y W1Tθˆ1 − W2Tθˆ2 22 + (µ2∗ + oP(1))EX,y{ θˆ1 − θˆ2 2} .
We consider two random independent choices of W1, W2, and deﬁne Sav := E{S(W1, W2)}, thus obtaining:

Sav = 2µ21 E W Tθˆ(W ) 22 − E E[W Tθˆ(W )|X, β0] 22 + 2µ2∗E θˆ(λ) 22 + o(1) . (18)

8. Theorem 1 in Mei and Montanari (2019) holds for the risk, conditional on the realization of X, y. The statement given here is obtained simply my taking expectation over X, y.

55

A. D’Amour, K. Heller, D. Moldovan, et al

In order to evaluate the asymptotics of this expression, we recall some formulas that follow from Mei and Montanari (2019):

W Tθˆ(W ) 2 = r2 ·

D1(ζ, ψ1, ψ2, λ)

s2 D2(ζ, ψ1, ψ2, λ)

+·

+ oP(1) , (19)

2 µ2∗ (χζ2 − 1)D0(ζ, ψ1, ψ2, λ) µ2∗ D0(ζ, ψ1, ψ2, λ)

E(W Tθˆ(W )|β0) =

1 H (ζ, ψ1, ψ2, λ) + o(1) β0 ,

(20)

µ1

θˆ(W ) 2 = r2 G1(ζ, ψ1, ψ2, λ) + s2 G2(ζ, ψ1, ψ2, λ) + oP(1) ,

(21)

2 µ2∗ G0(ζ, ψ1, ψ2, λ) µ2∗ G0(ζ, ψ1, ψ2, λ)

Here the terms oP(1) converge to 0 in L1, and we used the following notations:

D0(ζ, ψ1, ψ2, λ) = χ5ζ6 − 3χ4ζ4 + (ψ1 + ψ2 − ψ1ψ2 − 1)χ3ζ6 + 2χ3ζ4 + 3χ3ζ2

(22)

+ (3ψ1ψ2 − ψ2 − ψ1 − 1)χ2ζ4 − 2χ2ζ2 − χ2 − 3ψ1ψ2χζ2 + ψ1ψ2 ,

D1(ζ, ψ1, ψ2, λ) = χ6ζ6 − 2χ5ζ4 − (ψ1ψ2 − ψ1 − ψ2 + 1)χ4ζ6 + χ4ζ4

(23)

+ χ4ζ2 − 2(1 − ψ1ψ2)χ3ζ4 − (ψ1 + ψ2 + ψ1ψ2 + 1)χ2ζ2 − χ2 ,

D2(ζ, ψ1, ψ2, λ) = −(ψ1 − 1)χ3ζ4 − χ3ζ2 + (ψ1 + 1)χ2ζ2 + χ2 ,

(24)

G0(ζ, ψ1, ψ2, λ) = −χ5ζ6 + 3χ4ζ4 + (ψ1ψ2 − ψ2 − ψ1 + 1)χ3ζ6 − 2χ3ζ4 − 3χ3ζ2

(25)

+ (ψ1 + ψ2 − 3ψ1ψ2 + 1)χ2ζ4 + 2χ2ζ2 + χ2 + 3ψ1ψ2χζ2 − ψ1ψ2 ,

G1(ζ, ψ1, ψ2, λ) = −χ2(χζ4 − χζ2 + ψ2ζ2 + ζ2 − χψ2ζ4 + 1 ,

(26)

G2(ζ, ψ1, ψ2, λ) = χ2(χζ2 − 1)(χ2ζ4 − 2χζ2 + ζ2 + 1) ,

(27)

ζ2χ

H (ζ, ψ1, ψ2, λ) = ζ2χ − 1 .

(28)

We also claim that, for s = 0 we have

E(W Tθˆ(W )|X, β0) = µd1 XTX µd21 XTX + µ2∗(1 + q)Id −1β0 + err(d, λ) , (29)

q := − (ψ2 − ψ1)+ ,

(30)

χ0

1 + ζ2 − ψ!ζ2 − (1 + ζ2 − ψ1ζ2)2 + 4ψ1ζ2

χ0 =

2ζ 2

,

(31)

where the error term err(d, λ) satisﬁes

lim lim E

err(d, λ)

2 2

= 0.

(32)

λ→0 d→∞

We refer to Section E.6 for a sketch of the proof of this claim. Using Eq. (29) and the asymptotics of the Stieltjes transform of the spectrum of Wishart matrices,
it follows that

lim lim E
λ→0 d→∞

E[W Taˆ(λ)|X, β0] 22 = r2L (ζ, ψ1, ψ2) ,

1+q

1+q

1+q 2

L (ζ, ψ1, ψ2) := 1 − 2 ζ2 g − ζ2 ; ψ2 + ζ2 g

1+q − ζ2 ; ψ2 ,
(33)

g(z; ψ2) := ψ2 − 1 − z − ∆ , 2z

∆ := − (ψ2 − 1 − z)2 − 4z ,

−ψ2 + 1 + z + ∆ −ψ2 − 1 + z + ∆

g (z; ψ2) =

2z2

−

.

2z∆

56

Underspecification in Machine Learning

Using Eqs. (19), (21), (33) in Eq. (18), we ﬁnally obtain:

lim lim S = 2r2

ζ2D1 − L + G1 .

av
λ→0 d→∞

(χζ2 − 1)D0

G0

(34)

In Figure 3 in the main text we compare this asymptotic prediction with numerical simulations for

d = 40. We report the in-distribution sensitivity Sav normalized by the risk R(W , P). In the classical

underparametrized regime N/n → 0, the sensitivity is small. However, as the number of neurons

increases, S/R grows rapidly, with S/R not far from 2 over a large interval. Notice that S/R = 2 has a

special meaning. Letting hi(x) = fτi (x) − f∗(x), it corresponds to

h1 − h2

2 L2

=

2

h1

2 L2

=

2

h2

2L2 ,

i.e. h1, h2 L2 = 0. In other words, two models generated with two random choices of τ as ‘as

orthogonal as they can be’.

E.5 Random features model: Distribution shift

In order to explore the eﬀect of a distribution shift in the random√features model, we consider the case of a mean shift. Namely, xtest = x0 + x where x ∼ Unif(Sd−1( d)) is again uniform on the sphere, and x0 is deterministic and adversarial for a given choice W0, under the constraint x0 2 ≤ ∆. We denote this distribution by PW0,∆. We will construct a speciﬁc perturbation x0 that produces a large increase in R(W0, PW0,∆) but a small change on R(W , PW0,∆) for a typical random W independent of W0. We leave to future work the problem of determining the worst case perturbation x0.
We next consider the risk when the ﬁrst layer weights are W , and the test distribution is PW0,∆. Using the fact that x0 2 = ∆ x 2, we get

R(W , PW0,∆) = E ( β0, xtest − θˆ(W ), σ(W xtest) )2 = E ( β0, x + β0, x0 − θˆ(W ), σ(W x) − θˆ(W ), σ (W x)

W x0 )2 + oP(1)

(a)
=E

( β0, x

+

β0, x0

−

θˆ(W ), σ(W x)

− µ1 θˆ(W ), W x0 )2

+ oP(1)

(=b) R(W , PW0,∆) + β0 − µ1W Tθˆ(W ), x0 2 + oP(1) ,

where (a) follows by replacing σ ( wi, x ) by its expectation over x Eσ ( xi, x ) = Eσ (G) + o(1) =
µ1 + o(1), and (b) since E(x) = 0. The choice x0 that maximizes the risk R(W0, PW0,∆) is x0 = ∆(β0 − µ1W0Tθˆ(W0))/ β0 −
µ1W0Tθˆ(W0) 2. However this mean shift can have a signiﬁcant component along β0, which results in a large increase of R(W ; PW0,∆) for other W as well. To avoid this, we project this vector orthogonally to β0:

P⊥β W0Tθˆ(W0)

x0 = −∆

0
P⊥ W Tθˆ(W

)

.

β0 0

02

(35)

This results in the following expression for the test error on the shifted distribution:

R(W , PW0,∆) = R(W , P) + ∆2µ21T (W , W0) + oP(1) ,

(36)

P⊥β W Tθˆ(W ), P⊥β W0Tθˆ(W0)

T (W , W0) :=

0

0

P⊥ W Tθˆ(W ) 2

.

β0 0

02

(37)

We ﬁrst consider the case W = W0. We then have

ET (W0, W0) := E{ P⊥β0 W0Tθˆ(W0) 22} (38)

= E{ W0Tθˆ(W0) 22} − r12 E{ β0, W0Tθˆ(W0) 2} (39)

(a)
=

r2

ζ2D1 − H 2 + s2 ζ2D2 + o(1) ,

(40)

(χζ2 − 1)D0

D0

57

A. D’Amour, K. Heller, D. Moldovan, et al

where (a) follows by Eqs. (19) and (20). For W independent of W0, we have (for s = 0)

ET (W , W0) := E

( W Tθˆ(W ), W0Tθˆ(W0) − r−2 β0, W0Tθˆ(W0) β0, W1Tθˆ(W1) )2

W0Tθˆ(W0)

2 2

−

r−2

β0, W0Tθˆ(W0)

2

EX,y{ EW [W Tθˆ(W )|X, y] 2} − r−2(EW ,X,y β0, W Tθˆ(W ) )2 2

=

E

{ W Tθˆ(W ) 2} − r−2(E

β , W Tθˆ(W ) )2

+ oP(1)

W ,X,y

2

W ,X,y 0

T1(ζ, ψ1, ψ2, λ)2

=

+ oP(1) ,

(41)

T0(ζ, ψ1, ψ2, λ)

where

T (ζ, ψ , ψ , λ) = ζ2D1(ζ, ψ1, ψ2, λ) − H 2(ζ, ψ , ψ , λ) ,

(42)

0

12

(χζ2 − 1)D0(ζ, ψ1, ψ2, λ)

12

lim T1(ζ, ψ1, ψ2, λ) = L (ζ, ψ1, ψ2) − H 2(ζ, ψ1, ψ2, 0) .

(43)

λ→0

In Figure 2 of the main text, we plot the ratios

EW0 R(W0; PW0,∆) , EW R(W ; P)

EW ,W0 R(W ; PW0,∆) . EW R(W ; P)

(44)

Note that the perturbation introduced here is extremely small in ∞ norm. Namely x0 ∞ ≈ ∆ (2 log d)/d: as d gets larger, this is much smaller than the typical entry of x, which is of order 1.

E.6 Random features model: Derivation Eq. (29)

In this section we outline the derivation of Eq. (29). Let Uσ = σ(XW T) ∈ Rn×N (where σ is applied entrywise to the matrix XW T). Then

E{W Tθˆ(W )|X, y} = E{W T(UσTUσ + du2I)−1UσTy|X, y}

(45)

= EW {W T(UσTUσ + du2I)−1UσTXβ0} .

(46)

where u2 = λψ1ψ2. We will work within the ‘noisy linear features model’ of Mei and Montanari (2019) which replaces Uσ with

U = µ1XW T + µ∗Z ,

(47)

where (Zij )i≤n,j≤N ∼iid N(0, 1), (Xij )i≤n,j≤d ∼iid N(0, 1), (Wij )i≤N,j≤d ∼iid N(0, 1/d). The universality results of Mei and Montanari (2019) suggest that this substitution produces an error that is asymptotically negligible, namely:

E{W Tθˆ(W )|X, y} = Q˜(X)β0 + err0(d, λ) ,

(48)

Q˜(X) := EW ,Z W T(U TUσ + du2IN )−1U TX .

(49)

Note that, conditional on X U , W are jointly Gaussian, and therefore it is easy to compute the conditional expectation

−1

E{W T|U , X} = µ1 µ2∗dId + µ21XTX XTU .

(50)

We therefore get

−1

Q˜(X) = µ1 µ2∗dId + µ21XTX XTQ(X)X ,

(51)

Q(X) := EW ,Z {U (U TU + du2IN )−1U T} .

(52)

58

Underspecification in Machine Learning

At this point we claim that, for ψ1 = ψ2,

Q(X) = µd21 XXT + µ2∗(1 + q)Id µd21 XXT + µ2∗(1 + q)In −1 + Err(λ, d) , (53)
−1
where the Err(λ, d) is negligible for our purposes (namely µ1 µ2∗dId + µ21XTX XTErrXβ0 2 = oP(1) when λ → 0 after d → ∞). The main claim of this section —Eq. (29)— follows by substituting (53) in Eqs. (48), (51) after some algebraic manipulations.
In the overparametrized regime ψ1 > ψ2 (which is the main focus of our analysis) Eq. (53) is straightforward. Notice that in this case q = 0, and therefore this claim amounts to Q(X) = In + Err(λ, d). Indeed this is straightforward since in that case U has full column rank, and minimum singular value bounded away from zero. Therefore

Q(X) − EW ,Z {U (U TU )−1U T} ≤ Cu2 = C λ ,

(54)

op

and U (U TU )−1U T = In. In the underparametrized regime ψ1 < ψ2 the result can be obtained making use of Ledoit and
Péché (2011).

59

