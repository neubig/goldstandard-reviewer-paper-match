Journal of Machine Learning Research 15 (2014) 3963-4009

Submitted 5/14; Published 12/14

Eﬀective Sampling and Learning for Mallows Models with Pairwise-Preference Data

Tyler Lu Craig Boutilier Department of Computer Science University of Toronto 6 King’s College Rd. Toronto, ON, Canada M5S 3G4

tl@cs.toronto.edu cebly@cs.toronto.edu

Editor: Guy Lebanon

Abstract

Learning preference distributions is a critical problem in many areas (e.g., recommender systems, IR, social choice). However, many existing learning and inference methods impose restrictive assumptions on the form of user preferences that can be admitted as evidence. We relax these restrictions by considering as data arbitrary pairwise comparisons of alternatives, which represent the fundamental building blocks of ordinal rankings. We develop the ﬁrst algorithms for learning Mallows models (and mixtures thereof) from pairwise comparison data. At the heart of our technique is a new algorithm, the generalized repeated insertion model (GRIM), which allows sampling from arbitrary ranking distributions, and conditional Mallows models in particular. While we show that sampling from a Mallows model with pairwise evidence is computationally diﬃcult in general, we develop approximate samplers that are exact for many important special cases—and have provable bounds with pairwise evidence—and derive algorithms for evaluating log-likelihood, learning Mallows mixtures, and non-parametric estimation. Experiments on real-world data sets demonstrate the eﬀectiveness of our approach.1

Keywords: models

preference learning, ranking, incomplete data, Mallows models, mixture

1. Introduction
With the abundance of preference data from search engines, review sites, etc., there is tremendous demand for learning detailed models of user preferences to support personalized recommendation, information retrieval, social choice, and other applications. Much work has focused on ordinal preference models and learning user or group rankings of alternatives or items. Within this setting, we can distinguish two classes of models. First, we may wish to learn an underlying objective (or “correct”) ranking from noisy data or noisy expressions of user preferences (e.g., as in web search, where user selection suggests relevance), a view adopted frequently in IR and “learning to rank” (Burges, 2010) and occasionally in social choice (Young, 1995). Second, we might assume that users have diﬀerent
1. Some parts of this paper appeared in: T. Lu and C. Boutilier, Learning Mallows Models with Pairwise Preferences, Proceedings of the Twenty-Eighth International Conference on Machine Learning (ICML 2011), pp.145-152, Bellevue, WA (2011).

c 2014 Tyler Lu and Craig Boutilier.

Lu and Boutilier
types with inherently distinct preferences, and learn a population model that explains this diversity. Learning preference types (e.g., by segmenting or clustering the population) is key to eﬀective personalization and preference elicitation in recommender systems, social choice, and numerous other domains. For example, with a learned population preference distribution, choice data obtained from a speciﬁc user allows inferences to be drawn about her preferences. In this work, we focus on the latter setting, learning preference distributions when users have genuinely distinct preferences.
Considerable work in machine learning has exploited ranking models developed in the statistics and psychometrics literature, such as the Mallows model (Mallows, 1957), the Plackett-Luce model (Plackett, 1975; Luce, 1959), and others (Marden, 1995), as well as their non-parametric representations (Lebanon and Mao, 2008). However, most research to date provides methods for learning preference distributions using very restricted forms of evidence about individual user preferences, whether passively observed or actively elicited, ranging from complete rankings, to top-t/bottom-t alternatives, to partitioned preferences (Lebanon and Mao, 2008). Missing from this list are arbitrary pairwise comparisons of the form “alternative a is preferred to alternative b.” Such pairwise preferences form the building blocks of almost all reasonable evidence about preferences, and subsumes the most general evidential models proposed in the literature. Furthermore, preferences in this form naturally arise in active elicitation of user preferences and choice contexts (e.g., web search, product comparison, advertisement clicks), where a user selects one alternative over others in some set (Louviere et al., 2000). In general, data about a user’s preferences will often take the form of arbitrary choice sets as is common in web search, online advertising, product comparison, etc. But none of the techniques and algorithms developed to date can learn from such choice sets. These preferences can be as simple as a single paired comparison: “I like alternative a better than b,” or as complex as a set of comparisons: “I like a better than b, c, . . ., and I like z better than y, x, . . .” In this sense, pairwise comparisons should be viewed as the fundamental building block and universal language of ordinal preference ranking.2
While learning with pairwise preferences is clearly of great importance, it is widely believed that learning probabilistic models of ordinal preference using paired comparison data is impractically diﬃcult (indeed, we show this formally below). As a consequence, the Mallows model is often shunned in favor of more inference-friendly models (e.g., PlackettLuce, which accommodates more general, but still restrictive, preferences; see Cheng et al., 2010; Guiver and Snelson, 2009). To date, no methods have been proposed for learning from arbitrary pairwise preferences in any of the commonly used ranking models in machine learning. We tackle this problem directly by developing techniques for learning Mallows models, and mixtures thereof, from pairwise preference data.
Our core contribution is the generalized repeated insertion model (GRIM), a new method for sampling from arbitrary ranking distributions—including conditional Mallows—that generalizes the repeated insertion method for unconditional sampling of Mallows models (Doignon et al., 2004). We show that even evaluating the log-likelihood under a Mallows model with respect to arbitrary ordinal data is #P-hard, implying that learning will be at
2. Of course, ordinal preferences do not capture strength of preference; but real-valued or scaled preferences (e.g., movie or book ratings) can be converted to pairwise preferences readily, albeit with some loss of information.
3964

Learning Mallows Models with Pairwise Preferences
least as diﬃcult. However, we derive another method, which we call AMP, which eﬃciently, though approximately, samples from any conditional Mallows distribution given arbitrary pairwise evidence. Moreover, we show that AMP is exact for important classes of evidence (including partitioned preferences), and that empirically it provides very close approximations given general pairwise evidence. We use this sampler as the core of a Monte Carlo EM algorithm to learn Mallows mixtures, evaluate log-likelihood, and make predictions about missing preferences. We also extend the non-parametric framework of Lebanon and Mao (2008) to handle unrestricted ordinal preference data. Experiments show our algorithms can eﬀectively learn Mallows mixtures, with reasonable running time, on data sets with hundreds of alternatives and thousands of users. Our sampling algorithm can be adapted rather easily to other models as well (e.g., we show how a simple modiﬁcation allows sampling from Mallows models with a weighted Kendall-tau metric).
The remainder of the paper is organized as follows. In Section 2 we describe the necessary background on ordinal preferences, Mallows models, and the repeated insertion method (Doignon et al., 2004) for Mallows distributions, which we extend later in the paper. We also discuss related work on learning probabilistic preference models. We introduce our main technical tool, the generalized repeated insertion method (GRIM), in Section 3. We show how it can be used to sample from Mallows mixtures conditioned on incomplete preferences by ﬁrst deﬁning an approximate, but direct sampler AMP that is exact for important special cases, and analyzing its computational and statistical properties. We then develop Metropolis and Gibbs sampling methods that exploit AMP to soundly sample any Mallows or Mallows mixture posterior. In Section 4 we develop an EM algorithm for learning a Mallows mixture from arbitrary pairwise comparison data that leverages our sampling algorithms, and provide experimental results of this procedure on several real-world data sets in Section 5. Section 6 extends the framework of Lebanon and Mao (2008) for non-parametric estimation to handle evidence in the form of arbitrary ordinal preferences. We conclude in Section 7 with a discussion of future directions.
2. Preliminaries
We begin by describing the ordinal preferences (rankings) used in the work, providing a brief overview of several common probabilistic preference models, with an emphasis on the Mallows φ-model (and mixtures) and models of partial preference data. We then outline Doignon et al. (2004) repeated insertion model for sampling preferences from a Mallows distribution (and draw connections to older models for sampling rankings proposed by Condorcet, Kemeny and Young). We also brieﬂy discuss related work on learning probabilistic preference models.
2.1 Ordinal Preferences
We assume a set of m alternatives A = {a1, . . . , am} and n agents N = {1, . . . , n}. Each agent has preferences over the set of alternatives represented by a total ordering or ranking
over A. We write x y to mean prefers x to y. Rankings can be represented as permutations of A. For any positive integer b, let [b] = {1, . . . , b}. We often represent a ranking as a permutation or bijection σ : A → [m], where σ(a) is the rank or position of a in the ranking. Thus, for i ∈ [m], σ−1(i) is the alternative with rank i. We write
3965

Lu and Boutilier

σ = σ1σ2 · · · σm for a ranking with i-th ranked alternative σi ∈ A, and σ for the induced preference relation. For any X ⊆ A, let σ|X denote the ranking obtained by restricting σ to alternatives in X. Let 1[·] be the indicator function.
Generally, we do not have access to the complete preferences of agents, but only partial
information about their rankings (e.g., based on choice behavior, query responses, etc.). We
assume this data has a very general form: for each agent we have a set of revealed pairwise
preference comparisons over A, or simply preferences:

v = {x1 y1, . . . , xk yk }.
Intuitively, these reﬂect information about ’s preferences revealed by some process. For example, this could represent product-ratings data; preference revealed by selection or purchase of certain items (e.g., web links, products) over others, or responses to survey data.
Let tc(v ) denote the transitive closure of v , i.e., the smallest transitive relation containing v . We write {x, y} ∈ v if there is a comparison between x and y in v and, similarly, {x, y} ∈ tc(v ) if x, y are comparable in its transitive closure. Since preferences are strict, tc(v ) is a strict partial order on A. We assume each v is consistent, i.e., tc(v ) contains no cycles.3 Preferences v are complete if and only if tc(v) is a total order on A. Let Ω(v) denote the set of linear extensions of v, i.e., those rankings consistent with v. Let Ω = Ω(∅) be the set of all m! complete preferences. A collection V = (v1, . . . , vn) is a (partial) preference proﬁle—we assume that the observed data used for inference and learning purposes in our work takes this form. Given ranking σ = σ1σ2 · · · σm and preference v, we deﬁne the dissimilarity or disagreement d(v, σ) between the two to be:

d(v, σ) =

1[σj σi ∈ tc(v)].

(1)

i<j≤m

Dissimilarity between a partial preference and a ranking is the number of pairwise disagree-
ments among the relative ranking of alternatives, i.e., those pairs in v that are misordered
relative to σ. If v is a complete ranking, d(v, σ) is the classic Kendall-tau metric on rank-
ings. Likewise, deﬁne s(v, σ) to be the number of pairwise comparisons in tc(v) that are
consistent with σ. We have that d(v, σ) + s(v, σ) is the number of comparisons in tc(v). If v is complete, then d(v, σ) + s(v, σ) = m2 .
Arbitrary sets v of pairwise comparisons can be used to model a wide range of realistic revealed preferences:4

• Complete rankings require m − 1 paired comparisons (e.g, a b elicited with at most m(m − 1)/2 paired comparison queries.

c . . .), and can be

• Top-t preferences (Busse et al., 2007) require that users provide a complete ranking of their top t most preferred alternatives. These can be represented using m − 1 pairs:

3. Many of the concepts for probabilistic modeling, inference and learning developed in this paper can be applied mutatis mutandis to models where revealed preferences are noisy; however, we leave this topic to future research.
4. One exception to this is information about preferences that involve “disjunctive” constraints. For instance, a response to the question “What alternative is ranked tth?” cannot be mapped to a set of pairwise preferences unless the positions t are queried in ascending or descending order (hence inducing top-t or bottom-t preferences).

3966

Learning Mallows Models with Pairwise Preferences
t − 1 comparisons to order the top t alternatives, and m − t pairs to ensure the t-th alternative is ranked above the remaining m − t. Bottom-t preferences are similar. • Complete rankings of subsets X ⊆ A (Guiver and Snelson, 2009; Cheng et al., 2010) are also representable in the obvious fashion (requiring k − 1 comparisons if |X| = k). • Preferences revealed by the choice of an alternative a from X ⊆ A (Louviere et al., 2000) can also be represented using k − 1 pairs of the form a b for each b ∈ X \ {a} (where |X| = k). Sets of such choices are captured in the obvious way. • Ordinal ratings data: if alternatives are scored on an ordinal scale s (e.g., a scale of 1–5 where 1 is most preferred), we simply include a b whenever s(a) < s(b), assuming that alternatives with the same rating cannot be compared using the level of granularity provided.
Much of the existing work in learning or modelling distributions over ordinal preferences restricts the class of representable preferences. Much work has focused on top-t preferences (Busse et al., 2007; Meila and Chen, 2010; Gormley and Murphy, 2007; Fligner and Verducci, 1986, 1993), and its generalizations (Lebanon and Mao, 2008); other papers have worked with rankings of a subset of alternatives (Guiver and Snelson, 2009; Cheng et al., 2010). The main issue in allowing arbitrary consistent collections of paired preferences, which can represent all of the above special cases, is the diﬃcult inference problem that results. The primary aim of this work is to develop tractable inference algorithms for a much broader and realistic class of preferences. Before closing our discussion of ordinal preferences, we deﬁne a recently studied and relatively expressive class of preferences
Deﬁnition 1 (Lebanon and Mao 2008) A partial preference v is a partitioned preference if A can be partitioned into subsets A1, . . . , Aq s.t.: (a) for all i < j ≤ q, if x ∈ Ai and y ∈ Aj then x tc(v) y; and (b) for each i ≤ q, alternatives in Ai are incomparable under tc(v).
Partitioned preferences are quite general, subsuming some of the special cases above, including top-t or bottom-t preferences, or ratings data. However, they cannot represent many naturally occurring preferences, including those as simple as a single pairwise comparison a b. We demonstrate below that our techniques can be applied eﬀectively to such preferences.
2.2 Mallows Models and Sampling Procedures
There are many distributional models of rankings that have been developed in psychometrics, statistics and econometrics to explain choice behavior (Marden, 1995 provides a good overview). Two of the more popular in the machine learning community are the Mallows model (Mallows, 1957) and the Plackett-Luce model (Plackett, 1975; Luce, 1959). We focus on Mallows in this work, though we believe our methods can be extended to other models.
2.2.1 The Mallows Model
The Mallows φ-model (which we simply call the Mallows model hereafter) is typical of a wide-range of distance-based ranking models (Mallows, 1957; Marden, 1995). As above, let
3967

Lu and Boutilier

d be the Kendall-tau distance. The Mallows model is parameterized by a modal or reference

ranking σ and a dispersion parameter φ ∈ (0, 1]. For any ranking r, the Mallows model

speciﬁes:

P (r) = P (r | σ, φ) = 1 φd(r,σ) ,

(2)

Z

where Z = r ∈Ω φd(r ,σ) is the normalization constant. It can be shown that

Z = 1 · (1 + φ) · (1 + φ + φ2) · · · (1 + · · · + φm−1).

(3)

When φ = 1 we obtain the uniform distribution over Ω (in the social choice literature, this
model is known as impartial culture). As φ → 0, the distribution concentrates all mass on σ. The model can also be expressed as P (r|σ, λ) = Z1 e−λd(r,σ), where λ = − ln φ ≥ 0. Various extensions and generalizations of this model have been developed (e.g., using other
distance measures) (Marden, 1995).

2.2.2 Condorcet’s Decision Problem
We describe a simple sampling procedure proposed by Mallows, Condorcet and further analyzed by Young, since this will motivate the RIM sampler discussed in Section 2.2.3. Mallows (1957) explained his model using process in which a judge assesses alternatives by repeatedly making pairwise comparisons. The outcome of such a comparison is stochastic and depends on the reference ranking σ. If x and y are compared and x is preferred to y in σ, then the judge “correctly” assesses x y with probability 1 − pxy, and erroneously assesses y x with probability pxy < 1/2. Each assessment is independent of other comparisons. Mallows’ process generated a pairwise comparison for each pair of alternatives as described: after all paired comparisons are made, if the result is consistent (i.e., corresponds to a ranking), it is accepted; otherwise the process is repeated. While the error probability pxy can depend in a fairly general way on their positions in σ, if pxy = p for all x, y then we obtain the Mallows model.
Such a probabilistic view of rankings was studied two centuries earlier by Nicolas de Condorcet in the context of collective political decision making (Condorcet, 1785). He modeled his view of the role of government, that of making the “right decisions,” by considering the selection from a set of choices (e.g., policies), one that maximizes beneﬁt to society. Members of society, or voters, express their opinion in the form of a ranking over choices. He assumed that some (latent) objective ranking orders choices from most to least beneﬁcial to society and that each voter is able to provide an independent, random assessment of relative rank of any pair of choices: if a b, in the objective ranking a voter will assess that to be the case with probability 1 − p, with a error probability less than 1/2. Instead of studying the probabilistic model per se, Condorcet addressed the decision problem: how to ﬁnd the ranking most likely to be correct. For the case of three alternatives, he proved that the ranking which minimized the total number pairwise preference disagreements (i.e., Kendall-tau distance) with respect to the stated voter rankings was the most likely to be correct.
In modern parlance, Condorcet showed how to compute the maximum likelihood estimator (MLE) of the objective or reference ranking. Kemeny (1959) proposed the Kemeny ranking as a general method for aggregating noisy voter rankings, extending Condorcet’s

3968

Learning Mallows Models with Pairwise Preferences

approach to accommodate any number of alternatives. The Kemeny ranking is that which minimizes total number of pairwise preference disagreements with the set of voter rankings, which Kemeny justiﬁed axiomatically (showing it to be the only aggregate ranking that satisﬁes certain intuitive axioms). A statistical rationale for Kemeny’s approach was provided by Young (1995), who extended Condorcet’s analysis, showing that, for any number of alternatives, under Condorcet’s noise model, the MLE of the reference ranking is in fact the Kemeny ranking. These two independent threads (Condorcet-Kemeny-Young and Mallows) can both be viewed as statistical estimation of a noisy ranking model. We tie these threads together, showing that Condorcet’s noise model for any number of alternatives corresponds to the Mallows models (which implies, by Young’s result, that the Kemeny ranking is the MLE for the Mallows model). The Condorcet-Mallows noisy ranking process can be formalized as follows:

Pairwise Comparison Sampling of Mallows 1. Let σ be the reference ranking and 0 ≤ p ≤ 1/2. 2. Initialize v ← ∅. 3. For each pair of items x, y in A, such that x σ y,

(a) with probability 1 − p add x (b) otherwise add y x to v.

y to v,

4. If v is intransitive, go back to step 1 and start over. 5. v is transitive and corresponds to a ranking.

This pairwise comparison process generates rankings in accordance with the Mallows model (Equation 2), a fact shown by Mallows (1957), but which we derive here (since it will be instructive below). Consider the following distribution over rankings v:

P (v | σ, p) = 1 p if v and σ disagree on x, y (4) Z {x,y}⊆A 1 − p otherwise,

where Z is the normalization constant (i.e., the sum of the probabilities generated by the above procedure, over all transitive, complete preferences). The form of this distribution corresponds exactly to the rankings generated. This can be seen by noticing that the generating procedure independently decides for each pair of alternatives x, y, with a ﬂip of p-biased coin, whether to order them according to σ. Since intransitive preferences v are discarded by the procedure, the generating procedure corresponds to P . We can simplify the expression for P to:

P (v | σ, p) = 1 pd(v,σ)(1 − p)s(v,σ) Z

= 1 pd(v,σ)(1 − p)(m2 )−d(v,σ) Z

= 1 (1 − p)(m2 )

p d(v,σ) .

(5)

Z

1−p

3969

Lu and Boutilier

By setting φ = 1−p p , recalling the deﬁnition of Z (Equation 3), and noticing that

Z = (1 − p)(m2 )Z = (1 − p)(m2 ) 1 + p 1−p

(6)

p

p2

p m−1

1+ 1−p + 1−p ··· 1+···+ 1−p

, (7)

we obtain Equation 2. The log-likelihood, given observed complete rankings r1, . . . , rn, is

n
[d(r , σ) ln φ − ln Z] .
=1

Hence, the MLE ranking is the minimizer of

n =1

d(r

, σ),

namely,

the

Kemeny

ranking.

2.2.3 The Repeated Insertion Model
The Condorcet/Mallows sampling procedure for drawing rankings from the Mallows distribution can be very ineﬃcient, since it relies on rejection of partially constructed rankings as soon as a single circular, or non-transitive, triad (a b c a) is drawn. While the original motivation for these models was not computational, eﬃcient sampling is important for a variety of inference and learning tasks. Doignon et al. (2004) introduce the repeated insertion model (RIM) for the analysis of probabilistic models of approval voting, but which also provides a much more eﬀective means of sampling from a Mallows distribution.
RIM is a generative process that gives rise to a family of distributions over rankings and provides a practical way to sample rankings from a Mallows model. The model assumes some reference ranking σ = σ1σ2 · · · σm, and insertion probabilities pij for each i ≤ m, j ≤ i. RIM generates a new output ranking using the following process, proceeding in m steps. At step 1, σ1 is added to the output ranking. At step 2, σ2 is inserted above σ1 with probability p2,1 and inserted below with probability p2,2 = 1 − p2,1. More generally, at the i-th step, the output ranking will be an ordering of σ1, . . . , σi−1 and σi will be inserted at rank j ≤ i with probability pij. Critically, the insertion probabilities are independent of the ordering of the previously inserted alternatives.
It is easy to see that one can generate any ranking with the appropriate insertion positions. As we describe below, Doignon et al. (2004) show that one can sample from a Mallows distribution using RIM with appropriate insertion probabilities. We now introduce several concepts that can be used to more easily formalize and analyze RIM, and our subsequent extensions of it.

Deﬁnition 2 Let σ = σ1 · · · σm be a reference ranking. Let an insertion vector be any positive integer vector j = (j1, . . . , jm) satisfying ji ≤ i, ∀i ≤ m; and let I be the set of such insertion vectors. A repeated insertion function Φσ : I → Ω maps an insertion vector j into a ranking Φσ(j) by placing each σi, in turn, into rank ji, for all i ≤ m.

This deﬁnition is best illustrated with an example. Consider the insertion vector (1, 1, 2, 3) and reference ranking σ = abcd. In this case, Φσ(1, 1, 2, 3) = bcda because: we ﬁrst insert a into rank 1; we then insert b into rank 1, shifting a down to obtain partial ranking ba; we then insert c into rank 2, leaving b in place, but moving a down, obtaining bca; ﬁnally,

3970

Learning Mallows Models with Pairwise Preferences

we insert d at rank 3, giving bcda. By the same process we obtain Φσ(1, 2, 3, 4) = abcd, and Φσ(1, 1, 1, 1) = dcba. Given reference ranking σ, there is a one-to-one correspondence between rankings and insertion vectors.

Observation 3 For any reference ranking σ, the repeated insertion function Φσ is a bijection between I and Ω.

Sampling using RIM can characterized as follows:

Deﬁnition 4 The repeated insertion model is a probabilistic model over rankings deﬁned

by a reference ranking σ, the repeated insertion function Φσ(j1, . . . , jm) and a sequence of

insertion probabilities piji for i ≤ m, ji ≤ i, such that

i j=1

pij

= 1,

∀i ≤ m.

A

ranking

is

generated at random by ﬁrst drawing an insertion vector j = (j1, . . . , jm) ∈ I, where each ji

is drawn independently with probability piji, and then applying the insertion function Φσ(j).

Let Φ−σ 1(r) = (j1, . . . , jm). Then the probability of generating a particular ranking r under RIM is i≤m piji. It is easy to see that the Kendall-tau distance between the reference ranking and the ranking induced by an insertion vector is the sum of the number “insertion
misorderings” over all alternatives:

Proposition 5 For any insertion vector j = (j1, . . . , jm) ∈ I, we have that

m

i − ji = d(Φσ(j), σ).

(8)

i=1

Proof Observe that whenever σi is inserted at the ji-th position, it creates i − ji pairwise misorderings with respect to alternatives σ1, . . . , σi−1. All pairwise misorderings can be accounted for this way. Summing over all i ≤ m gives the Kendall-tau distance.

Doignon et al. (2004) show that by setting the insertion probabilities pij appropriately, the resulting generative process corresponds to the Mallows model. We reprove their Theorem here, since the proof will be instructive later.

Theorem 6 (Doignon et al. 2004) By setting insertion probabilities pij = φi−j/(1 + φ + · · · + φi−1) for j ≤ i ≤ m, the distribution induced by RIM with insertion function Φσ is
identical to that of the Mallows model with reference ranking σ and dispersion parameter φ.

Proof We reprove the Doignon et al. (2004) theorem. Let r be any ranking and σ the

reference ranking of the Mallows model. Let Φ−σ 1(r) = (j1, . . . , jm) be the insertion ranks.

If we multiply the factors φi−ji across i ≤ m this gives φ

m i=1

i−ji

=

φd(r,σ)

by

Proposition

5.

This term φd(r,σ) is exactly the proportional probability of r in Mallows. The denominator

of

m i=1

piji

is

(1 + φ)(1 + φ + φ2) · · · (1 + φ + · · · + φm−1)

regardless

of

r—this

is

exactly

the

normalizing constant in Mallows model. Interestingly, this gives an alternate proof of the

normalization constant in the Mallows model.

Thus RIM oﬀers a simple, useful way to sample rankings from the Mallows model while maintaining consistent partial rankings at each stage. In contrast to the rejection sampling approach of Condorcet/Mallows, RIM can be much more eﬀective since it does not require the rejection of intransitive triads (which may occur with high probability if φ is large). We summarize the RIM approach from Mallows model:

3971

Lu and Boutilier

RIM Sampling of Mallows 1. Let σ = σ1 · · · σm be the reference ranking and φ the dispersion. 2. Start with an empty ranking r. 3. For i = 1..m:
• Insert σi into r at rank position j ≤ i with probability φi−j/(1 + φ + · · · + φi−1).

RIM has worst-case quadratic running time (required number of draws from a Bernoulli distribution) when sampling from a Mallows model (this can be explained in much the same way as the complexity of insertion sort). However, the average-case time complexity can be much smaller, since insertions at each stage of the algorithm are likely to occur near the bottom of the partial ranking.

Proposition 7 The expected time complexity of repeated insertion sampling for a Mallows

model (σ, φ) is

O

min

m(1

+

φm+1)

−

φ(1

−

φm) ,

m2

.

1−φ

(1 − φ)2

Proof Suppose we have O(1) access to biased coin ﬂips. The implementation will be as

follows. Place σ1 in the ﬁrst rank. Then loop for i = 2 to m. Let pij = φi−j/

i−1 j =0

φj

.

Sample a rank position j to insert σi: start with j = i, ﬂip a coin with probability pij, if

success insert at rank j. Otherwise decrease j by 1, ﬂip a coin with probability pij/(1 −

j >j pij ), if success, insert at rank j, otherwise decrease j by 1 and repeat this process until j = 1. By the chain rule, the probability of insertion at rank j is exactly what Mallows

model requires. For each σi, when the sampled insertion rank position is j, it would require

at most i − j + 1 coin ﬂips. The expected running time, i.e., total number of coin ﬂips, if

φ < 1, is proportional to

m

ij−=10(j + 1)φj

m

=

1 − iφi

i=1

i−1 j=0

φj

i=1 1 − φ

m(1 + φm+1) φ(1 − φm)

≤

1 − φ − (1 − φ)2 .

This means one can eﬀectively sample in linear time if φ is not too close to 1. If φ = 1, the expected running time is O(m2).

Sampling with Weighted Kendall-tau. To illustrate the ﬂexibility of RIM, we show it can be used to sample from a Mallows model using a weighted Kendall-tau distance. For two rankings r and σ and insertion vector j = (j1, . . . , jm) such that Φσ(j) = r, one can deﬁne a weighted Kendall-tau distance (Shieh, 1998) with respect to positive weights w = (w1, . . . , wm) as follows
m
dw(r, σ) = wi(i − ji).
i=1
Recall that by Proposition 5, if w = 1, then dw is the standard Kendall-tau distance. Otherwise, this weighted Kendall-tau is sensitive to the pairwise misorderings of top-ranked alternatives in σ.
3972

Learning Mallows Models with Pairwise Preferences

One can sample from a Mallows model deﬁned by Pw(r) ∝ e−dw(r,σ) using RIM as

follows. Let φi = e−wi for i ≤ m. If we deﬁne the insertion probability of σi at position

ji ≤ i to be φii−ji/(1 + φi + · · · + φii−1), then the probability of generating r is proportional

to e

m i=1

(

i

−j

i

)

ln

φ

i

=

e−dw (r,σ) .

2.3 A Mallows Mixture Model for Incomplete Preferences
While distributions such as Mallows or its mixture formulation (Murphy and Martin, 2003) give rise to complete rankings, there is relatively little work on generative models for partial rankings, and in particular, models that generate arbitrary (consistent) sets of pairwise comparisons. We introduce such a generative model in this section upon which to base our subsequent learning and inference procedures given such pairwise evidence.
A Mallows mixture distribution with K components is parameterized by mixing proportions π = (π1, . . . , πK ), reference rankings σ = (σ(1), . . . , σ(K)), and dispersion parameters φ = (φ1, . . . , φK). Rankings are generated randomly by selecting one of the K components according to the multinomial distribution with parameters π. We sometimes represent this with a unit component indicator vector z = (z1, . . . , zK) ∈ {0, 1}K in which the only entry of z set to 1 is that of the selected component. If zk = 1, then ranking r is drawn from the Mallows distribution with parameters σ(k), φk.
In our model for partial preferences, we assume that each agent possesses a latent ranking r, where r is drawn from a mixture of Mallows distributions. We obtain the set of pairwise comparisons for by assuming a single additional parameter α which generates random pairs of alternatives. Intuitively, this reﬂects a process in which, given ’s latent ranking r, each pair of alternatives is selected independently with probability α, and ’s preference for that pair, as dictated by r, is revealed. That is,

P (v | r, α) = α|v|(1 − α)(m2 )−|v| if r ∈ Ω(v), (9)

0

otherwise.

This model reﬂects the relatively straightforward missing at random assumption (Ghahra-

mani and Jordan, 1995), in which there is no correlation among those pairwise preferences

that are missing/observed, nor any between observed pairs and the underlying ranking

(e.g., the positions of the observed pairs). The missing at random assumption is not always

realistic (Marlin and Zemel, 2007). We also note that this model assumes a single global

parameter α that indicates the expected degree of completeness of each agent ’s partial pref-

erences. Allowing agent-speciﬁc completeness parameters α and moving beyond “missing

at random” are important directions. However, this model serves as a reasonable starting

point for investigation.

Figure 1 illustrates a graphical model for the entire process. The resulting joint distri-

bution is

P (v, r, z | π, σ, φ, α) = P (v | r, α)P (r | z, σ, φ)P (z | π).

(10)

In our basic inference and learning problem, we take the observed data to be a preference proﬁle V = (v1, . . . , vn) of n agents, and we let Z = (z1, . . . , zn) denote the corresponding latent component memberships (i.e., zi indicates the mixture component where vi is generated from).

3973

Lu and Boutilier

π zn σ

r

φ

v

α

Figure 1: The generative model of incomplete preferences. Observed data v, a set of pairwise comparisons, is shaded.

2.4 Related Work
There is a large literature on ranking in the machine learning, statistics, economics, and theory of computation communities. It includes a variety of approaches, evaluation criteria, heuristics and applications, driven by several distinct motivations. In this section we brieﬂy review two somewhat distinct lines of research.
The ﬁrst body of work is that on rank aggregation. Roughly speaking, the aim is to ﬁnd the best objective ranking given complete or partial observations generated by some noisy process involving the (latent) objective ranking. For example, such a ranking may be a ranking of web pages expressing a typical user’s (relative) degree of satisfaction with the pages. Observed information may consist of feedback, in the form of expert ratings or user preferences expressed implicitly via web page clicks on a search results page. In other applications, observed data may include partial rankings (e.g., in political elections), or pairwise comparisons (e.g., in sports leagues). Given such feedback, the ranking system will aggregate and optimize some objective function that attempts to capture user or population satisfaction such as NDCG—common in the IR ﬁeld—(Burges et al., 2005; Volkovs and Zemel, 2009), misordered pairs (Cohen et al., 1999; Freund et al., 2003; Joachims, 2002; R. Herbrich and Obermayer, 2000), binary relevance (Agarwal and Roth, 2005; Rudin, 2009), and objectives from social choice theory (e.g., Kemeny, Borda rankings). For example, in machine learning, the area of learning to rank (LETOR) has been a topic of much research since the late 1990s, starting with the work of Cohen et al. (1999). Research into ranking systems often seeks strong generalization capabilities, in the sense that it can produce an objective ranking given a previously unencountered ranking problem using new attributes (e.g., rank web pages given a new search query). Much of this research has indeed been focused on web ranking applications (e.g., the Yahoo! Learning to Rank Challenge; see Burges, 2010). More recently, Busa-Fekete et al. (2014) have developed active learning algorithms for inferring certain distributional properties of the Mallows model.
3974

Learning Mallows Models with Pairwise Preferences
There are also communities in statistics and computational social choice that are concerned with estimating the maximum likelihood ranking under some distributional assumptions. Often such models—for example, the Mallows and Plackett-Luce models discussed above—assume a central, modal or reference objective ranking at which the distribution is peaked. A fundamental problem is estimation of this objective ranking from a collection of ordinal preference data. For example, the Kemeny ranking can be interpreted as a maximum likelihood estimate of the modal ranking in a Mallows model (Young, 1995). Other such interpretations of common rank aggregation rules also exist (Conitzer and Sandholm, 2005; Conitzer et al., 2009).
The above perspective, that of computing an objective ranking, applies to many situations (e.g., one would expect the ranking of web pages for a search query in “norovirus symptoms” to be objectively stable, since users will largely agree the informativeness of retrieved web pages). However, in many settings this is entirely inappropriate. When a group of individuals plans an activity together, such as going to a restaurant for dinner, the ranking of restaurants should clearly depend on the personal tastes and preferences of the individuals involved. In such cases, a distribution over a population’s subjective preferences better reﬂects reality. A second, growing, body of work aims to assess (individual or aggregate/group) rankings of options, or decisions, by explicitly using, modelling or reasoning about the diversity of user preferences. This is a more general problem than that of objective rank aggregation. For example, the Netﬂix collaborative ﬁltering competition has initiated much research on predicting a user’s movie ratings given the ratings for other movies, including their own and those of other users. Other relevant research on such ranking work includes label ranking (Hu¨llermeier et al., 2008), which seeks to aggregate sparse preference data of “similar users” into personalized preferences.
In recent years there has been growing interest in applying probabilistic models of preferences from statistics, psychometrics, and econometrics to model a population’s preferences. This is the context in which our work is situated. We focus on learning such preference distributions, including multimodal distributions over preferences where each mode (cluster) corresponds to a “sub-type” within the population. Much recent research has focused on using the single-peaked Mallows model as a basis for multimodal mixture distributions. One of the ﬁrst papers to propose an algorithm for learning Mallows mixtures is that of Murphy and Martin (2003). Their method assumes that training data takes the form of complete preference rankings (individual preferences), and has a running time that is factorial in the number of alternatives. Busse et al. (2007) develop a tractable EM algorithm for Mallows mixtures where preferences are restricted to be of the top-t type. A recent extension by Meila and Chen (2010) of Mallows mixtures allows for a Bayesian treatment in choosing the number of components using Dirichlet process mixtures, and oﬀers experiments on considerably larger data sets. Recent work has also studied ﬁtting temporal mixture models (a variation on the Bradley-Terry model) using EM (Francis et al., 2014).
Aside from mixture models, Lebanon and Mao (2008) propose a non-parametric kernel density estimator for rankings, which places a “smooth Mallows bump” on each training preference. They derive an eﬃciently computable, closed-form formula for the evaluation of the estimator. However, they restrict their training data to partitioned preferences (see above), a more general concept than top-t rankings, but signiﬁcantly less expressive than arbitrary pairwise comparisons. In contrast to our work, they do not address how to learn
3975

Lu and Boutilier
the kernel bandwidth parameter (see Section 6 for further discussion). There has been recent work on sampling algorithms for rankings that shares some similarities with the GRIM algorithm we develop here. This includes a sampling algorithm based on a generalization of the Plackett-Luce model (Volkovs and Zemel, 2012), inspired by bipartite matching problems that occur in certain application domains. Biernacki and Jacques (2013) propose a noisy insertion-sort model of rankings and develop EM algorithms for estimating its parameters. This is related to RIM but with some minor diﬀerences. However, none of this work addresses the question of sampling from a posterior distribution given partial preferences as evidence.
Apart from the the Mallows model, the Plackett-Luce model has also been popular as a representation of preferences. Recent work on learning and inference with this model includes: an approach to Bayesian inference of the modal ranking (Guiver and Snelson, 2009), but where training preferences are limited to ranking of all of alternatives in some subset of alternatives; and a method for learning a mixture model given top-k preferences (Gormley and Murphy, 2008), with application to political voting data.
Huang and Guestrin (2009) develop the riﬄe independence model, which partitions a set of alternatives into two sets: a ranking of each set is generated stochastically (and independently); then a stochastic process is used to interleave or “riﬄe” the two resulting rankings to produce a combined ranking. The model can applied hierarchically, with the same process used to generate the required subrankings. Huang et al. (2012) show that inference in this model is tractable for certain classes of observations. Of particular note is that fact that conditioning on partitioned preferences (which they term “partial ranking observations”) can be accomplished eﬃciently . Interestingly, Mallows models can be represented using the riﬄe independence model.
3. Generalized Repeated Insertion Model
Our ultimate goal is to support eﬀective learning and inference with Mallows models (and by extension, Mallows mixtures) given observed data or evidence in the form of partial preference proﬁles consisting of arbitrary pairwise comparisons. Sampling is, of course, an important aspect of this. The rejection sampling models discussed above can obviously be extended to accommodate pairwise observations, but are likely to be extremely ineﬃcient. By contrast, while RIM provides a powerful tool for sampling from Mallows models (and mixtures), it samples unconditionally, without allowing for (direct) conditioning on evidence. In this section, we describe and analyze a generalized version of the RIM technique that permits conditioning at each insertion step. In fact, our generalized repeated insertion model (GRIM) can be used to sample from arbitrary rank distributions. We begin in Section 3.1 by describing GRIM in this general, abstract fashion. The primary focus of our theoretical and computational analysis in Section 3.2, however, will be on its use for Mallows distributions.
3.1 Sampling from Arbitrary Ranking Distributions
We ﬁrst present the generalized repeated insertion model (GRIM) abstractly as a means of sampling from any distribution over rankings. GRIM is based on a relatively simple insight, namely, that the chain rule allows us to represent any distribution over rankings in a concise
3976

Learning Mallows Models with Pairwise Preferences

way, as long as we admit dependencies in our insertion probabilities. Speciﬁcally, we allow

the insertion probabilities for any alternative σi in the reference ranking to be conditioned

on the ordering of the previously inserted alternatives (σ1, . . . , σi−1).

Let Q be any distribution over rankings and σ an (arbitrary) reference ranking. Recall

that we can (uniquely) represent any ranking r ∈ Ω using σ and an insertion vector jr = (j1r, . . . , jmr ) ∈ I, where r = Φσ(jr). Thus Q can be represented by a distribution Q over the space I of insertion vectors, i.e., Q (jr) = Q(r). Similarly, for k < m, any partial ranking

r[k] = (r1, . . . , rk) of the alternatives {σ1, . . . , σk}, can be represented by a partial insertion

vector

j[k]

=

(j

r 1

,

.

.

.

,

j

r k

).

Letting

Q(r[k]) = {Q(r) : r1 r2 · · · rk} and Q (j[k]) = {Q (j ) : j [k] = j[k]},

we have Q (j[k]) = Q(r[k]). We deﬁne conditional insertion probabilities:

pij | j[i−1] = Q (ji = j | j[i − 1]).

(11)

This denotes the probability with which the ith alternative σi in the reference ranking is

inserted

at

position

j

≤

i,

conditioned

on

the

speciﬁc

insertions

(j1r

,

.

.

.

,

j

r i−1

)

of

all

previous

alternatives. By the chain rule, we have

Q (j) = Q (jm|j[m − 1])Q (jm−1|j[m − 2]) · · · Q (j[1]).

Suppose we apply RIM with conditional insertion probabilities pij|j[i−1] deﬁned above; that is, we draw random insertion vectors j by sampling j1 through jm, in turn, but with each conditioned on the previously sampled components. The chain rule ensures that the resulting insertion vector is sampled from the distribution Q . Hence the induced distribution over rankings r = Φσ(j) is Q. We call the aforementioned procedure the generalized repeated insertion model (GRIM). Based on the arguments above, we have:
Theorem 8 Let Q be any ranking distribution and σ a reference ranking. For any r ∈ Ω, with insertion vector jr (i.e., r = Φσ(jr)), GRIM, using the insertion probabilities in Equation 11, generates insertion vector jr with probability Q (jr) = Q(r).

For instance, GRIM can be used to sample from a (conditional) Mallows model given evidence in the form of pairwise comparisons, as shown in the following example.

Example 1 We illustrate GRIM using a simple example, sampling from a (conditional) Mallows model over A = {a, b, c}, with dispersion φ, given evidence v = {a c}. The following table describes the steps in the process:

Insert a, b

Insert c given ab

Insert c given ba

r Insertion Prob. r Insertion Prob. r Insertion Prob.

a P (ja = 1) = 1

ab

P

(jb

=

1)

=

1 1+φ

ba

P

(jb

=

2)

=

φ 1+φ

cab P (jc = 1)= 0

acb

P

(jc

=

2)

=

φ 1+φ

abc

P

(jc

=

3)

=

1 1+φ

cba P (jc = 1) = 0 bca P (jc = 2) = 0 bac P (jc = 3) = 1

The resulting ranking distribution Q is given by the product of the conditional insertion probabilities: Q(abc) = 1/(1 + φ)2; Q(acb) = φ/(1 + φ)2; and Q(bac) = φ/(1 + φ). As
required, Q(r) = 0 iﬀ r is inconsistent with evidence v.

3977

Lu and Boutilier

3.2 Sampling from Mallows Posteriors
We now develop and analyze several techniques for sampling from (mixtures of) Mallows models given partial preference proﬁles as evidence. We use the term Mallows posterior to refer to the conditional distribution that arises from incorporating evidence—in the form of a set of pairwise comparisons—into a known Mallows model. This is the primary inference task facing a system making predictions about a speciﬁc user’s preferences given pairwise evidence from that user, assuming a reasonably stable population model. This stands in contrast to the more general problem of learning the parameters of a Mallows model (a problem we address in Section 4).

3.2.1 Intractability of Sampling
One key diﬃculty with enabling inference conditioned on pairwise comparisons is the intractability of the posterior. In the above model (Equation 10), where agent ’s incomplete preference v is observed, it is intractable to work with the posterior P (r, z|v , π, σ, φ, α) even when the mixture model has a single component, a fact we prove below. One typical approach is to rely on sampling to estimate the posterior. To this end, we develop a polynomial-time posterior sampling algorithm based on GRIM, but relying on approximation of the relevant conditional insertion probabilities.
While GRIM allows sampling from arbitrary distributions over rankings, as presented above it is largely a theoretical device, since it requires inference to compute the required conditional probabilities. Thus to use GRIM to sample from a Mallows posterior, given arbitrary pairwise comparisons v, we must ﬁrst derive these required terms. The Mallows posterior is given by

φd(r,σ)

Pv(r) = P (r | v) =

φd(r ,σ) 1[r ∈ Ω(v)],

(12)

r ∈Ω(v)

which requires summing over an intractable number of rankings to compute the normalization constant.
We could use RIM for rejection sampling: sample unconditional insertion ranks, and reject a ranking at any stage if it is inconsistent with v. However, this is impractical because of the high probability of rejection. One can also modify the pairwise comparison sampling model (see Section 2.2.2) to reject inconsistent pairwise comparisons. However, if |v| is small relative to m, then for values of φ that are not too small, the probability of rejection is very high. For instance, if φ is close to 1, m = 120 and 30 alternatives appear in v, any three alternatives the probability of a cyclic triad for any triple (e.g., a b, b c, c a) is ≈ 1/4. The 90 alternatives unconstrained by v can be divided into 30 groups of 3 alternatives, hence the probability that a cycle occurs among at least one triad is at least 1 − (3/4)30 = 0.9998. This is a lower bound on the probability of rejection, showing rejection sampling to be impractical in many settings.
The main obstacle to using GRIM for sampling is computation of the insertion probabilities of a speciﬁc alternatives given the inserted positions all previous alternatives, as given by Equation 11, when Q (more precisely, the corresponding Q) is the Mallows posterior. This essentially involves computing a high-order marginal over rankings, and turns

3978

Learning Mallows Models with Pairwise Preferences
out to be #P-hard, even with a uniform distribution over Ω(v). The following result on the complexity of counting linear extensions of a partial order will be useful below:
Theorem 9 (Brightwell and Winkler 1991) Given a partial order v, computing the number of linear extensions of v, that is |Ω(v)|, is #P-complete.
To show that computing a function f (x) is #P-hard for input x, it is suﬃcient to show that a #P-complete problem can be reduced to it in polynomial time.
Proposition 10 Given v, a reference ordering σ, a partial ranking r1 · · · ri−1 over σ1, . . . , σi−1, and j ≤ i, computing the probability of inserting σi at rank j with respect to the uniform Mallows posterior P (i.e., computing P (r) ∝ 1[r ∈ Ω(v)]) is #P-hard.
Proof We reduce the problem of counting the number of linear extensions of incomplete preferences v, which is a #P-complete problem, to that of computing the desired insertion probabilities, showing the problem to be #P-hard. Given v, notice that any r = r1 . . . rm ∈ Ω(v) has a uniform posterior probability of 1/|Ω(v)|. Let Φ−σ 1(r) = (j1, . . . , jm). Assume the existence of an algorithm f to compute the required insertion probabilities. We can use it to solve the counting problem as follows: we use f to compute piji = Pr(insert σi at rank ji | r|{σ1,...,σi−1}) with partial order v for each i ∈ {2, . . . , m} (i.e., m − 1 applications of f ). By Theorem 8, we know the posterior probability of r is 1/|Ω(v)| = i piji; thus we can compute |Ω(v)| by inverting the product of the insertion probabilities. Note that this reduction can be computed in polynomial time: we can construct any r ∈ Ω(v) by using a topological sort algorithm, and we require only m − 1 calls to the algorithm insertion algorithm f .
This result shows that it is hard to sample exactly in general, and suggests that computing the normalization constant in a Mallows posterior is diﬃcult. This would also imply a computational complexity obstacle in the work on non-parametric estimators with a Mallows kernel (Lebanon and Mao, 2008) for an arbitrary set of pairwise comparisons. Nevertheless we develop an approximate sampler AMP that is computationally very eﬃcient. While its approximation quality can be quite poor in the worst case, we see below that, empirically, it produces excellent posterior approximations. We also derive bounds that delineate circumstances under which it will provide approximations with low error.
3.2.2 AMP: An Approximate Sampler
AMP is based on the same intuitions as those illustrated in Example 1, where instead of computing the correct insertion probabilities, we use the (unconditional) insertion probabilities used by RIM, but subject to constraints imposed by v. First, we compute the transitive closure tc(v) of v. Then we use a modiﬁed repeated insertion procedure where at each step, the alternative being inserted can only be placed in positions that do not contradict tc(v). We can show that the valid insertion positions for any alternative, given v, form a contiguous region of the ranking (see Figure 2 for an illustration).
Proposition 11 Given partial preference v, let the insertion of i−1 alternatives σ1, . . . , σi−1 induce a ranking r1 · · · ri−1 that is consistent with tc(v). Let Li = {i < i|ri tc(v) σi} and
3979

Lu and Boutilier

e b adc
l5 = 2 h5 = 3 v = {b e, e d}
Figure 2: Valid insertion ranks for e are {l5, . . . , h5} = {2, 3} given previous insertions and constraints v.

Algorithm 1 AMP Approximate Mallows Posterior
Input: v, σ, φ 1: r ← σ1 2: for i = 2..m do 3: Calculate li and hi from Equations 13 and 14. 4: Insert σi in r at rank j ∈ {li, . . . , hi} with probability
5: end for Output: r

. φi−j
li≤j ≤hi φi−j

Hi = {i < i|ri ≺tc(v) σi}. Then inserting σi at rank j is consistent with tc(v) if and only if li ≤ j ≤ hi, where
li = 1 if Li = ∅ (13) max(i ∈ Li) + 1 otherwise,
hi = i if Hi = ∅ (14) min(i ∈ Hi) otherwise.
Proof Inserting σi at any rank position less than li is impossible since either li = 1 (we can’t insert in rank 0) or σi lies above rli, which contradicts the requirement imposed by tc(v) that rli must be ranked higher. A similar argument can be made for inserting in rank below hi since rhi needs to be below σi. Finally, inserting into any rank in {li, . . . , hi} does not violate tc(v) since the alternative will be inserted below all alternatives that must precede it in tc(v) and all alternatives that must succeed it.

Proposition 11 immediately suggests an implementation of the GRIM algorithm, AMP, for approximate sampling of the Mallows posterior—AMP is outlined in Algorithm 1. It ﬁrst initializes ranking r with σ1 at rank 1. Then for each i = 2 . . . m, it computes li, hi and inserts σi at rank j ∈ {li, . . . , hi} with probability proportional to φi−j. Note that tc(v), which is required as part of the algorithm, can be computed via a modiﬁed depthﬁrst search. AMP induces a sampling distribution Pˆv that does not match the posterior Pv exactly: indeed the KL-divergence between the two can be severe, as the following example shows.
Example 2 Let A = {a1, . . . am} and v = a2 a3 · · · am. Let P be the uniform Mallows prior (φ = 1) with σ = a1 · · · am. There are m rankings in Ω(v), one ranking
3980

Learning Mallows Models with Pairwise Preferences

ri for each placement of a1 into rank position 1 ≤ i ≤ m. That is, r1 = a1a2 · · · am and ri = a2 · · · aia1ai+1 · · · am for i ≥ 2. The true Mallows posterior Pv is uniform over Ω(v). But AMP induces an approximation with Pˆv(ri) = 2−i for i ≤ m − 1 and Pˆv(rm) = 2−m−1.
To see this, note that to construct ri, AMP would need to insert alternatives a2, . . . , ai
successively, each with probability 1/2, above a1. Then ai+1 must be inserted below a1 with
probability 1/2, and ﬁnally the remaining alternatives ai+2, . . . , am can only be inserted at the bottom (with probability 1). Hence, the KL-divergence between Pv and Pˆv is

KL(Pv||Pˆv) =

m
Pv(ri) log2
i=1

Pv (ri ) Pˆv (ri )

m−1 1

1/m 1

1/m

=

m log2 2−i + m log2 2−m+1

i=1

= 1 − m1 + m 2− 1 − log2 m .

3.2.3 Statistical Properties of AMP
Example 2 shows that AMP may provide poor approximations in the worst case; however we will see below (Section 5) that it performs very well in practice. We can also prove interesting properties, and provide theoretical guarantees of exact sampling in important special cases.
We ﬁrst observe that AMP always produces a valid ranking; in other words, valid insertion positions always exist given any consistent v.
Proposition 12 For all i ≥ 2 and all rankings of alternatives σ1, . . . , σi−1 that is consistent with v, we have that li ≤ hi, where li and hi are deﬁned in Equation 13 and 14, respectively. That is, AMP always has a position at which to insert alternative σi.
Proof Let r be a ranking of σ1, . . . , σi−1 consistent with v. Let x be the lowest ranking alternative in r such that x tc(v) σi and y the highest-ranked alternative in r with y ≺tc(v) σi. By transitivity, x tc(v) y. Now if hi < li (as deﬁned in terms of r) this implies y r x, but this contradicts the assumption that r is consistent with v.
Furthermore, the approximate posterior has the same support as the true posterior:
Proposition 13 The support of the distribution over rankings as deﬁned by AMP is equal to Ω(v) which is equal to the support of the Mallows posterior as given in Equation 12.
Proof By Proposition 11, the algorithm never violates the constraints in tc(v), and by Proposition 12, it will always have at least one valid insertion position. Hence the algorithm always outputs a ranking consistent with v. Now, let r ∈ Ω(v) and Φ−σ 1(r) = (j1, . . . , jm) be the its corresponding insertion vector. We show that for all i ≤ m, ji ∈ {li, . . . , hi}. If this is not true, then there exists a smallest i ≤ m such that ji ∈/ {li , . . . , hi } (note i ≥ 2 since the ﬁrst alternative is always inserted at the ﬁrst position). However, Proposition 11 asserts that this insertion rank would lead to a ranking inconsistent with v—so this is not possible. Since AMP places positive probability on any insertion position in {li, . . . , hi} then r has positive probability under AMP.

3981

Lu and Boutilier

Proposition 14 For any r ∈ Ω(v), the probability AMP will output r is

Pˆv(r) =

φd(r,σ) mi=1(φi−hi + φi−hi+1 + · · · + φi−li ) .

(15)

Proof Let Φ−σ 1(r) = (j1, . . . , jm) be the insertion ranks. We have already established in Proposition 13 that AMP puts positive probability on these valid insertion ranks. In fact
the probability of r under the algorithm (see Algorithm 1) is

m

φi−ji

(φi−li + φi−li−1 + · · · + φi−hi ) =

i=1

=

φ

m i=1

i−ji

mi=1(φi−li + φi−li−1 + · · · + φi−hi )

φd(r,σ) mi=1(φi−li + φi−li−1 + · · · + φi−hi ) ,

where the last equality comes from Proposition 5.

Using this result we can show that if v lies in the class of partitioned preferences, AMP’s induced distribution is exactly the Mallows posterior:

Proposition 15 (Lebanon and Mao 2008) Let σ be a reference ranking. Let v be a partitioned preference (see Deﬁnition 1) with partition A1, . . . , Aq of A. Let δ = |{(x, y)|y σ x, x ∈ Ai, y ∈ Aj, i, j ∈ [q], i < j}|, which is the number of pairs of alternatives, that span diﬀerent subsets of the partition, that are misordered with respect to σ. Then

q−1

q

δ=

1[y

i=1 x∈Ai j=i+1 y∈Aj

σ x],

q |Ai|

φd(r,σ) = φδ

(1 + φ + φ2 + · · · + φj−1).

r∈Ω(v)

i=1 j=1

(16) (17)

Notice that Equation 17 represents the normalization constant in Mallows posterior. The intuition underlying Equation 17 is that, for any r ∈ Ω(v), the misorderings contributed by alternatives that span two subsets, as given by δ, are the same (hence the leading factor) whereas within a subset Ai alternatives can be ordered arbitrarily (hence the product of normalization constants for |Ai|).
Proposition 16 Given a partitioned preference v, the distribution induced by AMP, Pˆv, is equal to the true Mallows posterior Pv.

Proof Since the numerator in Equation 15 ( which denotes the probability that AMP out-
puts r) is the same as the proportional probability of the Mallows posterior, it is suﬃcient
to show that the denominator in Equation 15 equals the Mallows posterior normalization constant given by Equation 17. Suppose σ = σ1 · · · σm. Let v be a partitioned preference A1, . . . , Aq. Consider alternatives in Ai such that σ|Ai = σt1σt2 · · · σt|Ai| (i.e., the ranking of alternatives in Ai according to σ). For any k ∈ {1, . . . , |Ai|}, suppose alternatives A = {σ1, . . . , σtk−1} are inserted. The structure of the resulting ranking is as follows: the alternatives (A1 ∪ A2 ∪ · · · ∪ Ai−1) ∩ A must lie at the top of the ranking; the alternatives

3982

Learning Mallows Models with Pairwise Preferences

Ai ∩ A = {σt1, . . . , σtk−1} are in the middle; and Btk = (Ai+1 ∪ · · · ∪ Aq) ∩ A are at bottom. When inserting σtk at rank j, we have j ∈ {ltk , . . . , htk }, where htk = tk − |Btk | and ltk = htk − |Ai ∩ A | = tk − (k − 1) − |Btk |. Hence σtk is inserted at rank j with probability

φtk −j

φtk −j

φtk−htk + · · · +

φtk −ltk

=

.

φ|Btk | + · · · φk−1+|Btk |

The denominator can be written φ|Btk |(1 + · · · + φk−1). Observe that Btk consists of all

alternatives from A that are above σtk in σ, but are below it in v (since all such alternatives

belong to Ai+1 ∪ · · · ∪ Aq). So

|Ai| k=1

|Btk |

is

the

total

number

of

pairs

(x,

y),

where

x

∈

Ai

and y ∈ Ai+1 ∪ · · · ∪ Aq, that are misordered with respect to σ. Thus inserting alternatives

in Ai contributes a factor of

|Ai|
φ|Btk |(1 + · · · + φk−1) = φ
k=1

x∈Ai

q j=i+1

y∈Aj 1[y

|Ai|
σx] (1 + · · · + φk−1)
k=1

to the denominator in Equation 15. Once all alternatives have been inserted, the denominator becomes

q
φ i=1

x∈Ai

q j=i+1

y∈Aj 1[y

q |Ai|

σ x]

(1 + · · · + φk−1).

i=1 k=1

This is exactly the Mallows posterior normalization constant in Equation 17.

As a consequence, AMP provides exact sampling in the case of partitioned preferences, In general, this is not the case with arbitrary partial preferences (pairwise comparisons).
We now derive bounds on the relative error of AMP’s posterior, bounding the ratio between the sample probability of an arbitrary ranking r for AMP and the true posterior probability. The main technical challenge is deriving a bound on the Mallows posterior normalization constant. We can obtain an upper bound by exploiting the pairwise comparison interpretation of Mallows model (see Section 2.2.2).

Theorem 17 (Upper Bound on Normalization Constant) Let σ be a reference rank-

ing, φ ∈ (0, 1] and v a preference. The Mallows posterior normalization constant is upper

bounded by

φd(r,σ) ≤ φd(v,σ)(1 + φ)(m2 )−d(v,σ)−s(v,σ).

(18)

r∈Ω(v)

Proof The LHS of Equation 18 can be written in terms Equation 4, by setting φ = p/(1−p) (see Section 2.2.2 for derivations of the pairwise comparison interpretation of Mallows) as follows:

φd(r,σ) = Z ·

P (r|σ, p)

r∈Ω(v)

r∈Ω(v)

(19)

=Z· 1 Z
r∈Ω(v) {x,y}⊆A

p 1−p

if r and σ disagree on x, y otherwise,

3983

Lu and Boutilier

where p = φ/(1 + φ), Z is given by Equation 6 and Z is given by Equation 3, thus the

constant

in

front

simpliﬁes

to

1

/

(1

−

p

)(

m 2

)

.

Since

r

must

be

consistent

with

v,

if

x

and

y

are

comparable under v, then r must be agree with v on (x, y), i.e., if x tc(v) y then x r y.

So

P (r|σ, p) = 1 pd(v,σ)(1 − p)s(v,σ) Z
{x,y}∈/tc(v)

p 1−p

if r and σ disagree on x, y otherwise.

Hence, since Ω(v) is contained in the set of all intransitive relations on A that is consistent

with comparisons in tc(v), we must have (for k =

m 2

− d(v, σ) − s(v, σ))

P (r|σ, p) ≤ 1 pd(v,σ)(1 − p)s(v,σ)

k
pzi (1 − p)1−zi ,

Z

r∈Ω(v)

z∈{0,1}k i=1

= 1 pd(v,σ)(1 − p)s(v,σ). Z

Z·

P (r|σ, p) ≤

1 pd(v,σ)(1 − p)s(v,σ).

r∈Ω(v)

(1 − p)(m2 )

(20)

Combining Equation 20 with Equation 19, and noting that p = φ/(1 + φ), we obtain Equation 18.
Equation 18 tells us if d(v, σ) increases (i.e., v increasingly disagrees with σ), then the ﬁrst factor dominates and upper bound gets smaller—this reﬂects our natural intuitions since the set Ω(v) gets “further away” from reference ranking σ and hence its probability mass is small. We also see that if |tc(v)| is small, then d(v, σ) + s(v, σ) is small and the upper bound increases since the second factor dominates. This too makes sense because Ω(v) is large and has greater probability mass. If s(v, σ) is large, more constraints are placed on v, hence Pr(Ω(v)) is smaller, and likewise the upper bound decreases. The following example illustrates that this bound may be quite loose in some cases, but tight in others.

Example 3 Consider again the partial ranking evidence from Example 2, where v = a2 · · · am, the alternatives are {a1, . . . , am}, and our reference ranking is σ = a1a2 · · · am.

Recall that there are m rankings in Ω(v), one ranking ri for each placement of a1 into rank

position i. Now the term on the LHS of Equation 18, i.e., the true value of the normalization

constant, is

m
φd(ri,σ) = 1 + φ + φ2 + · · · + φm.

i=1

Note that d(v, σ) = 0 and s(v, σ) = m2−1 since all pairwise comparisons in tc(v) agree with σ. Thus, the term on the RHS of Equation 18, i.e., the upper bound is

φ0(1 + φ)(m2 )−0−(m2−1) = (1 + φ)m−1.

This upper bound on the normalization constant gets tight as φ → 0, but becomes exponentially loose in m as φ → 1.

3984

Learning Mallows Models with Pairwise Preferences

Before we derive a lower bound, we introduce some notions from order theory.
Deﬁnition 18 Let v be a partial preference. An anti-chain of v is a subset X of A such that for every x, y ∈ X they are incomparable under tc(v). A maximum anti-chain is an anti-chain whose size is at least the size of any anti-chain. The width of v, w(v) is the size of a maximum anti-chain of v.

Theorem 19 (Lower Bound on Normalization Constant) Let σ be a reference rank-
ing, and φ ∈ (0, 1]. Let X be a maximum anti-chain of v, Y = {a ∈ A\X | ∃x ∈ X, a tc(v) x} and Z = A\(X ∪ Y ). Let δ = |{(x, y)|x ∈ X, y ∈ Y, x σ y}| + |{(y, z)|y ∈ Y, z ∈ Z, z σ y}| + |{(x, z)|x ∈ X, z ∈ Z, z σ x}|. Denote by tc(v)|Y and tc(v)|Z the transitive closure of v restricted to the subsets Y and Z, respectively. Also let Ω(tc(v)|Y ) denote those rankings over Y that are consistent with tc(v)|Y , and similarly for Ω(tc(v)|Z). We have

 φd(r,σ) ≥ φδ 

 φd(r,σ|Y ) 


w(v) i−1

φd(r,σ|Z )

φj .

r∈Ω(v)

r∈Ω(tc(v)|Y )

r∈Ω(tc(v)|Z )

i=1 j=0

(21)

Proof We ﬁrst show that Z = {a ∈ A\X | ∃x ∈ X, x tc(v) a} = Z. If a ∈ A\X does not belong to Y then it must be comparable to at least one element in x ∈ X otherwise we can add it to Y and obtain a larger anti-chain. Hence, since a is not in Y , then x tc(v) a. Also, note that if a ∈ Y then a ∈/ Z . This is because if a belonged to both Y and Z, then there exists x1, x2 ∈ X such that x1 tc(v) a and a tc(v) x2 this would mean x1 tc(v) x2 which contradicts the anti-chain property of X. For a particular alternative in X, alternatives in
Y are either incomparable to it or must be preferred to it, similarly alternatives in Z are
either incomparable or must be dis-preferred to it.
This also implies no alternative in Z can be preferred over alternatives in Y since if this were to happen, i.e., if z tc(v) y where z ∈ Z, y ∈ Y , then ∃x ∈ X such that y tc(v) x, this implies z tc(v) x which is impossible from the above observation that Z ∩ Y = ∅.
Consider all rankings Ω(v) where we place alternatives of Y at the top, X in the middle
and Z at the bottom. Within Y and Z we rank alternatives respecting tc(v) and since X
is an anti-chain, rank these alternatives without restrictions. That is

Ω(v) = {r|∀y ∈ Y, x ∈ X, z ∈ Z, y r x, x r z, r|Y ∈ Ω(tc(v)|Y ), r|Z ∈ Ω(tc(v)|Z )}.
Now we argue Ω(v) ⊆ Ω(v). Note that we satisfy preference constraints when ranking within Y , X and Z. Also as we showed above, alternatives in Y are never dis-preferred to alternatives in X or Z and alternatives in X are never dis-preferred to alternatives in Z.
For the lower bound, ﬁrst observe if r ∈ Ω(v) then d(r, σ) = d(r|Y , σ|Y ) + d(r|X , σ|X ) + d(r|Z, σ|Z)+δ where δ is deﬁned in the theorem as the number of misorderings of alternatives across X, Y, Z, which is independent of r. Hence,





φd(r,σ) ≥

φd(r,σ) = φδ 

φd(r,σ|Y )

r∈Ω(v)

r∈Ω(v)

r∈Ω(tc(v)|Y )









φd(r,σ|X ) 

φd(r,σ|Z ) .

r∈Ω(tc(v)|X )

r∈Ω(tc(v)|Z )

3985

Lu and Boutilier

Finally, it can be seen that the sum inside the third factor is exactly the normalization
constant of an unconstrained Mallows model with |X| = w(v) alternatives, and hence equal to wi=(v1) ij−=10 φj, the second and fourth factors involve sums over rankings of Y and Z consistent with tc(v). This proves the lower bound.

While the lower bound is not presented in a convenient closed-form, it is useful nonetheless
if w(v) is large: if there are few preference constraints in v (e.g., v involves only a small
subset of alternatives) we expect Ω(v) to be large and hence have higher probability mass. We recover the true Mallows normalization constant if v = ∅ since w(v) = m. If v is highly constrained—Ω(v) has smaller probability mass—then w(v) is small, but so are the factors involving summations in Equation 21. Note that φδ decreases as the number of comparisons
in v that disagree with σ increases; this again corresponds to intuition. With these bounds in hand, we can bound the quality of the posterior estimate Pˆv(r)
produced by AMP:

Corollary 20 Let L and U be the lower and upper bound as in Theorems 19 and 17, respectively. Then for r ∈ Ω(v), where li and hi are deﬁned in Proposition 11, we have

L

Pˆv (r)

m

hi

≤

≤

φi−j Pv(r)

i=1 j=li

U

m

hi

. φi−j

i=1 j=li

(22)

Proof Pˆv(r) has the form given in Proposition 14 while Pv(r) ∝ φd(r,σ). Then apply upper and lower bounds on the normalizing constant of Pv(r).

3.2.4 MMP: An MCMC Sampler Based on AMP
While AMP may have (theoretically) poor worst-case performance, we use it as the basis for a statistically sound sampler MMP, by exploiting AMP to propose new rankings for the Metropolis algorithm. With Equation 15, we can derive the acceptance ratio for Metropolis. At step t + 1 of Metropolis, let r(t) be the previous sampled ranking. Ranking r, proposed by AMP independently of r(t), will be accepted as the t + 1st sample r(t+1) with probability a∗ r, r(t) , where:

a∗ r, r(t)

  φd(r,σ)/Zv = min 1,  φd(r(t),σ)/Zv

φd(r(t) ,σ)



 m i=1 φi−hti +φi−hti +1+···+φi−lti  φd(r,σ)

m i=1

φi−hi

+φi−hi +1 +···+φi−li

 m  hi−li+1

 hti−lit+1

= min 1,

φhti −hi (1−φhi−li+1)

i=1  1−φhti−lti+1

if φ = 1  .
otherwise

(23)

Here the lis and his are deﬁned as in Equations 13 and 14, respectively (with respect to r; and lit), and hti are deﬁned similarly, but with respect to r(t). The term Zv = r ∈Ω(v) φd(r ,σ) is the normalization constant of the Mallows posterior (given partial evidence v). The
algorithm is speciﬁed in detail in Algorithm 2.
Exploiting Proposition 13, we can show:

3986

Learning Mallows Models with Pairwise Preferences

Algorithm 2 MMP Sample Mallows Posterior using Metropolis

Input: v, σ, φ, number of steps T

1: for t = 1..T do

2: r ← AMP(v, σ, φ)

3: a ∼ Uniform[0,1]

4: r(t) ← r

if t = 1 or a ≤ a∗(r, r(t−1))

r(t−1) otherwise

5: end for

Output: r(T )

Theorem 21 The Markov chain induced by MMP is ergodic on the class of states (rankings) Ω(v).
Proof Note that the acceptance ratio as given in Equation 23 is always positive. The proposal distribution AMP draws rankings that are independent of previous rankings and by Proposition 13, its support is Ω(v). Hence, for any r ∈ Ω(v), MMP has positive probability of making a transition to any ranking in Ω(v)—thus establishing that Ω(v) is a recurrent class—including itself—implying aperiodicity.
Thus, along with the detailed balance property of Metropolis, we have that the steady state distribution of MMP is exactly the Mallows posterior Pv(r).

3.3 Sampling Mallows Mixture Posterior
Extending the GRIM, AMP and MMP algorithms to sampling from a mixture of Mallows models is straightforward. Recall the mixture posterior:

P (r, z|v, π, σ, φ) =

P (v|r, α)P (r|z, σ, φ)P (z|π) . z r∈Ω P (v|r, α)P (r|z, σ, φ)P (z|π)

We use Gibbs sampling to alternate between r and z, since the posterior does not factor in
a way that permits us to draw samples exactly by sampling one variable, then conditionally sampling another. We initialize the process with some z(0) and r(0), then repeatedly sample z conditional on r, and r conditional on z. For the tth sample, z(t) is drawn from a
multinomial with K outcomes:

P (z : zk = 1|r(t−1), π, σ, φ) = =

P (r(t−1)|z, σ, φ)P (z|π)

z P (r(t−1)|z , σ, φ)P (z |π)

φkd(r(t−1),σ(k))πk .

K k =1

φd(r(t−1),σ(k
k

))πk

To sample r(t) given zt, we use:

P (r|z(t), v, π, σ, φ) =

P (v|r)P (r|z(t), σ, φ)P (z(t)|π) .

(24)

r ∈Ω P (v|r )P (r |z(t), σ, φ)P (z(t)|π)

3987

Lu and Boutilier

Algorithm 3 SP: Sample Mallows Mixture Posterior using Gibbs
Input: v, π, σ, φ, number of steps T 1: Initialize r(0) (e.g., topological sort on v)
2: for t = 1..T do 3: z(t) ∼ P (·|r(t−1), π, σ, φ) ∝ φkd(r(t−1),σ(k))πk 4: Suppose z(t) is indicator for kth component. 5: r(t) ← AMP or MMP(v, σ(k), φk) 6: end for Output: (z(T ), r(T ))

Note that the term P (z(t)|π) in the numerator and denominator cancels, and the missing completely at random assumption (see Equation 9) implies P (v|r) = 1[r ∈ Ω(v)]f (v), where f is a function independent of r. Thus Equation 24 becomes Equation 12 (conditioned on parameters σ(k), φk). This is exactly the Mallows posterior sampling problem addressed in the previous section. Combining Gibbs sampling with sampling from a single component gives the overall SP algorithm, which is detailed in Algorithm 3. We note that this sampler is described using either MMP to exactly sample rankings (given the sampled mixture component) or AMP to allow more tractable, but approximate, sampling of rankings (see Line 5). In our experiments, we ﬁnd that AMP works well within this Gibbs sampler.

4. EM Learning Algorithm for Mallows Mixtures
Armed with the sampling algorithms derived from GRIM, we now turn to maximum likelihood learning of the parameters π, σ, and φ of a Mallows mixture using the expectation maximization (EM) algorithm. Before detailing our EM algorithm, we ﬁrst consider the evaluation of the Mallows mixture log-likelihood in Section 4.1, which can be used to select the number of mixture components, or to test EM learning convergence. We then review the EM algorithm in Section 4.2 before detailing the steps of our EM learning procedure for Mallows mixture models in Section 4.3. In Section 4.4 we analyze the running time of our learning algorithm and suggest several ways to improve its performance.

4.1 Evaluating Log-Likelihood

The log-likelihood in our mixture model is





Lα(π, σ, φ|V ) = ln 

P (v |r )P (r |z , σ, φ)P (z |π) .

(25)

∈N

z r ∈Ω

This can be rewritten as

Lα(π, σ, φ | V ) =


K
ln 



πkP (r

|σ(k), φk)α|v

|(1

−

α)(m2 )−|v

|


∈N

k=1 r ∈Ω(v )


K

=

ln 

 πkP (r |σ(k), φk) + ln α|v |(1 − α)(m2 )−|v | .

∈N

k=1 r ∈Ω(v )

3988

Learning Mallows Models with Pairwise Preferences

Note that the latter term involving α is decoupled from the other parameters, and in fact its maximum likelihood estimate is α∗ = ∈N 2|v |/(nm(m − 1)). Since we are only interested in the log-likelihood as a function of the other parameters, we can ignore this
additive constant and focus on


K

d(r ,σ(k)) 

L(π, σ, φ | V ) = ln 

πkφk  ,

∈N k=1 r ∈Ω(v ) Zk

(26)

where Zk is the Mallows normalization constant. Unfortunately, evaluating this term is provably hard.

Theorem 22 Let V = (v1, . . . , vn) be a proﬁle of partial preferences. Computing the loglikelihood L(π, σ, φ|V ) is #P-hard.

Proof We reduce the problem of counting the number of linear extensions of a partial order to this problem (see Theorem 9). Let v be a partial order for which we wish to count its linear extensions. We encode the input to log-likelihood computation as follows: let V = (v), K = 1 with φ = 1, and let σ be an arbitrary ranking. We have L = L(π, σ, φ|V ) = ln r∈Ω(v) 1/m!. Thus we can recover the number of linear extensions by computing exp(L) · m!. That this can be accomplished in polynomial time can be seen by noting that L is polynomial in m and we can use the power series expansion i≥0 Lim!/i!, where we can truncating the series after a polynomial number of steps, after which the terms in the expansion no longer impact the integer portion of the solution (number of extensions).
Given the computational diﬃculty of evaluating the log-likelihood exactly, we consider approximations. We can rewrite the log-likelihood as

K

L(π, σ, φ|V ) = ln πk E 1[r ∈ Ω(v )] ,

∈N

k=1 P (r|σ(k),φk)

and estimate the inner expectations by sampling from the Mallows model P (r|σ(k), φk). However, this can require exponential sample complexity in the worst case (e.g., if K = 1 and v is far from σ, i.e., d(v, σ) is large, then to ensure v is in the sample requires a sample set of exponential size in expectation). But we can rewrite the summation inside the log as

L(π, σ, φ|V ) =





K πk

d(r,σ(k))

ln  Zk

φk

,

∈N

k=1 r∈Ω(v )

and evaluate r∈Ω(v ) φkd(r,σ(k)) using importance sampling:

φd(r,σ(k)) = E

k

r∼Pˆv

r∈Ω(v )

φdk(r,σ(k)) . Pˆv (r|σ(k), φk)

(27)

3989

Lu and Boutilier

We generate samples r(k1), . . . , r(kT ) with AMP(v , σ(k), φk) for substitute Pˆv from Equation 15 into Equation 27 to obtain:

≤ n and k ≤ K, then

 K πk 1
ln  ∈N k=1 Zk T

( kt)



T m i−li

φjk ,

t=1 i=1 j=i−hi( kt)

where h(i kt) and li( kt) are deﬁned in Equations 14 and 13, and deﬁned with respect to r(kt), σ(k), and v . We can simplify the expression inside the log and derive the estimate:

Lˆ(π, σ, φ|V ) = ln
∈N

1K T T πk·
k=1 t=1


1
 m!
φk

mi=1(h(i kt)
m i=1 i−hi( kt)

− li( kt) + 1)

( kt) (
m 1−φkhi −li

i=1

1−φik

kt) +1

 if φk = 1
 . (28)  otherwise

As a matter of practical implementation, to ensure the sum of terms inside the log do not evaluate to zero (as it may be too small to be represented using common ﬂoating point standards), we observe that given numbers a and b with a > b > 0, ln(a + b) = ln(a) + ln(1 + b/a). Thus even if a and b are too small to be represented as ﬂoating point data types, we still obtain good approximations if ln(a) can be readily evaluated. This same technique can be used to ensure numerical stability.

4.2 The EM Algorithm

A popular approach to maximum likelihood estimation is the expectation maximization

(EM) algorithm (Dempster et al., 1977). It is applied to probabilistic models in which a set

of parameters θ determine the values of random variables, but observed data is available

for only some of these variables. Let v denote the observed variables, and h the remaining

unobserved (hidden or latent). In our model, we have θ = (π, σ, φ, α), while v consists of

a set of pairwise comparisons and h = (z, r) consist of the mixture-component assignment

and its underlying complete preference ranking. EM is eﬀectively a local search algorithm,

which alternates between two steps. The E-step computes a posterior distribution over the hidden variables given the observed variables and a current estimate θ˜ of the model

parameters:

E-Step: P (h|v, θ˜).

The M-step computes, as its new estimate, those model parameters θ that maximize the expected value (w.r.t. θ˜) of the log-likelihood (using the posterior computed in the E-step):

M-step: max E ln P (h, v|θ).
θ P (h|v,θ˜)

These steps are iterated until convergence. Indeed, EM converges and gives a locally optimal solution, since each iteration of EM will increase the log-likelihood. In general one does not need to maximize the log-likelihood in the M-step, but simply increase it. An important

3990

Learning Mallows Models with Pairwise Preferences

variation of EM called Monte Carlo EM is used when the posterior in the E-step is hard to compute (e.g., when dealing with large discrete event spaces, such as rankings). In Monte Carlo EM, ones samples from the posterior in the E-step, and in the M-step simply optimizes the choice of parameters with respect to the empirical (sample) expectation.

4.3 Monte Carlo EM for Mallows Mixtures
Learning a Mallows mixture is challenging, since even evaluating its log-likelihood is #Phard. A straightforward application of EM yields the following algorithm:

Initialization. Initialize values for πold, σold, and φold. E-step. Compute/estimate the posterior P (z , r |v , πold, σold, φold) for all ∈ N .
M-step. Compute model parameters that maximize expected log-likelihood:

πnew, σnew, φnew = argmax

E

[ln P (v , r , z |π, σ, φ)]

π,σ,φ ∈N P (r ,z |v ,πold,σold,φold)

= argmax

P (r , z |v , πold, σold, φold) ln P (v , r , z |π, σ, φ).

π,σ,φ ∈N z r ∈Ω

Exact estimation in the E-step and optimization in the M-step is of course diﬃcult due to the intractability of the Mallows posterior. Hence we resort to Monte Carlo EM and exploit our sampling methods to render EM tractable as follows. We initialize the parameters with values πold, σold, and φold. For the E-step, instead of working directly with the posterior, we use GRIM-based Gibbs sampling (see Section 3.3) to obtain samples (z(t), r(t))Tt=1 from the posteriors P (r , z |v , πold, σold, φold) of each agent ≤ n. We note once again that Gibbs sampling may use either approximate AMP or the full-ﬂedged MCMC MMP to generate rankings.
In the M-step, we maximize the expected log-likelihood using the empirical expectation with respect to the generated samples:

n
πnew, σnew, φnew = argmax

1

T

ln P (v , r(t), z(t)|π, σ, φ).

π,σ,φ =1 T t=1

(29)

We show below in Theorem 23 that we can perform this maximization by adjusting
the three (sets of) parameters in sequence—speciﬁcally, if the parameters are maximized in the order π, σ and φ (and the ﬁrst two can be maximized independently), this provides a globally optimal solution for the M-step (i.e., the solution obtained by optimizing parameters simultaneously). However, optimization of σ, in particular, is NP-hard (as we discuss below), so we use a local search heuristic to approximate the choice of reference rankings
in the M-step. We now detail the steps involved in the M-step optimization. Somewhat abusing notation, let indicator vector z(t) denote the mixture component to
which the tth sample derived from preference belongs. We partition the collection of all
agent samples (over all ) into such classes: let Sk = (ρk1, . . . , ρkjk ) be the sub-sample of the rankings r(t), over all ∈ N, t ∈ [T ], that are drawn from the kth component of the mixture model, i.e., where z(t) = k. Note that j1 + · · · + jK = nT . We can rewrite the

3991

Lu and Boutilier

objective in the M-step as
1 K jk ln P (v (k,i)|ρki)P (ρki|σ(k), φk)P (k|πk),
T
k=1 i=1
where (k, i) is the agent in sample ρk,i. We ignore ln P (v (k,i)|ρki), which only impacts α; and we know ρki ∈ Ω(v (k,i)). Thus, we can rewrite the objective as

K jk

ln π + d(ρ , σ(k)) ln φ − m ln 1 − φwk .

k

ki

k

1 − φk

k=1 i=1

w=1

(30)

where the last summation is the log of the Mallows normalization term.

Optimizing π. We apply the method of Lagrange multipliers. The Lagrangian L =

(

K k=1

jk i=1

ln πk)

+

λ(π1

+

···

+

πK

−

1),

where

we

have

removed

irrelevant

terms

of

the

objective not involving π. Taking the gradient, setting to zero and solving the system of

equations ∇π,λL = 0, we obtain:

πk = jk , ∀k ≤ K.

(31)

nT

Optimizing σ. The only term involving σ in Equation 30 is

K k=1

jk i=1

d(ρki

,

σ(k)

)

ln

φk

.

Since ln φk is a negative scaling factor, and we can optimize the reference rankings σ(k) for

each mixture component independently, we obtain:

jk
σ(k)∗ = argmin d(ρki, σ(k)).
σ(k) i=1

(32)

Optimizing the choice of reference ranking σ(k) within a mixture component requires computation of the Kemeny ranking with respect to the rankings in Sk. This is, unfortunately, an NP-hard problem (Bartholdi III et al., 1989). To maintain tractability, we exploit the notion of local Kemenization (Dwork et al., 2001): instead of optimizing the ranking, we compute a locally optimal σ(k), in which swapping any two adjacent alternatives in σ(k) does not reduce the sum of distances in the Kemeny objective. While this may not result in optimal rankings, it has been shown to be extremely eﬀective experimentally (Dwork et al., 2001; Busse et al., 2007).
We detail our local Kemenization algorithm in Algorithm 4. It works by ﬁrst initializing the new ranking σ(k) to that from the previous EM iteration, σold,(k). Then, for each alternative x, starting with those at the top of the ranking and moving downwards, we evaluate swaps of x with the element above it, say y, and proceeding with the swap if the majority of rankings in Sk prefer x over y. This proceeds until the ﬁrst potential swap of x fails (at which point we move on to the next alternative). This results in a locally optimal ranking (Dwork et al., 2001). Note we need not store all rankings in Sk; we require only its pairwise tournament graph, which is a complete directed graph with vertices corresponding to the alternatives A and the weight of each edge x → y set to be cxy = |{ρ ∈ Sk : y ρ x}|. Here cxy is the “cost” of placing x above y.

3992

Learning Mallows Models with Pairwise Preferences

Algorithm 4 LocalKemeny

Input: Sk = (ρk1, . . . , ρkjk ) 1: σ ← σkold 2: Compute pairwise tournament graph:

3: for all pair (x, y) : x, y ∈ A and x = y do

4: cxy = |{ρ ∈ Sk : y ρ x}|.

5: end for

6: d ← {x,y} : x σ(k) y cxy 7: for i = 2..m do

8: x ← alternative in ith rank of σ

9: for j = i − 1..1 do

10:

y ← alternative in jth rank of σ

11:

if cxy < cyx then

12:

Swap x with y

13:

d ← d − cxy + cyx

14:

else

15:

quit this loop

16:

end if

17: end for

18: end for

Output: σ, Kemeny cost d

Optimizing φ. When optimizing φ in Equation 30, the objective decomposes into a sum that permits independent optimization of each φk. Exact optimization of φk is diﬃcult; however, we can use gradient ascent with:

∂ (Equation 30) = d(Sk, σ(k)) − j m [(i − 1)φk − i]φik−1 + 1 ,

∂φk

φk

k

(1 − φik)(1 − φk)

i=1

where d(Sk, σ(k)) = LocalKemeny.

jk i=1

d(ρki,

σ(k))

is

the

Kemeny

objective,

which

we

obtain

after

running

Theorem 23 Let π∗ be given by Equation 31, σ∗ be given by Equation 32, and φ∗ be the optimal φ in Equation 30 where π is replaced with π∗ and σ is replaced with σ∗. Then π∗, σ∗ and φ∗ is a globally optimal solution to Equation 29.

Proof Regardless of the values of σ and φ, π is optimized by Equation 31 (see our anal-
ysis above), giving the optimal solution. It is also easy to see that the optimal reference
rankings σ are the Kemeny rankings corresponding to ranking sets S1, . . . , SK, respectively, independent of the value of φ. Finally, if we substitute the optimal values π∗ and σ∗ into Equation 30, its optimal solution φ∗ forms part of the optimal solution (π∗, σ∗, φ∗) to
Equation 29.

It isn’t diﬃcult to see that a “locally optimal” pair (σ, φ) obtained by optimizing σ ﬁrst, then φ is a locally optimal pair for Equation 29. Hence the resulting EM estimates

3993

Lu and Boutilier
are also locally optimal with respect to the likelihood (Neal and Hinton, 1999). While no approximation bounds can be given, this lends some support to the optimization approach we adopt. To test the convergence of EM, one can test the convergence of the parameters (use Kendall-tau distance to measure σ against that of the previous iteration). One can also measure whether the log-likelihood is converging.
To reduce problems with local maxima, we initialize the mixture parameters using a Kmeans clustering approach where distances are measured using Kendall-tau rather than the usual squared Euclidean distance. One can use a modiﬁed version of Lloyd’s 1982 method for K-means, where the “centroid” (pertaining to Lloyd’s method) of a set of rankings can is simply its Kemeny ranking.
4.4 Complexity of EM Steps
We analyze the running time of one iteration of our EM approach. In the E-step, we sample variables (z, r). We need not store the ranking r for the component corresponding to z, since in the M-step we do not need the actual rankings in Sk, but only its pairwise tournament graph. Hence we need only update the tournament graph corresponding to component z with sample r, which takes O(m2) time. When sampling r, let TMetro be the number of Metropolis steps before using the next sample. Each draw of r from AMP requires O(m2) time. Sampling z requires O(Km log m) time since Kendall-tau distance can be computed in O(m log m) time. Let TGibbs be the number of Gibbs sampling steps run Gibbs before outputting a sample and suppose we restart Gibbs after each such sample. Suppose also we draw TP posterior samples for each data point v . Then the E-step takes O(nTP TGibbs(TMetrom2 + Km log m)) time. In practice, one can chose a very small number of samples, and run relatively few steps, when running the MCMC methods. Indeed, in our experiments below, we don’t use MMP within the Gibbs sampler, but instead use AMP directly (this can be viewed as running Metropolis for a single step); we discuss this further below. In principle, posterior sampling can be executed in parallel, with multiple processors handling the sampling and tournament graph updates for disjoint subsets of the data v , with the results from diﬀerent processors merged into the K tournament graphs.
For the M-step, updating π takes constant time, while updating the component reference rankings σ takes O(Km2) time. Optimizing φ can also be realized eﬀectively, for instance, by using gradient ascent and bounding number of iterations. Hence the M-step requires O(Km2) time. Space complexity is dominated by the size of the K tournament graphs, hence is O(Km2).
Various techniques can be used to speed up computation from a practical perspective. Instead of storing the tournament graphs, which require quadratic memory, one can instead approximate the Kemeny ranking for any component using the Borda count to rank alternatives, which is a 5-approximation to Kemeny (Coppersmith et al., 2006), and often provides much better approximations in practice. If using Borda, when generating a complete ranking r in the posterior-sampling step (E-step) belonging to component k, one need only to update the Borda scores of all alternatives within component k; in the M-step we simply rank alternatives (within each component) according to their sampled Borda scores. We still need the Kemeny distance between the resulting Borda ranking and the sampled rankings, but this can be approximated by re-running the E-step and evaluating the Kendall-tau
3994

Learning Mallows Models with Pairwise Preferences
distance in an online fashion. One might also consider using Spearman footrule distance, which can be computed in O(m) time rather than O(m log m) as in Kendall-tau, since it is 2-approximation to the Kemeny distance (Diaconis, 1988).
5. Experiments
We perform a series of experiments to validate the eﬃcacy of our sampling and learning algorithms, to discover interesting properties of the learned mixture models on several popular data sets, and to evaluate the predictive power of our learned models to help predict missing preferences. We ﬁrst assess the quality of our GRIM-based posterior sampling method AMP, measuring its accuracy relative to the true Mallows posterior. We then measure the approximation quality of our Monte Carlo algorithm for evaluating the Mallows mixture log-likelihood. Next we apply our EM algorithm to learn mixture models using several data sets: synthetically generated data sets, a Movielens ratings data set (with large m); and a sushi preference data set. The synthetic data experiments conﬁrm the eﬀectiveness of our EM algorithm while also revealing insights on how the size of preference data (either n or α) impacts learning. We also remark on some of its connections to crowdsourcing. Finally we assess the predictive accuracy of the learned models by conditioning on partial preference information and inferring the probability of the missing pairwise comparison preferences. In all experiments, we use Equation 26 to measure log-likelihood.
5.1 Sampling Quality
We ﬁrst assess how well AMP approximates the true Mallows posterior Pv using randomly generated (synthetic) data. We vary parameters m, φ and α, while ﬁxing a canonical reference ranking σ = (1, 2, · · · m). For each parameter setting, we generate 20 preferences v (e.g., the partial preferences of 20 agents) using our mixture model (see Section 2.3 and Equations 9 and 10), and evaluate the exact KL-divergence of Pv with respect to Pˆv5 This divergence is normalized by the entropy of Pv, since, when increasing m, KL-divergence and entropy both increase. Results are shown in Figure 3, with ﬁxed and varying parameters for all three plots described in the caption. These results indicate that AMP approximates the posterior very well, with average normalized KL error ranging from 1–5%, across the parameter ranges tested.
5.2 Evaluating Log-Likelihood
In Section 4.1 we showed the #P-hardness of evaluating the log-likelihood and derived a Monte Carlo estimator that uses the AMP sampler. We evaluate the quality of the approximation produced by this estimator in this section. We vary three parameters to generate three experiments: (a) the number of alternatives m; (b) the number of mixture components K; and (c) the number of samples T per agent and per component (Equation 28). In all experiments, we ﬁx the number of agents (i.e., the number of input preferences) at n = 50.
5. To compute KL-divergence, we need only consider consistent completions of our partial preferences. This set of rankings usually has size much smaller than m!, and can be enumerated by modifying the topological sort algorithm.
3995

Lu and Boutilier

Ratio: KL to true posterior entropy Ratio: KL to true posterior entropy Ratio: KL to true posterior entropy

0.2
0.15
0.1
0.05
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 alpha

0.2
0.15
0.1
0.05
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 phi

0.2
0.15
0.1
0.05
0 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 #items

Figure 3: Comparing the posterior generated by AMP to the true Mallows posterior: normalized KL-divergence. The box-and-whisker plots have boxes shown the 25-75 percentile range over 20 runs, with the line inside box indicating the median, and the ‘+’ symbols outliers. From left to right: Plot 1: Varying α, while ﬁxing φ = 0.5, m = 10. Plot 2: Varying φ, while ﬁxing α = 0.2, m = 10. Plot 3: Varying m, ﬁxing φ = 0.5 and for m ≤ 13, α = 0.2, for m > 13, α = 0.5.

In setting (a) (varying m), we generate v from a mixture model with K = 3 and π = (1/3, 1/3, 1/3), φ = (1/2, 1/2, 1/2) and α = 0.2. Each σk (k ≤ K) is drawn uniformly at random from Ω.
In setting (b) (varying K), we generate v from a mixture model with K components, where m = 8, π = (1/K, . . . , 1/K), φ = (1/2, . . . , 1/2) and α = 0.2. Again σ drawn uniformly at random as in setting (a).
In setting (c) (varying T ), parameters are K = 1, m = 8, σ chosen uniformly at random, φ = 0.5 and α = 0.2.
The parameters for which we evaluated the log-likelihood are generated as follows: mixture weights π are sampled from a “uniform” Dirichlet distribution with a parameter vector (i.e., equivalent sample size counts) consisting of K 5s. The reference rankings σ were drawn uniformly at random, and φ is drawn uniformly at random from interval (0, 1).
The results for all three settings are shown in Figure 4. Overall we see that the Monte Carlo approximation is very good, and improves signiﬁcantly while reducing variance as we increase the sample size for each agent’s log-likelihood (as captured by K · T ). Increasing m slightly degrades approximation quality, although it oﬀers excellent estimates across the entire range of tested values.
5.3 EM Mixture Learning
We now evaluate our EM mixture-learning algorithms on the synthetic, Sushi and Movielens data sets.
5.3.1 Synthetic Data
Having empirically established that AMP provides good approximations to the true posterior, and that the log-likelihood can be closely approximated by importance sampling, we now evaluate how eﬀective our EM algorithm is at recovering parameters in a controlled
3996

Learning Mallows Models with Pairwise Preferences

Log likelihood approx. ratio Log likelihood approx. ratio Log likelihood approx. ratio

1.04

1.03

1.02

1.01

1

0.99

0.98

4

5

6

7

8

9

#items

1.08 1.06 1.04 1.02
1 1

2

3

5

7

10

#components

1.15

1.1

1.05

1

1

2

5 10 20 40 120

T

Figure 4: Comparing the ratio of the true log-likelihood to its Monte Carlo approximation. 20 instances are run per parameter setting. From left to right: Plot 1: Varying m, while ﬁxing T = 5. Plot 2: Varying K while ﬁxing T = 5. Plot 3: Varying T . Other parameter values are described in the text.

setting, using synthetic data generated from models with known parameters. We emphasize that the following experiments all use AMP within the Gibbs sampler in the E-step of Monte Carlo EM, rather than the MCMC algorithm MMP, given the approximation quality of AMP as well as its much better tractability.
We perform four experiments in which we vary: (a) α, the (expected) fraction of pairwise comparisons revealed from each preference; (b) the number of alternatives m; (c) the number of mixture components K; and (d) the number of agent preferences (data set size). In each experiment, we generate random model parameters as follows: π is drawn from a Dirichlet distribution with a uniform parameter vector of 5s; σ is drawn uniformly at random; and φ values are drawn uniformly at random from [0.2, 0.8]. Training data is generated using our probabilistic model with these parameters. When varying the single parameter for each experiment, we ﬁx the other three, with ﬁxed values: α = 0.2, m = 20, K = 3 and n = 50 × K. We analyze the performance of EM by (approximately) evaluating the ratio of the log-likelihood of the learned parameters to that of the true model parameters (π, σ, φ) on test data (preferences) generated from the true model—we set ntest = n and αtest = 1.
Results are shown in Figure 5 and provide some interesting insights. First they suggest that learning is more eﬀective when either of α or n is larger (i.e., when we have more preference data for training). We also see that learning performance degrades when we increase the number of mixture components—this is hardly surprising, since there is less data per component as we increase K. Finally, learning improves as m increases for ﬁxed values of α. This holds because the transitive closure for larger m tends to oﬀer more preference information. For instance, a1 a2 a3 a4 a5 a6 provides 5 comparisons, and corresponds to 1/9 of all comparisons when m = 10, while leaving many comparisons unavailable, even after taking its transitive closure. By contrast, a1 a2 · · · a100 has 99 comparisons which is only 1/50 of all comparisons when m = 100; but its transitive closure is a complete ranking.
These observations have interesting implications when considering information elicitation via “wisdom of the crowds.” When estimating a single objective ranking (i.e., K = 1), the amount of data needed for reliable estimation can be obtained by either increasing α (the
3997

Lu and Boutilier

Ratio of log likelihoods

1.25

1.2

1.15

1.1

1.05

1

0.1

0.2

0.5

0.7

1.0

alpha

1.15

1.1

1.05

1 1

2

3

5

10

#components

Ratio of log likelihoods

Ratio of log likelihoods

1.25 1.2
1.15 1.1
1.05 1 10
1.08
1.06
1.04
1.02
1 100

20

50

100

#items

150

500

#agents

1000

Ratio of log likelihoods

Figure 5: Performance of EM on synthetic data. Each plot shows the ratio of the loglikelihood of learned parameters to those of the true model parameters. Each parameter setting averages results of 20 instances. Log-likelihoods are approximated as in Section 4.1 with T = 10. Other parameter settings are described in the text.

average number of pairwise comparisons revealed per agent) and decreasing n (the number of agents queried) or by increasing n and decreasing α. In other words, one can obtain the same “eﬀective” data by either asking more agents about their objective assessments while decreasing the number of questions per agent, as asking fewer agents to respond, but demanding more pairwise assessments per agent.
5.3.2 Sushi Data
The Sushi data set consists of 5000 complete rankings over 10 varieties of sushi indicating sushi preferences (Kamishima et al., 2005). We used 3500 preferences for training and 1500 for validation. We ran EM experiments by generating revealed pairwise comparisons for training with various probabilities α. To mitigate issues with local maxima, we ran EM ten times (more than is necessary) for each instance. Figure 6 shows that, even without complete preferences, EM learns well even with only 30-50% of all paired comparisons, though it degrades signiﬁcantly at 20%, in part because only 10 alternatives are ranked (still performance at 20% is good when K = 1, 2). With K = 6 components, a good ﬁt is found when training on complete preferences: Table 1 shows the learned clusters (all with reasonably low dispersion), illustrating interesting patterns (e.g., fatty tuna is strongly preferred by all but one group; a strong correlation exists across groups in preference/dispreference for salmon roe and sea urchin, which are “atypical ﬁsh”; and cucumber roll is consistently dispreferred).
3998

Learning Mallows Models with Pairwise Preferences

avg log likelihood

−13.6 −13.7 −13.8 −13.9
−14 −14.1 −14.2 −14.3 −14.4

alpha=1.0 alpha=0.5 alpha=0.4 alpha=0.3 alpha=0.2

0

2

4

6

8

10 12 14 16 18 20

#components

Figure 6: Sushi data set. Plots of average validation log-likelihood when the training data, pairwise comparisons, are revealed with probabilities α ∈ {0.2, 0.3, 0.4, 0.5, 1.0}. Learning degrades as α gets closer to 0.2, that is, as more pairwise comparisons are removed.

π0 = 0.17 φ0 = 0.66 fatty tuna salmon roe tuna sea eel tuna roll shrimp egg squid cucumber roll sea urchin

π1 = 0.15 φ1 = 0.74 shrimp sea eel squid egg fatty tuna tuna tuna roll cucumber roll salmon roe sea urchin

π2 = 0.17 φ2 = 0.61 sea urchin fatty tuna sea eel salmon roe shrimp tuna squid tuna roll egg cucumber roll

π3 = 0.18 φ3 = 0.64 fatty tuna tuna shrimp tuna roll squid sea eel egg cucumber roll salmon roe sea urchin

π4 = 0.16 φ4 = 0.61 fatty tuna sea urchin tuna salmon roe sea eel tuna roll shrimp squid egg cucumber roll

π5 = 0.18 φ5 = 0.62 fatty tuna sea urchin salmon roe shrimp tuna squid tuna roll sea eel egg cucumber roll

Table 1: Learned model for K = 6 on the sushi data set with complete preferences.

3999

Lu and Boutilier
5.3.3 Movielens Data
We apply our EM algorithm to a subset of the Movielens data set (see www.grouplens.org) to ﬁnd “preference types” across users. We use the 200 (out of roughly 3900) most frequently rated movies, and the ratings of the 5980 users (out of roughly 6000) who rated at least one of these. Integer ratings from 1 to 5 are converted to pairwise preferences in the obvious way (for ties, no preference was added to v). For example, if A and B had rating 5, C had rating 3 and D rating 1 then the user preference becomes v = {A C, A D, B C, B D, C D}. We discard preferences that are empty when restricted to the top 200 movies, and use 3986 preferences for training and 1994 for validation. We run EM with number of components K = 1, . . . , 20; for each K we ran EM 20 times to mitigate the impact of local maxima. For each K, we evaluate average log-likelihood of the best run on the validation set to select the number of mixture components K. Log-likelihoods are approximated using our Monte Carlo estimator (with K · T = 120).6
Log-likelihood results are shown in Figure 7 as a function of the number of mixture components. These results suggest that the best component sizes are K = 10 and K = 5 on the validation set. The learned model with K = 5 is displayed in Table 2, with each component ranking truncated to the top-20 movies. The ﬁve references rankings in this case are have some intuitive interpretation, but do not seem to exhibit the same separation as in the Sushi data set, in part due to the non-trivial overlap involving a number of “universally popular” movies (e.g., two movies, The Shawshank Redemption and The Usual Suspects, occur in all ﬁve components; two more occur in four, and more than 30 occur in three). Note also that the dispersion of each component is extremely high, approaching 1.
Despite this, certain patterns can be discerned. especially by focusing on reasonably unique movies, those than occur in only one or two components. For example, the second component contains the following “unique” movies: Monty Python and the Holy Grail, The Maltese Falcon, Blade Runner, One Flew Over the Cuckoo’s Nest, A Clockwork Orange, 2001: A Space Odyssey, North by Northwest, Pulp Fiction, Chinatown, and Apocalypse Now. Themes within this cluster of unique movies include “older” science ﬁction, ultraviolence, actor Jack Nicholson and director Stanley Kubrick. The average date of the (top) twenty movies within this component is 1970, which is signiﬁcantly lower than those of other components.
The same analysis of the ﬁfth component shows the following “unique” movies: A Christmas Story, This is Spinal Tap, American Beauty, Pulp Fiction, The Princess Bride, Forrest Gump, Fight Club, Fargo, Ferris Bueller’s Day Oﬀ, Raising Arizona, Good Will Hunting, and The Matrix. Many of the movies here would commonly be characterized as “quirky,” including ﬁve “quirky comedies,” and several that tend toward extreme violence. The movies in this component also have a signiﬁcantly later average date, 1992, than the others.
6. The C++ implementation of our algorithms have EM wall clock times of 15–20 minutes (Intel Xeon dual-core, 3GHz), certainly practical for a data set of this size. In other data sets, given the smaller number of alternatives, run times are much faster.
4000

Learning Mallows Models with Pairwise Preferences

avg log likelihood

−49.6

−49.8

−50

−50.2

−50.4

−50.6

−50.8

−51 −51.2

train validation

−51.4

0

2

4

6

8

10 12 14 16 18 20

#components

Figure 7: Movielens data set: average training and validation log likelihoods on the learned model parameters of diﬀerent component sizes.

π1 = 0.24, φ1 = 0.98 Citizen Kane (1941)

Godfather, The (1972)

Dr. Strangelove (1963)

Schindler’s List (1993) Rear Window (1954)

Shawshank Redemp-

tion, The (1994)

American

Beauty

(1999)

Godfather: Part II,

The (1974)

One Flew Over the

Cuckoo’s Nest (1975)

Casablanca (1942)

Usual Suspects, The (1995) Pulp Fiction (1994)

Monty Python and the Holy Grail (1974) Fargo (1996)

Life Is Beautiful (1997)

Graduate, The (1967)

North by Northwest (1959) GoodFellas (1990)
Chinatown (1974)
Raiders of the Lost Ark (1981)

π2 = 0.23, φ2 = 0.98 Godfather, The (1972)
Dr. Strangelove (1963)
Citizen Kane (1941)
Casablanca (1942) Star Wars: Episode IV - A New Hope (1977) Usual Suspects, The (1995) Raiders of the Lost Ark (1981) Monty Python and the Holy Grail (1974) Rear Window (1954)
Maltese Falcon, The (1941) Blade Runner (1982)
One Flew Over the Cuckoo’s Nest (1975) Clockwork Orange, A (1971) 2001: A Space Odyssey (1968) North by Northwest (1959) Pulp Fiction (1994)

Godfather: Part II, The (1974) Chinatown (1974)

Apocalypse

Now

(1979)

Shawshank Redemp-

tion, The (1994)

π3 = 0.21, φ3 = 0.98 Raiders of the Lost Ark (1981)
Godfather, The (1972)

Schindler’s List (1993)

Rear Window (1954) Star Wars: Episode IV - A New Hope (1977) Shawshank Redemption, The (1994) Casablanca (1942)

Sixth Sense, (1999) Psycho (1960)

The

Citizen Kane (1941)

Sting, The (1973)

Usual Suspects, The (1995) Saving Private Ryan (1998) Godfather: Part II, The (1974) Silence of the Lambs, The (1991) Wizard of Oz, The (1939)

Dr. Strangelove (1963)

Jaws (1975)

Braveheart (1995)

Aliens (1986)

π4 = 0.19, φ4 = 0.98 Shawshank Redemption, The (1994) Life Is Beautiful (1997)
Raiders of the Lost Ark (1981) Schindler’s List (1993) Star Wars: Episode IV - A New Hope (1977) Matrix, The (1999)
Sixth Sense, The (1999) Sting, The (1973)
Forrest Gump (1994)
Usual Suspects, The (1995) Braveheart (1995)
Green Mile, The (1999)
Indiana Jones and the Last Crusade (1989) Saving Private Ryan (1998) Princess Bride, The (1987) Star Wars: Episode V - The Empire Strikes Back (1980) Silence of the Lambs, The (1991) Good Will Hunting (1997) Ferris Bueller’s Day Oﬀ (1986) When Harry Met Sally (1989)

π5 = 0.13, φ5 = 0.97 Usual Suspects, The (1995) Shawshank Redemption, The (1994) Schindler’s List (1993)

Life Is Beautiful (1997)

Christmas Story, A

(1983)

This Is Spinal Tap

(1984)

American

Beauty

(1999)

Sixth Sense, The

(1999)

Pulp Fiction (1994)

Princess Bride, The (1987) Silence of the Lambs, The (1991) Godfather, The (1972)

Forrest Gump (1994)

Fight Club (1999)

Fargo (1996)

Ferris Bueller’s Day Oﬀ (1986)

Raising Arizona (1987)

Saving Private Ryan (1998) Good Will Hunting (1997) Matrix, The (1999)

Table 2: Learned model for K = 5 on Movielens. Shows the top 20 (out of 200) movies.

4001

Lu and Boutilier

5.4 Predicting Missing Pairwise Preferences

In our prediction experiments, we seek to evaluate the performance of the learned models in predicting unseen pairwise comparisons. In particular, we use the complete sushi data set, train our mixture model on the ﬁrst 3500 complete rankings (we train for all K = 1, . . . , 20), and select the best K by evaluating the log-likelihood on the validation data set, which consists of 500 complete rankings. It turns out that a mixture model with K = 6 was most suitable.
To test posterior prediction performance, we use 1000 complete rankings, distinct from both the training and validation sets, and randomly remove a fraction 1 − α of the pairwise comparisons from each ranking, then compute the transitive closure of the remaining comparisons to obtain partial preferences. We generate preferences for four diﬀerent values of α. With α = 0, all preferences are removed; with α = 0.25, 42% of the pairwise comparisons are left after computing transitive closures; with α = 0.5, 76% of the all pairwise comparisons remain; and with α = 0.75, 83% of the pairwise comparisons are left.
We conditioned the learned model on the partial preferences of each agent in turn, to obtain posterior distributions over which we can infer each agent’s missing pairwise comparisons. In making predictions, we use our posterior sampling algorithm SP to sample complete rankings, which we then use to update a tournament graph—recall, this is a set of counts cab to count the number of rankings for which a b, for all a, b ∈ A. Then we estimate the posterior probability P (a b | v) by cabc+abcba .
We deﬁne our prediction loss as follows. Suppose we have a complete ranking r with its corresponding partial preference v obtained as described above. For a given a r b that is unobserved in tc(v), we deﬁne the posterior prediction loss to be Pˆ(a ≺ b | v) = cabc+bacba . Let M (v) = {(a, b) : a r b, a b ∈/ tc(v)} be the set of missing pairwise comparisons in v. We deﬁne the average loss of v as

εv =

(a,b)∈M(v ) Pˆ(a ≺ b|v) |M (v )| .

We next deﬁne the average loss per preference to be

1n ε = n εv ,
=1

where n is the number of distinct agents or preferences (in this case n = 1000). For a r b, let D(a, b) = r(b) − r(a) be the diﬀerence in their rank positions and MD(v) = {(a, b) ∈ M (v) : D(a, b) = D}. We also measure the average loss at distance D as follows:

εD =

n =1

(a,b)∈M (v ) Pˆ(a ≺ b|v)

n=1 |MD D(v )| .

The results for average loss per preference are as follows:

• ε = 0.43 for preferences generated with α = 0; • ε = 0.35 for α = 0.25, • ε = 0.39 for α = 0.5, and

4002

Learning Mallows Models with Pairwise Preferences

average loss

0.5

alpha=0

0.45

alpha=.25

alpha=.5

0.4

alpha=.75

0.35

0.3

0.25

0.2

0.15

0.1

0.05

0

1

2

3

4

5

6

7

8

9

distance

Figure 8: Sushi prediction results: average prediction loss for missing pairwise comparisons for pairs at diﬀerent distances in the underlying ranking.

• ε = 0.44 for α = 0.75.
(We interpret these results below). Results for εD at various distances are plotted in Figure 8. Since these results are extremely sensitive to the number of pairwise comparisons available in the data at diﬀerent distances, we show the number of such comparisons, per distance, in Table 3.
The results indicate that predictive performance is weakly accurate when pairs are close in distance, but improves as the distance between the predicted alternatives increases in the underlying ranking. For α = 0.75, the average loss at distances 5 and 6 is higher than expected, but this is due to the small number of comparisons missing available for testing (and in general) at those distances. We also observe that the number of comparisons of a particular distance decreases as a function of the distance—this is more pronounced for smaller values of α. This can be attributed to the use of transitive closure: the further apart a pair of alternatives are in the underlying ranking, the less likely it is that we will remove all of the pairwise comparisons required render the two alternatives incomparable after taking the transitive closure of the preferences that remain. As a consequence of the skewed distribution of missing pairs available for prediction at speciﬁc distances, the average loss per preference does not in fact decrease as α increases. For example, it is 0.39 for α = 0.5, and 0.44 for α = 0.75; this is because the relative number of missing comparisons at smaller distances (which are more diﬃcult to predict) is much greater when α = 0.75 than when α = 0.5 (as shown in Table 3).
6. Applications to Non-Parametric Estimators
Lebanon and Mao (2008) propose a non-parametric estimator for Mallows models when observations form partitioned preferences. This estimator is an analogue of typical kernel
4003

Lu and Boutilier

α=0 α = .25 α = .5 α = .75

D=1 9000 6610 4429 2230

D=2 8000 5487 2911 824

D=3 7000 4393 1683 244

D=4 6000 3459 898 62

D=5 5000 2492 443 9

D=6 4000 1744 191 2

D=7 3000 1092 65 0

D=8 2000 606 22 0

D=9 1000 256 1 0

Table 3: The number of missing pairwise comparisons (over all agents) among pairs that are distance D from each other, with preferences generated by randomly deleting fraction 1 − α of preferences, then taking the transitive closure of the remaining comparisons.

density estimators, but over the space of rankings. Their purpose, similar to mixture models, is to model the distribution of real ranking data. The idea is to place “smooth unimodal bumps,” formulated as a single Mallows model, at every input (training) preference. This is much like a mixture model with the number of components equal to the number of preferences in the training data. They oﬀer closed-form solutions by exploiting the existence of the closed-form for the Mallows normalization constant when partitioned preferences are observed. Unfortunately, with general pairwise comparisons, computing this normalization constant is intractable unless #P=P. In contrast to our contributions above, they do not address the question of how to ﬁnd a maximum likelihood estimate of the Mallows dispersion parameter, also known as the kernel width, which they suggest as being “extremely diﬃcult.”
It turns out we can use AMP for approximate marginalization to support non-parametric estimation with general preference data. This shows the potential applicability of our sampling algorithm to a wider range of problems where observations consist of pairwise comparisons. We illustrate its application by deﬁning a non-parametric estimator and deriving a Monte Carlo evaluation formula suitable for incomplete preferences.
Deﬁne a joint distribution q over the probability space Ω(v ) × Ω:

φd(r,s) q (s, r) = |Ω(v )|Zφ , (33)

where Zφ is the Mallows normalization constant with respect to dispersion φ. This distribution corresponds to drawing a ranking s uniformly at random from Ω(v ) and then drawing r from a Mallows distribution with reference ranking s and dispersion φ. The estimator, extended in the style of Lebanon and Mao (2008) to any set of paired comparisons, is simply:

1 p(v) =

q (s ∈ Ω(v ), r ∈ Ω(v))

(34)

n

∈N

1

φd(r,s)

= n

|Ω(v )|Zφ .

∈N s∈Ω(v ) r∈Ω(v)

Note that this is a distribution over rankings and not incomplete preferences, that is, a marginal over Ω(v). A special case arises when V consists entirely of complete rankings,

4004

Learning Mallows Models with Pairwise Preferences

which simpliﬁes to a mixture of Mallows models with n equally weighted components,
each with one of the observed rankings v as its reference ranking, and dispersion φ. This estimator can be useful for inference over the posterior p(r|v) = p(r)1[r ∈ Ω(v)]/p(v) for r ∈ Ω(v). For any ﬁxed v, let f (s) = r∈Ω(v) φd(r,s). Then we have

1

1

p(v) = nZφ

|Ω(v )| f (s)

∈N s∈Ω(v )

1

= nZφ

E f (s),
s∼Ω(v )

∈N

where s is drawn uniformly from Ω(v ). One can estimate the expectation by importance sampling. Suppose we draw, for each , rankings s(1), . . . , s(T ) using AMP(v , σ, φ = 1)
to approximate uniform sampling (e.g., choose some ranking σ from Ω(v )). Let w t = 1/Pˆv (s(t)), which has a closed-form given by Equation 15. Then the estimate is

1n pˆ(v) =
nZφ =1

Tt=1 w tf (s(t)) .

T t=1

w

t

Evaluating f (s(t)) is generally intractable, but again, it can be approximated using our earlier techniques, as given by Equation 27. In summary, we can realize non-parametric estimation using a nested sampling procedure to ﬁrst approximate the outer expectation over s, followed by the inner summation f (s).

7. Conclusion and Future Work
We have developed a set of algorithms to support the eﬃcient and eﬀective learning of ranking or preference distributions when the observed data comprise a set of unrestricted pairwise comparisons of alternatives. Given the fundamental nature of pairwise comparisons in revealed preference, our methods extend the reach of rank learning in a vital way. One of our main technical contributions, the GRIM algorithm, allows sampling of arbitrary distributions, including Mallows models conditioned on pairwise data. It supports a tractable approximation to the #P-hard problem of log-likelihood evaluation of Mallows mixtures; and it forms the heart of an EM algorithm that was shown to be quite eﬀective in our experiments. GRIM can also be used for non-parametric estimation.
We are pursuing a number of interesting directions, including various extensions and applications of the model we have developed here. One of the weaknesses with Mallows is its lack of ﬂexibility in various dimensions, such as allowing diﬀerent dispersion “rates” in diﬀerent regions of the ranking. Models that allow more ﬂexibility while controlling for overﬁtting could lead to more realistic ranking models for real-world settings. Other extensions include exploration of other probabilistic models of incomplete preferences that employ diﬀerent distributions over rankings, such as Plackett-Luce or weighted Mallows; that account for noisy comparison data from users; and that account for data that is not missing at random—this may occur, say, in settings in which a bias exists towards observing preferences for higher ranked alternatives.

4005

Lu and Boutilier
In another vein, we are interested in exploiting learned preference models of the type developed here for decision-theoretic tasks in social choice or personalized recommendation. Learned preferences can be leveraged in both active preference elicitation (e.g., in social choice or group decision making (Lu and Boutilier, 2011)), or in passive (purely observational) settings. It would also be interesting to apply GRIM to other posterior distributions such as energy models, and to compare it to diﬀerent MCMC techniques like chain ﬂipping (Dellaert et al., 2003).
Acknowledgments
This research was supported by the the Natural Sciences and Engineering Research Council (NSERC) of Canada. Tyler Lu was supported by a Google Ph.D. Fellowship.
References
Shivani Agarwal and Dan Roth. Learnability of bipartite ranking functions. In Proceedings of the Eighteenth Annual Conference on Learning Theory (COLT-05), pages 16–31, 2005.
John Bartholdi III, Craig Tovey, and Michael Trick. Voting schemes for which it can be diﬃcult to tell who won the election. Social Choice and Welfare, 6(2):157–165, 1989.
Christophe Biernacki and Julien Jacques. A generative model for rank data based on insertion sort algorithm. Computational Statistics and Data Analysis, 58:162–176, 2013.
Graham Brightwell and Peter Winkler. Counting linear extensions is #P-complete. In ACM Symposium on the Theory of Computing, pages 175–181, 1991.
Chris J.C. Burges. From RankNet to LambdaRank to LambdaMART: An overview. Technical Report MSR-TR-2010-82, Microsoft Research, July 2010.
Christopher J. C. Burges, Tal Shaked, Erin Renshaw, Ari Lazier, Matt Deeds, Nicole Hamilton, and Gregory N. Hullender. Learning to rank using gradient descent. In Proceedings of the Twenty-Second International Conference on Machine Learning (ICML-05), pages 89–96, Bonn, Germany, 2005.
R´obert Busa-Fekete, Eyke Hu¨llermeier, and Bal´azs Sz¨or´enyi. Preference-based rank elicitation using statistical models: The case of Mallows. In Proceedings of the Thirty-ﬁrst International Conference on Machine Learning (ICML-14), pages 1071–1079, Beijing, China, 2014.
Ludwig M. Busse, Peter Orbanz, and Joachim M. Buhmann. Cluster analysis of heterogeneous rank data. In Proceedings of the Twenty-Fourth International Conference on Machine Learning (ICML-07), pages 113–120, Corvallis, OR, 2007.
Weiwei Cheng, Krzysztof Dembczynski, and Eyke Hu¨llermeier. Label ranking methods based on the Plackett-Luce model. In Proceedings of the Twenty-Seventh International Conference on Machine Learning (ICML-10), pages 215–222, Haifa, Israel, 2010.
4006

Learning Mallows Models with Pairwise Preferences
William W. Cohen, Robert E. Schapire, and Yoram Singer. Learning to order things. Journal of Artiﬁcial Intelligence Research, 10:243–270, 1999.
marquis de (Marie Jean Antoine Nicolas de Caritat) Condorcet. Essai sur l’Application de l’Analyse a la Probabilite des Decisions rendues a la Probabilite des Voix. Paris: L’Imprimerie Royale, 1785.
Vincent Conitzer and Tuomas Sandholm. Common voting rules as maximum likelihood estimators. In Proceedings of the Twenty-First Conference on Uncertainty in Artiﬁcial Intelligence (UAI-05), pages 145–152, Edinburgh, 2005.
Vincent Conitzer, Matthew Rognlie, and Lirong Xia. Preference functions that score rankings and maximum likelihood estimation. In Proceedings of the Twenty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-09), pages 109–115, Pasadena, CA, 2009.
Don Coppersmith, Lisa Fleischer, and Atri Rudra. Ordering by weighted number of wins gives a good ranking for weighted tournaments. In Proceedings of the Seventeenth Annual ACM-SIAM Symposium on Discrete Algorithms (SODA-06), pages 776–782, Miami, FL, 2006.
Frank Dellaert, Steven M. Seitz, Charles E. Thorpe, and Sebastian Thrun. EM, MCMC, and chain ﬂipping for structure from motion with unknown correspondence. Machine Learning, 50(1–2):45–71, 2003.
A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society, Series B, 39(1):1–38, 1977.
Persi Diaconis. Group Representations in Probability and Statistics, volume 11 of Lecture Notes–Monograph Series. Institute of Mathematical Statistics, Hayward, CA, 1988.
Jean-Paul Doignon, Aleksandar Pekec, and Michel Regenwetter. The repeated insertion model for rankings: Missing link between two subset choice models. Psychometrika, 69 (1):33–54, 2004.
Cynthia Dwork, Ravi Kumar, Moni Naor, and D. Sivakumar. Rank aggregation methods for the web. In Proceedings of the Tenth International World Wide Web Conference (WWW-01), pages 613–622, Hong Kong, 2001. ACM.
M. A. Fligner and J. S. Verducci. Distance based ranking models. Journal of the Royal Statistical Society. Series B (Methodological), 48(3):359–369, 1986. ISSN 00359246.
Michael A. Fligner and Joseph S. Verducci, editors. Probability Models and Statistical Analysis for Ranking Data, volume 80 of Lecture Notes in Statistics. New York: SpringerVerlag, 1993.
Brian Francis, Regina Dittrich, Reinhold Hatzinger, and Les Humphreys. A mixture model for longitudinal partially ranked data. Communications in Statistics — Theory and Methods, 43(4):722–734, 2014.
4007

Lu and Boutilier
Yoav Freund, Raj D. Iyer, Robert E. Schapire, and Yoram Singer. An eﬃcient boosting algorithm for combining preferences. Journal of Machine Learning Research, 4:933–969, 2003.
Zoubin Ghahramani and Mike I. Jordan. Learning from incomplete data. Technical report, Lab Memo No. 1509, CBCL Paper No. 108, MIT AI Lab, 1995.
Isobel Claire Gormley and Thomas Brendan Murphy. A latent space model for rank data. In Edoardo M. Airoldi, David M. Blei, Stephen E. Fienberg, Anna Goldenberg, Eric P. Xing, and Alice X. Zheng, editors, Statistical Network Analysis: Models, Issues, and New Directions, pages 90–102. Springer, 2007.
Isobel Claire Gormley and Thomas Brendan Murphy. Exploring voting blocs within the Irish electorate. Journal of the American Statistical Association, 103(483):1014–1027, 2008.
John Guiver and Edward Snelson. Bayesian inference for Plackett-Luce ranking models. In Proceedings of the Twenty-Sixth International Conference on Machine Learning (ICML09), pages 377–384, Montreal, 2009. ISBN 978-1-60558-516-1.
Jonathan Huang and Carlos Guestrin. Riﬄed independence for ranked data. In Advances in Neural Information Processing Systems 21, pages 799–807, Vancouver, 2009.
Jonathan Huang, Ashish Kapoor, and Carlos Guestrin. Riﬄed independence for eﬃcient inference with partial ranking. Journal of Artiﬁcial Intelligence Research, 44:491–532, 2012.
Eyke Hu¨llermeier, Johannes Fu¨rnkranz, Weiwei Cheng, and Klaus Brinker. Label ranking by learning pairwise preferences. Artiﬁcial Intelligence, 172(16–17):1897–1916, 2008.
Thorsten Joachims. Optimizing search engines using clickthrough data. In KDD, pages 133–142, 2002.
Toshihiro Kamishima, Hideto Kazawa, and Shotaro Akaho. Supervised ordering: An empirical survey. In IEEE International Conference on Data Mining (ICDM-05), pages 673–676, Houston, TX, 2005.
John G. Kemeny. Mathematics without numbers. Daedalus, 88(4):577–591, 1959.
Guy Lebanon and Yi Mao. Non-parametric modeling of partially ranked data. Journal of Machine Learning Research, 9:2401–2429, 2008.
Stuart P. Lloyd. Least squares quantization in PCM. IEEE Transactions on Information Theory, 28(2):129–137, March 1982.
Jordan J. Louviere, David A. Hensher, and Joﬀre D. Swait. Stated Choice Methods: Analysis and Application. Cambridge University Press, Cambridge, 2000.
Tyler Lu and Craig Boutilier. Robust approximation and incremental elicitation in voting protocols. In Proceedings of the Twenty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-11), pages 287–293, Barcelona, 2011.
4008

Learning Mallows Models with Pairwise Preferences
R. Duncan Luce. Individual Choice Behavior: A Theoretical Analysis. Wiley, 1959. Colin L. Mallows. Non-null ranking models. In Biometrika, volume 44, pages 114–130,
1957. John I. Marden. Analyzing and Modeling Rank Data. Chapman and Hall, London, 1995. Benjamin M. Marlin and Richard S. Zemel. Collaborative ﬁltering and the missing at
random assumption. In Proceedings of the Twenty-Third Conference on Uncertainty in Artiﬁcial Intelligence (UAI-07), pages 50–54, Vancouver, 2007. Marina Meila and Harr Chen. Dirichlet process mixtures of generalized Mallows models. In Proceedings of the Twenty-Sixth Conference on Uncertainty in Artiﬁcial Intelligence (UAI-10), pages 358–367, Catalina Island, CA, USA, 2010. Thomas Brendan Murphy and Donal Martin. Mixtures of distance-based models for ranking data. Computational Statistics and Data Analysis, 41:645–655, January 2003. Radford Neal and Geoﬀ Hinton. A view of the EM algorithm that justiﬁes incremental, sparse, and other variants. In M. Jordan, editor, Learning in Graphical Models, pages 355–368. MIT Press, Cambridge, MA, 1999. R. Plackett. The analysis of permutations. Applied Statistics, 24:193–202, 1975. T. Graepel R. Herbrich and K. Obermayer. Large margin rank boundaries for ordinal regression. In Peter J. Bartlett, Bernhard Sch¨olkopf, Dale Schuurmans, and Alex J. Smola, editors, Advances in Large Margin Classiﬁers, pages 115–132. MIT Press, 2000. Cynthia Rudin. The p-norm push: A simple convex ranking algorithm that concentrates at the top of the list. Journal of Machine Learning Research, 10:2233–2271, 2009. Grace S. Shieh. A weighted Kendall’s tau statistic. Statistics and Probability Letters, 39 (1):17–24, 1998. Maksims Volkovs and Richard S. Zemel. Boltzrank: Learning to maximize expected ranking gain. In Proceedings of the Twenty-Sixth International Conference on Machine Learning (ICML-09), page 137, Montreal, 2009. Maksims Volkovs and Richard S. Zemel. Eﬃcient sampling for bipartite matching problems. In Advances in Neural Information Processing Systems 25 (NIPS-12), pages 1322–1330, Lake Tahoe, NV, 2012. Peyton Young. Optimal voting rules. Journal of Economic Perspectives, 9:51–64, 1995.
4009

