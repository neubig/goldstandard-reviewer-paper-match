arXiv:1209.3672v3 [math.ST] 1 Jul 2014

1-Bit Matrix Completion
Mark A. Davenport, Yaniv Plan, Ewout van den Berg, Mary Wootters∗
September 2012 (Revised May 2014)
Abstract In this paper we develop a theory of matrix completion for the extreme case of noisy 1-bit observations. Instead of observing a subset of the real-valued entries of a matrix M , we obtain a small number of binary (1-bit) measurements generated according to a probability distribution determined by the realvalued entries of M . The central question we ask is whether or not it is possible to obtain an accurate estimate of M from this data. In general this would seem impossible, but we show that the maximum likelihood estimate under a suitable constraint returns an accurate estimate of M when M ∞ ≤ α and rank(M ) ≤ r. If the log-likelihood is a concave function (e.g., the logistic or probit observation models), then we can obtain this maximum likelihood estimate by optimizing a convex program. In addition, we also show that if instead of recovering M we simply wish to obtain an estimate of the distribution generating the 1-bit measurements, then we can eliminate the requirement that M ∞ ≤ α. For both cases, we provide lower bounds showing that these estimates are near-optimal. We conclude with a suite of experiments that both verify the implications of our theorems as well as illustrate some of the practical applications of 1-bit matrix completion. In particular, we compare our program to standard matrix completion methods on movie rating data in which users submit ratings from 1 to 5. In order to use our program, we quantize this data to a single bit, but we allow the standard matrix completion program to have access to the original ratings (from 1 to 5). Surprisingly, the approach based on binary data performs signiﬁcantly better.
1 Introduction
The problem of recovering a matrix from an incomplete sampling of its entries—also known as matrix completion—arises in a wide variety of practical situations. In many of these settings, however, the observations are not only incomplete, but also highly quantized, often even to a single bit. In this paper we consider a statistical model for such data where instead of observing a real-valued entry as in the original matrix completion problem, we are now only able to see a positive or negative rating. This binary output is generated according to a probability distribution which is parameterized by the corresponding entry of the unknown low-rank matrix M . The central question we ask in this paper is: “Given observations of this form, can we recover the underlying matrix?”
Questions of this form are often asked in the context of binary PCA or logistic PCA. There are a number of compelling algorithmic papers on these subjects, including [19, 22, 38, 57, 64], which suggest positive answers on simulated and real-world data. In this paper, we give the ﬁrst theoretical accuracy guarantees under a generalized linear model. We show that O(rd) binary observations are suﬃcient to accurately recover a d × d, rank-r matrix by convex programming. Our theory is inspired by the unquantized matrix completion problem and the closely related problem of 1-bit compressed sensing, described below.
∗M.A. Davenport is with the School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, GA. Email: {mdav@gatech.edu}.
Y. Plan is with the Department of Mathematics, University of Michigan, Ann Arbor, MI. Email: {yplan@umich.edu}. E. van den Berg is with the IBM T.J. Watson Research Center, Yorktown Heights, NY. Email: {evandenberg@us.ibm.com}. M. Wootters is with the Department of Mathematics, University of Michigan, Ann Arbor, MI. Email: {wootters@umich.edu}. This work was partially supported by NSF grants DMS-0906812, DMS-1004718, DMS-1103909, CCF-0743372, and CCF1350616 and NRL grant N00173-14-2-C001.
1

1.1 Matrix completion
Matrix completion arises in a wide variety of practical contexts, including collaborative ﬁltering [28], system identiﬁcation [45], sensor localization [7, 59, 60], rank aggregation [26], and many more. While many of these applications have a relatively long history, recent advances in the closely related ﬁeld of compressed sensing [13, 21, 23] have enabled a burst of progress in the last few years, and we now have a strong base of theoretical results concerning matrix completion [15–17, 25, 30, 36, 37, 39–42, 49, 55, 56]. A typical result from this literature is that a generic d × d matrix of rank r can be exactly recovered from O(r d polylog(d)) randomly chosen entries. Similar results can be established in the case of noisy observations and approximately low-rank matrices [15, 25, 37, 39–42, 49, 56].
Although these results are quite impressive, there is an important gap between the statement of the problem as considered in the matrix completion literature and many of the most common applications discussed therein. As an example, consider collaborative ﬁltering and the now-famous “Netﬂix problem.” In this setting, we assume that there is some unknown matrix whose entries each represent a rating for a particular user on a particular movie. Since any user will rate only a small subset of possible movies, we are only able to observe a small fraction of the total entries in the matrix, and our goal is to infer the unseen ratings from the observed ones. If the rating matrix has low rank, then this would seem to be the exact problem studied in the matrix completion literature. However, there is a subtle diﬀerence: the theory developed in this literature generally assumes that observations consist of (possibly noisy) continuousvalued entries of the matrix, whereas in the Netﬂix problem the observations are “quantized” to the set of integers between 1 and 5. If we believe that it is possible for a user’s true rating for a particular movie to be, for example, 4.5, then we must account for the impact of this “quantization noise” on our recovery. Of course, one could potentially treat quantization simply as a form of bounded noise, but this is somewhat unsatisfying because the ratings aren’t just quantized — there are also hard limits placed on the minimum and maximum allowable ratings. (Why should we suppose that a movie given a rating of 5 could not have a true underlying rating of 6 or 7 or 10?) The inadequacy of standard matrix completion techniques in dealing with this eﬀect is particularly pronounced when we consider recommender systems where each rating consists of a single bit representing a positive or negative rating (consider for example rating music on Pandora, the relevance of advertisements on Hulu, or posts on sites such as MathOverﬂow). In such a case, the assumptions made in the existing theory of matrix completion do not apply, standard algorithms are ill-posed, and alternative theory is required.
1.2 Statistical learning theory and matrix completion
While the theory of matrix completion gained quite a bit of momentum following advances in compressed sensing, earlier results from Srebro et al. [62, 63] also addressed this problem from a slightly diﬀerent perspective rooted in the framework of statistical learning theory. These results also deal with the binary setting that we consider here. They take a model-free approach and prove generalization error bounds; that is, they give conditions under which good agreement on the observed data implies good agreement on the entire matrix. For example, in [63], agreement is roughly measured via the fraction of predicted signs that are correct, but this can also be extended to other notions of agreement [62]. From the perspective of statistical learning theory, this corresponds to bounding the generalization error under various classes of loss functions.
One important diﬀerence between the statistical learning approach and the path taken in this paper is that here we focus on parameter estimation — that is, we seek to recover the matrix M itself (or the distribution parameterized by M that governs our observations) — although we also prove generalization error bounds en route to our main results on parameter and distribution recovery. We discuss the relationship between our approach and the statistical learning approach in more detail in Section A.1 (see Remark 3). Brieﬂy, our generalization error bounds correspond to the case where the loss function is the log likelihood, and do not seem to ﬁt directly within the framework of the existing literature.
2

1.3 1-bit compressed sensing and sparse logistic regression
As noted above, matrix completion is closely related to the ﬁeld of compressed sensing, where a theory to deal with single-bit quantization has recently been developed [10, 32, 33, 43, 51, 52]. In compressed sensing, one can recover an s-sparse vector in Rd from O(s log(d/s)) random linear measurements—several diﬀerent random measurement structures are compatible with this theory. In 1-bit compressed sensing, only the signs of these measurements are observed, but an s-sparse signal can still be approximately recovered from the same number of measurements [1, 33, 51, 52]. However, the only measurement ensembles which are currently known to give such guarantees are Gaussian or sub-Gaussian [1], and are thus of a quite diﬀerent ﬂavor than the kinds of samples obtained in the matrix completion setting. A similar theory is available for the closely related problem of sparse binomial regression, which considers more classical statistical models [2, 11, 34, 46, 48, 52, 54, 65] and allows non-Gaussian measurements. Our aim here is to develop results for matrix completion of the same ﬂavor as 1-bit compressed sensing and sparse logistic regression.

1.4 Challenges

In this paper, we extend the theory of matrix completion to the case of 1-bit observations. We consider a general observation model but focus mainly on two particular possibilities: the models of logistic and probit regression. We discuss these models in greater detail in Section 2.1, but ﬁrst we note that several new challenges arise when trying to leverage results in 1-bit compressed sensing and sparse logistic regression to develop a theory for 1-bit matrix completion. First, matrix completion is in some sense a more challenging problem than compressed sensing. Speciﬁcally, some additional diﬃculty arises because the set of low-rank matrices is “coherent” with single entry measurements (see [30]). In particular, the sampling operator does not act as a near-isometry on all matrices of interest, and thus the natural analogue to the restricted isometry property from compressed sensing cannot hold in general—there will always be certain low-rank matrices that we cannot hope to recover without essentially sampling every entry of the matrix. For example, consider a matrix that consists of a single nonzero entry (which we might never observe). The typical way to deal with this possibility is to consider a reduced set of low-rank matrices by placing restrictions on the entry-wise maximum of the matrix or its singular vectors—informally, we require that the matrix is not too “spiky”.
We introduce an entirely new dimension of ill-posedness by restricting ourselves to 1-bit observations. To illustrate this, we describe one version of 1-bit matrix completion in more detail (the general problem deﬁnition is given in Section 2.1 below). Consider a d × d matrix M with rank r. Suppose we observe a subset Ω of entries of a matrix Y . The entries of Y depend on M in the following way:

Yi,j = +1 if Mi,j + Zi,j ≥ 0

(1)

−1 if Mi,j + Zi,j < 0

where Z is a matrix containing noise. This latent variable model is the direct analogue to the usual 1bit compressed sensing observation model. In this setting, we view the matrix M as more than just a parameter of the distribution of Y ; M represents the real underlying quantity of interest that we would like to estimate. Unfortunately, in what would seem to be the most benign setting—when Ω is the set of all entries, Z = 0, and M has rank 1 and a bounded entry-wise maximum—the problem of recovering M is ill-posed. To see this, let M = uv∗ for any vectors u, v ∈ Rd, and for simplicity assume that there are no zero entries in u or v. Now let u and v be any vectors with the same sign pattern as u and v respectively. It is apparent that both M and M = uv∗ will yield the same observations Y , and thus M and M are indistinguishable. Note that while it is obvious that this 1-bit measurement process will destroy any information we have regarding the scaling of M , this ill-posedness remains even if we knew something about the scaling a priori (such as the Frobenius norm of M ). For any given set of observations, there will always be radically diﬀerent possible matrices that are all consistent with observed measurements.
After considering this example, the problem might seem hopeless. However, an interesting surprise is that when we add noise to the problem (that is, when Z = 0 is an appropriate stochastic matrix) the

3

picture completely changes—this noise has a “dithering” eﬀect and the problem becomes well-posed. In fact, we will show that in this setting we can sometimes recover M to the same degree of accuracy that is possible when given access to completely unquantized measurements! In particular, under appropriate conditions, O(rd) measurements are suﬃcient to accurately recover M .

1.5 Applications
The problem of 1-bit matrix completion arises in nearly every application that has been proposed for “unquantized” matrix completion. To name a few:
• Recommender systems: As mentioned above, collaborative ﬁltering systems often involve discretized recommendations [28]. In many cases, each observation will consist simply of a “thumbs up” or “thumbs down” thus delivering only 1 bit of information (consider for example rating music on Pandora, the relevance of advertisements on Hulu, or posts on sites such as MathOverﬂow). Such cases are a natural application for 1-bit matrix completion.
• Analysis of survey data: Another potential application for matrix completion is to analyze incomplete survey data. Such data is almost always heavily quantized since people are generally not able to distinguish between more than 7 ± 2 categories [47]. 1-bit matrix completion provides a method for analyzing incomplete (or potentially even complete) survey designs containing simple yes/no or agree/disagree questions.
• Distance matrix recovery and multidimensional scaling: Yet another common motivation for matrix completion is to localize nodes in a sensor network from the observation of just a few inter-node distances [7, 59, 60]. This is essentially a special case of multidimensional scaling (MDS) from incomplete data [8]. In general, work in the area assumes real-valued measurements. However, in the sensor network example (as well as many other MDS scenarios), the measurements may be very coarse and might only indicate whether the nodes are within or outside of some communication range. While there is some existing work on MDS using binary data [29] and MDS using incomplete observations with other kinds of non-metric data [61], 1-bit matrix completion promises to provide a principled and unifying approach to such problems.
• Quantum state tomography: Low-rank matrix recovery from incomplete observations also has applications to quantum state tomography [31]. In this scenario, mixed quantum states are represented as Hermitian matrices with nuclear norm equal to 1. When the state is nearly pure, the matrix can be well approximated by a low-rank matrix and, in particular, ﬁts the model given in Section 2.2 up to a rescaling. Furthermore, Pauli-operator-based measurements give probabilistic binary outputs. However, these are based on the inner products with the Pauli matrices, and thus of a slightly diﬀerent ﬂavor than the measurements considered in this paper. Nevertheless, while we do not address this scenario directly, our theory of 1-bit matrix completion could easily be adapted to quantum state tomography.

1.6 Notation

We now provide a brief summary of some of the key notation used in this paper. We use [d] to denote the

set of integers {1, . . . , d}. We use capital boldface to denote a matrix (e.g., M ) and standard text to denote

its entries (e.g., Mi,j). Similarly, we let 0 denote the matrix of all-zeros and 1 the matrix of all-ones. We

let M denote the operator norm of M , M F = i,j Mi2,j denote the Frobenius norm of M , M ∗

denote the nuclear or Schatten-1 norm of M (the sum of the singular values), and M ∞ = maxi,j |Mi,j| denote the entry-wise inﬁnity-norm of M . We will use the Hellinger distance, which, for two scalars

p, q ∈ [0, 1], is given by

d2 (p, q) := (√p − √q)2 + ( 1 − p − 1 − q)2.
H

4

This gives a standard notion of distance between two binary probability distributions. We also allow the

Hellinger distance to act on matrices via the average Hellinger distance over their entries: for matrices

P , Q ∈ [0, 1]d1×d2, we deﬁne

d2 (P , Q) = 1
H

d2H (Pi,j , Qi,j ).

d1d2 i,j

Finally, for an event E ,1[E] is the indicator function for that event, i.e., 1[E] is 1 if E occurs and 0 otherwise.

1.7 Organization of the paper
We proceed in Section 2 by describing the 1-bit matrix completion problem in greater detail. In Section 3 we state our main results. Speciﬁcally, we propose a pair of convex programs for the 1-bit matrix completion problem and establish upper bounds on the accuracy with which these can recover the matrix M and the distribution of the observations Y . We also establish lower bounds, showing that our upper bounds are nearly optimal. In Section 4 we describe numerical implementations of our proposed convex programs and demonstrate their performance on a number of synthetic and real-world examples. Section 5 concludes with a brief discussion of future directions. The proofs of our main results are provided in the appendix.

2 The 1-bit matrix completion problem

2.1 Observation model
We now introduce the more general observation model that we study in this paper. Given a matrix M ∈ Rd1×d2, a subset of indices Ω ⊂ [d1] × [d2], and a diﬀerentiable function f : R → [0, 1], we observe

Yi,j = +1 with probability f (Mi,j),

for (i, j) ∈ Ω.

(2)

−1 with probability 1 − f (Mi,j)

We will leave f general for now and discuss a few common choices just below. As has been important in previous work on matrix completion, we assume that Ω is chosen at random with E |Ω| = n. Speciﬁcally, we assume that Ω follows a binomial model in which each entry (i, j) ∈ [d1] × [d2] is included in Ω with probability d1nd2 , independently.
Before discussing some particular choices for f , we ﬁrst note that while the observation model described in (2) may appear on the surface to be somewhat diﬀerent from the setup in (1), they are actually equivalent if f behaves likes a cumulative distribution function. Speciﬁcally, for the model in (1), if Z has i.i.d. entries, then by setting f (x) := P (Z1,1 ≥ −x), the model in (1) reduces to that in (2). Similarly, for any choice of f (x) in (2), if we deﬁne Z as having i.i.d. entries drawn from a distribution whose cumulative distribution function is given by FZ(x) = P (z ≤ x) = 1 − f (−x), then (2) reduces to (1). Of course, in any given situation one of these observation models may seem more or less natural than the other—for example, (1) may seem more appropriate when M is viewed as a latent variable which we might be interested in estimating, while (2) may make more sense when M is viewed as just a parameter of a distribution. Ultimately, however, the two models are equivalent.
We now consider two natural choices for f (or equivalently, for Z):

Example 1 (Logistic regression/Logistic noise). The logistic regression model, which is common in statis-

tics,

is

captured

by

(2)

with

f (x)

=

ex 1+ex

and

by

(1)

with

Zi,j

i.i.d.

according

to

the

standard

logistic

distribution.

Example 2 (Probit regression/Gaussian noise). The probit regression model is captured by (2) by setting
f (x) = 1 − Φ(−x/σ) = Φ(x/σ) where Φ is the cumulative distribution function of a standard Gaussian and by (1) with Zi,j i.i.d. according to a mean-zero Gaussian distribution with variance σ2.

5

2.2 Approximately low-rank matrices

The majority of the literature on matrix completion assumes that the ﬁrst r singular values of M are
nonzero and the remainder are exactly zero. However, in many applications the singular values instead
exhibit only a gradual decay towards zero. Thus, in this paper√we allow a relaxation of the assumption that M has rank exactly r. Instead, we assume that M ∗ ≤ α rd1d2, where α is a parameter left to be determined, but which will often be of constant order. In other words, the singular values of M belong to a scaled 1 ball. In compressed sensing, belonging to an p ball with p ∈ (0, 1] is a common relaxation of exact sparsity; in matrix completion, the nuclear-norm ball (or Schatten-1 ball) plays an analogous role.
See [18] for further compelling reasons√to use the nuclear-norm ball relaxation. The particular choice of scaling, α rd1d2, arises from the following considerations. Suppose that each
entry of M is bounded in magnitude by α and that rank(M ) ≤ r. Then

√ M ∗≤ r M F ≤

rd1d2 M ∞ ≤ α rd1d2.

√ Thus, the assumption that M ∗ ≤ α rd1d2 is a relaxation of the conditions that rank(M ) ≤ r and
M ∞ ≤ α. The condition that M ∞ ≤ α essentially means that the probability of seeing a +1 or −1 does not depend on the dimension. It is also a way of enforcing that M should not be too “spiky”;

as discussed above this is an important assumption in order to make the recovery of M well-posed (e.g.,

see [49]).

3 Main results
We now state our main results. We will have two goals—the ﬁrst is to accurately recover M itself, and the second is to accurately recover the distribution of Y given by f (M ).1 All proofs are contained in the appendix.

3.1 Convex programming
In order to approximate either M or f (M ), we will maximize the log-likelihood function of the optimization variable X given our observations subject to a set of convex constraints. In our case, the log-likelihood function is given by

LΩ,Y (X) :=

1[Yi,j=1] log(f (Xi,j )) + 1[Yi,j=−1] log(1 − f (Xi,j )) .

(i,j)∈Ω

To recover M , we will use the solution to the following program:

M = arg max LΩ,Y (X) subject to

X ∗ ≤ α rd1d2 and X ∞ ≤ α.

(3)

X

To recover the distribution f (M ), we need not enforce the inﬁnity-norm constraint, and will use the following simpler program:

M = arg max LΩ,Y (X) subject to

X ∗ ≤ α rd1d2

(4)

X

In many cases, LΩ,Y (X) is a concave function and thus the above programs are convex. This can be easily checked in the case of the logistic model and can also be veriﬁed in the case of the probit model (e.g., see [68]).

1Strictly speaking, f (M ) ∈ [0, 1]d1×d2 is simply a matrix of scalars, but these scalars implicitly deﬁne the distribution of Y , so we will sometimes abuse notation slightly and refer to f (M ) as the distribution of Y .

6

3.2 Recovery of the matrix

We now state our main result concerning the recovery of the matrix M . As discussed in Section 1.4 we place a “non-spikiness” condition on M to make recovery possible; we enforce this with an inﬁnity-norm constraint. Further, some assumptions must be made on f for recovery of M to be feasible. We deﬁne two quantities Lα and βα which control the “steepness” and “ﬂatness” of f , respectively:

|f (x)|

f (x)(1 − f (x))

Lα

:=

sup
|x|≤α

f (x)(1

−

f (x))

and

βα := sup
|x|≤α

(f (x))2

.

(5)

In this paper we will restrict our attention to f such that Lα and βα are well-deﬁned. In particular, we assume that f and f are non-zero in [−α, α]. This assumption is fairly mild—for example, it includes the
logistic and probit models (as we will see below in Remark 1). The quantity Lα appears only in our upper bounds, but it is generally well behaved. The quantity βα appears both in our upper and lower bounds. Intuitively, it controls the “ﬂatness” of f in the interval [−α, α]—the ﬂatter f is, the larger βα is. It is clear that some dependence on βα is necessary. Indeed, if f is perfectly ﬂat, then the magnitudes of the entries of M cannot be recovered, as seen in the noiseless case discussed in Section 1.4. Of course, when α
is a ﬁxed constant and f is a ﬁxed function, both Lα and βα are bounded by ﬁxed constants independent of the dimension.
√ Theorem 1. Assume that M ∗ ≤ α d1d2r and M ∞ ≤ α. Suppose that Ω is chosen at random following the binomial model of Section 2.1 with E |Ω| = n. Suppose that Y is generated as in (2). Let Lα
and βα be as in (5). Consider the solution M to (3). Then with probability at least 1 − C1/(d1 + d2),

d11d2 M − M 2F ≤ Cα

r(d1 + d2) n

1 + (d1 + d2) log(d1d2) n

with Cα := C2αLαβα. If n ≥ (d1 + d2) log(d1d2) then this simpliﬁes to

1 M − M 2 ≤ √2Cα r(d1 + d2) .

(6)

d1d2

F

n

Above, C1 and C2 are absolute constants.

Note that the theorem also holds if Ω = [d1]×[d2], i.e., if we sample each entry exactly once or observe a complete realization of Y . Even in this context, the ability to accurately recover M is somewhat surprising.

Remark 1 (Recovery in the logistic and probit models). The logistic model satisﬁes the hypotheses of

Theorem

1

with

βα

=

(1+eα)2 eα

≈

eα

and

Lα

=

1.

The

probit

model

has

βα

≤

c1

σ

2

e

α2 2σ2

and

Lα

≤

c2

α σ

+

1

σ

where we can take c1 = π and c2 = 8. In particular, in the probit model the bound in (6) reduces to

1 M − M 2 ≤ C α + 1 exp α2 σα r(d1 + d2) .

(7)

d1d2

F

σ

2σ2

n

Hence, when σ < α, increasing the size of the noise leads to signiﬁcantly improved error bounds—this is not an artifact of the proof. We will see in Section 3.4 that the exponential dependence on α in the logistic model (and on α/σ in the probit model) is intrinsic to the problem. Intuitively, we should expect this since for such models, as M ∞ grows large, we can essentially revert to the noiseless setting where estimation of M is impossible. Furthermore, in Section 3.4 we will also see that when α (or α/σ) is bounded by a constant, the error bound (6) is optimal up to a constant factor. Fortunately, in many applications, one would expect α to be small, and in particular to have little, if any, dependence on the dimension. This ensures that each measurement will always have a non-vanishing probability of returning 1 as well as a non-vanishing probability of returning −1.

7

Remark 2 (Nuc√lear norm constraint). The assumption made in Theorem 1 (as well as Theorem 2 below) that M ∗ ≤ α d1d2r does not mean that we are requiring the matrix M to be low rank. We express this constraint in terms of r to aid the intuition of researchers well-versed in the existing literature on low-rank
matrix recovery. (If M is exactly rank r and satisﬁes M ∞ ≤ α, then as discussed in Section 2.2, M
w√ill automatically satisfy this constraint.) If one desires, one may simplify the presentation by replacing α d1d2r with a parameter λ and simply requiring M ∗ ≤ λ, in which case (6) reduces to

d2H (f (M ), f (M )) ≤ C3Lαβα

λ min(d1, d2) · n

for a numerical constant C3.

3.3 Recovery of the distribution
In many situations, we might not be interested in the underlying matrix M , but rather in determining the distribution of the entries of Y . For example, in recommender systems, a natural question would be to determine the likelihood that a user would enjoy a particular unrated item.
Surprisingly, this distribution may be accurately recovered without any restriction on the inﬁnity-norm of M . This may be unexpected to those familiar with the matrix completion literature in which “nonspikiness” constraints seem to be unavoidable. In fact, we will show in Section 3.4 that the bound in Theorem 2 is near-optimal; further, we will show that even under the added constraint that M ∞ ≤ α, it would be impossible to estimate f (M ) signiﬁcantly more accurately.
√ Theorem 2. Assume that M ∗ ≤ α d1d2r. Suppose that Ω is chosen at random following the binomial model of Section 2.1 with E |Ω| = n. Suppose that Y is generated as in (2), and let L = limα→∞ Lα. Let M be the solution to (4). Then with probability at least 1 − C1/(d1 + d2),

d2H (f (M ), f (M )) ≤ C2αL r(d1 n+ d2) 1 + (d1 + d2)nlog(d1d2) . (8)

Furthermore, as long as n ≥ (d1 + d2) log(d1d2), we have

d2 (f (M ), f (M )) ≤ √2C2αL r(d1 + d2) .

(9)

H

n

Above, C1 and C2 are absolute constants.

While L = 1 for the logistic model, the astute reader will have noticed that for the probit model L is unbounded—that is, Lα tends to ∞ as α → ∞. L would also be unbounded for the case where f (x) takes values of 1 or 0 outside of some range (as would be the case in (1) if the distribution of the noise had compact support). Fortunately, however, we can recover a result for these cases by enforcing an inﬁnitynorm constraint, as described in Theorem 6 below. Moreover, for a large class of functions, f , L is indeed bounded. For example, in the latent variable version of (1) if the entries Zi,j are at least as fat-tailed as an exponential random variable, then L is bounded. To be more precise, suppose that f is continuously diﬀerentiable and for simplicity assume that the distribution of Zi,j is symmetric and |f (x)| /(1 − f (x)) is monotonic for x suﬃciently large. If P (|Zi,j| ≥ t) ≥ C exp(−ct) for all t ≥ 0, then one can show that L is ﬁnite. This property is also essentially equivalent to the requirement that a distribution have bounded hazard rate. As noted above, this property holds for the logistic distribution, but also for many other common distributions, including the Laplacian, Student’s t, Cauchy, and others.

3.4 Room for improvement?
We now discuss the extent to which Theorems 1 and 2 are optimal. We give three theorems, all proved using information theoretic methods, which show that these results are nearly tight, even when some of

8

our assumptions are relaxed. Theorem 3 gives a lower bound to nearly match the upper bound on the

error in recovering M derived in Theorem 1. Theorem 4 compares our upper bounds to those available

without discretization and shows that very little is lost when discretizing to a single bit. Finally, Theorem

5 gives a lower bound matching, up to a constant factor, the upper bound on the error in recovering the

distribution f (M ) given in Theorem 2. Theorem 5 also shows that Theorem 2 does not suﬀer by dropping

the canonical “spikiness” constraint.

Our lower bounds require a few assumptions, so before we delve into the bounds themselves, we brieﬂy

argue that these assumptions are rather innocuous. First, without loss of generality (since we can always

adjust f to account for rescaling M ), we assume that α ≥ 1. Next, we require that the parameters be

suﬃciently large so that

α2r max{d1, d2} ≥ C0

(10)

for an absolute constant C0. Note that we could replace this with a simpler, but still mild, condition that d1 > C0. Finally, we also require that r ≥ c where c is either 1 or 4 and that r ≤ O(min{d1, d2}/α2), where O(·) hides parameters (which may diﬀer in each Theorem) that we make explicit below. This last

assumption simply means that we are in the situation where r is signiﬁcantly smaller than d1 and d2, i.e., the matrix is of approximately low rank.

In the following, let

K = M : M ∗ ≤ α rd1d2, M ∞ ≤ α

(11)

denote the set of matrices whose recovery is guaranteed by Theorem 1.

3.4.1 Recovery from 1-bit measurements
Theorem 3. Fix α, r, d1, and d2 to be such that r ≥ 4 and (10) holds. Let βα be deﬁned as in (5), and suppose that f (x) is decreasing for x > 0. Let Ω be any subset of [d1] × [d2] with cardinality n, and let Y be as in (2). Consider any algorithm which, for any M ∈ K, takes as input Yi,j for (i, j) ∈ Ω and returns M . Then there exists M ∈ K such that with probability at least 3/4,
d11d2 M − M 2F ≥ min C1, C2α β 43 α r max{nd1, d2} (12)
as long as the right-hand side of (12) exceeds rα2/ min(d1, d2). Above, C1 and C2 are absolute constants.2
The requirement that the right-hand side of (12) be larger than rα2/ min(d1, d2) is satisﬁed as long as r ≤ O(min{d1, d2}/α2). In particular, it is satisﬁed whenever
r ≤ C3 min(1, β0)α· 2min(d1, d2)
for a ﬁxed constant C3. Note also that in the latent variable model in (1), f (x) is simply the probability density of Zi,j. Thus, the requirement that f (x) be decreasing is simply asking the probability density to have decreasing tails. One can easily check that this is satisﬁed for the logistic and probit models.
Note that if α is bounded by a constant and f is ﬁxed (in which case βα and βα are bounded by a constant), then the lower bound of Theorem 3 matches the upper bo√und given in (6) up to a constant. When α is not treated as a constant, the bounds diﬀer by a factor of βα. In the logistic model βα ≈ eα and so this amounts to the diﬀerence between eα/2 and eα. The probit model has a similar change in the constant of the exponent.
2Here and in the theorems below, the choice of 3/4 in the probability bound is arbitrary, and can be adjusted at the cost of changing C0 in (10) and C1 and C2. Similarly, β 3 α can be replaced by β(1− )α for any > 0.
4

9

3.4.2 Recovery from unquantized measurements
Next we show that, surprisingly, very little is lost by discretizing to a single bit. In Theorem 4, we consider an “unquantized” version of the latent variable model in (1) with Gaussian noise. That is, let Z be a matrix of i.i.d. Gaussian random variables, and suppose the noisy entries Mi,j + Zi,j are observed directly, without discretization. In this setting, we give a lower bound that still nearly matches the upper bound given in Theorem 1, up to the βα term.
Theorem 4. Fix α, r, d1, and d2 to be such that r ≥ 1 and (10) holds. Let Ω be any subset of [d1] × [d2] with cardinality n, and let Z be a d1 × d2 matrix with i.i.d. Gaussian entries with variance σ2. Consider any algorithm which, for any M ∈ K, takes as input Yi,j = Mi,j + Zi,j for (i, j) ∈ Ω and returns M . Then there exists M ∈ K such that with probability at least 3/4,
d11d2 M − M 2F ≥ min C1, C2ασ r max{nd1, d2} (13)
as long as the right-hand side of (13) exceeds rα2/ min(d1, d2). Above, C1 and C2 are absolute constants.
The requirement that the right-hand side of (13) be larger than rα2/ min(d1, d2) is satisﬁed whenever
r ≤ C3 min(1, σ2)α· 2min(d1, d2)
for a ﬁxed constant C3. Following Remark 1, the lower bound given in (13) matches the upper bound proven in Theorem 1 for
the solution to (4) up to a constant, as long as α/σ is bounded by a constant. In other words:
When the signal-to-noise ratio is constant, almost nothing is lost by quantizing to a single bit.
Perhaps it is not particularly surprising that 1-bit quantization induces little loss of information in the regime where the noise is comparable to the underlying quantity we wish to estimate—however, what is somewhat of a surprise is that the simple convex program in (4) can successfully recover all of the information contained in these 1-bit measurements.
Before proceeding, we also brieﬂy note that our Theorem 4 is somewhat similar to Theorem 3 in [49]. The authors in [49] co√nsider slightly diﬀerent sets K: these sets are more restrictive in the sense that it is required that α ≥ 32 log n and less restrictive because the nuclear-norm constraint may be replaced by a general Schatten-p norm constraint. It was important for us to allow α = O(1) in order to compare with our upper bounds due to the exponential dependence of βα on α in Theorem 1 for the probit model. This led to some new challenges in the proof. Finally, it is also noteworthy that our statements hold for arbitrary sets Ω, while the argument in [49] is only valid for a random choice of Ω.
3.4.3 Recovery of the distribution from 1-bit measurements
To conclude we address the optimality of Theorem 2. We show that under mild conditions on f , any algorithm that recovers the distribution f (M ) must yield an estimate whose Hellinger distance deviates from the true distribution by an amount proportional to α rd1d2/n, matching the upper bound of (9) up to a constant. Notice that the lower bound holds even if the algorithm is promised that M ∞ ≤ α, which the upper bound did not require.
Theorem 5. Fix α, r, d1, and d2 to be such that r ≥ 4 and (10) holds. Let L1 be deﬁned as in (5), and suppose that f (x) ≥ c and c ≤ f (x) ≤ 1 − c for x ∈ [−1, 1], for some constants c, c > 0. Let Ω be any subset of [d1] × [d2] with cardinality n, and let Y be as in (2). Consider any algorithm which, for
10

any M ∈ K, takes as input Yi,j for (i, j) ∈ Ω and returns M . Then there exists M ∈ K such that with probability at least 3/4,

d2H (f (M ), f (M )) ≥ min C1, C2 Lα1 r max{nd1, d2} (14)

as long as the right-hand side of (14) exceeds rα2/ min(d1, d2). Above, C1 and C2 are constants that depend on c, c .

The requirement that the right-hand side of (14) be larger than rα2/ min(d1, d2) is satisﬁed whenever

r ≤ C3 min(1, L−1 2) · min(d1, d2) α2

for a constant C3 that depends only on c, c . Note also that the condition that f and f be well-behaved

in the interval [−1, 1] is satisﬁed for the logistic model with c = 1/4 and c

=

1 1+e

≤ 0.269.

Similarly, we

may take c = 0.242 and c = 0.159 in the probit model.

4 Simulations
4.1 Implementation
Before presenting the proofs of our main results, we provide algorithms and a suite of numerical experiments to demonstrate their usefulness in practice.3 We present algorithms to solve the convex programs (3) and (4), and using these we can recover M (or f (M )) via 1-bit matrix completion.

4.1.1 Optimization algorithm We begin with the observation that both (3) and (4) are instances of the more general formulation

minimize f (x) subject to x ∈ C,

(15)

x

where f (x) is a smooth convex function from Rd → R, and C is a closed convex set in Rd. In particular, deﬁning V to be the bijective linear mapping that vectorizes Rd1×d2 to Rd1d2, we have f (x) := −FΩ,Y (V−1x) and, depending on whether we want to solve (3) or (4), C equal to either V(C1) or V(C2), where

C1 := {X : X ∗ ≤ τ } and C2 := {X : X ∗ ≤ τ, X ∞ ≤ κ}.

(16)

One possible solver for problems of the form (15) is the nonmonotone spectral projected-gradient (SPG) algorithm proposed by Birgin et al. [6]. Another possibility is to use an accelerated proximal-gradient methods for the minimization of composite functions [4, 50], which are useful for solving optimization problems of the form
minimize f (x) + g(x),
x
where f (x) and g(x) are convex functions with g(x) possibly non-smooth. This formulation reduces to (15) when choosing g(x) to be the extended-real indicator function corresponding to C:

g(x) = 0 x ∈ C +∞ otherwise.
3The code for these algorithms, as well as for the subsequent experiments, is available online at http://users.ece.gatech.edu/∼mdavenport/.

11

Both algorithms are iterative and require at each iteration the evaluation of f (x), its gradient ∇f (x), and an orthogonal projection onto C (i.e., the prox-function of g(x))

PC(v) := arg min x − v 2 subject to x ∈ C.

(17)

x

For our experiments we use the SPG algorithm, which we describe in more detail below. The implementation of the algorithm is based on the SPGL1 code [66, 67].

4.1.2 Spectral projected-gradient method
In basic gradient-descent algorithms for unconstrained minimization of a convex function f (x), iterates are of the form xk+1 = xk − αk∇f (xk), where the step length αk ∈ (0, 1] is chosen such that suﬃcient descent in the objective function f (x) is achieved. When the constraint x ∈ C is added, the basic scheme can no longer guarantee feasibility of the iterates. Projected gradient methods resolve this problem by including on orthogonal projections back onto the feasible set (17) at each iteration.
The nonmonotone SPG algorithm described in [6] modiﬁes the basic projected gradient method in two major ways. First, it scales the initial search direction using the spectral step-length γk as proposed by Barzilai and Borwein [3]. Second, it relaxes monotonicity of the objective values by requiring suﬃcient descent relative to the maximum objective over the last t iterates (or k when k < t). Two types of line search are considered. The ﬁrst type is curvilinear and traces the following path:

x(α) := PC(xk − αγk∇f (xk)).

(18)

The second type ﬁrst determines a projected gradient step, and uses this to obtain the search direction dk:

dk = PC(xk − γk∇f (xk)) − xk.

Next, a line search is done along the linear trajectory

x(α) := xk + αdk.

(19)

In either case, once the step length α is chosen, we set xk+1 = x(α), and proceed with the next iteration. In the 1-bit matrix completion formulation proposed in this paper, the projection onto C forms the main
computational bottleneck. As a result, it is crucial to keep the number of projections to a minimum, and our implementation therefore relies primarily on the line search along the linear trajectory given by (19). The more expensive curvilinear line search is used only when the linear one fails. We have observed that this situation tends to arise only when xk is near optimal.
The optimality condition for (15) is that

PC(x − ∇f (x)) = x.

(20)

Our implementation checks if (20) is approximately satisﬁed. In addition it imposes bounds on the total number of iterations and the run time.

4.1.3 Orthogonal projections onto the feasible sets

It is well known that the orthogonal projection onto the nuclear-norm ball C1 amounts to singular-value soft thresholding [12]. In particular, let X = U ΣV ∗ with Σ = diag(σ1, . . . , σd), then

PC(X) = Sλ(X) := U max{Σ − λI, 0}V ∗,

where the maximum is taken entrywise, and λ ≥ 0 is the smallest value for which

d i=1

max{σi

−

λ,

0}

≤

τ

.

12

Unfortunately, no closed form solution is known for the orthogonal projection onto C2. However, the underlying problem

PC (X) := arg min 1 X − Z 2 subject to Z ≤ τ, Z ≤ κ,

(21)

2

Z

2

F

∗

∞

can be solved using iterative methods. In particular, we can rewrite (21) as

minimize

1 2

X −W

2 F

subject to

W ∞ ≤ κ, Z ∗ ≤ τ, W = Z.

(22)

Z,W

and apply the alternating-direction method of multipliers (ADMM) [24, 27]. The augmented Lagrangian for (22) with respect to the constraint W = Z is given by

Lµ(Y , W , Z) = 12 X − W 2F − Y , W − Z + µ2 W − Z 2F + I[ W ∞≤κ] + I[ Z ∗≤τ] The ADMM iterates the following steps to solve (22):

Step 0. Initialize k = 0, and select µk, Y k, W k, Zk such that W k ∞ ≤ κ and Zk ∗ ≤ τ .

Step 1. Minimize Lµ(Y k, W, Zk) with respect to W , which can be rewritten as

W k+1 := arg min

W − (X + Y k + µZk)/(1 + µ))

2 F

subject to

W

W ∞ ≤ κ.

This is exactly the orthogonal projection of B = (X + Y k + µZk)/(1 + µ) onto {W | W ∞ ≤ κ}, and gives W k+1(i, j) = min{κ, max{−κ, B(i, j)}}.

Step 2. Minimize Lµ(Y k, W k+1, Z) with respect to Z. This gives

Zk+1 = arg min

Z − (W k+1 − 1/µkY k)

2 F

Z

subject to

Z ∗ ≤ τ,

and simpliﬁes to Zk+1 = PC1(W k+1 − 1/µkY k). Step 3. Update Y k+1 = Y k − µ(W k+1 − Zk+1), set µk+1 = 1.05 µk, and increment k.

Step 4. Return Z = Zk when W k − Zk F ≤ ε and Zk ∞ − κ ≤ ε for some suﬃciently small ε > 0. Otherwise, repeat steps 1–4.

4.2 Synthetic experiments
To evaluate the performance of this algorithm in practice and to conﬁrm the theoretical results described above, we ﬁrst performed a number of synthetic experiments. In particular, we constructed a random d × d matrix M with rank r by forming M = M 1M ∗2 where M 1 and M 2 are d × r matrices with entries drawn i.i.d. from a uniform distribution on [− 12 , 12 ]. The matrix is then scaled so that M ∞ = 1. We then obtained 1-bit observations by adding Gaussian noise of variance σ2 and recording the sign of the resulting value.
We begin by comparing the performance of the algorithms in (3) and (4) over a range of diﬀerent values of σ. In this experiment we set d = 500, r = 1, and n = 0.15d2, and we measured performance of each approach using the squared Frobenius norm of the error (normalized by the norm of the original matrix M ) and averaged the results over 15 draws of M .4 The results are shown in Figure 1. We observe that for both approaches, the performance is poor when there is too little noise (when σ is small) and when there
4In evaluating these algorithms we found that it was beneﬁcial in practice to follow the recovery by a “debiasing” step where the recovered matrix M is forced to be rank r by computing the SVD of M and hard thresholding the singular values. In cases where we report the Frobenius norm of the error, we performed this debiasing step, although it does not dramatically impact the performance.

13

2 F

2 F

M

M −M

1.4 1.2
1 0.8 0.6 0.4 0.2
−3

Program in (3) Program in (4)

−2

−1

0

1

log10 σ

Figure 1: Average relative error in the results obtained using algorithms (3) and (4) over a range of diﬀerent values of σ. We observe that for both approaches, the performance is poor when there is too little noise (when σ is small) and when there is too much noise (when σ is large). In the case of moderate noise, we observe that (4) appears to perform almost as well as (3), even though we do not have any theoretical guarantees for (4).

is too much noise (when σ is large). These two regimes correspond to the cases where the noise is either so small that the observations are essentially noise-free or when the noise is so large that each observation is essentially a coin toss. In the regime where the noise is of moderate power, we observe better performance for both approaches. Perhaps somewhat surprisingly, we ﬁnd that for much of this range, the approach in (4) appears to perform almost as well as (3), even though we do not have any theoretical guarantees for (4). This suggests that adding the inﬁnity-norm constraint as in (3) may have only limited practical beneﬁt, despite the key role this constraint played in our analysis. By using the simpler program in (4) one can greatly simplify the projection step in the algorithm, so in practice this approach may be preferable.
We also conducted experiments evaluating the performance of both (3) and (4) as a function of n for a particular choice of σ. The results showing the impact of n on (4) are shown in Figure 2 (the results for (3) at this noise level are almost indistinguishable). In this experiment we set d = 200, and chose σ ≈ 0.18 such that log10(σ) = 0.75, which lies in the regime where the noise is neither negligible nor overwhelming. We considered matrices with rank r = 3, 5, 10 and evaluated the performance over a range of n. Figure 2(a) shows the performance in terms of the relative Frobenius norm of the error, and Figure 2(b) shows the performance in terms of the Hellinger distance between the recovered distributions. Consistent with our theoretical results, we observe a decay in the error (under both performance metrics) that appears to behave roughly on the order of n−1/2.

4.3 Collaborative ﬁltering

To evaluate the performance of our algorithm in a practical setting, we consider the MovieLens (100k)

data set, which is available for download at http://www.grouplens.org/node/73. This data set consists

of 100,000 movie ratings from 1000 users on 1700 movies, with each rating occurring on a scale from 1

to 5. For testing purposes, we converted these ratings to binary observations by comparing each rating

to the average rating for the entire dataset (which is approximately 3.5). We then apply the algorithm

in

(4)

(using

the

logistic

model

of

f (x)

=

ex 1+ex

)

on

a

subset

of

95,000

ratings

to

recover

an

estimate

of

M.

However, since our only source of data is the quantized ratings, there is no “ground truth” against which

to measure the accuracy of our recovery. Thus, we instead evaluate our performance by checking to see if

the estimate of M accurately predicts the sign of the remaining 5000 unobserved ratings in our dataset.

The result of this simulation is shown in the ﬁrst line of Table 1, which gives the accuracy in predicting

whether the unobserved ratings are above or below the average rating of 3.5.

14

2 F

2

0.5

r=3

r=3

r=5

r=5

1.5

r = 10

0.4

)

)

r = 10

M

(

f

,

)

0.3 1
0.2

M

(

f

(

0.5

d

2 H

0.1

2 F

M

M −M

00 0.2 0.4 0.6 0.8 1 m/d2

00 0.2 0.4 0.6 0.8 1 m/d2

(a)

(b)

Figure 2: The impact of n on (4). (a) shows the performance in terms of the relative Frobenius norm of the error. (b) shows the performance in terms of the Hellinger distance between the recovered distributions. In both cases, we observe a decay in the error (under both performance metrics) that appears to behave roughly on the order of n−1/2.

Original rating 1 2 3 4 5 Overall 1-bit matrix completion 79% 73% 58% 75% 89% 73% “Standard” matrix completion 64% 56% 44% 65% 75% 60%
Table 1: Results of a comparison between a 1-bit matrix completion approach and “standard” matrix completion on the MovieLens 100k dataset, as evaluated by predicting whether the unobserved ratings were above or below average. Overall, the 1-bit approach achieves an accuracy of 73%, which is signiﬁcantly better than the 60% accuracy achieved by standard methods. The 1-bit approach also outperforms the standard approach for each possible individual rating, although we see that both methods perform poorly when the true rating is very close to the average rating.

By comparison, the second line in the table shows the results obtained using a “standard” method that uses the raw ratings (on a scale from 1 to 5) and tries to minimize the nuclear norm of the recovered matrix subject to a constraint that requires the Frobenius norm of the diﬀerence between the recovered and observed entries to be suﬃciently small. We implement this traditional approach using the TFOCS software package [5], and evaluate the results using the same error criterion as the 1-bit matrix completion approach—namely, we compare the recovered ratings to the average (recovered) rating. This approach depends on a number of input parameters: α in (3), the constraint on the Frobenius norm in the traditional case, as well as the internal parameter µ in TFOCS. We determine the parameter values by simply performing a grid search and selecting those values that lead to the best performance.
Perhaps somewhat surprisingly, the 1-bit approach performs signiﬁcantly better than the traditional one, even though the traditional approach is given more information in the form of the raw ratings, instead of the binary observations.5 The intuition as to how this might be possible is that the standard approach is likely paying a signiﬁcant penalty for actually requiring that the recovered matrix yields numerical ratings close to “1” or “5” when a user’s true preference could extend beyond this scale.
5While not reported in the table, we also observed that the 1-bit approach is relatively insensitive to the choice of α, so that this improvement in performance does not rely on a careful parameter setting.

15

5 Discussion
Many of the applications of matrix completion consider discrete observations, often in the form of binary measurements. However, matrix completion from noiseless binary measurements is extremely ill-posed, even if one collects a binary measurement for each of the matrix entries. Fortunately, when there are some stochastic variations (noise) in the observations, recovery becomes well-posed. In this paper we have shown that the unknown matrix can be accurately and eﬃciently recovered from binary measurements in this setting. When the inﬁnity norm of the unknown matrix is bounded by a constant, our error bounds are tight to within a constant and even match what is possible for undiscretized data. We have also shown that the binary probability distribution can be reconstructed over the entire matrix without any assumption on the inﬁnity-norm, and we have provided a matching lower bound (up to a constant).
Our theory considers approximately low-rank matrices—in particular, we assume that the singular values belong to a scaled Schatten-1 ball. It would be interesting to see whether more accurate reconstruction could be achieved under the assumption that the unknown matrix has precisely r nonzero singular values. We conjecture that the Lagrangian formulation of the problem could be fruitful for this endeavor. It would also be interesting to study whether our ideas can be extended to deal with measurements that are quantized to more than 2 (but still a small number) of diﬀerent values, but we leave such investigations for future work.

Acknowledgements
We would like to thank Roman Vershynin for helpful discussions and Wenxin Zhou for pointing out an error in an earlier version of the proof of Theorem 6.

A Proofs of the main results

We now provide the proofs of the main theorems presented in Section 3. To begin, we ﬁrst deﬁne some additional notation that we will need for the proofs. For two probability distributions P and Q on a ﬁnite set A, D(P Q) will denote the Kullback-Leibler (KL) divergence,

D(P Q) = P(x) log P(x) , x∈A Q(x)

where P(x) denotes the probability of the outcome x under the distribution P. We will abuse this notation slightly by overloading it in two ways. First, for scalar inputs p, q ∈ [0, 1], we will set

D(p q) = p log p + (1 − p) log 1 − p .

q

1−q

Second, for two matrices P , Q ∈ [0, 1]d1×d2, we deﬁne 1
D(P Q) = d1d2 D(Pi,j Qi,j).
i,j

We ﬁrst prove Theorem 2. Theorem 1 will then follow from an approximation argument. Finally, our lower bounds will be proved in Section A.3 using information theoretic arguments.

A.1 Proof of Theorem 2
We will actually prove a slightly more general statement, which will be helpful in the proof of Theorem 1. We will assume that M ∞ ≤ γ, and we will modify the program (4) to enforce X ∞ ≤ γ. That is, we

16

will consider the program

M = arg max LΩ,Y (X) subject to

X ∗ ≤ α rd1d2 and X ∞ ≤ γ.

(23)

X

We will then send γ → ∞ to recover the statement of Theorem 2. Formally, we prove the following theorem.
√ Theorem 6. Assume that M ∗ ≤ α rd1d2 and M ∞ ≤ γ. Suppose that Ω is chosen at random following the binomial model of Section 2.1 and satisfying E |Ω| = n. Suppose that Y is generated as in (2),
and let Lγ be as in (5). Let M be the solution to (23). Then with probability at least 1 − C1/(d1 + d2),

d2H (f (M ), f (M )) ≤ C2Lγα r(d1 n+ d2) 1 + (d1 + d2)nlog(d1d2) . (24) Above, C1 and C2 are absolute constants.
For the proof of Theorem 6, it will be convenient to work with the function L¯Ω,Y (X) = LΩ,Y (X) − LΩ,Y (0)
rather than with LΩ,Y itself. The key will be to establish the following concentration inequality. Lemma 1. Let G ⊂ Rd1×d2 be
G = X ∈ Rd1×d2 : X ∗ ≤ α rd1d2

for some r ≤ min{d1, d2} and α ≥ 0. Then

P

sup

|L¯Ω,Y

(X )

−

E L¯Ω,Y

(X )|

≥

√ C0αLγ r

n(d1 + d2) + d1d2 log(d1d2)

≤

C1 ,

(25)

X ∈G

d1 + d2

where C0 and C1 are absolute constants and the probability and the expectation are both over the choice of Ω and the draw of Y .

We will prove this lemma below, but ﬁrst we show how it implies Theorem 6. To begin, notice that for any choice of X,

E L¯Ω,Y (X) − L¯Ω,Y (M ) = E [LΩ,Y (X) − LΩ,Y (M )]

=n d1d2 i,j

f (Mi,j) log

f (Xi,j) f (Mi,j)

= −nD(f (M ) f (X)),

+ (1 − f (Mi,j)) log

1 − f (Xi,j) 1 − f (Mi,j)

where the expectation is over both Ω and Y . Next, note that by assumption M ∈ G. Then, we have for any X ∈ G

L¯Ω,Y (X) − L¯Ω,Y (M ) = E L¯Ω,Y (X) − L¯Ω,Y (M ) + L¯Ω,Y (X) − E L¯Ω,Y (X) − L¯Ω,Y (M ) − E ≤ E L¯Ω,Y (X) − L¯Ω,Y (M ) + 2 sup L¯Ω,Y (X) − E L¯Ω,Y (X)
X ∈G
= −nD(f (M ) f (X)) + 2 sup L¯Ω,Y (X) − E L¯Ω,Y (X) .
X ∈G

L¯Ω,Y (M )

Moreover, from the deﬁnition of M we also have that M ∈ G and LΩ,Y (M ) ≥ LΩ,Y (M ). Thus 0 ≤ −nD(f (M ) f (M )) + 2 sup L¯Ω,Y (X) − E L¯Ω,Y (X) .
X ∈G

17

Applying Lemma 1, we obtain that with probability at least 1 − C1/(d1 + d2), we have √
0 ≤ −nD(f (M ) f (M )) + 2C0αLγ r n(d1 + d2) + d1d2 log(d1d2) √
In this case, by rearranging and applying the fact that d1d2 ≤ d1 + d2, we obtain

D(f (M ) f (M )) ≤ 2C0αLγ r(d1 + d2) 1 + (d1 + d2) log(d1d2)

(26)

n

n

Finally, we note that the KL divergence can easily be bounded below by the Hellinger distance:

d2H (p, q) ≤ D(p q).

This is a simple consequence of Jensen’s inequality combined with the fact that 1 − x ≤ − log x. Thus, from (26) we obtain

d2H (f (M ), f (M )) ≤ 2C0αLγ

r(d1 + d2) n

1 + (d1 + d2) log(d1d2) , n

which establishes Theorem 6. Theorem 2 then follows by taking the limit as γ → ∞.

Remark 3 (Relationship to existing theory). The crux of the proof of the theorem involves bounding the

quantity

L¯Ω,Y (M ) − L¯Ω,Y (M )

(27)

via Lemma 1. In the language of statistical learning theory [9, 35], (27) is a generalization error, with respect to a loss function corresponding to L¯. In that literature, there are standard approaches to proving bounds on quantities of this type. We use a few of these tools (namely, symmetrization and contraction principles) in the proof of Lemma 1. However, our proof also deviates from these approaches in a few ways. Most notably, we use a moment argument to establish concentration, while in the classiﬁcation literature it is common to use approaches based on the method of bounded diﬀerences. In our problem setup, using bounded diﬀerences fails to yield a satisfactory answer because in the most general case we do not impose an ∞-norm constraint on M . This in turn implies that our diﬀerences are not well-bounded. Speciﬁcally, consider the empirical process whose moment we bound in (30), i.e.,

sup E ◦ ∆Ω, X .
X ∈G
The supremum is a function of the independent random variables Ei,j · ∆i,j. To check the eﬀectiveness of the method of bounded diﬀerences, suppose Ei,j∆i,j = 1 (for some√i, j), and is replaced by −1. Then the empirical process can change by as much as 2 maxX∈G X ∞ = α rd1d2, which is too large to yield an eﬀective answer.
The fact that L¯ is not well-bounded in this way is also why the generalization error bounds of [62, 63] do not immediately generalize to provide results analogous to Theorem 2. To obtain this result, we must choose a loss function such that (27) reduces to the KL divergence, and unfortunately, this loss function is not well-behaved.
Finally, we also note that symmetrization and contraction principles, applied to bound empirical processes, are key tools in the theory of unquantized matrix completion. The empirical process that we must bound to prove Lemma 1 is a discrete analog of a similar process considered in Section 5 of [49].

Proof of Lemma 1. We begin by noting that for any h > 0, by using Markov’s inequality we have that

P

sup

|L¯Ω,Y

(X )

−

E L¯Ω,Y

(X )|

≥

√ C0αLγ r

n(d1 + d2) + d1d2 log(d1d2)

X ∈G

=P

sup |L¯Ω,Y (X) − E L¯Ω,Y (X)|h ≥

√ C0αLγ r

h
n(d1 + d2) + d1d2 log(d1d2)

X ∈G

E supX∈G |L¯Ω,Y (X) − E L¯Ω,Y (X)|h

≤

√

h.

(28)

C0αLγ r n(d1 + d2) + d1d2 log(d1d2)

18

The bound in (25) will follow by combining this with an upper bound on E supX∈G |L¯Ω,Y (X) − E L¯Ω,Y (X)|h and setting h = log(d1 + d2). Towards this end, note that we can write the deﬁnition of L¯Ω,Y as
L¯Ω,Y (X) = 1[(i,j)∈Ω] 1[Yi,j=1] log f (fX(0i,)j ) + 1[Yi,j=−1] log 1 1−−f (fX(0i,)j ) .
i,j
By a symmetrization argument (Lemma 6.3 in [44]),

E sup |L¯Ω,Y (X) − E L¯Ω,Y (X)|h

X ∈G





h

≤ 2h E  sup

εi,j 1[(i,j)∈Ω] 1[Y =1] log f (Xi,j ) + 1[Y =−1] log 1 − f (Xi,j )

 ,

X∈G i,j

i,j

f (0)

i,j

1 − f (0)

where the εi,j are i.i.d. Rademacher random variables and the expectation in the upper bound is with respect to both Ω and Y as well as with respect to the εi,j. To bound the latter term, we apply a

contraction principle (Theorem 4.12 in [44]). By the deﬁnition of Lγ and the assumption that M ∞ ≤ γ,

both

1 log f (x)

Lγ

f (0)

1

1 − f (x)

and Lγ log 1 − f (0)

are contractions that vanish at 0. Thus, up to a factor of 2, the expected value of the supremum can only

decrease when these are replaced by Xi,j and −Xi,j respectively. We obtain  h

E sup L¯Ω,Y (X) − E L¯Ω,Y (X) h ≤ 2h(2Lγ)h E  sup

εi,j 1[(i,j)∈Ω] 1[Yi,j =1]Xi,j − 1[Yi,j =−1]Xi,j 

X ∈G

X∈G i,j

= (4Lγ)h E sup | ∆Ω ◦ E ◦ Y , X |h ,

(29)

X ∈G

where E denotes the matrix with entries given by εi,j, ∆Ω denotes the indicator matrix for Ω (so that [∆Ω]i,j = 1 if (i, j) ∈ Ω and 0 otherwise), and ◦ denotes the Hadamard product. Using the facts that the distribution of E ◦ Y is the same as the distribution of E and that | A, B | ≤ A B ∗, we have that

E sup | ∆Ω ◦ E ◦ Y , X |h = E sup | E ◦ ∆Ω, X |h

(30)

X ∈G

X ∈G

≤E

sup

E ◦ ∆Ω

h

X

h ∗

X ∈G

=α

h
d1d2r E

E ◦ ∆Ω h ,

(31)

To bound E E ◦ ∆Ω h , observe that E ◦ ∆Ω is a matrix with i.i.d. zero mean entries and thus by

Theorem 1.1 of [58],





h 

h 

E

E ◦ ∆Ω h

≤ C E 

max

d2


2
∆i,j  + E 

max

d1
∆i,j

2


1≤i≤d1 j=1

1≤j≤d2 i=1

for some constant C. This in turn implies that

 





h



1 h



 1 

h

h

E E ◦ ∆Ω h

1 h

≤

C

1 h

E



max

d2


2
∆i,j 



1≤i≤d1

+ E  max
1≤j≤d2

d1
∆i,j

2


 .

j=1

i=1

(32)

19

We ﬁrst focus on the row sum

d2 j=1

∆i,j

for

a

particular

choice

of

i.

Using

Bernstein’s

inequality,

for

all

t > 0 we have


d2
P
j=1

∆i,j − n d1d2

 > t ≤ 2 exp −t2/2 .
n/d1 + t/3

In particular, if we set t ≥ 6n/d1, then for each i we have





d2
P

∆

−n

> t ≤ 2 exp(−t) = 2P (W > t) ,

(33)

i,j d1d2

i

j=1

where W1, . . . , Wd1 are i.i.d. exponential random variables.

Below we use the fact that for any positive random variable q we can write E q =

∞ 0

P

(q

≥

t),

allowing

us to bound







h



1 h

E 

max

d2


2
∆i,j 

≤

1≤i≤d1 j=1



n 

d2

d1

+

E  max
1≤i≤d1

j=1

∆i,j − n d1d2

h  h1
2




h 21h

n 

d2

n 

≤

d1

+

E  max
1≤i≤d1

∆i,j − d1d2 

j=1





h   21h

n ∞

d2

n



=

d1 +  0

P  max
1≤i≤d1

∆i,j − d1d2 ≥ t dt

j=1





h   21h

n  6n h ∞



d2

n



≤ d1 +  d1

+

P  max

(6n/d )h

1≤i≤d1

∆i,j − d1d2 ≥ t dt

1

j=1

≤ n+ d1
≤ n+ d1

6n h + 2 ∞ P max W h ≥ t d1 (6n/d1)h 1≤i≤d1 i

1

6n

h
+ 2E

( max Wi)h

2h
.

d1

1≤i≤d1

1 2h
dt

Above, we have used the triangle inequality in the ﬁrst line, followed by Jensen’s inequality in the second line. In the ﬁfth line, (33), along with independence, allows us to introduce maxi Wi. By standard computations for exponential random variables,

E max Wih ≤ E
1≤i≤d1

h

max Wi − log d1
1≤i≤d1

+ logh(d1)

≤ 2h! + logh(d1).

Thus, we have







h



1 h

E 

max

d2


2
∆i,j 

≤

1≤i≤d1 j=1

n+ d1

1

6n

h
+2

logh(d1) + 2(h!)

2h

d1

20

√ ≤ (1 + 6)
√ ≤ (1 + 6)

n + 2 21h d1

1√ log d1 + 2 2h h

n

√

d1 + (2 + 2) log(d1 + d2),

using the choice h = log(d1 + d2) ≥ 1 in the ﬁnal line. A similar argument bounds the column sums, and thus from (32) we conclude that

E E ◦ ∆Ω h

1 1

√

h ≤ C h (1 + 6)





n+ d1

n

√

d2 + (2 + 2) log(d1 + d2)





≤

C

1 h

(1

+

√ 6)



2n(d1

+

d2)



+

(2

+

√ 2)

d1d2

log(d1 + d2)

1

√

≤ C h 2(1 + 6)

n(d1 + d2) + d1d2 log(d1 + d2) , d1d2

where the second and third inequalities both follow from Jensen’s inequality. Combining this with (29) and (31), we obtain

E sup L¯Ω,Y (X) − E L¯Ω,Y (X) h
X ∈G

1

h

1

√

√

≤ C h 8(1 + 6)αLγ r n(d1 + d2) + d1d2 log(d1 + d2).

Plugging this into (28) we obtain that the probability in (28) is upper bounded by

√ log(d1+d2)

C 8(1 + 6)

≤ C,

C0

d1 + d2

√ provided that C0 ≥ 8(1 + 6)/e, which establishes the lemma.

A.2 Proof of Theorem 1

The proof of Theorem 1 follows immediately from Theorem 6 (with γ = α) combined with the following lemma.

Lemma 2. Let f be a diﬀerentiable function and let M ∞ , M ∞ ≤ α. Then

d2 (f (M ), f (M )) ≥ inf

(f (ξ))2

M −M

2
F.

H

|ξ|≤α 8f (ξ)(1 − f (ξ)) d1d2

Proof. For any pair of entries x = Mi,j and y = Mi,j, write

2
f (x) − f (y) +

1 − f (x) − 1 − f (y) 2 ≥ 1 2

f (x) − f (y) −

2
f (x) − f (y) .

Using Taylor’s theorem to expand the quantity inside the square, for some ξ between x and y,

2
f (x) − f (y) +

1 − f (x) −

1 − f (y)

2≥ 1 2

2
f (ξ)(y − x) + f (ξ)(y − x) 2 f (ξ) 2 1 − f (ξ)

≥ 1 (f (ξ))2(y − x)2 1 + 1

8

f (ξ) 1 − f (ξ)

= (f (ξ))2 (y − x)2. 8f (ξ)(1 − f (ξ))

The lemma follows by summing across all entries and dividing by d1d2.

21

A.3 Lower bounds
The proofs of our lower bounds each follow a similar outline, using classical information theoretic techniques that have also proven useful in the context of compressed sensing [14, 53]. At a high level, our argument involves ﬁrst showing the existence of a set of matrices X , so that for each X(i) = X(j) ∈ X , X(i) −X(j) F is large. We will imagine obtaining measurements of a randomly chosen matrix in X and then running an arbitrary recovery procedure. If the recovered matrix is suﬃciently close to the original matrix, then we could determine which element of X was chosen. However, Fano’s inequality will imply that the probability of correctly identifying the chosen matrix is small, which will induce a lower bound on how close the recovered matrix can be to the original matrix.
In the proofs of Theorems 3, 4, and 5, we will assume without loss of generality that d2 ≥ d1. Before providing these proofs, however, we ﬁrst consider the construction of the set X .

A.3.1 Packing set construction

Lemma

3.

Let

K

be

deﬁned

as

in

(11),

let

γ

≤1

be

such

that

r γ2

is

an

integer,

and

suppose

that

r γ2

≤ d1.

There is a set X ⊂ K with

|X | ≥ exp rd2

16γ2

with the following properties:

1. For all X ∈ X , each entry has |Xi,j| = αγ.

2. For all X(i), X(j) ∈ X , i = j,

X(i) − X(j) 2 > α2γ2d1d2 .

F

2

Proof. We use a probabilistic argument. The set X will by constructed by drawing

|X | = exp rd2

(34)

16γ2

matrices

X

independently

from

the

following

distribution.

Set

B

=

r γ2

.

The

matrix

will

consist

of

blocks

of dimensions B × d2, stacked on top of each other. The entries of the ﬁrst block (that is, Xi,j for

(i, j) ∈ [B] × [d2]) will be i.i.d. symmetric random variables with values ±αγ. Then X will be ﬁlled out

by copying this block as many times as will ﬁt. That is,

Xi,j := Xi ,j where i = i (mod B) + 1.

Now we argue that with nonzero probability, this set will have all the desired properties. For X ∈ X ,

X ∞ = αγ ≤ α.

Further, because rank X ≤ B, √
X ∗≤ B X F =

r γ2 d1d2αγ = α rd1d2.

Thus X ⊂ K, and all that remains is to show that X satisﬁes requirement 2.

22

For X, W drawn from the above distribution,

X −W

2 F

=

(Xi,j − Wi,j )2

i,j

≥ d1

(Xi,j − Wi,j )2

B i∈[B] j∈[d2]

= 4α2γ2 d1

δi,j

B i∈[B] j∈[d2]

=: 4α2γ2 d1 Z(X, W ). B

where the δi,j are independent 0/1 Bernoulli random variables with mean 1/2. By Hoeﬀding’s inequality

and a union bound,

P min Z(X, W ) ≤ d2B ≤ |X | exp(−Bd2/8).

X=W ∈X

4

2

One can check that for X of the size given in (34), the right-hand side of the above tail bound is less than 1, and thus the event that Z(X, W ) > d2B/4 for all X = W ∈ X has non-zero probability. In this event,

X − W 2F > α2γ2 dB1 d2B ≥ α2γ22d1d2 ,

where the second inequality uses the assumption that d1 ≥ B and the fact that x ≥ x/2 for all x ≥ 1. Hence, requirement (2) holds with nonzero probability and thus the desired set exists.

A.3.2 Proof of Theorem 3

Before we prove Theorem 3, we will need the following lemma about the KL divergence.

Lemma 4. Suppose that x, y ∈ (0, 1). Then

(x − y)2 D(x y) ≤ y(1 − y) .

Proof. Without loss of generality, we may assume that x ≤ y. Indeed, D(1 − x 1 − y) = D(x y), and either x ≤ y or 1 − x ≤ 1 − y. Let z = y − x. A simple computation shows that

∂

z

∂z D(x x + z) = (x + z)(1 − x − z) .

Thus, by Taylor’s theorem, there is some ξ ∈ [0, z] so that

ξ D(x y) = D(x x) + z (x + ξ)(1 − x − ξ) .

Since the right hand side is increasing in ξ, we may replace ξ with z and conclude

(y − x)2 D(x y) ≤ y(1 − y) ,

as desired.

23

Now, for the proof of Theorem 3, we choose so that

2 = min 1 , C2α β3α/4 rd2 ,

(35)

1024

n

where C2 is an absolute constant to be speciﬁed later. We will next use Lemma 3 to construct a set X ,

choosing

γ

so

that

r γ2

is

an

integer

and

√

4 2 ≤γ≤ 8 .

α

α

We can make such a choice because ≤ 1/32 and r ≥ 4. We verify that such a choice for γ satisﬁes the

requirements of Lemma 3. Indeed, since ≤ 1/32 and α ≥ 1 we have γ ≤ 1/4 < 1. Further, we assume in the theorem that the right-hand side of (35) is larger than Crα2/d1 which implies that r/γ2 ≤ d1 for an
appropriate choice of C.

Let Xα/2,γ be the set whose existence is guaranteed by Lemma 3 with this choice of γ, and with α/2 instead of α. We will construct X by setting

X := X + α 1 − γ2 1 : X ∈ Xα/2,γ

Note that X has the same size as Xα/2,γ, i.e., |X | satisﬁes (34). X also has the same bound on pairwise

distances

X(i) − X(j) 2 ≥ α2γ2d1d2 ≥ 4d1d2 2,

(36)

F

8

and every entry of X ∈ X has

|Xi,j| ∈ {α, α },

where α = (1 − γ)α. Further, since for X ∈ Xα/2,γ,

X + α(1 − γ/2)1 ∗ ≤ X ∗ + α(1 − γ/2) d1d2 ≤ α rd1d2

for r ≥ 4 as in the theorem statement. Now suppose for the sake of a contradiction that there exists an algorithm such that for any X ∈ K,
when given access to the measurements Y Ω, returns an X such that

1 X−X 2 < 2

(37)

d1d2

F

with probability at least 1/4. We will imagine running this algorithm on a matrix X chosen uniformly at

random from X . Let

X∗ = arg min

X(i) − X

2 F

.

X (i) ∈X

It is easy to check that if (37) holds, then X∗ = X. Indeed, for any X ∈ X with X = X, from (37) and (36) we have that

X − X F = X − X + X − X F ≥ X − X F − X − X F > 2 d1d2 − d1d2 = d1d2 .

At the same time, since X ∈ X is a candidate for X∗, we have that

X∗ − X F ≤ X − X F ≤ d1d2 .

Thus, if (37) holds, then X∗ − X F < X − X F for any X ∈ X with X = X, and hence we must have X∗ = X. By assumption, (37) holds with probability at least 1/4, and thus

P (X = X∗) ≤ 3 .

(38)

4

24

We will show that this probability must in fact be large, generating our contradiction. By a variant of Fano’s inequality

∗

maxX(k)=X( ) D(Y Ω|X(k) Y Ω|X( )) + 1

P (X = X ) ≥ 1 −

log |X |

.

(39)

Because each entry of Y is independent,6

D := D(Y Ω|X(k)

Y Ω|X( )) =

D(Yi,j |Xi(,kj)

(i,j)∈Ω

Yi,j |Xi(,j)).

Each term in the sum is either 0, D(α α ), or D(α α). By Lemma 4, all of these are bounded above by D(Yi,j|Xi(,kj) Yi,j|Xi(,j)) ≤ f(f(α(α))(1−−f f(α(α)))2) ,

and so, from the intermediate value theorem, for some ξ ∈ [α , α], (f (α) − f (α ))2 (f (ξ))2(α − α )2
D ≤ n f (α )(1 − f (α )) ≤ n f (α )(1 − f (α )) .

Using the assumption that f (x) is decreasing for x > 0 and the deﬁnition of α = (1 − γ)α, we have

D ≤ n(γα)2 ≤ 64n 2 .

βα

βα

Then from (39) and (38),









1 ≤1−

(X = X∗) ≤

D+1

64n 2
≤ 16γ2  βα

+ 1  ≤ 1024

64n 2
2  βα

+ 1.

(40)

4

P

log |X |

rd2

α2rd2

We now show that for appropriate values of C0 and C2, this leads to a contradiction. First suppose that

64n 2 ≤ βα . In this case we have

1 ≤ 1024 2 2 ,

4

α2rd2

which together with (35) implies that α2rd2 ≤ 8. If we set C0 > 8 in (10), then this would lead to a contradiction. Thus, suppose now that 64n 2 > βα . Then (40) simpliﬁes to

1 1024 · 128 · n 4 4 < βα α2rd2 .

Thus

√ 2 > α β√α rd2 .
512 2 n

Note β is in√creasing as a function of α and α ≥ 3α/4 (since γ ≤ 1/4). Thus, βα ≥ β3α/4. Setting C2 ≤ 1/512 2 in (35) now leads to a contradiction, and hence (37) must fail to hold with probability at least 3/4, which proves the theorem.

6Note that here, to be consistent with the literature we are referencing regarding Fano’s inequality, D is deﬁned slightly diﬀerently than elsewhere in the paper where we would weight D by 1/d1d2.

25

A.3.3 Proof of Theorem 4

Choose so that

2 = min 1 , C2ασ rd2

(41)

16

n

for an absolute constant C2 to be determined later. As in the proof of Theorem 3, we will consider running

such an algorithm on a random element in a set X ⊂ K. For our set X , we will use the set whose existence

is

guaranteed

by

Lemma

3.

We

will

set

γ

so

that

r γ2

is

an

integer

and

√

2 2 ≤γ≤ 4 .

α

α

This is possible since ≤ 1/4 and r, α ≥ 1. One can check that γ satisﬁes the assumptions of Lemma 3. Now suppose that X ∈ X is chosen uniformly at random, and let Y = (X + Z)|Ω as in the statement
of the theorem. Let X be any estimate of X obtained from Y Ω. We begin by bounding the mutual information I(X; X) in the following lemma (which is analogous to [20, Equation 9.16]).

Lemma 5. Proof. We begin by noting that

I(X; X) ≤ n log σ2 + α2γ2 . 2

I(XΩ; Y ) = h(XΩ + ZΩ) − h(XΩ + ZΩ|XΩ) = h(XΩ + ZΩ) − h(ZΩ), where h denotes the diﬀerential entropy. Let ξ denote a matrix of i.i.d. ±1 entries. Then

h(XΩ ◦ ξ + ZΩ) = h((XΩ + ZΩ) ◦ ξ) ≥ h((XΩ + ZΩ) ◦ ξ | ξ) = h(XΩ + ZΩ),

and so, letting X = X ◦ ξ,

I(XΩ; Y ) ≤ h(XΩ + ZΩ) − h(ZΩ).

Treating XΩ + ZΩ as a random vector of length n, we compute the covariance matrix as

Σ := E vec(XΩ + ZΩ) vec(XΩ + ZΩ)T = σ2 + (αγ)2 In.

By Theorem 8.6.5 in [20],

h(XΩ + ZΩ) ≤ 1 log ((2πe)n det(Σ)) = 1 log (2πe)n(σ2 + (αγ)2)n .

2

2

We

have

that

h(Z Ω )

=

1 2

log

(2πe)nσ2n

, and so

I(XΩ; Y ) ≤ n log 1 + αγ 2 .

2

σ

Then the data processing inequality implies

I(X; X) ≤ n log 1 + αγ 2 ,

2

σ

which establishes the lemma.

26

We now proceed by using essentially the same argument as in the proof of Theorem 3. Speciﬁcally, we suppose for the sake of a contradiction that there exists an algorithm such that for any X ∈ K, when given access to the measurements Y Ω, returns an X such that

1 X−X 2 < 2

(42)

d1d2

F

with probability at least 1/4. As before, if we set

X∗ = arg min

X(i) − X

2 F

X (i) ∈X

then we can show that if (42) holds, then X∗ = X. Thus, if (42) holds with probability at least 1/4 then

P (X = X∗) ≤ 3 .

(43)

4

However, by Fano’s inequality, the probability that X = X is at least

P X = X ≥ H(X|X) − 1 = H(X) − I(X; X) − 1 ≥ 1 − I(X; X) + 1

log(|X |)

log(|X |)

log |X |

Plugging in |X | from Lemma 3 and I(X; X) from Lemma 5, and using the inequality log(1 + z) ≤ z, we

obtain

P X = X ≥ 1 − 16γ2 n αγ 2 + 1 . rd2 2 σ

Combining this with (43) and using the fact that γ ≤ 4 /α, we obtain

1 256 2

2

4 ≤ α2rd2 8n σ2 + 1 .

We now argue, as before, that this leads to a contradiction. Speciﬁcally, if 8n 2/σ2 ≤ 1, then together with (41) this implies that α2rd2 ≤ 128. If we set C0 > 128 in (10), then this would lead to a contradiction. Thus, suppose now that 8n 2/σ2 > 1, in which case we have

2 > ασ rd2 . 128 n
Thus, setting C2 ≤ 1/128 in (41) leads to a contradiction, and hence (42) must fail to hold with probability at least 3/4, which proves the theorem.

A.3.4 Proof of Theorem 5

The proof of Theorem 5 also mirrors the proof of Theorem 3. The main diﬀerence is the observation that

the set constructed in Lemma 3 also works with the Hellinger distance. We begin as before by choosing

so that

2 = min c , C2 α rd2 ,

(44)

16 L1 n

where C2 is an absolute constant to be determined. Set γ to be an integer so that

2√2 ≤ γ ≤ 4 .

αc

αc

This is possible since by assumption α ≥ 1 and ≤ 4c . One can check that γ satisﬁes the assumptions of Lemma 3.

27

As in the proof of Theorem 3, we will consider running such an algorithm on a random element in a
set X ⊂ K. For our set X , we will use the set whose existence is guaranteed by Lemma 3. Note that since the Hellinger distance is bounded below by the Frobenius norm, we have that for all X(i) = X(j) ∈ X ,

d2 (f (X(i)) − f (X(j))) ≥ f (X(i)) − f (X(j)) 2 ≥ c2 X(i) − X(j) 2 > c2 α2γ2d1d2 ≥ 4d1d2 2.

H

F

F2

Now suppose for the sake of a contradiction that there exists an algorithm such that for any X ∈ K, when given access to the measurements Y Ω, returns an X such that

d2H (f (X), f (X)) < 2

(45)

with probability at least 1/4. If we set

X∗ = arg min d2H (f (X(i)) − f (X))
X (i) ∈X
then we can show that if (45) holds, then X∗ = X. Thus, if (45) holds with probability at least 1/4 then

P (X = X∗) ≤ 3 .

(46)

4

However, we may again apply Fano’s inequality as in (39). Using Lemma 4 we have

D(Yi,j |Xi(,kj)

( ) (f (αγ) − f (−αγ))2

4(f (ξ))2α2γ2

4f 2(ξ)L2αγ α2γ2

Yi,j|Xi,j ) ≤ f (αγ)(1 − f (αγ)) ≤ f (αγ)(1 − f (αγ)) ≤ f (αγ)(1 − f (αγ)) ,

for some |ξ| ≤ αγ, where Lαγ is as in (5). By the assumption that c < |f (x)| < 1 − c for |x| < 1, and that

αγ ≤ α 4 ≤ 4 ≤ 1,

αc

c

we obtain

D(Yi,j |Xi(,kj)

Yi,j|X( )) ≤ 4c L21α2γ2 ≤ C L2 2,

i,j

1−c

1

where C = 64c /(c2(1 − c )). Thus, from (39), we have

1 ≤ C nL21 2 + 1 ≤ 256 2 C nL21 2 + 1 .

4

log |X |

c2

α2rd2

We now argue once again that this leads to a contradiction. Speciﬁcally, if C nL21 2 ≤ 1, then together with (44) this implies that α2rd2 ≤ 128/c. If we set C0 > 128/c in (10), then this would lead to a contradiction. Thus, suppose now that C nL21 2 > 1, in which case we have

2 > √c α rd2 . 32 2C L1 n
√ Thus setting C2 ≤ c/32 2C in (44) leads to a contradiction, and hence (45) must fail to hold with probability at least 3/4, which proves the theorem.

References
[1] A. Ai, A. Lapanowski, Y. Plan, and R. Vershynin. One-bit compressed sensing with non-gaussian measurements. Linear Alg. Appl., 441:222–239, 2014.
[2] F. Bach. Self-concordant analysis for logistic regression. Elec. J. Stat., 4:384–414, 2010.
28

[3] J. Barzilai and J. Borwein. Two-point step size gradient methods. IMA J. Num. Anal., 8(1):141–148, 1988.
[4] A. Beck and M. Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse problems. SIAM J. Imag. Sci., 2(1):183–202, 2009.
[5] S. Becker, E. Cand`es, and M. Grant. Templates for convex cone problems with applications to sparse signal recovery. Math. Program. Comput., 3(3):165–218, 2011.
[6] E. Birgin, J. Mart´ınez, and M. Raydan. Nonmonotone spectral projected gradient methods on convex sets. SIAM J. Optimization, 10(4):1196–1211, 2000.
[7] P. Biswas, T. C. Lian, T. C. Wang, and Y. Ye. Semideﬁnite programming based algorithms for sensor network localization. ACM Trans. Sen. Netw., 2(2):188–220, 2006.
[8] I. Borg and P. Groenen. Modern multidimensional scaling. Springer Science, New York, NY, 2010. [9] S. Boucheron, O. Bousquet, and G. Lugosi. Theory of classiﬁcation: A survey of some recent advances.
ESAIM: Probability and Statistics, 9:323–375, 2005. [10] P. Boufounos and R. Baraniuk. 1-Bit compressive sensing. In Proc. IEEE Conf. Inform. Science and
Systems (CISS), Princeton, NJ, Mar. 2008. [11] F. Bunea. Honest variable selection in linear and logistic regression models via 1 and 1 + 2 penal-
ization. Elec. J. Stat., 2:1153–1194, 2008. [12] J.-F. Cai, E. Cand`es, and Z. Shen. A singular value thresholding algorithm for matrix completion.
SIAM J. Optimization, 20(4):1956–1982, 2010. [13] E. Cand`es. Compressive sampling. In Proc. Int. Congress of Math., Madrid, Spain, Aug. 2006. [14] E. Cand`es and M. Davenport. How well can we estimate a sparse vector? Appl. Comput. Harmon.
Anal., 34(2):317–323, 2013. [15] E. Cand`es and Y. Plan. Matrix completion with noise. Proc. IEEE, 98(6):925–936, 2010. [16] E. Cand`es and B. Recht. Exact matrix completion via convex optimization. Found. Comput. Math.,
9(6):717–772, 2009. [17] E. Cand`es and T. Tao. The power of convex relaxation: Near-optimal matrix completion. IEEE
Trans. Inform. Theory, 56(5):2053–2080, 2010. [18] S. Chatterjee. Matrix estimation by universal singular value thresholding. 2012. Arxiv preprint
1212.1247. [19] M. Collins, S. Dasgupta, and R. Schapire. A generalization of principal component analysis to the
exponential family. In Proc. Adv. in Neural Processing Systems (NIPS), Vancouver, BC, Dec. 2001. [20] T. Cover and J. Thomas. Elements of information theory. Wiley-Interscience, New York, NY, 1991. [21] M. Davenport, M. Duarte, Y. Eldar, and G. Kutyniok. Introduction to compressed sensing. In Y. Eldar
and G. Kutyniok, editors, Compressed Sensing: Theory and Applications. Cambridge University Press, Cambridge, UK, 2012. [22] J. De Leeuw. Principal component analysis of binary data by iterated singular value decomposition. Comput. Stat. Data Anal., 50:21–39, 2006. [23] D. Donoho. Compressed sensing. IEEE Trans. Inform. Theory, 52(4):1289–1306, 2006. [24] D. Gabay and B. Mercier. A dual algorithm for the solution of nonlinear variational problems via ﬁnite element approximations. Comp. Math. Appl., 2:17–40, 1976. [25] S. Ga¨ıﬀas and G. Lecu´e. Sharp oracle inequalities for the prediction of a high-dimensional matrix. 2010. Arxiv preprint 1008.4886.
29

[26] D. Gleich and L.-H. Lim. Rank aggregation via nuclear norm minimization. In Proc. ACM SIGKDD Int. Conf. on Knowledge, Discovery, and Data Mining (KDD), San Diego, CA, Aug. 2011.
[27] R. Glowinski and A. Marrocco. Sur l’approximation, par elements ﬁnis d’ordre un, et la resolution, par penalisation-dualit´e, d’une classe de problems de Dirichlet non lineares. Revue Fran¸caise d’Automatique, Informatique, et Recherche Op´erationelle, 9:41–76, 1975.
[28] D. Goldberg, D. Nichols, B. Oki, and D. Terry. Using collaborative ﬁltering to weave an information tapestry. Comm. ACM, 35(12):61–70, 1992.
[29] P. Green and Y. Wind. Multivariate decisions in marketing: A measurement approach. Dryden, Hinsdale, IL, 1973.
[30] D. Gross. Recovering low-rank matrices from few coeﬃcients in any basis. IEEE Trans. Inform. Theory, 57(3):1548–1566, 2011.
[31] D. Gross, Y. Liu, S. Flammia, S. Becker, and J. Eisert. Quantum state tomography via compressed sensing. Physical Review Letters, 105(15):150401, 2010.
[32] A. Gupta, R. Nowak, and B. Recht. Sample complexity for 1-bit compressed sensing and sparse classiﬁcation. In Proc. IEEE Int. Symp. Inform. Theory (ISIT), Austin, TX, June 2010.
[33] L. Jacques, J. Laska, P. Boufounos, and R. Baraniuk. Robust 1-bit compressive sensing via binary stable embeddings of sparse vectors. IEEE Trans. Inform. Theory, 59(4):2082–2102, 2013.
[34] S. Kakade, O. Shamir, K. Sridharan, and A. Tewari. Learning exponential families in high-dimensions: Strong convexity and sparsity. In Proc. Int. Conf. Art. Intell. Stat. (AISTATS), Clearwater Beach, FL, Apr. 2009.
[35] P. Kar, B. Sriperumbudur, P. Jain, and H. Karnick. On the generalization ability of online learning algorithms for pairwise loss functions. In Proc. Int. Conf. Machine Learning (ICML), Atlanta, GA, June 2013.
[36] R. Keshavan, A. Montanari, and S. Oh. Matrix completion from a few entries. IEEE Trans. Inform. Theory, 56(6):2980–2998, 2010.
[37] R. Keshavan, A. Montanari, and S. Oh. Matrix completion from noisy entries. J. Machine Learning Research, 11:2057–2078, 2010.
[38] M. Khan, B. Marlin, G. Bouchard, and K. Murphy. Variational bounds for mixed-data factor analysis. In Proc. Adv. in Neural Processing Systems (NIPS), Vancouver, BC, Dec. 2010.
[39] O. Klopp. High dimensional matrix estimation with unknown variance of the noise. 2011. Arxiv preprint 1112.3055.
[40] O. Klopp. Rank penalized estimators for high-dimensional matrices. Elec. J. Stat., 5:1161–1183, 2011.
[41] V. Koltchinskii. Von Neumann entropy penalization and low-rank matrix estimation. Ann. Stat., 39(6):2936–2973, 2012.
[42] V. Koltchinskii, K. Lounici, and A. Tsybakov. Nuclear-norm penalization and optimal rates for noisy low-rank matrix completion. Ann. Stat., 39(5):2302–2329, 2011.
[43] J. Laska and R. Baraniuk. Regime change: Bit-depth versus measurement-rate in compressive sensing. IEEE Trans. Signal Processing, 60(7):3496–3505, 2012.
[44] M. Ledoux and M. Talagrand. Probability in Banach Spaces: Isoperimetry and Processes. SpringerVerlag, Berlin, Germany, 1991.
[45] Z. Liu and L. Vandenberghe. Interior-point method for nuclear norm approximation with application to system identiﬁcation. SIAM J. Matrix Analysis and Applications, 31(3):1235–1256, 2009.
[46] L. Meier, S. Van De Geer, and P. Bu¨hlmann. The group lasso for logistic regression. J. Royal Stat. Soc. B, 70(1):53–71, 2008.
30

[47] G. Miller. The magical number seven, plus or minus two: Some limits on our capacity for processing information. Psychological Rev., 63(2):81–97, 1956.
[48] S. Negahban, P. Ravikumar, M. Wainwright, and B. Yu. A uniﬁed framework for high-dimensional analysis of m-estimators with decomposable regularizers. Stat. Sci., 27(4):538–557, 2012.
[49] S. Negahban and M. Wainwright. Restricted strong convexity and weighted matrix completion: Optimal bounds with noise. J. Machine Learning Research, 13:1665–1697, 2012.
[50] Y. Nesterov. Introductory Lectures on Convex Optimization. Kluwer Academic Publisher, Dordrecht, The Netherlands, 2004.
[51] Y. Plan and R. Vershynin. One-bit compressed sensing by linear programming. Comm. Pure Appl. Math., 66(8):1275–1297, 2013.
[52] Y. Plan and R. Vershynin. Robust 1-bit compressed sensing and sparse logistic regression: A convex programming approach. IEEE Trans. Inform. Theory, 59(1):482–494, 2013.
[53] G. Raskutti, M. Wainwright, and B. Yu. Minimax rates of estimation for high-dimensional linear regression over q-balls. IEEE Trans. Inform. Theory, 57(10):6976–6994, 2011.
[54] P. Ravikumar, M. Wainwright, and J. Laﬀerty. High-dimensional Ising model selection using 1regularized logistic regression. Ann. Stat., 38(3):1287–1319, 2010.
[55] B. Recht. A simpler approach to matrix completion. J. Machine Learning Research, 12:3413–3430, 2011.
[56] A. Rohde and A. Tsybakov. Estimation of high-dimensional low-rank matrices. Ann. Stat., 39(2):887– 930, 2011.
[57] A. Schein, L. Saul, and L. Ungar. A generalized linear model for principal component analysis of binary data. In Proc. Int. Conf. Art. Intell. Stat. (AISTATS), Key West, FL, Jan. 2003.
[58] Y. Seginer. The expected norm of random matrices. Combinatorics, Probability, and Computing, 9(2):149–166, 2000.
[59] A. Singer. A remark on global positioning from local distances. Proc. Natl. Acad. Sci., 105(28):9507– 9511, 2008.
[60] A. Singer and M. Cucuringu. Uniqueness of low-rank matrix completion by rigidity theory. SIAM J. Matrix Analysis and Applications, 31(4):1621–1641, 2010.
[61] I. Spence and D. Domoney. Single subject incomplete designs for nonmetric multidimensional scaling. Psychometrika, 39(4):469–490, 1974.
[62] N. Srebro, N. Alon, and T. Jaakkola. Generalization error bounds for collaborative prediction with low-rank matrices. In Proc. Adv. in Neural Processing Systems (NIPS), Vancouver, BC, Dec. 2004.
[63] N. Srebro, J. D. Rennie, and T. Jaakkola. Maximum-margin matrix factorization. In Proc. Adv. in Neural Processing Systems (NIPS), Vancouver, BC, Dec. 2004.
[64] M. Tipping. Probabilistic visualization of high-dimensional binary data. In Proc. Adv. in Neural Processing Systems (NIPS), Vancouver, BC, Dec. 1998.
[65] S. Van De Geer. High-dimensional generalized linear models and the lasso. Ann. Stat., 36(2):614–645, 2008.
[66] E. van den Berg and M. P. Friedlander. SPGL1, 2007. http://www.cs.ubc.ca/ mpf/spgl1/.
[67] E. van den Berg and M. P. Friedlander. Probing the Pareto frontier for basis-pursuit solutions. SIAM J. on Scientiﬁc Computing, 31(2):890–912, 2008.
[68] A. Zymnis, S. Boyd, and E. Cand`es. Compressed sensing with quantized measurements. IEEE Signal Processing Lett., 17(2):149–152, 2010.
31

