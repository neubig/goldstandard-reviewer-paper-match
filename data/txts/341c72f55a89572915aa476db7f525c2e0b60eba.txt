Systematic Analysis of Cluster Similarity Indices: How to Validate Validation Measures

arXiv:1911.04773v7 [cs.DM] 26 Aug 2021

Martijn Go¨sgens 1 Alexey Tikhonov 2 Liudmila Prokhorenkova 3 4 5

Abstract
Many cluster similarity indices are used to evaluate clustering algorithms, and choosing the best one for a particular task remains an open problem. We demonstrate that this problem is crucial: there are many disagreements among the indices, these disagreements do affect which algorithms are preferred in applications, and this can lead to degraded performance in real-world systems. We propose a theoretical framework to tackle this problem: we develop a list of desirable properties and conduct an extensive theoretical analysis to verify which indices satisfy them. This allows for making an informed choice: given a particular application, one can ﬁrst select properties that are desirable for the task and then identify indices satisfying these. Our work uniﬁes and considerably extends existing attempts at analyzing cluster similarity indices: we introduce new properties, formalize existing ones, and mathematically prove or disprove each property for an extensive list of validation indices. This broader and more rigorous approach leads to recommendations that considerably differ from how validation indices are currently being chosen by practitioners. Some of the most popular indices are even shown to be dominated by previously overlooked ones.
1. Introduction
Clustering is an unsupervised machine learning problem, where the task is to group objects that are similar to each other. In network analysis, a related problem is called community detection, where groupings are based on relations between items (links), and the obtained clusters are expected
1Eindhoven University of Technology, Eindhoven, The Netherlands 2Yandex, Berlin, Germany 3Yandex, Moscow, Russia 4Moscow Institute of Physics and Technology, Moscow, Russia 5HSE University, Moscow, Russia. Correspondence to: Martijn Go¨sgens <research@martijngosgens.nl>.
Proceedings of the 38 th International Conference on Machine Learning, PMLR 139, 2021. Copyright 2021 by the author(s).

to be densely interconnected. Clustering is used across various applications, including text mining, online advertisement, anomaly detection, and many others (Xu & Tian, 2015; Allahyari et al., 2017).
To measure the quality of a clustering algorithm, one can use either internal or external measures. Internal measures evaluate the consistency of the clustering result with the data being clustered, e.g., Silhouette, Hubert-Gamma, Dunn indices or modularity in network analysis (Newman & Girvan, 2004). Unfortunately, it is often unclear whether optimizing any of these measures would translate into improved quality in practical applications. External (cluster similarity) measures compare the candidate partition with a reference one (obtained, e.g., by human assessors). A comparison with such a gold standard partition, when it is available, is more reliable. There are many tasks where external evaluation is applicable: text clustering (Amigo´ et al., 2009), topic modeling (Virtanen & Girolami, 2019), Web categorization (Wibowo & Williams, 2002), face clustering (Wang et al., 2019), news aggregation (see Section 3), and others. Often, when there is no reference partition available, it is possible to let a group of experts annotate a subset of items and compare the algorithms on this subset.
Dozens of cluster similarity measures exist and which one should be used is a subject of debate (Lei et al., 2017). In this paper, we systematically analyze the problem of choosing the best cluster similarity index. We start with a series of experiments demonstrating the importance of the problem (Section 3). First, we construct simple examples showing the inconsistency of all pairs of different similarity indices. Then, we demonstrate that such disagreements often occur in practice when well-known clustering algorithms are applied to real datasets. Finally, we illustrate how an improper choice of a similarity index can affect the performance of production systems.
So, the question is: how to compare cluster similarity indices and decide which one is best for a particular application? Ideally, we would want to choose an index for which good similarity scores translate to good real-world performance. However, opportunities to experimentally perform such a validation of validation indices are rare, typically expensive, and do not generalize to other applications. In contrast, we

2

Systematic Analysis of Cluster Similarity Indices: How to Validate Validation Measures

suggest a theoretical approach: we formally deﬁne properties that are desirable across various applications, discuss their importance, and formally analyze which similarity indices satisfy them (Section 4). This theoretical framework allows practitioners to choose the best index based on relevant properties for their applications. In Section 5, we show how this choice can be made and discuss indices that are expected to be suitable across various applications.
Among the considered properties, constant baseline is arguably the most important and non-trivial one. Informally, a sensible index should not prefer one candidate partition over another just because it has too large or too small clusters. Constant baseline is a particular focus of the current research. We develop a rigorous theoretical framework for analyzing this property. In this respect, our work improves over the previous (mostly empirical) research on constant baseline of particular indices (Strehl, 2002; Albatineh et al., 2006; Vinh et al., 2009; 2010; Lei et al., 2017).
While the ideas discussed in the paper can be applied to all similarity indices, we provide an additional theoretical characterization of pair-counting ones (e.g., Rand and Jaccard), which gives an analytical background for further studies of pair-counting indices. We formally prove that among dozens of known indices, only two have all the properties except for being a distance: Correlation Coefﬁcient and Sokal & Sneath’s ﬁrst index (Lei et al., 2017). Surprisingly, both indices are rarely used for cluster evaluation. Correlation Coefﬁcient has the additional advantage of being easily convertible to a distance measure via the arccosine function. The obtained index has all the properties except constant baseline, which is still satisﬁed asymptotically.
To sum up, our main contributions are the following:
• We formally deﬁne properties that are desirable across various applications. We analyze an extensive list of cluster similarity indices and mathematically prove or disprove all properties for each of them (Tables 3, 4).
• We provide a methodology for choosing a suitable validation index for a particular application. In particular, we identify previously overlooked indices that dominate the most popular ones (Section 5).
• We formalize the notion of constant baseline and provide a framework for its analysis; for pair-counting indices, we introduce the notion of asymptotic constant baseline (Section 4.6). We also provide a deﬁnition for monotonicity that uniﬁes and extends previous attempts; for pair-counting indices, we introduce a strengthening of monotonicity (Section 4.5).
We believe that our uniﬁed and extensive analysis provides a useful tool for researchers and practitioners because research outcomes and application performances are highly dependent on the validation index that is chosen.

Comparison with prior work While there are previous attempts to analyze cluster similarity indices, our work uniﬁes and signiﬁcantly extends them. In particular, Lei et al. (2017) only consider biases of pair-counting indices, Meila˘ (2007) analyzes properties of Variation of Information, and Vinh et al. (2010) analyze information-theoretic indices.
Amigo´ et al. (2009) consider properties desirable for text clustering and mostly focus on monotonicity. Most importantly, Amigo´ et al. (2009) do not consider constant baseline (the absence of preference towards speciﬁc cluster sizes), which we found to be extremely important. In contrast, the problem of indices favoring clusterings with smaller or larger clusters has been identiﬁed by, e.g., Albatineh et al. (2006); Lei et al. (2017); Vinh et al. (2009; 2010). This problem is typically addressed by modifying a particular index (or family of indices) such that the obtained measure does not suffer from this problem. However, as we show in this paper, these modiﬁcations often lead to other important properties not being satisﬁed. We refer to Appendix A for a more detailed comparison to related research.
In the current paper, we introduce new properties, formalize existing ones, and mathematically prove or disprove each property for an extensive list of validation indices. This broader and more rigorous approach leads to conclusions that considerably differ from how validation indices are currently being chosen.
2. Cluster Similarity Indices
We consider clustering n elements numbered from 1 to n, so that a clustering can be represented by a partition of {1, . . . , n} into disjoint subsets. Capital letters A, B, C will be used to name the clusterings, and we will represent them as A = {A1, . . . , AkA }, where Ai is the set of elements belonging to i-th cluster. If a pair of elements v, w ∈ V lie in the same cluster in A, we refer to them as an intra-cluster pair of A, while inter-cluster pair will be used otherwise. The total number of pairs is denoted by N = n2 . The value that an index V assigns to the similarity between partitions A and B will be denoted by V (A, B). We now deﬁne some of the indices used throughout the paper. A more comprehensive list, together with formal deﬁnitions, is given in Appendices B.1, B.2.
Pair-counting indices consider clusterings to be similar if they agree on many pairs. Formally, let A be the N -dimensional vector indexed by the set of element-pairs, where the entry corresponding to (v, w) equals 1 if (v, w) is an intra-cluster pair and 0 otherwise. Let MAB be the N × 2 matrix that results from concatenating the two (column-) vectors A and B. Each row of MAB is either 11, 10, 01, or 00. Let the pair-counts N11, N10, N01, N00 denote the number of occurrences for each of these rows in MAB.

3

Systematic Analysis of Cluster Similarity Indices: How to Validate Validation Measures

Deﬁnition 1. A pair-counting index is a similarity index that can be expressed as a function of the pair-counts N11, N10, N01, N00.

Some popular pair-counting indices are Rand and Jaccard:

R=

N11 + N00

, J=

N11 .

N11 + N10 + N01 + N00

N11 + N10 + N01

Adjusted Rand (AR) is an adaptation of Rand ensuring that when B is random, we have AR(A, B) = 0 in expectation. A less widely used index is the Pearson Correlation Coefﬁcient (CC) between the binary incidence vectors A and B.1 Another index, which we discuss further in more details, is the Correlation Distance CD(A, B) := π1 arccos CC(A, B). In Appendix B.2, we formally deﬁne 27 known pair-counting indices and only mention those of particular interest throughout the main text.
Information-theoretic indices consider clusterings similar if they share a lot of information, i.e., if little information is needed to transform one clustering into the other. Formally, let H(A) := H(|A1|/n, . . . , |AkA |/n) be the Shannon entropy of the cluster-label distribution of A. Similarly, the joint entropy H(A, B) is deﬁned as the entropy of the distribution with probabilities (pij)i∈[kA],j∈[kB], where pij = |Ai ∩ Bj|/n. Then, the mutual information of two clusterings can be deﬁned as M (A, B) = H(A) + H(B) − H(A, B). There are multiple ways of normalizing the mutual information:

M (A, B)

NMI(A, B) =

,

(H(A) + H(B))/2

M (A, B) NMImax(A, B) = max{H(A), H(B)} .

NMI is known to be biased towards smaller clusters, and several modiﬁcations try to mitigate this bias: Adjusted Mutual Information (AMI) and Standardized Mutual Information (SMI) subtract the expected mutual information from M (A, B) and normalize the obtained value (Vinh et al., 2009), while Fair NMI (FNMI) multiplies NMI by a penalty factor e−|kA−kB|/kA (Amelio & Pizzuti, 2015).

3. Motivating Experiments
Evidently, many different cluster similarity indices are used by researchers and practitioners. A natural question is: how to choose the best one? Before trying to answer this question, it is important to understand whether the problem is relevant. Indeed, if the indices are very similar to each other and agree in most practical applications, then one can safely
1Spearman and Pearson correlation are equal when comparing binary vectors. Kendall rank correlation for binary vectors coincides with the Hubert index that is linearly equivalent to Rand.

(a) FNMI, R, AR, J, D, W, FMeasure, BCubed

(b) NMI, NMImax, VI, AMI, S&S, CC, CD

Figure 1. Inconsistency of indices: shapes denote the reference partition, captions indicate indices favoring the candidate.
Table 1. Inconsistency on real-world clustering datasets, %

NMI VI AR S&S CC

NMI VI AR S&S CC

– 40.3 15.7 20.1 18.5 – 37.6 36.0 37.2 – 11.7 8.3 – 3.6 –

take any index. In this section, we demonstrate that this is not the case, and that the choice matters.
First, we illustrate the inconsistency of all indices. We say that two indices V1 and V2 are inconsistent for a triplet of partitions (A, B1, B2) if V1(A, B1) > V1(A, B2) but V2(A, B1) < V2(A, B2). We took 15 popular cluster similarity measures and constructed just four triplets such that each pair of indices is inconsistent for at least one triplet. One such triplet is shown in Figure 1: for this simple example, about half of the indices prefer the left candidate, while the others prefer the right one. Other examples can be found in Appendix F.1.
Thus, we see that the indices differ. But can this affect conclusions obtained in experiments on real data? To check that, we ran 8 well-known clustering algorithms (Scikitlearn, 2020) on 16 real-world datasets from the UCI machine learning repository (Dua & Graff, 2017). Each dataset, together with a pair of algorithms, gives a triplet of partitions (A, B1, B2), where A is a reference partition and B1, B2 are provided by two algorithms. For a given pair of indices and all such triplets, we look at whether the indices are consistent. Table 1 shows the relative inconsistency for several popular indices.2 The inconsistency rate is signiﬁcant: e.g., popular measures Adjusted Rand and Variation of Information disagree in almost 40% of the cases. Importantly, the best agreeing indices are S&S and CC, which satisfy most of our properties, as shown in the next section.
2The extended table together with a detailed description of the experimental setup and more analysis is given in Appendix F.2.

4

Systematic Analysis of Cluster Similarity Indices: How to Validate Validation Measures

Table 2. Comparing algorithms according to different indices

NMI FNMI AMI VI R AR J S&S CC

A1
0.9479 0.9304 0.7815 0.5662 0.9915 0.5999 0.4329 0.8004 0.6004

A2
0.9482 0.8722 0.7533 0.5503 0.9901 0.6213 0.4556 0.8262 0.6371

notation V (A, B) and V (N11, N10, N01, N00).
Some of the indices have slight variants that are essentially the same. For example, the Hubert index (Hubert, 1977) is a linear transformation of the Rand index: H = 2R − 1. All the properties deﬁned in this paper are invariant under linear transformations and interchanging A and B. Hence, we deﬁne the following linear equivalence relation on similarity indices and check the properties for at most one representative of each equivalence class.
Deﬁnition 2. Similarity indices V1 and V2 are linearly equivalent if there exists a nonconstant linear function f such that either V1(A, B) = f (V2(A, B)) or V1(A, B) = f (V2(B, A)).

To demonstrate that the choice of similarity index may affect the ﬁnal performance in a real production scenario, we conducted an experiment within a major news aggregator system. The system groups news articles to events and shows the list of most important events to users. For grouping, a clustering algorithm is used, and the quality of this algorithm affects the user experience: merging different clusters may lead to not showing an important event, while too much splitting may cause duplicate events. When comparing several candidate clustering algorithms, it is important to determine which one is the best for the system. Online experiments are expensive and can be used only for the best candidates. Thus, we need a tool for an ofﬂine comparison. For this purpose, we manually created a reference partition on a small fraction of news articles to evaluate the candidates. We performed such an ofﬂine comparison for two candidate algorithms A1 and A2 and observed that different indices preferred different algorithms (see Table 2). In particular, well-known FNMI, AMI, and Rand prefer A1 that disagrees with most of the indices. Then, we launched an online user experiment and veriﬁed that the candidate A2 is better for the system according to user preferences. This shows the importance of choosing the right index for ofﬂine comparisons. See Appendix F.3 for a more detailed description of this experiment.
4. Analysis of Cluster Similarity Indices
In this section, we motivate and formally deﬁne properties that are desirable for cluster similarity indices. We start with simple and intuitive ones that can be useful in some applications but not always necessary. Then, we discuss more complicated properties, ending with constant baseline, which is extremely important but least trivial. In Tables 3 and 4, indices of particular interest are listed along with the properties satisﬁed. In Appendix C, we give the proofs for all entries of these tables. For pair-counting indices we perform a more detailed analysis and deﬁne additional properties. For such indices, we interchangeably use the

This allows us to conveniently restrict to indices for which higher numerical values indicate higher similarity of partitions. Appendix Table 6 in lists equivalences among indices.
4.1. Property 1: Maximal Agreement
The numerical value that an index assigns to a similarity must be easily interpretable. In particular, it should be easy to see whether the candidate clustering is maximally similar to (i.e., coincides with) the reference clustering. Formally, we require that V (A, A) = cmax is constant and is a strict upper bound for V (A, B) for all A = B. The equivalence from Deﬁnition 2 allows us to assume that V (A, A) is a maximum w.l.o.g. This property is easy to check, and it is satisﬁed by almost all indices, except for SMI and Wallace.

Property 1 : Minimal Agreement The maximal agreement property makes the upper range of the index interpretable. Similarly, a numerical value for low agreement would make the lower range interpretable. A minimal agreement is not well deﬁned for general partitions: it is unclear which partition is most dissimilar to a given one. However, by Lemma 1 in Appendix B.3, pair-counting indices form a subclass of graph similarity indices. For a graph with edge-set E, it is clear that the most dissimilar graph is its complement (i.e., with edge-set EC). Comparing a graph to its complement results in pair-counts N11 = N00 = 0 and N10 + N01 = N . This motivates the following deﬁnition:
Deﬁnition 3. A pair-counting index V has the minimal agreement property if there exists a constant cmin so that V (N11, N10, N01, N00) ≥ cmin with equality if and only if N11 = N00 = 0.

This property is satisﬁed by Rand, Correlation Coefﬁcient,

and Sokal&Sneath, while it is violated by Jaccard, Wal-

lace, and Dice. Adjusted Rand does not have this property

since substituting N11 = N00 = 0 gives the non-constant

AR(0, N10, N01, 0) = − 1

N10 N01
2

.

2 N −N10N01

5

Systematic Analysis of Cluster Similarity Indices: How to Validate Validation Measures

4.2. Property 2: Symmetry
Similarity is intuitively understood as a symmetric concept. Therefore, a good similarity index is expected to be symmetric, i.e., V (A, B) = V (B, A) for all partitions A, B.3 Tables 3 and 4 show that most indices are symmetric. The asymmetric ones are precision and recall (Wallace) and FNMI (Amelio & Pizzuti, 2015), which is a product of NMI and an asymmetric penalty factor.
4.3. Property 3: Linear Complexity
For clustering tasks on large datasets, running time is crucial, and algorithms with superlinear time can be infeasible. In these cases, a validation index with superlinear running time would be a signiﬁcant bottleneck. Furthermore, computationally heavy indices also tend to be complicated and hard to interpret intuitively. We say that an index has linear complexity when its worst-case running time is O(n). In Appendix C.2, we prove that any pair-counting index has O(n) complexity. Many general indices have this property as well, except for SMI and AMI.
4.4. Property 4. Distance
For some applications, a distance-interpretation of dissimilarity may be desirable: whenever A is similar to B and B is similar to C, then A should also be somewhat similar to C. For example, assume that the reference clustering (e.g., labeled by experts) is an approximation of the ground truth. In such situations, it may be reasonable to argue that the reference clustering is at most a distance ε from the true one, so that the triangle inequality bounds the dissimilarity of the candidate clustering to the unknown true clustering.
A function d is a distance metric if it satisﬁes three distance axioms: 1) symmetry (d(A, B) = d(B, A)); 2) positivedeﬁniteness (d(A, B) ≥ 0 with equality iff A = B); 3) the triangle inequality (d(A, C) ≤ d(A, B)+d(B, C)). We say that V is linearly transformable to a distance metric if there exists a linearly equivalent index that satisﬁes these three distance axioms. Note that all three axioms are invariant under rescaling of d. We have already imposed symmetry as a separate property, and positive-deﬁniteness is equivalent to the maximal agreement property. Therefore, whenever V has these two properties, it satisﬁes the distance property iff d(A, B) = cmax − V (A, B) satisﬁes the triangle inequality, for cmax as deﬁned in Section 4.1.
Examples of popular indices having this property are Variation of Information and the Mirkin metric. In Vinh et al. (2010), it is proved that when Mutual Information is nor-
3In some applications, A and B may have different roles (e.g., reference and candidate partitions), and an asymmetric index may be suitable if there are different consequences of making false positives or false negatives.

malized by the maximum of entropies, the resulting NMI is equivalent to a distance metric. A proof that the Jaccard index is equivalent to a distance is given in Kosub (2019). See Appendix C.1 for all the proofs.
Correlation Distance Among all the considered indices, there are two pair-counting ones having all the properties except for being a distance: Sokal&Sneath and Correlation Coefﬁcient. However, the correlation coefﬁcient can be transformed to a distance metric via a non-linear transformation. We deﬁne Correlation Distance (CD) as CD(A, B) := π1 arccos CC(A, B), where CC is the Pearson correlation coefﬁcient and the factor 1/π scales the index to [0, 1]. To the best of our knowledge, this Correlation Distance has never before been used as a similarity index for comparing clusterings throughout the literature.
Theorem 1. The Correlation Distance is indeed a distance.

Proof. A proof of this is given in (Van Dongen & Enright, 2012). We give an alternative proof that allows for a geometric interpretation. First, we map each partition A to an N -dimensional vector on the unit sphere by



√1


1

 

N mA

 u(A) :=

A− N m

1



A−

A
N

1

− √1 1

N

if kA = 1, if 1 < kA < n, if kA = n,

where 1 is the N -dimensional all-one vector, A is the

binary vector representation of a partition introduced in

Section 2, and mA = N11 + N10 is the number of intra-
community pairs of A. Straightforward computation gives A − mNA 1 = mA(N − mA)/N , and standard inner
product

A − mNA 1, B − mNB 1

= N11 − mAmB N
= N11N00 − N10N01 , N

so that the inner product indeed corresponds to CC:

u(A), u(B) =

N11N00 − N10N01 mA(N − mA)mB(N − mB)

= CC(A, B).

It is a well-known fact that the inner product of two vectors of unit length corresponds to the cosine of their angle. Hence, taking the arccosine gives us the angle. The angle between unit vectors corresponds to the distance along the unit hypersphere. As u is an injection from the set of partitions to points on the unit sphere, we may conclude that this index is indeed a distance on the set of partitions.

In Section 4.6, we show that the distance property of Correlation Distance is achieved at the cost of not having the exact constant baseline, though it is still satisﬁed asymptotically.

6

Systematic Analysis of Cluster Similarity Indices: How to Validate Validation Measures

Table 3. Requirements for general similarity indices

Table 4. Requirements for pair-counting indices4

Max. agreement Symmetry Distance Lin. complexity Monotonicity Const. baseline Max. agreement Min. agreement Symmetry Distance Lin. complexity Monotonicity Strong monotonicity Const. baseline As. const. baseline Type of bias

NMI       NMImax      
FNMI       VI      
SMI       FMeasure      
BCubed       AMI      

R        AR         
J        W         D         CC          S&S          CD         

4.5. Property 5: Monotonicity
When one clustering is changed such that it resembles the other clustering more, the similarity score ought to improve. Hence, we require an index to be monotone w.r.t. changes that increase the similarity. This can be formalized via the following deﬁnition.
Deﬁnition 4. For clusterings A and B, we say that B is an A-consistent improvement of B iff B = B and all pairs of elements agreeing in A and B also agree in A and B .
This leads to the following monotonicity property.
Deﬁnition 5. An index V satisﬁes the monotonicity property if for every two clusterings A, B with 1 < kA < n and any B that is an A-consistent improvement of B, it holds that V (A, B ) > V (A, B) and V (B , A) > V (B, A).
The trivial cases kA = 1 and kA = n were excluded to avoid inconsistencies with the constant baseline property deﬁned in Section 4.6. To look at monotonicity from a different perspective, we deﬁne the following operations:
• Perfect split: B is a perfect split of B (w.r.t. A) if B is obtained from B by splitting a single cluster B1 into two clusters B1, B2 such that no two elements of the same cluster of A are in different parts of this split, i.e., for all i, Ai ∩ B1 is a subset of either B1 or B2.
• Perfect merge: We say that B is a perfect merge of B (w.r.t. A) if there exists some Ai and B1, B2 ⊂ Ai such that B is obtained by merging B1, B2 into B1.
The following theorem gives an alternative deﬁnition of monotonicity and is proven in Appendix E.1.
Theorem 2. B is an A-consistent improvement of B iff B can be obtained from B by a sequence of perfect splits and perfect merges.
Note that this monotonicity is a stronger form of the ﬁrst two constraints deﬁned in (Amigo´ et al., 2009): Cluster Homo-

geneity is a weaker form of our monotonicity w.r.t. perfect splits, while Cluster Equivalence is equivalent to our monotonicity w.r.t. perfect merges.
Monotonicity is a critical property that should be satisﬁed by any sensible index. Surprisingly, not all indices satisfy this: we have found counterexamples that prove that SMI, FNMI, and Wallace do not have the monotonicity property. Furthermore, for NMI, whether monotonicity is satisﬁed depends on the normalization: the normalization by the average of the entropies has monotonicity, while the normalization by the maximum of the entropies does not.
Property 5 . Strong Monotonicity For pair-counting indices, we can deﬁne a stronger monotonicity property in terms of pair-counts.
Deﬁnition 6. A pair-counting index V satisﬁes strong monotonicity if it is increasing in N11, N00 when N10 + N01 > 0, and decreasing in N10, N01 when N11 +N00 > 0.
Note that the conditions N10 +N01 > 0 and N11 +N00 > 0 are needed to avoid contradicting maximal and minimal agreement respectively. This property is stronger than monotonicity as it additionally allows for comparing similarities across different settings: we could compare the similarity between A1, B1 on n1 elements with the similarity between A2, B2 on n2 elements, even when n1 = n2. This ability to compare similarity scores across different numbers of elements is similar to the Few data points property of SMI (Romano et al., 2014) that allows its scale to have a similar interpretation across different settings.
We found several examples of indices that have Property 5 while not satisfying Property 5 . Jaccard and Dice indices are constant w.r.t. N00, so they are not strongly monotone.
4All known pair-counting indices excluded from this table do not satisfy either constant baseline, symmetry, or maximal agreement.

7

Systematic Analysis of Cluster Similarity Indices: How to Validate Validation Measures

A more interesting example is the Adjusted Rand index, which may become strictly larger if we only increase N10.
4.6. Property 6. Constant Baseline
This property is arguably the most signiﬁcant: it is less intuitive than the other ones and may lead to unexpected consequences in practice. Informally, a good similarity index should not give a preference to a candidate clustering B over another clustering C just because B has many or few clusters. This intuition can be formalized using random partitions: assume that we have some reference clustering A and two random partitions B and C. While intuitively both random guesses are equally bad approximations of A, it has been known throughout the literature (Albatineh et al., 2006; Vinh et al., 2009; 2010; Romano et al., 2014) that some indices tend to give higher scores for random guesses with a larger number of clusters. Ideally, we want the similarity value of a random candidate w.r.t. the reference partition to have a ﬁxed expected value cbase (independent of A or the sizes of B). However, this does require a careful formalization of random candidates.
Deﬁnition 7. We say that a distribution over clusterings B is element-symmetric if for every two clusterings B and B that have the same cluster-sizes, B returns B and B with equal probabilities.
This allows us to deﬁne the constant baseline property.
Deﬁnition 8. An index V satisﬁes the constant baseline property if there exists a constant cbase so that, for any clustering A with 1 < kA < n and every element-symmetric distribution B, it holds that EB∼B[V (A, B)] = cbase.
In the deﬁnition, we have excluded the cases where A is a trivial clustering consisting of either 1 or n clusters. Including them would cause contradictions with maximal agreement whenever we choose B as the (element-symmetric) distribution that returns A with probability 1. In Appendix D.1, we prove that to verify whether an index satisﬁes Deﬁnition 8, it sufﬁces to check whether it holds for distributions B that are uniform over clusterings with ﬁxed cluster sizes. From this equivalence, it will also follow that Deﬁnition 8 is indeed symmetric. Note that the formulation in terms of element-symmetric distributions allows for a wide range of clustering distributions. For example, the cluster sizes could be drawn from a power-law distribution, which is often observed in practice (Arenas et al., 2004; Clauset et al., 2004).
Constant baseline is extremely important in many practical applications: if an index violates this property, then its optimization may lead to undesirably biased results. For instance, if a biased index is used to choose the best algorithm among several candidates, then it is likely that the decision will be biased towards those who produce too large

or too small clusters. This problem is often attributed to NMI (Vinh et al., 2009; Romano et al., 2014), but we found that almost all indices suffer from it. The only indices that satisfy the constant baseline property are Adjusted Rand index, Correlation Coefﬁcient, SMI, and AMI with cbase = 0 and Sokal&Sneath with cbase = 1/2. Interestingly, out of these ﬁve indices, three were speciﬁcally designed to satisfy this property, which made them less intuitive and resulted in other important properties being violated.
The only condition under which the constant baseline property can be safely ignored is knowing in advance all cluster sizes. In this case, bias towards particular cluster sizes would not affect decisions. However, we are not aware of any practical application where such an assumption can be made. Note that knowing only the number of clusters is insufﬁcient. We illustrate this in Appendix D.4, where we also show that the bias of indices violating the constant baseline is easy to identify empirically.

Property 6 : Asymptotic Constant Baseline For pair-
counting indices, a deeper analysis of the constant baseline property is possible. Let mA = N11 + N10, mB = N11 + N01 be the number of intra-cluster pairs of A and B, respectively. If the distribution B is uniform over clusterings with given sizes, then mA and mB are both constant. Furthermore, the pair-counts N10, N01, N00 are functions of N, mA, mB, N11. Hence, to ﬁnd the expected value of the index, we need to inspect it as a function of a single random variable N11. For a random pair, the probability that it is an intra-cluster pair of both clusterings is mAmB/N 2, so the expected values of the pair-counts are

N11 := mAmB , N
N01 := mB − N11,

N10 := mA − N11,

(1)

N00 := N − mA − mB + N11.

We can use these values to deﬁne a weaker variant of constant baseline.
Deﬁnition 9. A pair-counting index V has an asymptotic constant baseline if there exists a constant cbase so that V N11, N10, N01, N00 = cbase for all mA, mB ∈ (0, N ).

In contrast to Deﬁnition 8, asymptotic constant baseline is very easy to verify: one can substitute the values from (1) to the index and check whether the obtained value is constant. Another important observation is that under mild assumptions V (N11, N10, N01, N00) converges in probability to V N11, N10, N01, N00 as n grows which justiﬁes the usage of the name asymptotic constant baseline, see Appendix D.2 for more details.
Note that the non-linear transformation of Correlation Coefﬁcient to Correlation Distance makes the latter one violate the constant baseline property. CD does, however, still have

8

Systematic Analysis of Cluster Similarity Indices: How to Validate Validation Measures

the asymptotic constant baseline at 1/2 and we prove in Appendix E.2 that the expectation in Deﬁnition 8 is very close to this value.5
Biases of Cluster Similarity Indices Given the fact that there are so many biased indices, one may be interested in what kind of candidates they favor. While it is unclear how to formalize this concept for general validation indices, we can do this for pair-counting ones by analyzing them in terms of a single variable: the number of inter-cluster pairs. This value characterizes the granularity of a clustering: it is high when the clustering consists of many small clusters while it is low if it consists of a few large clusters.
Informally, we say that an index suffers from PairDec bias if it may favor less inter-cluster pairs. Similarly, PairInc bias means that an index may prefer more inter-cluster pairs. These biases can be formalized as follows.
Deﬁnition 10. Let V be a pair-counting index and deﬁne V (s)(mA, mB) = V N11, N10, N01, N00 for the expected pair-counts as deﬁned in (1). We say that
(i) V suffers from PairDec bias if there are mA, mB ∈ (0, N ) such that dmdB V (s)(mA, mB) > 0;
(ii) V suffers from PairInc bias if there are mA, mB ∈ (0, N ) such that dmdB V (s)(mA, mB) < 0.
Note that this deﬁnition does require V (s) to be differentiable in mA and mB. However, this is the case for all pair-counting indices in this work. Applying this deﬁnition to Jaccard J (s)(mA, mB ) = N(mA+mmABm)−BmAmB and Rand R(s)(mA, mB) = 1 − (mA + mB)/N + 2mAmB/N 2 immediately shows that Jaccard suffers from PairDec bias and Rand suffers from both biases. The direction of the monotonicity for the bias of Rand is determined by the condition 2mA > N . Performing the same for Wallace and Dice shows that both suffer from PairDec bias. Note that an index satisfying the asymptotic constant baseline property will not have any of these biases as V (s)(mA, mB) = cbase.
While there have been previous attempts to characterize types of biases (Lei et al., 2017), they mostly rely on analyses based on the number of clusters. However, our analysis shows that the number of clusters is not the correct variable for such a characterization of pair-counting indices. While having many clusters often goes hand-in-hand with having many inter-cluster pairs, it is not always the case: if there are signiﬁcant differences between the cluster sizes (e.g., one large cluster and many small clusters), then the clustering may consist of many clusters while having relatively few inter-cluster pairs. We discuss this in more detail in Appendix E.3. Additionally, Experiments shown in Figures 3
5There is also another transformation of CC to a distance CD = 2(1 − CC). However, it can be shown that CD approximates a constant baseline less well than CD.

and 4 of the Appendix show that in such cases, most indices have a similar bias as if there were few clusters, which is consistent with our characterization of such biases in terms of the number of inter-cluster pairs.
5. Discussion and Conclusion
At this point, we better understand the theoretical properties of cluster similarity indices, so it is time to answer the question: which index is the best? Unfortunately, there is no simple answer, but we can make an informed decision. In this section, we sum up what we have learned, argue that there are indices that are strictly better alternatives than some widely used ones, and give practical advice on how to choose a suitable index for a given application.
Among all properties discussed in this paper, monotonicity is the most crucial one. Violating this property is a fatal problem: such indices can prefer candidates which are strictly worse than others. Hence, we advise against using the well-known NMImax, FMeasure, FNMI, and SMI indices.
The constant baseline property is much less trivial but is equally important: it addresses the problem of preferring some partitions only because they have small or large clusters. This property is essential unless you know all cluster sizes. Since we are not aware of practical applications where all cluster sizes are known, we assume below that this is not the case.6 This requirement is satisﬁed by just a few indices, so we are only left with AMI, Adjusted Rand (AR), Correlation Coefﬁcient (CC), and Sokal&Sneath (S&S). Additionally, Correlation Distance (CD) satisﬁes constant baseline asymptotically and deviations from the exact constant baseline are extremely small (see Appendix E.2).
Let us note that among the remaining indices, AR is strictly dominated by CC and S&S since it does not have the minimum agreement and strong monotonicity. Also, similarly to AMI, AR is speciﬁcally created to have a constant baseline, which made this index more complex and less intuitive than other pair-counting indices. Hence, we are only left with four indices: AMI, S&S, CC, and CD.
According to their theoretical properties, all these indices are good, and any of them can be chosen. Figure 2 illustrates how a ﬁnal decision can be made. First, one can decide whether the distance property is needed. For example, suppose one wants to cluster the algorithms by comparing the partitions provided by them. If one would want to use a metric clustering algorithm for this, the index would have to be a distance. In this case, CD would be the best choice. If the distance property is not needed, one could base the decision
6However, in applications where such an assumption holds, it can be reasonable to use, e.g., BCubed, Variation of Information, and NMI.

9

Systematic Analysis of Cluster Similarity Indices: How to Validate Validation Measures

Yes Use CD

Do you need metric properties
of scores?
No

No
Do you need fast index
computation?

Less

Importance of small clusters

More

Yes Use CC or S&S

Use CC or S&S

Use AMI

Figure 2. Example of how one can make a decision among good cluster similarity indices.

on computational complexity. In many large-scale applications, using clustering algorithms with higher than linear running time is infeasible. Understandably, it is undesirable if the computation of a validation score takes longer than the actual clustering algorithm. Another example is multiple comparisons: choosing the best algorithm among many candidates (differing, e.g., by a parameter value). If fast computation is required, then AMI is not a proper choice, and one has to choose between CC and S&S. Otherwise, all three indices are suitable according to our formal constraints.
Let us discuss an (informal) criterion that may help to choose between AMI and pair-counting alternatives. Different indices may favor a different balance between errors in small and large clusters. In particular, all pair-counting indices give larger weights to errors in large clusters: misclassifying one element in a cluster of size k costs k − 1 incorrect pairs. It is known (empirically) that informationtheoretic indices do not have this property and give a higher weight to small clusters (Amigo´ et al., 2009).7 Amigo´ et al. (2009) argue that for their particular application (text clustering), it is desirable not to give a higher weight to large clusters. In contrast, there are applications where the opposite may hold. For instance, consider a system that groups user photos based on identity and shows these clusters to a user as a ranked list. In this case, a user is likely to investigate the largest clusters consisting of known people and would rarely spot an error in a small cluster. The same applies to any system that ranks the clusters, e.g., to news aggregators. Based on what is desirable for a particular application, one can choose between AMI and pair-counting CC and S&S.
The ﬁnal decision between CC and S&S is hard to make
7This is an interesting aspect that has not received much attention in our research since we believe that the desired balance between large and small clusters may differ per application and we are not aware of a proper formalization of this “level of balance” in a general form.

since they are equally good in terms of their theoretical properties. Interestingly, although some works (Choi et al., 2010; Lei et al., 2017) list Pearson correlation as a cluster similarity index, it has not received attention that our results suggest it deserves, similarly to S&S. First, both indices are interpretable. CC is a correlation between the two incidence vectors, which is a very natural concept. S&S is the average of precision, recall (for binary classiﬁcation of pairs) and their inverted counterparts, which can also be intuitively understood. Also, CC and S&S usually agree in practice: in Tables 1 and 7 we can see that they have the largest agreement. Hence, one can take any of these indices. Another option would be to check whether there are situations where these indices disagree and, if this happens, perform an experiment similar to what we did in Section 3 for news aggregation. While some properties listed in Tables 3 and 4 are not mentioned in the discussion above, they can be important for particular applications. For instance, maximum and minimum agreements are useful for interpretability, but they can also be essential if some operations are performed over the index values: e.g., averaging the scores of different algorithms. Symmetry can be necessary if there is no “gold standard” partition, but algorithms are compared only to each other.
Finally, let us remark that in an early version of this paper, we conjectured that the constant baseline and distance properties are mutually exclusive. This turns out to be true: in ongoing work, we prove an impossibility theorem: for pair-counting indices monotonicity, distance, and constant baseline cannot be simultaneously satisﬁed.
Acknowledgements
Most of this work was done while Martijn Go¨sgens was visiting Yandex and Moscow Institute of Physics and Technology (Russia). The work of Martijn Go¨sgens is supported by the Netherlands Organisation for Scientiﬁc Research (NWO) through the Gravitation NETWORKS grant no. 024.002.003. The work of Liudmila Prokhorenkova is supported by the Ministry of Education and Science of the Russian Federation in the framework of MegaGrant 075-152019-1926 and by the Russian President grant supporting leading scientiﬁc schools of the Russian Federation NSh2540.2020.1. Furthermore, the authors would like to thank Nelly Litvak and Remco van der Hofstad for their helpful feedback and guidance throughout this project. We also thank Borislav Kozlovskii for the help with the news aggregator experiment.

10

Systematic Analysis of Cluster Similarity Indices: How to Validate Validation Measures

References
Albatineh, A. N., Niewiadomska-Bugaj, M., and Mihalko, D. On similarity indices and correction for chance agreement. Journal of Classiﬁcation, 23(2):301–313, 2006.
Allahyari, M., Pouriyeh, S., Asseﬁ, M., Safaei, S., Trippe, E. D., Gutierrez, J. B., and Kochut, K. A brief survey of text mining: Classiﬁcation, clustering and extraction techniques. arXiv preprint arXiv:1707.02919, 2017.
Amelio, A. and Pizzuti, C. Is normalized mutual information a fair measure for comparing community detection methods? In Proceedings of the 2015 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining 2015, pp. 1584–1585, 2015.
Amigo´, E., Gonzalo, J., Artiles, J., and Verdejo, F. A comparison of extrinsic clustering evaluation metrics based on formal constraints. Information retrieval, 12(4):461–486, 2009.
Arenas, A., Danon, L., Diaz-Guilera, A., Gleiser, P. M., and Guimera, R. Community analysis in social networks. The European Physical Journal B, 38(2):373–380, 2004.
Batagelj, V. and Bren, M. Comparing resemblance measures. Journal of classiﬁcation, 12(1):73–90, 1995.
Ben-David, S. and Ackerman, M. Measures of clustering quality: A working set of axioms for clustering. Advances in neural information processing systems, 21:121–128, 2008.
Choi, S.-S., Cha, S.-H., and Tappert, C. C. A survey of binary similarity and distance measures. Journal of Systemics, Cybernetics and Informatics, 8(1):43–48, 2010.
Clauset, A., Newman, M. E., and Moore, C. Finding community structure in very large networks. Physical review E, 70(6):066111, 2004.
Donnat, C. and Holmes, S. Tracking network dynamics: A survey of distances and similarity metrics. arXiv preprint arXiv:1801.07351, 2018.
Dua, D. and Graff, C. UCI machine learning repository, 2017. URL http://archive.ics.uci.edu/ml.
GitHub. Clustering datasets. https://github.com/ deric/clustering-benchmark, 2020.
Hubert, L. Nominal scale response agreement as a generalized correlation. British Journal of Mathematical and Statistical Psychology, 30(1):98–103, 1977.
Hubert, L. and Arabie, P. Comparing partitions. Journal of classiﬁcation, 2(1):193–218, 1985.
Kleinberg, J. An impossibility theorem for clustering. Advances in neural information processing systems, 15:463– 470, 2002.

Kosub, S. A note on the triangle inequality for the jaccard distance. Pattern Recognition Letters, 120:36–38, 2019.

Lei, Y., Bezdek, J. C., Romano, S., Vinh, N. X., Chan, J., and Bailey, J. Ground truth bias in external cluster validity indices. Pattern Recognition, 65:58–70, 2017.

Meila˘, M. Comparing clusterings—an information based distance. Journal of multivariate analysis, 98(5):873–895, 2007.

Newman, M. E. and Girvan, M. Finding and evaluating community structure in networks. Physical review E, 69 (2):026113, 2004.

Romano, S., Bailey, J., Nguyen, V., and Verspoor, K. Standardized mutual information for clustering comparisons: one step further in adjustment for chance. In International Conference on Machine Learning, pp. 1143–1151, 2014.

Romano, S., Vinh, N. X., Bailey, J., and Verspoor, K. Adjusting for chance clustering comparison measures. The Journal of Machine Learning Research, 17(1):4635–4666, 2016.

Scikit-learn.

Clustering algorithms.

https:

//scikit-learn.org/stable/modules/

clustering.html, 2020.

Strehl, A. Relationship-based clustering and cluster ensembles for high-dimensional data mining. PhD thesis, 2002.

Van Dongen, S. and Enright, A. J. Metric distances derived from cosine similarity and pearson and spearman correlations. arXiv preprint arXiv:1208.3145, 2012.

Van Laarhoven, T. and Marchiori, E. Axioms for graph clustering quality functions. The Journal of Machine Learning Research, 15(1):193–215, 2014.

Vinh, N. X., Epps, J., and Bailey, J. Information theoretic measures for clusterings comparison: is a correction for chance necessary? In Proceedings of the 26th annual international conference on machine learning, pp. 1073– 1080, 2009.

Vinh, N. X., Epps, J., and Bailey, J. Information theoretic measures for clusterings comparison: Variants, properties, normalization and correction for chance. The Journal of Machine Learning Research, 11:2837–2854, 2010.

Virtanen, S. and Girolami, M. Precision-recall balanced topic modelling. In Advances in Neural Information Processing Systems, pp. 6750–6759, 2019.

11

Systematic Analysis of Cluster Similarity Indices: How to Validate Validation Measures

Wang, Z., Zheng, L., Li, Y., and Wang, S. Linkage based face clustering via graph convolution network. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1117–1125, 2019.
Wibowo, W. and Williams, H. E. Strategies for minimising

errors in hierarchical web categorisation. In Proceedings of the eleventh international conference on Information and knowledge management, pp. 525–531, 2002.
Xu, D. and Tian, Y. A comprehensive survey of clustering algorithms. Annals of Data Science, 2(2):165–193, 2015.

12

Systematic Analysis of Cluster Similarity Indices: How to Validate Validation Measures

A. Further Related Work

Several attempts to the comparative analysis of cluster similarity indices have been made in the literature, both in machine learning and complex networks communities. In particular, the problem of indices favoring clusterings with smaller or larger clusters has been identiﬁed (Albatineh et al., 2006; Vinh et al., 2009; 2010; Lei et al., 2017). The most popular approach to resolving the bias of an index is to subtract its expected value and normalize the resulting quantity to obtain an index that satisﬁes the maximum agreement property. This approach has let to ‘adjusted’ indices such as AR (Hubert & Arabie, 1985) and AMI (Vinh et al., 2009). In Albatineh et al. (2006), the family of pair-counting indices L is introduced for which adjusted forms can be computed easily. This family corresponds to the set of all pair-counting indices that are linear functions of N11 for ﬁxed N11 + N10, N11 + N01. In (Romano et al., 2016), a generalization of information-theoretic indices by the Tsallis q-entropy is given and this is shown to correspond to pair-counting indices for q = 2. Formulas are provided for adjusting these generalized indices for chance.
A disadvantage of this adjustment scheme is that an index can be normalized in many ways, while it is difﬁcult to grasp the differences between these normalizations intuitively. For example, three variants of AMI have been introduced (Vinh et al., 2009), and we show that normalization by the maximum entropies results in an index that fails monotonicity. Romano et al. (2014) go one step further by standardizing mutual information, while Amelio & Pizzuti (2015) multiply NMI with a penalty factor that decreases with the difference in the number of clusters.
In summary, all these works take a popular biased index and ‘patch’ it to get rid of this bias. This approach has two disadvantages: ﬁrstly, these patches often introduce new problems (e.g., FNMI and SMI fail monotonicity), and secondly, the resulting index is usually less interpretable than the original. We have taken a different approach in our work: instead of patching existing indices, we analyze previously introduced indices to see whether they satisfy more properties. Our analysis shows that AR is dominated by Pearson correlation, which was introduced more than 100 years before AR. Therefore, there was no need to construct AR from Rand in the ﬁrst place.
In Lei et al. (2017), the biases of pair-counting indices are characterized. They deﬁne these biases as a preference towards either few or many clusters. They prove that the direction of Rand’s bias depends on the Havrda-Charvat entropy of the reference clustering. In the present work, we show that the number of clusters is not an adequate quantity for expressing these biases. We introduce methods to easily analyze the bias of any pair-counting index and simplify the condition for the direction of Rand’s bias to mA < N/2.
A paper closely related to the current research (Amigo´ et al., 2009) formulates several constraints (axioms) for cluster similarity indices. Their cluster homogeneity is a weaker analog of our monotonicity w.r.t. perfect splits while their cluster equivalence is equivalent to our monotonicity w.r.t. perfect merges. The third rag bag constraint is motivated by a subjective claim that “introducing disorder into a disordered cluster is less harmful than introducing disorder into a clean cluster”. While this is important for their particular application (text clustering), we found no other work that deemed this constraint necessary; hence, we disregarded this constraint in the current research. The last constraint by Amigo´ et al. (2009) concerns the balance between making errors in large and small clusters. Though this is an interesting aspect that has not received much attention in our research, this constraint poses a particular balance while we believe that the desired balance may differ per application. Hence, this property seems to be non-binary and we are not aware of a proper formalization of this “level of balance” in a general form. Hence, we do not include this in our list of formal properties. The most principal difference of our work compared to Amigo´ et al. (2009) is the constant baseline which was not analyzed in their work. We ﬁnd this property extremely important while it is failed by most of the widely used indices including their BCubed. To conclude, our research gives a more comprehensive list of constraints and focuses on those that are desirable in a wide range of applications. We also cover all similarity indices often used in the literature and give formal proofs for all index-property combinations.
A property similar to our monotonicity property is also given in Meila˘ (2007), where the similarity between clusterings A and B is upper-bounded by the similarity between A and A ⊗ B (as deﬁned in Section C.4). One can show that this property is implied by our monotonicity but not vice versa, i.e., the variant proposed by Meila˘ (2007) is weaker. Our analysis of monotonicity generalizes and uniﬁes previous approaches to this problem, see Theorem 2, which relates consistent improvements to perfect splits and merges.
While we focus on external cluster similarity indices that compare a candidate partition with a reference one, there are also internal similarity measures that estimate the quality of partitions with respect to internal structure of data (e.g., Silhouette, Hubert-Gamma, Dunn, and many other indices). Kleinberg (2002) used an axiomatic approach for internal measures and

13

Systematic Analysis of Cluster Similarity Indices: How to Validate Validation Measures

proved an impossibility theorem: there are three simple and natural constraints such that no internal clustering measure can satisfy all of them. More work in this direction can be found in, e.g., Ben-David & Ackerman (2008). In network analysis, internal measures compare a candidate partition with the underlying graph structure. They quantify how well a community structure (given by a partition) ﬁts the graph and are often referred to as goodness or quality measures. The most well-known example is modularity (Newman & Girvan, 2004). Axioms that these measures ought to satisfy are given in (Ben-David & Ackerman, 2008; Van Laarhoven & Marchiori, 2014). Note that all pair-counting indices discussed in this paper can also be used for graph-partition similarity, as we discuss in Section B.3.

B. Cluster Similarity Indices

B.1. General Indices

Here we give the deﬁnitions of the indices listed in Table 3. We deﬁne the contingency variables as nij = |Ai ∩ Bj|. We note that all indices discussed in this paper can be expressed as functions of these contingency variables.

The F-Measure is deﬁned as the harmonic mean of recall and precision. Recall is deﬁned as

1 kA

r(A, B) = n

max {nij},
j∈[kB ]

i=1

and precision is its symmetric counterpart r(B, A).

In (Amigo´ et al., 2009), recall is redeﬁned as

1 kA 1 kB

r (A, B) =

n2ij ,

n i=1 |Ai| j=1

and BCubed is deﬁned as the harmonic mean of r (A, B) and r (B, A).

The remainder of the indices are information-theoretic and require some additional deﬁnitions. Let p1, . . . , p be a discrete distribution (i.e., all values are nonnegative and sum to 1). The Shannon entropy is then deﬁned as

H(p1, . . . , p ) := − pi log(pi).
i=1
The entropy of a clustering is deﬁned as the entropy of the cluster-label distribution of a random item, i.e.,
H(A) := H(|A1|/n, . . . , |AkA |/n), and similarly for H(B). The joint entropy H(A, B) is then deﬁned as the entropy of the distribution with probabilities (pij )i∈[kA],j∈[kB], where pij = nij /n. Variation of Information (Meila˘, 2007) is deﬁned as
VI(A, B) = 2H(A, B) − H(A) − H(B).

Mutual information is deﬁned as

M (A, B) = H(A) + H(B) − H(A, B).

The mutual information between A and B is upper-bounded by H(A) and H(B), which gives multiple possibilities to
normalize the mutual information. In this paper, we discuss two normalizations: normalization by the average of the entropies 12 (H(A) + H(B)), and normalization by the maximum of entropies max{H(A), H(B)}. We will refer to the corresponding indices as NMI and NMImax, respectively:

M (A, B)

NMI(A, B) =

,

(H(A) + H(B))/2

M (A, B) NMImax(A, B) = max{H(A), H(B)} .

14

Systematic Analysis of Cluster Similarity Indices: How to Validate Validation Measures

Fair NMI is a variant of NMI that includes a factor that penalizes large differences in the number of clusters (Amelio & Pizzuti, 2015). It is given by
FNMI(A, B) = e−|kA−kB|/kA NMI(A, B).

In this deﬁnition, NMI may be normalized in various ways. We note that a different normalization would not result in more properties being satisﬁed.

Adjusted Mutual Information addresses for the bias of NMI by subtracting the expected mutual information (Vinh et al.,

2009). It is given by

AMI(A, B) =

M (A, B) − EB ∼C(S(B))[M (A, B )] . H(A) · H(B) − EB ∼C(S(B))[M (A, B )]

Here, a normalization by the geometric mean of the entropies is used, while other normalizations are also used (Vinh et al., 2009).

Standardized Mutual Information standardizes the mutual information w.r.t. random permutations of the items (Romano et al., 2014), i.e.,
SMI(A, B) = M (A, B) − EB ∼C(S(B))(M (A, B )) , σB ∼C(S(B))(M (A, B ))

where σ denotes the standard deviation. Calculating the expected value and standard deviation of the mutual information is nontrivial and requires signiﬁcantly more computation power than other indices. For this, we refer to the original paper (Romano et al., 2014). Note that this index is symmetric since it does not matter whether we keep A constant while randomly permuting B or keep B constant while randomly permuting A.

B.2. Pair-counting Indices and Their Equivalences
Pair-counting similarity indices are deﬁned in Table 5. Table 6 lists linearly equivalent indices (see Deﬁnition 2). Note that our linear equivalence differs from the less restrictive monotonous equivalence given in (Batagelj & Bren, 1995). In the current work, we have to restrict to linear equivalence as the constant baseline property is not invariant to non-linear transformations.

B.3. Deﬁning the Subclass of Pair-counting Indices
From Deﬁnition 1, it follows that a pair-counting index is a function of two binary vectors A, B of length N . Note that this binary-vector representation has some redundancy: whenever u, v and v, w form intra-cluster pairs, we know that u, w must also be an intra-cluster pair. Hence, not every binary vector of length N represents a clustering. The class of N -dimensional binary vectors is, however, isomorphic to the class of undirected graphs on n vertices. Therefore, pair-counting indices are also able to measure the similarity between graphs. For example, for an undirected graph G = (V, E), one can consider its incidence vector G = (1{{v, w} ∈ E})v,w∈V . Hence, pair-counting indices can be used to measure the similarity between two graphs or between a graph and a clustering. So, one may see a connection between graph and cluster similarity indices. For example, the Mirkin metric is a pair-counting index that coincides with the Hamming distance between the edge-sets of two graphs (Donnat & Holmes, 2018). Another example is the Jaccard graph distance, which turns out to be more appropriate for comparing sparse graphs (Donnat & Holmes, 2018). Thus, all pair-counting indices and their properties discussed in the current paper can also be applied to graph-graph and graph-partition similarities.
In this section, we show that the subclass of pair-counting similarity indices can be uniquely deﬁned by the property of being pair-symmetric.
For two graphs G1 and G2 let MG1G2 denote the N × 2 matrix that is obtained by concatenating their adjacency vectors. Let us write VM(G)(MG1G2 ) for the similarity between two graphs G1, G2 according to some graph similarity index V (G). We will now characterize all pair-counting similarity indices as a subclass of the class of similarity indices between undirected graphs.
Deﬁnition 11. We deﬁne a graph similarity index VM(G)(MG1G2 ) to be pair-symmetric if interchanging two rows of MG1,G2 leaves the index unchanged.
8Throughout the literature, the Mirkin metric is deﬁned as 2(N10 + N01), but we use this variant as it satisﬁes the scale-invariance.

15

Systematic Analysis of Cluster Similarity Indices: How to Validate Validation Measures

Table 5. A selection of pair-counting indices. Most of these indices are taken from (Lei et al., 2017).

Index (Abbreviation) Rand (R)
Adjusted Rand (AR)
Jaccard (J) Jaccard Distance (JD) Wallace1 (W ) Wallace2 Dice Correlation Coefﬁcient (CC)
Correlation Distance (CD)
Sokal&Sneath-I (S&S)
Minkowski Hubert (H) Fowlkes&Mallow
Sokal&Sneath-II Normalized Mirkin8 Kulczynski McConnaughey Yule Baulieu-I Russell&Rao Fager&McGowan Peirce Baulieu-II Sokal&Sneath-III Gower&Legendre Rogers&Tanimoto Goodman&Kruskal

Expression

N11 +N00 N11 +N10 +N01 +N00

N11− (N N1111++N N1100)+(N N0111+ +N N0001) (N11+N10)+2 (N11+N01) − (N N1111++N N1100)+(N N0111+ +N N0001)

N11 N11 +N10 +N01

N10 +N01 N11 +N10 +N01

N11 N11 +N10

N11 N11 +N01

2N11 2N11 +N10 +N01

√

N11 N00 −N10 N01

(N11 +N10 )(N11 +N01 )(N00 +N10 )(N00 +N01 )

π1 arccos

√

N11 N00 −N10 N01

(N11 +N10 )(N11 +N01 )(N00 +N10 )(N00 +N01 )

+ + + 1

N11

4 N11+N10

N11 N11 +N01

N00 N00 +N10

N00 N00 +N01

N10 +N01 N11 +N10

N11 +N00 −N10 −N01 N11 +N10 +N01 +N00

√

N11

(N11 +N10 )(N11 +N01 )

1 2

N11

1 2

N11

+N10

+N01

N10 +N01 N11 +N10 +N01 +N00

+ 1

N11

2 N11+N10

N11 N11 +N01

N121 −N10 N01 (N11 +N10 )(N11 +N01 )

N11 N00 −N10 N01 N11 N10 +N01 N00

(N11 +N10 +N01 +N00 )(N11 +N00 )+(N10 −N01 )2 (N11 +N10 +N01 +N00 )2

N11 N11 +N10 +N01 +N00

√ − N11
(N11 +N10 )(N11 +N01 )

√1 2 N11+N10

N11 N00 −N10 N01

(N11 +N01 )(N00 +N10 )

N11 N00 −N10 N01 (N11 +N10 +N01 +N00 )2

√

N11 N00

(N11 +N10 )(N11 +N01 )(N00 +N10 )(N00 +N01 )

N11 +N00

N11

+

1 2

(N10

+N01

)+N00

N11 +N00 N11 +2(N10 +N01 )+N00

N11 N00 −N10 N01 N11 N00 +N10 N01

We give the following result. Lemma 1. The class of pair-symmetric graph similarity indices coincides with the class of pair-counting cluster similarity indices.
Proof. A matrix is an ordered list of its rows. An unordered list is a multiset. Hence, when we disregard the ordering of

16

Systematic Analysis of Cluster Similarity Indices: How to Validate Validation Measures

Table 6. Equivalent pair-counting indices

Representative Index
Rand Jaccard Wallace1 Kulczynski

Equivalent indices
Normalized Mirkin Metric, Hubert Jaccard Distance Wallace2 McConnaughey

the matrix MAB, we get a multiset of the rows. This multiset contains at most four distinct elements with multiplicities corresponding to the four pair-counts. Therefore, each VM(G)(MAB) that is symmetric w.r.t. interchanging rows is equivalently a function of the pair-counts of A and B.
C. Checking Properties for Indices
In this section, we check all non-trivial properties for all indices. The properties of symmetry, maximal/minimal agreement and asymptotic constant baseline can trivially be tested by simply checking V (B, A) = V (A, B), V (A, A) = cmax, V (0, N10, N01, 0) = cmin and V N11, N10, N01, N00 = cbase respectively. For pair-counting indices, we will frequently use the notation pAB = N11/N, pA = (N11 + N10)/N, pB = (N11 + N01)/N and write V (p)(pAB, pA, pB) instead of V (N11, N10, N01, N00).
C.1. Distance C.1.1. POSITIVE CASES NMI and VI. In (Vinh et al., 2010) it is proven that for max-normalization 1 − NMI is a distance, while in (Meila˘, 2007) it is proven that VI is a distance.
Rand. The Mirkin metric 1 − R corresponds to a rescaled version of the size of the symmetric difference between the sets of intra-cluster pairs. The symmetric difference is known to be a distance metric.
Jaccard. In (Kosub, 2019), it is proven that the Jaccard distance 1 − J is indeed a distance.
Correlation Distance. In Theorem 1 it is proven that Correlation Distance is indeed a distance.
C.1.2. NEGATIVE CASES To prove that an index that satisﬁes symmetry and maximal agreement is not linearly transformable to a distance metric, we only need to disprove the triangle inequality for one instance of its equivalence class that is nonnegative and equals zero for maximal agreement.
FNMI and Wallace. These indices cannot be transformed to distances as they are not symmetric.
SMI. SMI does not satisfy the maximal agreement property (Romano et al., 2014), so it cannot be transformed to a metric.
FMeasure and BCubed. We will use a simple counter-example, where |V | = 3, kA = 1, kB = 2, kC = 3. Let us denote the FMeasure and BCubed by F M, BC respectively. We get
1 − FM(A, C) = 1 − 0.5 > (1 − 0.8) + (1 − 0.8) = (1 − FM(A, B)) + (1 − FM(B, C))
and
1 − BC(A, C) = 1 − 0.5 > (1 − 0.71) + (1 − 0.8) ≈ (1 − BC(A, B)) + (1 − BC(B, C)),
so that both indices violate the triangle inequality in this case.

17

Systematic Analysis of Cluster Similarity Indices: How to Validate Validation Measures

Adjusted Rand, Dice, Correlation Coefﬁcient, Sokal&Sneath and AMI. For these indices, we use the following counter-example: Let A = {{0, 1}, {2}, {3}}, B = {{0, 1}, {2, 3}}, C = {{0}, {1}, {2, 3}}. Then pAB = pBC = 1/6 and pAC = 0 while pA = pC = 1/6 and pB = 1/3. By substituting these variables, one can see that
1 − V (p)(pAC , pA, pC ) > (1 − V (p)(pAB, pA, pB)) + (1 − V (p)(pBC , pB, pC )),
holds for each of these indices, contradicting the triangle inequality. The same A, B and C also form a counter-example for AMI.

C.2. Linear Complexity
We will frequently make use of the following lemma: Lemma 2. The nonzero values of nij can be computed in O(n).

Proof. We will store these nonzero values in a hash-table that maps the pairs (i, j) to their value nij. These values are obtained by iterating through all n elements and incrementing the corresponding value of nij. For hash-tables, searches and insertions are known to have amortized complexity complexity O(1), meaning that any sequence of n such actions has worst-case running time of O(n), from which the result follows.

C.2.1. POSITIVE CASES
NMI, FNMI and VI. Given the positive values of nij, it is clear that the joint and marginal entropy values can be computed in O(n). From these values, the indices can be computed in constant time, leading to a worst-case running time of O(n).

FMeasure and BCubed. Note that in the expressions of recall and precision as deﬁned by these indices, only the positive values of nij contribute. Furthermore, all of the variables ai, bj and nij appear at most once, so that these can indeed be computed in O(n).
Pair-counting indices. Note that N11 = nij>1 n2ij can obviously be computed in O(n). Similarly, mA = ki=A1 a2i and mB can be computed in O(kA), O(kB) respectively. The other pair-counts are then obtained by N10 = mA − N11, N01 = mB − N11 and N00 = N − mA − mB + N11.

C.2.2. NEGATIVE CASES: AMI AND SMI.
Both of these require the computation of the expected mutual information. It has been known (Romano et al., 2016) that this has a worst-case running time of O(n · max{kA, kB}) while max{kA, kA} can be O(n).

C.3. Strong Monotonicity

C.3.1. POSITIVE CASES

Correlation Coefﬁcient. This index has the property that inverting one of the binary vectors results in the index ﬂipping

sign. Furthermore, the index is symmetric. Therefore, we only need to prove that this index is increasing in N11. We take

the

derivative

and

omit

the

constant

factor

((N00

+

N10)(N00

+

N01))−

1 2

:

N00 − (N11N00 − N10N01) · 12 (2N11 + N10 + N01)

(N11 + N10)(N11 + N01)

[(N11 + N10)(N11 + N01)]1.5

= 12 N11N00(N10 + N01) + N00N10N01 + 21 N10N01(2N11 + N10 + N01) > 0.

[(N11 + N10)(N11 + N01)]1.5

[(N11 + N10)(N11 + N01)]1.5

Correlation Distance. The correlation distance satisﬁes strong monotonicity as it is a monotone transformation of the correlation coefﬁcient, which meets the property.

Sokal&Sneath. All four fractions are nondecreasing in N11, N00 and nonincreasing in N10, N01 while for each of the variables there is one fraction that satisﬁes the monotonicity strictly so that the index is strongly monotonous.

18

Systematic Analysis of Cluster Similarity Indices: How to Validate Validation Measures

Rand Index. For the Rand index, it can be easily seen from the form of the index that it is increasing in N11, N00 and decreasing in N10, N01 so that it meets the property.

C.3.2. NEGATIVE CASES
Jaccard, Wallace, Dice. All these three indices are constant w.r.t. N00. Therefore, these indices do not satisfy strong monotonicity.

Adjusted Rand. It holds that

AR(1, 2, 1, 0) < AR(1, 3, 1, 0),

so that the index does not meet the strong monotonicity property.

C.4. Monotonicity
C.4.1. POSITIVE CASES
Rand, Correlation Coefﬁcient, Sokal&Sneath, Correlation Distance. Strong monotonicity implies monotonicity. Therefore, these pair-counting indices satisfy the monotonicity property.

Jaccard and Dice. It can be easily seen that these indices are increasing in N11 while decreasing in N10, N01. For N00, we note that whenever N00 gets increased, either N10 or N01 must decrease, resulting in an increase of the index. Therefore, these indices satisfy monotonicity.

Adjusted Rand. Note that for b, b + d > 0, it holds that

a+c a

ad

> ⇔c> .

(2)

b+d b

b

We will let a, b denote the numerator and denomenator of Adjusted Rand while c, d will denote their change when incrementing N11 or N00 while decrementing N10 or N01. For Adjusted Rand, we have

1 a = N11 − N (N11 + N10)(N11 + N01),

1 b = a + 2 (N10 + N01).

Because of this, when we increment either N11 or N00 while decrementing either N10 or N01, we get d = c − 12 . Hence, we need to prove c > a(c − 12 )/b, or, equivalently

a c>−

= N1 (N11 + N10)(N11 + N01) − N11 .

2(b − a)

N10 + N01

For simplicity we rewrite this to

c + pAB − pApB > 0, pA + pB − 2pAB

where

pAB

=

NN11 ,

pA

=

1 N

(N11

+

N10)

and

pB

=

1 N

(N11

+

N01).

If

we

increment

N00

while

decrementing

either

N10

or N01, then c ∈ {pA, pB}. The symmetry of AR allows us to w.l.o.g. assume that c = pA. We write

p + pAB − pApB = p2A + (1 − 2pA)pAB .

A pA + pB − 2pAB

pA + pB − 2pAB

When pA ≤ 12 , then this is clearly positive. For the case pA > 12 , we bound pAB ≤ pA and bound the numerator by p2A + (1 − 2pA)pA = (1 − pA)pA > 0.

This proves the monotonicity for increasing N00. When incrementing N11 while decrementing either N10 or N01, we get c ∈ {1 − pA, 1 − pB}. Again, we assume w.l.o.g. that c = 1 − pA and write

1 − pA + pAB − pApB = pA(1 − pA) + (1 − 2pA)(pB − pAB) .

pA + pB − 2pAB

pA + pB − 2pAB

19

Systematic Analysis of Cluster Similarity Indices: How to Validate Validation Measures

This is clearly positive whenever pA ≤ 12 . When pA > 12 , we bound pAB ≥ pA + pB − 1 and rewrite the numerator as

pA(1 − pA) + (1 − 2pA)(pA − 1) = (1 − pA)(3pA − 1) > 0.

This proves monotonicity for increasing N11. Hence, the monotonicity property is met.

NMI and VI. Let B be obtained by a perfect split of a cluster B1 into B1, B2. Note that this increases the entropy of the candidate while keeping the joint entropy constant. Let us denote this increase in the candidate entropy by the conditional entropy H(B |B) = H(B ) − H(B) > 0. Now, for NMI, the numerator increases by H(B |B) while the denominator increases by at most H(B |B) (dependent on H(A) and the speciﬁc normalization that is used). Therefore, NMI increases. Similarly, VI decreases by H(B |B). Concluding, both NMI and VI are monotonous w.r.t. perfect splits. Now let B be obtained by a perfect merge of B1, B2 into B1 . This results in a difference of the entropy of the candidate H(B ) − H(B) = −H(B|B ) < 0. The joint entropy decreases by the same amount, so that the mutual information remains unchanged. Therefore, the numerator of NMI remains unchanged while the denominator may or may not change, depending on the normalization. For min- or max-normalization, it may remain unchanged while for any other average it increases. Hence, NMI does not satisfy monotonicity w.r.t. perfect merges for min- and max-normalization but does satisfy this for average-normalization. For VI, the distance will decrease by H(B|B ) so that it indeed satisﬁes monotonicity w.r.t. perfect merges.

AMI. Let B be obtained by splitting a cluster B1 into B1, B2. This split increases the mutual information by H(B |B) − H(A ⊗ B |A ⊗ B). Recall the deﬁnition of the meet A ⊗ B from C.4 and note that the joint entropy equals H(A ⊗ B). For a perfect split we have H(A ⊗ B |A ⊗ B) = 0. The expected mutual information changes with
EA ∼C(S(A))[M (A , B ) − M (A , B)] = H(B |B) − EA ∼C(S(A))[H(A ⊗ B ) − H(A ⊗ B)],
where we choose to randomize A instead of B and B for simplicity. Note that for all A ,
H(A ⊗ B) − H(A ⊗ B ) = H(A ⊗ B |A ⊗ B) ≥ 0,
with equality if and only if the split is a perfect split w.r.t. A . Unless A consists exclusively of singleton clusters, there is a positive probability that this split is not perfect, so that the expected value is positive. Furthermore, for the normalization term, we have H(A)H(B ) < H(A)H(B) + H(B |B). Combining this, we get
AMI(A, B ) = M (A, B) − EA ∼C(S(A))[M (A , B)] + EA ∼C(S(A))[H(A ⊗ B |A ⊗ B)]
H(A)H(B ) − H(B |B) − EA ∼C(S(A))[M (A , B)] + EA ∼C(S(A))[H(A ⊗ B |A ⊗ B)] > M (A, B) − EA ∼C(S(A))[M (A , B)] + EA ∼C(S(A))[H(A ⊗ B |A ⊗ B)]
H(A)H(B) − EA ∼C(S(A))[M (A , B)] + EA ∼C(S(A))[H(A ⊗ B |A ⊗ B)] > M (A, B) − EA ∼C(S(A))[M (A , B)] = AMI(A, B).
H(A)H(B) − EA ∼C(S(A))[M (A , B)]
This proves that AMI satisﬁes monotonicity w.r.t. perfect splits. Now let B be obtained by a perfect merge of B1, B2 into B1 . Again, we have H(B ) − H(B) = −H(B|B < 0) and M (A, B ) = M (A, B). Let A ∼ C(S(A)) (again, randomizing A instead of B and B for simplicity), then H(A ⊗ B ) ≥ H(A ⊗ B) − H(B|B ) with equality if and only if B is a perfect merge w.r.t. A which happens with probability strictly less than 1 (unless A consists of a single cluster). Therefore, as long as kA > 1, the expected mutual

20

Systematic Analysis of Cluster Similarity Indices: How to Validate Validation Measures

information decreases. For the normalization, we have H(A)H(B ) < H(A)H(B). Hence,

AMI(A, B ) =

M (A, B ) − EA ∼C(S(A))[M (A , B )] H(A)H(B ) − EA ∼C(S(A))[M (A , B )]

= M (A, B) − EA ∼C(S(A))[M (A , B )] H(A)H(B ) − EA ∼C(S(A))[M (A , B )]

> M (A, B) − EA ∼C(S(A))[M (A , B)] H(A)H(B ) − EA ∼C(S(A))[M (A , B)]

> M (A, B) − EA ∼C(S(A))[M (A , B)] H(A)H(B) − EA ∼C(S(A))[M (A , B)]

= AMI(A, B).

BCubed. Note that a perfect merge increases BCubed recall while leaving BCubed precision unchanged and that a perfect split increases precision while leaving recall unchanged. Hence, the harmonic mean increases.

C.4.2. NEGATIVE CASES
FMeasure. We give a numerical counter-example: consider A = {{0, . . . , 6}}, B = {{0, 1, 2, 3}, {4, 5}, {6}} and merge the last two clusters to obtain B = {{0, 1, 2, 3}, {4, 5, 6}}. Then, the FMeasure remains unchanged and equal to 0.73, violating monotonicity w.r.t. perfect merges.

FNMI We will give the following numerical counter-example: Consider A = {{0, 1}, {2}, {3}}, B = {{0}, {1}, {2, 3}} and merge the ﬁrst two clusters to obtain B = {{0, 1}, {2, 3}}. This results in
FNMI(A, B) ≈ 0.67 > 0.57 ≈ FNMI(A, B ).
This non-monotonicity is caused by the penalty factor that equals 1 for the pair A, B and equals exp(−1/3) ≈ 0.72 for A, B .

SMI. For this numerical counter-example we rely on the Matlab-implementation of the index by its original authors (Romano et al., 2014). Let A = {{0, . . . , 4}, {5}}, B = {{0, 1}, {2, 3}, {4}, {5}} and consider merging the two clusters resulting in B = {{0, 1, 2, 3}, {4}, {5}}. The index remains unchanged and equals 2 before and after the merge.

Wallace. Let kA = 1 and let kB > 1. Then any merge of B is a perfect merge, but no increase occurs since W1(A, B) = 1.

C.5. Constant Baseline
C.5.1. POSITIVE CASES
AMI and SMI. Both of these indices satisfy the constant baseline by construction since the expected mutual information is subtracted from the actual mutual information in the numerator.

Adjusted Rand, Correlation Coefﬁcient and Sokal&Sneath. These indices all satisfy ACB while being linear in pAB-linear for ﬁxed pA, pB. Thus, by linearity of expectation, the expected value equals the asymptotic constant.

C.5.2. NEGATIVE CASES

For all the following indices, we will analyse the counter-example given by kA = kB = n − 1. For each index, we will compute the expected value and show that it is not constant. All of these indices satisfy the maximal agreement property and maximal agreement is achieved with probability 1/N (the probability that the single intra-pair of A coincides with the single intra-pair of B). Furthermore, each case where the intra-pairs do not coincide will result in the same contingency variables and hence the same value of the index. We will refer to this value as cn(V ). Therefore, the expected value will only have to be taken over two values and will be given by

1

N −1

E[V (A, B)] = N cmax + N cn(V ).

21

Systematic Analysis of Cluster Similarity Indices: How to Validate Validation Measures

For each of these indices we will conclude that this is a non-constant function of n so that the index does not satisfy the constant baseline property.

Jaccard and Dice. For both these indices we have cmax = 1 and cn(V ) = 0 (as N11 = 0 whenever the intra-pairs do not coincide). Hence, E[V (A, B)] = N1 , which is not constant.
Rand and Wallace. As both functions are linear in N11 for ﬁxed mA = N11 + N10, mB = N11 + N01, we can compute the expected value by simply substituting N11 = mAmB/N . This will result in expected values 1 − 2/N + 2/N 2 and 1/N for Rand and Wallace respectively, which are both non-constant.

Correlation distance. Here cmax = 0 and

1

0 − 1/N 2

cn(CD) = π arccos (N − 1)/N 2 ,

so that the expected value will be given by

N −1

1

E[CD(A, B)] =

arccos −

.

Nπ

N −1

This is non-constant (it evaluates to 0.44, 0.47 for n = 3, 4 respectively). Note that this expected value converges to 12 for n → ∞, which is indeed the asymptotic baseline of the index.

FNMI and NMI. Note that in this case kA = kB so that the penalty term of FNMI will equal 1 and FNMI will coincide with NMI. Again cmax = 1. For the case where the intra-pairs do not coincide, the joint entropy will equal H(A, B) = ln(n) while each of the marginal entropies will equal

This results in

n−2

2

2

H(A) = H(B) =

ln(n) + ln(n/2) = ln(n) − ln(2).

n

n

n

2H(A) − H(A, B)

2 ln(n)

cn(NMI) =

H (A)

=1−

,

n ln(n) − 2 ln(2)

and the expected value will be given by the non-constant

N − 1 2 ln(n)

E[NMI(A, B)] = 1 −

.

N n ln(n) − 2 ln(2)

Note that as H(A) = H(B), all normalizations of MI will be equal so that this counter-example proves that none of the variants of (F)NMI satisfy the constant baseline property.

Variation of Information. In this case cmax = 0. We will use the entropies from the NMI-computations to conclude that

N −1

N −14

E[VI(A, B)] =

(2H(A, B) − H(A) − H(B)) =

ln(2),

N

Nn

which is again non-constant.

F-measure. Here cmax = 1. In the case where the intra-pairs do not coincide, all contingency variables will be either

one or zero so that both recall and precision will equal 1 − 1/n so that cn(FM) = 1 − 1/n. This results in the following

non-constant expected value

N −11

E[FM(A, B)] = 1 −

.

Nn

Note that because recall equals precision in both cases, this counter-example also works for other averages than the harmonic average.

22

Systematic Analysis of Cluster Similarity Indices: How to Validate Validation Measures

BCubed. Again cmax = 1. In the other case, the recall and precision will again be equal. Because for BCubed, the

contribution

of

cluster

i

is

given

by

1 n

max{n2ij }/|Ai |,

the

contributions

of

the

one-

and

two-clusters

will

be

given

by

n1 ,

1 2n

respectively.

Hence,

cn(BC)

=

n−2 n

+

1 2n

=

1

−

3 2n

and

we

get

the

non-constant

N −1 3

E[BC(A, B)] = 1 −

·.

N 2n

We note that again, this counter-example can be extended to non-harmonic averages of the BCubed recall and precision.

D. Further Analysis of Constant Baseline Property
D.1. Analysis of Exact Constant Baseline Property
In this section we will prove equivalence between Deﬁnition 8 and another formulation. Let S(B) denote the speciﬁcation of the cluster sizes of the clustering B, i.e., S(B) := [|B1|, . . . , |BkB |], where [. . . ] denotes a multiset. For a cluster sizes speciﬁcation s, let C(s) be the uniform distribution over clusterings B with S(B) = s. We prove the following result: Lemma 3. An index V has a constant baseline if and only if there exists a constant cbase so that, for any clustering A with 1 < kA < n and cluster sizes speciﬁcation s, it holds that EB∼C(s)[V (A, B)] = cbase.
Proof. One direction follows readily from the fact that C(s) is an element-symmetric distribution for every s. For the other direction, we write
EB∼B[V (A, B)] = PB∼B(S(B) = s) EB∼B[V (A, B)|S(B) = s]
s
= PB∼B(S(B) = s) EB∼C(s)[V (A, B)]
s
= PB∼B(S(B) = s) cbase = cbase,
s
where the sum ranges over cluster-sizes of n elements.
Symmetry of constant baseline Note that drawing B ∼ C(S(B)) is equivalent to obtaining B by randomly permuting the cluster-assignments of B. Note that for the expectation EB ∼C(S(B))[V (A, B )], it does not matter whether we randomly permute the labels of B or A, i.e.
EB ∼C(S(B))[V (A, B )] = EA ∼C(S(A))[V (A , B)].
This shows that the deﬁnition of constant baseline is indeed symmetric.
D.2. Analysis of Asymptotic Constant Baseline Property
Deﬁnition 12. An index V is said to be scale-invariant, if it can be expressed as a continuous function of the three variables pA := mA/N, pB := mB/N and pAB := N11/N .
All indices in Table 4 are scale-invariant. For such indices, we will write V (p)(pAB, pA, pB). Note that when B ∼ C(s) for some s, the values pA, pB are constants while pAB is a random variable. Therefore, we further write PAB to stress that this is a random variable. Theorem 3. Let V be a scale-invariant pair-counting index, and consider a sequence of clusterings A(n) and cluster-size speciﬁcations s(n). Let N1(1n), N1(0n), N0(1n), N0(0n) be the corresponding pair-counts. Then, for any ε > 0, as n → ∞,
P V N1(1n), N1(0n), N0(1n), N0(0n) − V N1(1n), N1(0n), N0(1n), N0(0n) > ε → 0.

Proof. We prove the equivalent statement V (p) PA(nB), p(An), p(Bn) − V (p) p(An)p(Bn), p(An), p(Bn) →P 0 .

23

Systematic Analysis of Cluster Similarity Indices: How to Validate Validation Measures

We ﬁrst prove that PA(nB) − p(An)p(Bn) →P 0 so that the above follows from the continuous mapping theorem. Chebychev’s

inequality gives

(n)

(n) (n)

1

(n)

P PAB − pA pB > ε ≤ n 2ε2 Var N11 → 0.

2

The last step follows from the fact that Var(N11) = o(n4), as we will prove in the remainder of this section. Even though in the deﬁnition, A is ﬁxed while B is randomly permuted, it is convenient to equivalently consider both clusterings are

randomly permuted for this proof.

We will show that Var(N11) = o(n4). To compute the variance, we ﬁrst inspect the second moment. Let A(S) denote the indicator function of the event that all elements of S ⊂ {1, . . . , n} are in the same cluster in A. Deﬁne B(S) similarly and let AB(S) = A(S)B(S). Let e, e1, e2 range over subsets of {1, . . . , n} of size 2. We write

N121 =

2
AB(e)
e

= AB(e1)AB(e2)
e1 ,e2

=

AB(e1)AB(e2) +

AB(e1)AB(e2) +

AB(e1)AB(e2)

|e1 ∩e2 |=2

|e1 ∩e2 |=1

|e1 ∩e2 |=0

=N11 +

AB(e1 ∪ e2) +

AB(e1)AB(e2).

|e1 ∩e2 |=1

e1 ∩e2 =∅

We take the expectation

E[N121] = E[N11] + 6 n3 E[AB({v1, v2, v3})] + n2

n−2 2 E[AB(e1)AB(e2)],

where v1, v2, v3 ∈ V distinct and e1 ∩ e2 = ∅. The ﬁrst two terms are obviously o(n4). We inspect the last term

n n−2

n

n−2

2 2 E[AB(e1)AB(e2)] = 2 P(e1 ⊂ Ai ∩ Bj) × 2 E[AB(e2)|e1 ⊂ Ai ∩ Bj] . (3)

i,j

Now we rewrite E[N11]2 to

E[N11]2 = n

n P(e1 ⊂ Ai ∩ Bj) E[AB(e2)].

2 i,j 2

Note that

n 2

E[AB(e2)]

>

n−2 2

E[AB(e2)] so that the difference between (3) and E[N11]2

can be bounded by

n n−2

22

P(e1 ⊂ Ai ∩ Bj) · (E[AB(e2)|e1 ⊂ Ai ∩ Bj] − E[AB(e2)]).

i,j

As n2 n−2 2 = O(n4), what remains to be proven is

P(e1 ⊂ Ai ∩ Bj) · (E[AB(e2)|e1 ⊂ Ai ∩ Bj] − E[AB(e2)]) = o(1).
i,j

Note that it is sufﬁcient to prove that

E[AB(e2)|e1 ⊂ Ai ∩ Bj] − E[AB(e2)] = o(1),

for all i, j. Note that E[AB(e2)] = mAmB/N 2, while

(mA − (2ai − 3))(mB − (2bj − 3))

E[AB(e2)|e1 ⊂ Ai ∩ Bj] =

(N − (2n − 3))2 .

24

Systematic Analysis of Cluster Similarity Indices: How to Validate Validation Measures

Hence, the difference will be given by

(mA − (2ai − 3))(mB − (2bj − 3)) − mAmB

(N − (2n − 3))2

N2

= N 2(mA − (2ai − 3))(mB − (2bj − 3)) − (N − (2n − 3))2mAmB

N 2(N − (2n − 3))2

N 2(N − (2n − 3))2

= N 2((2ai − 3)(2bj − 3) − mA(2bj − 3) − mB(2ai − 3)) + mAmB(2N (2n − 3) − (2n − 3)2)

N 2(N − (2n − 3))2

N 2(N − (2n − 3))2

= ((2ai − 3)(2bj − 3) − mA(2bj − 3) − mB(2ai − 3)) + mAmB (2N (2n − 3) − (2n − 3)2)

(N − (2n − 3))2

N2

(N − (2n − 3))2

O(n3) =

+ mAmB

O(n3)

(N − (2n − 3))2 N 2 N 2(N − (2n − 3))2

=o(1),

as required.

D.3. Statistical Tests for Constant Baseline
In this section, we provide two statistical tests: one test to check whether an index V satisﬁes the constant baseline property and another to check whether V has a selection bias towards certain cluster sizes.

Checking constant baseline. test the null hypothesis that

Given a reference clustering A and a number of cluster sizes speciﬁcations s1, . . . , sk, we EB∼C(si)[V (A, B)]

is constant in i = 1, . . . , k. We do so by using one-way Analysis Of Variance (ANOVA). For each cluster sizes speciﬁcation, we generate r clusterings. Although ANOVA assumes the data to be normally distributed, it is known to be robust for sufﬁciently large groups (i.e., large r).

Checking selection bias. In (Romano et al., 2014) it is observed that some indices with a constant baseline do have a selection bias; when we have a pool of random clusterings of various sizes and select the one that has the highest score w.r.t. a reference clustering, there is a bias of selecting certain cluster sizes. We test this bias in the following way: given a reference clustering A and cluster sizes speciﬁcations s1, . . . , sk, we repeatedly generate B1 ∼ C(s1), . . . , Bk ∼ C(sk). The null-hypothesis will be that each of these clusterings Bi has an equal chance of maximizing V (A, Bi). We test this hypothesis by generating r pools and using the Chi-squared test.
We emphasize that these statistical tests cannot prove whether an index satisﬁes the property or has a bias. Both will return a conﬁdence level p with which the null hypothesis can be rejected. Furthermore, for an index to not have these biases, the null hypothesis should be true for all choices of A, s1, . . . , sk, which is impossible to verify statistically.
The statistical tests have been implemented in Python and the code is available at https://github.com/ MartijnGosgens/validation_indices. We applied the tests to the indices of Tables 3 and 4. We chose n = 50, 100, 150, . . . , 1000 and r = 500. For the cluster sizes, we deﬁne the balanced cluster sizes BS(n, k) to be the cluster-size speciﬁcation for k clusters of which n − k ∗ n/k clusters have size n/k while the remainder have size n/k . Then we choose A(n) to be a clustering with sizes BS(n, n0.5 ) and consider candidates with sizes s(1n) = BS(n, n0.25 ), s(2n) = BS(n, n0.5 ), s(3n) = BS(n, n0.75 ). For each n, the statistical test returns a p-value. We use Fisher’s method to combine these p-values into one single p-value and then reject the constant baseline if p < 0.05. The obtained results agree with Tables 3 and 4 except for Correlation Distance, which is so close to having a constant baseline that the tests are unable to detect it.

D.4. Illustrating Signiﬁcance of Constant Baseline
In this section, we conduct two experiments illustrating the biases of various indices. We perform two experiments that allow us to identify the direction of the bias in different situations. Our reference clustering corresponds to the expert-annotated

25

Systematic Analysis of Cluster Similarity Indices: How to Validate Validation Measures

clustering of the production experiment described in Section 3 and Appendix F.3, where n = 924 items are grouped into kA = 431 clusters (305 of them consist of a single element).
In the ﬁrst experiment, we randomly cluster the items into k approximately equally sized clusters for various k. Figure 3 shows the averages and 90% conﬁdence bands for each index. It can be seen that some indices (e.g., NMI and Rand) have a clear increasing baseline while others (e.g., Jaccard and VI) have a decreasing baseline. In contrast, all unbiased indices have a constant baseline.
In Section 4.6 we argued that these biases could not be described in terms of the number of clusters alone. Our second experiment illustrates that the bias also heavily depends on the sizes of the clusters. In this case, items are randomly clustered into 32 clusters, 31 of which are “small” clusters of size s while one cluster has size n − 31 · s, where s is varied between 1 and 28. In Figure 4, that the biases are clearly visible. This shows that, even when ﬁxing the number of clusters, biased indices may heavily distort an experiment’s outcome.
Finally, recall that we have proven that the baseline of CD is only asymptotically constant. Figures 3 and 4 show that for practical purposes its baseline can be considered constant.

E. Additional Results
E.1. Proof of Theorem 2
Let B be an A-consistent improvement of B. We deﬁne
B ⊗ B = {Bj ∩ Bj |Bj ∈ B, Bj ∈ B , Bj ∩ Bj = ∅}
and show that B ⊗ B can be obtained from B by a sequence of perfect splits, while B can be obtained from B ⊗ B by a sequence of perfect merges. Indeed, the assumption that B does not introduce new disagreeing pairs guarantees that any Bj ∈ B can be split into Bj ∩ B1, . . . , Bj ∩ BkB without splitting over any intra-cluster pairs of A. Let us prove that B can be obtained from B ⊗ B by perfect merges. Suppose there are two B1 , B2 ∈ B ⊗ B such that both are subsets of some Bj . Assume that this merge is not perfect, then there must be v ∈ B1 , w ∈ B2 such that v, w are in different clusters of A. As v, w are in the same cluster of B , it follows from the deﬁnition of B ⊗ B that v, w must be in different clusters of B. Hence, v, w is an inter-cluster pair in both A and B, while it is an intra-cluster pair of B , contradicting the assumption that B is an A-consistent improvement of B. This concludes the proof.

E.2. Deviation of CD from Constant Baseline

Theorem. Given ground truth A with a number of clusters 1 < kA < n, a cluster-size speciﬁcation s and a random partition B ∼ C(s), the expected difference between Correlation Distance and its baseline is given by

1

1 ∞ (2k)! EB∼C(s)[CC(A, B)2k+1]

EB∼C(s)[CD(A, B)] − 2 = − π 22k(k!)2

2k + 1

.

k=1

Proof. We take the Taylor expansion of the arccosine around CC(A, B) = 0 and get

1 1 ∞ (2k)! CC(A, B)2k+1

CD(A, B) = − 2π

22k (k!)2

. 2k + 1

k=0

We take the expectation of both sides and note that the ﬁrst moment of CC equals zero, so the starting index is k = 1.

For B ∼ C(s) and large n, the value CC(A, B) will be concentrated around 0. This explains that in practice, the mean tends to be very close to the asymptotic baseline.

E.3. Comparison with Lei et al. (2017)
Lei et al. (2017) describe the following biases for cluster similarity indices: NCinc — the average value for a random guess increases monotonically with the Number of Clusters (NC) of the candidate; NCdec — the average value for a random guess decreases monotonically with the number of clusters, and GTbias — the direction of the monotonicity depends on the

26

Systematic Analysis of Cluster Similarity Indices: How to Validate Validation Measures

0.8

NMI

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0.8

NMImax

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0.7

FNMI

0.6

0.5

0.4

0.3

0.2

0.1

0.0100 0.0075 0.0050 0.0025 0.0000 0.0025 0.0050 0.0075 0.0100
0.5
0.4
0.3
0.2
0.1
0.0

BCubed

AMI 5.5 5.0 4.5 4.0 3.5 3.0 2.5 2.0
1.0
0.9
0.8
0.7
0.6
0.5

VI 0.5 0.4 0.3 0.2 0.1

FMeasure

AR 0.002 0.001 0.000 0.001 0.002 R

0.010

J 0.5

W 0.0200

D

0.008 0.4 00..00115705

0.006

0.3

0.0125

0.0100

0.004

0.2

0.0075

0.002

0.1

0.0050

0.0025

0.000

0.0

0.0000

0.503 0.502 0.501 0.500 0.499 0.498
0

S&S 0.003

0.002

0.001

0.000

0.001

0.002

100 200 300 400 500

0

Number of clusters

100 200 300 400 Number of clusters

CC 0.50075

0.50050

0.50025

0.50000

0.49975

0.49950

0.49925

0.49900

500

0

CD 100 200 300 400 500
Number of clusters

Figure 3. The reference clustering of Appendix F.3 (n = 924 and kA = 431) is compared to random clusterings. Each clustering consists of k approximately equally-sized clusters, where k is varied between 2 and 512. For each k, 200 random clusterings are generated. For each index, we plot the average score, along with a 90% conﬁdence band.

27

Systematic Analysis of Cluster Similarity Indices: How to Validate Validation Measures

0.5

NMI

0.4

0.3

0.2

0.1

0.40

NMImax

0.35

0.30

0.25

0.20

0.15

0.10

0.05

0.200

FNMI

0.175

0.150

0.125

0.100

0.075

0.050

0.025

0.020

AMI

0.015

5.2

0.010

0.005

5.0

0.000

0.005

4.8

0.010

0.015

4.6

0.020

4.4

VI 0.160 0.155 0.150 0.145 0.140

FMeasure

0.087

BCubed 1.0

R

0.004

AR

0.086 0.8 0.003

0.085

0.084

0.002 0.6

0.083

0.001

0.082

0.4

0.000

0.081

0.001

0.080

0.2

0.002

0.079

0.003

0.012

J 1.0

W 0.024

D

0.011

0.8

0.022

0.010

0.6

0.020

0.009

0.4

0.018

0.008

0.2 0.016

0.007

0.014

0.0

0.515 0.510 0.505 0.500 0.495 0.490 0.485
0

S&S 0.010 0.005 0.000 0.005 0.010

5 10 15 20 25

0

Small cluster size

CC 0.504

0.503

0.502

0.501

0.500

0.499

0.498

0.497

5 10 15 20 25 Small cluster size

0.496 0

CD 5 10 15 20 25
Small cluster size

Figure 4. The reference clustering of Appendix F.3 (n = 924 and kA = 431) is compared to random clusterings. Each clustering consists of 31 “small” clusters of size s while the last cluster has size 924 − 31 · s, where s is varied between 1 and 28. For each s, 200 random clusterings are generated. For each index, we plot the average score, along with a 90% conﬁdence band.

28

Systematic Analysis of Cluster Similarity Indices: How to Validate Validation Measures

speciﬁc Ground Truth (GT), i.e., on the reference partition. In particular, the authors conclude from numerical experiments that Jaccard suffers from NCdec and analytically prove that Rand suffers from GTbias, where the direction of the bias depends on the quadratic entropy of the ground truth clustering. Here we argue that these biases are not well deﬁned, suggest replacing them by well-deﬁned analogs, and show how our analysis allows to easily test indices on these biases.
We argue that the quantity of interest should not be the number of clusters, but the number of inter-cluster pairs of the candidate. Theorem 3 shows that the asymptotic value of the index depends on the number of intra-cluster pairs of both clusterings (or equivalently, the number of inter-cluster pairs). The key insight is that more clusters do not necessarily imply more inter-cluster pairs. For example, let s denote a cluster-sizes speciﬁcation for 3 clusters each of size > 2. Now let s be the cluster-sizes speciﬁcation for one cluster of size 2 and clusters of size 1. Then, any B ∼ C(s) will have 3 clusters and N − 3 2 inter-cluster pairs while any B ∼ C(s ) will have + 1 > 3 clusters and N − 22 < N − 3 2 intra-cluster pairs. For any ground truth A with cluster-sizes s, we have E[J(A, B )] > E[J(A, B)] because of a smaller amount of inter-cluster pairs In contrast, Lei et al. (2017) classiﬁes Jaccard as an NCdec index, so that we would expect the inequality to be the other way around, contradicting the deﬁnition of NCdec. The PairInc and PairDec biases that are deﬁned in Deﬁnition 10 are sound versions of these NCinc and NCdec biases because they depend on the expected number of agreeing pairs. This allows to analytically determine which bias a given pair-counting index has.

F. Experiment
F.1. Synthetic Experiment
In this experiment, we construct several simple examples to illustrate the inconsistency among the indices. Recall that two indices V1 and V2 are inconsistent for a triplet of partitions (A, B1, B2) if V1(A, B1) > V1(A, B2) but V2(A, B1) < V2(A, B2).
We take all indices from Tables 3 and 4 and construct several triplets of partitions to distinguish them all. Let us note that the pairs Dice vs Jaccard and CC vs CD cannot be inconsistent since they are monotonically transformable to each other. Also, we do not compare with SMI since it is much more computationally complex than all other indices. Thus, we end up with 13 indices and are looking for simple inconsistency examples.
The theoretical minimum of examples needed to ﬁnd inconsistency for all pairs of 13 indices is 4. We were able to ﬁnd such four examples, see Figure 5. In this ﬁgure, we show four inconsistency triplets. For each triplet, the shapes (triangle, square, etc.) denote the reference partition A. Left and right ﬁgures show candidate partitions B1 and B2. In the caption, we specify which similarity indices favor this candidate partition over the other one.
It is easy to see that for each pair of indices, there is a simple example where they disagree. For example, NMI and NMImax are inconsistent for triplets 3. Also, we know that Jaccard in general favors larger clusters, while Rand and NMI often prefer smaller ones. Hence, they often disagree in this way (see the triplets 2 and 4).
F.2. Experiments on Real Datasets
In this section, we test whether the inconsistency affects conclusions obtained in experiments on real data.
For that, we used the following 16 UCI datasets (Dua & Graff, 2017): Arrhythmia, Balance Scale, Ecoli, Heart Statlog, Letter, Segment, Vehicle, WDBC, Wine, Wisc, Cpu, Iono, Iris, Sonar, Thy, Zoo (see GitHub (2020) for datasets and references). The values of the “target class” ﬁeld were used as a reference partition.
On these datasets, we ran 8 well-known clustering algorithms (Scikit-learn, 2020): KMeans, AfﬁnityPropagation, MeanShift, AgglomerativeClustering, DBSCAN, OPTICS, Birch, GaussianMixture. For AgglomerativeClustering, we used 4 different linkage types (‘ward’, ‘average’, ‘complete’, ‘single’). For GaussianMixture, we used 4 different covariance types (‘spherical’, ‘diag’, ‘tied’, ‘full’). For methods requiring the number of clusters as a parameter (KMeans, Birch, AgglomerativeClustering, GaussianMixture), we took up to 4 different values (less than 4 if some of them are equal): 2, ref-clusters, max(2,ref-clusters/2), min(items, 2·ref-clusters), where ref-clusters is the number of clusters in the reference partition and items is the number of elements in the dataset. For MeanShift, we used the option cluster all = T rue. All other settings were default or taken from examples in the sklearn manual.
8The code is available at https://github.com/MartijnGosgens/validation_indices.

29

Systematic Analysis of Cluster Similarity Indices: How to Validate Validation Measures

(1a) FNMI, Rand, AdjRand, Jaccard, Dice, Wallace, FMeasure, BCubed

(1b) NMI, NMImax, VI, AMI, S&S, CC, CD

(2a) NMI, NMImax, FNMI, Rand, FMeasure, BCubed

(2b) VI, AMI, AdjRand, Jaccard, Dice, Wallace, S&S, CC, CD

(3a) NMImax, Rand, AdjRand, Jaccard, Dice, S&S, CC, CD, FMeasure

(3b) NMI, VI, FNMI, AMI, Wallace, BCubed

(4a) NMI, NMImax, FNMI, AMI, Rand, AdjRand, CC, CD

(4b) VI, Jaccard, Dice, Wallace, S&S, FMeasure, BCubed

Figure 5. Inconsistency of indices: each row corresponds to a triplet of partitions, shapes denote the reference partitions, the captions indicate which indices favor the corresponding candidate.

30

Systematic Analysis of Cluster Similarity Indices: How to Validate Validation Measures

Table 7. Inconsistency of indices on real-world clustering datasets, %

NMI NMImax VI FNMI AMI R AR

J W S&S CC FMeas BCub

NMI

–

NMImax

VI

FNMI

AMI

R

AR

J

W

S&S

CC

FMeas

BCub

5.4 40.3 – 41.1 –

17.3 9.2 13.4 15.7 35.2 68.4 20.1 18.5 16.5 13.2 12.5 14.1 34.3 68.8 21.1 18.9 34.7 41.8 45.2 37.6 17.1 28.8 36.0 37.2
– 23.3 24.0 19.0 29.9 57.0 26.7 23.8 – 21.1 17.3 33.3 61.3 15.1 13.6 – 15.5 35.6 71.5 21.1 20.7 – 23.5 59.4 11.7 8.3 – 35.9 23.1 23.8 – 53.5 54.8 – 3.6 –

31.7 32.0 30.3 32.4 18.1 13.6 27.5 26.7 35.0 34.4 32.5 35.8 25.3 28.1 10.7 9.7 40.7 37.4 26.2 27.8 27.0 28.8
– 7.7 –

Table 8. Algorithms preferred by different indices

NMI NMImax VI FNMI AMI R AR J W S&S CC FMeas BCub

k=2

2

1

94

2 0 4 6 10 3 3

7

7

k = 2 · ref 8

9

16

8 10 6 4 0 7 7

3

3

For all datasets, we calculated all the partitions for all methods described above. We removed all partitions having only one cluster or which raised any calculation error. Then, we considered all possible triplets A, B1, B2, where A is a reference partition and B1 and B2 are candidates obtained with two different algorithms. We have 8688 such triplets in total. For each triplet, we check whether the indices are consistent. The inconsistency frequency is shown in Table 7. Note that Wallace is highly asymmetrical and does not satisfy most of the properties, so it is not surprising that it is in general very inconsistent with others. However, the inconsistency rates are signiﬁcant even for widely used pairs of indices such as, e.g., Variation of Information vs NMI (40.3%, which is an extremely high disagreement). Interestingly, the best agreeing indices are S&S and CC which satisfy most of our properties. This means that conclusions made with these indices are likely to be similar.
Actually, one can show that all indices are inconsistent using only one dataset. This holds for 11 out of 16 datasets: heart-statlog, iris, segment, thy, arrhythmia, vehicle, zoo, ecoli, balance-scale, letter, wine. We do not present statistics for individual datasets since we found the aggregated Table 7 to be more useful.
Finally, to illustrate the biases of indices, we compare two KMeans algorithms with k = 2 and k = 2·ref-clusters. The comparison is performed on 10 datasets (where both algorithms are successfully completed). The results are shown in Table 8. In this table, biases and inconsistency are clearly seen. We see that NMI and NMImax almost always prefer the larger number of clusters. In contrast, Variation of Information and Rand usually prefer k = 2 (Rand prefers k = 2 in all cases).
F.3. Production Experiment
To show that the choice of similarity index may have an effect on the ﬁnal quality of a production algorithm, we conducted an experiment within a major news aggregator system. The system aggregates all news articles to events and shows the list of most important events to users. For grouping, a clustering algorithm is used and the quality of this algorithm affects the user experience: merging different clusters may lead to not showing an important event, while too much splitting may cause the presence of duplicate events.
There is an algorithm Aprod currently used in production and two alternative algorithms A1 and A2. To decide which alternative is better for the system, we need to compare them. For that, it is possible to either perform an online experiment or make an ofﬂine comparison, which is much cheaper and allows us to compare more alternatives. For the ofﬂine comparison, we manually grouped 1K news articles about volleyball, collected during a period of three days, into events. Then, we compared the obtained reference partition with partitions Aprod, A1, and A2 obtained by Aprod, A1, and A2, respectively (see Table 9). According to most of the indices, A2 is closer to the reference partition than A1, and A1 is closer than Aprod.

31

Systematic Analysis of Cluster Similarity Indices: How to Validate Validation Measures

Table 9. Similarity of candidate partitions to the reference one. In bold are the inconsistently ranked pairs of partitions. For some indices, we ﬂipped the sign of the index, so that larger values correspond to better agreement.

NMI NMImax FNMI AMI VI FMeasure BCubed R AR J W D S&S CC CD

Aprod
0.9326 0.8928 0.7551 0.6710 -0.6996 0.8675 0.8302 0.9827 0.4911 0.3320 0.8323 0.4985 0.7926 0.5376 -0.3193

A1
0.9479 0.9457 0.9304 0.7815 -0.5662 0.8782 0.8431 0.9915 0.5999 0.4329 0.6287 0.6042 0.8004 0.6004 -0.2950

A2
0.9482 0.9298 0.8722 0.7533 -0.5503 0.8852 0.8543 0.9901 0.6213 0.4556 0.8010 0.6260 0.8262 0.6371 -0.2802

However, according to some indices, including the well-known NMImax, NMI, and Rand, A1 better corresponds to the reference partition than A2. As a result, we see that in practical application different similarity indices may differently rank the algorithms.
To further see which algorithm better agrees with user preferences, we launched the following online experiment. During one week we compared Aprod and A1 and during another — Aprod and A2 (it is not technically possible to compare A1 and A2 simultaneously). In the ﬁrst experiment, A1 gave +0.75% clicks on events shown to users; in the second, A2 gave +2.7%, which clearly conﬁrms that these algorithms have different effects on user experience and A2 is a better alternative than A1. Most similarity indices having nice properties, including CC, CD, and S&S, are in agreement with user preferences. In contrast, AMI ranks A1 higher than A2. This can be explained by the fact that AMI gives more weight to small clusters compared to pair-counting indices, which can be undesirable for this particular application, as we discuss in Section 5.

