Near-Optimal Reviewer Splitting in Two-Phase Paper Reviewing and Conference Experiment Design

Steven Jecmen Carnegie Mellon University
sjecmen@cs.cmu.edu

Hanrui Zhang Duke University hrzhang@cs.duke.edu

Ryan Liu Carnegie Mellon University ryanliu@andrew.cmu.edu

Fei Fang Carnegie Mellon University
feif@cs.cmu.edu

Vincent Conitzer Duke University conitzer@cs.duke.edu

Nihar B. Shah Carnegie Mellon University
nihars@cs.cmu.edu

arXiv:2108.06371v1 [cs.AI] 13 Aug 2021

Abstract
Many scientiﬁc conferences employ a two-phase paper review process, where some papers are assigned additional reviewers after the initial reviews are submitted. Many conferences also design and run experiments on their paper review process, where some papers are assigned reviewers who provide reviews under an experimental condition. In this paper, we consider the question: how should reviewers be divided between phases or conditions in order to maximize total assignment similarity? We make several contributions towards answering this question. First, we prove that when the set of papers requiring additional review is unknown, a simpliﬁed variant of this problem is NP-hard. Second, we empirically show that across several datasets pertaining to real conference data, dividing reviewers between phases/conditions uniformly at random allows an assignment that is nearly as good as the oracle optimal assignment. This uniformly random choice is practical for both the two-phase and conference experiment design settings. Third, we provide explanations of this phenomenon by providing theoretical bounds on the suboptimality of this random strategy under certain natural conditions. From these easily-interpretable conditions, we provide actionable insights to conference program chairs about whether a random reviewer split is suitable for their conference.
1 Introduction
Peer review is a widely-adopted method for evaluating scientiﬁc research. Careful assignment of reviewers to papers is critically important in order to ensure that reviewers have the requisite expertise and that the resulting reviews are of high quality. At large scientiﬁc conferences, the paper assignment is usually chosen by solving an optimization problem. Given a set of papers, a set of reviewers, and a similarity matrix consisting of scores representing the level of expertise each reviewer has for each paper, the standard paper assignment problem is to ﬁnd an assignment of reviewers to papers that maximizes total similarity, subject to constraints on the reviewer and paper loads. This standard paper assignment problem is a simple matching problem and so can be eﬃciently solved (for example, through linear programming). Our work is motivated by two scenarios that arise in the context of paper assignment in conference peer review.
Motivation 1: Two-phase paper assignment. Many conferences (e.g., AAAI 2021-2022, IJCAI 2022) have adopted a two-phase review process. After the initial reviews are submitted, a subset of papers proceed to a second phase of reviews with additional reviewers assigned. There are a variety of reasons that a two-phase reviewing process can be helpful. For example, the process can be used to triage papers based on reviews in the ﬁrst phase. This can allow the conference to solicit additional reviews only on papers that obtained suﬃciently high ratings in the ﬁrst phase and have any chance of getting accepted (as done at AAAI 2021). The second phase can also help focus on evaluation of the papers in the “messy middle”—the papers at the borderline between acceptance and rejection. This messy middle model [1], which hypothesizes
1

that the acceptance decisions for some percentage of submitted papers are eﬀectively random, was proposed after the NeurIPS 2014 experiment [2] in order to explain the observed inconsistency in acceptance decisions. Additional reviewers could improve the evaluation of these papers to more accurately discern which should be accepted. Later analysis of the NeurIPS 2015 and 2016 review process found that the size of the messy middle in these conferences was 45% and 30% of submissions respectively [3]. A second phase of reviews can also be used to help compensate for reviewers who were unresponsive or minimally responsive in the ﬁrst phase, who can no longer review due to problems in their personal lives, who discovered conﬂicts they had with a paper assigned to them and recused themselves from it, etc.
In all of these cases, the set of papers that will require additional review is unknown beforehand. While some venues choose to recruit new reviewers after knowing which papers proceed to phase two, the tight timeline of many conferences makes it hard to recruit new reviewers after phase one. In SIGMOD 2019 [4]: “Additional reviews were solicited manually by the chairs and this was a huge time sink, especially when some reviewers refused to take on the additional assignment. The additional review solicitation needs to be automated and reviewer expectations need to be set appropriately beforehand.” For this reason, it is best if all the reviewers are recruited at the beginning, and a key question is then how to assign reviewers to papers in the ﬁrst phase such that enough review capacity is saved for the second phase.
Motivation 2: Conference experiment design. Reviewers also need to be split into two groups when conferences run controlled experiments on the paper review process. Conferences often run such experiments to test changes to the review process. For example, the WSDM 2017 conference conducted an experiment to test the eﬀects of single-blind versus double-blind reviewing [5]. As another example, the aforementioned NeurIPS 2014 experiment tested the consistency of acceptance decisions by providing some papers with a second set of reviews from a separate group of reviewers. In these experiments, all papers receive reviews conducted in the usual manner (the control condition), but a random subset of papers are additionally assigned reviewers who provide reviews under an experimental condition. In the NeurIPS 2014 experiment, a random 10% of papers were put in the experimental condition and received a second set of reviews. In the WSDM 2017 experiment, the subset of papers was the full paper set; that is, all papers were reviewed under both single-blind and double-blind conditions. The key question is then how to divide the reviewers between the control and experimental conditions. As in the NeurIPS 2014 and WSDM 2017 experiments, this is often done randomly for statistical purposes. However, conferences still want to ensure that the resulting assignment of papers to reviewers is of high similarity.
As our results apply to both the two-phase and experiment design settings, we will use the generic terminology of “stages” to refer to both phases and conditions simultaneously across the two settings.
Problem outline. In this paper, we formally analyze the two-stage paper assignment problem, which encompasses both above motivations. As stated earlier, the standard paper assignment problem is to maximize the total similarity of the assignment subject to load constraints and is eﬃciently solvable. However, in the two-stage paper assignment problem, we must additionally decide how much of each reviewer’s capacity should be saved to review papers in the second stage. We assume that the fraction of papers that will need additional reviews is known and that the set of second-stage papers is chosen uniformly at random.
Because of constraints present in each setting, the maximum-similarity paper assignment across the two stages cannot be achieved. In the two-phase setting, the set of papers that will need to be reviewed in the second stage is unobserved when the ﬁrst-stage assignment is made, making the problem one of stochastic optimization. In the experiment design setting, reviewers are often randomized between stages for statistical purposes. We show that a simple strategy for choosing reviewers to save for the second stage performs near-optimally in terms of assignment similarity and can be used in either setting.
Contributions. Our contributions are as follows. First, we identify and formulate the two-stage paper assignment problem, an issue of practical importance to modern conferences, with applications to two-phase paper assignment and conference experiment design (Section 3). Second, we prove that a simpliﬁed version of the problem is NP-hard, suggesting that the problem may not be eﬃciently solvable (Section 4). Third, we empirically show that a very simple “random split” strategy, which chooses a subset of reviewers uniformly at random to save for the second stage, gives near-optimal assignments on real conference similarity scores (Section 5.1). This result is summarized in Figure 1, which shows the assignment similarity achieved using random split as compared to the oracle optimal assignment (which views the set of second-stage papers
2

Fraction of optimal similarity Fraction of optimal similarity

1.0

1.0

0.8

0.8

0.6

0.6

0.4

0.4

0.2

0.2

0.0 ICLR ICLR PrefLib3 PrefLib3 Bid1 Bid1 SIGIR SIGIR =0.1 =0.5 =0.1 =0.5 =0.1 =0.5 =0.1 =0.5
(a) Second-stage papers drawn uniformly at random

0.0 Top =0.1

T=o0p.5

M=id0d.l1e

M=id0d.l5e

(b) Second-stage papers chosen as the top- or middle-scoring papers from ICLR

Figure 1: Range of assignment similarities found over 10 random reviewer splits on real conference data, as a fraction of the oracle optimal assignment’s similarity (computed after observing the second-stage papers). β indicates the fraction of papers in the second stage. The ICLR similarities [6] (911 papers, 2435 reviewers) are constructed from text-matching between papers and reviewers’ past work, PrefLib3 [7] (176 papers, 146 reviewers) and Bid1 [8] (600 papers, 400 reviewers) similarities are constructed from bidding data, and SIGIR [9] similarities (73 papers, 189 reviewers) are constructed from reviewer and paper subject areas.

before optimally assigning reviewers across both stages) for several datasets. We ﬁnd that all random reviewer splits achieve at least 90% of the oracle optimal solution’s similarity on all datasets and at least 94% on all but two experiments. These results hold across similarities constructed via a variety of methods used in practice (including text-matching, bidding, and subject areas), indicating that random split is robust across methods of similarity construction. They also hold both when the second-stage papers are drawn uniformly at random (as in Figure 1a) and when they are selected based on the review scores of the papers (as in Figure 1b). In practice, this means that program chairs planning a two-phase review process or a conference experiment can simply split reviewers across the two phases/conditions at random without concerning themselves with the potential reduction in assignment quality.
We also show that this good performance is not achieved in general: there exist similarity matrices on which random split performs very poorly (Section 5.2).
Fourth, we theoretically explain why random split performs well on our real conference similarity matrices by deriving theoretical bounds on the suboptimality of this random strategy under certain natural conditions (Sections 6 and 7). We consider two such suﬃcient conditions here, which are met by our datasets: if the reviewer-paper similarity matrix is low-rank, and if the similarity matrix allows for a high-value assignment (in terms of total similarity) with a large number of reviewers assigned to each paper. From these results, we give key actionable insights to conference program chairs to help them decide–well before the reviewers and/or papers are known–if random split is likely to perform well in their conference.
All of the code for our empirical results is freely available online.1
2 Related Work
Our work assumes that the “similarities” between reviewers and authors are given. In practice, there are several ways in which these similarities are computed, and diﬀerent program chairs often make diﬀerent decisions on how this computation is done. The similarities are generally computed using one or more of the following three sources of data:
• Text-matching of papers: Natural language processing techniques [10–14] are used to match the text of the submitted paper with the text of the reviewers’ past papers.
1https://github.com/sjecmen/multistage_reviewing_bounds

3

• Subject areas: The program chairs create a list of subject areas relevant to the conference. Each reviewer selects a subset of these subject areas that are representative of their expertise, and each submitted paper is accompanied by the authors selecting the subject areas relevant to the paper.
• Bids: A number of conferences adopt a bidding system, where reviewers are shown a list of (some of) the papers that are submitted to the conference (and which do not conﬂict with them) and asked to indicate the papers which they are willing to review [8, 15, 16].
If more than one such source of data is used by the conference, they are combined in a manner deemed suitable by the program chairs [3]. These computed similarities are then used to assign reviewers to papers. By far the most popular method of doing this assignment is to solve an optimization problem that maximizes the sum of the similarities of the assigned reviewer-paper pairs, subject to constraints on the reviewer and paper loads [14, 17–21]. Given its widespread popularity, we analyze this sum-similarity objective in our paper.
That being said, there are other objectives that are also proposed for automated assignment using the similarities, such as the max-min fairness objective [22–24]. A recent work [25] proposes assignments via optimizing the sum similarity but with some randomness in order to prevent fraud in peer review. Another line of work [6, 26, 27] proposes assigning reviewers to papers in a manner that a reviewer cannot inﬂuence the outcome of their own paper by manipulating the reviews they provide. Finally, in practice, the conference organizers may also additionally apply manual tweaks to the outputs of any such automated procedure.
At a high level, the problem we study in the two-phase setting shares several common characteristics with problems in online (stochastic) matching [28–32], often considered in the context of ride-sharing, kidney exchange, or internet advertising. Particularly related to our results is the line of research on two-stage stochastic matching [33–37], which generally focuses on providing algorithms with tight approximation ratios that hold for any (i.e., worst-case) problem instances. To the best of our knowledge, the speciﬁc stochastic matching problem we consider (which arises in the context of paper assignment for peer review) has not previously been studied. Additionally, in contrast to this line of work, we aim to provide and justify simple and practical solutions (such as choosing reviewers uniformly at random) based on data-dependent conditions likely to hold in real-world paper assignment instances.
The simpliﬁed version of our problem considered in Section 4 can be seen as an instance of maximizing a submodular function subject to a cardinality constraint (see Appendix C). The paper [38] gives an approximation algorithm achieving an approximation ratio of no greater than 0.5. However, this guarantee is very weak in the paper assignment setting since it can be trivially achieved by maximizing similarity in the ﬁrst stage alone.
One motivation for our work is that of running controlled experiments in peer review. Controlled experiments pertaining to peer review are conducted in many diﬀerent scientiﬁc ﬁelds [39–43], including several controlled experiments recently conducted in computer science [2, 5, 44, 45]. These experiments have also led to a relatively nascent line of work on careful design of experimental methods for peer review [46, 47], and our work sheds some light in this direction in terms of trading oﬀ assignment quality with randomization in the assignment. Some other experiments in conferences [48–50] do not operate under controlled settings, but exploit certain changes in the conference policy such as a switch from single blind to double blind reviewing (i.e., natural experiments). Overall, experiments oﬀer important insights into the peer review process; see [51] for more discussion on challenges in peer review and some solutions.
3 Problem Formulation
In this section, we formally deﬁne the two-stage paper assignment problem. Given a set of n papers P = [n] and a set of m reviewers R = [m], deﬁne S ∈ [0, 1]m×n as the similarity scores between each reviewer and paper. An assignment of papers to reviewers is represented as a matrix A ∈ {0, 1}m×n, where Arp = 1 if reviewer r is assigned to paper p and Arp = 0 otherwise. In the standard paper assignment problem, the objective is to ﬁnd an assignment A of reviewers to papers such that the total similarity r∈R,p∈P ArpSrp is maximized, subject to constraints that each paper is assigned exactly a certain load of reviewers, each reviewer is assigned to at most a certain load of papers, and any reviewer-paper pairs with a conﬂict of interest are not assigned [14, 18, 20, 24, 52]. In this work, we accommodate conﬂicts of interest by assuming
4

the corresponding similarities are set to 0. This problem can be formulated as a min-cost ﬂow problem or as
a linear program, and can be eﬃciently solved.
In a two-stage assignment, all papers P require a certain number of reviewers in the ﬁrst stage and a
subset of papers P2 ⊆ P require additional review in the second stage. We assume that P2 consists of a ﬁxed fraction β of papers and is drawn uniformly at random from P. Speciﬁcally, for some β ∈ { n1 , . . . , nn }, we assume that P2 ∼ Uβn(P), the uniform distribution over all subsets of size βn of P. In the two-phase setting, the fraction β itself can be viewed as a parameter that the program chairs set based on available
reviewer resources, or it can be estimated from past editions of the conference. Our empirical results detailed
in Section 5.1 also cover the case where papers are chosen for the second phase based on their ﬁrst-phase
review scores. In the conference experiment design setting, the value of β and the uniform distribution of
P2 are both experiment design choices. The choice of a uniform distribution for P2 is common, as in the NeurIPS 2014 and WSDM 2017 experiments. The question we analyze is: how should reviewers be assigned
to papers across the two stages?
Before continuing further, we introduce some notation. For subsets R ⊆ R and P ⊆ P, desired paper load pap ∈ Z+, and maximum reviewer load rev ∈ Z+, deﬁne M(R , P ; rev, pap) ⊆ {0, 1}m×n as the set of valid assignment matrices on R and P . Formally, A ∈ M(R , P ; rev, pap) if and only if r∈R Arp = pap for all p ∈ P , p∈P Arp ≤ rev for all r ∈ R , and Arp = 0 for all (r, p) ∈ R × P .
The two-stage paper assignment problem is to maximize the total similarity of the paper assignment across
both stages. Without loss of generality, we instead consider the mean similarity so that later results will be easier to interpret. Fix a stage one paper load (p1a)p, a stage two paper load (p2a)p, and an overall reviewer load rev such that (p1a)pn + (p2a)pβn ≤ revm (i.e., the number of reviews required by papers is no greater than the number of reviews that can be supplied by reviewers). Given P2, the oracle optimal assignment has mean similarity

Q∗(P2) =

max

A∈M(R,P; rev , (p1a)p),

B∈M(R,P2; rev , (p2a)p)

1 (p1a)pn +





(2) 

ArpSrp +

BrpSrp

papβn r∈R,p∈P

r∈R,p∈P2

subject to

Arp + Brp ≤ rev

p∈P

∀r ∈ R.

The last constraint ensures that each reviewer’s assignment across both stages does not exceed the maximum reviewer load. Just like the standard paper assignment problem, the oracle optimal assignment for a given P2 can be found eﬃciently. However, in both the two-phase and experiment design settings, this oracle optimal assignment is either unachievable or undesirable. In the two-phase setting, the set of papers P2 requiring additional review is unknown until after the stage one assignment is chosen. Thus, the oracle optimal assignment cannot be computed beforehand. In the experiment design setting, the assignment of reviewers to conditions is commonly randomized in order to gain statistical power, as was done in the WSDM 2017 and NeurIPS 2014 experiments. Thus, a deterministic choice of assignment may not be desirable. Additionally, depending on the experiment setup, it may not be possible for a reviewer to review papers in both conditions. In what follows, we use this oracle optimal assignment value as an unachievable baseline for comparison.
We instead consider simple strategies for the two-stage assignment problem that choose a subset R2 ⊆ R of reviewers to save for the second stage without observing P2, leaving reviewers R1 = R \ R2 to be assigned to papers in the ﬁrst stage. Unlike the oracle optimal assignment, such strategies are feasible to implement in both settings since they do not require knowledge of P2, do not split reviewer loads across conditions, and allow for a random choice of R2. The mean similarity of the optimal assignment when reviewers R2 and papers P2 are in the second stage is





Q(R2, P2) =

1 (p1a)pn +

(2)  max

ArpSrp + max

BrpSrp . 

βn A∈M(R\(1R) 2,P; r∈R\R2,p∈P

B ∈M(R2 ,P2 ; (2) r∈R2,p∈P2

pap

rev , pap)

rev , pap)

We require that rev|R2| ≥ (p2a)pβn and rev(m − |R2|) ≥ (p1a)pn for feasibility in both stages. Given R1, R2, and P2, the optimal paper assignment in each stage can be eﬃciently computed using standard methods. Thus, the the diﬃculty of the problem lies entirely in choosing R2.

5

The expected mean similarity of the optimal assignment when saving reviewers R2 for the second stage is

f (R2) = EP2∼Uβn(P) [Q(R2, P2)] .

We can also evaluate the suboptimality of R2 as compared to the oracle optimal assignment as

Q∗(P2) − Q(R2, P2),

where P2 ∼ Uβn(P).

Note that Q∗ and Q are bounded in [0, 1], so that both f and the suboptimality are also bounded in [0, 1].

In our theoretical analysis, for simplicity, we assume that

rev =

(1) pap

=

(2) pap

=

1,

leaving

this

implicit

in

f , Q, and Q∗ throughout the paper. We also assume m = (1 + β)n in our analysis unless speciﬁed otherwise,

so that |R2| = βn. The intuition behind our results carries over to the cases of general loads and excess

reviewers, which are covered by our empirical results in Section 5.1. All asymptotic bounds are given as n

grows.

4 Hardness

In the two-phase setting, the oracle optimal assignment is unachievable because R2 must be chosen before

observing P2. Therefore, conferences must choose R2 to maximize f , the expected mean similarity of the

assignment across both stages. In this section, we demonstrate that maximizing a variant of f is NP-hard,

indicating that it is unlikely that f can be optimized eﬃciently.

First, note that evaluating f (R2) requires computing an expectation over the draw of P2, which naively

requires evaluating a sum over the optimal assignment value for

n βn

possible choices of P2. This number

is exponential in the input size, so an eﬃcient algorithm for this problem would have to either optimize

f without evaluating it or compute this expectation without computing the optimal assignment for each

possible P2.

Instead of attempting to optimize f exactly, a standard approach from two-stage stochastic optimization is

to simplify the problem by sampling as follows [53,54]. First, take some ﬁxed number of samples P2(1), . . . , P2(K)

from Uβn(P). Then, rather than optimizing an average over all P2 in the support of Uβn(P), choose R2 to

optimize an average over only all sampled sets:

1K

(i)

f (R2) = K Q(R2, P2 ).

i=1

This is a natural simpliﬁcation of the two-stage paper assignment problem, because the sum in the objective is now taken over only a constant K subsets rather than an exponential number. However, this problem is still not eﬃciently solvable, as the following theorem shows.

Theorem 1. It is NP-hard to ﬁnd R2 ⊆ R such that f (R2) is maximized, even when K = 3.

Proof sketch. We reduce from 3-Dimensional Matching [55], which asks if there exists a way to select k tuples from a set T ⊆ X × Y × Z where |X| = |Y | = |Z| = k such that all elements of X, Y , and Z are selected exactly once. We construct 3 samples of second-stage papers corresponding to X, Y , and Z respectively, and construct reviewers corresponding to elements of T . These reviewers have 1 similarity with the papers in their tuple, and 0 similarity with all other papers. Thus, checking if there exists a choice of R2 which gives full expected similarity in the second stage would require solving 3-Dimensional Matching. We add additional reviewers and papers to ensure that this choice of R2 is optimal over both stages.

The full proof is presented in Appendix D.1. Since it is NP-hard to ﬁnd the optimal R2 even when estimating the objective by sampling three random choices of P2, this suggests that the original objective f may be hard to optimize eﬃciently. Therefore, in the two-phase assignment setting, we instead look for eﬃcient approximation algorithms.

6

5 Our Approach: Random Split

Our proposed approach for ﬁnding a two-stage assignment is extremely simple: choose R2 uniformly at random (i.e., R2 ∼ Uβn(R)). We refer to this as a “random split” of reviewers into the two review stages.
In the two-phase setting, random split is an eﬃcient approximation algorithm for the problem of optimizing f , which is likely diﬃcult (as shown in Section 4). Because random split does not execute f , it produces a two-stage paper assignment without needing to estimate f by sampling.
In the conference experiment design setting, our proposed random-split strategy corresponds to a uniform random choice of reviewers for the experimental condition. Recall that in this setting, assigning reviewers to conditions uniformly at random is already a common experimental setup. The performance of random split on f therefore indicates how well this common setup performs in terms of the expected assignment similarity.
In our theoretical results, we often refer to the suboptimality of random split, deﬁned as the suboptimality of R2 chosen via random split when P2 is chosen uniformly at random:

Q∗(P2) − Q(R2, P2),

where P2 ∼ Uβn(P), R2 ∼ Uβn(R).

(1)

Recall from Section 3 that Q∗(P2) is the mean similarity of the oracle optimal assignment given second-stage papers P2 and that Q(R2, P2) is the mean similarity of the optimal assignment given second-stage reviewers and papers R2, P2. Additionally, many of our results evaluate the expected mean similarity under random split:

ER2∼Uβn(R) [f (R2)] = ER2∼Uβn(R),P2∼Uβn(P) [Q(R2, P2)] .
In the following subsections, we ﬁrst elaborate on the good performance random split displays empirically before showing that there exist cases where random split performs very poorly.

5.1 Empirical Performance
As introduced earlier in Figure 1, random split performs very well in practice on four real conference similarity matrices. The ﬁrst is a similarity matrix recreated using text-matching on ICLR 2018 data [6]. The second is constructed using reviewer bid data for an AI conference (conference 3) from PrefLib dataset MD-00002 [7]. The third (denoted Bid1) is a sample of the bidding data from a major computer science conference [8]. In both of these bidding datasets, we transformed “yes,” “maybe,” and “no response” bids into similarities of 1, 0.5, and 0.25 respectively, as is often done in practice [3]. The fourth similarity matrix is constructed from the subject areas of ACM SIGIR 2007 papers and the subject areas of the past work of their authors (assumed to be the reviewers) [9]; we set the similarity between each reviewer and paper to be equal to the number of matching subject areas out of the 25 total, normalized so that each entry of the matrix is in [0, 1]. In Appendix A, we present further empirical results including additional datasets. In Appendix B, we present additional empirical results particularly relevant to the conference experiment design setting.
We run several experiments, each corresponding to a choice of dataset and β. Each experiment consists of 10 trials, where in each trial we sample a random reviewer split and a set of second-stage papers. We then present the range of assignment values achieved across the trials as percentages of the oracle optimal assignments for each trial. The oracle optimal assignment for a trial is found by choosing the optimal assignment of reviewers across both stages after observing P2. We set paper loads of 2 in each stage (as done in AAAI 2021), and limit reviewer loads to be at most 6 (a realistic reviewer load [3]). Since these datasets have excess reviewers, we choose R2 to have size 1+ββ m so that the proportions of reviewers and papers in the second stage are equal.
In Figure 1a, P2 is drawn uniformly at random in each trial (as in the problem formulation). We see that all trials of random split achieve at least 90% of the oracle optimal solution’s similarity on all datasets, with all trials on all but two experiments achieving at least 94%. We see additionally that the randomness of the reviewer choice does not cause much variance in the value of the assignment, as there is at most a 5% diﬀerence between the minimum and maximum similarity (as a percentage of oracle optimal) for each experiment. Note that this is true despite the fact that the similarity matrices of the diﬀerent datasets are constructed in several diﬀerent ways, indicating that random split is robust across methods of similarity construction.

7

In Figure 1b, P2 is chosen as a ﬁxed set for all trials based on the actual review scores received by the papers at ICLR 2018 [56] (as review scores were not available for other datasets). We run trials where either the top-scoring papers or the messy-middle papers are given additional reviews. Since about 37% of papers were accepted, we deﬁne the messy middle as the range of 1+ββ m papers centered on the 63rd-percentile paper when ordered by score. These are sets of papers that a conference may potentially want to assign additional reviewers to. In all cases, random split shows consistently good performance, similar to when P2 was drawn uniformly at random. All trials achieve at least 95% of the oracle optimal similarity, with at most a 2% diﬀerence between the minimum and maximum for each experiment. This suggests that the good performance of random reviewer split naturally holds in these practical cases.
5.2 A Counterexample
The good results random split shows in practice may be somewhat surprising because random split does not perform well in all settings. The following theorem shows that for any β, there exist instances of the two-stage paper assignment problem where the suboptimality of random split (1) is Ω(1) in expectation.
Theorem 2. For any constant β ∈ [0, 1], there exists n0 such that for all n ≥ n0 where βn ∈ Z+, there exist instances of the two-stage paper assignment problem where the suboptimality of random split is at least (1+β4β)3 in expectation.
Proof sketch. Consider β = 1. We construct a similarity matrix where every reviewer has similarity 1 with only 1 paper, and all papers have similarity 1 with only 2 reviewers. The optimal reviewer split puts the two good reviewers for each paper in separate stages and always achieves a mean similarity of 1. Random split puts both good reviewers in the same stage with at least constant probability for each paper, giving a constant mean similarity < 1.
The full proof is presented in Appendix D.2. Note that the above lower bound on the objective value of random split holds even in the easy case of β = 1, where the problem could be solved simply through standard paper assignment methods. This case is particularly relevant in the conference experiment setting, where all papers are commonly reviewed under both conditions (as in the WSDM 2017 experiment). Although the above lower bound demonstrates that random split cannot hope to do well in general, the constructed example is unrealistic for real conferences. However, program chairs may understandably want some guarantee that a random reviewer split will work well for their conference before deciding to use it. Ideally, this guarantee should be given before the precise similarity matrix for the conference is known, since the similarities may not be known in full until late in the planning process. In the following sections, we provide such guarantees, thereby showing that the good performance of random split is not just an artifact of our speciﬁc datasets. We focus our attention on two suﬃcient conditions on the similarity matrix under which we show random split performs well. These conditions are natural for real similarity matrices, implying that random split will perform well for many real conferences, whether in the context of a two-phase review process or a conference experiment. Using these conditions, we provide actionable insights to program chairs based on simple properties of their conference’s similarities that they may have intuition about. These insights are designed to be useful well before the full paper and reviewer sets are known.
6 Condition 1: Low-Rank Similarity Matrix
The ﬁrst condition we consider is that the similarity matrix S has low rank k. This condition naturally arises in practice when reviewer-paper similarities are calculated from the number of subject area agreements between reviewers and papers; in such cases, the rank is no greater than the number of subject areas. For example, the SIGIR similarity matrix used in Figure 1 is constructed in this way and thus has rank no greater than 25 (the number of subject areas). In this section, we provide asymptotic upper and lower bounds on the suboptimality of random split for constant-rank similarity matrices.
8

6.1 Theoretical Bounds

We ﬁrst provide an upper bound on the suboptimality of random split (1). This shows that random reviewer

splits perform well on constant-rank similarity matrices, including the SIGIR similarity matrix examined

earlier. More precisely, the following theorem shows that if the similarity matrix S has constant rank k, the

suboptimality

of

random

split

is

at

most

O

(

n

−

1 2

)

when

k

=

1,

O

(

n

−

1 2

+o(1)

)

when

k

=

2,

and

O

(

n−

1 k

+o

(1)

)

when k ≥ 3 with high probability.

Theorem 3. Consider any constants β ∈ [0, 1] and k ∈ Z+. There exists n0 and constants C, η such that, for any n ≥ n0 where βn ∈ Z+ and for any similarity matrix S ∈ [0, 1](1+β)n×n of rank k, the suboptimality
of random split is at most:

•

C

(log

n)η

n−

1 2

if

k=1

•

C

(log

n)η

n−

1 2

+

log

1 log

n

if k = 2

•

C

(log

n)η

n−

1 k

+

log

1 log

n

if k ≥ 3

with probability at least 1 − n1 (where log indicates the base-2 logarithm).
Proof sketch. By Lemma 4 of [57], a rank k similarity matrix S ∈ [0, 1]m×n can be factored into vectors ur ∈ Rk for each r ∈ R and vp ∈ Rk for each p ∈ P such that Srp = ur, vp , ||ur||2 ≤ k1/4, and ||vp||2 ≤ k1/4. We cover the k-dimensional ball containing all paper vectors with smaller cells, and consider a reviewer to be in one of these cells if the oracle optimal assignment (given P2) assigns it to a paper in that cell. Using a concentration inequality on the number of reviewers and papers in each cell in each stage, we can upper bound the number of reviewers that we cannot match to papers within the same cell. We then increase the size of the cells and attempt to match the remaining reviewers in this way, continuing until all reviewers are matched. We upper bound the suboptimality of the resulting assignment by the L2 distance between a reviewer’s assigned paper and the paper they are assigned by the oracle optimal assignment.

The constants C and η may depend on k, which is itself assumed to be constant. The full proof is presented in Appendix D.3.
For constant-rank similarity matrices, the suboptimality diminishes as n grows, unlike when the rank of the similarity matrix is unrestricted. Conceptually, our proof technique of ﬁnding a minimum-distance matching between two samples of points resembles the optimal transport problem solved when ﬁnding the Wasserstein distance between a probability distribution and its empirical measure. Thus, our upper bounds nearly match those found in the literature on the expected empirical 1-Wasserstein distance for continuous measures (see [58] and references therein).
We now complement the above upper bound with lower bounds on the suboptimality of random split (1) for constant rank similarity matrices. The following theorem shows that, for similarity matrices of constant rank k, the suboptimality of random split is Ω(n−1/2) in expectation and Ω(n−2/k) with high probability.

Theorem 4. Suppose β = 1. For any constant k ∈ Z+, there exists n0 and constants C, ζ such that for all n ≥ n0:
(a) There exist instances of the two-stage paper assignment problem with similarity matrices S ∈ [0, 1]2n×n of rank k such that the suboptimality of random split is at least Cn−1/2 in expectation.

(b) There exist instances of the two-stage paper assignment problem with similarity matrices S ∈ [0, 1]2n×n of rank k such that the suboptimality of random split is at least Cn−2/k with probability 1 − ζe−n/10.

Proof sketch. (a) We construct k groups of reviewers and papers, where all reviewers and papers in the same

group have similarity 1 with each other and similarity 0 with all other reviewers/papers. The ﬁrst group

contains

n 2

papers

and

n

reviewers.

The

optimal

reviewer

split

puts

half

of

each

group’s

reviewers

in

each

stage and assigns all rev√iewers to papers with similarity 1. By an anti-concentration inequality, with constant

probability, at least Ω( n) reviewers in the ﬁrst group cannot be assigned to a paper in their group under

random split.

(b) We construct a vector in Rk for each reviewer and each paper and set the similarity between that

reviewer and that paper to be the inner product of their corresponding vectors. We place one paper vector

9

√ and two reviewer vectors at each point in an evenly-spaced grid throughout the cube [0, 1/ k]k. The resulting similarity matrix has rank k. The optimal assignment assigns the two reviewers at each point to the paper at that point. With high probability, random split places Ω(n) pairs of reviewer vectors into the same stage. One of each of these reviewer pairs must be assigned to a paper at a diﬀerent point, which is at least Ω(n−1/k) away in L2 distance. The suboptimality of the resulting assignment can be written in terms of the total squared L2 distance between each reviewer and their assigned paper, giving the stated bound.
The constants C and ζ may depend on k, which is itself assumed to be constant. The full proof is presented in Appendix D.4.
6.2 Interpretation of Results
As discussed earlier in this section, certain methods of constructing similarities (such as counting subject area agreements) may inherently lead to low-rank similarity matrices. If a conference is using such a method, the results in this section provide guarantees to the program chairs that random split will perform well, particularly if the rank of the matrix is low compared to the number of papers and reviewers. Alternatively, program chairs may be able to estimate that their reviewers and papers can be grouped into a small number of communities with little variation within them, in which case the similarity matrix may also be low rank.

7 Condition 2: High-Value, Large-Load Assignment
A natural condition on the similarity matrix to consider is that each paper has a large number µ of reviewers with high similarity for that paper. It turns out that this condition is insuﬃcient for guaranteeing good performance of random split, since the same group of µ reviewers could have high similarity with all papers, thus satisfying this condition without changing the assignment value by much (since we can only assign these reviewers to a few papers). In this section, we consider a condition on the similarity matrix that is similar in spirit: the existence of a high-value assignment (in terms of total similarity) on the full reviewer and paper sets where each paper is assigned a large number (1 + β)µ of reviewers. Our proposed condition handles the issue with the naive “large number of reviewers” condition by requiring that the high-value reviewers for each paper can all be simultaneously assigned.
In the following subsections, we ﬁrst provide theoretical guarantees about the performance of random split under this condition. We then demonstrate that this condition helps to explain the good performance of random split on the real similarity matrices presented earlier.

7.1 Theoretical Bounds
The ﬁrst result of this section gives a lower bound on the expected value of random split in terms of the value of a single, large-load assignment. All results in this section still hold if there are excess reviewers (i.e., if m ≥ (1 + β)n and R2 ∼ U β m(R)).
1+β
Theorem 5. Consider any µ ∈ [10, 000] and β ∈ 1010 , . . . , 110000 such that βµ ∈ Z+. If there exists an assignment A(µ) ∈ M(R, P; µ, (1 + β)µ) with mean similarity s(µ), choosing R2 via random split gives that

ER [f (R2)] ≥ s(µ) 1 −

β

1

2

+ 1−β .

2

2π(1 + β)2µ

1+β

A similar bound holds when βµ is not integral, with some additional small terms due to rounding.
Proof sketch. We construct assignments with paper and reviewer loads of at most µ in stage one and at most βµ in stage two using the reviewer-paper pairs assigned by A(µ). We drop any extra assignments at random so that no reviewers and papers are overloaded, and assume any pairs that must be assigned from outside of A(µ) have similarity 0. From within each of these larger assignments, we can ﬁnd an assignment with paper and reviewer loads of 1 with at least the same mean similarity. The expected mean similarity of these assignments can be written as the expectation of a function of binomial random variables. Approximating

10

these by normal random variables and checking via simulation that this is in fact a lower bound for the stated values of β and µ, we get the stated bound.

The more general version of the bound and the full proof are stated in Appendix D.5. The above bound works well when the reviewer-paper pairs in the large-load assignment are all nearly equally valuable. However, it cannot take advantage of the fact that certain reviewers may be extremely valuable for a certain paper and can be prioritized for assignment to that paper when possible. The next result uses additional information about the value of an assignment with smaller loads, along with a large-load assignment disjoint from the small assignment, to make use of these highly valuable reviewer-paper pairs in the case where β = 1. Recall from Section 5.2 that β = 1 is still not an easy case for random split in general and is particularly relevant for the conference experiment setting.

Theorem 6. Suppose β = 1, and consider any µ ∈ [10, 000] such that

µ 4

∈ Z+.

Suppose there exists

an assignment A(1) ∈ M(R, P; 1, 2) with mean similarity s(1). Suppose there also exists an assignment

A(µ) ∈ M(R, P; µ, 2µ) with mean similarity s(µ) that does not contain any of the pairs assigned in A(1).

Then, choosing R2 via random split gives that

ER [f (R2)] ≥ 3 s(1) +

1.44 1− √

2

4

µ

1 s(µ). 4

A similar bound holds when µ4 is not integral, with some additional small terms due to rounding.

Proof sketch. We ﬁrst attempt to assign as many pairs as possible from within A(1); in expectation we can

assign

3 4

of

them.

Among

the

remaining

reviewers

and

papers,

we

attempt

to

construct

assignments

with

paper and reviewer loads of µ4 in both stages from within the reviewer-paper pairs assigned by A(µ). This is

done in a similar way as in Theorem 5.

The more general version of the bound and the full proof are stated in Appendix D.6. If we consider A(1) as the optimal assignment and assume that µ is divisible by 4, we get an approximation ratio (between the random split assignment and oracle optimal assignment’s similarities) of 34 + γ4µ 1 − 1√.4µ4
where γµ = ss((µ1)) . With µ = 8, we achieve an approximation ratio of at least 34 + γ88 . Additionally, if γµ → 1 as n grows for any µ = ω(1), the suboptimality of random split (1) approaches 0. For example, this means that the suboptimality of random split approaches 0 as n grows if the mean similarity of an assignment with paper loads of log n improves faster than the mean similarity of the optimal assignment.

7.2 Empirical Evaluation
We now show the performance of these bounds on our real conference datasets in order to evaluate the extent to which they explain the good performance of random split. We use three of the conference datasets introduced earlier with β = 1. In Appendix A, we evaluate the bounds on additional datasets (including the SIGIR dataset). On PrefLib3 and Bid1, the problem is infeasible with paper and reviewer load constraints of 1 since m < 2n, so we modify the datasets by splitting each reviewer into 3 copies as follows. For each paper, we arbitrarily give one of the copies the same similarity as the original reviewer and give the other copies similarity 0. In this way, the similarity of the optimal assignment on this modiﬁed dataset is no greater than the similarity of the optimal assignment on the original dataset.
In Figure 2, we vary the value of the parameter µ (indicating the loads of the assignment A(µ)) and show the bounds of Theorem 5 and Theorem 6 as compared to the estimated expected value of random split. The estimated expected value is averaged over 10 trials with the standard error of the mean shaded, although it is sometimes not visible because it is small. We see that on these datasets, the bound of Theorem 5 performs best for low values of µ and not very well for higher values, likely due to the presence of a few “star” reviewers for each paper which hold a lot of the value. By making use of extra information about the values of these reviewers, the bound of Theorem 6 achieves a high fraction of the actual random split value. Although this bound is maximized at large values of µ on these datasets, it is close to its maximum even with reasonably low values of µ. For example, on ICLR, the lower bound achieves 86% of the estimated expected value of random split with µ = 8. This indicates the good performance of random split is explained well by the presence of just a few good reviewers per paper that can be simultaneously assigned.

11

Average assignment value Average assignment value Average assignment value

Random split expectation

0.100.04

0.8

0.08

0.6

0.060.02 0.4

0.04

0.020.00

0.2

0.00 0.02

Assi5gnment l1o0ad param15eter

20

0.0

Theorem 5 bound Assi5gnment l1o0ad param15eter 20

Theorem 6 bound

0.6

0.4

0.2

0.0

5 10 15 20

Assignment load parameter

0.04 (a) ICLR

(b) PrefLib3

(c) Bid1

Figure 2: Performance of the “high-value large-load” bounds on real conference datasets, β = 1. On the x-axis we vary the pa0rameter µ, whic2h determines th4e loads of the a6ssignment A(µ)8used in the bound. The best setting of µ for each bound is circled.

7.3 Interpretation of Results
Although our results in this section are stated in terms of the precise values of high-load assignments, they can be interpreted by program chairs in a simple and practical way. Roughly, our results indicate that if several good reviewers can be simultaneously assigned to each paper (as was the case for the three conference similarity matrices in Figure 2), random split will perform well. When considering the potential performance of randomly splitting reviewers, program chairs should consider the reviewer and paper pools they expect to have at their conference and make a judgement about how many good-quality reviewers they think could be assigned to each paper (if the reviewer loads are scaled up proportionately). For example, the program chairs of a large AI conference might be conﬁdent that the top several reviewers for each paper are about equally valuable (due to the depth of the reviewer pool) and could be assigned to each paper with only a modest loss in average review quality; this would imply that random split would perform very well for this conference.
8 Conclusion
We showed that randomly splitting reviewers between two reviewing phases or two reviewing conditions produces near-optimal assignments on realistic conference similarity matrices. Our analysis of this phenomenon can help future program chairs make decisions about whether random split will work well for their conference’s two-phase review process, based on their assessment of whether a few simple conditions are applicable to their case. In the setting of conference experiment design, our analysis allows program chairs to understand if running an experiment on their review process will signiﬁcantly impact their assignment quality.
In addition, our results can potentially be further generalized to related reviewing models such as those of academic journals (which accept submissions on a rolling basis), or to other multi-stage resource allocation problems that involve matching resources based on similarities. For example, datacenters receiving a large batch of jobs may have to select some to run on various servers immediately and some to run later when additional servers have been freed, or hospitals may want to assign nurses to shifts based on expertise but without knowledge of which expertise will be most applicable in later shifts.
One limitation of our work is that while our empirical results demonstrate the eﬀectiveness of the randomsplit strategy with real conference data, our theoretical results make the simplifying assumption that paper and reviewer loads are 1, which is unrealistic for real conferences. However, we believe that incorporating this detail would not change our explanations for the good performance of random split. Another limitation is that we assume the set of papers requiring reviews in the second stage is drawn uniformly at random. Although this is a reasonable belief without further information in the two-phase setting, one direction for future work is to consider non-uniform distributions of second-stage papers and analyze if a form of random split still performs well there.
Our work could potentially produce negative outcomes in the form of worse paper assignments if program

12

chairs decide to use random split on an incorrect belief that their conference will ﬁt our conditions. However, program chairs are required to make such decisions about how to perform the paper assignment anyway, so this is not a signiﬁcant increase in risk. The use of random reviewer splits, as opposed to some alternate strategy where reviewers can self-select their stage, could also negatively impact reviewers with strong preferences over which stage they review in (e.g., due to schedule constraints). These preferences should ideally be taken into account along with the similarity of the resulting assignment when choosing the reviewer split; we leave this as an interesting direction for future work.
Acknowledgments
This work was supported by NSF CAREER awards 1942124 and 2046640, NSF grant CIF-1763734, NSF grants IIS-1850477 and IIS-1814056, and a Google Research Scholar Award.
References
[1] Eric Price. The NIPS experiment. http://blog.mrtz.org/2014/12/15/the-nips-experiment.html, 2014 (accessed May 17, 2021).
[2] Neil D. Lawrence. The NIPS experiment. https://inverseprobability.com/2014/12/16/ the-nips-experiment, 2014 (accessed May 17, 2021).
[3] Nihar B. Shah, Behzad Tabibian, Krikamol Muandet, Isabelle Guyon, and Ulrike Von Luxburg. Design and analysis of the NIPS 2016 review process. The Journal of Machine Learning Research, 19(1):1913– 1946, 2018.
[4] Anastasia Ailamaki, Periklis Chrysogelos, Amol Deshpande, and Tim Kraska. The SIGMOD 2019 research track reviewing system. ACM SIGMOD Record, 48(2):47–54, 2019.
[5] Andrew Tomkins, Min Zhang, and William D. Heavlin. Reviewer bias in single- versus double-blind peer review. Proceedings of the National Academy of Sciences, 114(48):12708–12713, 2017.
[6] Yichong Xu, Han Zhao, Xiaofei Shi, Jeremy Zhang, and Nihar B. Shah. On strategyproof conference peer review. arXiv preprint arXiv:1806.06266, 2018.
[7] Nicholas Mattei and Toby Walsh. PrefLib: A library of preference data http://preflib.org. In 3rd International Conference on Algorithmic Decision Theory, Lecture Notes in Artiﬁcial Intelligence. Springer, 2013.
[8] Reshef Meir, Jérôme Lang, Julien Lesca, Natan Kaminsky, and Nicholas Mattei. A market-inspired bidding scheme for peer review paper assignment. In Games, Agents, and Incentives Workshop at AAMAS, 2020.
[9] Maryam Karimzadehgan, ChengXiang Zhai, and Geneva Belford. Multi-aspect expertise matching for review assignment. In 17th ACM Conference on Information and Knowledge Management, pages 1113–1122, 2008.
[10] David Mimno and Andrew McCallum. Expertise modeling for matching papers with reviewers. In 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’07, pages 500–509, New York, NY, USA, 2007. ACM.
[11] Xiang Liu, Torsten Suel, and Nasir Memon. A robust model for paper reviewer assignment. In 8th ACM Conference on Recommender Systems, RecSys ’14, pages 25–32, New York, NY, USA, 2014. ACM.
[12] Marko A. Rodriguez and Johan Bollen. An algorithm to determine peer-reviewers. In 17th ACM Conference on Information and Knowledge Management, CIKM ’08, pages 319–328, New York, NY, USA, 2008. ACM.
13

[13] Hong Diep Tran, Guillaume Cabanac, and Gilles Hubert. Expert suggestion for conference program committees. In 11th International Conference on Research Challenges in Information Science, pages 221–232, May 2017.
[14] Laurent Charlin and Richard S. Zemel. The Toronto Paper Matching System: An automated paperreviewer assignment system. In ICML Workshop on Peer Reviewing and Publishing Models, 2013.
[15] Guillaume Cabanac and Thomas Preuss. Capitalizing on order eﬀects in the bids of peer-reviewed conferences to secure reviews by expert referees. Journal of the Association for Information Science and Technology, 64(2):405–415, 2013.
[16] Tanner Fiez, Nihar Shah, and Lillian Ratliﬀ. A SUPER* algorithm to optimize paper bidding in peer review. In 36th Conference on Uncertainty in Artiﬁcial Intelligence, 2020.
[17] Cheng Long, Raymond Wong, Yu Peng, and Liangliang Ye. On good and fair paper-reviewer assignment. In IEEE International Conference on Data Mining, pages 1145–1150, 12 2013.
[18] Judy Goldsmith and Robert H. Sloan. The AI conference paper assignment problem. AAAI Workshop, WS-07-10:53–57, 12 2007.
[19] Wenbin Tang, Jie Tang, and Chenhao Tan. Expertise matching via constraint-based optimization. In IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology, WI-IAT ’10, pages 34–41, Washington, DC, USA, 2010. IEEE Computer Society.
[20] Peter A. Flach, Sebastian Spiegler, Bruno Golénia, Simon Price, John Guiver, Ralf Herbrich, Thore Graepel, and Mohammed J. Zaki. Novel tools to streamline the conference review process: Experiences from SIGKDD’09. SIGKDD Explorations Newsletter, 11(2):63–67, May 2010.
[21] Camillo J. Taylor. On the optimal assignment of conference papers to reviewers. University of Pennsylvania Department of Computer and Information Science Technical Report, 2008.
[22] Naveen Garg, Telikepalli Kavitha, Amit Kumar, Kurt Mehlhorn, and Julián Mestre. Assigning papers to referees. Algorithmica, 58(1):119–136, Sep 2010.
[23] Ivan Stelmakh, Nihar Shah, and Aarti Singh. PeerReview4All: Fair and accurate reviewer assignment in peer review. In Conference on Algorithmic Learning Theory, 2019.
[24] Ari Kobren, Barna Saha, and Andrew McCallum. Paper matching with local fairness constraints. In ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2019.
[25] Steven Jecmen, Hanrui Zhang, Ryan Liu, Nihar B. Shah, Vincent Conitzer, and Fei Fang. Mitigating manipulation in peer review via randomized reviewer assignments. In Advances in Neural Information Processing Systems, 2020.
[26] Noga Alon, Felix Fischer, Ariel Procaccia, and Moshe Tennenholtz. Sum of us: Strategyproof selection from the selectors. In Conference on Theoretical Aspects of Rationality and Knowledge, pages 101–110, 2011.
[27] Haris Aziz, Omer Lev, Nicholas Mattei, Jeﬀrey S. Rosenschein, and Toby Walsh. Strategyproof peer selection using randomization, partitioning, and apportionment. Artiﬁcial Intelligence, 275:295–309, 2019.
[28] Richard M. Karp, Umesh V. Vazirani, and Vijay V. Vazirani. An optimal algorithm for online bipartite matching. In 22nd Annual ACM Symposium on Theory of Computing, pages 352–358, 1990.
[29] Jon Feldman, Aranyak Mehta, Vahab Mirrokni, and Shan Muthukrishnan. Online stochastic matching: Beating 1-1/e. In 50th Annual IEEE Symposium on Foundations of Computer Science, pages 117–126. IEEE, 2009.
14

[30] John P. Dickerson, Ariel D. Procaccia, and Tuomas Sandholm. Dynamic matching via weighted myopia with application to kidney exchange. In AAAI Conference on Artiﬁcial Intelligence, 2012.
[31] John Dickerson, Karthik Sankararaman, Aravind Srinivasan, and Pan Xu. Allocation problems in ridesharing platforms: Online matching with oﬄine reusable resources. In AAAI Conference on Artiﬁcial Intelligence, volume 32, 2018.
[32] Brian Brubach, Karthik Abinav Sankararaman, Aravind Srinivasan, and Pan Xu. New algorithms, better bounds, and a novel model for online stochastic matching. In 24th Annual European Symposium on Algorithms, 2016.
[33] Nan Kong and Andrew J. Schaefer. A factor 1/2 approximation algorithm for two-stage stochastic matching problems. European Journal of Operational Research, 172(3):740–746, 2006.
[34] Irit Katriel, Claire Kenyon-Mathieu, and Eli Upfal. Commitment under uncertainty: Two-stage stochastic matching problems. Theoretical Computer Science, 408(2-3):213–223, 2008.
[35] Bruno Escoﬃer, Laurent Gourvès, Jérôme Monnot, and Olivier Spanjaard. Two-stage stochastic matching and spanning tree problems: Polynomial instances and approximation. European Journal of Operational Research, 205(1):19–30, 2010.
[36] Euiwoong Lee and Sahil Singla. Maximum matching in the online batch-arrival model. ACM Transactions on Algorithms, 16(4):1–31, 2020.
[37] Yiding Feng, Rad Niazadeh, and Amin Saberi. Two-stage stochastic matching with application to ride hailing. In ACM-SIAM Symposium on Discrete Algorithms, pages 2862–2877. SIAM, 2021.
[38] Niv Buchbinder, Moran Feldman, Joseph Naor, and Roy Schwartz. Submodular maximization with cardinality constraints. In ACM-SIAM Symposium on Discrete Algorithms, pages 1433–1452. SIAM, 2014.
[39] J. Scott Armstrong. Unintelligible management research and academic prestige. Interfaces, 10(2):80–86, 1980.
[40] Elizabeth Pier, Joshua Raclaw, Anna Kaatz, Markus Brauer, Molly Carnes, Mitchell Nathan, and Cecilia Ford. Your comments are meaner than your score: Score calibration talk inﬂuences intra- and inter-panel variability during scientiﬁc grant peer review. Research Evaluation, 26(1):1–14, 2017.
[41] Misha Teplitskiy, Hardeep Ranu, Gary Gray, Michael Menietti, Eva Guinan, and Karim R. Lakhani. Do experts listen to other experts? Field experimental evidence from peer review. Preprint https://www. hbs.edu/ris/Publication%20Files/19-107_06115731-d0ae-4a11-ab1d-ecaec2118921.pdf, 2019.
[42] Stephen J. Ceci and Douglas P. Peters. Peer review: A study of reliability. Change: The Magazine of Higher Learning, 14(6):44–48, 1982.
[43] Ferdinando Patat, Wolfgang Kerzendorf, Dominic Bordelon, Glen Van de Ven, and Tyler Pritchard. The distributed peer review experiment. The Messenger, 177:3–13, 2019.
[44] Ivan Stelmakh, Nihar B. Shah, Aarti Singh, and Hal Daumé III. Prior and prejudice: The novice reviewers’ bias against resubmissions in conference peer review. In ACM Conference on ComputerSupported Cooperative Work and Social Computing, 2021.
[45] Ivan Stelmakh, Charvi Rastogi, Nihar B. Shah, Aarti Singh, and Hal Daumé III. A large scale randomized controlled trial on herding in peer-review discussions. arXiv preprint arXiv:2011.15083, 2020.
[46] Ivan Stelmakh, Nihar B. Shah, and Aarti Singh. On testing for biases in peer review. In Advances in Neural Information Processing Systems, volume 32, pages 5286–5296, 2019.
[47] Ivan Stelmakh, Nihar B. Shah, and Aarti Singh. Catch me if I can: Detecting strategic behaviour in peer assessment. In AAAI Conference on Artiﬁcial Intelligence, 2021.
15

[48] Samuel Madden and David DeWitt. Impact of double-blind reviewing on SIGMOD publication rates. ACM SIGMOD Record, 35(2):29–32, 2006.
[49] Anthony K.H. Tung. Impact of double blind reviewing on SIGMOD publication: A more detailed analysis. ACM SIGMOD Record, 35(3):6–7, 2006.
[50] Emaad Manzoor and Nihar B. Shah. Uncovering latent biases in text: Method and application to peer review. In AAAI Conference on Artiﬁcial Intelligence, 2021.
[51] Nihar B. Shah. Systemic challenges and solutions on bias and unfairness in peer review. Preprint http://www.cs.cmu.edu/~nihars/preprints/Shah_Survey_PeerReview.pdf, 2021.
[52] Laurent Charlin, Richard Zemel, and Craig Boutilier. A framework for optimizing paper matching. In 27th Conference on Uncertainty in Artiﬁcial Intelligence, pages 86–95, 2011.
[53] Liyi Dai, Ching-Hua Chen, and John R. Birge. Convergence properties of two-stage stochastic programming. Journal of Optimization Theory and Applications, 106(3):489–509, 2000.
[54] Alan J. King and R. Tyrrell Rockafellar. Asymptotic theory for solutions in statistical estimation and stochastic programming. Mathematics of Operations Research, 18(1):148–162, 1993.
[55] Richard M. Karp. Reducibility among combinatorial problems. In Complexity of Computer Computations, pages 85–103. Springer, 1972.
[56] Horace He. OpenReview explorer. https://github.com/Chillee/OpenReviewExplorer, 2020 (accessed May 26, 2021).
[57] Thomas Rothvoss. A direct proof for Lovett’s bound on the communication complexity of low rank matrices. arXiv preprint arXiv:1409.6366, 2014.
[58] Victor M. Panaretos and Yoav Zemel. Statistical aspects of Wasserstein distances. Annual Review of Statistics and Its Application, 6:405–431, 2019.
[59] Gurobi Optimization LLC. Gurobi Optimizer reference manual, 2021.
[60] Ariel Kulik, Kanthi Sarpatwar, Baruch Schieber, and Hadas Shachnai. Generalized assignment via submodular optimization with reserved capacity. arXiv preprint arXiv:1907.01745, 2019.
[61] Shayan Oveis Gharan and Jan Vondrák. Submodular maximization by simulated annealing. In ACMSIAM Symposium on Discrete Algorithms, pages 1098–1116. SIAM, 2011.
[62] Uriel Feige, Vahab S. Mirrokni, and Jan Vondrák. Maximizing non-monotone submodular functions. SIAM Journal on Computing, 40(4):1133–1153, 2011.
[63] Wassily Hoeﬀding. Probability inequalities for sums of bounded random variables. In The Collected Works of Wassily Hoeﬀding, pages 409–426. Springer, 1994.
[64] Eric Blais and Ryan O’Donnell. Lower bounds for testing function isomorphism. In IEEE 25th Annual Conference on Computational Complexity, pages 235–246. IEEE, 2010.
[65] Herbert Robbins. A remark on Stirling’s formula. The American Mathematical Monthly, 62(1):26–29, 1955.
[66] Eric Budish, Yeon-Koo Che, Fuhito Kojima, and Paul Milgrom. Implementing random assignments: A generalization of the Birkhoﬀ-von Neumann theorem. In Cowles Summer Conference, 2009.
Appendices
16

1.0

Fraction of optimal similarity

0.8

0.6

0.4

0.2

0.0 ICLR =0.3

IC=L1R Pre=fL0i.b33 Pre=fL1ib3 B=id01.3 Bi=d11 (a) Additional values of β

S=IG0I.R3 SI=GI1R

1.0

0.8

0.6

0.4

0.2

0.0 PrefLib1 PrefLib1 PrefLib1 PrefLib2 PrefLib2 PrefLib2 Bid2

Bid2

Bid2

=0.1

=0.3

=0.5

=0.1

=0.3

=0.5

=0.1

=0.3

=0.5

(b) Additional similarity matrices

Figure 3: Additional results showing ranges of values found over 10 random reviewer splits.

Fraction of optimal similarity

A Additional Empirical Results
We ﬁrst present empirical evaluations showing the performance of random split on additional values of β for the similarity matrices used in Section 5.1 (Figure 3a), as well as on additional similarity matrices constructed from bidding data (Figure 3b). Two of these additional similarity matrices were constructed using bid data for the two other AI conferences (conferences 1 and 2) from PrefLib dataset MD-00002 [7] with sizes (n = 54, m = 31) and (n = 52, m = 24) respectively. Another additional similarity matrix (marked Bid2) was constructed from another sample of the bidding data from a major computer science conference [8] with size (n = 1200, m = 300). As in the bidding datasets shown earlier, we transformed “yes,” “maybe,” and “no response” bids into similarities of 1, 0.5, and 0.25 respectively.
We run several experiments, each corresponding to a choice of dataset and β. Each experiment consists of 10 trials, where in each trial we sample a random reviewer split and a set of second-stage papers, and report the range of assignment values found as percentages of the oracle optimal assignments for each trial. We set paper loads of 2 in each stage, and limit reviewer loads to be at most 6 for all datasets except PrefLib2 and Bid2, which limit reviewer loads to be at most 12 (for feasibility). As in Section 5.1, we draw R2 uniformly at random with size 1+ββ m and draw P2 uniformly at random with size βn. In general, we see that random split performs very well on these datasets as well. We see that all trials of random split achieve at least 88% of the oracle optimal solution’s similarity on all datasets, with all trials on all but three experiments achieving at least 94%. The range of values on each experiment is generally small (at most 7%), with the largest ranges occuring on the small PrefLib datasets.
We additionally test the bounds of Section 7 on these datasets as well as the SIGIR dataset to evaluate

17

Average assignment value

Average assignment value

Random split expectation

Theorem 5 bound

Theorem 6 bound

0.04

0.6

0.8

0.02

0.6

0.00.04

0.4

0.00.22

0.2

0.00.40

5 10 15 20

Assignment load parameter

0

2

4

(a) PrefLib1

0.0

5 10 15 20

Assignment load parameter

6

8

(b) PrefLib2

0.6

0.5

0.4

0.3

0.2

0.1

0.0

5 10 15 20

Assignment load parameter

Average assignment value

0.5

0.4

0.3

0.2

0.1

0.0

5 10 15 20

Assignment load parameter

Average assignment value

(c) Bid2

(d) SIGIR

Figure 4: Performance of Theorem 5 and 6 bounds on additional real conference datasets, β = 1. The best setting of µ shown for each bound is circled.

how well they explain the performance of random split. On PrefLib1, Preﬂib2, and Bid2, we scale up the number of reviewers by 4, 5, and 8 respectively for feasibility, as described in Section 7.2. On the x-axis we vary the parameter µ, which determines the loads of the assignment A(µ) used in the bound.
In Figure 4, we see similar results to those shown earlier. The Theorem 5 bound performs best at low values of µ. The Theorem 6 bound performs better at higher values of µ, although for some datasets a more moderate value of µ is better since the assignment value s(µ) drops too quickly at higher µ. From the Theorem 6 bound, we see that the good performance of random split on these datasets is generally explained fairly well by the large-load, high-similarity assignment.
All empirical evaluations in this paper were run on a computer with 8 cores and 16 GB of RAM, running Ubuntu 18.04 and solving the LPs with Gurobi 9.0.2 [59].
B Empirical Results for Paper-Split Variant
In this section, we provide some additional empirical results that are particularly relevant to the conference experiment design setting. Sometimes, conferences may not have the reviewing resources to provide a signiﬁcant number of papers with two sets of reviews as part of an experiment. Instead, they may want to provide reviews to each paper under only one of the conditions. If the papers and reviewers are both split between conditions uniformly at random, this can be seen as a variant of our standard two-stage paper

18

Fraction of optimal similarity

1.0

0.8

0.6

0.4

0.2

0.0 ICLR

PrefLib3

Bid1

SIGIR

Figure 5: Range of values found over 10 random reviewer splits when papers split between stages.

assignment problem where only papers in P1 = P \ P2 are assigned reviewers in stage one. To test whether such experiments will still give high-similarity assignments in practice, we conduct
additional empirical evaluations. The results of these experiments are shown in Figure 5. For each dataset of those introduced in Section 5.1, we take 10 samples of random reviewer and paper splits where |R2| = m/2 and |P2| = n/2 so that half of the reviewers and papers are in each stage (i.e., each condition). We then ﬁnd assignments in each stage with paper loads of 3 and reviewer loads of at most 6 (standard conference loads), and display the range of assignment values found as a fraction of the oracle optimal assignment’s value. On all datasets, all trials of random reviewer split achieve over 75% of the oracle optimal assignment’s total similarity with low variation (at most 4%). On ICLR and SIGIR, all trials achieve over 90% of the oracle optimal similarity. Overall, the average assignment quality is slightly worse than in the standard model (where all papers are in stage one). This is likely because it is more diﬃcult for reviewers to be assigned to their optimal papers when each paper is in only one of the two stages.

C Submodularity of f

In this section, we show that the problem of optimizing f (or f ) is actually an instance of submodular

optimization. For simplicity, we consider the case where m = (1 + β)n and

rev =

(1) pap

=

(2) pap

=

1.

For some set N , a function g : 2N → R is submodular if g(A ∪ {u}) − g(A) ≥ g(B ∪ {u}) − g(B) for all

A ⊆ B ⊆ N and all u ∈ N \ B. Since f and f are deﬁned only for R2 where |R2| = βn, we modify them to

be deﬁned over 2R.

Recall that for subsets R ⊆ R and P ⊆ P, desired paper load pap, and maximum reviewer load rev,

M(R , P ; rev, pap) is the set of assignment matrices that assign a load of exactly pap to all papers in P

(and a load of at most rev to all reviewers in R ). Deﬁne M (R , P ; rev, pap) as the set of assignment

matrices that assign a load of at most pap to all papers in P (and a load of at most rev to all reviewers in

R ). Formally, A ∈ M (R , P ; rev, pap) if and only if r∈R Arp ≤ pap for all p ∈ P , p∈P Arp ≤ rev

for all r ∈ R , and Arp = 0 for all (r, p) ∈ R × P .

Consider the modiﬁed version of Q





1

Q (R2, P2) = (1 + β)n A∈M (Rm\aRx2,P;1,1)

ArpSrp +

max

B∈M (R2,P2;1,1)

BrpSrp

r∈R\R2 ,p∈P

r∈R2 ,p∈P2

which allows papers to be underloaded and so is deﬁned for all R2 and P2. Deﬁne fsub(R2) = EP2 [Q (R2, P2)]

and

f sub(R2)

=

1 K

K i=1

Q

(R2,

P2(i))

as

modiﬁcations

of

f

and

f.

Since

S

≥ 0,

there

exists

a

maximum-

similarity assignment from within M (R , P ; 1, 1) that meets all paper load constraints with equality when

|R | ≥ |P | and thus is contained in M(R , P ; 1, 1). Also, M(R , P ; 1, 1) ⊆ M (R , P ; 1, 1). Thus, when

|R2| = βn, Q(R2, P2) = Q (R2, P2). Therefore, subject to the constraint |R2| = βn, maximizing fsub (or

f sub) is equivalent to maximizing f (or f ).

19

Proposition 1. fsub and f sub are submodular in R2.
Proof. Note that maxA∈M (R ,P ;1,1) r∈R ,p∈P ArpSrp is a submodular function of the reviewer set R when the paper set P is held ﬁxed [60]. Submodularity in R2 is equivalent to submodularity in R1 = R \ R2, so Q (R2, P2) is submodular in R2. As sums over terms submodular in R2, fsub and f sub are submodular in R2.
Therefore, the two-stage paper assignment problem is an instance of maximizing a non-monotone submodular function subject to a cardinality constraint |R2| = βn. However, no value oracle for the function f is available due to the expectation over P2. Since a polynomial-time value oracle is available for f , the paper [38] gives an approximation algorithm achieving an approximation ratio of no greater than 0.5 (depending on β). This guarantee does not imply much about the assignment quality, since it can be trivially achieved by maximizing assignment similarity in the ﬁrst stage only. Furthermore, it is known that achieving an approximation ratio of greater than 0.5 requires an exponential number of queries to the value oracle; this holds true even without the cardinality constraint [61, 62]. Thus, generic algorithms for submodular maximization are not helpful for our problem.
D Proofs
D.1 Proof of Theorem 1
We will show that it is NP-hard to determine if there exists a choice of R2 with value f (R2) = 1 when K = 3, for some instance of P2(1), P2(2), P2(3). If such a choice exists, it would be the optimal solution. Therefore, any algorithm to optimize f would be able to determine if there exists a solution with value 1, solving an NP-hard problem. This implies the NP-hardness of optimizing f .
We reduce from 3-Dimensional Matching, an NP-complete problem [55]. An instance of 3-Dimensional Matching consists of three sets X, Y, Z of size s, and a collection of tuples T ⊆ X × Y × Z. It asks whether there exists a selection of s tuples from T that includes each element of X, Y , and Z exactly once.
Given such an instance of 3-Dimensional Matching, we construct an instance of the two-stage paper assignment problem with n = |T | + 2s, m = |T | + 3s, and β = mn − 1 ( rev = (p1a)p = (p2a)p = 1). βn = s papers and reviewers will be in stage two. The ﬁrst s papers correspond to elements of X, the next s to elements of Y , and the next s to elements of Z; the remaining |T | − s papers are “dummy papers” that all reviewers can review. The ﬁrst 3s reviewers are “specialty reviewers” corresponding to each of the ﬁrst 3s papers, and the remaining |T | reviewers correspond to each of the elements of T . We construct the K = 3 sampled subsets P2(1) = {1, . . . , s}, P2(2) = {s + 1, . . . , 2s}, P2(3) = {2s + 1, . . . , 3s}, where the elements of these sets correspond to the elements of X, Y , and Z respectively. We then construct S as follows. For i ∈ [3s], set Sii = 1 and Sij = 0 for all j ∈ [3s], j = i. For the remaining reviewers i ∈ {3s + 1, . . . , 3s + |T |} and for papers j ∈ [3s], set Sij = 1 if the element corresponding to j in X ∪ Y ∪ Z is included in the tuple corresponding to i in T . Finally, for the remaining papers j ∈ {3s + 1, . . . , |T | + 2s}, set Sij = 1 for all reviewers i.
Suppose we have a “yes” instance of 3-Dimensional Matching, so there exists a choice of s tuples from T that cover each element of X, Y , and Z. Choose the corresponding s reviewers as R2 and the remaining reviewers as R1. In stage one, we can assign each specialty reviewer to each of their corresponding papers and each of the remaining |T | − s reviewers in R1 to dummy papers. In stage two, for each of the three possible samples, there exists one reviewer that has similarity 1 with each paper since the corresponding choice of tuples from T cover X, Y , and Z. Therefore, this partition achieves f (R2) = 1.
Suppose we have a “no” instance of 3-Dimensional Matching, so no choice of s tuples from T covers each element of X, Y , and Z. We claim that no choice of R2 will achieve s total similarity in the second stage. First, suppose we include a speciality reviewer in R2. This reviewer has similarity 1 with only one paper, so there exists a sample of stage two papers P2(i) such that this reviewer must be assigned to a paper with which it has similarity 0. Therefore, f (R2) cannot be 1 when a specialty reviewer is in R2 and so R2 must be chosen from the reviewers corresponding to elements of T . However, no choice of s tuples covers each element of X, Y , and Z. Therefore, for every choice of R2, some reviewer must be assigned to a paper with which they have similarity 0 for at least one of the sampled sets of stage two papers. This means that f (R2) = 1 is unachievable.
20

D.2 Proof of Theorem 2
For any β ∈ [0, 1], choose any n such that βn ∈ Z+. We construct the following similarity matrix. Paper i has similarity 1 with reviewer i, and also with reviewer n + i if i ≤ βn. All other similarities are 0.
On this example, the oracle optimal assignment for any P2 is to assign reviewers {1, . . . , n} to papers in the ﬁrst stage, since this maximizes the similarity across both stages. This choice gives a total similarity of n in stage one and an expected similarity of β2n in stage two (since each reviewer’s matching paper is in stage two with probability β), for a total similarity of n(1 + β2). Since there are (1 + β)n total assignments, the expected mean similarity is 11++ββ2 .
Now consider the assignment after randomly splitting reviewers. Any paper p ≤ βn has two reviewers a, b with similarity 1. For suﬃciently large n ≥ 11++4ββ , the expected value of this paper’s assignment is

(P [a ∈ R1 ∧ b ∈ R2] + P [b ∈ R1 ∧ a ∈ R2])(1 + P [p ∈ P2])

+ P [a, b ∈ R1] + P [a, b ∈ R2]P [p ∈ P2]

n

βn

n

n−1

βn

βn − 1

=2

(1 + β) +

+

β

(1 + β)n (1 + β)n − 1

(1 + β)n (1 + β)n − 1 (1 + β)n (1 + β)n − 1

β

1

β3

≤2 (1 + β) −

1

+

(1 + β)2

+

(1 + β)2

n

1 + 4β

1

β3

≤ 2(1 + β) + (1 + β)2 + (1 + β)2 .

There are βn of these papers. Any of the remaining papers p > βn has only one reviewer a with similarity 1. The expected value of this
paper’s assignment is

P [a ∈ R1] + P [a ∈ R2]P [p ∈ P2]

1 + β2

=

.

1+β

There are (1 − β)n of these papers. Totalling over all papers and dividing by the total number of assignments, the mean expected similarity
of random split is at most

1 + 4β

1

β3

2(1 + β) + (1 + β)2 + (1 + β)2

β (1 + β2)(1 − β) 1 + β + (1 + β)2 .

The suboptimality is therefore at least

1 + β2 −
1+β

1 + 4β

1

β3

2(1 + β) + (1 + β)2 + (1 + β)2

β (1 + β2)(1 − β)

− 1+β

(1 + β)2

(1 + β2)2β = (1 + β)2 −

1 + 4β

1

β3

2(1 + β) + (1 + β)2 + (1 + β)2

β 1+β

=

2(1

+

β2)(1

+

β)

−

1 (1

+

4β)(1

+

β)

−

1

−

β3

β

2

(1 + β)3

=

1

−

1 β

+

β3

β

22

(1 + β)3

β4 ≥ (1 + β)3 .

D.3 Proof of Theorem 3
By Lemma 4 of [57], a rank k similarity matrix S ∈ [0, 1](1+β)n×n can be factored into vectors ur ∈ Rk for each reviewer r and vp ∈ Rk for each paper p such that Srp = ur, vp , ||ur||2 ≤ k1/4, and ||vp||2 ≤ k1/4.

21

Consider the ball of radius k1/4 in Rk in which the paper vectors vp lie. We cover this ball with smaller “cells” by dividing the containing k-dimensional hypercube with side length 2k1/4 along each dimension to
create some number of smaller hypercubes. If we divide the containing hypercube into t equal-sized segments along each dimension, there are tk cells in total and the maximum L2 distance between two points in a cell is . 2k3/4
t
We construct L layers of cells in this way, where the cells increase in size between layers. Denote by ti the number of divisions along each dimension at layer i. We choose ti = 2Zi for some integer Zi for all layers i so that each cell at layer i is fully contained within a single cell at each higher layer. Denote by si the desired maximum within-cell distance at layer i. This distance is achieved if ti is at least 2ks3i/4 , so the minimum such ti that is also a power of two is at most 4ks3i/4 . This gives that there are at most zi = 4ks3i/4 k cells in layer i.
In what follows, we say that a paper p is in some cell if its vector vp is in the cell. (Papers on the border of multiple cells at layer 1 are considered to be in an arbitrary one of the bordering cells so that each paper
is in exactly one cell. At higher layers, such papers are considered to be in the cell containing their layer 1
cell.) We say that a reviewer is in a cell if it is assigned to a paper in that cell by the oracle optimal paper
assignment (given P2). Given P2 and R2 produced by random split, we proceed through layers from 1 to L in order to match
reviewers to papers in the same stage. We match as many reviewers as possible to papers that are within the
same cell at each layer i, and then continue to layer i + 1. Deﬁne ni as an upper bound on the number of reviewers unmatched before matching within layer i; n1 = (1 + β)n. The diﬀerence in value between the assignment A produced in this way and the oracle optimal assignment A∗ (which we call the “value gap”) is

(A∗rp − Arp) ur, vp
r∈R,p∈P

=

ArpA∗rp∗ ur, vp∗ − vp

r∈R,p∈P ,p∗ ∈P

≤

A

r

p

A

∗ rp∗

||ur

||2

||v

p

∗

−

vp||2

r∈R,p∈P ,p∗ ∈P

≤ k1/4

A

r

p

A

∗ rp∗

||v

p

∗

−

vp||2.

r∈R,p∈P ,p∗ ∈P

Consider some cell containing x papers. All x of these papers are in stage one. Deﬁne Hyp(N, K, M )

as the hypergeometric distribution where N is the population size, M is the number of draws, and K is

the number of successes in the population; by symmetry Hyp(N, K, M ) is equivalent to Hyp(N, M, K).

The number of stage two papers has distribution Hyp(n, x, βn). With probability 1 − 2δ, by Hoeﬀding’s

inequality [63] and using the symmetry property, within βx ±

x 2

ln(1/δ)

of

the

papers

in

this

cell

are

also

in stage two. (In this section, ln indicates the logarithm with base e and log indicates the logarithm with

base 2.) Call y the total number of reviewers in the cell. There are exactly the same number of reviewers

as total stage one and two papers in this cell, so y is within (1 + β)x ±

x 2

ln(1/δ)

and

is

at

most

2x.

Since R2 is produced by random split, the number of reviewers in this cell in stage one has distribution

Hyp((1 + β)n, y, n) and the number of reviewers in this cell in stage two has distribution Hyp((1 + β)n, y, βn).

By Hoeﬀding’s inequality and again using symmetry, the number of reviewers in the cell in stage one is at most

1+yβ + y2 ln(1/δ) ≤ x + x2 ln(1/δ) + x ln(1/δ) ≤ x + 3x ln(1/δ) with probability 1 − 2δ (conditioned

on the earlier event concerning the number of stage-two papers). By this argument, with probability 1 − 4δ

(again conditioned on the earlier event), there are within x ± 3x ln(1/δ) reviewers in stage one in the cell

and within βx ± 3x ln(1/δ) reviewers in stage two in the cell. In total, there are at most nL cells with a non-zero number of papers across all layers and so the total probability of error in any of the bounds is at most 6δLn.
Assume that this high probability event occurs. In layer i, in any cell j with xj papers (all of which are in

stage one), the number of stage one reviewers is within xj ± 3xj ln(1/δ). Any reviewers in this cell matched at earlier layers must have been matched to papers also in this cell. Therefore, the number of unmatched stage one reviewers after matching within this cell is at most 3xj ln(1/δ). The number of stage two reviewers

is within βxj ± 3xj ln(1/δ) and the number of stage two papers is within βxj ± x2j ln(1/δ). Therefore,

the number of unmatched stage two reviewers after matching within this cell is at most 6xj ln(1/δ). In total over both stages, the total number of unmatched reviewers after matching in layer i is at most

22

ni+1 =

zi j=1

18xj ln(1/δ) ≤

18zin ln(1/δ). All of the reviewers matched at layer i are matched to papers

at most si away from their optimal paper assignment. Across all layers, the value gap is therefore bounded by

k1/4

L−1 i=1

nisi

+

2nLk1/4

, since everything at layer L is matched to whatever remains regardless of sL.

We now determine how to set si for all layers i. We choose s1 = s and set other si such that nisi = (1+β)ns

for all i. This leads to the recursively-deﬁned values of si = (1+nβi)ns , zi = (4k3/4)ks−i k, ni+1 = with initial values n1 = (1 + β)n and s1 = s. Unrolling the iteration, we see that

zin18 ln(1/δ)

si = sik2−1n 12 s(4k3/4)− k2 (18 ln(1/δ))− 21 (1 + β)

1
= n2

i−2 j=0

(

k 2

)j

s

i−1 j=0

(

k 2

)

j

(4k

3/

4

)−

k 2

i−2 j=0

(

k 2

)j

(18

ln(1/δ

))−

1 2

( i−2
j=0

k 2

)j

(1

+

β)

i−2 j=0

(

k 2

)j

for i ≥ 2. Deﬁning such that s = 1(81+lnβ(1)/2δn) ,

(1 + β)2n

1 2

i−2 j=0

(

k 2

)j

+

si = 18 ln(1/δ)

i−1 j=0

(

k 2

)j

(4k

3/

4

)−

k 2

i−2 j=0

(

k 2

)j

for i ≥ 2. This gives a value gap of at most k1/4(1 + β)1+2 n1+ (18 ln(1/δ))− continue in cases on the value of k.
Case k = 1. Note that ij−=10 k2 j = 2 1 − 21i , so

(L − 1) + 2k1/4s−L1 . We now

si =

(1 + β)2n 18 ln(1/δ)

1−

1 i−1

+

2(1−

1 i

)

2

2

(4k3/4)−1+

1 2i−1

.

Choose = − 12 + 2(2L1−1) so that sL = (4k3/4)−1+ 2L1−1 . Setting δ = (2n)−3 and L = log log n, for suﬃciently large n, the value gap is bounded by

k1/4(1

+

β)

1 2L −1

n

1 2

+

1 2(2L −1)

(18

ln(1/δ))

1 2

−

1 2(2L −1)

(L

−

1)

+

2k1/4(4k3/4)1−

1 2L−1

≤

(1

+

1
β)n 2

2log

(

n)

1 2(log n−

1)

1
(54 ln(2n)) 2

((log

log

n

−

1)

+

8)

1

1

≤ 2(1 + β)n 2 (54 ln(2n)) 2 ((log log n − 1) + 8)

≤

C (log

n)η n

1 2

for some constants C, η with probability 1 − 6 lo8gnlo2g n ≥ 1 − n1 . Case k = 2. Note that ij−=10 k2 j = i, so

si =

(1 + β)2n 18 ln(1/δ)

21 (i−1)+ i
(4k3/4)−i+1.

Choose

=

− 12

+

1 2L

so

that

sL

=

(4k3/4)−L+1.

Setting

δ

=

(2n)−3

and

L

=

log log n,

for

suﬃciently

large

n,

the value gap is bounded by

k1/4(1

+

β)

1 L

n

1 2

+

1 2L

(18

ln(1/δ))

1 2

−

1 2L

(L − 1) + 2k1/4(4k3/4)L−1

≤

k1/4(1

+

β

)n

1 2

+

2

1 log log

n

(54

1
ln(2n)) 2

(log log n − 1) + 2k1/4(log n)log(4k3/4)

≤

C (log

n

)η

n

1 2

+

log

1 log n

for some constants C, η with probability 1 − 6 lo8gnlo2g n ≥ 1 − n1 . Case k ≥ 3. Note that ij−=10 k2 j = ( k2k)−i−1 1 , so
2

si =

(1 + β)2n 18 ln(1/δ)

(

1 2

+

)

(k/2)i −1 (k/2)−1

−

1 2

(

k 2

)i−1

k

(4

k

3/

4

−
)

2

(k/2)i −1 (k/2)−1

+(

k 2

)

i

.

23

Choose

= − k1 + (k(/122−)Lk1−)1 so that sL = (4k3/4)− k2

(k/2)L −1 (k/2)−1

for suﬃciently large n, the value gap is bounded by

+( k2 )L . Setting δ = (2n)−3 and L = log log log n ,
log(k/2)

2

2(

1 2

−

1 k

)

1

(

1 2

−

1 k

)

1

(

1 2

−

1 k

)

k1/4(1 + β)1− k + (k/2)L−1 n1− k + (k/2)L−1 (18 ln(1/δ)) k − (k/2)L−1

k
(L − 1) + 2k1/4(4k3/4) 2

(k/2)L −1 (k/2)−1

−(

k 2

)L

≤ k1/4(1 + β)n1− k1 + lo(g12lo−g nk1−)1 (54 ln(2n)) k1

≤ k1/4(1 + β)n1− k1 + lo(g12lo−g nk1−)1 (54 ln(2n)) k1

≤

C (log

n)η

n1−

1 k

+

log

1 log

n

log log log n − 1 + 2k1/4(4k3/4)2 log log n log(k/2)
log log log n − 1 + 2k1/4(log n)2 log(4k3/4) log(k/2)

for some constants C, η with probability 1 − 68lloogg(lokg/2lo)gn2n ≥ 1 − n1 . To get the suboptimality, divide the value gap by (1 + β)n ≤ 2n.

D.4 Proof of Theorem 4

(a) Choose n large enough such that k ≤ n2 . We deﬁne k groups of reviewers and papers such that all papers have similarity 1 with all reviewers within the same group and similarity 0 with all other reviewers. Group 1

contains all papers p ≤

n 2

and all reviewers r ≤ 2

n 2

.

Each other group 2, . . . , k contains 2 reviewers and 1

paper. All papers and reviewers not in any group have all similarities 0. This similarity matrix has rank k.

The oracle optimal assignment for any P2 will split the reviewers in each group evenly between stages, so

all papers in any group can be assigned a similarity-1 reviewer in both stages. This gives a total similarity of

at least n + 2(k − 1).

Deﬁne X as the random variable representing the number of reviewers from group 1 selected to be in R2.

X ∼ Hyp(2n, 2 n/2 , n), the hypergeometric distribution corresponding to the number of successes when n

items are sampled without replacement from a population of 2n items where 2 n/2 of them are successes.

By Lemma 2.1 of [64], P [X = t] ≤ Cσ for any t ≥ 0 where σ2 = n2/2 1 − nn/2 and C is an absolute

constant.

Since

σ2

≥

n 4

1 − 12 − n1

≥

n 16

for n ≥ 4, P [X

= t] ≤

√4C n

for suﬃciently large n.

Therefore,

√

√

√

P n2 − 16nC + 1 ≤ X ≤ n2 + 16nC ≤ 12 . With probability at least 12 , at least 16nC of the reviewers in group 1

cannot be √matched to an optimal paper in their stage. Therefore, the total expected similarity is no greater

than n − n + 2(k − 1) and the expected diﬀerence in value from the oracle optimal assignment is at least

√

32C

32nC . Since there are 2n assignments, the suboptimality is at least 64C1√n .

(b) We construct a similarity matrix by creating a vector in Rk for each reviewer and each paper, and setting the similarity between that reviewer and that√paper to be the inner product of their corresponding vectors. Consider the cube in Rk contained in [0, 1/ k]k. We construct a grid of points within this cube by evenly spacing z = n1/k along each axis and ﬁlling in the remaining points so that there are zk ≥ n grid points in total. Place the n paper vectors at arbitrary (unique) points on this grid, so that each vector is at least √k n11/k ≥ 2√k1n1/k away from any other paper vector. Place the 2n reviewer vectors such that 2 are at each grid point with a paper vector. The inner product of any two vectors is in [0, 1], so this is a valid similarity matrix. The 2n × k and n × k matrices where the rows are the reviewer and paper vectors respectively have linearly independent columns and so have rank k; thus, the similarity matrix has rank k.
We claim that the oracle optimal matching across both stages chooses one reviewer from each grid point and matches it to the paper at the same point. Suppose we have a matching where this is not the case. There must exist a cycle of matched reviewer and paper pairs where the corresponding vectors are not paired with themselves and are instead paired (x1, x2), (x2, x3), . . . , (xK , x1). This cycle has a total similarity of (using

24

xK+1 to refer to x1)

K
xi, xi+1
i=1

K
≤ ||xi||2||xi+1||2
i=1 K
≤ ||xi||22
i=1 K
= xi, xi
i=1

so the matching value can be improved by changing the cycle so that reviewers and papers at the same grid point are matched. The second inequality is because 2ab ≤ a2 + b2 for any a, b ∈ R. Therefore, the claimed matching is indeed optimal.
Now, consider the sample of n reviewers in stage one produced by a random split of reviewers. The following lemma shows that with probability 1 − O(e−n/10), Θ(n) grid points have both reviewers present in stage one under random split.

Lemma 1. There exists n0 and a constant ζ such that for all n ≥ n0, the probability that less than n/100 grid points have both reviewers in stage one after a random reviewer split is at most ζe−n/10.

Proof. There are at most na 3n−a ways to assign reviewers to stages such that a pairs of reviewers at the same grid point are in stage one. For all n and a such that n + 1 ≥ 4a, na 3n−a = a−n1 n+a1−a 3n−a ≥ a−n1 3n−a+1.

Setting a = n/100, n/n100 ≤ (100e)n/100 ≤ exp(0.06n) and 3n−(n/100) ≤ exp(1.09n). Therefore, the

number of ways to assign reviewers to stages such that less than n/100 pairs are in stage one is at most

(n/100)−1 b=0

nb 3n−b ≤ (n/100) n/n100 3n−(n/100) ≤ exp(1.15n + ln(0.01n)).

Using Sterling inequalities [65],

√

√

the total number of ways to assign reviewers to stages is

2n n

≥ e22√πn 22n ≥ 2e2π exp(1.35n − 0.5 ln(n)).

Therefore, the probability that less than n/100 grid points have both reviewers in stage one is at most

e√2 exp(−0.2n+ln(0.01n)+0.5 ln(n)) ≤ e√2 exp(−0.1n) exp(−0.1n+ln(0.01n)+0.5 ln(n)) ≤ e√2 exp(−0.1n)

2π

2π

2π

for suﬃciently large n.

Therefore, with high probability, at least n/100 reviewers must be assigned to a paper at a diﬀerent grid

point.

Consider the assignments produced in each stage after random split, and consider the reviewers not

assigned to their optimal papers by these assignments. From the set of vectors corresponding to these

suboptimally-assigned reviewers, we can construct some number K of disjoint cycles Cj = {x(1j), . . . , x(Kjj) },

where a reviewer with vector x(ij) is assigned to the paper with vector x(i+j)1 when the optimal assignment

would assign them to the paper with vector x(ij). By Lemma 1,

K j=1

|Cj |

≥

1n00 .

The

diﬀerence

in

value

between the random-split assignments and the optimal assignment is

K Kj
x(ij), x(ij) − x(i+j)1
j=1 i=1

1 K Kj (j)

(j) 2

= 2

||xi − xi+1||2

j=1 i=1

1

K

≥ n−2/k |Cj |

8k

j=1

≥ 1 n−2/k n

8k

100

with probability at least 1 − ζe−n/10 for suﬃciently large n. Dividing by 2n, the suboptimality is at least 16010k n−2/k.

25

D.5 Proof of Theorem 5
In this section, we state and prove a more general version of the bound in Theorem 5 that does not require that βµ be integral. This result immediately implies the result of Theorem 5.
In the proof, we use the following lemma. We prove this lemma following the proof of the main theorem. For some set N and some constant p ∈ [0, 1], deﬁne distribution Ip(N ) as the distribution over all subsets A ⊆ N induced by choosing each item x ∈ N to be in A independently with probability p. Recall from Appendix C the deﬁnition of Q , a modiﬁed version of Q that allows papers to be underloaded (i.e., assigned fewer reviewers than their load).
Lemma 2. Consider the modiﬁed version of f : f (R2) = EP2∼Iβ(P) [Q (R2, P2)].f draws P2 ∼ Iβ(P) rather than P2 ∼ Uβn(P) and allows papers to be underloaded. Then,

ER2∼Iβ/(1+β)(R) [f (R2)] ≤ ER2∼U(β/(1+β))m(R)[f (R2)].
This lemma shows that when attempting to lower bound the expected value of random split, we can analyze as if the second-stage reviewers and papers were drawn independently.
We now state and prove the main theorem. We abuse notation slightly by deﬁning M(R, P; rev, pap) to include all assignments where papers are assigned either pap or pap reviewers when pap is not integral; i.e., A ∈ M(R , P ; rev, pap) if and only if pap ≤ r∈R Arp ≤ pap for all p ∈ P , p∈P Arp ≤ rev for all r ∈ R , and Arp = 0 for all (r, p) ∈ R × P .
Theorem 5 (Generalized). Consider any µ ∈ [10, 000] and β ∈ 1100 , . . . , 110000 . Let = βµ − βµ . If there exists an assignment A(µ) ∈ M(R, P; µ, (1 + β)µ) with mean similarity s(µ), choosing R2 via random split gives that

ER2 [f (R2)] ≥

s(µ) 1 −

β 2π(1 + β) (1 + β)µ

1

(1 + 2β)

2

+ 1−β −

1+β

(1 + β) βµ

1−

.

(1 + β)µ

Proof. By Lemma 2, we can consider drawing P2 ∼ Iβ(P) and R2 ∼ Iβ/(1+β)(R) and allowing papers to be

underloaded. For all reviewers r ∈ R, deﬁne the random variables Zr = 1 w.p. 1/(1 + β) 2 w.p. β/(1 + β)

representing

the stage that reviewer r is randomly chosen to be in. Deﬁne the random variables Yp = 1 w.p. β 0 w.p. 1 − β
representing whether p ∈ P2. All of these random variables are independent. Also, denote by v(µ) = s(µ)(1 + β)nµ the total similarity value of assignment A(µ), and denote by vp(µ) and vr(µ) the total similarity value of the assignments for paper p and reviewer r respectively in assignment A(µ).
The proof works as follows. We form an assignment B(1) in stage one with paper loads of at most µ and reviewer loads of at most µ, and form an assignment B(2) in stage two with paper loads of at most βµ and reviewer loads of at most βµ . We do this by initially assigning all reviewer-paper pairs from A(µ) that are
present in the same stage, and then randomly removing assignments from each paper or reviewer that is
overloaded. We then ﬁnd “ﬁnal assignments” (i.e., assignments that are feasible solutions for the two-stage assignment problem) from within B(1) and B(2).

Stage One: First, consider stage one. Deﬁne Binom(N, p) as the binomial distribution with N trials and p probability of success; denote by f the Binomial pmf. The number of reviewers assigned by A(µ) to paper p

and present in stage one is a Binom

λp,

1 1+β

variable, where λp ∈ { (1 + β)µ , (1 + β)µ }. Suppose that

we observe the set of such reviewers, randomly remove reviewers from this set until its size is at most µ, and then assign these reviewers to p in our stage one assignment B(1). Since each reviewer has at most µ assigned papers in A(µ), B(1) satisﬁes the desired load constraints on both sides. The expected total value of

26

the assigned reviewers after we drop reviewers from each paper at random is

E

Br(1p)Srp

r∈R

µ
=f
x=0

1 x; λp, 1 + β

λp

v(µ) x +

f

p λp

x=µ+1

1 x; λp, 1 + β

vp(µ) = λp EX∼Binom(λp, 1+1β ) [min(X, µ)]

vp(µ) ≥ (1 + β)µ EX∼Binom( (1+β)µ , 1+1β ) [min(X, µ)] .

v(µ) µ p λp

Summing over all papers,





E

Br(1p)Srp ≥

p∈P r∈R

v(µ) (1 + β)µ EX∼Binom( (1+β)µ , 1+1β ) [min(X, µ)] .

Due to the loads, the matrix µ1 B(1) has row sums at most 1 and column sums at most 1. By a generalization

of the Birkhoﬀ-von Neumann theorem [66], this can be written as a convex combination of matrices with all

entries in {0, 1}, all row sums at most 1, and all column sums at most 1. Each of these matrices represents an

assignment obeying the reviewer and paper load constraints for the ﬁnal assignment (since we allow papers

to be underloaded), so they are all valid ﬁnal assignments in stage one. At least one of these assignments

must

have

a

total

value

at

least

1 µ

of

the

value

of

B(1).

Stage Two: Now, consider stage two. The number of reviewers assigned by A(µ) to paper p present in stage

two is a Binom

λp,

β 1+β

variable, where λp ∈ { (1 + β)µ , (1 + β)µ }. The number of papers assigned

by A(µ) to a reviewer r present in stage two is a Binom(µ, β) random variable. We ﬁrst calculate the total expected value of all assignments in A(µ) and present in stage two (without dropping assignments from

overloaded reviewers/papers):





(µ)

β2 (µ)

E

Arp

Srp

=

1

+

v β

.

r∈R2 ,p∈P2

We then construct assignment B(2a) from the pairs assigned in A(µ) and present in stage two by dropping reviewers from each paper at random until all papers have a load of at most βµ , with a value on paper p (if present in stage two) of

E

Br(2pa)Srp p ∈ P2

r∈R

βµ
=f
x=0

β x; λp, 1 + β

λp

v(µ) x +

f

p λp

x= βµ +1

β x; λp, 1 + β

vp(µ) = λp EX∼Binom(λp, 1+ββ ) [min (X, βµ )]

vp(µ) ≥ (1 + β)µ EX∼Binom( (1+β)µ , 1+ββ ) [min (X, βµ )] .

v(µ) βµ p λp

Each paper is present in stage two with probability β, so the overall value is





E

Br(2pa)Srp ≥

p∈P r∈R

βv(µ) (1 + β)µ EX∼Binom( (1+β)µ , 1+ββ ) [min (X, βµ )] .

We separately construct assignment B(2b) from the pairs assigned in A(µ) and present in stage two by dropping papers from each reviewer at random until all reviewers have a load of at most βµ , with a value on reviewer

27

r (if present in stage two) of





βµ

µ

E  Br(2pb)Srp r ∈ R2 = f (x; µ, β) vr(µ) µx +

f (x; µ, β) vr(µ) βµµ

p∈P

x=0

x= βµ +1

vr(µ) = µ EX∼Binom(µ,β) [min (X, βµ )] .

Totalling across all reviewers, since each reviewer is present in stage two with probability 1+ββ ,





(2b)

βv(µ)

E

Brp Srp = (1 + β)µ EX∼Binom(µ,β) [min (X, βµ )] .

r∈R p∈P

Deﬁne B(2) as the intersection of the assigned pairs in B(2a) and B(2b); B(2) satisﬁes the desired load constraints on both sides. Its expected value is lower-bounded by the total expected value of B(2a) and B(2b) less the expected value of the pairs assigned in A(µ) and present in stage two, since the pairs assigned in B(2a) and B(2b) are subsets of the stage two pairs assigned in A(µ).





E

Br(2p)Srp ≥

r∈R,p∈P

βv(µ) (1 + β)µ EX∼Binom( (1+β)µ , 1+ββ ) [min (X, βµ )]
βv(µ) + (1 + β)µ EX∼Binom(µ,β) [min (X, βµ )] . − β2 v(µ).
1+β

By the same Birkhoﬀ-von Neumann argument as used in stage one, there exists a valid ﬁnal assignment

in stage two with paper loads of at most 1, reviewer loads of at most 1, and value at least

1 βµ

of the value

of B(2).

Total: Sum the total value of the 1-load assignment in both stages and divide by (1 + β)n to get a lower bound on the expected mean similarity:

s(µ) (1 +µβ)µ EX∼Binom( (1+β)µ , 1+1β ) min Xµ , 1

βµ

+

(1 + β)µ

EX ∼B inom(

(1+β)µ

,

β 1+β

)

min

X ,1
βµ

β + (1 + β) EX∼Binom(µ,β) min

X ,1
βµ

β2µ −
(1 + β) βµ

≥ s(µ)

µ (1 + β)µ

EX∼Binom( (1+β)µ , 1+1β ) min

X ,1
µ

+ β EX∼Binom( (1+β)µ , 1+ββ ) min

X ,1
βµ

+ EX∼Binom(µ,β) min X , 1 − 1 . (2) βµ

Since the above bound is a function of the binomial pmf, we search for a simpler approximation. Say that X ∼ Binom(N, p) and q = 1 − p. The above bound is a function of E min X , 1 for three binomial
random variables where N p ≤ . We approximate these binomials as if they were normals Z ∼ N (N p, N pq),

28

since X√−NNpqp converges in distribution to a standard normal. We use fZ as the pdf of Z, FZ as the cdf of Z, and Φ as the standard normal cdf.

X

Z

E min , 1 ≈ E min , 1

Z

Z

= E min , 1 |Z ≤ P [Z ≤ ] + E min , 1 |Z > P [Z > ]

= 1 N p − N pq fZ ( ) FZ ( ) + 1 − FZ ( ) FZ ( )

N pq

Np

= 1 − fZ ( ) − FZ ( ) 1 −

≥1− ≥1−

q

− Np

−Φ √

2πN p

N pq

q

Np

− 1−

.

2πN p

Np 1−

In total, deﬁning + = βµ − βµ, − = βµ − βµ , and = + + −, this approximation to the lower bound gives

s(µ)

µ

(1 + β)µ

 (1+β)µ 

(1 + β)µ

1 − β − Φ  µ − 1+β  1 −

2π (1 + β)µ

(1+β)µ β

(1 + β)µ

(1+β)2

+β 1−

1 −
2π (1 + β)µ β

1−β 2πµβ

 (1+β)µ β  β (1 + β)µ

βµ − βµ

 βµ

−Φ  βµ − 1+β  1 − − Φ

1−



(3)

(1+β)µ β

βµ (1 + β)

(1 − β)βµ

βµ

(1+β)2

≥ s(µ) 1 − 2

β

1

β(1 − β)

−

(1 + β) 2π (1 + β)µ (1 + β) 2πµ

1 + 2β

β (1 + β)µ

−

1−

1+β

βµ (1 + β)

(1 + β)µ (1 + β)µ

≥ s(µ) 1 −

β 2

2π(1 + β) (1 + β)µ

(1 + 2β) −
(1 + β) βµ

β −+ + 1+β

1 + 1−β
1+β
+
1− (1 + β)µ

≥ s(µ) 1 −

β 2

2π(1 + β) (1 + β)µ

(1 + 2β) −
(1 + β) βµ

1− (1 + β)µ

1 +
1+β .

1−β

Via simulation, we conﬁrm that the approximation in (3) is in fact a lower bound on the expression in (2) for all µ ∈ [104] and β ∈ 1010 , . . . , 110000 . In Figure 2 of Section 7, we plot the more precise bound of (3).

D.5.1 Proof of Lemma 2 It remains to prove Lemma 2. We ﬁrst prove a supplementary lemma, from which the main lemma follows.

29

Lemma 3. Consider a set N of N items and a submodular function g : 2N → R. Then, EA∼Ip(N )[g(A)] ≤ EA∼UpN (N )[g(A)].
Proof. Consider the following randomized procedure h, which takes in a set D ⊆ N and constructs a set containing exactly pN items. If |D| = pN , return h(D) = D. If x = |D| − pN > 0, then choose a subset B ⊆ D uniformly at random such that |B| = x and return h(D) = D \ B. If x = pN − |D| > 0, choose a subset C ⊆ N \ D uniformly at random such that |C| = x and return h(D) = D ∪ C.
If D ∼ Ip(N ) then h(D) ∼ UpN (N ), since all subsets of size pN have an equal chance to be created. We will show that ED∼Ip(N )[g(h(D)) − g(D)] ≥ 0, proving that ED∼Ip(N )[g(D)] ≤ EA∼UpN (N )[g(A)]. More speciﬁcally, we show that for each x > 0,

ED∼Ip(N )[g(h(D)) − g(D) | |D| = pN + x] + ED∼Ip(N )[g(h(D)) − g(D) | |D| = pN − x] ≥ 0.
Since g is submodular, for any subsets A ⊆ N , C ⊆ A, B ⊆ N \A, we have that g((A\C)∪B)−g(A\C) ≥ g(A ∪ B) − g(A).

ED∼Ip(N )[g(h(D)) − g(D) | |D| = pN + x]

+ ED∼Ip(N )[g(h(D)) − g(D) | |D| = pN − x]

1 = N pN +x

g(D \ B) − g(D)

pN +x

x D⊆N :|D|=pN +x B⊆D:|B|=x

1 + N N −pN +x

g(D ∪ C) − g(D)

(4)

pN −x

x

D⊆N :|D|=pN −x C⊆N \D:|C|=x

1 = N N −pN

g(A) − g(A ∪ B)

pN

x A⊆N :|A|=pN B⊆N \A:|B|=x

1

+ N pN

g(A) − g(A \ C)

(5)

pN x A⊆N :|A|=pN C⊆A:|C|=x

1 = N pN N −pN

pN x

x

g(A) − g(A ∪ B) + g(A) − g(A \ C)

A⊆N :|A|=pN B⊆N \A:|B|=x C⊆A:|C|=x

1 = N pN N −pN

pN x

x

g(A) − g(A ∪ B) + g((A \ C) ∪ B) − g(A \ C)

(6)

A⊆N :|A|=pN B⊆N \A:|B|=x C⊆A:|C|=x

≥ 0.

(4) writes out the expected value as a sum over all choices of D and all sets sampled by the procedure h. (5) rewrites the sums using A = D \ B and A = D ∪ C; each choice of D, B in the original sum corresponds to exactly one choice of A, B in the new sum. (6) re-arranges the sum to exchange each g(A) term for a g((A \ C) ∪ B) term; in both cases each set of size pN is counted pxN N−xpN times in the sum (exactly once for each choice of B, C).
We also show that f and Q are submodular.
Proposition 2. Q (R2, P2) is submodular in R2 and P2. Further, f is submodular in R2.
Proof. Note that maxA∈M (R ,P ;1,1) r∈R ,p∈P ArpSrp is a submodular function of the reviewer set R when the paper set P is held ﬁxed and of the paper set P when the reviewer set is held ﬁxed [60]. Submodularity in R2 is equivalent to submodularity in R1 = R \ R2, so Q (R2, P2) is submodular in R2 and P2. As a sum over terms submodular in R2, f is submodular in R2.

30

We now prove the main lemma. Since S ≥ 0, there exists a maximum-similarity assignment from within M (R , P ; 1, 1) that meets all paper load constraints with equality when |R | ≥ |P |, and thus is contained in M(R , P ; 1, 1). Also, M(R , P ; 1, 1) ⊆ M (R , P ; 1, 1). Thus, when |R2| ≥ βn and m − |R2| ≥ n, Q(R2, P2) = Q (R2, P2). Further, by Proposition 2, Q is submodular in P2. Therefore, by Lemma 3, f (R2) ≥ f (R2) whenever |R2| = 1+ββ m (since m ≥ (1 + β)n). This shows that
ER2∼U(β/(1+β))m(R)[f (R2)] ≥ E [f R2∼U(β/(1+β))m(R) (R2)].
By Proposition 2, f is submodular in R2. Therefore, by Lemma 3,
E [f R2∼U(β/(1+β))m(R) (R2)] ≥ ER2∼Iβ/(1+β)(R) [f (R2)] .

D.6 Proof of Theorem 6

In this section, we state and prove a more general version of the bound in Theorem 6 that does not require that µ4 be integral. This result immediately implies the result of Theorem 6.

Theorem 6 (Generalized). Suppose β = 1, and consider any µ ∈ [10, 000]. Deﬁne = µ4 − µ4 . Suppose there exists an assignment A(1) ∈ M(R, P; 1, 2) with mean similarity s(1). Suppose there also exists an
assignment A(µ) ∈ M(R, P; µ, 2µ) with mean similarity s(µ) that does not contain any of the pairs assigned
in A(1). Then, choosing R2 via random split gives that

√√

3 (1) s(µ)

7+ 6 3

ER2

[f (R2)]

≥

s 4

+ 4

1 − 2√πµ − µ/4

.

Proof. We attempt to construct an assignment in each stage in two rounds. We ﬁrst match all available pairs from A(1) (tiebreaking randomly between the two reviewers if both are available), and then attempt to construct a larger assignment from A(µ).
By Lemma 2, we can consider drawing P2 ∼ Iβ(P) and R2 ∼ Iβ/(1+β)(R) and allowing papers to be

underloaded. For all reviewers r ∈ R, deﬁne the random variables Zr = 1 w.p. 1/2 2 w.p. 1/2

representing the

stage that reviewer r is randomly chosen to be in. For each pair of reviewers (i, j) that are matched to the

same paper in A(1), deﬁne the random variables Fij = i w.p. 1/2 j w.p. 1/2

representing the reviewer that will be

assigned in round one if both are in the same stage. All of these random variables are independent. Deﬁne the total similarity value of the assignments as v(1) = 2ns(1) and v(µ) = 2nµs(µ). For A(µ), deﬁne the total similarity value assigned to paper p and reviewer r respectively as vp(µ) and vr(µ).

Round One: We ﬁrst match all available pairs from A(1). For any paper p ∈ P, call a, b the two reviewers

assigned to p by A(1). The value assigned to paper p across both stages is represented by a random

variable

Vp

= I[Za

= Zb](Sap + Sbp) + I[Za

= Zb](SapI[Fab

= a] + SbpI[Fab

= b]).

E[Vp] =

3 4

(Sap

+

Sbp),

so

E[ p∈P Vp] = 34 v(1) is the total expected value assigned in round 1.

Round Two: Fixing the round one assignments, we now attempt to ﬁnd a matching for all remaining papers and reviewers by matching pairs from within A(µ). We ﬁrst attempt to ﬁnd an assignment with paper and reviewer loads of at most θ = µ/4 among the remaining reviewers and papers in each stage. We start with the pairs from A(µ) that both are present in this stage and were not matched in round one, and randomly drop entries from each reviewer and paper until they are no longer overloaded. This argument mirrors the one made in the proof of Theorem 5.
We consider stage one without loss of generality. We start by constructing an assignment C to include all pairs assigned in A(µ) where the reviewer and paper both were unmatched in round one and are in stage one. Each reviewer-paper pair in A(µ) can be assigned in C with probability 312 , so E r∈R,p∈P CrpSrp = v3(µ2) .

31

We then construct an assignment B(1a) from C by removing assigned reviewers from each paper at random until each paper has load at most θ. Fix some paper p, and deﬁne Wp as the event that paper p was not assigned in round one. The number of reviewers assigned to p in A(µ) that are in stage one and not assigned in round one is a Binom(2µ, 1/8) random variable. The expected value assigned to p in this assignment is (using f as the Binomial pmf),

θ

x 2µ

θ

Br(1pa)Srp Wp =

f

(x

;

2µ,

1/

8)v

(µ) p

+

f

(x

;

2µ,

1

/8)

v

(µ) p

E

2µ

2µ

r∈R

x=0

x=θ+1

vp(µ) = 2µ EX∼Binom(2µ,1/8) [min(X, θ)] .

Summing over all papers, since each paper has a 1/4 change of being unmatched in round one,





(1a)

v(µ)

E

Brp Srp = 8µ EX∼Binom(2µ,1/8) [min(X, θ)] .

p∈P r∈R

We separately construct an assignment B(1b) from C by removing assigned papers from each reviewer
at random until each reviewer has load at most θ. Fix some reviewer r, and deﬁne Wr as the event that reviewer r was not assigned in round one. The number of papers assigned to r in A(µ) that are not assigned
in round one is a Binom(µ, 1/4) random variable. The expected value assigned to r in this assignment is,





θ

xµ

θ

Br(1pb)Srp Wr =

f

(

x;

µ,

1/

4)v

(µ) r

+

f

(x

;

µ,

1/4)

v

(µ) r

E

µ

µ

p∈P

x=0

x=θ+1

vr(µ) = µ EX∼Binom(µ,1/4) [min(X, θ)] .

Summing over all reviewers, since each reviewer has a 1/8 change of being both unmatched in round one and present in stage one,





(1b)

v(µ)

E

Brp Srp = 8µ EX∼Binom(µ,1/4) [min(X, θ)] .

r∈R p∈P

Finally, we construct B(1) to include all pairs assigned in both B(1a) and B(1b). It has value at least equal to the total value of B(1a) and B(1b) less the value of C, since the assigned pairs in B(1a) and B(1b) are
subsets of the assigned pairs in C.





E

Br(1p)Srp

r∈R,p∈P

v(µ)

µ

≥ 8µ EX∼Binom(2µ,1/8) [min(X, θ)] + EX∼Binom(µ,1/4) [min(X, θ)] − 4 .

By construction this assignment has paper loads of at most θ and reviewer loads of at most θ (among all

reviewers and papers unmatched in round one and present in stage one).

By a generalization of the Birkhoﬀ-von Neumann theorem [66], there exists an assignment with paper

loads of at most 1 and reviewer loads of at most 1 among all reviewers and papers unmatched in round one

and

present

in

stage

one,

with

value

at

least

1 θ

of

the

value

of

B(1).

Totalling

over

both

stages

and

dividing

by 2n, the round two assignments contribute at least

s(µ)

X

X

µ

min , 1 + EX∼Binom(µ,1/4) min , 1 −

(7)

4 EX∼Binom(2µ,1/8)

θ

θ

4θ

32

to the mean assignment value. If X ∼ Binom(N, p), the above bound is a function of E min X , 1 for two binomial random variables
where N p ≤ . Using the normal approximation presented in the proof of Theorem 5, we get the following
approximation to the above bound (deﬁning = µ/4 − (µ/4)):

 √√





 

s(µ)

7+ 6

µ/4

1 − √ − 1 −

Φ 

+Φ

 + 1

(8)

4

2 πµ

µ/4

7µ

3µ

32

16

√√

s(µ)

7+ 6

µ/4

≥ 4 1 − 2√πµ − 3 1 − µ/4

√√

s(µ)

7+ 6 3

= 4 1 − 2√πµ − µ/4 .

Via simulation, we conﬁrm that the approximation in (8) is in fact a lower bound on the expression in (7) for all µ ∈ [104]. In Figure 2 of Section 7, we plot the more precise bound of (8).

33

