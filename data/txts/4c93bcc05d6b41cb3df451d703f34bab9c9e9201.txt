Research Challenges in Designing
Di erentially Private Text Generation Mechanisms
Oluwaseyi Feyisetan, Abhinav Aggarwal, Zekun Xu, Nathanael Teissier Amazon
{sey,aggabhin,zeku,na eis}@amazon.com
Abstract Accurately learning from user data while ensuring quanti able privacy guarantees provides an opportunity to build be er Machine Learning (ML) models while maintaining user trust. Recent literature has demonstrated the applicability of a generalized form of Di erential Privacy to provide guarantees over text queries. Such mechanisms add privacy preserving noise to vectorial representations of text in high dimension and return a text based projection of the noisy vectors. However, these mechanisms are sub-optimal in their trade-o between privacy and utility. is is due to factors such as a xed global sensitivity which leads to too much noise added in dense spaces while simultaneously guaranteeing protection for sensitive outliers. In this proposal paper, we describe some challenges in balancing the tradeo between privacy and utility for these di erentially private text mechanisms. At a high level, we provide two proposals: (1) a framework called which defers some of the noise to a privacy ampli cation step and (2), an additional suite of three di erent techniques for calibrating the noise based on the local region around a word. Our objective in this paper is not to evaluate a single solution but to further the conversation on these challenges and chart pathways for building be er mechanisms.
1 Introduction
Privacy has emerged as a topic of strategic consequence across all computational elds – from machine learning, to natural language processing and statistics. Whether it is to satisfy compliance regulations, or build trust among customers, there is a general consensus about the need to provide privacy guarantees to users whose datasets serve as inputs to arbitrary functions provided by external processors. Within the mathematical and statistical disciplines, Di erential Privacy [DMNS06] has emerged as a gold standard for evaluating theoretical privacy claims. At a high level, a randomized algorithm is di erentially private if its output distribution is similar when the algorithm runs on two neighboring input databases. e notion of similarity is controlled by a parameter ε ≥ 0 that de nes the strength of the privacy guarantee. Similarly, it is possible to train di erentially private deep learning [SS15, ACG+16] models by extending the methods from the statistical literature to the universal function approximators in neural networks. However, while Di erential Privacy (DP) comes with strong theoretical guarantees, and the literature around the eld is quite mature, designing di erentially private mechanisms for generating text is less studied.
As a result, within the eld of traditional and computational linguistics, the norm is to apply anonymization techniques such as k-anonymity [Swe02] and its variants. While this o ers a more intuitive way of expressing privacy guarantees as a function of an aggregation parameter k, all such methods are provably non-private [KKMN09]. Nevertheless, recent works such as [FDM19, FDD19, FBDD20] have a empted to directly adapt the methods of DP to Natural Language Processing (NLP) by borrowing ideas from the
1

privacy methods used for location data [ABCP13]. In DP, one way privacy is a ained by adding ‘properly calibrated noise’ to the output of a mechanism [DMNS06], or to gradient computations for deep learning [ACG+16]. e premise of such ‘DP for text’ methods is predicated on adding noise to the vector representation of words in a high dimensional embedding space, and projecting the noisy vectors back to the discrete vocabulary space.
Unlike statistical queries however, language generation comes with a unique set of problems. Consider a simple counting query where the objective is to return the number of people who exhibit a certain property x. e sensitivity of such a query is 1 since a new individual can only increase the count by 1. With text however, the sensitivity is much larger and is driven by the richness of the vocabulary, and how it is represented in the metric space under consideration. In this paper we propose strategies for increasing the utility of these ’DP for text’ mechanisms by reducing the noise required while maintaining the desired privacy guarantees.
2 Privacy Implications and reat Model
We consider a system where users generate training data (as text) which is then made available to an analyst. e analyst’s utility requirement is to assess the quality of a downstream metric (e.g., ML model accuracy) derived from this data. e analyst therefore requires clear form access to the input data (e.g., for augmentation, aggregation, or annotation) to continuously improve downstream utility. In this model however – akin to a modular unit in the overall threat vector of [KMA+19] and speci c model of [FBDD20], it is possible that the analyst learns more information about the user e.g., their identity, or some property [WE18], than is required to play their role of improving the utility metric. An example where textual data was used for re-identi cation can be seen with the AOL data release [BZH06].
3 Challenges in Designing Private Text
Consider a set of n users, each with data xi ∈ X . Each user wishes to release up to m messages in a privacy preserving manner while maximizing the utility gained from the release of the messages. One approach is for each user to submit their messages (xi,1 , . . . , xi,m ) in clear form to a trusted curator. e curator then proceeds to apply a privacy preserving randomized mechanism R(∗) to the analysis A(x) on the aggregated data. e privacy mechanism works by injecting noise to the results of the analysis. is technique corresponds to the curator model of DP [DMNS06], however, it requires that the users trust the curator. is is the proposed approach for preserving privacy in the upcoming U.S. Census [Abo18]. e curator model results in high utility since noise is applied only once on the aggregated data; however, a parallel approach cannot be clearly drawn for private text synthesis.
Another theoretical approach is for each user to apply the encoding or randomizing mechanism R : X → Ym to their data. e resulting n · m messages (yi,1 , . . . , yi,m ) = R(xi) for each user is then passed to the curator for analysis A : Y∗ → Z. is corresponds to Local DP (LDP) [KLN+11], since each user randomizes their data locally. e model provides stronger privacy guarantees in the presence of an untrusted curator. However, it incurs more error than the curator model because it requires multiple local R(xi) transformations (as opposed to one by the trusted curator). As a result, it has mainly been successfully adopted by companies with large user bases (such as Microso [DKY17], Google [EPK14], and Apple [Tea17]) which compensate for the error. e local model is more amenable for text [FDM19] and the literature builds on this framework.
2

e error accrued in the local model is exacerbated by the output range of the randomization function R(xi). As an example, for one-bit messages (e.g., a coin ip) where f : X → {0, 1}, the overall error goes down faster as the number of users increase, given the small output size of 2. Using a die roll with 6 outputs, the noise smooths out a bit slower. However, for analysis over vector representations of words f : X → Rd, where d is the dimensionality of a word embedding model, and the number of words in the vocabulary could exceed thousands, the resulting analysis leads to far more noisy outputs. e noise (and by extension, the error) increases because of the DP promise, i.e., to guarantee privacy and protect all outliers, there must be a non-zero probability for transforming any given x to any other x . We loosely correlate this size of the output space with the sensitivity of the function f . erefore, when the sensitivity is large, more noise is required to preserve privacy.
e challenge with designing privacy mechanisms for text stems from these aforementioned issues. We observe that unlike the natural distribution of values over the number line, the vector representation of words in an embedding space tends to be non-uniform. e distance between words carries information as to their semantic similarities, and as a result, there are sparse regions and dense regions. Conversely, the privacy guarantees from di erential privacy extends to every word in the entire space (leading to the large noise required to ascertain worst-case protections). is problem is not unique to the text space, however, it has been be er studied in the statistical privacy literature. For example, the theoretical sensitivity for computing the median of an arbitrary set of numbers is in nite, but, in most dataset scenarios, the sensitivity is smaller as values coalesce around the median [NRS07]. Similar considerations have also been explored in private release of graph statistics [BBDS13, KNRS13].
In this exploratory paper we examine these challenges from di erent lenses:
1. Can we reduce the noise by deferring additional privacy guarantees to other ampli cation mechanisms that do not require noise (e.g., sub-sampling, shu ing, k-aggregation have all been proposed in the literature [LQS12, BEM+17]);
2. Can we re-calibrate the noise added such that it varies for every word depending on the density of the space surrounding the current word – rather than resorting to a single global sensitivity?
To address (1), we propose framing the private data release problem within the central DP [EFM+19] paradigm by recommending a generalized form of the ESA protocol of [BEM+17] which we denote as LAC. For (2), we propose three di erent methods that can be adopted to directly reduce the noise: density modulated noise, calibrating the noise to data sensitivity, and truncating the noise using a variety of approaches.
We now give some preliminaries before expounding into the details of our proposals.

4 Preliminaries and Current Methods

Let X n be a collection of datasets from users. e Hamming distance dH (x, x ) between two datasets

x, x ∈ X n is the number of entries on which x and x di er i.e., dH (x, x ) = |{i : xi = xi}| =

|X | i=1

|xi

−

xi|. e datasets x, x ∈ X are adjacent (we denote this as x ∼ x ) if dH (x, x ) = 1.

De nition 1 (Di erential Privacy [DMNS06]). A randomized algorithm A : X n → Z is ε-di erentially private if for every pair of adjacent datasets x ∼ x ∈ X n and every Z ⊆ Range(A),

Pr[A(x) ∈ Z] ≤ eεPr[A(x ) ∈ Z].

3

Figure 1: e privacy mechanism consists of a local randomizer which adds the noise, a privacy ampli cation module and a curator that aggregates the data.
A DP algorithm protects a user by ensuring that its output distribution is approximately the same, whether or not the user was in the dataset used as an input to the algorithm. DP is usually achieved by applying noise drawn from a Laplace distribution scaled by the sensitivity of the analysis function.
Several pieces of research have demonstrated generalized DP (also known as dX privacy) for di erent metric spaces and distance functions [CABP13, ABCP13, CPS15, FBDD20, FDM19, FDD19]. For example, [CABP13] demonstrated how the Manha an distance metric was used to preserve privacy when releasing the number of days from a reference point. Similarly, the Chebyshev metric (chessboard distance) was adapted to perturb the output of smart meter readings [WE18, CABP13] providing privacy with respect to TV channels being viewed. Further, the Euclidean distance was utilized by [ABCP13, CPS15] in a 2 dimensional coordinate system to privately report the location of users, and nally, [FDM19] applied the Wasserstein metric in higher dimensions to demonstrate privacy preserving textual analysis using the metric space realized by word embeddings.
is work focuses on preserving privacy in high dimensional metric spaces equipped with the Euclidean metric. To achieve this form of metric di erential privacy (dX privacy), using a corollary to the Laplace mechanism, noise is sampled from an n−dimensional Laplacian and added to the output of the desired mechanism.
5 Proposal 1: Deferred Ampli cation
Our mechanism starts with a protocol similar to the privacy strategy of the local model. Given a set of n users, each with m data submissions xi ∈ X . Each user applies the dX privacy mechanism L : X → Ym to their data. e resulting n · m messages (yi,1 , . . . , yi,m ) = L(xi) for each user is then passed to the curator C : Y∗ → Z.
Our proposal includes an additional step that ampli es the privacy guarantees. Between the local noise injection L(x) and the curator analysis C(y), we introduce a privacy ampli cation step A : Y∗ → Y∗ which takes in the result of the message perturbations from all the users A(∪ni=1L(xi)), ampli es the privacy and outputs it to the curator.
To get an intuition on how can be used to improve utility while preserve privacy, consider the standard randomized response of [War65]. Given a bit b ∈ {0, 1} and privacy parameter ε. To output a privatized bit ˆb, we set ˆb = b with probability p = 1+eεeε , otherwise ˆb = 1 − b. To improve the utility of this mechanism, we need to increase ε. However, in the local model, an adversary can map the output {bˆ1, . . . , bˆn} to the n corresponding users. erefore, the parameter p has to be close to 21 otherwise ˆb ≈ b and the privacy guarantees are meaningless. us, to maintain the original (privacy) guarantees (while improving the utility), we need an additional mechanism that’s di erent from the bit ipping noise addition. e desired property is such that the privacy guarantees are still meaningful when p 21 .
In building composite DP algorithms, tools for privacy ampli cation are used to design mechanisms
4

Algorithm 1: Composite privacy mechanism
// Localizer Input: word w ∈ W, parameters m, for each n users Output: word wˆ ∈ W for i ∈ {1, . . . , m} do
Noise η ∼ Lap(∆f /ε) φˆ = φ(w) + η release wˆ
// Amplifier Input: Multiset {wˆi}i∈[n], outputs of randomizers Output: Multiset {wˆi}i∈[n], uniform permutes of [n] for i ∈ {n − 1, . . . , 1} do
j ← random integer such that 0 ≤ j ≤ i exchange wi and wj release {w}
// Curator Input: Multiset {yi}i∈[n], with yi ∈ Y Compute z = A(y) release z
that provide additional guarantees than the initial privacy protocol. Probably the most studied technique is privacy ‘ampli cation by sub-sampling’ [CM06, KLN+11, BBG18],
which states in its basic form that an ε-DP mechanism applied to a q fraction sub-sample of the initial population, yields an ε -DP mechanism, where ε ≈ qε. Other approaches such as [LQS12] and [FDBD19] have proposed augmenting sub-sampling with a k−anonymity parameter. Another class of ampli cation is by contractive iteration [FMTT18] for privacy preserving ML models.
5.1 Ampli cation Model Spotlight: e Shu ler
In this work, we highlight the shu e mechanism [BEM+17, EFM+19, CSU+19, BBGN19] to amplify the privacy guarantees. While shu ing on its own o ers no DP guarantees (unlike sub-sampling, which does), when combined with LDP, it has the advantage of maintaining the underlying statistics of the dataset by not ‘throwing away’ any of the data. e shu er de-links data by masking its origin and confounding its provenance. For shu ing to be a viable ampli cation model, the Analyzer and Randomizer outputs must be amenable to shu ing, and not rely on any discriminating characteristics that link an individual to their contributions.
e pseudo-code in Alg. 1 provides a high level overview of the composite privacy mechanism using a shu er. Each user contributes their data which passes through a local privacy randomizer. e noisy outputs are then passed to a shu er which permutes the order of the source of the perturbed data. e overall protocol P, thus, consists of (L, A, C) and is modeled around the Encode, Shu e, Analyze ( ) architecture of [BEM+17].
In principle, shu ing can be implemented via multi-party computation (MPC), mixnets, running the shu er on secure hardware or via a trusted third party [MSZ15, CSU+19, BEM+17, ADK+19].
5

5.2 Selecting a Privacy Ampli cation Model
We provide some high level proposals: Shu ler: can be used to generate text that’s fed into linear classi ers with high utility. For example, a mechanism that outputs a sentiment class based on private perturbed data can still yield high utility on user de-linked and shu ed data. Sub-sampler: For other use cases such as personalization which require some form of user linked data, a sub-sampler can be used instead of a shu er. is will be more suitable if the data is reasonably uniform (without outliers). K-threshold: with randomized sub-sampling can be used for cases where the underlying data follows a long tail distribution such as for annotating data in crowdsourcing or training generalized ML models with user data.

6 Proposal 2: Improved Randomizers
e randomizer R is based on the dX metric privacy mechanism described by [FBDD20] on word embeddings where the distance between word vectors is represented as the Euclidean metric. Alg. 2 presents
an overview of that mechanism. A similar mechanism was also proposed by [FDM19], however, the dis-
tance metric was the Earth mover distance. Similarly, [FDD19] extended the model to demonstrate pre-
serving privacy using noise sampled from Hyperbolic space. e metric space of interest is as de ned
by word embedding models which organize discrete words in a continuous space such that the similarity in the space re ects their semantic a nity. Models such as W 2V [MSC+13], G V [PSM14], and T [BGJM17] create such a mapping φ : W → Rd, where the distance function is expressed as d : W ×W → [0, ∞). e distance d(w, w ) between a pair of words is therefore given as φ(w)−φ(w ) , where · is the Euclidean norm on Rd.

Algorithm 2: Privacy Mechanism of [FBDD20]

Input: string x = w1w2 · · · w , privacy param ε > 0 for i ∈ {1, . . . , } do
Word embedding φi = φ(wi)
Sample noise N with density pN (z) ∝ exp(−ε z Discretization wˆi = argminu∈W φ(u) − φˆi Insert wˆi in ith position of xˆ.

). Perturb embedding with noise φˆi = φi + N .

release xˆ

is mechanism however leads to sub-optimal accuracies due to a lack of uniformity in the embedding space. In particular, to achieve a certain level of privacy protection, the amount of noise is controlled by the worst-case word, which roughly corresponds to the word whose embedding is farther apart from any other word (i.e., the global sensitivity). erefore, at a given level of ε, a unique word like nudiustertian will be perturbed similarly to a common word like drunk which has over 2, 000 possible synonyms1. To improve on this, we propose a variation of the original mechanism that can provide a xed level of plausible deniability [BSG17], measured in terms of the proxy statistics of [FBDD20] with less noise, thus yielding more accuracy. In other words, the improved mechanisms should provide the same level of plausible deniability as the original mechanism, but under a larger value of ε. To achieve this goal, we propose three di erent strategies:
1https://www.mhpbooks.com/books/drunk/

6

1. De ning a prior over the word embeddings to account for the space variability.
2. Calibrating the injected noise to the local sensitivity of the metric space.
3. Adopting a truncated noise mechanism within an admissible region.
6.1 Density-Modulated Noise
We observe that Alg. 2 can be interpreted as an instance of the exponential mechanism [MT07] together with a post-processing step (Alg. 2: Line 5). Further, noise sampling via the exponential mechanism assumes a base measure µ(z) with a uniform distribution over the feasible range. Accordingly, line 3 of Alg. 2 can be expanded as pN (z) ∝ µ(z) × exp(−ε z ). However, the distribution of words in Rd is not uniform over the embedding space. As a consequence of Zipf’s law, some words occur more frequently in a dataset and are surrounded by dense regions of similar words in the embedding space [Gab99].
A natural way to “bias” an exponential mechanism without changing its privacy properties is to modulate it with a public “prior” µ(z). For example, such a prior can be constructed over a publicly available corpus such as Wikipedia or Common Crawl. e question we address in this section is whether we can design an appropriate, potentially unnormalized, prior such that the resulting exponential mechanism that samples from pN (z) ∝ µ(z) × exp(−ε z ) provides more accurate answers than the original mechanism under similar privacy constraints. An important research challenge in this direction is that by incorporating this correction to improve accuracy, we might end up with a mechanism that is computationally hard to sample from.
To obtain a prior that will solve the non-uniformity in the privacy mechanism using a vanilla word embedding is to modulate the distribution by a prior that captures the distribution of words in Rd induced by the word embedding. By introducing a prior that assigns high probability to dense areas of the embedding and low probability to sparse areas of the embedding, we can achieve the same level of plausible deniability statistics with smaller values of ε, hence, mitigating the worst-case e ect that is observed in the unmodulated mechanism around sparse areas of the embedding.
One way to produce this prior measure µ(z) is to take a kernel density estimator with Radial Basis Function kernels on the resulting embedding, i.e., µ(z) ∝ u∈W exp − z − φ(u) 2/2σ2 for some tuned variance σ2. However, it is not immediately clear how to sample from the modulated mechanism that on input w has density pN (z) ∝ µ(z) × exp(−ε z ) for µ(z) de ned above. Rather than sampling directly, we can either opt for an approximation to the distribution, or adopt indirect sampling strategies such as the Metropolis–Hastings algorithm [MRR+53, Has70].
Another observation is that we don’t need to pay the cost of an expensive sampling every time we want to use the mechanism. Instead, by introducing the projection step of the sampled vector to the closest word embedding, we can represent the mechanism by a W × W matrix containing the probabilities Pr[M (w) = w ], where M is the complete mechanism. We can precompute and store these ||W ||2 probabilities and then use this matrix to de ne the output distribution every time we run the mechanism.
However, even though we can potentially make accuracy gains by incorporating the prior µ that captures the distribution of words in Rd, we need to assume that the a acker is an informed adversary [DMNS06]. Consequently, given that the adversary knows the prior (e.g., since the word embeddings are public), the a acker’s objective is to determine the user’s actual word w, given the output w of the mechanism R. e probability that w is the word that generated w is given by the posterior probability distribution:
7

Pr(w|w ) =

µ(x) · Pr[R(w) = w ] .
w∈W µ(x) · Pr[R(w) = w ]

e privacy objective is to protect against a Bayesian adversary that can perform an optimal inference a ack [STT+12] of the form:

wˆ = argmin Pr(w|w )d(φ(wˆ), φ(w))
wˆ∈W w∈W
where word wˆ is the adversary’s inference given the output w of the randomizer, when the original word is w, and d is the Euclidean distance between the word vectors.

6.2 Calibrating Noise to Data Sensitivity
In the proof of [FBDD20], wˆ is calibrated at the worst-case distance T from w and w which is analogous to the global sensitivity. We can however, have a data dependent sensitivity de nition over the metric space:

De nition 2 (Local sensitivity [NRS07]). x ∼ x ∈ X as,

e local sensitivity of a function f : X n → Rd is given for

∆Lf = max f (x) − f (x ) 1
x :d(x,x )=1

e local sensitivity of f with respect to x is how much f (x ) can di er from f (x) for any x adjacent to the input x (and not any possible entry x). We observe that the global sensitivity ∆Gf = maxx ∆Lf (x). However, a mechanism that adds noise scaled to the local sensitivity does not preserve DP as the noise
magnitude can leak information [NRS07]. To address this, for example, [NRS07] adds noise calibrated to a smooth bound on the local sensitivity. e noise is typically sampled from the Laplace distribution. us, if we consider wˆ at a distance 0 < t < T , then the local sensitivity ∆Lf is:

∆Lf (t) = w :dm(wa,wx )≤t ∆Lf . (1)

However, for our rare word w =nudiustertian, the local sensitivity might still leak information on output wˆ. As a result, we can construct the smooth sensitivity ∆Sf as a β−smooth upper bound [NRS07] on the

local sensitivity. e desired properties of the bound include that:

(1) ∀w ∈ W :

∆Sf (w) ≥ ∆Lf (w)

(2) ∀w, w ∈ W :

∆Sf (w) ≤ eβ · ∆Sf (w )

Observe that the smooth bound is equal to the global sensitivity ∆Gf when β = 0. erefore, the

smallest function ∆Sf∗,β that satis es the two stated properties is the smooth sensitivity of the underlying

function f and can be stated as:

∆Sf∗,β (w) = w :dm(wa,wx )≤t ∆Lf (w ) · e−βd(w,w )

However, we cannot describe the local and smooth sensitivity this way since the local sensitivity construction in Def 2 was de ned for integer-valued metrics (such as the Hamming distance). To translate this

8

to real-valued metrics as is required for dχ-privacy, we can adopt the approach of [LPP20] for de ning the local sensitivity in metric spaces.
First, we consider each word embedding vector as a point in some Banach space. A Banach space is a vector space with a metric that allows the computation of vector length and distance between vectors. For example, our n−dimensional Euclidean space of word embeddings, with the Euclidean norm is a Banach space.
Next, we observe from [KNRS13] that the local sensitivity of a function is similar to its derivative (e.g., taking the limits in Eqn 1 as t → 0). erefore, the aim is to nd an analog of a suitable derivative for continuous functions. One option is for the local sensitivity to be de ned as the Fre´chet derivative in Banach spaces. [LPP20] described an approach for this and they demonstrated how to apply noise sampled from the Cauchy distribution to satisfy the DP guarantees. Additional research would be needed to explore the direct application of the method of local (and then smooth) sensitivity calibration to embedding spaces.
Truncated Noise Mechanisms e standard dχ-privacy mechanisms were designed by borrowing ideas from the privacy methods used for location data [ABCP13]. One of the proposed approaches in that work was to truncate the mechanism to report only points within the limits of the area of interest. To achieve this, they de ne an ‘acceptable area’ of admissible points A ⊂ R2 (i.e., location privacy in 2 − d space) beyond which results are truncated to the closest point in A. Other truncation mechanisms have been explored in traditional DP including the truncated Laplacian [GDGK18], and truncated geometric mechanism within the context of dχ-privacy [CABP13].
Designing a corollary for text based dχ-privacy requires an approach to se ing the truncation bounds while maintaining the privacy guarantees. We identify 2 potential ways of achieving this: (1) Distance based truncation; and (2) K-nearest neighbor based truncation. To achieve (1) we can de ne a distance based limit similar to [ABCP13]. In this approach, a word can only get perturbed to words within the distance-de ned admissible area A ∈ U. e maximum distance τ between a word and the farthest word in A is de ned and xed a-priori. To handle words that fall outside the noise limits, [ABCP13] proposes a discretization step to select the closest word in A. Another option is for the mechanism to concentrate the probability of selecting a word to the admissible area A, while assigning a residual probability to satisfy the DP guarantee on the entire set U. erefore, when the noise exceeds the distance τ , a replacement is randomly drawn from the set U − A.
e downside of this approach is seen in regions of sharply varying density e.g., in the embedding space where one word has 2, 000 synonyms which potentially fall within A and the rare word with no neighbors in A. erefore to achieve (2) rather than having a xed distance from each word, we can also de ne the (randomized) k−closest words as delineating our acceptable area. One potential bene t of this is, in dense spaces, we can select closer candidates while simultaneously guaranteeing that isolated words are replaced by one of their k−nearest neighbors regardless of how far o it is.
Implementing either of these mechanisms however come with their own set of challenges. For example, there isn’t a direct way to set a maximum distance when drawing the multivariate laplacian noise that was proposed by [FBDD20]. One option will be to x τ , or the distance to the max randomized k as the local sensitivity described in Sec 6.2. Another option will be to rethink the entire design of the randomizers such that the noise is not added to the vector representation of the words, but to these τ distances. Connections to Related Work e traditional DP literature contains techniques to limit the privacy preserving noise added to a mechanism. Just as with the median example, these approaches take into consideration the actual dataset as opposed to the worst case guarantees of a possible theoretical construct. In one work, [NRS07] introduced the notion of smooth sensitivity where a smooth upper bound on the local
9

sensitivity is used to determine how much noise is added. ey demonstrated this method for privately computing the median of a dataset, and the number of triangles in a graph.
Similarly, [DL09] introduced a paradigm called Propose-Test-Release (PTR) where: the algorithm proposes a bound on sensitivity, tests the adequacy of the bound on the given dataset, and halts if the sensitivity is too high. e PTR technique is connected to robust statistics which is concerned with outliers and rounding errors in a dataset [DS10].
In related work, [KNRS13] extended the notion of limiting the noise for private graph analysis where the degree bound (a function of the number of nodes in the graph) can be arbitrary. To achieve this, they set a D bound on the graph which aims to keep the sensitivity low while retaining as large a fraction of the graph as possible. Similarly within the context of graphs (speci cally, social networks), [BBDS13] introduce the idea of restricted sensitivity without se ing a bound on the degree of the graph. Instead, they de ne a hypothesis (a subset of all possible datasets) over which they compute the sensitivity.
ese all describe principled approaches to limit the magnitude of noise applied to a privacy preserving mechanism in the contexts of statistical and graph analysis by rede ning the sensitivity that controls the noise. As opposed to the reviewed techniques, our representations are within a metric space de ned by word embeddings. e sensitivity is therefore described over the vocabulary of the embeddings. erefore, unlike statistical analysis where releasing noisy counts still contains informative value, adding noise to word representations, can result in a rapid degradation of utility.

7 Conclusion and Future work
In this proposal paper, we surveyed some of the challenges of building di erentially private mechanisms for generating text based on word embeddings. We investigated approaches built on the dχ-privacy framework in Euclidean space. e core issues stem from the non-uniformity of the metric space de ned by embeddings and the need to provide worst case guarantees for outliers as required by di erential privacy.
is necessitates a large amount of noise thus leading to utility impacts on downstream tasks that rely on the generated text as input features.
Our approach was to explore the resulting utility issues from di erent perspectives: rst, considering methods of reducing the required noise by deferring additional guarantees to other privacy ampli cation mechanisms that do not require noise (such as shu ing). We then proposed three ways to reduce the needed noise by accounting for the density around the word under consideration. ese included introducing a prior, re-calibrating the noise, or truncating the noise. In future work, we plan to explore these approaches in detail and provide a study on what works, when it works, and why. Our aim is to provide a principled approach to studying these mechanisms in order to accelerate the research and drive adoption.

References

[ABCP13] Miguel E Andre´s, Nicola´s E Bordenabe, Konstantinos Chatzikokolakis, and Catuscia Palamidessi. Geo-indistinguishability: Di erential privacy for location-based systems. In ACM CCS, pages 901–914. ACM, 2013.

[Abo18]

John M Abowd. e US census bureau adopts di erential privacy. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 2867– 2867. ACM, 2018.

10

[ACG+16] Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov, Kunal Talwar, and Li Zhang. Deep learning with di erential privacy. In ACM SIGSAC CCS, pages 308–318. ACM, 2016.

[ADK+19] Joshua Allen, Bolin Ding, Janardhan Kulkarni, Harsha Nori, Olga Ohrimenko, and Sergey Yekhanin. An algorithmic framework for di erentially private data analysis on trusted processors. In Advances in Neural Information Processing Systems, pages 13657–13668, 2019.

[BBDS13] Jeremiah Blocki, Avrim Blum, Anupam Da a, and Or She et. Di erentially private data analysis of social networks via restricted sensitivity. In ITCS, pages 87–96. ACM, 2013.

[BBG18]

Borja Balle, Gilles Barthe, and Marco Gaboardi. Privacy ampli cation by subsampling: Tight analyses via couplings and divergences. In Advances in Neural Information Processing Systems, pages 6277–6287, 2018.

[BBGN19] Borja Balle, James Bell, Adria` Gasco´n, and Kobbi Nissim. model. In CRYPTO, pages 638–667. Springer, 2019.

e privacy blanket of the shu e

[BEM+17] Andrea Bi au, U´ lfar Erlingsson, Petros Maniatis, Ilya Mironov, Ananth Raghunathan, David Lie, Mitch Rudominer, et al. Prochlo: Strong privacy for analytics in the crowd. In SOSP, pages 441–459. ACM, 2017.

[BGJM17] Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. Enriching word vectors with subword information. TACL, 5, 2017.

[BSG17]

Vincent Bindschaedler, Reza Shokri, and Carl A Gunter. Plausible deniability for privacypreserving data synthesis. VLDB Endowment, 10(5):481–492, 2017.

[BZH06] Michael Barbaro, Tom Zeller, and Saul Hansell. A face is exposed for AOL searcher no. 4417749. New York Times, 9(2008):8, 2006.

[CABP13] Konstantinos Chatzikokolakis, Miguel E Andre´s, Nicola´s Emilio Bordenabe, and Catuscia Palamidessi. Broadening the scope of di erential privacy using metrics. In PETS, 2013.

[CM06]

Kamalika Chaudhuri and Nina Mishra. When random sampling preserves privacy. In Annual International Cryptology Conference, pages 198–213. Springer, 2006.

[CPS15]

Konstantinos Chatzikokolakis, Catuscia Palamidessi, and Marco Stronati. Constructing elastic distinguishability metrics for location privacy. PETS, 2015.

[CSU+19]

Albert Cheu, Adam Smith, Jonathan Ullman, David Zeber, and Maxim Zhilyaev. Distributed di erential privacy via shu ing. In Annual International Conference on the eory and Applications of Cryptographic Techniques, pages 375–403. Springer, 2019.

[DKY17] Bolin Ding, Janardhan Kulkarni, and Sergey Yekhanin. Collecting telemetry data privately. In NeurIPS, 2017.

[DL09]

Cynthia Dwork and Jing Lei. Di erential privacy and robust statistics. In STOC, volume 9, pages 371–380, 2009.

11

[DMNS06] Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to sensitivity in private data analysis. In TCC, pages 265–284. Springer, 2006.

[DS10]

Cynthia Dwork and Adam Smith. Di erential privacy for statistics: What we know and what we want to learn. Journal of Privacy and Con dentiality, 1(2), 2010.

[EFM+19] U´ lfar Erlingsson, Vitaly Feldman, Ilya Mironov, Ananth Raghunathan, Kunal Talwar, and Abhradeep akurta. Ampli cation by shu ing: From local to central di erential privacy via anonymity. In ACM-SIAM, 2019.

[EPK14]

U´ lfar Erlingsson, Vasyl Pihur, and Aleksandra Korolova. Rappor: Randomized aggregatable privacy-preserving ordinal response. In ACM SIGSAC CCS, 2014.

[FBDD20] Oluwaseyi Feyisetan, Borja Balle, omas Drake, and Tom Diethe. Privacy- and utilitypreserving textual analysis via calibrated multivariate perturbations. In ACM WSDM, 2020.

[FDBD19] Oluwaseyi Feyisetan, omas Drake, Borja Balle, and Tom Diethe. Privacy-preserving active learning on sensitive data for user intent classi cation. arXiv preprint arXiv:1903.11112, 2019.

[FDD19] Oluwaseyi Feyisetan, Tom Diethe, and omas Drake. Leveraging hierarchical representations for preserving privacy and utility in text. In IEEE ICDM, 2019.

[FDM19] Natasha Fernandes, Mark Dras, and Annabelle McIver. Generalised di erential privacy for text document processing. Principles of Security and Trust, 2019.

[FMTT18] Vitaly Feldman, Ilya Mironov, Kunal Talwar, and Abhradeep akurta. Privacy ampli cation by iteration. In FOCS, pages 521–532. IEEE, 2018.

[Gab99]

Xavier Gabaix. Zipf’s law for cities: an explanation. 114(3):739–767, 1999.

e arterly journal of economics,

[GDGK18] an Geng, Wei Ding, Ruiqi Guo, and Sanjiv Kumar. Truncated laplacian mechanism for approximate di erential privacy. arXiv preprint arXiv:1810.00877, 2018.
[Has70] W Keith Hastings. Monte carlo sampling methods using markov chains and their applications. 1970.

[KKMN09] Aleksandra Korolova, Krishnaram Kenthapadi, Nina Mishra, and Alexandros Ntoulas. Releasing search queries and clicks privately. In WebConf. ACM, 2009.
[KLN+11] Shiva Kasiviswanathan, Homin Lee, Kobbi Nissim, Sofya Raskhodnikova, and Adam Smith. What can we learn privately? SIAM Journal on Computing, 40(3), 2011.
[KMA+19] Peter Kairouz, H. Brendan McMahan, Brendan Avent, Aure´lien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji, Keith Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, Rafael G. L. D’Oliveira, Salim El Rouayheb, David Evans, Josh Gardner, Zachary Garre , Adria` Gasco´n, Badih Ghazi, Phillip B. Gibbons, Marco Gruteser, Zaid Harchaoui, Chaoyang He, Lie He, Zhouyuan Huo, Ben Hutchinson, Justin Hsu, Martin Jaggi, Tara Javidi, Gauri Joshi, Mikhail Khodak, Jakub Konecˇny´, Aleksandra Korolova, Farinaz Koushanfar, Sanmi Koyejo, Tancre`de Lepoint, Yang Liu, Prateek Mi al, Mehryar Mohri, Richard Nock, Ayfer O¨ zgu¨r, Rasmus Pagh,

12

Mariana Raykova, Hang Qi, Daniel Ramage, Ramesh Raskar, Dawn Song, Weikang Song, Sebastian U. Stich, Ziteng Sun, Ananda eertha Suresh, Florian Trame`r, Praneeth Vepakomma, Jianyu Wang, Li Xiong, Zheng Xu, Qiang Yang, Felix X. Yu, Han Yu, and Sen Zhao. Advances and open problems in federated learning, 2019.

[KNRS13] Shiva Prasad Kasiviswanathan, Kobbi Nissim, Sofya Raskhodnikova, and Adam Smith. Analyzing graphs with node di erential privacy. In eory of Cryptography Conference, pages 457–476. Springer, 2013.

[LPP20]

Peeter Laud, Alisa Pankova, and Martin Pe ai. A framework of metrics for di erential privacy from local sensitivity. Proceedings on Privacy Enhancing Technologies, 2020(2):175–208, 2020.

[LQS12]

Ninghui Li, Wahbeh Qardaji, and Dong Su. On sampling, anonymization, and di erential privacy or, k-anonymization meets di erential privacy. In ASIA-CCS, pages 32–33, 2012.

[MRR+53] Nicholas Metropolis, Arianna W Rosenbluth, Marshall N Rosenbluth, Augusta H Teller, and

Edward Teller. Equation of state calculations by fast computing machines. chemical physics, 21(6):1087–1092, 1953.

e journal of

[MSC+13] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Je Dean. Distributed representations of words and phrases and their compositionality. In NeurIPS, pages 3111–3119, 2013.

[MSZ15]

Mahnush Movahedi, Jared Saia, and Mahdi Zamani. Secure multi-party shu ing. In International Colloquium on Structural Information and Communication Complexity, pages 459–473. Springer, 2015.

[MT07]

Frank McSherry and Kunal Talwar. Mechanism design via di erential privacy. In FOCS, volume 7, pages 94–103, 2007.

[NRS07]

Kobbi Nissim, Sofya Raskhodnikova, and Adam Smith. Smooth sensitivity and sampling in private data analysis. In ACM Symposium on eory of Computing, pages 75–84. ACM, 2007.

[PSM14] Je rey Pennington, Richard Socher, and Christopher Manning. Glove: Global vectors for word representation. In EMNLP, pages 1532–1543, 2014.

[SS15]

Reza Shokri and Vitaly Shmatikov. Privacy-preserving deep learning. In ACM CCS, pages 1310–1321, 2015.

[STT+12]

Reza Shokri, George eodorakopoulos, Carmela Troncoso, Jean-Pierre Hubaux, and JeanYves Le Boudec. Protecting location privacy: optimal strategy against localization a acks. In ACM CCS, pages 617–627. ACM, 2012.

[Swe02] Latanya Sweeney. k-anonymity: A model for protecting privacy. IJUFKS, 10(05):557–570, 2002.

[Tea17]

Apple’s Di erential Privacy Team. Learning with privacy at scale. Apple Machine Learning Journal, 1(9), 2017.

[War65]

Stanley L Warner. Randomized response: A survey technique for eliminating evasive answer bias. Journal of the American Statistical Association, 60(309):63–69, 1965.

13

[WE18]

Isabel Wagner and David Eckho . Technical privacy metrics: a systematic survey. ACM CSUR, 51(3):57, 2018.

14

