Detecting Attackable Sentences in Arguments

arXiv:2010.02660v1 [cs.CL] 6 Oct 2020

Yohan Jo1 Seojin Bang1 Emaad Manzoor2 Eduard Hovy1 Chris Reed3 1School of Computer Science, Carnegie Mellon University, USA
2Heinz College of Information Systems and Public Policy, Carnegie Mellon University, USA 3Centre for Argument Technology, University of Dundee, UK
{yohanj,seojinb}@cs.cmu.edu, {emaad,hovy}@cmu.edu
c.a.reed@dundee.ac.uk

OP: I believe that Communism is not as bad as everyone says

Abstract
Finding attackable sentences in an argument is the ﬁrst step toward successful refutation in argumentation. We present a ﬁrst large-scale analysis of sentence attackability in online arguments. We analyze driving reasons for attacks in argumentation and identify relevant characteristics of sentences. We demonstrate that a sentence’s attackability is associated with many of these characteristics regarding the sentence’s content, proposition types, and tone, and that an external knowledge source can provide useful information about attackability. Building on these ﬁndings, we demonstrate that machine learning models can automatically detect attackable sentences in arguments, signiﬁcantly better than several baselines and comparably well to laypeople.1
1 Introduction
Effectively refuting an argument is an important skill in persuasion dialogue, and the ﬁrst step is to ﬁnd appropriate points to attack in the argument. Prior work in NLP has studied argument quality (Wachsmuth et al., 2017a; Habernal and Gurevych, 2016a) and counterargument generation (Hua et al., 2019; Wachsmuth et al., 2018). But these studies mainly concern an argument’s overall quality and making counterarguments toward the main claim, without investigating what parts of an argument are attackable for successful persuasion. Nevertheless, attacking speciﬁc points of an argument is common and effective; in our data of online discussions, challengers who successfully change the original poster’s view are 1.5 times more likely to quote speciﬁc sentences of the argument for attacks than unsuccessful challengers (Figure 1). In this paper, we examine how to computationally
1Our data and source code are available at: github. com/yohanjo/emnlp20_arg_attack

>A society where everyone is equal seems great to me That's one of the big problems with communism what is equality? Is everyone equal? [...]
>it removes some of the basic faults in society, such as poverty, homelessness, joblessness, as well as touching on moral values such as greed, and envy Yes there are problems within society but this doesn't mean there is a fault with society. [...]
>I believe a proper Communist society (I.E. one that is not a dictatorship like Joseph Stalin or Fidel Castro) furthermore, it is unlikely we could ever get a true communist society due to human nature. [...]
Figure 1: A comment to a post entitled “I believe that Communism is not as bad as everyone says”. It quotes and attacks some sentences in the post (red with “>”)
detect attackable sentences in arguments. This attackability information would help people make persuasive refutations and strengthen an argument by solidifying potentially attackable points.
To examine the characteristics of attackable sentences in an argument, we ﬁrst conduct a qualitative analysis of reasons for attacks in online arguments. Our data comes from discussions in the ChangeMyView (CMV) forum on Reddit. In CMV, users challenge the viewpoints of original posters (OPs), and those who succeed receive a ∆ from the OPs. In this setting, sentences that are attacked and lead to the OP’s view change are considered “attackable”, i.e., targets that are worth attacking. Admittedly, persuasion has to do with “how” to attack as well, but this is beyond the scope of this paper. We only focus on choosing proper sentences to attack, which is a prerequisite for effective persuasion.
This analysis of reasons for attacks, along with argumentation theory and discourse studies, provide insights into what characteristics of sentences are relevant to attackability. Informed by these insights, we extract features that represent relevant sentence characteristics, clustered into four categories: content, external knowledge, proposition

@inproceedings{Jo:2020attack, author = {Jo, Yohan and Bang, Seojin and Manzoor, Emaad and Hovy, Eduard and Reed, Chris }, title = {Detecting Attackable Sentences in Arguments}, booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing}, year = {2020}}

types, and tone. We demonstrate the effects of individual features on sentence attackability, in regard to whether a sentence would be attacked and whether a sentence would be attacked successfully.
Building on these ﬁndings, we examine the efﬁcacy of machine learning models in detecting attackable sentences in arguments. We demonstrate that their decisions match the gold standard signiﬁcantly better than several baselines and comparably well to laypeople.
To the best of our knowledge, this work is the ﬁrst large-scale analysis of sentence attackability in arguments. Our contributions are as follows:
• We introduce the problem of detecting attackable sentences in arguments and release the processed data from online discussions and the external knowledge source we used.
• We analyze driving reasons for attacks in arguments and the effects of sentence characteristics on a sentence’s attackability.
• We demonstrate the performance of machine learning models for detecting attackable sentences, setting a baseline for this challenging task and suggesting future directions.
2 Background
The strength of an argument is a long-studied topic, dating back to Aristotle (2007), who suggested three aspects of argument persuasiveness: ethos (the arguer’s credibility), logos (logic), and pathos (appeal to the hearer’s emotion). More recently, Wachsmuth et al. (2017b) summarized various aspects of argument quality studied in argumentation theory and NLP, such as clarity, relevance, and arrangement. Some research took empirical approaches and collected argument evaluation criteria from human evaluators (Habernal and Gurevych, 2016a; Wachsmuth et al., 2017a). By adopting some of these aspects, computational models have been proposed to automatically evaluate argument quality in various settings, such as essays (Ke et al., 2019), online comments (Gu et al., 2018), and pairwise ranking (Habernal and Gurevych, 2016b). While these taxonomies help understand and evaluate the quality of an argument as a whole, little empirical analysis has been done in terms of what to attack in an argument to persuade the arguer.
What can be attacked in an argument has been studied more in argumentation theory. Particularly, Walton et al. (2008) present argumentation schemes and critical questions (CQs). Argument schemes

are reasoning types commonly used in daily argumentation. For instance, the scheme of argument from cause to effect has the conclusion “B will occur” supported by the premise “if A occurs, B will occur. In this case, A occurs”. Each scheme is associated with a set of CQs for judging the argument to be good or fallacious. CQs for the above scheme include “How strong is the causal generalization?” and “Are there other factors that interfere with the causal effect?” Unlike the general argument quality described in the previous paragraph, CQs serve as an evaluation tool that specify local attackable points in an argument. They have been adopted for grading essays (Song et al., 2017) and teaching argumentation skills (Nussbaum et al., 2018). Some of the sentence characteristics in our work are informed by argumentation schemes and CQs.
NLP researchers have widely studied the effectiveness of counterarguments on persuasion (Tan et al., 2016; Cano-Basave and He, 2016; Wei et al., 2016; Wang et al., 2017; Morio et al., 2019) and how to generate counterarguments (Hua et al., 2019; Wachsmuth et al., 2018). Most of the work focuses on the characteristics of counterarguments with respect to topics and styles, without consideration of what points to attack. On the other hand, some studies aimed to model the salience of individual sentences in attacked arguments by paying different degrees of attention to sentences using attention mechanism (Jo et al., 2018; Ji et al., 2018). While their approaches helped to predict the success of persuasion, it was difﬁcult to interpret what constitute the salience or attackability of sentences. To address this limitation, we quantify and analyze the characteristics of sentences that are attacked and lead to the arguer’s view change.
3 Data
Here we describe how we collected and labeled our data.
3.1 Data Collection
We use online discussions from the ChangeMyView (CMV) subreddit2. In this forum, users post their views on various issues and invite other users to challenge their views. If a comment changes the original poster (OP)’s view, the OP acknowledges it by replying to the comment with a ∆ symbol. The high quality of the discussions in this forum is maintained through several mod-
2https://www.reddit.com/r/changemyview

eration rules, such as the minimum length of an original post and the maximum response time of OPs. As a result, CMV discussions have been used in many NLP studies (Chakrabarty et al., 2019; Morio et al., 2019; Jo et al., 2018; Musi, 2017; Wei et al., 2016; Tan et al., 2016).
We scraped CMV posts and comments written between January 1, 2014 and September 30, 2019, using the Pushshift API. We split them into a dev set (Jan 2014–Jan 2018 for training and Feb 2018– Nov 2018 for validation) and a test set (Dec 2018– Sep 2019), with the ratio of 6:2:2. We split the data by time to measure our models’ generality to unseen subjects.
As the characteristics of arguments vary across different issues, we categorized the posts into domains using LDA. For each post, we chose as its domain the topic that has the highest standard score; topics comprising common words were excluded. We tried different numbers of topics (25, 30, 35, 40) and ﬁnalized on 40, as it achieves the lowest perplexity. This process resulted in 30 domains (excluding common-word topics): media, abortion, sex, election, Reddit, human economy, gender, race, family, life, crime, relationship, movie, world, game, tax, law, money, drug, war, religion, job, food, power, school, college, music, gun, and Jewish (from most frequent to least, ranging 5%–2%).
3.2 Labeling Attackability
Since we are interested in which parts of a post are attacked by comments and whether the attacks lead to successful view changes, our goal here is to label each sentence in a post as successfully attacked, unsuccessfully attacked, or unattacked. We only consider comments directly replying to each post (toplevel comments), as lower-level comments usually address the same points as their parent comments (as will be validated at the end of the section).
Attacked vs. Unattacked: Some comments use direct quotes with the > symbol to address speciﬁc sentences of the post (Figure 1). Each quote is matched with the longest sequence of sentences in the post using the Levenshtein edit distance (allowing a distance of 2 characters for typos). A matched text span should contain at least one word and four characters, and cover at least 80% of the quote to exclude cases where the > symbol is used to quote external content. As a result, 98% of the matched spans cover the corresponding quotes entirely. Additionally, a sentence in the post is considered to

be quoted if at least four non-stopwords appear in a comment’s sentence. For example:
Post: ... If you do something, you should be prepared to accept the consequences. ... Comment: ... I guess my point is, even if you do believe that “If you do something, you should be prepared to accept the consequences,” you can still feel bad for the victims. ...
We considered manually annotating attacked sentences too, but it turned out to be extremely timeconsuming and subjective (Appendix A). We tried to automate it using heuristics (word overlap and vector embeddings), but precision severely deteriorated. As we value the precision of labels over recall, we only use the method described in the previous paragraph. Chakrabarty et al. (2019) used the same method to collect attack relations in CMV.
Successfully vs. Unsuccessfully Attacked: After each sentence in a post is labeled as attacked or not, each attacked sentence is further labeled as successfully attacked if any of the comments that attack it, or their lower-level comments win a ∆.
We post-process the resulting labels to increase their validity. First, as a challenger and the OP have discussion down the comment thread, the challenger might attack different sentences than the originally attacked ones and change the OP’s view. In this case, it is ambiguous which sentences contribute to the view change. Hence, we extract quotes from all lower-level comments of ∆-winning challengers, and if any of the quotes attack new sentences, this challenger’s attacks are excluded from the labeling of successfully attacked. This case is not common, however (0.2%).
Second, if a comment attacks many sentences in the post and change the OP’s view, some of them may not contribute to the view change but are still labeled as successfully attacked. To reduce this noise, comments that have more than three quotes are excluded from the labeling of successfully attacked3. This amounts to 12% of top-level comments (63% of comments have only one quote, 17% two quotes, and 8% three quotes).
Lastly, we veriﬁed if quoted sentences are actually attacked. We randomly selected 500 comments and checked if each quoted sentence is purely agreed with without any opposition, challenge, or question. This case was rare (0.4%)4, so we do
3This allows our subsequent analyses to capture stronger signals for successful attacks than without this process.
4Further, this case happened in only one out of the 500 comments (0.2%), where the author agreed with 4 quoted sentences. In CMV, challengers do use concessions but hardly

Dataset

Attacked

#posts #sentences
#attacked

#posts Successful #sentences
#successful

Train
25,839 420,545 119,254
3,785 66,628
8,746

Val
8,763 133,090
40,163
1,235 20,240
2,718

Test
8,558 134,375
40,354
1,064 17,129
2,288

Table 1: Data statistics. “Attacked” contains posts with at least one attacked sentence. “Successful” contains posts with at least one successfully attacked sentence.

R1 S is true but does not support the main claim (19%) R2 S misses cases suggesting opposite judgment (18%) R3 S has exceptions (17%) R4 S is false (12%) R5 S misses nuanced distinctions of a concept (8%) R6 S is unlikely to happen (6%) R7 S has no evidence (6%) R8 S uses an invalid assumption or hypothetical (4%) R9 S contradicts statements in the argument (4%) R10 Other (4%)
(a) Rationales for attacking a sentence (S).
F1 Personal opinion (28%) F2 Invalid hypothetical (26%) F3 Invalid generalization (13%) F4 No evidence (11%) F5 Absolute statement (7%) F6 Concession (5%) F7 Restrictive qualiﬁer (5%) F8 Other (5%)
(b) Motivating factors for attacks.
Table 2: Rationales and motivating factors for attacks.

not further process this case. Table 1 shows some statistics of the ﬁnal data.
4 Quantifying Sentence Characteristics
As the ﬁrst step for analyzing the characteristics of attackable sentences, we examine driving reasons for attacks and quantify relevant characteristics.
4.1 Rationales and Motivation for Attacks
To analyze rationales for attacks, two authors examined quotes and rebuttals in the training data (one successful and one unsuccessful comment for each post). From 156 attacks, we identiﬁed 10 main rationales (Table 2a), which are ﬁner-grained than the refutation reasons in prior work (Wei et al., 2016). The most common rationale is that the sentence is factually correct but is irrelevant to the main claim (19%). Counterexample-related rationales are also common: the sentence misses an example suggest-
quote the OP’s sentences just to agree.

ing the opposite judgment to the sentence’s own (18%) and the sentence has exceptions (17%).
This analysis is based on polished rebuttals, which mostly emphasize logical aspects, and cannot fully capture other factors that motivate attacks. Hence, we conducted a complementary analysis, where an undergraduate student chose three sentences to attack for each of 50 posts and speciﬁed the reasons in their own terms (Table 2b). The most common factor is that the sentence is only a personal opinion (28%). Invalid hypotheticals are also a common factor (26%). The tone of a sentence motivates attacks as well, such as generalization (13%), absoluteness (7%), and concession (5%).
4.2 Feature Extraction
Based on these analyses, we cluster various sentence characteristics into four categories—content, external knowledge, proposition types, and tone.5
4.2.1 Content
Content and logic play the most important role in CMV discussions. We extract the content of each sentence at two levels: TFIDF-weighted n-grams (n = 1, 2, 3) and sentence-level topics. Each sentence is assigned one topic using Sentence LDA (Jo and Oh, 2011). We train a model on posts in the training set and apply it to all posts, exploring the number of topics ∈ {10, 50, 100}.6
4.2.2 External Knowledge
External knowledge sources may provide information as to how truthful or convincing a sentence is (e.g., Table 2a-R2, R3, R4, R7 and Table 2b-F4). As our knowledge source, we use kialo.com—a collaborative argument platform over more than 1.4K issues. Each issue has a main statement, and users can respond to any existing statement with pro/con statements (1-2 sentences), building an argumentation tree. Kialo has advantages over structured knowledge bases and Wikipedia in that it includes many debatable statements; many attacked sentences are subjective judgments (§4.1), so factbased knowledge sources may have limited utility. In addition, each statement in Kialo has pro/con counts, which may reﬂect the convincingness of the statement. We scraped 1,417 argumentation trees and 130K statements (written until Oct 2019).
5Some rationales in Table 2a (e.g., R1 and R9) are difﬁcult to operationalize reliably using the current NLP technology and thus are not included in our features.
6We also tried features based on semantic frames using SLING (Ringgaard et al., 2017), but they were not helpful.

For each sentence in CMV, we retrieve simi-

lar statements in Kialo that have at least 5 com-

mon words7 and compute the following three fea-

tures. Frequency is the number of retrieved state-

ments; sentences that are not suitable for argu-

mentation are unlikely to appear in Kialo. This

feature is computed as log2(N + 1), where N is the number of retrieved statements. Attrac-

tiveness is the average number of responses for

the matched statements, reﬂecting how debatable

the sentence is. It is computed as log2(M + 1),

where

M

=

1 N

N i=1

Ri

and

Ri

is

the

number

of

responses for the ith retrieved statement. Lastly,

extremeness

is

1 N

N i=1

|Pi

−

Ni|,

where

Pi

and

Ni are the proportions (between 0 and 1) of pro

responses and con responses for the ith retrieved

statement. A sentence that most people would see

ﬂawed would have a high extremeness value.

4.2.3 Proposition Types
Sentences convey different types of propositions, such as predictions and hypotheticals. No proposition types are fallacious by nature, but some of them may make it harder to generate a sound argument. They also communicate different moods, causing the hearer to react differently. We extract 13 binary features for proposition types. They are all based on lexicons and regular expressions, which are available in Appendix C.
Questions express the intent of information seeking. Depending on the form, we deﬁne three features: confusion (e.g., I don’t understand), why/how (e.g., why ...?), and other.
Normative sentences suggest that an action be carried out. Due to their imperative mood, they can sound face-threatening and thus attract attacks.
Prediction sentences predict a future event. They can be attacked with reasons why the prediction is unlikely (Table 2a-R6), as in critical questions for argument from cause to effect (Walton et al., 2008).
Hypothetical sentences may make implausible assumptions (Table 2a-R8 and Table 2b-F2) or restrict the applicability of the argument too much (Table 2b-F7).
Citation often strengthens a claim using authority, but the credibility of the source could be attacked (Walton et al., 2008).
Comparison may reﬂect personal preferences that are vulnerable to attacks (Table 2b-F1).

7Similarity measures based on word embeddings and knowledge representation did not help (Appendix B).

Examples in a sentence may be attacked for their invalidity (Walton et al., 2008) or counterexamples (Table 2a-R3).
Deﬁnitions form a ground for arguments, and challengers could undermine an argument by attacking this basis (e.g., Table 2a-R5).
Personal stories are the arguer’s experiences, whose validity is difﬁcult to refute. A sentence with a personal story has subject I and a non-epistemic verb; or it has my modifying non-epistemic nouns.
Inclusive sentences that mention you and we engage the hearer into the discourse (Hyland, 2005), making the argument more vulnerable to attacks.
4.2.4 Tone
Challengers are inﬂuenced by the tone of an argument, e.g., subjectiveness, absoluteness, or conﬁdence (Table 2b). We extract 8 features for the tone of sentences.
Subjectivity comprises judgments, which are often attacked due to counterexamples (Table 2a-R2) or their arbitrariness (Table 2b-F1, Walton et al. (2008)). The subjectivity of a sentence is the average subjectivity score of words based on the Subjectivity Lexicon (Wilson et al., 2005) (non-neutral words of “weaksubj” = 0.5 and “strongsubj” = 1).
Concreteness is the inverse of abstract diction, whose meaning depends on subjective perceptions and experiences. The concreteness of a sentence is the sum of the standardized word scores based on Brysbaert et al. (2014)’s concreteness lexicon.
Qualiﬁcation expresses the level of generality of a claim, where absolute statements can motivate attacks (Table 2b-R3). The qualiﬁcation score of a sentence is the average word score based on our lexicon of qualiﬁers and generality words.
Hedging can sound unconvincing (Durik et al., 2008) and motivate attacks. A sentence’s hedging score is the sum of word scores based on our lexicon of downtoners and boosters.
Sentiment represents the valence of a sentence. Polar judgments may attract more attacks than neutral statements. We calculate the sentiment of each sentence with BERT (Devlin et al., 2018) trained on the data of SemEval 2017 Task 4 (Rosenthal et al., 2017). Sentiment score is a continuous value ranging between -1 (negative) and +1 (positive), and sentiment categories are nominal (positive, neutral, and negative)8. In addition, we compute the
8We achieved an average recall of 0.705, which is higher than the winner team’s performance of 0.681.

scores of arousal (intensity) and dominance (control) as the sum of the standardized word scores based on Warriner et al. (2013)’s lexicon.
5 Task 1: Attackability Characteristics
One of our goals in this paper is to analyze what characteristics of sentences are associated with a sentence’s attackability. Hence, in this section, we measure the effect size and statistical signiﬁcance of each feature toward two labels: (i) whether a sentence is attacked or not, using the dev set of the “Attacked” dataset (N =553,635), (ii) whether a sentence is attacked successfully or unsuccessfully, using all attacked sentences (N =159,417).9 Since the effects of characteristics may depend on the issue being discussed, the effect of each feature is estimated conditioned on the domain of each post using a logistic regression, and the statistical signiﬁcance of the effect is assessed using the Wald test. For interpretation purposes, we use odds ratio (OR)—the exponent of the effect size.10
5.1 Content
Attacked sentences tend to mention big issues like gender, race, and health as revealed in topics 47, 8, and 6 (Table 3) and n-grams life, weapons, women, society, and men (Table 7 in Appendix E). These issues are also positively correlated with successful attacks. On the other hand, mentioning relatively personal issues (tv, friends, topic 38) seems negatively correlated with successful attacks. So do forum-speciﬁc messages (cmv, thank, topic 4).
Attacking seemingly evidenced sentences appears to be effective for persuasion when properly done. Successfully attacked sentences are likely to mention speciﬁc data (data, %) and be the OP’s speciﬁc reasons under bullet points (2. and 3.).
n-grams capture various characteristics that are vulnerable to attacks, such as uncertainty and absoluteness (i believe, never), hypotheticals (if i), questions (?, why), and norms (should).
9Simply measuring the predictive power of features in a prediction setting provides an incomplete picture of the roles of the characteristics. Some features may not have drastic contribution to prediction due to their infrequency, although they may have signiﬁcant effects on attackability.
10Odds are the ratio of the probability of a sentence being (successfully) attacked to the probability of being not (successfully) attacked; OR is the ratio of odds when the value of the characteristic increases by one unit (Appendix D).

Content

Knowledge

Proposition Types

Feature
Topic47: Gender† Topic8: Race† Topic6: Food† Topic38: Movie & Show† Topic4: CMV-Speciﬁc†
Kialo Frequency (log2) Kialo Attractiveness (log2) Kialo Extremeness
Question - Confusion† Question - Why/How† Question - Other† Citation† Deﬁnition† Normative† Prediction† Hypothetical† Comparison† Example† Personal Story† Use of You† Use of We†
Subjectivity‡ Concreteness‡ Hedges‡ Quantiﬁcation‡ Sentiment Score‡ Sentiment: Positive† Sentiment: Neutral† Sentiment: Negative† Arousal‡ Dominance‡

Attacked
1.37 (***) 1.19 (***) 1.00 ( ) 1.03 ( ) 0.16 (***)
1.18 (***) 1.30 (***) 1.51 (***)
0.97 ( ) 1.77 (***) 1.16 (***) 0.53 (***) 1.04 ( ) 1.26 (***) 1.22 (***) 1.29 (***) 1.25 (***) 1.20 (***) 0.70 (***) 1.18 (***) 1.24 (***)
1.03 (***) 0.87 (***) 1.04 (***) 0.97 (***) 0.87 (***) 0.76 (***) 0.82 (***) 1.34 (***) 1.02 (***) 1.07 (***)

Successful
1.34 (***) 1.21 ( ** ) 1.39 (***) 0.78 (***) 0.36 ( ** )
1.07 (***) 1.18 (***) 1.19 (***)
1.29 ( * ) 1.27 (***) 1.11 ( * ) 1.17 ( * ) 1.32 ( ** ) 1.10 ( ** ) 1.02 ( ) 1.07 ( ) 1.02 ( ) 1.17 ( * ) 1.09 ( ** ) 1.04 ( ) 0.98 ( )
0.97 (***) 0.92 (***) 1.06 (***) 1.02 ( ) 1.00 ( ) 0.99 ( ) 1.00 ( ) 1.00 ( ) 0.95 (***) 1.08 (***)

Tone

Table 3: Odds ratio (OR) and statistical signiﬁcance of features. An effect is positive (blue) if OR > 1 and negative (red) if OR < 1. (†: binary, ‡: standardized / *: p < 0.05, **: p < 0.01, ***: p < 0.001)

5.2 External Knowledge
The Kialo-based knowledge features provide significant information about whether a sentence would be attacked successfully (Table 3). As the frequency of matched statements in Kialo increases twice, the odds for successful attack increase by 7%. As an example, the following attacked sentence has 18 matched statements in Kialo.
I feel like it is a parents right and responsibility to make important decisions for their child.
The attractiveness feature has a stronger effect; as matched statements have twice more responses, the odds for successful attack increase by 18%, probably due to higher debatability.
A sentence being completely extreme (i.e., the matched sentences have only pro or con responses) increases the odds for successful attack by 19%.
As expected, the argumentative nature of Kialo

allows its statements to match many subjective sentences in CMV and serves as an effective information source for a sentence’s attackability.
5.3 Proposition Types
Questions, especially why/how, are effective targets for successful attack (Table 3). Although challengers do not pay special attention to expressions of confusion (see column “Attacked”), they are positively correlated with successful attack (OR=1.29).
Citations are often used to back up an argument and have a low chance of being attacked, reducing the odds by half. However, properly attacking citations signiﬁcantly increases the odds for successful attack by 17%. Similarly, personal stories have a low chance of being attacked and deﬁnitions do not attract challengers’ attacks, but attacking them is found to be effective for successful persuasion.
All other features for proposition types have signiﬁcantly positive effects on being attacked (OR=1.18–1.29), but only normative and example sentences are correlated with successful attack.
5.4 Tone
Successfully attacked sentences tend to have lower subjectivity and arousal (Table 3), in line with the previous observation that they are more data- and reference-based than unsuccessfully attacked sentences. In contrast, sentences about concrete concepts are found to be less attackable.
Uncertainty (high hedging) and absoluteness (low qualiﬁcation) both increase the chance of attacks, which aligns with the motivating factors for attacks (Table 2b), while only hedges are positively correlated with successful attacks, implying the importance of addressing the arguer’s uncertainty.
Negative sentences with high arousal and dominance have a high chance of being attacked, but most of these characteristics have either no or negative effects on successful attacks.
5.5 Discussion
We have found some evidence that, somewhat counter-intuitively, seemingly evidenced sentences are more effective to attack. Such sentences use speciﬁc data (data, %), citations, and deﬁnitions. Although attacking these sentences may require even stronger evidence and deeper knowledge, arguers seem to change their viewpoints when a fact they believe with evidence is undermined. In addition, it seems very important and effective to

identify and address what the arguer is confused (confusion) or uncertain (hedges) about.
Our analysis also reveals some discrepancies between the characteristics of sentences that challengers commonly think are attackable and those that are indeed attackable. Challengers are often attracted to subjective and negative sentences with high arousal, but successfully attacked sentences have rather lower subjectivity and arousal, and have no difference in negativity compared to unsuccessfully attacked sentences. Furthermore, challengers pay less attention to personal stories, while successful attacks address personal stories more often.
6 Task 2: Attackability Prediction
Now we examine how well computational models can detect attackable sentences in arguments.
6.1 Problem Formulation
This task is cast as ranking sentences in each post by their attackability scores predicted by a regression model. We consider two types of attackability: (i) whether a sentence will be attacked or not, (ii) whether a sentence will be successfully attacked or not (attacked unsuccessfully + unattacked). For both settings, we consider posts that have at least one sentence with the positive label (Table 1).
We use three evaluation metrics. P@1 is the precision of the ﬁrst ranked sentence, measuring the model’s accuracy when choosing one sentence to attack for each post. Less strictly, A@3 gives a score of 1 if any of the top 3 sentences is a positive instance and 0 otherwise. AUC measures individual sentence-level accuracy—how likely positive sentences are assigned higher probabilities.
6.2 Comparison Models
For machine learning models, we explore two logistic regression models to compute the probability of the positive label for each sentence, which becomes the sentence’s attackability score. LR is a basic logistic regression with our features11 (Section 4) and binary variables for domains. We explored feature selection using L1-norm and regularization using L2-norm.12 BERT is logistic regression where our features are replaced with the BERT embedding of the input sentence (Devlin et al., 2018). Contextualized BERT embeddings have achieved
11We tried the number of topics ∈ {10, 50, 100}, and 50 has the best AUC on the val set for both prediction settings.
12We also tried a multilayer perceptron to model feature interactions, but it consistently performed worse than LR.

Random Length
LR (×) Content (×) Knowledge (×) Prop Type (×) Tone BERT
Humans†

Attacked

Successful

P@1 A@3 AUC P@1 A@3 AUC

35.9 66.0 50.1 18.9 45.0 50.1 42.9 73.7 54.5 22.3 52.1 55.7

47.1 76.2 61.7 24.2 54.5 59.3 45.2 74.4 58.1 24.0 52.6 57.0 47.0 76.0 61.7 24.1 54.3 59.0 46.7 75.9 61.5 24.4 53.6 59.0 47.0 76.0 61.9 25.2 56.2 59.4 49.6 77.8 64.4 28.3 57.2 62.0

51.7 80.1 – 27.8 54.2 –

Table 4: Prediction accuracy. All LR/BERT scores (rows 3–8) have standard deviations between 0.1 and 1.1, signiﬁcantly outperforming “Length”. †The average bootstrap accuracy after resampling 100K times with sample size 200—the standard deviations of P@1 and A@3 range between 2.1 and 3.5.

state-of-the-art performance in many NLP tasks. We use the pretrained, uncased base model from Hugging Face (Wolf et al., 2019) and ﬁne-tune it during training.13
We explore two baseline models. Random is to rank sentences randomly. Length is to rank sentences from longest to shortest, with the intuition that longer sentences may contain more information and thus more content to attack as well.
Lastly, we estimate laypeople’s performance on this task. Three undergraduate students each read 100 posts and rank three sentences to attack for each post. Posts that have at least one positive instance are randomly selected from the test set.14
6.3 Results
All computational models were run 10 times, and their average accuracy is reported in Table 4. Both the LR and BERT models signiﬁcantly outperform the baselines, while the BERT model performs best. For predicting attacked sentences, the BERT model’s top 1 decisions match the gold standard 50% of the time; its decisions match 78% of the time when three sentences are chosen. Predicting successfully attacked sentences is harder, but the performance gap between our models and the baselines gets larger. The BERT model’s top 1 decisions match the gold standard 28% of the time—a 27% and 10% boost from random and length-based performance, respectively.
13Details for reproducibility are in Appendix F. 14We were interested in the performance of young adults who are academically active and have a moderate level of life experience. Their performance may not represent the general population, though.

To examine the contribution of each feature category, we did ablation tests based on the best performing LR model (Table 4 rows 4–7). The two prediction settings show similar tendencies. Regarding P@1 for successful attack, content has the highest contribution, followed by knowledge, proposition types, and tone. This result reafﬁrms the importance of content for a sentence’s attackability. But the other features still have signiﬁcant contribution, yielding higher P@1 and AUC (Table 4 row 4) than the baselines.
It is worth noting that our features, despite the lower accuracy than the BERT model, are clearly informative of attackability prediction as Table 4 row 3 shows. Moreover, since they directly operationalize the sentence characteristics we compiled, it is pretty transparent that they capture relevant information that contributes to sentence attackability and help us better understand what characteristics have positive and negative signals for sentence attackability. We speculate that transformer models like BERT are capable of encoding these characteristics more sophisticatedly and may include some additional information, e.g., lexical patterns, leading to higher accuracy. But at the same time, it is less clear exactly what they capture and whether they capture relevant information or irrelevant statistics, as is often the case in computational argumentation (Niven and Kao, 2019).
Figure 2 illustrates how LR allows us to interpret the contribution of different features to attackability, by visualizing a post with important features highlighted. For instance, external knowledge plays a crucial role in this post; all successfully attacked sentences match substantially more Kialo statements than other sentences. The attackability scores of these sentences are also increased by the use of hypotheticals and certain n-grams like could. These features align well with the actual attacks by successful challengers. For instance, they pointed out that the expulsion of Russian diplomats (sentence 2) is not an aggressive reaction because the diplomats can be simply replaced with new ones. Kialo has a discussion on the relationship between the U.S. and Russia, and one statement puts forward exactly the same point that the expulsion was a forceful-looking but indeed a nice gesture. Similarly, a successful challenger pointed out the consistent attitude of the U.S. toward regime change in North Korea (sentence 3), and the North Korean regime is a controversial topic in Kialo. Lastly,

I'm typing this post mostly from anxiety considering recent events, but hopefully this post will spark optimistic discussion that I don't see often in the news or online or such. With the appointment of John Bolton as the National Security Adviser and John Pompeo as the Secretary of State, two men known for hawkish and pro-war behavior in their previous statements and actions, the US has appeared to take a more aggressive stance in foreign policy, seen with the expulsion of sixty Russian diplomats following minor controversy in the United Kingdom. Also, despite planned negotiations with Kim Jong-Un concerning the future of North Korea, the US, and NK's nuclear arsenal,
President Trump has ﬁlled out his cabinet/diplomacy team with people who are in favor of things such as a regime change or attacking North Korea, further stirring things up for a potential falling out. If talks between the two nations break down, the US does not have much more of a reason to withhold from attacking North Korea, which is a plan that seems to be favorable among higher ofﬁcials. Considering that this is also sort of a proxy scufﬂe between us and China/ Russia, attacking or otherwise provoking North Korea or Russia could lead to situations ranging from a worldwide economic downturn to nuclear holocaust. Is conﬂict the current trajectory of international relations? How would we otherwise not engage in some sort of scufﬂe?

Prediction (0.12) Personal (-0.20) Topic37 (-0.21)
KialoFreq (0.98) Topic5 (0.39) KialoAttr (0.05) KialoExtr (-0.07)
KialoFreq (0.75) Topic5 (0.39) Example (0.11) KialoAttr (0.07) KialoExtr (-0.07)
Topic5 (0.39) KialoFreq (0.22) KialoAttr (0.13) Hypothetical (-0.06) KialoExtr (-0.11)
KialoFreq (0.45) Topic5 (0.39) KialoAttr (0.26) KialoExtr (-0.05) Use of "We" (-0.18)
Topic5 (0.39) QuestOther (0.39)
Why/How (0.91) Use of "We" (-0.18) Topic37 (-0.21)

in how humans and machines choose sentences to attack.
7 Conclusion
We studied how to detect attackable sentences in arguments for successful persuasion. Using online arguments, we demonstrated that a sentence’s attackability is associated with many of its characteristics regarding its content, proposition types, and tone, and that Kialo provides useful information about attackability. Based on these ﬁndings we demonstrated that machine learning models can automatically detect attackable sentences, comparably well to laypeople.
Our work contributes a new application to the growing literature on causal inference from text (Egami et al., 2018), in the setting of “text as a treatment”. Speciﬁcally, our ﬁndings in Section 5

Figure 2: Prediction visualization. Background color pave the way towards answering the causal ques-

886dg7)

indicates predicted attackability (blue: hiSghu,crceeds: slo2w()t.3_8tgio5nu:kwh)ould attacking a certain type of sentence Success 3 (t3_

mostly

from

anxiety

Successfully
considering

attacked

sentences

arTeheunladset rPlirneesidd.entFiaelae-lection(e(2.g01.6,) qaundesmtoisot nsusccoeerdiengxpressions

of

confusion)

in

an

ully this post will spark opttuimriestsicwPPitreehrdsoihcntiaiogln(-h(00./2.1l0o2))w weights are indicelaetcetdionwsihthavebpluroev/erendth.at elecatriognusmareenmtorienacbroeutapsaertythCeompparroisobna(0b.2i0l)ity of persuading

see often in the news or online or Topic37 (-0.21)

afﬁliations than actual views or the character of the individual

I believe that socialism

ent of John Bolton as the National KialoFreq (0.98)

being elected. In one of the most etxhtreemeoepxianmipolens,hRooyldMeoorr?e While our ﬁndings suggestnext step for the U.S. It

Topic35 (-0.07)

vastly successful people

ohn Pompeo as the Secretary of Topic5 (0.39) r hawkish and pro-war behavior in KialoAttr (0.05)

wacacussebdacokfedsexbuyalthmeisRcoenpduucbtlicaaninndiPtsaireatyxluehavleynapstsohaoutulhtgheofhseemswinoaarssbout the characteristics of sen-income to provide services

ts

and

actions,

the

UoSnheassucKicaleoEsxstrf(u-0l.07c) hallenger

attackesdimthpley

hbeycpaousthe ehteicwaals

a

Retepunbcliecsant.haTthicsaanlsobaelloswus cKKciiaaelloosFFsrreefqqu((00l..l71y24))

attacked,

the same
establish-universal

opportunities. Ev health care,

outcomes in sentences aggressive stance in foreign policy, KialoFreq (0.75)
of sixty Russian diplomats following Topic5 (0.39)

4

and

5,

vwpoitteohrsoinutottibrneesgleaarzcoyh,uinagtstthmheaanvtyaluweilslisnaingmd pcclhyaauvroastceatfeloirrtyothf etihiner

party Prediction (0.12)
paersconredible manner

would

require(affordable

collage),

a

l

e United Kingdom. Also, despite Example (0.11)

KialoExtr (-0.08)

hour), and not to get s

th Kim Jong-Un concerntinhgosthee outcomes KialoAttr (0.07) are not plausible,tahenyd atrheevLotRingmfoord. elOur Coandgrdesrseissssinlogw canonineffoﬁcuienntdeCromsp,asrisuonc(h0.20a)s the challenger’sinterested in capital then

the

US,

and

NK's

also captures the nuclear arsenal,

KialoExtr (-0.07) Topic5 (0.39)

use

of

because Democrats
hypothetfioccaulseadndon thoeppwosoinrgd

and Republicans are more
one raenpotuhetratthiaon nth(eMy aanrezooon r

Teoptica39l.(-,0.122)020)

and

their fair
persuasive(wages or

share back to taxes), and it

th sh

ed out his cabinet/diplomacy team

favor

of

things

such

as

acroeguimled

as highly KialoFreq (0.22)
KialoAttr (0.13)

indicative

of

attdaevcelkopainbgilaictytu.alMsoloutrioens abortion. It is the job of

to issues like gun control and
elecstekd iolflﬁcriaelsﬂteo cretperedsenitnALthL eofir KaiatltoaFrceqk(0(.8T6)an

et

al.,

2014).

Wegovernment to make sure speak about socialism they

h Korea, further stirring things up Hypothetical (-0.06)

KialoAttr (0.20)

. If talks between the twosnuatciocnsesKsifauloElxtar n(-0d.11e) rroneous cases atrhee ipneoAplpepofetnhediridxistHric.t/state/cloeuantvrye, ntohtijsusatnthaelypseiosptleo future work.

(Soviet Union). I believ nations are a weak constit

oes not have much more of a KialoFreq (0.45)

that voted for them or agree with them, and following the ideals Use of "We" (-0.19)

attacking North Korea, which iLs aa yTpoepioc5p(0l.3e9)perform signiﬁcanotlfya pboelittticearl tpharatyndothesenot allowOfour rthisw. oPorlikticacl poaurtlides bKeialoiFmreqp(r0.o23v) ed

also

by

includ-revolution instead of a economic policy and can

favorable s is also

BERT among higher ofﬁcials.
sort of a proxy scufﬂe

KialoAttr (0.26)
model for KialoExtr (-0.05)

predicting

attackfbooerctdehussineetnofﬁtcteihennintckaenisnd, tbeirnumapstporofprbialitanecgkforadniisdsscwuoehusitrets,heaat npadfrfeotchtpisethisretieKisalo(Acttor (h0.1e4r) ence,

cohesiveness)c.urrent

governing

body.

I

/Russia,

attacking

or

otherwise Use of "We" (-0.18)
only coTompicp5a(0r.3a9)bly well

for

successefnutlirleycaoutntatryc. keAdlsos,emn-any

KialoExtr (-0.12)
yFouungrtvhoteerrs,daorngotutmhineknttahitsionKiaslotFrreuqc(0t.u23r) e

(support

relationscountry's sudo-socialist are a perfect example of

i h

r Russia could lead to situations
QuestOther (0.39)

way--many Americans are becoming disenfranchised with the KialoAttr (0.22)

people. B.T.W. this is m

tences (Table 4 ide economic downturn to nuclear
nﬂict the current trajectory of Why/How (0.91)

row

9).

Persuasievnetiraergpoulitmicalesnysttaemti.onThis

isbaentowutdeaetend ssysetenmt,eanndceeitsheor rNloarmcaktiveth(0.e18r)eof)

might

providereasonable debates with you

How would we in otherwise nCotMUVse orfe"Wqeu" (i-0r.e18s) substantial donomfetehadeisnpteokoapndlaeop.tworlcehadnggeec,ompleutseleyftuolbienttfeor ﬁrtmthaetnioeendsabTKooiapuliocEt3x3et(r0a(.-00c8.1)h2) sentence’s attacka-

scufﬂe?

Topic37 (-0.21)

but laypeople do not have such expertise for many bility.

domains. The BERT model, however, seems to take
advantage of the large data and encodes useful lin- Acknowledgments

t3_87as7t)

guistic patterns that are predictive of attackability. A similar tendency has been observed in predict- This research was supported by the Kwanjeong ing persuasive refutation (Guo et al., 20E2rr0o),nweoheurse 2 (t3E_d9u5cawtiqo1n2al) Foundation.

riends with Amanda andaBamileay.chToipnice15-l(-e0.a18r)ned model outperformeI drealliazeyIpheavoepalbei.as because I grew up in a big city in KialoFreq (0.78)

References h of them on a platonic leveNl, beuvteI rtPheresloenasl s(-0,.2i0n) our task, the humans aCnadnatdhaeaBndEnRotTa single person I knew owned a gun and

Comparison (0.20) Topic43 (0.19)

interest in Bailey because she's Topic15 (-0.18)

most law enforcement ofﬁcers I saw on the street also didn't KialoAttr (0.13)

ot to say that Amanda is ugmly,ojudstelPesresoenaml (-0.t2o0) make similar decisioncasrr;y gtuhnseanadsI speorc-eive Canada to generally be safer than Use of "We" (-0.19)

her body structure. Another Topic15 (-0.18)

the open carry US state that I now live in. I see zero Personal (-0.20)

support this is when ycouiafteieol nPebrseontawl (-e0.e20n) their choices of senterneacseosn itos ohwingha ,gun, nAotriesvetnotfloer .h2un0t0in7g.. OI tnhinRkheTtoopirc4i3c(.0.1A9) Theory of Civic Dis-

lete stranger, because You know absolutely

nowofthitthinheigr odUTosdpeicso1f5"rY(a-o0ut.1i"8o()-0s.15r)anging

between

3.43hbuen(ettneorshpusnh1toinu)gldamnuysdseeblfo.wIs baenldicevaoreruotrhwesse.prIe(saTdermnacitneIs'ovlefagntueendvs einrbyToGpice43o(r0g.1e9)

Alexander

Kennedy).

ould envision a happy rela3tio.n3sh3ip (tUToospepico1f5"3Y(-o)0u..1"8()-I0.n15t)erestingly,

the

LR

msocoiedtyelmahkeasssoaciety

less

Normative (0.18)
Osafxe faonrddwUe nwiovueldrsailtlybPe reTospsic,4U3 (0S.1A9) .

ir looks. I feel this way because

safer if there were fewer of them and they were far more Personal (-0.20)

hanging out with my frienlodsw(oaf sKsioalocFiraeqti(0o.2n3) with the human decisidoifnﬁscufltoarndtoexppe1nsive to buy on the black market rather KialoFreq (0.75)

myself o don't

(OR=2.65), but the "wow we'd make such a Topic15 (-0.18)
feel the desire to enter a Use of "We" (-0.19)

association

than being able to pick
exceedlost utsihngecaBshEanRdTwith

oneMupaeracsilyBartya sgubnasehorwt, pAarkminyg
no background check. I know that

BCoemthpariWsona(r0r.2i0n) er,
Topic43 (0.19)

and

Victor

Ku-

Personal (-0.20)

violence can be committed wpitehromthearnw.e2a0po1n4s.suCchoanscrePrteednicetiosns(0r.1a2t)ings for 40 thousand

model for top 3 (OR=3.69). It would bkeniviens toerrreusnntiinnggsomeone goveenrewriathllay ckarn. owBunt wEengKliiaslohAttwr (0o.0r6d) lemmas. Behavior

to further examine the similarities andhadveiflfaewrseanbocuet swho can dRriveesaeacarrchanMd iet'tshaocdtusal,ly46KU(isa3elo)oE:fx9"tWr0(e-4"0.–(1-029.)119)1.

more difﬁcult to kill people with such things and less Topic43 (0.19)

efﬁcient.

Example (0.11)

KialoFreq (0.36) Topic32 (0.11) Use of "We" (-0.19)

Amparo Elizabeth Cano-Basave and Yulan He. 2016. A Study of the Impact of Persuasive Argumentation in Political Debates. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1405–1413, San Diego, California. Association for Computational Linguistics.
Tuhin Chakrabarty, Christopher Hidey, Smaranda Muresan, Kathy McKeown, and Alyssa Hwang. 2019. AMPERSAND: Argument Mining for PERSuAsive oNline Discussions. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2926–2936, Hong Kong, China. Association for Computational Linguistics.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. arXiv.
Amanda M Durik, M Anne Britt, Rebecca Reynolds, and Jennifer Storey. 2008. The Effects of Hedges in Persuasive Arguments: A Nuanced Analysis of Language. Journal of Language and Social Psychology, 27(3):217–234.
Naoki Egami, Christian J Fong, Justin Grimmer, Margaret E Roberts, and Brandon M Stewart. 2018. How to make causal inferences using texts. arXiv preprint arXiv:1802.02163.
Yunfan Gu, Zhongyu Wei, Maoran Xu, Hao Fu, Yang Liu, and Xuan-Jing Huang. 2018. Incorporating Topic Aspects for Online Comment Convincingness Evaluation. Proceedings of the 5th Workshop on Argument Mining, pages 97–104.
Zhen Guo, Zhe Zhang, and Munindar Singh. 2020. In Opinion Holders’ Shoes: Modeling Cumulative Inﬂuence for View Change in Online Argumentation. In Proceedings of The Web Conference 2020, WWW, page 2388–2399, New York, NY, USA. Association for Computing Machinery.
Ivan Habernal and Iryna Gurevych. 2016a. What makes a convincing argument? Empirical analysis and detecting attributes of convincingness in Web argumentation. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1214–1223, Austin, Texas. Association for Computational Linguistics.
Ivan Habernal and Iryna Gurevych. 2016b. Which argument is more convincing? Analyzing and predicting convincingness of Web arguments using bidirectional LSTM. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1589– 1599, Berlin, Germany. Association for Computational Linguistics.

Xinyu Hua, Zhe Hu, and Lu Wang. 2019. Argument Generation with Retrieval, Planning, and Realization. arXiv.
Ken Hyland. 2005. Metadiscourse : Exploring Interaction in Writing. Continuum Discourse Series. Continuum.
Lu Ji, Zhongyu Wei, Xiangkun Hu, Yang Liu, Qi Zhang, and Xuanjing Huang. 2018. Incorporating Argument-Level Interactions for Persuasion Comments Evaluation using Co-attention Model. In Proceedings of COLING , the th International Conference on Computational Linguistics, pages 1–12.
Yohan Jo and Alice H Oh. 2011. Aspect and Sentiment Uniﬁcation Model for Online Review Analysis. In Proceedings of the fourth ACM international conference on Web search and data mining, pages 815–824.
Yohan Jo, Shivani Poddar, Byungsoo Jeon, Qinlan Shen, Carolyn Penstein Rose´, and Graham Neubig. 2018. Attentive Interaction Model: Modeling Changes in View in Argumentation. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 103–116, New Orleans, Louisiana. Association for Computational Linguistics.
Zixuan Ke, Hrishikesh Inamdar, Hui Lin, and Vincent Ng. 2019. Give Me More Feedback II: Annotating Thesis Strength and Related Attributes in Student Essays. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3994–4004, Florence, Italy. Association for Computational Linguistics.
Emaad Manzoor, George H. Chen, Dokyun Lee, and Michael D. Smith. 2020. Inﬂuence via ethos: On the persuasive power of reputation in deliberation online. arXiv preprint arXiv:2006.00707.
Gaku Morio, Ryo Egawa, and Katsuhide Fujita. 2019. Revealing and Predicting Online Persuasion Strategy with Elementary Units. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 6273–6278, Hong Kong, China. Association for Computational Linguistics.
Elena Musi. 2017. How did you change my view? A corpus-based study of concessions’ argumentative role. Discourse Studies, 20(2):270–288.
Timothy Niven and Hung-Yu Kao. 2019. Probing Neural Network Comprehension of Natural Language Arguments. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, ACL, pages 4658 – 4664.

E Michael Nussbaum, Ian J Dove, Nathan Slife, CarolAnne M Kardash, Reﬁka Turgut, and David Vallett. 2018. Using critical questions to evaluate written and oral arguments in an undergraduate general education seminar: a quasi-experimental study. Reading and Writing, 19(2):1–22.

Nils Reimers, Benjamin Schiller, Tilman Beck, Johannes Daxenberger, Christian Stab, and Iryna Gurevych. 2019. Classiﬁcation and Clustering of Arguments with Contextualized Word Embeddings. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 567– 578, Florence, Italy. Association for Computational Linguistics.

Michael Ringgaard, Rahul Gupta, and Fernando C. N. Pereira. 2017. SLING: A framework for frame semantic parsing. CoRR, abs/1710.07032.

Sara Rosenthal, Noura Farra, and Preslav Nakov. 2017. SemEval-2017 Task 4: Sentiment Analysis in Twitter. In Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017), pages 502–518, Vancouver, Canada. Association for Computational Linguistics.

Yi Song, Paul Deane, and Beata Beigman Klebanov. 2017. Toward the Automated Scoring of Written Arguments: Developing an Innovative Approach for Annotation. ETS Research Report . . . , 152(3):157.

Chenhao Tan, Lillian Lee, and Bo Pang. 2014. The effect of wording on message propagation: Topic- and author-controlled natural experiments on Twitter. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 175–185, Baltimore, Maryland. Association for Computational Linguistics.

Chenhao Tan, Vlad Niculae, Cristian DanescuNiculescu-Mizil, and Lillian Lee. 2016. Winning Arguments: Interaction Dynamics and Persuasion Strategies in Good-faith Online Discussions. In Proceedings of the 25th International Conference on World Wide Web, pages 613–624. International World Wide Web Conferences Steering Committee.

URL1.

http://www-di.inf.puc-rio.br/

˜endler/students/Hedging_Handout.pdf.

URL2. https://github.com/words/hedges/ blob/master/data.txt.

URL3.

https://www.janefriedman.com/

hedge-word-inflation-words-prune/.

Henning Wachsmuth, Nona Naderi, Ivan Habernal, Yufang Hou, Graeme Hirst, Iryna Gurevych, and Benno Stein. 2017a. Argumentation Quality Assessment: Theory vs. Practice. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 250–255, Vancouver, Canada. Association for Computational Linguistics.

Henning Wachsmuth, Nona Naderi, Yufang Hou, Yonatan Bilu, Vinodkumar Prabhakaran, Tim Alberdingk Thijm, Graeme Hirst, and Benno Stein. 2017b. Computational Argumentation Quality Assessment in Natural Language. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 176–187, Valencia, Spain. Association for Computational Linguistics.
Henning Wachsmuth, Shahbaz Syed, and Benno Stein. 2018. Retrieval of the Best Counterargument without Prior Topic Knowledge. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 241–251. Association for Computational Linguistics.
Douglas Walton, Chris Reed, and Fabrizio Macagno. 2008. Argumentation Schemes. Cambridge University Press.
Lu Wang, Nick Beauchamp, Sarah Shugars, and Kechen Qin. 2017. Winning on the Merits: The Joint Effects of Content and Style on Debate Outcomes. Transactions of Association of Computational Linguistics, 5:219–232.
Amy Beth Warriner, Victor Kuperman, and Marc Brysbaert. 2013. Norms of valence, arousal, and dominance for 13,915 English lemmas. Behavior Research Methods, 45(4):1191–1207.
Zhongyu Wei, Yang Liu, and Yi Li. 2016. Is This Post Persuasive? Ranking Argumentative Comments in Online Forum. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 195– 200, Berlin, Germany. Association for Computational Linguistics.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005. Recognizing Contextual Polarity in PhraseLevel Sentiment Analysis. In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, pages 347–354, Vancouver, British Columbia, Canada. Association for Computational Linguistics.
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Re´mi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. 2019. HuggingFace’s Transformers: State-of-the-art Natural Language Processing. arXiv.

A Annotating Attacked Sentences
We tried capturing sentences in posts that are addressed by comments but not directly quoted. To see its feasibility, we randomly sampled 100 post-comment pairs that do not contain direct quotes and then asked an undergraduate native speaker of English (who has no knowledge about this work) to mark attacked sentences in each post, if any. This revealed two challenges. First, human annotation is subjective when compared to a co-author’s result and very time-consuming (2.5 min/comment). Second, we tried several methods to automatically identify attacked sentences. We compared the similarity between each post sentence with the comment (ﬁrst sentence of the comment, ﬁrst sentence of each paragraph, or all comment text) based on word overlap with/without synonym expansion and the GloVe embeddings. But it turned out to be difﬁcult to get similar results to human annotations. Therefore, we decided to use only those sentences that are direct quoted or have at least 4 common words with a comment’s sentence as the most reliable labels.

B External Knowledge
In this section, we describe the methods that we explored to use Kialo as a knowledge base but that were not successful.
B.1 UKP Sentence Embedding-Based Retrieval
We measured the similarity between CMV sentences and Kialo statements using the UKP sentence embedding—BERT embeddings ﬁne-tuned to measure argument similarity (Reimers et al., 2019). Specifically, the authors provide pretrained embeddings constructed by appending a ﬁnal softmax layer to BERT to predict a numerical dissimilarity score between 0 and 1 for each sentence pair in the UKP ASPECT corpus. The 3,595 sentence pairs in this corpus were drawn from 28 controversial topics and annotated via crowd workers to be “unrelated” or of “no”, “some” or “high” similarity. They report a mean F1-score of 65.39% on a held-out subset of this corpus, which was closest to human performance (F1=78.34%) among all competing methods that were not provided with additional information about the argument topic.
We used this ﬁne-tuned model to measure the dissimilarity between each CMV sentence and Kialo statements. Based on this information, we extracted the feature UKP Avg Distance 10, which is the average dissimilarity score of the 10 Kialo statements that are closest to the sentence. This score is expected to be low if a sentence has many similar statements in Kialo. In addition, we extracted the same frequency, attractiveness, and extremeness features as in §4.2.2. Here, we determine whether a CMV sentence and a Kialo statement are “matched” based on several dissimilarity thresholds (0.1, 0.2, 0.3, 0.4); A Kialo statement is considered matched with a CMV sentence if the dissimilarity is below the selected threshold.
B.2 Semantic Frame-Based Knowledge
We extracted semantic frames from CMV sentences and Kialo statements, using Google SLING (Ringgaard et al., 2017). For each frame in a sentence or statement, a “knowledge piece” is deﬁned as the concatenation of the predicate and arguments (except negation); the predicate is lemmatized and the arguments are stemmed to remove differences in verb/noun forms. We also mark each knowledge piece as negated if the frame contains negation. Example knowledge pieces include:
• ARG0:peopl-ARG1:right-ARGM-MOD:should-PRED:have (Negation: true) • ARG1:person-ARG2:abl-ARGM-MOD:should-PRED:be (Negation: false) For each CMV sentence, we extracted two features: the count of knowledge pieces in Kialo that are consistent with those in the sentence, and the count of knowledge pieces in Kialo that are conﬂicting with those in the sentence. Two knowledge pieces are considered consistent if they are identical, and conﬂicting if they are identical but negated. Attackable sentences are expected to have many consistent and conﬂicting knowledge pieces in Kialo. If we assume that most statements in Kialo are truthful, attackable sentences may have more conﬂicting knowledge pieces than consistent knowledge pieces.
B.3 Word Sequence-Based Knowledge
Treating each frame as a separate knowledge piece does not capture the dependencies between multiple predicates within a sentence. Hence, we tried a simple method to capture this information, where a knowledge pieces is deﬁned as the concatenation of verbs, nouns, adjectives, modal, prepositions, subordinating conjunctions, numbers, and existential there within a sentence; but independent clauses (e.g., a because clause) were separated off. All words were lemmatized. Each knowledge piece is negated if the source text has negation words. Example knowledge pieces include:
• gender-be-social-construct (Negation: true) • congress-shall-make-law-respect-establishment-of-religion-prohibit-free-exercise (Negation: false) For each CMV sentence, we extracted the same two features as in semantic frame-based knowledge pieces: the count of knowledge pieces in Kialo that are consistent with those in the sentence, and the count of knowledge pieces in Kialo that are conﬂicting with those in the sentence.

B.4 Effects and Statistical Signiﬁcance
The effects and statistical signiﬁcance of the above features were estimated in the same way as §5 and are shown in Table 5. Word sequence-based knowledge has no effect, probably because not many knowledge pieces are matched. Most of the other features have signiﬁcant effects only for “Attacked”. We speculate that a difﬁculty comes from the fact that both vector embedding-based matching and frame-based matching are inaccurate in many cases. UKP sentence embeddings often retrieve Kialo statements that are only topically related to a CMV sentence. Similarly, frame-based knowledge pieces often cannot capture complex information conveyed in a CMV sentence. In contrast, word overlap-based matching seems to be more reliable and better retrieve Kialo statements that have similar content to a CMV sentence.

Knowledge Feature

Word Overlap Frequency (log2) Word Overlap Attractiveness (log2) Word Overlap Extremeness

UKP UKP UKP UKP UKP UKP UKP UKP UKP UKP UKP UKP UKP

Avg Distance 10‡ 0.1 Frequency† 0.1 Attractiveness†
0.1 Extremeness 0.2 Frequency† 0.2 Attractiveness†
0.2 Extremeness 0.3 Frequency† 0.3 Attractiveness†
0.3 Extremeness 0.4 Frequency† 0.4 Attractiveness†
0.4 Extremeness

Frame Knowledge Consistent Frame Knowledge Conﬂict

Word Sequence Knowledge Consistent Word Sequence Knowledge Conﬂict

Attacked
1.18 (***) 1.30 (***) 1.51 (***)
0.93 (***) 1.08 ( * ) 1.11 ( * ) 3.49 ( * ) 1.02 ( ** ) 1.05 (***) 1.69 (***) 1.04 (***) 1.09 (***) 2.44 (***) 1.04 (***) 1.12 (***) 2.35 (***)
1.28 (***) 1.37 (***)
1.05 ( ) 1.18 ( )

Successful
1.07 (***) 1.18 (***) 1.19 (***)
0.98 ( * ) 0.99 ( ) 1.08 ( ) 6.77 ( ) 1.01 ( ) 1.06 ( ) 1.76 ( ) 1.01 ( ) 1.02 ( ) 1.40 ( ) 1.01 ( ** ) 1.01 ( ) 1.02 ( )
1.01 ( ) 1.08 ( )
0.98 ( ) 1.49 ( )

Table 5: Odds ratio (OR) and statistical signiﬁcance of features. An effect is positive (blue) if OR > 1 and negative (red) if OR < 1. (†: log2, ‡: standardized / *: p < 0.05, **: p < 0.01, ***: p < 0.001)

C Lexicons
Table 6 shows the lexicons and regular expressions used in feature extraction. r"pattern" represents a regular expression.

Feature Question - Confusion Question - Why/How Question - Other Normative Prediction Hypothetical Citation
Comparison Examples Deﬁnition Personal Story
Use of You Use of We Subjectivity Concreteness Hedges
Qualiﬁcation
Arousal Dominance

Pattern
r"(ˆ| )i (\S + ){,2}(not|n’t|never) (understand|know)", r"(not|n’t) make sense", r"(ˆ| )i (\S + ){,2}(curious|confused)", r"(ˆ| )i (\S + ){,2}wonder", r"(me|myself) wonder"
r"(ˆ| )(why|how).*\?" ? should, must, “(have|has) to”, “have got to”, “’ve got to”, gotta, need, needs r"(am$|$’m$|$are$|$’re$|$is$|$’s) (not )?(going to$|$gonna)", will, won’t, would, shall
r"(ˆ|, )if|unless" r" {PATTERN} that [ˆ.,!?]" (PATTERN: said, reported, mentioned, declared, claimed, admitted, explained, insisted, promised, suggested, recommended, denied, blamed, apologized, agreed, answered, argued, complained, conﬁrmed, proposed, replied, stated, told, warned, revealed), according to, r"https?:" than, compared to
r"(ˆ| )(for example|for instance|such as|e\.g\.)( |$)" deﬁne, deﬁnition Epistemic verbs: think, believe, see, know, feel, say, understand, mean, sure, agree, argue, consider, guess, realize, hope, support, aware, disagree, post, mention, admit, accept, assume, convince, wish, appreciate, speak, suppose, doubt, explain, wonder, discuss, view, suggest, recognize, respond, acknowledge, clarify, state, sorry, advocate, propose, deﬁne, apologize, curious, ﬁgure, claim, concede, debate, list, oppose, describe, suspect, reply, bet, realise, defend, convinced, offend, concern, intend, certain, conclude, reject, challenge, thank, condone, value, skeptical, contend, anticipate, maintain, justify, recommend, conﬁdent, promise, guarantee, comment, unsure, elaborate, posit, swear, dispute, imply, misunderstand. Epistemic nouns: view, opinion, mind, point, argument, belief, post, head, position, reasoning, understanding, thought, reason, question, knowledge, perspective, idea, way, stance, vote, best, cmv, response, deﬁnition, viewpoint, example, claim, logic, conclusion, thinking, comment, statement, theory, bias, assumption, answer, perception, intention, contention, word, proposal, thesis, interpretation, reply, guess, evidence, explanation, hypothesis, assertion, objection, criticism, worldview, impression, apology, philosophy you, your, yours
r"(ˆ| )we |(?<!the) (us|our|ours)( |$)"
Wilson et al. (2005) Brysbaert et al. (2014) Downtoners (score=1): allegedly, apparently, appear to, conceivably, could be, doubtful, fairly, hopefully, i assume, i believe, i do not believe, i doubt, i feel, i do not feel, i guess, i speculate, i think, i do not think, if anything, imo, imply, in my mind, in my opinion, in my understanding, in my view, it be possible, it look like, it do not look like, kind of, mainly, may, maybe, might, my impression be, my thinking be, my understanding be, perhaps, possibly, potentially, presumably, probably, quite, rather, relatively, seem, somehow, somewhat, sort of, supposedly, to my knowledge, virtually, would. Boosters (score=-1): be deﬁnite, deﬁnitely, directly, enormously, entirely, evidently, exactly, explicitly, extremely, fundamentally, greatly, highly, in fact, incredibly, indeed, inevitably, intrinsically, invariably, literally, necessarily, no way, be obvious, obviously, perfectly, precisely, really, be self-evident, be sure, surely, totally, truly, be unambiguous, unambiguously, be undeniable, undeniably, undoubtedly, be unquestionable, unquestionably, very, wholly (Hyland, 2005; URL1; URL2) Qualiﬁers (score=1): a bit, a few, a large amount of, a little, a lot of, a number of, almost, approximately, except, generally, if, in general, largely, likely, lots of, majority of, many, more or less, most, mostly, much, nearly, normally, occasionally, often, overall, partly, plenty of, rarely, roughly, several, some, sometimes, tend, ton of, tons of, typically, unless, unlikely, usually. Generality words (score=-1): all, always, every, everybody, everyone, everything, never, no, no one, nobody, none, neither, not any, ever, forever (Hyland, 2005; URL2; URL3) Warriner et al. (2013) Warriner et al. (2013)

Table 6: Lexicons and regular expressions used in feature extraction.

D Statistical Model for Feature Effects
For each feature, we use the following logistic regression model:
P(Y = 1) log 1 − P(Y = 1) =β0 + βX X + α1D1 + · · · + α|D|D|D|,
where X is a continuous or binary explanatory variable that takes the value of a characteristic that we are interested in. Dd (d = 1, · · · , |D|) is a binary variable that takes 1 if the sentence belongs to the d-th domain. Y is a binary response variable that takes 1 if the sentence is attacked or if the sentence is attacked successfully. βX is the regression coefﬁcient of the characteristic X, which is the main value of our interest for examining the association between the characteristic and the response; exp (βX ) is the odds ratio (OR) that is interpreted as the change of odds (i.e., the ratio of the probability that a sentence is (successfully) attacked to the probability that a sentence is not (successfully) attacked) when the value of the characteristic increases by one unit. If βX is signiﬁcant, we can infer that X has an effect on Y. If βX is positive (and signiﬁcant), we can infer that the characteristic and the response have positive association, and vice versa.

E Important n-gram Features
Table 7 shows the top 100 n-grams that have the highest or lowest weights for attacked sentences (vs. unattacked sentences) and for successfully attacked sentences (vs. unsuccessfully attacked).

High Low

Attacked (vs. Unattacked)
is are no - ? life why women should to society men a nothing 1 ) would money if i they n’t people if *
someone 2 . human never believe 2 ) 3 . your i believe and 5 . americans tax 4 , being : - : * feel because * the
than could republicans do be government ) sex 3 ) nobody why should the government ” i seems religion
their ca ca n’t less 4 . pay world war an ) the 6 . without , why science 4 ) reason humans animals racism military selﬁsh racist of when social 3 gun makes you speech climate get kids have can white should i , is * **
proven how can
edit cmv i / ? / thanks ( edit : [ ! post ] ] ( this thank thank you comments please view — &gt; discussion here topic sorry changed my view some cmv . posts . ” my delta comment i will points responses : 1 . of you / ) article title i ’ll ’ll = thanks for now ’m &amp; got i ’m was ** edit above recently reddit view . lot i was below change my hi ’s a few edit 2 on this again “ ) . my view . this post discuss arguments you all deltas few there are 1 . i ’ve / ) — i have currently edit 2 :
comments . let me a lot hello let i still here . background course ) — context you guys appreciate
thread perspective and i posted

Attacked Successfully (vs. Unsuccessfully)
is without are ? would public life women weapons data how can usa no should if sex of . , would n’t why money % someone the us customers coffee since 1 :
skills are a end 3 . available , they technology 2 . - , if people with cost need a car the pretty much racist so many to know third such as white dog could be
towards the americans song actions seems formal , he gender is nothing this : power see teams job years videos rates why would cream expectations ca god people feet global i believe sounds n’t the 100
think that it crime to pay ﬁrstly because , why immoral and not can also scooby ” i issues % of ca n’t marriages ability in many
edit cmv i thanks / edit : view this thank ! 1 . deﬁnitely ] post discussion thank you some ’s a changed that this
here i have tv points today responses above , it ’s ] ( perspective both thought i was to any do this ( there are &gt; continue to currently : i delta comments certainly taxes my you can discuss matters person a please let me got , that not all ’m i ’m more of n’t want to obvious posts friends has been honest true . background great hypocritical case . work , account not the results article
bit all the that would be grow whose thread ﬁne . point . do you remember still hope now standard thanks for asking try to go started wealth = bitcoin
series arguments super does n’t

Table 7: n-grams (n = 1, 2, 3) with the highest/lowest weights. Different n-grams are split by a space, and words within an n-gram are split by “ ”.

F Reproducibility Checklist

Criterion Computing infrastructure
Average runtime
Number of parameters Validation performance
Bounds for hyperparameters
Hyperparameter conﬁgurations for bestperforming models Number of hyperparameter search trials Method of choosing hyperparameter values Criterion for selecting optimal hyperparameter values

LR
Intel(R) Core(TM) i7-3770K CPU @ 3.50GHz / 19GiB System memory
Attacked: 225.9 mins / Successful: 31.5 mins 20,105
Attacked: P@1=47.4, A@3=75.8, AUC=61.8 / Successful: P@1=26.5,
A@3=54.6, AUC=60.1
Norm: {L1, L2} / Regularization weight: {1e-4, 1e-3, 1e-2, 1e-1} Norm: L2 / Regularization weight:
1e-1 8
Grid search
AUC

Table 8: Reproducibility checklist.

BERT
Intel(R) Core(TM) i7-8700K CPU @ 3.70GHz / 31GiB System memory /
NVIDIA GP102 [TITAN Xp] Attacked: 279.5 mins / Successful:
43.4 mins 108M
Attacked: P@1=50.3, A@3=77.6, AUC=64.6 / Successful: P@1=28.3,
A@3=57.2, AUC=62.0
Learning rate: 1e-5 / Adam : 1e-8
Learning rate: 1e-5 / Adam : 1e-8
(No hyperparameter search) (No hyperparameter search)
(No hyperparameter search)

G Prediction Results
Table 9 shows the prediction accuracy with an additional metric mean average precision (MAP).

P@1

Random

35.9

Length

42.9

Logistic Regression 47.1

(×) Content

45.2

(×) Knowledge

47.0

(×) Prop Types

46.7

(×) Tone

47.0

BERT

49.6

Human

51.7

Attacked

Any@3

MAP

66.0

48.0

73.7

53.7

76.2

56.5

74.4

54.7

76.0

56.4

75.9

56.2

76.0

56.4

77.8

57.9

80.1

–

AUC
50.1 54.5
61.7 58.1 61.7 61.5 61.9 64.4
–

Successfully Attacked

P@1

Any@3

MAP

AUC

18.9

45.0

34.0

50.1

22.3

52.1

38.8

55.7

24.2

54.5

41.0

59.3

24.0

52.6

39.9

57.0

24.1

54.3

40.5

59.0

24.4

53.6

40.7

59.0

25.2

56.2

41.4

59.4

28.3

57.2

43.1

62.0

27.8

54.2

–

–

Table 9: Prediction accuracy.

H Visualization Examples
For the successful example in Figure 3a, the model ﬁnds evidence for the successfully attacked sentences 3 and 5 from the external knowledge source (Kialo). Although some of the other sentences (7–8) also match Kialo statements, the degree of match is relatively low, and the model determines that their n-grams reduce attackability (many, think, needs). Sentence 4 is properly found to have high attackability, since it makes a comparison and contains many n-grams predictive of attackability (because, Democrats, Republicans, opposing).
For the successful example in Figure 3b, topics play important roles for determining attackability. The topics of the successfully attacked sentences 2–4 all increase attackability, whereas the topics of other sentences 5–9 reduce attackability.
For the erroneous example in Figure 4a, all sentences have relatively little evidence for attackability/unattackability. The model determines sentence 5 to have relatively high attackability because of many n-grams that increase attackability (know, absolutely, nothing). On the other hand, the successfully attacked sentence 6 is assigned a low attackability score despite its match with Kialo statements, because its use of we, personal stories, and certain n-grams (many, times, and friends).
For the erroneous example in Figure 4b, the model ﬁnds sentence 4 to have high attackability because it matches with Kialo statements, makes a comparison and prediction, and certain n-grams (believe, presence, society, market). Sentence 5 is also assigned a relatively high attackability score due to its use of examples and certain n-grams (know, committed, weapons). However, these sentences were not successfully attacked. In contrast, the successfully attacked sentences 2–4 do not have strong enough evidence for attackability compared to their negatively signals, such as personal stories and n-grams own and I.

regime change or attacking North Korea, further stirring things up for a potential falling out. If talks between the two nations break down, the US does not have much more of a reason to withhold from attacking North Korea, which is a plan that seems to be favorable among higher ofﬁcials. Considering that this is also sort of a proxy scufﬂe between us and China/ Russia, attacking or otherwise provoking North Korea or Russia could lead to situations ranging from a worldwide economic downturn to nuclear holocaust. Is conﬂict the current trajectory of international relations? How would we otherwise not engage in some sort of scufﬂe?

Hypothetical (-0.06) KialoExtr (-0.11)
KialoFreq (0.45) Topic5 (0.39) KialoAttr (0.26) KialoExtr (-0.05) Use of "We" (-0.18)
Topic5 (0.39) QuestOther (0.39)
Why/How (0.91) Use of "We" (-0.18) Topic37 (-0.21)

nsidering optimistic r online or National
ecretary of behavior in
US has ign policy, following so, despite erning the ar arsenal, macy team s a regime things up wo nations more of a which is a r ofﬁcials. xy scufﬂe otherwise situations to nuclear ectory of rwise not

Prediction (0.12) Personal (-0.20) Topic37 (-0.21)
KialoFreq (0.98) Topic5 (0.39) KialoAttr (0.05) KialoExtr (-0.07)
KialoFreq (0.75) Topic5 (0.39) Example (0.11) KialoAttr (0.07) KialoExtr (-0.07)
Topic5 (0.39) KialoFreq (0.22) KialoAttr (0.13) Hypothetical (-0.06) KialoExtr (-0.11)
KialoFreq (0.45) Topic5 (0.39) KialoAttr (0.26) KialoExtr (-0.05) Use of "We" (-0.18)
Topic5 (0.39) QuestOther (0.39)
Why/How (0.91) Use of "We" (-0.18) Topic37 (-0.21)

ucceeding ut party
ndividual
oy Moore he was
f minors
so allows ir party
he person
nefﬁcient nd mBoairleey. bauret I oonnly prhoyl siacanlldy)
I'm not vtidAeLncLe otfo opmeopplelete eh.e idYeaolsu opuacrotiuesld hdeitrhilsooikss. afnfegcint gthoeut nwkowthwies'd irwe ittohetnhteer nd either e needs

Comparison (0.20)
Topic35 (-0.07)
KialoFreq (0.72) KialoFreq (0.14) Prediction (0.12) KialoExtr (-0.08)
CToompipca1r5is(-o0n.1(80).20) TPoeprisco3n9a(l-0(-.01.22)0) Topic15 (-0.18) Personal (-0.20) KialoFreq (0.86) KTioaploicA1t5tr((-00..2180)) Personal (-0.20) UUsseeooff""WYoeu""(-(0-0.1.195)) Topic15 (-0.18) KUiasleooFfre"qYo(u0."2(3-)0.15) KTioaploicA1t5tr((-00..1184)) KialoExtr (-0.12) KialoFreq (0.23) KTioaploicF1r5eq(-0(0.1.283) ) KUiasleooAftt"rW(0e."2(2-)0.19) NPoerrmsoantiavle(-(00..2108)) Topic33 (0.08) KialoExtr (-0.12)

Success 2 (t3_8g5ukh)

The last Presidential election (2016) and most succeeding elections have proven that elections are more about party afﬁliations than actual views or the character of the individual being elected. In one of the most extreme examples, Roy Moore was backed by the Republican Party even though he was accused of sexual misconduct and sexual assault of minors simply because he was a Republican. This also allows voters to be lazy, as many will simply vote for their party without researching the values and character of the person they are voting for. Our Congress is slow an inefﬁcient because Democrats and Republicans are more focused on opposing one another than they are on developing actual solutions to issues like gun control and abortion. It is the job of elected ofﬁcials to represent ALL of the people of their district/state/country, not just the people that voted for them or agree with them, and following the ideals of a political party does not allow for this. Political parties force us to think in terms of black and white, and this is both inefﬁcient and inappropriate for issues that affect the entire country. Also, many young voters do not think this way--many Americans are becoming disenfranchised with the entire political system. This is an outdated system, and either
nofetehdeSsptueoocapdclaeep.t sorsch3ang(etc3o_m8plketqeldy gtokb)etter ﬁt the needs

Comparison (0.20)
Topic35 (-0.07)
KialoFreq (0.72) KialoFreq (0.14) Prediction (0.12) KialoExtr (-0.08)
Comparison (0.20) Topic39 (-0.12)
KialoFreq (0.86) KialoAttr (0.20)
Use of "We" (-0.19)
KialoFreq (0.23) KialoAttr (0.14) KialoExtr (-0.12) KialoFreq (0.23) KialoAttr (0.22) Normative (0.18) Topic33 (0.08) KialoExtr (-0.12)

(a) Successful example 2.

I believe that socialism is an obvious and humanitarian
next step for the U.S. It should be the responsibility of
vastly successful people to provide a tiny fraction of their
income to provide services for people who were not given
the same opportunities. Everyone has the right to safety,
Erroneous 2 (t3_95wq12) universal health care, social security, education (affordable
collage), a livable minimum wage ($15 per hour), and not to
get screwed over by businesses more interested in capital then people. BuIsrineaelsiszeesI dhaovne'ta bgiiavsebetchaeuisre fIagirreshwarue pbainckatobigthceity community tihneCyalenaacdhaofafnodf n(wotagaessionrgtalexpeesrs)o, nanIdknitewshoowuneldd a be the respgounnsiabnilditymosft thlaewgoenvfeorrcnemmeennt toftﬁocemrsakIessauwre othnetyhe do. Whenstmreeat naylsopdeiodpnle't scpareraykgaubnosuatndsoIcipaelirscmeivethCeaynqaudoateto nations likgeetnheeraUl.lSy.Sb.eRs. a(feSrothvainetthUeniopne).n IcabreryliUevSesttahtaettthhaet I problems nwoitwh thlievseein.atiIosnseearzeearowreeaakscoonnstiotuotiwonnthaagt ustnem, nsot from a vioelveentforrehvuonlutitniogn. Iinthsitnekadhunotfersasphooluitlidcaulsoenbeow. s Socialism isaanndecaorrnoowmsi.c poIliacdymaitndI'vceannbeevueserd bineecnoopheurantitoinng with the cumrryesntelgf.ovIebrneilnigevbeotdhye. preIsebneclieevoef gtuhnsaitn smoacnieyty European mcaokuenstrsyo'sciseutdyol-essos csaiafeliasnt didweeaws o(ulilkde aullnbiveerssaafler healthcare) aifrtehearepwerefercet feewxaemr oplfethoefm haonwd sthoecyiawlisemre fcaarnmboere beneﬁcial todpifeﬁocpullet .anBd.Te.Wxp.enthsiivse itso mbyuyﬁrosntthpeobslatc,kI mcaanrk'tet wait to haverarethaesronthaabnlebdeeinbgateasblwe itthoypoiuckaloln!e up easily at a gun
show parking lot using cash and with no
background c(hbe)ckS.uccIekssnfouwl exthaamt vpiloele3n.ce can be
committed with other weapons such as knives or
running soFmiegounree 3ov: eSruwcicthesascfualr.exBaumt pwleesh.ave
laws about who can drive a car and it's actually more difﬁcult to kill people with such things and
less efﬁcient.

Topic5 (0.39)
Normative (0.18) Topic28 (0.16)
KialoFreq (0.93) KialoAttr (0.09) ToKpiiacl4o6Fr(e0q.0(60).78) KiCaolomEpxatrri(s-o0.n11(0).20)
Topic43 (0.19) ToKpiiacl3o0At(t-r0(.107.1)3) ToUpsice3o0f ("-W0.e1"7)(-0.19)
Personal (-0.20) KiTaolopAict4tr3((00..4149)) KialoFreq (0.23) ToTpoipci3c043(-0(0.1.179))
Normative (0.18) KiTaolopFicr4e3q ((00..1396)) KiPaelorsAottnra(l0(.-108.2) 0) Topic2 (-0.54)
KialoFreq (0.75) UsCeoomfp"aYroisuo"n(-(0.1205) PeTrospoinc4a3l ((-00..1290)) ToPpriecd9ic(-t1io.0n4()0.12)
KialoAttr (0.06) KialoExtr (-0.12) Use of "We" (-0.19) Topic43 (0.19) Example (0.11)
KialoFreq (0.36) Topic32 (0.11) Use of "We" (-0.19)

a big city w owned a w on the anada to
tate that I gun, not use bows hunting society l be safer far more

KialoFreq (0.78) Comparison (0.20) Topic43 (0.19) KialoAttr (0.13) Use of "We" (-0.19) Personal (-0.20)
Topic43 (0.19)
Topic43 (0.19) Normative (0.18)
Topic43 (0.19) Personal (-0.20)
KialoFreq (0.75)

4

Success 3 (t3_
I believe that socialism is next step for the U.S. It vastly successful people to income to provide services fo the same opportunities. Ever universal health care, social se collage), a livable minimum w get screwed over by businesse people. Businesses don't g community they leach off of be the responsibility of the g do. When many people sp nations like the U.S.S.R. (S problems with these nations from a violent revolution Socialism is an economic polic with the current governing b European country's sudo-s healthcare) are a perfect ex beneﬁcial to people. B.T.W wait to have reasonable debates

ion (0.12) al (-0.20) 7 (-0.21) eq (0.98) (0.39) tr (0.05) xtr (-0.07) eq (0.75) (0.39) le (0.11) tr (0.07) xtr (-0.07) (0.39) eq (0.22) tr (0.13) etical (-0.06) xtr (-0.11) eq (0.45) (0.39) tr (0.26) xtr (-0.05) "We" (-0.18) (0.39) Other (0.39) ow (0.91) "We" (-0.18) 7 (-0.21)
5 (-0.18) al (-0.20) 5 (-0.18) al (-0.20) 5 (-0.18) al (-0.20) "You" (-0.15) 5 (-0.18) "You" (-0.15) 5 (-0.18)
eq (0.23) 5 (-0.18) "We" (-0.19) al (-0.20)

elections have pPrreosivdeennt Tthrautmeplehacstiﬁollneds oaurtehims coarbeineatb/doiupltopmaacrytyteamComKpiaaloriFsroeqn ((00..2220))

afﬁliations than acwcthiuatnhaglpe evooirepwaltetsacwokrhinotghaNereocritnhh faKarvoaorecrat,oeffrutrhotihfnetgrhssetsiurircnihndgiavstihdaiunrageglsiumpe KHiyaplooAthtetrti(c0a.1l 3(-)0.06)

being elected. In foonr ea opof tethnetiaml foaslltinegxtoruemt. eIfetxalaksmbpetlwesee, nRtohye tMwoonraetions KialoExtr (-0.11)

was

backed

bybtrehaek

dRoewpnu, bthliecaUnS

Pdaoretys

envoet nhavtheomuguhchhemworaesof

Topic35 (-0.07) a KialoFreq (0.45)

accused of sexuarleamsoisncotondwuitchthoaldnfdromsaettxacukainlg aNsosrathulKt oorefa,mwihniochrsis a Topic5 (0.39)

simply

becauspelanhethawt asesema sRteopbuebflaivcoaranb.le
Considering that this is also

saoTmrthoionsfgaahlsigophroeaxrllyoofwsﬁcscuiafﬂlsKKe. iiaallKKooiiFFaallrrooeeAEqqxtt((trr00(..(071-0.242.0))65) )

voters to be lazyb, eatwseemn auns yanwdillCshiinma/pRluyssivao, teatftaocrkitnhgeiorr poatrhteyrwisPeredUiscetioofn"W(0e.1"2(-)0.18)

without researchpirnogvotkhineg NvoarlthuKesoreaanodr Rcuhsasiraaccotueldr loefadthteo spietrusaotnionKs ialTooEpxict5r ((-00.3.098) )

they are votinrgangfiongr. froOmura Cwoonrlgdwreisdse iesconsolmoicwdoawnntiunrenfﬁtociennutclear QuestOther (0.39)

because Demhoolcorcaatusst. andIs Rcoenpﬂuicbt litchaencsurreanrtetramjecotorrey oCf omWphayr/iHsoown((00.9.210) )

international relations? How would we otherwise noTtopUics3e9o(f-"0W.1e2")(-0.18)

focused on openpgoasgienign soomne esoratnooftshceurfﬂeth?an they are on Topic37 (-0.21)

developing actual solutions to issues like gun control and

abortion.

It is the job of elected ofﬁcials to represent ALL of

KialoFreq (0.86) KialoAttr (0.20)

the people of their district/state/country, not just the people

that voted for them or agree with them, and following the ideals Use of "We" (-0.19)

of a political party does not allow for this. Political parties
force us to think in terms of black and white, and this is
both inefﬁcient and inappropriate for issues that affect the
entire country. AElrsroo, nmeaonyusyo1ung(tv3o_ter8s7dao sn7ottt)hink this
way--many Americans are becoming disenfranchised with the

KialoFreq (0.23) KialoAttr (0.14) KialoExtr (-0.12)
KialoFreq (0.23) KialoAttr (0.22)

entire politSicoal lesyt'sstemsa.y I'Tmhigsooids afnrieonudtdsatewditshysAtemma,ndaandanedithBearileyN. orTmopaitciv1e5 ((-00..118)

needs to adI a'mptcomr cphataibnlgeewcitoh mbopthleotfetlhyetmo obneattpelartoﬁnticthleevenl, ebeudt IsonlyTopPice3rs3o(n0a.l0(8-0).20)

of the peoptalkee.a romantic interest in Bailey because she's (physicallyK) ialTooEpxict1r5(-(0-0.1.128))

my type. Not to say that Amanda is ugly, just that I'm not Personal (-0.20)

really into her body structure. Another piece of evidence to

Topic15 (-0.18) Personal (-0.20)

support this is when you feel attracted to a complete Use of "You" (-0.15)

stranger, because of their physical appearance. You Topic15 (-0.18)

know absolutely nothing about them yet, you could Use of "You" (-0.15)

envision a happy relationship with them just from their looks. Topic15 (-0.18)

I feel this way because many times when I'm hanging out KialoFreq (0.23) with my friends (of both genders) I think to myself "wow we'd Topic15 (-0.18)

Erroneous 2 (t3_95wq12) make such a good couple" but even so don't feel the desire to enter Use of "We" (-0.19)

a relationship with them.

Personal (-0.20)

(a) Erroneous example 1.

I realize I have a bias because I grew up in a big city in Canada and not a single person I knew owned a gun and most law enforcement ofﬁcers I saw on the street also didn't carry guns and I perceive Canada to generally be safer than the open carry US state that I now live in. I see zero reason to own a gun, not even for hunting. I think hunters should use bows and arrows. I admit I've never been hunting myself. I believe the presence of guns in society makes society less safe and we would all be safer if there were fewer of them and they were far more difﬁcult and expensive to buy on the black market rather than being able to pick one up easily at a gun show parking lot using cash and with no background check. I know that violence can be committed with other weapons such as knives or running someone over with a car. But we have laws about who can drive a car and it's actually more difﬁcult to kill people with such things and less efﬁcient.

KialoFreq (0.78) Comparison (0.20) Topic43 (0.19) KialoAttr (0.13) Use of "We" (-0.19) Personal (-0.20)
Topic43 (0.19)
Topic43 (0.19) Normative (0.18)
Topic43 (0.19) Personal (-0.20)
KialoFreq (0.75) Comparison (0.20) Topic43 (0.19) Prediction (0.12) KialoAttr (0.06) KialoExtr (-0.12) Use of "We" (-0.19)
Topic43 (0.19) Example (0.11)
KialoFreq (0.36) Topic32 (0.11) Use of "We" (-0.19)

(b) Erroneous example 2.

Figure 4: Erroneous examples.

4

dIevbeelolpiienvg eacthtautasl osocliuatiloins anbeoxtrtsiotne.p Iftoirs tthhee jUob.S
the people of their district
tvhaasttlvyotseducfocr ethsesmfuolr paegorep oinfcaompoelittoicparlopvairdtye dseorevsic
ftohrcee ussamtoe thopinpkoritnuntietiremss.

buontivherisnaelfﬁhceiaenltth acnadrei,nsaop ecnotlliaregec)o, uantrliyv. abAlelsmoi,nimma wgaeyt--mscraenwyedAomveerricbyanbsusai epnetoiprele.poliBticuaslinsyesstseems. doT nceoemdmsutnoitaydtahpteoyr clheaancghe ocf
of the people.
be the responsibility of

do. When many peo

nations like the U.S.S

problems with these na

from a violent revol

Socialism is an economi

with the current govern

EheualrtohcpaerEea)rnraocroenuenatoryp'usersfse

beneﬁcial to people. B

wait

to

I realize I have have reasonable

a d

in Canada and

gun and most la

street also didn't

generally be s

now live in. I s

even for huntin

and arrows.

myself. I belie

makes society

if there were f

difﬁcult and e

rather than being

show parkin

background c

committed wi

running someo

laws about who

more difﬁcult

less efﬁcient.

