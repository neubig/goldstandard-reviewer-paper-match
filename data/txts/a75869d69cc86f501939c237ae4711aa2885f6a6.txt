Meta-Learning for Low-Resource Neural Machine Translation
Jiatao Gu*†, Yong Wang*†, Yun Chen†, Kyunghyun Cho‡ and Victor O.K. Li†
†The University of Hong Kong ‡New York University, CIFAR Azrieli Global Scholar †{jiataogu, wangyong, vli}@eee.hku.hk
†
yun.chencreek@gmail.com
‡
kyunghyun.cho@nyu.edu

arXiv:1808.08437v1 [cs.CL] 25 Aug 2018

Abstract
In this paper, we propose to extend the recently introduced model-agnostic meta-learning algorithm (MAML, Finn et al., 2017) for lowresource neural machine translation (NMT). We frame low-resource translation as a metalearning problem, and we learn to adapt to low-resource languages based on multilingual high-resource language tasks. We use the universal lexical representation (Gu et al., 2018b) to overcome the input-output mismatch across different languages. We evaluate the proposed meta-learning strategy using eighteen European languages (Bg, Cs, Da, De, El, Es, Et, Fr, Hu, It, Lt, Nl, Pl, Pt, Sk, Sl, Sv and Ru) as source tasks and ﬁve diverse languages (Ro, Lv, Fi, Tr and Ko) as target tasks. We show that the proposed approach signiﬁcantly outperforms the multilingual, transfer learning based approach (Zoph et al., 2016) and enables us to train a competitive NMT system with only a fraction of training examples. For instance, the proposed approach can achieve as high as 22.04 BLEU on Romanian-English WMT’16 by seeing only 16,000 translated words (∼ 600 parallel sentences).
1 Introduction
Despite the massive success brought by neural machine translation (NMT, Sutskever et al., 2014; Bahdanau et al., 2015; Vaswani et al., 2017), it has been noticed that the vanilla NMT often lags behind conventional machine translation systems, such as statistical phrase-based translation systems (PBMT, Koehn et al., 2003), for low-resource language pairs (see, e.g., Koehn and Knowles, 2017). In the past few years, various approaches have been proposed to address this issue. The ﬁrst attempts at tackling this problem exploited the availability of monolingual corpora (Gulcehre
* Equal contribution.

et al., 2015; Sennrich et al., 2015; Zhang and Zong, 2016). It was later followed by approaches based on multilingual translation, in which the goal was to exploit knowledge from high-resource language pairs by training a single NMT system on a mix of high-resource and low-resource language pairs (Firat et al., 2016a,b; Lee et al., 2016; Johnson et al., 2016; Ha et al., 2016b). Its variant, transfer learning, was also proposed by Zoph et al. (2016), in which an NMT system is pretrained on a high-resource language pair before being ﬁnetuned on a target low-resource language pair.
In this paper, we follow up on these latest approaches based on multilingual NMT and propose a meta-learning algorithm for low-resource neural machine translation. We start by arguing that the recently proposed model-agnostic meta-learning algorithm (MAML, Finn et al., 2017) could be applied to low-resource machine translation by viewing language pairs as separate tasks. This view enables us to use MAML to ﬁnd the initialization of model parameters that facilitate fast adaptation for a new language pair with a minimal amount of training examples (§3). Furthermore, the vanilla MAML however cannot handle tasks with mismatched input and output. We overcome this limitation by incorporating the universal lexical representation (Gu et al., 2018b) and adapting it for the meta-learning scenario (§3.3).
We extensively evaluate the effectiveness and generalizing ability of the proposed meta-learning algorithm on low-resource neural machine translation. We utilize 17 languages from Europarl and Russian from WMT as the source tasks and test the meta-learned parameter initialization against ﬁve target languages (Ro, Lv, Fi, Tr and Ko), in all cases translating to English. Our experiments using only up to 160k tokens in each of the target task reveal that the proposed meta-learning approach outperforms the multilingual translation

approach across all the target language pairs, and the gap grows as the number of training examples decreases.
2 Background
Neural Machine Translation (NMT) Given a source sentence X = {x1, ..., xT }, a neural machine translation model factors the distribution over possible output sentences Y = {y1, ..., yT } into a chain of conditional probabilities with a leftto-right causal structure:
T +1
p(Y |X; θ) = p(yt|y0:t−1, x1:T ; θ), (1)
t=1
where special tokens y0 ( bos ) and yT +1 ( eos ) are used to represent the beginning and the end of a target sentence. These conditional probabilities are parameterized using a neural network. Typically, an encoder-decoder architecture (Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015) with a RNN-based decoder is used. More recently, architectures without any recurrent structures (Gehring et al., 2017; Vaswani et al., 2017) have been proposed and shown to speed up training while achieving state-of-the-art performance.
Low Resource Translation NMT is known to easily over-ﬁt and result in an inferior performance when the training data is limited (Koehn and Knowles, 2017). In general, there are two ways for handling the problem of low resource translation: (1) utilizing the resource of unlabeled monolingual data, and (2) sharing the knowledge between low- and high-resource language pairs. Many research efforts have been spent on incorporating the monolingual corpora into machine translation, such as multi-task learning (Gulcehre et al., 2015; Zhang and Zong, 2016), back-translation (Sennrich et al., 2015), dual learning (He et al., 2016) and unsupervised machine translation with monolingual corpora only for both sides (Artetxe et al., 2017b; Lample et al., 2017; Yang et al., 2018).
For the second approach, prior researches have worked on methods to exploit the knowledge of auxiliary translations, or even auxiliary tasks. For instance, Cheng et al. (2016); Chen et al. (2017); Lee et al. (2017); Chen et al. (2018) investigate the use of a pivot to build a translation path between two languages even without any directed resource. The pivot can be a third language or even an image in multimodal domains. When pivots are

not easy to obtain, Firat et al. (2016a); Lee et al. (2016); Johnson et al. (2016) have shown that the structure of NMT is suitable for multilingual machine translation. Gu et al. (2018b) also showed that such a multilingual NMT system could improve the performance of low resource translation by using a universal lexical representation to share embedding information across languages.
All the previous work for multilingual NMT assume the joint training of multiple high-resource languages naturally results in a universal space (for both the input representation and the model) which, however, is not necessarily true, especially for very low resource cases.
Meta Learning In the machine learning community, meta-learning, or learning-to-learn, has recently received interests. Meta-learning tries to solve the problem of “fast adaptation on new training data.” One of the most successful applications of meta-learning has been on few-shot (or oneshot) learning (Lake et al., 2015), where a neural network is trained to readily learn to classify inputs based on only one or a few training examples. There are two categories of meta-learning:
1. learning a meta-policy for updating model parameters (see, e.g., Andrychowicz et al., 2016; Ha et al., 2016a; Mishra et al., 2017)
2. learning a good parameter initialization for fast adaptation (see, e.g., Finn et al., 2017; Vinyals et al., 2016; Snell et al., 2017).
In this paper, we propose to use a meta-learning algorithm for low-resource neural machine translation based on the second category. More speciﬁcally, we extend the idea of model-agnostic metalearning (MAML, Finn et al., 2017) in the multilingual scenario.
3 Meta Learning for Low-Resource Neural Machine Translation
The underlying idea of MAML is to use a set of source tasks T 1, . . . , T K to ﬁnd the initialization of parameters θ0 from which learning a target task T 0 would require only a small number of training examples. In the context of machine translation, this amounts to using many high-resource language pairs to ﬁnd good initial parameters and training a new translation model on a low-resource language starting from the found initial parame-

Fast Adaptation

MetaNMT

Emb
initialize

X_train

Meta Learning

Meta-Train

Meta-Test

X_test

Emb

NMT

Loss

Y_train

Y_test

Tk

Loss

query
Universal Lexical Representation

Translation Task Generator

Forward Pass Gradient Pass

Meta Gradient Pass Parameter Tying

Figure 1: The graphical illustration of the training process of the proposed MetaNMT. For each episode, one task (language pair) is sampled for meta-learning. The boxes and arrows in blue are mainly involved in language-speciﬁc learning (§3.1), and those in purple in meta-learning (§3.2).

ters. This process can be understood as θ∗ = Learn(T 0; MetaLearn(T 1, . . . , T K )).

That is, we meta-learn the initialization from auxiliary tasks and continue to learn the target task. We refer the proposed meta-learning method for NMT to MetaNMT. See Fig. 1 for the overall illustration.

3.1 Learn: language-speciﬁc learning Given any initial parameters θ0 (which can be either random or meta-learned),
the prior distribution of the parameters of a desired NMT model can be deﬁned as an isotropic Guassian:
θi ∼ N (θi0, 1/β),

where 1/β is a variance. With this prior distri-
bution, we formulate the language-speciﬁc learning process Learn(DT ; θ0) as maximizing the logposterior of the model parameters given data DT :

Learn(DT ; θ0) = arg max LDT (θ)
θ

= arg max

log p(Y |X, θ) − β θ − θ0 2,

θ

(X,Y )∈DT

where we assume p(X|θ) to be uniform. The ﬁrst term above corresponds to the maximum likelihood criterion often used for training a usual NMT system. The second term discourages the newly learned model from deviating too much from the initial parameters, alleviating the issue of overﬁtting when there is not enough training data. In practice, we solve the problem above by maximizing the ﬁrst term with gradient-based optimization and early-stopping after only a few update steps.

Thus, in the low-resource scenario, ﬁnding a good initialization θ0 strongly correlates the ﬁnal performance of the resulting model.
3.2 MetaLearn We ﬁnd the initialization θ0 by repeatedly simulating low-resource translation scenarios using auxiliary, high-resource language pairs. Following Finn et al. (2017), we achieve this goal by deﬁning the meta-objective function as

L(θ) =EkEDT k ,DT k (2)





 log p(Y |X; Learn(DT k ; θ)) ,

(X,Y

)∈D
T

k

where k ∼ U({1, . . . , K}) refers to one metalearning episode, and DT , DT follow the uniform distribution over T ’s data.
We maximize the meta-objective function using stochastic approximation (Robbins and Monro, 1951) with gradient descent. For each episode, we uniformly sample one source task at random, T k. We then sample two subsets of training examples independently from the chosen task, DT k and DT k . We use the former to simulate languagespeciﬁc learning and the latter to evaluate its outcome. Assuming a single gradient step is taken only the with learning rate η, the simulation is:
θk = Learn(DT k ; θ) = θ − η∇θLDT k (θ).

Once the simulation of learning is done, we evalu-
ate the updated parameters θk on DT k , The gradient computed from this evaluation, which we
refer to as meta-gradient, is used to update the

Ro Lv

Ro
AEs Lv

Fr A Es Pt

(a) Transfer Learning

(b) Multilingual Transfer Learning

Ro Lv

Fr Es
A
Pt

(c) Meta Learning

Figure 2: An intuitive illustration in which we use solid lines to represent the learning of initialization, and dashed lines to show the path of ﬁne-tuning.

meta model θ. It is possible to aggregate multiple episodes of source tasks before updating θ:

θ ← θ − η ∇θLDT k (θk),
k

where η is the meta learning rate. Unlike a usual learning scenario, the resulting
model θ0 from this meta-learning procedure is not necessarily a good model on its own. It is however a good starting point for training a good model using only a few steps of learning. In the context of machine translation, this procedure can be understood as ﬁnding the initialization of a neural machine translation system that could quickly adapt to a new language pair by simulating such a fast adaptation scenario using many high-resource language pairs.
Meta-Gradient We use the following approximation property

∇(x + νv) − ∇(x) H(x)v ≈
ν to approximate the meta-gradient:1

∇θLD (θ ) = ∇θ LD (θ )∇θ(θ − η∇θLD(θ))

= ∇θ LD (θ ) − η∇θ LD (θ )Hθ(LD(θ))

≈ ∇θ LD (θ ) − η ∇θLD(θ) − ∇θLD(θ) ,

ν

θˆ

θ

where ν is a small constant and

θˆ = θ + ν∇θ LD (θ ).

In practice, we ﬁnd that it is also possible to ignore the second-order term, ending up with the following simpliﬁed update rule:

∇θLD (θ ) ≈ ∇θ LD (θ ).

(3)

1We omit the subscript k for simplicity.

Related Work: Multilingual Transfer Learning The proposed MetaNMT differs from the existing framework of multilingual translation (Lee et al., 2016; Johnson et al., 2016; Gu et al., 2018b) or transfer learning (Zoph et al., 2016). The latter can be thought of as solving the following problem:





max Lmulti(θ) = Ek 

log p(Y |X; θ) ,

θ

(X,Y )∈Dk

where Dk is the training set of the k-th task, or language pair. The target low-resource language pair could either be a part of joint training or be trained separately starting from the solution θ0 found from solving the above problem.
The major difference between the proposed MetaNMT and these multilingual transfer approaches is that the latter do not consider how learning happens with the target, low-resource language pair. The former explicitly incorporates the learning process within the framework by simulating it repeatedly in Eq. (2). As we will see later in the experiments, this results in a substantial gap in the ﬁnal performance on the low-resource task.

Illustration In Fig. 2, we contrast transfer learning, multilingual learning and meta-learning using three source language pairs (Fr-En, Es-En and Pt-En) and two target pairs (Ro-En and Lv-En). Transfer learning trains an NMT system speciﬁcally for a source language pair (Es-En) and ﬁnetunes the system for each target language pair (RoEn, Lv-En). Multilingual learning often trains a single NMT system that can handle many different language pairs (Fr-En, Pt-En, Es-En), which may or may not include the target pairs (Ro-En, LvEn). If not, it ﬁnetunes the system for each target pair, similarly to transfer learning. Both of these however aim at directly solving the source tasks. On the other hand, meta-learning trains the NMT system to be useful for ﬁne-tuning on various tasks including the source and target tasks. This is done by repeatedly simulating the learning process on

low-resource languages using many high-resource language pairs (Fr-En, Pt-En, Es-En).

3.3 Uniﬁed Lexical Representation
I/O mismatch across language pairs One major challenge that limits applying meta-learning for low resource machine translation is that the approach outlined above assumes the input and output spaces are shared across all the source and target tasks. This, however, does not apply to machine translation in general due to the vocabulary mismatch across different languages. In multilingual translation, this issue has been tackled by using a vocabulary of sub-words (Sennrich et al., 2015) or characters (Lee et al., 2016) shared across multiple languages. This surface-level sharing is however limited, as it cannot be applied to languages exhibiting distinct orthography (e.g., IndoEuroepan languages vs. Korean.)

Universal Lexical Representation (ULR) We

tackle this issue by dynamically building a vo-

cabulary speciﬁc to each language using a key-

value memory network (Miller et al., 2016; Gul-

cehre et al., 2018), as was done successfully for

low-resource machine translation recently by Gu

et al. (2018b). We start with multilingual word em-

bedding matrices

k query

∈

R|Vk |×d

pretrained on

large monolingual corpora, where Vk is the vo-

cabulary of the k-th language. These embedding

vectors can be obtained with small dictionaries of

seed word pairs (Artetxe et al., 2017a; Smith et al.,

2017) or in a fully unsupervised manner (Zhang

et al., 2017; Conneau et al., 2018). We take one of

these languages k to build universal lexical repre-

sentation consisting of a universal embedding ma-

trix u ∈ RM×d and a corresponding key matrix

key ∈ RM×d, where M < |Vk|. Both

k query

and

key are ﬁxed during meta-learning. We then com-

pute the language-speciﬁc embedding of token x

from the language k as the convex sum of the uni-

versal embedding vectors by

M
0[x] = αi u[i],
i=1
where αi ∝ exp − τ1 key[i] A kquery[x] and τ is set to 0.05. This approach allows us to handle languages with different vocabularies using a ﬁxed number of shared parameters ( u, key and A.)
Learning of ULR It is not desirable to update the universal embedding matrix u when ﬁne-

Ro-En Lv-En Fi-En Tr-En Ko-En

# of sents.
0.61 M 4.46 M 2.63 M 0.21 M 0.09 M

# of En tokens
16.66 M 67.24 M 64.50 M
5.58 M 2.33 M

Dev
− 20.24 17.38 15.45 6.88

Test
31.76 15.15 20.20 13.74
5.97

Table 1: Statistics of full datasets of the target language pairs. BLEU scores on the dev and test sets are reported from a supervised Transformer model with the same architecture.

tuning on a small corpus which contains a limited set of unique tokens in the target language, as it could adversely inﬂuence the other tokens’ embedding vectors. We thus estimate the change to each embedding vector induced by languagespeciﬁc learning by a separate parameter ∆ k[x]:
k[x] = 0[x] + ∆ k[x].
During language-speciﬁc learning, the ULR 0[x] is held constant, while only ∆ k[x] is updated, starting from an all-zero vector. On the other hand, we hold ∆ k[x]’s constant while updating u and A during the meta-learning stage.
4 Experimental Settings
4.1 Dataset
Target Tasks We show the effectiveness of the proposed meta-learning method for low resource NMT with extremely limited training examples on ﬁve diverse target languages: Romanian (Ro) from WMT’16,2 Latvian (Lv), Finnish (Fi), Turkish (Tr) from WMT’17,3 and Korean (Ko) from Korean Parallel Dataset.4 We use the ofﬁcially provided train, dev and test splits for all these languages. The statistics of these languages are presented in Table 1. We simulate the low-resource translation scenarios by randomly sub-sampling the training set with different sizes.
Source Tasks We use the following languages from Europarl5: Bulgarian (Bg), Czech (Cs), Danish (Da), German (De), Greek (El), Spanish (Es), Estonian (Et), French (Fr), Hungarian (Hu), Italian (It), Lithuanian (Lt), Dutch (Nl), Polish (Pl), Portuguese (Pt), Slovak (Sk), Slovene (Sl) and
2 http://www.statmt.org/wmt16/translation-task.html 3 http://www.statmt.org/wmt17/translation-task.html 4 https://sites.google.com/site/koreanparalleldata/ 5 http://www.statmt.org/europarl/

BLEU

20

19

18.7018.58

18 17 16.7416.74

16

15

all

9.5

9.0

8.5

8.25

8.0

7.83

7.5 7.15 7.32 7.0

all

20.0019.84 17.9018.06
emb + enc (a) Ro-En
9.14 8.79
8.05 7.79
emb + enc (c) Fi-En

MultiNMT (Ro-En valid) MultiNMT (Lv-En valid) MetaNMT (Ro-En valid) MetaNMT (Lv-En valid)
18.3417.96
14.2514.57
emb
MultiNMT (Ro-En valid) MultiNMT (Lv-En valid) MetaNMT (Ro-En valid) MetaNMT (Lv-En valid)
8.60 7.92
6.76 6.94
emb

BLEU

BLEU

8.5 8.0 7.88 8.04

7.5

7.24

6.99 7.0

6.5 6.27 6.39

7.01 6.35

6.0
all

emb + enc

(b) Lv-En

6.5

6.0

5.76

5.5 5.38 5.32 5.54

6.53 5.89 5.80 6.02

5.0

4.5

4.0
all

emb + enc

(d) Tr-En

MultiNMT (Ro-En valid) MultiNMT (Lv-En valid) MetaNMT (Ro-En valid) MetaNMT (Lv-En valid)
7.66 7.37
5.87 5.65
emb
MultiNMT (Ro-En valid) MultiNMT (Lv-En valid) MetaNMT (Ro-En valid) MetaNMT (Lv-En valid)
5.01 4.82 3.85 4.01
emb

BLEU

Figure 3: BLEU scores reported on test sets for {Ro, Lv, Fi, Tr} to En, where each model is ﬁrst learned from 6 source tasks (Es, Fr, It, Pt, De, Ru) and then ﬁne-tuned on randomly sampled training sets with around 16,000 English tokens per run. The error bars show the standard deviation calculated from 5 runs.

Swedish (Sv), in addition to Russian (Ru)6 to learn the intilization for ﬁne-tuning. In our experiments, different combinations of source tasks are explored to see the effects from the source tasks.

Validation We pick either Ro-En or Lv-En as a validation set for meta-learning and test the generalization capability on the remaining target tasks. This allows us to study the strict form of metalearning, in which target tasks are unknown during both training and model selection.

Preprocessing and ULR Initialization As de-

scribed in §3.3, we initialize the query embed-

ding vectors

k query

of

all

the

languages.

For

each

language, we use the monolingual corpora built

from Wikipedia7 and the parallel corpus. The con-

catenated corpus is ﬁrst tokenized and segmented

using byte-pair encoding (BPE, Sennrich et al.,

2016), resulting in 40, 000 subwords for each lan-

guage. We then estimate word vectors using fast-

Text (Bojanowski et al., 2016) and align them

across all the languages in an unsupervised way

6 A subsample of approximately 2M pairs from WMT’17. 7 We use the most recent Wikipedia dump (2018.5) from
https://dumps.wikimedia.org/backup-index.html.

using MUSE (Conneau et al., 2018) to get multilingual word vectors. We use the multilingual word vectors of the 20,000 most frequent words in English to form the universal embedding matrix u.
4.2 Model and Learning
Model We utilize the recently proposed Transformer (Vaswani et al., 2017) as an underlying NMT system. We implement Transformer in this paper based on (Gu et al., 2018a)8 and modify it to use the universal lexical representation from §3.3. We use the default set of hyperparameters (dmodel = dhidden = 512, nlayer = 6, nhead = 8, nbatch = 4000, twarmup = 16000) for all the language pairs and across all the experimental settings. We refer the readers to (Vaswani et al., 2017; Gu et al., 2018a) for the details of the model. However, since the proposed metalearning method is model-agnostic, it can be easily extended to any other NMT architectures, e.g. RNN-based sequence-to-sequence models with attention (Bahdanau et al., 2015).
8 https://github.com/salesforce/nonauto-nmt

Meta-Train

Ro-En zero ﬁnetune

Lv-En zero ﬁnetune

Fi-En zero ﬁnetune

Tr-En zero ﬁnetune

Ko-En zero ﬁnetune

−

00.00 ± .00

0.00 ± .00

0.00 ± .00

0.00 ± .00

0.00 ± .00

Es

9.20 15.71 ± .22 2.23 4.65 ± .12 2.73 5.55 ± .08 1.56 4.14 ± .03 0.63 1.40 ± .09

Es Fr

12.35 17.46 ± .41 2.86 5.05 ± .04 3.71 6.08 ± .01 2.17 4.56 ± .20 0.61 1.70 ± .14

Es Fr It Pt

13.88 18.54 ± .19 3.88 5.63 ± .11 4.93 6.80 ± .04 2.49 4.82 ± .10 0.82 1.90 ± .07

De Ru 10.60 16.05 ± .31 5.15 7.19 ± .17 6.62 7.98 ± .22 3.20 6.02 ± .11 1.19 2.16 ± .09

Es Fr It Pt De Ru 15.93 20.00 ± .27 6.33 7.88 ± .14 7.89 9.14 ± .05 3.72 6.02 ± .13 1.28 2.44 ± .11

All

18.12 22.04 ± .23 9.58 10.44 ± .17 11.39 12.63 ± .22 5.34 8.97 ± .08 1.96 3.97 ± .10

Full Supervised

31.76

15.15

20.20

13.74

5.97

Table 2: BLEU Scores w.r.t. the source task set for all ﬁve target tasks.

BLEU

BLEU

25

23.95

21.33

23.16

20

18.53

19.84 20.70

16.13 15

16.61

18.06

Ro-En MetaNMT

11.55

Ro-En MultiNMT

12

11.78

10.12

11.32

10

8.58

9.14 9.41

8 7.89 7.28 7.79 Fi-En MetaNMT

6 5.41

Fi-En MultiNMT

0

4K

16K 40K

160K

Figure 4: BLEU Scores w.r.t. the size of the target task’s training set.

Learning We meta-learn using various sets of source languages to investigate the effect of source task choice. For each episode, by default, we use a single gradient step of language-speciﬁc learning with Adam (Kingma and Ba, 2014) per computing the meta-gradient, which is computed by the ﬁrst-order approximation in Eq. (3).
For each target task, we sample training examples to form a low-resource task. We build tasks of 4k, 16k, 40k and 160k English tokens for each language. We randomly sample the training set ﬁve times for each experiment and report the average score and its standard deviation. Each ﬁne-tuning is done on a training set, early-stopped on a validation set and evaluated on a test set. In default without notation, datasets of 16k tokens are used.

Fine-tuning Strategies The transformer consists of three modules; embedding, encoder and decoder. We update all three modules during metalearning, but during ﬁne-tuning, we can selectively tune only a subset of these modules. Following (Zoph et al., 2016), we consider three ﬁne-tuning

strategies; (1) ﬁne-tuning all the modules (all), (2) ﬁne-tuning the embedding and encoder, but freezing the parameters of the decoder (emb+enc) and (3) ﬁne-tuning the embedding only (emb).
5 Results
vs. Multilingual Transfer Learning We metalearn the initial models on all the source tasks using either Ro-En or Lv-En as a validation task. We also train the initial models to be multilingual translation systems. We ﬁne-tune them using the four target tasks (Ro-En, Lv-En, Fi-En and Tr-En; 16k tokens each) and compare the proposed meta-learning strategy and the multilingual, transfer learning strategy. As presented in Fig. 3, the proposed learning approach signiﬁcantly outperforms the multilingual, transfer learning strategy across all the target tasks regardless of which target task was used for early stopping. We also notice that the emb+enc strategy is most effective for both meta-learning and transfer learning approaches. With the proposed meta-learning and emb+enc ﬁne-tuning, the ﬁnal NMT systems trained using only a fraction of all available training examples achieve 2/3 (Ro-En) and 1/2 (Lv-En, Fi-En and Tr-En) of the BLEU score achieved by the models trained with full training sets.
vs. Statistical Machine Translation We also test the same Ro-En datasets with 16, 000 target tokens using the default setting of Phrase-based MT (Moses) with the dev set for adjusting the parameters and the test set for calculating the ﬁnal performance. We obtain 4.79(±0.234) BLEU point, which is higher than the standard NMT performance (0 BLEU). It is however still lower than both the multi-NMT and meta-NMT.
Impact of Validation Tasks Similarly to training any other neural network, meta-learning still requires early-stopping to avoid overﬁtting to a

BLEU

speciﬁc set of source tasks. In doing so, we observe that the choice of a validation task has nonnegligible impact on the ﬁnal performance. For instance, as shown in Fig. 3, Fi-En beneﬁts more when Ro-En is used for validation, while the opposite happens with Tr-En. The relationship between the task similarity and the impact of a validation task must be investigated further in the future.
Training Set Size We vary the size of the target task’s training set and compare the proposed meta-learning strategy and multilingual, transfer learning strategy. We use the emb+enc ﬁne-tuning on Ro-En and Fi-En. Fig. 4 demonstrates that the meta-learning approach is more robust to the drop in the size of the target task’s training set. The gap between the meta-learning and transfer learning grows as the size shrinks, conﬁrming the effectiveness of the proposed approach on extremely lowresource language pairs.

15

10

MetaNMT Fine-tune

5

MetaNMT Zero-shot MultiNMT Fine-tune

MultiNMT Zero-shot

00

20K 40K 60K 80K 100K 120K

Meta-learning steps

Figure 5: The learning curves of BLEU scores on the validation task (Ro-En).

Impact of Source Tasks In Table 2, we present the results on all ﬁve target tasks obtained while varying the source task set. We ﬁrst see that it is always beneﬁcial to use more source tasks. Although the impact of adding more source tasks varies from one language to another, there is up to 2× improvement going from one source task to 18 source tasks (Lv-En, Fi-En, Tr-En and Ko-En). The same trend can be observed even without any ﬁne-tuning (i.e., unsupervised translation, (Lample et al., 2017; Artetxe et al., 2017b)). In addition, the choice of source languages has different implications for different target languages. For instance, Ro-En beneﬁts more from {Es, Fr, It, Pt} than from {De, Ru}, while the opposite effect is observed with all the other target tasks.
Training Curves The beneﬁt of meta-learning over multilingual translation is clearly demonstrated when we look at the training curves in Fig. 5. With the multilingual, transfer learning ap-

proach, we observe that training rapidly saturates and eventually degrades, as the model overﬁts to the source tasks. MetaNMT on the other hand continues to improve and never degrades, as the metaobjective ensures that the model is adequate for ﬁne-tuning on target tasks rather than for solving the source tasks.
Sample Translations We present some sample translations from the tested models in Table 3. Inspecting these examples provides the insight into the proposed meta-learning algorithm. For instance, we observe that the meta-learned model without any ﬁne-tuning produces a word-by-word translation in the ﬁrst example (Tr-En), which is due to the successful use of the universal lexcial representation and the meta-learned initialization. The system however cannot reorder tokens from Turkish to English, as it has not seen any training example of Tr-En. After seeing around 600 sentence pairs (16K English tokens), the model rapidly learns to correctly reorder tokens to form a better translation. A similar phenomenon is observed in the Ko-En example. These cases could be found across different language pairs.
6 Conclusion
In this paper, we proposed a meta-learning algorithm for low-resource neural machine translation that exploits the availability of high-resource languages pairs. We based the proposed algorithm on the recently proposed model-agnostic metalearning and adapted it to work with multiple languages that do not share a common vocabulary using the technique of universal lexcal representation, resulting in MetaNMT. Our extensive evaluation, using 18 high-resource source tasks and 5 low-resource target tasks, has shown that the proposed MetaNMT signiﬁcantly outperforms the existing approach of multilingual, transfer learning in low-resource neural machine translation across all the language pairs considered.
The proposed approach opens new opportunities for neural machine translation. First, it is a principled framework for incorporating various extra sources of data, such as source- and targetside monolingual corpora. Second, it is a generic framework that can easily accommodate existing and future neural machine translation systems.

Source (Tr) Target Meta-0 Meta-16k
Source (Ko) Target Meta-0
Meta-16k

google mu¨lteciler ic¸in 11 milyon dolar toplamak u¨zere bag˘ıs¸ es¸les¸tirme kampanyasını bas¸lattı . google launches donation-matching campaign to raise $ 11 million for refugees . google refugee fund for usd 11 million has launched a campaign for donation . google has launched a campaign to collect $ 11 million for refugees .
이번에 체포되어 기소된 사람들 중에는 퇴역한 군 고위관리 , 언론인 , 정치인 , 경제인 등이 포함됐다 among the suspects are retired military ofﬁcials , journalists , politicians , businessmen and others . last year , convicted people , among other people , of a high-ranking army of journalists in economic and economic policies , were included . the arrested persons were included in the charge , including the military ofﬁcials , journalists , politicians and economists .

Table 3: Sample translations for Tr-En and Ko-En highlight the impact of ﬁne-tuning which results in syntactically better formed translations. We highlight tokens of interest in terms of reordering.

Acknowledgement
This research was supported in part by the Facebook Low Resource Neural Machine Translation Award. This work was also partly supported by Samsung Advanced Institute of Technology (Next Generation Deep Learning: from pattern recognition to AI) and Samsung Electronics (Improving Deep Learning using Latent Structure). KC thanks support by eBay, TenCent, NVIDIA and CIFAR.
References
Marcin Andrychowicz, Misha Denil, Sergio Gomez, Matthew W Hoffman, David Pfau, Tom Schaul, and Nando de Freitas. 2016. Learning to learn by gradient descent by gradient descent. In Advances in Neural Information Processing Systems, pages 3981–3989.
Mikel Artetxe, Gorka Labaka, and Eneko Agirre. 2017a. Learning bilingual word embeddings with (almost) no bilingual data. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), volume 1, pages 451–462.
Mikel Artetxe, Gorka Labaka, Eneko Agirre, and Kyunghyun Cho. 2017b. Unsupervised neural machine translation. arXiv preprint arXiv:1710.11041.
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural machine translation by jointly learning to align and translate. In ICLR.
Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. 2016. Enriching word vectors with subword information. arXiv preprint arXiv:1607.04606.
Yun Chen, Yang Liu, Yong Cheng, and Victor OK Li. 2017. A teacher-student framework for zeroresource neural machine translation. arXiv preprint arXiv:1705.00753.

Yun Chen, Yang Liu, and Victor OK Li. 2018. Zeroresource neural machine translation with multiagent communication game. arXiv preprint arXiv:1802.03116.
Yong Cheng, Yang Liu, Qian Yang, Maosong Sun, and Wei Xu. 2016. Neural machine translation with pivot languages. arXiv preprint arXiv:1611.04928.
Kyunghyun Cho, Bart van Merrie¨nboer, Dzmitry Bahdanau, and Yoshua Bengio. 2014. On the properties of neural machine translation: Encoder–Decoder approaches. In Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation.
Alexis Conneau, Guillaume Lample, Marc’Aurelio Ranzato, Ludovic Denoyer, and Herve´ Je´gou. 2018. Word translation without parallel data. International Conference on Learning Representations.
Chelsea Finn, Pieter Abbeel, and Sergey Levine. 2017. Model-agnostic meta-learning for fast adaptation of deep networks. arXiv preprint arXiv:1703.03400.
Orhan Firat, Kyunghyun Cho, and Yoshua Bengio. 2016a. Multi-way, multilingual neural machine translation with a shared attention mechanism. In NAACL.
Orhan Firat, Baskaran Sankaran, Yaser Al-Onaizan, Fatos T Yarman Vural, and Kyunghyun Cho. 2016b. Zero-resource translation with multi-lingual neural machine translation. In EMNLP.
Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann Dauphin. 2017. Convolutional sequence to sequence learning. arXiv preprint arXiv:1705.03122.
Jiatao Gu, James Bradbury, Caiming Xiong, Victor O. K. Li, and Richard Socher. 2018a. Nonautoregressive neural machine translation. ICLR.
Jiatao Gu, Hany Hassan, Jacob Devlin, and Victor OK Li. 2018b. Universal neural machine translation for extremely low resource languages. arXiv preprint arXiv:1802.05368.
Caglar Gulcehre, Sarath Chandar, Kyunghyun Cho, and Yoshua Bengio. 2018. Dynamic neural turing machine with continuous and discrete addressing schemes. Neural computation, 30(4):857–884.

Caglar Gulcehre, Orhan Firat, Kelvin Xu, Kyunghyun Cho, Loic Barrault, Huei-Chi Lin, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2015. On using monolingual corpora in neural machine translation. arXiv preprint arXiv:1503.03535.
David Ha, Andrew Dai, and Quoc V Le. 2016a. Hypernetworks. arXiv preprint arXiv:1609.09106.
Thanh-Le Ha, Jan Niehues, and Alexander Waibel. 2016b. Toward multilingual neural machine translation with universal encoder and decoder. arXiv preprint arXiv:1611.04798.
Di He, Yingce Xia, Tao Qin, Liwei Wang, Nenghai Yu, Tieyan Liu, and Wei-Ying Ma. 2016. Dual learning for machine translation. In Advances in Neural Information Processing Systems, pages 820–828.
Melvin Johnson, Mike Schuster, Quoc V Le, Maxim Krikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat, Fernanda Vie´gas, Martin Wattenberg, Greg Corrado, et al. 2016. Google’s multilingual neural machine translation system: enabling zero-shot translation. arXiv preprint arXiv:1611.04558.
Diederik Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980.
Philipp Koehn and Rebecca Knowles. 2017. Six challenges for neural machine translation. arXiv preprint arXiv:1706.03872.
Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language TechnologyVolume 1, pages 48–54. Association for Computational Linguistics.
Brenden M Lake, Ruslan Salakhutdinov, and Joshua B Tenenbaum. 2015. Human-level concept learning through probabilistic program induction. Science, 350(6266):1332–1338.
Guillaume Lample, Ludovic Denoyer, and Marc’Aurelio Ranzato. 2017. Unsupervised machine translation using monolingual corpora only. arXiv preprint arXiv:1711.00043.
Jason Lee, Kyunghyun Cho, and Thomas Hofmann. 2016. Fully character-level neural machine translation without explicit segmentation. arXiv preprint arXiv:1610.03017.
Jason Lee, Kyunghyun Cho, Jason Weston, and Douwe Kiela. 2017. Emergent translation in multi-agent communication. arXiv preprint arXiv:1710.06922.
Alexander Miller, Adam Fisch, Jesse Dodge, AmirHossein Karimi, Antoine Bordes, and Jason Weston. 2016. Key-value memory networks for directly reading documents. arXiv preprint arXiv:1606.03126.

Nikhil Mishra, Mostafa Rohaninejad, Xi Chen, and Pieter Abbeel. 2017. Meta-learning with temporal convolutions. arXiv preprint arXiv:1707.03141.
Herbert Robbins and Sutton Monro. 1951. A stochastic approximation method. The annals of mathematical statistics, pages 400–407.
Rico Sennrich, Barry Haddow, and Alexandra Birch. 2015. Improving neural machine translation models with monolingual data. arXiv preprint arXiv:1511.06709.
Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Edinburgh neural machine translation systems for wmt 16. arXiv preprint arXiv:1606.02891.
Samuel L Smith, David HP Turban, Steven Hamblin, and Nils Y Hammerla. 2017. Ofﬂine bilingual word vectors, orthogonal transformations and the inverted softmax. arXiv preprint arXiv:1702.03859.
Jake Snell, Kevin Swersky, and Richard Zemel. 2017. Prototypical networks for few-shot learning. In Advances in Neural Information Processing Systems, pages 4080–4090.
Ilya Sutskever, Oriol Vinyals, and Quoˆc Leˆ. 2014. Sequence to sequence learning with neural networks. In NIPS.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. arXiv preprint arXiv:1706.03762.
Oriol Vinyals, Charles Blundell, Tim Lillicrap, Daan Wierstra, et al. 2016. Matching networks for one shot learning. In Advances in Neural Information Processing Systems, pages 3630–3638.
Zhen Yang, Wei Chen, Feng Wang, and Bo Xu. 2018. Unsupervised neural machine translation with weight sharing. arXiv preprint arXiv:1804.09057.
Jiajun Zhang and Chengqing Zong. 2016. Exploiting source-side monolingual data in neural machine translation. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1535–1545.
Meng Zhang, Yang Liu, Huanbo Luan, and Maosong Sun. 2017. Earth mover’s distance minimization for unsupervised bilingual lexicon induction. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1934– 1945. Association for Computational Linguistics.
Barret Zoph, Deniz Yuret, Jonathan May, and Kevin Knight. 2016. Transfer learning for lowresource neural machine translation. arXiv preprint arXiv:1604.02201.

