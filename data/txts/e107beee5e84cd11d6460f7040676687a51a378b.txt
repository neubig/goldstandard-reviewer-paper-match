End-to-end reconstruction meets data-driven regularization for inverse problems

arXiv:2106.03538v1 [cs.CV] 7 Jun 2021

Subhadip Mukherjee∗1, Marcello Carioni∗1, Ozan Öktem2, and Carola-Bibiane Schönlieb1 1Department of Applied Mathematics and Theoretical Physics, University of Cambridge, UK 2Department of Mathematics, KTH–Royal institute of Technology, Sweden ∗Equal contribution authors
Emails: {sm2467, mc2250, cbs31}@cam.ac.uk, ozan@kth.se

Abstract
We propose an unsupervised approach for learning end-to-end reconstruction operators for ill-posed inverse problems. The proposed method combines the classical variational framework with iterative unrolling, which essentially seeks to minimize a weighted combination of the expected distortion in the measurement space and the Wasserstein-1 distance between the distributions of the reconstruction and ground-truth. More speciﬁcally, the regularizer in the variational setting is parametrized by a deep neural network and learned simultaneously with the unrolled reconstruction operator. The variational problem is then initialized with the reconstruction of the unrolled operator and solved iteratively till convergence. Notably, it takes signiﬁcantly fewer iterations to converge, thanks to the excellent initialization obtained via the unrolled operator. The resulting approach combines the computational efﬁciency of end-to-end unrolled reconstruction with the wellposedness and noise-stability guarantees of the variational setting. Moreover, we demonstrate with the example of X-ray computed tomography (CT) that our approach outperforms state-of-the-art unsupervised methods, and that it outperforms or is on par with state-of-the-art supervised learned reconstruction approaches.

1 Introduction

Inverse problems are ubiquitous in imaging applications, wherein one seeks to recover an unknown model parameter x ∈ X from its incomplete and noisy measurement, given by

yδ = A(x) + e ∈ Y.

Here, the forward operator A : X → Y models the measurement process in the absence of noise, and e, with e 2 ≤ δ, denotes the measurement noise. For example, in computed tomography (CT), the forward operator computes line integrals of x over a predetermined set of lines in R3 and the goal is to reconstruct x from its projections along these lines. Without any further information about x, inverse problems are typically ill-posed, meaning that there could be several reconstructions that are consistent with the measurement, even without any noise.

The variational framework circumvents ill-posedness by encoding prior knowledge about x via a regularization functional R : X → R. In the variational setting, one solves

min LY(yδ, A(x)) + λ R(x),

(1)

x∈X

where LY : Y × Y → R+ measures data-ﬁdelity and R penalizes undesirable or unlikely solutions.
The penalty λ > 0 balances the regularization strength with the ﬁdelity of the reconstruction. The variational problem (1) is said to be well-posed if it has a unique solution varying continuously in yδ.

Preprint. Under review.

The success of deep learning in recent years has led to a surge of data-driven approaches for solving inverse problems [5], especially in imaging applications. These methods come broadly in two ﬂavors: (i) end-to-end trained models that aim to directly map the measurement to the corresponding parameter and (ii) learned regularization methods that seek to ﬁnd a data-adaptive regularizer instead of handcrafting it. Techniques in both categories have their relative advantages and demerits. Speciﬁcally, end-to-end approaches offer fast reconstruction of astounding quality, but lack in terms of theoretical guarantees and need supervised data (i.e., pairs of input and target images) for training. On the contrary, learned regularization methods inherit the provable well-posedness properties of the variational setting and can be trained in an unsupervised manner, however the reconstruction entails solving a high-dimensional optimization problem, which is often slow and computationally demanding.
Our work derives ideas from learned optimization and adversarial machine learning, and makes an attempt to combine the best features of both aforementioned paradigms. In particular, the proposed method offers the ﬂexibility of unsupervised training, produces fast reconstructions comparable to end-to-end supervised methods in quality, while enjoying the well-posedness and stability guarantees of the learned regularization framework. We ﬁrst provide a brief overview of the literature on data-driven techniques for inverse problems before explaining our speciﬁc contributions in detail.
1.1 Related works
End-to-end fully learned methods for imaging inverse problems either map the measurement directly to the image [30, 19], or learn to eliminate the artifacts from a model-based technique [10]. Such approaches are data-intensive and may generalize poorly when trained on limited data. Iterative unrolling [17, 29, 2, 11, 16], with its origin in the seminal work by Gregor and LeCun on datadriven sparse coding [9], employs reconstruction networks that are inspired by optimization-based approaches and hence are interpretable. The unrolling paradigm enables one to encode the knowledge about the acquisition physics into the model architecture [3], thereby achieving data-efﬁciency. Nevertheless, end-to-end trained methods are supervised, and it is often challenging to obtain a large ensemble of paired data, especially in medical imaging applications.
Learned regularization methods, broadly speaking, aim to learn a data-driven regularizer in the variational setting. Some notable approaches in this paradigm include adversarial regularization (AR) [14] and its convex counterpart [18], network Tikhonov (NETT) [13], total deep variation (TDV) [12], etc., wherein one explicitly parametrizes the regularization functional using a neural network. The regularization by denoising (RED) approach aims to solve inverse problems by using a denoiser inside an algorithm for minimizing the variational objective [21, 22, 7]. The Plug-and-play (PnP) method [25] with a learned denoiser is also implicitly equivalent to data-driven regularization, subject to additional constraints on the denoiser [20]. The deep image prior technique [26] does not require training, but it seeks to regularize the solution by restricting it to be in the range of a deep generator and can thus be interpreted broadly as a deep learning-based regularization scheme. It is relatively easier to analyze learned regularization schemes using the machinery of classical functional analysis [24], but they fall short in terms of reconstruction quality. Moreover, these methods require one to solve a high-dimensional, potentially non-convex, variational problem, leading to slow reconstruction and lack of provable convergence.
1.2 Speciﬁc contributions
Our work seeks to combine iterative unrolling with data-adaptive regularization via an adversarial learning framework, and hence is referred to as unrolled adversarial regularization (UAR). The proposed approach learns a data-adaptive regularizer parametrized by a neural network, together with an iteratively unrolled reconstruction network that minimizes the corresponding expected variational loss in an adversarial setting. Unlike AR [14] where the undesirable images are taken as the pseudoinverse reconstruction and kept ﬁxed throughout the training, we update them with the output of the unrolled reconstruction network in each training step, and, in turn, use them to further improve the regularizer. Thanks to the Kantorovich-Rubinstein (KR) duality [4], the alternating learning strategy of the reconstruction and the regularizer networks is equivalent to minimizing the expected data-ﬁdelity over the distribution of the measurements, penalized by the Wasserstein-1 distance between the distribution of the reconstruction and the ground-truth. Once trained, the reconstruction operator produces a fast, end-to-end reconstruction. We show that this efﬁcient reconstruction can be
2

improved further by a reﬁnement step that involves running a few iterations of gradient-descent on the variational loss with the corresponding regularizer, starting from this initial estimate. The reﬁnement step not only produces reconstructions that outperform state-of-the-art unsupervised methods and are competitive with supervised methods, but also facilitates a well-posedness and stability analyses akin to classical variational approaches [24]. Our theoretical results on the learned unrolled operator and the regularizer are corroborated by strong experimental evidence for the CT inverse problem.

2 The proposed unrolled adversarial regularization (UAR) approach

In this section, we give a short mathematical background on optimal transport, followed by a detailed description of the UAR framework, including the training protocol and the network architectures.

2.1 Background on Optimal transport
Optimal transport theory [8, 27] has recently gained prominence in the context of measuring the distance between two probability distributions. In particular, given two probability distributions π1 and π2 on Rn, the Wasserstein-1 distance between them is deﬁned as

W1(π1, π2) := inf

x1 − x2 2 dµ(x1, x2),

(2)

µ∈Π(π1 ,π2 )

where Π(π1, π2) denotes all transport plans having π1 and π2 as marginals. The Wasserstein distance has proven to be suitable for deep learning tasks, when the data is assumed to be concentrated on low-dimensional manifolds in Rn. It has been shown that in such cases, the Wasserstein distance provides a usable gradient during training [4], as opposed to other popular divergence measures.
By the KR duality, the Wasserstein-1 distance can be computed equivalently by solving a maximization problem over the space of 1-Lipschitz functions (denoted by L1) as

W1(π1, π2) = sup R(x1) dπ1(x1) − R(x2) dπ2(x2),

(3)

R∈L1

provided that π1 and π2 have compact support [23]. Finally, we recall the deﬁnition of push-forward
of probability measures, which is used extensively in our theoretical exposition. Given a probability
measure π on A and a measurable map T : A → B, we deﬁne the push-forward of π by T (denoted as T#π) as a probability measure on B such that T#π(B) = π(T −1(B)), for all measurable B ⊂ B.

2.2 Training strategy and model parametrization for UAR
The principal idea behind UAR is to learn an unrolled deep network Gφ : Y → X for reconstruction, together with a regularization functional Rθ : X → R parametrized by another convolutional neural network (CNN). The role of Rθ is to discern ground-truth images from images produced by Gφ, while Gφ learns to minimize the variational loss with Rθ as the regularizer. As the images produced by Gφ gets better, Rθ faces a progressively harder task of telling them apart from the ground-truth images, thus leading to an improved regularizer. On the other hand, as the regularizer improves, the quality of reconstructions obtained using Gφ improves simultaneously. Consequently, Gφ and Rθ helps each other improve as the training progresses via an alternating update scheme.

2.2.1 Adversarial training

Let us denote by πx the ground-truth distribution and by πyδ the distribution of the noisy measurement. The UAR algorithm trains Gφ and Rθ simultaneously starting from an appropriate initialization. At the kth iteration of training, the parameters φ of the reconstruction network are updated as

φk ∈ arg min Jk(1)(φ), where Jk(1)(φ) := Eπyδ
φ

A(Gφ(yδ)) − yδ 22 + λ Rθk (Gφ(yδ)) , (4)

for a ﬁxed regularizer parameter θk. Subsequently, the regularizer parameters are updated as

θk+1 ∈ arg max Jk(2)(θ), where Jk(2)(θ) := Eπyδ Rθ Gφk (yδ) − Eπx [Rθ (x)] . (5)
θ:Rθ ∈L1

3

Algorithm 1 Learning unrolled adversarial regularization (UAR).
1. Input: Data-set {xi}Ni=1 ∼ πx and yj Nj=1 ∼ πyδ , initial reconstruction network parameter φ and regularizer parameter θ, batch-size nb = 1, penalty λ = 0.1, gradient penalty λgp = 10.0, Adam optimizer parameters (β1, β2) = (0.50, 0.99). 2. (Learn a baseline regularizer) for mini-batch k = 1, 2, · · · , 10 nNb , do:
• Sample xj ∼ πx, yj ∼ πyδ , and j ∼ uniform [0, 1]; for 1 ≤ j ≤ nb. Compute uj = A†(yj) and x(j ) = j xj + (1 − j ) uj .
• θ ← Adamη,β1,β2 (θ, ∇ J˜1(θ)), where η = 10−4, and

J˜ (θ) = 1 nb R (x ) − R (u ) + λ

1

nb

θj

θj

gp

j=1

∇Rθ x(j )

2
−1 .
2

3. (Learn a baseline reconstruction operator) for mini-batch k = 1, 2, · · · , 5 nNb , do:

• Sample yj ∼ πyδ , and compute J˜2(φ) = n1b nj=b 1 yj − A Gφ(yj ) • φ ← Adamη,β1,β2 (φ, ∇ J˜2(φ)), with η = 10−4.

22 + λ Rθ Gφ(yj ) .

4. (Jointly train Rθ and Gφ adversarially) for mini-batch k = 1, 2, · · · , 25 nNb , do:

• Sample xj, yj, and j ∼ uniform [0, 1]; for 1 ≤ j ≤ nb. Compute uj = Gφ(yj) and

x(j ) = j xj + (1 − j ) uj .

• θ ← Adamη,β1,β2 (θ, ∇ J˜1(θ)), where J˜1(θ) is as in Step 2, with η = 2 × 10−5. • Update φ ← Adamη,β1,β2 (φ, ∇ J˜2(φ)) twice, with J˜2(φ) as in Step 3, and η = 2 × 10−5.

5. Output: The trained networks Gφ and Rθ.

The learning protocol for UAR is unsupervised, since the loss functionals Jk(1)(φ) and Jk(2)(θ) can be computed based solely on the marginals πx and πyδ . The alternating update algorithm in (4) and (5) essentially seeks to solve the min-max variational problem given by

min max Eπ A(Gφ(yδ)) − yδ 2 + λ Eπ Rθ Gφ(yδ) − Eπ [Rθ (x)] . (6)

φ θ:Rθ ∈L1 yδ

2

yδ

x

Thanks to KR duality in (3) and the deﬁnition of push-forward, (6) can be reformulated as

min Eπ

A(Gφ(yδ)) − yδ

2
+ λW1

(Gφ)#πyδ , πx

.

(7)

φ

yδ

2

We refer the reader to Section 3 for a mathematically rigorous statement of this equivalence as well
as for a well-posedness theory of the problem in (7). Note that the equivalence of the alternating
minimization procedure and the variational problem in (7) holds only if the regularizer is fully
optimized in every iteration. Nevertheless, in practice, the reconstruction and regularizer networks
are not fully optimized in every iteration. Instead, one reﬁnes the parameters by performing one (or a few) Adam updates on the corresponding loss functionals. Notably, if W1 (Gφk )#πyδ , πx = 0, i.e., the parameters of G are such that the reconstructed images match the ground-truth in distribution, the loss functional Jk(2)(θ) and its gradient vanish, leading to no further update of θ. Thus, both networks stop updating when the outputs of Gφ are indistinguishable from the ground-truth images. The concrete training steps are listed in Algorithm 11.

2.2.2 Iteratively unrolled reconstruction operator
The objective of Gφ is to approximate the minimizer of the variational loss with Rθ as the regularizer. Therefore, an iterative unrolling strategy akin to [3] is adopted for parameterizing Gφ. Iterative unrolling seeks to mimic the variational minimizer via a primal-dual-style algorithm [6], with the proximal operators in the image and measurement spaces replaced with trainable CNNs. Although the variational loss in our case is non-convex, this parametrization for Gφ is chosen because of its
1Codes at https://github.com/Subhadip-1/unrolling_meets_data_driven_regularization.

4

expressive power over a generic network. Initialized with x(0) = A† yδ and h(0) = 0, Gφ produces

a reconstruction x(L) by iteratively applying the CNNs Λ ( ) and Λ ( ) in X and Y, respectively:

φp

φd

h( +1) = Γ ( ) h( ), σ( ) A(x ), yδ , and x( +1) = Λ ( ) x( ), τ ( ) A∗(h +1) , 0 ≤ ≤ L−1.

φd

φp

The step-size parameters σ( ) and τ ( ) are also made learnable and initialized as σ( ) = τ ( ) = 0.01 for each layer . The number of layers L is typically much smaller (we take L = 20) than the number of iterations needed by an iterative primal-dual scheme to converge, thus expediting the reconstruction by two orders of magnitude once trained.
The regularizer Rθ is taken as a deep CNN with six convolutional layers, followed by one averagepooling and two dense layers in the end.

2.2.3 Variational regularization as a reﬁnement step

The unrolled operator Gφ∗ trained by solving the min-max problem in (6) provides reasonably good reconstruction when evaluated on X-ray CT, and already outperforms state-of-the-art unsupervised
methods (c.f. Section 4). We demonstrate that the regularizer Rθ∗ obtained together with Gφ∗ by solving (6) can be used in the variational framework to further improve the quality of the end-to-end reconstruction Gφ∗ (yδ) for a given yδ ∈ Y. Speciﬁcally, we solve the variational problem

min

A(x) − yδ 2 + λ

Rθ∗ (x) + σ

x

2 2

,

(8)

x∈X

where λ , σ ≥ 0, by applying gradient descent, initialized with Gφ∗ (yδ). The additional Tikhonov term in (8) ensures coercivity of the overall regularizer, making it amenable to the standard wellposedness analysis [24]. Practically, it improves the stability of the gradient descent optimizer for (8). In practice, one essentially gets the same reconstruction with σ = 0 subject to early stopping (100 iterations). Notably, the ﬁdelity term in (8) is the 2 distance, instead of the squared- 2 ﬁdelity. We have empirically observed that this choice of the ﬁdelity term improves the quality of the reconstruction, possibly due to the higher gradient of the objective in the initial solution Gφ∗ (yδ). Since the end-to-end reconstruction gives an excellent initial point, it takes signiﬁcantly fewer iterations for gradiet-descent to recover the optimal solution to (8), and therefore UAR retains its edge in reconstruction time over fully variational approaches with learned regularizers (e.g., AR [14] or its convex version [18]).

3 Theoretical results

The theoretical properties of UAR are stated in this section and their proofs are provided in the supplementary document. Throughout this section, we assume that X = Rn and Y = Rk, and

A1. πx is compactly supported and πyδ is supported on a compact set K ⊂ Rk for every δ ≥ 0.

We then consider the following problem:

inf sup J1 Gφ, R|λ, πyδ := Eπ yδ − A Gφ(yδ) 2 + λ Eπ R(Gφ(yδ)) − Eπ [R(x)] .

φ R∈L1

yδ

2

yδ

x

(9)

Problem (9) is identical to the min-max variational problem deﬁned in (6), with the only difference

that the maximization in R is performed over the space of all 1-Lipschitz functions. Basically,

we consider the theoretical limiting case where the neural networks Rθ are expressive enough to

approximate all functions in L1 with arbitrary accuracy. We make the following assumptions on Gφ:

A2. Gφ is parametrized over a ﬁnite dimensional compact set K, i.e. φ ∈ K. A3. Gφn → Gφ pointwise whenever φn → φ. A4. supφ∈K Gφ ∞ < ∞.

Assumptions A2-A4 are satisﬁed, for instance, when Gφ is parametrized by a neural network whose weights are kept bounded during training. These assumptions apply to all results in this section.

5

3.1 Well-posedness of the adversarial loss

Here, we prove well-posedness and stability to noise for the optimal reconstructions. As a consequence of the KR duality, (9) can be equivalently expressed as

inf J2 Gφ|λ, πyδ := Eπ

yδ − A Gφ(yδ)

2
+ λ W1(πx, (Gφ)#πyδ ) .

(10)

φ

yδ

2

In the next theorem, we prove this equivalence, showing the existence of an optimal Gφ and R for (9). Theorem 1. Problems (9) and (10) admit an optimal solution and

inf sup J1 Gφ, R|λ, πyδ = inf J2 Gφ|λ, πyδ .

(11)

φ R∈L1

φ

Moreover, if (Gφ∗ , R∗) is optimal for (9), then Gφ∗ is optimal for (10). Conversely, if Gφ∗ is optimal for (10), then (Gφ∗ , R∗) is optimal for (9), for all R∗ ∈ arg maxR∈L1 Eπyδ R(Gφ∗ (yδ)) −Eπx [R(x)].

Next, we study the stability of the optimal reconstruction Gφ∗ to noise. We consider Gφn , where

φn ∈ arg inf J2 Gφ|λ, πyδn ,

(12)

φ

and show that Gφn → Gφ∗ as δn → δ, thus establishing noise-stability of the unrolled reconstruction.
Theorem 2 (Stability to noise). Suppose, for given a sequence of noise levels δn → δ ∈ [0, ∞), it holds that πyδn → πyδ in total variation. Then, with φn as in (12), Gφn → Gφ∗ up to sub-sequences.

3.2 Effect of λ on the end-to-end reconstruction

In order to analyze the effect of the parameter λ in (10) on the resulting reconstruction Gφ∗ , it is convenient to introduce the following two sets:
ΦL := φ : Eπyδ yδ − A Gφ(yδ) 22 = 0 and ΦW := φ : (Gφ)#πyδ = πx .
We assume that both ΦL and ΦW are non-empty, which is tantamount to asking that the parametrization of the end-to-end reconstruction operator is expressive enough to approximate a right inverse of A (ΦL = ∅) and a transport map from πyδ to πx (ΦW = ∅), and therefore is not very restrictive (keeping in view the enormous approximation power of unrolled deep architectures).
Proposition 1. Let Gφ∗ be a minimizer for (10). Then, it holds that

• Eπy yδ − A Gφ∗ (yδ) 22 ≤ λW1(πx, (Gφ)#πyδ ), for every φ ∈ ΦL.

1 • W1(πx, (Gφ∗ )#πyδ ) ≤ Eπ

yδ − A Gφ(yδ) 2 ,

λ yδ

2

for every φ ∈ ΦW.

The previous proposition shows in a quantitative way that for small λ, the optimal Gφ∗ has less expected distortion in the measurement space as the quantity Eπyδ yδ − A Gφ∗ (yδ) 22 is small. On the other hand, if λ is large, then the optimal Gφ∗ maps πyδ is closer to πx as the quantity W(πx, (Gφ∗ )#πyδ ) is small. Therefore, the regularization is stronger in this case.

We extend this analysis by studying the behavior of the unrolled reconstruction as λ converges to 0

and to +∞. Consider a sequence of parameters λn > 0 and the minimizer of the objective in (10)

with parameter λn:

φn ∈ arg inf J2 Gφ|λn, πyδ .

(13)

φ

Theorem 3. Let λn → 0. Then, there exists φ∗1 ∈ arg min W1(πx, (Gφ)#πyδ ) such that Gφn → Gφ∗1
φ∈ΦL

up to sub-sequences, and lim

1 λ

inf

J2

Gφ|λn, πyδ

= W1(πx, (Gφ∗1 )#πyδ ).

n→∞ n φ

Theorem 4. Let λn → +∞. Then, there exists φ∗2 ∈ arg min Eπyδ yδ − A Gφ(yδ) 22 such that
φ∈ΦW

Gφ → Gφ∗ up to sub-sequences, and lim inf J2 Gφ|λn, πyδ = Eπ yδ − A Gφ∗ (yδ) 2.

n

2

n→∞ φ

yδ

2

2

6

method

PSNR (dB)

SSIM

FBP TV

Supervised methods U-Net LPD

Unsupervised methods

AR

ACR

λ = 0.001

UAR

λ = 0.01 λ = 0.1

λ = 1.0

UAR with λ = λ = 0.1

reﬁnement

21.28 ± 0.13 30.31 ± 0.52
34.50 ± 0.65 35.69 ± 0.60
33.84 ± 0.63 31.55 ± 0.54 21.59 ± 0.11 25.25 ± 0.08 34.35 ± 0.66 33.27 ± 0.76 34.77 ± 0.67

0.20 ± 0.02 0.78 ± 0.01
0.90 ± 0.01 0.91 ± 0.01
0.86 ± 0.01 0.85 ± 0.01 0.22 ± 0.02 0.37 ± 0.01 0.88 ± 0.01 0.87 ± 0.01 0.90 ± 0.01

# param.
1 1

reconstruction time (ms)
37.0 ± 4.6 28371.4 ± 1281.5

7215233 1138720

44.4 ± 12.5 279.8 ± 12.8

19338465 22567.1 ± 309.7 606610 109952.4 ± 497.8

20477186

252.7 ± 13.3

– 5863.3 ± 106.1

Table 1: Average PSNR and SSIM (with their standard deviations) for different reconstruction methods. The reconstruction times and the number of learnable parameters are also indicated. Without any reﬁnement, UAR outperforms AR and ACR in reconstruction quality and reduces the reconstruction time by a couple of orders of magnitude. With the reﬁnement, UAR becomes on par with supervised post-processing, but the reconstruction time is still four times smaller than AR.

Theorems 3 and 4 characterize the optimal end-to-end reconstruction Gφ∗ as λ → 0 and λ → ∞, respectively. Speciﬁcally, if λ → 0, Gφ∗ minimizes the Wasserstein distance between reconstruction and ground-truth among all the reconstruction operators that achieve zero expected data-distortion. In particular, Gφ∗ is close to the right inverse of A that minimizes the Wasserstein distance. Therefore, when λ is very small, we expect to obtain a reconstruction that is close to the unregularized solution in quality. If λ → ∞ on the other hand, the operator Gφ∗ is close to a transport map between πx and πyδ , i.e., (Gφ∗ )#πyδ = πx, which minimizes the expected data-distortion. Therefore the reconstruction produces realistic images, but they are not consistent with the measurement. These theoretical observations are corroborated by the numerical results (c.f. Section 4, Fig. 2). One has to thus select a λ that optimally trades-off data-distortion with the Wasserstein distance to achieve the best reconstruction performance.
3.3 End-to-end reconstruction vis-à-vis the variational solution
The goal of this section is two-fold. Firstly, we theoretically justify the fact that the end-to-end reconstruction performs well, despite minimizing the expected loss over the distribution πyδ . Secondly, we analyze the role of the regularizer in the variational setting in reﬁning the end-to-end reconstruction.
It is important to remark that the the end-to-end reconstruction is trained in on the expected variational loss computed using samples from πyδ and πx. Therefore, the end-to-end reconstruction cannot learn a point-wise correspondence between measurement and model parameter, but only a distributional correspondence. Despite that, the end-to-end reconstruction achieves excellent performance for a given measurement vector y. A justiﬁcation of such phenomena is given by the next proposition.
Proposition 2. Let (Gφ∗ , R∗) be an optimal pair for (9) such that R∗ ≥ 0 almost everywhere under (Gφ)# πyδ . Deﬁne M1 := Eπyδ yδ − A Gφ∗ (yδ) 22 and M2 := W1(πx, (Gφ∗ )#πyδ ). Then, the following two upper bounds hold for every η > 0:
• Pπyδ yδ : yδ − A Gφ∗ (yδ) 22 ≥ η ≤ Mη1 .
• Suppose, R∗(x) = 0 for πx-almost every x. Then, P(Gφ∗ )#πyδ x : R∗(x) ≥ η ≤ Mη2 .
7

(a) ground-truth

(b) FBP: 21.19 dB, 0.22 (c) TV: 29.85 dB, 0.79 (d) U-net: 34.42 dB, 0.90

(e) LPD: 35.76 dB, 0.92 (f) ACR: 31.24 dB, 0.86 (g) AR: 33.52 dB, 0.86 (h) UAR: 33.85 dB, 0.87
Figure 1: Reconstruction on Mayo clinic data. UAR achieves better reconstruction quality than AR and ACR, while signiﬁcantly reducing the reconstruction time (c.f. Table 1). The reduction in reconstruction time comes at the expense of higher training complexity as compared to AR.

Proposition 2 provides an estimate in probability of the sets {yδ : yδ − A Gφ∗ (yδ) 22 ≥ η} and {x : R∗(x) ≥ η}. In particular, if M1 is small, then yδ − A Gφ∗ (yδ) 22 is small in probability. If instead M2 is small, then R∗(x) is small in probability on the support of (Gφ∗ )#πyδ , implying that samples Gφ∗ (yδ) are difﬁcult to distinguish from the ground-truth. We remark that the assumption R∗(x) = 0 can be justiﬁed using a data manifold assumption as in Section 3.3. of [14]. We now analyze the role of the regularizer R∗ in the optimization of the variational problem (8) that
reﬁnes the end-to-end reconstruction Gφ∗ . We rely on a similar distributional analysis as the one performed in [14]. For η > 0, consider the transformation by a gradient-descent step on R∗ given by gη(x) = x − η ∇R∗(x). Using the shorthand πG∗ := (Gφ∗ )#πyδ , and by denoting the distribution of gη(x) as πη := (gη)#πG∗ for x ∼ πG∗ , we have the following theorem.

Theorem 5 ([14]). Suppose that η → W1(πη, πx) is differentiable at η = 0. Then, the derivative at

η

=

0

satisﬁes

d dη

W1

(π

η

,

πx

)

= −EπG∗ ∇R∗(x) 22.

η=0

This theorem states that a gradient-descent step performed on R∗ at x = Gφ∗ (yδ) decreases the Wasserstein distance with respect to the ground-truth distribution πx. Therefore, if the gradientdescent step to solve the variational problem (8) is initialized with the reconstruction Gφ∗ (yδ), the next iterate gets pushed closer to the ground-truth distribution πx. We stress that this property holds because of the chosen initialization point, due to the relation between R∗ and Gφ∗ . For a different initialization, this property may not hold.

4 Numerical results
On the application front, we consider the prototypical inverse problem of CT reconstruction from noisy sparse-view projections. The abdominal CT scans for 10 patients, made publicly available by the Mayo-Clinic for the low-dose CT grand challenge [15], were used in our numerical experiments. Speciﬁcally, 2250 2D slices of size 512 × 512 corresponding to 9 patients were used to train the models, while 128 slices from the remaining one patient were used for evaluation. The projections were simulated in ODL [1] using a parallel-beam geometry with 200 uniformly spaced angular positions of the source, with 400 lines per angle. Subsequently, Gaussian noise with standard deviation σe = 2.0 was added to the projection data to simulate noisy sinograms.

8

(a) λ = 0.001: 21.60, 0.21 (b) λ = 0.01: 25.33, 0.37 (c) λ = 0.1: 34.65, 0.88 (d) λ = 1.0: 33.96, 0.88
Figure 2: Reconstruction of UAR for different λ. For λ → 0, the unrolled generator seeks to ﬁnd the minimizer of the expected data-ﬁdelity loss, hence the reconstruction looks similar to FBP.
(a) ground-truth (b) 34.94, 0.88 (c) 35.46, 0.90 (d) ground-truth (e) 33.84, 0.87 (f) 34.24, 0.89
Figure 3: Effect of reﬁnement: (b) and (e): end-to-end reconstruction Gφ∗ (yδ); (c) and (f): the respective reﬁned reconstructions. The PSNR (dB) and SSIM scores are indicated below.
The proposed UAR method is compared with two classical model-based approaches for CT, namely ﬁltered back-projection (FBP) and total variation (TV). The LPD method [3] and U-net-based postprocessing [10] of FBP are chosen as two supervised approaches for comparison. The AR approach [14] and its convex variant [18], referred to as adversarial convex regularizer (ACR) , are taken as the competing unsupervised approaches. For LPD and AR, we develop a PyTorch-based implementation based on their publicly available TensorFlow codes2 3, while for ACR, we use the publicly available PyTorch implementation4. The unrolled network Gφ has 20 layers, with 5 × 5 ﬁlters in both primal and dual spaces to increase the overall receptive ﬁeld for sparse-view measurements. The hyper-parameters involved in training the UAR are speciﬁed in Algorithm 1. We found that ﬁrst training a baseline regularizer and a corresponding baseline reconstruction operator helps stabilize the training process. Training the UAR model took approximately 30 hours on an NVIDIA Quadro RTX 6000 GPU (24 GB of memory). The average performance on the test images in terms of PSNR and SSIM [28] indicates that UAR (with λ = 0.1) outperforms AR and ACR by 0.3 dB and 2.6 dB, approximately. We would like to emphasize that this gain was found to be consistent across all test images and not just realized on average. With the reﬁnement step, UAR surpasses AR by almost 0.7 dB and becomes on par with U-net post-processing. The end-to-end UAR reconstruction is a couple of orders of magnitude faster than AR, while the reduction in reconstruction time is by a factor of 4 with the reﬁnement. The reconstructions of a representative test image using the competing methods are shown in Fig. 1 for a visual comparison. The effect of λ on the reconstruction of UAR is demonstrated in Fig. 2, which conﬁrms the theoretical results in Section 3.2. The reﬁnement step also visibly improves the reconstruction quality of the end-to-end operator, as shown in Fig. 3.
5 Conclusions and limitations
To the best of our knowledge, this work makes the ﬁrst attempt to blend end-to-end reconstruction with data-driven regularization via an adversarial learning framework. Our UAR approach retains
2LPD: https://github.com/adler-j/learned_primal_dual. 3AR: https://github.com/lunz-s/DeepAdverserialRegulariser. 4ACR: https://github.com/Subhadip-1/data_driven_convex_regularization.
9

the fast reconstruction of the former together with provable guarantees of the latter. We rigorously analyze the proposed framework in terms of well-posedness, noise-stability, and the effect of the regularization penalty, and establish a link between the trained reconstruction operator and the corresponding variational objective. We show strong numerical evidence of the efﬁcacy of the UAR approach for CT reconstruction, wherein it achieves the same performance as supervised data-driven post-processing and outperforms competing unsupervised techniques. Our work paves the way to better understand the role of adversarially learned regularizers in solving ill-posed inverse problems, although several important aspects need further investigation. Since the learned regularizer is non-convex, the performance of gradient-descent on the variational objective greatly depends on initialization. This problem is partly addressed by the unrolled reconstruction operator that efﬁciently computes a better initial point for gradient descent. However, the precise relationship between the end-to-end reconstruction and the variational minimizer for a given measurement vector remains elusive. Moreover, the quality of the reconstruction relies on the expressive power of neural networks and thus suffers from the curse of dimensionality. We believe that addressing such limitations will be important to better understand adversarial regularization methods.
6 Acknowledgment
MC acknowledges support from the Royal Society (Newton International Fellowship NIF\R1\192048 Minimal partitions as a robustness boost for neural network classiﬁers). CBS acknowledges support from the Philip Leverhulme Prize, the Royal Society Wolfson Fellowship, the EPSRC grants EP/S026045/1 and EP/T003553/1, EP/N014588/1, EP/T017961/1, the Wellcome Innovator Award RG98755, the Leverhulme Trust project Unveiling the invisible, the European Union Horizon 2020 research and innovation programme under the Marie Skodowska-Curie grant agreement No. 777826 NoMADS, the Cantab Capital Institute for the Mathematics of Information, and the Alan Turing Institute. SM acknowledges Thomas Buddenkotte for testing out the codes and the Wellcome Trust for funding and supporting his research.
References
[1] J. Adler, H. Kohr, and O. Öktem. Operator discretization library (odl). Software available from https://github.com/odlgroup/odl, 2017.
[2] Jonas Adler and Ozan Öktem. Solving ill-posed inverse problems using iterative deep neural networks. Inverse Problems, 33(12), 2009.
[3] Jonas Adler and Ozan Öktem. Learned primal-dual reconstruction. IEEE transactions on medical imaging, 37(6):1322–1332, 2018.
[4] Martin Arjovsky, Soumith Chintala, and Léon Bottou. Wasserstein generative adversarial networks. In Proceedings of the 34th International Conference on Machine Learning, pages 214–223, 2017.
[5] Simon Arridge, Peter Maass, Ozan Öktem, and Carola-Bibiane Schönlieb. Solving inverse problems using data-driven models. Acta Numerica, 28:1–174, 2019.
[6] A. Chambolle and T. Pock. A ﬁrst-order primal-dual algorithm for convex problems with applications to imaging. J. Math. Imaging and Vision, 40(1):120–145, 2010.
[7] Stanley H Chan, Xiran Wang, and Omar A Elgendy. Plug-and-play admm for image restoration: Fixed-point convergence and applications. IEEE Transactions on Computational Imaging, 3(1): 84–98, 2016.
[8] M. Cuturi and G. Peyré. Computational Optimal Transport. Arxiv preprint arXiv:1803.00567, 2019. https://arxiv.org/pdf/1803.00567.pdf.
[9] K. Gregor and Y. LeCun. Learning fast approximations of sparse coding. In Intl. Conf. on Machine Learning, 2010.
[10] K. H. Jin, M. T. McCann, E. Froustey, and M. Unser. Deep convolutional neural network for inverse problems in imaging. IEEE Transactions on Image Processing, 26(9):4509–4522, 2017.
10

[11] Erich Kobler, Teresa Klatzer, Kerstin Hammernik, and Thomas Pock. Variational networks: connecting variational methods and deep learning. In German conference on pattern recognition, pages 281–293. Springer, 2017.
[12] Erich Kobler, Alexander Efﬂand, Karl Kunisch, and Thomas Pock. Total deep variation for linear inverse problems. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 7549–7558, 2020.
[13] Housen Li, Johannes Schwab, Stephan Antholzer, and Markus Haltmeier. NETT: solving inverse problems with deep neural networks. Inverse Problems, 36(6), 2020.
[14] Sebastian Lunz, Ozan Öktem, and Carola-Bibiane Schönlieb. Adversarial regularizers in inverse problems. In Advances in Neural Information Processing Systems, pages 8507–8516, 2018.
[15] C. McCollough. Tfg-207a-04: Overview of the low dose ct grand challenge. Medical Physics, 43(6):3759–3760, 2014.
[16] Tim Meinhardt, Michael Moller, Caner Hazirbas, and Daniel Cremers. Learning proximal operators: Using denoising networks for regularizing inverse imaging problems. In Proceedings of the IEEE International Conference on Computer Vision, pages 1781–1790, 2017.
[17] V. Monga, Y. Li, and Y. Eldar. Algorithm unrolling: Interpretable, efﬁcient deep learning for signal and image processing. arXiv preprint arXiv:1912.10557v3, 2019.
[18] S. Mukherjee, S. Dittmer, Z. Shumaylov, S. Lunz, O. Öktem, and C.-B. Schönlieb. Learned convex regularizers for inverse problems. arXiv preprint arXiv:2008.02839v2, 2021.
[19] Changheun Oh, Dongchan Kim, Jun-Young Chung, Yeji Han, and H. Park. Eter-net: End to end mr image reconstruction using recurrent neural network. In MLMIR@MICCAI, 2018.
[20] J.-C. Pesquet, A. Repetti, M. Terris, and Y. Wiaux. Learning maximally monotone operators for image recovery. arXiv preprint arXiv:2012.13247v2, Apr. 2021.
[21] E. T. Reehorst and P. Schniter. Regularization by denoising: clariﬁcations and new interpretations. IEEE Transactions on Computational Imaging, 5(1):52–67, 2019.
[22] Yaniv Romano, Michael Elad, and Peyman Milanfar. The little engine that could: Regularization by denoising (red). SIAM Journal on Imaging Sciences, 10(4):1804–1844, 2017.
[23] F. Santambrogio. Optimal Transport for Applied Mathematicians. Birkhäuser Basel, 2015.
[24] Otmar Scherzer, Markus Grasmair, Harald Grossauer, Markus Haltmeier, and Frank Lenzen. Variational methods in imaging. Springer, 2009.
[25] Y. Sun, B. Wohlberg, and Kamilov U. S. An online plug-and-play algorithm for regularized image reconstruction. IEEE Transactions on Computational Imaging, 5(3):395–408, 2019.
[26] Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Deep image prior. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 9446–9454, 2018.
[27] C. Villani. Optimal transport — Old and new, volume 338 of Grundlehren der mathematischen Wissenschaften. Springer Berlin Heidelberg, 2009. doi: 10.1007/978-3-540-71050-9.
[28] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli. Image quality assessment: From error visibility to structural similarity. IEEE Transactions on Image Processing, 13(4):600–612, 2004.
[29] Y. Yang, J. Sun, H. Li, and Z. Xu. Deep admm-net for compressive sensing mri. In Advances in Neural Information Processing Systems, 2016.
[30] B. Zhu, J. Z. Liu, S. F. Cauley, B. R. Rosen, and M. S. Rosen. Image reconstruction by domain-transform manifold learning. Nature, 555:487–492, 2018.
11

A Proofs of the theoretical results

In this section, we prove the theoretical results stated in Section 3. First, we recall the setting and the main deﬁnitions. For the set of assumptions used in this section, we refer to Assumptions A1 – A4 stated in Section 3. The objective of the adversarial optimization is deﬁned as

inf sup J1 Gφ, R|λ, πyδ := Eπ yδ − A Gφ(yδ) 2 + λ Eπ R(Gφ(yδ))

φ R∈L1

yδ

2

yδ

In Section 3, we claimed that the problem (14) is well-posed and is equivalent to

− Eπx [R(x)] . (14)

inf J2 Gφ|λ, πyδ := Eπ

yδ − A Gφ(yδ)

2
+ λ W1(πx, (Gφ)#πyδ ) .

(15)

φ

yδ

2

This shows the connection between the training objective and the Wasserstein-1 distance between the ground-truth distribution and the distribution of the reconstruction. Here, we prove the theorems stated in Section 3 regarding well-posedness (Theorem 1), stability to noise (Theorem 2), and dependence on the parameter λ (Proposition 1, Theorem 3, and Theorem 4) for (14) and (15). Moreover, we further discuss the relation between (15) and the variational problem used as a reﬁnement and prove Proposition 2.
We recall the dominated convergence theorem below, which is used as one of the main tools in our proofs. For the sake of completeness, we also recall the deﬁnition of narrow convergence of measures.
Dominated convergence theorem: Consider a sequence of measurable functions {fn}n∈N deﬁned on a measure space (Ω, F, µ) such that fn → f pointwise for a measurable function f deﬁned on
(Ω, F, µ). Suppose that for any x ∈ Ω, |fn(x)| ≤ g(x), where |g| dµ < ∞. Then, it holds that
Ω

lim |fn − f | dµ = 0
n→∞ Ω

and consequently, lim fn dµ = f dµ.

n→∞ Ω

Ω

Narrow convergence of measures: Consider a sequence of measures {µn}n∈N deﬁned on a measurable space (Ω, F). Given a measure µ deﬁned on (Ω, F) we say that µn narrowly converges to µ
if

lim ϕ dµn = ϕ dµ
n→+∞
for every ϕ ∈ Cb(Ω), where we denote by Cb(Ω) the set of bounded continuous functions on Ω.

A.1 Well-posedness of the adversarial loss: Proofs of Theorem 1 and Theorem 2

A.1.1 Proof of Theorem 1

We start by proving the existence of an optimal solution for (15). Let Gφn be a minimizing sequence for (15), namely a sequence of reconstruction operators such that

lim J2 Gφn |λ, πyδ = inf J2 Gφ|λ, πyδ .

(16)

n→+∞

φ

As φn ∈ K and K is compact and ﬁnite dimensional (see Assumption A2), there exists φ∗ ∈ K such that, up to sub-sequences, φn → φ∗ and consequently Gφn → Gφ∗ pointwise (see Assumption A3). We now show that Gφ∗ is a minimum for (15). Thanks to the continuity of A, we know that
yδ − A Gφn (yδ) 22 → yδ − A Gφ∗ (yδ) 22 pointwise. Moreover, using Assumptions A1 and A4, the bound

yδ − A Gφn (yδ) 22 ≤ sup 2 yδ 22 + 2 A 2op
yδ ∈supp(πyδ )

2

sup Gφ ∞
φ∈K

<∞

12

holds for every yδ ∈ supp(πyδ ), where we denote by A op the operator norm of A. Therefore, by applying the dominated convergence theorem, we obtain that

lim Eπ

yδ − A Gφ

(yδ )

2
= Eπ

yδ − A Gφ∗ (yδ) 2 .

(17)

n→+∞ yδ

n

2

yδ

2

Notice now that for every ϕ ∈ Cb(Rk).

ϕ(x) d[(Gφn )#πyδ ] − ϕ(x) d[(Gφ∗ )#πyδ ] ≤ |ϕ(Gφn (yδ)) − ϕ(Gφ∗ (yδ))| dπyδ → 0

as n → +∞, using, again, the dominated convergence theorem together with Assumption A1. Thus, the probability measures (Gφn )#πyδ converge narrowly to (Gφ∗ )#πyδ as n → +∞. Moreover, using again dominated convergence, together with the bound supφ∈K Gφ ∞ < ∞ (see Assumption A4), we also have
x 2 d(Gφn )#πyδ − x 2 d(Gφ∗ )#πyδ ≤ | Gφn (yδ) 2 − Gφ∗ (yδ) 2| dπyδ → 0 (18)

as n → +∞. Thus, using [23, Theorem 5.11], we infer that limn→+∞ W1(πx, (Gφn )#πyδ ) = W1(πx, (Gφ∗ )#πyδ ). Finally using such convergence, together with (17) and (16), we conclude that

J2 Gφ∗ |λ, πyδ = Eπyδ yδ − A Gφ∗ (yδ) 22 + λ W1(πx, (Gφ∗ )#πyδ )

= lim Eπ

yδ − A Gφ

(yδ )

2
+ λ W1(πx, (Gφ

)#πyδ )

n→+∞ yδ

n

2

n

= lim J2 Gφn |λ, πyδ
n→+∞

= inf J2 Gφ|λ, πyδ ,
φ

thus showing that Gφ∗ is a minimum for (15).

We now show that (14) and (15) are equivalent. Using the Kantorovich-Rubinstein duality [23,
Theorem 1.39] and bound supφ∈K Gφ ∞ < ∞ (Assumption A4), we have that for every φ ∈ K, there exists Rφ ∈ L1 such that

Rφ ∈ arg max Eπyδ R(Gφ(yδ)) − Eπx [R(x)] , and (19)
R∈L1

Eπ δ Rφ(Gφ(yδ)) − Eπx Rφ(x) = W1(πx, (Gφ)#πyδ ) .

(20)

y

Therefore, denoting by Gφ∗ the minimum for (15), it holds that

inf sup J1 Gφ, R|λ, πyδ
φ R∈L1

= inf Eπ yδ − A Gφ(yδ) 2 + λ sup Eπ R(Gφ(yδ)) − Eπ [R(x)]

φ

yδ

2

yδ

R∈L1

x

= inf Eπ yδ − A Gφ(yδ) 2 + λ Eπ Rφ(Gφ(yδ)) − Eπ Rφ(x)

φ

yδ

2

yδ

x

= inf Eπ

yδ − A Gφ(yδ)

2
+ λ W1(πx, (Gφ)#πyδ )

φ

yδ

2

= Eπyδ yδ − A Gφ∗ (yδ) 22 + λ Eπyδ R∗(Gφ∗ (yδ)) − Eπx [R∗(x)] ,

where R∗ is any 1-Lipschitz function such that

R∗ ∈ arg max Eπyδ R(Gφ∗ (yδ)) − Eπx [R(x)] .
R∈L1

In particular, (11) in Theorem 1 holds and the pair (Gφ∗ , R∗) is optimal for (14). Viceversa, if (Gφ∗ , R∗) is optimal for (14), then for every φˆ ∈ K we have

J2 Gφ∗ |λ, πyδ = sup J1 Gφ∗ , R|λ, πyδ = inf sup J1 Gφ, R|λ, πyδ

R∈L1

φ R∈L1

≤ sup J1 Gφˆ, R|λ, πyδ = J2 Gφˆ|λ, πyδ
R∈L1
where we used the optimality of (Gφ∗ , R∗) together with (19) and (20), showing that Gφ∗ is a minimizer for (15).

13

A.1.2 Proof of Theorem 2

Let δn be a sequence converging to δ as n → +∞ and

φn ∈ arg inf J2 Gφ|λ, πyδn .

(21)

φ

Recall that πyδn converges in total variation to πyδ . We denote this convergence by

lim πyδn − πyδ M = 0 .

(22)

n→+∞

Using the fact that φn ∈ K and K is compact and ﬁnite dimensional, we know that there exists φ∗ ∈ K such that φn → φ∗, up to sub-sequences. In particular, by Assumption A3, Gφn → Gφ∗ , up to sub-sequences. We need to prove that

φ∗ ∈ arg min J2 Gφ|λ, πyδ .

(23)

φ

First, notice that as πyδn → πyδ in total variation, it holds that for every bounded, measurable function f ,

f dπyδn → f dπyδ .

(24)

Therefore, using the fact that Gφ is bounded for every φ ∈ K (Assumption A4), A is linear, and the supports of πyδ and πyδ are uniformly contained in a common compact set (Assumption A1), it holds
n
for every Gφ that

lim

yδ − A Gφ(yδ)

2 2

dπyδn

=

yδ − A Gφ(yδ)

2 2

dπyδ

.

(25)

n→+∞

Moreover, thanks to the dominated convergence theorem, together with the pointwise convergence Gφn → Gφ∗ and the uniform bound supn Gφn ∞ < ∞ (Assumption A4), we have

lim

yδ − A Gφn (yδ)

2 2

dπyδ

=

yδ − A Gφ∗ (yδ)

2 2

dπyδ

.

(26)

n→+∞

Therefore

lim sup
n→+∞

yδ − A Gφn (yδ)

2 2

dπyδn

−

yδ − A Gφ∗ (yδ)

2 2

dπyδ

= lim sup
n→+∞

yδ − A Gφn (yδ)

2 2

d(πyδn

− πyδ

+ πyδ ) −

yδ − A Gφ∗ (yδ)

2 2

dπyδ

≤ lim sup
n→+∞

y − A Gφn (yδ)

2 2

dπyδ

−

y − A Gφ∗ (yδ)

2 2

dπyδ

+

yδ − A Gφn (yδ)

2 2

d(πyδ

− πyδn )

= lim sup

yδ − A Gφn (yδ)

2 2

d(πyδ

− πyδn )

(27)

n→+∞

≤ lim sup

2 yδ

2 2

+

2(

A

op sup

Gφn

∞)2 d πyδ − πyδn

(28)

n→+∞

n

2

≤

sup 2

yδ

2 2

+

2

A

2 op

sup

Gφ ∞

lim sup πyδ − πyδn M = 0

(29)

yδ ∈K

φ∈K

n→+∞

where in (27) we use (26) and in (28)–(29) we use (22) together with the fact that Gφn are uniformly bounded (Assumption A4), A is linear and the supports of πyδ and πyδ are uniformly contained in a
n
common compact set K (Assumption A1).

14

Consider now a test function ϕ ∈ Cb(Rk). Notice that

lim sup
n→+∞

ϕ(Gφn (yδ)) dπyδn − ϕ(Gφ∗ (yδ)) dπyδ

≤ lim sup
n→+∞

ϕ(Gφn (yδ)) d(πyδn − πyδ ) +

ϕ(Gφn (yδ)) dπyδ − ϕ(Gφ∗ (yδ)) dπyδ

≤ lim sup
n→+∞

|ϕ(Gφn (yδ))| d|πyδn − πyδ | +

ϕ(Gφn (y)) dπyδ − ϕ(Gφ∗ (yδ)) dπyδ

≤ lim sup ϕ ∞ πyδn − πyδ M +
n→+∞

ϕ(Gφn (yδ)) dπyδ − ϕ(Gφ∗ (yδ)) dπyδ

= 0,

where we use again (22) together with the pointwise convergence Gφn → Gφ∗ and the compactness of the support of πyδ . Such estimate prove that (Gφn )#πyδn converges narrowly to (Gφ∗ )#πyδ . Moreover, adapting the previous to test function ϕ(x) = x 2 and using additionally that supn Gφn ∞ < ∞ (see Assumption A4) we infer

lim
n→+∞

x 2 d[(Gφn )#πyδn ] − x 2 d[(Gφ∗ )#πyδ ] = 0

which, thanks to [23, Theorem 5.11] and together with the narrow convergence (Gφn )#πyδn → (Gφ∗ )#πyδ implies

lim W1(πx, (Gφn )#πyδn ) = W1(πx, (Gφ∗ )#πyδ )

(30)

n→+∞

and similarly

lim W1(πx, (Gφ)#πyδn ) = W1(πx, (Gφ)#πyδ ) for all φ ∈ K.

(31)

n→+∞

We are ﬁnally in position to prove (23). Let φ ∈ K a competitor for the variational problem in (23). Then thanks to the optimality of φn

J2 Gφn |λ, πyδn ≤ J2 Gφ|λ, πyδn

for every n. Passing to the limit in the previous inequality using (30), (31), (25) and (29) we obtain

J2 Gφ∗ |λ, πyδ ≤ J2 Gφ|λ, πyδ

(32)

as we wanted to prove.

A.2 Effect of λ on the end-to-end reconstruction. Proofs of Proposition 1, Theorem 3 and Theorem 4
Here we prove Proposition 1, Theorem 3 and Theorem 4. We remind the reader the deﬁnition of the function spaces
ΦL := φ : Eπyδ yδ − A Gφ(yδ) 22 = 0 and ΦW := φ : (Gφ)#πyδ = πx that we assume to be non-empty.

A.2.1 Proof of Proposition 1
Let Gφ∗ be a minimizer for (15). Then for every φ ∈ ΦL we easily estimate Eπyδ yδ − A Gφ∗ (yδ) 22 ≤ Eπyδ yδ − A Gφ(yδ) 22 + λ W1(πx, (Gφ)#πyδ ) − λ W1(πx, (Gφ∗ )#πyδ ) ≤ λ W1(πx, (Gφ)#πyδ ),
leading to the ﬁrst estimate in Proposition 1. Moreover, for every φ ∈ ΦW we obtain the second estimate in Proposition 1, that is
λW1(πx, (Gφ∗ )#πyδ ) ≤ Eπyδ yδ − A Gφ(yδ) 22 + λ W1(πx, (Gφ)#πyδ ) − Eπyδ yδ − A G(yδ) 22 ≤ Eπyδ yδ − A Gφ(yδ) 22 ,
where we used that for every φ ∈ ΦW, W1(πx, (Gφ)#πyδ ) = 0.

15

A.2.2 Proof of Theorem 3

We are assuming λn → 0 and

φn ∈ arg inf J2 Gφ|λn, πyδ .

(33)

φ

First, using the fact that φn ∈ K and K is compact and ﬁnite dimensional we know that there exists φ∗1 ∈ K such that φn → φ∗1, up to sub-sequences. In particular, it also holds that Gφn → Gφ∗1 , up to sub-sequences, by Assumption A3. It remains to prove that

φ∗1 ∈ arg min W1(πx, (Gφ)#πyδ ).

(34)

φ∈ΦL

First notice that by Proposition 1 we can select φ ∈ ΦL such that

Eπyδ yδ − A Gφn (yδ) 22 ≤ λnW(πx, (Gφ)#πyδ )

for every n. So, taking the limit for n → +∞ and using that λn → 0 together with (17) (where
again we used Assumptions A1 and A4, and the dominated convergence theorem) we obtain Eπyδ yδ − A Gφ∗1 (yδ) 22 = 0. Now, let φ ∈ ΦL. Using (33) we have that for every n

λnW1(πx, (Gφn )#πyδ ) ≤ λnW1(πx, (Gφn )#πyδ ) + Eπyδ yδ − A Gφn (yδ) 22

≤ λnW1(πx, (Gφ)#πyδ ).

(35)

With similar arguments as in the proof of Theorem 1 we can prove that the probability mea-
sures (Gφn )#πyδ converge narrowly to (Gφ∗1 )#πyδ as n → +∞. Additionally using the bound supφ∈K Gφ ∞ < ∞ (see Assumption A4) we can repeat the computation in (18) to prove that limn→+∞ W1(πx, (Gφn )#πyδ ) = W1(πx, (Gφ∗1 )#πyδ ) [23, Theorem 5.11]. So, passing to the limit in (35) we conclude that

W1(πx, (Gφ∗ )#πyδ ) = lim W1(πx, (Gφ )#πyδ ) ≤ W1(πx, (Gφ)#πyδ )

1

n→+∞

n

showing (34). We now prove the convergence lim

1 λ

inf

J2

Gφ|λn, πyδ

n→∞ n φ

Notice that using that, as Eπyδ yδ − A Gφ∗1 (yδ) 22 = 0 and (33) we have

= W1(πx, (Gφ∗1 )#πyδ ).

1 λn J2 Gφn |λn, πyδ ≤ W1(πx, (Gφ∗1 )#πyδ )

and trivially

1 λn J2 Gφn |λn, πyδ ≥ W1(πx, (Gφn )#πyδ ).

So, passing to the limit in the previous estimates and using that limn→+∞ W1(πx, (Gφn )#πyδ ) = W1(πx, (Gφ∗1 )#πyδ ) we prove the desired convergence.

A.2.3 Proof of Theorem 4

We are assuming λn → +∞ and

φn ∈ arg inf J2 Gφ|λn, πyδ .

(36)

φ

First, using the fact that φn ∈ K and K is compact and ﬁnite dimensional we know that there exists φ∗2 ∈ K such that φn → φ∗2, up to sub-sequences. In particular, it also holds that Gφn → Gφ∗2 , up to sub-sequences, by Assumption A3. It remains to prove that
φ∗2 ∈ arg min Eπyδ yδ − A Gφ(yδ) 22 . (37)
φ∈ΦW

First notice that by Proposition 1 we can select φ ∈ ΦW such that

Eπ yδ − A Gφ(yδ) 2

(πx, (Gφ )#πyδ ) ≤ yδ

2

(38)

W1

n

λn

16

for every n. With similar arguments as in the proof of Theorem 1, using Assumption A1 and
Assumption A4 together with [23, Theorem 5.11] there holds that limn→+∞ W1(πx, (Gφn )#πyδ ) = W1(πx, (Gφ∗2 )#πyδ ). So, taking the limit in (38) for n → +∞ and using that λn → +∞ we obtain W1(πx, (Gφ∗2 )#πyδ ) = 0.

Let now φ ∈ ΦW. Using (36) we have that for every n Eπyδ yδ − A Gφn (yδ) 22 ≤ Eπyδ yδ − A Gφn (yδ) 22 + λnW1(πx, (Gφn )#πyδ )

≤ Eπyδ yδ − A Gφ(yδ) 22 . (39)

With similar arguments as in the proof of Theorem 1, using dominated convergence theorem together with Assumption A1 and Assumption A4 it holds that

lim Eπ

yδ − A Gφ

(yδ )

2
= Eπ

yδ − A Gφ∗ (yδ) 2 .

n→+∞ yδ

n

2

yδ

2

2

(40)

So, passing to the limit in (39) we conclude that

Eπ

yδ − A Gφ∗ (yδ) 2 = lim Eπ

yδ − A Gφ

(yδ )

2
≤ Eπ

yδ − A Gφ(yδ) 2

yδ

2

2 n→+∞ yδ

n

2

yδ

2

showing (37).

We now show the convergence lim inf J2 Gφ|λn, πyδ = Eπ yδ − A Gφ∗ (yδ) 2. Using that,

n→∞ φ

yδ

2

2

W1(πx, (Gφ∗2 )#πyδ ) = 0, together with (36) we have

J2 Gφn |λn, πyδ ≤ Eπyδ yδ − A Gφ∗2 (yδ) 22

and trivially

J2 Gφn |λn, πyδ ≥ Eπyδ yδ − A Gφn (yδ) 22 .

So, passing to the limit in the previous estimate and using (40) we prove the desired convergence.

A.3 End-to-end reconstruction vis-à-vis the variational solution. Proof of Proposition 2 and further discussion
A.3.1 Proof of Proposition 2
The ﬁrst upper bound in Proposition 2 is a simple application of Markov inequality for probability measures, which states that every non-negative random variable U satisﬁes P (U ≥ η) ≤ E[ηU] , for any η > 0.
For the second upper bound notice that using Theorem 1 we have
R∗(x) d[(Gφ∗ )#πyδ ] = R∗(x) d[(Gφ∗ )#πyδ ] − R∗(x) dπx = W1(πx, (Gφ∗ )#πyδ )
(41) where we also use the assumption: R∗(x) = 0 for πx-almost every x. Therefore the second upper bound in Proposition 2 follows from an application of Markov inequality, thanks to the assumed positivity of R∗.
We remark the assumption regarding the positivity of R∗ is not restrictive, as R∗ + C is optimal for every C ∈ R. However, it is not always true that R∗(x) = 0 for πx-almost every x. As discussed in Section 3, such assumption can be justiﬁed using a suitable weak manifold assumption for πx.
We conclude this section by further discussing the content of Theorem 5. As already noted, this theorem ensures that a gradient-descent step performed on R∗ at x = Gφ∗ (yδ) decreases the Wasserstein distance with respect to the ground-truth distribution πx. Therefore, if the gradientdescent step to solve the variational problem (8) is initialized with the reconstruction Gφ∗ (yδ), the next iterate gets pushed closer to the ground-truth distribution πx. If we additionally use the same weak manifold assumption as in [14] it is possible to prove that an optimal regularizer R∗ is given by the distance function from the ground-truth manifold (see [14]). In this case, if we additionally assume that the projection from Gφ∗ (yδ) to the manifold is unique, then the gradient of R∗ in that point is a unit vector from Gφ∗ (yδ) to the unique projection point. Such consideration strengthens even more our claim that an iterate of gradient descent initialized in Gφ∗ (yδ) gets pushed closer to the ground-truth distribution πx. A graphical representation of such effect is presented in Fig. 4.

17

Gφ∗ (y) ∇R∗

πx πG∗

Figure 4: A step of gradient descent applied to the initial point Gφ∗ (y) is moving the point in the direction ∇R∗(Gφ∗ (y)) closer to the data distribution πx.
B Additional numerical results
Here, we provide a comparison of different algorithms on another test image from the Mayo-clinic lowdose CT challenge dataset [15] (See Figure 5 below). The purpose of this example is to demonstrate that the gain in performance achieved by UAR over the competing algorithms is consistent over different test images.

18

(a) ground-truth

(b) FBP: 21.59, 0.24

(c) TV: 29.16, 0.77

(d) U-net: 32.69, 0.87

(e) LPD: 34.05, 0.89

(f) ACR: 30.14, 0.83

(g) AR: 32.14, 0.84

(h) UAR (end-to-end): 32.80, 0.86 (i) UAR (reﬁned): 33.15, 0.87

Figure 5: Another numerical example on the Mayo clinic data [15]. As we see, UAR (reﬁned) signiﬁcantly outperforms AR and ACR, and achieves slightly better reconstruction quality than U-net-based post-processing, which is a supervised approach. To see the reduction in reconstruction time using UAR as compared to competing variational methods (such as TV, AR, and ACR), refer to Table 1 in Section 4.

19

