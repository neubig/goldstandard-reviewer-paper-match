Direct Uncertainty Prediction for Medical Second Opinions

arXiv:1807.01771v4 [cs.LG] 29 May 2019

Maithra Raghu * 1 2 Katy Blumer * 2 Rory Sayres 2 Ziad Obermeyer 3 Robert Kleinberg 1 Sendhil Mullainathan 4 Jon Kleinberg 1

Abstract
The issue of disagreements amongst human experts is a ubiquitous one in both machine learning and medicine. In medicine, this often corresponds to doctor disagreements on a patient diagnosis. In this work, we show that machine learning models can be trained to give uncertainty scores to data instances that might result in high expert disagreements. In particular, they can identify patient cases that would beneﬁt most from a medical second opinion. Our central methodological ﬁnding is that Direct Uncertainty Prediction (DUP), training a model to predict an uncertainty score directly from the raw patient features, works better than Uncertainty Via Classiﬁcation, the two-step process of training a classiﬁer and postprocessing the output distribution to give an uncertainty score. We show this both with a theoretical result, and on extensive evaluations on a large scale medical imaging application.
1. Introduction
In both the practice of machine learning and the practice of medicine, a serious challenge is presented by disagreements amongst human labels. Machine learning classiﬁcation models are typically developed on large datasets consisting of (xi, yi) (data instance, label) pairs. These are collected (Russakovsky & Fei-Fei, 2010; Welinder & Perona, 2010) by assigning each raw instance xi to multiple human evaluators, yielding several labels yi(1), yi(2), ...yi(ni). Unsurprisingly, these labels often have disagreements amongst them and must be carefully aggregated to give a single target value.
This label disagreement issue becomes a full-ﬂedged clinical problem in the healthcare domain. Despite the human
*Equal contribution 1Department of Computer Science, Cornell University 2Google Brain 3UC Berkeley School of Public Health 4Chicago Booth School of Business. Correspondence to: Maithra Raghu <maithrar@gmail.com>.
Proceedings of the 36 th International Conference on Machine Learning, Long Beach, California, PMLR 97, 2019. Copyright 2019 by the author(s).

labellers now being highly trained medical experts (doctors), disagreements (on the diagnosis) persist (Van Such et al., 2017; Abrams et al., 1994; AAO, 2002; Gulshan et al., 2016; Rajpurkar et al., 2017). One example is (Van Such et al., 2017), where agreement between referral and ﬁnal diagnoses in a cohort of two hundred and eighty patients is studied. Exact agreement is only found in 12% of cases, but more concerningly, 21% of cases have signiﬁcant disagreements. This latter group also turns out to be the most costly to treat. Other examples are given by (Daniel, 2004), a study of tuberculosis diagnosis, showing that radiologists disagree with colleagues 25% of the time, and with themselves 20% of the time, and (Elmore et al., 2015), studying disagreement on cancer diagnosis from breast biopsies.
These disagreements arise not solely from random noise (Rolnick et al., 2017), but from expert judgment and bias. In particular, some patient cases xi intrinsically contain features that result in greater expert uncertainty (e.g. Figure 2.) This motivates applying machine learning to predict which patients are likely to give rise to the most doctor disagreement. We call this the medical second opinion problem. Such a model could be deployed to automatically identify patients that might need a second doctor’s opinion.
Mathematically, given a patient instance xi, we are interested in assigning a scalar uncertainty score to xi, h(xi) that reﬂects the amount of expert disagreement on xi. For each xi, we have multiple labels yi(1), yi(2), ...yi(ni), each corresponding to a different individual doctor’s grade.
One natural approach is to ﬁrst train a classiﬁer mapping xi to the yi(j), e.g. via the empirical distribution of labels pˆi. For ungraded examples, a measure of spread of the output distribution of the classiﬁer (e.g. variance) could be used to give a score. We call this Uncertainty via Classiﬁcation (UVC).
An alternate approach, Direct Uncertainty Prediction (DUP), is to learn a function hdup directly mapping xi to a scalar uncertainty score. The basic contrast with Uncertainty via Classiﬁcation is illustrated in Figure 1. Our central methodological ﬁnding is that Direct Uncertainty Prediction (provably) works better than the two step process of Uncertainty via Classiﬁcation.

Direct Uncertainty Prediction for Medical Second Opinions

Figure 1. Different ways of computing an uncertainty scores. An uncertainty score h(xi) for xi can be computed by the two step process of Uncertainty via Classiﬁcation: training a classiﬁer on pairs (data instance, empirical grade distribution from yi(j)) (xi, pi), and then post processing the classiﬁer output distribution to get an uncertainty score. h(xi) can also be learned directly on xi, i.e. Direct Uncertainty Prediction. DUP models are trained on pairs (data instance, target uncertainty function on empirical grade distribution), (xi, U (pi)). Theoretical and empirical results support the greater effectiveness of Direct Uncertainty Prediction.
In particular, our three main contributions are the following:
1. We deﬁne simple methods of performing Direct Uncertainty Prediction on data instances xi with multiple noisy labels. We prove that under a natural model for the data, DUP gives an unbiased estimate of the true uncertainty scores U (xi), while Uncertainty via Classiﬁcation has a bias term. We then demonstrate this in a synthetic setting of mixtures of Gaussians, and on an image blurring detection task on the standard SVHN and CIFAR-10 benchmarks.
2. We train UVC and DUP models on a large-scale medical imaging task. As predicted by the theory, we ﬁnd that DUP models perform better at identifying patient cases that will result in large disagreements amongst doctors.
3. On a small gold standard adjudicated test set, we study how well our existing DUP and UVC models can identify patient cases where the individual doctor grade disagrees with a consensus adjudicated diagnosis. This adjudicated grade is a proxy for the best possible doctor diagnosis. All DUP models perform better than all UVC models on all evaluations on this task, in both an uncertainty score setting and a ranking application.
2. Direct Uncertainty Prediction

Proportion

Histogram of Doctor Grades for

1.0

Image 1 with Adj Grade 1

0.8

0.6

0.4

0.2

0.0 0

1

2

3

4

5

6

Label Value

Histogram of Doctor Grades for

1.0

Image 2 with Adj Grade 1

0.8

0.6

0.4

0.2

0.0 0

1

2

3

4

5

6

Label Value

Proportion

Figure 2. Patient cases have features resulting in higher doctor disagreement. The two rows give example datapoints in our dataset. The patient images xi, xj are in the left column, and on the right we have the empirical probability distribution (histogram) of the multiple individual doctor DR grades. For the top image, all doctors agreed that the grade should be 1, while there was a signiﬁcant spread for the bottom image. When later performing an adjudication process (Section 5), where doctors discuss their initial diagnoses with each other and come to a consensus, both patient cases were given an adjudicated DR grade of 1.

learning a scalar uncertainty scoring function h(x) on patient instances x, which signiﬁes the amount of expert disagreement arising from x.

To do so, we must ﬁrst deﬁne a target uncertainty scor-
ing function U (·). Our data consists of pairs of the
form (patient features, multiple individual doctor labels), (xi; yi(1), yi(2), ...yi(ni)) (Figure 2). Letting c1, ..., ck be the different possible doctor grades, we can deﬁne the empir-
ical grade distribution – the empirical histogram: pˆi = [pˆ(i1), ..., pˆ(ik)], with

pˆ(il) =

1j yi(j)=cl
ni

Our target uncertainty scoring function U (·) then computes an uncertainty score for xi using the empirical histogram pˆi. One such function, which computes the probability that two draws from the empirical histogram will disagree is

k
Udisagree(xi) = Udisagree(pˆi) = 1 − (pˆ(il))2 (1)
l=1

Another uncertainty score, which penalizes larger disagreements more, is the variance:

k
Uvar(xi) = Uvar(pˆi) = cl·(pˆ(il))2−
l=1

(l) 2
cl · pˆi (2)

Our core prediction problem, motivated by identifying patients who need a medical second opinion, centers around

For a large family of these uncertainty scoring functions (including entropy, variance, etc) we can show that Direct

Direct Uncertainty Prediction for Medical Second Opinions

Uncertainty Prediction gives an unbiased estimate of U (pˆi), whereas uncertainty via classiﬁcation has a bias term.
The key observation is that while we want our model to predict doctor disagreement, it does not see all the patient information the doctors do. In particular, the model must predict doctor disagreement based off of only xi (in our setting, images). In contrast, human doctors see not only xi but other extensive information, such as patient medical history, family medical history, patient characteristics (age, symptom descriptions, etc) (De Fauw et al., 2018).
Letting o denote all patient features seen by the doctors, we can think of xi as being the image of o under a (many to one) mapping g, which hides the additional patient information, i.e. xi = g(o). Suppose there are k possible doctor grades, c1, .., ck. Let f denote the joint distribution over patient features and doctor grades. In particular, let O be a random variable representing patient features, and Y the doctor label for O. Then our density function assigns a probability to (patient features, doctor grade) pairs (o, y).
This can also be deﬁned with a vectorized version of the grades: let Yl = 1Y =cl , the event that O is diagnosed as cl. Then we deﬁne the vector Y = [Y1, ..., Yk]. f is therefore also a density over the points f (O = o, Y = y). Let the marginal probability of the patient features be fO, with fO = y f (O, y).
Given an uncertainty scoring function U (·), we would like to predict the disagreement in labels amongst doctors who have seen the patient features O. But as the patient features O and doctor grades Y are jointly distributed according to f , this is just the uncertainty of the expected value of Y under the posterior of Y given o. In particular, we are interested in predicting:
U y · f (Y = y|O) = U (E[Y|O])
y
This is a function taking as input a patient’s features. For a particular patient’s features o, we get a scalar uncertainty score given by
U (E[Y|O = o])

Uncertainty via classiﬁcation huvc does this in reverse order, ﬁrst computing the expected posterior, and assigning an uncertainty score to that:
huvc(x) = U (E[Y|g(O) = x])
= U E[Y|O = o]fO(o|g(O) = x)
o
In this setting we can show Theorem 1. Using the above notation
(i) hdup is an unbiased estimate of the true uncertainty
(ii) For any concave uncertainty scoring function U (·) (which includes Udisagree, Uvar), uncertainty via classiﬁcation, huvc has a bias term.
The full proof is the Appendix. A sketch is as follows: the unbiased result arises from the tower rule (law of total expectations). The bias of huvc follows by the concavity of U (), Jensen’s inequality, and the fact that g(·) truly hides some patient features. For Udisagree and Uvar, we can compute this bias term exactly (full computation in Appendix): Corollary 1. For Udisagree, Uvar the bias term is:
(i) Bias of huvc with Udisagree:

Eg(O)

V arO|g(O) E[Yl|O] g(O)
l

(ii) Bias of huvc with Uvar:

Eg(O) V arO|g(O)

l · E[Yl|O] g(O)
l

In Sections 4, 5 we train both Direct Uncertainty Prediction (DUP) models and Uncertainty Via Classiﬁcation (UVC) models on a large scale medical imaging task. However, to gain intuition for the theoretical results, we ﬁrst study a toy case on a mixture of Gaussians.

However, our model doesn’t see o, but only x = g(O). We make the mild assumptions that Y is conditionally independent of g(O) given O, and that g(·) truly hides information, loosely that O|g(O) = x is not a point mass (see Appendix for details.) In this setting, direct uncertainty prediction, hdup computes the expectation of the uncertainty scores of all the possible posteriors, i.e.
hdup(x) = E [U (E[Y|O])|g(O) = x]
= U (E[Y|O = o])fO(o|g(O) = x)
o

2.1. Toy Example on Mixture of Gaussians

To illustrate the formalism in a simpliﬁed setting, we con-

sider the following pedagogical toy example. Suppose our

data is generated by a mixture of k Gaussians. Let fi ∼

N (µi, σi2), and qi be mixture probabilities. Then f (o, y =

i) = qifi(o) and the marginal fO(o) =

k i=1

qifi(o).

Ad-

ditionally, the probability of a particular class l given o,

f (y = l|o) is simply

. ql fl (o)

k i=1

qi fi (o)

Two 1-D Gaussians: As the ﬁrst, most simple case, suppose we have two one dimensional Gaussians, the ﬁrst, f1 =

Direct Uncertainty Prediction for Medical Second Opinions

Model Type
UVC DUP

(3d, 5G)
69.1% 74.6%

(5d, 4G)
62.0% 71.2%

(10d, 4G)
56.0% 63.4%

Model
UVC DUP

SVHN (disagree)
75.8% 88.0%

CIFAR-10 (disagree)
79.1% 85.3%

Table 1. DUP and UVC trained to predict disagreement on mixtures of Gaussians. We train DUP and UVC models on different mixtures of Gaussians, with(nd, mG) denoting a mixture of m Gaussians in m dimensions. Results are in percentage AUC over 3 repeats. The means of the Gaussians are drawn iid from a multivariate normal distribution (full setup in Appendix.) We see that the DUP model performs much better than the UVC model at identifying datapoints with high disagreement in the labels.

N (−1, 1) and the second, f2 = N (1, 1). Assume that the mixture probabilities q1, q2 are equal to 0.5. Given o drawn from this mixture q1f1 + q2f2, we’d like to estimate U (f (y|o)). Suppose the model sees x = g(o) = |o|, the absolute value of o. Then, DUP can estimate the uncertainty exactly:
E [U (E[Y|O])|x = |o|] =0.5 · U (E[Y|O = o]) + 0.5 · U (E[Y|O = −o]) = U (E[Y|O = o])
= U [f (1|o), 1 − f (1|o)]

where the third line follows by the symmetry of the two distributions, with

f (1|o) =

0.5f1(o)

0.5f1(o) + 0.5f2(o)

On the other hand, the expected posterior over labels in UVC, E[Y|x = |o|], is just [0.5, 0.5], as by symmetry, given x = g(o) = |o|, o is equally likely to come from f1 or f2. So UVC outputs a constant uncertainty score U ([0.5, 0.5]) for all x = |o|, despite the true varying uncertainty scores.

Training DUPs and UVCs on Mixture of Gaussians: In Table 1 we train DUPs and UVCs on a few different mixture of Gaussian settings. We generate data o from a Gaussian mixture with iid centers, and labels for the data using the posterior over the different centers given o. We use these labels to score o on its uncertainty (using Udisagree). We then train a model on x = g(o) = |o| to predict whether x is low or high uncertainty. (Full details in Appendix.) We see that DUP performs consistently better than UVC.

2.2. Example on SVHN and CIFAR-10
Another empirical demonstration is given by training DUP and UVC to predict label agreement in an image blurring setting. For a source image in SVHN or CIFAR-10, we ﬁrst apply a Gaussian blur, with a variance chosen for that source image. Then, we draw three noisy labels for the source image, where the noise distribution over labels corresponds to

Table 2. DUP and UVC trained to predict label disagreement corresponding to image blurring on SVHN and CIFAR-10. DUP outperforms UVC on predicting label disagreement on SVHN and CIFAR-10, where the labels are drawn from a noisy distribution that varies depending on how much blurring the source image has been subjected to. Full details in Appendix.
the severity of the image blur. For example, for an image that has a Gaussian blur of variance 0 (i.e. no blurring), the distribution over labels is a point mass on the true label. For an image that has been blurred severely, there is signiﬁcant mass on incorrect labels. (Exact distributional details are given in the Appendix.) We train DUP and UVC models on this dataset and evaluate their ability to predict label disagreement. We again ﬁnd that DUP models outperfom UVC models. This is despite the setting not directly mapping onto the statement of Theorem 1 – there is no obscuring function g. This suggests the beneﬁts of DUP are more general than the precise theoretical setting. We also observe that the DUP and UVC models learn different features (see Appendix.)
3. Related Work
The challenges posed by expert disagreement is an important one, and prior work has put forward several approaches to address some of these issues. Under the assumption that the noise distribution is conditionally independent of the data instance given the true label, (Natarajan et al., 2013; Sukhbaatar et al., 2014; Reed et al., 2014; Sheng et al., 2008) provide theoretical analysis along with algorithms to denoise the labels as training progresses, or efﬁciently collect new labels. However, the conditional independence assumption does not hold in our setting (Figure 2.) Other work relaxes this assumption by deﬁning a domain speciﬁc generative model for how noise arises (Mnih & Hinton, 2012; Xiao et al., 2015; Veit et al., 2017) with some methods using additional clean data to pretrain models to form a good prior for learning. Related techniques have also been explored in semantic segmentation (Gurari et al., 2018; Kohl et al., 2018). Modeling uncertainty in the context of noisy data has also been looked at through Bayesian techniques (Kendall & Gal, 2017; Tanno et al., 2017), and (for different models) in the context of crowdsourcing by (Werling et al., 2015; Wauthier & Jordan, 2011). A related line of work (Dawid et al., 1979; Welinder & Perona, 2010) has looked at studying the per labeler error rates, which also requires the additional information of labeler ids, an assumption we relax. Most related is (Guan et al., 2018), where a multiheaded neural network is used to model different labelers. Surprisingly however, the best model is independent of image features, which is the source of signal in our experiments.

Direct Uncertainty Prediction for Medical Second Opinions

Task
Variance Prediction Variance Prediction Variance Prediction Variance Prediction Variance Prediction Variance Prediction
Disagreement Prediction Disagreement Prediction Disagreement Prediction Disagreement Prediction
Variance Prediction Disagreement Prediction

UVC UVC DUP DUP DUP DUP
UVC UVC DUP DUP
DUP DUP

Model Type
Histogram-E2E Histogram-PC Variance-E2E Variance-P Variance-PR Variance-PRC
Histogram-E2E Histogram-PC Disagree-P Disagree-PC
Disagree-PC Variance-PRC

Performance (AUC)
70.6% 70.6% 72.9% 74.4% 74.6% 74.8%
73.4% 76.6% 78.1% 78.1%
73.3% 77.3%

Table 3. Performance (percentage AUC) averaged over three runs for UVC and DUPs on Variance Prediction and Disagreement
Prediction tasks. The UVC baselines, which ﬁrst train a classiﬁer on the empirical grade histogram, are denoted Histogram-. DUPs are trained on either Tt(rdaiisnagree) or Tt(rvaairn), and denoted Disagree-, Variance- respectively. The top two sets of rows shows the performance of the baseline (and a strengthened baseline Histogram-PC using Prelogit embeddings and Calibration) compared to Variance and Disagree DUPs on the (1) Variance Prediction task (evaluation on Tt(evsatr)) and (2) Disagreement Prediction task (evaluation on Tt(edsitsagree)). We see that in both of these settings, the DUPs perform better than the baselines. Additionally, the third set of rows shows tests a Variance
DUP on the disagreement task, and vice versa for the Disagreement DUP. We see that both of these also perform better than the baselines.

4. Doctor Disagreements in DR
Our main application studies the effectiveness of Direct Uncertainty Predictors (DUPs) and Uncertainty via Classiﬁcation (UVC) in identifying patient cases with high disagreements amongst doctors in a large-scale medical imaging setting. These patients stand to beneﬁt most from a medical second opinion.
The task contains patient data in the form of retinal fundus photographs (Gulshan et al., 2016), large (587 x 587) images of the back of the eye. These photographs can be used to diagnose the patient with different kinds of eye diseases. One such eye disease is Diabetic Retinopathy (DR), which, despite being treatable if caught early enough, remains a leading cause of blindness (Ahsan, 2015).
DR is graded on a 5-class scale: a grade of 1 corresponds to no DR, 2 to mild DR, 3 to moderate DR, 4 to severe and 5 to proliferative DR (AAO, 2002). There is an important clinical threshold at grade 3, with grades 3 and above corresponding to referable DR (needing immediate specialist attention), and 1, 2 being non-referable. Clinically, the most costly mistake is not referring referable patients, which poses a high risk of blindness.
Our main dataset T has many features typical of medical imaging datasets. T has larger but much fewer images than in natural image datasets such as ImageNet. Each image xi has a few (typically one to three) individual doctor grades yi(1), ..., yi(ni). These grades are also very noisy, with more than 20% of the images having large (referable/non-

referable) disagreement amongst the grades.
4.1. Task Setup
In this section we describe the setup for training variants of DUPs and UVCs using a train test split on T . We outline the resulting model performances in Table 3, which measure how successful the models are in identifying cases where doctors most disagree with each other and consequently where a medical second opinion might be most useful. In Section 5, we perform a different evaluation (disagreement with consensus) of the best performing DUPs and UVCs on a special, gold standard adjudicated test set. In both evaluation settings, we ﬁnd that DUPs noticeably outperform UVCs.
The DUP and UVC models are trained and evaluated using a train/test split on T , Ttrain, Ttest. This split is constructed using the patient ids of the xi ∈ T , with 20% of patient ids being set aside to form Ttest and 80% to form Ttrain (of which 10% is sometimes used as a validation set.) Splitting by patient ids is important to ensure that multiple images xi, xj ∈ T corresponding to a single patient are correctly split (Gulshan et al., 2016).
We apply Udisagree(·) to the xi in Ttrain, Ttest with more than one doctor label to form a new train/test split Tt(rdaiisnagree), Tt(edsitsagree). We repeat this with Uvar(·) to also form a train/test split Tt(rvaairn), Tt(evsatr). These two datasets capture the two different medical interpretations of DR grades:

Direct Uncertainty Prediction for Medical Second Opinions

Categorical Grade Interpretation: The DR grades can be interpreted as categorical classes, as each grade has speciﬁc features associated with it. A grade of 2 always means microaneurysms, while a grade of 5 can refer to lesions or laser scars (AAO, 2002). The Tt(rdaiisnagree), Tt(edsitsagree) data measures disagreement in this categorical setting.
Continuous Grade Interpretation: While there are speciﬁc features associated with each DR grade, patient conditions tend to progress sequentially through the different DR grades. The Tt(rvaairn), Tt(evsatr) data thus accounts for the magnitude of differences in doctor grades.
Having formed Tt(rdaiisnagree), Tt(edsitsagree) and Tt(rvaairn), Tt(evsatr), which consist of pairs (xi, Udisagree(pˆi)) and (xi, Uvar(pˆi)) respectively, we binarize the uncertainty scores Udisagree(pˆi), Uvar(pˆi) into 0 (low uncertainty) or 1 (high uncertainty) to form our ﬁnal prediction targets. We denote these UdBisagree(pˆi), UvBar(pˆi). More details on this can be found in Appendix Section D.

Figure 3. Labels for the adjudicated dataset A. The small, gold standard adjudicated dataset A has a very different label structure to the main dataset T . Each image has many individual doctor grades (typically more than 10 grades). These doctors also tend to be specialists, with higher rates of agreement. Additionally, each image has a single adjudicated grade, where three doctors ﬁrst grade the image individually, and then come together to discuss the diagnosis and ﬁnally give a single, consensus diagnosis.

4.2. Models and First Experimental Results
We train both UVCs and DUPs on this data. All models rely on an Inception-v3 base that, following prior work (Gulshan et al., 2016), is initialized with pretrained weights on ImageNet. The UVC is performed by ﬁrst training a classiﬁer hc on (xi, pˆi) pairs in Ttrain. The output prob-
ability distribution of the classiﬁer, p˜i = hc(xi) is then
used as input to the uncertainty scoring function U (·), i.e. huvc(xi) = U ◦ hc(xi) In contrast, the DUPs are trained directly on the pairs (xi, UdBisagree(pˆi)), (xi, UvBar(pˆi)), i.e. hdup(xi) directly tries to learn the value of U B(pˆi)
The results of evaluating these models (on Tt(edsitsagree) and Tv(adrisagree)) are given in Table 3. The Variance Prediction task corresponds to evaluation on Tv(adrisagree), and the Disagreement Prediction task to evaluation on Tt(edsitsagree). Both tasks correspond to identifying patients where there is high disagreement amongst doctors. As is typical in medical applications due to class imbalances, performance is given via area under the ROC curve (AUC) (Gulshan et al., 2016; Rajpurkar et al., 2017).
From the ﬁrst two sets of rows, we see that DUP models perform better than their UVC counterparts on both tasks. The third set of rows shows the effect of using a variance DUP (Variance-PRC) on the disagreement task and a disagree DUP (Disagree-PC) on the variance task. While these don’t perform as well as the best DUP models on their respective tasks, they still beat both the baseline and the strengthened baseline. Below we describe some of the different UVC and DUP variants, with more details in Appendix Section D.
UVC Models The UVC models are trained on (image, empirical grade histogram) (xi, pˆi) pairs, and denoted

Histogram- in Table 3. The simplest UVC is HistogramE2E, the same model used in (Gulshan et al., 2016). We improved this baseline by instead taking the prelogit embeddings of Histogram-E2E, and training a small neural network (fully connected, two hidden layers width 300) with temperature scaling (as in (Guo et al., 2017)) only on xi with multiple labels. This gave the strengthened baseline Histogram-PC.
DUP Variance Models The simplest Variance DUP is Variance-E2E, which is analogous to Histogram-E2E, except trained on Tt(rvaairn). This performed better than Histogram-E2E, but as Tt(rvaairn) is small for an Inceptionv3, we trained a small neural network (fully connected, two hidden layers width 300) on the prelogit embeddings, called Variance-P. Small variants of Variance-P (details in Appendix Section D) give Variance-PR, and Variance-PRC.
DUP Disagreement Models Informed by the variance models, the Disagree-P model was designed exactly like the Variance-P model (a small fully connected network on prelogit embeddings), but trained on Tt(rdaiisnagree). A small variant of this with calibration gave Disagree-PC.
In the Appendix, we demonstrate similar results using entropy as the uncertainty function, as well as experiments studying convergence speed and ﬁnite sample behaviour of DUP and UVC. We ﬁnd that the performance gap between DUP and UVC is robust to train data size, and manifests early in training.

Direct Uncertainty Prediction for Medical Second Opinions

UVC UVC UVC UVC DUP DUP DUP DUP

Model Type
Histogram-E2E-Var Histogram-E2E-Disagree Histogram-PC-Var Histogram-PC-Disagree Variance-PR Variance-PRC Disagree-P Disagree-PC

Majority
78.1% 78.5% 77.9% 79.0% 80.0% 79.8% 81.0% 80.9%

Median
78.2% 78.5% 78.0% 78.9% 79.9% 79.7% 80.8% 80.9%

Majority = 1
81.3% 80.5% 80.2% 80.8% 83.1% 82.7% 84.6% 84.5%

Median = 1
78.1% 77.0% 77.7% 79.2% 80.5% 80.2% 81.9% 81.8%

Referable
85.5% 84.2% 85.0% 84.8% 85.9% 85.9% 86.2% 86.2%

Table 4. Evaluating models (percentage AUC) on predicting disagreement between an average individual doctor grade and the adjudicated grade. We evaluate our models’s performance using multiple different aggregation metrics (majority, median, binarized non-referable/referable median) as well as special cases of interest (no DR according to majority, no DR according to median). We observe that all direct uncertainty models (Variance-, Disagree-) outperform all classiﬁer-based models (Histogram-) on all tasks.

5. Predicting Disagreement with Consensus: Adjudicated Evaluation
Section 4 trained DUPs and UVCs on Ttrain, and evaluated them on their ability to identify patient cases where individual doctors were most likely to disagree with each other. Here, we take these trained DUPs/UVCs, and perform an adjudicated evaluation, to satisfy two additional goals.
Firstly, and most importantly, the clinical question of interest is not only in identifying patients where individual doctors disagree with each other, but cases where a more thorough diagnosis – the best possible doctor grade – would disagree signiﬁcantly with the individual doctor grade. Evaluation on a gold-standard adjudicated dataset A enables us to test for this: each image xi ∈ A not only has many individual doctor grades (by specialists in the disease) but also a single adjudicated grade. This grade is determined by a group of doctors seeking to reach a consensus on the diagnosis through discussion (Krause et al., 2018). Figure 3 illustrates the setup.
We can thus evaluate on this question by seeing if high model uncertainty scores correspond to disagreements between the (average) individual doctor grade and the adjudicated grade. More speciﬁcally, we compute different aggregations of the individual doctor grades for xi ∈ A, and give a binary label for whether this aggregation agrees with the adjudicated grade (0 for agreement, 1 for disagreement). We then see if our model uncertainty scores is predictive of the binary label.
Secondly, our evaluation on A also provides a more accurate reﬂection of our models’s performance, with less confounding noise. The labels in A (both individual and adjudicated) are much cleaner, with greater consistency amongst doctors. As A is used solely for evaluation (all evaluated models are trained on Ttrain, Section 4), this introduces a distribution shift, but the predicted uncertainty scores transfer well. The results are shown in Table 4. We evaluate on several differ-

ent aggregations of individual doctor grades. Like (Gulshan et al., 2016), we compare agreement between the majority vote of the individual grades and the adjudicated grades. To compensate for a bias of individual doctors giving lower DR grades (Krause et al., 2018), we also look at agreement between the median individual grade and adjudicated grade. Additionally, we look at referable/non-referable DR grade agreement. We binarize both the individual doctor grades and the adjudicated grade into 0/1 non-referable/referable, and check agreement between the median binarized grades and adjudicated grade. Finally, we also look at the special case where the average doctor grade is 1 (no DR), and compare agreement with the adjudicated grade.
We evaluate both baseline models (Histogram-E2E, Histogram-PC) as well as the best performing DUPs, (Variance-PR, Variance-PRC, Disagree-P, Disagree-PC.) The additional -Var, -Disagree sufﬁxes on the baseline models indicate which uncertainty function (Uvar or Udisagree)
was used to postprocess the classiﬁer output distribution p˜ to
get an uncertainty score. We ﬁnd that all DUPs outperform all the baselines on all evaluations.
5.1. Ranking Evaluation
A frequent practical challenge in healthcare is to rank cases in order of hardest (needing most attention) to easiest (needing least attention), (Harrell Jr et al., 1984). Therefore, we evaluate how well our models can rank cases from greatest disagreement between the adjudicated and individual grades to least disagreement between the adjudicated and individual grades. To do this however, we need a continuous ground truth value reﬂecting this disagreement, instead of the binary 0/1 agree/disagree used above. One natural way to do this is to compute the Wasserstein distance between the empirical histogram (individual grade distribution) and the adjudicated grade.
At a high level, the Wasserstein distance computes the minimal cost required to move a probability distribution p(1) to

Direct Uncertainty Prediction for Medical Second Opinions

UVC UVC UVC UVC DUP DUP DUP DUP

Prediction Type
Histogram-E2E-Var Histogram-E2E-Disagree Histogram-PC-Var Histogram-PC-Disagree Variance-PR Variance-PRC Disagree-P Disagree-PC
2 Doctors 3 Doctors 4 Doctors 5 Doctors 6 Doctors

Absolute Val
0.650 0.645 0.638 0.660 0.671 0.665 0.682 0.680
0.460 0.585 0.641 0.676 0.728

2-Wasserstein
0.644 0.633 0.639 0.655 0.664 0.658 0.670 0.669
0.448 0.576 0.634 0.670 0.712

Binary Disagree
0.643 0.643 0.619 0.649 0.660 0.656 0.676 0.675
0.455 0.580 0.644 0.675 0.718

Table 5. Ranking evaluation of models uncertainty scores using Spearman’s rank correlation. In the top set of rows, we compare the ranking induced by the model uncertainty scores to the (ground truth) ranking induced by the Wasserstein distance between the empirical grade histogram and the adjudicated grade. We use three different metrics for evaluating Wasserstein distance: absolute value distance, 2-Wasserstein and Binary agree/disagree (more details in Appendix F.) Again, we see that all DUPs outperform all baselines on all metrics. The second set of rows provides another way to interpret these results. We subsample n doctors to create a new subsampled empirical grade histogram, and compare the ranking induced by the Wasserstein distance between this and the adjudicated grade to the ground truth ranking. We can thus say that the average DUP ranking corresponds to having 5 doctor grades, and the average UVC ranking corresponds to 4 doctor grades.

a probability distribution p(2) with respect to a given metric d(·). In our setting, p(1) is the empirical histogram pˆi of xi, and p(2) is the point mass at the adjudicated grade ai. When one distribution is a point mass, the Wasserstein distance has a simple interpretation:
Theorem 2. Let p(1) and p(2) be two probability distributions, with p(2) a point mass with non-zero value a. Let d(·) be a given metric. The Wasserstein distance between p(1) and p(2), ||p(1) − p(2)||w with respect to d(·) can be written as
||p(1) − p(2)||w = EC∼p(1) [d(C, a)]
The proof is in Appendix F. In our setting, the theorem says that the (continuous) disagreement score for xi ∈ A is just the expected distance between a grade drawn from the empirical histogram and the adjudicated grade. We consider three different distance functions d(·): (a) the absolute value of the grade difference, (b) the 2 − W asserstein distance, a metrization of the squared distance penalizing large grade differences more (details in Appendix F) and (c) a 0/1 binary agree/disagree metric, in line with the categorical and continous interpretations of DR grades, Section 4.
We compare the ranking induced by this continuous disagreement score on A with the ranking induced by the model’s predicted uncertainty scores. To evaluate the similarity of these rankings, we use Spearman’s rank correlation (Spearman, 1904), which takes a value between [−1, 1]. A −1 indicates perfect negative rank correlation, 1 a perfect

positive rank correlation and 0 no correlation. The results are shown in Table 5. Similar to Table 4, we observe strong performance with DUPs: all DUPs beat all the baselines on all the different distances.
This task also enables a natural comparison between the models and doctors. In particular, we can compute a third ranking over A, by sampling n individual doctor grades, and computing the Wasserstein distance between this subsampled empirical histogram and the adjudicated grade. This experiment tells us how many doctor grades are needed to give a ranking as accurate as the models. For DUPs, we need on average 5 doctors, while for the UVC baseline, we need on average 4 doctors.
6. Discussion
In this paper, we show that machine learning models can successfully be used to predict data instances that give rise to high expert disagreement. The main motivation for this prediction problem is the medical domain, where some patient cases result in signiﬁcant differences in doctor diagnoses, and may beneﬁt greatly from a medical second opinion. We show, both with a formal result and through extensive experiments, that Direct Uncertainty Prediction, which learns an uncertainty score directly from the raw patient features, performs signiﬁcantly better than Uncertainty via Classiﬁcation. Future work might look at transferring these techniques to different data modalities, and extending the applications to machine learning data denoising.

Direct Uncertainty Prediction for Medical Second Opinions

ACKNOWLEDGMENTS
We thank Varun Gulshan and Arunachalam Narayanaswamy for detailed advice and discussion on the model, and David Sontag and Olga Russakovsky for speciﬁc suggestions. We also thank Quoc Le, Martin Wattenberg, Jonathan Krause, Lily Peng and Dale Webster for general feedback. We thank Naama Hammel and Zahra Rastegar for helpful medical insights. Robert Kleinberg’s work was partially supported by NSF grant CCF-1512964.
References
AAO. International Clinical Diabetic Retinopathy Disease Severity Scale Detailed Table. American Academy of Ophthalmology, 2002.
Abrams, L. S., Scott, I. U., Spaeth, G. L., Quigley, H. A., and Varma, R. Agreement among optometrists, ophthalmologists, and residents in evaluating the optic disc for glaucoma. Ophthalmology, 101(10):1662–1667, 1994.
Ahsan, H. Diabetic retinopathy – biomolecules and multiple pathophysiology. Diabetes and Metabolic Syndrome: Clincal Research and Review, pp. 51–54, 2015.
Chen, I., Johansson, F. D., and Sontag, D. Why is my classiﬁer discriminatory? In Bengio, S., Wallach, H., Larochelle, H., Grauman, K., Cesa-Bianchi, N., and Garnett, R. (eds.), Advances in Neural Information Processing Systems 31, pp. 3543–3554. Curran Associates, Inc., 2018.
Daniel, T. M. Toman’s tuberculosis. Case detection, treatment and monitoring: questions and answers. ASTMH, 2004.
Dawid, P., Skene, A. M., Dawidt, A. P., and Skene, A. M. Maximum likelihood estimation of observer error-rates using the em algorithm. Applied Statistics, pp. 20–28, 1979.
De Fauw, J., Ledsam, J. R., Romera-Paredes, B., Nikolov, S., Tomasev, N., Blackwell, S., Askham, H., Glorot, X., ODonoghue, B., Visentin, D., et al. Clinically applicable deep learning for diagnosis and referral in retinal disease. Nature medicine, 24(9):1342, 2018.
Elmore, J. G., Longton, G. M., Carney, P. A., Geller, B. M., Onega, T., Tosteson, A. N., Nelson, H. D., Pepe, M. S., Allison, K. H., Schnitt, S. J., et al. Diagnostic concordance among pathologists interpreting breast biopsy specimens. Jama, 313(11):1122–1132, 2015.
Guan, M. Y., Gulshan, V., Dai, A. M., and Hinton, G. E. Who said what: Modeling individual labelers improves classiﬁcation, 2018. URL

https://aaai.org/ocs/index.php/AAAI/ AAAI18/paper/view/16970.
Gulshan, V., Peng, L., Coram, M., Stumpe, M. C., Wu, D., Narayanaswamy, A., Venugopalan, S., Widner, K., Madams, T., Cuadros, J., Kim, R., Raman, R., Nelson, P. Q., Mega, J., and Webster, D. Development and validation of a deep learning algorithm for detection of diabetic retinopathy in retinal fundus photographs. JAMA, 316 (22):2402–2410, 2016.
Guo, C., Pleiss, G., Sun, Y., and Weinberger, K. Q. On calibration of modern neural networks. abs/1706.04599, 2017. URL http://arxiv.org/abs/1706. 04599.
Gurari, D., He, K., Xiong, B., Zhang, J., Sameki, M., Jain, S. D., Sclaroff, S., Betke, M., and Grauman, K. Predicting foreground object ambiguity and efﬁciently crowdsourcing the segmentation (s). International Journal of Computer Vision, 126(7):714–730, 2018.
Harrell Jr, F. E., Lee, K. L., Califf, R. M., Pryor, D. B., and Rosati, R. A. Regression modelling strategies for improved prognostic prediction. Statistics in medicine, 3 (2):143–152, 1984.
Kendall, A. and Gal, Y. What uncertainties do we need in bayesian deep learning for computer vision? In Advances in Neural Information Processing Systems 2017, 4-9 December 2017, Long Beach, CA, USA, volume 30, pp. 5580–5590, 2017.
Kohl, S. A., Romera-Paredes, B., Meyer, C., De Fauw, J., Ledsam, J. R., Maier-Hein, K. H., Eslami, S., Rezende, D. J., and Ronneberger, O. A probabilistic u-net for segmentation of ambiguous images. arXiv preprint arXiv:1806.05034, 2018.
Krause, J., Gulshan, V., Rahimy, E., Karth, P., Widner, K., Corrado, G. S., Peng, L., and Webster, D. R. Grader variability and the importance of reference standards for evaluating machine learning models for diabetic retinopathy. Ophthalmology, 125 8:1264–1272, 2018.
Mnih, V. and Hinton, G. Learning to label aerial images from noisy data. International Conference on Machine Learning, 2012.
Natarajan, N., Dhillon, I. S., Ravikumar, P. K., and Tewari, A. Learning with noisy labels. In Advances in Neural Information Processing Systems 26, pp. 1196–1204. 2013.
Rajpurkar, P., Irvin, J., Zhu, K., Yang, B., Mehta, H., Duan, T., Ding, D., Bagul, A., Langlotz, C., Shpanskaya, K., Lungren, M. P., and Ng, A. Y. Chexnet: Radiologist-level pneumonia detection on chest x-rays with deep learning. abs/1711.05225, 2017. URL http://arxiv.org/ abs/1711.05225.

Direct Uncertainty Prediction for Medical Second Opinions

Reed, S. E., Lee, H., Anguelov, D., Szegedy, C., Erhan, D., and Rabinovich, A. Training deep neural networks on noisy labels with bootstrapping. abs/1412.6596, 2014. URL http://arxiv.org/abs/1412.6596.
Rolnick, D., Veit, A., Belongie, S. J., and Shavit, N. Deep learning is robust to massive label noise. CoRR, abs/1705.10694, 2017. URL http://arxiv.org/ abs/1705.10694.
Russakovsky, O. and Fei-Fei, L. Attribute learning in largescale datasets. In European Conference of Computer Vision (ECCV), International Workshop on Parts and Attributes, 2010.
Sheng, V. S., Provost, F., and Ipeirotis, P. G. Get another label? improving data quality and data mining using multiple, noisy labelers. In Proceedings of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’08, pp. 614– 622. ACM, 2008. ISBN 978-1-60558-193-4. doi: 10.1145/1401890.1401965.
Smilkov, D., Thorat, N., Kim, B., Vie´gas, F., and Wattenberg, M. Smoothgrad: removing noise by adding noise. arXiv preprint arXiv:1706.03825, 2017.
Spearman, C. The proof and measurement of association between two things. The American Journal of Psychology, pp. 72–101, 1904.
Sukhbaatar, S., Bruna, J., Paluri, M., Bourdev, L., and Fergus, R. Training convolutional networks with noisy labels. CoRR, abs/1406.2080, 2014. URL http://arxiv. org/abs/1406.2080.
Sundararajan, M., Taly, A., and Yan, Q. Axiomatic attribution for deep networks. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pp. 3319–3328. JMLR. org, 2017.
Tanno, R., Worrall, D., Ghosh, A., Kaden, E., N. Sotiropoulos, S., Criminisi, A., and C. Alexander, D. Bayesian image quality transfer with cnns: Exploring uncertainty in dmri super-resolution. Medical Image Computing and Computer Assisted Intervention, pp. 611–619, 2017.
Van Such, M., Lohr, R., Beckman, T., and Naessens, J. M. Extent of diagnostic agreement among medical referrals. Journal of evaluation in clinical practice, 23(4):870–874, 2017.
Veit, A., Alldrin, N., Chechik, G., Krasin, I., Gupta, A., and Belongie, S. J. Learning from noisy large-scale datasets with minimal supervision. 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 6575–6583, 2017.

Wauthier, F. L. and Jordan, M. I. Bayesian bias mitigation for crowdsourcing. In Shawe-Taylor, J., Zemel, R. S., Bartlett, P. L., Pereira, F., and Weinberger, K. Q. (eds.), Advances in Neural Information Processing Systems 24, pp. 1800–1808. 2011.
Welinder, P. and Perona, P. Online crowdsourcing: Rating annotators and obtaining cost-effective labels. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR Workshops 2010, San Francisco, CA, USA, 13-18 June, 2010, pp. 25–32, 2010. doi: 10.1109/ CVPRW.2010.5543189. URL https://doi.org/ 10.1109/CVPRW.2010.5543189.
Werling, K., Chaganty, A. T., Liang, P. S., and Manning, C. D. On-the-job learning with bayesian decision theory. In Cortes, C., Lawrence, N. D., Lee, D. D., Sugiyama, M., and Garnett, R. (eds.), Advances in Neural Information Processing Systems 28, pp. 3465–3473. 2015.
Xiao, T., Xia, T., Yang, Y., Huang, C., and Wang, X. Learning from massive noisy labeled data for image classiﬁcation. 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2691–2699, 2015.

Direct Uncertainty Prediction for Medical Second Opinions
Appendix A. Proofs of Direct Uncertainty Prediction Results
We ﬁrst prove Theorem 1.

Proof. To show unbiasedness of hdup, we need to show that E[hdup] = E[U (Y)]. But from the tower law (law of total expectation):
E[hdup] = E E U (E[Y|O]) g(O) = E[U (E[Y|O])]
To prove the biasedness of huvc, ﬁrst note that huvc = U (E[Y|g(O)]) = U (E[E[Y|O]|g(O)])
by the conditional independence of Y, g(O) given O. Next, by the fact that U (·) is concave and Jensen’s inequality,
huvc = U (E[E[Y|O]|g(O)]) ≥ E[U (E[Y|O]|g(O)] = hdup This is a strict inequality whenever the distribution of posteriors induced by conditioning on g(O) is not a point-mass. Therefore we have that huvc overestimates the the true uncertainty U (Y).

For speciﬁc U (·), we can compute the bias term by ﬁrst computing huvc − hdup and then taking an expectation. For Udisagree, we have

huvc = U (E[Y|g(O)]) = 1 − E[E[Yl|O]|g(O)]2

l

and hdup = E[U (E[Y|O])|g(O)] = 1 − E[E[Yl|O]2|g(O)]

l

And so,

huvc − hdup = E[E[Yl|O]2|g(O)] − E[E[Yl|O]|g(O)]2

l

l

But this is just

V ar

E[E[Yl|O]|g(O)]

l

Taking expectations over values of g(O) gives the bias, i.e.

E V ar

E[E[Yl|O]|g(O)]
l

For Uvar, we have

huvc = l2E[E[Yl|O]|g(O)] −
l
and 

hdup = E  l2E[Yl|O] −
l

And so huvc − hdup becomes



2



E

lE[Yl|O]

l

g(O) −

2
lE[E[Yl|O]|g(O)]
l

2



lE[Yl|O]
l

g(O)

2
lE[E[Yl|O]|g(O)]
l

Which is just

V ar

l · E[Yl|O] g(O)

l

Taking expectations over values of g(O) like before gives the result.

Direct Uncertainty Prediction for Medical Second Opinions
B. Mixture of Gaussians Setting
We train a DUP and UVC (Figure 1) on a synthetic task where data is generated from a mixture of Gaussians. All of our settings have uniform mixtures of Gaussians, with the Gaussian mean vectors being drawn from N (0, 1/d) (d corresponding to the dimension) so that in expectation, each mean vector has norm 1. The variance is set to be the identity. Like before, we set x = g(o) = |o|. We draw ﬁve labels for each x from the posterior distribution over Gaussian centers given x, and apply Udisagree() to the empirical histogram. As with the medical imaging application, we threshold these uncertainty scores (with threshold 0.5) to give a binary low uncertainty/high uncertainty label, which we use to train our DUPs and UVCs. Results are given in percentage AUC to account for some settings having unbalanced classes.
We train fully connected networks with two hidden layers of width 300 on this task, using the SGD with momentum optimizer and an initial learning rate of 0.01.
C. SVHN and CIFAR-10 Setting
In Section 2.2, we train DUP and UVC models to predict label disagreement on a synthetic task on SVHN and CIFAR-10. The task setup is as follows: for each image in SVHN/CIFAR-10, we decide on a variance (0, 1, 2, 3) for a Gaussian ﬁlter that is applied to the image. Three labels are then drawn for the image from a noisy distribution over labels, with the label noise distribution depending on the variance of the Gaussian ﬁlter. Speciﬁcally, for a Gaussian ﬁlter with variance 0, the noise distribution is just a point mass on the true label. For a Gaussian ﬁlter with variance 1, the three labels are drawn from a distribution with 0.02 mass on four incorrect labels, and the remaining 0.92 mass on the correct label. For variance equal to 2, the labels are drawn from a distribution with 0.08 mass on four different labels, and remaining mass on the true label. For variance 3, this mass is now 0.12 on the incorrect labels.
A simple conv network, with 3x3 kernels and channels 64 − −128 − −256, followed by fully connected layers of width 1000 and 200 (each with batch normalization) is trained on this dataset, with the UVC model trained on the empirical histogram, and the DUP model trained on a binary agree/disagree target. (Disagreement threshold is if at least one label disagrees.) We ﬁnd that DUP outperforms UVC on both SVHN and CIFAR-10.
Learned Features Interestingly, we also observe that the features learned by the DUP and UVC models are different to each other. We apply saliency maps, speciﬁcally SmoothGrad (Smilkov et al., 2017) and IntGrad (Sundararajan et al., 2017) to study the features that DUP and UVC pay attention to in the input image.
D. Details of DUP in the Medical Domain
As described in Section 4, to train DUP models, we threshold the scores given by applying Udisagree, Uvar to the data (xi, pˆi). Preliminary experiments in trying to directly regress onto the raw scores using mean-squared error performed poorly.
We threshold the scores as follows. For Uvar we thresholded at approximately 2/9, the variance when three doctors have more than an ’off by one’ disagreement: more than a single disagreement, or a single grade disagreement.
For Udisagree, where only the number of disagreements counts, we thresholded at 0.3, to prioritize being sensitive enough to disagreement cases and having more than 20% of the data marked as high disagreement. We also experimented with using soft targets for disagreement classiﬁcation, but the results (Table 6) showed that this was less effective than than having the binary 0/1 scores, likely because this makes the classiﬁcation problem more like a regression.
Our model consists of an Inception-v3 base, with the ImageNet head removed and a small (2 hidden layer, 300 hidden units) fully connected neural network using Inception-v3 PreLogits to perform DUP. The full Inception-v3 network is trained with a batch size of 8 and learning rate 0.001 with the Adam optimizer. For training only the small neural network, we use the SGD with momentum optimizer, a batch size of 32 and learning rate of 0.01.
Prelogits, Calibration and Regularization Our training data for DUP models, Tt(rvaairn), Tt(rdaiisnagree), only consists of xi with more than one label, and is too small to effectively train an Inception sized model end to end. Therefore, we use the prelogit embeddings of xi from a pretrained DR classiﬁcation model (Histogram-E2E), and training smaller models on top of these embeddings. We do this both for the baseline, getting the Histogram-PC model, as well as the DUP models, Variance-PRC and Disagree-PC.

Direct Uncertainty Prediction for Medical Second Opinions

0 5 10 15 20 25 30
0 5 10 15 20 25 30
0 5 10 15 20 25 30
0 5 10 15 20 25 30
0 5 10 15 20 25 30
0 5 10 15 20 25 30
0 5 10 15 20 25 30
0 5 10 15 20 25 30

SmoothGrad SmoothGrad SmoothGrad SmoothGrad

SmoothGrad SmoothGrad SmoothGrad SmoothGrad

IntGrad IntGrad IntGrad IntGrad

IntGrad IntGrad IntGrad IntGrad

Figure 4. Saliency maps for DUP and UVC models on the SVHN/CIFAR-10 disagreement task. The plot shows two images from the blurred CIFAR-10 dataset and two images from the blurred SVHN dataset. The second column is SmoothGrad applied to the UVC model, and the third SmoothGrad applied to the DUP model. The third and fourth columns show IntGrad applied to the DUP and UVC models. We observe that the DUP and UVC models appear to be paying attention to different features of the dataset.

Model Type
Disagree Soft Targets Disagree-P Disagree-PC

Ttest AUC
76.3% 78.1% 78.1%

Majority
79.0% 81.0% 80.9%

Median
78.7% 80.8% 80.9%

Majority= 1
81.6% 84.6% 84.5%

Median= 1
79.0% 81.9% 81.8%

Referable
84.7% 86.2% 86.2%

Table 6. Using soft targets for disagreement prediction does not help in performance (AUC). Holdout AUC column corresponds to Disagreement Prediction Performance in Table 3, other columns refer to Table 4 in main text.

Direct Uncertainty Prediction for Medical Second Opinions

Task Variance Prediction
Variance Prediction Disagreement Prediction

Model Type Variance-E2E-2H
Variance-LR Disagree-LR

Performance (AUC) 72.7%
72.4% 75.9%

Table 7. Additional results from table 3.

Task
Entropy Prediction Entropy Prediction

UVC DUP

Model Type
Histogram-PC Disagree-P

Performance (AUC)
75.5% 77.2%

Table 8. DUP and UVC models trained with entropy as a target function. Again, we see that the DUP model outperforms the UVC model.
The C sufﬁx of all of these models corresponds to calibration on the logits. Following the ﬁndings of (Guo et al., 2017), we apply temperature scaling on the logits: we set the predictions of the model to be f ( /T ) where f is the softmax function, applied pointwise, and are the logits. We initialize T to 1, and then split e.g. Tt(rdaiisnagree) into a Ttr(adiinsagree) and a Tv(adliidsagree), with 10% of the data in the validation set. We train as normal on Ttr(adiinsagree), with T ﬁxed at 1, and then train on Tv(adliidsagree), by only varying the temperature T , and holding all other parameters ﬁxed.
The use of Prelogit embeddings and Calibration gives the strongest performing baseline UVC and DUPs: Histogram-PC, Variance-PRC and Disagree-PC. For the Variance DUP, an additional regularization term is added to the loss by having a separate regressing on the raw variance value.
Additional Model: Variance-E2E We tried a variant of Variance-E2E, Variance-E2E-2H, which has one head for predicting variance and the other for classifying, to enable usage of all the data. We then evaluate the variance head on Ttest, but in fact noticed a small drop in performance, Table 7.
Do we need the Prelogit embeddings? We tried seeing if we could match performance by training on pretrained classiﬁer logits instead of the prelogit embeddings. Despite controlling for parameter difference by experimenting with more hidden layers, we found we were unable to match performance from the prelogit layer, Table 7, compare to Table 3. This demonstrates that some information is lost between the prelogit and logit layers.
E. Additional Results: Entropy, Finite Sample Behavior and Convergence Analysis
We performed additional experiments to further understand the properties of DUP and UVC models. For these experiments, we compare a representative DUP model, Disagree-P, to a representative UVC model, Histogram-PC.
Theorem 1 states that DUP offers beneﬁts over UVC for concave target uncertainty functions. This is a natural property for measures of spread, simply stating that the measure of spread increases with averaging (probability distributions). In the main text, we concentrate on two such speciﬁc uncertainty functions, Uvar and Udisagree, which are particularly suited to the domain. However, other standard uncertainty functions, such as entropy, are also concave. We test the performance of DUP (Disagree-P) and UVC (Histogram-PC) with Uentropy as the target function.
The results are shown in Table 8, where we again see that DUP outperforms UVC.
We also study how model performance is impacted by different training set sizes (similar to the analysis in (Chen et al., 2018)). We subsample different amounts of the original training set Tt(rdaiisnagree), and train DUP and UVC models on this subset. The results over 5 repeats of different subsamples and optimization runs are shown in Figure 5.
We see that the performance gap between DUP and UVC is robust to train data size differences. Additionally, when ≥ 30% of the training data is used, DUP and UVC performance remains relatively constant. This supports carrying over the results of Theorem 1) and the full joint distribution f (o, y) to the ﬁnite data setting.

AUC AUC

0.785 0.780 0.775 0.770 0.765 0.7600.0

Direct Uncertainty Prediction for Medical Second Opinions

Performance with Varying Amounts of Training Data
DUP UVC 0.2 0.4 0.6 0.8 1.0 Fraction of Data Sampled

0.80 Performance through Training

0.79

DUP UVC

0.78

0.77

0.76

0.75

0.74

0.73

0.720%

20% 40% 60% 80% 100% Percentage Train Time

Figure 5. DUP and UVC performance during training and when varying train data size. We study DUP (Disagree-P) and UVC (Histogram-PC) performance for varying amounts of training data. We ﬁnd that the gap in performance is robust to variations in dataset size. For more than 30% of the data, performance of DUP and UVC remains relatively constant, supporting the applicability of Theorem 1 in the ﬁnite data setting. The right plot looks at performance through training, with the gap appearing rapidly early in training, and slowly widening.
We also study convergence of DUP and UVC models. We ﬁnd that the performance gap between DUP and UVC manifests very early in training (Figure 5, right plot), and continues to gradually widen through training.

F. Background on the Wasserstein Distance
Given two probability distributions f, g, and letting Π(f, g) be all product probability distributions with marginals f , g, the Wasserstein distance between p, q is
||f − g||w = inf E(r,t)∼π [d(r, t)]
π∈Π(f,g)
where d(, ) is some metric. This distance has connections to optimal transport, and corresponds to the minimum cost (with respect to d(, )) of moving the mass in distribution f so that it is matches the mass in distribution g. We can represent the amount of mass to move from r to t with π(r, t). To be consistent with the mass at the start, f (r), and the mass at the end g(t) we must have that t π(r, t ) = f (r) and r π(r , t) = g(t).
The result in the main text follows from the following theorem:
Theorem 3. If f, g are (discrete) probability distributions and g is a point mass distribution at t0, then π ∈ Π(f, g) is uniquely deﬁned as:
π(r, t) = 0 if t = t0 f(r) if t = t0

Proof. The proof is direct: for t = t0, we must have r π(r , t) = g(t) = 0, and so t π(r, t ) = π(r, t0) = f (r). We consider three different distances d(, ):

1 Absolute Value d(r, t) = |r − t|. This follows an interpretation in which the grades are equally spaced, so that all successive grade differences have the same weight.
2 2-Wasserstein Distance d(r, t) = (r − t)2, and, to make into a metric ||f − g||w = E(r,t)∼π [d(r, t)] 1/2
This adds a higher penalty for larger grade differences. 3 Binary Disagreement We set d(r, t) = 0 if r = t and 1 otherwise.

