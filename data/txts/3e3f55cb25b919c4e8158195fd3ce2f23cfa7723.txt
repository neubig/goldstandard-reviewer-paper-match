Counterfactual VQA: A Cause-Effect Look at Language Bias
Yulei Niu1 Kaihua Tang1 Hanwang Zhang1 Zhiwu Lu2,3 Xian-Sheng Hua4 Ji-Rong Wen2,3
1Nanyang Technological University, Singapore 2Gaoling School of Artiﬁcial Intelligence, Renmin University of China, Beijing, China 3Beijing Key Laboratory of Big Data Management and Analysis Methods 4Damo Academy, Alibaba Group
yn.yuleiniu@gmail.com, {kaihua001@e., hanwangzhang@}ntu.edu.sg, {luzhiwu, jrwen}@ruc.edu.cn

arXiv:2006.04315v4 [cs.CV] 1 Apr 2021

Abstract
VQA models may tend to rely on language bias as a shortcut and thus fail to sufﬁciently learn the multi-modal knowledge from both vision and language. Recent debiasing methods proposed to exclude the language prior during inference. However, they fail to disentangle the “good” language context and “bad” language bias from the whole. In this paper, we investigate how to mitigate language bias in VQA. Motivated by causal effects, we proposed a novel counterfactual inference framework, which enables us to capture the language bias as the direct causal effect of questions on answers and reduce the language bias by subtracting the direct language effect from the total causal effect. Experiments demonstrate that our proposed counterfactual inference framework 1) is general to various VQA backbones and fusion strategies, 2) achieves competitive performance on the language-bias sensitive VQA-CP dataset while performs robustly on the balanced VQA v2 dataset without any augmented data. The code is available at https://github.com/yuleiniu/cfvqa.
1. Introduction
Visual Question Answering (VQA) [8, 4] has become the fundamental building block that underpins many frontier interactive AI systems, such as visual dialog [16], visionand-language navigation [6], and visual commonsense reasoning [54]. VQA systems are required to perform visual analysis, language understanding, and multi-modal reasoning. Recent studies [20, 3, 8, 20, 27] found that VQA models may rely on spurious linguistic correlations rather than multi-modal reasoning. For instance, simply answering “tennis” to the sport-related questions and “yes” to the questions “Do you see a ...” can achieve approximately 40% and 90% accuracy on the VQA v1.0 dataset. As a result, VQA models will fail to generalize well if they simply memorize the strong language priors in the training data [2, 20], especially on the recently proposed VQA-CP [3] dataset where the priors are quite different in the training and test sets.

One straightforward solution to mitigate language bias is to enhance the training data by using extra annotations or data augmentation. In particular, visual [15] and textual [25] explanations are used to improve the visual grounding ability [40, 47]. Besides, counterfactual training samples generation [12, 1, 58, 19, 31] helps to balance the training data, and outperform other debiasing methods by large margins on VQA-CP. These methods demonstrate the effect of debiased training to improve the generalizability of VQA models. However, it is worth noting that VQA-CP was proposed to validate whether VQA models can disentangle the learned visual knowledge and memorized language priors [3]. Therefore, how to make unbiased inference under biased training still remains a major challenge in VQA. Another popular solution [11, 14] is using a separate question-only branch to learn the language prior in the training set. During the test stage, the prior is mitigated by excluding the extra branch. However, we argue that the language prior consists of both “bad” language bias (e.g., binding the color of bananas with the major color “yellow”) and “good” language context (e.g., narrowing the answer space based on the question type “what color”). Simply excluding the extra branch cannot make use of the good context. Indeed, it is still challenging for recent debiasing VQA methods to disentangle the good and bad from the whole.
Motivated by counterfactual reasoning and causal effects [32, 33, 34], we propose a novel counterfactual inference framework called CF-VQA to reduce language bias in VQA. Overall, we formulate language bias as the direct causal effect of questions on answers, and mitigate the bias by subtracting the direct language effect from the total causal effect. As illustrated in Figure 1, we introduce two scenarios, conventional VQA and counterfactual VQA, to estimate the total causal effect and direct language effect, respectively. These two scenarios are deﬁned as follows:
Conventional VQA: What will answer A be, if machine hears question Q, sees image V , and extracts the multimodal knowledge K?
Counterfactual VQA: What would A be, if machine hears Q, but had not extracted K or seen V ?

1

Biased Training

What color are the bananas?

A: Yellow.

A: Yellow.

A: Yellow.

A: Yellow.

A: Green.

Debiased Inference

What color are the bananas?

mostly yellow, seldom green green banana
language prior
multi-modal knowledge

(total effect)
yellow green white
Conventional VQA

(Imagination)

Answer: Yellow.
yellow green white
(traditional strategy)

Answer: Green.
yellow green white

mostly yellow, seldom green
green banana
yellow green white

(pure language effect)
yellow green white

Counterfactual VQA

yellow green white

(CF-VQA)

Figure 1: Our cause-effect look at language bias in VQA. Conventional VQA depicts the fact where machine hears the question and extracts the multi-modal knowledge. Counterfactual VQA depicts the scenario where machine hears the question but the knowledge is blocked. We subtract the pure language effect from the total effect for debiased inference.

Intuitively, conventional VQA depicts the scenario where both Q and V are available. In this case, we can estimate the total causal effect of V and Q on A. However, conventional VQA cannot disentangle the single-modal linguistic correlation and multi-modal reasoning, i.e., direct and indirect effects. Therefore, we consider the following counterfactual question: “What would have happened if the machine had not performed multi-modal reasoning?” The answer to this question can be obtained by imagining a scenario where the machine hears Q, but the multi-modal knowledge K is intervened under the no-treatment condition, i.e., V and Q had not been accessible. Since the response of K to Q is blocked, VQA models can only rely on the singlemodal impact. Therefore, language bias can be identiﬁed by estimating the direct causal effect of Q on A, i.e., pure language effect. The training stages follows language-prior based methods [11, 14] that train an ensemble model with a prevailing VQA model and single-modal branches. During the test stage, CF-VQA uses the debiased causal effect for inference, which is obtained by subtracting the pure language effect from the total effect. Perhaps surprisingly, recent language-prior based methods [11, 14] can be further uniﬁed into our proposed counterfactual inference framework as special cases. In particular, CV-VQA can easily improve RUBi [11] by 7.5% with only one more learnable parameter. Experimental results show that CF-VQA outperforms the methods without data argumentation by large margins on the VQA-CP dataset [3] while remaining stable on the balanced VQA v2 dataset [20].

The main contribution of this paper is threefold. First, our counterfactual inference framework is the ﬁrst to formulate language bias in VQA as causal effects. Second, we provide a novel causality-based interpretation for recent debiasing VQA works [11, 14]. Third, our cause-effect look is general and suitable for different baseline VQA architectures and fusion strategies.
2. Related Work
Language Bias in VQA can be interpreted in two ways. First, there exists strong correlations between questions and answers, which reﬂects the “language prior” [20, 3]. Simply answering “tennis” to the sport-related questions can achieve approximately 40% accuracy on the VQA v1.0 dataset. Second, the questioner tends to ask about the objects seen in the image, which leads to the “visual priming bias” [8, 20, 27]. Simply answering “yes” to all the questions “Do you see a ...” achieves nearly 90% accuracy on the VQA v1.0 dataset. In both ways, machines may merely focus on the question rather than the visual content. This serious shortcut limits the generalization of VQA models [2, 56, 20], especially when the test scenario is quite different from the training scenario. Debiasing Strategies in VQA. Recently, a new VQA split, Visual Question Answering under Changing Priors (VQACP) [3], was proposed to evaluate the generalizability of VQA models. In VQA-CP, the distributions of answers for every question type are different during training and

2

test stages. Most of recent solutions to reduce the language bias in VQA can be grouped into three categories, strengthening visual grounding [40, 47], weakening language prior [36, 11, 14], and implicit/explicit data argumentation [12, 1, 43, 58]. First, human visual [15] and textual [25] explanations are exploited to strengthen the visual grounding in VQA [40, 47]. Second, ensemble-based methods proposed to use a separated QA branch to capture the language prior under adversarial learning [36] or multi-task learning [11, 14]. Third, recent works [12, 1] automatically generate additional question-image pairs to balance the distribution of training data. In this paper, the language-prior based methods [11, 14] can be uniﬁed into our proposed counterfactual inference framework as special cases. Causality-inspired Computer Vision. Counterfactual thinking and causal inference have inspired several studies in computer vision, including visual explanations [21, 7, 45, 51], scene graph generation [13, 41], image recognition [41], video analysis [17, 28], zero-shot and few-shot learning [53, 52] incremental learning [24], representation learning [46, 57], semantic segmentation [55], and visionlanguage tasks [12, 42, 35, 48, 18, 49]. Especailly, counterfactual learning has been exploited in recent VQA studies [12, 42, 1]. Different from these works that generate counterfactual samples for debiased training, our causeeffect look focuses on counterfactual inference with even biased training data.

3. Preliminaries
In this section, we introduce the used concepts of causal inference [32, 33, 38, 34]. In the following, we represent a random variable as a capital letter (e.g., X), and denote its observed value as a lowercase letter (e.g., x). Causal graph reﬂects the causal relationships between variables, which is represented as a directed acyclic graph G = {V, E}, where V denotes the set of variables and E represents the cause-and-effect relationships. Figure 2(a) shows an example of causal graph that consists of three variables. If the variable X has a direct effect on the variable Y , we say that Y is the child of X, i.e., X → Y . If X has an indirect effect on Y via the variable M , we say that M acts as a mediator between X and Y , i.e., X → M → Y . Counterfactual notations are used to translate causal assumptions from graphs to formulas. The value that Y would obtain if X is set to x and M is set to m is denoted as:

Yx,m = Y (X = x, M = m).1

(1)

In the factual scenario, we have m = Mx = M (X = x). In the counterfactual scenario, X is set as different values for M and Y . For example, Yx,Mx∗ describes the situation
1If there is no confounder of X, then we have that do(X = x) is equivalent to X = x and can omit the do operator.

CauCseause EffeEctffect MedMiaetodriator

(a) Causal Graph

(b) Counterfactual Notations

Figure 2: (a) Example of causal graph. (b) Examples of
counterfactual notations. White nodes are at the value X = x while gray nodes are at the value X = x∗.

where X is set to x and M is set to the value when X had been x∗, i.e., Yx,Mx∗ = Y (X = x, M = M (X = x∗)). Note that X can be simultaneously set to different values x and x∗ only in the counterfactual world. Figure 2(b) illustrates
examples of counterfactual notations.
Causal effects reﬂect the comparisons between two poten-
tial outcomes of the same individual given two different
treatments [39, 37]. Supposed that X = x represents “under treatment condition” and X = x∗ represents “under no-treatment condition”2. The total effect (TE) of treatment
X = x on Y compares two hypothetical situations X = x and X = x∗, which is denoted as:

TE = Yx,Mx − Yx∗,Mx∗ .

(2)

Total effect can be decomposed into natural direct effect
(NDE) and total indirect effect (TIE). NDE denotes the effect of X on Y with the mediator M blocked, and expresses the increase in Y with X changing from x∗ to x, while M is set to the value it would have obtained at X = x∗, i.e., the response of M to the treatment X = x is disabled:

NDE = Yx,Mx∗ − Yx∗,Mx∗ .

(3)

TIE is the difference between TE and NDE, denoted as:

TIE = TE − NDE = Yx,Mx − Yx,Mx∗ .

(4)

TE can be also decomposed into natural indirect effect (NIE) and total direct effect (TDE). Similarly, NIE reﬂects the effect of X on Y through the mediator M , i.e., X → M → Y , while the direct effect on X → Y is blocked by setting X as x∗. NIE is denoted as:

NIE = Yx∗,Mx − Yx∗,Mx∗ .

(5)

In Section 4, we will further discuss the meanings and differences of these effects in VQA.
2For example, to estimate the effect of a drug on a disease, X = x represents taking the drug, while X = x∗ represents not taking the drug.

3

4. Cause-Effect Look at VQA
Following the common formulation, we deﬁne the VQA task as a multi-class classiﬁcation problem. VQA models are required to select an answer from the candidate set A = {a} given an image V = v and a question Q = q.
4.1. Cause-Effect Look
The causal graph of VQA is illustrated in Figure 3(a). The effect of V and Q on A can be divided into the singlemodal impact and multi-modal impact. The single-modal impact captures the direct effect of V or Q on A via V → A or Q → A. The multi-modal impact captures the indirect effect of V and Q on A via the multi-modal knowledge K, i.e., V, Q → K → A. We propose to exclude pure language effect on Q → A to reduce language bias in VQA.
Following the counterfactual notations in Eq. (1), we denote the score that the answer a (e.g., “green”) would obtain if V is set to v (e.g., an image which includes green bananas) and Q is set to q (e.g., a question “What color are the bananas?”) as

Yv,q(a) = Y (a; V = v, Q = q).

Without loss of generality, we omit a for simplicity, i.e., Yv,q = Y (V = v, Q = q). Similarly, the counterfactual notation of K is denoted as Kv,q = K(V = v, Q = q).
As shown in Figure 3(a), there exist three paths directly connected to A, i.e., Q → A, V → A, and K → A. Therefore, we rewrite Yv,q as the function of Q, V and K:

Yv,q = Zq,v,k = Z(Q = q, V = v, K = k), (6)

where k = Kv,q. Following the deﬁnition of causal effects in Section 3, the total effect (TE) of V = v and Q = q on A = a can be written as:

TE = Yv,q − Yv∗,q∗ = Zq,v,k − Zq∗,v∗,k∗ ,

(7)

where k∗ = Kv∗,q∗ . Here v∗ and q∗ represent the notreatment condition where v and q are not given.
As we have discussed in Section 1, VQA models may suffer from the spurious correlation between questions and answers, and thus fail to conduct effective multi-modal reasoning. Therefore, we expect VQA models to exclude the direct impact of questions. To achieve this goal, we proposed counterfactual VQA to estimate the causal effect of Q = q on A = a by blocking the effect of K and V . Counterfactual VQA describes the scenario where Q is set to q and K would attain the value k∗ when Q had been q∗ and V had been v∗. Since the response of mediator K to inputs is blocked, the model can only rely on the given question for decision making. Figure 3(b) shows the comparison between conventional VQA and counterfactual VQA. We obtain the natural direct effect (NDE) of Q on A by comparing

(a)

(b)

Figure 3: (a) Causal graph for VQA. Q: question. V : image. K: multi-modal knowledge. A: answer. (b) Comparison between conventional VQA (left) and counterfactual VQA (right). White nodes are at the value V = v and Q = q while gray nodes are at the value V = v∗ and Q = q∗.

counterfactual VQA to the no-treatment conditions:

NDE = Zq,v∗,k∗ − Zq∗,v∗,k∗ .

(8)

Since the effect of Q on the intermediate K is blocked (i.e., K = k∗), NDE explicitly captures the language bias. Furthermore, the reduction of language bias can be realized by subtracting NDE from TE, which is represented as:

TIE = TE − NDE = Zq,v,k − Zq,v∗,k∗ .

(9)

We select the answer with the maximum TIE for inference, which is totally different from traditional strategies that is based on the posterior probability i.e., P (a|v, q).
4.2. Implementation
Parameterization. The calculation of the score Zq,v,k in Eq. (6) is parameterized by three neural models FQ, FV , FV Q and one fusion function h as:

Zq = FQ(q), Zv = FV (v), Zk = FV Q(v, q), (10)
Zq,v,k = h(Zq, Zv, Zk),

where FQ is the language-only branch (i.e., Q → A), FV is the vision-only branch (i.e., V → A), and FV Q is the visionlanguage branch (i.e., V, Q → K → A). The output scores are fused by the function h to obtain the ﬁnal score Zq,v,k.
As introduced in Section 4.1, the no-treatment condition is deﬁned as blocking the signal from vision or language, i.e., v or q is not given. We represent the no-treatment conditions as V = v∗ = ∅ and Q = q∗ = ∅. Note that neural models cannot deal with no-treatment conditions where the inputs are void. Therefore, we assume that the model will randomly guess with equal probability under the notreatment conditions. In this case, Zq, Zv and Zk in Eq. (10) can be represented as:

zq = FQ(q) if Q = q

Zq = z∗ = c

, if Q = ∅

(11)

q

4

loss

loss

loss

loss

loss

loss

VQA model

VQA model

VQA model

QA model

VQA model

VA model

VQA model

QA model

VA model

VQA model

QA model

VA model

VQA model

QA model

(train)

(test)

(a) traditional VQA

(train)

(test)

(b) language-prior based VQA

(train)

(test) (c) CF-VQA

Figure 4: Comparison between our CF-VQA and other VQA methods. (a) Traditional VQA methods use a single VQA model. (b) Language-prior based methods [11, 14] use an additional QA model to capture the language prior during the training stage. The QA model is not used during testing. (c) CF-VQA maintains the QA model during the test stage, and makes inference based on the debiased causal effect. The VA model is optional depending on the causal graph assumption.

zv = FV (v) if V = v

Zv = z∗ = c

,

(12)

if V = ∅

v

zk = FV Q(v, q) if V = v and Q = q

Zk = z∗ = c
k

, (13) if V = ∅ or Q = ∅

where c denotes a learnable parameter. We use the uniform distribution assumption for two reasons. Firstly, as for us human, we would like to make a wild guess if we have absolutely no idea about the speciﬁc treatments, including question types or topics. Secondly, as zv∗ and zk∗ are used to estimated NDE of Q, the uniform distribution can guarantee a safe estimation. We further empirically validate the assumption in the ablation study. Fusion Strategies. We expect that the fused score Zq,v,k is a combination of Zq, Zv and Zk. We proposed two nonlinear fusion variants, Harmonic (HM) and SUM:

(HM) h(Zq, Zv, Zk) = log ZHM , (14) 1 + ZHM
where ZHM = σ(Zq) · σ(Zv) · σ(Zk).

(SUM)

h(Zq, Zv, Zk) = log σ(ZSUM),

(15)

where ZSUM = Zq + Zv + Zk. Training. The training strategy follows [11]. As illustrated in Figure 4(c), given a triplet (v, q, a) where a is the groundtruth answer of image-question pair (v, q), the branches are optimized by minimizing the cross-entropy losses over the scores Zq,v,k, Zq and Zv:

Lcls = LVQA(v, q, a) + LQA(q, a) + LVA(v, a), (16)

where LVQA, LQA and LVA are over Zq,v,k, Zq and Zv. Note that we introduce a learnable parameter c in Eq. (11)(13), which controls the sharpness of the distribution of Zq,v∗,k∗ like the softmax temperature [23]. We hypothesize that the sharpness of NDE should be similar to that of

(a)

(b)

Figure 5: (a) Simpliﬁed VQA causal graph. (b) Comparison between conventional VQA and counterfactual VQA.

TE. Otherwise, an improper c would lead to the result that TIE in Eq. (4) is dominated by TE or NDE. Thus, we use Kullback-Leibler divergence to estimate c:

1 Lkl =

−p(a|q, v, k) log p(a|q, v∗, k∗), (17)

|A| a∈A

where p(a|q, v, k) = softmax(Zq,v,k) and p(a|q, v∗, k∗) = softmax(Zq,v∗,k∗ ). Only c is updated when minimizing Lkl. The ﬁnal loss is the combination of Lcls and Lkl:

L=

Lcls + Lkl

(v,q,a)∈D

(18)

Inference. As discussed in Section 4.1, we use the debiased causal effect for inference, which is implemented as:

TIE = TE − NDE = Zq,v,k − Zq,v∗,k∗ = h(zq, zv, zk) − h(zq, zv∗, zk∗). (19)

4.3. Revisiting RUBi and Learned-Mixin
Note that recent language-prior based methods RUBi [11] and Learned-Mixin [14] use an ensemble model that consists a vision-language branch FV Q and a question-only branch FQ. Besides, they simply exclude

5

Table 1: Comparison on VQA-CP v2 test set and VQA v2 val set. Best and second best results obtained without extra generated training samples are highlighted in each column. “Base.” indicates the VQA base model. We report the average accuracy of CF-VQA over 5 experiments with different random seeds.

Test set

VQA-CP v2 test

Methods

Base.

All

Y/N

Num.

GVQA [3]

–

31.30

57.99

SAN [50]

–

24.96

38.35

UpDn [5]

–

39.74

42.27

S-MRL [11]

–

38.46

42.85

methods based on modifying language module follow:

DLR [26]

UpDn

48.87

70.99

VGQE [30]

UpDn

48.75

–

VGQE [30]

S-MRL

50.11

66.35

methods based on strengthening visual attention follow:

AttAlign [40]

UpDn

39.37

43.02

HINT [40]

UpDn

46.73

67.27

SCR [47]

UpDn

49.45

72.36

methods based on weakening language prior follow:

AdvReg. [36]

UpDn

41.17

65.49

RUBi [11]

UpDn

44.23

67.05

RUBi [11]

S-MRL

47.11

68.65

LM [14]

UpDn

48.78

72.78

LM+H [14]

UpDn

52.01

72.58

CF-VQA (HM) CF-VQA (HM) CF-VQA (SUM) CF-VQA (SUM)

UpDn S-MRL UpDn S-MRL

49.74±0.26 51.27±0.25 53.55±0.10 55.05±0.12

74.81±0.51 77.80±0.83 91.15±0.06 90.61±0.28

13.68 11.14 11.93 12.81
18.72 –
27.08
11.89 10.61 10.93
15.48 17.48 20.28 14.61 31.12 18.46±0.39 20.64±1.01 13.03±0.21 21.50±0.86

methods based on balancing training data follow:

CVL [1]

UpDn

42.12

Unshufﬂing [43]

UpDn

42.39

RandImg [44]

UpDn

55.37

SSL [58]

UpDn

57.59

CSS [12]

UpDn

58.95

CSS+CL [31]

UpDn

59.18

Mutant [19]

UpDn

61.72

45.72 47.72 83.89 86.53 84.37 86.99 88.90

12.45 14.43 41.60 29.87 49.42 49.89 49.68

Other
22.14 21.74 46.05 43.20
45.57 –
46.77
45.00 45.88 48.02
35.48 39.61 43.18 45.58 46.97 45.19±0.27 45.76±0.21 44.97±0.20 45.61±0.16
48.34 47.24 44.20 50.03 48.21 47.16 50.78

All
48.24 52.41 63.48 63.10

VQA v2 val

Y/N

Num.

72.03 70.06 81.18
–

31.17 39.28 42.14
–

Other
34.65 47.84 55.66
–

57.96 64.04 63.18

76.82 – –

39.33 – –

48.54 – –

63.24 63.38 62.2

80.99 81.18 78.8

42.55 42.99 41.6

55.22 55.56 54.5

62.75 –
61.16 63.26 56.35
63.73±0.07 62.49±0.06 63.54±0.09 60.94±0.15

79.84 – –
81.16 65.06
82.15±0.08 81.19±0.09 82.51±0.12 81.13±0.15

42.35 – –
42.22 37.63
44.29±0.54 44.64±0.20 43.96±0.17 43.86±0.40

55.16 – –
55.22 54.69
54.86±0.07 52.98±0.14 54.30±0.09 50.11±0.16

– 61.08 57.24 63.73 59.91 57.29 62.56

– 78.32 76.53
– 73.25 67.27 82.07

– 42.16 33.87
– 39.77 38.40 42.52

– 52.81 48.57
– 55.11 54.71 53.28

FQ and use zk = FV Q(v, q) during the inference stage. The conceptual comparison between our CF-VQA and language-prior based strategies are illustrated in Figure 4. These methods can be uniﬁed into our counterfactual inference framework, which (1) follow a simpliﬁed causal graph (Fig. 5(a)) without the direct path V → A, and (2) use natural indirect effect (NIE) in Eq. (5) for inference. The detailed analysis is provided in the appendix.
5. Experiments
We mainly conduct the experiments on the VQA-CP [3] dataset. VQA-CP is proposed to evaluate the robustness of VQA models when the answer distributions of training and test splits are signiﬁcantly different. In addition, we also report the results on the balanced VQA v2 dataset to see whether the approach over-corrects language bias. The models are evaluated via accuracy. We conduct experiments with three baseline VQA architectures: Stacked Attention Network (SAN) [50], Bottom-up and Top-down Attention (UpDn) [5], and a simpliﬁed MUREL [10] (S-MRL) [11].

5.1. Quantitative Results
We ﬁrst compare CF-VQA with state-of-the-art methods. Recent approaches can be grouped as follows. (1) Methods that modify language modules proposed to decouple the linguistic concepts (DLR) [26] or generate visually-grounded question representations (VGQE) [30]. (2) Methods that strengthen visual attention exploit human visual [15] or textual [25] explanations, including AttAlign [40], HINT [40] and SCR [47]. (3) Methods that weaken language prior proposed to directly formulate the language prior by a separate question-only branch, including AdvReg. [36], RUBi [11] and Learned-Mixin (LM) [14]. (4) Methods that balance training data proposed to change the training distribution for unbiased training, including CVL [1], Unshufﬂing [43], RandImg [44], SSL [58], CSS [12], CL [31], and Mutant [19]. Unshufﬂing [43] partitions the training set into multiple invariant subsets. Other methods generate counterfactual training samples by masking or transforming critical words and objects [12, 19] or replacing the image [1, 44, 58].

6

Table 2: Ablation of CF-VQA on VQA-CP v2 test set. “SAN/UpDn/S-MRL” denotes the baseline VQA model. “HM/SUM”
represents the strategies that train the ensemble model and test with only the vision-language branch following ensemblebased method [11, 14]. ∗ represents the reproduced results.

SAN∗ HM + CF-VQA SUM + CF-VQA

All 33.18 45.89 48.10 43.98 50.15

Y/N 38.57 70.37 77.68 68.98 87.95

Num. 12.25 23.99 22.19 17.32 16.46

Other 36.10 39.07 39.71 38.19 39.59

UpDn∗ HM + CF-VQA SUM + CF-VQA

All 37.69 47.97 49.74 47.29 53.55

Y/N 43.17 69.19 74.81 72.26 91.15

Num. 12.53 18.80 18.46 12.54 13.03

Other 41.72 44.86 45.19 43.74 44.97

S-MRL∗ HM + CF-VQA SUM + CF-VQA

All 37.09 49.37 51.27 48.27 55.05

Y/N 41.39 73.20 77.80 74.60 90.61

Num. 12.46 20.10 20.64 20.96 21.50

Other 41.60 44.92 45.76 41.96 45.61

Table 3: Ablation of CF-VQA with the simpliﬁed causal graph on VQA-CP v2 test set. “SAN/UpDn/S-MRL” denotes
the baseline VQA model. “HM/SUM” represents the strategies that train the ensemble model and test with only the visionlanguage branch following ensemble-based method [11, 14]. ∗ represents the reproduced results.

SAN∗ HM + CF-VQA SUM + CF-VQA

All 33.18 45.48 49.43 42.35 49.85

Y/N 38.57 71.32 83.82 62.33 87.75

Num. 12.25 17.19 17.52 16.64 16.15

Other 36.10 39.71 40.16 38.94 39.24

UpDn∗ HM + CF-VQA SUM + CF-VQA

All 37.69 46.50 49.53 47.10 53.55

Y/N 43.17 67.54 77.02 70.00 91.15

Num. 12.53 12.83 12.86 12.80 12.81

Other 41.72 44.72 45.18 44.51 45.02

S-MRL∗ HM + CF-VQA SUM + CF-VQA

All 37.09 49.57 52.68 49.42 54.52

Y/N 41.39 72.31 82.05 74.43 90.69

Num. 12.46 20.28 20.76 20.52 21.84

Other 41.60 45.68 46.04 44.24 44.53

Table 4: Ablation of assumptions for counterfactual outputs on VQA-CP v2 test set.

S-MRL [11] HM
SUM

random prior uniform
random prior uniform

All
38.46 31.27 46.29 51.27
27.52 38.06 55.05

Y/N
42.85 29.69 61.88 77.80
28.00 41.43 90.61

Num.
12.81 42.87 20.03 20.64
37.88 14.90 21.50

Other
43.20 28.91 45.33 45.76
24.42 42.64 45.61

The results on VQA-CP v2 and VQA v2 are reported in Table 1. Most of the methods that explicitly generate training samples [44, 58, 12, 31, 19] outperform others by large margins. However, these methods explicitly change the training priors, which violates the original intention of VQA-CP, i.e., to evaluate whether VQA models are driven by memorizing priors in training data [3]. Therefore, we do not directly compare CF-VQA with these methods for fairness. Overall, compared to non-augmentation approaches, our proposed CF-VQA achieves a new state-of-the-art performance on VQA-CP v2. With a deep look at the question type, we ﬁnd that the improvement on “Yes/No” questions is extremely large (from ∼70% to ∼90%), which indicates that language bias have different effects on different types of questions. Besides, methods with extra annotations or generated training samples effectively improves the accuracy on “Other” questions, while CF-VQA achieves comparable performance to other methods. It is worth noting that LM [14] achieves a competitive performance on VQA-CP v2 with an additional language entropy penalty (LM+H). However, the accuracy drops signiﬁcantly by ∼7% on VQA v2, which indicates that the entropy penalty forces the model to over-correct language

Table 5: The comparison between CF-VQA and RUBi.

S-MRL [11] RUBi [11] + CF-VQA

VQA-CP v2 test

All
38.46 47.11±0.51 54.69±0.98

Y/N 42.85 68.65 89.90

Num. 12.81 20.28 32.39

Other 43.20 43.18 42.01

VQA v2 val
All 63.10 61.16 60.53

bias, especially on “Yes/No” questions. As a comparison, CF-VQA is more robust on VQA v2.
We further conduct ablation studies to validate (1) the generalizability of CF-VQA to both baseline VQA architectures, fusion strategies and causal graphs, and (2) the distribution assumption under the no-treatment conditions. As shown in Table 2 and 3, CF-VQA outperforms ensemblebased strategies by over 2% for HM and over 5% for SUM in all cases. To empirically validate the distribution assumption, we proposed two candidate assumptions. “Random” denotes that ci for answer ai are learned without any constraint. “Prior” denotes that {ci} obey the prior distribution of the training set. As shown in Table 4, “random” and “prior” even perform worse than the baselines. As we discussed in Section 4.2, the possible reason is that the uniform distribution assumption guarantees a safe estimation of NDE, i.e., the biased language effect.
Based on our cause-effect look, CF-VQA can easily improve RUBi by replacing NIE with TIE and introducing only one learnable parameter. The details are provided in the appendix. As shown in Table 5, CF-VQA improves RUBi by over 7.5% (i.e., from 47.11 to 54.69) on VQA-CP v2, while the accuracy slightly drops on VQA v2. Compared to our proposed symmetric fusion strategies HM and SUM, the standard deviation of RUBi’s accuracy is larger, which indicates that our proposed symmetric fusion strategies are more stable and robust.

7

Q: Is this room large or small?
Base ssmall large 6.2
big 1.3 medium 0.1
full 0.1

RUBi

Ours

92.3 yes

37.0 ssmmall

77.3

no

25.1

large 6.9

old

12.6

neither 3.6

both

11.2

big 3.5

smmaalll

8.6

yes 1.4

Q: What is being poured?

Base

wine

22.6

glass

13.8

bottle 6.0

wine glass 5.1

alcohol 4.9

RUBi

wine glass

42.3 wine

glass

38.9 glass

wine

16.7

blender

bottle 1.0

cup

champagne 0.5

table

Ours 30.5
6.1 5.4 3.4 3.3

Q: How many birds are depicted?

Base

0

28.8

2

14.6

1

12.3

4

10.4

3

8.4

RUBi

0

51.3

2

24.5

1 10.8

4 6.9

3 5.2

Ours

1

10.1

5

8.3

4

7.5

8

7.2

0

7.1

Q: Is this fruit fresh or frozen?

Base

RUBi

Ours

both

26.3 orange

25.7 fresh

90.1

yes

16.9

red

15.7

yes 2.2

raw

12.1

none

14.3

no 1.9

cooked

11.7

white

13.7

both 1.2

freshh 8.8

yellow

8.7

fruits 1.0

Q: What type of flowers are theses?

Base

tulips

24.5

rroses

23.9

pink

10.1

rose

9.3

red 5.9

pink rose roses red lily

RUBi 48.3
23.7 7.7 7.4 2.6

rroses tulips lilies
rose tulip

Ours 18.6
12.2 11.8 10.6 7.0

Q: How many dolls are on the bed?

Base

0

48.5

2

40.6

1 7.6

3 1.6

4 1.0

RUBi

Ours

0

88.6

2

39.9

1 6.1

1

28.1

2 4.9

0

24.7

3 0.3

3 4.6

4 0.1

no 0.8

Figure 6: Qualitative comparison on VQA-CP v2 test split. Red bold answer denotes the ground-truth one.

is this

what kind

on the wrong context “is this”. For the right example at the

second row, although RUBi successfully locates the ﬂowers,

it wrongly focuses on visual attributes (i.e., “pink”) rather

than categories (i.e., “what type”). These results highlight

the importance of language context, which is not consid-

ered by language prior based approaches. The examples

at the third row show the failure cases on number-related

questions. It remains a open problem how to improve the

accuracy of number-related questions.

Train Test Base RUBi Ours

Train Test Base RUBi Ours

Figure 7: Answer distributions on VQA-CP v2.

5.2. Qualitative Results
The qualitative results are provided to validate whether CF-VQA can effectively reduce language bias and retain language context. As illustrated in Figure 7, CF-VQA can successfully overcome language bias on yes/no questions compared to RUBi, while the baseline model suffers from the memorized language prior on the training set. Besides, for “what kind” questions, RUBi prefers the meaningless answer “none” rather than speciﬁc ones. Although CFVQA cannot recover the answer distribution very well, it attempts to respond with more meaningful answers (e.g., wood, frisbee). Examples in Figure 6 further illustrate how CF-VQA preserves language context for inference. For the left top example, CF-VQA recognizes the correct context “large or small”, while RUBi tends to answer yes/no based

6. Conclusion
In this paper, we proposed a novel counterfactual inference framework CF-VQA to reduce language bias in VQA. The bias is formulated as the direct causal effect of questions on answers and estimated by counterfactual reasoning. The reduction of language bias is realized by subtract the direct linguistic effect from the total causal effect. Experimental results demonstrate the effectiveness and generalizability of CF-VQA. Furthermore, recent debiasing studies [11, 14] can be uniﬁed into our proposed counterfactual inference framework. Surprisingly, we can further improve RUBi [11] by simply changing a few lines of code and including only one more learnable parameter based on our cause-effect look. In the future, we will consider how to make balances between robustness and debiasing ability. Acknowledgements We would like to thank anonymous ACs and reviewers for their valuable discussion and insightful suggestions. This work was supported in part by NTU-Alibaba JRI, MOE AcRF Tier 2 grant, National Natural Science Foundation of China (61976220 and 61832017), and Beijing Outstanding Young Scientist Program (BJJWZYJH012019100020098).

8

This supplementary document is organized as follows:
• Section 7 introduces that RUBi [11] and LearnedMixin [14] can be uniﬁed into our counterfactual inference framework.
• Section 8 provides an analysis of estimating NDE using the learnable parameter.
• Section 9 describes the implementation details.
• Section 10 describes the supplementary quantitative and qualitative results.

7. Revisiting RUBi and Learned-Mixin
As mentioned in Section 4.3, RUBi [11] and LearnedMixin [14] can be uniﬁed into our counterfactual inference framework, which (1) follow a simpliﬁed causal graph without the direct path V → A, and (2) use natural indirect effect (NIE) for inference. The detailed analysis is provided as follows.
7.1. Cause-Effect Look
Recent works RUBi [11] and Learned-Mixin [14] apply an ensemble architecture with a vision-language branch FVQ and a question-only branch FQ, while the direct relation between vision and answer is not formulated. The architecture is shown in Figure 8 (a).
Note that total effect can be decomposed into natural direct effect (NDE) and total indirect effect (TIE). As introduced in the main paper, we remove language bias by subtracting the natural direct effect from the total effect. The TIE is calculated by:

TE = Zq,k − Zq∗,k∗ ,

NDE = Zq,k∗ − Zq∗,k∗ ,

(20)

TIE = TE − NDE = Zq,k − Zq,k∗ ,

which corresponds to Eq. (4) in the main paper. An alternative option to reduce language bias is to substract the total direct effect (TDE) of questions on answers from total effect, which is formulated as:

TDE = Zq,k − Zq∗,k, (21)
NIE = TE − TDE = Zq∗,k − Zq∗,k∗ .

Intuitively, both TIE and NIE reﬂect the increase of conﬁ-
dence for the answer given the visual knowledge, i.e., from k∗ to k. The difference between TIE and NIE is the exis-
tence of question q. The question q is block to calculate NIE (i.e., q∗), while q is given to calculate TIE. We use
TIE to reserve q as the language context. In addition, both
TDE and NDE reﬂect the increase of conﬁdence for the answer given the question, i.e., from q∗ to q. The difference

Algorithm 1 Improving RUBi [11] using CF-VQA

1: function RUBI(v, q, is Training; θ, c)

2: zq = FQ(q)

3: zk = FV Q(v, q)

4: if is Training then

5:

z = zk · σ(zq)

6:

updating θ according to Lcls

7:

updating c according to Lkl

8: else

9:

z = zk z = (zk − c) · σ(zq)

10: end if

11: return z

12: end function

between TDE and NDE is also the existence of question q. Note that we hope to exclude the effect directly caused by question. Therefore, the mediator knowledge should be blocked when estimating the pure language effect, which is captured by NDE.
7.2. Implementation
RUBi [11] and Learned-Mixin (LM) [14] use the following fusion strategies for ensemble-based training:

(RUBi)

h(Zq, Zk) = Zk · σ(Zq)

(22)

(LM) h(Zq, Zk) = log σ(Zk) + g(k) · log σ(Zq) (23)

where σ(·) represents the sigmoid function, and g(·) is a learned function Rd → R1 with the knowledge representation k ∈ Rd as input and a scalar weight as output. During
the test stage, they use Zk for inference.
Perhaps supering As for RUBi, NIE is calculated as:

NIE = zk · σ(c) − c · σ(c) ∝ zk

(24)

Zq∗ ,k

Zq∗ ,k∗

As for Learned-Mixin, NIE is calculated as:

NIE = (log σ(zk) + g(k) · log σ(c))
Zq∗ ,k
− (log σ(c) + g(k∗) · log σ(c)) ∝ zk (25)
Zq∗ ,k∗
where c, g(k) and g(k∗) are constants for the same sample. Therefore, we have NIE ∝ zk for both RUBi and Learned-Mixin, which is exactly the output score of the vision-language branch FV Q. Note that RUBi and LearnedMixin simply preserve the vision-language branch and uses zk for inference. From our cause-effect view, RUBi and Learned-Mixin use natural indirect effect for inference.

9

loss

loss

loss

loss

VQA model

QA model

VQA model

VQA model

QA model

VQA model

QA model

VQA model

QA model

(train)

(test)

(a) language-prior based VQA

(train)

(test)

(b) CF-VQA

Figure 8: Comparison between our CF-VQA and language-prior based methods [11, 14] based on the simpliﬁed causal graph.

7.3. Improving RUBi [11]
Thanks to our cause-effect look, RUBi [11] can be improved using CF-VQA, i.e., using TIE for inference. Speciﬁcally, TIE for RUBi is calculated as:

TIE = zk · σ(zq) − c · σ(zq)

(26)

Zq,k

Zq,k∗

where c denotes a learnable parameter. Table 5 in the main paper demonstrates that CF-VQA can outperform RUBi by 7% on VQA-CP v2. The red notes in Algorithm 1 show how RUBi is improved by changing several lines of code.

8. Analysis of Estimating NDE
In Section 4.2 in the main paper, we claimed that the learnable parameter c controls the sharpness of Zq,v∗,k∗ for estimating NDE. We give an intuitive analysis here.
For Harmonic (HM), we have:

(HM) Zq,v∗,k∗ = log σ(zq) · cHM , (27) 1 + σ(zq) · cHM
where cHM = (σ(c))2 ∈ (0, 1). We approximate the limits of Zq,v∗,k∗ and TIE = Zq,v,k − Zq,v∗,k∗ as:

(HM)

lim Zq,v∗,k∗ = −∞
cHM →0

lim TIE
cHM →0

= zq,v,k − C

∝ zq,v,k,

(28)

where we use a extremely negative number C to replace −∞ for valid estimation of TIE. In this case, NDE is estimated as the same constant for all the answers, and TIE is dominated by zq,v,k, which means that the language bias is

not reduced. For cHM → 1, we have

(HM)

lim Zq,v∗,k∗ = log σ(zq)

cHM →1

1 + σ(zq)

lim TIE
cHM →1

= log σ(zv) · σ(zk) · (1 + σ(zq)) . 1 + σ(zq) · σ(zv) · σ(zk)

(29)

For SUM, we have

(SUM)

Zq,v∗,k∗ = log σ(zq + 2c),

(30)

where c ∈ (−∞, +∞). We approximate the limits of Zq,v∗,k∗ and TIE = Zq,v,k − Zq,v∗,k∗ as:

lim Zq,v∗,k∗ = −∞
c→−∞

(SUM)

lim TIE = zq,v,k − C

(31)

c→−∞

∝ zq,v,k.

Similar to HM, TIE is dominated by zq,v,k. For c → +∞, we have:

lim Zq,v∗,k∗ = 0

c→+∞

(SUM)

(32)

lim TIE = zq,v,k.

c→+∞

Also, TIE is dominated by zq,v,k. In both cases, the language bias cannot be excluded. This analysis shows that a extremely large or small c will fail to estimate NDE and TIE, and it is necessary to control the sharpness of NDE by selecting a optimal c. In the main paper, we use a KLdivergence in Eq. (17) to force the sharpness of NDE similar to that of TE.

9. Implementation Details
We use the same implementation of RUBi [11] for fair comparison, including feature representation, baseline architectures, and optimization.

10

Table 6: Comparison on VQA-CP v2 val set. “Base.” indicates the VQA base model.

GRLS [22] GradSup[42] RandImg [44] CF-VQA (HM) CF-VQA (SUM) CF-VQA (HM) CF-VQA (SUM)

Base. – –
UpDn UpDn UpDn S-MRL S-MRL

All 56.90 62.4 54.24 65.47 60.29 63.08 57.86

VQA-CP v2

val (in-domain) Y/N Num. 69.23 42.50 77.8 43.8 64.22 34.40 79.09 45.86 66.32 47.48 75.76 44.88 66.24 44.98

Other 49.36 53.6 50.46 57.86 57.96 55.99 53.38

test (OOD) All 42.33 46.8 55.37 49.74 51.27 53.55 55.05

VQA v2
val (in-domain) All 51.92 – 57.24 63.73 62.49 63.54 60.94

Image Representation. Following the popular bottom-up attention mechanism [5], we use a Faster R-CNN based framework to extract visual features. We select top-K region proposals for each image, where K is ﬁxed as 36.
Question Representation. Following [10, 11], we ﬁrst lowercase all the questions and remove the punctuation, and then use the pretrained Skip-thought encoder [29] with ﬁnetuning. The size of ﬁnal embedding is set as 4800.
Vision-Language Branch. The vision-language branch consists of the image representation, question representation, and a visual knowledge encoder. The baseline models for encoding visual knowledge includes SAN [50], UpDn [5], and a simpliﬁed version of the recent architecture MUREL [10] (S-MUREL) proposed in [11]. In short, SMUREL consists of a BLOCK [9] bilinear fusion between image and question representations for each region, and a MLP classiﬁer composed of three fully connected layers with ReLU activations. The dimension are 2,048, 2,048, and 3,000. More details can be found in [11].
Language-Only Branch. The language-only branch consists of the question representation and a question-only classiﬁer. The question-only classiﬁer is implemented by a MLP with three fully connect layers with ReLU activations. Note that this MLP has the same structure with the classiﬁer for vision-language branch with different parameters.
Vision-Only Branch. The vision-only branch is composed of the question representation and a vision-only classiﬁer. The vision-only classiﬁer has the same structure as the language-only classiﬁer with different parameters.
Optimization. All the experiments are conducted with the Adam optimizer for 22 epochs. The learning rate linearly increases from 1.5 × 10−4 to 6 × 10−4 for the ﬁrst 7 epochs, and decays after 14 epochs by multiplying 0.25 every two epochs. The batch size is set as 256.
Datasets. The experiments are conducted on VQA-CP [3] and VQA [20] datasets. VQA-CP v1 and v2 are created by re-organizing the train and val splits of the VQA v1 and v2 datasets, respectively [3].

10. Supplementary Experimental Results
We have conducted the ablation study and compared CFVQA with state-of-the-art methods in the main paper. In this section, we show supplementary experimental results.
10.1. Quantitative Results
As suggested by [44, 22, 42, 43], we further hold out 8,000 instances from the training set (i.e., VQA-CP v2 val) to measure the in-domain performance. Note that the results on VQA v2 val set also measure the in-domain performance. The results are given in Table 6. Compared to GRLS [22], all of our variants outperform GRLS by large margins for both in-domain and out-of-distribution (OOD) settings. Compared tp GradSup [42], CF-VQA (HM) achieves better results on both VQA-CP val set and test set. Compared to RandImg [44], CF-VQA (SUM) achieves competitive results on VQA-CP v2 test set, and outperforms RandImge on in-domain settings by over 3%. These results demonstrate that CF-VQA not only effectively reduces language bias, but also performs robustly.
Table 7 shows the ablation study on VQA-CP v1 test split. As shown in Table 7, CF-VQA is general to both baseline VQA architectures and fusion strategies, which is also demonstrated by the results on VQA-CP v2. Table 8 shows the ablation study on VQA-CP v1 test split using the simpliﬁed causal graph. Similarly, CF-VQA achieves signiﬁcant improvement for different baseline VQA architectures and fusion strategies.
10.2. Qualitative Results
Figure 9 illustrates examples to show how CF-VQA improves RUBi by simply replacing natural indirect effect with total indirect effect for inference following Algorithm 1. The examples show that CF-VQA beneﬁts from language context, e.g., “large or small”, “deep or shallow”, and “real or a statue” in the ﬁrst row. Some failure cases are shown in the last two rows. First, CF-VQA may tend to generate broad answers, e.g., “houses” v.s “church”, and “vegetables” v.s “peas”. Second, CF-VQA may ignore vi-

11

Table 7: Ablation of CF-VQA on VQA-CP v1 test set. “SAN/UpDn/S-MRL” denotes the baseline VQA model. “HM/SUM”
represents the strategies that train the ensemble model and test with only the vision-language branch following ensemblebased method [11, 14]. ∗ represents the reproduced results.

SAN∗ Harmonic + CF-VQA SUM + CF-VQA

All 32.50 49.29 52.06 38.34 52.87

Y/N 36.86 72.73 80.38 49.88 84.94

Num. 12.47 20.57 16.88 15.82 14.85

Other 36.22 37.51 38.04 35.91 36.26

UpDn∗ Harmonic + CF-VQA SUM + CF-VQA

All 37.08 55.75 55.16 52.78 57.39

Y/N 42.46 80.65 82.27 78.71 88.46

Num. 12.76 24.72 16.14 14.30 14.80

Other 41.50 43.46 43.87 42.45 43.61

S-MRL∗ Harmonic + CF-VQA SUM + CF-VQA

All 36.68 53.55 55.26 49.44 57.03

Y/N 42.72 79.38 82.13 76.49 89.02

Num. 12.59 17.39 18.03 16.23 17.08

Other 40.35 42.38 43.49 35.90 41.27

Table 8: Ablation of CF-VQA with the simpliﬁed causal graph on VQA-CP v1 test set. “SAN/UpDn/S-MRL” denotes
the baseline VQA model. “HM/SUM” represents the strategies that train the ensemble model and test with only the visionlanguage branch following ensemble-based method [11, 14]. ∗ represents the reproduced results.

SAN∗ Harmonic + CF-VQA SUM + CF-VQA

All 32.50 46.83 54.48 40.08 52.73

Y/N 36.86 66.64 83.73 54.15 84.64

Num. 12.47 19.45 22.73 15.53 16.02

Other 36.22 38.13 38.15 35.95 35.75

UpDn∗ Harmonic + CF-VQA SUM + CF-VQA

All 37.08 54.13 56.19 51.20 56.80

Y/N 42.46 80.60 85.08 74.70 87.76

Num. 12.76 15.75 16.00 13.61 13.89

Other 41.50 43.24 43.61 42.94 43.25

S-MRL∗ Harmonic + CF-VQA SUM + CF-VQA

All 36.68 54.51 56.82 52.54 57.07

Y/N 42.72 80.82 86.01 78.42 89.28

Num. 12.59 17.30 17.38 16.77 17.39

Other 40.35 43.29 43.63 41.18 41.00

sual content like traditional likelihood strategy. Therefore, there remains the challenge about how to balance visual understanding and language context.
References
[1] Ehsan Abbasnejad, Damien Teney, Amin Parvaneh, Javen Shi, and Anton van den Hengel. Counterfactual vision and language learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10044–10054, 2020. 1, 3, 6
[2] Aishwarya Agrawal, Dhruv Batra, and Devi Parikh. Analyzing the behavior of visual question answering models. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1955–1960, 2016. 1, 2
[3] Aishwarya Agrawal, Dhruv Batra, Devi Parikh, and Aniruddha Kembhavi. Don’t just assume; look and answer: Overcoming priors for visual question answering. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4971–4980, 2018. 1, 2, 6, 7, 11
[4] Aishwarya Agrawal, Jiasen Lu, Stanislaw Antol, Margaret Mitchell, C Lawrence Zitnick, Devi Parikh, and Dhruv Batra. Vqa: Visual question answering. International Journal of Computer Vision, 123(1):4–31, 2017. 1
[5] Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould, and Lei Zhang. Bottom-up and top-down attention for image captioning and visual question answering. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 6077–6086, 2018. 6, 11
[6] Peter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark Johnson, Niko Su¨nderhauf, Ian Reid, Stephen Gould, and Anton van den Hengel. Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments. In Proceedings of the IEEE Conference

on Computer Vision and Pattern Recognition, pages 3674– 3683, 2018. 1 [7] Lisa Anne Hendricks, Ronghang Hu, Trevor Darrell, and Zeynep Akata. Grounding visual explanations. In Proceedings of the European Conference on Computer Vision (ECCV), pages 264–279, 2018. 3 [8] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence Zitnick, and Devi Parikh. Vqa: Visual question answering. In Proceedings of the IEEE international conference on computer vision, pages 2425– 2433, 2015. 1, 2 [9] Hedi Ben-Younes, Remi Cadene, Nicolas Thome, and Matthieu Cord. Block: Bilinear superdiagonal fusion for visual question answering and visual relationship detection. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 33, pages 8102–8109, 2019. 11 [10] Remi Cadene, Hedi Ben-Younes, Matthieu Cord, and Nicolas Thome. Murel: Multimodal relational reasoning for visual question answering. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1989–1998, 2019. 6, 11 [11] Remi Cadene, Corentin Dancette, Matthieu Cord, Devi Parikh, et al. Rubi: Reducing unimodal biases for visual question answering. Advances in Neural Information Processing Systems, 32:841–852, 2019. 1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12 [12] Long Chen, Xin Yan, Jun Xiao, Hanwang Zhang, Shiliang Pu, and Yueting Zhuang. Counterfactual samples synthesizing for robust visual question answering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10800–10809, 2020. 1, 3, 6, 7 [13] Long Chen, Hanwang Zhang, Jun Xiao, Xiangnan He, Shiliang Pu, and Shih-Fu Chang. Counterfactual critic multiagent training for scene graph generation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 4613–4623, 2019. 3

12

What color are the shoes?

RUBi + CF-VQA

white

27.0%

pink

17.1%

red and white 10.4%

white and red 7.8%

pink and white 7.4%

RUBi

pink

56.7%

white

31.8%

pink and white 5.4%

red

2.4%

wrhite and red 1.6%

Is this area large or small?

RUBi + CF-VQA

small

94.6%

large

3.5%

big

1.5%

medium 0.3%

huge

0.1%

old yes no both small

RUBi 36.3% 32.8% 18.7% 5.8% 3.0%

Is this natural or artificial light?

RUBi + CF-VQA

natural 50.4%

yes

25.6%

both

7.7%

curved

4.8%

main

3.9%

RUBi

no

72.6%

yes

18.5%

none

3.2%

old

1.3%

unknown 1.3%

Is this fruit fresh or frozen?

RUBi + CF-VQA

fresh

63.9%

frozen

8.3%

half

3.8%

salt

2.2%

wheat

1.6%

orange red none white yellow

RUBi

25.7% 15.7% 14.3% 13.7% 8.7%

What brand is the man's shirt?

RUBi + CF-VQA

nike

60.1%

adidas

12.9%

billabong 10.4%

hurley

8.8%

polo

3.3%

RUBi

billabong 42.8%

none

19.1%

blue

12.1%

white

6.6%

hurley

4.1%

What brand is the racket?

RUBi + CF-VQA

wilson

50.2%

nike

24.1%

prince

9.1%

head

8.5%

adidas

5.6%

adidas wilson nike w white

RUBi

83.0% 6.5% 4.8% 2.8% 0.8%

What type of herb is on the left?

RUBi + CF-VQA

parsley

52.1%

ginger

9.2%

lily

6.0%

pepper

5.7%

maple

2.8%

orange red none white yellow

RUBi

73.7% 18.6% 2.7% 1.7% 0.9%

What type of sneakers are the players playing in?

RUBi + CF-VQA

cleats

40.7%

baseball 10.9%

tennis shoes 8.6%

converse 5.9%

giants

5.7%

RUBi

baseball

92.5%

cleats

6.1%

baseball cap 0.6%

don’t know 0.1%

yes

0.1%

What type of flower is in the vase?

RUBi + CF-VQA

rose

31.0%

daisy

28.3%

carnation 16.0%

lily

6.2%

lilly

3.0%

pink rose red roses purple

RUBi 69.2% 21.8% 4.6% 2.1% 0.8%

What are these machines used for?

RUBi + CF-VQA

money

39.2%

transportation 28.2%

parking 13.4%

parking meter riding

3.6% 2.0%

RUBi

parking

90.0%

money

8.6%

picture

0.6%

parking

0.2%

meter

driving

0.1%

Why are they wearing wetsuits?

RUBi + CF-VQA

safety

67.9%

surf

7.7%

yes

5.1%

protection 4.0%

surfing

3.5%

RUBi

surfing

77.5%

surf

11.9%

yes

3.9%

sunny

2.1%

walking

1.0%

Is this water deep or shallow?

RUBi + CF-VQA

deep

97.0%

shallow 2.5%

yes

0.3%

ascending 0.1%

choppy

0.1%

RUBi

no

40.3%

yes

35.7%

unknown 12.1%

expert

2.3%

old

1.8%

Is this bus going or coming?

RUBi + CF-VQA

going

78.4%

stopped

8.6%

coming

6.1%

leaving

1.1%

city

0.9%

RUBi

police

36.3%

going

30.6%

forward

11.9%

coming

6.2%

no

4.3%

What brand is on the coffee cup?

RUBi + CF-VQA

starbucks 89.6%

dunkin donuts coca cola

5.1% 4.7%

coke

0.5%

jones

0.1%

RUBi

none

37.7%

unknown 29.6%

can’t tell not sure not possible

7.8% 4.2% 3.5%

What brand of phone is this?

RUBi + CF-VQA

iphone

25.9%

motorola 24.5%

samsung 19.0%

htc

10.2%

apple

3.3%

RUBi

nintendo 68.2%

wii

26.3%

none

4.3%

unknown 0.3%

iphone

0.3%

What brand is the box?

RUBi + CF-VQA

hp

32.2%

canon

21.5%

dell

15.1%

toshiba

13.4%

head

8.0%

RUBi

dell

64.2%

hp

26.5%

windows 6.9%

adidas

0.9%

toshiba

0.6%

What type of seeds are stuck to the outside of the bun?

RUBi + CF-VQA

RUBi

sesame

99.5% unknown 41.7%

pepper

0.1% none

30.1%

regular

0.1% yellow

8.6%

sunflower 0.1% sesame

6.6%

0

0.1% white

2.9%

What type of sink is seen in the picture?

RUBi + CF-VQA

pedestal 49.6%

bathroom 10.3%

porcelain 8.0%

ceramic 7.1%

white

4.1%

RUBi

bathroom 48.5%

pedestal 40.0%

white

5.6%

ceramic

5.2%

porcelain 1.0%

What type of building is pictured in the photo?

RUBi + CF-VQA

clock tower 28.8%

church

15.1%

apartment 9.5%

school

7.9%

tower

7.1%

RUBi

unknown 91.0%

none

3.7%

yellow

3.0%

sesame

0.7%

white

0.5%

What are these buildings?

RUBi + CF-VQA

houses

26.8%

skyscrapers 13.8%

office

6.7%

tower

6.4%

church

6.2%

RUBi

church

73.1%

city

8.4%

building

5.5%

london

1.8%

castle

1.6%

Why are the people gathered?

RUBi + CF-VQA

parade

82.3%

riding horses 5.8%

jousting 2.3%

race horseback riding

2.1% 1.7%

RUBi

polo

40.3%

rodeo

27.1%

horseback 12.2%

riding

riding horses 9.3%

jousting

7.2%

Is this horse real or a statue?

RUBi + CF-VQA

statue

90.8%

toy

5.9%

real

2.7%

model

0.2%

big

0.1%

horse no none neither yes

RUBi

84.8% 13.0% 1.0% 0.5% 0.4%

Is this airport in the city or country?

RUBi + CF-VQA

city

93.1%

country 6.2%

yes

0.5%

both

0.1%

United States 0.1%

RUBi

New York 25.3%

yes

22.7%

unknown 18.4%

not sure

7.9%

no

6.6%

What brand of truck is shown?

RUBi + CF-VQA

ford

58.5%

volvo

9.0%

toyota

8.8%

dodge

5.4%

chevy

4.0%

RUBi

no

29.2%

yes

27.7%

unknown 10.9%

expert

6.2%

old

5.0%

What brand of soda is on the nightstand?

RUBi + CF-VQA

coke

42.2%

coca cola 38.9%

coca-cola 8,9%

starbucks 1.6%

fanta

1.6%

RUBi

none

45.2%

nothing

34.5%

unknown 9.0%

not possible 3.3%

white

1.5%

What brand is shown?

RUBi + CF-VQA

harley

55.7%

yamaha

20.6%

honda

7.8%

toyota nike

6.8% 2.8%

RUBi

yamaha

34.8%

harley

33.9%

harley

18.2%

davidson

honda

4.6%

kawasaki 2.7%

What type of hairstyle is the girl wearing?

RUBi + CF-VQA

ponytail 19.5%

braid

18.7%

straight

11.4%

long

11.0%

bob

8.6%

RUBi

striped

26.2%

curly

14.1%

stripes

11.7%

blue

10.0%

sweater

5.2%

What type of RUBi + CF-VQA does

this man wear?

RUBi + CF-VQA

bow tie

74.7%

bow

15.1%

bowtie

8.0%

regular

0.8%

horizontal 0.6%

RUBi

striped

50.6%

curly

21.1%

stripes

7.8%

blue

3.9%

sweater

3.5%

Where on the cow's body is there a tag?

RUBi + CF-VQA

ear

48.8%

yes

17.3%

back

9.3%

head

5.0%

legs

3.2%

RUBi

yes

76.8%

no

6.5%

left

3.0%

unknown 2.9%

bowl

2.5%

What are those round green things?

RUBi + CF-VQA

vegetables 27.3%

peas

19.5%

grapes

10.2%

beets

6.6%

fruit

5.9%

RUBi

peas

97.3%

grapes

1.2%

vegetables 1.2%

fruit

0.1%

blueberries 0.1%

What kind of window covering is shown?

RUBi + CF-VQA

blinds

96.5%

shade

2.9%

sheet

0.3%

curtains 0.2%

cloth

0.1%

RUBi

curtain

16.8%

canopy

15.1%

fan

14.9%

curtains 7.9%

sheet

7.7%

Figure 9: Qualitative comparison of RUBi and RUBi+CF-VQA on VQA-CP v2 test split. Red bold answer denotes the ground-truth one.

Is this food hot or cold?

[14] Christopher Clark, Mark Yatskar, and Luke Zettlemoyer. Don’t take the easy way out: Ensemble based methods for avoiding known dataset biases. In Proceedings of the 2019

Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 4060–

13

4073, 2019. 1, 2, 3, 5, 6, 7, 8, 9, 10, 12
[15] Abhishek Das, Harsh Agrawal, Larry Zitnick, Devi Parikh, and Dhruv Batra. Human attention in visual question answering: Do humans and deep networks look at the same regions? Computer Vision and Image Understanding, 163:90– 100, 2017. 1, 3, 6
[16] Abhishek Das, Satwik Kottur, Khushi Gupta, Avi Singh, Deshraj Yadav, Jose´ MF Moura, Devi Parikh, and Dhruv Batra. Visual dialog. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 326–335, 2017. 1
[17] Zhiyuan Fang, Shu Kong, Charless Fowlkes, and Yezhou Yang. Modularized textual grounding for counterfactual resilience. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 6378–6388, 2019. 3
[18] Tsu-Jui Fu, Xin Wang, Scott Grafton, Miguel Eckstein, and William Yang Wang. Iterative language-based image editing via self-supervised counterfactual reasoning. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 4413–4422, 2020. 3
[19] Tejas Gokhale, Pratyay Banerjee, Chitta Baral, and Yezhou Yang. Mutant: A training paradigm for out-of-distribution generalization in visual question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 878–892, 2020. 1, 6, 7
[20] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the v in vqa matter: Elevating the role of image understanding in visual question answering. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 6904–6913, 2017. 1, 2, 11
[21] Yash Goyal, Ziyan Wu, Jan Ernst, Dhruv Batra, Devi Parikh, and Stefan Lee. Counterfactual visual explanations. In International Conference on Machine Learning, pages 2376– 2384. PMLR, 2019. 3
[22] Gabriel Grand and Yonatan Belinkov. Adversarial regularization for visual question answering: Strengths, shortcomings, and side effects. In Proceedings of the Second Workshop on Shortcomings in Vision and Language, pages 1–13, 2019. 11
[23] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015. 5
[24] Xinting Hu, Kaihua Tang, Chunyan Miao, Xian-Sheng Hua, and Hanwang Zhang. Distilling causal effect of data in classincremental learning. arXiv preprint arXiv:2103.01737, 2021. 3
[25] Dong Huk Park, Lisa Anne Hendricks, Zeynep Akata, Anna Rohrbach, Bernt Schiele, Trevor Darrell, and Marcus Rohrbach. Multimodal explanations: Justifying decisions and pointing to the evidence. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 8779–8788, 2018. 1, 3, 6
[26] Chenchen Jing, Yuwei Wu, Xiaoxun Zhang, Yunde Jia, and Qi Wu. Overcoming language priors in vqa via decomposed

linguistic representations. In AAAI, pages 11181–11188, 2020. 6
[27] Kushal Kaﬂe and Christopher Kanan. An analysis of visual question answering algorithms. In Proceedings of the IEEE International Conference on Computer Vision, pages 1965– 1973, 2017. 1, 2
[28] Atsushi Kanehira, Kentaro Takemoto, Sho Inayoshi, and Tatsuya Harada. Multimodal explanations by predicting counterfactuality in videos. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 8594–8602, 2019. 3
[29] Ryan Kiros, Yukun Zhu, Ruslan R Salakhutdinov, Richard Zemel, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. Skip-thought vectors. In Advances in neural information processing systems, pages 3294–3302, 2015. 11
[30] Gouthaman KV and Anurag Mittal. Reducing language biases in visual question answering with visually-grounded question encoder. arXiv preprint arXiv:2007.06198, 2020. 6
[31] Zujie Liang, Weitao Jiang, Haifeng Hu, and Jiaying Zhu. Learning to contrast the counterfactual samples for robust visual question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 3285–3292, 2020. 1, 6, 7
[32] Judea Pearl. Causality: models, reasoning and inference, volume 29. Springer, 2000. 1, 3
[33] Judea Pearl. Direct and indirect effects. In Proceedings of the seventeenth conference on uncertainty in artiﬁcial intelligence, pages 411–420. Morgan Kaufmann Publishers Inc., 2001. 1, 3
[34] Judea Pearl and Dana Mackenzie. The book of why: the new science of cause and effect. Basic Books, 2018. 1, 3
[35] Jiaxin Qi, Yulei Niu, Jianqiang Huang, and Hanwang Zhang. Two causal principles for improving visual dialog. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10860–10869, 2020. 3
[36] Sainandan Ramakrishnan, Aishwarya Agrawal, and Stefan Lee. Overcoming language priors in visual question answering with adversarial regularization. In Advances in Neural Information Processing Systems, pages 1541–1551, 2018. 3, 6
[37] James Robins. A new approach to causal inference in mortality studies with a sustained exposure period—application to control of the healthy worker survivor effect. Mathematical modelling, 7(9-12):1393–1512, 1986. 3
[38] James M Robins. Semantics of causal dag models and the identiﬁcation of direct and indirect effects. Oxford Statistical Science Series, pages 70–82, 2003. 3
[39] Donald B Rubin. Bayesian inference for causal effects: The role of randomization. The Annals of statistics, pages 34–58, 1978. 3
[40] Ramprasaath R Selvaraju, Stefan Lee, Yilin Shen, Hongxia Jin, Shalini Ghosh, Larry Heck, Dhruv Batra, and Devi Parikh. Taking a hint: Leveraging explanations to make vision and language models more grounded. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2591–2600, 2019. 1, 3, 6

14

[41] Kaihua Tang, Yulei Niu, Jianqiang Huang, Jiaxin Shi, and Hanwang Zhang. Unbiased scene graph generation from biased training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3716– 3725, 2020. 3
[42] Damien Teney, Ehsan Abbasnedjad, and Anton van den Hengel. Learning what makes a difference from counterfactual examples and gradient supervision. arXiv preprint arXiv:2004.09034, 2020. 3, 11
[43] Damien Teney, Ehsan Abbasnejad, and Anton van den Hengel. Unshufﬂing data for improved generalization. arXiv preprint arXiv:2002.11894, 2020. 3, 6, 11
[44] Damien Teney, Kushal Kaﬂe, Robik Shrestha, Ehsan Abbasnejad, Christopher Kanan, and Anton van den Hengel. On the value of out-of-distribution testing: An example of goodhart’s law. arXiv preprint arXiv:2005.09241, 2020. 6, 7, 11
[45] Pei Wang and Nuno Vasconcelos. Scout: Self-aware discriminant counterfactual explanations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8981–8990, 2020. 3
[46] Tan Wang, Jianqiang Huang, Hanwang Zhang, and Qianru Sun. Visual commonsense r-cnn. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10760–10770, 2020. 3
[47] Jialin Wu and Raymond J Mooney. Self-critical reasoning for robust visual question answering. arXiv preprint arXiv:1905.09998, 2019. 1, 3, 6
[48] Xu Yang, Hanwang Zhang, and Jianfei Cai. Deconfounded image captioning: A causal retrospect. arXiv preprint arXiv:2003.03923, 2020. 3
[49] Xu Yang, Hanwang Zhang, Guojun Qi, and Jianfei Cai. Causal attention for vision-language tasks. arXiv preprint arXiv:2103.03493, 2021. 3
[50] Zichao Yang, Xiaodong He, Jianfeng Gao, Li Deng, and Alex Smola. Stacked attention networks for image question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 21–29, 2016. 6, 11
[51] Kexin Yi, Chuang Gan, Yunzhu Li, Pushmeet Kohli, Jiajun Wu, Antonio Torralba, and Joshua B Tenenbaum. Clevrer: Collision events for video representation and reasoning. arXiv preprint arXiv:1910.01442, 2019. 3
[52] Zhongqi Yue, Tan Wang, Hanwang Zhang, Qianru Sun, and Xian-Sheng Hua. Counterfactual zero-shot and open-set visual recognition. arXiv preprint arXiv:2103.00887, 2021. 3
[53] Zhongqi Yue, Hanwang Zhang, Qianru Sun, and Xian-Sheng Hua. Interventional few-shot learning. arXiv preprint arXiv:2009.13000, 2020. 3
[54] Rowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin Choi. From recognition to cognition: Visual commonsense reasoning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 6720–6731, 2019. 1
[55] Dong Zhang, Hanwang Zhang, Jinhui Tang, Xiansheng Hua, and Qianru Sun. Causal intervention for weakly-supervised semantic segmentation. arXiv preprint arXiv:2009.12547, 2020. 3

[56] Peng Zhang, Yash Goyal, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Yin and yang: Balancing and answering binary visual questions. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5014–5022, 2016. 2
[57] Shengyu Zhang, Tan Jiang, Tan Wang, Kun Kuang, Zhou Zhao, Jianke Zhu, Jin Yu, Hongxia Yang, and Fei Wu. Devlbert: Learning deconfounded visio-linguistic representations. In Proceedings of the 28th ACM International Conference on Multimedia, pages 4373–4382, 2020. 3
[58] Xi Zhu, Zhendong Mao, Chunxiao Liu, Peng Zhang, Bin Wang, and Yongdong Zhang. Overcoming language priors with self-supervised learning for visual question answering. arXiv preprint arXiv:2012.11528, 2020. 1, 3, 6, 7

15

