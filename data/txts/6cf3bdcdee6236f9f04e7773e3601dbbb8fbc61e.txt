Simple Questions Generate Named Entity Recognition Datasets

Hyunjae Kim1

Jaehyo Yoo1 Seunghyun Yoon2 Jinhyuk Lee1 1Korea University 2Adobe Research

Jaewoo Kang1

{hyunjae-kim,jaehyoyoo,jinhyuk_lee,kangj}@korea.ac.kr

syoon@adobe.com

arXiv:2112.08808v1 [cs.CL] 16 Dec 2021

Abstract
Named entity recognition (NER) is a task of extracting named entities of speciﬁc types from text. Current NER models often rely on human-annotated datasets requiring the vast engagement of professional knowledge on the target domain and entities. This work introduces an ask-to-generate approach, which automatically generates NER datasets by asking simple natural language questions that reﬂect the needs for entity types (e.g., “Which disease?”) to an open-domain question answering system. Without using any in-domain resources (i.e., training sentences, labels, or in-domain dictionaries), our models solely trained on our generated datasets largely outperform previous weakly supervised models on six NER benchmarks across four different domains. Surprisingly, on NCBI-disease, our model achieves 75.5 F1 score and even outperforms the previous best weakly supervised model by 4.1 F1 score, which utilizes a rich indomain dictionary provided by domain experts. Formulating the needs of NER with natural language also allows us to build NER models for ﬁne-grained entity types such as Award, where our model even outperforms fully supervised models. On three few-shot NER benchmarks, our model achieves new state-of-the-art performance.1
1 Introduction
Every named entity recognition (NER) dataset reﬂects one’s needs to extract speciﬁc entity types. For instance, NCBI-disease (Dog˘an et al., 2014) was created to support the needs of extracting disease entities from text. Recent NER models show strong performance when trained on the carefully designed human-annotated datasets (Lample et al., 2016; Li et al., 2020; Lee et al., 2020). But, suppose we want to build an NER model for extracting microbe entities or other speciﬁc types that do
1Code available at https://github.com/dmis-lab/GeNER.

not have human-annotated datasets. Whenever we want to extract new entity types, should we rely on professional knowledge to create new datasets?
Previous weakly supervised NER models (Shang et al., 2018b; Liang et al., 2020) tackle this problem with rich in-domain dictionaries (e.g., The Comparative Toxicogenomics Database) and unlabeled training sentences (i.e., in-domain sentences), where entities in the dictionary are used to annotate the sentences. However, these approaches easily fail in practice since in-domain dictionaries and sentences are often unavailable or expensive to construct for many entity types. It will be challenging to build NER models for metabolite or ﬁghter aircraft without having expert-level knowledge for building dictionaries or searching in-domain sentences to annotate.
In this work, we introduce GeNER, an automated dataset Generation framework for NER, which automatically constructs high-quality NER datasets. In particular, one’s concrete needs for NER are described as simple natural language questions such as “Which [TYPE]?,” where [TYPE] is substituted for required entity types (e.g., “Which disease?”). Such questions do not require any professional knowledge on the target domain and allow non-experts to easily build domain-speciﬁc NER datasets. Using a phrase retrieval model designed for open-domain question answering (QA) (Lee et al., 2021a), GeNER retrieves candidate entities (i.e., phrases) as well as evidence sentences from a large-scale open-domain corpus (e.g., Wikipedia). The retrieved entities form a dictionary, which annotates the evidence sentences to create the dataset. We then train standard NER models on our generated dataset based on self-training (Liang et al., 2020). As shown in Table 1, this type of ask-togenerate approach completely removes the dependency on in-domain resources (i.e., zero-resource) while signiﬁcantly outperforming the existing lowresource model (Li et al., 2021).

Model

Resource

NCBI- CrossNER

Xtrain / Ytrain / V disease (Award)

Rich-resource setting (w/ training label or in-domain dict.)

Fully-supervised

//

88.6

75.4

BOND

/ /

71.4

-

Low- or zero-resource setting

TALLOR

/ /

44.3

-

GeNER (ours)

 / /

75.5

80.9

Table 1: Comparison of existing approaches in NER. Each method is categorized based on how much they rely on in-domain resources during training. Xtrain: (unlabeled) training sentences. Ytrain: human annotated training labels. V: in-domain dictionaries by domain experts. In-domain resources are either fully used (), partially used ( ), or not used (). GeNER is the ﬁrst zero-resource model and shows strong F1 score on several NER benchmarks such as NCBI-disease (Dog˘an et al., 2014) and CrossNER (Award) (Liu et al., 2021b).

We demonstrate the effectiveness of GeNER using six popular NER benchmarks across four different domains: news (Tjong Kim Sang, 2002), Wikipedia (Balasuriya et al., 2009), Twitter (Strauss et al., 2016), and biomedicine (Dog˘an et al., 2014; Li et al., 2016; Krallinger et al., 2015). Models solely trained on our generated datasets from GeNER signiﬁcantly outperform the strong low-resource model TALLOR (Li et al., 2021) on all benchmarks up to 41.8 F1 score. Surprisingly, our models even outperform the previous best weakly supervised model BOND (Liang et al., 2020) on Wikigold by 12.3 F1 score and NCBIdisease by 4.2 F1 score, although our model does not use any in-domain dictionaries or in-domain corpus (e.g., PubMed). We show GeNER is useful for building NER models for ﬁne-grained entity types such as enzyme and award from CrossNER (Liu et al., 2021b). On a few-shot NER setting, models initialized with GeNER achieve new state-of-the-art results on three benchmarks (Huang et al., 2020), largely outperforming previous fewshot models. We analyze several aspects of generated datasets that contribute to better NER performance and show how GeNER works on many different entity types.
Our contributions are summarized as follows:
• We introduce GeNER, which formulates the needs of NER as simple questions and generates NER datasets.
• GeNER largely outperforms existing low-

resource models on six NER benchmarks while often outperforming the previous best weakly supervised model that uses rich indomain resources.
• GeNER even outperforms fully-supervised models on several ﬁne-grained NER datasets and achieves new state-of-the-art on the fewshot NER benchmarks.
• We provide comprehensive analysis of GeNER and show that the quality of generated datasets is directly related to the ﬁnal NER performance.
2 Background
2.1 Named Entity Recognition
NER aims to identify named entities of pre-deﬁned types in text. Let D = {X, Y} be a dataset, where X = {xn}Nn=1 is the set of unlabeled sentences, Y = {yn}Nn=1 is the corresponding token-level labels for each sentence, and N is the size of the dataset. In supervised learning, D is split into Dtrain = {Xtrain, Ytrain} and Dtest = {Xtest, Ytest}, where Dtrain is used to train NER models and Dtest is used to evaluate the models.
Weakly supervised NER Instead of using humanannotated labels Ytrain, weakly supervised NER models rely on in-domain dictionaries V built by domain experts (Yang and Katiyar, 2020; Shang et al., 2018b; Cao et al., 2019; Liang et al., 2020). In-domain dictionaries are used to generate weak labels Yˆ train for (unlabeled) training sentences Xtrain by annotating any occurrences of named entities from the dictionary. Models are then trained on Dˆtrain = {Xtrain, Yˆ train} and evaluated on Dtest. In this work, we disregard all the in-domain resources including Xtrain, Ytrain, and V, and propose to automatically generate a new dataset D˜train = {X˜ train, Y˜ train} by asking simple questions to an open-domain QA model.
2.2 Open-domain Question Answering
Open-domain QA ﬁnds answers from a largescale corpus, which is not limited to speciﬁc domains (Chen et al., 2017). Among many other open-domain QA approaches, Seo et al. (2019) propose phrase retrieval, which formulates question answering as retrieving pre-indexed phrases. This brings signiﬁcant beneﬁts in scalability since the similarity search can be efﬁciently implemented.

“We want to find disease entities but we do not have training data and experts on diseases.”

Step 1. Query Formulation Which disease?
Question Encoder q

Step 2. Retrieval

Step 3. Dictionary Matching

• Leprosy, also known as Hansen's disease, is caused by a bacillus, "Mycobacterium leprae".
• Vasculitis, for instance, is usually diagnosed on biopsy.
•…

• Leprosy, also known as Hansen's disease, is caused by a bacillus, "Mycobacterium leprae".
• Vasculitis, for instance, is usually diagnosed on biopsy.
•…

Phrase Index

• Leprosy, • Vasculitis, •… • Hansen’s
disease

Pseudo-dictionary
• Leprosy • Vasculitis
Normalization • …
• Hansen’s disease

Step 4. Self-training
Final model

Student model

Update

Weak label

Teacher model

Iterative process

Figure 1: An overview of GeNER. Given the needs of extracting disease entities from text, GeNER automatically generates an NER dataset for the disease entities without resorting to human-annotated training data or domain experts. (1) Query formulation: the needs of NER are ﬁrst formulated as simple natural language questions. (2) Retrieval: we use an open-domain QA model to retrieve relevant phrases (i.e., entities) as well as sentences to annotate. (3) Dictionary matching: retrieved sentences are annotated by the normalized phrases. (4) Self-training: we train NER models purely on our generated dataset using self-training. See Section 3 for more details.

Recent advancement in phrase retrieval (Lee et al., 2021a) designs the retrieval purely with dense vectors as follows:

s = Es(s, W), q = Eq(q),

s∗ = arg max(s q),

(1)

s∈S (W )

where W is a large-scale open-domain corpus, S(W) is the set of all phrases in W, q is the input question, and Eq and Es are question and phrase encoders that produce dense vectors q and s. The candidate answer s∗ can be retrieved along with the evidence document or sentence due to its extractive nature (Lee et al., 2021b). In our experiments, we use evidence sentences that contain each s∗ as our training sentences X˜ train and leverage the retrieved phrases s∗ to generate weak labels Y˜ train.
Asking simple natural language questions to open-domain QA models allows us to automatically ﬁnd named entities of interest and mine indomain sentences to annotate. We use a dense phrase retrieval model (Lee et al., 2021a) as our open-domain QA model due to its run-time efﬁciency when retrieving thousands of phrases and sentences to construct NER datasets, which also exhibits strong open-domain QA accuracy.2
2While we can also adopt conventional retriever-reader models (Lee et al., 2019; Karpukhin et al., 2020) as our opendomain QA models, extracting a large number of phrases is often not supported for them and non-trivial to implement.

2.3 Connection to Zero-shot NER
Zero-shot NER aims to build models that generalize to unseen entity types without corresponding labels. In other words, it has hard constraints that the entity types in Ytest are not observed during training over Dtrain. To tackle this task, researchers propose to utilize external descriptions of entities for recognizing the target entities (Aly et al., 2021; Wang et al., 2021). On the other hand, our method directly generates datasets for any entity type of interest. Notably, Li et al. (2020) formulate NER as supervised question answering, where a model should ﬁnd the answers (i.e., entities) for questions about entity types in the given sentences, which also supports zero-shot NER. While our method also beneﬁts from the format of QA, we use natural language questions to generate new NER datasets from open-domain QA models.
3 Method
In this section, we describe GeNER, which automatically generates NER datasets from scratch and provides an NER model trained on our dataset. The overview of GeNER is outlined in Figure 1.
3.1 Query Formulation
In GeNER, we ﬁrst formulate one’s needs to recognize speciﬁc types of named entities as natural language questions. For instance, we ask “Which disease?” to extract disease entities. In general, we use a template “Which [TYPE]?,” where [TYPE] is

substituted for an entity type of interest. An input question q is then encoded as a question vector q as in Equation (1).
Reﬂecting the need of NER datasets Since we evaluate our models on Dtest, our questions need to reﬂect the needs of each NER dataset. However, in most NER benchmarks, entity types are coarsely deﬁned and they can be broken down into many subtypes. These subtypes are often deﬁned differently by the dataset. For instance, the organization type of CoNLL-2003 (Tjong Kim Sang, 2002) mostly includes companies and sports teams, but the organization type of Wikigold (Balasuriya et al., 2009) additionally covers bands as well. To understand the needs of each NER dataset, we analyze their validation sets and formulate subquestions. For instance, as shown in Table 2, we use nine subquestions for CoNLL-2003 and three for BC5CDR. Each subquestion is independently processed to retrieve relevant phrases and sentences as described in the next section.
3.2 Retrieval
Based on the question vector for each subquestion, we mine unlabeled training sentences X˜ train and generate pseudo-dictionary V˜. Given L subquestions for all entity types (e.g., L = 9 for CoNLL-2003), we use the phrase retrieval model DensePhrases (Lee et al., 2021a) to retrieve relevant phrases and evidence sentences. For the retrieval, we use the 2018-12-20 Wikipedia snapshot for all of our experiments. Note that while other domain-speciﬁc corpora such as PubMed can be used for domain-speciﬁc NER, we use Wikipedia as our corpus since it covers many different domains making GeNER generally applicable.
Normalization We retrieve top phrases s∗ for each subquestion and normalize them to reﬁne noisy spans. The set of normalized phrases becomes our pseudo-dictionary V˜. Normalization rules (Appendix C) vary depending on the datasets or entity types due to different annotation guidelines or inherent characteristics of different entity types. For instance, we split any phrases containing the conjunction ‘and’ into two different phrases for CoNLL-2003, whereas we do not apply this rule to biomedical NER datasets since they consider this as a single composite mention. Furthermore, some entity types (e.g., song) can start with an article ‘the’ while other entity types are usually annotated without it. Hence, we regard the normalization

Entity Type CoNLL-2003 person
location
organization BC5CDR disease chemical

[TYPE]
sports player politician actor country city state in the USA sports team institution company
disease chemical compound drug

kl 5,000 5,000 5,000 10,000 10,000

Rules 1,3 1,3 1,3 9 9

Table 2: Subquestions and hyperparameters used for CONLL-2003 and BC5CDR. Each subquestion is formulated as “Which [TYPE]?” and used for the retrieval. kl: number of unique sentences retrieved for each subquestion. Rule 2, 4, 5, 6, 7, 8, and 10 are commonly applied for all subquestions, thus we omit them for simplicity. Normalization rules are detailed in Appendix C. See Table 14 for details of other datasets.

rules as hyperparameters that need to be tuned on validation sets. In real-world scenarios, these rules can be pre-deﬁned by users.
Training sentences Lee et al. (2021b) show how phrase retrieval models can be used to retrieve different granularities of text such as sentences and paragraphs. Intuitively, sentences that contain the retrieved phrases are selected and the score of each sentence is computed by the maximum phrase score within the sentence. Based on their formulation, we also obtain top kl unique sentences for each subquestion that contains each phrase s∗. We gather k1 + · · · + kL sentences in total, which become our unlabeled training sentences X˜ train. Note that we do not have a hyperparameter for the size of pseudo-dictionary V˜ and it changes based on the number of unique sentences to retrieve.3
3.3 Dictionary Matching
This stage annotates the unlabeled sentences X˜ train and generates Y˜ train using the pseudo-dictionary V˜. For every phrase (or entity) in V˜, we annotate every occurrence of the phrase in X˜ train with the BIO tagging scheme (Ramshaw and Marcus, 1995) and generate Y˜ train. By this dictionary
3If we have more than two phrases contained in the same sentence, we keep all of them in V˜.

Domain News Wikipedia Twitter
Biomedicine
Natural science Literature Artiﬁcial intelligence

Dataset (# Types)
CoNLL-2003 (3)
Wikigold (3)
WNUT-16 (9)
NCBI-disease (1) BC5CDR (2) CHEMDNER (1) Enzyme♦ (1) Astronomical object♦ (1) Award♦ (1) Conference♦ (1)

Training # Sents # Labels

14,987 20,061

1,142

1,842

2,394

1,271

5,432 4,582 30,884

5,134 9,387 29,530

200

22

200

121

100

34

100

24

Validation # Sents # Labels

3,469

5,022

280

523

1,000

529

923 4,602 30,841

787 9,596 29,543

450

48

450

373

400

124

350

89

Test # Sents # Labels

3,685

4,947

274

484

3,850

2,889

942 4,812 26,561

960 9,809 25,388

543

80

543

337

416

141

431

93

Table 3: Statistics of NER datasets. # Types: the number of entity types. # Sents: the number of sentences. # Labels: the number of entity-level annotations. ♦: ﬁne-grained entities derived from the CrossNER dataset (Liu et al., 2021b). Note that we do not use any training resources (e.g., training sentences or labels) but only use the generated dataset from GeNER. See Appendix A for detailed description of each dataset and Table 14 for hyperparameters (e.g., number of subquestions) for each dataset.

matching process, we prevent possible false negatives that might occur when annotating only initially retrieved phrases as entities (See Step 3 in Figure 1). Finally, we obtain the training data D˜train = {X˜ train, Y˜ train}, which reﬂects our needs formulated in natural language questions.
3.4 Self-training
We can directly use D˜train to train NER models. However, weak labels are often incomplete and direct training can be suboptimal for NER models (Yang et al., 2018; Shang et al., 2018b; Liang et al., 2020). Thus, we train our models using the self-training method (Liang et al., 2020) to mitigate noises from weak labels. First, we initialize a teacher model with the generated D˜train. The teacher then annotates X˜ train, and a student model is trained on the re-annotated corpus. For each iteration, the teacher model is updated as the student model, and we use the student model as our ﬁnal NER model. See Appendix A for more details on this self-training process. For teacher and student models, we ﬁrst use Transformer-based pre-trained language models to obtain contextualized token representations, which are then used to predict token-level labels by a simple linear classiﬁer. We use RoBERTa (Liu et al., 2019) for most domains except for the biomedical domain where we use BioBERT (Lee et al., 2020). Note that GeNER is a model-agnostic framework and other recent techniques can be adopted in the training process (Liu et al., 2021a; Meng et al., 2021).

4 Experiments
4.1 Low-resource NER
GeNER does not use any in-domain resources, which we call as a zero-resource model. On popular NER benchmarks, we compare GeNER with other low-resource models that rely on in-domain resources as little as possible. For the evaluation metrics, we use entity-level precision (P), recall (R), and F1 score (F1).
Datasets We use six popular NER benchmarks across four domains: CoNLL-2003 (Tjong Kim Sang, 2002), Wikigold (Balasuriya et al., 2009), WNUT-16 (Strauss et al., 2016), NCBIdisease (Dog˘an et al., 2014), BC5CDR (Li et al., 2016), and CHEMDNER (Krallinger et al., 2015).4 See Table 3 for detailed statistics of each dataset.
Baselines Among low-resource models, TALLOR (Li et al., 2021) uses the least amount of in-domain resources: unlabeled training sentences Xtrain and seed entities (i.e., a small dictionary V containing 20 to 60 entities). In addition to TALLOR, we provide baselines that use the same indomain resources as TALLOR: Seed Entities, Neural Tagger, and Self-training. See Appendix A for more details of each baseline. We also report the performance of rich-resource models, which have an access to either human-annotated training labels Ytrain or rich in-domain dictionaries V constructed by domain experts. This includes fully su-
4Following Li et al. (2021), we exclude the miscellaneous and others since the needs for entities are ambiguous.

Model

Resource Xtrain / Ytrain / V

CoNLL-2003

P

R F1

Rich-resource setting (w/ training label or in-domain dict.)

RoBERTa (Liu et al., 2019) BOND (Liang et al., 2020)

// / /

93.1 93.9 93.5 80.6 82.4 81.5

Low- or zero-resource setting

Seed Entities Neural Tagger Self-training TALLOR† (Li et al., 2021) TALLOR (Li et al., 2021) GeNER (ours)

 / / / / / / / / / /  / /

95.1 2.8 5.3 71.8 13.6 22.9 43.0 31.6 36.4 64.3 64.1 64.2 59.3 58.4 60.2 73.1 69.0 71.0

Wikigold

P

R F1

86.5 87.2 86.8 58.2 61.5 59.8

90.5 3.9 7.5

58.8 4.1 7.7

32.8 17.4 22.7

-

-

-

35.0 26.8 30.3

65.8 79.9 72.5

WNUT-16

P

R F1

57.3 60.8 59.0 47.0 48.4 47.7

67.7 1.5 2.9

0.5 7.4 1.0

25.0 19.6 22.3

-

-

-

32.0 23.7 27.2

44.8 54.0 48.5

Model

Resource Xtrain / Ytrain / V

NCBI-disease

P

R F1

Rich-resource setting (w/ training label or in-domain dict.)

BioBERT (Lee et al., 2020) BOND (Liang et al., 2020)

// / /

86.6 90.5 88.5 87.5 60.3 71.4

Low- or zero-resource setting

Seed Entities Neural Tagger Self-training TALLOR† (Li et al., 2021) TALLOR (Li et al., 2021) GeNER (ours)

 / / / / / / / / / /  / /

88.8 10.7 19.1

75.2 24.9 37.4

67.5 35.1 46.2

-

-

-

61.5 34.7 44.3

78.6 72.7 75.5

BC5CDR

P

R F1

86.7 90.5 88.6 81.0 80.3 80.6

95.7 3.6 6.9 93.1 9.7 17.6 73.3 12.7 21.6 66.5 66.9 66.7 65.6 56.8 61.9 68.6 73.3 70.8

CHEMDNER

P

R F1

91.4 91.1 91.2

-

-

-

93.7 12.2 21.5 74.8 21.6 33.5 41.2 44.7 42.9 63.0 60.2 61.6 61.6 51.5 56.1 58.6 68.8 63.2

Table 4: Performance of NER models on six datasets. Xtrain: (unlabeled) training sentences. Ytrain: human annotated training labels. V: in-domain dictionaries by domain experts. In-domain resources are either fully used (), partially used ( ), or not used (). †: utilizes n-gram statistics from the test set (Ytest). Among low- or zero-resource models, best scores are in boldface and scores higher than that of BOND are denoted as .

pervised models (Liu et al., 2019; Lee et al., 2020) and the previous best weakly supervised model BOND (Liang et al., 2020). Note that all baseline models use unlabeled training sentences Xtrain as well as different amount of other in-domain resources (Ytrain or V), while GeNER is purely trained on our generated datasets without any indomain resources.
Results Table 4 shows our main results on six NER benchmarks. Without any in-domain resources, GeNER largely outperforms all low-resource models in terms of F1 score. In particular, GeNER signiﬁcantly outperforms the strongest low-resource model TALLOR by 20.2 F1 score on average (macro averaged over six datasets). The gap in performance is very large especially when the size of a training set is small (e.g., Wikigold and WNUT16). This is because most baseline models need to learn from weak labels annotated on a small number of training sentences. GeNER does not suffer from this problem as it uses automatically

generated datasets, which can also be very large. Surprisingly, GeNER even outperforms BOND on Wikigold, WNUT-16, and NCBI-disease by 12.3, 0.8 and 4.1 F1 score, respectively. 5
4.2 Fine-grained NER
One of the main beneﬁts of GeNER is that we can easily create datasets for new entity types. To validate the effectiveness of GeNER on extracting ﬁne-grained entities, we use four ﬁne-grained datasets derived from CrossNER (Liu et al., 2021b): enzyme and astronomical object, award, and conference. We chose these four types since they are less ambiguous than others (e.g., event) so that using a single subquestion is often enough. We create these ﬁne-grained datasets by removing labels for other entity types in CrossNER. The statistics of the datasets are summarized in Table 3.
5Note that it is difﬁcult to train BOND on CHEMDNER due to the lack of rich dictionaries for CHEMDNER, which shows the limitation of weakly supervised methods.

Model
Supervised
GeNER + Fine-tuning

Enzyme
56.4
49.5 63.1

Astr.
78.0
71.9 86.8

Award
75.4
80.9 81.6

Conf.
49.4
41.1 64.0

Table 5: Performance of RoBERTa (Supervised) and GeNER on ﬁne-grained entity types. F1 score is reported. Astr. and Conf. indicate astronomical object and conference, respectively.

Model
Supervised + NSP + Self-training
QUIP (standard) QUIP (prompt)
GeNER + Fine-tuning

CoNLL -2003
53.5‡ 61.4‡ 65.4‡ 70.0‡ 74.0‡
71.0 75.0

Wikigold
47.0‡ 64.0‡ 68.4‡ 67.6 70.6
72.5 73.3

BC5CDR
55.0 -
61.8 65.7
70.8 76.2

Table 6: Performance of few-shot NER models on three NER datasets. Average F1 score on each test set is reported. ‡: scores from Huang et al. (2020) and Jia et al. (2021). For the supervised models and GeNER, we use RoBERTa for CoNLL-2003 and Wikigold, and BioBERT for BC5CDR.

Results We compare GeNER with fully supervised RoBERTa in Table 5. We ﬁnd that GeNER is very competitive with the fully-supervised model and sometimes even outperforms the fully supervised model (Award). The performance can be further improved by ﬁne-tuning GeNER on each training set (+ Fine-tuning), which shows that GeNER can be used as a data augmentation method. In Section 5.3, we showcase the possibility of GeNER for extremely ﬁne-grained entity types such as “dish made with eggs” and “satellite made by an American company.”
4.3 Few-shot NER
While we compare GeNER with low- or richresource models, few-shot NER is another approach to tackle low-resource NER. Unlike using the entire training sentences with additional resources, few-shot NER models use a smaller number of training sentences and their labels (i.e., Xtrain = , Ytrain = ). Following Huang et al. (2020) and Jia et al. (2021), we compare GeNER with few-shot NER models on three NER datasets (CoNLL-2003, Wikigold, and BC5CDR). See Appendix B for more details on baseline models and datasets used in the few-shot experiments.

Template
Which [TYPE]?
list of [TYPE] example of [TYPE] What [TYPE]? [TYPE]

P@100
97.4
79.4 66.4 90.7 69.6

Diversity
44.6
56.3 50.9 48.7 58.9

F1 Score kl = 100 5, 000

52.3

72.7

53.6

72.6

49.7

57.9

53.3

61.0

56.1

67.4

Table 7: Retrieval quality and NER performance of different question templates. P@100 and diversity are macro-averaged over different types, and F1 score on the CoNLL-2003 validation set is reported. Models retrieve either 100 or 5, 000 sentences for each subquestion. See Section 5.1 for more details.

[TYPE]
organization
sports team + company + band

CoNLL-2003
27.3
49.9 53.3 55.3

Wikigold
35.8
46.8 57.2 60.7

Table 8: Performance of GeNER with different sets of subquestions for the organization type on the CoNLL2003 and Wikigold validation sets. F1 score on the organization type is reported. Each [TYPE] is used with the question template “Which [TYPE]?”.

Results Table 6 shows the performance of fewshot NER models and GeNER. Before ﬁne-tuning GeNER, it already outperforms fully-supervised models as well as the recent few-shot NER model QUIP (Jia et al., 2021). When ﬁne-tuned on the same set of few-shot examples, GeNER achieves new state-of-the-art performance on all datasets.
5 Analysis
5.1 Ablation Study
Question templates We test ﬁve different question templates in GeNER. We compare them in terms of their phrase retrieval quality and ﬁnal NER performance. To measure the retrieval quality, we manually check how many top 100 phrases for each subquestion are entities of correct types and compute the precision (P@100). We also measure the number of unique phrases in the top-100 retrievals (diversity). Table 7 shows that “Which [TYPE]?” has the highest P@100 while “[TYPE]” has the best diversity. While the diversity measure correlates well with the performance when retrieving kl = 100 sentences for each subquestion, retrieving a larger number of sentences (kl = 5, 000) mitigates the low diversity problem and shows the best performance overall.

Model
Rule 4, 8, 10
− Rule 4 (remove “the”) − Rule 8 (add abbreviations) − Rule 10 (reﬁne boundaries) + Rule 1 (split “and”)
+ Rule 9 (keep single tokens)

NCBI -disease
57.5
54.7 54.8 53.7 54.3
73.7

BC5CDR
71.8
71.5 70.8 70.8 70.7
68.3

Table 9: F1 score of GeNER with different sets of normalization rules on the NCBI-disease and BC5CDR validation sets. See Appendix C for a detailed description of each rule. In this experiment, Rule 2, 5, 6, and 7 are also applied in both datasets, but we omit them in the table for simplicity.

Effect of subquestions GeNER uses subquestions for each dataset to better reﬂect the needs of each NER dataset. In Table 8, we report the performance of GeNER on the CoNLL-2003 and Wikigold validation sets with different sets of subquestions. Compared to using only “organization” question, using multiple subquestions has better performance while being more explicit about the needs for NER. Interestingly, although CoNLL-2003 does not contain many band names unlike Wikigold, both datasets beneﬁt from using “band” as an additional subquestion. This implies their context may help generalize to other organization entities.
Effect of normalization rules Table 9 shows the ablation of different normalization rules for the generation of two disease NER dataets: NCBI-disease and BC5CDR. While Rule 4, 8, and 10 are the common rules that are important to add on both datasets, adding Rule 1 is harmful on both since they treat disease entities in conjunction as a single composite entity (e.g., breast and ovarian cancer). Performance after adding Rule 9 shows that even if they are both disease NER datasets, they might require different normalization rules.
Effect of self-training Figre 2 shows the effect of self-training with three different dictionaries: seed entities from TALLOR, in-domain dictionary from BOND, and pseudo-dictionary from GeNER.6 Performance when step=0 denotes the dictionary matching performance before any selftraining. Although our pseudo-dictionary is initially incomplete compared to the in-domain dictionary, self-training largely closes the gap, which
6The size of the dictionary from BOND is over 300k, and ours is 7,352. The set of seed entities consists of 20 entities.

F1 Score

90

74.1

73.7

70

58.3 50
39.2

Seed entities In-domain dictionary (BOND) Pseudo-dictionary (GeNER)

30 26.4
20.2

10

0

1

2

3

4

5

6

Self-training Step (1k)

Figure 2: Effect of self-training with three different dictionaries from TALLOR, BOND, and GeNER. F1 score on the NCBI-disease validation set is reported. Best scores for each dictionary are labeled.

Step
1. Query Formulation 2. Retrieval 3. Dictionary Matching 4. Self-training
Total Time

CoNLL -2003
0.7s 82.5s 9m 16.3s 58m 25.0s
1h 9m 3.8s

CrossNER (Award)
0.6s 16.3s 1m 54.0s 8m 17.2s
10m 28.1s

Table 10: Complexity analysis of GeNER. For the selftraining step, time taken for ﬁnding the best model on each validation set is reported. Time per each step is measured on a server with Intel Xeon(R) Silver 4210 CPU @ 2.20GHz and a single 24GB GPU (RTX 3090).

does not happen for seed entities.
5.2 Complexity Analysis
From automatic dataset generation to model training, GeNER is a very efﬁcient framework. In Table 10, we show the time taken for each step in GeNER for two NER datasets. Dataset generation steps (Step 1, 2, and 3) mostly take less than 10 minutes in total while the self-training step (Step 4) might take longer based on the number of total training sentences. For instance, we have L = 9 subquestions and kl = 5, 000 sentences for each subquestion, which sum up to 45, 000 training sentences for CoNLL-2003. On the other hand, we have a single subquestion and kl = 10, 000 sentences for CrossNER (Award).
5.3 Qualitative Analysis
Visualization The top phrases of GeNER are retrieved based on the similarity score between the phrase (i.e., entity) vectors and our question vector.

[TYPE] song nominated for the Grammy Awards
dish made with eggs
satellite made by an American company
conll_15 (42).html

Retrieved Entities from GeNER
Hotline Bling, Love Me like You Do, Mystery of Love, Can’t Stop the Feeling!, The Price is Wrong, ...
Eggs Benedict, Pancakes, Shakshouka, Omelettes, Huevos rancheros, Chilaquiles, Menemen, ...
GE-2, AMC-2, Ariel 1, Syncom 3, CHIPSat, Telstar, Explorer 1, Westar 1, SkyTerra-1, ...

P@50 0.96 0.80 0.88

Diversity 0.90 0.78 0.82

Table 11: Retrieval entities from GeNER for ﬁne-grained entity types. “Which [TYPE]?” is used as a question.

Boston Red Sox Which sports team?
Zinedine Zidane
Which sports player?

question. This is due to the ﬂexibility of the natural language questions, which can easily provide NER models for specialized entity types.

California Which state in the USA?

China France

Jakarta

Which actor?

Liege

Which country? Which city? Which politician?

Context diversity Since dense representations of text can capture subtle semantic relationships between the context and the question (Karpukhin et al., 2020; Lee et al., 2021a), our simple questions often retrieve sentences with diverse context.

Boris Yeltsin Which institution?
OSCE Which company?
CITIC Pacific

Person Location Organization Entity vector Question vector

We found that almost half of the retrieved sentences for “Which disease?,” do not contain “disease” in their context. As shown in Table 12, our retrieved sentences have much more diverse context than sentences from the rule-based model (Li et al., 2021).
6 Discussion and Future Work

Figure 3: Visualization of GeNER question vectors and −100 entities in−50the CoNLL-20003 validation set5.0 Vectors are 100GeNER is the150ﬁrst attempt to automatically gener-

visualized with t-SNE.

ate NER datasets without relying on any in-domain

resources. We discuss some of the important as-

pects of GeNER that are not explored in depth and

To understand how GeNER works, we visualize the provide possible future directions.

question vectors q used by GeNER and the entity vectors computed from the CoNLL-2003 validation set. Question vectors are encoded from the question encoder of DensePhrases while its phrase encoder is used to compute the entity vectors of annotated entities in CoNLL-2003. From Figure 3, we observe that the entities in the validation set are well separated based on their entity types. This indicates that the phrase encoder of DensePhrases already provides high-quality entity representations for NER. Also, we observe that our question vectors cover different groups of entity vectors, which will eventually retrieve entities of correct types.
Retrieved entities We further show the potential of GeNER on three entities that are extremely ﬁnegrained such as “satellite made by an American company.” Similar to Section 5.1, we manually measure retrieval performance using precision at 50 (P@50) and diversity. As shown in Table 11, accurate and diverse entities can be retrieved for each

Better QA models Since GeNER is a modelagnostic framework, we can also employ stronger
1/1
open-domain QA models that often rely on the retriever-reader approach (Fajcik et al., 2021). However, due to the large number of phrases that we need to retrieve (e.g., 5,000), it is much more convenient to employ phrase retrieval models that have better run-time efﬁciency as well as strong accuracy.7 Whether the advancement of open-domain QA models can also translate to the improvement of GeNER is an interesting research direction.
Multi-type NER Our results in Section 4.2 show how ﬂexible GeNER is with ﬁne-trained entities. While we can try to directly train an NER model for all 60 ﬁne-grained entity types in CrossNER (Liu et al., 2021b) (except for the miscellaneous type),
7Our preliminary experiments of using DPR (Karpukhin et al., 2020) showed much lower retrieval performance (P@100) and slower inference speed. It is also difﬁcult to scale DPR to extract more than a hundred phrases.

TALLOR (entities and their context for each rule)
Rule: POStag=“NOUN” ∧ PostNgram=“attack” [1] Acute hepatitis attack after exposure to telithromycin. [2] This is not consistent with a CNS origin of migraine attack.
Rule: PreNgram=“in patients with” ∧ PostNgram=“’s disease” [1] . . . an increased mortality in patients with Parkinson’s disease (PD) . . . [2] . . . in the treatment of psychosis and disruptive behaviors in patients with Alzheimer’s disease.
GeNER (entities and their context for “Which disease?”)
[1] Leprosy has affected humanity for thousands of years. [2] Heart disease is one of the leading causes of death in the world. [3] During this war an outbreak of syphilis occurred among the French troops. [4] . . . , typhus being at once the most contagious and the most preventable of diseases, . . . [5] When syphilis was ﬁrst deﬁnitely recorded in Europe in 1495, its pustules often . . .
Table 12: Comparison of extracted entities and their context from TALLOR (Li et al., 2021) and GeNER (ours) for disease NER. While TALLOR relies on explicit rules based on POS tags or n-grams, GeNER discovers named entities more implicitly, which appear in more diverse context. Note that the context of TALLOR is from the BC5CDR training set (sentences from PubMed) while that of GeNER is mined from Wikipedia.

we found that it is difﬁcult to balance the number of entities for each type to train a strong multitype NER model. For instance, the number of states in the USA is much smaller than the number of sports players. This might require ﬁnding the right number of entities for each type without an exhaustive hyperparameter search.
Other applications GeNER is our ﬁrst instantiation of the ask-to-generate approach. We can easily think of other applications such as relation extraction. For instance, if we want to train a relation extraction model for the drug-disease relationship, we can simply ask “Which drug is effective for disease?” and use retrieved sentences as positive training instances. We can use retrieved phrases as objects (drug) and leverage NER models to ﬁnd subject entities (disease) in the evidence sentence. It will be interesting to compare this approach to distantly supervised approaches (Mintz et al., 2009), which we leave as future work.
7 Conclusion

the-art on three NER benchmarks. Our analysis shows how the quality of the generated datasets is related to the ﬁnal model performance and also reveals the differences in annotation guidelines of each dataset. We hope our models serve as a strong baseline in a low- or zero-resource NER.
Acknowledgements
We thank Jungsoo Park, Gyuwan Kim, Mujeen Sung, Sungdong Kim, Yonghwa Choi, and Wonjin Yoon for the helpful feedback. This research was supported by (1) National Research Foundation of Korea (NRF-2020R1A2C3010638), (2) the MSIT (Ministry of Science and ICT), Korea, under the ICT Creative Consilience program (IITP-20212020-0-01819) supervised by the IITP (Institute for Information & communications Technology Planning & Evaluation), and (3) a grant of the Korea Health Technology R&D Project through the Korea Health Industry Development Institute (KHIDI), funded by the Ministry of Health & Welfare, Republic of Korea (grant number: HR20C0021).

In this work, we introduce GeNER, an automated NER dataset generation framework. Without using any in-domain resources, GeNER largely outperforms existing low-resource models that rely on unlabeled training sentences and in-domain dictionaries. GeNER often outperforms the previous best weakly supervised model that uses large in-domain dictionaries and also outperforms fully supervised models on ﬁne-grained datasets. When evaluated on the few-shot setting, we achieve new state-of-

References
Rami Aly, Andreas Vlachos, and Ryan McDonald. 2021. Leveraging type descriptions for zero-shot named entity recognition and classiﬁcation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers).
Dominic Balasuriya, Nicky Ringland, Joel Nothman, Tara Murphy, and James R. Curran. 2009. Named

entity recognition in Wikipedia. In Proceedings of the 2009 Workshop on The People’s Web Meets NLP: Collaboratively Constructed Semantic Resources (People’s Web).
Yixin Cao, Zikun Hu, Tat-seng Chua, Zhiyuan Liu, and Heng Ji. 2019. Low-resource name tagging learned with weakly labeled data. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP).
Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. 2017. Reading Wikipedia to answer opendomain questions. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers).
Leyang Cui, Yu Wu, Jian Liu, Sen Yang, and Yue Zhang. 2021. Template-based named entity recognition using BART. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021.
Rezarta Islamaj Dog˘an, Robert Leaman, and Zhiyong Lu. 2014. Ncbi disease corpus: a resource for disease name recognition and concept normalization. Journal of biomedical informatics, 47.
Martin Fajcik, Martin Docekal, Karel Ondrej, and Pavel Smrz. 2021. R2-D2: A modular baseline for open-domain question answering. In Findings of the Association for Computational Linguistics: EMNLP 2021.
Abbas Ghaddar and Philippe Langlais. 2017. Winer: A wikipedia annotated corpus for named entity recognition. In Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 413–422.
Jiaxin Huang, Chunyuan Li, Krishan Subudhi, Damien Jose, Shobana Balakrishnan, Weizhu Chen, Baolin Peng, Jianfeng Gao, and Jiawei Han. 2020. Fewshot named entity recognition: A comprehensive study. arXiv preprint arXiv:2012.14978.
Robin Jia, Mike Lewis, and Luke Zettlemoyer. 2021. Question answering infused pre-training of generalpurpose contextualized representations. arXiv preprint arXiv:2106.08190.
Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense passage retrieval for open-domain question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP).
Martin Krallinger, Obdulia Rabal, Florian Leitner, Miguel Vazquez, David Salgado, Zhiyong Lu, Robert Leaman, Yanan Lu, Donghong Ji, Daniel M Lowe, et al. 2015. The chemdner corpus of chemicals and drugs and its annotation principles. Journal of cheminformatics, 7(1).

Guillaume Lample, Miguel Ballesteros, Sandeep Subramanian, Kazuya Kawakami, and Chris Dyer. 2016. Neural architectures for named entity recognition. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies.
Jinhyuk Lee, Mujeen Sung, Jaewoo Kang, and Danqi Chen. 2021a. Learning dense representations of phrases at scale. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers).
Jinhyuk Lee, Alexander Wettig, and Danqi Chen. 2021b. Phrase retrieval learns passage retrieval, too. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing.
Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang. 2020. Biobert: a pre-trained biomedical language representation model for biomedical text mining. Bioinformatics, 36(4).
Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. 2019. Latent retrieval for weakly supervised open domain question answering. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics.
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics.
Jiacheng Li, Haibo Ding, Jingbo Shang, Julian McAuley, and Zhe Feng. 2021. Weakly supervised named entity tagging with learnable logical rules. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers).
Jiao Li, Yueping Sun, Robin J Johnson, Daniela Sciaky, Chih-Hsuan Wei, Robert Leaman, Allan Peter Davis, Carolyn J Mattingly, Thomas C Wiegers, and Zhiyong Lu. 2016. Biocreative v cdr task corpus: a resource for chemical disease relation extraction. Database, 2016.
Xiaoya Li, Jingrong Feng, Yuxian Meng, Qinghong Han, Fei Wu, and Jiwei Li. 2020. A uniﬁed MRC framework for named entity recognition. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics.
Chen Liang, Yue Yu, Haoming Jiang, Siawpeng Er, Ruijia Wang, Tuo Zhao, and Chao Zhang. 2020. BOND: bert-assisted open-domain named entity

recognition with distant supervision. In KDD ’20: The 26th ACM SIGKDD Conference on Knowledge Discovery and Data Mining.
Kun Liu, Yao Fu, Chuanqi Tan, Mosha Chen, Ningyu Zhang, Songfang Huang, and Sheng Gao. 2021a. Noisy-labeled NER with conﬁdence estimation. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692.
Zihan Liu, Yan Xu, Tiezheng Yu, Wenliang Dai, Ziwei Ji, Samuel Cahyawijaya, Andrea Madotto, and Pascale Fung. 2021b. Crossner: evaluating crossdomain named entity recognition. Proceedings of the AAAI Conference on Artiﬁcial Intelligence, 35.
Yu Meng, Yunyi Zhang, Jiaxin Huang, Xuan Wang, Yu Zhang, Heng Ji, and Jiawei Han. 2021. Distantlysupervised named entity recognition with noiserobust learning and language model augmented selftraining. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing.
Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky. 2009. Distant supervision for relation extraction without labeled data. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP.
Lance Ramshaw and Mitch Marcus. 1995. Text chunking using transformation-based learning. In Third Workshop on Very Large Corpora.
Minjoon Seo, Jinhyuk Lee, Tom Kwiatkowski, Ankur Parikh, Ali Farhadi, and Hannaneh Hajishirzi. 2019. Real-time open-domain question answering with dense-sparse phrase index. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics.
Jingbo Shang, Jialu Liu, Meng Jiang, Xiang Ren, Clare R Voss, and Jiawei Han. 2018a. Automated phrase mining from massive text corpora. IEEE Transactions on Knowledge and Data Engineering, 30(10).
Jingbo Shang, Liyuan Liu, Xiaotao Gu, Xiang Ren, Teng Ren, and Jiawei Han. 2018b. Learning named entity tagger using domain-speciﬁc dictionary. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing.
Benjamin Strauss, Bethany Toma, Alan Ritter, MarieCatherine de Marneffe, and Wei Xu. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT).

Erik F. Tjong Kim Sang. 2002. Introduction to the CoNLL-2002 shared task: Language-independent named entity recognition. In COLING-02: The 6th Conference on Natural Language Learning 2002 (CoNLL-2002).
Yaqing Wang, Haoda Chu, Chao Zhang, and Jing Gao. 2021. Learning from language description: Low-shot named entity recognition via decomposed framework. In Findings of the Association for Computational Linguistics: EMNLP 2021.
Yaosheng Yang, Wenliang Chen, Zhenghua Li, Zhengqiu He, and Min Zhang. 2018. Distantly supervised NER with partial annotation learning and reinforcement learning. In Proceedings of the 27th International Conference on Computational Linguistics.
Yi Yang and Arzoo Katiyar. 2020. Simple and effective few-shot named entity recognition with structured nearest neighbor learning. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP).

A Low-resource NER
A.1 Datasets
News We use CoNLL-2003 (Tjong Kim Sang, 2002), which is one of the most commonly used NER benchmarks. The data consists of Reuters news articles with three entity types: person, location, and organization.
Wikipedia Wikigold (Balasuriya et al., 2009) uses entity types that are the same as CoNLL-2003, but their subcategories are very different due to differences in domains. Also it is a relatively smaller dataset compared to others.
Twitter WNUT-16 (Strauss et al., 2016) consists of nine entity types annotated on tweets. It includes entity types such as TV show, movie, and musician.
Biomedicine We use three benchmarks commonly used in the biomedical domain, where manual annotations are particularly expensive as a high level of professional knowledge is required. NCBI-disease (Dog˘an et al., 2014) is a collection of 793 PubMed abstracts with manually annotated disease entities. BC5CDR (Li et al., 2016) consists of 1,500 manually annotated PubMed abstracts with disease and chemical entities. CHEMDNER (Krallinger et al., 2015) is a collection of 10,000 PubMed abstracts with manually annotated chemical entities, which is the largest corpus in our experiments.
A.2 Baselines
Seed Entities This model directly matches seed entities and the test corpus. For CoNLL-2003, BC5CDR, and CHEMDNER, we use the same seed entities as Li et al. (2021). For the other benchmarks, since there are no pre-deﬁned seed entities, we manually select frequent and high-precision entities in the training sets, following Li et al. (2021). Table 13 shows the list of seed entities.
Neural Tagger We ﬁrst annotate (unlabeled) training sentences by dictionary matching using seed entities as the dictionary, and then train RoBERTa or BioBERT on the generated corpus.
Self-training This model is a weak version of BOND that uses seed entities as the dictionary. See the description of BOND in this section.
TALLOR (Li et al., 2021) This model is a strong baseline model that starts with 20-60 initial labeling rules (called seed rules) and automatically expands

its labeling rule set. A seed rule is a string matching between entity candidates (i.e., spans of text) in the in-domain sentences and seed entities, e.g., “if a candidate matches the seed entity Britain, then annotate the candidate with the location type.” A neural model is trained on the sentences annotated by seed rules, and then the model generates pseudo labels again, similar to a self-training framework. Finally, new labeling rules are mined from the sentences annotated by the neural model. This process is performed iteratively.
BOND (Liang et al., 2020) The BOND framework ﬁrst generates weak labels by dictionary matching between the in-domain sentences and in-domain dictionary, and then trains NER models based on self-training, which is the previous best weakly supervised method. In-domain dictionaries are created using external resources such as Wikidata and several online websites.8 In the ﬁrst iteration, teacher and student models are initialized to common language models (e.g. RoBERTa), and the teacher is ﬁne-tuned on the weak labels. The teacher then re-annotates the in-domain sentences, and the student is trained on the newly generated labels by the teacher. In the following iterations, the teacher is updated as the (trained) student in the previous iteration.
A.3 Implementation Details
As mentioned in Section Section 3, we selected subquestions for each entity type based on the validation sets. The entire subquestions are listed in Table 14. We used public PyTorch implementation provided by Liu et al. (2019) and Lee et al. (2020)9 for implementing the Neural Tagger baselines and our ﬁne-tuning models. For BOND, we used the ofﬁcial code base provided by the authors for implementing BOND.10 Also, we used the same code base for the self-training step in GeNER. Hyperparameters in self-training are detailed in Table 15.
Implementation of TALLOR Although we used the ofﬁcial code base provided by the authors,11 our implemented version of TALLOR is lower than the reported performance (scores with † in Table 4). This is because, in the original implementation, ngram statistics of the test set was used in the entity
8For the biomedical domain, we use the dictionary provided by Shang et al. (2018b).
9https://github.com/dmis-lab/biobert-pytorch 10https://github.com/cliang1453/BOND 11https://github.com/JiachengLi1995/TALLOR

Dataset CoNLL-2003 Wikigold
WNUT-16
NCBI-disease BC5CDR CHEMDNER

Entity type person location location person location
organization person location product facility company sports team TV show movie music artist
disease
disease
chemical
chemical

# Seeds 7 8 5 5 10
10 2 3 8 5 3 1 1 1 1
20
10
10
60

Seed entities
wasim akram, waqar younis, mushraq ahmed, aamir sohail, saeed anwar, bill clinton, mother teresa
britain, italy, russia, sweden, belgium, iraq, south africa, united states
osce, nato, honda, interfax, marseille
cabral, bobick, belgrano, behe, moses mendelssohn
maaa, ncaa, 139th, major league baseball, cbs cable, bcit, montreal hockey club, 882 6pr, konami, 30 seconds to mars
england, indonesia, old goa, chicago, ontario, aabenraa county, illinois, hay street, b & sr, cal anderson park
lindsay lohan, scooter braun
belgium, toronto, arizona
ipad, htc desire z, iphone, pumpkin moonshine, coke, ﬂip minohd, club penguin, xbox 360
visions lounge, frat house hattiesburg, empire state building, disney world, club blu
twitter, youtube, facebook
jv soccer
friday night lights
iron man 2
kings of leon
dmd, pws, myotonic dystrophy, g6pd deﬁciency, hd, pku, aniridia, duchenne muscular dystrophy, fap, a - t, tay - sachs disease, tsd, fmf, prader - willi syndrome, amn, wiskott - aldrich syndrome, huntington disease, pelizaeus - merzbacher disease, bmd
proteinuria, esrd, thrombosis, tremor, hepatotoxicity, hypertensive, thrombotic microangiopathy, thrombocytopenia, akathisia, confusion
nicotine, morphine, haloperidol, warfarin, clonidine, creatinine, isoproterenol, cyclophosphamide, sirolimus, tacrolimus
glucose, cholesterol, glutathione, ethanol, androgen, graphene, glutamate, dopamine, cocaine, serotonin, estrogen, nicotine, tyrosine, resveratrol, nitric oxide, cisplatin, alcohol, superoxide, curcumin, metformin, amino acid, testosterone, ﬂavonoids, camp, methanol, amino acids, fatty acids, polyphenols, nmda, silica, 5-ht, oxygen, calcium, copper, cadmium, arsenic, zinc, mercury, (1) h, ca (2+)

Table 13: List of seed entities used in our experiments. All seed entities are in lowercase. We use the seed entities provided by Li et al. (2021) for CoNLL-2003, BC5CDR, and CHEMDNER. For the remaining datasets where seed entities are not provided, we manually select frequent and high-precision entities, following Li et al. (2021).

candidate generation process of TALLOR. On the other hand, we implemented TALLOR using only the training corpus for a more realistic setting.

source data) as well as a few examples on the target data (Yang and Katiyar, 2020; Cui et al., 2021), which is different from our few-shot setting.

B Few-shot NER
B.1 Settings
We compare models on CoNLL-2003, Wikigold, and BC5CDR in few-shot experiments, where a small number of examples are sampled 5 times for each dataset, and results are averaged over 5 runs, following Jia et al. (2021). We obtained the sampled datasets from Huang et al. (2020) for CoNLL2003 and Wikigold,12 and we sampled 10 examples 5 times for BC5CDR. Unlike the other experiments, we use the miscellaneous type in the few-shot experiments. We exclude some few-shot NER models since they use a sufﬁcient amount of seen data (i.e.,
12https://github.com/ few-shot-NER-benchmark/BaselineCode

B.2 Baselines
Supervised This model is directly trained on fewshot examples. For CoNLL-2003 and Wikigold, we used the scores reported by Huang et al. (2020). For BC5CDR, we trained BioBERT and reported its performance.
Noisy supervised pre-training (NSP) (Huang et al., 2020) NSP pre-trains models on the largescale corpus WiNER (Ghaddar and Langlais, 2017), which consists of 2013 English Wikipedia documents and pseudo labels for 113 ﬁne-grained entity types. The labels are automatically generated based on anchor links and coreference resolution. Pre-trained models are then ﬁne-tuned on a few examples for the target data.

Dataset Wikigold
WNUT-16
NCBI-disease CHEMDNER CrossNER

Entity Type person location organization person location product
facility
company sports team TV show movie music artist disease
chemical enzyme astronomical object award conference

[TYPE] athlete, musician, politician, actor, director country, city, state in the USA, road, island association, sports team, company, band, record label actor, politician, sports player, author country, city, state in the USA song, video game, mobile app software, operating system, car, smart phone, camera facility, cafe, restaurant, college, music venue sports facility company, technology company news agency, magazine sports team TV program movie band, rapper, musician, singer disease neoplastic disease, disorder, illness, condition chemical compound, drug chemical element, chemical formula enzyme astronomical object award conference on artiﬁcial intelligence

kl 1,000 1,000 1,000 1,000 1,00 1,000 1,000 1,000 1,000 5,000 1,000 5,000 5,000 5,000 1,000 20,000 5,000 30,000 5,000 5,000 5,000 10,000 5,000

Rules 1,3,4 1,3,4 1,3,4 1,3,4 1,3,4
3 1,3,4
3 1,3,4 1,3,4 1,3 1,3,4
3 3 3 4 4 4,9 4,9 1,4,9 1,3,4 1,3,4 3

Table 14: Subquestions and hyperparameters used for NER benchmarks. Each subquestion is formulated as “Which [TYPE]?” and used for the retrieval. kl: number of unique sentences retrieved for each subquestion. Normalization rules are detailed in Appendix C. Note that we omit Rule 2, 5, 6, 7, 8, and 10, since they are applied in all experiments.

Self-training This model is trained based on selftraining (Huang et al., 2020). The core idea is similar to BOND in Section 4.1, but training details are slightly different. For detailed information, refer to Huang et al. (2020).
QUIP (standard) (Jia et al., 2021) QUIP is a contextualized representation model pre-trained with 80 million question-answer pairs, which are automatically generated by the BART-large model (Lewis et al., 2020). QUIP (standard) consists of a QUIP encoder with a randomly initialized linear output layer and is ﬁne-tuned on a few examples.
QUIP (Q-prompt) (Jia et al., 2021) The output layer of this model is not initialized randomly, but rather as QUIP’s representations for question prompts. For example, for the organization type in CoNLL-2003, the corresponding weights of the output layer are initialized as the representation of the question “What is an organization?.” Jia et al.

(2021) showed that this initialization strategy is effective in few-shot NER.
GeNER Our GeNER framework automatically generates training data from natural language questions. RoBERTa or BioBERT is then trained on the generated data based on self-training. Note that a few examples for the target data are not directly used to train this model, but only to create query vectors.
GeNER + Fine-tuning We further train GeNER on the target data (i.e., a few examples), which is similar to Table 5.
C Normalization Rules
This section details normalization rules. See Table 16 for the summary of all rules.
• Rule 1 Some retrieved strings contain multiple entities, linked by the conjunction “and.” We simply split such strings based on “and.”

Model Dataset

T1 T3

CoNLL-2003 900 300

Wikigold

500 200

WNUT-16

900 300

GeNER

NCBI-disease 900 450

BC5CDR

500 400

CHEMDNER 900 450

Enzyme Astr. Award Conf.

350 700 500 300 350 500 350 700

BOND

CoNLL-2003 900 450

Wikigold

900 300

WNUT-16

900 300

NCBI-disease 900 450

BC5CDR

500 300

CoNLL-2003 400 100

Wikigold

350 200

Self-training WNUT-16

500 100

NCBI-disease 200 100

BC5CDR

500 100

CHEMDNER 900 450

Table 15: Hyperparameter conﬁguration in selftraining of GeNER and baselines. T1 is the early stopping step before updating the model, and T3 is the period of the update. For more detailed descriptions of T1 and T3, refer to Liang et al. (2020).

However, this rule should be applied depending on the annotation guideline of a dataset. For instance, biomedical NER datasets such as NCBI-disease consider composite mentions (e.g., "colorectal, endometrial, and ovarian cancers") as one entity.13 Also, this rule should not be applied to movie entities (e.g., “Harry Potter and the Sorcerer’s Stone).”
• Rule 2 DensePhrases frequently returns strings with punctuation at the end of the string, such as commas or quotation marks. We remove these noises in our experiments (e.g., Leprosy, → Leprosy). However, for some entities such as song, punctuation may not be a noise since such entities can contain them in their names.
• Rule 3 We exclude entities whose letters are all lowercase. Many entities in the real world contain one or more uppercase letters, e.g., the ﬁrst letter of most person, location, and organization entities is capitalized. Therefore,
13https://www.ncbi.nlm.nih.gov/ CBBresearch/Dogan/DISEASE/Guidelines. html

retrieved strings in which all letters are lowercase are more likely to be noisy. However, lowercase entities are common for some domains (e.g., the biomedical domain).
• Rule 4 We remove the deﬁnite article “the” from the string (e.g., “the Boston Red Sox” → “Boston Red Sox”). This rule should be applied depending on the annotation guideline of a dataset or the superﬁcial characteristics of an entity. For instance, since band entities sometimes include “the” in their name, this rule should not be applied to such entities.
• Rule 5 We exclude entities with a length less than 3 as short strings can cause lots of noise in dictionary matching. However, it is not recommended to apply this rule to some entities with short strings.
• Rule 6 We exclude entities whose lowercase strings are in the stopword list such as “WAS” (Wiskott-Aldrich Syndrome) and “US” (United States), as they can cause a lot of falsepositive noise in the dictionary matching process. At the same time, this rule can produce false-negative noise in the generated dataset, but self-training mitigates this.
• Rule 7 We exclude entities whose strings are equal to “{ENTITY}” in the discrete query. For example, if we ask the query “Which disease?”, and the resulting phrase is “disease,” we do not use the phrase.
• Rule 8 Since named entities are often abbreviated, it is important to annotate abbreviations in avoiding false-negative noise from them. We detect abbreviations of retrieved phrases using the ScispaCy abbreviation tool.14 For instance, when the phrase “Crohn’s disease” is retrieved with the evidence sentence “Crohn’s disease (CD) is one of the two main forms of inﬂammatory bowel disease.,” its abbreviation “CD” is detected and added to the dictionary.
• Rule 9 During string matching, candidate entities and sentences are converted to lowercase by default. However, since single-token entities tend to be noisy compared to multitoken entities when they are lowercase, we keep single-token entities capitalized, by applying this rule.
14https://github.com/allenai/scispacy

Number
1 2 3 4 5 6 7 8 9 10

Normalization Rule
Split the string based on the conjunction “and.” Remove punctuation from the end of the string. Exclude entities whose letters are all lowercase. Remove the deﬁnite article “the" from the string. Exclude entities whose length is less than 3. Exclude entities whose lowercase strings are in the stopword list. Exclude target entity if the string . Add abbreviations. Preserve the appearance of single-token entities. Apply AutoPhrase (Shang et al., 2018a)

Table 16: List of normalization rules with brief description. See Appendix C for more details.

• Rule 10 We use the phrase mining tool AutoPhrase (Shang et al., 2018a) to reﬁne spans of phrases in dictionary matching. Speciﬁcally, if the span of a retrieved phrase is included by that of the phrase detected by AutoPhrase, we expand the span of the retrieved phrase to that of the detected one.

