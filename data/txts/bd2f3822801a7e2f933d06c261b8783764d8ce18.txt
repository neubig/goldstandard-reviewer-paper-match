Understanding, Detecting, and Separating Out-of-Distribution Samples and Adversarial
Samples in Text Classiﬁcation

arXiv:2204.04458v1 [cs.CL] 9 Apr 2022

Cheng-Han Chiang National Taiwan University
dcml0714@ntu.edu.tw

Hung-yi Lee National Taiwan University
hungyilee@ntu.edu.tw

Abstract
In this paper, we study the differences and commonalities between statistically out-of-distribution (OOD) samples and adversarial (Adv) samples, both of which hurting a text classiﬁcation model’s performance. We conduct analyses to compare the two types of anomalies (OOD and Adv samples) with the in-distribution (ID) ones from three aspects: the input features, the hidden representations in each layer of the model, and the output probability distributions of the classiﬁer. We ﬁnd that OOD samples expose their aberration starting from the ﬁrst layer, while the abnormalities of Adv samples do not emerge until the deeper layers of the model. We also illustrate that the models’ output probabilities for Adv samples tend to be more unconﬁdent. Based on our observations, we propose a simple method to separate ID, OOD, and Adv samples using the hidden representations and output probabilities of the model. On multiple combinations of ID, OOD datasets, and Adv attacks, our proposed method shows exceptional results on distinguishing ID, OOD, and Adv samples.
1 Introduction
Deep learning-based text classiﬁcation models have achieved overwhelming success on miscellaneous benchmark datasets [1, 11] and real-world applications. Despite their great success, the performances of those models are shown to degrade when faced with data samples drawn from a distribution that is very different from the training distribution (i.e., in-distribution data), including samples that are statistically out-of-distribution (OOD) [4], and maliciously created adversaries (Adv) [19, 13, 9]. In this work, we refer to OOD samples as those samples that are statistically different from the ID dataset. Adversarial examples refer to those samples created adversarially and intentionally by some wicked individuals, intending to bring down the performance of a trained model.
We argue that for a robust text classiﬁer, it should be able to distinguish between Adv and OOD samples. Since Adv samples are crafted to harm the model performance while OOD samples are not, they should be treated differently: if the model detects an input as an Adv example, it can preprocess the input into a non-adversarial example before feeding it into the text classiﬁer [20]; if an input sample is detected as an OOD sample, the model should not make any prediction since the result might not make any sense. While detecting OOD samples [23, 4, 22] and detecting Adv samples [24, 16] have both been studied in natural language processing (NLP) previously, none has studied how to detect OOD and Adv samples when they are simultaneously presented to a deep text classiﬁer. In previous works, an OOD detection method can only separate the input samples into two groups, but it is unclear if it separates the input based on whether they are ID samples or not, or whether the inputs are OOD samples or not. If the method separates the input samples based on whether they are ID samples or not, an Adv sample will be categorized into the non-ID groups, falling into the same group as the OOD ones. Contrarily, when the method separates the input samples based
Preprint. Under review.

on whether they are OOD samples or not, an Adv sample will be categorized into the non-OOD groups, falling into the same group as the ID samples. Considering that a text classiﬁer in real-world use case will encounter different types of anomaly and should act differently for different types toward OOD and Adv samples, it is important for a robust text classiﬁer to separate OOD and Adv samples.
However, identifying OOD and Adv samples from ID samples cannot be achieved before we develop a more uniﬁed and in-depth awareness of their differences. While both kinds of anomalies hurt the model performance, it is unclear whether Adv samples are just some kinds of OOD samples, or they are fundamentally different. In this work, we aim to answer the following two questions:
1. How different are OOD and Adv samples? 2. How can we separate OOD, Adv, and ID samples?
To the best of our knowledge, no prior work has answered either of the above questions. We perform analyses that compare OOD and Adv samples against ID ones from three perspectives: the input features, the hidden representations in different layers of the model, and the output probability distributions of the classiﬁer. Based on our observations, we propose a simple yet effective method in two stages to separate ID, OOD, and Adv samples: ﬁrst, separate OOD samples from ID and Adv ones based on their hidden features; next, separate Adv from ID ones by the model’s output distribution. Abundant experiments on various combinations of 3 ID datasets, 4 OOD datasets, and 4 Adv attacks verify the effectiveness of our proposed method.
Our contributions are as follows:
• We ﬁnd that the hidden representations of the Adv samples are not abnormal until the last few layers of the model, while OOD samples’ hidden representations are highly distinguishable from ID ones starting from the ﬁrst layer.
• We ﬁnd that the maximum probability of the classiﬁer’s prediction is an excellent indicator of Adv samples, and this can help one separate Adv samples from ID samples.
• We propose an elegant and successful two-staged detection approach to separate ID, OOD, and Adv samples.
2 Datasets, Attack Methods, and Setup
In this work, we scrutinize the differences among OOD and ADV samples and attempt to separate them. Referring to a sample as an OOD one will need to ﬁrst deﬁne what ID samples are. We brieﬂy introduce the datasets for ID and OOD samples and the attack methods used for generating Adv samples from ID samples.
2.1 In-Distribution Datasets
IMDB [12] IMDB is a movie review dataset for binary sentiment classiﬁcation that contains long and polar movie reviews. The goal of the task is to predict a piece of movie review to be a positive one or a negative one.
SST-2 [18] SST-2 is also a binary sentiment classiﬁcation dataset containing movie reviews. Very different from IMDB, SST-2 contains pithy reviews whose lengths are signiﬁcantly less than the reviews in IMDB.
AG-News [25] AG-News is a text classiﬁcation dataset that involves classifying news articles into 4 classes based on their topics, which are world news, sports news, business news, and science/tech news.
2.2 Out-of-Distribution Datasets
The statistical difference between ID and OOD datasets may include the lengths of the text, the styles, and the topics. Whether a dataset is out-of-distribution to another dataset depends on how we deﬁne in-distribution and out-of-distribution. For example, SST-2 can be seen as the OOD dataset of
2

IMDB, since their text lengths are very different. From another viewpoint, both SST-2 and IMDB involve movie reviews and are datasets for binary sentiment classiﬁcation; they can be considered as in-distribution with each other. In our work, we simply consider different datasets to be OOD datasets, as in Hendrycks and Gimpel [3]. In this case, SST-2 and AG-News are both OOD datasets for IMDB; but their degree of out-of-distribution is different. Each of the three datasets in Section 2.1 serves as an OOD dataset for others.
Yelp Polarity[25] We also include Yelp Polarity as another OOD dataset for each ID dataset in Section 2.1. Yelp-Polarity is a dataset for binary sentiment classiﬁcation which is composed of restaurant reviews.
2.3 Adversarial Attack Methods
We follow Yoo et al. [24] and use four synonym substitution attack methods: Textfooler [5], Probability Weighted Word Saliency (Ren et al. 17, PWWS), BERT-based Adversarial Examples (Garg and Ramakrishnan 2, BAE), and a variant of Textfooler called TF-adj [14]. All of the four methods generate Adv samples by iteratively substituting words in the benign text with their synonyms to ﬂip the prediction of the trained text classiﬁer. The previous three methods differ in how they determine which word to be substituted ﬁrst, and what word should be replaced by. Textfooler and TF-adj are mostly the same, while TF-adj ﬁlters legitimate ADV examples with stricter constraints.
We use the Adv samples provided by a recently proposed benchmark [24]. They generate Adv examples using the four attacks on the three ID datasets using different models by TextAttack library [15]. In this work, we only use the Adv examples generated by attacking the text classiﬁers ﬁne-tuned from BERT [1] and RoBERTa [11]. Each ID dataset will have four adversarial datasets that are obtained by attacking some ID samples using the four attacks introduced previously.
More details on the datasets and adversarial attacks can be found in Appendix B.
2.4 Experiment Setup
For an ID dataset in Section 2.1, there will be 3 kinds of OOD datasets (from Section 2.2) and 4 kinds of attacks (from Section 2.3), making a total of 12 different combinations of {OOD, Adv}. We use DID and DOOD to denote ID and OOD dataset. Speciﬁcally, the dataset used to train the text classiﬁer is denoted by DID,train, while those samples not used to train the classiﬁer form DID,test. DAdv is used to denote the Adv datasets obtained by attacking some ID samples using an adversarial attack method. To compare the samples from OOD and Adv datasets, we sample N instances from DID,test, N instances from DOOD, and N instances from DAdv. We then extract the three types of features (input features, hidden representations, and output probabilities) from the text classiﬁer trained on DID,train for analyses and experiments in Section 3 and Section 4. In all our experiments in Section 3 and Section 4, we set N to 500.1 We ﬁnd that our results are invariant to how we sample those instances from the dataset.
The instances in DAdv are not generated by attacking those instances in DID,test or DID,train None of the adversarial examples in DAdv have benign counterparts in DID,test or DID,train that only differ in some synonyms. The process of constructing DAdv and DID,test is detailed in Appendix B.2.
The text classiﬁcation models we use are ﬁne-tuned from BERT and RoBERTa provided by TextAttack [15] and available at Huggingface models. We use C to denote the number of classes of the text classiﬁer, for example, C = 2 for the model trained on SST-2. We only adopt text classiﬁers ﬁne-tuned from pre-trained transformer masked language models (MLMs) as their performance are exceptional and widely used in the current NLP community. They are also shown to be better detectors for OOD samples [4].
3 How Different OOD and Adv Samples?
1 Except SST-2 and AG-News attacked under TF-adj; the generated adversarial examples are far less 500 due to its low attack success rate. Thus, the results for these two datasets attacked by TF-adj may seem quite different to other datasets and attack methods. Refer to Table 2 for details.
3

To separate OOD and Adv samples, we ﬁrst try to understand whether there exists any differences between the two types of data from three aspects: input features, hidden representations, and the output probability distribution of the text classiﬁer. Additionally, we want to know how abnormal the samples in DOOD and DAdv are compared to the in-distribution samples; hence, we also include instances from DID,test for the analyses in this section. This is important since we are not just going to separate OOD and Adv samples, we are separating them from the ID samples; thus, we need to understand how they deviate from ID samples.

Count

Data type

200

ID Adv

OOD

150

100

50

0 0.00

0.01

0.02

0.03

0.04

0.05

Score

(a) DID,test: IMDB, DOOD: SST-2, DAdv:

PWWS.

60

Data type ID

50

Adv OOD

40

Count

30

3.1 Input Features

20

10

3.1.1 Method

0 0.005 0.010 0.015Score 0.020 0.025 0.030

The input features to the text classiﬁer are the words/tokens in the text. We are interested in knowing whether the words used in OOD and Adv samples are different or not. We extract the bag-ofword (BOW) feature of each sentence sampled from DID,test, DOOD, and DAdv dataset. The BOW feature is a vector having the dimension of the vocab-

(b) DID,test: AG-News, DOOD: IMDB, DAdv: BAE.

60

Data type ID

50

Adv OOD

40

Count

30

20

ulary size, and each entry is the TF-IDF of the vo- 10

cabulary. We use two different sets of vocabulary for tokenizing the sentences: the tokenizer of bert-baseuncased and roberta-base.

0

0.005 0.010 0.015 0.020 0.025

Score

(c) DID,test: SST-2, DOOD: AG-News, DAdv:

BAE.

We extract the BOW features using the following procedure. First, we sample 8N instances for each C classes of DID,train, and use them to calculate the inverse document frequency (IDF) of each token.

Figure 1: The cosine similarity score between DID,train for samples from DID,test, DOOD, and DAdv.

After obtaining the IDFs, we can calculate the BOW

features for any given sentence by counting the term frequency (TF) of each token in the sentence and weighted by the IDF of each token. Last, we normalize the BOW feature such that its l2-norm is 1.

As an indicator of how different samples drawn from DID,test, DOOD, or DAdv are, we compare their cosine similarity score with DID,train. For a sentence and its BOW feature, we deﬁne its cosine similarity score with DID,train as the BOW feature’s cosine similarity with the mean of all BOW features of sentences in DID,train.

3.1.2 Results
We plot the distributions of the cosine similarity scores for different combinations of ID, OOD, and Adv datasets, and we select some representative results. We only show the results when the vocabulary is that of bert-base-uncased as using roberta-base’s vocabulary does not change our observations. For most combinations of DID,test, DOOD, and DAdv, the results look like Figure 1a. The cosine similarity scores of the instances from DOOD are much lower, compared with DID,test and DAdv. This is understandable since the samples from DOOD can have a very different vocabulary distribution from DID,train if the domains of ID and OOD are different enough. The cosine similarity scores of DAdv and DID,test behave more similarly, this may spring from the fact that DAdv are generated from sentences in the in-distribution dataset by swapping less than 25% of words with their synonyms, and the resulting adversarial examples still have similar BOW features with the in-distribution dataset.
But the previous observations do not always hold: In Figure 1b, while the distribution of DAdv and DID,test is still very alike, we observe that the BOW features of DOOD can be even closer with DID,train compared with DAdv and DID,test. We also ﬁnd that, in certain combinations of ID and

4

OOD datasets, the results will be like Figure 1c: the distribution of the cosine similarity scores of samples from DID,test, DOOD and DAdv are very similar and indistinguishable.
Based on the above results, while the cosine similarity scores of samples from ID, OOD, and Adv datasets can sometimes be very different, using input features to separate ID, OOD, and Adv samples is undoubtedly a bad idea. This is because we are not able to separate ID and Adv samples simply based on a sentence’s BOW feature for all cases in Figure 1. And if we set a threshold of the similarity score, we are not able to ensure whether samples higher than the threshold or lower than the threshold is from DOOD, as in Figure 1a,1b.

3.2 Hidden Representations

3.2.1 Method

Next, we aim to understand whether the hidden representations extracted by the model show any differences when the input is from DOOD and DAdv. Given the ith sentence xi from a dataset, each transformer layer l in the text classiﬁer will calculate the hidden representations when forwarding xi through the model. We are interested in how the hidden representations vary from layer to layer, and whether different types of samples will behave differently.
To that end, we ﬁt a maximum likelihood estimator (MLE) for each layer’s hidden representations using DID,train. We sample 8N C samples from DID,train, each class with 8N samples. The hidden representations hli for each xi in DID,train through all layers l ∈ [1, 2, · · · , 12] are extracted. Layer 1 is the ﬁrst transformer layer and layer 12 is the last layer, which is the closest layer to the output. hli is a tensor of the shape [Ti, d], where Ti is the number of tokens in xi including CLS and SEP tokens, and d is the dimension of a hidden representation for a single token. To form the hidden representation for ﬁtting the MLE, we aggregate the hidden features of a sentence in a single layer by either 1) taking the hidden representation of the CLS token, denoted as hli,CLS, or 2) averaging hli along the sequence length Ti, denoted as hli,Avg. We will use h¯li to refer to either hli,CLS or hli,Avg later on. We use h¯li to ﬁt a C class-conditional Gaussian distributions with a tied covariance Σl. The class mean µlc of class c for the multivariate Gaussian and covariance Σl are estimated by:
µˆlc = 81N h¯li, (1)
i:yi =c
Σˆ l = 8N1C h¯li − µˆlc h¯li − µˆlc , (2)
c i:yi=c
where yi ∈ [1, · · · , C] is the label to xi.
After having the MLE parameters ﬁtted, we can assign a score Sl(xi) to describe how close a testing sample xi is to DID,train in terms of the hidden features at the lth layer by using the largest log-likelihood of the class-conditional Gaussian distribution, i.e.,

Sl(xi)

=

−1

max

− log

1

+ h¯l − µˆl (Σˆ l)−1 h¯l − µˆl , (3)

c2

(2π)d|Σˆ l|

i

c

i

c

where h¯li is the hidden representation for a testing sample xi at the l-th layer.

3.2.2 Results

We show the distribution of the log likelihood scores calculated using Equation 3 for samples from DID,test, DOOD, DAdv across different layers in Figure 2; the aggregation method used is CLS. We immediately observe an interesting trend that is very different between DOOD, DAdv. The Sl(xi) distribution of OOD samples is distributed more leftward from ID samples in layer 1 and layer 10, indicating that through those layers, OOD samples are utterly different from ID samples. As shown in Figure 2c, the hidden representations of OOD samples become much more similar with ID samples in terms of the distribution of the scores, compared with the OOD features in the shallow layers.
Unlike OOD samples, Adv samples’ hidden representations much resemble those of ID samples in the ﬁrst layer. The abnormality of Adv samples only start to expose deep down in the 10th layer, while the score distribution of hidden representations of ID and Adv samples are still highly overlapped.

5

It was not until the last two layers2 that Adv samples can be better distinguished from ID samples. Instead of gradually deviating from ID samples from the ﬁrst

350 Data type

300

ID

Adv

250

OOD

Count

layer, the anomaly of the hidden representations from 200

Adv samples reveals itself abruptly in the deeper lay- 150

ers of the network. While we only show a speciﬁc 100

combination of DID,test, DOOD, DAdv, we ﬁnd that 50

the above observations are very general and hold in

040000

30000

20000

10000

Score

0

all combinations of DID,test, DOOD, DAdv we use.

(a) Layer 1

We leave the results for other datasets and adversaries combinations in Appendix D. We also ﬁnd that the above phenomenons can be observed when the hidden representations are hli,CLS or hli,Avg, and they are more pronounced when we use hli,CLS.
As another way to illustrate how differently the hid-

Count

160 Data type

140

ID

120

Adv OOD

100

80

60

40

20

den representations from OOD and Adv samples evolve across layers, we design the following experiment. We evaluate how difﬁcult it is to separate OOD samples from ID samples using the hidden representations of each layer with a threshold-based detector, and we compare it to how difﬁcult it is to separate Adv samples from ID samples with another detector. A threshold-based detector measures the conﬁdence

Count

0 5000 4000 3000 2000 1000 0 1000 Score
(b) Layer 10

Data type

250

ID Adv

200

OOD

150

100

score of a given sentence, and assigns it as positive if 50

the conﬁdence score is higher than the threshold. In this subsection, the conﬁdence score is Sl(xi). Note

0 6000 5000 4000 3000 2000 1000 0 1000 Score

that in this setting, a detector will be presented with

(c) Layer 12

the ID data and only one type of anomaly, either statistical or adversarial, and its job is to determine whether the input is abnormal or not based on the score. We assign the ID samples as positive (having label 1) and abnormal ones as negative (having label 0). If the detector can distinguish the abnormal ones

Figure 2: The score Sl(xi) of DID,test,
DOOD, and DAdv, the hidden representa-
tions among a sentence is aggregated with hli,CLS . DID,test: IMDB, DOOD: SST-2, DAdv: TextFooler.

from the ID ones, that indicates the two types of sam-

ples are distributed far enough for the detector to make the decision. We report the area under the

receiver operating characteristic curve (AUROC) as the performance indicator of the detector. A

perfect detector will have an AUROC of 1 and a random detector will have an AUROC of 0.5.

We demonstrate a canonical result when DID,test is IMDB in Figure 3. First, from the left-hand side, we can easily observe that all the hidden features of three different DOOD can be easily separated from the features of DID,test in early layers. As the layer gets deeper, the OOD features may get more similar with the ID samples’ features, such as SST-2 or Yelp Polarity in Figure 3, or they can remain dissimilar from ID samples’ features, which is the case for AG-News in Figure 3.
We then turn our attention to the detection results for separating Adv from ID in the right-hand side in Figure 3. We see that for all adversarial attacks used for generating the Adv samples, the detection performance is miserable if we use the features from the early layers of the text classiﬁer. However, the situation twists in the later layers of the network, with a burst in AUROC starting in layer 10 and lasting until the last layer for most Adv samples.

AUROC AUROC

1.0

1.0 OOD/Adv

BAE

0.9

0.9

TextFooler TF-adj

0.8

0.8

PWWS

0.7

0.7

0.6

OOD/Adv SST-2

0.6

AG News

0.5

Yelp Polarity

0.5

2 4 L6ayer 8 10 12

2 4 L6ayer 8 10 12

Figure 3: Left: The detection results to sep-
arate DOOD from DID,test. Right: The detection results to separate DAdv from DID,test. DID,test: IMDB, model: BERT ﬁne-tuned on IMDB, aggregation method: hli,CLS .

2We only show the last layer in Figure 2

6

Again, despite that we only show the results of when DID,test is IMDB in Figure 3, we ﬁnd all the above observations hold for the other two ID datasets: OOD samples can be easily distinguished from ID by their hidden representations starting from the ﬁrst layer, and they may stay equally identiﬁable as they get deeper or become more similar with ID samples’ hidden representations. We empirically ﬁnd that the features from layer 2 are the best for separating DID,test and DOOD for all combinations of ID and OOD datasets; the worst-case AUROC among all the tested combinations is around 0.85. Contrarily, Adv samples are so similar to ID samples in terms of the score distribution of their features such that in shallow layers of the network, the two of them are inseparable. The features of Adv samples only seem out-of-place within the features of ID samples after the 10th layer. The features from the last layer are usually the best for separating ID and Adv samples. The AUROC for different combinations of DID,test and DAdv may differ, with TextFooler and PWWS mostly the easiest to separate and having an AUROC of at least 0.8, Adv samples generated by BAE is harder to detect and has an AUROC at least 0.7.
3.3 Output Probability Distribution
3.3.1 Method
In this section, we aim to understand whether the output probability distribution of the text classiﬁer looks different when the input is OOD or Adv samples. The output probability distribution of a text classiﬁer is a C-dimensional vector with each entry representing the probability score of a class. It was previously shown that models tend to be over-conﬁdent on ID samples, so the maximum probability might be a good indicator for whether a sample is ID or OOD [3]. However, it is unclear how conﬁdent the text classiﬁer is when faced with Adv samples and whether this conﬁdence can be an indicator of Adv samples.
To compare the output probability distribution of the models for DID,test, DOOD, and DAdv, we sample N samples from each of the three datasets. We extract the output probability distribution of a sample by taking the softmax of the logit distribution from the model’s output to form a probability distribution over C classes, and we use the maximum probability among the classes as an indicator of how conﬁdent the model is toward the input sample. We do not use temperature scaling [10] when we calculate the probability distribution from the softmax distribution.
3.3.2 Results
Overall, we ﬁnd that there are two different patterns for the distribution of the maximum probability that mainly differ in how the maximum probabilities of samples from DOOD distribute, as displayed in Figure 4a and 4b. For samples from DID,test, the models are always conﬁdent about them, and the maximum probabilities are mostly around 1. Contrarily, the models are consistently unsure about their prediction when the input is from DAdv, and the maximum probabilities scatter from 1/C to 1. The maximum probability distribution for samples from OOD datasets is rather conﬂicting for different combinations of DID,test and DOOD. The model can be quite conﬁdent on samples from OOD as in Figure 4b, but it can also be far less assured as in Figure 4a.
The above interesting observations from Figure 4 give the following message: The maximum probability can be lousy at determining whether an input is OOD or not, but it sure has the potential to be used in detecting Adv samples.
4 Separating OOD and Adv Samples
A text classiﬁcation model needs to be able to distinguish between OOD and Adv samples. When a text classiﬁer sees an OOD sample, the model cannot handle the input since it is something it has not been trained on. The model should not predict an OOD sample since the prediction may not even make sense. On the other hand, when the model is input with an ADV sample, the input can be converted into its benign counterpart for prediction. Knowing that the current input is an adversarial one also helps the model developer to know that some malicious users may be attacking the model, and measures need to be undertaken to secure the model. In this section, we focus on designing a method to separate OOD and Adv samples.
7

Count Count

500 Data type

ID

400

Adv

OOD

300

200

100

0 0.5 0.6 0.7 Score 0.8 0.9 1.0
(a) DID,test: IMDB, DOOD: AG-News, DAdv: BAE

350 Data type

300

ID

Adv

250

OOD

200

150

100

50

0 0.3 0.4

(b) DID,test: BAE

0.5 0.6 Score0.7 0.8
AG-News, DOOD:

0.9
SST-2,

1.0
DAdv :

Figure 4: The maximum probabilities of the text classiﬁcation models ﬁne-tuned from BERT for different combinations of DID,test, DOOD, and DAdv.

Given an input sentence, we want to tell which kind of anomaly it is if it is indeed an anomaly. Thus, the proposed method needs to be able to separate ID, OOD, and Adv samples. Existing works on OOD detections [23, 4, 22] do not discuss how their proposed method behaves when the input is Adv sample, and previous works on Adv detection [24, 16] do not test their methods on OOD samples. Lee et al. [6] proposed a general framework to detect both OOD and ADV examples, but they do not discuss how to separate the two types of anomaly.
The scenario in this section is as follows: We have a trained text classiﬁer and the corresponding DID,train, and we want to separate OOD, Adv, and ID samples using some threshold-based detectors, instead of training a classiﬁer to distinguish different types of samples [26]. The threshold of the detectors can be determined with or without the knowledge of the OOD and Adv datasets, detailed in Appendix E.2.3.
4.1 Method
We propose to detect whether a given input is from DID,test, DOOD or DAdv by the following two-stage pipeline: In the ﬁrst stage, we use the hidden representations from the earlier layers to build a threshold-based OOD detector using Sl(xi), whose goal is to separate OOD samples from ID and Adv samples. If the detector thinks that input is not an OOD sample, then we proceed to the second stage. In the second stage, we use the maximum output probability of the model to build another threshold-based detector that aims to distinguish Adv samples from ID samples.
Using hidden features from shallow layers to isolate OOD samples in stage 1 is feasible based on the following observation in Section 3.2: OOD samples deviate from ID samples in the earlier layers, while Adv ones highly resemble ID ones in the earlier layers and only start to deviate from ID samples in the deeper layers. The detector can thus separate those hidden features into two groups, one that does not resemble ID samples and one that resemble ID samples (including ID and Adv ones). Using the maximum probability to separate Adv samples from ID samples in stage 2 should work as we observed that models tend to be unconﬁdent when the inputs are Adv samples, as shown in Section 3.3. While model can also be unconﬁdent about some OOD samples, they will not cause any problems in stage 2 as they should be detected in stage 1 and not proceeding to stage 2.
Our proposed framework is simple and does not require additional models for detection; we only need the class-conditional Gaussian’s parameters and the threshold for the two detectors.
4.2 Experiment
In the experiment in this section, we sample the instances as stated in Section 2.4. In stage 1, we assign the OOD samples as negative, and we assign both ID samples and Adv samples as positive. This is different from our experiment in Section 3.2, in which the detectors will only be presented with a single type of anomaly and the ID samples. We use the score S2(xi), the score calculated based on the hidden representations from the second layer, to build the detector in stage 1.
The results are shown in Table 1a. We see that under all combinations of ID, OOD, and Adv datasets, we can build a detector that can almost perfectly separate OOD samples from ID and Adv samples. While in Table 1a, we build a detector for each combination of {DID,test, DOOD, DAdv},
8

Table 1: 1a:The AUROC of the detectors for separating OOD samples from ID and Adv samples.

The AUROC for an entry is the averaged AUROC for a speciﬁc combination of {DID,test, DOOD}

when varying DAdv. The variance across different DAdv is small and not shown. 1b: The AUROCs

of the detectors for separating Adv samples from ID ones.

ID OOD

IMDB SST-2 AG-News

ID IMDB SST-2 AG-News Adv

IMDB

-

0.99

0.93

SST-2

1.00

-

0.86

AG-News 1.00 0.98

-

Yelp Polarity 0.96 0.97

0.87

TextFooler 0.98 0.90

0.97

PWWS 0.98 0.88

0.94

BAE

0.97 0.75

0.89

TF-adj

0.82 0.85

0.88

(a)

(b)

it is possible to use only one detector to separate all different DOOD from a ﬁxed DID,test and all different DAdv. This can be done easily by selecting the highest threshold among the thresholds of the detectors of different OOD detectors for a ﬁxed DID,test (more precisely explained in Appendix F.3).
Having the OOD samples separated in stage 1, the detector in stage 2 only needs to focus on dividing ID and Adv samples. We use another threshold-based detector, which makes the decision based on the maximum probability discussed in Section 3.3. The detector should assign the ID samples as positive and the Adv samples as negative. In our experiments, we assume that the detector in stage 1 is ideal such that all OOD samples will be detected in stage 1 and not presented to the detector in stage 2. This is a reasonable assumption, given that the AUROCs of the detectors in stage 1 are very high.
The detection results for stage 2 is presented in Table 1b. We see that the detectors in stage 2 give a very decent performance on all combinations of ID and Adv datasets. Note that the setting in stage 2 is the same as in Yoo et al. [24], where they aim to detect Adv samples among ID samples; our results in Table 1b show that detection based on the maximum probability is a simpler yet equally powerful detection method. While we build different detectors for different Adv datasets for a ﬁxed ID dataset, it is also possible to build a single detector for all possible Adv datasets. This is because we already know the maximum probability of Adv samples is lower for all kinds of Adv datasets.
Combining the detectors in stage 1 and stage 2, we can separate ID, OOD, and Adv samples. We leave the experiment setup and results of cascading stage 1 and 2 in Appendix E.2. Simple as our method is, it shows non-trivial results on separating ID, OOD, and Adv samples, an important task that has been overlooked by the NLP community.
5 Discussion and Conclusion
In this work, we analyze the differences between two types of anomalies, OOD and Adv samples benchmarked against ID samples. We conduct comprehensive analyses to characterize OOD and Adv samples from three different aspects: the input features, the hidden representations across different layers of the model, and the output probability distributions of the model. We show that the similarity score based on BOW features is unstable among different combinations of ID/OOD/Adv data, which refrains us from utilizing them for separating OOD samples and Adv samples. From the aspect of hidden representations, we observe that OOD samples are very different from ID samples starting from the ﬁrst layer of the model. On the contrary, Adv samples seem just like the ID ones for the ﬁrst nine layers, and their abnormality only becomes evident in the last three layers. We also ﬁnd that the models tend to be less conﬁdent, in terms of the maximum probability, for Adv samples. Based on our analysis, we propose an original, simple yet effective two-staged detection method to separate the ID, OOD, and Adv samples. We show the superiority of our proposed method by abundant experiments over various combinations of ID, OOD, and Adv datasets. The analyses in our work elevate our knowledge of anomalies that bring down the model, and we believe our proposed method will help build more robust text classiﬁcation models.
When deploying a deep learning model in real-world, it is important for it to act properly when faced with anomaly examples, and the ﬁrst step to achieve this goal is to correctly identify it as an anomaly. Moreover, knowing what kind of anomaly, including Adv and OOD samples, is presented to the
9

model will make the model to process the anomalous data in a more robust way. While we only focus on separating the Adv and OOD samples in text classiﬁcation, we believe that this is an important topic for developing more trust-worthy machine learning models in all domains, including computer vision, speech processing, and other non-classiﬁcation tasks in NLP. As a pioneering work in this topic, we believe our thorough analysis and exhaustive experiments shed lights on this topic.
10

References
[1] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pretraining of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171– 4186, Minneapolis, Minnesota. Association for Computational Linguistics.
[2] Siddhant Garg and Goutham Ramakrishnan. 2020. BAE: BERT-based adversarial examples for text classiﬁcation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6174–6181, Online. Association for Computational Linguistics.
[3] Dan Hendrycks and Kevin Gimpel. 2017. A baseline for detecting misclassiﬁed and out-ofdistribution examples in neural networks. Proceedings of International Conference on Learning Representations.
[4] Dan Hendrycks, Xiaoyuan Liu, Eric Wallace, Adam Dziedzic, Rishabh Krishnan, and Dawn Song. 2020. Pretrained transformers improve out-of-distribution robustness. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2744–2751, Online. Association for Computational Linguistics.
[5] Di Jin, Zhijing Jin, Joey Tianyi Zhou, and Peter Szolovits. 2020. Is bert really robust? a strong baseline for natural language attack on text classiﬁcation and entailment. In Proceedings of the AAAI conference on artiﬁcial intelligence, volume 34, pages 8018–8025.
[6] Kimin Lee, Kibok Lee, Honglak Lee, and Jinwoo Shin. 2018. A simple uniﬁed framework for detecting out-of-distribution samples and adversarial attacks. Advances in neural information processing systems, 31.
[7] Quentin Lhoest, Albert Villanova del Moral, Patrick von Platen, Thomas Wolf, Mario Šaško, Yacine Jernite, Abhishek Thakur, Lewis Tunstall, Suraj Patil, Mariama Drame, Julien Chaumond, Julien Plu, Joe Davison, Simon Brandeis, Victor Sanh, Teven Le Scao, Kevin Canwen Xu, Nicolas Patry, Steven Liu, Angelina McMillan-Major, Philipp Schmid, Sylvain Gugger, Nathan Raw, Sylvain Lesage, Anton Lozhkov, Matthew Carrigan, Théo Matussière, Leandro von Werra, Lysandre Debut, Stas Bekman, and Clément Delangue. 2021. huggingface/datasets: 1.15.1.
[8] Quentin Lhoest, Albert Villanova del Moral, Yacine Jernite, Abhishek Thakur, Patrick von Platen, Suraj Patil, Julien Chaumond, Mariama Drame, Julien Plu, Lewis Tunstall, Joe Davison, Mario Šaško, Gunjan Chhablani, Bhavitvya Malik, Simon Brandeis, Teven Le Scao, Victor Sanh, Canwen Xu, Nicolas Patry, Angelina McMillan-Major, Philipp Schmid, Sylvain Gugger, Clément Delangue, Théo Matussière, Lysandre Debut, Stas Bekman, Pierric Cistac, Thibault Goehringer, Victor Mustar, François Lagunas, Alexander Rush, and Thomas Wolf. 2021. Datasets: A community library for natural language processing. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 175–184, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.
[9] Linyang Li, Ruotian Ma, Qipeng Guo, Xiangyang Xue, and Xipeng Qiu. 2020. BERT-ATTACK: Adversarial attack against BERT using BERT. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6193–6202, Online. Association for Computational Linguistics.
[10] Shiyu Liang, Yixuan Li, and R Srikant. 2018. Enhancing the reliability of out-of-distribution image detection in neural networks. In 6th International Conference on Learning Representations, ICLR 2018.
[11] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692.
[12] Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. 2011. Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 142–150, Portland, Oregon, USA. Association for Computational Linguistics.
11

[13] Rishabh Maheshwary, Saket Maheshwary, and Vikram Pudi. 2021. Generating natural language attacks in a hard label black box setting. In Proceedings of the 35th AAAI Conference on Artiﬁcial Intelligence.
[14] John Morris, Eli Liﬂand, Jack Lanchantin, Yangfeng Ji, and Yanjun Qi. 2020. Reevaluating adversarial examples in natural language. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3829–3839, Online. Association for Computational Linguistics.
[15] John Morris, Eli Liﬂand, Jin Yong Yoo, Jake Grigsby, Di Jin, and Yanjun Qi. 2020. Textattack: A framework for adversarial attacks, data augmentation, and adversarial training in nlp. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 119–126.
[16] Danish Pruthi, Bhuwan Dhingra, and Zachary C. Lipton. 2019. Combating adversarial misspellings with robust word recognition. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5582–5591, Florence, Italy. Association for Computational Linguistics.
[17] Shuhuai Ren, Yihe Deng, Kun He, and Wanxiang Che. 2019. Generating natural language adversarial examples through probability weighted word saliency. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1085–1097, Florence, Italy. Association for Computational Linguistics.
[18] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1631–1642, Seattle, Washington, USA. Association for Computational Linguistics.
[19] Yuxuan Wang, Wanxiang Che, Ivan Titov, Shay B. Cohen, Zhilin Lei, and Ting Liu. 2021. A closer look into the robustness of neural dependency parsers using better adversarial examples. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 2344–2354, Online. Association for Computational Linguistics.
[20] Zhaoyang Wang and Hongtao Wang. 2020. Defense of word-level adversarial attacks via random substitution encoding. In International Conference on Knowledge Science, Engineering and Management, pages 312–324. Springer.
[21] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. 2020. Transformers: Stateof-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 38–45, Online. Association for Computational Linguistics.
[22] Hong Xu, Keqing He, Yuanmeng Yan, Sihong Liu, Zijun Liu, and Weiran Xu. 2020. A deep generative distance-based classiﬁer for out-of-domain detection with mahalanobis space. In Proceedings of the 28th International Conference on Computational Linguistics, pages 1452–1460, Barcelona, Spain (Online). International Committee on Computational Linguistics.
[23] Keyang Xu, Tongzheng Ren, Shikun Zhang, Yihao Feng, and Caiming Xiong. 2021. Unsupervised out-of-domain detection via pre-trained transformers. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 1052–1061, Online. Association for Computational Linguistics.
[24] KiYoon Yoo, Jangho Kim, Jiho Jang, and Nojun Kwak. 2022. Detection of word adversarial examples in text classiﬁcation: Benchmark and baseline via robust density estimation. arXiv preprint arXiv:2203.01677.
[25] Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015. Character-level convolutional networks for text classiﬁcation. Advances in neural information processing systems, 28:649–657.
[26] Yichao Zhou, Jyun-Yu Jiang, Kai-Wei Chang, and Wei Wang. 2019. Learning to discriminate perturbations for blocking adversarial attacks in text classiﬁcation. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International
12

Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 4904–4913, Hong Kong, China. Association for Computational Linguistics.
Checklist
The checklist follows the references. Please read the checklist guidelines carefully for information on how to answer these questions. For each question, change the default [TODO] to [Yes] , [No] , or [N/A] . You are strongly encouraged to include a justiﬁcation to your answer, either by referencing the appropriate section of your paper or providing a brief inline description. For example:
• Did you include the license to the code and datasets? [Yes] See Section ??. • Did you include the license to the code and datasets? [No] The code and the data are
proprietary. • Did you include the license to the code and datasets? [N/A]
Please do not modify the questions and only use the provided macros for your answers. Note that the Checklist section does not count towards the page limit. In your paper, please delete this instructions block and only keep the Checklist section heading above along with the questions/answers below.
1. For all authors... (a) Do the main claims made in the abstract and introduction accurately reﬂect the paper’s contributions and scope? [Yes] See Abstract and 1. (b) Did you describe the limitations of your work? [Yes] In Section5. (c) Did you discuss any potential negative societal impacts of your work? [Yes] See Appendix A. (d) Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes] See Appendix A.
2. If you are including theoretical results... (a) Did you state the full set of assumptions of all theoretical results? [N/A] (b) Did you include complete proofs of all theoretical results? [N/A]
3. If you ran experiments... (a) Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes] The code is submitted with the paper and will be made publicly available once the paper is accepted. (b) Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes] See Appendix B. (c) Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [Yes] We provide some error bars as in Figure 3. As we stated in Section 2.4, we do not report the variations among different runs since their variance is small. (d) Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] See Appendix C.2.
4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... (a) If your work uses existing assets, did you cite the creators? [Yes] See Section 2.1 and Section 2.2. (b) Did you mention the license of the assets? [No] We are not able to ﬁnd the licenses of the assets. (c) Did you include any new assets either in the supplemental material or as a URL? [N/A]
(d) Did you discuss whether and how consent was obtained from people whose data you’re using/curating? [No] We cannot ﬁnd the license of the datasets, but we believe that their releasing of the datasets is for the community to use them.
13

(e) Did you discuss whether the data you are using/curating contains personally identiﬁable information or offensive content? [Yes] See Appendix B.
5. If you used crowdsourcing or conducted research with human subjects... (a) Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A] (b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A] (c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A]
A Broader Impact and Ethical Impact
Our work aims toward building more robust text classiﬁers. We believe our work has positive contribution on making AI system more trust-worthy. Comparing OOD and Adv samples in order to build more robust models is not only important for text classiﬁcation models, but also necessary for all deep learning models in natural language processing, computer vision, and speech processing. We highlight the importance of being able to separate OOD and Adv samples for a model, which is a critical research problem that has yet to be emphasized. It is also a real problem a model will need to tackle when it is deployed in our daily life. We open a new and crucial direction for improving model robustness in the AI community.
Our paper is motivated by making the text classiﬁcation models more robust. We cannot think of any possibility that our work may be mis-used by any vicious user with bad intention. However, if a malicious user knows that the model can detect Adv samples, he or she may try to craft stronger adversaries to bypass the detection method; this is the only possible risk we can think of. We believe that we do not violate the Ethic Guidelines in NeurIPS 2022.
B Datasets and Adversarial Attacks
B.1 Datasets
For all the ID and OOD datasets in Section 2.1 and Section 2.2, we load them using the Huggingface Datasets library [7, 8]. In our experiments, DID,train is the training split of the ID dataset, DOOD is the testing split of OOD the dataset. How DID,test and DAdv are obtained will be explained in the next subsection.
Datasets we use are mostly for sentiment analysis, which means they may contain subjective opinions that may be offensive to some individuals. However, we leave them as is because removing those possibly offensive languages may make the sentiment analysis task unable to be conducted. Also, removing speciﬁc kinds of words from the dataset during testing may cause the distribution to be slightly different with the distribution that the text classiﬁer is trained on. Given that our goal includes separating ID and OOD samples, we do not want to create any possible mismatch in the ID training data and ID testing data.
B.2 Adversarial Samples
We download the Adv from here provided by Yoo et al. [24]. They provide sentences resulting from attacking the text classiﬁers using TextAttack [15], including successful and failed adversaries, and the attack is done on the testing or development split of the ID dataset. For an attack result for a speciﬁc text classiﬁer using a speciﬁc adversarial attack, we ﬁrst select those instances that successfully fool the text classiﬁer. For those successful adversaries, we split them into two groups with an equal number of samples, and one of them will be used as the DAdv. For the other group of successful adversaries, we take their benign counterparts to form the DID,test. In case when DID,test contains too few samples, we will add the benign counterparts in the failed attacks to enlarge the size of DID,test. This way, we can ensure that DID,test and DAdv will not contain pairs of samples that are benign/adversarial to each other.
For input preprocessing, we remove the ’<br>’, ’<br />’, and ’//’ in some datasets to prevent the model from distinguishing ID and OOD samples based on those artifacts.
14

C Implementation Details

C.1 Hidden Features

In Equation 3, there is a (2π)d term in the ﬁrst summand. In our implementation, we omit that (2π)d term as it is a constant when d is ﬁxed, which is 768. Thus, the score Sl(xi) is in fact calculated by the following formula:

Sl(xi)

=

−1 max

1 − log

+ hl − µˆl (Σˆ l)−1 hl − µˆl . (4)

c2

|Σˆ l|

i

c

i

c

The Sl(xi) show in Figure 2 and Figure 5 is calculated using Equation 4.

C.2 Run Time and Infrastructure
Our codes are implemented with Pytorch, and we use Huggingface Transformers [21] and datasets [8] to load ﬁne-tuned text classiﬁcation models and datasets. Running the full set of our experiments takes less than 24 hours with a single Tesla V100. All of the text classiﬁcation models have around 110M parameters.

D Supplementary Figures for Section 3.2
In Figure 5, we provide an illustration of how the score distributions based on the hidden feature in Section 3.2 evolve over the layers for a different combination of ID, OOD, Adv datasets. In Figure 5, the text classiﬁer is ﬁne-tuned from RoBERTa; this shows that our results are general across different pre-trained models.
Figure 6 is the supplementary ﬁgure for Figure 3. From the left-hand side of all ﬁgures, we can ﬁnd that all OOD datasets illustrate similar behaviors for all models: It is generally harder to differentiate between OOD and ID in the earlier layers of the model. When the layers get deeper, some OOD datasets become inseparable with ID datasets while some can still be easily separated from by the features. The behavior of Adv datasets, on the right-hand side, is also similar to that we presented in Figure 3: Adv datasets are similar with ID samples in the shallower layers, and they grow more different with ID samples abruptly in the deeper layers. While the detection results for TF-adj is poor for some datasets, we remind the readers that this is mainly due to the size of DAdv are much smaller, as noted in Footnote 1. Refer to Table 2 for the N for different DAdv.

Table 2: N for different DAdv. ID IMDB SST-2 AG-News
Adv

BERT

TextFooler 500 500

500

PWWS

500 500

500

BAE

500 500

500

TF-adj

500 40

172

RoBERTa

TextFooler 500 500

500

PWWS

500 500

500

BAE

500 500

500

TF-adj

409 30

183

E Supplementary Results for Section 4
E.1 Stage 1 and Stage 2 Results of RoBERTa In this section, we show the stage 1 and stage 2 separation results for RoBERTa. The stage 1 detection result is in Table 3, and the stage 2 detection result is in Table 4.
15

Count

200

Data type

175

ID Adv

150

OOD

125

100

75

50

25

0 3000 2000 1000 0 Score1000 2000 3000 4000

(a) Layer 2

Data type

200

ID

Adv

150 OOD

Count

100

50

Count

Count

0

1000

70

60

50

40

30

20

10

0

3000

120 100 80 60 40 20
0 2000 1500

250 200 150

0

Sco1re000

2000

(b) Layer 4

3000 Data type
ID Adv OOD

2000 100S0core 0
(c) Layer 6

1000 2000 Data type ID Adv OOD

1000 500 Scor0e 500
(d) Layer 8

1000 1500 Data type ID Adv OOD

100

50

0 5000 4000 3000 2000 1000 0 Score
(e) Layer 10

350 300 250 200 150 100 50
0 25000

20000 15000 Scor1e0000 5000
(f) Layer 12

1000 2000 Data type ID Adv OOD
0

Count

Count

Figure 5: The score Sl(xi) distribution, aggregation: hli,CLS. DID,test: AG-News, DOOD: IMDB, DAdv: PWWS, model: RoBERTa.

E.2 Experiments for Cascading Stage 1 and Stage 2 In this section, we explain how we cascade stage 1 and stage 2.
16

AUROC AUROC

1.0

1.0 OOD/Adv

BAE

0.9

0.9

TextFooler TF-adj

0.8

0.8

PWWS

0.7

0.7

0.6

OOD/Adv SST-2

0.6

IMDB

0.5

Yelp Polarity

0.5

2 4 L6ayer 8 10 12

2 4 L6ayer 8 10 12

(a) DID,test: AG-News, model: BERT

AUROC AUROC

1.0

1.0

OOD/Adv

BAE

0.9

0.9

TextFooler TF-adj

0.8

0.8

PWWS

0.7

0.7

0.6

OOD/Adv AG News

0.6

IMDB

0.5

Yelp Polarity

0.5

2 4 L6ayer 8 10 12

2 4 L6ayer 8 10 12

(b) DID,test: SST-2, model: BERT

AUROC AUROC

1.0

1.0 OOD/Adv

BAE

0.9

0.9

TextFooler TF-adj

0.8

0.8

PWWS

0.7

0.7

0.6

OOD/Adv SST-2

0.6

IMDB

0.5

Yelp Polarity

0.5

2 4 L6ayer 8 10 12

2 4 L6ayer 8 10 12

(c) DID,test: AG-News, model: RoBERTa

AUROC AUROC

1.0

1.0 OOD/Adv

BAE

0.9

0.9

TextFooler TF-adj

0.8

0.8

PWWS

0.7

0.7

0.6

OOD/Adv SST-2

0.6

AG News

0.5

Yelp Polarity

0.5

2 4 L6ayer 8 10 12

2 4 L6ayer 8 10 12

(d) DID,test: IMDB, model: RoBERTa

AUROC AUROC

1.0

1.0 OOD/Adv

BAE

0.9

0.9

TextFooler TF-adj

0.8

0.8

PWWS

0.7

0.7

0.6

OOD/Adv AG News

0.6

IMDB

0.5

Yelp Polarity

0.5

2 4 L6ayer 8 10 12

2 4 L6ayer 8 10 12

(e) DID,test: SST-2, model: RoBERTa

Figure 6: Left: The detection results to separate DOOD from DID,test. Right: The detection results to separate DAdv from DID,test. Aggregation method: hli,CLS.

E.2.1 Experiment Setup We have a trained text classiﬁer and the corresponding DID,train, and we want to separate OOD, Adv, and ID samples using some threshold-based detectors, instead of training a classiﬁer to distinguish
17

Table 3: The AUROC of detector for separating OOD samples from ID and Adv samples. The AUROC score for a entry is the averaged AUROC for a speciﬁc combination of {DID,test, DOOD} when we vary the DAdv. The variance across different DAdv is very small and thus not shown. The text classiﬁcation model is ﬁne-tuned from RoBERTa.
ID IMDB SST-2 AG-News OOD

IMDB

-

1.00

0.99

SST-2

1.00

-

0.92

AG-News 1.00 1.00

-

Yelp Polarity 0.95 1.00

0.95

Table 4: The AUROCs of the detectors for separating Adv samples from ID ones. The text classiﬁcation model is ﬁne-tuned from RoBERTa.
ID IMDB SST-2 AG-News Adv

TextFooler 0.99 0.91

0.93

PWWS

0.99 0.90

0.95

BAE

0.98 0.78

0.92

TF-adj

0.87 0.79

0.84

different types of samples. We also assume we have a in-distribution development set, DID,dev, for selecting the thresholds of the classiﬁer. We select the threshold of stage 1 using the following procedure: First, we calculate the score S2(xi), the score calculated based on the hidden representations from the second layer, of each samples in DID,dev. The from the set S1 = {S2(xi)|xi ∈ DID,dev}, we set the threshold t1 by the 5-th percentile of S1. That is, there will be 5% of instances in DID,dev whose S2(xi) is less than or equal to t1.
In stage 2, the threshold t2 is set in a similar way. We collect the maximum output probability of every instances in DID,dev to form the set S2 and set t2 to the 5-th percentile of S2. Note that in this setting, we do not require any knowledge of the Adv and OOD datasets. Also note that in our setting, for a single ID dataset, the t1 for all OOD datasets is the same, and the t2 for all kinds of adversarial attacks is the same. The performance can sure be improved if one have the knowledge of OOD and Adv datasets, and the 5-th percentile may not be the best way for selecting the threshold. Selecting the 5-th percentile as the threshold means that we think the 5% that are the least similar with ID’s development samples should be considered abnormal.
When testing the detectors, we sample 500 instances from DID,test, DOOD, and DAdv, except those datasets that are too small to sample 500 instances. Since separating these three types of datasets is just a 3-way classiﬁcation problem, we simply report the accuracy.
E.2.2 Results
We show the results in Table 5, 6, and 7, those results are based on text classiﬁers ﬁne-tuned from BERT. Is is obvious that the results of our proposed method perform far better than random guessing, which should give 33% accuracy in case when all three types of datasets have 500 instances.

Table 5: The accuracy for separating ID, Adv, and OOD samples, ID is IMDB. Yelp is short for Yelp Polarity.
OOD AG-News SST-2 Yelp Adv

BAE PWWS TextFooler TF-adj

0.890 0.909 0.914 0.722

0.890 0.909 0.914 0.722

0.784 0.808 0.831 0.634

18

Table 6: The accuracy for separating ID, Adv, and OOD samples, ID is SST-2. Yelp is short for Yelp Polarity.
OOD AG-News IMDB Yelp Adv

BAE PWWS TextFooler TF-adj

0.644 0.739 0.784 0.889

0.671 0.760 0.809 0.918

0.618 0.714 0.762 0.850

Table 7: The accuracy for separating ID, Adv, and OOD samples, ID is AG-News. Yelp is short for Yelp Polarity.
OOD IMDB SST-2 Yelp Adv

BAE PWWS TextFooler TF-adj

0.542 0.669 0.747 0.565

0.634 0.744 0.823 0.704

0.533 0.661 0.751 0.553

E.2.3 Setting the Thresholds in Stage 1 and Stage 2
With the Knowledge of OOD and Adv Datasets In this case, the thresholds can be easily selected. Just ﬁnd the optimal threshold for a pre-deﬁned precision/recall score by considering the scores the ID, OOD, and Adv samples.
Without the Knowledge of OOD and Adv Datasets In this case, the thresholds can be determined by following the procedures in Appendix E.2.1.
F FAQ
Q1 Why do you use the vocabulary of bert-base-uncased and roberta-base for the BOW features in Section 3.1?
A1 We choose these two vocabulary sets for better comparison with the experiments that utilize features extracted from the text classiﬁers ﬁne-tuned from BERT and RoBERTa, whose input are the sentences tokenized by the two tokenizers.
Q2 Have you tried other methods for comparing the BOW feature’s similarity with the ID ones?
A2 Given a sentence and its BOW feature, we also try to calculate the cosine similarity with each BOW feature in DID,train and take the maximum one. We ﬁnd the results using this method will mostly be inferior to the method we used in Section 3.1.
Q3 How to construct a single OOD detector for all kinds of OOD datasets for stage 1 in Section 4?
A3 Consider we want to separate OOD samples from ID samples and Adv samples, where the ID samples are from IMDB. Since all types of Adv look exactly like ID samples in earlier layers, we do not need to build detectors for all combinations of DOOD, DAdv for a ﬁxed DID,test; we just need to build detectors for different DOOD for a ﬁxed DID,test. We build the OOD detector to detect SST-2, the OOD detector for AG-News, and the OOD detector for Yelp Polarity. These three detectors differ in how they set the threshold of labeling the inputs as positive based on their score. What we need to do is to select the highest threshold among the previous three thresholds, and based on Table 1a, we might want to use the threshold of the detector for Yelp Polarity.
Q4 Where does the variance in the left-hand side of Figure 3 come from? A4 It is the variance of sampling different DID,test. In our implementations, for the same
ID dataset, the DID,test for different DAdv may consist different instances. Considering a benign testing sample in the testing/development set of ID dataset, different attacking
19

methods can successfully attack that instance while the attack may also fail. If the instance is successfully attacked, then its adversarial counterpart will be included in DAdv, and the original instance cannot be included in DID,test since we require DID,test and DAdv to consist no benign/adversarial pairs. On the other hand, if an instance was not successfully attacked, then it can be included in the DID,test. This makes different DAdv to have slightly different DID,test. Q5 In Section 4, why do the authors choose to use the maximum probability in Section 3.3 as an indicator of Adv samples, instead of using the score Sl(xi) based on the hidden representations in the deeper layers? Based on the results in Figure 3 and Figure 6, using S12(xi) can also separate Adv and ID samples. A5 While using S12(xi) can also separate Adv samples from ID ones, we ﬁnd that using the maximum probability from the output generally leads to better detection results. This can be observed from comparing Figure 3 and Figure 6 with Table 1b and Table 4. Also, when using S12(xi), we also need to ﬁt the features with a class-condition normal distribution, which requires additional parameters. Thus, we choose to use maximum probability due to its better performance and less parameters. Q6 Why do the authors only include results of BERT in the paper? A6 This is because the results for models ﬁne-tuned from RoBERTa are like the results obtained from the text classiﬁers ﬁne-tuned from BERT. Q7 Isn’t using the maximum probability to identify Adv samples unreasonable? Adv samples are made to make the model to be unconﬁdent about, so it is odd to use it as an indicator as Adv samples. A7 No, Adv samples are NOT designed to make the model unsure about their predictions. Adv samples are crafted such the model makes the wrong prediction; the model can be very conﬁdent but wrong, it can also be unconﬁdent and wrong. We show that the text classiﬁer is unconﬁdent and wrong on those Adv sample, and we use this as an indicator of Adv samples.
20

