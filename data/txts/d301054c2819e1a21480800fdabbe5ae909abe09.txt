Leveraging Language to Learn Program Abstractions and Search Heuristics

arXiv:2106.11053v3 [cs.LG] 3 May 2022

Catherine Wong 1 Kevin Ellis 2 Joshua B. Tenenbaum 1 3 Jacob Andreas 1

Abstract
Inductive program synthesis, or inferring programs from examples of desired behavior, offers a general paradigm for building interpretable, robust, and generalizable machine learning systems. Effective program synthesis depends on two key ingredients: a strong library of functions from which to build programs, and an efﬁcient search strategy for ﬁnding programs that solve a given task. We introduce LAPS (Language for Abstraction and Program Search), a technique for using natural language annotations to guide joint learning of libraries and neurally-guided search models for synthesis. When integrated into a state-of-theart library learning system (DreamCoder), LAPS produces higher-quality libraries and improves search efﬁciency and generalization on three domains – string editing, image composition, and abstract reasoning about scenes – even when no natural language hints are available at test time.
1. Introduction
Machine learning approaches based on program synthesis– the automatic inference of symbolic programs–can offer robustness, interpretability, veriﬁability, and strong generalization in few-shot learning settings (Appel et al., 2017; Lake et al., 2017). Many machine learning tasks can be formulated as program synthesis problems, including data manipulation (Delaware et al., 2015; Gulwani et al., 2017), semantic parsing (Artzi & Zettlemoyer, 2013; Liang, 2016), structured visual understanding (Johnson et al., 2017b; Yi et al., 2018), image generation (Ellis et al., 2017; Ganin et al., 2018), and policy learning (Fikes & Nilsson, 1971; Cropper & Muggleton, 2015; Silver et al., 2020). This paper introduces Language for Abstraction and Program Search (LAPS), a framework for improving the efﬁciency and generalizability of learned program synthesis
1MIT 2Cornell University 3Center for Brains, Minds and Machines (CBMM) - MIT. Correspondence to: Catherine Wong <catwong@mit.edu>. Proceedings of the 38 th International Conference on Machine Learning, PMLR 139, 2021. Copyright 2021 by the author(s).

models using natural language supervision. In LAPS, language guides learning of both libraries of reusable program abstractions and heuristics for searching in the space of programs. High-quality program libraries and search methods are the main ingredients of effective program synthesis approaches (Gulwani et al., 2017). Recent approaches to program synthesis have attempted to learn search models (Gulwani et al., 2015; Polozov & Gulwani, 2015; Balog et al., 2016; Devlin et al., 2017), program libraries, or both jointly from data (Shin et al., 2019; Dumancic´ & Cropper; Ellis et al., 2021; 2020; La´zaro-Gredilla et al., 2019), but even the current best learning approaches can be computationally inefﬁcient (often requiring upwards of thousands of CPU hours to bootstrap learning) and do not always discover generalizable libraries or search strategies. LAPS builds on the intuition that natural language offers a powerful source of information for tackling both learning problems. Language simultaneously provides an efﬁcient channel for communicating the structure of the search space (an instruction like draw a large hexagon next to a small pentagon decomposes a complex graphics task into highlevel parts) and a lexicon that names important reusable concepts in a given domain (for instance, suggesting that a function to draw variable-sized polygons might be useful for future graphics tasks). In this work we show how inducing jointly compositional generative models over natural language and programs provides a strong scaffold for library and search model learning in a hierarchical program induction model. When integrated into a state-of-the-art learning algorithm, DreamCoder (Ellis et al., 2021; 2018), our approach dramatically improves performance on three different synthesis domains: string editing, structured image generation and scene understanding. Compared to the base synthesis approach, LAPS solves and learns more quickly from synthesis tasks, and produces higher-quality libraries that improve generalization to downstream tasks without natural language hints. LAPS builds on several recent developments in (nonlanguage-based) program synthesis, so we begin with a review of related work (Sec. 2), then formalize the search and library learning problems (Sec. 3) and base synthesis algorithm (Sec. 4). We then describe how LAPS extends the base algorithm to include language in learning (Sec. 5) and conclude with empirical results (Sec. 6).

Leveraging Language to Learn Program Abstractions and Search Heuristics

A. Base learned synthesis algorithm (DreamCoder)

L

Library

....leverages compositional generativity of programs to learn

Training tasks with no ground truth programs

+ Iteratively learned library as a generative prior over programs

(i) Conditional neural search learned from program samples can struggle to generalize to hard training tasks

(ii) Abstractions learned from training programs may be overfit to training tasks

Program
Executed example

move_pen for unit_line
* learned_fn_0

forward sample programs

(for ∞ (move_pen (* unit_line 3) (/ 2π 6)))

learned executionconditioned inverse

abstract over discovered programs

(for ∞ (move_pen (* unit_line 3) (/ 2π 6)))
learned_fn_0 = (for ∞ (move_pen (* unit_line 3) (/ 2π x )))

add back to learned library

J

Joint librarylanguage model

(Program, language)

Executed example and large six gon annotation

B. Language for abstraction and program search (LAPS)

large six gon
four nested squares
a small nine gon next to a small square a five sided snowflake with a short line and a small seven gon as arms
Language-annotated training tasks

move_pen for unit_line *
large_fn gon_fn

“gon” “line” “large”

forward sample programs and language

(for ∞ (move (* unit_line 3) (/ 2π 6)))
“large six gon”

learned execution and language conditioned inverse

abstract jointly over programs and language

(for ∞ (move _pen (* unit_line 3) (/ 2π 6)))
large six gon
large_fn = (* unit_line 3) gon_fn = (for ∞ (move_pen
x (/ 2π y )))

add back to learned library

+ Iteratively learned jointly compositional generative models over program library and language

(i) Neural search learns from generated language-annotated programs to condition on language as a high-level training signal

(ii) Abstraction is structured over language to learn functions that compose like language

....leverages compositional generativity of language to learn programs

Figure 1. Our model, Language for Abstraction and Program Search (LAPS) integrates natural language into base learned synthesis algorithms formulated as hierarchical Bayesian inference (A, left) for jointly learning a library of program abstractions and a neural search heuristic for synthesis. We give an extended formulation (B, left) deﬁned jointly over the program library and natural language descriptions of synthesis tasks, that can be used to incorporate natural language into both abstraction and search heuristic learning. When incorporated into a concrete learning algorithm, DreamCoder (A, right) we show that LAPS allows the model to leverage language richly during training to improve the generalization of both the learned neural search model and the learned library of program abstractions.

2. Related Work
Our work draws on recent program synthesis approaches that learn to synthesize programs from examples using neural models to guide search (Gulwani et al., 2015; Balog et al., 2016; Parisotto et al., 2016; Devlin et al., 2017; Polosukhin & Skidanov, 2018; Abolaﬁa et al., 2018; Nye et al., 2019; Ellis et al., 2019; Si et al., 2019; Ye et al., 2020a); and learn libraries of symbolic abstractions from a collection of related programs or tasks (Dechter et al., 2013; Zhang et al., 2017; Shin et al., 2019; Dumancic´ & Cropper; Ellis et al., 2018; 2021). Our formulation builds on hierarchical Bayesian formulations of program learning that frame both synthesis and library learning as probabilistic inference (Liang et al., 2010; Lake et al., 2015; Ellis et al., 2021). Natural language has also been used to scaffold latent representation learning (Frome et al., 2013; Jia & Liang, 2016; Andreas et al., 2017; Ye et al., 2020b; Goyal et al., 2020; Liang et al., 2020; Mu et al., 2019; Luketina et al., 2019), and as a high-level speciﬁcation for program synthesis tasks (Ye et al., 2020a; Nye et al., 2019; Polosukhin & Skidanov, 2018; Ye et al., 2020b; Desai et al., 2016; Srivastava et al., 2017). Here we present an approach that integrates language annotations in training for learning a more generalizable library and program search model that can be used after training with no additional annotations for new tasks.

3. Inductive synthesis and library learning
Consider the problem of writing a graphics program to draw the large hexagon image in the left column of Fig. 1. This is an inductive program synthesis problem: a task t (like draw a large hexagon) is speciﬁed with examples of what a program should do, where each example is given as an input x (in this case, the blank image canvas) and the desired output y (the large hexagon image). A program ρ solves the task if it produces outputs that are consistent with the speciﬁcation when executed – that is, if evaluating ρ under an execution model E yields ρ E(x) = y. Program synthesis begins with a library L = {l0, ..ln} containing the set of primitives that can be combined to produce solution programs, such as the (pseudo-code) primitive functions in a simple graphics language:
L = move pen|unit line|for|*|π|∞|0|1|2|... which draw lines on a canvas parameterized by their length and angle. Given a library, there is also the problem of search: effective program synthesis requires a search strategy S that can be given a task speciﬁcation (such as the image of a hexagon) and automatically discover a solution program like the one shown in Fig. 1:
(for ∞(move pen(∗ unit line 3)(/ 2π 6)) by searching over programs built from functions in L.

Leveraging Language to Learn Program Abstractions and Search Heuristics

Both of these ingredients – the library L, and the search strategy S – can be made much more efﬁcient if the synthesis engine will be expected to solve multiple related problems. In the graphics domain, for example, synthesis of the various images depicted in Fig. 1 is much more easily accomplished using a library like
L = polygon|large line|small line...

in which the original hexagon task can be expressed as

polygon(6, large line)

A good library already provides a foundation for efﬁcient search by making solutions easier to express. Even with such a library, search can be further guided by information about the prior structure of programs (for example, the fact that polygon is typically called with a large line or small line function as a second argument) and by information about the target task itself (for example, the fact that the target image contains six line segments). Thus, one way to describe an effective search strategy S is via a prior over programs P[ρ|L] in the library and a conditional inference model for inferring P[ρ|t, L], the distribution over programs likely intended by the observed task examples t. The foregoing discussion lays out the basic ingredients of a hierarchical Bayesian formulation of program synthesis (used in learning algorithms like (Ellis et al., 2021; Lake et al., 2015; Dechter et al., 2013); see the graphical model in Fig. 1A, left) for jointly learning a library and conditional search model from a dataset T of synthesis tasks. We denote a prior over programs as P[ρ|L, θL], on a library L with parameters θL. Given the observed tasks, we deﬁne the likelihood of the latent library and parameters as:

Φ(L, θL) = P[L, θL]

P[t|ρ]P[ρ|L, θL] (1)

t∈T ρ

where P[L, θL] is a prior over all possible libraries and parameterizations, and P[t|ρ] is the likelihood that each inductive task t is consistent with a program ρ (for our purposes, P[t|ρ] = 1 if the program produces the desired output examples and 0 otherwise.) Learning in this model means estimating the optimal library and its parameters

L∗, θL∗ = arg max Φ(L, θL)

(2)

L,θL

describe a more complete integration) in our approach, but ﬁrst describe a concrete implementation of Eq. 2 on which we can realize the language-enriched model.

4. Base learning algorithm: DreamCoder
The LAPS framework we describe in this paper is a general one for extending Bayesian models of program learning like the one in Eq. 2 to incorporate information from language. For concreteness, however, our presentation and experiments build on the speciﬁc DreamCoder algorithm of Ellis et al. (2021), which we brieﬂy review here. We choose DreamCoder because it exposes a modular implementation of the library and search learning problems in Eq. 2 and has previously demonstrated state-of-the-art performance across a variety of synthesis domains (Ellis et al., 2021; 2020). DreamCoder is initialized with a base library L0 of starting primitives and a dataset of training tasks T . It returns a learned ﬁnal library Lf augmented with program abstractions and a learned neural search model Q(ρ|t, L) that predicts high probability programs conditioned on the task examples. Learning is iterative: DreamCoder alternately searches for solution programs to the training tasks (given a current library Li and search model Qi) and updates the library and search model based on new solved tasks. We give details on each component below.

4.1. Program prior DreamCoder deﬁnes the prior over programs as a probabilistic context free grammar (PFCG; Johnson 1998) for programs generated as productions from a library L of functions l ∈ L 1. Formally, DreamCoder assigns a real-valued weight θLi to each library function, which when normalized yields a production probability P[l|L, θL]. The prior probability of a program ρ is given by

P[ρ|L, θL] = P[l|L, θL]

(3)

l∈ρ

the weighted product of probabilities of all of its constituent library functions. As all P[l|L, θL] < 1, this is equivalent to a description length prior over programs: longer programs (with more constitutent elements) will have lower prior probability under Eq. 3 since P[l|L, θL] monotonically decreases as |ρ| = |{l ∈ ρ}| increases.

along with a conditional model P[ρ|t, L∗] that can infer programs for new tasks. This formulation also foreshadows a straightforward way in which linguistic descriptions of tasks (like those in the ﬁrst column of Fig. 1) could be integrated into learning: we could simply extend the conditional model as P[ρ|t, dt, L∗] to include a task’s description dt. We come back to this (and

4.2. Amortized conditional inference To identify programs that solve tasks t while obtaining high probability under P[ρ|L, θL], DreamCoder trains a neural
1In addition to initial and learned functions, Ellis et al. (2021) deﬁne L to also include any initial literals and a rule for generating variables, such that programs can be completely generated as productions from the PCFG. We use the same formulation.

Leveraging Language to Learn Program Abstractions and Search Heuristics

search heuristic Qi(ρ|t, Li) at each iteration i to approximate the inverse conditional model. The heuristic uses a neural model trained to predict programs written in the current library Li according to the posterior:
Qi(ρ|t, Li) ≈ P[ρ|t, (Li, θLi)] ∝ P[t|ρ]P[ρ|(Li, θLi)] (4)
conditioned on an encoding of the training examples (e.g. an embedding of the image in the task speciﬁcation). This model is trained in the distant supervision setting (which begins with no supervised program data) by leveraging the forward generative model: sampling programs from the prior, executing them to produce observed tasks, and then minimizing Q(ρ|t, L) in Eq. 4 on the sampled programs, conditioned on their executions. This generative training procedure is generally applicable to any neural implementation of Q(ρ|t, L). (But see Ellis et al. (2021) and our supplementary material for additional details on the model architecture, which we reimplement in our experiments.)

4.3. Abstraction learning as program compression (maximizing the likelihood of programs)

The DreamCoder algorithm also iteratively updates the library (Li, θLi ) to approximately optimize Eq. 2 (ﬁnding L∗, θL∗ which maximize the likelihood of the inferred latent programs). Ellis et al. (2021) leverage equivalence to a compression problem deﬁned over programs and the library. As discussed in 4.1, the PCFG program prior is equivalent to a description length prior over programs. Ellis et al. (2021) place an additional Dirichlet prior over the library description length:





P [L] ∝ exp −λ size(ρ)

(5)

ρ∈L

Estimating the optimal library then becomes the problem of inferring new library abstractions which can jointly compress the latent training programs (rewritten under the new library Li+1) and the description length |Li+1| of the updated library (to optimize for shared abstractions across programs). This objective would still require inference over all possible ways of refactoring the latent programs under the updated library. Ellis et al. (2021) approximate this by only considering candidate abstractions and program refactorings that can be found via an efﬁcient lambda-abstraction algorithm. As an example, this could refactor the large hexagon program

(for ∞(move pen(∗ unit line 3)(/ 2π 6))

to expose a candidate abstraction like

λx.(for ∞(move pen(∗ unit line 3)(/ 2π x))

while also rewriting the original program using this abstraction. Notably, this fragment – which draws polygons with

lines of length 3 for sides – is not the most intuitively generalizable for the graphics domain. A programmer with more domain-speciﬁc prior knowledge would probably prefer an abstraction like λxy.(for ∞(move pen(∗ unit line y)(/ 2π x)) which additionally parameterizes the polygon by the length of its sides, and is semantically equivalent to the high-level polygon fn described in the problem setup in Sec. 3. However, learning abstractions by compressing the library and current solved training tasks may actually disfavor this more intuitively generalizable (but less compressive) candidate. Our second key goal in introducing language will be to leverage it as an additional source of prior knowledge to improve abstraction generalization.
5. Our Approach: Language for Abstraction and Program Search
Our work considers how the general learning problem – jointly learning the library L which deﬁnes the prior over programs and the conditional search strategy S which inverts from tasks to programs – can be enriched in the language-annotated setting. Here, at least a subset of the training tasks are additionally annotated with a natural language description dt (such as the natural language description large six gon for the large hexagon drawing task in Fig. 1B). Language offers a more direct source of information for discovering a library like the one in our setup,
L = polygon|large line|small line... if we leverage the expectation that generalizable abstractions (like a candidate polygon function) should correspond systematically to named fragments in natural language (like the token gon). Language can also be leveraged by the conditional search model: learning systematic correspondences between language and programs from descriptions like large six gon should inform search on new tasks (like the one described as a small nine gon next to a small square in Fig. 1B) on the basis of shared language (like gon). Our approach, LAPS (Language for Abstraction and Program Search) formalizes these intuitions by extending the hierarchical Bayesian problem formulation over programs given in Sec. 3 to additionally generate natural language task descriptions (see graphical model in Fig 1B, left). In particular, we assume the existence of a jointly generative model J(ρ, dt) over latent programs that solve tasks, and corresponding natural language descriptions. We rewrite the original prior over programs P[ρ|L, θL] deﬁned on a library L to a joint prior P[ρ, dt|J, θJ ], and extend the distribution in Eq. 1 over the latent joint model J with parameters θJ ,

Leveraging Language to Learn Program Abstractions and Search Heuristics

written as

Φ(J, θJ ) = P[J, θJ ]

P[t|ρ]P[ρ, dt|J, θJ ] (6)

t∈T ρ

Learning in the language-augmented setting now involves estimating the optimal joint model and its parameters

J ∗, θJ∗ = arg max Φ(J, θJ )

(7)

J,θJ

along with a language-conditioned model P[ρ|t, d, J∗] that can infer programs for new tasks based on both speciﬁcation examples and task descriptions. In the remainder of this section we ﬁrst describe a general joint model formulation that can be learned from languageannotated training tasks. We then show how the joint framework allows natural language to inform learning at both the abstraction and search level in a concrete example, using DreamCoder as the base hierarchical algorithm.

5.1. Joint prior over programs and language Base prior We formulate our joint prior over language and programs as

P[ρ, dt] = P[ρ|L, θL]P[dt|ρ, L]

(8)

decomposed as the product of the original program prior deﬁned on a program library P[ρ|L, θL], and a learned program-to-natural-language “translation” model T (dt|ρ, L) ≈ P[dt|ρ, L] which describes how natural language descriptions are generated for latent programs (in our running example, this model would describe how the large six gon description was generated conditioned on the program solution for that task.) This decomposition builds modularly on the original program prior deﬁned only on the library L. Learning T (dt|ρ, L) formalizes the intuition that there should be a learnable relationship between language that describes tasks and latent programs that solve them. T (dt|ρ, L) can be implemented in many ways (e.g. (Wong & Mooney, 2007; Joshi & Schabes, 1997; Bahdanau et al., 2014; Chen et al., 2018)), compatible with the vast literature on structured translation between languages, including natural languages and programming languages. Our experiments use the translation model popularly known as IBM Model 4 (Brown et al., 1993), one of a class of well-studied Bayesian machine translation models (Gal & Blunsom, 2013) which decompose T (dt|ρ, L) into

T (dt|ρ, L) ∝

PT [w|l]

(9)

w∈dt ,l∈ρ

a product of learned token-level translation probabilities PT [w|l] between individual functions l in a task’s latent

program ρ and words w in the task description dt. (See supplementary materials for model implementation and training details.) This token-level decomposition more directly captures the intuition in our setup: that abstractions in a programming library generally correspond systematically to individual names in natural language descriptions, and that the inverse conditional search can be guided based on a generally compositional relationship between program primitives and words. This formulation also allows these compositional relationships to be inferred from fewer observed examples than would be possible with other translation models with weaker inductive biases. However, Eq. 8 should extend to include any similar translation model and need not include this stronger decomposition.
Adding richer priors In LAPS, the joint model can also provide a controllable interface for incorporating additional prior knowledge about language into learning. Learned translation models are often ﬁt to only maximize the likelihood of the observed language (here, with respect to inferred latent training programs). However, our formulation also supports T (dt|ρ, L) enriched to include additional priors over language (such as speaker-speciﬁc language usage, or pragmatics models that capture a speakers’ other communicative goals (Grice, 1989; Goodman & Frank, 2016).) In our experiments (Sec. 6.1) we showcase this with results from an extended model incorporating an additional mutual exclusivity prior. Mutual exclusivity models the expectation that newly encountered words should correspond to different meanings than known ones. This prior has been shown to play an important role in language learning in cognitive science (Frank et al., 2009; Markman & Wachtel, 1988), and in machine learning models (Gandhi & Lake, 2019). In the synthesis setting, mutual exclusivity can capture the expectation that “new” words (which appear in descriptions of currently unsolved tasks) are more likely to correspond to different program components than those used in solved training tasks (and for which there would otherwise be no signal to learn a translation model in the distant setting). Our extended model incorporates this prior by updating Eq. 9 to distinguish between Wknown (words that appear in solved training tasks with latent programs) and Wnew (newly encountered words) as

TME(dt|ρ, L) ∝

(1[w ∈ Wknown]PT [w|l])

w∈d,l∈ρ

(10)

(1[w ∈ Wnew]P[l|L, θL]−1])

where new words are modeled as inversely related to primitives under the program prior (ﬁt to previously solved tasks) – modeling the expectation that new words more likely relate to less-used program components than those used so far.

Leveraging Language to Learn Program Abstractions and Search Heuristics

5.2. Integrating the joint model into amortized conditional search

The joint model allows LAPS to incorporate natural language into the learned conditional search model over programs. In place of the original neural amortized model in the base algorithm (Sec. 4.2), we train an extended, languageconditioned model Qi(ρ|t, dt, Ji) at each iteration to predict programs according to:

Q(ρ|t, dt, Ji) ≈ P[ρ|t, dt, J, θJ ]

∝ P[t|ρ]P[ρ, dt|J, θJ ]

(11)

∝ P[t|ρ]P[dt|ρ]P[ρ|L, θL]

≈ P[t|ρ]T (dt|ρ, L)P[ρ|L, θL]

which amortizes program inference under our joint model formulation. Importantly, we can train this neural model using samples from the joint generative model, consisting of sampled programs and corresponding generated language. As with the original learning setting, this sample-based training allows LAPS to learn a generalizable, languageconditioned neural search heuristic, capable of leveraging compositional patterns in natural language, from very few examples in the distant supervision setting. We can also now see the beneﬁts of richer language-speciﬁc priors (such as mutual exclusivity): the neural model trained to amortize inference from the joint generative model can also approximate the mutual exclusivity bias, enabling better exploration and generalization in the presence of new words.

5.3. Abstraction learning as joint model compression

The extended joint model objective in Eq. 2 and 7 also allows LAPS to incorporate natural language into abstraction learning. Extending the compression-based abstraction objective in the base algorithm – which optimized for libraries that maximally compress the latent training programs and library – requires deﬁning a prior over the language-program translation model T in terms of the optimal program library.

We place a prior over T deﬁned on a program library L and a natural language token vocabulary W as

P[T |L] ∝

−I(PT [w|l])

(12)

l∈L,w∈W

where −I(PT [w|l]) = − log(PT [w|l]). This models the intuition that a good library contains program abstractions which correspond well to individual language tokens, and reduce entropy in the compositional translation model. Deﬁning the prior compositionally also allows the algorithm to maintain the desirably property from (Ellis et al., 2021), in which the joint likelihood can be efﬁciently re-approximated with respect to individual candidate program abstractions based on their constituent subcomponents l and corresponding translation distributions PT [w|l] under the current translation model. As in the base synthesis algorithm, we

Algorithm 1 Input: Initial library L0, annotated training tasks (T, D) Initialize J ← uniform; training task solutions p ← {} for i ≤ f do Qi(ρ|t, dt) ← Train on (p, T, dt) and samples ∼ J p ← programs from search amortized with Qi Li ← abstractions optimized over (p, J) p ← programs rewritten using abstractions from Li Ji ← Fit θL and T (dt|ρ) to (p, dt) end for Return Qf , Lf
fully re-estimate a new translation model at each iteration Ti+1(dt|ρi+1, Li+1) to ﬁt the updated library and refactored programs. See the supplement for extended details. Taken together, Alg. 1 summarizes the concrete algorithm using LAPS to incorporate language into (Ellis et al., 2021).
6. Experiments
We demonstrate LAPS on three different domains: string editing, compositional graphics drawing, and scene reasoning, which we choose to represent a diverse range of tasks and accompanying language (Fig. 2). In all three domains, we ﬁnd that compared to the base synthesizer, LAPS learns and solves heldout synthesis problems faster (Table 1, Sec. 1-2), and produces higher-quality libraries that improve generalization even when natural language hints are not available after training (Table 1, Sec. 3). Below we summarize each domain. We then discuss results showing that LAPS is effective because of how the hierarchical model incorporates language during learning: we ﬁnd that (1) LAPS searches more effectively during training, enabling it to solve and learn from more diverse training tasks than the baseline model; (2) LAPS abstracts more effectively during training, adding in more generalizable library routines as it learns; and (3) LAPS can use language during testing if it is available, as an important additional source of high-level information during synthesis. 6.1. Domains All three domains consist of a dataset of inductive synthesis tasks t speciﬁed as input/output examples; procedurally generated synthetic language annotations; and human language annotations sourced from Mechanical Turk. We use synthetic language as our primary evaluation benchmark: we are interested in a controlled probe of learning when words are systematically reused and composed, but refer to more abstract concepts than in the initial base programming language. However, we also use human language to evaluate the practicality of our approach in real-world settings.

Leveraging Language to Learn Program Abstractions and Search Heuristics

A. String Editing (shown with sample I/O examples of n=30 and random human description of n=3)

cools → gcools cultivator → gcultivator bloomed → bloomed
(Synth) if the word starts with consonant vowel add g before that

pavings → pavinb forgiveness → forgiveneb enterprises → enterprises
if the word ends with consonant s replace that with b

topazes -> topaz suburbs -> suburbs reckless -> reckls
if there is e s remove that

(Human) if word begins with consonant followed by vowel , add an g to the beginning

if the word ends with a consonant and s then change them both to b

remove the e s from the word

shouldering -> shoululdering hath -> hath outrun -> oututrunun if there is u any letter double that
the next letter with the letter u should be repeated as a pair for this transformation

B. Scene Reasoning (shown with sample I/O examples of n=7 and random human description of n=2)

Original CLEVR (sample templates from full set)

Extended scene manipulation and counterfactuals

2

metal

3

1

rubber

3

2

metal

0

What number of gray rubber cubes are there? how many grey rubber cubes do you see

There is another thing that is the same color as the large rubber thing; what is it made of? what material is the other object that is the same color as the large rubber object

What if the gray sphere became a small green metal sphere?

If you removed the red things, how many spheres would be left?

what if the grey ball morphed into a small count the spheres would be left after

green ball

removing the red things

C. Compositional Graphics (shown with random human description of n=3)

Simple shapes

Complex objects

a small triangle small triangle
a medium square one medium square
a medium eight gon octogon

a seven pointed star a seven sided snowflake with long triangles as arms
a four stepped zigzag four step ladder going from top to bottom

Compositional objects and relations
a small five gon next to a small seven gon a five sided gon beside a seven sided gon
a small nine gon separated by a big space from a small circle nine gon on left with small circle on right not connected

a big circle just a circle

a greek spiral with eight turns a long line that curls in on itself at right angles

a small triangle connected by a big line to a medium triangle a small triangle with a long line and a medium triangle

four nested squares four stacked squares
six small five gons in a row six overlapped pentagons going left to right
seven sided snowflake with a short space and a short line and a short space and a small triangle as arms a seven sided snowflake with seven triangles and line

D. Example initial graphics primitives
for move pen-up
+ -
1 2

....and example program abstractions learned with language shown with learned high probability p(word | primitive)

f0=(λ (x y z) (for x (λ (u v) (move z y v))))
move pen in parameterized loop

f9=(f0 ∞ ε)

0.07 | semicircle

smooth curve

a small semicircle (f19 (f9 0 x))
a medium semicircle (f3 (f9 0 x))
a big semicircle (f9 (* (/ ε 1) 5) x)

... f4=(λ (x y z) (f0 x (/ 2π
y) 1 z))

0.09 | small

rotates and draws a unit line

f5=(λ (x y) (f4 x x y))

...

0.27 | gon 0.22 | small

rotational symmetry by number of sides

a small five gon (f5 5 x)
a small nine gon (f5 9 x)
a medium seven gon (f5 2 (f20 7 x))

f17=(λ (x) (pen-up (λ (y) (f16 x y))))

0.67 | separated 0.06 | space

lift pen between consecutive shapes

f24=(λ (x y) (f23 (λ (z u) (f21 y 0 x u))))

0.09 | snowflake 0.09 | arms

rotate shapes around axis

eight sided snowflake with a small seven gon as arms (f24 7 8 x)
five sided snowflake with a short line and a medium five gon as arms (f24 5 (λ (x) (get/set (λ (y) (f2 1 (f41 5 y)))x)) z)

. . .

Figure 2. (A, B, C) Example tasks from all three synthesis domains shown with synthetic and sample human language annotations. Inductive synthesis domains are shown with a random subset (n=3) of the paired input/output examples. Human language annotations are also randomly sampled (all domains were annotated by multiple people for a broader range of language.) (D) Representative initial program primitives and library abstractions learned with LAPS for the graphics domain. Shown with example tasks solved with synthesized programs containing the learned abstractions and high probability natural language learned from the joint model.

Leveraging Language to Learn Program Abstractions and Search Heuristics

Additional information for all domains is in the supplement. String editing: structured string transformation problems taken from (Andreas et al., 2017) (n=1000 train; n=500 test). Tasks consist of input dictionary strings transformed using randomly sampled regular expression transducer (30 I/O examples per task). We choose this domain to demonstrate LAPS on an important classic synthesis domain (Lau & Weld, 1998). The dataset of Andreas et al. (2017) contains human annotations; synthetic language annotations are generated over the ground-truth regexes using templates based on the original human annotations. We initialize synthesizers with functional programming primitives (map, fold, cons, car, cdr, length, index) and character constants (following the simpler text editing domain in the baseline paper (Ellis et al., 2021)). The neural search model encodes the I/O task examples as character arrays with a bidirectional GRU. Compositional graphics: inverse graphics problems (n=200 train; n=111 test) where each task is speciﬁed by an image and solved by synthesizing a program in LOGO Turtle graphics (Abelson & DiSessa, 1986). This is inspired by the graphics domain in (Ellis et al., 2021) but re-designed to be more challenging (ground-truth programs are much longer on average in the base programming language) and explicitly compositional. Synthetic language annotations are generated with high-level templates over the objects and relations in each task; human annotations are sourced as image descriptions from MTurk. We initialize synthesizers with the graphics primitives in (Ellis et al., 2021). The neural model encodes image examples with a CNN. Structured scene reasoning: inductive scene reasoning tasks (n= 212 train; n=115 test) where each synthesis problem is speciﬁed by a structured input scene, and outputs can be a number (how many red rubber things are there?), a boolean value (are there more blue things than green?), or another scene (what if all of the red things turned blue?). This domain is modeled on CLEVR (Johnson et al., 2017a) but designed to support inductive synthesis tasks speciﬁed over the symbolic scene representations (an array of objects represented as dictionaries of attributes) from the original CLEVR task generator in Johnson et al. (2017a). We also add new tasks that require generating or imagining latent scenes (how many metal things would be left if all the blue cylinders were removed?), which are not solvable in the original high-level DSL hand-designed for Johnson et al. (2017b) (and used in synthesis-based approaches like Yi et al. (2018)). We include these to demonstrate a key feature of our approach: the ability to learn generalizable libraries from a basic but expressive set of primitives, rather than restricting the program space pre-emptively with a hand-designed language. We use synthetic language annotations from the original templates in (Johnson et al., 2017a) (and templates written in the same style for the

extended tasks); human annotations are sourced from annotators shown the same tasks. We initialize synthesizers with functional programming primitives similar to the stringediting domain, with domain-speciﬁc query functions and constants (get color(x); get shape(x); blue; cube). The neural model encodes the task examples as ﬂattened arrays of object attributes using a bidirectional GRU.
6.2. Results On all three domains, we compare our model against the baseline synthesizer (Table 1, DreamCoder, no language); a multimodal baseline (Table 1, multimodal, no generative model) that trains a neural model directly on solved training tasks (similar to neural synthesis models like DeepCoder (Devlin et al., 2017) but augmented to condition on language); and ablated LAPS variants (Table 1; LAPS rows) to evaluate the additive contributions of the individual learning components. We compare all models using a matched search budget per task and number of training iterations overall, determined using a hyperparameter search with the baseline. The supplement contains full details (and code) to replicate all experiments; and additional qualitative results. We ﬁnd that: (1) LAPS searches more effectively during training, enabling it to solve and learn from more training tasks than the baseline synthesizer. Under the hierarchical model formulation, search and abstraction are closely related: successfully solving tasks is the basis for abstraction learning. Comparing the model learning trajectories (Fig. 3) on training tasks shows that the LAPS models consistently search more effectively during training: at each iteration they solve more tasks within a given time budget. Fig. 3 also highlights that LAPS models improve training robustness in the distant learning setting: as in the baseline paper (Ellis et al., 2021), we ﬁnd the baseline model learning to be highly variable without a training curriculum (compare training curves from Fig. 3 with different random seed replications; and the best vs. mean performance, Table 1.) Comparing the LAPS ablations also suggests that linguistic priors (like mutual exclusivity) can indeed be practically useful here during learning (Table 1, compare LAPS with ME and without). What if we do use a curriculum? In the scene reasoning domain (where previous approaches (e.g. Mao et al. 2019) have argued for a curriculum), we also test a simple curriculum by ordering tasks according to their natural language token length (which can be evaluated without ground truth programs). Table 1 shows that our model is still more effective, and that non-curriculum performance is in fact comparable to curriculum performance. (2) LAPS abstracts more effectively during training, adding in more generalizable library routines as it learns. The

Leveraging Language to Learn Program Abstractions and Search Heuristics

Table 1. % held-out test-tasks solved. To compare robustness, we run random seed replications in the graphics domain for the synthetic language dataset. Best reports the best model across replications; Mean averages across replications.

Language

Model

Strings (ntest = 500)

Graphics (ntest = 111)

Scenes (ntest = 115)

% Solved

% Solved (Best) % Solved (Mean) % Solved (Curric.) % Solved (Mean.)

Synth train/test Synth train/test

DreamCoder (no language)

33.4

49.55

42. 64

67.80

73.9

Multimodal (no generative translation model)

46.00

26.12

23.20

76.50

49.5

Synth train/test Synth train/test Synth train/test

LAPS in neural search

52.20

92.79

52.93

95.6

88.1

LAPS + mutual exclusivity

57.00

86.49

80.18

96.5

82.3

LAPS + ME + language-program compression

54.60

98.19

81.98

95.6

95.9

Synth train/human test

LAPS + ME + language-program compression

54.60

89.20

–

97.4

–

Human train/human test

LAPS + ME + language-program compression

48.60

58.55

–

95.6

–

No language at test

No language on train/test

Original DSL; Enumerative

0.06

0.00

–

27.8

–

No language on train/test

DreamCoder (best library): Enumerative

27.2

41.44

–

53.6

–

No lang at test

LAPS (best library): Enumerative

33.2

62.16

–

93.04

–

No lang at test

LAPS (best library): example-only neural synthesis

52.4

91.0

–

95.6

–

DreamCoder (no language)

Multimodal (no generative)

LAPS in neural search

LAPS + mutual exclusivity LAPS + ME + lang. compression

% Solved (0 – 100%)

# Learning Iterations (0 – 27)
Figure 3. Learning curves comparing baselines and LAPS models in Table 1, showing % heldout tasks solved on the graphics domain over random training task orderings. (Mean results in Table 1 shows average test-time performance from the trained model replications.)

variability across training replications in the baselines also highlights a challenge for abstraction learning: not all shared subroutines encountered in training generalize well to new tasks. Adding poor abstractions can actually be detrimental: they increase the combinatorial search space. We ﬁnd that our approach produces higher-quality libraries after training: Table 1 (no language at test time section) shows that we consistently improve performance in a head-to-head comparison using enumerative search from the library priors alone – in some domains, enumerative search with our model’s library outperforms neurally guided search from the baseline model. We also ﬁnd the learned library is effective for neurally-guided synthesis when no language hints are available after training (Table 1, no language at test, example-guided synthesis), showing that LAPS incorporates language to learn a more effective library overall, which generalizes to the non-language setting. See supplement for example learned abstractions from Lf . (3) LAPS can use language during testing if it is available, though it doesn’t need to for competitive performance. Clearly, language can provide a useful source of high-level information if it is available for new tasks. Our approach produces a neural synthesizer pre-trained to condition on language where available. Results on all three domains show that the model can use it to achieve additional performance gains (Table 1, see language at test rows). We also ﬁnd that the models trained on synthetic annotations generalize effectively to natural human language at test (Table 1, synth train, human test), suggesting that even if human annotation is too costly, in many cases hand-writing natural language tem-

plates to accompany a few ground-truth programs is likely sufﬁcient (and easier than hand designing a full DSL).
7. Conclusion
We presented Language for Abstraction and Program Search (LAPS). LAPS builds on hierarchical Bayesian models of program learning: we offer a general framework for introducing jointly generative models over programs and language into learned synthesis. Going forwards, an important avenue for future work will be exploring different concrete implementations of the base algorithm and translation model which relates programs to language. A promising future direction could leverage recent structured, neural joint models that can learn the compositional units of language, and incorporate pre-trained language representations (Joshi & Schabes, 1997; Wiseman et al., 2018; Kim et al., 2019). The hierarchical Bayesian framing also draws connections to computational cognitive models which model human conceptual representations and learning (Goodman et al., 2014; Fodor, 1975; Rule, 2020) as inference over program-like representations. Future human experiments could explore LAPS as a cognitive model, combining paradigms for studying language learning with those for studying non-linguistic abstraction and search (e.g. Smith et al. 2003; Hawkins et al. 2019; Lake et al. 2015; 2019; Tian et al. 2020).
Acknowledgements: Many thanks to M. Nye, J. Mu, A. Marzoev, J. Fan, R. Hawkins, R. Levy, L. Schulz and our anonymous reviewers for invaluable feedback. Supported by grants from the Air Force Ofﬁce of Scientiﬁc Research, the NSF under Grant No.

Leveraging Language to Learn Program Abstractions and Search Heuristics

References
Abelson, H. and DiSessa, A. A. Turtle geometry: The computer as a medium for exploring mathematics. MIT press, 1986.
Abolaﬁa, D. A., Norouzi, M., Shen, J., Zhao, R., and Le, Q. V. Neural program synthesis with priority queue training. arXiv preprint arXiv:1801.03526, 2018.
Andreas, J., Klein, D., and Levine, S. Learning with latent language. arXiv preprint arXiv:1711.00482, 2017.
Appel, A. W., Beringer, L., Chlipala, A., Pierce, B. C., Shao, Z., Weirich, S., and Zdancewic, S. Position paper: the science of deep speciﬁcation. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences, 375(2104):20160331, 2017.
Artzi, Y. and Zettlemoyer, L. Weakly supervised learning of semantic parsers for mapping instructions to actions. Transactions of the Association for Computational Linguistics, 1:49–62, 2013.
Bahdanau, D., Cho, K., and Bengio, Y. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473, 2014.
Balog, M., Gaunt, A. L., Brockschmidt, M., Nowozin, S., and Tarlow, D. Deepcoder: Learning to write programs. arXiv preprint arXiv:1611.01989, 2016.
Brown, P. F., Della Pietra, S. A., Della Pietra, V. J., and Mercer, R. L. The mathematics of statistical machine translation: Parameter estimation. Computational linguistics, 19(2):263–311, 1993.
Chen, X., Liu, C., and Song, D. Tree-to-tree neural networks for program translation. arXiv preprint arXiv:1802.03691, 2018.
Cropper, A. and Muggleton, S. H. Learning efﬁcient logical robot strategies involving composable objects. AAAI Press/International Joint Conferences on Artiﬁcial Intelligence, 2015.
Dechter, E., Malmaud, J., Adams, R. P., and Tenenbaum, J. B. Bootstrap learning via modular concept discovery. In Twenty-Third International Joint Conference on Artiﬁcial Intelligence, 2013.
Delaware, B., Pit-Claudel, C., Gross, J., and Chlipala, A. Fiat: Deductive synthesis of abstract data types in a proof assistant. Acm Sigplan Notices, 50(1):689–700, 2015.
1918839 and NSF-funded Center for Brains, Minds, and Machines, the MIT-IBM Watson AI Lab, Google, Microsoft and Amazon.

Desai, A., Gulwani, S., Hingorani, V., Jain, N., Karkare, A., Marron, M., and Roy, S. Program synthesis using natural language. In Proceedings of the 38th International Conference on Software Engineering, pp. 345–356, 2016.
Devlin, J., Uesato, J., Bhupatiraju, S., Singh, R., Mohamed, A.-r., and Kohli, P. Robustﬁll: Neural program learning under noisy i/o. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pp. 990– 998. JMLR. org, 2017.
Dumancic´, S. and Cropper, A. Inventing abstractions by refactoring knowledge.
Ellis, K., Ritchie, D., Solar-Lezama, A., and Tenenbaum, J. B. Learning to infer graphics programs from handdrawn images. arXiv preprint arXiv:1707.09627, 2017.
Ellis, K., Morales, L., Sable´-Meyer, M., Solar-Lezama, A., and Tenenbaum, J. Learning libraries of subroutines for neurally–guided bayesian program induction. In Advances in Neural Information Processing Systems, pp. 7805–7815, 2018.
Ellis, K., Nye, M., Pu, Y., Sosa, F., Tenenbaum, J., and SolarLezama, A. Write, execute, assess: Program synthesis with a repl. arXiv preprint arXiv:1906.04604, 2019.
Ellis, K., Wong, C., Nye, M., Sable´-Meyer, M., Cary, L., Morales, L., Hewitt, L., Solar-Lezama, A., and Tenenbaum, J. Dreamcoder: Growing generalizable, interpretable knowledge with wake-sleep bayesian program learning. ArXiv preprint, 2020.
Ellis, K., Wong, C., Nye, M., Sable´-Meyer, M., Cary, L., Morales, L., Hewitt, L., Solar-Lezama, A., and Tenenbaum, J. Dreamcoder: Bootstrapping inductive programsynthesis with wake-sleep library learning. PLDI 2021, 2021.
Fikes, R. E. and Nilsson, N. J. Strips: A new approach to the application of theorem proving to problem solving. Artiﬁcial intelligence, 2(3-4):189–208, 1971.
Fodor, J. A. The language of thought, volume 5. Harvard university press, 1975.
Frank, M. C., Goodman, N. D., and Tenenbaum, J. B. Using speakers’ referential intentions to model early crosssituational word learning. Psychological science, 20(5): 578–585, 2009.
Frome, A., Corrado, G. S., Shlens, J., Bengio, S., Dean, J., Ranzato, M., and Mikolov, T. Devise: A deep visualsemantic embedding model. In Advances in neural information processing systems, pp. 2121–2129, 2013.

Leveraging Language to Learn Program Abstractions and Search Heuristics

Gal, Y. and Blunsom, P. A systematic bayesian treatment of the ibm alignment models. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 969–977, 2013.
Gandhi, K. and Lake, B. M. Mutual exclusivity as a challenge for deep neural networks. arXiv preprint arXiv:1906.10197, 2019.
Ganin, Y., Kulkarni, T., Babuschkin, I., Eslami, S. A., and Vinyals, O. Synthesizing programs for images using reinforced adversarial learning. In International Conference on Machine Learning, pp. 1666–1675. PMLR, 2018.
Goodman, N. D. and Frank, M. C. Pragmatic language interpretation as probabilistic inference. Trends in cognitive sciences, 20(11):818–829, 2016.
Goodman, N. D., Tenenbaum, J. B., and Gerstenberg, T. Concepts in a probabilistic language of thought. Technical report, Center for Brains, Minds and Machines (CBMM), 2014.
Goyal, P., Niekum, S., and Mooney, R. J. Pixl2r: Guiding reinforcement learning using natural language by mapping pixels to rewards. arXiv preprint arXiv:2007.15543, 2020.
Grice, P. Studies in the Way of Words. Harvard University Press, 1989.
Gulwani, S., Herna´ndez-Orallo, J., Kitzelmann, E., Muggleton, S. H., Schmid, U., and Zorn, B. Inductive programming meets the real world. Communications of the ACM, 58(11):90–99, 2015.
Gulwani, S., Polozov, O., Singh, R., et al. Program synthesis. Foundations and Trends® in Programming Languages, 4 (1-2):1–119, 2017.
Hawkins, R. X., Goodman, N. D., and Goldstone, R. L. The emergence of social norms and conventions. Trends in cognitive sciences, 23(2):158–169, 2019.
Jia, R. and Liang, P. Data recombination for neural semantic parsing. arXiv preprint arXiv:1606.03622, 2016.
Johnson, J., Hariharan, B., Van Der Maaten, L., Fei-Fei, L., Lawrence Zitnick, C., and Girshick, R. Clevr: A diagnostic dataset for compositional language and elementary visual reasoning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2901–2910, 2017a.
Johnson, J., Hariharan, B., Van Der Maaten, L., Hoffman, J., Fei-Fei, L., Lawrence Zitnick, C., and Girshick, R. Inferring and executing programs for visual reasoning.

In Proceedings of the IEEE International Conference on Computer Vision, pp. 2989–2998, 2017b. Johnson, M. Pcfg models of linguistic tree representations. Computational Linguistics, 24(4):613–632, 1998. Joshi, A. K. and Schabes, Y. Tree-adjoining grammars. In Handbook of formal languages, pp. 69–123. Springer, 1997. Kim, Y., Dyer, C., and Rush, A. M. Compound probabilistic context-free grammars for grammar induction. arXiv preprint arXiv:1906.10225, 2019. Lake, B. M., Salakhutdinov, R., and Tenenbaum, J. B. Human-level concept learning through probabilistic program induction. Science, 350(6266):1332–1338, 2015. Lake, B. M., Ullman, T. D., Tenenbaum, J. B., and Gershman, S. J. Building machines that learn and think like people. Behavioral and brain sciences, 40, 2017. Lake, B. M., Linzen, T., and Baroni, M. Human few-shot learning of compositional instructions. arXiv preprint arXiv:1901.04587, 2019. Lau, T. A. and Weld, D. S. Programming by demonstration: An inductive learning formulation. In Proceedings of the 4th international conference on Intelligent user interfaces, pp. 145–152, 1998. La´zaro-Gredilla, M., Lin, D., Guntupalli, J. S., and George, D. Beyond imitation: Zero-shot task transfer on robots by learning concepts as cognitive programs. Science Robotics, 4(26), 2019. Liang, P. Learning executable semantic parsers for natural language understanding. Communications of the ACM, 59(9):68–76, 2016. Liang, P., Jordan, M. I., and Klein, D. Learning programs: A hierarchical bayesian approach. In Proceedings of the 27th International Conference on Machine Learning (ICML-10), pp. 639–646, 2010. Liang, W., Zou, J., and Yu, Z. Alice: Active learning with contrastive natural language explanations. arXiv preprint arXiv:2009.10259, 2020. Luketina, J., Nardelli, N., Farquhar, G., Foerster, J., Andreas, J., Grefenstette, E., Whiteson, S., and Rockta¨schel, T. A survey of reinforcement learning informed by natural language. arXiv preprint arXiv:1906.03926, 2019. Mao, J., Gan, C., Kohli, P., Tenenbaum, J. B., and Wu, J. The neuro-symbolic concept learner: Interpreting scenes, words, and sentences from natural supervision. arXiv preprint arXiv:1904.12584, 2019.

Leveraging Language to Learn Program Abstractions and Search Heuristics

Markman, E. M. and Wachtel, G. F. Children’s use of mutual exclusivity to constrain the meanings of words. Cognitive psychology, 20(2):121–157, 1988.
Mu, J., Liang, P., and Goodman, N. Shaping visual representations with language for few-shot classiﬁcation. arXiv preprint arXiv:1911.02683, 2019.
Nye, M., Hewitt, L., Tenenbaum, J., and Solar-Lezama, A. Learning to infer program sketches. arXiv preprint arXiv:1902.06349, 2019.
Parisotto, E., Mohamed, A.-r., Singh, R., Li, L., Zhou, D., and Kohli, P. Neuro-symbolic program synthesis. arXiv preprint arXiv:1611.01855, 2016.
Polosukhin, I. and Skidanov, A. Neural program search: Solving data processing tasks from description and examples. 2018.
Polozov, O. and Gulwani, S. Flashmeta: a framework for inductive program synthesis. In Proceedings of the 2015 ACM SIGPLAN International Conference on ObjectOriented Programming, Systems, Languages, and Applications, pp. 107–126, 2015.
Rule, J. S. The child as hacker: building more human-like models of learning. PhD thesis, Massachusetts Institute of Technology, 2020.
Shin, E. C., Allamanis, M., Brockschmidt, M., and Polozov, A. Program synthesis and semantic parsing with learned code idioms. In Advances in Neural Information Processing Systems, pp. 10824–10834, 2019.

Tian, L. Y., Ellis, K., Kryven, M., and Tenenbaum, J. B. Learning abstract structure for drawing by efﬁcient motor program induction. arXiv preprint arXiv:2008.03519, 2020.
Wiseman, S., Shieber, S. M., and Rush, A. M. Learning neural templates for text generation. arXiv preprint arXiv:1808.10122, 2018.
Wong, Y. W. and Mooney, R. Learning synchronous grammars for semantic parsing with lambda calculus. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pp. 960–967, 2007.
Ye, X., Chen, Q., Dillig, I., and Durrett, G. Benchmarking multimodal regex synthesis with complex structures. arXiv preprint arXiv:2005.00663, 2020a.
Ye, X., Chen, Q., Dillig, I., and Durrett, G. Optimal neural program synthesis from multimodal speciﬁcations. arXiv preprint arXiv:2010.01678, 2020b.
Yi, K., Wu, J., Gan, C., Torralba, A., Kohli, P., and Tenenbaum, J. Neural-symbolic vqa: Disentangling reasoning from vision and language understanding. In Advances in Neural Information Processing Systems, pp. 1031–1042, 2018.
Zhang, Y., Pasupat, P., and Liang, P. Macro grammars and holistic triggering for efﬁcient semantic parsing. arXiv preprint arXiv:1707.07806, 2017.

Si, X., Yang, Y., Dai, H., Naik, M., and Song, L. Learning a meta-solver for syntax-guided program synthesis. In International Conference on Learning Representations, 2019.

Silver, T., Allen, K. R., Lew, A. K., Kaelbling, L. P., and Tenenbaum, J. Few-shot bayesian imitation learning with logical program policies. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 34, pp. 10251– 10258, 2020.

Smith, K., Brighton, H., and Kirby, S. Complex systems in language evolution: the cultural emergence of compositional structure. Advances in Complex Systems, 6(04): 537–558, 2003.

Srivastava, S., Labutov, I., and Mitchell, T. Joint concept learning and semantic parsing from natural language explanations. In Proceedings of the 2017 conference on empirical methods in natural language processing, pp. 1527–1536, 2017.

Supplemental: Leveraging Language to Learn Program Search Heuristics and Abstractions

arXiv:2106.11053v3 [cs.LG] 3 May 2022

This contains the supplemental appendix to the 2021 ICML paper. It is organized sequentially in reference to the main text; S{N} refers back to section N in the main text. A complete release of code for our implementation, including command line scripts to replicate the experiments in the paper and links to the datasets, can be found at: https://bit.ly/3g9361W.

S4. Base learning algorithm: DreamCoder
The LAPS framework described in the main paper (Sec. 5) is a general one for extending Bayesian models of program learning to incorporate information from natural language (see (Liang et al., 2010; Lake et al., 2015; Dechter et al., 2013; Lake et al., 2013)). Our concrete implementation and experiments use the DreamCoder approach of (Ellis et al., 2021; 2018) as the base synthesis algorithm, which implements the hierarchical Bayesian formulation of program learning. It deﬁnes a modular interface with two primary learning components: a learned conditional inference model for search (as a neural search heuristic); and a learned abstraction algorithm for updating the program prior (based on program refactoring and compression) (Ellis et al., 2021). Each of these learning components has been additionally implemented in other work (such as (Devlin et al., 2017; Polosukhin & Skidanov, 2018; Nye et al., 2019; Parisotto et al., 2016; Balog et al., 2016) for neurally guided synthesis, and (Dechter et al., 2013; Zhang et al., 2017; Shin et al., 2019; Artzi et al., 2014; Dumancic´ & Cropper) for program abstraction learning). This supplementary section provides theoretical and implementation details on the DreamCoder algorithm we use in our experiments (summarized in Sec. 4). We match our implementation as closely as possible to the original work for comparison with published baselines. We provide key details relevant to the language-guided extension, but strongly recommend the original works which introduce the DreamCoder algorithm (Ellis et al., 2021; 2018) for further reference.
S4.1 Program prior and MDL equivalence Hierarchical Bayesian program learning formulations require a prior over expressible programs. DreamCoder is learned iteratively: it is initialized with a base library L0 and returns a library Lf containing program abstractions learned from solving training tasks. Therefore, DreamCoder deﬁnes its program prior with respect to the current library Li maintained at each iteration. This is parame-

terized as a simple PCFG P[ρ|L, θL] whose productions are of the form li → lj ∈ L, each with a real-valued weight θLl, where the probability of a program ρ is given by P[ρ|L, θL] = l∈ρ P[l|L, θL] (Sec. 4.1). Minor complexity arises in order to support typing (Pierce, 2002): following (Ellis et al., 2018), the library Li is implemented as a set of polymorphically typed λ-calculus expressions. The only change this produces to the original prior deﬁnition is to restrict the set of possible productions under the PCFG: that is, permissible productions are of the form li → lj ∈ {L|li → lj is well typed}. The prior probabilities of programs are therefore calculated with respect to the set of well-typed productions. As discussed in the main paper, this prior deﬁnition is equivalent to a minimum description-length prior over programs under (L, θL) when all θL < 1.0, as the product of additional productions in an expression will strictly decrease as the number of productions in an expression increases.
S4.2 Amortized conditional inference
Figure 1. Architecture of the neural model Qi(ρ|t, Li). The model takes as input task examples t. These are encoded using a domainspeciﬁc encoder E(t). Task encodings feed to an MLP and activation layer and output a tensor Q. This parameterizes a distribution over program bigrams in the ﬁnal DSL, which deﬁnes a conditional distribution from which to enumerate programs during search.
To identify programs that solve tasks t while obtaining high probability under P[ρ|L, θL], DreamCoder trains a neural search heuristic Qi(ρ|t, Li) at each iteration i to approximate the inverse model. The training procedure in (Ellis et al., 2021) (summarized in Sec. 4.2) is a key contribution of the original work for learning in the distant supervision setting. The model is trained on samples from the generative prior (providing an endless

Supplemental: Leveraging Language to Learn Program Search Heuristics and Abstractions

training stream of random synthesis tasks); and this procedure should generalize immediately to any neural model for predicting programs conditioned on the task speciﬁcation (e.g. (Devlin et al., 2017; Polosukhin & Skidanov, 2018; Nye et al., 2019; Parisotto et al., 2016; Balog et al., 2016)). The model is also supervised on any original training task examples and their program solutions discovered during learning. In our experiments we use the baseline neural model architecture in (Ellis et al., 2021). This is parameterized by two modular components:
1. A domain-speciﬁc task encoder E(t). This encodes the task examples (e.g. images in the graphics program domain, or input-output strings in the text editing domain) that are input to the neural model. This task encoder architecture is deﬁned domain-speciﬁcally based on the form of the task examples (e.g. a CNN for the graphics domain). It outputs a ﬁxed dimensional embedding for any given task as input to the model. In our experiments this is a 64-dimensional embedding across all domains (See S6.1 for domain-speciﬁc architectures; and released code.)
2. A conditional model over programs Q(ρ|E(t)). This component receives the task encoding as input and outputs a distribution over programs. Following (Ellis et al., 2021), this is a 2-layer fully-connected MLP (with 64 hidden units and a ﬁnal tanh activation layer) that outputs a ﬁxed-dimensional real-valued tensor encoding a distribution over programs in the library L as output. The real-valued tensor corresponds to weights over program primitives conditioned on their local context in the syntax tree of the program, consisting of the parent node in the syntax tree and which argument is being generated. This functions as a ‘bigram transition model’ over trees that encodes the likelihood of transitions from one primitive to the next. Q returns this as a (|L| + 1) × (|L| + 2) × A-dimensional tensor, where A is the maximum arity of any primitive in the library.
This parameterization supports fast sampling of programs during conditional synthesis: the neural model runs once per task (to encode the task examples and produce the bigram transition model) and the resulting parameterization can then be used to sample programs during synthesis (e.g. by enumerating programs by expanding trees (as ‘bigrams’ over parent and children primitives) ranked in order of their likelihood starting from the program root.) Following (Ellis et al., 2021), the neural model is trained to optimize the following MAP inference objective on the

training tasks and the sampled tasks from the prior:

LMAP = Et∼(L,θL)

log Q

arg max P[ρ|t, L, θL]
ρ

t (1)

S4.3 Abstraction learning as program compression DreamCoder learns new abstractions to approximately optimize for Eq. 2 (main paper), which infers an optimal library and parameters with respect to the observed programs on the training tasks. The DreamCoder abstraction algorithm is a primary contribution of the original work in (Ellis et al., 2021), and is discussed extensively in (Ellis et al., 2021). We therefore provide additional technical details here that are relevant to its integration with LAPS in our experiments, but strongly encourage referencing (Ellis et al., 2021) for the full implementation. As discussed in (Ellis et al., 2021) and our main work, DreamCoder approaches abstraction using an equivalence between Eq. 3 and the minimum description length of the prior (as the description length of the library) and the programs produced from the prior (under the PCFG deﬁnition of the prior). Therefore, in practice, inferring the optimal library is equivalent to inferring the library which maximally compresses the description length of the library and the description length of programs which explain the training tasks. In particular, DreamCoder optimizes the following compression objective with respect to the training tasks T and the ﬁnite beam Bt of program solutions discovered for each training task during learning:

log P[L] + arg max log P[t|ρ] max P[ρ |L, θL]

θL t∈T

ρ∈Bt

ρ −→∗ρ

+ log P[θL|L] − |θL|0

(2)

The key aspect of this algorithm is that it considers abstractions which compress not only the programs as they are currently written, but any semantically equivalent refactorings of these programs. Speciﬁcally, as programs are written in a λ-calculus, refactoring refers to any program which is equivalent up to β-reduction (i.e., function application/variable substitution (Pierce, 2002)). A primary contribution of the original work in (Ellis et al., 2021) is an efﬁcient algorithm for computing these refactorings that is unchanged when we integrate language; we refer to the original text for details.

In our work, the primary important aspect of this aspect is that refactorings are deﬁned compositionally over the existing program primitives. Speciﬁcally, refactorings can be efﬁciently calculated according to semantic equivalences in the the λ-calculus (namely, that function application and variable substitution guarantee that the resulting refactored programs are equivalent. Abstractions created by variable

Supplemental: Leveraging Language to Learn Program Search Heuristics and Abstractions

substitution will always be composed of subcomponents from the initial library.) We take advantage of this compositionality when deﬁning our joint abstraction algorithm over natural language. Deﬁning an initial compositional translation model between language and the program components ensures that we can approximate compression in the joint model after the programs are refactored, without needing to induce an entirely new translation model over language and the refactored programs.
S5. Our Approach: Language for Abstraction and Program Search
This section now describes technical details for the concrete LAPS implementation in our reported experiments, which is deﬁned over the DreamCoder implementation. We structure this section according to the parallel implementations in the base algorithm for clarity. However, except for the speciﬁcs of the joint-abstraction algorithm, the technical implementation of each component should extend directly to most other similar learned synthesis algorithms (e.g. the joint model implementation should be reusable in any synthesis algorithm that uses an explicit symbolic library of primitives.)
S5.1 Joint prior over programs and language LAPS extends the prior P[ρ] over programs under the library to a joint prior J(ρ, dt) over programs for a given task and their natural language descriptions dt (Sec. 5.1). We formulate this prior as
J(ρ, dt) = P[ρ|L, θL]P[dt|ρ, L]
the product of the original prior over programs P [ρ|L, θL] deﬁned on the program library, and a program to descriptions “translation” model T (dt|ρ, L) ≈ P[dt|ρ, L] that describes how descriptions are generated for programs written in the library. The concrete implementation described in the main paper uses a translation model that additionally decomposes compositionally over language and programs–in particular, on the basis of token-token translation distributions PT [w|l] between words w ∈ dt and l ∈ L. Many available translation and semantic parsing models (such as synchronous grammars over natural language and programs) preserve this further compositional requirement (e.g. (Artzi et al., 2014; Wong & Mooney, 2006)). See Figure S3 (supplement) for example samples from the generative model on the graphics domain at earlier and later stages of training. Our implementation uses a classical statistical machine translation model (the Model 4 version of the IBM Statis-

tical Machine Translation models (Gal & Blunsom, 2013)) whose parameters can be tractably estimated from very few paired programs and descriptions (in the distant supervision setting used in the original work, there may be no more than a couple of hundred training tasks in the full dataset, and fewer than 10 solved tasks on which to train the translation model at any given time.) In addition to inference in small data settings, this translation model has a fully compositional generative deﬁnition (Gal & Blunsom, 2013) that allows it to be easily used to train the neural amortized inference model which conditions on language.

Despite this, however, this translation model (and the further inductive biases used to speciﬁcally relate program trees to sentences) make strong compositonality assumptions about the relationship between program primitives and words as a joint generative model of programs and language; we ﬁnd that these inductive biases are useful in the small data setting and produce empirically successful results. However, this is likely because of how the joint model is used during training, which does not require a perfect generative model of language (or language with respect to programs) for either amortizing inference or abstraction in order to use language as a heuristic during learning.

A full deﬁnition of the statistical translation model we use can be found in (Gal & Blunsom, 2013). We re-summarize important details here. The IBM family of translation models estimates the conditional token-token probabilities PT [w|l] on the basis of alignment variables al,d, which specify a direct correspondence between tokens in parallel texts (e.g. a word in a task description and a program primitive.) These alignments are many:many between tokens in programs and natural language sentences – a given word can correspond to multiple primitives, and vice versa. Conditioned on a set of alignments from paired programs and descriptions, the conditional probabilities in both directions (the probability of generating a program primitive in a program based on the presence of a word in a sentence, and vice versa) are deﬁned by marginalizing over the alignment variables. We provide one direction (PT [w|l]), as the other is symmetrical:

m

PT [w|l] ∝ ... P[w, a1...am|l] ∝ q(ai|i, l, m)

a1

am

i=1

where ai are alignment variables inferred over a paired corpus and q(j|i, l, m) can be interpreted as the probability of alignment variable ai (for the token with index i in a program) taking value j (where j is an index into the corresponding sentence) conditioned on the lengths l and m of the program and natural language sentence (Gal & Blunsom, 2013).

These alignments are inferred by approximately inverting the generative model in (Gal & Blunsom, 2013) to maxi-

Supplemental: Leveraging Language to Learn Program Search Heuristics and Abstractions

mize the likelihood of the observed paired sentences and programs. One implementation detail: the alignment algorithm operates over pairs of strings. For convenience we infer alignments between sentences and linearized token sequences in the program tree (which can be done with complete recoverability of the original program tree (Andreas et al., 2013)). This is another inductive assumption that we choose after preliminary experimentation and ﬁnd that our implementation yields strong empirical results regardless. The IBM translation model is a noisy-channel generative model that requires an additional language model p(d) to generate language (Gal & Blunsom, 2013; Heaﬁeld, 2011). We use an efﬁcient parallelized implementation for inferring the translation model parameters from (Koehn et al., 2007), which also contains a basic language model inference algorithm inferred over the full corpus of training task sentences (as a trigram model, which we again ﬁnd simple but effective for our very small data setting). Speciﬁc model hyperparameters for all experiments are available in the released code repo (in the experiment runtime commands.)
Mutual exclusivity: Section 5.1 of the main paper also describes how the joint model can be modiﬁed to include language-speciﬁc priors, such as a simple implementation of the well-known mutual exclusivity prior documented in the cognitive language-learning literature (Markman & Wachtel, 1988; Gandhi & Lake, 2019) and given a Bayesian formulation in (Frank et al., 2009). We provide an implementation to demonstrate that the joint model can be easily extended: speciﬁcally, a simple mutual exclusivity assumption can be added into the joint model by simply updating the compositional translation model to include additional distributions tME(dnew|l) where dnew are words that only appear in unsolved training tasks and
tME(dnew|l) ∝ αP[l|L, θL]−1
new words are now assumed to correspond to primitives inversely proportional to their current usage under the learned program prior. As we show in the next section, incorporating this prior at the level of the joint model can be used to approximate mutual exclusivity assumptions in the learned search heuristic, encouraging exploration in the presence of new words. Practically, we calculate the mutual exclusivity prior in our concrete implementation by leveraging the alignments upon which our token-token translation probabilities are deﬁned. Speciﬁcally, we add pseudoalignments between each dnew and each l ∝ αP[l|L, θL]−1; when the token-token translation probabilities marginalize over the latent alignments and these pseudo alignments, the resulting translation probabilities encode the mutual exclusivity prior.

S5.2 Integrating the joint model into amortized conditional search
Figure 2. Architecture of the language-conditioned neural model Q(ρ|d, t). The model takes as input task examples t. These are encoded using a domain-speciﬁc encoder E(t). The model additionally takes in task descriptions d, encoded using a languag encoder ED(t) (implemented as a GRU). Task encodings are concatendated and feed to an MLP and activation layer and output a tensor Q. This parameterizes a distribution over program bigrams in the ﬁnal DSL, which deﬁnes a conditional distribution from which to enumerate programs during search.
The amortized conditional inference model Q(ρ|t) (Sec. 4.2) extends straightforwardly in LAPS to condition on language Q(ρ|d, t) (Sec. 5.2). Importantly, the training procedure in Sec. 4.2 (training the neural model on samples from the prior) also extends to the language-enriched condition (training the neural model on samples from the joint prior, which include generated language annotations.) In our experiments we implement the concrete neural model Q(ρ|d, t) in our experiments by extending modularly on the original model in (Ellis et al., 2021) (and in the supplemental S4.2) for direct comparison. Our full architecture therefore has three modular components to additionally condition on language:
1. A natural language task descriptions encoder ED(d). This receives the task description d as input. We implement this as an RNN model using a bidirectional GRU (Cho et al., 2014) with 64 hidden units; we embed natural language symbols as 64-dimensional vectors, and randomly initialize and backpropagate through the embedding during training. We tokenize the sentences in u on whitespace and concatenate each sentence, delimited by special start and end of sentence tokens. At test time, we replace any OOV tokens with a special UNK token.
2. A domain-speciﬁc task encoder E(t), following S4.2. 3. A bigram transition model over program primitives,
following S4.2. To condition jointly on ED(d) and E(t) we simply concatenate these two embeddings and update the ﬁrst layer of the MLP to take the 128dimensional concatenated embeddings as input.

Supplemental: Leveraging Language to Learn Program Search Heuristics and Abstractions

5.3 Abstraction learning as joint model compression Finally, the abstraction learning model in (Ellis et al., 2021) can also be generalized to condition on language, by extending the optimal library inference algorithm with respect to the program prior to an optimal library inference algorithm with respect to the joint model over language and programs (Eq. 6 and 7, main text.) In our concrete implementation with respect to the DreamCoder algorithm, this means extending the descriptionlength compression objective – originally deﬁned over the program library and training task programs – to include the translation model deﬁnition. The main paper deﬁnes a description-length prior over the compositional translation model (Eq. 10). Optimizing this tractably requires redeﬁning the abstraction algorithm in (Ellis et al., 2021) – which refactors λ-calculus programs via lambda-abstraction (see S4.3 for a summary) – to also jointly re-estimate the description length of the translation model T (dt|ρ, L ) using the refactored programs under the new candidate library L . We implement an efﬁcient approximation that can be calculated with respect to the classical statistical translation model described in S4.1 (Gal & Blunsom, 2013). In particular, we leverage the alignment-based deﬁnition (which uses latent correspondences inferred between program tokens and sentence tokens in paired programs and descriptions) to approximate −H(PT [w|l]) = − log(PT [w|l]), the entropy of the token-token translation probabilities. Speciﬁcally, as the IBM model deﬁnes the conditional tokentoken probabilities

PT [w|l] ∝ ... P[w, a1...am|l]

a1

am

marginalized over alignments, where (slightly abusing notation) in any given paired program and sentence description we will have estimated a set of alignments awj,lk...ln between the j-th token in the description corresponding to one or more tokens lk...ln in the paired program. We therefore deﬁne the description-length of each token-token translation as the sum of the description lengths of the alignments which express it under a library L:

... P[d, a1...am|l, L] ∝ ... |ai|L

ai

am

a1

am

and the description lengths under the refactored library L containing new abstractions compresses according to

|awj ,lk...ln |L < |awj ,lk...ln |L ⇐⇒ (3) {licontains only lk...ln as subcomponents|lk...ln} and we say that a primitive l ∈ L is a subcomponent of a refactored abstraction l ∈ L if the abstraction can be

β-reduced such that l appears in it. That is, a refactored alignment a : wi → {l ...ln} is compressed only when a new abstraction l encapsulates over a strict subset of the constituent program primitives already aligned to the word in the original alignment. This allows us to re-approximate the description length of the new translation model with respect to a semantically-equivalent program refactoring without inducing PT [w|l] from scratch (which would require retraining the full translation model over the sentences and refactored programs.)
S6. Experiments
This section describes additional details on each of the domains – string editing, compositional graphics, and scene understanding – in Section 6 of the main paper (see Figure 2, main text for examples from all three domains, shown along with the synthetic and human language annotations). We also provide additional details on the model and baseline hyperparameters available for each domain. All datasets generated for these experiments (including human language annotations) are released and links to static repositories are provided in the code release. We also release a complete set of commands to exactly replicate all model experiments. All experiments for were conducted on a high-powered computing cluster using a ﬁxed training budget of wall-clock search time per task for all models and baselines in a given domain (determined via hyperparameter search using the baseline model per domain, and reported on a per-domain basis below). The experiments on the string editing and graphics domains used models trained using 48 CPUs for search (using the original parallel enumerative search implemented in the released code for the DreamCoder model in (Ellis et al., 2021)); and the experiments trained on the scene reasoning task used 24 CPUs (as preliminary experiments revealed that these experiments required shorter search time for our main model, and we wished to reduce the carbon footprint of the remaining experiments after our ﬁrst two domains.) For all experiments we train the neural models for 1 ×104 gradient steps. For experiments with language-guided compression, we use an upper bound of 5 new abstractions introduced per iteration. For mutual exclusivity experiments, we set αME = 0.1. For all experiments, during programonly compression (see (Ellis et al., 2021) for a discussion of program-only compression hyperparameters) we use the hyperparameters from (Ellis et al., 2021) for parsimony with earlier work: a structure penalty of 1.5 and pseudocounts = 30.

Supplemental: Leveraging Language to Learn Program Search Heuristics and Abstractions

S6.1 Domains (See Figure 2, main text for examples from all three domains, shown along with the synthetic and human language annotations.) As discussed in the main paper, each domain consists of a dataset of tasks; a set of procedurally generated synthetic language annotations; and a set of human language annotations provided by Mechanical Turk workers; we also described the base primitives L0 with which all models (including baselines and ablations) were initialized for each domain.
S6.1.1 STRING EDITING Tasks: structured string transformation problems taken from a publicly released dataset in (Andreas et al., 2017) (n=1000 train; n=500 test). Tasks consist of input dictionary strings transformed using randomly sampled regular expression transducer (n=30 examples per task). Transducers were sampled according to abstract templates deﬁned in (Andreas et al., 2017) and required identifying matched sequences of characters and adding letters before them; removing sequences; replacing them with new sequences, or doubling the sequence each time they appeared (See Figure 2A, main text). Language data: The human language dataset for this domain was previously collected by (Andreas et al., 2017). We deﬁned a synthetic grammar of high-level templates over the ground truth regular expression transducers (corresponding to the original templates used to generate the tasks.) The synthetic templates were deﬁned based on language from the original human annotations, and in most cases closely matched the true human provided annotations (which were generally quite structured), though with signiﬁcantly less variation (the original language contained multiple human descriptions per task. We generate a single synthetic for each one. The synthetic dataset has a vocabulary size of n=44 for both train and test. We use the human annotations in the original dataset when evaluating on human data, which have a vocabulary of n=727 (train) and n=622 (test).) We generate a synthetic dataset on this domain partly because of inaccuracies noted in (Andreas et al., 2017). The released code contains the complete generation procedure for these synthetic annotations. See Figure 2A for representative tasks with examples, synthetic language, and human descriptions. Initial program primitives: We initialize all models with a set L0 of LISP-like primitives that operate over substring sequences to both construct regular expression match sequences and manipulate strings, augmented with three text manipulation-speciﬁc primitives intended for executing constructed regular expression sequences; t is a polymorphic type variable using standard Hindley-Milner polymorphism typing (Pierce, 2002). The execution engine does include

a regex-matching model; however, the synthesis model is naive to this execution engine and simply searches for manipulations over the input strings and the regexes as data arrays. L0 contains 14 substring manipulation primitives, given below with type information. We also give a semantic gloss for primitives that are not standard LISP primitives.
• if (bool → t → t → t) • cons (t → list(t) → list(t)) • car (list(t) → t) • cdr list(t) → list(t • map ((t0 → t1) → list(t0) → list(t1)) • tail (list(t) → t) • append (t → list(t) → list(t))
Appends element to end of list. • revcdr (list(t) → list(t))
Takes all except the last element of the list. • match (substr → substr → bool)
Returns true if the ﬁrst argument, when executed as a regular expression, matches the second argument. • regexsplit (substr → fullstr → list(substr)) Attempts to execute the ﬁrst argument as a regular expression, and splits the second argument into a list of substrings, using the regular expression match as a delimiter (and includes the matched sequences in the returned list.) • flatten (list(substr) → fullstr) Flattens a list of substrings back into a string. • rconcat (substr → substr → substr) Concatenates two substrings. • rnot (substr → substr) Takes a substring argument s and returns the substring literal [ˆ s] • ror (substr → substr → substr) Takes substring literals a and b and returns the substring literal ((a)—(b))
We also include 26 character constants of type substr and constants dot (regular expression wildcard character) and empty (empty string). Domain hyperparameters We largely follow prior work (Ellis et al., 2021) to set algorithm training parameters; the

Supplemental: Leveraging Language to Learn Program Search Heuristics and Abstractions

earlier (Ellis et al., 2021) uses a 720s enumerative search budget for solving both text editing and general list manipulation tasks. We use the same 720s enumerative budget here. The encoder E(t) follows the domain-speciﬁc encoder used for text and list editing problems in (Ellis et al., 2021), a 2-layer GRU with 64 hidden units. The model is trained for a ﬁxed gradient step budget (10,000 gradient steps) and we sample equally at random between supervision on the solved training tasks (and their solution programs in the current DSL) and samples from the joint generative model. As with (Ellis et al., 2021), when generating tasks from the generative model, we use randomly sample inputs (on which we execute generated programs to produce an output.)
S6.1.2 COMPOSITIONAL GRAPHICS Tasks: inverse graphics problems (n=200 train; n=111 test) where each synthesis problem is speciﬁed by an image and solved by synthesizing a program in LOGO Turtle graphics (Abelson & DiSessa, 1986). The domain is inspired by the graphics domain in (Ellis et al., 2021) but intentionally re-designed to be much more challenging (ground-truth programs are much longer on average in the base programming language) and explicitly compositional: the training and testing tasks contain simple shape tasks deﬁned by compositional parameters for a set of basic shapes (a small triangle, a medium square; a small semicircle); complex shape tasks that require inferring more challenging (and longer) parameterized shapes (a greek spiral with eight turns); and compositional tasks deﬁned by geometric rules and relations over the simple shapes (a seven sided snowﬂake with a short line and a small triangle as arms; a small triangle connected by a big space from a small circle) (See Figure 2C). Simple parameterized shapes are either polygons (triangle, square, [n] gon), curves (semicircle, circle) or lines. Simple shapes are parameterized by one of three sizes (small or short; medium; and big). When generating synthetic language descriptions, pluralized objects are tokenized with separate tokens for the noun lemma and a token for the plural sufﬁx (e.g. square s). Complex parameterized shapes require constructing more complex images out of basic lines, and are intended to evaluate performance on tasks that pose a greater search challenge in the initial DSL, and whose structure is not directly cued by compositional relationships over easier components. Further, the complex shapes can be solved using abstractions (e.g. for repeatedly rotating a pen at right angles) that are not directly cued by shared lexical names – we evaluate the algorithm’s ability to learn and use abstractions that correspond to useful sublexical structures shared across multiple lexemes. We deﬁne four template families for complex shapes: spirals, staircases, zigzags, and stars.

Compositional graphics tasks invoke compositional relationships over the simple parameterized shapes. We deﬁne templates for generating 6 families of compositional tasks: nested, next to, separated by, connected by, in a row, and snowﬂakes. Language data: We gather human language annotations by asking Mechanical Turk workers to write an image description for the rendered graphics images that specify each task. Each worker labeled 20 training and 10 testing images after viewing a disjoint, randomly sampled set of 15 example images paired with their synthetic language captions. (Workers were asked to write a short, clear description that a person or robot could use to recreate the picture, and told that the examples were paired with automatically generated captions as an example of the kinds of descriptions you could write for this picture.) We control for description quality by requiring workers to complete a reference task on their own descriptions: after writing their initial annotations, workers were required to correctly match each annotation to the target image (from amidst a set of 12 distractors drawn heuristically from similar images on the full task dataset, and other images they themselves had described), and only annotations correctly matched to the target image were retained (workers were given a chance to redescribe pictures they failed to match to their own captions.) We preprocess the human dataset minimally to standardize number terms (e.g. we use the same token type for both 3 and three) and to split plurals into a lemma and sufﬁx, as in the synthetic dataset. The ﬁnal dataset has a vocabulary size of n=562 for both train and test.
As with the string editing domain, we deﬁne a synthetic dataset using parameterized templates based on systematic language reused in the human annotations (see Figure 2A for a comparison between human annotations and synthetic language); as with that domain, we choose a synthetic dataset to ensure systematic re-use of high level terms for repeated compositional objects (such as the “n-gon” or “snowﬂake” terminology.)
We then generate graphics tasks by deﬁning parameterized templates over ground truth programs in L0, and a corresponding generator for synthesizing natural language descriptions based on each ground truth program. It is important to note that the templates are deﬁned at any extremely high level and were written with respect to low-level programs in a simple graphics language (many of which were derived by generalizing compositionally over complex structures in (Ellis et al., 2021), such as the ‘snowﬂake’ images). Initial program primitives: For comparison with prior work, our initial library on this domain (and the base language used to generate the ground truth graphics programs) is an implementation of the LOGO Graphics DSL used in (Ellis et al., 2021), which consists of four typed, impera-

Supplemental: Leveraging Language to Learn Program Search Heuristics and Abstractions

tive primitives modeled within the λ−calculus with a state monad S:
move: distance → angle → S → S pen-up: (S → S) → S → S for: int → (S → S) → S → S get/set: (S → S) → S → S
as well as four arithmetic operators (+, -, *. /), integer constants (1-9), unit distances and angles (1 meter and 2π radians), and special values ∞ and . Figure 3 (main text) shows examples of the graphics tasks, synthetic descriptions, human descriptions, and sample programs in the ground truth initial DSL. Domain hyperparameters We largely follow prior work (Ellis et al., 2021) to set algorithm training parameters. Consistent with the graphics program experiments in (Ellis et al., 2021), we train all models, including baselines and ablations, using an enumerative search budget of 1800s per task (both when using pure enumerative search from the DSL prior, and neurally-guided search conditioned on the task examples and language descriptions); the results in Table 1 compare the relative advantage of our model given this ﬁxed search time. We train all models on 48 CPUs during parallel enumerative search, and run the algorithm for a maximum of 27 iterations (see learning curves. As we run multiple random seed replications of models in this domain, we tuned the iteration limit based on performance on the ﬁrst replication, allowing models models to train while performance continued to increase. To conserve computational resources, we later stopped several of our own model replications before 27 iterations, as they had reached near ceiling performance. As we report the best held-out test score across all 27 iterations for any one model, the early stopping would only serve to give a conservative estimate on performance for these models.) We randomly reorder the training set of tasks once before the ﬁrst loop, then iterate through batches of n=40 tasks at each iteration; learning curves show results from evaluating on held-out tasks every n=3 iterations. The encoder E(t) follows the domain-speciﬁc encoder used for the original graphics domain in (Ellis et al., 2021) for a more direct comparison: we use a 6-layer CNN, where each layer consists of a 64x64 2D convolutional sublayer with kernel size = 3, a RELU activation sublayer, and a maxpooling sublayer with kernel size = 2. The model is trained for a ﬁxed gradient step budget (10,000 gradient steps) and we sample equally at random between supervision on the solved training tasks (and their solution programs in the current DSL) and samples from the joint generative model.

S6.1.3 SCENE REASONING Tasks: inductive scene reasoning tasks (n= 212 train; n=115 test) where each synthesis problem is speciﬁed by a structured input scene, and outputs can be a number (how many red rubber things are there?), a boolean value (are there more blue things than green things?), or another scene (what if all of the red things turned blue?). This domain is modeled on CLEVR (Johnson et al., 2017) but designed to support non-linguistic, inductive synthesis in the programming-byexample paradigm: each task is speciﬁed with n=7 paired input output examples. See Figure 2B, main text for example tasks showcasing the original and extended templates, synthetic language annotations, and human language annotations. The dataset includes questions randomly generated from the following subset of the original CLEVR question templates (see (Johnson et al., 2017) for additional details on the task generation process and question templates; we also release our own augmented question generation code and the full dataset):
• zero hop: questions that require counting or answering an attribute query about a subset of objects in the scene. (e.g. How many small cylinders are there?; What material is the purple thing?).
• one hop: questions similar to the zero hop tasks, but that require reasoning over an additional relational query (e.g What number of things are right the small gray thing?).
• single or: questions that additionally introduce a disjunction between sets of objects. (e.g. How many objects are either large metal spheres or large rubber things?)).
• (compare integer: questions that additionally introduce a ≥ or ≤ operator between counts of sets of objects. (e.g. Is the number of large rubber cubes less than the number of large green rubber things?)
• same relate: questions that additionally require reasoning about other objects with the same attribute as a speciﬁed object. (e.g. How many other things are there of the same size as the cyan thing?).
We choose these templates as a representative subset of the style of the full CLEVR dataset, that requires the full language of high-level primitives in (Johnson et al., 2017) to solve. We omit some longer questions in the same format (e.g. two hop) as our intention is to compare synthesis baselines, rather than to achieve SOTA performance on CLEVR: this would likely only increase the computing resources needed to compare the various methods and we

Supplemental: Leveraging Language to Learn Program Search Heuristics and Abstractions

already found a signiﬁcant differential between our model and the baselines on the shorter questions.) We also add new question templates generated in the style of the original CLEVR tasks, but designed to model other common AI tasks (such as generating new scenes based on existing ones) and to require new abstractions (that were not expressible in the original restricted symbolic language used to generate scenes in (Johnson et al., 2017)):
• localization: questions for object localization. These return an output scene consisting of a localized set of objects based on a set of query attributes (e.g. Find the gray rubber thing.).
• remove: questions that either return an output scene with a subset of the objects removed, or that query about latent scenes where a subset of objects has bee removed. (e.g What if you removed all of the gray metal things?; If you removed the green cubes, how many cubes would be left?).
• transform: questions that either return an output scene where a subset of the objects has been transformed to set new attributes, or that query about latent scenes where a subset of objects has been modiﬁed this way. (e.g What if all the blue metal things became rubber things?; If all of the large yellow rubber things became gray spheres, how many gray spheres would there be?).
We treat these as program synthesis tasks: the input scenes are speciﬁed as symbolic scene graphs consisting of an array of structured, objects deﬁned as a dictionary of their attributes, and programs are designed to manipulate these structured arrays (this data structure is the original format in which scenes themselves are generated in (Johnson et al., 2017); the images displayed in Figure 3, main text are rendered using the original image rendering pipeline). Our intention is not to build a visual reasoning architecture: rather, we are interested in learning structured manipulations of scenes. We see work in inverse graphics (such as (Yi et al., 2018)) which outputs a structured scene graph based on pixel images as the ﬁrst step in a symbolic processing and reasoning pipeline as analogous; we are interested in the structured manipulation of these scene representations. Language data: Synthetic language annotations are generated based on the original high-level templates in (Johnson et al., 2017), as well as additional templates we deﬁne for the extended questions in the same style. We gather human language annotations by asking Mechanical Turk workers to write an instruction or question describing the set of inductive examples. However, due to the difﬁculty of solving certain tasks in a limited time frame based on the inductive examples alone (such as the questions about disjunctions

over scenes), we show Mechanical Turk workers the synthetic descriptions for this domain and ask them to write a semantically similar description that changes more than one word in the original caption, and that would be ”more natural for a human to understand”. This paraphrasing paradigm is similar to that used in (Wang et al., 2015), though we ﬁnd that in comparison to other domains it generates less diverse language data.) We remove all punctuation, tokenize on spaces, and use an additional domain heuristic to stem all plurals (e.g. cubes). Initial program primitives: We initialize all models with a set L0 of LISP-like primitives. These are similar to the initial list manipulation primitives used in the string editing domain: as both domains can be treated as manipulating structured arrays, we are interested in learning differentiated, domain-speciﬁc abstractions based on a very similar base language. L0 also includes primitives for querying attributes of objects on the domain (these are typed getters that simply query the object dictionary of attributes) and several domain-speciﬁc functions necessary for manipulating these attribute. We deliberately use a much more base level programming language than the high-level, domain-speciﬁc language hand-designed in (Johnson et al., 2017); our goal is to learn the necessary abstractions. We give a semantic gloss for primitives that are not standard LISP primitives.
• if (bool → t → t → t) • cons (object → list(object) →
list(object))
• car (list(object) → object) • map ((t0 → t1) → list(t0) → list(t1)) • fold ((list(t) → list(t)) → (t → list(t) →
list(t)) → list(t))
• len (list(t) → int) • > (list(t) → bool) • < (list(t) → bool) • set union (list(t) → list(t) →
list(t))
• set intersect (list(t) → list(t) → list(t))
• set difference (list(t) → list(t) → list(t))
• relate (object → relation → list(t)) Returns an array of objects that satisfy a spatial relation with respect to an input object.

Supplemental: Leveraging Language to Learn Program Search Heuristics and Abstractions

We also include equality comparators for each of the attribute types (e.g. eq color?; getters for each attribute, and setters for each attribute. We also include integer constants 0-9 for counting and constants for the attributes (blue, red, big, small, rubber, metal) based on the original object and spatial relation constants (Johnson et al., 2017). Domain hyperparameters: We run a coarse hyperparameter search based on the baseline model to set the domain hyperparameters. We train all models, including baselines and ablations, using an enumerative search budget of 1000s per task and run the models for a maximum of 5 iterations. we run multiple random seed replications reordering the training set, in the same way as the compositional graphics domain. The results in Table 1 also compare a curriculum ordering of the training set based on the number of tokens in the synthetic language captions (split on spaces.) The encoder E(t) is a variant of the RNN-based domainspeciﬁc encoder used for text and list editing problems in (Ellis et al., 2021) (as well as the string editing domain). The model is trained for a ﬁxed gradient step budget (10,000 gradient steps) and we sample equally at random between supervision on the solved training tasks (and their solution programs in the current DSL) and samples from the joint generative model. As with (Ellis et al., 2021), when generating tasks from the generative model, we use randomly sample inputs (on which we execute generated programs to produce an output.) We encode the symbolic scene data structures with the RNN by encoding a ﬂattened version of the scene graph. The scene graph is originally stored as a dictionary of attributes; when ﬂattened, we indicate the dictionary structure using special tokens to denote the keys and the start and end of any array delimiters (the original scene graph is fully reconstructable from the ﬂattened version.)
S 6.2 Results and Additional Qualitative Results In this section, we discuss additional qualitative results from an in depth exploration of the graphics domain that were omitted from the main paper for space, but provide additional insight on the behavior of the learned model in the hardest learning domain (based on the differential between baseline and LAPS-augmented performance.) Learned abstractions and synthesized programs. Figure S4 (supplement) show sample abstractions in the ﬁnal libraries Lf for the best performing models in the graphics domain as a concrete exemplar of abstractions that are learned and how they are used, along with sample tasks solved with these abstractions. The ﬁgures are shown as dependency graphs to indicate how progressively more complex abstractions build on abstractions at prior iterations of learning; we also show selected probabilities from the translation model (depicted are examples from the top-3

primitive translations for a given word; some primitives are not high probability translations for any word.) Joint generative model samples. Figure S3 (supplement) shows samples from the joint generative model on the graphics domain (programs from the library which are executed to produce the task example image, and translated to produce language annotations) at early and later stages of training, indicating that the joint model itself improves as learning improves, which itself allows better training for the conditional inference model and better abstraction guiding based on language.
References
Abelson, H. and DiSessa, A. A. Turtle geometry: The computer as a medium for exploring mathematics. MIT press, 1986.
Andreas, J., Vlachos, A., and Clark, S. Semantic parsing as machine translation. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pp. 47–52, 2013.
Andreas, J., Klein, D., and Levine, S. Learning with latent language. arXiv preprint arXiv:1711.00482, 2017.
Artzi, Y., Das, D., and Petrov, S. Learning compact lexicons for ccg semantic parsing. 2014.
Balog, M., Gaunt, A. L., Brockschmidt, M., Nowozin, S., and Tarlow, D. Deepcoder: Learning to write programs. arXiv preprint arXiv:1611.01989, 2016.
Cho, K., Van Merrie¨nboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H., and Bengio, Y. Learning phrase representations using rnn encoder-decoder for statistical machine translation. arXiv preprint arXiv:1406.1078, 2014.
Dechter, E., Malmaud, J., Adams, R. P., and Tenenbaum, J. B. Bootstrap learning via modular concept discovery. In Twenty-Third International Joint Conference on Artiﬁcial Intelligence, 2013.
Devlin, J., Uesato, J., Bhupatiraju, S., Singh, R., Mohamed, A.-r., and Kohli, P. Robustﬁll: Neural program learning under noisy i/o. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pp. 990– 998. JMLR. org, 2017.
Dumancic´, S. and Cropper, A. Inventing abstractions by refactoring knowledge.
Ellis, K., Morales, L., Sable´-Meyer, M., Solar-Lezama, A., and Tenenbaum, J. Learning libraries of subroutines for neurally–guided bayesian program induction. In Advances in Neural Information Processing Systems, pp. 7805–7815, 2018.

Supplemental: Leveraging Language to Learn Program Search Heuristics and Abstractions

Joint generative model from learned translation model T

Joint model samples : Iteration 3
𝑡̂ =

L, 𝜃

T

⍴"
Sample programs from prior

Translate to
𝑑& produce
language
𝑡̂ Epxroedcuuctee to
examples

𝑑& = a small five gons a small five gon a small square

a small five gon gon a small six gon short line

a small five gon and a medium five gon (f6 4 4 1 x)

𝜌" = (f0 5 0 1 x)

(f1 1 (f2 5 x))

Joint model samples : Iteration 15

small 5 gons in a row
(f41 (λ (x) ((f48 x)) (fn21 1 5 5 x))

a small 6 gon as arms
(f34 9 6 x)

3 small 5 gon and a small triangle
(f17 (λ (x) (get/set (λ (z) (f12 1 (f24 (x z))))(f20 (λ (u) (f15 5 u)) 1 3 3))) v)

a small square connected by a short line and a small triangle as arms
((f10 3 x) f34 (f31 (λ (x) x) (λ (y z) z)) 4 9 u))

Figure 3. (left) Joint generative model J over programs sampled from the DSL prior and natural language produced by the translation model T (D|L), inferred from solved training tasks. Samples from the model are used to train a neural synthesizer to guide search on more challenging, unsolved tasks. (right) Samples from the J generative model in the graphics domain shows how program complexity increases and generated language improves across iterations, as the system both adds richer abstractions to the DSL and learns better alignments over the solution set, enabling the trained neural model to solve more complex tasks
.

Original DSL primitives New primitives added through abstraction learning

for

. . .

move
pen-up
1 0.31 | line 0.31 | short 0.09 | a
2
3 0.91 | three 0.98 | triangle
4 0.94 | four 0.89 | square Learned translation probabilities p(π | u)

f0=(λ (x y z) (for x (λ (u v) (move z y v))))
move pen in parameterized loop

f4=(λ (x y z) (f0 x (/ 2π y) 1 z))

0.09 | small

rotates and draws a unit line

f9=(f0 ∞ ε)

0.07 | semicircle

a small semicircle (f19 (f9 0 x))
a medium semicircle (f3 (f9 0 x))
a big semicircle (f9 (* (/ ε 1) 5) x)

a small five gon (f5 5 x)
a small nine gon (f5 9 x)
a medium seven gon (f5 2 (f20 7 x))

f5=(λ (x y) (f4 x x y))

0.27 | gon 0.22 | small

rotational symmetry by number of sides

four small squares in a row (f5 2 (f6 1 4 4 x))
six small five gons in a row (f6 1 6 5 x)

a seven stepped staircase (f32 7 (get/set (λ (x) x) y))
a four stepped staircase (f32 4 (get/set (λ (x) x) y))
a five stepped zigzag (f25 (λ (x) x) 3 8 (f32 5 y)

f6=(λ (x y z u) (for y (λ (v w) (f5 z (f5 x w))) u))

...

f32=(λ (x) (for x (λ (y z) (move 1 (/ 2π 4) (move 1

(- 2π (/ 2π 4)) z)))))

1.0 | stepped 0.64 | staircase 0.36 | zigzag

... ... f14=(λ (x y) (for 7 (λ (z
u) (f9 x u)) y))

f17=(λ (x) (pen-up (λ (y) (f16 x y))))

f24=(λ (x y) (f23 (λ (z u) (f21 y 0 x u))))

0.16 | circle 0.08 | turns 0.09 | nested

0.67 | separated 0.15 | next 0.06 | space

0.09 | snowflake 0.09 | arms

a big circle (f14 (logo_DIVL 1 4) x)
a small circle (f14 (logo_DIVL ε 1) x)
two nested circles (f14 ε (f14 ε (f16 x)))

a small circle next to a small six gon (f14 ε (f14 ε (f17 2 (f5 6 x))))
a small nine gon next to a medium square (f5 9 (f5 1 (f17 1 (f20 4 x))))

eight sided snowflake with a small seven gon as arms (f24 7 8 x)
five sided snowflake with a short line and a medium five gon as arms (f24 5 (λ (x) (get/set (λ (y) (f2 1 (f41 5 y))) x)) z)

Figure 4. Abstractions and programs learned for the graphics domain. Sample abstractions (right) learned from a minimal starting DSL (left) for solving progressively more complex graphics program synthesis tasks with language annotations. Also shown with translation probabilities. Our iterative algorithm learns alignment-based translation probabilities between natural language words and program primitives to guide program search and abstraction (depicted are examples from the top-3 primitive translations for a given word; some primitives are not high probability translations for any word.

Supplemental: Leveraging Language to Learn Program Search Heuristics and Abstractions

Ellis, K., Wong, C., Nye, M., Sable´-Meyer, M., Cary, L., Morales, L., Hewitt, L., Solar-Lezama, A., and Tenenbaum, J. Dreamcoder: Bootstrapping inductive programsynthesis with wake-sleep library learning. PLDI 2021, 2021.
Frank, M. C., Goodman, N. D., and Tenenbaum, J. B. Using speakers’ referential intentions to model early crosssituational word learning. Psychological science, 20(5): 578–585, 2009.
Gal, Y. and Blunsom, P. A systematic bayesian treatment of the ibm alignment models. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 969–977, 2013.
Gandhi, K. and Lake, B. M. Mutual exclusivity as a challenge for deep neural networks. arXiv preprint arXiv:1906.10197, 2019.
Heaﬁeld, K. Kenlm: Faster and smaller language model queries. In Proceedings of the sixth workshop on statistical machine translation, pp. 187–197. Association for Computational Linguistics, 2011.
Johnson, J., Hariharan, B., Van Der Maaten, L., Hoffman, J., Fei-Fei, L., Lawrence Zitnick, C., and Girshick, R. Inferring and executing programs for visual reasoning. In Proceedings of the IEEE International Conference on Computer Vision, pp. 2989–2998, 2017.
Koehn, P., Hoang, H., Birch, A., Callison-Burch, C., Federico, M., Bertoldi, N., Cowan, B., Shen, W., Moran, C., Zens, R., et al. Moses: Open source toolkit for statistical machine translation. In Proceedings of the 45th annual meeting of the association for computational linguistics companion volume proceedings of the demo and poster sessions, pp. 177–180, 2007.
Lake, B. M., Salakhutdinov, R. R., and Tenenbaum, J. Oneshot learning by inverting a compositional causal process. In Advances in neural information processing systems, pp. 2526–2534, 2013.

Nye, M., Hewitt, L., Tenenbaum, J., and Solar-Lezama, A. Learning to infer program sketches. arXiv preprint arXiv:1902.06349, 2019.
Parisotto, E., Mohamed, A.-r., Singh, R., Li, L., Zhou, D., and Kohli, P. Neuro-symbolic program synthesis. arXiv preprint arXiv:1611.01855, 2016.
Pierce, B. C. Types and programming languages. MIT Press, 2002. ISBN 978-0-262-16209-8.
Polosukhin, I. and Skidanov, A. Neural program search: Solving data processing tasks from description and examples. 2018.
Shin, E. C., Allamanis, M., Brockschmidt, M., and Polozov, A. Program synthesis and semantic parsing with learned code idioms. In Advances in Neural Information Processing Systems, pp. 10824–10834, 2019.
Wang, Y., Berant, J., and Liang, P. Building a semantic parser overnight. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 1332– 1342, 2015.
Wong, Y. W. and Mooney, R. J. Learning for semantic parsing with statistical machine translation. In Proceedings of the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics, pp. 439–446. Association for Computational Linguistics, 2006.
Yi, K., Wu, J., Gan, C., Torralba, A., Kohli, P., and Tenenbaum, J. Neural-symbolic vqa: Disentangling reasoning from vision and language understanding. In Advances in Neural Information Processing Systems, pp. 1031–1042, 2018.
Zhang, Y., Pasupat, P., and Liang, P. Macro grammars and holistic triggering for efﬁcient semantic parsing. arXiv preprint arXiv:1707.07806, 2017.

Lake, B. M., Salakhutdinov, R., and Tenenbaum, J. B. Human-level concept learning through probabilistic program induction. Science, 350(6266):1332–1338, 2015.

Liang, P., Jordan, M. I., and Klein, D. Learning programs: A hierarchical bayesian approach. In Proceedings of the 27th International Conference on Machine Learning (ICML-10), pp. 639–646, 2010.

Markman, E. M. and Wachtel, G. F. Children’s use of mutual exclusivity to constrain the meanings of words. Cognitive psychology, 20(2):121–157, 1988.

