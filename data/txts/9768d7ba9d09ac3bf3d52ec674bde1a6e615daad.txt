Feeling the Bern: Adaptive Estimators for Bernoulli Probabilities of Pairwise Comparisons

arXiv:1603.06881v1 [cs.LG] 22 Mar 2016

Nihar B. Shah
Dept. of EECS UC Berkeley
nihar@eecs.berkeley.edu

Sivaraman Balakrishnan
Dept. of Statistics CMU
siva@stat.cmu.edu

Martin J. Wainwright
Dept. of EECS and Statistics UC Berkeley
wainwrig@berkeley.edu

Abstract
We study methods for aggregating pairwise comparison data in order to estimate outcome probabilities for future comparisons among a collection of n items. Working within a ﬂexible framework that imposes only a form of strong stochastic transitivity (SST), we introduce an adaptivity index deﬁned by the indiﬀerence sets of the pairwise comparison probabilities. In addition to measuring the usual worst-case risk of an estimator, this adaptivity index also captures the extent to which the estimator adapts to instance-speciﬁc diﬃculty relative to an oracle estimator. We prove three main results that involve this adaptivity index and diﬀerent algorithms. First, we propose a three-step estimator termed Cou√nt-Randomize-Least squares (CRL), and show that it has adaptivity index upper bounded as n up to logarithmic factors. We then show that that conditional on the hardness of planted√clique, no computationally eﬃcient estimator can achieve an adaptivity index smaller than n. Second, we show that a regularized leas√t squares estimator can achieve a poly-logarithmic adaptivity index, thereby demonstrating a n-gap between optimal and computationally achievable adaptivity. Finally, we prove that the standard least squares estimator, which is known to be optimally adaptive in several closely related problems, fails to adapt in the context of estimating pairwise probabilities.

1 Introduction
There is an extensive literature on modeling and analyzing data in the form of pairwise comparisons between items, with much of the earliest literature focusing on applications in voting, social choice theory, and tournaments. The advent of new internet-scale applications, particularly search engine ranking [RKJ08], online gaming [HMG07], and crowdsourcing [SBB+15], has renewed interest in ranking problems, particularly in the statistical and computational challenges that arise from the aggregation of large data sets of paired comparisons.
The problem of aggregating pairwise comparisons, which may be inconsistent and/or noisy, presents a number of core challenges, including: (i) how to produce a consensus ranking from the paired comparisons [BM08, RGLA15, SW15]; (ii) how to estimate a notional “quality” for each of the underlying objects [NOS12, HOX14, SBB+15]; and (iii) how to estimate the probability of the

outcomes of subsequent comparisons [Cha14, SBGW15]. In this paper, we focus on the third task— that is, the problem of estimating the probability that one object is preferred to another. Accurate knowledge of such pairwise comparison probabilities is useful in various applications, including (in operations research) estimating the probability of a customer picking one product over another, or (in sports bookmaking and tournament design) estimating the probability of one team beating another.
In more detail, given a set of n items {1, . . . , n}, the paired comparison probabilities can be described by an (n×n) matrix M ∗ in which the (i, j)th entry corresponds to the probability that item i beats item j. From this perspective, problem of estimating the comparison probabilities amounts to estimating the unknown matrix M ∗. In practice, one expects that the pairwise comparison probabilities exhibit some form of structure, and in this paper, in line with some past work on the problem, we assume that the entries of the matrix M ∗ satisfy the strong stochastic transitivity (SST) constraint. It is important to note that the SST constraint is considerably weaker than standard parametric assumptions that are often made in the literature—for instance, that the entries of M ∗ follow a Bradley-Terry-Luce [BT52, Luc59] or Thurstone [Thu27] model. The SST constraint is quite ﬂexible and models satisfying this constraint often provide excellent ﬁts to paired comparison data in a variety of applications. There is also a substantial body of empirical work that validates the SST assumption—for instance, see the papers [ML65, DM59, BW97] in the psychology and economics literatures.
On the theoretical front, some past work [Cha14, SBGW15] has studied the problem of estimating SST matrices in the Frobenius norm. These works focus exclusively on the global minimax error, meaning that the performance of any estimator is assessed in a worst-case sense globally over the entire SST class. It is well-known that the criterion of global minimax can lead to a poor understanding of an estimator, especially in situations where the intrinsic diﬃculty of the estimation task is highly variable over the parameter space (see, for instance, the discussion and references in Donoho et al. [DJKP95]). In such situations, it can be fruitful to benchmark the risk of an estimator against that of a so-called oracle estimator that is provided with side-information about the local structure of the parameter space. Such a benchmark can be used to show that a given estimator is adaptive, in the sense that even though it is not given side-information about the problem instance, it is able to achieve lower risk for “easier” problems (e.g., see the papers [Can06, Kol11, CL11] for results of this type).1 In this paper, we study the problem-speciﬁc diﬃculty of estimating a pairwise comparison matrix M ∗ by introducing an adaptivity index that involves the size of the indiﬀerence sets in the matrix M ∗. These indiﬀerence sets, which arise in many relevant applications, correspond to subsets of items that are all equally desirable.
In addition, our work makes contributions to a growing body of work (e.g., [BR13, MW15, Wai14]) that studies the notion of a computationally-constrained statistical risk.
1The term “adaptivity” in this paper derives its meaning from the literature on statistics, and refers to the property of an estimator of automatically adapting its performance to the complexity of the problem. It should not be confused with the notion of “adaptive sampling” used in the context of sequential design or adaptive learning, which refers to the ability to obtain samples one at a time in a sequential and data-dependent manner.
2

In more detail, we make the following contributions in this paper: • We show that the risk of estimating a pairwise comparison probability matrix M ∗ depends
strongly on the size of its largest indiﬀerence set. This fact motivates us to deﬁne an adaptivity index that benchmarks the performance of an estimator relative to that of an oracle estimator that is given additional side information about the size of the indiﬀerence sets in M ∗. By deﬁnition, an estimator with lower values of this index is said to exhibit better adaptivity, and the oracle estimator has an adaptivity index of 1. • We characterize the fundamental limits of adaptivity, in particular by proposing a regularized least squares estimator with a carefully chosen regularization function. With a suitable choice of regularization parameter, we prove that this estimator achieves an O(1) adaptivity index, which matches the best possible up to poly-logarithmic factors. • We then show that conditional on the planted clique hardness conjecture, √the adaptivity index achieved by any polynomial-time algorithm must be lower bounded as Ω( n). This result exhibits an interesting gap between the adaptivity of polynomial-time versus statistically optimal estimators. • We propose a computationally-eﬃcient three-step “Count–Randomize–Least squares” (CRL) estimato√r for estimation of SST matrices, and show that its adaptivity index is upper bounded as O( n). Due to the aforementioned lower bound, the CRL estimator has the best possible adaptivity among all possible computationally eﬃcient estimators. • Finally, we investigate the adaptivity of the standard (unregularized) least squares estimator. This estimator is found to have good, or even optimal adaptivity in several related problems, and is also minimax-optimal for the problem of estimating SST matrices. We prove that surprisingly, the adaptivity of the least squares estimator for estimating SST matrices is of the order Θ(n), which is as bad as a constant estimator that is independent of the data.
The remainder of this paper is organized as follows. We begin in Section 2 with background on the problem. Section 3 is devoted to the statement of our main results, as well as discussion of their consequences. In Section 4, we provide the proofs of our main results, with the more technical details deferred to appendices. Finally, Section 5 presents concluding remarks.
2 Background and problem setting
In this section, we provide background and a more precise problem statement.
2.1 Estimation from pairwise comparisons
Given a collection of n items, suppose that we arrange the paired comparison probabilities in a matrix M ∗ ∈ [0, 1]n×n, where Mi∗j is the probability that item i is preferred to item j in a paired comparison. Accordingly, the upper and lower halves of M ∗ are related by the shifted-skewsymmetry condition Mj∗i = 1 − Mi∗j for all i, j ∈ [n], where we assume that Mi∗i = 0.5 for all i ∈ [n]
3

for concreteness. In other words, the shifted matrix M ∗ − 12 11T is skew-symmetric. Here we have adopted the standard shorthand [n] : = {1, 2, . . . , n}.
Suppose that we observe a random matrix Y ∈ {0, 1}n×n with (upper-triangular) independent Bernoulli entries, in particular, with

P[Yij = 1] = Mi∗j

for every 1 ≤ i ≤ j ≤ n,

(1)

and Yji = 1 − Yij except on the diagonal. We take the diagonal entries Yii to be {0, 1} with equal probability, for every i ∈ [n]. The focus of this work is not to evaluate the eﬀects of the choice of the pairs compared, but to understand the eﬀects of the noise models. Consequently, we restrict attention to the case of a single observation per pair, but keeping mind in that one may extend the result to other observation models via techniques similar to those proposed in our past work [SBB+15, SBGW15]. Based on observing Y , our goal in this paper is to recover an accurate estimate, in the squared Frobenius norm, of the full matrix M ∗.
We consider matrices M ∗ that satisfy the constraint of strong stochastic transitivity (SST), which reﬂects the natural transitivity of any complete ordering. Formally, suppose that the set of all items [n] is endowed with a complete ordering π∗. We use the notation π∗(i) ≻ π∗(j) to indicate that item i is preferred to item j in the total ordering π∗. We say that the M ∗ satisﬁes the SST condition with respect to the permutation π∗—or is π∗-SST for short—if

Mi∗k ≥ Mj∗k for every triple (i, j, k) such that π∗(i) ≻ π∗(j).

(2)

The intuition underlying this constraint is as follows: since i dominates j in the true underlying order, when we make noisy comparisons, the probability that i is preferred to k should be at least as large as the probability that j is preferred to k. The class of all SST matrices is given by
CSST : = M ∈ [0, 1]n×n | Mba = 1 − Mab ∀ (a, b) and ∃ π such that M is π-SST . (3)

The goal of this paper2 is to design estimators that can estimate the true underlying matrix M ∗ ∈ CSST from the observed matrix Y .

2.2 Indiﬀerence sets

We now turn to the notion of indiﬀerence sets, which allows for a ﬁner-grained characterization of

the diﬃculty of estimating a particular matrix. Suppose that the set [n] of all items is partitioned

into the union of s disjoint sets {Pi}si=1 of sizes k = (k1, . . . , ks) such that

s i=1

ki

=

n.

For

reasons

to be clariﬁed in a moment, we term each of these sets as an indiﬀerence set. We write i ∼ i′ to

mean that the pair i and i′ belong to the same index set, and we say that a matrix M ∗ ∈ Rn×n

respects the indiﬀerence set partition {Pi}si=1 if

Mi∗j = Mi∗′j′ for all quadruples (i, j, i′, j′) such that i ∼ i′ and j ∼ j′.

(4)

2We note that an accurate estimate of M ∗ leads to an accurate estimate of the underlying permutation as well [SBGW15].

4

For instance, in the special case of a two-contiguous-block partition, the matrix M ∗ must have a (2 × 2) block structure, with all entries equaling 1/2 in the two diagonal blocks, all entries equaling α ∈ [0, 1] in the upper right block, and equaling (1 − α) in the lower left block. Intuitively, matrices with this type of block structure should be easier to estimate.

Indiﬀerence sets arise in various applications of ranking: for instance, in buying cars, frugal customers may be indiﬀerent between high-priced cars; or in ranking news items, people from a certain country may be indiﬀerent to the domestic news from other countries. Block structures of this type are also studied in other types of matrix estimation problems, in which contexts they have been termed communities, blocks, or level sets, depending on the application under consideration. For instance, see the papers [AS15, MW15, CGS15] as well as references therein for more discussion in such structures.

Given the number of partitions s and their size vector k = (k1, . . . , ks), we let CSST(s, k) denote the subset of CSST comprising all SST matrices that respect some indiﬀerence set partition {Pi}si=1 of sizes k. The size of the largest indiﬀerence set kmax : = k ∞ = max ki plays an important
i∈{1,...,s}
role in our analysis. We also use the notation CSST(kmax) to denote all SST matrices that have at
least one indiﬀerence set of size at least kmax, that is,

CSST(kmax) : =

CSST(s, k), .

k ∞≥kmax

Finally, with a minor abuse of notation, for any matrix M ∈ CSST, we let kmax(M ) denote the size of the largest indiﬀerence set in M .

2.3 An oracle estimator and the adaptivity index

We begin by deﬁning a benchmark based on the performance of the best estimator that has sideinformation that M ∗ ∈ CSST(s, k), along with the values of (s, k). We evaluate any such estimator M (s, k) based on its mean-squared Frobenius error

n

E[|||M (s, k) − M ∗|||2F] = E

Mij(s, k) − Mi∗j 2 ,

(5)

i,j=1

where the expectation is taken with respect to the random matrix Y ∈ {0, 1}n×n of noisy comparisons. With this notation, the (s, k)-oracle risk is given by

Rn(s, k) : = inf

sup E[|||M (s, k) − M ∗|||2F],

(6)

M (s,k) M ∗∈CSST(s,k)

where the inﬁmum is taken over all measurable functions M (s, k) of the data Y .

For a given estimator M that does not know the values of (s, k), we can then compare its performance to this benchmark via the (s, k)-adaptivity index

sup E[|||M − M ∗|||2F]

αn(M ; s, k) : = M∗∈CSST(s,Rk)n(s, k) .

(7a)

5

The global adaptivity index αn(M ) of an estimator M is then given by

αn(M ) : = max αn(M ; s, k).
s,k: k ∞<n

(7b)

In this deﬁnition, we restrict the maximum to the interval k ∞ < n since in the (degenerate) case of k ∞ = n, the only valid matrix M ∗ is the all-half matrix and hence the estimator with the knowledge of the parameters trivially incurs an error of zero.
Given these deﬁnitions, the goal is to construct estimators that are computable in polynomial time, and possess a low adaptivity index. Finally, we note that an estimator with a low adaptivity index also achieves a good worst-case risk: any estimator M with global adaptivity index αn(M ) ≤ γ is minimax-optimal within a factor γ.

3 Main results
In this section, we present the main results of this paper on both statistical and computational aspects of the adaptivity index. We begin with an auxiliary result on the risk of the oracle estimator which is useful for our subsequent analysis.

3.1 Risk of the oracle estimator

Recall from Section 2.3 that the oracle estimator has access to additional side information on the values of the number s and the sizes k = (k1, . . . , ks) of the indiﬀerence sets of the true underlying matrix M ∗. The oracle estimator is deﬁned as the estimator that incurs the lowest possible risk (6) among all such estimators.
The following result provides tight bounds on the risk of the oracle estimator.

Proposition 1. There are positive universal constants cℓ and cu such that the (s, k)-oracle risk (6) is sandwiched as

cℓ(n − kmax) ≤ Rn(s, k) ≤ cu(n − kmax + 1)(log n)2.

(8)

Proposition 1 provides a characterization minimax risk of estimation under various subclasses of CSST. Remarkably, the minimax risk depends on only the size kmax : = k ∞ of the largest indiﬀerence set: given this value, it is not aﬀected by the number of indiﬀerence sets s nor their sizes k. This property is in sharp contrast to known results [CGS15] for the related problem of bivariate isotonic regression, in which the number s of indiﬀerence sets does play a strong role.

Note

that

when

kmax

<

n,

we

have

1 2

(n

−

kmax

+ 1)

≤

(n − kmax),

and

consequently

the

lower

bound

in

(8)

can

be

replaced

by

cℓ 2

(n

−

kmax

+ 1).

6

3.2 Fundamental limits on adaptivity

Proposition 1 provides a sharp characterization of the denominator in the adaptivity index (7a). In this section, we investigate the fundamental limits of adaptivity by studying the numerator but disregarding computational constraints. The main result of this section is to show that a suitably regularized form of least-squares estimation has optimal adaptivity up to logarithmic factors.

More precisely, recall that kmax(M ) denotes the size of the largest indiﬀerence set in the matrix M . Given the observed matrix Y , consider the M -estimator

MREG ∈ arg min |||M − Y |||2F − kmax(M )(log n)3 .

(9)

M ∈CSST

Here the inclusion of term −kmax(M ), along with its logarithmic weight, serves to “reward” the estimator for returning a matrix with a relatively large maximum indiﬀerence set. As our later analysis in Section 3.5 will clarify, the inclusion of this term is essential: the unregularized form of least-squares has very poor adaptivity properties.

The following theorem provides an upper bound on the estimation error and the adaptivity of the estimator MREG.

Theorem 1. There are universal constants cu and c′u such that for every M ∗ ∈ CSST, the regularized least squares estimator (9) has squared Frobenius error at most

n12 |||M ∗ − MREG|||2F ≤ cu n − kmaxn(2M ∗) + 1 (log n)3,

(10a)

with

probability

at

least

1

−

e−

1 2

(log

n)2

.

Consequently,

its

adaptivity

index

is

upper

bounded

as

αn(MREG) ≤ c′u(log n)3.

(10b)

Since the adaptivity index of any estimator is at least 1 by deﬁnition, we conclude that the regularized least squares estimator MREG is optimal up to logarithmic factors.
The reader may notice that the optimization problem (9) deﬁning the regularized least squares estimator MREG is non-trivial to solve; it involves both a nonconvex regularizer, as well as a nonconvex constraint set. We shed light on the intrinsic complexity of computing this estimator in Section 3.4, where we investigate the adaptivity index achievable by estimators that are computable in polynomial time.

3.3 Adaptivity of the CRL estimator
In this section, we propose a polynomial-time computable estimator termed the Count-RandomizeLeast-Squares (CRL) estimator, and prove an upper bound on its adaptivity index. In order to deﬁne the CRL estimator, we requre some additional notation. For any permutation π on n items, let CSST(π) ⊆ CSST denote the set of all SST matrices that are faithful to the permutation π—that is
CSST(π) : = M ∈ [0, 1]n×n | Mba = 1 − Mab ∀ (a, b), Mik ≥ Mjk ∀ i, j, k ∈ [n] s.t. π(i) > π(j) . (11)

7

One can verify that the sets {CSST(π)} for all permutations π on n items form a partition of the SST class CSST.

The CRL estimator acts on the observed matrix Y and outputs an estimate MCRL ∈ CSST via a

three step procedure:

Step 1 (Count): For each i ∈ [n], compute the total number Ni =

n j=1

Yij

of

pairwise

comparisons

that it wins. Order the n items in terms of {Ni}ni=1, with ties broken arbitrarily. √

Step 2 (Randomize): Find the largest subset of items S such that |Ni − Nj| ≤ n log n for all

i, j ∈ S. Using the order computed in Step 1, permute this (contiguous) subset of items uniformly

at random within the subset. Denote the resulting permutation as πCRL.

Step 3 (Least squares): Compute the least squares estimate assuming that the permutation πCRL is

the true permutation of the items:

MCRL ∈ arg min |||Y − M |||2F.
M ∈CSST(πCRL)

(12)

The optimization problem (12) corresponds to a projection onto the polytope of bi-isotone matrices contained within the hypercube [0, 1]n, along with skew symmetry constraints. Problems of the form (12) have been studied in past work [BDPR84, RWDR88, Cha14, KRS15], and the estimator MCRL is indeed computable in polynomial time. By construction, it is agnostic to the values of (s, k).

To provide intuition for the second step of randomization, it serves to discard “non-robust” information from the order computed in Step 1. Any such information corresponds to noise due to the Bernoulli sampling process, as opposed to structural information about the matrix. If we do not perform this second step—eﬀectively retaining considerable bias from Step 1—then then isotonic regression procedure in Step√3 may amplify it, lea√ding to a poorly performing estimator. To clarify our choice of threshold T = n log(n), the factor n corresponds to the standard deviation of a typical win count Ni (as a sum of Bernoulli variables), whereas the log n serves to control ﬂuctuations in a union bound.

The following theorem provides an upper bound on the adaptivity index achieved by the CRL estimator.

Theorem 2. There are universal constants cu and c′u such that for every M ∗ ∈ CSST, the CRL estimator MCRL has squared Frobenius norm error

n12 |||MCRL − M ∗|||2F ≤ cu n − kmanx3(/M2 ∗) + 1 (log n)8,

(13a)

with probability at least 1 − n−20. Consequently, its adaptivity index is upper bounded as

αn (MCRL )

≤

c′

√ n(log

u

n)8.

(13b)

It is worth noting that equation (13a) in yields an upper bound on the minimax risk of the CRL estimator—namely

sup 1 E[|||M − M ∗|||2] ≤ cu (lo√g n)8 ,

M ∗∈CSST n2

CRL

F

n

8

with this worst-case achieved when kmax(M ∗) = 1. Up to logarithmic factors, this bound matches the best known upper bound on the minimax rate of polynomial-time estimators [SBGW15, Theorem 2].

3.4 A lower bound on adaptivity for polynomial-time algorithms

By comparing the guarantee (13b) for the CRL estimator with the corresponding guarantee (10b) for the regularized least-squares estimato√r, we see that (apart from log factors and constants), their adaptivity indices diﬀer by a factor of n. Given this polynomial gap, it is natural to wonder whether our analysis of the CRL estimator might be improved, or if not, whether there is another polynomial-time estimator with a lower adaptivity index than the CRL estimator. In this section, we answer both of these questions in the negative, at least conditionally on a certain well-known conjecture in average case complexity theory.
More precisely, we prove a lower bound that relies on the average-case hardness of the planted clique problem [Jer92, Kuˇc95]. The use of this conjecture as a hardness assumption is widespread in the literature [JP00, AAK+07, Dug14], and there is now substantial evidence in the literature supporting the conjecture [Jer92, FK03, MPW15, DM15]. It has also been used as a tool in proving hardness results for sparse PCA and related matrix recovery problems [BR13, MW15].
In informal terms, the planted clique conjecture asserts that it is hard to detect the presence of a planted clique in an Erd˝os-R´enyi random graph. In order to state it more precisely, let G(n, κ) be a random graph on n vertices constructed in one of the following two ways:
H0: Every edge is included in G(n, κ) independently with probability 12 . H1: Every edge is included in G(n, κ) independently with probability 21 . In addition, a set of κ
vertices is chosen uniformly at random and all edges with both endpoints in the chosen set are added to G. The planted clique conjecture then asserts that when κ = o(√n), then there is no polynomial-time algorithm that can correctly distinguish between H0 and H1 with an error probability that is strictly bounded below 1/2.
Using this conjectured hardness as a building block, we have the following result:

Theorem 3. Suppose that the planted clique conjecture holds. Then there is a universal constant cℓ > 0 such that for any polynomial-time computable estimator M , its adaptivity index is lower bounded as

αn(M

)

≥

√ cℓ n(log

n)−3.

Together, the upper and lower bounds of Theorems 2 and 3 imply that the estimator MCRL achieves the optimal adaptivity index (up to logarithmic factors) among all computationally eﬃcient estimators.

9

3.5 Negative results for the least squares estimator

In this section, we study the adaptivity of the (unregularized) least squares estimator given by

MLS ∈ arg min |||Y − M |||2F.

(14)

M ∈CSST

Least squares estimators of this type are known to possess very good adaptivity in various other problems of shape-constrained estimation; for instance, see the papers [C+11, CGS13, CL15, CGS15, Bel16] and references therein for various examples of such phenomena. From our own past work [SBGW15], the estimator (14) is known to be minimax optimal for estimating SST matrices.
Given this context, the following theorem provides a surprising result—namely, that the leastsquares estimator (14) has remarkably poor adaptivity:

Theorem 4. There is a universal constant cℓ > 0 such that the adaptivity index of the least squares estimate (14) is lower bounded as

αn(MLS) ≥ cℓ n (log n)−2.

(15)

In order to understand why the lower bound (15) is very strong, consider the trivial estimator M0

that

simply

ignores

the

data,

and

returns

the

constant

matrix

M0

=

1 2

11T

.

It

can

be

veriﬁed

that

we have

|||M ∗ − 21 11T |||2F ≤ 3n(n − kmax(M ∗) + 1)

for every M ∗ ∈ CSST. Thus, for this trival estimator M0, we have αn(M0) ≤ cun. Comparing to the lower bound (15), we see that apart from logarithmic factors, the adaptivity of the least squares estimator is no better than that of the trivial estimator M0.

4 Proofs
In this section, we present the proofs of our results. We note in passing that our proofs additionally lead to some auxiliary results that may be of independent interest. These auxiliary results pertain to the problem of bivariate isotonic regression—that is, estimating M ∗ when the underlying permutation is known—an important problem in the ﬁeld of shape-constrained estimation [RWDR88, THT11, Cha14]. Prior works restrict attention to the expected error and assume that the underlying permutation is correctly speciﬁed; our results provide exponential tail bounds and also address settings when the permutation is misspeciﬁed.
A few comments about assumptions and notation are in order. In all of our proofs, so as to avoid degeneracies, we assume that the number of items n is greater than a universal constant. (The cases when n is smaller than some universal constant all follow by adjusting the pre-factors in front of our results suitably.) For any matrix M , we use kmax(M ) to denote the size of the
10

largest indiﬀerence set in M , and we deﬁne k∗ = kmax(M ∗). The notation c, c1, cu, cℓ etc. all denote positive universal constants. For any two square matrices A and B of the same size, we let A, B = trace(AT B) denote their trace inner product. For an (n × n) matrix M and any permutation π on n items, we let π(M ) denote an (n × n) matrix obtained by permuting the rows and columns of M by π. For a given class C of matrices, metric ρ and tolerance ǫ > 0, we use N (ǫ, C, ρ) to denote the ǫ covering number of the class C in the metric ρ. The metric entropy is given by the logarithm of the covering number—namely log N (ǫ, C, ρ).
It is also convenient to introduce a linearized form of the observation model (1). Observe that we can write the observation matrix Y in a linearized fashion as

Y = M ∗ + W,

(16a)

where W ∈ [−1, 1]n×n is a random matrix with independent zero-mean entries for every i > j, and and Wji = −Wij for every i < j. For i > j, its entries follow the distribution

Wij ∼

1 − Mi∗j −Mi∗j

with probability Mi∗j with probability 1 − Mi∗j.

(16b)

In summary, all entries of the matrix W above the main diagonal are independent, zero-mean, and uniformly bounded by 1 in absolute value. This fact plays an important role in several parts of our proofs.

4.1 A general upper bound on regularized M-estimators

In this section, we prove a general upper bound that applies to a relatively broad class of regularized M -estimators for SST matrices. Given a matrix Y generated from the model (16a), consider an estimator of the form

M ∈ arg min |||Y − M |||2F + λ(M ) .

(17)

M ∈C

Here λ : [0, 1]n×n → R+ is a regularization function to be speciﬁed by the user, and C is some subset of the class CSST of SST matrices. Our goal is to derive a high-probability bound on the Frobenius norm error |||M − M ∗|||F. As is well-known from theory on M -estimators [vdG00, BBM05, Kol06],
doing so requires studying the empirical process in a localized sense.

In order to do so, it is convenient to consider sets of the form

CDIFF(M ∗, t, b, C) : = {α(M − M ∗) | M ∈ C, α ∈ [0, 1], b|||α(M − M ∗)|||F ≤ bt},

where t ∈ [0, n], and b ∈ {0, 1}. The binary ﬂag b controls whether or not the set is localized around M ∗, and the radius t controls the extent to which the set is localized.

11

In the analysis to follow, we assume that for each ǫ ≥ n−8, the ǫ-metric entropy of CDIFF(M ∗, t, b, C) satisﬁes an upper bound of the form

log N (ǫ, CDIFF(M ∗, t, b, C), ||| · |||F) ≤ t2b(g(ǫM2 ∗))2 + (h(M ∗))2,

(18a)

where g : Rn×n → R+ and h : Rn×n → R+ are some functions. In the sequel, we provide concrete examples of sets C and functions (g, h) for which a bound of this form holds.

Given (g, h, λ), we can then deﬁne a critical radius δn ≥ 0 as

δn2 = c (g(M ∗) log n)1+b + (h(M ∗))2 + λ(M ∗) + n−7 ,

(18b)

where c > 0 is a universal constant. The following result guarantees that the Frobenius norm can be controlled by the square of this critical radius:

Lemma 1. For any set C satisfying the metric entropy bound (18a), the Frobenius norm of the M -estimator (17) can be controlled as

P |||M − M ∗|||2F > uδn2 ≤ e−uδn2 for all u ≥ 1,

(19)

where δn is the critical radius (18b).

The signiﬁcance of this claim is that it reduces the problem of controlling the error in the M estimator to bounding the metric entropy (as in equation (18a)), and then computing the critical radius (18b). The remainder of this section is devoted to the proof of this claim.

4.1.1 Proof of Lemma 1

Deﬁne the diﬀerence ∆ = M − M ∗ between M ∗ and the optimal solution M to the constrained least-squares problem. Since M is optimal and M ∗ is feasible, we have
|||Y − M |||2F + λ(M ) ≤ |||Y − M ∗|||2F + λ(M ∗).

Following some algebra, and using the assumed non-negativity condition λ(·) ≥ 0, we arrive at the basic inequality

1 |||∆|||2 ≤ ∆, W + λ(M ∗),

2

F

where W ∈ [0, 1]n×n is the noise matrix in the linearized observation model (16a), and ∆, W denotes the trace inner product between ∆ and W .

Now deﬁne the supremum Z(t) : =

sup

D, W . With this deﬁnition, we ﬁnd that

D∈CDIFF(M ∗,t,b,C)

the error matrix ∆ satisﬁes the inequality

12 |||∆|||2F ≤ ∆, W + λ(M ∗) ≤ Z |||∆|||F + λ(M ∗). (20)

12

Thus, in order to obtain a high probability bound, we need to understand the behavior of the random quantity Z(t).
By deﬁnition, the set CDIFF(M ∗, t, b, C) is star-shaped, meaning that αD ∈ CDIFF(M ∗, t, b, C) for every α ∈ [0, 1] and every D ∈ CDIFF(M ∗, t, b, C). Using this star-shaped property, it is straightforward to verify that Z(t) grows at most linearly with t, ensuring that there is a non-empty set of scalars t > 0 satisfying the critical inequality:

E[Z(t)] + λ(M ∗) ≤ t2 .

(21)

2

Our interest is in an upper bound on the smallest (strictly) positive solution δn to the critical inequality (21). Moreover, our goal is to show that for every t ≥ δn, we have |||∆|||2F ≤ ctδn with probability at least 1 − c1e−c2tδn .

Deﬁne the “bad” event

At : = ∃∆ ∈ CDIFF(M ∗, t) | |||∆|||F ≥ tδn and ∆, W + λ(M ∗) ≥ 2|||∆|||F tδn . (22)

Using the star-shaped property of CDIFF(M ∗, t, b, C) and the fact that λ(·) ≥ 0, it follows by a rescaling argument that

P[At] ≤ P[Z(δn) + λ(M ∗) ≥ 2δn tδn] for all t ≥ δn.

The entries of W lie in [−1, 1], have a mean of zero, are i.i.d. on and above the diagonal, and satisfy skew-symmetry. Moreover, the function W → Z(u) is convex and Lipschitz with parameter u. Consequently, by Ledoux’s concentration theorem [Led01, Theorem 5.9], we have

P Z(δn) ≥ E[Z(δn)] + tδnδn ≤ e−c1tδn for all t ≥ δn,

for

some

universal

constant

c1.

By

the

deﬁnition

of

δn,

we

have

E[Z(δn)] + λ(M ∗)

≤

δn2

≤

√ δn tδn

for any t ≥ δn, and consequently

P[At] ≤ P[Z(δn) + λ(M ∗) ≥ 2δn tδn ≤ e−c1tδn for all t ≥ δn.

√

√

Consequently, either |||∆|||F ≤ tδn, or we have |||∆|||F > tδn. In the latte√r case, conditioning on

the complement Act , the basic inequality (20) implies that 12 |||∆|||2F ≤ 2|||∆|||F tδn. Putting together

the pieces yields that

|||∆|||F ≤ 4 tδn,

with

probability

at

least

1 − e−c1tδn

for

every

t

≥

δn.

Substituting

u

=

t δn

,

we

get

P |||∆|||2F > c2uδn2 ≤ e−c1uδn2 ,

(23)

for every u ≥ 1.
In order to determine a feasible δn satisfying the critical inequality (21), we need to bound the expectation E[Z(δn)]. To this end, we introduce an auxiliary lemma:

13

Lemma 2. There is a universal constant c such that for any set C satisfying the metric entropy bound (18a), we have

E[Z(t)] ≤ c tbg(M ∗) log n + t h(M ∗) + n−7

for all t ≥ 0.

(24)

See Section 4.1.2 for the proof of this claim. Using Lemma 2, we see that the critical inequality (21) is satisﬁed for

δn = c0

g(M ∗) log n 21 (b+1) + h(M ∗) +

λ(M

∗)

+

n−

7 2

,

for a positive universal constant c0. With this choice, our claim follows from the tail bound (23), absorbing the constants c1 and c2 into c0.

It remains to prove Lemma 2.

4.1.2 Proof of Lemma 2

By the truncated form of Dudley’s entropy inequality, we have

t

E[Z(t)] ≤ c inf nδ +

δ∈[0,n]

δ

2

log N (ǫ, CDIFF(M ∗, t, b, C), |||.|||F)dǫ

t

≤ c 2n−7 +

log N (ǫ, CDIFF(M ∗, t, b, C), |||.|||F)dǫ ,

(25)

n−8

where the second step follows by setting δ = 2n−8. Combining our assumed upper bound (18a) on the metric entropy with the earlier inequality (25) yields

E[Z(t)] ≤ c 2n−7 + tbg(M ∗) log(nt) + th(M ∗) ≤ 2c 2n−7 + tbg(M ∗) log n + th(M ∗) ,

where the ﬁnal step uses the upper bound t ≤ n. We have thus established the claimed bound (24).

4.2 Proof of Proposition 1
We are now equipped to prove bounds on the risk incurred by the oracle estimator from equation (6).

4.2.1 Upper bound
Let k∗ = k ∞ denote the size of the largest indiﬀerence set in M ∗, and recall that the oracle estimator knows the value of k∗. For our upper bound, we use Lemma 1 from the previous section with
C = CSST(k∗), λ(M ) = 0, and b = 0.

14

With these choices, the estimator (17) for which Lemma 1 provides guarantees is equivalent to the oracle estimator (6). We then have

CDIFF(M ∗, t, CSST(k∗)) = α(M − M ∗) | M ∈ C, α ∈ [0, 1] .

In order to apply the result of Lemma 1, we need to compute the metric entropy of the set C . DIFF For ease of exposition, we further deﬁne the set

CSST(k) : = {αM | M ∈ CSST(k), α ∈ [0, 1]}.

Since M ∗ ∈ CSST(k∗), the metric entropy of CDIFF is at most twice the metric entropy of CSST(k∗). The following lemma provides an upper bound on the metric entropy of the set CSST(k):

Lemma 3. For every ǫ > 0 and every integer k ∈ [n], the metric entropy is bounded as

(n − k + 1)2

n2

log N (ǫ, CSST(k), |||.|||F) ≤ c

ǫ2

log ǫ + c(n − k + 1) log n,

(26)

where c > 0 is a universal constant.

With this lemma, we are now equipped to prove the upper bound in Proposition 1. The bound (26) implies that
log N (ǫ, CDIFF(M ∗, t, CSST(k∗)), |||.|||F) ≤ c′ (n − kǫ∗2 + 1)2 (log n)2 + c′(n − k∗ + 1) log n, for all ǫ ≥ n−8. Consequently, a bound of the form (18a) holds with g(M ∗) = √c′(n − k∗ + 1) log n and h(M ∗) = c′(n − k∗ + 1) log n. Applying Lemma 1 with u = 1 yields
P |||M (s, k) − M ∗|||2F > c(n − k∗ + 1)(log n)2 ≤ e−(n−k∗+1)(log n)2 ,
where c > 0 is a universal constant. Integrating this tail bound (and using the fact that the Frobenius norm is bounded as |||M (s, k) − M ∗|||F ≤ n) gives the claimed result.

4.2.2 Lower bound

We now turn to proving the lower bound in Proposition 1. By re-ordering as necesseary, we may assume without loss of generality that k1 ≥ · · · ≥ ks, so that kmax = k1. The proof relies on the following technical preliminary that establishes a lower bound on the minimax rates of estimation when there are two indiﬀerence sets.

Lemma 4. If there are s = 2 indiﬀerence sets (say, of sizes k1 ≥ k2), then any estimator M has error lower bounded as

sup

1 E[|||M − M ∗|||2] ≥ cℓ n − k1 .

M ∗∈CSST(2,(k1,k2)) n2

F

n2

(27)

15

See Section 4.2.4 for the proof of this claim.
Let us now complete the proof of the lower bound in Proposition 1. We split the analysis into two cases depending on the size of the largest indiﬀerence set.

Case I: First, suppose that k1 > n3 . We then observe that CSST(2, (k1, n − k1)) is a subset of CSST(k): indeed, every matrix in CSST(2, (k1, n − k1)) can be seen as a matrix in CSST(k) which has identical

values in entries corresponding to all items not in the largest indiﬀerence set. Since the induced

set CSST(2, (k1, n − k1)) is a subset of CSST(k), the lower bound for estimating a matrix in CSST(k)

is at least as large as the lower bound for estimating a matrix in the class CSST(2, (k1, n − k1)).

Now

applying

Lemma

4

to

the

set

CSST(2, (k1, n − k1))

yields

a

lower

bound

of

cℓ

min{n−k1 n2

,k1

}

.

Since

k1

>

n3 ,

we

have

k1

≥

n

−k 2

1

.

As

a

result,

we

get

a

lower

bound

of

cℓ 2

n−k1 n2

.

Case II: Alternatively, suppose that k1 ≤ n3 . In this case, we claim that there exists a value u ∈ [n/3, 2n/3] such that CSST(2, (u, n − u)) is a subset of the set CSST(k) with k1 ≤ n3 . Observe that for any collection of sets with sizes k with k1 ≤ n3 , there is a grouping of sets into two groups, both of size between n/3 and 2n/3. This is true since the largest set is of size at most n/3. Denoting
the size of either of these groups as u, we have established our earlier claim.

As in the previous case, we can now apply Lemma 4 to the subset CSST(2, (u, n − u)) to obtain

a

lower

bound

of

cℓ

1 3n

≥

cℓ

n−k1 3n2

.

4.2.3 Proof of Lemma 3
In order to upper bound the metric entropy of CSST(k), we ﬁrst separate out the contributions of the permutation and the bivariate monotonicity conditions. Let CSST(id)(k) denote the subset of matrices in CSST(k) that are faithful to the identity permutation. With this notation, the ǫ-metric entropy of CSST(k∗) is upper bounded by the sum of two parts:
(a) the ǫ-metric entropy of the set CSST(id)(k); and
(b) the logarithm of the number of distinct permutations of the n items.
Due to the presence of an indiﬀerence set of size at least k, the quantity in (b) is upper bounded by log( nk!! ) ≤ (n − k) log n.
We now upper bound the ǫ-metric entropy of the set CSST(id)(k). We do so by partitioning the n2 positions in the matrix, computing the ǫ-metric entropy of each partition separately, and then adding up these metric entropies. More precisely, letting Sk ⊆ [n] denote some set of k items that belong to the same indiﬀerence set, let us partition the entries of each matrix into four sub-matrices as follows:
(i) The (k × k) sub-matrix comprising entries (i, j) where both i ∈ Sk and j ∈ Sk; (ii) the (k × (n − k)) sub-matrix comprising entries (i, j) where i ∈ Sk and j ∈ [n]\Sk; (iii) ((n − k) × k) sub-matrix comprising entries (i, j) where i ∈ [n]\Sk and j ∈ Sk; and
16

(iv) the ((n − k) × (n − k)) sub-matrix comprising entries (i, j) where both i ∈ [n]\Sk and j ∈ [n]\Sk.

By construction, the metric entropy of CSST(id)(k) is at most the sum of the metric entropies of these sub-matrices.

The set of sub-matrices in (i) comprises only constant matrices, and hence its metric entropy is

at most log nǫ . Any sub-matrix from set (ii) has constant-valued columns, and so the metric entropy of this set is upper bounded by (n − k) log nǫ . An identical bound holds for the set of sub-matrices in (iii). Finally, the set of sub-matrices in (iv) are all contained in the set of all ((n − k) × (n − k)) SST

matrices. The metric entropy of the SST class is analyzed in Theorem 1 of our past work [SBGW15],

where we showed that the metric entropy of this set is at most 2

n−k 2 ǫ

log

n−k ǫ

2 + (n − k) log n.

Summing up each of these metric entropies, some algebraic manipulations yield the claimed result.

4.2.4 Proof of Lemma 4

For the ﬁrst part of the proof, we assume k2 is greater than a universal constant. (See the analysis of Case 2 below for how to handle small values of k2.) Under this condition, the Gilbert-Varshamov bound [Gil52, Var57] guarantees the existence of a binary code B of length k2, minimum Hamming distance c0k2, and number of code words card(B) = T = 2ck2. (As usual, the quantities c and c0 are positive numerical constants.)

We now construct a set of T matrices contained within the set CSST(2, (k1, k2)), whose constituents have a one-to-one correspondence with the T codewords of the binary code constructed

above. Let items S = {1, . . . , k1} correspond to the ﬁrst indiﬀerence set, so that the complementary set Sc : = {k1 + 1, . . . , n} indexes the second indiﬀerence set.

Fix some δ ∈ (0, 13 ], whose precise value is to be speciﬁed later. Deﬁne the base matrix M (0)

with entries



 21

if i, j ∈ S or i, j ∈ Sc

Mij (0)

=

1
 2

+

δ

1 2

−

δ

if i ∈ S and j ∈ Sc if i ∈ Sc and j ∈ S.

For any other codeword z ∈ B, the matrix M (z) is deﬁned by starting with the base matrix M (0), and then swapping row/column i with row/column (k1 + i) if and only if zi = 1. For instance, if the codeword is z = [1 1 0 · · · 0], then the new ordering in the matrix M (z) is given by (k1 + 1), (k1 + 2), 3, . . . , k1, 1, 2, (k1 + 3), . . . , n, which is obtained by swapping the ﬁrst two items of the two indiﬀerence sets.
We have thus constructed a set of T matrices that are contained within the set CSST(2, (k1, k2)). We now evaluate certain properties of these matrices which will allow us prove the claimed lower bound. Consider any two matrices M1 and M2 in this set. Since any two codewords in our binary code have a Hamming distance at least c0k2, we have from the aforementioned construction:

c1k2nδ2 ≤ |||M1 − M2|||2F ≤ 2k2nδ2,

17

for a constant c1 ∈ (0, 1).

Let PM1 and PM2 correspond to the distributions of the random matrix Y based on Bernoulli sampling (1) from the matrices M1 and M2, respectively. Since δ ∈ (0, 13 ], all entries of the matrices M1 and M2 lie in the interval [1/3, 2/3]. Under this boundedness condition, the KL divergence may
be sandwiched by the Frobenius norm up to constant factors. Applying this result in the current
setting yields

c2k2nδ2 ≤ DKL(PM1 PM2) ≤ c3k2nδ2,

again for positive universal constants c2 and c3. An application of Fano’s inequality to this set gives that the error incurred by any estimator M is lower bounded as

sup

E[|||M − M ∗|||2 ] ≥ c1k2nδ2 1 − c3k2nδ2 + log 2 .

M ∗∈CSST(2,(k1,k2))

F

2

ck2

(28)

From this point, we split the remainder of the analysis into two cases.

Case 1: First suppose that k2 is larger than some suitably large (but still universal) constant. In

this

case,

we

may

set

δ2

=

c′′ n

for

a

small

enough

universal

constant

c′′,

and

the

Fano

bound

(28)

then implies that

sup

E[|||M − M ∗|||2F] ≥ c′k2,

M ∗∈CSST(2,(k1,k2))

for some universal constant c′ > 0. Since k2 = n − k1, this completes the proof the claimed lower bound (27) in this case.

Case 2: Otherwise, the parameter k2 is smaller than the universal constant in the above part of the proof. In this case, the claimed lower bound (27) on E[|||M − M ∗|||2F] is just a constant, and we can handle this case with a diﬀerent argument. In particular, suppose that the estimator is given
partition forming the two indiﬀerence sets, and only needs to estimate the parameter δ. For this
purpose, the suﬃcient statistics of the observation matrix Y are those entries of the observation
matrix that correspond to matches between two items of diﬀerent indiﬀerence sets; note that there
are k1k2 such entries in total. From standard bounds on estimation of a single Bernoulli probability, any estimator δˆ of δ must have mean-squared error lower bounded as E[(δ − δˆ)2] ≥ k1ck2 . Finally, observe that the error in estimating the matrix M ∗ in the squared Frobenius norm is at least 2k1k2 times the error in estimating the parameter δ. We have thus established the claimed lower bound
of a constant.

4.3 Proof of Theorem 1

We now prove the upper bound (10a) for the regularized least squares estimator (9). Note that it has the equivalent representation

MREG ∈ arg min |||Y − M |||2F + (n − kmax(M ) + 1)(log n)3 .

(29)

M ∈CSST

18

Deﬁning k∗ : = kmax(M ∗), it is also convenient to consider the family of estimators

Mk ∈ arg min

|||Y − M |||2F + (n − kmax(M ) + 1)(log n)3 ,

M ∈CSST(k)∪CSST(k∗)

(30)

where k ranges over [n]. Note that these estimators cannot be computed in practice (since the value of k∗ is unknown), but they are convenient for our analysis, in particular because MREG = Mk for some value k ∈ [n].

We ﬁrst show that there exists a universal constant c0 > 0 such that

P |||Mk − M ∗|||2F > c0(n − k∗ + 1)(log n)3 ≤ e−(log n)2

(31)

for each ﬁxed k ∈ [n]. Since MREG = Mk for some k, we then have

P |||MREG − M ∗|||2F > c0(n − k∗ + 1)(log n)3

≤ P max |||Mk − M ∗|||2F > c0(n − k∗ + 1)(log n)3
k∈[n]

(i)
≤

ne−(log

n)2

≤

e−

1 2

(l

og

n

)2

.

where step (i) follows from the union bound. We have thus established the claimed tail bound (10a).

In order to prove the bound (10b) on the adaptivity index, we ﬁrst integrate the tail bound (10a). Since all entries of M ∗ and MREG all lie in [0, 1], we have |||M ∗ −MREG|||2F ≤ n2, and so this integration step yields an analogous bound on the expected error:
E[|||M ∗ − MREG|||2F] ≤ cu(n − kmax(M ∗) + 1)(log n)3.

Coupled with the lower bound on the risk of the oracle estimator established in Proposition 1, we obtain the claimed bound (10b) on the adaptivity index of MREG.

It remains to prove the tail bound (31). We proceed via a two step argument: ﬁrst we use the general upper bound given by Lemma 1 to derive a weaker version of the required bound; and second, we then reﬁne this weaker bound so as to obtain the bound (31).

Establishing a weaker bound: Beginning with the ﬁrst step, let us apply Lemma 1 with the choices

b = 0, C = CSST(k) ∪ CSST(k∗), and λ(M ) = (n − kmax(M ) + 1)(log n)3. With these choices, the CDIFF(M ∗, t) in the statement of Lemma 1 takes the form
CDIFF(M ∗, t) ⊆ {α(M − M ∗) | α ∈ [0, 1], M ∈ CSST(k) ∪ CSST(k∗)}.

Lemma 3 implies that

log N (ǫ, CDIFF(M ∗, t), |||.|||F) ≤ c (n − min{ǫk2, k∗} + 1)2 (log n)2 + c(n − min{k, k∗} + 1) log n for all ǫ ≥ n−8. Applying Lemma 1 with u = 1 then yields

P |||Mk − M ∗|||2F > c(n − min{k, k∗} + 1)(log n)2 ≤ e−c(log n)2 .

(32)

Note that this bound is weaker than the desired bound (31), since min{k, k∗} ≤ k∗. Thus, our next step is to reﬁne it.

19

Reﬁning the bound (32): Before proceeding with the proof, we must take care of one subtlety. Recall that the set CSST(k∗) consists of all matrices in CSST that have an indiﬀerence set containing at least (but not necessarily exactly) k∗ items. If k ≥ k∗, then the bound (32) is equivalent to the bound (31). Otherwise, we evaluate the estimator Mk for the choices k = 1, . . . , k∗ − 1 (in this particular order). For any k ∈ {1, . . . , k∗ − 1} under consideration, suppose kmax(Mk) = k′ < k. Then the estimate under consideration is either also an optimal estimator for the case of Mk′, or it is suboptimal for the aggregate estimation problem (29). In the former case, the error incurred
by this estimate is already handled in the analysis of Mk′, and in the latter case, it is irrelevant. Consequently, it suﬃces to evaluate the case when kmax(Mk) = k.
Observe that the matrix Mk is optimal for the optimization problem (30) and the matrix M ∗ lies in the feasible set. Consequently, we have the basic inequality:

|||Y − Mk|||2F + (n − k + 1)(log n)3 ≤ |||Y − M ∗|||2F + (n − k∗ + 1)(log n)3.

Using the linearized form of the observation model (16a), some simple algebraic manipulations give
12 |||Mk − M ∗|||2F ≤ Mk − M ∗, W − (n − k + 1)(log n)3 + (n − k∗ + 1)(log n)3, (33) where W is the noise matrix (16b) in the linearized form of the model. The following lemma helps bound the ﬁrst term on the right hand side of inequality (33). Consistent with the notation elsewhere in the paper, for any value of t > 0, let us deﬁne a set of matrices CDIFF(M ∗, t) ⊆ CSST as
CDIFF(M ∗, t) : = {α(M − M ∗) | M ∈ CSST(k), α ∈ [0, 1], |||α(M − M ∗)|||F ≤ t}.

With this notation, we then have the following result:

Lemma 5. For any M ∗ ∈ CSST, any ﬁxed k ∈ [n], and any t > 0, we have

sup

D, W ≤ ct (n − min{k, k∗} + 1) log n + c(n − min{k, k∗} + 1)(log n)2 (34)

D∈CDIFF(M ∗,t)

with probability at least 1 − e−(log n)2 .

See Section 4.3.1 for the proof of this lemma. From our weaker guarantee (32), we know that |||Mk −M ∗|||F ≤ c′ (n − min{k, k∗} + 1)(log n)2,
with high probability. Consequently, the term Mk − M ∗, W is upper bounded by the quantity (34) for some value of t ≤ c′ (n − min{k, k∗} + 1)(log n)2, and hence
Mk − M ∗, W ≤ c′′(n − min{k, k∗} + 1)(log n)2,
with probability at least 1 − e−(log n)2. Applying this bound to the basic inequality (33) and performing some algebraic manipulations yields the claimed result (31).

20

4.3.1 Proof of Lemma 5

Consider the function ζ : [0, n] → R+ given by ζ(t) : = sup D, W . In order to control
D∈CDIFF(M ∗,t)
the behavior of this function, we ﬁrst bound the metric entropy of the set CDIFF(M ∗, t). Note that Lemma 3 ensures that
log N (ǫ, CDIFF(M ∗, t), ||| · |||F) ≤ c (n − min{ǫk2, k∗} + 1)2 log nǫ 2 + c(n − min{k, k∗} + 1) log n. Based on this metric entropy bound, the truncated version of Dudley’s entropy integral then guarantees that
E[ζ(t)] ≤ c(n − min{k, k∗} + 1)(log n)2 + ct (n − min{k, k∗} + 1) log n.

It can be veriﬁed that the function ζ(t) is t-Lipschitz. Moreover, the random matrix W has entries (16b) that are independent on and above the diagonal, bounded by 1 in absolute value, and satisfy skew-symmetry. Consequently, Ledoux’s concentration theorem [Led01, Theorem 5.9] guarantees that
P ζ(t) ≥ E[ζ(t)] + tv ≤ e−v2 for all v ≥ 0.

Combining the pieces, we ﬁnd that P ζ(t) ≥ c(n − min{k, k∗} + 1)(log n)2 + ct

(n − min{k, k∗} + 1) log n + tv ≤ e−v2,

valid for all v ≥ 0. Setting v = (n − min{k, k∗} + 1) log n yields the claimed result.

4.4 Proof of Theorem 2

We now prove the upper bound for the CRL estimator, as stated in Theorem 2. In order to simplify the presentation, we assume without loss of generality that the true permutation of the n items is the identity permutation id. Let πCRL = (π1, . . . , πn) denote the permutation obtained at the end of the second step of the CRL estimator. The following lemma proves two useful properties of the outcomes of the ﬁrst two steps.

Lemma 6. With probability at least 1 − n−20, the permutation πCRL obtained at the end of the ﬁrst

two steps of the estimator satisﬁes the following two properties:

(a) maxi∈[n]

n |M ∗ − M ∗

| ≤ √n(log n)2, and

ℓ=1 iℓ

πCRL(i)ℓ

(b) the group of similar items obtained in the ﬁrst step is of size at least k∗ = kmax(M ∗).

See Section 4.4.1 for the proof of this claim.

Given Lemma 6, let us complete the proof of the theorem. Let Π denote the set of all permuta-
tions on n items which satisfy the two conditions (a) and (b) stated in Lemma 6. Given that every entry of M ∗ lies in the interval [0, 1], any permutation πˆ ∈ Π satisﬁes

|||M ∗ − πˆ(M ∗)|||2F =

(Mi∗ℓ − Mπˆ∗(i)ℓ)2 ≤

| Mi∗ℓ − Mπˆ∗(i)ℓ | .

i∈[n] ℓ∈[n]

i∈[n] ℓ∈[n]

(35)

21

Now consider any item i ∈ [n]. Incorrectly estimating item i as lying in position πˆ(i) contributes a non-zero error only if either item i or item πˆ(i) lies in the (n − k∗)-sized set of items outside the largest indiﬀerence set. Consequently, there are at most 2(n − k∗) values of i in the sum (35) that
make a n√on-zero contribution. Moreover, from property (a) of Lemma 6, each such item contributes at most n(log n)2 to the error. As a consequence, we have the upper bound

|||M ∗ − πˆ(M ∗)|||2 ≤ 2(n − k∗)√n(log n)2.

(36)

F

Let us now analyze the third step of the CRL estimator. The problem of bivariate isotonic regression refers to estimation of the matrix M ∗ ∈ CSST when the true underlying permutation of the items is known a priori. In our case, the permutation is known only approximately, so that we need also to track the associated approximation error. In order to derive a tail bound on the error of bivariate isotonic regression, we call upon the general upper bound proved earlier in Lemma 1 with the choices b = 1, C = CSST(id), and λ = 0. Now let
CDIFF(M ∗, t) : = {α(M − M ∗) | M ∈ CSST(id)}.

The following lemma uses a result from the paper [CGS15] to derive an upper bound on the metric entropy of CDIFF(M ∗, t). For any matrix M ∗ ∈ CSST, let s(M ∗) denote the number of indiﬀerence sets in M ∗.
Lemma 7. For every ǫ > n−8 and t ∈ (0, n], we have the metric entropy bound
log N (ǫ, CDIFF(M ∗, t), |||.|||F) ≤ c t2(s(M ∗)ǫ)22(log n)6 . where c > 0 is a universal constant.
With this bound on the metric entropy, an application of Lemma 1 with u = (n(s−(Mk∗∗+)1)2)2 gives that for every M ∗ ∈ CSST(id), the least squares estimator Mid ∈ arg min |||M − Y |||2F incurs an error
M ∈CSST(id)
upper bounded as
|||Mid − M ∗|||2F ≤ c(n − k∗ + 1)2(log n)8,
with probability at least 1 − e−(n−k∗+1)2(log n)8 . Note that this application of Lemma 1 is valid since s(M ∗) ≤ n − k∗ + 1 and hence u ≥ 1. Furthermore, it follows from a corollary of Theorem 1 in the paper [SBGW15] that
|||Mid − M ∗|||2F ≤ cn(log n)2,
with probability at least 1 − e−cn. Combining these upper bounds yields
|||Mid − M ∗|||2F ≤ c min{(n − k∗ + 1)2, n}(log n)8 (≤i) c(n − k∗ + 1)√n(log n)8, (37)

22

with probability at least 1 − e−c(n−k∗+1)2(log n)8 , where c is a positive universal constant. Inequality (i) makes use of the bound min{u2, v2} ≤ uv for any two non-negative numbers u and v.
Let us put together the analysis of the approximation error (36) in the permutation obtained in the ﬁrst two steps and the error (37) in estimating the matrix in the third step. To this end, consider any permutation πˆ ∈ Π. For clarity, we augment the notation of MCRL (deﬁned in (12)) and use MCRL(Y, πˆ) to represent the estimator MCRL under the permutation πˆ for the observation matrix Y , that is,
MCRL(Y, πˆ) : = arg min |||M − Y |||2F.
M ∈CSST(πˆ)
Consider any matrix M ∗ ∈ CSST(id) under the identity permutation. We can then write
|||MCRL(M ∗ + W, πˆ) − M ∗|||2F = |||MCRL(M ∗ + W, πˆ) − MCRL(πˆ(M ∗) + W, πˆ) + MCRL(πˆ(M ∗) + W, πˆ) − M ∗|||2F ≤ 2|||MCRL(M ∗ + W, πˆ) − MCRL(πˆ(M ∗) + W, πˆ)|||2F + 2|||MCRL(πˆ(M ∗) + W, πˆ) − M ∗|||2F. (38)

We separately bound the two terms on the right hand side of equation (38). First observe that the least squares step of the estimator MCRL (for a given permutation πˆ in its second argument) is a projection onto the convex set CSST(πˆ), and hence we have the deterministic bound

|||MCRL(M ∗ + W, πˆ) − MCRL(πˆ(M ∗) + W, πˆ)|||2F ≤ |||M ∗ − πˆ(M ∗)|||2F.

(39a)

In addition, we have |||MCRL(πˆ(M ∗) + W, πˆ) − M ∗|||2F ≤ 2|||MCRL(πˆ(M ∗) + W, πˆ) − πˆ(M ∗)|||2F + 2|||πˆ(M ∗) − M ∗|||2F. (39b)

From our earlier bound (37), we have that for any ﬁxed permutation πˆ ∈ Π, the least squares estimate satisﬁes
|||MCRL(πˆ(M ∗) + W, πˆ) − πˆ(M ∗)|||2F ≤ cu(n − k∗ + 1)√n(log n)8, (40)
with probability at least 1 − e−c(n−k∗+1)2(log n)8 . In conjunction, the bounds (36), (38), (39a), (39b) and (40) imply that for any ﬁxed πˆ ∈ Π, P |||MCRL(M ∗ + W, πˆ) − M ∗|||2F ≤ cu(n − k∗ + 1)√n(log n)8 ≥ 1 − e−c(n−k∗+1)2(log n)8 . (41)

Although we are guaranteed that πCRL ∈ Π, we cannot apply the bound (41) directly to it, since πCRL is a data-dependent quantity. In order to circumvent this issue, we need to obtain a uniform version of the bound (41), and we do so by applying the union bound over the data-dependent component of πCRL.
In more detail, let us consider Steps 1 and 2 of the CRL algorithm as ﬁrst obtaining a total ordering of the n items via a count of the number of pairwise victories, then converting it to a partial order by putting all items in the subset identiﬁed by Step 2 in an equivalence class, and

23

then obtaining a total ordering by permuting the items in the equivalence class in a data-independent

manner. Lemma 6 ensures that the size of this equivalence class is at least k∗. Consequently, the

number

of

possible

(data-dependent)

partial

orders

obtained

is

at

most

n! k∗!

≤ e(n−k∗) log n.

Taking

a union bound over each of these e(n−k∗) log n cases, we get that

P |||MCRL(M ∗ + W, πCRL) − M ∗|||2F ≤ cu(n − k∗ + 1)√n(log n)8 | πCRL ∈ Π ≥ 1 − e−(log n)7 .

Recalling that Lemma 6 ensures that P πCRL ∈ Π ≥ 1 − n−20, we have established the claim.

It remains to prove the two auxiliary lemmas stated above.

4.4.1 Proof of Lemma 6

We ﬁrst prove that for any ﬁxed item i ∈ [n], the inequality of part (a) holds with probability at least 1 − n−22. The claimed result then follows via a union bound over all items.

Consider any item j > i such that

n ∗ n ∗√

2

Miℓ − Mjℓ > n(log n) .

ℓ=1

ℓ=1

(42)

An application of the Bernstein inequality then gives (see the proof of Theorem 1 in the paper [SW15] for details) that

n

n

1

P

Yjℓ ≥ Yiℓ ≤ n23 .

ℓ=1

ℓ=1

Likewise, for any item j < i such that

n ℓ=1

Yjℓ

≤

1 n23

.

n ℓ=1

Mj∗ℓ

−

n M ∗ > √n(log n)2, we have P
ℓ=1 iℓ

n ℓ=1

Yiℓ

≥

Now consider any j ≥ i. In order for item i to be located in position j in the total order given

by the row sums, there must be at least (j − i) items in the set {i + 1, . . . , n} whose row sums are

at least as big as the sum of the ith row of Y . In particular, there must be at least one item in the

set {j, . . . , n} such that its row sum is as big as the sum of the ith row of Y . It follows from our

results

above

that

under

the

condition

(42),

this

event

occurs

with

probability

no

more

than

1 n21

.

Likewise when j ≤ i, thereby proving the claim.

We now move to the condition of part (b). Observe that for any two items i and j in the same indiﬀerence set, we have that Mi∗ℓ = Mj∗ℓ for every ℓ ∈ [n]. An application of the Bernstein inequality now gives that

n

n

√

1

P Yjℓ − Yiℓ ≥ n log n ≤ n23 .

ℓ=1

ℓ=1

A union bound over all pairs of items in the largest indiﬀerence set gives that all k∗ items in √
the largest indiﬀerence set have their row sums diﬀering from each other by at most n log n.

Consequently, the group must be of at least this size.

24

4.4.2 Proof of Lemma 7

For the proof, it is be convenient to deﬁne a class CSST(; [-1,1]) that is similar to the class CSST(id), but contains matrices with entries in [−1, 1]:

CSST(t; [−1, 1]) : = M ∈ [−1, 1]n×n | |||M |||F ≤ t, Mkℓ ≥ Mij whenever k ≤ i and ℓ ≥ j .

We now call upon Theorem 3.3 of the paper [CGS15]. It provides the following upper bound on the metric entropy of bivariate isotonic matrices within a Frobenius ball:

t2(log n)4 log N (ǫ, CSST(t; [−1, 1]), ||| · |||F) ≤ c0 ǫ2 Substituting ǫ ≥ n−8 and t ≤ n yields

log t log n 2. ǫ

t2(log n)6

log N (ǫ, CSST(t; [−1, 1]), ||| · |||F) ≤ c ǫ2 .

(43)

We now use this result to derive an upper bound on the metric entropy of the set CDIFF(M ∗, t). Consider the following partition of the entries of any (n × n) matrix into (s(M ∗))2 submatrices. Submatrix (i, j) ∈ [s(M ∗)] × [s(M ∗)] in this partition is the (ki × kj) submatrix corresponding to the pairwise comparison probabilities between every item in the ith indiﬀerence set with every item in the jth indiﬀerence set in M ∗. Such a partition ensures that each partitioned submatrix of M ∗ is a constant matrix. Consequently, for any M ∈ CDIFF(M ∗, t), each partitioned submatrix
belongs to the set of matrices CSST(t; [−1, 1]) (where we slightly abuse notation to ignore the size
of the matrices as long as no dimension is greater than (n × n)). The metric entropy of the set of matrices in CDIFF(M ∗, t) can now be upper bounded by the sum of the metric entropies of each set
of submatrices. Consequently, we have

log N (ǫ, CDIFF(M ∗, t), ||| · |||F) ≤ (s(M ∗))2 log N (ǫ, CSST(t; [−1, 1]), ||| · |||F)

t2(s(M ∗))2(log n)6

≤c

ǫ2

,

where the ﬁnal inequality follows from our earlier bound (43).

4.5 Proof of Theorem 3

We now turn to the proof of the lower bound for polynomial-time computable estimators, as stated in Theorem 3. We proceed via a reduction argument. Consider any estimator that has Frobenius norm error upper bounded as
sup E[|||M − M ∗|||2F] ≤ cu√n(n − kmax(M ∗) + 1)(log n)−1. (44)
M ∗∈CSST

We show that any such estimator deﬁnes a method that, with probability at least 1 − √ 1 , is able

√

log n

to

identify

the

presence

or

absence

a

planted

clique

with

n log log n

vertices.

This

result,

coupled

with

25

the upper bound on the risk of the oracle estimator established in Proposition 1 proves the claim of Theorem 3.

Our reduction from the bound (44) proceeds by identifying a subclass of CSST, and showing that any estimator satisfying the bound (44) on on this subclass can be used to identify a planted clique in an Erd˝os-R´enyi random graph. Natu√rally, in order to leverage the planted clique conjecture, we need the planted clique to be of size o( n).

Our construction involves a partition with s = 3 components, maximum indiﬀerence set size

kmax = k1 = n − 2k√, with the remaining two indiﬀerence sets of size k2 = k3 = k. We choose the

parameter

k

:=

n log log n

so

that

any

constant

multiple

of

it

will

be

within

the

hardness

regime

of

planted clique (for suﬃciently large values of n). Now let M0∗ be a matrix with all ones in the

(k × k) sub-matrix in its top-right, zeros on the corresponding sub-matrix in the bottom-left and

all other entries set equal to 21 . By construction, the matrix M0∗ belongs to the class CSST(kmax)

with kmax = n − 2k.

For

any

permutation

π

on

n 2

items

and

any

(n × n)

matrix

M,

deﬁne

another

(n × n)

matrix

Pπ(M ) by applying the permutation π to:

•

the

ﬁrst

n 2

rows

of

M∗,

and

the

last

n 2

rows

of

M∗

•

the

ﬁrst

n 2

columns

of

M ∗,

and

to

the

last

n 2

columns

of

M∗.

We then deﬁne the set CSST : = Pπ(M0∗) | for all permutations π on [n/2] . By construction, it is a subset of CSST(n − kmax).

For any estimator M that satisﬁes the bound (44), we have

√

sup

E[|||M − M ∗|||2] ≤ ck n .

M ∗∈CSST∪{ 12 11T }

F

log n

On the other hand, Markov’s inequality implies that

√

√

E[|||M − M ∗|||2] > c √k n P |||M − M ∗|||2 ≥ c √k n .

F

log n

F

log n

Combining the two bounds, we ﬁnd that

√

P |||M − M ∗|||2 < √c k n ≥ 1 − √ 1 .

(45)

F

log n

log n

Consider

the

set

of

(

n 2

×

n 2

)

matrices

comprising

the

top-right

(

n 2

×

n 2

)

sub-matrix

of

every

matrix

in CSST. We claim that this set is identical to the set of all possible matrices in the planted clique

problem

with

n 2

vertices

and

a

planted

clique

of

size

k.

Indeed,

the

set

contains

the

all-half

matrix

corresponding to the absence of a planted clique, and all symmetric matrices that have all entries

equal to half except for a (k × k) all-ones submatrix corresponding to the planted clique.

Now consider the problem of testing the hypotheses of whether M ∗ is equal to the all-half

matrix (“no planted clique”) or if it lies in CSST (“planted clique”). Let us consider a decision rule

that

declares

the

absence

of

a

planted

clique

if

|||M

−

1 2

11T

|||2F

≤

116 k2,

and

the

presence

of

a

planted

clique otherwise.

26

Null case: antees that

On

one

hand,

if

there

is

no

planted

clique

(M ∗

=

1 2

11T

),

then

the

bound

(45)

guar-

√

|||M − 1 11T |||2 < k √ n

(46)

2

F

log n

√
with probability at least 1 − √lo1g n . Recalling that k = log long n , we ﬁnd that our decision rule can detect the absence of the planted clique with probability at least 1 − √lo1g n .

Case of planted clique: On the other hand, if there is a planted clique (M ∗ ∈ CSST), then we have

|||M − 1 11T |||2 ≥ 1 |||M ∗ − 1 11T |||2 − |||M − M ∗|||2 = 1 k2 − |||M − M ∗|||2.

2

F2

2

F

F4

F

Thus, in this case, the bound (45) guarantees that

√

|||M − 1 11T |||2 ≥ 1 k2 − √k n ,

2

F4

log n

√
with probability at least 1 − √lo1g n . Since k = log long n , our decision rule successfully detects the presence of a planted clique with probability at least 1 − √lo1g n .

In summary, given the planted clique conjecture, our decision rule cannot be computed in polynomial time. Since it can be computed in polynomial-time given the estimator M , it must also

be the case that M cannot be computed in polynomial time, as claimed.

4.6 Proof of Theorem 4
We now prove lower bounds on the standard least-squares estimator. A central piece in our proof is the following lemma, which characterizes an interesting structural property of the least-squares estimator.
Lemma 8. Let M ∗ = 21 11T and consider any matrix Y ∈ {0, 1}n×n satisfying the shifted-skewsymmetry condition. Then the least squares estimator MLS from equation (14) must satisfy the quadratic equation
|||Y − M ∗|||2F = |||Y − MLS|||2F + |||M ∗ − MLS|||2F.
See Section 4.6.1 for the proof of this claim.

Let us now complete the proof of Theorem 4 using Lemma 8. Our strategy is as follows: we ﬁrst construct a “bad” matrix M ∈ CSST that is far from M ∗ but close to Y . We then use Lemma 8 to show that the least squares estimate MLS must also be far from M ∗.

27

In the matrix Y , let item ℓ be an item that has won the maximum number of pairwise

comparisons—that is ℓ ∈ arg max

n j=1

Yj

i.

Let S denote the set of all items that are beaten

i∈[n]

by item ℓ—that is, S : = {j ∈ [n]\{ℓ} | Yℓj = 1}. Note that card(S) ≥ n−2 1 . Now deﬁne a matrix M ∈ CSST with entries Mℓ,j = 1 = 1 − Mj,ℓ for every j ∈ S, and all remaining entries equal to 21 .

Some simple calculations then give

|||Y − M ∗|||2F = |||Y − M |||2F + |||M ∗ − M |||2F, and

|||M − M ∗|||2 ≥ n − 1 .

F

4

(47a) (47b)

Next we exploit the structural property of the least squares solution guaranteed by Lemma 8. Together with the conditions (47) and the fact that |||Y −MLS|||2F ≤ |||Y −M|||2F, some simple algebraic manipulations yield the lower bound
|||M ∗ − MLS|||2F ≥ n −4 1 . (48)
This result holds for any arbitrary observation matrix Y , and consequently, holds with probability 1 when the observation matrix Y is drawn at random. For kmax = n − 1, Proposition 1 yields an upper bound of c(log n)2 on the oracle risk. Combining this upper bound with the lower bound (48) yields the claimed lower bound on the adaptivity index of the least squares estimator.

4.6.1 Proof of Lemma 8

From our earlier construction of M in Section 4.6, we know that |||Y − MLS|||F ≤ |||Y − M |||F < |||Y − M ∗|||F, which guarantees that MLS = M ∗. Consequently, we may consider the line

L(M ∗, MLS) : = {θM ∗ + (1 − θ)MLS | θ ∈ R}

that passes through the two points M ∗ and MLS. Given this line, consider the auxiliary estimator

M1 : = arg min |||Y − M |||2F.
M ∈L(M ∗,MLS )

(49)

Since M1 is the Euclidean projection of Y onto this line, it must satisfy the Pythagorean relation

|||Y − M ∗|||2F = |||Y − M1|||2F + |||M ∗ − M1|||2F.

(50)

Let Π[0,1] : Rn×n → [0, 1]n×n denote the Euclidean projection of any (n × n) matrix onto the hypercube [0, 1]n×n. This projection actually has a simple closed-form expression: it simply clips
every entry of the matrix M to lie in the unit interval [0, 1]. Since projection onto the convex set [0, 1]n×n is non-expansive, we must have

|||Y − M1|||2F ≥ |||Π[0,1](Y ) − Π[0,1](M1)|||2F = |||Y − Π[0,1](M1)|||2F.

(51)

28

Here the ﬁnal equation follows since Y ∈ [0, 1]n×n, and hence Π[0,1](Y ) = Y .

Furthermore, we claim that Π[0,1](M1) ∈ CSST. In order to prove this claim, ﬁrst recall that the

matrix

M1

can

be

written

as

M1

= (1 − θ)(MLS

−

1 2

11T

)

+

12 11T

for

some

θ ∈ R,

and

MLS

∈ CSST.

Consequently, if θ ≤ 1, then the rows/columns of the projected matrix Π[0,1](M1) obey the same

monotonicity conditions as those of MLS; conversely, if θ > 1, the rows/columns obey an inverted

set of monotonicity conditions, again speciﬁed by the rows/columns of M1. Moreover, since the two matrices MLS and 12 11T satisfy shifted-skew-symmetry, so does the matrix M1. One can further verify that any two real numbers a ≥ b must also satisfy the inequalities

min(a, 1) ≥ min(b, 1), and max(a, 0) ≥ max(b, 0).

If in addition, the pair (a, b) satisfy the constraint, a + b = 1, then we have max(min(a, 1), 0) + max(min(b, 1), 0) = 1. Using these elementary facts, it can be veriﬁed that the monotonicity and shifted-skew-symmetry conditions of any matrix are thus retained by the projection Π[0,1].
The arguments above imply that Π[0,1](M1) ∈ CSST and hence the matrix Π[0,1](M1) is feasible for the optimization problem (14). By the optimality of MLS, we must have |||Y − Π[0,1](M1)|||2F ≥ |||Y − MLS|||2F. Coupled with the inequality (51), we ﬁnd that
|||Y − M1|||2F ≥ |||Y − MLS|||2F.

On the other hand, since MLS is feasible for the optimization problem (49) and M1 is the optimal solution, we must actually have
|||Y − M1|||2F = |||Y − MLS|||2F,
so that MLS is also optimal for the optimization problem (49). However, the optimization problem (49) amounts to Euclidean projection on to a line, it must have a unique minimizer, which implies that MLS = M1. Substituting this condition in the Pythagorean relation (50) yields the claimed result.

5 Conclusions
We proposed the notion of an adaptivity index to measure the abilities of any estimator to automatically adapt to the intrinsic complexity of the problem. This notion helps to obtain a more nuanced evaluation of any estimator that is more informative than the classical notion of the worstcase error. We provided sharp characterizations of the optimal adaptivity that can be achieved in a statistical (information-theoretic) sense, and that can be achieved by computationally eﬃcient estimators.
The logarithmic factors in our results arise from corresponding logarithmic factors in the metric entropy results of Gao and Wellner [GW07], and understanding their necessity is an open question. In statistical practice, we often desire estimators, that perform well in a variety of diﬀerent
29

senses. We believe that estimating SST matrices at the minimax-optimal rate in Frobenius norm, as studied in more detail in the paper [SBGW15], is also computationally diﬃcult. We hope to formally establish this in future work. Finally, developing a broader understanding of fundamental limits imposed by computational considerations in statistical problems is an important avenue for continued investigation.
Acknowledgements: This work was partially supported by NSF grant CIF-31712-23800, ONRMURI grant DOD 002888, and AFOSR grant FA9550-14-1-0016. The work of NBS was supported in part by a Microsoft Research PhD fellowship.

References

[AAK+07] [AS15]
[BBM05] [BDPR84]
[Bel16] [BM08] [BR13]
[BT52] [BW97] [C+11] [Can06] [CGS13]

N. Alon, A. Andoni, T. Kaufman, K. Matulef, R. Rubinfeld, and N. Xie. Testing k-wise and almost k-wise independence. In ACM STOC, 2007.
E. Abbe and C. Sandon. Recovering communities in the general stochastic block model without knowing the parameters. In Advances in Neural Information Processing Systems, pages 676–684, 2015.
P. L. Bartlett, O. Bousquet, and S. Mendelson. Local Rademacher complexities. Annals of Statistics, 33(4):1497–1537, 2005.
G. Bril, R. Dykstra, C. Pillers, and T. Robertson. Algorithm as 206: isotonic regression in two independent variables. Journal of the Royal Statistical Society. Series C (Applied Statistics), 33(3):352–357, 1984.
P. C. Bellec. Adaptive conﬁdence sets in shape restricted regression. arXiv preprint arXiv:1601.05766, 2016.
M. Braverman and E. Mossel. Noisy sorting without resampling. In Proc. ACM-SIAM symposium on Discrete algorithms, pages 268–276, 2008.
Q. Berthet and P. Rigollet. Complexity theoretic lower bounds for sparse principal component detection. In COLT 2013 - The 26th Annual Conference on Learning Theory, June 12-14, 2013, Princeton University, NJ, USA, 2013.
R. A. Bradley and M. E. Terry. Rank analysis of incomplete block designs: I. The method of paired comparisons. Biometrika, pages 324–345, 1952.
T. P. Ballinger and N. T. Wilcox. Decisions, error and heterogeneity. The Economic Journal, 107(443):1090–1105, 1997.
E. Cator et al. Adaptivity and optimality of the monotone least-squares estimator. Bernoulli, 17(2):714–735, 2011.
E. J. Candes. Modern statistical estimation via oracle inequalities. Acta numerica, 15:257–325, 2006.
S. Chatterjee, A. Guntuboyina, and B. Sen. On risk bounds in isotonic and other shape restricted regression problems. arXiv preprint arXiv:1311.3765, 2013.

30

[CGS15] [Cha14] [CL11] [CL15] [DJKP95]
[DM59] [DM15] [Dug14] [FK03] [Gil52] [GW07] [HMG07] [HOX14] [Jer92] [JP00] [Kol06] [Kol11]

S. Chatterjee, A. Guntuboyina, and B. Sen. On matrix estimation under monotonicity constraints. arXiv:1506.03430, 2015.
S. Chatterjee. Matrix estimation by universal singular value thresholding. The Annals of Statistics, 43(1):177–214, 2014.
T. T. Cai and M. G. Low. A framework for estimation of convex functions. Technical report, Technical report, 2011.
S. Chatterjee and J. Laﬀerty. Adaptive risk bounds in unimodal regression. arXiv preprint arXiv:1512.02956, 2015.
D. L. Donoho, I. M. Johnstone, G. Kerkyacharian, and D. Picard. Wavelet shrinkage: asymptopia? Journal of the Royal Statistical Society. Series B (Methodological), pages 301–369, 1995.
D. Davidson and J. Marschak. Experimental tests of a stochastic decision theory. Measurement: Deﬁnitions and theories, pages 233–69, 1959.
Y. Deshpande and A. Montanari. Improved sum-of-squares lower bounds for hidden clique and hidden submatrix problems. arXiv:1502.06590, 2015.
S. Dughmi. On the hardness of signaling. In IEEE Foundations of Computer Science (FOCS), pages 354–363, 2014.
U. Feige and R. Krauthgamer. The probable value of the lov´asz–schrijver relaxations for maximum independent set. SIAM Journal on Computing, 32(2):345–370, 2003.
E. N. Gilbert. A comparison of signalling alphabets. Bell System Technical Journal, 31(3):504–522, 1952.
F. Gao and J. A. Wellner. Entropy estimate for high-dimensional monotonic functions. Journal of Multivariate Analysis, 98(9):1751–1764, 2007.
R. Herbrich, T. Minka, and T. Graepel. Trueskill: A Bayesian skill rating system. In Advances in Neural Information Processing Systems, 2007.
B. Hajek, S. Oh, and J. Xu. Minimax-optimal inference from partial rankings. In Advances in Neural Information Processing Systems, pages 1475–1483, 2014.
M. Jerrum. Large cliques elude the metropolis process. Random Structures & Algorithms, 3(4):347–359, 1992.
A. Juels and M. Peinado. Hiding cliques for cryptographic security. Designs, Codes and Cryptography, 20(3):269–280, 2000.
V. Koltchinskii. Local Rademacher complexities and oracle inequalities in risk minimization. Annals of Statistics, 34(6):2593–2656, 2006.
V. Koltchinskii. Oracle Inequalities in Empirical Risk Minimization and Sparse Recovery Problems, volume 38. Springer Science & Business Media, 2011.

31

[KRS15]

R. Kyng, A. Rao, and S. Sachdeva. Fast, provable algorithms for isotonic regression in all l p-norms. In Advances in Neural Information Processing Systems, pages 2701–2709, 2015.

[Kuˇc95]

L. Kuˇcera. Expected complexity of graph partitioning problems. Discrete Applied Mathematics, 57(2):193–212, 1995.

[Led01]

M. Ledoux. The Concentration of Measure Phenomenon. Mathematical Surveys and Monographs. American Mathematical Society, Providence, RI, 2001.

[Luc59] R. D. Luce. Individual choice behavior: A theoretical analysis. New York: Wiley, 1959.

[ML65]

D. H. McLaughlin and R. D. Luce. Stochastic transitivity and cancellation of preferences between bitter-sweet solutions. Psychonomic Science, 2(1-12):89–90, 1965.

[MPW15] R. Meka, A. Potechin, and A. Wigderson. Sum-of-squares lower bounds for planted clique. arXiv:1503.06447, 2015.

[MW15]

Z. Ma and Y. Wu. Computational barriers in minimax submatrix detection. The Annals of Statistics, 43(3):1089–1116, 2015.

[NOS12]

S. Negahban, S. Oh, and D. Shah. Iterative ranking from pair-wise comparisons. In Advances in Neural Information Processing Systems, pages 2474–2482, 2012.

[RGLA15] A. Rajkumar, S. Ghoshal, L.-H. Lim, and S. Agarwal. Ranking from stochastic pairwise preferences: Recovering Condorcet winners and tournament solution sets at the top. In International Conference on Machine Learning, 2015.

[RKJ08]

F. Radlinski, M. Kurup, and T. Joachims. How does clickthrough data reﬂect retrieval quality? In ACM conference on Information and knowledge management, pages 43–52, 2008.

[RWDR88] T. Robertson, F. Wright, R. L. Dykstra, and T. Robertson. Order restricted statistical inference, volume 229. Wiley New York, 1988.

[SBB+15]

N. B. Shah, S. Balakrishnan, J. Bradley, A. Parekh, K. Ramchandran, and M. Wainwright. Estimation from pairwise comparisons: Sharp minimax bounds with topology dependence. In Conference on Artiﬁcial Intelligence and Statistics, pages 856–865, 2015.

[SBGW15] N. B. Shah, S. Balakrishnan, A. Guntuboyina, and M. J. Wainwright. Stochastically transitive models for pairwise comparisons: Statistical and computational issues. arXiv preprint 1510.05610, 2015.

[SW15]

N. B. Shah and M. J. Wainwright. Simple, robust and optimal ranking from pairwise comparisons. arXiv preprint 1512.08949, 2015.

[THT11]

R. J. Tibshirani, H. Hoeﬂing, and R. Tibshirani. Nearly-isotonic regression. Technometrics, 53(1):54–61, 2011.

[Thu27]

L. L. Thurstone. A law of comparative judgment. Psychological Review, 34(4):273, 1927.

32

[Var57] [vdG00] [Wai14]

R. Varshamov. Estimate of the number of signals in error correcting codes. In Dokl. Akad. Nauk SSSR, 1957.
S. van de Geer. Empirical Processes in M-estimation, volume 6. Cambridge university press, 2000.
M. J. Wainwright. Constrained forms of statistical minimax: Computation, communication and privacy. In Proceedings of the International Congress of Mathematicians, Seoul, Korea, 2014.

33

