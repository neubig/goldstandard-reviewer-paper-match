Deep Learning Through the Lens of Example Diﬃculty

Robert J. N. Baldock∗ Google Research, Brain Team
rjnbaldock@gmail.com

Hartmut Maennel Google Research, Brain Team
hartmutm@google.com

Behnam Neyshabur Google Research, Blueshift Team
neyshabur@google.com

arXiv:2106.09647v2 [cs.LG] 18 Jun 2021

Abstract
Existing work on understanding deep learning often employs measures that compress all data-dependent information into a few numbers. In this work, we adopt a perspective based on the role of individual examples. We introduce a measure of the computational diﬃculty of making a prediction for a given input: the (eﬀective) prediction depth. Our extensive investigation reveals surprising yet simple relationships between the prediction depth of a given input and the model’s uncertainty, conﬁdence, accuracy and speed of learning for that data point. We further categorize diﬃcult examples into three interpretable groups, demonstrate how these groups are processed diﬀerently inside deep models and showcase how this understanding allows us to improve prediction accuracy. Insights from our study lead to a coherent view of a number of separately reported phenomena in the literature: early layers generalize while later layers memorize; early layers converge faster and networks learn easy data and simple functions ﬁrst.
1 Introduction
Much of the existing work on understanding deep learning “integrates out” the data, viewing the inductive bias of the model, or the properties of the optimizer as central to the success of the approach. Examples of such work include studies of eigenvalues of the Hessian and the geometry of the loss landscape (Ghorbani et al., 2019; Yao et al., 2020; Sagun et al., 2016; Li et al., 2018; Pennington and Bahri, 2017; Sagun et al., 2018), studies of margin and eﬀective generalization measures (Long and Sedghi, 2019; Unterthiner et al., 2020; Jiang et al., 2020, 2018; Kawaguchi et al., 2017) and mean-ﬁeld studies of stochastic optimization (Smith et al., 2021; Stephan et al., 2017; Smith and Le, 2018). However, in practice, we are rarely concerned with only the average behavior of a model.
One pathway to understanding the principles that govern how deep models process data is to study the properties of deep models for data points with diﬀerent “amounts” or “types” of example diﬃculty. There are a number of deﬁnitions of example diﬃculty in the literature (E.g. see Carlini et al. (2019); Hooker et al. (2019); Lalor et al. (2018); Agarwal and Hooker (2020)). Two are particularly relevant to this work. Firstly, the probability of predicting the ground truth label for an example, when that example is omitted from the training set (Jiang et al., 2021), which represents a statistical view of example diﬃculty. Secondly, the diﬃculty of learning an example, parameterized by the earliest training iteration after which the model predicts the ground truth class for that example in all subsequent iterations (Toneva et al., 2019). This measure represents a learning view of example diﬃculty 1.
These notions suﬀer from two fundamental limitations. While early-exit strategies in computer vision (Teerapittayanon et al., 2016; Huang et al., 2018) and NLP (Dehghani et al., 2018; Liu et al., 2020b; Schwartz et al., 2020; Xin et al., 2020) suggest predictions for easier examples require less computation, the above example diﬃculty notions do not encapsulate the processing of data inside a given converged model. Moreover,
∗Work completed as part of the Google AI Residency Program 1We expand on other notions of example diﬃculty in Appendix B.
1

Input Clock (CIFAR100) Digit 8 (SVHN) Clock (CIFAR100) Digit 8 (SVHN) Examples predicted in the first layer Examples predicted in the last layer

Probability

0.8

Fashion MNIST

0.6

SVHN CIFAR10

0.4

CIFAR100

0.2

0.0 0 1 2 3 4 5 6 7 8 9 10 Prediction Depth

Figure 1: Deep models use fewer layers to (eﬀectively) determine the prediction for easy examples and more layers
for hard examples. Left: A cartoon illustrating the deﬁnition of prediction depth (given in Section 2.1). Also shown are training examples from CIFAR100 (“Clock”) and SVHN (“Digit 8”). The examples shown are predicted at the input (ﬁrst layer) or softmax (last layer) of ResNet18. The examples predicted in the input are visually typical (“easy”), while those predicted in the softmax are mislabeled and/or visually confusing (“hard” examples). To ﬁnd the prediction depth, we build k-NN classiﬁers from the embeddings of the training set in diﬀerent layers of the model. The prediction depth corresponds to the earliest layer at which the predictions of all subsequent k-NN classiﬁers converge to a ﬁxed label. Right: Probability of prediction depth in ResNet18 models for four datasets (training split). We see that the four distributions have diﬀerent characteristic prediction depths. Ranking the mean prediction depths of these datasets in ascending order, we observe: Fashion MNIST (smallest), SVHN (second), CIFAR10 (third), and CIFAR100 (largest). This order aligns with how one might intuitively rank the diﬃculties of these classiﬁcation tasks.

existing notions of example diﬃculty (E.g. Carlini et al. (2019)) provide a one-dimensional view of diﬃculty which can not distinguish between examples that are diﬃcult for diﬀerent reasons.
In this paper, we take a signiﬁcant step towards resolving the above shortcomings. To take the processing of the data into account we propose a new measure of example diﬃculty, the prediction depth, which is determined from the hidden embeddings. To escape the one-dimensional view of diﬃculty, we introduce three distinct diﬃculty types by relating the hidden embeddings for an input to high-level concepts about example diﬃculty: “Does this example look mislabeled?”; “Is classifying this example only easy if the label is given?”; “Is this example ambiguous both with and without its label?”. Furthermore, we show how this enhanced notion of example diﬃculty can unify our understanding of several seemingly unrelated phenomena in deep learning. We hope that the results presented in this work will aid the development of models that capture heteroscedastic uncertainty, our understanding of how deep networks respond to distributional shift, and the advancement of curriculum learning approaches and machine learning fairness. These connections are discussed in Section 5.
Contributions Our main contributions are as follows:
• We introduce a measure of computational example diﬃculty: the prediction depth (PD). The prediction depth, illustrated in Figure 1, represents the number of hidden layers after which the network’s ﬁnal prediction is already (eﬀectively) determined (Section 2).
• We show that the prediction depth is larger for examples that visually appear to be more diﬃcult, and that prediction depth is consistent between architectures and random seeds (Section 2.2).
• Our empirical investigation reveals that prediction depth appears to establish a linear lower bound on the consistency of a prediction. We further show that predictions are on average more accurate for validation points with small prediction depths (Section 3.1).
• We demonstrate that ﬁnal predictions for data points that converge earlier during training are typically determined in earlier layers which establishes a correspondence between the training history of the network and the processing of data in the hidden layers (Section 3.2).
2

• We show that both the adversarial input margin and the output margin are larger for examples with smaller prediction depths. We further design an intervention to reduce the output margin of a network and show that this leads to predictions being made only in the latest hidden layers (Section 3.3).
• We identify three extreme forms of example diﬃculty by considering the prediction depth in the training and validation splits independently and demonstrate how a simple algorithm that uses the hidden embeddings in one middle layer to make predictions can lead to dramatic improvements in accuracy for inputs that strongly exhibit a speciﬁc form of example diﬃculty (Section 4).
• We use our results to present a coherent picture of deep learning that unify four seemingly unrelated deep learning phenomena: early layers generalize while later layers memorize; networks converge from input layer towards output layer; easy examples are learned ﬁrst and networks present simpler functions earlier in training (Section 5).
Experimental Setup: To ensure that our results are robust to the choice of architectures and datasets, we report empirical ﬁndings for ResNet18 (He et al., 2016), VGG16 (Simonyan and Zisserman, 2015) and MLP architectures trained on CIFAR10, CIFAR100 (Krizhevsky et al., 2009), Fashion MNIST (FMNIST) (Xiao et al., 2017) and SVHN (Netzer et al., 2011) datasets. All models were trained using SGD with momentum. Our MLP comprises 7 hidden layers of width 2048 with ReLU activations. Details of the datasets, architectures, and hyperparameters used can be found in Appendix A.
Related Work: Our work uses hidden layer probes to determine example diﬃculty. We have discussed how our study relates to prior work on example diﬃculty. Hidden layer probes have also been used to study deep learning. Deep k-NN methods (Papernot and McDaniel, 2018) determine their predictions and estimate their own uncertainties by comparing the hidden embeddings of an input to those of the training set. Cohen et al. (2018) showed that SVM, k-Nearest Neighbors (k-NN) and logistic regression probes achieve similar accuracies. However, they did not study the processing of individual data points nor did they relate the k-NN accuracy to notions of example diﬃculty. Alain and Bengio (2017) used linear classiﬁer probes in the hidden layers to interrogate deep models and demonstrated that linear separability of the embeddings increases monotonically with depth. We provide a more detailed discussion of related work in Appendix B.
2 Prediction Depth: a Computational View of Example Diﬃculty
We discussed the statistical and learning views of example diﬃculty in Section 1. In this section, we introduce a computational view of example diﬃculty parametrized by the prediction depth as deﬁned in Section 2.1. This computational view asserts that, for “easy” examples, a deep model’s ﬁnal prediction is eﬀectively made after only a few layers, while more layers are used for “diﬃcult” examples.
2.1 Deﬁnition
Asserting that the ﬁnal prediction is eﬀectively determined in earlier layers of a model, before the output, we estimate the depth at which a prediction is made for a given input as follows 2:
1. We construct k-NN classiﬁer probes from the embeddings of the training set after particular layers of the network, including the input and the ﬁnal softmax. The placement of k-NN probes is described in Appendix A.5. We use k = 30 in the k-NN probes. Appendix A.4 establishes that the k-NN accuracies we report are insensitive to k over a wide range.
2. A prediction is deﬁned to be made at a depth L = l if the k-NN classiﬁcation after layer L = l − 1 is diﬀerent from the network’s ﬁnal classiﬁcation, but the classiﬁcations of k-NN probes after every layer
2In the process of arriving at this deﬁnition of the prediction depth we considered several alternatives, including using the ground truth class in place of the predicted class and using logistic regression probes in place of k-NN probes. See Appendix E for a discussion on the choices we made in our deﬁnition.
3

Validation Train
ResN18 VGG16
MLP ResN18 VGG16
MLP ResN18 VGG16
MLP ResN18 VGG16
MLP
VGG16

FMNIST SVHN CIFAR10 CIFAR100

ResN18

1.0

VGG16

MLP

0.5

ResN18

VGG16 MLP

0.0

0.8 0.4 0.00.0 0.5 1.0 0.0
ResNet18

102 101 100
0M.L4P 0.8

Figure 2: Consistency of prediction depth between architectures and random seeds. Left: The panel shows the
correlation coeﬃcient between prediction depths in diﬀerent architectures, for both train and validation splits in four datasets. Diagonal comparisons between an architecture and itself show the correlation for the same architecture trained with diﬀerent random seeds. Right: Histograms comparing the mean value of prediction depth obtained for each data point in the training set of CIFAR10 from an ensemble of 250 trained models. In this plot, for visual simplicity, we rescale prediction depth to the interval [0, 1] for each network. Similar results for all other datasets are presented in Appendix C.1.

L ≥ l are all equal to the ﬁnal classiﬁcation of the network. Data points consistently classiﬁed by all k-NN probes are determined to be (eﬀectively) predicted in layer 0 (the input) 3.
It is worth noting that the prediction depth can be calculated for all data points: both in the training and validation splits. This leads to two notions of computational diﬃculty:
• The diﬃculty of predicting the (given) class for an input (in the training split) • The diﬃculty of making a prediction for an input, unseen in advance (from the validation split)
We examine both notions of computational diﬃculty in this paper and use the distinction between them to describe diﬀerent forms of example diﬃculty in Section 4.
2.2 Prediction depth is a meaningful and robust notion of example diﬃculty
In this section we show that prediction depth agrees with intuitive notions of example diﬃculty and that it is consistent between diﬀerent training runs and similar architectures.
Prediction depth is higher for examples and datasets that seem more diﬃcult If prediction depth is a sensible measure of example diﬃculty then we would expect the following sanity checks to be observed:
1. Individual data points that are visually confusing or mislabeled should have larger prediction depths as compared to images that are clear examples of their class.
2. Data points from tasks that are intuitively simpler should have lower prediction depths on average.
Figure 1 shows that the prediction depth passes both of these sanity checks.
Prediction depth is consistent across random seeds and similar architectures Figure 2 shows that the prediction depth is highly consistent between diﬀerent architectures and random seeds for all datasets. Perfect agreement is not expected as diﬀerent deep learning algorithms have diﬀerent inductive biases which aﬀects the perceived diﬃculty of examples. We observe stronger correlation between prediction depth for
3Implementation details can be found in Appendix A.6.1.

4

Consistency Score Consistency Score Consistency Score

1.00 All Data

0.75

100

0.50

0.25

10 1

0.000 1 2 3 4 5 6 7 8 910 Prediction Depth

1G.0r0ound Truth = Consensus101

0.75

100

0.50

0.25

10 1

0.000 1 2 3 4 5 6 7 8 910 Prediction Depth

1G.0r0ound Truth
0.75

Consensus
101

0.50

100

0.25

0.000 1 2 3 4 5 6 7 8 910 Prediction Depth

Figure 3: Consistency score vs. prediction depth in the validation split (left) can be understood as the superposition
of two simple functions (middle and right). We trained 250 ResNet18 models on CIFAR10, with 90:10% random train:validation splits as described in Appendix A. These histograms compare the frequency of correct predictions to the average prediction depth for a data point when it occurs in the validation split. The average prediction depth forms two, surprisingly simple, linear bounds on the consistency score (see Section 3.1 for a full description.) This Figure is reproduced for all datasets and architectures in Appendix C.2, illustrating the consistency of this result.

ResNet18 and VGG16, than between VGG16 and MLP. This may be explained by the fact that ResNet18 and VGG16 are both convolutional networks and we expect their inductive biases to be more similar to one another than to MLP.

3 Deep Learning Phenomena Through the Lens of Prediction Depth
In this section, we explore how the prediction depth can be used to better understand three important aspects of Deep Learning: accuracy and consistency of a prediction; the order in which data is learned and the simplicity of the learned function (as measured by the margin) in the vicinity of a data point.

3.1 Depth of a prediction gives a linear lower bound on its consistency

Adopting a statistical view of example diﬃculty, Jiang et al. (2021) identiﬁed example diﬃculty with the expected accuracy of the learning algorithm for a given input, averaged over models trained on diﬀerent random subsets of the training set with diﬀerent random seeds. In this section, we clarify the relationship between the prediction depth and the expected accuracy by disentangling the accuracy from the sensitivity of predictions to the particular training split and random seed. Following Jiang et al. (2021), we measure the expected accuracy using the consistency score.

Consistency score Cˆ: Consistency score is the frequency of classifying an example correctly when it is omitted from the training set. An empirical estimator of the consistency score for a validation point (x, y) is given by (Jiang et al., 2021):

CˆA,S (x, y) = Eˆr˜n

[δyA ,y ]

(1)

S ∼S \{(x,y)}

where A is a deep learning algorithm (architecture, loss and optimizer), y is the ground truth class for x, S˜ is a random subset of n points sampled from a training dataset S excluding (x, y), yA is the predicted
class of x for A trained with data S˜, δ is the Kronecker delta and Eˆr denotes empirical averaging with r i.i.d. samples of such subsets S˜.

Figure 3 (left panel) shows the relationship between consistency score and prediction depth. This plot indicates a surprising piecewise linear boundary which is symmetric around consistency score 12 . This suggests the existence of a missing concept that could simplify the picture. We next show that the missing concept is
the notion of a consensus class which is deﬁned below.

5

Consensus-Consistency Consensus-Consistency
Accuracy

1.0

101

0.8

0.6

100

00..24 10 1

0.00 2 4 6 8 10 Prediction Depth

1.0

0.8 FMNIST

SVHN

0.6

CIFAR10 CIFAR100

0 2Predic4tion D6epth8 10

1.0

0.8

0.6

FMNIST SVHN

0.4

CIFAR10 CIFAR100

0.2 0 2 4 6 8 10

Prediction Depth

Figure 4: Left: Prediction depth provides us with a linear lower bound on consensus-consistency. Results for
CIFAR100 with ResNet18. We train 250 models (90:10% random train:validation splits) and compare the average prediction depth when a point occurs in the validation set, to the consensus-consistency of the corresponding predictions. Predictions made for points with low mean prediction depths are highly consistent. Conversely, predictions for points with high mean prediction depths are typically more sensitive to the particular training split and random seed used during training. This left plot shows the result for CIFAR100 with ResNet18. Middle: Prediction depth in one model predicts the consensus-consistency of an ensemble that does not include that model. For each dataset we train 25 ResNet18 models with the full training set (see Appendix A). The consensus-consistency of each test point is obtained from 24 of the models, while the prediction depth is obtained from the remaining 1 model. We see that prediction depth in one model predicts the consensus-consistency of a separate ensemble: a measure of the uncertainty of the prediction. The size of each marker in the middle and right plots shows the fraction of the dataset with each prediction depth. Right: Prediction depth predicts accuracy. For each dataset we train 250 ResNet18 models (90:10% random train:validation splits). Each time a point appears in the validation split we record the prediction depth and whether the prediction was correct. Predictions made in earlier layers are more likely to be correct. Consistency of these plots is demonstrated for all datasets and architectures in Appendix C.2 where we also describe the relationship between the prediction depth and the entropy of the predictions for an ensemble.

Consensus class yˆA: The consensus class of x is deﬁned as the predicted class for input x by a majority voting ensemble of r models each of which is trained on a randomly chosen subset S˜ ∼n S\{(x, y)} 4.

Figure 3 (middle and right) shows how conditioning on whether consensus class matches the ground truth can change the relationship between consistency score and the prediction depth. For points where the consensus class matches the ground truth (middle) we see that the prediction depth forms a, surprisingly simple, linear lower bound on the consistency score. For points where the consensus class diﬀers from the ground truth (right) at low prediction depth the consistency score is bounded from above by a line that reﬂects the bound from the middle plot in Cˆ = 12 , suggesting that such points are repeatedly mislabeled with a wrong class label. At high prediction depth, the consistency score is low, which suggests highly inconsistent predictions and low accuracy. This result suggests a simple hypothesis: that predictions with low prediction depth are consistent with the consensus class, whether that matches the ground truth class or not, while predictions made in later layers depend strongly on the speciﬁc training split and random seed used for training and initialization. We measure consistency with the consensus class using the consensus-consistency score.

Consensus-consistency score C∗: The fraction of models in an ensemble that predict the ensemble’s consensus class yˆA (x) for an unseen input x.

CA∗ ,S (x) = Eˆr˜n

δyA ,yˆA (x)

(2)

S ∼S \{(x,y)}

where the notation is the same as in (1) 5.
4Implementation details can be found in Appendix A.6.2 5Consensus-consistency score is a measure of uncertainty and can be used for calibration (Lakshminarayanan et al., 2017; Wenzel et al., 2020; Wen et al., 2019). See Appendix A.6.3 for details of our implementation.

6

Iteration Learned Accuracy
k-NN Accuracy

1.5

FMNIST SVHN

1.0

CIFAR10 CIFAR100

0.5

0.0 0 2 4 6 8 10 12 14 Prediction Depth

1.0 0.8 0.6 0.4 0.2 0.0 0 20 40 60 80
Epoch

1.0 0.8 0.6 0.4 0.2 0.0 0 2 4 6 8 10 12 14
Layer

Unchanged Data Mislabelled Data M(OisrilgaibneallleLdabDealtsa) Test

Figure 5: Left: Data points with small prediction depths are on average learned before data points with higher
prediction depths. We train 250 VGG16 models for each dataset, using a 90:10% random train:validation split as described in Appendix A. Each time an input appears in the validation split we record the prediction depth and the iteration learned in that model. This plot shows the average iteration learned for data points at each prediction depth. Marker size shows the fraction of the dataset with each prediction depth. Middle and right: The training learning curve (middle) shares several important features with the inference learning curve (right). Blue, yellow and green curves represent diﬀerent components of the CIFAR10 training split, in which we have randomized (and ﬁxed) 40% of the labels, and red curves show the test split. The middle and right plots show results from 5 random seeds. The inference learning curve (right) is the sequence of k-NN probe accuracy values for each split. All three plots show results for VGG16. The hyperparameters used are given in Appendix A.
Figure 4 (left) establishes that our simple hypothesis is indeed correct: the prediction depth forms a linear lower bound on the consensus-consistency score for all data points, irrespective of whether the consensus class matches or diﬀers from the ground truth. Interestingly, Figure 4 (middle and right) shows how the prediction depth in a single model, can be used to estimate both of these quantities. That is, predictions of data points with lower prediction depth are both more likely to be consistent and more likely to be correct.
3.2 The prediction depth of an input is correlated with its learning diﬃculty
In Section 3.1, we describe the relationship between the prediction depth, which represents a computational view of example diﬃculty and the consistency and consensus-consistency scores, which represent a statistical view. In this section we compare prediction depth to a learning view of example diﬃculty. We measure the diﬃculty of learning an example by the speed at which the model’s prediction converges for that input during training. The following deﬁnition is adapted from Toneva et al. (2019):
Iteration learned A data point is said to be learned by a classiﬁer at training iteration t = τ if the predicted class at iteration t = τ − 1 is diﬀerent from the ﬁnal prediction of the converged network and the predictions at all iterations t ≥ τ are equal to the ﬁnal prediction of the converged network. Data points consistently classiﬁed after all training steps and at the moment of initialization, are said to be learned in step t = 0 6.
Figure 5 (left plot) shows the positive correlation between the prediction depth and the iteration learned, for all four datasets in VGG16. Consistent results are presented for all architectures and datasets, in both the validation and training splits in Appendix C.3. As a result of the reported correlation, we anticipate that many of the data points correctly classiﬁed by the k-NN probe in a particular layer should also be correctly classiﬁed by the network at a corresponding interval of training steps. If this is correct then we would expect there to be a visual correspondence between the training learning curve (which shows how the accuracy of the network changes during training) and the accuracy of the k-NN probes as data passes from input, through the network, towards the output layer. We call the series of k-NN probe accuracies the inference learning curve.
To test this hypothesis we train a model on a training split where a subset of labels are corrupted and compare the training and inference learning curves on four splits of the data: unchanged training data;
6Note that this deﬁnition can be applied to points in both training and validation splits. In order to compare diﬀerent models and datasets we rescale the iteration learned in each model so that the 95th percentile occurs at 1.0 and network initialization at 0.
7

Figure 6: Left and Middle: Test examples with smaller prediction depths, on average, have larger output and input
margins. We train 25 VGG16 models with diﬀerent random seeds on CIFAR10 (see Appendix A for details) and compare the mean prediction depth of each test point in these 25 runs to its mean output and input margins (log scales). Correlation coeﬃcients are −0.70 (output margin) and −0.69 (input margin). Although the prediction depth could be at most 14, no data point has an average prediction depth greater than 12. Right: An intervention that does not encourage large output margin (“0-Hinge”) results, as predicted, in models where the predictions are eﬀectively determined in higher layers in the network compared to the standard training (“CE”).
mislabeled training data; the original labels of the mislabeled training data and the test split. In Figure 5 (middle and right plots) we see that many of the important features of the training learning curve are indeed present in the inference learning curve. During training (middle), mislabeled data are initially processed as though they are a member of their original class (before they were mislabeled) (Liu et al., 2020a). After an initial period of learning, the network begins to learn the new (random) labels that have been assigned to those data points, so the orange curve moves upwards, and the green curve downwards. At this point, a maximum is observed in the training accuracy (Arpit et al., 2017). In the right plot we see that these same phenomena occur in the inference learning curve.
3.3 Deep models exhibit larger margins for inputs with lower prediction depth
It is reported in the literature that deep networks learn functions of increasing complexity during training (Hu et al., 2020; Kalimeris et al., 2019). We frame this observation diﬀerently: the learned function is “locally simpler” in the vicinity of data points with smaller prediction depths, and these points are typically learned earlier in training (Section 3.2).
Two known measures of the simplicity of a learned function are the output margin (the diﬀerence between the largest and second-largest logits) and the adversarial input margin (the smallest norm required for an adversarial perturbation in the input to change the model’s class prediction). We estimate the adversarial input margin, γ, with a linear approximation (Jiang et al., 2018): for an input x with predicted class i, γ minj=i |∇x|z(iz−i−zjz|j)| where zj is the logit returned by the network for class j. Figure 6 (left and middle plots) show that data points with smaller prediction depths have both larger input and output margins on average and that variances of the input and output margins decrease as the prediction depth increases.
To illustrate the strength of the relationship between the prediction depth and output margin, we demonstrate that reducing the output margin of the learned function results in a model that clusters the data only in the latest layers: such a solution has a very high average prediction depth. We do not minimize the output margin directly but rather use a loss and an optimizer that do not encourage high output margin. Naturally there are many unknowns that may contribute to this eﬀect. We simply report the intervention and the outcome.
The intervention is performed as follows: we construct a loss function that does not promote conﬁdence: a zero-margin hinge loss (“0-Hinge”), and optimize the network using full-batch gradient descent with momentum and very small learning rate. For an input x with label i the 0-Hinge loss is given by l(x) = j=i max(0, zi −zj) where zj represents the logit for class j. The form of this intervention is justiﬁed in Appendix A.7. As a
8

Class: Bird

Easy

Looks like a different class Ambiguous w/o its label

Ambiguous

Figure 7: The prediction depth can be the same, or very diﬀerent for the same input when it occurs in the train and
validation splits. Corners of this plot correspond to diﬀerent forms of example diﬃculty. (See Section 4 for discussion.) We train 250 ResNet18 models on CIFAR10 with random 90:10% train:validation splits as described in Appendix A. These histograms compare average prediction depth for each data point when it occurs in the validation split vs the training split. This behavior is consistently reproduced for all datasets and architectures in Appendix C.5. Below we show extreme (not hand-chosen) images of “Birds” that appear closest to the corners of this plot. The consensus class is given above each image (tiebreaks favor the class “Bird”.).

control, we additionally train a model in the standard fashion using the cross-entropy loss and SGD with momentum and large initial learning rate. Since full-batch gradients are computationally expensive, we train on a subset of CIFAR10 (see Appendix A.7, where we also give the hyperparameters and learning curves.). The output margin obtained with the intervention is 5 orders of magnitude smaller than in the control experiment: 2.0 × 10−4 ± 2.0 × 10−4 for the 0-Hinge loss and 1.6 × 101 ± 0.50 × 101 for cross-entropy loss. Figure 6 (right) compares the accuracies of the k-NN probes resulting from these training approaches. The 0-Hinge loss training achieves only a marginal improvement in accuracy (red) over an untrained network (purple), and the training split is accurately clustered only in the latest layers. This conﬁrms the predicted behavior: the intervention leads to a model that exhibits both very small average output margins and very late clustering of the data. Very late clustering of the data implies high prediction depths since the k-NN probe classiﬁcations change in the latest layers for many data points.

4 Beyond a One-Dimensional Picture of Example Diﬃculty
In this section we transcend the one-dimensional picture of example diﬃculty by identifying diﬀerent underlying reasons behind the diﬃculty of an example, in a way that is general to diﬀerent architectures and datasets.
Figure 7 shows that the prediction depth can be diﬀerent when an input occurs in the training split vs. the validation split. Thus, there are two axes of example diﬃculty:
1. Diﬃculty of making a prediction when an input is in the validation set
2. Diﬃculty of ﬁnding commonalities during training with other examples of the same ground truth class
Both axes have a range from “clear” to “ambiguous”. In Section 3.1 we show that predictions made for validation points with later prediction depths are often inconsistent, with low consensus-consistency. Conversely, a low prediction depth typically indicates an input with high consensus-consistency. For Axis 1 we will identify validation points with low prediction depths as “clear” and those with high prediction depths as “ambiguous”. We will additionally identify a low or high prediction depth in the training split with examples that are respectively “clear” and “ambiguous” on Axis 2. By making combinations of low/high values of (PDVal., PDTrain) we obtain four extremes of example diﬃculty:

9

k-NN: Ground Truth k-NN: Consensus Class

1.0 0.8 0.6 0.4 0.2 0.0 0 1 2 3 4 5 6 7 8 9 10
Layer

1.0 0.8 0.6 0.4 0.2 0.0 0 1 2 3 4 5 6 7 8 9 10
Layer

Easy Ambiguous Ambiguous w/o its label Looks like a different class

Figure 8: Average k-NN probe conﬁdence (solid lines) and accuracy (dotted lines) for the ground truth class (left)
and consensus class (right), in the validation split for examples exhibiting extreme forms of diﬃculty. Mean values for 100 examples with each form of diﬃculty, identiﬁed as the 100 examples closest to the corners in Figure 7 (left). This result is for CIFAR10 with ResNet18: similar plots for all datasets and architectures are shown in Appendix C.6. See Section 4 for the discussion of the result and how it can be used to improve prediction accuracy.

Easy examples: (Low PDVal., Low PDTrain). Such examples are often visually typical members of their class and the predicted label nearly always matches the ground truth.
Looks like a diﬀerent class: (Low PDVal., High PDTrain). In the validation set, there is a clear (and nearly always incorrect) classiﬁcation for such an input, but it is diﬃcult to connect such inputs to other examples of their ground truth class during training. Mislabeled examples are of this kind, as are visually confusing images which at ﬁrst appear to show something else.
Ambiguous unless the label is given: (High PDVal., Low PDTrain). These examples are diﬃcult to connect to their predicted class in the validation split but easy to connect to their ground truth class during training. These points may, for example, visually resemble both their own class and another class. They are likely to be misclassiﬁed.
Ambiguous: (High PDVal., High PDTrain). These examples may be corrupted or show an example of a rare sub-class. Predictions for these inputs can depend strongly on the random seed used for training and initialization.
In Figure 7 we visualize CIFAR10 “Bird” images with the extreme forms of example diﬃculty for ResNet18, as identiﬁed using the prediction depth in the training and validation splits. In the full dataset (left panel) we see that the prediction depth can be very diﬀerent in the training and validation splits: the two prediction depths are typically similar for points where the consensus class is equal to the ground truth (right panel), but can be very diﬀerent when the consensus class is diﬀerent from the ground truth (middle panel). This behavior is consistently reproduced for all datasets and architectures in Appendix C.5.
Looking at these examples of the class “Bird” with diﬀerent diﬃculty types, we observe that ResNet18 ﬁnds small garden birds easiest, while birds in ﬂight against a blue background “look like airplanes”, ostriches are “ambiguous without their label” and the “ambiguous” examples are either unclear photographs or examples of rare sub-groups that don’t appear frequently in the data. We found the consensus-consistency of inputs that are “Ambiguous” or “Ambiguous without its label” to be signiﬁcantly lower than those of examples that are “Easy” or “Look like a diﬀerent class”.
In order to better understand how networks process examples with diﬀerent, extreme forms of example diﬃculty, Fig. 8 examines how the k-NN conﬁdence (fraction of votes) and accuracy of the ground truth class and of the consensus class progress, as validation points pass through the network. “Easy” examples are classiﬁed as their consensus class (which is equal to their ground truth class) in all k-NN probes and the conﬁdence in the consensus class steadily increases as data points proceed through the hidden layers. Examples that “look like a diﬀerent class” are also processed as members of their consensus class, similarly to “easy” examples. However, unlike “easy” examples, their consensus classes do not match their ground

10

truth classes. Examples that are “ambiguous without their labels” are initially processed as members of their ground truth classes with intermediate conﬁdence, but in later layers become mistaken for their consensus class. “Ambiguous” examples are processed with low conﬁdence and accuracy in the early layers, for both ground truth and consensus classes. In later layers “ambiguous” examples are recognized, with intermediate conﬁdence and accuracy, as members of the consensus class, which matches the ground truth class for a sizeable fraction of “ambiguous” examples.
Improving the prediction accuracy Can the prediction accuracy be improved using our understanding of how each class of diﬃcult examples are processed by deep models? Figure 8 suggest that k-NN probes in intermediate layers may be more accurate than the full deep model for examples that are “ambiguous without their label” (data points closest to the lower right corner of Figure 7). In order to test this hypothesis, we compare the accuracy of the k-NN probe in layer 4 to the full model’s prediction for the 100 examples closest to the lower right corner of Figure 8. We obtain a striking improvement in accuracy from 25% to 98% for these examples. This showcases how insights from this study can be directly used to improve prediction accuracy.
5 Discussion
Summary We have introduced a notion of example diﬃculty called the prediction depth, which uses the processing of data inside the network to score the diﬃculty of an example. We have shown how the prediction depth is related to the accuracy and uncertainty of a prediction, the adversarial input margin and the output margin of the learned solution, and that data points that are easier according to the prediction depth are also typically learned earlier in training. We have also shown that the diﬃculty of an example can be both similar, or very diﬀerent depending on whether an input appears in the validation split or the training split, and described four extremes of example diﬃculty. For data points that are “ambiguous without their label”, we have demonstrated how returning the k-NN prediction in a middle layer can lead to impressive increases in model accuracy: for CIFAR10 in ResNet18 we obtained an increase in accuracy from 25% to 98% for the inputs that are most “ambiguous without their label”.
Connecting known phenomena In the literature, the following phenomena are separately reported from diﬀerent experimental paradigms:
1. Early layers generalize while later layers memorize (Stephenson et al., 2021). 2. Model layers converge from input layer towards output layer (Raghu et al., 2017; Morcos et al., 2018). 3. Deep models learn easy data (Jiang et al., 2021; Toneva et al., 2019) and simple functions ﬁrst (Hu
et al., 2020; Kalimeris et al., 2019).
Following this paper, a coherent and closely related picture emerges:
1. Predictions made in early layers are more likely to be consistent than those made in later layers. Consistent predictions are likely to be correct and the expected accuracy of inconsistent predictions is naturally low (Section 3.1).
2. Data points learned early in training typically have smaller prediction depths than those learned later during training (Section 3.2).
3. On average, deep neural networks exhibit wider input and output margins (common measures of “local simplicity”) in the vicinity of data with smaller prediction depths (Section 3.3).
Pertinence of example diﬃculty to topics in machine learning Curriculum Learning attempts to treat hard examples diﬀerently from easy examples during training. Robustness to distribution shifts that change the relative frequencies of common and rare subgroups in the test set (which we have shown can
11

have diﬀerent forms of example diﬃculty) is important for ML Fairness. Methods developed to address heteroscedastic uncertainty typically address example diﬃculty as a one-dimensional quantity. We expand upon the relevance of our work to these three topics in Appendix D.
Limitations We believe that the results we report stem from a deep model’s representation, which is hierarchical by construction. We expect that the same results will therefore apply in larger models, larger datasets, and tasks other than image classiﬁcation, but testing this remains as further work. Although we demonstrate that returning the results of a hidden k-NN can yield dramatic increases in accuracy for examples that are “ambiguous without their label”, we otherwise do not explore ways to practically apply the insights we present. In particular, we expressly do not claim that all that is required for good accuracy is to reduce the prediction depth: freezing later layers of the network would not be expected to result in good generalization.
Acknowledgment
We would like to thank Hanie Sedghi, Ilya Tolstikhin, Ibrahim Alabdulmohsin, Daniel Keysers and Julian Eisenschlos for valuable discussions on the topic and Arthur Baldock for proofreading the manuscript.
References
Agarwal, C. and Hooker, S. (2020). Estimating example diﬃculty using variance of gradients. In ICML, Workshop on Human Interpretability in Machine Learning (WHI).
Alain, G. and Bengio, Y. (2017). Understanding intermediate layers using linear classiﬁer probes. In International Conference on Learning Representations (Workshop).
Arpit, D., Jastrzebski, S., Ballas, N., Krueger, D., Bengio, E., Kanwal, M. S., Maharaj, T., Fischer, A., Courville, A., Bengio, Y., et al. (2017). A closer look at memorization in deep networks. In International Conference on Machine Learning.
Bahri, D., Jiang, H., and Gupta, M. (2020). Deep k-NN for noisy labels. In International Conference on Machine Learning.
Bengio, Y., Louradour, J., Collobert, R., and Weston, J. (2009). Curriculum learning. In Proceedings of International Conference on Machine Learning.
Carlini, N., Erlingsson, U., and Papernot, N. (2019). Distribution density, tails, and outliers in machine learning: Metrics and applications. arXiv preprint arXiv:1910.13427.
Chatterjee, S. (2019). Coherent gradients: An approach to understanding generalization in gradient descentbased optimization. In International Conference on Learning Representations.
Cohen, G., Sapiro, G., and Giryes, R. (2018). DNN or k-NN: That is the generalize vs. memorize question. In NeurIPS, Workshop on Integration of Deep Learning Theories.
Dehghani, M., Gouws, S., Vinyals, O., Uszkoreit, J., and Kaiser, L. (2018). Universal transformers. In International Conference on Learning Representations.
Elman, J. L. (1993). Learning and development in neural networks: The importance of starting small. Cognition, 48(1):71–99.
Feldman, V. and Zhang, C. (2020). What neural networks memorize and why: Discovering the long tail via inﬂuence estimation. In Proceedings of the 34th International Conference on Neural Information Processing Systems.
12

Ghorbani, B., Krishnan, S., and Xiao, Y. (2019). An investigation into neural net optimization via Hessian eigenvalue density. In International Conference on Machine Learning.
Hacohen, G., Choshen, L., and Weinshall, D. (2020). Let’s agree to agree: Neural networks share classiﬁcation order on real datasets. In International Conference on Machine Learning.
Hacohen, G. and Weinshall, D. (2019). On the power of curriculum learning in training deep networks. In International Conference on Machine Learning.
He, K., Zhang, X., Ren, S., and Sun, J. (2016). Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition.
Hooker, S., Courville, A., Clark, G., Dauphin, Y., and Frome, A. (2019). What do compressed deep neural networks forget? arXiv preprint arXiv:1911.05248.
Hooker, S., Moorosi, N., Clark, G., Bengio, S., and Denton, E. (2020). Characterising bias in compressed models. arXiv preprint arXiv:2010.03058.
Hu, W., Xiao, L., Adlam, B., and Pennington, J. (2020). The surprising simplicity of the early-time learning dynamics of neural networks. In Proceedings of the 34th International Conference on Neural Information Processing Systems.
Huang, G., Chen, D., Li, T., Wu, F., van der Maaten, L., and Weinberger, K. (2018). Multi-scale dense networks for resource eﬃcient image classiﬁcation. In International Conference on Learning Representations.
Jiang, Y., Krishnan, D., Mobahi, H., and Bengio, S. (2018). Predicting the generalization gap in deep networks with margin distributions. In International Conference on Learning Representations.
Jiang, Y., Neyshabur, B., Mobahi, H., Krishnan, D., and Bengio, S. (2020). Fantastic generalization measures and where to ﬁnd them. In International Conference on Learning Representations.
Jiang, Z., Zhang, C., Talwar, K., and Mozer, M. C. (2021). Characterizing structural regularities of labeled data in overparameterized models. In International Conference on Machine Learning.
Kalimeris, D., Kaplun, G., Nakkiran, P., Edelman, B., Yang, T., Barak, B., and Zhang, H. (2019). SGD on neural networks learns functions of increasing complexity. In Advances in Neural Information Processing Systems, volume 32.
Kawaguchi, K., Kaelbling, L. P., and Bengio, Y. (2017). Generalization in deep learning. arXiv preprint arXiv:1710.05468.
Kendall, A. and Gal, Y. (2017). What uncertainties do we need in Bayesian deep learning for computer vision? In Proceedings of the 31st International Conference on Neural Information Processing Systems.
Kendall, A., Gal, Y., and Cipolla, R. (2018). Multi-task learning using uncertainty to weigh losses for scene geometry and semantics. In Proceedings of the IEEE conference on computer vision and pattern recognition.
Keskar, N. S., Nocedal, J., Tang, P. T. P., Mudigere, D., and Smelyanskiy, M. (2017). On large-batch training for deep learning: Generalization gap and sharp minima. In International Conference on Learning Representations.
Kolesnikov, A., Beyer, L., Zhai, X., Puigcerver, J., Yung, J., Gelly, S., and Houlsby, N. (2020). Big transfer (BiT): General visual representation learning. In European Conference on Computer Vision.
Krizhevsky, A., Hinton, G., et al. (2009). Learning multiple layers of features from tiny images. Technical Report.
13

Lakshminarayanan, B., Pritzel, A., and Blundell, C. (2017). Simple and scalable predictive uncertainty estimation using deep ensembles. In Proceedings of the 31st International Conference on Neural Information Processing Systems.
Lalor, J. P., Wu, H., Munkhdalai, T., and Yu, H. (2018). Understanding deep learning performance through an examination of test set diﬃculty: A psychometric case study. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.
Li, H., Xu, Z., Taylor, G., Studer, C., and Goldstein, T. (2018). Visualizing the loss landscape of neural nets. In Proceedings of the 32nd International Conference on Neural Information Processing Systems.
Liu, S., Niles-Weed, J., Razavian, N., and Fernandez-Granda, C. (2020a). Early-learning regularization prevents memorization of noisy labels. Advances in Neural Information Processing Systems, 33.
Liu, W., Zhou, P., Wang, Z., Zhao, Z., Deng, H., and JU, Q. (2020b). FastBERT: A self-distilling BERT with adaptive inference time. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6035–6044.
Long, P. M. and Sedghi, H. (2019). Generalization bounds for deep convolutional neural networks. In International Conference on Learning Representations.
Mangalam, K. and Prabhu, V. (2019). Do deep neural networks learn shallow learnable examples ﬁrst? In ICML, Workshop on Identifying and Understanding Deep Learning Phenomena.
Morcos, A. S., Raghu, M., and Bengio, S. (2018). Insights on representational similarity in neural networks with canonical correlation. In Proceedings of the 32nd International Conference on Neural Information Processing Systems.
Nagarajan, V., Andreassen, A., and Neyshabur, B. (2021). Understanding the failure modes of out-ofdistribution generalization. In International Conference on Learning Representations.
Netzer, Y., Wang, T., Coates, A., Bissacco, A., Wu, B., and Ng, A. Y. (2011). Reading digits in natural images with unsupervised feature learning. Technical Report.
Neyshabur, B., Bhojanapalli, S., McAllester, D., and Srebro, N. (2017). Exploring generalization in deep learning. In Proceedings of the 31st International Conference on Neural Information Processing Systems.
Papernot, N. and McDaniel, P. (2018). Deep k-Nearest Neighbors: Towards conﬁdent, interpretable and robust deep learning. arXiv preprint arXiv:1803.04765.
Pennington, J. and Bahri, Y. (2017). Geometry of neural network loss surfaces via random matrix theory. In International Conference on Machine Learning.
Raghu, M., Gilmer, J., Yosinski, J., and Sohl-Dickstein, J. (2017). SVCCA: singular vector canonical correlation analysis for deep learning dynamics and interpretability. In Proceedings of the 31st International Conference on Neural Information Processing Systems.
Recht, B., Roelofs, R., Schmidt, L., and Shankar, V. (2019). Do ImageNet classiﬁers generalize to ImageNet? In International Conference on Machine Learning.
Sagun, L., Bottou, L., and LeCun, Y. (2016). Eigenvalues of the Hessian in deep learning: Singularity and beyond. arXiv preprint arXiv:1611.07476.
Sagun, L., Evci, U., Guney, V. U., Dauphin, Y., and Bottou, L. (2018). Empirical analysis of the Hessian of over-parametrized neural networks. In International Conference on Learning Representations (Workshop).
Sanger, T. D. (1994). Neural network learning control of robot manipulators using gradually increasing task diﬃculty. IEEE transactions on Robotics and Automation, 10(3):323–333.
14

Schwartz, R., Stanovsky, G., Swayamdipta, S., Dodge, J., and Smith, N. A. (2020). The right tool for the job: Matching model and instance complexities. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics.
Simonyan, K. and Zisserman, A. (2015). Very deep convolutional networks for large-scale image recognition. In International Conference on Learning Representations.
Smith, S. L., Dherin, B., Barrett, D. G., and De, S. (2021). On the origin of implicit regularization in stochastic gradient descent. In International Conference on Learning Representations.
Smith, S. L., Kindermans, P.-J., Ying, C., and Le, Q. V. (2018). Don’t decay the learning rate, increase the batch size. In International Conference on Learning Representations.
Smith, S. L. and Le, Q. V. (2018). A Bayesian perspective on generalization and stochastic gradient descent. In International Conference on Learning Representations.
Soudry, D., Hoﬀer, E., Nacson, M. S., Gunasekar, S., and Srebro, N. (2018). The implicit bias of gradient descent on separable data. The Journal of Machine Learning Research, 19(1):2822–2878.
Stephan, M., Hoﬀman, M. D., Blei, D. M., et al. (2017). Stochastic gradient descent as approximate Bayesian inference. Journal of Machine Learning Research, 18(134):1–35.
Stephenson, C., suchismita padhy, Ganesh, A., Hui, Y., Tang, H., and Chung, S. (2021). On the geometry of generalization and memorization in deep neural networks. In International Conference on Learning Representations.
Teerapittayanon, S., McDanel, B., and Kung, H.-T. (2016). BranchyNet: Fast inference via early exiting from deep neural networks. In International Conference on Pattern Recognition.
Toneva, M., Sordoni, A., des Combes, R. T., Trischler, A., Bengio, Y., and Gordon, G. J. (2019). An empirical study of example forgetting during deep neural network learning. In International Conference on Learning Representations.
Unterthiner, T., Keysers, D., Gelly, S., Bousquet, O., and Tolstikhin, I. (2020). Predicting neural network accuracy from weights. arXiv preprint arXiv:2002.11448.
Wen, Y., Tran, D., and Ba, J. (2019). BatchEnsemble: An alternative approach to eﬃcient ensemble and lifelong learning. In International Conference on Learning Representations.
Wenzel, F., Snoek, J., Tran, D., and Jenatton, R. (2020). Hyperparameter ensembles for robustness and uncertainty quantiﬁcation. In Proceedings of the 34th International Conference on Neural Information Processing Systems.
Wu, X., Dyer, E., and Neyshabur, B. (2021). When do curricula work? In International Conference on Learning Representations.
Xiao, H., Rasul, K., and Vollgraf, R. (2017). Fashion-MNIST: A novel image dataset for benchmarking machine learning algorithms. arXiv preprint arXiv:1708.07747.
Xin, J., Tang, R., Lee, J., Yu, Y., and Lin, J. (2020). DeeBERT: Dynamic early exiting for accelerating BERT inference. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics.
Yao, Z., Gholami, A., Keutzer, K., and Mahoney, M. (2020). PyHessian: Neural networks through the lens of the Hessian. In International Conference on Machine Learning (Workshop).
Zielinski, P., Krishnan, S., and Chatterjee, S. (2020). Weak and strong gradient directions: Explaining memorization, generalization, and hardness of examples at scale. arXiv preprint arXiv:2003.07422.
15

A Detailed Description of Experiments, Architectures and Hyperparameter Optimization
For each combination of dataset (CIFAR10, CIFAR100, Fashion MNIST, SVHN) and architecture (ResNet18, VGG16, MLP) we train 250 models with a 10% validation split selected at random each time, and an additional 25 models on the full training set.
A.1 Datasets
CIFAR10 / CIFAR100: Reference: (Krizhevsky et al., 2009). License: MIT. URL: https://www.cs.toronto.edu/~kriz/cifar.html
Fashion MNIST: Reference: (Xiao et al., 2017). License: MIT. URL: https://github.com/zalandoresearch/fashion-mnist
Street View House Numbers: Reference: (Netzer et al., 2011). License: CC0. URLs: http://ufldl.stanford.edu/housenumbers/ https://www.kaggle.com/stanfordu/street-view-house-numbers
A.2 Architectures
A.2.1 ResNet18
We implemented the standard ResNet18 architecture for CIFAR10 (He et al., 2016), except that we replaced Batch Norm with Group Norm and applied Weight Standardization, following recent state of the art (Kolesnikov et al., 2020).
A.2.2 VGG16
We used VGG16 (Simonyan and Zisserman, 2015), except that we removed the ﬁnal three dense layers: a standard modiﬁcation for datasets smaller than ImageNet. We also did not use batch-norm or dropout: our focus is on understanding trends in example diﬃculty and we do not expect the results to be dependent on these devices.
A.2.3 MLP
Our MLP architecture comprises seven hidden layers with ReLU activations. We chose seven layers after performing the experiments shown in Figure A.9. There we show the accuracies of k-NN probes placed after each operation of two MLP architectures, depths 15 layers and 7 layers, both of width 2048. We used CIFAR10 with 40% ﬁxed random label noise as a reasonably diﬃcult model classiﬁcation task, to choose the depth.
A.2.4 Data augmentation
We did not apply data augmentation: diﬀerent data augmentation schemes could be expected to have disparate eﬀects on diﬀerent examples, but we do not expect them to change the overall phenomena that we report here. We leave the use of data augmentation to subsequent studies.
16

k-NN Probe Accuracy
FlIanttpeunt DReenLseU DReenLseU DReenLseU DReenLseU DReenLseU DReenLseU DReenLseU DReenLseU DReenLseU DReenLseU DReenLseU DReenLseU DReenLseU DReenLseU DReenLseU Logits
k-NN Probe Accuracy
Input Flatten Dense
ReLU Dense ReLU Dense ReLU Dense ReLU Dense ReLU Dense ReLU Dense ReLU Logits

15 Layers

7 Layers

1.00

Unchanged Data

1.00 Unchanged data

0.75

Mislabelled Data Test

0.75

Mislabelled data Test

0.50

0.50

0.25

0.25

0.00

0.00

Operation

Operation

Figure A.9: Seven layers are suﬃcient in MLP for CIFAR10 with 40% random label noise. CIFAR10 with 40% random label noise. For this plot, k-NN probes were placed after every operation in two MLP architectures of the same width (2048) but diﬀerent depths. Left: 15 dense layers; Right: 7 dense layers. Separate accuracies are reported for the test split, those data points in the training split with unchanged labels and the randomly mislabeled data in the training split.

A.3 Hyperparameter optimization
For each architecture and dataset we initially performed 104 steps of SGD with momentum, using all combinations of the following hyperparameters: learning rate ∈ [4 × 101, 1 × 10−1, 4 × 10−2, 1 × 10−2, 4 × 10−3, 1 × 10−3, 4 × 10−4, 1 × 10−4] ; momentum ∈ [0.0, 0.5, 0.9, 0.95]; weight decay ∈ [0, 5 × 10−4]. In CIFAR10, we additionally considered a learning rate of 2 × 10−2. For each dataset and architecture we selected the 7 most accurate and stable training curves, extended the number of training steps and added a learning rate schedule, reducing the learning rate in steps of 15 . At least two rounds of optimization were performed to adapt the learning rate schedule for each combination of architecture and dataset. In each case a mini-batch size of 256 was used. The ﬁnal parameters obtained are shown in Table 1, which also gives the hyperparameters used in Sec. 3.3 and Appendix A.2.3 for CIFAR10 with 40% label noise. Final accuracies of the trained models are given in Table 2.
A.4 Convergence and consistency of k-NN probe accuracies
We tested the convergence of k in k-NN for VGG16 on CIFAR10. Figure A.10 shows the accuracies of k-NN probes after every operation of the network for k ∈ [3, 10, 30]. We see that these k-NN probe accuracies are insensitive to k for k = 30.
Figure A.9 shows separate results for ﬁve independent training runs. Similarly, Figure 5 (right) and Figure 6 (right) each show the mean and uncertainty on the k-NN probe accuracies from 5 independent runs. The spread of results in these ﬁgures is tight, demonstrating consistency of the results.
A.5 Placement of k-NN probes
For prediction depth, in MLP we constructed k-NN probes after the dense operations and the softmax, in VGG16 after the convolutions and softmax, and in ResNet18 we constructed the probes after the initial Group Norm operation, the sum operations at the end of each block and after the softmax operation.
From ﬁgures A.9 and A.10 it is clear that there are upper and lower envelopes that bound the k-NN probe accuracies: the lower envelope corresponds to the ReLU activations and the upper envelope to the operations immediately preceding them. We chose the preceding operations which, in eﬀect, conceptually shifts the
17

ResNet18 VGG16 MLP
ResNet18 VGG16 MLP
ResNet18 VGG16 MLP
VGG16 MLP
ResNet18 VGG16 MLP

Learning Rate Momentum Weight Decay Schedule / steps

SVHN

4 × 10−2

0.95

0.0

[7000]

4 × 10−2

0.9

0.0

[3000, 6000, 1000]

4 × 10−2

0.9

0.0

[2500, 5500, 2000]

Fashion MNIST

1 × 10−2

0.95

0.0

[4000, 3000]

1 × 10−2

0.95

0.0

[3000, 6000, 1000]

4 × 10−2

0.5

0.0

[10000, 2500]

CIFAR10

4 × 10−2

0.95

0.0

[7000]

4 × 10−2

0.9

0.0

[5000, 1000]

2 × 10−2

0.9

0.0

[5000, 1250, 1000]

CIFAR10 w/ 40% (Fixed) Randomized Labels

4 × 10−2

0.9

0.0

[5000, 10000]

2 × 10−2

0.9

0.0

[12000, 1250, 4000

CIFAR100

1 × 10−1

0.95

0.0

[6000]

4 × 10−2

0.9

0.0

[2500, 7500]

1 × 10−1

0.95

0.0

[2500, 6000, 1500]

Table 1: Training parameters for each model and dataset.

ResNet18 VGG16 MLP

SVHN
95% 95% 85%

Fashion MNIST
93% 93% 90%

CIFAR10
83% 83% 59%

CIFAR100
56% 45% 29%

Table 2: Final accuracies of the trained models.

ReLU activations to the “start” of a layer rather than the “end” of the preceding layer.
A.6 Notes on deﬁnitions
A.6.1 Consistency of the model’s prediction with the k-NN probe after the softmax layer
Deep classiﬁer models are trained to create linear separation of the classes in the softmax layer. There is nearly perfect agreement between the k-NN probe after the softmax layer and predictions of the full model. In the rare case that the k-NN probe after the softmax predicts a diﬀerent class from the full network we do not assign a prediction depth. Such data points are extremely rare: we found zero such data points in the large majority of models and always fewer than 1 in 104.
A.6.2 Tiebreaks in the consensus class
When obtaining the consensus class, if predictions are tied between more than one class and the ground truth is in the tiebreak, then we break the tie in favor of the ground truth class. If the ground truth is not in the tiebreak then we report the tied class with the lowest integer index. This choice was motivated by ease of implementation. We are conﬁdent that the overall results we report are unaﬀected by this choice.

18

k-NN Probe Accuracy
ICRnoepLnuvtU MaxCRPoeoLnovlU
CCRooeLnnvvU MaxCRPoeoLnovlU
CRRoeeLLnvUU MaxCRPoeoLnovlU
CCRooeLnnvvU CRRoeeLLnvUU MaxCRPoeoLnovlU CCRooeLnnvvU MaFlxaRtPteoLeonlU SoftMax

1.0

0.8

0.6

k = 30

0.4

k = 10

0.2

k=3

Operation
Figure A.10: CIFAR10, VGG16. k-NN probe accuracies after each operation for k ∈ [3, 10, 30]. Solid lines: training set. Dotted lines: test set. Diﬀerences in these results are comparable to the scatter observed for networks trained with diﬀerent random seeds at k = 30.
A.6.3 Estimating the consensus-consistency
We used the same ensemble to obtain both the consensus class yˆA (x) and the consensus-consistency score. Thus we are reporting relationships between observables for a given ensemble. This is a biased estimator of (2): an unbiased estimator could have been constructed by training an additional set of models to obtain the consensus class, but at greater cost. We are conﬁdent that this does not aﬀect the conclusions of this study.
A.7 Justiﬁcation and hyperparameters for the output margin intervention
A number of published works informed the design of our intervention. Firstly, Soudry et al. (2018) demonstrate that the cross-entropy (CE) loss leads to large margins. In contrast to the cross-entropy, the 0-Hinge loss has zero gradient if the prediction is correct, so it does not push the model to become arbitrarily conﬁdent. Secondly, Keskar et al. (2017) show that smaller batch sizes lead to the discovery of ﬂatter minima, which also corresponds to a wider margin (Neyshabur et al., 2017). Thirdly, Keskar et al. (2017), Smith and Le (2018) and Smith et al. (2018) show that the gradient noise level in stochastic gradient descent is proportional to LeBaarntcinhgSRizaete . Having an appreciable noise level early in training plays an important role in ﬁnding the ﬂatter minima with larger output margins reported in Keskar et al. (2017). Our intervention to minimize the margin therefore combines both of the following changes:
1. Changing the loss from cross-entropy to the 0-Hinge loss 2. Minimizing the learning rate and making the batch size as large as possible
To test whether both or only one of these changes is required to obtain small output margins, we performed separate runs, without any intervention, applying the changes individually and applying them together. The starting point (the control) is training with cross-entropy loss and SGD with momentum and large initial learning rate.
We trained VGG16 on CIFAR10. The hyperparameters, presented in Table 3, were set for each loss, to obtain nearly smooth learning curves for full-batch gradient descent and very noisy learning curves for SGD. In Figure A.11 we show the learning curves for these models. Since full-batch gradients are expensive to compute we restricted the experiments to separating two classes (“Horse” and “Deer”) with 4096 training images in total (evenly split).
19

Name
CE, SGD CE, GD 0-Hinge, SGD 0-Hinge, GD

Batch Size
256 4096 256 4096

Initial Learning Rate
4 × 10−3 6.4 × 10−6 4 × 10−2 6.4 × 10−5

Schedule / Steps
[3200] [1.2 × 106] [5000, 2500] [8 × 105]

Momentum
0.9 0.9 0.9 0.95

Table 3: Hyperparameters for all combinations of CE vs. 0-Hinge loss and SGD with momentum and large

initial learning rate vs. GD with momentum and small learning rate. In the learning rate schedules we

reduced

the

learning

rate

by

a

factor

of

1 5

for

each

new

set

of

training

steps.

Weight

decay

was

not

employed

in these calculations since we do not expect typical, modest amounts of weight decay to qualitatively aﬀect

the results.

Accuracy Accuracy Accuracy Accuracy

1.0 Cross-Ent, SGD

1.0 0-Hinge, SGD

1.0 Cross-Ent, GD

1.0 0-Hinge, GD

0.8

0.8

0.8

0.8

0.6

0.6

0.6

0.6

0.4

0 50 E1p0o0ch 150 200 0 100 2E0p0oc3h00 400 500 0.0 0.3 Ep0o.6ch 0.9 11e.62 0.000 2.001 E4p.0o0c1h 6.002 81.e0502

Figure A.11: Training curves for Cross-Entropy and 0-Hinge Losses, with either SGD with momentum and large
initial learning rate, or GD with momentum and a small learning rate. The initial learning rates and schedules are set to obtain nearly smooth learning curves for GD and noisy learning curves for SGD. Each plot shows ﬁve separate learning curves. Solid lines show training accuracies and dotted lines show test accuracies.

Table 4 lists the mean accuracy and output margin for all four combinations of loss function and optimizer. We can see that the combination of both changes yields the smallest mean output margin, 102 times smaller than the next smallest margin. Figure A.12 presents the k-NN probe accuracies in the hidden layers for all four combinations of loss and optimizer. The combined intervention, which has the smallest margin, leads to the data being accurately clustered in the very latest layers.

Name
CE, SGD CE, GD 0-Hinge, SGD 0-Hinge, GD

Mean Accuracy
87.6% 86.7% 83.9% 69.5%

Mean Output Margin
1.6 × 101 1.1 × 101 6 × 10−2 2.0 × 10−4

Table 4: Mean accuracy and output margin for CE vs. 0-Hinge losses and SGD with momentum and large initial learning rate vs. GD with momentum and small learning rate.

B Further Related Work
Previous studies of deep learning on the level of individual data points have: sought to explain its accuracy by focusing on the interference of per-example gradients during training (Chatterjee, 2019; Zielinski et al., 2020); improved our understanding of deep learning by studying its performance on datasets with partially randomized labels, which corresponds to a speciﬁc binary partitioning of example diﬃculty Arpit et al. (2017); quantiﬁed example diﬃculty using 5 diﬀerent observables: 1) the change in a network’s output for elements of the training set after subsequent ﬁne-tuning on a disjoint dataset, 2) the adversarial input margin of an
20

k-NN Accuracy

Cross-Entropy, SGD
1.00

0.75

0.50

Untrained

0.25

Train

Test

0.00 0 1 2 3 4 5 6 7 8 9 1011121314

Layer

0-Hinge, SGD
1.00

0.75

0.50

Untrained

0.25

Train

Test

0.00 0 1 2 3 4 5 6 7 8 9 1011121314

Layer

k-NN accuracy

k-NN accuracy

Cross-Entropy, GD
1.00

0.75

0.50

Untrained

0.25

Train

Test

0.00 0 1 2 3 4 5 6 7 8 9 1011121314

Layer

0-Hinge, GD
1.00

0.75

0.50

Untrained

0.25

Train

Test

0.00 0 1 2 3 4 5 6 7 8 9 1011121314

Layer

k-NN Accuracy

Figure A.12: Accuracies of k-NN probes in the hidden layers of VGG16, resulting from each combination of
Cross-Entropy vs. 0-Hinge loss and SGD with momentum and large initial learning rate vs. GD with momentum and small learning rate. In each case we compare to the probes for untrained (freshly initialized) networks. Only the 0-Hinge with gradient descent using momentum and small learning rate (“0-Hinge, GD”) leads to clustering in the latest layers.
example, 3) the agreement of models in an ensemble, 4) the average conﬁdence of models in an ensemble, and 5) the disparate impact of diﬀerential privacy Carlini et al. (2019); identiﬁed diﬃcult examples with those disproportionately impacted by pruning and compression Hooker et al. (2019), with those whose classiﬁcations are more often forgotten during training Toneva et al. (2019), and with those that are least likely to be correctly classiﬁed in the validation set Jiang et al. (2021); demonstrated a correspondence between those examples that a human ﬁnds diﬃcult and examples a machine ﬁnds diﬃcult Lalor et al. (2018). In contrast to these works, we study the computational diﬃculty of inferring the class of an input: the amount of computation used to connect that input with its class label inside the network. Our deﬁnition of example diﬃculty is precisely described in Section 2.
In Hacohen et al. (2020) the authors report that the order during training in which data points are learned is common between diﬀerent architectures and random seeds in deep learning. In light of the correlation between prediction depth and the order of learning data points (as reported in Section 3.2), their result reﬂects the sanity checks performed in Section 2.2: that prediction depth is consistent between architectures and random seeds.
Distinct from the forms of example diﬃculty we describe in Section 4, Hooker et al. (2019) propose four diﬀerent forms of example diﬃculty: “ground truth label incorrect or inadequate”, “multiple-object image”, “corrupted image”, “ﬁne-grained classiﬁcation”. The forms of diﬃculty we describe in this paper follow directly from the computational diﬃculty of the examples, derived from the model’s behavior. In contrast, Hooker

21

et al. (2019) employ intuitive notions of diﬃculty to deﬁne their four forms and ask humans to assign diﬃcult examples to these categories.
The Deep k-Nearest Neighbors method Papernot and McDaniel (2018) builds a series of k-NN probes in the hidden spaces of the network. When a test example is processed by the network, Deep k-NN identiﬁes the nearest neighbors of the example in every layer, and then classiﬁes the example according to the class labels of the aggregated nearest neighbors. By comparing the number of neighbors the example has of the predicted class to the number of similarly labeled nearest neighbors that were recorded (across all layers) for examples in a hold-out test set, Deep k-NN is able to quantify the probability that the prediction is correct and to identify OOD examples. However, the authors do not report the phenomena reported here. Our results may yet enable the development of new Deep k-NN methods. Another algorithm Bahri et al. (2020) constructs a k-NN probe in the logit space of a network, and demonstrates that this enables improved detection of mislabeled data.

C Consistency of the Main Results Reported in the Paper

C.1 Consistency of prediction depth between architectures

To visually reinforce the correlations reported in Figure 2 (right), Figures C.13 to C.16 reproduce the result from Figure 2 (right) for all datasets in both the training and validation splits. For each combination of dataset and architecture we trained 250 models with random 90:10% training:validation splits as described in Appendix A. These histograms compare the mean prediction depths of the data points between diﬀerent architectures. Separate plots are shown for the training and validation splits. In each case we’ve rescaled prediction depth to the interval [0, 1] for visual ease of comparison between datasets. Each histogram is accompanied by the corresponding Spearman’s Correlation Coeﬃcient.

1.0

Trainin1g.0Split

0.5 0.00.0 0.5 1.0
PD: R=e0sN.9e1t18

102 0.5
100 0.00.0 0.5 1.0
PD:=V0G.G8216

1.0

Validati1o.n0 Split

102

0.5

0.5

100

0.00.0 0.5 1.0 0.00.0 0.5 1.0

PD: R=e0sN.9e0t18

PD:=V0G.7G316

Figure C.13: Consistency of prediction depth between architectures for SVHN. Histograms comparing the mean value of prediction depth obtained for each data point, across the ensemble of trained models. Left pair: training split. Right pair: validation split. Spearman’s Correlation Coeﬃcient is given beneath each plot. See Appendix C.1 for details.

PD: VGG16 PD: MLP PD: VGG16 PD: MLP

C.2 Relationship between prediction depth and prediction consistency
Figures C.17 and C.18 reproduce the results of Figure 3 and Figure 4 (left) for every dataset and architecture. The gradients of the linear bounds reported in the paper depend on the diﬃculty of the classiﬁcation task: easier tasks are solved after fewer layers.
Figure C.19 reproduces Figure 4 (middle) for every dataset and architecture. Similarly, Figure C.20 reproduces Figure 4 (right) for all datasets and architectures. Related to Figure C.19, in Figure C.21 we show that the prediction depth in one model can be used to estimate the prediction entropy of an ensemble
22

PD: VGG16 PD: MLP PD: VGG16 PD: MLP

1.0

Trainin1g.0Split

103

0.5

0.5

101

0.00.0 0.5 1.0 0.00.0 0.5 1.0

PD: R=e0sN.9e1t18

PD:=V0G.G8916

1.0

Validati1o.n0 Split

103

0.5

0.5

101

0.00.0 0.5 1.0 0.00.0 0.5 1.0

PD: R=e0sN.8e5t18

PD:=V0G.G7716

PD: VGG16 PD: MLP PD: VGG16 PD: MLP

Figure C.14: Consistency of prediction depth between architectures for Fashion MNIST. Histograms comparing the mean value of prediction depth obtained for each data point, across the ensemble of trained models. Left pair: training split. Right pair: validation split. Spearman’s Correlation Coeﬃcient is given beneath each plot. See Appendix C.1 for details.

1.0

Trainin1g.0Split

102

0.5

0.5

101

100

0.00.0 0.5 1.0 0.00.0 0.5 1.0

PD: R=e0sN.7e8t18

PD:=V0G.7G616

1.0

Validati1o.n0 Split

101

0.5

0.5

100

0.00.0 0.5 1.0 0.00.0 0.5 1.0

PD: R=e0sN.7e8t18

PD:=V0G.G6916

Figure C.15: Consistency of prediction depth between architectures for CIFAR100. Histograms comparing the mean value of prediction depth obtained for each data point, across the ensemble of trained models. Left pair: training split. Right pair: validation split. Spearman’s Correlation Coeﬃcient is given beneath each plot. See Appendix C.1 for details.

of models, where members of the ensemble have the same architecture and are trained using the same hyperparameters but with diﬀerent random seeds.

Prediction entropy: The entropy of predictions in an ensemble for an unseen input x. Consider an ensemble of models trained on r random subsets of the complete dataset S˜∼S\{(x, y)} (which explicitly do not
include (x, y)). We obtain the normalized histogram of the one-hot predictions of this ensemble for the
input x. The prediction entropy is the entropy of that histogram. For N classes the entropy of the
prediction histogram is given by

N

S(x) = − pi(x) log pi(x)

(3)

i=1

where pi(x) represents the fraction of models that predicted the class i for input x.

Figure C.22 shows the histogram of average prediction depth (validation set) vs. prediction entropy for each dataset and architecture. We remark that the mean prediction depth deﬁnes a linear upper bound on the prediction entropy similar to the corresponding linear lower bound on the consensus-consistency score (Figures C.17 and C.18).

23

PD: VGG16 PD: MLP PD: VGG16 PD: MLP

1.0

Trainin1g.0Split

102

0.5

0.5

101

100

0.00.0 0.5 1.0 0.00.0 0.5 1.0

PD: R=e0sN.7e5t18

PD:=V0G.G6616

1.0

Validati1o.n0 Split

102

0.5

0.5

101

100

0.00.0 0.5 1.0 0.00.0 0.5 1.0

PD: R=e0sN.7e6t18

PD:=V0G.G5016

Figure C.16: Consistency of prediction depth between architectures for CIFAR10. Histograms comparing the mean value of prediction depth obtained for each data point, across the ensemble of trained models. Left pair: training split. Right pair: validation split. Spearman’s Correlation Coeﬃcient is given beneath each plot. See Appendix C.1 for details.
C.3 Comparison of prediction depth and iteration learned
Figure C.23 reproduces the result shown in Figure 5 (left) for every architecture and dataset. To give a more complete picture of the relationship between the prediction depth and the iteration learned, Figures C.24 to C.27 show histograms of the mean prediction depth and iteration learned for each data point when it occurs in both the training and validation splits. As described in Appendix A, for each dataset and architecture we trained 250 models with random 90:10% validation:train splits. Each time a data point appears in either split we record the prediction depth and the iteration learned. These histograms compare the mean prediction depth to the mean iteration learned for all data points in both the train and validation splits. The Spearman’s Correlation Coeﬃcient is given beneath each plot.
C.4 Consistency of margin results
Figures C.28 to C.31 reproduce Figure 6 (left and middle) for all datasets and architectures in both the training and test splits.
C.5 Consistent two-dimensional relationship between prediction depths in the training and validation splits
Figures C.32 to C.35 demonstrate consistency of the histograms shown in Figure 7 for all datasets and architectures. As described in Appendix A, for each dataset and architecture we trained 250 models with random 90:10% validation:train splits. Each time a data point appears in either split we record the prediction depth. These histograms compare the mean prediction depths in the two splits for all data points which can be very diﬀerent from each other, depending on whether the consensus class matches or diﬀers from the ground truth class.
C.6 Evolution of clustering in the hidden layers for the diﬀerent forms of example diﬃculty
Figures C.36 to C.47 reproduce similar behavior to that shown in Figure 8 for all datasets and architectures. Please see Figure 8 for a detailed description.

24

Consistency

Consistency

Consistency

All Data CIFAR10

1.0

101

0.5

100

10-1

0.00 2 4 6 8 10

Prediction Depth

1G.0round Truth Consensus

0.5

100

0.00 2 4 6 8 10 Prediction Depth

CCoonnssiestnesnucs-y

ResNet18 All data

1.0

101

0.5

100

10-1

0.00 2 4 6 8 10

Prediction Depth

1G.0round Truth = Consensus
101

Consistency

0.5

100

10-1 0.00 2 4 6 8 10
Prediction Depth

All Data

CIFAR10 MLP All data

1.0

101 1.0

101

CCoonnssiestnesnucs-y

0.5

100 0.5

100

10-1 0.00 2 4 6 8
Prediction Depth

10-1 0.00 2 4 6 8
Prediction Depth

1G.0round Truth Consensus

1G.0round Truth = Consensus
101

0.5

100 0.5

100

Consistency

0.00 2 4 6 8 Prediction Depth

0.0 0

2

4

6

10-1 8

Prediction Depth

CCoonnssiestnesnucs-y

All Data CIFAR100 VGG16 All data

1.0

1.0

100

100

0.5

10-1 0.5

10-1

0.00 2 4 6 8 10 12 14 Prediction Depth
1G.0round Truth Consensus

0.5

100

0.00 2 4 6 8 10 12 14 10-1 Prediction Depth

Consistency

0.00 2 4 6 8 10 12 14 Prediction Depth
1G.0round Truth = Consensus
100 0.5 10-1 0.00 2 4 6 8 10 12 14
Prediction Depth

Consistency

Consistency

Consistency

Consistency

Consistency

Consistency

All Data CIFAR10

1.0

101

0.5

100

10-1

0.00 2 4 6 8 10 12 14 Prediction Depth

1G.0round Truth Consensus

0.5

100

0.00 2 4 6 8 10 12 14 Prediction Depth

Consistency

CCoonnssiestnesnucs-y

VGG16 All data

1.0

101

0.5

100

10-1

0.00 2 4 6 8 10 12 14

Prediction Depth

1G.0round Truth = Consensus
101

0.5

100

10-1

0.00 2 4 6 8 10 12 14

Prediction Depth

All Data CIFAR100
1.0

100

0.5

10-1

0.00 2 4 6 8 10 Prediction Depth

1G.0round Truth

Consensus
101

0.5

100

0.00 2 4 6 8 10 Prediction Depth

Consistency

CCoonnssiestnesnucs-y

ResNet18 All data
1.0 0.5 100
10-1 0.00 2 4 6 8 10
Prediction Depth
1G.0round Truth = Consensus101

0.5

100

10-1

0.00 2 4 6 8 10

Prediction Depth

CCoonnssiestnesnucs-y

All Data CIFAR100 MLP All data

1.0

1.0

100

100

0.5

0.5

10-1

10-1

0.00 2 4 6 8 Prediction Depth

0.00 2 4 6 8 Prediction Depth

1G.0round Truth

Consensus
101

1G.0round Truth = Consensus

Consistency

0.5

100

0.00 2 4 6 8 Prediction Depth

0.5

100

0.0 0

2

4

6

10-1 8

Prediction Depth

Consistency

Consistency

Consistency

Figure C.17: This ﬁgure demonstrates the consistency of the behavior shown in Figure 3 and Figure 4 (left) for all architectures with CIFAR10 and CIFAR100.
D Pertinence of example diﬃculty to topics in machine learning
We will describe the relevance of our work to distribution shift and robustness; algorithmic fairness, curriculum learning and models that explicitly address heteroscedastic uncertainty.
Distribution Shift and Robustness: Recent work has hypothesized that the linear relationship between the performance of a model before and after distribution shift could potentially be explained in a theory based on the diﬃculty of examples (Recht et al., 2019). Recent work has additionally discussed how examples that belong to a minority group might appear diﬃcult to classify correctly under distribution shift (Nagarajan et al., 2021). Therefore it seems natural to suppose that the richer picture of example
25

Consistency

Consistency

Consistency

All Data Fashion MNIST ResNet18 All data

1.0

101 1.0

101

CCoonnssiestnesnucs-y

0.5

100 0.5

100

0.0 0

2

4

6

10-1 8 10

Prediction Depth

0.0 0

2

4

6

10-1 8 10

Prediction Depth

1G.0round Truth Consensus

1G.0round Truth = Consensus
101

Consistency

0.5

100

0.00 2 4 6 8 10 Prediction Depth

0.5

100

0.0 0

2

4

6

10-1 8 10

Prediction Depth

All Data Fashion MNIST MLP All data

1.0

101 1.0

101

CCoonnssiestnesnucs-y

0.5

100 0.5

100

0.0 0

2

4

6

10-1 8

Prediction Depth

0.00 2 4 6 8 10-1 Prediction Depth

1G.0round Truth Consensus
101

1G.0round Truth = Consensus
101

Consistency

0.5

0.0 0

2

4

6

100 8

Prediction Depth

0.5

100

0.00 2 4 6 8 10-1 Prediction Depth

All Data

SVHN VGG16 All data

1.0

101 1.0

101

CCoonnssiestnesnucs-y

0.5

0.5

100

10-1

10-1

0.00 2 4 6 8 10 12 14 Prediction Depth

0.00 2 4 6 8 10 12 14 Prediction Depth

1G.0round Truth

Consensus
3 × 100 2 × 100

0.5

100

0.0 0

2

4

6

6 × 10-1 8 10 12 14

Prediction Depth

Consistency

1G.0round Truth = Consensus
101

0.5

100

10-1

0.00 2 4 6 8 10 12 14

Prediction Depth

Consistency

Consistency

Consistency

Consistency

Consistency

Consistency

All Data Fashion MNIST VGG16 All data

1.0

101 1.0

101

CCoonnssiestnesnucs-y

0.5

100 0.5

100

10-1 0.00 2 4 6 8 10 12 14
Prediction Depth

10-1 0.00 2 4 6 8 10 12 14
Prediction Depth

Consistency

1G.0round Truth
0.5

Consensus
100

1G.0round Truth = Consensus
101

0.5

100

0.00 2 4 6 8 10 12 14 Prediction Depth

0.00 2 4 6 8 10 12 14 10-1 Prediction Depth

All Data SVHN
1.0 101

0.5

100

10-1

0.00 2 4 6 8 10

Prediction Depth

1G.0round Truth Consensus

0.5 100
0.00 2 4 6 8 10 Prediction Depth

ResNet18 All data
1.0 101

CCoonnssiestnesnucs-y

0.5

100

10-1 0.00 2 4 6 8 10
Prediction Depth

Consistency

1G.0round Truth = Consensus
101

0.5

100

10-1 0.00 2 4 6 8 10
Prediction Depth

All Data

SVHN MLP

All data

1.0

102 1.0

102

CCoonnssiestnesnucs-y

0.5

100 0.5

100

0.00 2 4 6 8 Prediction Depth
1G.0round Truth Consensus

0.00 2 4 6 8 Prediction Depth

1G.0round

Truth

=

Consensus
102

Consistency

0.5

100 0.5

100

0.00 2 4 6 8 Prediction Depth

0.00 2 4 6 8 Prediction Depth

Consistency

Consistency

Consistency

Figure C.18: This ﬁgure demonstrates the consistency of the behavior shown in Figure 3 and Figure 4 (left) for all architectures with Fashion MNIST and SVHN.
diﬃculty we introduce could lead to a deeper understanding of distribution shift and aid with the development of more robust algorithms.
Curriculum Learning: This class of training algorithms exploits additional information about a dataset (obtained in advance) to present easier examples earlier in the training process (Elman, 1993; Sanger, 1994; Bengio et al., 2009). Diﬀerent notions of diﬃculty have been the subject of several related studies (Bengio et al., 2009; Toneva et al., 2019; Hacohen and Weinshall, 2019) and it has been shown that (neglecting the cost of obtaining the curriculum) following a curriculum can improve training time signiﬁcantly, particularly for large training data (Wu et al., 2021). We envisage that richer, more eﬀective curricula could be designed by distinguishing diﬀerent forms of example diﬃculty. This could,
26

Consensus-Consistency

Consensus-Consistency

Consensus-Consistency

ResNet18
1.0

VGG16
1.0

MLP
1.0

0.8 FMNIST

0.8

0.8

SVHN 0.6 CIFAR10

0.6

0.6

CIFAR100 0 2 4 6 8 10

0.4 0 2 4 6 8 10 12 14

0.4 0

2

4

6

8

Prediction Depth

Prediction Depth

Prediction Depth

Accuracy

Figure C.19: This ﬁgure demonstrates the consistency of the result shown in Figure 4 (middle) for all datasets and architectures.

ResNet18
1.0

0.8

0.6

FMNIST SVHN

0.4 CIFAR10

CIFAR100

0.2 0 2 4 6 8 10

Prediction Depth

Accuracy

VGG16
1.0

MLP
1.0

0.8

0.8

Accuracy

0.6

0.6

0.4

0.4

0.2

0.2

0 2 4 6 8 10 12 14 0.0 0 2 4 6 8

Prediction Depth

Prediction Depth

Prediction Entropy

Figure C.20: This ﬁgure demonstrates the consistency of the result shown in Figure 4 (right) for all datasets and architectures.

Prediction Entropy

Prediction Entropy

ResNet18

1.5 FMNIST

1.0

SVHN CIFAR10

0.5 CIFAR100

VGG16
1.5
1.0
0.5

MLP
1.5
1.0
0.5

0.0 0 2Predi4ction 6Depth8 10

0.0

0.0

0 2 Pr4edic6tion8Dep1t0h 12 14

0 Pr2edictio4n Dept6h 8

Figure C.21: The prediction depth in one model can be used to estimate the prediction entropy of an ensemble. The size of the marker indicates the fraction of data points with each prediction depth. We trained 25 models on each dataset and architecture with diﬀerent random seeds. We take the prediction depth from one trained model and report the average prediction entropy of the corresponding data points, where the prediction entropy is determined from the remaining 24 models. As in Figure C.19, predictions for data points with smaller prediction depths have lower mean entropy (are more consistent) than those of data points with larger prediction depths.
for example, be achieved setting the curriculum according to a each data point’s location in Figure 7.
Algorithmic Fairness: We have seen that mislabeled data is processed similarly to data that simply looks

27

mislabeled to the algorithm (both “look like a diﬀerent class”). This presents a fairness challenge when ﬁltering “noisy labels”. Similarly, we have seen that examples of rare subgroups (which are essential to include in the training set for robustness (Feldman and Zhang, 2020) and fairness (Hooker et al., 2020) are processed similarly to truly “ambiguous” inputs. Finding ways to deal with “label noise” without biasing against these subgroups remains an open challenge. In further work, we anticipate that examining datasets in an enlarged space of diﬀerent example diﬃculty measures (Jiang et al., 2021; Toneva et al., 2019; Carlini et al., 2019; Hooker et al., 2019; Lalor et al., 2018; Agarwal and Hooker, 2020) may allow algorithms that distinguish between these diﬀerent sources of label noise to reach higher accuracy and to be fairer. Heteroscedastic Uncertainty: There are a class of models with two heads, one to model the mean and the other the uncertainty of the prediction (E.g. Kendall and Gal (2017); Kendall et al. (2018)). These models learn to become uncertain on diﬃcult inputs and treat example diﬃculty as a one-dimensional quantity. It seems highly likely that this uncertainty will lead to the model down-weighting examples of rare subgroups in the data. We suggest that methods for modeling uncertainty could additionally be tasked with estimating the location of a training point in Figure 7. It seems plausible to suppose that new models able to distinguish the form of an example’s diﬃculty could later be reﬁned to be fairer, more accurate and better calibrated.
E Alternative Deﬁnitions for Prediction Depth
Instead of using the network’s ﬁnal prediction on a data point to assign the prediction depth, one could instead use the ground truth label. This would require a diﬀerent rule for assigning a prediction depth to validation data points that are incorrectly classiﬁed as compared to data points that are correctly classiﬁed. We consider our deﬁnition to be simpler than combining two separate rules.
One could alternatively have deﬁned the prediction depth for each example by ﬁrst leaving it out of the training set, and then training networks of diﬀerent depths to identify the number of layers required to classify it correctly. In fact, architectures of diﬀerent depths have diﬀerent inductive biases, so the relative diﬃculty of inputs can become inverted with changing depth (Mangalam and Prabhu, 2019). Such an approach would be expensive but could lead to a rich picture of how example diﬃculty changes with architecture.
Another potential approach would have been to use a linear classiﬁer such as Logistic Regression in the embedding spaces. Indeed linear probes, logistic regression and SVM probes have been previously applied to the hidden spaces of DNNs (E.g. Cohen et al. (2018); Alain and Bengio (2017)).
Figure E.48 compares the behavior of k-NN probes and Logistic Regression (LR) probes after the convolution operations of VGG16 with CIFAR10. LR is able to completely separate the training set after the ﬁrst convolution operation. We also show the behavior when training LR on a random 50% of the dataset and predicting on the other half. k-NN shows lower accuracy until the classes become entirely clustered. We chose k-NN probes for this investigation.
28

Prediction Entropy

Prediction Entropy

2

102

1 100
00.0 0.2 0.4 0.6 0.8 1.0 Predic=tio0n.7D4epth

2

101

1

100

10 1 00.0 0.2 0.4 0.6 0.8 1.0
Predic=tio0n.6D9epth

1.5

102

1.0

0.5

100

0.00.0 0.2 0.4 0.6 0.8 1.0 Predic=tio0n.6D8epth

2 102

1 100

00.0 0.2 0.4 0.6 0.8 1.0 Predic=tio0n.6D7epth

Prediction Entropy

Prediction Entropy

Prediction Entropy

Prediction Entropy

2

102

1 100
00.0 0.2 0.4 0.6 0.8 1.0 Predic=tio0n.7D0epth
3 101
2 100
1 10 1
00.0 0.2 0.4 0.6 0.8 1.0 Predic=tio0n.7D8epth

1.5

102

1.0

0.5

100

0.00.0 0.2 0.4 0.6 0.8 1.0 Predic=tio0n.7D5epth

2 102

1 100

00.0 0.2 0.4 0.6 0.8 1.0 Predic=tio0n.6D3epth

Prediction Entropy

Prediction Entropy

Prediction Entropy

Prediction Entropy

2

102

1 100

00.0 0.2 0.4 0.6 0.8 1.0 Predic=tio0n.7D6epth

3

2

101

1

100

10 1 00.0 0.2 0.4 0.6 0.8 1.0
Predic=tio0n.7D8epth

1.5 1.0 102

0.5

100

0.00.0 0.2 0.4 0.6 0.8 1.0 Predic=tio0n.7D4epth

2

102

1 100

00.0 0.2 0.4 0.6 0.8 1.0 Predic=tio0n.7D9epth

Prediction Entropy

Prediction Entropy

Figure C.22: First (Top) Row: CIFAR10. Second Row: CIFAR100. Third Row: Fashion MNIST. Fourth (Bottom) Row: SVHN. Left Column: ResNet18. Middle Column: VGG16. Right Column: MLP. Histograms showing consistency of the relationship between prediction depth in the validation set and prediction entropy of an ensemble. As described in Appendix A, for each dataset and architecture we trained 250 models with random 90:10% validation:train splits. Each time a data point appears in the validation split we record the prediction depth and the prediction. These histograms compare the average prediction depth for each data point to its prediction entropy. We observe that the prediction depth gives linear upper bounds for the prediction entropy as it does linear lower bounds for the consensus-consistency (Figures C.17 and C.18).

29

Iteration Learned Iteration Learned Iteration Learned

ResNet18

1.0

FMNIST SVHN

0.8

CIFAR10 CIFAR100

0.6

VGG16
1.5
1.0

MLP
1.0 0.8 0.6

0.4

0.5

0.4

0.2

0.2

0.0 0 2 4 6 8 10

0.0 0 2 4 6 8 10 12 14

0.0 0

2

4

6

8

Prediction Depth

Prediction Depth

Prediction Depth

Figure C.23: This ﬁgure demonstrates the consistency of the result shown in Figure 5 (left) for all datasets and architectures.

30

Iteration Learned: Train

Iteration Learned: Train

1.25 1.00 0.75 0.50 0.25 0.000.0

0P.r2edict0io.4n De0p.t6h: Tr0a.i8n = 0.74

101 100 1.0 10 1

1.25 1.00 0.75 0.50 0.25 0.000.0

0P.r2edict0io.4n De0p.t6h: Tr0a.i8n = 0.62

101 100 1.0

1.25

1.00

101

0.75

0.50

100

0.25

0.000.0 0.2 0.4 0.6 0.8 1.0 Prediction D=ep0t.6h3: Validation

Iteration Learned: Validation

Iteration Learned: Validation

Iteration Learned: Validation

1.25

1.00

101

0.75

0.50

100

0.25

0.000.0 0.2 0.4 0.6 0.8 1.0 10 1 Prediction D=ep0t.7h9: Validation

1.25

1.00

101

0.75

0.50

100

0.25

0.000.0 0.2 0.4 0.6 0.8 1.0 Prediction D=ep0t.7h0: Validation

1.25

1.00

101

0.75

0.50

100

0.25

0.000.0 0.2 0.4 0.6 0.8 1.0 Prediction D=ep0t.6h3: Validation

Figure C.24: CIFAR10. Top row: ResNet18. Middle row: VGG16. Bottom row: MLP. Histogram comparing the mean prediction depth to the mean iteration learned when each data point occurs in either the training split (left column) or the validation split (right column). See Appendix C.3 for a description of the experiments performed.

Iteration Learned: Validation

31

Iteration Learned: Train

Iteration Learned: Train

1.25 1.00 0.75 0.50 0.25 0.000.0

0P.r2edict0io.4n De0p.t6h: Tr0a.i8n = 0.68

101
100
10 1 1.0

1.25 1.00 0.75 0.50 0.25 0.000.0

0P.r2edict0io.4n De0p.t6h: Tr0a.i8n = 0.72

101
100
10 1 1.0

1.25

1.00

101

0.75

0.50

100

0.25

0.000.0 0.2 0.4 0.6 0.8 1.0 10 1 Prediction D=ep0t.7h9: Validation

Iteration Learned: Validation

Iteration Learned: Validation

Iteration Learned: Validation

1.25

1.00

101

0.75

0.50

100

0.25 0.000.0 0.2 0.4 0.6 0.8 1.0 10 1
Prediction D=ep0t.7h5: Validation

1.25

1.00

101

0.75

0.50

100

0.25

0.000.0 0.2 0.4 0.6 0.8 1.0 Prediction D=ep0t.8h3: Validation

1.25

1.00

101

0.75

0.50

100

0.25

0.000.0 0.2 0.4 0.6 0.8 1.0 10 1 Prediction D=ep0t.7h9: Validation

Figure C.25: CIFAR100. Top row: ResNet18. Middle row: VGG16. Bottom row: MLP. Histogram comparing the mean prediction depth to the mean iteration learned when each data point occurs in either the training split (left column) or the validation split (right column). See Appendix C.3 for a description of the experiments performed.

Iteration Learned: Validation

32

Iteration Learned: Train

Iteration Learned: Train

1.25 1.00 0.75 0.50 0.25 0.000.0

0P.r2edict0io.4n De0p.t6h: Tr0a.i8n = 0.67

103 102 101 100 1.0 10 1

1.25

103

1.00 0.75 0.50 0.25 0.000.0

0P.r2edict0io.4n De0p.t6h: Tr0a.i8n = 0.74

102 101 100 1.0 10 1

1.25

103

1.00

102

0.75

101

0.50

0.25

100

0.000.0 0.2 0.4 0.6 0.8 1.0 10 1 Prediction D=ep0t.7h0: Validation

Iteration Learned: Validation

Iteration Learned: Validation

Iteration Learned: Validation

1.25

103

1.00

102

0.75

0.50

101

0.25

100

0.000.0 0.2 0.4 0.6 0.8 1.0 10 1 Prediction D=ep0t.7h0: Validation

1.25

103

1.00

102

0.75

101

0.50

0.25

100

0.000.0 0.2 0.4 0.6 0.8 1.0 10 1 Prediction D=ep0t.7h9: Validation

1.25

103

1.00

102

0.75

101

0.50

0.25

100

0.000.0 0.2 0.4 0.6 0.8 1.0 10 1 Prediction D=ep0t.7h0: Validation

Figure C.26: Fashion MNIST. Top row: ResNet18. Middle row: VGG16. Bottom row: MLP. Histogram comparing the mean prediction depth to the mean iteration learned when each data point occurs in either the training split (left column) or the validation split (right column). In this case, the large majority of the data is already learned in the input layer. See Appendix C.3 for a description of the experiments performed.

Iteration Learned: Validation

33

Iteration Learned: Train

Iteration Learned: Train

1.25 1.00 0.75 0.50 0.25 0.000.0

0P.r2edict0io.4n De0p.t6h: Tr0a.i8n = 0.73

102 101 100 10 1 1.0

1.25 1.00 0.75 0.50 0.25 0.000.0

0P.r2edict0io.4n De0p.t6h: Tr0a.i8n = 0.75

102 101 100 10 1 1.0

1.25

1.00

102

0.75

101

0.50

0.25

100

0.000.0 0.2 0.4 0.6 0.8 1.0 10 1 Prediction D=ep0t.8h3: Validation

Iteration Learned: Validation

Iteration Learned: Validation

Iteration Learned: Validation

1.25

1.00

102

0.75

101

0.50

100

0.25

0.000.0

0.2

0.4

0.6

0.8

10 1 1.0

Prediction D=ep0t.7h7: Validation

1.25

102

1.00

0.75

101

0.50

100

0.25

0.000.0

0.2

0.4

0.6

0.8

10 1 1.0

Prediction D=ep0t.7h7: Validation

1.25

1.00

102

0.75

101

0.50

0.25

100

0.000.0 0.2 0.4 0.6 0.8 1.0 10 1 Prediction D=ep0t.8h3: Validation

Figure C.27: SVHN. Top row: ResNet18. Middle row: VGG16. Bottom row: MLP. Histogram comparing the mean prediction depth to the mean iteration learned when each data point occurs in either the training split (left column) or the validation split (right column). See Appendix C.3 for a description of the experiments performed.

Iteration Learned: Validation

34

Output Margin

Output Margin

SVHN ResNet18 Test
= -0.73

103

101

102

101

0 2 4 6 8 10 100 Prediction Depth

SVHN VGG16 Test
= -0.62 102

101

101

0 2 4 6 8 10 12 14 100 Prediction Depth
SVHN MLP Test
= -0.78 102
101 101

0 2 4 6 8 100 Prediction Depth

Input Margin

Input Margin

Input Margin

SVHN ResNet18 Test

= -0.58

100

102

10 1

101

0 2 4 6 8 10 100 Prediction Depth

SVHN VGG16 Test
= -0.63

100

102

10 1

101

0 2 4 6 8 10 12 14 100 Prediction Depth

SVHN MLP Test
= -0.73

100

102

101

10 1 0

2

4

6

8 100

Prediction Depth

Output Margin

Output Margin

Output Margin

SVHN ResNet18 Train

2 × 101

= -0.62 103

102

101

101

0 2 4 6 8 10 100 Prediction Depth

SVHN VGG16 Train
= -0.51 103

102

101

101

0 2 4 6 8 10 12 14 100 Prediction Depth

SVHN MLP Train
= -0.66 103

102

101

101

0 2 4 6 8 100 Prediction Depth

Input Margin

Input Margin

Input Margin

SVHN

ResNet18 Train
= -0.48

103

100

102

101

10 1 0

2

4

6

8 10 100

Prediction Depth

SVHN VGG16 Train
= -0.57 103

100

102

101

10 1 0

2

4

6

8 10 12 14

100

Prediction Depth

SVHN MLP Train
= -0.57 103

100

102

101

0 2 4 6 8 100 Prediction Depth

Figure C.28: Consistency of Figure 6, showing the correlation between prediction depth, and the input and output margins (log scale) for both the test and training splits of SVHN. The correlation coeﬃcient between the prediction depth and the logarithm of the margin is given in each plot. For each architecture, we train 25 models with diﬀerent random seeds on the full training split. We record the input and output margins together with the prediction depth for every data point in both the train and test splits. These histograms compare the mean values of each margin to the mean prediction depth for all data points.

Output Margin

35

Output Margin

Output Margin

Fashion MNIST ResNet18 Test
= -0.72

101

102

101

0 2 4 6 8 10 100 Prediction Depth

Fashion MNIST VGG16 Test
= -0.73 102

101

101

0 2 4 6 8 10 12 14 100 Prediction Depth
Fashion MNIST MLP Test
= -0.71 102
101 101

0 2 4 6 8 100 Prediction Depth

Input Margin

Input Margin

Input Margin

Fashion MNIST ResNet18 Test
= -0.65 102

10 1

101

0 2 4 6 8 10 100 Prediction Depth
Fashion MNIST VGG16 Test
100 = -0.70 102

101 10 1
0 2 4 6 8 10 12 14 100 Prediction Depth

Fashion MNIST MLP Test
= -0.69 102 100
101

10 1 0

2

4

6

8 100

Prediction Depth

Output Margin

Output Margin

Output Margin

2F×a1s0h1ion

MNIST

ResNet18
= -0.52

Train
103

102

101

101

0 2 4 6 8 10 100 Prediction Depth

4 3

×F1a0s1hion
× 101

MNIST

VGG16
= -0.60

Train
103

2 × 101

102

101

101

6 × 1000 2 4 6 8 10 12 14 100 Prediction Depth

Fashion MNIST MLP Train

4 × 101 3 × 101

= -0.50 103

2 × 101

102

101

101

6 × 100 0

2

4

6

8 100

Prediction Depth

Input Margin

Input Margin

Input Margin

Fashion MNIST ResNet18 Train
= -0.41 103

102

10 1

101

0 2 4 6 8 10 100 Prediction Depth

Fashion MNIST VGG16 Train

= -0.58 103

100

102

101

10 1 0 2 4 6 8 10 12 14 100 Prediction Depth
Fashion MNIST MLP Train
= -0.50 103

102 100
101

0 2 4 6 8 100 Prediction Depth

Figure C.29: Consistency of Figure 6, showing the correlation between prediction depth, and the input and output margins (log scale) for both the test and training splits of Fashion MNIST. The correlation coeﬃcient between the prediction depth and the logarithm of the margin is given in each plot. For each architecture, we train 25 models with diﬀerent random seeds on the full training split. We record the input and output margins together with the prediction depth for every data point in both the train and test splits. These histograms compare the mean values of each margin to the mean prediction depth for all data points.

Output Margin

36

Output Margin

Output Margin

CIFAR10 ResNet18 Test
= -0.79

101

101

0 2 4 6 8 10 100 Prediction Depth
CIFAR10 VGG16 Test
= -0.70
101 101

0 2 4 6 8 10 12 14 100 Prediction Depth
CIFAR10 MLP Test
= -0.72

101

101

0 2 4 6 8 100 Prediction Depth

Input Margin

Input Margin

Input Margin

CIFAR10 ResNet18 Test

100

= -0.74

101 10 1

0 2 4 6 8 10 100 Prediction Depth

CIFAR10 VGG16 Test

100

= -0.69

101

10 1

0 2 4 6 8 10 12 14 100 Prediction Depth

CIFAR10 MLP Test
= -0.72

100

101

10

1
0

2

4

6

8 100

Prediction Depth

Output Margin

Output Margin

Output Margin

CIFAR10 ResNet18 Train
= -0.61 102 2 × 101

101

101 0

2

4

6

8 10 100

Prediction Depth

CIFAR10 VGG16 Train
= -0.54 102

101

101 0

2

4

6

8 10 12 14

100

Prediction Depth

CIFAR10 MLP Train
= -0.40 102

101

101

Input Margin

Input Margin

CIFAR10 ResNet18 Train
100 = -0.46 102 101

0 2 4 6 8 10 100 Prediction Depth

CIFAR10 VGG16 Train

100

= -0.43 102

6 × 10 1

4 × 10 1

101

3 × 10 1

2 × 10 1 0

2

4

6

8 10 12 14

100

Prediction Depth

CIFAR10 MLP Train
= -0.33 102

100

101

Input Margin

0 2 4 6 8 100 Prediction Depth

0 2 4 6 8 100 Prediction Depth

Figure C.30: Consistency of Figure 6, showing the correlation between prediction depth, and the input and output margins (log scale) for both the test and training splits of CIFAR10. The correlation coeﬃcient between the prediction depth and the logarithm of the margin is given in each plot. For each architecture, we train 25 models with diﬀerent random seeds on the full training split. We record the input and output margins together with the prediction depth for every data point in both the train and test splits. These histograms compare the mean values of each margin to the mean prediction depth for all data points.

Output Margin

37

Output Margin

Output Margin

CIFAR100 ResNet18 Test
= -0.76 101
101

100 0 2 4 6 8 10 100 Prediction Depth

102CIFAR100

VGG16 Test
= -0.81

101

101

0 2 4 6 8 10 12 14 100 Prediction Depth

CIFAR100 MLP Test
= -0.74

101

101

100 0 2 4 6 8 100 Prediction Depth

Input Margin

Input Margin

Input Margin

CIFAR100 ResNet18 Test

100

= -0.70

10 1

101

0 2 4 6 8 10 100 Prediction Depth

CIFAR100 VGG16 Test

100

= -0.79

101

10 1

0 2 4 6 8 10 12 14 100 Prediction Depth
CIFAR100 MLP Test
= -0.77 100
101

10 1 0 2 4 6 8 100 Prediction Depth

Output Margin

Output Margin

Output Margin

2 × 1C0I1FAR100 ResN=et-01.848Train103
102

101

101

0 2 4 6 8 10 100 Prediction Depth

10C2 IFAR100

VGG16 Train
= -0.61

102

101 101
0 2 4 6 8 10 12 14 100 Prediction Depth
CIFAR100 MLP Train
= -0.47 102

101

101

0 2 4 6 8 100 Prediction Depth

Input Margin

Input Margin

Input Margin

CIFAR100 ResNet18 Train

100

= -0.26

102

101

10 1 0

2

4

6

8 10 100

Prediction Depth

CIFAR100 VGG16 Train
100 = -0.51 102

101

0 2 4 6 8 10 12 14 100 Prediction Depth
CIFAR100 MLP Train
= -0.45 102 100
101
0 2 4 6 8 100 Prediction Depth

Figure C.31: Consistency of Figure 6, showing the correlation between prediction depth, and the input and output margins (log scale) for both the test and training splits of CIFAR100. The correlation coeﬃcient between the prediction depth and the logarithm of the margin is given in each plot. For each architecture, we train 25 models with diﬀerent random seeds on the full training split. We record the input and output margins together with the prediction depth for every data point in both the train and test splits. These histograms compare the mean values of each margin to the mean prediction depth for all data points.

Output Margin

38

PD: Train

PD: Train

ConsCeIFnAsuRs10=RGersoNuentd18Truth

10

8

100

6 4

10 1

2

10 2

00 2 4 6 8 10

PD: Validation

ConseCnIFsuAsR1=0GVrGouGn1d6 Truth

1124

100

180

10 1

46

10 2

020 2 4 6 8 10 12 14

PD: Validation

ConsenCsIuFsA=R1G0rMouLnPd Truth

78

101

56

34 2

10 1

010 1 2 3 4 5 6 7 8

PD: Validation

PD: Train

PD: Train

PD: Train

ConsCeInFsAuRs10 RGersoNuentd18Truth

10

8

10 1

6

4

2

00 2 4 6 8 10 10 2

PD: Validation

ConseCnsIFuAsR10GVrGoGun1d6 Truth

1124

10 1

180

46

020 2 4 6 8 10 12 14 10 2

PD: Validation

ConsensCuIFsAR1G0rMouLnPd Truth

78

56

10 1

34

12

10 2

00 1 2 3 4 5 6 7 8

PD: Validation

Figure C.32: Demonstrating consistency of the histograms shown in Figure 7 for all architectures on CIFAR10. These histograms compare the mean prediction depth when each data point occurs in either the validation split or the training split. Results are shown separately for data points where the consensus class is the same as or diﬀerent from the ground truth label. See Appendix C.5 for a description.

PD: Train

39

PD: Train

PD: Train

ConCsIeFnAsRu1s0=0 GRerosuNnedt1T8ruth

10 8

100

6

10 1

4

2

10 2

00 2 4 6 8 10

PD: Validation

ConsCenIFsAuRs1=00GVroGuGn1d6Truth

1124

180

10 1

46

10 2

020 2 4 6 8 10 12 14

PD: Validation

ConsenCsIFuAsR=10G0roMuLnPd Truth

78

56

100

34

10 1

12 0

10 2

0 1 P2D:3Va4lida5tio6n 7 8

PD: Train

PD: Train

PD: Train

ConsCeIFnAsuRs100 RGerosuNnedt1T8ruth

10 8

100

6 4

10 1

2

10 2

00 2 4 6 8 10

PD: Validation

ConseCnIFsuAsR100GVroGuGn1d6Truth

1124

10 1

180

46

10 2

020 2 4 6 8 10 12 14

PD: Validation

ConsenCsIuFsAR10G0roMuLnPd Truth

78 6

10 1

45

23

10 2

010 1 2 3 4 5 6 7 8

PD: Validation

Figure C.33: Demonstrating consistency of the histograms shown in Figure 7 for all architectures on CIFAR100. These histograms compare the mean prediction depth when each data point occurs in either the validation split or the training split. Results are shown separately for data points where the consensus class is the same as or diﬀerent from the ground truth label. See Appendix C.5 for a description.

PD: Train

40

PD: Train

PD: Train

CFoanssheinosnuMs N=ISGTroRuensdNeTtr1u8th

10 8

101

6

4

10 1

2

00 2 4 6 8 10

PD: Validation

CoFnassehnisounsM=NGISrTouVnGdGT1r6uth

1124

180

100

46

10 2

020 2 4 6 8 10 12 14

PD: Validation

ConFseanshsuiosn=MGNrISoTunMdLTPruth

78 6

101

45

23

10 1

010 1 2 3 4 5 6 7 8

PD: Validation

PD: Train

PD: Train

PD: Train

CoFnassehniosnusMNISGTroRuensNdeTtr1u8th

10

8

100

6

4 2

10 1

00 2 4 6 8 10

PD: Validation

ConFsaesnhsiouns MNGISrTouVnGdGT1r6uth

1124

180

10 1

46

020 2 4 6 8 10 12 14

PD: Validation

ConsFeansshuiosn MGNIrSoTunMdLTPruth

78 6

100

45

23

10 1

010 1 2 3 4 5 6 7 8

PD: Validation

Figure C.34: Demonstrating consistency of the histograms shown in Figure 7 for all architectures on Fashion MNIST. These histograms compare the mean prediction depth when each data point occurs in either the validation split or the training split. Results are shown separately for data points where the consensus class is the same as or diﬀerent from the ground truth label. See Appendix C.5 for a description.

PD: Train

41

PD: Train

PD: Train

ConseSnVsHuNs =ReGsrNoeutn1d8 Truth

10 8

101

6

4

10 1

2

00 2 4 6 8 10

PD: Validation

ConsenSsVuHsN=VGGrGou1n6d Truth

1124

180

100

46

10 2

020 2 4 6 8 10 12 14

PD: Validation

ConsensSuVsH=NGMroLuPnd Truth

78 6

101

45

23

10 1

010 1 2 3 4 5 6 7 8

PD: Validation

PD: Train

PD: Train

PD: Train

ConseSnVsuHsN ReGsNroeutn1d8 Truth

10

8

6

4

10 1

2

00 2 4 6 8 10

PD: Validation

ConsenSsuVsHN VGGrGo1un6d Truth

1124

180

10 1

46

020 2 4 6 8 10 12 14

PD: Validation

ConsensuSsVHNGMrLoPund Truth

78

56 4

10 1

23

010 1 2 3 4 5 6 7 8

PD: Validation

Figure C.35: Demonstrating consistency of the histograms shown in Figure 7 for all architectures on SVHN. These histograms compare the mean prediction depth when each data point occurs in either the validation split or the training split. Results are shown separately for data points where the consensus class is the same as or diﬀerent from the ground truth label. See Appendix C.5 for a description.

PD: Train

42

k-NN: Ground Truth

k-NN: Ground Truth

1.0 0.8 0.6 0.4 0.2 0.0
0 1 2 3 4 5 6 7 8 9 10 Layer

k-NN: Consensus Class

1.0 0.8 0.6 0.4 0.2 0.0 0 1 2 3 4 5 6 7 8 9 10
Layer

Easy Ambiguous Ambiguous w/o its label Looks like a different class

Figure C.36: Reproducing Figure 8 for ResNet18 on CIFAR10.

1.0 0.8 0.6 0.4 0.2 0.0 0 2 4 6 8 10 12 14
Layer

k-NN: Consensus Class

1.0 0.8 0.6 0.4 0.2 0.0 0 2 4 6 8 10 12 14
Layer

Easy Ambiguous Ambiguous w/o its label Looks like a different class

Figure C.37: Reproducing Figure 8 for VGG16 on CIFAR10.

1.0 0.8 0.6 0.4 0.2 0.0 0 1 2 3 4 5 6 7 8
Layer

k-NN: Consensus Class

1.0 0.8 0.6 0.4 0.2 0.0 0 1 2 3 4 5 6 7 8
Layer

Easy Ambiguous Ambiguous w/o its label Looks like a different class

Figure C.38: Reproducing Figure 8 for MLP on CIFAR10.

1.0 0.8 0.6 0.4 0.2 0.0 0 1 2 3 4 5 6 7 8 9 10
Layer

k-NN: Consensus Class

1.0 0.8 0.6 0.4 0.2 0.0 0 1 2 3 4 5 6 7 8 9 10
Layer

Easy Ambiguous Ambiguous w/o its label Looks like a different class

Figure C.39: Reproducing Figure 8 for ResNet18 on CIFAR100.

k-NN: Ground Truth

k-NN: Ground Truth

43

k-NN: Ground Truth

k-NN: Ground Truth

1.0 0.8 0.6 0.4 0.2 0.0 0 2 4 6 8 10 12 14
Layer

k-NN: Consensus Class

1.0 0.8 0.6 0.4 0.2 0.0 0 2 4 6 8 10 12 14
Layer

Easy Ambiguous Ambiguous w/o its label Looks like a different class

Figure C.40: Reproducing Figure 8 for VGG16 on CIFAR100.

1.0 0.8 0.6 0.4 0.2 0.0 0 1 2 3 4 5 6 7 8
Layer

k-NN: Consensus Class

1.0 0.8 0.6 0.4 0.2 0.0 0 1 2 3 4 5 6 7 8
Layer

Easy Ambiguous Ambiguous w/o its label Looks like a different class

Figure C.41: Reproducing Figure 8 for MLP on CIFAR100.

1.0 0.8 0.6 0.4 0.2 0.0 0 1 2 3 4 5 6 7 8 9 10
Layer

k-NN: Consensus Class

1.0 0.8 0.6 0.4 0.2 0.0 0 1 2 3 4 5 6 7 8 9 10
Layer

Easy Ambiguous Ambiguous w/o its label Looks like a different class

Figure C.42: Reproducing Figure 8 for ResNet18 on SVHN.

1.0 0.8 0.6 0.4 0.2 0.0 0 2 4 6 8 10 12 14
Layer

k-NN: Consensus Class

1.0 0.8 0.6 0.4 0.2 0.0 0 2 4 6 8 10 12 14
Layer

Easy Ambiguous Ambiguous w/o its label Looks like a different class

Figure C.43: Reproducing Figure 8 for VGG16 on SVHN.

k-NN: Ground Truth

k-NN: Ground Truth

44

k-NN: Ground Truth

k-NN: Ground Truth

1.0 0.8 0.6 0.4 0.2 0.0 0 1 2 3 4 5 6 7 8
Layer

k-NN: Consensus Class

1.0 0.8 0.6 0.4 0.2 0.0 0 1 2 3 4 5 6 7 8
Layer

Easy Ambiguous Ambiguous w/o its label Looks like a different class

Figure C.44: Reproducing Figure 8 for MLP on SVHN.

1.0 0.8 0.6 0.4 0.2 0.0 0 1 2 3 4 5 6 7 8 9 10
Layer

k-NN: Consensus Class

1.0 0.8 0.6 0.4 0.2 0.0 0 1 2 3 4 5 6 7 8 9 10
Layer

Easy Ambiguous Ambiguous w/o its label Looks like a different class

Figure C.45: Reproducing Figure 8 for ResNet18 on Fashion MNIST.

1.0 0.8 0.6 0.4 0.2 0.0 0 2 4 6 8 10 12 14
Layer

k-NN: Consensus Class

1.0 0.8 0.6 0.4 0.2 0.0 0 2 4 6 8 10 12 14
Layer

Easy Ambiguous Ambiguous w/o its label Looks like a different class

Figure C.46: Reproducing Figure 8 for VGG16 on Fashion MNIST.

1.0 0.8 0.6 0.4 0.2 0.0 0 1 2 3 4 5 6 7 8
Layer

k-NN: Consensus Class

1.0 0.8 0.6 0.4 0.2 0.0 0 1 2 3 4 5 6 7 8
Layer

Easy Ambiguous Ambiguous w/o its label Looks like a different class

Figure C.47: Reproducing Figure 8 for MLP on Fashion MNIST.

k-NN: Ground Truth

k-NN: Ground Truth

45

Accuracy

1.0

0.8

0.6

0.4

LR: Training Set (full) LR: Training Set (half)

0.2

kNN Probes: Training Set

0 1 2 3 4 5 6La7yer8 9 1011121314

Figure E.48: Comparison of k-NN probe and Logistic Regression (LR) probe accuracies for VGG16 trained on CIFAR10. LR is already able to divide the training set into linearly separated classes after the ﬁrst convolutional operation. In red we show the accuracy of LR probes trained on a random subset (half) of the data and predicting on the other half. These results are converged (closely repeatable between diﬀerent trained VGG16 models).

46

