An Identiﬁable Double VAE For Disentangled Representations

arXiv:2010.09360v2 [cs.LG] 10 Feb 2021

Graziano Mita 1 2 Maurizio Filippone 1 Pietro Michiardi 1

Abstract
A large part of the literature on learning disentangled representations focuses on variational autoencoders (VAE). Recent developments demonstrate that disentanglement cannot be obtained in a fully unsupervised setting without inductive biases on models and data. However, Khemakhem et al., AISTATS, 2020 suggest that employing a particular form of factorized prior, conditionally dependent on auxiliary variables complementing input observations, can be one such bias, resulting in an identiﬁable model with guarantees on disentanglement. Working along this line, we propose a novel VAE-based generative model with theoretical guarantees on identiﬁability. We obtain our conditional prior over the latents by learning an optimal representation, which imposes an additional strength on their regularization. We also extend our method to semi-supervised settings. Experimental results indicate superior performance with respect to state-of-the-art approaches, according to several established metrics proposed in the literature on disentanglement.

1. Introduction

Representation learning aims at learning data representations such that it is easier to extract useful information when building classiﬁers or other predictive tasks (Bengio et al., 2013). Representation learning seeks to obtain the following properties: i) expressiveness: a reasonably-sized representation should allow to distinguish among a high number of different input conﬁgurations; ii) abstractness: learned representations should capture high-level features; iii) invariance: representation should be invariant to local changes of input conﬁgurations; iv) interpretability: learned representations should allow each dimension to be informative about the given task. These properties are at the core of disentangled

1EURECOM, 06410 Biot (France) 2SAP Labs France,

06250 Mougins (France). Correspondence to: Graziano

Mita <graziano.mita@eurecom.fr>, Maurizio Filippone

<maurizio.ﬁlippone@eurecom.fr>,

Pietro Michiardi

<pietro.michiardi@eurecom.fr>.

z1
z2
Figure 1. Toy example where each zi controls a given ground-truth factor: z1 the size, z2 the shape of the 2D objects.
representations. In disentangled representation learning, the main assumption is that high-dimensional observations x are the result of a (possibly nonlinear) transformation applied to a low dimensional latent variable of independent generative factors, called ground-truth factors, capturing semantically meaningful concepts. Input observations can be thought of as the result of a probabilistic generative process, where latent variables z are ﬁrst sampled from a prior distribution p(z), and then the observations x are sampled from p(x|z). The goal is to learn a representation of the data that captures the generative factors. In simple terms, illustrated in ﬁg. 1, each dimension of a disentangled representation refers to a single factor of variation.
In this work, we focus on deep generative models, and in particular those based on variational autoencoders (VAE), to learn disentangled representations. A well known theoretical result asserts that disentanglement is essentially impossible in a fully unsupervised setting, without inductive biases on models and data (Locatello et al., 2019). However, inducing a disentangled structure into the latent space where z lies is feasible by incorporating auxiliary information u about the ground-truth factors in the model. The type and amount of supervision deﬁne different families of disentanglement methods, often classiﬁed as supervised, semi-supervised, and weakly-supervised. In most of these methods, the auxiliary variables u become an integral part of the latent space. However, recent work (Khemakhem et al., 2020) indicates that there are alternative strategies to beneﬁt from auxiliary information, such as using it to impose a structure on the latent space. In their proposal, this is done by learning a prior distribution on the latent space, where the crucial aspect is that this is conditioned on auxiliary information u that is coupled with every input observations. Under mild assumptions, it is possible to show

An Identiﬁable Double VAE For Disentangled Representations

that such form of conditioning implies model identiﬁability, allowing one to recover the original ground-truth factors and therefore providing principled disentanglement.
In this work, we propose a novel generative model that, like Khemakhem et al. (2020), uses a conditional prior and has theoretical identiﬁability guarantees. We show that our method naturally imposes an optimality constraint, in information theoretic terms, on the conditional prior: this improves the regularization on the function that maps input observations to latent variables, which translates in tangible improvements of disentanglement in practice. Since assuming to have access to auxiliary variables for each input observations, both at training and testing time, is not practical in many applications, we also propose a semi-supervised variant of our method.
Our Contributions: i) We present a detailed overview of VAE-based disentanglement methods using a uniﬁed notation. Our focus is on the role of the regularization term. We introduce a distinction between direct matching approaches, in which ground-truth factors are directly matched to the latent space, and indirect matching approaches, where a prior distribution over the latents is used to structure the learned latent space. ii) We design a new method, that we call Identiﬁable Double VAE (IDVAE) since its ELBO can be seen as a combination of two variational autoencoders, that is identiﬁabile, in theory, and that learns an optimal conditional prior, which is truly desirable in practice. We additionally propose a semi-supervised version of IDVAE to make our method applicable also when auxiliary information is available for a subset of the input observations only. iii) We design an experimental protocol that uses four well-known datasets, and established disentanglement metrics. We compare our method to several state-of-the-art competitors and demonstrate that IDVAE achieves superior disentanglement performance across most experiments.

ated by pθ∗ (x, z) = pθ∗ (x|z)pθ∗ (z), where θ∗ are the true,

but unknown parameters. Then, the goal is to learn θ ∈ Θ

such that:

pθ(x, z) = pθ∗ (x, z).

(2)

When eq. (2) holds, it is then possible to recover the generative ground-truth factors. Unfortunately, by observing x alone, we can estimate the marginal density pθ(x) ≈ pθ∗ (x), but there are no guarantees about learning the true generative model pθ∗ (x, z). This is only feasible for models satisfying the following implication:

∀(θ, θ ) : pθ(x) = pθ (x) =⇒ θ = θ .

(3)

When eq. (3) holds, the estimated and the true marginal distribution match, and their parameters match too. Then, the model is identiﬁable (Khemakhem et al., 2020) and, as a consequence, it allows one to recover the latent ground-truth factors and obtain a disentangled representation:
pθ(x) = pθ (x) =⇒ pθ(x, z) = pθ (x, z). (4)

A practical goal is to aim for model identiﬁability up to trivial transformations, such as permutation and scaling; as long as ground-truth factors can be identiﬁed, their order and scale is irrelevant.

3. Related work
Today, a large body of work to learn disentangled representations is based on generative models. In this work, we focus on VAE-based approaches (Kingma & Welling, 2014; Rezende et al., 2014).
Variational Autoencoder. A standard VAE learns the parameters of eq. (1) by introducing an inference model qφ(z|x) to derive an ELBO as follows:

2. Preliminaries
Let x ∈ Rn be some input observations, which are the result of a transformation of independent latent ground-truth factors z ∈ Rd through a function f : Rd → Rn. Then, we have that x = f (z) + , where is a Gaussian noise term: ∼ N (0, Σ), and independent of z. Let consider the following generative model:

pθ(x, z) = pθ(x|z)pθ(z),

(1)

where θ ∈ Θ is a vector of model parameters, pθ(z) =

d i=1

pθ

(zi)

represents

the

factorized

prior

probability

dis-

tribution over the latents and pθ(x|z) is the conditional

distribution to recover x from z. The decoder function f (z)

determines the way z is transformed into x within pθ(x|z).

Assume to observe some data D = {x(1), · · · , x(N)} gener-

LVAE = Eqφ(z|x)[log pθ(x|z)] − KL(qφ(z|x)||p(z)), (5)
where, by abuse of notation, we write x in place of x(i). This avoids clutter in the presentation of VAE-based models, but, clearly, the marginal log-likelihood is composed of a sum of such ELBO terms, one for each observation x(i) (Kingma & Welling, 2014).
The distribution pθ(x|z) has the role of a decoder, whereas qφ(z|x) can be seen as an encoder, and it is generally assumed to be a factorized Gaussian with a diagonal covariance matrix. Both distributions are parameterized with neural networks, with parameters θ and variational parameters φ. The prior p(z) is generally a factorized, isotropic unit Gaussian.
The ﬁrst term of eq. (5) relates to the reconstruction of the input data using latent variables sampled from the variational approximation of the true posterior. The second

An Identiﬁable Double VAE For Disentangled Representations

term is a regularization term, which pushes the approximate posterior qφ(z|x) to match the prior on the latent space. Maximizing eq. (5) across observations implies learning the parameters such that the reconstruction performance is high, and the regularization term is small.

Since both terms that appear in the regularization of eq. (5) are factorized Gaussians with diagonal covariance, one way to interpret the individual components zi of the latent space is to view them as independent white noise Gaussian channels (Burgess et al., 2017). When the KL term is zero, the latent channels zi have zero capacity: this happens when the approximate posterior qφ(z|x) matches exactly the prior pθ(z). In this case, however, the reconstruction term is penalized. To increase KL(·) > 0, it is necessary to decrease the overlap between channels, and reduce their variances.

Unsupervised disentanglement learning. The above understanding of the regularization term is at the basis of many variants of the original VAE model, that strive to increase the pressure on the regularization term, or elements thereof, to achieve disentanglement, without sacriﬁcing reconstruction properties too much. For example Higgins et al. (2017) propose β-VAE, which modiﬁes eq. (5) by introducing a hyper-parameter β to gauge the pressure on the regularization term throughout the learning process:

Lβ−VAE = Eqφ(z|x)[log pθ(x|z)] − βKL(qφ(z|x)||p(z)). (6)
When β > 1, the encoder distribution qφ(z|x) is pushed towards the unit Gaussian prior p(z). In light of the discussion above, the strong penalization of the KL term in β-VAE affects the latent channel distribution, by reducing the spread of their means, and increasing their variances.

Many methods build on β-VAE (Burgess et al., 2017; Kim & Mnih, 2018; Kumar et al., 2018; Chen et al., 2018; Zhao et al., 2019), rewriting the ELBO in slightly different ways. A generalization of the KL term decomposition proposed by Hoffman & Johnson (2016); Makhzani & Frey (2017) is the following (Chen et al., 2018):

Ex[KL(qφ(z|x)||p(z))] =

I(x; z) + KL(q(z)|| q(zj))) + KL(q(zj)||p(zj))

j

j

where q(z) is the aggregated posterior and I(x; z) is the mutual information between x and z. Penalizing I(x; z) can be harmful to reconstruction purposes, but enforcing a factorized aggregated posterior encourages independence across the dimensions of z, favouring disentanglement. The dimensional independence in the latent space is encouraged by the second term, known as total correlation (TC). The third term is a further regularization, preventing the aggregate posterior to deviate too much from the factorized prior.

Note that unsupervised VAE-based approaches approxi-

mate the data marginal distribution pθ(x), but there are no guarantees to recover the true joint probability distribution pθ(x, z), having acces to the input observations x only (Khemakhem et al., 2020). Pushing the model to learn a representation with statistically independent dimensions is not a sufﬁcient condition to obtain full disentanglement. These considerations were recently formalized in the impossibility result (Locatello et al., 2019), but they were already known in the nonlinear ICA literature (Comon, 1994; Hyva¨rinen & Pajunen, 1999).
Auxiliary variables and disentanglement. To overcome the above limitations, a key idea is to incorporate an inductive bias in the model. The choice of the variational family and prior distribution can be one of such bias (Mathieu et al., 2019; Kumar & Poole, 2020). Alternatively, it is possible to rely on additional information about the ground-truth factors, which we indicate as u ∈ Rm. When auxiliary observed variables u are available, they can be used jointly with z to reconstruct the original input x. These methods are usually classiﬁed under the semi/weakly supervised family. More speciﬁcally Shu et al. (2020) identify three forms of weak supervision: restricted labeling (Kingma et al., 2014; Cheung et al., 2015; Siddharth et al., 2017; Klys et al., 2018), match/group pairing (Bouchacourt et al., 2018; Hosoya, 2019; Locatello et al., 2020a), and rank pairing (Chen & Batmanghelich, 2020a;b). In the extreme case, when all ground-truth factors are known for all the input samples, we label them as supervised disentanglement methods.

As for unsupervised counterpart, methods relying on auxiliary observed variables u differ in how the regularization term(s) are designed. Some approaches use a “supervised” regularization term to directly match z and the available ground-truth factors u: we refer to this form of regularization as direct matching. An example is what we here call FULLVAE method (Locatello et al., 2020b), which optimizes the following ELBO:

LFULLVAE = Lβ-VAE − γRs(qφ(z|x), u),

(7)

where Rs(·) is a loss function between the latent and the ground-truth factors (in the original implementation it is a binary cross entropy loss). Other approaches employ a KL divergence term between the posterior and the prior over the latents: we refer to this form of regularization as indirect matching. In other words, direct matching methods require explicit knowledge of one or more ground-truth factors, whereas indirect matching can also use weak information about them. Shu et al. (2020) demonstrated that indirect matching methods can enforce some properties in the latent space, leading to what they deﬁne as consistency and restrictiveness. To obtain full disentanglement, a method must satisfy both properties on all the latent dimensions. A recent work by Khemakhem et al. (2020) establishes a theoretical framework to obtain model identiﬁability, which is related

An Identiﬁable Double VAE For Disentangled Representations

to disentanglement. They propose a new generative model called IVAE, that learns a disentangled representation using a factorized prior from the exponential family, crucially conditioned on u. In practical applications, the conditional prior is chosen to be a Gaussian location-scale family, where the mean and variance of each latent dimension zi are expressed as a function of u. Then, it is possible to derive the following ELBO for the IVAE model:

LIVAE = Eqφ(z|x,u)[log pθ(x|z)]

− βKL(qφ(z|x, u)||pθ(z|u)).

(8)

In eq. (8), we recognize the usual structure of a reconstruction, and a regularization term. A remarkable advancement of the IVAE model relates to identiﬁability properties: next, we present a new approach to learn an identiﬁable model that leads to disentangled representations, by using an optimal factorized prior, conditionally dependent on auxiliary observed variables. We also extend our method to deal with more realistic semi-supervised settings.

4. IDVAE: Identiﬁable Double VAE
Let x ∈ Rn, and u ∈ Rm be two observed random variables, and z ∈ Rd a low-dimensional latent variable, with d ≤ n. Then, consider the following generative models:

pθ(x, z|u) = pf (x|z)pT,η(z|u),

(9)

pf (x|z) = p (x − f (z)),

(10)

pT,η(z|u) = hi(zi)gi(u) exp Ti(zi) ηi(u) , (11)
i

and

pϑ(z, u) = pϑ(u|z)p(z),

(12)

where θ = (f , T, η) and ϑ are model parameters. Equation (9) corresponds to the process of generating x given the latents z. Equation (10) implies that x = f (z) + , with ∼ N (0, Σ). We approximate the injective function f with a neural network. Equation (11) is an exponential conditionally factorial distribution (Bishop, 2006), where hi is the base measure, gi(u) is the normalizing constant, Ti = [Ti,1, · · · , Ti,k] are the sufﬁcient statistics, and ηi(u) = [ηi,1, · · · , ηi,k] are the corresponding parameters. The dimension of each sufﬁcient statistic k is ﬁxed. Equation (12) formalizes the additional process to obtain u given z through pϑ(u|z), where p(z) is a prior over the latents, usually a factorized, isotropic unit Gaussian.

Given a dataset D = {(x(1), u(1)), · · · , (x(N), u(N))} of observations generated according to eqs. (9) to (12), we are interested in ﬁnding a variational bound L for the marginal data log-likelihood p(x, u), which we derive as follows:

log p(x, u) = KL(qφ(z|x, u)||pθ(z|x, u)) + L(θ, φ),

where, by abuse of notation, we write x and u in place of x(i) and u(i), which we do hereafter as well.
Since the KL term is non-negative, we have the following variational lower bound: log p(x, u) ≥ L(θ, φ). Now, we can write the ELBO, which resembles that of eq. (8), but includes an additional term:

L(θ, φ) = Eqφ(z|x,u)[log pf (x|z)]
− βKL(qφ(z|x, u)||pT,η(z|u)) + log p(u), (13)

where we introduce the parameter β to gauge the pressure on the KL term. Next, focusing on the generative model in eq. (12), we derive the following variational lower bound for log p(u) in eq. (13), log p(u) ≥ Lprior(ϑ, ψ):

Lprior(ϑ, ψ) = Eqψ(z|u)[log pϑ(u|z)]

− KL(qψ(z|u)||p(z)),

(14)

Combining eq. (13) and eq. (14), we obtain:
LIDVAE(θ, φ, ϑ, ψ) ≥ ≥ Eqφ(z|x,u)[log pf (x|z)] − βKL(qφ(z|x, u)||pT,η(z|u))
1
+ Eqψ(z|u)[log pϑ(u|z)] − KL(qψ(z|u)||p(z)) . (15)
2
We call our method IDVAE, Identiﬁable Double VAE, because it can be seen as the combination of two variational autoencoders 1 and 2 , with independent parameters. In principle, when we optimize the ELBO by summing across all datapoints, e.g. using a doubly stochastic approach (Titsias & La´zaro-Gredilla, 2014) and automatic differentiation, we could treat the two parts separately. However, nothing would prevent the conditional prior pT,η(z|u) and its variational approximation qψ(z|u) to converge to different distributions. Thus, we further make the modeling assumption of constraining the conditional prior in 1 to be exactly the variational appoximation learned in 2 , which belongs to the exponential family.

4.1. Identiﬁability properties
Next, we set up notations and deﬁnitions for a general theory of identiﬁability of generative models (Khemakhem et al., 2020), and show that IDVAE, under mild conditions, is identiﬁable.
Notation. Concerning the exponential conditionally factorial distribution in eq. (11), we denote by T(z) the vector of concatenated sufﬁcient statistics deﬁned as follows: T(z) = [T1(z1) , · · · , Td(zd) ] ∈ Rdk. We denote

An Identiﬁable Double VAE For Disentangled Representations

by η(u) the vector of its parameters deﬁned as follows: η(u) = [η1(u) , · · · , ηd(u) ] ∈ Rdk. Deﬁnition 1. Let ∼ be an equivalence relation on the parameter space Θ. We say that eq. (1) is ∼-identiﬁable if pθ(x) = pθ∗ (x) =⇒ θ ∼ θ∗. Deﬁnition 2. Let ∼ be the equivalence relation on Θ deﬁned as follows: (f , T, η) ∼ (f , T , η ) ⇔ ∃A, c : T(f −1(x)) = AT (f −1(x)) + c, ∀x ∈ X , where A is a dk × dk matrix and c is a vector of dimension dk. If A is invertible, we denote this relation by ∼A.
Deﬁnition 2 establishes a speciﬁc equivalence relation that allows to recover the sufﬁcient statistics of our model up to a linear matrix multiplication. Theorem 1. (Khemakhem et al., 2020) Assume we observe data sampled from pθ(x, z|u) = pf (x|z)pT,η(z|u), where pf (x|z) as in eq. (10) and pT,η(z|u) as in eq. (11), with parameters θ = (f , T, η). Assume the following holds:
i The set {x ∈ X : φ (x) = 0} has measure zero, where φ is the characteristic function of the density p deﬁned in pf (x|z) = p (x − f (z)).
ii The function f is injective.
iii The sufﬁcient statistics Ti,j in eq. (11) are differentiable almost everywhere, and linearly independent on any subset of X of measure greater than zero.
iv Being k the dimensionality of the sufﬁcient statistics Ti in eq. (11) and d the dimensionality of z, there exist dk + 1 distinct point u0, ..., udk such that the dk × dk matrix E deﬁned as follows is invertible:
E = (η(u1) − η(u0); · · · ; η(udk) − η(u0)) (16)
Then the parameters θ = (f , T, η) are ∼A-identiﬁable.
Theorem 1 (sketch of the proof in Appendix C) guarantees a general form of identiﬁability for IDVAE. Under more restrictive conditions on f and T, following the same reasoning of Khemakhem et al. (2020), it is also possible to reduce A to a permutation matrix.
Note that, in practice, all the VAE-based methods we discuss in this work are approximate. When using a simple, synthetic dataset, where the generative process is controlled, full disentanglement can be veriﬁed experimentally (Khemakhem et al., 2020). However, in a realistic setting, the modeling choice for both qφ(z|x, u) and qψ(z|u) can have an impact on disentanglement. Even when recognition models have enough capacity to ﬁt the data (in our experiments they are Gaussian with diagonal covariance), theoretical guarantees might still fall short, despite the availability of auxiliary variables for all input observations. This could be due to, for example, suboptimal solutions found by the optimization algorithm or to the ﬁnite data regime.

4.2. Learning an optimal conditional prior

In this paper, we advocate for a particular form of a conditional prior, that is the result of learning an optimal representation z, of auxiliary, observed variables u.

In general, an optimal representation, for a generic task y (in our case, we aim at reconstructing u) is deﬁned in terms of sufﬁciency and minimality: z is sufﬁcient for the task y if I(u; y) = I(z; y), where I(·; ·) is the mutual information; z is minimal if it compresses the input such that it discards all variability that is not relevant for the task (Achille & Soatto, 2016). As shown in (Tishby et al., 1999), the so called Information Bottleneck (IB) can be used to learn an optimal representation z for the task y, which amounts to optimizing the following Lagrangian:

LIB = H(y|u) + βI(u; z),

(17)

where we denote the entropy by H(·), with the constant β controlling the trade-off between sufﬁciency and minimality. It is easy to show that eq. (17) and eq. (14) are equivalent (with β = 1) when the task is reconstruction.

In our method, we learn the conditional prior qψ(z|u) in
part 2 of eq. (15), and use it in part 1 by setting pT,η(z|u) = qψ(z|u). In light of above discussion, this is equivalent to imposing an additional constraint that pushes the conditional prior to learn an optimal representation
z from u; the KL term of part 1 in eq. (15) pushes qφ(z|x, u) toward the optimal conditional prior, which results in superior regularization quality.

Note that Theorem 1 requires auxiliary variables u to be expressive enough to recover all the independent factors through the parameters η(u). In information theoretic terms, u must be sufﬁcient to recover the ground-truth factors, but there is no explicit need for the extra optimality constraint on pT,η(z|u). While Theorem 1 remains valid for an optimal conditional prior, we demonstrate experimentally that, when variational approximations, sub-optimal solutions, or ﬁnite data size spoil theoretical results, learning an optimal conditional prior is truly desirable.

4.3. A semi-supervised variant of IDVAE

So far, we worked under the assumption that the auxiliary information u is consistently available for every x. In real scenarios, it is more likely to observe u for a subset of the input observations. Thus, we propose a variation of IDVAE for a semi-supervised setting. We consider a new objective function that consists of two terms (Kingma et al., 2014):

LSS-IDVAE =

Ll(x, u) + Lu(x),

(x,u)∼pl

x∼pu

(18)

Ll(x, u) = LIDVAE(x, u),

(19)

Lu(x) = Eqζ(u|x)[Ll(x, u)] + H(qζ(u|x)), (20)

An Identiﬁable Double VAE For Disentangled Representations

where Ll and Lu are the labeled and unlabeled terms respectively; qζ(u|x) in eq. (20) is used to derive u from x when u is not provided as input. To be precise, we should add to eq. (18) a third term – E(x,u)∼pl [log qζ(u|x)] – such that it can learn also from labeled data. Clearly, this method also applies to the work from Khemakhem et al. (2020).
5. Experiments
5.1. Experimental settings
Methods. We compare IDVAE against three disentanglement methods: β-VAE, FULLVAE, IVAE. β-VAE (Higgins et al., 2017) is a baseline for indirect matching methods where no ground-truth factor is known at training time and the only way to enforce a disentangled representation is by increasing the strength of the regularization term through the hyper-parameter β. FULLVAE (Locatello et al., 2020b) is the representative of direct matching methods: it can be considered as a standard β-VAE with an additional regularization term, weighted by an hyper-parameter γ, to match the latent space to the target ground-truth factors. As done in the original implementation, we use a binary cross entropy loss for FULLVAE, where the targets are normalized in [0, 1]; we also set β = 1, to measure the impact of the supervised loss term only. IVAE (Khemakhem et al., 2020) is another indirect matching method where the regularization term, weighted again by β, involves a conditional prior. We additionally report the results for the semi-supervised versions of FULLVAE, IVAE, and IDVAE, which we denote as SS-FULLVAE, SS-IVAE1, SS-IDVAE, respectively. Variational approximations, and the conditional priors, are Gaussian distributions with diagonal covariance. All methods have been implemented in PyTorch (Paszke et al., 2019).
Datasets. We consider four common datasets in the disentanglement literature, where observations are images built as a deterministic function of known generative factors: DSPRITES (Higgins et al., 2017), SHAPES3D (Kim & Mnih, 2018), CARS3D (Reed et al., 2015) and SMALLNORB (LeCun et al., 2004). We have full control on the generative process and explicit access to the ground-truth factors. All ground-truth factors are normalized in the range [0, 1]; for discrete factors, we implicitly assume an ordering before applying normalization. All images are reshaped to a 64×64 size. A short description of the datasets is reported in table 1. Implementations of the generative process for each dataset are based on the code provided by Locatello et al. (2019).
Disentanglement metrics. In the literature, several metrics have been proposed to measure disentanglement, with known advantages and disadvantages, and ability to capture
1The original work (Khemakhem et al., 2020) is not semisupervised. We extended it for our comparative analysis.

Dataset
DSPRITES
CARS3D SHAPES3D
SMALLNORB

Size
737’280 17’568 480’000
24’300

Ground-truth factors (distinct values)
shape(3), scale(6), orientation(40), x(32), y(32) elevation(4), azimuth(24), object type (183) ﬂoor color(10), wall color(10), object color(8), object size(8), object type(4), azimuth(15) category(5), elevation(9), azimuth(18), light(6)

Table 1. Main characteristics of the datasets.

different aspects of disentanglement. We report the results for some of the most popular metrics: beta score (Higgins et al., 2017), MIG (Chen et al., 2018), SAP (Kumar et al., 2018), modularity and explicitness (Ridgeway & Mozer, 2018), all with values between 0 and 1. The implementation of the metrics is based on Locatello et al. (2019). We refer the reader to appendix E for further details.
Experimental protocol. In order to fairly evaluate the impact of the regularization terms, all the tested methods have the same convolutional architecture (widely adopted in most recent works), optimizer, hyper-parameters of the optimizer and batch size. The latent dimension z is ﬁxed to the true number of ground-truth factors. The conditional prior in IVAE is a MLP network; in IDVAE we use a simple MLPVAE. The same architecture is taken for the conditional prior of the semi-supervised counterparts. Moreover, qζ(u|x) is implemented by a convolutional neural network. We refer the reader to the Appendix D for more details.
We tried six different values of regularization strength associated to the target regularization term of each method – β for β-VAE, IVAE and IDVAE, and γ for FULLVAE: [1, 2, 4, 6, 8, 16]. These are recurring values in the disentanglement literature. For each model conﬁguration and dataset, we run the training procedure with 10 random seeds, given that all methods are susceptible to initialization values. After 300’000 training iterations, every model is evaluated according to the disentanglement metrics described above. For FULLVAE, IVAE and IDVAE, all ground-truth factors are provided as input, although IVAE and IDVAE work as well with a subset of them (or with any other additionally observed variable). We apply the same protocol for the semisupervised experiments too, where we provide, at training time, all the ground-truth factors for a subset of the input observations only, 1% and 10% respectively. At testing time, u is instead estimated from qζ(u|x).
5.2. Experimental results
Qualitative Evaluation. Latent traversal is a simple approach to visualize disentangled representations, by plotting the effects that each latent dimension of a randomly selected sample has on the reconstructed output. In ﬁg. 6a, we evaluate a conﬁguration (single seed) of our IDVAE model trained on DSPRITES (other datasets in Appendix F). Every row of the ﬁgure represents a latent dimension that we vary in the

An Identiﬁable Double VAE For Disentangled Representations

z1

z2

z3

z4

z5

-3

+3

Figure 2. Latent traversal of IDVAE model trained on DSPRITES.

range [−3, 3], while keeping the other dimensions ﬁxed. We can see that z1 has learned orientation reasonably well; z2 is responsible of the object scale; z4 and z5 reﬂect changes on the vertical and horizontal axis, respectively. z3 tried to learn, without success, shape changes. Next, we rely on disentanglement metrics to make a quantitative comparison among the tested methods.
Disentanglement Evaluation. In ﬁg. 3, we report, for each method and for each dataset, the ranges of the beta score and explicitness values with a box-plot. The variance of the box-plots is due to the random seeds and regularization strengths, which are the only parameters we vary. Furthermore, ﬁg. 3 includes the results for SS-IDVAE, SSIVAE and SS-FULLVAE (trained with 1% and 10% labeled samples), with different shades of green, blue, and red, respectively. The remaining evaluation metrics can be found in Appendix F, but they are essentially all correlated, as also noted in Locatello et al. (2019).
Overall, we observe, as expected, that β-VAE is often the worst method. Indeed, it has no access to any additional information at training time except the data itself. Despite this, β-VAE disentanglement performance is surprisingly not that far from FULLVAE that directly matches the latent space with the ground-truth factors. In some cases, β-VAE obtains very high beta scores (see outliers), such as for DSPRITES and CARS3D datasets, conﬁrming the sensitivity to random initialization of unsupervised methods (Locatello et al., 2019). Note also that FULLVAE exhibits inconsistent performance across the four datasets.
IDVAE emerges as the best method across several disentanglement metrics, except for SMALLNORB, where FULLVAE’s beta score is slightly better. For this speciﬁc dataset and metric, there are no considerable differences among methods, since most of the box-plots overlap. We note that IDVAE outperforms IVAE: considering that the two methods differ for the way the conditional prior is learned, our experiments show that an optimal conditional prior, as we propose in this work, offers substantial beneﬁts in terms of disentanglement and it is the only reason for IDVAE superiority. Finally, although both IVAE and IDVAE have theoretical guarantees on

disentanglement and use the full set of ground-truth factors as input, they do not always obtain the maximum evaluation score, in practice. This is in line with the considerations in section 4.1.
The analysis above remains valid if we consider the semisupervised versions of the tested methods, too. We observe that, with the exception of SMALLNORB, SS-IDVAE’s disentanglement performance coherently increases when it observes more labeled instances. The same trend is generally followed by SS-IVAE. SS-FULLVAE, instead, seems to be less susceptible to the number of labeled instances. In general, even a small percentage of labeled instances (1%) is enough for SS-IDVAE to outperform β-VAE and to keep up with FULLVAE that is, however, a fully supervised method. This suggests that SS-IDVAE is a valid choice for applications where collecting additional information about the training data is difﬁcult or expensive.
Impact of the regularization strength. The disentanglement performance of each method might change drastically as a function of the regularization strength: some approaches might work signiﬁcantly better in some ranges and very badly in others. In ﬁg. 4, we plot, for each method and for each dataset, the median of the beta score and explicitness evaluation values as a function of the regularization strength. This is also useful to see if there are methods that consistently dominate others. In this case, we do not report the results for SS-IDVAE, SS-IVAE, and SS-FULLVAE to make the plots more easily readable. Additional disentanglement score results, including the semi-supervised versions, can be found in appendix F.
Across all the datasets, IDVAE achieves the best median scores for a wide range of regularization strengths. In DSPRITES, CARS3D, and SHAPES3D, IVAE dominates all the other methods (IDVAE is largely dominant also considering the remaining evaluations metrics). The performance of IVAE and FULLVAE can match that of IDVAE in some datasets, but the behavior is not consistent: if we focus on beta score, IVAE is the second best method in CARS3D and SHAPES3D, whereas in DSPRITES and SMALLNORB, performance drops when we increase the regularization strength – even β-VAE performs better; FULLVAE behaves well for DSPRITES and SMALLNORB, but it is on pair only with β-VAE in CARS3D and SHAPES3D.
By observing the evolution of the disentanglement scores, it appears that there is no clear strategy to choose the regularization strength. For IDVAE, in datasets such as DSPRITES and CARS3D, the regularization strength does not signiﬁcantly affect the beta score; in SHAPES3D and SMALLNORB, we note instead a decreasing monotonic trend. The situation is similar if we look at the explicitness, but it differs if we consider different disentanglement metrics. It is plausible

An Identiﬁable Double VAE For Disentangled Representations

Beta score

DSPRITES
0.8 0.6
1 2 3 4 5 6 7 8 9 10

CARS3D
1.0

SHAPES3D

0.75 0.8

0.50

0.6

0.25

1 2 3 4 5 6 7 8 9 10

1 2 3 4 5 6 7 8 9 10

SMALLNORB 1.0
0.8
0.6
1 2 3 4 5 6 7 8 9 10

0.85
0.80
0.75 1 2 3 4 5 6 7 8 9 10 Model

0.8 0.7
1 2 3 4 5 6 7 8 9 10 Model

0.8
0.6 1 2 3 4 5 6 7 8 9 10 Model

0.8
0.7
1 2 3 4 5 6 7 8 9 10 Model

Explicitness

Figure 3. Beta score and explicitness (the higher the better). 1=β-VAE, 2=SS-IDVAE (1%), 3=SS-IDVAE (10%), 4=IDVAE, 5=SS-IVAE (1%), 6=SS-IVAE (10%), 7=IVAE, 8=SS-FULLVAE (1%), 9=SS-FULLVAE (10%), 10=FULLVAE. Percentage of labeled samples in parenthesis.

Beta score

DSPRITES 0.8

CARS3D
0.9 0.8

SHAPES3D
0.8
0.6

SMALLNORB 0.8

0.7

0.4

0.6

0.6

5

10

15

5

10

15

5

10

15

5

10

15

0.85

0.80

0.8

0.75

0.8

0.80 0.7

0.70

0.7

0.75

5

10

15

0.6

5

10

15

5

10

15

5

10

15

Regularization strength

Regularization strength

Regularization strength

Regularization strength

β-VAE

IDVAE

IVAE

FULLVAE

Explicitness

Figure 4. Beta score and explicitness median (the higher the better) as a function of the regularization strength.

to deduce that the regularization strength is both model and data speciﬁc, and it is also affected by the choice of the disentanglement metric.
5.3. Limitations
In our experimental campaign we use the same convolutional architecture for all the methods we compare. We do not vary the optimization hyper-parameters and the dimension of the latent variables. Hence, we cannot ensure that every method runs in its best conditions. Nevertheless, our experimental protocol makes our analysis independent of method-speciﬁc optimizations, and has the beneﬁt of reducing training times.
Also, we use the whole set of ground-truth factors as auxiliary variables, in the semi-supervised settings too, whereas it is possible to study the impact of only a subset of the factors to be available. Moreover, IDVAE and IVAE can use any kind of auxiliary variables, as long as they are informative about the ground-truth factors: they are not restricted to using, e.g., labels corresponding to input data, as we (and many other studies) do in our experiments.
Finally, we do not study the implications and beneﬁts of

disentanglement for solving complex downstream tasks, which is an interesting task that we leave for future work.
6. Conclusion
In this work, we made a step further in the design of identiﬁable generative models to learn disentangled representations. IDVAE uses a prior that encodes ground-truth factor information captured by auxiliary observed variables. The key idea was to learn an optimal representation of the latent space, deﬁned by an inference network on the posterior of the latent variables, given the auxiliary variables. Such posterior is then used as a prior on the latent variables of a second generative model, whose inference network learns a mapping between input observations and latents. We also proposed a semi-supervised version of IDVAE that can be applied when auxiliary variables are available for a subset of the input observations only. Experimental results offer evidence that IDVAE and SS-IDVAE often outperforms existing alternatives to learn disentangled representations, according to several established metrics.

An Identiﬁable Double VAE For Disentangled Representations

References
Achille, A. and Soatto, S. Information dropout: learning optimal representations through noise. IEEE Trans. Pattern Anal. Mach. Intell., 2016.
Bengio, Y., Courville, A., and Vincent, P. Representation learning: A review and new perspectives. IEEE Trans. Pattern Anal. Mach. Intell., 2013.
Bishop, C. M. Pattern Recognition and Machine Learning (Information Science and Statistics). Springer-Verlag, 2006.
Bouchacourt, D., Tomioka, R., and Nowozin, S. Multi-level variational autoencoder: Learning disentangled representations from grouped observations. In Proc. of the 32nd AAAI Conf. on Artif. Intel., AAAI, 2018.
Burgess, C. P., Higgins, I., Pal, A., Matthey, L., Watters, N., Desjardins, G., and Lerchner, A. Understanding disentangling in β-vae. In Proc. of the 30th Int. Conf. on Neural Inf. Proc. Sys., NeurIPS, 2017.
Chen, J. and Batmanghelich, K. Weakly supervised disentanglement by pairwise similarities. In Proc. of the 34th AAAI Conf. on Artif. Intel., AAAI, 2020a.
Chen, J. and Batmanghelich, K. Robust ordinal vae: Employing noisy pairwise comparisons for disentanglement. ArXiv, 2020b.
Chen, T. Q., Li, X., Grosse, R. B., and Duvenaud, D. K. Isolating sources of disentanglement in variational autoencoders. In Proc. of the 31st Int. Conf. on Neural Inf. Proc. Sys., NeurIPS, 2018.
Cheung, B., Livezey, J. A., Bansal, A. K., and Olshausen, B. A. Discovering hidden factors of variation in deep networks. In CoRR, 2015.
Comon, P. Independent component analysis, a new concept? Signal Process., 36:287–314, 1994.
Higgins, I., Matthey, L., Pal, A., Burgess, C., Glorot, X., Botvinick, M. M., Mohamed, S., and Lerchner, A. betavae: Learning basic visual concepts with a constrained variational framework. In Proc. of the 5th Int. Conf. on Learn. Repr., ICLR, 2017.

Hyva¨rinen, A. and Pajunen, P. Nonlinear independent component analysis: Existence and uniqueness results. Neural networks, 12:429–439, 1999.
Khemakhem, I., Kingma., D. P., Mont, R. P., and Hyva¨rinen, A. Variational autoencoders and nonlinear ica: A unifying framework. In Proc. of the 23rd Int. Conf. on Artif. Intel. and Stat., AISTATS, 2020.
Kim, H. and Mnih, A. Disentangling by factorising. In Proc. of the 35th Int. Conf. on Mach. Learn., ICML, 2018.
Kingma, D. P. and Welling, M. Auto-encoding variational bayes. In Proc. of the 2nd Int. Conf. on Learn. Repr., ICLR, 2014.
Kingma, D. P., Mohamed, S., Rezende, D. J., and Welling, M. Semi-supervised learning with deep generative models. In Proc. of the 27th Int. Conf. on Neural Inf. Proc. Sys., NeurIPS, 2014.
Klys, J., Snell, J., and Zemel, R. Learning latent subspaces in variational autoencoders. In Proc. of the 31st Int. Conf. on Neural Inf. Proc. Sys., NeurIPS, 2018.
Kumar, A. and Poole, B. On implicit regularization in β-VAEs. In Proceedings of the 37th International Conference on Machine Learning, ICML, 2020.
Kumar, A., Sattigeri, P., and Balakrishnan, A. Variational inference of disentangled latent concepts from unlabeled observations. In Proc. of the 6th Int. Conf. on Learn. Repr., ICLR, 2018.
LeCun, Y., Huang, F. J., and Bottou, L. Learning methods for generic object recognition with invariance to pose and lighting. In Proc. of the 2004 IEEE Comput. Society Conf. on Comput. Vision and Pat. Recogn., CVPR, 2004.
Locatello, F., Bauer, S., Lucic, M., Gelly, S., Scho¨lkopf, B., and Bachem, O. Challenging common assumptions in the unsupervised learning of disentangled representations. In Proc. of the 36th Int. Conf. on Mach. Learn., ICML, 2019.
Locatello, F., Poole, B., Ra¨tsch, G., Scho¨lkopf, B., Bachem, O., and Tschannen, M. Weakly-supervised disentanglement without compromises. In Proc. of the 37th Int. Conf. on Mach. Learn., ICML, 2020a.

Hoffman, M. D. and Johnson, M. J. Elbo surgery: yet another way to carve up the variational evidence lower bound. In Workshop in Adv. in Approx. Bayes. Infer., NeurIPS, 2016.

Locatello, F., Tschannen, M., Bauer, S., Ra¨tsch, G., Scho¨lkopf, B., and Bachem, O. Disentangling factors of variation using few labels. In Proc. of the 8th Int. Conf. on Learn. Repr., ICLR, 2020b.

Hosoya, H. Group-based learning of disentangled representations with generalizability for novel contents. In Proc. of the 28th Int. Joint Conf. on Artif. Intel., IJCAI, 2019.

Makhzani, A. and Frey, B. J. Pixelgan autoencoders. In Proc. of the 30th Int. Conf. on Neural Inf. Proc. Sys., NeurIPS, 2017.

An Identiﬁable Double VAE For Disentangled Representations
Mathieu, E., Rainforth, T., Siddharth, N., and Teh, Y. W. Disentangling disentanglement in variational autoencoders. In Proceedings of the 36th International Conference on Machine Learning, ICML, 2019.
Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Kopf, A., Yang, E., DeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L., Bai, J., and Chintala, S. Pytorch: An imperative style, high-performance deep learning library. In Proc. of the 32nd Int. Conf. on Neural Inf. Proc. Sys., NeurIPS. 2019.
Reed, S. E., Zhang, Y., Zhang, Y., and Lee, H. Deep visual analogy-making. In Proc. of the 28th Int. Conf. on Neural Inf. Proc. Sys., NeurIPS. Curran Associates, Inc., 2015.
Rezende, D. J., Mohamed, S., and Wiestra, D. Stochastic backpropagation and approximate inference in deep generative models. In Proc. of the 31st Int. Conf. on Mach. Learn., ICML, 2014.
Ridgeway, K. and Mozer, M. C. Learning deep disentangled embeddings with the f-statistic loss. In Proc. of the 31st Int. Conf. on Neural Inf. Proc. Sys., NeurIPS, 2018.
Shu, R., Chen, Y., Kumar, A., Ermon, S., and Poole, B. Weakly supervised disentanglement with guarantees. In Proc. of the 8th Int. Conf. on Learn. Repr., ICLR, 2020.
Siddharth, N., Paige, B., van de Meent, J., Desmaison, A., Goodman, N., Kohli, P., Wood, F., and Torr, P. Learning disentangled representations with semi-supervised deep generative models. In Proc. of the 30th Int. Conf. on Neural Inf. Proc. Sys., NeurIPS, 2017.
Tishby, N., Pereira, F. C., and Bialek, W. The information bottleneck method. In Proc. of the 34th Annual Allert. Conf. on Comm. Contr and Comput., 1999.
Titsias, M. and La´zaro-Gredilla, M. Doubly stochastic variational bayes for non-conjugate inference. In Proc. of the 31st Int. Conf. on Mach. Learn., ICML, 2014.
Zhao, S., Song, J., and Ermon, S. Infovae: Balancing learning and inference in variational autoencoders. In Proc. of the 33rd AAAI Conf. on Artif. Intel., AAAI, 2019.

An Identiﬁable Double VAE For Disentangled Representations
A. ELBO derivation for IDVAE

log p(x, u) = log p(x, u, z)dz =

where:

= log p(x|u, z)p(z|u)p(u)dz =

= log

p(x|u, z)p(z|u)p(u)

q(z|x, u)

q(z|x, u)dz ≥ LIDVAE

p(x|u, z)p(z|u)p(u)

≥ Eq(z|x,u)[log

]= q(z|x, u)

= Eq(z|x,u)[log p(x|u, z)] − KL(q(z|x, u)||p(z|u)) + log p(u),

(21)

log p(u) = log p(u, z)dz ≥ Lprior =

= Eq(z|u)[log p(u|z)] − KL(q(z|u)||p(z)).

(22)

B. ELBO derivation for SS-IDVAE

log p(x) = log p(x, u, z)dudz =

= log p(x|u, z)p(z|u)p(u)dudz =

= log

p(x|u, z)p(z|u)p(u) q(u, z|x)dudz ≥
q(u, z|x)

p(x|u, z)p(z|u)p(u)

≥ Eq(u,z|x)[log

]= q(u, z|x)

p(x|u, z)p(z|u)p(u) = Eq(z|x,u)q(u|x)[log q(z|x, u)q(u|x) ] =

= Eq(u|x)[LIDVAE] + H(q(u|x)).

(23)

Combining eqs. (21) to (23) we obtain LSS-IDVAE, where it is clear that we use the sum over the data samples instead of the expectation. As stated in the main paper, we also add the term – E(x,u)∼pl [log q(u|x)] – such that it can learn also from labeled data.

C. Sketch of the proof of Theorem 1
In this section, we report a sketch of the proof of Theorem 1. Following the proof strategy of Khemakhem et al. (2020), the proof consists of three main steps.
In the ﬁrst step, we use assumption (i) to demonstrate that observed data distributions are equal to noiseless distributions. Supposing to have two sets of parameters (f , T, η) and (˜f , T˜ , η˜), with a change of variable x¯ = f (z) = ˜f (z), we show that:

p˜T,η,f ,u(x) = p˜T˜ ,η˜,˜f ,u˜ (x),

(24)

where:

p˜T,η,f,u(x) = pT,η(f −1(x)|u)|detJf−1 (x)|1X (x)

(25)

An Identiﬁable Double VAE For Disentangled Representations
In the second step, we use assumption (iv) to remove all the terms that are a function of x or u. By substituting pT,η with its exponential conditionally factorial form, taking the log of both sides of eq. (25), we obtain dk + 1 equations. Then:

T(f −1(x)) = AT (f −1(x)) + c.

(26)

In the last step, assumptions (i) and (iii) are used to show that the linear transformation is invertible and so (f , T, η) ∼ (˜f , T˜ , η˜). This concludes the proof.
For a full derivation of the proof, we point the reader to section B of the supplement in Khemakhem et al. (2020), which holds also for our variant of the theorem.
D. Model architectures, parameters and hyperparameters
All the selected methods (including the semi-supervised variants) share the same convolutional architecture. The conditional prior in IVAE is a MLP network, in IDVAE we use a simple MLPVAE, both with leaky ReLU activation functions. The ground-truth factor learner implementing qζ(u|x) in SS-IDVAE and SS-IVAE is a convolutional neural network.

Encoder
Input: 64 × 64× number of channels 4 × 4 conv, 32 ReLU, stride 2 4 × 4 conv, 32 ReLU, stride 2 4 × 4 conv, 64 ReLU, stride 2 4 × 4 conv, 64 ReLU, stride 2 FC 256*, FC 2 × d

Decoder
Input: Rd, where d is the number of ground-truth factors FC, 256 ReLU FC, 4 × 4 × 64 ReLU 4 × 4 upconv, 64 ReLU, stride 2 4 × 4 upconv, 32 ReLU, stride 2 4 × 4 upconv, 32 ReLU, stride 2 4 × 4 upconv, number of channels, stride 2

Table 2. Main Encoder-Decoder architecture. In IVAE and IDVAE, we give u as input to the fully connected layer of the Encoder which size becomes 256 + d.

Conditional Prior Encoder
FC, 1000 leaky ReLU FC, 1000 leaky ReLU FC, 1000 leaky ReLU FC 2 × d

Conditional Prior Decoder
FC, 1000 leaky ReLU FC, 1000 leaky ReLU FC, 1000 leaky ReLU FC d

Table 3. IDVAE Conditional Prior Encoder-Decoder architecture. IVAE uses the encoder only.

Ground-truth Factor Learner
Input: 64 × 64× number of channels. d is the number of ground-truth factors. 4 × 4 conv, 32 ReLU, stride 2 4 × 4 conv, 32 ReLU, stride 2 4 × 4 conv, 64 ReLU, stride 2 4 × 4 conv, 64 ReLU, stride 2 FC 256, FC 2 × d
Table 4. Ground-truth factor learner implementing qζ(u|x) in SS-IDVAE and SS-IVAE.

An Identiﬁable Double VAE For Disentangled Representations

Parameters
batch size optimizer Adam: beta1 Adam: beta2 Adam: epsilon Adam: learning rate training steps

Values
64 Adam 0.9 0.999 1e-8 1e-4 300’000

Table 5. Common hyperparameters to each of the considered methods.

E. Implementation of disentanglement metrics
Beta score The idea behind the beta score (Higgins et al., 2017) is to ﬁx a random ground-truth factor and sample two mini batches of observations from the corresponding generative model. The encoder is then used to obtain a learned representation from the observations (with a ground-truth factor in common). The dimension-wise absolute difference between the two representation is computed and a simple linear classiﬁer C is used to predict the corresponding ground-truth factor. This is repeated batch size times and the accuracy of the predictor is the disentanglement metric score.
MIG - Mutual Information Gap The mutual information gap (MIG) (Chen et al., 2018) is computed as the average, normalized difference between the highest and second highest mutual information of each ground-truth factor with the dimensions of the learned representation. As done in Locatello et al. (2019), we consider the mean representation. and compute the discrete mutual information by binning each dimension of the mean learned representation into n bins bins.
Modularity and Explicitness A representation is modular if each dimension depends on at most one ground-truth factor. Ridgeway & Mozer (2018) propose to measure the Modularity as the average normalized squared difference of the mutual information of the factor of variations with the highest and second-highest mutual information with a dimension of the learned representation. A representation is explicit if it is easy to predict a factor of variation. To compute the explicitness, they train a one-versus-rest logistic regression classiﬁer to predict the ground-truth factor of variation and measeure its ROC-AUC. In the current implementation, observations are discretized into n bins bins.
SAP - Separated Attribute Predictability According to Kumar et al. (2018), the Separated Attribute Predictability (SAP) score is computed from a score matrix where each entry is the linear regression or classiﬁcation score (in case of discrete factors) of predicting a given ground-truth factors with a given dimension of the learned representation. The (SAP) score is the average difference of the prediction error of the two most predictive learned dimensions for each factor. As done in (Locatello et al., 2019), we use a linear SVM as classiﬁer.
As explained in the main paper, the implementation of the selected disentanglement evaluation metrics is based on Locatello et al. (2019). We report the main parameters in table 6.

Disentanglement metrics
Beta score MIG Modularity and Explicitness SAP score

Parameters
train size=10’000, test size=5’000, batch size=64, predictor=logistic regression train size=10’000, n bins=20 train size=10’000, test size=5’000, batch size=16, n bins=20 train size=10’000, test size=5’000, batch size=16, predictor=linear SVM, C=0.01

Table 6. Disentanglement metrics and their parameters.

An Identiﬁable Double VAE For Disentangled Representations
F. Full experiments
In this section, we report the full set of experiments, including reconstructions and latent traversals.

(a) DSPRITES: original observations.

(b) DSPRITES: reconstructions by IDVAE.

(c) CARS3D: original observations.

(d) CARS3D: reconstructions by IDVAE.

(e) SHAPES3D: original observations.

(f) SHAPES3D: reconstructions by IDVAE.

(g) SMALLNORB: original observations.

(h) SMALLNORB: reconstructions by IDVAE.

Figure 5. Original observations vs IDVAE reconstructions.

An Identiﬁable Double VAE For Disentangled Representations
(a) DSPRITES.
(b) CARS3D.
(c) SHAPES3D.
(d) SMALLNORB. Figure 6. IDVAE latent traversals. Each row corresponds to a dimension of z, that we vary in the range [−3, 3]. We can see that, in some cases, changing a dimension can affect multiple ground-truth factors, meaning that IDVAE has not obtained full disentanglement. (a) From top to bottom: orientation, scale, shape(?), posY, posX. (b) From top to bottom: azimuth, elevation, object type. (c) From top to bottom: wall color, ﬂoor color, object type, azimuth, object color, object size. (d) azimuth, elevation, lighting, category.

An Identiﬁable Double VAE For Disentangled Representations

Beta score

MIG

DSPRITES
0.9 0.8 0.7 0.6 0.5
1 2 3 4 5 6 7 8 9 10

CARS3D
1.0 0.9 0.8 0.7 0.6 0.5
1 2 3 4 5 6 7 8 9 10

SHAPES3D
0.8 0.6 0.4
1 2 3 4 5 6 7 8 9 10

SMALLNORB 1.0
0.9
0.8
0.7
0.6
0.5 1 2 3 4 5 6 7 8 9 10

0.4

0.08

0.3

0.06

0.2

0.04

0.1

0.02

0.0

0.00

1 2 3 4 5 6 7 8 9 10

1 2 3 4 5 6 7 8 9 10

0.3 0.2 0.1 0.0
1 2 3 4 5 6 7 8 9 10

0.5 0.4 0.3 0.2 0.1 0.0
1 2 3 4 5 6 7 8 9 10

1.00 0.95 0.90 0.85 0.80
1 2 3 4 5 6 7 8 9 10

1.0

1.00

1.00

0.9

0.95

0.95

0.8

0.90

0.90

0.85

0.7

0.85

0.80

0.80

0.6

0.75

1 2 3 4 5 6 7 8 9 10

1 2 3 4 5 6 7 8 9 10

1 2 3 4 5 6 7 8 9 10

0.875 0.80
0.850

0.825

0.75

0.800 0.70
0.775

0.750

0.65

1 2 3 4 5 6 7 8 9 10

1 2 3 4 5 6 7 8 9 10

0.9 0.85

0.8

0.80

0.75 0.7
0.70

0.6

0.65

1 2 3 4 5 6 7 8 9 10

1 2 3 4 5 6 7 8 9 10

Modularity

Explicitness

SAP score

0.6

0.4 0.4

0.3

0.3 0.3

0.4

0.2

0.2

0.2

0.2 0.1 0.1 0.1

0.0
1 2 3 4 5 6 7 8 9 10 Model

0.0
1 2 3 4 5 6 7 8 9 10 Model

0.0
1 2 3 4 5 6 7 8 9 10 Model

0.0
1 2 3 4 5 6 7 8 9 10 Model

Figure 7. Beta score, MIG, Modularity, Explicitness, and SAP (the higher the better). 1=β-VAE, 2=SS-IDVAE (1%), 3=SS-IDVAE (10%), 4=IDVAE, 5=SS-IVAE (1%), 6=SS-IVAE (10%), 7=IVAE, 8=SS-FULLVAE (1%), 9=SS-FULLVAE (10%), 10=FULLVAE. Percentage of labeled samples in parenthesis.

Beta score

MIG score

Modularity

0.9 0.8 0.7 0.6 0.4 0.3 0.2 0.1
0.975 0.950 0.925 0.900 0.875 0.850
0.84 0.82 0.80 0.78 0.76
0.6 0.4 0.2 0.0

DSPRITES

5

10

5

10

An Identiﬁable Double VAE For Disentangled Representations

0.9 0.8 0.7 0.6 15 0.08 0.06 0.04 0.02 15

CARS3D

5

10

5

10

0.8 0.7 0.6 0.5 0.4 0.3 15
0.125 0.100 0.075 0.050 0.025
15

SHAPES3D

5

10

0.9 0.8 0.7 0.6 15

0.3 0.2 0.1

5

10

15

SMALLNORB

5

10

15

5

10

15

0.95

0.90

0.85

0.80

5

10

15

0.800 0.775 0.750 0.725 0.700 0.675

5

10

15

Regularization strength

0.3

0.2

0.1

0.0

5

10

15

Regularization strength

β-VAE SS-IDVAE (1%)

0.95

0.90

0.85

5

10

15

5

10

15

Regularization strength

0.80
0.85 0.80 0.75 0.70 0.65 0.60

0.2

0.1

0.0

5

10

15

Regularization strength

SS-IDVAE (10%)
IDVAE

SS-IVAE (1%) SS-IVAE (10%)

0.95

0.90

0.85

0.80

5

10

15

5

10

15

5

10

15

Regularization strength

5

10

15

Regularization strength

IVAE
SS-FULLVAE (1%)

0.85

0.80

0.75

0.70

0.65

5

10

15

Regularization strength

0.4

0.3

0.2

0.1

0.0

5

10

15

Regularization strength

SS-FULLVAE (10%)
FULLVAE

Explicitness

SAP score

Figure 8. Beta score, MIG, modularity, explicitness and SAP median (the higher the better) as a function of the regularization strength, for each method on DSPRITES, CARS3D, SHAPES3D, SMALLNORB.

