Proceedings of Machine Learning for Healthcare 2016

JMLR W&C Track Volume 56

Modeling Missing Data in Clinical Time Series with RNNs

Zachary C. Lipton Department of Computer Science and Engineering University of California, San Diego La Jolla, CA 92093, USA

zlipton@cs.ucsd.edu

David C. Kale USC Information Sciences Institute Marina del Rey, CA, USA

kale@isi.edu

Randall Wetzel Laura P. and Leland K. Whittier Virtual Pediatric Intensive Care Unit Children’s Hospital LA Los Angeles, CA 90089

rwetzel@chla.usc.edu

arXiv:1606.04130v5 [cs.LG] 11 Nov 2016

Abstract
We demonstrate a simple strategy to cope with missing data in sequential inputs, addressing the task of multilabel classiﬁcation of diagnoses given clinical time series. Collected from the pediatric intensive care unit (PICU) at Children’s Hospital Los Angeles, our data consists of multivariate time series of observations. The measurements are irregularly spaced, leading to missingness patterns in temporally discretized sequences. While these artifacts are typically handled by imputation, we achieve superior predictive performance by treating the artifacts as features. Unlike linear models, recurrent neural networks can realize this improvement using only simple binary indicators of missingness. For linear models, we show an alternative strategy to capture this signal. Training models on missingness patterns only, we show that for some diseases, what tests are run can be as predictive as the results themselves.
1. Introduction
For each admitted patient, hospital intensive care units record large amounts data in electronic health records (EHRs). Clinical staﬀ routinely chart vital signs during hourly rounds and when patients are unstable. EHRs record lab test results and medications as they are ordered or delivered by physicians and nurses. As a result, EHRs contain rich sequences of clinical observations depicting both patients’ health and care received. We would like to mine these time series to build accurate predictive models for diagnosis and other applications. Recurrent neural networks (RNNs) are well-suited to learning sequential or temporal relationships from such time series. RNNs oﬀer unprecedented predictive power in myriad sequence learning domains, including natural language processing, speech, video, and handwriting. Recently, Lipton et al. (2016) demonstrated the eﬃcacy of RNNs for multilabel classiﬁcation of diagnoses in clinical time series data.
c 2016.

HR DIA BP SYS BP TEMP RESPRATE FRAC O2 O2 SAT END CO2 CAP RATE PH URINE OUT GLUCOSE GLASGOW
Figure 1: Missingness artifacts created by discretization
However, medical time series data present modeling problems not found in the clean academic datasets on which most RNN research focuses. Clinical observations are recorded irregularly, with measurement frequency varying between patients, across variables, and even over time. In one common modeling strategy, we represent these observations as a sequence with discrete, ﬁxed-width time steps. Problematically, the resulting sequences often contain missing values (Marlin et al., 2012). These values are typically not missing at random, but reﬂect decisions by caregivers. Thus, the pattern of recorded measurements contain potential information about the state of the patient. However, most often, researchers ﬁll missing values using heuristic or unsupervised imputation (Lasko et al., 2013), ignoring the potential predictive value of the missingness itself.
In this work we extend the methodology of Lipton et al. (2016) for RNN-based multilabel prediction of diagnoses. We focus on data gathered from the Children’s Hospital Los Angeles pediatric intensive care unit (PICU). Unlike Lipton et al. (2016), who approach missing data via heuristic imputation, we directly model missingness as a feature, achieving superior predictive performance. RNNs can realize this improvement using only simple binary indicators for missingness. However, linear models are unable to use indicator features as eﬀectively. While RNNs can learn arbitrary functions, capturing the interactions between the missingness indicators the sequence of observation inputs, linear models can only learn substitution values. For linear models, we introduce an alternative strategy to capture this signal, using a small number of simple hand-engineered features.
Our experiments demonstrate the beneﬁt modeling missing data as a ﬁrst-class feature. Our methods improve the performance of RNNs, multilayer perceptrons (MLPs), and linear models. Additionally we analyze the predictive value of missing data information by training models on the missingness indicators only. We show that for several diseases, what tests are run can be as predictive as the actual measurements. While we focus on classifying diagnoses, our methods can be applied to any predictive modeling problem involving sequence data and missing values, such as early prediction of sepsis (Henry et al., 2015) or real-time risk modeling (Wiens et al., 2012).
It is worth noting that we may not want our predictive models to rely upon the patterns of treatment, as argued by Caruana et al. (2015). Once deployed, our models may inﬂu-

ence the treatment protocols, shifting the distribution of future data, and thus invalidating their predictions. Nonetheless, doctors at present often utilize knowledge of past care, and treatment signal can leak into the actual measurements themselves in ways that suﬃciently powerful models can exploit. As a ﬁnal contribution of this paper, we present a critical discussion of these practical and philosophical issues.
2. Data
Our dataset consists of patient records extracted from the EHR system at CHLA (Marlin et al., 2012; Che et al., 2015) as part of an IRB-approved study. In all, the dataset contains 10, 401 PICU episodes. Each episode describes the stay of one patient in the PICU for a period of at least 12 hours. In addition, each patient record contains a static set of diagnostic codes, annotated by physicians either during or after each PICU visit.
2.1 Inputs
In their rawest representation, episodes consist of irregularly spaced measurements of 13 variables: diastolic and systolic blood pressure, peripheral capillary reﬁll rate, end-tidal CO2 (ETCO2), fraction of inspired O2 (FIO2), total Glascow coma scale, blood glucose, heart rate, pH, respiratory rate, blood oxygen saturation, body temperature, and urine output. To render our data suitable for learning with RNNs, we convert to discrete sequences of hourly time steps, where time step t covers the interval between hours t and t + 1, closed on the left but open on the right. Because actual admission times are not recorded reliably, we use the time of the ﬁrst recorded observation as time step t = 0. We combine multiple measurements of the same variable within the same hour window by taking their mean.
Vital signs, such as heart rate, are typically measured about once per hour, while lab tests requiring a blood draw (e.g., glucose) are measured on the order of once per day (see appendix B for measurement frequency statistics). In addition, the timing of and time between observations varies across patients and over time. The resulting sequential representation have many missing values, and some variables missing altogether.
Note that our methods can be sensitive to the duration of our discrete time step. For example, halving the duration would double the length of the sequences, making learning by backpropagation through time more challenging (Bengio et al., 1994). For our data, such cost would not be justiﬁed because the most frequently measured variables (vital signs) are only recorded about once per hour. For higher frequency recordings of variables with faster dynamics, a shorter time step might be warranted.
To better condition our inputs, we scale each variable to the [0, 1] interval, using expertdeﬁned ranges. Additionally, we correct for diﬀerences in heart rate, respiratory rate, (Fleming et al., 2011) and blood pressure (NHBPEP Working Group 2004) due to age and gender using tables of normal values from large population studies.
2.2 Diagnostic labels
In this work, we formulate phenotyping (Oellrich et al, 2015) as multilabel classiﬁcation of sequences. Our labels include 429 distinct diagnosis codes from an in-house taxonomy at CHLA, similar to ICD-9 codes (World Health Organization, 2004) commonly used in medical

informatics research. These labels include a wide range of acute conditions, such as acute respiratory distress, congestive heart failure, and sepsis. A full list is given in appendix A. We focus on the 128 most frequent, each having at least 50 positive examples in our dataset. Naturally, the diagnoses are not mutually exclusive. In our data set, the average patient is associated with 2.24 diagnoses. Additionally, the base rates of the diagnoses vary widely (see appendix A).

3. Recurrent Neural Networks for Multilabel Classiﬁcation

While our focus in this paper is on missing data, for completeness, we review the LSTM RNN architecture for performing multilabel classiﬁcation of diagnoses introduced by Lipton
R et al. (2016). Formally, given a series of observations x(1), ..., x(T ), we desire a classiﬁer to
generate hypotheses yˆ of the true labels y, where each input xt ∈ D and the output yˆ ∈ [0, 1]K. Here, D denotes the input dimension, K denotes the number of labels, t indexes sequence steps, and for any example, T denotes the length of that sequence.
Our proposed RNN uses LSTM memory cells (Hochreiter and Schmidhuber, 1997) with forget gates (Gers et al., 2000) but without peephole connections (Gers et al., 2003). As output, we use a fully connected layer followed by an element-wise logistic activation function σ. We apply log loss (binary cross-entropy) as the loss function at each output node.
The following equations give the update for a layer of memory cells h(lt), where h(l−t)1 stands for the previous layer at the same sequence step (a previous LSTM layer or the input x(t)) and h(lt−1) stands for the same layer at the previous sequence step:
g(lt) = φ(Wlgxh(l−t)1 + Wlghh(lt−1) + bgl ) i(lt) = σ(Wlixh(l−t)1 + Wlihh(lt−1) + bil ) f (lt) = σ(Wlfxh(l−t)1 + Wlfhh(lt−1) + bfl ) o(lt) = σ(Wloxh(l−t)1 + Wlohh(lt−1) + bol ) s(lt) = g(lt) i(li) + s(lt−1) f (lt) h(lt) = φ(s(lt)) o(lt)

In these equations, σ stands for an element-wise application of the logistic function, φ stands for an element-wise application of the tanh function, and is the Hadamard (element-wise) product. The input, output, and forget gates are denoted by i, o, and f respectively, while g is the input node and has a tanh activation.
The loss at a single sequence step is the average log loss calculated across all labels:

1 l=K

loss(yˆ, y) = K

−(yl · log(yˆl) + (1 − yl) · log(1 − yˆl)).

l=1

To overcome the diﬃculty of learning to pass information across long sequences, we use the target replication strategy proposed by Lipton et al. (2016), in which we replicate the static targets at each sequence step providing a local error signal. This technique is also motivated by our problem: we desire to make accurate predictions even if the sequence were

truncated (as in early-warning systems). To calculate loss, we take a convex combination of the ﬁnal step loss and the average of the losses over predictions yˆ(t) at all steps t:
α · 1 T loss(yˆ(t), y(t)) + (1 − α) · loss(yˆ(T ), y(T )) T
t=1
where α ∈ [0, 1] is a hyper-parameter determining the relative importance of performance on the intermediary vs. ﬁnal targets. At inference time, we consider only the output at the ﬁnal step.
4. Missing Data
In this section, we explain our procedures for imputation, missing data indicator sequences, engineering features of missing data patterns.
4.1 Imputation To address the missing data problem, we consider two diﬀerent imputation strategies (forward-ﬁlling and zero imputation), as well as direct modeling via indicator variables. Because imputation and direct modeling are not mutually exclusive, we also evaluate them in combination. Suppose that x(it) is “missing.” In our zero-imputation strategy, we simply set x(it) := 0 whenever it is missing. In our forward-ﬁlling strategy, we impute x(it) as follows:
• If there is at least one previously recorded measurement of variable i at a time t < t, we perform forward-ﬁlling by setting x(it) := x(it ).
• If there is no previous recorded measurement (or if the variable is missing entirely), then we impute the median estimated over all measurements in the training data.
This strategy is motivated by the intuition that clinical staﬀ record measurements at intervals proportional to rate at which they are believed or observed to change. Heart rate, which can change rapidly, is monitored much more frequently than blood pH. Thus it seems reasonable to assume that a value has changed little since the last time it was measured.
Figure 2: (top left) no imputation or indicators, (bottom left) imputation absent indicators, (top right) indicators but no imputation, (bottom right) indicators and imputation. Time ﬂows from left to right.

4.2 Learning with Missing Data Indicators Our indicator variable approach to missing data consists of augmenting our inputs with binary variables m(it) for every x(it), where m(it) := 1 if x(it) is imputed and 0 otherwise. Through their hidden state computations, RNNs can use these indicators to learn arbitrary functions of the past observations and missingness patterns. However, given the same data, linear models can only learn hard substitution rules. To see why, consider a linear model that outputs prediction f (z), where z = i wi · xi. With indicator variables, we might say that z = i wi · xi + i θi · mi where θi are the weights for each mi. If xi is set to 0 and mi to 1, whenever the feature xi is missing, then the impact on the output θi · mi = θi is exactly equal to the contribution wi · x∗i for some x∗i = θi/wi. In other words, the linear model can only use the indicator in a way that depends neither on the previously observed values (x1i ...xit−1), nor any other evidence in the inputs.
Figure 3: Depiction of RNN zero-ﬁlled inputs and missing data indicators.
Note that for a linear model, the impact of a missing data indicator on predictions must be monotonic. In contrast, the RNN might infer that for one patient heart rate is missing because they went for a walk, while for another it might signify an emergency. Also note that even without indicators, the RNN might learn to recognize ﬁlled-in vs real values. For example, with forward-ﬁlling, the RNN could learn to recognize exact repeats. For zero-ﬁlling, the RNN could recognize that values set to exactly 0 were likely missing measurements.
4.3 Hand-engineered missing data features To overcome the limits of the linear model, we also designed features from the indicator sequences. As much as possible, we limited ourselves to features that are simple to calculate, intuitive, and task-agnostic. The ﬁrst is a binary indicator for whether a variable was measured at all. Additionally, we compute the mean and standard deviation of the indicator sequence. The mean captures the frequency with which each variable is measured which carries information about the severity of a patient’s condition. The standard deviation, on the other hand, computes a non-monotonic function of frequency that is maximized when a variable is missing exactly 50% of the time. We also compute the frequency with which a variable switches from measured to missing or vice versa across adjacent sequence steps.

Finally, we add features that capture the relative timing of the ﬁrst and last measurements of a variable, computed as the number of hours until the measurement divided by the length of the full sequence.

5. Experiments

We now present the training details and empirical ﬁndings of our experiments. Our LSTM

RNNs each have 2 hidden layers of 128 LSTM cells each, non-recurrent dropout of 0.5, and

2 2

weight decay of 10−6.

We train on 80% of data, setting aside 10% each for validation

and testing. We train each RNN for 100 epochs, retaining the parameters corresponding to

the epoch with the lowest validation loss.

We compare the performance of RNNs against logistic regression and multilayer per-

ceptrons (MLPs). We apply 2 regularization to the logistic regression model. The MLP has 3 hidden layers with 500 nodes each, rectiﬁed linear unit activations, and dropout (with

probability of 0.5), choosing the number of layers and nodes by validation performance. We

train the MLP using stochastic gradient descent with momentum.

We evaluate each baseline with two sets of features: raw and hand-engineered. Note

that our baselines cannot be applied directly to variable-length inputs. For the raw features,

we concatenate three 12-hour subsequences, one each from the beginning, middle, and

end of the time series. For shorter time series, these intervals may overlap. Thus raw

representations contain 2×3×12×13 = 936 features. We train each baseline on ﬁve diﬀerent

combinations of raw inputs: (1) measurements with zero-ﬁlling, (2) measurements with

forward-ﬁlling, (2) measurements with zero-ﬁlling + missing data indicators, (4) forward-

ﬁlling + missing data indicators, and (5) missing data indicators only.

Our hand-engineered features capture central tendencies, variability, extremes, and

trends. These include the ﬁrst and last measurements and their diﬀerence, maximum and

minimum values, mean and standard deviation, median and 25th and 75th percentiles, and

the slope and intercept of least squares line ﬁt. We also computed the 8 missing data

features described in section 4. We improve upon the baselines in Lipton et al. (2016) by

computing the hand-engineered features over diﬀerent windows of time, giving them access

to greater temporal information and enabling them to better model patterns of missingness.

We extract hand-engineered features from the entire time series and from three possibly

overlapping intervals: the ﬁrst and last 12 hours and the interval between (for shorter se-

quences, we instead use the middle 12 hours). This yields a total of 4 × 12 × 13 = 624

and 4 × 8 × 13 = 416 hand-engineered measurement and missing data features, respectively.

We train baseline models on three diﬀerent combinations of hand-engineered features: (1)

measurement-only, (2) indicator-only, and (3) measurement and indicator.

We evaluate all models on the same training, validation, and test splits. Our evaluation

metrics include area under the ROC curve (AUC) and F1 score (with threshold chosen based

on validation performance). We report both micro-averaged (calculated across all predic-

tions) and macro-averaged (calculated separately on each label, then averaged) measures to

mitigate the weaknesses in each (Lipton et al., 2014). Finally we also report precision at 10,

whose maximum is 0.2238 because we have on average 2.238 diagnoses per patient. This

metric seems appropriate because we could imagine this technology would be integrated into

a diagnostic assistant. In that case, its role might be to suggest the most likely diagnoses

among which a professional doctor would choose. Precision at 10 evaluates the quality of the top 10 suggestions.

Classiﬁcation performance for 128 ICU phenotypes

Model

Micro AUC Macro AUC Micro F1

Base Rate Best Possible

0.7128

0.5

1.0

1.0

0.1346 1.0

Logistic Regression

Log Reg - Zeros Log Reg - Impute Log Reg - Zeros & Indicators Log Reg - Impute & Indicators Log Reg - Indicators Only

0.8108 0.8201 0.8143 0.8242 0.7929

0.7244 0.7455 0.7269 0.7442 0.6924

0.2149 0.2404 0.2239 0.2467 0.1952

Multilayer Perceptron

MLP - Zeros MLP - Impute MLP - Zeros & Indicators MLP - Impute & Indicators MLP - Indicators Only

0.8263 0.8376 0.8381 0.8419 0.8112

0.7502 0.7708 0.7705 0.7805 0.7321

0.2344 0.2557 0.2530 0.2637 0.1962

LSTMs

LSTM - Zeros LSTM - Impute LSTM - Zeros & Indicators LSTM - Impute & Indicators LSTM - Indicators Only

0.8662 0.8600 0.8730 0.8689 0.8409

0.8133 0.8062 0.8250 0.8206 0.7834

0.2909 0.2967 0.3041 0.3027 0.2403

Models using Hand-Engineered Features

Log Reg HE Log Reg HE Indicators Log Reg HE Indicators Only MLP HE MLP HE Indicators MLP HE Indicators Only

0.8396 0.8472 0.8187 0.8599 0.8669 0.8371

0.7714 0.7752 0.7322 0.8052 0.8160 0.7682

0.2708 0.2841 0.2287 0.2953 0.2954 0.2351

Macro F1 0.0343 1.0
0.0999 0.1189 0.1082 0.1234 0.0889
0.1072 0.1245 0.1224 0.1296 0.0949
0.1557 0.1569 0.1656 0.1609 0.1291
0.1327 0.1376 0.1001 0.1556 0.1610 0.1179

P@10 0.0788 0.2281
0.1014 0.1038 0.1017 0.1045 0.0939
0.1048 0.1031 0.1067 0.1082 0.0947
0.1176 0.1159 0.1215 0.1196 0.1074
0.1118 0.1165 0.1020 0.1168 0.1202 0.1028

Table 1: Performance on aggregate metrics for logistic regression (Log Reg), MLP, and LSTM classiﬁers with and without imputation and missing data indicators.

5.1 Results
The best overall model by all metrics (micro AUC of 0.8730) is an LSTM with zeroimputation and missing data indicators. It outperforms both the strongest MLP baseline and LSTMs absent missing data indicators. For the LSTMs using either imputation strategy, adding the missing data indicators improves performance in all metrics. While all models improve with access to missing data indicators, this information confers less beneﬁt to the raw input linear baselines, consistent with theory discussed in subsection 4.2.
The results achieved by logistic regression with hand-engineered features indicates that our simple hand-engineered missing data features do a reasonably good job of capturing important information that neural networks are able to mine automatically. We also ﬁnd

that LSTMs (with or without indicators) appear to perform better with zero-ﬁlling than with with imputed values. Interestingly, this is not true for either baseline. It suggests that the LSTM may be learning to recognize missing values implicitly by recognizing a tight range about the value zero and inferring that this is a missing value. If this is true, perhaps imputation interferes with the LSTM’s ability to implicitly recognize missing values. Overall, the ability to implicitly infer missingness may have broader implications. It suggests that we might never completely hide this information from a suﬃciently powerful model.
6. Related Work
This work builds upon research relating to missing values and machine learning for medical informatics. The basic RNN methodology for phenotyping derives from Lipton et al. (2016), addressing a dataset and problem described by Che et al. (2015). The methods rely upon LSTM RNNs (Hochreiter and Schmidhuber, 1997; Gers et al., 2000) trained by backpropagation through time (Hinton et al., 2006; Werbos, 1988). A comprehensive perspective on the history and modern applications of RNNs is provided by Lipton et al. (2015), while Lipton et al. (2016) list many of the previous works that have applied neural networks to digital health data.
While a long and rich literature addresses pattern recognition with missing data (Cohen and Cohen, 1975; Allison, 2001), most of this literature addresses ﬁxed-length feature vectors (Garc´ıa-Laencina et al., 2010; Pigott, 2001). Indicator variables for missing data were ﬁrst proposed by Cohen and Cohen (1975), but we could not ﬁnd papers that combine missing data indicators with RNNs. Only a handful of papers address missing data in the context of RNNS. Bengio and Gingras (1996) demonstrate a scheme by which the RNN learns to ﬁll in the missing values such that the ﬁlled-in values minimize output error. In 2001, Parveen and Green (2001) built upon this method to improve automatic speech recognition. Barker et al. (2001) suggests using a mask of indicators in a scheme for weighting the contribution of reliable vs corrupted data in the ﬁnal prediction. Tresp and Briegel (1998) address missing values by combining an RNN with a linear state space model to handle uncertainty. This paper may be one of the ﬁrst to engineer explicit features of missingness patterns in order to improve discriminative performance. Also, to our knowledge, we are the ﬁrst to harness patterns of missing data to improve the classiﬁcation of critical care phenotypes.
7. Discussion
Data processing and discriminative learning have often been regarded as separate disciplines. Through this separation of concerns, the complementarity of missing data indicators and training RNNs for classiﬁcation has been overlooked. This paper proposes that patterns of missing values are an underutilized source of predictive power and that RNNs, unlike linear models, can eﬀectively mine this signal from sequences of indicator values. Our hypotheses are conﬁrmed by empirical evidence. Additionally, we introduce and conﬁrm the utility of a simple set of features, engineered from the sequence of missingness indicators, that can improve performance of linear models. These techniques are simple to implement and broadly applicable and seem likely to confer similar beneﬁts on other sequential prediction tasks, when data is missing not at random. One example might include ﬁnancial data, where failures to report accounting details could suggest internal problems at a company.

7.1 The Perils and Inevitability of Modeling Treatment Patterns
For medical applications, the predictive power of missing data raises important philosophical concerns. We train models with supervised learning, and verify their utility by assessing the accuracy of their classiﬁcations on hold-out test data. However, in practice, we hope to make treatment decisions based on these predictions, exposing a fundamental incongruity between the problem on which our models are trained and those for which they are ultimately deployed. As articulated in Lipton (2016), these supervised models, trained oﬄine, cannot account for changes that their deployment might confer upon the real world, possibly invalidating their predictions. Caruana et al. (2015) present a compelling case in which a pneumonia risk model predicted a lower risk of death for patients who also have asthma. The better outcomes of the asthma patients, as it turns out, owed to the more aggressive treatment they received. The model, if deployed, might be used to choose less aggressive treatment for the patients with both pneumonia and asthma, clearly a sub-optimal course of action.
On the other hand, to some degree, learning from treatment signal may be inevitable. Any imputation might leak some information about which values are likely imputed and which are not. Thus any suﬃciently powerful supervised model might catch on to some amount of missingness signal, as was the case in our experiments with the LSTM using zeroﬁlled missing values. Even physiologic measurements contain information owing to patterns of treatment, possibly reﬂecting the medications patients receive and the procedures they undergo.
Sometimes the patterns of treatments may be a reasonable and valuable source of information. Doctors already rely on this kind of signal habitually: they read through charts, noting which other doctors have seen a patient, inferring what their opinions might have been from which tests they ordered. While, in some circumstances, it may be problematic for learning models to rely on this signal, removing it entirely may be both diﬃcult and undesirable.
7.2 Complex Models or Complex Features?
Our work also shows that using only simple features, RNNs can achieve state of the art performance classifying clinical time series. The RNNs far outperform linear models. Still, in our experience, there is a strong bias among practitioners toward more familiar models even when they require substantial feature engineering.
In our experiments, we undertook extensive eﬀorts to engineer features to boost the performance of both linear models and MLPs. Ultimately, while RNNs performed best on raw data, we could approach its performance with an MLP and signiﬁcantly improve the linear model by using hand-engineered features and windowing. A question then emerges: how should we evaluate the trade-oﬀ between more complex models and more complex features? To the extent that linear models are believed to be more interpretable than neural networks, most popular notions of interpretability hinge upon the intelligibility of the features (Lipton, 2016). When performance of the linear model comes at the price of this intelligibility, we might ask if this trade-oﬀ undermines the linear model’s chief advantage. Additionally, such a model, while still inferior to the RNN, relies on applicationspeciﬁc features less likely to be useful on other datasets and tasks. In contrast, RNNs

seem better equipped to generalize to diﬀerent tasks. While the model may be complex, the inputs remain intelligible, opening the possibility to various post-hoc interpretations (Lipton, 2016).
7.3 Future Work
We see several promising next steps following this work. First, we would like to validate this methodology on tasks with more immediate clinical impact, such as predicting sepsis, mortality, or length of stay. Second, we’d like to extend this work towards predicting clinical decisions. Called policy imitation in the reinforcement literature, such work could pave the way to providing real-time decision support. Finally, we see machine learning as cooperating with a human decision-maker. Thus a machine learning model needn’t always make a prediction/classiﬁcation; it could also abstain. We hope to make use of the latest advances in mining uncertainty information from neural networks to make conﬁdence-rated predictions.
8. Acknowledgments
Zachary C. Lipton was supported by the Division of Biomedical Informatics at the University of California, San Diego, via training grant (T15LM011271) from the NIH/NLM. David Kale was supported by the Alfred E. Mann Innovation in Engineering Doctoral Fellowship. The VPICU was supported by grants from the Laura P. and Leland K. Whittier Foundation. We acknowledge NVIDIA Corporation for Tesla K40 GPU hardware donation and Professors Charles Elkan and Greg Ver Steeg for their support and advice.
References
Paul D Allison. Missing data. 2001.
Jon Barker, Phil Green, and Martin Cooke. Linking auditory scene analysis and robust asr by missing data techniques. 2001.
Yoshua Bengio and Francois Gingras. Recurrent neural networks for missing or asynchronous data. 1996.
Yoshua Bengio, Patrice Simard, and Paolo Frasconi. Learning long-term dependencies with gradient descent is diﬃcult. Neural Networks, IEEE Transactions on, 1994.
Rich Caruana, Yin Lou, Johannes Gehrke, Paul Koch, Marc Sturm, and Noemie Elhadad. Intelligible models for healthcare: Predicting pneumonia risk and hospital 30-day readmission. In KDD, 2015.
Zhengping Che, David C. Kale, Wenzhe Li, Mohammad Taha Bahadori, and Yan Liu. Deep computational phenotyping. In KDD. ACM, 2015.
Jacob Cohen and Patricia Cohen. Applied multiple regression/correlation analysis for the behavioral sciences. 1975.

Susannah Fleming, Matthew Thompson, Richard Stevens, Carl Heneghan, Annette Plu¨ddemann, Ian Maconochie, Lionel Tarassenko, and David Mant. Normal ranges of heart rate and respiratory rate in children from birth to 18 years: A systematic review of observational studies. The Lancet, 2011.
Pedro J Garc´ıa-Laencina, Jos´e-Luis Sancho-G´omez, and An´ıbal R Figueiras-Vidal. Pattern classiﬁcation with missing data: a review. Neural Computing and Applications, 2010.
Felix A. Gers, Ju¨rgen Schmidhuber, and Fred Cummins. Learning to forget: Continual prediction with LSTM. Neural Computation, 2000.
Felix A. Gers, Nicol N. Schraudolph, and Ju¨rgen Schmidhuber. Learning precise timing with LSTM recurrent networks. JMLR, 2003.
Katharine E Henry, David N Hager, Peter J Pronovost, and Suchi Saria. A targeted realtime early warning score (trewscore) for septic shock. Science Translational Medicine, 2015.
Geoﬀrey E. Hinton, Simon Osindero, and Yee Whye Teh. A fast learning algorithm for deep belief nets. Neural Computation, 2006.
Sepp Hochreiter and Ju¨rgen Schmidhuber. Long short-term memory. Neural Computation, 1997.
Thomas A. Lasko, Joshua C. Denny, and Mia A. Levy. Computational phenotype discovery using unsupervised feature learning over noisy, sparse, and irregular clinical data. PLoS ONE, 2013.
Zachary C Lipton. The mythos of model interpretability. ICML Workshop on Human Interpretability in Machine Learning, 2016.
Zachary C Lipton, Charles Elkan, and Balakrishnan Naryanaswamy. Optimal thresholding of classiﬁers to maximize F1 measure. In Machine Learning and Knowledge Discovery in Databases. 2014.
Zachary C. Lipton, John Berkowitz, and Charles Elkan. A critical review of recurrent neural networks for sequence learning. arXiv:1506.00019, 2015.
Zachary C Lipton, David C Kale, Charles Elkan, and Randall Wetzell. Learning to diagnose with lstm recurrent neural networks. ICLR, 2016.
Ben M. Marlin, David C. Kale, Robinder G. Khemani, and Randall C. Wetzel. Unsupervised pattern discovery in electronic health care data using probabilistic clustering models. In Proceedings of the 2nd ACM SIGHIT International Health Informatics Symposium, 2012.
National High Blood Pressure Education Program Working Group on Children and Adolescents. The fourth report on the diagnosis, evaluation, and treatment of high blood pressure in children and adolescents. Pediatrics, 2004.
Anika Oellrich et al. The digital revolution in phenotyping. Brieﬁngs in Bioinformatics, 2015.

Shahla Parveen and P Green. Speech recognition with missing data using recurrent neural nets. In NIPS, 2001.
Therese D Pigott. A review of methods for missing data. Educational research and evaluation, 2001.
Volker Tresp and Thomas Briegel. A solution for missing data in recurrent neural networks with an application to blood glucose prediction. In NIPS. 1998.
Paul J Werbos. Generalization of backpropagation with application to a recurrent gas market model. Neural Networks, 1988.
Jenna Wiens, Eric Horvitz, and John V Guttag. Patient risk stratiﬁcation for hospitalassociated c. diﬀ as a time-series classiﬁcation task. In NIPS, 2012.
World Health Organization. International statistical classiﬁcation of diseases and related health problems. World Health Organization, 2004.

Appendix A. Per Diagnosis Classiﬁcation Performance
In this appendix, we provide per-diagnosis AUC and F1 scores for three representative LSTM models trained with imputed measurements, with imputation plus missing indicators, and with indicators only. By comparing performance on individual diagnoses, we can gain some insight into the relationship between missing values and diﬀerent conditions. Rows are sorted in descending order based on the F1 score of the imputation plus indicators model. It is worth noting that F1 scores are sensitive to threshold, which we chose in order to optimize per-disease validation F1, sometimes based on a very small number of positive cases. Thus, there are cases where one model will have superior AUC but worse F1.

Classiﬁer Performance on Each Diagnostic Code, Sorted by F1

Condition

Base rate

Msmt.

AUC

F1

Msmt. + indic.

AUC

F1

Diabetes mellitus with ketoacidosis Asthma with status asthmaticus Scoliosis (idiopathic) Tumor, cerebral Renal transplant, status post Liver transplant, status post Acute respiratory distress syndrome Developmental delay Diabetes insipidus End stage renal disease (on dialysis) Seizure disorder Acute respiratory failure Cystic ﬁbrosis Septic shock Respiratory distress, other Intracranial injury, closed Arteriovenous malformation Seizures, status epilepticus Pneumonia due to adenovirus Leukemia (acute, without remission) Dissem. intravascular coagulopathy Septicemia, other Bronchiolitis Congestive heart failure Upper airway obstruc. (UAO), other Diabetes mellitus type I, stable Cerebral palsy (infantile) Coagulopathy UAO, ENT surgery, post-status Hypertension, systemic Acute renal failure, unspeciﬁed Trauma, vehicular Hepatic fail. (acute necrosis of liver) Craniosynostosis (anomalies of skull) Prematurity (<37 weeks gestation) Hydrocephalus, other (congenital) Pneumothorax Congenital muscular dystrophy Cardiomyopathy (primary) Pulmonary edema

0.0125 0.0202 0.1419 0.0917 0.0122 0.0106 0.0193 0.0876 0.0127 0.0241 0.0816 0.0981 0.0076 0.0316 0.0716 0.0525 0.0223 0.0348 0.0123 0.0287 0.0099 0.0240 0.0162 0.0133 0.0378 0.0064 0.0262 0.0131 0.0302 0.0169 0.0191 0.0308 0.0176 0.0064 0.0321 0.0381 0.0134 0.0121 0.0191 0.0076

1.0000 0.9384 0.9143 0.8827 0.9667 0.7534 0.9696 0.8108 0.9220 0.8548 0.7610 0.8414 0.8628 0.8296 0.8411 0.8886 0.8620 0.8381 0.8604 0.8585 0.9556 0.8586 0.9513 0.8748 0.8206 0.7105 0.8230 0.7501 0.9059 0.8740 0.9242 0.8673 0.8489 0.7824 0.7520 0.7118 0.8220 0.8427 0.7508 0.8839

0.8889 0.6800 0.6566 0.5636 0.2963 0.3158 0.3590 0.4382 0.2727 0.2778 0.3694 0.4128 0.2353 0.3363 0.3873 0.2817 0.3590 0.4158 0.1250 0.2783 0.5000 0.2400 0.2667 0.1429 0.2564 0.0000 0.2609 0.1111 0.4058 0.2105 0.2381 0.2105 0.2222 0.0000 0.1548 0.2099 0.1176 0.2500 0.1290 0.0769

0.9999 0.8907 0.8970 0.8799 0.9544 0.8283 0.9705 0.8382 0.9486 0.8800 0.7937 0.8391 0.8740 0.8911 0.8502 0.9002 0.8716 0.8505 0.9065 0.8845 0.9532 0.8870 0.9395 0.8756 0.8573 0.9625 0.8359 0.8098 0.8733 0.8831 0.9510 0.8649 0.9239 0.9267 0.7542 0.7500 0.7957 0.8491 0.6057 0.8385

0.9333 0.6383 0.6174 0.5560 0.4762 0.4762 0.4557 0.4331 0.4286 0.4186 0.4059 0.3835 0.3810 0.3750 0.3719 0.3711 0.3704 0.3704 0.3077 0.3059 0.2857 0.2812 0.2703 0.2703 0.2542 0.2500 0.2500 0.2449 0.2400 0.2388 0.2381 0.2381 0.2308 0.2286 0.2245 0.2241 0.2188 0.2143 0.2143 0.2105

Indic.

AUC

F1

0.9906 0.8652 0.8435 0.8312 0.9490 0.8271 0.9361 0.6912 0.9266 0.9043 0.6431 0.8358 0.8189 0.8506 0.7857 0.8442 0.8494 0.8440 0.8792 0.8551 0.9555 0.7593 0.8826 0.8326 0.8350 0.9356 0.6773 0.8548 0.8364 0.8216 0.9507 0.8022 0.8598 0.8443 0.7042 0.7065 0.7552 0.7460 0.6372 0.8071

0.7059 0.5417 0.5235 0.4627 0.5600 0.2581 0.3333 0.2366 0.4000 0.4255 0.1957 0.4542 0.0000 0.1429 0.2143 0.3208 0.2857 0.3226 0.1818 0.2703 0.2500 0.0000 0.1778 0.1364 0.1964 0.3333 0.0980 0.1667 0.1975 0.2857 0.3291 0.1395 0.1935 0.0315 0.1266 0.1961 0.3243 0.0800 0.1818 0.0870

Table 2: AUC and F1 scores for individual diagnostic codes.

Classiﬁer Performance on Each Diagnostic Code, Sorted by F1

Condition

Base rate

Msmt.

AUC

F1

Msmt. + indic.

AUC

F1

Indic.

AUC

F1

(Acute) pancreatitis Tumor, disseminated or metastatic Hematoma, intracranial Neutropenia (agranulocytosis) Arrhythmia, other Child abuse, suspected Encephalopathy, hypoxic/ischemic Epidural hematoma Tumor, gastrointestinal Craniofacial malformation Gastroesophageal reﬂux Pneumonia, bacterial (pneumococ.) Pneumonia, undetermined Cerebral edema Pneumonia due to inhalation Metabolic or endocrine disorder Disorder of kidney and ureter, other Urinary tract infection Subdural hematoma Near drowning Cardiac arrest, outside hospital Pleural eﬀusion Bronchopulmonary dysplasia Hyponatremia Suspected septicemia, rule out Thrombocytopenia (Benign) intracranial hypertension Pericardial eﬀusion Pulmonary contusion Surgery, gastrointestinal Respiratory Arrest Trauma, abdominal Atrial septal defect Genetic abnormality Arrhythmia, ventricular Hematologic disorder, other Asthma, stable Neuroﬁbromatosis Tumor, bone Shock, hypovolemic Gastrointestinal bleed, other

0.0106 0.0180 0.0299 0.0108 0.0087 0.0065 0.0116 0.0098 0.0100 0.0133 0.0182 0.0186 0.0179 0.0059 0.0078 0.0095 0.0204 0.0137 0.0147 0.0068 0.0118 0.0113 0.0252 0.0056 0.0143 0.0112 0.0099 0.0055 0.0068 0.0104 0.0062 0.0105 0.0107 0.0557 0.0062 0.0114 0.0171 0.0079 0.0090 0.0088 0.0064

0.8712 0.7178 0.7724 0.8285 0.8536 0.9544 0.8242 0.7389 0.8112 0.8707 0.7571 0.8876 0.8323 0.8275 0.7917 0.7718 0.8486 0.7478 0.8270 0.8296 0.8932 0.8549 0.8309 0.5707 0.7378 0.7381 0.8494 0.8997 0.9029 0.6705 0.8404 0.7426 0.7766 0.6629 0.8532 0.6736 0.7010 0.8022 0.8830 0.7703 0.8325

0.0769 0.0938 0.2278 0.0000 0.0000 0.2222 0.1429 0.0455 0.1429 0.2667 0.1818 0.1333 0.1481 0.0000 0.1111 0.0364 0.2857 0.1154 0.1449 0.0741 0.0976 0.1081 0.1915 0.0187 0.0923 0.0822 0.0000 0.0870 0.0606 0.0714 0.0000 0.1667 0.0727 0.1324 0.0303 0.0800 0.0925 0.0469 0.0727 0.0000 0.0541

0.9512 0.7415 0.8249 0.8114 0.8977 0.8642 0.8571 0.8233 0.8636 0.8514 0.8554 0.8806 0.8269 0.9469 0.8602 0.6929 0.8650 0.7402 0.8884 0.7917 0.8791 0.8186 0.7952 0.7398 0.7402 0.7857 0.9018 0.9085 0.8831 0.6666 0.8741 0.8623 0.7765 0.6470 0.8703 0.6898 0.6607 0.7984 0.8174 0.8433 0.7974

0.2000 0.1967 0.1892 0.1852 0.1818 0.1818 0.1818 0.1818 0.1778 0.1778 0.1690 0.1600 0.1583 0.1538 0.1538 0.1538 0.1500 0.1481 0.1429 0.1404 0.1379 0.1351 0.1304 0.1176 0.1029 0.1026 0.1020 0.1017 0.0984 0.0976 0.0952 0.0930 0.0909 0.0876 0.0870 0.0870 0.0870 0.0816 0.0800 0.0741 0.0741

0.8182 0.6837 0.7518 0.8335 0.8654 0.8227 0.8009 0.7936 0.8732 0.6928 0.7739 0.8616 0.7772 0.9195 0.8268 0.6319 0.8238 0.7229 0.8190 0.6897 0.8881 0.7605 0.8503 0.8775 0.6769 0.8585 0.8586 0.9000 0.8197 0.5545 0.8127 0.6991 0.7197 0.5705 0.8182 0.8074 0.5907 0.7388 0.7649 0.8040 0.7996

0.0571 0.1062 0.1474 0.2609 0.0000 0.0870 0.0800 0.1000 0.0984 0.2286 0.1600 0.0000 0.0947 0.1500 0.0566 0.2000 0.2500 0.0588 0.0476 0.0397 0.0556 0.1151 0.1203 0.0000 0.0000 0.0800 0.1224 0.0714 0.0225 0.0233 0.0444 0.0426 0.0000 0.1165 0.1250 0.0800 0.0741 0.0160 0.0417 0.0000 0.0909

Classiﬁer Performance on Each Diagnostic Code, Sorted by F1

Condition

Base rate

Msmt.

AUC

F1

Msmt. + indic.

AUC

F1

Chromosomal abnormality Encephalopathy, other Respiratory syncytial virus (Hereditary) hemolytic anemia, other Obstructive sleep apnea Apnea, central Neuromuscular, other Anemia, acquired Meningitis, bacterial Trauma, long bone injury Bowel (intestinal) obstruction Neurologic disorder, other Panhypopituitarism Thyroid dysfunction Coma Spinal cord lesion Pneumonia, other (mycoplasma) Trauma, blunt Surgery, thoracic Neuroblastoma Obesity Obstructed ventriculoperitoneal shunt Ventricular septal defect Croup Syndrome, UAO Sickle-cell anemia, unspeciﬁed Biliary atresia Metabolic acidosis (¡7.1) Immunologic disorder, other Pulmonary hypertension, other Trauma, chest Spinal muscular atrophy Trauma, unspeciﬁed Bone marrow transplant, status post Surgery, orthopaedic Gastrointestinal bleed, upper Arrhythmia, supraventricular tachy. Congenital central alveolar hypovent. Tetralogy of fallot Cardiac disorder, other Hydrocephalus, shunt failure Cerebral infarction (CVA) Congenital heart disorder, other Gastrointestinal disorder, other Aspiration Dehydration Tumor, thoracic UAO, extubation, status post

0.0173 0.0093 0.0130 0.0088 0.0185 0.0142 0.0132 0.0056 0.0070 0.0096 0.0104 0.0288 0.0057 0.0072 0.0056 0.0133 0.0188 0.0065 0.0058 0.0059 0.0098 0.0073 0.0119 0.0069 0.0080 0.0063 0.0083 0.0094 0.0112 0.0051 0.0052 0.0065 0.0097 0.0180 0.0063 0.0055 0.0057 0.0061 0.0071 0.0083 0.0058 0.0084 0.0139 0.0072 0.0105 0.0077 0.0085

0.8047 0.8265 0.8876 0.7582 0.7564 0.7871 0.7163 0.7378 0.4431 0.8757 0.7512 0.7628 0.7763 0.6310 0.6483 0.7298 0.8589 0.9156 0.7405 0.6526 0.7503 0.6824 0.6641 0.9418 0.6262 0.9383 0.9475 0.9539 0.9259 0.9261 0.9666 0.7153 0.8161 0.7839 0.8388 0.8178 0.7067 0.5759 0.7229 0.7715 0.6766 0.7590 0.6755 0.6727 0.7356 0.6931 0.8295

0.1034 0.1250 0.2857 0.0548 0.0613 0.1600 0.0452 0.1017 0.0000 0.0952 0.0984 0.1481 0.0000 0.0369 0.1250 0.0585 0.1613 0.0513 0.0000 0.0306 0.0365 0.0267 0.1081 0.2222 0.0000 0.2667 0.1818 0.1500 0.2500 0.0000 0.0000 0.1481 0.5217 0.1029 0.0000 0.0385 0.0000 0.0000 0.0519 0.0000 0.0000 0.0000 0.0336 0.0533 0.0690 0.0513 0.0672

0.7197 0.8736 0.9145 0.8544 0.8200 0.8134 0.7069 0.7596 0.7676 0.9085 0.6559 0.6978 0.7724 0.6420 0.6823 0.7052 0.8792 0.8138 0.6948 0.7268 0.6814 0.7114 0.5680 0.9834 0.9627 0.9164 0.9046 0.8868 0.8826 0.8818 0.8658 0.8657 0.8562 0.8192 0.8078 0.7867 0.7716 0.7614 0.7552 0.7542 0.7495 0.7277 0.6821 0.6734 0.6636 0.6249 0.6063

0.0714 0.0688 0.0645 0.0645 0.0645 0.0625 0.0619 0.0615 0.0606 0.0597 0.0597 0.0588 0.0571 0.0541 0.0513 0.0488 0.0476 0.0469 0.0469 0.0360 0.0351 0.0331 0.0294 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000

Indic.

AUC

F1

0.6300 0.8335 0.8716 0.9125 0.8087 0.8051 0.6484 0.8129 0.5480 0.7946 0.6936 0.5971 0.6415 0.6661 0.7155 0.8168 0.8424 0.7426 0.6087 0.7775 0.6647 0.7516 0.5593 0.9682 0.8661 0.7589 0.9143 0.8969 0.8098 0.7820 0.8362 0.8224 0.8505 0.7331 0.7256 0.8199 0.7282 0.7637 0.6287 0.7986 0.7148 0.7803 0.6465 0.6792 0.5899 0.6815 0.6128

0.1600 0.1250 0.0930 0.0513 0.1111 0.0000 0.0392 0.0519 0.0000 0.1176 0.0424 0.0769 0.0000 0.0000 0.0000 0.0414 0.1164 0.0177 0.0909 0.0346 0.0667 0.0667 0.0444 0.2222 0.1250 0.0714 0.1538 0.1212 0.0000 0.0000 0.0000 0.0594 0.1695 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0635 0.1333 0.0583 0.1026 0.0333 0.0000 0.0292 0.0000

Appendix B. Missing
In this appendix, we present information about the sampling rates and missingness characteristics of our 13 variables. The ﬁrst column lists the average number of measurements per hour in all episodes with at least one measurement (excluding episodes where the variable is missing entirely). The second column lists the fraction of episodes in which the variable is missing completely (there are zero measurements). The third column lists the missing rate in the resulting discretized sequences.

Variable
Diabstolic blood pressure Systolic blood pressure Peripheral capillary refall rate End-tidal CO2 Fraction inspired O2 Total glasgow coma scale Glucose Heart rate pH Respiratory rate Pulse oximetry Temperature Urine output

Msmt./hour
0.5162 0.5158 1.0419 0.9318 1.3004 1.0394 1.4359 0.2477 1.4580 0.2523 0.1937 1.0210 1.1160

Missing entirely
0.0135 0.0135 0.0140 0.5710 0.1545 0.0149 0.1323 0.0133 0.3053 0.0147 0.0022 0.0137 0.0353

Frac. missing
0.1571 0.1569 0.5250 0.5727 0.7873 0.5250 0.9265 0.0329 0.9384 0.0465 0.0326 0.5235 0.5980

Table 3: Sampling rates and missingness statistics for all 13 features.

