STraTA: Self-Training with Task Augmentation for Better Few-shot Learning

Tu Vu1,2

, Minh-Thang Luong1, Quoc V. Le1, Grady Simon1, Mohit Iyyer2 Google Research1
University of Massachusetts Amherst2
{ttvu,thangluong,qvl,gradys}@google.com
{tuvu,miyyer}@cs.umass.edu

arXiv:2109.06270v2 [cs.CL] 12 Apr 2022 Accuracy

Abstract

Despite their recent successes in tackling many NLP tasks, large-scale pre-trained language models do not perform as well in few-shot settings where only a handful of training examples are available. To address this shortcoming, we propose STRATA, which stands for Self-Training with Task Augmentation, an approach that builds on two key ideas for effective leverage of unlabeled data. First, STRATA uses task augmentation, a novel technique that synthesizes a large amount of data for auxiliary-task ﬁne-tuning from target-task unlabeled texts. Second, STRATA performs selftraining by further ﬁne-tuning the strong base model created by task augmentation on a broad distribution of pseudo-labeled data. Our experiments demonstrate that STRATA can substantially improve sample efﬁciency across 12 fewshot benchmarks. Remarkably, on the SST-2 sentiment dataset, STRATA, with only 8 training examples per class, achieves comparable results to standard ﬁne-tuning with 67K training examples. Our analyses reveal that task augmentation and self-training are both complementary and independently effective.
1 Introduction
Recent advances in NLP demonstrate the effectiveness of applying large-scale pretrained language models to downstream tasks (Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019; Lan et al., 2020; Raffel et al., 2020; Brown et al., 2020; He et al., 2021a). While these models have achieved state-of-the-art results on many NLP benchmarks, they struggle when given limited training data. For instance, Devlin et al. (2019) ﬁnd that BERT is prone to degenerate performance on small datasets. While enormous language models like GPT-3 (Brown et al., 2020) exhibit the ability to solve a new task from only a few examples without
Work done as a student researcher at Google Brain.

# labeled examples per class
Figure 1: Our Self-Training with Task Augmentation (STRATA) approach substantially improves sample efﬁciency across different tasks. For example, when given only 8 labeled examples per class from the SST-2 sentiment dataset, STRATA is competitive with standard ﬁnetuning on 67K examples; on the SCITAIL entailment dataset, with 512 labeled examples per class, STRATA surpasses standard ﬁne-tuning on 27K examples.
any ﬁne-tuning, their performance still lags far behind state-of-the-art ﬁne-tuning results. Manually annotating large amounts of training data will likely improve performance but can also be prohibitively expensive to obtain for many tasks and domains. In this paper, we propose STRATA, an approach that combines two complementary methods, SelfTraining and Task Augmentation, to effectively leverage unlabeled data, which is comparatively cheaper to obtain.1
At a high level, task augmentation exploits unlabeled texts from the domain of a given target task to simulate a large amount of in-domain training data for the auxiliary task of natural language inference (NLI), which is then used to train a given model before applying it to the target task. To achieve this, we ﬁrst build an NLI data generator by ﬁne-tuning a pre-trained generative language model on the MNLI dataset (Williams et al., 2018) in a text-to-text format. Then, given a target task (e.g., sentiment analysis) with unlabeled texts (e.g., his acting was re-
1Code and pre-trained models available at https://github.com/google-research/ google-research/tree/master/STraTA.

Task Augmentation

Pre-trained Language Model

Auxiliary-task Model

Task-speciﬁc Unlabeled Texts

Data Generation
Model

Synthetic In-domain Auxiliary-task
Data

Self-training

Teacher Model

Inference

Labeled Data

Pseudo-labeled Data
Use a broad distribution

Student Model

Repeat until convergence

Figure 2: An illustration of our Self-Training with Task Augmentation (STRATA) approach. In task augmentation, we train an NLI data generation model and use it to synthesize a large amount of in-domain NLI training data for each given target task, which is then used for auxiliary (intermediate) ﬁne-tuning. Our self-training algorithm iteratively learns a better model using a concatenation of labeled and pseudo-labeled examples. At each iteration, we always start with the auxiliary-task model produced by task augmentation and train on a broad distribution of pseudo-labeled data.

ally awful), we use the NLI data generator to generate NLI examples (e.g., [his acting was really awful, he gave an incredible performance, contradiction]). We show that task augmentation alone can significantly improve downstream performance across different tasks, generally outperforming other ﬁnetuning approaches, such as target-task language model ﬁne-tuning (Howard and Ruder, 2018; Gururangan et al., 2020) and intermediate-task ﬁnetuning on MNLI (Phang et al., 2019), in both highand low-data regimes.
Having obtained a strong auxiliary-task model with task augmentation, STRATA uses this model as a base model for self-training. Speciﬁcally, at each at iteration, the base model is ﬁne-tuned using the available labeled data for the target task. Then, the resulting model’s predictions on unlabeled examples2 are used as pseudo-labels to augment the original labeled data set. The newly formed labeled data set is then used to learn a better model in the next iteration, and this procedure is repeated for a number of iterations until a stopping criterion is reached. While self-training has been extensively studied (Rosenberg et al., 2005; McClosky et al., 2006; He et al., 2020; Xie et al., 2020b; Du et al., 2021), our experiments reveal that using a strong base model and training on a broad distribution of pseudo-labeled data are key factors for successful deployment in NLP.
Using our STRATA approach, we are able to sig-
2We use the term unlabeled texts to refer to pieces of text (e.g., sentences) from the target domain, and the term unlabeled examples to refer to examples that can be annotated using the set of class labels for the target task.

niﬁcantly improve sample efﬁciency, in terms of both performance and variance, across 12 NLP benchmark datasets. For instance, on the SST-2 sentiment dataset (Socher et al., 2013), with only 8 training examples per class, we achieve comparable results to standard ﬁne-tuning with 67K training examples (see Figure 1).
Our main contributions are as follows:
1. We propose task augmentation, a novel data augmentation-based ﬁne-tuning method, and show its effectiveness in comparison to other competing ﬁne-tuning approaches.
2. We propose a simple yet effective self-training algorithm and highlight important ingredients for successful self-training, which we hope will enable the wider adoption of self-training in NLP.
3. With STRATA, we demonstrate the effectiveness of combining task augmentation and selftraining in improving sample efﬁciency across NLP benchmarks.
2 Task augmentation
Labeled data is often expensive and timeconsuming to obtain, which motivates approaches that learn from both labeled and unlabeled data. More formally, assume we are given a target task T with a labeled data set LT = {(xi, yi)}M i=1 and an unlabeled data set U T = {(xj)}Nj=1. The unlabeled data U T can be created artiﬁcially by removing the ground-truth labels y from LT (as

in our main experiments), or it can come from additional unlabeled texts from the target domain or from related datasets/domains (see Section 5). Our methods, task augmentation and self-training, take advantage of the unlabeled data U T to maximize performance on the target task T , even when the number of labeled examples M is small (e.g., M = 16). In this section, we ﬁrst present a framework and implementation for task augmentation, which uses natural language inference (NLI) as an auxiliary (intermediate) training task to improve downstream performance.
2.1 A framework for task augmentation
Task augmentation builds on a recent body of NLP research on intermediate-task training (Phang et al., 2019; Vu et al., 2020), in which a pre-trained language model, such as BERT, is ﬁne-tuned on an auxiliary task before the target task.3 In previous work on intermediate ﬁne-tuning, the auxiliary dataset used is a ﬁxed target task-independent dataset, such as MNLI or SQUAD (Rajpurkar et al., 2016). An obvious limitation of this choice is the domain mismatch between the auxiliary and target tasks, which our proposed task augmentation method addresses. More speciﬁcally, we ﬁne-tune a pre-trained generative language model and use it to synthesize a large amount of in-domain training data from U T for an auxiliary task A, which is then used to improve performance of a model on the target task T (Figure 2, left).4 In this work, we choose NLI as the auxiliary task for two main reasons: (1) NLI has been shown to be an effective auxiliary task for a variety of target tasks (Conneau et al., 2017; Phang et al., 2019), and (2) existing NLI datasets contain large training sets, which allows us to train a reliable data generator.
Generating synthetic NLI data: To obtain an NLI data generator, we ﬁne-tune the pre-trained T53B model (Raffel et al., 2020) on MNLI, which contains 393K sentence pairs labeled as {entailment, contradiction, neutral}. We cast each MNLI training example (sentA, sentB) → label into a textto-text format (label, sentA) → sentB to obtain ﬁne-tuning examples that look like [entailment,
3This process differs from traditional data augmentation approaches (e.g., lexical substitution, or back-translation), which yield negligible improvements when combined with large-scale pre-trained language models (Wei and Zou, 2019; Yang et al., 2020).
4Traditional data augmentation is a special case of our framework where the auxiliary task is identical to the target task (A ≡ T ).

the facts are accessible to you → you have access to the facts].5 We ﬁne-tune T5 on this dataset with a constant learning rate of 0.001 for 216 = 65, 536 steps using the Adafactor optimizer (Shazeer and Stern, 2018). The ﬁne-tuned T5 data generator produces augmented examples for all target datasets. Speciﬁcally, at inference time, we feed the model an NLI label (e.g., entailment) and an unlabeled sentence xj from the target domain to produce some output sentence xk: (entailment, xj) → xk (see Appendix B for example outputs). Data for intermediate ﬁne-tuning is then formed by creating examples like (xj, xk) → entailment. This approach has several advantages: (1) training labels are free, and (2) by using overgeneration, we can produce a large amount of in-domain NLI training data even for target tasks with small datasets.
Overgeneration and ﬁltering: Following Puri et al. (2020), we perform overgeneration and ﬁltering to increase the quantity and quality of the synthetic NLI training data. Concretely, we generate 100 output samples per input with top-k (k = 40) sampling (duplicates are removed) and use a BERT model ﬁne-tuned on MNLI (in the original format) as an NLI classiﬁer to ﬁlter synthetic training examples. We keep a synthetic example if the NLI classiﬁer produces the same label as that fed to the NLI data generator and is also conﬁdent about its prediction.6 For all experiments, we perform intermediate ﬁne-tuning on examples from both the original MNLI dataset and the ﬁnal ﬁltered task augmentation dataset.7
3 Self-training
While task augmentation uses unlabeled texts to produce synthetic data for an intermediate task, self-training is a complementary approach that improves a model by training directly on the target task using pseudo-labeled examples. In this section, we explore a simple self-training algorithm in which a model learns to improve itself
5We ﬁne-tune a separate T5 model per class label. To overcome biases in MNLI where the hypotheses are usually shorter than the premises, we also include reversed examples: (reversed label, sentB) → sentA.
6We use an example when its predicted probability exceeding a certain threshold τ . We choose a value for τ in [0.3, 0.4, . . . , 0.9] for each target task based on performance on the original MNLI development set.
7A two-stage intermediate ﬁne-tuning procedure where the model is ﬁrst trained on the synthetic data before being ﬁne-tuned on the original data typically works better, and this is used in our experiments.

Algorithm 1: Our self-training algorithm
initialization
t=0
Form a base model f0, which is initialized with pre-trained parameters from a pretraining/intermediate ﬁne-tuning stage, and then learn a teacher model f1 by training f0 on the original labeled data set L.
repeat
t=t+1
1. Use the current teacher model ft to annotate (for t = 1) or re-annotate (for t > 1) all of the examples in U to obtain a set U of pseudo-labeled examples.
2. Add the whole set U of pseudo-labeled examples to the original labeled data set L to form a new labeled data set.
3. Learn a student model ft+1 by training the base model f0 on the current labeled data set and optionally ﬁne-tune it on L. The resulting student model ft+1 is used as a teacher for the next iteration.
until convergence or the maximum number of iterations is reached
using its predictions on unlabeled examples from a given target task. Our method differs from traditional self-training methods in that we leverage a strong base model and allow it to learn from all available pseudo-labeled examples at every iteration, regardless of model conﬁdence. Formally, given a target task T with a small labeled data set L = {(xi, yi)}M i=1 and an unlabeled data set U = {(xj)}Nj=1, where M N , we summarize our self-training algorithm in Algorithm 1.
Starting with a strong base model: An important ingredient in self-training algorithms is the base model f0. Successful self-training typically requires a good base model, which can provide a large proportion of “correct” predictions or pseudolabels on unlabeled examples; otherwise, errors can be propagated or magniﬁed in later stages of self-training. At each self-training iteration, we always start from the same base model f0, which is initialized with pre-trained parameters from a pretraining/intermediate ﬁne-tuning stage (e.g., the auxiliary task training stage in task augmentation),8 and then ﬁne-tune all of its parameters using the available labeled and pseudo-labeled data.9
8We ﬁnd empirically that starting from the base model f0 works better than from the model ft−1 obtained in the previous iteration.
9He et al. (2020) ﬁnd that further ﬁne-tuning the resulting model on the original labeled data set L improves machine

Self-training on a broad distribution of pseudolabeled data: Another important factor is the selection of pseudo-labeled examples at each selftraining iteration. Traditional self-training approaches usually select a small set of examples where the current teacher model ft is highly conﬁdent (e.g., the probability of the predicted class label is above a threshold) to add to the labeled data set at each iteration until the unlabeled data pool U is exhausted. This can be problematic as state-of-the-art language models like BERT are overconﬁdent and poorly calibrated (Jiang et al., 2021). In preliminary experiments, we tried several calibration methods, including temperature scaling (Guo et al., 2017), label smoothing (Müller et al., 2019), and conﬁdence penalties (Pereyra et al., 2017), but all of which failed to fully address this problem. Instead, we encourage learning from a “natural” broad distribution of pseudo-labeled data by adding the whole set U of pseudo-labeled examples to the original labeled data set L at each self-training iteration.10 At each iteration t > 1, we also re-annotate all of the examples in the original unlabeled data pool U with ft, as we expect ft is better than ft−1.
4 Experiments
We perform experiments across 12 different NLP datasets and three different data regimes (including a few-shot setting). Task augmentation consistently improves over prior ﬁne-tuning approaches in all three regimes, and the combination of self-training and task augmentation, STRATA, results in higher performance and lower variance than competing approaches when given only 8 labeled examples per class from each dataset.
4.1 Datasets & data regimes
The datasets used in our study (Table 1)11 come from two common language understanding benchmarks: GLUE (Wang et al., 2019b) and SENTEVAL (Conneau and Kiela, 2018). Due to restricted test set access for GLUE datasets, we held out a small subset of the training set for validation and
translation models. We use development set performance to decide whether or not to perform this ﬁne-tuning step for each dataset.
10We ﬁnd that removing examples with the lowestconﬁdence pseudo labels can be helpful for some tasks. One can use a development set, upon availability, to assess if this ﬁltering is necessary.
11Appendix A contains more details about characteristics and associated evaluation metrics for each dataset.

Task
text classiﬁcation/regression
SNLI (Bowman et al., 2015) MNLI (Williams et al., 2018) QQP (Iyer et al., 2017) QNLI (Wang et al., 2019b) SST-2 (Socher et al., 2013) SCITAIL (Khot et al., 2018) SST-5 (Socher et al., 2013) STS-B (Cer et al., 2017) SICK-E (Marelli et al., 2014) SICK-R (Marelli et al., 2014) CR (Hu and Liu, 2004) MRPC (Dolan and Brockett, 2005) RTE (Dagan et al., 2005, et seq.)

|Train|
570K 393K 364K 105K
67K 27K 8.5K
7K 4.5K 4.5K
4K 3.7K 2.5K

Table 1: Datasets used in our experiments.

report results on the original development set for each task. The training set without ground-truth labels is used as unlabeled data U T .
We consider three data regimes by varying the amount of labeled training data across the downstream tasks: FULL (all labeled training data), LIMITED (1024 random labeled training examples), and FEW-SHOT (8 random labeled training examples per class).12 Since ﬁne-tuning BERT can be unstable on small datasets (Devlin et al., 2019), we perform 10 random restarts where there are less than 10K training examples and report the mean and standard deviation.13 Since large development sets are impractical in low-resource settings (Oliver et al., 2018; Kann et al., 2019), we randomly sample 256 development examples for each task in the LIMITED and FEW-SHOT regimes. Additionally, in the FEW-SHOT regime, we experiment with a real-world scenario where there is no development set access.
4.2 Setup
As in Devlin et al. (2019), our input format for all tasks contains a [CLS] token followed by a single text segment or a concatenation of text segments (e.g., a premise-hypothesis pair) separated with a [SEP] token. We feed the ﬁnal [CLS] representation into a task-speciﬁc classiﬁcation layer and ﬁne-tune all the parameters end-to-end on the downstream tasks. For both ﬁne-tuning and selftraining, we perform early stopping based on development set performance. We use the Transformers library (Wolf et al., 2019) and its recommended
12For regression tasks, we partition the output interval [0, 5] into ﬁve bins and sample 8 examples from each bin.
13We resample examples for each restart.

hyperparameters for all experiments.14
4.3 Methods
We experiment with task augmentation (TA) and self-training (ST) individually, as well as the combined approach STRATA, which uses the auxiliarytask model from task augmentation as the base model for self-training. We compare our methods to the following baselines:
LMFT & ITFTMNLI: We compare our methods against commonly-used ﬁne-tuning approaches, including target-task language model ﬁne-tuning (LMFT; Howard and Ruder, 2018; Gururangan et al., 2020)—in which a model is ﬁrst trained with the language model objective on taskspeciﬁc unlabeled data before being ﬁne-tuned on the target task—and intermediate-task ﬁne-tuning on MNLI (ITFTMNLI; Phang et al., 2019)—which ﬁrst trains a model on MNLI before ﬁne-tuning it on the target task.
LM-BFF & EFL: We also include results from recent work on prompt-based (LM-BFF; Gao et al., 2021) and entailment-based (EFL; Wang et al., 2021) ﬁne-tuning,15 which has been shown to outperform the GPT-3-style “in-context learning” approach (Brown et al., 2020) for few-shot learning. These approaches do not assume access to taskspeciﬁc unlabeled data and are not directly comparable to our methods due to differences in model architecture and experimental settings.
SENTAUG-ST: Closely related to our work, Du et al. (2021) propose a sentence augmentation method that retrieves a large amount of “in-domain” data for a given task from a large bank of Web sentences. A base model trained on task-speciﬁc labeled data is applied to obtain pseudo-labels for the retrieved sentences, which are then added to the original labeled set to train a better model.
4.4 Results and Discussion
Table 2 shows the main results of our experiments with task augmentation and self-training. Below, we ﬁrst provide an overview of these results before analyzing them in more detail.
14While individual task performance can likely be further improved with more involved hyperparameter tuning, we standardize hyperparameters across tasks to cut down on computational expense. Our experiments were conducted on Google Cloud with 100% renewable energy.
15Results taken from Wang et al. (2021).

Model

SNLI QQP QNLI SST-2 SCITAIL SST-5 STS-B SICK-E SICK-R CR MRPC

BERTLARGE + LMFT + ITFTMNLI + TA

FULL

91.1 88.4 91.9 92.4 95.3

53.70.9 89.60.2 87.90.6

91.0 88.1 90.4 93.5 95.3

54.00.4 89.50.2 87.70.5

91.1 88.2 91.6 93.5 96.5

54.00.8 90.30.3 89.90.2

91.9 88.5 92.5 94.7 96.9

55.70.8 90.90.2 90.70.3

84.40.4 84.00.5 86.30.3 87.00.3

91.70.6 91.60.8 92.00.6 93.30.6

89.00.8 89.51.0 89.70.9 90.80.7

BERTLARGE + LMFT + ITFTMNLI + TA

77.40.6 75.81.5 85.20.4 87.30.3

74.11.0 71.60.5 74.00.5 75.70.5

81.70.9 80.52.0 83.50.5 85.00.5

LIMITED 89.80.6 88.90.8 90.00.8 91.70.7

(1024 total 90.90.7 87.72.3 92.11.1 92.31.1

training 49.11.3 49.23.1 49.41.2 51.41.0

examples) 88.20.4 88.40.4 87.80.8 89.00.6

84.80.7 83.20.6 88.80.5 89.40.4

80.20.4 78.50.6 83.20.7 84.30.4

91.20.6 90.90.7 91.30.7 92.60.6

85.71.7 84.91.1 86.40.9 88.00.8

BERTBASE + LMFT + ITFTMNLI + TA + ST + ITFTMNLI + ST + STRATA

43.72.2 45.23.9 75.25.7 83.30.8 65.05.8 83.20.3 85.70.2

55.96.5 57.26.2 63.77.0 68.71.5 69.95.9 70.75.9 74.50.4

59.010.9 57.69.1 62.85.1 70.13.4 71.611.3 81.51.2 82.10.5

FEW-SHOT (8 training examples per class)

59.18.4 67.16.6

30.52.0 73.64.5 61.34.1

64.98.7 64.08.0

33.41.9 75.44.4 59.34.0

76.87.2 75.85.6

35.02.6 80.21.1 80.41.9

80.36.6 78.53.2

37.43.0 80.71.5 81.12.4

62.710.4 68.68.3

33.93.5 80.52.2 68.14.5

88.02.1 83.74.4

39.52.0 84.20.8 81.82.6

90.10.8 86.33.5

41.31.5 84.70.5 84.91.2

59.72.7 58.32.0 73.52.7 75.91.8 64.02.4 75.82.2 77.61.6

65.28.2 72.46.0 79.23.6 86.52.2 78.26.3 85.62.3 90.50.8

72.410.2 73.98.6 74.38.0 74.56.5 80.51.8 80.61.2 81.00.8

BERTLARGE + LMFT + ITFTMNLI + TA + ST + ITFTMNLI + ST + STRATA

43.14.4 39.62.6 79.93.1 84.80.7 69.39.2 85.40.3 87.30.3

58.54.7 52.74.7 62.69.0 64.66.3 74.31.2 74.80.7 75.10.2

64.46.1 52.21.6 64.54.4 71.54.0 85.41.7 86.11.1 86.40.8

66.18.7 66.39.3 80.75.0 85.51.4 81.912.2 89.70.7 91.70.7

68.89.5 66.410.6 72.311.2 79.04.5 79.94.8 86.24.2 87.32.9

35.21.3 36.82.9 36.42.1 38.53.0 42.01.5 42.22.0 43.02.3

74.63.8 75.49.4 75.54.0 78.92.4 82.82.3 84.11.7 84.51.6

66.54.5 58.86.9 77.83.8 81.23.9 77.33.1 84.32.0 86.31.8

66.63.3 51.67.0 73.52.8 77.51.4 73.12.3 78.41.3 79.01.0

72.06.0 75.65.9 82.63.0 88.61.3 88.11.3 89.31.0 90.00.6

79.92.0 80.52.4 72.87.9 78.26.6 81.20.5 81.41.2 81.50.7

Prompt-based (LM-BFF; Gao et al., 2021) and entailment-based (EFL; Wang et al., 2021) ﬁne-tuning approaches

ROBERTALARGE

38.41.3 58.89.9 52.71.8 60.53.1 –

–

24.58.4 –

–

61.95.1 76.13.9

+ LM-BFF

52.01.7 68.21.2 61.83.2 79.96.0 –

–

66.03.2 –

–

88.62.3 78.52.3

+ EFL

81.01.1 67.32.6 68.03.4 90.81.0 –

–

71.01.3 –

–

92.30.4 76.21.3

RTE
68.67.2 66.57.3 82.31.4 83.81.1
66.82.7 65.23.4 81.11.3 82.91.8
51.42.5 50.93.9 62.213.5 67.67.1 50.73.1 62.512.0 70.62.4
53.13.3 52.84.8 69.714.6 77.06.3 53.94.3 72.75.4 77.15.4
55.01.3 63.32.1 85.80.9

Table 2: STRATA signiﬁcantly improves results across 12 NLP benchmark datasets (numbers in the subscript indicate the standard deviation across 10 random seeds). See Appendix C for full results.

Baselines: LMFT is not always helpful and can even hurt performance (e.g., on QNLI, a task built from Wikipedia, which is part of BERT’s pretraining data). Du et al. (2021) also observe a decrease in performance when using LMFT with task-speciﬁc in-domain unlabeled data retrieved from Web data. ITFTMNLI signiﬁcantly outperforms LMFT in many cases, particularly on target tasks closely related to MNLI.
Task augmentation signiﬁcantly improves results on downstream tasks: The ﬁrst three blocks of Table 2 show the results for TA, which improves almost all target tasks across all three data regimes. TA even improves results on SNLI in the FULL regime, where there is a large amount of labeled data available (570K examples). Changing the data regimes signiﬁcantly impacts the average absolute performance gain over the vanilla BERTLARGE across target tasks, which is lowest in the FULL regime (+2.7%) and highest in the FEWSHOT regime (+13.0%). SNLI (+41.7%) and RTE (+23.9%) beneﬁt the most from TA in the FEW-SHOT regime. TA also signiﬁcantly outperforms both LMFT and ITFTMNLI, particularly in the low-data

regimes (+16.4% and +4.8%, respectively).
Adding self-training further boosts downstream performance when task-speciﬁc unlabeled examples are available: The third block of Table 2 shows that in the FEW-SHOT regime, adding ST to TA, which results in STRATA, further boosts downstream performance. In particular, STRATA performs the best across target tasks, achieving up to +44.2% absolute improvement on SNLI over . BERTLARGE Overall, STRATA provides an average absolute performance gain of +20.9% and +18.4% for BERTBASE and , BERTLARGE respectively. Using ST alone also leads to large improvements over the vanilla BERT models; however, the performance gain largely depends on the target task.
Using a stronger base model leads to better selftraining results: Our experiment results show that self-training is complementary to different BERT models across target tasks—the stronger the BERT base model, the better self-training results. BERT + TA yields better self-training results than BERT + ITFTMNLI, and both are better than the vanilla BERT. Combinations of BERTLARGE and

Model

SST-2 SST-5 CR

Ours (8 examples per class)

BERTBASE

69.86.5

+ TA

85.50.6

+ ST

74.99.0

+ STRATA

90.80.6

BERTLARGE + TA + ST + STRATA

75.63.3 87.30.3 90.60.3 92.40.1

32.82.0 41.00.8 38.30.8 43.11.1
36.60.4 41.71.1 43.80.4 45.50.7

73.10.5 88.70.2 85.61.8 91.40.2
79.30.7 90.00.4 89.01.1 90.60.0

Du et al. (2021) (20 examples per class) ROBERTALARGE 83.62.7 42.31.6 88.91.7
+ SENTAUG-ST 86.72.3 44.41.0 89.72.0

Table 3: Compared to Du et al. (2021), our approach leads to better downstream performance, despite using a weaker base model (BERT vs. ROBERTA) and with less labeled examples.

ST typically outperform that of BERTBASE and ST. Interestingly, BERTLARGE + ST is competitive with BERTLARGE + STRATA on several tasks (e.g., QQP and QNLI), and this does not hold for BERTBASE.
Comparison to recent published work: The last three rows of Table 2 and the last two rows of Table 3 show results from recent published work.16 Broadly, our methods lead to better performance compared to these approaches. However, due to differences in evaluation methodology (e.g., models, training/development data subsets, number of random restarts, and other factors), we refrain from explicitly ranking the approaches.
5 Analysis of few-shot learning results
Having established the effectiveness of both task augmentation and self-training in the few-shot setting, we conduct a series of analysis experiments in this section to explore the source of the observed improvements.
Sample efﬁciency with STRATA: Figure 1 illustrates how our STRATA approach improves sample efﬁciency as the number of examples per class increases. For the SST-2 sentiment dataset, despite using only K = 8 training examples per class, STRATA has already nearly saturated its performance, achieving results competitive with standard ﬁne-tuning over the whole dataset of 67K
16While Wang et al. (2021) report results for LM-BFF and EFL across 5 random data subsets using a ﬁxed set of seeds, Du et al. (2021) tried 10 seeds for each of their 5 random data subsets and report the mean of the top 3 seeds. To be more comparable to (Du et al., 2021), we report the mean of our top 3 random seeds in Table 3.

Model
RANDBASE + STRATA
BERTBASE + STRATA
BERTLARGE + STRATA

SST-2
50.01.6 78.60.9
59.18.4 90.10.8
66.18.7 91.70.7

SCITAIL
50.72.4 64.43.1
67.16.6 86.33.5
68.89.5 87.32.9

Table 4: Our approach yields improvements even when starting with a randomly-initialized model, but pretraining helps considerably.

labeled examples. On the harder task of SCITAIL, STRATA continues to improve as K increases, and surpasses the performance of standard ﬁne-tuning with the whole dataset of 27K labeled examples at K = 512.
STRATA improves a randomly-initialized base model: Table 4 shows that our STRATA approach does not require a powerful pre-trained base model to exhibit improvements: when applied to a randomly initialized Transfomer model (RANDBASE) with the same architecture as BERTBASE, RANDBASE + STRATA outperforms the vanilla BERTBASE by a large margin on SST-2, while being competitive on SCITAIL. Additionally, BERTBASE + STRATA substantially outperforms the vanilla BERTLARGE by 24% and 17.5% on SST-2 and SCITAIL, respectively.
Self-training on a broad distribution of pseudo-labeled data: Previous self-training algorithms (Rosenberg et al., 2005; McClosky et al., 2006; Sohn et al., 2020; Du et al., 2021) typically add a small set of unlabeled examples with the highest-conﬁdence pseudo labels to the labeled data set L at each iteration. In contrast, our approach adds all pseudo-labeled examples to L at every iteration regardless of conﬁdence. We compare the two approaches in Figure 3, which shows the labeling accuracy (% of unlabeled examples that are labeled correctly) on the development set (dev), the test set (test), and the unlabeled data pool (predict) of the SST-2 sentiment dataset. In the iterative conﬁdence ﬁltering-based approach (left plot), a ﬁxed number (in this plot, 32) of most conﬁdently labeled examples are added to the labeled set L at each iteration (the self-train line shows the labeling accuracy of these examples); once they have been added, they are not removed, and this process is repeated until the unlabeled set U is exhausted. As can be seen, this approach works well for

Labeling accuracy

Model
BERTBASE
BERTBASE + TA + STIN + STOUT + STIN + OUT

SCITAIL
67.16.6
78.53.2 86.33.5 81.43.7 82.62.6

CR
65.28.2
86.52.2 90.50.8 88.31.9 88.31.5

MRPC
72.410.2
74.56.5 81.00.8 80.31.9 80.21.1

RTE
51.42.5
67.67.1 70.62.4 71.23.2 69.94.0

# self-training iterations
Figure 3: On the SST-2 sentiment dataset, traditional conﬁdence ﬁltering-based self-training (left) yields poor results compared to our approach, which trains on all pseudo-labels at each iteration (right).
the several ﬁrst self-training iterations (3-5), but then labeling accuracy begins to degrade. In contrast, our algorithm (right plot) gradually and consistently improves labeling accuracy before converging at some iteration. These results suggest that strong base models beneﬁt from including even signiﬁcantly noisy pseudo-labels in self-training, as opposed to training on a narrow distribution of high-conﬁdence predictions.
Does self-training work with out-ofdomain/distribution (OOD) unlabeled examples? We investigate this question by applying self-training on top of BERTBASE + TA. We consider SOURCE → TARGET task pairs where training data from the source task without ground-truth labels is used as OOD unlabeled data for the target task. We experiment with several task pairs, including MNLI → SCITAIL, SST-2 → CR, QQP → MRPC, and MNLI → RTE. As shown in Table 5, self-training with OOD unlabeled examples (STOUT) is also helpful, offering an average absolute performance gain of +3.5% over the strong BERTBASE + TA baseline. However, using OOD unlabeled examples typically leads to worse self-training results compared to using in-domain unlabeled examples (STIN), except for the case MNLI → RTE, and combining the two types of unlabeled examples (STIN + OUT) does not bring further improvements over STIN.
Towards realistic evaluation in few-shot learning: In real-world low-resource scenarios, it is often impractical to rely on a development set (Oliver et al., 2018; Kann et al., 2019). With so little data, it may be more effective to use all labeled data for training. To examine the applicability of our methods to this real-world setting, here we con-

Table 5: Self-training with out-of-domain unlabeled examples also results in improvements, but using indomain data works signiﬁcantly better.

Model
BERTBASE + LMFT + ITFTMNLI + TA + STRATA

SST-2
58.88.4 (↓ 0.3) 64.08.1 (↓ 0.9) 76.57.2 (↓ 0.3) 79.86.3 (↓ 0.5) 86.62.6 (↓ 3.5)

SCITAIL
61.55.4 (↓ 5.6) 59.35.6 (↓ 4.7) 76.25.4 (↑ 0.4) 77.83.3 (↓ 0.7) 80.63.0 (↓ 5.7)

Table 6: In a realistic evaluation without a development set, our STRATA approach still leads to signiﬁcant improvements on top of BERTBASE. In parentheses, we show the absolute increase (↑) or decrease (↓) in performance compared to the same method used with a development set.

sider an evaluation that does not make use of a development set. Rather than using early stopping, we ﬁne-tune each model for a ﬁxed number of 512 steps. We checkpoint every 30 steps and evaluate a single model obtained by averaging the last 5 model checkpoints. For self-training, we perform a ﬁxed number of 30 self-training iterations, each following the same ﬁne-tuning procedure.
Table 6 summarizes our results. Broadly, all models perform worse in this setting than when a development set is available. Our STRATA approach still provides signiﬁcant improvements over BERTBASE, but much worse than the same method used with a development set. We conjecture that this is because without a development set, the model achieves somewhat lower accuracy in each self-training iteration, and these errors compound through later iterations.
6 Related Work
Improving language model ﬁne-tuning: Finetuning has been the most common approach for applying pre-trained language models to downstream tasks. However, it typically requires a target dataset of thousands to tens of thousands of examples to work well (Yogatama et al., 2019; Brown et al., 2020). Many methods have been proposed to improve performance and stability of pre-trained language models on small datasets, in-

cluding language model ﬁne-tuning on unlabeled data from the target domain (Howard and Ruder, 2018; Gururangan et al., 2020), intermediate-task ﬁne-tuning (Phang et al., 2019), multi-task preﬁnetuning (Aghajanyan et al., 2021a), better design choices and training strategies (Mosbach et al., 2021; Zhang et al., 2021), and regularizationoriented techniques (Jiang et al., 2020; Aghajanyan et al., 2021b). More related to our work is research on intermediate-task training that makes use of data-rich tasks (Phang et al., 2019), tasks that require complex reasoning and inference (Pruksachatkun et al., 2020), and beneﬁcial relationships among tasks (Vu et al., 2020, 2021).
Few-shot learning: Our work also relates to research in few-shot learning. In previous work, ﬁne-tuning is combined with other learning strategies to improve few-shot performance, including consistency training (Xie et al., 2020a), metalearning (Bansal et al., 2020), self-training (Du et al., 2021; Sun et al., 2020), and contrastive learning (Gunel et al., 2021). Other work has focused on prompt-based/entailment-based few-shot learning approaches (Brown et al., 2020; Schick and Schütze, 2021; Gao et al., 2021; Tam et al., 2021; Wang et al., 2021). Notably, Brown et al. (2020) demonstrate remarkable few-shot learning performance with a single frozen GPT-3 model, although its performance still lags far behind state-of-the-art ﬁne-tuning results.
Generative data augmentation: Recent work explores the generation capabilities of large-scale generative language models, such as GPT-2 (Radford et al., 2019) and T5 (Raffel et al., 2020), to generate synthetic training data for different tasks, including text classiﬁcation (Anaby-Tavor et al., 2020; Lee et al., 2021; Schick and Schütze, 2021), question answering (Puri et al., 2020), and commonsense reasoning (Yang et al., 2020). Yang et al. (2020) show that such a generative approach consistently outperforms previous data augmentation methods based on back-translation (Sennrich et al., 2016; Xie et al., 2020a).
Semi-supervised learning: Another area upon which our work builds is semi-supervised learning (SSL). Recent work has combined self-training with other techniques, e.g., noise injection (He et al., 2020; Xie et al., 2020b), consistency regularization and pseudo-labeling (Sohn et al., 2020), to develop powerful SSL algorithms. Du et al. (2021) show

that self-training improves upon language model pre-training.
Data augmentation for self-training: There has been interest in data augmentation methods for self-training, where task-speciﬁc in-domain data is either retrieved from a large bank of Web data (Du et al., 2021) or synthesized by training task-speciﬁc generative models (He et al., 2021b). Unlike these approaches, we use a single NLI data generator to produce in-domain NLI training examples for all tasks. Additionally, we demonstrate the importance of self-training on a broad distribution of pseudolabeled data. These approaches are complementary, and combining them is a promising direction for future work.
7 Limitations & Conclusion
Task augmentation and self-training provide complementary ways to leverage task-speciﬁc unlabeled data for improved downstream performance. While task augmentation utilizes unlabeled texts to synthesize a large amount of in-domain data for an auxiliary training task, self-training uses a model’s predictions on unlabeled examples to improve the model itself. When combining these methods in STRATA, we are able to substantially improve sample efﬁciency across 12 NLP benchmark datasets. That said, each method has its own limitations. While our implementation uses NLI as an auxiliary task in task augmentation, there are target tasks for which NLI may not be helpful (e.g., on grammatical acceptability judgments, as shown in Wang et al. (2019a)). Additionally, other auxiliary tasks may increase improvements (e.g., QNLI beneﬁts more from QA tasks (Vu et al., 2020)). We leave exploration of other auxiliary tasks to future work. Finally, our self-training algorithm (like prior approaches) assumes access to task-speciﬁc unlabeled examples, which might be non-trivial to acquire for some applications.
Acknowledgments
We thank David Berthelot, Colin Raffel, Kalpesh Krishna, Zi Yang, Jenny Lee, Guolong Su, and Nan Hua for useful discussions and valuable feedback at different stages of this project. We would also like to thank the anonymous reviewers, Kenton Lee, Zihang Dai, Ed H. Chi, Nader Akoury, Brendan O’Connor, Zhiyang Xu, Andrew Drozdov, and the rest of the UMass NLP group for their thoughtful comments and suggestions.

References
Armen Aghajanyan, Anchit Gupta, Akshat Shrivastava, Xilun Chen, Luke Zettlemoyer, and Sonal Gupta. 2021a. Muppet: Massive multi-task representations with pre-ﬁnetuning. arXiv preprint arXiv:2101.11038.
Armen Aghajanyan, Akshat Shrivastava, Anchit Gupta, Naman Goyal, Luke Zettlemoyer, and Sonal Gupta. 2021b. Better ﬁne-tuning by reducing representational collapse. In Proceedings of the 9th International Conference on Learning Representations (ICLR 2021).
Ateret Anaby-Tavor, Boaz Carmeli, Esther Goldbraich, Amir Kantor, George Kour, Segev Shlomov, Naama Tepper, and Naama Zwerdling. 2020. Do not have enough data? deep learning to the rescue! Proceedings of the 33th AAAI Conference on Artiﬁcial Intelligence (AAAI 2019), 34(05):7383–7390.
Trapit Bansal, Rishikesh Jha, Tsendsuren Munkhdalai, and Andrew McCallum. 2020. Self-supervised meta-learning for few-shot natural language classiﬁcation tasks. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP 2020), pages 522–534.
Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. 2015. A large annotated corpus for learning natural language inference. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP 2015), pages 632–642.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel HerbertVoss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Proceedings of the 34th Conference on Neural Information Processing Systems (NeurIPS 2020), volume 33, pages 1877–1901.
Daniel Cer, Mona Diab, Eneko Agirre, Iñigo LopezGazpio, and Lucia Specia. 2017. SemEval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation. In Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval 2017), pages 1–14.
Alexis Conneau and Douwe Kiela. 2018. SentEval: An evaluation toolkit for universal sentence representations. In Proceedings of the 11th International Conference on Language Resources and Evaluation (LREC 2018).
Alexis Conneau, Douwe Kiela, Holger Schwenk, Loïc Barrault, and Antoine Bordes. 2017. Supervised

learning of universal sentence representations from natural language inference data. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing (EMNLP 2017), pages 670–680.
Ido Dagan, Oren Glickman, and Bernardo Magnini. 2005. The pascal recognising textual entailment challenge. In Proceedings of the 1st International Conference on Machine Learning Challenges: Evaluating Predictive Uncertainty Visual Object Classiﬁcation, and Recognizing Textual Entailment (MLCW 2005), page 177–190.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL 2019), pages 4171–4186.
William B. Dolan and Chris Brockett. 2005. Automatically constructing a corpus of sentential paraphrases. In Proceedings of the 3rd International Workshop on Paraphrasing (IWP 2005).
Jingfei Du, Edouard Grave, Beliz Gunel, Vishrav Chaudhary, Onur Celebi, Michael Auli, Veselin Stoyanov, and Alexis Conneau. 2021. Self-training improves pre-training for natural language understanding. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL 2021), pages 5408–5418.
Tianyu Gao, Adam Fisch, and Danqi Chen. 2021. Making pre-trained language models better few-shot learners. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (ACL-IJCNLP 2021), pages 3816–3830.
Beliz Gunel, Jingfei Du, Alexis Conneau, and Ves Stoyanov. 2021. Supervised contrastive learning for pretrained language model ﬁne-tuning. In Proceedings of the 9th International Conference on Learning Representations (ICLR 2021).
Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger. 2017. On calibration of modern neural networks. In Proceedings of the 34th International Conference on Machine Learning (PMLR 2017), volume 70, pages 1321–1330.
Suchin Gururangan, Ana Marasovic´, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey, and Noah A. Smith. 2020. Don’t stop pretraining: Adapt language models to domains and tasks. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL 2020), pages 8342–8360.

Junxian He, Jiatao Gu, Jiajun Shen, and Marc’Aurelio Ranzato. 2020. Revisiting self-training for neural sequence generation. In Proceedings of the 8th International Conference on Learning Representations (ICLR 2020).
Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. 2021a. Deberta: Decoding-enhanced bert with disentangled attention. In Proceedings of the 9th International Conference on Learning Representations (ICLR 2021).
Xuanli He, Islam Nassar, Jamie Kiros, Gholamreza Haffari, and Mohammad Norouzi. 2021b. Generate, annotate, and learn: Nlp with synthetic text. arXiv preprint arXiv:2106.06168.
Jeremy Howard and Sebastian Ruder. 2018. Universal language model ﬁne-tuning for text classiﬁcation. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (ACL 2018), pages 328–339.
Minqing Hu and Bing Liu. 2004. Mining and summarizing customer reviews. In Proceedings of the 10th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD 2004), page 168–177.
Shankar Iyer, Nikhil Dandekar, and Kornél Csernai. 2017. First Quora Dataset Release: Question pairs.
Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, and Tuo Zhao. 2020. SMART: Robust and efﬁcient ﬁne-tuning for pretrained natural language models through principled regularized optimization. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL 2020), pages 2177–2190.
Zhengbao Jiang, Haibo Ding, Jun Araki, and Graham Neubig. 2021. How can we know when language models know? on the calibration of language models for question answering. Transactions of the Association for Computational Linguistics (TACL 2021).
Katharina Kann, Kyunghyun Cho, and Samuel R. Bowman. 2019. Towards realistic practices in lowresource natural language processing: The development set. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP 2019), pages 3342–3349.
Tushar Khot, Ashish Sabharwal, and Peter Clark. 2018. Scitail: A textual entailment dataset from science question answering. In Proceedings of the 32th AAAI Conference on Artiﬁcial Intelligence (AAAI 2018).
Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. 2020. ALBERT: A lite BERT for self-supervised learning of language representations. In Proceedings of the 8th International Conference on Learning Representations (ICLR 2020).

Kenton Lee, Kelvin Guu, Luheng He, Tim Dozat, and Hyung Won Chung. 2021. Neural data augmentation via example extrapolation. arXiv preprint arXiv:2102.01335.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692.
Marco Marelli, Stefano Menini, Marco Baroni, Luisa Bentivogli, Raffaella Bernardi, and Roberto Zamparelli. 2014. A SICK cure for the evaluation of compositional distributional semantic models. In Proceedings of the 9th International Conference on Language Resources and Evaluation (LREC 2014), pages 216–223.
David McClosky, Eugene Charniak, and Mark Johnson. 2006. Effective self-training for parsing. In Proceedings of the 2006 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL 2006), pages 152–159.
Marius Mosbach, Maksym Andriushchenko, and Dietrich Klakow. 2021. On the stability of ﬁne-tuning bert: Misconceptions, explanations, and strong baselines. In Proceedings of the 9th International Conference on Learning Representations (ICLR 2021).
Rafael Müller, Simon Kornblith, and Geoffrey E Hinton. 2019. When does label smoothing help? In Proceedings of the 33th Conference on Neural Information Processing Systems (NeurIPS 2019), volume 32. Curran Associates, Inc.
Avital Oliver, Augustus Odena, Colin A Raffel, Ekin Dogus Cubuk, and Ian Goodfellow. 2018. Realistic evaluation of deep semi-supervised learning algorithms. In Proceedings of the 32th Conference on Neural Information Processing Systems (NeurIPS 2018), volume 31, page 3239–3250.
Gabriel Pereyra, George Tucker, Jan Chorowski, Łukasz Kaiser, and Geoffrey Hinton. 2017. Regularizing neural networks by penalizing conﬁdent output distributions. In Proceedings of the 5th International Conference on Learning Representations (ICLR 2017).
Jason Phang, Thibault Févry, and Samuel R Bowman. 2019. Sentence encoders on stilts: Supplementary training on intermediate labeled-data tasks. arXiv preprint arXiv:1811.01088.
Yada Pruksachatkun, Jason Phang, Haokun Liu, Phu Mon Htut, Xiaoyi Zhang, Richard Yuanzhe Pang, Clara Vania, Katharina Kann, and Samuel R. Bowman. 2020. Intermediate-task transfer learning with pretrained language models: When and why does it work? In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL 2020), pages 5231–5247.

Raul Puri, Ryan Spring, Mohammad Shoeybi, Mostofa Patwary, and Bryan Catanzaro. 2020. Training question answering models from synthetic data. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP 2020), pages 5811–5826.
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a uniﬁed text-to-text transformer. Journal of Machine Learning Research (JMLR 2020), 21(140):1–67.
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: 100,000+ questions for machine comprehension of text. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP 2016), pages 2383– 2392.
Chuck Rosenberg, Martial Hebert, and Henry Schneiderman. 2005. Semi-supervised self-training of object detection models. In Proceedings of the 7th IEEE Workshops on Application of Computer Vision (WACV-MOTION 2005), volume 1, pages 29–36.
Timo Schick and Hinrich Schütze. 2021. It’s not just size that matters: Small language models are also few-shot learners. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL 2021), pages 2339– 2352.
Timo Schick and Hinrich Schütze. 2021. Generating datasets with pretrained language models. arXiv preprint arXiv:2104.07540.
Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Improving neural machine translation models with monolingual data. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL 2016), pages 86–96.
Noam Shazeer and Mitchell Stern. 2018. Adafactor: Adaptive learning rates with sublinear memory cost. arXiv preprint arXiv:1804.04235.
Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP 2013), pages 1631–1642.
Kihyuk Sohn, David Berthelot, Nicholas Carlini, Zizhao Zhang, Han Zhang, Colin A Raffel, Ekin Dogus Cubuk, Alexey Kurakin, and Chun-Liang Li.

2020. Fixmatch: Simplifying semi-supervised learning with consistency and conﬁdence. In Proceedings of the 34th Conference on Neural Information Processing Systems (NeurIPS 2020), volume 33, pages 596–608.
Zijun Sun, Chun Fan, Xiaofei Sun, Yuxian Meng, Fei Wu, and Jiwei Li. 2020. Neural semi-supervised learning for text classiﬁcation under large-scale pretraining. arXiv preprint arXiv:2011.08626.
Derek Tam, Rakesh R Menon, Mohit Bansal, Shashank Srivastava, and Colin Raffel. 2021. Improving and simplifying pattern exploiting training. arXiv preprint arXiv:2103.11955.
Tu Vu, Brian Lester, Noah Constant, Rami Al-Rfou, and Daniel Cer. 2021. Spot: Better frozen model adaptation through soft prompt transfer. arXiv preprint arXiv:2110.07904.
Tu Vu, Tong Wang, Tsendsuren Munkhdalai, Alessandro Sordoni, Adam Trischler, Andrew MattarellaMicke, Subhransu Maji, and Mohit Iyyer. 2020. Exploring and predicting transferability across NLP tasks. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP 2020), pages 7882–7926.
Alex Wang, Jan Hula, Patrick Xia, Raghavendra Pappagari, R. Thomas McCoy, Roma Patel, Najoung Kim, Ian Tenney, Yinghui Huang, Katherin Yu, Shuning Jin, Berlin Chen, Benjamin Van Durme, Edouard Grave, Ellie Pavlick, and Samuel R. Bowman. 2019a. Can you tell me how to get past sesame street? sentence-level pretraining beyond language modeling. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL 2019), pages 4465–4476.
Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. 2019b. Glue: A multi-task benchmark and analysis platform for natural language understanding. Proceedings of the 7th International Conference on Learning Representations (ICLR 2019).
Sinong Wang, Han Fang, Madian Khabsa, Hanzi Mao, and Hao Ma. 2021. Entailment as few-shot learner. arXiv preprint arXiv:2104.14690.
Jason Wei and Kai Zou. 2019. EDA: Easy data augmentation techniques for boosting performance on text classiﬁcation tasks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP 2019), pages 6382–6388.
Adina Williams, Nikita Nangia, and Samuel Bowman. 2018. A broad-coverage challenge corpus for sentence understanding through inference. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL 2018), pages 1112–1122.

Thomas Wolf, L Debut, V Sanh, J Chaumond, C Delangue, A Moi, P Cistac, T Rault, R Louf, M Funtowicz, et al. 2019. Huggingface’s transformers: State-of-the-art natural language processing. arXiv preprint arXiv:1910.03771.
Qizhe Xie, Zihang Dai, Eduard Hovy, Thang Luong, and Quoc Le. 2020a. Unsupervised data augmentation for consistency training. In Proceedings of the 34th Conference on Neural Information Processing Systems (NeurIPS 2020, volume 33, pages 6256– 6268.
Qizhe Xie, Minh-Thang Luong, Eduard Hovy, and Quoc V. Le. 2020b. Self-training with noisy student improves imagenet classiﬁcation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR 2020), pages 10687–10698.
Yiben Yang, Chaitanya Malaviya, Jared Fernandez, Swabha Swayamdipta, Ronan Le Bras, Ji-Ping Wang, Chandra Bhagavatula, Yejin Choi, and Doug Downey. 2020. Generative data augmentation for commonsense reasoning. In Findings of the Association for Computational Linguistics: EMNLP 2020 (Findings of EMNLP 2020), pages 1008–1025.
Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le. 2019. Xlnet: Generalized autoregressive pretraining for language understanding. In Proceedings of the 33th Conference on Neural Information Processing Systems (NeurIPS 2019), volume 32, pages 5753–5763.
Dani Yogatama, Cyprien de Masson d’Autume, Jerome Connor, Tomas Kocisky, Mike Chrzanowski, Lingpeng Kong, Angeliki Lazaridou, Wang Ling, Lei Yu, Chris Dyer, and Phil Blunsom. 2019. Learning and evaluating general linguistic intelligence. arXiv preprint arXiv:1901.11373.
Tianyi Zhang, Felix Wu, Arzoo Katiyar, Kilian Q. Weinberger, and Yoav Artzi. 2021. Revisiting fewsample bert ﬁne-tuning. In Proceedings of the 9th International Conference on Learning Representations (ICLR 2021).

Appendices
A Additional details for the datasets used in our study
The datasets used in our experiments come from two common language understanding benchmarks: GLUE (Wang et al., 2019b) and SENTEVAL (Conneau and Kiela, 2018). See Table 7 for details about dataset characteristics. We report F1 scores for QQP and MRPC, Spearman correlations for STSB and SICK-R, and accuracy scores for the other tasks.
B Example outputs of our NLI data generator
Table 8 shows example outputs of our NLI data generator using unlabeled texts from SST-2 and SCITAIL.

C Full experiment results across data regimes
We report the full results for our experiments in the FULL, LIMITED, and FEW-SHOT data regimes in Table 9, Table 10, and Table 11, respectively.

Task
text classiﬁcation/regression
SNLI (Bowman et al., 2015) MNLI (Williams et al., 2018) QQP (Iyer et al., 2017) QNLI (Wang et al., 2019b) SST-2 (Socher et al., 2013) SCITAIL (Khot et al., 2018) SST-5 (Socher et al., 2013) STS-B (Cer et al., 2017) SICK-E (Marelli et al., 2014) SICK-R (Marelli et al., 2014) CR (Hu and Liu, 2004) MRPC (Dolan and Brockett, 2005) RTE (Dagan et al., 2005, et seq.)

| Train |
570K 393K 364K 105K 67K 27K 8.5K 7K 4.5K 4.5K 4K 3.7K 2.5K

Task type
NLI NLI paraphrase identiﬁcation QA-NLI sentiment analysis NLI sentiment analysis semantic similarity NLI semantic similarity sentiment analysis paraphrase identiﬁcation NLI

Domain
misc. misc. social QA Wikipedia movie reviews science QA movie reviews misc. misc. misc. product reviews news news, Wikipedia

Table 7: Datasets used in our experiments and their characteristics, sorted by training data set size.

unlabeled texts from SST-2

Input: a cast of a-list brit actors

+ entailment

→ Output: the cast includes british actors.

+ neutral

→ Output: the cast includes a-list brit actors such as nicole kidman and charlie chaplin.

+ contradiction → Output: the cast was all new and unknown.

Input: both heartbreaking and heartwarming ... just a simple fable done in an artless sytle , but it ’s tremendously moving

+ entailment

→ Output: the story is moving and touching.

+ neutral

→ Output: the artless style used to make the fable was a favorite of many.

+ contradiction → Output: the fable is artful and sophisticated but it is also heartbreaking.

Input: gaping plot holes sink this ‘ sub ’ - standard thriller and drag audience enthusiasm to crush depth

+ entailment

→ Output: this movie’s plot holes seriously affect the enjoyment of the movie.

+ neutral

→ Output: the movies pace is too slow for most audiences.

+ contradiction → Output: the movie is a fun, easy romp of a thriller.

Input: putting the primitive murderer inside a high-tech space station unleashes a pandora ’s box of special effects that run the gamut

from cheesy to cheesier to cheesiest

+ entailment

→ Output: the special effects are all over the place.

+ entailment

→ Output: putting a murderer in a space station leads to all sorts of cheesy special effects.

+ entailment

→ Output: the murderer was inside a space station.

Input: vera has created a provocative , absorbing drama that reveals the curse of a self-hatred instilled by rigid social mores .

+ neutral

→ Output: vera has created a provocative drama that is absorbing and has a great cast.

+ neutral

→ Output: vera has created a provocative drama that shows the curse of self-hatred and shows the bonds

between women.

+ neutral

→ Output: vera’s drama, which has been loved by millions, is about the curse of self-hatred.

Input: ﬁlmmaking from one of french cinema ’s master craftsmen + contradiction → Output: the ﬁlmmaking is from a newcomer to the french cinema. + contradiction → Output: the ﬁlmmaking is from a new blood in the american cinema. + contradiction → Output: the ﬁlmmaking of one of italian cinema’s master craftsmen

unlabeled texts from SCITAIL

Input: Surface tension is responsible for the fact that small insects can walk on water.

+ entailment

→ Output: small insects can walk on water due to surface tension.

+ neutral

→ Output: it’s difﬁcult for large insects to walk on water.

+ contradiction → Output: small insects cannot walk on water because of surface tension.

Input: Tamiﬂu inhibits spread of virus.

+ entailment

→ Output: the virus is less spread when tamiﬂu is used.

+ neutral

→ Output: tamiﬂu is a good way to treat hiv.

+ contradiction → Output: tamiﬂu promotes viral spread.

Input: Animals can transport the waste material known as urea in the circulatory system and store it safely at high concentrations.

+ entailment

→ Output: animals can transport waste material in their circulatory systems.

+ neutral

→ Output: animals can store urea in their bloodstreams for up to a year.

+ contradiction → Output: only plants can transport the waste material known as urea in their circulatory systems.

Input: A number of applications of biomass, wind, hydropower and solar thermal are presently cost competitive with fossil fuels.

+ entailment

→ Output: many biomass applications are now cost competitive with fossil fuels.

+ entailment

→ Output: many alternatives to fossil fuels are cost competitive.

+ entailment

→ Output: some biofuels are now cost competitive with fossil fuels.

Input: A cell wall is not present in animal cells.

+ neutral

→ Output: in contrast, plant cells have a cell wall.

+ neutral

→ Output: in addition to not having a cell wall, animal cells also lack mitochondria.

+ neutral

→ Output: in animal cells, there is no cell wall, said the biologist.

Input: A bathymetric map can show the features of the bottom of a body of water. + contradiction → Output: a bathymetric map shows the top of a body of water. + contradiction → Output: a bathymetric map shows the features of the sky. + contradiction → Output: a bathymetric map shows what is on the surface of the water.

Table 8: Example outputs of our NLI data generator using unlabeled texts from SST-2 and SCITAIL.

Model
BERTBASE + LMFT + ITFTMNLI + TA
BERTLARGE + LMFT + ITFTMNLI + TA

SNLI
90.3 90.8 91.0 91.2
91.1 91.0 91.1 91.9

QQP
87.8 87.8 87.7 88.1
88.4 88.1 88.2 88.5

QNLI
90.6 90.2 90.3 90.9
91.9 90.4 91.6 92.5

SST-2
91.7 91.3 93.0 93.9
92.4 93.5 93.5 94.7

SCITAIL
93.2 92.9 95.8 96.3
95.3 95.3 96.5 96.9

SST-5
52.70.8 52.80.9 53.80.8 54.30.9
53.70.9 54.00.4 54.00.8 55.70.8

STS-B
88.90.3 89.30.3 90.10.1 90.10.1
89.60.2 89.50.2 90.30.3 90.90.2

SICK-E
86.70.5 86.80.8 89.50.3 90.10.3
87.90.6 87.70.5 89.90.2 90.70.3

SICK-R
82.90.5 82.70.5 85.30.6 85.60.3
84.40.4 84.00.5 86.30.3 87.00.3

CR
91.00.9 90.51.0 91.70.7 92.20.5
91.70.6 91.60.8 92.00.6 93.30.6

MRPC
87.91.0 87.90.6 89.81.1 90.10.8
89.00.8 89.51.0 89.70.9 90.80.7

RTE
63.52.3 63.93.7 78.11.9 79.30.9
68.67.2 66.57.3 82.31.4 83.81.1

Table 9: Our experiment results in the FULL data regime.

Model
BERTBASE + LMFT + ITFTMNLI + TA
BERTLARGE + LMFT + ITFTMNLI + TA

SNLI
71.71.1 73.42.1 82.90.3 85.70.3
77.40.6 75.81.5 85.20.4 87.30.3

QQP
71.70.4 72.10.6 73.30.7 75.30.4
74.11.0 71.60.5 74.00.5 75.70.5

QNLI
78.70.7 76.33.1 81.60.9 82.50.8
81.70.9 80.52.0 83.50.5 85.00.5

SST-2
87.41.0 86.91.2 87.80.6 90.40.7
89.80.6 88.90.8 90.00.8 91.70.7

SCITAIL
88.31.2 88.41.4 90.31.1 90.70.8
90.90.7 87.72.3 92.11.1 92.31.1

SST-5
47.11.3 47.51.3 48.81.0 49.21.3
49.11.3 49.23.1 49.41.2 51.41.0

STS-B
86.80.6 87.20.7 88.50.3 88.50.3
88.20.4 88.40.4 87.80.8 89.00.6

SICK-E
81.50.6 81.10.6 87.60.5 88.50.5
84.80.7 83.20.6 88.80.5 89.40.4

SICK-R
76.70.8 75.90.7 81.70.8 82.40.6
80.20.4 78.50.6 83.20.7 84.30.4

CR
89.90.7 91.10.8 90.00.6 91.41.0
91.20.6 90.90.7 91.30.7 92.60.6

MRPC
83.91.1 84.40.6 87.00.9 87.30.7
85.71.7 84.91.1 86.40.9 88.00.8

RTE
61.71.3 63.22.3 78.01.3 78.71.2
66.82.7 65.23.4 81.11.3 82.91.8

Table 10: Our experiment results in the LIMITED data regime.

Model
BERTBASE + LMFT + ITFTMNLI + TA + ST + ITFTMNLI + ST + STRATA
BERTLARGE + LMFT + ITFTMNLI + TA + ST + ITFTMNLI + ST + STRATA

SNLI
43.72.2 45.23.9 75.25.7 83.30.8 65.05.8 83.20.3 85.70.2
43.14.4 39.62.6 79.93.1 84.80.7 69.39.2 85.40.3 87.30.3

QQP
55.96.5 57.26.2 63.77.0 68.71.5 69.95.9 70.75.9 74.50.4
58.54.7 52.74.7 62.69.0 64.66.3 74.31.2 74.80.7 75.10.2

QNLI
59.010.9 57.69.1 62.85.1 70.13.4 71.611.3 81.51.2 82.10.5
64.46.1 52.21.6 64.54.4 71.54.0 85.41.7 86.11.1 86.40.8

SST-2
59.18.4 64.98.7 76.87.2 80.36.6 62.710.4 88.02.1 90.10.8
66.18.7 66.39.3 80.75.0 85.51.4 81.912.2 89.70.7 91.70.7

SCITAIL
67.16.6 64.08.0 75.85.6 78.53.2 68.68.3 83.74.4 86.33.5
68.89.5 66.410.6 72.311.2 79.04.5 79.94.8 86.24.2 87.32.9

SST-5
30.52.0 33.41.9 35.02.6 37.43.0 33.93.5 39.52.0 41.31.5
35.21.3 36.82.9 36.42.1 38.53.0 42.01.5 42.22.0 43.02.3

STS-B
73.64.5 75.44.4 80.21.1 80.71.5 80.52.2 84.20.8 84.70.5
74.63.8 75.49.4 75.54.0 78.92.4 82.82.3 84.11.7 84.51.6

SICK-E
61.34.1 59.34.0 80.41.9 81.12.4 68.14.5 81.82.6 84.91.2
66.54.5 58.86.9 77.83.8 81.23.9 77.33.1 84.32.0 86.31.8

SICK-R
59.72.7 58.32.0 73.52.7 75.91.8 64.02.4 75.82.2 77.61.6
66.63.3 51.67.0 73.52.8 77.51.4 73.12.3 78.41.3 79.01.0

CR
65.28.2 72.46.0 79.23.6 86.52.2 78.26.3 85.62.3 90.50.8
72.06.0 75.65.9 82.63.0 88.61.3 88.11.3 89.31.0 90.00.6

MRPC
72.410.2 73.98.6 74.38.0 74.56.5 80.51.8 80.61.2 81.00.8
79.92.0 80.52.4 72.87.9 78.26.6 81.20.5 81.41.2 81.50.7

RTE
51.42.5 50.93.9 62.213.5 67.67.1 50.73.1 62.512.0 70.62.4
53.13.3 52.84.8 69.714.6 77.06.3 53.94.3 72.75.4 77.15.4

Table 11: Our experiment results in the FEW-SHOT data regime.

