Quizbowl

arXiv:1904.04792v2 [cs.CL] 12 Feb 2021

Quizbowl: The Case for Incremental Question Answering

Pedro Rodriguez Shi Feng Department of Computer Science University of Maryland at College Park College Park, MD
Mohit Iyyer College of Information and Computer Sciences University of Massachusetts Amherst Amherst, MA
He He Department of Computer Science, Courant Institute New York University New York, NY
Jordan Boyd-Graber Department of Computer Science, iSchool, umiacs, lsc University of Maryland at College Park College Park, MD

pedro@cs.umd.edu shifeng@cs.umd.edu
miyyer@cs.umass.edu
hehe@cs.nyu.edu
jbg@umiacs.umd.edu

Editor:

Abstract
Scholastic trivia competitions test knowledge and intelligence through mastery of question answering. Modern question answering benchmarks are one variant of the Turing test. Specifically, answering a set of questions as well as a human is a minimum bar towards demonstrating human-like intelligence. This paper makes the case that the format of one competition—where participants can answer in the middle of hearing a question (incremental)—better differentiates the skill between (human or machine) players. Additionally, merging a sequential decision-making sub-task with question answering provides a good setting for research in model calibration and opponent modeling. Thus, embedded in this task are three machine learning challenges: (1) factoid qa over thousands of Wikipedia-like answers, (2) calibration of the qa model’s confidence scores, and (3) sequential decisionmaking that incorporates knowledge of the qa model, its calibration, and what the opponent may do. We make two contributions: (1) collecting and curating a large factoid qa dataset and an accompanying gameplay dataset, and (2) developing a model that addresses these three machine learning challenges. In addition to offline evaluation, we pitted our model against some of the most accomplished trivia players in the world in a series of exhibition matches spanning several years. Throughout this paper, we show that collaborations with the vibrant trivia community have contributed to the quality of our dataset, spawned new research directions, and doubled as an exciting way to engage the public with research in machine learning and natural language processing.
Keywords: Factoid Question Answering, Sequential Decision-Making, Natural Language Processing
1

Rodriguez, Feng, Iyyer, He, and Boyd-Graber
1. Introduction
At its premiere, the librettist of this opera portrayed a character who asks for a glass of wine with his dying wish. That character in this opera is instructed to ring some bells to summon his love. At its beginning, a man who claims to have killed a serpent has a padlock put on his mouth because of his lying. The plot of this opera concerns a series of tests that Tamino must undergo to rescue Tamina from Sorastro. For 10 points, name this Wolfgang Mozart opera titled for an enchanted woodwind instrument. Answer: The Magic Flute
Figure 1: qb is a trivia game where questions begin with clues that are initially difficult, but become progressively easier until a giveaway at the end of the question. Players answer as soon as they know the answer so as a result the earlier they answer the more knowledgeable they are. For example, answering after the first sentence indicates the player recognizes the librettist (Emanual Schikaneder) and knows that they played Papageno in The Magic Flute (die Zauberflöte). Answering at the end of the question only requires surface knowledge of Mozart’s opera works.
Answering questions is an important skill for both humans and computers. Exams form the foundation of educational systems and—for many societies—of the civil system (Fukuyama, 1995). Computers answering questions in the Turing test is the standard definition of artificial intelligence (Turing, 1995). But another more trivial form of question answering is more pervasive in popular culture.
Trivia games are pervasive and popular: from quizzing in India (Roy, 2016) to “What? Where? When?” in Russia (Korin, 2002) to “Who wants to be a Millionaire” (Clarke et al., 2001; Lam et al., 2003), trivia encourages people to acquire, recall, and reason over facts. For computers, Yampolskiy (2013) argues that these skills are ai-complete: solve question answering and you have solved ai generally. Our central thesis in this article is that the intense research in question answering would benefit in adopting the innovations and lessons learned from human trivia competition, as embodied in a trivia format called Quizbowl (qb).
In qb, questions are posed incrementally—word by word—and players must interrupt the question when they know the answer (Figure 1). Thus, it rewards players who can answer with less information than their opponents. This is not just a gimmick to separate it from other question answering formats: players must simultaneously think about what is the most likely answer and after every word decide whether it is better to answer or wait for more information. To succeed, players and machines alike must answer questions, maintain accurate estimates of their confidence, and factor their opponents’ abilities. The combination of these skills makes qb challenging for machine learning algorithms.
A dedicated and skilled community forged qb over decades (Section 2), creating a diverse and large dataset (Section 3). We refer to this dataset as the qanta dataset because (in our opinion) Question Answering is Not a Trivial Activity.1
1. Dataset available at http://datasets.qanta.org.
2

1962 CUhnaivlelernsgitey 1958 VSancaDnodraeln 1953 CoOllnegReaBdioowl

Quizbowl

1979

Trivial Pursuit

1977 CoIlnlegAeCBUoI wl

1965 PubBQeguiinzzes

2009 NOAvQerTAtaCkUeIs 1996 NAFQouTn, dPeAdCE 1991 ACF Founded

Birth of Modern Trivia

Popularization

Professionalization

Figure 2: Trivia has gone from a laid-back pastime to an organized, semi-professional competition format. The qb framework, in particular, which arose from College Bowl (us) and University Challenge (uk) emphasizes fairness and the ability to discover the better question answerer. As organizations such as the Academic Competition Federation and National Academic Quiz Tournaments emerged, the format has focused on academic, well-run tournaments.

Playing qb requires deciding what to answer (Section 5) and when to answer (Section 6). Our final contribution is a framework that combines independent systems for each of these sub-tasks. Despite its simplicity, our implementation of this framework is competitive with the best players. Section 8 showcases qb as a platform for simultaneously advancing research and educating the public about the limits of machine learning through live human–computer competitions. Finally, we discuss ongoing and future research using trivia questions to build machines that are as capable of reasoning and connecting facts as humans.
2. Why Quizbowl?
When discussing machine learning and trivia, the elephant in the room is always ibm’s tourde-force match (Ferrucci et al., 2010) against Ken Jennings and Brad Rutter on Jeopardy! Rather than ignore the obvious comparisons, we take this on directly and use the well-known Jeopardy! context—which we gratefully acknowledge as making our own work possible—as a point of comparison, as qb is a better differentiator of skill between participants, be they human or machine (Sections 2.1 and 2.2).2 While this section will have more discussion of the history of trivia than the typical machine learning paper, the hard-won lessons humans learned about question answering transfer into machine question answering.
The qa format categorization of Gardner et al. (2020b) names three tasks where framing the problem as qa is useful : (1) filling human information needs, (2) qa as annotation or probe, and (3) as a transfer mechanism. Like Searchqa (Dunn et al., 2017, Jeopardy!), qb does not explicitly probe specific linguistic phenomena; it uses language to ask what
2. Boyd-Graber et al. (2012) introduce qb as a factoid question answering task, Iyyer et al. (2015) further develop algorithms for answering questions, and He et al. (2016) improve live play. This journal article synthesizes our prior work scattered across disparate publications, drops artificial limitations (e.g., ignoring categories or rare answers), and evaluates models in offline, online, and live environments. Moreover, it connects the earlier work with question answering datasets that followed such as squad.
3

Rodriguez, Feng, Iyyer, He, and Boyd-Graber
humans know. In contrast to questions posed to search engines or digital assistants (Nguyen et al., 2016; Kwiatkowski et al., 2019), qb is less ambiguous: question writers ensure that the descriptions uniquely identify one and only one answer, a non-trivial goal (Voorhees and Tice, 2000). Thus, the goals and challenges in qb are similar to—yet distinct from—open domain information-seeking.
The qb format is compelling and consistent because of its evolution (Figure 2) over its fifty-year history.3 Many of the challenges the nlp community faces in collecting good question answering datasets at scale (Hermann et al., 2015) were first encountered by trivia aficionados. For example, avoiding predictive yet useless patterns in data (Jia and Liang, 2017; Kaushik and Lipton, 2018); players do not like re-used clues making questions trivially easy. Trivia players also aim to write questions that require multi-hop reasoning; datasets like HotPotQA (Yang et al., 2018) have similar goals, but writing questions that truly require multi-hop reasoning is challenging (Min et al., 2019). We distill these lessons, describe the craft of question writing that makes qb a compelling question answering task (Section 2.3), and enumerate some nlp challenges required to truly solve qb (Section 2.4). We conclude by framing qb as a hybrid task between question answering and sequential decision-making (Section 2.5).
2.1 What is a Buzzer Race?
The scapegoat for every Jeopardy! loser and the foundation of every Jeopardy! winner is the buzzer (Harris, 2006). A buzzer is a small handheld device that players press to signal that they can correctly respond to a clue. The fundamental difference between Jeopardy! and qb—and what makes qb more suitable for research—is how clues are revealed and how players use the buzzer.
Jeopardy! is a television show and uses the buzzer to introduce uncertainty, randomness, and thus excitement for the viewer at home. In Jeopardy!, players can only use the buzzer when the moderator has finished reading the question.4 If players use the buzzer before the question is finished, they are locked out and prevented from answering the question for a fraction of a second (an eternity in the fast-paced game of Jeopardy!).
This advantaged Watson in its match against two opponents with feeble human thumbs and reflexes, as Jeopardy! uses the buzzer to determine who among those who know the answer has the fastest reflexes.5 While Watson gets an electronic signal when it was allowed
3. After returning from World War II and inspired by uso morale-building activities, Canadian Don Reid sketched out the format with the first host Allen Ludden. After a radio premiere in 1953, College Bowl moved to television in 1959 and became the first television show to win a Peabody Baber (2015). The format established many careers: the future president of the National Broadcasting Corporation (nbc), Grant Tinker, served as the game’s first scorekeeper (the newly designed game and its scoring was so confusing that Allen Ludden often had to ad lib to let Tinker catch up). The format was intriguing enough that Granada studios copied it—initially without permission—into what became the uk cultural touchstone University Challenge (Taylor et al., 2012), establishing the career of Bamber Gascoigne.
4. In Jeopardy! terminology is reversed so that a moderator reads clues termed answers to which players must supply the correct question. To avoid confusion, we follow standard terminology.
5. In a Ken Jennings interview with npr Malone (2019), the host Kenny Malone summarized it well as “To some degree, Jeopardy! is kind of a video game, and a crappy video game where it’s, like, light goes on, press button—that’s it.” Ken Jennings agreed, but characterized it as “beautiful art and not a really crappy video game”.
4

Quizbowl
to buzz, the two humans watch for a light next to the Jeopardy! game board to know when to buzz. Thus, Watson—an electronic buzzing machine—snags the first choice of questions, while the two humans fight over the scraps. In Jeopardy! reflexes are almost as important as knowledge. Next we show how the structure of qb questions and its use of a buzzer rewards depth of knowledge rather than reflexes.
2.2 Pyramidality and Buzzers
In contrast, qb is a game honed by trivia enthusiasts which uses buzzers as a tool to determine who knows the most about a subject. This is possible because the questions are interruptable. Unlike Jeopardy!, players can interrupt the questions when they know the answer (recall questions are multi-sentence in qb). This would make for bad television (people like to play along at home and cannot when they cannot hear the whole question), but makes for a better trivia game that also requires decision-making under uncertainty.
This alone is insufficient however; if an easy clue appears early in the question then knowing hard clues later in the question is irrelevant. Questions that can be answered with only a fraction of their input are a bad foundation for research (Sugawara et al., 2018; Feng et al., 2019). qb addresses this problem by structuring questions pyramidally. In pyramidal questions, clues are incorporated so that harder, more obscure information comes first in the question, and easier, more obvious information comes at the end of the question. Thus, when a player answers before their opponents, they are more knowledgeable than their opponents.
This also makes qb an attractive machine learning research domain. The giveaways are often easy for computers too: they are prominent on Wikipedia pages and have appeared in many questions. Thus, it is easy for computers to answer most questions at some point: qb is not an impossibly difficult problem. The challenge then becomes to answer the questions earlier, using more obscure information and higher-order reasoning.
Humans who play qb have the same yearning; they can answer most of the questions, but they want to deepen their knowledge to buzz in just a little earlier. They keep practicing, playing questions and going to tournaments to slowly build skill and knowledge. qb is engineered for this to be a rewarding experience.
The same striving can motivate researchers: it does not take much to buzz in a word earlier. As small incremental improvements accumulate, we can have more robust, comprehensive question answering systems. And because qb has a consistent evaluation framework, it is easy to see whose hard work has paid off.
Thus, the form of qb questions—the product of decades of refining how to measure the processing and retrieval of information of humans—can also compare machines’ question answering ability. We next describe the cultural norms of question writing in the qb community that contribute to making it a challenging task for humans and machines alike.
2.3 The Craft of Question Writing
The goal of qb is to reward “real” knowledge. This goal is the product of a long history that has resulted in community norms that have evolved the competition into a thriving, carefully designed trivia ecosystem. By adopting these conventions, machine learning can benefit from the best practices for question answering evaluation without repeating the same mistakes.
5

Rodriguez, Feng, Iyyer, He, and Boyd-Graber
Every year, question writers in the community focus on creating high quality questions that are novel and pyramidal. Experts write thousands of questions each year.6 To maintain the quality and integrity of competition, the community enforces rules consistent with machine learning’s question for generalization as described by Boyd-Graber and Börschinger (2020): avoiding ambiguity, ensuring correctness, eschewing previously used clues, and allowing for fair comparisons between teams (Lujan and Teitler, 2003; Vinokurov, 2007; Maddipoti, 2012) of 10,000 middle school students, 40,000 high school students, and 3,200 college students (National Academic Quiz Tournaments, 2020). At the same time, in preparation for tournaments students study questions from previous years.
These dueling groups—players and writers—create a factual arms race that is the foundation for the quality of qb questions. Aligning annotators’ motivations (von Ahn, 2006)—such as playing a game—with the goals of the data collection improves the quality and quantity of data. A similar arms race between dataset exploiters (attackers) and those seeking to make datasets more robust (defenders) exists in other machine learning domains like computer vision (Carlini and Wagner, 2017; Hendrycks et al., 2019) and Build-It, Break-It, (Fix-It) style tasks (Ettinger et al., 2017; Thorne et al., 2019; Dinan et al.; Nie et al., 2020).
In qb, answers are uniquely identifiable named entities such as—but not limited to— people, places, events, and literary works. These answers are “typified by a noun phrase” as in Kupiec (1993) and later in the trec qa track (Voorhees, 2003). Similar answer types are also used by other factoid question answering datasets such as SimpleQuestions (Bordes et al., 2015), SearchQA (Dunn et al., 2017), TriviaQA (Joshi et al., 2017), and NaturalQuestions’ short answers (Kwiatkowski et al., 2019). In its full generality, qb is an Open Domain qa task (Chen et al., 2017; Chen and Yih, 2020). However, since the vast majority of answers correspond to one of the six million entities in Wikipedia (Section 3.4),7 we approximate the open-domain setting by defining this as our source of answers (Section 9.1 reframes this in reading comprehension’s span selection format). Like the ontology of ImageNet (Deng et al., 2009), no formalism is perfect, but it enables automatic answer evaluation and linking to a knowledge base. In qb though, the challenge though is not in framing an answer, it is in answering at the earliest possible moment.
The pyramidal construction of questions—combined with incrementality—makes qb a more fair and granular comparison. For example, the first sentence of Figure 1—also known as the lead in—while obscure, uniquely identifies a single opera. Questions that begin misleadingly are scorned and derided in online discussions tournament as “neg bait”;8 Thus, writers ensure that all clues are uniquely identifying even at the start.
6. Regional competition questions are written by participants; championship competition questions are written by professionals hired by either the Academic Competition Federation (acf), National Academic Quiz Tournaments (naqt), or the Partnership for Academic Competition Excellence (pace). While the exact organizational structure varies, initial draft questions are vetted and edited by domain experts.
7. A minority of answers cannot be mapped. Some answers do not have a page because Wikipedia is incomplete (e.g., not all book characters have Wikipedia pages). Other entities are excluded by Wikipedia editorial decisions: they lack notability, are combined with other entities(e.g., Gargantua and Pantagruel and Romulus and Remus). Other abstract answers will likely never have Wikipedia pages (women with one leg, ways Sean Bean has died in films).
8. “Negging” refers to interrupting a question with a wrong answer; while wrong answers do happen, a response with a valid chain of reasoning should be accepted. Only poorly written questions admit multiple viable answers.
6

Quizbowl
The entirety of questions are carefully crafted, not just the lead-in. Middle clues reward knowledge but cannot be too easy: frequent clues in questions or clues prominent in the subject’s Wikipedia page are considered “stock” and should be reserved for the end. These same insights have been embraced by machine learning in the guise of adversarial methods (Jia and Liang, 2017) to eschewing superficial pattern matching. In contrast, the final giveaway clue should be direct and well-known enough; someone with even a passing knowledge of The Magic Flute would be able to answer.
This is the product of a complicated and nuanced social dynamic in the qb community. Top teams and novice teams often play on the same questions; questions are—in part—meant to teach (Gall, 1970) so are best when they are fun and fair for all. The pyramidal structure ensures that top teams use their deep knowledge and quick thinking to buzz on the very first clues, but novice teams are entertained and learning until they get to an accessible clue. Just about everyone answers all questions (it is considered a failure of the question writer if the question “goes dead” without an answer).
qb is not just used to test knowledge; it also helps discover new information and as a result diversifies questions (“oh, I did not know the connection between the band the Monkees and correction fluid!”).9 While most players will not recognize the first clue (otherwise the question would not be pyramidal), it should be interesting and connect to things the player would care about. For example, in our Magic Flute question, we learn that the librettist appeared in the premiere, a neat bit of trivia that we can tuck away once we learn the answer.
These norms have established qb questions as a framework to both test and educate human players. Our thesis is that these same properties can also train and evaluate machine question answering systems. Next, we highlight the nlp and ml challenges in qb.
2.4 Quizbowl for Natural Language Processing Research
We return to Figure 1, which exemplifies nlp challenges common to many qb questions. We already discussed (pyramidality): each sentence uniquely identifies the answer but each is easier than the last. The most knowledgable answers earlier and “wins” the question. But what makes the question difficult apart from obscurity (Boyce-Jacinoand Simon DeDeo, 2018)? Answering questions early is significantly easier if machines can resolve coreference (Ng, 2010) and entity linking (Shen et al., 2015).
First, the computer should recognize “the librettist” as Schikaneder, whose name never appears in the question. This special case of entity linking to knowledge bases is sometimes called Wikification (Cheng and Roth, 2013; Roth et al., 2014). The computer must recognize that “the librettist” refers to a specific person (mention detection), recognize that it is relevant to the question, and then connect it a knowledge base (entity linking).
In addition to linking to entities outside the question, another challenge is connecting coreferences within a question. The interplay between coreference and question answering is well known (Stuckardt, 2003), but Guha et al. (2015) argue that qb coreference is particularly challenging: referring expressions are longer and oblique, world knowledge is needed, and entities are named after other referring expressions. Take the character Tamino (Figure 1): while he is eventually mentioned by name, it is not until after he has been referred to
9. Bette Nesmith Graham, the mother of Monkees band member Michael Nesmith, invented correction fluid in 1956.
7

Rodriguez, Feng, Iyyer, He, and Boyd-Graber
obliquely (“a man who claims to have killed a serpent”). The character Papageno (portrayed by Schikaneder) is even worse; while referred two twice (“character who asks for a glass of wine”, “That character”), Papageno is never mentioned by name. To fully solve the question, a model may have to solve a difficult coreference problem and link the reference to Papageno and Schikaneder.
These inferences, like in the clue about “the librettist”, are often called higher-order reasoning since they require creating and combining inference rules to derive conclusions about multiple pieces of information (Lin and Pantel, 2001). Questions that require only a single lookup in a knowledge base or a single ir query are uninteresting for both humans and computers; thus, they are shunned for qb lead-in clues. Indeed, the first sentences in qb questions are the most difficult clues for humans and computers because they often incorporate surprising, quirky relationships that require skill and reasoning to recognize and disentangle. Interest in multi-hop question answering led to the creation WikiHop through templates (Welbl et al., 2018) and HotPotQA through crowdsourcing (Yang et al., 2018). In contrast to these artificially or crowdsourced created datasets, qb questions focus on links that experts view as relevant and important.
Finally, even the final clue (called a “giveaway” because it’s so easy for humans) could pose issues for a computer. Connecting “enchanted woodwind instrument” to The Magic Flute requires solving wordplay. While not all questions have all of these features, these features are typical of qb questions and showcase their richness.
Crowdsourced datasets like OpenBooksQA (Mihaylov et al., 2018) and CommonSenseQA (Talmor et al., 2019) have artifacts that algorithms can game (Geva et al., 2019): they find the right answer for silly reasons. For example, answering correctly with just a handful of words from a squad question (Feng et al., 2018), none of a bAbI question (Kaushik and Lipton, 2018), or the image in a question about an image (Goyal et al., 2017). Although the qanta dataset and other “naturally occurring” data likely do contain machine exploitable patterns, they do not face the same quality issues since the author’s motivation is intrinsic: to write an entertaining and educational question as in qb.
2.5 Quizbowl for Machine Learning Research
While answering questions showcases the nlp challenges, deciding when to answer showcases the ml challenges related to decision theory (Raiffa, 1968). As in games like Poker (Brown and Sandholm, 2019), qb players have incomplete information: they do not know when their opponent will answer, do not know what clues will be revealed next, or if they will know the next clues. In our buzzer model, the qa model output is but one piece of information used to make the decision—under uncertainty—of when to buzz in. Since a decision must be made at every time step (word), we call this an incremental classification task.
We formalize the incremental classification task as a Markov Decision Process (Zubek and Dietterich, 2002, mdp). The actions in this mdp correspond to what a player can do in a real game: click the buzzer and provide their current best answer or wait (one more word) for more information. The non-terminal states in the state space are parameterized by the text of the question revealed up to the current time step, the player’s current best guess, and which player (if any) has already buzzed incorrectly. Rewards are only given at terminal states and transitions to those states are determined by which player correctly answered
8

Quizbowl
first. Additionally, we treat the opponent as a component of the environment as opposed to another agent in the game.10 This task—the buzzing task—has connections to work in model confidence calibration offline (Yu et al., 2011; Nguyen and O’Connor, 2015) as well as online (Kuleshov and Ermon, 2017), cost-sensitive learning (Elkan, 2001), acquisition of features with a budget (Lizotte et al., 2003), and incremental classification (Melville et al., 2005).
For humans, effective qb play involves maintaining a correctness estimate of their best answer, weighing the cost and benefits of answering now versus waiting, and making buzzing decisions from this information. Naively, one might assume that model calibration is as simple as examining the probability output by the (neural) qa system, but neural models are often especially poorly calibrated (Guo et al., 2017) and calibrations often fail to generalize to out of domain test data (Kamath et al., 2020). Since qb training data spans many years, models must also contend with domain shift (Ovadia et al., 2019). Model calibration is naturally related to deciding when to buzz—also known as answer triggering in qa and information retrieval (Voorhees, 2001; Yang et al., 2015).
Unlike standard answer triggering though, in qb the expected costs and benefits are continually changing. Specifically, there are costs for obtaining new information (seeing more words) and costs for misclassifications (guessing incorrectly or waiting too long). This parallels the setting where doctors iteratively conduct medical tests until they are confident in a patient’s diagnosis (Zubek and Dietterich, 2002; Chai et al., 2004).
Although this can be framed as reinforcement learning, we instead frame buzzing in Section 6 as incremental classification as in Trapeznikov and Saligrama (2013). In this framing, a binary classifier at each time step determines when to stop obtaining new information and render the decision of the underlying (qa) model. As Trapeznikov and Saligrama (2013) note, evaluation in this scenario is conceptually simple: compare the costs incurred to benefits gained.
Evaluation We evaluate the performance of our systems through a combination of standalone comparisons (Section 7.1) and simulated qb matches (Section 7.3). For standalone evaluation we incrementally feed systems new words and record their responses. We then calculate accuracy for each position in the question (e.g., after the first sentence, halfway through the question, and at the end). While standalone evaluations are useful for developing systems, the best way to compare systems and humans is with evaluations that mimic qb tournaments.
A recurring theme is our mutually beneficial collaboration with the qb community: host outreach exhibitions (Section 8), annotate data, play with and against our systems (Section 10.3), and collect the qanta dataset. This community created this rigorous format for question answering over decades and continues to help understand and measure the question answering abilities of machines.
3. QANTA Dataset
This section describes the qanta dataset from the qb community (Section 3.1). The over 100,000 human-authored, English questions from qb trivia tournaments (Section 3.2) allows
10. This is not precisely true in our live exhibition matches; although we treat the opponent as part of the environment, our human opponents do not and usually adapt to how our system plays. For instance, it initially had difficulty with pop culture questions.
9

Rodriguez, Feng, Iyyer, He, and Boyd-Graber

Dataset
SimpleQuestions (Bordes et al., 2015) Triviaqa (Joshi et al., 2017) squad 1.0 (Rajpurkar et al., 2016) Searchqa (Dunn et al., 2017) NaturalQuestions (Kwiatkowski et al., 2019)
qanta 2012 (Boyd-Graber et al., 2012) qanta 2014 (Iyyer et al., 2014) qanta 2018 (This Work)

QA Pairs (Sentences / Questions)
100K 95K 100K 216K 315K
47.8K / 7.95K 162K / 30.7K 650K / 120K

Tokens
.614M 1.21M .988M 4.08M 2.95M
1.07M 4.01M 11.4M

Table 1: The qanta dataset is larger than most question answering datasets in qa pairs (120K). However, for most qb instances each sentence in a question can be considered a qa pair so the true size of the dataset is closer to 650K QA pairs. In Section 5 using sentence level qa pairs for training greatly improves model accuracy. The qanta dataset has more tokens than all other qa datasets. Statistics for qanta 2012 and 2013 only include publicly available data.

systems to learn what to answer. More uniquely, 3.9 million filtered records of humans playing qb online (Section 3.3) allows systems learn when to “buzz in” against opponents (Section 4).
3.1 Dataset Sources
The qb community maintains and curates several public databases of questions spanning 1997 to today.11 On average, 10,000 questions are written every year. Our dataset has 119,247 questions with over 650 thousand sentences and 11.4 million tokens.
To help players practice and to build a dataset showing how humans play, we built the first website for playing qb online (Figure 3a). After initial popularity, we shut down the site; however, enterprising members of the qb community resurrected and improved the application. 89,477 players used the successor (Figure 3b) and have practiced 5.1 million times on 131,075 unique questions. A filtered12 subset of 3.9 million player records forms the second component of our dataset, which we call gameplay data.
3.2 QANTA Questions
Table 1 compares qa datasets written by humans. Because often each qb sentence has enough information for players to answer, each qanta instance can be broken into four to six pseudo sentence-answer pairs. Although our dataset does not have the most questions, it is significantly larger in the number of sentences and tokens.
In addition to qanta having more sentences, questions are longer (Figure 4), especially compared to crowd-sourced datasets. As a side effect of both being longer and not crowdsourced, qb sentences are syntactically complex and topically diverse (Figure 5).
11. Questions in were obtained (with permission) from http://quizdb.org and http://protobowl.com. 12. We include only a player’s first play on a question and exclude players with less than twenty questions.
10

Quizbowl

Answering questions as:

Jordan

Question from ACF Nationals 2007 (Berkeley) Don't show questions from this packet again

Don't show questions from this tournament again

Text Reveal Speed
One is Monte Carlo if at least half of the possible results for all x in a language it says "yes" and "no" otherwise. One is called unambiguous if for any x there is at most one accepting computation. One is called oblivious if the position of the cursor at the t-th step depends only on the t and the length of the input. One is non-deterministic if its sets of next states may contain more than one element. For ten points, identify this model of computation named for
Answer (or press space)
(a) Our 2012 interface was the first way to play qb online.

Correct!
People who answered before you did:
Kevin Parag Irene
People who answered after you did:
Cecilia Jim
Incorrect Answers:
Turntable Toaster Computer Poland Algorithm

(b) The qb interface for collecting most of our gameplay records. It improved over our own through features like real-time competitive play and chatrooms.
Figure 3: Our interface and a popular modern interface for playing qb online. Both interfaces reveal questions word-by-word until a player interrupts the system and makes a guess.

11

Rodriguez, Feng, Iyyer, He, and Boyd-Graber

Length in Words

QB (sentence) 40

Jeopardy!

TriviaQA

SQuAD

SimpleQuestions

30

20

10

0

0 10 20 30 40 50 0 10 20 30 0

10 0

10

20 0 10 20

Number of Questions (Thousands)

Figure 4: Size of question answering datasets. Questions in the qanta dataset have longer sentences than any other dataset. The instances from SimpleQuestions, SQuAD, and Triviaqa are comparatively short which makes it less likely that they are as diverse of qb or Jeopardy!. For each dataset we compare the lengths of questions rather than paired context paragraphs; to avoid the histogram being overly skewed we remove the top 5% of examples by length from each dataset.

3.2.1 Dataset Diversity
Creating diverse datasets is a shared goal between researchers developing nlp resources and organizers of qb tournaments. qb questions are syntactically diverse with dense coreference (Guha et al., 2015) and cover a wide range of topics. Diversity takes the form of questions that reflect the topical, temporal, and geographical breadth of a classical liberal education. For example, the Academic Competition Federation mandates that literature cover American, British, European, and world literature (Vinokurov et al., 2014). Moreover, authors must “vary questions across time periods”—with no more than one post 1990 literature—and questions must “span a variety of answers such as authors, novels, poems, criticism, essays, etc.” There are similarly detailed proscriptions for the rest of the distribution.
Figure 5 shows the category and sub-category distribution over areas such as history, literature, science, and fine arts. Taken together, qb is a topically diverse dataset across broad categories and finer-grained sub-categories. This diversity contrasts with a sample of 150 questions from NaturalQuestions (Kwiatkowski et al., 2019)13 which indicates that questions are predominantly about Pop Culture (40%), History (19%), and Science (15%); see Appendix B for complete results. This emphasizes that to do well, players and systems need to have both breadth and depth of knowledge.
3.2.2 Answer Diversity
qb questions are also diverse in the kinds of entities that appear as answers (25K entities in the training data). A dataset which is topically diverse, but only asks about people is not ideal. Using the Wikidata knowledge graph we obtain the type of each answer and plot
13. The authors annotated 150 questions from the development set using the same categories as qb.
12

Quizbowl

Norse Classic
Other European Ancient ClasEsiurcoaple British
Amer ican

Euro British pean World Europe
Other Classical Classic

erican Am

None

World

Literature

History

None

None

Science

Biology hysics try
P mis Che

Fine Arts None

logy Mytho
cience Social S

Current Events
Religion GPhilosophy Pop eograp Cultu hy
re

None Norse
None
Norse
None
Norse
None

Pop C Nor Other ulturese
None

None

None

ACom OthMearth
NorEartshtronomputer se Sciencey Science
Visual Auditory

Norse
Norse logy
Anthropo mics Econoology
Religion/Mythology GPehoilgorsaopphyhy Psych Art
Audiovisual
Music Other

Figure 5: Questions in qb cover most if not all academic topics taught in school such as history, literature, science, the fine arts, and social sciences. Even within a single category, questions cover a range of topics. Topically, the dataset is biased towards American and European topics in literature and history.

frequencies in Figure 6. Most questions ask about people (human), but with a broad diversity among other types.
These two breakdowns show that qb is topically and answer-wise diverse. To qb aficionados this is unsurprising; the primary educational goal of qb is to encourage students to improve their mastery over wide ranges of knowledge. We now turn to details about the gameplay dataset.
3.3 Gameplay Records
Like the 2002 trec qa track (Voorhees, 2004), squad 2.0 (Rajpurkar et al., 2018), and nq (Kwiatkowski et al., 2019), deciding when not to answer is crucial to playing qb. Unlike these tasks though, deciding when to answer is not just model calibration or triggering, but also should reflect the opponent’s behavior (Billings et al., 1998). To address this, we use gameplay data (Table 2) which contain records of quizbowlers playing questions from prior tournaments: words in each question were revealed one-by-one until the player guessed the question’s answer. We use these records as (1) training data so that models can learn to imitate an oracle buzzing policy (Coates et al., 2008; Ross and Bagnell, 2010; Ross et al., 2011) and (2) as human baselines for offline evaluations (Section 7).
Like Mandel et al. (2014), gameplay records both simulate humans for training and evaluating policies. To simulate play against a human, we see which agent—human or machine—first switches from the wait action to the buzz action. For example, in Table 2 the user correctly guessed “Atlanta” at word forty-seven. If an agent played against this player they would need to answer correctly before word forty-seven to win. In all but one outcome,
13

Rodriguez, Feng, Iyyer, He, and Boyd-Graber

ce l Scien Socia phy Philoso
nce Scie

R Pop eligion Culture

Fine Arts

History

Science

human

Literature

History Literaiatul rSecience rts Soc A Fine n
eligiology R ytho ophy e
M hilos ultur P op C P

NOMATCH literary work
Literature

t onflic re c ical figu
tholog
my country

Myth

History

anatomical structure

dynasty

ethnic group

part of a plant

treaty physical quantity

pa taxon film

type

of

quantum

religion parti

state of the

cle river

ficptihoysicalUpnritoedpSetates

mu

nal characterrty

sical com disease

chemicacl iintytingposition

Science History History Science History
Fine Science Science Pop Culture Religion GeoSgcriaepnhcye ScieHnisctoery Literature FinSeciAerntcse
H Arts Geog Scienceistoryraphy

nce l Scie Socia

ology Fine Arts

History

Fine Arts

Literaturey

Geograph

Religion History
Social Science Philosophy

Figure 6: Distribution of wikidata.org answer types (“instance of” relation) further broken down by category. Most answers have matching types and reference a person, literary work, or geographic entity. Among these types, there is a good balance of answers spread across literature, history, fine arts, and science. Answer types with only one category are largely self-explanatory (e.g., mythological answers types to the mythology category). The special category “NOMATCH” are answers without a matched type and similar types are merged into larger categories.

Date UID QID Position Guess Result Question text

Thu Oct 29 2015 08:55:37 GMT-0400 (EDT) 9e7f7dde8fdac32b18ed3a09d058fe85d1798fe7 5476992dea23cca90550b622 47 atlanta True This Arcadian wounded a creature sent to punish Oeneus for improperly worshipping Artemis and killed the centaurs Rhaecus and Hylaeus. . .

Table 2: An entry from the gameplay dataset where the player correctly guesses “Atlanta” at word 47. The entry qid matches with the proto_id field in the question dataset where additional information is stored such as the source tournament and year.

14

Quizbowl
replaying the human record exactly recreates a live face-off. When a machine incorrectly buzzes first we lack what the human would ultimately guess, so we assume their guess would have been correct since skilled players almost always answer correctly by the end of the question. During training, these data help agents learn optimal buzzing policies based on their own uncertainty, the questions, and their opponents’ history (He et al., 2016).14
With this data, we compute how models would fare against human players individually, players partitioned by skill, and in expectation (Section 7.1.2). In contrast to this strategy, crowdsourced tasks (e.g., squad) often use the accuracy of a single annotator to represent human performance, but this is problematic as it collapses the distribution of human ability to a single crowd-worker and does not accurately reflect a task’s upper bound compared to multiple annotation (Nangia and Bowman, 2019; Kwiatkowski et al., 2019). In the gameplay data, we have ample data with which to robustly estimate average and sub-group human skill; for example, 90,611 of the 131,075 questions have been played at least five times. This wealth of gameplay data is one aspect of qb’s strength for comparing humans and machines.
An additional aspect unique to trivia games is that participants are intrinsically motivated experts. Compensation—i.e., extrinsic motivation—in crowdsourcing is notoriously difficult. If they feel underpaid, workers do not give their best effort (Gneezy and Rustichini, 2000), and increasing pay does not always translate to quality (Mason and Watts, 2009). In light of this, Mason and Watts (2009) recommend intrinsic motivation, a proven motivator for annotating images (von Ahn and Dabbish, 2004) and protein folding (Cooper et al., 2010). Second, although multiple non-expert annotations can approach gold standard annotation, experts are better participants when available (Snow et al., 2008). Thus, other tasks may understate human performance with crowdworkers lacking proper incentives or skills.
Good quizbowlers are both accurate and quick. To measure skill, we compute and plot in Figure 7 the joint distribution of average player accuracy and buzzing position (percent of the question revealed). The ideal player would have a low average buzzing position (early guesser) and high accuracy; thus, the best players reside in the upper left region. On average, players buzzes with 65% of the question shown with 60% accuracy (Figure 7). Although there are other factoid qa and—more specifically—trivia datasets, qb is the first and only dataset with a large dataset of gameplay records which allows us to train models and run offline benchmarks.
3.4 Preprocessing
Before moving to model development, we describe necessary preprocessing to questions eliminate answer ambiguity, pair questions to gameplay data, and creating dataset folds that enable independent yet coordinated training of distinct guessing and buzzing models. Preprocessing is covered in significantly more detail in Appendix A.
14. In this article, we significantly expand the number of player-question records. We also make the setting significantly harder by not restricting questions to only the most frequently asked about answers (1K versus 24K). Finally, we create a new evaluation procedure (Section 7.1.2) that better estimates how models fare in the real-world versus human players. The first version of the gameplay dataset and models was introduced in: He He, Jordan Boyd-Graber, and Hal Daumé III. Opponent Modeling in Deep Reinforcement Learning. International Conference on Machine Learning, 2016.
15

Rodriguez, Feng, Iyyer, He, and Boyd-Graber

Accuracy

1 0.75 0.50 0.25
0 0

0.25

0.50

0.75

Average buzzing position

Density

Density

Density

0.6

0.4

0.2

0

2.5

5

7.5

10

Log number of records

2.5

2

1.5

1

0.5

0

0

0.25

0.50

0.75

1

Accuracy

3

2

1

0

0

0.25

0.50

0.75

Average buzzing position

Figure 7: Left: each protobowl user is represented by a dot, positioned by average accuracy and buzzing position; size and color indicate the number of questions answered by each user. Right: distribution of number of questions answered, accuracy, and buzzing position of all users. An average player buzzes with 65% of the question shown, and achieves about 60% accuracy.

Matching QB Answers to Wikipedia Pages Throughout this work we frame qb as a classification task over the set of Wikipedia page entities (Section 2.3), which necessarily requires pairing answers to distinct pages if on exists. We pair questions and their answers to Wikipedia pages in two steps: parsing potential answers from moderator instructions and matching to Wikipedia entities.15 In qb, the “answers” are in actuality instructions to the moderator that may provide additional detail on what answers are acceptable. For example, answer strings like “Second Vatican Council [or Vatican II]” indicate to accept either surface form of the same concept. Fortunately, the vast majority of these “answer instructions” are automatically parsable due to their semi-regular structure. The second step—described further in Appendix A.4—matches parsed answers to pages through a combination of strict textual matching, expert-curated matching rules (e.g., only match “camp” to Camp_(style) if “style” or “kitsch” are mentioned), and expert annotated pairings between questions and pages.16 In total, we paired 119,093 out of 132,849 with Wikipedia titles (examples in Appendix A.4).
Dataset Folds The goal of the folds in the qanta dataset is to standardize the training and evaluation of models for the guessing and buzzing sub-tasks. Towards this goal, we sub-divide the qanta dataset by sub-task and standard machine learning folds (e.g., training, development, and test). We create the standard machine learning folds by partitioning the
15. We preprocess the English Wikipedia 4/18/2018 dump with https://github.com/attardi/ wikiextractor.
16. Primarily, the authors of this article annotated the answer to page pairings.
16

Quizbowl

Fold
train + guess train + buzz dev + guess dev + buzz test + guess test + buzz unassigned
All

Number of Questions
96, 221 16, 706
1, 055 1, 161 2, 151 1, 953 13, 602
132, 849

Table 3: We assign each question in our dataset to either the train, development, or test fold. Questions in the development and test folds come from national championship tournaments which typically have the highest quality questions. The development and test folds are temporally separated from the train and development folds to avoid leakage. Questions in each fold are assigned a “guess” or “buzz” association depending on if they have gameplay data. Unassigned refers to questions for which we could not map their answer strings to Wikipedia titles or there did not exist an appropriate page to match to.

data according to tournament type and year. To increase the quality of evaluation questions, we only include questions from championship level tournaments in the development and test folds.17 To derive the final folds, we temporally divide the data (Arlot and Celisse, 2010) so that only (championship) questions from 2015 and onward are used in evaluation folds.
The subdivision by task simultaneously addresses the issue that some questions lack gameplay data (thus are not helpful for buzzer training) and partitioning the data so that the buzzer calibrates against questions unseen during training (details in Appendix A.3). Table 3 shows the size of each sub-fold; unassigned questions correspond to those where the answer to page matching process failed. Finally, hundreds of new qb questions are created every year which provides an opportunity for continually adding new training questions and replacing outdated test questions. Ultimately, this may help temper overconfidence in the generalization of models (Patel et al., 2008) since we expect there to be covariate shift, prior probability shift, and domain shift in the data (Quionero-Candela et al., 2009) as questions evolve to reflect modern events.
The qanta datasets, a copy of the Wikipedia data used, intermediate artifacts, and other related datasets are available at http://datasets.qanta.org.
4. Deciding When and What to Answer
One could imagine many machine learning models for playing qb: an end-to-end reinforcement learning model or a heavily pipelined model that determines category, answer type, answer, and decides when to buzz. Without making any value judgment on the right answer, our approach divides the task into two subsystems: guessing and buzzing (Figure 8). This
17. We use questions from acf Regionals, acf Nationals, acf Fall, pace nsc, and nasat from 2015 onward for development and test sets.
17

Rodriguez, Feng, Iyyer, He, and Boyd-Graber

Input
At its premiere, the librettist of this opera portrayed a character who asks for a glass of wine with his dying wish

Guesser

Guess: Cavellaria Rusticana Score: .0287

Output

Model
... For 10 points, name this Wolfgang Mozart Opera titled for an enchanted woodwind instrument.

Guesser

Guess: The Magic Flute Score: .997

Buzzer

Action: Wait

Buzzer

Action: Buzz

Figure 8: The qanta framework for playing Quiz Bowl with semi-independent guesser and buzzer models. After each word in the input is revealed the guesser model outputs its best guesses. The buzzer uses these in combination with positional and gameplay features to decide whether to take the buzz or wait action. The guesser is trained as a question answering system that provides guesses given the input text seen so far. Buzzers take on dual roles as calibrators of the guesser confidence scores and cost-sensitive decision classifiers by using the guesser’s score, positional features, and human gameplay data.

approach mirrors ibm Watson’s18 two model design (Ferrucci et al., 2010; Tesauro et al., 2013). The first model answers questions, and the second decides when to buzz. Dividing a larger task into sub-tasks is common throughout machine learning, particularly when the second is making a prediction based on the first’s prediction. For example, this design pattern is used in object detection (Girshick et al., 2014, generate bounding box candidates then classify them), entity linking (Ling et al., 2015, generate candidate mentions and then disambiguate them to knowledge base entries), and confidence estimation for automatic speech recognition systems (Kalgaonkar et al., 2015). In our factorization, guessing is based solely on question text. At each time step (word), the guessing model outputs its best guess, and the buzzing model determines whether to buzz or wait based on the guesser’s confidence and features derived from the game state. This factorization cleanly reduces the guesser to question answering while framing the buzzer as a cost-sensitive confidence calibrator.
This division of modeling labor makes it significantly easier to train the buzzer as a learned calibrator of the guesser’s softmax classifier predictions. This is crucial since the probabilities in neural softmax classifiers are unreliable (Guo et al., 2017). Like how we train a calibration model (buzzer) over a classifier (guesser), Corbière et al. (2019) train a calibration model on top of a image classification model which is a more effective approach in high dimensional spaces compared to nearest-neighbor based confidence measures (Jiang et al., 2018). However, not all buzzing errors are equal in severity; thus, part of the buzzer’s challenge is in incorporating cost-sensitive classification. By partioning model responsibilities into separate guessing and buzzing models, we can mitigate the calibration-based drawbacks of neural softmax classifiers while naturally using gameplay data for cost-sensitive decisionmaking.
Machines playing qb by guessing and buzzing semi-independently is also convenient from an engineering perspective: it simplifies model training and is easier to debug. More
18. In Watson, the second system also determines wagers on Daily Doubles, wagers on Final Jeopardy, and chooses the next question (e.g., history for $500)
18

Quizbowl
importantly, it allows us and subsequent researchers to focus on a sub-task of their choosing or the task as a whole. If you are interested in only question answering, focus on the guesser. If you are interested in multiagent cooperation or confidence estimation, focus on the buzzer. Following the discussion of our guessing (Section 5) and buzzing (Section 6) systems we describe our evaluations and results in Section 7.1. Section 8 summarizes the outcomes of our live, in-person, exhibition matches against some of the best trivia players in the world.
5. Guessing QB Answers
Guessing answers to questions is a factoid question answering task and the first step towards our models playing qb (Figure 8). We frame the question answering sub-task in qb as high dimensional multi-class classification over Wikipedia entities (i.e., answers are entities defined by distinct Wikipedia pages). This section describes three families of question answering models: information retrieval models (Section 5.1), linear models (Section 5.2), and neural models (Section 5.3). Despite distinct differences, these approaches share a common structure: create a vector representation x of the input question, create a vector representation for each candidate answer ai, and then return the answer Ai corresponding to arg maxi f (x, ai) where f is some similarity function.19
5.1 Explicit Pattern Matching with Information Retrieval
The first model family we discuss are traditional information retrieval (ir) models based on the vector space model (Salton et al., 1975). Vector space models are particularly effective when term overlap is a useful signal—as in factoid qb (Lewis et al., 2020). For example, although early clues avoid keyword usage, giveaways often include terms like “Wolfgang Mozart” and “Tamino” that make reaching an answer easier. Consequently, our vector space ir prove to be a strong baseline (Section 7.1).
To frame this as an ir search problem, we treat guessing as document retrieval. Input questions are search queries and embedded into a tf-idf (Jones, 1972; Rajaraman and Ullman, 2011) vector x. For each answer Ai ∈ Atrain in the qb training data, we concatenate all training questions with that answer into a document Di embedded as ai into the same vector space.20 The textual similarity function f is Okapi bm25 (Robertson and Walker, 1994) and scores answers ai against x. During inference, we return the answer Ai of the highest scoring document Di. We implement our model using Apache Lucene and Elastic Search (Gormley and Tong, 2015).
However, the ir model’s reliance on pattern matching often fails early in the question. For example, in the first sentence from Figure 1 the author intentionally avoids keywords (“a character who asks for a glass of wine with his dying wish”). Purely traditional ir methods, while effective, are limited since they rely on keywords and cannot “soft match” terms semantically. Thus, we move on to machine learning methods that address some of these shortcomings.
19. For brevity and clarity, we omit bias terms. 20. We also tested, one document per training example, different values for bm25 coefficients, and the default
Lucene practical scoring function.
19

Rodriguez, Feng, Iyyer, He, and Boyd-Graber

At its premiere, the librettist of this...

w0

w1

wk

Word Embeddings

v0

v1

vk

Composition Function (DAN, RNN, CNN...)

Fixed Size Representation h

Classiﬁer (Linear + Softmax)

Guess

Figure 9: All our neural models feed their input to an embedding function, then a composition function, and finally a classification function. The primary variation across our models is the choice of composition function used to compute a fixed, example-level representation from its variable length input.

5.2 Trainable Pattern Matching with Linear Models
In addition to the ir model, we also test a linear model baseline that reduces multi-class classification to one-versus-all binary classification. While an ir model derives term weights from corpus statistics and a hand-crafted weighting scheme, a one-versus-all linear model with one-hot term features x finds term weights that maximize the probability of the correct binary prediction for each answer. The input features x are derived from a combination of sparse n-grams and skip-grams.21 Since the number of classes is too high for standard one-versus-all multi-class classification,22 we instead use a logarithmic time one-versus-all model (Agarwal et al., 2014; Daumé et al., 2017). However, this model is limited since it only considers linear relationships between n-gram terms, the model uses–at best—local word order, and the sparse representation does not take advantage of the distributional hypothesis (Harris, 1954). Next we describe neural models that use more sophisticated forms of representation and composition to address these shortcomings.
5.3 Neural Network Models
The final family of methods we consider for qb question answering are neural methods. We describe the shared components of the neural models (e.g., general architectures and training details) and compare their composition functions.
In our model (Figure 9), we follow a widely used architecture in nlp to embed words independently in a vector space, contextualize their representations, temporally reduce representations, and then classify with a softmax layer (Collobert and Weston, 2008). The first component of the model embeds question q with k tokens into m-dimensional representations
21. The order of n-grams and skip-grams was determined by hyper parameter search 22. There are approximately 25,000 distinct answers.
20

Quizbowl

w = [w1, . . . , wk]. Next, a function c(·) : Rk×m → Rk×l contextualizes words as l-dimensional embeddings v = [v1, . . . , vk] = c(w). Since this is still a variable length sequence of representations and the classifier requires a fixed size representation, we use a reducer r(·) : Rk×m → Rn to derive an n-dimensional dense feature vector x = r(v). We call specific
pairs of contextualizers and reducers composition functions. The final model component—the classifier—computes logit scores si = xT · ai as the dot product between the features x and trainable answer embeddings ai. From this, we use the softmax to compute a probability distribution
exp(s) p = softmax(s) = ∑︁ki=1 exp(si) (1)
over answers and train the model with the cross entropy loss

k

∑︂

L = yi log(pˆi)

(2)

i

where yi = 1 for the true answer and yi = 0 otherwise. In our experiments, we evaluate three classes of composition functions (i.e., contextualizer-reducer pairs): unordered composition with deep averaging networks (Iyyer et al., 2015), recurrent network-based composition (Elman, 1990; Hochreiter and Schmidhuber, 1997; Palangi et al., 2016; Cho et al., 2014), and transformer-based composition (Vaswani et al., 2017; Devlin et al., 2019).

5.3.1 Unordered Composition with Deep Averaging Networks

Our first (unordered) neural composition function is the deep averaging network (dan). We introduced dans as a simple, effective, and efficient method for qb question answering.23

Despite their disregard of word order, dans are competitive with more sophisticated models

on classification tasks such as sentiment analysis (Iyyer et al., 2015). Although there are

cases where word order and syntax matter, many questions are answerable using only key

phrases. For example, predicting the mostly likely answer to the bag of words “inventor,

relativity, special, general” is easy; they are strongly associated with Albert Einstein.

All composition functions—such as dans—are fully described by the choice of contextu-

alizer and reducer. In dans, the contextualizer c is the identity function, and the reducer is

broken into two components. First, the dan averages word embeddings v to create an initial

hidden state

1

k
∑︂

h0 = k vi. (3)

i=1

The final fixed-size representation x = hz is computed with z feed-forward layers through

the recurrence

hi = gelu(Wi · hi−1 + bi)

(4)

where Wi and bi are parameters of the model and gelu is the Guassian Error Linear Unit (Hendrycks and Gimpel, 2016). Although dans are not the most accurate model, they are an attractive trade-off between accuracy and computation cost.

23. This article has new experiments comparing new composition functions and focuses on incorporating additional data. The dan first was introduced in: Mohit Iyyer, Varun Manjunatha, Jordan Boyd-Graber, and Hal Daumé III. Deep Unordered Composition Rivals Syntactic Methods for Text Classification. Association for Computational Linguistics, 2015.

21

Rodriguez, Feng, Iyyer, He, and Boyd-Graber

5.3.2 Ordered Composition
In contrast to dans, order-aware models like rnns, lstms, and grus can model long range dependencies in supervised tasks (Linzen et al., 2016). Since all these models belong to the family of recurrent models, we choose one variant to describe in terms of its associated contextualizer and reducer.24 In our model, the composition function

c(v) = gru(v)

(5)

is a multi-layer, bi-directional gru (Cho et al., 2014). The reducer

r(v) = [vk(forward); v0(backward)]

(6)

concatenates the final layer’s forward and backward hidden states. Combined, this forms the first ordered composition we test.
Transformer models, however, better represent context at the cost of complexity (Vaswani et al., 2017; Devlin et al., 2019). Specifically, we input the cls token, question, and sep token to uncased bert-base. Thus, the contextualizer

c(v) = bert(v)

(7)

is simply bert and the reducer

1

k
∑︂

r(v) = k vi (8)

i=1

is the average of the output states from the final layer associated with the question’s wordpiece tokens.25 Next, we move on from model descriptions to training specifics.

5.3.3 Training Details
In standard qa tasks, training over full questions is standard, but with qb’s incremental setup this results in less accurate predictions. If the example is the complete text, the model ignores difficult clues to focus on the “easy” part of the question, preventing learning from “hard” clues. Instead of each training example being one question, we use each of a question’s sentences as a singe training example. While this training scheme carries the downside that during training models may not learn long-range dependencies across sentences, the accuracy improvement outweighs the disadvantages. In addition to these two approaches, we also tested variable-length, but did not observe an improvement over the sentence-based training scheme.26
In non-transformer models we use 300-dimensional word embeddings initialized with glove for words in the vocabulary and randomly initialized embeddings otherwise.27 We regularize these models with dropout (Srivastava et al., 2014) and batch normalization (Ioffe and Szegedy, 2015). Loss functions were optimized with adam (Kingma and Ba, 2015) and
24. Hyper parameter optimization indicated that gru networks were the most accurate recurrent model. 25. We also tested using the cls token with worse results. 26. Variable length training creates k training examples from a question comprised of k sentences. Each
example includes the text from the start position up to and including sentence k. 27. Randomly initialized embeddings use a normal distribution with mean zero and standard deviation one.

22

Quizbowl
models were trained with early stopping, and learning rate annealing. All neural models were implemented in PyTorch (Paszke et al., 2019) and AllenNLP (Gardner et al., 2017).
We optimize hyper parameters by running each setting once and record the parameter settings corresponding to the top development set accuracy. The models with the best parameters are run an (additional) five times to create estimates of variance for each tracked metric (Section 7.1).
Although not exhaustive, these models are strong baselines for the question answering component of qb. Section 10 identifies areas for future modeling work; throughout the rest of this work however we focus on completing a description of our approach to playing qb by combining guessers and buzzer (Section 6). Following this we describe how we evaluate these systems independently (Section 7.1), jointly (Section 7.3), offline (Section 7.1.2), and live (Section 8).
6. Buzzing
Winning qb requires answering accurately with as little information as possible. It is crucial— for humans and computers alike—to accurately measure their confidence and buzz as early as possible without being overly aggressive. The first part of our system, the guesser, optimizes for guessing accuracy; the second part, the buzzer, focuses on deciding when to buzz. Since questions are revealed word-by-word the buzzer makes a binary decision at each word: buzz and answer with the current best guess, or wait for more clues.
The outcome of this action depends on the answers from both our guesser and the opponent.28 To make this clear, we review the game’s mechanics. If we buzz with the correct answer before the opponent can do so, we win 10 points; but if we buzz with an incorrect answer, we lose 5 points immediately, and since we cannot buzz again, the opponent can wait till the end of the question to answer, which might cost us 10 extra points in the competition.
Before we discuss our strategy to buzzing, consider a buzzer with perfect knowledge of whether the guesser is correct or not, but does not know anything about the opponent: a locally optimal buzzer. This buzzer would buzz as soon as the guesser gets the answer correct. A stronger buzzer exists: an omnipotent buzzer with perfect knowledge of what the opponent will do; it would exploit the opponent’s weaknesses: delay buzzing whenever an opponent might err. The agent would then get a higher relative reward: once from the opponent’s mistake and then for getting it correct.
The buzzer we develop in this paper targets a locally optimal strategy: we focus on predicting the correctness of the guesser and do not model the opponent. This buzzer is effective: it both defeats players in our gameplay dataset (Section 3.3) and playing against real human players (Section 8). The opponent modeling extension has been explored by previous work, and we discuss it in Section 9.
6.1 A Classification Approach to Buzzing
Given the initial formulation of buzzing as a mdp (Section 2.5), it would be natural to learn the task with reinforcement learning using the final score; however, we instead use a convenient
28. We use point values from the typical American format of the game. The exact values are unimportant, as they change the particulars of strategy but not the approach.
23

Rodriguez, Feng, Iyyer, He, and Boyd-Graber
reduction to binary classification. Since we can compute the optimal buzzing position easily as opposed to with expensive rollouts, we can reduce the problem to classification (Lagoudakis and Parr, 2003). At each time step, the model looks at the sequence of guesses that the guesser has generated so far, and makes a binary decision of whether to buzz or to wait. Under the locally optimal assumption, the ground truth action at each time step equals the correctness of the top guess: it should buzz if and only if the current top guess is correct. Another view of this process is that the buzzer is learning to imitate the oracle buzzing policy from the ground truth actions (Coates et al., 2008; Ross and Bagnell, 2010; Ross et al., 2011). Alternatively, the buzzer can also be seen as an uncertainty estimator (Hendrycks and Gimpel, 2017) of the guesser.
The guesses create a distribution over all possible answers. If this distribution faithfully reflects the uncertainty of guesses, the buzzer could be a simple “if–then” rule: buzz as soon as the guesser probability for any guess gets over a certain threshold. This threshold system is our first baseline, and we tune the threshold value on a held-out dataset.
However, this does not work because the confidence of neural models is ill-calibrated (Guo et al., 2017; Feng et al., 2018). Our neural network guesser often outputs a long tail distribution over answers concentrated on the top few guesses, and the confidence score of the top guess is often higher than the actual uncertainty (the chance of being correct). To counter these issues, we extract features from the top ten guesser scores train a classifier on top of them. Some important features include a normalized version of the top ten scores and the gap between them; the full list of features are in Appendix A.5.
There is also important temporal information; for example, if the guesser’s top prediction’s score steadily increases, this signals the guesser is certain about the top guess. Conversely, a fluctuating top prediction (the answer is Hope Diamond. . . no, I mean Parasaurolophus. . . no, I mean Tennis Court Oath) is a sign that perhaps the guesser is not that confident (regardless of the ostensible score). To capture this, we compare the current guesser scores with the previous time steps and extract features such as the change in the score associated with the current best guess, and whether the ranking of the current top guess changed in this time step. The full list of temporal features can be found in the Appendix A.5.
To summarize, at each time step, we extract a feature vector, including current and temporal features, from the sequence of guesses generated by the guesser so far. We implement the classifier with both fully connected Multi-layer Perceptron (mlp) and with Recurrent Neural Network (rnn). The classifier outputs a score between zero and one indicating the estimated probability of buzzing. Following the locally optimal assumption, we use the correctness of the top guess as ground truth action: buzz if correct and wait if otherwise We train the classifier with logistic regression; during testing, we buzz as soon as the buzzer outputs a score greater than 0.5. Both models are implemented in Chainer (Tokui et al., 2015); we use hidden size of 100, and lstm as the recurrent architecture. We train the buzzer on the “buzzertrain” fold of the dataset, which does not overlap with the training set of the guesser, for twenty epochs with Adam optimizer (Kingma and Ba, 2015). Both buzzers have test accuracy of above 80%, however, the classification accuracy does not directly translate into the buzzer’s performance as part of the pipeline, which we look at next.
24

Quizbowl
7. Offline Evaluation
A central thesis of our work is that the construction of qb questions lends itself to a fairer evaluation of both humans and machine qa models: to see who is better at answering questions, see who can answer the question first. However, this is often impractical during model development, especially if the questions are “new” (they have not been played by humans or computers). Moreover, a researcher might be uninterested in solving the buzzing problem. Offline evaluations where the guesser and buzzer are evaluated independently with static data strikes a balance between ease of model development and faithfulness to qb’s format. To address this, Section 7.1 describes the metrics to compare offline model accuracy. Following an error analysis (Section 7.2), Section 7.3 evaluates buzzing models by replacing this oracle buzzer with trained buzzing models.
7.1 Evaluating the Guesser
Ideally, we would compare systems in a head-to-head competition where the model (or human) who correctly buzzed and answered the most questions would win (Section 8). However, this involves live play, necessitates a buzzing strategy, and complicates evaluation of the guesser in isolation. Intuitively though, a model that consistently buzzes correctly earlier in the question is better than a model that buzzes late in the question. In our evaluations, we use three metrics that reflect this intuition: accuracy early in the question, accuracy late in the question, and the expected probability of beating a human assuming an optimal buzzing strategy.
7.1.1 Accuracy-Based Evaluation
The easiest and most common method for evaluating closed domain question answering methods is accuracy over all questions in the test set. We report two variants of this: (1) accuracy using the first sentence and (2) accuracy using the full question. While it is possible to answer some questions during the first sentence, it is the first and hardest position we can guarantee could be answered. Although we report accuracy on full questions, this metric is a minimum bar: the last clues are intentionally easy (Section 2.3). However, while start-of-question and end-of-question accuracy help development and comparison with other qa tasks, it is silent on human–computer comparison. We address this shortcoming next.
7.1.2 Expected Probability of Defeating Human Players
While comparing when systems buzz is the gold standard, we lack gameplay records for all test set questions, and it is unreasonable to assume it is easy to obtain them. Instead, marginalize over empirical human gameplay to estimate the probability π(t) that a human would have correctly answered a question by position t. Then, we combine this with model predictions and marginalize over t to obtain the expected probability of winning against an average player on an average gameplay question. A similar idea—to compute the expected probability of winning a heads up match—has also been used in machine translation (Bojar et al., 2013).
25

Rodriguez, Feng, Iyyer, He, and Boyd-Graber

Empirical Probability of Winning

1 0.75 0.50 0.25
0

0.25

0.50

0.75

Position in Question (%)

DaEtaxpSeocutrecde Wins Curve Score Average of Most Played Question with 215 Plays Question with 205 Plays Question with 198 Plays Question with 197 Plays Question with 177 Plays Question with 176 Plays Question with 173 Plays Question with 172 Plays Question with 167 Plays Question with 166 Plays
DaCtaurTvyepeScore | Average Single Question
1

Figure 10: We plot the expected wins score with respect to buzzing position (solid dark blue). For the ten most played questions in the buzztest fold we show the empirical distribution for each individual question (dotted lines) and when aggregated together (solid light blue). Among the most played questions, expected wins over-rewards early buzzes, but appropriately rewards end-of-question buzzes.

We compute the expected probability of winning (ew) in two steps. First, we compute

the proportion of players

π(t) = 1 − Nt ,

(9)

N

that have answered a question correctly by position t. N is the total number of questionplayer records and Nt is the number of question-player records where the player answered correctly by position t. We empirically estimate the expected probability of winning

π(t) = 0.0775t − 1.278t2 + 0.588t3

(10)

from the gameplay data as a cubic polynomial (Figure 10). At t = 0, the potential payoff is at its highest since no one has answered the question. At t = ∞, the potential payoff is at its lowest; all the players who would have correctly answered the question already have. If the computer gets the question right at the end, it would only score points against opponents who did not know the answer at all or answered incorrectly earlier in the question.
ew marginalizes over all questions q and all positions j, and counts how many times model m produced a guess g(m, q, j) that matched the answer of the question a(q). Specifically we compute

1

∞
∑︂ ∑︂

EW(m) = Em [pwin] = |Q| 1 [g(m, q, j) = a(q)] π (j) , (11)

q∈Q j=1

26

Quizbowl

Model

Accuracy (%)

Start

End

Dev

Test

Dev

Test

Top Mean Top Mean Top Mean Top Mean

E [pwin] Dev Test

Linear 2.56 2.56±0. 1.58 1.58±0. 11.9 11.9±0. 9.25 9.25±0. 6.62 4.96

ir

9.48

9.48 6.23

6.23 62.2

62.2 54.5

54.5 45.8 38.8

dan 10.7 10.4±0.3 8.28 7.88±0.3 60.0 59.1±0.9 51.0 51.4±1 42.6 35.5

rnn 10.5 9.46±0.7 7.86 7.78±0.4 52.3 51.8±1 46.4 45.9±0.9 27.6 23.3

bert 12.5 11.1±0.8 9.34 9.49±0.3 53.4 55.0±0.9 47.0 48.8±0.9 36.6 31.6

Table 4: We compare several models by accuracy at start-of-question, end-of-question, and ew. In the table, models are sorted by start-of-question development set accuracy. Standard deviations for non-ir models are derived from five trials; standard deviation is not reported for the ir model since it is deterministic.

where

1 |Q|

is

the

count

of

question–position

records.

The

indicator

function

is

exactly

an

oracle buzzer: it gives credit if and only if the answer is correct. However, this rewards

models with unstable predictions; for example, a model would be rewarded twice for a

sequence of predictions that were correct, wrong, and correct. We discourage this model

behavior by using a stable variant of ew which only awards points if the current answer

and all subsequent answers are correct. With this formalism, it is also straightforward to

compute the expected winning probability for any guesser-buzzer combination by replacing

the oracle buzzer (indicator function) with a function that equals one if the guess is correct

and the buzzer yielded a “buzz” decision. We compare buzzers in Section 6, but now move to

experimental results for the guesser.

7.1.3 Guesser Comparison Experiments
We evaluate our guessers using start accuracy, end accuracy, and expected wins (Table 4). All models struggle at the start of the question with the best accuracy at only 11%. This is unsurprising: the first sentence contains the most difficult clues and is difficult for even the best human players. Models fare significantly better near the end of the question with giveaway clues. However, even the best model’s 61% accuracy leaves much room for future work.
While the bert model has the best early-question accuracy, it lags behind the ir and dan for end of question accuracy. We suspect that order-aware models over-emphasize less important parts of the question; additionally, the gap between sentence training and full question inference advantages models that did not need to learn an aggregation over longer sequences. This pattern is also reflected in the ew scores;bert—as expected—outperforms the rnn model. Finally, across accuracy and ew we see substantial drops between the development and test sets, which suggests overfitting. Next, we investigate the errors models make.
27

Rodriguez, Feng, Iyyer, He, and Boyd-Graber

Bert: Wrong

IR: Wrong

Bert: Correct IR: Correct

Bert: Wrong

IR: Wrong

Bert: Correct IR: Correct
(a) Start of the question error comparison (b) End of the question error comparison
Figure 11: The bert and ir models are mostly wrong or correct on the same subset of questions. At the end of the question, most of the questions the bert model is correct on, the ir model is also correct on.

7.2 Identifying Sources of Error
This section identifies and characterizes several failure modes of our models. First we compare the predictions of blackbox neural models and ir model—an explicit pattern matcher (Section 7.2.1). Following this we identify data skew towards popular answers as a major source of error for less popular answers (Section 7.2.2). Lastly, we manually break down the test errors of one model (Section 7.2.3).
7.2.1 Behavioral Comparison of Neural and IR Models
One way to analyze black-box models like neural networks is to compare their predictions to better understood models like the ir model. If their predictions—and thus exterior behavior—are similar to a better understood model it suggests that they may operate similarly. Figure 11 shows that even the bert and ir models are correct and wrong on many of the same examples at end-of-question. Since one model—the ir model—is an explicit pattern matcher this hints that neural qb models learn to be pattern matcher as hinted by other work (Jia and Liang, 2017; Rajpurkar et al., 2018; Feng et al., 2018).
Next we investigate this pattern matching hypothesis at the instance-level. For our instance-level analysis we sample examples of correct and incorrect predictions. First we randomly sample a test question that all models answer correctly after the first sentence (Figure 12). This particular example has similar phrasing to a training example (“A holder of this title commissioned. . . miniatures”) so it is unsurprising that all models get it right.
In our second analysis, we focus on a specific answer (Turbulence) and its twenty-seven training questions. Figure 13 shows a sample question for this answer that the rnn model answered correctly but that ir model did not. The most frequent words in the training data
28

Quizbowl
Test Question (first sentence): A holder of this title commissioned a set of miniatures to accompany the story collection Tales of a Parrot. Training Question (matched fragment): A holder of this title commissioned Abd al-Samad to work on miniatures for books such as the Tutinama and the Hamzanama. Answer: Mughal Emperors
Figure 12: A test question that was answered correctly by all models after the first sentence; a normally very difficult task for both humans and machines. A very similar training example allows all models to answer the question through trivial pattern matching.
Test Question (first sentence): This phenomenon is resolved without the help of a theoretical model in costly DNS methods, which numerically solve for the rank-2 tensor appearing in the RANS equations. Answer: Turbulence Score (rnn): .0113 Synonym Attacks: phenomenon → event, model → representation
Figure 13: Only the rnn model answers this question correctly. To test the robustness of the model to semantically equivalent input modifications, we use sears-based (Ribeiro et al., 2018) synonym attacks and cause the model prediction to become incorrect. Although this exposes a flaw of the model, it is also likely that the low confidence score would likely lead a buzzer model to obstain; this highlights one benefit of implicitly incorporating confidence estimation into the evaluation.
for this answer are “phenomenon” (twenty-three times), “model” (seventeen times), “equation” (thirteen times), “numerically” (once), and “tensor” (once). In this analysis we removed or substituted these word with synonyms and then checked if the model’s prediction was the same.
Substituting words in this question shows that the model is over-reliant on specific terms. After removing the term “phenomenon,” the model changed its answer to Ising model (a mathematical model of ferromagnetism). If we instead substitute the term with synonyms such as “occurrence”, “event”, and “observable event” the answers are still incorrect. Similarly, if “model” is replaced by “representation” the rnn also makes incorrect predictions. At least for this question, the model is not robust to these semantics-preserving modifications (Ribeiro et al., 2018). Next we move to aggregate error analysis.
7.2.2 Errors Caused by Data Sparsity
For many test set answers, scarcity of training data is a significant source of error. Most egregiously, 17.9% of test questions have zero corresponding training examples. Beyond these questions, many more answers have few training examples. While some topics are frequently asked about, one goal of question writers is to introduce new topics for students
29

Rodriguez, Feng, Iyyer, He, and Boyd-Graber

Number of Answers

10000 1000 100
10 1

Electromagnetism

Romanian_language Treaty_ofQ_uPaereiqs_u(e1g763) Violin_sonata

Garbage_collection_(computer_science) Freyja Bose Einstein_condensate Golgi_apparatus Thor
Japan

1

2

3 45

10

25

50

100

Number of Training Examples per Answer

Figure 14: The distribution of training examples per unique answer is heavily skewed. The most frequent answer (Japan) occurs about 100 times. Nearly half of the questions have one training example and just over sixty percent have either one or two training examples.

to learn from. For example, although physics is a common general topic, Electromagnetism has only been an answer to one qb question. The distribution of training examples per unique answers is skewed (Figure 14), and countries—like Japan—are asked about much more frequently. Unsurprisingly, plotting the number of training examples per test question answer versus model accuracy shows significant drops in accuracy for about half of the test questions (Figure 15).
7.2.3 Error Breakdown
We conclude our error analysis by inspecting and breaking down the errors made by the rnn model at the start and end of questions. Of the 2, 151 questions in the test set, 386 have zero training examples leaving 1, 765 questions that are answerable by our models. Of the remaining questions, the rnn answers 1, 540 incorrectly after the first sentence and 481 at the end of the question. To avoid errors likely due to data scarcity, we only look to questions with at least 25 training examples; the number of errors on this subset at the start and the end of the question is 289 and 36. Table 5 lists reasons for model errors on a random sample of 50 errors from the start of the question and all 36 errors from the end of the question.
The predominant sources of error are when the model predicts the correct answer type (e.g., person, country, place), but chooses the incorrect member of that type. This accounts for errors such as choosing the wrong person, country, place, or event. The rnn especially confuses countries; for example, in Figure 16, it confuses the Spain and the United States, the parties to the Adam Onis Treaty. The relative absence of incorrect answer type errors at the end of questions may be attributable to the tendency of late clues including the answer type (such as “name this country. . . ”).
30

Quizbowl

MoBdEeRl T DAN IR RNN VW

Start

End

Test Accuracy

0.4
5Q0%ueostfioTnesst
0.2

5Q0%ueostfioTnesst

0

0 25 50 75

0 25 50 75

N or Fewer Training Examples

Figure 15: The more an answer is asked in the training set, the easier it is for all models, both at the start and end of the question. This is a significant source of errors since accuracy on at least 50% of test questions—those with seven or less training examples—is significantly lower for all models.

In the process of manual error breakdown we also found five annotation errors where the assigned Wikipedia answer did not match the true answer. This low number of errors further validates the robustness of our answer mapping process.
7.3 Evaluating the Buzzer
We first evaluate our buzzer against the locally optimal buzzer which buzzes as soon as the guesser gets the answer correct. However, this can be overly ambitious and unrealistic since the guesser is not perfectly stable: it can get the answer correct by chance, then vacillate between several candidates before settling down to the correct answer. To account for this instability, we find the first position that the guesser stabilizes to the correct answer and set it as the optimal buzzing position. In other words, we compare against the optimal buzzer used in the definition of stable ew score. To be exact, we start at the last position that the guess is correct, go backwards until the guess is incorrect and consider this the locally
31

Rodriguez, Feng, Iyyer, He, and Boyd-Graber

Error Reason
Wrong Country Wrong Person Wrong Place Wrong Type Wrong Event Nonsense Annotation

Start Count
11 16 1 15 0 7 1

End Count
17 2 5 5 1 2 4

Table 5: The table is an error breakdown for questions with at least twenty-five training examples. To analyze errors at the start of questions, we randomly sampled fifty errors and for end of question took all thirty-six errors. End of question errors are primarily wrong country errors as in Figure 16 where the model answers United States instead of Spain. Errors at the start of the question though are more diverse. The most common error is guessing the correct answer type, but not the specific member of that type; examples of this error class include answering Albert Einstein instead of Alan Turing, or Iowa instead of Idaho.

Test Question: This country seized four vessels owned by Captain John Meares, which were registered in Macau and disguised with Portuguese flags, starting a dispute over fishing rights. To further negotiations with this country, Thomas Jefferson signed the so-called “Two Million Dollar Act.” This country agreed not to police a disputed spot of land, which was subsequently settled by outlaws and “Redbones”, and which was called the “Neutral Ground.” This country was humiliated by England in the Nootka Crisis. Harman Blennerhassett’s farm on an island in the Ohio River was intended as the launching point of an expedition against this European country’s possessions in a plan exposed by James Wilkinson. This country settled navigation rights with the United States in Pinckney’s Treaty, which dealt with the disputed “West” section of a colony it owned. For 10 points, name this European country which agreed to the Adams-Onis Treaty handing over Florida. Guess: United States Answer: Spain
Figure 16: Although the answer to this question is Spain, many of the terms and phrases mentioned are correlated with the United States. Thus, the rnn model answers United States instead of the correct answer Spain. This is one of many examples where the model answers with the correct answer type (country), but incorrect member of that type.

optimal buzzing position; we set the ground truth to all positions before this to zero, and all positions after it to one.
We use the same guesser (rnn) in combination with different buzzers (Table 6), and quantitatively compare their expected wins (Section 7.1.2). Both mlp and rnn buzzers win against the static threshold baseline, but there is a considerable gap between rnn and the optimal buzzer.
32

Quizbowl

This instrument plays the only extended solo in the overture to Verdi’s 1 Luisa

Miller. This is the solo instrument in a piece that opens with the movement “The

Perilous Shore”. This instrument introduces the main theme to “The Pines of

Janiculum” from Respighi’s The Pines of Rome. This instrument has a long solo at

the beginning of the Adagio from Rachmaninoff’s Second Symphony, and it first

states the Shaker theme in Copland’s Appalachian Spring. John Adams’ Gnarly

Buttons is for this instrument. Heinrich Baermann, a virtuoso on this instrument,

was the dedicatee of Carl Maria von Weber’s two concertos for it. The basset

horn is a variant of, for 10 points, what 2 single-reed woodwind instrument,

which plays a notable glissando at the opening of Gershwin’s 3 Rhapsody in

Blue?

Answer: Clarinet Optimal Buzz: Correct:

Wrong:

Threshold Buzz: Bassoon MLP Buzz: Bassoon RNN Buzz: Clarinet

Human Buzzes: 1 Violin, 2 Obo, 3 Flute, Clarinet

Figure 17: In this question, the Threshold and mlp buzzers are too aggressive and buzz before the guesser’s answer is correct. In contrast, the rnn is more conservative and buzzes shortly after the optimal point which is—by a wide margin—still earlier than the earliest (correct) human buzz .

Model
Threshold mlp rnn
Optimal

acc
0.840 0.849 1.0

ew
0.013 0.272 0.302 0.502

Score
-9.98 -2.31 -1.01 2.19

Table 6: The accuracy (acc), expected wins (ew), and qb score (Score) of each buzzer on the validation set. Both mlp and rnn outperform the static threshold baseline by a large margin, but there is still a considerable gap from the optimal buzzer.

Low expected wins means the buzzer is either too aggressive or not aggressive enough. To characterize their weaknesses, we compare the buzzers’ behavior over time (Figure 18). The static threshold buzzer is too aggressive, especially early in the questions as is also seen in Figure 17. This behavior to some extent resonates with the observation that the confidence of neural models needs calibration (Guo et al., 2017). The difference between mlp and rnn is small but rnn is less likely to be overly aggressive early in the question.
For a more fine-grained analysis, we simulate games where our system plays against individual human players using the gameplay dataset (Section 3.3). Based on the guesser, we classify questions as “possible” or not. If the guesser gets the answer correct before the opponent answers, it is possible for the buzzer to win the question. Otherwise, it is impossible for the buzzer to do anything to beat the opponent. Based on this categorization, Figure 19 further breaks down the outcomes: the rnn is less likely to be overly aggressive in both possible and impossible cases.
33

Rodriguez, Feng, Iyyer, He, and Boyd-Graber

Frequency

1 Threshold MLP RNN 0.75 0.50 0.25
0 0 0.25 0.50 0.75 1 0 0.25 0.50 0.75 1 0 0.25 0.50 0.75 1
Position

Buzzing
Both Neither Only buzzer Only optimal

Figure 18: Comparing buzzers’ behavior over time against the optimal buzzer. The red crossed area and dotted blue area combined indicates when the buzzer thinks that the guesser is correct, the other two combined when the buzzer thinks the guesser is wrong. The red (crossed) and orange (unhatched) areas combined indicates when the buzzer matches the optimal buzzer. Our goal is to maximize the red areas and minimize the blue areas. The static threshold baseline is overly aggressive, especially at earlier positions in the question (large dotted blue area); mlp and rnn both behaves reasonably well, and the aggressiveness of rnn is slightly more balanced early on in the question.

Possibility

True False
True False
True False
0

300

600

900

Count

RNN

Threshold MLP

Outcome
15 10 5 0 -5 -10 -15

Figure 19: Breaking down the buzzer’s performance on the individual question level. Impossible question means there is nothing the buzzer can do to beat the opponent. It is clearer that rnn performs better than mlp, making fewer mistakes of being overly aggressive.

8. Live Exhibition Events
No amount of thorough experimentation and analysis of machine learning systems can match the public interest and rubber-meets-the-road practicalities of live matches between humans and machines. ibm’s Watson in Jeopardy! (Ferrucci et al., 2010), Deep Blue in chess (hsiung Hsu et al., 1995), and Google’s AlphaGo in Go (Silver et al., 2016) were both tremendous
34

Quizbowl
scientific achievements and cultural watersheds. In the case of chess and Go, they transformed how the games are played through insight gained from the collaboration of humans and machines. Lastly, although our offline evaluation is reasonable, a live evaluation verifies that the two correspond since that is not always clear (Hersh et al., 2000).
In a similar spirit, we have hosted eight live events since 2015 where we showcase our research to the public by having humans and machines compete against each other.29 Except for our nips 2015 Best Demonstration against ml researchers, our system’s opponents have been strong trivia players. Their achievements include victories in numerous national qb championships (high school and college), Jeopardy!, and similar trivia competitions.
Our inaugural event in 2015 at the qb High School National Competition Tournament (hsnct) pitted an early and vastly different version of our system against a team of tournament organizers in a match that ended in a tie.30 Later that year, a similar system defeated Ken Jennings of Jeopardy! fame at the University of Washington, but lost convincingly (145–345) at hsnct 2016. The subsequent year at hsnct 2017, our redesigned system narrowly defeated its opponents (260–215). This system was used an ir guesser with rnn buzzer combined with a question type classifier (Li and Roth, 2002).31 Although this impressive result appears to follow the trend of machines improving until they defeat skilled humans, it is far from the whole story.
In parallel with these events, we hosted events where teams—humans and machines—were selected from open competition. Our first of these style events was hosted as part of a naacl 2016 workshop on question answering. Before the event local high school teams competed against each other, and researchers submitted their machine systems which also played simulated matches against each other. At the event the best human and machine teams played against each other with the high school team defeating an early version of Studio ousia’s system (Yamada et al., 2017, 2018).32 In 2017, we hosted a similar workshop at nips where an improved version of ousia’s system yet again defeated its machine competition, but this time also defeated the invited human team.
Events and collaborations like these show that qb is more than just another question answering task. By engaging with the qb community to digitize qb questions in a machinereadable form we not only made our research possible, but started the ecosystem of tools that students now rely on to practice with before competitions. In the next step towards deeper collaboration with this community we are building ways for humans and machines to cooperate in competition (Feng and Boyd-Graber, 2019) and in writing questions (Wallace et al., 2019). We accomplish all this while simultaneously providing ways for students of all ages to engage with and benefit from research through our live exhibition events.
9. Related Work
Quizbowl is a question answering and sequential decision-making task. This section begins by positioning qb—as a qa task—amongst early and more contemporary qa and reading
29. Videos of our events are available at http://events.qanta.org. 30. Our software did not handle ties correctly and terminated instead of playing tiebreaker questions. 31. In absolute terms, the type classifier did not improve accuracy; however, in our matches we display the
top five scoring guesses and the type classifer improved the “plausibility” of that list. 32. The ousia system embeds words and entities separately, and uses a dan-based architecture over these.
35

Rodriguez, Feng, Iyyer, He, and Boyd-Graber
comprehension tasks including those that have shifted towards multi-step or adversarial questions. Following this, we make move towards the decision-making aspect of qb by first discussing connections with model calibration and prediction under domain shift. Having discussed the uncertainty in model confidence scores, we finally discuss handling uncertainty caused by opponents through opponent modeling.
9.1 Question Answering Datasets
qa and rc datasets vary significantly in form and focus; here we focus on factoid qa and reading comprehension tasks.
The least complex types of questions are often called “simple questions” since they can be answered by a single simple fact. For example, SimpleQuestions (Bordes et al., 2015) as specifically designed so that questions can be answered using one knowledge-base triplet, and WikiMovies (Miller et al., 2016) automatically generates questions from knowledge base triplets. Similarly, WebQuestions (Berant et al., 2013) uses the Google Suggest api to collect questions containing specific entities and crowdworkers only answer questions using Freebase facts. Despite the relative ease in creating these datasets, they lack complexity and linguistic diversity, leading near-complete solutions for SimpleQuestions (Petrochuk and Zettlemoyer, 2018). In qb, the capability of answering simple question like these is a bare minimum requirement and takes form as the final, “giveaway” clues at the end of questions which even novice (human) players usually answer correctly.
Humans have played trivia games and tournaments for decades (Boyd-Graber and Börschinger, 2020) and as a result there are ample non-qb sources of questions. The most famous example—Jeopardy!—was converted into the Searchqa dataset (Dunn et al., 2017). Other trivia-based qa datasets include TriviaQA which is built from fourteen trivia sites (Joshi et al., 2017) and Quasar-T which is built from questions collected by a reddit user (Dhingra et al., 2017). While some of these are paired with potentially useful supporting evidence, a hallmark of these datasets and qb is that the question alone unambiguously identifies an answer.
However, providing supporting documents to answer questions is another popular way to frame qa tasks. The best examples of tasks with unambiguous questions and verified supporting documents are Trecqa (Voorhees and Tice, 2000) and NaturalQuestions (Kwiatkowski et al., 2019). In these tasks, the questions—user queries from search engines—were written without knowledge of any particular supporting documents and afterwards annotators attempted to find appropriate supporting documents. Although Triviaqa provides potentially relevant documents, they are not human verified; similarly, ms marco (Nguyen et al., 2016), WikiReading (Hewlett et al., 2016), and Newsqa (Trischler et al., 2017) also provide unverified supporting documents. squad in particular falls outside this paradigm since—in general—questions are dependent on the selected context paragraph.
Another set of tasks focuses on creating questions that require multiple supporting documents and multi-step reasoning. In qb, early clues are often a composition of multiple facts about the answer. For example, to guess “Die Zauberflöte” from “At its premiere, the librettist of this opera portrayed a character who asks for a glass of wine with his dying wish”, one would have to combine two pieces of text from Wikipedia: “Papageno enters. The priests grant his request for a glass of wine and he expresses his desire for a wife.” and “Emanuel
36

Quizbowl
Schikaneder, librettist of Die Zauberflöte, shown performing in the role of Papageno”. While this work focuses on tossup qb questions, a second type of qb question—bonuses—emphasize this multi-step aspect through multi-part questions (Elgohary et al., 2018).
Multi-step reasoning datasets primarily differ by providing ground-truth annotations to sufficient supporting documents. In Wikihop (Welbl et al., 2018), multi-hop questions are automatically constructed from Wikipedia, Wikidata, and WikiReading. HotPotQA (Yang et al., 2018) follows a similar structure, but question text is crowdsourced rather than automatically generated. However, show Min et al. (2019) that although these questions are meant to be only solvable by using multi-step reasoning, that many are answerable with single-hop reasoning. Subsequent datasets like qasc (Khot et al., 2019), drop (Dua et al., 2019), and break (Wolfson et al., 2020) focus on creating questions that are much more likely to require multi-step reasoning through adversarial annotation. However, multi-step questions are not the only way to make questions more difficult.
Adversarial question authoring and adversarial filtering (Zellers et al., 2018) are other ways to increase difficulty. The general framework of adversarial authoring filters questions either during or after annotation by whether or not a strong baseline answers it correctly. For example, Wallace et al. (2019) show that when qb question writers—while authoring a question—are shown what a model would answer and why, that they create questions that are significantly more difficult for machines while being no more difficult for human players. Along similar lines, Bartolo et al. (2020) let writers see what a model would answer, but iteratively create new adversarial questions, re-train the model, and collect new adversarial questions anew. Although contrast sets explicitly do not have a model in the loop (Gardner et al., 2020a), they are similarly intended to challenge models through example perturbations. Other effective example perturbations for automatically creating adversarial examples include adding sentences to context paragraphs (Jia and Liang, 2017) and in general semantic-preserving permutations (Ribeiro et al., 2018). The common thread in all these works is to make questions more difficult for machines so that models continue to improve.
9.2 Answer Triggering and Model Calibration
Just as in qb’s buzzer, in real-world applications of machine learning it is important to know when to trust a model’s confidence in its predictions (Jiang et al., 2012). In qa, knowing when to answer is known as answer triggering (Yang et al., 2015; Rajpurkar et al., 2018) and the core task—correctly estimating model confidence—is model calibration (Zadrozny and Elkan). Having machines that accurately convey their confidence is doubly important since humans have the unfortunate tendency to place too much trust in machines (Sundar, 2007). Despite this, the rise of deep learning seems to have only made this more challenging (Guo et al., 2017; Feng et al., 2018). Fortunately, recent work has made progress in this problem by making connections to out-of-domain detection (Kamath et al., 2020). Along similar lines, the buzzing task in qb partially dependent on having well-calibrated models.
9.3 Opponent Modeling
qb is far from the only game where players benefit from modeling opponent behavior. Opponent modeling is particularly important in games with hidden information—in qb
37

Rodriguez, Feng, Iyyer, He, and Boyd-Graber
this takes the form of the yet-to-be-revealed question and the per-question skill of the opponent. One classic example of opponent modeling under uncertainty is Poker (Billings et al., 1998) where identifying and adapting to opponents is central to the game. In games like Scrabble, opponent behavior can even be used to infer hidden information—such as their remaining tiles—to make it easier to anticipate and counter their strategy (Richards and Amir, 2007). Similarly, in real time strategy games the opponent’s strategy is often hidden and hierarchically structured (Schadd et al., 2007); they may play aggressive—similar to aggressive buzzing—making defensive play more advantageous. Traditionally, qb is played in teams so a full model should account for your own team’s skills as well as the mixture of opponent skills (e.g., certain players may be better at history questions). Although this is unaddressed in qb, this general framing has been considered in games where players compete with each other for limited resources, but are advantaged by doing so semi-cooperatively (Von Der Osten et al., 2017). One potential area of future work is in focusing on opponent modeling in qb with an emphasis on accounting for teams of players and strategies that evolve over the course of a match (e.g., play less conservatively when trailing).
10. Future Work
Here we identify research directions that are particularly well suited to the qanta dataset and qb. Rather than focus on general research directions in question answering we identify areas that best take advantage of qb’s unique aspects. Specifically we focus on directions using its interruptible and pyramidal nature, the perpetual influx of new and diverse questions from annual tournaments, and the supportive community interested in making qb even more supportive of scholastic learning and achievement.
10.1 Generalization in Factoid Question Answering
Although some syntactic forms in qb may be overly specific, its ever-growing size and diversity makes it attractive for studying more generalizable factoid question answering. The reasons for this are twofold. First, as discussed in Section 3, the qanta dataset is already diverse in topics, syntax, and in range of answers (over twenty-five thousand considering all data folds). Second and more importantly though the dataset is growing year over year. Since 2007 the size of the dataset has over quadrupled, and the growth shows no sign of slowing down. Figure 20 shows this growth over the past twenty years.
As the dataset continues to grow it will demand that machines and humans broaden their knowledge of past events while also updating their knowledge with current events. Every year presents an opportunity to test both how well models generalize to novel questions and how well they generalize to questions about current events. For example, in a 2017 exhibition match our model missed a question about the company responsible for driving down rocket launch costs (SpaceX); a phenomenon which only manifested itself several years prior. With this ever present influx of new questions every year we believe this opens new research directions in testing the generalization of models.
38

Quizbowl

120000

QuTaonttaitlyQuestions Distinct Answers

Count up to Year (inclusive)

90000

60000

30000

0 1997 1999 2001 2003 2005 2007 2009 2011 2013 2015 2017 Year
Figure 20: The growth of the qanta dataset in number of questions and number of distinct answers over the past twenty years starting in 1997. The dataset has grown by at least 5,000 questions every year since 2010. All questions with matched answers are included, and we construct the plot by using the tournament year of each question. Independently, participation in qb (and thus number of students writing questions) has roughly doubled every year since 2008.
10.2 Few Shot Learning and Domain Adaptation
One well-suited research direction with qb is adopting methods from domain adaptation and few-shot learning for factoid question answering. A large source of error in our models is a scarcity of training examples for most answers (see Figures 14 and 15). Zero and few-shot learning in nlp and Computer vision face similar challenges of using shared structure to improve the use of the scarce training data (Xian et al., 2018). Correcting errors induced by the scarcity of training examples is an exciting research direction. From (frustratingly) easy methods like simple feature augmentation with source and target domain features (Daume III, 2007; Kim et al., 2016) to more sophisticated adversarial methods (Chen and Cardie, 2018) there are a variety of options investigate.
10.3 Robust, Explainable and Trustable Machine Learning
Developing robust (Goel et al., 2021), explainable (Belinkov and Glass, 2019) and trustable (Mitchell et al., 2019) machine learning systems overlaps with the goals of machine learning researchers
39

Rodriguez, Feng, Iyyer, He, and Boyd-Graber
and the qb community. For example, continually evolving, robustness-centric evaluations33 would pair well with the annual writing of new questions. A concrete direction for future work is to integrate adversarial question authoring (Wallace et al., 2019; Bartolo et al., 2020) into these dynamic evaluations. While these works focus on creating explicitly adversarial examples, an alternative approach to creating difficult questions is to expand diversity of questions and answers since they are naturally difficult for qa models (Section 7.2.2). Ideally, nlp models should eventually be robust to adversarial and diverse inputs, but that likely requires improving both data and models.
Humans can improve at games like Chess and Go by learning from machines; similarly, if models are explainable qb players can learn from and cooperate with machines as well. For example, Feng and Boyd-Graber (2019) built and evaluated interpretations of machine learning models based on how effective they were at improving human live play. A related research direction would be to use these interfaces and insights about models of human learning—such as the effectiveness of spaced repetition (Ebbinghaus, 1885)—to improve knowledge retention as Settles and Meeder (2016) do for language learning. Both of these directions fuse work in human-computer interaction and interpretation of machine learning algorithms.
Our collaborative research in qb thus far is only a beginning. qb supports work in factoid question answering and sequential decision-making. We list several challenges in these based on our experiments, but there are certainly more. Beyond playing qb, there are opportunities in human-in-the-loop research that can improve interpretations of machine learning models while producing useful artifacts such as adversarial datasets. We hope that our work in establishing the qanta datasets and qb as a machine learning task empowers others to contribute to these and other future research.
11. Conclusion
This article introduces and argues for qb: an incremental question answering task. Solving qb questions requires sophisticated nlp such as resolving complex coreference, multi-hop reasoning, and understanding the relationships between the plethora of entities that could be answers. Fundamental to answering qb questions is that the questions are incremental; this is both fun and good for research. It is fun because it allows for live, engaging competitions between humans and computers. This format—the product of refining human question answering competitions over decades—is also good for research because it allows for fair, comprehensive comparison of systems and iterative improvement as systems answer questions earlier and earlier.
To evaluate systems we use three methods: offline accuracy-based metrics adapted to the incremental nature of qb, simulated matches against machines and humans, and live exhibition matches. Although the best models have sixty percent accuracy at the end of questions, this is well below the best players, and the first clues remain particularly challenging.
Improving qb models can incorporate many commonplace tasks in nlp other than question answering. Reasoning about entities can be improved through better named entity
33. Platforms like https://dynabench.org and https://robustnessgym.com propose frameworks for improving robustness evaluations.
40

Quizbowl
recognition, entity linking, coreference and anaphora resolution. Some of the more difficult clues in qb however presume that the player has read and integrated the content of books such as important plot points. Further work in reading comprehension and summarization could help answer some of these questions. At a more general level, the extraction of information from external knowledge sources (such as books or Wikipedia) is important since the distribution of training examples per answer is heavily skewed and some new questions ask about current events. Improving qb models requires and can further motivate advances in these tasks and others.
However, the benefits to research go beyond format or specific sub-tasks, and extend to our symbiotic collaboration with the public. Exhibition matches double as outreach events and opportunities to put machine systems to the test on previously unseen questions. Another area of active research is in collaborating with the qb community to further improve the quality of questions for humans and machines alike. Writers are empowered with machine learning tools to discover bad clues which helps create questions more interesting to humans that consequently better test the generalization of systems. In this the goals of the qb and communities align; we both seek to create datasets that discriminate different levels of language and knowledge understanding.
Beyond the specific qb community, live exhibitions also serve the general public. Exhibition games demonstrate what nlp and ml systems can do and what they cannot. When the tricks that computers use to answer question are revealed to lay audiences, some of the mystique is lost, but it can also encourage enthusiasts to investigate our techniques to see if they can do better. Our open data and code facilitates this open competition, and the qb community helps make it fun and engaging.
qb isn’t just another dataset or task; it is a rich platform for nlp research that co-evolves with the qb community. From the digitization of qb questions to our online interface which created and popularized online qb play. From cooperative play between humans and machines to providing tools for better writing better question we have built a symbiotic relationship that continues to yield productive research while moving the qb community forward. We hope that new unforeseen research directions will continue emerging from this collaboration while simultaneously giving back to the qb community through new and exciting ways of engaging with state-of-the-art research in machine learning and natural language processing.
Acknowledgments
Rodriguez and Boyd-Graber are supported by National Science Foundation Grants IIS1320538 and IIS1822494. Feng is supported under subcontract to Raytheon BBN Technologies by DARPA award HR001-15-C-0113. Amazon Web Services Cloud Credits for Research provided computational resources that supported many of our experiments and internal infrastructure. Any opinions, findings, conclusions, or recommendations expressed here are those of the authors and do not necessarily reflect the view of the sponsor.
Many individuals contributed their ideas and work to this paper. Davis Yoshida was influential in developing a Wikipedia data augmentation method (eventually subsumed by bert) and contributed to many insightful discussions. Others who have been intellectually involved in our work with qb include Brianna Satinoff, Anupam Guha, Danny Bouman,
41

Rodriguez, Feng, Iyyer, He, and Boyd-Graber
Varun Manjunatha, Hal Daume III, Leonardo Claudino, and Richard Socher. We also thank the members of the clip lab at the University of Maryland for helpful discussion.
Next, we appreciate those that improved this paper through comments, edits, and reference suggestions on the manuscript. In particular, we thank Yogarshi Vyas, Joseph Barrow, Alvin Grissom, Hal Daumé III, Philip Resnik, and Kianté Brantley for their feedback.
Nathan Murphy and R. Robert Hentzel have been incredibly supportive of using qb for outreach with the public; they have multiple times hosted our exhibition events at the high school national championship tournaments. We also thank the participants that played against machine systems in these events; these include Ophir Lifshitz, Auroni Gupta, Jennie Yang, Vincent Doehr, Rahul Keyal, Kion You, James Malouf, Rob Carson, Scott Blish, Dylan Minarik, Niki Peters, Colby Burnett, Ben Ingram, Alex Jacob, and Kristin Sausville. Ken Jennings played against our system at an event hosted by Noah Smith at the University of Washington. Ikuya Yamada and Studio ousia entered their systems which competed against human teams at several of our events.
Finally, this work would never have been possible without the support of the qb community. We are grateful to the original authors of questions, and those who helped us collect our current dataset. The maintainers of quizdb.org and protobowl.com allowed us to use their websites to build our dataset. The first versions of work in qb used a dataset collected by Shivaram Venkataraman and Jerry Vinokurov. These first versions used questions from the qb Academic Competition Federation. National Academic Quiz Tournaments, LLC provided access to their proprietary questions which we used in prior iterations of our systems.
Appendices
Appendix A. Preprocessing
This section provides a detailed description of preprocessing on the question dataset.
A.1 Aligning and De-duplicating Questions
Since we obtain machine-readable versions of questions from two online sources it is necessary to ensure that we do not include the same question twice. We use the metadata associated with each question such as tournament and year. As part of our preprocessing we manually align the values of these fields.34 We use these fields to ensure that questions for each tournament and year are included only once.
A.2 Textual Preprocessing
Models should define their own textual preprocessing so we only preprocess the text to remove qb specific artifacts. Most of these artifacts are instructions to the moderator or organizer such as “MODERATOR NOTE:”, “Description Required”, “15 pts:”, or a reference to the category of the question; we use regular expression rules to remove these. Since
34. We also align category and sub-category fields.
42

Quizbowl
we report results on accuracy after the first sentence in questions we also provide a set of canonical sentence tokenization indices computed using spacy.35
A.3 Fold Assignment
We divide qanta dataset questions into training, development, and test folds based on the competitiveness and year of the source tournament. Since championship tournaments typically have the highest quality questions we use questions from championship tournaments 2015 and onward as development and test sets. All other questions are used as the training set.
Table 3 shows the divisions of each fold; each train, dev, or test fold is assigned to be used for either determining what to answer (guessing) or when to answer (buzzing). Questions in “guess” folds are used for developing question answering systems as in Section 5. Questions in the “buzz” folds are used for developing agents that decide when to answer as in Section 6.
When we assign folds to qb questions we aim to create useful splits for guessing and buzzing while preserving the integrity of the development and test sets. Namely, when we create test and development folds we make the division into folds not depend on whether or not gameplay data exists. If it were the case that by making this unconditional assignment the number of questions with gameplay data is too small this would be a problem. We do not find this to be a problem however.
For test set questions this is easily accomplished by using an implicit quality filter and a temporal split; use only questions from national championship tournaments, questions from 2016 are used in the buzzing test set, and questions from 2017 and 2018 are used for the guessing test set. Following this we pair the test fold for buzzing with gameplay data, and are fortunate that the number of questions is not small.
To create the development sets we use questions from 2015 which are randomly split with equal probability into guessing and buzzing specific folds. Similarly to the test set we associate gameplay data after this assignment occurs to preserve its integrity against any bias that conditioning on having gameplay data would have.
For the training data we make a weaker attempt to eliminate bias in favor of ensuring that the training folds for guessing and buzzing are large enough. We first divide the training questions with an 80/20 split. Questions in the eighty percent split are assigned to the guessing fold. Each remaining question is assigned to the buzzing fold if it has gameplay data, otherwise it is assigned to the guessing fold. Figure 3 shows the result of this folding procedure.
A.4 Matching QB Answers to Wikipedia Pages
The automatic rule based part of this process is composed of two phases: an expansion phase in that produces variants of the answer text, and a match phase that determines when one of these variants is a match to a Wikipedia page. The rules in the expansion phase can be as simple as exact text match to expanding “The {Master of Flémalle} or Robert {Campin}” to “{Master of Flémalle}” and “Robert {Campin}”. In this case, multiple matches result in “Robert Campin” being the answer page: after removing braces “The Master of Flémalle”
35. https://spacy.io
43

Rodriguez, Feng, Iyyer, He, and Boyd-Graber

Original qb Answer Nora Helmer {Gauss}’s law for the electric field Thomas Hutchinson linearity {caldera}s William Holman {Hunt} {plasma}s {Second Vatican Council} [or {Vatican II}] {Jainism} {Electronegativity} Hubert Selby, Jr. (The) Entry of Christ into Brussels (accept equivalents due to translation) Depictions of Speech [accept equivalents] stress

Matched Wikipedia Page A_Doll’s_House No Mapping Found Thomas_Hutchinson_(governor) Linearity Caldera William_Holman_Hunt Plasma_(physics) Second_Vatican_Council Jainism Electronegativity Hubert_Selby_Jr. Christ’s_Entry_Into_Brussels_in_1889
No Mapping Found Stress_(mechanics)

Table 7: A random sample of qb answer strings and their matched Wikipedia pages. Answer mappings are easy to obtain accurately since most failures in exact matching are due to qb specific syntax that can be accounted for by rule based matching. Combined with manual annotation to find common non-exact matches, this process succeeds on 119,093 of 132,849.

Wikipedia redirects to “Robert Campin” and “Robert Campin” is also an exact match. These rules are incredibly effective at finding answers buried in qb specific notation such as the random sample in Table 7. When matches disagree we use the match that modified the original answer text the least.
There are inevitably cases where the automatic system fails to find a match, or finds the wrong match. Qualitatively these are often caused by disambiguation errors such as failing to differentiate between “Guernica” the city versus the painting by Picasso, small differences in answer strings, and when there is no suitable Wikipedia page. To correct or verify these errors we (the authors), and skilled members of the QB community (such as tournament organizers and participants from our exhibition matches) manually annotated a significant fraction of the training data, and all the test data.
Rather than doing manual annotation of each question, we begin by defining mappings of answer strings to Wikipedia pages so that when that string occurs multiple times it does not require manual annotation for every occurrence of that answer in questions. However, this has the serious drawback that if the answer string is ambiguous then it may result in mislabeled answers. To avoid this problem we design a manual process whereby annotators update three sets of answer-to-Wikipedia mappings: unambiguous, ambiguous, and direct mappings.
44

Quizbowl
Unambiguous annotations contain a list of answer strings that when seen map to a specific Wikipedia page. As the name implies, we only insert annotations here when the answer unambiguously identifies the corresponding Wikipedia page. Ambiguous annotations similarly contain a list of answer strings, but are paired with a list of disambiguation words. If the answer string is seen, at least one word is in the question text, and there are no other ambiguous matches, then it is mapped. For example, if the answer string is “amazon” and the question contains the word “river” then we assume “Amazon river” is the correct page while if the question mentions “bezos” then the correct page is “Amazon (company)”. Finally, direct mappings match the answer for specific questions.
The last major design decision in this process addresses how we prevent information from the test data to leak into the training data. The root of the data leak issue is that the distribution of answers between training and test data often results in only approximately 80% of test set answers occurring in the training data. We observed this phenomena empirically in both our data and the distribution of answers from our numerous exhibition events. If all answer strings are naively combined, then mapped, this implies that the training data will be biased towards its answers containing an over abundance of test set answers. A major difference between this and prior versions of the qanta dataset is finding and fixing this issue.
We correct this error by separating the answer string pool for training and test questions. Although this results in more annotation work, it avoids information leakage. While reviewing our annotation procedure we noticed another source of bias. Recall that we do not exhaustively annotate the training data. In our initial annotation we did not fully annotate the test data, and by doing so introduced a bias towards easier-to-annotate questions in the test set. To eliminate this bias—and make it as similar to playing a qb tournament as possible—we annotated every question in the test set.36
A.5 Buzzer features
The guesser updates its list of guesses whenever a new word of the qb question is revealed. At each time step, the buzzer extracts features from both the current and all past guesses and predict whether the current guess is correct. It is important to include past guesses as the dynamics of the guesser’s confidence contains strong signal about its correctness: the guesser usually starts with some random guess when little information is provided, then fluctuates between several plausible answers—just as humans do, and finally stablizes to a single answer, at which point the buzzer should buzz. Below is the full list of buzzer features we use in the experiments:
• Probabilities of the current top 3 guesses • Change of top 3 probabilites from the previous step • Gaps between probabilities of the top 3 guesses • Binary indicator of whether each of the top 5 guesses increased its ranking from previous
step • Mean and variance of probabilities of the current top 3 guesses • Mean and variance of probabilities of the previous top 3 guesses
36. Specifically, we either pair each test set answer strings with a Wikipedia title or mark it as not having a corresponding Wikipedia title.
45

Rodriguez, Feng, Iyyer, He, and Boyd-Graber

Category

N Percent

Pop Culture History Science Other Social Science Geography Religion Literature Philosophy Fine Arts

55

40%

26

19%

20

15%

13 9.6%

7 5.1%

6 4.4%

2 1.5%

5 3.7%

1 0.74%

1 0.74%

Total w/ Category 136 100%

No Category

14

Total

150

Table 8: A breakdown of NaturalQuestion example topics using qb categories. Most questions are about pop culture and the distribution includes many fewer questions about Literature and Fine Arts.

Appendix B. Natural Questions Categories
Section 3.2.1 analyzes the topical diversity of qb questions and makes comparisons to NaturalQuestions. To compare to NaturalQuestions—which does not have category labels— we annotated a random subset of 150 questions using qb’s categories (Table 8).
References
Alekh Agarwal, Olivier Chapelle, Miroslav Dudík, and John Langford. A reliable effective terascale linear learning system. Journal of Machine Learning Research, 15:1111–1133, 2014.
Sylvain Arlot and Alain Celisse. A survey of cross-validation procedures for model selection. Statistics surveys, 4:40–79, 2010. ISSN 1935-7516. doi: 10.1214/09-SS054. URL https: //projecteuclid.org/euclid.ssu/1268143839.
David Baber. Television Game Show Hosts: Biographies of 32 Stars. McFarland, 2015. ISBN 9781476604800.
Max Bartolo, Alastair Roberts, Johannes Welbl, Sebastian Riedel, and Pontus Stenetorp. Beat the AI: Investigating adversarial human annotation for reading comprehension. Transactions of the Association for Computational Linguistics, 8:662–678, 2020. doi: 10.1162/tacl\_a\_00338. URL https://www.aclweb.org/anthology/2020.tacl-1.43.
Yonatan Belinkov and James R. Glass. Analysis methods in neural language processing: A survey. Transactions of the Association for Computational Linguistics, 7:49–72, 2019.
46

Quizbowl
Jonathan Berant, Andrew Chou, Roy Frostig, and Percy S. Liang. Semantic parsing on freebase from question-answer pairs. In Proceedings of Empirical Methods in Natural Language Processing, 2013.
Darse Billings, Denis Papp, Jonathan Schaeffer, and Duane Szafron. Opponent modeling in poker. 1998.
Ondřej Bojar, Christian Buck, Chris Callison-Burch, Christian Federmann, Barry Haddow, Philipp Koehn, Christof Monz, Matt Post, Radu Soricut, and Lucia Specia. Findings of the 2013 Workshop on Statistical Machine Translation. pages 1–44, August 2013. URL https://www.aclweb.org/anthology/W13-2201/.
Antoine Bordes, Nicolas Usunier, Sumit Chopra, and Jason Weston. Large-scale simple question answering with memory networks. arXiv preprint arXiv:1506.02075, 2015.
Christina Boyce-Jacinoand Simon DeDeo. Opacity, obscurity, and the geometry of questionasking. arXiv preprint arXiv:1809.08291, 2018.
Jordan Boyd-Graber and Benjamin Börschinger. What question answering can learn from trivia nerds. In Proceedings of the Association for Computational Linguistics, 2020.
Jordan Boyd-Graber, Brianna Satinoff, He He, and Hal Daumé III. Besting the quiz master: Crowdsourcing incremental classification games. In Proceedings of Empirical Methods in Natural Language Processing, 2012.
Noam Brown and Tuomas Sandholm. Superhuman AI for multiplayer poker. Science, 365 (6456):885–890, August 2019. ISSN 0036-8075, 1095-9203. doi: 10.1126/science.aay2400. URL http://dx.doi.org/10.1126/science.aay2400.
Nicholas Carlini and David Wagner. Adversarial examples are not easily detected: Bypassing ten detection methods. In Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security, AISec ’17, pages 3–14, New York, NY, USA, November 2017. Association for Computing Machinery.
Xiaoyong Chai, Lin Deng, Qiang Yang, and Charles X. Ling. Test-cost sensitive naive bayes classification. Fourth IEEE International Conference on Data Mining (ICDM’04), pages 51–58, 2004.
Danqi Chen and Wen-Tau Yih. Open-Domain question answering. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts, 2020.
Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. Reading wikipedia to answer open-domain questions. In Proceedings of the Association for Computational Linguistics, 2017.
Xilun Chen and Claire Cardie. Multinomial adversarial networks for multi-domain text classification. In Conference of the North American Chapter of the Association for Computational Linguistics, 2018.
47

Rodriguez, Feng, Iyyer, He, and Boyd-Graber
Xiao Cheng and Dan Roth. Relational inference for wikification. In Proceedings of Empirical Methods in Natural Language Processing, 2013.
Kyunghyun Cho, Bart van Merrienboer, Dzmitry Bahdanau, and Yoshua Bengio. On the properties of neural machine translation: Encoder-decoder approaches. In Proceedings of Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, 2014.
Charles L A Clarke, Gordon V Cormack, and Thomas R Lynam. Exploiting redundancy in question answering. In Proceedings of the ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’01, pages 358–365, New York, NY, USA, September 2001. Association for Computing Machinery. ISBN 9781581133318. doi: 10.1145/383952.384024. URL https://doi.org/10.1145/383952.384024.
Adam Coates, Pieter Abbeel, and Andrew Y Ng. Learning for control from multiple demonstrations. In Proceedings of the International Conference of Machine Learning, 2008.
Ronan Collobert and Jason Weston. A unified architecture for natural language processing: deep neural networks with multitask learning. In Proceedings of the International Conference of Machine Learning, 2008.
Seth Cooper, Firas Khatib, Adrien Treuille, Janos Barbero, Jeehyung Lee, Michael Beenen, Andrew Leaver-Fay, David Baker, Zoran Popović, and Foldit Players. Predicting protein structures with a multiplayer online game. Nature, 466(7307):756–760, August 2010.
Charles Corbière, Nicolas Thome, Avner Bar-Hen, Matthieu Cord, and Patrick Pérez. Addressing failure prediction by learning model confidence. In Proceedings of Advances in Neural Information Processing Systems, 2019.
Hal Daumé, Nikos Karampatziakis, John Langford, and Paul Mineiro. Logarithmic time one-against-some. In Proceedings of the International Conference of Machine Learning, 2017.
Hal Daume III. Frustratingly easy domain adaptation. In Proceedings of the Association for Computational Linguistics, 2007.
J Deng, W Dong, R Socher, L Li, Kai Li, and Li Fei-Fei. ImageNet: A large-scale hierarchical image database. In Computer Vision and Pattern Recognition, 2009.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. 2019.
Bhuwan Dhingra, Kathryn Mazaitis, and William W. Cohen. Quasar: Datasets for question answering by search and reading. arXiv preprint arXiv:1707.03904, 2017.
Emily Dinan, Samuel Humeau, Bharath Chintagunta, and Jason Weston. Build it break it fix it for dialogue safety: Robustness from adversarial human attack. In Proceedings of Empirical Methods in Natural Language Processing.
48

Quizbowl
Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. Drop: A reading comprehension benchmark requiring discrete reasoning over paragraphs. In Conference of the North American Chapter of the Association for Computational Linguistics, 2019.
Matthew Dunn, Levent Sagun, Mike Higgins, V. Ugur Güney, Volkan Cirik, and Kyunghyun Cho. Searchqa: A new q&a dataset augmented with context from a search engine. arXiv preprint arXiv:1704.05179, 2017.
H. Ebbinghaus. Memory: a contribution to experimental psychology. Teachers College, Columbia University, New York, NY, USA, 1885.
Ahmed Elgohary, Chen Zhao, and Jordan Boyd-Graber. Dataset and baselines for sequential open-domain question answering. In Proceedings of Empirical Methods in Natural Language Processing, 2018.
Charles Elkan. The foundations of cost-sensitive learning. In International Joint Conference on Artificial Intelligence, IJCAI’01, pages 973–978, San Francisco, CA, USA, August 2001. Morgan Kaufmann Publishers Inc. ISBN 9781558608122. URL https://dl.acm.org/ doi/10.5555/1642194.1642224.
Jeffrey L. Elman. Finding structure in time. Cognitive Science, 14:179–211, 1990.
Allyson Ettinger, Sudha Rao, Hal Daumé, III, and Emily M Bender. Towards linguistically generalizable NLP systems: A workshop and shared task. In Proceedings of the First Workshop on Building Linguistically Generalizable NLP Systems, pages 1–10, Stroudsburg, PA, USA, 2017. Association for Computational Linguistics.
Shi Feng and Jordan Boyd-Graber. What can ai do for me: Evaluating machine learning interpretations in cooperative play. In International Conference on Intelligent User Interfaces, 2019.
Shi Feng, Eric Wallace, Alvin Grissom II, Mohit Iyyer, Pedro Rodriguez, and Jordan BoydGraber. Pathologies of neural models make interpretations difficult. In Proceedings of Empirical Methods in Natural Language Processing, 2018.
Shi Feng, Eric Wallace, and Jordan Boyd-Graber. Misleading failures of partial-input baselines. In Proceedings of the Association for Computational Linguistics, 2019.
David A. Ferrucci, Eric W. Brown, Jennifer Chu-Carroll, James Fan, David Gondek, Aditya Kalyanpur, Adam Lally, J. William Murdock, Eric Nyberg, John M. Prager, Nico Schlaefer, and Christopher A. Welty. Building watson: An overview of the deepqa project. AI Magazine, 31:59–79, 2010.
Francis Fukuyama. Confucianism and democracy. Journal of Democracy, 6(2):20–33, 1995. doi: 10.1353/jod.1995.0029. URL https://doi.org/10.1353/jod.1995.0029.
Meredith D Gall. The use of questions in teaching. Review of educational research, 40 (5):707–721, December 1970. ISSN 0034-6543. doi: 10.3102/00346543040005707. URL https://doi.org/10.3102/00346543040005707.
49

Rodriguez, Feng, Iyyer, He, and Boyd-Graber
Matt Gardner, Joel Grus, Mark Neumann, Oyvind Tafjord, Pradeep Dasigi, Nelson F. Liu, Matthew Peters, Michael Schmitz, and Luke S. Zettlemoyer. Allennlp: A deep semantic natural language processing platform. 2017.
Matt Gardner, Yoav Artzi, Victoria Basmov, Jonathan Berant, Ben Bogin, Sihao Chen, Pradeep Dasigi, Dheeru Dua, Yanai Elazar, Ananth Gottumukkala, Nitish Gupta, Hannaneh Hajishirzi, Gabriel Ilharco, Daniel Khashabi, Kevin Lin, Jiangming Liu, Nelson F Liu, Phoebe Mulcaire, Qiang Ning, Sameer Singh, Noah A Smith, Sanjay Subramanian, Reut Tsarfaty, Eric Wallace, Ally Zhang, and Ben Zhou. Evaluating models’ local decision boundaries via contrast sets. In Proceedings of Empirical Methods in Natural Language Processing, 2020a. doi: 10.18653/v1/2020.findings-emnlp.117. URL https://www.aclweb.org/anthology/2020.findings-emnlp.117.
Matt Gardner, Jonathan Berant, Hannaneh Hajishirzi, Alon Talmor, and Sewon Min. Question answering is a format; when is it useful? In Proceedings of the Association for Computational Linguistics, 2020b.
Mor Geva, Yoav Goldberg, and Jonathan Berant. Are we modeling the task or the annotator? an investigation of annotator bias in natural language understanding datasets. In Proceedings of Empirical Methods in Natural Language Processing, 2019.
R Girshick, J Donahue, T Darrell, and J Malik. Rich feature hierarchies for accurate object detection and semantic segmentation. In Computer Vision and Pattern Recognition, 2014.
Uri Gneezy and Aldo Rustichini. Pay enough or don’t pay at all *. The quarterly journal of economics, 115(3):791–810, August 2000.
Karan Goel, Nazneen Rajani, Jesse Vig, Samson Tan, Jason Wu, Stephan Zheng, Caiming Xiong, Mohit Bansal, and Christopher Ré. Robustness gym: Unifying the NLP evaluation landscape. January 2021. URL http://arxiv.org/abs/2101.04840.
Clinton Gormley and Zachary Tong. Elasticsearch: The Definitive Guide. " O’Reilly Media, Inc.", 2015.
Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the v in vqa matter: Elevating the role of image understanding in visual question answering. International Journal of Computer Vision, 127:398–414, 2017.
Anupam Guha, Mohit Iyyer, Danny Bouman, and Jordan Boyd-Graber. Removing the training wheels: A coreference dataset that entertains humans and challenges computers. In Conference of the North American Chapter of the Association for Computational Linguistics, 2015.
Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger. On calibration of modern neural networks. In Proceedings of the International Conference of Machine Learning, 2017.
Bob Harris. Prisoner of Trebekistan: a decade in Jeopardy! 2006.
Zellig S Harris. Distributional structure. Word, 10(2-3):146–162, 1954.
50

Quizbowl
He He, Jordan L. Boyd-Graber, Kevin Kwok, and Hal Daumé III. Opponent modeling in deep reinforcement learning. In Proceedings of the International Conference of Machine Learning, 2016.
Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). 2016.
Dan Hendrycks and Kevin Gimpel. A baseline for detecting misclassified and out-ofdistribution examples in neural networks. In Proceedings of the International Conference on Learning Representations, 2017.
Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. Natural adversarial examples. July 2019.
Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. Teaching machines to read and comprehend. In Proceedings of Advances in Neural Information Processing Systems, 2015.
William Hersh, Andrew Turpin, Susan Price, Benjamin Chan, Dale Kramer, Lynetta Sacherek, and Daniel Olson. Do batch and user evaluations give the same results? In Proceedings of the ACM SIGIR Conference on Research and Development in Information Retrieval, 2000.
Daniel Hewlett, Alexandre Lacoste, Llion Jones, Illia Polosukhin, Andrew Fandrianto, Jay Han, Matthew Kelcey, and David Berthelot. Wikireading: A novel large-scale language understanding task over wikipedia. In Proceedings of the Association for Computational Linguistics, 2016.
Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural Computation, 9: 1735–1780, 1997.
Feng hsiung Hsu, Murray Campbell, and A. Joseph Hoane. Deep blue system overview. In Proceedings of the 9th International Conference on Supercomputing, pages 240–244, 1995.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In Proceedings of the International Conference of Machine Learning, 2015.
Mohit Iyyer, Jordan Boyd-Graber, Leonardo Claudino, Richard Socher, and Hal Daumé III. A neural network for factoid question answering over paragraphs. In Proceedings of Empirical Methods in Natural Language Processing, 2014.
Mohit Iyyer, Varun Manjunatha, Jordan Boyd-Graber, and Hal Daumé III. Deep unordered composition rivals syntactic methods for text classification. In Proceedings of the Association for Computational Linguistics, 2015.
Robin Jia and Percy Liang. Adversarial examples for evaluating reading comprehension systems. In Proceedings of Empirical Methods in Natural Language Processing, 2017.
Heinrich Jiang, Been Kim, Melody Guan, and Maya Gupta. To trust or not to trust a classifier. In Proceedings of Advances in Neural Information Processing Systems, 2018.
51

Rodriguez, Feng, Iyyer, He, and Boyd-Graber
Xiaoqian Jiang, Melanie Osl, Jihoon Kim, and Lucila Ohno-Machado. Calibrating predictive model estimates to support personalized medicine. Journal of the American Medical Informatics Association: JAMIA, 19(2):263–274, March 2012.
Karen Spärck Jones. A statistical interpretation of term specificity and its application in retrieval. Journal of Documentation, 28(1):11–21:133,525, 1972.
Mandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke Zettlemoyer. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In Proceedings of the Association for Computational Linguistics, 2017.
K Kalgaonkar, C Liu, Y Gong, and K Yao. Estimating cdonfidence scores on ASR results using recurrent neural networks. In 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2015.
Amita Kamath, Robin Jia, and Percy Liang. Selective question answering under domain shift. In Proceedings of the Association for Computational Linguistics, 2020.
Divyansh Kaushik and Zachary Chase Lipton. How much reading does reading comprehension require? a critical investigation of popular benchmarks. In Proceedings of Empirical Methods in Natural Language Processing, 2018.
Tushar Khot, Peter Clark, Michal Guerquin, Paul Edward Jansen, and Ashish Sabharwal. Qasc: A dataset for question answering via sentence composition. Association for the Advancement of Artificial Intelligence, 2019.
Young-Bum Kim, Karl Stratos, and Ruhi Sarikaya. Frustratingly easy neural domain adaptation. In Proceedings of International Conference on Computational Linguistics, 2016.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Proceedings of the International Conference on Learning Representations, 2015.
A. Korin. Fenomen “Chto? Gde? Kogda?”. Eksmo, 2002.
Volodymyr Kuleshov and Stefano Ermon. Estimating uncertainty online against an adversary. In Association for the Advancement of Artificial Intelligence, volume 31, 2017.
Julian Kupiec. Murax: A robust linguistic approach for question answering using an on-line encyclopedia. In Proceedings of the ACM SIGIR Conference on Research and Development in Information Retrieval, 1993.
Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Matthew Kelcey, Jacob Devlin, Kenton Lee, Kristina N. Toutanova, Llion Jones, Ming-Wei Chang, Andrew Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural questions: a benchmark for question answering research. Transactions of the Association for Computational Linguistics, 2019.
Michail G Lagoudakis and Ronald Parr. Reinforcement learning as classification: Leveraging modern classifiers. In Proceedings of the International Conference of Machine Learning, 2003.
52

Quizbowl
S K Lam, D Pennock, Dan Cosley, and S Lawrence. 1 billion pages = 1 million dollars? mining the web to play “who wants to be a millionaire?”. In Proceedings of the Nineteenth Conference on Uncertainty in Artificial Intelligence, 2003.
Patrick Lewis, Pontus Stenetorp, and Sebastian Riedel. Question and answer Test-Train overlap in Open-Domain question answering datasets. 2020. URL http://arxiv.org/ abs/2008.02637.
Xin Li and Dan Roth. Learning question classifiers. In Proceedings of International Conference on Computational Linguistics, 2002.
Dekang Lin and Patrick Pantel. Discovery of inference rules for question-answering. Natural Language Engineering, 7:343–360, 2001.
Xiao Ling, Sameer Singh, and Daniel S Weld. Design challenges for entity linking. Transactions of the Association for Computational Linguistics, 3:315–328, December 2015.
Tal Linzen, Emmanuel Dupoux, and Yoav Goldberg. Assessing the ability of lstms to learn syntax-sensitive dependencies. Transactions of the Association for Computational Linguistics, 4:521–535, 2016.
Daniel J Lizotte, Omid Madani, and Russell Greiner. Budgeted learning of Naive-Bayes classifiers. In Proceedings of the Nineteenth Conference on Uncertainty in Artificial Intelligence, 2003.
Paul Lujan and Seth Teitler. Writing good quizbowl questions: A quick primer. https: //www.ocf.berkeley.edu/~quizbowl/qb-writing.html, October 2003. Accessed: 201812-04.
Subash Maddipoti. Subash maddipoti’s tips on question writing. https://acf-quizbowl. com/documents/subash-maddipotis-tips-on-question-writing/, March 2012. Accessed: 2018-12-04.
Kenny Malone. How uncle Jamie broke Jeopardy. Planet Money, (912), May 2019.
Travis Mandel, Yun-En Liu, Sergey Levine, Emma Brunskill, and Zoran Popovic. Offline policy evaluation across representations with applications to educational games. In Proceedings of the 2014 international conference on Autonomous agents and multi-agent systems, 2014.
Winter Mason and Duncan J Watts. Financial incentives and the “performance of crowds”. In Proceedings of the ACM SIGKDD Workshop on Human Computation, HCOMP ’09. Association for Computing Machinery, 2009.
P Melville, M Saar-Tsechansky, F Provost, and R Mooney. An expected utility approach to active feature-value acquisition. In Fifth IEEE International Conference on Data Mining (ICDM’05), pages 4 pp.–, November 2005. doi: 10.1109/ICDM.2005.23. URL http://dx.doi.org/10.1109/ICDM.2005.23.
53

Rodriguez, Feng, Iyyer, He, and Boyd-Graber
Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct electricity? a new dataset for open book question answering. In Proceedings of Empirical Methods in Natural Language Processing, 2018.
Alexander H. Miller, Adam Fisch, Jesse Dodge, Amir-Hossein Karimi, Antoine Bordes, and Jason Weston. Key-value memory networks for directly reading documents. In Proceedings of Empirical Methods in Natural Language Processing, 2016.
Sewon Min, Eric Wallace, Sameer Singh, Matt Gardner, Hannaneh Hajishirzi, and Luke S. Zettlemoyer. Compositional questions do not necessitate multi-hop reasoning. 2019.
Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, and Timnit Gebru. Model cards for model reporting. In Proceedings of the conference on fairness, accountability, and transparency, pages 220–229, 2019.
Nikita Nangia and Samuel R Bowman. Human vs. muppet: A conservative estimate of human performance on the GLUE benchmark. In Proceedings of the Association for Computational Linguistics, 2019.
LLC National Academic Quiz Tournaments. Naqt | press guide. https://www.naqt.com/ nationals/press-guide.jsp, 2020. Accessed: 2020-03-31.
Vincent Ng. Supervised noun phrase coreference research: The first fifteen years. In Proceedings of the Association for Computational Linguistics, 2010.
Khanh Nguyen and Brendan O’Connor. Posterior calibration and exploratory analysis for natural language processing models. In Proceedings of Empirical Methods in Natural Language Processing, Stroudsburg, PA, USA, 2015. Association for Computational Linguistics. doi: 10.18653/v1/D15-1182. URL http://aclweb.org/anthology/D15-1182.
Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and Li Deng. MS MARCO: A human generated machine reading comprehension dataset. In Proceedings of the NIPS Workshop on Cognitive Computation: Integrating neural and symbolic approaches, 2016.
Yixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and Douwe Kiela. Adversarial NLI: A new benchmark for natural language understanding. In Proceedings of the Association for Computational Linguistics, 2020.
Yaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, D Sculley, Sebastian Nowozin, Joshua V Dillon, Balaji Lakshminarayanan, and Jasper Snoek. Can you trust your model’s uncertainty? evaluating predictive uncertainty under dataset shift. In Proceedings of Advances in Neural Information Processing Systems, 2019.
Hamid Palangi, Li Deng, Yelong Shen, Jianfeng Gao, Xiaodong He, Jianshu Chen, Xinying Song, and Rabab Kreidieh Ward. Deep sentence embedding using long short-term memory networks: Analysis and application to information retrieval. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 24:694–707, 2016.
54

Quizbowl
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems 32, pages 8024–8035. Curran Associates, Inc., 2019. URL http://papers.neurips.cc/paper/ 9015-pytorch-an-imperative-style-high-performance-deep-learning-library. pdf.
Kayur Patel, James Fogarty, James A. Landay, and Beverly L. Harrison. Investigating statistical machine learning as a tool for software development. In International Conference on Human Factors in Computing Systems, 2008.
Michael Petrochuk and Luke S. Zettlemoyer. Simplequestions nearly solved: A new upperbound and baseline approach. In Proceedings of Empirical Methods in Natural Language Processing, 2018.
Joaquin Quionero-Candela, Masashi Sugiyama, Anton Schwaighofer, and Neil D Lawrence. Dataset shift in machine learning. The MIT Press, 2009.
Howard Raiffa. Decision analysis: introductory lectures on choices under uncertainty. 1968. URL https://psycnet.apa.org/fulltext/1968-35027-000.pdf.
Anand Rajaraman and Jeffrey David Ullman. Data Mining, page 1–17. Cambridge University Press, 2011. doi: 10.1017/CBO9781139058452.002.
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100, 000+ questions for machine comprehension of text. In Proceedings of Empirical Methods in Natural Language Processing, 2016.
Pranav Rajpurkar, Robin Jia, and Percy Liang. Know what you don’t know: Unanswerable questions for squad. In Proceedings of the Association for Computational Linguistics, 2018.
Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. Semantically equivalent adversarial rules for debugging nlp models. In Proceedings of the Association for Computational Linguistics, 2018.
Mark Richards and Eyal Amir. Opponent modeling in scrabble. In International Joint Conference on Artificial Intelligence, 2007.
Stephen E. Robertson and Steve Walker. Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval. In Proceedings of the ACM SIGIR Conference on Research and Development in Information Retrieval, 1994.
Stephane Ross and Drew Bagnell. Efficient reductions for imitation learning. In Proceedings of Artificial Intelligence and Statistics, 2010.
55

Rodriguez, Feng, Iyyer, He, and Boyd-Graber
Stephane Ross, Geoffrey Gordon, and Drew Bagnell. A reduction of imitation learning and structured prediction to No-Regret online learning. In Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics, 2011.
Dan Roth, Heng Ji, Ming-Wei Chang, and Taylor Cassidy. Wikification and beyond: The challenges of entity and concept grounding. In Proceedings of the Association for Computational Linguistics, 2014.
Abhijit Roy. (In)visible Publics: Television and Participatory Culture in India, pages 201– 221. 01 2016. ISBN ISBN-10: 1138666718 ISBN-13: 978-1138666719. doi: 10.4324/ 9781315095196-11.
G Salton, A Wong, and C S Yang. A vector space model for automatic indexing. Communications of the ACM, 18(11):613–620, November 1975.
Frederik Schadd, Sander Bakkes, and Pieter Spronck. Opponent modeling in Real-Time strategy games. 2007.
Burr Settles and Brendan Meeder. A trainable spaced repetition model for language learning. In Proceedings of the Association for Computational Linguistics, 2016.
Wei Shen, Jianyong Wang, and Jiawei Han. Entity linking with a knowledge base: Issues, techniques, and solutions. IEEE Transactions on Knowledge and Data Engineering, 27: 443–460, 2015.
David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George van den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Vedavyas Panneershelvam, Marc Lanctot, Sander Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy P. Lillicrap, Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis. Mastering the game of go with deep neural networks and tree search. Nature, 529:484–489, 2016.
Rion Snow, Brendan O’Connor, Daniel Jurafsky, and Andrew Ng. Cheap and fast – but is it good? evaluating non-expert annotations for natural language tasks. In Proceedings of Empirical Methods in Natural Language Processing, 2008.
Nitish Srivastava, Geoffrey E. Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan R. Salakhutdinov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine Learning Research, 15:1929–1958, 2014.
Roland Stuckardt. Coreference-based summarization and question answering: a case for high precision anaphor resolution. In International Symposium on Reference Resolution, 2003.
Saku Sugawara, Kentaro Inui, Satoshi Sekine, and Akiko Aizawa. What makes reading comprehension questions easier? In Proceedings of Empirical Methods in Natural Language Processing, 2018.
S. Shyam Sundar. The MAIN model : A heuristic approach to understanding technology effects on credibility. In Miriam J Metzger and Andrew J Flanagin, editors, Digital media, youth, and credibility, pages 73–100. The MIT Press, 2007.
56

Quizbowl
Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. CommonsenseQA: A question answering challenge targeting commonsense knowledge. In Proceedings of the Association for Computational Linguistics, 2019.
David Taylor, Colin McNulty, and Jo Meek. Your starter for ten: 50 years of university challenge, 2012. URL https://www.bbc.co.uk/sounds/play/b01m49vh.
Gerald Tesauro, David Gondek, Jonathan Lenchner, James Fan, and John M. Prager. Analysis of watson’s strategies for playing jeopardy! Journal of Artificial Intelligence Research, 47: 205–251, 2013.
James Thorne, Andreas Vlachos, Oana Cocarascu, Christos Christodoulopoulos, and Arpit Mittal. The FEVER2.0 shared task. In Proceedings of the Second Workshop on Fact Extraction and VERification (FEVER), pages 1–6, Stroudsburg, PA, USA, 2019. Association for Computational Linguistics.
Seiya Tokui, Kenta Oono, Shohei Hido, and Justin Clayton. Chainer: a next-generation open source framework for deep learning. In Proceedings of Workshop on Machine Learning Systems (LearningSys) in The Twenty-ninth Annual Conference on Neural Information Processing Systems, 2015.
Kirill Trapeznikov and Venkatesh Saligrama. Supervised sequential classification under budget constraints. 31:581–589, 2013.
Adam Trischler, Tong Wang, Xingdi Yuan, Justin Harris, Alessandro Sordoni, Philip Bachman, and Kaheer Suleman. Newsqa: A machine comprehension dataset. In Proceedings of the 2nd Workshop on Representation Learning for NLP, 2017.
Alan M. Turing. Computers &amp; thought. chapter Computing Machinery and Intelligence, pages 11–35. MIT Press, Cambridge, MA, USA, 1995. ISBN 0-262-56092-5. URL http: //dl.acm.org/citation.cfm?id=216408.216410.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Proceedings of Advances in Neural Information Processing Systems, 2017.
Jerry Vinokurov. How to write questions. http://hsquizbowl.org/forums/viewtopic. php?f=30&t=3945, April 2007. Accessed: 2018-12-04.
Jerry Vinokurov, Gautam Kandlikar, Gaurav Kandlikar, Matthew Jackson, Ryan Westbrook, and Rob Carson. 2014-15 packet submission guidelines. https://acf-quizbowl.com/ archives/archived-guidelines/2014-15-packet-submission-guidelines/, 2014. Accessed: 2019-06-23.
Luis von Ahn. Games with a purpose. Computer, 39:92 – 94, 07 2006. doi: 10.1109/MC. 2006.196.
Luis von Ahn and Laura Dabbish. Labeling images with a computer game. In International Conference on Human Factors in Computing Systems, 2004.
57

Rodriguez, Feng, Iyyer, He, and Boyd-Graber
F B Von Der Osten, M Kirley, and T Miller. The minds of many: Opponent modeling in a stochastic game. In International Joint Conference on Artificial Intelligence, 2017.
Ellen M. Voorhees. Overview of the trec 2001 question answering track. In Proceedings of the Text REtrieval Conference, 2001.
Ellen M Voorhees. Evaluating the evaluation: A case study using the TREC 2002 question answering track. In Conference of the North American Chapter of the Association for Computational Linguistics, 2003.
Ellen M Voorhees. Overview of the TREC 2002 question answering track. In Text Retrieval Conference, 2004.
Ellen M. Voorhees and Dawn M. Tice. Building a question answering test collection. In Proceedings of the ACM SIGIR Conference on Research and Development in Information Retrieval, 2000.
Eric Wallace, Pedro Rodriguez, Shi Feng, and Jordan Boyd-Graber. Trick me if you can: Human-in-the-loop generation of adversarial question answering examples. In Transactions of the Association for Computational Linguistics, 2019.
Johannes Welbl, Pontus Stenetorp, and Sebastian Riedel. Constructing datasets for multi-hop reading comprehension across documents. Transactions of the Association for Computational Linguistics, 6:287–302, 2018.
Tomer Wolfson, Mor Geva, Ankit Gupta, Matt Gardner, Yoav Goldberg, Daniel Deutch, and Jonathan Berant. Break it down: A question understanding benchmark. Transactions of the Association for Computational Linguistics, 8:183–198, April 2020.
Yongqin Xian, Christoph H. Lampert, Bernt Schiele, and Zeynep Akata. Zero-shot learning - a comprehensive evaluation of the good, the bad and the ugly. IEEE transactions on pattern analysis and machine intelligence, 2018.
Ikuya Yamada, Hiroyuki Shindo, Hideaki Takeda, and Yoshiyasu Takefuji. Learning distributed representations of texts and entities from knowledge base. Transactions of the Association for Computational Linguistics, 5:397–411, 2017.
Ikuya Yamada, Ryuji Tamaki, Hiroyuki Shindo, and Yoshiyasu Takefuji. Studio ousia’s quiz bowl question answering system. arXiv preprint arXiv:1803.08652, 2018.
Roman V. Yampolskiy. Turing Test as a Defining Feature of AI-Completeness, pages 3– 17. Springer Berlin Heidelberg, Berlin, Heidelberg, 2013. ISBN 978-3-642-29694-9. doi: 10.1007/978-3-642-29694-9_1.
Yi Yang, Wen tau Yih, and Christopher Meek. WikiQA: A challenge dataset for open-domain question answering. In Proceedings of Empirical Methods in Natural Language Processing, 2015.
58

Quizbowl
Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. Hotpotqa: A dataset for diverse, explainable multi-hop question answering. In Proceedings of Empirical Methods in Natural Language Processing, 2018.
D Yu, J Li, and L Deng. Calibration of confidence measures in speech recognition. IEEE transactions on audio, speech, and language processing, 19(8):2461–2473, November 2011. ISSN 1558-7916, 1558-7924. doi: 10.1109/TASL.2011.2141988. URL http://dx.doi.org/ 10.1109/TASL.2011.2141988.
Bianca Zadrozny and Charles Elkan. Obtaining calibrated probability estimates from decision trees and naive bayesian classifiers. In Proceedings of the International Conference of Machine Learning.
Rowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin Choi. SWAG: A Large-Scale adversarial dataset for grounded commonsense inference. In Proceedings of Empirical Methods in Natural Language Processing, 2018.
Valentina Bayer Zubek and Thomas G. Dietterich. Pruning improves heuristic search for cost-sensitive learning. In Proceedings of the International Conference of Machine Learning, 2002.
59

