ESPNET-TTS: UNIFIED, REPRODUCIBLE, AND INTEGRATABLE OPEN SOURCE END-TO-END TEXT-TO-SPEECH TOOLKIT
Tomoki Hayashi1,2, Ryuichi Yamamoto3, Katsuki Inoue4, Takenori Yoshimura1,2, Shinji Watanabe5, Tomoki Toda1, Kazuya Takeda1, Yu Zhang6, and Xu Tan7 1Nagoya University, 2Human Dataware Lab. Co., Ltd., 3LINE Corp.,
4Okayama University, 5Johns Hopkins University, 6Google AI, 7Microsoft Research

arXiv:1910.10909v2 [cs.CL] 17 Feb 2020

ABSTRACT
This paper introduces a new end-to-end text-to-speech (E2E-TTS) toolkit named ESPnet-TTS, which is an extension of the open-source speech processing toolkit ESPnet. The toolkit supports state-of-theart E2E-TTS models, including Tacotron 2, Transformer TTS, and FastSpeech, and also provides recipes inspired by the Kaldi automatic speech recognition (ASR) toolkit. The recipes are based on the design uniﬁed with the ESPnet ASR recipe, providing high reproducibility. The toolkit also provides pre-trained models and samples of all of the recipes so that users can use it as a baseline. Furthermore, the uniﬁed design enables the integration of ASR functions with TTS, e.g., ASR-based objective evaluation and semi-supervised learning with both ASR and TTS models. This paper describes the design of the toolkit and experimental evaluation in comparison with other toolkits. The experimental results show that our models can achieve state-of-the-art performance comparable to the other latest toolkits, resulting in a mean opinion score (MOS) of 4.25 on the LJSpeech dataset. The toolkit is publicly available at https://g ithub.com/espnet/espnet.
Index Terms— Open-source, end-to-end, text-to-speech
1. INTRODUCTION
Text-to-speech (TTS) is the technology to generate speech from the given input text. TTS is an essential component for many applications such as navigation announcements on smartphones and in cars, interactive interfaces of smart assistant systems, etc. Many researchers in speech processing communities have focused on this topic and open-source toolkits such as HTS [1] and Merlin [2] have helped to accelerate their research of conventional statistical parametric speech synthesis (SPSS) systems based on a hidden Markov model (HMM) [3] and a deep neural network (DNN) [4].
Recently with the success of deep learning techniques, end-toend TTS (E2E-TTS) systems have grown in popularity and have even started replacing conventional TTS systems in production [5]– [9]. E2E-TTS systems do not require a complex text processing front-end depending on the language expert knowledge but also hand-annotated (e.g., phoneme alignments) corpus, and can be simply trained from pairs of text and speech. Furthermore, it has been reported that the E2E-TTS achieves high perceptual quality comparable to professionally recorded speech [6] by incorporating neural vocoders such as WaveNet [10], [11] and WaveRNN [12]. E2E-TTS is one of the most important topics in this ﬁeld and various extensions including controllable and emotional E2E-TTS [13], [14] have been developed to advance the technology.
In this paper, we introduce a new E2E-TTS toolkit named ESPnet-TTS, which is an extension of the open-source speech processing toolkit ESPnet [15], [16]. The toolkit is developed for the

research purpose to make E2E-TTS systems more user-friendly and to accelerate research in this ﬁeld. The toolkit not only supports state-of-the-art E2E-TTS models such as Tacotron 2 [6], Transformer TTS [8], and FastSpeech [9] but also provides Kaldi automatic speech recognition (ASR) toolkit [17] style recipes. The recipe is based on the design uniﬁed with the ASR recipe and includes all of the procedures required to reproduce the results. The toolkit provides a number of recipes for more than ten languages, which include single-speaker TTS as well as multi-speaker one and speaker adaptation. Pre-trained models and generated samples of all of the recipes are also provided so that users can easily use it as a baseline or perform TTS demonstrations. Furthermore, thanks to the uniﬁed design among TTS and ASR, we can easily integrate ASR functions with TTS, for example, ASR-based objective evaluation by character error rate (CER), which is useful to automatically ﬁnd alignment failures (e.g., repetitions and deletions) in E2E-TTS [7]. This paper describes the basic design of the toolkit and comparative experimental evaluation with other TTS toolkits. Subjective evaluation results verify that our best model outperforms other TTS toolkits, achieving a mean opinion score (MOS) of 4.25 on the LJSpeech dataset.
2. RELATED WORK
This section brieﬂy compares ESPnet-TTS to other open-source TTS toolkits. First, we focus on conventional TTS toolkits for SPSS: HTS [1] and Merlin [2]. HTS is one of the most popular toolkits to build an HMM/DNN-based SPSS system. In the HMM-era, HTS has made huge contributions to developing speech synthesis technologies. Although it can also build a simple feed-forward neural network (FNN)-based speech synthesis system, recent neural network techniques are not currently supported. Merlin is designed for DNN-based SPSS systems and it supports various types of neural networks including mixture density networks (MDN) [18] and recurrent neural networks (RNNs) [19], [20]. These toolkits have contributed various research topics and applications. However, conventional SPSS systems are based on the complicated pipeline structure and each part is typically optimized separately, which may result in suboptimal performance as the whole TTS system. On the other hand, our ESPnet-TTS is based on the end-to-end approach. It signiﬁcantly simpliﬁes the system structure and provides better results by optimizing the whole system in an end-to-end manner. Furthermore, as the text processing part is integrated in the E2E model, the E2E approach allows us to construct TTS systems for various language data without expert knowledge of the target language.
Next, we focus on the comparison with other E2E-TTS toolkits. Here, we pick up the following six toolkits available on GitHub1:
1We selected the well-maintained toolkits which got over 1000 stars.

Table 1. Comparison with other open-source E2E-TTS toolkits, where “⋆” represents under construction. Note that numbers in the table are as of October 21st, 2019.

r9y9

Kyubyong Rayhane NVIDIA Mozilla OpenSeq2Seq ESPnet-TTS

Deep Voice 3

Tacotron

Tacotron 2

Transformer TTS

Centaur1

FastSpeech

⋆

Support multi-speaker?

⋆

Support adaptation?

Support neural vocoder?

Support other tasks?

Provide pre-trained model?

Provide pre-trained vocoder?

# of stars in GitHub

1.1k

1.6k

1.2k

1.2k

1.4k

1.0k

1.5k

# of supported datasets2

5

3

2

1

6

2

11

# of supported languages2

3

1

8

1

8

8

11

Input type Backend License

Char / Phn PyTorch
MIT

Char TensorFlow Apache 2.0

Char TensorFlow
MIT

Char PyTorch BSD-3

Char / Phn PyTorch MPL 2.0

Char TensorFlow Apache 2.0

Char / Phn PyTorch Apache 2.0

• r9y9: r9y9/deepvoice3_pytorch [21], • Kyubyong: Kyubyong/tacotron [22], • Rayhane: Rayhane-mamah/Tacotron-2 [23], • NVIDIA: NVIDIA/tacotron2 [24], • Mozilla: mozilla/TTS [25], • OpenSeq2Seq: NVIDIA/OpenSeq2Seq [26].
Table 1 summarizes the differences between our toolkit and the other E2E-TTS toolkits. Compared to the other toolkits, our ESPnetTTS provides three state-of-the-art E2E-TTS models including Tacotron 2, Transformer TTS, and FastSpeech. Moreover, a number of reproducible recipes are provided for more than ten languages with support for multi-speaker TTS as well as speaker adaptation techniques. While the other toolkits provide a limited number of pre-trained models and samples, we provide them for all of the recipes, enabling the researchers to use it as a baseline system for their research and general users to play with TTS demonstrations. Furthermore, since our ESPnet-TTS is an extension of ESPnet, both ASR and TTS recipes are based on a uniﬁed design, which allows us to easily integrate ASR functions with TTS. For example, ASRbased objective evaluation for TTS systems and advanced research topics such as the semi-supervised learning [27]–[30] can be realized by combining ASR and TTS modules in the uniﬁed framework.
3. FEATURES OF ESPNET-TTS
The ESPnet-TTS consists of two main components: a library of E2E-TTS neural network models and recipes including all of the procedures to complete experiments. The library part is written in Python using PyTorch [31] as a main neural network library. The recipes are all-in-one style scripts written in Bash and follow the Kaldi [17] style. The following sections describe the details.
3.1. Models
We support three E2E-TTS models2: Tacotron 2 [6], Transformer TTS [8], and FastSpeech [9]. The input for each model is the sequence of characters or phonemes and the output is the sequence of acoustic features (e.g. log Mel-ﬁlter bank features).
1Centaur is OpenSeq2Seq’s hand-designed model. 2We only counted items in the ofﬁcial repository, not including fork ones. 2We refer the E2E-TTS as the text to acoustic feature conversion system. The vocoder part is not included unless it is explicitly mentioned.

Tacotron 2 is an RNN-based sequence-to-sequence model. It consists of a bi-directional LSTM-based encoder and a unidirectional LSTM-based decoder with location sensitive attention [32]. Different from the original Tacotron 2, we also support the forward attention w/ or w/o a transition agent [33], which helps to learn diagonal attention. As for the Transformer TTS, it adopts multi-head self-attention mechanism. By replacing the RNNs to the parallelizable self-attention structure, it enables faster and more efﬁcient training while maintaining the high perceptual quality comparable to the Tacotron 2 [8]. For FastSpeech, it designs a feedforward Transformer architecture for non-autoregressive generation under the teacher-student training pipeline [9].
Furthermore, to provide multi-speaker TTS functionality, we support the use of speaker embedding as the auxiliary input for our E2E-TTS models. We use the pre-trained x-vector [34] provided by Kaldi as the speaker embedding.
3.2. Training
In training, we use several training criteria: the L1 loss and L2 loss for the predicted feature sequence and the weighted Sigmoid crossentropy for the stop token sequence. Additionally, we support the guided attention loss [35], which forces the attention weights to be diagonal and accelerates the learning of the diagonal attention.
Thanks to PyTorch [31], we support multi-GPU training which greatly reduces the training time, especially in the case of Transformer TTS. This is because it needs a large batch size (e.g., > 64) to train Transformer stably [8]. However, this means that the training of Transformer requires many GPUs, which is inconvenient for light users. To avoid this issue, we support dynamic batch making and gradient accumulation. In dynamic batch making, the batch size is automatically adjusted according to the length of the inputs and/or outputs. By using this scheme, we can avoid the out of memory error of GPU caused by a very long sentence, thereby improving GPU utilization. The gradient accumulation performs backpropagation for several batches and then updates model parameters once. This allows us to use a pseudo large batch size and as a result, we can sucessfully train Transformer with only a single GPU.
3.3. Synthesis
In synthesis, ﬁrst we generate the log Mel-ﬁlter bank feature sequence using the trained E2E-TTS models and then use the Grifﬁn–Lim algorithm (GL) [36], WaveNet vocoder (WNV) [10], [11],

Fig. 1. Comparison between the ﬂow of the ASR and TTS recipe.
or Parallel WaveGAN (PWG) [37] to generate speech from the sequence of features. In the case of GL, we convert the sequence of log Mel-ﬁlter bank features to a linear spectrogram and then apply GL to the spectrogram. The conversion is performed by applying the inverse Mel basis or the convolutional bank highway network GRU (CBHG) network [5].
In the case of the WNV and PWG, we use the generated log Melﬁlter bank sequence as the auxiliary input of the network to generate a waveform. We support two types of WNVs: one is that using a 16-bit mixture of logistics (MoL) [38] and the other is that using an 8-bit Softmax with the time-invariant noise shaping [39], which can reduce perceptual noise in the high frequency band [40]. WNV can greatly improve the naturalness of generated speech but it requires a long time to generate. On the other hand, since PWG is the nonautoregressive model, it can generate much faster than the real-time while keeping the quality comparable to WNV [41].
3.4. Kaldi-style recipes
ESPnet-TTS follows Kaldi-style data processing and provides all-inone recipes that consist of several stages. As you can see in Fig. 1, the stages from −1 to 2 are the same in both recipes, which means we use the same data format for both the ASR and TTS models, allowing the interconversion between the ASR and TTS recipes. The supported datasets in ESPnet-TTS are summarized in Table 2.
3.5. Integration with ASR functions
Thanks to the uniﬁed design, we can integrate some ASR functions with TTS. Here we introduce some interesting functions based on the integration.
One of the common problems of E2E-TTS is that the generated speech sometimes includes the deletion and/or repetition of words in the input text due to alignment errors. To address this issue, we provide the ASR-based objective evaluation using the CER. Similarly to TTS, many ASR pre-trained models are provided and the TTS recipes are easily converted to ASR recipes thanks to the uniﬁcation of the recipe design. Therefore, we can evaluate the CER of generated speech using pre-trained ASR models or those trained on the converted ASR recipe and then automatically detect the deletion and/or repetition of words.
Another example based on the integration is the advanced recipes that combine ASR modules with TTS modules. For example, we provide the recipe based on ASR-TTS cycle consistency training [29] and semi-supervised training using ASR and TTS [28]. These recipes provide many tips on how to combine ASR and TTS modules and help to accelerate the further advanced research topics of the end-to-end processing.

4. EXPERIMENTAL EVALUATION
4.1. Experimental condition
To demonstrate the performance of our models, we conducted experimental evaluations using the LJSpeech dataset [49]. The dataset consists of 24 hours of English speech from a single speaker. We used 12,600 utterances for training, 250 utterances for validation, and 250 utterances for evaluation. To compare the performance, we trained the following six models:
• Tacotron2.v2: Tacotron 2 [6] using the forward attention w/ a transition agent [33]
• Tacotron2.v3: Tacotron 2 trained w/ location sensitive attention [32] and the guided attention loss [35]
• Transformer.v1: Transformer TTS [8] w/ the guided attention loss
• Transformer.v3: Transformer TTS w/ the guided attention loss and phonemes as the input
• FastSpeech.v2: FastSpeech [9] trained w/ Transformer.v1 as a teacher w/o knowledge distillation
• FastSpeech.v3: FastSpeech + Post-Net [6] trained w/ Transformer.v3 as a teacher w/o knowledge distillation
For Tacotron 2, we used the same hyperparameters in [6] except for the type of the attention mechanism and the loss functions. For Transformer TTS, we did not use the encoder Prenet in [8] since it did not provide a signiﬁcant difference in our preliminary experiments. For the phoneme conversion, we used an English graphemeto-phoneme (G2P) library [53], which combines the pronunciation dictionary lookup and the sequence-to-sequence network-based predictions. For FastSpeech, to simplify the training stage, we did not use the knowledge distillation, using the natural feature sequence as the target sequence, which is expected to affect the voice quality of FastSpeech. We will add knowledge distillation in the next step. The MoL-WNV was used for all of the above models, which was trained with the features extracted from natural speech.
4.2. Objective evaluation
First, we evaluated the E2E-TTS models using the ASR-based objective measure character error rate (CER). As the ASR model, we used the Transformer trained on the Librispeech dataset [54]. Objective evaluation results are shown in Table 3, where Sub, Del, and Ins represent substitution, deletion, and insertion errors, respectively. A comparison between Tacotron 2 and Transformer TTS shows that the Transformer TTS caused more deletion errors than Tacotron 2. One of the possible reasons is that the multi-head attention is based on the dot product attention and therefore, the restriction of causality or continuity in the attention is weaker than the location-sensitive attention and the forward attention. Therefore, it is expected that the multi-head attention based on the other types of attention mechanisms will reduce the errors. FastSpeech.v2 achieved the best result among models, reducing all of the errors in comparison to its teacher model Transformer.v1. This is reasonable because FastSpeech is a non-autoregressive model which, theoretically, does not cause the repetition of the word [9]. However, FastSpeech.v3 is worse than FastSpeech.v2, especially in terms of the deletion errors. This degradation is caused by the error of the text processing front-end which converts characters into phonemes.
Next, we evaluated the speed of the feature generation using the real-time factor (RTF). In this evaluation, we used only characterbased models to focus on the difference of the model architecture. The evaluation was conducted with 16 threads of CPUs (Xeon Gold,

Table 2. Supported datasets in ESPnet-TTS, where “→” represents down sampling, “single” represents the single speaker model, “multi” represents the multi-speaker model using a pre-trained speaker embedding, and “adaptation” represents the speaker adaptation which performs ﬁne-tuning of the pre-trained model using a small amount of speech.

Dataset

Available languages

Sampling rate [kHz] # of speakers Length [hours] Recipe type Input type

Blizzard17 [42] ARCTIC [43] CSMSC [44] JNAS [45] JSUT [46] JVS [47] LibriTTS [48] LJSpeech [49]
M-AILABS [50] TWEB [51] VAIS1000 [52]

En En Zn Jp Jp Jp En En
En, De, Fr, It, Es, Pl, Uk, Ru En Vi

44.1 → 22.05 16
48 → 24 16
48 → 24 24 24
22.05
16 12 16

1 7 1 306 1 100 2,456 1 301 1 1

6

Single

Char

7

Adaptation

Char

12

Single

Pinyin

60

Multi

Phn

10

Single

Phn

30

Adaptation

Phn

585

Multi

Char

24

Single Char / Phn

999

Single

Char

72

Single

Char

1

Single

Char

Table 3. ASR-based CER results.

Method

Sub [%] Del [%] Ins [%] CER [%]

Groundtruth

0.3

0.7

0.3

1.3

Tacotron2.v2

0.4

1.0

3.62

5.0

Tacotron2.v3

0.5

1.2

0.3

2.1

Transformer.v1 0.6

1.7

0.5

2.8

Transformer.v3 0.5

1.8

0.5

2.8

FastSpeech.v2

0.3

0.9

0.3

1.6

FastSpeech.v3

0.4

1.3

0.4

2.1

Table 4. Averaged RTF results with a standard deviation.

Method

RTF on CPU RTF on GPU

Tacotron2.v2 Tacotron2.v3 Transformer.v1 FastSpeech.v2

0.216 ± 0.016 0.226 ± 0.016 0.851 ± 0.076 0.015 ± 0.005

0.104 ± 0.006 0.094 ± 0.009 0.634 ± 0.025 0.003 ± 0.004

3.00 GHz) and a single GPU (NVIDIA TITAN V). From the results shown in Table 4, all of the models can generate the features in less than RTF = 1.0 even on CPU. Transformer TTS is slower than Tacotron 2 but FastSpeech is much faster than the other models. Especially on GPU, FastSpeech is 30 times faster than Tacotron 2 and 200 times faster than Transformer TTS. Since FastSpeech is a nonautoregressive model, it can fully utilize the GPU without the bottleneck of the loop processing. Therefore, the improvement rate on GPU is higher than the other models.

Table 5. MOS results with 95% conﬁdence interval.

Method

MOS

Groundtruth

4.46 ± 0.05

Tacotron2.v2 Tacotron2.v3 Transformer.v1 Transformer.v3

4.14 ± 0.06 4.20 ± 0.06 4.17 ± 0.06 4.25 ± 0.06

Merlin [2] Mozilla [25] NVIDIA [24]

2.69 ± 0.09 3.91 ± 0.07 4.21 ± 0.06

Turk and the number of subjects was 101. Each subject evaluated at least 20 samples and rated the naturalness of each sample on a 5-point scale: 5 for excellent, 4 for good, 3 for fair, 2 for poor, and 1 for bad. We limited subjects to people who live in the US and instructed them to use headphones and work in a quiet room.
The subjective evaluation result is shown in Table 54. From the results, our Transformer TTS and Tacotron 2 achieved comparable performance with the other toolkit and the target speech, especially the phoneme-based Transformer (v3) outperformed all of the models. Tacotron 2 and Transformer TTS are almost the same performance in terms of the naturalness. It is expected that if we use phonemes as the input in Tacotron 2, the naturalness will be improved as the same as the Transformer TTS.

4.3. Subjective evaluation
Finally, we conducted subjective evaluations using MOS on naturalness3. To verify the performance of our models, we used the following models provided by other open-source toolkits for comparison:
• Merlin [2]: Conventional SPSS system w/ WORLD [55], • NVIDIA [24]: Pre-trained Tacotron 2 w/ WaveGlow [56], • Mozilla [25]: Pre-trained Tacotron 2 w/ WaveRNN [12].
Since there is no pre-trained model of the conventional SPSS system trained on the LJSpeech dataset, we manually constructed duration/acoustic models, each was based on 2 layer bidirectional LSTM RNN with 256 hidden units. The models were trained using Merlin and r9y9/nnmnkwii [57]. For E2E-TTS models, we selected two repositories from Table 1 which ofﬁcially provide pre-trained models. Note that these pre-trained models were trained on the same dataset but the split of the dataset might be different (these models may have used evaluation data for training). We used 100 sentences randomly selected from evaluation data for the subjective evaluation. The evaluation was conducted through Amazon Mechanical

5. SUMMARY
This paper introduced a new E2E-TTS toolkit named ESPnet-TTS as an extension of open-source speech processing toolkit ESPnet. The toolkit has been developed for the research purpose to make E2E-TTS systems more friendly and accelerate this research ﬁeld. The toolkit supports not only state-of-the-art E2E-TTS models but also various TTS recipes whose design is uniﬁed with ASR recipes, providing high reproducibility. The experimental evaluation results demonstrated that our models can achieve state-of-the-art performance comparable to the other latest toolkits, resulting in MOS of 4.25 on the LJSpeech dataset.
In future work, we will work on the training with knowledge distillation, the support of various types of embedding such as the emotion and the accent, and further customizable network structure (e.g., multi-head attention with various attention mechanisms).
6. ACKNOWLEDGEMENT
We would like to thank Dr. Heiga Zen for his valuable comments.

1The speakers in mixed data are not counted. 2The large insertion error is caused by the number of repetitions in a spe-
ciﬁc utterance. If we remove the sentence, it is comparable to the others. 3Audio samples used in the subjective evaluation are available at the fol-
lowing URL: https://espnet.github.io/icassp2020-tts

4The MOS of both our FastSpeech models have not yet reached the MOS of the teacher models [9]. The reason for the performance degradation from the teacher model might be because we did not use knowledge distillation. We will further investigate the reason in future work.

7. REFERENCES
[1] HTS working group, HMM/DNN-based speech synthesis system (HTS), http://hts.sp.nitech.ac.jp/, 2016.
[2] Z. Wu, O. Watts, and S. King, “Merlin: An open source neural network speech synthesis system,” in Proc. SSW, 2016, pp. 202–207.
[3] K. Tokuda, Y. Nankaku, T. Toda, et al., “Speech synthesis based on hidden Markov models,” Proceedings of the IEEE, vol. 101, no. 5, pp. 1234–1252, 2013.
[4] H. Zen, A. Senior, and M. Schuster, “Statistical parametric speech synthesis using deep neural networks,” in ICASSP, 2013, pp. 7962– 7966.
[5] Y. Wang, R. Skerry-Ryan, D. Stanton, et al., “Tacotron: Towards endto-end speech synthesis,” in Proc. Interspeech, 2017.
[6] J. Shen, R. Pang, R. J. Weiss, et al., “Natural TTS synthesis by conditioning WaveNet on Mel spectrogram predictions,” in ICASSP, 2018, pp. 4779–4783.
[7] W. Ping, K. Peng, A. Gibiansky, et al., “Deep Voice 3: Scaling textto-speech with convolutional sequence learning,” in ICLR, 2018.
[8] N. Li, S. Liu, Y. Liu, et al., “Close to human quality TTS with Transformer,” ArXiv preprint arXiv:1809.08895, 2018.
[9] Y. Ren, Y. Ruan, X. Tan, et al., “Fastspeech: Fast, robust and controllable text to speech,” in NIPS, 2019, pp. 3165–3174.
[10] A. van den Oord, S. Dieleman, H. Zen, et al., “WaveNet: A generative model for raw audio,” ArXiv preprint arXiv:1609.03499, 2016.
[11] A. Tamamori, T. Hayashi, K. Kobayashi, et al., “Speaker-dependent WaveNet vocoder,” in Proc. Interspeech, 2017, pp. 1118–1122.
[12] N. Kalchbrenner, E. Elsen, K. Simonyan, et al., “Efﬁcient neural audio synthesis,” in ICML, 2018, pp. 2415–2424.
[13] W.-N. Hsu, Y. Zhang, R. J. Weiss, et al., “Hierarchical generative modeling for controllable speech synthesis,” in ICLR, 2019.
[14] Y. Wang, D. Stanton, Y. Zhang, et al., “Style tokens: Unsupervised style modeling, control and transfer in end-to-end speech synthesis,” ArXiv preprint arXiv:1803.09017, 2018.
[15] S. Watanabe, T. Hori, S. Karita, et al., “ESPnet: End-to-end speech processing toolkit,” in Proc. Interspeech, 2018, pp. 2207–2211.
[16] S. Karita, N. Chen, T. Hayashi, et al., “A comparative study on Transformer vs RNN in speech applications,” in Proc. ASRU, 2019, pp. 499–456.
[17] D. Povey, A. Ghoshal, G. Boulianne, et al., “The Kaldi speech recognition toolkit,” in Proc. ASRU, 2011.
[18] H. Zen and A. Senior, “Deep mixture density networks for acoustic modeling in statistical parametric speech synthesis,” in ICASSP, 2014, pp. 3844–3848.
[19] J. Chung, C. Gulcehre, K. Cho, et al., “Empirical evaluation of gated recurrent neural networks on sequence modeling,” ArXiv preprint arXiv:1412.3555, 2014.
[20] Z. Wu and S. King, “Investigating gated recurrent networks for speech synthesis,” in ICASSP, 2016, pp. 5140–5144.
[21] R. Yamamoto, r9y9/deepvoice3 pytorch, https://github.com /r9y9/deepvoice3_pytorch, 2019.
[22] K. Park, Kyubuyong/tacotron, https://github.com/Kyubyon g/tacotron, 2019.
[23] R. Mama, Rayhane-mamah/Tacotron-2, https://github.com/ Rayhane-mamah/Tacotron-2, 2019.
[24] R. Valle, NVIDIA/tacotron2, https://github.com/NVIDIA/t acotron2, 2019.
[25] E. Go¨lge, mozilla/TTS, https://github.com/mozilla/TTS, 2019.
[26] O. Kuchaiev, B. Ginsburg, I. Gitman, et al., “Mixed-precision training for NLP and speech recognition with OpenSeq2Seq,” ArXiv preprint arXiv:1805.10387, 2018.
[27] T. Hayashi, S. Watanabe, Y. Zhang, et al., “Back-translation-style data augmentation for end-to-end ASR,” in Proc. SLT, 2018, pp. 426–433.
[28] S. Karita, S. Watanabe, T. Iwata, et al., “Semi-supervised end-toend speech recognition using text-to-speech and autoencoders,” in ICASSP, 2019, pp. 6166–6170.

[29] M. K. Baskar, S. Watanabe, R. Astudillo, et al., “Self-supervised sequence-to-sequence ASR using unpaired speech and text,” ArXiv preprint arXiv:1905.01152, 2019.
[30] Y. Ren, X. Tan, T. Qin, et al., “Almost unsupervised text to speech and automatic speech recognition,” in ICML, 2019, pp. 5410–5419.
[31] A. Paszke, S. Gross, S. Chintala, et al., “Automatic differentiation in PyTorch,” in NIPS Autodiff Workshop, 2017.
[32] J. K. Chorowski, D. Bahdanau, D. Serdyuk, et al., “Attention-based models for speech recognition,” in NIPS, 2015, pp. 577–585.
[33] J.-X. Zhang, Z.-H. Ling, and L.-R. Dai, “Forward attention in sequence-to-sequence acoustic modeling for speech synthesis,” in ICASSP, 2018, pp. 4789–4793.
[34] D. Snyder, D. Garcia-Romero, G. Sell, et al., “X-vectors: Robust DNN embeddings for speaker recognition,” in ICASSP, 2018, pp. 5329–5333.
[35] H. Tachibana, K. Uenoyama, and S. Aihara, “Efﬁciently trainable text-to-speech system based on deep convolutional networks with guided attention,” in ICASSP, 2018, pp. 4784–4788.
[36] N. Perraudin, P. Balazs, and P. L. Søndergaard, “A fast Grifﬁn-Lim algorithm,” in Proc. WASPAA, 2013, pp. 1–4.
[37] R. Yamamoto, E. Song, and J.-M. Kim, “Parallel WaveGAN: A fast waveform generation model based on generative adversarial networks with multi-resolution spectrogram,” in ICASSP (in press), 2020.
[38] R. Yamamoto, r9y9/wavenet vocoder, https://github.com/r 9y9/wavenet_vocoder, 2019.
[39] T. Hayashi, kan-bayashi/PytorchWaveNetVocoder, https://gith ub.com/kan-bayashi/PytorchWaveNetVocoder, 2019.
[40] K. Tachibana, T. Toda, Y. Shiga, et al., “An investigation of noise shaping with perceptual weighting for WaveNet-based speech generation,” in ICASSP, 2018, pp. 5664–5668.
[41] T. Hayashi, kan-bayashi/ParallelWaveGAN, https://github.c om/kan-bayashi/ParallelWaveGAN, 2019.
[42] S. King, L. Wihlborg, and W. Guo, “The Blizzard Challenge 2017,” in Proc. Blizzard Challenge Workshop, 2017.
[43] J. Kominek and A. W. Black, “The CMU Arctic speech databases,” in Proc. SSW, 2004.
[44] D. Baker, Chinese Standard Mandarin Speech Copus, https://ww w.data-baker.com/open_source.html, 2017.
[45] K. Itou, M. Yamamoto, K. Takeda, et al., “JNAS: Japanese speech corpus for large vocabulary continuous speech recognition research,” Acoustical Science and Technology, vol. 20, no. 3, pp. 199–206, 1999.
[46] R. Sonobe, S. Takamichi, and H. Saruwatari, “JSUT corpus: Free large-scale Japanese speech corpus for end-to-end speech synthesis,” ArXiv preprint arXiv:1711.00354, 2017.
[47] S. Takamichi, K. Mitsui, Y. Saito, et al., “JVS corpus: Free Japanese multi-speaker voice corpus,” ArXiv preprint arXiv:1908.06248, 2019.
[48] H. Zen, V. Dang, R. Clark, et al., “LibriTTS: A corpus derived from LibriSpeech for text-to-speech,” ArXiv preprint arXiv:1904.02882, 2019.
[49] K. Ito, The LJ speech dataset, https://keithito.com/LJ-Sp eech-Dataset/, 2017.
[50] I. Solak, The M-AILABS speech dataset, https://www.caito.d e/2019/01/the-m-ailabs-speech-dataset/, 2019.
[51] K. Park, The World English Bible: A large, single-speaker speech datasaet in english, https://www.kaggle.com/bryanpark /the-world-english-bible-speech-dataset, 2017.
[52] Q. T. Do and C. M. Luong, VAIS-1000: A Vietnamese speech synthesis corpus, http://dx.doi.org/10.21227/H2B887, 2017.
[53] K. Park and J. Kim, G2pe, https://github.com/Kyubyong/ g2p, 2019.
[54] V. Panayotov, G. Chen, D. Povey, et al., “LibriSpeech: An ASR corpus based on public domain audio books,” in ICASSP, 2015, pp. 5206–5210.
[55] M. Morise, F. Yokomori, and K. Ozawa, “WORLD: A vocoder-based high-quality speech synthesis system for real-time applications,” IEICE Trans. Inf. & Syst., vol. 99, no. 7, pp. 1877–1884, 2016.
[56] R. Prenger, R. Valle, and B. Catanzaro, “WaveGlow: A ﬂow-based generative network for speech synthesis,” in ICASSP, 2019, pp. 3617– 3621.
[57] R. Yamamoto, r9y9/nnmnkwii, https://github.com/r9y9/n nmnkwii, 2019.

