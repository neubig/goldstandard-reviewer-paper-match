arXiv:1611.01211v8 [cs.LG] 13 Mar 2018

Combating Reinforcement Learning’s Sisyphean Curse with Intrinsic Fear
Zachary C. Lipton1,2,3, Kamyar Azizzadenesheli4, Abhishek Kumar3, Lihong Li5, Jianfeng Gao6, Li Deng7
Carnegie Mellon University1, Amazon AI2, University of California, San Diego3, Univerisity of California, Irvine4, Google5, Microsoft Research6, Citadel7
zlipton@cmu.edu, kazizzad@uci.edu, abkumar@ucsd.edu { lihongli, jfgao, deng } @microsoft.com
March 15, 2018
Abstract Many practical environments contain catastrophic states that an optimal agent would visit infrequently or never. Even on toy problems, Deep Reinforcement Learning (DRL) agents tend to periodically revisit these states upon forgetting their existence under a new policy. We introduce intrinsic fear (IF), a learned reward shaping that guards DRL agents against periodic catastrophes. IF agents possess a fear model trained to predict the probability of imminent catastrophe. This score is then used to penalize the Qlearning objective. Our theoretical analysis bounds the reduction in average return due to learning on the perturbed objective. We also prove robustness to classi cation errors. As a bonus, IF models tend to learn faster, owing to reward shaping. Experiments demonstrate that intrinsic-fear DQNs solve otherwise pathological environments and improve on several Atari games.
1 Introduction
Following the success of deep reinforcement learning (DRL) on Atari games [22] and the board game of Go [29], researchers are increasingly exploring practical applications. Some investigated applications include robotics [17], dialogue systems [9, 19], energy management [25], and self-driving cars [27]. Amid this push to apply DRL, we might ask, can we trust these agents in the wild? Agents acting society may cause harm. A self-driving car might hit pedestrians and a domestic robot might injure a child. Agents might also cause self-injury, and while Atari lives lost are inconsequential, robots are expensive.
Unfortunately, it may not be feasible to prevent all catastrophes without requiring extensive prior knowledge [10]. Moreover, for typical DQNs, providing large negative rewards does not solve the problem: as soon as the catastrophic trajectories are ushed from the replay bu er, the updated Q-function ceases to discourage revisiting these states.
In this paper, we de ne avoidable catastrophes as states that prior knowledge dictates an optimal policy should visit rarely or never. Additionally, we de ne danger states—those from which a catastrophic state can
1

be reached in a small number of steps, and assume that the optimal policy does visit the danger states rarely or never. The notion of a danger state might seem odd absent any assumptions about the transition function. With a fully-connected transition matrix, all states are danger states. However, physical environments are not fully connected. A car cannot be parked this second, underwater one second later.
This work primarily addresses how we might prevent DRL agents from perpetually making the same mistakes. As a bonus, we show that the prior knowledge knowledge that catastrophic states should be avoided accelerates learning. Our experiments show that even on simple toy problems, the classic deep Q-network (DQN) algorithm fails badly, repeatedly visiting catastrophic states so long as they continue to learn. This poses a formidable obstacle to using DQNs in the real world. How can we trust a DRL-based agent that was doomed to periodically experience catastrophes, just to remember that they exist? Imagine a self-driving car that had to periodically hit a few pedestrians to remember that it is undesirable.
In the tabular setting, an RL agent never forgets the learned dynamics of its environment, even as its policy evolves. Moreover, when the Markovian assumption holds, convergence to a globally optimal policy is guaranteed. However, the tabular approach becomes infeasible in high-dimensional, continuous state spaces. The trouble for DQNs owes to the use of function approximation [24]. When training a DQN, we successively update a neural network based on experiences. These experiences might be sampled in an online fashion, from a trailing window (experience replay bu er), or uniformly from all past experiences. Regardless of which mode we use to train the network, eventually, states that a learned policy never encounters will come to form an in nitesimally small region of the training distribution. At such times, our networks su er the well-known problem of catastrophic forgetting [21, 20]. Nothing prevents the DQN’s policy from drifting back towards one that revisits forgotten catastrophic mistakes.
We illustrate the brittleness of modern DRL algorithms with a simple pathological problem called Adventure Seeker. This problem consists of a one-dimensional continuous state, two actions, simple dynamics, and admits an analytic solution. Nevertheless, the DQN fails. We then show that similar dynamics exist in the classic RL environment Cart-Pole.
To combat these problems, we propose the intrinsic fear (IF) algorithm. In this approach, we train a supervised fear model that predicts which states are likely to lead to a catastrophe within kr steps. The output of the fear model (a probability), scaled by a fear factor penalizes the Q-learning target. Crucially, the fear model maintains bu ers of both safe and danger states. This model never forgets danger states, which is possible due to the infrequency of catastrophes.
We validate the approach both empirically and theoretically. Our experiments address Adventure Seeker, Cartpole, and several Atari games. In these environments, we label every lost life as a catastrophe. On the toy environments, IF agents learns to avoid catastrophe inde nitely. In Seaquest experiments, the IF agent achieves higher reward and in Asteroids, the IF agent achieves both higher reward and fewer catastrophes. The improvement on Freeway is most dramatic.
We also make the following theoretical contributions: First, we prove that when the reward is bounded and the optimal policy rarely visits the danger states, an optimal policy learned on the perturbed reward function has approximately the same return as the optimal policy learned on the original value function. Second, we prove that our method is robust to noise in the danger model.
2

2 Intrinsic fear

An agent interacts with its environment via a Markov decision process, or MDP, (S, A, T , R, γ ). At each step t, the agent observes a state s ∈ S and then chooses an action a ∈ A according to its policy π . The environment then transitions to state st+1 ∈ S according to transition dynamics T (st+1|st , at ) and generates a reward rt with expectation R(s, a). This cycle continues until each episode terminates.

An agent seeks to maximize the cumulative discounted return

T t =0

γ

t rt

.

Temporal-di

erences methods

[31] like Q-learning [33] model the Q-function, which gives the optimal discounted total reward of a

state-action pair. Problems of practical interest tend to have large state spaces, thus the Q-function is

typically approximated by parametric models such as neural networks.

In Q-learning with function approximation, an agent collects experiences by acting greedily with respect to Q(s, a; θQ ) and updates its parameters θQ . Updates proceed as follows. For a given experience (st , at , rt , st+1), we minimize the squared Bellman error:

L = (Q(st , at ; θQ ) − t )2

(1)

for t = rt + γ · maxa Q(st+1, a ; θQ ). Traditionally, the parameterised Q(s, a; θ ) is trained by stochastic approximation, estimating the loss on each experience as it is encountered, yielding the update:

θt+1 ←θt + α ( t − Q(st , at ; θt ))∇Q(st , at ; θt ) .

(2)

Q-learning methods also require an exploration strategy for action selection. For simplicity, we consider only the ϵ-greedy heuristic. A few tricks help to stabilize Q-learning with function approximation. Notably, with experience replay [18], the RL agent maintains a bu er of experiences, of experience to update the Q-function.
We propose a new formulation: Suppose there exists a subset C ⊂ S of known catastrophe states/ And assume that for a given environment, the optimal policy rarely enters from which catastrophe states are reachable in a short number of steps. We de ne the distance d(si , sj ) to be length N of the smallest sequence of transitions {(st , at , rt , st+1)}tN=1 that traverses state space from si to sj .1 De nition 2.1. Suppose a priori knowledge that acting according to the optimal policy π ∗, an agent rarely encounters states s ∈ S that lie within distance d(s, c) < kτ for any catastrophe state c ∈ C. Then each state s for which ∃c ∈ C s.t. d(s, c) < kτ is a danger state.
In Algorithm 1, the agent maintains both a DQN and a separate, supervised fear model F : S → [0, 1]. F provides an auxiliary source of reward, penalizing the Q-learner for entering likely danger states. In our case, we use a neural network of the same architecture as the DQN (but for the output layer). While one could sharing weights between the two networks, such tricks are not relevant to this paper’s contribution.
We train the fear model to predict the probability that any state will lead to catastrophe within k moves. Over the course of training, our agent adds each experience (s, a, r , s ) to its experience replay bu er. Whenever a catastrophe is reached at, say, the nth turn of an episode, we add the preceding kr (fear radius) states to a danger bu er. We add the rst n − kr states of that episode to a safe bu er. When n < kr , all states for that episode are added to the list of danger states. Then after each turn, in addition to updating the Q-network, we update the fear model, sampling 50% of states from the danger bu er, assigning them label 1, and the remaining 50% from the safe bu er, assigning them label 0.
1In the stochastic dynamics setting, the distance is the minimum mean passing time between the states.

3

Algorithm 1 Training DQN with Intrinsic Fear

1: Input: Q (DQN), F (fear model), fear factor λ, fear phase-in length kλ, fear radius kr 2: Output: Learned parameters θQ and θF 3: Initialize parameters θQ and θF randomly

4: Initialize replay bu er D, danger state bu er DD , and safe state bu er DS 5: Start per-episode turn counter ne 6: for t in 1:T do

7: With probability ϵ select random action at

8: Otherwise, select a greedy action at = arg maxa Q(st , a; θQ )

9: Execute action at in environment, observing reward rt and successor state st+1

10: Store transition (st , at , rt , st+1) in D

11: if st+1 is a catastrophe state then

12:

Add states st−kr through st to DD

13: else

14:

Add states st−ne through st−kr −1 to DS

15: Sample a random mini-batch of transitions (sτ , aτ , rτ , sτ +1) from D 16: λτ ← min(λ, λk·λt )

 

for terminal sτ +1 :

 

 rτ − λτ 

17:

τ

 ←

for non-terminal sτ +1 :



 rτ + maxa Q(sτ +1, a ; θQ )− 

  

λ · F (sτ +1; θF )

  

18:

θQ

 ← θQ

− η · ∇θQ (

τ

− Q(sτ , aτ ; θQ ))2



19: Sample random mini-batch sj with 50% of examples from DD and 50% from DS

20: j ← 01,, ffoorr ssjj ∈∈ DDSD 21: θF ← θF − η · ∇θF lossF ( j , F (sj ; θF ))

For each update to the DQN, we perturb the TD target t . Instead of updating Q(st , at ; θQ ) towards rt + maxa Q(st+1, a ; θQ ), we modify the target by subtracting the intrinsic fear:

IF t

= rt

+ max Q(st+1, a

;θQ) − λ

· F (st+1; θF )

(3)

a

where F (s; θF ) is the fear model and λ is a fear factor determining the scale of the impact of intrinsic fear on the Q-function update.

3 Analysis
Note that IF perturbs the objective function. Thus, one might be concerned that the perturbed reward might lead to a sub-optimal policy. Fortunately, as we will show formally, if the labeled catastrophe states and danger zone do not violate our assumptions, and if the fear model reaches arbitrarily high accuracy, then this will not happen. For an MDP, M = S, A, T , R, γ , with 0 ≤ γ ≤ 1, the average reward return is as follows:
4

limT →∞

1 T

EM

T t

rt

|π

ηM (π ) := 

(1 − γ )EM t∞ γ t rt |π



if γ = 1 if 0 ≤ γ < 1

The optimal policy π ∗ of the model M is the policy which maximizes the average reward return, π ∗ = maxπ ∈P η(π ) where P is a set of stationary polices. Theorem 1. For a given MDP, M, with γ ∈ [0, 1] and a catastrophe detector f , let π ∗ denote any optimal
policy of M, and π˜ denote an optimal policy of M equipped with fear model F , and λ, environment (M, F ). If the probability π ∗ visits the states in the danger zone is at most ϵ, and 0 ≤ R(s, a) ≤ 1, then

ηM (π ∗) ≥ ηM (π˜ ) ≥ ηM,F (π˜ ) ≥ ηM (π ∗) − λϵ .

(4)

In other words, π˜ is λϵ-optimal in the original MDP.

Proof. The policy π ∗ visits the fear zone with probability at most ϵ. Therefore, applying π ∗ on the environment with intrinsic fear (M, F ), provides a expected return of at least ηM (π ∗) − ϵλ. Since there exists a policy with this expected return on (M, F ), therefore, the optimal policy of (M, F ), must result in an expected return of at least ηM (π ∗) − ϵλ on (M, F ), i.e. ηM,F (π˜ ) ≥ ηM (π ∗) − ϵλ. The expected return ηM,F (π˜ ) decomposes into two parts: (i) the expected return from original environment M, ηM (π˜ ), (ii) the expected return from the fear model. If π˜ visits the fear zone with probability at most ϵ˜, then ηM,F (π˜ ) ≥ ηM (π˜ ) − λϵ˜. Therefore, applying π˜ on M promises an expected return of at least ηM (π ∗) − ϵλ + ϵ˜λ, lower bounded by ηM (π ∗) − ϵλ.

It is worth noting that the theorem holds for any optimal policy of M. If one of them does not visit the fear zone at all (i.e., ϵ = 0), then ηM (π ∗) = ηM,F (π˜ ) and the fear signal can boost up the process of learning the optimal policy.
Since we empirically learn the fear model F using collected data of some nite sample size N , our RL agent has access to an imperfect fear model Fˆ, and therefore, computes the optimal policy based on Fˆ. In this case, the RL agent trains with intrinsic fear generated by Fˆ, learning a di erent value function than the RL agent with perfect F . To show the robustness against errors in Fˆ, we are interested in the average deviation in the value functions of the two agents.
Our second main theoretical result, given in Theorem 2, allows the RL agent to use a smaller discount factor, denoted γplan, than the actual one (γplan ≤ γ ), to reduce the planning horizon and computation cost. Moreover, when an estimated model of the environment is used, Jiang et al. [2015] shows that using a smaller discount factor for planning may prevent over- tting to the estimated model. Our result demonstrates that using a smaller discount factor for planning can reduce reduction of expected return when an estimated fear model is used. Speci cally, for a given environment, with fear model F1 and discount factor γ1, let VFπ1F,∗γ2,1γ2 (s), s ∈ S, denote the state value function under the optimal policy of an environment with fear model F2 and the discount factor γ2. In the same environment, let ωπ (s) denote the visitation distribution over states under policy π . We are interested in the average reduction on expected return caused by an imperfect classi er; this

5

reduction, denoted L(F , F , γ , γplan), is de ned as

∫ (1 − γ )

π∗

π∗

π∗
F,γ

ω

F,γpl an (s )

VF

F,γ
,γ

(s

)

−

VF

,γ

plan (s)

ds .

s ∈S

Theorem 2. Suppose γplan ≤ γ , and δ ∈ (0, 1). Let Fˆ be the fear model in F with minimum empirical risk on
N samples. For a given MDP model, the average reduction on expected return, L(F , F , γ , γplan), vanishes as N increase: with probability at least 1 − δ ,

L = O λ 1−γ

V

C(F

)

+

log

1 δ

+

(γ

− γplan)

,

(5)

1 − γplan

N

1 − γplan

where VC(F ) is the VC dimension of the hypothesis class F .

π∗

π∗
F,γ

Proof. In order to analyze

VF

F,γ
,γ

(s

)

−

VF

,γ

plan (s)

,

which is always non-negative,

we decompose it as

follows:

π∗

π∗

π∗

π∗
F,γ

VF,Fγ,γ (s) − VF,Fγ,γ (s) + VF,Fγ,γ (s) − VF,γ plan (s)

(6)

plan

plan

The rst term is the di erence in the expected returns of πF∗,γ under two di erent discount factors, starting from s:

∞

E

(γ t

−

γ

t p

l

a

n

)r

t

|s

0

=

s

,

π

∗ F

,

γ

,

F

,

M

.

(7)

t =0

Since rt ≤ 1, ∀t , using the geometric series, Eq. 7 is upper bounded by 1−1γ − 1−γ1plan = (1−γγp−lγapnl)a(n1−γ ) .

π∗

π∗

F,γpl an

(s)

−

V F,γplan
F,γ

(s)

since

πF∗,γ

is an optimal policy of an

The second term is upper bounded by VF,γplan

plan

π∗

environment equipped with (F , γplan). Furthermore, as γplan ≤ γ and rt ≥ 0, we have VF,Fγ,γplan (s) ≥

π∗

π∗

π∗

F,γplan (s). Therefore, the second term of Eq. 6 is upper bounded by V F,γplan (s) − V F,γplan (s), which is

VF,γpl an

F ,γpl an

F ,γpl an

the deviation of the value function under two di erent close policies. Since F and F are close, we expect

that this deviation to be small. With one more decomposition step

π∗

π∗

F,γpl an

(s)

−

V F,γplan
F,γ

(s)

=

VF,γpl an

plan

πF∗,γpl an

πF∗,γpl an

VF,γplan (s) − VF,γ (s)

plan

π∗

π∗

π∗

π∗

F,γplan (s) − V F,γplan (s) + V F,γplan (s) − VF,Fγ,γplan (s) .

+ VF,γplan

F ,γpl an

F ,γpl an

plan

Since the middle term in this equation is non-positive, we can ignore it for the purpose of upper-bounding the left-hand side. The upper bound is sum of the remaining two terms which is also upper bounded by 2

6

times of the maximum of them;

2 ∗ max ∗

Vπ
F,γ

(s) − VFπ,γplan (s) ,

π

∈

{π F , γp l an

,

π
F,γpl an

}

plan

which is the deviation in values of di erent domains. The value functions satisfy the Bellman equation for any π :

VFπ,γplan (s) =R(s, π (s)) + λF (s)

∫ +γplan s T∈S(s |s, π (s))VFπ,γplan (s )ds

V π (s) =R(s, π (s)) + λF (s)

(8)

F ,γpl an

∫

+γplan T (s |s, π (s))V π (s )ds

(9)

s ∈S

F ,γpl an

which can be solved using iterative updates of dynamic programing. Let Viπ (s) and Viπ (s) respectably denote the i’th iteration of the dynamic programmings corresponding to the rst and second equalities in Eq. 8. Therefore, for any state

Viπ (s)−Viπ (s) = λ F (s) − λ F (s)

∫

+γplan T (s |s, π (s)) Vi−1(s ) − Vi−1(s ) ds
s ∈S

i

≤λ γplan T π i F − F (s) ,

(10)

i =0

where (T π )i is a kernel and denotes the transition operator applied i times to itself. The classi cation error

π∗

F (s) − F (s)

is the zero-one loss of binary classi

∫ er, therefore, its expectation

ω F,γplan (s) F (s) − F (s) ds

s ∈S

is bounded by 3200 V C(FN)+log δ1 with probability at least 1 − δ [32, 12]. As long as the operator (T π )i is a

linear operator,

∫ π∗ ω F,γplan (s)

V π (s)

− V π (s)

ds

≤

λ

3200

V

C(F

)

+

log

1 δ

.

(11)

i

i

s ∈S

1 − γplan

N

Therefore, L(F , F , γ , γplan) is bounded by (1 − γ ) times of sum of Eq. 11 and 1−1γ−pγlan , with probability at least 1 − δ .

Theorem 2 holds for both nite and continuous state-action MDPs. Over the course of our experiments, we discovered the following pattern: Intrinsic fear models are more e ective when the fear radius kr is large enough that the model can experience danger states at a safe distance and correct the policy, without experiencing many catastrophes. When the fear radius is too small, the danger probability is only nonzero at states from which catastrophes are inevitable anyway and intrinsic fear seems not to help. We also found that wider fear factors train more stably when phased in over the course of many episodes. So, in all of our experiments we gradually phase in the fear factor from 0 to λ reaching full strength at predetermined time step kλ.

7

4 Environments
We demonstrate our algorithms on the following environments: (i) Adventure Seeker, a toy pathological environment that we designed to demonstrate catastrophic forgetting; (ii) Cartpole, a classic RL environment; and (ii) the Atari games Seaquest, Asteroids, and Freeway [3].
Adventure Seeker We imagine a player placed on a hill, sloping upward to the right (Figure 1(a)). At each turn, the player can move to the right (up the hill) or left (down the hill). The environment adjusts the player’s position accordingly, adding some random noise. Between the left and right edges of the hill, the player gets more reward for spending time higher on the hill. But if the player goes too far to the right, she will fall o , terminating the episode (catastrophe). Formally, the state is single continuous variable s ∈ [0, 1.0], denoting the player’s position. The starting position for each episode is chosen uniformly at random in the interval [.25, .75]. The available actions consist only of {−1, +1} (left and right). Given an action at in state st , T (st+1|st , at ) the successor state is produced st+1 ← st + .01·at +η where η ∼ N (0, .012). The reward at each turn is st (proportional to height). The player falls o the hill, entering the catastrophic terminating state, whenever st+1 > 1.0 or st+1 < 0.0.
This game should be easy to solve. There exists a threshold above which the agent should always choose to go left and below which it should always go right. And yet a DQN agent will periodically die. Initially, the DQN quickly learns a good policy and avoids the catastrophe, but over the course of continued training, the agent, owing to the shape of the reward function, collapses to a policy which always moves right, regardless of the state. We might critically ask in what real-world scenario, we could depend upon a system that cannot solve Adventure Seeker.
Cart-Pole In this classic RL environment, an agent balances a pole atop a cart (Figure 1(b)). Qualitatively, the game exhibits four distinct catastrophe modes. The pole could fall down to the right or fall down to the left. Additionally, the cart could run o the right boundary of the screen or run o the left. Formally, at each time, the agent observes a four-dimensional state vector (x, , θ, ω) consisting respectively of the cart position, cart velocity, pole angle, and the pole’s angular velocity. At each time step, the agent chooses an action, applying a force of either −1 or +1. For every time step that the pole remains upright and the cart remains on the screen, the agent receives a reward of 1. If the pole falls, the episode terminates, giving a return of 0 from the penultimate state. In experiments, we use the implementation CartPole-v0 contained in the openAI gym [6]. Like Adventure Seeker, this problem admits an analytic solution. A perfect policy should never drop the pole. But, as with Adventure Seeker, a DQN converges to a constant rate of catastrophes per turn.
Atari games In addition to these pathological cases, we address Freeway, Asteroids, and Seaquest, games from the Atari Learning Environment. In Freeway, the agent controls a chicken with a goal of crossing the road while dodging tra c. The chicken loses a life and starts from the original location if hit by a car. Points are only rewarded for successfully crossing the road. In Asteroids, the agent pilots a ship and gains points from shooting the asteroids. She must avoid colliding with asteroids which cost it lives. In Seaquest, a player swims under water. Periodically, as the oxygen gets low, she must rise to the surface for oxygen. Additionally, shes swim across the screen. The player gains points each time she shoots a sh. Colliding
8

(a) Adventure Seeker

(b) Cart-Pole

(c) Seaquest

(d) Asteroids

(e) Freeway

Figure 1: In experiments, we consider two toy environments (a,b) and the Atari games Seaquest (c), Asteroids (d), and Freeway (e)

with a sh or running out of oxygen result in death. In all three games, the agent has 3 lives, and the nal death is a terminal state. We label each loss of a life as a catastrophe state.

5 Experiments
First, on the toy examples, We evaluate standard DQNs and intrinsic fear DQNs using multilayer perceptrons (MLPs) with a single hidden layer and 128 hidden nodes. We train all MLPs by stochastic gradient descent using the Adam optimizer [16]. In Adventure Seeker, an agent can escape from danger with only a few time steps of notice, so we set the fear radius kr to 5. We phase in the fear factor quickly, reaching full strength in just 1000 steps. On this
9

(a) Seaquest

(b) Asteroids

(c) Freeway

(d) Seaquest

(e) Asteroids

(f) Freeway

Figure 2: Catastrophes ( rst row) and reward/episode (second row) for DQNs and Intrinsic Fear. On Adventure Seeker, all Intrinsic Fear models cease to “die” within 14 runs, giving unbounded (unplottable) reward thereafter. On Seaquest, the IF model achieves a similar catastrophe rate but signi cantly higher total reward. On Asteroids, the IF model outperforms DQN. For Freeway, a randomly exploring DQN (under our time limit) never gets reward but IF model learns successfully.

problem we set the fear factor λ to 40. For Cart-Pole, we set a wider fear radius of kr = 20. We initially tried training this model with a short fear radius but made the following observation: One some runs, IF-DQN would surviving for millions of experiences, while on other runs, it might experience many catastrophes. Manually examining fear model output on successful vs unsuccessful runs, we noticed that on the bad runs, the fear model outputs non-zero probability of danger for precisely the 5 moves before a catastrophe. In Cart-Pole, by that time, it is too to correct course. On the more successful runs, the fear model often outputs predictions in the range .1 − .5. We suspect that the gradation between mildly dangerous states and those with certain danger provides a richer reward signal to the DQN.
On both the Adventure Seeker and Cart-Pole environments, DQNs augmented by intrinsic fear far outperform their otherwise identical counterparts. We also compared IF to some traditional approaches for mitigating catastrophic forgetting. For example, we tried a memory-based method in which we preferentially sample the catastrophic states for updating the model, but they did not improve over the DQN. It seems that the notion of a danger zone is necessary here.
For Seaquest, Asteroids, and Freeway, we use a fear radius of 5 and a fear factor of .5. For all Atari games, the IF models outperform their DQN counterparts. Interestingly while for all games, the IF models achieve higher reward, on Seaquest, IF-DQNs have similar catastrophe rates (Figure 2). Perhaps the IF-DQN enters a region of policy space with a strong incentives to exchange catastrophes for higher reward. This result suggests an interplay between the various reward signals that warrants further exploration. For Asteroids and Freeway, the improvements are more dramatic. Over just a few thousand episodes of Freeway, a randomly exploring DQN achieves zero reward. However, the reward shaping of intrinsic fear leads to rapid improvement.

10

6 Related work
The paper studies safety in RL, intrinsically motivated RL, and the stability of Q-learning with function approximation under distributional shift. Our work also has some connection to reward shaping. We attempt to highlight the most relevant papers here. Several papers address safety in RL. Garcıa and Fernández [2015] provide a thorough review on the topic, identifying two main classes of methods: those that perturb the objective function and those that use external knowledge to improve the safety of exploration.
While a typical reinforcement learner optimizes expected return, some papers suggest that a safely acting agent should also minimize risk. Hans et al. [2008] de nes a fatality as any return below some threshold τ . They propose a solution comprised of a safety function, which identi es unsafe states, and a backup model, which navigates away from those states. Their work, which only addresses the tabular setting, suggests that an agent should minimize the probability of fatality instead of maximizing the expected return. Heger [1994] suggests an alternative Q-learning objective concerned with the minimum (vs. expected) return. Other papers suggest modifying the objective to penalize policies with high-variance returns [10, 8]. Maximizing expected returns while minimizing their variance is a classic problem in nance, where a common objective is the ratio of expected return to its standard deviation [28]. Moreover, Azizzadenesheli et al. [2018] suggests to learn the variance over the returns in order to make safe decisions at each decision step. Moldovan and Abbeel [2012] give a de nition of safety based on ergodicity. They consider a fatality to be a state from which one cannot return to the start state. Shalev-Shwartz et al. [2016] theoretically analyzes how strong a penalty should be to discourage accidents. They also consider hard constraints to ensure safety. None of the above works address the case where distributional shift dooms an agent to perpetually revisit known catastrophic failure modes. Other papers incorporate external knowledge into the exploration process. Typically, this requires access to an oracle or extensive prior knowledge of the environment. In the extreme case, some papers suggest con ning the policy search to a known subset of safe policies. For reasonably complex environments or classes of policies, this seems infeasible.
The potential oscillatory or divergent behavior of Q-learners with function approximation has been previously identi ed [5, 2, 11]. Outside of RL, the problem of covariate shift has been extensively studied [30]. Murata and Ozawa [2005] addresses the problem of catastrophic forgetting owing to distributional shift in RL with function approximation, proposing a memory-based solution. Many papers address intrinsic rewards, which are internally assigned, vs the standard (extrinsic) reward. Typically, intrinsic rewards are used to encourage exploration [26, 4] and to acquire a modular set of skills [7]. Some papers refer to the intrinsic reward for discovery as curiosity. Like classic work on intrinsic motivation, our methods perturb the reward function. But instead of assigning bonuses to encourage discovery of novel transitions, we assign penalties to discourage catastrophic transitions.
Key di erences In this paper, we undertake a novel treatment of safe reinforcement learning, While the literature o ers several notions of safety in reinforcement learning, we see the following problem: Existing safety research that perturbs the reward function requires little foreknowledge, but fundamentally changes the objective globally. On the other hand, processes relying on expert knowledge may presume an unreasonable level of foreknowledge. Moreover, little of the prior work on safe reinforcement learning, to the best of our knowledge, speci cally addresses the problem of catastrophic forgetting. This paper proposes a new class of algorithms for avoiding catastrophic states and a theoretical analysis supporting its robustness.
11

7 Conclusions
Our experiments demonstrate that DQNs are susceptible to periodically repeating mistakes, however bad, raising questions about their real-world utility when harm can come of actions. While it is easy to visualize these problems on toy examples, similar dynamics are embedded in more complex domains. Consider a domestic robot acting as a barber. The robot might receive positive feedback for giving a closer shave. This reward encourages closer contact at a steeper angle. Of course, the shape of this reward function belies the catastrophe lurking just past the optimal shave. Similar dynamics might be imagines in a vehicle that is rewarded for traveling faster but could risk an accident with excessive speed. Our results with the intrinsic fear model suggest that with only a small amount of prior knowledge (the ability to recognize catastrophe states after the fact), we can simultaneously accelerate learning and avoid catastrophic states. This work is a step towards combating DRL’s tendency to revisit catastrophic states due to catastrophic forgetting.
References
[1] Kamyar Azizzadenesheli, Emma Brunskill, and Animashree Anandkumar. E cient exploration through bayesian deep q-networks. arXiv preprint arXiv:1802.04412, 2018.
[2] Leemon Baird. Residual algorithms: Reinforcement learning with function approximation. 1995. [3] Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning environment:
An evaluation platform for general agents. J. Artif. Intell. Res.(JAIR), 2013. [4] Marc G Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, and Remi Munos.
Unifying count-based exploration and intrinsic motivation. In NIPS, 2016. [5] Justin Boyan and Andrew W Moore. Generalization in reinforcement learning: Safely approximating
the value function. In NIPS, 1995. [6] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. OpenAI gym, 2016. arxiv.org/abs/1606.01540. [7] Nuttapong Chentanez, Andrew G Barto, and Satinder P Singh. Intrinsically motivated reinforcement
learning. In NIPS, 2004. [8] Yinlam Chow, Aviv Tamar, Shie Mannor, and Marco Pavone. Risk-sensitive and robust decision-making:
A CVaR optimization approach. In NIPS, 2015. [9] Mehdi Fatemi, Layla El Asri, Hannes Schulz, Jing He, and Kaheer Suleman. Policy networks with
two-stage training for dialogue systems. In SIGDIAL, 2016. [10] Javier Garcıa and Fernando Fernández. A comprehensive survey on safe reinforcement learning. JMLR,
2015. [11] Geo rey J Gordon. Chattering in SARSA(λ). Technical report, CMU, 1996. [12] Steve Hanneke. The optimal sample complexity of PAC learning. JMLR, 2016. [13] Alexander Hans, Daniel Schneegaß, Anton Maximilian Schäfer, and Ste en Udluft. Safe exploration
for reinforcement learning. In ESANN, 2008.
12

[14] Matthias Heger. Consideration of risk in reinforcement learning. In Machine Learning, 1994. [15] Nan Jiang, Alex Kulesza, Satinder Singh, and Richard Lewis. The dependence of e ective planning
horizon on model accuracy. In International Conference on Autonomous Agents and Multiagent Systems, 2015. [16] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015. [17] Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel. End-to-end training of deep visuomotor policies. JMLR, 2016. [18] Long-Ji Lin. Self-improving reactive agents based on reinforcement learning, planning and teaching. Machine learning, 1992. [19] Zachary C Lipton, Jianfeng Gao, Lihong Li, Xiujun Li, Faisal Ahmed, and Li Deng. E cient exploration for dialogue policy learning with bbq networks & replay bu er spiking. In AAAI, 2018. [20] James L McClelland, Bruce L McNaughton, and Randall C O’Reilly. Why there are complementary learning systems in the hippocampus and neocortex: Insights from the successes and failures of connectionist models of learning and memory. Psychological Review, 1995. [21] Michael McCloskey and Neal J Cohen. Catastrophic interference in connectionist networks: The sequential learning problem. Psychology of learning and motivation, 1989. [22] Volodymyr Mnih et al. Human-level control through deep reinforcement learning. Nature, 2015. [23] Teodor Mihai Moldovan and Pieter Abbeel. Safe exploration in Markov decision processes. In ICML, 2012. [24] Makoto Murata and Seiichi Ozawa. A memory-based reinforcement learning model utilizing macroactions. In Adaptive and Natural Computing Algorithms. 2005. [25] Will Night. The AI that cut google’s energy bill could soon help you. MIT Tech Review, 2016. [26] Jurgen Schmidhuber. A possibility for implementing curiosity and boredom in model-building neural controllers. In From animals to animats: SAB90, 1991. [27] Shai Shalev-Shwartz, Shaked Shammah, and Amnon Shashua. Safe, multi-agent, reinforcement learning for autonomous driving. 2016. [28] William F Sharpe. Mutual fund performance. The Journal of Business, 1966. [29] David Silver et al. Mastering the game of go with deep neural networks and tree search. Nature, 2016. [30] Masashi Sugiyama and Motoaki Kawanabe. Machine learning in non-stationary environments: Introduction to covariate shift adaptation. MIT Press, 2012. [31] Richard S. Sutton. Learning to predict by the methods of temporal di erences. Machine Learning, 1988. [32] Vladimir Vapnik. The nature of statistical learning theory. Springer science & business media, 2013. [33] Christopher J.C.H. Watkins and Peter Dayan. Q-learning. Machine Learning, 1992.
13

An extension to the Theorem 2

In practice, we gradually learn and improve F where the di erence between learned F after two consecrative

π∗

π∗

updates, Ft and Ft+1, consequently, ω Ft ,γplan and ω Ft+1,γplan decrease. While Ft+1 is learned through using

π∗

the samples drawn from ω Ft ,γplan , with high probability

∫

π∗
ω Ft ,γplan (s)

F (s) − Ft+

(s )

ds

≤

VC(F ) + log 3200

1 δ

s ∈S

1

N

π∗
But in the nal bound in Theorem 2, we interested in ∫s ∈S ω Ft+1,γplan (s) F (s) − Ft+1(s) ds. Via decomposing in into two terms

∫

π∗

∫

π∗

π∗

ω Ft ,γplan (s) F (s) − Ft +1(s) ds +

|ω Ft+1,γplan (s) − ω Ft ,γplan (s)|ds

s ∈S

s ∈S

π∗

π∗

Therefore, an extra term of λ 1−γ1plan ∫s ∈S |ω Ft+1,γplan (s) − ω Ft ,γplan (s)|ds appears in the

Theorem 2.

nal bound of

Regarding

the

choice

of

γpl

an

,

if

λ

V

C ( F )+log N

1 δ

is less than one, then the best choice of γplan is γ . Other wise,

if

V

C ( F )+log

1 δ

N

is equal to exact error in the model estimation, and is greater than 1, then the best γplan is 0.

Since, V C(FN)+log δ1 is an upper bound, not an exact error, on the model estimation, the choice of zero for

γplan is not recommended, and a choice of γplan ≤ γ is preferred.

14

