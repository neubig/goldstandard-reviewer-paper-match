arXiv:1711.03066v1 [cs.IR] 8 Nov 2017

A Simple Derivation of the Heap’s Law from the Generalized Zipf’s Law
Leonid Boytsov
November, 2017
Abstract I reproduce a rather simple formal derivation of the Heaps’ law from the generalized Zipf’s law, which I previously published in Russian [7].
1 Introduction
There are two well-known regularities in natural language texts, which are known as Zipf’s and Heaps’ laws. According to the original Zipf’s law, a probability of encountering the i-th most frequent word wi is inversely proportional to the word’s rank i:
pi = O(1/i). This law which was not actually discovered by Zipf [5] is not applicable to arbitrarily large texts. The obvious reason is that the sum of inverse ranks does not converge to a ﬁnite number. A slightly generalized variant, henceforth generalized Zipf’s law, is likely a more accurate text model:
pi = O(1/iα),
where α > 1. Heaps’ law [3] also discovered by Herdan [2] approximates the number of
unique words in the text of length n:
| ∪ni=1 {wi}| = O(nβ),
where β < 1. Heaps’ law says that the number of unique words grows roughly sub-linearly as a power function of the total number of words (with the exponent strictly smaller than one).
Somewhat surprisingly, Baeza-Yates and G. Navarro [1] argued (although a bit informally) that constants in Heaps’ and Zipf’s laws are reciprocal numbers:
α ≈ 1/β.
They also veriﬁed this empirically.
1

This work inspired several people (including yours truly) to formally derive Heaps’ law from the generalized Zipf’s law (all derivations seem to have relied on a text generation process where words are sampled independently). It is hard to tell who did it earlier. In particular, I have a recollection that Amir Dembo (from Stanford) produced an analgous derivation (not relying on the property of the Gamma function), but, apparently, he did not publish his result. Leijenhorst and Weide published a more general result (their derivation starts from Zipf-Mandelbrot distribution rather than from generalized Zipf) in 2005 [6]. My own proof was published in Russian in 2003 [7]. Here, I reproduce it for completeness. I tried to keep it as simple as possible: some of the more formal argument is given in footnotes.

2 Formal Derivation

As a reminder we assume that the text is created by a random process where words are sampled independently from an inﬁnite vocabulary. This is not the most realistic assumption, however, it is not clear how one can incorporate word dependencies into the proof. The probability of sampling the i-th most frequent word is deﬁned by the generalized Zipf’s law, i.e.,

1

pi = H(α) · iα ,

(∗)

where α > 1 and H(α) = ∞ i=1 1/iα is a normalizing constant. The number of unique words in the text is also a random variable X, which
can be represented as an inﬁnite sum of random variables Xi. Note that Xi is equal to one if the text contains at least one word wi and is zero otherwise. The objective of this proof is to estimate the expected number of unique words EX:

EX = E

∞
Xi
i=1

The proof only cares about an asymptotic behavior of EX with respect to the total number of text words n, i.e., all the derivations are big-O estimates.
Because words are sampled randomly and independently, a probability of not selecting word wi after n trials is equal to (1 − pi)n. Hence, Xi has the Bernoulli distribution with the success probability

p(Xi = 1) = 1 − (1 − pi)n,

where pi is a probability of word occurrence according to the generalized Zipf’s law given by Eq. (*). Therefore, we can rewrite the expected number of unique words as follows:

∞

∞

1

n

EX = 1 − (1 − pi)n = 1 − 1 −

H (α)iα

i=1

i=1

2

What can we say about this series in general and about the summation term 1 − (1 − pi)n in particular?
• Because 0 < 1 − (1 − pi)n < n · pi and {pi} is a convergent series, our series converges.1

• The summation term can be interpreted as a real valued function of the
variable i. The value of this function decreases monotonically with i. The function is positive for i ≥ 0 and is upper bounded by one.2

Thanks to these properties, we can replace the sum of the series with the following big-O equivalent integral from 1 to ∞:3

∞

1

n

1− 1−

dx

(∗∗)

1

H (α)xα

Using

the

variable

substitution

y

=

xH

(α

)

1 α

,

we

rewrite

(**)

as

follows:

∞
1 1−
H (α)
1
H(α) α

1 1 − yα

n
dy

Because the integrand is positive and upper bounded by one, the value of

the

integral

for

the

segment

[0,

H

(α

)

1 α

]

is

a

constant

with

respect

to

n.

H (α)

is a constant as well. Therefore, the value of the integral is big-O equivalent to

the value of the following integral which goes from one to inﬁnity:

∞ 1n 1 − 1 − yα dy
1

We further rewrite this by applying the Binomial theorem to the integrand:

∞

n

∞

n

∞n

1 1− 1 − yα

dy =

1 − (−1)iCni y1αi dy =

(−1)iCni y1αi dy

1

1

i=0

1 i=1

Because α > 1, every summand in the integrand has absolute convergence.4 Hence, the integral of the ﬁnite sum is equal to the following sum of integrals:

n

∞

n

Cni (−1)i

1 yαi dy =

Cni (−1)i iα 1− 1 =

i=1

1

i=1

1The upper bound for the series term follows from 1 − qn = (1 − q)(qn−1 + qn−2 + . . . + 1),
which is upper bounded by (1 − q) · n for 0 < q < 1 and positive n. 2pi decreases with i; 1−pi increases with i; (1−pi)n increases with i; 1−(1−pi)n decreases
with i. 3 Using monotonicity it is easy to show that the integral from 1 to ∞ is smaller than the
sum of the series, but the integral from 0 to ∞ is larger than the sum of the series. The
diﬀerence between two integral values is less than one. 4This is concerned with the convergence of the integral with respect to its inﬁnite upper
bound.

3

1n

1

=

Cni (−1)i

=

α

i − (1/α)

i=1

(because the term for i = 0 is equal to minus one)

1n

=1+

Cni (−1)i

α

i=0

1 i − (1/α)

(∗ ∗ ∗)

Using induction one can demonstrate that (also see [4, §1.2.6, Exercise 48]):

i (−1)i

n!

Cn i + x = x(x + 1) . . . (x + n)

i≥0

This allows to rewrite Eq. (***) as follows:

1/α · n! 1+
(−1/α)(1 − 1/α) . . . (n − 1/α)

Now, using the formula nxn!
Γ(x) = lim n→∞ x(x + 1) . . . (x + n)

and its corrollary

n!

= O Γ(x) · n−x

x(x + 1) . . . (x + n)

with x = −1/α we obtain that (***) is big-O equivalent to

Γ(−1/α) · n1/α = O n1/α .

In other words, the constant β is in the Heaps’ law is inversely proportional to the constant α in the generalized Zipf’s law.

References
[1] Ricardo A. Baeza-Yates and Gonzalo Navarro. Block addressing indices for approximate text retrieval. JASIS, 51(1):69–82, 2000.
[2] Leo Egghe. Untangling Herdan’s Law and Heaps’ Law: Mathematical and informetric arguments. J. Am. Soc. Inf. Sci. Technol., 58(5):702–709, March 2007.
[3] H. S. Heaps. Information Retrieval: Computational and Theoretical Aspects. Academic Press, Inc., Orlando, FL, USA, 1978.
[4] Donald Ervin Knuth. The art of computer programming, Volume II: Seminumerical Algorithms, 3rd Edition. Addison-Wesley, 1998.

4

[5] David M. W. Powers. Applications and explanations of Zipf’s Law. In Proceedings of the Joint Conference on New Methods in Language Processing and Computational Natural Language Learning, NeMLaP/CoNLL 1998, Macquarie University, Sydney, NSW, Australia, January 11-17, 1998, pages 151–160, 1998.
[6] D.C. van Leijenhorst and Th.P. van der Weide. A formal derivation of Heaps’ Law. Information Sciences, 170(2):263 – 272, 2005.
[7] ЛМ Бойцов. Синтез системы автоматической коррекции, индексации и поиска текстовой информации, 2003.
5

