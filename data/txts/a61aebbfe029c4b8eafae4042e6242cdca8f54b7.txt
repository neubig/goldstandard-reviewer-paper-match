Relation-Guided Pre-Training for Open-Domain Question Answering
Ziniu Hu, Yizhou Sun, Kai-Wei Chang University of California, Los Angeles
{bull, yzsun, kwchang}@cs.ucla.edu

arXiv:2109.10346v1 [cs.CL] 21 Sep 2021

Abstract
Answering complex open-domain questions requires understanding the latent relations between involving entities. However, we found that the existing QA datasets are extremely imbalanced in some types of relations, which hurts the generalization performance over questions with long-tail relations. To remedy this problem, in this paper, we propose a Relation-Guided Pre-Training (RGPT-QA) framework1. We ﬁrst generate a relational QA dataset covering a wide range of relations from both the Wikidata triplets and Wikipedia hyperlinks. We then pre-train a QA model to infer the latent relations from the question, and then conduct extractive QA to get the target answer entity. We demonstrate that by pretraining with propoed RGPT-QA techique, the popular open-domain QA model, Dense Passage Retriever (DPR), achieves 2.2%, 2.4%, and 6.3% absolute improvement in Exact Match accuracy on Natural Questions, TriviaQA, and WebQuestions. Particularly, we show that RGPT-QA improves signiﬁcantly on questions with long-tail relations.
1 Introduction
Open domain question answering is a challenging task that answers factoid questions based on evidence in a large corpus (e.g., Wikipedia). Most open-domain QA systems follow retriever-reader pipeline (Chen et al., 2017), in which a retriever selects a subset of candidate entities and associated passages from the corpus that might contain the answer, then a reader extracts a text span from the passages as the answer. This process involves multiple entities that are relevant to answer the question. The QA system is required to extract these entities from the question and passages and identify the (latent) semantic relations between these entities in order to answer the question. For example, to
1Dataset and code are released at https://github. com/acbull/RGPT-QA.

answer the following question: “Where did Steph Curry play college basketball at?”, the QA model is required to reason the implicit relation triplet Steph Curry, Educated At, Davidson College to identify the correct answer.
To capture the relation knowledge required to answer questions, most QA systems rely on humanannotated supervised QA datasets. However, it is expensive and tedious to annotate a large set of QA pairs that cover enough relational facts for training a strong QA model. In addition, we showed that even for a large QA dataset like Natural Questions (Kwiatkowski et al., 2019), its training set only covers 16.4% of relations in WikiData (Vrandecic and Krötzsch, 2014) knowledge graph. Moreover, for those covered relations, the frequency distribution is imbalanced, i.e., 30% of relation types appear only once. Consequently, for the questions involving infrequent (a.k.a, long-tail) relations in the training set, the QA exact match accuracy is 22.4% lower than average. Such a biased relation distribution of existing QA datasets severely hurts the generalization of trained QA systems.
To improve the open-domain QA systems for questions with long-tail relations, in this paper, we propose RGPT-QA, a simple yet effective RelationGuided Pre-training framework for training QA models with augmented relationa facts from knowledge graph. The framework consists of two steps: 1) generate a relational QA dataset that covers a wide range of relations without human labeling; 2) pre-train a QA model to predict latent relations from questions and conduct extractive QA.
The key of our framework is to generate a relational QA dataset that align entities in Wikipedia passages with structured knowledge graph (e.g., WikiData). We call such a dataset Grounded Relational Wiki-Graph. In this graph, each edge indicates the relationship of two connected entities, and the edge is linked to a passage in Wikipedia describing this relationship. As WikiData knowledge

CDF

1.0 0.8 0.6 0.4 0.2 0.0 1 2 5 10 20 50 100 200 500 1000
Frequency of each Relation (Log-Scale)
Figure 1: Cumulative distribution function (CDF) of relation frequency in Natural Question Training set.

Figure 2: Exact Match accuracy of a trained DPR model in validation set with different relation frequency in training set.

graph also suffers from low coverage of long-tail entities and relations, we further convert hyperlinks in Wikipedia into knowledge triplets without specifying relation labels. Next, we link each relation triplet to a Wikipedia passage to help generate natural questions. We assume that if one passage in the Wiki-page of source entity contains the target entity, then the context in this passage describes the relationship between the two entities. With the constructed graph, we use a template to synthesize question and answer pairs and then pre-train the QA model to capture the relational facts for answering complex open-domain questions.
As a pre-training method, RGPT-QA can be incorporated with any open-domain QA system. In this paper, we utilize the recently developed Dense Passage Retriever (DPR) (Karpukhin et al., 2020) as the base QA system to evaluate the proposed pretraining effectiveness. Experimental results show that RGPT-QA enhances DPR’s Exact Match accuracy by 2.2%, 2.4%, and 6.3% on Natural Questions, TriviaQA and WebQuestions respectively. Compared with the existing QA pre-training methods (Lee et al., 2019; Guu et al., 2020a; Lewis et al., 2019), RGPT-QA explicitly captures a wide range of relational facts and thus achieves better performance. Moreover, for the questions containing long-tail relations in Natural Questions, the performance is improved by 10.9%, showing that RGPT-QA alleviates the unbalanced relation distribution problem in the existing QA datasets.
The key contributions of this paper are:
• We propose RGPT-QA, a pre-training method to inject knowledge from relational facts in knowledge graph into QA models.
• RGPT-QA enhances the performance of a popular QA model, i.e., DPR, especially on the questions with long-tail relations.

2 Preliminary and Empirical Analysis
In this section, we ﬁrstly introduce the retrieverreader pipeline for open-domain QA, and then we analyze how the relation distribution in existing QA datsets inﬂuence generalization performance.
Open-Domain Question Answering. We focus on open-domain question answering that requires to extract answer from a large corpus (e.g. Wikipedia) C = {pi}Ni=1 containing N passages. Most open-domain QA systems follow a retrieverreader pipeline proposed by Chen et al. (2017). Given a factoid question q, the QA system ﬁrst retrieves K relevant passages {pj}Kj=1 from the corpus C. Then a reading comprehension module extracts a text span wstart, . . . , wend from one of these retrieved passages as the answer a to the question. Some QA dataset annotated the passage where the answer a is derived. We called this passage ground truth passage.
For the retriever, earlier systems utilize termbased retrieval methods, such as TF-IDF and BM25, which fails to capture the semantic relationship between question and passage beyond lexical matching. Recent studies (Lee et al., 2019; Karpukhin et al., 2020; Dhingra et al., 2020) use BERT-like pretrained language model to encode the question and passages independently into dense representations, and use maximum inner product search (MIPS) algorithms (Shrivastava and Li, 2014) to efﬁciently retrieve the most similar passage for each question. In this paper, we utilize Dense Passage Retriever (DPR) (Karpukhin et al., 2020) as the base QA model.
Relation Bias of Existing QA Datasets. We ﬁrst explore how much relational knowledge between entities is required to answer the questions in the existing open-domain QA dataset. We con-

duct an empirical study to analyze the relation distribution in Natural Questions, one of the largest open-domain QA datasets, and how it inﬂuences QA model’s performance.
For each question in Natural Question training set, we ﬁrst select the entity that the ground-truth passage is associated with. We then combine the entity with the answer as an entity pair, and check whether we can ﬁnd a relation triplet in WikiData describing the relation between these two entities. Out of 58,880 training QA pairs, there are 23,499 pairs that could be aligned. The aligned QA pairs cover 329 relations, which accounts for 16.4% of the total 2,008 relations in WikiData. For most unaligned QA pairs, the answers are not entities and thus cannot be aligned to the graph.
In addition to the low relation coverage issue in Natural Question, we also ﬁnd that the relation distribution is imbalanced. As showed in Figure 1, 90% of relations have frequency less than 41, and 30% of relations appear only once. On the contrary, the most frequent relation “P161 (cast member)” appears 1,915 times out of 9,238 aligned QA pairs. A complete list of all these relations with aligned QA pairs is shown in Table 6-9 in Appendix.
We then study whether the imbalanced relation distribution inﬂuences the performance of QA models trained on these datasets. We use a DPR model trained on training set of Natural Questions and then calculate the Exact Match accuracy in validation set of each aligned QA pairs. We then analyze the correlation of the accuracy with the relation frequency in training set. As illustrated in Figure 2, the validation set accuracy is overall proportional to the relation frequency in training set. For those relations with frequency less than 5, the average accuracy is only 20.3%, much lower than the average accuracy 42.7% over all samples in validation set. This shows that the relation bias in existing QA datasets severely inﬂuences the generalization of QA models to questions with long-tail relations.
3 Method
In this section, we will discuss RGPT-QA framework in: 1) how to generate relational QA dataset for the pre-training purpose; and 2) how to construct a self-training task to empower QA model to capture relational facts.

# of linked Entity # of relation labels # of labelled triplet # of unlabeled triplet (hyperlink) # of grounded descriptions per triplet

5,640,366 2,008
14,463,728 66,796,110
1.25

Table 1: Statistics of Grounded Relational Wiki-Graph.

3.1 Construct QA Pre-Training Dataset
To help QA model capture the knowledge from relation facts required to answer open-domain questions, we ﬁrst focus on generating QA pre-training dataset, in which there exist relation connections between the source entity in questions to the target answer. Speciﬁcally, each QA pair datapoint d = s, r, t , q, p+ consists of three components: 1) relational triplet s, r, t , in which r denotes the relation between source entity s and target entity t; 2) question q in natural language asking which entity has relation r to source entity s, with target entity t as the correct answer; 3) positive context passage p+ ∈ C[s], a passage from source entity’s Wiki-page that contains the target answer t.
Grounded Relational Wiki-Graph. To generate QA pre-training dataset, leveraging the relation triplets in knowledge graph, e.g., WikiData, is a natural choice to deﬁne questions that require relation reasoning. We therefore construct Grounded Relational Wiki-Graph, in which each relation triplet s, r, t is linked to a set of description passages {desc.(s, t)} in the Wiki-page of entity s. These descriptions would be later utilized to generate questions q and positive context passages p+.
To construct such a graph, we use the 2021 Jan. English dump of Wikidata and Wikipedia. For each Wikipedia hyperlink s, ?, t (? denotes the relation is unlabeled), the passage containing anchored text to t in the Wiki-page of s naturally ﬁts our requirement for desc.(s, t). For each WikiData relation triplet s, r, t , if the two entities are linked by a hyperlink in Wikipedia, we label the relation of the aligned hyperlink as r. For the other triplets s, r, t without alignment with hyperlinks, we extract all mentioning of target entity t from the Wiki-page of s, and use the context passage as desc.(s, t). The dataset statistics are shown in Table 1.
Relational QA Pair Generation In the following, we introduce the details to generate the relational QA pair from the constructed graph.
Recent unsupervised QA studies (Li et al., 2020; Pan et al., 2020) revealed that if the question q and

Figure 3: Example of a generated relational QA pair from Grounded Relational Wiki-Graph.

context passage p+ share a large lexical overlap, then the QA model could utilize low-level lexical patterns as shortcuts to ﬁnd the answer. These shortcuts hinder the model from learning to comprehend the passages and answer the questions, hurting model’s generalizability. To avoid this lexical overlap issue, we aim to generate questions from a passage that is different from the context passage p+.
We ﬁrst select all the entity pairs s, t that have mutual links in the Grounded Relational WikiGraph, with desc.(s, t) and desc(t, s) in part of Wikipage of s and t respectively , describing the relationship between the two entities. Without loss of generality, we denote s as source entity and t as the target answer. The passage desc.(s, t) containing target answer t can be used as the positive passage p+.
Next, we generate a question that is lexically different from p+ using the following template:
q(s, r, t) = [MASK(r)] of [s] which [desc.(t, s)]?
in which MASK(r) is a relation mask token. As desc.(t, s) contains source entity s, it provides information to describe the relationship between s and t, based on which the QA model should learn to infer the latent relation r, and retrieve positive passage p+ = desc.(s, t) and extract answer entity t. In addition, as desc.(t, s) and desc.(s, t) come from different Wiki-page, our question generation

procedure can avoid the lexical overlap issue that often occur in prior Unsupervised QA methods.
Mask Target Answer. As description desc.(t, s) is from target answer t’s wiki-page, it often contains the name of entity t. We thus need to mask t from the question. Otherwise, the pre-trained model can simply identify the answer to a question based on the local patterns.
As an example, in Figure 3, we show how to generate question for triplet Stephen Curry, ?, Splash Brothers . We ﬁrstly retrieve two descriptive passages desc.(s, t) and desc.(t, s) in two entities’ wiki pages. Using the template, we generate the question along with the ground-truth passage. We then mask out the target entity in question and source entity in true passage (will discuss later in retrieval pre-training) to avoid shortcut. A list of generated relational QA pairs are shown in Table 10 in Appendix.
3.2 Relation-Guided QA Pre-Training
With the generated relational QA dataset, we introduce how to pre-train both retriever and reader components in the QA model.
3.2.1 Relation Prediction Pre-Training
Our generated QA dataset contains the relation label r between the source entity s and the answer target t. Therefore, we design a self-training task to guide the model to predict the latent relation

in question, which can beneﬁt both retriever and Two-Level Negative Passage Sampling. As we

reader. Speciﬁcally, we adopt a linear projection cannot enumerate all other passages in the denomi-

layer LR(·) over the BERT[CLS] token embedding nator of Eq(1), we need to sample a set of negative

to predict the relation over the WikiData relation passages for contrastive learning. Previous stud-

set. The pre-training loss of relation prediction is: ies (Karpukhin et al., 2020) have revealed that it is

1 Lrel = B − log P (r | q; θ),
q

essential that the sampled negative passages should be hard enough to train the retriever. As the question and passage embeddings are encoded indepen-

dently, DPR can efﬁciently calculate the similarity

Self-Distillation for Unlabelled Relation The of each question to all passages in the batch via

hyperlinks in wikipedia also provide valuable im- dot product. Based on this property, as long as the

plicit information about the relations between enti- passages within a batch are similar to each other,

ties. To leverage them, we use the trained relation predictor at each epoch with ﬁxed parameter θˆ as

they serve the hard cases of negative passages to others. We thus propose a two-level negative pas-

teacher model to assign soft label and then progres- sage sampling strategy to construct hard cases for

sively train the relation predictor as student model training the retriever in the following.

based on the assigned labels in the next epoch. This

We ﬁrst sample at the level of entity. Given a set

approach is referred to as self-distillation in the lit- of randomly sampled b entities, we adopt random

erature (Xie et al., 2020; Chen et al., 2020). We walk from these seed entities over the Grounded

minimize this self-distillation loss as:

Relational Wiki-Graph to get B entities. As the

1 Ldistill = B
q

connected entities have a relationship, their true − log P (rˆ| q; θ) · sg P (rˆ| q; θˆ) , passages are also semantically similar, and thus

rˆ

serve as good negative samples. We then conduct

where sg(·) denotes the operation of stop gradient, which avoids back propagation to the teacher network with ﬁxed parameter θˆ. rˆ is enumerating all the relation labels.
As the relation predictor at early stages cannot give a reasonable prediction, we put a dynamic weight schedule to Ldistill by a time-dependent weighting term 1 − e−epoch, which ramps up from zero to one. Combing the weighted self-distillation loss Ldistill with the supervised relation loss Lrel, we get the ﬁnal relation loss Lˆrel to train the model capturing all relational facts covered in the Grounded Relational Wiki-Graph.

sampling at the level of passage. For each source entity si with positive passage p+i ∈ C[si], we randomly pick K other passages from the same Wiki-page to form a negative passage set p−i,j ∈ C[si], s.t. p−i,j = p+i Kj=1. These negative passages are similar to p+i , as they all describe the same entity si.
After we collect both the positive and K nega-
tive passages for all the entities, we use the passage
encoder EncP to get a passage embedding matrix P with dimension (1 + K) · B × d . We also
use question encoder EncQ to get question embedding matrix Q with dimension B × d . We then get a similarity matrix S = QPT with dimension

3.2.2 Dense Retrieval Pre-Training

B × (1 + K) · B , in which the diagonal entry

The goal of dense retrieval pre-training is to get a question encoder EncQ and a passage encoder EncP to map questions and all passages in the

corresponds to the similarity between question and its positive passage. We thus calculate the retrieval loss with in-batch negative samples via:

Wiki Corpus C into an embedding space, such that
each question q is close to its ground-truth positive context passage p+ in the embedding space. The

1 Lretr = B

i∈[1,B]

− log softmax(S) [i,i] . (2)

objective is as follows:

Pretr(p+ | q, C) =

exp sim(q, p+) , (1)
p∈C exp sim(q, p)

Masking Source Entity. As the true passage p+i = desc.(s, t) might contain the name of source entity s. We mask out all the tokens of s from the
extracted passages, so that the model is required to

where sim(q, p) is the cosine similarity between the understand the passages for correct retrieval instead

normalized embeddings of question and passage. of exploiting a shortcut.

3.2.3 Reading Comprehension Pre-Training
The goal of reading comprehension pre-training is to get a neural reader that re-ranks the top-k retrieved passages and extracts an answer span from each passage as the answer. The probability of a passage contains the target answer t, and each token in the selected passage being the starting/ending positions of an t are deﬁned as:

Prank(t ∈ p) =

exp Lrank BERTCLS(q, p) , pˆ exp Lrank BERTCLS(q, pˆ)

Pstart(i | p, q) = exp Lstart BERT[i](q, p) , j exp Lstart BERT[j](q, p)

Pend(i | p, q) = exp Lend BERT[i](q, p) . j exp Lend BERT[j](q, p)

where L∗ are linear project layers with different parameters. Note that the re-ranking module adopts
cross-attention over questions and passages rather
than the dot product of two independently encoded
embedding used in retriever. For each QA pair d = s, r, t , q, p+ , we select m other passages in wiki-page of entity s as negative passages, and maximize Prank(t ∈ p+). Then, we calculate Pstart(i | p+, q) and Pend(i | p+, q) and maximize the probability for the ground-truth span of target answer t. Combing the passage re-ranking
and span extraction objectives, we get readingcomprehension loss Lread.

4 Experiments

In this section, we evaluate RGPT-QA on three open-domain QA datasets: Natural Questions (NQ), Trivia QA and Web Questions (WQ).

4.1 Experiment Settings
We follow the pre-processing procedure described in DPR (Karpukhin et al., 2020) for a fair comparison. We use the English Wikipedia from Dec. 20, 2018 and split each article into passages of 100 disjoint words as the corpus. For each question in all the three datasets, we use a passage from the processed Wikipedia which contains the answer as positive passages. We evaluate the QA system by Exact Match (EM) Accuracy on the correct answer.
Our RGPT-QA could be integrated with any open-domain QA system. In this paper, we incorporate it with the recently developed QA system, Dense Passage Retriever (DPR) (Karpukhin et al., 2020) to evaluate our pre-training framework. The DPR model uses the RoBERTa-base (d=768, l=12)

model as the base encoder. We ﬁrst pre-train the retriever and reader in DPR using RGPT-QA. For retriever, we use the negative passage sampling strategy (c.f. Sec. 3.2.2), with initial entity size set to be 12, batch size of 128 and the hard negative passage number of 2. For reader, we randomly sample 64 source entities per batch to calculate the loss. For each entity, we sample 2 hard negative passages for re-ranking. We pre-train both the retriever and reader for 20 epochs using AdamW optimizer and a learning rate warm-up followed by linear decay. Pre-training is run on 8 Tesla V100 GPUs for two days. After the pre-training, we ﬁne-tune the retriever and reader on each QA dataset following the same procedure and hyper-parameters described in DPR (Karpukhin et al., 2020).
QA Pre-Training Baselines. We compare RGPT-QA with three recently proposed pretraining methods for open-domain QA.
T5 (Raffel et al., 2020) adopts multiple generative tasks to pre-train a generative model. The ﬁne-tuned QA models directly generate answers without needing an additional retrieval step.
ORQA (Lee et al., 2019) adopts a Inverse Cloze Task (ICT) to pre-train retriever, which forces each sentence’s embedding close to context sentences.
REALM (Guu et al., 2020a) incorporates a retriever as a module into language model and trains the whole model over masked entity spans.
We directly report the results listed in their papers as they follow the same experiment settings.
We also add two knowledge-guided language models as baselines. Though not targeted at QA problem, these two methods are both designed to capture structured knowledge.
KnowBERT (Peters et al., 2019) adds entity embedding to each entity mention in text, and adopts the entity linking objective to pre-train the model.
KEPLER (Wang et al., 2019) uses Knowledge Embedding objective, i.e., TransE, to guide embedding encoded over entity description.
We initialize DPR base encoders by the released pre-trained models of these two work, and then ﬁnetune on each QA dataset with the same procedure.
We also add a Unsupervised Question Answering (Unsup.QA) (Lewis et al., 2019) as a baseline. For each entity as the answer, Unsup.QA selects a passage containing the entity as context passage and a cloze question. The cloze question is later rewritten by a machine translator to natural language. We use the generated QA dataset to pre-train both

Pre-Trained for QA Supervised

QA System Name
BM25+BERT (Lee et al., 2019) HardEM (Min et al., 2019a) GraphRetriever (Min et al., 2019b) PathRetriever (Asai et al., 2020) DPR (Karpukhin et al., 2020)
T5 (large) (Raffel et al., 2020) ORQA (Lee et al., 2019) REALMWiki (Guu et al., 2020a) REALMNews (Guu et al., 2020a) DPR (KnowBERT (Peters et al., 2019)) DPR (KEPLER (Wang et al., 2019)) DPR (Unsup.QA (Lewis et al., 2019))
Ours, DPR (RGPT-QA)

Pre-Training Task for QA
-
T5 (Multitask) ICT
REALM REALM Entity Linking TransE Cloze Translation
RGPT-QA

NQ (58.9k/3.6k)
26.5 28.1 34.5 32.6 41.5
29.8 33.3 39.2 40.4 39.1 40.9 41.9
43.7

Trivia QA (60.4k/11.3k)
47.1 50.9 56.0
56.8
45.0
56.4 57.1 57.3
59.2

WQ (2.5k/2k)
17.7 -
36.4 -
34.6
32.2 36.4 40.2 40.7 34.8 35.2 36.5
40.9

Table 2: End-to-end QA Exact Match Accuracy (%) on test sets of three Open-Domain QA datasets, with the number of train/test examples shown in paretheses below. All the results except the last four rows are copied from the original papers. “–” denotes no results are available. Models in the ﬁrst block are initialized by BERT/RoBERTa and then directly ﬁne-tuned on the supervised QA datasets. While models in the second block are initialized by RoBERTa and then tuned on some QA pre-training tasks ﬁrst, and then ﬁne-tuned on the supervised QA datasets.

the retriever and reader of the DPR framework. 4.2 Experimental Results

Pre-Train Model
RoBERTa KnowBERT KEPLER Unsup.QA
RGPT-QA

NQ
78.4 / 63.3 76.7 / 62.6 77.9 / 62.8 78.6 / 63.7
80.1 / 64.8

Trivia QA
79.4 / 72.6 78.9 / 72.2 79.7 / 72.9 79.9 / 73.0
81.2 / 73.7

WQ
73.2 / 58.1 73.4 / 58.3 74.5 / 58.6 74.5 / 59.1
76.7 / 61.0

Table 3: Retrieval (left) accuracy over Top-20 results and Reader (right) Exact Match over Golden-Passages on validation sets of three Open-Domain QA datasets.

Mask NPS Ldistill Lrel NQ Trivia QA WQ







 44.3

59.8

41.4





  39.7 56.3

34.2





  43.5 58.1 39.8







 43.8 59.3 40.8







 43.1 58.5 40.0

Table 4: Ablation of RGPT-QA components on validation sets of three Open-Domain QA datasets. Mask: Mask target entity from question and source entity from passage; NPS: Two-level Negative Passage Sampling.

Table 2 summarizes the overall EM accuracy of the QA systems on the three datasets. The DPR framework pre-trained by RGPT-QA outperforms all other open-domain QA systems. Comparing with DPR without pre-training, RGPT-QA achieves 2.2%, 2.4% and 6.3% enhancement in EM accuracy on the three datasets.

B K NQ Trivia QA WQ
128 2 80.1 81.2 76.6 128 1 79.7 80.8 76.1 64 2 79.6 80.6 75.8 64 1 79.2 80.1 75.3
Table 5: Ablation of batch size and negative sampling for retrieval pre-training. B: Batch Size; K: Number of other passages as negative sample.
Comparing with other pre-training tasks for QA, RGPT-QA outperforms ORQA by 10.4%, 14.2% and 4.5% on the three datasets, and outperforms REALMNews by 3.3% and 0.2% on NQ and WQ. This demonstrates that the model performance can be enhanced by leveraging relational QA dataset guided by Grounded Relational Wiki-Graph. We provide a detailed analysis in Sec. 4.3.
KnowBERT and KEPER encode structural knowledge into pre-trained language models. Both models focus on generating meaningful entity embedding, and are not designed to infer relations between entities for question answering. From the table, KEPLER trained via TransE performs slightly better than KnowBERT trained via entity linking, and RGPT-QA outperforms KEPLER by 2.8%, 2.1%, 5.7% on the three datasets.
Similar to RGPT-QA, Unsup.QA (Lewis et al., 2019) also generates QA data from Wikipedia. This baseline slightly improves DPR by 0.4%, 0.5%, 1.9% on the three datasets, while our RGPT-QA outperforms it by 1.8%, 1.9%, 4.4%. As discussed in Sec 3.1, one of the main reasons that

our graph-based QA generation strategy performs better is that we adopt grounded description passages desc.(t, s) and desc.(s, t) from different documents as questions and contexts. This avoids the lexical overlap problem in Unsup.QA and help model to capture relational facts.
We also show the retrieval and reader performance separately on validation sets in Table 3. Compared with DPR without pre-training, RGPTQA improves top-20 accuracy of Retriever by 1.7%, 1.8%, and 3.5%, and improves EM accuracy of Reader by 1.5%, 1.1%, and 2.9%. Also, RGPTQA outperforms all the other pre-training baselines. This shows that RGPT-QA improves both the retrieval and reader steps of open-domain QA.

Valid Exact Match Accuracy

45

Supervised

40

KGPT-QA

35

30

25

20

15

10

0.5% 1%Percen2%tage of Tr5a%ining Q10A%Pairs2(0L%og-Scale5)0% 100%

Figure 4: Few-shot QA experiment. Figure shows EM accuracy in validation set of DPR model with and without RGPT-QA pre-training, ﬁne-tuned with different percentage of data on Natural Questions.

Ablation Studies. We then analyze the importance of each model component in RGPT-QA. One key strategy is to mask out the target answer from questions and mask out source entities from passages during retrieval training. This can avoid the model using the entity surface to ﬁnd the correct passage and answer. Without using masking strategy, the average EM performance drops 5.1%. This shows that it is essential to apply the mask strategy to avoid shortcut in QA pre-training. Next, we replace the hard negative passage sampling during retrieval pre-training with random batch sampling. The average EM performance drops 1.4%, showing the importance of hard negative samples. Finally, we study the unsupervised relation loss Ldistill and the supervised Lrel. Removing them leads to 0.5% and 1.3% performance drop, which shows the beneﬁt of training the model to explicitly infer the relation from questions.
Another key component is the negative passage sampling for dense retrieval pre-training. We study how the batch size and number of negative sample inﬂuence the performance of trained retrieval. As is shown in Table 5, increasing batch size and negative sample size can improve the performance of retriever. Even with a small batch size and negative sample, our pre-training framework could still achieves better performance against non-pretrain baseline, showing that our approach is not sensitive to these two hyperparameters.
Few-Shot QA Performance. We analyze the improvement of RGPT-QA when only a few labelled training samples are available. We ﬁne-tune DPR initialized by RGPT-QA on subset of Natural Questions with different percentages. As is shown in

Figure 5: Long-tail relation experiment. EM accuracy of questions in validation set with different relation frequency in training set.
Figure 4, RGPT-QA consistently outperforms DPR without pre-training, and the improvement is more signiﬁcant with small data. Speciﬁcally, when only 0.5% (594) labelled QA pairs are provided, the DPR pre-trained by RGPT-QA can still achieve 26.0% Val EM accuracy, signiﬁcantly higher than 9.4% achieved by the DPR without pre-training. The results show that RGPT-QA provides a good initialization for QA systems and reduce the requirement of large human-annotated QA dataset.
4.3 Generalization for long-tail relations.
As pointed out in Section 2, existing QA datasets suffer high relation bias, and thus a QA model trained on these datasets cannot generalize well to questions with long-tail relations. We thus analyze whether our RGPT-QA can remedy this issue. As is shown in Figure 5, the performance improvement of RGPT-QA against the supervised baseline is much more signiﬁcant for the questions with infrequent relations. Speciﬁcally, for all relations appear less than 5 times in training set, the average EM accuracy of RGPT-QA is 33.3%, signiﬁcantly higher than 22.4% achieved by DPR without pre-

training. This indicates that our relation QA generation method could indeed improve the performance on QA pairs with long-tail relations. Detailed prediction results are shown in Table 11 in Appendix.
5 Related Works
Unsupervised QA via Question Generation To train a QA system without human annotation of QA pairs, Unsupervised QA has been proposed by Lewis et al. (2019) to generate synthetic context, question, answer data for training QA models. Lewis et al. (2019) synthesize the QA data by: 1) run NER or noun chunkers over randomly sampled English Wikipedia paragraphs to extract answers; 2) Treat the paragraphs surrounding the answer as context; 3) Treat the context as clozestyle question and feed into a unsupervised machine translator to generate natural questions. Some follow-up works also utilize template (Fabbri et al., 2020) and pre-trained language model (Puri et al., 2020) over masked cloze-style questions for more human-readable questions. These cloze-style unsupervised QA methods achieve promising performance than previous heuristic QA baselines but underperform supervised ones. The main limitation is that the question is generated with the masked context as input, resulting in severe overlap of lexicon and word surface with the context. Consequently, the QA model might utilize the lexical pattern as a shortcut to ﬁnd the answer. To address the problem of context-question lexical overlap, Dhingra et al. (2018) assume each article has an introductory paragraph, and use this paragraph to generate answer. Li et al. (2020) retrieve the Wikipedia cited document as context, Pan et al. (2020) leverage structured tables to extract key information from context, with which to synthesize questions.
To tackle the challenges in previous studies, our framework propose to leverage the Wikipedia hyperlinks and Wikidata relations as the bridge to connect two entities with linked descriptions. With one description as question and the other as context, the question and context are semantically relevant and lexical different, which naturally solve the problem without involving any additional module.
Knowledge-Guided Pre-Training Recently, researchers investigated to inject structured knowledge into pre-trained language models. Zhang et al. (2019) and Peters et al. (2019) propose to add entity embedding to each entity mentions in text, and

add entity linking objective to guide model capture structured knowledge. Wang et al. (2019) encode entity text description as entity embeddings and train them via TransE objective. Though these work show improvements over several natural language understanding tasks, they are not dedicated to open-domain question answering tasks.
There are also several pre-training studies for QA. For retrieval, Lee et al. (2019) propose an inverse cloze task, which treats a random sentence as query and the surrounding contexts as groundtruth evidence to train a QA retrieval model. Guu et al. (2020b) propose to explicitly add a retriever module in the language model to train the retriever via language modelling pre-training. For reader, Xiong et al. (2020) propose to a weakly supervised pre-training objective. They construct some fake sentences by replacing the entities in a sentence with the other entities of the same type, and train the model to discriminate original sentence from the fake ones. Verga et al. (2020) incorporate the knowledge graph triplets into language model, so the model could utilize the triplets to predict correct entity. Sun et al. (2021) extend this work by learning a virtual knowledge base by inferring the relation between two co-occurring entity pairs.
Compared with these works, our RGPT-QA mainly differs in: 1) We do not change the base QA model, so the pre-training framework could be applied to any QA systems. 2) We explicitly model the relations between entities, which proves to beneﬁt QA pairs with less frequent relation patterns.
6 Conclusion
In this paper, we propose a simple yet effective pre-training framework RGPT-QA. We leverage both the Wikipedia hyperlinks and Wikidata relation triplets to construct Grounded Relational WikiGraph, based on which we generate relational QA dataset. We then pre-train a QA model to infer the latent relation from the question, and then conduct extractive QA to get the target answer entity. RGPT-QA improves the performance of the stateof-the-art QA frameworks, especially for questions with long-tail relations.
Acknowledgement
This work was partially supported by NSF III1705169, NSF 1937599, DARPA HR00112090027, Okawa Foundation Grant, and Amazon Research Awards.

References
Akari Asai, Kazuma Hashimoto, Hannaneh Hajishirzi, Richard Socher, and Caiming Xiong. 2020. Learning to retrieve reasoning paths over wikipedia graph for question answering. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net.
Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. 2017. Reading wikipedia to answer opendomain questions. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, ACL 2017, Vancouver, Canada, July 30 August 4, Volume 1: Long Papers, pages 1870–1879. Association for Computational Linguistics.
Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, and Geoffrey E. Hinton. 2020. Big self-supervised models are strong semi-supervised learners. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual.
Bhuwan Dhingra, Danish Pruthi, and Dheeraj Rajagopal. 2018. Simple and effective semi-supervised question answering. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT, New Orleans, Louisiana, USA, June 1-6, 2018, Volume 2 (Short Papers), pages 582–587. Association for Computational Linguistics.
Bhuwan Dhingra, Manzil Zaheer, Vidhisha Balachandran, Graham Neubig, Ruslan Salakhutdinov, and William W. Cohen. 2020. Differentiable reasoning over a virtual knowledge base. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net.
Alexander R. Fabbri, Patrick Ng, Zhiguo Wang, Ramesh Nallapati, and Bing Xiang. 2020. Templatebased question generation from retrieved sentences for improved unsupervised question answering. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, pages 4508–4513. Association for Computational Linguistics.
Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. 2020a. REALM: retrieval-augmented language model pre-training. CoRR, abs/2002.08909.
Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. 2020b. Retrieval augmented language model pre-training. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pages 3929–3938. PMLR.

Vladimir Karpukhin, Barlas Oguz, Sewon Min, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense passage retrieval for open-domain question answering. CoRR, abs/2004.04906.
Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redﬁeld, Michael Collins, Ankur P. Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natural questions: a benchmark for question answering research. Trans. Assoc. Comput. Linguistics, 7:452–466.
Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. 2019. Latent retrieval for weakly supervised open domain question answering. In Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers, pages 6086–6096. Association for Computational Linguistics.
Patrick S. H. Lewis, Ludovic Denoyer, and Sebastian Riedel. 2019. Unsupervised question answering by cloze translation. In Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers, pages 4896–4910. Association for Computational Linguistics.
Zhongli Li, Wenhui Wang, Li Dong, Furu Wei, and Ke Xu. 2020. Harvesting and reﬁning questionanswer pairs for unsupervised QA. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, pages 6719–6728. Association for Computational Linguistics.
Sewon Min, Danqi Chen, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2019a. A discrete hard EM approach for weakly supervised question answering. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019, pages 2851– 2864. Association for Computational Linguistics.
Sewon Min, Danqi Chen, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2019b. Knowledge guided text retrieval and reading for open domain question answering. CoRR, abs/1911.03868.
Liangming Pan, Wenhu Chen, Wenhan Xiong, MinYen Kan, and William Yang Wang. 2020. Unsupervised multi-hop question answering by question generation. CoRR, abs/2010.12623.
Matthew E. Peters, Mark Neumann, Robert L. Logan IV, Roy Schwartz, Vidur Joshi, Sameer Singh, and Noah A. Smith. 2019. Knowledge enhanced contextual word representations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International

Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019, pages 43–54. Association for Computational Linguistics.
Raul Puri, Ryan Spring, Mohammad Shoeybi, Mostofa Patwary, and Bryan Catanzaro. 2020. Training question answering models from synthetic data. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020, pages 5811–5826. Association for Computational Linguistics.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a uniﬁed text-to-text transformer. J. Mach. Learn. Res., 21:140:1–140:67.
Anshumali Shrivastava and Ping Li. 2014. Asymmetric LSH (ALSH) for sublinear time maximum inner product search (MIPS). In Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems 2014, December 8-13 2014, Montreal, Quebec, Canada, pages 2321–2329.
Haitian Sun, Pat Verga, Bhuwan Dhingra, Ruslan Salakhutdinov, and William W. Cohen. 2021. Reasoning over virtual knowledge bases with open predicate relations. CoRR, abs/2102.07043.
Pat Verga, Haitian Sun, Livio Baldini Soares, and William W. Cohen. 2020. Facts as experts: Adaptable and interpretable neural memory over symbolic knowledge. CoRR, abs/2007.00849.
Denny Vrandecic and Markus Krötzsch. 2014. Wikidata: a free collaborative knowledgebase. Commun. ACM, 57(10):78–85.
Xiaozhi Wang, Tianyu Gao, Zhaocheng Zhu, Zhiyuan Liu, Juanzi Li, and Jian Tang. 2019. KEPLER: A uniﬁed model for knowledge embedding and pre-trained language representation. CoRR, abs/1911.06136.
Qizhe Xie, Minh-Thang Luong, Eduard H. Hovy, and Quoc V. Le. 2020. Self-training with noisy student improves imagenet classiﬁcation. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020, pages 10684–10695. IEEE.
Wenhan Xiong, Jingfei Du, William Yang Wang, and Veselin Stoyanov. 2020. Pretrained encyclopedia: Weakly supervised knowledge-pretrained language model. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net.
Zhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang, Maosong Sun, and Qun Liu. 2019. ERNIE: enhanced language representation with informative entities. In Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL

2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers, pages 1441–1451. Association for Computational Linguistics.

Relation
P161 (cast member) P175 (performer) P676 (lyrics by) P86 (composer) P725 (voice actor) P1346 (winner) P50 (author) P17 (country) P527 (has part) P162 (producer) P276 (location) P840 (narrative location) P915 (ﬁlming location) P710 (participant) P170 (creator) P1308 (ofﬁceholder) P361 (part of) P39r (R: position held) P138 (named after) P112 (founded by) P161r (R: cast member) P31r (R: instance of) P58 (screenwriter) P61 (discoverer or inventor) P26 (spouse) P1923 (participating team) P166r (R: award received) P674 (characters) P279 (subclass of) P361r (R: part of) P131 (is located in) P279r (R: subclass of) P54 (member of sports team) P1344r (R: participant in) P495 (country of origin) P39 (position held) P127 (owned by) P607r (R: conﬂict) P31 (instance of) P1441r (R: present in work) P175r (R: performer) P36 (capital) P921 (main subject) P186 (material used) P179r (R: part of the series) P793r (R: signiﬁcant event) P115 (home venue) P371 (presenter) P180 (depicts) P800r (R: notable work) P136 (genre) P1431 (executive producer) P47 (shares border with) P54r (R: member of sports team) P144 (based on) P57 (director) P488 (chairperson) P403 (watercourse outﬂow) P1889 (different from) P1441 (present in work) P734 (family name) P1269 (facet of) P706 (takes place in) P176 (manufacturer) P84 (architect) P150 (contains) P1532 (country for sport) P800 (notable work) P641 (sport) P1001 (applies to jurisdiction) P206 (on lake) P178 (developer) P166 (award received) P102r (R: party) P449 (original broadcaster) P2438 (narrator) P264 (record label) P674r (R: characters) P1891 (signatory) P138r (R: named after) P69 (educated at) P1877 (after a work by)

Frequency
1915 1844 519 442 334 283 263 257 198 134 117 103
98 88 87 87 74 64 64 61 60 57 57 55 53 52 51 50 49 49 46 45 41 38 37 34 33 32 32 29 28 28 24 22 22 21 21 21 19 19 17 16 16 16 16 15 15 15 14 14 14 14 13 13 12 12 12 12 11 11 11 11 11 11 10 10 10 10 10 10 10 10

Question
what was the geeks name in 16 candles who sang the original blinded by the light who sings the song i can see clearly now the rain is gone who made the beavis and butthead theme song who plays the voice of tiana in princess and the frog who has won the 2017 womens singles wimbledon tennis tournament where does the saying standing on the shoulders of giants come from where did the black panther party take place the unit of area in mks system is who is in the video do n ’t worry be happy where will the summer olympics be held in 2020 what state is a christmas story based in where was the movie the english patient ﬁlmed who died at the gunﬁght at okay corral who came up with britain ’s got talent who is the ﬁrst lady of the usa who sings if you want to destroy my sweater who is the attorney general for new jersey who proved that mar ’s orbit is elliptical not circular who created a settlement house with the help of other social reformers who is miss sue in the blind side the world ’s oldest epic tale told in poetry is called the epic of who wrote the story for the shape of water who developed the analytical engine which had features of present day computers who does young catherine marry in wuthering heights who did the bengals play in the super bowl which indian actor has won the most national awards who said better to reign in hell than serve in heaven when does dna replication occur during the eukaryotic cell cycle where does the transmission of electrical impulses in the heart begin where is saba university school of medicine located what are the names of the three pedals on a piano what team does steph curry brother play for who won rupauls drag race all stars three where was the movie snow white and the huntsman ﬁlmed who is the present speaker of lok sabha 2018 who owns the independent newspaper in the uk in the civil war who had more soldiers what kind of bridge is the mackinac bridge what is the dads name in the adams family who does sean astin play in lord of the rings what is the capital of dadra and nagar haveli what disease did susannah have in brain on ﬁre what is the liquid in a magic 8 ball what is the second book in the mortal instruments series which territories did the us gain in the spanish-american war where does portland ’s nba basketball team the portland trailblazers play who won beat bobby ﬂay shrimp and grits who r the 4 presidents on mt . rushmore the explorer accurately mapped the coasts of europe and north africa scott joplin is best known as a composer of what kind of music who hosted the daily show before trevor noah which indian states share a border with delhi who scored the ﬁrst goal in dallas stars history the tribute money depicts a scene from the who is the director of welcome to new york who is the leader of the democratic party now what sea does the nile river ﬂow into how to do alt codes on a mac when does luke skywalker ﬁnd out leia is his sister who threw the ﬁrst brick in the stonewall riots which supreme court case established the separate but equal doctrine what region of the world is greece in who built the gerald r ford aircraft carrier scottish architect who developed st martins in the ﬁeld what is the name of capital of argentina cristiano ronaldo what country does he play for what was the ﬁrst book that charles dickens published what is the number 1 sport in the usa who won the schenck v. united states case where is ellis island located in new york ms ofﬁce 2000 was developed by which company who won best actor in the academy awards this year who was known as the father of indian national congress what cbs channel is the late late show on whos the main character in the great gatsby who did the soundtrack for beverly hills cop where is the story of joseph in the bible found who has started reducing emissions from deforestation and forest degradation roman god of underworld also called orcus and pluto where did jaren jackson senior play college basketball the movie catch me if you can is based on who

True Answer
anthony michael hall bruce springsteen johnny nash mike judge anika noni rose garbiñe muguruza bernard of chartres united states metre bobby mcferrin tokyo indiana tunisia billy clanton simon cowell melania trump weezer gurbir grewal nicolaus copernicus ellen gates starr kathy bates epic of gilgamesh vanessa taylor charles babbage hareton earnshaw san francisco 49ers amitabh bachchan satan mitosis sinoatrial node saba soft pedal dallas mavericks trixie mattel united kingdom sumitra mahajan alexander lebedev union army suspension bridge gomez addams samwise gamgee silvassa anti-nmda receptor encephalitis alcohol city of ashes puerto rico moda center bobby ﬂay abraham lincoln piri reis ragtime jon stewart uttar pradesh neal broten gospel of matthew chakri toleti tom perez mediterranean sea option key return of the jedi johnson plessy v. ferguson southern europe newport news shipbuilding james gibbs buenos aires portugal the pickwick papers american football united states upper new york bay microsoft gary oldman mahatma gandhi cbs nick carraway mca records book of genesis brazil pluto georgetown university frank abagnale

Table 6: Relation with grounded QA pairs of Natural Questions Training Set (Top 1-82 by frequency).

Relation
P155 (follows) P1029 (crew member) P3342 (signiﬁcant person) P749 (parent organization) P735 (given name) P463r (R: member of) P1376 (capital of) P156 (followed by) P451 (unmarried partner) P40 (child) P159 (headquarters location) P287 (designed by) P551 (residence) P647 (drafted by) P30 (continent) P634 (captain) P828 (has cause) P123 (publisher) P2408 (set in period) P27r (R: country of citizenship) P135r (R: movement) P101r (R: ﬁeld of work) P941 (inspired by) P136r (R: genre) P466r (R: occupant) P119r (R: place of burial) P88 (commissioned by) P110 (illustrator) P1366 (replaced by) P169 (chief executive ofﬁcer) P3279 (statistical leader) P2388 (leader’s ofﬁce) P53r (R: family) P2522r (R: victory) P823 (speaker) P748 (appointed by) P1363 (points/goal scored by) P22 (father) P1027 (conferred by) P750 (distributed by) P825 (dedicated to) P974 (tributary) P8031 (perpetrator) P885 (river source) P631 (structural engineer) P17r (R: country) P98 (editor) P737 (inﬂuenced by) P206r (R: on lake) P2789 (connects with) P740 (location of formation) P4743 (animal breed) P466 (occupant) P2868r (R: subject has role) P5053 (fastest lap) P106r (R: occupation) P50r (R: author) P118 (league) P2416r (R: sport discipline) P1552 (has quality) P8111 (unit) P179 (part of the series) P131r (R: is located in) P7047 (enemy of) P725r (R: voice actor) P61r (R: discoverer or inventor) P6 (head of government) P264r (R: record label) P462 (color) P533 (target) P972 (catalog) P1344 (participant in) P106 (occupation) P1366r (R: replaced by) P171r (R: parent taxon) P1411 (nominated for) P8345 (media franchise) P1433 (published in) P20r (R: place of death) P87 (librettist) P3764 (pole position) P559 (terminus)

Frequency
10 10 10 9 9 9 8 8 8 8 8 8 8 8 8 7 7 7 7 7 7 7 6 6 6 6 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3

Question
what is the latest george rr martin book who was the ﬁrst to step on moon who was picked over kevin durant in the draft what does chi mean in chi st lukes who won in the war of alexander and porus countries in the warsaw pact during the cold war cape town is the capital of what country the things we do for love song artist who does elena date in the vampire diaries howard stark is the father of what superhero where is the head ofﬁce of rbi located who built the world ﬁrst binary digit computer z1 where did dorothy live in the wizard of oz who does dwyane wade play for in the nba on what continents was the roman empire located at the height of its expansion who is the captain of kolkata knight riders what is the most common manifestation of portal hypertension – induced splenomegaly who made all the call of duty games when did hunchback of notre dame take place who was the last ruler of the tang dynasty who wanted the catholic church to reform and address who invented the steam engine in the 1800s who does squealer in animal farm represent in the russian revolution who are the founding fathers of hip hop where did the patriots play before gillette stadium who is buried in the great mausoleum at forest lawn glendale who built the castle in just one day scary stories to tell in the dark artist the old greek city-state of byzantium was rebuilt and became known as who become the ceo indian it company wipro in 2016 who is the captain of argentina national team ﬁfa world cup 2018 who does the us department of justice report to who began the ﬁrst dynasty of egyptian rulers who won season 2 of food network star who wrote we shall ﬁght on the beaches who can appoint comptroller and auditor general of india who scored the winning goal for england in the 1966 world cup ﬁnal who was the king after david in the bible who presents national ﬁlm award traditionally in india who own the rights to the black panther movie who was the song candle in the wind written about a tributary ﬂowing into the mississippi from the east is the who was the guy who shoot in las vegas what is the starting point of the mississippi river who designed the ﬁrst tunnel under the river thames what are the countries of the united arab emirates who was an abolitionist who published and autobiography and anti-slavery newspaper qbasic is the extension of which programming language where does the river mekong start and end a ship traveling through the panama canal could be crossing from the where did the beatles started their career as a band what kind of dog is bo and sunny who used to play in the alamo dome who is the commander in chief of military who won the 2018 chinese formula 1 grand prix who is the griot that sings the epic what is the title of langston hughes ’s ﬁrst book of poetry what conference is ohio state in for football who has the world record for the long jump which metal does the word ’ ferrous ’ refer to answer in words not symbols unit of measure for area of a triangle which games are in crash bandicoot n sane trilogy what is the name of capital of andhra pradesh who sent doomsday to the end of time who does the voice of the cat in the hat who discover the simple microscope ﬁrst time and when who was the founder of the mauryan empire this artist was signed in 1952 by atlantic and brought a string of hits what color was the white house when it was built who was killed in the ides of march who is on the top ten most wanted india ’s ﬁrst olympic medal win as a free nation what did pete best play in the beatles what is the old name for south africa what type of organism is made up of prokaryotic cells who won best director at the academy awards what is the ﬁrst star wars movie in the series the story of seven ages by william shakespeare who was the explorer that reached the cape of good hope at the southern tip of africa who wrote the libretto for dido and aeneas who won the abu dhabi grand prix 2017 what is the southern end of the appalachian trail

Answer
a dance with dragons neil armstrong greg oden catholic health initiatives alexander soviet union south africa 10cc stefan salvatore iron man mumbai konrad zuse kansas miami heat asia dinesh karthik cirrhosis activision 1482 emperor ai of tang martin luther james watt vyacheslav molotov grandmaster ﬂash foxboro stadium michael jackson toyotomi hideyoshi stephen gammell constantinople abidali neemuchwala lionel messi united states attorney general narmer guy ﬁeri winston churchill president of india geoff hurst solomon directorate of ﬁlm festivals walt disney studios motion pictures marilyn monroe ohio river stephen paddock lake itasca marc isambard brunel sharjah frederick douglass quickbasic mekong delta atlantic ocean liverpool portuguese water dog utsa roadrunners president of the united states daniel ricciardo balla fasséké the weary blues big ten conference galina chistyakova iron square metre crash bandicoot amaravati superman martin short zacharias janssen chandragupta maurya ray charles white julius caesar alexis ﬂores 1948 summer olympics drummer union of south africa archaea guillermo del toro star wars as you like it bartolomeu dias nahum tate valtteri bottas springer mountain

Table 7: Relation with grounded QA pairs of Natural Questions Training Set (Top 83-164 by frequency).

Relation
P366 (use) P706r (R: takes place in) P610 (highest point) P461 (opposite of) P467 (legislated by) P272 (production company) P140r (R: religion) P1419 (shape) P942r (R: theme music) P376 (planet) P5202 (adapted by) P171 (parent taxon) P509 (cause of death) P527r (R: has part) P2849 (produced by) P460 (said to be the same as) P1346r (R: winner) P2341 (indigenous to) P355 (subsidiary) P457 (foundational text) P108r (R: employer) P1923r (R: participating team) P8345r (R: media franchise) P112r (R: founded by) P113r (R: airline hub) P156r (R: followed by) P137 (operator) P1552r (R: has quality) P2175 (disease treated) P25 (mother) P170r (R: creator) P641r (R: sport) P451r (R: unmarried partner) P4584 (ﬁrst appearance) P2670 (has parts of the class) P1040 (ﬁlm editor) P1056r (R: material produced) P1192r (R: connecting service) P1830 (owner of) P241r (R: military branch) P111 (measure of) P19r (R: place of birth) P291 (place of publication) P1056 (material produced) P140 (religion) P137r (R: operator) P162r (R: producer) P1582 (fruit of (taxon)) P286 (head coach) P118r (R: league) P413 (ﬁelding position) P35 (head of state) P3173 (offers view on) P7959 (historic county) P598r (R: commands) P306 (operating system) P101 (ﬁeld of work) P27 (country of citizenship) P463 (member of) P4969 (derivative work) P19 (place of birth) P3938 (named by) P157r (R: killed by) P607 (conﬂict) P366r (R: use) P551r (R: residence) P113 (airline hub) P927r (R: anatomical location) P1000 (record held) P2541 (operating area) P4647 (place of ﬁrst performan) P483 (studio) P197r (R: adjacent station) P36r (R: capital) P1589r (R: lowest point) P669 (located on street) P1478 (has immediate cause) P1269r (R: facet of) P2679 (author of foreword) P669r (R: located on street) P837r (R: day in year) P3113 (does not have part)

Frequency
3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1

Question
what did the chinese use oracle bones for what seven countries make up the subcontinent of south asia what is the highest point in the pyrenees mountains in france the results of dehydration reactions can be reversed by which group is responsible for adopting the declaration of independence what is the tv show riverdale based off of who is the leader of the baptist denomination what is the shape of the earth ’s orbit around the sun clubs who sing you ’ll never walk alone this planet is home to the great red spot who wrote the lyrics for the song my way trees of the betel nut genus of palms what was the cause of the tollund man ’s death the bronchi are considered to be part of the where does red blood cell formation occur in adults the word zion is an ancient biblical term that referred to what city when did the philadelphia eagles last win the super bowl dogri language is spoken in which state of india the main agency under the department of homeland security that is responsible for border security is where does one look to ﬁnd the powers of a corporation who is the current ceo of mcdonald ’s corporation last time houston astros have been to the world series what star wars movie came out before the last jedi real name of raj chandra in rani rashmoni what airline has its hub in charlotte nc what is the origin of the coptic language who owns the white house in washington dc what physical quantity is a measure of the amount of inertia and object has topiramate ( topamax trokendi ) is used to treat which of the following diseases who is carries mother on days of our lives when was beverly cleary ’s ﬁrst book published where do the rocks from curling come from who does raven end up with in the comics what was the ﬁrst game waluigi was in what do you rest a golf ball on who is the director of the ﬁlm avatar who introduced the ﬁrst micro processor in 1971 where does the eurostar leave from in paris where do the carolina panthers play home games who served as the general of confederate forces during the civil war joule is unit of . in mks system who was the last person to live in versaille where was the institutes of the christian religion published by product of saponiﬁcation of fats and oils of which religion is the avesta a sacred book where do the ﬁsher cats play in nh producer and director of silence of the lambs a plant that produces a type of bean 2 ) who is the current manager of liverpool fc which nrl teams have never won a premiership what position did ryan tannehill play in college the longest serving samma ruler in sindh was where is the leaning tower of pisa in italy located archipelago that includes neolithic settlement of skara brae union generals civil war army of the potomac what operating system does the macbook pro have what did robert moog contribute to the music industry in the 1960s where is the actress that played wonder woman from what band is the girl from the grinch in what is the ﬁrst book of pretty little liars where did anakin live before he met qui-gon who developed the concept of an iron law of wages who does sansa end up with in game of thrones what battle did the tuskegee airmen help win what kind of wax are crayons made from who lived in the land of nod east of eden where does porter airlines ﬂy from in toronto where do the ilium the ischium and the pubis meet who holds the world record for 100 meters what states does the i pass work in where does medea go at the end of the play where was the dark side of the moon recorded where does the rocky mountaineer leave from in vancouver what country is in between poland and lithuania which state is bordered to the north by the artic ocean what area of paris is the eiffel tower the united states ’ war on terror began in the wake of which of the following events the enlightenment idea of separation of powers included which branches of government who wrote the current edition of the catechism where did the beatles take the abbey road picture what are three other names for makar sankranti which element in group 1 is not an alkaline metal

Answer
pyromancy sri lanka aneto hydration reaction second continental congress archie comics thomas helwys ellipse liverpool f.c. jupiter paul anka areca hanging respiratory system bone marrow jerusalem super bowl lii himachal pradesh u.s. customs and border protection articles of incorporation steve easterbrook 2017 world series the empire strikes back babughat american airlines egyptian language national park service mass epilepsy anna dimera henry huggins ailsa craig beast boy mario tennis tee james cameron intel gare du nord bank of america stadium robert e. lee energy louis xvi basel soap zoroastrianism northeast delta dental stadium edward saxon fabaceae jürgen klopp new zealand warriors quarterback jam nizamuddin ii pisa orkney ambrose burnside macos electronic music israel the pretty reckless pretty little liars tatooine ferdinand lassalle ramsay bolton world war ii parafﬁn wax cain billy bishop toronto city airport acetabulum usain bolt illinois athens abbey road studios paciﬁc central station kaliningrad oblast alaska champ de mars september 11 attacks legislature pope john paul ii abbey road studios magh bihu hydrogen

Table 8: Relation with grounded QA pairs of Natural Questions Training Set (Top 165-246 by frequency).

Relation
P7047r (R: enemy of) P59r (R: constellation) P3092 (ﬁlm crew member) P2348r (R: time period) P736 (cover art by) P469 (lakes on river) P205 (basin country) P921r (R: main subject) P4934 (calculated from) P1411r (R: nominated for) P4147 (conjugate acid) P276r (R: location) P413r (R: ﬁelding position) P710r (R: participant) P2563r (R: superpower) P2596 (culture) P1071r (R: location of creation) P1535r (R: used by) P400 (platform) P4913 (dialect of) P1066 (student of) P3342r (R: signiﬁcant person) P86r (R: composer) P1427 (start point) P3373 (sibling) P2512r (R: series spin-off) P2505r (R: carries) P5009 (complies with) P2094 (competition class) P1889r (R: different from) P7937 (form of creative work) P522 (type of orbit) P1303 (instrument) P737r (R: inﬂuenced by) P263 (ofﬁcial residence) P201 (lake outﬂow) P178r (R: developer) P1312 (has facet polytope) P20 (place of death) P2936 (language used) P460r (R: said to be the same as) P682r (R: biological process) P3300 (musical conductor) P547 (commemorates) P2079 (fabrication method) P1037 (director / manager) P972r (R: catalog) P263r (R: ofﬁcial residence) P2152 (antiparticle) P1462 (standards body) P664r (R: organizer) P937 (work location) P4675r (R: appears in the form of) P2596r (R: culture) P2554 (production designer) P1038 (relative) P3301 (broadcast by) P943 (programmer) P30r (R: continent) P135 (movement) P5051 (towards) P676r (R: lyrics by) P364 (original language) P1071 (location of creation) P400r (R: platform) P452 (industry) P598 (commands) P1303r (R: instrument) P3491 (muscle insertion) P530 (diplomatic relation) P1542 (has effect) P1336 (territory claimed by) P747 (editions) P7153 (signiﬁcant place) P610r (R: highest point) P1809 (choreographer) P81 (connecting line) P122 (type of government) P97r (R: noble title) P4552 (mountain range) P658 (tracklist) P195 (collection) P609 (terminus location)

Frequency
1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1

Question
who took out the governor ’s eye on walking dead brightest star in the constellation lyra dan word who pioneered animated movies with his short feature steamboat willie in 1928 the main port of axum was the red sea city of who wrote all quite on the western front where does the water from the nile come from in what country would you ﬁnd the yellow river who began the systematic study of political science a quantity 15 m / s to the north is a measure of who won the ﬁrst oscar for best actress give the name and formula for the acid derived from the following anion chlorite the area between the tigris and euphrates rivers who has the most clean sheets in the world in the second punic war between carthage and rome carthage formed an alliance with who taught defence against the dark arts in book number 5 which american civilization was located in a rain forest where does the young ones develop in humans what programming language is used in microsoft access what consoles can you play star wars battlefront on what type of arabic is spoken in palestine who is the minister during the regime of chandragupta who went before michael jordan in the draft who wrote the power of love celine dion where did the tour de france start in 1954 who is the older brother mario or luigi which came ﬁrst family guy or american dad where does the appalachian trail cross the hudson river what type of port is used by ﬂash drives what weight class did muhammad ali ﬁght in what name is given to fats that are liquid at room temperature wagner ’s tristan und isolde is an example of what ’s the orbit of the international space station what kind of bass does john cooper play who are the members of 3 6 maﬁa where did zeus spend most of his time where does the water from lake okeechobee drain operating system developed in 1969 at at&t ’s bell laboratories what is the opposite side of a right angle triangle where did omri build his new political capital what is the national language of saudi arabia what color is a school bus yellow or orange which protein is responsible for the breakdown of a ﬁbrin clot who did the music for ready player one name of ship that landed at plymouth rock the medium of the artwork that decorates the sistine chapel ceiling is who led the red shirts to victory in sicily who is number one on america ’s most wanted which greek god ruled over a gloomy kingdom a packet or unit of light energy is called a who is responsible for creating the standards used on the internet when did they start using gloves in ufc where did beethoven live most of his life what was robin ’s name in batman and robin a ruined city on crete centre of the minoan bronze age civilisation who made the movie all dogs go to heaven what is the ﬁrst name of huey ’s dewey ’s and louie ’s uncle who is broadcasting the super bowl on sunday who wrote the ﬁrst computer virus called elk cloner is puerto rico in north or central america what kind of art did claude monet paint which part of the cerebral hemisphere is supplied by the middle cerebral artery what beatles songs does paul play drums on what language do they speak in kite runner a town in the netherlands known for the production of a tin glazed earthenware name of the windows phone 8.1 virtual assistant what did the hudson bay company do for canada who controlled or ordered the viet cong in combat who introduced the bass clarinet as a solo instrument in jazz what is the origin and insertion of the semimembranosus which two countries are on the western border of bolivia what disease is caused by bacterium treponema pallidum the falkland islands are off the coast of what south american country what is the latest ms ofﬁce for mac on which island is the uss arizona memorial the highest peak in north america mt . mckinley ( or denali ) is located in the state of who danced the lead role in appalachian spring what line is parsons green on tube map what type of government did european settlers create in south africa in 1909 who was crowned the ﬁrst holy roman emperor what mountain range is the blue mountains part of what was u2 ’s lead single from ’ the joshua tree ’ where is the original star spangled banner located where does route 66 start on the east coast

Answer
michonne vega walt disney adulis erich maria remarque lake victoria china american political science review velocity janet gaynor chlorous acid mesopotamia iker casillas massylii dolores umbridge maya civilization uterus visual basic for applications xbox one south levantine arabic chanakya hakeem olajuwon candy derouge amsterdam mario family guy bear mountain bridge usb mass storage device class heavyweight oil opera low earth orbit bass guitar juicy j mount olympus caloosahatchee river unix hypotenuse samaria arabic chrome yellow plasmin alan silvestri mayﬂower fresco giuseppe garibaldi jason derek brown hades photon internet engineering task force ufc 14 vienna dick grayson knossos don bluth donald duck nbc rich skrenta puerto rico impressionism cerebrum dear prudence dari delft cortana retail hoàng va˘n thái herbie mann medial condyle of tibia chile syphilis argentina microsoft ofﬁce 2016 honolulu alaska martha graham district line constitutional monarchy charlemagne great dividing range with or without you national museum of american history chicago

Table 9: Relation with grounded QA pairs of Natural Questions Training Set (Top 247-329 by frequency).

Triplet Question True Passage Pred. Relation Pred. Answer
Triplet Question True Passage Pred. Relation Pred. Answer
Triplet Question True Passage Pred. Relation Pred. Answer
Triplet Question True Passage Pred. Relation Pred. Answer
Triplet Question True Passage Pred. Relation Pred. Answer
Triplet Question True Passage Pred. Relation Pred. Answer
Triplet Question True Passage Pred. Relation Pred. Answer
Triplet Question True Passage Pred. Relation Pred. Answer
Triplet Question True Passage Pred. Relation Pred. Answer
Triplet Question True Passage Pred. Relation Pred. Answer
Triplet Question True Passage Pred. Relation Pred. Answer
Triplet Question True Passage Pred. Relation Pred. Answer
Triplet Question True Passage Pred. Relation Pred. Answer
Triplet Question True Passage Pred. Relation Pred. Answer

<’edward heath’, ?, "admiral’s cup"> <mask> of edward heath which 1971 the british prime minister, edward heath, captained one of the winning boats. recent history. ...he captained britain’s winning team for the admiral’s cup in 1971 – while prime minister – and also captained the team in the 1979 fastnet race... participant in: 0.33, winner: 0.19, participant: 0.04, victory: 0.03, sport: 0.03 admiral’s cup ()
<’scary stories to tell in the dark’, ’P110 (illustrated by)’, ’stephen gammell’> <mask> of scary stories to tell in the dark which evocative, nightmarish illustrations for alvin schwartz’s "scary stories to tell in the dark" trilogy, he has illustrated nearly seventy scary stories to tell in the dark is a series of three collections of short horror stories for children, written by alvin schwartz and originally illustrated by stephen gammell... illustrator: 0.13, creator: 0.11, author: 0.07, editor: 0.02, notable work: 0.02 stephen gammell ()
<’heeley’, ?, ’shefﬁeld tramway’> <mask> of heeley which ﬁrst routes, to attercliffe and carbrook, brightside, heeley, nether edge and owlerton opened between 1873 ...shefﬁeld’s old tramway stretched from shefﬁeld city centre to woodseats and heeley was at a time the terminus... located in the administrative territorial entity: 0.21, located in the administrative territorial entity: 0.14, location: 0.07, shares border with: 0.04, terminus: 0.03 old tramway ()
<’pablo goncálvez’, ?, ’patricia miller (tennis)’> <mask> of pablo goncálvez which luisa, was the ﬁrst victim of uruguayan serial killer pablo goncálvez, who suffocated the 26-year old to ...the victim was 26 years old, had a degree in history and a practicing teacher, and was the sister of the well-known tennis player patricia miller... sibling: 0.29, relative: 0.13, spouse: 0.04, relative: 0.04, place of burial: 0.02 patricia miller ()
<’chai prakan district’, ’P131r (R: located in the administrative territorial entity)’, ’chai prakan’, > <mask> of chai prakan district which, is home to the district headquarters of<mask><mask><mask><mask> district in the far north of<mask>iang m<mask> province ...chai prakan is divided into four sub-districts ("tambons"), which are further subdivided into 44 administrative villages ("muban")... located in the administrative territorial entity: 0.43, capital: 0.42, contains administrative territorial entity: 0.04, different from: 0.02, contains settlement: 0.01 chai prakarn ()
<’gothic western’, ?, ’lorin morgan-richards’> <mask> of gothic western which lifestyle and his series "the goodbye family" has been categorized as gothic western. in addition to his work, rich ...in the young adult series, "the goodbye family" by lorin morgan-richards has been considered gothic western with an element of humor... genre: 0.92, movement: 0.02, ﬁeld of work: 0.0, genre: 0.0, occupation: 0.0 lorin morgan-richards ()
<’quentin bell’, ’P40 (child)’, ’virginia nicholson’> has kid of quentin bell which her father was the writer and art historian quentin bell, nephew of ...they had three children: julian bell, an artist and muralist; cressida bell, a notable textile designer; and virginia nicholson, the writer of "charleston: a bloomsbury house... child: 0.98, father: 0.0, student: 0.0, sibling: 0.0, relative: 0.0 virginia nicholson ()
<’take me back to london’, ’P361 (part of)’, ’no.6 collaborations project’> <mask> of take me back to london which the border" featuring cabello and cardi b, and "take me back to london" featuring stormzy ...it was released as the eighth single from sheeran’s fourth studio album "no.6 collaborations project" (2019)... part of: 0.88, performer: 0.07, lyrics by: 0.01, producer: 0.0, followed by: 0.0 "no.6 collaborations project ()
<’u.s. route 441 in georgia’, ? , ’lakemont, georgia’> <mask> of u.s. route 441 in georgia which area between u.s. route 23/441 and<mask> rabun.<mask><mask> has a post ofﬁce with zip code ...from there it passes through the blue ridge mountain communities of wiley, lakemont, and tiger, the latter of which includes... terminus location: 0.15, terminus: 0.11, located in the administrative territorial entity: 0.08, terminus: 0.05, connects with: 0.03 wiley ()
<’anjelica huston’, ’P57r (R: directed by)’, ’agnes browne’> <mask> of anjelica huston which irish romantic comedy-drama ﬁlm directed, produced by, and starring anjelica huston, based on the book "the mammy" by brendan o ...her next directorial effort, the irish dramedy "agnes browne" (1999) —in which she also starred as the title character— was released to mixed reviews... terminus location: 0.15, terminus: 0.11, located in the administrative territorial entity: 0.08, terminus: 0.05, connects with: 0.03 "agnes browne ()
<’cadillac eldorado’, ?, ’oldsmobile toronado’> <mask> of cadillac eldorado which 1967, cadillac adopted its own version of the upp for the cadillac eldor<mask>, using the cadillac v8 engine. ...by 2000, the eldorado was the last of a dying breed: its buick riviera and oldsmobile toronado stablemates had been discontinued, as had its perennial rival the lincoln mark... follows: 0.38, followed by: 0.05, brand: 0.04, based on: 0.02, subclass of: 0.02 oldsmobile toronado ()
<’corsican nuthatch’, ’P138 (named after)’, ’john whitehead (explorer)’> eponym of corsican nuthatch which82 where he discovered a bird new to science, the corsican nuthatch. white<mask> travelled in malacca, north borneo, ...the corsican nuthatch was discovered by the english collector john whitehead in june 1883 when he shot a specimen while on a trip in the corsican mountains... named after: 0.97, discoverer or inventor: 0.01, named after: 0.0, different from: 0.0, place served by transport hub: 0.0 john whitehead ()
<’mutual information’, ?, ’information content’> <mask> of mutual information which formula_13 is also often used for the related quantity of mutual<mask>, many authors use a lowercase formula_14 for ...it quantiﬁes the "amount of information" (in units such as shannons (bits), nats or hartleys) obtained about one random variable through observing the other random variable... subclass of: 0.48, different from: 0.08, opposite of: 0.06, subclass of: 0.05, said to be the same as: 0.03 information theory ()
<’oculus (ﬁlm)’, ’P272 (production company)’, ’intrepid pictures’> <mask> of oculus (ﬁlm) which". in may 2012 ﬁlmdistrict acquired the ﬁlm rights to what would become "oculus". soon after, the ﬁlm released on april 11 ...eventually, intrepid pictures expressed interest in producing the ﬁlm "as long as you don’t do it found footage".... production company: 0.29, producer: 0.19, distributed by: 0.07, screenwriter: 0.04, director: 0.03 intrepid pictures ()

Table 10: Examples of generated Relational QA datapoints and the predicted relation and answer by DPR pre-trained via RGPT-QA.

Relation Name
R: based on subject has role practiced by industry made from offers view on R: residence mother R: employer R: has part indigenous to river source tributary R: family R: genre R: genre parent organization narrator educated at director executive producer R: player of R: player of shares border with depicts depicts R: notable work presenter presenter material used main subject instance of instance of country of origin R: subclass of R: subclass of located in R: part of R: part of characters R: award received participating team spouse R: instance of R: instance of R: instance of named after part of part of ofﬁceholder participant participant participant ﬁlming location ﬁlming location

Freq
0 0 0 1 2 2 2 2 3 3 3 4 5 5 6 6 8 10 10 15 16 16 16 16 19 19 19 21 21 22 24 29 29 37 44 44 46 49 49 50 51 52 53 56 56 56 64 70 70 87 88 88 88 98 98

Question
theme song to bridge on the river kwai phenothiazines such as chlorpromazine were the ﬁrst type of who does the call to prayer in islam what product or market does netﬂix deal with mohair is made from the ﬂeece of what animal where is the leaning tower of pisa built who is the founder of ramoji ﬁlm city who bore abraham ﬁrst son in the bible who is the youngest judge currently sitting on the u.s. supreme court corpora cavernosa and corpus spongiosum are anatomic structures of urdu is the ofﬁcial language of which state what is the source of the colorado river river that joins the severn near chepstow crossword who was the second ruler of the davidic monarchy who is considered by many to be the father of soul who brought surf music to a national audience who owns ﬂying j and pilot truck stops who plays the mom in cheaper by the dozen where did the gabbie show go to college who did the movie i can only imagine who stars in the movie the quiet place pitt players in the nﬂ hall of fame who was the captain when india played its ﬁrst-ever odi what state is directly west of north dakota who raised the american ﬂag on iwo jima faces of the presidents on mt. rushmore who won the 2015 great british baking show who presented gardeners world from 2008 to 2010 who are the new hosts of british bake off what kind of meat is on a t-bone new york times co v sullivan held that there must be proof of what kind of money do they use in russia how does a plane wing create lift which physics concept applies who used the springﬁeld riﬂe in the civil war waste water that contain solid and liquid excreta refers to when a blood vessel is injured the ﬁrst phase in hemostasis to occur is what part of new york is coney island what regions of south asia have the highest population densities what led to the downfall of the incan empire the settlement of the israelites in canaan is the theme of which book most number of national awards for best actress who did melbourne beat in the 1964 grand ﬁnal who does jackson end up with in sons of anarchy which of the following is the si unit for length what is the most abundant neurotransmitter in the nervous system pricing tactics lower the price of a product below cost who was saint patrick’s day named after arabian sea is the part of which ocean who produces the most tires in the world what is the name of the governor of new jersey what two groups were ﬁghting in the chinese civil war who played the superbowl halftime show last year who came second in the overall ranked of the tour de france last year what city does the terminator take place in where was back to the future three ﬁlmed

True Answer
the river kwai march antipsychotic muezzin streaming media angora goat pisa ramoji rao hagar neil gorsuch penis pakistan la poudre pass river wye solomon james brown the beach boys pilot corporation bonnie hunt university of pittsburgh erwin brothers john krasinski mike ditka ajit wadekar montana michael strank abraham lincoln nadiya hussain toby buckland noel ﬁelding beef actual malice kopeks force united states sewage coagulation brooklyn philippines battle of cajamarca joshua shabana azmi collingwood football club tara knowles metre serotonin loss leader saint patrick indian ocean lego phil murphy communist party of china bruno mars rigoberto urán los angeles monument valley

RGPT-QA Prediction
the river kwai march psychiatry muezzin streaming media angora goat pisa ramoji rao sarah neil gorsuch penis jharkhand la poudre pass river lugg jeroboam james brown the beach boys pilot corporation bonnie hunt university of pittsburgh the erwin brothers john krasinski ruben brown srinivasaraghavan montana ira hayes thomas jefferson joanne wheatley joe swift noel ﬁelding cut from the short loin malice ruble or rouble newton’s second law marine corps sewage wound healing brooklyn philippines captured the book of joshua ﬁve collingwood tara knowles meter serotonin loss leader saint patrick northern indian ocean lego tire: lego tire a lego phil murphy communist party of china beyoncé rigoberto urán los angeles monument valley

Supervised DPR Prediction
march medication mosque netﬂix goat pisa the leaning tower of pisa telugu ﬁlm producer ramoji rao yishma’el leonard i. garth corpus cavernosum jammu and kashmir colorado begins at la poudre pass lugg solomon’s son, rehoboam sam cooke dean berkshire hathaway kate the university of pittsburgh bart millard emily blunt tony dorsett s manitoba rene gagnon, ira hayes theodore roosevelt edd kimber carol klein and joe swift sandi toksvig tenderloin truth the russian ruble or rouble reaction force army pathogens endothelial injury borough of brooklyn indonesia victory book of joshua three melbourne football club opie winston litre glutamate increase in proﬁts saint patrick the northern indian ocean lego blocks. lego democrat phil murphy republic of china coldplay chris froome hemdale jamestown, california

Table 11: Comparison of the prediction of DPR initialized by RGPT-QA with DPR without pre-training. These are all samples that two models made different predictions, and the relation frequency in the training set is less than 100.

Question Who played mr darling on andy grifﬁth show Who voices ﬂik in a bug’s life Who’s the dad of blair waldorf’s baby
Where do you think glaciers can be found today
When did ginny weasley join the quidditch team When does far cry 5 for ps4 come out
When do millennials end and gen z start
Who killed hotchner’s wife in criminal minds
Who said walk tall and carry a big stick
Who does the voice of sheen from jimmy neutron How many seasons of gossip girl are there What can be used to detect the charge of particles Who was robin in the original batman series What is the song funky cold medina about What do you call a quarter pounder in france Who developed the ﬁrst alternating current electric system Who won s5 of rupaul’s drag race When was the svalbard global seed vault built
Who was the mother of dragons married to
Which organization sets monetary policy for the united states What season of the voice was miley cyrus on Upon which document in american history is the language of the declaration of sentiments based What kind of car does dale earnhardt jr drive How many times did brazil win the ﬁfa world cup
Second life is an example of a
What percentage of the world’s population lives in east asia From which body part shurpnakha drive her name How many chapters does the gospel of john have Who sang the original always on my mind Where does the amazon river start and ﬁnish Who did dwayne wade play for last year Who owns the rights to the power rangers

Predicted Answer Denver Pyle Dave Foley Chuck
rocky mountains
half-blood prince 2018
mid-1990s to mid-2000s
George Foyet u.s. president theodore roosevelt Jeffrey Garcia
6 ionization detectors
Burt Ward a love potion royal cheese
Galileo Ferraris
Jinkx Monsoon 2006
Khal Drogo
the federal reserve
eleventh united states declaration
of independence chevrolet camaro
ﬁve massively multiplayer online role-playing games
22%
ﬁngernails four
b.j. thomas atlantic ocean the miami heat
Hasbro

True Answer Denver Pyle Dave Foley
Chuck mountain ranges on every continent half-blood prince march 27, 2018
mid-1990s
George Foyet
theodore roosevelt
Jeffrey Garcia 6
particle detector Burt Ward
a ﬁctional aphrodisiac royal cheese
Nikola Tesla
Jinkx Monsoon 2006
Dothraki Horselord Khal Drogo
the federal reserve
season 11 united states declaration
of independence chevrolet ﬁve
an online virtual world
22%
ﬁngernails 21
gwen mccrae atlantic ocean
miami heat Hasbro

Match?                   
     
      

Predicted Relation P175 (performer) P725 (voice actor)
P26 (spouse) P31r (R: instance of) P674r (R: characters) P400 (game platform) P155 (preceded by)
P7047 (enemy of) P170 (creator)
P734 (family name) P527 (has part)
P279r (R: subclass of) P161 (cast member) P138 (named after) P1889 (different from)
P61 (inventor) P1346 (winner) P88 (built for)
P26 (spouse)
P1001 (jurisdiction) P179 (part of series)
P144 (based on) P54 (played for) P1344 (participant in) P31 (instance of)
P276r (R: located in) P186 (ingredient) P527 (has part) P175 (performer)
P403 (watercourse outﬂow) P647 (drafted by) P127 (owned by)

Table 12: Predicted relations for those QA pairs in Natural Questions Valid Set that cannot be aligned to WikiData.

