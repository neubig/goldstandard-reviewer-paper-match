Approximate Regions of Attraction in Learning with Decision-Dependent Distributions
Roy Dong1, Lillian J. Ratliff2
1 University of Illinois at Urbana-Champaign 2 University of Washington, Seattle
roydong@illinois.edu, ratlifﬂ@uw.edu

arXiv:2107.00055v2 [cs.LG] 1 Oct 2021

Abstract
As data-driven methods are deployed in real-world settings, the processes that generate the observed data will often react to the decisions of the learner. For example, a data source may have some incentive for the algorithm to provide a particular label (e.g. approve a bank loan), and manipulate their features accordingly. Work in strategic classiﬁcation and decisiondependent distributions seeks to characterize the closed-loop behavior of deploying learning algorithms by explicitly considering the effect of the classiﬁer on the underlying data distribution. More recently, works in performative prediction seek to classify the closed-loop behavior by considering general properties of the mapping from classiﬁer to data distribution, rather than an explicit form. Building on this notion, we analyze repeated risk minimization as the perturbed trajectories of the gradient ﬂows of performative risk minimization. We consider the case where there may be multiple local minimizers of performative risk, motivated by situations where the initial conditions may have signiﬁcant impact on the long-term behavior of the system. We provide sufﬁcient conditions to characterize the region of attraction for the various equilibria in this settings. Additionally, we introduce the notion of performative alignment, which provides a geometric condition on the convergence of repeated risk minimization to performative risk minimizers.
1 Introduction
Data-driven methods are growing increasingly popular in practice. Most classical machine learning and statistical methods view the underlying process which generates the data as ﬁxed: the study is primarily focused on the mapping from data distributions to classiﬁer. However, it is important to consider the effects in the other direction as well: how does the classiﬁer chosen by a learner change the data distribution the learner sees? In particular, how do we close the loop around machine learning deployments in practice?
These closed loop effects can arise in many real world settings. One instance is strategic classiﬁcation: whenever a data source has a stake in which label a classiﬁer applies to it, they will seek cost-effective ways to manipulate their data to earn the desired label. For example, credit scoring classiﬁers are heavily guarded for fear of the potential for gaming (Hardt et al. 2016). Alternatively, deployments of the classiﬁer can both skew future datasets and also have causal inﬂuences over the real-world processes at play. For

example, a classiﬁer that predicts crime recidivism inﬂuences the opportunities available to individuals (Dressel and Farid 2018).
Formally, we consider this problem in the framework introduced in Perdomo et al. (2020). Let (z, x) denote the loss when the learner’s decision is x (e.g. x can be the parameters of the chosen classiﬁer) and the data has realized value z. Furthermore, let D(x) denote the data distribution when the learner’s decision is x. In this framework, the performative risk is given by:

P R(x) = EZ∼D(x)[ (Z, x)]

(1)

Whereas classical machine learning results treat the distribution Z ∼ D as ﬁxed, the performative prediction framework models the decision-dependent distribution as a mapping D(·). However, in many real world-deployments, this decision-dependent distribution shift may not be explicitly included in the learner’s updates. This leads to algorithms based on inexact repeated minimization. Deﬁne the decoupled performative risk as:

R(x1, x2) = EZ∼D(x2)[ (Z, x1)]

(2)

The decoupled performative risk R(x1, x2) separates the two ways that the decision variable x affects the performative risk. Through the x1 argument, x affects the classiﬁcation error; through the x2 argument, x causes a decision-dependent distribution shift. Thus, when the decision-dependent distri-
bution shift is not accounted for, the repeated gradient descent
method yields the following update rule:

xk+1 = xk − αk(∇x1 R(xk, xk) + ηk)

(3)

Here, (ηk)k is some zero-mean noise process. Note that the gradient is evaluated only with respect to the ﬁrst argument, i.e. the updates are based only on the effect of x on the loss function, and ignore the distribution shift caused by x. In other words, the learner draws several observations from the distribution D(xk), and, treating this distribution as ﬁxed, updates their model parameters xk+1 based on stochastic gradient descent: they are descending the gradient of the cost function y → R(y, xk).
In this paper, we shall analyze the steady-state behavior of the continuous-time ﬂows corresponding to Equation (3):

x˙ = −∇x1 R(x, x)

(4)

The connections between the ﬂow of Equation (4) and the repeated gradient descent method in Equation (3) can be drawn using results in stochastic approximation, i.e. the latter can be seen as a noisy forward Euler discretization of the former. For more details, we refer the reader to Borkar (2008).
In particular, we focus on settings where there may be multiple local equilibria, and classify their regions of attraction for these equilibria. In many settings of interest, there may be multiple steady-state outcomes, and it is of interest to determine which outcome will be chosen by the dynamics in Equation (3). Our results allow us to characterize which regions of the parameter space will converge to which equilibria. We discuss this example in greater formal detail in Section 3.1.
Our main theoretical results can be informally summarized as follows. Theorem 1 states that trajectories of inexact repeated risk minimization will converge exponentially fast to a neighborhood of local performative risk minimizers, and stay in this neighborhood for all future time. It also provides a sufﬁcient condition to under-approximate the regions of attraction for each local performative risk minimizer. In the special case of vanishing perturbations, these trajectories will converge to the minimizers themselves. As a corollary, this implies that performatively stable points will be near performatively optimal points, which was ﬁrst observed in Perdomo et al. (2020) under a different set of conditions. We note that Theorem 1 requires conditions on the curvature of the performative risk: the sublevel sets {x : P R(x) ≤ c} must grow in a precise fashion, such that upper and lower bounds on the performative risk P R(x) imply upper and lower bounds on the norm of the argument x. Furthermore, the gradient of the performative risk must not vary too wildly around nearby points. This is formalized in Assumption 1. Theorem 2 states a geometric condition on the performative perturbation which ensures that trajectories of repeated risk minimization will converge to local performative risk minimizers, intuitively based on the idea that the perturbation does not push against convergence. This result does not require the strong curvature assumptions of Theorem 1.
These results allow us to identify the regions of attraction for various steady-state outcomes. As observed in Miller, Perdomo, and Zrnic (2021), these various outcomes can be interpreted as different echo chambers: essentially the decision variable x can act as a sort of self-fulﬁlling prophecy.1 In settings with multiple echo chambers, we consider the question of which echo chamber will come to dominate, based on the initialization of the learner.
The rest of the paper is organized as follows. In Section 2, we discuss the related literature. In Section 3, we introduce the problem statement and the mathematical concepts used for our results, and provide a motivating example in Sec-
1It is worth noting that we take a slightly different interpretation of an ‘echo chamber’ in this paper. In Miller, Perdomo, and Zrnic (2021), the echo chambers are deﬁned as performatively stable points. In this paper, we consider the regions near each locally performatively optimal point as an echo chamber. As we will discuss in Section 3.1, we are interested in settings where there may be many local performative risk minimizers that attract learning methods depending on initialization.

tion 3.1. In Section 4, we analyze the gradient ﬂow associated with performative risk minimization, and in Section 5, we analyze the ﬂows associated with repeated risk minimization. We demonstrate numerical results in Section 6, and provide closing remarks in Section 7.
2 Background
There has been a great deal of interest in studying decisiondependent distributions. In the context of operations research, this has been studied under either the name decision-dependent uncertainty or endogenous uncertainty. In Jonsbra˚ten, Wets, and Woodruff (1998), Jonsbra˚ten (1998), and Goel and Grossmann (2004), the authors considered oil ﬁeld optimization, with a framework that captures how information revelation can be affected by one’s decisions. In Peeta et al. (2010), the authors consider infrastructure investment, and how investments can affect the future likelihood of disasters. For a taxonomy of the work in the operations research community, we refer the reader to Hellemo, Barton, and Tomasgard (2018).
Another form of decision-dependent distributions is strategic classiﬁcation. In these works, the data source is seen as a utility-maximizing agent. The distribution shift resulting from the learner’s decision is modeled by a best response function. In Hardt et al. (2016) and Bru¨ckner and Scheffer (2011), the authors formulate the problem as a Stackelberg game where the data source responds to the announced classiﬁer. In Dong et al. (2018), the authors consider when the data source’s preferences are hidden information and provide sufﬁcient conditions for convexity of the overall strategic classiﬁcation task. In Akyol, Langbort, and Basar (2016), the authors quantify the cost of strategic classiﬁcation for the classiﬁer. In Milli et al. (2019) and Hu, Immorlica, and Vaughan (2019), the authors note that certain groups may be disproportionately affected as institutions incorporate methods to counter data sources gaming the classiﬁer. In Miller, Milli, and Hardt (2020), the authors formulate strategic classiﬁcation in a causal framework.
Most related to our work is recent efforts in performative prediction. This was introduced in (Perdomo et al. 2020). In this formulation, rather than explicitly modeling the form of the distribution shift, it proposes to analyze the decisiondependent distribution shift in terms of general properties of the D(·) mapping, where D(x) is the distribution of the data when the learner’s decision is x. In Perdomo et al. (2020), the authors introduced the concepts related to performative prediction, demonstrated that neither the performatively stable nor performatively optimal points are subsets of each other, provided sufﬁcient conditions for exact repeated risk minimization (deﬁned as ﬁnding the exact minima with respect to D(xk) at each time step) to converge, and provided conditions in which performatively stable points are near performatively optimal points. In Mendler-Du¨nner et al. (2020), the authors analyze inexact repeated risk minimization (deﬁned as an update step with respect to D(xk) at each time step) from a stochastic optimization framework. In this paper, we build on the inexact repeated risk minimization framework. Miller, Perdomo, and Zrnic (2021) provided sufﬁcient conditions for performative risk itself to be convex. Brown,

Hod, and Kalemaj (2020) extended these results to settings where the distribution updates may have an internal state. In Drusvyatskiy and Xiao (2020), the authors show that many inexact repeated risk minimization algorithms will also converge nicely, due to the way in which the performative perturbation decays near the solution. This shares many ideas with our work here, but we focus on the case where there may be multiple attractive equilibria, and generalize to settings where the perturbation itself may not vanish. In contrast to previous works which provide sufﬁcient conditions to guarantee that an outcome is approached globally, we focus on understanding local regions of attraction for various outcomes.
This work draws on ideas from control theory; in particular, the analysis of gradient ﬂows, Lyapunov functions, and perturbation analysis are the tools we use throughout. We refer the reader to Hirsch, Smale, and Devaney (2012) and Khalil (2001) as good references for these suite of tools.
3 Performative prediction, ﬂows, and perturbations
In this section, we introduce the mathematical concepts used throughout this paper. As previously mentioned, the framework used throughout this paper builds on the framework of performative prediction, introduced in Perdomo et al. (2020).
In Section 1, we have already deﬁned the performative risk in Equation (1) and the decoupled performative risk in Equation (2). Furthermore, we say that x is a local performative risk minimizer is x is a local minima of P R(·). We say x is locally performatively stable if x is a local minima of y → R(y, x). In general, neither imply the other (Perdomo et al. 2020).
Additionally, we consider the performative risk minimizing (PRM) gradient ﬂow, deﬁned by the following differential equation:
x˙ P R = −∇P R(xP R)
= −∇x1 R(xP R, xP R) − ∇x2 R(xP R, xP R) (5) =: fP R(xP R)
This vector ﬁeld can be represented by the gradient of a function, which lends the ﬂow to nice analysis. Under mild conditions, the trajectories of Equation (5) will converge to local minima of the performative risk.
However, as noted in Section 1, many deployments of machine learning do not explicitly model the distribution shift, and, consequently, do not directly minimize the performative risk. We deﬁne the repeated gradient descent (RGD) ﬂow as solutions to the differential equation:
x˙ RR = −∇x1 R(xRR, xRR) =: fRR(xRR) (6)
We deﬁne the performative perturbation:
g(x) := ∇x2 R(x, x) = fRR(x) − fP R(x)
In this paper, we view the PRM gradient ﬂow as the nominal dynamics, and the RGD ﬂow as the perturbed dynamics. The PRM gradient ﬂow has nice properties arising from the fact it is a gradient ﬂow, and, under certain conditions on the performative perturbation, we can prove properties about the

RGD ﬂow, which is the quantity of interest. In particular, we

show ultimate bounds on the distance between the trajectories

of RGD ﬂow and the local performative risk minimizers. This

also implies that under certain conditions on the performative

risk, all performatively stable points are near performative

risk minimizers, as was observed in Perdomo et al. (2020).

Throughout this paper, we will be using tools from pertur-

bation analysis in control theory. For a complete vector ﬁeld

x˙ = f (x), let ϕf (·; x0) denote the unique solution to the

differential equation with initial condition x(0) = x0. For a

scalar-valued function V and a vector ﬁeld f , we can deﬁne

the

derivative

along

trajectories

as

Lf V

(x)

=

∂V ∂x

f (x).

We

say a point x is an equilibrium point if f (x) = 0. An equi-

librium point x is locally asymptotically stable if there ex-

ists a neighborhood U x such that limt→∞ ϕf (t; x ) = x

for all x ∈ U . A set A is positively invariant if for all

x0 ∈ A and t ≥ 0, we have ϕf (t; x0) ∈ A. Additionally, given a set A ⊂ Rn, we say two points x and y are

path-connected in A if there exists a continuous function

γ : [0, 1] → A such that γ(0) = x and γ(1) = y. This forms

an equivalence relation deﬁned on A, and each equivalence

class is a connected component of A. Additionally, we will

use W1(·) to denote the Wasserstein distance, also known

as the earth mover’s distance.

3.1 Examples
Before we present our analysis of the PRM gradient ﬂow and the RGD ﬂow, we introduce an example which motivates the study of performative risk in non-convex settings and multiple local equilibria.

Squared error loss and Bernoulli distributions Consider the loss function (z, x) = 12 |z − x|2, where x is a scalar. Furthermore, suppose that the decision-dependent distribution D(x) is simply Z = 1 with probability p(x) and Z = 0 with probability 1 − p(x), for some function p(·). In this case,
the decoupled performative risk is given by:

R(x1, x2) = p(x2) 1 |1 − x1|2 + (1 − p(x2)) 1 |x1|2

2

2

= 21 [x21 + p(x2)(1 − 2x1)] (7)

We will analyze this model in two ways. First, we will consider general p(·), and, ﬁxing the loss function (·), identify a class of decision-dependent distribution shifts p(·) which

can still ensure convergence to performative risk minimizers,

using Theorem 2. Second, we will consider a concrete example for p(·), and demonstrate how to apply Theorem 1 to understand the regions of convergence.
As our concrete example of p(·), consider the following function as a candidate for p(·):

 exp 1 + 1−(−x−1 1)2

if x ∈ (0, 1)

ϕ(x) = 0

if x ≤ 0

(8)



1

if x ≥ 1

This function is chosen because ϕ(x) = 1 for x ≥ 1, ϕ(x) = 0 for x ≤ 0, and it is continuously differentiable.

The derivative is:

ϕ (x) = ϕ(x) (1−2((x1−−1x))2)2 if x ∈ (0, 1) (9)

0

otherwise

ϕ(·) and its derivative is visualized in Figure 1(a). Since p(0) = 0 and p(1) = 1 for this choice of p(·), Equation (7) directly implies that there are two performative risk minimizers: x = 0 and x = 1. Similarly, we can see that these points are performatively stable as well. The corresponding performative risk and gradients are visualized in Figure 1(b)–(c).
We note that ∇x1 R(x, x) crosses the x-axis at 0.23. As such, RGM ﬂow will converge to x = 0 for x < 0.23, and will converge to x = 1 for x > 0.23. In contrast, ∇P R(x) crosses the x-axis at 0.40, so dynamics that directly minimize the performative risk, e.g. the PRM ﬂow, will converge to x = 0 for x < 0.40 and will converge to x = 1 for x > 0.40.
In this example, there are multiple performative risk minimizers and performatively stable points. Performative risk minimization and repeated gradient descent can converge to different steady-state results, and it is of interest which initializations will converge to which equilibria under both dynamics. In the sequel, we shall demonstrate how different functions p(·) can lead to different steady-state outcomes, as well as how our theoretical results can provide conditions on p(·) such that we achieve convergence to performative risk minimizers, even when performing repeated approximate risk minimization.

4 Analysis of performative risk minimizing
gradient ﬂow
In this section, we consider PRM gradient ﬂow, deﬁned by Equation (5). We observe that gradient ﬂows provide complete vector ﬁelds, and that trajectories will converge to local performative risk minimizers under very mild conditions.
First, we state a proposition guaranteeing that ﬂow is welldeﬁned. The compact sublevel sets ensure that trajectories of Equation (5) remain bounded, which is sufﬁcient to guarantee existence and uniqueness of solutions globally. For proof of the following proposition, we refer the reader to either Khalil (2001, Section 3.1) or Hirsch, Smale, and Devaney (2012, Section 9.3).
Proposition 1 (Existence and uniqueness of gradient ﬂows). Suppose the performative risk P R(·) is continuously differentiable, and its sublevel sets {x : P R(x) ≤ c} are compact for every c ∈ R. Then for any initial condition xP R(0) = x0, there exists a unique solution to the differential equation in Equation (5), deﬁned for all t ≥ 0.
Next, we note that gradient ﬂows have nice properties from the perspective of optimization. Namely: every isolated local minima is locally asymptotically stable, and we can provide sufﬁcient conditions to characterize a subset of the region of convergence.
Proposition 2 (Convergence of gradient ﬂows). Suppose the performative risk P R(·) is twice continuously differentiable, and x∗ is an isolated local performative risk minimizer. Then x∗ is a locally asymptotically stable equilibrium of Equation (5). Furthermore, take any c such that P R(x∗) ≤ c. Let

A ⊆ {x : P R(x) ≤ c} denote the connected component of {x : P R(x) ≤ c} that contains x∗. If x∗ is the only local
performative minimizer in A, then all solutions with initial conditions in A converge to x∗.

Proof. Since x∗ is an isolated local minimizer and the perfor-
mative risk is twice continuously differentiable, there exists a neighborhood U x∗ such that ∇P R(·) is non-zero for all x = x∗. By continuity, there exists some constant such that the connected component of {x : P R(x) ≤ P R(x∗) + } containing x∗ is contained in U . Since it is a sublevel set of
P R(·) and LfPR P R(x) < 0 on its boundary, it is positively invariant. Furthermore, since LfPR (x) < 0 for all x = x∗ on this set, x∗ is locally asymptotically stable by standard
Lyapunov arguments (see, e.g. Khalil (2001, Section 4)).

The sublevel sets of the performative risk are positively invariant with respect to the PRM gradient ﬂow. Furthermore, because of the continuity of trajectories, each connected component will also be positively invariant. This, in tandem with the fact that trajectories must either converge to a local minima or go off to inﬁnity, also implies the previous proposition.
With minimal assumptions, isolated local performative risk minimizers are all locally attractive in the PRM gradient ﬂow. In Section 5, we will view the PRM gradient ﬂow as the nominal dynamics. From this perspective, we analyze the RGD ﬂow as a perturbation from these nominal dynamics. To be able to do any perturbation-based analysis, we will need some stronger conditions on the convergence of the gradient ﬂow associated with performative risk minimization. We note these assumptions here.

Assumption 1 (Sufﬁcient curvature of the performative risk). Fix some isolated local performative risk minimizer x∗. We
assume there exists positive constants c1, c2, c3 and c4 such that the following holds in a neighborhood of x∗:

c1|x − x∗|2 ≤ P R(x) − P R(x∗) ≤ c2|x − x∗|2 (10)

c3|x − x∗| ≤ |∇P R(x)| ≤ c4|x − x∗|

(11)

We will let r denote the radius of this neighborhood, so the above inequalities are valid on the set {x : |x − x∗| ≤ r}.

Assumption 1 provides conditions on which V (x) = P R(x)−P R(x∗) can be used as a Lyapunov function locally. Next, we provide conditions directly on the loss (·) and the decision-dependent distribution shift D(·) which can ensure that Assumption 1 holds, or approximately holds. First, we provide sufﬁcient conditions for the bounds in Equation (10).
Proposition 3 (Performative risk bounds). Let x∗ be a performative risk minimizer and ﬁx any x. If:

1. (·, x) is L1 Lipschitz continuous 2. W1(D(x), D(x∗)) ≤ L2|x − x∗|2 3. (z, ·) is m-strongly convex and L3-smooth for every z
Then: (m/2 − L1L2)|x − x∗|2 ≤ P R(x) − P R(x∗) ≤ (L1L2 + L3/2)|x − x∗|2.

Proof. First, we can break up the performative risk into two parts: P R(x) − P R(x∗) = R(x, x) − R(x∗, x∗) = [R(x, x) − R(x, x∗)] + [R(x, x∗) − R(x∗, x∗)]. Note that R(x, x) − R(x, x∗) = EZ∼D(x)[ (Z, x)] −

Performative risk for p(x) = (x)

2.0

(x)

1.5

(x) 0.10

01..50 0.05

0.0 0.0 0.2 0.4 x 0.6 0.8 1.0

0.00 0.0 0.2 0.4 x 0.6 0.8 1.0

(a)

(b)

Gradients for p(x) = (x)
0.25

0.00

0.25

x1R(x, x)

0.50

x2R(x, x) PR(x)

0.0 0.2 0.4 x 0.6 0.8 1.0

(c)

Figure 1: As an illustrative example, we consider a setting where with the squared error is used as the loss function, and the decision-dependent distribution shift modiﬁes the parameters of a Bernoulli distribution, as discussed in Section 3.1. (a) A visualization of an example decision-dependent distribution shift ϕ(x), as deﬁned in Equation (8), and its derivative, as derived in Equation (9). (b) The performative risk with Pr(Z = 1) = p(x) = ϕ(x). (c) The corresponding gradients for p(x) = ϕ(x).

EZ∼D(x∗)[ (Z, x)]. Conditions (1) and (2), along with

Kantorovich-Rubenstein duality (Villani 2003), implies

this quantity is bounded in absolute value: |R(x, x) −

R(x, x∗)| ≤ L1L2|x − x∗|2. On the other hand, R(x, x∗) − R(x∗, x∗) = EZ∼D(x∗)[ (Z, x) − (Z, x∗)]. By convexity

and L3-smoothness, (z, x) − (z, x∗) ≤ ∇x (z, x∗), x −

x∗ + L3 |x − x∗|2 for any z; taking the expectation and noting that2∇P R(x∗) = 0, we have R(x, x∗) − R(x∗, x∗) ≤

L3 |x − x∗|2. In the other direction, using strong convex-

2
ity

and

similar

arguments,

we

get:

R(x,

x∗)

−

R(x∗,

x∗)

≥

m2 |x − x∗|2. Combining these results yields the desired results.

Note that Condition (2) in Proposition 3 is a variation on the typical -sensitivity deﬁnition. Recall that -sensitivity states that for any x and y, W1(D(x), D(y)) ≤ |x − y| (Perdomo et al. 2020). In contrast, Condition (2) only requires this condition to hold around the point x∗, but requires a stricter bound for x close to x∗. This bound is also more lax than -sensitivity farther away from x∗.
Next, we provide sufﬁcient conditions for a bound on the absolute value of the gradient of the performative risk. This does not exactly recover Assumption 1, as there are additive constants on the upper and lower bounds which do not scale with |x − x∗|. However, these additive constants will be small for decision-dependent distribution shifts D(·) with sufﬁciently small sensitivity parameter .
Proposition 4 (Gradient bounds of the performative risk). Let x∗ be a performative risk minimizer and ﬁx any x. If:
1. (·, x) and (·, x∗) are both L1 Lipschitz continuous 2. (z, ·) is m-strongly convex and L3-smooth for every z 3. D(·) is -sensitive, i.e. W1(D(x), D(y)) ≤ |x − y| 4. ∇x (·, x) is L4 Lipschitz continuous
Then: (m − L4)|x − x∗| − 2 L1 ≤ |∇P R(x)| ≤ (L3 + L4)|x − x∗| + 2 L1.

Proof. Similar to the previous proposition, we break apart this gradient. Note that ∇P R(x∗) = 0, so: |∇P R(x)| = |∇P R(x) − ∇P R(x∗)| = |∇x1 R(x, x) − ∇x1 R(x∗, x∗) + ∇x2 R(x, x) − ∇x2 R(x∗, x∗)|. For the ∇x1 terms, we have: m|x − x∗| ≤ |∇x1 R(x, x∗) − ∇x1 R(x∗, x∗)| ≤ L3|x − x∗| by standard convexity arguments, and |∇x1 R(x, x) −

∇x1 R(x, x∗)| ≤ L4|x − x∗| by the same KantorovichRubenstein duality argument as the previous proposition. For the ∇x2 terms, note that the mapping x2 → R(x, x2) is L1 Lipschitz continuous. Thus, |∇x2 R(x, x)| ≤ L1 and similarly |∇x2 R(x∗, x∗)|. Combining these inequalities yields the desired result.
Depending on the situation, we may be able to directly verify Assumption 1, although, for more complex settings, this is likely to be very difﬁcult. Propositions 3 and 4 provide a set of sufﬁcient conditions for this assumption to approximately hold, but checking the conditions on the decision-dependent distribution shift D(·) may be difﬁcult in practice as well. This is one limitation of this current work, and we believe it is an interesting future research direction to identify conditions which are easy to verify, even in settings with limited information about the distribution shift itself.
5 Analysis of repeated risk minimizing ﬂow
In the previous section, we consider the PRM gradient ﬂow and showed that the trajectories converge to local performative risk minimizers in very general settings. In this section, we will consider the RGD ﬂow, deﬁned by Equation (6). The RGD ﬂow is not necessarily a gradient ﬂow, and generally will not inherit the nice properties we saw in Section 4.
The following theorem provides conditions on the transient response and steady-state behavior of the RGD ﬂow. Prior to T , the trajectories converge exponentially quickly. After T , we have an ultimate bound that holds.
Theorem 1 (Ultimate bounds for RGD ﬂow). Fix any isolated performative risk minimizer x∗ and suppose the conditions of Assumption 1 hold. Let (ci)4i=1 denote the constants from Assumption 1 and r > 0 denote the radius where the inequalities are valid.
Suppose that there exists positive constants < c23/c4 and δ such that the following holds on U = {x : |x − x∗| ≤ r}:
|∇x2 R(x, x)| ≤ |x − x∗| + δ (12)
Additionally, suppose the initial condition satisﬁes:
|x0 − x∗| ≤ c1 r c2

Take any θ ∈ (0, 1) such that:

δ ≤ c2 (1 − θ)r(c2/c4 − )

c1

3

Then, there exists a T ≥ 0 such that: • For all t ≤ T :

|ϕfRR (t; x0)−x∗| ≤

c2 exp(−tθ(c2−c4 )/2c2)|x0−x∗|

c1

3

• For all t ≥ T :

|ϕfRR (t; x0) − x∗| ≤ Proof. See Appendix A.

c2

c4δ

c1 (1 − θ)(c23 − c4 )

Note that, in the special case where δ = 0, we have that the RGD ﬂow converges exponentially quickly to x∗ locally.
Similarly, in the special case where Assumption 1 holds everywhere (i.e. r = ∞), then there is only one minimizer x∗, and all initial conditions converge to a neighborhood of x∗ exponentially fast.
Additionally, note that locally performatively stable points
are equilibria of the RGD ﬂow. This result provides con-
straints on where performatively stable points can be located.
Consider the special case where Assumption 1 holds globally (i.e. r = ∞) and, consequently, there exists only one minimizer x∗. In this special case, Theorem 1 shows that all performatively stable points must be close to x∗. The phe-
nomena that, under certain conditions, performatively stable
points are near performative risk minimizers, was ﬁrst noted
in Perdomo et al. (2020). Our results here provide another set
of conditions under which the same result holds.

5.1 Performative alignment
From the previous analysis, we also identify conditions on the directions of the performative perturbations that are sufﬁcient to show the convergence of Equation (6), the RGD ﬂow, to performative risk minimizers.
Theorem 2 (Performative alignment). Suppose x∗ is a isolated local performative risk minimizer and the following holds for all x in a neighborhood of x∗:
|∇x2 R(x, x)|2 ≤ −∇x1 R(x, x), ∇x2 R(x, x) (13)
Then x∗ is a locally asymptotically stable equilibrium point of the RGD ﬂow, given by Equation (6). Note that this does not require Assumption 1.
Proof. Let V (x) = P R(x) − P R(x∗). Since x∗ is a locally asymptotically equilibria of the PRM ﬂow, we have: V (x∗) = 0, V (x) > 0 for x = 0, and LfPR V (x) < 0 for x = 0. The performative alignment condition ensures that LfPR+gV (x) < 0 as well, and the desired result follows.
We refer to Equation (13) as the performative alignment condition. This condition states that the performative perturbation never increases the performative risk, and the convergence of performative risk minimization is sufﬁcient to guarantee convergence of repeated risk minimization. In other

words, the perturbation is pointing in the correct direction to ensure that P R(·) − P R(x∗) can still act as a Lyapunov function.
Another perspective on performative alignment is to consider the performative risk as a bilinear form whose arguments are parameterized by x. In particular, consider the decoupled performative risk R(·, ·). Let x := (·, x) and let µx denote the probability distribution associated with D(x). Then, we can write R(x1, x2) = µx2 , x1 . From this perspective, R(·, ·) is a bilinear form in x and µx. As such, the performative alignment condition becomes a condition on the way in which and µ are parameterized by x.
In Section 6.2, we apply Theorem 2 to the example outlined in Section 3.1. It provides insight into one of the ways to use Theorem 2: when we ﬁx a loss (·), we can view the performative alignment condition as specifying a class of decision-dependent distribution shifts which do not hamper the convergence of RGD to performative risk minimizers.
6 Numerical examples
In this section, we revisit the model introduced in Section 3.1, where we considered the squared error loss and Bernoulli distributions. We demonstrate how the results of Sections 4 and 5 can be applied. First, we show that the example satisﬁes Assumption 1 and we calculate its corresponding constants. Second, we apply Theorem 1 and show the theoretical convergence rates match simulated trajectories. Finally, we also apply Theorem 2 to the example from Section 3.1 and characterize the class of distribution shifts satisfy the performative alignment condition.
6.1 Checking the curvature of the performative risk and region of convergence
Recall the example from Section 3.1, where x was a scalar, the loss function was the squared error, and the decisiondependent distribution was a Bernoulli random variable whose distribution was determined by p(·). In this section, we consider the speciﬁc decision-dependent distribution shift p = ϕ, which is deﬁned in Equation (8).
When we consider this example, we can see that the bounds on Assumption 1 cannot hold globally, which matches our previous observation that there are multiple isolated performative risk minimizers. However, these bounds may hold locally: we can view the constants (ci)4i=1 from Assumption 1 as a function of the size of the domain r.
For concreteness, let us focus on the equilibrium point x = 0. Recall that Assumption 1 must hold locally, on the domain {x : |x − x∗| ≤ r}. As we increase r, the constants will worsen; we visualize this in Figure 2(a)–(b). Note that these bounds only have to hold locally around the equilibria, as visualized in Figure 2(c). Furthermore, the gradient bounds in Assumption 1 cannot hold beyond r > 0.40, since ∇P R(x) = 0 at that point.
Recall that the convergence results of Theorem 1 can only apply to all initial conditions satisfying |x0−x∗| < c1/c2r; we visualize this as well in Figure 2(a). On the set (0, 0.40],
we can see the quantity c1/c2r is the largest at r = 0.4, with constants c1 = 0.50 and c2 = 1.78. Thus, around the

1.5 1.0 0.5 0.0 0.0

c1 and c2 bounds as a function of r
c1 c2
c1/c2 r
0.2 0.4 r 0.6 0.8 1.0 (a)

c3 and c4 bounds as a function of r

4

c3

3

c4

2

1

0 0.00 0.05 0.10 0.15 0.r20 0.25 0.30 0.35 0.40

(b)

0.6

quadratic bounds for r = 0.5

PR(x)

0.4

lower bound upper bound

0.2

0.0 1.00 0.75 0.50 0.25 0.x00 0.25 0.50 0.75 1.00
(c)

Figure 2: We verify that the performative risk bounds in Assumption 1 are satisﬁed in the example discussed in Section 3.1. (a) As a function of r (the radius of the domain where the inequalities hold), we show the tightest constants c1 and c2 for the bound.
We also plot c1/c2r, which is the radius of a neighborhood of x = 0 to which Theorem 1 can be applied. (b) As a function of r, we show the tightest constants for c3 and c4. (c) Choosing the c1 and c2 constants for r = 0.5, we visualize how the quadratic bounds hold for the performative risk locally.

equilibrium x = 0, the theorem can be applied to all points in the set {x : |x| ≤ 0.21}, with δ = 0. Thus, our theorem shows that all points in this neighborhood of x = 0 will converge. This under-approximates the true region of attraction, which we numerically saw to be {x : x < 0.23}.
6.2 Performative alignment with squared error and Bernoulli distributions
We again consider the example from Section 3.1. However, in this section, we consider a general decision-dependent distribution shift p(·). We suppose that p(0) = 0 and p(1) = 1, so we have two performative risk minimizers as in our previous example. We have ∇x1 R(x, x) = x − p(x) and ∇x2 R(x, x) = (1/2 − x)p (x). The performative alignment condition becomes:
|1/2 − x|2|p (x)|2 ≤ (p(x) − x)(1/2 − x)p (x) (14)
Theorem 2 states that if this condition holds for all x ∈ (0, c), then any initial conditions x0 ∈ (0, c) will converge to x = 0. Similarly, if this condition holds for all x ∈ (c, 1), then all initial conditions in (c, 1) will converge to x = 1. Theorem 2 also implies that this condition cannot be satisﬁed for all x ∈ (0, 1), as then these initial conditions would converge to both x = 0 and x = 1.
If we suppose that p(·) is monotonic on (0, 1), i.e. p (x) ≥ 0, we can also interpret the performative alignment condition as follows. For x ∈ (1/2, 1), the performative alignment condition becomes p(x)−x ≥ (1/2−x)p (x). In this regime, (1/2−x)p (x) ≤ 0. In this setting, if p(x)−x is too negative, the RGD ﬂow will push x away from the nearby minimizer x = 1. Similarly, for x ∈ (0, 1/2), the condition becomes p(x)−x ≤ (1/2−x)p (x). In this regime, (1/2−x)p (x) ≥ 0, and the condition states that p(x) − x cannot be too large, or the RGD ﬂow will push x away from the minimizer x = 0.
In this section, we used Theorem 2 to identify conditions on the decision-dependent distribution shift p(·) which ensure that the performative risk does not increase even when the dynamics follow repeated gradient descent. For this example, the condition is that p satisﬁes Equation (14) for all x ∈ (0, c). More generally, the performative alignment condition allow

us to specify a class of distribution shifts which behave well with respect to performative risk minimization.
7 Closing remarks
In this paper, we analyzed the problem of performative prediction in settings where multiple isolated equilibria may be of interest. We analyzed the gradient ﬂow of performative risk minimization, and identiﬁed regions of attraction for various equilibria. We viewed repeated gradient descent ﬂow as a perturbation of the PRM gradient ﬂow. In particular, we used a Lyapunov function for the PRM gradient ﬂow to analyze the trajectories of the RGD ﬂow. We found conditions on which RGD ﬂow will converge to the local PRM minimizers, and conditions on which they will converge to a neighborhood of PRM minimizers.
These results provide a method to analyze the regions of attraction for various equilibria under repeated risk minimization. In real-world settings with decision-dependent distributions, we expect many situations where the initialization may have a signiﬁcant outcome on the trajectories and ﬁnal outcomes.
References
Akyol, E.; Langbort, C.; and Basar, T. 2016. Price of Transparency in Strategic Machine Learning. In 3rd Workshop on Fairness, Accountability, and Transparency in Machine Learning. Borkar, V. S. 2008. Stochastic Approximation: A Dynamical Systems Viewpoint. Hindustan Book Agency.
Brown, G.; Hod, S.; and Kalemaj, I. 2020. Performative Prediction in a Stateful World. arXiV. Bru¨ckner, M.; and Scheffer, T. 2011. Stackelberg Games for Adversarial Prediction Problems. In Proceedings of the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’11, 547–555. New York, NY, USA: Association for Computing Machinery. ISBN 9781450308137.
Dong, J.; Roth, A.; Schutzman, Z.; Waggoner, B.; and Wu, Z. S. 2018. Strategic Classiﬁcation from Revealed Preferences. In Proceedings of the 2018 ACM Conference on

Economics and Computation, EC ’18, 55–70. New York, NY, USA: Association for Computing Machinery. ISBN 9781450358293.
Dressel, J.; and Farid, H. 2018. The accuracy, fairness, and limits of predicting recidivism. Science Advances, 4(1).
Drusvyatskiy, D.; and Xiao, L. 2020. Stochastic optimization with decision-dependent distributions. arXiV.
Goel, V.; and Grossmann, I. E. 2004. A stochastic programming approach to planning of offshore gas ﬁeld developments under uncertainty in reserves. Computers & Chemical Engineering, 28(8): 1409–1429.
Hardt, M.; Megiddo, N.; Papadimitriou, C.; and Wootters, M. 2016. Strategic Classiﬁcation. In Proceedings of the 2016 ACM Conference on Innovations in Theoretical Computer Science, ITCS ’16, 111–122. New York, NY, USA: Association for Computing Machinery. ISBN 9781450340571.
Hellemo, L.; Barton, P. I.; and Tomasgard, A. 2018. Decisiondependent probabilities in stochastic programs with recourse. Computational Management Science, 15(3): 369–395.
Hirsch, M. W.; Smale, S.; and Devaney, R. L. 2012. Differential Equations, Dynamical Systems, and an Introduction to Chaos. Academic Press, 3rd edition.
Hu, L.; Immorlica, N.; and Vaughan, J. W. 2019. The Disparate Effects of Strategic Manipulation. In Proceedings of the Conference on Fairness, Accountability, and Transparency, FAT* ’19, 259–268. New York, NY, USA: Association for Computing Machinery. ISBN 9781450361255.
Jonsbra˚ten, T. W. 1998. Oil ﬁeld optimization under price uncertainty. Journal of the Operational Research Society, 49(8): 811–818.
Jonsbra˚ten, T. W.; Wets, R. J.-B.; and Woodruff, D. L. 1998. A class of stochastic programs with decision dependent random elements. Annals of Operations Research, 82(0): 83– 106.
Khalil, H. K. 2001. Nonlinear Systems. Pearson, 3rd edition.
Mendler-Du¨nner, C.; Perdomo, J.; Zrnic, T.; and Hardt, M. 2020. Stochastic Optimization for Performative Prediction. In Larochelle, H.; Ranzato, M.; Hadsell, R.; Balcan, M. F.; and Lin, H., eds., Advances in Neural Information Processing Systems, volume 33, 4929–4939. Curran Associates, Inc.
Miller, J.; Milli, S.; and Hardt, M. 2020. Strategic Classiﬁcation is Causal Modeling in Disguise. In III, H. D.; and Singh, A., eds., Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, 6917–6926. PMLR.
Miller, J.; Perdomo, J. C.; and Zrnic, T. 2021. Outside the Echo Chamber: Optimizing the Performative Risk. arXiV.
Milli, S.; Miller, J.; Dragan, A. D.; and Hardt, M. 2019. The Social Cost of Strategic Classiﬁcation. In Proceedings of the Conference on Fairness, Accountability, and Transparency, FAT* ’19, 230–239. New York, NY, USA: Association for Computing Machinery. ISBN 9781450361255.
Peeta, S.; Sibel Salman, F.; Gunnec, D.; and Viswanath, K. 2010. Pre-disaster investment decisions for strengthening a highway network. Computers & Operations Research, 37(10): 1708–1719.

Perdomo, J.; Zrnic, T.; Mendler-Du¨nner, C.; and Hardt, M. 2020. Performative Prediction. In Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, 7599–7609. PMLR.
Villani, C. 2003. Topics in Optimal Transportation. American Mathematical Society.

A Proof of Theorem 1

Let V (x) = R(x, x) − R(x∗, x∗). Note that V (x) ≥ 0 on U = {x : |x − x∗| ≤ r} and V (x) = 0 if and only if x = x∗.

Furthermore, note that

∂V ∂x

(x)

=

[∇x1

R(x,

x)

+

∇x2

R(x,

x)]

.

Consider the function t → V (ϕfRR (t; x0)) and its time derivative. Also, let xRR(t) = ϕfRR (t; x0). Taking the derivative

along trajectories of the repeated risk minimization ﬂow and using Equations (11) and (12):

Lf +gV = ∂V (fP R(x) + g) = −|∇x R + ∇x R|2 + ∇x R + ∇x R, ∇x R ≤

PR

∂x

1

2

1

2

2

−c23|xRR − x∗|2 + c4|xRR − x∗||∇x2 R| ≤ −c23|xRR − x∗|2 + c4 |xRR − x∗|2 + c4δ|xRR − x∗|

These inequalities are valid so long as xRR(t) stays within U , which we will ensure later in the proof. Note that small (by assumption) to ensure that −c23 + c4 < 0.
Let α := c23 − c4 > 0. Take any θ ∈ (0, 1) and note that:

is sufﬁciently

LfPR+gV (xRR) ≤ −θα|xRR − x∗|2 − (1 − θ)α|xRR − x∗|2 + c4δ|xRR − x∗|

Let µ(θ) := c4δ/(1 − θ)α. If |xRR − x∗| ≥ µ(θ), then:

LfP R+gV (xRR) ≤ −θα|xRR − x∗|2

Trajectories of Equation (6) has two stages: a transient due to its initial condition, and then an ultimate bound due to the perturbation. Let T (θ) = inf {t ≥ 0 : |xRR(t) − x∗| ≤ µ(θ)}. Prior to T (θ), we have:

d

V

(xRR(t))

≤

−θα|xRR(t)

−

x∗|2

≤

θα −V

(xRR(t))

dt

c2

The latter follows from Equation (10). By the comparison principle (see, e.g. (Khalil 2001, Lemma 3.4)), we have V (xRR(t)) ≤ exp(−tθα/c2)V (x0). Again using Equation (10), this yields the following inequality, valid for all t ≤ T (θ):

|xRR(t) − x∗| ≤

c2 exp(−tθα/2c2)|x0 − x∗| c1

Note that this inequality also provides an upper bound on T (θ). Additionally, note that this implies the bound |xRR(t) − x∗| ≤ r,
by our assumption on the initial condition. Prior to T (θ), our trajectory stays in U , where our inequalities are valid. At time T (θ), we have |xRR(t) − x∗| ≤ µ(θ). Note that this inequality implies V (xRR(t)) ≤ c2µ2(θ). Since LfPR+gV < 0
on the boundary of Ω(θ) := {x : V (x) ≤ c2µ2(θ)}, we have that Ω(θ) is a positively invariant set. So, for all t ≥ T (θ), we
have xRR(t) ∈ Ω(θ). Using Equation (10), we have the following for all t ≥ T (θ):

|xRR(t) − x∗| ≤

c2 µ(θ) c1

The condition on θ ensures that this quantity is bounded by r, and the trajectory stays in U for t ≥ T (θ). This proves our desired result.

