IMPROVED MASK-CTC FOR NON-AUTOREGRESSIVE END-TO-END ASR Yosuke Higuchi1, Hirofumi Inaguma2, Shinji Watanabe3, Tetsuji Ogawa1, Tetsunori Kobayashi1
1 Waseda University, Japan 2 Kyoto University, Japan 3 Johns Hopkins University, USA

arXiv:2010.13270v2 [eess.AS] 16 Feb 2021

ABSTRACT
For real-world deployment of automatic speech recognition (ASR), the system is desired to be capable of fast inference while relieving the requirement of computational resources. The recently proposed end-to-end ASR system based on mask-predict with connectionist temporal classiﬁcation (CTC), Mask-CTC, fulﬁlls this demand by generating tokens in a non-autoregressive fashion. While MaskCTC achieves remarkably fast inference speed, its recognition performance falls behind that of conventional autoregressive (AR) systems. To boost the performance of Mask-CTC, we ﬁrst propose to enhance the encoder network architecture by employing a recently proposed architecture called Conformer. Next, we propose new training and decoding methods by introducing auxiliary objective to predict the length of a partial target sequence, which allows the model to delete or insert tokens during inference. Experimental results on different ASR tasks show that the proposed approaches improve Mask-CTC signiﬁcantly, outperforming a standard CTC model (15.5% → 9.1% WER on WSJ). Moreover, MaskCTC now achieves competitive results to AR models with no degradation of inference speed (< 0.1 RTF using CPU). We also show a potential application of Mask-CTC to end-to-end speech translation.
Index Terms— Non-autoregressive sequence generation, connectionist temporal classiﬁcation, end-to-end speech recognition, end-to-end speech translation
1. INTRODUCTION
End-to-end automatic speech recognition (E2E-ASR) is a task which directly converts speech into text based on sequence-to-sequence modeling [1–4]. Compared with the traditional hybrid system built upon separately trained modules [5], E2E-ASR greatly simpliﬁes its model training and inference, reducing the cost of model development and the requirement of computational resources. Besides, E2E-ASR has achieved comparable results with those of the hybrid system in diverse tasks [6–9]. Recent model architectures and techniques have further enhanced the performance of E2E-ASR [10–15].
Many of the previous studies on E2E-ASR focus on an autoregressive (AR) model [3, 4], which estimates the likelihood of a sequence generation based on a left-to-right probabilistic chain rule. The AR model often performs the best among other E2E-ASR architectures [6]. However, it suffers from slow inference speed, requiring L-step incremental calculations of the model to generate L tokens. While some neural models, including Transformer [16], permit for efﬁcient inference using GPU, it becomes highly computation intensive in production environments. For example, mobile devices are often constrained by limited on-device resources, where only CPU is generally available for the model inference1. If the inference algo-
1It may be argued that the inference can be done on a cloud server but running the model entirely on-device is important from privacy perspective [9].

rithm is light and simple, ASR can be performed on-device at lowpower consumption with fast recognition speed. Building E2E-ASR models with fast inference speed while suppressing the model calculations is a critical factor for the real-world deployment.
Contrary to the AR framework, a non-autoregressive (NAR) model generates a sequence within a constant number of the inference steps [1, 17]. NAR models in neural machine translation have achieved competitive performance to AR models [18–25]. Several approaches have been proposed to realize a NAR model in E2EASR [26–31]. Connectionist temporal classiﬁcation (CTC) predicts a frame-wise latent alignment between input speech frames and output tokens, and generates a target sequence based on a conditional independence assumption between the frame predictions [26]. However, such assumption limits the performance of CTC compared with that of AR models [32]. Imputer [28] effectively models the contextual dependencies by iteratively predicting the latent alignments of CTC based on mask prediction [22, 33]. Despite achieving comparable performance with that of AR models, Imputer processes the frame-level sequence, consisting of hundreds of units, using the self-attention layers [16], which cost computations proportional to the square of a sequence length. On the other hand, Mask-CTC [29] generates a sequence shorter than Imputer by reﬁning a token-level CTC output with the mask prediction, achieving fast inference speed of less than 0.1 real time factor (RTF) using CPU.
In this work, we focus on improving Mask-CTC based E2EASR, which suits for the production owing to its fast inference. Mask-CTC starts a sequence generation from an output of CTC to avoid the cumbersome length prediction of a target sequence, which is required in the previous NAR framework [17]. However, MaskCTC is confronted with some problems related to this dependence on the CTC output. Firstly, the performance of Mask-CTC is strongly limited to that of CTC since only minor errors in the CTC output, such as spelling mistakes, are subjected to the reﬁnement. Secondly, the length of a target sequence is ﬁxed with that of the CTC output throughout the decoding, making it difﬁcult to recover deletion or insertion errors in the CTC output. To overcome the former, we adopt Conformer [15], which yields better speech processing than Transformer, to improve the CTC performance. For the latter, we introduce new training and decoding strategies to handle the deletion and insertion errors during inference. We also explore the potential of Mask-CTC to the end-to-end speech translation (E2E-ST) task.
2. NON-AUTOREGRESSIVE END-TO-END ASR
E2E-ASR aims to model a conditional probability P (Y |X), which directly maps a T -length input sequence X = (xt ∈ RD|t = 1, ..., T ) into a L-length output sequence Y = (yl ∈ V|l = 1, ..., L). Here, xt is a D-dimensional acoustic feature at frame t, yl is an output token at position l, and V is a vocabulary.
In this section, we review non-autoregressive (NAR) models, which perform sequence generation within a constant number of inference steps in parallel with token positions.

2.1. Connectionist temporal classiﬁcation (CTC)
CTC predicts a frame-level input-output alignment A = (at ∈ V ∪ {ǫ}|t = 1, ..., T ) by introducing a special blank symbol ǫ [1]. Based on a conditional independence assumption per frame, CTC models the probability P (Y |X) by marginalizing over all possible paths as:

Pctc(Y |X) =

P (A|X),

(1)

A∈β−1(Y )

where β−1(Y ) denotes all possible alignments compatible with Y . The conditional independence assumption enables the model to perform NAR sequence generation in a single forward pass.

2.2. Mask-CTC
Due to the conditional independence assumption, the CTC-based model generally suffers from poor recognition performance [6]. Mask-CTC has been proposed to mitigate this problem by iteratively reﬁning an output of CTC with bi-directional contexts of tokens [29]. Mask-CTC adopts an encoder-decoder model built upon Transformer blocks [16]. CTC is applied to the encoder output [12, 34] and the decoder is trained via the masked language model (MLM) objective [22, 33]. For training the MLM decoder, randomly sampled tokens Ymask are replaced by a special mask token <MASK>. Ymask are then predicted conditioning on the rest observed (unmasked) tokens Yobs (= Y \ Ymask) as follows:

Pmlm(Ymask|Yobs, X) =

P (y|Yobs, X).

(2)

y∈Ymask

The number of masked tokens Nmask (= |Ymask|) are sampled from a uniform distribution of 1 to L as in [22, 23]. With the CTC objective in Eq. (1), Mask-CTC optimizes model parameters by maximiz-
ing the following log-likelihood as:

LNAR = α log Pctc(Y |X)+ (1 − α) log Pmlm(Ymask|Yobs, X), (3)

where α (0 ≤ α ≤ 1) is a tunable parameter.
During inference, an output of CTC is obtained through greedy decoding by suppressing repeated tokens and removing blank symbols. Then, low-conﬁdence tokens are replaced with <MASK> by thresholding the posterior probabilities of CTC with Pthres. The resulting unmasked tokens are then fed into the MLM decoder to predict the masked tokens. With this two-pass inference based on the CTC and MLM decoding, the errors in the CTC output, caused by the conditional independence assumption, are expected to be corrected conditioning on the high-conﬁdence tokens in the entire output sequence. The CTC output can be further improved by gradually ﬁlling in the masked tokens in multiple steps K. At the n-th step, yl ∈ Ym(na)sk is predicted as follows:

yl

=

argmax

Pmlm(yl

=

y

|Y

(n) obs

,

X

)

,

(4)

y

where top C positions with the highest decoder probabilities are selected to be predicted at each iteration. By setting C = ⌊Nmask/K⌋, inference can be completed in constant K steps.

3. PROPOSED IMPROVEMENTS OF MASK-CTC
Making use of the CTC output during inference, Mask-CTC effectively avoids the cumbersome length prediction of a target sequence [17], which is rather challenging in ASR [27, 29]. However, there are some drawbacks to this dependence on the CTC output.

Firstly, the performance of CTC limits that of Mask-CTC because the MLM decoder can only make minor changes to the CTC output, such as correcting spelling errors. Secondly, as the target length is ﬁxed with that of the CTC output throughout the decoding, it is difﬁcult to recover deletion or insertion errors.
To tackle these problems, we propose to (1) enhance the encoder architecture by adopting the state-of-the-art encoder architecture, Conformer [15], and (2) introduce new training and decoding methods for the MLM decoder to handle the insertion and deletion errors during inference.

3.1. Conformer
Conformer integrates a convolution module and Macaron-Net style feed-forward network (FFN) [35] into the Transformer encoder block [16]. While the self-attention layer is effective at modeling long-range global context, the convolution layer increases the ability to capture local patterns in a sequence, which is effective for speech processing. We expect the Conformer encoder to boost the performance of CTC and thus improve the overall performance of Mask-CTC accordingly.

3.2. Dynamic length prediction (DLP)

3.2.1. Training

Inspired by [21, 36], we propose dynamic length prediction (DLP), in which the MLM decoder is trained so as to dynamically predict the length of a partial target sequence from a corresponding masked token at each iteration. In addition to the mask prediction task in Eq. (2), which is analogous to solving substitution errors, the decoder is trained to solve simulated deletion and insertion errors.

Deletion-simulated task makes the decoder predict the length of a
partial target sequence, having “one or more” tokens, from a corre-
sponding masked token. For example, given a ground-truth sequence Y = [y1, y2, y3, y4] and its masked sequence [y1, <MASK>, y4] by merging y2 and y3 to <MASK>, the decoder is trained to predict a symbol 2 from the masked position, which corresponds to the length of the partial sequence [y2, y3]. This task makes the decoder aware of the possibility that the masked token has one or more correspond-
ing output tokens, simulating a recover from deletion error in the decoder inputs. To generate such masks, partial tokens in Y are randomly sampled and replaced with <MASK> as Ymask in Eq. (2). Then the consecutive masks are integrated into one single mask to form a masked sequence Y del (|Y del|≤ |Y |), consisting of masked tokens Ymdealsk and observed tokens Yodbesl (= Y del \ Ymdaelsk). The target length labels, Ddel = {di ∈ Z|i = 1, ..., |Ymdaelsk|}, are obtained from the above mask integration process. Ddel is predicted conditioning on the observed tokens Yodbesl as:

Plp(Ddel|Yodbesl, X) =

P

(

d

i

|Y

del obs

,

X

)

,

(5)

i

where Plp is a posterior probability distribution of the mask length.

Insertion-simulated task makes the decoder predict “zero” from
a masked token, indicating the mask corresponds to no partial target sequence. For example, given a ground-truth sequence Y = [y1, y2, y3, y4] and a masked sequence [y1, y2, y3, <MASK>, y4], the decoder is trained to predict a symbol 0 from the masked position as there is no corresponding tokens in Y . This way, we can make the decoder aware of the redundant masked token, simulating a re-
cover from insertion error in the decoder inputs. To obtain masks for this task, we randomly insert <MASK> into Y to form a masked sequence Y ins (|Y ins|> |Y |), consisting of masked tokens Yminassk and observed tokens Yoibnss (= Y ins \ Yminassk). The target lengths

Algorithm 1 Shrink-and-Expand Decoding
Input: K: iteration step, X: intput speech, Yˆ = {yˆl}Ll=1: CTC output 1: Calculate Pl,mlm = Pmlm(yl = yˆl|Yˆ , X) for each yˆl ∈ Yˆ 2: Obtain Yˆmask = {yˆl|Pl,mlm < Pthres}Ll=1 3: Mask Yˆ , where Yˆ = ∅ (yˆl ∈ Yˆmask) // <MASK>
yˆl (yˆl ∈ Yˆobs = Yˆ \ Yˆmask) 4: C = ⌊|Yˆmask|/K⌋ // #masks predicted in each loop 5: while stopping criterion not met do
6: // Shrink (Ex. [yˆ1, ∅, ∅, ∅, yˆ5, ∅, yˆ7] → [yˆ1, ∅, yˆ5, ∅, yˆ7]) 7: Shrink masks in Yˆ and update Yˆmask, Yˆobs accordingly
8: // Expand (Ex. [yˆ1, ∅, yˆ5, ∅, yˆ7] → [yˆ1, ∅, ∅, yˆ5, yˆ7]) 9: Calculate Pl,lp(dl|Yˆobs, X) for each yˆl ∈ Yˆmask 10: Expand masks in Yˆ based on argmaxd Pl,lp(dl = d|Yˆobs, X)
and update Yˆmask, Yˆobs accordingly
11: Calculate Pl,mlm(yl|Yˆobs, X) for each yˆl ∈ Yˆmask 12: Predict masks in Yˆ as argmaxy Pl,mlm(yl = y|Yˆobs, X),
where yˆl with top-C highest probabilities are selected 13: Update Yˆmask, Yˆobs 14: end while 15: return Yˆ

Dins = {di = 0|i = 1, ..., |Yminassk|} of the inserted masks are predicted as:

Plp(Dins|Yoibnss, X) = P (di|Yoibnss, X).

(6)

i

Both tasks are trained jointly by a shared single linear layer followed by a softmax classiﬁer (we set the maximum class to 50) on top of the decoder. The objective of the overall DLP is formulated by combining Eqs. (5) and (6) as:

LLP = log Plp(Ddel|Yodbesl, X) + log Plp(Dins|Yoibnss, X). (7)

Finally, a new Mask-CTC model is trained with a loss combining the conventional objective LNAR from Eq. (3) and the objective of the proposed DLP LLP from Eq. (7) as follows:

LNAR−LP = LNAR + βLLP,

(8)

where β (> 0) is a tunable parameter.

3.2.2. Inference
Alg. 1 shows the proposed shrink-and-expand decoding algorithm, which allows delete and insert tokens during sequence generation. Compared with the conventional method [29] (explained in Sec. 2.2), the proposed decoding differs in the masking process of the CTC output and the prediction process of the masked tokens.
While the previous method detects low-conﬁdence tokens in a CTC output by thresholding the posterior probabilities of CTC, the proposed decoding refers to the probabilities of the MLM decoder (line 2 in Alg. 1). Taking advantage of the bi-directional contexts of tokens, it appeared that the decoder probabilities are more suitable for detecting the CTC errors.
Shrink-and-expand decoding dynamically changes the target sequence length by deleting or inserting masks at each iteration. Shrink step (line 7 in Alg. 1) integrates consecutive masks into one single mask. The integrated masks are then fed to the decoder to predict the length of each mask (line 9 in Alg. 1) required to match the length of an expected target sequence. Expand step (line 10 in Alg. 1) modiﬁes the number of each mask based on the predicted length. For example, if the length of a mask is predicted as 2, we insert a mask to form two consecutive masks, and if predicted as 0, we delete the mask from the target sequence. Finally, the masked tokens are pre-

dicted as the previous way in Eq. (4). Note that shrink-and-expand decoding requires two forward calculations of the decoder for each inference step: one for predicting the length of each mask and one for predicting target tokens.
4. EXPERIMENTS
To evaluate the effectiveness of the proposed improvements of Mask-CTC, we conducted experiments on E2E-ASR models using ESPnet [37]. The recognition performance and the inference speed were evaluated based on word error rate (WER) and real time factor (RTF), respectively. All of the decodings were done without using external language models (LMs).
4.1. Datasets
The experiments were carried out using three tasks: the 81 hours Wall Street Journal (WSJ) [38], the 210 hours TEDLIUM2 [39] and the 16 hours Voxforge in Italian [40]. For the network inputs, we used 80 mel-scale ﬁlterbank coefﬁcients with three-dimensional pitch features extracted using Kaldi [41]. To avoid overﬁtting, we chose data augmentation techniques from speed perturbation [42] and SpecAugment [43], depending on the tasks and models. For the tokenization of target texts, we used characters for WSJ and Voxforge. For TEDLIUM2, we generated a 500 subword vocabulary based on Byte Pair Encoding (BPE) algorithm [44].
4.2. Experimental setup
For all the tasks, we adopted the same Transformer [16] architecture as in [12], where the number of heads H, the dimension of a self-attention layer datt, the dimension of a feed-forward layer dﬀ were set to 4, 256, and 2048, respectively. The encoder consisted of 2 CNN-based downsampling layers followed by 12 self-attention layers and the decoder consisted of 6 self-attention layers. For Conformer encoder [15], we used the same conﬁguration for the selfattention layer as in Transformer, except dﬀ was set to 1024.2 For the model training, we tuned hyper-parameters (e.g., training epochs, etc.) following the recipes provided by ESPnet and we will make our conﬁgurations publicly available on ESPnet to ensure reproducibility. The loss weight α in Eq. (3) was set to 0.3 and β in Eq. (8) was set to 0.1 for Voxforge and 1.0 for WSJ and TEDLIUM2. After the training, a ﬁnal model was obtained by averaging model parameters over the 10 – 50 checkpoints with the best validation performance. During inference, the threshold Pthres for the conventional MaskCTC (explained in Sec. 2.2) was tuned from values of 0.9, 0.99, 0.999. For the proposed decoding in Sec. 3.2, we ﬁxed Pthres to 0.5 for all tasks. RTF was measured using utterances in dev93 of WSJ using Intel(R) Xeon(R) Gold 6148 CPU, 2.40GHz.
4.3. Evaluated models
We evaluated different E2E-ASR models, each of which can either be autoregressive (AR) or non-autoregressive (NAR). AR indicates an AR model trained with the joint CTC-attention objective [12, 34] and, for inference, we used the joint CTC-attention decoding [45] with beam search. CTC indicates a NAR model simply trained with the CTC objective as in Eq. (1) [26]. Mask-CTC indicates a NAR model trained with the conventional Mask-CTC framework [29] as explained in Sec. 2.2. We also applied the proposed dynamic length prediction (DLP, explained in Sec. 3.2) to Mask-CTC.
For each model, we compared the results between Transformer (TF) and Conformer (CF) encoders.
2We adjusted dﬀ to avoid the Conformer-based model becoming slow during inference due to the increase in number of parameters.

Table 1. Word error rate (WER) and real time factor (RTF) on WSJ. The proposed improvements on Mask-CTC (the use of Conformer and dynamic length prediction) are compared with CTC and autoregressive (AR) models. K denotes the number of inference steps required to generate each output token. RTF was measured on dev93 using CPU. For each Mask-CTC model, RTF was calculated for K = 10.

Model

#Params (M)

dev93 WSJ (WER) eval92

RTF

Speedup

Autoregressive
A1 Transformer-AR [12] A2 + beam search A3 Conformer-AR A4 + beam search

K = L (avg. 99.9)

27.2

13.5

27.2

12.8

30.4

11.4

30.4

11.1

K = L (avg. 100.3)
10.8 10.6 8.8 8.5

0.456±0.005 5.067±0.012 0.474±0.009 5.094±0.031

1.00× 0.09× 0.96× 0.09×

Non-autoregressive

K ≤ C (C: const.)

0

1

5 10

K ≤ C (C: const.)

0

1

5 10

B1 Transformer-CTC

17.7

19.4 –

− − 15.5 − − −

0.021±0.000

B2 Transformer-Mask-CTC [29]

27.2

15.5 15.2 14.9 14.9 12.5 12.2 12.0 12.0 0.063±0.001

B3 + dynamic length prediction 27.2 15.5 14.0 13.9 13.8 12.4 11.1 10.8 10.8 0.074±0.001

C1 Conformer-CTC

20.9

13.0 − − − 10.8 − − −

0.033±0.000

C2 Conformer-Mask-CTC

30.4 11.9 11.8 11.7 11.7 9.4 9.2 9.2 9.1 0.063±0.000

C3 + dynamic length prediction 30.4 11.8 11.3 11.3 11.3 9.6 9.3 9.1 9.1 0.080±0.000

21.71× 7.24× 6.16×
13.81× 7.24× 5.70×

Table 2. Word error rates (WER) on Voxforge Italian and TEDLIUM2. Results with beam search are reported in parentheses.

Model
TF-AR [12] CF-AR

#itr K
L L

Voxforge
35.5 (35.7) 29.8 (29.8)

TEDLIUM2
9.5 (8.9) 8.4 (7.9)

TF-CTC

0

56.1

16.6

TF-Mask-CTC [29] 10

38.3

10.9

+ DLP

5

35.1

10.6

CF-CTC

0

31.8

9.5

CF-Mask-CTC

10

29.2

8.6

+ DLP

5

29.0

9.7

4.4. Results
Table 1 shows the results on WSJ. By comparing the results among NAR models using Transformer (B*), the proposed DLP (B3) outperformed the conventional Mask-CTC (B2). We can conclude that DLP successfully recovered insertion and deletion errors in the CTC output, looking at B2 and B3 having the same CTC results (K = 0) and B3 resulted in better improvement. The performance of CTC signiﬁcantly improved by using Conformer (B1, C1) and Mask-CTC greatly beneﬁted from it (C2). The errors were further reduced by applying DLP (C3), achieving 9.1% in eval92 which was the best among the NAR models and better than that of the state-of-the-art model without LM [46,47]. By comparing results between NAR and AR models, Mask-CTC achieved highly competitive performance to AR models for both Transformer (A1, B3) and Conformer (A3, C3), demonstrating the effectiveness of the proposed methods for improving the original Mask-CTC.
In terms of the decoding speed using CPU, all NAR models (B*, C*) were at least 5.7 times faster than the AR models (A*), achieving RTF of under 0.1. The speed of Mask-CTC slowed down by applying DLP (B3, C3) due to the increase in number of decoder calculations (explained in Sec. 3.2.2). However, with DLP, the error rates converged faster in less iterations (K = 5) and thus the inference speed was the same or even faster than the original Mask-CTC.
Table 2 shows the results on Voxforge and TEDLIUM2. MaskCTC achieved comparable results with those of AR models by the proposed improvements. However, DLP did not improve CF-MaskCTC on TEDLIUM2. We observed the performance of CF-MaskCTC was accurate enough and DLP was not effective.

Table 3. Speech translation results on Fisher-CallHome Spanish

Model

Fisher (BLEU) dev dev2 test

CallHome (BLEU) devtest evltest

TF-AR TF-CTC TF-Mask-CTC + CTC greedy + original decoding + mask-predict (MP) + restricted MP

47.01 45.57
45.93 44.80 47.43 49.94

47.89 46.97
46.82 45.40 48.14 49.42

47.19 45.97
46.17 44.39 46.96 48.66

18.11 15.99
15.73 14.14 16.52 16.96

17.95 15.91
15.60 14.14 16.42 16.79

4.5. End-to-end speech translation (E2E-ST)
To see a potential application to other speech tasks, we applied Mask-CTC framework to the E2E-ST task, following [48]. For NAR models, we used sequence-level knowledge distillation [49]. Table 3 shows the results on Fisher-CallHome Spanish corpus [50]. Since input-output alignments are non-monotonic in this task, we observed the conﬁdence ﬁltering based on the CTC probabilities did not work well, unlike ASR. Next, we performed the mask-predict (MP) decoding proposed in MT [22] by starting from all <MASK> and conﬁrmed some gains over CTC. Finally, we initialized a target sequence with the ﬁltered CTC output as in the ASR task and then performed the MP decoding. Here, the number of masked tokens at each iteration are truncated by the number of initial masked tokens Nmask (restricted MP) to keep information from the CTC output for the later iterations. This way, the results were further improved from the CTC greedy results by a large margin. Moreover, interestingly, Mask-CTC outperformed the AR model on this corpus.
5. CONCLUSION
This paper proposed improvements of Mask-CTC based nonautoregressive E2E-ASR. We adopted the Conformer encoder to boost the performance of CTC and introduced new training tasks for the model to dynamically delete or insert tokens during inference. The experimental results demonstrated the effectiveness of the improved Mask-CTC, achieving competitive performance to autoregressive models with no degradation of inference speed. We also showed Mask-CTC framework can be used for end-to-end speech translation. Our future plan is to integrate an extrenal language model into Mask-CTC while keeping the decoding speed fast.

6. REFERENCES
[1] Alex Graves et al., “Connectionist temporal classiﬁcation: labelling unsegmented sequence data with recurrent neural networks,” in Proc. of ICML, 2006, pp. 369–376.
[2] Alex Graves, “Sequence transduction with recurrent neural networks,” arXiv preprint arXiv:1211.3711, 2012.
[3] Ilya Sutskever and other, “Sequence to sequence learning with neural networks,” in Proc. of NeurIPS, 2014, pp. 3104–3112.
[4] Dzmitry Bahdanau et al., “Neural machine translation by jointly learning to align and translate,” in Proc. of ICLR, 2015.
[5] Geoffrey Hinton et al., “Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups,” IEEE Signal processing magazine, vol. 29, no. 6, pp. 82–97, 2012.
[6] Chung-Cheng Chiu et al., “State-of-the-art speech recognition with sequence-to-sequence models,” in Proc. of ICASSP, 2018, pp. 4774–4778.
[7] Christoph Lu¨scher et al., “RWTH ASR systems for librispeech: Hybrid vs attention,” in Proc. of Interspeech, 2019, pp. 231– 235.
[8] Shigeki Karita et al., “A comparative study on Transformer vs RNN in speech applications,” in Proc. of ASRU, 2019, pp. 449–456.
[9] Tara N Sainath et al., “A streaming on-device end-to-end model surpassing server-side conventional model quality and latency,” in Proc. of ICASSP, 2020, pp. 6059–6063.
[10] Linhao Dong et al., “Speech-Transformer: a no-recurrence sequence-to-sequence model for speech recognition,” in Proc. of ICASSP, 2018, pp. 5884–5888.
[11] Tara N Sainath et al., “Two-pass end-to-end speech recognition,” in Proc. of Interspeech, 2019, pp. 2773–2777.
[12] Shigeki Karita et al., “Improving Transformer-based end-toend speech recognition with connectionist temporal classiﬁcation and language model integration,” in Proc. of Interspeech, 2019, pp. 1408–1412.
[13] Samuel Kriman et al., “Quartznet: Deep automatic speech recognition with 1d time-channel separable convolutions,” in Proc. of ICASSP, 2020, pp. 6124–6128.
[14] Wei Han et al., “ContextNet: Improving convolutional neural networks for automatic speech recognition with global context,” in Proc. of Interspeech, 2020.
[15] Anmol Gulati et al., “Conformer: Convolution-augmented Transformer for speech recognition,” in Proc. of Interspeech, 2020.
[16] Ashish Vaswani et al., “Attention is all you need,” in Proc. of NeurIPS, 2017, pp. 5998–6008.
[17] Jiatao Gu et al., “Non-autoregressive neural machine translation,” in Proc. of ICLR, 2018.
[18] Jindˇrich Libovicky´ and Jindˇrich Helcl, “End-to-end nonautoregressive neural machine translation with connectionist temporal classiﬁcation,” in Proc. of EMNLP, 2018, pp. 3016– 3021.
[19] Jason Lee et al., “Deterministic non-autoregressive neural sequence modeling by iterative reﬁnement,” in Proc. of EMNLP, 2018, pp. 1173–1182.
[20] Mitchell Stern et al., “Insertion Transformer: Flexible sequence generation via insertion operations,” in Proc. of ICML, 2019, pp. 5976–5985.
[21] Jiatao Gu et al., “Levenshtein Transformer,” in Proc. of NeurIPS, 2019, pp. 11181–11191.
[22] Marjan Ghazvininejad et al., “Mask-predict: Parallel decoding of conditional masked language models,” in Proc. of EMNLPIJCNLP, 2019, pp. 6114–6123.
[23] Marjan Ghazvininejad et al., “Semi-autoregressive training improves mask-predict decoding,” arXiv preprint arXiv:2001.08785, 2020.

[24] Chitwan Saharia et al., “Non-autoregressive machine translation with latent alignments,” arXiv preprint arXiv:2004.07437, 2020.
[25] Xuezhe Ma et al., “FlowSeq: Non-autoregressive conditional sequence generation with generative ﬂow,” in Proc. of EMNLP-IJCNLP, 2019, pp. 4273–4283.
[26] Alex Graves and Navdeep Jaitly, “Towards end-to-end speech recognition with recurrent neural networks,” in Proceedings of ICML, 2014, pp. 1764–1772.
[27] Nanxin Chen et al., “Listen and ﬁll in the missing letters: Nonautoregressive Transformer for speech recognition,” arXiv preprint arXiv:1911.04908, 2019.
[28] William Chan et al., “Imputer: Sequence modelling via imputation and dynamic programming,” in Proc. of ICML, 2020.
[29] Yosuke Higuchi et al., “Mask CTC: Non-autoregressive endto-end ASR with CTC and mask predict,” in Proc. of Interspeech, 2020.
[30] Yuya Fujita et al., “Insertion-based modeling for end-to-end automatic speech recognition,” in Proc. of Interspeech, 2020.
[31] Zhengkun Tian et al., “Spike-triggered non-autoregressive Transformer for end-to-end speech recognition,” in Proc. of Interspeech, 2020.
[32] Eric Battenberg et al., “Exploring neural transducers for endto-end speech recognition,” in Proc. of ASRU, 2017, pp. 206– 213.
[33] Jacob Devlin et al., “BERT: Pre-training of deep bidirectional transformers for language understanding,” in Proc. of NAACLHLT, 2019, pp. 4171–4186.
[34] Suyoun Kim et al., “Joint CTC-attention based end-to-end speech recognition using multi-task learning,” in Proc. of ICASSP, 2017, pp. 4835–4839.
[35] Yiping Lu et al., “Understanding and improving transformer from a multi-particle dynamic system point of view,” in Proc. ICLR, 2020.
[36] Yi Ren et al., “FastSpeech: Fast, robust and controllable text to speech,” in Proc. of NeurIPS, 2019.
[37] Shinji Watanabe et al., “ESPnet: End-to-end speech processing toolkit,” in Proc. of Interspeech, 2018, pp. 2207–2211.
[38] Douglas B Paul and Janet M Baker, “The design for the wall street journal-based CSR corpus,” in Proc. of Workshop on Speech and Natural Language, 1992, pp. 357–362.
[39] Anthony Rousseau et al., “Enhancing the TED-LIUM corpus with selected data for language modeling and more TED talks,” in Porc. of LREC, May 2014, pp. 3935–3939.
[40] “Voxforge,” http://www.voxforge.org.
[41] Daniel Povey et al., “The Kaldi speech recognition toolkit,” in Proc. of ASRU, 2011.
[42] Tom Ko et al., “Audio augmentation for speech recognition,” in Proc. of Interspeech, 2015.
[43] Daniel S Park et al., “SpecAugment: A simple data augmentation method for automatic speech recognition,” in Proc. of Interspeech, 2019, pp. 2613–2617.
[44] Rico Sennrich et al., “Neural machine translation of rare words with subword units,” in Proc. of ACL, 2016, pp. 1715–1725.
[45] Takaaki Hori et al., “Joint CTC/attention decoding for end-toend speech recognition,” in Proc. of ACL, 2017, pp. 518–529.
[46] Sara Sabour et al., “Optimal completion distillation for sequence learning,” in Proc. of ICLR, 2019.
[47] Lasse Borgholt et al., “Do end-to-end speech recognition models care about context?,” in Proc. of Interspeech, 2020.
[48] Hirofumi Inaguma et al., “ESPnet-ST: All-in-one speech translation toolkit,” in Proc. ACL: System Demonstrations, 2020, pp. 302–311.
[49] Yoon Kim et al., “Sequence-level knowledge distillation,” in Proc. of EMNLP, 2016, pp. 1317–1327.
[50] Matt Post et al., “Improved speech-to-text translation with the Fisher and Callhome Spanish–English speech translation corpus,” in Proc. of IWSLT, 2013.

