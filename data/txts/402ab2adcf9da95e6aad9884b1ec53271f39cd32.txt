1
Inverse Extended Kalman Filter
Himali Singh, Arpan Chattopadhyay∗ and Kumar Vijay Mishra∗

arXiv:2201.01539v1 [math.OC] 5 Jan 2022

Abstract—Recent advances in counter-adversarial systems have garnered signiﬁcant research interest in inverse ﬁltering from a Bayesian perspective. For example, interest in estimating the adversary’s Kalman ﬁlter tracked estimate with the purpose of predicting the adversary’s future steps has led to recent formulations of inverse Kalman ﬁlter (IKF). In this context of inverse ﬁltering, we address the key challenges of nonlinear process dynamics and unknown input to the forward ﬁlter by proposing inverse extended Kalman ﬁlter (I-EKF). We derive I-EKF with and without an unknown input by considering nonlinearity in both forward and inverse state-space models. In the process, I-KF-withunknown-input is also obtained. We then provide theoretical stability guarantees using both bounded nonlinearity and unknown matrix approaches. We further generalize these formulations and results to the case of higher-order, Gaussian-sum, and dithered I-EKFs. Numerical experiments validate our methods for various proposed inverse ﬁlters using the recursive Crame´r-Rao lower bound as a benchmark.
Index Terms—Bayesian ﬁltering, counter-adversarial systems, extended Kalman ﬁlter, inverse ﬁltering, nonlinear processes.
I. INTRODUCTION
In many engineering applications, it is desired to infer the parameters of a ﬁltering system by observing its output. This inverse ﬁltering is useful in applications such as system identiﬁcation, fault detection, image deblurring, and signal deconvolution [1, 2]. Conventional inverse ﬁltering is limited to non-dynamic systems. However, applications such as cognitive and counter-adversarial systems [3] have recently been shown to require designing the inverse of classical stochastic ﬁlters such as hidden Markov model (HMM) ﬁlter [4] and Kalman ﬁlter (KF) [5]. The cognitive systems are intelligent units that sense the environment, learn relevant information about it, and then adapt themselves in real-time to optimally enhance their performance. For example, a cognitive radar [6] adapts both transmitter and receiver processing in order to achieve desired goals such as improved target detection [7] and tracking [8]. In this context, [9] recently introduced inverse cognition, in the form of inverse stochastic ﬁlters, to detect cognitive sensor and further estimate the information that the same sensor may have learnt. In this paper, we focus on inverse stochastic ﬁltering for such inverse cognition applications.
At the heart of inverse cognition are two agents: ‘us’ (e.g., an intelligent target) and an ‘adversary’ (e.g., a sensor or radar) equipped with a Bayesian tracker. The adversary infers an estimate of our kinematic state and cognitively adapts its actions based on this estimate. ‘We’ observe adversary’s actions with the goal to predict its future actions in a Bayesian sense. In particular, [10] developed stochastic revealed preferences-based algorithms to ascertain if the adversary’s actions are consistent with optimizing a utility function; and if so, estimate that function.
∗A. C. and K. V. M. have made equal contributions. H. S. and A. C. are with the Electrical Engineering Department, Indian Institute of Technology Delhi, India. Email: {eez208426, arpanc}@ee.iitd.ac.in. K. V. M. is with the United States CCDC Army Research Laboratory, Adelphi, MD 20783 USA. E-mail: kvm@ieee.org. A. C. acknowledges support via faculty seed grant and professional development allowance from IIT Delhi. H. S. acknowledges support via Ministry of Human Resource Development Fellowship and Prime Minister Research Fellowship. The conference precursor of this work has been submitted to the 2022 American Control Conference (ACC).

If the target aims to guard against the adversary’s future actions, it requires an estimate of the adversary’s inference. This is precisely the objective of inverse Bayesian ﬁltering. In (forward) Bayesian ﬁltering, given noisy observations, a posterior distribution of the underlying state is obtained. An example is the KF, which provides optimal estimates of the underlying state in linear system dynamics with Gaussian measurement and process noises. The inverse ﬁltering problem, on the other hand, is concerned with estimating this posterior distribution of a Bayesian ﬁlter given the noisy measurements of the posterior. An example of such a system is the recently introduced inverse Kalman ﬁlter (I-KF) [9]. Note that, historically, the Wiener ﬁlter – a special case of KF when the process is stationary – has long been used for frequency-domain inverse ﬁltering for deblurring in image processing [11]. Further, some early works [12] have investigated the inverse problem of ﬁnding cost criterion for a control policy.
Although KF and its continuous-time variant Kalman-Bucy ﬁlter [13] are highly effective in many practical applications, they are optimal for only linear and Gaussian models. In practice, many engineering problems involve nonlinear processes [14, 15]. In these cases, a linearized KF, which employs a linear system whose states represent the deviations from a nominal trajectory of a nonlinear system, is used. The KF estimates the deviations from the nominal trajectory and obtains an estimate of the states of the nonlinear system. The linearized KF is extended to directly estimate the states of a nonlinear system in the extended KF (EKF) [16]. The linearization is locally at the state estimates through Taylor series expansion. This is very similar to the Volterra series ﬁlters [17] that are nonlinear counterparts of adaptive linear ﬁlters.
While inverse nonlinear ﬁlters have been studied for adaptive systems in some previous works [18, 19], the inverse of nonlinear stochastic ﬁlters such as EKF remain unexamined so far. To address the aforementioned nonlinear inverse cognition scenarios, contrary to prior works which focus on only linear I-KF [9], our goal is to derive and analyze inverse EKF (I-EKF). The standard EKF has its limitations. Unlike KF, the ﬁlter and (ﬁlter) gain equations of EKF are not decoupled and hence ofﬂine computations of these EKF quantities are not possible. The EKF performs poorly when the dynamic system is signiﬁcantly nonlinear. It is also very sensitive to initialization because of Taylor approximation, and may even completely fail [20].
These EKF drawbacks have led to the development of variants such as higher-order EKFs [21], which include terms beyond the ﬁrst-order in the Taylor series. These ﬁlters reduce the linearization errors that are inherent to the EKF and may provide an improved estimation at the cost of higher complexity and computations. The second-order EKF (SOEKF) [22, 23] performs better than EKF but its stability is not guaranteed [24]. Therefore, in this paper, we derive both the stability conditions of I-EKF under various scenarios and the inverses of a few prominent EKF variants.
Preliminary results of this work appeared in our conference publication [25], where only I-EKF was formulated. In this paper, we extend our methods to several other inverse ﬁltering systems and applications. Our main contributions are: 1) I-KF and I-EKF with unknown inputs. In the inverse cognition scenario, the target may introduce additional motion or jamming that is known to the target but not to the adversarial cognitive sensor. While deriving I-EKF, we therefore consider this more general

2

nonlinear system model with unknown input; this I-EKF simpliﬁes in the absence of unknown excitation. In the process, we also obtain I-KF-with-unknown-input that was not examined in the I-KF of [9]. 2) Augmented states for I-EKF. For systems with unknown inputs, the adversary’s state estimate depends on its estimate of the unknown input. As a result, the adversary’s forward ﬁlters vary with system models. We overcome this challenge by considering augmented states in the inverse ﬁlter so that the unknown estimation is performed jointly with state estimation, including for KF with direct feedthrough. For different inverse ﬁlters, separate augmented states are considered depending on the state transitions for the inverse ﬁlter. 3) Stability of I-EKF. In general, stability and convergence results for nonlinear KFs, and more so for their inverses, are difﬁcult to obtain. In this work, we show the stability of I-EKF using two techniques. The ﬁrst approach is based on bounded nonlinearities, which has been earlier employed for proving stochastic stability of discrete-time [26] and continuous-time [27] EKFs. Here, the estimation error was shown to be both exponentially bounded in the mean square sense and with probability one. The second method relaxes the bound on the initial estimation error by introducing unknown matrices to model the linearization errors [28]. Besides providing the sufﬁcient conditions for error boundedness, this approach also rigorously justiﬁes the enlarging of the noise covariance matrices to stabilize the ﬁlter [29]. Because of the dependence of the I-EKF’s error dynamics on the forward ﬁlter’s recursive updates, the derivations of these theoretical guarantees are not straightforward. 4) Extensions to inverses of EKF variants. We also extend the IEKF theory and stability proof to SOEKF. The presence of second order terms poses additional challenges in deriving the theoretical guarantees for both forward and inverse SOEKF. In addition, we consider the inverse of Gaussian-sum EKF (GS-EKF) which comprises of several individual EKFs [30, 31]. Here, the a posteriori density function is approximated by a sum of Gaussian density functions each of which has its own separate EKF. In situations where the estimation error is small, the a posteriori density is approximated adequately by one Gaussian density, and the Gaussian sum ﬁlter reduces to the EKF; the same is true for their inverses. Finally, we also examine the inverse of dithered EKF (DEKF) [32], which was shown to perform better when nonlinearities are cone-bounded. This ﬁlter introduces dither signals prior to the nonlinearities to tighten the cone-bounds and hence, improve stability properties. We validate the estimation errors of all inverse EKFs through extensive numerical experiments with recursive Crame´r-Rao lower bound (RCRLB) [33] as the performance metric.
The rest of the paper is organized as follows. In the next section, we provide the background of inverse cognition model. The inverse EKF with unknown input is then derived in the Section III for the case of the forward EKF with and without direct feed-through. Here, we also obtain the standard I-EKF in the absence of unknown input. Then, similar cases are considered for inverse KF with unknown input in Section IV. We then discuss the inverse of some EKF variants in Section V before deriving the stability conditions in Section VI. In Section VII, we corroborate our results with numerical experiments before concluding in Section VIII.
Throughout the paper, we reserve boldface lowercase and uppercase letters for vectors (column vectors) and matrices, respectively. The notation [a]i is used to denote the i-th component of vector a and [A]i,j denotes the (i, j)-th component of matrix A. The transpose operation is denoted by (·)T . The norm denoted by || · ||2 represents the l2 norm for a vector. The notation Tr(A), rank(A), ||A||, and A ∞, respectively, denote the trace, rank, spectral norm and maximum row sum norm of A. For matrices A and B, the inequality A B means that B − A is a positive semideﬁnite

(p.s.d.) matrix. For a function f : Rn → Rm, ∇f denotes the Rm×n Jacobian matrix. Similarly, for a function f : Rn → R, ∇f and ∇2f denote the gradient vector (Rn×1) and Hessian matrix (Rn×n), respectively. A ‘n×n’ identity matrix is denoted by In and a n×m all zero matrix is denoted by 0n×m. The notation {ai}i1≤i≤i2 denotes a set of elements indexed by integer i. The notation x ∼ N (µ, Q)
represents a random variable drawn from a normal distribution with
mean µ and covariance matrix Q while x ∼ U [ul, uu] means a random variable drawn from the uniform distribution over [ul, uu].

II. DESIDERATA FOR INVERSE COGNITION
Consider a discrete-time stochastic dynamical system as ‘our’ state evolution process {xk}k≥0, where xk ∈ Rn×1 is the state at the k-th time instant. Our current state is known to us perfectly. The control input uk ∈ Rm×1 is known to us but not to the adversary. In a linear state-space model, we denote the state-transition and control input matrices by F ∈ Rn×n and B ∈ Rn×m, respectively. Our state evolves as

xk+1 = Fxk + Buk + wk,

(1)

where wk ∼ N (0n×1, Q) is the process noise with covariance matrix Q ∈ Rn×n. At the adversary, the observation and control input matrices are given by H ∈ Rp×n and D ∈ Rp×m, respectively. The adversary makes a noisy observation yk ∈ Rp×1 at time k as

yk = Hxk + Duk + vk,

(2)

where vk ∼ N (0p×1, R) is the adversary’s measurement noise with covariance matrix R ∈ Rp×p.
The adversary uses {yj}1≤j≤k to compute the estimate xˆk of our
current state xk using a (forward) stochastic ﬁlter. The adversary then uses this estimate to administer an action matrix G ∈ Rna×n on xˆk.
We make noisy observations of this action as

ak = Gxˆk + k ∈ Rna×1,

(3)

where k ∼ N (0na×1, Σ ) is our measurement noise with covariance matrix Σ ∈ Rna×na . Finally, ‘we’ use {aj , xj , uj }1≤j≤k to compute the estimate xˆk ∈ Rn×1 of xˆk in the (inverse) stochastic ﬁlter, where Σk is the associated estimation covariance matrix of xˆk. Deﬁne uˆk to be the estimate of uk as computed in the adversary’s forward ﬁlter, while uˆk is an estimate of uˆk as computed by ‘our’ inverse ﬁlter. The noise processes {wk}k≥0, {vk}k≥1 and { k}k≥1 are mutually independent and i.i.d. across time. These noise distributions are known to ‘us’ as well as the adversary. When the unknown input is absent, either B = 0n×m or D = 0p×m or
both vanish. Throughout the paper, we assume that both parties
(adversary and ‘us’) have perfect knowledge of the system model
and parameters.
When the system dynamics are nonlinear, then the matrix pairs {F, B}, {H, D}, and the matrix G are replaced by nonlinear functions f (·, ·), h(·, ·), and g(·), respectively, as

xk+1 = f (xk, uk) + wk,

(4)

yk = h(xk, uk) + vk,

(5)

ak = g(xˆk) + k.

(6)

This is a direct feed-through (DF) model, wherein yk depends on the unknown input. Without DF, the adversary’s observations are

yk = h(xk) + vk.

(7)

We show in the following Section III, the presence or absence of the unknown input leads to different solution approaches towards forward and inverse ﬁlters. For simplicity, the presence of known exogenous inputs is also ignored in state evolution and observations.

3

However, it is trivial to extend the inverse ﬁlters developed in this paper for these modiﬁcations in the system model. Throughout the paper, we focus on discrete-time models.
III. I-EKF WITH UNKNOWN INPUT

hk(uˆk−2, xˆk−1, xk, vk)
= Kuk−1(HkBk−1uˆk−2 − h(f (xˆk−1, uˆk−2)) + h(xk) + vk), (11)

fk(xˆk, uˆk−2, xˆk−1, xk, xk+1, vk, vk+1)

= φk(xˆk, hk(uˆk−2, xˆk−1, xk, vk), xk+1, vk+1).

(12)

In order to extend KF to the nonlinear processes, EKF linearizes the model about the nominal values of the state vector and control input. The EKF is similar to the iterated least squares (ILS) method except that the former is for dynamical systems and the latter is not [34]. Note that the optimal forward EKFs with and without DF are conceptually different. In the latter case, while the observation yk is unaffected by the unknown input uk, it is still dependent on uk−1 through xk; this induces a one-step delay in the adversary’s estimate of uk. On the other hand, with DF, there is no such delay in estimating uk. We now show that this difference results in different inverse ﬁlters for these two cases.
A. I-EKF-without-DF unknown input

In these state transition equations, ‘our’ actual states xk and xk+1

are perfectly known to us and henceforth treated as known exogenous

inputs. Note that, unlike the forward ﬁlter, the process noise terms vk and vk+1 are non-additive because the ﬁlter gains Kxk+1 and Kuk−1

depend on the previous estimates (through the Jacobians). Denote zˆk+1 =. xˆTk+1 uˆTk−1 T . The state transition of the

augmented state zk+1 depends on the estimate xˆk−1 which we approximate by our previous estimate xˆk−1. With this approxima-

tion, xˆk−1 is treated as a known exogenous input for the inverse ﬁlter while the augmented process noise vector is vkT vkT+1 T .

Deﬁne the Jacobians Fzk =.

∇ˆ fk xˆk
0m×n

∇ˆ fk uˆ

.

k−2 , and Gk+1 =

∇ˆ hk

uˆ k−2

∇xˆˆk+1|k g 0na×m with respect to the augmented state; Jacobian

Consider the non-linear system without DF given by (4) and (7).

Linearize the model functions as Fk =. ∇xf (x, uˆk−1)|x=xˆ , Bk =.

∇uf (xˆk, u)|u=uˆ and Hk+1 =. ∇xh(x)|x=xˆ

.

k

k−1

k+1|k

1) Forward ﬁlter: The forward ﬁlter’s recursive state estimation

procedure ﬁrst obtains the prediction xˆk+1|k of the current state using the previous state and input estimates, with Σxk+1|k as the associated

state prediction error covariance matrix of xˆk+1|k. Then, the state
and input gain matrices Kxk+1 and Kuk , respectively, are computed along with the input estimation (with delay) covariance matrix Σuk . Finally, the state xˆk+1, input uˆk, and covariance matrix Σxk+1 are updated using current observation yk+1, and gain matrices Kxk+1 and Kuk . Note that the current observation yk+1 provides an estimate uˆk

of the input uk at the previous time step. The recursive equations of

the adversary’s forward EKF are [35]:

Prediction: xˆk+1|k = f (xˆk, uˆk−1),

(8)

Gain computation: Σxk+1|k = FkΣxkFTk + Q,

−1

Kxk+1 = Σxk+1|kHTk+1 Hk+1Σxk+1|kHTk+1 + R

,

Σuk = BTk HTk+1R−1(Ip×p − Hk+1Kxk+1)Hk+1Bk −1 ,

Kuk = Σuk BTk HTk+1R−1(Ip×p − Hk+1Kxk+1),

Update: xˆk+1 = xˆk+1|k + Kxk+1(yk+1 − h(xˆk+1|k)),

(9)

uˆk = Kuk (yk+1 − h(xˆk+1|k) + Hk+1Bkuˆk−1),

(10)

Covariance matrix update: Σxk+1

= (In×n − Kxk+1Hk+1) Σxk+1|k + BkΣuk BTk (In×n − Kxk+1Hk+1)T

Forward ﬁlter exists if rank(Σuk ) = m, for all k ≥ 0, and p ≥ m [35].
2) Inverse ﬁlter: Consider an augmented state vector zk = xˆTk uˆTk−2 T . ‘Our’ observation ak in (6) is the ﬁrst observation that contains the information about unknown input estimate uˆk−2, because of the delay in forward ﬁlter input estimate. Hence, the delayed estimate uˆk−2 is considered in the augmented state zk. Deﬁne

φk(xˆk, uˆk−1, xk+1, vk+1) = f (xˆk, uˆk−1) − Kxk+1h(f (xˆk, uˆk−1)) + Kxk+1h(xk+1) + Kxk+1vk+1.

Fvk =.

∇vk fk ∇vk hk

∇vk+1 fk 0m×p

with respect to the augmented process

noise vector; and Q = F v R

k

k 0p×p

0pR×p (Fvk)T . Then, the

recursive form of the I-EKF-without-DF yields the estimate zˆk of

the augmented state and the associated covariance matrix Σk as:

Prediction: xˆk+1|k = fk(xˆk, uˆk−2, xˆk−1, xk, xk+1, 0p×1, 0p×1),

uˆk−1|k = hk(uˆk−2, xˆk−1, xk, 0p×1),

zˆk+1|k = xˆTk+1|k

T
uˆTk−1|k ,

Σk+1|k = FzkΣk(Fzk)T + Qk, Update: Sk+1 = Gk+1Σk+1|kGTk+1 + Σ , zˆk+1 = zˆk+1|k + Σk+1|kGTk+1S−k+11 ak+1 − g(xˆk+1|k) , Σk+1 = Σk+1|k − Σk+1|kGTk+1S−k+11Gk+1Σk+1|k.

(13) (14) (15) (16)

The recursions of I-EKF-without-DF take the same form as that of the standard EKF [36] but with modiﬁed system matrices. In particular, the former employs an augmented state such that the Jacobian of the state transition function with respect to the state is computed as Fzk while for the latter, it is simply Fk =. ∇xf (x)|x=xˆk . Further, linearization Fvk of the state transition function with respect to the noise terms yields the process noise covariance matrix approximation Qk because of the non-additive noise terms, i.e. vk and vk+1 in (11) and (12). Note that, in the standard KF or EKF, the noise term is .additive.
The forward ﬁlter gains Kxk+1 and Kuk−1 are treated as timevarying parameters of the state transition equation and not as a function of the state and input estimates (xˆk and uˆk−1) in the inverse ﬁlter. The inverse ﬁlter approximates them by evaluating their values at its own estimates (xˆk and uˆk−1) recursively in the similar manner as the forward ﬁlter evaluates them using its own estimates. On the contrary, in I-KF formulation introduced in [9], the forward Kalman gain Kk+1 is deterministic, fully determined by the model parameters for a given initial covariance estimate Σ0, and computed ofﬂine independent of the current I-KF’s estimate.

B. I-EKF-with-DF unknown input

From (7)-(10), state transition equations of augmented state vector are xˆk+1 = fk(xˆk, uˆk−2, xˆk−1, xk, xk+1, vk, vk+1) and uˆk−1 = hk(uˆk−2, xˆk−1, xk, vk), where the state transition functions are

Consider the non-linear system with DF given by (4) and (5).

Linearize the functions as Fk =. ∇xf (x, uˆk)|x=xˆ , Hk+1 =.

∇xh(x, uˆk)|x=xˆ

and

Dk

=.

k
∇uh(xˆk+1|k, u)|u=uˆ

.

k+1|k

k

4

1) Forward ﬁlter: Denote the state and input estimation covariance and gain matrices identical to Section III-A. Here, the current observation yk+1 depends on the current unknown input uk+1 such

Linearize the functions as Fk =. ∇xf (x)|x=xˆ and Hk+1 =. k
∇xh(x)|x=xˆk+1|k . Then, ceteris paribus, setting Bk = 0n×p and neglecting computation of Σuk , Kuk and uˆk in forward EKF-without-

that the forward ﬁlter infers uˆk+1 without any delay. For input estimation covariance without delay, we use Σuk . Then, the recursive

DF yields forward EKF-without-unknown-input whose state prediction and updates are

form of forward EKF-with-DF is [37]

Prediction: xˆk+1|k = f (xˆk, uˆk), Σxk+1|k = FkΣxkFTk + Q,

(17)

xˆk+1|k = f (xˆk),

(21)

xˆk+1 = xˆk+1|k + Kk+1(yk+1 − h(xˆk+1|k)),

(22)

Gain computation: Kxk+1 = Σxk+1|kHTk+1(Hk+1Σxk+1|kHTk+1 + R)−1, Σuk+1 = DTk R−1(Ip×p − Hk+1Kxk+1)Dk −1 , Kuk+1 = Σuk+1DTk R−1(Ip×p − Hk+1Kxk+1),

with Kk+1 = Σk+1|kHTk+1 Hk+1Σk+1|kHTk+1 + R −1. Here, we have dropped the superscript in the covariance matrix Σxk+1|k and gain Kxk+1 to replace with Σk+1|k and Kk+1, respectively
(because only the state estimation covariances and gains are computed

Update: uˆk+1 = Kuk+1 yk+1 − h(xˆk+1|k, uˆk) + Dkuˆk ,

(18)

xˆk+1 = xˆk+1|k + Kxk+1 yk+1 − h(xˆk+1|k, uˆk) − Dk(uˆk+1 − uˆk) , (19)

Covariance matrix update: Σxk+1

here). Thence, the I-EKF-without-DF’s state transition equations and recursions yield I-EKF-without-unknown-input. Dropping the input estimate term in the augmented state zk, the state transition equations become

= (In×n + Kxk+1DkΣuk+1DTk R−1Hk+1)(In×n − Kxk+1Hk+1)Σxk+1|k. xˆk+1 = fk(xˆk, xk+1, vk+1)

The forward ﬁlter exists if rank(Dk) = m for all k ≥ 0, which implies p ≥ m [37].
2) Inverse ﬁlter: Consider an augmented state vector zk = xˆTk uˆTk T (note the absence of delay in the input estimate). Deﬁne
φk(xˆk, uˆk, uˆk+1, xk+1, uk+1, vk+1) = f (xˆk, uˆk) − Kxk+1h(f (xˆk, uˆk), uˆk) − Kxk+1Dk(uˆk+1 − uˆk)
+ Kxk+1h(xk+1, uk+1) + Kxk+1vk+1.
From (5) and (17)-(19), state transitions for inverse ﬁlter are xˆk+1 = fk(xˆk, uˆk, xk+1, uk+1, vk+1) and uˆk+1 = hk(xˆk, uˆk, xk+1, uk+1, vk+1), where

= f (xˆk) − Kk+1h(f (xˆk)) + Kk+1h(xk+1) + Kk+1vk+1. (23)

Denote Fxk =. ∇xfk(x, xk+1, 0p×1)| ˆ , Gk+1 =.

∇ g(x)

, Fv

=.

x=xˆk
∇ f (xˆ , x , v)|

, and

x

x=xˆˆk+1|k

k

v k k k+1

v=0p×1

Qk = FvkR(Fvk)T . Then, the recursive form of I-EKF is

similar to I-EKF-without-DF except that the I-EKF’s predicted

state estimate and the associated prediction covariance matrix are computed, respectively, as xˆk+1|k = fk(xˆk, xk+1, 0p×1) and Σk+1|k = FxkΣk(Fxk)T + Qk, followed by the update procedure in (14)-(16).

Unlike I-KF [9], the I-EKF approximates the forward gain Kk+1

online at its own estimates recursively and is sensitive to the initial

hk(xˆk, uˆk, xk+1, uk+1, vk+1)

estimate of forward EKF’s initial covariance matrix. The I-EKF could

= Kuk+1(h(xk+1, uk+1) + vk+1 − h(f (xˆk, uˆk), uˆk) + Dkuˆk) fk(xˆk, uˆk, xk+1, uk+1, vk+1) = φk(xˆk, uˆk, hk(xˆk, uˆk, xk+1, uk+1, vk+1), xk+1, uk+1, vk+1).

be applied in various non-linear target tracking applications, where EKF is a popular forward ﬁlter [38].
The two-step prediction-update formulation (as discussed for EKF and I-EKF so far) infers an estimate of the current state. However,

Then, ceteris paribus, following similar steps as in I-EKFwithout-DF, the I-EKF-with-DF estimate zˆk = xˆTk uˆTk T

from observations (6) is computed recursively. The predicted augmented state is zˆk+1|k = xˆTk+1|k uˆTk+1|k T ,

where xˆk+1|k

=

fk(xˆk, uˆk, xk+1, uk+1, 0p×1) and

uˆk+1|k = hk(xˆk, uˆk, xk+1, uk+1, 0p×1). Hereafter, the remaining

steps are as in (13)-(16). For I-EKF-with-DF, the Jacobians with

respect to the augmented state are Fz =.

∇ˆ fk xˆk

∇ˆ fk uˆ k

and

k

∇ˆ hk ∇ˆ hk

xˆk

uˆ k

Gk+1 =. ∇ˆ g 0na×m ; the Jacobian with respect to the

xˆk+1|k

process noise term is Fvk =.

∇vk+1 fk ; and Q = F vR(Fv )T .

∇v hk

k

k

k

k+1

Here, unlike I-EKF-without-DF, the inverse ﬁlter’s prediction

often for stability analyses, the one-step prediction formulation is

analytically more useful. This formulation is concerned with pre-

dicting the next state. In this formulation, the estimate xˆk is the

one-step prediction estimate i.e. an estimate of our state xk at k-th

instant given the observations {yj}1≤j≤k−1 up to time instant k − 1

with Σk as the corresponding prediction covariance matrix. Consider

the forward one-step prediction EKF formulation [26] for the same

system but with Fk =. ∇xf (x)|x=xˆ and Hk =. ∇xh(x)|x=xˆ as

k

k

Kk = FkΣkHTk (HkΣkHTk + R)−1,

(24)

xˆk+1 = f (xˆk) + Kk(yk − h(xˆk)),

(25)

Σk+1 = FkΣkFTk + Q − Kk(HkΣkHTk + R)KTk .

(26)

From (7) and (25), the state transition equation for one-step formulation of I-EKF is

dispenses with any approximation xˆk−1. The absence of delay in input estimation also results in a simpliﬁed process noise term vk+1, in place of the augmented noise vector of I-EKF-without-DF.
Examples of EKF with unknown inputs include fault detection with unknown excitations [37] and missile-target interception with unknown target acceleration [35]. The inverse cognition in these applications would then resort to the I-EKFs described until now.
C. I-EKF without any unknown inputs

xˆk+1 = fk(xˆk, xk, vk)

. =

f

(xˆ k )

−

Kk h(xˆ k )

+

Kk h(xk )

+

Kk vk .

Using this state transition equation, the I-EKF one-step prediction

formulation follows directly from EKF’s one-step prediction for-

mulation treating ak as the observation with the Jacobians with

respect to state estimate Fxk = ∇xfk(x, xk, 0)| ˆ = Fk − KkHk

x=xˆk

and Gk = ∇xg(x)| ˆ , and the process noise covariance matrix

x=xˆk

Qk

=

Kk

RK

T k

.

Consider a non-linear system model without unknown inputs in the system equations (4) and (7), i.e.,

xk+1 = f (xk) + wk.

(20)

IV. INVERSE KF WITH UNKNOWN INPUT
For linear Gaussian state-space models, our methods developed in the previous section are useful in extending the I-KF mentioned in [9]

5

to unknown input. Again, the forward KFs employed by the adversary with and without DF are conceptually different [39] because of the delay involved in input estimation. The forward KFs with unknown input provide unbiased minimum variance state and input estimates.

A. I-KF-without-DF

Consider the system in (1) and (2) with D = 0p×m.
1) Forward ﬁlter: Unlike EKF-without-DF, the forward KFwithout-DF considers an intermediate state update step using the estimated unknown input before the ﬁnal state updates. In this step, the unknown input is ﬁrst estimated (with one-step delay) using the current observation yk+1 and input estimation gain matrix Mk+1. In the update step, the current state estimate xˆk+1 is computed by again considering the current observation yk+1. The forward KF-withoutDF is [40]:

Prediction: xˆk+1|k = Fxˆk, Σk+1|k = FΣkFT + Q,

(27)

Unknown input estimation: Sk+1 = HΣk+1|kHT + R,

(28)

Mk+1 = (BT HT S−k+11HB)−1BT HT S−k+11,

(29)

uˆk = Mk+1(yk+1 − Hxˆk+1|k),

(30)

xk+1|k+1 = xˆk+1|k + Buˆk,

(31)

Σk+1|k+1 = (In×n − BMk+1H)Σk+1|k(In×n − BMk+1H)T (32)

+ BMk+1RMTk+1BT ,

Update: Kk+1 = Σk+1|kHT S−k+11,

(33)

xˆk+1 = xk+1|k+1 + Kk+1(yk+1 − Hxk+1|k+1),

(34)

Σk+1 = Σk+1|k+1 − Kk+1(Σk+1|k+1HT − BMk+1R)T .

(35)

The forward ﬁlter exists if rank(HB) = rank(B) = m which implies n ≥ m and p ≥ m such that uˆk is an unbiased minimum variance estimate of the unknown input [40]. Here, unlike I-EKFs, the gain matrices Kk+1 and Mk+1, are deterministic and completely determined by the model parameters and the initial covariance matrix similar to I-KF [9].
2) Inverse ﬁlter: Denote Fk = (In×n − Kk+1H)(In×n − BMk+1H)F and Ek = BMk+1 − Kk+1HBMk+1 + Kk+1. From (2) with D = 0p×m, and (27)-(34), the state transition equation for I-KF-without-DF is

xˆk+1 = Fkxˆk + EkHxk+1 + Ekvk+1.

(36)

Unlike the state transition (11) and (12) of I-EKF-without-DF, the state transition for I-KF-without-DF is not an explicit function of the forward ﬁlter input estimate and hence, an augmented state is not needed. The difference arises from the forward EKF-without-DF, where the current input estimate explicitly depends on the previous input estimates as observed in (10), which is not the case in KFwithout-DF. The recursive form of I-KF-without-DF with observation (3) is:

Prediction: xˆk+1|k = Fkxˆk + EkHxk+1, Σk+1|k = FkΣkFTk + Qk, (37)

Update: Sk+1 = GΣk+1|kGT + Σ ,

(38)

xˆk+1 = xˆk+1|k + Σk+1|kGT S−k+11(ak+1 − Gxˆk+1|k),

(39)

Σk+1 = Σk+1|k − Σk+1|kGT S−k+11GΣk+1|k,

(40)

where Qk = EkRETk is (inverse) process noise covariance matrix.

B. I-KF-with-DF Consider the linear system model with DF given by (1) and (2).

1) Forward ﬁlter: Denote the state estimation covariance, input
estimation (without delay) covariance, and cross-covariance of state and input estimates by Σxk, Σuk and Σxku, respectively. The forward KF-with-DF is [39]:

Prediction: xˆk+1|k = Fxˆk + Buˆk,

(41)

Σxk+1|k = F

B Σxk Σuk x

Σxku Σuk

FT BT + Q,

Gain computation: Sk+1 = HΣxk+1|kHT + R,

Mk+1 = (DT S−k+11D)−1DT S−k+11,

Kk+1 = Σxk+1|kHT S−k+11,

Update: uˆk+1 = Mk+1(yk+1 − Hxˆk+1|k),

(42)

xˆk+1 = xˆk+1|k + Kk+1(yk+1 − Hxˆk+1|k − Duˆk+1),

(43)

Covariance updates: Σuk+1 = (DT S−k+11D)−1,

Σxk+1 = Σxk+1|k − Kk+1(Sk+1 − DΣuk+1DT )KTk+1,

Σxk+u 1 = (Σuk+x 1)T = −Kk+1DΣuk+1.

The forward ﬁlter exists if rank(D) = m (which implies p ≥ m). Again, the gain matrices Kk+1 and Mk+1 are deterministic and completely determined by the system model and initial covariance
matrices estimates. 2) Inverse ﬁlter: Consider an augmented state vector zk =
xˆTk uˆTk T . Denote Fk = (In×n − Kk+1H + Kk+1DMk+1H)F, Bk = (In×n − Kk+1H + Kk+1DMk+1H)B, Ek = Kk+1(Ip×p − DMk+1), Hk = −Mk+1HF and Dk = −Mk+1HB. From (2), and (41)-(43), the state transition equations for I-KF-with-DF are

xˆk+1 = Fkxˆk + Bkuˆk + EkHxk+1 + EkDuk+1 + Ekvk+1, and

uˆk+1 = Hkxˆk + Dkuˆk + Mk+1Hxk+1 + Mk+1Duk+1 + Mk+1vk+1.

Also, (Ekvk+1)T (Mk+1vk+1)T T is the augmented noise vec-

tor involved in this state transition with noise covariance matrix Qk =

Ek RETk

Ek RMTk+1

Mk+1RETk Mk+1RMTk+1 . Then, ceteris paribus, following

similar steps as in I-KF-without-DF, the I-KF-with-DF computes the estimate zˆk = xˆTk uˆTk T of the augmented state vector using the

observation ak given by (3). The system matrices for the augmented

state are Fz = Fk Bk and G = G 0n ×m . The I-KF-

k

Hk Dk

a

with-DF predicts the augmented state as

xˆk+1|k = Fkxˆk + Bkuˆk + EkHxk+1 + EkDuk+1,

uˆk+1|k = Hkxˆk + Dkuˆk + Mk+1Hxk+1 + Mk+1Duk+1,

zˆk+1|k = xˆTk+1|k

T
uˆTk+1|k , Σk+1|k = FzkΣk(Fzk)T + Qk,

followed by the update procedure (38)-(40) with G and xˆk+1 replaced by G and zˆk+1, respectively.
Since the observation yk explicitly depends on the unknown input uk for a system with DF, I-KF-with-DF and I-EKF-with-DF require perfect knowledge of the current input uk as a known exogenous input to obtain their state and input estimates, which is not the case in I-KF-without-DF and I-EKF-without-DF.

V. EXTENSIONS TO INVERSES OF EKF VARIANTS
Several advanced versions of EKF exist, each aiming toward improving either the estimation accuracy (SOEKF or higher-order, GS-EKF), convergence (iterated EKF), stability (DEKF) or practical feasibility (hybrid EKF). In applications such as parameter estimation, EKF may not necessarily converge to true parameter values [41] unless some corrective terms are added. The iterated EKF, in which the correction equation is iterated a ﬁxed number of times, was proposed to improve the convergence properties of the EKF [42]. The

6

hybrid EKF is more practical because it employs continuous-time dynamics and discrete-time measurements [15]. The literature also reports grid-based EKF [43] that is a pre-cursor of particle ﬁlter. A comparison of nonlinear KFs is available in [44, 45]. In the following, we extend our theory to derive the inverses of some common EKF variants such as SOEKF, GS-EKF, and DEKF. We consider the nonlinear system model without unknown inputs as in (20), (7) and (6). We also derive the one-step prediction formulation of SOEKF and its inverse.

Here, besides the gain matrix Kk+1, the Hessian terms are also

treated as time-varying parameters of the state transition and approx-

imated by evaluating them using the previous I-SOEKF’s estimate xˆk. When the second-order terms are neglected, the forward SOEKF

and I-SOEKF reduce to, respectively, forward EKF and I-EKF.

3) One-step prediction formulation: The one-step prediction for-

mulation of SOEKF for the considered system model can be de-

rived following the derivation of two-step recursion formulation

of SOEKF as outlined in [46]. Denote Fk =. ∇xf (x)|x=xˆ and

.

k

Hk = ∇xh(x)|x=xˆk , the recursive equations obtained are

A. Inverse SOEKF

In SOEKF, the forward and invese ﬁlters consider second-order

terms in Taylor series expansion. Linearize the functions as Fk =.

∇xf (x)|x=xˆ and Hk+1 =. ∇xh(x)|x=xˆ

.

k

k+1|k

1) Forward ﬁlter: Denoting the i-th Euclidean basis vectors in Rn×1 and Rp×1, by ai and bi, respectively, the forward SOEKF is

[46]

n
Prediction: xˆk+1|k = f (xˆk) + 12 aiTr ∇2 [f (xˆk)]i Σk , (44)
i=1

Σk+1|k = FkΣkFTk + Q

nn

1 +

aiaTj Tr ∇2 [f (xˆk)]i Σk∇2 [f (xˆk)]j Σk ,

2 i=1 j=1

Update:
p
1 yˆk+1|k = h(xˆk+1|k) + 2 biTr
i=1

∇2 h(xˆk+1|k) i Σk+1|k

, (45)

n

1 xk = f (xˆk) + 2

aiTr ∇2 [f (xˆk)]i Σk ,

i=1

Σk = FkΣkFTk + Q
nn
+ 12 aiaTj Tr
i=1 j=1

∇2 [f (xˆk)]i Σk∇2 [f (xˆk)]j Σk

Sk = HkΣkHTk + R
pp
+ 12 bibTj Tr
i=1 j=1

∇2 [h(xˆk)]i Σk∇2 [h(xˆk)]j Σk

(47) , (48) , (49)

np

1 Mk =

aibTj Tr ∇2 [f (xˆk)]i Σk∇2 [h(xˆk)]j Σk , (50)

2 i=1 j=1

Kk = (FkΣkHTk + Mk)S−k 1,

(51)

p

xˆk+1 = xk + Kk

1 yk − h(xˆk) − 2

biTr ∇2 [h(xˆk)]i Σk ,

i=1

(52)

Sk+1 = Hk+1Σk+1|kHTk+1 + R

Σk+1 = Σk − KkSkKTk .

(53)

pp
1

From (7), (47) and (52), the state transition equation for one-step

+ 2

bibTj Tr ∇2 h(xˆk+1|k) i Σk+1|k∇2 h(xˆk+1|k) j Σk+1|k , formulation of I-SOEKF is

i=1 j=1

.

xˆk+1 = xˆk+1|k + Σk+1|kHTk+1S−k+11(yk+1 − yˆk+1|k),

(46)

xˆk+1 = f k(xˆk, xk, vk)

n

Σk+1 = Σk+1|k − Σk+1|kHTk+1S−k+11Hk+1Σk+1|k,

1 = f (xˆk) − Kkh(xˆk) + 2

aiTr ∇2 [f (xˆk)]i Σk

where yˆk+1|k is the predicted observation. Unlike EKF, the SOEKF

i=1 p

includes Hessian terms in the prediction equations.

1 − Kk

biTr ∇2 [h(xˆk)] Σk + Kkh(xk) + Kkvk.

2) Inverse ﬁlter: Deﬁne f (xˆ, Σ)

=

f (xˆ) +

2

i

i=1

12 ni=1 aiTr ∇2 [f (xˆ)]i Σ and h(xˆ, Σ) = h(xˆ) + (54)

1 2

p i=1

biTr

∇2 [h(xˆ)]i Σ

.

Using

(7)

and

(44)-(46),

the

state

Again, the inverse ﬁlter follows directly from SOEKF’s one-step

transition equation for the inverse SOEKF (I-SOEKF) is
xˆk+1 = f k(xˆk, xk+1, vk+1) = f (xˆk, Σk) − Kk+1h(f (xˆk, Σk), Σk+1|k) + Kk+1h(xk+1) + Kk+1vk+1.

prediction formulation considering this state transition. The Jacobians

with respect to the state estimate are Fk =. ∇xf (x, xk, 0)| ˆ =

.

k

x=xˆk

Fk − KkHk and Gk = ∇xg(x)| ˆ , with the process noise

x=xˆk

covariance matrix Qk = KkRkKTk .

where Kk+1 = Σk+1|kHTk+1S−k+11 is SOEKF’s gain matrix.

B. Inverse GS-EKF

The I-SOEKF recursions now follow directly from forward SOEKF’s recursions treating ak as the observation. Denote the Jacobians and the Hessian with respect to the state estimate xˆk as Fk =. ∇xf (x, xk+1, 0)| ˆ , Gk+1 =. ∇xg(x)| ˆ and ∇2 f k(xˆk)ki =. ∇2x f kx(=xxˆ,kxk+1, 0) i |x=xˆˆk , and xt=hexˆk+p1ro|kcess noise covariance matrix as Qk = VkRVTk with Vk =. ∇vf k(xˆk, xk+1, v)|v=0. The I-SOEKF’s prediction is

The GS-EKF [36] assumes the posterior distribution of the state

estimate to be a weighted sum of Gaussian densities, which are

updated recursively based on the current observation.

1) Forward ﬁlter: In the forward GS-EKF, we consider l Gaus-

sians denoting the i-th Gaussian probability density function, with

mean xi,k and covariance Σi,k at k-th time-step, as γ[x −

xi, Σi]

=

(2π

)−n/2

|Σi

|−1/2

e{−

1 2

(x−xi

)T

Σ− i 1

(x−xi

)}

.

Given

ob-

n servations Yk = {yj}1≤j≤k up to k-th time instant, the pos-

Prediction: xˆk+1|k = f k(xˆk, xk+1, 0) + 1 aiTr ∇2 f k(xˆk) Σk , terior distribution of state xk, is approximated as p(xk|Yk) =

2 i=1

i

l i=1

ci,k γ [xk

− xi,k, Σi,k],

where

ci,k

is

i-th

Gaussian’s .

weight

at

Σk+1|k = FkΣkFTk + Qk

k-th

time

step. .

Linearizing

the

functions

as

Fi,k

=

∇xf (x)|x=xi,k

nn

and Hi,k+1 = ∇xh(x)|x=xi,k+1|k for the i-th Gaussian, the means

1 +

aiaTj Tr ∇2 f k(xˆk) Σk∇2 f k(xˆk) Σk .

{xi,k}1≤i≤l and covariance matrices {Σi,k}1≤i≤l are updated based

2 i=1 j=1

i

j

on the current observation yk+1 using independent EKF recursions

7

for each Gaussian. Finally, the weights {ci,k}1≤i≤l are updated as [36]:

ci,k+1 =

ci,kγ[yk+1 − h(xi,k+1|k), Si,k+1] ,

l i

=1

ci

,k γ [yk+1

−

h(xi

,k+1|k ),

Si

,k+1 ]

where xi,k+1|k is the i-th Gaussian’s predicted mean and Si,k+1 = Hi,k+1Σi,k+1|kHTi,k+1 + R with Σi,k+1|k as the

prediction covariance matrix of xi,k+1|k. With these updated

Gaussians, the point-estimate xˆk and the associated covariance

matrix Σk are xˆk+1 =

l i=1

ci,k+1 xi,k+1

and

Σk+1

=

l i=1

ci,k+1

Σi,k+1 + (xˆk+1 − xi,k+1)(xˆk+1 − xi,k+1)T

.

2) Inverse ﬁlter: Consider an augmented state vector zk =

{{xi,k}1≤i≤l, {ci,k}1≤i≤l} (means and weights of forward GS-

EKF). Then, substituting for observation yk+1 from (7) in the forward

ﬁlter’s updates similar to various inverse ﬁlters earlier and denoting the i-th EKF’s gain matrix as Ki,k+1 = Σi,k+1|kHTi,k+1S−i,k1+1 yields the state transition equations for inverse GS-EKF (I-GS-EKF)

as

xi,k+1 = f (xi,k) + Ki,k+1 h(xk+1) + vk+1 − h(f (xi,k)) ,

ci,k+1 =

ci,kγ[h(xk+1) + vk+1 − h(f (xi,k)), Si,k+1] .

l i

=1

ci

,k γ [h(xk+1 )

+

vk+1

−

h(f (xi

,k )),

Si

,k+1 ]

(55)

Treating {Ki,k+1, Si,k+1}1≤i≤l as time-varying parameters of the

state transition equations, which are approximated in a similar way as

we approximated the gain matrices for various inverse ﬁlters earlier,

the overall state transition in terms of the augmented state is zk+1 =

f k(zk, xk+1, vk+1). Similarly, the observation ak as a function of

augmented state zk is ak = g(zk) + k = g

l i=1

ci,k xi,k

+ k.

The I-GS-EKF approximates the posterior distribution of the

augmented state as a sum of l Gaussians with its recursions again

following directly from forward GS-EKF’s recursions treating ak as

the observation. However, the inverse ﬁlter estimates an l(n + 1)-

dimensional augmented state zk with the Jacobians with respect

to the state denoted as Fj,k =. ∇zf (z, xk+1, 0)|z=z and

.

k

j,k

Gj,k+1 = ∇zg(z)|z=zj,k+1|k , and the process noise covariance

matrix as Qk = Vj,kRVTj,k with Vj,k =. ∇vf k(zj,k, xk+1, v)|v=0

for the j-th inverse ﬁlter’s Gaussian updates. The point estimate zˆk

consists of the estimates {xˆi,k, cˆi,k}1≤i≤l of the forward GS-EKF’s

means and weights such that the point estimate xˆk of the forward

ﬁlter’s estimate xˆk is xˆk =

l i=1

cˆi,k xˆ i,k .

When the forward ﬁlter considers only one Gaussian (l = 1),

the forward GS-EKF reduces to forward EKF with the only weight

c1,k = 1 for all k. Hence, this weight need not be considered in

the augmented state and zk reduces to x1,k which is the estimate

xˆk itself. Similarly, I-GS-EKF also reduces to I-EKF if only one

Gaussian is considered (l = 1).

C. Inverse DEKF
Consider the adversary employing DEKF [32] as its forward ﬁlter. In DEKF, the output non-linearities are modiﬁed using dither signals so as to tighten the cone-bounds. Dithering tightens this cone such that the non-linearities are smoothened but it may also degrade the near-optimal performance of the EKF after the initial transient phase of estimation. Therefore, dithering is introduced only during the initial transient phase with the aim to improve the ﬁlter’s transient performance and avoid divergence. Denote the dither amplitude which controls the tightness of the cone-bounds by d and its amplitude probability density function by p(a). The observation function h(x) is dithered as h∗(x) = −dd h(a + x)p(a)da. If d = d0e−k/τ , where d0 and τ are constants and ‘k’ denotes the time index, then h∗(x) → h(x) exponentially as k → ∞ during the transient phase.

The forward DEKF follows from conventional forward EKF of Section III-C by replacing h(·) with h∗(·) as the observation function of yk during the initial transient phase and hence, the inverse DEKF (I-DEKF) also follows from I-EKF of Section III-C. The dither of the adversary’s ﬁlter is assumed to be known to us or may be estimated separately. Otherwise, the I-DEKF may also proceed with the unmodiﬁed observation function. We show in Section VII-D that these two formulations, labeled I-DEKF-1 and I-DEKF-2, respectively, generally vary in their estimation performances.

VI. STABILITY ANALYSES
For continuous-time nonlinear Kalman ﬁltering, some convergence results were mentioned in [47]. In case of EKF, sufﬁcient conditions for stability for non-linear systems with linear output map were described in [48]. The asymptotic convergence of EKF for a special class of systems, where EKF is applied for joint state and parameter estimation of linear stochastic systems, was studied in [41, 49]. If the nonlinearities have known bounds, then the Riccati equation is slightly modiﬁed to guarantee stability for the continuous-time EKF [50].
To derive the sufﬁcient conditions for stochastic stability of nonlinear ﬁlters, one of the common approaches is to introduce unknown instrumental matrices to account for the linearization errors [28]. It does not assume any bound on the estimation error, but its sufﬁcient conditions for stability, especially the bounds assumed on the unknown matrices, are difﬁcult to verify for practical systems.
Alternatively, [26] considers the one-step prediction formulation of the ﬁlter and provides sufﬁcient conditions under which the state prediction error is exponentially bounded in mean-squared sense. We restate some deﬁnitions and a useful Lemma from [26].

Deﬁnition 1 (Exponential mean-squared boundedness [26]). A

stochastic process {ζk}k≥0 is deﬁned to be exponentially bounded

in mean-squared sense if there are real numbers η, ν > 0 and

0 < λ < 1 such that E

ζk

2 2

≤ ηE

ζ0

2 2

λk + ν

holds for

every k ≥ 0.

Deﬁnition 2 (Boundedness with probability one [26]). A stochastic process {ζk}k≥0 is deﬁned to be bounded with probability one if supk≥0 ζk 2 < ∞ holds with probability one.

Lemma 1 (Boundedness of stochastic process [26, Lemma 2.1]). Consider a function Vk(ζk) of the stochastic process ζk and real numbers vmin, vmax, µ > 0, and 0 < λ ≤ 1 such that for all k ≥ 0

vmin

ζk

2 2

≤

Vk (ζk )

≤

vmax

ζk

22 ,

and

E [Vk+1(ζk+1)|ζk] − Vk(ζk) ≤ µ − λVk(ζk).

Then, the stochastic process {ζk}k≥0 is exponentially bounded in mean-squared sense, i.e.,

k−1

E

ζk

2 2

≤ vmax E

ζ0 22 (1 − λ)k + µ

(1 − λ)i,

vmin

vmin i=1

for every k ≥ 0. Further, {ζk}k≥0 is also bounded with probability one.

In the bounded mean-squared sense, [26, Sec. III] showed that, while the two-step prediction and update recursion (described in previous sections) and one-step formulation of (forward) ﬁlters may differ in their performance and transient behaviour, they have same convergence properties. However, the conditions of Lemma 1 were proved to hold when the error remained within suitable bounds; the guarantees fail if the error exceeds this bound at any instant. However,

8

it was numerically shown [26, Sec. V] that the bound on the error was only of theoretical interest and, in practice, the ﬁlter remained stable for much larger estimation errors.
In the following, we ﬁrst derive stability conditions for I-KFwithout-DF in which we rely on the stability of the forward KFwithout-DF as proved in [51]. The procedure is similar for the stability of I-KF-with-DF and I-KF-without-unknown-input [9] and hence, we omit the details for these ﬁlters. For I-EKF stability, we employ both unknown matrix and bounded non-linearity approaches. In the process, we also derive the forward EKF stability conditions using unknown matrix approach; note that the same was obtained using bounded non-linearity method in [26]. It is possible to extend these results to a general class of Gaussian ﬁlters [29], of which EKF is a special case, whose estimation error dynamics are represented by [52, eq. (2)]. Finally, we obtain stability conditions of forward and inverse SOEKF in the bounded mean-squared sense.

A. I-KF-with-unknown-input
Consider I-KF-without-DF of Section IV-A, where the forward ﬁlter is asymptotically stable under the sufﬁcient conditions provided by [51]. The following Theorem 1 states conditions for stability of the inverse ﬁlter.
Theorem 1 (Stability of I-KF-without-DF). Consider an asymptotically stable forward KF-without-DF (27)-(35) such that the gain matrices Mk and Kk asymptotically approach to limiting gain matrices M and K, respectively. The measurement noise covariance matrix Σ is positive deﬁnite (p.d.). Denote the limiting matrices F = (I − KH)(I − BMH)F and Q = ERET , where E = BM − KHBM + K. Then, the I-KF-without-DF (37)-(40) is asymptotically stable under the assumption that pair (F,G) is observable and the pair (F,C) is controllable for the system given by (3) and (36), where C is such that Q = CT C.
Proof: See Appendix A. Note that, for stability of I-KF-with-DF, the stability conditions of basic KF need to hold true for the augmented state considered in inverse ﬁlter formulation of Section IV-B. For the stability conditions of forward KF-with-DF, we refer the reader to [51].

B. I-EKF-without-unknown-input: Unknown matrix approach
Consider the two-step prediction and update formulation of I-EKF of Section III-C, with EKF-without-unknown-input as the forward ﬁlter.
1) Forward EKF stability: Denote the forward EKF’s state prediction, state estimation and measurement prediction errors by xk+1|k =. xk+1 − xˆk+1|k, xk =. xk − xˆk and yk =. yk − yˆk, with yˆk = h(xˆk|k−1), respectively. Using (20), (21) and the Taylor series expansion of f (·) at xˆk, we get
xk+1|k = Fk(xk − xˆk) + wk + O( xk − xˆk 22) ≈ Fkxk + wk.

We consider the general case of time-varying process and measurement noise covariances and denote Q, R and Σ by Qk, Rk and Rk, respectively.
To account for the residuals and obtain an exact equality, we introduce an unknown instrumental diagonal matrix Uxk ∈ Rn×n [28, 53] as

xk+1|k = Uxk Fkxk + wk.

(56)

However, using (22), we have xk = xk|k−1 − Kkyk, which when substituted in (56) yields

xk+1|k = Uxk Fkxk|k−1 − Uxk FkKkyk + wk.

Similarly, using Taylor series expansion of h(·) at xˆk+1|k in (7) and introducing an unknown diagonal matrix Uyk+1 ∈ Rp×p gives
yk+1 = Uyk+1Hk+1xk+1|k + vk+1.

The prediction error dynamics of the forward EKF becomes

xk+1|k = Uxk Fk(I − KkUykHk)xk|k−1 − Uxk FkKkvk + wk. (57)

Denote the true prediction covariance by Pk+1|k = E xk+1|kxTk+1|k . Deﬁne δPk+1|k as the difference of estimated prediction covariance Σk+1|k and the
true prediction covariance Pk+1|k while ∆Pk+1|k
as the error in the approximation of the expectation E UxkFk(I − KkUykHk)xk|k−1xTk|k−1(I − KkUykHk)T FTk Uxk by UxkFk(I − KkUykHk)Σk|k−1(I − KkUykHk)T FTk Uxk. Denoting Qˆ k = Qk + UxkFkKkRkKTk FTk Uxk + δPk+1|k + ∆Pk+1|k and
following similar steps as in [28, 53], we have

Σk+1|k = Uxk Fk(I − KkUykHk)Σk|k−1(I − KkUykHk)T FTk Uxk + Qˆ k.

Similarly, denoting the true measurement prediction covariance and true cross-covariance by Pyky+1 and Pxk+y 1, respectively, we obtain

Sk+1 = Uyk+1Hk+1Σk+1|kHTk+1Uyk+1 + Rˆ k+1,

Σxk+y 1 =

Σk+1|kUxk+y 1HTk+1Uyk+1, Σk+1|kHTk+1Uyk+1Uxk+y 1,

n≥p ,
n<p

where Rˆ k+1 = Rk+1 + ∆Pyky+1 + δPyky+1 and Uxk+y 1 is an unknown
instrumental matrix introduced to account for errors in the estimated cross-covariance Σxk+y 1 [52].
The following Theorem 2 provides stability conditions for the forward EKF using the unknown matrices Uxk, Uyk and Uxky.

Theorem 2 (Stochastic stability of forward EKF). Consider the nonlinear stochastic system in (20) and (7). The two-step forward EKF formulation is as in Section III-C. Let the following assumptions hold true:
1) There exist positive real numbers f , h, α, β, γ, σ, σ, q, r, qˆ and rˆ such that the following bounds are fulﬁlled for all k ≥ 0.

Fk Uxky
rˆI

≤ f, ≤ γ, Rˆ k,

Hk ≤ h, Uxk ≤ α, Qk qI, Rk rI, σI Σk|k−1 σI.

Uyk ≤ β, qˆI Qˆ k,

2) Uxk and Fk are non-singular for every k ≥ 0. Then, the prediction error xk|k−1 and the estimation error xk of the forward EKF are exponentially bounded in mean-squared sense and bounded with probability one provided that the constants satisfy the inequality

σγh2β2 < rˆ.

(58)

Proof: See Appendix B.

2) Inverse EKF stability: For a stable forward EKF in the previous

subsection, we prove the stochastic stability of the I-EKF as an

extension of Theorem 2. Similar to the forward EKF, we introduce

x

a

unknown matrices Uk and Uk to account for the errors in the

xa

linearization of functions fk(·) and g(·), respectively, and Uk for

the errors in cross-covariance matrix estimation. Similarly, denote Qˆ k

and Rˆ k as the counterparts of Qˆ k and Rˆ k, respectively, in the I-EKF

dynamics. The following Theorem 3 states the stability criteria for

I-EKF. Note that, when compared to Theorem 2, the following result

requires an additional condition rI Rk for all k ≥ 0 for some

r > 0.

9

Theorem 3 (Stochastic stability of I-EKF). Consider the adversary’s forward EKF that is stable as per Theorem 2. Additionally, assume that the following hold true for all k ≥ 0.

rI Rk, Rk I,

Gk ≤ g, cˆI Qˆ k,

Uak ≤ c, dˆI Rˆ k,

Uxka ≤ d, pI Σk|k−1 pI,

for some real positive constants r, g, c, d, , cˆ, dˆ, p, p. Then, the state
estimation error of I-EKF is exponentially bounded in mean-squared
sense and bounded with probability one provided that the constants satisfy the inequality pdg2c2 < dˆ.

Proof: See Appendix C. Note that Theorem 2 requires both Qˆ k and Rˆ k to be p.d. In general, the difference matrices ∆Pk+1|k, δPk+1|k, ∆Pyky+1 and δPyky+1 may not be p.d. One could enhance the stability of EKF by enlarging the noise covariance matrices by adding sufﬁciently large
∆Qk and ∆Rk to Qk and Rk, respectively [28, 52]. The same
argument also holds true for I-EKF noise covariance matrices.

C. I-EKF-without-unknown-input: Bounded non-linearity method
Consider the forward EKF’s one step prediction formulation (24)(26). Using Taylor series expansion around the estimate xˆk, we have

f (xk) − f (xˆk) = Fk(xk − xˆk) + φ(xk, xˆk), h(xk) − h(xˆk) = Hk(xk − xˆk) + χ(xk, xˆk),

where φ(·) and χ(·) are suitable non-linear functions to account for the higher-order terms of the expansions. Denoting the estimation error by ek =. xk − xˆk, the error dynamics of the forward ﬁlter is

ek+1 = (Fk − KkHk)ek + rk + sk,

(59)

where rk = φ(xk, xˆk) − Kkχ(xk, xˆk) and sk = wk − Kkvk. The following Theorem 4 (reproduced from [26]) provides sufﬁ-
cient conditions for the stochastic stability of forward EKF.

Theorem 4 (Exponential boundedness of forward EKF’s error [26]). Consider a non-linear stochastic system deﬁned by (20) and (7), and the one-step prediction formulation of forward EKF (24)-(26). Let the following assumptions hold true.
1) There exist positive real numbers f ,h,σ,σ,q,r, δ such that the following bounds are fulﬁlled for all k ≥ 0.

σI Σk σI, rI Rk δI,

qI Qk Fk ≤ f ,

δI, Hk ≤ h.

2) Fk is non singular for every k ≥ 0. 3) There exist positive real numbers κφ, φ, κχ, χ such that the
non-linear functions φ(·) and χ(·) satisfy

φ(x, xˆ) 2 ≤ κφ

x − xˆ

2 2

for

χ(x, xˆ) 2 ≤ κχ

x − xˆ

2 2

for

x − xˆ 2 ≤ φ, x − xˆ 2 ≤ χ.

Then the estimation error given by (59) is exponentially bounded in mean-squared sense and bounded with probability one provided that the estimation error is bounded by suitable constant > 0.

Theorem 4 guarantees that the estimation error remains exponentially bounded in mean-squared sense as long as the error is within suitable bounds. Further, the mean drift E[Vk+1(ek+1)|ek]−Vk(ek) for a suitably deﬁned Vk(·) (for application of Lemma 1) is negative when ≤ ek 2 ≤ , which drives the system towards zero error in an expected sense. However, with some ﬁnite probability, the estimation error at some time-steps may be outside the bound. In this case, we cannot guarantee with probability one that the error will be within bound again at some future time-steps. As mentioned earlier, bounded non-linearity approach may not provide theoretical

guarantees for the ﬁlter to be stable for all time-steps but, practically, the ﬁlter remains stable even if the estimation error is outside the bound provided that the assumed bounds on the system model are satisﬁed.
For the inverse ﬁlter observations (6), the Taylor series expansion of g(·) at estimate xˆk of I-EKF’s one step prediction formulation of Section III-C, considering suitable non-linear function χ(·) is

g(xˆk) − g(xˆk) = Gk(xˆk − xˆk) + χ(xˆk, xˆk).

Finally, the error dynamics of the inverse ﬁlter, with the estimation error denoted by ek =. xˆk − xˆk and the inverse ﬁlter’s Kalman gain and estimation error covariance matrix by Kk and Σk, respectively, is

ek+1 = (Fxk − KkGk)ek + rk + sk,

(60)

where rk = φk(xˆk, xˆk) − Kkχ(xˆk, xˆk) and sk = Kkvk − Kk k with φk(xˆk, xˆk) = φ(xˆk, xˆk) − Kkχ(xˆk, xˆk).
The following Theorem 5 guarantees the stability of I-EKF. Note the additional assumption of Hk to be full column rank for all k ≥ 0, which implies p ≥ n.

Theorem 5 (Exponential boundedness of I-EKF’s error). Consider the adversary’s forward one-step prediction EKF that is stable as per Theorem 4. Additionally, assume that the following hold true.
1) There exist positive real numbers g, m, m, , , δ such that the following bounds are fulﬁlled for all k ≥ 0.

Gk ≤ g, mI Σk mI, I Rk δI.

2) Hk is full column rank for every k ≥ 0. 3) There exist positive real numbers κχ¯ and χ¯ such that the non-
linear function χ(·) satisﬁes

χ(xˆ, xˆ) 2 ≤ κχ¯

xˆ − xˆ

2 2

for

xˆ − xˆ 2 ≤ χ¯.

Then, the estimation error for I-EKF given by (60) is exponentially bounded in mean-squared sense and bounded with probability one provided that the estimation error is bounded by suitable constant
> 0.

Proof: See Appendix D.

D. I-SOEKF
The error dynamics of SOEKF cannot be expressed in a linear form [52, eq. (2)] for application of unknown matrix approach because of second-order terms. Therefore, we derive stability conditions using the bounded non-linearity approach here.
1) Forward SOEKF stability: Consider the one-step SOEKF’s formulation (47)-(53). Considering second-order terms as well, the Taylor series expansion of functions f (·) and h(·) at the estimate xˆk are

f (xk) − f (xˆk) = Fk(xk − xˆk)
n
+ 12 ai(xk − xˆk)T ∇2 [f (xˆk)]i (xk − xˆk) + φ(xk, xˆk),
i=1
h(xk) − h(xˆk) = Hk(xk − xˆk)
p
+ 21 bi(xk − xˆk)T ∇2 [h(xˆk)]i (xk − xˆk) + χ(xk, xˆk),
i=1
where φ(·) and χ(·) are suitable non-linear functions to account for third and higher-order terms in the expansions. Using these expansions, the error dynamics of the forward ﬁlter with ek =. xk − xˆk is

ek+1 = (Fk − KkHk)ek + rk + qk + sk,

(61)

10

where

rk = φ(xk, xˆk) − Kkχ(xk, xˆk),

n

n

1 qk = 2

aieTk ∇2 [f (xˆk)]i ek − 12

aiTr ∇2 [f (xˆk)]i Σk

i=1

i=1

p

p

1 − 2 Kk

bieTk ∇2 [h(xˆk)]i ek + 12 Kk

biTr ∇2 [h(xˆk)]i Σk ,

i=1

i=1

sk = wk − Kkvk.

The following Theorem 6 provides sufﬁcient conditions for the stochastic stability of forward SOEKF.

Theorem 6 (Exponential boundedness of forward SOEKF’s error). Consider the non-linear stochastic system deﬁned by (20) and (7), and SOEKF’s one-step prediction formulation (47)-(53). Let the following assumptions hold true.
1) There exist positive real numbers f , h, σ, σ, q, r, a, b, δ and real numbers a, b (not necessarily positive) such that the following bounds are satisﬁed for all k ≥ 0.

σI Σk σI, Fk ≤ f , Hk ≤ h,
rI Rk δI, qI Qk δI, aI ∇2 [f (xˆk)]i aI ∀i ∈ {1, 2, . . . , n}, bI ∇2 [h(xˆk)]j bI ∀j ∈ {1, 2, . . . , p}.

2) Fk is non-singular and F−k 1 satisﬁes the following bound for all k ≥ 0 for some positive real number f ,

F−k 1 ≤ f .

3) There exist positive real numbers κφ, φ, κχ, χ such that the non-linear functions φ(·) and χ(·) satisfy

φ(x, xˆ)

2 ≤ κφ

x − xˆ

3 2

f or

χ(x, xˆ)

2 ≤ κχ

x − xˆ

3 2

f or

x − xˆ 2 ≤ φ, x − xˆ 2 ≤ χ.

rk = φk(xˆk, xˆk) − Kkχ(xˆk, xˆk),

n

n

1 qk = 2

aieTk ∇2 f k(xˆk) i ek − 12

aiTr ∇2 f k(xˆk) i Σk

i=1

i=1

1

na

1

na

− 2 Kk dieTk ∇2 g(xˆk) i ek + 2 Kk diTr ∇2 g(xˆk) i Σk ,

i=1

i=1

sk = Kkvk − Kk k,

with φk(xˆk, xˆk) = φ(xˆk, xˆk) − Kkχ(xˆk, xˆk) and

1 2

n i=1

aieTk

∇2

f k(xˆk) i ek

=

1 2

n i=1

aieTk

∇2

f (xˆk) i ek −

12 Kk

p i=1

bieTk

∇2

h(xˆk) i ek.

Here,

the

error

in

approximations of the terms

1 2

n i=1

aiTr

∇2 [f (xˆk)]i Σk

and 12 Kk

p i=1

biTr

∇2 [h(xˆk)]i Σk

by I-SOEKF (as mentioned

in Section V-A) are neglected. Also, using the bounds assumed

in Theorem 6, these approximation errors are bounded by positive

constants.

The following Theorem 7 states the conditions for stability of I-

SOEKF.

Theorem 7 (Exponential boundedness of I-SOEKF’s error). Consider the adversary’s forward SOEKF’s one-step prediction formulation that is stable as per Theorem 6. Additionally, assume that the following hold true.
1) There exist positive real numbers g, m, m, , c, δ and a real number c (not necessarily positive) such that the following bounds are fulﬁlled for all k ≥ 0.

Gk ≤ g, mI Σk cI ∇2 g(xˆk) i cI

mI, I Rk δI, ∀i ∈ {1, 2, . . . , na}.

2) HkΣkFTk + MTk is full column rank matrix for every k ≥ 0. 3) There exist positive real numbers κχ¯, χ¯ such that the non-linear
function χ(·) satisﬁes

χ(xˆ, xˆ)

2 ≤ κχ¯

xˆ − xˆ

3 2

f or

xˆ − xˆ 2 ≤ χ¯.

Then the estimation error given by (61) is exponentially bounded in mean-squared sense if the estimation error is within bound for a suitable constant > 0,

2r

f < habσ2n√np ,

(62)

Then, the estimation error of I-SOEKF given by (65) is exponentially bounded in mean-squared sense if the estimation error is bounded by a suitable constant > 0 and the bound constants also satisfy the equivalent conditions of (62), (63), and (64) for the inverse ﬁlter dynamics.

q > c,

(63)

Proof: See Appendix F.

and

1 α2

δ=

− csec ,

(64)

κnoise 2σ

for some < , where c, α, κnoise and csec are constants that depend on the bounds assumed on the system.

VII. NUMERICAL EXPERIMENTS
We illustrate the performance of proposed I-EKF considering two different example systems. The efﬁcacy of I-EKF is demonstrated by comparing the estimation error with RCRLB. The CRLB provides

Proof: See Appendix E.
2) Inverse SOEKF stability: Considering a suitable non-linear function χ(·), the Taylor series expansion of g(·) at estimate xˆk

a lower bound on mean-squared error (MSE) and is widely used to assess the performance of an estimator. For the discrete-time nonlinear ﬁltering, we employ the RCRLB [33]

of I-SOEKF’s one-step prediction formulation is

E (xk − xˆk)(xk − xˆk)T J−k 1,

g(xˆk) − g(xˆk) = Gk(xˆk − xˆk)

na

where Jk is the Fisher information matrix

1 +

di(xˆk − xˆk)T ∇2 g(xˆk) (xˆk − xˆk) + χ(xˆk, xˆk),

2

i

i=1

∂2 ln p(Y k, Xk)

Jk = E −

.

∂x2k

where di is the i-th Euclidean basis vector in Rna×1. Finally, the

error dynamics of the inverse ﬁlter with estimation error denoted by Here, Xk = {x0, x1, . . . , xk} is the state vector series while Y k =

ek =. xˆk − xˆk and the inverse ﬁlter’s Kalman gain and estimation {y0, y1, . . . , yk} are the noisy observations. Also, p(Y k, Xk) is the

error covariance matrix by Kk and Σk, respectively, is

joint probability density of pair (Y k, Xk) and xˆk (a function of Y k)

is an estimate of xk with ∂2(·) denoting the Hessian with second

ek+1 = (Fk − KkGk)ek + rk + qk + sk,

(65)

∂x2

order partial derivatives. The sequence {Jk} of information matrices

where

can be computed recursively as [33]

11

Fig. 1. RMSE, AMSE and RCRLB for forward and inverse ﬁlters (a) KFwithout-DF; (b) KF-with-DF.

where

Jk = D2k2 − D2k1(Jk−1 + D1k1)−1D1k2,

(66)

11

∂2 ln p(xk|xk−1)

Dk = E −

∂x2

,

k−1

12

∂2 ln p(xk|xk−1)

Dk = E −

∂ xk ∂ xk−1

= (D2k1)T ,

22

∂2 ln p(xk|xk−1)

∂2 ln p(yk|xk)

Dk = E −

∂x2

+E −

∂x2

.

k

k

For the non-linear system given by (20) and (7), the forward information matrices {Jk} recursions reduces to [28]

Jk+1 = Q−k 1 + HTk+1R−k+11Hk+1 − Q−k 1Fk(Jk + FTk Q−k 1Fk)−1FTk Q−k 1,

(67)

where Fk = ∇xf (x)|x=xk and Hk = ∇xh(x)|x=xk . Note that,

for the information matrices recursion, the Jacobians Fk and Hk are

evaluated at the true state xk while for forward EKF recursions, these

are evaluated at the estimates of the state. These recursions can be

trivially extended to other system models considered in this paper as

well as to compute the posterior information matrix Jk for inverse ﬁlter’s estimate xˆk.

Throughout all experiments, 100 time-steps (indexed by k) were

considered. The initial information matrices J0 and J0 were set to

Σ−0 1

and

−1
Σ0 ,

respectively,

unless

mentioned

otherwise.

A. Inverse KF with unknown inputs

Consider a discrete-time linear system without DF [54],

0.1 0.5 0.08 0

xk+1 = 0.6 0.01 0.04 xk + 2 uk + wk,

0.1 0.7 0.05

1

110 yk = 0 1 1 xk + vk,

ak = 1 1 1 xˆk + k,

with wk ∼ N (0, 10I3), vk ∼ N (0, 20I2) and k ∼ N (0, 25). The unknown input uk was set to 50 for 1 ≤ k ≤ 50 and −50 thereafter. The initial state was x0 = [1, 1, 1]T . For the forward ﬁlter, the initial state estimate was set to [0, 0, 0]T with initial covariance Σ0 = 10I3. For the inverse ﬁlter, the initial state estimate was set to x0 (known to ‘us’) itself with initial covariance Σ0 = 15I3.
For KF-with-DF, we considered the same linear system with a
modiﬁed forward ﬁlter’s observations as [55]:

110

0

yk = 0 1 1 xk + 1 uk + vk.

Fig. 2. AMSE and RCRLB for forward and inverse ﬁlters: (a) EKF and SOEKF; (b) GS-EKF (I-GS-EKF-2 and I-GS-EKF-5, respectively, for l = 2 and 5) compared to EKF and I-EKF, averaged over 200 runs.

Here, the initial input estimate was set to 10 with initial input estimate covariance Σu0 = 10 and initial cross-covariance Σx0u = [0, 0, 0]T .

For the inverse ﬁlter, the initial estimate of the augmented state z0 was set to [1, 1, 1, 50]T with initial covariance Σ0 = 15I4.

Fig. 1 shows the time-averaged MSE (AMSE)

=

(

k i=1

xi − xˆi

22)/nk at k-th time step for n-dimensional

actual state xi and its estimate xˆi, and RCRLB for state estimation

for both forward and inverse ﬁlters in the two cases, respectively,

averaged over 20 runs. For KF-without-DF, we plot the root MSE

(RMSE) = ( xk − xˆk 22)/n for comparison here but omit it

for later plots for clarity. The RCRLB value for state estimation is

Tr(J−1) with J denoting the associated information matrix.

Fig. 1 shows that the effect of change in unknown input after

50 time-steps is negligible for KF-without-DF in both forward and

inverse ﬁlters. However, for KF-with-DF, the sudden change in

unknown input leads to an increase in state estimation error of the

forward ﬁlter and, consequently, of the inverse ﬁlter. The estimation

error of I-KF-without-DF is less than that of the corresponding

forward ﬁlter for all time-steps. On the other hand, for KF-with-

DF, the inverse ﬁlter’s estimation error converges to a higher value

as compared to the forward ﬁlter, even though the initial estimation

error assumed for the inverse ﬁlter is less than that assumed for the

forward ﬁlter. Only I-KF-without-DF efﬁciently achieves the RCRLB

bound on the estimation error.

B. Inverse ﬁlters for EKF, SOEKF and GS-EKF

Consider the discrete-time non-linear system model of FM demodulator without unknown inputs [36, Sec. 8.2]

xk+1

. =

λk+1

=

exp (−T /β)

0 λk + 1 wk,

θk+1

−β exp (−T /β) − 1 1 θk

−β

y

√ =2

sin θk

+v ,

a

= λˆ2 +

,

k

cos θk

kk

kk

with wk ∼ N (0, 0.01), vk ∼ N (0, I2), k ∼ N (0, 5), T = 2π/16 and β = 100. Here, the observation function g(·) for the inverse ﬁlter is quadratic. Also, λˆk is the forward EKF’s estimate of λk.
The initial state x0 =. [λ0, θ0]T was set randomly with λ0 ∼ N (0, 1) and θ0 ∼ U[−π, π]. All the initial state estimates of all forward and inverse ﬁlters including mean estimates for GS-EKF were also similarly drawn at random. The initial covariances were set to Σ0 = 10I2 and Σ0 = 5I2 for forward and inverse EKF as well as SOEKF. In the case of GS-EKF, we considered 5 Gaussians for the forward ﬁlter with the initial covariances and weights set to 10I2 and

12

1/5, respectively. For I-GS-EKF, an augmented state zk of 5 means {xi,k}1≤i≤5 and 5 weights {ci,k}1≤i≤5 was considered resulting in a 15-dimensional state vector with the initial weight estimates set to 1/5 and the initial covariance estimates {Σj,0}1≤j≤l as 5I15.
The phase term of the state θ and its estimates θˆ and θˆ (for both prediction and measurement updates) were considered to be modulo 2π [36]. Note that the process covariance Q is a singular matrix. For numerical stability and to facilitate computation of Q−1 for evaluating information matrices Jk, we used an enlarged covariance matrix by adding 10−10I2 to Q in the forward ﬁlters. Similarly, we added 10−10I2 to Qk in the inverse ﬁlter because Qk is time-varying and may be ill-conditioned. The initial J0 was taken close to the inverse of the steady state estimation covariance matrix of the forward ﬁlter. The initial J0 only affects the RCRLB calculated for initial few time-steps. The RCRLB after these initial time-steps (around 20 for the considered system) shows same behaviour irrespective of the initial J0. The Gaussian noise term vk+1 in I-GS-EKF’s state transition (55) is transformed through a non-linear function γ[·, ·] such that (67) is not applicable. The RCRLB in this case is derived using the general Jk recursions given by (66).
Fig. 2 shows that the forward GS-EKF with l = 5 performs better than both forward EKF and forward SOEKF. This negligible effect of including second-order terms in the forward SOEKF is also reﬂected in the RCRLB of the inverse ﬁlters. Both I-EKF and ISOEKF converge to the same steady-state estimation error values, which is also higher than that of the corresponding forward ﬁlters. Being suboptimal ﬁlters, the forward as well as inverse EKF and SOEKF do not achieve the RCRLB on the estimation error. However, the difference between AMSE and RCRLB for the inverse ﬁlters is less than that for the forward ﬁlters. We conclude that I-EKF and I-SOEKF are more efﬁcient here.
For GS-EKF, the estimation error of the inverse ﬁlter is same as that of the forward ﬁlter when l = 2 but improves signiﬁcantly when l = 5. Note that this improvement in performance comes at the expense of increased computational complexity because the inverse ﬁlter estimates an augmented state of dimension ‘l(n + 1)’, which is larger than the forward ﬁlter’s state dimension ‘n’.
The I-EKF assumes initial covariance Σ0 as 5I2 (the true Σ0 of forward EKF is 10I2) and a random initial state for these recursions. In spite of this difference in the initial estimates, I-EKF’s error performance is comparable to that of the forward EKF. Interestingly, despite similar differences in the initial estimates, I-GS-EKF with l = 5 outperforms the forward GS-EKF.

C. Inverse EKF with unknown inputs

For inverse EKF with unknown input, we modiﬁed the non-linear system model of Section VII-B to include an unknown input uk as

xk+1

. =

λk+1

θk+1

= exp (−T /β)

0 λk + 0.001 uk + 1 wk,

−β exp (−T /β) − 1 1 θk

1

−β

where unknown input uk was set to π/4 for 1 ≤ k ≤ 50 and −π/4 thereafter. The observation yk of the forward EKF-without-DF was same as in Section VII-B. Consider a linear measurement ak for the inverse ﬁlter as ak = λˆk + k. For the forward ﬁlter, the initial input estimate was set to 0 while the inverse ﬁlter initial augmented state estimate consisted of the true state x0 and true input u0 (known to ‘us’) with initial covariance estimate Σ0 = 15I3.
Similarly, for system with DF, we again considered the same non-linear system (without any unknown input in xk state transition) but with a modiﬁed forward ﬁlter’s observation yk =

Fig. 3. (a) Time-averaged RMSE for forward and inverse EKF with and without DF; (b) Absolute error and RCRLB for forward and inverse EKF as well as DEKF. I-DEKF-1 is the I-DEKF without considering the modiﬁed dithered function in the inverse ﬁlter formulation while I-DEKF-2 considers the modiﬁed function.

√ 2

sin (θk + uk)

+ v . The input estimates uˆ and uˆ were also, as

cos (θk + uk) k

before, modulo 2π. Again, the Gaussian noise terms in the inverse

ﬁlter state transitions are transformed through non-linear functions

such that (67) is not applicable. Fig. 3(a) shows that for both EKF

with and without DF, the change in unknown input after 50 time-steps

does not increase the estimation error (as for KF-with-DF in Fig.

1(b)). The estimation error of I-EKF-without-DF (I-EKF-with-DF)

is higher (lower) than that of the corresponding forward ﬁlter. Any

change in unknown input affects the inverse ﬁlter’s performance only

when a signiﬁcant change occurs in the forward ﬁlter’s performance.

D. Inverse DEKF

Consider the application of coordinate estimation of a stationary target from bearing observations taken by a moving sensor [32]. The actual coordinates of the stationary target are (X, Y ) and that of the sensor at k-th time instant are (ak, bk). The constant velocity of sensor is s. The forward and inverse EKF as well as DEKF were implemented in a modiﬁed coordinate basis with the state estimate xk = [ak/Y, s/Y, s, X/Y ]T and system model









1 ∆t 0 0

0

0 1 0 0

∆t/Y 

xk+1 = 0 0 1 0 xk +  ∆t  wk,

0 0 01

0

yk = arctan([xk]4 − [xk]1) + vk, ak = ([xˆk]4)2 + k,

where wk ∼ N (0, 0.12), vk ∼ N (0, 22), k ∼ N (0, 1.52) and ∆t = 20 s. The initial covariance estimates were set to Σ0 = diag(4.44 × 10−7, 0.5 × 10−6, 1, 0.1) and Σ0 = diag(10−6, 6 × 10−7, 5, 0.5), respectively, for the forward and inverse ﬁlters. The initial state estimate for inverse ﬁlters were [0, 0.002, 200, 2]T . All
other parameters of the system including the dither and the estimates
were identical to those in [32]. With 200 time-steps, the modiﬁed observation function h∗(·) in the
forward DEKF replaced h(·) up to 80 time-steps. Fig. 3(b) shows the
absolute error and RCRLB, averaged over 400 runs, for estimation of X/Y whose estimate at the k-th time instant are given by [xˆk]4 and [xˆk]4 of the forward and inverse ﬁlters. The RCRLB at k-th time instant is J−k 1 4,4. The inverse ﬁlters’ estimation errors were signiﬁcantly lower than that of the forward ﬁlters. While I-DEKF-1
and IDEKF-2 differ in their transient performance, they converge to
the same steady-state error as I-EKF.

13

VIII. SUMMARY
We studied the inverse ﬁltering problem for non-linear systems with and without unknown inputs in the context of counter-adversarial

Denoting the inverse ﬁlter’s one-step prediction error as ek+1|k =. xˆk+1 − xˆk+1|k, the error dynamics for the inverse ﬁlter is obtained
from this asymptotic form using (3) as

applications. These inverse ﬁlters allow ‘us’ to infer an estimate of the adversary’s estimate, given ‘our’ noisy observations of adversary’s actions. For systems with unknown inputs, the adversary’s observations may or may not be affected by the unknown input which is known to ‘us’ but not to the adversary. Addressing these two

ek+1|k = F − FΣGT (GΣGT + R)−1G ek|k−1 − FΣGT (GΣGT + R)−1 k + Evk+1.
Since F − FΣGT (GΣGT + R)−1G has eigenvalues strictly within the unit circle, this error dynamics is asymptotically stable.

cases, we developed I-EKF and I-KF (each with and without DF) for non-linear and linear system dynamics, respectively. For systems without unknown inputs, we extended the theory of I-EKF to its variants, namely, I-SOEKF, I-GS-EKF and I-DEKF. These variants may provide improved estimation performance or stability depending on the system.
We investigated theoretical guarantees for the stability of I-KFwithout-DF, I-EKF and I-SOEKF. In particular, stochastic stability of a forward ﬁlter with certain additional assumptions on the system is also sufﬁcient for its inverse ﬁlter to be stable. For I-EKF, we considered two different approaches to study its stability, each with its own advantages. We also derived similar stability results for ISOEKF. The asymptotic stability of I-KF-without-DF was obtained by extending standard KF stability results.

APPENDIX B PROOF OF THEOREM 2

For simplicity, we consider the case of n ≥ p with Uxk+y 1 ∈ Rn×n. It is trivial to show that the proof remains valid for n < p as well. Using the expressions for Σxk+y 1 and Sk+1, we have

Kk+1 = Σk+1|kUxk+y 1HTk+1Uyk+1∗ ∗ Uyk+1Hk+1Σk+1|kHTk+1Uyk+1 + Rˆ k+1

−1
,

Σk+1 = Σk+1|k − Σk+1|kUxk+y 1HTk+1Uyk+1∗ ∗ Uyk+1Hk+1Σk+1|kHTk+1Uyk+1 + Rˆ k+1

−1
∗

∗ Uyk+1Hk+1(Uxk+y 1)T Σk+1|k.

We demonstrated the efﬁcacy of the different inverse ﬁlters through numerical examples using RCRLB as a performance measure. For the non-linear system without unknown inputs, we considered the FM demodulation and coordinate estimation applications. The FM demodulator model was also extended for systems with unknown inputs. Our experiments suggest that the impact of the unknown input on inverse ﬁlter’s performance highly depends on its impact on the forward ﬁlter’s performance. For certain systems, the inverse ﬁlter

Deﬁne Vk(xk|k−1) = xTk|k−1Σ−k|1k−1xk|k−1. Using the bounds assumed on Σk|k−1, we have for all k ≥ 0

1

xk|k−1

2 2

≤ Vk(xk|k−1) ≤

1

xk|k−1

22 .

σ

σ

Hence, the ﬁrst condition of Lemma 1 is satisﬁed with vmin = 1/σ and vmax = 1/σ.
Using (57) and the independence of noise terms, we have

may perform more efﬁciently than the forward ﬁlter.

E Vk+1(xk+1|k)|xk|k−1

APPENDIX A

= xTk|k−1(Uxk Fk(I − KkUykHk))T Σ−k+11|k(Uxk Fk(I − KkUykHk))xk|k−1

PROOF OF THEOREM 1

+ E vkT (Uxk FkKk)T Σ−k+11|k(Uxk FkKk)vk|xk|k−1

conUvnedrgeer tthoe FstaabnildityEa,ssruemsppetcitoivneolyf, twhehefroerwFard=ﬁl(teIr,−FKk Han)d(IE−k + E wkT Σ−k+11|kwk|xk|k−1 . (68)

BMH)F and E = BM − KHBM + K, obtained by replacing The difference of two matrices A − B is invertible if maximum

Kk+1 and Mk+1 by the limiting matrices K and M, respectively, singular value of B is strictly less than the minimum singular value

in Fk and Ek. In this limiting case, the state transition equation (36) of A. Using the assumed bounds, we have Kk ≤ k = (σγhβ)/rˆ.

becomes

xˆk+1 = Fxˆk + EHxk+1 + Evk+1.

Hence, maximum singular value of KkUykHk is upper-bounded by (σγh2β2)/rˆ and the inequality (58) guarantees that I − KkUykHk is invertible (singular value of I is 1) such that

From (37), (38), and (40) and substituting the limiting matrices, the following Riccati equation is obtained

Σk+1|k = Uxk Fk(I − KkUykHk)·

T

T

−1

T (Σk|k−1 + (Uxk Fk(I − KkUykHk))−1Qˆ k((Uxk Fk(I − KkUykHk))−1)T )·

Σk+1|k = F Σk|k−1 − Σk|k−1G (GΣk|k−1G + Q,

+ R)

GΣk|k−1 F (I − KkUykHk)T FTk Uxk ,

where Q = ERET . For the forward ﬁlter to be stable, covariance R needs to be p.d. [51] and hence, Q is a p.s.d. matrix. With R being

because Uxk and Fk are also assumed to be invertible. Again with the

assumed bounds, we have

U

x k

Fk

(

I

−

Kk

U

y k

H

k

)

≤ αf (1 + kβh)

which implies

p.d. and the observability and controllability assumptions, Σk|k−1 tends to a unique p.d. matrix Σ satisfying

(Uxk Fk(I − KkUykHk))−1Qˆ k((Uxk Fk(I − KkUykHk))−1)T

Σ = F[Σ − ΣGT GΣGT + R −1 GΣ]FT + Q,

qˆ I.
(αf (1 + kβh))2

and F − FΣGT (GΣGT + R)−1G has eigenvalues strictly within the unit circle. These results follow directly from the application of [56, Proposition 4.1, Sec. 4.1] similar to the stability and convergence results for the standard KF for linear systems [56, Appendix E.4].
In this limiting case, the inverse ﬁlter prediction and update equations take the following asymptotic form
xˆk+1|k = Fxˆk + EHxk+1, xˆk+1 = xˆk+1|k + ΣGT (GΣGT + R)−1(ak+1 − Gxˆk+1|k).

Using this bound in the expression of Σk+1|k as in [53], we have

(Uxk Fk(I − KkUykHk))T Σ−k+11|k(Uxk Fk(I − KkUykHk)) (1 − λ)Σ−k|1k−1,

where 1 − λ =

−1

qˆ

with 0 < λ < 1.

1 + σ(αf (1+kβh))2

The last two expectation terms in (68) can be bounded by µ = (rpα2f 2k2/σ) + (qn/σ) > 0 following similar steps as in [53]

such that

14

E Vk+1(xk+1|k)|xk|k−1 − Vk(xk|k−1) ≤ −λVk(xk|k−1) + µ.
Hence, the second condition of Lemma 1 is also satisﬁed and the prediction error xk|k−1 is exponentially bounded in mean-squared sense and bounded with probability one.
Furthermore, with the bounds assumed on various matrices, it is straightforward to show that
E xk 2 ≤ (1 + kβh)2E xk|k−1 2 + k2rp.
Finally, the exponential boundedness of xk|k−1 leads to xk also being exponentially bounded in mean-squared sense as well as bounded with probability one.

APPENDIX C PROOF OF THEOREM 3

We will show that the I-EKF’s dynamics also satisﬁes the as-
sumptions of Theorem 2. For this, the following conditions C1-C13
need to hold true for all k ≥ 0 for some real positive constants a, g, b, c, d, qˆ, , cˆ, dˆ, p, p.

C1 Fxk ≤ a;

C2

x
Uk

≤ b;

C3

x
Uk

is

non-singular;

C4 Fxk is non-singular;

C5 Qk qI;

C6 Gk ≤ g;

C7

a
Uk

≤ c;

C8

xa
Uk

≤ d;

C9 Rk I; C10 cˆI Qˆ k; C11 dˆI Rˆ k;

C12 pI Σk|k−1 pI; and

C13 the constants satisfy the inequality pdg2c2 < dˆ.

Next, we prove that under the assumptions of Theorem 3, C1-
C13 are satisﬁed. From the I-EKF’s state transition (23), the Jacobians Fxk = Fk − Kk+1Hk+1Fk and Fvk = Kk+1 such that Qk = Kk+1Rk+1KTk+1.
For C1, using Kk+1 ≤ k (as proved in Theorem 2) and the
bounds on Fk and Hk+1 from the assumptions of Theorem 2, it is
trivial to show that

Fxk = Fk − Kk+1Hk+1Fk ≤ f + khf .

Hence, C1 is satisﬁed with a = f + khf .

For

C2-C4,

consider

the

unknown

matrix

x
Uk

introduced

to

account

for the residuals in linearization of fk(·). Let xˆk+1|k and xˆk denote

the state prediction error and state estimation error of I-EKF. Similar

to forward EKF with the introduction of the unknown matrix, we

have

xˆk+1|k = Uxk (Fk − Kk+1Hk+1Fk)xˆk + Kk+1vk+1. (69)

Also,
xˆk+1|k = f (xˆk) − f (xˆk) − Kk+1(h(f (xˆk)) − h(f (xˆk))) + Kk+1vk+1.
Using the unknown matrices Uxk and Uyk introduced in the linearization of f (·) and h(·), respectively, we have

xˆk+1|k = (Uxk Fk − Kk+1Uyk+1Hk+1Uxk Fk)xˆk + Kk+1vk+1.

Comparing with (69), we have Uxk (I − Kk+1Hk+1)Fk = (I − Kk+1Uyk+1Hk+1)Uxk Fk. (70)

With the additional assumption of rI Rk and using matrix inversion lemma as in proof of [26, Lemma 3.1], we have

−1

(I − Kk+1Hk+1)Σk+1|k = Σ−k+11|k + HTk+1R−k+11Hk+1

.

Since Σk+1|k is invertible by the assumptions of Theorem 2, I − Kk+1Hk+1 is invertible for all k ≥ 0 and

(I − Kk+1Hk+1)−1 = I + Σk+1|kHTk+1R−k+11Hk+1.

With the bounds assumed on various matrices, we have

(I − Kk+1Hk+1)−1

σh2 ≤1+ .
r

Furthermore, using this bound and the invertibility of I−Kk+1Hk+1 x
in (70), it is straightforward to show that Uk = (I − Kk+1Uyk+1Hk+1)Uxk(I − Kk+1Hk+1)−1 is non-singular (both Uxk and I − Kk+1Uyk+1Hk+1 are invertible under the assumptions of Theorem 2) and satisﬁes

Uxk ≤ α(1 + kβh)(1 + (σh2)/r).

Also, since both I−Kk+1Hk+1 and Fk are invertible, Fxk = Fk(I−

Kk+1Hk+1) is non-singular. Hence, C2-C4 are also satisﬁed with b = α(1 + kβh)(1 + (σh2)/r).

For C5, using the upper bound on Rk from assumptions of

Theorem 2, we have Qk rKk+1KTk+1. Since, Kk+1 ≤ k,

the maximum eigenvalue of Kk+1KTk+1 is bounded by k2 such that

2

2

Qk k rI. Hence, C5 is satisﬁed with = k r.

The conditions C6-C13 are assumed to hold true in Theorem 3.

Hence, all the conditions hold true for the I-EKF dynamics and

Theorem 2 is applicable for the I-EKF as well i.e. the estimation

error is exponentially bounded in mean-squared sense and bounded

with probability one.

APPENDIX D PROOF OF THEOREM 5

We will show that the error dynamics of the I-EKF given by (60)

satisﬁes the following conditions for all k ≥ 0 for some real positive

constants c, κφ¯, φ¯.

C1 cI Qk. C2 Fxk is non-singular matrix for all k ≥ 0.

C3

φk(xˆ, xˆ)

2

≤

κφ¯

xˆ − xˆ

2 2

for

all

xˆ − xˆ 2 ≤

κφ¯ > 0 and φ¯ > 0.

φ¯ for some

All other conditions of Theorem 4 can be proved to hold true

for the I-EKF’s error dynamics under the assumptions of Theorem

5 following similar approach as in proof of Theorem 3, such that

the estimation error given by (60) is exponentially bounded in mean-

squared sense and bounded with probability one provided that the

estimation error is bounded with > 0 where depends on the

various bounds in the same manner as depends in the forward ﬁlter

case.

For C1, using the bound on Rk from one of the assumptions of

Theorem 4, we have

Qk = KkRkKTk rKkKTk .

Substituting for Kk, we have

KkKTk = FkΣkHTk (HkΣkHTk + Rk)−2HkΣkFTk .

With the assumption that Hk is full column rank, KkKTk is p.d. as Fk is assumed to be non-singular in Theorem 4. Hence, there exists a constant q > 0 which is the minimum eigenvalue of KkKTk such that KkKTk qI and Qk rqI. Hence, C1 is satisﬁed with c = rq.
For C2, Fxk = Fk − KkHk is proved to be invertible for all k ≥ 0
as an intermediate result in the proof of Theorem 4 in [26, Lemma
3.1].

15

For C3, using Kk ≤ (f σh/r) (proved in [26, Lemma 3.1]) and the bounds on functions φ(·) and χ(·) from the assumptions of Theorem 4, we have

φk(xˆ, xˆ) 2 ≤ ≤

φ(xˆ, xˆ)

f σh 2+

χ(xˆ, xˆ)

2

r

f σh κφ + r κχ

xˆ − xˆ 22,

for xˆ − xˆ 2 ≤ min( φ, χ). Hence, C3 is satisﬁed with κφ¯ = κφ + (f σh/r)κχ and φ¯ = min( φ, χ).

APPENDIX E PROOF OF THEOREM 6

A. Preliminaries to the Proof

Lemma 2. Under the assumptions of Theorem 6, the following

bounds hold true for all k ≥ 0.

1)

n i=1

n j=1

aiaTj

Tr

∇2 [f (xˆk)]i Σk∇2 [f (xˆk)]j Σk

p.s.d. matrix and satisﬁes the upper bound

is a

nn
aiaTj Tr
i=1 j=1

∇2 [f (xˆk)]i Σk∇2 [f (xˆk)]j Σk

a2σ2n2I.

2)

p i=1

p j=1

bibTj

Tr

∇2 [h(xˆk)]i Σk∇2 [h(xˆk)]j Σk

p.s.d. matrix and satisﬁes the upper bound

is a

pp
bibTj Tr
i=1 j=1

∇2 [h(xˆk)]i Σk∇2 [h(xˆk)]j Σk

b2σ2npI.

3) Mk ≤ β with β > 0.

Proof: Using the bounds from the assumptions of Theorem 6, we have for all i, j ∈ {1, 2, . . . , n}
a2σ2I ∇2 [f (xˆk)]i Σk∇2 [f (xˆk)]j Σk a2σ2I, which implies

a2σ2n ≤ Tr ∇2 [f (xˆk)]i Σk∇2 [f (xˆk)]j Σk ≤ a2σ2n.

Now,

n i=1

n j=1

aiaTj

is

an

n×n

all-ones

matrix

with

n

as

one

of

its eigenvalue and all other (n − 1) eigenvalues are zero. Hence,

n i=1

n j=1

aiaTj

Tr

∇2 [f (xˆk)]i Σk∇2 [f (xˆk)]j Σk

is a p.s.d.

matrix and satisﬁes the ﬁrst bound in the lemma. Similarly, the bound

on

p i=1

p j=1

bibTj

Tr

∇2 [h(xˆk)]i Σk∇2 [h(xˆk)]j Σk

can be

d√erived. Further, the maximum singular value of

n i=1

p j=1

aibTj

is

np and hence, using the bounds, we can show that √

Mk

satisﬁes

Mk ≤ β with β = 1 abσ2n np.

2

Lemma 3. Under the assumptions of Theorem 6, there exists a real number α with 0 < α < 1 such that

(Fk − KkHk)T Σ−k+11(Fk − KkHk) (1 − α)Σ−k 1.

Proof: Using (48) and (53), we have

Σk+1 = FkΣkFTk + Qk − KkSkKTk

nn

1 +

aiaTj Tr ∇2 [f (xˆk)]i Σk∇2 [f (xˆk)]j Σk .

2 i=1 j=1

Using the positive semi-deﬁniteness of the last term (as proved in Lemma 2) and substituting for KkSk using (51), we have

Σk+1 FkΣkFTk + Qk − (FkΣkHTk + Mk)KTk .

Rearranging the terms,

Σk+1

(Fk − KkHk)Σk(Fk − KkHk)T + Qk + KkHkΣk(Fk − KkHk)T − MkKTk .

First, we consider the last two terms. Putting Mk = KkSk − FkΣkHTk , the last terms can be expressed as
KkHkΣk(Fk − KkHk)T − MkKTk = KkHkΣkFTk + FkΣkHTk KTk − Kk(Sk + HkΣkHTk )KTk .
Let A = KkHkΣkFTk . Using the bounds of Theorem 6 and Lemma 2, we can show that A ≤ fσh(frσh+β) . Also, A + AT is a symmetric ‘n × n’ matrix with

This implies

A + AT

2f σh(f σh + β)

≤

.

r

2f σh(f σh + β)

−

I

r

A + AT

2f σh(f σh + β) I.
r

Using this and other bounds, we have

KkHkΣk(Fk − KkHk)T − MkKTk −cI,

where c = 2fσh(frσh+β) + 2σh2 + δ + 12 b2σ2np positive constant. Hence,

f σh+β r

Σk+1 (Fk − KkHk)Σk(Fk − KkHk)T + Qk − cI.

2
is a

Similar to the proof of [26, Lemma 3.1], we can prove Fk − KkHk to be invertible using matrix inversion lemma. Here, F−k 1(Fk − KkHk)Σk = Σk − Σk(HTk + Σ−k 1F−k 1Mk)S−k 1HkΣk. The R.H.S. is in the form of matrix inversion lemma:
(A + UCV)−1 = A−1 − A−1U(C−1 + VA−1U)−1VA−1,

where A and C are invertible matrices. Hence,

the matrix inversion lemma is applicable if Rk +

1 2

p i=1

p j=1

bibTj

Tr

∇2 [h(xˆk)]i Σk∇2 [h(xˆk)]j Σk

−

HkF−k 1Mk is invertible (because C needs to be invertible). Since

the difference of two matrices X − Y is invertible if maximum

singular value of Y is strictly less than the minimum singular value of

X, the required difference is invertible if HkF−k 1Mk < r because

Rk + 21

p i=1

p j=1

bibTj

Tr

∇2 [h(xˆk)]i Σk∇2 [h(xˆk)]j Σk

rI. But HkF−k 1Mk ≤ hβ F−k 1 . Substituting the value of β as derived in Lemma 2, the sufﬁcient condition for invertibility of the

required matrix is

F−k 1 < habσ22rn√np .

The condition (62) is sufﬁcient for this to be satisﬁed. Note that this condition is a sufﬁcient but not necessary condition for invertibility of Fk − KkHk. With this condition, Lemma 3 can be proved using similar approach as in [26, Lemma 3.1] with

−1
q−c 1 − α = 1 + σ(f + (f σh2 + βh)/r)2 .

The condition (63) is sufﬁcient for 0 < α < 1. Substituting for constant c and using q ≤ δ and r ≤ δ, the
inequality (63) requires the following inequality to be satisﬁed

2f σh(f σh + β) +

2σh2 + δ + 1 b2σ2np

δ

2

fσh + β 2 < δ.
δ

Rearranging the terms, we have

δ3 − (f σh + β)(3f σh + β)δ − (f σh + β)2 2σh2 + 1 b2σ2np > 0. 2

It can be observed that this inequality requires the noise bound δ to be large enough and other bounds on matrices like f , h etc. to be small.

16

Lemma 4. Under the assumptions of Theorem 6, there exists positive real constants κnonl, such that

rTk Σ−k+11(2(Fk − KkHk)ek + rk) ≤ κnonl ek 42,

for ek 2 ≤ .

Proof: The proof follows from [26, Lemma 3.2] with κnonl = κσ 2 f + fσh2r+βh + κ 2 , where κ = κφ + κχ fσhr+β and
= min( φ, χ).

Lemma 5. Under the assumptions of Theorem 6, there exists positive real constant κnoise independent of δ such that

E[sTk Σ−k+11sk] ≤ κnoiseδ.

Proof: The proof follows from [26, Lemma 3.3] with κnoise = nσ + f 2hσ2rσ2 2p .
Lemma 6. Under the assumptions of Theorem 6, there exist positive real constants κsec, csec such that

qTk Σ−k+11(2(Fk − KkHk)ek + 2rk + qk) ≤ κsec

ek

3 2

+

csec ,

for ek 2 ≤ where is same as in Lemma 4.
Proof: Using the upper bound on ∇2 [f (xˆk)]i from the assumptions of Theorem 6, we have

eTk ∇2 [f (xˆk)]i ek ≤ a ek 22,

which implies

n
aieTk ∇2 [f (xˆk)]i ek 2 ≤ an ek 22.
i=1

Similarly, using other bounds, it is trivial to show that for all k ≥ 0, the following bounds are satisﬁed

n

aiTr ∇2 [f (xˆk)]i Σk 2 ≤ aσn2,

i=1

p

Kk

bieTk ∇2 [h(xˆk)]i ek

bp(f σh + β) 2≤

ek

22 ,

i=1 r

p
Kk biTr ∇2 [h(xˆk)]i Σk
i=1

bσnp(f σh + β)

2≤

.

r

Using these bounds, we have

qk

2 ≤ κq

ek

2 2

+

cq ,

where κq = 12 an + bp(fσrh+β) and cq =
12 aσn2 + bσnp(frσh+β) . Using the bounds on qk 2 and rk 2 as used in [26, Lemma
3.2], the lemma can be proved with

κsec = κq σ

f σh2 + βh 2 f+
r

+ 2κ 2 + κq

, and

csec = c2q + κqcq 2 + cq 2 f + f σh2 + βh + 2κ 2 + κq .

σ

σ

σ

r

B. Proof of the Theorem
Consider Vk(ek) = eTk Σ−k 1ek. Using the bounds on Σk from assumptions of Theorem 6, we have

1

ek

2 2

≤ Vk(ek) ≤

1

ek

22 .

σ

σ

Also, substituting for ek+1 using (61), we have

Vk+1(ek+1) = eTk (Fk − KkHk)T Σ−k+11(Fk − KkHk)ek + rTk Σ−k+11(2(Fk − KkHk)ek + rk) + qTk Σ−k+11(2(Fk − KkHk)ek + 2rk + qk) + sTk Σ−k+11sk + 2sTk Σ−k+11((Fk − KkHk)ek + rk + qk).

Using Lemmata 3, 4, and 6, we have for ek 2 ≤

Vk+1(ek+1) ≤(1 − α)Vk(ek) + κnonl

ek

4 2

+

κsec

ek

3 2

+

csec

+ sTk Σ−k+11sk + 2sTk Σ−k+11((Fk − KkHk)ek + rk + qk).

The last term sTk Σ−k+11((Fk − KkHk)ek + rk + qk) vanishes on taking expectation conditioned on ek and hence, for ek 2 ≤ ,

E[]Vk+1(ek+1)|ek] ≤(1 − α)Vk(ek) + κnonl

ek

4 2

+

κsec

ek

3 2

+ csec + κnoiseδ,

where the bound of Lemma 5 is applied. But, for ek 2 ≤ ,

κnonl

ek

4 2

+

κsec

ek

3 2

≤

(κnonl

+ κsec) ek 2 ek 22.

Choosing = min , 2σ(κnonαl +κsec) , we have for ek 2 ≤ ,

κnonl

ek

4 2

+

κsec

ek

3 2

≤

α Vk (ek ),

2

which implies

α E[Vk+1(ek+1)|ek] − Vk(ek) ≤ − 2 Vk(ek) + csec + κnoiseδ.

Hence, Lemma 1 is applicable. However, to have negative mean drift,

we require δ to be small enough such that there exists some <

to satisfy (64). This condition ensures that for ≤ ek 2 ≤ , E[Vk+1(ek+1)|ek] − Vk(ek) ≤ 0 is fulﬁlled and the estimation error ek remains exponentially bounded in mean-squared sense if the error

is within bound. Note that the conditions (63) and (64) suggest that

δ needs to be chosen appropriately such that both the conditions could

be satisﬁed simultaneously. However, the exact limits on δ depend

on the other bounds of the system dynamics. Furthermore, as with

the bounded non-linearity approach for EKF, these bounds may be

very conservative and it is possible that the estimation error remains

bounded outside this range as well [26, Sec. V].

APPENDIX F PROOF OF THEOREM 7
From the state transition function of I-SOEKF (54), we have
p
∇2 f k(xˆk) i = ∇2 f (xˆk) i − [Kk]i,j ∇2 h(xˆk) j .
j=1

Using the upper bounds on Hessian matrices from the assumptions of Theorem 6, we have

∇2 f k(xˆk) i





p

a − b [Kk]i,j  I

j=1





p

a + |b| |[Kk]i,j | I.

j=1

But, the row sum

p i=1

|[Kk]i,√j |

≤

Kk ∞. Further, using equiva-

lence of norms i.e. Kk ∞ ≤ p Kk , we have

∇2 f k(xˆk) i

√ fσh + β

a + |b| p

I.

r

Similarly,

∇2 f k(xˆk) i





p

a − b [Kk]i,j  I

j=1

b√p(f σh + β)

a−

I.

r

Hence, with d = a − (b√p(f σh + β)/r) and d = a + (|b|√p(f σh + β)/r), we have dI ∇2 f k(xˆk) i dI for all i ∈ {1, 2, . . . , n}.
The proof of the remaining conditions of Theorem 6 for I-SOEKF
dynamics follows from the proof of Theorem 5.

17

REFERENCES
[1] J. Idier, Bayesian approach to inverse problems. John Wiley & Sons, 2013.
[2] F. Gustafsson, “Statistical signal processing approaches to fault detection,” Annual Reviews in Control, vol. 31, no. 1, pp. 41–54, 2007.
[3] S. Haykin, “Cognitive radar: A way of the future,” IEEE signal processing magazine, vol. 23, no. 1, pp. 30–40, 2006.
[4] R. J. Elliott, L. Aggoun, and J. B. Moore, Hidden Markov models: Estimation and control. Springer, 2008, vol. 29.
[5] R. E. Kalman, “A new approach to linear ﬁltering and prediction problems,” Journal of Basic Engineering, vol. 82, no. 1, pp. 35–45, 1960.
[6] K. V. Mishra, M. B. Shankar, and B. Ottersten, “Toward metacognitive radars: Concept and applications,” in IEEE International Radar Conference, 2020, pp. 77–82.
[7] K. V. Mishra and Y. C. Eldar, “Performance of time delay estimation in a cognitive radar,” in IEEE International Conference on Acoustics, Speech and Signal Processing, 2017, pp. 3141–3145.
[8] N. Sharaga, J. Tabrikian, and H. Messer, “Optimal cognitive beamforming for target tracking in MIMO radar/sonar,” IEEE Journal of Selected Topics in Signal Processing, vol. 9, no. 8, pp. 1440–1450, 2015.
[9] V. Krishnamurthy and M. Rangaswamy, “How to calibrate your adversary’s capabilities? Inverse ﬁltering for counter-autonomous systems,” IEEE Transactions on Signal Processing, vol. 67, no. 24, pp. 6511– 6525, 2019.
[10] V. Krishnamurthy, D. Angley, R. Evans, and B. Moran, “Identifying cognitive radars - Inverse reinforcement learning using revealed preferences,” IEEE Transactions on Signal Processing, vol. 68, pp. 4529–4542, 2020.
[11] J. Biemond, R. L. Lagendijk, and R. M. Mersereau, “Iterative methods for image deblurring,” Proceedings of the IEEE, vol. 78, no. 5, pp. 856– 883, 1990.
[12] R. E. Kalman, “When is a linear control system optimal?” Journal of Basic Engineering, vol. 86, no. 1, p. 51–60, 1964.
[13] R. E. Kalman and R. S. Bucy, “New results in linear ﬁltering and prediction theory,” Journal of Basic Engineering, vol. 83, no. 1, pp. 95–108, 1961.
[14] S. Haykin, Kalman ﬁltering and neural networks. John Wiley & Sons, 2004, vol. 47.
[15] D. Simon, Optimal state estimation: Kalman, H∞, and nonlinear approaches. John Wiley & Sons, 2006.
[16] S. F. Schmidt, “Application of state-space methods to navigation problems,” in Advances in control systems, 1966, vol. 3, pp. 293–340.
[17] A. Zaknich, Principles of adaptive ﬁlters and self-learning systems. Springer, 2005.
[18] D. Broomhead and J. Huke, “Nonlinear inverse ﬁltering in the presence of noise,” in AIP Conference Proceedings, vol. 375, no. 1, 1996, pp. 337–359.
[19] M.-Y. Shen and C.-C. J. Kuo, “A robust nonlinear ﬁltering approach to inverse halftoning,” Journal of Visual Communication and Image Representation, vol. 12, no. 1, pp. 84–95, 2001.
[20] R. Tenney, R. Hebbert, and N. Sandell, “A tracking ﬁlter for maneuvering sources,” IEEE Transactions on Automatic Control, vol. 22, no. 2, pp. 246–251, 1977.
[21] A. H. Jazwinski, Stochastic processes and ﬁltering theory. Courier Corporation, 2007.
[22] R. Bass, V. Norum, and L. Schwartz, “Optimal multichannel nonlinear ﬁltering,” Journal of Mathematical Analysis and Applications, vol. 16, no. 1, pp. 152–164, 1966.
[23] A. Jazwinski, “Filtering for nonlinear dynamical systems,” IEEE Transactions on Automatic Control, vol. 11, no. 4, pp. 765–766, 1966.
[24] H. Kushner, “Approximations to optimal nonlinear ﬁlters,” IEEE Transactions on Automatic Control, vol. 12, no. 5, pp. 546–556, 1967.
[25] H. Singh, A. Chattopadhyay, and K. V. Mishra, “Inverse cognition in nonlinear sensing systems,” in American Control Conference, 2022, submitted.
[26] K. Reif, S. Gunther, E. Yaz, and R. Unbehauen, “Stochastic stability of the discrete-time extended Kalman ﬁlter,” IEEE Transactions on Automatic Control, vol. 44, no. 4, pp. 714–728, 1999.
[27] ——, “Stochastic stability of the continuous-time extended Kalman ﬁlter,” IEE Proceedings - Control Theory and Applications, vol. 147, pp. 45–52(7), 2000.
[28] K. Xiong, H. Zhang, and C. Chan, “Performance evaluation of UKFbased nonlinear ﬁltering,” Automatica, vol. 42, no. 2, pp. 261–270, 2006.

[29] Y. Wu, D. Hu, and X. Hu, “Comments on “Performance evaluation of UKF-based nonlinear ﬁltering”,” Automatica, vol. 43, no. 3, pp. 567– 568, 2007.
[30] P. Tam and J. Moore, “A Gaussian sum approach to phase and frequency estimation,” IEEE Transactions on Communications, vol. 25, no. 9, pp. 935–942, 1977.
[31] D. Alspach and H. Sorenson, “Nonlinear Bayesian estimation using Gaussian sum approximations,” IEEE Transactions on Automatic Control, vol. 17, no. 4, pp. 439–448, 1972.
[32] H. Weiss and J. Moore, “Improved extended Kalman ﬁlter design for passive tracking,” IEEE Transactions on Automatic Control, vol. 25, no. 4, pp. 807–811, 1980.
[33] P. Tichavsky, C. H. Muravchik, and A. Nehorai, “Posterior Crame´rRao bounds for discrete-time nonlinear ﬁltering,” IEEE Transactions on signal processing, vol. 46, no. 5, pp. 1386–1396, 1998.
[34] J. M. Mendel, Lessons in estimation theory for signal processing, communications, and control. Prentice Hall, 1995.
[35] S. Pan, H. Su, J. Chu, and H. Wang, “Applying a novel extended Kalman ﬁlter to missile - target interception with APN guidance law: A benchmark case study,” Control Engineering Practice, vol. 18, no. 2, pp. 159–167, 2010.
[36] B. D. Anderson and J. B. Moore, Optimal ﬁltering. Courier Corporation, 2012.
[37] J. Yang, S. Pan, and H. Huang, “An adaptive extended Kalman ﬁlter for structural damage identiﬁcations II: Unknown inputs,” Structural Control and Health Monitoring, vol. 14, no. 3, pp. 497–521, 2007.
[38] B. Ristic, S. Arulampalam, and N. Gordon, Beyond the Kalman ﬁlter: Particle ﬁlters for tracking applications. Artech house, 2003.
[39] S. Gillijns and B. De Moor, “Unbiased minimum-variance input and state estimation for linear discrete-time systems with direct feedthrough,” Automatica, vol. 43, no. 5, pp. 934–937, 2007.
[40] ——, “Unbiased minimum-variance input and state estimation for linear discrete-time systems,” Automatica, vol. 43, no. 1, pp. 111–116, 2007.
[41] L. Ljung, “Asymptotic behavior of the extended Kalman ﬁlter as a parameter estimator for linear systems,” IEEE Transactions on Automatic Control, vol. 24, no. 1, pp. 36–50, 1979.
[42] R. P. Wishner, J. A. Tabaczynski, and M. Athans, “A comparison of three non-linear ﬁlters,” Automatica, vol. 5, no. 4, pp. 487–496, 1969.
[43] R. S. Bucy and K. D. Senne, “Digital synthesis of non-linear ﬁlters,” Automatica, vol. 7, no. 3, pp. 287–298, 1971.
[44] L. Schwartz and E. Stear, “A computational comparison of several nonlinear ﬁlters,” IEEE Transactions on Automatic Control, vol. 13, no. 1, pp. 83–86, 1968.
[45] M. Netto, L. Gimeno, and M. Mendes, “On the optimal and suboptimal nonlinear ﬁltering problem for discrete-time systems,” IEEE Transactions on Automatic Control, vol. 23, no. 6, pp. 1062–1067, 1978.
[46] Y. Bar-Shalom, X. R. Li, and T. Kirubarajan, Estimation with applications to tracking and navigation: theory algorithms and software. John Wiley & Sons, 2004.
[47] A. J. Krener, “The Convergence of the Extended Kalman Filter,” in Directions in mathematical systems theory and optimization. Springer, 2003, pp. 173–182.
[48] B. F. La Scala, R. R. Bitmead, and M. R. James, “Conditions for stability of the extended Kalman ﬁlter and their application to the frequency tracking problem,” Mathematics of Control, Signals and Systems, vol. 8, no. 1, pp. 1–26, 1995.
[49] B. Ursin, “Asymptotic convergence properties of the extended Kalman ﬁlter using ﬁltered state estimates,” IEEE Transactions on Automatic Control, vol. 25, no. 6, pp. 1207–1211, 1980.
[50] K. Reif, F. Sonnemann, and R. Unbehauen, “An EKF-based nonlinear observer with a prescribed degree of stability,” Automatica, vol. 34, no. 9, pp. 1119–1123, 1998.
[51] H. Fang and R. A. De Callafon, “On the asymptotic stability of minimum-variance unbiased input and state estimation,” Automatica, vol. 48, no. 12, pp. 3183–3186, 2012.
[52] K. Xiong, H. Zhang, and C. Chan, “Author’s reply to “Comments on ‘Performance evaluation of ukf-based nonlinear ﬁltering”’,” Automatica, vol. 43, no. 3, pp. 569–570, 2007.
[53] L. Li and Y. Xia, “Stochastic stability of the unscented Kalman ﬁlter with intermittent observations,” Automatica, vol. 48, no. 5, pp. 978–981, 2012.
[54] C.-S. Hsieh, “Robust two-stage Kalman ﬁlters for systems with unknown inputs,” IEEE Transactions on Automatic Control, vol. 45, no. 12, pp. 2374–2378, 2000.
[55] S. Pan, H. Su, H. Wang, and J. Chu, “The study of joint input and state estimation with Kalman ﬁltering,” Transactions of the Institute of Measurement and Control, vol. 33, no. 8, pp. 901–918, 2011.

18
[56] D. P. Bertsekas, Dynamic programming and optimal control. Athena Scientiﬁc Belmont, 1995, vol. 1, no. 2.

