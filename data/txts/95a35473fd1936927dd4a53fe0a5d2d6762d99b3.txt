arXiv:2112.09312v2 [cs.SD] 17 Mar 2022

Published as a conference paper at ICLR 2022
MIDI-DDSP: DETAILED CONTROL OF MUSICAL
PERFORMANCE VIA HIERARCHICAL MODELING
Yusong Wu1, Ethan Manilow2,4, Yi Deng3, Rigel Swavely4, Kyle Kastner1, Tim Cooijmans1, Aaron Courville1, Cheng-Zhi Anna Huang∗1,4 , Jesse Engel∗4 ∗ Equal Contribution 1Mila, Quebec Artiﬁcial Intelligence Institute, Universite´ de Montre´al, 2Northwestern University, 3New York University, 4Google Brain wu.yusong@mila.quebec {emanilow, annahuang, jesseengel}@google.com
ABSTRACT
Musical expression requires control of both what notes are played, and how they are performed. Conventional audio synthesizers provide detailed expressive controls, but at the cost of realism. Black-box neural audio synthesis and concatenative samplers can produce realistic audio, but have few mechanisms for control. In this work, we introduce MIDI-DDSP a hierarchical model of musical instruments that enables both realistic neural audio synthesis and detailed user control. Starting from interpretable Differentiable Digital Signal Processing (DDSP) synthesis parameters, we infer musical notes and high-level properties of their expressive performance (such as timbre, vibrato, dynamics, and articulation). This creates a 3-level hierarchy (notes, performance, synthesis) that affords individuals the option to intervene at each level, or utilize trained priors (performance given notes, synthesis given performance) for creative assistance. Through quantitative experiments and listening tests, we demonstrate that this hierarchy can reconstruct high-ﬁdelity audio, accurately predict performance attributes for a note sequence, independently manipulate the attributes of a given performance, and as a complete system, generate realistic audio from a novel note sequence. By utilizing an interpretable hierarchy, with multiple levels of granularity, MIDI-DDSP opens the door to assistive tools to empower individuals across a diverse range of musical experience. 1
1 INTRODUCTION
Generative models are most useful to creators if they can generate realistic outputs, afford many avenues for control, and easily ﬁt into existing creative workﬂows (Huang et al., 2020). Deep generative models are expressive function approximators, capable of generating realistic samples in many domains (Ramesh et al., 2021; Brown et al., 2020; van den Oord et al., 2016), but often at the cost of interactivity, restricting users to rigid black-box input-output pairings without interpretable access to the internals of the network. In contrast, structured models chain several stages of interpretable intermediate representations with expressive networks, while still allowing users to interact throughout the hierarchy. For example, these techniques have been especially effective in computer vision and speech, where systems are optimized for both realism and control (Lee et al., 2021b; Chan et al., 2019; Zhang et al., 2019; Wang et al., 2018a; Morrison et al., 2020; Ren et al., 2020).
For music generation, despite recent progress, current tools still fall short of this ideal (Figure 1, right). Deep networks can either generate realistic full-band audio (Dhariwal et al., 2020) or provide detailed controls of attributes such as pitch, dynamics, and timbre (De´fossez et al., 2018; Engel et al.,
1Online resources: Code: https://github.com/magenta/midi-ddsp Audio Examples: https://midi-ddsp.github.io/ Colab Demo: https://colab.research.google.com/github/magenta/midi-ddsp/blob/ main/midi_ddsp/colab/MIDI_DDSP_Demo.ipynb
1

Published as a conference paper at ICLR 2022
Figure 1: (Left) The MIDI-DDSP architecture. MIDI-DDSP extracts interpretable features at the performance and synthesis levels, building a modeling hierarchy by learning feature generation at each level. Red and blue components indicate encoding and decoding respectively. Shaded boxes represent modules with learned parameters. Both expression features and notes are extracted directly from synthesis parameters. (Right) Synthesizers have wide range of control, but struggle to convey realism. Neural audio synthesis and concatenative samplers can produce realistic audio, but have limited control. MIDI-DDSP enables both realistic neural audio synthesis and detailed user control.
2019; 2020a; Hawthorne et al., 2019; Wang & Yang, 2019) but not both. Many existing workﬂows use the MIDI speciﬁcation (Association et al., 1996) to Conventional DSP synthesizers (Chowning, 1973; Roads, 1988) provide extensive control but make it difﬁcult to generate realistic instrument timbre, while concatenative samplers (Schwarz, 2007) play back high-ﬁdelity recordings of isolated musical notes, but require manually stitching together performances with limited control over expression and continuity. In this paper, we propose MIDI-DDSP, a hierarchical generative model of musical performance to provide both realism and control (Figure 1, left). Similar to conventional synthesizers and samplers that use the MIDI standard (Association et al., 1996), MIDI-DDSP converts note timing, pitch, and expression information into ﬁne-grained parameter control of DDSP synthesizer modules. We take inspiration from the hierarchical structure underlying the process of creating music. A composer writes a piece as a series of notes. A performer interprets these notes through a myriad of nuanced, sub-second choices about articulation, dynamics, and expression. These expressive gestures are realized as audio through the short-time pitch and timbre changes of the physical vibration of the instrument. MIDI-DDSP is built on a similar 3-level hierarchy (notes, performance, synthesis) with interpretable representations at each level. While the efﬁcient DDSP synthesis representation (low-level) allows for high-ﬁdelity audio synthesis (Engel et al., 2020a), users can also control the notes to be played (high-level), and the expression with which they are performed (mid-level). A qualitative example of this is shown in Figure 2, where a given performance on violin is manipulated at all three levels (notes, expression, synthesis parameters) to create a new realistic yet personalized performance. As seen in Figure 1 (left), MIDI-DDSP can be viewed similarly to a multi-level autoencoder. The hierarchy has three separately trainable modules (DDSP Inference, Synthesis Generator, Expression Generator) and three ﬁxed functions/heuristics (DDSP Synthesis, Feature Extraction, Note Detection). These modules enable MIDI-DDSP to conditionally generate at any level of the hierarchy,
2

Published as a conference paper at ICLR 2022
Figure 2: An example of detailed user control. Given an initial generation from the full MIDI-DDSP model (top), an expert musician can adjust notes (blue), performance attributes (green), and lowlevel synthesis parameters (yellow) to craft a personalized expression of a musical piece (bottom).
providing creative assistance by ﬁlling in the details of a performance, synthesizing audio for new note sequences, or even fully automating music generation when paired with a separate note generating model. It is important to note that the system relies on pitch detection and note detection, so is currently limited to training on recordings of single monophonic instruments. This approach has the potential to be extend to polyphonic recordings via multi-instrument transcription (Hawthorne et al., 2021; Engel et al., 2020b; Bittner et al., 2017) and multi-pitch tracking, which is an exciting avenue to explore for future work. Finally, we also show that each stage can be made conditional on instrument identity, training on 13 separate instruments with a single model. For clarity, we summarize the core contributions of this work:
• We propose MIDI-DDSP, a 3-level hierarchical generative model of music (notes, performance, synthesis), and train a single model capable of realistic audio synthesis for 13 different instruments. (Section 3)
• Expression Attributes: We introduce heuristics to extract mid-level per-note expression attributes from low-level synthesis parameters. (Figure 4)
• User Control: Quantitative studies conﬁrm that manipulating the expression attributes creates a corresponding effect in the synthesizer parameters, and we qualitatively demonstrate the detailed control that is available to users manipulating all three levels of the hierarchy. (Table 2 and Figure 2)
• Assistive Generation: Reconstruction experiments show that MIDI-DDSP can make assistive predictions at each level of the hierarchy, accurately resynthesizing audio, predicting synthesis parameters from note-wise expression attributes, and auto-regressively predicting note-wise expression attributes from a note sequence. (Tables 1a, 1b, 1c)
• Realistic Note Synthesis: An extensive listening study ﬁnds that MIDI-DDSP can synthesize audio from new note sequences (not seen during training) with higher realism than both comparable neural approaches and professional concatenative sampler software. (Figure 5)
• Automatic Music Generation: We demonstrate that pairing MIDI-DDSP with a pretrained note generation model enables full-stack automatic music generation. As an example, we use Coconet (Huang et al., 2017) to generate and synthesize novel 4-part Bach chorales for a variety of instruments. (Figure 6)
2 RELATED WORK
Note Synthesis. Existing neural synthesis models allow either high-level manipulation of note pitch, velocity, and timing (Hawthorne et al., 2019; Kim et al., 2019; Wang & Yang, 2019; Manzelli et al.,
3

Published as a conference paper at ICLR 2022
Figure 3: Separate training procedures for the three modules in MIDI-DDSP. (Left) The DDSP Inference module predicts synthesis parameters from audio and is trained via an audio reconstruction loss on the resynthesized audio. (Middle) The Synthesis Generator module predicts synthesis parameters from notes and their expression attributes (shown as a 6-dimensional color map) and is trained via a reconstruction loss and an adversarial loss. (Right) The Expression Generator module autoregressively predicts note expression given a note sequence and is trained with teacher forcing. Encoding processes are shown in red, and decoding processes are shown in blue and loss calculations are shown in yellow. Thicker arrows indicate the process that is being trained in each level. Ground-truth data are shown in solid frames, while model predictions are shown in dashed frames.
2018), or low-level synthesis parameters (Jonason et al., 2020; Castellon et al., 2020; Blaauw & Bonada, 2017). MIDI-DDSP connects these two approaches by enabling both high-level note controls and low-level synthesis manipulation in a single system. Most related to this work is MIDI2Params (Castellon et al., 2020), a hierarchical model that autoregressively predicts frame-wise pitch and loudness contours to drive the original DDSP autoencoder (Engel et al., 2020a). MIDI-DDSP builds on this work by adding an additional level of hierarchy for the note expression, training a new more accurate DDSP base model, and explicitly modeling the synthesizer coefﬁcients output by that model, rather than the pitch and loudness inputs to the model. We extensively compare to our reimplementation of MIDI2Params as a baseline throughout the paper. Hierarchical Audio Modelling. Audio waveforms have dependencies over timescales spanning several orders of magnitude, lending themselves to hierarchical modeling. For example, Dieleman et al. (2018) and Dhariwal et al. (2020) both choose to encode audio as discrete latent codes at different time resolutions, and apply autoregressive models as priors over those codes. MIDI-DDSP applies a similar approach in spirit, but constructs a hierarchy based on semantic musical structure (note, performance, synthesis), allowing interpretable manipulation by users. Expressive Performance Analysis and Synthesis. Many prior systems pair analysis and synthesis functions to capture expressive performance characteristics (Canazza et al., 2004; Yang et al., 2016; Shih et al., 2017). Such methods often use heuristic functions to generate parameters for driving synthesizers or selecting and modifying sample units. MIDI-DDSP similarly uses feature extraction, but each level is paired with a differentiable neural network function that directly learns the mapping to expression and synthesis controls for more realistic audio synthesis.
3 MODEL ARCHITECTURE
3.1 DDSP SYNTHESIS AND INFERENCE
Differentiable Digital Signal Processing (DDSP) (Engel et al., 2020a) enables differentiable audio synthesis by using a harmonic plus noise model (Serra & Smith, 1990). Full details are provided in Appendix B.1. Brieﬂy, an oscillator bank synthesizes a harmonic signal from a fundamental frequency f0(t), a base amplitude a(t), and a distribution over harmonic amplitudes h(t), where the dimensionality of h is the number of harmonics. The noise signal is generated by ﬁltering uniform noise with linearly spaced ﬁlter banks, where η(t) represents the magnitude of noise output
4

Published as a conference paper at ICLR 2022
Figure 4: In MIDI-DDSP, manipulating note-level expression can effectively change the synthesislevel quantities. We show by taking a test-set sample (middle row) and adjusting each expression control value to lowest (bottom row) and highest (upper row), how each synthesis quantities (rightmost legend) would change. The dashed gray line in each plot indicates the note boundary.
from each ﬁlter in time. In this study, we use 60 harmonics and 65 noise ﬁlter banks, giving 127 total synthesis parameters each time frame (s(t) = (f0(t), a(t), h(t), η(t))). The ﬁnal audio is the addition of harmonic and noise signals. Since the synthesis process is differentiable, Engel et al. (2020a) demonstrate that it is possible to train a neural network to predict the other synthesis parameters given f0(t) and the loudness of the audio, and optimize a multi-scale spectral loss (Wang et al., 2019; Engel et al., 2020a) of the resynthesized audio (Figure 3 left). f0(t) is extracted by a pre-trained CREPE model (Kim et al., 2018), and the loudness is extracted via an A-weighting of the power spectrum (Hantrakul et al., 2019; McCurdy, 1936). We extend this work for our DDSP Inference module, by providing an additional input features extracted by a CNN (Lecun et al., 1998) from log-scale Mel-spectrogram of the audio, that produces higher quality resynthesis (Table 1a). Full architectural details are provided in Appendix B.2.
3.2 EXPRESSION CONTROLS
We aim to model aspects of expressive performance with a continuous variable. For example, this enables a performer to choose how loud the note should be performed, or how much vibrato to apply. We deﬁne a 6-dimensional vector, ei, for each note, ni, where each dimension corresponds to one of the six expression controls (detailed in Appendix B.3), scaled within [0, 1]. These are extracted from synthesis parameters s(t) and applied within the ith note, ni, in a note sequence: Volume: Controls the volume of a note, extracted by taking average amplitude over a note. Volume ﬂuctuation: Determines the magnitude of a volume change across a note. Used with the volume peak position described below, this can make a note crescendo or decrescendo. This is extracted by calculating the standard deviation of the amplitude over a note. Volume peak position: Controls where, over the duration of a note, the peak volume occurs. Zero value corresponds to decrescendo notes, whereas one corresponds to crescendo notes. The volume peak position is extracted by calculating the relative position of maximum amplitude in the note. Vibrato: Controls the extent of the vibrato of a note. Vibrato is a musical technique deﬁned by pulsating the pitch of a note. Vibrato is extracted by applying Discrete Fourier Transform (DFT) on the fundamental frequency f0(t) in a note and selecting the peak amplitude. Brightness: Controls the timbre of a note where higher values correspond to louder high-frequency harmonics. Brightness is determined by calculating the average harmonic centroid of a note. Attack Noise: Controls how much noise occurs at the start of the note (the attack), e.g., the ﬂuctuation of string and bow. Attack noise can determine whether two notes sound consecutively or separately. Attack noise is extracted by taking a note’s average noise magnitude in the ﬁrst ten frames (40ms).
5

Published as a conference paper at ICLR 2022

3.3 SYNTHESIS GENERATOR
Given the output of the per-note Expression Controls, ei for i = 1, ..., I notes, and a corresponding note sequence, ni, the Synthesis Generator predicts the frame-level synthesis parameters that, in turn, generate audio. Note expression controls are unpooled (repeated) over the duration of the corresponding note to make a conditioning sequence, c(t) = [(e1, n1), ..., (eI , nI )], with the same length as the fundamental frequency curve, f0(t).
The Synthesis Generator, gθ, is an autoregressive recurrent neural net (RNN) is used to predict a fundamental frequency, fˆ0(t) given conditioning sequence, and a convolutional generative adversarial network (GAN), gφ, is used to predict the other synthesis parameters given conditioning sequence and generated fundamental frequency:

fˆ0(t) = gθ(c(t)), aˆ(t), hˆ (t), ηˆ(t) = gφ(c(t), fˆ0(t)),

(1)

where θ denotes trainable parameters in the autoregressive RNN, and φ indicates trainable param-
eters in the GAN. Architectural details for both of these details is provided in Appendix B.4. The autoregressive RNN is trained using cross-entropy loss Lce. The generator of the GAN is trained by a multi-scale spectral loss Lspec (Eq. ??) and an adversarial objective consisting of a least-squares GAN (LSGAN) Llsgan (Mao et al., 2017) loss and a feature matching loss Lfm (Appendix B.4, Eqs. 15-18) (Kumar et al., 2019). Thus, the total loss applied to the Synthesis Generator can be
written as:

L = (Lce + Lspec) + α(Llsgan + γLfm)

(2)

where α and γ are settable hyperparameters that control the overall GAN loss and feature matching loss, respectively. For training, the ground-truth f0(t) is input to the GAN, thus there is no gradient from the GAN to the autoregressive RNN.

3.4 EXPRESSION GENERATOR
The Expression Generator uses an autoregressive RNN to predict note expression controls from note sequence (Appendix B.6). A single-layer bidirectional GRU extracts context information from input, and a two-layer autoregressive GRU generates note expression. The Expression Generator is trained by mean square error (MSE) loss between ground-truth note expression and teacher-forced prediction (Figure 3, right). The note sequence used to train the Expression Generator can either be extracted or comes from human labels. To show the full potential of MIDI-DDSP, we use the ground-truth note boundary label from dataset in all experiments for best accuracy. However, in future work note transcription models can be used to provide the note labels.

4 EXPERIMENTS
The structured hierarchy and explicit latent representations used in MIDI-DDSP beneﬁt music control as well as music modeling. We design a set of experiments to answer the following questions: First, does the system generate realistic audio, and if so, how does each module contribute? How does this compare to existing systems? And, second, is the system capable of enabling note-level, performance-level, and synthesis-level control? How effective are these controls?
4.1 DATASET
To demonstrate modeling a variety of instruments, we use the URMP dataset (Li et al., 2018), a publicly-available audio dataset containing monophonic solo performances of a variety of instruments. URMP is widely used in music synthesis research (Bitton et al., 2020; Hayes et al., 2021; Zhao et al., 2019; Engel et al., 2020b). The URMP dataset contains solo performance recordings and ground truth note boundaries from 13 string and wind instruments, which allows us to test MIDI-DDSP on many different instruments. The recordings in the URMP dataset are played by students, and the performance quality is substantially lower compared to virtuoso datasets used in

6

Published as a conference paper at ICLR 2022

Table 1: Each module in MIDI-DDSP produces a high-quality reconstruction and accurate prediction. We show reconstruction accuracy of each MIDI-DDSP module against a comparable method.

Model DDSP Inference Engel et al. (2020a)
(a)

Spectral Loss Model 4.28 Synthesis Generator 5.00 MIDI2Params
(b)

RMSE Models 0.19 Expression Generator 0.26 MIDI2Params
(c)

RMSE
0.14 0.23

Figure 5: (left) Comparing the log-scale Mel spectrograms of synthesis results from test-set note sequences, MIDI-DDSP synthesizes more realistic audio (more similar to ground-truth and DDSP Inference) than prior score-to-audio work MIDI2Params (enlarged in Figure 7). The synthesis quality is also reﬂected in the listening study (right), where the MIDI-DDSP synthesis is perceived as more realistic than the professional concatenative sampler Ableton and the freely available FluidSynth.
other work (Hawthorne et al., 2019). The URMP dataset contains 3.75 hours of 117 unique solo recordings, where 85 recordings in 3 hours are used as the training set, and 35 recordings in 0.75 hours are used as the hold-out test set.
4.2 MODEL ACCURACY
Modules in MIDI-DDSP can accurately reconstruct output at multiple levels of the hierarchy (Figure 3). We evaluate the reconstruction quality of MIDI-DDSP by evaluating each module, and report the average value across all test set samples in Table 1.
DDSP Inference We measure the difference between reconstruction and ground-truth in the audio spectral loss for our DDSP Inference module and compared it with the original DDSP autoencoder. As shown in Table 1a, with an additional CNN to extract features, the DDSP Inference module can reconstruct audio more accurately than the original DDSP Autoencoder.
Synthesis Generator We predict synthesis parameters from ground-truth note expression and then extract note expression back from the generated synthesis parameters. We measure the root mean square error (RMSE) between note expressions. The prior approach MIDI2Params directly generates synthesis parameters from notes and does not have access to note expressions. However, we can extract note expressions from the generated synthesis parameters and compare them to ground truth. As shown in Table 1b, the Synthesis Generator can faithfully reconstruct the input note expression, whereas without access to note expression, MIDI2Params generates larger error.
Expression Generator We take ground-truth MIDI and evaluate the likelihood of the ground-truth note expressions under the model. As the Expression Generator is autoregressive, we use teacherforcing to sequentially accumulate the squared error note by note. The total error thus computed can be interpreted as a log-likelihood. We again compare to MIDI2Params, where we autoregressively condition its own output within and on ground-truth across notes to obtain a note-wise metric. That is, MIDI2Params sees the ground truth of past notes, but sees its own output for the current note. As shown in Table 1c, the Expression Generator can accurately predict the note expression. In
7

Published as a conference paper at ICLR 2022

Table 2: The note expression outputs are strongly correlated with input adjustment. The Pearson correlation r-values are shown in the table (all entries p < 0.0001). The bold numbers indicate a Pearson r-value larger than 0.7, which we consider to indicate strong correlation between the input control and the respective output quantity. For simplicity, only four instruments are shown. More results can be found in Table 7.

All instruments Violin Viola Cello Double bass

Volume
.97 .99 .98 .97 .98

Vol. Fluc.
.78 .84 .74 .64 .85

Vol. Peak Pos.
.57 .80 .70 .54 .34

Vibrato
.70 .86 .82 .74 .84

Brightness
.92 .96 .98 .98 .99

Attack Noise
.93 .97 .97 .94 .95

comparison, MIDI2Params without performance-level modeling suffers from predicting the note expression with a higher error when compared to our frame-wise sequence models.
4.3 AUDIO QUALITY EVALUATION BY HUMAN LISTENERS
We evaluate the audio quality of MIDI-DDSP via a listening test. We compare ground truth audio from the URMP dataset to MIDI-DDSP and four other sources: a stripped down version of our system, containing just our DDSP Inference module (Section 3.1), MIDI2Params (Castellon et al., 2020), and two concatenative samplers: FluidSynth and Ableton (detailed in Appendix D.1). DDSP Inference infers synthesis parameters from the ground truth audio; it serves as an upper bound on what is attainable with MIDI-DDSP, which has to predict expression and synthesis parameters from MIDI. MIDI2Params is prior work that synthesizes audio from MIDI by predicting frame-wise loudness and pitch contour, which is fed as input to a DDSP autoencoder.
Participants in the listening test were presented with two 8-second clips, and asked which clip sounded more like a person playing on a real violin, on a 5-point Likert scale. We collected 960 ratings, with each source involved in 320 pair-wise comparisons. Figure 5 shows the number of comparisons in which each source was selected as more realistic. According to a post-hoc analysis using the Wilcoxon signed-rank test with Bonferroni correction (with p < 0.01/15), the orderings shown in Figure 5 (right) are all statistically signiﬁcant with the exception of ground truth versus DDSP Inference and MIDI2Params versus FluidSynth (Table 6). In particular, MIDI-DDSP was signiﬁcantly preferred over MIDI2Params, Ableton, and FluidSynth.
The difference among the sources can also be seen from visual inspection of the spectrograms (Figure 5, left). While the DDSP Inference module faithfully re-synthesizes the ground-truth audio, MIDI-DDSP generates a coherent performance from a series notes, and has rich, varying expressions across the notes. MIDI2Params failed to generate coherent expressions within a note, generating unrealistic pitch and loudness contours. Also, MIDI2Params stopped the note in the middle when generating the ﬁfth note, suggesting that modelling expressive performance only in synthesis level is limited in long-term coherence even inside a single note. On the contrary, the note expression modeling in MIDI-DDSP allows it to model dependencies at the granularity of the note sequence and use synthesis parameters to model the frame-wise parameter changing inside a single note. The two concatenative synthesizers Ableton and FluidSynth generate the same note expression with identical vibrato and volume for all notes. Although the expression is coherent inside a single note, they fail to generate expression dependencies between different notes automatically.
4.4 EFFECTS OF NOTE EXPRESSION CONTROLS
To evaluate the behavior of the note expression controls, we measure how well each control correlates with itself after a roundtrip through synthesis. That is, for each sample in the test set, we interpolate the control from lowest (0) to highest (1) in an interval of 0.1 and generate synthesis parameters. Then we extract the note expressions from these synthesis parameters. Table 2 reports the correlation between the value we put in and the value observed after synthesis. All controls exhibit
8

Published as a conference paper at ICLR 2022
Figure 6: MIDI-DDSP can take input from different sources (human or other models) by designing explicit latent representations at each level. A full hierarchical generative model for music can be constructed by connecting MIDI-DDSP with an automatic composition model. Here, we show MIDI-DDSP taking note input from a score level Bach composition model and automatically synthesizing a Bach quartet by generating explicit latent for each level in the hierarchy.
strong correlation as desired, except for volume peak position. A low correlation may stem from characteristics of the instrument, or imbalances of those performance techniques in the dataset. Figure 4 illustrates how each note expression affects properties of the sound. For example, as we increase vibrato, we see stronger ﬂuctuations in pitch. Similarly, changing the volume peak position changes the shape of the amplitude curve.
4.5 FINE GRAINED CONTROL OR FULL END-TO-END GENERATION The structured modelling approach of MIDI-DDSP enables end users to have as much or as little control over the output as they want. A user can add manipulations at certain levels of the hierarchy or let the model guide the synthesis. On one end of this spectrum, Figure 2 shows the results of an end user manipulating each level of MIDI-DDSP. Because different levels of the MIDI-DDSP hierarchy correspond with different musical attributes, a user can make manipulations at the note-level to change the attack noise and volume to create staccato notes (second green box in Figure 2) or a user could make adjustments to the synthesis-level to control the pitch contour for making a “pitch bend” (yellow box in Figure 2). On the other end of the spectrum, MIDI-DDSP can be paired with generative symbolic music models to make fully generated, realistic end-to-end performances. As shown in Figure 6, MIDI-DDSP can be combined with a composition Bach chorales model COCONET (Huang et al., 2017), to form a fully generated musical quartet that sounds like real instruments performance. Readers are encouraged to listen to both the hand-tuned and end-to-end performances on our accompanying website.
5 CONCLUSION
We proposed MIDI-DDSP, a hierarchical music modeling system that factorizes the generation of audio to note, performance, and synthesis levels. By proposing explicit representations for each level alongside modeling note expression, MIDI-DDSP enables effective manipulation and realistic automatic generation of music. We show, experimentally, that the input controls for MIDI-DDSP are correlated with desired performance characteristics (e.g., vibrato, volume, etc). We also show that listeners preferred MIDI-DDSP over existing systems, while enabling ﬁne-grained control of these characteristics. MIDI-DDSP can also connect to other models to construct a full audio generation model, where beginners can obtain realistic novel music from scratch, while expert users can manipulate results based on model prediction to realize unique musical design.
9

Published as a conference paper at ICLR 2022
REFERENCES
MIDI Manufacturers Association et al. The complete midi 1.0 detailed speciﬁcation. Los Angeles, CA, The MIDI Manufacturers Association, 1996.
Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016.
Rachel M Bittner, Brian McFee, Justin Salamon, Peter Li, and Juan Pablo Bello. Deep salience representations for f0 estimation in polyphonic music. In ISMIR, pp. 63–70, 2017.
Adrien Bitton, Philippe Esling, and Tatsuya Harada. Vector-quantized timbre representation. arXiv preprint arXiv:2007.06349, 2020.
Merlijn Blaauw and Jordi Bonada. A Neural Parametric Singing Synthesizer Modeling Timbre and Expression from Natural Songs. Applied Sciences, 7(12):1313, dec 2017. ISSN 2076-3417. doi: 10.3390/app7121313.
Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020.
Sergio Canazza, Giovanni De Poli, Carlo Drioli, Antonio Roda, and Alvise Vidolin. Modeling and control of expressiveness in music performance. Proceedings of the IEEE, 92(4):686–701, 2004.
Rodrigo Castellon, Chris Donahue, and Percy Liang. Towards realistic midi instrument synthesizers. In NeurIPS Workshop on Machine Learning for Creativity and Design (2020), 2020.
Caroline Chan, Shiry Ginosar, Tinghui Zhou, and Alexei A Efros. Everybody dance now. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 5933–5942, 2019.
John M Chowning. The synthesis of complex audio spectra by means of frequency modulation. Journal of the audio engineering society, 21(7):526–534, 1973.
Alexandre De´fossez, Neil Zeghidour, Nicolas Usunier, Le´on Bottou, and Francis Bach. Sing: Symbol-to-instrument neural generator. arXiv preprint arXiv:1810.09785, 2018.
Prafulla Dhariwal, Heewoo Jun, Christine Payne, Jong Wook Kim, Alec Radford, and Ilya Sutskever. Jukebox: A generative model for music. arXiv preprint arXiv:2005.00341, 2020.
Sander Dieleman, Aaron van den Oord, and Karen Simonyan. The challenge of realistic music generation: modelling raw audio at scale. In Advances in Neural Information Processing Systems, pp. 7989–7999, 2018.
Jesse Engel, Kumar Krishna Agrawal, Shuo Chen, Ishaan Gulrajani, Chris Donahue, and Adam Roberts. Gansynth: Adversarial neural audio synthesis. In ICLR, 2019.
Jesse Engel, Lamtharn Hantrakul, Chenjie Gu, and Adam Roberts. DDSP: Differentiable digital signal processing. In International Conference on Learning Representations, 2020a.
Jesse Engel, Rigel Swavely, Lamtharn Hantrakul, Adam Roberts, and Curtis Hawthorne. Selfsupervised pitch detection by inverse audio synthesis. In International Conference on Machine Learning, Self-supervised Audio and Speech Workshop, 2020b.
Ian J Goodfellow, J Pouget-Abadie, M Mirza, B Xu, D Warde-Farley, S Ozair, A Courville, and Y Bengio. Generative adversarial networks. arxiv e-prints. arXiv preprint arXiv:1406.2661, 1406, 2014.
Lamtharn Hantrakul, Jesse H Engel, Adam Roberts, and Chenjie Gu. Fast and ﬂexible neural audio synthesis. In ISMIR, pp. 524–530, 2019.
Curtis Hawthorne, Andriy Stasyuk, Adam Roberts, Ian Simon, Cheng-Zhi Anna Huang, Sander Dieleman, Erich Elsen, Jesse Engel, and Douglas Eck. Enabling factorized piano music modeling and generation with the MAESTRO dataset. In International Conference on Learning Representations, 2019.
10

Published as a conference paper at ICLR 2022
Curtis Hawthorne, Ian Simon, Rigel Swavely, Ethan Manilow, and Jesse Engel. Sequence-tosequence piano transcription with transformers. arXiv preprint arXiv:2107.09142, 2021.
Ben Hayes, Charalampos Saitis, and Gyo¨rgy Fazekas. Neural waveshaping synthesis. arXiv preprint arXiv:2107.05050, 2021.
Sepp Hochreiter and Ju¨rgen Schmidhuber. Long short-term memory. Neural computation, 9(8): 1735–1780, 1997.
Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text degeneration. In International Conference on Learning Representations, 2020.
Cheng-Zhi Anna Huang, Tim Cooijmans, Adam Roberts, Aaron Courville, and Douglas Eck. Counterpoint by convolution. In Proceedings of ISMIR 2017, 2017.
Cheng-Zhi Anna Huang, Hendrik Vincent Koops, Ed Newton-Rex, Monica Dinculescu, and Carrie J. Cai. Ai song contest: Human-ai co-creation in songwriting. ArXiv, abs/2010.05388, 2020.
Nicolas Jonason, Bob Sturm, and Carl Thome´. The control-synthesis approach for making expressive and controllable neural music synthesizers. In 2020 AI Music Creativity Conference, 2020.
Jong Wook Kim, Justin Salamon, Peter Li, and Juan Pablo Bello. Crepe: A convolutional representation for pitch estimation. In 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 161–165. IEEE, 2018.
Jong Wook Kim, Rachel Bittner, Aparna Kumar, and Juan Pablo Bello. Neural music synthesis for ﬂexible timbre control. In ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 176–180. IEEE, 2019.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.
Qiuqiang Kong, Yin Cao, Turab Iqbal, Yuxuan Wang, Wenwu Wang, and Mark D Plumbley. Panns: Large-scale pretrained audio neural networks for audio pattern recognition. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 28:2880–2894, 2020.
Kundan Kumar, Rithesh Kumar, Thibault de Boissiere, Lucas Gestin, Wei Zhen Teoh, Jose Sotelo, Alexandre de Bre´bisson, Yoshua Bengio, and Aaron C Courville. Melgan: Generative adversarial networks for conditional waveform synthesis. In Advances in Neural Information Processing Systems, pp. 14910–14921, 2019.
Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998. doi: 10.1109/5.726791.
Sang-Hoon Lee, Hyun-Wook Yoon, Hyeong-Rae Noh, Ji-Hoon Kim, and Seong-Whan Lee. Multispectrogan: High-diversity and high-ﬁdelity spectrogram generation with adversarial style combination for speech synthesis. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 35, pp. 13198–13206, 2021a.
Wonkwang Lee, Whie Jung, Han Zhang, Ting Chen, Jing Yu Koh, Thomas Huang, Hyungsuk Yoon, Honglak Lee, and Seunghoon Hong. Revisiting hierarchical approach for persistent long-term video prediction. arXiv preprint arXiv:2104.06697, 2021b.
Bochen Li, Xinzhao Liu, Karthik Dinesh, Zhiyao Duan, and Gaurav Sharma. Creating a multitrack classical music performance dataset for multimodal music analysis: Challenges, insights, and applications. IEEE Transactions on Multimedia, 21(2):522–535, 2018.
Pei-Ching Li, Li Su, Yi-Hsuan Yang, Alvin WY Su, et al. Analysis of expressive musical terms in violin using score-informed and expression-based audio features. In ISMIR, pp. 809–815, 2015.
Andrew L Maas, Awni Y Hannun, and Andrew Y Ng. Rectiﬁer nonlinearities improve neural network acoustic models. In Proc. icml, volume 30, pp. 3. Citeseer, 2013.
11

Published as a conference paper at ICLR 2022
Rachel Manzelli, Vijay Thakkar, Ali Siahkamari, and Brian Kulis. Conditioning deep generative raw audio models for structured automatic music. In 19th International Society for Music Information Retrieval Conference, 2018.
Xudong Mao, Qing Li, Haoran Xie, Raymond YK Lau, Zhen Wang, and Stephen Paul Smolley. Least squares generative adversarial networks. In Proceedings of the IEEE international conference on computer vision, pp. 2794–2802, 2017.
Marco Marchini, Rafael Ramirez, Panos Papiotis, and Esteban Maestre. The sense of ensemble: a machine learning approach to expressive performance modelling in string quartets. Journal of New Music Research, 43(3):303–317, 2014.
RG McCurdy. Tentative standards for sound level meters. Electrical Engineering, 55(3):260–263, 1936.
Max Morrison, Zeyu Jin, Justin Salamon, Nicholas J Bryan, and Gautham J Mysore. Controllable neural prosody synthesis. In Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH, volume 2020-Octob, pp. 4437–4441, 2020. doi: 10.21437/Interspeech.2020-2918.
Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. arXiv preprint arXiv:2102.12092, 2021.
Yi Ren, Chenxu Hu, Xu Tan, Tao Qin, Sheng Zhao, Zhou Zhao, and Tie-Yan Liu. Fastspeech 2: Fast and high-quality end-to-end text to speech. arXiv preprint arXiv:2006.04558, 2020.
Curtis Roads. Introduction to granular synthesis. Computer Music Journal, 12(2):11–13, 1988.
Diemo Schwarz. Corpus-based concatenative synthesis. IEEE signal processing magazine, 24(2): 92–104, 2007.
Xavier Serra and Julius Smith. Spectral modeling synthesis: A sound analysis/synthesis system based on a deterministic plus stochastic decomposition. Computer Music Journal, 14(4):12–24, 1990.
Chi-Ching Shih, Pei-Ching Li, Yi-Ju Lin, Yu-Lin Wang, Alvin WY Su, Li Su, and Yi-Hsuan Yang. Analysis and synthesis of the violin playing style of heifetz and oistrakh. In Proceedings of the 20th International Conference on Digital Audio Effects (DAFx-17), 2017.
Aa¨ron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. Wavenet: A generative model for raw audio. In 9th ISCA Speech Synthesis Workshop, pp. 125–125, 2016.
Bryan Wang and Yi-Hsuan Yang. Performancenet: Score-to-audio music generation with multi-band convolutional residual network. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 33, pp. 1174–1181, 2019.
Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao, Jan Kautz, and Bryan Catanzaro. Highresolution image synthesis and semantic manipulation with conditional gans. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 8798–8807, 2018a.
Xin Wang, Shinji Takaki, and Junichi Yamagishi. Autoregressive neural f0 model for statistical parametric speech synthesis. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 26(8):1406–1419, 2018b.
Xin Wang, Shinji Takaki, and Junichi Yamagishi. Neural source-ﬁlter waveform models for statistical parametric speech synthesis. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 28:402–415, 2019.
Bing Xu, Naiyan Wang, Tianqi Chen, and Mu Li. Empirical evaluation of rectiﬁed activations in convolutional network. Proceedings of the International Conference on Machine Learning (ICML) Workshop, 2015.
12

Published as a conference paper at ICLR 2022 Chih-Hong Yang, Pei-Ching Li, AW Su, Li Su, Yi-Hsuan Yang, et al. Automatic violin synthesis
using expressive musical term features. In Proceedings of the 19th International Conference on Digital Audio Effects (DAFx-16), pp. 1–7. Brno, Czech Republic, 2016. Jiangning Zhang, Xianfang Zeng, Yusu Pan, Yong Liu, Yu Ding, and Changjie Fan. Faceswapnet: Landmark guided many-to-many face reenactment. arXiv preprint arXiv:1905.11805, 2, 2019. Hang Zhao, Chuang Gan, Wei-Chiu Ma, and Antonio Torralba. The sound of motions. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 1735–1744, 2019.
13

Published as a conference paper at ICLR 2022
A APPENDIX

Figure 7: The enlarged log-scale Mel spectrograms of synthesis results in Figure 5

B MODEL DETAILS
B.1 DDSP SYNTHESIZER
Differentiable Digital Signal Processing (DDSP) (Engel et al., 2020a) enables differentiable audio synthesis by using a harmonic plus noise model (Serra & Smith, 1990). The harmonic signal is synthesized using a bank of Kh sinusoidal oscillators parameterized by fundamental frequency f0(t), harmonic amplitudes a(t), and harmonic distribution h(t). The noise signal is synthesized by a ﬁltered noise synthesizer parameterized by noise magnitudes η(t). Due to the strong inductive bias introduced by DDSP, the synthesis parameters are highly interpretable, i.e., opposed to some highdimensional learned latent vector producing audio, with DDSP models the network’s output is input to the harmonic plus noise model, whose parameters are interpretable by deﬁnition. For the DDSP synthesis used in this work, s(t) = (f0(t), a(t), h(t), η(t)), where f0(t) ∈ R1×t, a(t) ∈ R1×t, h(t) ∈ R60×t, η(t) ∈ R65×t.
Same as the original DDSP synthesizer, the noise signal is generated by ﬁltering uniform noise given noise magnitudes η(t) in Kn frequency bins. An exponential sigmoid nonlinearity is applied to the raw neural network output to generate the a(t), h(t) and η(t): exp-sigmoid(x) = 2.0 · sigmoid(x)log 10 + , where = 10−7. The harmonic distribution h(t) is further normalized to constrain amplitudes of each harmonic component: Kk=0 hk(t) = 1.
In this paper, we use 60 harmonic bins to synthesize harmonic signal, and 65 magnitude bins to synthesize ﬁltered noise. The frame size is set to 64 samples per frame, and the sample rate of the audio synthesis is set to 16000 Hz. After x(t) is synthesized, a reverb module is applied to x(t) to capture essential reverberation in the environment and instrument. The reverb module is implemented as a frequency domain convolution with a learnable impulse response parameter. This paper uses different reverb parameters for different instruments, and the reverb parameters are learned with back-propagation. The learnable impulse response of the reverb is set to have a length of 48000 sample points. In experiments, we found the latter part of the impulse response, which has minimal impact on the timbre and environmental reverb, would cause a very long lingering sound. Thus, in inference time, we add an exponential decay to the impulse response after 16000 samples to constraint the lingering effect:

IR (t) = IR(t),

0 ≤ t ≤ 16000

IR (t) = IR(t) · exp (−4(t − 16000)), 16000 < t ≤ 48000

(3)

14

Published as a conference paper at ICLR 2022
Figure 8: The effect of modifying the ‘Volume’ and ’Volume Fluctuation’ note expression for a sample. Each row shows the amplitudes of the notes when ﬁxing ’Volume Fluctuation’ and changing ’Volume’, while each column shows the amplitudes of the notes when ﬁxing ’Volume’ and changing ’Volume Fluctuation’. The number indicates the amount of modiﬁcation to the note expression control where (+0, +0) is the original sample. Upper ﬁgure shows the spectrogram of generations, and the bottom ﬁgure shows the amplitude of the generations. The cartoon on the side indicates how the modiﬁed feature would change the synthesis parameters. The gray dash line in the bottom ﬁgure indicates the note boundaries. where IR(t) is the original impulse response, and IR (t) is the impulse response after decay. B.2 DDSP INFERENCE In work proposed by Engel et al. (2020a), an autoencoder is built to reconstruct audio by predicting synthesis parameters from audio features. We refer to this prior work as the “DDSP autoencoder.” Given an audio, fundamental frequency is estimated by CREPE (Kim et al., 2018) model, and the loudness is extracted via an A-weighting of the power spectrum (Hantrakul et al., 2019). Then, the
15

Published as a conference paper at ICLR 2022

Figure 9: The effect of modifying different note expression parameters on an existing sample. The

number indicates the amount of modiﬁcation to the note expression control where (+0) is the original

sample. Upper ﬁgure shows the spectrogram of generations, and the bottom ﬁgure shows the quan-

tity of the generations affected by the control. The cartoon on the side indicates how the modiﬁed

feature would change the synthesis parameters. The gray dash line in the bottom ﬁgure indicates the

note boundaries.

16

Published as a conference paper at ICLR 2022

Figure 10: The architecture of the DDSP Inference. The DDSP Inference module extract f0 and loudness from audio, and use an 8-layer CNN to extract features from Mel-spectrogram. A bidirectional LSTM takes in all features from audio and predict synthesis parameters.

fundamental frequency and loudness are input to a multi-layer perceptron (MLP) respectively. The output of the MLPs are concatenated and passed to a uni-directional GRU. Finally, an MLP is used to predict synthesis parameters from GRU output.
Our DDSP Inference module differs from the above DDSP autoencoder by enabling more information extracted from audio input. The architecture of the DDSP Inference is shown in Figure 10. In addition to fundamental frequency estimated by CREPE and loundess extracted via A-weighting of the power spectrum, an 8-layer convolutional neural network (CNN) (Lecun et al., 1998) is used to extract features from log-scale Mel-spectrogram, and a bi-directional long-short term memory network (LSTM) (Hochreiter & Schmidhuber, 1997) is then applied to extract contextual features. The use of CNN applied on log-scale Mel-spectrogram help model to extract more information from audio input, thus enabling more accurate synthesis parameter estimation.
In our DDSP Inference module, a fully-connected network is applied on fundamental frequency and loudness. The output is concatenated with the output of CNN, send to the bi-directional LSTM to extract contextual features. Another fully connected layer are used to map the features to synthesis parameters. The CNN network in the DDSP Inference module is similar to the one used by Kong et al. (2020) for computational efﬁciency. The detailed architecture of the CNN is shown in Table 3.
The DDSP Inference Module is optimized via Adam optimizer in a batch size of 16 and a learning rate of 3e − 4. We choose a log-scale Mel-spectrogram with 64 frequency dimensions in this work to extract features by CNN. The features extracted by CNN are mapped to 256 dimensions, and concatenated to the features extracted from the fundamental frequency and loudness. When trained in the multi-instrument setting, a 64 dimensional instrument embedding is also concatenated to the aforementioned feature vectors. The bi-directional LSTM has a hidden dimension of 256.
B.3 EXPRESSION CONTROLS
Given frame-wise extracted synthesis parameters s(t) for the ith note, ni, in a note sequence, we say that the ith note starts at frame Ton and ends at frame Toff. We deﬁne τ ∈ [Ton, Toff] to be every frame that a note is active and the total frame duration of the note is Tn = Toff − Ton. The synthesis parameters for a the whole duration of the note are deﬁned by a fundamental frequency f0(τ ) contour, an amplitude contour a(τ ), a harmonic distribution h(τ ), and a set of noise magnitudes η(τ ).
The amplitude contour, a(τ ), and noise magnitudes η(τ ) are transformed into dB scale:

a (τ ) = 20 log10 a(τ ), η (τ ) = 20 log10 η(τ )

(4)

The note expression controls are extracted as follows:

17

Published as a conference paper at ICLR 2022

ConvBlock
Conv2d Batch norm + ReLU Conv2d Batch norm + ReLU Average pooling

Kernel Size
(3,3) -
(3,3) -
(1,2)

Stride
(1,1) -
(1,1) -

Filter Size
KF ilters -
KF ilters -

Mel-CNN
LogMelSpectrogram ConvBlock Dropout ConvBlock Dropout ConvBlock Dropout ConvBlock Reshape Dense

Output Size
(1000, 64, 1) (1000, 32, 64) (1000, 32, 64) (1000, 16, 128) (1000, 16, 128) (1000, 8, 256) (1000, 8, 256) (1000, 4, 512) (1000, 2048) (1000, 256)

Filter Size
64 128 256 512 256

Dropout Rate
0.2 0.2 0.2 -

Table 3: The architecture of the Mel-CNN (bottom) used to extract features from log-scale Melspectrogram in the DDSP Inference module. The Mel-CNN uses convolutinal blocks deﬁned in the ﬁrst table below.

Volume is the sum of the amplitude contour (in dB) normalized over the length of the note, i.e., the

mean amplitude over the note,

1 Tn

a (i).

(5)

Tn i=1

Volume ﬂuctuation measures the standard deviation of the amplitude curve (in dB):

1 Tn

(a (i) − a (τ ))2.

(6)

Tn i=1

where a (τ ) is the mean amplitude over the whole note.
Volume peak position is the location (in time) of the highest amplitude value normalized over the length of the note,
1 Tn arg mi ax a (i) ∀i ∈ [1, Tn]. (7)
Vibrato is inspired by previous works on expressive performance analysis (Marchini et al., 2014; Li et al., 2015; Yang et al., 2016). The vibrato is calculated by applying Discrete Fourier Transform (DFT) to the fundamental frequency sequence:

max F {f0(t)}i,

(8)

i

where F{·} deﬁnes the DFT function. Only notes with a vibrato rate between 3 to 9 Hz and longer than 200ms are recorded. Otherwise, the vibrato of the note is set to 0. In calculating the DFT function, the fundamental frequency, f0(t), is zero-padded to 1000 frames.
Brightness is deﬁned as the spectral centroid (in bin numbers) of the harmonic distribution,
1 Tn |h| k · hk(i), (9) Tn i k=1

18

Published as a conference paper at ICLR 2022

Figure 11: The architecture of the Synthesis Generator. The Synthesis Generator is a GAN whose generator (left) takes in per-note Expression Controls and instrument embedding as a conditioning sequence (red box, left) and produces DDSP synthesis parameters, i.e., f0, Amplitudes, Harmonic Distribution, and Noise Magnitudes (green boxes, middle). These synthesis parameters get turned into audio by the DDSP modules.

where hk(i) represents the k-th bin of the harmonic distribution, h(τ ) in the i-th time-step, and we use |h| to refer to the number of bins in the harmonic distribution used by the DDSP module (we use |h| = 60, see Appendix B.2).
Attack Noise refers to the amount of noise that occurs at the beginning of a note. Many instruments have a high amount of noise in the ﬁrst few milliseconds of a note (e.g., a bow scraping across a violin string), before the harmonic components are heard. We determine attack noise like

1 N |η| k

η (i),

(10)

N i=1 k=1

where N determines how many of the ﬁrst few frames we look at to determine the attack noise (we set N = 10, corresponding to 40ms), η k(i) represents the k-th bin of the dB-scaled noise magnitudes in the i-th time-step, and |η| is the number of noise magnitude bins in the DDSP module (set to |η| = 65, see Appendix B.2).
Recall that all expression controls are normalized to be in the interval [0.0, 1.0], concatenated to a 6-dimensional vector, and repeated for the full frame duration of the note, Tn, before we use them with the MIDI-DDSP networks. See Sections 3.2 and 3.3 for additional information. We note that these are not the only ways to determine the “expression” of a note, nor are our deﬁnitions deﬁnitive. Expression does not even need to be hand designed, and could perhaps be learned in an unsupervised way by neural networks. However, we designed our expression controls because they are all inherently interpretable (compared to black box neural net features). We leave the exploration of other ways to deﬁne expression for future work.
For constructing a conditioning sequence from these note expressions and an input note sequence, the frame-wise note expressions and note pitch is expanded to a note-length sequence by repeating these parameters for the number of frames the note occupies. Then, these note expression and pitch parameters are concatenated with note onsets and offsets (a binary ﬂag at the start and end of a note, respectively), and a scalar note positioning code to provide additional information about note boundaries.

B.4 SYNTHESIS GENERATOR
The Synthesis Generator is a Generative Adversarial Network (GAN) Goodfellow et al. (2014) whose generator takes in the per-note Expression Controls, and produces the DDSP synthesis parameters. The architecture of the Synthesis Generator is shown in Figure 11. The Synthesis Generator

19

Published as a conference paper at ICLR 2022

itself contains two networks: an autoregressive RNN, and a stack of dilated 1-D convolutions. The autoregressive RNN that generates an f0 curve based on a note sequence (e.g., MIDI) and the corresponding Note Expression Controls (described in Section B.3). The CNN stack then generates an amplitude curve, a harmonic distribution, and noise magnitudes which are passed to the DDSP modules for synthesis. The generated f0 curve, amplitude curve, harmonic distribution, and noise magnitudes are all passed to a discriminator.
f0 Generation using the Autoregressive RNN The autoregressive RNN for f0 generation consists of a single-layer Bi-LSTM and a 2-layer GRU that autoregressively samples the note’s exact pitch, by predicting f0 frequency offset (in units of semitones) with respect to a known f0 MIDI note number for the current MIDI note. For example, if at some frame the note speciﬁed by the note sequence is A4, which has a MIDI number2 of f0A4 = 69, and the ground truth f0 in the audio is f0GT = 69.2, the autoregressive RNN is expected to predict f0GT − f0A4 = 0.2, indicating that the f0 at the current frame should be 0.2 semitones above the f0 determined by the current MIDI note, f0A4 = 69. The autoregressive RNN outputs a categorical distribution of pitch offsets in the range of [−1.00, 1.00] semitones, quantized to 201 bins, where each bin represents 0.01 semitone. The ﬁnal frequency, f0, input to DDSP synthesizer is converted from semitone units to Hz using f (n) = 440 · 2(n−69)/12, where f is the frequency in Hz, and n is the integer MIDI note number. Both the single-layer Bi-LSTM and a 2-layer GRU in the Synthesis Generator have a hidden dimension of 256. At inference time, the pitch offset is sampled using nucleus sampling (Holtzman et al., 2020) with p = 0.95 to avoid sudden unrealistic change in fundamental frequency contour. Similar approaches for using autoregressive RNNs to sample f0 curves has been proposed for speech synthesis and conversion (Wang et al., 2018b; Morrison et al., 2020).
Generating the Rest of the Synthesis Parameters A stack of dilated 1-D convolution layers, a la WaveNet (van den Oord et al., 2016), is used to generate the rest of synthesis parameters, i.e., the base amplitude for the note, a(t), the harmonic amplitudes, h(t), and the noise magnitudes η(t). This network uses the predicted f0(t), and concatenated expression control vector, c(t), as conditioning. This network consists of 4 convolutional stacks, with each stack having ﬁve layers of 1-D convolution with exponentially increasing dilation rate followed by ReLU activation (Maas et al., 2013) and layer normalization (Ba et al., 2016). For clarity, we refer to this network as the “Dilated CNN.”
The conditioning sequence input is mapped to a 256-dimensional vector by a linear layer before being sent into an autoregressive RNN and dilated convolution network. If trained on the multiinstrument dataset, a 64-dimension instrument embedding is concatenated after the linear layer. A dropout of rate 0.5 is applied to all GRU units, and during training the input is teacher-forced to avoid overﬁtting and exposure bias.
The full details of the dilated convolutional network architecture are shown in Table 4.
Discriminator for the Synthesis Generator The architecture of the discriminator used with the Synthesis Generator is shown in Figure 12, and the detailed architecture of each discriminator block is shown in Table 5. The discriminator is motivated by the multi-scale discriminator network in previous works generating waveforms and spectrograms (Kumar et al., 2019; Lee et al., 2021a). It consists of 3 discriminator networks with same architecture that take input signals with different downsample rate in an effort to learn the features of synthesis parameters at different time resolutions. Each discriminator network consists of 4 blocks at each scale, and each block extracts features from predicted synthesis parameters and the conditioning sequence. For each feature stream in a block, two 1-D convolutional layers are used with Leaky-ReLU activation function (Xu et al., 2015), with skip connections and layer normalization (Ba et al., 2016).
Synthesis Generator Losses The Synthesis Generator is trained by minimizing both reconstruction loss and adversarial loss:

L = Lrecon + αLadv = (LCE(f0) + Lspec) + αLadv,

(11)

2https://www.inspiredacoustics.com/en/MIDI_note_numbers_and_center_ frequencies

20

Published as a conference paper at ICLR 2022

Dilated Stack

Conv1d ReLU + layer Add residual Conv1d ReLU + layer Add residual Conv1d ReLU + layer Add residual Conv1d ReLU + layer Add residual Conv1d ReLU + layer Add residual

norm norm norm norm norm

Kernel Size
3 3 3 3 3 -

Dilation Rate
1 2 4 8 16 -

Stride
1 1 1 1 1 -

Filter Size
KF ilters -
KF ilters -
KF ilters -
KF ilters -
KF ilters -

Dilated CNN
Conditioning Sequence Conv1d Dilated Stack Dilated Stack Dilated Stack Dilated Stack Layer norm Dense

Output Size
(1000, 384) (1000, 128) (1000, 128) (1000, 128) (1000, 128) (1000, 128) (1000, 128) (1000, 126)

Kernel Size
3 3 3 3 3 -

Dilation Rate
1 -

Stride
1 1 1 1 1 -

Filter Size
128 128 128 128 128
126

Table 4: The architecture of dilated convolution network (bottom) used in the Synthesis Generator to generate amplitude, harmonic distribution and noise magnitude. The dilated convolution network uses dialted stack layers deﬁned in the ﬁrst table below.

Discriminator Block conditioning sequence
Conditioning Sequence Conv1d Add output from Discriminator Block of synthesizer parameters Leaky ReLU Add residual and layer norm

Output Size
(Tin, Dc) (Tin/2, 256)
(Tin/2, 256) (Tin/2, 256) (Tin/2, 256)

Kernel Size
3
-

Stride
2
-

Filter Size
256
-

Discriminator Block synthesis parameters
synthesis parameters Conv1d Leaky ReLU Add residual and layer norm

Output Size
(Tin, Ds) (Tin/2, 256) (Tin/2, 256) (Tin/2, 256)

Kernel Size
3 -

Stride
2 -

Filter Size
256
-

Table 5: Details of the discriminator blocks used in the Synthesis Generator.

where Lrecon is the reconstruction loss, Ladv is the adversarial loss, and α is a hyperparameter that controls the weight of the adversarial loss term. The reconstruction loss, Lrecon, consists of two pieces: a cross-entropy loss, LCE(f0), on the f0 to train the autoregressive RNN, and a multi-scale spectral loss, Lspec to train the Dilated CNN.
21

Published as a conference paper at ICLR 2022

Figure 12: The architecture of the discriminator used in the Synthesis Generator.

The cross-entropy loss for the autoregressive RNN is deﬁned as

LCE(f0) = − f0i log fˆ0i,

(12)

i

where i is the length of the entire sequence in frames, f0 is the ground truth fundamental frequency curve, and fˆ0 is the estimated fundamental frequency curve.
The multi-scale spectral loss, Lspec, is used for the reconstruction loss. This loss is used for reconstruction in the original DDSP paper (Engel et al., 2020a). The multi-scale spectral loss computes the L1 difference between the magnitude spectrogram of the predicted and target audio by comparing the computed spectrograms at a number of different FFT sizes. Given the magnitude spectrogram of the predicted audio Sˆi and that of the target audio Si with FFT size i, the multi-scale spectral loss computes the L1 difference between Sˆi and Si as well as log Sˆi and log Si:

L(sip)ec = ||Si − Sˆi||1 + β|| log Si − log Sˆi||1,

(13)

Lspec = L(sip)ec ∀i ∈ {2048, 1024, 512, 256, 128, 64}.

(14)

i

The adversarial loss, Ladv, used to train the dilated CNN in the Synthesis Generator. This loss combines a least-squares GAN (LSGAN) (Mao et al., 2017) objective and a feature matching objective objective (Kumar et al., 2019):

Ladv = Llsgan + γLfm.

(15)

Given discriminator network Dk in k-th scale, the LSGAN objective to train the Synthesis Generator can be written as

Llsgan = Ec

||Dk(sˆ, c) − 1||2 ,

(16)

k

and the LSGAN objective for training the discriminator is given by

min E [||Dk(s, c) − 1||2 + ||Dk(sˆ, c)||2] , ∀k.

(17)

Dk

In this work, we use 3 scales, so k = [1, 2, 3]. Given the output of i-th feature map from one of the 4 layers of the k-th discriminator Dk, the feature map matching objective is calculated as L1 difference between corresponding feature map:

22

Published as a conference paper at ICLR 2022

Figure 13: The effect of GAN in overcoming the “over-smoothing” problem in harmonic distribution generation. From top to bottom: (top) the ground-truth harmonic distribution of the test-set sample, (middle) the harmonic distribution of the same sample predicted without a discriminator used in training, (bottom) the harmonic distribution predicted with discriminator and adversarial training.

4 1 (i)

(i)

Lfm = Es,c

Ni ||Dk (s, c) − Dk (sˆ, c)||1 ,

(18)

i=1

where Ni is the number of units in i-th layer of the feature map.
In training, we use a stop gradient between the adversarial loss and the autoregressive RNN. That is, the RNN is trained by the cross-entropy loss only. The Synthesis Generator is trained using 4 seconds of audio with a frame length of 4ms, resulting in a sequence length of 1000. The Synthesis Generator is optimized via Adam optimizer in a batch size of 16 and a learning rate of 0.0003, with an exponential learning rate decay at a rate of 0.99 per 1000 steps. The discriminator is optimized using Adam optimizer in a batch size of 16 and a learning rate of 0.0001. α = 1, β = 1, and γ = 10 are used for loss coefﬁcients.

B.5 ON THE IMPROVEMENT OF USING GAN FOR THE SYNTHESIS GENERATOR
Without GAN training, the Synthesis Generator will suffer from the “over-smoothing” problem where the model generates uniform and smoothed harmonic distribution and noise magnitudes over the whole note. Figure 13 shows this effect qualitatively. With the introduction of adversarial training, the proposed model overcomes the over-smoothing problem potentially caused by one-tomany mapping.

B.6 EXPRESSION GENERATOR
The Expression Generator creates a set of Expression Controls (see Section B.3) given an input note sequence. The architecture of the Expression Generator is shown in Figure 14. The Expression Generator consists of two parts: a single-layer bidirectional GRU extracts context information from input, and a two-layer autoregressive GRU generates note expression.

23

Published as a conference paper at ICLR 2022
Figure 14: The architecture of the Expression Generator. The Expression Generator consists of two parts: a single-layer bidirectional GRU extracts context information from input, and a two-layer autoregressive GRU generates note expression. The input to the bi-directional GRU is a concatenation of pitch embedding vector, duration feature vector and instrument embedding.
In the input to the Expression Generator, the discrete pitch is mapped into a feature vector of 64 dimensions via an embedding layer, and the scalar duration is mapped into a feature vector of 64 dimensions via a fully-connected layer. The instrument embedding input is a 64 dimension feature vector. The input to the bi-directional GRU is a concatenation of pitch embedding vector, duration feature vector and instrument embedding. The bi-directional GRU and auto-regressive GRU all use a hidden size of 128 and a dropout rate of 0.5. Input dropout with a rate of 0.5 is applied to the teacher-forced input in training time to avoid over-ﬁtting and exposure bias. The output layer of the Expression Generator is a two-layer Multi-layer Perceptron (MLP), which consists of a fully connected layer with a layer normalization before the ReLU nonlinearity used in Engel et al. (2020a). Data augmentation is applied in the training of the Expression Generator. For the input note sequence, we randomly transpose {−3, −2, −1, 0, 1, 2, 3} semitone(s) and randomly stretch the duration by a factor of {0.9, 0.95, 1, 1.05, 1.1}. The Expression Generator is trained on a sequence length of 64 notes and a batch size of 256. Adam optimizer (Kingma & Ba, 2014) is used in training with a learning rate of 0.0001. For training the Expression Generator, we use the mean-square loss (MSE) between the estimated Expression Controls and the ground truth Expression Controls (see Section B.3).
B.7 OTHER TRAINING DETAILS
The Expression Generator, Synthesis Generator, and DDSP Inference Module are trained separately. The Expression Generator is trained for 5000 steps, the Synthesis Generator is trained for 40000 steps, and the DDSP Inference Module is trained for 10000 steps.
C DATASET
The 13 instruments in the URMP dataset are violin, viola, cello, double bass, ﬂute, oboe, clarinet, saxophone, bassoon, trumpet, horn, trombone, tuba. In this paper, we regard both the soprano saxophone and tenor saxophone in URMP dataset as saxophone. The recordings in the dataset have a sample rate of 48kHz but we downsampled them to 16kHz to match the sample rate of the DDSP synthesis module. To train the Synthesis Generator and DDSP Inference, we segmented the recordings into 4 seconds clips with 50% overlap. In the URMP dataset, the solo recordings are part of ensemble pieces. Splitting the same piece played by different instruments into training and test sets can cause data leakage. Thus, we split the dataset based on a random shufﬂe of the recordings, and then moved pieces post-hoc such that the same piece does not appear in both training and test set.
24

Published as a conference paper at ICLR 2022
D EXPERIMENT DETAILS
D.1 DETAILS OF BASELINE METHODS We use the orchestral strings pack in Ableton3 without any manual adjustment to synthesize the audio from Ableton. For FluidSynth, we used the FluidR3 GM.sf24 sound font. We reimplemented the MIDI2Param system proposed by (Castellon et al., 2020) by following the ofﬁcial implementation on GitHub5. The MIDI2Params system is only trained in the single instrument setting for the listening test experiments as is done in the original paper. However, as a comparison in our multi-instrument scenario, we trained a multi-instrument MIDI2Params model that had an additional instrument embedding concatenated to the model input. D.2 DETAILS OF THE LISTENING TEST In total, we gathered 960 ratings based on 360 pairwise comparisons from 14 participants for our listening test. Participants were presented with two unlabeled audio clips and asked “Which one of the following recordings sound most like a person playing it on a real violin?” Participants were asked to wear headphones, and were able to listen to the audio clips as many times as they pleased before submitting their answers. The participants chose their preference using a 5-point Likert-like scale, with the ﬁrst option being ”Strong preference for Audio Clip 1”, the middle option indicating ”No preference”, and the ﬁnal option being ”Strong preference for Audio Clip 2.” The pairs of unlabeled clips were drawn from the set of [Ground-truth, DDSP Inference, MIDIDDSP, Ableton, MIDI2Params, Fluidsynth]. Participants were recruited through a Google internal data labeling platform, and were selected based on a pre-screening pilot study to ensure that the participant was able to provide reasonable evaluations of audio recordings. In this pilot study, we ﬁltered out individuals who rated FluidSynth examples as sounding more like a real violin recording than the Ground-truth violin recording. Participants were not screened based on their musical background. A Kruskal-Wallis H test of the ratings showed that there is at least one statistically signiﬁcant difference between the models: χ2(2) = 395.35, p < 0.01. The number of wins for each pair comparison and a Wilcoxon signed-rank test for each pair is shown in Table 6.
3https://www.ableton.com/en/packs/orchestral-strings/ 4https://member.keymusician.com/Member/FluidR3_GM/index.html 5https://github.com/rodrigo-castellon/midi2params
25

Published as a conference paper at ICLR 2022

Pairs

wins ties losses p value

Ableton

MIDI2Params

39 1

Ableton

DDSP Inference 11 0

Ableton

FluidSynth

42 0

Ableton

Ground-truth

14 1

Ableton

MIDI-DDSP

50

MIDI2Params DDSP Inference 8 0

MIDI2Params FluidSynth

31 0

MIDI2Params Ground-truth

80

MIDI2Params MIDI-DDSP

80

DDSP Inference FluidSynth

55 0

DDSP Inference Ground-truth

35 0

DDSP Inference MIDI-DDSP

44 0

FluidSynth

Ground-truth

40

FluidSynth

MIDI-DDSP

90

Ground-truth

MIDI-DDSP

44 0

24 7.61e-5 53 2.98e-27 22 0.0002 49 6.62e-26 59 5.34e-16 56 5.73e-42 33 0.027* 56 2.43e-40 56 7.16e-28 9 2.97e-39 29 0.024* 20 6.02e-05 60 1.00e-37 55 7.25e-26 20 0.00018

Table 6: A post-hoc comparison of each pair on their pairwise comparisons with each other, using
the Wilcoxon signed-rank test for matched samples (“win” means the ﬁrst item in the pair is selected) . p value less than 0.01/15 = 6.67×10−4 yields a statistically signiﬁcant difference. Only two pairs
are not signiﬁcantly different (DDSP Inference vs. Ground-truth, MIDI2Params vs. FluidSynth),
and are marked with an asterisk (*).

E EXPRESSION ATTRIBUTE CONTROLS

violin viola cello double bass ﬂute oboe clarinet saxophone bassoon trumpet horn trombone tuba

Volume
.99 .98 .97 .98 .99 .97 .98 .97 .99 .97 .97 .97 .98

Vol. Fluc.
.84 .74 .64 .85 .87 .79 .88 .71 .90 .88 .88 .93 .91

Vol. Peak Pos.
.80 .70 .54 .34 .48 .72 .63 .43 .56 .55 .41 .59 .15

Vibrato
.86 .82 .74 .84 .63 .91 .72 .80 .91 .73 .64 .52 .22

Brightness
.96 .98 .98 .99 .90 .87 .88 .94 .99 .92 .94 .99 .98

Attack Noise
.97 .97 .94 .95 .97 .97 .97 .96 .96 .92 .95 .96 .93

Table 7: The Pearson correlation result of all instruments described in Table 2. The Pearson correlation r-values are shown in the table, while p-values are omitted as in all entries p < 0.0001. We consider Pearson r-values larger than 0.7 (marked on bold) to mean that our input controls are strongly correlated with the output. For Volume, Brightness, and Attack Noise, all instruments show a strong correlation; for Volume Fluctuation, and Vibrato, the majority of instruments sho a strong correlation.

F ACKNOWLEDGEMENTS AND OPEN-SOURCE IMAGE ATTRIBUTIONS
We would like to thank Yi Ren for the discussion on Generative Adversarial Nets for audio generation. We would like to thank Shujun Han for providing advice on making and arranging ﬁgures in the paper. We would also like to thank Mark Cartwright for his feedback on the paper and fruitful discussions about it.
26

Published as a conference paper at ICLR 2022 The icons used throughout the paper are used under the Creative Commons license via the Noun Project. We gratefully acknowledge the following creators of these images:
• Audio by cataicon from the Noun Project. • bassoon by Symbolon from the Noun Project. • Clarinet by Symbolon from the Noun Project. • composer by Pham Duy Phuong Hung from the Noun Project. • Flute by Symbolon from the Noun Project. • Neural Network by Ian Rahmadi Kurniawan from the Noun Project. • oboe by Symbolon from the Noun Project. • Synthesizer by Jino from the Noun Project. • Violin by Olena Panasovska from the Noun Project. • Violinist by Luis Prado from the Noun Project.
27

