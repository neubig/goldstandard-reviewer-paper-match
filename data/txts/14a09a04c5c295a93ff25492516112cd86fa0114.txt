Multi-space Variational Encoder-Decoders for Semi-supervised Labeled Sequence Transduction
Chunting Zhou, Graham Neubig Language Technologies Institute
Carnegie Mellon University ctzhou,gneubig@cs.cmu.edu

arXiv:1704.01691v2 [cs.CL] 17 Aug 2017

Abstract
Labeled sequence transduction is a task of transforming one sequence into another sequence that satisﬁes desiderata speciﬁed by a set of labels. In this paper we propose multi-space variational encoderdecoders, a new model for labeled sequence transduction with semi-supervised learning. The generative model can use neural networks to handle both discrete and continuous latent variables to exploit various features of data. Experiments show that our model provides not only a powerful supervised framework but also can effectively take advantage of the unlabeled data. On the SIGMORPHON morphological inﬂection benchmark, our model outperforms single-model state-ofart results by a large margin for the majority of languages.1
1 Introduction
This paper proposes a model for labeled sequence transduction tasks, tasks where we are given an input sequence and a set of labels, from which we are expected to generate an output sequence that reﬂects the content of the input sequence and desiderata speciﬁed by the labels. Several examples of these tasks exist in prior work: using labels to moderate politeness in machine translation results (Sennrich et al., 2016), modifying the output language of a machine translation system (Johnson et al., 2016), or controlling the length of a summary in summarization (Kikuchi et al., 2016). In particular, however, we are motivated by the task of morphological reinﬂection (Cotterell et al.,
1An implementation of our model are available at https://github.com/violet-zct/ MSVED-morph-reinflection.

POS=Verb, Tense=Past

playing

played

Model Supervised Learning

plays Semi-Supervised Learning
Figure 1: Standard supervised labeled sequence transduction, and our proposed semi-supervised method.

2016), which we will use as an example in our description and test bed for our models.
In morphologically rich languages, different afﬁxes (i.e. preﬁxes, inﬁxes, sufﬁxes) can be combined with the lemma to reﬂect various syntactic and semantic features of a word. The ability to accurately analyze and generate morphological forms is crucial to creating applications such as machine translation (Chahuneau et al., 2013; Toutanova et al., 2008) or information retrieval (Darwish and Oard, 2007) in these languages. As shown in 1, re-inﬂection of an inﬂected form given the target linguistic labels is a challenging subtask of handling morphology as a whole, in which we take as input an inﬂected form (in the example, “playing”) and labels representing the desired form (“pos=Verb, tense=Past”) and must generate the desired form (“played”).
Approaches to this task include those utilizing hand-crafted linguistic rules and heuristics (Taji et al., 2016), as well as learning-based approaches using alignment and extracted transduction rules (Durrett and DeNero, 2013; Alegria and Etxeberria, 2016; Nicolai et al., 2016). There have also been methods proposed using neural sequenceto-sequence models (Faruqui et al., 2016; Kann et al., 2016; Ostling, 2016), and currently ensembles of attentional encoder-decoder models (Kann and Schu¨tze, 2016a,b) have achieved state-of-art results on this task. One feature of these neural models however, is that they are trained in a

largely supervised fashion (top of Fig. 1), using data explicitly labeled with the input sequence and labels, along with the output representation. Needless to say, the ability to obtain this annotated data for many languages is limited. However, we can expect that for most languages we can obtain large amounts of unlabeled surface forms that may allow for semi-supervised learning over this unlabeled data (entirety of Fig. 1).2
In this work, we propose a new framework for labeled sequence transduction problems: multi-space variational encoder-decoders (MSVED, §3.3). MSVEDs employ continuous or discrete latent variables belonging to multiple separate probability distributions3 to explain the observed data. In the example of morphological reinﬂection, we introduce a vector of continuous random variables that represent the lemma of the source and target words, and also one discrete random variable for each of the labels, which are on the source or the target side.
This model has the advantage of both providing a powerful modeling framework for supervised learning, and allowing for learning in an unsupervised setting. For labeled data, we maximize the variational lower bound on the marginal log likelihood of the data and annotated labels. For unlabeled data, we train an auto-encoder to reconstruct a word conditioned on its lemma and morphological labels. While these labels are unavailable, a set of discrete latent variables are associated with each unlabeled word. Afterwards we can perform posterior inference on these latent variables and maximize the variational lower bound on the marginal log likelihood of data.
Experiments on the SIGMORPHON morphological reinﬂection task (Cotterell et al., 2016) ﬁnd that our model beats the state-of-the-art for a single model in the majority of languages, and is particularly effective in languages with more complicated inﬂectional phenomena. Further, we ﬁnd that semi-supervised learning allows for signiﬁcant further gains. Finally, qualitative evaluation of lemma representations ﬁnds that our model is able to learn lemma embeddings that match with human intuition.
2Faruqui et al. (2016) have attempted a limited form of semi-supervised learning by re-ranking with a standard ngram language model, but this is not integrated with the learning process for the neural model and gains are limited.
3Analogous to multi-space hidden Markov models (Tokuda et al., 2002)

2 Labeled Sequence Transduction

In this section, we ﬁrst present some notations re-

garding labeled sequence transduction problems in

general, then describe a particular instantiation for

morphological reinﬂection.

Notation: Labeled sequence transduction problems involve transforming a source sequence x(s) into a target sequence x(t), with some labels

describing the particular variety of transforma-

tion to be performed. We use discrete variables

y1(t), y2(t), · · · , yK(t) to denote the labels associated

with each target sequence, where K is the total

number

of

labels.

Let

y(t)

=

[

y

(t) 1

,

y2(t)

,

·

·

·

, yK(t)]

denote a vector of these discrete variables. Each discrete variable yk(t) represents a categorical fea-

ture pertaining to the target sequence, and has a

set of possible labels. In the later sections, we also use y(t) and yk(t) to denote discrete latent variables corresponding to these labels.

Given a source sequence x(s) and a set of associated target labels y(t), our goal is to generate a target sequence x(t) that exhibits the features speciﬁed by y(t) using a probabilistic model p(x(t)|x(s), y(t)). The best target sequence xˆ(t) is

then given by:

xˆ(t) = arg max p(x(t)|x(s), y(t)). (1)
x(t)

Morphological Reinﬂection Problem: In morphological reinﬂection, the source sequence x(s)
consists of the characters in an inﬂected word (e.g., “played”), while the associated labels y(t) describe some linguistic features (e.g., yp(to)s = Verb, yt(etn)se = Past) that we hope to realize in the target. The target sequence x(t) is there-
fore the characters of the re-inﬂected form of the
source word (e.g., “played”) that satisfy the linguistic features speciﬁed by y(t). For this task, each discrete variable yk(t) has a set of possible labels (e.g. pos=V, pos=ADJ, etc) and follows a
multinomial distribution.

3 Proposed Method
3.1 Preliminaries: Variational Autoencoder
As mentioned above, our proposed model uses probabilistic latent variables in a model based on neural networks. The variational autoencoder (Kingma and Welling, 2014) is an efﬁcient way to handle (continuous) latent variables in neural

z

zy zy

z

y(t)

z

y(t)

x
(a) VAE

x
(b) Labeled MSVAE

x
(c) MSVAE

(s)

(t)

x

x

(d) Labeled MSVED

(s)

(t)

x

x

(e) MSVED

Figure 2: Graphical models of (a) VAE, (b) labeled MSVAE, (c) MSVAE, (d) labeled MSVED, and (e) MSVED. White circles are latent variables and shaded circles are observed variables. Dashed lines indicate the inference process while the solid lines indicate the generative process.

models. We describe it brieﬂy here, and interested readers can refer to Doersch (2016) for details. The VAE learns a generative model of the probability p(x|z) of observed data x given a latent variable z, and simultaneously uses a recognition model q(z|x) at learning time to estimate z for a particular observation x (Fig. 2(a)). q(·) and p(·) are modeled using neural networks parameterized by φ and θ respectively, and these parameters are learned by maximizing the variational lower bound on the marginal log likelihood of data:

log pθ(x) ≥ Ez∼qφ(z|x)[log pθ(x|z)]−

KL(qφ(z|x)||p(z))

(2)

The KL-divergence term (a standard feature of variational methods) ensures that the distributions estimated by the recognition model qφ(z|x) do not deviate far from our prior probability p(z) of the values of the latent variables. To optimize the parameters with gradient descent, Kingma and Welling (2014) introduce a reparameterization trick that allows for training using simple backpropagation w.r.t. the Gaussian latent variables z. Speciﬁcally, we can express z as a deterministic variable z = gφ( , x) where is an independent Gaussian noise variable ∼ N (0, 1). The mean µ and the variance σ2 of z are reparameterized by the differentiable functions w.r.t. φ. Thus, instead of generating z from qφ(z|x), we sample the auxiliary variable and obtain z = µφ(x) + σφ(x) ◦ , which enables gradients to backpropagate through φ.

3.2 Multi-space Variational Autoencoders
As an intermediate step to our full model, we next describe a generative model for a single sequence with both continuous and discrete latent variables, the multi-space variational auto-encoder (MSVAE). MSVAEs are a combination of two threads of previous work: deep generative models with both continuous/discrete latent variables for classiﬁcation problems (Kingma et al., 2014;

Maaløe et al., 2016) and VAEs with only continuous variables for sequential data (Bowman et al., 2016; Chung et al., 2015; Zhang et al., 2016; Fabius and van Amersfoort, 2014; Bayer and Osendorfer, 2014). In MSVAEs, we have an observed sequence x, continuous latent variables z like the VAE, as well as discrete variables y.
In the case of the morphology example, x can be interpreted as an inﬂected word to be generated. y is a vector representing its linguistic labels, either annotated by an annotator in the observed case, or unannotated in the unobserved case. z is a vector of latent continuous variables, e.g. a latent embedding of the lemma that captures all the information about x that is not already represented in labels y.
MSVAE: Because inﬂected words can be naturally thought of as “lemma+morphological labels”, to interpret a word, we resort to discrete and continuous latent variables that represent the linguistic labels and the lemma respectively. In this case when the labels of the sequence y is not observed, we perform inference over possible linguistic labels and these inferred labels are referenced in generating x.
The generative model pθ(x, y, z) = p(z)pπ(y)pθ(x|y, z) is deﬁned as:

p(z) = N (z|0, I)

(3)

pπ(y) = Cat(yk|πk)

(4)

k

pθ(x|y, z) = f (x; y, z, θ).

(5)

Like the standard VAE, we assume the prior of the latent variable z is a diagonal Gaussian distribution with zero mean and unit variance. We assume that each variable in y is independent, resulting in a factorized distribution in Eq. 4, where Cat(yk|πk) is a multinomial distribution with parameters πk. For the purposes of this study, we set these to a uniform distribution πk,j = |π1k| . f (x; y, z, θ) calculates the likelihood of x, a function parametrized by deep neural networks. Speciﬁcally, we employ an RNN decoder to generate the target word conditioned on the lemma variable z and linguistic labels y, detailed in §5.
When inferring the latent variables from the given data x, we assume the joint distribution of latent variables z and y has a factorized form, i.e. q(z, y|x) = q(z|x)q(y|x) as shown in Fig. 2(c).

The inference model is deﬁned as follows:

qφ(z|x) = N (z|µφ(x), diag(σφ2(x))) (6)

qφ(y|x) = qφ(yk|x)
k

= Cat(yk|πφ(x))

(7)

k

where the inference distribution over z is a diagonal Gaussian distribution with mean and variance parameterized by neural networks. The inference model q(y|x) on labels y has the form of a discriminative classiﬁer that generates a set of multinomial probability vectors πφ(x) over all labels for each tag yk. We represent each multinomial distribution q(yk|x) with an MLP.
The MSVAE is trained by maximizing the following variational lower bound U(x) on the objective for unlabeled data:

pθ(x, y, z) log pθ(x) ≥ E(y,z)∼qφ(y,z|x) log qφ(y, z|x)

= Ey∼qφ(y|x)[Ez∼qφ(z|x)[log pθ(x|z, y)]

− KL(qφ(z|x)||p(z)) + log pπ(y)

− log qφ(y|x)] = U (x)

(8)

Note that this introduction of discrete variables requires more sophisticated optimization algorithms, which we will discuss in §4.1. Labeled MSVAE: When y is observed as shown in Fig. 2(b), we maximize the following variational lower bound on the marginal log likelihood of the data and the labels:

pθ(x, y, z) log pθ(x, y) ≥ Ez∼qφ(z|x) log qφ(z|x) =

Ez∼qφ(z|x)[log pθ(x|y, z) + log pπ(y)]

− KL(qφ(z|x)||p(z))

(9)

which is a simple extension to Eq. 2. Note that when labels are not observed, the in-
ference model qφ(y|x) has the form of a discriminative classiﬁer, thus we can use observed labels as the supervision signal to learn a better classiﬁer. In this case we also minimize the following cross entropy as the classiﬁcation loss:

D(x, y) = E(x,y)∼pl(x,y)[− log qφ(y|x)] (10)
where pl(x, y) is the distribution of labeled data. This is a form of multi-task learning, as this additional loss also informs the learning of our representations.

3.3 Multi-space Variational Encoder-Decoders

Finally, we discuss the full proposed method: the multi-space variational encoder-decoder (MSVED), which generates the target x(t) from the source x(s) and labels y(t). Again, we discuss two cases of this model: labels of the target sequence are observed and not observed. MSVED: The graphical model for the MSVED is given in Fig. 2 (e). Because the labels of target sequence are not observed, once again we treat them as discrete latent variables and make inference on the these labels conditioned on the target sequence. The generative process for the MSVED is very similar to that of the MSVAE with one important exception: while the standard MSVAE conditions the recognition model q(z|x) on x, then generates x itself, the MSVED conditions the recognition model q(z|x(s)) on the source x(s), then generates the target x(t). Because only the recognition model is changed, the generative equations for pθ(x(t), y(t), z) are exactly the same as Eqs. 3–5 with x(t) swapped for x and y(t) swapped for y. The variational lower bound on the conditional log likelihood, however, is affected by the recognition model, and thus is computed as:

log pθ(x(t)|x(s))

pθ(x(t), y(t), z|x(s)) ≥E(y(t),z)∼qφ(y(t),z|x(s),x(t)) log qφ(y(t), z|x(s), x(t))

=Ey(t)∼qφ(y(t)|x(t))[Ez∼qφ(z|x(s))[log pθ(x(t)|y(t), z)]

− KL(qφ(z|x(s))||p(z)) + log pπ(y(t))

− log qφ(y(t)|x(t))] = Lu(x(t)|x(s))

(11)

Labeled MSVED: When the complete form of x(s), y(t), and x(t) is observed in our training data,
the graphical model of the labeled MSVED model
is illustrated in Fig. 2 (d). We maximize the vari-
ational lower bound on the conditional log likelihood of observing x(t) and y(t) as follows:

log pθ(x(t), y(t)|x(s))

pθ(x(t), y(t), z|x(s))

≥ Ez∼qφ(z|x(s)) log

qφ(z|x(s))

= Ez∼qφ(z|x(s))[log pθ(x(t)|y(t), z) + log pπ(y(t))]−

KL(qφ(z|x(s))||p(z)) = Ll(x(t), y(t)|x(s)) (12)

4 Learning MSVED

Now that we have described our overall model, we discuss details of the learning process that prove

useful to its success.
4.1 Learning Discrete Latent Variables
One challenge in training our model is that it is not trivial to perform back-propagation through discrete random variables, and thus it is difﬁcult to learn in the models containing discrete tags such as MSVAE or MSVED.4 To alleviate this problem, we use the recently proposed Gumbel-Softmax trick (Maddison et al., 2014; Gumbel and Lieblein, 1954; Jang et al., 2017; J. et al., 2017) to create a differentiable estimator for categorical variables.
The Gumbel-Max trick (Gumbel and Lieblein, 1954) offers a simple way to draw samples from a categorical distribution with class probabilities π1, π2, · · · by using the argmax operation as follows: one hot(arg maxi[gi + log πi]), where g1, g2, · · · are i.i.d. samples drawn from the Gumbel(0,1) distribution.5 When making inferences on the morphological labels y1, y2, · · · , the GumbelMax trick can be approximated by the continuous softmax function with temperature τ to generate a sample vector yˆi for each label i:
exp((log(πij) + gij)/τ ) yˆij = Nk=i 1 exp((log(πik) + gik)/τ (13)
where Ni is the number of classes of label i. When τ approaches zero, the generated sample yˆi becomes a one-hot vector. When τ > 0, yˆi is smooth w.r.t πi. In experiments, we start with a relatively large temperature and decrease it gradually.
4.2 Learning Continuous Latent Variables
MSVED aims at generating the target sequence conditioned on the latent variable z and the target labels y(t). This requires the encoder to generate an informative representation z encoding the content of the x(s). However, the variational lower bound in our loss function contains the KL-divergence between the approximate posterior qφ(z|x) and the prior p(z), which is relatively easy to learn compared with learning to generate output from a latent representation. We observe that with the vanilla implementation the KL cost quickly decreases to near zero, setting qφ(z|x) equal to standard normal distribution. In
4 Kingma et al. (2014) solve this problem by marginalizing over all labels, but this is infeasible in our case where we have an exponential number of label combinations.
5The Gumbel (0,1) distribution can be sampled by ﬁrst drawing u ∼ Uniform(0,1) and computing g = − log(− log(u)).

this case, the RNN decoder can easily rely on the true output of last time step during training to decode the next token, which degenerates into an RNN language model. Hence, the latent variables are ignored by the decoder and cannot encode any useful information. The latent variable z learns an undesirable distribution that coincides with the imposed prior distribution but has no contribution to the decoder. To force the decoder to use the latent variables, we take the following two approaches which are similar to Bowman et al. (2016). KL-Divergence Annealing: We add a coefﬁcient λ to the KL cost and gradually anneal it from zero to a predeﬁned threshold λm. At the early stage of training, we set λ to be zero and let the model ﬁrst ﬁgure out how to project the representation of the source sequence to a roughly right point in the space and then regularize it with the KL cost. Although we are not optimizing the tight variational lower bound, the model balances well between generation and regularization. This technique can also be seen in (Kocˇisky` et al., 2016; Miao and Blunsom, 2016). Input Dropout in the Decoder: Besides annealing the KL cost, we also randomly drop out the input token with a probability of β at each time step of the decoder during learning. The previous ground-truth token embedding is replaced with a zero vector when dropped. In this way, the RNN decoder could not fully rely on the ground-truth previous token, which ensures that the decoder uses information encoded in the latent variables.
5 Architecture for Morphological Reinﬂection
Training details: For the morphological reinﬂection task, our supervised training data consists of source x(s), target x(t), and target tags y(t). We test three variants of our model trained using different types of data and different loss functions. First, the single-directional supervised model (SDSup) is purely supervised: it only decodes the target word from the given source word with the loss function Ll(x(t), y(t)|x(s)) from Eq. 12. Second, the bi-directional supervised model (BDSup) is trained in both directions: decoding the target word from the source word and decoding the source word from the target word, which corresponds to the loss function Ll(x(t), y(t)|x(s)) + Lu(x(s)|x(t)) using Eqs. 11–12. Finally, the semisupervised model (Semi-sup) is trained to maxi-

Language
Turkish Hungarian Spanish Russian Navajo Maltese Arabic Georgian German Finnish
Avg. Acc

#LD
12,798 19,200 12,799 12,798 12,635 19,200 12,797 12,795 12,777 12,800
–

#ULD
29,608 34,025 72,151 67,691
6,839 46,918 53,791 46,562 56,246 74,687
–

Proposed MSVED SD-Sup BD-Sup Semi-sup

93.25 97.00
88.32 75.77 85.00 84.83 79.13 89.31 75.55 75.59

95.66† 98.54†
91.50
83.07 95.37† 88.21† 92.62† 93.63†
84.08
85.11

97.25‡ 99.16‡
93.74 86.80‡ 98.25‡ 88.46‡ 93.37‡ 95.97‡ 90.28‡ 91.20‡

84.38 90.78† 93.45‡

Baseline MED Single Ensemble

89.56
96.46 94.74†‡ 83.55†
63.62
79.59
72.58
91.06 89.11† 85.63†

95.00 98.37
96.69 87.13 83.00 84.25 82.80 96.21 92.41 93.18

84.59

90.90

Table 1: Results for Task 3 of SIGMORPHON 2016 on Morphology Reinﬂection. † represents the best single supervised model score, ‡ represents the best model including semi-supervised models, and bold represents the best score overall. #LD and #ULD are the number of supervised data and unlabeled words respectively.

Source Word ka lb

✏ ⇠ N (0, 1) k ä

⌃(x) z µ(x)

<w> k

+

y1T y2T y3T

Reinﬂected Form
......

y

T 4

.

.

.

.

Supervised Variational Encoder Decoder

Source Word ka lb

✏ ⇠ N (0, 1) k a
⌃(x) () z
µx

Source Word
......

Multinomial Sampling

<w> k

y1 2 {

}..

y1

pos: V, N, ADJ

y 2{

}

2 def: DEF, INDEF

+ y2 y3 2 {num: DU, SG, PL}...

y3

...

y4

...

···

Unsupervised Variational Auto-encoder

Figure 3: Model architecture for labeled and unlabeled data. For the encoder-decoder model, only one direction from the source to target is given. The classiﬁcation model is not illustrated in the diagram.
mize the variational lower bounds and minimize the classiﬁcation cross-entropy error of 10.

L(x(s), x(t), y(t), x) = α · U (x) + Lu(x(s)|x(t))

+ Ll(x(t), y(t)|x(s)) − D(x(t), y(t))

(14)

The weight α controls the relative weight between the loss from unlabeled data and labeled data.
We use Monte Carlo methods to estimate the expectation over the posterior distribution q(z|x) and q(y|x) inside the objective function 14. Speciﬁcally, we draw Gumbel noise and Gaussian noise one at a time to compute the latent variables y and z.
The overall model architecture is shown in Fig. 3. Each character and each label is associated with a continuous vector. We employ Gated Recurrent Units (GRUs) for the encoder and de-

→− ←− coder. Let ht and ht denote the hidden state of the forward and backward encoder RNN at time step t. u is the hidden representation of x(s) concatenating the last hidden state from both directions
−→ ←− i.e. [hT ; hT ] where T is the word length. u is used as the input for the inference model on z. We represent µ(u) and σ2(u) as MLPs and sample z from N (µ(u), diag(σ2(u))), using z = µ + σ ◦ , where ∼ N (0, I). Similarly, we can obtain the hidden representation of x(t) and use this as input to the inference model on each label yi(t) which is also an MLP following a softmax layer to generate the categorical probabilities of target labels.
In decoding, we use 3 types of information in calculating the probability of the next character : (1) the current decoder state, (2) a tag context vector using attention (Bahdanau et al., 2015) over the tag embeddings, and (3) the latent variable z. The intuition behind this design is that we would like the model to constantly consider the lemma represented by z, and also reference the tag corresponding to the current morpheme being generated at this point. We do not marginalize over the latent variable z however, instead we use the mode µ of z as the latent representation for z. We use beam search with a beam size of 8 to perform search over the character vocabulary at each decoding time step.
Other experimental setups: All hyperparameters are tuned on the validation set, and include the following: For KL cost annealing, λm is set to be 0.2 for all language settings. For character drop-out at the decoder, we empirically set β to be 0.4 for all languages. We set the dimension of character embeddings to be 300, tag label embeddings to be 200, RNN hidden state to be 256, and

latent variable z to be 150. We set α the weight for the unsupervised loss to be 0.8. We train the model with Adadelta (Zeiler, 2012) and use earlystop with a patience of 10.
6 Experiments
6.1 Background: SIGMORPHON 2016
SIGMORPHON 2016 is a shared task on morphological inﬂection over 10 different morphologically rich languages. There are a total of three tasks, the most difﬁcult of which is task 3, which requires the system to output the reinﬂection of an inﬂected word.6 The training data format in task 3 is in triples: (source word, target labels, target word). In the test phase, the system is asked to generate the target word given a source word and the target labels. There are a total of three tracks for each task, divided based the amount of supervised data that can be used to solve the problem, among which track 2 has the strictest limitation of only using data for the corresponding task. As this is an ideal testbed for our method, which can learn from unlabeled data, we choose track 2 and task 3 to test our our model’s ability to exploit this data.
As a baseline, we compare our results with the MED system (Kann and Schu¨tze, 2016a) which achieved state-of-the-art results in the shared task. This system used an encoder-decoder model with attention on the concatenated source word and target labels. Its best result is obtained from an ensemble of ﬁve RNN encoder-decoders (Ensemble). To make a fair comparison with our models, which don’t use ensembling, we also calculated single model results (Single).
All models are trained using the labeled training data provided for task 3. For our semi-supervised model (Semi-sup), we also leverage unlabeled data from the training and validation data for tasks 1 and 2 to train variational auto-encoders.
6.2 Results and Analysis
From the results in Tab. 1, we can glean a number of observations. First, comparing the results of our full Semi-sup model, we can see that for all languages except Spanish, it achieves accuracies better than the single MED system, often by a large margin. Even compared to the MED ensembled model, our single-model system is quite competitive, achieving higher accuracies for Hungarian,
6Task 1 is inﬂection of a lemma word and task 2 is reinﬂection but also provides the source word labels.

Language
Turkish Hungarian Spanish Russian Navajo Maltese Arabic Georgian German Finnish

Preﬁx
0.21 0.00 0.09 0.66 77.64 48.81 68.52 4.46 0.84 0.02

Stem
1.12 0.08 3.25 7.70 18.38 11.05 37.04 0.41 3.32 12.33

Sufﬁx
98.75 99.79 90.74 85.00 26.40 98.74 88.24 92.47 89.19 96.16

Table 2: Percentage of inﬂected word forms that have modiﬁed each part of the lemma (Cotterell et al., 2016) (some words can be inﬂected zero or multiple times, thus sums may not add to 100%).

Accuracy

100 95 Navajo 90 85 80 75 70 65 600.2 0.3

Arabic Maltese

GeoHrguTinaugnrkairsihan Finnish Spanish
German
Russian

0.4 0.5 0.6 0.7 0.8 Percentage of suffixing inflection

MSVED MED (1)
0.9 1.0

Figure 4: Performance on test data w.r.t. the percentage of sufﬁxing inﬂection. Points with the same x-axis value correspond to the same language results.

Navajo, Maltese, and Arabic, as well as achieving average accuracies that are state-of-the-art.
Next, comparing the different varieties of our proposed models, we can see that the semisupervised model consistently outperforms the bidirectional model for all languages. And similarly, the bidirectional model consistently outperforms the single direction model. From these results, we can conclude that the unlabeled data is beneﬁcial to learn useful latent variables that can be used to decode the corresponding word.
Examining the linguistic characteristics of the models in which our model performs well provides even more interesting insights. Cotterell et al. (2016) estimate how often the inﬂection process involves preﬁx changes, stem-internal changes or sufﬁx changes, the results of which are shown in Tab. 2. Among the many languages, the inﬂection processes of Arabic, Maltese and Navajo are relatively diverse, and contain a large amount of all three forms of inﬂection. By examining the experimental results together with the morphological inﬂection process of different languages, we found that among all the languages, Navajo, Maltese and Arabic obtain the largest gains in performance compared with the ensem-

a l - i m¯a r ¯a t i y y ¯a t u

n ´ı d a j i d l e e h

case=NOM mood=None
pos=ADJ per=None

mood=REAL
0.8 pos=V

poss=None

per=4

0.6

num=PL tense=None

num=PL

0.4

aspect=None voice=None gen=FEM def=DEF

aspect=

IPFV/PROG

0.2

arg=None 0.0

Figure 5: Two examples of attention weights on target linguistic labels: Arabic (Left) and Navajo (Right). When a tag equals None, it means the word does not have this tag.

bled MED system. To demonstrate this visually, in Fig. 4, we compare the semi-supervised MSVED with the MED single model w.r.t. the percentage of sufﬁxing inﬂection of each language, showing this clear trend.
This strongly demonstrates that our model is agnostic to different morphological inﬂection forms whereas the conventional encoder-decoder with attention on the source input tends to perform better on sufﬁxing-oriented morphological inﬂection. We hypothesize that for languages that the inﬂection mostly comes from sufﬁxing, transduction is relatively easy because the source and target words share the same preﬁx and the decoder can copy the preﬁx of the source word via attention. However, for languages in which different inﬂections of a lemma go through different morphological processes, the inﬂected word and the target word may differ greatly and thus it is crucial to ﬁrst analyze the lemma of the inﬂected word before generating the corresponding the reinﬂection form based on the target labels. This is precisely what our model does by extracting the lemma representation z learned by the variational inference model.

6.3 Analysis on Tag Attention
To analyze how the decoder attends to the linguistic labels associated with the target word, we randomly pick two words from the Arabic and Navajo test set and plot the attention weight in Fig. 5. The Arabic word “al-’ima¯ra¯tiyya¯tu” is an adjective which means “Emirati”, and its source word in the test data is “’ima¯ra¯tiyyin” 7. Both of these are declensions of “’ima¯ra¯tiyy”. The source word is
7https://en.wiktionary.org/wiki/%D8% A5%D9%85%D8%A7%D8%B1%D8%A7%D8%AA%D9%8A

Figure 6: Visualization of latent variables z for Maltese with 35 pseudo-lemma groups in the ﬁgure.
singular, masculine, genitive and indeﬁnite, while the required inﬂection is plural, feminine, nominative and deﬁnite. We can see from the left heat map that the attention weights are turned on at several positions of the word when generating corresponding inﬂections. For example, “al-” in Arabic is the deﬁnite article that marks deﬁnite nouns. The same phenomenon can also be observed in the Navajo example, as well as other languages, but due to space limitation, we don’t provide detailed analysis here.
6.4 Visualization of Latent Lemmas
To investigate the learned latent representations, in this section we visualize the z vectors, examining whether the latent space groups together words with the same lemma. Each sample in SIGMORPHON 2016 contains source word and target words which share the same lemma. We run a heuristic process to assign pairs of words to groups that likely share a lemma by grouping together word pairs for which at least one of the words in each pair shares a surface form. This process is not error free – errors may occur in the case where multiple lemmas share the same surface form – but in general the groupings will generally reﬂect lemmas except in these rare erroneous cases, so we dub each of these groups a pseudo-lemma.
In Fig. 6, we randomly pick 1500 words from Maltese and visualize the continuous latent vectors of these words. We compute the latent vectors as µφ(x) in the variational posterior inference (Eq. 6) without adding the variance. As expected, words that belong to the same pseudo-lemma (in the same color) are projected into adjacent points in the two-dimensional space. This demonstrates that the continuous latent variable captures the canonical form of a set of words and demonstrates the effectiveness of the proposed representation.

Language Turkish Maltese Finnish

Src Word
kocama yaratmamdan bitimizde
ndammhomli tqoz˙z˙hieli tissikkmuhomli
verovapaatta turrumme sukunimin

Tgt Labels
pos=N,poss=PSS1S,case=ESS,num=SG pos=N,case=NOM,num=SG
pos=N,tense=PST,per=1,num=SG
pos=V,polar=NEG,tense=PST,num=SG pos=V,polar=NEG,tense=PST,num=SG pos=V,polar=POS,tense=PST,num=PL
pos=ADJ,case=PRT,num=PL pos=V,mood=POT,tense=PRS,num=PL
pos=N,case=PRIV,num=PL

Gold Tgt
kocamda yaratma
bittik
tindammhiex tqoz˙z˙x
ssikkmulna
verovapaita turtunemme
sukunimitt

MED
kocama yaratma bitiydik
ndammejthiex tqoz˙z˙x
tissikkmulna
verovappaita turtunemme sukunimeitta

Ours
kocamda yaratman
bittim
tindammhiex qaz˙z˙ejtx
tissikkmulna
verovapaita turrunemme sukunimeitta

Table 3: Randomly picked output examples on the test data. Within each block, the ﬁrst, second and third lines are outputs that ours is correct and MED’s is wrong, ours is wrong and MED’s is correct, both are wrong respectively.

6.5 Analyzing Effects of Size of Unlabeled Data
From Tab. 1, we can see that semi-supervised learning always performs better than supervised learning without unlabeled data. In this section, we investigate to what extent the size of unlabeled data can help with performance. We process a German corpus from a 2017 Wikipedia dump and obtain more than 100,000 German words. These words are ranked in order of occurrence frequency in Wikipedia. The data contains a certain amount of noise since we did not apply any special processing. We shufﬂe all unlabeled data from both the Wikipedia and the data provided in the shared task used in previous experiments, and increase the number of unlabeled words used in learning by 10,000 each time, and ﬁnally use all the unlabeled data (more than 150,000 words) to train the model. Fig. 7 shows that the performance on the test data improves as the amount of unlabeled data increases, which implies that the unsupervised learning continues to help improve the model’s ability to model the latent lemma representation even as we scale to a noisy, real, and relatively large-scale dataset. Note that the growth rate of the performance grows slower as more data is added, because although the number of unlabeled data is increasing, the model has seen most word patterns in a relatively small vocabulary.
6.6 Case Study on Reinﬂected Words
In Tab. 3, we examine some model outputs on the test data from the MED system and our model respectively. It can be seen that most errors of MED and our models can be ascribed to either over-copy or under-copy of characters. In particular, from the complete outputs we observe that our model tends to be more aggressive in its changes, resulting in

Accuracy on test data (%) 0 1e4 2e4 3e4 5e4 >15e5

92 91 90 89 88 87 86 85 84 83
# Unlabeled words
Figure 7: Performance on the German test data w.r.t. the amount of unlabeled Wikipedia data.
it performing more complicated transformations, both successfully (such as Maltese “ndammhomli” to “tindammhiex”) and unsuccessfully (“tqoz˙z˙x” to “qaz˙z˙ejtx”). In contrast, the attentional encoderdecoder model is more conservative in its changes, likely because it is less effective in learning an abstracted representation for the lemma, and instead copies characters directly from the input.
7 Conclusion and Future Work
In this work, we propose a multi-space variational encoder-decoder framework for labeled sequence transduction problem. The MSVED performs well in the task of morphological reinﬂection, outperforming the state of the art, and further improving with the addition of external unlabeled data. Future work will adapt this framework to other sequence transduction scenarios such as machine translation, dialogue generation, question answering, where continuous and discrete latent variables can be abstracted to guide sequence generation.
Acknowledgments
The authors thank Jiatao Gu, Xuezhe Ma, Zihang Dai and Pengcheng Yin for their helpful discussions. This work has been supported in part by an Amazon Academic Research Award.

References
In˜aki Alegria and Izaskun Etxeberria. 2016. Ehu at the sigmorphon 2016 shared task. a simple proposal: Grapheme-to-phoneme for inﬂection. In Proceedings of the 2016 Meeting of SIGMORPHON .
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural machine translation by jointly learning to align and translate. The International Conference on Learning Representations .
Justin Bayer and Christian Osendorfer. 2014. Learning stochastic recurrent networks. arXiv preprint arXiv:1411.7610 .
Samuel R Bowman, Luke Vilnis, Oriol Vinyals, Andrew M Dai, Rafal Jozefowicz, and Samy Bengio. 2016. Generating sentences from a continuous space. Proceedings of CoNLL .
Victor Chahuneau, Eva Schlinger, Noah A Smith, and Chris Dyer. 2013. Translating into morphologically rich languages with synthetic phrases. Association for Computational Linguistics.
Junyoung Chung, Kyle Kastner, Laurent Dinh, Kratarth Goel, Aaron C Courville, and Yoshua Bengio. 2015. A recurrent latent variable model for sequential data. In Advances in neural information processing systems. pages 2980–2988.
R. Cotterell, C. Kirov, J. Sylak-Glassman, D. Yarowsky, J. Eisner, and M. Hulden. 2016. The sigmorphon 2016 shared taskmorphological reinﬂection. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics.
Kareem Darwish and Douglas W Oard. 2007. Adapting morphology for arabic information retrieval. In Arabic Computational Morphology, Springer, pages 245–262.
Carl Doersch. 2016. Tutorial on variational autoencoders. arXiv preprint arXiv:1606.05908 .
Greg Durrett and John DeNero. 2013. Supervised learning of complete morphological paradigms. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics, Atlanta, Georgia, pages 1185–1195.
Otto Fabius and Joost R van Amersfoort. 2014. Variational recurrent auto-encoders. arXiv preprint arXiv:1412.6581 .
Manaal Faruqui, Yulia Tsvetkov, Graham Neubig, and Chris Dyer. 2016. Morphological inﬂection generation using character sequence to sequence learning. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics, San Diego, California, pages 634–643.

Emil Julius Gumbel and Julius Lieblein. 1954. Statistical theory of extreme values and some practical applications: a series of lectures. US Government Printing Ofﬁce Washington .
Maddison Chris J., Andriy Mnih, and Yee Whye Teh. 2017. The concrete distribution: A continuous relaxation of discrete random variables. In The International Conference on Learning Representations..
Eric Jang, Shixiang Gu, and Ben Poole. 2017. Categorical reparameterization with gumbel-softmax. In The International Conference on Learning Representations..
Melvin Johnson, Mike Schuster, Quoc V Le, Maxim Krikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat, Fernanda Vie´gas, Martin Wattenberg, Greg Corrado, et al. 2016. Google’s multilingual neural machine translation system: Enabling zero-shot translation. arXiv preprint arXiv:1611.04558 .
Katharina Kann, Ryan Cotterell, and Hinrich Schu¨tze. 2016. Neural multi-source morphological reinﬂection. arXiv preprint arXiv:1612.06027 .
Katharina Kann and Hinrich Schu¨tze. 2016a. Med: The lmu system for the sigmorphon 2016 shared task on morphological reinﬂection. In In Proceedings of the 14th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology. Berlin, Germany.
Katharina Kann and Hinrich Schu¨tze. 2016b. Singlemodel encoder-decoder with explicit morphological representation for reinﬂection. In In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. Berlin, Germany.
Yuta Kikuchi, Graham Neubig, Ryohei Sasano, Hiroya Takamura, and Manabu Okumura. 2016. Controlling output length in neural encoder-decoders. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Austin, Texas, pages 1328–1338.
Diederik P Kingma, Shakir Mohamed, Danilo Jimenez Rezende, and Max Welling. 2014. Semi-supervised learning with deep generative models. In Advances in Neural Information Processing Systems. Montre´al, Canada, pages 3581–3589.
D.P. Kingma and M. Welling. 2014. Auto-encoding variational bayes. In The International Conference on Learning Representations.
Toma´sˇ Kocˇisky`, Ga´bor Melis, Edward Grefenstette, Chris Dyer, Wang Ling, Phil Blunsom, and Karl Moritz Hermann. 2016. Semantic parsing with semi-supervised sequential autoencoders. the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP) .

Lars Maaløe, Casper Kaae Sønderby, Søren Kaae Sønderby, and Ole Winther. 2016. Auxiliary deep generative models. Proceedings of the 33rd International Conference on Machine Learning .
Chris J Maddison, Daniel Tarlow, and Tom Minka. 2014. A* sampling. In Advances in Neural Information Processing Systems. pages 3086–3094.
Yishu Miao and Phil Blunsom. 2016. Language as a latent variable: Discrete generative models for sentence compression. the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP) .
Garrett Nicolai, Bradley Hauer, Adam St. Arnaud, and Grzegorz Kondrak. 2016. Morphological reinﬂection via discriminative string transduction. In Proceedings of the 2016 Meeting of SIGMORPHON .
Robert Ostling. 2016. Morphological reinﬂection with convolutional neural networks. In Proceedings of the 14th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology page 23.
Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Controlling politeness in neural machine translation via side constraints. In Proceedings of the 2016 Conference of The North American Chapter of the Association for Computational Linguistics (NAACL). pages 35–40.
Dima Taji, Ramy Eskander, Nizar Habash, and Owen Rambow. 2016. The columbia university - new york university abu dhabi sigmorphon 2016 morphological reinﬂection shared task submission. In Proceedings of the 2016 Meeting of SIGMORPHON .
Keiichi Tokuda, Takashi Masuko, Noboru Miyazaki, and Takao Kobayashi. 2002. Multi-space probability distribution hmm. IEICE TRANSACTIONS on Information and Systems 85(3):455–464.
Kristina Toutanova, Hisami Suzuki, and Achim Ruopp. 2008. Applying morphology generation models to machine translation. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics. pages 514–522.
Matthew D Zeiler. 2012. Adadelta: an adaptive learning rate method. arXiv preprint arXiv:1212.5701 .
Biao Zhang, Deyi Xiong, Jinsong Su, Hong Duan, and Min Zhang. 2016. Variational neural machine translation. Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics .

