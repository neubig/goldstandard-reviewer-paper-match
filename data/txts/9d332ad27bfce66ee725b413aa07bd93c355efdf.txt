arXiv:2112.02721v1 [cs.CL] 6 Dec 2021

NL-Augmenter A Framework for Task-Sensitive Natural Language Augmentation
December 5, 2021
Kaustubh D. Dhole3,18†, Varun Gangal7†, Sebastian Gehrmann23†, Aadesh Gupta3†, Zhenhao Li32†, Saad Mahamood90†, Abinaya Mahendiran45†, Simon Mille53†,
Ashish Srivastava2†, Samson Tan48,91†, Tongshuang Wu81†, Jascha Sohl-Dickstein22†, Jinho D. Choi18†, Eduard Hovy7†, Ondrˇej Dušek10†, Sebastian Ruder13†, Sajant Anand68, Nagender Aneja74, Rabin Banjade77, Lisa Barthe19, Hanna Behnke32, Ian Berlot-Attwell80, Connor Boyle81, Caroline Brun49, Marco Antonio Sobrevilla Cabezudo79, Samuel Cahyawijaya26, Emile Chapuis52, Wanxiang Che24, Mukund Choudhary37, Christian Clauss33, Pierre Colombo52,
Filip Cornell41, Gautier Dagan84, Mayukh Das63, Tanay Dixit30, Thomas Dopierre39, Paul-Alexis Dray89, Suchitra Dubey1, Tatiana Ekeinhor86, Marco Di Giovanni51,
Rishabh Gupta28, Rishabh Gupta29, Louanes Hamla19, Sang Han73, Fabrice Harel-Canada70, Antoine Honoré86, Ishan Jindal27, Przemyslaw K. Joniak66, Denis Kleyko75, Venelin Kovatchev65,
Kalpesh Krishna71, Ashutosh Kumar34, Stefan Langer59, Seungjae Ryan Lee55, Corey James Levinson33, Hualou Liang15, Kaizhao Liang76, Zhexiong Liu78,
Andrey Lukyanenko43, Vukosi Marivate14, Gerard de Melo25, Simon Meoni33, Maxime Meyer86, Afnan Mir4, Naﬁse Sadat Moosavi62, Niklas Muennighoﬀ50, Timothy Sum Hon Mun64,
Kenton Murray40, Marcin Namysl20, Maria Obedkova33, Priti Oli77, Nivranshu Pasricha46, Jan Pﬁster83, Richard Plant17, Vinay Prabhu73, Vasile Pa˘is, 57, Libo Qin24, Shahab Raji58, Pawan Kumar Rajpoot56, Vikas Raunak44, Roy Rinberg11, Nicolas Roberts82, Juan Diego Rodriguez72, Claude Roux49, Vasconcellos P. H. S.54, Ananya B. Sai30,
Robin M. Schmidt16, Thomas Scialom89, Tshephisho Sefara12, Saqib N. Shamsi88, Xudong Shen48, Yiwen Shi15, Haoyue Shi67, Anna Shvets19, Nick Siegel4, Damien Sileo42, Jamie Simon68, Chandan Singh68, Roman Sitelew33, Priyank Soni3, Taylor Sorensen6, William Soto61,
Aman Srivastava85, KV Aditya Srivatsa37, Tony Sun69, Mukund Varma T30, A Tabassum47, Fiona Anting Tan36, Ryan Teehan9, Mo Tiwari60, Marie Tolkiehn8, Athena Wang4, Zijian Wang33, Zijie J. Wang21, Gloria Wang31, Fuxuan Wei24, Bryan Wilie35, Genta Indra Winata5, Xinyi Wu81,
Witold Wydman´ ski38, Tianbao Xie24, Usama Yaseen59, M. Yee33, Jing Zhang18, Yue Zhang87
1ACKO, 2Agara Labs, 3Amelia R&D, New York, 4Applied Research Laboratories, The University of Texas at Austin, 5Bloomberg, 6Brigham Young University, 7Carnegie Mellon University, 8Center for Data and Computing in Natural Sciences, Universität Hamburg, 9Charles River Analytics, 10Charles University, Prague, 11Columbia University, 12Council for Scientiﬁc
and Industrial Research, 13DeepMind, 14Department of Computer Science, University of Pretoria, 15Drexel University, 16Eberhard Karls University of Tübingen, 17Edinburgh Napier University, 18Emory University, 19Fablab by Inetum in Paris, 20Fraunhofer IAIS, 21Georgia Tech, 22Google Brain, 23Google Research, 24Harbin Institute of Technology, 25Hasso Plattner Institute / University of Potsdam, 26Hong Kong University of Science and Technology, 27IBM Research, 28IIIT Delhi, 29IIT
Delhi, 30IIT Madras, 31Illinois Mathematics and Science Academy, 32Imperial College, London, 33Independent, 34Indian Institute of Science, Bangalore, 35Institut Teknologi Bandung, 36Institute of Data Science, National University of Singapore, 37International Institute of Information Technology, Hyderabad, 38Jagiellonian University, Poland, 39Jean Monnet University,
40Johns Hopkins’, 41KTH Royal Institute of Technology, 42KU Leuven, 43MTS AI, France, 44Microsoft, Redmond, WA, 45Mphasis NEXT Labs, 46National University of Ireland Galway, 47National University of Science and Technology, Pakistan,
48National University of Singapore, 49Naver Labs Europe, 50Peking University, 51Politecnico di Milano and University of Bologna, 52Polytechnic Institute of Paris, 53Pompeu Fabra University, 54Pontiﬁcal Catholic University of Minas Gerais, Brazil, 55Princeton University, 56Rakuten India, 57Research Institute for Artiﬁcial Intelligence Mihai Dra˘ga˘nescu, Romanian Academy, 58Rutgers University, 59Siemens AG, 60Stanford University, 61SyNaLP, LORIA, 62TU Darmstadt, 63Technical University of
Braunschweig, 64The Alan Turing Institute, 65The University of Texas at Austin; (University of Barcelona, University of Birmingham), 66The University of Tokyo, 67Toyota Technological Institute at Chicago, 68UC Berkeley, 69UC Santa Barbara / Google, 70UCLA, 71UMass Amherst, 72UT Austin, 73UnifyID, 74Universiti Brunei Darussalam, 75University of California,
Berkeley and Research Institutes of Sweden, 76University of Illinois, Urbana Champaign, 77University of Memphis, 78University of Pittsburgh, 79University of São Paulo, 80University of Toronto, 81University of Washington, 82University of
Wisconsin–Madison, 83University of Würzburg, 84Universty of Edinburgh, 85VMware, 86Vade, 87Westlake Institute for Advanced Study, 88Whirlpool Corporation, 89reciTAL, 90trivago N.V., 91Salesforce Research

Abstract
Data augmentation is an important component in the robustness evaluation of models in natural language processing (NLP) and in enhancing the diversity of the data they are trained on. In this paper, we present NL-Augmenter, a new participatory Pythonbased natural language augmentation framework which supports the creation of both transformations (modiﬁcations to the data) and ﬁlters (data splits according to speciﬁc features). We describe the framework and an initial set of 117 transformations and 23 ﬁlters for a variety of natural language tasks. We demonstrate the eﬃcacy of NL-Augmenter by using several of its tranformations to analyze the robustness of popular natural language models. The infrastructure, datacards and robutstness analysis results are available publicly on the NL-Augmenter repository (https://github. com/GEM-benchmark/NL-Augmenter).
1 Introduction
Data augmentation, the act of creating new datapoints by slightly modifying copies or creating synthetic data based on existing data, is an important component in the robustness evaluation of models in natural language processing (NLP) and in enhancing the diversity of their training data. Most data augmentation techniques create examples through transformations of existing examples which are based on prior task-speciﬁc knowledge (Feng et al., 2021; Chen et al., 2021). Such transformations seek to disrupt model predictions or can be used as training candidates for improving regularization and denoising models, for example, through consistency training (Xie et al., 2020); Figure 1 shows a few possible transformations for a sample sentence.
However, a vast majority of transformations do not alter the structure of examples in drastic and meaningful ways, rendering them qualitatively less eﬀective as potential training or test examples. Moreover, diﬀerent NLP tasks may beneﬁt from transforming diﬀerent linguistic properties. for example, changing the word “happy” to “very happy” in an input is more relevant for sentiment analysis than for summarization (Mille et al., 2021). Despite
† Organizers & Steering Committee * Please send requests to the correspondence email: nlaugmenter@googlegroups.com.

this, many transformations may be universially useful, for example changing places to ones from different geographic regions, or changing names to those from diﬀerent cultures. As such, having a single place to collect both task-speciﬁc and taskindependent augmentations will ease the barrier to creating appropriate suites of augmentations that should be applied to diﬀerent tasks.
Natural language and its long-tailed nature (Bamman, 2017) lead to a very high diversity of possible surface forms. If only a handful number of ways to paraphrase a text are available, it can be hard to generalize across radically diﬀerent surface forms. Besides, data drawn i.i.d. from such long-tailed distribution represents itself exactly in proportion to its occurence in the dataset. Evaluating NLP systems on such data means that the head of the distribution is emphasized even in the test dataset and that rare phenomena are implicitly ignored. However, informed transformations or the identiﬁcation of such tail examples may require a wide range of domain knowledge or speciﬁc cultural backgrounds. We thus argue that a collection of transformations that should be applied to NLP datasets should be done by capitalizing on the “wisdom-of-researchers”.
To enable more diverse and better characterized data during testing and training, we create a Pythonbased natural language augmentation framework, NL-Augmenter.1 With the help of researchers across subﬁelds in computational linguistics and NLP, we collect many creative ways to augment data for natural language tasks. To encourage taskspeciﬁc implementations, we tie each transformation to a widely-used data format (e.g. text pair, a question-answer pair, etc.) along with various task types (e.g. entailment, tagging, etc.) that they intend to beneﬁt. We demonstrate the eﬃcacy of NL-Augmenter by using some of its transformations to analyze the robustness of popular natural language models.
A majority of the augmentations that the framework supports are transformations of single sentences that aim to paraphrase these sentences in various ways. NL-Augmenter loosens the deﬁnition of “transformations” from the logic-centric view of strict equivalence to the more descriptive view of linguistics, closely resembling Bhagat and Hovy (2013)’s “quasi-paraphrases”. We extend
1https://github.com/GEM-benchmark/ NL-Augmenter

ashish

John is fond of expensive Italian pizzas. John likes e ensive Italian pizzas .

John likes expensive Italian pizzas(italian dish of flattened bread and toppings). John likes expensive Italian pizzas .#LikesPizzas #Likes #John #Pizzas John confirmed that he likes expensive Italian pizzas. John likes expensive Italienisch pizzas . Jo4n lik3s 3xpensiv3 1talian pizzas .

John likes expensive Italian pizzas.

NL-Augmenter

John expensive .

John likes expensive Italian food .

Expensive italian pizzas, John likes.

John likes pure bead Italian pizzas.

Joḫn ⱡikẽs ęxṕensίѷḝ ‫إ‬է al aἣ p‫ ׀‬ƨzas .

John is a big fan of Italy, especially of the rich and cheap pizzas.

John likes expensive actually Italian actually pizzas In my opinion .

JJoohhnn lliikkeess eexxppeennssiivvee IIttaalliiaann ppiizzzzaass ..

Figure 1: A few randomly chosen transformations of NL-Augmenter for the original sentence John likes expensive pizzas. While the meaning (almost) always remains the same and identiﬁable by humans, models can have a much harder time representing the transformed sentences.
ashish

this to accommodate noise, intentional and accidental human mistakes, sociolinguistic variation, semantically-valid style, syntax changes, as well as artiﬁcial constructs that are unambiguous to humans (Tan et al., 2021b). Some transformations vary the socio-linguistic perspective permitting a crucial source of variation wherein language goals span beyond conveying ideas and content.
In addition to transformations, NL-Augmenter also provides a variety of ﬁlters, which can be used to ﬁlter data and create subpopulations of given inputs, according to features such as input complexity, input size, etc. Unlike a transformation, the output of a ﬁlter is a boolean value, indicating whether the input meets the ﬁlter criterion, e.g. whether the input text is toxic. The ﬁlters allow splitting existing datasets and hence evaluating models on subsets with speciﬁc linguistic properties.
In this paper, we apply the collected transformations and ﬁlters to several datasets and show to what exent the diﬀerent types of perturbations do aﬀect some models. The paper is organized as follows. We ﬁrst discuss the rise of participatory benchmarks in Section 2. In Section 3, we introduce the participatory workshop and the repository of NL-Augmenter. In Section 4, we present the robustness analysis performed on the participants’

submissions, and in Section 4 we provide a broader impact discussion. All ﬁlters and transformations are listed and described in details in the Appendix.
2 Related Work
NL-Augmenter enables both data augmentation and robustness testing by constructing the library in a participatory fashion. We provide an overview of the related work in these lines of research.
2.1 Evolving Participatory Benchmarks
To address the problem of under-resourced African languages in machine translation, Masakhane adopted a bottom-up, participatory approach to construct machine translation benchmarks for over thirty languages (Nekoto et al., 2020). This collaborative approach is increasingly adopted in the NLP communtiy to create evolving benchmarks in response to the rapid pace of NLP progress. The Generation Evaluation and Metrics benchmark (Gehrmann et al., 2021), which started the development of NL-Augmenter, is a participatory project to document and improve tasks and their evaluation in natural language generation. BIGBench2 proposes a collaborative framework to collect a large suite of few-shot tasks to gauge the
2https://github.com/google/BIG-bench

abilities of large, pretrained language models. DynaBench (Kiela et al., 2021) proposes to iteratively evaluate models in a human-in-the-loop fashion by constructing more challenging examples after each round of model evaluation. SyntaxGym (Gauthier et al., 2020) provides a standardized platform for researchers to contribute and use evaluation sets and focuses on targeted syntactic evaluation of Language Models (LMs), particularly psycholinguistically motivated ones. In contrast, our transformations operate on a larger variety of tasks and model types — they are not required to be syntactically or even linguistically motivated.
2.2 Wisdom-of-Researchers
There are many ways to introduce variation in a sentence without altering its meaning; the lived experiences of a diverse group of individuals could help with identifying and codifying the myriad dimensions of variation as executable transformations (Tan et al., 2021b). Leveraging the wisdomof-the-crowd (Galton, 1907; Yi et al., 2010) is common in our ﬁeld of natural language processing, with the use of Amazon Mechanical Turk to generate and annotate data in exchange for monetary returns. The aforementioned BIG-Bench project, hosted on GitHub, oﬀers co-authorship in exchange for task contribution. We can think of this as a sort of wisdom-of-researchers. Similarly, we crowdsource transformations, in the form of Python code snippets, in return for co-authorship.
2.3 Robustness Evaluation Tools
There are many projects with similar goals that inspired NL-Augmenter. For example Gardner et al. (2020) proposed creating “contrast” sets of perturbed test examples. In their approach, each example is manually perturbed, which may lead to higher-quality results but is costly to replicate for each new task due to scale and annotator cost. TextAttack (Morris et al., 2020) is a library enabling the adversarial evaluation of English NLP models. Partially overcoming this limitation, TextFlint (Wang et al., 2021a), supports robustness evaluation in English and Chinese. It covers linguistic and task-specﬁcic transformations, adversarial attacks, and subpopulation analyses. In contrast, while the majority are focused on English, NL-Augmenter comprises transformations and ﬁlters that work for many diﬀerent languages and each contribution can specify a set of supported languages.

Robustness Gym (Goel et al., 2021) uniﬁes four diﬀerent types of robustness tests — subpopulations, transformations, adversarial attacks, and evaluation sets — under a single interface. However, it depends on existing libraries for its transformations. In contrast, NL-Augmenter focuses on compiling a set of transformations in an open source and collaborative fashion, which is reﬂected in its size and diversity. Checklist (Ribeiro et al., 2020) argues for the need to go beyond simple accuracy and evaluate the model on basic linguistic capabilities, for example their response to negations. Polyjuice (Wu et al., 2021) perturbs examples using GPT-2 — though this is automatic and scalable, it oﬀers limited control over type of challenging examples generated, making ﬁne-grained analysis beyond global challenge-set level diﬃcult. In contrast, our method oﬀers a richer taxonomy with 117 (and growing) transformations for extensive analysis and comparison.
Tan et al. (2021b) propose decomposing each real world environment into a set of dimensions before using randomly sampled and adversarially optimized transformations to measure the model’s average- and worst-case performance along each dimension. NL-Augmenter can be used, out-of-thebox, to measure average-case performance and we plan to extend it to support worst-case evaluation.
3 NL-Augmenter
NL-Augmenter is a crowd-sourced suite to facilitate rapid augmentation of data for NLP tasks to assist in training and evaluating models. NLaugmenter was introduced in Mille et al. (2021) in the context of the creation of evaluation suites for the GEM benchmark (Gehrmann et al., 2021); three types of evaluation sets were proposed: (i) transformations, i.e. original test sets are perturbed in diﬀerent ways (e.g. backtranslation, introduction of typographical errors, etc.), (ii) subpopulations, i.e. test subsets ﬁltered according to features such as input complexity, input size, etc.; and (iii) data shifts, i.e. new test sets that do not contain any of the original test set material.
In this paper, we present a participant-driven repository for creating and testing transformations and ﬁlters, and for applying them to all dataset splits (training, development, evaluation) and to all NLP tasks (NLG, labeling, question answering, etc.). As shown by Mille et al. (2021), applying ﬁlters and tranformations to developmen-

Format of a Transformation The name of the transformation, ReplaceFinancialAmount followed by the interface SentenceOperation.
The tasks that the transformation is applicable to.The languages for which transformations are generated. And the relevant keywords which categorise the transformation.
class ReplaceFinancialAmount(SentenceOperation): tasks = [ TaskType.TEXT_CLASSIFICATION, TaskType.TEXT_TO_TEXT_GENERATION,] languages = ["en"] keywords = [ "lexical", "rule-based", "external-knowledge-based", "possible-meaning-alteration", "high-precision"]
def __init__(self, seed: int = 0, max_outputs: int = 1): super().__init__(seed=seed, max_outputs=max_outputs)
def generate(self, sentence: str) -> List[str]:
""" The actual logic of the transformation. The ‘generate‘ method takes in a sentence and returns multiple transformed sentences.
""" return transformed_sentences
Figure 2: Participants were expected to write their python class adhering to the above format.

t/evaluation data splits allows for testing the robust- review criteria (see Appendix A) guided partici-

ness of models and for identifying possible biases; pants to follow a style guide, incorporate test cases

on the other hand, applying transformations and ﬁl- in JSON format, and encouraged novelty and speci-

ters to training data (data augmentation) allows for ﬁcity. Apart from the general software develop-

possibly mitigating the detected robustness and bias ment advantages of test cases, they made reviewing

issues (Wang et al., 2021b; Pruksachatkun et al., 1 simpler by providing an overview of the transfor-

2021; Si et al., 2021).

mation’s capability and scope of generations.

In this section, we provide organizational details, list the transformations and ﬁlters that the reposi- 3.2 Transformations and ﬁlters

tory currently contains, and we present the list of Tables 1 and 2 list respectively the 117 transforma-

tags we associated to tranformations and ﬁlters and tions and 23 ﬁlters that are currently found in the

how we introduced them.

NL-Augmenter repository (alphabetically ordered

3.1 Participatory Workshop on GitHub

according to the submission name in the repository). For each transformation/ﬁlter, a link to the

A workshop was organized towards constructing corresponding Appendix subsection is provided,

this full-ﬂedged participant-driven repository. Un- where a detailed description, illustrations and an ex-

like a traditional workshop wherein people submit ernal link to implementation in the NL-Augmenter

papers, participants were asked to submit python repository can be found.

implementations of transformations to the GitHub repository. Organizers of this workshop created a base repository extending Mille et al. (2021)’s

3.3 Tags for the classiﬁcation of perturbations

NLG evaluation suite and incorporated a set of in- We deﬁned a list of tags which are useful for an

terfaces, each of which catered to popular NL ex- eﬃcient navigation in the pool of existing perturba-

ample formats. This formed the backbone of the tions and for understanding the performance char-

repository. A sample set of transformations and acteristics of the contributed transformations and

ﬁlters alongwith evaluation scripts were provided ﬁlters (see e.g. the robustness analysis presented

as starter code. Figure 2 show an annotated code in Section 4.2). There are three main categories of

snippet of a submission. Following the format of tags: (i) General properties tags, (ii) Output proper-

BIG-Bench’s review process, multiple review crite- ties tags, and (iii) Processing properties tags.

ria were designed for accepting contributions. The

General properties tags are shown in Table 3,

Transformation
Abbreviation Transformation Add Hash-Tags Adjectives Antonyms Switch AmericanizeBritishizeEnglish AntonymsSubstitute Auxiliary Negation Removal AzertyQwertyCharsSwap BackTranslation BackTranslation for Named Entity Recognition Butter Fingers Perturbation Butter Fingers Perturbation For Indian Languages Change Character Case Change Date Format Change Person Named Entities Change Two Way Named Entities Chinese Antonym and Synonym Substitution Chinese Pinyin Butter Fingers Perturbation Chinese Person NE and Gender Perturbation Chinese (Simpliﬁed and Traditional) Perturbation City Names Transformation Close Homophones Swap Color Transformation Concatenate Two Random Sentences (Bilingual) Concatenate Two Random Sentences (Monolingual) Concept2Sentence Contextual Meaning Perturbation Contractions and Expansions Perturbation Correct Common Misspellings Country/State Abbreviation Decontextualisation of the main Event Diacritic Removal Disability/Diﬀerently Abled Transformation Discourse Marker Substitution Diverse Paraphrase Generation Dislexia Words Swap Emoji Icon Transformation Emojify English Inﬂectional Variation English Mention Replacement for NER Filler Word Augmentation Style Transfer from Informal to Formal French Conjugation Substitution Gender And Culture Diversity Name Changer Neopronoun Substitution Gender Neutral Rewrite GenderSwapper GeoNames Transformation German Gender Swap Grapheme to Phoneme Substitution Greetings and Farewells Hashtagify Insert English and French Abbreviations Leet Transformation Lexical Counterfactual Generator Longer Location for NER Longer Location Names for testing NER Longer Names for NER Lost in Translation Mixed Language Perturbation

App.
B.1 B.2 B.3 B.4 B.5 B.6 B.7 B.8 B.9 B.10 B.11 B.12 B.13 B.14 B.15 B.16 B.17 B.18 B.19 B.20 B.21 B.22 B.23 B.24 B.25 B.26 B.27 B.28 B.29 B.30 B.31 B.32 B.33 B.34 B.35 B.36 B.37 B.38 B.39 B.40 B.41 B.42 B.43 B.44 B.45 B.46 B.47 B.48 B.49 B.50 B.51 B.52 B.53 B.54 B.55 B.56 B.57 B.58 B.59

Transformation
Mix transliteration MR Value Replacement Multilingual Back Translation Multilingual Dictionary Based Code Switch Multilingual Lexicon Perturbation Causal Negation and Strengthening Question Rephrasing transformation English Noun Compound Paraphraser [N+N] Number to Word Numeric to Word OCR Perturbation Add Noun Deﬁnition Pig Latin Cipher Pinyin Chinese Character Transcription SRL Argument Exchange ProtAugment Diverse Paraphrasing Punctuation Question-Question Paraphraser for QA Question in CAPS Random Word Deletion Random Upper-Case Transformation Double Context QA Replace Abbreviations and Acronyms Replace Financial Amounts Replace Numerical Values Replace Spelling Replace nouns with hyponyms or hypernyms Sampled Sentence Additions Sentence Reordering Emoji Addition for Sentiment Data Shuﬄe Within Segments Simple Ciphers Slangiﬁcator Spanish Gender Swap Speech Disﬂuency Perturbation Paraphrasing through Style Transfer Subject Object Switch Sentence Summarizaiton Suspecting Paraphraser for QA Swap Characters Perturbation Synonym Insertion Synonym Substitution Syntactically Diverse Paraphrasing Subsequence Substitution for Seq. Tagging Tense Token Replacement Based on Lookup Tables Transformer Fill Added Underscore Trick Unit converter Urban Thesaurus Swap Use Acronyms Visual Attack Letter Weekday Month Abbreviation Whitespace Perturbation Context Noise for QA Writing System Replacement Yes-No Question Perturbation Yoda Transformation

App.
B.60 B.61 B.62 B.63 B.64 B.65 B.66 B.67 B.68 B.69 B.70 B.71 B.72 B.73 B.74 B.75 B.76 B.77 B.78 B.79 B.80 B.81 B.82 B.83 B.84 B.85 B.86 B.87 B.88 B.89 B.90 B.91 B.92 B.93 B.94 B.95 B.96 B.97 B.98 B.99 B.100 B.101 B.102 B.103 B.104 B.105 B.106 B.107 B.108 B.109 B.110 B.111 B.112 B.113 B.114 B.115 B.116 B.117

Table 1: List of transformations and link to their detailed descriptions in Appendix

Filter

Code-Mixing Filter Diacritics Filter Encoding Filter Englishness Filter Gender Bias Filter Group Inequity Filter Keyword Filter Language Filter Length Filter Named-entity-count Filter Numeric Filter Oscillatory Hallucinations

Filter

App.
C.1 C.2 C.3 C.4 C.5 C.6 C.7 C.8 C.9 C.10 C.11 C.12

Filter

Polarity Filter Quantitative Question Question type ﬁlter Repetitions Filter Phonetic Match Filter Special Casing Filter Speech-Tag Filter Token-Amount ﬁlter Toxicity Filter Universal Bias Filter Yes/no question ﬁlter

Filter

App.
C.13 C.14 C.15 C.16 C.17 C.18 C.19 C.20 C.21 C.22 C.23

Table 2: List of ﬁlters and link to their detailed descriptions in Appendix

Property Augmented set type General purpose Task type
Language(s) Linguistic level

Deﬁnition Transformation or Filter (Subpopulation)?
What will the data be used for? Augmenting training data? Testing robustness? Finding and ﬁxing biases? Etc. For which NLP task(s) will the perturbation be beneﬁcial?
To which language(s) is is the perturbation applied? On which linguistic level does the perturbation operate?

Tags Filter, Transformation, Multiple (specify), Unclear, N/A
Augmentation, Bias, Robustness, Other (specify), Multiple (specify), Unclear, N/A
Quality estimation, Question answering, Question generation, RDF-to-text generation, Sentiment analysis, Table-to-text generation, Text classiﬁcation, Text tagging, Text-to-text generation
*
Discourse, Semantic, Style, Lexical, Syntactic, Word-order, Morphological, Character, Other (specify), Multiple (specify), Unclear, N/A

Table 3: Criteria and possible tags for General Properties of perturbations

and cover the type of the augmentation, i.e. whether it is a transformation or a ﬁlter (Augmented set type), its general purpose, i.e. whether it is intended for augmentation, robustness, etc. (General purpose), for which NLP tasks the created data will be useful (Task type), to which languages it has been applied (Language(s)), and on which linguistic level of representation it operates, i.e. semantic, syntactic, lexical, etc. (Linguistic level).
Output properties tags, shown in Table 4, apply to transformations only; they provide indications about how the data was aﬀected during the respective transformations. There are currently six properties in this category: one to capture the number of diﬀerent outputs that a transformation can produce (Output/Input ratio), one to capture in which aspect the input and the output are alike (Input/Ouptut similarity), and four to capture intrinsic qualities of the produced text or structured data, namely how were the meaning, the grammaticality, the readability and the naturalness aﬀected by the transforma-

tion (respectively Meaning preservation, Grammaticality preservation, Readability preservation and Naturalness preservation). Note that apart from Output/Input ratio, the output properties tags need to be speciﬁed manually for each transformation/ﬁlter (see Section 3.4), and are thus subject to the interpretation of the annotator.
Processing properties tags, shown in Table 5, capture information related to the type of processing applied on the input (Input data processing), the type of algorithm used (Algorithm type), how it is implemented (Implementation), its estimated precision and recall (Precision/recall) and computational complexity (Computational complexity / Time), and whether an accelerator is required to apply the transformation/ﬁlter (GPU required?).
3.4 Tag retrieval and assignment
Transformation and ﬁlters are assigned tags for each of the properties listed in Tables 3-5. There are two sources for the tags: (i) assigning them

Property Output/input ratio Input/output similarity Meaning preservation
Grammaticality preservation
Readability preservation
Naturalness preservation

Deﬁnition Does the transformation generate one single output for each input, or a few, or many?
On which level are the input and output similar (if applicable)?
If you compare the output with the input, how is the meaning aﬀected by the transformation?
If you compare the output with the input, how is the grammatical correctness aﬀected by the transformation?
If you compare the output with the input, how is the easyness of read aﬀected by the transformation?
If you compare the output with the input, how is the naturalness of the text aﬀected by the transformation?

Tags
=1, >1 (Low), >1 (High), Multiple (specify), Unclear, N/A

Aural, Meaning, Visual, Other (specify), Multiple (specify), Unclear, N/A

Always-preserved,

Possibly-changed,

Always-changed, Possibly-added, Always-

added, Possibly-removed, Always-removed,

Multiple (specify), Unclear, N/A

Always-preserved, Always-impaired, Always-improved, Unclear, N/A

Possibly-impaired, Possibly-improved, Multiple (specify),

Always-preserved, Always-impaired, Always-improved, Unclear, N/A

Possibly-impaired, Possibly-improved, Multiple (specify),

Always-preserved, Always-impaired, Always-improved, Unclear, N/A

Possibly-impaired, Possibly-improved, Multiple (specify),

Table 4: Criteria and possible tags for Output Properties of perturbations (applicable to tranformations only)

Property Input data processing
Implementation Algorithm type
Precision/recall
GPU Required? Computational complexity / Time

Deﬁnition What kind of NL processing is applied to the input?
Is the perturbation implemented as rule-based or model-based? What type of algorithm is used to implement the perturbation?
To what extent does the perturbation generate what it intends to generate (precision)? To what extent does the perturbation return an output for any input (recall)? Is GPU needed to run the perturbation? How would you assess the computational complexity of running the perturbation? Does it need a lot of time to run?

Tags
Addition, Chunking, Paraphrasing, Parsing, PoS-Tagging, Removal, Segmentation, Simpliﬁcation, Stemming, Substitution, Tokenisation, Translation, Other (specify), Multiple (specify), Unclear, N/A
Model-based, Rule-based, Both, Unclear, N/A
API-based, External-knowledge-based, LSTM-based, Transformer-Based, Other (specify), Multiple (specify), Unclear, N/A
High-precision-High-recall, High-precisionLow-recall, Low-precision-High-recall, Lowprecision-Low-recall, Unclear, N/A
No, Yes, Unclear, N/A
High, Medium, Low

Table 5: Criteria and possible tags for Processing Properties of perturbations

manually, and (ii) using existing metadata embedded in the respective source code implementations of each given transformation and ﬁlter. The in-code metadata (see e.g. the Keywords ﬁeld in Figure 2) provides descriptions for each one identiﬁcable aspects such as the language(s) supported, the type of task that the transformation or ﬁlter is applicable for, and other characteristical keywords. The speciﬁcation and type of this metadata was predeﬁned as a requirement for all contributors to the

NL-Augmenter project to enable identiﬁcation of the type of transformation of ﬁlter being written by their respective author(s).
This metadata was initally collected through the creation of an automated script which programatically iterated through each transformation and ﬁlter and gathered all stated metadata. The metadata was then mapped by the script into discrete property groups as deﬁned in Tables 3-5. All contributing authors were invited to review the initally collected

metadata and, where possible, add additional data.
4 Robustness Analysis
All authors of the accepted perturbations were asked to provide the task performance scores for each of their respective transformations or ﬁlters. In Section 4.1 we provide details on how the scores were obtained, and in Section 4.2 we provide a ﬁrst analysis of these scores.
4.1 Experiment
The perturbations are currently split into three groups, according to the task(s) they will be evaluated on: text classiﬁcation tasks, tagging tasks, and question-answering tasks. For experiments in this paper, we focus on text classiﬁcation and on the relevant perturbations. We compare the models’ performance on the original data and on the perturbed data. The percentage of sentences being changed by a transformation (transformation rate) and the percentage of performance drop on the perturbed data compared to the performance on the original data (score variation) are reported.
Tasks. We choose four evaluation datasets among three English NLP tasks: (1) sentiment analysis on both short sentences (SST-2 (Socher et al., 2013)) and full paragraphs (IMDB Movie Review (Maas et al., 2011)), (2) Duplicate question detection (QQP) (Wang et al., 2019a), and (3) Natural Language Inference (MNLI) (Williams et al., 2017). These tasks cover both classiﬁcations on single sentences, as well as pairwise comparisons, and have been widely used in various counterfactual analysis and augmentation experiments (Wu et al., 2021; Kaushik et al., 2019; Gardner et al., 2020; Ribeiro et al., 2020).
Evaluation models. We represent each dataset/task with its corresponding most downloaded large model hosted on Huggingface (Wolf et al., 2020), resulting in four models for evaluation: roberta-base-SST-2, roberta-base-imdb, roberta-large-mnli, and bert-base-uncased-QQP.3
Perturbation strategy. For each task, we perturb a random sample of 20% of the validation set. Since all the transformations are on single text snippets, for datasets with sentence pairs, i.e., QQP
3huggingface.co/{ textattack/roberta-base-SST-2, textattack/roberta-base-imdb, textattack/bert-base-uncased-QQP, roberta-large-mnli }

and MNLI, we perturb the ﬁrst question and the premise sentence, respectively.
4.2 Results and Analysis
In this section, Tables 6 to 16 show the results of the robustness analysis performed on the four datasets described in Section 4.1 and presented according to the tags introduced in Section 3.3.
General purpose (Table 6): Transformations designed with a “robustness testing” objective displayed mean performance drops between 9% and 13.7% across models. Interestingly, 34 sentence transformations designed for “augmentation” tasks showed similar mean robustness drops ranging between 4% and 13%, emphasizing the need to draw on the paraphrasing literature to improve robustness testing.
Task type (Table 7): The results table shows that there is not necessarily a correlation between which task a transformation is marked to be relevant for and which task it actually challenges the robustness of the models on.
Linguistic level (Table 8): Transformations making character level and morphological changes were able to show drastic levels of drops in performance compared to those making lexical or syntactic changes. These drops in performance were consistent across all four models. roberta-large ﬁnetuned on the MNLI dataset was the most brittle - character-level transformations on an average dropped performance by over 31% and morphological changes dropped it by 28% while those which made lexical changes displayed a mean drop of 4.4%. The visual_attack_letters (B.111) transformation, which replaces characters with similarly looking ones (like y and v), shows a large accuracy drop from 94% to 56% on the ‘robertabase‘ model ﬁne tuned on SST. ‘bert-base-uncased‘ ﬁne-tuned on the QQP dataset drops from 92 to 69. roberta-large-mnli drops from 91 to 47. In the case of visual_attack_letters, one can easily conceive a scenario in which a model is applied to OCR text which likely exhibit similar properties. In this case, one may expect similarly poor performance, arguably attributed to a narrow set of characters that the models have been exposed to.
Meaning preservation (Table 10): 22 transformations which were marked as highly meaning preserving surprisingly showed a larger average performance drop as compared to 20 of those which were marked as possibly meaning changing. Not discounting the possibility of the noisiness of the

SST-2 Roberta-base QQP BERT-base-unc. MNLI Roberta-large IMDB Roberta-base

Tag

#All #Evl RT VarS #Evl RT VarS #Evl RT VarS #Evl RT VarS

Augmentation 34

Bias

3

Robustness

15

Other*

1

Multiple*

21

20 0.63 -13.25 20 0.75 -6

1 0.5 -5

2 0.52 -11.5

8 0.82 -9.38 7 0.59 -8.14

1 0.5 -38

1 0.5 -23

13 0.72 -4.15 13 0.64 -5.08

18 0.74 -8.89 17 0.73 -4.41

2 0.53 -16

1 0.71 0

7 0.65 -12.14 7 0.88 -13.71

1 0.5 -44

1 0.6

1

12 0.68 -4.08 11 0.92 -5.64

Total

74 43

43

40

37

Table 6: Results of the robustness evaluation from the perspective of the General purpose criterion (#All = Total number of tags, #Evl Total number of evaluations collected, RT = Transformation rate, VarS = Score variation)

SST-2 Roberta-base QQP BERT-base-unc. MNLI Roberta-large IMDB Roberta-base

Tag

#All #Evl RT VarS #Evl RT VarS #Evl RT VarS #Evl RT VarS

Qual. estim.

2

Question ans.

3

Question gen.

2

RDF to text

1

Sentiment ana. 4

Table to text

1

Text class.

95

Text tagging

25

Text to text gen. 92

2 0.52 -2.5 2 0.51 -6

2 0.7 -0.5 2 0.89 -1.5

1 0.41 0

1 0.77 -1

1 0.01 0

1 0.02 0

1 0.99 -12

1 0.99 -14

1 0.01 0

1 0.02 0

52 0.71 -9.27 52 0.68 -6.21

17 0.79 -10.94 17 0.64 -6.82

49 0.69 -8.86 49 0.66 -5.86

2 0.53 -6.5 1 0.56 0

2 0.77 -1

2 0.98 -4

1 0.54 -2

1 0.97 -5

1 0.04 0

1 0.21 0

1 0.93 -18

1

1

-15

1 0.04 0

1 0.21 0

49 0.69 -8.33 43 0.83 -5.74

16 0.66 -9.75 13 0.84 -9.23

46 0.68 -7.57 40 0.79 -5.62

Total

231 126

126

119

103

Table 7: Results of the robustness evaluation from the perspective of the Task type criterion (#All = Total number of tags, #Evl Total number of evaluations collected, RT = Transformation rate, VarS = Score variation)

SST-2 Roberta-base QQP BERT-base-unc. MNLI Roberta-large IMDB Roberta-base

Tag

#All #Evl RT VarS #Evl RT VarS #Evl RT VarS #Evl RT VarS

Semantic

3

Lexical

44

Syntactic

3

Word-order

2

Morphological 3

Character

6

Other*

1

Multiple*

25

Unclear

1

1

1

-35

1

1

-20

30 0.67 -5.83 30 0.61 -5

1

1

-8

1 0.74 -7

2 0.6 -1.5 2 0.61 -1

2 0.75 -25.5 2 0.75 -21.5

2

1 -16.5 2 1.0 -12.5

1

0

0

1 0.7

-4

9 0.74 -11.22 9 0.71 -7

1

1

-46

1 0.79 -2

1 1.0 -42

1

1

-3

30 0.64 -4.4 25 0.73 -2.44

1 0.85 -15

1

1

0

2 0.63 -2

1

1

0

2 0.75 -28.5 2 0.8 -4.5

1 0.95 -31

2

1 -26

0

1

1

-1

9 0.74 -12.56 8 0.8 -14.5

0

0

Total

92 49

49

46

41

Table 8: Results of the robustness evaluation from the perspective of the Linguistic level criterion (#All = Total number of tags, #Evl Total number of evaluations collected, RT = Transformation rate, VarS = Score variation)

SST-2 Roberta-base QQP BERT-base-unc. MNLI Roberta-large IMDB Roberta-base

Tag

#All #Evl RT VarS #Evl RT VarS #Evl RT VarS #Evl RT VarS

Aural

5

Meaning 51

Visual

12

Other*

5

Multiple* 2

N/A

2

3

1 -4.33 3 0.7 -6.67

31 0.6 -8.58 32 0.64 -5.72

7 0.86 -15.29 6 0.8 -10.17

1 0.83 0

1 0.55 -4

1

1

-34

1

1

-20

2 0.92 -1

2 0.67 -6

2 0.7 -6.5 31 0.64 -7.52 5 0.8 -12.8 1 0.69 -2 1 1.0 -38 2 0.77 -5

3 0.85 -3.67

28 0.74 -5.75

5 0.92 -1

0

2

1

-23

0

Total

77 45

45

42

38

Table 9: Results of the robustness evaluation from the perspective of the Input/output similarity criterion (#All = Total number of tags, #Evl Total number of evaluations collected, RT = Transformation rate, VarS = Score variation)

SST-2 Roberta-base QQP BERT-base-unc. MNLI Roberta-large IMDB Roberta-base

Tag

#All #Evl RT VarS #Evl RT VarS #Evl RT VarS #Evl RT VarS

Alw. preserved 40

Poss. changed 33

Alw. changed 12

Alw. added

2

Poss. removed 2

22 0.65 -9.77 22 0.63 -7.36

20 0.78 -5.45 20 0.73 -5.15

5 0.7 -4

5 0.54 -5.4

1

0

-94

1 0.7

-4

2

1 -18 2

1

-13

22 0.61 -11.23 19 0.72 -9.89

17 0.75 -4.76 18 0.87 -1.5

5 0.61 -6.8 3 0.78 -7.33

1 0.78 0

1 0.99 -1

2 0.88 -23.5 1

1

-3

Total

89 50

50

47

42

Table 10: Results of the robustness evaluation from the perspective of the Meaning preservation criterion (#All = Total number of tags, #Evl Total number of evaluations collected, RT = Transformation rate, VarS = Score variation)

SST-2 Roberta-base QQP BERT-base-unc. MNLI Roberta-large IMDB Roberta-base

Tag

#All #Evl RT VarS #Evl RT VarS #Evl RT VarS #Evl RT VarS

Alw. preserved 31

Poss. impaired 36

Alw. impaired 2

Poss. improved 6

Unclear

1

N/A

2

19 0.59 -10.58 19 0.52 -4.63

20 0.69 -3.15 20 0.69 -4.55

1 0.93 -7

1 0.94 -20

6 0.83 -16.33 6 0.8 -8.17

1

1

-34

1

1

-20

2

1 -23.5 2

1

-22

18 0.53 -8.11 17 0.76 -4.94

19 0.72 -4.21 18 0.81 -2.11

1 0.92 -16 1

1

-1

5 0.79 -14.8 2 0.52 -1.5

1 1.0 -38

1

1

-45

2

1

-27

2

1 -36.5

Total

79 49

49

46

41

Table 11: Results of the robustness evaluation from the perspective of the Grammaticality preservation criterion (#All = Total number of tags, #Evl Total number of evaluations collected, RT = Transformation rate, VarS = Score variation)

SST-2 Roberta-base QQP BERT-base-unc. MNLI Roberta-large IMDB Roberta-base

Tag

#All #Evl RT VarS #Evl RT VarS #Evl RT VarS #Evl RT VarS

Alw. preserved 25

Poss. impaired 38

Alw. impaired 9

Poss. improved 4

Alw. improved 2

Unclear

1

15 0.66 -3

15 0.54 -3.47

24 0.64 -10.67 24 0.69 -6.25

4

1 -25.25 4 1.0 -17.25

4 0.75 -11.75 4 0.75 -8.75

1

-1

1

-6

1

1

0

1 0.06 0

15 0.56 -5.53 12 0.83 -2.33

22 0.69 -6.59 22 0.79 -2.41

3 0.98 -36.67 4

1

-40

4 0.75 -16.25 2 0.52 -1.5

1 0.77 -5

0

1 0.15 0

1 0.32 0

Total

79 49

49

46

41

Table 12: Results of the robustness evaluation from the perspective of the Readability preservation criterion (#All = Total number of tags, #Evl Total number of evaluations collected, RT = Transformation rate, VarS = Score variation)

SST-2 Roberta-base QQP BERT-base-unc. MNLI Roberta-large IMDB Roberta-base

Tag

#All #Evl RT VarS #Evl RT VarS #Evl RT VarS #Evl RT VarS

Alw. preserved 18

Poss. impaired 45

Alw. impaired 8

Poss. improved 4

Unclear

1

9 0.59 -3.33 10 0.52 -3.5

29 0.66 -8.48 29 0.64 -5.38

4 1.0 -20.5 4 1.0 -16.25

4 0.75 -11.75 4 0.75 -8.75

1

1

-34

1

1

-20

9 0.51 -7.44 9 0.75 -2.56

27 0.67 -5.15 24 0.79 -1.75

4 0.97 -23.25 4

1 -32.25

4 0.75 -16.25 2 0.52 -1.5

1 1.0 -38

1

1

-45

Total

77 47

48

45

40

Table 13: Results of the robustness evaluation from the perspective of the Naturalness preservation criterion (#All = Total number of tags, #Evl Total number of evaluations collected, RT = Transformation rate, VarS = Score variation)

SST-2 Roberta-base QQP BERT-base-unc. MNLI Roberta-large IMDB Roberta-base

Tag

#All #Evl RT VarS #Evl RT VarS #Evl RT VarS #Evl RT VarS

Addition

1

Paraphrasing 5

Parsing

1

PoS-Tagging 5

Removal

2

Segmentation 3

Substitution 17

Tokenisation 23

Translation

3

Other*

3

Multiple*

13

Unclear

1

N/A

3

1

0

-94

1 0.7

-4

5 0.79 -1.8 5 0.74 -5.6

1 0.02 0

1 0.16 -1

3 0.44 -11.67 3 0.54 -6.67

2

1

-4.5

2 0.74 -6.5

1

1

-4

1 0.93 -6

13 0.63 -8.08 14 0.61 -8

9 0.67 -4.89 9 0.5 -4.22

2 0.99 -11

2 0.99 -13.5

2

1

-17

2 1.0 -10

6 0.69 -1.33 5 0.6 -2.2

1

1

-46

1 0.79 -2

2 0.85 -18.5 2 0.9 -14

1 0.78 0

1 0.99 -1

4 0.77 -6.25 3 0.77 -0.67

1 0.15 0

1 0.59 0

3 0.54 -14.33 2 0.98 -1.5

2 0.81 -10

1

1

0

1 0.94 -5

1

1

-4

14 0.64 -9.36 13 0.67 -5

9 0.54 -4.56 10 0.76 -3.8

2 0.97 -18.5 1

1

-15

1 0.95 -38

2

1 -23

5 0.58 -4.8 3 0.72 -2

0

0

2 0.89 -20.5 2

1

-32

Total

81 48

48

45

40

Table 14: Results of the robustness evaluation from the perspective of the Input data processing criterion (#All = Total number of tags, #Evl Total number of evaluations collected, RT = Transformation rate, VarS = Score variation)

SST-2 Roberta-base QQP BERT-base-unc. MNLI Roberta-large IMDB Roberta-base

Tag

#All #Evl RT VarS #Evl RT VarS #Evl RT VarS #Evl RT VarS

Model-based 19

Rule-based 66

Both

6

Unclear

1

11 0.95 -11.27 11 0.93 -7.64

38 0.65 -9.24 38 0.61 -6.26

2 0.31 0

2 0.5 -0.5

1

1

-7

1 0.84 -4

9 0.93 -11.78 7 0.81 -2.43

37 0.64 -8.14 34 0.79 -6.5

2 0.42 -1.5 1 0.97 -5

1 0.9 -2

1

1

-1

Total

103 52

52

49

43

Table 15: Results of the robustness evaluation from the perspective of the Implementation criterion (#All = Total number of tags, #Evl Total number of evaluations collected, RT = Transformation rate, VarS = Score variation)

SST-2 Roberta-base QQP BERT-base-unc. MNLI Roberta-large IMDB Roberta-base

Tag

#All #Evl RT VarS #Evl RT VarS #Evl RT VarS #Evl RT VarS

API-based

22

Ext. K.-based 33

LSTM-based 1

Transf.-based 15

Multiple*

3

Unclear

1

N/A

24

14 0.78 -7.86 14 0.67 -7

19 0.47 -11 19 0.55 -6.95

1

1

0

1 1.0

0

7 0.89 -9.57 7 0.85 -5.29

1 0.41 0

1 0.77 -1

0

0

4 1.0 -13.25 4 0.77 -8.5

13 0.73 -9.23 11 0.88 -11.45

19 0.55 -7.89 20 0.68 -4.45

0 0.9

1

1

-1

6 0.87 -7.17 1

1

-4

1 0.54 -2

1 0.97 -5

0

1

-1

4 0.75 -18.75 3 0.89 -6

Total

103 46

46

43

38

Table 16: Results of the robustness evaluation from the perspective of the Algorithm type criterion (#All = Total number of tags, #Evl Total number of evaluations collected, RT = Transformation rate, VarS = Score variation)

transformation’s logic, we believe further investigation could help understand whether models focus on the meaning of words or sentences or take shortcuts by focusing on commonly occuring surface forms associated with a particular prediction, as was already shown for some phenomena by McCoy et al. (2019), among others.
Grammaticality preservation (Table 11): Preserving grammaticality did not correlate with high robustness. Transformations marked as grammaticality always-preserved showed signiﬁcant average drops of 10.6%, 8.1% and 4.6% across roberta-base-SST-2, roberta-large-mnli and bert-base-uncased-QQP respectively. For example, the grapheme_to_phoneme transforma-

tion showed drastic drops in performance: 13%, 20% and 13% respectively.
Readability and Naturalness (Tables 12-13): In general, as expected, the transformations tagged as modifying the readability or naturalness show large drops across all tasks and models, in particular the ones tagged as “always imparing” the input.
Unsurprisingly, many of the injected perturbations, despite being artiﬁcial would not distract human readers from the actual meaning and intent of the text (e.g. simple_ciphers transformation (B.91)). Character level perturbations might not distract human readers as much as compared to word level perturbations but the above language models on the other hand behaved contrarily. Such

departure from learning meaningful abstractions is further validated with the low correlation of grammaticality preservation and robustness. These results further re-question how we can expand these models from being just pure statistical learners to those which can incorporate meaning and surfacelevel abstraction, both across natural as well as artiﬁcial constructs. The large drops in performance of such perturbations necessitate looking at expanding training sets with even artiﬁcial data sources as well expand our deﬁnitions of text similarity from pure linguistic ones to those which abstract morphological, visual and other errors which can be unambiguous to humans.
Tables 9, 14, 15 and 16 show the robustness scores for Input/Output similarity, Input processing, Implementation and Algorithm type respectively. The score drops for these criteria may not be easily interpretable; e.g. that model-based implementations showed comparatively larger average drops as compared to rule-based implementations may not be due to the diﬀerence in implementation, but rather to which transformations were implemented that way .
5 Discussion and Broader Impact
Limitations In Section 4.2, we analyze the results of applying some of the transformations on existing datasets and running models on the perturbed data. Even though it was not possible to test all of the currently existing perturbations (mostly due to time constraints), the overall results show that the tested perturbations do pose a challenge to diﬀerent models on diﬀerent tasks, with quasisystematic score drops. However, with so many transformations applied to four diﬀerent datasets, the presented robustness analysis can only be shallow, and a separate analysis of each transformation would be needed in order to get more informative insights. Second, our superﬁcial analysis above relies on tags which were in many cases annotated by hand, and some of the suprising results (e.g. meaning-preserving transformations are more challenging than non-meaning-preserving ones) may reﬂect a lack of consistency in the annotations. We believe that assessing the quality of the tag assignment so as to ensure a high inter-annotator agreement will be needed for reliable analyses in the future. Finally, the current robustness analysis only shows that the perturbations are eﬀective for detecting a possible weakness in a model; further

experiments are needed to demonstrate that the perturbations can also help mitigating the weaknesses they bring to light.
Dilution of Contributions While this is not our intent, there is a risk in large scale collections of work like this that individual contributions are being less appreciated than releasing them as a standalone project. This risk is a tradeoﬀ with the advantage that it becomes much easier to switch between diﬀerent transformations, which can lead to a better adoption of introduced methods. To proactively give appropriate credit, each transformation has a data card mentioning the contributors and all participants are listed as co-authors of this paper. We further encourage all users of our repository to cite the work that a speciﬁc implementation builds on, if appropriate. The relevant citations are listed on the respective data cards and in the description in the appendix. In the same vein, there is a risk of NL-Augmenter as a whole to monopolize the augmentation space due to its large scope, leading to less usage of related work which may cover additional transformations or ﬁlters. While this is not our intention and we actively worked with contributors to related repositories to integrate their work, we encourage researchers to try other solutions as well.
Participatory Setup Conducting research in environments with a shared mission, a low barrier of entry, and directly involving aﬀected communities was popularizied by Nekoto et al. (2020). This kind of participatory work has many advantages, most notably that it changes the typically prescriptive research workﬂow toward a more inclusive one. Another advantage is that through open science, anyone can help shape the overall mission and improve the end result. Following the related BIG-bench project, we aimed to design NLAugmenter in a similar spirit – by providing the infrastructure, the participation barrier is reduced to ﬁlling a templated interface and providing test example. By making the interface as ﬂexible as possible, the contributions range from ﬁlters for subpopulations with speciﬁc protected attributes to transformations via neural style transfer. Through this wide range, we hope that researchers can apply a wider range of augmentation and evaluations strategies to their data and models.

6 Conclusion
In this paper, we introduced NL-Augmenter, a framework for text transformations and ﬁlters with the goal to assisst in robustness testing and other data augmentation tasks. We demonstrated that through an open participation strategy, NLAugmenter can cover a much wider set of languages, tasks, transformations, and ﬁlters than related work without a loss of focus. In total, our repository provides > 117 transformations and > 23 ﬁlters which are all documented and tested and will contribute toward more robust NLP models and an evaluation thereof. As we point out in our analysis, there is much room to improve NL-Augmenter. We welcome future contributions to improve its coverage of the potential augmentation space and to address its current shortcomings. Future work may further include data augmentation experiments at a larger scale to investigate the eﬀect on model robustness.
7 Organization
NL-Augmenter is a large eﬀort organized by researchers and developers ranging across diﬀerent niches of NLP. To acknowledge everyone’s contributions, we list the contribution statements below for all. Steering Committee: Kaustubh Dhole, Varun Gangal, Sebastian Gehrmann, Aadesh Gupta, Zhenhao Li, Saad Mahmood, Simon Mille, Jascha SohlDickstein, Ashish Srivastava, Samson Tan, Tongshuang Wu and Abinaya Mahendiran make up the steering committee. Jinho D. Choi, Eduard Hovy & Sebastian Ruder provided guidance and feedback. Kaustubh Dhole coordinates and leads the NL-Augmenter eﬀort. All others provide feedback and discuss larger decisions regarding the direction of NL-Augmenter and act as organizers and reviewers. Repository: Kaustubh, Aadesh, Zhenhao, Tongshuang, Ashish, Saad, Varun & Abinaya created the interfaces and the base repository NLAugmenter for participants to contribute. This was also a continuation of the repository developed for creating challenge sets (Mille et al., 2021) for GEM (Gehrmann et al., 2021). All the other authors expanded this repository with their implementations. Reviewers: Kaustubh, Simon, Zhenhao, Sebastian, Varun, Samson, Abinaya, Saad, Tongshuang, Aadesh, Ondrej were involved in reviewing the

submissions of participants of the ﬁrst phase. In the 2nd phase, all other authors performed a crossreview, in which participants were paired with 3 other partcipants. This was followed by a meta review by the organizers.
Robustness Evaluation: Ashish, Tongshuang, Kaustubh & Zhenhao created the evaluation engine. Simon, Kaustubh, Saad, Abinaya & Tongshuang performed the robustness analysis.
Website: Aadesh and Sebastian created the webpages for the project.
References
2006. Respectful Disability Language: Here’s What’s Up! https://www.aucd.org/docs/add/sa_ summits/Language%20Doc.pdf.
David Bamman. 2017. Natural language processing for the long tail. In DH.
Alexandre Berard, Ioan Calapodescu, and Claude Roux. 2019. Naver labs europe’s systems for the wmt19 machine translation robustness task. arXiv preprint arXiv:1907.06488.
Rahul Bhagat and Eduard Hovy. 2013. What is a paraphrase? Computational Linguistics, 39(3):463–472.
Abhinav Bhatt and Kaustubh D. Dhole. 2020. Benchmarking biorelex for entity tagging and relation extraction.
Steven Bird. 2006. Nltk: the natural language toolkit. In Proceedings of the COLING/ACL 2006 Interactive Presentation Sessions, pages 69–72.
Smorga’s Board. 2021. Frequently misspelled word list for dyslexia.
Claire Bonial, Jena Hwang, Julia Bonn, Kathryn Conger, Olga Babko-Malaya, and Martha Palmer. 2012. English propbank annotation guidelines. Center for Computational Language and Education Research Institute of Cognitive Science University of Colorado at Boulder, 48.
Jiaao Chen, Derek Tam, Colin Raﬀel, Mohit Bansal, and Diyi Yang. 2021. An empirical survey of data augmentation for limited data learning in nlp.
Xiang Dai and Heike Adel. 2020. An analysis of simple data augmentation for named entity recognition. In Proceedings of the 28th International Conference on Computational Linguistics, pages 3861– 3867, Barcelona, Spain (Online). International Committee on Computational Linguistics.
Prithiviraj Damodaran. Styleformer.

Sebastian Deorowicz and Marcin G Ciura. 2005. Correcting spelling errors by modelling their causes. International journal of applied mathematics and computer science, 15:275–285.
Kaustubh D. Dhole. 2020. Resolving intent ambiguities by retrieving discriminative clarifying questions.
Emily Dinan, Angela Fan, Adina Williams, Jack Urbanek, Douwe Kiela, and Jason Weston. 2020. Queens are powerful too: Mitigating gender bias in dialogue generation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 8173–8188, Online. Association for Computational Linguistics.
William B Dolan and Chris Brockett. 2005. Automatically constructing a corpus of sentential paraphrases. In Proceedings of the Third International Workshop on Paraphrasing (IWP2005).
Thomas Dopierre, Christophe Gravier, and Wilfried Logerais. 2021. Protaugment: Unsupervised diverse short-texts paraphrasing for intent detection metalearning. CoRR, abs/2105.12995.
Steﬀen Eger and Yannik Benz. 2020. From hero to zéroe: A benchmark of low-level adversarial attacks.
Steﬀen Eger, Gözde Gül S¸ ahin, Andreas Rücklé, JiUng Lee, Claudia Schulz, Mohsen Mesgar, Krishnkant Swarnkar, Edwin Simpson, and Iryna Gurevych. 2019a. Text processing like humans do: Visually attacking and shielding NLP systems. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 1634–1647, Minneapolis, Minnesota. Association for Computational Linguistics.
Steﬀen Eger, Gözde Gül Sahin, Andreas Rücklé, JiUng Lee, Claudia Schulz, Mohsen Mesgar, Krishnkant Swarnkar, Edwin Simpson, and Iryna Gurevych. 2019b. Text processing like humans do: Visually attacking and shielding NLP systems. CoRR, abs/1903.11508.
Ben Eisner, Tim Rocktäschel, Isabelle Augenstein, Matko Bošnjak, and Sebastian Riedel. 2016. emoji2vec: Learning emoji representations from their description. In Proceedings of The Fourth International Workshop on Natural Language Processing for Social Media, pages 48–54, Austin, TX, USA. Association for Computational Linguistics.
Marzieh Fadaee, Arianna Bisazza, and Christof Monz. 2017. Data augmentation for low-resource neural machine translation. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 567– 573, Vancouver, Canada. Association for Computational Linguistics.

Angela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi Ma, Ahmed El-Kishky, Siddharth Goyal, Mandeep Baines, Onur Celebi, Guillaume Wenzek, Vishrav Chaudhary, et al. 2021. Beyond english-centric multilingual machine translation. Journal of Machine Learning Research, 22(107):1–48.

Angela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi Ma, Ahmed El-Kishky, Siddharth Goyal, Mandeep Baines, Onur Çelebi, Guillaume Wenzek, Vishrav Chaudhary, Naman Goyal, Tom Birch, Vitaliy Liptchinsky, Sergey Edunov, Edouard Grave, Michael Auli, and Armand Joulin. 2020. Beyond english-centric multilingual machine translation. ArXiv, abs/2010.11125.

Steven Y Feng, Varun Gangal, Jason Wei, Sarath Chandar, Soroush Vosoughi, Teruko Mitamura, and Eduard Hovy. 2021. A survey of data augmentation approaches for nlp. arXiv preprint arXiv:2105.03075.

Francis Galton. 1907. Vox populi (the wisdom of crowds). Nature, 75(7):450–451.

Varun Gangal, Steven Y Feng, and Teruko Mitamura. 2021. narrative reordering problem. arXiv:2104.06669.

Eduard Hovy, Nareor: The arXiv preprint

Matt Gardner, Yoav Artzi, Victoria Basmov, Jonathan Berant, Ben Bogin, Sihao Chen, Pradeep Dasigi, Dheeru Dua, Yanai Elazar, Ananth Gottumukkala, et al. 2020. Evaluating models’ local decision boundaries via contrast sets. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings, pages 1307–1323.

Jon Gauthier, Jennifer Hu, Ethan Wilcox, Peng Qian, and Roger Levy. 2020. Syntaxgym: An online platform for targeted evaluation of language models. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 70–76.

Sebastian Gehrmann, Tosin Adewumi, Karmanya Aggarwal, Pawan Sasanka Ammanamanchi, Anuoluwapo Aremu, Antoine Bosselut, Khyathi Raghavi Chandu, Miruna-Adriana Clinciu, Dipanjan Das, Kaustubh Dhole, Wanyu Du, Esin Durmus, Ondˇrej Dušek, Chris Chinenye Emezue, Varun Gangal, Cristina Garbacea, Tatsunori Hashimoto, Yufang Hou, Yacine Jernite, Harsh Jhamtani, Yangfeng Ji, Shailza Jolly, Mihir Kale, Dhruv Kumar, Faisal Ladhak, Aman Madaan, Mounica Maddela, Khyati Mahajan, Saad Mahamood, Bodhisattwa Prasad Majumder, Pedro Henrique Martins, Angelina McMillanMajor, Simon Mille, Emiel van Miltenburg, Moin Nadeem, Shashi Narayan, Vitaly Nikolaev, Andre Niyongabo Rubungo, Salomey Osei, Ankur Parikh, Laura Perez-Beltrachini, Niranjan Ramesh Rao, Vikas Raunak, Juan Diego Rodriguez, Sashank Santhanam, João Sedoc, Thibault Sellam, Samira Shaikh, Anastasia Shimorina, Marco Antonio Sobrevilla Cabezudo, Hendrik Strobelt, Nishant

Subramani, Wei Xu, Diyi Yang, Akhila Yerukola, and Jiawei Zhou. 2021. The GEM benchmark: Natural language generation, its evaluation and metrics. In Proceedings of the 1st Workshop on Natural Language Generation, Evaluation, and Metrics (GEM 2021), pages 96–120, Online. Association for Computational Linguistics.
Daniel Gildea and Martha Stone Palmer. 2002. The necessity of parsing for predicate argument recognition. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, July 6-12, 2002, Philadelphia, PA, USA, pages 239–246. ACL.
Karan Goel, Nazneen Rajani, Jesse Vig, Samson Tan, Jason Wu, Stephan Zheng, Caiming Xiong annd Mohit Bansal, and Christopher Ré. 2021. Robustness Gym: Unifying the NLP evaluation landscape. arXiv preprint arXiv:2101.04840.
Yoav Goldberg. 2017. Neural network methods for natural language processing. Synthesis lectures on human language technologies, 10(1):1–309.
Tanya Goyal and Greg Durrett. 2020. Neural syntactic preordering for controlled paraphrase generation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 238– 252, Online. Association for Computational Linguistics.
Sharath Chandra Guntuku, Mingyang Li, Louis Tay, and Lyle H Ungar. 2019. Studying cultural diﬀerences in emoji usage across the east and the west. In Proceedings of the International AAAI Conference on Web and Social Media, volume 13, pages 226– 235.
Aadesh Gupta, Kaustubh D. Dhole, Rahul Tarway, Swetha Prabhakar, and Ashish Shrivastava. 2021. Candle: Decomposing conditional and conjunctive queries for task-oriented dialogue systems.
Fabrice Harel-Canada. 2021. Sibyl.
Iris Hendrickx, Zornitsa Kozareva, Preslav Nakov, Diarmuid Ó Séaghdha, Stan Szpakowicz, and Tony Veale. 2013. Semeval-2013 task 4: Free paraphrases of noun compounds. In Second Joint Conference on Lexical and Computational Semantics (* SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval 2013), pages 138–143.
hyperreality@GitHub. American british english translator. https://github.com/hyperreality/ American-British-English-Translator.
Hamid Jalalzai, Pierre Colombo, Chloé Clavel, Eric Gaussier, Giovanna Varni, Emmanuel Vignon, and Anne Sabourin. 2020. Heavy-tailed representations, text polarity classiﬁcation &amp; data augmentation. In Advances in Neural Information Processing Systems, volume 33, pages 4295–4307. Curran Associates, Inc.

Robin Jia and Percy Liang. 2017a. Adversarial examples for evaluating reading comprehension systems. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, EMNLP 2017, Copenhagen, Denmark, September 911, 2017, pages 2021–2031. Association for Computational Linguistics.
Robin Jia and Percy Liang. 2017b. Adversarial examples for evaluating reading comprehension systems. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2021–2031, Copenhagen, Denmark. Association for Computational Linguistics.
Ishan Jindal, Ranit Aharonov, Siddhartha Brahma, Huaiyu Zhu, and Yunyao Li. 2020. Improved semantic role labeling using parameterized neighborhood memory adaptation. arXiv preprint arXiv:2011.14459.
Divyansh Kaushik, Eduard Hovy, and Zachary C Lipton. 2019. Learning the diﬀerence that makes a diﬀerence with counterfactually-augmented data. arXiv preprint arXiv:1909.12434.
Hrant Khachatrian, Lilit Nersisyan, Karen Hambardzumyan, Tigran Galstyan, Anna Hakobyan, Arsen Arakelyan, A. Rzhetsky, and A. G. Galstyan. 2019. Biorelex 1.0: Biological relation extraction benchmark. In BioNLP@ACL.
Douwe Kiela, Max Bartolo, Yixin Nie, Divyansh Kaushik, Atticus Geiger, Zhengxuan Wu, Bertie Vidgen, Grusha Prasad, Amanpreet Singh, Pratik Ringshia, Zhiyi Ma, Tristan Thrush, Sebastian Riedel, Zeerak Waseem, Pontus Stenetorp, Robin Jia, Mohit Bansal, Christopher Potts, and Adina Williams. 2021. Dynabench: Rethinking benchmarking in NLP. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4110–4124, Online. Association for Computational Linguistics.
Paul R. Kingsbury and Martha Palmer. 2002. From treebank to propbank. In Proceedings of the Third International Conference on Language Resources and Evaluation, LREC 2002, May 29-31, 2002, Las Palmas, Canary Islands, Spain. European Language Resources Association.
Tomáš Kocˇisky`, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, Gábor Melis, and Edward Grefenstette. 2018. The narrativeqa reading comprehension challenge. Transactions of the Association for Computational Linguistics, 6:317–328.
Venelin Kovatchev, Phillip Smith, Mark Lee, and Rory Devine. 2021. Can vectors read minds better than experts? comparing data augmentation strategies for the automated scoring of children’s mindreading ability. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),

pages 1196–1206, Online. Association for Computational Linguistics.
Kalpesh Krishna, John Wieting, and Mohit Iyyer. 2020. Reformulating unsupervised style transfer as paraphrase generation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 737–762, Online. Association for Computational Linguistics.
Ashutosh Kumar, Satwik Bhattamishra, Manik Bhandari, and Partha Talukdar. 2019. Submodular optimization-based diverse paraphrasing and its effectiveness in data augmentation. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 3609–3619, Minneapolis, Minnesota. Association for Computational Linguistics.
Guillaume Lample, Alexis Conneau, Marc’Aurelio Ranzato, Ludovic Denoyer, and Hervé Jégou. 2018. Word translation without parallel data. In International Conference on Learning Representations.
Charlyn M Laserna, Yi-Tai Seih, and James W Pennebaker. 2014. Um... who like says you know: Filler word use as a function of age, gender, and personality. Journal of Language and Social Psychology, 33(3):328–338.
Mark Lauer. 1995. Designing Statistical Language Learners: Experiments on Noun Compounds. Ph.D. thesis.
Kenton Lee, Luheng He, and Luke Zettlemoyer. 2018. Higher-order coreference resolution with coarse-toﬁne inference. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 687–692.
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. Bart: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7871–7880.
Quentin Lhoest, Albert Villanova del Moral, Yacine Jernite, Abhishek Thakur, Patrick von Platen, Suraj Patil, Julien Chaumond, Mariama Drame, Julien Plu, Lewis Tunstall, Joe Davison, Mario Šaško, Gunjan Chhablani, Bhavitvya Malik, Simon Brandeis, Teven Le Scao, Victor Sanh, Canwen Xu, Nicolas Patry, Angelina McMillan-Major, Philipp Schmid, Sylvain Gugger, Clément Delangue, Théo Matussière, Lysandre Debut, Stas Bekman, Pierric Cistac, Thibault Goehringer, Victor Mustar, François Lagunas, Alexander M. Rush, and Thomas Wolf. 2021a. Datasets: A community library for natural language processing.

Quentin Lhoest, Albert Villanova del Moral, Patrick von Platen, Thomas Wolf, Mario Šaško, Yacine Jernite, Abhishek Thakur, Lewis Tunstall, Suraj Patil, Mariama Drame, Julien Chaumond, Julien Plu, Joe Davison, Simon Brandeis, Victor Sanh, Teven Le Scao, Kevin Canwen Xu, Nicolas Patry, Steven Liu, Angelina McMillan-Major, Philipp Schmid, Sylvain Gugger, Nathan Raw, Sylvain Lesage, Anton Lozhkov, Matthew Carrigan, Théo Matussière, Leandro von Werra, Lysandre Debut, Stas Bekman, and Clément Delangue. 2021b. huggingface/datasets: 1.14.0.

Dianqi Li, Yizhe Zhang, Hao Peng, Liqun Chen, Chris Brockett, Ming-Ting Sun, and Bill Dolan. 2020a. Contextualized perturbation for textual adversarial attack. arXiv preprint arXiv:2009.07502.

Dianqi Li, Yizhe Zhang, Hao Peng, Liqun Chen, Chris Brockett, Ming-Ting Sun, and Bill Dolan. 2020b. Contextualized perturbation for textual adversarial attack. CoRR, abs/2009.07502.

Zhenhao Li and Lucia Specia. 2019. Improving neural machine translation robustness via data augmentation: Beyond back-translation. Proceedings of the 5th Workshop on Noisy User-generated Text (WNUT 2019).

Bill Yuchen Lin, Wangchunshu Zhou, Ming Shen, Pei Zhou, Chandra Bhagavatula, Yejin Choi, and Xiang Ren. 2019. Commongen: A constrained text generation challenge for generative commonsense reasoning. arXiv preprint arXiv:1911.03705.

Zihan Liu, Genta Indra Winata, and Pascale Fung. 2021. Continual mixed-language pre-training for extremely low-resource neural machine translation. arXiv preprint arXiv:2105.03953.

Zihan Liu, Genta Indra Winata, Zhaojiang Lin, Peng Xu, and Pascale Fung. 2020. Attention-informed mixed-language training for zero-shot cross-lingual task-oriented dialogue systems. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 34, pages 8433–8440.

Lajanugen Logeswaran, Honglak Lee, and Samy Bengio. 2018. Content preserving text generation with attribute controls. In Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montréal, Canada, pages 5108–5118.

Kaiji Lu, Piotr Mardziel, Fangjing Wu, Preetam Amancharla, and Anupam Datta. 2019. Gender bias in neural natural language processing.

Edward Ma. 2019.

Nlp augmentation.

https://github.com/makcedward/nlpaug.

Andrew Maas, Raymond E Daly, Peter T Pham, Dan Huang, Andrew Y Ng, and Christopher Potts. 2011. Learning word vectors for sentiment analysis. In

Proceedings of the 49th annual meeting of the association for computational linguistics: Human language technologies, pages 142–150.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313–330.
Vukosi Marivate and Tshephisho Sefara. 2020. Improving short text classiﬁcation through global augmentation methods. In International Cross-Domain Conference for Machine Learning and Knowledge Extraction, pages 385–399. Springer.
Tom McCoy, Ellie Pavlick, and Tal Linzen. 2019. Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3428–3448, Florence, Italy. Association for Computational Linguistics.
Merriam-Webster. What is a diacritic, anyway?
Simon Mille, Kaustubh D. Dhole, Saad Mahamood, Laura Perez-Beltrachini, Varun Gangal, Mihir Kale, Emiel van Miltenburg, and Sebastian Gehrmann. 2021. Automatic construction of evaluation suites for natural language generation datasets.
George A Miller. 1998. WordNet: An electronic lexical database. MIT press.
Shubhanshu Mishra, Sijun He, and Luca Belli. 2020. Assessing demographic bias in named entity recognition. CoRR, abs/2008.03415.
John Morris, Eli Liﬂand, Jin Yong Yoo, Jake Grigsby, Di Jin, and Yanjun Qi. 2020. Textattack: A framework for adversarial attacks, data augmentation, and adversarial training in nlp. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 119–126.
Marcin Namysl, Sven Behnke, and Joachim Köhler. 2020. NAT: Noise-aware training for robust neural sequence labeling. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1501–1517, Online. Association for Computational Linguistics.
Marcin Namysl, Sven Behnke, and Joachim Köhler. 2021. Empirical error modeling improves robustness of noisy neural sequence labeling. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 314–329, Online. Association for Computational Linguistics.
Wilhelmina Nekoto, Vukosi Marivate, Tshinondiwa Matsila, Timi Fasubaa, Taiwo Fagbohungbe, Solomon Oluwole Akinola, Shamsuddeen Muhammad, Salomon Kabongo Kabenamualu, Salomey Osei, Freshia Sackey, Rubungo Andre Niyongabo, Ricky Macharm, Perez Ogayo, Orevaoghene Ahia,

Musie Meressa Berhe, Mofetoluwa Adeyemi, Masabata Mokgesi-Selinga, Lawrence Okegbemi, Laura Martinus, Kolawole Tajudeen, Kevin Degila, Kelechi Ogueji, Kathleen Siminyu, Julia Kreutzer, Jason Webster, Jamiil Toure Ali, Jade Abbott, Iroro Orife, Ignatius Ezeani, Idris Abdulkadir Dangana, Herman Kamper, Hady Elsahar, Goodness Duru, Ghollah Kioko, Murhabazi Espoir, Elan van Biljon, Daniel Whitenack, Christopher Onyefuluchi, Chris Chinenye Emezue, Bonaventure F. P. Dossou, Blessing Sibanda, Blessing Bassey, Ayodele Olabiyi, Arshath Ramkilowan, Alp Öktem, Adewale Akinfaderin, and Abdallah Bashir. 2020. Participatory research for low-resourced machine translation: A case study in African languages. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 2144–2160, Online. Association for Computational Linguistics.
Toan Q Nguyen, Kenton Murray, and David Chiang. 2021. Data augmentation by concatenation for lowresource translation: A mystery and a solution. In Proceedings of the International Workshop on Spoken Language Translation, Online. Association for Computational Linguistics.
Vasile Florian Pais. 2019. Contributions to semantic processing of texts; Identiﬁcation of entities and relations between textual units; Case study on Romanian language. Ph.D. thesis.
Martha Palmer, Paul R. Kingsbury, and Daniel Gildea. 2005. The proposition bank: An annotated corpus of semantic roles. Comput. Linguistics, 31(1):71–106.
Soham Parikh, Ananya B. Sai, Preksha Nema, and Mitesh M. Khapra. 2019. Eliminet: A model for eliminating options for reading comprehension with multiple choice questions. CoRR, abs/1904.02651.
Kyubyong Park and Seanie Lee. 2020. g2pm: A neural grapheme-to-phoneme conversion package for mandarin chinese based on a new open benchmark dataset. CoRR, abs/2004.03136.
Charles Pierse. 2021. Transformers Interpret.
Aleksandra Piktus, Necati Bora Edizel, Piotr Bojanowski, Edouard Grave, Rui Ferreira, and Fabrizio Silvestri. 2019. Misspelling oblivious word embeddings. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 3226–3234, Minneapolis, Minnesota. Association for Computational Linguistics.
Emily Pitler, Mridhula Raghupathy, Hena Mehta, Ani Nenkova, Alan Lee, and Aravind Joshi. 2008. Easily identiﬁable discourse relations. In Coling 2008: Companion volume: Posters, pages 87–90, Manchester, UK. Coling 2008 Organizing Committee.
Girishkumar Ponkiya, Rudra Murthy, Pushpak Bhattacharyya, and Girish Palshikar. 2020. Looking inside noun compounds: Unsupervised prepositional

and free paraphrasing using language models. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings, pages 4313–4323.
Girishkumar Ponkiya, Kevin Patel, Pushpak Bhattacharyya, and Girish Palshikar. 2018. Treat us like the sequences we are: Prepositional paraphrasing of noun compounds using lstm. In Proceedings of the 27th International Conference on Computational Linguistics, pages 1827–1836.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Miltsakaki, Livio Robaldo, Aravind Joshi, and Bonnie Webber. 2008. The Penn Discourse TreeBank 2.0. In Proceedings of the Sixth International Conference on Language Resources and Evaluation (LREC’08), Marrakech, Morocco. European Language Resources Association (ELRA).
Yada Pruksachatkun, Satyapriya Krishna, Jwala Dhamala, Rahul Gupta, and Kai-Wei Chang. 2021. Does robustness improve fairness? approaching fairness with word substitution robustness methods for text classiﬁcation. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 3320–3331, Online. Association for Computational Linguistics.
Libo Qin, Minheng Ni, Yue Zhang, and Wanxiang Che. 2020. Cosda-ml: Multi-lingual code-switching data augmentation for zero-shot cross-lingual nlp. In Proceedings of the Twenty-Ninth International Joint Conference on Artiﬁcial Intelligence, IJCAI20, pages 3853–3860. International Joint Conferences on Artiﬁcial Intelligence Organization. Main track.
Alec Radford, Jeﬀ Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners.
Colin Raﬀel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2019. Exploring the limits of transfer learning with a uniﬁed text-to-text transformer.
Julio Raﬀo. 2021. WGND 2.0.
Vikas Raunak, Arul Menezes, and Marcin JunczysDowmunt. 2021. The curious case of hallucinations in neural machine translation. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1172–1183, Online. Association for Computational Linguistics.
Abhilasha Ravichander, Siddharth Dalmia, Maria Ryskina, Florian Metze, Eduard Hovy, and Alan W Black. 2021. NoiseQA: Challenge Set Evaluation for User-Centric Question Answering. In Conference of the European Chapter of the Association for Computational Linguistics (EACL), Online.

Mehdi Regina, Maxime Meyer, and Sébastien Goutal. 2020. Text data augmentation: Towards better detection of spear-phishing emails. CoRR, abs/2007.02033.
Marco Tulio Ribeiro, Tongshuang Wu, Carlos Guestrin, and Sameer Singh. 2020. Beyond accuracy: Behavioral testing of NLP models with CheckList. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4902– 4912, Online. Association for Computational Linguistics.
Haoyue Shi, Karen Livescu, and Kevin Gimpel. 2021. Substructure substitution: Structured data augmentation for NLP. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 3494–3508, Online. Association for Computational Linguistics.
Peng Shi and Jimmy Lin. 2019a. Simple bert models for relation extraction and semantic role labeling. arXiv preprint arXiv:1904.05255.
Peng Shi and Jimmy Lin. 2019b. Simple BERT models for relation extraction and semantic role labeling. CoRR, abs/1904.05255.
Ashish Shrivastava, Kaustubh Dhole, Abhinav Bhatt, and Sharvani Raghunath. 2021. Saying No is An Art: Contextualized Fallback Responses for Unanswerable Dialogue Queries. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers), pages 87–92, Online. Association for Computational Linguistics.
Vered Shwartz and Ido Dagan. 2018. Paraphrase to explicate: Revealing implicit noun-compound relations. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1200–1211.
Chenglei Si, Zhengyan Zhang, Fanchao Qi, Zhiyuan Liu, Yasheng Wang, Qun Liu, and Maosong Sun. 2021. Better robustness by more coverage: Adversarial and mixup data augmentation for robust ﬁnetuning. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 1569– 1576, Online. Association for Computational Linguistics.
R. Smith. 2007. An overview of the tesseract OCR engine. In 9th International Conference on Document Analysis and Recognition (ICDAR 2007), 2326 September, Curitiba, Paraná, Brazil, pages 629– 633. IEEE Computer Society.
Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,

pages 1631–1642, Seattle, Washington, USA. Association for Computational Linguistics.
Amane Sugiyama and Naoki Yoshinaga. 2019. Data augmentation using back-translation for contextaware neural machine translation. In Proceedings of the Fourth Workshop on Discourse in Machine Translation (DiscoMT 2019), pages 35–44, Hong Kong, China. Association for Computational Linguistics.
Tony Sun, Kellie Webster, Apurva Shah, William Yang Wang, and Melvin Johnson. 2021. They, them, theirs: Rewriting with gender-neutral english. CoRR, abs/2102.06788.
Fiona Anting Tan, Devamanyu Hazarika, See-Kiong Ng, Soujanya Poria, and Roger Zimmermann. 2021a. Causal augmentation for causal sentence classiﬁcation. In Proceedings of the First Workshop on Causal Inference and NLP, pages 1–20, Punta Cana, Dominican Republic. Association for Computational Linguistics.
Samson Tan and Shaﬁq Joty. 2021. Code-mixing on sesame street: Dawn of the adversarial polyglots. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 3596–3616, Online. Association for Computational Linguistics.
Samson Tan, Shaﬁq Joty, Kathy Baxter, Araz Taeihagh, Gregory A. Bennett, and Min-Yen Kan. 2021b. Reliability testing for natural language processing systems. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 4153–4169, Online. Association for Computational Linguistics.
Samson Tan, Shaﬁq Joty, Min-Yen Kan, and Richard Socher. 2020. It’s morphin’ time! Combating linguistic discrimination with inﬂectional perturbations. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2920–2935, Online. Association for Computational Linguistics.
Jörg Tiedemann and Santhosh Thottingal. 2020. OPUS-MT — Building open translation services for the World. In Proceedings of the 22nd Annual Conferenec of the European Association for Machine Translation (EAMT), Lisbon, Portugal.
Ashwin Vijayakumar, Michael Cogswell, Ramprasaath Selvaraju, Qing Sun, Stefan Lee, David Crandall, and Dhruv Batra. 2018. Diverse beam search for improved description of complex scenes.
Ashwin K. Vijayakumar, Michael Cogswell, Ramprasaath R. Selvaraju, Qing Sun, Stefan Lee, David J. Crandall, and Dhruv Batra. 2016. Diverse beam search: Decoding diverse solutions from neural sequence models. CoRR, abs/1610.02424.

Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. 2019a. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net.
Xiao Wang, Qin Liu, Tao Gui, Qi Zhang, Yicheng Zou, Xin Zhou, Jiacheng Ye, Yongxin Zhang, Rui Zheng, Zexiong Pang, Qinzhuo Wu, Zhengyan Li, Chong Zhang, Ruotian Ma, Zichu Fei, Ruijian Cai, Jun Zhao, Xingwu Hu, Zhiheng Yan, Yiding Tan, Yuan Hu, Qiyuan Bian, Zhihua Liu, Shan Qin, Bolin Zhu, Xiaoyu Xing, Jinlan Fu, Yue Zhang, Minlong Peng, Xiaoqing Zheng, Yaqian Zhou, Zhongyu Wei, Xipeng Qiu, and Xuanjing Huang. 2021a. TextFlint: Uniﬁed multilingual robustness evaluation toolkit for natural language processing. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations, pages 347–355, Online. Association for Computational Linguistics.
Yuxuan Wang, Wanxiang Che, Jiang Guo, Yijia Liu, and Ting Liu. 2019b. Cross-lingual bert transformation for zero-shot dependency parsing. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 5721–5727.
Yuxuan Wang, Wanxiang Che, Ivan Titov, Shay B. Cohen, Zhilin Lei, and Ting Liu. 2021b. A closer look into the robustness of neural dependency parsers using better adversarial examples. In Findings of the Association for Computational Linguistics: ACLIJCNLP 2021, pages 2344–2354, Online. Association for Computational Linguistics.
Jason W. Wei and Kai Zou. 2019. EDA: easy data augmentation techniques for boosting performance on text classiﬁcation tasks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019, pages 6381–6387. Association for Computational Linguistics.
John Wieting and Kevin Gimpel. 2017. Pushing the limits of paraphrastic sentence embeddings with millions of machine translations. In arXiv preprint arXiv:1711.05732.
John Wieting, Jonathan Mallinson, and Kevin Gimpel. 2017. Learning paraphrastic sentence embeddings from back-translated bitext. In Proceedings of Empirical Methods in Natural Language Processing.
Adina Williams, Nikita Nangia, and Samuel R Bowman. 2017. A broad-coverage challenge corpus for sentence understanding through inference. arXiv preprint arXiv:1704.05426.

Steven Wilson, Walid Magdy, Barbara McGillivray, Kiran Garimella, and Gareth Tyson. 2020. Urban dictionary embeddings for slang NLP applications. In Proceedings of the 12th Language Resources and Evaluation Conference, pages 4764–4773, Marseille, France. European Language Resources Association.
Sam Wiseman and Alexander M. Rush. 2016. Sequence-to-sequence learning as beam-search optimization. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1296–1306, Austin, Texas. Association for Computational Linguistics.
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. 2020. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 38–45, Online. Association for Computational Linguistics.
Tongshuang Wu, Marco Tulio Ribeiro, Jeﬀrey Heer, and Daniel S Weld. 2021. Polyjuice: Generating counterfactuals for explaining, evaluating, and improving models. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics.
Qizhe Xie, Zihang Dai, Eduard Hovy, Thang Luong, and Quoc Le. 2020. Unsupervised data augmentation for consistency training. Advances in Neural Information Processing Systems, 33.
Liang Xu, Qianqian Dong, Cong Yu, Yin Tian, Weitang Liu, Lu Li, and Xuanwei Zhang. 2020. Cluener2020: Fine-grained name entity recognition for chinese. arXiv preprint arXiv:2001.04351.
Usama Yaseen and Stefan Langer. 2021. Data augmentation for low-resource named entity recognition using backtranslation. CoRR, abs/2108.11703.
Sheng Kung Yi, Mark Steyvers, Michael Lee, and Matthew Dry. 2010. Wisdom of the crowds in minimum spanning tree problems. In Proceedings of the Annual Meeting of the Cognitive Science Society, volume 32.
Alex Yorke. butter-ﬁngers. https://github.com/ alexyorke/butter-fingers.
Yunfei. Chinese-Names-Corpus . https://github. com/wainshine/Chinese-Names-Corpus.
Jing Zhang, Bonggun Shin, Jinho D Choi, and Joyce C Ho. 2021. Smat: An attention-based deep learning solution to the automation of schema matching. In European Conference on Advances in Databases and Information Systems, pages 260–274. Springer.

Wei Emma Zhang, Quan Z. Sheng, and Ahoud Abdulrahmn F. Alhazmi. 2019a. Generating textual adversarial examples for deep learning models: A survey. CoRR, abs/1901.06796.
Yuan Zhang, Jason Baldridge, and Luheng He. 2019b. PAWS: paraphrase adversaries from word scrambling. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pages 1298–1308. Association for Computational Linguistics.
Zhe Zhao, Hui Chen, Jinbin Zhang, Xin Zhao, Tao Liu, Wei Lu, Xi Chen, Haotang Deng, Qi Ju, and Xiaoyong Du. 2019. Uer: An open-source toolkit for pretraining models. EMNLP-IJCNLP 2019, page 241.

A Review criteria for submission evaluation
Figure 3 shows the detailed review criteria used for evaluating the transformation and ﬁlters submissions.

B Transformations
The following is the list of all accepted transformations to NL-Augmenter project. Many of the transformations tokenize the sentences using SpaCy4 or NLTK (Bird, 2006) tokenizers. We discuss the implementations of each alongwith their limitations. The title of each transformation subsection is clickable and redirects to the actual python implementation. Many of the transformations use external libraries and we urge readers to look at each implementation and its corresponding ‘requirements.txt‘ ﬁles.

B.1 Abbreviation Transformation
This transformation replaces a word or phrase with its abbreviated counterpart “homework” -> “hwk” using a web-scraped slang dictionary.5
You ) yu driving at 80 miles per hour ) mph is why insurance is ) tis so freaking ) friggin expensive.

B.2 Add Hash-Tags

This transformation uses words in the text to generate hashtags. These hastags are then appeneded to the original text. Using the same words appearing

4https://spacy.io/ 5Scraped from dictionary

https://www.noslang.com/

Figure 3: Participants and reviewers were provided with a set of review criteria.

in the sentence to generate the hashtags acts as redundant noise that models should learn to ignore. Hashtags are widespread in social media channels and are used to draw attention to the source text and also as a quick stylistic device.
I love domino’s pizza. ) #LovePizza #Love #I #Pizza

B.3 Adjectives Antonyms Switch
This transformation switches English adjectives in a sentence with their WordNet (Miller, 1998) antonyms to generate new sentences with possibly diﬀerent meanings and can be useful for tasks like Paraphrase Detection, Paraphrase Generation, Semantic Similarity, and Recognizing Textual En-

tailment. Amanda’s mother was very
beautiful ) ugly .
B.4 AmericanizeBritishizeEnglish
This transformation takes a sentence and tries to convert it from British English to American English and vice-versa. A select set of words have been taken from hyperreality@GitHub.
I love the pastel colours ) colors
B.5 AntonymsSubstitute
This transformation introduces semantic diversity by replacing an even number of adjective/adverb antonyms in a given text. We assume that an even number of antonyms transforms will revert back sentence semantics; however, an odd number of transforms will revert the semantics. Thus, our transform only applies to the sentence that has an even number of revertible adjectives or adverbs. We called this mechanism double negation.
Steve is able ) unable to recommend movies that depicts the lives of beautiful ) ugly minds.
B.6 Auxiliary Negation Removal
This is a low-coverage transformation which targets sentences that contain negations. It removes negations in English auxillaries and attempts to generate new sentences with the oppposite meaning.
Ujjal Dev Dosanjh was not ) Ujjal Dev Dosanjh was the 1st Premier of British Columbia from 1871 to 1872.
B.7 AzertyQwertyCharsSwap
Preferably use the above download link, as the release tarballs are generated deterministically ) qre generqted deterministicqlly whereas GitHub’s are not.
B.8 BackTranslation
This transformation translates a given English sentence into German and back to English.This transformation acts like a light paraphraser. Multiple variations can be easily created via changing parameters like the language as well as the translation models which are available in plenty. Backtranslation has been quite popular now and has been a quick way to augment examples (Li and Specia, 2019; Sugiyama and Yoshinaga, 2019).

Andrew finally returned ) eventually gave Chris the French book the French book I bought last week.
B.9 BackTranslation for Named Entity Recognition
This transformation splits the token sequences into segments of entity mention(s) and “contexts” around the entity mention(s). Backtranslation is used to paraphrase the contexts around the entity mention(s), thus resulting in a diﬀerent surface form from the original token sequence. The resultant tokens are also assigned new tags. Exploiting this transformation has shown to empirically beneﬁt named entity tagging (Yaseen and Langer, 2021) and hence could arguably beneﬁt other lowresource tagging tasks (Bhatt and Dhole, 2020; Khachatrian et al., 2019; Gupta et al., 2021).
B.10 Butter Fingers Perturbation
This perturbation adds noise to all types of text sources (sentence, paragraph, etc.) proportional to noise erupting from keyboard typos making common spelling errors. Few letters picked at random are replaced with letters which are at keyboard positions near the source letter. The implementation has been borrowed from here (Yorke) as used in (Mille et al., 2021). There has also been some recent work in NoiseQA (Ravichander et al., 2021) to mimick keyboard typos.
Sentences ) Senhences with gapping, such as Paul likes coffee ) coffwe and Mary tea, lack an overt predicate to indicate ) indicatx the relation ) relauion between two or more arguments ) argumentd .
B.11 Butter Fingers Perturbation For Indian Languages
This implements the butter ﬁngers perturbation as used above for 7 Indian languages: Bangla, Gujarati, Hindi, Kannada, Malayalam, Oriya, Punjabi, Tamil, and Telugu. The implementation considers the InScript keyboard 6 which is decreed as a standard for Indian scripts.
B.12 Change Character Case
This transformation acts like a perturbation and randomly swaps the casing of some of the letters.
6https://en.wikipedia.org/wiki/InScript_ keyboard

The transformation’s outputs will not work with uncased models or languages without casing.
Alice in Wonderland is a 2010 American live- action ) actIon / animated ) anImated dark fantasy ) faNtasy adventure film.

B.13 Change Date Format
This transformation changes the format of dates. The first known case of COVID-19
was identified in Wuhan, China in December ) Dec 2019.

B.14 Change Person Named Entities
This perturbation changes the name of the person from one name to another by making use of the lexicon of person names in Ribeiro et al. (2020).
Andrew ) Nathaniel finally returned the French book to Chris that I bought last week

B.15 Change Two Way Named Entities
This perturbation also changes the name of the person but also makes a parallel change in the label or reference text with the same name making it useful for text-to-text generation tasks.
He finally returned the French book to Chris ) Austin that I bought last week

B.16 Chinese Antonym and Synonym Substitution
This transformation substitutes Chinese words with their synonyms or antonyms by using the Chinese dictionary7 and NLP Chinese Data Augmentation dictionary.8

B.17 Chinese Pinyin Butter Fingers Perturbation

This transformation implements the Butter Fingers Perturbation for Chinese characters. Few Chinese words and characters that are picked at random will be substituted with others that have similar pinyin (based on the default Pinyin keyboards in Windows and Mac OS). It uses a database of 16142 Chinese characters 9 and its associated pinyins to generate the perturbations for Chinese characters. A smaller

7Chinese Dictionary:

https://github.com/

guotong1988/chinese_dictionary

8NLP Chinese Data Augmentation: https://github.

com/425776024/nlpcda

9https://github.com/pwxcoo/chinese-xinhua

database of 3500 10 more frequently seen Chinese characters are also used in the perturbations with a higher probability of being used compared to less frequently seen Chinese characters. It also uses a database of 575173 words 11 that are combined from several sources 12 in order to generate perturbations for Chinese words.
B.18 Chinese Person Named Entities and Gender Perturbation
This perturbation adds noise to all types of text sources containing Chinese names (sentence, paragraph, etc.) by swapping a Chinese name with another Chinese name whilst also allowing the possibility of gender swap. CLUENER (Xu et al., 2020; Zhao et al., 2019) is used for tagging named entities in Chinese. The list of names is taken from the Chinese Names Corpus! (Yunfei). It can provide assistance in detecting biases present in language models and the ability to infer implicit gender information when presented with gender-speciﬁc names. This can also be useful in mitigating representation biases in the input text.
B.19 Chinese (Simpliﬁed & Traditional) Perturbation
This perturbation adds noise to all types of text sources containing Chinese words and characters (sentence, paragraph, etc.) by changing the words and characters between Simpliﬁed and Traditional Chinese as well as other variants of Chinese Characters such as Japanese Kanji, character-level and phrase-level conversion, character variant conversion and regional idioms among Mainland China, Taiwan and Hong Kong, all available as conﬁgurations originally in the OpenChineseConvert project .13
B.20 City Names Transformation
This transformation replaces instances of populous and well-known cities in Spanish and English sentences with instances of less populous and less well-known cities to help reveal demographic biases (Mishra et al., 2020) prevelant in named entity recognition models. The choice of cities have been taken from the World Cities Dataset .14
10https://github.com/elephantnose/characters 11http://thuocl.thunlp.org/ 12https://github.com/fighting41love/Chinese_ from_dongxiexidian 13https://github.com/BYVoid/OpenCC 14https://www.kaggle.com/juanmah/ world-cities

The team was established in Dallas ) Viera West in 1898 and was a charter member of the NFL in 1920.
B.21 Close Homophones Swap
Humans are generally guided by their senses and are unconsciously robust against phonetic attacks. Such types of attacks are highly popular in languages like English which has an irregular mapping between pronunciation and spelling (Eger and Benz, 2020). This transformation mimics writing behaviors where users swap words with similar homophones either intentionally or by accident. This transformation acts like a perturbation to test robustness. Few words picked at random are replaced with words with similar homophones which sound similar or look similar. Some of the word choices might not be completely natural to normal human behavior, since humans "prefer" some words over others even they sound exactly the same. So it might not be fully reﬂecting the natural distribution of intentional or unintentional swapping of words.
Sentences with gapping, such as Paul likes coffee and Mary tea ) Tee , lack an overt predicate to indicate the ) Thee relation between two or more ) Morr arguments.
B.22 Color Transformation
This transformation augments the input sentence by randomly replacing mentioned colors with diﬀerent ones from the 147 extended color keywords speciﬁed by the World Wide Web Consortium (W3C) .15 Some of the colors include “dark sea green”, “misty rose”, “burly wood”.
Tom bought 3 apples, 1 orange ) misty rose , and 4 bananas and paid $10.
B.23 Concatenate Two Random Sentences (Bilingual)
Given a dataset, this transformation concatenates a sentence with a previously occuring sentence as explained in (Nguyen et al., 2021). A monolingual version is mentioned in the subsequent subsection below. This concatenation would beneﬁt all text tasks that use a transformer (and likely other sequence-to-sequence architectures). Previously published work (Nguyen et al., 2021) has shown
15https://www.w3.org/TR/2021/ REC-css-color-3-20210805/

a large gain in performance of low-resource machine translation using this method. In particular, the learned model is stronger due to being able to see training data that has context diversity, length diversity, and (to a lesser extent) position shifting.
B.24 Concatenate Two Random Sentences (Monolingual)
This is the monolingual counterpart of the above. I am just generating a very very
very long sentence to make sure that the method is able to handle it. It does not even need to be a sentence. Right? This is not splitting on punctuation... I am just generating a very very very long sentence to make sure that the method is able to handle it. It does not even need to be a sentence. Right? This is not splitting on punctuation...
B.25 Concept2Sentence
This transformation intakes a sentence, its associated integer label, and (optionally) a dataset name that is supported by huggingface/datasets (Lhoest et al., 2021a,b). It works by extracting keyword concepts from the original sentence, passing them into a BART (Lewis et al., 2020) transformer trained on CommonGen (Lin et al., 2019) to generate a new, related sentence which reﬂects the extracted concepts. Providing a dataset allows the function to use transformers-interpret (Pierse, 2021) to identify the most critical concepts for use in the generative step. Underneath the hood, this transform makes use of the Sibyl tool (HarelCanada, 2021), which is capable of also transforming the label as well. However, this particular implementation of C2S generates new text that is invariant (INV) with respect to the label. Since the model is trained on CommonGen, which is focussed on image captioning, the style of the output sentence would be geared towards scenic descritions and might not necessarily adhere to the syntax of the original sentence. Besides, it can be hard to argue that a handful subset of keywords could provide a complete description of the original sentence.
B.26 Contextual Meaning Perturbation
This transformation was designed to model the "Chinese Whispers" or "Telephone" children’s game: The transformed sentence appears ﬂuent

and somewhat logical, but the meaning of the original sentence might not be preserved. To achieve logical coherence, a pre-trained language model is used to replace words with alternatives that match the context of the sentence. Grammar mistakes are reduced by limiting the type of words considered for changes (based on POS tagging) and replacing adjectives with adjectives, nouns with nouns, etc. where possible.
This transformation beneﬁts users who seek perturbations that preserve ﬂuency but not the meaning of the sentence. For instance, it can be used in scenarios where the meaning is relevant to the task, but the model shows a tendency to over-rely on simpler features such as the grammatical correctness and general coherence of the sentence. A real-world example would be the training of quality estimation models for machine translation (does the translation maintain the meaning of the source?) or for text summarisation (does the summary capture the content of the source?).
Word substitution with pre-trained language models has been explored in diﬀerent settings. For example, the augmentation library nlpaug (Ma, 2019) and the adversarial attack library TextAttack (Morris et al., 2020) include contextual perturbation methods. However, their implementations do not oﬀer control over the type of words that should be perturbed and introduce a large number of grammar mistakes. If the aim is to change the sentence’s meaning while preserving its ﬂuency, this transformation can help to get the same eﬀect with signiﬁcantly fewer grammatical errors. Li et al. (2020a) propose an alternative approach to achieve a similar objective.
B.27 Contractions and Expansions Perturbation
This perturbation substitutes the text with popular expansions and contractions, e.g., “I’m” is changed to “I am”and vice versa. The list of commonly used contractions & expansions and the implementation of perturbation has been taken from Checklist (Ribeiro et al., 2020).
He often does n’t ) not come to school.
B.28 Correct Common Misspellings
This transformation acts like a lightweight spellchecker and corrects common misspellings appearing in text by looking for words in Wikipedia’s Lists of Common Misspellings.

Andrew andd ) and Alice finally returnd ) returned the French book that I bought lastr ) last week
B.29 Country/State Abbreviation
This transformation replaces country and state names with their common abbreviations.16 Abbreviations can be common across diﬀerent locations:
“MH” can refer to Country Meath in Ireland as well as the state of Maharashtra in India and hence this transformation might result in a slight loss of information, especially if the surrounding context doesn’t have enough signals.
One health officer and one epidemiologist have boarded the ship in San Diego, CA ) California on April 13, 2015 to conduct an environmental health assessment.
B.30 Decontextualisation of the main Event
Semantic Role Labelling (SRL) is a powerful shallow semantic representation to determine who did what to whom, when, and where (and why and how etc). The core arguments generally talk about the participants involved in the event. Addtionally, contextual arguments on the other hand provide more speciﬁc information about the event. After tagging a sentence with an appropriate semantic role labels using an SRL labeller (Jindal et al., 2020; Shi and Lin, 2019a). This transformation crops out contextual arguments to create a new sentence with a minimal description of the event. Helping to generate textual pairs for entailment.
B.31 Diacritic Removal
“Diacritics are marks placed above or below (or sometimes next to) a letter in a word to indicate a particular pronunciation — in regard to accent, tone, or stress — as well as meaning, especially when a homograph exists without the marked letter or letters.” Merriam-Webster. This transformation removes these diacritics or accented characters, and replaces them with their non-accented versions. It can be common for non-native or inexperienced speakers to miss out on any accents and specify non-accented versions.
She lookèd ) looked east an she lookèd ) looked west.
16Countries States Cities Database: https://github. com/dr5hn/countries-states-cities-database

B.32 Disability/Diﬀerently Abled Transformation
Disrespectful language can make people feel excluded and represent an obstacle towards their full participation in the society (Res, 2006). This lowcoverage transformation substitutes outdated references to references of disabilities with more appropriate and respectful ones which avoid negative connotations. A small list of inclusive words and phrases have been taken from a public article on inclusive communication, Wikipedia’s list of disability-related terms with negative connotations, terms to avoid while writing about disability.
They are deaf ) person or people with a hearing disability.
B.33 Discourse Marker Substitution
This perturbation replaces a discourse marker in a sentence by a semantically equivalent marker. Previous work has identiﬁed discourse markers that have low ambiguity (Pitler et al., 2008). This transformation uses the corpus analysis on PDTB 2.0 (Prasad et al., 2008) to identify discourse markers that are associated with a discourse relation with a chance of at least 0.5. Then, a marker is replaced with a diﬀerent marker that is associated to the same semantic class.
It has plunged 13% since ) inasmuch as July to around 26 cents a pound. A year ago ethylene sold for 33 cents
B.34 Diverse Paraphrase Generation Using SubModular Optimization and Diverse Beam Search
This transformation generates multiple paraphrases of a sentence by employing 4 candidate selection methods on top of a base set of backtranslation models. 1) DiPS (Kumar et al., 2019) 2) Diverse Beam Search (Vijayakumar et al., 2018) 3) Beam Search (Wiseman and Rush, 2016) 4) Random. Unlike beam search which generally focusses on the top-k candidates, DiPS introduces a novel formulation of using submodular optimisation to focus on generating more diverse paraphrases and has been proven to be an eﬀective data augmenter for tasks like intent recognition and paraphrase detection (Kumar et al., 2019). Diverse Beam Search attempts to generate diverse sequences by employing a diversity promoting alternative to the classical

beam search (Wiseman and Rush, 2016).

B.35 Dislexia Words Swap
This transformation acts like a perturbation by altering some words of the sentences with abberations (Board, 2021) that are likely to happen in the context of dyslexia.
Biden hails your ) you’re relationship with Australia just days after new partnership drew ire from France.

B.36 Emoji Icon Transformation
This transformation converts emojis into their equivalent keyboard format (e.g., -> ":)" ) and vice versa (e.g., ":)" -> ).

B.37 Emojify

This transformation augments the input sentence by swapping words with emojis of similar meanings. Emojis, introduced in 1997 as a set of pictograms used in digital messaging, have become deeply integrated into our daily communication. More than 10% of tweets17 and more than 35% of Instagram posts18 include one or more emojis in 2015. Given the ubiquitousness of emojis, there is a growing body of work researching the linguistic and cultural aspects of emojis (Guntuku et al., 2019) and how we can leverage the use of emojis to help solve NLP tasks (Eisner et al., 2016).
Apple is looking at buying U.K.

startup for $132 billion. )

is

at

startup for $

.

B.38 English Inﬂectional Variation
This transformation adds inﬂectional variation to English words and can be used to test the robustness of models against inﬂectional variations. In English, each inﬂection generally maps to a PartOf-Speech tag 19 in the Penn Treebank (Marcus et al., 1993). For each content word in the sentence, it is ﬁrst lemmatised before randomly sampling a valid POS category and reinﬂecting the word according to the new category. The sampling process for each word is constrained using its POS tag to maintain the original sense for polysemous words.
17https://blog.twitter.com/en_us/a/2015/ emoji-usage-in-tv-conversation
18https://instagram-engineering.com/ 19Penn TreeBank POS

This has been adapted from the Morpheus (Tan et al., 2020) adversarial attack.
Ujjal Dev Dosanjh served ) serve as 33rd Premier ) Premiers of British Columbia from 2000 to 2001
B.39 English Mention Replacement for NER
This transformation randomly swaps an entity mention with another entity mention of the same entity type. Exploiting this transformation as a data augmentation strategy has been empirically shown to improve the performance of underlying (NER) models (Dai and Adel, 2020).
B.40 Filler Word Augmentation
This augmentation adds noise in the form of colloquial ﬁller phrases. 23 diﬀerent phrases are chosen across 3 diﬀerent categories: general ﬁller words and phrases ("uhm", "err", "actually", "like", "you know"...), phrases emphasizing speaker opinion/mental state ("I think/believe/mean", "I would say"...) & phrases indicating uncertainty ("maybe", "perhaps", "probably", "possibly", "most likely").The latter two categories had shown promising results Kovatchev et al. (2021) when they were concatenated at the beginning of the sentence unlike this implementation which perform insertions at any random positions. Filler words are based on the work of Laserna et al. (2014) but have not been explored in the context of data augmentation.
B.41 Style Transfer from Informal to Formal
This transformation transfers the style of text from formal to informal and vice versa. It uses the implementation of Styleformer (Damodaran).
What you upto ) currently doing ?
B.42 French Conjugation Substitution
This transformation change the conjugation of verbs for simple french sentences with a speciﬁed tense. It detects the pronouns used in the sentence in order to conjugate accordingly whenever a sentence contains diﬀerents verbs. This version only works for indicative tenses. It also only works for simple direct sentences (subject, verb, COD/COI), which contains a pronoun as subject (il, elle, je etc.). It does not detect when the subject is a couple of nouns ("les enfants" or "la jeune femme").

B.43 Gender And Culture Diversity Name Changer (1-way and 2-way)
Corpora exhibits many representational biases and this transformation focuses on one particular mediator, the personal names. It diversiﬁes names in the corpora along two critical dimensions, gender and cultural background. Technically, the transformation samples a (country, gender) pair and then randomly draws a name from that (country, gender) pair to replace the original name. We collected 42812 distinct names from 141 countries.They are primarily from the World Gender Name Dictionary (Raﬀo, 2021).
Common name augmentations do not consider their gender and cultural implication. Thus, they do not necessarily mitigate biases or promote the minority’s representation because the augmented name may be from the same gender and cultural background. This is the case, for example in the CheckList’s (Ribeiro et al., 2020) implemented name augmentation. Taking the interaction of the names therein with ours, 34.0%, 33.5%, 31.9%, 30.8% of them are popular names in US, Canada, Australia, and UK, respectively. Only 0.4%, 0.4%, 0.5%, 2.1% of them are from India, Korea, China, and Kazakhstan.
Rachel ) Charity Green, a sheltered but friendly woman, flees her wedding day and wealthy yet unfulfilling life.
B.44 Neopronoun Substitution
This transformation performs grammatically correct substitution from English to English of the gendered pronouns, he/she, in a given sentence with their neopronoun counterparts, based on a list compiled by UNC Greensboro and LGBTA WIKI.20 NLP models, such as those for neural machine translation, often fail to recognize the neopronouns and treat them as proper nouns. This transformation seeks to render the training data used in NLP pipelines more neopronoun aware to reduce the risk of trans-erasure. The reason why a simple look-uptable approach might not work is due to the fact that the case may diﬀer depending on the context.
She ) They had her ) their friends tell her ) them about the event.
20https://intercultural. uncg.edu/wp-content/uploads/ Neopronouns-Explained-UNCG-Intercultural-Engagement. pdf

B.45 Gender Neutral Rewrite
This transformation involves rewriting an English sentence containing a single gendered entity with its gender-neutral variant. One application is machine translation, when translating from a language with gender-neutral pronouns (e.g. Turkish) to a language with gendered pronouns (e.g. English). This transformation is based on the algorithm proposed by Sun et al. (2021).
His ) Their dream is to be a fireman ) firefighter when he ) they grows ) grow up.
B.46 GenderSwapper
This transformation introduces gender diversity to the given data. If used as data augmentation for training, the transformation might mitigate gender bias, as shown in Dinan et al. (2020). It also might be used to create a gender-balanced evaluation dataset to expose the gender bias of pre-trained models. This transformation performs lexical substitution of the opposite gender. The list of gender pairs (shepherd <–> shepherdess) is taken from Lu et al. (2019). Genderwise names used from Ribeiro et al. (2020) are also randomly swapped.
B.47 GeoNames Transformation
This transformation augments the input sentence with information based on location entities (specifically cities and countries) available in the GeoNames database.21 E.g., if a country name is found, the name of the country is appended with information about the country like its capital city, its neighbouring countries, its continent, etc. Some initial ideas of this nature were explored in Pais (2019).
B.48 German Gender Swap
This transformation replaces the masculine nouns and pronouns with their female counterparts for German sentences from a total of 2226 common German names.22
Er ) Sie ist ein Arzt ) eine Ärztin und mein Vater ) meine Mutter .
B.49 Grapheme to Phoneme Substitution
This transformation adds noise to a sentence by randomly converting words to their phonemes.
21http://download.geonames.org/export/dump/ 22https://de.wiktionary.org/wiki/Verzeichnis: Deutsch/Namen

Grapheme-to-phoneme substitution is useful in NLP systems operating on speech. An example of grapheme to phoneme substitution is “permit” → P ER0 M IH1 T’.
B.50 Greetings and Farewells
This transformation replaces greetings (e.g. "Hi", "Howdy") and farewells (e.g. "See you", "Good night") with their synonymous equivalents.
Hey ) Hi everyone. It’s nice ) Pleased to meet you. How have ) are you been ?
B.51 Hashtagify
This transformation modiﬁes an input sentence by identifying named entities and other common words and turning them into hashtags, as often used in social media.
B.52 Insert English and French Abbreviations
This perturbation replaces in texts some well known English and French words or expressions with (one of) their abbreviations. Many of the abbreviations covered here are quite common on social medias platforms, even though some of them are quite generic. This implementation is partly inspired by recent work in Machine Translation (Berard et al., 2019).
B.53 Leet Transformation
Visual perturbations are often used to disguise offensive comments on social media (e.g., “!d10t”) or as a distinct writing style (“1337” in “leet speak”) (Eger et al., 2019a), especially common in scenarios like video gaming. Humans are unconsciously robust to such visually similar texts. This perturbation replaces letters with their visually similar “leet” counterparts.23
Ujjal Dev Dosanjh served ) U7jal 0ev D0san74 serv3d as 33rd Premier of British Columbia from ) Pr33i3r 0f 8ritis4 00lu36ia fr0m 2000 to ) t0 2001
B.54 Lexical Counterfactual Generator
This transformation generates counterfactuals by simply substituting negative words like “not”, “neither” in one sentence of a semantically similar sentence pair. The substituted sentence is then backtranslated in an attempt to correct for grammatical-
23https://simple.wikipedia.org/wiki/Leet

ity. This transformation would be useful for tasks like entailment and paraphrase detection.
B.55 Longer Location for NER
This transformation augments data for Named Entity Recognition (NER) tasks by augmenting examples which have a Location Tag. Names of locations are expanded by appending them with cardinal directions like “south”, “N”, “northwest”, etc. The transformation ensures that the tags of the new sentence are accordingly modiﬁed.
B.56 Longer Location Names for testing NER
This transformation augments data for Named Entity Recognition (NER) tasks by augmenting examples that have a Location (LOC) Tag. Names of location are expanded by inserting random preﬁx or postﬁx word(s). The transformation also ensures that the labels of the new tags are accordingly modiﬁed.
B.57 Longer Names for NER
This transformation augments data for Named Entity Recognition (NER) tasks by augmenting examples which have a Person Tag. Names of people are expanded by inserting random characters as initials. The transformation also ensures that the labels of the new tags are accordingly modiﬁed.
B.58 Lost in Translation
This transformation is a generalization of the BackTranslation transformation to any sequence of languages supported by the Helsinki-NLP OpusMT models (Tiedemann and Thottingal, 2020).
Andrew finally returned ) brought Chris back the French book the French book I bought last week I bought last week
B.59 Mixed Language Perturbation
Mixed language training has been eﬀective for cross-lingual tasks (Liu et al., 2020), to help generate data for low-resource scenarios (Liu et al., 2021) and for multilingual translation (Fan et al., 2021). Two transformations translate randomly picked words in the text from English to other languages (e.g., German). It can be used to test the robustness of a model in a multilingual setting.
Andrew finally returned the ) die Comic book to Chris that I bought last week ) woche

B.60 Mix transliteration
This transformation transliterates randomly picked words from the input sentence (of given source languae script) to a target language script. It can be used to train/test multilingual models to improve/evaluate their ability to understand complete or partially transliterated text.
B.61 MR Value Replacement
This perturbation adds noise to a key-value meaning representation (MR) (and its corresponding sentence) by randomly substituting values/words with their synonyms (or related words). This transformation uses a simple strategy to align values of a MR and tokens in the corresponding sentence inspired by how synonyms are substituted for tasks like machine translation (Fadaee et al., 2017). This way, there could be some problems in complex sentences. Besides, the transformation might introduce non-grammatical segments.
B.62 Multilingual Back Translation
This transformation translates a given sentence from a given language into a pivot language and then back to the original language. This transformation is a simple paraphraser that works on 100 different languages. Back Translation has been quite popular now and has been a quick way to augment (Li and Specia, 2019; Sugiyama and Yoshinaga, 2019; Fan et al., 2020).
Being honest ) Honesty should be one of our most important character traits ) characteristics
B.63 Multilingual Dictionary Based Code Switch
This transformation generates multi-lingual codeswitching data to ﬁne-tune encoders of large language models (Qin et al., 2020; Tan and Joty, 2021; Wang et al., 2019b) by making use of bilingual dictionaries of MUSE (Lample et al., 2018).
B.64 Multilingual Lexicon Perturbation
This perturbation helps to creates code-mixed sentences for both high-resource and low-resource languages by randomly translating words with a speciﬁed probability from any supported languages (e.g., English) to other supported languages (e.g., Chinese) by using a multilingual lexicon. Thus, it can be used to generate code-mixed training data to improve models for multilingual and cross-lingual settings. As of now 100 languages are supported and

3000 common English words listed on ef.com 24 are supported. The lexicon implementation is also 160x faster than its model based counterpart.
B.65 Causal Negation & Strengthening
This transformation is targeted at augmenting Causal Relations in text and adapts the code from the paper Causal Augmentation for Causal Sentence Classiﬁcation (Tan et al., 2021a). There are two operations: 1. Causal Negation: Negative words like "not, no, did not" are introduced into sentences to unlink the causal relation. 2. Causal Strengthening: Causal meaning is strengthened by converting weaker modal words into stronger ones like "may" to "will" to assert causal strength.
The implementation provides users with the option to amend causal meaning automatically from the root word of the sentence, or by explicitly highlighting the index of the word they wish to amend. Additionally, we include WordNet (Miller, 1998) synonyms and tense matching to allow for more natural augmentations.
The rs7044343 polymorphism could be ) was involved in regulating the production of IL-33.
B.66 Question Rephrasing transformation
This implementation rephrases questions for sentence tasks by using the T5 model used in B.75 for Question Answering tasks.
B.67 English Noun Compound Paraphraser [N+N]
This transformation replaces two-word noun compounds with a paraphrase, based on the compound paraphrase dataset from SemEval 2013 Task 4 (Hendrickx et al., 2013). It currently only works for English. Any two-word compound that appears in a dataset of noun compound paraphrases will be replaced by a paraphrase. If more than one twoword compound appears, then all combinations of compound paraphrases (including no paraphrase at all) will be returned. For example, the paraphrases of “club house” include “house for club activities”, “house for club members”, “house in which a club meets”, etc. We start with replacing paraphrases with the highest score (the speciﬁed frequency in the annotated dataset), and paraphrases with the same score (ties) are sorted randomly. This transformation currently only checks for noun com-
24https://www.ef.com/wwen/english-resources/ english-vocabulary/top-3000-words/

pounds from Hendrickx et al. (2013) and therefore has low coverage. To improve it, other datasets could be added, e.g., from Ponkiya et al. (2018) or Lauer (1995). To attain even wider-coverage (at the expense of lower precision), machine learning approaches such as Shwartz and Dagan (2018) or Ponkiya et al. (2020) could be considered. In addition, some of the the paraphrases in Hendrickx et al. (2013) sound a little odd (e.g., "blood cell" -> "cell of blood") and may not ﬁt well in context.
B.68 Number to Word
This transformation acts like a perturbation to improve robustness on processing numerical values. The perturbated sentence contains the same information as the initial sentence but with a diﬀerent representation of numbers.
B.69 Numeric to Word
This transformation translates numbers in numeric form to their textual representations. This includes general numbers, long numbers, basic math characters, currency, date, time, phone numbers, etc.
B.70 OCR Perturbation
This transformation directly induces Optical Character Recognition (OCR) errors into the input text. It renders the input sentence as an image and recognizes the rendered text using the OCR engine Tesseract 4 (Smith, 2007). It works with text in English, French, Spanish, and German. The implementation follows previous work by Namysl et al. (2021).
B.71 Add Noun Deﬁnition
This transformation appends noun deﬁnitions onto the original nouns in a sentence. Deﬁnitions of nouns are collected from Wikidata .25
B.72 Pig Latin Cipher
This transformation translates the original text into pig latin. Pig Latin is a well-known deterministic transformation of English words, and can be viewed as a cipher which can be deciphered by a human with relative ease. The resulting sentences are completely unlike examples typically used in language model training. As such, this augmentation change the input into inputs which are diﬃcult for a language model to interpret, while being relatively easy for a human to interpret.
25https://www.wikidata.org/wiki/Wikidata: Main_Page

B.73 Pinyin Chinese Character Transcription
This transformation transcribes Chinese characters into their Mandarin pronunciation using the Pinyin romanization scheme. The Character-to-Pinyin converter at the core of this transformation is a neural model by Park and Lee (2020).
B.74 SRL Argument Exchange
This perturbation adds noise to all types of English text sources (sentence, paragraph, etc.) proportional to the number of arguments identiﬁed by SRL BERT (Shi and Lin, 2019b). Diﬀerent rules are applied to deterministically modify the sentence in a meaning-preserving manner. Rules look as follows: if ARGM-LOC and ARGM-TMP both present, exchange them. Example: [ARG0: Alex] [V: left] [ARG2: for Delhi] [ARGM-COM: with his wife] [ARGM-TMP: at 5 pm] . → Alex left for Delhi at 5 pm with his wife. The transformation relies on propbank annotations (Bonial et al., 2012; Kingsbury and Palmer, 2002; Palmer et al., 2005; Gildea and Palmer, 2002).
B.75 ProtAugment Diverse Paraphrasing
This transformation utilizes the ProtAugment method by Dopierre et al. (2021). The paraphrase generation model is a BART model (Lewis et al., 2020), ﬁnetuned on the paraphrase generation task using 3 datasets: Google-PAWS (Zhang et al., 2019b), MSR (Dolan and Brockett, 2005), Quora.26
When parpahrasing a sentence, the transformation useu Diverse Beam Search (Vijayakumar et al., 2016) to generate diverse outputs. The diversity penalty term is by default set to 0.5 but can be set to custom values. Additionally, the transformation can use the following generation constraints: (1) A fraction of the words in the input sentence are forbidden in the paraphrase (default 0.7). (2) All bi-grams in the input sentence are forbidden in the paraphrase. This means the paraphrase cannot contain any bi-gram that are in the input sentence. This constraint enforces the paraphrase generation model to change the sentence structure.
B.76 Punctuation
This transformation removes/adds punctuation from an English sentence. This transformation was
26https://quoradata.quora.com/ First-Quora-Dataset-Release-Question-Pairs

ﬁrst introduced by Mille et al. (2021) and used as an example implemention for NL-Augmenter.
B.77 Question-Question Paraphraser for QA
This transformation creates new QA pairs by generating question paraphrases from a T5 model ﬁnetuned on Quora Question pairs .27 Generated questions can have a very diﬀerent surface form from the original question making it a strong paraphrase generator. A T5 model (Raﬀel et al., 2019; Wolf et al., 2020) ﬁne tuned 28 on the Quora Question Pairs dataset was being used to generate question paraphrases. This transformation would beneﬁt Question Answering, Question Generation as well as other tasks which could indirectly beneﬁt eg. for dialog tasks (Shrivastava et al., 2021; Dhole, 2020).
B.78 Question in CAPS
This transformation upper-cases the context of a question answering example. It also adds uppercased versions of the original answers to the set of acceptable model responses.
B.79 Random Word Deletion
This transformation randomly removes a word with a given probability p (by default 0.25). The transformation relies on whitespace tokenization and thus only works for English and other languages that are segmented via whitespace. Due to the destructive nature of the transformation, it is likely that the meaning of a sequence may be changed as a result of the change. A similar transformation was suggested by Wei and Zou (2019). Word dropout (Goldberg, 2017) has been common to help models understand unknown words encountered during evaluation by exposing them to this unknown-word condition during training itself.
B.80 Random Upper-Case Transformation
This perturbation adds noise to all types of text sources (sentence, paragraph, etc.) by randomly adding upper cased letters. With a default probably of 0.1, each character in a sequence is upper-cased. This transformation does not rely on a tokenizer and thus works with all languages that have upper and lower-case letters. One limiation of this transformation is that it will not aﬀect a tokenizer that does lower case for all input. A similar transformation was suggested by Wei and Zou (2019). Further
27Quora Question Pairs 28https://huggingface.co/ramsrigouthamg/t5_ paraphraser

improvement of this transformation exists by potentially relying on extreme value theory (Jalalzai et al., 2020).
B.81 Double Context QA
This transformation repeats the context of a question answering example. This should not change the result in any way.
B.82 Replace Abbreviations and Acronyms
This transformation changes abbreviations and acronyms appearing in an English text to their expanded form and respectively, changes expanded abbreviations and acronyms appearing in a text to their shorter form. For example, “send this ﬁle asap to human resources” might be changed to “send this ﬁle as soon as possible to HR”. The list of abbreviations and acronyms used in this transformation where manually gathered focusing on common abbreviations present in business communications. When abbreviation are context-dependent or highly speciﬁc, the induced change may change the meaning of a text, or an abbreviation may not be available in the lookup. The transformation was ﬁrst introduced by Regina et al. (2020).
B.83 Replace Financial Amounts
This transformation replaces ﬁnancial amounts throughout a text with the same value in a diﬀerent currency. The replacement changes the amount, the writing format as well as the currency of the ﬁnancial amount. For example, the sentence “I owe Fred € 20 and I need € 10 for the bus.” might be changed to “I owe Fred 2 906.37 Yen and I need 1 453.19 Yen for the bus.” The transformation was ﬁrst introduced by Regina et al. (2020).
B.84 Replace Numerical Values
This transformation looks for numerical values in an English text and replaces it with another random value of the same cardinality. For example, “6.9” may be replaced by “4.2”, or “333” by “789”. The transformation was ﬁrst introduced by Mille et al. (2021).
B.85 Replace Spelling
This transformation adds noise to all types of English text sources (sentence, paragraph, etc.) using corpora of common spelling errors introduced by Deorowicz and Ciura (2005). Each word with a common misspelling is replaced by the version

with mistake with a probability p which by default is set to 0.2.
B.86 Replace nouns with hyponyms or hypernyms
This transformation replaces common nouns with other related words that are either hyponyms or hypernyms. Hyponyms of a word are more speciﬁc in meaning (such as a sub-class of the word), eg: ’spoon’ is a hyponym of ’cutlery’. Hypernyms are related words with a broader meaning (such as a generic category /super-class of the word), eg: ’colour’ is a hypernym of ’red’. Not every word will have a hypernym or hyponym.
B.87 Sampled Sentence Additions
This transformation adds generated sentence to all types of English text sources (sentence, paragraph, etc.) by passing the input text to a GPT-2 model (Radford et al., 2019). By default, GPT-XL is used, together with the prompt “paraphrase:” appended to the original text, after which up to 75 tokens are sampled. Since the additional text is sampled from a model, the model may introduce harmful language or generate text that contradicts the earlier text or changes its meaning. The idea to sample one or more additional sentences was ﬁrst introduced by Jia and Liang (2017a).
B.88 Sentence Reordering
This perturbation adds noise to all types of text sources (paragraph, document, etc.) by randomly shuﬄing the order of sentences in the input text (Lewis et al., 2020). Sentences are ﬁrst partially decontextualized by resolving coreference (Lee et al., 2018).
This transformation is limited to input text that has more than one sentence. There are still cases where coreference can not be enough for decontextualization. For example, there could be occurences of ellipsis as demonstrated by Gangal et al. (2021) or events could be mentioned in a narrative style which makes it diﬃcult to perform re-ordering or shuﬄing (Kocˇisky` et al., 2018) while keeping the context of the discourse intact.
B.89 Emoji Addition for Sentiment Data
This transformation adds positive emojis and smileys to positive sentiment data and negative emojis to negative sentiment data. For non-labelled data, it adds neutral smileys.

B.90 Shuﬄe Within Segments
In this transformation, a token sequence, for example BIO-tagged, is split into coherent segments. Thus, each segment corresponds to either a mention or a sequence of out-of-mention tokens. For example, a sentence “She did not complain of headache or any other neurological symptoms .” with tags O O O O O B-problem O B-problem I-problem I-problem I-problem O is split into ﬁve segments: [She did not complain of ], [headache], [or], [any other neurological symptoms], [.]. Then for each segment, a binomial distribution (p=0.5) is used to decide whether it should be shuﬄed. If yes, the order of the tokens within the segment is shuﬄed while the label order is kept unchanged. This transformation is inspired by Dai and Adel (2020).
B.91 Simple Ciphers
This transformation modiﬁes the text in ways that a human could rapidly decipher, but which make the input sequences almost completely unlike typical input sequences which are used during language model training. This transformation includes the following text modiﬁcations: double the characters, double the words, add spaces between the characters, reverse all characters in the text, reverse the characters within each word, reverse the order of the words in the text, substitute homoglyphs, rot13 cipher.
B.92 Slangiﬁcator
This transformation replaces some of the words (in particular, nouns, adjectives, and adverbs) of an English text with their corresponding slang. The replacement is done with the subset of the "Dictionary of English Slang & Colloquialisms".29 The amount of replacement is proportional to the corresponding probabilities of replacement (by default, 0.5 for nouns, adjectives, and adverbs each).
B.93 Spanish Gender Swap
This transformation changes the gender of all animate entities (mostly referring to people, and some animals) in a given Spanish sentence from masculine to feminine. This includes masculine nouns with feminine equivalents (e.g., doctor → doctora), nouns with a common gender (“sustantivos comunes en cuanto al género”, e.g., el violinista → la violinista), personal pronouns, and (optionally) given names often used with a given gender
29http://www.peevish.co.uk/slang/index.htm

(e.g., Pedro → Alicia). Epicene nouns are excluded. In addition, the gender of adjectives, determiners, pronouns and participles are modiﬁed in order to maintain the grammatical agreement.
B.94 Speech Disﬂuency Perturbation
This perturbation randomly inserts speech disﬂuencies in the form of ﬁller words into English texts. With a given probability (0.2 by default), a speech disﬂuency is inserted between words. The default disﬂuencies are "um", "uh", "erm", "ah", and "er". At least one ﬁller word is always inserted by this transformation.
B.95 Paraphrasing through Style Transfer
This transformation provides a range of possible styles of writing English language. The following styles can be chosen:
• Shakespeare - Trained on written works by Shakespeare.
• Switchboard - Trained on a collection of conversational speech transcripts.
• Tweets - Trained on 5.2M English tweets. • Bible - Trained on texts from the Bible. • Romantic poetry - Trained on romantic poetry. • Basic - A light, basic paraphraser with no spe-
ciﬁc style. The transformation follows the models and formulations by Krishna et al. (2020).
B.96 Subject Object Switch
This transformation switches the subject and object of English sentences to generate new sentences with a very high surface similarity but very diﬀerent meaning. This can be used, for example, for augmenting data for models that assess Semantic Similarity.
B.97 Sentence Summarizaiton
This transformation compresses English sentences by extracting subjects, verbs, and objects of the sentence. It also retains any negations. For example, “Stillwater is not a 2010 American liveaction/animated dark fantasy adventure ﬁlm” turns into “Stillwater !is ﬁlm”. Zhang et al. (2021) used a similar idea to this transformation.
B.98 Suspecting Paraphraser for QA
This paraphraser transforms a yes/no question into a declarative sentence with a question tag ,30 which
30https://www.englishclub.com/grammar/ tag-questions.htm

helps to add more question speciﬁc informality to the dataset. Example: ”Did the American National Shipment company really break its own ﬂeet?” -> ”The American National Shipment company really broke its own ﬂeet, didn’t it”.
B.99 Swap Characters Perturbation
This perturbation randomly swaps two adjacent characters in a sentence or a paragraph with a default probability (Zhang et al., 2019a).
B.100 Synonym Insertion
This perturbation adds noise to all types of text sources (sentence, paragraph, etc.) by randomly inserting synonyms of randomly selected words excluding punctuations and stopwords (Marivate and Sefara, 2020).
B.101 Synonym Substitution
This perturbation randomly substitutes some words in an English text with their WordNet (Miller, 1998) synonyms.
B.102 Syntactically Diverse Paraphrasing using Sow Reap models
This transformation is capable of generating multiple syntactically diverse paraphrases for a given sentence based on the work of Goyal and Durrett (2020). The model paraphrases inputs using a two step framework: 1) SOW (Source Order reWriting): This step enumerates multiple feasible syntactic transformations of the input sentence. 2) REAP (REarrangement Aware Paraphrasing): This step conditions on the multiple reorderings/ rearragements produced by SOW and outputs diverse paraphrases corresponding to these reoderings. The transformation is designed to work only on singlesentence inputs. Multi-sentence inputs results in an empty string/no transformation. The model are trained on the ParaNMT-50M dataset (Wieting and Gimpel, 2017; Wieting et al., 2017), which can be argued to be a bit noisy.
B.103 Subsequence Substitution for Sequence Tagging
This transformation performs same-label subsequence substitution for the task of sequence tagging, which replaces a subsequence of the input tokens with another one that has the same sequence of tags (Shi et al., 2021). This is done as follows: (1) Draw a subsequence A from the input (tokens, tags) tuple. (2) Draw a subsequence B within the

whole dataset, with the same tag subsequence. (3) Substitute A with B in the input example.
B.104 Change English Tense
This transformation converts English sentences from one tense to the other, for example simple present to simple past. This transformation was introduced by Logeswaran et al. (2018).
B.105 Token Replacement Based on Lookup Tables
This transformation replaces input tokens with their perturbed versions sampled from a given lookup table of replacement candidates. Lookup tables containing OCR errors and misspellings from prior work are given as examples. Thus, by default, the transformation induces plausible OCR errors and human typos to the input sentence.
The transformation is an adapted and improved version of the lookup table-based noise induction method from Namysl et al. (2020). The OCR lookup table is from Namysl et al. (2021) and the misspellings from Piktus et al. (2019).
B.106 Transformer Fill
This perturbation replaces words based on recommendations from a masked language model. The transformation can limit replacements to certain POS tags (all enabled by default). Many previous papers have used this technique for data augmentation (Ribeiro et al., 2020; Li et al., 2020b, inter alia).
B.107 Underscore Trick
This perturbation adds noise to the text sources like sentence, paragraph, etc. This transformation acts like a perturbation to test robustness. It replaces some random spaces with underscores (or even other selected symbols). This perturbation would beneﬁt all tasks which have a sentence/paragraph/document as input like text classiﬁcation and text generation, especially on tasks related to understanding/generating scripts.
B.108 Unit converter
This transformation converts length and weight measures to diﬀerent units (e.g., kilometers to miles) picking at random the new unit but converting accurately the quantity. The transformation conserves the format of the original quantity: "100 pounds" is converted to "1600 ounces" but "onehundred pounds" is converted to "one thousand,

six hundred ounces". Generated transformations display high similarity to the source sentences.
B.109 Urban Thesaurus Swap
This perturbation randomly picks nouns from the input source to convert to related terms drawn from the Urban Dictionary 31 resource. It can be applied to an input text to produce semantically-similar output texts in order to generate more robust test sets. We ﬁrst select nouns at random, then query the Urban Thesaurus website 32 to obtain a list of related terms to swap in (Wilson et al., 2020).
B.110 Use Acronyms
This transformation changes groups of words for their equivalent acronyms. It’s a simple substitution of groups of words for their acronyms. It helps to increase the size of the dataset as well as improving the understanding of acronyms of models trained on data augmented with this transformation. This transformations works to increase the data for any task that has input texts. It is specially interesting for tasks on semantic similarity, where models should be aware of the equivalence between a set of words and their acronym. The quality of the transformation depends on the list of acronyms. As of now, this list was scraped from wikipedia’s List of Acronyms 33 and naively ﬁltered, which leaves space for improvement .
B.111 Visual Attack Letter
This perturbation replaces letters with visually similar, but diﬀerent, letters. Every letter was embedded into 576-dimensions. The nearest neighbors are obtained through cosine distance. To obtain the embeddings the letter was resized into a 24x24 image, then ﬂattened and scaled. This follows the Image Based Character Embedding (ICES) (Eger et al., 2019a).
The top neighbors from each letter are chosen. Some were removed by judgment (e.g. the nearest neighbors for ’v’ are many variations of the letter ’y’) which did not qualify from the image embedding (Eger et al., 2019b).
B.112 Weekday Month Abbreviation
This transformation abbreviates or expands the names of months and weekdays, e.g. Mon. ->
31https://www.urbandictionary.com/ 32https://urbanthesaurus.org/ 33https://en.wikipedia.org/wiki/Lists_of_ acronyms

Monday. Generated transformations display high similarity to the source sentences and does not alter the meaning and the semantic of the original texts. It does not abbreviate plural names, e.g. Sundays, and does not inﬂuence texts without names of weekdays or months.
B.113 Whitespace Perturbation
This perturbation adds noise to text by randomly removing or adding whitespaces.
B.114 Context Noise for QA
This transformation chooses a set of words at random from the context and the question and forms a sentence out of them. The sentence is then prepended or appended to the context to create a new QA pair. The transformation is inspired by the the AddAny method described in Adversarial SQUAD (Jia and Liang, 2017b). However, instead of probing the model to generate adversaries, random words from the context and the question are simply selected and joined together into a sentence, ignoring grammaticality. The transformation attempts to create novel QA pairs assuming that the introduction of random words to the context is less likely to change the answer choice to an asked question.
B.115 Writing System Replacement
This transformation replaces the writing system of the input with another writing system. We use CJK Uniﬁed Ideographs34 as the source of characters for the generated writing systems. The transformation would beneﬁt text classiﬁcation tasks, especially in the cases where the input writing system is undeciphered.
B.116 Yes-No Question Perturbation
This transformation turns English non-compound statements into yes-no questions. The generated questions can be answered by the statements that were used to generate them. The text is left largely unchanged other than the fronted/modiﬁed/added auxiliaries and be-verbs.
The transformation works by getting dependency parse and POS tags from a machine learning model and applying human-engineered, rule-based transformations to those parses/tags. This transformation would particularly beneﬁt question-answering
34https://en.wikipedia.org/wiki/CJK_Unified_ Ideographs

and question-generation tasks, as well as providing surplus legal text for language modeling and masked language modeling.
B.117 Yoda Transformation
This perturbation modiﬁes sentences to ﬂip the clauses such that it reads like "Yoda Speak". For example, "Much to learn, you still have". This form of construction is sometimes called "XSV", where "the “X” being a stand-in for whatever chunk of the sentence goes with the verb", and appears very rarely in English normally. The rarity of this construction in ordinary language makes it particularly well suited for NL augmentation and serves as a relatively easy but potentially powerful test of robustness.
C Filters
The following is the list of all submitted ﬁlters to NL-Augmenter. Filters are used to ﬁlter data and create subpopulations of given inputs, according to features such as input complexity, input size, etc. Therefore, the output of a ﬁlter is a boolean value, indicating that whether the input meet the ﬁlter criterion. We discuss the implementations of each ﬁlter alongwith their limitations. The title of each ﬁlter subsection is clickable and redirects to the actual python implementation.
C.1 Code-Mixing Filter
This ﬁlter identiﬁes whether the input text is codemixed. It checks that there is at least one sentence in the text where there are tokens representing at least ‘k’ unique languages (with at least a ‘threshold‘ level of conﬁdence that the token is of that language). It is useful for collecting code-mixed data to test the model’s performance on multilingual tasks. The ﬁlter relies on ftlid35 for language detection, therefore, this ﬁlter might be limited by the performance of the language detection tool.
(containing code-mixing) Yo estaba con Esteban yesterday, he was telling me about lo que su esposa vio en los Estados Unidos. )True
C.2 Diacritics Filter
This ﬁlter checks whether any character in the sentence has a diacritic. It can be used to create splits of the dataet where the sentences have diacritics. Accented characters are typically among the rarer characters and checking the model performance on
35https://pypi.org/project/ftlid/

such a split might help investigate model robustness.
(containing diacritics) She lookèd east an she lookèd west. )True
C.3 Encoding Filter
This ﬁlter ﬁlters examples which contain characters outside a given encoding. It can be used to ﬁnd examples containing e.g. non-ASCII Unicode characters. Filtering out and testing examples that contain these characters can provide feedback on how to improve the models accordingly, since most models are trained with plain English text, which contains mostly ASCII characters. Sometimes nonASCII character are even explicitly stripped away.
(containing non-ASCII characters) That souvenir sure was expensive at 60£.. or was it 60€? )True
C.4 Englishness Filter
This ﬁlter identiﬁes texts that contain uniquely British spellings, vocabulary, or slang. The ﬁlter uses a vocabulary of common British words/phrases and checks the number of occurence of British words in the given texts. The text is selected if the number exceeds a pre-deﬁned threshold.
(containing British spellings) Colour is an attribute of light that is perceived by the human eye. )True
C.5 Gender Bias Filter
This ﬁlter ﬁlters a text corpus to measure gender fairness with respect to a female gender representation. It supports four languages (i.e. English, French, Polish and Russian) and can be used to deﬁne whether the female gender is suﬃciently represented in a tested subset of sentences. The ﬁlter uses a list of lexicals, which includes ﬁlter categories such as personal pronouns, words deﬁning the relation, titles and names, corresponding to the female and male genders accordingly.
(texts with unbalanced representation) "He went home", "He drives a car", "She has returned" )True
C.6 Group Inequity Filter
This is a bilingual ﬁlter (for English and French languages), which helps to discover potential group inequity issues in the text corpus. It is a topic agnostic ﬁlter which accepts user-deﬁned parameters, consisting of keywords inherent to minor group

(which potentially might suﬀer from the discrimination), major group, minor factor and major factor. The ﬁlter ﬁrst ﬂags the sentences as belonging to the minor, and the major groups, and then, the sentences from each of the groups are used to deﬁne the intersection with both factors. The ﬁlter then compares whether the percentage of major factors exceeds that of the minor factors to determine if the sentences have group inequity issues.
(containing group inequity issues) "He is a doctor", "She is a nurse", "She works at the hospital" )True
C.7 Keyword Filter
This is a simple ﬁlter, which ﬁlters examples based on a pre-deﬁned set of keywords. It can be useful in creating splits for a speciﬁc domain.
(containing keyword "at") Andrew played cricket in India )True
C.8 Language Filter
This ﬁlter selects texts that match any of a given set of ISO 639-1 language codes (the default language being English). Language matching is performed using a pre-trained langid.py model instance. The model provides normalized conﬁdence scores. A minimum threshold score needs to be set, and all sentences with conﬁdence scores above this threshold are accepted by the ﬁlter.
(is English texts) Mein Luftkissenfahrzeug ist voller Aale )False
C.9 Length Filter
This ﬁlter ﬁlters data with the input text length matching a speciﬁed threshold. It can be useful in creating data with diﬀerent length distributions.
(containing more than 3 words) Andrew played cricket in India )True
C.10 Named-entity-count Filter
This ﬁlter ﬁlters data where the number of Named Entities in the input match a speciﬁed threshold (based on the supported conditions).
(containing more than 1 named entity) Novak Djokovic is the greatest tennis player of all time. )True
C.11 Numeric Filter
This ﬁlter ﬁlters example which contain a numeric value. In the tasks like textual entailment, question answering etc., a quantity (number) could directly aﬀect the ﬁnal label/response. This ﬁlter can be

used to create splits to measure the performance separately on texts containing numeric values.
(containing numbers in texts) John bought a car worth dollar twenty ﬁve thousand . )True
C.12 Oscillatory Hallucinations Filter
This ﬁlter is designed to operate in text generation systems’ outputs, with the purpose of extracting oscillatory hallucinations. Oscillatory hallucinations are one class of hallucinations characterized by repeating bigram structure in the output(Raunak et al., 2021). Typically, these behaviors are observed in models trained on noisy corpora. The ﬁlter counts the frequency of bigrams in both source and target texts, and compare the frequency diﬀerence with a pre-set threshold to determine whether the texts includes oscillatory hallucinations.
(containing hallucinations in target texts) Source: "Community, European Parliament common regional policy, Mediterranean region", Target: "Arbeitsbedingungen, beruﬂiche Bildung, beruﬂiche Bildung, beruﬂiche Bildung" )True
C.13 Polarity Filter
This ﬁlter ﬁlters a transformed text if it does not retain the same polarity as an original text. This ﬁlter helps not to distort training data during augmentation for sentiment analysis-related tasks. While generating new data for a sentiment analysis task, it is important to make sure that generated data is labelled correctly.
(texts retaining polarity) "Hotel is terrible", "Hotel is great" )False
C.14 Quantitative Question Filter
This is a simple rule-based ﬁlter that can be used to identify quantitative questions. It can help to analyse models’ performance on questions which require numerical understanding. It is also useful to study possible biases in question generation.
(being quantitative question) How long does the journey take? )True
C.15 Question type ﬁlter
This ﬁlter helps identify the question category of a question answering example based on the question word or the named entity type of the answer. Knowledge of the question type can help in the development of question answering systems (Parikh et al., 2019) as well as for assessing performance on individual splits.

(being where question) Where is Delhi located ? )True
C.16 Repetitions Filter
This ﬁlter ﬁnds texts with repetitions with simple heuristic rules. It might be helpful in ﬁnding repetitions that frequently occur in the spoken language data.
(containing repetitions in texts) I I want to sleep )True
C.17 Phonetic Match Filter
This ﬁlter selects texts that contain matching entries to a list of supplied keywords. It ﬁrst transform the input sentence and the keywords into phenetic units and then compare whether the two phenetic unit sets have overlap.
(containing homophones of keyword "trombone") I left my trombno on the train )True
C.18 Special Casing Filter
This ﬁlter checks if the input sentence has a special casing, i.e. the string is either all lowercased, all uppercased or has title casing. It might be useful for creating splits that contain texts with unusual casing, e.g. misspellings.
(text being uppercased/lowercased/titlecased) let’s go to chipotle )True
C.19 Speech-Tag Filter
This ﬁlter ﬁlters an example text based on a set of speech tags and identiﬁes whether the count of selected POS tags meet the pre-deﬁned conditions (e.g. above the threshold).
(containing 1 verb and 2 numbers in texts) It all happened between November 2007 and November 2008. )True
C.20 Token-Amount ﬁlter
This ﬁlter ﬁlters an example text based on whether certain keywords are present in a speciﬁed amount.
(containing 2 occurances of "in") Andrew played cricket in a soccer stadium in India at 9pm )True
C.21 Toxicity Filter
This ﬁlter ﬁlters an example text which has a toxicity value matching a particular threshold. It uses a pre-trained toxicity detector, which can provide 7 toxicity scores. All the 7 types of toxicity scores can be used as criteria for the ﬁltering.

(text being toxic) I disagree. It is not supposed to work that way. )False
C.22 Universal Bias Filter
This ﬁlter works the same way as the Gender Bias Filter, but measures balance of representation for more categories (religion, race, ethnicity, gender, sexual orientation, age, appearance, disability, experience, education, economic status). The lexical seeds representing these categories are currently available in English only, however the pool of languages can be extended by a simple addition of the lexical seeds in a desired language to the lexicals.json ﬁle.
(texts being biased) "He is going to make a cake.", "She is going to program", "Nobody likes washing dishes", "She agreed to help him" )False
C.23 Yes/no question ﬁlter
This ﬁlter allows to select questions that can be correctly answered with either ’yes’ or ’no’. Since it is rule-based, the limitation of this ﬁlter is that questions that are ambiguous might not be recognized.
(text being yes/no question) Wasn’t she angry when you told her about the accident? )True

