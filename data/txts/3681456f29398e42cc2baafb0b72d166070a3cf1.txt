Global Convergence of Policy Gradient for Sequential Zero-Sum Linear Quadratic Dynamic Games

Jingjing Bu

Lillian J. Ratliﬀ

Mehran Mesbahi∗

arXiv:1911.04672v1 [eess.SY] 12 Nov 2019

Abstract
We propose projection-free sequential algorithms for linear-quadratic dynamics games. These policy gradient based algorithms are akin to Stackelberg leadership model and can be extended to model-free settings. We show that if the “leader” performs natural gradient descent/ascent, then the proposed algorithm has a global sublinear convergence to the Nash equilibrium. Moreover, if the leader adopts a quasi-Newton policy, the algorithm enjoys a global quadratic convergence. Along the way, we examine and clarify the intricacies of adopting sequential policy updates for LQ games, namely, issues pertaining to stabilization, indeﬁnite cost structure, and circumventing projection steps.
Keywords: Dynamic LQ games; stabilizing policies; sequential algorithms
1 Introduction
Linear-quadratic (LQ) dynamic and diﬀerential games exemplify situations where two players inﬂuence an underlying linear dynamics in order to respectively, minimize and maximize a given quadratic cost on the state and the control over an inﬁnite time-horizon.1 This LQ game setup has a rich history in system and control theory, not only due to its wide range of applications but also since it directly extends its popular one player twin, the celebrated linear quadratic regular (LQR) problem [Ber13, Eng05, Zha05]. LQR on the other hand, is one of the foundations of modern system theory [Wil71, AM07]. This partially stems from the fact that the elegant analysis of minimizing a quadratic (inﬁnite horizon) cost over an inﬁnite dimensional function space leads to a solution that is in the constant feedback form, that can be obtained via solving the algebraic Riccati equation (ARE) [LR95]. As such, solving LQR and its variants have often been approached from the perspective of exploiting the structure of ARE [BIM12]. The ARE facilitates solving for the so-called cost-to-go (encoded by a positive semi-deﬁnite matrix), that can subsequently be used to characterize the optimal state feedback gain. This general point of view has also inﬂuenced the “data-driven” approaches for solving the generic LQR problem. For instance, in the value-iteration for reinforcement learning (RL)—e.g., Q learning—one aims to ﬁrst estimate the cost-to-go at a given time instance and through this estimate, update the state feedback gain. In recent years, RL has witnessed major advances for wide range of decision-making problems (see, e.g., [SHM+16, MKS+15]).
Direct policy update is another algorithmic pillar for decision-making over time. The conceptual simplicity of policy optimization oﬀers advantages in terms of computational scalability
∗The authors are with the University of Washington, Seattle, WA, 98195; Emails: {bu,ratliﬄ,mesbahi}@uw.edu 1We will adopt the convention of referring to the continuous time scenario as diﬀerential games. Moreover, in this paper, we focus on inﬁnite horizon LQ games without a discount factor.
1

1 INTRODUCTION
and potential extensions to model-free settings. As such, there is a renewed interest in analyzing the classical LQR problem under the RL framework from the perspective of direct policy updates [DMM+17, FGKM18]. The extension of LQR control to multiple-agent settings, i.e., LQ dynamic and diﬀerential games, has also been explored by the game theory and optimal control communities [BO99, VMKL17]. Two-person zero-sum LQ dynamic and diﬀerential games are particular instances of this more general setting where two players aim to optimize an objective with directly opposing goals subject to a shared linear dynamical system. More precisely, one player attempts to minimize the objective while the other aims to maximize it. This framework has important applications in H∞ optimal control [BB08]. In fact, as it is well-known in control theory, the saddle point solution—i.e., a Nash equilibrium (NE)—of an inﬁnite horizon LQ game can be obtained via the corresponding generalized algebraic Riccati equation (GARE). As such, seeking the NE in a zero-sum LQ dynamic game revolves around solving the GARE [SW94]. Recently, multi-agent RL [SLZ+18, JCD+19] has also achieved impressive performance by using direct policy gradient updates. Since LQ dynamic games have explicit solutions via the GARE, understanding the performance of policy gradient based algorithms for LQ games could serve as a benchmark, and providing deeper insights into theoretical guarantees of multi-agent RL in more general settings [VMKL17]. In the meantime, the application of policy optimization algorithms in the game setting proves to require more intricate analysis due to the fact that the inﬁnite horizon cost is undiscounted and potentially unbounded per stage. As such, it is well known that devising direct policy iterations for undiscounted and unbounded per stage cost functions in the RL setting is nontrivial [Ber05]. The cost structure of standard LQR, however, streamlines the design of policy based iterations [Hew71, FGKM18, BMFM19].2 Nevertheless in policy iteration, special care has to be exercised to ensure that the iterative policy updates are in fact stabilizing. The stabilization issues is particularly relevant in the LQ dynamic games. Note that the policy space for LQ games is an open set admitting a cartesian product structure. Hence, in the policy updates for LQ dynamic games (say in the RL setting), we must guarantee that the iterates jointly stay in the open set as otherwise the “simulator” would diverge. Recently, Zhang et al. [ZYB19], using certain assumptions and relying on a projection step, have proposed a sequential direct policy updates with a sublinear convergence for LQ dynamic games.
Contributions. In this paper, we ﬁrst clarify the setting for discussing sequential LQ dynamic games, particularly addressing issues pertaining to stabilization. We then propose leader-follower type algorithms that resemble the Stackelberg leadership model [BO99]. Speciﬁcally, in the proposed iterative algorithms for LQ games, one player is designated as a leader and the other as a follower. We require that the leader plays natural gradient or quasi-Newton policies while the follower can play any ﬁrst-order based policies. In particular, we do not require a speciﬁc player to be the leader as long as the algorithm can be initialized appropriately. We prove that if the leader performs a natural gradient policy update, then the proposed leader-follower algorithm has a global sublinear convergence and asymptotic linear convergence rate. In the meantime, if the leader adopts a quasiNewton policy update, the algorithm converges at a global quadratic rate. Moreover, we show that gradient policy (respectively, natural gradient and quasi-Newton policies) has a global linear (respectively, linear and quadratic) convergence to the optimal stabilizing feedback gains even when the state cost matrix Q is indeﬁnite. This result essentially extends the results on standard LQR investigated in [FGKM18, BMFM19], where the analysis relies on the assumption of having Q ≻ 0; this extension is of independent interest for various optimal control applications as well, e.g.,
2In [FGKM18, BMFM19] it has been assumed that the quadratic state cost is via a positive deﬁnite Q; this is not “standard” as only the detectability of the pair (Q, A) and Q ⪰ 0 is required for LQR synthesis.
2

2 NOTATION AND BACKGROUND
control with conﬂicting objectives [Wil71]). Compared with the results presented in [ZYB19], the contributions of this work include the following:
(1) We remove the “nonstandard assumption” that the NE point (K∗, L∗) satisﬁes Q − L⊺∗R2L∗ ≻ 0.3 We note that such an assumption is not standard in the LQ literature and as such, needs further justiﬁcation beyond its algorithmic implications. In fact, in the analysis presented in [ZYB19], it is crucial for the convergence of the algorithm to a priori know the positive number ε > 0 for which {L ∶ Q − L⊺∗R2L∗ ⪰ εI}; moreover, one has to be able to project onto this set.
(2) Our setting allows for larger stepsizes for the policy iteration in LQ dynamic games, greatly improving its practical performance. This is facilitated by providing insights into the stabilizing policy updates through a careful analysis of the corresponding indeﬁnite GARE.
(3) We clarify the interplay between key concepts in control and optimization in the convergence analysis of the proposed iterative algorithms for LQ dynamic games. This is inline with our belief that identifying the role of concepts such stabilizability and detectability in the convergence analysis of “data-guided" algorithms for decision making problems with an embedded dynamic system is of paramount importance.
(4) We show that the quasi-Newton policy has a global quadratic convergence rate for LQ dynamic games. This result might be of independent interest for discrete-time GARE, data-driven or not. To the best of our knowledge, the proposed algorithm is the ﬁrst iterative approach for discrete-time GARE with a global quadratic convergence.
(5) Finally, we show that in the proposed iterative algorithms for LQ dynamic games, any player can assume the role of the “leader" whereas in [ZYB19], it is required that the player maximizing the cost be designated as the leader. As such, we clarify the algorithmic source of asymmetry in the leader-follower setup for solving this class of dynamic game problems.
2 Notation and Background
We use the symbols ≺, ⪯, ≻, ⪰ to denote the ordering induced by the positive semideﬁnite (p.s.d.) cone. Namely, A ⪰ B means that A − B is positive semideﬁnite. For a symmetric matrix M ∈ Rn×n, we denote the eigenvalues in non-increasing order, i.e., λ1(M ) ≤ . . . ≤ λn(M ).
Let us recall relevant deﬁnitions and results from control theory. A matrix A ∈ Rn×n is Schur if all the eigenvalues of A are inside the open unit disk of C, i.e., ρ(A) < 1 where ρ(⋅) denotes the spectral radius. A pair (A, B) with A ∈ Rn×n and B ∈ Rn×m is stabilizable if there exists some K ∈ Rm×n such that A − BK is Schur. A pair (C, A) is detectable if (A⊺, C⊺) is stabilizable. An eigenvalue λ of A is (C, A)-observable if
rank λI − A = n. C
A matrix K ∈ Rm×n is stabilizing for system pair (A, B) if A−BK is Schur; it is marginally stabilizing if ρ(A − BK) = 1. For ﬁxed A ∈ Rn×n and Q ∈ Rn×n, the Lyapunov matrix is of the form
A⊺XA + Q − X = 0.
3It is noted that in [ZYB19] that the projection step is not generally required in implementations; however the convergence analysis presented in [ZYB19] is based on such a projection.
3

3 LQ DYNAMIC GAMES AND SOME OF ITS ANALYTIC PROPERTIES

For a system pair (A, B), Q ∈ Rn and R ∈ GLn(R), the discrete algebraic Riccati equation (DARE) is of the form

(1)

A⊺XA − X − A⊺XB(R + B⊺XB)−1B⊺XA + Q = 0.

Next, we recall a result on standard linear-quadratic-regulator (LQR) control.
Theorem 2.1. If Q ⪰ 0, R ≻ 0, (A, B) is stabilizable and the spectrum of A on the unit disk (centered at the origin) in C is (Q, A)-observable, then there exists a unique maximal solution X+ to DARE (1).4 Moreover, the inﬁnite-horizon LQR cost is x⊺0X+x0 and the optimal feedback control K∗ is stabilizing and characterized by K∗ = (R + B⊺X+B)−1B⊺X+A.
In the presentation, we shall refer a solution X0 to (1) as stabilizing if the corresponding feedback gain K0 = (R + B⊺X0B)−1B⊺X0A is stabilizing; a solution X0 is almost stabilizing if the corresponding gain K0 = (R + B⊺X0B)−1B⊺X0A is marginally stabilizing.
In the sequential LQ game setup, one key diﬀerence from standard LQR is that Q may be indeﬁnite in the corresponding DARE. As such, the following generalization of the above theorem becomes particularly relevant.
Theorem 2.2. Suppose that Q = Q⊺, R ≻ 0, (A, B) is stabilizable and there exists a solution X to DARE (1). Then there exists a maximal solution X+ to DARE such that the LQR cost is given by x⊺0X+x0. Moreover the optimal feedback control is given by K∗ = (R + B⊺XB)−1B⊺X+A and the eigenvalues of A − BK∗ lie inside the closed unit disk of C.
For DAREs with an indeﬁnite Q, we recall a theorem concerning the existence of solutions.
Theorem 2.3 (Theorem 13.1.1 in [LR95]). Suppose that (A, B) is stabilizable, R = R⊺ is invertible, Q = Q⊺ (no deﬁniteness assumption), and there exists a symmetric solution Xˆ to the matrix inequality,
R(X) := A⊺XA + Q − X − A⊺XB(R + B⊺XB)−1B⊺XA ⪰ 0,

with R + B⊺X˜ B ⪰ 0. Then there exists a maximal solution X+ to (1) such that R + B⊺X+B ≻ 0. Moreover, all eigenvalues of A − B(R + B⊺XB)−1B⊺X+A are inside the closed unit disk.
The map RA,B,Q,R ∶ Rn×n → Rn×n will be referred as Riccati map in our analysis; we will also suppress its dependency on system parameters A, B, Q, R. In our subsequent analysis, these system parameters will not remain constant as the corresponding feedback gains are iteratively updated.

3 LQ Dynamic Games and some of its Analytic Properties
In this section, we review the setup of zero-sum LQ games. In particular we discuss the modiﬁed sequential formulation of LQ games and make a few analytical observations that are of independent interest. We note that some of these observations have only become necessary in the context of sequential policy updates for LQ dynamic games.
4Where the notion of maximality is with respect to the p.s.d. ordering.

4

3 LQ DYNAMIC GAMES AND SOME OF ITS ANALYTIC PROPERTIES

3.1 Zero-sum LQ Dynamic Games

3.1 Zero-sum LQ Dynamic Games
In the standard setup of LQ game, we consider a (discrete-time) linear time invariant model of the form,

x(k + 1) = Ax(k) − B1u1(k) − B2u2(k), x(0) = x0,

where A ∈ Rn×n, B1 ∈ Rn×m1, B2 ∈ Rn×m2, u1(k) and u2(k) are strategies played by two players. The cost incurred for both players is the quadratic cost

∞
J(u1, u2, x0) = ⟨x(k), Qx(k)⟩ + ⟨u1(k), R1u1(k)⟩ − ⟨u2(k), R2u2(k)⟩ ,
k=0

where

Q

∈

S+n,

R1

∈

S

++ m1

,

R2

∈

S+m+2 ,

and

x0

is

the

initial

condition;5

the

underlying

inner

product

is

denoted by ⟨⋅, ⋅⟩. In this setting, player one chooses its policy to minimize J while player two aims

to maximize it.

The players’ strategy space that we will be particularly interested in are closed-loop static linear

policies, namely, policies of the form u1(k) = Kx(k) and u2(k) = Lx(k), where K ∈ Rm1×n and L ∈ Rm2×n. Note that the cost function is guaranteed to be ﬁnite over the set of Schur stabilizing

feedback gains,

S = {(K, L) ∈ Rm×n × Rm×n ∶ ρ(A − B1K − B2L) < 1}.

Indeed, if (K, L) ∈ S, with initial condition x0, the cost will be given by,

J (u1, u2, x0) = x⊺ ⎛ ∞ [(A − B1K − B2L)⊺]j (Q + K⊺R1K − L⊺R2L)(A − B1K − B2L)j⎞ x0

0 ⎝j=0

⎠

= x⊺0Xx0,

where X solves the Lyapunov matrix equation,

(2)

(A − B1K − B2L)⊺X(A − B1K − B2L) + Q + K⊺R1K − L⊺R2L = 0.

Note that (2) has a unique solution if (K, L) ∈ S. We say that a pair of strategies (K, L) is admissible if (K, L) ∈ S.

Remark 3.1. An elegant result in H∞ control theory states that the minimax problem, inf sup {J (u1, u2) xu1,u2 ∈ ℓ2(N)},
u1∈ℓ2(N) u2∈ℓ2(N)

where ℓ2(N) denotes the Banach space of all square summable sequences and xu1,u2 denotes the state trajectory after adopting control signals u1, u2, has a unique saddle point for all initial conditions if and only if there exists two static linear feedback gains K∗ and L∗ such u1(k) = K∗x(k) and u2(k) = L∗x(k) satisfying the saddle point condition [Sto90]. Hence, the restriction of the optimization process to static linear policies in the LQ game setting is without loss of generality.
5The notation Sn+ and Sn++ designate, respectively, n × n symmetric positive semideﬁnite and positive deﬁnite matrices.

5

3 LQ DYNAMIC GAMES AND SOME OF ITS ANALYTIC PROPERTIES

3.1 Zero-sum LQ Dynamic Games

A stabilizing Nash equilibrium for the zero-sum game is the pair of actions {u∗1(k), u∗2 (k)} = {K∗x(k), L∗x(k)} such that,

(3)

J (u∗1 (k), u2(k)) ≤ J (u∗1 (k), u∗2 (k)) ≤ J (u1(k), u∗2 (k)),

for all initial states x0 and all u1(k), u2(k) for which (u1(k), u∗2 (k)) and (u∗1(k), u2(k)) are both admissible pairs. We emphasize that it is important that (u1(k), u∗2 (k)) and (u∗1(k), u2(k)) are
stabilizing action pairs in the inequality (3). To demonstrate this delicate situation, we denote by
Sπi the projection of S onto the ith coordinate, i.e.,

Sπ1 = {K ∶ ∃L such that A − B1K − B2L is Schur}, Sπ2 = {L ∶ ∃K such that A − B1K − B2L is Schur},

and SKˆ , SLˆ as sets deﬁned by,
SKˆ = {L ∶ A − B1Kˆ − B2L is Schur}, SLˆ = {K ∶ A − B2Lˆ − B1K is Schur}.
Clearly, SK∗ ⊂ Sπ2 and SL∗ ⊂ Sπ1. This means that it is not the case that for all K ∈ Sπ1 and all L ∈ Sπ2, the corresponding actions u1(k) = Kx(k) and u2(k) = Lx(k) yield
J (u∗1 (k), u2(k)) ≤ J (u∗1 (k), u∗2 (k)) ≤ J (u1(k), u∗2 (k)).
This is simply due to the fact that (Kˆ , L∗) is not guaranteed to be stabilizing for all Kˆ ∈ Sπ1. Note that the cost function J is a function of polices K, L and initial condition x0. Since we
are interested in the Nash equilibrium independent of the initial conditions, naturally, we should formulate cost functions for both players to reﬂect this independent. Indeed, this point has been discussed in [BMFM19] where it has been argued that such a formulation is in general necessary for the cost functions to be well deﬁned (see details in §III [BMFM19]). The independence with respect to initial conditions can be achieved by either sampling x0 from a distribution with full-rank covariance [FGKM18], or choosing a spanning set {w1, . . . , wn} ⊆ Rn [BMFM19], and deﬁning the value function over S as,
n
f (K, L) = Jwi(K, L),
i=1
where Jwi(K, L) is the cost by choosing initial state wi, u1(k) = Kx(k) and u2(k) = Lx(k). Note that over the set S the value of function f admits a compact form,
f (K, L) = Tr(XΣ),
where Σ = ∑ni=1 wiwi⊺ and X is the solution to (2).
Behavior of f on Sc: How the cost function f behaves near the boundary ∂S is of paramount importance in the design of iterative algorithms for LQ games. In the standard LQR problem (corresponding to a single player case in the game setup), the cost function diverges to +∞ when the feedback gain approaches the boundary of this set (see [BMFM19] for details). In fact, this property guarantees stability of the obtained solution via ﬁrst order iterative algorithms for suitable choice of stepsize. However, the behavior of f on the boundary ∂S could be more intricate. For
6

3 LQ DYNAMIC GAMES AND SOME OF ITS ANALYTIC PROPERTIES

3.2 Stabilizing Policies in Sequential Zero-Sum LQ Games

example, if (K, L) ∈ ∂S, i.e., ρ(A − B1K − B2L) = 1, then it is possible that the cost is still ﬁnite for
both players. This happens when an eigenvalue of A − B1K − B2L on the unit disk in the complex plane is not (Q + K⊺R1K − L⊺R2L, A − B1K − B2L)-observable. To see this, we observe that for
every ωi, the series

∞ Jω (K, L) = ω⊺ ⎛ ∞ ((A − B1K − B2L)⊺)j (Q + K⊺R1K − L⊺R2L)(A − B1K − B2L)j ⎞ ωi

i
j=0

i ⎝j=0

⎠

is convergent to a ﬁnite (real) number if the marginally stable modes are not detectable. Even on S¯c (complement of closure of S), f could be ﬁnite if all the non-stable modes of (A−B2K −B2L) are not (Q + K⊺R1K − L⊺R2L, A − B1K − B2L)-observable. The complication suggests that the function
value is no longer a valid indictor of stability. We remark that such a situation does not occur in the
LQ setting examined in [FGKM18, BMFM19], as it has been assumed that Q is positive deﬁnite.

3.2 Stabilizing Policies in Sequential Zero-Sum LQ Games

Another subtle situation arising in sequential zero-sum LQ dynamic game is as follows: there is clearly no incentive for player 1 to destabilize the dynamics. However, from the perspective of player 2, making the states diverge to inﬁnity could be desirable as the player aims to maximize the cost. For player 1, in the situation where Q − L⊺R2L is not positive semideﬁnite, it is also possible that the best policy is not the one in Sπ1. Hence, in round j, in order to guarantee that the game can be continued, it is important that both players choose their respective policies in Sπ1 and Sπ2. We may then stipulate that both players play Schur stable policies. We can justify this constraint by insisting that both players have an incentive to stabilize the system in the ﬁrst place. This can also be encoded in the cost function for the player. That is, we may deﬁne the cost functions for player 1 and player 2 by,

f1(K, L) = δSπ1 (K) + f (K, L), f2(K, L) = −δSπ2 (L) + f (K, L),

where δSπi (x) is the indicator function of the set

⎧⎪⎪0, δSπi (x) = ⎨⎪⎪⎩+∞,

x ∈ Sπi, x ∉ Sπi.

Then we have two cost functions deﬁned everywhere for both players and assume a ﬁnite value on
S which agree with each other, i.e., f (K, L). We still need to be careful in realizing that there are points for which the function value is indeterminate. For example, it is possible to ﬁnd a point (Kˆ , Lˆ) such that f (Kˆ , Lˆ) = −∞; then f1(Kˆ , Lˆ) = +∞ − ∞. To resolve this complication, we shall declare the function value to be the ﬁrst summand; namely, if f1(Kˆ , Lˆ) = +∞ − ∞, then f1(Kˆ , Lˆ) ≡ +∞.
From the perspective of sequential algorithm design, these newly introduced cost functions
would constrain both players to play policies in S. It might be tempting to design projection based
algorithms. However, this can be diﬃcult since describing the sets Sπ1 and Sπ2 for given system (A, B) is not straightforward. We shall see later that by exploiting the problem structure, we can
design sequential algorithms for both players to guarantee this condition without any projection
step.

7

3 LQ DYNAMIC GAMES AND SOME OF ITS ANALYTIC PROPERTIES

3.3 Analytic Properties of the Cost Function

3.3 Analytic Properties of the Cost Function
In this section we shall clarify analytical properties of the cost functions in terms of the polices played by the two players; that is, we consider the cost function f (K, L) over S.6 To begin with, we observe the set S even though is not convex, still possesses nice topological properties.
Proposition 3.2. The set S is open, contractible (i.e., path-connected and simply connected) and in general non-convex.
Proof. It suﬃces to note that by Kalman Decomposition [Won12], there exists some T ∈ GLn(R) such that,

T AT −1 = ⎛A˜11 A˜12⎞ , T [B1, B2] = B˜1 ,

⎝ 0 A˜22⎠

0

where (A˜11, B˜1) is controllable and A˜22 is Schur. Suppose that B˜1 ∈ Rn1×(l1+l2) and further observe that S can be diﬀeomorphically identiﬁed by S(A˜11,B˜1) × R(n−n1)×(l1+l2). The statement then follows by the results reported in [BMM19].
As the set S is generally not convex, Proposition 3.2 assures us that algorithms based on local search (e.g. gradient descent) can potentially reach the Nash equilibrium. If S had more than one path-connected components, it will be impossible to guarantee the convergence to Nash equilibrium under random initialization. Moreover this observation implies that f is not convex-concave as the domain is not even convex.
We next observe that the value function is smooth and indeed real analytic, i.e., f ∈ Cω(S).
Proposition 3.3. One has f ∈ Cω(S).
Proof. For (K, L) ∈ S, f is the composition

(K, L) ↦ X(K, L) ↦ Tr(XΣ),

where X solves (2). But vec(X) = I ⊗ I − A⊺K,L ⊗ A⊺K,L −1 vec(Q + K⊺R1K − L⊺R2L),

by Cramer’s Rule; the proof thus follows.
As f is smooth, its partial derivatives with respect to K and L can be characterized as follows.
Proposition 3.4. On the set S, the gradients of f with respect to its arguments are given by, ∂K f (K, L) = (R1K − B1⊺XAK,L)Y, ∂Lf (K, L) = (−R2L − B2⊺XAK,L)Y,

where X solves the Lyapunov equation (2) and Y solves the Lyapunov equation,

(4)

AK,LY A⊺K,L + Σ = 0.

6In our formulation, the two players have diﬀerent cost functions. But over the set S, the cost functions coincide.

8

3 LQ DYNAMIC GAMES AND SOME OF ITS ANALYTIC PROPERTIES

3.4 A Key Assumption and its Implications

Proof. It suﬃces to rewrite the Lyapunov equation in the form

⎛A − B1 B2 ⎝

K ⎞⊺ ⎛ L ⎠ X ⎝A − B1 B2

K⎞

K ⊺ R1 0

L ⎠ − X + Q + L 0 −R2

By the result in [FGKM18, BMFM19], the gradient of f is given by ∇f (K, L) = (R1K − B1⊺XAK,L)Y . (−R2K − B1⊺XAK,L)Y

K = 0. L

We now observe that Y (K, L) is a smooth function in (K, L) and is positive deﬁnite everywhere on S. Hence Y (K, L) is a well-deﬁned Riemannian metric on S. Under this Riemannian metric, we can thereby identify the gradient. In learning and statistics literature, such a gradient is referred as a “natural gradient.” We shall use Nf,K and Nf,L to denote the natural gradient of f over K and L, respectively. Namely,
Nf,K (K, L) = R1K − B1⊺XAK,L, Nf,L(K, L) = −R2L − B2⊺XAK,L.

3.4 A Key Assumption and its Implications

Throughout the manuscript, we have the following standing assumption.

Assumption 1. There exists a stabilizing Nash Equilibrium (K∗, L∗) ∈ S for the zero-sum game over the system dynamic (A, [B1, B2]). Moreover, the corresponding value matrix X∗ = X∗(K∗, L∗) satisﬁes at least one of the following conditions:

(a1): R1 + B1⊺X∗B1 ≻ 0 and R2 − B2⊺X∗B2 + B2⊺X∗(R1 + B1⊺X∗B1)−1B1B2 ≻ 0.

(a2): −R2 + B2⊺X∗B1 ≺ 0 and R1 + B1⊺X∗B1 − B1⊺X∗B2(−R2 + B2⊺X∗B2)−1B2⊺X∗B1 ≻ 0.

Remark 3.5. The existence of a stabilizing Nash is a necessary assumption adopted in the LQ literature [BB08]. However, we do not constrain the value matrix X∗ to be positive semideﬁnite, as assumed in [SW94, BB08, ZYB19]. The deﬁniteness is useful when the LQ game formulation is tied to H∞ control. However, from the LQ game perspective, this association seems unnecessary. Conditions (a1) or (a2) are necessary if it is desired to extract unique policies from the optimal value matrix X∗. Namely, if the total derivative of f vanishes, i.e.,

R1 0 0 −R2

K − B1⊺ XA + B1⊺ X B1 B2 = 0,

L

B2⊺

B2⊺

conditions (a1) or (a2) are suﬃcient to guarantee the uniqueness of the solution in S. Indeed, assumptions (a1) or (a2) are “almost necessary.” If (K∗, L∗) is a NE, then f (⋅, L∗) achieves a local minimum at K∗, i.e., ∇KKf (K∗, L∗)[E, E] = ⟨E, (R1 +B1⊺X∗B1)EY∗⟩ ≥ 0 (note that by assumption, K∗ is in the interior of S and thus the second-order partial derivative is well-deﬁned). Similarly, ∇LLf (K∗, L∗) = −R2 + B2⊺X∗B2 ⪯ 0. We relax these two necessary conditions to hold as strictly positive (respectively, negative) deﬁnite.7 In fact, in the sequential LQ formulation, the inequalities
7If we do not relax the semideﬁniteness conditions, the NE would be solutions to GARE involving Moore-Penrose inverse. This will introduce other complications than practically needed.

9

4 ORACLE MODELS FOR SEQUENTIAL LQ GAMES

in these two conditions correspond to certain “quasi-Newton” directions and as such play a central role in our convergence analysis (see §5 and §6 for details.).
Moreover, we shall subsequently see that assumptions (a1) and (a2) lead to distinct choices of leaders in the sequential algorithms. More speciﬁcally, if we assume condition (a1), the leader of the sequential algorithm should be player L; for assumption (a2), player K should be the designated leader.

We observe several implications of this assumption.

Proposition 3.6. Under Assumption 1, we have following implications:

a. The pair (A, [B1, B2]) is stabilizable.

b. X∗ is symmetric and solves the Generalized Algebraic Riccati Equation (GARE),

(5)

A⊺XA − X + Q + B1⊺XA ⊺ R1 + B1⊺XB1

B1⊺XB2 −1 B1⊺XA = 0.

B2⊺X A

B2⊺XB1 −R2 + B2⊺XB2 B2⊺XA

c. X∗ is unique among all almost stabilizing solutions of (5).

Proof. The statement in (a) is immediate since A − B1K∗ − B2L∗ is Schur. In order to show (b), we note that since (K∗, L∗) is a stabilizing Nash Equilibrium, then X∗ is the solution of the Lyapunov equation (2); it thus follows that X∗ is symmetric. Further, note that the partial gradients of f vanish at (K∗, L∗), namely (K∗, L∗) ∈ S solves the equations
∇K f (K, L) = (R1K − B1⊺XAK,L)Y = 0, ∇Lf (K, L) = (−R2L − B2⊺XAK,L)Y = 0.

Substituting this in the Lyapunov equation (2), it follows that (K∗, L∗) solves the GARE (5). Note that the inverse,

R1 + B1⊺XB1 B2⊺ X B1

B⊺XB2 −1
1
−R2 + B2⊺XB2

is well-deﬁned at X∗ by the conditions a1 or a2 in the assumption. For the statement in (c), by Lemma 3.1 [SW94], X∗ is the unique stabilizing solution. It remains to show that there does not exist almost stabilizing solution to (5). Suppose there exists a pair (K, L) ∈ ∂S, i.e., ρ(A − B1K − B2L) = 1, solving (5) with solution X. Then taking the diﬀerence between the identity (5) at (K∗, L∗) and (K, L), we have,
A⊺∗(X∗ − X)AK,L = X∗ − X.
Since AK,L is marginally stable and A∗ is stable, then I ⊗ I − A⊺K,L ⊗ A∗ is invertible and thus X∗ − X = 0.

4 Oracle Models for Sequential LQ Games
In this work, we assume that both players have access to oracles that return either gradient, natural gradient or quasi-Newton directions. Suppose that OK and OL are the orcales for the two players respectively. The players will query their respective oracles in a sequential manner: if player 1 query the oracle, we assume the policy played by player 2 is ﬁxed during the query and this policy

10

4 ORACLE MODELS FOR SEQUENTIAL LQ GAMES

4.1 Motivation

is transparent to oracle OK of player 1. As f is in general not convex-concave, if two players have the same oracles and play greedily using the information they have acquired, theoretically, there is no guarantee that they will eventually converge to the Nash equilibrium. In order to obtain theoretical guarantees, we assume that player 1 can access an oracle that computes the minimizer of f (K, L) over K for a ﬁxed L. This oracle can be constructed out of the simple ﬁrst-order oracles by repeatedly performing gradient descent/natural gradient descent/quasi-Newton type steps. More explicitly, we shall assume that for player 1, if player 2’s policy is Lˆ, the oracle can return K ← arg minK f (K, Lˆ).

4.1 Motivation

We shall present the motivation for equipping player 1 with a more powerful oracle model. As ﬁnding the Nash equilibrium is equivalent to solving the saddle point of f (K, L), from the perspective of player 2, we may associate a value function independent of player 1. Namely, we may deﬁne a function of the form,

g(L)

=

⎧⎪⎪inf ⎨

K

∈SL

f

(K,

L),

⎪⎪⎩−∞,

if L ∈ Sπ2, otherwise.

If g(L) possesses a smoothness property, we may consider projected gradient ascent over the policy
space. However, reﬂecting over g(L) would reveal that g(L) is not necessarily even continuous on Sπ2. For example, if Q − L⊺R2L ≺ 0, then infK f (K, L) could be −∞. But on the other hand, by Danskin’s Theorem, g(L) is diﬀerentiable at L ∈ Sπ2, where f (K, L) admits a unique minimizer
over K.

Lemma 4.1. Suppose that U ⊆ dom(g) is an open subset such that for every L ∈ U ,

arg min f (K, L)
K ∈SL

exits and is unique. Then g(L) is diﬀerentiable and its gradient is,

∇g(L) = ∇Lf (KL, L), where KL = arg min f (K, L).
K
Traditionally Danskin’s theorem would require that for every L, the minimization of K is over a common compact set. This is not the situation in our case as SL is not compact nor common over L8. The statement of Lemma 4.1 instead follows from a variant of Danskin’s Theorem in [BR95].
Proof. We only need to observe that f (K, L) is C∞ in both variables and thus Fréchet diﬀerentiable. Hence, the assumptions of Hypothesis D2 in [BR95] are satisﬁed. By Theorem D2 in [BR95], g(L) is directionally diﬀerentiable in every direction. As the minimizer KL is unique, the directional derivative is uniform in every direction and consequently, g(L) is diﬀerentiable.

The next issue that needs to be addressed is whether U is empty. It turns out that by standard LQR theory, {L ∈ dom(g) ∶ Q − L⊺R2L ≻ 0} is a subset in U . We can thus outline an update rule
assuming that g(L) is Lipschitz, namely, a projected gradient ascent over L as,

Lj+1 = PSπ2 Lj + ηj ∇g(Lj ) ,
8Namely, it is not necessarily true SL1 = SL2 if L1 ≠ L2.

11

5 ALGORITHM: NATURAL GRADIENT POLICY ON L

where ∇g(Lj ) is given by

∇g(Lj ) = (−R2L − B2T XKL ,Lj AKL ,Lj )YKL ,Lj with KLj = arg min f (K, Lj ).

j

j

j

K

As already noted, we do not have a full description of the nonconvex set Sπ2 and a projection would rather be prohibitive. What we shall propose instead, are update rules that guarantee all of its iterates stay in the set Sπ2; this will be achieved without a projection step by exploiting the problem structure.
Another interesting interpretation of our setup is to consider this game from the perspective of player 2: we have a game played by player 2 with a greedy adversary. Each time player 2 chooses a policy L′, the adversary (player 1) would try to act greedily, i.e., minimize the cost f (K, L′) over K. The goal for player 2 is to achieve the Nash equilibrium point for himself/herself and the greedy adversary. The information player 2 can acquire from the game (i.e., oracle) is the ﬁrst-order information (function value and gradient). As such, player 2 has an obligation to guarantee that along the iterates {Lj}, the oracle could return meaningful ﬁrst-order information of g(L), i.e., g(Lj) is diﬀerentiable for every j.

5 Algorithm: Natural Gradient Policy on L
Throughout §5 and §6, we assume that condition (a1) in our assumption holds, i.e., R1 + B1⊺X∗B1 ≻ 0, −R2 + B2⊺X∗B2 + B2⊺X∗B1(R1 + B1⊺X∗B1)−1B1⊺X∗B2 ≺ 0,
and an oracle OK that returns the stabilizing minimizer f (K, L) for any ﬁxed L, provided that such minimizer exists. Note that the unique minimizer corresponds to the maximal solution X+ to the algebraic Riccati equation (with ﬁxed L), namely,
(A − B2L)⊺X(A − B2L) − X + Q − L⊺R2L − (A − B2L)⊺XB1(R + B1⊺XB1)−1B1⊺X(A − B2L) = 0.
We shall subsequently discuss how to construct this oracle by policy gradient based algorithms in §7.
5.1 Algorithm
The algorithm is given by:
Algorithm 1 Natural Gradient Policy for LQ Game 1: Initialize L0 such that (A − B1L0, B2) is stabilizable and the DARE (A − B2L0)⊺X(A − B2L0) − X + Q − L⊺0R2L0 − (A − B2L0)⊺XB1(R1 + B1⊺XB1)B1⊺X(A − B2L0) = 0
has a stabilizing solution X+ with R1 + B1⊺X+B1 ≻ 0. 2: if j ≥ 1 then 3: Set: Kj−1 ← arg minK f (K, Lj−1). 4: Set: Lj = Lj−1 + ηj Ng(Lj ) ≡ Lj−1 + ηj Nf,L(Kj−1, Lj−1). 5: end if
We note that the initialization step is generally nontrivial. However, if we further assume that (A, B1) is stabilizable, Q ⪰ 0 and those eigenvalues of A lying on the unit disk are (Q, A)-detectable,

12

5 ALGORITHM: NATURAL GRADIENT POLICY ON L

5.2 Convergence Analysis

then we can choose L0 = 09. For the general case, we may need to check invariant subspaces of system parameters (see [LR95] for details.)

5.2 Convergence Analysis

To simplify the notation let,

UK,L = R1K − B⊺XK,LAK,L, VK,L = −R2L − B⊺XK,LAK,L;

namely 2UK,L = Nf,K(K, L) and 2VK,L = Nf,L(K, L). First a useful observation.

Lemma 5.1 (NG Comparison Lemma). Suppose that (K, L) and (Kˆ , Lˆ) are both stabilizing and let X and Xˆ be the corresponding value matrices. Then

a.

X

−

Xˆ

=

A⊺ˆ ˆ (X
K,L

−

Xˆ )AKˆ ,Lˆ

+

(K

−

Kˆ )⊺UK,L

+

U⊺K,L(K

−

Kˆ )

−

(K

−

Kˆ )⊺R1(K

−

Kˆ )

+(L − Lˆ)⊺VK,L + VK⊺ ,L(L − Lˆ) + (L − Lˆ)⊺R2(L − Lˆ) − (AK,L − AKˆ ,Lˆ )⊺X(AK,L − AKˆ ,Lˆ ).

b.

X

−

Xˆ

=

A⊺K,L(X

−

Xˆ )AK,L

+

(K

−

Kˆ )⊺UKˆ ,Lˆ

+

U⊺ˆ ˆ (K
K,L

−

Kˆ )

+

(K

−

Kˆ )⊺R1(K

−

Kˆ )

+(L

−

Lˆ)⊺VKˆ ,Lˆ

+

V⊺ˆ ˆ (L
K,L

−

Lˆ)

−

(L

−

Lˆ)⊺R2(L

−

Lˆ)

+

(AK,L

−

AKˆ ,Lˆ )⊺Xˆ

(AK,L

−

AKˆ ,Lˆ ).

Remark 5.2. Item (b) of this lemma was observed in [ZYB19]. Our presentation oﬀers a control theoretic perspective on its proof.

Proof. We prove item (a); item (b) can be proved in a similar manner. It suﬃces to take the diﬀerence of the Lyapunov equations:

A⊺K,LXAK,L + Q + K⊺R1K − L⊺R2L = X, A⊺Kˆ ,Lˆ Xˆ AKˆ ,Lˆ + Q + Kˆ ⊺R1Kˆ − Lˆ⊺R2Lˆ = Xˆ .

Indeed, a few algebraic operations reveal that

X

−

Xˆ

=

A⊺ˆ ˆ (X
K,L

−

Xˆ )AKˆ ,Lˆ

+

(K

−

Kˆ )⊺UK,L

+

U⊺K,L(K

−

Kˆ )

−

(K

−

Kˆ )⊺(R1)(K

−

Kˆ )

+ (L − Lˆ)⊺VK,L + VK⊺ ,L(L − Lˆ) + (L − Lˆ)⊺R2(L − Lˆ) − (AK,L − AKˆ ,Lˆ )⊺X(AK,L − AKˆ ,Lˆ).

We now observe another version of comparison lemma when L, L˜ ∈ dom(g). Indeed, this lemma will play a more prominent role in our convergence analysis.

Lemma 5.3 (Comparison Lemma 2). Suppose that L, L˜ ∈ dom(g), namely there exists K, K˜ such that

K = arg min f (K′, L),
K ′ ∈SL

K˜ = arg min f (K′, L˜).
K ′ ∈SL˜

9This is indeed the standard assumption in the LQ literature.

13

5 ALGORITHM: NATURAL GRADIENT POLICY ON L

5.2 Convergence Analysis

Further, suppose that the algebraic Riccati map RA−B2L,B1,Q−L⊺R2L,R1(X˜ ) is well-deﬁned, i.e., R1 + B1⊺X˜ B1 is invertible. Recall that the Riccati map is given by,
RA−B2L,B1,Q−L⊺R2L,R1(X˜ ) = (A − B2L)⊺X˜ (A − B2L) − X˜ + Q − L⊺R2L − (A − B2L)⊺X˜ B1(R1 + B1⊺X˜ B1)−1B1⊺X˜ (A − B2L).

Let X and X˜ be the corresponding value matrix. Putting E := R1 + B1⊺X˜ B1, F := B1⊺X˜ (A − B2L),

then X − X˜ = A⊺K,L(X − X˜ )AK,L + RA−B2L,B1,Q−L⊺R2L,R1 (X˜ ) + (EK − F)⊺E−1(EK − F).

Moreover,

RA−B2 L,B1 ,Q−L⊺ R2 L,R1

=

(L

−

L˜)⊺VK˜ ,L˜

+

V⊺˜ ˜ (L
K,L

−

L˜)

−

(L

−

L˜)⊺OX˜ (L

−

L˜),

where

OX˜ := R2 − B2⊺X˜ B2 + B2⊺X˜ B1(R1 + B1⊺X˜ B1)−1B1⊺X˜ B2.

Proof. Note that X solves the Lyapunov matrix equation,

X = (A − B1K − B2L)⊺X(A − B1K − B2L) + Q − L⊺R2L + K⊺R1K,

with K = (R1 + B1⊺XB1)−1B1⊺X(A − B2L). Then
X − X˜ − A⊺K,L(X − X˜ )AK,L = A⊺K,LX˜ AK,L − X˜ + Q − L⊺R2L + K⊺R1K = (A − B2L)⊺X˜ (A − B2L) − X˜ + Q − L⊺R2L + K⊺(R1 + B1⊺X˜ B1)K − K⊺B1⊺X˜ (A − B2L) − (A − B2L)⊺X˜ B1K = RA−B2L,B1,Q−L⊺R2L,R1 (X˜ ) + (A − B2L)⊺X˜ B1(R1 + B1⊺X˜ B1)−1B1⊺X˜ (A − B2L) + K⊺(R1 + B1⊺X˜ B1)K − K⊺B1⊺X˜ (A − B2L) − (A − B2L)⊺X˜ B1K = RA−B2L,B1,Q−L⊺R2L,R1 (X˜ ) + (EK − F)⊺E−1(EK − F).

Since X˜ satisﬁes the algebraic Riccati equation (A − B2L˜)⊺X˜ (A − B2L˜) + Q − L˜⊺R2L˜ − (A − B2L˜)⊺X˜ B1(R1 + B1⊺X˜ B1)−1B1⊺X˜ (A − B2L˜) = X˜ ,

it follows that,
RA−B2L,B1,Q−L⊺R2L,R1(X˜ ) = (A − B2L)⊺X˜ (A − B2L) − L⊺R2L − (A − B2L)⊺X˜ B1(R1 + B1⊺X˜ B1)−1B1⊺X˜ (A − B2L) − (A − B2L˜)⊺X˜ (A − B2L˜) + L˜⊺R2L˜ + (A − B2L˜)⊺X˜ B1(R1 + B1⊺X˜ B1)−1B1⊺X˜ (A − B2L˜)
= (L − L˜)⊺(−R2L˜ − B2⊺X˜ AK˜ ,L˜ ) + (−R2L˜ − B2⊺X˜ AX˜,L˜ )⊺(L − L˜) − (L − L˜)⊺(R2 − B2⊺X˜ B2 + B2⊺X˜ B1(R1 + B1⊺X˜ B1)−1B1⊺X˜ B2)(L − L˜).

14

5 ALGORITHM: NATURAL GRADIENT POLICY ON L

5.2 Convergence Analysis

Let us now prove the convergence of the proposed algorithm. In the following analysis, Kj’s are exclusively used as the unique stabilizing minimizers,10 for f (K, Lj ) over K.11 To simplify the
notation, let

∆ := XKj−1,Lj−1 , Oj−1 := R2 − B2⊺XKj−1,Lj−1 B2 + B2⊺XKj−1,Lj−1 B1(R1 + B1⊺XKj−1,Lj−1 B1)−1B1⊺XKj−1,Lj−1 B2.

We shall ﬁrst show that if Algorithm 1 is initialized appropriately, then with stepsize

ηj−1 < 1 , λn(Oj−1)

Algorithm 1 generates a sequence {Lj} satisfying properties listed in the following lemma.
Lemma 5.4. Suppose that Algorithm 1 is initialized appropriately. With stepsize ηj−1 ≤ λn(O1j−1) , we then have,

a. (A − B2Lj, B1) is stabilizable for every j ≥ 1.

b. Oj ≻ 0 for every j ≥ 1.

c. For every j ≥ 1, f (K, Lj ) is bounded below over K and there exists a unique minimizer Kj, which forms a stabilizing pair (Kj, Lj ). Namely, the DARE

0 = (A − B2Lj)⊺X(A − B2Lj) − X

(6)

+ Q − L⊺R2Lj − (A − B2Lj)⊺XB1(R1 + B⊺XB1)−1B⊺X(A − B2Lj ),

j

1

1

admits a stabilizing maximal solution X+ satisfying R + B1⊺X+B1 ≻ 0. d. Putting Λ = XKj,Lj , Ej = R1 + B1⊺XKj,Lj B1 and Fj = B1⊺XKj,Lj (A − B2Lj) we have

Λ − ∆ = A⊺Kj ,Lj (Λ − ∆)AKj,Lj + VK⊺ j−1,Lj−1 4ηj−1I − 4ηj2Oj−1 + (Ej−1Kj − Fj−1)−1E−j−11(Ej−1Kj − Fj−1).

VKj −1 ,Lj −1

Proof. It suﬃces to prove the lemma by induction since all items holds at j = 0 (by initialization of the algorithm). We shall ﬁrst suppose that (A − B2Lj, B1) is stabilizable, i.e., (a) holds. Note that this property is not automatically guaranteed and we subsequently provide an analysis to carefully remove this assumption.12 First note that by our assumption, ∆ is the maximal stabilizing solution of the DARE,13
0 = (A − B2Lj−1)⊺X(A − B2Lj−1) − X
+ Q − L⊺j−1R2Lj−1 − (A − B2Lj−1)⊺XB1(R1 + B1⊺XB1)−1B1⊺X(A − B2Lj−1),
10Depending on the structure of our problem, it is possible that there exists non-stabilizing minimizers. But here we are only concerned with minimizers in the set S.
11Of course, we need to guarantee that for Lj, there exists a unique minimizer. 12A side remark on our proof strategy: in linear system theory, a number of synthesis results are developed under the assumption of stabilizability of the system. We will utilize these observations here; however, to use those tools, we must assume that (A − B2Lj , B1) is stabilizable. But this is also one of our goals to show. The reader might recognize certain circular line of reasoning here. Indeed, one of our contributions is pseudo trick devised to circumvent this issue: we ﬁrst assume stabilizability and then use the results developed to arrive at a contradiction if the system had not been stabilizable. 13This can be considered as an LQR problem for system (A − B2Lj−1, B1) with state cost matrix Q − L⊺j−1R2Lj−1.

15

5 ALGORITHM: NATURAL GRADIENT POLICY ON L

5.2 Convergence Analysis

and Kj−1 = (R1 + B1⊺∆B1)−1B1⊺∆(A − B2Lj−1). Now adopt the update rule,
Lj = Lj−1 − ηj−1Ng (Lj−1) = Lj−1 − 2ηj−1VKj−1,Lj−1 .
by Lemma 5.3 it follows
(7) RA−B2Lj ,B1,Q−L⊺j R2Lj,R1 (Xj−1) = VK⊺ j−1,Lj−1 4ηj−1I − 4ηj2−1Oj−1) VKj−1,Lj−1 .
Thereby with the stepsize ηj−1 ≤ λn(1O) , we have
(8) RA−B2Lj,B1,Q−L⊺j R2Lj,R1 (Xj−1) = VKj−1,Lj−1 4ηj−1I − 4ηj2−1Oj−1 VKj−1,Lj−1 ⪰ 0.
By Theorem 2.3 (note that the inequality (8) is crucial for applying the theorem), there exists a maximal solution X+ ⪰ ∆ to the DARE (6) and moreover, with
K+ = (R1 + B1⊺X+B1)−1B1⊺X+(A − B2Lj ),
the eigenvalues of A − B2Lj − B1K+ are in the closed unit disk of C. Equivalently, X+ solves the following Lyapunov equation,
(A − B1K+ − B2Lj)⊺X+(A − B1K+ − B2Lj ) + Q + (K+)⊺R1K+ − L⊺j R2Lj = X+. Item (d) thereby follows from the ﬁrst part of Lemma 5.3. We now observe that K+ is indeed stabilizing. Suppose that this is not the case; then there exists v ∈ Cn such that (A−B2Lj −B1K+)v = λv with λ = 1. Hence,

v⊺ A⊺K+,Lj (X+ − ∆)AK+,Lj v + v⊺ VK⊺ j−1,Lj−1 4ηj−1I − 4ηj2Oj−1 VKj−1,Lj−1 v ≤ v⊺(X+ − ∆)v.

This would imply that VKj−1,Lj−1 v = 0. But this means that,

(9)

Ljv = Lj−1v.

By Lemma 5.1, we have

A⊺K+,Lj (X+ − ∆)AK+,Lj − (X+ − ∆) + VK⊺ j−1,Lj−1 4ηj−1I − 4ηj2R2 VKj−1,Lj−1 +(K+ − Kj−1)⊺R1(K+ − Kj−1) + (AK+,Lj − AKj−1,Lj−1 )⊺∆(AK+,Lj − AKj−1,Lj−1 ) = 0.
Multiplying v⊺ and v on each side and combining the resulting expression with (9), we obtain,

Kj−1v = K+v.

But now we have

(A − B1Kj−1 − B2Lj−1)v = Av − B1K+v − B2Lj v = λv.

This is a contradiction to the Schur stability of (Kj−1, Lj−1). For item (b), note that Xj ⪯ X∗, so R2 − B2⊺Xj B2 ≻ 0 and consequently Oj ≻ 0 as R1 + B1⊺Xj B1 ⪰ R1 + B1⊺Xj−1B1 ≻ 0. Hence, we have completed the proof for items (b), (c), (d) under the assumption of item (a).

16

5 ALGORITHM: NATURAL GRADIENT POLICY ON L

5.2 Convergence Analysis

We now argue that with the stepsize ηj−1 ≤ 1 λn(Oj−1), this assumption of stabilizability is indeed valid; namely, item (a) holds. Consider the ray {Lt ∶ Lt = Lj−1 + t2VKj−1,Lj−1 }. We ﬁrst note
that there exists a maximal half-open interval [0, σ) such that (A−B2Lt, B1) is stabilizable for every t ∈ [0, σ) and (A − B2Lσ, B1) is not stabilizable (this is due to the fact that stabilizability is an open condition; see Proposition C.1 for a proof). Now suppose that σ < λn(O1j−1) . We may take a sequence
tl ↑ σ, and note that (A − B2Ltl , B1) is stabilizable for every tl. Let us denote the corresponding sequence of solutions to the DARE by {Ztl }. By our previous arguments, ∆ ⪯ Ztl ⪯ X∗, where
X∗ is the corresponding value matrix at Nash equilibrium point (K∗, L∗). Denote by L as the set of all limit points of the sequence {Ztl }∞ l=1. By Weirestrass-Balzano, the set L is nonempty as the sequence is bounded. Clearly, for every Z ∈ L, ∆ ⪯ Z ⪯ X∗. By continuity, Z must solve the DARE,

(10) (A − B2Lσ)⊺X(A − B2Lσ) + Q − L⊺σR2Lσ − (A − B2Lσ)⊺XB1(R1 + B1⊺XB1)−1B1⊺X(A − B2Lσ) = X.
Putting K′ = (R1 + B1⊺ZB1)−1B1⊺Z(A − B2Lj), we claim that A − B2Lσ − B1K′ is Schur stable. This is a consequence of (d) and the Comparison Lemma 5.1: it suﬃces to observe that AK′,Lσ is marginally stable satisfying (d) and,

A⊺K′,Lσ (Z − ∆)AK′,Lσ − (Z − ∆) + VK⊺ j−1,Lj−1 4σI − 4σ2Oj−1 VKj−1,Lj−1 ⪯ 0.
Proceeding similar to the above line of reasoning, we can show that A−B2Lσ −B1K′ is Schur stable. But this contradicts our standing assumption that (A − B2Lσ, B1) is not stabilizable. Hence, for all ηj−1 ≤ 1 λn(Oj−1), the pair (A − B2Lj, B1) is indeed stabilizable.

We are now ready to state the convergence rate for the algorithm.

Theorem 5.5. If the stepsize is taken as ηj = 1 (2λn(Oj−1)), then,

∞ Ng(Lj ) 2 ≤ 1 g(L∗) − g(L0) ,

j=0

Fη

where η ∈ R+ is some positive constant.

Remark 5.6. This theorem suggests the gradient will vanish at a sublinear rate. As we know, there is a unique stationary point of g; this means the sublinear convergence to global maximum, i.e., Nash equilibrium point.

Proof. Let η = infj 1 λn(Oj) and note that Oj ⪰ R2 − B2⊺X∗B2, so η > 0. It suﬃces to note that by Lemma 5.4, we have

g(Lj ) − g(Lj−1) = Tr((XKj,Lj − XKj−1,Lj−1 )Σ)

≥ 1 Tr YK ,L Ng(Lj−1)⊺Ng(Lj−1)

λn(Oj−1)

jj

≥η

Ng (Lj −1 )

2 F

.

Telescoping the sum and noting that g(L) is bounded above by g(L∗), we have

∞ Ng(Lj) 2 ≤ 1 g(L∗) − g(L0) < ∞.

j=0

Fη

17

6 ALGORITHM: QUASI-NEWTON ITERATIONS OF L

We observe that the convergence rate is asymptotically linear. This is a consequence of the local curvature of g(L). Indeed, if we compute the Hessian at g(L∗), the action of the Hessian (see Appendix A for details) is given by
∇2g(L∗)[E, E] = −2⟨OX∗ E, EY∗⟩. As ∇2g(L∗) is negative deﬁnite, −g(L) is locally strongly convex around a convex neighborhood of L∗. It thus follows that gradient descent enjoys a linear convergence rate around L∗.
6 Algorithm: quasi-Newton Iterations of L
In this section, we shall assume that the oracle OL returns the quasi-Newton direction. The motivation of quasi-Newton is to investigate the second-order local approximation of g(L). Indeed, we may observe that,
g(L + ∆L) ≈ g(L) + 2⟨YL+∆LNg(L), ∆L⟩ − ⟨OL∆L, ∆L⟩.

Algorithm 2 quasi-Newton Policy for LQ Game 1: Initialize (K0, L0) ∈ S such that (Q − L⊺0R2L0, A − B2L0) is detectable and the DARE

(A − B2L0)⊺X(A − B2L0) − X + Q − L⊺0R2L0 + (A − B2L0)⊺XB1(R1 + B1⊺XB1)B1⊺X(A − B2L0)

is solvable in SL0 ≡ {K ∈ Rm1×n ∶ A − B2L0 − B1K is Schur}. 2: if j ≥ 1 then

3: Set: Kj ← arg minK f (K, Lj−1).

4:

Set: Lj = Lj−1 + ηj−1O−j−112(−R2Lj−1 − B2⊺XKj−1,Lj−1 AKj−1,Lj−1 ).

5: end if

6.1 Convergence Analysis

We ﬁrst prove a result that can be considered as a counterpart to Lemma 5.4. Lemma 6.1. Suppose that Algorithm 2 is initialized appropriately. With stepsize ηj−1 ≤ λn(O1j−1) , we then have,
a. (A − B2Lj, B1) is stabilizable for every j ≥ 1.
b. Oj ≻ 0 for every j ≥ 1.
c. For every j ≥ 1, f (K, Lj ) is bounded below over K and there exists a unique minimizer Kj, which forms a stabilizing pair (Kj, Lj ). Namely, the DARE
(A − B2Lj)⊺X(A − B2Lj) + Q − L⊺j R2Lj − (A − B2Lj)⊺XB1(R1 + B1⊺XB1)−1B1⊺X(A − B2Lj ) = X, admits a stabilizing maximal solution X+ satisfying R + B1⊺X+B1 ≻ 0. d. Putting Λ = XKj,Lj , Ej = R1 + B1⊺XKj,Lj B1 and Fj = B1⊺XKj,Lj (A − B2Lj) we have

Λ − ∆ = A⊺Kj,Lj (Λ − ∆)AKj ,Lj + VK⊺ j−1,Lj−1 4ηj−1O−j−11 − 4ηj2O−j−11 + (Ej−1Kj − Fj−1)⊺E−j−11(Ej−1Kj − Fj−1).

VKj −1 ,Lj −1

18

6 ALGORITHM: QUASI-NEWTON ITERATIONS OF L

6.1 Convergence Analysis

Proof. The proof proceeds similar to Lemma 5.4. The key diﬀerence is that the algebraic Riccati map assumes a new form with the quasi-Newton update. Namely, with quasi-Newton iteration,

RA−B2Lj ,B1,Q−L⊺j R2Lj,R1 (Xj−1) = 4ηj−1I − 4ηj2−1 VK⊺ j−1,Lj−1 O−j−11VKj−1,Lj−1 . The statements then follows from almost same arguments as in Lemma 5.4.

We are now ready to state the convergence rate for the algorithm. Theorem 6.2. If the stepsize is taken as η = 1 2, then
g(L∗) − g(Lj) ≤ q(g(L∗) − g(Lj−1))2, for some q > 0.

Proof. By Lemma 6.1, the sequence of value matrices {Xj} is monotonically nondecreasing and bounded above. Thereby Xj → X∗ as j → ∞. It follows the set E = {Xj} ∪ {X∗} is compact. Substituting Kj−1 = (R1 + B1⊺Xj−1B1)−1B1⊺Xj−1(A − B2Lj−1) into the update rule, we get
Lj = −O−j−11B2−1Xj−1(A − B1(R1 + B1⊺Xj−1B1)−1B1⊺Xj−1A).
By Lemma 5.3 (take X˜ = X∗ and X = Xj and note VK∗,L∗ = 0),
∞
X∗ − Xj ⪯ (A⊺j )ν (L∗ − Lj )⊺O∗(L∗ − Lj ) Aνj ,
ν=0
where
O∗ = R2 − B2⊺X∗B2 + B2⊺X∗B1(R1 + B1⊺X∗B1)−1B1⊺X∗B2.

It follows that,

g(L∗) − g(Lj ) ≤ Tr(Y∗(L∗ − Lj)⊺O∗(L∗ − Lj)) ≤ λn(Y∗)λn(O∗) Tr((L∗ − Lj)⊺(L∗ − Lj)).

We observe L∗ − Lj = −O−∗1B2⊺X∗(A − B1(R1 + B1⊺X∗B1)−1B1⊺X∗A) + O−j−11B2⊺Xj−1(A − B1(R1 + B1⊺Xj−1B1)−1B1⊺Xj−1A),

and further, note that the map φ given by

X ↦ −O−X1B2⊺X A − B1(R1 + B1⊺XB1)−1B1⊺XA

is smooth where,

OX = R2 − B2⊺XB2 + B2⊺XB1(R1 + B1⊺XB1)−1B1⊺XB2.

So over the compact set E, we can ﬁnd a Lipschitz constant β of φ, namely, for every X, X′ ∈ E, we have φ(X) − φ(X′) F ≤ β X − X′ F . Then

L∗ − Lj

2 F

=

φ(X∗) − φ(Xj)

2 F

≤ β2

X∗ − Xj−1

2 F

.

Hence

g(L∗) − g(Lj ) ≤ c X∗ − Xj−1

2 F

≤q

2
g(L∗) − g(Lj−1) ,

where c, q > 0 are constants.

19

7 POLICY GRADIENT ALGORITHMS FOR SOLVING K ← ARG MINK F (K, L)

7 Policy Gradient Algorithms for Solving K ← arg minK f (K, L)

In this section, we describe how we can use policy gradient based oracles to solve the minimization problem for ﬁxed L. If the iterates Q − Lj⊺R2Lj were positive deﬁnite, policy based updates, e.g., gradient descent, natural gradient descent and quasi-Newton iterations for standard LQR problem as treated in [FGKM18] [BMFM19], could be adopted. Then the oracle OK can be constructed by repeatedly performing the procedure to the desired precision. However, there are no guarantees that this condition would be valid in the LQ dynamic game setup. We shall prove that with the assumption that there exists a maximal stabilizing solution to the algebraic Riccati equation, gradient descent (respectively, natural gradient descent and quasi-Newton) converges to the maximal solution of the ARE at a linear (respectively, linear and quadratic) rate. In this direction, recall that the gradient, natural gradient and quasi-Newton directions for ﬁxed L are given by,

∇Kf (K, L) = 2(R1K − B1⊺XAK,L)Y =: g(K), Nf,K (K, L) = 2(R1K − B1⊺XAK,L) =: n(K), qNf,K (K, L) = 2(R1 + B1⊺XB1)−1(R1K − B1⊺XAK,L) =: qn(K).

We ﬁrst provide the convergence analysis for the case of natural gradient descent.

Theorem 7.1 (Natural Gradient Analysis). Suppose that with ﬁxed L, the ARE

(A − B2L)⊺X(A − B2L) − X + (A − B2L)⊺XB1(R1 + B1⊺XB1)−1B1⊺X(A − B2L) + Q − L⊺R2L = 0,

has a maximal stabilizing solution X+. Then the update rule,

Mi+1 = Mi −

1

2(R1Mi − B1⊺Xi(A − B2L − B1Mi)),

2λn(R1 + B1⊺XiB1)

where Xi solves the Lyapunov equation (A − B2L − B1Mi)⊺Xi + Xi(A − B2Lj − B1Mi) + Q − L⊺R2L + Ki⊺R1Ki = 0,
converges linearly to K+ provided M0 ∈ SL. That is,

for some q ∈ (0, 1).

Mi − M∗

2 F

≤ qi

M0 − M∗

2 F

,

Remark 7.2. The diﬀerence between above theorem and the standard results treated in [FGKM18,
BMFM19] is that we are not assuming Q to be positive deﬁnite. Mind that in the above problem, the matrix Q−L⊺j R2Lj corresponding to the penalization on states in the standard LQR, can be indeﬁnite in our sequential algorithms. The one step progression would follow from Theorem in [BMFM19].
However, the important diﬀerence is that the cost function is no longer coercive, requiring a separate analysis for establishing the stability of the iterates.14

Proof. The analysis in [BMFM19] on the one-step progression of the natural gradient descent holds here and thus the convergence rate would remain the same if we can prove that the iterates remain stabilizing.
14We note that stability of the iterates in natural gradient update was not explicitly shown in [FGKM18]. Some of the perturbation arguments for gradient descent in [FGKM18] can however be applied to argue for this property. Such an argument would however rely on the strict positivity of the minimum eigenvalue of Q.

20

7 POLICY GRADIENT ALGORITHMS FOR SOLVING K ← ARG MINK F (K, L)

By induction, it suﬃces to argue that with the chosen stepsize, Mi is stabilizing provided that Mi−1 is. Consider the ray {Mt = Mi−1 −tn(Mi−1) ∶ t ≥ 0}. Note that by openness of S and continuity of eigenvalues, there is a maximal interval [0, ζ) such that Mi−1 + tn(Mi−1) is stabilizing for t ∈ [0, ζ) and Mi−1 + ζn(Mi−1) is marginally stabilizing. Now suppose that ζ ≤ 2λn(R1+B11⊺Xi−1B1) and take a sequence tl ∈ [0, ζ) such that tl → ζ. Consider the sequence of value matrices {Xtl } and denote by L as the set of all limit points of {Xtj }. Note that L is nonempty since the sequence is bounded by X+ ⪯ Xtl ⪯ Xi−1 for every tl.15 By continuity, any Z ∈ L solves,
(A − B1Mζ − B2L)⊺Z(A − B1Mζ − B2L) − Z + Q − L⊺R2L + Mζ⊺R1Mζ = 0.

But this is a contradiction, since by the Comparison Lemma 5.1, we have

(A − B1Mζ − B2L)⊺(Z − X+)(A − B1Mζ − B2L) − (Z − X+) + (Mζ − K+)⊺R1(Mζ − K+) = 0.

Suppose that (λ, v) is the eigenvalue-eigenvector pair of A − B1Mζ − B2L1 such that (A − B1Mζ − B2Lj)v = λv and λ = 1. Then as Z − X+ ⪰ 0, we would have Mζ v = K+v. But this is a contradiction to the assumption that K+ is a stabilizing solution. Hence {Xi} is a monotonically non-increasing sequence bounded below by X+. As such, the sequence of iterates {Mi} would converge linearly to K+ following the arguments in [BMFM19].

We mention that the above stability argument can be applied for the sequence generated by the quasi-Newton iteration as well. The quadratic convergence rate for such a sequence would then follow from the proof in [BMFM19].

Theorem 7.3 (Quasi-Newton Analysis). Suppose that with a ﬁxed L, the ARE

(A − B2L)⊺X(A − B2L) − X + (A − B2L)⊺XB1(R1 + B1⊺XB1)−1B1⊺X(A − B2L) + Q − L⊺R2L = 0,

has a maximal stabilizing solution X+. Then the update rule Mi+1 = Mi − 1 2(R1 + B1⊺XiB1)−1(R1Mi − B1⊺Xi(A − B2L − B1Mi)), 2
where Xi solves the Lyapunov equation (A − B2Lj − B1Mi)⊺Xi + Xi(A − B2Lj − B1Mi) + Q − L⊺j R2Lj + Ki⊺R1Ki = 0,
converges quadratically to K+ provided M0 ∈ S. That is,

for some q > 0.

Mi − M∗

F

≤q

M0 − M∗

2 F

,

The gradient policy analysis requires more work since the stepsize developed in [BMFM19] involves the smallest eigenvalue λ1(Q). However by carefully replacing “λ1(Q) related quantities” in [BMFM19], one can still prove the global linear convergence rate as follows.
15Note that it is not guaranteed that Xtj is convergent. The limit points are also not necessarily well-ordered in the ordering induced by the p.s.d. cone.

21

9 SWITCHING THE LEADER IN THE SEQUENTIAL ALGORITHMS

Theorem 7.4 (Gradient Analysis). Suppose that with a ﬁxed L, the ARE

(A − B2L)⊺X(A − B2L) − X + (A − B2L)⊺XB1(R1 + B1⊺XB1)−1B1⊺X(A − B2L) + Q − L⊺R2L = 0,

has a maximal stabilizing solution X+. Then the update rule

Mi+1 = Mi − ηi2(R1Mi − B1⊺Xi(A − B2L − B1Mi))YMi,L,

where Xi solves the Lyapunov equation

(A − B2Lj − B1Mi)⊺Xi + Xi(A − B2Lj − B1Mi) + Q − L⊺j R2Lj + Ki⊺R1Ki = 0, converges linearly to K+ provided M0 ∈ S. That is,

Mi − M∗

2 F

≤ qi

M0 − M∗

2 F

,

for some q ∈ (0, 1).

The convergence analysis of gradient policy follows closely of the idea presented in [BMFM19].
In [BMFM19], the compactness of sublevel sets was used to devise the stepsize rule to guarantee a
suﬃcient decrease in the cost and stability of the iterates. The proof of compactness in [BMFM19] however, relies on the positive deﬁniteness of Q − L⊺j R2Lj.16 It is also not valid to assume that the function is coercive. But, we can show that an analogous strategy adopted in [BMFM19] can
be employed to derive a suitable stepsize for the game setup. The details analysis are defered to
Appendix B.

8 Comments on Adopting Gradient Polices for L
Gradient policy update for LQ games has been discussed in [ZYB19], where a projection step is required in updating the policy L. In particular, in [ZYB19], it has been stated that a projection step onto the set Ω = {L ∶ Q − L⊺R2L ⪰ 0} would guarantee that L is stabilizing. The key issue however is the stabilizability of (A − B2L, B1); as such, it is not valid to assume that every L ∈ Ω would yield a stabilizable pair (A − B2L, B1). In fact, the approach adopted in our work would not work for gradient policy either, as we rely on a monotonicity property of the corresponding value matrix; gradient policy would only decrease the cost function without any guarantees to decrease the value matrix (with respect to the p.s.d. ordering). If we assume that the Nash equilibrium L∗ ∈ Ω and could guarantee that (A − B2Lj, B1) is stabilizable, then it would be warranted that f (K, Lj) has a unique minimizer for every Lj; in this case, the approach adopted in this paper would provide a simpler proof for convergence of gradient policies for LQ games.

9 Switching the Leader in the Sequential Algorithms
We shall demonstrate in this section if the condition a2 in the assumption holds, it might not be guaranteed that g(L) is diﬀerentiable in a neighborhood of L∗. In this case, however, choosing the leader to be player K would converge. The analysis would proceed in a similar manner. First, we observe that we can deﬁne a value function,
h(K) = sup f (K, L).
L∈SK
Following virtually the same argument, we can the establish the following.
16Or the observability of (Q − L⊺j R2Lj , A).

22

9 SWITCHING THE LEADER IN THE SEQUENTIAL ALGORITHMS
Proposition 9.1. Suppose that U ⊆ dom(h) is an open set such that for every K ∈ U, there is a unique maximizer of f (K, L) over L. Then h(L) is diﬀerentiable and the gradient is given by
∇h(K) = ∇K f (K, LK ), where LK = arg max f (K, L).
L∈SK
The algorithm for player K using natural gradient policy can be described similarly to Algorithm 1.
Algorithm 3 Natural Gradient Policy for LQ Game 1: Initialize K0 such that (A − B1K0, B2) is stabilizable and the DARE (A − B1K0)Z(A − B1K0) − Q − K0⊺R1K0 − (A − B1K0)⊺ZB2(R2 + B2⊺ZB2)−1B2⊺Z(A − B1K0) = Z.
has a maximal symmetric solution Z+ with R2 + B2⊺Z+B2 ≻ 0. 2: if j ≥ 1 then 3: Set: Lj−1 ← arg maxL f (Kj−1, L). 4: Set: Kj = Kj−1 − ηj Nh(Lj ) ≡ Kj−1 − ηjNf,K (Kj−1, Lj−1). 5: end if
We observe that for ﬁxed K′, if L′ is the unique stabilizing maximizer of f (K′, L) over L, then substituting ∇Lf (K′, L′) = 0 into the Lyapunov matrix equation, L′ solves the following ARE,
(11) (A − B1K′)Z(A − B1K′) + Q + K′⊺R1K′ − (A − B1K′)⊺ZB2(−R2 + B2⊺ZB2)−1B2⊺Z(A − B1K′) = Z.
To utilize the theory developed in standard ARE, which concerns a minimization problem, we may consider following modiﬁcation:
(12) (A − B1K′)W (A − B1K′) − Q − K′⊺R1K′ − (A − B1K′)⊺W B2(R2 + B2⊺W B2)−1B2⊺W (A − B1K′) = W.
We observe that if W solves (12), then −W solves (11). Now the analysis can be done almost in parallel. Lemma 9.2. Suppose that Lj−1 is the unique stabilizing maximizer of f (Kj−1, L), i.e., Lj−1 = arg minL f (Kj−1, L). Putting ∆ = XKj−1,Lj−1 and
Oj−1 = R1 + B1⊺∆B1 + B1⊺∆B2(R2 − B2⊺∆B2)−1B2⊺∆B1,
with stepsize ηj−1 ≤ λn(O1j−1) , we then have, a. (A − B1Kj, B2) is stabilizable. b. f (Kj, L) is bounded above over L and there exists a unique stabilizing maximizer Lj, namely (Kj, Lj) is a stabilizing pair. c. Putting Λ = XKj,Lj , we have
∆ − Λ ⪰ A⊺Kj,Lj (∆ − Λ)AKj ,Lj + U⊺Kj−1,Lj−1 −4ηj−1I + 4ηj2Oj−1 UKj−1,Lj−1 .
23

10 CONCLUDING REMARKS
Proof. The proof proceeds similarly to Lemma 5.4. Indeed, putting ∆˜ = −∆ and Λ˜ = −Λ, we observe ∆˜ and Λ˜ solves the DARE (12) and also note the DARE (12) has the same form with the DARE considered in Lemma 5.4. Further, note that the update rule is equivalent to
Kj = Kj−1 − ηj−12(R1Kj−1 − B1⊺∆AKj−1,Lj−1 ) = Kj−1 − ηj−12(R1Kj−1 + B1⊺∆˜ AKj−1,Lj−1 ) = Kj−1 + 2ηj−1(−R1Kj−1 − B1⊺∆˜ AKj−1,Lj−1 ).
In view of these observations, by the same machinery we employed in Lemma 5.4, we conclude Λ˜ − ∆˜ ⪰ A⊺Kj,Lj (Λ˜ − ∆˜ )AKj ,Lj + U˜ ⊺Kj−1,Lj−1 4ηj−1I − 4ηj2O˜ j−1 U˜ Kj−1,Lj−1 ,
where U˜ Kj−1,Lj−1 = −R1Kj−1 − B⊺∆˜ AKj−1,Lj−1 , O˜ j−1 = R1 − B1⊺∆˜ B1 + B1⊺∆˜ B2(R2 + B2⊺∆˜ B2)−1B2⊺∆˜ B1.
It thus follows that, −Λ + ∆ ⪰ A⊺Kj,Lj (∆ − Λ)AKj ,Lj + U⊺Kj−1,Lj−1 −4ηj−1I + 4ηj2Oj−1 UKj−1,Lj−1 .
Now it is straightforward to conclude the sublinear convergence rate of Algorithm 3. Lemma 9.3. Suppose {Kj} are the iterates generated by Algorithm 3. Then we have
∞
Nh(Kj) ≤ η h(K0) − h(K∗) ,
j=0
where η > 0 is some positive number. The analysis of quasi-Newton method with K as the leader proceeds in a similar manner as
Algorithm 2; as such we omit the details here.
10 Concluding Remarks
The papers considers sequential policy-based algorithms for LQ dynamic games. We prove global convergence of several mixed-policy algorithms as well as identifying the role of control theoretic constructs in their analysis. Moreover, we have clariﬁed a number of intricate issues pertaining to stabilization for LQ games and indeﬁnite cost structure, while removing restrictive assumptions and circumventing the projection step.
Acknowledgements
The authors thank Henk van Waarde and Shahriar Talebi for many helpful discussions.
24

A HESSIAN OF G(L)
Appendix
A Hessian of g(L)
In this section, we compute the Hessian of g(L) at a point of diﬀerentiation of L0. Indeed, we shall assume stronger assumptions of L0: there is a unique stabilizing minimizer of f (K, L0) over L0, denoted by K0. Throughout the section, we denote A0 = A − B1K0 − B2L0. Note, by assumption the DARE is solvable (A − B2L0)⊺X(A − B2L0) + Q − L⊺0R2L0 + (A − B2L0)⊺XB1(R1 + B1⊺XB1)−1B1⊺X(A − B2L0) = X, and the maximal solution X0 is stabilizing, i.e., K0 = (R1 +B1⊺X0B1)−1B1⊺X(A−B2L0) is stabilizing the system (A − B2L0, B1). As we have noted, the gradient of g(L0) is given by
∇g(L0) = 2(−R2L0 − B2⊺X0(A − B2L0 − B1K0))Y0,
where Y0 is the solution to the Lyapunov matrix equation A0Y A⊺0 + Σ = Y.
We now compute the Fréchet derivative of φ(L0) = 2(−R2L0 − B2⊺X0A0). Note φ ∶ Rm2×n → Rm2×n, the Fréchet derivative Dφ(L0) is a bounded linear map in L(Rm2×n, Rm2×n). So the action of Dφ(L0) at any E ∈ Rm1×n, denoted by Dφ(L0)[E] =: D ∈ Rm2×n is given by
D = 2(−R2E − B2⊺X0′ (E)(A − B2L0 − B1K0) − B2⊺X0(−B2E) − B2⊺X0(−B2K0′ (E))), where X0′ (E) ∈ Rn×n (respectively, K0′ (E)) is the action of the Fréchet derivative of X0 (respectively, K0) with respect to L0. Here we concern X0, K0 as maps of L. Now X0′ (E) satisﬁes
A⊺0X0′ (E)A0 − X0′ (E) − E⊺R2L0 − L⊺0R2E + K0′ (E)⊺R1K0 + K0⊺R1K0′ (E) −(B2E + B1K0′ (E))⊺X0A0 − A⊺0X0(B1K0′ (E) + B2E) = 0.
Noting R1K0 − B1⊺X0A0 = 0, we have X0′ (E) is the solution to the following Lyapunov equation A⊺0 X0′ (E)A0 − X0′ (E) − E⊺(R2L0 + B2⊺X0A0) − L⊺0(R2E + A⊺0 X0B2E) = 0.
As A0 is Schur, the solution exists and is unique
∞
X0′ (E) = (A⊺0 )j [−E⊺(R2L0 + B2⊺X0A0) − L⊺0(R2E + A⊺0X0B2E)]Aj0.
j=0
Similarly, we may compute K0′ (E) = (R1 + B1⊺X0B1)−1B1⊺X0(−B2E) + (R1 + B1X0B1)−1B1⊺X0′ (E)(A − B2L0) + (R1 + B1⊺X0B1)−1B1⊺X0′ (E)B1(R1 + B1⊺X0B1)−1B1⊺X0(A − B2L0),
and
∞
Y0′(E) = Aj0 A0Y0(−B2E)⊺ + (−B2E)Y0A⊺0 (A⊺0 )j .
j=0
25

B GRADIENT POLICY ANALYSIS FOR NONSTANDARD LQR
Combining the computations, we have the action of the Hessian is given by ⟨∇2g(L)E, E⟩ = 2⟨−R2 + B2⊺X0B2 − B2⊺X0B1(R1 + B1⊺X0B1)−1B1⊺X0B2)E, E⟩
+ 2⟨−B2⊺X0′ (E)A0 − B2⊺X0B1(R1 + B1X0B1)−1B1⊺X0′ (E)(A − B2L0), E⟩ + 2⟨B2⊺X0B1(R1 + B1⊺X0B1)−1B1⊺X0′ (E)B1(R1 + B1⊺X0B1)−1B1⊺X0(A − B2L0), E⟩ + 2⟨(−R2L0 − B2⊺X0A0)Y0′(E), E⟩
It is instructive to note that at L∗, X∗′ (E) = 0 since −R2L∗ − B2⊺X∗A∗ = 0. So the action of Hessian at L∗ is given by
∇2g(L∗)[E, E] = ⟨ −R2 + B2⊺X0B2 − B2⊺X0B1(R1 + B1⊺X0B1)−1B1⊺X0B2) EY0, E⟩.
That is, ∇2g(L∗) is a positive deﬁnite operator by condition (a1) in the assumption. Hence, −g(L) is locally strongly convex in a convex neighborhood.
Remark A.1. If we ignore the formality, the above computation is nothing but a linear approximation.
B Gradient Policy Analysis for Nonstandard LQR
This section is devoted to the proof of Theorem 7.4. As it was pointed out previously, the strategy for devising a stepsize guaranteeing linear convergence for the LQ game setup is similar to the one presented in [BMFM19]. However, the convergence analysis for the game setup is more involved as one can not estimate the needed quantities using the current function values due to the indeﬁniteness of the term Q − L⊺R2L. However, as we will show, a perturbation bound would circumvent this issue and allows deriving the required stepsize.
In order to simplify the notation, let ψ(M ) := f (M, L), U = R1M − B1⊺X(A − B2L − B1M ), AM = A − B2L − B1M.
Note that in our analysis L is always ﬁxed; we also adopt the notation AL := A − B2L. If Mη = A − B2L − B1(M − η2UY ) is stabilizing,
ψ(M ) − ψ(Mη) = 4η Tr U⊺U(Y Y (η) − ηaY YηY ) , where a = λn(R1 + B1⊺M B1), and Y (η) solves the Lyapunov matrix equation
(A − B2L − B1Mη)⊺Y (η)(A − B2L − B1Mη) + Σ = Y (η).
Now deﬁne a univariate function φ as, φ(η) = Tr U⊺U(Y Y (η) − ηaY YηY ) .
We observe that φ(η) is well-deﬁned locally around 0 by openness of the set SL.17 Further, observe that φ(0) > 0 if the gradient does not vanish at M . Now our goal is to characterize a step size such that φ(η) > 0. In this direction, we ﬁrst observe a perturbation bound on Y (η).
17φ is well-deﬁned only if A − B2L − B1Mη is Schur.
26

B GRADIENT POLICY ANALYSIS FOR NONSTANDARD LQR

Proposition B.1. Putting µ1 =

Y

2 B1UY

2 2

λ1(Σ)

and

µ2

=

Y

2 B1UY

2 A − B2L 2 λ1(Σ),

if we let

η0 =

1√+ µµ221 − µ2 , 2 µ1 2µ1

and supposing that Aη = A − B2L − B1Mη is Schur stable for every η ≤ η0, then for all η ≤ η0,

Y (η) 2 ≤ β0 Y 2,

where β0 = 1−4µ1η012−4µ2η0 > 0. Proof. Taking the diﬀerence of the corresponding Lyapunov matrix equations, we have

Y (η) − Y − (AL − B1M )(Y (η) − Y )(AL − B1M )⊺ = ALY (η)2η(B1UY )⊺ + 2ηB1UY Y (η)AL + 4η2B1UY Y (η)(B1UY )⊺

⪯

Yη 2

4η B1UY

2 AL

+ 4η2 B1UY

2 2

I

⪯

Yη 2

4η B1UY

2 AL

+ 4η2 B1UY

2 2

Σ.

λ1(Σ)

It thus follows that,

Y

−Y ⪯

Yη 2

4η B1UY

2 AL

+ 4η2 B1UY

2 2

Y.

η

λ1(Σ)

Hence,

Y

⎛1 −

Y

2

4η B1UY

2 AL

+ 4η2 B1UY

2 2

⎞≤

Y

.

η 2⎝

λ1(Σ)

⎠2

The proof is completed by a direct computation and noting that 1 β0 = 1 − µ1η02 − 4µ2η0 > 0 with the choice of η0 and for every η ≤ η0,
1 − 4µ1η2 − 4µ2η ≥ 1 − 4µ1η02 − 4µ2η0.

We now present an important result for our analysis. The basic idea of this lemma is as follows: if [0, c) is the largest interval such that At is Schur stable for every t ∈ [0, c) and Ac is marginally Schur stable,18 then we can ﬁnd a number c0 < c such that ψ(Ms) ≤ ψ(M ) for every s ∈ [0, c0].
Lemma B.2. Let c be the largest real positive number such that At is Schur stable for every t ∈ [0, c) and Ac is marginally Schur stable19. Let
a1 = aβ0λn(Y ) + 4 U 2β0[λn(Y )]2, a2 = a4 U 2β0[λn(Y )]2;
18Such a c exists by the openness of the set of stabilizing gains. 19Here we have assumed that c is not +∞. Of course, if c = +∞, then any stepsize would remain stabilizing.
27

C A USEFUL CONTROL THEORETIC OBSERVATION

then with
η1 ≤ min(c − ε, η0, c0),
where ε > 0 is an arbitrary positive real number and
c0 < 1 + a21 − a2 , a2 4a22 2a1
one has φ(η1) ≥ 0.
Proof. The computations follow a similar method used in [BMFM19] by replacing the estimate of Y (θ) by the bound in the above proposition (see details in Lemma 5.5 in [BMFM19]).
If one could explicitly compute the c in the above result, then a deterministic choice of stepsize could be chosen; however, this is not feasible. Fortunately, we can show that c > min(η0, c0). This would then imply that one can choose the stepsize η = min(η0, c0).
Theorem B.3. With the stepsize η = min(η0, c0), Mη remains stabilizing and φ(η) ≥ 0.
Proof. Let η = min(η0, c0). It suﬃces to prove that for every t ∈ [0, η], At is Schur stabilizing and φ(t) ≥ 0. We prove this by contradiction. Suppose that this is not the case. Then by continuity of eigenvalues, there exists a number η′ ≤ η such that As is stabilizing for every s ∈ [0, η′) and Mη′ is stabilizing. If this is the case, the choice of η0, c0 guarantees that for every s ∈ [0, η′), φ(s) is well-deﬁned and φ(s) ≥ 0. Now take a sequence ti → η′ and consider the corresponding sequence of value matrices {Xti}. Note that the sequence of function values Tr(Xti Σ) satisﬁes,
Tr(X+Σ) ≤ Tr(Xti Σ) ≤ Tr(XΣ),
since φ(t) ≥ 0. But this implies that {Xti } is a bounded sequence (note that the above inequality on function values does not guarantee the boundedness of the sequence; it is crucial that Xti ⪰ X+). Hence by a similar argument adopted in the proof of Theorem 7.1, these observations establish a contradiction, and as such, the proposed stepsize guarantees stabilization.
It is now straightforward to conclude the convergence rate.

C A Useful Control Theoretic Observation

In the proof to Lemma 5.4 and 6.1, we have used the fact that the set {L ∶ (A−B2L, B1) is stabilizable} is open; here is the justiﬁcation. Proposition C.1. Suppose A ∈ Rn×n, B1 ∈ Rn×m1 and B2 ∈ Rn×m2 are ﬁxed. Then the set
L = {L ∈ Rm2×n ∶ (A − B2L, B1) is stabilizable}

is open in Rm2×n.

Proof. Recall a pair (A − B2L, B1) is stabilizable if and only if there exists some F ∈ Rm1×n such

that A − B2L − B1F is Schur. So (A − B2L, B1) is stabilizable if and only if there exists X ≻ 0 and

F ∈ Rm1×n such that

(A − B2L − B1F )⊺X(A − B2L − B1F ) − X ≺ 0.

28

REFERENCES

REFERENCES

Now

consider

the

map

ψ

∶

S++
n

×

Rm2×n

×

Rm1×n

→

R

by

(X, L, F ) ↦ (A − B2L − B1F )⊺X(A − B2L − B1F ) − X ↦ λmax (A − B2L − B1F )⊺X(A − B2L − B1F ) − X .

The map ψ is continuous as it is a composition of continuous maps. It thus follows that ψ−1((−∞, 0)) is open. We now observe that L ≡ π2(ψ−1(−∞, 0)) where π2 is the projection onto the second
coordinate. Since projection map is open map20, L is open.

References

[AM07]

Brian D O Anderson and John B Moore. Optimal Control: Linear Quadratic Methods. Dover, February 2007.

[BB08]

Tamer Başar and Pierre Bernhard. H∞ Optimal Control and related Minimax Design Problems: a Dynamic Game Approach. Springer Science & Business Media, 2008.

[Ber05]

D.P. Bertsekas. Dynamic Programming and Optimal Control. Number v. 2 in Athena Scientiﬁc optimization and computation series. Athena Scientiﬁc, 2005.

[Ber13] Pierre Bernhard. Linear quadratic Zero-Sum Two-Person diﬀerential games, 2013.

[BIM12]

Dario A Bini, Bruno Iannazzo, and Beatrice Meini. Numerical Solution of Algebraic Riccati Equations, volume 9. SIAM, 2012.

[BMFM19] Jingjing Bu, Afshin Mesbahi, Maryam Fazel, and Mehran Mesbahi. LQR through the lens of ﬁrst order methods: Discrete-time case. arXiv preprint arXiv:1907.08921, 2019.

[BMM19]

Jingjing Bu, Afshin Mesbahi, and Mehran Mesbahi. On topological and metrical properties of stabilizing feedback gains: the MIMO case. arXiv preprint arXiv:1904.02737, 2019.

[BO99]

Tamer Basar and Geert Jan Olsder. Dynamic Noncooperative Game Theory, volume 23. SIAM, 1999.

[BR95]

Pierre Bernhard and Alain Rapaport. On a theorem of Danskin with an application to a theorem of von Neumann-Sion. Nonlinear Analysis, 24(8):1163–1182, 1995.

[DMM+17] Sarah Dean, Horia Mania, Nikolai Matni, Benjamin Recht, and Stephen Tu. On the sample complexity of the linear quadratic regulator. arXiv preprint arXiv:1710.01688, 2017.

[Eng05]

Jacob Engwerda. LQ Dynamic Optimization and Diﬀerential Games. John Wiley & Sons, November 2005.

[FGKM18] Maryam Fazel, Rong Ge, Sham Kakade, and Mehran Mesbahi. Global convergence of policy gradient methods for the linear quadratic regulator. In Proceedings of the 35th International Conference on Machine Learning, pages 1467–1476, 2018.
20A map f ∶ X → Y is open if f (U ) is open in Y whenever U ⊆ X is an open set.

29

REFERENCES

REFERENCES

[Hew71]

G. Hewer. An iterative technique for the computation of the steady state gains for the discrete optimal regulator. IEEE Transactions on Automatic Control, 16(4):382–384, 1971.

[JCD+19]

Max Jaderberg, Wojciech M Czarnecki, Iain Dunning, Luke Marris, Guy Lever, Antonio Garcia Castaneda, Charles Beattie, Neil C Rabinowitz, Ari S Morcos, Avraham Ruderman, et al. Human-level performance in 3d multiplayer games with populationbased reinforcement learning. Science, 364(6443):859–865, 2019.

[LR95]

Peter Lancaster and Leiba Rodman. Algebraic Riccati Equations. Oxford University Press, New York, NY, 1995.

[MKS+15]

Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529, 2015.

[SHM+16]

David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural networks and tree search. nature, 529(7587):484, 2016.

[SLZ+18]

Sriram Srinivasan, Marc Lanctot, Vinicius Zambaldi, Julien Pérolat, Karl Tuyls, Rémi Munos, and Michael Bowling. Actor-critic policy optimization in partially observable multiagent environments. In Advances in Neural Information Processing Systems, pages 3422–3435, 2018.

[Sto90] Anton Stoorvogel. The H∞ Control Problem: a State Space Approach. Citeseer, 1990.

[SW94]

Anton A Stoorvogel and Arie JTM Weeren. The discrete-time Riccati equation related to the H∞ control problem. IEEE Transactions on Automatic Control, 39(3):686–691, 1994.

[VMKL17] Kyriakos G Vamvoudakis, Hamidreza Modares, Bahare Kiumarsi, and Frank L Lewis. Game Theory-Based control system algorithms with Real-Time reinforcement learning: How to solve multiplayer games online. IEEE Control Syst., 37(1):33–52, 2017.

[Wil71]

Jan Willems. Least squares stationary optimal control and the algebraic Riccati equation. IEEE Transactions on Automatic Control, 16(6):621–634, 1971.

[Won12]

W.M. Wonham. Linear Multivariable Control: a Geometric Approach: A Geometric Approach. Stochastic Modelling and Applied Probability. Springer New York, 2012.

[Zha05]

Pingjian Zhang. Some results on Two-Person Zero-Sum linear quadratic diﬀerential games. SIAM J. Control Optim., 43(6):2157–2165, January 2005.

[ZYB19]

Kaiqing Zhang, Zhuoran Yang, and Tamer Başar. Policy optimization provably converges to Nash equilibria in zero-sum linear quadratic games. arXiv preprint arXiv:1906.00729, 2019.

30

