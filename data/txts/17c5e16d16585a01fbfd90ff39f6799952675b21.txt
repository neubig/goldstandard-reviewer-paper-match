JOINT MODELING OF CODE-SWITCHED AND MONOLINGUAL ASR VIA CONDITIONAL FACTORIZATION
Brian Yan1, Chunlei Zhang2, Meng Yu2, Shi-Xiong Zhang2, Siddharth Dalmia1, Dan Berrebbi1, Chao Weng3, Shinji Watanabe1, Dong Yu2
1Carnegie Mellon University, USA, 2Tencent AI Lab, USA, 3Tencent AI Lab, China

arXiv:2111.15016v1 [cs.CL] 29 Nov 2021

ABSTRACT
Conversational bilingual speech encompasses three types of utterances: two purely monolingual types and one intra-sententially code-switched type. In this work, we propose a general framework to jointly model the likelihoods of the monolingual and code-switch sub-tasks that comprise bilingual speech recognition. By deﬁning the monolingual sub-tasks with label-to-frame synchronization, our joint modeling framework can be conditionally factorized such that the ﬁnal bilingual output, which may or may not be code-switched, is obtained given only monolingual information. We show that this conditionally factorized joint framework can be modeled by an endto-end differentiable neural network. We demonstrate the efﬁcacy of our proposed model on bilingual Mandarin-English speech recognition across both monolingual and code-switched corpora.
Index Terms— code-switched ASR, bilingual ASR, RNN-T
1. INTRODUCTION
Conversational spoken language deﬁes monolithic form, but rather is highly adaptive to situational cues such as who you are speaking to or what you are speaking about [1, 2]. For instance, bilingual speakers often code-switch between different languages to facilitate communication [3, 4]. In fact, the act of bilingual code-switching itself may convey various aspects about the speaker such as a desire to spark an interpersonal connection [5], a level of subject-matter expertise [6], or an afﬁnity to some socio-economic status [7].
In order to broadly cover bilingual speech, recognition systems need to recognize not only monolingual utterances from two different languages, but also intra-sententially code-switched (CS) utterances where both languages are present [3, 4]. While recent advancements in the related ﬁeld of multilingual speech recognition have signiﬁcantly improved the language coverage of a single system by training on mixtures of monolingual utterances [8, 9], these works typically do not account for intra-sentential CS. Prior works have adapted large-scale multilingual models to more ﬂexibly identify language switch points [10, 11], but performance is dependent on the cross-lingual dynamics of the selected languages [12].
Another approach is to directly optimize towards intra-sentential CS for a particular bilingual pair. A signiﬁcant portion of recent works are ameliorating the linguistic differences between two unrelated languages by explicitly deﬁning cross-lingual phone-merging rules [13–15] or by implicitly learning latent language identity representations [16–21]. The other signiﬁcant portion of recent works are ameliorating the scarcity of paired CS speech data through data efﬁcient methods that incorporate monolingual data into both acoustic [22–25] and language modeling [24–28] as well as through data
Work done while Brian was interning with Tencent AI Lab.

SINGLE MULTI

Y

Y,I

SEPARATION Y

MIXTURE Y

CONDITIONAL Z

YM

YE

G

ZM

ZE

I

YM

YE

X (a) X

X (b) X

(c) X

Fig. 1. Probabilistic graphical model representations of different formulations of the bilingual ASR task, separated into three categories: (a) direct, (b) divide-and-conquer, and (c) conditional.

augmentation techniques that generate synthetic CS data [29–32]. However, works focusing narrowly on intra-sentential CS often ignore or sacrﬁce performance on monolingual scenarios [33, 34]
We are interested conversational bilingual speech recognition systems (ASR) that can cover both monolingual and intrasententially CS scenarios. In particular, we are interested in systems that can 1) indiscriminately recognize both monolingual and intrasententially CS utterances, 2) efﬁciently leverage monolingual and CS ASR training data, and 3) be built in an end-to-end manner.
We ﬁrst propose a formulation of the bilingual ASR problem as a conditionally factorized joint model of monolingual and CS ASR where the ﬁnal output is obtained given only monolingual label-toframe synchronized information §3.1. We then apply an end-toend differentiable neural network, which we call the Conditional RNN-Transducer (RNN-T), to model our conditional joint formulation §3.2. We show the efﬁcacy of the Conditional RNN-T model in both monolingual and CS scenarios compared to several baselines §5. Next, we demonstrate the Language-Separation ability, an added beneﬁt of our proposed model §5.1. Finally, we validate a key conditional independence assumption in our framework by showing experimentally that given monolingual label-to-frame information, no other information from the observation is required §5.2.

2. BACKGROUND AND MOTIVATION
In this section, we interpret the underlying probabilistic graphical models of prior works in bilingual ASR, as applied to MandarinEnglish, to motivate our conditionally factorized framework in §3.
2.1. Direct Graphical Models
Bilingual ASR is a sequence mapping from a T -length speech feature sequence, X = {xt ∈ RD|t = 1, ..., T }, to an N -length label sequence, Y = {yt ∈ (VM ∪ VE)|n = 1, ..., L} consisting of Mandarin VM and English VE. As shown in Figure 1.a, the simplest approach models the bilingual output Y as a random variable

with a single dependency on the observation X. Both the Mandarin VM and English VE vocabularies are regarded as part of the same set of output units and no explicit distinctions are made between the languages. This uniﬁcation of two languages from unrelated families complicates the ASR task via phonetic ambiguities, but handcrafting phone-merging rules may alleviate this issue [13–15].
Alternatively, multi-tasking with language-identiﬁcation can induce useful latent distinctions between Mandarin and English representations. Here, we introduce language ID I as another random variable with a single dependency on the observation X. Note that the bilingual output Y and the language-ID I do not interact with each other aside from being implicitly related as they both directly depend on X. During training, this multi-tasking approach helps to resolve cross-lingual ambiguities within the shared bilingual latent representations [16–21]. During inference, the bilingual output is obtained directly from the observation similar to the single dependency approach. The main drawback of these direct approaches is the potentially high complexity of the X → Y dependency due to conﬂicting linguistics of different two languages.
2.2. Divide-and-Conquer Graphical Models
Alternatively, the bilingual ASR task can be decomposed into its sub-component monolingual parts as in the divide-and-conquer approaches shown in Figure 1.b. The separational approach uses language ID I to segment the observation X into monolingual parts before passing those to monolingual recognizers which separately predict Mandarin Y M = {ylM ∈ VM |l = 1, ..., L} and English Y E = {ylE ∈ VE|l = 1, ..., L}. Finally, the monolingual outputs Y M and Y E are stitched together accordingly to obtain the ﬁnal bilingual output Y . This approach successfully introduces simpler X → Y M and X → Y E dependencies, but does so at the cost of the additional dependencies associated with the language ID random variable I: I → Y M , I → Y E, and I → Y . Therefore, performance becomes highly dependent on the ability to correctly predict the language identity at a segment level [35–39].
The mixture-based approach avoids this dependency on the language ID I by instead introducing another random variable G, a gating mechanism. This approach ﬁrst noisily models X → Y M and X → Y E, but maintains latent representations of each so as to avoid early decisions. Then, it models the gating mechanism G as a function of these latent monolingual representations; we approximate this dependency in our graph as (X, Y M , Y E) → G. Finally, the gating mechanism G is used to fuse the latent monolingual representations to ultimately obtain the bilingual output Y . This is a promising approach that efﬁciently combines information from monolingual experts by operating entirely within the latent space [22–24]. However, mixture-based approaches much like their separational cousins incur the cost of the additional dependencies associated with the gating random variable G. Our motivation is to decompose the bilingual ASR problem into monolingual sub-tasks without incurring any additional random variables.
3. PROPOSED FRAMEWORK
In this section, we ﬁrst propose a framework using label-to-frame synchronization to obtain bilingual outputs given only conditional monolingual information. Then we show an end-to-end differentiable method to model our conditionally factorized framework.
3.1. Conditionally Factorized Formulation of Bilingual ASR
Rather than treating CS ASR as a single sequence transduction task, we seek to decompose it into three portions: 1) recognizing Man-

darin 2) recognizing English and 3) composing recognized monolin-
gual segments into a bilingual sequence which may or may not be CS. However, given only monolingual portions of the output Y M and Y E we cannot form Y without the order in which they should be composed. Therefore, in order to satisfy our desired conditional
probabilistic graph (shown in Figure 1.c), we need richer monolin-
gual representations which contain ordering information. Consider that for each output T -length observation sequence X
and L-length bilingual label sequence Y , there are a number of possible T -length label-to-frame sequences Z = {zt ∈ VM ∪ VE ∪ {∅}|t = 1 . . . T }. Here, zt is either a surface-level unit or a blank symbol denoting a null emission as in Connectionist Temporal Clas-
siﬁcation (CTC) [40]. The posterior of the bilingual label sequence p(Y |X) is then factorized as follows:

p(Y |X) =

p(Z |X )

(1)

Z∈Z(Y )

Note that the summation is over all possible label-to-frame alignments Z ∈ Z(Y ) for a given observation X and label Y pair.1
Next, we re-formulate any bilingual label-to-frame sequence
in terms of its constituent monolingual label-to-frame sequences ZM = {ztM ∈ VM ∪ {∅}|t = 1 . . . T } and ZE = {ztE ∈ VE ∪ {∅}|t = 1 . . . T }. Now, we can indeed obtain the bilingual Z given only conditional monolingual information ZM and ZE:

ztM , if ztM ∈ VM and ztE = ∅ 

zt = ztE , if ztE ∈ VE and ztM = ∅

(2)

∅, if ztM = ∅ and ztE = ∅

Note that at position t only one of ztM or ztE may be a surface-
level unit while the other must be the blank symbol by deﬁnition. It is possible that both ztM and ztE are blank symbols, but it is not
possible that both are surface-level units.
Following this interpretation of the bilingual sequence Z in terms of its monolingual parts ZM and ZE, we can re-formulate the posterior p(Z|X) as a joint likelihood p(Z, ZM , ZE|X):

p(Z|X) = p(Z, ZM , ZE|X)

(3)

= p(Z|ZM , ZE, X)p(ZM , ZE|X)

(4)

≈ p(Z|ZM , ZE, X)p(ZM |X)p(ZE|X) (5) &

From Eq. (4) to Eq. (5), we make two key independence assumptions. The ﬁrst is that given ZM and ZE, no other information from the observation X is required to determine Z since the monolingual
label-to-frame sequences already contain ordering information. We
show that this assumption holds experimentally in §5.2. The second assumption is that p(ZM |X) and p(ZE|X) are independent, allow-
ing for separate modeling of monolingual posteriors.

3.2. Conditional RNN Transducer

3.2.1. Monolingual Modules

The monolingual label-to-frame posteriors p(ZM |X) and p(ZE|X) are further factorized with the chain-rule as follows:

T

p(ZM |X) ≈

p

(

z

M t

|

X

,

z

1M:¨t−¨1)

(6)

¨

t=1

T

p(ZE|X) ≈

p

(

z

E t

|

X

,

z

1E:¨t−¨1)

(7)

¨

t=1

1Z maps to Y deterministically via repeat and blank removal rules [41].

Table 1. Results comparing the Conditional RNN-T models to Vanilla and Gating RNN-T baselines on intra-sententially code-switched (CS), monolingual Mandarin, and monolingual English test sets. The upper half shows results when using only CS data during ﬁne-tuning while the bottom half shows results when using CS + monolingual data during ﬁne-tuning. All models are pre-trained on monolingual data.

Model Type
Direct Mixture Conditional Conditional
Direct Mixture Conditional Conditional

Model Name
Vanilla RNN-T [21, 24] Gating RNN-T [22, 24] Our Proposed Model + Language-Separation (LS)
Single RNN-T [21, 24] Gating RNN-T [22, 24] Our Proposed Model + Language-Separation (LS)

Pre-trained Encoder(s)
   
   

Fine-tuning Data
CS CS CS CS
CS + M CS + M CS + M CS + M

CODE-SWITCHED MER CER WER
12.3 9.9 34.3 11.5 9.1 33.0 11.5 9.1 33.2 11.1 8.7 32.7
11.3 9.3 30.8 11.2 8.8 34.7 10.3 8.2 29.5 10.2 8.1 29.2

MONO-MAN CER
17.9 17.7 15.5 15.3
6.5 5.7 5.4 5.3

MONO-ENG WER
81.4 78.3 82.9 82.7
17.8 34.6 16.5 16.3

Note that we make the conditional independence assumption here as
a modeling choice, mitigating label / exposure bias issues [42, 43].
Our proposed Conditional RNN-T models the posterior as follows. Given the observed speech feature sequence X, a Mandarin-
only ENCODERM maps to a sequence of hidden representations hM = {hM t ∈ RD|t = 1, ..., T } and an English-only ENCODERE maps to a sequence of hidden representations hE = {hEt ∈ RD|t = 1, ..., T }. Separate linear projection layers followed by softmax
activations, SOFTMAXOUTM and SOFTMAXOUTM , yield the posteriors p(ztM |X) and p(ztE|X). We train these modules using CTC loss functions LM CTC and LE CTC [40].

3.2.2. Bilingual Module
The bilingual conditional likelihood p(Z|ZM , ZE) is further factorized with the chain-rule as well:

T +L

p(Z|ZM , ZE) =

p(zi|ZM , ZE , z1:i−1)

(8)

i=1

The monolingual alignment information is passed to the bilingual module via the hidden representations hM and hE. We therefore approximate P (zi|ZM , ZE, z1:i−1) as follows:

htENC = hM t + hEt hlDEC = DECODER(z1:l−1) htJN,lT = JOINT(htENC, hlDEC) p(zi|hM , hE , z1:i−1) = SOFTMAXOUT(htJN,lT)

(9) (10) (11) (12)

Note that (12) approximates p(zi|ZM , ZE, z1:i−1) from (8) by using the latent representations hM and hE to pass the monolingual label-to-frame information ZM and ZE.2 We train these modules
using the RNN-T loss function LRNNT [41].

3.2.3. Full Network With Equations (1) and (5), the posterior p(Y |X) ﬁnally becomes:

p(Y |X) ≈ p(Z|ZM , ZE) p(ZM |X) p(ZE|X) (13)

Z

ZM

ZE

=∆prnnt(Y |ZM ,ZE ) =∆pctc(Y M |X) =∆pctc(Y E |X)

2Alternatively, logit or softmax normalized could be used here [44].

where the monolingual CTC, pctc(Y M |X) and pctc(Y E|X), and bilingual RNN-T, prnnt(Y |ZM , ZE), objective functions are de-
ﬁned as summations over all possible frame-to-label sequences ZM ∈ ZM (Y M ), ZE ∈ ZE(Y E), and Z ∈ Z(Y ) respectively.
We train our Conditional RNN-T model using an initial monolingual pre-training step to maximize pctc(Y M |X) and pctc(Y E|X). We then ﬁne-tune the entire network to maximize prnnt(Y |ZM , ZE),
including both the bilingual and pre-trained monolingual modules,
on a mixture of monolingual and CS data. Since this ﬁrst regime
only implicitly applies monolingual conditioning via pre-trained ini-
tialization, we propose an explicit alternative regime which we call
the Conditional RNN-T + Language-Separation (LS). This variant
of our proposed model uses a multi-tasked loss LLS with tunable λ:

LLS = λLRNNT + (1 − λ)(LM CTC + LE CTC)

(14)

The monolingual ground truths Y M and Y E are obtained by applying a language-speciﬁc mask to the bilingual ground truth Y .3

4. DATA AND EXPERIMENTAL SETUP
Data: We use 200h of intra-sententially CS training data from the ASRU 2019 shared task where Mandarin is the matrix and English is the embedded language [45]. We use 500h of monolingual Mandarin data for pre-training and a 200h subset for ﬁne-tuning [45]. We use 700h of accented monolingual English data for pre-training and a 200h subset for ﬁne-tuning [46]. We use provided test CS data and generate our own splits for monolingual test sets. Experimental Setup: All our models were trained using the ESPnet toolkit [47]. Our input features are global mean-variance normalized 83 log-mel ﬁlterbank and pitch features [48]. We apply the Switchboard Strong (SS) augmentation policy of SpecAugment [49]. We combine 5000 Mandarin characters with 5000 English BPE [50] units to form the output vocabulary. ENCODERS are conformers [51, 52] with 12 blocks, kernel size of 15, 2048 feed-forward dim, 256 attention dim, and 4 heads. DECODERS are LSTMs [47, 53] with 1 layer, 1024 embed dim, 512 unit dim, and 512 joint dim. The direct RNN-T baseline with only 1 encoder uses a doubled 512 attention dim, so all models have about 100M params. We use the Adam optimizer to train 80 epochs with an inverse square root decay schedule, a transformer-lr scale [47] of 1, 25k warmup steps, and an effective batchsize of 192. ENCODERS are pretrained using the hybrid CTC/Attention framework [54]. We use beam-size of 10 during inference. Ablation studies on CTC sub-nets use greedy decoding.
3This masking is applicable to both monolingual and CS Y . E.g. if yl ∈ VM ∀l, then the masked Y E is the empty string and Y M is the entire Y .

Ref: 我 有 两 个 ▁QUESTION 比 较 长 …

Mandarin CTC

English CTC

Blank Top Non-Blank
Frames
Fig. 2. Example illustrating the label-to-frame posteriors of the monolingual CTC sub-nets for an intra-sententially CS utterance.

Table 2. Ablation study examining the language-separation ability of the monolingual CTC sub-nets, p(ZM |X) and p(ZE|X), on the
CS dev set. The sub-nets are expected to transcribe speech from their
languages while ignoring speech in the other. Performance is evaluated using monolingual parts Y M and Y E of the ground-truth CS
label sequence Y . CER/WER and Insertion Rate (INS) are shown.

Model
Cond. RNN-T Cond. RNN-T + LS

MAN PORTION OF CS Sub-Net CER INS
p(ZM |X) 11.8 3.7 p(ZM |X) 8.6 0.7

ENG PORTION OF CS Sub-Net WER INS
p(ZE|X) 42.7 7.9 p(ZE|X) 37.1 4.6

5. RESULTS
In Table 1, we compare the CS and monolingual performances of our Conditional RNN-T and Conditional RNN-T + LS models to direct and mixture-based baselines, which are our re-implementations of Vanilla RNN-T [21, 24] and Gating RNN-T [22, 24] described in prior works. The top and bottom portions of Table 1 compare results when using only CS data versus using both CS and monolingual data during ﬁne-tuning. Not only did all models perform signiﬁcantly better on monolingual sets when using monolingual ﬁne-tuning data, they also improved on the CS set suggesting that the monolingual data is indeed supplementing the CS training data.4
As shown in the bottom half of Table 1, the Gating RNN-T slightly outperforms the Vanilla RNN-T on CS and monolingual Mandarin but is degraded on monolingual English, suggesting that the gating mechanism overly focuses on outputting Mandarin due to the high skew of the CS training data towards Mandarin. Our proposed Conditional RNN-T model outperforms the best baselines consistently across evaluation sets. On the monolingual sets, the Conditional RNN-T model performs similarly to monolingual-only models trained on the same data. Further, the Language-Separation loss incrementally improves across all sets suggesting a beneﬁt to the explicit monolingual conditioning method described in Eq. (14). We examine this beneﬁt further in the subsequent sub-section.
5.1. Language-Separation Ability of Monolingual Modules
Recall that in Eq. (2) we state that the bilingual Z is speciﬁed in terms of its monolingual parts, ZM and ZE, such that at any position t only one of ztM or ztE may be non-blank. If our proposed Conditional RNN-T model is indeed modeling p(ZM |X) and
4Unlike the shared task in [45], we evaluate on monolingual corpora and do not use the provided 3-gram LM, the full monolingual data during ﬁnetuning, data augmentation besides SpecAugment, or LID multi-tasking.

Table 3. Experimental validation of conditional independence assumption in the bilingual module which models p(Z|ZM , ZE, X).
&
The 3-Encoder variant removes this assumption in its bilingual module p(Z|ZM , ZE, X). Results are shown on the CS dev set.

Model
Cond. RNN-T + LS 3-Enc. RNN-T + LS

Bilingual Condition
p(Z|ZM , ZE) p(Z|ZM , ZE, X)

CODE-SWITCHED MER CER WER
11.1 8.9 31.1 11.2 9.0 31.1

p(ZE|X) with this property, it logically follows that each monolingual CTC sub-net would be capable of emitting labels for frames corresponding to their language while emitting blanks for frames in the other language; we refer to this diarization-like [37] ability as Language-Separation. This Language-Separation is observable in Figure 2 which depicts the blank and non-blank posterior values of each monolingual CTC sub-net for a snippet of CS speech. Here, the Mandarin side emits characters while the English side emits blanks except for frames 30 to 40 where the opposite occurs.
As mentioned in §3.2.3, there are two ways to optimize the Conditional RNN-T model towards the conditionally factorized formulation in Eq. (13). Table 2 shows that the implicit way, which uses pre-training to condition the bilingual task p(Z|ZM , ZE), produces monolingual CTC sub-nets that reasonably able perform LanguageSeparation on CS data. However, the explicit way described in Eq. (14) is preferred since it provides a supervised Language-Separation signal. Thus, the resultant monolingual CTC sub-nets of this Conditional RNN-T + LS model have a greater Language-Separation ability as indicated by the reduced insertion errors for the same CS data.
5.2. Conditional Independence of p(Z|ZM , ZE) from X
Finally, we experimentally validate the conditional independence assumption in Eq. (5) that the ﬁnal bilingual output Z depends only on monolingual alignment information ZM and ZE and nothing else from the observation X. In this study, we augment the Conditional RNN-T + LS model with a third ENCODERA which maps the observation X to hidden representations hA = {hAt ∈ RD|t = 1, ..., T }. This hA is then added as a third term to the fusion in Eq. (9) thereby allowing the bilingual module to model p(Z|ZM , ZE, X) instead of p(Z|ZM , ZE). We call this modiﬁed model the 3-Encoder RNN-T + LS and train it in the same way as the Conditional RNN-T + LS model. As shown in Table 3, this 3-Encoder variant does not capture any additional useful information from the observation X but rather performs slightly worse than our Conditional RNN-T + LS model.
6. CONCLUSION
We present an end-to-end framework for jointly modeling CS and monolingual ASR with conditional factorization such that the bilingual task is logically decomposed into simpler sub-components. We show improvements on both CS and monolingual ASR over prior works, suggesting that our general joint modeling approach is promising towards building robust bilingual systems. In future work, we seek to extend our approach with joint decoding [54], differentiable WFST representations of code-switching linguistics [55], and modular neural network methods [44].
7. ACKNOWLEDGEMENTS
We would like to thank Florian Boyer, Jia Cui, Xuankai Chang, Hirofumi Inaguma, and Koshak for their support.

8. REFERENCES
[1] P. Auer, Code-switching in conversation: Language, interaction and identity. Routledge, 2013.
[2] M. Dragojevic, J. Gasiorek, and H. Giles, “Communication accommodation theory,” Intl. Encyclopedia of Interpersonal Comm., 2015.
[3] R. R. Heredia and J. Altarriba, “Bilingual language mixing: Why do bilinguals code-switch?” Curr. Dir. in Psychological Science, 2001.
[4] L. Hou, Y. Liu, Y. Gao, et al., “A study of types and characteristics of code-switching in mandarin-english speech,” Proc. WSTCSMC, 2020.
[5] E. Ahn, C. Jimenez, Y. Tsvetkov, et al., “What code-switching strategies are effective in dialog systems?” In Proc. SCiL, 2020.
[6] M. Yoder, S. Rijhwani, C. Rose´, et al., “Code-switching as a social act: The case of arabic wikipedia talk pages,” in NLP+CSS, 2017.
[7] H. Liu, “Attitudes toward different types of chinese-english codeswitching,” Sage Open, 2019.
[8] O. Adams, M. Wiesner, S. Watanabe, et al., “Massively multilingual adversarial speech recognition,” in Proc. NAACL-HLT, 2019.
[9] V. Pratap, A. Sriram, P. Tomasello, et al., “Massively multilingual asr: 50 languages, 1 model, 1 billion parameters,” Proc. Interspeech, 2020.
[10] B. Li, Y. Zhang, T. Sainath, et al., “Bytes are all you need: End-to-end multilingual speech recognition with bytes,” in Proc. ICASSP, 2019.
[11] L. Zhou, J. Li, E. Sun, et al., “A conﬁgurable multilingual model is all you need to recognize all languages,” arXiv 2107.05876, 2021.
[12] H. Seki, S. Watanabe, T. Hori, et al., “End-to-end language-tracking speech recognizer for mixed-language speech,” in ICASSP, 2018.
[13] T. Lyudovyk and V. Pylypenko, “Code-switching speech recognition for closely related languages,” in Proc. SLTU, 2014.
[14] N. Luo, D. Jiang, S. Zhao, et al., “Towards end-to-end code-switching speech recognition,” arXiv preprint arXiv:1810.13091, 2018.
[15] S. Sivasankaran, B. M. L. Srivastava, S. Sitaram, et al., “Phone merging for code-switched speech recognition,” in Proc. CALCS, 2018.
[16] E. Yılmaz, H. van den Heuvel, and D. Van Leeuwen, “Investigating bilingual deep neural networks for automatic recognition of codeswitching frisian speech,” Procedia Computer Science, 2016.
[17] X. Song, Y. Zou, S. Huang, et al., “Investigating multi-task learning for automatic speech recognition with code-switching between mandarin and english,” in Proc. IALP, 2017.
[18] Z. Zeng, Y. Khassanov, V. T. Pham, et al., “On end-to-end mandarinenglish code-switching speech recognition,” Interspeech, 2018.
[19] S. Kim and M. L. Seltzer, “Towards language-universal end-to-end speech recognition,” in Proc. ICASSP, 2018.
[20] C. Shan, C. Weng, G. Wang, et al., “Investigating end-to-end speech recognition for mandarin-english code-switching,” in ICASSP, 2019.
[21] S. Zhang, J. Yi, Z. Tian, et al., “Rnn-t with language bias for end-toend man-eng code-switching speech recognition,” ISCSLP, 2021.
[22] Y. Lu, M. Huang, H. Li, et al., “Bi-encoder transformer network for mandarin-english code-switching speech recognition using mixture of experts.,” in Proc. Interspeech, 2020.
[23] X. Zhou, E. Yılmaz, Y. Long, et al., “Multi-encoder-decoder transformer for code-switching speech recognition,” Interspeech, 2020.
[24] S. Dalmia, Y. Liu, S. Ronanki, et al., “Transformer-transducers for code-switched speech recognition,” in Proc. ICASSP, 2021.
[25] S. Zhang, J. Yi, Z. Tian, et al., “Decoupling pronunciation and language for end-to-end code-switching asr,” in Proc. ICASSP, 2021.
[26] H. Gonen and Y. Goldberg, “Language modeling for code-switching: Evaluation, integration of monolingual data, and discriminative training,” Proc. EMNLP, 2018.
[27] C. Shan, C. Weng, G. Wang, et al., “Component fusion: Learning replaceable language model component for end-to-end speech recognition system,” in Proc. ICASSP, 2019.

[28] S.-P. Chuang, T.-W. Sung, and H.-y. Lee, “Training code-switching language model with monolingual data,” in Proc. ICASSP, 2020.
[29] C.-T. Chang, S.-P. Chuang, and H.-Y. Lee, “Code-switching sentence generation by generative adversarial networks and its application to data augmentation,” arXiv preprint arXiv:1811.02356, 2018.
[30] A. Pratapa, G. Bhat, M. Choudhury, et al., “Language modeling for code-mixing: Linguistic theory based synthetic data,” in ACL, 2018.
[31] M. Ma, B. Ramabhadran, J. Emond, et al., “Comparison of data augmentation and adaptation strategies for code-switched automatic speech recognition,” in Proc. ICASSP, 2019.
[32] G. Lee, X. Yue, and H. Li, “Linguistically motivated parallel data augment for code-switch language modeling.,” in Interspeech, 2019.
[33] S. Shah, B. Abraham, S. Sitaram, et al., “Learning to recognize codeswitched speech without forgetting monolingual speech recognition,” arXiv preprint arXiv:2006.00782, 2020.
[34] S. Sitaram, K. R. Chandu, S. K. Rallabandi, et al., “A survey of codeswitched speech and language processing,” arXiv, 2019.
[35] J. Y. Chan, P. Ching, T. Lee, et al., “Detection of language boundary in code-switching by bi-phone probabilities,” in Proc. ISCSLP, 2004.
[36] J. Weiner, N. T. Vu, D. Telaar, et al., “Integration of language identiﬁcation into a recognition system for spoken conversations containing code-switches,” in Proc. SLTU, 2012.
[37] D.-C. Lyu, E.-S. Chng, and H. Li, “Language diarization for codeswitch conversational speech,” in Proc. ICASSP, 2013.
[38] S. Rallabandi, S. Sitaram, and A. W. Black, “Automatic detection of code-switching style from acoustics,” in Proc. CALCS, 2018.
[39] K. Li, J. Li, G. Ye, et al., “Towards code-switching asr for end-to-end ctc models,” in Proc. ICASSP, 2019.
[40] A. Graves, S. Ferna´ndez, F. Gomez, et al., “Connectionist temporal classiﬁcation: Labelling unsegmented sequence data with recurrent neural networks,” in Proc. ICML, 2006.
[41] A. Graves, “Sequence Transduction with Recurrent Neural Networks,” in Proc. ICML, 2012.
[42] M. Ranzato, S. Chopra, M. Auli, et al., “Sequence level training with recurrent neural networks,” arXiv preprint arXiv:1511.06732, 2015.
[43] L. Bottou, Y. Bengio, and Y. Le Cun, “Global training of doc processing systems using graph transformer networks,” in CVPR, 1997.
[44] S. Dalmia, A. Mohamed, M. Lewis, et al., “Enforcing encoderdecoder modularity in sequence-to-sequence models,” arXiv, 2019.
[45] X. Shi, Q. Feng, and L. Xie, “The asru 2019 mandarin-english codeswitching speech recognition challenge: Open datasets, tracks, methods and results,” Proc. WSTCSMC, 2020.
[46] Speechocean, King-asr-190: Chinese english speech recognition corpus, http://speechocean.com/, 2017.
[47] S. Watanabe, T. Hori, S. Karita, et al., “Espnet: End-to-end speech processing toolkit,” Proc. Interspeech, 2018.
[48] D. Povey, A. Ghoshal, G. Boulianne, et al., “The kaldi speech recognition toolkit,” in Proc. ASRU, 2011.
[49] D. S. Park, W. Chan, Y. Zhang, et al., “Specaugment: Simple data augmentation for automatic speech recognition,” Interspeech, 2019.
[50] R. Sennrich, B. Haddow, and A. Birch, “Neural machine translation of rare words with subword units,” Proc. ACL, 2015.
[51] A. Gulati, J. Qin, C.-C. Chiu, et al., “Conformer: Convolutionaugmented transformer for speech recognition,” Interspeech, 2020.
[52] P. Guo, F. Boyer, X. Chang, et al., “Recent developments on espnet toolkit boosted by conformer,” in Proc. ICASSP, 2021.
[53] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” Neural computation, 1997.
[54] S. Watanabe, T. Hori, S. Kim, et al., “Hybrid ctc/attention architecture for end-to-end speech recognition,” JSTSP, 2017.
[55] B. Yan, S. Dalmia, D. R. Mortensen, et al., “Differentiable allophone graphs for language-universal speech recognition,” Interspeech, 2021.

