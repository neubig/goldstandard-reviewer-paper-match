arXiv:2106.00545v3 [cs.LG] 2 Nov 2021

Counterfactual Invariance to Spurious Correlations: Why and How to Pass Stress Tests
Victor Veitch1,2, Alexander D’Amour1, Steve Yadlowsky1, and Jacob Eisenstein1
1Google Research 2University of Chicago
Abstract
Informally, a ‘spurious correlation’ is the dependence of a model on some aspect of the input data that an analyst thinks shouldn’t matter. In machine learning, these have a know-it-when-you-see-it character; e.g., changing the gender of a sentence’s subject changes a sentiment predictor’s output. To check for spurious correlations, we can ‘stress test’ models by perturbing irrelevant parts of input data and seeing if model predictions change. In this paper, we study stress testing using the tools of causal inference. We introduce counterfactual invariance as a formalization of the requirement that changing irrelevant parts of the input shouldn’t change model predictions. We connect counterfactual invariance to out-of-domain model performance, and provide practical schemes for learning (approximately) counterfactual invariant predictors (without access to counterfactual examples). It turns out that both the means and implications of counterfactual invariance depend fundamentally on the true underlying causal structure of the data—in particular, whether the label causes the features or the features cause the label. Distinct causal structures require distinct regularization schemes to induce counterfactual invariance. Similarly, counterfactual invariance implies different domain shift guarantees depending on the underlying causal structure. This theory is supported by empirical results on text classiﬁcation.
1 Introduction
Our focus in this paper is the sort of spurious correlations revealed by “poke it and see what happens” testing procedures for machine-learning models. For example, we might test a sentiment analysis tool by changing one proper noun for another (“tasty Mexican food” to “tasty Indian food”), with the expectation that the predicted sentiment should not change. This kind of perturbative stress testing is increasingly popular: it is straightforward to understand and offers a natural way to test the behavior of models against the expectations of practitioners [Rib+20; Wu+19; Nai+18].
Intuitively, models that pass such stress tests are preferable to those that do not. However, fundamental questions about the use and meaning of perturbative stress tests remain open. For instance, what is the connection between passing stress tests and model performance on prediction? Eliminating predictor dependence on a spurious correlation should help with domain shifts that affect the spurious correlation—but how do we make this precise? And, how should we develop models that pass stress tests when our ability to generate perturbed examples is limited? For example, automatically perturbing the sentiment of a document in a general fashion is difﬁcult.
The ad hoc nature of stress testing makes it difﬁcult to give general answers to such questions. In this paper, we will use the tools of causal inference to formalize what it means for models
1

to pass stress tests, and use this formalization to answer the questions above. We will formalize passing stress tests as counterfactual invariance, a condition on how a predictor should behave when given certain (unobserved) counterfactual input data. We will then derive implications of counterfactual invariance that can be measured in the observed data. Regularizing predictors to satisfy these observable implications provides a means for achieving (partial) counterfactual invariance. Then, we will connect counterfactual invariance to robust prediction under certain domain shifts, with the aim of clarifying what counterfactual invariance buys and when it is desirable.
An important insight that emerges from the formalization is that the true underlying causal structure of the data has fundamental implications for both model training and guarantees. Methods for handing ‘spurious correlations’ in data with a given causal structure need not perform well when blindly translated to another causal structure.
Counterfactual Invariance Consider the problem of learning a predictor f that predicts a label Y from covariates X . In this paper, we’re interested in constructing predictors whose predictions are invariant to certain perturbations on X . Our ﬁrst task is to formalize the invariance requirement.
To that end, assume that there is an additional variable Z that captures information that should not inﬂuence predictions. However, Z may causally inﬂuence the covariates X . Using the potential outcomes notation, let X (z) to denote the counterfactual X we would have seen had Z been set to z, leaving all else ﬁxed. Informally, we can understand perturbative stress tests as a way of producing particular realizations of counterfactual pairs X (z), X (z ) that differ by an intervention on z. Then, we formalize the requirement that an arbitrary change to z does not change predictions:
Deﬁnition 1.1. A predictor f is counterfactually invariant to Z if f (X (z)) = f (X (z )) almost everywhere, for all z, z in the sample space of Z. When Z is clear from context, we’ll just say the predictor is counterfactually invariant.
2 Causal Structure
Counterfactual invariance is a condition on how the predicted label behaves under interventions on parts of the input data. However, intuitions about stress testing are based on how the true label behaves under interventions on parts of the input data. We will see that the true causal structure fundamentally affects both the implications of counterfactual invariance, and the techniques we use to achieve it. To study this phenomenon, we’ll use two causal structures that are commonly encountered in applications; see Figure 1.
2.1 Prediction in the Causal Direction
We begin with the case where X is a cause of Y .
Example 2.1. We want to automatically classify the quality of product reviews. Each review has a number of “helpful” votes Y (from site users). We predict Y using the text of the product review X . However, we ﬁnd interventions on the sentiment Z of the text change our prediction; changing “Great shoes!” to “Bad shoes!” changes the prediction.
In the examples in this paper, the covariate X is text data. Usually, the causal relationship between the text and Y and Z will be complex—e.g., the relationships may depend on abstract, unlabeled, parts of the text such as topic, writing quality, or tone. In principle, we could enumerate all such latent variables, construct a causal graph capturing the relationships between these variables and Y, Z, and use this causal structure to study counterfactual invariance. For instance, if we think that topic causally inﬂuences the helpfulness Y , but is
2

X Z⊥

Z

X Y ∧Z

Y

X Y⊥

Y

X Z⊥

X Y ∧Z
Z X Y⊥

(a) Causal direction

(b) Anticausal direction

Figure 1: Causal models for the data generating process. We decompose the observed covariate X into latent parts deﬁned by their causal relationships with Z and Y . Solid arrows denote causal relationships, while dashed lines denote non-causal associations. The differences between these causal structures will turn out to be key for understanding counterfactual invariance.

not inﬂuenced by sentiment Z, then we could build a counterfactually invariant predictor by extracting the topic and predicting Y using topic alone. However, exhaustively articulating all possible such variables is a herculean task.
Instead, notice that the only thing that’s relevant about these latent variables is their causal relationship with Y and Z. Accordingly, we’ll decompose the observed variable X into parts deﬁned by their causal relationships with Y and Z. We remain agnostic to the semantic interpretation of these parts. Namely, we deﬁne X Z⊥ as the part of X that is not causally inﬂuenced by Z (but may inﬂuence Y ), XY⊥ as the part that does not causally inﬂuence Y (but may be inﬂuenced by Z), and XY ∧Z is the remaining part that is both inﬂuenced by Z and that inﬂuences Y . The causal structure is shown in Figure 1a.
We see there are two paths that lead to Y and Z being associated. The ﬁrst is when Z affects XY ∧Z which, in turn, affects Y . For example, a very enthusiastic reviewer might write a longer, more detailed review, which will in turn be more helpful. The second is when a common cause or selection effect in the data generating process induces an association between Z and Y , which we denote with a dashed arrow. For example, if books tend to get more positive reviews, and also people who buy books are more likely to ﬂag reviews as helpful, then the product type would be a common cause of sentiment and helpfulness.
2.2 Prediction in the Anti-Causal Direction
We also consider the case where Y causes X .
Example 2.2. We want to predict the star rating Y of movie reviews from the text X . However, we ﬁnd that predictions are inﬂuenced by the movie genre Z; e.g., changing “Adam Sandler” (a comedy actor) to “Hugh Grant” (a romance actor) changes the predictions.
Figure 1b shows the causal structure. Here, the observed X is inﬂuenced by both Y and Z. Again, we decompose X into parts deﬁned by their causal relationship with Z and Y . Here, Z (and thus XY⊥) can be associated with Y through two paths. First, if XY ∧Z is non-trivial, then conditioning on it causes a dependence between Z and Y (because XY ∧Z is a collider). For example, if Adam Sandler tends to appear in good comedy movies but bad movies of other genres then seeing “Sandler” in the text induces a dependency between sentiment and genre. Second, Z and Y may be associated due to a common cause, or due to selection effects in the data collection protocol—this is represented by the dashed line between Z and Y . For example, fans of romantic comedies may tend to give higher reviews (to all ﬁlms) than fans of horror movies.

3

2.3 Non-Causal Associations
Frequently, a predictor trained to predict Y from X will rely on XY⊥, even though there is no causal connection between Y and XY⊥, and therefore will fail counterfactual invariance. The reason is that XY⊥ serves as a proxy for Z, and Z is predictive of Y due to the non-causal (dashed line) association.
There are two mechanisms that can induce such associations. First, Y and Z may be confounded: they are both inﬂuenced by an unobserved common cause U. For example, people who review books may be more upbeat than people who review clothing. This leads to positive sentiments and high helpfulness votes for books, creating an association between sentiment and helpfulness. Second, Y and Z may be subject to selection: there is some condition (event) S that depends on Y and Z, such that a data point from the population is included in the sample only if S = 1 holds. For example, our training data might only include movies with at least 100 reviews. If only excellent horror movies have so many reviews (but most rom-coms get that many), then this selection would induce an association between genre and score. Formally, the dashed-line causal graphs mean our sample is distributed according to P(X , Y, Z) = P(X , Y, Z, u | S = 1)dP(u) where Y, Z are caused by U and are causes of S, and (X , Y, Z) are causally related according to the graph.
In addition to the non-causal dashed-line relationship, there is also dependency induced by between Y and Z by XY ∧Z . Whether or not each of these dependencies is “spurious” is a problem-speciﬁc judgement that must be made by each analyst based on their particular use case. E.g., using genre to predict sentiment may or may not be reasonable, depending on the actual application in mind. However, there is a special case that captures a common intuition for purely spurious association.
Deﬁnition 2.3. We say that the association between Y and Z is purely spurious if Y ⊥ X | X Z⊥, Z. That is, if the dashed-line association did not exist (removed by conditioning on Z) then the part of X that is not inﬂuenced by Z would sufﬁce to estimate Y .
3 Observable Signatures of Counterfactually Invariant Predictors
We now consider the question of how to achieve counterfactual invariance in practice. The challenge is that counterfactual invariance is deﬁned by the behavior of the predictor on counterfactual data that is never actually observed. This makes checking counterfactual invariance impossible. Instead, we’ll derive a signature of counterfactual invariance that actually can be measured—and enforced—using ordinary datasets where Z (or a proxy) is measured. For example, the star rating of a review as a proxy for sentiment, or genre labels in the movie review case.
Intuitively, a predictor f is counterfactually invariant if it depends only on X Z⊥, the part of X that is not affected by Z. To formalize this, we need to show that such a X Z⊥ is well deﬁned:
Lemma 3.1. Let X Z⊥ be a X -measurable random variable such that, for all measurable functions f , we have that f is counterfactually invariant if and only if f (X ) is X Z⊥-measurable. If Z is discrete1 then such a X Z⊥ exists. Accordingly, we’d like to construct a predictor that is a function of X Z⊥ only (i.e., is X Z⊥ measurable). The key insight is that we can use the causal graphs to read off a set of
1In fact, it sufﬁces that all potential outcomes {Y (z)}z are jointly measurable with respect to a single wellbehaved sigma algebra; discrete Z is sufﬁcient but not necessary.
4

conditional independence relationships that are satisﬁed by Z, X Z⊥, Y . Critically, these conditional independence relationships are testable from the observed data. Thus, they provide a signature of counterfactual invariance:
Theorem 3.2. If f is a counterfactually invariant predictor:
1. Under the anti-causal graph, f (X ) ⊥ Z | Y .
2. Under the causal-direction graph, if Y and Z are not subject to selection (but possibly confounded), f (X ) ⊥ Z.
3. Under the causal-direction graph, if the association is purely spurious, Y ⊥ X | X Z⊥, Z, and Y and Z are not confounded (but possibly selected), f (X ) ⊥ Z | Y .
Remark 3.3 (Connection to Fairness). This result has an interesting interpretation when Z is a protected attribute (e.g., sex or race) that we’d like to be fair with respect to. There are many ways of formalizing fairness, which are usually mutually incompatible. In the fairness setting, counterfactual invariance is equivalent to counterfactual fairness [Kus+17; Gar+19], the condition f (X ) ⊥ Z is equivalent to demographic parity, and the condition f (X ) ⊥ Z | Y is equivalent to equalized odds [Meh+19]. Theorem 3.2 then says that counterfactual fairness implies either demographic parity or equalized odds, depending on the true underlying causal structure of the problem. Hence, the relationship between disparate fairness notions is clariﬁed by taking the underlying causal structure into account. This also suggests we can take counterfactual fairness as the fundamental notion, then use demographic parity in the causal-confounding case and equalized odds otherwise. However, we leave the (ethical) question of whether this is a sound strategy to future work.

Causal Regularization Without access to counterfactual examples, we cannot directly enforce counterfactual invariance. However, we can require a trained model to satisfy the counterfactual invariance signature of Theorem 3.2. The hope is that enforcing the signature will lead the model to be counterfactually invariant. To do this, we regularize the model to satisfy the appropriate conditional independence condition. For simplicity of exposition, we restrict to binary Y and Z. The (inﬁnite data) regularization terms are

marginal regularization = MMD(P( f (X ) | Z = 0), P( f (X ) | Z = 1))

(3.1)

conditional regularization = MMD(P( f (X ) | Z = 0, Y = 0), P( f (X ) | Z = 1, Y = 0))

+ MMD(P( f (X ) | Z = 0, Y = 1), P( f (X ) | Z = 1, Y = 1)). (3.2)

Maximum mean discrepancy (MMD) is a metric on probability measures.2 The marginal independence condition is equivalent to (3.1) equal 0, and the conditional independence is equivalent to (3.2) equal 0. In practice, we can estimate the MMD with ﬁnite data samples [Gre+12]. When training with stochastic gradient descent, we compute the penalty on each minibatch.

The procedure is then: if the data has causal-direction structure and the Y ↔ Z association is due to confounding, add the marginal regularization term to the the training objective. If the data has anti-causal structure, or the association is due to selection, add the conditional regularization term instead. In this way, we regularize towards models that satisfy the counterfactual invariance signature.

A key point is that the regularizer we must use depends on the true causal structure. The conditional and marginal independence conditions are generally incompatible. Enforcing the condition that is mismatched to the true underlying causal structure may fail to encourage counterfactual invariance, or may throw away more information than is required.

2The choice of regularization by MMD is for concreteness. Any technique for enforcing the independence signatures would do in principle—e.g., adversarial methods borrowed from the fairness literature. The key point here is the observation that different causal structures imply different independence signatures.

5

Gap to Counterfactual Invariance The conditional independence signature of Theorem 3.2 is necessary but not sufﬁcient for counterfactual invariance. This is for two reasons. First, counterfactual invariance applies to individual datapoint realizations, but the signature is distributional. In particular, the invariance P( f (X ) | do(Z = z)) = P( f (X ) | do(Z = z )) for all z, z would also imply the conditional independence signature. But, this invariance is weaker than counterfactual invariance, since it doesn’t require access to counterfactual realizations. Second, f (X ) ⊥ Z does not imply, in general, that Z is not a cause of f (X ). This (unusual) behavior can happen if, e.g., there are levels of Z that we do not observe in the training data, or there are variables omitted from the causal graph that are a common cause of Z and X .
Unfortunately, the gap between the signature and counterfactual invariance is a fundamental restriction of using observational data. The conditional independence signature is in some sense the closest proxy for counterfactual invariance we can hope for. In section 5, we’ll see that enforcing the signature does a good job of enforcing counterfactual invariance in practice.
4 Performance Out of Domain
Counterfactual invariance is an intuitively desirable property for a predictor to have. However, it’s not immediately clear how it relates to model performance as measured by, e.g., accuracy. Intuitively, eliminating predictor dependence on a spurious Z may help with domain shift, where the data distribution in the target domain differs from the distribution of the training data. We now turn to formalizing this idea.
First, we must articulate the set of domain shifts to be considered. In our setting, the natural thing is to hold the causal relationships ﬁxed across domains, but to allow the non-causal (“spurious”) dependence between Y and Z to vary. Demanding that the causal relationships stay ﬁxed reﬂects the requirement that the causal structure describes the dynamics of an underlying real-world process—e.g., the author’s sentiment is always a cause (not an effect) of the text in all domains. On the other hand, the dependency between Y and Z induced by either confounding or selection can vary without changing the underlying causal structure. For confounding, the distribution of the confounder may differ between domains—e.g., books are rare in training, but common in deployment. For selection, the selection criterion may differ between domains—e.g., we include only frequently reviewed movies in training, but make predictions for all movies in deployment.
We want to capture spurious domain shifts by considering domain shifts induced by selection or confounding. However, there is an additional nuance. Changes to the marginal distribution of Y will affect the risk of a predictor, even in the absence of any spurious association between Y and Z. Therefore, we restrict to shifts that preserve the marginal distribution of Y .
Deﬁnition 4.1. We say that distributions P, Q are causally compatible if both obey the same causal graph, P(Y ) = Q(Y ), and there is a confounder U and/or selection conditions S, S˜ such that P = P(X , Y, Z | U, S = 1)dP˜(U) and Q = P(X , Y, Z | U, S˜ = 1)dQ˜(U) for some P˜(U), Q˜(U).
We can now connect counterfactual invariance and robustness to domain shift.
Theorem 4.2. Let invar be the set of all counterfactually invariant predictors. Let L be either square error or cross entropy loss. And, let f ∗ := argminf ∈ invar P [L(Y, f (X ))] be the counterfactually invariant risk minimizer. Suppose that the target distribution Q is causally compatible with the training distribution P. Suppose that any of the following conditions hold:
1. the data obeys the anti-causal graph
6

2. the data obeys the causal-direction graph, there is no confounding (but possibly selection), and the association is purely spurious, Y ⊥ X | X Z⊥, Z, or
3. the data obeys the causal-direction graph, there is no selection (but possibly confounding), the association is purely spurious and the causal effect of X Z⊥ on Y is additive, i.e., the true data generating process is

Y

←

g

(

X

⊥ Z

)

+

g˜(U) + ξ

where

[ξ | X Z⊥] = 0,

(4.1)

for some functions g, g˜.

Then, the training domain counterfactually invariant risk minimizer is also the target domain counterfactually invariant risk minimizer, f ∗ = argminf ∈ invar Q[L(Y, f (X ))].
Remark 4.3. The causal case with confounding requires an additional assumption (additive structure) because, e.g., an interaction between confounder and X Z⊥ can yield a case where X Z⊥ and Y have a different relationship in each domain (whence, out-of-domain learning is impossible).

This result gives a recipe for ﬁnding a good predictor in the target domain even without access to any target domain examples at training time. Namely, ﬁnd the counterfactually invariant risk minimizer in the training domain. In practice, we can use the regularization scheme of section 3 to (approximately) achieve this. We’ll see in section 5 that this works well in practice.

Optimality Theorem 4.2 begs the question: if the only thing we know about the target setting is that it’s causally compatible with the training data, is the best predictor the counterfactually invariant predictor with lowest training risk? A natural way to formalize this question is to study the predictor with the best performance in the worst case target distribution. We deﬁne = {Q : Q causally compatible with P} and the -minimax predictor fm∗inimax = argminf ∈ maxQ∈ Q[L(Y, f (X )]. The question is then: what’s the relationship between the counterfactually invariant risk minimizer and the minimax predictor?
Theorem 4.4. The counterfactually invariant risk minimizer is not -minimax in general. However, under the conditions of Theorem 4.2, if the association is purely spurious, XY ∧Z ⊥ Y | X Z⊥, Z, and P(Z, Y ) satisﬁes overlap, then the two predictors are the same. By overlap we mean that P(Z, Y ) is a discrete distribution such that for all (z, y), if P(z, y) > 0 then there is some y = y such that also P(z, y ) > 0.
Conceptually, Theorem 4.4 just says that the counterfactually invariant predictor excludes XY ∧Z , even when this information is useful in every domain. In the purely spurious case, XY ∧Z carries no useful information, so counterfactual invariance is optimal.

5 Experiments
The main claims of the paper are: 1. Stress test violations can be reduced by (conditional) independence regularization. 2. This reduction will improve out-of-domain prediction performance. 3. To get the full effect, the imposed penalty must match the causal structure of the data.
Setup To assess these claims, we’ll examine the behavior of predictors trained with the marginal or conditinal regularization on multiple text datasets that have either causal or anti-causal structure. We expect to see that marginal regularization improves stress test and out-of-domain performance on data with causal-confounded structure, and conditional regularization improves these on data with anti-causal structure.

7

|P(1|x) P(1|x)| Pr(Y(X) Y(X))

0.10

test accuracy

0.100

penalty coeff

0.56 0.64

0.075

1 10

0.05

0.72

0.050

100

0.80 penalty type

0.025

1000 penalty type

0.00

conditional MMD 0.000

conditional MMD

0.002 0.003 0.004 0.005 conditional MMD

marginal MMD

0.84

0.85

0.86

0.87

test accuracy

marginal MMD

Figure 2: Regularizing conditional MMD improves counterfactual invariance on synthetic anticausal data. Sufﬁciently high regularization of marginal MMD also improves invariance, but impairs accuracy. Dashed lines show baseline performance of an unregularized predictor. Left: lower conditional MMD implies that predictive probabilities are invariant to perturbation. Although marginal MMD penalization can result in low conditional MMD and good stress test performance, this comes at the cost of very low in-domain accuracy. Right: MMD regularization reduces the rate of predicted label ﬂips on perturbed data, with little affect on in-domain accuracy. Conditional MMD regularization reduces predicted label ﬂips to 1.4%, while the best result for marginal MMD is 2.8%.

For each experiment, we use BERT [Dev+19] ﬁnetuned to predict a label Y from the text as our base model. We train multiple causally-regularized models on the each dataset. The training varies by whether we use the conditional or marginal penalty, and by the strength of the regularization term. That is, we train identical architectures using CrossEntropy + λ · Regularizer as the objective function, where we vary λ and take Regularizer as either the marginal penalty, (3.1), or conditional penalty, (3.2). We compare these models’ predictions on data with causal and anti-causal structure.
See supplement for experimental details.

5.1 Robustness to Stress Tests
First, we examine whether enforcing the causal regularization actually helps to enforce counterfactual invariance. We create counterfactual (stress test) examples by perturbing the input data and compare the prediction on these. We build the experimental datasets using Amazon reviews from the product category “Clothing, Shoes, and Jewelry” [NLM19].

Synthetic To study the relationship between counterfactual invariance and the distributional signature of Theorem 3.2, we construct a synthetic confound. For each review, we draw a Bernoulli random Z, and then perturb the text X so that the common words “the” and “a” carry information about Z: for example, we replace “the” with the token “thexxxxx” when Z = 1. We take Y to be the review score, and subsample so Y is balanced. This data has anti-causal structure: the text X is written to explain the score Y . Further, we expect that the Y, Z association is purely spurious, because “the” and “a” carry little information about the label.
We train the models on data where P(Y = Z) = 0.3. We then create perturbed stress-test datasets by changing each example Xi(z) to the counterfactual Xi(1 − z) (using the synthetic model). By measuring the performance of each model on the perturbed data, we can test whether the distributional properties enforced by the regularizers result in counterfactual invariance at the instance level. Figure 2 shows that conditional regularization (matching the anti-causal structure) reduces checklist failures, as measured by the frequency that the predicted label changes due to perturbation as well as the mean absolute difference in predictive probabilities that is induced by perturbation.

Natural To study the relationship in real data, we use the review data in a different way. We now take Z to be the score, binarized as Z ∈ {1 or 2 stars, 4 or 5 stars}. We use this Z as a proxy for sentiment, and consider problems where sentiment should (plausibly) not have a causal effect on Y . For the causal prediction problem, we take Y to be the helpfulness score of the review (binarized as described below). This is causal because readers decide whether the review is helpful based on the text. For the anti-causal prediction problem, we

8

|P(1|x) P(1|x)|

Dataset = anticausal 0.040 0.035 0.030
1 2.6 7.0 18 49 128 penalty coeff

0.100 Dataset = causal

0.075

conditional marginal

0.050

0.025

1 2.6 7.0 18 49 128 penalty coeff

Figure 3: Penalizing the MMD matching the causal structure improves stress test performance on natural product review data. Note that penalizing the wrong MMD may not help: the marginal MMD hurts on the anticausal dataset. Perturbations are generated by swapping positive and negative sentiment adjectives in examples.

take Y to be whether “Clothing” is included as a category tag for the product under review (e.g., boots typically do not have this tag). This is anti-causal because the product category affects the text.
We control the strength of the spurious association between Y and Z. In the anti-causal case, this is done by selection: we randomly subset the data to enforce a target level of dependence between Y and Z. The causal-direction case with confounding is more complicated. To manipulate confounding strength, we binarize the number of helpfulness votes V in a manner determined by the target level of association. We take Y = 1[V > TZ ] where TZ is a Z-dependent threshold, chosen to induce a target association. We choose P(Y = 0 | Z = 0) = P(Y = 1 | Z = 1) = 0.3. We balance Z by subsampling, which also balances Y .
Now, we create stress test perturbations of these datasets by randomly changing adjectives in the examples. Using predeﬁned lists of postive sentiment adjectives and negative sentiment adjectives, we swap any adjective that shows up on a list with a randomly sampled adjective from the other list. This preserves basic sentence structure, and thus creates a limited set of counterfactual pairs that differ on sentiment.
Results for differences in predicted probabilities between original and perturbed data are shown in Figure 3. Each point is a trained model, which vary in measured MMD on the test data and on sensitivity to perturbations. Recall that the conditional independence signature of Theorem 3.2 are necessary but not sufﬁcient for counterfactual invariance, so it’s not certain that regularizing to reduce the MMD will reduce perturbation sensitivity. Happily, we see that regularizing to reduce the MMD that matches the causal structure does indeed reduce sensitivity to perturbations.
Notice that regularizing the causally mismatched MMD can have strange effects. Regularizing marginal MMD in the anti-causal case actually makes the model more sensitive to perturbations!
5.2 Domain Shift
Next, we study the effect of causal regularization on model performance under domain shift.
Natural Product Review We again use the natural Amazon review data described above. For both the causal and anti-causal data, we create multiple test sets with variable spurious correlation strength. This is done in the manner described above, varying P(Y = 0 | Z = 0) = P(Y = 1 | Z = 1) = γ. Here, γ is the strength of spurious association. The test sets are out-of-domain samples. By design, Y is balanced in each dataset, so these samples are causally compatible with the training data. For both the causal and anti-causal datasets, the training data has P(Y = 0 | Z = 0) = P(Y = 1 | Z = 1) = 0.3. We train a classiﬁer for

9

Accuracy

Accuracy

0.90 0.88 0.85 0.83 0.80 0.2 0.4 0.6 0.8
Ptest(Y = 1|Z = 1) = Ptest(Y = 0|Z = 0)

Worst Domain Accuracy

0.90

conditional MMD 0.004

0.88

0.008

0.85

0.012 0.016

0.83

0.020

0.80

penalty type conditional

0.800 0.825 0.850 0.875 0.900 Test Accuracy

marginal none

Worst Domain Accuracy

Anti-Causal Data: conditional regularization improves domain-shift robustness.
0.80

0.70

0.70

0.60

0.60

0.50 0.2 0.4 0.6 0.8 Ptest(Y = 1|Z = 1) = Ptest(Y = 0|Z = 0)

0.50 0.5 0.6 0.7 Test Accuracy

marginal MMD 0.004 0.008 0.012 0.016 0.020
penalty type conditional marginal none

Causal-Direction Data: marginal regularization improves domain-shift robustness.

Figure 4: The best domain-shift robustness is obtained by using the regularizer that matches the underlying causal structure of the data. The plots show out-of-domain accuracy for models trained on the (natural) review data. In each row, the left ﬁgure shows out-of-domain accuracies (lines are models), with the X -axis showing the level of spurious correlation in the test data (0.3 is the training condition); the right ﬁgure shows worst out-of-domain accuracy versus in-domain test accuracy (dots are models).

each regularization type and regularization strength, and measure the accuracy on each test domain. The results are shown in Figure 4.
First, the unregularized predictors do indeed learn to rely on the spurious association between sentiment and the label. The accuracy of these predictors decays dramatically as the spurious assocation moves from negative (0.3) to positive—in the causal case, the unregularized predictor is worse than chance in the 0.8 domain.
Following section 3, the regularization that matches the underlying causal structure should yield a predictor that is (approximately) counterfactually invariant. Following Theorem 4.2, we expect that good performance of a counterfactually-invariant predictor in the training domain should imply good performance in each of the other domains. Indeed, we see that this is so. Models that are regularized to have small values of the appropriate MMD do indeed have better out-of-domain performance. Such models have somewhat worse in-domain performance, because they no longer exploit the spurious correlation.
MNLI Data For an additional test on naturally-occurring confounds, we use the multigenre natural language inference (MNLI) dataset [WNB18]. Instances are concatenations of two sentences, and the label describes the semantic relationship between them, Y ∈ {contradiction, entailment, neutral}. There is a well-known confound in this dataset: examples where the second sentence contain a negation word (e.g., “not”) are much more likely to be labeled as contradictions [Gur+18]. Following Sagawa et al. [Sag+20], we set Z to indicate whether one of a small set of negation words is present. Although Z is derived from the text X , it can be viewed as a proxy for a latent variable indicating whether the author intended to use negation in the text. This is an anti-causal prediction problem: the annotators were instructed to write text to reﬂect the desired label [WNB18].
Following Sagawa et al. [Sag+20], we divide the MNLI data into groups by (Y, Z) and compute the “worst group accuracy” across all such groups. Because this is an anti-causal

10

worst group accuracy worst group accuracy

0.6 0.4 0.2 0.0
0.0 0.5 2.0 8.0 32.0 128.0 penalty coeff

0.75

0.70

conditional MMD marginal MMD

0.65

0.60

0.55

0.500.75 0.76 0.77 0.78 0.79 0.80 0.81 0.82 overall accuracy

Figure 5: Conditional MMD penalization improves robustness in anti-causal MNLI data. Marginal
regularization does not improve over the baseline unregularized model, shown with dashed lines. Left: Conditional regularization improves minimum accuracy across (Y, Z) groups. When overregularized, the predictor returns the same Yˆ for all inputs, yielding a worst-group accuracy of 0. Right: Conditional MMD regularization signiﬁcantly improves worst (Y, Z) group accuracy ( y-axis) while only mildly reducing overall accuracy (x-axis).

problem, we predict that the conditional MMD is a more appropriate penalty than the marginal MMD. As shown in Figure 5, this prediction holds: conditional MMD regularization dramatically improves performance on the worst group, while only lightly impacting the overall accuracy across groups.

6 Related work
Several papers draw a connection between causality and domain shifts [SS18; SCS19; Arj+20; Mei18; PBM16; RC+18; Zha+13; Sch+12]. Typically, this work considers a prediction setting where the covariates X include both causes and effects of Y , and it is unknown which is which. The goal is to learn to predict Y using only its causal parents. Zhang et al. [Zha+13] considers anti-causal domain shift induced by changing P(Y ) and proposes a data reweighting scheme. By contrast, counterfactual invariance is an example-wise notion of invariance, not an invariance across environments. In particular, learning a counterfactually invariant predictor only requires access to data from a single environment. Theorem 4.2 establishes that counterfactual invariance does imply a certain domain generalization guarantee over causally-compatible domains. Note though that the notion of causal compatibility is not generally the same as class of domain shifts previously considered. For example, we have invariant performance in the anti-causal setting, but this is ruled out by Arjovsky et al. [Arj+20].
A related body of work focuses on “causal representation learning” [Bes+19; Loc+20; Sch+21; Arj+20]. Our approach follows this tradition, but focuses on splitting X into components deﬁned by their causal relationships with the label Y and an additional covariate Z. Rather than attempting to infer the causal relationship between X and Y , we show that domain knowledge of this relationship is essential for obtaining counterfactually-invariant predictors. The role of causal vs anti-causal data generation in semi-supervised learning and transfer learning has also been studied [Sch+21; Sch+12]. In this paper we focus on a different implication of the causal vs anti-causal distinction.
Another line of work considers the case where either the counterfactuals X (z), X (z ) are observed for at least some data points, or where it’s assumed that there’s enough structural knowledge to (learn to) generate counterfactual examples [RPH21; Wu+21; Gar+19; Mit+20; WZ19; KCC20; KHL20; TAH20]. Kusner et al. [Kus+17] and Garg et al. [Gar+19] in particular examine a notion of counterfactual fairness that can be seen as equivalent to counterfactual invariance. In these papers, approximate counterfactuals are produced by direct manipulation of the features (e.g., change male to female names), generative models (e.g., style transfer of images), or crowdsourcing. Then, these counterfactuals can either be used as additional training data or the predictor can be regularized such that it cannot distinguish between Xi(z) and Xi(z ). This strategy can be viewed as enforcing counterfactual invariance directly; an advantage is that it avoids the necessary-but-notsufﬁcient nuance of Theorem 3.2. However, counterfactual examples can be difﬁcult to

11

obtain for language data in many realistic problem domains, and it may be difﬁcult to learn to generalize from such examples [HLB20].
The marginal and conditional independencies of Theorem 3.2 have appeared in other contexts. As discussed in remark 3.3, if we think of Z as a protected attribute and f as a ‘fair’ classiﬁer, then counterfactual invariance is counterfactual fairness, the marginal independence is demographic parity, and the conditional independence is equalized odds [Meh+19]. In another setting, one approach to domain adaptation is to seek representations φ such that either φ(X ) [e.g., MBS13; Bak+13; Gan+16; Tze+14] or φ(X ) | Y [e.g., MLM19; Yan+17] have the same distribution in each environment. The connection here is subtler; it is inappropriate to interpret Z as a domain label (we only have access to one domain at train time, and all values of Z are present in each domain). To clarify the connection, consider introducing an extra variable E that labels the domain. For concreteness, consider the anti-causal case where the spurious association between Y and Z is due to an observed confounder U. Now, suppose that E is a cause of U. Then, E labels the distribution of the unobserved confounder, and thus different values of E correspond to different causally compatible domains. Now, if we draw the causal graph we can read off that Xz⊥ ⊥ E|Y . That is, we arrive at the same conditional independence criterion. However, it’s essentially a coincidence that this matches the criterion we’d use if we’d observed Z—the two variables have totally different causal meanings and interpretations. For example, in the review data case, we might take Z to be review score and E to be the website the review is collected from. The review quality doesn’t need to be counterfactually invariant to the website, and indeed we wouldn’t expect it to be!
Finally, this work can be viewed as part of a recent line of work using counterfactuals to examine the connection between example-wise robustness (stress testing) and distributionallevel robustness (domain shift) [e.g., TAH20; RPH21; Kau+21]. Teney et al. [TAH20] use near counterfactuals with different labels in a modiﬁed training objective, and show improved performance out-of-domain empirically. Robey et al. [RPH21] use image models to generate counterfactuals, and incorporate the model into training, showing out-of-domain improvement. They assume a somewhat different causal structure—articulating the precise connection is an interesting future direction. Kaushik et al. [Kau+21] study a linear-Gaussian model and argue that out-of-domain performance can be used as a signal to distinguish ‘causal’ and ‘spurious’ features.
7 Discussion
We used the tools of causal inference to formalize and study perturbative stress tests. A main insight of the paper is that counterfactual desiderata can be linked to observationally-testable conditional independence criteria. This requires consideration of the true underlying causal structure of the data. Done correctly, the link yields a simple procedure for enforcing the counterfactual desiderata, and mitigating the effects of domain shift.
The main limitation of the paper is the restrictive causal structures we consider. In particular, we require that X Z⊥, the part of X not causally affected by Z, is also statistically independent of Z in the observed data. However, in practice these may be dependent due to a common cause. In this case, the procedure here will be overly conservative, throwing away more information than required. Additionally, it is not obvious how to apply the ideas described here to more complicated causal situations, which can occur in structured prediction (e.g., question answering). Extending the ideas to handle richer causal structures is an important direction for future work. The work described here can provide a template for this research program.
12

8 Acknowledgments
Thanks to Kevin Murphy, Been Kim, Molly Roberts, Justin Grimmer, and Brandon Stewart for feedback on an earlier version.

References

[Arj+20] [Bak+13] [Bes+19] [Dev+19]
[Gan+16] [Gar+19] [Gre+12] [Gur+18]
[HLB20] [KHL20] [Kau+21] [KCC20] [Kus+17] [Loc+20] [MLM19] [Meh+19]

M. Arjovsky, L. Bottou, I. Gulrajani, and D. Lopez-Paz. Invariant Risk Minimization. 2020. arXiv: 1907.02893 [stat.ML]. M. Baktashmotlagh, M. T. Harandi, B. C. Lovell, and M. Salzmann. “Unsupervised domain adaptation by domain invariant projection”. In: Proceedings of the 2013 IEEE International Conference on Computer Vision. 2013. M. Besserve, A. Mehrjou, R. Sun, and B. Schölkopf. Counterfactuals uncover the modular structure of deep generative models. 2019. arXiv: 1812.03253 [cs.LG]. J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. “Bert: pre-training of deep bidirectional transformers for language understanding”. In: Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). 2019. Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle, F. Laviolette, M. Marchand, and V. Lempitsky. “Domain-adversarial training of neural networks”. In: J. Mach. Learn. Res. 1 (2016). S. Garg, V. Perot, N. Limtiaco, A. Taly, E. H. Chi, and A. Beutel. “Counterfactual fairness in text classiﬁcation through robustness”. In: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society. 2019. A. Gretton, K. M. Borgwardt, M. J. Rasch, B. Schölkopf, and A. Smola. “A kernel two-sample test”. In: The Journal of Machine Learning Research 1 (2012). S. Gururangan, S. Swayamdipta, O. Levy, R. Schwartz, S. Bowman, and N. A. Smith. “Annotation artifacts in natural language inference data”. In: Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers). 2018. W. Huang, H. Liu, and S. R. Bowman. Counterfactually-Augmented SNLI Training Data Does Not Yield Better Generalization Than Unaugmented Data. 2020. arXiv: 2010.04762 [cs.CL]. D. Kaushik, E. Hovy, and Z. C. Lipton. Learning the Difference that Makes a Difference with Counterfactually-Augmented Data. 2020. arXiv: 1909.12434 [cs.CL]. D. Kaushik, A. Setlur, E. Hovy, and Z. C. Lipton. Explaining The Efﬁcacy of Counterfactually Augmented Data. 2021. arXiv: 2010.02114 [cs.CL]. V. Kumar, A. Choudhary, and E. Cho. “Data augmentation using pre-trained transformer models”. In: Proceedings of the 2nd Workshop on Life-long Learning for Spoken Language Systems. 2020. M. J. Kusner, J. Loftus, C. Russell, and R. Silva. “Counterfactual fairness”. In: Advances in Neural Information Processing Systems. 2017. F. Locatello, B. Poole, G. Raetsch, B. Schölkopf, O. Bachem, and M. Tschannen. “Weakly-supervised disentanglement without compromises”. In: Proceedings of the 37th International Conference on Machine Learning. 2020. J. Manders, T. van Laarhoven, and E. Marchiori. Adversarial Alignment of Class Prediction Uncertainties for Domain Adaptation. 2019. arXiv: 1804.04448 [stat.ML]. N. Mehrabi, F. Morstatter, N. Saxena, K. Lerman, and A. Galstyan. A Survey on Bias and Fairness in Machine Learning. 2019. arXiv: 1908.09635 [cs.LG].

13

[Mei18] [Mit+20] [MBS13] [Nai+18] [NLM19] [PBM16] [Rib+20] [RPH21] [RC+18] [Sag+20]
[Sch+12] [Sch+21] [SCS19] [SS18]
[TAH20] [Tze+14] [WZ19]
[WNB18]

N. Meinshausen. “Causality from a distributional robustness point of view”. In: 2018 IEEE Data Science Workshop (DSW). 2018. J. Mitrovic, B. McWilliams, J. Walker, L. Buesing, and C. Blundell. Representation Learning via Invariant Causal Mechanisms. 2020. arXiv: 2010.07922 [cs.LG]. K. Muandet, D. Balduzzi, and B. Schölkopf. “Domain generalization via invariant feature representation”. In: Proceedings of the 30th International Conference on Machine Learning. 2013. A. Naik, A. Ravichander, N. Sadeh, C. Rose, and G. Neubig. “Stress test evaluation for natural language inference”. In: Proceedings of the 27th International Conference on Computational Linguistics. 2018. J. Ni, J. Li, and J. McAuley. “Justifying recommendations using distantly-labeled reviews and ﬁned-grained aspects”. In: Empirical Methods in Natural Language Processing (EMNLP) (2019). J. Peters, P. Bühlmann, and N. Meinshausen. “Causal inference by using invariant prediction: identiﬁcation and conﬁdence intervals”. In: Journal of the Royal Statistical Society. Series B (Statistical Methodology) 5 (2016). M. T. Ribeiro, T. Wu, C. Guestrin, and S. Singh. “Beyond accuracy: behavioral testing of nlp models with checklist”. In: Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. 2020. A. Robey, G. J. Pappas, and H. Hassani. Model-Based Domain Generalization. 2021. arXiv: 2102.11436 [stat.ML]. M. Rojas-Carulla, B. Schölkopf, R. Turner, and J. Peters. “Invariant models for causal transfer learning”. In: Journal of Machine Learning Research 36 (2018). S. Sagawa, P. W. Koh, T. B. Hashimoto, and P. Liang. “Distributionally robust neural networks for group shifts: on the importance of regularization for worstcase generalization”. In: International Conference on Learning Representations. 2020. B. Schölkopf, D. Janzing, J. Peters, E. Sgouritsa, K. Zhang, and J. Mooij. “On causal and anticausal learning”. In: Proceedings of the 29th International Coference on International Conference on Machine Learning. 2012. B. Schölkopf, F. Locatello, S. Bauer, N. R. Ke, N. Kalchbrenner, A. Goyal, and Y. Bengio. “Toward causal representation learning”. In: Proceedings of the IEEE 5 (2021). A. Subbaswamy, B. Chen, and S. Saria. A Universal Hierarchy of Shift-Stable Distributions and the Tradeoff Between Stability and Performance. 2019. arXiv: 1905.11374 [stat.ML]. A. Subbaswamy and S. Saria. “Counterfactual normalization: proactively addressing dataset shift and improving reliability using causal mechanisms”. In: Proceedings of the 34th Conference on Uncertainty in Artiﬁcial Intelligence (UAI), 2018. 2018. D. Teney, E. Abbasnejad, and A. van den Hengel. “Learning what makes a difference from counterfactual examples and gradient supervision”. In: CoRR (2020). arXiv: 2004.09034. E. Tzeng, J. Hoffman, N. Zhang, K. Saenko, and T. Darrell. Deep Domain Confusion: Maximizing for Domain Invariance. 2014. arXiv: 1412.3474 [cs.CV]. J. Wei and K. Zou. “EDA: easy data augmentation techniques for boosting performance on text classiﬁcation tasks”. In: Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 2019. A. Williams, N. Nangia, and S. Bowman. “A broad-coverage challenge corpus for sentence understanding through inference”. In: Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers). 2018.

14

[Wu+19] [Wu+21] [Yan+17] [Zha+13]

T. Wu, M. T. Ribeiro, J. Heer, and D. Weld. “Errudite: scalable, reproducible, and testable error analysis”. In: Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. 2019. T. Wu, M. T. Ribeiro, J. Heer, and D. S. Weld. “Polyjuice: generating counterfactuals for explaining, evaluating, and improving models”. In: Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics. 2021. H. Yan, Y. Ding, P. Li, Q. Wang, Y. Xu, and W. Zuo. Mind the Class Weight Bias: Weighted Maximum Mean Discrepancy for Unsupervised Domain Adaptation. 2017. arXiv: 1705.00609 [cs.CV]. K. Zhang, B. Schölkopf, K. Muandet, and Z. Wang. “Domain adaptation under target and conditional shift”. In: Proceedings of the 30th International Conference on Machine Learning. 2013.

15

A Proofs
Lemma 3.1. Let X Z⊥ be a X -measurable random variable such that, for all measurable functions f , we have that f is counterfactually invariant if and only if f (X ) is X Z⊥-measurable. If Z is discrete3 then such a X Z⊥ exists.

Proof. Write {X (z)}z for the potential outcomes. First notice that if f (X ) is {X (z)}zmeasurable then f (X ) is counterfactually invariant. This is essentially by deﬁnition—
intervention on Z doesn’t change the potential outcomes, so it doesn’t change the value
of f (X ). Conversely, if f is counterfactually invariant, then f (X ) is {X (z)}z-measurable. To see this, notice that X = z 1[Z = z]X (z) is determined by Z and {X (z)}z, so f (X ) = f˜(Z, {X (z)}z) for f˜(z, {x(z)}z) = f ( z 1[z = z]x(z)). Now, if f˜ depends only on {X (z)}z we’re done. So suppose that there is z, z such that f˜(z, {X (z)}z) = f˜(z , {X (z)}z) (almost everywhere). But then f (X (z)) = f (X (z )), contradicting counterfactual invariance.

Now, deﬁne X⊥ = σ(X ) ∧ σ({X (z)}z) as the intersection of sigma algebra of X and the Z

sigma algebra of the potential outcomes {X (z)}z. Because X⊥ is the intersection of sigma Z
algebras, it is itself a sigma algebra. Because every X⊥ -measurable random variable is Z

{X (z)}z-measurable, we have that Z is not a cause of any X⊥ -measurable random variable Z

(i.e., there is no arrow from Z to X Z⊥). Because, for f counterfactually invariant, f (X ) is

both X -measurable and {X (z)}z-measurable, it is also X⊥ -measurable. X⊥ is countably

Z

Z

generated, as {X (z)}z and X are both Borel measurable. Therefore, we can take X Z⊥ to be any random variable such that σ(X Z⊥) = XZ⊥ .

Theorem 3.2. If f is a counterfactually invariant predictor:
1. Under the anti-causal graph, f (X ) ⊥ Z | Y .
2. Under the causal-direction graph, if Y and Z are not subject to selection (but possibly confounded), f (X ) ⊥ Z.
3. Under the causal-direction graph, if the association is purely spurious, Y ⊥ X | X Z⊥, Z, and Y and Z are not confounded (but possibly selected), f (X ) ⊥ Z | Y .

Proof.

Reading

d -separation

from

the

causal

graphs,

we

have

X

⊥ Z

⊥

Z in the causal-direction

graph when Y and Z are not selected on, and X Z⊥ ⊥ Z | Y for the other cases. By assumption,

f is a counterfactually-invariant predictor, which means that f is X Z⊥-measurable.

Theorem 4.2. Let invar be the set of all counterfactually invariant predictors. Let L be either square error or cross entropy loss. And, let f ∗ := argminf ∈ invar P [L(Y, f (X ))] be the counterfactually invariant risk minimizer. Suppose that the target distribution Q is causally
compatible with the training distribution P. Suppose that any of the following conditions hold:

1. the data obeys the anti-causal graph

2. the data obeys the causal-direction graph, there is no confounding (but possibly selection), and the association is purely spurious, Y ⊥ X | X Z⊥, Z, or
3. the data obeys the causal-direction graph, there is no selection (but possibly confounding), the association is purely spurious and the causal effect of X Z⊥ on Y is additive, i.e., the true data generating process is

Y

←

g

(

X

⊥ Z

)

+

g˜(U) + ξ

where

[ξ | X Z⊥] = 0,

(4.1)

3In fact, it sufﬁces that all potential outcomes {Y (z)}z are jointly measurable with respect to a single wellbehaved sigma algebra; discrete Z is sufﬁcient but not necessary.

16

for some functions g, g˜.
Then, the training domain counterfactually invariant risk minimizer is also the target domain counterfactually invariant risk minimizer, f ∗ = argminf ∈ invar Q[L(Y, f (X ))].

Proof. First, since counterfactual invariance implies X Z⊥-measurable,

argmin P [L(Y, f (X )] = argmin P [L(Y, f (X Z⊥)].

f ∈ invar

f

(A.1)

It is well-known that under squared error or cross entropy loss the minimizer is f ∗(xZ⊥) = P [Y | xZ⊥]. By the same argument, the counterfactually invariant risk minimizer in the
target domain is Q[Y | xZ⊥]. Thus, our task is to show P [Y | xZ⊥] = Q[Y | xZ⊥].

We begin with the anti-causal case. We have that P(Y | X Z⊥) = P(X Z⊥ | Y )P(Y )/

P

(X

⊥ Z

|

Y

)d

P

(Y

).

By assumption, P(Y ) = Q(Y ). So, it sufﬁces to show that P(X Z⊥ | Y ) = Q(X Z⊥ | Y ). To that end, from the anti-causal direction graph we have that X Z⊥ ⊥ S, U | Y . Then,

P(X Z⊥ | Y ) = P(X Z⊥ | Y, U, S = 1)dP˜(U)

(A.2)

= P(X Z⊥ | Y, U, S˜ = 1)dQ˜(U)

(A.3)

= Q(X Z⊥ | Y ),

(A.4)

where

the

ﬁrst

and

third

lines

are

causal

compatibility,

and

the

second

line

is

from

X

⊥ Z

⊥

S, S˜, U | Y .

The causal-direction case with no confounding follows essentially the same argument.

For the causal-direction case without selection,

P [Y

|

X Z⊥] =

g

(

X

⊥ Z

)

+

= g(X Z⊥) +

P [g˜(U) | X Z⊥] + P [g˜(U)] + 0.

P [ξ | X Z⊥]

(A.5) (A.6)

The ﬁrst line is the assumed additivity. The second line follows because P [ξ | X Z⊥] = 0

for

all

causally

compatible

distributions

(P

(ξ

,

X

⊥ Z

)

doesn’t

change),

and

U

⊥

X Z⊥.

Taking

an expectation over X Z⊥, we have

P[Y ] =

P

[

g

(X

⊥ Z

)]

+

P [g˜(U)]. By the same token,

Q[Y ] = Q[g(X Z⊥)] + Q[g˜(U)]. But, P [g(X Z⊥)] = Q[g(X Z⊥)], since changes to the

confounder don’t change the distribution of X Z⊥ (that is, X Z⊥ ⊥ U). And, by assumption,

Q[Y ] = P [Y ]. Together, these imply that P [g˜(U)] = Q[g˜(U)]. Whence, from (A.6),

we have P [Y | X Z⊥] = Q[Y | X Z⊥], as required.

Theorem 4.4. The counterfactually invariant risk minimizer is not -minimax in general. However, under the conditions of Theorem 4.2, if the association is purely spurious, XY ∧Z ⊥ Y | X Z⊥, Z, and P(Z, Y ) satisﬁes overlap, then the two predictors are the same. By overlap we mean that P(Z, Y ) is a discrete distribution such that for all (z, y), if P(z, y) > 0 then there is some y = y such that also P(z, y ) > 0.

Proof. The reason that the predictors are not the same in general is that the counterfactually
invariant predictor will always exclude information in XY ∧Z , even when this information is helpful for predicting Y in all target settings. For example, consider the case where Y, Z are
binary, X = XY ∧Z and, in the anti-causal direction, XY ∧Z = AND(Y, Z). With cross-entropy loss, the counterfactually invariant predictor is just the constant [Y ], but the decision rule that uses f (X ) = 1 if X = 1 is always better. In the causal case, consider XY ∧Z = Z and Y = XY∧Z.

17

Informally, the second claim follows because—in the absence of XY ∧Z information—any predictor f that’s better than the counterfactually invariant predictor when Y and Z are
positively correlated will be worse when Y and Z are negatively correlated.

To formalize this, we begin by considering the case where Y is binary and X = XY⊥. So, in particular, the counterfactually invariant predictor is just some constant c. Let f be any

predictor that uses the information in XY⊥. Our goal is to show that Q[L( f (XY⊥), Y )] > Q[L(c, Y )] for at least one test distribution (so that f is not minimax). To that end, let P be
any distribution where f (XY⊥) has lower risk than c (this must exist, or we’re done). Then, deﬁne A = {(z, y) : P [L( f (XY⊥), y) | z] < L(c, y)}. In words: A is the collection of z, y points where f did better than the constant predictor. Since f is better than the constant

predictor overall, we must have P(A) > 0. Now, deﬁne Ac = {(z, 1 − y) : (z, y) ∈ A}. That

is, the set constructed by ﬂipping the label for every instance where f did better. By the

overlap assumption, P(Ac) > 0. By construction, f is worse than c on Ac. Further, S = 1A is

a random variable that has the causal structure required by a selection variable (it’s a child

of Y and Z and nothing else). So, the distribution Q deﬁned by selection on S is causally

compatible with P and satisﬁes

Q

[

L

(

f

(X

⊥ Y

),

Y

)]

>

Q[L(c, Y )], as required.

To relax the requirement that X = XY⊥, just repeat the same argument conditional on each value of X Z⊥. To relax the condition that Y is binary, swap the ﬂipped label 1 − y for any label y with worse risk.

B Experimental Details
B.1 Model
All experiments use BERT as the base predictor. We use bert_en_uncased_L-12_H768_A-12 from TensorFlow Hub and do not modify any parameters. Following standard practice, predictions are made using a linear map from the representation layer. We use CrossEntropy loss as the training objective. We train with vanilla stochastic gradient descent, batch size 1024, and learning rate 1e − 5 × 1024. We use patience 10 early stopping on validation risk. Each model was trained using 2 Tensor Processing Units.
For the MMD regularizer, we use the estimator of Gretton et al. [Gre+12] with the Gaussian RBF kernel. We set kernel bandwidth to 10.0. We compute the MMD on (log f0(x), . . . , log fk(x)), where f j(x) is the model estimate of P(Y = k | x). (Note: this is log, not logit—the later has an extra, irrelevant, degree of freedom). We use log-spaced regularization coefﬁcients between 0 and 128.
B.2 Data
We don’t do any pre-processing on the MNLI data.
The Amazon review data is from [NLM19].
B.2.1 Inducing Dependence Between Y and Z in Amazon Product Reviews
To produce the causal data with P(Y = 1 | Z = 1) = P(Y = 0 | Z = 0) = γ 1. Randomly drop reviews with 0 helpful votes V , until both P(V > 0 | Z = 1) > γ and P(V > 0 | Z = 0) > 1 − γ. 2. Find the smallest Tz such that P(V > T1 | Z = 1) < γ and P(V > T0 | Z = 0) < 1 − γ. 3. Set Y = 1[V > T0] for each Z = 0 example and Y = 1[V > T1] for each Z = 1 example.

18

4. Randomly ﬂip Y = 0 to Y = 1 in examples where (Z = 0, V = T0 + 1) or (Z = 1, V = T1 + 1), until P(Y = 1 | Z = 1) > γ and P(Y = 1 | Z = 0) > 1 − γ.
After data splitting, we have 58393 training examples, 16221 test examples, and 6489 validation examples. To produce the anti-causal data with P(Y = 1 | Z = 1) = P(Y = 0 | Z = 0) = γ, choose a random subset with the target association. After data splitting, we have 157616 training examples, 43783 test examples, and 17513 validation examples. B.2.2 Synthetic Counterfactuals in Product Review Data We select 105 product reviews from the Amazon “clothing, shoes, and jewelery” dataset, and assign Y = 1 if the review is 4 or 5 stars, and Y = 0 otherwise. For each review, we use only the ﬁrst twenty tokens of text. We then assign Z as a Bernoulli random variable with P(Z = 1) = 12 . When Z = 1, we replace the tokens “and” and “the” with “andxxxxx” and “thexxxxx” respectively; for Z = 0 we use the sufﬁx “yyyyy” instead. Counterfactuals can then be produced by swapping the sufﬁxes. To induce a dependency between Y and Z, we randomly resample so as to achieve γ = 0.3 and P(Y = 1) = 12 , using the same procedure that was used on the anti-causal model of “natural” product reviews. After selection there are 13, 315 training instances and 3, 699 test instances.
19

