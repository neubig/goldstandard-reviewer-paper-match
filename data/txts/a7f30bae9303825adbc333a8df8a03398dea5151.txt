Revisiting the Importance of Encoding Logic Rules in Sentiment Classiﬁcation

Kalpesh Krishna♠♣

Preethi Jyothi♠

Mohit Iyyer♣

Indian Institute of Technology, Bombay♠ University of Massachusetts, Amherst♣ {kalpesh,miyyer}@cs.umass.edu
pjyothi@cse.iitb.ac.in

arXiv:1808.07733v1 [cs.CL] 23 Aug 2018

Abstract
We analyze the performance of different sentiment classiﬁcation models on syntacticallycomplex inputs like A-but-B sentences. The ﬁrst contribution of this analysis addresses reproducible research: to meaningfully compare different models, their accuracies must be averaged over far more random seeds than what has traditionally been reported. With proper averaging in place, we notice that the distillation model described in Hu et al. (2016), which incorporates explicit logic rules for sentiment classiﬁcation, is ineffective. In contrast, using contextualized ELMo embeddings (Peters et al., 2018a) instead of logic rules yields signiﬁcantly better performance. Additionally, we provide analysis and visualizations that demonstrate ELMo’s ability to implicitly learn logic rules. Finally, a crowdsourced analysis reveals how ELMo outperforms baseline models even on sentences with ambiguous sentiment labels.
1 Introduction
In this paper, we explore the effectiveness of methods designed to improve sentiment classiﬁcation (positive vs. negative) of sentences that contain complex syntactic structures. While simple bag-of-words or lexicon-based methods (Pang and Lee, 2005; Wang and Manning, 2012; Iyyer et al., 2015) achieve good performance on this task, they are unequipped to deal with syntactic structures that affect sentiment, such as contrastive conjunctions (i.e., sentences of the form “A-but-B”) or negations. Neural models that explicitly encode word order (Kim, 2014), syntax (Socher et al., 2013; Tai et al., 2015) and semantic features (Li et al., 2017) have been proposed with the aim of improving performance on these more complicated sentences. Recently, Hu et al. (2016) incorporate logical rules into a neural model and

show that these rules increase the model’s accuracy on sentences containing contrastive conjunctions, while Peters et al. (2018a) demonstrate increased overall accuracy on sentiment analysis by initializing a model with representations from a language model trained on millions of sentences.
In this work, we carry out an in-depth study of the effectiveness of the techniques in Hu et al. (2016) and Peters et al. (2018a) for sentiment classiﬁcation of complex sentences. Part of our contribution is to identify an important gap in the methodology used in Hu et al. (2016) for performance measurement, which is addressed by averaging the experiments over several executions. With the averaging in place, we obtain three key ﬁndings: (1) the improvements in Hu et al. (2016) can almost entirely be attributed to just one of their two proposed mechanisms and are also less pronounced than previously reported; (2) contextualized word embeddings (Peters et al., 2018a) incorporate the “A-but-B” rules more effectively without explicitly programming for them; and (3) an analysis using crowdsourcing reveals a bigger picture where the errors in the automated systems have a striking correlation with the inherent sentiment-ambiguity in the data.
2 Logic Rules in Sentiment Classiﬁcation
Here we brieﬂy review background from Hu et al. (2016) to provide a foundation for our reanalysis in the next section. We focus on a logic rule for sentences containing an “A-but-B” structure (the only rule for which Hu et al. (2016) provide experimental results). Intuitively, the logic rule for such sentences is that the sentiment associated with the whole sentence should be the same as the sentiment associated with phrase “B”.1
1The rule is vacuously true if the sentence does not have this structure.

More formally, let pθ(y|x) denote the probability assigned to the label y ∈ {+, −} for an input x by the baseline model using parameters θ. A logic rule is (softly) encoded as a variable rθ(x, y) ∈ [0, 1] indicating how well labeling x with y satisﬁes the rule. For the case of A-but-B sentences, rθ(x, y) = pθ(y|B) if x has the structure A-but-B (and 1 otherwise). Next, we discuss the two techniques from Hu et al. (2016) for incorporating rules into models: projection, which directly alters a trained model, and distillation, which progressively adjusts the loss function during training.
Projection. The ﬁrst technique is to project a trained model into a rule-regularized subspace, in a fashion similar to Ganchev et al. (2010). More precisely, a given model pθ is projected to a model qθ deﬁned by the optimum value of q in the following optimization problem:2
min KL(q(X, Y )||pθ(X, Y )) + C ξx
q,ξ≥0 x∈X
s.t. (1 − Ey←q(·|x)[rθ(x, y)]) ≤ ξx

binarized subset of the popular Stanford Sentiment Treebank (SST) (Socher et al., 2013). The dataset includes phrase-level labels in addition to sentence-level labels (see Table 1 for detailed statistics); following Hu et al. (2016), we use both types of labels for the comparisons in Section 3.2. In all other experiments, we use only sentencelevel labels, and our baseline model for all experiments is the CNN architecture from Kim (2014).
3 A Reanalysis
In this section we reanalyze the effectiveness of the techniques of Hu et al. (2016) and ﬁnd that most of the performance gain is due to projection and not knowledge distillation. The discrepancy with the original analysis can be attributed to the relatively small dataset and the resulting variance across random initializations. We start by analyzing the baseline CNN by Kim (2014) to point out the need for an averaged analysis.

Here q(X, Y ) denotes the distribution of (x, y)

when x is drawn uniformly from the set X and

y is drawn according to q(·|x).

0.6

Accuracy (%)

Iterative Rule Knowledge Distillation. The second technique is to transfer the domain knowledge encoded in the logic rules into a neural network’s parameters. Following Hinton et al. (2015), a “student” model pθ can learn from the “teacher” model qθ, by using a loss function πH(pθ, Ptrue) + (1 − π)H(pθ, qθ) during training, where Ptrue denotes the distribution implied by the ground truth, H(·, ·) denotes the cross-entropy function, and π is a hyperparameter. Hu et al. (2016) computes qθ after every gradient update by projecting the current pθ, as described above. Note that both mechanisms can be combined: After fully training pθ using the iterative distillation process above, the projection step can be applied one more time to obtain qθ which is then used as the trained model.
Dataset. All of our experiments (as well as those in Hu et al. (2016)) use the SST2 dataset, a
2The formulation in Hu et al. (2016) includes another hyperparameter λ per rule, to control its relative importance; when there is only one rule, as in our case, this parameter can be absorbed into C.

0.4

0.2

0.0 83.47

85.64 86.16 86.49

87.20

Number of epochs of training

Figure 1: Variation in models trained on SST-2 (sentenceonly). Accuracies of 100 randomly initialized models are plotted against the number of epochs of training (in gray), along with their average accuracies (in red, with 95% conﬁdence interval error bars). The inset density plot shows the distribution of accuracies when trained with early stopping.

3.1 Importance of Averaging
We run the baseline CNN by Kim (2014) across 100 random seeds, training on sentence-level la-

Number of
Instances A-but-B Negations Discourse

Phrases
76961 3.5% 2.0% 5.0%

Train
6920 11.1% 17.5% 24.6%

Dev
872 11.5% 18.3% 26.0%

Test
1821 11.5% 17.2% 24.5%

Table 1: Statistics of SST2 dataset. Here “Discourse” includes both A-but-B and negation sentences. The mean length of sentences is in terms of the word count.

no-project project

Reported Test Accuracy  (Hu et al., 2016)

no-distill

distill

87.2 +0.7 87.9

+1.6 88.8 +0.5
+1.4 89.3

Averaged Test Accuracy

no-distill

distill

87.66 +1.07 88.73

+0.29 87.97 +0.80
+0.04 88.77

Averaged A-but-B accuracy

no-distill

distill

80.25 +9.31 89.56

+1.92 82.17 +6.96
-0.43 89.13

Figure 2: Comparison of the accuracy improvements reported in Hu et al. (2016) and those obtained by averaging over 100 random seeds. The last two columns show the (averaged) accuracy improvements for A-but-B style sentences. All models use the publicly available implementation of Hu et al. (2016) trained on phrase-level SST2 data.

bels. We observe a large amount of variation from run-to-run, which is unsurprising given the small dataset size. The inset density plot in Figure 1 shows the range of accuracies (83.47 to 87.20) along with 25, 50 and 75 percentiles.3 The ﬁgure also shows how the variance persists even after the average converges: the accuracies of 100 models trained for 20 epochs each are plotted in gray, and their average is shown in red.
We conclude that, to be reproducible, only averaged accuracies should be reported in this task and dataset. This mirrors the conclusion from a detailed analysis by Reimers and Gurevych (2017) in the context of named entity recognition.
3.2 Performance of Hu et al. (2016)
We carry out an averaged analysis of the publicly available implementation4 of Hu et al. (2016). Our analysis reveals that the reported performance of their two mechanisms (projection and distillation) is in fact affected by the high variability across random seeds. Our more robust averaged analysis yields a somewhat different conclusion of their effectiveness.
In Figure 2, the ﬁrst two columns show the reported accuracies in Hu et al. (2016) for models trained with and without distillation (corresponding to using values π = 1 and π = 0.95t in the tth epoch, respectively). The two rows show the results for models with and without a ﬁnal projection into the rule-regularized space. We keep our hyper-parameters identical to Hu et al. (2016).5
The baseline system (no-project, no-distill) is identical to the system of Kim (2014). All the systems are trained on the phrase-level SST2 dataset
3We use early stopping based on validation performance for all models in the density plot.
4https://github.com/ZhitingHu/logicnn/ 5In particular, C = 6 for projection.

with early stopping on the development set. The number inside each arrow indicates the improvement in accuracy by adding either the projection or the distillation component to the training algorithm. Note that the reported ﬁgures suggest that while both components help in improving accuracy, the distillation component is much more helpful than the projection component.
The next two columns, which show the results of repeating the above analysis after averaging over 100 random seeds, contradict this claim. The averaged ﬁgures show lower overall accuracy increases, and, more importantly, they attribute these improvements almost entirely to the projection component rather than the distillation component. To conﬁrm this result, we repeat our averaged analysis restricted to only “A-but-B” sentences targeted by the rule (shown in the last two columns). We again observe that the effect of projection is pronounced, while distillation offers little or no advantage in comparison.
4 Contextualized Word Embeddings
Traditional context-independent word embeddings like word2vec (Mikolov et al., 2013) or GloVe (Pennington et al., 2014) are ﬁxed vectors for every word in the vocabulary. In contrast, contextualized embeddings are dynamic representations, dependent on the current context of the word. We hypothesize that contextualized word embeddings might inherently capture these logic rules due to increasing the effective context size for the CNN layer in Kim (2014). Following the recent success of ELMo (Peters et al., 2018a) in sentiment analysis, we utilize the TensorFlow Hub implementation of ELMo6 and feed these contextualized embeddings into our CNN model. We
6https://tfhub.dev/google/elmo/1

ﬁne-tune the ELMo LSTM weights along with the CNN weights on the downstream CNN task. As in Section 3, we check performance with and without the ﬁnal projection into the rule-regularized space. We present our results in Table 2.
Switching to ELMo word embeddings improves performance by 2.9 percentage points on an average, corresponding to about 53 test sentences. Of these, about 32 sentences (60% of the improvement) correspond to A-but-B and negation style sentences, which is substantial when considering that only 24.5% of test sentences include these discourse relations (Table 1). As further evidence that ELMo helps on these speciﬁc constructions, the non-ELMo baseline model (no-project, no-distill) gets 255 sentences wrong in the test corpus on average, only 89 (34.8%) of which are A-but-B style or negations.
Statistical Signiﬁcance: Using a two-sided Kolmogorov-Smirnov statistic (Massey Jr, 1951) with α = 0.001 for the results in Table 2, we ﬁnd that ELMo and projection each yield statistically signiﬁcant improvements, but distillation does not. Also, with ELMo, projection is not signiﬁcant. Speciﬁc comparisons have been added in the Appendix, in Table A3.
KL Divergence Analysis: We observe no signiﬁcant gains by projecting a trained ELMo model into an A-but-B rule-regularized space, unlike the other models. We conﬁrm that ELMo’s predictions are much closer to the A-but-B rule’s manifold than those of the other models by computing KL(qθ||pθ) where pθ and qθ are the original and projected distributions: Averaged across all A-butB sentences and 100 seeds, this gives 0.27, 0.26 and 0.13 for the Kim (2014), Hu et al. (2016) with distillation and ELMo systems respectively.
Intra-sentence Similarity: To understand the information captured by ELMo embeddings for A-but-B sentences, we measure the cosine similarity between the word vectors of every pair of words within the A-but-B sentence (Peters et al., 2018b). We compare the intra-sentence similarity for ﬁne-tuned word2vec embeddings (baseline), ELMo embeddings without ﬁne-tuning and ﬁnally ﬁne-tuned ELMo embeddings in Figure 3. In the ﬁne-tuned ELMo embeddings, we notice the words within the A and within the B part of the A-but-B sentence share the same part of the vector space. This pattern is less visible in the

Model

no-distill no-project

no-distill

project

distill 7 no-project

distill

project

ELMo no-project

ELMo

project

Test
85.98 86.54
86.11 86.62
88.89 88.96

but
78.69 83.40
79.04 83.32
86.51 87.20

but or neg
80.13 -
-
87.24 -

Table 2: Average performance (across 100 seeds) of ELMo on the SST2 task. We show performance on A-but-B sentences (“but”), negations (“neg”).
ELMo embeddings without ﬁne-tuning and absent in the word2vec embeddings. This observation is indicative of ELMo’s ability to learn speciﬁc rules for A-but-B sentences in sentiment classiﬁcation. More intra-sentence similarity heatmaps for A-but-B sentences are in Figure A1.

5 Crowdsourced Experiments
We conduct a crowdsourced analysis that reveals that SST2 data has signiﬁcant levels of ambiguity even for human labelers. We discover that ELMo’s performance improvements over the baseline are robust across varying levels of ambiguity, whereas the advantage of Hu et al. (2016) is reversed in sentences of low ambiguity (restricting to A-but-B style sentences).
Our crowdsourced experiment was conducted on Figure Eight.8 Nine workers scored the sentiment of each A-but-B and negation sentence in the test SST2 split as 0 (negative), 0.5 (neutral) or 1 (positive). (SST originally had three crowdworkers choose a sentiment rating from 1 to 25 for every phrase.) More details regarding the crowd experiment’s parameters have been provided in Appendix A.
We average the scores across all users for each sentence. Sentences with a score in the range (x, 1] are marked as positive (where x ∈ [0.5, 1)), sentences in [0, 1 − x) marked as negative, and sentences in [1 − x, x] are marked as neutral. For instance, “ﬂat , but with a revelatory performance by michelle williams” (score=0.56) is neutral when x = 0.6.9 We present statistics of our dataset10 in Table 3. Inter-annotator agree-
7Trained on sentences and not phrase-level labels for a fair comparison with baseline and ELMo, unlike Section 3.2.
8 https://www.figure-eight.com/ 9More examples of neutral sentences have been provided in the Appendix in Table A1, as well as a few “ﬂipped” sentences receiving an average score opposite to their SST2 label (Table A2). 10The dataset along with source code can be found in

there are slow and repetitive parts , but it has just enough spice to keep it interesting there are slow and repetitive parts , but it has just enough spice to keep it interesting there are slow and repetitive parts , but it has just enough spice to keep it interesting

there are

0.4

there

are

0.6

there

are

0.6

slow

slow

slow

and

0.3

and

0.5

and

0.5

repetitive

repetitive

repetitive

parts ,

parts

0.2

,

parts

0.4

,

0.4

but

but

but

it

it

it

0.3

has just

0.1

has just

has

0.3

just

0.2

enough

enough

enough

spice to

0.0

spice to

spice

0.2

to

0.1

keep

keep

keep

it interesting

0.1

it interesting

it

0.0

interesting

Figure 3: Heat map showing the cosine similarity between pairs of word vectors within a single sentence. The left ﬁgure has ﬁne-tuned word2vec embeddings. The middle ﬁgure contains the original ELMo embeddings without any ﬁne-tuning. The right ﬁgure contains ﬁne-tuned ELMo embeddings. For better visualization, the cosine similarity between identical words has been set equal to the minimum value in the heat map.

ment was computed using Fleiss’ Kappa (κ). As expected, inter-annotator agreement is higher for higher thresholds (less ambiguous sentences). According to Landis and Koch (1977), κ ∈ (0.2, 0.4] corresponds to “fair agreement”, whereas κ ∈ (0.4, 0.6] corresponds to “moderate agreement”.
We next compute the accuracy of our model for each threshold by removing the corresponding neutral sentences. Higher thresholds correspond to sets of less ambiguous sentences. Table 3 shows that ELMo’s performance gains in Table 2 extends across all thresholds. In Figure 4 we compare all the models on the A-but-B sentences in this set. Across all thresholds, we notice trends similar to previous sections: 1) ELMo performs the best among all models on A-but-B style sentences, and projection results in only a slight improvement; 2) models in Hu et al. (2016) (with and without distillation) beneﬁt considerably from projection; but 3) distillation offers little improvement (with or without projection). Also, as the ambiguity threshold increases, we see decreasing gains from projection on all models. In fact, beyond the 0.85 threshold, projection degrades the average performance, indicating that projection is useful for more ambiguous sentences.
6 Conclusion
We present an analysis comparing techniques for incorporating logic rules into sentiment classiﬁcation systems. Our analysis included a metastudy highlighting the issue of stochasticity in performance across runs and the inherent ambiguity in the sentiment classiﬁcation task itself, which was tackled using an averaged analysis and
https://github.com/martiansideofthemoon/ logic-rules-sentiment.

Threshold
Neutral Sentiment Flipped Sentiment Fleiss’ Kappa (κ)
no-distill, no-project ELMo, no-project

0.50
10 15 0.38
81.32 87.56

0.66
70 4 0.42
83.54 90.00

0.75
95 2 0.44
84.54 91.31

0.90
234 0
0.58
87.55 93.14

Table 3: Number of sentences in the crowdsourced study

(447 sentences) which got marked as neutral and which got

the opposite of their labels in the SST2 dataset, using vari-

ous thresholds. Inter-annotator agreement is computed using

Fleiss’ Kappa. Average accuracies of the baseline and ELMo

(over 100 seeds) on non-neutral sentences are also shown.

96 no-distill, no-project

94

no-distill, project distill, no-project

92

distill, project ELMo, no-project

90 ELMo, project

test performance

88

86

84

82

800.4

0.5

0.6

0.7

0.8

0.9

1.0

threshold

Figure 4: Average performance on the A-but-B part of the crowd-sourced dataset (210 sentences, 100 seeds)). For each threshold, only non-neutral sentences are used for evaluation.

a crowdsourced experiment identifying ambiguous sentences. We present evidence that a recently proposed contextualized word embedding model (ELMo) (Peters et al., 2018a) implicitly learns logic rules for sentiment classiﬁcation of complex sentences like A-but-B sentences. Future work includes a ﬁne-grained quantitative study of ELMo word vectors for logically complex sentences along the lines of Peters et al. (2018b).

References
Kuzman Ganchev, Jennifer Gillenwater, Ben Taskar, et al. 2010. Posterior regularization for structured latent variable models. Journal of Machine Learning Research, 11(Jul):2001–2049.
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015. Distilling the knowledge in a neural network. NIPS Deep Learning and Representation Learning Workshop.
Zhiting Hu, Xuezhe Ma, Zhengzhong Liu, Eduard Hovy, and Eric Xing. 2016. Harnessing deep neural networks with logic rules. In Association for Computational Linguistics (ACL).
Mohit Iyyer, Varun Manjunatha, Jordan Boyd-Graber, and Hal Daume´ III. 2015. Deep unordered composition rivals syntactic methods for text classiﬁcation. In Association for Computational Linguistics (ACL).
Yoon Kim. 2014. Convolutional neural networks for sentence classiﬁcation. In Empirical Methods in Natural Language Processing (EMNLP).
J Richard Landis and Gary G Koch. 1977. The measurement of observer agreement for categorical data. Biometrics, pages 159–174.
Shen Li, Zhe Zhao, Tao Liu, Renfen Hu, and Xiaoyong Du. 2017. Initializing convolutional ﬁlters with semantic features for text classiﬁcation. In Empirical Methods in Natural Language Processing (EMNLP).
Frank J Massey Jr. 1951. The Kolmogorov-Smirnov test for goodness of ﬁt. Journal of the American statistical Association, 46(253):68–78.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efﬁcient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.
Bo Pang and Lillian Lee. 2005. Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales. In Association for Computational Linguistics (ACL).
Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. GloVe: Global vectors for word representation. In Empirical Methods in Natural Language Processing (EMNLP).
Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018a. Deep contextualized word representations. In North American Association for Computational Linguistics (NAACL).
Matthew E. Peters, Mark Neumann, Wen tau Yih, and Luke Zettlemoyer. 2018b. Dissecting contextual word embeddings: Architecture and representation. In Empirical Methods in Natural Language Processing (EMNLP).

Nils Reimers and Iryna Gurevych. 2017. Reporting score distributions makes a difference: Performance study of LSTM-networks for sequence tagging. In Empirical Methods in Natural Language Processing (EMNLP).
Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Empirical Methods in Natural Language Processing (EMNLP).
Kai Sheng Tai, Richard Socher, and Christopher D Manning. 2015. Improved semantic representations from tree-structured long short-term memory networks. In Association for Computational Linguistics (ACL).
S. I. Wang and C. Manning. 2012. Baselines and bigrams: Simple, good sentiment and text classiﬁcation. In Association for Computational Linguistics (ACL).

Appendix
A Crowdsourcing Details
Crowd workers residing in ﬁve English-speaking countries (United States, United Kingdom, New Zealand, Australia and Canada) were hired. Each crowd worker had a Level 2 or higher rating on Figure Eight, which corresponds to a “group of more experienced, higher accuracy contributors”. Each contributor had to pass a test questionnaire to be eligible to take part in the experiment. Test questions were also hidden throughout the task and untrusted contributions were removed from the ﬁnal dataset. For greater quality control, an upper limit of 75 judgments per contributor was enforced. Crowd workers were paid a total of $1 for 50 judgments. An internal unpaid workforce (including the ﬁrst and second author of the paper) of 7 contributors was used to speed up data collection.

# Judgments Positive Negative Neutral

1

1

7

3

2

4

2

3

4

4

2

3

Average
0.50 0.56 0.44 0.61

Sentence
the ﬁght scenes are fun , but it grows tedious it ’s not exactly a gourmet meal but the fare is fair , even coming from the drive thru propelled not by characters but by caricatures not everything works , but the average is higher than in mary and most other recent comedies

Table A1: Examples of neutral sentences for a threshold of 0.66

# Judgments Positive Negative Neutral

1

5

3

6

0

3

0

5

4

Average 0.28 0.83 0.22

Original Positive Negative Positive

Sentence
de niro and mcdormand give solid performances , but their screen time is sabotaged by the story ’s inability to create interest
son of the bride may be a good half hour too long but comes replete with a ﬂattering sense of mystery and quietness
wasabi is slight fare indeed , with the entire project having the feel of something tossed off quickly ( like one of hubert ’s punches ) , but it should go down smoothly enough with popcorn

Table A2: Examples of ﬂipped sentiment sentences, for a threshold of 0.66

Model 1

vs

Model 2

Signiﬁcant

distill no-distill
ELMo

no-project no-project no-project

distill

project

Yes

no-distill

project

Yes

ELMo project

No

no-distill no-project no-distill project

distill no-project

No

distill project

No

no-distill distill
no-distill distill

no-project no-project
project project

ELMo no-project

Yes

ELMo no-project

Yes

ELMo

project

Yes

ELMo

project

Yes

Table A3: Statistical signiﬁcance using a two-sided Kolmogorov-Smirnov statistic (Massey Jr, 1951) with α = 0.001.

all ends well , sort of , but the frenzied comic moments never click all ends well , sort of , but the frenzied comic moments never click all ends well , sort of , but the frenzied comic moments never click

all ends well
, sort
of , but the frenzied comic moments never click
marisa tomei
is good
, but just
a kiss
is just
a mess

marisa tomei is good , but just a kiss is just a mess marisa tomei is good , but just a kiss is just a mess marisa tomei is good , but just a kiss is just a mess

0.4

all

ends

0.3

well

,

sort

0.2

of

,

0.1

but the

frenzied

0.0

comic

moments

never

0.1

click

0.4 marisa

tomei

0.3

is

good

,

0.2

but

just

0.1

a

kiss

0.0

is

just

a 0.1 mess

0.6

all ends

well

0.5

,

sort

0.4

of ,

but

0.3

the

frenzied

0.2

comic moments

never

0.1

click

marisa 0.6 tomei

is

0.5 good,

but

0.4 just a

kiss

0.3

is

just

0.2

a

mess

0.5 0.4 0.3 0.2 0.1
0.6 0.5 0.4 0.3 0.2 0.1 0.0

the irwins emerge unscathed , but the fictional footage is unconvincing and criminally badly acted the irwins emerge unscathed , but the fictional footage is unconvincing and criminally badly acted the irwins emerge unscathed , but the fictional footage is unconvincing and criminally badly acted

the

0.3

the

0.6

the

irwins

irwins

irwins

emerge

emerge

emerge

0.5

unscathed ,

0.2

unscathed ,

0.5 unscathed ,

but

but

but

0.4

the fictional

0.1

the fictional

0.4

the

fictional

0.3

footage

footage

footage

is

is

0.3

is

unconvincing

0.0 unconvincing

unconvincing

0.2

and criminally

and criminally

0.2

and criminally

badly

0.1

badly

badly

0.1

acted

acted

0.1

acted

Figure A1: Heat map showing the cosine similarity between pairs of word vectors within a single sentence. The leftmost column has word2vec (Mikolov et al., 2013) embeddings, ﬁne-tuned on the downstream task (SST2). The middle column contains the original ELMo embeddings (Peters et al., 2018a) without any ﬁne-tuning. The representations from the three layers (token layer and two LSTM layers) have been averaged. The rightmost column contains ELMo embeddings ﬁne-tuned on the downstream task. For better visualization, the cosine similarity between identical words has been set equal to the minimum value in the map.

