arXiv:1902.04811v2 [cs.LG] 4 Sep 2019

On Nonconvex Optimization for Machine Learning: Gradients, Stochasticity, and Saddle Points

Chi Jin University of California, Berkeley chijin@cs.berkeley.edu

Praneeth Netrapalli Microsoft Research, India praneeth@microsoft.com

Rong Ge Duke University rongge@cs.duke.edu

Sham M. Kakade University of Washington, Seattle sham@cs.washington.edu

Michael I. Jordan University of California, Berkeley jordan@cs.berkeley.edu

September 5, 2019

Abstract
Gradient descent (GD) and stochastic gradient descent (SGD) are the workhorses of large-scale machine learning. While classical theory focused on analyzing the performance of these methods in convex optimization problems, the most notable successes in machine learning have involved nonconvex optimization, and a gap has arisen between theory and practice. Indeed, traditional analyses of GD and SGD show that both algorithms converge to stationary points efﬁciently. But these analyses do not take into account the possibility of converging to saddle points. More recent theory has shown that GD and SGD can avoid saddle points, but the dependence on dimension in these analyses is polynomial. For modern machine learning, where the dimension can be in the millions, such dependence would be catastrophic. We analyze perturbed versions of GD and SGD and show that they are truly efﬁcient—their dimension dependence is only polylogarithmic. Indeed, these algorithms converge to second-order stationary points in essentially the same time as they take to converge to classical ﬁrst-order stationary points.
1 Introduction
One of the principal discoveries in machine learning in recent years is an empirical one—that simple algorithms often sufﬁce to solve difﬁcult real-world learning problems. Machine learning algorithms generally arise via formulations as optimization problems, and, despite a massive classical toolbox of sophisticated optimization algorithms and a major modern effort to further develop that toolbox, the simplest algorithms— gradient descent, which dates to the 1840s [Cauchy, 1847] and stochastic gradient descent, which dates to the 1950s [Robbins and Monro, 1951]—reign supreme in machine learning.
This empirical discovery is appealing in many ways. First, at the scale of modern machine learning applications—often involving many millions of data points and millions of parameters—complex algorithms
A preliminary version of this paper, with a subset of the results that are presented here, was presented at ICML 2017 and appeared in the proceedings as Jin et al. [2017a].
1

are generally infeasible, so that the only hope is that simple algorithms might be not only feasible but successful. Second, simple algorithms are easier to implement, debug, and maintain. Third, as the ﬁeld of machine learning transforms into a real-world engineering discipline, it will be necessary to develop solid theoretical foundations for entire systems that employ machine learning algorithms at their core, and such an effort seems less daunting if the basic ingredients are simple.
These developments were presaged and supported by optimization researchers such as Nemirovskii, Nesterov, and Polyak who, from the 1960s until the present day, have pursued an in-depth study of ﬁrst-order, gradient-based algorithms, developing novel algorithms and accompanying theory [Nemirovskii and Yudin, 1983, Nesterov, 1998, Polyak, 1963]. Their results have included lower bounds and algorithms that achieve those lower bounds. This line of work has made clear that even simple algorithms require delicate theoretical treatment when they are studied in large-scale settings. Thus, much of the focus has been on the setting of convex optimization where many of the complexities have been stripped away. This has allowed the development of an elegant theory, and has provided a solid jumping-off point for further analysis that has brought additional computational constraints into play—including distributed platforms, fault tolerance, communication bottlenecks, and asynchronous computation [Recht et al., 2011, Zhang et al., 2012, Smith et al., 2018].
The most notable machine-learning success stories, however, have generally involved nonconvex optimization formulations, and a gap has arisen between theory and practice. Attempts to ﬁll this gap include Choromanska et al. [2014] in the setting of learning multi-layer neural networks, Bandeira et al. [2016], Mei et al. [2017] for synchronization and MaxCut, Boumal et al. [2016] for smooth semideﬁnite programs, Bhojanapalli et al. [2016] for matrix sensing, Ge et al. [2016] for matrix completion, and Ge et al. [2017] for robust principal component analysis. But there remains a need to develop general theory that relates the convergence of machine learning algorithms to geometry and dynamics.
Most of the theory in the optimization literature has focused on the relationship between the number of iterations of the algorithm and a suitable notion of accuracy. Dimension is often neglected in such analyses, in part because in the convex setting even the simplest algorithms, including gradient descent, are provably independent of dimension. In developing algorithmic theory for nonconvex optimization formulations of machine learning problems, however, it is critically important to study iteration complexity as a function of of dimension, which can be in the millions. Moreover, we cannot resort to asymptotics—we are interested in problems at all scales.
In the current paper we have three goals. The ﬁrst is to show that signiﬁcant progress has been made in recent years in the theoretical analysis of algorithms for nonconvex machine learning. The second is to extend that line of analysis to handle both stochastic and non-stochastic algorithms in a single framework. In both cases we upper bound the iteration complexity as a function of both accuracy and dimension. The third is to exhibit a simple proof that exposes the core of the phenomenon that determines the dimension dependence.
Nonconvex optimization problems are intractable in general. Progress has been in machine learning by noting that in many problems the principal difﬁculty is not local minima, either because there are no spurious local minima (we review a list of such problems in Section 2) or because empirical work has shown that the local minima that are found by local gradient-based algorithms tend to be effective in terms of the ultimate goal of machine learning, which is performance on a test set. The problem then becomes one of avoiding saddle points, which are ubiquitous in machine learning architectures. Saddle points slow down gradientbased algorithms and in millions of dimensions they are potentially a major bottleneck for such algorithms. The theoretical problem becomes that of characterizing the iteration complexity of avoiding saddle points, as a function of target accuracy and dimension.
2

We brieﬂy mention some of the most relevant theoretical context for this problem here, providing a more thorough review of related work in Section 1.1 and in the Appendix. Lee et al. [2016] showed that gradient descent, under random initialization or with perturbations, asymptotically avoids saddle points with probability one. Ge et al. [2015] provided a more quantitative (nonasymptotic) characterization of gradient descent augmented with a suitable perturbation, showing that its convergence rate in the presence of saddle points is upper bounded by an expression of the form poly(d, ǫ−1), where d is the dimension and ǫ is the accuracy. While these convergence results are inspiring, they are signiﬁcantly worse than the convergence of gradient descent in the convex setting, or its convergence in the nonconvex setting where convergence to a saddle point is not excluded—in both cases the rate is independent of d—and they do not seem to accord with the empirical success of gradient-based methods in high-dimensional problems. Thus we ask whether these results, which are upper bounds, can be improved.
The current paper provides a positive answer to this question. We shows that suitably-perturbed versions of gradient descent and stochastic gradient descent escape saddle points in a number of iterations that is only polylogarithmic in dimension. More technically, deﬁning a notion of ǫ-second-order stationarity (see Section 2), which rules out saddle points, to be contrasted with classical ǫ-ﬁrst-order stationarity, which simply means near vanishing of the gradient, and which therefore does not rule out saddle points, we show that:
• Perturbed gradient descent (PGD) ﬁnds ǫ-second-order stationary points in O˜(ǫ−2) iterations, where O˜(·) hides only absolute constants and poylogarithmic factors. Compared to the O(ǫ−2) iterations required by gradient descent (GD) to ﬁnd ﬁrst-order stationary points [Nesterov, 1998], this involves only additional polylogarithmic factors in d.
• In the stochastic setting where stochastic gradients are Lipschitz, perturbed stochastic gradient descent (PSGD) ﬁnds ǫ-second-order stationary points in O˜(ǫ−4) iterations. Compared to the O(ǫ−4) iterations required by stochastic gradient descent (SGD) to ﬁnd ﬁrst-order stationary points [Ghadimi and Lan, 2013], this again incurs overhead that is only polylogarithmic in d.
• When stochastic gradients are not Lipschitz, PSGD ﬁnds ǫ-second-order stationary points in O˜(dǫ−4) iterations—this involves an additional linear factor in d.
1.1 Related work
In this section we discuss related work on convergence guarantees for ﬁnding second-order stationary points. Some key comparisons are summarized in Table 1, and an augmented table is provided in Appendix A.
Non-stochastic settings. Classical approaches to ﬁnding second-order stationary points assume access to second-order information, in particular the Hessian matrix of second derivatives. Examples of such approaches include the cubic regularization method [Nesterov and Polyak, 2006] and trust-region methods [Curtis et al., 2014], both of which require O(ǫ−1.5) queries of gradients and Hessians. This favorable convergence rate is, however, obtained at a high cost per iteration, owing to the fact that Hessian matrices scale quadratically with respect to dimension. In practice researchers have turned to ﬁrst-order methods, which only utilize gradients and are therefore are substantially cheaper per iteration.
Before turning to pure ﬁrst-order algorithms, we mention a line of research that is based on the assumption of a Hessian-vector product oracle [Carmon et al., 2016, Agarwal et al., 2017]. For architectures
3

Setting Non-stochastic
Stochastic

Algorithm GD [Nesterov, 2000]
PGD SGD [Ghadimi and Lan, 2013]
PSGD (with Assumption C) PSGD (no Assumption C)

Iterations O(ǫ−2) O˜(ǫ−2) O(ǫ−4) O˜(ǫ−4) O˜(dǫ−4)

Guarantees ﬁrst-order stationary point second-order stationary point ﬁrst-order stationary point second-order stationary point second-order stationary point

Table 1: A high level summary of the results of this paper and their comparison to prior state of the art for GD and SGD algorithms. This table only highlights the dependences on d and ǫ. See Section 1 for a description of these results. See Section 1.1 and Appendix A for a more detailed comparison with other related works.

such as deep neural networks, Hessian-vector products can be computed efﬁciently via automatic differentiation, and it is thus possible to obtain much of the effect of second-order methods with a complexity closer to that of ﬁrst-order methods. Indeed, the algorithms have convergence rates of O˜(ǫ−1.75) gradient queries [Carmon et al., 2016, Agarwal et al., 2017]. Their implementation, however, involved nested loops, and accordingly a concern with the setting of hyperparameters. Thus despite the favorable convergence rate, these algorithms have not yet found their way into practical implementations.
Given the preference among practitioners for simple, single-loop algorithms, and the striking empirical successes obtained with such algorithms, it is important to pin down the theoretical properties of such algorithms. In such analyses, the results for second-order and Hessian-vector algorithms serve as baselines. Of key interest is the convergence rate not merely as a function of the accuracy ǫ but also as a function of the dimension d. Indeed, while second-order algorithms can use the structure of the Hessian to readily avoid unhelpful directions even in high-dimensional spaces. Without the Hessian there is a concern that algorithms may scale poorly as a function of dimension.
Ge et al. [2015] and Levy [2016] studied simple variants of gradient descent, and found that while it has favorable scaling in terms of ǫ, it requires poly(d) gradient queries to ﬁnd second-order stationary points. These results are only upper bounds, however. The practical success of gradient descent suggests that the poly(d) scaling may be overly pessimistic. Indeed, in an early version of the results presented here, Jin et al. [2017a] showed that a simple perturbed version of gradient descent ﬁnds second-order stationary points in O˜(ǫ−2) gradient queries, paying only a logarithmic overhead compared to the rate associated with ﬁnding ﬁrst-order stationary points. This result is summarized in the ﬁrst two lines of Table 1. In followup work, Jin et al. [2017b] show that a perturbed version of celebrated Nesterov’s accelerated gradient descent [Nesterov, 1983] enjoys a faster convergence rate of O˜(ǫ−1.75), again with logarithmic dimension dependence.
Stochastic setting with Lipschitz gradient. We now turn to the setting in which the learning algorithm only has access to stochastic gradients and where the stochastic is extrinsic; i.e., not under the control of the algorithm. Most existing work assumes that the stochastic gradients are Lipschitz (or equivalently that the underlying functions are gradient-Lipschitz, see Assumption C). Under this assumption, and an additional Hessian-vector product oracle, Allen-Zhu [2018], Zhou et al. [2018], Tripuraneni et al. [2018] de-

4

signed algorithms that have an iteration complexity of O˜(ǫ−3.5). Xu et al. [2018], Allen-Zhu and Li [2017] obtain similar results without the requirement of a Hessian-vector product oracle. The sharpest rates in this category have been obtained by Fang et al. [2018] and Zhou and Gu [2019], who show that the iteration complexity can be further reduced to O˜(ǫ−3). Again, however, this line of works consists of double-loop algorithms, and it remains unclear whether they will have an impact on practice.
Among single-loop algorithms that are simple variants of stochastic gradient descent (SGD), Ge et al. [2015] showed that a particular variant has an iteration complexity for ﬁnding second-order stationary points that is upper bounded by d4poly(ǫ−1). Daneshmand et al. [2018] presented an alternative variant of SGD and showed that—if the variance of the stochastic gradient along the escaping direction of saddle points is at least γ for all saddle points—then the algorithm ﬁnds second-order stationary points in O˜(γ−4ǫ−5) iterations. In general, however, γ scales as 1/d, which implies a complexity of O˜(d4ǫ−5).
In the current paper, we demonstrate that a simple perturbed version of SGD achieves a convergence rate of O˜(ǫ−4), which matches the speed of SGD to ﬁnd a ﬁrst-order stationary point up to polylogarithmic factors in dimension. Concurrent to our work, Fang et al. [2019] analyzed SGD with averaging over last few iterates, and obtained a faster convergence rate of O˜(ǫ−3.5).
General stochastic setting. There is signiﬁcantly less work in the general setting in which the stochastic gradients are no longer guaranteed to be Lipschitz. In fact, only the results of Ge et al. [2015] and Daneshmand et al. [2018] apply here, and both of them require at least Ω(d4) gradient queries to ﬁnd second-order stationary points. The current paper brings this dependence down to linear dimension dependence. See the last three lines in Table 1 for a summary of the results in the stochastic case.
Other settings. Finally, there are also several recent results in the setting in which objective functions can be written as a ﬁnite sum of individual functions. We refer readers to Reddi et al. [2017], Allen-Zhu and Li [2017] and Lei et al. [2018] and the references therein for further reading.
1.2 Organization
In Section 2, we review some algorithmic and mathematical preliminaries. Section 3 presents several examples of nonconvex problems in machine learning, demonstrating how second-order stationarity can ensure approximate global optimality. In Section 4, we present the algorithms that we analyze and present our main theoretical results for perturbed GD and SGD. In Section 5, we present the proof for the non-stochastic case (perturbed GD), which illustrates some of our key ideas. The proof for the stochastic setting is presented in the Appendix. We conclude in Section 6.
2 Background
In this section, we introduce our notation and present deﬁnitions and assumptions. We also overview existing results in nonconvex optimization, in both the deterministic and stochastic settings.
2.1 Notation
We use bold upper-case letters A, B to denote matrices and bold lower-case letters x, y to denote vectors. For vectors we use · to denote the ℓ2-norm, and for matrices we use · and · F to denote spectral (or operator) norm and Frobenius norm respectively. We use λmin(·) to denote the smallest eigenvalue of a
5

matrix. For a function f : Rd → R, we use ∇f and ∇2f to denote the gradient and Hessian, and f ⋆ to denote the global minimum of function f . We use notation O(·), Θ(·), Ω(·) to hide only absolute constants which do not depend on any problem parameter, and notation O˜(·), Θ˜ (·), Ω˜ (·) to hide absolute constants and factors that are only polylogarithmically dependent on all problem parameters.

2.2 Nonconvex optimization and gradient descent

In this paper, we are interested in solving general unconstrained optimization problems of the form:

min f (x),
x∈Rd
where f is a smooth function which can be nonconvex. In particular we assume that f has Lipschitz gradients and Lipschitz Hessians, which ensures that the gradient and Hessian can not change too rapidly.

Deﬁnition 1. A differentiable function f is ℓ-gradient Lipschitz if:

∇f (x1) − ∇f (x2) ≤ ℓ x1 − x2 ∀ x1, x2.

Deﬁnition 2. A twice-differentiable function f is ρ-Hessian Lipschitz if: ∇2f (x1) − ∇2f (x2) ≤ ρ x1 − x2 ∀ x1, x2.

Assumption A. The function f is ℓ-gradient Lipschitz and ρ-Hessian Lipschitz.

Our point of departure is the classical Gradient Descent (GD) algorithm, whose update takes following

form:

xt+1 = xt − η∇f (xt),

(1)

where η > 0 is a step size or learning rate. Since the problem of ﬁnding a global optimum for general nonconvex functions is NP-hard, the classical literature in optimization has resorted to a local surrogate— ﬁrst-order stationarity.

Deﬁnition 3. For a differentiable function f , x is a ﬁrst-order stationary point if ∇f (x) = 0. Deﬁnition 4. For a differentiable function f , x is an ǫ-ﬁrst-order stationary point if ∇f (x) ≤ ǫ.

It is of major importance that gradient descent converges to a ﬁrst-order stationary point in a number of iterations that is independent of dimension. This fact, referred to as “dimension-free convergence” in the optimization literature, is captured in the following classical theorem.

Theorem 5 ([Nesterov, 1998]). For any ǫ > 0, assume the function f (·) is ℓ-gradient Lipschitz, and set the step size as η = 1/ℓ. Then, the gradient descent algorithm in Eq. (1) will visit an ǫ-stationary point at least once in the following number of iterations:

ℓ(f (x0) − f ⋆)

ǫ2

.

Note that in this formulation, the last iterate is not guaranteed to be a stationary point. However, it is not hard to ﬁgure out which iterate is the stationary point by calculating the norm of the gradient at every iteration.
A ﬁrst-order stationary point can be a local minimum, a local maximum or even a saddle point:

6

Deﬁnition 6. For a differentiable function f , a stationary point x is a
• local minimum, if there exists δ > 0 such that f (x) ≤ f (y) for any y with y − x ≤ δ.
• local maximum, if there exists δ > 0 such that f (x) ≥ f (y) for any y with y − x ≤ δ.
• saddle point, otherwise.
For minimization problems, both saddle points and local maxima are clearly undesirable. Our focus will be “saddle points,” although our results also apply directly to local maxima as well. Unfortunately, distinguishing saddle points from local minima for smooth functions is still NP-hard in general [Nesterov, 2000]. To avoid these hardness results, we focus on a subclass of saddle points.
Deﬁnition 7. For a twice-differentiable function f , x is a strict saddle point if x is a stationary point and λmin(∇2f (x)) < 0.
A generic saddle point must satisfy that λmin(∇2f (x)) ≤ 0. Being “strict” simply rules out the case where λmin(∇2f (x)) = 0. We reformulate our goal as that of ﬁnding stationary points that are not strict saddle points.
Deﬁnition 8. For twice-differentiable function f (·), x is a second-order stationary point if
∇f (x) = 0, and ∇2f (x) 0.
Deﬁnition 9. For a ρ-Hessian Lipschitz function f (·), x is an ǫ-second-order stationary point if: ∇f (x) ≤ ǫ and ∇2f (x) −√ρǫ · I.
Our deﬁnition again makes use of an ǫ-ball around the stationary point so that we can discuss rates, and the condition on the Hessian in Deﬁnition 4 uses the Hessian Lipschitz parameter ρ to retain a single accuracy parameter and to match the units of the gradient and Hessian, following the convention of Nesterov and Polyak [2006].
Although second-order stationarity is only a necessary condition for being a local minimum, a line of recent work in the machine learning literature shows that for many popular models in machine learning, all ǫ-second-order stationary points are approximate global minima. Thus for these models ﬁnding secondorder stationary points is sufﬁcient for solving those problems. See Section 3 for references and discussion of these results.

2.3 Stochastic approximation

We turn to the stochastic approximation setting, where we cannot access the exact gradient ∇f (·) directly. Instead for any point x, a gradient query will return a stochastic gradient g(x; θ), where θ is a random variable drawn from a distribution D. The key property that we assume for stochastic gradients is that they are unbiased: ∇f (x) = Eθ∼D [g(x; θ)]. That is, the average value of the stochastic gradient equals the true gradient. In short, the update of Stochastic Gradient Descent (SGD) is:

Sample θt ∼ D, xt+1 = xt − η∇g(xt; θt)

(2)

Other than being an unbiased estimator of true gradient, another standard assumption on the stochastic gradients is that their variance is bounded by some number σ2:
Eθ∼D g(x, θ) − ∇f (x) 2 ≤ σ2

7

When we are interested in high-probability bounds, we make the following stronger assumption on the tail of the distribution. Assumption B. For any x ∈ Rd, the stochastic gradient g(x; θ) with θ ∼ D satisﬁes:
Eg(x; θ) = ∇f (x), P ( g(x; θ) − ∇f (x) ≥ t) ≤ 2 exp(−t2/(2σ2)), ∀t ∈ R.

We note this assumption is more general than the standard notion of a sub-Gaussian random vector, which assumes E exp( v, X − EX ) ≤ exp(σ2 v 2/d) for any v ∈ Rd. The latter assumption requires the distribution to be “isotropic” while our assumption does not. By Lemma 33 we know that both bounded random vectors and standard sub-Gaussian random vector are special cases of our more general setting.
Prior work shows that stochastic gradient descent converges to ﬁrst-order stationary points in a number of iterations that is independent of dimension.

Theorem 10 ([Ghadimi and Lan, 2013]). For any ǫ, δ > 0, assume that the function f is ℓ-gradient Lipschitz, that the stochastic gradient g satisﬁes Assumption B, and let the step size scale as η = Θ˜ (ℓ−1(1 + σ2/ǫ2)−1). Then, with probability at least 1 − δ, stochastic gradient descent will visit an ǫ-stationary point
at least once in the following number of iterations:

O˜ ℓ(f (x0) − f ⋆) 1 + σ2 .

ǫ2

ǫ2

3 On the Sufﬁciency of Second-Order Stationarity
In this section we show that for a wide class of nonconvex problems in machine learning and signal processing, all second-order stationary points are global minima. Thus, for this class of problems, ﬁnding second-order stationary points efﬁciently is equivalent to solving the problem. We focus on the underlying global geometry that these problems have in common.
Problems for which second-order stationary points are global minima include tensor decomposition [Ge et al., 2015], dictionary learning [Sun et al., 2016a], phase retrieval [Sun et al., 2016b], synchronization and MaxCut [Bandeira et al., 2016, Mei et al., 2017], smooth semideﬁnite programs [Boumal et al., 2016], and many problems related to low-rank matrix factorization, such as matrix sensing [Bhojanapalli et al., 2016], matrix completion [Ge et al., 2016] and robust principale component analysis [Ge et al., 2017]. In particular, these papers show that by adding appropriate regularization terms, and under mild conditions, there are two key geometric properties satisﬁed by the corresponding objective functions: (a) All local minima are global minima. There might be multiple local minima due to permutation, but they are all equally good; (b) All saddle points have at least one direction with strictly negative curvature, thus are strict saddle points. we summarize the consequences of these properties in the following proposition, for which we omit the proof as it follows essentially by deﬁnition.
Proposition 11. If a function f satisﬁes (a) all local minima are global minima; (b) all saddle points (including local maxima) are strict saddle points, then all second-order stationary points are global minima.
This implies that the core problem for these nonconvex machine-learning applications is to ﬁnd secondorder stationary points efﬁciently. If we can prove that some simple variants of GD and SGD converges to second-order stationary points efﬁciently, then we immediately establish global convergence results for these algorithms for all of the above applications (i.e. convergence from arbitrary initialization).

8

Before we turn to such convergence results, consider, as an illustrative example of this class of nonconvex problems, the problem of ﬁnding the leading eigenvector of a positive semideﬁnite matrix M ∈ Rd×d.
We deﬁne the following objective:

min f (x) = 1 xx⊤ − M 2,

(3)

x∈Rd

2

F

Denote the eigenvalues and eigenvectors of M as (λi, vi) for i = 1, . . . , d, and assume there is a gap between the ﬁrst and√second eigenvalues: λ1 > λ2 ≥ λ3 ≥ . . . ≥ λd ≥ 0. In this case, the global optimal solutions are x = ± λ1v1 giving the top eigenvector direction.
The objective function (3) is nonconvex as a function of x. In order to optimize this objective via
gradient-descent methods, we need to analyze the global landscape of the objective function. Its gradient
and Hessian are of the form:

∇f (x) =(xx⊤ − M)x

∇2f (x) = x 2I + 2xx⊤ − M.

Therefore, all stationary points satisfy th√e equation Mx =

x 2x.

√ Thus they are 0 and ± λivi for

i = 1, . . . , d. We already know that ± λ1v1 are global minima, thus they are also local minima and

are equivalent up to a sign difference. For the remaining stationary points x†, we note that their Hessian al-

ways has strict negative curvature along the v1 direction: v1⊤∇2f (x†)v1 ≤ λ2 − λ1 < 0. Thus these points

are strict saddle points. Having established the preconditions for Proposition 11, we are able to conclude:

Corollary 12. Assume that M is a positive semideﬁnite matrix whose top two eigenvalues are λ1 > λ2 ≥ 0. For the problem of minimizing the objective Eq. (3), all second-order stationary points are global optima.

Further analysis can be carried out to establish the ǫ-robust version of Corollary 12. Informally, it can be shown that under technical conditions, for polynomially small ǫ, all ǫ-second-order stationary point are close to global optima. We refer the readers to Ge et al. [2017] for the formal statement.

4 Main Results
In this section, we present our main results on the efﬁciency of GD and SGD in the nonconvex setting. We ﬁrst study the case where the exact gradients are accessible, where we focus on an algorithm that we refer to as Perturbed Gradient Descent (PGD). We then turn to the stochastic setting, and present the results for Perturbed SGD and its mini-batch version.
4.1 Non-stochastic setting
We begin by considering the case in which exact gradients are available, such that GD can be implemented. For convex problems, GD is efﬁcient, but, as can be seen in Eq.(1), GD makes a non-zero step only when the gradient is non-zero, and thus in the nonconvex setting it will be stuck at saddle points if initialized there. We thus consider a simple variant of GD which adds randomness to the iterates at each step (Algorithm 1). The question is whether such a simple procedure can be efﬁcient, particularly in terms of its dimension dependence.
At each iteration, Algorithm 1 is almost the same as gradient descent, except it adds a small isotropic random Gaussian perturbation to the gradient. The perturbation ξt is sampled from a zero-mean Gaussian
9

Algorithm 1 Perturbed Gradient Descent (PGD) Input: x0, step size η, perturbation radius r.
for t = 0, 1, . . . , do xt+1 ← xt − η(∇f (xt) + ξt), ξt ∼ N (0, (r2/d)I)
with covariance (r2/d)I so that E ξt 2 = r2. We note that Algorithm 1 is different from that studied in Jin et al. [2017a], where perturbation was added only when certain conditions hold.
We now show that if we pick r = Θ˜ (ǫ) in Algorithm 1, PGD will ﬁnd an ǫ-second-order stationary point in a number of iterations that has only a polylogarithmic dependence on dimension.
Theorem 13. Let the function f (·) satisfy Assumption A. Then, for any ǫ, δ > 0, the PGD algorithm (Algorithm 1), with parameters η = Θ˜ (1/ℓ) and r = Θ˜ (ǫ), will visit an ǫ−second-order stationary point at least once in the following number of iterations, with probability at least 1 − δ:
O˜ ℓ(f (x0) − f ⋆) , ǫ2
where O˜ and Θ˜ hide polylogarithmic factors in d, ℓ, ρ, 1/ǫ, 1/δ and ∆f := f (x0) − f ⋆.
Remark 14. If we wish to output an ǫ-second-order stationary point, it sufﬁces to run PGD for double the number of iterations in Theorem 13. A simple change to the proof shows that half of the iterates will be ǫ-second-order stationary points in this case, so that if we output an iterate uniformly at random, with at least a constant probability it will be an ǫ-second-order stationary point.
Remark 15. We have chosen the distribution of the perturbations to be Gaussian in Algorithm 1 for simplicity. This choice is not necessary. The key properties needed for the perturbation distributions are (a) that the tail of the distribution is sufﬁciently light such that an appropriate concentration inequality holds, and (b) the variance in every direction is bounded below.
Comparing Theorem 13 to the classical result in Theorem 5, our result shows that PGD ﬁnds secondorder stationary points in almost the same time as GD ﬁnds ﬁrst-order stationary points, up to only logarithmic factors. Therefore, strict saddle points are computationally benign for ﬁrst-order gradient methods.
Comparing to Theorem 5, we see that Theorem 13 makes an additional smoothness assumption. This assumption is essential in separating strict saddle points from second-order stationary points.
4.2 Stochastic setting
In the stochastic approximation setting, exact gradients ∇f (·) are no longer available, and the algorithms only have access to unbiased stochastic gradients: g(·; θ) such that ∇f (x) = Eθ∼D [g(x; θ)].
In machine learning, the stochastic gradient g is often obtained as an exact gradient of a smooth function: g(·; θ) = ∇f (·; θ). We formalize this assumption. Assumption C. For any θ ∈ supp(D), g(·; θ) is ℓ˜-Lipschitz:
g(x1; θ) − g(x2; θ) ≤ ℓ˜ x1 − x2 ∀ x1, x2.
In the special case where g(·; θ) = ∇f (·; θ) for some twice-differentiable function f (·; θ), Assumption C ensures that the spectral norm of Hessian of function f (·; θ) is bounded by ℓ˜ for all θ. Therefore, the
10

Algorithm 2 Perturbed Stochastic Gradient Descent (PSGD)

Input: x0, step size η, perturbation radius r.

for t = 0, 1, . . . , do

sample θt ∼ D xt+1 ← xt − η(g(xt; θt) + ξt),

ξt ∼ N (0, (r2/d)I)

Algorithm 3 Mini-batch Perturbed Stochastic Gradient Descent (Mini-batch PSGD)

Input: x0, step size η, perturbation radius r.

for t = 0, 1, . . . , do

sample {θt(1), · · · θt(m)} ∼ D

gt(xt) ←

m i=1

g(xt

;

θt(i)

)/m

xt+1 ← xt − η(gt(xt) + ξt),

ξt ∼ N (0, (r2/d)I)

stochastic Hessian also enjoys good concentration properties, which allows algorithms to ﬁnd points with a second-order characterization. In contrast, when Assumption C no longer holds, the problem of ﬁnding second-order stationary points becomes due to the lack of concentration of the stochastic Hessian. In the current paper, we treat both cases, by allowing ℓ˜ = +∞ to encode the assertion that Assumption C does not hold.
We are now ready to present a guarantee on the efﬁciency of PSGD (Algorithm 2) for ﬁnding a secondorder stationary point. We make the following choices of parameters for Algorithm 2:

η = Θ˜ ( 1 ),

r = Θ˜ (ǫ√N),

where

N = 1 + min

σ2 ℓ˜2 σ2d +√ ,

(4)

ℓ·N

ǫ2 ℓ ρǫ ǫ2

Theorem 16. Let the function f satisfy Assumption A and assume that the stochastic gradient g satisﬁes Assumption B (or Assumption C optionally). For any ǫ, δ > 0, the PSGD algorithm (Algorithm 2), with parameter (η, r) chosen as in Eq. (4), will visit an ǫ−second-order stationary point at least once in the following number of iterations, with probability at least 1 − δ:

O˜ ℓ(f (x0) − f ⋆) · N . ǫ2

We note that Remark 14 and Remark 15 apply directly to Theorem 16.

In

Theorem 16 provides a the former case, taking

result for both the ǫ such that σ2/ǫ2

s≥cenℓ˜a2r/io(ℓi√n ρwǫh),icwh eAhssauvme pthtiaotnNC

holds and when ≈ 1 + σ2/ǫ2.

it does not. Our results

then show that perturbed SGD ﬁnds a second-order stationary points in O˜(ǫ−4) iterations, which matches

Theorem 10 up to logarithmic factors. In the general case where Assumption C does not hold (ℓ˜ = ∞), we have N = 1 + σ2d/ǫ2, and Theo-
rem 16 guarantees that PSGD ﬁnds an ǫ-second-order stationary point in O˜(dǫ−4) iterations. Comparing to

Theorem 10, we see that the algorithm pays an additional cost that is linear in dimension d.

Finally, Theorem 16 can be easily extended to the mini-batch setting, with parameters chosen as:

η = Θ˜ ( 1 ),

r = Θ˜ (ǫ√M),

where

M = 1 + 1 min

σ2 ℓ˜2 σ2d +√ ,

.

(5)

ℓ·M

m

ǫ2 ℓ ρǫ ǫ2

11

Algorithm 4 Perturbed Gradient Descent (Variant)
Input: x0, step size η, perturbation radius r, time interval T , tolerance ǫ. tperturb = 0 for t = 0, 1, . . . , T do if ∇f (xt) ≤ ǫ and t − tperturb > T then xt ← xt − ηξt, (ξt ∼ Uniform(B0(r))); tperturb ← t xt+1 ← xt − η∇f (xt)

Theorem 17 (Mini-batch version). Let the function f satisfy Assumption A and assume that the stochastic gradient g satisﬁes Assumption B (or C optionally). Then, for any ǫ, δ, m > 0, the mini-batch PSGD algorithm (Algorithm 3), with parameters (η, r) chosen as in Eq. (5), will visit an ǫ−second-order stationary point at least once in the following number of iterations, with probability at least 1 − δ:
O˜ ℓ(f (x0) − f ⋆) · M . ǫ2
Theorem 17 says that if the mini-batch size m is not too large—m ≤ N, where N is deﬁned in Eq. (4)— then mini-batch PSGD will reduce the number of iterations linearly, while not increasing the total number of stochastic gradient queries.

5 Proofs

We provide full proofs of our main results, Theorem 16 and Theorem 17, in Appendix B. (Theorem 13 follows directly from the proof of Theorem 16 by setting σ = 0). These proofs require novel concentration inequalities and other tools from stochastic analysis to handle the relatively complex way in which stochasticity interacts with geometry in the neighborhood of saddle points. In the current section we circumvent some of these complexities by presenting a conceptually straightforward proof for an algorithm that is a variant of PGD. This algorithm, summarized in Algorithm 4, removes some of the stochasticity of PGD by restricting the way in which perturbation noise is added. The algorithm is more complex than PGD, but the proof is streamlined, allowing the core concepts underlying the full proof to be conveyed more simply.
The following theorem is the specialization of Theorem 13 to the setting of Algorithm 4.
Theorem 18. Let f satisfy Assumption A and deﬁne ∆f := f (x0)−f ⋆. For any ǫ, δ > 0, the PGD (Variant) algorithm (Algorithm 4), with parameters η, r, T chosen as in Eq. (6) and with ι = c · log(dℓ∆f /(ρǫδ)), has the property that at least one half of its iterations of will be ǫ−second order stationary points, after the following number of iterations, and with probability at least 1 − δ:
O˜ ℓ∆f . ǫ2
Here c is an absolute constant.
We proceed to the proof of the theorem. We ﬁrst specify the choice of hyperparameters η, r, and T , and two quantities F and S which are frequently used:

1

ǫ

ℓ

1 ǫ3

1ǫ

η = ℓ , r = 400ι3 , T = √ρǫ · ι, F = 50ι3 ρ , S = 4ι ρ .

(6)

12

Our high-level proof strategy is a proof by contradiction: when the current iterate is not an ǫ-second order
stationary point, it must either have a large gradient or have a strictly negative Hessian, and we prove that
in either case, PGD must yield a signiﬁcant decrease in function value in a controlled number of iterations. Also, since the function value can not decrease more than f (x0) − f ⋆, we know that the total number of iterates that are not ǫ-second order stationary points can not be very large.
First, we bound the rate of decrease when the gradient is large.

Lemma 19 (Descent Lemma). If f (·) satisﬁes Assumption A and η ≤ 1/ℓ, then the gradient descent se-

quence {xt} satisﬁes:

f (xt+1) − f (xt) ≤ −η ∇f (xt) 2/2.

Proof. Due to the ℓ-gradient Lipschitz assumption, we have:

f (xt+1) ≤ f (xt) + ∇f (xt), xt+1 − xt + ℓ xt+1 − xt 2 2

= f (xt) − η ∇f (xt) 2 + η2ℓ ∇f (xt) 2 ≤ f (xt) − η ∇f (xt) 2

2

2

We now show that if the starting point has a strictly negative eigenvalue of the Hessian, then adding a perturbation and following by gradient descent will yield a signiﬁcant decrease in function value in T iterations.

Lanedmλmmain2(∇0 (2Efs(cx˜a)p)in≤g −Sa√dρdlǫe. LPeotinxt0s)=. Ax˜ss+umηeξ t(hξa∼t fU(·n)ifsoartmisﬁ(Bes0(Ars)s)u)m. RptuionngrAa,dx˜iensat tdiseﬁsecsent∇stfa(rxt˜in)g f≤romǫ,

x0. This yields:

√

P(f (xT

)

−

f (x˜)

≤

−F /2)

≥

1

−

ℓ √

d

· ι228−ι,

ρǫ

where xT is the T th gradient descent iterate starting from x0.

In order to prove this, we need to prove two lemmas, and the major simpliﬁcation over Jin et al. [2017a] comes from the following lemma which says that if function value does not decrease too much over t iterations, then all iterates {xτ }tτ=0 will remain in a small neighborhood of x0.
Lemma 21 (Improve or Localize). Under the setting of Lemma 19, for any t ≥ τ > 0:

xτ − x0 ≤ 2ηt(f (x0) − f (xt)).

Proof. Given the gradient update, xt+1 = xt − η∇f (xt), we have that for any τ ≤ t:

xτ − x0

t
≤

(1) t
xτ − xτ−1 ≤ [t

xτ − xτ −1 2] 12

τ =1

τ =1

t
= [η2t

∇f (xτ−1)

2] 12

(2)
≤

2ηt(f (x0) − f (xt)),

τ =1

where step (1) uses Cauchy-Schwarz inequality, and step (2) is due to Lemma 19.

13

Second, we show that the region in which GD will get remain in a small local neighborhood for at least T iterations if initialized there (which we refer to as the “stuck region”) is thin. We show this by tracking any pair of points that differ only in an escaping direction and are at least ω far apart. We show that at least one sequence of the two GD sequences initialized at these points is guaranteed to escape the saddle point with high probability, so that the width of the stuck region along an escaping direction is at most ω.
Le√mma 22 (Coupling Sequence). Suppose f (·) satisﬁes Assumption A and x˜ satisﬁes λmin(∇2f (x˜)) ≤ − ρǫ. Let {xt}, {x′t} be two gradient descent sequences which satisfy: (1) max{ x0 − x˜ , x′0 − x˜ } ≤ ηr; and (2) x0 − x′0 = ηr0e1, where e1 is the minimum eigenvector of ∇2f (x˜) and r0 > ω := 22−ιℓS . Then:
min{f (xT ) − f (x0), f (x′T ) − f (x′0)} ≤ −F .
Proof. Assume the contrary, that is, min{f (xT ) − f (x0), f (x′T ) − f (x′0)} > −F . Lemma 21 implies localization of both sequences around x˜; that is, for any t ≤ T :

max{ xt − x˜ , x′t − x˜ } ≤ max{ xt − x0 , x′t − x′0 } + max{ x0 − x˜ , x′0 − x˜ }

≤ 2ηT F + ηr ≤ S ,

(7)

where the last step is due to our choice of η, r, T , F , S , as in Eq. (6), and ℓ/√ρǫ ≥ 1.1 On the other hand, we can write the update equation for the difference xˆt := xt − x′t as:

xˆt+1 =xˆt − η[∇f (xt) − ∇f (x′t)] = (I − ηH)xˆt − η∆txˆt

t
= (I − ηH)t+1xˆ0 − η (I − ηH)t−τ ∆τ xˆτ ,

p(t+1)

τ =0

q(t+1)

where H = ∇2f (x˜) and ∆t = 01[∇2f (x′t + θ(xt − x′t) − H]dθ. We note that p(t) arises from the initial difference xˆ0, and q(t) is an error term which arises from the fact that the function f is not quadratic. We now use induction to show that the error term q(t) is always small compared to the leading term p(t). That is, we wish to show:
q(t) ≤ p(t) /2, t ∈ [T ].
The claim is true for the base case t = 0 as q(0) = 0 ≤ xˆ0 /2 = p(0) /2. Now suppose the induction claim is true up to t. Denote λmin(∇2f (x0)) = −γ. Note that xˆ0 lies in the direction of the minimum eigenvector of ∇2f (x0). Thus for any τ ≤ t, we have:

xˆτ ≤ p(τ ) + q(τ ) ≤ 2 p(τ ) = 2 (I − ηH)τ xˆ0 = 2(1 + ηγ)τ ηr0.

By the Hessian Lipschitz property, we have ∆t ≤ ρ max{ xt − x˜ , x′t − x˜ } ≤ ρS , therefore:

q(t + 1)

t

t

= η (I − ηH)t−τ ∆τ xˆτ ≤ ηρS

(I − ηH)t−τ xˆτ

τ =0

τ =0

t
≤ 2ηρS (1 + ηγ)tηr0 ≤ 2ηρS T (1 + ηγ)tηr0 ≤ 2ηρS T

τ =0

p(t + 1) ,

1We note that when ℓ/√ρǫ < 1, ǫ-second-order stationary points are equivalent to ǫ-ﬁrst-order stationary points due to the function f being ℓ-gradient Lipschitz. In this case, the problem of ﬁnding ǫ-second-order stationary points becomes straightforward.

14

0

Figure 1: Pertubation ball in 3D and “thin pancake” shape stuck region

Figure 2: Pertubation ball in 2D and “narrow band” stuck region under gradient ﬂow

where the second-to-last inequality uses t + 1 ≤ T . By our choice of the hyperparameter in Eq. (6), we have 2ηρS T ≤ 1/2, which ﬁnishes the inductive proof.
Finally, the induction claim implies:

max{ xT − x0 , x′T − x0 } ≥ 1 xˆ(T ) ≥ 1 [ p(T ) − q(T ) ] ≥ 1 [ p(T )

2

2

4

= (1 + ηγ)T

ηr0

(1)
≥

2ι−2ηr0

>

S,

4

where step (1) uses the fact (1 + x)1/x ≥ 2 for any x ∈ (0, 1]. This contradicts the localization property of Eq. (7), which ﬁnishes the proof.

Equipped with Lemma 21 and Lemma 22, we are ready to prove Lemma 20.

Proof of Lemma 20. Recall x0 ∼ Uniform(Bx˜(ηr)). We refer to Bx˜(ηr) the perturbation ball, and deﬁne the stuck region within the perturbation ball to be the set of points starting from which GD requires more than T steps to escape:

Xstuck := {x ∈ Bx˜(ηr) | {xt} is GD sequence with x0 = x, and f (xT ) − f (x0) > −F }.

See Figure 1 and Figure 2 for illustrations. Although the shape of the stuck region can be very complicated,

according to Lemma 22 we know that the width of Xstuck along the e1 direction is at most ηω. That is,

Vol(Xstuck) ≤ Vol(Bd0−1(ηr))ηω. Therefore:

Vol(X ) ηω × Vol(Bd−1(ηr))

ω Γ( d + 1) ω

√ d ℓd

P(x0

∈

Xstuck)

=

stuck
Vol(Bd (ηr))

≤

0
Vol(Bd(ηr))

= r√π Γ( d2 + 1 ) ≤ r · π ≤ √ρǫ · ι228−ι.

x˜

0

22

On the event {x0 ∈ Xstuck}, due to our parameter choice in Eq. (6), we have:

ℓη2r2 f (xT ) − f (x˜) = [f (xT ) − f (x0)] + [f (x0) − f (x˜)] ≤ −F + ǫηr + 2 ≤ −F /2.

This ﬁnishes the proof.

15

With Lemma 19 and Lemma 20 in hand, it is not hard to establish Theorem 18.

Proof of Theorem 18. First, we set the total number of iterations T to be:

T = 8 max (f (x0) − f ⋆)T , (f (x0) − f ⋆) = O ℓ(f (x0) − f ⋆) · ι4 .

F

ηǫ2

ǫ2

Next,

we

choose

ι

=

c

·

log(

dℓ∆f ρǫδ

),

with

a

large

enough

absolute

constant

c

such

that:

(T ℓ√d/√ρǫ) · ι228−ι ≤ δ.

We then argue that, with probability 1 − δ, Algorithm 4 will add a perturbation at most T /(4T ) times. This is because if otherwise, we can appeal to Lemma 20 every time we add a perturbation, and conclude:

f (xT ) ≤ f (x0) − T F /(4T ) < f ⋆,

which can not happen. Finally, excluding those iterations that are within T steps after adding perturbations, we still have 3T /4 steps left. They are either large gradient steps, ∇f (xt) ≥ ǫ, or ǫ-second order stationary points. Within them, we know that the number of large gradient steps cannot be more than T /4.
This is true because if otherwise, by Lemma 19:

f (xT ) ≤ f (x0) − T ηǫ2/4 < f ⋆,

which again cannot happen. Therefore, we conclude that at least T /2 of the iterates must be ǫ−second order stationary points.

6 Conclusions
We have shown that simple perturbed versions of GD and SGD escape saddle points and ﬁnd second-order stationary points in essentially the same time that classical GD and SGD take to ﬁnd ﬁrst-order stationary points. The overheads are only logarithmic factors in dimensionality in both the non-stochastic setting and the stochastic setting with Lipschitz stochastic gradient. In the general stochastic setting, the overhead is a linear factor in dimension.
Combined with previous literature that shows that all second-order stationary points are global optima for a broad class of nonconvex optimization problems in machine learning and signal processing, our results directly provide efﬁcient guarantees for solving those nonconvex problem via simple local search approaches. We now discuss several possible future directions, and further connections to other ﬁelds.
Optimal rates for ﬁnding second-order stationary points. Carmon et al. [2017a] have presented lower bounds that imply that GD achieves the optimal rate for ﬁnding stationary points for gradient Lipschitz functions. In our setting, we additionally assume that the Hessian is Lipschitz. This implies that GD is no longer necessarily an optimal algorithm. While our results show that variants of GD are efﬁcient in this setting, one would additionally like to know whether they are close to optimality.
Focusing on ﬁrst-order algorithms, the best known gradient query complexity for ﬁnding second-order stationary points of functions with Lipschitz gradient and Lipschitz Hessian is O˜(ǫ−1.75) [Carmon et al., 2016, Agarwal et al., 2017, Jin et al., 2017b], while the existing lower bound is Ω(ǫ−12/7) by Carmon et al. [2017b]. Note that this lower bound is restricted to deterministic algorithms, and thus does not apply to
16

most existing algorithms for escaping saddle points as they are all randomized algorithms. For the stochastic setting with Lipschitz stochastic gradients, the best known query complexity is O˜(ǫ−3) [Fang et al., 2018, Zhou and Gu, 2019], while the lower bound remains open. See Appendix A for further discussion of the literature.
Escaping high-order saddle points. The current paper focuses on escaping strict saddle points and ﬁnding second-order stationary points. More generally, one can deﬁne nth-order stationary points as points that satisﬁes the KKT necessary conditions for being local minima up to nth-order derivatives. It becomes more challenging to ﬁnd nth-order stationary points as n increases, since it is necessary to escape higherorder saddle points. In terms of worst-case efﬁciency, Nesterov [2000] rules out the possibility of efﬁcient algorithms for ﬁnding nth-order stationary points for all n ≥ 4, showing that the problem is NP-hard. Anandkumar and Ge [2016] present a third-order algorithm that ﬁnds third-order stationary points in polynomial time. It remains open whether simple variants of GD can also ﬁnd third-order stationary points efﬁciently. It is unlikely that the overhead will still be small or only logarithmic in this case, but it is not clear what to expect for the overhead. A related question is to identify applications where third-order stationarity is needed, in addition to second-order stationarity, to achieve global optimality.
Connection to gradient Langevin dynamics. The Bayesian counterpart of SGD is the Langevin Monte Carlo (LMC) algorithm [Roberts et al., 1996], which performs the following iterative update:
xt+1 = xt − η(∇f (xt) + 2/(ηβ)wt), where wt ∼ N (0, I).
Here β is known as the inverse temperature. When the step size η goes to zero, the distribution of the LMC iterates is known to converge to the stationary distribution µ(x) ∝ e−βf(x) [Roberts et al., 1996].
While the LMC algorithm is superﬁcially very close to stochastic gradient descent, the goals for the two algorithms are quite different.
• Convergence: While the focus of the optimization literature is to ﬁnd stationary points, the goal of the LMC algorithm is to converge to a stationary distribution (i.e., to mix rapidly).
• Scaling of Noise: The scaling of the stochasticity in SGD—and in particular the size of the perturbation that we consider in the current paper—is much smaller than the scaling considered in the LMC literature. Running our algorithm is equivalent to running LMC with temperature β−1 ∝ d−1. In this low-temperature or small-noise regime, the algorithm can no longer mix efﬁciently for smooth nonconvex functions, as it requires Ω(ed) steps in the worst case [Bovier et al., 2004]. However, with this small amount of noise, the algorithm can still perform local search efﬁciently, and can ﬁnd a second-order stationary point in a small number of iterations, as shown in Theorem 13.
Recent work of Zhang et al. [2017] studied the time that LMC takes to hit a second-order stationary point as a criterion for convergence, instead of the traditional mixing time to a stationary distribution. In this analysis, the runtime is no longer exponential, but it is still polynomially dependent on dimension d with large degree.
On the necessity of adding perturbations. We have shown that adding perturbations to the iterations of GD or SGD allows these algorithms to escape saddle points efﬁciently. As an alternative, one can also simply run GD with random initialization, and try to escape saddle points using only the randomness due to the initialization. Although this alternative algorithm exhibits asymptotic convergence [Lee et al., 2016], it
17

does not yield efﬁcient convergence in general. Du et al. [2017] shows that even with fairly natural random initialization schemes and non-pathological functions, randomly initialized GD can be signiﬁcantly slowed by saddle points, taking exponential time to escape them.
Acknowledgements
We thank Tongyang Li and Quanquan Gu for valuable discussions. This work was supported in part by the Mathematical Data Science program of the Ofﬁce of Naval Research under grant number N00014-181-2764. Rong Ge also acknowledges the funding support from NSF CCF-1704656, NSF CCF-1845171 (CAREER), Sloan Fellowship and Google Faculty Research Award.
References
Naman Agarwal, Zeyuan Allen-Zhu, Brian Bullins, Elad Hazan, and Tengyu Ma. Finding approximate local minima faster than gradient descent. In Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing, pages 1195–1199. ACM, 2017.
Zeyuan Allen-Zhu. Natasha 2: Faster non-convex optimization than SGD. In Advances in Neural Information Processing Systems, pages 2680–2691, 2018.
Zeyuan Allen-Zhu and Yuanzhi Li. Neon2: Finding local minima via ﬁrst-order oracles. arXiv preprint arXiv:1711.06673, 2017.
Animashree Anandkumar and Rong Ge. Efﬁcient approaches for escaping higher-order saddle points in non-convex optimization. In Conference on Learning Theory, pages 81–102, 2016.
Afonso S Bandeira, Nicolas Boumal, and Vladislav Voroninski. On the low-rank approach for semideﬁnite programs arising in synchronization and community detection. In Conference on Learning Theory, pages 361–382, 2016.
Srinadh Bhojanapalli, Behnam Neyshabur, and Nati Srebro. Global optimality of local search for low rank matrix recovery. In Advances in Neural Information Processing Systems, pages 3873–3881, 2016.
Nicolas Boumal, Vlad Voroninski, and Afonso Bandeira. The non-convex Burer-Monteiro approach works on smooth semideﬁnite programs. In Advances in Neural Information Processing Systems, pages 2757– 2765, 2016.
Anton Bovier, Michael Eckhoff, Ve´ronique Gayrard, and Markus Klein. Metastability in reversible diffusion processes I: Sharp asymptotics for capacities and exit times. Journal of the European Mathematical Society, 6(4):399–424, 2004.
Yair Carmon and John C Duchi. Gradient descent efﬁciently ﬁnds the cubic-regularized non-convex Newton step. arXiv preprint arXiv:1612.00547, 2016.
Yair Carmon, John C Duchi, Oliver Hinder, and Aaron Sidford. Accelerated methods for non-convex optimization. arXiv preprint arXiv:1611.00756, 2016.
Yair Carmon, John C Duchi, Oliver Hinder, and Aaron Sidford. Lower bounds for ﬁnding stationary points I. arXiv preprint arXiv:1710.11606, 2017a.
18

Yair Carmon, John C Duchi, Oliver Hinder, and Aaron Sidford. Lower bounds for ﬁnding stationary points II: First-order methods. arXiv preprint arXiv:1711.00841, 2017b.
Louis Augustin Cauchy. Me´thode ge´ne´rale pour la re´solution des syste´mes d’e´quations simultanees. C. R. Acad. Sci. Paris, 1847.
Anna Choromanska, Mikael Henaff, Michael Mathieu, Ge´rard Ben Arous, and Yann LeCun. The loss surface of multilayer networks. arXiv:1412.0233, 2014.
Frank E Curtis, Daniel P Robinson, and Mohammadreza Samadi. A trust region algorithm with a worst-case iteration complexity of o(ǫ−3/2) for nonconvex optimization. Mathematical Programming, pages 1–32, 2014.
Hadi Daneshmand, Jonas Kohler, Aurelien Lucchi, and Thomas Hofmann. Escaping saddles with stochastic gradients. arXiv preprint arXiv:1803.05999, 2018.
Simon S Du, Chi Jin, Jason D Lee, Michael I Jordan, Aarti Singh, and Barnabas Poczos. Gradient descent can take exponential time to escape saddle points. In Advances in Neural Information Processing Systems, pages 1067–1077, 2017.
Cong Fang, Chris Junchi Li, Zhouchen Lin, and Tong Zhang. Spider: Near-optimal non-convex optimization via stochastic path-integrated differential estimator. In Advances in Neural Information Processing Systems, pages 687–697, 2018.
Cong Fang, Zhouchen Lin, and Tong Zhang. Sharp analysis for nonconvex SGD escaping from saddle points. arXiv preprint arXiv:1902.00247, 2019.
Rong Ge, Furong Huang, Chi Jin, and Yang Yuan. Escaping from saddle points—online stochastic gradient for tensor decomposition. In Conference on Computational Learning Theory (COLT), 2015.
Rong Ge, Jason D Lee, and Tengyu Ma. Matrix completion has no spurious local minimum. In Advances in Neural Information Processing Systems, pages 2973–2981, 2016.
Rong Ge, Chi Jin, and Yi Zheng. No spurious local minima in nonconvex low rank problems: A uniﬁed geometric analysis. arXiv preprint arXiv:1704.00708, 2017.
Saeed Ghadimi and Guanghui Lan. Stochastic ﬁrst-and zeroth-order methods for nonconvex stochastic programming. SIAM Journal on Optimization, 23(4):2341–2368, 2013.
Chi Jin, Rong Ge, Praneeth Netrapalli, Sham M Kakade, and Michael I Jordan. How to escape saddle points efﬁciently. In International Conference on Machine Learning (ICML), 2017a.
Chi Jin, Praneeth Netrapalli, and Michael I Jordan. Accelerated gradient descent escapes saddle points faster than gradient descent. arXiv preprint arXiv:1711.10456, 2017b.
Chi Jin, Praneeth Netrapalli, Rong Ge, Sham M. Kakade, and Michael I. Jordan. A short note on concentration inequalities for random vectors with subgaussian norm. arXiv preprint arXiv:1902.03736, 2019.
Jason D Lee, Max Simchowitz, Michael I Jordan, and Benjamin Recht. Gradient descent only converges to minimizers. In Conference on Learning Theory, pages 1246–1257, 2016.
19

Lihua Lei, Cheng Ju, Jianbo Chen, and Michael I. Jordan. Finding approximate local minima faster than gradient descent. In Advances in Neural Information Processing (NIPS) 31, Red Hook, NY, 2018. Curran Associates.
Kﬁr Y Levy. The power of normalization: Faster evasion of saddle points. arXiv preprint arXiv:1611.04831, 2016.
Song Mei, Theodor Misiakiewicz, Andrea Montanari, and Roberto I Oliveira. Solving SDPs for synchronization and maxcut problems via the Grothendieck inequality. In Conference on Learning Theory (COLT), pages 1476–1515, 2017.
Arkadii Nemirovskii and David Borisovich Yudin. Problem Complexity and Method Efﬁciency in Pptimization. Wiley, 1983.
Yurii Nesterov. A method of solving a convex programming problem with convergence rate o (1/k2). Soviet Mathematics Doklady, 27:372–376, 1983.
Yurii Nesterov. Introductory Lectures on Convex Programming Volume I: Basic Course. Springer, 1998.
Yurii Nesterov. Squared functional systems and optimization problems. In High performance optimization, pages 405–440. Springer, 2000.
Yurii Nesterov and Boris T Polyak. Cubic regularization of Newton method and its global performance. Mathematical Programming, 108(1):177–205, 2006.
Boris T Polyak. Gradient methods for the minimisation of functionals. USSR Computational Mathematics and Mathematical Physics, 3(4):864–878, 1963.
Benjamin Recht, Christopher Re, Stephen Wright, and Feng Niu. Hogwild: A lock-free approach to parallelizing stochastic gradient descent. In Advances in neural information processing systems, pages 693– 701, 2011.
Sashank J Reddi, Manzil Zaheer, Suvrit Sra, Barnabas Poczos, Francis Bach, Ruslan Salakhutdinov, and Alexander J Smola. A generic approach for escaping saddle points. arXiv preprint arXiv:1709.01434, 2017.
Herbert Robbins and Sutton Monro. A stochastic approximation method. Annals of Mathematical Statistics, pages 400–407, 1951.
Gareth O Roberts, Richard L Tweedie, et al. Exponential convergence of Langevin distributions and their discrete approximations. Bernoulli, 2(4):341–363, 1996.
Virginia Smith, Simone Forte, Chenxin Ma, Martin Takac, Michael I. Jordan, and Martin Jaggi. Cocoa: A general framework for communication-efﬁcient distributed optimization. Journal of Machine Learning Research, 18:1–49, 2018.
Ju Sun, Qing Qu, and John Wright. Complete dictionary recovery over the sphere I: Overview and the geometric picture. IEEE Transactions on Information Theory, 2016a.
Ju Sun, Qing Qu, and John Wright. A geometric analysis of phase retrieval. In Information Theory (ISIT), 2016 IEEE International Symposium on, pages 2379–2383. IEEE, 2016b.
20

Nilesh Tripuraneni, Mitchell Stern, Chi Jin, Jeffrey Regier, and Michael I Jordan. Stochastic cubic regularization for fast nonconvex optimization. In Advances in Neural Information Processing Systems, pages 2904–2913, 2018.
Yi Xu, Jing Rong, and Tianbao Yang. First-order stochastic algorithms for escaping from saddle points in almost linear time. In Advances in Neural Information Processing Systems, pages 5535–5545, 2018.
Yuchen Zhang, Martin J Wainwright, and John C Duchi. Communication-efﬁcient algorithms for statistical optimization. In Advances in Neural Information Processing Systems, pages 1502–1510, 2012.
Yuchen Zhang, Percy Liang, and Moses Charikar. A hitting time analysis of stochastic gradient Langevin dynamics. arXiv preprint arXiv:1702.05575, 2017.
Dongruo Zhou and Quanquan Gu. Stochastic recursive variance-reduced cubic regularization methods. arXiv preprint arXiv:1901.11518, 2019.
Dongruo Zhou, Pan Xu, and Quanquan Gu. Finding local minima via stochastic nested variance reduction. arXiv preprint arXiv:1806.08782, 2018.
21

Algorithm Noisy GD [Ge et al., 2015] Normalized GD [Levy, 2016] PGD (conference verison of this work)
[Jin et al., 2017a] †Perturbed AGD [Jin et al., 2017b]
FastCubic [Agarwal et al., 2017] Carmon et al. [2016]
Carmon and Duchi [2016]

Iterations d4poly(ǫ−1) O(d3 · poly(ǫ−1))
O˜(ǫ−2)
O˜(ǫ−1.75) O˜(ǫ−1.75) O˜(ǫ−1.75) O˜(ǫ−2)

Simplicity single-loop
double-loop

Table 2: A summary of related work on ﬁrst-order algorithms to ﬁnd second-order stationary points in nonstochastic settings. This table only highlights the dependencies on d and ǫ.† denotes followup work on the
conference version of this paper [Jin et al., 2017a].

Algorithm
Noisy GD [Ge et al., 2015] CNC-SGD [Daneshmand et al., 2018]
PSGD (this work) ∗SGD with averaging [Fang et al., 2019]
Natasha 2 [Allen-Zhu, 2018] Stochastic Cubic [Tripuraneni et al., 2018]
SPIDER [Fang et al., 2018] SRVRC [Zhou and Gu, 2019]

Iterations (with Assumption C)
d4poly(ǫ−1) O˜(d4ǫ−5) O˜(ǫ−4) O˜(ǫ−3.5)
O˜(ǫ−3.5) O˜(ǫ−3.5) O˜(ǫ−3) O˜(ǫ−3)

Iterations (no Assumption C)
d4poly(ǫ−1) O˜(d4ǫ−5) O˜(dǫ−4)
×
× × × ×

Simplicity single-loop
double-loop

Table 3: A summary of related work on ﬁrst-order algorithms to ﬁnd second-order stationary points in the stochastic setting. This table only highlights the dependencies on d and ǫ. ∗ denotes independent work.

A Tables of Related Work
In Table 2 and Table 3, we present a detailed comparison of our results with other related work in both nonstochastic and stochastic settings. See Section 1.1 for the full text descriptions. We note that our algorithms are simple variants of standard GD and SGD, which are the simplest among all the algorithms listed in the
22

table.

B Proofs for Stochastic Setting
In this section, we provide proofs for our main results—Theorem 16 and Theorem 17. Theorem 13 can be proved as a special case of Theorem 16 by taking σ = 0.

B.1 Notation
Recall the update equation of Algorithm 2 is xt+1 ← xt − η(g(xt; θt) + ξt), where ξt ∼ N (0, (r2/d)I). Throughout this section, we denote ζt := g(xt; θt)−∇f (xt), as the noise part within the stochastic gradient. For simplicity, we also denote ζ˜t := ζt + ξt, which is the summation of noise from the stochastic gradient and the injected perturbation, and σ˜2 := σ2 + r2. Then the update equation can be rewritten as xt+1 ← xt − η(∇f (xt) + ζ˜t). We also denote Ft = σ(ζ0, ξ0, . . . , ζt, ξt) as the corresponding ﬁltration up to time step t. We choose parameters in Algorithm 2 as follows:

1

√

ι

1 ǫ3

2ǫ

η = ι9 · ℓN , r = ι · ǫ N, T := η√ρǫ , F := ι5 ρ , S := ι2 ρ , (8)

where N and the log factor ι are deﬁned as: σ2 ℓ˜2 σ2d
N = 1 + min ǫ2 + ℓ√ρǫ , ǫ2 ,

ι = µ · log dℓ∆f N . ρǫδ

Here, µ is a sufﬁciently large absolute constant to be determined later. Note also that throughout this section c denotes an absolute constant that does not depend on the choice of µ. Its value may change from line to line.

B.2 Descent lemma

We ﬁrst prove that the change in the function value can be always decomposed into the decrease due to the magnitudes of gradients and the possible increase due to randomness in both the stochastic gradients and the perturbations.

Lemma 23 (Descent Lemma). Under Assumption A, B, there exists an absolute constant c such that, for any ﬁxed t, t0, ι > 0, if η ≤ 1/ℓ, then with at least 1 − 4e−ι probability, the sequence PSGD(η, r) (Algorithm 2) satisﬁes: (denote σ˜2 = σ2 + r2)

η t−1 f (xt0+t) − f (xt0 ) ≤ − 8
i=0

∇f (xt0+i) 2 + c · ησ˜2(ηℓt + ι).

Proof. Since Algorithm 2 is Markovian, the operations in each iteration do not depend on time step t. Thus, it sufﬁces to prove Lemma 23 for special case t0 = 0. Recall the update equation:

xt+1 ← xt − η(∇f (xt) + ζ˜t),

23

where ζ˜t = ζt + ξt. By assumption, we know ζt|Ft−1 is zero-mean nSG(σ). Also ξt|Ft−1 comes from N (0, (r2/d)I), and thus by Lemma 33 it is zero-mean nSG(c · r) for some absolute constant c. By a Taylor expansion and the assumptions of an ℓ-gradient Lipschitz and η ≤ 1/ℓ, we have:

f (xt+1) ≤f (xt) + ∇f (xt), xt+1 − xt + ℓ xt+1 − xt 2 2
≤f (xt) − η ∇f (xt), ∇f (xt) + ζ˜t + η2ℓ 3 ∇f (xt) 2 + 3 ζ˜t 2 22

≤f (xt) − η ∇f (xt) 2 − η ∇f (xt), ζ˜t + 3 η2ℓ ζ˜t 2.

4

2

Summing over this inequality, we have the following:

f (x ) − f (x ) ≤ − η t−1

t−1
∇f (x ) 2 − η ∇f (x ), ζ˜

+ 3 η2ℓ t−1

ζ˜

2.

(9)

t

0

4

i

ii 2

i

i=0

i=0

i=0

For the second term on the right-hand side, applying Lemma 39, there exists an absolute constant c, such that, with probability 1 − 2e−ι:

t−1
−η ∇f (x ), ζ˜

≤ η t−1

∇f (x ) 2 + cησ˜2ι.

ii 8

i

i=0

i=0

For the third term on the right-hand side of Eq. (9), applying Lemma 38, with probability 1 − 2e−ι:

3 η2ℓ t−1 2 i=0

t−1
ζ˜i 2 ≤ 3η2ℓ ( ζi 2 +
i=0

ξi 2) ≤ cη2ℓσ˜2(t + ι).

Substituting these inequalities into Eq. (9), and noting the fact η ≤ 1/ℓ, we have with probability 1 − 4e−ι:

This ﬁnishes the proof.

f (xt) − f (x0) ≤ − η8 t−1 ∇f (xi) 2 + cησ˜2(ηℓt + ι).
i=0

The descent lemma allows us to show the following “Improve or Localize” phenomenon for perturbed SGD. That is, with high probability over a small number of iterations, either the function value decreases signiﬁcantly, or the iterates stay within a small local region.

Lemma 24 (Improve or Localize). Under the same setting as in Lemma 23, with probability at least 1 − 8dt · e−ι, the sequence PSGD(η, r) (Algorithm 2) satisﬁes:

∀τ ≤ t : xt0+τ − xt0 2 ≤ cηt · [f (xt0 ) − f (xt0+τ ) + ησ˜2(ηℓt + ι)].

Proof. By a similar argument as in the proof of Lemma 23, it sufﬁces to prove Lemma 24 in the special case t0 = 0. According to Lemma 23, with probability 1 − 4e−ι, for some absolute constant c:
t−1 ∇f (xi) 2 ≤ η8 [f (x0) − f (xt)] + cσ˜2(ηℓt + ι).
i=0

24

Therefore, for any ﬁxed τ ≤ t, with probability 1 − 8d · e−ι,

xτ − x0

τ −1
2 =η2 (∇f (xi) + ζ˜i) 2 ≤ 2η2

τ −1

τ −1

∇f (xi) 2 +

ζ˜i 2

i=0

i=0

i=0

(1)

τ −1

t−1

≤ 2η2t ∇f (xi) 2 + cη2σ˜2tι ≤ 2η2t ∇f (xi) 2 + cη2σ˜2tι

i=0

i=0

≤cηt[f (x0) − f (xt) + ησ˜2(ηℓt + ι)].

where in step (1) we use the Cauchy-Schwarz inequality and Lemma 36. Finally, applying a union bound for all τ ≤ t, we ﬁnish the proof.

B.3 Escaping saddle points

Lemma 23 shows that large gradients contribute to the fast decrease of the function value. In this subsection, we will show that starting in the vicinity of strict saddle points will also enable PSGD to decrease the function value rapidly. Concretely, this entire subsection will be devoted to proving the following lemma:

Lemma 25 (Escaping Saddle Point). Given Assumption A, B, there exists an absolute constant cmax such

that, for any ﬁxed t0 > 0, ι > cmax log(ℓ
2

√d/(ρǫ)), if η, r, F , T

are chosen as in Eq. (8), and xt0 satisﬁes

∇f (xt0) ≤ ǫ and λmin(∇ f (xt0)) ≤ − ρǫ, then the sequence PSGD(η, r) (Algorithm 2) satisﬁes:

P(f (xt0+T ) − f (xt0 ) ≤ 0.1F ) ≥ 1 − 4e−ι and √
P(f (xt0+T ) − f (xt0 ) ≤ −F ) ≥ 1/3 − 5dT 2 · log(S d/(ηr))e−ι.

Since Algorithm 2 is Markovian, the operations in each iterations do not depend on time step t. Thus, it sufﬁces to prove Lemma 25 for special case t0 = 0. To prove this lemma, we ﬁrst need to introduce the concept of a coupling sequence.

Notation: Throughout this subsection, we let H := ∇2f (x0), e1 be the minimum eigendirection of H, and γ := λmin(H). We also let P−1 be the projection onto the subspace complement of e1. Deﬁnition 26 (Coupling Sequence). Consider sequences {xi} and {x′i} that are obtained as separate runs of the PSGD (algorithm 2), both starting from x0. They are coupled if both sequences share the same randomness P−1ξτ and θτ , while in e1 direction we have e⊤1 ξτ = −e⊤1 ξτ′ .
The ﬁrst thing we can show is that if the function values of both sequences do not exhibit a sufﬁcient decrease, then both sequences are localized in a small ball around x0 within T iterations.
Lemma 27 (Localization). Under the notation of Lemma 28, we have:
P(min{f (xT ) − f (x0),f (x′T ) − f (x0)} ≤ −F , or ∀t ≤ T : max{ xt − x0 2, x′t − x0 2} ≤ S 2) ≥ 1 − 16dT · e−ι.
Proof. This lemma follows from applying Lemma 24 on both sequences and using a union bound.
The overall proof strategy for Lemma 25 is to show that localization happens with a very small probability, thus at least one of the sequence must have sufﬁcient descent. In order to prove this, we study the dynamics of the difference of the coupling sequence.

25

Lemma 28 (Dynamics of the Coupling Sequence Difference). Consider coupling sequences {xi} and {x′i} as in Deﬁnition 26 and let xˆt := xi − x′i. Then xˆt = −qh(t) − qsg(t) − qp(t), where:

t−1

t−1

t−1

qh(t) := η (I − ηH)t−1−τ ∆τ xˆτ , qsg(t) := η (I − ηH)t−1−τ ζˆτ , qp(t) := η (I − ηH)t−1−τ ξˆτ .

τ =0

τ =0

τ =0

Here ∆t :=

1 0

∇2

f

(ψ

xt

+ (1 − ψ)x′t)dψ

− H,

and

ζˆτ

:=

ζτ

− ζτ′ ,

ξˆτ

:=

ξτ

− ξτ′ .

Proof. Recall ζi = g(xi; θi) − ∇f (xi), thus, we have the update formula:

xt+1 = xt − η(g(xt; θt) + ξt) = xt − η(∇f (xt) + ζt + ξt).

Taking the difference between {xi} and {x′i}:

xˆt+1 =xt+1 − x′t+1 = xˆt − η(∇f (xt) − ∇f (x′t) + ζt − ζt′ + (ξt − ξt′)) =xˆt − η[(H + ∆t)xˆt + ζˆt + ξˆt] = (I − ηH)xˆt − η[∆txˆt + ζˆt + e1e⊤1 ξˆt]

= − η t(I − ηH)t−τ (∆τ xˆτ + ζˆτ + ξˆτ )),
τ =0

where ∆t :=

1 0

∇2f (ψxt

+

(1

−

ψ)x′t)dψ

−

H.

This

ﬁnishes

the

proof.

At a high level, we will show that with constant probability, qp(t) is the dominating term which controls the behavior of the dynamics, and qh(t) and qsg(t) will stay small compared to qp(t). To show this, we prove the following three lemmas.

Lemma 29. Denote α(t) := tτ−=10(1 + ηγ)2(t−1−τ√) 21 and β(t) := (1 + ηγ)t/√2ηγ. If ηγ ∈ [0, 1], then (1) α(t) ≤ β(t) for any t ∈ N; and (2) α(t) ≥ β(t)/ 3 for t ≥ ln(2)/(ηγ).

Proof. By summing a geometric sequence:

α2(t)

:=

t−1
(1

+

ηγ)2(t−1−τ )

=

(1

+

ηγ)2t

−

1.

τ=0 2ηγ + (ηγ)2

Thus, the claim that α(t) ≤ β(t) for any t ∈ N follows immediately. On the other hand, note that for t ≥ ln(2)/(ηγ), we have (1+ηγ)2t ≥ 22 ln 2 ≥ 2, where the second claim follows by a short calculation.

Lemma 30. Under the notation of Lemma 28 and Lemma 29, letting −γ := λmin(H), we have ∀t > 0:

P( qp(t)

≤

cβ√(t)ηr

·

√ ι)

≥

1

−

2e−ι

d

P( qp(T ) ≥ β(T√)ηr ) ≥ 2 . 10 d 3

Proof.

Note that ξˆτ

√ is one-dimensional Gaussian random variable with standard deviation 2r/ d along the

e1 direction. As an immediate result, η tτ=0(I −ηH)t−τ ξˆτ is also a one-dimensional Gaussian distribution

since the summation of Gaussians is again Gaussian. Finally note that e1 is an eigendirection of H with cor-

responding eigenvalue −γ, and by Lemma 29 we have α(t) ≤ β(t). Then, the ﬁrst inequality immediately

follows from the standard concentration of Ga√ussian measure, and the second inequality follows from the fact if Z ∼ N (0, σ2) then P(|Z| ≤ λσ) ≤ 2λ/ 2π ≤ λ.

26

Lemma 31. There exists an absolute constant cmax such that, for any ι ≥ cmax, under the notation of Lemma 28 and Lemma 29, and letting −γ := λmin(H), we have:

P(min{f (xT ) − f (x0),f (x′T ) − f (x0)} ≤ −F , or

√

∀t ≤ T :

qh(t) + qsg(t)

≤

β(t√)ηr )

≥

1

−

10dT

2

·

S log(

d )e−ι.

20 d

ηr

Proof. For simplicity we denote E as the event {∀τ ≤ t : max{ xτ − x0 2, x′τ − x0 2} ≤ S 2}. We use induction to prove following claim for any t ∈ [0, T ]:

√

P(E ⇒ ∀τ ≤ t :

qh(τ ) + qsg(τ )

≤

β(τ√)ηr )

≥

1

−

10dT

t

·

S log(

d )e−ι

20 d

ηr

Then Lemma 31 follows directly from combining Lemma 27 and the induction claim. Clearly for the base case t = 0, the claim holds trivially, as qsg(0) = qh(0) = 0. Suppose the claim
holds for t, then by Lemma 30, with probability at least 1 − 2T e−ι, we have for any τ ≤ t:

xˆτ

≤ η qh(τ ) + qsg(τ )

+ η qp(τ )

≤

cβ√(τ )ηr

·

√ ι.

d

Then, under the condition max{ xτ − x0 2, x′τ − x0 2} ≤ S 2, by the Hessian Lipschitz property, we

have ∆τ =

1 0

∇2f (ψxτ

+

(1

−

ψ)x′τ )dψ

−

H

≤ ρ max{ xτ − x0 ,

x′τ − x0 } ≤ ρS . This gives

bounds on qh(t + 1) terms as:

qh(t + 1)

t
≤ η (1 + ηγ)t−τ ρS xˆτ
τ =0

≤ ηρS T cβ√(t)ηr ≤ β(t√)ηr ,

d

40 d

where the last step is due to ηρS T = 1/ι by Eq. (8). By picking ι larger than the absolute constant 40c, we have cηρS T ≤ 1/40.
Recall also that ζˆτ |Fτ−1 is the summation of a nSG(σ) random vector and a nSG(c · r) random vector. By Lemma 36, we know that with probability at least 1 − 4de−ι:
√ qsg(t + 1) ≤ cβ(t + 1)ησ ι

On the other hand, when assumption C is avaliable, we also have ζˆτ |Fτ−1 ∼ nSG(ℓ˜ xˆτ ), by applying Lemma√37 with B = α2(t) · η2ℓ˜2S 2; b = α2(t) · η2ℓ˜2 · η2r2/d, we know with probability at least 1 − 4d · log(S d/(ηr)) · e−ι:

qsg(t + 1) ≤ cηℓ˜ t (1 + ηγ)2(t−τ) · max{ xˆτ 2, η2dr2 }ι ≤ ηℓ˜√T · cβ√(td)ηr · √ι. (10)
τ =0

Finally, combining both cases, and by our choice of step size η, r as in Eq. (8) with ι large enough:

β(t)ηr

√

√ σ dι

β(t)r

qsg(t + 1) ≤ c √ · min{ηℓ˜ T ι,

}≤ √

d

r

40 d

and the induction follows by the triangle inequality and a union bound.

27

We are ready to prove Lemma 25, which is the focus of this subsection.

Proof of Lemma 25. We ﬁrst prove the ﬁrst claim P(f (xT ) − f (x0) ≤ 0.1F ) ≥ 1 − 4e−ι. Because of our choice of step size and Lemma 23, we have with probability 1 − 4e−ι:

f (xT ) − f (x0) ≤ cησ˜2(ηℓT + ι) ≤ 0.1F ,

where the last step is because our choice of parameters in Eq. (8)implies cησ˜2(ηℓT + ι) ≤ 2cF /ι and we

pick ι to be larger than an absolute constant 20c.

√

For the second claim, P(f (xT ) − f (x0) ≤ −F ) ≥ 1/3 − 5dT 2 · log(S d/(ηr))e−ι, we consider

coupling sequences {xi} and {x′i} as deﬁned in D√eﬁnition 26. Given Lemma 30 and Lemma 31, we know

that with probability at least 2/3−10dT 2 ·log(S d/(ηr))e−ι, if min{f (xT )−f (x0), f (x′T )−f (x0)} >

−F —i.e., both sequences are stuck around the saddle point—we must have:

qp(T ) ≥ β(T√)ηr , 10 d

qh(T ) + qsg(T ) ≤ β(T√)ηr . 20 d

By Lemma 28, when ι ≥ c · log(ℓ d/(ρǫ)) for a large absolute constant c, we have:

max{ xT − x0 , x′T − x0 } ≥ 1 xˆ(T ) ≥ 1 [ qp(T ) − qh(T ) + qsg(T ) ]

2

2

β(T )ηr (1 + ηγ)T ηr 2ιηr ≥ 40√d = 40√2ηγd ≤ 80√ηℓd , > S

which contradicts with Lemma 27. Ther√efore, we can conclude that P(min{f (xT ) − f (x0), f (x′T ) − f (x0)} ≤ −F ) ≥ 2/3 − 10dT 2 · log(S d/(ηr))e−ι. We also know that the marginal distribution of xT and x′T is the same, thus they have same probability to escape the saddle point. That is:

P(f (xT ) − f (x0) ≤ −F ) ≥ 1 P(min{f (xT ) − f (x0), f (x′T ) − f (x0)} ≤ −F )

2

√

≥1/3 − 5dT 2 · log(S d/(ηr))e−ι.

This ﬁnishes the proof.

B.4 Proof of Theorem 16

Lemma 23 and Lemma 25 describe the speed of decrease in the function values when either large gradients or strictly negative curvatures are present. Combining them gives the proof for our main theorem.

Proof of Theorem 16. First, we set the total number of iterations T to be:

T = 100 max (f (x0) − f ⋆)T , (f (x0) − f ⋆) = O ℓ(f (x0) − f ⋆) · N · ι9 .

F

ηǫ2

ǫ2

We will show that the following two claims hold simultaneously with probability 1 − δ:

1. At most T /4 iterates have large gradient; i.e., ∇f (xt) ≥ ǫ; 2. At most T /4 iterates are close to saddle points; i.e., ∇f (xt) ≤ ǫ and λmin(∇2f (xt)) ≤ −√ρǫ.

Therefore, at least T /2 iterates are ǫ-second order stationary point. We prove the two claims separately.

28

Claim 1. Suppose that within T steps, we have more than T /4 iterates for which gradient is large (i.e., ∇f (xt) ≥ ǫ). Recall that by Lemma 23 we have with probability 1 − 4e−ι:
f (xT ) − f (x0) ≤ − η8 T −1 ∇f (xi) 2 + cησ˜2(ηℓT + ι) ≤ −η T3ǫ22 − σ˜2(ηℓT + ι) .
i=0
Note that by our choice of η, r, T and picking ι larger than some absolute constant, we have T ǫ2/32 − σ˜2(ηℓT + ι) ≥ T ǫ2/64, and thus f (xT ) ≤ f (x0) − T ηǫ2/64 < f ⋆ which is not possible.

Claim 2. We ﬁrst deﬁne the stopping times that allow us to invoke Lemma 25: z1 = inf{τ | ∇f (xτ ) ≤ ǫ and λmin(f (xτ )) ≤ −√ρǫ} zi = inf{τ | τ > zi−1 + T and ∇f (xτ ) ≤ ǫ and λmin(f (xτ )) ≤ −√ρǫ},

∀i > 1.

Clearly, zi is a stopping time, and it is the ith time in the sequence along which we can apply Lemma 25. We also let M be the random variable M = max{i|zi + T ≤ T }. We can decompose the decrease f (xT ) − f (x0) as follows:

M
f (xT ) − f (x0) = [f (xzi+T ) − f (xzi)]
i=1
T1 M −1
+ [f (xT ) − f (xzM )] + [f (xz1 ) − f (x0)] + [f (xzi+1 ) − f (xzi+T )] .
i=1
T2

For the ﬁrst term T1, by Lemma 25 and a supermartingale concentration inequality, for each ﬁxed m ≤ T :

P m [f (xzi+T ) − f (xzi)] ≤ −(0.9m − c√m · ι)F ≥ 1 − 5dT 2T · log(S √d/(ηr))e−ι.
i=1

Since th√e random variable M ≤ T /T ≤ T , by a union bound, we know that with probability 1 − 5dT 2T 2 ·

log(S d/(ηr))e−ι:

√

T1 ≤ −(0.9M − c M · ι)F .

For the second term, by a union bound and Lemma 23 for all 0 ≤ t1, t2 ≤ T , with probability 1 − 4T 2e−ι:

T2 ≤ c · ησ˜2(ηℓT + 2M ι)

Therefore, if within T s√teps we have more than T /4 saddle points, then M ≥ T /4T , and with probaility 1 − 10dT 2T 2 · log(S d/(ηr))e−ι:
√ f (xT ) − f (x0) ≤ −(0.9M − c M · ι)F + c · ησ˜2(ηℓT + 2M ι) ≤ −0.4M F ≤ −0.4T F /T .

This will gives f (xT ) ≤ f (x0) − 0.4T F /T < f ⋆ which is not possible.

Finally, it is not hard to verify, by choosing ι = c · log

dℓ∆f N ρǫδ

for a large enough value of the absolute

constant c, we can make both claims hold with probability 1 − δ.

29

B.5 Proof of Theorem 17
Our proofs for PSGD easily generalize to the mini-batch setting.
Proof of Theorem 17. The proof is essentially the same as the proof of Theorem 16. The only difference is that, up to a log factor, mini-batch PSGD reduces the variance σ2 and ℓ˜2 xˆτ 2 in Eq. (10) by a factor of m, where m is the size of the mini-batch.

C Concentration Inequalities

In this section, we present the concentration inequalities required for this paper. Please refer to the technical note [Jin et al., 2019] for the proofs of Lemmas 33, 34, 36 and 37.
Recall the deﬁnition of a norm-subGaussian random vector.

Deﬁnition 32. A random vector X ∈ Rd is norm-subGaussian (or nSG(σ)), if there exists σ so that:

P ( X − EX

≥

t)

≤

2e−

t2 2σ2

,

∀t ∈ R.

Note that a bounded random vector and a subGaussian random vector are two special cases of a normsubGaussian random vector.
Lemma 33. There exists an absolute constant c so that following random vectors are nSG(c · σ).
1. A bounded random vector X ∈ Rd such that X ≤ σ.
2. A random vector X ∈ Rd, where X = ξe1 and the random variable ξ ∈ R is σ-subGaussian. 3. A random vector X ∈ Rd that is (σ/√d)-subGaussian.

Second, we have that if X is norm-subGaussian, then its norm square is subExponential, and its component along a single direction is subGaussian.
Lemma 34. There is an absolute constant c so that if the random vector X ∈ Rd is zero-mean nSG(σ), then X 2 is c · σ2-subExponential, and for any ﬁxed unit vector v ∈ Sd−1, v, X is c · σ-subGaussian.

For concentration, we are interested in the properties of norm-subGaussian martingale difference sequences. Concretely, they are sequences satisfying the following conditions.

Condition 35. Consider random vectors X1, . . . , Xn ∈ Rd, and corresponding ﬁltrations Fi = σ(X1, . . . , Xi) for i ∈ [n], such that Xi|Fi−1 is zero-mean nSG(σi) with σi ∈ Fi−1. That is:

E[Xi|Fi−1] = 0,

− t2
P ( Xi ≥ t|Fi−1) ≤ 2e 2σi2 ,

∀t ∈ R, ∀i ∈ [n].

Similar to subGaussian random variables, we can also prove a Hoeffding-type inequality for normsubGaussian random vectors which is tight up to a log(d) factor.

Lemma 36 (Hoeffding-type inequality for norm-subGaussian). Given X1, . . . , Xn ∈ Rd that satisfy condi-

tion 35, with ﬁxed {σi}, then for any ι > 0, there exists an absolute constant c such that, with probability at least 1 − 2d · e−ι:

n

n

Xi ≤ c ·

σi2 · ι.

i=1

i=1

30

When {σi} is also random, we have the following. Lemma 37. Given X1, . . . , Xn ∈ Rd that satisfy condition 35, then for any ι > 0, and B > b > 0, there exists an absolute constant c such that, with probability at least 1 − 2d log(B/b) · e−ι:

n
σi2 ≥ B or
i=1

n
Xi
i=1

≤c·

n
max{ σi2, b} · ι.
i=1

Finally, we can also provide concentration inequalities for the sum of norm squares of norm-subGaussian random vectors, and for the sum of inner products of norm-subGaussian random vectors with another set of random vectors.

Lemma 38. Given X1, . . . , Xn ∈ Rd that satisfy Condition 35 with ﬁxed σ1 = . . . = σn = σ, then there exists an absolute constant c such that, for any ι > 0, with probability at least 1 − e−ι:

n
Xi 2 ≤ c · σ2 (n + ι) .
i=1

Proof. Note there exists an absolute constant c such that E[ Xi 2|Fi−1] ≤ c · σ2, and Xi 2|Fi−1 is c · σ2-subExponential. This lemma directly follows from standard Bernstein concentration inequalities for subExponential random variables.

Lemma 39. Given X1, . . . , Xn ∈ Rd that satisfy Condition 35 and random vectors {ui} that satisfy ui ∈

Fi−1 for all i ∈ [n], then for any ι > 0, λ > 0, there exists absolute constant c such that, with probability at

least 1 − e−ι:

ui, Xi ≤ c · λ ui 2σi2 + λ1 · ι.

i

i

Proof. For any i ∈ [n] and ﬁxed λ > 0, since ui ∈ Fi−1, according to Lemma 34 there exists a constant c such that ui, Xi |Fi−1 is c · ui σi-subGaussian. Thus:

E[eλ ui,Xi |Fi−1] ≤ ec·λ2 ui 2σi2 .

Therefore, consider the following quantity:

Ee

ti=1(λ ui,Xi −c·λ2 ui 2σi2) = E e

t−1 i=1

λ

ui ,Xi

−c·

t i=1

λ2

ui

2 σi2

·E

eλ ut,Xt |Ft−1

≤E e

t−1 i=1

λ

ui ,Xi

−c·

t i=1

λ2

ui

2 σi2

· ec·λ2

ut

2 σt2

= Ee ti=−11(λ ui,Xi −c·λ2 ui 2σi2) ≤ 1.

By Markov’s inequality, for any t > 0:

t

P

(λ ui, Xi − c · λ2 ui 2σi2) ≥ t

i=1

≤ P e ti=1(λ ui,Xi −c·λ2 ui 2σi2) ≥ et ≤ e−tEe ti=1(λ ui,Xi −c·λ2 ui 2σi2) ≤ e−t.

This ﬁnishes the proof.

31

