Combinatorial Bandits for Incentivizing Agents with Dynamic Preferences

arXiv:1807.02297v1 [cs.LG] 6 Jul 2018

Tanner Fiez∗, Shreyas Sekar∗, Liyuan Zheng, and Lillian J. Ratliff Electrical Engineering Department, University of Washington

Abstract
The design of personalized incentives or recommendations to improve user engagement is gaining prominence as digital platform providers continually emerge. We propose a multi-armed bandit framework for matching incentives to users, whose preferences are unknown a priori and evolving dynamically in time, in a resource constrained environment. We design an algorithm that combines ideas from three distinct domains: (i) a greedy matching paradigm, (ii) the upper conﬁdence bound algorithm (UCB) for bandits, and (iii) mixing times from the theory of Markov chains. For this algorithm, we provide theoretical bounds on the regret and demonstrate its performance via both synthetic and realistic (matching supply and demand in a bike-sharing platform) examples.
1 INTRODUCTION
The theory of multi-armed bandits plays a key role in enabling personalization in the digital economy (Scott, 2015). Algorithms from this domain have successfully been deployed in a diverse array of applications including online advertising (Lu et al., 2010; Mehta and Mirrokni, 2011), crowdsourcing (Tran-Thanh et al., 2014), content recommendation (Li et al., 2010), and selecting user-speciﬁc incentives (Ghosh and Hummel, 2013; Jain et al., 2014) (e.g., a retailer offering discounts). On the theoretical side, this has been complemented by a litany of near-optimal regret bounds for multi-armed bandit settings with rich combinatorial structures and complex agent behavior models (Chen et al., 2016; Gai et al., 2011; Kveton et al., 2015; Sani et al., 2012). At a high
∗Authors contributed equally.

level, the broad appeal of bandit approaches for allocating resources to human agents stems from its focus on balancing exploration with exploitation, thereby allowing a decision-maker to efﬁciently identify users’ preferences without sacriﬁcing short-term rewards.
Implicit in most of these works is the notion that in largescale environments, a designer can simultaneously allocate resources to multiple users by running independent bandit instances. In reality, such independent decompositions do not make sense in applications where resources are subject to physical or monetary constraints. In simple terms, matching an agent to a resource immediately constrains the set of resources to which another agent can be matched. Such supply constraints may arise even when dealing with intangible products. For instance, social media platforms (e.g., Quora) seek to maximize user participation by offering incentives in the form of increased recognition—e.g., featured posts or badges (Immorlica et al., 2015). Of course, there are supply constraints on the number of posts or users that can be featured at a given time. As a consequence of these coupling constraints, much of the existing work on multi-armed bandits does not extend naturally to multi-agent economies.
Yet, another important aspect not addressed by the literature concerns human behavior. Speciﬁcally, users’ preferences over the various resources may be dynamic— i.e. evolve in time as they are repeatedly exposed to the available options. The problem faced by a designer in such a dynamic environment is compounded by the lack of information regarding each user’s current state or beliefs, as well as how these beliefs inﬂuence their preferences and their evolution in time.
Bearing in mind these limitations, we study a multiarmed bandit problem for matching multiple agents to a ﬁnite set of incentives1: each incentive belongs to a cate-
1We use the term incentive broadly to refer to any resource or action available to the agent. That is, incentives are not limited to monetary or ﬁnancial mechanisms.

gory and global capacity constraints control the number of incentives that can be chosen from each category. In our model, each agent has a preference proﬁle or a type that determines its rewards for being matched to different incentives. The agent’s type evolves according to a Markov decision process (MDP), and therefore, the rewards vary over time in a correlated fashion.
Our work is primarily motivated by the problem faced by a technological platform that seeks to not just maximize user engagement but also to encourage users to make changes in their status quo decision-making process by offering incentives. For concreteness, consider a bike-sharing service—an application we explore in our simulations—that seeks to identify optimal incentives for each user from a ﬁnite bundle of options—e.g., varying discount levels, free future rides, bulk ride offers, etc. Users’ preferences over the incentives may evolve with time depending on their current type, which in turn depends on their previous experience with the incentives. In addition to their marketing beneﬁts, such incentives can serve as a powerful instrument for nudging users to park their bikes at alternative locations—this can lead to spatially balanced supply and consequently, lower rejection rates (Singla et al., 2015).
1.1 CONTRIBUTIONS AND ORGANIZATION
Our objective is to design a multi-armed bandit algorithm that repeatedly matches agents to incentives in order to minimize the cumulative regret over a ﬁnite time horizon. Here, regret is deﬁned as the difference in the reward obtained by a problem speciﬁc benchmark strategy and the proposed algorithm (see Deﬁnition 1). A preliminary impediment in achieving this goal is the fact that the capacitated matching problem studied in this work is NP-Hard even in the ofﬂine case. The major challenge therefore is whether we can achieve sub-linear (in the length of the horizon) regret in the more general matching environment without any information on the users’ underlying beliefs or how they evolve?
Following preliminaries (Section 2), we introduce a simple greedy algorithm that provides a 1/3– approximation to the optimal ofﬂine matching solution (Section 3). Leveraging this ﬁrst contribution, the central result in this paper (Section 4) is a new multi-armed bandit algorithm—MatchGreedyEpochUCB (MG-EUCB)—for capacitated matching problems with time-evolving rewards. Our algorithm obtains logarithmic (and hence sub-linear) regret even for this more general bandit problem. The proposed approach combines ideas from three distinct domains: (i) the 1/3–rd approximate greedy matching algorithm, (ii) the traditional UCB algorithm (Auer et al., 2002), and

(iii) mixing times from the theory of Markov chains.
We validate our theoretical results (Section 5) by performing simulations on both synthetic and realistic instances derived using data obtained from a Boston-based bike-sharing service Hubway (hub). We compare our algorithm to existing UCB-based approaches and show that the proposed method enjoys favorable convergence rates, computational efﬁciency on large data sets, and does not get stuck at sub-optimal matching solutions.
1.2 BACKGROUND AND RELATED WORK
Two distinct features separate our model from the majority of work on the multi-armed bandit problem: (i) our focus on a capacitated matching problem with ﬁnite supply (every user cannot be matched to their optimal incentive), and (ii) the rewards associated with each agent evolve in a correlated fashion but the designer is unaware of each agent’s current state. Our work is closest to (Gai et al., 2011) which considers a matching problem with Markovian rewards. However, in their model the rewards associated with each edge evolve independently of the other edges; as we show via a simple example in Section 2.2, the correlated nature of rewards in our instance can lead to additional challenges and convergence to suboptimal matchings if we employ a traditional approach as in (Gai et al., 2011).
Our work also bears conceptual similarities to the rich literature on combinatorial bandits (Badanidiyuru et al., 2013; Chen et al., 2016; Kveton et al., 2014, 2015; Wen et al., 2015). However, unlike our work, these papers consider a model where the distribution of the rewards is static in time. For this reason, efﬁcient learning algorithms leveraging oracles to solve generic constrained combinatorial optimization problems developed for the combinatorial semi-bandit setting (Chen et al., 2016; Kveton et al., 2015) face similar limitations in our model as the approach of (Gai et al., 2011). Moreover, the rewards in our problem may not have a linear structure so the approach of (Wen et al., 2015) is not applicable.
The novelty in this work is not the combinatorial aspect but the interplay between combinatorial bandits and the edge rewards evolving according to an MDP. When an arm is selected by an oracle, the reward of every edge in the graph evolves—how it evolves depends on which arm is chosen. If the change occurs in a sub-optimal direction, this can affect future rewards. Indeed, the difﬁculties in our proofs do not stem from applying an oracle for combinatorial optimization, but from bounding the secondary regret that arises when rewards evolve in a sub-optimal way.
Finally, there is a somewhat parallel body of work

on single-agent reinforcement learning techniques (Azar et al., 2013; Jaksch et al., 2010; Mazumdar et al., 2017; Ratliff et al., 2018) and expert selection where the rewards on the arms evolve in a correlated fashion as in our work. In addition to our focus on multi-agent matchings, we remark that many of these works assume that the designer is aware (at least partially) of the agent’s exact state and thus, can eventually infer the nature of the evolution. Consequently, a major contribution of this work is the extension of UCB-based approaches to solve MDPs with a fully unobserved state and rewards associated with each edge that evolve in a correlated fashion.
2 PRELIMINARIES
A designer faces the problem of matching m agents to incentives (more generally jobs, goods, content, etc.) without violating certain capacity constraints. We model this setting by means of a bipartite graph (A, I, P) where A is the set of agents, I is the set of incentives to which the agents can be matched, and P = A × I is the set of all pairings between agents and incentives. We sometimes refer to P as the set of arms. In this regard, a matching is a set M ⊆ P such that every agent a ∈ A and incentive i ∈ I is present in at most one edge belonging to M .
Each agent a ∈ A is associated with a type or state θa ∈ Θa, which inﬂuences the reward received by this agent when matched with some incentive i ∈ I. When agent a is matched to incentive i, its type evolves according to a Markov process with transition probability kernel Pa,i : Θa × Θa → [0, 1]. Each pairing or edge of the bipartite graph is associated with some reward that depends on the agent–incentive pair, (a, i), as well as the type θa.
The designer’s policy (algorithm) is to compute a matching repeatedly over a ﬁnite time horizon in order to maximize the expected aggregate reward. In this work, we restrict our attention to a speciﬁc type of multi-armed bandit algorithm that we refer to as an epoch mixing policy. Formally, the execution of such a policy α is divided into a ﬁnite number of time indices [n] = {1, 2, . . . , n}, where n is the length of the time horizon. In each time index k ∈ [n], the policy selects a matching α(k) and repeatedly ‘plays’ this matching for τk > 0 iterations within this time index. We refer to the set of iterations within a time index collectively as an epoch. That is, within the k–th epoch, for each edge (a, i) ∈ α(k), agent a is matched to incentive i and the agent’s type is allowed to evolve for τk iterations. In short, an epoch mixing policy proceeds in two time scales—each selection of a matching corresponds to an epoch comprising of τk iterations for k ∈ [n], and there are a total of n epochs. It is worth noting that an epoch-based policy was used in the UCB2 algorithm (Auer et al., 2002), albeit with

stationary rewards.
Agents’ types evolve based on the incentives to which they are matched. Suppose that βa(k) denotes the type distribution on Θa at epoch k and i ∈ I is the incentive to which agent a is matched by α (i.e., (a, i) ∈ α(k)). Then, βa(k+1)(θa) = θ ∈Θa Paτ,ki(θ , θa)βa(k)(θ ).
For epoch k, the rewards are averaged over the τk iterations in that epoch. Let raθ,i denote the reward received by agent a when it is matched to incentive i given type θ ∈ Θa. We assume that raθ,i ∈ [0, 1] and is drawn from a distribution Tr(a, i, θ). The reward distributions for different edges and states in Θa are assumed to be independent of each other. Suppose that an algorithm α selects the edge (a, i) for τ iterations within an epoch. The observed reward at the end of this epoch is taken to be the time-averaged reward over the epoch. Speciﬁcally, suppose that the k–th epoch proceeds for τk iterations beginning with time tk—i.e. one plus the total iterations completed before this—and ending at time tk+1 − 1 = tk + τk − 1, and that θa(t) denotes agent a’s state at time t ∈ [tk, tk+1 − 1]. Then, the time-averaged reward in the epoch is given by rθaa,i(tk) = τ1k ttk=+t1k−1 raθa,i(t). We use the state as a superscript to denote dependence of the reward on the agent’s type at the beginning of the epoch. Finally, the total (time-averaged) reward due to a matching α(k) at the end of an epoch can be written as
(a,i)∈α(k) rθaa,i(tk).
We assume that the Markov chain corresponding to each edge (a, i) ∈ P is aperiodic and irreducible (Levin et al., 2009). We denote the stationary or steady-state distribution for this edge as πa,i : Θa → [0, 1]. Hence, we deﬁne the expected reward for edge (a, i), given its stationary distribution, to be µa,i = E θ∈Θa raθ,iπa,i(θ) where the expectation is with respect to the distribution on the reward given θ.

2.1 CAPACITATED MATCHING

Given P = A × I, the designer’s goal at the beginning

of each epoch is to select a matching M ⊆ P—i.e. a

collection of edges—that satisﬁes some cardinality con-

straints. We partition the edges in P into a mutually ex-

clusive set of classes allowing for edges possessing iden-

tical characteristics to be grouped together. In the bike-

sharing example, the various classes could denote types

of incentives—e.g., edges that match agents to discounts,

free-rides, etc. Suppose that C = {ξ1, ξ2, . . . , ξq} de-

notes a partitioning of the edge set such that (i) ξj ⊆ P

for all 1 ≤ j ≤ q, (ii)

q j=1

ξj

=

P,

and

(iii)

ξj

∩ ξj

=∅

for all j = j . We refer to each ξj as a class and for any

given edge (a, i) ∈ P, use c(a, i) to denote the class that

this edge belongs to, i.e., (a, i) ∈ c(a, i) and c(a, i) ∈ C.

Given a capacity vector b = (bξ1 , . . . , bξq ) indexed on the set of classes, we say that a matching M ⊆ P is a feasible solution to the capacitated matching problem if:
a) for every a ∈ A (resp., i ∈ I), the matching must contain at most one edge containing this agent (resp., incentive)
b) and, the total number of edges from each class ξj contained in the matching cannot be larger than bξj .
In summary, the capacitated matching problem can be formulated as the following integer program:

max (a,i)∈P w(a, i)x(a, i)

s.t. i∈I x(a, i) ≤ 1 ∀a ∈ A

a∈A x(a, i) ≤ 1 ∀i ∈ I

(P1)

(a,i)∈ξj x(a, i) ≤ bξj , ∀ξj ∈ C

x(a, i) ∈ {0, 1}, ∀(a, i) ∈ P

We use the notation {P, C, b, (w(a, i))(a,i)∈P } for a capacitated matching problem instance. In (P1), w(a, i) refers to the weight or the reward to be obtained from the given edge. The term x(a, i) is an indicator on whether the edge (a, i) is included in the solution to (P1). Clearly, the goal is to select a maximum weight matching subject to the constraints. In our online bandit problem, the designer’s actual goal in a ﬁxed epoch k is to maximize the quantity (a,i)∈P raθa,i(tk)x(a, i), i.e., w(a, i) = rθaa,i(tk). However, since the reward distributions and the current user type are not known beforehand, our MG-EUCB algorithm (detailed in Section 4.2) approximates this objective by setting the weights to be the average observed reward from the edges in combination with the corresponding conﬁdence bounds.

2.2 TECHNICAL CHALLENGES
There are two key obstacles involved in extending traditional bandit approaches to our combinatorial setting with evolving rewards, namely, cascading sub-optimality and correlated convergence. The ﬁrst phenomenon occurs when an agent a is matched to a sub-optimal arm i (incentive) because its optimal arm i∗ has already been assigned to another agent. Such sub-optimal pairings have the potential to cascade, e.g., when another agent a1 who is matched to i in the optimal solution can no longer receive this incentive and so on. Therefore, unlike the classical bandit analysis, the selection of sub-optimal arms cannot be directly mapped to the empirical rewards.
Correlated Convergence. As mentioned previously, in our model, the rewards depend on the type or state of an agent, and hence, the reward distribution on any given edge (a, i) can vary even when the algorithm does not select this edge. As a result, a na¨ıve application of a bandit algorithm can severely under-estimate the expected

reward on each edge and eventually converge to a suboptimal matching. A concrete example of the poor convergence effect is provided in Example 1. In Section 4.2, we describe how our central bandit algorithm limits the damage due to cascading while simultaneously avoiding the correlated convergence problem.
Example 1 (Failure of Classical UCB). Consider a problem instance with two agents A = {a1, a2}, two incentives I = {i1, i2} and identical state space i.e., Θa1 = Θa2 = {θ1, θ2}. The transition matrices and deterministic rewards for the agents for being matched to each incentive are depicted pictorially below: we assume that > 0 is a sufﬁciently small constant.

Agent a1

0

1−

Edge

(a1, i1) θ1

1

raθ11,i1 = 0

θ2 raθ21,i1 = 1

1− Edge (a1, i2) θ1

10 θ2

raθ11,i2 = 0.5

raθ21,i2 = 0.5

Agent a2

1−

0

1

Edge

(a2, i1) θ1

θ2

raθ12,i1 = 0.5

raθ22,i1 = 0.5

0

1−

Edge

(a2, i2) θ1

1

θ2

raθ12,i2 = 0

raθ22,i2 = 1

Figure 1: (a) State transition diagram and reward for each edge: note that the state is associated with the agent and not the edge.

Clearly, the optimal strategy is to repeatedly chose the

matching {(a1, i1), (a2, i2)} achieving a reward of (al-

most) two in each epoch. An implementation of tra-

ditional UCB for the matching problem—e.g., the ap-

proach in (Chen et al., 2016; Gai et al., 2011; Kveton

et al., 2015)—selects a matching based on the empirical

rewards and conﬁdence bounds for a total of

n k=1

τk

iterations, which are then divided into epochs for con-

venience. This approach converges to the sub-optimal

matching of M = {(a1, i2), (a2, i1)}. Indeed, every

time the algorithm selects this matching, both the agents’

states are reset to θ1 and when the algorithm explores

the optimum matching, the reward consistently happens

to be zero since the agents are in state θ1. Hence, the

rewards for the (edges in the) optimum matching are

grossly underestimated.

3 GREEDY OFFLINE MATCHING
In this section, we consider the capacitated matching problem in the ofﬂine case, where the edge weights are provided as input. The techniques developed in this section serve as a base in order to solve the more general online problem in the next section. More speciﬁcally, we assume that we are given an arbitrary instance of the capacitated matching problem {P, C, b, (w(a, i))(a,i)∈P }.

Algorithm 1 Capacitated-Greedy Matching Algorithm

1: function MG((w(a, i))(a,i)∈P , b) 2: G∗ ← ∅, E ← P

3: while E = ∅:

4: Select (a, i) = arg max(a ,i )∈E w(a , i )

5: if|G∗ ∩ c(a, i)| < bc(a,i) then

6:

G∗ ← G∗ ∪ {(a, i)}

7:

E ← E \ {(a , i )} ∀(a , i ) : a = a or i = i

else

8:

E ← E \ {(a, i)}

9: return G∗

10: end function

Given this instance, the designer’s objective is to solve (P1). Surprisingly, this problem turns out to be NPHard and thus cannot be optimally solved in polynomial time (Garey and Johnson, 1979)—this marks a stark contrast with the classic maximum weighted matching problem, which can be solved efﬁciently using the Hungarian method (Kuhn, 1955).
In view of these computational difﬁculties, we develop a simple greedy approach for the capacitated matching problem and formally prove that it results in a one-third approximation to the optimum solution. The greedy method studied in this work comes with a multitude of desirable properties that render it suitable for matching problems arising in large-scale economies. Firstly, the greedy algorithm has a running time of O(m2 log m), where m is the number of agents—this near-linear execution time in the number of edges makes it ideal for platforms comprising of a large number of agents. Secondly, since the output of the greedy algorithm depends only on the ordering of the edge weights and is not sensitive to their exact numerical value, learning approaches tend to converge faster to the ‘optimum solution’. This property is validated by our simulations (see Figure 2c). Finally, the performance of the greedy algorithm in practice (e.g., see Figure 2b) appears to be much closer to the optimum solution than the 1/3 approximation guaranteed by Theorem 1 below.
3.1 ANALYSIS OF GREEDY ALGORITHM
The greedy matching is outlined in Algorithm 1. Given an instance {P, C, b, (w(a, i))(a,i)∈P }, Algorithm 1 ‘greedily’ selects the highest weight feasible edge in each iteration—this step is repeated until all available edges that are feasible are added to G∗. Our main result in this section is that for any given instance of the capacitated matching problem, the matching G∗ returned by Algorithm 1 has a total weight that is at least 1/3–rd that of the maximum weight matching.

Theorem 1. For any given capacitated matching problem instance {P, C, b, (w(a, i))(a,i)∈P }, let G∗ denote the output of Algorithm 1 and M ∗ be any other feasible solution to the optimization problem in (P1) including the optimum matching. Then, (a,i)∈M∗ w(a, i) ≤ 3 (a,i)∈G∗ w(a, i).
The proof is based on a charging argument that takes into account the capacity constraints and can be found in Section B.1 of the supplementary material. At a high level, we take each edge belonging to the benchmark M ∗ and identify a corresponding edge in G∗ whose weight is larger than that of the benchmark edge. This allows us to charge the weight of the original edge to an edge in G∗. During the charging process, we ensure that no more than three edges in M ∗ are charged to each edge in G∗. This gives us an approximation factor of three.

3.2 PROPERTIES OF GREEDY MATCHINGS

We conclude this section by providing a hierarchi-

cal decomposition of the edges in P for a ﬁxed in-

stance {P, C, b, (w(a, i))(a,i)∈P }. In Section 4.1, we

will use this property to reconcile the ofﬂine version

of the problem with the online bandit case. Let G∗ =

{g1∗, g2∗, . . . , gm∗ } denote the matching computed by Al-

gorithm 1 for the given instance such that w(g1∗) ≥

w(g2∗) ≥ . . . ≥ w(gm∗ ) without loss of generality2. Next,

let

G∗j

=

{g1∗

,

g2∗

,

.

.

.

,

g

∗ j

}

for

all

1

≤

j

≤

m—i.e.

the

j

highest-weight edges in the greedy matching.

For each 1 ≤ j ≤ m, we deﬁne the infeasibility set HjG∗ as the set of edges in P that when added to G∗j violates the feasibility constraints of (P1). Finally, we use LGj ∗ to denote the marginal infeasibility sets—i.e. LG1 ∗ = H1G∗

and

LGj ∗ = HjG∗ \ HjG−∗1, ∀ 2 ≤ j ≤ m.

(1)

We note that the marginal infeasibility sets denote a mu-
tually exclusive partition of the edge set minus the greedy matching—i.e., 1≤j≤m LGj ∗ = P \ G∗. Moreover, since the greedy matching selects its edges in the decreasing order of weight, for any gj∗ ∈ G∗, and every (a, i) ∈ LGj ∗ , we have that w(gj∗) ≥ w(a, i).

Armed with our decomposition of the edges in P \ G∗, we now present a crucial structural lemma. The following lemma identiﬁes sufﬁcient conditions on the local ordering of the edge weights for two different instances under which the outputs of the greedy matching for the instances are non-identical.

Lemma 1. Given instances {P, C, b, (w(a, i))(a,i)∈P }

and {P, C, b, (w˜(a, i))(a,i)∈P } of the capacitated match-

ing

problem,

let

G∗

=

{g1∗

,

g2∗

,

.

.

.

,

g

∗ m

}

and

G˜

denote

2If g = (a, i), we abuse notation and let w(g) = w(a, i).

the output of Algorithm 1 for these instances, respectively. Let E1, E2 be conditions described as follows:
E1 ={∃j < j |(w˜(gj∗) < w˜(gj∗ )) ∧ (gj∗ ∈ G˜)} E2 ={∃gj∗ ∈ G∗, (a, i) ∈ LGj ∗ |
(w˜(gj∗) < w˜(a, i)) ∧ ((a, i) ∈ G˜)}.
If G∗ = G˜, then at least one of E1 or E2 must be true.
Lemma 1 is fundamental in the analysis of our MGEUCB algorithm because it provides a method to map the selection of each sub-optimal edge to a familiar condition comparing empirical rewards to stationary rewards.

4 ONLINE MATCHING—BANDIT ALGORITHM

In this section, we propose a multi-armed bandit algorithm for the capacitated matching problem and analyze its regret. For concreteness, we ﬁrst highlight the information and action sets available to the designer in the online problem. The designer is presented with a partial instance of the matching problem without the weights, i.e., {P, C, b} along with a ﬁxed time horizon of n epochs but has the ability to set the parameters (τ1, τ2, . . . , τn), where τk is the number of iterations under epoch k. The designer’s goal is to design a policy α that selects a matching α(k) in the k–th epoch that is a feasible solution for (P1). At the end of the k–th epoch, the designer observes the average reward raθa,i(k) for each (a, i) ∈ α(k) but not the agent’s type. We abuse notation and take θa(k) to be the agent’s state at the beginning of epoch k. The designer’s objective is to minimize the regret over the ﬁnite horizon.

The expected regret of a policy α is the difference in the expected aggregate reward of a benchmark matching and that of the matching returned by the policy, summed over n epochs. Owing to its favorable properties (see Section 3), we use the greedy matching on the stationary state rewards as our benchmark. Measuring the regret with respect to the unknown stationarydistribution is standard with MDPs (e.g., see (Gai et al., 2011; Tekin and Liu, 2010, 2012)). Formally, let G∗ denote the output of Algorithm 1 on the instance {P, C, b, (µa,i)(a,i)∈P }—i.e., with the weights w(a, i) equal the stationary state rewards µa,i.

Deﬁnition 1. The expected regret of a policy α with respect to the greedy matching G∗ is given by

Rα(n) = n

n
µa,i −

E[rθaa,i(k)],

(a,i)∈G∗

k=1 (a,i)∈α(k)

where the expectation is with respect to the reward and the state of the agents during each epoch.

4.1 REGRET DECOMPOSITION

As is usual in this type of analysis, we start by decompos-

ing the regret in terms of the number of selections of each

sub-optimal arm (edge). We state some assumptions and

deﬁne notation before proving our generic regret decom-

position theorem. A complete list of the notation used

can be found in Section A of the supplementary mate-

rial.

1. For analytic convenience, we assume that the number

of agents and incentives is balanced and therefore,

|A| = |I| = m. WLOG, every agent is matched to some incentive in G∗; if this is not the case, we can

add dummy incentives with zero reward.

2. Suppose that G∗

=

{

g1∗

,

g2∗

,

.

.

.

,

g

∗ m

}

such

that

µg∗ ≥ . . . ≥ µg∗ and let i∗(a) denote the incen-

1

m

tive that a is matched to in G∗. Let L∗1, . . . L∗m be the

marginal infeasibility sets as deﬁned in (1).

3. Suppose that τ0 ≥ 1 and τk = τ0 + ζk for some

non-negative integer ζ.

Let 1{·} be the indicator function—e.g., 1{(a, i) ∈

α(k)} is one when the edge (a, i) belongs to the match-

ing α(k), and zero otherwise. Deﬁne Taα,i(n) =

n k=1

1{(a,

i)

∈

α(k)}

to

be

the

random

variable

that

denotes the number of epochs in which an edge is se-

lected under an algorithm α. By relating E[Taα,i(n)] to the regret Rα(n), we are able to provide bounds on the

performance of α.

By adding and subtracting (a,i)∈P E[Taα,i(n)]µa,i from the equation in Deﬁnition 1, we get that

Rα(n) = + nk=1

(a,i)∈P E[Taα,i(n)](µa,i∗(a) − µa,i) (a,i)∈P E[1{(a, i) ∈ α(k)} µa,i − rθaa,i(k) ].

To further simplify the regret, we separate the edges in P by introducing the notion of a sub-optimal edge. Formally, for any given a ∈ A, deﬁne Sa := {(a, i) | µa,i∗(a) ≥ µa,i ∀i ∈ I} and S := a∈A Sa. Then, the regret bound in the above equation can be simpliﬁed by ignoring the contribution of the terms in P \ S. That is, since µa,i∗(a) < µa,i for all (a, i) ∈ P \ S,

Rα(n) ≤ (a,i)∈S E[Taα,i(n)](µa,i∗(a) − µa,i)

+

n k=1

(a,i)∈P E[1{(a, i) ∈ α(k)} µa,i − rθaa,i(k) ].

(2)

Recall from the deﬁnition of the marginal infeasibility sets in (1) that for any given (a, i) ∈ P \ G∗, there exists a unique edge gj∗ ∈ G∗ such that (a, i) ∈ L∗j . Deﬁne L−1(a, i) := gj∗ ∈ G∗ such that (a, i) ∈ L∗j . Now, we can deﬁne the reward gap for any given edge as follows:


 ∆a,i =


µa,i∗(a) − µa,i, µL−1(a,i) − µa,i, µgj∗−1 − µgj∗ ,

if (a, i) ∈ S if (a, i) ∈ (P \ G∗) \ S if (a, i) = gj∗ for j ≥ 2

This leads us to our main regret decomposition result which leverages mixing times for Markov chains (Fill, 1991) along with Equation (2) in deriving regret bounds. For an aperiodic, irreducible Markov chain Pa,i, using the notion that it convergences to its stationary state under repeated plays of a ﬁxed action, we can prove that for every arm (a, i), there exists a constant Ca,i > 0 such that E µa,i − raθa,i(k) ≤ Ca,i/τk—in fact, this result holds for all type distributions βa(k) of the agent. Proposition 1. Suppose for each (a, i) ∈ P, Pa,i is an aperiodic, irreducible Markov chain with corresponding constant Ca,i. Then, for a given algorithm α where τk = τ0 + ζk for some ﬁxed ζ > 0, we have that
Rα(n) ≤ (a,i)∈S Eα Taα,i(n) (∆a,i + Cτa0,i ) + m Cζ∗ 1 + log ζ(n − 1)/τ0 + 1 .
The proof of this proposition is in Section B.2 of the supplementary material.
4.2 MG-EUCB ALGORITHM AND ANALYSIS
In the initialization phase, the algorithm computes and plays a sequence of matchings M1, M2, . . . , Mp for a total of p epochs. The initial matchings ensure that every edge in P is selected at least once—the computation of these initial matchings relies on a greedy covering algorithm that is described in Section C.1 of the supplementary material. Following this, our algorithm maintains the cumulative empirical reward r¯a,i for every (a, i) ∈ P. At the beginning of (say) epoch k, the algorithm computes a greedy matching for the instance {P, C, b, (w(a, i))(a,i)∈P } where w(a, i) = r¯a,i/Ta,i + ca,i, i.e., the average empirical reward for the edge added to a suitably chosen conﬁdence window. The INCENT(·) function (Algorithm 4, described in the supplementary material since it is a trivial function) plays each edge in the greedy matching for τk iterations, where τk increases linearly with k. This process is repeated for n−p epochs. Prior to theoretically analyzing MG-EUCB, we return to Example 1 in order to provide intuition for how the algorithm overcomes correlated convergence of rewards.
Revisiting Example 1: Why does MG-EUCB work? In Example 1, the algorithm initially estimates the empirical reward of (a1, ii) and (a2, i2) to be zero respectively. However, during the UCB exploration phase, the matching M1 = (a1, i1), (a2, i2) is played again for epoch length > 1 and the state of agent a1 moves from θ1 to θ2 during the epoch. Therefore, the algorithm estimates the average reward of each edge within the epoch to be ≥ 0.5, and the empirical reward increases. This continues as the epoch length increases, so that eventually the

Algorithm 2 MatchGreedy-EpochUCB

1: procedure MG-EUCB(ζ, τ0, P)

2: t1 ← 0, r¯a,i ← 0 & Ta,i ← 1 ∀(a, i) ∈ P

3: M1, M2, . . . , Mp ⊂ P s.t. (a, i) ∈ Mj ⇔ (a, i) ∈/

M ∀ =j

see Supplement C.1 for details

4: INCENT(·)

see Alg. 4 in Supplement C

5: for1 ≤ n ≤ m

play each arm once

6: (r¯a,i)(a,i)∈Mn ← INCENT(Mn, tn, n, τ0, ζ)

7: tn+1 ← tn + τ0 + ζn

8: end for

9: while n > m

10:

MG

=

MG((r¯a,i/Ta,i

+

cTa,i
a,i

(n

))

(a,i)

∈P

)

11: (ra,i(tn))(a,i)∈MG ← INCENT(MG, tn, n, τ0, ζ)

12: r¯a,i ← r¯a,i + τ0+1ζn ra,i(tn) ∀(a, i) ∈ MG

13: Ta,i ← Ta,i + 1 ∀(a, i) ∈ MG

14: tn+1 ← tn + τ0 + ζn; n ← n + 1

15: end while

16: end procedure

empirical reward for (a1, i1) exceeds that of (a1, i2) and the algorithm correctly identiﬁes the optimal matching as we move from exploration to exploitation.
In order to characterize the regret of the MG-EUCB algorithm, Proposition 1 implies that it is sufﬁcient to bound the expected number of epochs in which our algorithm selects each sub-optimal edge. The following theorem presents an upper bound on this quantity.
Theorem 2. Consider a ﬁnite set of m agents A and incentives I with corresponding aperiodic, irreducible Markov chains Pa,i for each (a, i) ∈ P. Let α be the MG-EUCB algorithm with mixing time sequence {τk} where τk = τ0 + ζk, τ0 > 0, and ζ > 0. Then for every (a, i) ∈ S,

Eα[Ta,i(n)] ≤

4m2
2

∆a∗ ,i∗

ρ∗ ∗ √

2

√a τ,0i + 6 log n + 4 log m

+ 2(1 + log(n))

where (a∗, i∗) = argmax(a ,i )∈P\g∗

4 ∆2

+ ρ√a1 ,i1
τ

√

11

1

a1 ,i1

0

2

6 log n + 4 log m , and ρa,i is a constant speciﬁc to

edge (a, i).

The full proof of the theorem is provided can be found in the supplementary material.
Proof (sketch.) There are three key ingredients to the proof: (i) linearly increasing epoch lengths, (ii) overcoming cascading errors, and (iii) application of the AzumaHoeffding concentration inequality.
By increasing the epoch length linearly, MG-EUCB ensures that as the algorithm converges to the optimal

Mean Reward Mean Reward
% Optimal

1.2

Mean Reward vs Time

1.0

0.8 0.6
0

MG-EUCB+ C-UCB G-Optimal
2500 5000 7500 10000 Epoch
(a)

1.2

Mean Reward vs. Time

1.0

0.8 0.6
0

MG-EUCB+ H-EUCB+ G-Optimal H-Optimal

100000 Epoch

200000

(b)

100 Moving Average of Optimality

MG-EUCB+

80

MG-EUCB

H-EUCB+

60

H-EUCB

40

20

00

100000

200000

Epoch

(c)

Figure 2: Synthetic Experiments: Comparison of MG-EUCB(+) and H-EUCB(+) to their respective ofﬂine solutions (G- and H-
optimal, respectively) and to C-UCB (classical UCB). We use the following set up: (i) |A| = |I| = |Θa| = 10 (see Supplement D for more extensive experiments) (ii) each state transition matrix Pa,i associated with an arm (a, i) ∈ P was selected uniformly at random within the class of aperiodic and irreducible stochastic matrices; (iii) the reward for each arm, state pair raθ,i is drawn i.i.d. from a distribution Tr(a, i, θ) belonging to either a Bernoulli, Uniform, or Beta distribution; (iv) τ0 = 50 and ζ = 1.

matching, it also plays each arm for a longer duration within an epoch. This helps the algorithm to progressively discard sub-optimal arms without selecting them too many times when the epoch length is still small. At the same time, the epoch length is long enough to allow for sufﬁcient mixing and separation between multiple near-optimal matchings. If we ﬁx the epoch length as a constant, the resulting regret bounds are considerably worse because the agent states may never converge to the steady-state distributions.

To address cascading errors, we provide a useful characterization. For a given (a, i), suppose that uka,i(t) refers to the average empirical reward obtained from edge (a, i) up to epoch t−1 plus the upper conﬁdence bound parameter, given that edge (a, i) has been selected for exactly k times in epochs 1 to t − 1 . For any given epoch k
where the algorithm selects a sub-optimal matching, i.e., α(k) = G∗, we can apply Lemma 1 and get that at least
one of the following conditions must be true:

1. 1{∃j < j | ukg∗ (t) > ukg∗ (t) ∧ (gj∗ ∈ α(t))}

j

j

2. 1{∃j, (a, i) ∈ L∗j | ukg∗ (t) < uka,i(t) ∨ ((a, i) ∈ j

α(k))} = 1

This is a particularly useful characterization because it maps the selection of each sub-optimal edge to a familiar condition that compares the empirical rewards to the stationary rewards. Therefore, once each arm is selected for O(log(n)) epochs, the empirical rewards approach the ‘true’ rewards and our algorithm discards sub-optimal edges. Mathematically, this can be written as

Eα[Ta ,i (n)] = 1 +

n t=p+1

1{(a

,

i

)

∈

α(t)}

≤ m2 + m

n
+

t−1 t−1

j=1 (a,i)∈Lj t=p+1 s=1 k=

1{usg∗ (t) ≤ uka,i(t)} , j

where is some carefully chosen constant, L+j = L∗j ∪ {gj∗+1} and L+m = L∗m.
With this characterization, for each s, we ﬁnd an upper bound on the probability of the event {usg∗ (t) ≤ uka,i(t)}.
j
However, this is a non-trivial task since the reward obtained in any given epoch is not independent of the previous actions. Speciﬁcally, the underlying Markov process that generates the rewards is common across the edges connected to any given agent in the sense, that the initial distribution for each Markov chain that results from pulling an edge is the distribution at the end of the preceding pull. Therefore, we employ Azuma-Hoeffding (Azuma, 1967; Hoeffding, 1963), a concentration inequality that does not require independence in the arm-based observed rewards. Moreover, unlike the classical UCB analysis, the empirical reward can differ from the expected stationary reward due to the distributions Tr(a, i, θ) and βak,i = πa,i. To account for this additional error term, we use bounds on the convergence rates of Markov chains to guide the choice of the conﬁdence parameter cka,i(t) in Algorithm 2. Applying the Azuma-Hoeffding inequality, we can show that with high probability, the difference between the empirical reward and the stationary reward of edge (a, i) is no larger than cka,i(t).
As a direct consequence of Proposition 1 and Theorem 2, we get that for a ﬁxed instance, the regret only increases logarithmically with n.
5 EXPERIMENTS
In this section, we present a set of illustrative experiments with our algorithm (MG-EUCB) on synthetic and

Eﬃciency % Eﬃciency % Mean Reward

100

Eﬃciency vs. Time

80

60

40

20 00

MG-EUCB+ No Incentive
20000 40000 Epoch

G-UpBound 60000 80000

(a) Static Demand

100

Eﬃciency vs. Time

80

60

40

20 00

MG-EUCB+ No Incentive
20000 40000 Epoch

G-UpBound 60000 80000

(b) Random Demand

50 40 30 20 10
0

Mean Reward vs. Time Static Demand MG-EUCB+ Random Demand MG-EUCB+

20000 40000 Epoch
(c)

60000

80000

Figure 3: Bike-share Experiments: Figures 3a and 3b compare the efﬁciency (percentage of demand satisﬁed) of the bike-share system with two demand models under incentive matchings selected by MG-EUCB+ with upper and lower bounds given by the system performance when the incentives are computed via the benchmark greedy matching that uses the state information and when no incentives are offered respectively. In Figure 3c we plot the mean reward of the MG-EUCB+ algorithm with static and random demand which gives the expected number of agents who accept an incentive within each epoch.

real data. We observe much faster convergence with the greedy matching as compared to the Hungarian algorithm. Moreover, as is typical in the bandit literature (e.g., (Auer et al., 2002)), we show that a tuned version of our algorithm (MG-EUCB+), in which we reduce the coefﬁcient on the log(n) term in the UCB ‘conﬁdence parameter’ from six to three, further improves the convergence of our algorithm. Finally we show that our algorithm can be effectively used as an incentive design scheme to improve the performance of a bike-share system.
5.1 SYNTHETIC EXPERIMENTS
We ﬁrst highlight the failure of classical UCB approaches (C-UCB)—e.g., as in (Gai et al., 2011)—for problems with correlated reward evolution. In Figure 2a, we demonstrate that C-UCB converges almost immediately to a suboptimal solution, while this is not the case for our algorithm (MG-EUCB+). In Figure 2b, we compare MG-EUCB and MG-EUCB+ with a variant of Algorithm 2 that uses the Hungarian method (HEUCB) for matchings. While H-EUCB does have a ‘marginally’ higher mean reward, Figure 2c reveals that the MG-EUCB and MG-EUCB+ algorithms converge much faster to the optimum solution of the greedy matching than the Hungarian alternatives.
5.2 BIKE-SHARE EXPERIMENTS
In this problem, we seek to incentivize participants in a bike-sharing system; our goal is to alter their intended destination in order to balance the spatial supply of available bikes appropriately and meet future user demand. We use data from the Boston-based bike-sharing service Hubway (hub) to construct the example. Formally, we

consider matching each agent a to an incentive i = sa, meaning the algorithm proposes that agent a travel to station sa as opposed to its intended destination sa (potentially, for some monetary beneﬁt). The agent’s state θa controls the probability of accepting the incentive by means of a distance threshold parameter and a parameter of a Bernouilli distribution, both of which are drawn uniformly at random. More details on the data and problem setup can be found in Section D of the supplementary material.
Our bike-share simulations presented in Figure 3 show approximately a 40% improvement in system performance when compared to an environment without incentives and convergence towards an upper bound on system performance. Moreover, our algorithm achieves this signiﬁcant performance increase while on average matching less than 1% of users in the system to an incentive.
6 Conclusion
We combine ideas from greedy matching, the UCB multi-armed bandit strategy, and the theory of Markov chain mixing times to propose a bandit algorithm for matching incentives to users, whose preferences are unknown a priori and evolving dynamically in time, in a resource constrained environment. For this algorithm, we derive logarithmic gap-dependent regret bounds despite the additional technical challenges of cascading sub-optimality and correlated convergence. Finally, we demonstrate the empirical performance via examples.
Acknowledgments
This work is supported by NSF Awards CNS-1736582 and CNS-1656689. T. Fiez was also supported in part by an NDSEG Fellowship.

References
Hubway: Metro-boston’s bikeshare program. [available online: https://thehubway.com].
P. Auer, N. Cesa-Bianchi, and P. Fischer. Finite-time analysis of the multiarmed bandit problem. Machine Learning, 47(2):235–256, May 2002. doi: 10.1023/A: 1013689704352.
M. G. Azar, A. Lazaric, and E. Brunskill. Regret bounds for reinforcement learning with policy advice. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 97–112, 2013.
K. Azuma. Weighted sums of certain dependent random variables. Tohoku Math. J., 19(3):357–367, 1967. doi: 10.2748/tmj/1178243286.
A. Badanidiyuru, R. Kleinberg, and A. Slivkins. Bandits with knapsacks. In Proc. 54th Annual IEEE Symp. Foundations of Computer Science, pages 207– 216, 2013.
W. Chen, Y. Wang, Y. Yuan, and Q. Wang. Combinatorial multi-armed bandit and its extension to probabilistically triggered arms. J. Machine Learning Research, 17:50:1–50:33, 2016. URL http://jmlr.org/ papers/v17/14-298.html.
J. Fill. Eigenvalue bounds on convergence to stationarity for nonreversible markov chains, with an application to the exclusion process. Ann. Appl. Probab., 1(1): 62–87, 1991.
G. Folland. Real Analysis. Wiley, 2nd edition, 2007.
Y. Gai, B. Krishnamachari, and M. Liu. On the combinatorial multi-armed bandit problem with markovian rewards. In Proc. Global Communications Conf., pages 1–6, 2011. doi: 10.1109/GLOCOM.2011.6134244.
M. R. Garey and David S. Johnson. Computers and Intractability: A Guide to the Theory of NPCompleteness. W. H. Freeman, 1979. ISBN 0-71671044-7.
A. Ghosh and P. Hummel. Learning and incentives in user-generated content: multi-armed bandits with endogenous arms. In Proc. of ITCS 2013, pages 233– 246, 2013.
W. Hoeffding. Probability inequalities for sums of bounded random variables. J. American Statistical Association, 58(301):13–30, 1963. doi: 10.2307/ 2282952.
Nicole Immorlica, Gregory Stoddard, and Vasilis Syrgkanis. Social status and badge design. In Proceedings of the 24th International Conference on World Wide Web, WWW 2015, Florence, Italy, May 18-22, 2015, pages 473–483, 2015.

S. Jain, B. Narayanaswamy, and Y. Narahari. A multiarmed bandit incentive mechanism for crowdsourcing demand response in smart grids. In Proc. of AAAI 2014, pages 721–727, 2014.
T. Jaksch, R. Ortner, and P. Auer. Near-optimal Regret Bounds for Reinforcement Learning. J. Machine Learning Research, 11:1563–1600, 2010.
H. W. Kuhn. The hungarian method for the assignment problem. Naval Research Logistics, 2(1-2):83– 97, 1955.
B. Kveton, Z. Wen, A. Ashkan, H. Eydgahi, and B. Eriksson. Matroid bandits: Fast combinatorial optimization with learning. In Proc. of UAI 2014, pages 420–429, 2014.
Branislav Kveton, Zheng Wen, Azin Ashkan, and Csaba Szepesvari. Tight regret bounds for stochastic combinatorial semi-bandits. In Artiﬁcial Intelligence and Statistics, pages 535–543, 2015.
D. A. Levin, Y. Peres, and E. L. Wilmer. Markov Chains and Mixing Times. American Mathematical Society, 2009.
Lihong Li, Wei Chu, John Langford, and Robert E. Schapire. A contextual-bandit approach to personalized news article recommendation. In Proc. 19th Intern. Conf. World Wide Web, pages 661–670, 2010.
T. Lu, D. Pa´l, and M. Pa´l. Contextual multi-armed bandits. In Proc. of AISTATS 2010, pages 485–492, 2010.
E. Mazumdar, R. Dong, V. Ru´bies Royo, C. Tomlin, and S. S. Sastry. A Multi-Armed Bandit Approach for Online Expert Selection in Markov Decision Processes. arxiv:1707.05714, 2017.
A. Mehta and V. Mirrokni. Online ad serving: Theory and practice, 2011.
L. J. Ratliff, S. Sekar, L. Zheng, and T. Fiez. Incentives in the dark: Multi-armed bandits for evolving users with unknown type. arxiv, 2018.
Amir Sani, Alessandro Lazaric, and Re´mi Munos. Riskaversion in multi-armed bandits. In Proc. of NIPS 2012, pages 3284–3292, 2012.
S. L. Scott. Multi-armed bandit experiments in the online service economy. Applied Stochastic Models in Business and Industry, 31(1):37–45, 2015.
A. Singla, M. Santoni, G. Barto´k, P. Mukerji, M. Meenen, and Andreas Krause. Incentivizing users for balancing bike sharing systems. In Proc. of AAAI 2015, pages 723–729, 2015.
Cem Tekin and Mingyan Liu. Online algorithms for the multi-armed bandit problem with markovian rewards.

In Communication, Control, and Computing (Allerton), 2010 48th Annual Allerton Conference on, pages 1675–1682. IEEE, 2010.
Cem Tekin and Mingyan Liu. Online Learning of Rested and Restless Bandits. IEEE Transactions on Information Theory, 58(8):5588–5611, 2012.
L. Tran-Thanh, S. Stein, A. Rogers, and N. R. Jennings. Efﬁcient crowdsourcing of unknown experts using bounded multi-armed bandits. Artif. Intell., 214: 89–111, 2014.
Zheng Wen, Branislav Kveton, and Azin Ashkan. Efﬁcient learning in large-scale combinatorial semibandits. In International Conference on Machine Learning, pages 1113–1122, 2015.

A NOTATIONAL TABLE

notation A I P =A×I Θa Pa,i βa(t) πa,i µa,i τk raθa,i Tr(a, i, θa) rθa,i
bξl G∗ gj∗ i∗(a) L∗j
Sa S m n
θa(t) Ca,i, ρa,i C∗ Rα(n)
Ta,i(n)
Raθ,,kj
R¯ak,j
θa(tla,i) Xak,i Yak,i Qa,i(k)
cka,j (t)
uka,j (t)

meaning

set of agents set of incentives allowed agent-incentive pairs state (type) space of agent a transition probability kernel

agent a’s type distribution at epoch t

stationary distribution of (a, i) ∈ P

expected reward from (a, i) ∈ P

number of iterations matching offered

in epoch k, τk = τ0 + ζk, ζ > 0

random reward

agent a’s reward distribution

time-averaged reward during epoch k

raθa,i(k) = τ1k

r tk+1−1 θa(t)

t=tk

a,i

maximum number of edges of class ξl

greedy matching on weights (µa,i)

the edge having the j–th largest weight (µa,i) in G∗. incentive agent a is matched to in G∗

set of (a, i) ∈ P that become infeasible when gj∗ is added to matching

but not before that

set of edges (a, i) such that

µa,i ≤ µa,i∗(a)

a∈A Sa number of agents & incentives

the total number of epochs

state of agent a at the beginning

of epoch t

constants speciﬁc to each edge (a, i)

max(a,i)∈P\S Ca,i

regret of given matching policy

α at the end of n epochs

number of times edge (a, i)

selected in ﬁrst n epochs

reward on edge (a, i) when selected

for the k–th time given θa

average reward on ﬁrst k times (a, i)

is selected, i.e., k1

k i=1

Raθ,,kj

agent a’s state at the beginning

of epoch l

Raθ,,ki − E[Raθ,,ki |Fak,−i 1]

k j=1

Xaj,i

a

martingale

C2a,i ζ+1τ0 + ζ1 log 1 + kτ0ζ

upper conﬁdence parameter for edge

(a, j) after being selected for k times

average reward plus upper conﬁdence parameter for (a, j), i.e., R¯ak,j + cka,j

B PROOFS

B.1 PROOF OF THEOREM 1

Proof Our proof relies on what is referred to in the matching literature as a charging argument. In simple terms, we take each edge belonging to the benchmark M ∗ and identify a corresponding edge in G∗ whose weight is larger than that of the benchmark edge. This allows us to charge the weight of the original edge to an edge in G∗. During the charging process, we ensure that no more than three edges in M ∗ are charged to each edge in G∗. This gives us an approximation factor of three.
Suppose that an edge (a, i) belongs to M ∗ but not G∗. This implies that the edge (a, i) was removed from the set E at some iteration during the course of Algorithm 1. Moreover, as per the algorithm, this removal can happen in one of two ways: (i) via Line 7, in which case there exists some edge (a, i ) or (a , i) that was selected to G∗ ahead of (a, i), and (ii) via Line 8 in which case bξj edges belonging to class ξj = c(a, i) were added to G∗ before (a, i), as a result of which the capacity constraint for that class was met. Based on this, we divide the analysis into two cases.
Case I: Removal via Line 7. Without loss of generality, suppose that (a , i) is the edge added to G∗ during the iteration in which (a, i) is removed. Then, by deﬁnition, since (a , i) = arg max(a ,i )∈E w(a , i ) before the removal of (a, i) from E , we infer that

w(a, i) ≤ w(a , i)

(3)

Case II: Removal via Line 8. In this case, since the class ξj = c(a, i) has reached its capacity limit, and since the greedy algorithm selects edges in the decreasing order of weight, it must be the case that for every (a , i ) ∈ G∗ ∩ ξj, we have that
w(a, i) ≤ w(a , i ).
Since G∗ ∩ ξj contains exactly bξj , we can average the above equation over the edges in G∗ ∩ ξj to get that
w(a, i) ≤ bξ1j (a ,i )∈G∗∩ξj w(a , i ). (4)

Finally, we note that if edge (a, i) belongs to both the greedy matching and M ∗, we can simply ‘charge the weight of (a, i)’ to itself.
Now we can complete the proof by summing (3) and (4) over all the edges in M ∗. Formally, let M ∗ = M1∗ ∪ M2∗ such that M1∗ denotes the set of edges that are present in both M ∗ and G∗ as well as the edges that fall under the ﬁrst case. Similarly, let M2∗ denote the edges that fall

under the second case. Summing 3 over all of the edges in M1∗, we get that

(a,i)∈M∗ w(a, i) ≤ 2 (a,i)∈G∗ w(a, i). (5) 1
The factor of two in the right hand side comes from the fact that for any given edge (a, i) in G∗, at most two edges in M1∗ can be charged to this edge. Indeed, the only edges that can be charged to (a, i) must contain either the node a or the node i and in a matching, each node can appear in at most one edge. Next, summing (4) over all of the edges in M2∗, we get that

(a,i)∈M∗ w(a, i) = 2 ≤
=

ξj ∈C (a,i)∈M2∗∩ξj w(a, i)

ξj ∈C (a,i)∈ξj ∩G∗ w(a, i)

(a,i)∈G∗ w(a, i).

(6)

To see why this is the case, ﬁrst observe that in (4), for each edge in class ξj belonging to M2∗, all of the edges in class ξj in matching G∗ appear in the right hand side with coefﬁcient bξ1j . By deﬁnition,there are at most bξj edges of class ξj in M ∗ and exactly bξj edges of this class belong to G∗—if this were not the case, Line 8 of Algo-
rithm 1 would not be used. To conclude, the coefﬁcient for each edge in the right hand side is increased by 1
bξj
for every edge in M2∗ ∩ ξj, and summing over all edges, we get a coefﬁcient of one, therefore validating (6).
Summing (5) and (6), concludes the proof.

B.2 PROOF OF PROPOSITION 1

Properties of Markov Chains Before decomposing the regret, we brieﬂy digress to recall some classic results on mixing of Markov chains. For an ergoidic (i.e. irreducible and aperiodic) transition matrix on a ﬁnite state space Θ, let π be its stationary distribution and P˜ denote the time reversal of its transition matrix P —that is,

P˜(θ, θ

)

=

π(θ

)P (θ

, θ) .

π(θ)

The time reversal kernel P˜ is also ergodic with stationary distribution π. Deﬁne the multiplicative reversiblization M (P ) of P by M (P ) = P P˜ which is a reversible transition matrix itself. The eigenvalues of M (P ) are real
and non-negative so that the second largest eigenvalue λ1(M ) ∈ [0, 1] (Fill, 1991). Deﬁne chi-squared distance from stationary at time n by

2

(πn(θ) − π(θ))2

χn =

. π(θ)

θ

where πn = θ π0(θ)P n(θ, ·).

Proposition 2 ((Fill, 1991)). Let P be an er-

godic transition matrix on a ﬁnite state space Θ

and let π be 4 πn − π 2
maxπ0 ∈P (Θ)
1 (1−minθ π(θ))2 4 minθ π(θ)

the stationary distribution. Then ≤ λ1(M ) nχ20. Furthermore,

θ P n(θ, ·)π0(θ) − π(·) 2

≤

λ1(M ) n.

where P(Θ) us the space of probability distributions on Θ3.

From the perspective of a general epoch mixing policy α, the above proposition provides a bound on how close the distribution on types for the Markov chain is after τk time steps has elapsed when edge (a, i) is chosen.
Lemma 2. Consider an arbitrary epoch mixing policy α that selects a matching α(k) during the k–th epoch for τk iterations. For each arm (a, i) ∈ α(k), there exists a constant Ca,i > 0 such that

E µa,i − rθa(k) ≤ Ca,i

(7)

a,i

τk

The proof is a direct consequence of Proposition 2.
Proof Noting that µj = θ rθjπj(θ), a direct application of Proposition 2 gives us the following:

E
≤ τ1k ≤ τ1k = τ1k ≤ τ1k

θ rθj πj (θ) − τ1k ttk=+t1k−1 rθj,t θtk

ttk=+t1k−1 θ (πj (θ) − βt(θ))

ttk=+t1k−1 θ (πj (θ) − θ Pjt−tk (θ , θ)βtk (θ ))

ttk=+t1k−1 πj (·) −

θ

P

t j

−t

k

(

θ

,

·)βtk (θ

)

1

C λ = tk+1−1
t=tk

t−tk jj

Cj (1−λτjk ) τk(1−λj )

This is simply because of the fact that the expected reward is less than 1 by construction, the triangle inequality, and Fubini’s theorem (Folland, 2007, Theorem 2.37).

We also remark that Proposition 2 also implies that this bound holds for all βa(k) (i.e. the distribution of agent a’s type at the beginning of epoch k) and hence, is independent of the algorithm α.
Proof [Proposition 1] Consider the expression for regret from Deﬁnition 1:

Rα(n) = n gj∗∈G∗ µgj∗ − nk=1 (a,i)∈α(k) E[rθa,i],
3We remark that the bound in the above equation is easily computed by noting that χ2n is always bounded above by (minθ π(θ))−1(1 − minθ π(θ))2.

By adding and subtracting (a,i)∈P Taα,i(n)µa,i from the above equation, the cumulative regret can be written
as:

Rα(n) = n µa,i∗(a) −

Taα,i(n)µa,i

a∈A

(a,i)∈P

n

+

Taα,i(n)µa,i −

rθaa,i(k)

(a,i)∈P

k=1 (a,i)∈α(k)

=

Taα,i(n)µa,i∗(a) −

Taα,i(n)µa,i

(a,i)∈P

(a,i)∈P

+

Taα,i(n)µa,i

(a,i)∈P

n
−

1{(a, i) ∈ α(k)}rθaa,i(k)

k=1 (a,i)∈P

=

Taα,i(n)(µa,i∗(a) − µa,i)

(a,i)∈P

n
+

1{(a, i) ∈ α(k)} µa,i − rθaa,i(k)

k=1 (a,i)∈P

(8)

where 1(·) is the indicator function—e.g., 1{(a, i) ∈ α(k)} is one when the edge (a, i) belongs to the matching α(k). In the term (a,i)∈P Taα,i(n)µa,i∗(a), µa,i∗(a) appears exactly n times. Although one would expect the matching chosen by the policy (at least in the initial stages) to be sub-optimal compared to the benchmark greedy matching, it is highly possible that some individual edges (arms) may outperform those in the greedy matching. To account for this, we separate the edges in P into the sub-optimal edges and the super-optimal ones. Formally, for any given a ∈ A, deﬁne the set of suboptimal edges Sa as follows:

Sa = {(a, i) | µa,i∗(a) ≥ µa,i ∀i ∈ I}.

Suppose that S = a∈A Sa. Then, the regret bound in Equation (8) can be simpliﬁed by ignoring the contribu-
tion of the terms in P \ S. That is, since µa,i∗(a) < µa,i for all (a, i) ∈ P \ S, we have that:

Rα(n) ≤

Taα,i(n)(µa,i∗(a) − µa,i)

(a,i)∈S

n
+

1{(a, i) ∈ α(k)} µa,i − rθaa,i(k) .

k=1 (a,i)∈P

(9)

Next, we separate the second term above into the contribution of the edges in S and those in P \ S. That is,

n k=1

(a,i)∈P 1{(a, i) ∈ α(k)} µa,i − rθaa,i(k) can be

written as:

n
1{(a, i) ∈ α(k)} µa,i − rθaa,i(k)

k=1 (a,i)∈S

n
+

1{(a, i) ∈ α(k)} µa,i − rθaa,i(k)

k=1 (a,i)∈P\S

(10)

We can now use Lemma 2 to bound the difference be-

tween the empirical rewards and the stationary reward

during any given epoch. Suppose that τ0 ≥ 1 and τk = τ0 + ζk with ζ a non-zero natural number4. An ap-

plication of Lemma 2 and the tower property of expecta-

tion allows us to bound the ﬁrst term above, i.e., suppose

that T1 = E

n k=1

(a,i)∈S 1{(a, i) ∈ α(k)} µa,i −

rθaa,i(k) . Then,

n

T1 = Eα

1{(a, i) ∈ α(k)}E

k=1 (a,i)∈S

≤ Eα

µa,i − rθaa,i(k) θa(k)

n

Ca,i

1{(a, i) ∈ α(k)} τk

k=1 (a,i)∈S

n

Ca,i

≤ Eα

1{(a, i) ∈ α(k)} τ0

(a,i)∈S k=1

≤

Ca,i Eα[T α(n)]

(11)

τ0

j

(a,i)∈S

where we use the notation Eα to emphasize that

this expectation is now dependent only on the

algorithm where the number of times an arm

is chosen is a random variable. Analogously,

bound the second term of Equation 10, i.e., T2 =

E

n k=1

(a,i)∈P\S 1{(a, i) ∈ α(k)} µa,i − rθaa,i(k)

n

Ca,i

T2 ≤ Eα

1{(a, i) ∈ α(k)} τk

k=1 (a,i)∈P\S

n1

≤ τk

Ca,iEα 1{(a, i) ∈ α(k)}

k=1 (a,i)∈P\S

n1

≤ C∗ τk

Eα 1{(a, i) ∈ α(k)}

k=1 (a,i)∈P

n1 ≤ mC∗ τk ,
k=1

4There are other choices for the sequence {τk}; e.g., τk = akτ0. The choice we make allows for tighter bounds.

where C∗ = max(a,i)∈P\S Ca,i. Note that for any given epoch k, our policy selects at most m edges in the matching and therefore, (a,i)∈P\L Eα 1{(a, i) ∈ α(k)} ≤
(a,i)∈P Eα 1{(a, i) ∈ α(k)} ≤ m. Finally, we can bound the harmonic summation using the fact that τk = τ0 + ζk:

n1 T2 ≤ mC∗ τk
k=1

1

n−1+τ0/ζ 1

≤ mC∗ ζ 1 + τ0/ζ

dx x

1

n−1

≤ mC∗ ζ 1 + log τ0 + 1 (12)

Recall from the deﬁnition of the marginal infeasibility sets in Equation (1) that for any given (a, i) ∈ P \ G∗, there exists a unique edge gj∗ ∈ G∗ such that (a, i) ∈ L∗j . Deﬁne L−1(a, i) := gj∗ ∈ G∗ such that (a, i) ∈ L∗j . Now, we can deﬁne the reward gap for any given edge
(a, i) ∈ P as follows:

∆a,i = µa,i∗(a) − µa,i = µL−1(a,i) − µa,i = µgj∗−1 − µgj∗

if (a, i) ∈ S if (a, i) ∈ (P \ G∗) \ S if (a, i) = gj∗ for j ≥ 2.

Going back to our regret lower bound in (9) and decomposing the second term using (11) and (12), we get the main proposition.

B.3 PROOF OF THEOREM 2

Before proving Theorem 2, we state some useful supple-
mentary lemmas.
Lemma 3 (Azuma-Hoeffding Inequality (Azuma, 1967; Hoeffding, 1963)). Suppose (Zk)k∈Z+ is a martingale with respect to the ﬁltration (F k)k∈Z+ having bounded differences, i.e., there are ﬁnite, non-negative constants ck, k ≥ 1 such that |Zk − Zk−1| < ck almost surely.
Then for all t > 0

k

k

t2

P (Z − EZ ≤ −t) ≤ exp − 2

N (ck)2

.

k=1

We deﬁne some notation that is useful for the follow-
ing lemma as well the proof of Theorem 2. Consider
the MG-EUCB algorithm described in Algorithm 2. Let Raθ,,ji be the cumulative reward received when arm (a, i) is chosen for the j–th time where we include θ in the
subscript to note the state-dependence of the random reward. That is, Raθ,,ji = raθa,i(tja,i) where, by an abuse of notation, tja,i denotes the time instance at which edge (a, i)

is pulled for the j–th time and θa(tja,i) denotes the state of agent a during that epoch.

Deﬁne

the

ﬁltration

Fak,i

=

σ

(

Raθ,,1i

,

.

.

.

,

R

θ,k a,i

,

θa

(t

j 1

),

.

.

.

,

θ

a

(t

j k

))

—that

is,

the

smallest σ-algebra generated by the random vari-

ables

(

Raθ,,1i

,

.

.

.

,

R

θ,k a,i

,

θa

(t

1 a,i

)

,

.

.

.

,

θ

a

(

tka,i

)).

Let

Xak,i = Raθ,,ki − E[Raθ,,ki |Fak,−i 1] and Yak,i =

k j=1

Xaj,i.

We have that Yak,i is a martingale since E[Yak,+i 1|Fak,i] =

E[Xak,+i 1|Fak,i] + E[Yak,i|Fak,i] = Yak,i (since Yak,i is

Fak,i–measurable by construction) and E[|Yak,i|] < ∞

(rewards are bounded). Moreover, the boundedness of the rewards also implies the martingale Yak,i has bounded differences. Indeed, |Yak,i − Yak,−i 1| = |Xak,i| ≤ 1 almost surely since rewards are normalized to be on the interval

[0, 1], without loss of generality. Now, we are ready to

show an upper bound on the difference in the empirical

reward and the stationary state rewards.

Lemma 4. Given aperiodic, irreducible Markov chains Pa,i with corresponding stationary distributions µa,i for each (a, i) ∈ P and mixing sequence {τk} such that τk = τ0 + ζk, τ0 ≥ 1, we have that

E

µa,i

−

1 k

k j=1

E[Raθ,,ji

|Faj,−i 1]

≤ C2ak,i ζ+1τ0 + ζ1 log 1 + kτ0ζ

(13)

The proof of the above lemma follows a similar line of reasoning as Lemma 2.

Proof Since Θ is a ﬁnite set with ﬁnite elements

(i.e. |x| < ∞ for all x ∈ Θ), we are able to use

analogous reasoning as was used in Proposition 2 along

with the Markov property on the conditional expecta-

tion

E[Rij |Fij−1]

to

bound

µj

−

1 k

k i=1

E[Rij |Fij−1]

by

Ljk(k) for some constant Lj(k). Indeed, the quantity

V

=

E

µj

−

1 k

k i=1

E[Rij

|Fij−1]

can be simpliﬁed

as follows:

V = k1

k i=1

E µj − E[Rij |Fij−1]

≤ k1

k
i=1 E

θ rθj πj (θ)−

tji+1 −1

E (τij )−1

rθj,t Fij−1

t=tji

≤ k1

k
i=1 E

(τij )−1

tji+1 −1 t=tji

θ |πj(θ) − βt(θ)|

≤ k1

k i=1

Cj
2E

(τij )−1

ttji=+t1j−1(λj )(t−tji ) i

≤ k1

k Cj
i=1 2 E

(τij )−1(1 − (λj )τij )(1 − λj )−1

≤ k1 C2j 1−1λj

k
i=1 E

(τij )−1

,

where we have used the fact that the reward bounded al-

most surely on [0, 1]. Now, 1/τij is a random variable

with respect to the algorithm since at the i–th pull of arm

j we do not know a priori what iteration of the algo-

rithm we are on. However, at the i–th pull of arm, we

do know that the algorithm is at least at the i–th iter-

ation. Hence,

k
i=1 E

(τij )−1

≤

ki=1(τ0 + ζi)−1.

Now, for any a ≥ 1 and positive integer k, we have

that ai=+ak(i)−1 ≤ a1 + log(1 + ka ). Indeed, rewrite the summation in the lemma statement as ai=+ak i−1 = a−1 + ai=+ak+1 i−1 and apply the fundamental inequality,

(i)−1 ≤ ii−1 x−1dx, which holds for any i ≥ 1, repeat-

edly for i = a + 1, a + 2, . . . , a + k so that we have a

telescoping summation of integrals—i.e.

ai=+ak 1i = a1 +

a+k 1 i=a+1 i

1

a+k 1

1

a+k

≤+

dx = + log

.

aax

a

a

Thus, ki=1(τ0 + ζi)−1 ≤ (ζ + τ0)−1 + ζ1 log 1 + kτ0ζ so that (13) holds.

Proof [Theorem 2] We begin by formalizing the choice of the UCB parameter cka,i(t)—it is crucial that this parameter reﬂects the error due to both the Markov chain
and the randomness of rewards. Applying Lemma 4 to
our problem, we observe that the average error stemming from the randomness in the user state after k pulls of the edge (a, i) can be written as:

E

µa,i

−

1 k

k j=1

E[Raθ,,ji

|Faj,−i 1]

≤ C2ak,i ζ+1τ0 + ζ1 log 1 + kτ0ζ

Based on this, for each edge (a, i) and ‘pull count’ k, we deﬁne the constant Qa,i(k)

Qa,i(k) = Ca,i

1

1

kζ

+ log 1 +

.

2 ζ + τ0 ζ

τ0

Finally, we can now deﬁne the conﬁdence parameter as follows:

cka,i(t) = Qa,i(k)/k +

6

4

log(t) + log(m).

k

k

Coming back to the proof of Theorem 2, our primary goal is to map every selection of a sub-optimal edge to a condition on the relative empirical rewards between edges that can then be resolved using Azuma-Hoeffding inequality. Applying Lemma 1, we see that if MATCHGREEDY does not return the benchmark matching G∗

at epoch t and instead returns a matching α(t) = G∗, at least one of the above conditions must fail. Alternatively, this implies that one of the following two (inverse) conditions must be true:
1. 1{∃j < j | ugj∗ (t) > ugj∗ (t) ∧ (gj∗ ∈ α(t))} 2. 1{∃j, (a, i) ∈ L∗j | ugj∗ (t) < ua,i(t) ∨ ((a, i) ∈
α(t))} = 1

To express the above conditions in a concise manner, let us augment the sets L∗j to include edges from the greedy matching. Speciﬁcally, for all 1 ≤ j ≤ m − 1, let L+j = L∗j ∪ {gj∗+1} and L+m = L∗m. Observe that j L+j = P \ g1∗. Now, we can formally say that if the matching returned by the UCB algorithm during iteration t (call
this matching α(t)) does not coincide with the greedy
matching, then

1{∃1 ≤ j ≤ m, (a, i) ∈ L+j | ugj∗ (t)

< ua,i(t) ∧ (a, i) ∈ α(t)} = 1.

(14)

We will use the notation R¯ak,i

=

1 k

k j=1

Raθ,,ji

.

Since

Proposition 1 provides an upper bound for the regret in

terms of the number of times each (sub-optimal) edge

is chosen, it sufﬁces to bound the quantity Ta ,i (n),

which is the number of times our UCB algorithm selects

the edge (a , i ) given that (a , i ) ∈ S—i.e. µa ,i <

µa ,i∗(a ). Note that by deﬁnition, for any (a, i) ∈ S, the

edge (a , i ) does not belong to the greedy benchmark

matching G∗. Suppose that denotes an arbitrary integer

(to be formalized later). Then, we have that:

Ta ,i (n) = 1 +

n t=m+1

1{(a

,

i

)

∈

α(t)}

≤ 1 + nt=m+1 1{∃j, (a, i) ∈ L+j | ugj∗ (t)

< ua,i(t) ∧ (a, i) ∈ α(t)} (from (14)) ≤ 1 + nt=m+1 m j=1 (a,i)∈L+ j 1{ugj∗ (t) ≤

ua,i(t) ∧ (a, i) ∈ α(t)}

=1+ m

+

j=1 (a,i)∈Lj

nt=m+1 1{ugj∗ (t)

≤ ua,i(t) ∧ (a, i) ∈ α(t)}

≤1+ m

+

j=1 (a,i)∈Lj

+ nt=m+1 1{ugj∗ (t) ≤ ua,i(t)

∧ (a, i) ∈ α(t) ∧ Ta,i(t) > }

≤1+ m

+

j=1 (a,i)∈Lj

+ nt=m+1 1{ugj∗ (t) ≤ ua,i(t)

∧ Ta,i(t) > }

≤ m2 + m

+

j=1 (a,i)∈Lj

nt=m+1 1{ugj∗ (t)

≤ ua,i(t) ∧ Ta,i(t) > }

≤ m2 + m

+

j=1 (a,i)∈Lj

n t=m+1

1{min0<s<t usg∗ (t) ≤ max ≤k<t uka,i(t)} j

≤ m2 + m

+

j=1 (a,i)∈Lj

n t=m+1

t−1 s=1

t−1 k=

1{usg∗ (t)

≤

uka,i(t)}

j

= m2 + m

n
+

t−1

j=1 (a,i)∈Lj t=m+1 s=1

t−1 k=

1{R¯gs∗

+

csg∗ (t)

≤

R¯ak,i

+

cka,i(t)}

j

j

Now, R¯gs∗ + csg∗ (t) ≤ R¯ak,i + cka,i(t) implies that atleast

j

j

one of the following must hold:

R¯gsj∗ ≤ µgj∗ − csgj∗ (t)

(15)

R¯ak,i ≥ µa,i + cka,i(t)

(16)

µgj∗ < µa,i + 2cka,i(t) (17)

Indeed, suppose that all three of the above inequalities

are false. Then, usgj∗ (t) = R¯gsj∗ + csgj∗ (t) > µgj∗ ≥ µa,i +

2cka,i(t) > R¯ak,i + cka,i(t) = uka,i(t), which is, of course,

a

contradiction.

Hence,

if

R¯gs∗

+

c

s g

∗

(t

)

≤

R¯ak,i

+

cka,i(t),

j

j

then at least one of (15)–(17) holds. We bound the proba-

bility of events (15) and (16) using the Azuma-Hoeffding

inequality in Lemma 3 and ﬁnd an such that (17) is always false for every j, (a, i) ∈ L+j .

Towards this end, we apply Lemma 3 to the martingale
(Yak,i)k∈Z+ . Note that by the law of conditional expectations, E[Yak,i] = 0 so that Lemma 3 implies that for each arm (a, i) and any t > 0, P (Yak,i ≤ −t) ≤ exp(−t2/(2k)).

We need to relate the random variable Yak,i to the difference of the empirical mean of the average cumulative reward from its true value for each arm so that we can bound this difference. Consider the event

ω = µgj∗ − R¯gsj∗ ≥ γ

= µgj∗ − 1s

s l=1

E[Rgθ,∗l|Fgl−∗ 1]

j

j

+ 1s

s l=1

E[Rgθ,∗l|Fgl−∗ 1]

−

R¯gs∗

≥

γ

j

j

j

= µgj∗ − 1s

s l=1

E[Rgθ,∗l|Fgl−∗ 1]

−

1 s

Ygs∗

≥

γ

j

j

j

where we have added and subtracted the random variable

1 s

s l=1

E[Rgθ,∗l|Fgl−∗ 1].

By

Lemma

4,

j

j

ω⊂ =

1s Qgj∗ (s) − 1s Ygsj∗ ≥ γ 1s Ygsj∗ ≤ 1s Qgj∗ (s) − γ .

Hence,

P µgj∗ − R¯gsj∗ ≥ γ

≤ P 1s Ygsj∗ ≤ 1s Qgj∗ (s) − γ

1

1

2

≤ exp − 2 s γ − s Qgj∗ (s)

so that with γ

=

c

s g

∗

(t

)

=

j

1s Qgj∗ (s), we have,

6s log t + 4s log m +

P µgj∗ − R¯gsj∗ ≥ csgj∗ (t) ≤ t−3m−2.

Therefore, it follows that P (R¯gsj∗ ≤ µgj∗ − csgj∗ (t)) ≤ t−3m−2 and P (R¯ak,i ≥ µa,i + cka,i(t)∗) ≤ t−3m−2 which imply that (15) and (16) occur with very low prob-
ability.

Now, we choose to be the largest integer such that (17) is always false. Indeed, we choose it such that

µgj∗ − µa,i − 2cka,i(t)

> µg∗ − µa,i − 2 Qa,i( ) + j

6 log t + 4 log m > 0.

Plugging in Qa,i( ), we have

∆a,i − 2 C2a,i ζ+1τ0 + ζ1 log 1 + τζ0

+ 1 6 log t + 1 4 log m > 0.

(18)

Let ˜ = ζ/τ0 so that

∆a,i − 2 C2τa0,i 1˜ ζ+ζτ0 + 1˜ log 1 + ˜

+ 6 log t + 4 log m > 0.

√

√

Since 1/x < 1/ x and 1/x log(1 + x) < 1/ x on

[1, ∞), we have that

1

ζ

1 + log

1+˜

<

ζ

11 +

˜ ζ + τ0 ˜

ζ + τ0 ˜

˜

so that (18) reduces to ﬁnding the largest integer such

that

∆a,i − 2

√√

Ca,i

ζ

τ0

τ0

√ +√

2τ0 ζ + τ0 ζ

ζ

√

6 log t + 4 log m

+

√

>0

Rearranging and squaring terms, we get that (17) is false for

4 ρa,i

2

≥ ∆2a,i √τ0 + 6 log n + 4 log m . (19)

In the above equation, ρa,i is the edge-speciﬁc constant

ζ ρa,i = (

+ 1) C√a,i .

ζ + τ0

2ζ

In fact, we require that (17) be false for all 1 ≤ j ≤ m and (a, i) ∈ L+j . Therefore, we set the parameter to be

the maximum of the right hand side of (19). Formally, deﬁne (a∗, i∗) to be the edge in P \ g1∗ that maximizes the right hand side of (19). That is, for a given instance,

C UCB ALGORITHM
C.1 INITIAL PLAY OF UCB ALGORITHM

(a∗, i∗) = argmax(a1,i1)∈P\g1∗ + 6 log n + 4 log m

4 ∆2a1 ,i1
2

ρa1 ,i1 √
τ0 (20)

Then, by deﬁning as follows, we are assured that Equation 19 holds for all 1 ≤ j ≤ m and (a, i) ∈ L+j .

4 = ∆2a∗,i∗

ρa∗ ,i∗ √+
τ0

2
6 log n + 4 log m
(21)

Since the UCB algorithm estimates the average reward for each edge (a, i), it is customary to initialize a preliminary round where each arm is played exactly once. In the absence of any capacity constraints (e.g., bξl = m for all ξl ∈ C), it is easy to compute a sequence of m matchings so that every edge in P belongs to exactly one of these matchings. We now present a procedure that achieves the same effect even in the presence of arbitrary capacity constraints.

Hence, we can bound the number of plays of our original sub-optimal arm (a , j ) as follows:

E[Ta ,i (n)] ≤

m

n

m2 +

j=1 (a,i)∈L+ j t=m+1

t−1 t−1
s=1 k= P (R¯gsj∗ ≤ µgj∗ − csgj∗ (t))
+ P (R¯ak,i ≥ µa,i + cka,i(t))

4m2 ρa∗,i∗

≤ ∆2a∗,i∗

√+ τ0

2
6 log n + 4 log m

nt t

+

2t−3m−2

(a,i)∈P t=1 s=1 k=1

4m2 ≤ ∆2a∗,i∗

ρa∗ ,i∗ √+
τ0

2
6 log n + 4 log m

+ 2(1 + log(n)).

Algorithm 3 Computation of disjoint matchings that play each arm once

1: function MATCHINGS-INITIALPLAY(P)

2: E ← P

Edges not yet selected

3: i ← 1

Index for current matching

4: while E = ∅ do

5:

F ← E Feasible set for current matching

6:

M ←∅

7:

while F = ∅ do

8:

Select any (a, i) ∈ F

9:

if M ∪ (a, i) does not violate (P1) then

10:

M ← M ∪ (a, i)

11:

else

12:

F ← F \ (a, i).

13:

end if

14:

end while

15:

Mi ← M , i ← i + 1, E ← E \ M .

16: end while

17: return M1, M2, . . . , Mi−1

18: end function

As a direct consequence of Theorem 2, we can bound the regret of the MatchGreedy-EpochUCB policy.
Corollary 1 (Regret Bound for UCB). Consider α as the MatchGreedy-EpochUCB algorithm and suppose that τk = τ0 + ζk with τ0 ≥ 1. The regret bound is

Rα(n) ≤
(a,i)∈S

4m2 ∆2a∗ ,i∗

ρa∗ ,i∗ √+
τ0

6 log n + 4 log m 2

+ 2(1 + log(n))

∆a,i + Ca,i τ0

+ mC∗ 1 + log ζ(n − 1) + 1 ,

ζ

τ0

where (a∗, i∗) is an edge deﬁned in (20) and ρa∗,i∗ and Ca,i are edge-speciﬁc constants.

Informally, in some iteration i, the above algorithm greedily selects edges for matching Mi without violating the capacity constraints. When no additional edge can be added to Mi—a maximal matching—we move on to the next iteration.
Unfortunately, the number of matchings returned by this procedure can be quite large—in the worst case this can be as large as m2, where m is the number of agents or incentives. However, for more reasonable instances such as the ones considered in our simulations, we observe that the number of initial matchings required to play each edge at least once is much closer to the lower bound of m.

Mean Reward

1.1

Mean Reward vs Time

1.0

0.9

0.8

0.7

1.1

Agents=10, States=5

Agents=10, States=5

1.0

Agents=10, States=10

Mean Reward

Agents=10, States=10 0.9 Agents=10, States=15

Agents=10, States=15 0.8 Agents=10, States=20

Agents=10, States=20 0.7

Mean Reward vs Time

Agents=5, States=10 Agents=5, States=10 Agents=10, States=10 Agents=10, States=10 Agents=15, States=10 Agents=15, States=10 Agents=20, States=10 Agents=20, States=10

0.6 0

20000 Epoch

40000
(a)

0.6 0

20000 Epoch

40000
(b)

Figure 4: Figure 4a presents results demonstrating how the performance of our algorithm varies with the number of states given that the number of agents and incentives is ﬁxed for two instances of each conﬁguration. Figure 4b shows how the performance of the algorithm varies with the number of agents and incentives given that the number of states is ﬁxed for two instances of each conﬁguration.

Algorithm 4 Environment Implementation for Pulling a Matching (Set of Arms)

1: function INCENT(M , tn, n τ0,ζ) 2: ratn,i ← 0 ∀(a, i) ∈ M 3: for t ∈ [tn, tn + τ0 + ζn − 1] do

4:

for (a, i) ∈ M do

5:

offer incentive i to agent a

6:

receive reward raθa,i,t

7:

ratn,i ← raθ,,ti + ratn,i

8:

end for

9: end for 10: return (ratn,i)(a,i)∈M 11: end function

D ADDITIONAL EXPERIMENTS
D.1 COMPARISON OF TRADITIONAL UCB AND MG-EUCB FOR SIMPLE EXAMPLE
We return to the simple two-agent two-incentive instance depicted in Figure 1. We ignore the capacity constraints by assuming that there is a single class C1 such that every edge belongs to this class and bC1 = 2. Clearly, this instance only admits two unique matchings M ∗ = {(a1, i1), (a2, i2)}—the optimum matching—and M = {(a1, i2), (a2, i1)}—the sub-optimal matching.
As discussed previously, any traditional bandit approach that ignores the evolution of agent rewards would converge to the sub-optimal matching, i.e., M . To see why, observe that every time the algorithm selects the matching M , both the agents’ states are reset to θ1 . Following this, when the algorithms ‘explores’ the optimum matching, the reward consistently happens to be zero since the

agents are in state θ1. Owing to this, the traditional approach largely underestimates the rewards for the (edges in the) optimum matching and converges to M .

Cumulative Regret

5000 4000 3000 2000 1000
0 0

Classical UCB MG-EUCB

2000

4000

Number of Epochs

Figure 6: Comparison of the performance of classical UCB algorithms for matching problems versus the MatchGreedyEpochUCB algorithm for the example depicted in Figure 1.The length of horizon was n = 5000.

To validate this experimentally, we compare the perfor-

mance of our MatchGreedy-EpochUCB algorithm de-

scribed in Algorithm 2 to a conventional implementa-

tion of the UCB algorithm for matching problems (e.g.,

as in (Chen et al., 2016; Gai et al., 2011; Kveton et al.,

2015)). More speciﬁcally, we consider an implementa-

tion that runs for a total of

k i=1

τk

for

some

suitable

set of parameters—in each iteration, the algorithm se-

lects a matching based on the empirical rewards and the

conﬁdence bound. The iterations are then divided into

rewards for convenience and the time-average reward in

each epoch is computed and plotted alongside the same

metric for the MG-EUCB algorithm in Figure 6.

Our simulations support our prior conclusions. For ex-

100

Eﬃciency vs. Time

100

Eﬃciency vs. Time

80

80

Eﬃciency % Eﬃciency %

60

60

40

40

20 00

MG-EUCB+ No Incentive
20000 Epoch
(a) Static Demand

G-UpBound 40000

20 00

MG-EUCB+ No Incentive

G-UpBound

20000 Epoch

40000

(b) Random Demand

Figure 5: Bike-share experiments with utility model: Figures 5a and 5b compare the efﬁciency of the bike-share system with two demand models and a utility based behavioral model under incentive matchings selected by MG-EUCB+ with upper and lower bounds given by the system performance when the incentive matching is given by computing the optimal greedy matching at each epoch based on the current state information and when no incentives are offered respectively.

ample, after 5000 epochs, the classical UCB algorithm selects the sub-optimal matching over 99% of the time. Owing to this reason, the classical algorithm has a regret that grows linearly with the length of the horizon whereas the regret of our algorithm is almost zero for this instance.
D.2 ADDITIONAL SYNTHETIC EXPERIMENTS
In our synthetic simulations we ﬁxed the number of agents, incentives, and states equally as m = |A| = |I| = |Θa| = 10. We now present results in Figure 4 evaluating how the performance of our algorithm varies with each of these parameters. In Figure 4a, we observe that when the number of agents and incentives is ﬁxed, the number of states has a negligible impact on the rate of convergence to the optimal solution. This indicates that within this range of states the Markov chains mix rapidly and the edge dependent constants in the regret bound do not signiﬁcantly factor in. We ﬁnd in Figure 4b, as predicted by our regret bounds, the convergence slows as the number of agents in the problem increases.
D.3 ADDITIONAL BIKE-SHARE DESCRIPTION AND EXPERIMENTS
In this section we provide further motivation for the bikesharing problem as a matching problem, more detail on our problem setup, as well as additional experimental results. Bike-share programs must deal with varying

spatio-temporal demand to ensure that a high percentage of demand is met in order to satisfy customers and maximize proﬁt. To avoid both pile-ups of bikes at popular destinations and depletion of bikes at stations with high demand, bike sharing companies manually replenish and manipulate the spatial supply of bikes. This is costly to companies and an alternative is to attempt to incentivize users to alter their paths in order to balance the spatial supply of bikes in such a way that meets future demand. A successful incentive system could reduce the need for manually replenishing the supply of bikes at stations, saving money and time as a result.
Figure 7: Heatmap of the scaled initial supply of the Boston Hubway stations. Each bubble indicates the location of a station and are scaled in size and colored according to the number of bikes available at the station.
We consider the bike-share problem as a repeated game in our simulations. Speciﬁcally, at each epoch users move into the system seeking a bike from a station

Figure 8: This heatmap shows the spatial reduction in the number of rejections at each station in epoch 20000 from epoch 1000 corresponding to the result in Figure 3a. Positive numbers indicate how many fewer rejections occurred at the station at the later epoch than the earlier epoch. We observe a global reduction spatially in rejections nearly uniformly.
while simultaneously users transition from the location in which they picked up a bike to a location where they drop off the bike. In our simulations we allow the spatial supply of bikes to evolve based on the transitions of bikes between stations. We begin each simulation with the supply at each station given by the data scaled by a factor of two. As a result we have over 6000 agents in the system that can move between close to 200 stations.
We experimented with static and random demand models using quantities derived from the data. In the static demand model we set the demand between a directed pair of stations at each epoch to be the empirical mean of the number of transitions between the stations within 12PM– 1PM at each day over June, 2017 – August, 2017. In our random demand model we used the empirical means as the parameter of a Poisson distribution from which we sampled the demand at each epoch for each directed pair of stations. To justify this choice we have included several representative probability mass functions for the demand between stations and the Poisson distributions that were ﬁt to them in Figure 9. We also applied goodness of ﬁt tests to ensure this was a realistic modeling choice.
In our simulations we considered two behavioral models of the users in the system that govern how rewards are produced as well as the probability of a user accepting an incentive. As touched upon previously, in our bikeshare model, associated with the state of a user are a distance threshold parameter and a parameter of a Bernouilli distribution. The distance threshold gives the maximum distance a user is willing to be re-routed and is drawn uniformly at random for each state in [0, 4000] meters. The Bernouilli parameter gives the probability that a user will accept an incentive below its distance threshold for a particular state and is drawn uniformly at random in [0, 1]. In the primary behavioral model we consider based on a

Bernouilli distribution presented in Figure 3, if the distance between the two stations of the proposed incentive is less than the threshold parameter associated with an agent’s state the agent will accept the incentive with probability p and give a reward of one, otherwise the incentive will be rejected and a reward of zero will be given. We also investigate a utility-based model; this model is the same as the Bernouilli based model with the slight modiﬁcation that if an incentive is accepted following a successful realization of the Bernouilli draw, a reward is given that is proportional to the difference in distance between the threshold associated with a users state and the distance between the station the user intended to go to and the station of the proposed incentive.
We now give an overview of our results and the additional experiments we present in this section. We make two key favorable observations from the simulations in Figure 3 in which we investigated static and random demand with the Bernoulli behavioral model. First, compared to a naive baseline of the convergence of the system without any incentives our algorithm is able to increase the efﬁciency of the system approximately 40% with the static demand model. Furthermore, the extension to random demand does not reduce the performance signiﬁcantly. When comparing to an upper bound on performance we observe that our algorithm leads the system to approach this limit.
The mean matching rewards presented in Figure 3c can be interpreted as the mean number of incentives that are accepted and equivalently the mean of users re-routed. This result indicates that on average less than 1% of users are matched to an incentive. This is a highly desirable property as it means we only need to inﬂuence a small part of the population in order to get signiﬁcant performance gains. As a result, most users will only beneﬁt from the incentive system, while from the planners perspective the minuscule cost of incentivizing only a small portion of the population is a beneﬁcial.
We now show the results in Figure 5 of the static and random demand in combination with the utility based behavioral model. We generally draw the same conclusions as from Figure 3 with somewhat lower performance for the system. This is an expected result as the users are more sensitive to the extra distance they must travel due to an incentive and they are therefore more difﬁcult to incentivize. We note that we observed looking at the additional distances traveled due to an accepted incentive, that users under the utility based model do travel modestly less additional distance as a result of accepting an incentive than when we used the Bernouilli based model.

0.5

0.3

0.15

0.4

0.2

0.3

0.10

0.2

0.1

0.05

0.1

0.0 0

2

4

6

8 0.0 0

2

4

6

8 0.00 0

2

4

6

8

(a)

(b)

(c)

Figure 9: Each empirical probability mass function in the ﬁgure gives the probability on the number of users that transitioned between a pair of stations in the Boston Hubway dataset between 12PM–1PM each day between June, 2017 – August, 2017. The red lines show the Poisson distribution that we ﬁt to the distributions that we sampled from to generate random demand at each epoch of the simulation.

E IMPLEMENTATION DETAILS
We make a small modiﬁcation to the number of iterations within an epoch to reduce computation time of the MGEUCB algorithm. Speciﬁcally when the time-averaged reward has changed by no more than 5 × 10−4 between consecutive iterations for 200 iterations in a row— indicating the time averaged reward has converged—we end the epoch early. We ﬁnd that this leads to the number of iterations in an epoch being roughly in the range of 1000-1500. We observe this leads to a negligible change in the mean and cumulative rewards of the algorithm while signiﬁcantly speeding up computation over a large horizon.

The resource constraints that we considered were static over time. It is often the case that constraints of this form are time-varying or coupled over the decision-making horizon. A prominent example in online resource allocation is the Adwords problem. Due to the practical signiﬁcance, we plan to explore if our model can be adapted to capture this richer class of constraints.
Finally, we would like to make our model increasingly realistic from the designer’s and agents’ perspectives. From the designer’s point of view, this would be to incorporate incentive compatibility and fairness constraints. From the perspective of the agent, beyond the MDP dynamics, strategic behavior will be important to model and assess the impacts of going forward.

F Discussion
In this work we developed a bandit algorithm for matching incentives to users, whose preferences are unknown a priori and evolving dynamically in time, in a resource constrained environment. We theoretically analyzed the problem and derived logarithmic gap-dependent regret bounds. There are several interesting future lines of work that we believe are worth pursuing.
In this work, under the MDP dynamics we only investigated the combinatorial optimization problem of resource constrained matching and our proof techniques relied on the properties of the greedy matching paradigm. In future work, we are interested in attempting to extend this work to arbitrary combinatorial optimization problems with constraints in the case that the designer is allowed oracle access to solve the optimization problem, as has been done in the case without dynamics (Kveton et al., 2015; Wen et al., 2015).

