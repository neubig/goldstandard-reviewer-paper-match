Estimating Treatment Effects with Observed Confounders and Mediators

arXiv:2003.11991v3 [stat.ME] 14 Jun 2021

Shantanu Gupta

Zachary C. Lipton

Carnegie Mellon University {shantang,zlipton,dchilders}@cmu.edu

David Childers

Abstract
Given a causal graph, the do-calculus can express treatment effects as functionals of the observational joint distribution that can be estimated empirically. Sometimes the do-calculus identiﬁes multiple valid formulae, prompting us to compare the statistical properties of the corresponding estimators. For example, the backdoor formula applies when all confounders are observed and the frontdoor formula applies when an observed mediator transmits the causal effect. In this paper, we investigate the over-identiﬁed scenario where both confounders and mediators are observed, rendering both estimators valid. Addressing the linear Gaussian causal model, we demonstrate that either estimator can dominate the other by an unbounded constant factor. Next, we derive an optimal estimator, which leverages all observed variables, and bound its ﬁnite-sample variance. We show that it strictly outperforms the backdoor and frontdoor estimators and that this improvement can be unbounded. We also present a procedure for combining two datasets, one with observed confounders and another with observed mediators. Finally, we evaluate our methods on both simulated data and the IHDP and JTPA datasets.
1 INTRODUCTION
Causal effects are not, in general, identiﬁable from observational data alone. The fundamental insight of causal inference is that given structural assumptions on the data generating process, causal effects may become expressible as functionals of the joint distribution over observed variables. The do-calculus, introduced by Pearl [1995], provides a set of three rules that can be used to convert causal quantities into such functionals. We are motivated by the observation

that, for some causal graphs, treatment effects may be overidentiﬁed. Here, applications of the do-calculus produce distinct functionals, all of which, subject to positivity conditions, yield consistent estimators of the same causal effect. Consider a causal graph (see Figure 1) for which the treatment X, mediator M , confounder W , and outcome Y are all observable. Using the backdoor adjustment, we can express the average treatment effect of X on Y as a function of P (X, W, Y ), while the frontdoor adjustment expresses that same causal quantity via P (X, M, Y ) [Pearl, 1995]. In our experiments, we work with a real-world dataset that contains both confounders and mediators. Faced with the (fortunate) condition of overidentiﬁcation, our focus shifts from identiﬁcation: is our effect estimable?, to optimality: which among multiple valid estimators dominates from a standpoint of statistical efﬁciency?
In this paper, we address this very graph, focusing our analysis on the linear causal model [Wright, 1934], a central object of study in causal inference and econometrics and also explore the semiparametric setting. Over-identiﬁcation can arise in many other causal graphs (e.g. multiple backdoor adjustment sets, multiple instrumental variables, etc.). However, we focus on this graph because the frontdoor estimator is a canonical example of a novel identiﬁcation result derived using graphical models. It is central in the causality literature [Pearl and Mackenzie, 2018, Imbens, 2019] and is a natural ﬁrst step in the study of over-identiﬁed causal models. Deriving the ﬁnite sample variance of the backdoor and frontdoor estimators, and precisely characterizing conditions under which each dominates, we ﬁnd that either may outperform the other to an arbitrary degree depending on the underlying model parameters. These expressions can provide guidance to practitioners for assessing the suitability of each estimator. For example, one byproduct of our analysis is to characterize what properties make for the “ideal mediator”. Moreover, in the data collection phase, if one has a choice between collecting data on the mediator or the confounder, these expressions, together with the practitioner’s beliefs about likely ranges for model parameters,

Accepted for the 37th Conference on Uncertainty in Artiﬁcial Intelligence (UAI 2021).

W

2 RELATED WORK

X

M

Y

Figure 1: Causal graph with observed mediator and confounder. The backdoor and frontdoor estimators are both applicable.

can be used to decide what data to collect.
Next, we propose techniques that leverage both observed confounders and mediators. For the setting where we simultaneously observe both the confounder and the mediator, we introduce an estimator that optimally combines all information. We prove theoretically that this method achieves lower mean squared error (MSE) than both the backdoor and frontdoor estimators, for all settings of the underlying model parameters. Moreover, the extent to which this estimator can dominate the better of the backdoor and frontdoor estimators is unbounded. Subsequently, we consider the partially-observed setting in which two datasets are available, one with observed confounders (but not mediators) {(X, W, Y )}ni=1, and another with observed mediators (but not confounders) {(X, M, Y )}m i=1. Interestingly, the likelihood is convex given simultaneous observations but non-convex under partially-observed data. We introduce an estimator that is guaranteed to achieve higher likelihood than either the backdoor or frontdoor estimators. Finally, we evaluate our methods on synthetic, semisynthetic, and real datasets. Our proposed estimators that combine confounders and mediators always exhibit lower MSE than the backdoor and frontdoor estimators when our model assumptions are satisﬁed.
Our principal contributions are the following:
1. Derivation of the parameter regimes where either of the frontdoor and backdoor estimators dominate vis-avis sample efﬁciency.
2. Demonstration of strict (and unbounded) improvements of the optimal (combined) estimator over both the frontdoor and backdoor estimators.
3. Adaptation of a semi-parametric estimator to our graph, showing the beneﬁts of our approach in nonlinear settings.
4. Analysis for the partially observed case, where mediators and confounders are observed separately (but never simultaneously).

The backdoor adjustment formalizes the practice of controlling for known confounders and is widely applied in statistics and econometrics [Pearl, 2009, 2010, Perkovic´ et al., 2015]. The frontdoor adjustment, which leverages observed mediators to identify causal effects even amid unobserved confounding, has seen increasing application in real-world datasets [Bellemare and Bloem, 2019, Glynn and Kashin, 2018, 2017, Chinco and Mayer, 2016, Cohen and Malloy, 2014].
In the most similar work to ours, Glynn and Kashin [2018] compare the frontdoor and backdoor adjustments, computing bias (but not variance) formulas for each and performing sensitivity analysis. Exploring a real-world job training dataset, they demonstrate that the frontdoor estimator outperforms its backdoor counterpart (in terms of bias). The ﬁnite sample variance of the frontdoor estimator for the linear Gaussian case was previously derived by Kuroki [2000]. Ramsahai [2012] compare the frontdoor and backdoor estimators based on their asymptotic variances and also show that the combined estimator’s variance cannot be higher than the other two. Kuipers and Moffa [2020] derive the ﬁnite-sample variances of two possible adjustments in a three-variable binary causal graph and show that the optimal estimator depends on the model parameters. Henckel et al. [2019] introduce a graphical criterion for comparing the asymptotic variances of adjustment sets for the backdoor criterion in linear causal models. Rotnitzky and Smucler [2019] extend this work, showing that the same graphical criterion is valid for non-parametric causal models. They also present a semi-parametric efﬁcient estimator that exploits the conditional independencies in a causal graph.
Researchers have also worked to generalize the frontdoor criterion. Bareinboim et al. [2019] introduce the conditional frontdoor criterion, allowing for both treatmentmediator confounders and mediator-outcome confounders. Fulcher et al. [2020] propose a method for including observed confounders along with a mediator with discrete treatments.
The study of overidentiﬁed models dates at least back to Koopmans and Reiersøl [1950]. Sargan [1958], Hansen [1982] formalized the result that in the presence of overidentiﬁcation, multiple estimators can be combined to improve efﬁciency. This was extended to the non-parametric setting by Chen and Santos [2018]. A related line of work considers methods for combining multiple datasets for causal inference. Bareinboim and Pearl [2016] study the problem of handling biases while combining heterogeneous datasets, while Jackson et al. [2009] present Bayesian methods for combining datasets with different covariates and some common covariates.

3 PRELIMINARIES

In this work, we work within the structural causal model (SCM) framework due to Pearl [2009], formalizing causal relationships via directed acyclic graphs (DAGs). Each X → Y edge in this DAG indicates that the variable X is (potentially) a direct cause of variable Y . All measured variables are deterministic functions of their parents and a set of jointly independent per-variable noise terms.

Linear Gaussian SCM In linear Gaussian SCMs, each variable is assumed to be a linear function of its parents. The noise terms are assumed to be additive and Gaussian. In this paper, the ﬁnite sample results are derived for the linear Gaussian SCM for the overidentiﬁed confoundermediator graph (Figure 1), where the structural equations can be written as

wi = uwi ,

uwi ∼ N (0, σu2w )

xi = dwi + uxi , mi = cxi + um i ,

uxi ∼ N (0, σu2 )

x
um ∼ N (0, σ2 )

(1)

i

um

yi = ami + bwi + uyi , uyi ∼ N (0, σu2y ).

Here, wi, xi, mi, and yi are realized values of the random variables W, X, M, Y , respectively, and uwi , uxi , um i , uyi are realized values of the corresponding noise terms. The zero
mean assumption in Eq. 1 simpliﬁes analysis, but is not
necessary for the results presented in this paper.

3.1 THE BACKDOOR AND FRONTDOOR ADJUSTMENTS
The effect of a treatment X is expressible in terms of the post-intervention distributions of the outcome Y for different values of the treatment X = x. An intervention do(X = x) in a causal graph can be expressed via the mutilated graph that results from deleting all incoming arrows to X, setting X’s value to X = x for all instances, while keeping the SCM otherwise identical. This distribution is denoted as P (Y |do(X = x)).
The backdoor and frontdoor adjustments [Pearl, 2009] express treatment effects as functionals of the observational distribution. Consider our running example of the causal model in Figure 1. We denote X as the treatment, Y as the outcome, W as a confounder, and M as a mediator. Our goal is to estimate the causal quantity P (Y |do(X = x)).
Backdoor Adjustment When all confounders of both X and Y are observed—in our example, W —then the causal effect of X on Y , i.e., P (Y |do(X = x)) can be written as
P (Y |do(X = x))
= P (Y |X = x, W = w)P (W = w). (2)
w

Frontdoor Adjustment This technique applies even when the confounder W is unobserved. Here we require access to a mediator M that (i) is observed; (ii) transmits the entire causal effect from X to Y ; and (iii) is not inﬂuenced by the confounder W given X. The effect of X on Y is computed in two stages. We ﬁrst ﬁnd the effect of X on M , then the effect of M on Y as:
P (M = m|do(X = x)) = P (M = m|X = x) (3) P (Y |do(M = m))
= P (Y |M = m, X = x)P (X = x). (4)
x
We can then write the causal effect of X on Y as
P (Y |do(X = x))
= P (M = m|do(X = x))P (Y |do(M = m)).
m

4 VARIANCE OF BACKDOOR & FRONTDOOR ESTIMATORS

In this section, we analyze the backdoor and frontdoor estimators and characterize the regimes where each dominates. We work with the linear SCM described in Eq. 1. Throughout, our goal is to estimate the causal effect of X on Y . In terms of the underlying parameters of the linear SCM, the quantity that we wish to estimate is ac. Absent measurement error, both estimators are unbiased (see proof in Appendix C) and thus we focus our comparison on their respective variances.

Variance of the Backdoor Estimator The backdoor es-
timator requires only that we observe {X, Y, W } (but not
necessarily the mediator M ). Say we observe the samples {xi, yi, wi}ni=1. We can estimate the causal effect ac by taking the coefﬁcient on X in an OLS regression of Y on {X, W }. This controls for the confounder W and corre-
sponds naturally to the adjustment described in Eq. 2.

The ﬁnite sample and asymptotic variances of the backdoor estimator are (see proof in Appendix D.1)

a2σu2 + σu2

Var(ac)backdoor =

m
(n − 3)σ2

y,

ux

√

a2σu2 + σu2

lim Var( n(ac − ac))backdoor =
n→∞

m
σ2

y . (5)

ux

Variance of the Frontdoor Estimator The frontdoor
estimator is used when {X, Y, M } samples are observed. Say we observe the samples {xi, yi, mi}ni=1. First, we estimate c by taking the coefﬁcient on X in an OLS regression
of M on X. Let the estimate be c. This corresponds to the
adjustment in Eq. 3. Then, we estimate a by taking the coefﬁcient on M in an OLS regression of Y on {M, X}. Let

the estimate be af . This corresponds to the adjustment in Eq. 4.

5 COMBINING MEDIATORS & CONFOUNDERS

The ﬁnite sample variances of c and af are (see proof in Appendix D.2)

σu2

Var(c)

=

(n

−

m
2)(d2σ2

+ σ2 ) ,

(6)

uw

ux

b2σu2 σu2 + σu2 (d2σu2 + σu2 )

Var(af ) =

wx

y

w

(n − 3)(d2σ2 + σ2 )σ2

x.

(7)

uw

ux um

Having characterized the performance of each estimator
separately, we now consider optimal strategies for estimat-
ing treatment effects in the overidentiﬁed regime, where
we observe both the confounder and the mediator simultaneously. Say we observe n samples {xi, yi, wi, mi}ni=1. We show that the maximum likelihood estimator (MLE) is
strictly better than the backdoor and frontdoor estimators.

Using

the

facts

that

Cov(af , c)

=

0

and

Cov(a

2 f

,

c

2

)

=

Var(af )Var(c), the ﬁnite sample variance of the frontdoor

estimator is (see proof in Appendix D.2.4)

Var(af c) = c2Var(af ) + a2Var(c) + 2Var(af )Var(c).

The MLE will be optimal since our model satisﬁes the necessary regularity conditions for MLE optimality (by virtue of being linear and Gaussian). The combined estimator is unbiased (see Appendix C.3) and thus we focus on the variance.

(8) And the asymptotic variance, which does not require Gaus-

Let the vector si = [xi, yi, wi, mi] denote the ith sample. Since the data is multivariate Gaussian, the log-likelihood

sianity, is (see proof in Appendix D.2.5)

of the data is LL = − n2 log (det Σ) + Tr (ΣΣ−1) ,

√ lim Var( n(a

c − ac)) = c2(b2σu2w σu2x + σu2y D) + a2σu2m ,where Σ

=

Cov([X, Y, W, M ]) and Σ

=

1 n

n i=1

sisi⊤.

n→∞ f

Dσu2

D The MLE for a Gaussian graphical model is ΣMLE = Σ

m

where D = d2σu2w + σu2x .

(9) [Uhler, 2019]. Let the MLE estimates for parameters c and a be c and ac, respectively. Then

The Ideal Frontdoor Mediator A natural question then arises: what properties of a mediator make the frontdoor estimator most precise? We can see that Var(af c) is nonmonotonic in the mediator noise σum . Eq. 8 provides us with guidance. Var(af c) is a convex function of σu2m . The ideal mediator will have noise variance σu2∗m which minimizes Eq. 8. That is,
σu2∗m = arg min [Var(af c)]
σu2 m

c = Σ1,4 , ac = Σ1,4Σ3,3 − Σ1,3Σ3,4 .

Σ1,1

Σ3,3Σ4,4 − Σ23,4

(11)

The MLE estimate for c in Eq. 11 is the same as for the frontdoor—the coefﬁcient of X in an OLS regression of M on X. The MLE estimate for a in Eq. 11 is the coefﬁcient of M in an OLS regression of Y on {M, W }. The ﬁnite sample variance of ac is (see proof in Appendix D.3.1)

|c| =⇒ σu2∗m =

b2σu2w σu2x + σu2y D |a|

n−2 n − 3,

σu2

Var(ac)

=

(n

−

y
3)(c2σ2

+ σ2

).

(12)

ux

um

where D = d2σu2w + σu2x .

Comparison of Backdoor and Frontdoor Estimators The relative performance of the backdoor and frontdoor estimators depend on the underlying SCM’s parameters. Using Eqs. 5 and 8, the ratio of the backdoor to frontdoor variance is

RVar = Var(ac)backdoor

(10)

Var(af c)

(n − 2)σu2 D2(a2σu2 + σu2 )

= σ2 ((n − 3)a2σ4

m
D + (2σ2

m
+

c2

y
(n

−

2)D)E

)

,

ux

um

um

where D = (d2σu2w + σu2x ) and E = (b2σu2w σu2x + σu2y D). The backdoor estimator dominates when RVar < 1 and vice
versa when RVar > 1. Note that there exist parameters that cause any value of RVar > 0. In particular, as σu2x → 0, RVar → ∞ and as σu2x → ∞, RVar → 0, regardless of the sample size n. Thus, either estimator can dominate the
other by any arbitrary constant factor.

The variance of c is the same as the frontdoor case as in Eq. 7. Let r1 = nn−−35 , r2 = 3(nn−−42) , and L =

+ c2 σu2 y
c2σu2 x +σu2 m

a2σu2 m d2σu2 w +σu2 x

. We can bound the ﬁnite sam-

ple variance of the combined estimator as

L ≤ Var(acc) ≤ c2Var(ac) + a2Var(c) + r1 n
2|c|Var(ac) Var(c) + r2Var(ac)Var(c) . (13)

The lower bound is derived using the Cramer-Rao theorem (since the estimator is unbiased) and for the upper bound, we use the Cauchy-Schwarz inequality. The complete proof is in Appendix D.3.2. And the asymptotic variance, which does not require Gaussianity, is (see proof in Appendix D.3.3)

√

lim Var( n(acc − ac)) = L.

(14)

n→∞

The Ideal Mediator Just as with the frontdoor estimator,

we can ask what makes for an id√eal mediator in this case. Eq. 14 shows that limn→∞ Var( nacc) is a convex function of σu2m . The ideal mediator will have noise variance σu2∗m which minimizes the variance in Eq. 14. We use the asymptotic variance here since we only have ﬁnite-sample

bounds on the variance of the combined estimator. This

means that

σ2∗ = arg min

√ lim Var( nacc)

um

σ2

n→∞

um

=⇒ σ2∗ = max 0, |c|σuy d2σu2w + σu2x − c2σ2 .

um

|a|

ux

5.1 COMPARISON WITH BACKDOOR AND FRONTDOOR ESTIMATORS
We can compare Eqs. 5 and 14 too see that, asymptotically, the combined estimator has lower variance than the backdoor estimator for√all values of mo√del parameters. That is, as n → ∞, Var( nacc) ≤ Var( nac)backdoor. Similarly, we can compare Eqs. 9 and 14 to see that, asymptotically, the combined estimator is always better than the frontdoor estimator for √all values of m√odel parameters. That is, as n → ∞, Var( nacc) ≤ Var( naf c).
In the ﬁnite sample case, using Eqs. 5 and 13, we can see that for all model parameters, for a large enough n, the combined estimator will dominate the backdoor. That is, ∃N, s.t., ∀n > N, Var(acc) ≤ Var(ac)backdoor, where the dependence of N on the model parameters is stated in Appendix E.1. We can make a similar argument for the dominance of the combined estimator over the frontdoor estimator. Using Eqs 8 and 13, it can be shown that ∃N, s.t., ∀n > N, Var(acc) ≤ Var(af c), where the dependence of N on the model parameters is stated in Appendix E.2.
Next, we show that the combined estimator can dominate the better of the backdoor and frontdoor estimators by an arbitrary amount. That is, we show that the quantity R = min{Var(aVc)abr(acakdcocor),Var(af c)} is unbounded. Consider the case when Var(ac)backdoor = Var(af c). This condition holds for certain settings of the model parameters (see Appendix E.3 for an example). Here,
R = Var(ac)backdoor Var(acc)
≥ (n − 2)DE(a2σu2m√+ σu2y ) , (15) σu2x F + σu2y σu2m + 3σu2m H
where D = d2σu2w + σu2x , E = c2σu2x + σu2m , r1 = nn−−35 , r2 = nn−−24 , F = (n − 3)a2σu2m E, G = r1 √(σnu−m2)D , H = r1r2 + |c|(n − 2)D (|c| + G) and, in Eq. 15, we used

Eq. 13. We can see that as σux → 0, R → ∞ and thus R is unbounded. This shows that, even in ﬁnite samples, combining confounders and mediators can lead to an arbitrarily better estimator than the better of the backdoor and frontdoor estimators.

5.2 SEMI-PARAMETRIC ESTIMATORS
Fulcher et al. [2020] derive the efﬁcient inﬂuence function and semi-parametric efﬁciency bound for a generalized model with discrete treatment and non-linear relationships between the variables. While they allow for confounding of the treatment-mediator link and the mediator-outcome link, the graph in Figure 1 has additional restrictions. As per Chen and Santos [2018], this graph is locally overidentiﬁed. This suggests that it is possible to improve the estimator by Fulcher et al. [2020, Eq. (6)] (which we refer to as IFFulcher). In our model, there are two additional conditional independences compared to the graph studied in Fulcher et al. [2020]: Y ⊥⊥ X|(M, W ), and M ⊥⊥ W |X. We incorporate these conditional independences in IF-Fulcher by using E[Y |M, W, X] = E[Y |M, W ], and f (M |X, W ) = f (M |X) to create an estimator we refer to as IF-Restricted:

1n

f (M |x∗)

Ψ = n i=1(Yi − E[Y |Mi, Wi]) f (M |Xi) +

1{Xi = x∗} × E[Y |Mi, Wi]− P (Xi = x∗|Wi)

E[Y |m, Wi]f (m|Xi) + E[Y |m, Wi]f (m|x∗),

m

m

where, if f , P , and E are consistent estimators, then Ψ →p E[Y |do(X = x∗)]. By double robustness of the given estimator, if f , P , and E are correctly speciﬁed, then IF-Restricted has identical asymptotic distribution as IFFulcher. But using the additional restrictions improves estimation of nuisance functions. Thus we expect the proposed semi-parametric estimator to perform better in ﬁnite samples. Rotnitzky and Smucler [2019], in contemporaneous work, analyzed the same graph and showed that, in addition, the efﬁcient inﬂuence function is also changed when imposing these conditional independences (see Example 10 in their paper) (we refer to the estimator for this inﬂuence function as IF-Rotnitzky). For our experiments with binary treatments, we use linear regression for f , E and logistic regression for P . Another way to adapt IF-Fulcher is for the case when we do not observe the confounders (as in the frontdoor adjustment). In this case, we can set Wi = ∅ and apply Ψ. We call this special case IF-Frontdoor.

Asymptotic variance Asymptotic variance

65

64

63

62 0.0

Optimal k 0.2 0.4 0.6 0.8 1.0 Fraction of confounders: k = P / N

(a) Optimal k = 0.3

65.0

Optimal k

64.5

64.0

63.5

63.0 0.0 0.2 0.4 0.6 0.8 1.0 Fraction of confounders: k = P / N
(b) Optimal k = 0.5

Figure 2: The asymptotic variance vs k for two cases where the variance is minimized when k ∈ (0, 1). That is, collecting a mix of confounders and mediators is better than collecting only confounders or mediators.

6 COMBINING REVEALED-CONFOUNDER AND REVEALED-MEDIATOR DATASETS

We now consider a situation in which the practitioner has access to two datasets. In the ﬁrst one, the confounders are observed but the mediators are unobserved. In the second one, the mediators are observed but the confounders are unobserved. This situation might arise if data is collected by two groups, the ﬁrst selecting variables to measure to apply the backdoor adjustment and the second selecting variables to apply the frontdoor adjustment. Given the two datasets, we wish to optimally leverage all available data to estimate the effect of X on Y .

A naive approach would be to apply the backdoor and frontdoor estimator to the ﬁrst and second dataset, respectively, and take a weighted average of the two estimates. However, in this case, the variance will be between that of the frontdoor and backdoor estimator. We analyze the MLE, showing that this estimator has lower asymptotic variance than both the backdoor and frontdoor estimators.

Combined Log-Likelihood under Partial Observability

Say

we

have

P

samples

of

{

xi

,

y

i

,

wi

}

P i=

1

.

Let

each

such

sample be denoted by the vector pi = [xi, yi, wi]. More-

over, say we have Q samples of {xi, yi, mi}Qi=1. Let each such sample be denoted using the vector qj = [xj , yj, mj].

Let the observed data be represented as D. That is, D =

{p1, p2, . . . , pP , q1, q2, . . . , qQ}. Let N = P + Q and let

k

=

P N

.

Since

the

data

is

multivariate

Gaussian,

the

condi-

tional log-likelihood given k can be written as

LL(D|k) = − N [k log det Σp + Tr (ΣpΣ−1)

2

p

(1 − k) log det Σq + Tr (ΣqΣ−q 1) ],

+ (16)

where Σp = Cov([X, Y, W ]), Σq = Cov([X, Y, M ]),

Σp =

Pi=1Ppip⊤ i and Σq =

. Q
i=1

qi q⊤ i

Q

Cramer-Rao Lower Bound To compute the variance of the estimate of e = ac, we compute the Cramer-Rao

variance lower bound. We ﬁrst compute the Fisher information matrix I as I = −E ∇2θLL , where θ represents the eight model parameters. Let e be the MLE. Since regularity holds for our model (due to linearity and Gaussianity), the MoreLmE,ifsoarscyomnpsttaontitckal,lyasnNorm→al.∞U,swineg hthaeveC√raNme(er-R−aeo)th→deN (0, Ve), where Ve is a function of I−1. The closed form expression for Ve is given in Appendix F.1.
√ For any ﬁxed k ∈√(0, 1), (Ve − AVar( P ac)backdoor) < 0 and (Ve − AVar( Qaf c)) < 0, where AVar is asymptotic variance. This shows that the combined estimator always has lower asymptotic variance than that of the backdoor and frontdoor estimators on the individual datasets. Moreover, we also ﬁnd cases where the combined estimator outperforms both the backdoor and frontdoor estimators even when the total number of samples are the same.√That is, there exist model parameters s√uch that (Ve − AVar( N ac)backdoor) < 0 and (Ve − AVar( N af c)) < 0 for some k ∈ (0, 1). This means that is these cases, it is better to collect a mix of confounders and mediators rather than only collecting mediators or confounders. Despite having access to the same number of samples, a mix of confounders and mediators can lead to lower variance. This happens when the variances of the backdoor and frontdoor estimators are close to each other. In Figure 2, we present two examples of causal graphs where having a mix of confounders and mediators leads to the lowest asymptotic variance (see Appendix F.2 for parameter values).
The Maximum Likelihood Estimator Computing an analytical solution for the model parameters that maximizes the log-likelihood turns out to be intractable. As a result, we update our estimated parameters to maximize the likelihood numerically. The likelihood in Eq. 16 is non-convex. So we intialize the parameters using the two datasets (see Appendix F.3 for details) and run the Broyden–Fletcher–Goldfarb–Shanno (BFGS) algorithm [Fletcher, 2013] to maximize the likelihood. In our experiments, the non-convexity of the likelihood never proved a practical problem. When we ﬁnd the global minimum, this estimator is optimal and dominates both the backdoor and frontdoor estimators.
7 EXPERIMENTS
Synthetic Data To show that the empirical variance of the various estimators is close to the theoretical variance (Table 1), we randomly initialize parameters and for each instance, we compute the Mean and Standard Deviation of Absolute Percentage Error of theoretical variance as a predictor of empirical variance (see Appendix G). Next, we compare the estimators under different settings of the model parameters. Unless stated otherwise, the model parameter values we use for experiments are a = 10, b = 4, c = 5, d = 5, σu2w = 1, σu2x = 1, σu2m = 1, σu2y = 1.

MSE of estimator MSE of estimator

MSE of estimator MSE of estimator

10 8 6 4 2 0 50

frontdoor backdoor combined
60 70 80 90 100 Number of samples

(a) Backdoor better

25

frontdoor

20

backdoor

combined

15

10

5

0 50

60 70 80 90 100 Number of samples

(b) Frontdoor better

Figure 3: Comparison of MSE when confounders and mediators are observed simultaneously. Either of the backdoor or frontdoor estimators can dominate. The combined estimator is better than both.

2.5

frontdoor

2.0

backdoor

combined

1.5

1.0

0.5

0.0 200 400 600 800 1000 Number of samples per dataset

(a)

4

frontdoor

backdoor

combined 3

2

1

0 200 400 600 800 1000 Number of samples per dataset
(b)

Figure 4: Comparison of MSE when confounders and mediators are observed in separate datasets. The combined estimator dominates in both cases.

The quantity of interest is the causal effect ac = 50. For Figure 3a, we set {σu2x = 0.05, σu2m = 0.05}, which makes the backdoor estimator better as predicted by Eq. 10. For Figure 3b, we set {σu2w = 2, σu2x = 0.01, σu2m = 0.1} which makes the frontdoor estimator better as predicted by Eq. 10. The plots in Figure 3a and 3b corroborate these predictions at different sample sizes. Furthermore, the optimal combined estimator always outperforms both the backdoor and frontdoor estimators.
Next, we evaluate the procedure for combining datasets described in Section 6, generating two datasets with equal numbers of samples. In the ﬁrst, only {X, Y, W } are observed. In the second, only {X, Y, M } are observed. We set {σu2x = 0.05, σu2m = 0.05}, which makes the backdoor estimator better (Figure 4a), and then set {σu2w = 2, σu2x = 0.01, σu2m = 0.1}, which makes the frondoor estimator better (Figure 4b). The plots show that the combined estimator has lower MSE than either for various sample sizes (Figure 4), supporting our theoretical claims.
IHDP Dataset Hill [2011] constructed a dataset from the Infant Health and Development Program (IHDP). This semi-synthetic dataset has been used for benchmarking in causal inference [Shi et al., 2019, Shalit et al., 2017]. The dataset is is based on a randomized experiment to measure the effect of home visits from a specialist on future cognitive test scores of children. The treatment is binary

Table 1: Mean Absolute Percentage Error of the theoretical variance as a predictor of the empirical variance. The values are reported as mean ± std. The % error is small even for small sample sizes.

ESTIMATOR
BACKDOOR FRONTDOOR COMBINED

n = 50
0.36 ± .3 0.33 ± .2 1.20 ± 1.1

n = 100
0.32 ± .2 0.30 ± .2 0.97 ± .6

n = 200
0.34 ± .1 0.23 ± .1 0.58 ± .2

and the covariates contain both continuous and categorical variables representing measurements on the child and their mother. We use samples from the NPCI package [Dorie, 2016]. We converted the randomized data into an observational study by removing a biased subset of the treated group. This set contains 747 samples with 25 covariates.
We use the covariates and the treatment assignment from the real study. We use a procedure similar to Hill [2011] to simulate the mediator and the outcome. The mediator M takes the form M ∼ N (cX, σu2m ), where X is the treatment. The response Y takes the form Y ∼ N (aM + w⊤b, 1) where w is the vector of standardized (zero mean and unit variance) covariates and values in the vector b are randomly sampled (0, 1, 2, 3, 4) with probabilities (0.5, 0.2, 0.15, 0.1, 0.05). The ground truth causal effect is c × a.
We evaluate our estimators and the four IF estimators: IFFulcher (IF-Fulc), IF-Restricted (IF-Restr), IF-Frontdoor (IF-FD), and IF-Rotnitzky (IF-Rotz) (Section 5.2). We test the estimators on two settings of the model parameters (Table 2, the Complete dataset setting). The MSE values are computed across 1000 instantiations of the dataset created by simulating the mediators and outcomes. We ﬁrst evaluate the estimators on the complete dataset of 747 samples. We see that for Setting 1 (S1): a = 10, c = 5, σum = 1, the backdoor estimator dominates the frontdoor estimator whereas for Setting 2 (S2): a = 10, c = 1, σum = 2, the frontdoor estimator is better. In both cases, the combined estimator (Section 5) outperforms both estimators. Furthermore, we see that IF-Restricted outperforms IF-Frontdoor, showing the value of leveraging the covariates. Moreover, IF-Restricted also outperforms IF-Fulcher, suggesting that incorporating the additional model restrictions improves performance. Next, we randomly split the data into two sets, one with the confounder observed and the other with the mediator observed, ﬁnding that the estimator that combines the datasets (Section 6) outperforms the frontdoor and backdoor estimator (Table 2, the Partial dataset setting). We compute the MSE over 1000 realizations of the dataset.
National JTPA Study The National Job Training Partnership Act (JTPA) Study evaluates the effect of a job training program on future earnings. We use the dataset from

Table 2: Results on the IHDP and JTPA datasets. The complete (C) data setting is when {W, X, M, Y } are observed and partial (P) is with {X, M, Y } and {W, X, Y } observed
in two separate datasets.

E S T I M AT O R
BACKDOOR FRONTDOOR COMBINED IF-FD IF-RESTR IF-FULC IF-ROTZ
BACKDOOR FRONTDOOR COMBINED

DATA
C C C C C C C
P P P

IHDP MSE S1 S2
2.14 1.07 1.97 2.81 1.78 0.93 4.24 2.07 3.49 1.48 3.82 1.87 3.58 2.01
5.44 2.43 3.92 4.94 2.97 1.62

JTPA VAR MSE

NA 40.9K 33.1K 46.6K 40.4K 45.1K NA

NA 75.3K 70.1K 77.9K 42.1K 46.2K NA

NA 74.8K 79.5K

NA 115.1K 123.1K

Glynn and Kashin [2019]. The binary treatment X represents if a participant signed up for the program. The outcome Y represents future earnings. The collected covariates (like race, study location, age) are the confounders W . The covariates contain both categorical and continuous variables. There was non-compliance among the treated units. The binary mediator M represents compliance, that is, whether the participant make use of JTPA services after signing up. The study contained a randomized component which allowed us to compute the ground truth treatment effect, which was 862.74. Glynn and Kashin [2018] showed that the backdoor estimator has high bias, suggesting that there was unmeasured confounding, so we omit the backdoor estimator in our results. They also justify the assumptions required for the frontdoor estimator and show that it works well for this study.
A comparison of the frontdoor estimator, the combined estimator (Section 5), IF-Restricted, IF-Frontdoor and IFFulcher (Section 5.2) is shown in Table 2 (the “C” data setting). For IF-Restricted, we only use the f (M |X, W ) = f (M |X) restriction and do not use the E[Y |M, W, X] = E[Y |M, W ] restriction since it is not valid. We compute the variance and MSE using 1000 bootstrap iterations. The combined estimator has lower variance and MSE than the frontdoor estimator. IF-Restricted outperforms IFFrontdoor, reinforcing the utility of combined estimators. Furthermore, IF-Restricted outperforms IF-Fulcher, showing that using model restrictions is valuable. Next, we evaluate our procedure for the partially-observed setting (Section 6). We compute variance and MSE across 1000 bootstrap iterations. At each iteration, we randomly split our dataset into two datasets of equal size, one with revealed confounders, one with revealed mediators. The combined estimator does not outperform the frontdoor estimator (Ta-

ble 2, the “P” data setting). This is expected since the backdoor adjustment works poorly and the revealed-confounder data is unlikely to help. Despite this, the combined estimator does not suffer too badly and has low bias despite the required assumptions for one of the identiﬁcation strategies not holding.
8 DISCUSSION
In this paper, we studied over-identiﬁed graphs with confounders and mediators, showing that the two identiﬁcation strategies can lead to estimators with arbitrarily different variances. We show that having access to both confounders and mediators (either simultaneously or in separate datasets) can give (unbounded) performance gains. We also show that our results qualitatively apply to general nonlinear settings.
Future Work We see several promising lines for future work, including (i) extensions to more general graphs; (ii) online data collection subject to some cost structure over the observations; and (iii) leveraging overidentiﬁcation to mitigate errors due to measurement and confounding. Our experiments show the applicability of our methods in the frontdoor-backdoor graph, with combined estimators yielding gains in both linear and non-linear settings. We expect these insights to extend to other over-identiﬁed settings (e.g. graphs with multiple instrumental variables, multiple confounders, etc.) and we hope next to extend the results to more general over-identiﬁed causal graphs. Additionally, we plan to analyze the online data collection setting. Here, subject to budget constraints, a practitioner must choose which variables to observe at each time step. This direction seems especially important in medical applications (where each test may be costly) and survey studies (with a cap on the number of questions). Our current results suggest that the optimal strategy must depend on the model parameters. At each step, the revealed data will improve our estimates of the model parameters, in turn impacting what we collect in the future.
One potential limitation of the method is that situations where there exist multiple valid identiﬁcation formulas may be uncommon in practice, when ﬁnding a single source of identiﬁcation can already be difﬁcult. However, we believe that in reality, many identiﬁcation approaches are often available, but members of the community ﬁnd ﬂaws in each of the proposed estimators. In these cases, with multiple imperfect estimators of the same causal effect, we believe that overidentiﬁcation might be leveraged to create robust combined estimators.

References
Elias Bareinboim and Judea Pearl. Causal inference and the data-fusion problem. Proceedings of the National Academy of Sciences, 113(27):7345–7352, 2016.
Elias Bareinboim et al. Causal inference and data-fusion in econometrics. Technical report, arXiv. org, 2019.

Lars Peter Hansen. Large sample properties of generalized method of moments estimators. Econometrica, 50(4): 1029–1054, 1982.
Leonard Henckel, Emilija Perkovic´, and Marloes H Maathuis. Graphical criteria for efﬁcient total effect estimation via adjustment in causal linear models. arXiv preprint arXiv:1907.02435, 2019.

Marc F Bellemare and Jeffrey R Bloem. The paper of how: Estimating treatment effects using the front-door criterion. Working Paper, 2019.

Jennifer L Hill. Bayesian nonparametric modeling for causal inference. Journal of Computational and Graphical Statistics, 20(1):217–240, 2011.

Xiaohong Chen and Andres Santos. Overidentiﬁcation in regular models. Econometrica, 86(5):1771–1817, 2018.
Alex Chinco and Christopher Mayer. Misinformed speculators and mispricing in the housing market. The Review of Financial Studies, 29(2):486–522, 2016.
Lauren Cohen and Christopher J Malloy. Friends in high places. American Economic Journal: Economic Policy, 6(3):63–91, 2014.
V. Dorie. Non-parametrics for causal inference. https://github.com/vdorie/npci, 2016.
Morris L. Eaton. Chapter 8: The Wishart Distribution, volume Volume 53 of Lecture Notes–Monograph Series, pages 302–333. Institute of Mathematical Statistics, 2007. doi: 10.1214/lnms/1196285114. URL https://doi.org/10.1214/lnms/1196285114.
Roger Fletcher. Practical methods of optimization. John Wiley & Sons, 2013.
Isabel R Fulcher, Ilya Shpitser, Stella Marealle, and Eric J Tchetgen Tchetgen. Robust inference on population indirect causal effects: the generalized front door criterion. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 2020.
Adam N Glynn and Konstantin Kashin. Front-door difference-in-differences estimators. American Journal of Political Science, 61(4):989–1002, 2017.
Adam N Glynn and Konstantin Kashin. Front-door versus back-door adjustment with unmeasured confounding: Bias formulas for front-door and hybrid adjustments with application to a job training program. Journal of the American Statistical Association, 113(523):1040–1049, 2018.
Adam N. Glynn and Konstantin Kashin. Replication Data for: Front-Door Versus Back-Door Adjustment With Unmeasured Confounding: Bias Formulas for Front-Door and Hybrid Adjustments With Application to a Job Training Program, 2019. URL https://doi.org/10.7910/DVN/G7NNUL.

Guido Imbens. Potential outcome and directed acyclic graph approaches to causality: Relevance for empirical practice in economics. Technical report, National Bureau of Economic Research, 2019.
CH Jackson, NG Best, and Sylvia Richardson. Bayesian graphical models for regression on multiple data sets with different variables. Biostatistics, 10(2):335–351, 2009.
Tjalling C Koopmans and Olav Reiersøl. The identiﬁcation of structural characteristics. The Annals of Mathematical Statistics, 21(2):165–181, 1950.
Jack Kuipers and Giusi Moffa. The variance of causal effect estimators for binary v-structures. arXiv preprint arXiv:2004.09181, 2020.
Manabu Kuroki. Selection of post-treatment variables for estimating total effect from empirical research. Journal of the Japan Statistical Society, 2000.
Judea Pearl. Causal diagrams for empirical research. Biometrika, 82(4):669–688, 1995.
Judea Pearl. Causality. Cambridge university press, 2009.
Judea Pearl. The foundations of causal inference. Sociological Methodology, 40(1):75–149, 2010.
Judea Pearl and Dana Mackenzie. The book of why: the new science of cause and effect. Basic Books, 2018.
Emilija Perkovic´, Johannes Textor, Markus Kalisch, and Marloes H Maathuis. A complete generalized adjustment criterion. In Uncertainty in Artiﬁcial Intelligence (UAI), 2015.
Roland R Ramsahai. Supplementary variables for causal estimation. Wiley Online Library, 2012.
Andrea Rotnitzky and Ezequiel Smucler. Efﬁcient adjustment sets for population average treatment effect estimation in non-parametric causal graphical models. arXiv preprint arXiv:1912.00306, 2019.
John D Sargan. The estimation of economic relationships using instrumental variables. Econometrica: Journal of the Econometric Society, pages 393–415, 1958.

Uri Shalit, Fredrik D Johansson, and David Sontag. Estimating individual treatment effect: generalization bounds and algorithms. In International Conference on Machine Learning (ICML), 2017.
Claudia Shi, David Blei, and Victor Veitch. Adapting neural networks for the estimation of treatment effects. In Advances in Neural Information Processing Systems, pages 2503–2513, 2019.
C Uhler. Gaussian graphical models: An algebraic and geometric perspective. Chapter in Handbook of Graphical Models, 2019.
Sewall Wright. The method of path coefﬁcients. Ann. Math. Statist., 5(3):161–215, 09 1934. doi: 10.1214/ aoms/1177732676.

A BRIEF REVIEW OF OLS REGRESSION

Since we use OLS regression for our results, we brieﬂy review OLS estimators. We consider the following setup:

y = Xβ + e,

where y and e are n × 1 vectors, X is an n × d matrix of observations, and β is the d × 1 coefﬁcient vector that we want to estimate. If e ⊥⊥ X and e ∼ N (0, σe2In), where In is the n × n identity matrix, then the OLS estimate of β is
β = (X⊤X)−1X⊤y = β + (X⊤X)−1X⊤e,

with E[β] = β and Var(β) = σe2E[(X⊤X)−1]. If each row Xi of X is sampled from Xi i.∼i.d. N (0, Σ), then the distribution of (X⊤X)−1 is an Inverse-Wishart distribution. Then the variance of β is

Var(β) = σe2Σ−1 .

(17)

n−d−1

B COVARIANCE OF a AND c

B.1 FRONTDOOR ESTIMATOR

We prove that Cov(af , c) = 0 for the frontdoor estimator. The expressions for af and c are

c = ximi

(18)

x2i

=c+

xium i x2i

af = x2i miyi − ximi xiyi

(19)

x2i m2i − ( ximi)2

=a+

x2i miei − ximi xiei , x2i m2i − ( ximi)2

where ei = − db uxi + uyi . Using the fact the (ux, x) is bivariate normally distributed, we get

E[e|x] =

bσu2x x

d(d2σu2w + σu2x )

= F x,

(20)

where F = bσu2x . The covariance then is
d(d2σu2w +σu2x )

Cov(af , c) = E[(af − a)(c − c)]

= E[E[(af − a)(c − c)|x, m]]

=E E

x2i miei − ximi miei x2i m2i − ( ximi)2

xium i x2i

x, m

=E

x2i miE[ei|x] − mixi xiE[ei|x]

um i xi

(21)

x2i m2i − ( ximi)2

x2i

=E F

x2i mixi − mixi x2i x2i m2i − ( ximi)2

um i xi x2i

= 0,

where in Eq. 21 we used the expression from Eq. 20. Also, in Eq. 21, we took um i out of the conditional expectation because um i is given xi and mi (because um i = mi − cxi).

B.2 COMBINED ESTIMATOR

We prove that Cov(ac, c) = 0 for the combined estimator from Section 5. The expressions for ac and c are

c = ximi

(22)

x2i

=c+

xium i x2i

ac = wi2 miyi − wimi wiyi

(23)

wi2 m2i − ( wimi)2

=a+

wi2 miuyi − wimi wiuyi . wi2 m2i − ( wimi)2

The covariance is

Cov(ac, c) = E[(ac − a)(c − c)]

= E[E[(ac − a)(c − c)|x, m, w]]

=E

wi2 miE[uyi ] − miwi wiE[uyi ]

um i xi

(24)

wi2 m2i − ( wimi)2

x2i

= 0,

where in 24 we used the fact that E[uyi ] = 0.

C UNBIASEDNESS OF THE ESTIMATORS
C.1 BACKDOOR ESTIMATOR
Recall that for the backdoor estimator, we take the coefﬁcient of X in an OLS regression of Y on {X, W }. The outcome yi can be written as
yi = acxi + bwi + aum i + uyi . The error term aum i + uyi is independent of (xi, wi). In this case, the OLS estimator is unbiased. Therefore, E[acbackdoor] = ac.

C.2 FRONTDOOR ESTIMATOR

For the frontdoor estimator, we ﬁrst compute c by taking the coefﬁcient of X in an OLS regression of M on X. The mediator mi can be written as

mi = cxi + um i .

The error term um i is independent of xi. In this case, the OLS estimator is unbiased and hence, E[c] = c.
We then compute af by taking the coefﬁcient of M in an OLS regression of Y on {M, X}. The outcome yi can be written as

yi = ami + db xi − db uxi + uy.

In

this

case,

the

error

term

−

b d

uxi

+

uy

is

correlated

with

xi.

The

expression

for

a

is

given

in

Eq.

19.

The

expectation

E[af

]

is

E[af ] = a + E

x2i miei − ximi xiei x2i m2i − ( ximi)2

=a+E E

x2i miei − ximi xiei x, m x2i m2i − ( ximi)2

= a + E x2i miE[ei|x] − ximi xiE[ei|x]

(25)

x2i m2i − ( ximi)2

=a+E

x2i mi(F xi) − ximi xi(F xi) x2i m2i − ( ximi)2

= a,

where, in Eq. 25, the expression for E[ei|x] is taken from Eq. 20. Using the fact that Cov(af , c) = 0 (see proof in Appendix B.1), we can see that the frontdoor estimator is unbiased as

E[af c] = E[af ]E[c] + Cov(af , c) = ac.

C.3 COMBINED ESTIMATOR
In the combined estimator, the expression for c is the same as for the frontdoor estimator. Therefore, as shown in Appendix C.2, E[c] = c. We compute a by taking the coefﬁcient of M in an OLS regression of Y on {M, W }. The outcome yi can be written as
yi = ami + bwi + uyi . The error term uyi is independent of (mi, wi). In this case, the OLS estimator is unbiased. Therefore, E[ac] = a. Using the fact that Cov(ac, c) = 0 (see proof Appendix B.2), we can see that the combined estimator is unbiased as
E[acc] = E[ac]E[c] + Cov(ac, c) = ac.

D VARIANCE RESULTS FOR THE FRONTDOOR, BACKDOOR, AND COMBINED ESTIMATORS

D.1 BACKDOOR ESTIMATOR

The outcome yi can be written as

yi = acxi + bwi + aum i + uyi .

We estimate the causal effect ac by taking the coefﬁcient on X in an OLS regression of Y on {X, W }. Let Σ = Cov([X, W ]). Using Eq. 17, the ﬁnite sample variance of the backdoor estimator is

Var(aum + uy) Σ−1 1,1

Var(ac)backdoor =

n−3

a2σu2 + σu2

=

m
(n − 3)σ2

y.

ux

OLS estimators are asymptotically normally for arbitrary error distributions (and hence, Gaussianity is not needed). Therefore, the asymptotic variance of the backdoor estimator is

√ lim Var( n(ac − ac))

= Var(aum + uy) Σ−1 = a2σu2m + σu2y .

n→∞

backdoor

1,1

σu2

x

D.2 FRONTDOOR ESTIMATOR

D.2.1 Variance of c

The regression of M on X can be written as mi = cxi + um i . Let Σc = Var(X). Using Eq. 17, Var(c) is

Var(c) = Var(um)(Σ−c 1) =

σu2m

.

n−2

(n − 2)(d2σu2w + σu2x )

D.2.2 Variance of af

The regression of Y

on {M, X} can be written as yi

=

ami +

b d

xi

+

ei,

where

ei

=

−

b d

uxi

+

uyi .

In

this

case,

the

error

ei

is not independent of the regressor xi. Using the fact that (ux, x) has a bivariate normal distribution, Var(e|x) is

b2σu2 σu2 + σu2 (d2σu2 + σu2 )

Var(e|x) =

wx

y

w

(d2σ2 + σ2 )

x

uw

ux

(26)

:= Ve.

Note that Ve is a constant and does not depend on x. From Eqs. 18 and 19, we know that

c=c+ af = a +

xium i x2i
x2i miei − ximi xiei . x2i m2i − ( ximi)2

where ei = − db uxi + uyi . Let

A= C=

x2i miei − ximi xiei

x2i m2i − ( ximi)2

xi

u

m i

.

x2i

First, we derive the expression for Var(af ) as follows,

Var(af ) = Var(a + A)

= Var(A)

= Var(E[A|x, m]) + E[Var(A|x, m)]

= Var E

x2i miei − ximi xiei x, m x2i m2i − ( ximi)2

+ E[Var(A|x, m)]

= Var E x2i miE[ei|x] − ximi xiE[ei|x] + E[Var(A|x, m)]

(27)

x2i m2i − ( ximi)2

= Var E

x2i mi(F xi) − ximi xi(F xi) + E[Var(A|x, m)] x2i m2i − ( ximi)2

= E[Var(A|x, m)]

= E Var

x2i miei − ximi xiei x, m x2i m2i − ( ximi)2

=E ( x2i

1 m2i − (

ximi)2)2 Var

x2i miei − ximi xiei x, m

=E

1

Var(ei|xi) x2

( x2i m2i − ( ximi)2)2

i

x2i m2i − ( ximi)2

= VeE

x2i x2i m2i − ( ximi)2

= VeE [D] ,

(28)

where, in Eq. 27, we used the result from Eq. 20, and D = x2i m2i −x(2i ximi)2 . Using the fact that D has the distribution of a marginal from an inverse Wishart-distributed matrix, that is, if the matrix M ∼ IW(Cov([M, X])−1, n), then D = M1,1, in Eq. 28, we get

Var(af ) = VeE [D]

Cov([M, X])1−,11 = Ve n − 2 − 1

b2σu2 σu2 + σu2 (d2σu2 + σu2 )

=

wx

y

w

(n − 3)(d2σ2 + σ2 )σ2

x,

uw

ux um

where the expression for Ve is taken from Eq. 26.

D.2.3 Covariance of a2f and c2 We prove that Cov(a2f , c2) = Var(af )Var(c). This covariance can be written as

Cov(a2f , c2) = E[(a2f − E[a2f ])(c2 − E[c2])]

= E[(a2f − Var(af ) − E2[af ])(c2 − Var(c) − E2[c])]

= E[a2f c2] − Var(af )Var(c) − a2Var(c) − c2Var(af ) − a2c2.

(29)

We can write E[a2f c2] as

E[a2f c2] = E[(a + A)2(c + C)2]

= E[a2c2 + c2A2 + a2C2 + A2C2 + 2aAC2 + 2cCA2]

= a2c2 + c2Var(a) + a2Var(c) + E[A2C2] + E[2aAC2] + E[2cCA2].

(30)

Substituting the result from Eq. 30 in Eq. 29, we get

Cov(a2f , c2) = E[A2C2] + E[2aAC2] + E[2cCA2].

(31)

Now we expand each term in Eq. 31 separately. E[2aAC2] is

E[2aAC2] = 2aE

xium i 2 x2i

x2i miei − ximi xiei x2i m2i − ( ximi)2

= 2aE E

xium i 2 x2i

x2i miei − ximi xiei x2i m2i − ( ximi)2

x, m

xium i 2

x2i miE[ei|x] − ximi xiE[ei|x]

= 2aE

x2

x2 m2 − ( ximi)2

(32)

i

i

i

= 2aE

xium i 2 x2i

x2i mi(F xi) − ximi xi(F xi) x2i m2i − ( ximi)2

= 0,

(33)

where, in Eq. 32, the expression for E[e|x] is taken from Eq. 20.

Next, we simplify E[2cCA2] as

E[2cC A2 ] = 2cE = 2cE E

xium i x2i
xium i x2i

x2i miei − ximi xiei 2 x2i m2i − ( ximi)2
x2i miei − ximi xiei x2i m2i − ( ximi)2

2
x, m

= 2cE E

xium i ( x2i )2( miei)2 + ( ximi)2( xiei)2 − 2 x2i

x2i

( x2i m2i − ( ximi)2)2

miei

ximi

xiei x, m

= 2cE

xium i x2i

Var(e|x)( x2i )

F 2 2( +

x2i m2i − ( ximi)2

= 2cE

xium i x2i

Var(e|x)( x2i ) x2i m2i − ( ximi)2

= 2cVeE

xium i x2i

x2i x2i m2i − ( ximi)2

= 2cVeE (c − c)

x2i x2i m2i − ( ximi)2

= 2cVe E c

x2i

− cE

x2i m2i − ( ximi)2

x2i

x2i )2( ximi)2 − 2( x2i )2( ( x2i m2i − ( ximi)2)2

x2i m2i − (

ximi)2

ximi)2

= 2cVe (E [cD] − cE [D]) ,

(34)

where D = x2 i
section), we get

m2i −x(2i ximi)2 . Using the fact that c and D are independent of each other (see proof at the end of this

E[cD] = E[c]E[D]

= cE[D].

(35)

Substituting the result from Eq. 35 in Eq. 34, we get

E[2cCA2] = 2cVe (cE [D] − cE [D])

= 0.

(36)

We proceed similarly to Eq. 34 to write E[A2C2] as

E[A2C2] = VeE[C2D].

Then we further simplify E[A2C2] as

E[A2C2] = VeE[C2D]

= VeE[(c − c)2D]

= Ve E c2 E[D] − c2E[D]

= Ve Var(c)E[D] + E2 [c] E[D] − c2E[D]

= VeVar(c)E[D]

(37)

Cov([M, X])−1,11 = VeVar(c) n − 2 − 1

= Ve Var(c)

(38)

(n − 3)σu2m

= Var(af )Var(c),

(39)

where, in Eq. 37, we used the fact that if the matrix M ∼ IW(Cov([M, X])−1, n), then D = M1,1 (that is, D has the distribution of a marginal from an inverse Wishart-distributed matrix), and in Eq. 38, the expression for Ve is taken from
Eq. 26.

Substituting the results from Eqs. 33, 36, and 39 in Eq. 31, we get

Cov(a2f , c2) = Var(af )Var(c).

(40)

Proof that c and D are independent. Let Σ be the following sample covariance matrix:

Σ= 1 n

m2i mixi

mixi
2

.

xi

The distribution of Σ is a Wishart distribution. That is, Σ ∼ W(Cov([M, X]), n). Then (Σ1,1 − Σ1,2Σ−2,12Σ2,1) and (Σ2,1, Σ2,2) are independent [Eaton, 2007, Proposition 8.7]. We can see that

Σ1,1 − Σ1,2Σ−1Σ2,1 = m2i
2,2
= 1. D

x2i − ( x2i

ximi)2

Therefore, we get

1 ⊥⊥ D ∴ D ⊥⊥

x2i , ximi x2i , ximi

∴ D ⊥⊥

ximi x2i

∴ D ⊥⊥ c.

D.2.4 Finite Sample Variance of af c

The variance of the product of two random variables can be written as

Var(af c) = Cov(a2f , c2) + (Var(af ) + E2[af ])(Var(c) + E2[c]) − (Cov(af , c) + E[af ]E[c])2

(41)

= Cov(a2f , c2) + (Var(af ) + a2)(Var(c) + c2) − (Cov(af , c) + ac)2,

where

in

Eq.

41

we

used

the

facts

that

E[af ]

=

a,

and

E[c]

=

c

(see

Appendix

C.2).

Using

the

facts

that

Cov(a

2 f

,

c

2

)

=

Var(af )Var(c) (from Eq. 39) and Cov(af , c) = 0 (from Appendix B.1), we get

Var(af c) = a2Var(c) + c2Var(af ) + 2Var(c)Var(af ).

D.2.5 Asymptotic Variance of af c

Using asymptotic normality of OLS estimators, which does not require Gaussianity, we have

√ n

af − a

→d N 0, lim

Var√∞(af )

Cov(√naf , c)

c

c

n→∞ Cov( naf , c) Var∞(c)

√ ∴n

af − a

→d N 0, Var∞(af )

0

,

c

c

0

Var∞(c)

where Var∞(af ) and Var∞(c) are the asymptotic variances of af and c, respectively. The expressions for asymptotic variances are

Var (a ) = V Cov([M, X ])−1 = b2σu2w σu2x + σu2y (d2σu2w + σu2x )

∞f

e

1,1 (d2σu2w + σu2x )σu2m

Var∞(c) = Var(um)(Σ−1) = σu2m . c d2σu2w + σu2x

In order to compute the asymptotic variance of af c, we use the Delta method:

√

d

n(af c − ac) → N

0, c

a

Var∞(af )

0

c

0

Var∞(c) a

∴

√

d

n(af c − ac) → N

0, c2Var∞(af ) + a2Var∞(c)

.

D.3 COMBINED ESTIMATOR

D.3.1 Finite sample variance of ac

We can write the regression of Y on {M, W } as yi = ami + bwi + uyi . Let Σac = Cov([M, W ]). Using Eq. 17, we get

Var(uyi )(Σ−a 1)1,1

σu2y

Var(ac) =

c
n−3

= (n − 3)(c2σ2 + σ2 ) .

ux

um

D.3.2 Bounding the ﬁnite sample variance

We ﬁrst compute the lower bound of the combined estimator. Since the estimator is unbiased (see Appendix B.2), we can apply the Cramer-Rao theorem to lower bound the ﬁnite sample variance.

Let the vector si = [xi, yi, wi, mi] denote the ith sample. Since the data is multivariate Gaussian, the log-likelihood of the data is
LL = − n log (det Σ) + Tr (ΣΣ−1) , 2

where Σ

=

Cov([X, Y, W, M ]) and Σ

=

1 n

n i=1

sisi⊤.

Let

e

=

ac

and

e

=

acc.

Since

we

want

to

lower

bound

the

variance of e, we reparameterize the log-likelihood by replacing c with e/a to simplify calculations. Next, we compute the

Fisher Information Matrix for the eight model parameters:

 ∂2LL

∂2LL

∂2LL . . . ∂2LL 



∂e2
2

∂e∂a
2

∂e∂b
2

∂e∂σuy
2





∂ LL ∂a∂e

∂ LL ∂a2

∂ LL ∂a∂b

...

∂ LL ∂a∂σu



I = −E  .. .

...

...

...

.. y  . .

∂ 2 LL ∂σuy ∂e

∂∂σ2uLy L∂a ∂∂σ2uLyL∂b . . .

∂ 2 LL ∂ 2 σuy

Therefore, using the Cramer-Rao theorem, we have

Var(e) = Var(acc)

≥ (I−1)1,1

1 c2σu2y

a2σu2

=n

c2σ2 + σ2

+ d2σ2

m
+ σ2

.

ux

um

uw

ux

Next, we compute a ﬁnite sample upper bound for Cov(a2c , c2). We derive this in a similar manner as the frontdoor estimator in Appendix D.2.3. From Eqs. 22 and 23, we know that

ac = a + c=c+

wi2 miuyi − wimi wiuyi

wi2 m2i − ( wimi)2

xi

u

m i

.

x2i

Let

A= C=

wi2 miuyi − wimi wiuyi

wi2 m2i − ( wimi)2

xi

u

m i

.

x2i

Then, similarly to Eq. 31, we get

Cov(a2c , c2) = E[A2C2] + E[2aAC2] + E[2cCA2].

(42)

Now we simplify each term in Eq. 42 separately. E[2aAC2] can be simpliﬁed as

E[2aAC2] = 2aE

xium i 2 x2i

wi2 miuyi − wimi wiuyi wi2 m2i − ( wimi)2

=E E

xium i 2 x2i

wi2 miuyi − wimi wiuyi wi2 m2i − ( wimi)2

x, m, w

=E

xium i 2

wi2 miE[uyi ] − wimi wiE[uyi ]

(43)

x2i

wi2 m2i − ( wimi)2

= 0,

(44)

where, in Eq. 43, we used the fact that E[uy] = 0. Next, we simplify E[2cCA2] as

E[2cCA2] = 2cE

xium i x2i

wi2 miuyi − wimi wiuyi 2 wi2 m2i − ( wimi)2

= 2cE E

xium i x2i

wi2 miuyi − wimi wiuyi wi2 m2i − ( wimi)2

2
x, m, w

= 2cE

xium i x2i

Var(uy)

wi2 wi2 m2i − ( wimi)2

= 2cσu2y E [CD] , (45)

where D =

wi2

. We can upper bound the expression in Eq. 45 as

wi2 m2i −( wimi)2

E[2cCA2] = 2cσu2y E [CD]

≤ 2|c|σu2y E [CD] (46)

≤ 2|c|σu2y E[C2]E[D2]

= 2|c|σu2y Var(c)(Var(D) + E[D]2) (47)

= 2|c|σu2y

 Var(c)  2 (Cov([M, W ]))−1,11 2 +
(n − 2 − 1)2(n − 2 − 3)



(Cov([M, W ]))−1,11

2


n−2−1

= 2|c|σu2y

Var(c)

2

1

(n − 3)2(n − 5)(c2σ2 + σ2 )2 + (n − 3)2(c2σ2 + σ2 )2

ux

um

ux

um

σu2

=

2|c|

(n

−

y
3)(c2σ2

+ σ2

)

ux

um

Var(c)

n−3 n−5

= 2|c|Var(a) Var(c) n − 3 ,

(48)

n−5

where, in Eq. 46, we used the Cauchy–Schwarz inequality, and in Eq. 47, we used the fact that if the matrix M ∼ IW(Cov([M, W ])−1, n), then D = M1,1 (that is, D has the distribution of a marginal from an inverse Wishart-distributed matrix).
Similarly to Eq. 45, we simplify E[A2C2] as

E[A2C2] = σu2y E[C2D]. (49)

The expression in Eq. 49 can be upper bounded using the Cauchy-Schwarz inequality as

E[A2C2] = σu2y E[C2D]

≤ σu2y E[C4]E[D2]

= σu2y E[C4](Var(D) + E2[D])

= σu2y

 E[C4]  2 (Cov([M, W ]))−1,11 2 +
(n − 2 − 1)2(n − 2 − 3)



(Cov([M, W ]))−1,11

2


n−2−1

σu2

=

(n

−

y
3)(c2σ2

+ σ2

)

ux

um

n−3 n−5

E[C 4 ]

= Var(ac) n − 3 E[C4].

(50)

n−5

We can simplify E[C4] as follows,

E[C4] = E =E E

xium i 4 x2i

xi um i

4
x

x2i

=E (
=E (

1E x2i )4 1 x2)4 Var
i

4
xium i x
2
xium i x + E

22
xium i x

=E (
=E (
=E (

1 x2i )4
1 x2i )4
1 x2i )4

2
xium i x + σu4

2
x2i

Var m

Var σu2m

x2 ( xium i )2 x i σu2m x2i

+ σu4m

2
x2i

σu4m

2
x2i Var

( xium i )2 x σu2m x2i

+ σu4m

2
x2i

(51)

=E

1

σ4

( x2i )4 um

2
x2i 2 + σu4 m

2
x2i

=E (

1 x2i )4

3σu4m

2
x2i

= 3σu4m E (

1 x2i )2

= 3σ4 Var

1

12 +E

(52)

um

x2i

x2i

= 3σu4m

2

1

(n − 2)2(n − 4)(d2σ2 + σ2 )2 + (n − 2)2(d2σ2 + σ2 )2

uw

ux

uw

ux

σu4

=

3

(n

−

m
2)2(d2σ2

+ σ2 )2

uw

ux

n−2 n−4

= 3 Var(c2) 2 n − 2 ,

(53)

n−4

where, in Eq. 51, we used the fact that ( xium i )2 x has a Chi-squared distribution, that is, ( xium i )2 x ∼

σu2 m x2i

σu2 m x2i

χ2(1), and in Eq. 52, we used the fact that Scale-inv-χ2 n, (d2σu2wn+σu2x )2 .

1
2

has

a

scaled

inverse

Chi-squared

distribution, that

is,

xi

1
2

∼

xi

Substituting the result from Eq. 53 in Eq. 50, we get

E[A2C2] ≤ Var(ac)Var(c) 3(n − 3)(n − 2) .

(54)

(n − 5)(n − 4)

Substituting the results from Eqs. 44, 48, and 54 in Eq. 42, we get

Cov(a2c , c2) ≤

n−3

√ n−2

n − 5 2|c|Var(ac) Var(c) + 3 n − 4 Var(ac)Var(c) .

The variance of the product of two random variables can be written as

Var(acc) = Cov(a2c , c2) + (Var(ac) + E2[ac])(Var(c) + E2[c]) − (Cov(ac, c) + E[ac]E[c])2 = Cov(a2c , c2) + (Var(ac) + a2)(Var(c) + c2) − (Cov(ac, c) + ac)2,

where we used the facts that E[ac] = a, and E[c] = c (see Appendix C.3). Using the fact that Cov(ac, c) = 0 (see Appendix B.2) and the upper bound for Cov(a2c, c2), we get

Var(acc) ≤ c2Var(ac) + a2Var(c) +

n−3

√ n−2

n − 5 2|c|Var(ac) Var(c) + 3 n − 4 Var(ac)Var(c) .

D.3.3 Asymptotic variance

Using asymptotic normality of OLS estimators, which does not require Gaussianity, we have

√ n

ac − a

→d N 0, lim

Var√∞(ac)

Cov(√nac, c)

c

c

n→∞ Cov( nac, c) Var∞(c)

√ ∴n

ac − a

→d N 0, Var∞(ac)

0

,

c

c

0

Var∞(c)

where Var∞(ac) and Var∞(c) are the asymptotic variances of ac and c, respectively. The expressions for the asymptotic variances are

Var (a ) = Var(uy)(Σ−1) =

σu2y

∞c

i ac 1,1 c2σu2x + σu2m

Var∞(c) = Var(um)(Σ−1) = σu2m . c d2σu2w + σu2x

In order to compute the asymptotic variance of acc, we use the Delta method:

√

d

n(acc − ac) → N

0, c

a

Var∞(ac)

0

c

0

Var∞(c) a

√

d

=⇒ n(acc − ac) → N

0, c2Var∞(ac) + a2Var∞(c)

.

E COMPARISON OF COMBINED ESTIMATOR WITH BACKDOOR AND FRONTDOOR ESTIMATORS

In this section, we provide more details on the comparison of the combined estimator presented in 5 to the backdoor and frontdoor estimators.

E.1 COMPARISON WITH THE BACKDOOR ESTIMATOR

In Section 5.1, we made the claim that

∃N, s.t., ∀n > N, Var(acc) ≤ Var(ac)backdoor.

In this case, by comparing Eqs. 5 and 13, we have

2 σu4x F + d2σu2w (σu2m D + σu2x F ) + c2σu6x

√ c2σu6x D2(F + 2 3σu2m )

N=

σ2 D2

,

um

√ where D = d2σu2 + σu2 , E = c2σu2 + σu2 , and F = E + (1 + 2 3)σu2 . Thus, for a large enough n, the combined

w

x

x

m

m

estimator has lower variance than the backdoor estimator for all model parameter values.

E.2 COMPARISON WITH THE FRONTDOOR ESTIMATOR

In Section 5.1, we made the claim that

∃N, s.t., ∀n > N, Var(acc) ≤ Var(af c).

In this case, by comparing Eqs. 8 and 13, we have

√ 2 σu6 + 2 3c2σu4 σu2 − c4σu2 σu4 +

m

mx

mx

D + σu4m

√ σu4m + 4 3c2σu2m σu2x − 2c4σu4x

N=

D2

,

where D = c6σu6x. Thus, for a large enough n, the combined estimator has lower variance than the frontdoor estimator for all model parameter values.

E.3 COMBINED ESTIMATOR DOMINATES THE BETTER OF BACKDOOR AND FRONTDOOR

In this section, we provide more details for the claim in Section 5.1 that the combined estimator can dominate the better of the backdoor and frontdoor estimators by an arbitrary amount. We show that the quantity
R = min {Var(ac)backdoor, Var(af c)} Var(acc)
is unbounded.

We do this by considering the case when Var(ac)backdoor = Var(af c). Note that

Var(ac)backdoor = Var(af c)

−D(−a2σu4 ((n − 2)d2σu2 + σu2 ) + (−(n − 2)d2σu2 (σu2 − c2σu2 ) + σu2 E)σu2 )

=⇒ b =

m

w

x

w

m

σ2 σ4 (2σ2 + (n − 2)c2D)

x

x

y,

(55)

uw ux

um

where D = d2σu2w + σu2x , and E = (n − 2)c2σu2x − (n − 4)σu2m . Hence, if the parameter b is set to the value given in Eq. 55, the backdoor and frontdoor estimators will have equal variance. We have to ensure that the value of b is real. b will be
a real number if

|c| ≤ σum σux
n > 2.

1 − 2σu2x , and (n − 2)D

For the value of b in Eq. 55, the quantity R becomes

R = Var(ac)backdoor Var(acc)

(n − 2)DE(a2σu2 + σu2 )

≥

m

y

√

,

σu2x (n − 3)a2σu2m E + σu2y σu2m + 3σu2m r1r2 + |c|(n − 2)D |c| + r1 √(σnu−m2)D

where D = d2σu2w + σu2x , E = c2σu2x + σu2m , r1 =

n−3 n−5

and

r2

=

nn−−24 .

R does not depend on the parameter b. It is possible to set the other model parameters in a way that allows R to take any positive value. In particular, it can be seen that as σux → 0, R → ∞, which shows that R is unbounded.

F COMBINING PARTIALLY OBSERVED DATASETS

F.1 CRAMER-RAO LOWER BOUND

We are interested in estimating the value of the product ac. Let e = ac. We reparameterize the likelihood in Eq. 16 by replacing c with e/a. This simpliﬁes the calculations and improves numerical stability. Now, we have the following eight unknown model parameters: {e, a, b, d, σu2w , σu2x , σu2m , σu2y }.
In order to compute the variance of the estimate of parameter e = ac, we compute the Cramer-Rao variance lower bound. We ﬁrst compute the Fisher information matrix (FIM) I for the eight model parameters:

 ∂2LL

∂ 2 LL

∂2LL . . . ∂2LL 



∂e2
2

∂e∂a
2

∂e∂b
2

∂e∂σuy
2





∂ LL ∂a∂e

∂ LL ∂a2

∂ LL ∂a∂b

...

∂ LL ∂a∂σu



I = −E  .. .

...

...

...

.. y  .

∂2LL ∂σuy ∂e

∂∂σ2uLy L∂a ∂∂σ2uLyL∂b . . .

∂2LL ∂ 2 σuy

Let e be the MLE. Since standard regularity conditions hold for our model (due to linearity and Gaussianity), the MLE is asymptotically normal. We can use the Cramer-Rao theorem to get the asymptotic variance of e. That is, for constant k, as N → ∞, we have

√

d

N (e − e) → N (0, Ve), and

Ve = (I−1)1,1.

Below,

we

present

the

closed

form

expression

for

Ve.

Let

Ve

=

X Y

.

Then

X =(a2σu2m + σu2y )(−a8d2(k − 1)σu2w (σu2m )5(d2σu2w + σu2x )2 + a6(σu2m )3(d2σu2w + σu2x ) (b2σu2w σu2x (c2d4(σu2w )2 + d2σu2w (c2(k + 1)σu2x + (−2k2 + 2k + 1)σu2m ) + σu2x (c2kσu2x + σu2m )) + σu2y (d2σu2w + σu2x )(c2d4(σu2w )2 + d2σu2w (c2(k + 1)σu2x + (3 − 2k)σu2m )+ kσu2x (c2σu2x + σu2m ))) − 4a5bcd(k − 1)kσu2w σu2x (σu2m )3(d2σu2w + σu2x ) (b2σu2w σu2x + σu2y (d2σu2w + σu2x )) + a4(σu2m )2(b4(σu2w )2(σu2x )2(c2d4(σu2w )2+ d2σu2w (2c2(−k2 + k + 1)σu2x + (k + 1)σu2m ) + σu2x (c2(−2k2 + 2k + 1)σu2x + 2σu2m ))− b2σu2w σu2x σu2y (d2σu2w + σu2x )(2c2d4(k − 2)(σu2w )2 + d2σu2w (c2(4k2 − 3k − 5)σu2x + 2 (k2 − 2k − 1)σu2m ) + σu2x (c2(4k2 − 5k − 1)σu2x − 2(k + 1)σu2m )) − (σu2y )2(d2σu2w + σu2x )2 (c2d4(2k − 3)(σu2w )2 + d2σu2w (c2(2k2 − k − 3)σu2x + (k − 3)σu2m ) + kσu2x (c2(2k − 3)σu2x − 2σu2m ))) − 4a3bcd(k − 1)kσu2w σu2x (σu2m )2σu2y (d2σu2w + σu2x )(b2σu2w σu2x + σu2y (d2σu2w + σu2x ))+ a2σu2m (b2σu2w σu2x + σu2y (d2σu2w + σu2x ))(b4(σu2w )2(σu2x )2(c2(d2σu2w − (k − 2)σu2x ) + σu2m )+ b2σu2w σu2x σu2y (2c2d4(σu2w )2 + 2d2σu2w (c2(−k2)σu2x + k(c2σu2x + σu2m ) + 2c2σu2x ) + σu2x (2c2 (−k2 + k + 1)σu2x + (k + 1)σu2m )) + (σu2y )2(d2σu2w + σu2x )(d2σu2w + kσu2x )(σu2m − c2(2k − 3) (d2σu2w + σu2x ))) + c2(b2σu2w σu2x + σu2y (d2σu2w + σu2x ))2(b4(σu2w )2(σu2x )2 + b2σu2w σu2x σu2y (2d2kσu2w + kσu2x + σu2x ) + (σu2y )2(d2σu2w + σu2x )(d2σu2w + kσu2x ))),

Asymptotic variance Asymptotic variance Asymptotic variance

65

64

63

62
0.0

Optimal k
0.2 0.4 0.6 0.8 1.0
Fraction of confounders: k = P / N
(a)

65.0

Optimal k

64.5

64.0

63.5

63.0
0.0 0.2 0.4 0.6 0.8 1.0
Fraction of confounders: k = P / N
(b)

69 68 67 66 65 Optimal k
0.0 0.2 0.4 0.6 0.8 1.0 Fraction of confounders: k = P / N
(c)

Figure 5: Cases where collecting a mix of confounders and mediators is better than collecting only confounders or mediators.

and

Y =(a2σu2m (d2σu2w + σu2x ) + b2σu2w σu2x + σu2y (d2σu2w + σu2x ))(a6(−d2)(k − 1) σu2w (σu2m )4(d2σu2w + σu2x )2 + a4(σu2m )2(d2σu2w + σu2x )(b2σu2w σu2x (d2kσu2w (c2σu2x − 2kσu2m + 2σu2m ) + σu2x (c2kσu2x + σu2m )) + σu2y (d2σu2w + σu2x )(d2σu2w (c2kσu2x − 3kσu2m + 3σu2m ) + kσu2x (c2σu2x + σu2m ))) − 4a3bcd(k − 1)kσu2w σu2x (σu2m )2 (d2σu2w + σu2x )(b2σu2w σu2x + σu2y (d2σu2w + σu2x )) + a2σu2m (b4(σu2w )2(σu2x )2(σu2x (σu2m − 2c2(k − 1)kσu2x ) − d2(k − 1)σu2w (2c2kσu2x + σu2m )) + 2b2σu2w σu2x σu2y (d2σu2w + σu2x ) (σu2x (σu2m − 2c2(k − 1)kσu2x ) − 2d2(k − 1)kσu2w (c2σu2x + σu2m )) + (σu2y )2(d2σu2w + σu2x )2 (−d2(k − 1)σu2w (2c2kσu2x + 3σu2m ) − kσu2x (2c2(k − 1)σu2x + (k − 2)σu2m ))) − 4abcd(k − 1) kσu2w σu2x σu2m σu2y (d2σu2w + σu2x )(b2σu2w σu2x + σu2y (d2σu2w + σu2x )) + b6c2k(σu2w )3(σu2x )4 + b4 (σu2w )2(σu2x )2σu2y (d2σu2w + σu2x )(3c2kσu2x − kσu2m + σu2m ) + b2σu2w σu2x (σu2y )2(d2σu2w + σu2x ) (d2kσu2w (3c2σu2x − 2(k − 1)σu2m ) + σu2x (3c2kσu2x − k2σu2m + σu2m )) + (σu2y )3(d2σu2w + σu2x )2 (d2σu2w (c2kσu2x − kσu2m + σu2m ) + kσu2x (c2σu2x − kσu2m + σu2m ))),

where

k

=

P N

.

F.2 COMPARISON WITH FRONTDOOR AND BACKDOOR ESTIMATORS

In this section, we show some examples of regimes where the combining partially observed datasets results in lower

variance than applying either of the backdoor or frontdoor estimator even when the total number of samples are the same.

In other words, there exist settings of model parameters such that, for some k ∈ (0, 1), we have

√

√

Ve ≤ Var( N ac)backdoor, and Ve ≤ Var( N af c).

Figure 5 shows three examples where the optimal value of k is between 0 and 1. We plot the variance as predicted by the expression for Ve versus the value of k. The plots show that in some cases, it is better to collect a mix of confounders and mediators rather than only mediators or only confounders. The expression for Ve in the previous section allows us to verify that. This happens when the variance of the frontdoor and backdoor estimators do not differ by too much.
In Figure 5a, the model parameters are {a = 10, b = 3.7, c = 5, d = 5, σu2w = 1, σu2x = 1, σu2m = 0.64, σu2y = 1}. In this case, the variance of the frontdoor estimator is lower than the backdoor estimator. Despite this, it is not optimal to only collect mediators. The optimal value of k is 0.303, that is, 30% of the collected samples should be confounders and the rest should be mediators to achieve lowest variance.
In Figure 5b, the model parameters are {a = 10, b = 3.955, c = 5, d = 5, σu2w = 1, σu2x = 1, σu2m = 0.64, σu2y = 1}. In this case, the variance of the frontdoor estimator is almost equal to that of the backdoor estimator. The optimal ratio k is 0.505, that is, we should collect the same of amount of confounders as mediators.

In Figure 5c, the model parameters are {a = 10, b = 4.3, c = 5, d = 5, σu2w = 1, σu2x = 1, σu2m = 0.64, σu2y = 1}. In this case, the variance of the frontdoor estimator is greater than the backdoor estimator. The optimal ratio k is 0.735, that is, we
should collect the more confounders than mediators.

F.3 PARAMETER INITIALIZATION FOR FINDING THE MLE
The likelihood in Eq. 16 is non-convex. As a result, we cannot start with arbitrary initial values for model parameters because we might encounter a local minimum. To avoid this, we use the two datasets to initialize our parameter estimates. Each of the eight parameters can be identiﬁed using only data from one of the datasets. For example, d can be initialized using the revealed-confounder dataset (via OLS regression of X on W ). The parameter e is can be identiﬁed using either dataset, so we pick the value with lower bootstrapped variance.
After initializing the eight model parameters, we run the Broyden–Fletcher–Goldfarb–Shanno (BFGS) algorithm [Fletcher, 2013] to ﬁnd model parameters that minimize the negative log-likelihood.

G MORE DETAILS ON EXPERIMENTS

Here we provide more details for how results in Table 1 are generated. We initialize the model parameters by sampling 200 times from the following distributions:

a, b, c, d ∼ Unif[−10, 10]

σ2 , σ2 , σ2 , σ2 ∼ Unif[0.01, 2].

(56)

uw ux um uy

For each initialization, we compute the Mean Absolute Percentage Error (MAPE) of the theoretical variance as a predictor of empirical variance:

MAPE = |Vartheoretical − Varempirical| ∗ 100% Varempirical

We report the mean and standard deviation of the MAPE across 1000 realizations of datasets sampled from Eq. 56. We ﬁnd that the theoretical variance is close to the empirical variance even for small sample sizes (Table 1).

