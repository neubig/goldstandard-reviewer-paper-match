ANALYSIS OF MULTILINGUAL SEQUENCE-TO-SEQUENCE SPEECH RECOGNITION SYSTEMS

Martin Karaﬁa´t1, Murali Karthick Baskar1, Shinji Watanabe2, Takaaki Hori3 Matthew Wiesner2 and Jan “Honza” Cˇ ernocky´1
Brno University of Technology1, John Hopkins University2 Mitsubishi Electric Research Laboratories (MERL)3

arXiv:1811.03451v1 [eess.AS] 7 Nov 2018

ABSTRACT
This paper investigates the applications of various multilingual approaches developed in conventional hidden Markov model (HMM) systems to sequence-to-sequence (seq2seq) automatic speech recognition (ASR). On a set composed of Babel data, we ﬁrst show the effectiveness of multi-lingual training with stacked bottle-neck (SBN) features. Then we explore various architectures and training strategies of multi-lingual seq2seq models based on CTC-attention networks including combinations of output layer, CTC and/or attention component re-training. We also investigate the effectiveness of language-transfer learning in a very low resource scenario when the target language is not included in the original multi-lingual training data. Interestingly, we found multilingual features superior to multilingual models, and this ﬁnding suggests that we can efﬁciently combine the beneﬁts of the HMM system with the seq2seq system through these multilingual feature techniques.
Index Terms— sequence-to-sequence, CTC, multilingual training, language-transfer, ASR.
1. INTRODUCTION
The sequence-to-sequence (seq2seq) model proposed in [1, 2, 3] is a neural network (NN) architecture for performing sequence classiﬁcation. Later, it was also adopted to perform speech recognition [4, 5, 6]. The model allows to integrate the main blocks of ASR (acoustic model, alignment model and language model) into a single neural network architecture. The recent ASR advancements in connectionist temporal classiﬁcation (CTC) [6, 5] and attention [4, 7] based approaches have generated signiﬁcant interest in speech community to use seq2seq models. However, outperforming conventional hybrid RNN/DNN-HMM models with seq2seq requires a huge amount of data [8]. Intuitively, this is due to the range of roles this model needs to perform: alignment and language modeling along with acoustic to character label mapping.
The work reported here was carried out during the 2018 Jelinek Memorial Summer Workshop on Speech and Language Technologies, supported by Johns Hopkins University via gifts from Microsoft, Amazon, Google, Facebook, and MERL/Mitsubishi Electric. It was also supported by Czech Ministry of Education, Youth and Sports from the National Programme of Sustainability (NPU II) project ”IT4Innovations excellence in science - LQ1602” and by the Ofﬁce of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA) MATERIAL program, via Air Force Research Laboratory (AFRL) contract # FA8650-17-C-9118. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the ofﬁcial policies, either expressed or implied, of ODNI, IARPA, AFRL or the U.S. Government.

Multilingual approaches have been used in hybrid RNN/DNNHMM systems for tackling the problem of low-resource data. These include language adaptive training and shared layer retraining [9]. Parameter sharing investigated in our previous work [10] seems to be the most beneﬁcial.
Existing multilingual approaches for seq2seq modeling mainly focus on CTC. A multilingual CTC proposed in [11] uses a universal phone set, FST decoder and language model. The authors also use a linear hidden unit contribution (LHUC) [12] technique to rescale the hidden unit outputs for each language as a way to adapt to a particular language. Another work [13] on multilingual CTC shows the importance of language adaptive vectors as auxiliary input to the encoder in multilingual CTC model. The decoder used here is based on simple greedy search of applying argmax on every time frame. An extensive analysis of multilingual CTC performance with limited data is performed in [14]. Here, the authors use a word level FST decoder integrated with CTC during decoding.
On a similar front, attention models are explored within a multilingual setup in [15, 16], where an attempt was made to build an attention-based seq2seq model from multiple languages. The data is just pulled together assuming the target languages are seen during the training. Although our prior study [17] performs a preliminary investigation of transfer learning techniques to address the unseen languages during training, this is not an intensive study of covering various multi-lingual techniques.
In this paper, we explore the multilingual training approaches [18, 19] in hybrid RNN/DNN-HMMs and we incorporate them into the seq2seq models. In our recent work [19], we showed the multilingual acoustic models (BLSTM particularly) to be superior to multilingual acoustic features in RNN/DNN-HMM systems. Consequently, similar experiments are performed in this paper on a sequence-to-sequence scheme.
The main motivation and contribution behind this work is as follows:
• To incorporate the existing multilingual approaches in a joint CTC-attention [20] framework.
• To compare various multilingual approaches: multilingual features, model architectures, and transfer learning.
2. SEQUENCE-TO-SEQUENCE MODEL
In this work, we use the attention based approach [2] as it provides an effective methodology to perform sequence-to-sequence training. Considering the limitations of attention in performing monotonic alignment [21, 22], we choose to use CTC loss function to aid the attention mechanism in both training and decoding.

Let X = (xt|t = 1, . . . , T ) be a T -length speech feature sequence and C = (cl|l = 1, . . . , L) be an L-length grapheme sequence. A multi-objective learning framework Lmol proposed in [20] is used in this work to unify attention loss patt(C|X) and CTC loss pctc(C|X) with a linear interpolation weight λ, as follows:
Lmod = λ log pctc(C|X) + (1 − λ) log p∗att(C|X). (1)

The uniﬁed model beneﬁts from both effective sequence level training and the monotonic afforded by the CTC loss.
patt (C|X) represents the posterior probability of character label sequence C w.r.t input sequence X based on the attention approach, which is decomposed with the probabilistic chain rule, as follows:

L

p∗att (C|X) ≈ p (cl|c∗1, ...., c∗l−1, X) ,

(2)

l=1

where c∗l denotes the ground truth history. Detailed explanation of the attention mechanism is given later.
Similarly, pctc (C|X) represents the CTC posterior probability:

pctc (C|X) ≈

p(Z |X ),

(3)

Z ∈Z (C )

where Z = (zt|t = 1, . . . , T ) is a CTC state sequence composed of the original grapheme set and the additional blank symbol. Z(C) is a set of all possible sequences given the character sequence C.
The following paragraphs explain the encoder, attention decoder, CTC, and joint decoding used in our approach.

Encoder

In our approach, both CTC and attention use the same encoder func-

tion:

ht = Encoder(X),

(4)

where ht is an encoder output state at t. As Encoder(·), we use bidirectional LSTM (BLSTM).

Attention Decoder:

Location-aware attention mechanism [23] is used in this work. The output of location-aware attention is:

alt = LocationAttention {al−1,t }Tt =1 , ql−1, ht . (5)

Here, alt acts as attention weight, ql−1 denotes the decoder hidden state, and ht is the encoder output state deﬁned in (4). The locationattention function is given by a convolution and maps the attention weight of the previous label al−1 to a multi channel view ft for better representation:

ft = K ∗ al−1,

(6)

elt = gT tanh(Lin(ql−1) + Lin(ht) + LinB(ft)), (7)

alt = Softmax({elt}Tt=1)

(8)

Here, (7) provides the unnormalized attention vectors computed with the learnable vector g, linear transformation Lin(·), and afﬁne transformation LinB(·). Normalized attention weight are obtained in (8) by a standard Softmax(·) operation. Finally, the context vector rl is obtained as a weighted sum of the encoder output states ht over all frames, with the attention weight:

T

rl = altht.

(9)

t=1

Usage Train Target

Language
Cantonese Bengali Pashto Turkish
Vietnamese Haitian Tamil Kurdish Tokpisin Georgian
Assamese Swahili

Train # spkrs. # hours

952

126.73

720

55.18

959

70.26

963

68.98

954

78.62

724

60.11

724

62.11

502

37.69

482

35.32

490

45.35

720

54.35

491

40.0

Eval # spkrs. # hours

120

17.71

121

9.79

121

9.95

121

9.76

120

10.9

120

10.63

121

11.61

120

10.21

120

9.88

120

12.30

120

9.58

120

10.58

# of characters
3302 66 49 66 131 60 49 64 55 35 66 56

Table 1. Details of the BABEL data used for experiments.

The decoder function is an LSTM layer which decodes the next character output label cl from their previous label cl−1, hidden state of the decoder ql−1 and attention output rl:
p (cl|c1, ...., cl−1, X) = Decoder(rl, ql−1, cl−1) (10)
This equation is incrementally applied to form p∗att in (2).
Connectionist temporal classiﬁcation (CTC):
Unlike the attention approach, CTC does not use any speciﬁc decoder network. Instead, it invokes two important components to perform character level training and decoding: the ﬁrst one is an RNNbased encoding module p(Z|X). The second component contains a language model and state transition module. The CTC formalism is a special case [6] of hybrid DNN-HMM framework with the Bayes rule applied to obtain p(C|X).

Joint decoding:
Once we have both CTC and attention-based seq2seq models trained, both are jointly used for decoding as below:

log phyp(cl|c1, ...., cl−1, X) =

α log pctc(cl|c1, ...., cl−1, X)

(11)

+(1 − α) log patt(cl|c1, ...., cl−1, X)

Here log phyp is a ﬁnal score used during beam search. α controls the weight between attention and CTC models. α and multi-task learning weight λ in (1) are set differently in our experiments.

3. DATA
The experiments are conducted using the BABEL speech corpus collected during the IARPA Babel program. The corpus is mainly composed of conversational telephone speech (CTS) but some scripted recordings and far ﬁeld recordings are present as well. Table 1 presents the details of the languages used for training and evaluation in this work. We decided to evaluate also on training languages to see effect of multilingual training on training languages. Therefore, Tok Pisin, Georgian from “train” languages and Assamese, Swahili from “target” languages are taken for evaluation.

4. SEQUENCE TO SEQUENCE MODEL SETUP
The baseline systems are built on 80-dimensional Mel-ﬁlter bank (fbank) features extracted from the speech samples using a sliding window of size 25 ms with 10ms stride. KALDI toolkit [24] is used to perform the feature processing. The “fbank” features are then fed to a seq2seq model with the following conﬁguration:
The Bi-RNN [25] models mentioned above uses an LSTM [26] cell followed by a projection layer (BLSTMP). In our experiments below, we use only a character-level seq2seq model based on CTC and attention, as mentioned above. Thus, in the following experiments, we will use character error rate (% CER) as a suitable measure to analyze the model performance. All models are trained in ESPnet, end-to-end speech processing toolkit [27].

5. MULTILINGUAL FEATURES
Multilingual features are trained separately from seq2seq model according to a setup from our previous RNN/DNN-HMM work [19]. It allows us to easily combine traditional DNN techniques with the seq2seq model such as GMM based alignments for NN target estimation, phoneme units and frame-level randomization. Multilingual features incorporate additional knowledge from non-target languages into features which could better guide the seq2seq model.
5.1. Stacked Bottle-Neck feature extraction
The original idea of Stacked Bottle-Neck feature extraction is described in [28]. The scheme consists of two NN stages: The ﬁrst one is reading short temporal context, its output is stacked, downsampled, and fed into the second NN reading longer temporal information.
The ﬁrst stage bottle-neck NN input features are 24 log Mel ﬁlter bank outputs concatenated with fundamental frequency features. Conversation-side based mean subtraction is applied and 11 consecutive frames are stacked. Hamming window followed by discrete cosine transform (DCT) retaining 0th to 5th coefﬁcients are applied on the time trajectory of each parameter resulting in 37×6=222 coefﬁcients at the ﬁrst-stage NN input.
In this work, the ﬁrst-stage NN has 4 hidden layers with 1500 units in each except the bottle-neck (BN) one. The BN layer has 80 neurons. The neurons in the BN layer have linear activations as found optimal in [29]. 21 consecutive frames from the ﬁrst-stage NN are stacked, down-sampled (each 5 frame is taken) and fed into the second-stage NN with an architecture similar to the ﬁrst-stage NN, except of BN layer with only 30 neurons. Both neural networks were trained jointly as suggested in [29] in CNTK toolkit [30] with blocksoftmax ﬁnal layer [31]. Context-independent phoneme states are used as the training targets for the feature-extraction NN, otherwise the size of the ﬁnal layer would be prohibitive.
Finally, BN outputs from the second stage NN are used as features for further experiments and will be noted as “Mult11-SBN”.
5.2. Results
Figure 1 presents the performance curve of the seq2seq model with four “train” and “target” languages, as discussed in Section 3, by changing the amount of training data. It shows signiﬁcant performance drop of baseline, “fbank” based, systems when the amount of training data is lowered.
On the other hand, the multilingual features present: 1) significantly smaller performance degradation than baseline “fbank” fea-

Fig. 1. Monolingual models trained on top of multilingual features.

Features
FBANK Mult11-SBN

Swahili %CER
28.6 26.4

Amharic %CER
45.3 40.4

Tok Pisin %CER 32.2 26.8

Georgian %CER
34.8 33.2

Table 2. Monolingual models trained on top of multilingual features.

tures on small amounts of training data. 2) consistent improvement on both train (seen) and target (unseen) languages where we only use train (seen) languages in feature extractor training data. 3) signiﬁcant improvement even on the full training set, i.e., 1.6%-5.0% absolute (Table 2 summarizes the full training set results).
6. MULTILINGUAL MODELS
Next, we focus on the training of multilingual seq2seq models. As our models are character-based, the multilingual training dictionary is created by concatenation of all train languages, and the system is trained in same way as monolingual on concatenated data.
6.1. Direct decoding from multilingual NN
As the multilingual net is trained to convert a sequence of input features into sequence of output characters, any language from training set or an unknown language with compatible set of characters can be directly decoded. Obviously, characters from wrong language can be generated as the system needs to performs also language identiﬁcation (LID). Adding LID information as an additional feature, similarly to [32], complicates the system. Therefore, we experimented with “ﬁne-tuning” of the system into the target language by running a few epochs only on desired language data. This is in strengthening the target language characters, therefore it makes the system less prone to language- and character-set-mismatch errors.
The ﬁrst two rows of table 3 present signiﬁcant performance degradation from monolingual to multilingual seq2seq models caused by wrong decision of output characters in about 20% of test utterances. However, no out-of-language characters are observed after “ﬁne-tuning” and 1.5% and 4.7% improvement over monolingual baseline is reached.

Fig. 2. Fine-tuning of multilingual NN on Swahili.

Fig. 3. Comparison of various multilingual approaches on Swahili.

As mentioned above, multilingual NN can be ﬁne-tuned to the target language if character set is compatible with the training set. Figure 2 shows results on Swahili, which is not part of the training set. Similarly to experiments with multilingual features in Figure 1, the multilingual seq2seq systems are effective especially on small amounts of data, but also beat baseline models on full ∼50h language set.
6.2. Language-Transfer learning
Language-Transfer learning is necessary if target language characters differ from train set ones. The whole process can be described in three steps: 1) randomly initialize output layer parameters, 2) train only new parameters and freeze the remaining ones 3) “ﬁne-tune” the whole NN. Various experiments are conducted on level of output parameters including output softmax (Out), the attention (Att), and CTC parts. Table 4 compares all combinations and clearly shows that retraining of output softmax only is giving the best results.
Finally, we summarize the comparison of the use of multilingual features for the seq2seq model and language transfer learning of multilingual seq2seq model in Figure 3. Interestingly, on contrary to our previous observations on DNN-HMM systems [19], we found multilingual features superior to language transfer learning in seq2seq model case.
7. CONCLUSIONS
We have presented various multilingual approaches in seq2seq systems including multilingual features and multilingual models by leveraging our multilingual DNN-HMM expertise. Unlike DNN-

Model
Monolingual Multilingual Multilingual-ﬁne tuned

Tok Pisin %CER 32.2 37.2 27.5

Georgian %CER
34.8 51.1 33.3

Table 3. Multilingual ﬁne tuning of seq2seq model.

HMM systems [19], we obtain the opposite conclusion that multilingual features are more effective in seq2seq systems. It is probably due to efﬁcient fusion of two complementary approaches: explicit GMM-HMM alignment incorporated in BN features and seq2seq models in the ﬁnal system. With this ﬁnding, we will further explore efﬁcient combinations of the DNN-HMM and seq2seq systems as our future work.
8. REFERENCES
[1] Ilya Sutskever, Oriol Vinyals, and Quoc V Le, “Sequence to sequence learning with neural networks,” in Advances in neural information processing systems, 2014, pp. 3104–3112.
[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio, “Neural machine translation by jointly learning to align and translate,” arXiv preprint arXiv:1409.0473, 2014.
[3] Kyunghyun Cho, Bart Van Merrie¨nboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio, “Learning phrase representations using RNN encoder-decoder for statistical machine translation,” arXiv preprint arXiv:1406.1078, 2014.
[4] Jan K Chorowski, Dzmitry Bahdanau, Dmitriy Serdyuk, Kyunghyun Cho, and Yoshua Bengio, “Attention-based models for speech recognition,” in Advances in neural information processing systems, 2015, pp. 577–585.
[5] Alex Graves and Navdeep Jaitly, “Towards end-to-end speech recognition with recurrent neural networks.,” in ICML, 2014, vol. 14, pp. 1764–1772.

Language Transfer Monoling. Out Att+Out CTC+Out Att+CTC+Out

Swahili %CER
28.6 27.4 27.5 27.6 28.0

Amharic %CER
45.3 41.2 41.2 41.2 42.1

Tok Pisin %CER 32.2 27.7 28.3 27.9 27.6

Georgian %CER
34.8 33.6 34.2 33.7 34.1

Table 4. Multilingual Language Transfer

[6] Alex Graves, “Supervised sequence labelling,” in Supervised sequence labelling with recurrent neural networks, pp. 5–13. Springer, 2012.
[7] William Chan, Navdeep Jaitly, Quoc Le, and Oriol Vinyals, “Listen, attend and spell: A neural network for large vocabulary conversational speech recognition,” in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2016, pp. 4960–4964.
[8] Andrew Rosenberg, Kartik Audhkhasi, Abhinav Sethy, Bhuvana Ramabhadran, and Michael Picheny, “End-to-end speech recognition and keyword search on low-resource languages,” in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2017, pp. 5280–5284.
[9] Sibo Tong, Philip N Garner, and Herve´ Bourlard, “An investigation of deep neural networks for multilingual speech recognition training and adaptation,” Tech. Rep., 2017.
[10] Martin Karaﬁa´t, Murali Karthick Baskar, Pavel Mateˇjka, Karel Vesely`, Frantisˇek Gre´zl, and Jan Cˇ ernocky, “Multilingual blstm and speaker-speciﬁc vector adaptation in 2016 BUT Babel system,” in Spoken Language Technology Workshop (SLT), 2016 IEEE. IEEE, 2016, pp. 637–643.
[11] Sibo Tong, Philip N Garner, and Herve´ Bourlard, “Multilingual training and cross-lingual adaptation on CTC-based acoustic model,” arXiv preprint arXiv:1711.10025, 2017.
[12] Pawel Swietojanski and Steve Renals, “Learning hidden unit contributions for unsupervised speaker adaptation of neural network acoustic models,” in Spoken Language Technology Workshop (SLT), 2014 IEEE. IEEE, 2014, pp. 171–176.
[13] Markus Mu¨ller, Sebastian Stu¨ker, and Alex Waibel, “Language adaptive multilingual CTC speech recognition,” in International Conference on Speech and Computer. Springer, 2017, pp. 473–482.
[14] Siddharth Dalmia, Ramon Sanabria, Florian Metze, and Alan W. Black, “Sequence-based multi-lingual low resource speech recognition,” in ICASSP. 2018, pp. 4909–4913, IEEE.
[15] Shinji Watanabe, Takaaki Hori, and John R Hershey, “Language independent end-to-end architecture for joint language identiﬁcation and speech recognition,” in Automatic Speech Recognition and Understanding Workshop (ASRU), 2017 IEEE. IEEE, 2017, pp. 265–271.
[16] Shubham Toshniwal, Tara N Sainath, Ron J Weiss, Bo Li, Pedro Moreno, Eugene Weinstein, and Kanishka Rao, “Towards language-universal end-to-end speech recognition,” in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2018.
[17] Jaejin Cho, Murali Karthick Baskar, Ruizhi Li, Matthew Wiesner, Sri Harish Mallidi, Nelson Yalta, Martin Karaﬁat, Shinji Watanabe, and Takaaki Hori, “Multilingual sequence-tosequence speech recognition: architecture, transfer learning, and language modeling,” in IEEE Workshop on Spoken Language Technology (SLT), 2018.
[18] Zoltan Tuske, David Nolden, Ralf Schluter, and Hermann Ney, “Multilingual mrasta features for low-resource keyword search and speech recognition systems,” in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2014, pp. 7854–7858.

[19] Martin Karaﬁa´t, K. Murali Baskar, Pavel Mateˇjka, Karel Vesely´, Frantisˇek Gre´zl, and Jan Cˇ ernocky´, “Multilingual blstm and speaker-speciﬁc vector adaptation in 2016 but babel system,” in Proceedings of SLT 2016. 2016, pp. 637–643, IEEE Signal Processing Society.
[20] Shinji Watanabe, Takaaki Hori, Suyoun Kim, John R Hershey, and Tomoki Hayashi, “Hybrid CTC/attention architecture for end-to-end speech recognition,” IEEE Journal of Selected Topics in Signal Processing, vol. 11, no. 8, pp. 1240–1253, 2017.
[21] Matthias Sperber, Jan Niehues, Graham Neubig, Sebastian Stker, and Alex Waibel, “Self-attentional acoustic models,” in 19th Annual Conference of the International Speech Communication Association (InterSpeech 2018), 2018.
[22] Chung-Cheng Chiu and Colin Raffel, “Monotonic chunkwise attention,” CoRR, vol. abs/1712.05382, 2017.
[23] Jan Chorowski, Dzmitry Bahdanau, Dmitriy Serdyuk, Kyunghyun Cho, and Yoshua Bengio, “Attention-based models for speech recognition,” in Advances in Neural Information Processing Systems. 2015, vol. 2015-January, pp. 577– 585, Neural information processing systems foundation.
[24] Daniel Povey, Arnab Ghoshal, Gilles Boulianne, Lukas Burget, Ondrej Glembek, Nagendra Goel, Mirko Hannemann, Petr Motlicek, Yanmin Qian, Petr Schwarz, et al., “The Kaldi speech recognition toolkit,” in Automatic Speech Recognition and Understanding, 2011 IEEE Workshop on. IEEE, 2011, pp. 1–4.
[25] Mike Schuster and Kuldip K Paliwal, “Bidirectional recurrent neural networks,” IEEE Transactions on Signal Processing, vol. 45, no. 11, pp. 2673–2681, 1997.
[26] Sepp Hochreiter and Ju¨rgen Schmidhuber, “Long short-term memory,” Neural computation, vol. 9, no. 8, pp. 1735–1780, 1997.
[27] Shinji Watanabe, Takaaki Hori, Shigeki Karita, Tomoki Hayashi, Jiro Nishitoba, Yuya Unno, Nelson Enrique Yalta Soplin, Jahn Heymann, Matthew Wiesner, Nanxin Chen, et al., “ESPnet: End-to-end speech processing toolkit,” arXiv preprint arXiv:1804.00015, 2018.
[28] Martin Karaﬁa´t, Frantisˇek Gre´zl, Mirko Hannemann, Karel Vesely´, Igor Szoke, and Jan ”Honza” Cˇ ernocky´, “BUT 2014 Babel system: Analysis of adaptation in NN based systems,” in Proceedings of Interspeech 2014, Singapure, September 2014.
[29] Karel Vesely´, Martin Karaﬁa´t, and Frantisˇek Gre´zl, “Convolutive bottleneck network features for LVCSR,” in Proceedings of ASRU 2011, 2011, pp. 42–47.
[30] Amit Agarwal et al., “An introduction to computational networks and the computational network toolkit,” Tech. Rep. MSR-TR-2014-112, August 2014.
[31] Karel Vesely´, Martin Karaﬁa´t, Frantisˇek Gre´zl, Milosˇ Janda, and Ekaterina Egorova, “The language-independent bottleneck features,” in Proceedings of IEEE 2012 Workshop on Spoken Language Technology. 2012, pp. 336–341, IEEE Signal Processing Society.
[32] Suyoun Kim and Michael L. Seltzer, “Towards languageuniversal end-to-end speech recognition,” in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2018.

