On the Pareto Frontier of Regret Minimization and Best Arm Identiﬁcation in Stochastic Bandits

arXiv:2110.08627v1 [cs.LG] 16 Oct 2021

Zixin Zhong 1 2 Wang Chi Cheung 3 4 Vincent Y. F. Tan 1 4 2

Abstract
We study the Pareto frontier of two archetypal objectives in stochastic bandits, namely, regret minimization (RM) and best arm identiﬁcation (BAI) with a ﬁxed horizon. It is folklore that the balance between exploitation and exploration is crucial for both RM and BAI, but exploration is more critical in achieving the optimal performance for the latter objective. To make this precise, we ﬁrst design and analyze the BOBW-LIL’UCB(γ) algorithm, which achieves order-wise optimal performance for RM or BAI under different values of γ. Complementarily, we show that no algorithm can simultaneously perform optimally for both the RM and BAI objectives. More precisely, we establish non-trivial lower bounds on the regret achievable by any algorithm with a given BAI failure probability. This analysis shows that in some regimes BOBW-LIL’UCB(γ) achieves Pareto-optimality up to constant or small terms. Numerical experiments further demonstrate that when applied to difﬁcult instances, BOBW-LIL’UCB outperforms a close competitor UCBα (Degenne et al., 2019), which is designed for RM and BAI with a ﬁxed conﬁdence.
1. Introduction
Consider a drug company Dandit (Drug Bandit) that wants to design an effective vaccine for a certain virus. It has a certain number of feasible options, say L = 10. Because Dandit has a limited budget, it can only test vaccines for a ﬁxed number of times, say T = 1, 000. Using the limited number of tests, it wants to ﬁnd the option that will lead to the “best” outcome, e.g., the maximum efﬁcacy of the drug.
1Department of Mathematics, National University of Singapore, Singapore 2Department of Electrical and Computer Engineering, National University of Singapore, Singapore 3Department of Industrial Systems and Management, National University of Singapore, Singapore 4Institute of Operations Research and Analytics, National University of Singapore, Singapore. Correspondence to: Zixin Zhong <zixin.zhong@u.nus.edu>, Wang Chi Cheung <isecwc@nus.edu.sg>, Vincent Y. F. Tan <vtan@nus.edu.sg>.

At the same time, Dandit aims to protect individuals from potentially adverse side effects of the vaccines to be tested. How can Dandit ﬁnd the optimal drug design and, at the same time, protect the health of participants? We design an algorithm BOBW-LIL’UCB that allows Dandit to balance between these two competing targets. In complement, we also show that it is impossible for Dandit to achieve optimal performances for both targets simultaneously, and Dandit has to settle for operating on the Pareto frontier of the two objectives.
To solve Dandit’s problem, we study the Regret Minimization (RM) and Best Arm Identiﬁcation (BAI) problems for stochastic bandits with a ﬁxed time horizon or budget. While most existing works only study one of these two targets (Auer et al., 2002a; Audibert and Bubeck, 2010), Degenne et al. (2019) designed the UCBα algorithm for both RM and BAI with a ﬁxed conﬁdence. Therefore, these studies are not directly applicable to Dandit’s problem as Dandit is interested in obtaining the optimal item and minimizing the damage across a ﬁxed number of tests. However, our setting dovetails neatly with company Dandit’s goals. Dandit can utilize our algorithm to sequentially and adaptively select different design options to test the vaccines and to eventually balance between choosing the optimal vaccine and, in the process, mitigating any physical damage on the participants. We also show that Dandit cannot achieve both targets optimally and simultaneously.
Beyond any speciﬁc applications, we believe that this problem is of fundamental theoretical importance in the broad context of multi-armed bandits (MAB). In order to design an efﬁcient bandit algorithm, a well-known challenge is to balance between exploitation and exploration (Auer et al., 2002a; Lattimore and Szepesva´ri, 2020). Our work studies the Pareto frontier of RM and BAI, as well as the effects of exploitation and exploration on these two aims.
Main contributions. In stochastic bandits, there are L items with different unknown reward distributions. At each time step, a random reward is generated from each item’s distribution. Based on the previous observations, a learning agent selects an item and observes its reward. Given the number of time steps T ∈ N, the agent aims to maximize the cumulative rewards and to identify the optimal item with

On the Pareto Frontier of Regret Minimization and Best Arm Identiﬁcation in Stochastic Bandits

high probability.
Our ﬁrst main contribution is the BOBW-LIL’UCB(γ) algorithm. BOBW-LIL’UCB(γ) is designed for both RM and BAI over a ﬁxed time horizon, which achieves Paretooptimality of RM and BAI in some regimes.
(i) On one hand, we can shrink the conﬁdence radius of each item by increasing γ, which encourages BOBWLIL’UCB(γ) to pull items with high empirical mean rewards (exploitation) and leads to high rewards (i.e., small regret).
(ii) On the other hand, we can enlarge the conﬁdence radius by decreasing γ to encourage the exploration of items that have not been sufﬁciently pulled in previous time steps (exploration); this will result in a high BAI success probability.
The parameter γ in BOBW-LIL’UCB(γ) can be tuned such that either its cumulative regret or its failure probability almost matches the corresponding state-of-the-art lower bound (Lai and Robbins, 1985; Carpentier and Locatelli, 2016). The performance of BOBW-LIL’UCB(γ) implies that exploitation is more critical in achieving the optimal performance for RM, while exploration is more crucial for BAI.
Moreover, we evaluate the Pareto frontier of RM and BAI theoretically. Lattimore and Szepesva´ri (2020, Note 33.3) mention that an asymptotically optimal algorithm for RM incurs a failure probability that decays only polynomially with T . This implies that if the cumulative regret of an algorithm is small, this may adversely affect its performance on a BAI task. We generalize this observation for all RM and BAI algorithms. In detail, given the BAI failure probability of an algorithm, we establish non-trivial lower bounds on its regret. We conclude that it is impossible for any algorithm to achieve asymptotically optimal performances for both RM and BAI simultaneously.
Furthermore, BOBW-LIL’UCB(γ) empirically outperforms a close competitor UCBα (Degenne et al., 2019) in difﬁcult scenarios in which the differences between the optimal and suboptimal items are small. While both algorithms identify the optimal item with high probability, UCBα, designed for the ﬁxed-conﬁdence case, requires a longer horizon to do so and also suffers from larger regret. This demonstrates the superiority of BOBW-LIL’UCB(γ) under the ﬁxed-budget setting, which it is speciﬁcally designed for.
Novelty. (i) We are the ﬁrst to design an algorithm for both RM and BAI with a ﬁxed budget. We can adjust the proposed BOBW-LIL’UCB(γ) algorithm to perform (near)optimally for both RM and BAI with proper choices of γ. (ii) The performance of BOBW-LIL’UCB(γ) implies that exploitation is more crucial to obtain a small regret, while exploration is more critical to shrink the BAI failure

probability. (iii) We quantify the Pareto frontier of RM and BAI. We show that it is inevitable for any algorithm to compromise between RM and BAI in a ﬁxed horizon setting.
Literature review. Both the RM and BAI problems have been studied extensively for stochastic multi-armed bandits. Firstly, an RM algorithm aims to maximize its cumulative rewards, i.e., to minimize its regret (the gap between the highest cumulative rewards and the obtained rewards). One line of seminal works on RM involve the class of Upper Conﬁdence Bound (UCB) algorithms (Auer et al., 2002a; Garivier and Cappe´, 2011), while another line of works study Thompson sampling (TS) algorithms (Agrawal and Goyal, 2012; Russo and Van Roy, 2014; Agrawal and Goyal, 2017). Lai and Robbins (1985) derived a lower bound on the regret of any online algorithm.
Secondly, there are two complementary settings for BAI: (i) given T ∈ N, the agent aims to maximize the probability of ﬁnding the optimal item in at most T steps (Audibert and Bubeck, 2010; Karnin et al., 2013; Zhong et al., 2021); (ii) given δ > 0, the agent aims to ﬁnd the optimal item with the probability of at least 1 − δ in the smallest number of time steps (Bubeck et al., 2013; Kaufmann and Kalyanakrishnan, 2013). These two settings are known as the ﬁxed-budget and ﬁxed-conﬁdence settings respectively. Moreover, Carpentier and Locatelli (2016) established a lower bound on the failure probability of any algorithm in a ﬁxed time horizon.
While most existing works focus solely on RM or BAI, Degenne et al. (2019) explored both goals with a ﬁxed conﬁdence and proposed the UCBα algorithm. However, to the best of our knowledge, there is no existing analysis of one algorithm for both RM and BAI in a ﬁxed horizon. Our work ﬁlls in this gap by proposing the BOBW-LIL’UCB(γ) algorithm and proving that it achieves Pareto-optimality in some regimes. We also study the Pareto frontier of RM and BAI, which depends on the balance between exploitation and exploration. We show that a single algorithm cannot perform optimally for both RM and BAI simultaneously.
2. Problem Setup
For any n ∈ N, we denote the set {1, . . . , n} as [n]. Let there be L ∈ N ground items, contained in [L]. A random variable X (or its distribution) is σ-sub-Gaussian (σ-SG) if E[eλ(X−EX)] ≤ exp(λ2σ2/2). Each item i ∈ [L] is associated with a σ-SG reward distribution νi, mean wi, and variance σi2. The distributions {νi}i∈[L], means {wi}i∈[L], and variances {σi2}i∈[L] are unknown to the agent. We let {gi,t}Tt=1 be the i.i.d. sequence of rewards associated with item i during the T time steps; each gi,t is an independent sample from νi.
We focus on instances with a unique item of the highest mean reward, and assume that w1 > w2 ≥ . . . ≥ wL, so

On the Pareto Frontier of Regret Minimization and Best Arm Identiﬁcation in Stochastic Bandits

the unique optimal item i∗ = 1. Note that the items can, in general, be arranged in any order; the ordering that wi ≥ wj for i < j is employed to ease our discussion. We denote ∆1,i := w1 − wi as the optimality gap of item i, and assume ∆1,i ≤ 1 for all i ∈ [L]; this can be achieved by rescaling the instance if necessary. We deﬁne the minimal optimality gap
∆ := min ∆1,i.
i=1
Clearly, ∆ > 0. We characterize the hardness of an instance with the following canonical quantities:

1 H1 := ∆1,i
i=1

1 and H2 := i=1 ∆21,i .

(2.1)

The hardness quantity H1 is involved in some near-optimal regret bounds (Auer et al., 2002a; Agrawal and Goyal, 2012). The quantity H2 was ﬁrst introduced in Audibert and Bubeck (2010) and appears in many landmark works on BAI (Jamieson et al., 2014; Karnin et al., 2013; Carpentier and Locatelli, 2016).
The agent uses an online algorithm π to decide the item iπt to pull at each time step t, and the item iπou,Tt to output eventually. More formally, an online algorithm consists of a tuple π := ((πt)Tt=1, φπT,T ), where
• the sampling rule πt determines, based on the observation history, the item iπt to pull at time step t. That is, the random variable iπt is Ft−1-measurable, where Ft := σ(iπ1 , giπ1 ,1, . . . , iπt , giπt ,t);
• the recommendation rule φπT,T chooses an item iπou,Tt , that is, by deﬁnition, FT -measurable.
Moreover, we deﬁne the pseudo-regret RT of π as

T

RT (π) : = max E

gi,t − E

1≤i≤L

t=1

T

= T · w1 − E

wiπt .

t=1

T
giπt ,t
t=1

The algorithm π aims to both minimize the pseudo-regret
RT (π) and at the same time, to identify the optimal item with high probability, i.e., to minimize the failure probability eT (π) := Pr(iπou,Tt = 1). We omit T and/or π in the superscript or subscript when there is no cause of confusion.
We write RT (π) as RT (π, I), eT (π) as eT (π, I) when we wish to emphasize their dependence on both the algorithm
π and the instance I.

3. Discussion on Existing Algorithms
Although there is no existing work that analyzes a single algorithm for both RM and BAI in a ﬁxed horizon, it is natural to question if an algorithm which is originally designed

for RM can also perform well for BAI, and vice versa. In Table 3.1, we present the theoretical results from some existing works. We focus on algorithms that are with (potential) theoretical guarantees for both RM and BAI. We deﬁne

ip

L

Hp

:=

max
i=1

∆2

and

Cp := 2−p +

r−p

i

r=2

for p > 0 as in Shahrampour et al. (2017). We abbreviate SEQUENTIAL HALVING as SH, NONLINEAR SEQUENTIAL ELIMINATION with parameter p as NSE(p), and UCB-E with parameter a as UCB-E(a). Also see Appendix A for more discussions.

Table 3.1. Comparison among upper bounds for algorithms and lower bounds in stochastic bandits.

Algorithm/Instance SH
NSE(p) UCB-E(α log T ) BOBW-LIL’UCB(γ) Stochastic Bandits

Pseudo-regret RT Θ(T )
Θ(T )
6.3α2H1 log T (Corollary A.1) (Theorem 4.1) ≈ 4H1 log T (Lai and Robbins, 1985)

Failure Probability eT T
≈ exp − 8H2 log2 L
(Karnin et al., 2013) 2(T − L)
≈ exp − HpCp
(Shahrampour et al., 2017) 2LT 1−2α/25

(Audibert and Bubeck, 2010)

(Theorem 4.2)

1

400T

exp −

6

H2 log L

(Carpentier and Locatelli, 2016)

According to the discussions on RM and BAI in Lattimore and Szepesva´ri (2020), any algorithm with an asymptotically optimal regret would incur a failure probability lower bounded by Ω(T (1+o(1))(1+ε)2 ); this is much larger than the state-of-the-art lower bound Ω(exp(−400T /(H2 log L))) by Carpentier and Locatelli (2016). Therefore, we only include algorithms that were designed for BAI in Table 3.1.
Among the various BAI algorithms, SH and NSE(p) perform almost the best. However, their bounds on the failure probabilities are incomparable in general. The comparison among more BAI algorithms is provided in Table A.1. Due to the designs of SH and NSE(p), we surmise their regrets grow linearly with T , which is vacuous for the RM task.
Although UCB-E(α log T ) has upper bounds on both pseudo-regret and failure probability, its bound on the latter, which decays only polynomially fast with T when α is an absolute constant, is clearly suboptimal vis-a`-vis the stateof-the-art lower bound by Carpentier and Locatelli (2016). In order to achieve an exponentially decaying upper bound on eT (i.e., exp(−Θ(T ))), we need to set α = O(T / log T ), and hence the regret bound (see Corollary A.1 in the supplementary) will be O(T 2/ log T ), which is vacuous.
The discussion above raises a natural question. Is it possible to provide a non-trivial bound on the regret for an algorithm that performs optimally for BAI over a ﬁxed horizon?

On the Pareto Frontier of Regret Minimization and Best Arm Identiﬁcation in Stochastic Bandits

This motivates us to design the BOBW-LIL’UCB algorithm, which can be tuned to perform near-optimally for both RM and BAI.

4. The BOBW-LIL’UCB Algorithm

We design and analyze BOBW-LIL’UCB(γ) (BEST OF BOTH WORLDS-LAW OF ITERATED LOGS-UCB), an algorithm for both RM and BAI in a ﬁxed horizon. By choosing parameter γ judiciously, the guarantees of BOBWLIL’UCB(γ) match those of the state-of-the-art algorithms for both RM (up to log factors) and BAI (concerning the exponential term).

Algorithm 1 BOBW-LIL’UCB(γ)

1: Input: time budget T , size of ground set of items L, scale σ > 0, ε ∈ (0, 1), β ≥ e, and γ ∈ (0, 1).
2: Sample it = i for t = 1, . . . , L and set t = L. 3: For all i ∈ [L], compute Ni,L, gˆi,L, Ci,L,γ , Ui,L,γ :

t
Ni,t = 1{iu = i},
u=1
√ Ci,t,γ = 5σ(1+ ε)

gˆi,t =

tu=1 gi,t · 1{iu = i} , Ni,t

2(N1i+,tε) ·log log(β+(1γ+ε)Ni,t) ,

Ui,t,γ = gˆi,t + Ci,t,γ .

4: for t = L + 1, . . . , T do 5: Pull item it = arg maxi∈[L] Ui,t−1,γ . 6: For all i ∈ [L], update Ni,t, gˆi,t, Ci,t,γ , Ui,t,γ . 7: end for 8: Output iout = arg maxi∈[L] gˆi,T .

Design of algorithm. We design BOBW-LIL’UCB in the spirit of the law of the iterated logarithm (LIL) (Darling and Robbins, 1967; Jamieson et al., 2014). We remark that it is a variation of the LIL’UCB algorithm proposed by Jamieson et al. (2014). The three differences are: (i) to construct the conﬁdence radius Ci,t,γ, we replace (1+
β) and δ in LIL’UCB by 5 and γ in BOBW-LIL’UCB(γ) respectively;
(ii) in the design of Ci,t,γ, we also replace log((1 + ε)Ni,t) by log(β + (1 + ε)Ni,t);
(iii) BOBW-LIL’UCB(γ), which is designed for both RM and BAI in a ﬁxed horizon, involves no stopping rule since it proceeds for exactly T time steps; while LIL’UCB is designed for BAI with a ﬁxed conﬁdence.
Although our algorithm depends on the choices of ε, β, and γ, we term it as BOBW-LIL’UCB(γ) instead of the more verbose BOBW-LIL’UCB(ε, β, γ) because we scale the conﬁdence radius by only varying γ which adjusts the performance of the algorithm. More precisely, inspired by the LIL (see Theorem B.1), we design item i’s conﬁdence radius Ci,t,γ with Ni,t (the number of time steps when item i is pulled up to and including the tth time step) and gˆi,t

(the empirical mean of item i at time step t), and its upper conﬁdence bound Ui,t,γ accordingly.
The design of BOBW-LIL’UCB(γ) allows us to shrink Ci,t,γ, the conﬁdence radius of each item i, by increasing γ; and vice versa. Moreover, with a ﬁxed γ, if item i is rarely pulled in previous time steps, it has a small Ni,t and hence a large Ci,t,γ; and vice versa.
(i) Therefore, when γ increases, the dominant term in Ui,t,γ = gˆi,t + Ci,t,γ becomes the empirical mean gˆi,t. Since BOBW-LIL’UCB pulls the item with the largest Ui,t−1,γ at time step t, the algorithm tends to pull the item with the largest empirical mean in this case. In other words, a large γ encourages exploitation.
(ii) When γ decreases, the conﬁdence radius Ci,t,γ dominates Ui,t,γ. Consequently, BOBW-LIL’UCB is likely to pull items with large Ci,t,γ, i.e., the rarely pulled items with small Ni,t. This indicates that a small γ encourages exploration.
Altogether, we can scale Ui,t,γ by adjusting γ, which allows us to balance exploitation and exploration and trade-off between the twin objectives — RM and BAI.

Analysis for RM. We ﬁrst derive problem-dependent and problem-independent bounds on the pseudo-regret of BOBW-LIL’UCB(γ).

Theorem 4.1 (Bounds on the pseudo-regret of BOBW-LIL’UCB). Let ε ∈ (0, 1), β ≥ e, and γ ∈ (0, log(β + 1 + ε)/e). The pseudo-regret of BOBW-LIL’UCB(γ) satisﬁes

RT ≤ O σ2 ·

log(1/γ) ,

i=1 ∆1,i

RT ≤ O σ2√T L log log(T /(Lγ)) . γ
√ Furthermore, we can set γ = 1/ T to obtain

RT ≤ O σ2 · log T , RT ≤ O σ2√T L log T . i=1 ∆1,i

We observe that the order of the problem-dependent upper √
bound on the pseudo-regret of BOBW-LIL’UCB(1/ T )

almost matches that of the lower bound (Lai and Robbins,

1985). Moreover, the worst-case (problem-independent) up-

√

√

per bound of BOBW-LIL’UCB√(1/ T ) is O˜( T L), which

matches the lower bound O( T L) (Bubeck et al., 2012)

up to log factors. This implies that we can tune the pa-

rameter γ in the BOBW-LIL’UCB(γ) algorithm to obtain

close-to-optimal performance for RM.

We remark that when the optimal item is not unique, we can also derive analogous upper bounds on the pseudo-regret of BOBW-LIL’UCB(γ) using a similar line of analysis (see Proposition C.1).

On the Pareto Frontier of Regret Minimization and Best Arm Identiﬁcation in Stochastic Bandits

Analysis for BAI. Next, we upper bound the failure probability of BOBW-LIL’UCB(γ).

Theorem 4.2 (Bounds on the failure probability of BOBW-LIL’UCB). Let ε ∈ (0, 1), β ≥ e, and γ ∈ (0, log(β + 1 + ε)/e). Let ∆i = max{∆, ∆1,i} for all i ∈ [L]. The failure probability of BOBW-LIL’UCB(γ) satisﬁes

2L(2 + ε)

γ

1+ε

eT ≤ ε

,

(4.1)

log(1 + ε)

if

T − L L 72σ2

2.8

11σ(1 + ε)2

(1 + ε)3 ≥ i=1 ∆2i · log γ2 log

+β . ∆i

In particular, the bound on eT in (4.1) holds when γ ≥

γ1(∆, H2), where

γ1(∆, H2) =

2.8 log

√ 6 2.8σ(1 + ε)2
+β ∆

· exp

T −L − 144σ2(1 + ε)3(H2 + ∆−2) .

When γ assumes its lower bound γ1(∆, H2), we have

eT ≤ O˜ L exp −

T −L

144σ2(1 + ε)2(H2 + ∆−2)

. (4.2)

When T L, the gap between our upper bound in (4.2) and Ω(exp(−400T /(H2 log L))), the state-of-the-art lower bound (Carpentier and Locatelli, 2016), is manifested by the (pre-exponential) term L as well as the constant in the exponent. This indicates that BOBW-LIL’UCB(γ) can be adjusted to perform near-optimally for BAI over a ﬁxed horizon.
Further observation. As discussed earlier, BOBWLIL’UCB(γ) encourages more√exploitation than exploration when γ is large (e.g. γ = 1/ T ) and it stimulates more exploration when γ is small (e.g. γ = γ1(∆, H2)). Besides, Theorems 4.1 and 4.2 imply that the pseudo-regret of BOBW-LIL’UCB(γ) decreases with γ while its failure probability increases with γ. Therefore, to minimize the regret, we should increase γ to stimulate exploitation; and we should decrease γ to encourage exploration for obtaining a small failure probability. This indicates that an optimal RM algorithm encourages more exploitation compared to an optimal BAI one, and vice versa.

5. Pareto Frontier of RM and BAI
Theorems 4.1 and 4.2 together suggest that BOBWLIL’UCB(γ) cannot perform optimally for both RM and BAI simultaneously with a universal (or single) choice of γ. In this section, we prove that no algorithm can perform optimally for these two objectives simultaneously. Given a certain failure probability of an algorithm, our goal is to

establish a non-trivial lower bound on its pseudo-regret.
We ﬁrst consider bandit instances in which items have bounded rewards. Let B1(∆, R) denote the set of stochastic instances where (i) the minimal optimality gap ∆ ≥ ∆; and (ii) there exists R0 ∈ R such that the rewards are bounded in [R0, R0 + R]. Let B2(∆, R, H2) denote the set of instances that (i) belong to B1(∆, R), and (ii) have hardness quantities H2 ≤ H2.
Theorem 5.1. Let φT , ∆, R, H2 > 0. Let π be any algorithm with eT (π, I) ≤ exp(−φT )/4 for all I ∈ B1(∆, R). Then
(L − 1)R sup RT (π, I) ≥ φT · 8∆ ,
I ∈B1 (∆,R) 3
∆H 2 R sup RT (π, I) ≥ φT · 8 .
I ∈B2 (∆,R,H 2 )
In Theorem 5.1, we apply the bounds R0 and R0 + R on items’ rewards to classify instances. In general, ∆H ≤ (L − 1)/∆ holds for any instance, and equality holds when ∆1,i = ∆ for all i = 1. Therefore, B1(∆, R) = B2(∆, R, (L − 1)/(∆2)). When R > 1, the analysis for the set B2(∆, R, (L − 1)/(∆2)) provides a better bound (higher lower bound) for the set B1(∆, R).
Furthermore, we establish a similar analysis for instances in which the variance of each item’s reward distribution is bounded. Let B1(∆, V ) denote the set of instances where (i) the minimal optimality gap ∆ ≥ ∆; (ii) for each item i, the variance σi2 ≤ V . Let B2(∆, V , H2) denote the set of instances (i) that belong to B1(∆, V ), and (ii) have hardness quantities H2 ≤ H2. The key difference between the proofs of these two theorems lies in the design of hard instances. We elaborate on the details in Section 7.3.
Theorem 5.2. Let φT , ∆, V , H2 > 0. Let π be any algorithm with eT (π, I) ≤ exp(−φT )/4 for all I ∈ B1(∆, V ). Then
(L − 1)V sup RT (π, I) ≥ φT · 2∆ ,
I∈B1(∆,V )
sup RT (π, I) ≥ φT · ∆H2V . I∈B2(∆,V ,H2) 2
By characterizing stochastic rewards with different statistics, Theorems 5.1 and 5.2 provide different lower bounds on the pseudo-regret. We observe that when the rewards of items are bounded in [R0, R0 + R] for some R0 ∈ R, the variances of the rewards are bounded by R2/4. Therefore,
R2 B1(∆, R) ⊂ B1 ∆, 4 ,
R2 B2(∆, R, H2) ⊂ B2 ∆, 4 , H2 .

On the Pareto Frontier of Regret Minimization and Best Arm Identiﬁcation in Stochastic Bandits

Besides, it is clear that

R2 R2

B1(∆, R), B2(∆, R, h), B2

∆,

,h 4

⊂ B1 ∆, 4

.

Due to the relationship among these four sets of instances,

we let π be an algorithm with eT (π, I) ≤ exp(−φT )/4
2
in any instance of B1(∆, R /4), and compare the derived
lower bounds on its pseudo-regret RT (π, I) in Table 5.1.

Table 5.1. Lower bounds on RT when eT ≤ e−φT /4.

Instance Set

Bound on RT

B1(∆, R)
B2(∆, R, H2)
2
B1(∆, R /4)
2
B2(∆, R /4, H2)

φT · (L − 1)R/(8∆)
3
φT · ∆H2R /8
2
φT · (L − 1)R /(8∆)
2
φT · ∆H2R /8

Table 5.1 indicates that

• when the bound for B1(∆, R) (ﬁrst row of Table 5.1)
2
holds for B1(∆, R /4), the quantities L and ∆ are of the same order in the bounds derived for B1(∆, R) and
2
B1(∆, R /4) respectively; • similarly, when the bound for B2(∆, R, H2) (second
2
row of Table 5.1) holds for B2(∆, R /4, H2), the quantities L and ∆ are of the same order in the bounds for
2
B2(∆, R, H2) and B2(∆, R /4, H2).
Moreover, when R > 1, we can apply the analysis of B2(∆, R, H2) to obtain a better bound (higher lower bound)
2
for B2(∆, R /4, H2).
In any set of instances studied in Theorems 5.1 or 5.2,
• when φT linearly grows with T , which is typical in the bounds on eT (Karnin et al., 2013; Carpentier and Locatelli, 2016), the corresponding bound on RT grows linearly with T (vacuous);
• when the bound on RT grows with log T as in Garivier and Cappe´ (2011) and Lai and Robbins (1985), φT grows logarithmically with T (i.e., the failure probability only decays polynomially).
Thus, we cannot achieve optimal performances for both RM and BAI using any algorithm with ﬁxed parameters. Alternatively, we can apply BOBW-LIL’UCB(γ) to achieve the best of both objectives with proper choices of the single parameter γ.

6. On the Tightness of the Upper and Lower Bounds
To evaluate the tightness of the lower bounds derived in Section 5, we compare the upper and lower bounds on the pseudo-regret of BOBW-LIL’UCB(γ) when the horizon T

is allowed to be arbitrarily large. We show that the parameter γ is crucial for BOBW-LIL’UCB(γ) to achieve the Pareto frontier (up to constant or small terms) in some regimes.

Corollary 6.1√. Let π1 be the online algorithm BOBWLIL’UCB(1/ T ) and π2 be the online algorithm BOBWLIL’UCB(γ1(∆, H2)). Then

sup RT (π1, I)
I ∈B2 (∆,1,H 2 )

T ∈ Ω ∆H2 log L

L log T

O

,

∆

sup RT (π2, I)
I ∈B2 (∆,1,H 2 )

∈ Ω (T − L) · ∆−2H2 (H2 + ∆ )

L(T − L) O ∆(H2 + ∆−2) .

We observe from Corollary 6.1, which combines Theorems 4.1, 4.2, and 5.1, that
• For both π1 and π2, the upper and lower bounds on the regret match in their dependence on T ;
• For both π1 and π2, the gaps between the upper and lower bounds depend on the term ∆H2 in the lower bound and L/∆ in the upper bound. As H2 ≤ (L − 1)∆−2 for any instance, when ∆ = ∆1,i for all i = 1 (all suboptimal items have the same suboptimality gap), equality holds, and hence the bounds for π2 match exactly, while the bounds for π1 match up to a small log(1/L) term.
Corollary 6.1 also implies that the parameter γ in BOBWLIL-UCB(γ) is essential in tuning the algorithm such that it can perform optimally for either RM or BAI. This analysis shows that in some regimes, BOBW-LIL-UCB(γ) achieves Pareto-optimality up to constant or small (e.g., log(1/L)) terms.

Furthermore, Corollary 6.1 also suggests that the lower bound in Theorem 5.1 is almost tight, as it can be achieved by BOBW-LIL’UCB(γ). Hence, up to terms logarithmic in the parameters such as L, we have quantiﬁed the Pareto frontier for the trade-off between RM and BAI in stochastic bandits.

7. Sketches of the Proofs
We sketch the analysis of Theorems 4.1 to 5.2. The detailed proofs are postponed to Appendices C and D.
7.1. Proof sketch of Theorem 4.1
First, as usual, we write the pseudo-regret as
L
RT = ∆1,i · E[Ni,T ].
i=2
It is clear that to upper bound RT , it sufﬁces to upper bound E[Ni,T ] for each item i = 1.

On the Pareto Frontier of Regret Minimization and Best Arm Identiﬁcation in Stochastic Bandits

Concentration. By the law of the iterated logarithm (see Theorem B.1), Ui,t,γ upper bounds wi for each item i with high probability during the whole horizon.
Sufﬁcient observations. Since (i) BOBW-LIL’UCB pulls the item with the largest Ui,t−1,γ at time step t > L and (ii) the conﬁdence radius Ci,t−1,γ = Ui,t−1,γ − wi shrinks as item i has more observations, when Ni,T is sufﬁciently large for item i = 1, we have Ci,t−1,γ < ∆1,i/2 and Ui,t−1,γ < U1,t−1,γ. Therefore, item i = 1 will no longer be pulled in the subsequent time steps. This observation allows us to provide an almost sure upper bound on Ni,T , which completes the analysis of the problem-dependent bound on the pseudo-regret.
Worst-case bound. To obtain this, we divide the ground set [L] into two classes depending on whether the optimality gap ∆1,i ≥ L/T , which leads to a worst-case bound on the pseudo-regret.
7.2. Proof sketch of Theorem 4.2
We upper bound the failure probability of BOBWLIL’UCB(γ) with a similar analysis to that for UCB-E(a) (Audibert and Bubeck, 2010). The key difference results from the different designs of conﬁdence radii in the two algorithms.
In short, we show that when the empirical mean gˆi,t approaches the mean wi for each item i ∈ [L] with high probability, all items are observed for sufﬁciently many times, and the algorithm can identify the optimal item. To analyze the number of times we observe each item, we delicately apply the method of induction twice.
7.3. Proof sketch of Theorems 5.1 and 5.2
Design of instances. To begin with, we design L instances such that the difference of instance 1 and 2 ≤ ≤ L is manifested in the different reward distributions of item .
The key difference between the analyses of Theorems 5.1 and 5.2 pertains to the different designs of instances. (i) For Theorem 5.1, we design instances with bounded rewards by scaling Bernoulli random variables. (ii) For Theorem 5.2, we design instances with bounded variances by using Gaussian distributions. With the judicious design of instances, we prove Theorems 5.1 and 5.2 using parallel lines of analyses.
Change of measure. For any instance 2 ≤ ≤ L, we apply the high-probability Pinsker inequality to lower bound the sum of the failure probabilities in instances 1 and in terms of the KL divergence.
Moreover, according to Lemma B.4, the KL divergence between instances 1 and depends on (i) the KL divergence between the reward distributions of item in the two instances and (ii) the expected number of observations of item

in instance 1.
By the pigeonhole principle, there exists an instance 2 ≤ 1 ≤ L such that we can upper bound the expected number of observations of item 1 with the pseudo-regret. Therefore, for algorithms with bounded BAI failure probabilities, we can derive a minimax lower bound over the L instances on their pseudo-regrets.
Conclusion. Lastly, we classify an instance depending on its (i) minimal optimality gap; (ii) support of rewards (Theorem 5.1) or variance of rewards (Theorem 5.2); and (iii) hardness. Over a suitably constructed set of instances, we establish a lower bound on the regrets of algorithms with bounded failure probabilities.
8. Numerical Experiments
We numerically compare BOBW-LIL’UCB(γ) and UCBα as they are the only algorithms that can be tuned to perform (near-)optimally for both RM and BAI. Since BOBWLIL’UCB(γ) is designed for the ﬁxed-budget setting and UCBα is for the ﬁxed-conﬁdence setting, there cannot be a completely fair comparison between them. However, we attempt to perform fair comparisons as much as possible.
For BOBW-LIL’UCB(γ), we ﬁx ε = 0.01, β = e, and vary γ. For UCBα, we ﬁx δ = 0.01 and vary α. We set w1 = 0.5, and wi = 0.5 − ∆ for all i = 1. We let Bern(a) denote the Bernoulli distribution with parameter a. We consider either Bernoulli or Gaussian bandits, i.e., νi = Bern(wi) or νi = N (wi, 1).
We run BOBW-LIL’UCB(γ) for T = 105 time steps, when the horizon (stopping time) of UCBα depends on its stopping rule and the instance. Due to the difference between the ﬁxed-horizon and ﬁxed-conﬁdence settings, the regrets of each algorithm may be accumulated over different time horizons. For each choice of algorithm and instance, we run 100 independent trials.
In our experiments, each algorithm can identify the optimal item with high probability (≥ 95%) in each instance. Therefore, we focus on the comparison on (i) the time horizon each algorithm runs; and (ii) the regret incurred over its corresponding horizon.
We present the averages and standard deviations of the time horizons and regrets of each algorithm in Figures 8.1 and 8.2. We display the numerical results for Bernoulli and Gaussian instances in Figures 8.1 and 8.2 respectively. More numerical results that reinforce the conclusions herein are presented in Appendix E.
Under each instance presented in Figures 8.1 and 8.2, the regret of BOBW-LIL’UCB(γ) is reduced when γ grows, which corroborates Theorem 4.1. Both the regret and the

Algorithm

Algorithm

On the Pareto Frontier of Regret Minimization and Best Arm Identiﬁcation in Stochastic Bandits

BoBW(10 1) BoBW(10 4) BoBW(10 7)
UCB1.5 UCB3 UCB6
0 BoBW(10 1) BoBW(10 4) BoBW(10 7)
UCB1.5 UCB3 UCB6
0.0

BoBW-lil'UCB( )

UCB

BoBW(10 1)

BoBW(10 4)

Algorithm

BoBW(10 7)

UCB1.5

UCB3

UCB3

UCB6

0 T2ime Ho4rizon 16e5 0 2 Regre4t 16e4

T2ime Hor4izon 16e5 0 1 Regre2t 1e34 BoBW(10 1)

Algorithm

UCB6

BoBW(10 4) BoBW(10 7)

UCB1.5

0.0

0.2

0.4

0.6

0.8

1.0

UCB3 UC1eB66

0i.m5 e Ho1r.i0zon 11.e55 0.0 0.5Regre1t.0 11e.45

0.0 T0i.m5 e Ho1r.iz0on 11e.55 0 1 Regret2 1e34

T

Figure 8.2. Gaussian instances with L = 64. Top: ∆ = 0.1;

Figure 8.1. Bernoulli instances with L = 64. Top: ∆ = 0.05; below: ∆ = 0.2.

below: ∆ = 0.1.

stopping time of UCBα grow with α, which corroborates Degenne et al. (2019, Theorem 3). Moreover, we observe that the standard deviations of the regrets are larger for UCBα compared to BOBW-LIL’UCB(γ), which suggests that BOBW-LIL’UCB(γ) is more statistically robust and consistent in terms of the regret.
Note that a larger ∆ means that the difference between the optimal and suboptimal items is more pronounced, resulting in an easier instance and commensurately better performances across all algorithms. Indeed, when ∆ grows, the average pseudo-regrets of both BOBW-LIL’UCB(γ) and UCBα decrease. Concerning the time horizon, for the Bernoulli instances presented in Figure 8.1, we observe that the average time horizon of UCBα is larger than 105 when ∆ = 0.05 and is about 105 when ∆ = 0.1. Thus, in a difﬁcult instance with a small ∆ (e.g. ∆ = 0.05), BOBWLIL’UCB(γ) can identify the best item with high probability within the ﬁxed horizon T , when UCBα takes longer to do so, and also suffers from a larger regret. This observation, which can also be gleaned from Figure 8.2, suggests that given a ﬁxed horizon T , BOBW-LIL’UCB(γ) is superior in handling difﬁcult instances (with small ∆) compared to UCBα.

both objectives. To mitigate this limitation, we propose BOBW-LIL’UCB, an algorithm that nearly achieves Paretooptimality in some parameter regimes.
In real-life applications, it may be unrealistic to obtain i.i.d. stochastic rewards. Hence, the stochastic bandit model cannot be directly applied. This brings the study of adversarial bandits (Auer et al., 2002b; Abbasi-Yadkori et al., 2018) to the fore. Here, the rewards of each item are not necessarily drawn independently from the same distribution. In adversarial bandits, while there exists a lower bound on the regret of any algorithm (Gerchinovitz and Lattimore, 2016), there is no lower bound on the failure probability for BAI. Furthermore, there is no existing analysis of a single algorithm that is applicable to both RM and BAI in adversarial bandits. This serves as an interesting direction for future work. More ambitiously, as Abbasi-Yadkori et al. (2018) studied BAI in both stochastic and adversarial bandits, we wonder whether it is possible to design one algorithm for both RM and BAI for both stochastic and adversarial bandits. Lastly, we surmise that the Pareto frontier of RM and BAI and the trade-off between exploitation and exploration in adversarial bandits are similar to those in their stochastic counterparts.

9. Conclusion and Future Work
In summary, we explore the Pareto frontier of RM and BAI over a ﬁxed horizon. The performance of our BOBWLIL’UCB algorithm sheds light on the different emphases of RM and BAI. More precisely, exploitation is more critical than exploration for achieving the optimality of RM, while exploration is more crucial for BAI. Moreover, we prove that no algorithm can simultaneously perform optimally for

On the Pareto Frontier of Regret Minimization and Best Arm Identiﬁcation in Stochastic Bandits

References
Abbasi-Yadkori, Y., Bartlett, P., Gabillon, V., Malek, A., and Valko, M. (2018). Best of both worlds: Stochastic & adversarial best-arm identiﬁcation. In Proceedings of the 31st Conference On Learning Theory, volume 75 of Proceedings of Machine Learning Research, pages 918–949. PMLR.
Agrawal, S. and Goyal, N. (2012). Analysis of Thompson sampling for the multi-armed bandit problem. In Proceedings of the 25th Annual Conference on Learning Theory, pages 39.1–39.26.
Agrawal, S. and Goyal, N. (2017). Near-optimal regret bounds for Thompson sampling. Journal of the ACM (JACM), 64(5):1–24.
Audibert, J.-Y. and Bubeck, S. (2010). Best arm identiﬁcation in multi-armed bandits. In Proceedings of the 23th Conference on Learning Theory, pages 41–53.
Auer, P., Cesa-Bianchi, N., and Fischer, P. (2002a). Finitetime analysis of the multiarmed bandit problem. Machine Learning, 47(2-3):235–256.
Auer, P., Cesa-Bianchi, N., Freund, Y., and Schapire, R. E. (2002b). The nonstochastic multiarmed bandit problem. SIAM Journal of Computing, 32(1):48–77.
Bubeck, S., Cesa-Bianchi, N., et al. (2012). Regret analysis of stochastic and nonstochastic multi-armed bandit problems. Foundations and Trends® in Machine Learning, 5(1):1–122.
Bubeck, S., Wang, T., and Viswanathan, N. (2013). Multiple identiﬁcations in multi-armed bandits. In Proceedings of the 30th International Conference on Machine Learning, pages 258–265.
Carpentier, A. and Locatelli, A. (2016). Tight (lower) bounds for the ﬁxed budget best arm identiﬁcation bandit problem. In Feldman, V., Rakhlin, A., and Shamir, O., editors, 29th Annual Conference on Learning Theory, volume 49 of Proceedings of Machine Learning Research, pages 590–604, Columbia University, New York, New York, USA. PMLR.
Darling, D. A. and Robbins, H. (1967). Iterated logarithm inequalities. Proceedings of the National Academy of Sciences of the United States of America, 57(5):1188.
Degenne, R., Nedelec, T., Calauzenes, C., and Perchet, V. (2019). Bridging the gap between regret minimization and best arm identiﬁcation, with application to A/B tests. In Proceedings of the 22nd International Conference on Artiﬁcial Intelligence and Statistics, pages 1988–1996.

Dubhashi, D. P. and Panconesi, A. (2009). Concentration of measure for the analysis of randomized algorithms. Cambridge University Press.
Gabillon, V., Ghavamzadeh, M., and Lazaric, A. (2012). Best arm identiﬁcation: A uniﬁed approach to ﬁxed budget and ﬁxed conﬁdence. In Proceedings of the 25th International Conference on Neural Information Processing Systems.
Garivier, A. and Cappe´, O. (2011). The kl-ucb algorithm for bounded stochastic bandits and beyond. In Proceedings of the 24th Annual Conference on Learning Theory, pages 359–376.
Gerchinovitz, S. and Lattimore, T. (2016). Reﬁned lower bounds for adversarial bandits. In Proceedings of the 29th International Conference on Neural Information Processing Systems, pages 1198–1206.
Go¨tze, F., Sambale, H., and Sinulis, A. (2019). Higher order concentration for functions of weakly dependent random variables. Electronic Journal of Probability, 24:1 – 19.
Jamieson, K., Malloy, M., Nowak, R., and Bubeck, S. (2014). lil’ucb: An optimal exploration algorithm for multi-armed bandits. In Proceedings of the 27th Conference on Learning Theory, pages 423–439.
Karnin, Z., Koren, T., and Somekh, O. (2013). Almost optimal exploration in multi-armed bandits. In Proceedings of the 13th International Conference on Machine Learning, pages 1238–1246.
Kaufmann, E. and Kalyanakrishnan, S. (2013). Information complexity in bandit subset selection. In Proceedings of the 26th Annual Conference on Learning Theory, pages 228–251.
Lai, T. L. and Robbins, H. (1985). Asymptotically efﬁcient adaptive allocation rules. Advances in applied mathematics, 6(1):4–22.
Lattimore, T. and Szepesva´ri, C. (2020). Bandit algorithms. Cambridge University Press.
Russo, D. and Van Roy, B. (2014). Learning to optimize via posterior sampling. Mathematics of Operations Research, 39(4):1221–1243.
Shahrampour, S., Noshad, M., and Tarokh, V. (2017). On sequential elimination algorithms for best-arm identiﬁcation in multi-armed bandits. IEEE Transactions on Signal Processing, 65(16):4281–4292.
Tsybakov, A. B. (2008). Introduction to Nonparametric Estimation. Springer Publishing Company, Incorporated, 1st edition.

On the Pareto Frontier of Regret Minimization and Best Arm Identiﬁcation in Stochastic Bandits
Zhong, Z., Cheung, W. C., and Tan, V. (2021). Probabilistic sequential shrinking: A best arm identiﬁcation algorithm for stochastic bandits with corruptions. In Proceedings of the 38th International Conference on Machine Learning.

On the Pareto Frontier of Regret Minimization and Best Arm Identiﬁcation in Stochastic Bandits

A. Detailed discussion on existing algorithms

While most existing works only aim to perform either RM or BAI, Degenne et al. (2019) designed and analyzed an algorithm called UCBα for both RM and BAI under the ﬁxed-conﬁdence setting. Given any δ, UCBα aims to minimize the number of time steps τ so that eτ ≤ δ, and, at the same time, the incurred regret Rτ can also be upper bounded. Therefore, the focus of Degenne et al. (2019) differs from that of our work. We aim to study the pseudo-regret of an algorithm which can identify the best item with high probability in a ﬁxed horizon T in this work.

To the best of our knowledge, their is no existing work that analyzes a single algorithm for both RM and BAI under the ﬁxed-budget setting. However, it is natural to question if an algorithm which is originally designed for RM can also perform well for BAI, and vice versa. We study some algorithms that are originally designed to achieve optimal performance for either RM or BAI.

RM. According to the discussions on RM and BAI in Lattimore and Szepesva´ri (2020) (see the second point in Note 33.3), for any algorithm with a regret that (nearly) matches the state-of-the-art lower bound (Carpentier and Locatelli, 2016):

lim inf RT (π) ≥

∆1,i ,

T →∞ log T i=1 KL(νi ν1)

we can construct two instances I and I with

w1I > w2I ≥ . . . ≥ wLI , wiI = wiI + ε(w1I − wiI ), for some ε > 0

such that

eT (π, I) + eT (π, I ) ≥ Ω(T (1+o(1))(1+ε)2 ).

(A.1)

This serves as a basic observation on the limitation for BAI of an algorithm that performs (near-)optimally for RM.

BAI. Audibert and Bubeck (2010) were the ﬁrst to explore the BAI problem under the ﬁxed-budget setting. Carpentier and Locatelli (2016) provided a lower bound on the failure probability of any algorithm.

In the spirit of UCB1 (Auer et al., 2002a), Audibert and Bubeck (2010) designed UCB-E for BAI. We let UCB-E(a) denote the UCB-E algorithm when it is run with parameter a. When T is sufﬁciently large, we can upper bound the pseudo-regret of UCB-E(α log T ) (α ≥ 2) with a similar analysis as that for UCB1 (see Proof of Theorem 1 in Auer et al. (2002a)). Besides, we can upper bound its failure probability with Theorem 1 in Audibert and Bubeck (2010).
Corollary A.1. Let α > 12.5. Assume that gi,t ∈ [0, 1] for all i ∈ [L], and α log T ≤ 25(T −L)/(36H2). UCB-E(α log T ) satisﬁes

RT ≤ 2α2

log T

π2

+ 1+ ·

∆1,i ,

i=1 ∆1,i

3 i=1

eT ≤ 2LT (1−2α/25).

When the horizon T grows, Corollary A.1 indicates that the BAI failure probability of UCB-E(α log T ) decays only
polynomially fast. In order to achieve the upper bound on eT as exp(−Θ(T )), we need to set α = O(T / log T ), and hence the regret bound as shown in Corollary A.1 will be O(T 2/ log T ), which is vacuous.

A.1. Existing results under the ﬁxed-budget setting of BAI

We abbreviate SEQUENTIAL REJECTS as SR, SEQUENTIAL HALVING as SH, NONLINEAR SEQUENTIAL ELIMINATION
with parameter p as NSE(p). Besides, we simplify the bounds for algorithms which were initially analyzed for more general problems than identiﬁcation of the optimal item i∗. we deﬁne

ip Hp := mi=a1x ∆2i ,

L
Cp := 2−p + i−p
i=2

for p > 0 as in Shahrampour et al. (2017). We let UGAPEB(a) denote the UGAPEB algorithm when it is run with parameter a. In Table A.1, We present existing bounds from some seminal works. The algorithms are listed in chronological order.

On the Pareto Frontier of Regret Minimization and Best Arm Identiﬁcation in Stochastic Bandits Table A.1. Comparison under the ﬁxed-budget setting of BAI: upper bounds for algorithms and lower bounds in stochastic bandits.

Algorithm/Instance 25(T − L)
UCB-E 36H2
SR T −L
UGAPEB 16H2
SAR SH NSE(p)
Stochastic Bandits

Reference

Failure probability eT

Audibert and Bubeck (2010)

T −L 2T L exp −
18H2

Audibert and Bubeck (2010) Gabillon et al. (2012)

L(L − 1) exp

T −L

− (1/2 +

L i=2

1/i)H2

T −L 2T L exp −
8H2

Bubeck et al. (2013) Karnin et al. (2013) Shahrampour et al. (2017)

2L2 exp

T −L

− 8(1/2 +

L i=2

1/i)H2

T 3 log2 L · exp − 8H2 log2 L

2(T − L) (L − 1) exp −
HpCp

Carpentier and Locatelli (2016)

1 exp
6

400T −
H2 log L

(Lower Bound)

As discussed in Shahrampour et al. (2017), HpCp ≤ H2 log L in some special cases. Therefore, SH is better than NSE(p) if we disregard the sub-exponential term, while NSE(p) is better in some cases in its dependence on the exponential term.
However, they are incomparable in general.

B. Useful facts

B.1. Concentration

Theorem B.1 (Non-asymptotic law of the iterated logarithm; Jamieson et al. (2014), Lemma 3). Let X1, X2, . . . be i.i.d. zero-mean sub-Gaussian random variables with scale σ > 0; i.e. E[eλXi ] ≤ exp(λ2σ2/2). For all ε ∈ (0, 1) and
γ ∈ (0, log(1 + ε)/e), we have

1τ

√

Pr ∀τ ≥ 1, τ

Xs ≤ σ(1 + ε)

s=1

2(1 + ε) · log
τ

log((1 + ε)τ ) γ

2+ε

γ

1+ε

≥1−

.

ε log(1 + ε)

Theorem B.2 (Multiplicative variant of the Chernoff-Hoeffding bound; Dubhashi and Panconesi (2009), Theorem 1.1).

Suppose that X1, . . . , XT are independent [0, 1]-valued random variables, and let X =

T t=1

Xt.

Then

for

all

ε

∈

(0,

1),

ε2 Pr(X − EX ≥ εEX) ≤ exp − 3 EX ,

ε2 Pr(X − EX ≤ −εEX) ≤ exp − 3 EX .

B.2. Change of measure
Lemma B.3 (Tsybakov (2008), Lemma 2.6). Let P and Q be two probability distributions on the same measurable space. Then, for every measurable subset A (whose complement we denote by A¯),
P (A) + Q(A¯) ≥ 1 exp(−KL(P Q)). 2
Lemma B.4 (Gerchinovitz and Lattimore (2016), Lemma 1). Consider two instances 1 and 2. We let Ni,t denote the number of pulls of item i up to and including time step t. Under instance j (j = 1, 2),

On the Pareto Frontier of Regret Minimization and Best Arm Identiﬁcation in Stochastic Bandits
• we let (gij,t)Tt=1 be the sequence of rewards of item i and ijt be the pulled item at time step t, and let Pj,i denote the distribution of the gain of item i;
• we assume {gtj = (g1j,t, g2j,t, . . . , gLj ,t)}Tt=1 is an i.i.d. sequence, i.e., gtj1 and gtj2 are i.i.d. for t1 = t2 but {gij,t}Li=1 can be independent.
• we let ijt be the pulled item at time step t, and let Pj denote the probability law of the process {{ijt , gijjt,t}}Tt=1.

Then, we have

KL(P1

L
P2) = EP1 [Ni,T ] · KL(P1,i
i=1

P2,i).

B.3. KL divergence

Theorem B.5 (Pinsker’s and reverse Pinsker’s inequality; Go¨tze et al. (2019), Lemma 4.1). Let P and Q be two distributions that are deﬁned in the same ﬁnite space A and have the same support. We have

δ(P, Q)2

≤

1 KL(P, Q)

≤

1 δ(P, Q)2

2

αQ

where δ(P, Q) = sup{ |P (A) − Q(A)| A ⊂ A} = 12 αQ = minx∈X:Q(x)>0 Q(x).

x∈A |P (x) − Q(x)| is the total variational distance, and

Lemma B.6 (KL divergence between two Gaussian distributions). Let P1 = N (µ1, σ12), P2 = N (µ2, σ22). Then

σ2

σ12 + (µ1 − µ2)2 1

KL(P1||P2) = log σ1 +

2σ2

−. 2

2

C. Analysis of BOBW-LIL’UCB(γ)

Proposition C.1 (Bounds on the pseudo-regret of BOBW-LIL’UCB(γ)). Assume the distribution νi is sub-Gaussian with scale σ > 0 for all i ∈ [L], and w1 ≥ w2 ≥ . . . ≥ wL. Let ε ∈ (0, 1), β ≥ e, and γ ∈ (0, log(β + 1 + ε)/e). The pseudo-regret of BOBW-LIL’UCB(γ) satisﬁes

RT ≤ O σ2(1 + ε)3 ·

log(1/γ) , RT ≤ O σ2(1 + ε)3√T L log log(T /Lγ) .

i:∆1,i>0 ∆1,i γ

√ Furthermore, we can set γ = 1/ T to obtain

RT ≤ O σ2(1 + ε)3 ·

log T , RT ≤ O σ2(1 + ε)3√T L log T .

i:∆1,i>0 ∆1,i

C.1. Proof of Theorem 4.1

Theorem 4.1 (Bounds on the pseudo-regret of BOBW-LIL’UCB). Let ε ∈ (0, 1), β ≥ e, and γ ∈ (0, log(β + 1 + ε)/e). The pseudo-regret of BOBW-LIL’UCB(γ) satisﬁes

RT ≤ O σ2 ·

log(1/γ) ,

i=1 ∆1,i

RT ≤ O σ2√T L log log(T /(Lγ)) . γ
√ Furthermore, we can set γ = 1/ T to obtain

RT ≤ O σ2 · log T , RT ≤ O σ2√T L log T . i=1 ∆1,i

On the Pareto Frontier of Regret Minimization and Best Arm Identiﬁcation in Stochastic Bandits

Proof. Recall that we assume w1 > w2 ≥ . . . ≥ wL. Therefore, item 1 is optimal and ∆1,j > 0 for all j = 1.

Step 1: Concentration. Let Ei,γ := {∀t ≥ L, |gˆi,t − wi| ≤ Ci,t,γ} for all i ∈ [L]. We apply Theorem B.1 to show that

L i=1

Ei,γ

holds

with

high

probability.

Lemma C.2 (Concentration of gˆi,t). Fix any ε ∈ (0, 1) and γ ∈ (0, log(β + 1 + ε)/e). We have

L

2L(2 + ε)

γ

1+ε

Pr Ei,γ ≥ 1 − ε

. log(1 + ε)

i=1

Step 2: Bound on Ni,T for i = 1. Next, for all t > L, when

{ gˆ1,t−1 > w1 − C1,t−1,γ , gˆi,t−1 < wi + Ci,t−1,γ , ∆1,i > 2Ci,t−1,γ , ∀i = 1}

holds, we have

{ U1,t−1,γ = gˆ1,t−1 + C1,t−1,γ > w1 = wi + ∆1,i > wi + 2Ci,t−1,γ > gˆi,t−1 + Ci,t−1,γ = Ui,t−1,γ ∀i = 1 },

which indicates it = 1. In other words, when it = i = 1 for t > L, one of the following holds:

gˆ1,t−1 ≤ w1 − C1,t−1,γ , gˆi,t−1 ≥ wi + Ci,t−1,γ , ∆1,i ≤ 2Ci,t−1,γ ,

We see that

√ ∆1,i ≤ 2Ci,t−1,γ = 10σ(1 + ε)

2(1 + ε) · log
Ni,t−1

log(β + (1 + ε)Ni,t−1) γ

200σ2(1 + √ε)2(1 + ε)

log(β + (1 + ε)N )

⇔ Ni,t−1 ≤

· log

i,t−1 .

∆21,i

γ

In order to bound Ni,t−1, we derive the following lemma: Lemma C.3. For all τ > 0, b ≥ e, we have

log(aτ + b)

1.4

1.4ac

τ ≤ c log

⇒ τ ≤ c log

log

+b .

ρ

ρ

ρ

We apply Lemma C.3 with 200σ2(1 + √ε)2(1 + ε)
c = ∆21,i ,

a = 1 + ε,

ρ = γ, and a1 = 1.4

to obtain

200σ2(1 + √ε)2(1 + ε)

Ni,t−1 ≤

∆21,i

· log

a1 log γ

200a1σ2(1 + √ε)2(1 + ε)2 + β ∆21,iγ

Therefore, when t > L,

L i=1

Ei,γ

holds

and

Ni,t−1

>

N¯i,γ

for

all

i

=

1,

we

always

have

it

=

1.

Step 3: Conclusion. Consequently,

:= N¯i,γ .

RT = E ≤E

T
g1,t − git,t
t=1

=E

T
g1,t − git,t · 1
t=1

T
g1,t − git,t
t=1

L
Ei,γ
i=1

+ Pr

L

·1

Ei,γ

i=1

L
Ei,γ
i=1

+E

T

L

g1,t − git,t · 1

Ei,γ

t=1

i=1

On the Pareto Frontier of Regret Minimization and Best Arm Identiﬁcation in Stochastic Bandits

≤E
j=1

T
g1,t − git,t
t=1

L
· 1 it = j, Ei,γ
i=1

+ Pr

L
Ei,γ
i=1

≤ ∆1,j · E
j=1

T
·1(it = j)
t=1

L
Ei,γ
i=1

+ Pr

L
Ei,γ
i=1

= ∆1,j · E Nj,T
j=1

L
Ei,γ
i=1

+ Pr

L
Ei,γ
i=1

L

≤ ∆1,j · (2 + N¯j,γ ) + Pr

Ei,γ

j=1

i=1

200σ2(1 + √ε)2(1 + ε)

= 2∆1,i +

· log ∆1,i

i=1

i=1

2L(2 + ε)

γ

1+ε

+

ε

log(1 + ε)

200σ2(1 + √ε)2(1 + ε)

= 2∆1,i +

· log ∆1,i

i=1

i=1

2L(2 + ε)

γ

1+ε

+

.

ε

log(1 + ε)

a1 log γ

200a1σ2(1 + √ε)2(1 + ε)2 + β ∆21,iγ

2a1 log γ

√

√

10 2a1 · σ(1 + ε)(1 + ε)

√

+β

∆1,i γ

We see that a1 = 1.4 ≤ 2. If we divide the ground set into two classes depending on whether ∆1,i ≥ L/T , we have

RT ≤ T ·

L

200Lσ2(1 + √ε)2(1 + ε)

+ 2L +

log

T

L/T

4 log
γ

√ 20σ(1 + ε)(1 + ε)
L/T √γ + β

2L(2 + ε)

γ

1+ε

+

ε

log(1 + ε)

= √T L · 1 + 200σ2(1 + √ε)2(1 + ε) log

4 log
γ

√ 20σ(1 + ε)(1 + ε)
+β γL/T

2L(2 + ε)

γ

1+ε

+ 2L +

.

ε

log(1 + ε)

In short, we have RT ≤ O σ2(1 + ε)3 · log(1/γ) , RT ≤ O σ2(1 + ε)3√T L log log(T /Lγ) . i=1 ∆1,i γ

√ Let γ = 1/ T , we have
RT ≤ O σ2(1 + ε)3 · log T , RT ≤ O σ2(1 + ε)3√T L log T . i=1 ∆1,i

C.2. Proof of Theorem 4.2

Theorem 4.2 (Bounds on the failure probability of BOBW-LIL’UCB). Let ε ∈ (0, 1), β ≥ e, and γ ∈ (0, log(β + 1 + ε)/e).

Let ∆i = max{∆, ∆1,i} for all i ∈ [L]. The failure probability of BOBW-LIL’UCB(γ) satisﬁes

2L(2 + ε)

γ

1+ε

eT ≤ ε log(1 + ε) , (4.1)

On the Pareto Frontier of Regret Minimization and Best Arm Identiﬁcation in Stochastic Bandits

if

T − L L 72σ2

2.8

11σ(1 + ε)2

(1 + ε)3 ≥ i=1 ∆2i · log γ2 log

+β . ∆i

In particular, the bound on eT in (4.1) holds when γ ≥ γ1(∆, H2), where

γ1(∆, H2) =

2.8 log

√ 6 2.8σ(1 + ε)2
+β ∆

· exp

T −L − 144σ2(1 + ε)3(H2 + ∆−2) .

When γ assumes its lower bound γ1(∆, H2), we have

eT ≤ O˜ L exp −

T −L

.

144σ2(1 + ε)2(H2 + ∆−2)

(4.2)

Proof. Recall that we assume w1 > w2 ≥ . . . ≥ wL. We let ∆1 = w1 − w2 and ∆i = w1 − wi for all i = 1. Then ∆ = ∆1 and ∆1,i = ∆i for i = 1.

Step 1: Concentration. Let Ei,γ := {∀t ≥ L, |gˆi,t − wi| ≤ Ci,t,γ/5} for all i ∈ [L]. Similarly to Lemma C.2, we can apply Theorem B.1 to show that

L

2L(2 + ε)

γ

1+ε

Pr

Ei,γ ≥ 1 − ε

. log(1 + ε)

i=1

In the following, we prove that conditioning on the event

L i=1

Ei,γ

, we have iout = 1, which concludes the proof.

We assume

L i=1

Ei,γ

holds

from

now

on.

Since

iout

is

the

item

with

the

largest

empirical

mean,

we

have

gˆiout,T ≥ gˆi,t ∀i = iout, gˆiout,T ≥ wiout − Ciout,T,γ /5, wi + Ci,T,γ /5 ≥ gˆi,t ∀i = iout.

Consequently, to show iout = 1, it is sufﬁcient to show that

Ci,T,γ ≤ ∆i ⇔ ∆i ≥ 2Ci,T,γ = 2σ(1 + √ε)

5

2

5

2(1 + ε) · log
Ni,T

log(β + (1 + ε)Ni,T ) γ

8σ2(1 + √ε)2(1 + ε) ⇔ Ni,T ≥ ∆2i · log

log(β + (1 + ε)Ni,T ) γ

∀i ∈ [L].

(C.1)

Step 2: Upper bound Ni,T (i = 1). To begin with, we prove by induction that

72σ2(1 + √ε)2(1 + ε) Ni,t ≤ ∆2i · log

a1 log γ

72a1σ2(1 + √ε)2(1 + ε)2 + β ∆2i γ

+ 1 ∀i = 1.

(C.2)

Clearly, this inequality holds for all i = 1 when 1 ≤ t ≤ L. Now we assume that the inequality holds for all i = 1 at time t − 1(t > L). If it = i, we have Ni,t = Ni,t−1 and the inequality still holds for i. Otherwise, we have it = i and in particular Ui,t−1,γ ≥ U1,t−1,γ . Since

Ui,t−1,γ = gˆi,t−1 + Ci,t−1,γ ≤ wi + 6Ci,t−1,γ , 5

U1,t−1,γ = gˆ1,t−1 + C1,t−1,γ ≥ w1 + 4C1,t−1,γ ≥ w1 = wi + ∆i, 5

we have

6C

72σ2(1 + √ε)2(1 + ε)

i,t−1,γ ≥ ∆i ⇔ Ni,t−1 ≤

· log

log(β + (1 + ε)Ni,t−1)

5

∆2i

γ

(a)

72σ2(1 + √ε)2(1 + ε)

a

72a σ2(1 + √ε)2(1 + ε)2

⇒ Ni,t−1 ≤

· log 1 log

1

+β .

∆2i

γ

∆2i γ

We obtain (a) using Lemma C.3 with a1 = 1.4:

On the Pareto Frontier of Regret Minimization and Best Arm Identiﬁcation in Stochastic Bandits

Lemma C.3. For all τ > 0, b ≥ e, we have

log(aτ + b)

1.4

1.4ac

τ ≤ c log

⇒ τ ≤ c log

log

+b .

ρ

ρ

ρ

Subsequently, by using Ni,t = Ni,t−1 + 1, we obtain (C.2). Step 3: Lower bound Ni,T (i = 1). Next, we again prove by induction that

Ni,t ≥ 200σ2(1 + ε)(1 + √ε)2 · log

log(β + (1 + ε)Ni,t) γ

· min

1 25∆2i ,

1 36(C1,t−1,γ )2 ,

∀i = 1.

(C.3)

Clearly, this inequality holds for all i = 1 when 1 ≤ t ≤ L. Now we assume that these inequalities hold for all i = 1 at time t − 1(t > L). If it = 1, we have
Ni,t ≥ Ni,t−1 ∀i = 1, N1,t = N1,t−1,

which implies that the inequalities still hold for all i = 1. Otherwise, it = 1 indicates that U1,t−1,γ ≥ Ui,t−1,γ for all i = 1. Since

U1,t−1,γ = gˆ1,t−1 + C1,t−1,γ ≤ w1 + 6C1,t−1,γ , 5

Ui,t−1,γ = gˆi,t−1 + Ci,t−1,γ ≥ wi + 4Ci,t−1,γ , 5

we have

4Ci,t−1,γ ≤ ∆i + 6C1,t−1,γ

5

5

√ ⇔ Ci,t−1,γ = 5σ(1 + ε)

2(1 + ε) · log
Ni,t−1

log(β + (1 + ε)Ni,t−1) γ

≤ 5∆i + 6C1,t−1,γ 4

√

20σ(1 + ε)

⇔

·

5∆i + 6C1,t−1,γ

log log(β + (1 + ε)Ni,t−1) ≤ γ

Ni,t−1 2(1 + ε)

⇔ 400σ2(1 + √ε)2 · log log(β + (1 + ε)Ni,t−1) ≤ Ni,t−1

(5∆i + 6C1,t−1,γ )2

γ

2(1 + ε)

800σ2(1 + ε)(1 + √ε)2

log(β + (1 + ε)N )

⇔ Ni,t−1 ≥

· log

i,t−1 .

(5∆i + 6C1,t−1,γ )2

γ

We apply u + v ≤ 2 max{u, v} and Ni,t = Ni,t−1 for all i = 1 to obtain (C.3).

Step 4: Lower bound on N1,T . Recall that we want to show (C.1). (i) To show (C.1) holds for all i = 1, (C.3) indicates that it is sufﬁciently to show that

200σ2(1 + ε)(1 + √ε)2

36(C1,T −1,γ )2

· log

log(β + (1 + ε)Ni,T ) γ

8σ2(1 + √ε)2(1 + ε) ≥ ∆2i · log

log(β + (1 + ε)Ni,T ) . γ

Moreover, since ∆1 = min ∆i, it is sufﬁcient to show
i∈[L]

25

1

5∆1

36(C1,T −1,γ )2 ≥ ∆2 ⇔ C1,T −1,γ ≤

. 6

1

(ii) In order to show (C.1) holds for all i ∈ [L], it is sufﬁcient to show that

C1,T −1,γ ≤ 5∆1 . 6

This is implied by 72σ2(1 + √ε)2(1 + ε)
N1,T −1 ≥ ∆2i · log

a1 log γ

72a1βσ2(1 + √ε)2(1 + ε)2 ∆2i γ

On the Pareto Frontier of Regret Minimization and Best Arm Identiﬁcation in Stochastic Bandits

72σ2(1 + √ε)2(1 + ε)

a

72a σ2(1 + √ε)2(1 + ε)2

⇔ N1,T ≥

· log 1 log

1

+ β + 1.

∆2i

γ

∆2i γ

We obtain the inequality in the above display by applying Lemma C.3. Meanwhile, (C.2) and t =

L i=1

Ni,t

implies

that

72σ2(1 + √ε)2(1 + ε)

a

72a σ2(1 + √ε)2(1 + ε)2

N1,T = T − Ni,T ≥ T − (L − 1) −

· log 1 log

1

+β .

i=1 i=1 ∆2i γ ∆2i γ

Altogether, we complete the proof with L 72σ2(1 + √ε)2(1 + ε) i=1 ∆2i · log

a1 log γ

72a1σ2(1 + √ε)2(1 + ε)2 + β ∆2i γ

≤ T − L + 1.

(C.4)

Step 5: Conclusion. Since

72σ2(1 + √ε)2(1 + ε) · log a1 log 72a1σ2(1 + √ε)2(1 + ε)2 + β

∆2i γ √ ∆2i γ

72σ2(1 + ε)3

2a1

6 2a1 · σ(1 + ε)2

≤ ∆2i · log γ2 log

+β , ∆i

To show (C.4), it is sufﬁcient to have

L 72σ2(1 + ε)3 i=1 ∆2i · log

2a1 log γ2

6√2a1 · σ(1 + ε)2 +β
∆i

≤T −L+1

⇔ L 72σ2(1 + ε)3 · log 2a1 ≤ T − L + 1 − L 72σ2(1 + ε)3 · log log 6√2a1 · σ(1 + ε)2 + β

i=1 ∆2i γ2 i=1 ∆2i ∆i

√

T − L + 1 − L 72σ2∆−2 · (1 + ε)3 log log 6√2a1·σ(1+ε)2 + β

⇔ γ ≥ 2a1 · exp −

i=1

i

∆i

.

L i=1

144σ2(1

+

ε)3∆−i 2

Recall the deﬁnition of H2 in (2.1):

1 H2 = i=1 ∆21,i .

Furthermore, it sufﬁces to have

√ γ ≥ 2a1 · exp
= 2.8 · log

T −L

1 6√2a1 · σ(1 + ε)2

− 144σ2(1 + ε)3(H2 + 1/∆2

+ log )2

log

+β ∆1,2

1,2

√

6 2.8 · σ(1 + ε)2

T −L

∆1,2

+ β exp − 144σ2(1 + ε)3(H2 + 1/∆2 ) := γ1(∆1,2, H2).

1,2

Note that ∆ = ∆1,2. When γ = γ1(∆, H2),

2L(2 + ε) γ1(∆, H2) 1+ε

eT ≤ ε

log(1 + ε)

2L(2 + ε) = ε[log(1 + ε)]1+ε · 2.8 log

√ 6 2.8 · σ(1 + ε)2
+β ∆

(1+ε)/2
· exp

T −L − 144σ2(1 + ε)2(H2 + 1/∆2) .

On the Pareto Frontier of Regret Minimization and Best Arm Identiﬁcation in Stochastic Bandits

C.3. Proof of Lemma C.2 Lemma C.2 (Concentration of gˆi,t). Fix any ε ∈ (0, 1) and γ ∈ (0, log(β + 1 + ε)/e). We have

L

2L(2 + ε)

γ

1+ε

Pr Ei,γ ≥ 1 − ε

. log(1 + ε)

i=1

Proof. Let

Ei,γ := {∀t ≥ L, |gˆi,t − wi| ≤ Ci,t,γ }.

Then

√ Pr(Ei,γ) = Pr ∀t ≥ L, gˆi,t − wi ≤ 5σ(1 + ε)

2(1 + ε) · log
Ni,t

log(β + (1 + ε)Ni,t−1) γ

= Pr ∀Ni,t ≥ 1,

1t Ni,t gi,u · 1{iu = i}
u=1

√ − wi ≤ 5σ(1 + ε)

2(1 + ε) · log
Ni,t

log(β + (1 + ε)Ni,t) γ

When ε ∈ (0, 1) and γ ∈ (0, log(β + 1 + ε)/e), Theorem B.1 indicates that

2(2 + ε)

γ

1+ε

Pr(Ei,γ) ≥ 1 − ε log(1 + ε) .

Furthermore,

L

L

L

Pr

Ei,γ = 1 − Pr

Ei,γ = 1 − Pr

Ei,γ

i=1

i=1

i=1

L
≥ 1 − Pr Ei,γ
i=1

2L(2 + ε) ≥1−
ε

γ

1+ε

.

log(1 + ε)

C.4. Proof of Lemma C.3 Lemma C.3. For all τ > 0, b ≥ e, we have

log(aτ + b)

1.4

1.4ac

τ ≤ c log

⇒ τ ≤ c log

log

+b .

ρ

ρ

ρ

Proof. Let

log(aτ + b)

a1

a2c

f (τ ) = c log

ρ

,

τa1,a2 = c log

log ρ

+b ρ

.

Then

a1

a2c

1

a1

a2c

τa1,a2 ≥ f (τa1,a2 ) ⇔ c log

log ρ

+b ρ

≥ c log log ac log log

ρ

ρ

ρ

+b

⇔ a1 log a2c + b ≥ log ac log a1 log a2c

ρ

ρ

ρ

+b .

On the Pareto Frontier of Regret Minimization and Best Arm Identiﬁcation in Stochastic Bandits

Let a1 ≥ 1.4, then xa1 ≥ x log x for all x ≥ 1. To obtain τa1,a2 ≥ f (τa1,a2 ), it sufﬁces to have

a2c + b · log a2c + b ≥ ac log a1 log a2c + b,

ρ

ρ

ρ

ρ

which is implied by

a2c · log a2c + b ≥ aca1 log a2c .

ρ

ρ

ρ

ρ

The last inequality holds when a2 ≥ a · a1. Since τ − f (τ ) is monotonically increasing in τ , and τa1,a·a1 ≥ f (τa1,a·a1 ), i.e., τa1,a·a1 − f (τa1,a·a1 ) ≥ 0, we have

τ ≥ τ1.4,1.4a ⇒ τ − f (τ ) ≥ 0.

In other words,

1.4

1.4ac

τ ≤ f (τ ) ⇒ τ ≤ τ1.4,2.8 = c log ρ log ρ .

D. Analysis of the Pareto frontier of RM and BAI

D.1. Proof of Theorem 5.1

Theorem 5.1.

Let φT , ∆, R, H2

> 0. Let π be any algorithm with eT (π, I) ≤ exp(−φT )/4 for all I (L − 1)R
sup RT (π, I) ≥ φT · 8∆ ,
I ∈B1 (∆,R)
3
∆H 2 R sup RT (π, I) ≥ φT · 8 .
I ∈B2 (∆,R,H 2 )

∈ B1(∆, R). Then

Proof. Step 1: Construct instances. To begin with, we ﬁx any σ > 0, d ∈ (0, 1/4] for all 2 ≤ ≤ L. We let Bern(a) denote the Bernoulli distribution with parameter a. We deﬁne the following distributions:

ν1 := Bern(1/2), ν1 := Bern(1/2),

ν := Bern(1/2 − d ) ν := Bern(1/2 + d )

∀1 < ∀1 <

≤ L; ≤ L.

We construct L instances such that under instance (1 ≤ ≤ L), the stochastic reward of item i is drawn from distribution

νi := b · (νi1{i = } + νi1{i = }),

where b > 0. Under instance (1 ≤ ≤ L), we see item is optimal, and we deﬁne several other notations as follows:

(i) We let gi,t be the random reward of item i at time step t. Then gi,t ∈ {0, b}.

(ii) We let ∆i,j := E[

T t=1

gi,t

−

gk,t]/T

denote

the

gap

between

item

i

and

j.

Then

∆11,j = b · dj ∀2 ≤ j ≤ L, ∆ ,1 = b · d , ∆ ,j = b · d + b · dj ∀2 ≤ j, ≤ L, j = .

(iii) We denote the difﬁculty of the instance with

H2( ) := (∆ ,j )−2.
j=

Then H2(1) = max H2( ) ≤ (L − 1)b−2 · max d−2.

1≤ ≤L

2≤ ≤L

On the Pareto Frontier of Regret Minimization and Best Arm Identiﬁcation in Stochastic Bandits

(iv) We let it be the pulled item at time step t, and Ot = {iu, giu,u}tu=1 be the sequence of pulled items and observed rewards up to and including time step t.
(v) We let Pt be the measure on Ot, and let P ,i be the measure on the rewards of item i.

For simplicity, we abbreviate PT , OT as P , O respectively. Moreover, we let Ni(t) denote the number of pulls of item i up to and including time step t.

Step 2: Change of measure. First of all, we apply Lemmas B.3 and B.4 obtain that for all 1 ≤ ≤ L,

1

1

PO1r(iout = 1) + POr(iout = 1) ≥ 2 exp(−KL(P1 P )) = 2 exp(−EP1 [N (T )] · KL(P1,

P , )).

Suppose the expected pseudo-regret is upper bounded by Reg, we have

Reg ≥ EP1

T
1{i1t = 1} · (g11,t − gi1t,t)
t=1

T
= EPt1 [1{i1t = 1} · (g11,t − gi1t,t)]
t=1

LT

LT

= EPt1 [1{i1t = } · (g11,t − gi1t,t)] = EPt1 [(g11,t − gi1t,t)|i1t = ] · EPt1 [1{i1t = }]

=2 t=1

=2 t=1

LT

L

=

∆11, · EPt [1{i1t = }] = b · d · EP1 [N (T )].

1

=2 t=1

=2

Since H2( ) = j= (∆ ,j)−2, we have

Reg =
H2(1)

L=2 b · d · EP1 [N (T )]

L (∆1 )−2

=

=2 1,j

L=2 b · d · EP1 [N (T )] b−3 · L d−2 .
=2 j

Thus, by the pigeonhole principle, there exists 2 ≤ 1 ≤ L such that

b3d3 E · [N (T )] = b · d 1 · EP1 [N 1 (T )] ≤ Reg ⇔ E [N (T )] ≤

Reg .

1 P1

1

b−2 · d−2

H2(1)

P1

1

b3d3 H2(1)

1

1

Since d ∈ (0, 1/4] for all 1 ≤ ≤ L, we apply Theorem B.5 to obtain

1 PO1r(iout = 1) + OPr1 (iout = 1) ≥ 2 exp(−EP1 [N 1 (T )] · KL(P1, 1

1 P 1, 1 )) ≥ 2 exp

Reg

(2d )2

− b3d3 H (1) ·

1
1/4

.

12

Since PrO j (iout = 1) ≥ PrO 1 (iout = 1), we have

1

8Reg

max Pr(iout = ) ≥ exp

1≤ ≤L O

4

− H2(1)b3 · min d

.

2≤ ≤L

Step 3: Conclusion. We deﬁne Suppose algorithm π satisﬁes that

2 := arg max Pr(iout = ).
1≤ ≤L O

1

Pr (iout = 2) ≤ exp(−φT ),

O2

4

then we can lower bound its pseudo-regret as follows:

H2(1)b3 · min d

RT ≥ φT ·

2≤ ≤L .

8

On the Pareto Frontier of Regret Minimization and Best Arm Identiﬁcation in Stochastic Bandits

When d = d > 0 for all 2 ≤ ≤ L, we have H2(1) = (L − 1)/(b2d2).
Step 4: Classiﬁcation of instances. Suppose algorithm π satisﬁes that eT (π) ≤ exp(−φT )/4. Let B1(∆, R) denote the set of stochastic instances where (i) the minimal optimality gap ∆ ≥ ∆; and (ii) there exists R0 ∈ R such the rewards are bounded in [R0, R0 + R]. Then

(L − 1)R sup RT (π, I) ≥ φT · 8∆
I ∈B1 (∆,R)

∀∆, R > 0.

Let B2(∆, R, H2∗) denote the set of stochastic instances that (i) belong to B1(∆, R), and (ii) are with hardness parameter H2 ≤ H2∗. Then, we have

∆H ∗ R3 sup RT (π, I) ≥ φT · 2 I∈B2(∆,R,H2∗) 8

∀∆, R, H2∗ > 0.

D.2. Proof of Theorem 5.2

Theorem 5.2. Let φT , ∆, V , H2 > 0. Let π be any algorithm with eT (π, I) ≤ exp(−φT )/4 for all I ∈ B1(∆, V ). Then (L − 1)V
sup RT (π, I) ≥ φT · 2∆ ,
I∈B1(∆,V )
sup RT (π, I) ≥ φT · ∆H2V . I∈B2(∆,V ,H2) 2

Proof. Step 1: Construct instances. To begin with, we ﬁx any σ > 0, d > 0 for all 2 ≤ ≤ L. We deﬁne the following distributions:

ν1 := N (1/2, σ2), ν1 := N (1/2, σ2),

ν := N (1/2 − d , σ2) ν := N (1/2 + d , σ2)

∀1 < ≤ L; ∀1 < ≤ L.

We construct L instances such that under instance (1 ≤ ≤ L), the stochastic reward of item i is drawn from distribution

νi := νi1{i = } + νi1{i = }.

Under instance (1 ≤ ≤ L), we see item is optimal, and we deﬁne several other notations as follows:

(i) We let gi,t be the random reward of item i at time step t.

(ii) We let ∆i,j := E[

T t=1

gi,t

−

gk,t]/T

denote

the

gap

between

item

i

and

j.

Then

∆11,j = dj ∀2 ≤ j ≤ L, ∆ ,1 = d , ∆ ,j = d + dj ∀2 ≤ j, ≤ L, j = .

(iii) We denote the difﬁculty of the instance with

H2( ) := (∆ ,j )−2.
j=

Then H2(1) = max H2( ) ≤ (L − 1) · max d−2.

1≤ ≤L

2≤ ≤L

(iv) We let it be the pulled item at time step t, and Ot = {iu, giu,u}tu=1 be the sequence of pulled items and observed rewards up to and including time step t.

(v) We let Pt be the measure on Ot, and let P ,i be the measure on the rewards of item i.

On the Pareto Frontier of Regret Minimization and Best Arm Identiﬁcation in Stochastic Bandits

For simplicity, we abbreviate PT , OT as P , O respectively. Moreover, we let Ni(t) denote the number of pulls of item i up to and including time step t.

Step 2: Change of measure. First of all, we apply Lemmas B.3 and B.4 obtain that for all 1 ≤ ≤ L,

1

1

PO1r(iout = 1) + POr(iout = 1) ≥ 2 exp(−KL(P1 P )) = 2 exp(−EP1 [N (T )] · KL(P1,

P , )).

Suppose the expected pseudo-regret is upper bounded by Reg, we have

Reg ≥ EP1

T
1{i1t = 1} · (g11,t − gi1t,t)
t=1

T
= EPt1 [1{i1t = 1} · (g11,t − gi1t,t)]
t=1

LT

LT

= EPt1 [1{i1t = } · (g11,t − gi1t,t)] = EPt1 [(g11,t − gi1t,t)|i1t = ] · EPt1 [1{i1t = }]

=2 t=1

=2 t=1

LT

L

=

d · EPt [1{i1t = }] = d · EP1 [N (T )].

1

=2 t=1

=2

Since H2( ) = j= (∆ ,j)−2, we have

Reg =
H2(1)

L=2 d · EP1 [N (T )] L (∆1 )−2 =
=2 1,j

L=2 d · EP1 [N (T )] .

L =2

d−j 2

Thus, by the pigeonhole principle, there exists 2 ≤ 1 ≤ L such that

d3 E · [N (T )] = d 1 · EP1 [N 1 (T )] ≤ Reg ⇔ E [N (T )] ≤

Reg .

1 P1

1

d−2

H2(1)

P1

1

d3 H2(1)

1

1

Further, we apply Lemma B.6 to obtain 1
PO1r(iout = 1) + OPr1 (iout = 1) ≥ 2 exp(−EP1 [N 1 (T )] · KL(P1, 1

1 P 1, 1 )) ≥ 2 exp

Reg (2d )2

− d3 H (1) ·

1
2σ2

.

12

Since PrO j (iout = 1) ≥ PrO 1 (iout = 1), we have

1

2Reg

max Pr(iout = ) ≥ exp

1≤ ≤L O

4

− H2(1)σ2 · min d

.

2≤ ≤L

Step 3: Conclusion. We deﬁne Suppose algorithm π satisﬁes that

2 := arg max Pr(iout = ).
1≤ ≤L O

1

Pr (iout = 2) ≤ exp(−φT ),

O2

4

then we can lower bound its pseudo-regret as follows:

When d = d > 0 for all 2 ≤

H2(1)σ2 · min d

RT ≥ φT ·

2≤ ≤L .

2

≤ L, we have H2(1) = (L − 1)/d2.

On the Pareto Frontier of Regret Minimization and Best Arm Identiﬁcation in Stochastic Bandits

Step 4: Classiﬁcation of instances. Suppose algorithm π satisﬁes that eT (π) ≤ exp(−φT )/4. Let B1(∆, V ) denote the set of stochastic instances where (i) the minimal optimality gap ∆ ≥ ∆; (ii) for each item i, the variance σi2 ≤ V . Then

(L − 1)V sup RT (π, I) ≥ φT · 2∆
I∈B1(∆,V )

∀∆, V > 0.

Let B2(∆, V , H2) denote the set of stochastic instances (i) that belong to B1(∆, V ), and (ii) are with the hardness H2 ≤ H2. We have

sup RT (π, I) ≥ φT · ∆H2V I∈B2(∆,V ,H2) 2

∀∆, V , H2 > 0.

D.3. Proof of Corollary 6.1 √
Corollary 6.1. Let π1 be the online algorithm BOBW-LIL’UCB(1/ T ) and π2 be the online algorithm BOBW-
LIL’UCB(γ1(∆, H2)). Then

sup RT (π1, I)
I ∈B2 (∆,1,H 2 )

T ∈ Ω ∆H2 log L

L log T

O

,

∆

sup RT (π2, I)
I ∈B2 (∆,1,H 2 )

∈ Ω (T − L) · ∆−2H2 (H2 + ∆ )

L(T − L) O ∆(H2 + ∆−2) .

Proof. We consider the stochastic instances in B2(∆, 1, H2). By the classiﬁcation of instances in Theorem 5.1, these instances satisfy the conditions

gi,t ∈ [0, 1] ∀i, t, and H2 ≤ H2.

Therefore, the distribution νi is sub-Gaussian with scale σ = 1/2 for all i ∈ [L]. We assume T is sufﬁciently large such that

1 √ ≥ γ1 =
T

2.8 log

√ 6 2.8σ(1 + ε)2
+β ∆

√

3 2.8(1 + ε)2

= 2.8 log

+ β · exp

∆

· exp

T −L − 144σ2(1 + ε)3(H2 + 1/∆2)

T −L − 36(1 + ε)3(H2 + ∆−2) .

As a result, for all instance in B2(∆, 1, H2), since ∆ ≥ ∆ and H2 ≤ H2, we have

1 √≥
T

2.8 log

√ 3 2.8(1 + ε)2
+β ∆

· exp

T −L − 36(1 + ε)3(H2 + ∆−2) .

√ We let π1 denote BOBW-LIL’UCB(1/ T ) and π2 denote BOBW-LIL’UCB(γ1). One one hand, ﬁx any instance in B2(∆, 1, H2), Theorem 4.1 implies that

RT (π1) ∈ O (1 + ε)3H1 log T ,

and

RT (π2) ∈ O (1 + ε)3 ·

log(1/γ1) = O ∆1,i

1

T −L

∆1,i · H2 + ∆−2 .

i=1

i=1

On the Pareto Frontier of Regret Minimization and Best Arm Identiﬁcation in Stochastic Bandits

On the other hand, Theorem 4.2 implies that

2L(2 + ε) eT (π1) ≤ ε
2L(2 + ε) eT (π2) ≤ ε

1 log(1 + ε)
1 log(1 + ε)

1+ε
· T −(1+ε)/2,

1+ε
· 2.8 log

√ 3 1.4(1 + ε)2
+β ∆

(1+ε)/2
· exp

T −L − 36(1 + ε)2(H2 + ∆−2) .

Moreover, we can apply Theorem 5.1 to obtain that

T sup RT (π1, I) ∈ Ω (1 + ε) · ∆H2 log L
I ∈B2 (∆,1,H 2 )
(T − L) · ∆H2 I∈B2s(u∆p,1,H2) RT (π2, I) ∈ Ω (1 + ε)2(H2 + ∆−2) .

Altogether, we have

sup RT (π1, I) ∈ Ω
I ∈B2 (∆,1,H 2 )
sup RT (π2, I) ∈ Ω
I ∈B2 (∆,1,H 2 )

T (1 + ε) · ∆H2 log L
(T − L) · ∆H2 (1 + ε)2(H2 + ∆−2)

(1 + ε)3(L − 1) log T

O

,

∆

(L − 1)(T − L) O ∆(H2 + ∆−2) .

Neglecting the constant terms in 1 + ε completes the proof of Corollary 6.1.

E. Additional Numerical Results

In this section, we present more numerical results for a larger instance with L = 128 items. These ﬁgures yield the same conclusions as in Section 8. In particular, we see that when the gap ∆ is small (Figs. E.1 and E.4), BOBW-LIL’UCB(γ) performs much better, in terms of the regret and the average stopping time, compared to its closest competitor UCBα. When the gap is large, however, UCBα can outperform BOBW-LIL’UCB(γ). Hence, these two algorithms work well in different regimes but since stochastic bandits with small gaps correspond to “harder”, and thus more interesting, instances, BOBW-LIL’UCB(γ) is arguably a fundamental contribution to the study of stochastic bandits. Additionally, as we have shown, BOBW-LIL’UCB(γ) almost achieves the Pareto frontier of RM and BAI.

BoBW-lil'UCB( )

UCB

Algorithm

BoBW(10 1) BoBW(10 4) BoBW(10 7)
UCB1.5 UCB3 UCB6
0.0

UCB3
Ti0m.5e Horiz1o.n0 1e6 0

2 Regre4t 61e4

Figure E.1. L = 128, ∆ = 0.05, νi = Bern(wi).

UCB6

0.0 0.2 0.4 0.6 0.8 1.0 1e6

On the Pareto Frontier of Regret Minimization and Best Arm Identiﬁcation in Stochastic Bandits

Algorithm

BoBW(10 1) BoBW(10 4) BoBW(10 7)
UCB1.5 UCB3 UCB6
0

T1ime Hor2izon 13e5 0

1 Regre2t 13e4

Figure E.2. L = 128, ∆ = 0.1, νi = Bern(wi).

Algorithm

BoBW(10 1) BoBW(10 4) BoBW(10 7)
UCB1.5 UCB3 UCB6
0.0

Time0H.5orizon 11e.50 0

Re1gret 1e24

Figure E.3. L = 128, ∆ = 0.2, νi = Bern(wi).

Algorithm

BoBW(10 1) BoBW(10 4) BoBW(10 7)
UCB1.5 UCB3 UCB6
0.0

Ti0m.5e Horiz1o.n0 1e6 0.0

0.R5egret 1.0 1e5

Figure E.4. L = 128, ∆ = 0.1, νi = N (wi, 1).

Algorithm

BoBW(10 1) BoBW(10 4) BoBW(10 7)
UCB1.5 UCB3 UCB6
0

T1ime Hor2izon 13e5 0

2 Regre4t 16e4

Figure E.5. L = 128, ∆ = 0.2, νi = N (wi, 1).

On the Pareto Frontier of Regret Minimization and Best Arm Identiﬁcation in Stochastic Bandits

Algorithm

BoBW(10 1) BoBW(10 4) BoBW(10 7)
UCB1.5 UCB3 UCB6
0.0

Time0H.5orizon 11e.50 0

Re2gret 1e44

Figure E.6. L = 128, ∆ = 0.4, νi = N (wi, 1).

