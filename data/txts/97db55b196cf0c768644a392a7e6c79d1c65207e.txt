1
Improving Frame-Online Neural Speech Enhancement with Overlapped-Frame Prediction
Zhong-Qiu Wang and Shinji Watanabe

arXiv:2204.07566v1 [cs.SD] 15 Apr 2022

Abstract—Frame-online speech enhancement systems in the short-time Fourier transform (STFT) domain usually have an algorithmic latency equal to the window size due to the use of the overlap-add algorithm in the inverse STFT (iSTFT). This algorithmic latency allows the enhancement models to leverage future contextual information up to a length equal to the window size. However, current frame-online systems only partially leverage this future information. To fully exploit this information, this study proposes an overlapped-frame prediction technique for deep learning based frame-online speech enhancement, where at each frame our deep neural network (DNN) predicts the current and several past frames that are necessary for overlap-add, instead of only predicting the current frame. In addition, we propose a novel loss function to account for the scale difference between predicted and oracle target signals. Evaluations results on a noisyreverberant speech enhancement task show the effectiveness of the proposed algorithms.
Index Terms—Overlap-add, complex spectral mapping, online speech enhancement, deep learning.
I. INTRODUCTION
D EEP learning has elevated the performance of speech enhancement in the past decade [1]. Since the very ﬁrst success of deep learning in ofﬂine enhancement [2], there have been growing interests in using DNNs for lowlatency speech enhancement, as many application scenarios require online real-time processing. For example, the recent deep noise suppression challenges [3] target at speech enhancement in a monaural teleconferencing setup, requiring a processing latency less than 40 ms on a speciﬁed Intel i5 processor. Similar latency requirements exist in other related challenges [4], [5]. The recent Clarity challenge [6] aims at multi-microphone speech enhancement in a hearing aid setup, requiring an algorithmic latency of at maximum 5 ms.
Numerous deep learning based approaches [7]–[31] have been proposed for frame-online speech enhancement by using frame-online (or causal) DNN modules such as uni-directional recurrent networks, causal normalization layers, causal convolutions and causal attention layers. To the best of our knowledge, almost all the current DNN models adopt a singleframe prediction strategy for online enhancement. That is, the DNN models predict a frame of target signals at the current frame based on the current and past frames, and the prediction at the current frame is overlap-added with the predictions at nearby frames for signal re-synthesis. This leads to an algorithmic latency equal to the window length. However, this
Manuscript received on Mar. 14, 2022. Z.-Q. Wang and S. Watanabe are with Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA 15213, USA (e-mail: wang.zhongqiu41@gmail.com, shinjiw@cmu.edu).

Output signal
Predicted signal at frame t Predicted signal at frame t+1 Predicted signal at frame t+2 Predicted signal at frame t+3

Sample n

Sub-frame at frame t

Add

See Fig. 3

Inverse DFT & apply synthesis window Frame-Online DNN
Apply analysis window & DFT

Input signal at frame t Input signal at frame t+1 Input signal at frame t+2

Input signal at frame t+3 Input signal

8 ms 32 ms

Time
Fig. 1: Single-frame prediction. Best viewed in color.

strategy cannot fully leverage the future context afforded by the algorithmic latency. To explain this problem, we use an example STFT-based system shown in Fig. 1 for illustration. This example uses a window size of 32 ms and a hop size of 8 ms. The frame-online DNN model operates in an online streaming fashion, processing one frame at a time when a new frame arrives and producing a predicted frame of signals for each input frame. The predicted signals at nearby frames are then overlapped and added together (see the red-frame rectangle) to get the ﬁnal output signal for the sub-frame at frame t (marked in the top of the ﬁgure). As we can see, to get the prediction at sample index n (marked in the top), the system has to ﬁrst fully observe the input signals at frame t+3 before the DNN can perform feed-forwarding. The algorithmic latency is hence equal to the 32 ms window length. In this system, to get the predicted signals at frame t, the model only takes in the input signals up to frame t. The insight of this paper is that we can actually use the input signals up to frame t+3 for the DNN to get the predicted signal at frame t as well as at frame t+1, t+2 and t+3. The resulting predicted signals would very likely be better, because the DNN can leverage up to three frames of future context, and at the same time the algorithmic latency remains the same as the window size. This can be achieved by training our DNN model for overlappedframe prediction, where the current and past frames necessary for overlap-add are predicted at each frame at once.

Outp
Predicted s at fram
Pr

Sample n

Sub-frame at frame t

Output signal
Besides overlapped-frame prediction,Adwd e also propose a novel mePcrehdaictnedissimgnal atthfaratmeetqualizes the scales of predicted and oracle tarPgreedtictsedigsignnaallast frbameefot+r1e loss computation.
Predicted signal at frame t+2
Predicted siIgnIa.l atPfrRamOe Pt+O3 SED ALGORITHMS

Let us denote the monaural mixture y = s + v in the time Inverse DFT & apply synthesis window
domain as a summation of the target signal s and non-target Frame-Online DNN
signal v. In the STFT domain, we denote the mixture as Apply analysis window & DFT
Y = S+V , where Y , S and V are the STFT spectra of y, s and

v, respectiveInlpyut.siOgnaul rat fDramNeNt s predict S based on Y , if operating in the STFTInpdutosmignaaliant f;ramaentd+1 predict s based on y, if in the time

domain. ThiInspustesicgntailoant fradmeest+c2ribes the proposed overlapped-frame

prediction, Ianpluotnsiggnal wat firtahme ts+3ynthesis window design and DNN

conﬁgurations, and its extensions to time8-mds omain models. 32 ms

Input signal

A. Overlapped-Frame Prediction

Time

Let us denote the the STFT window and hop sizes by W and H samples, assuming W is a multiple of H, and let C = W/H be the number of overlapped frames in each window. The proposed system is illustrated in Fig. 2, which uses 32 ms window and 8 ms hop sizes as an example. Our DNN operates in a frame-online fashion and, at each frame, it predicts C frames consisting of the current and C − 1 immediate past frames. To get the sub-frame output at frame t (marked in the top of the ﬁgure), we overlap the C frames of predicted signals produced at frame t + C − 1, and add together the subframes marked by the red-frame rectangle (denoted as “partial sub-frame summation”). Clearly, the algorithmic latency is still equal to the window size, but we can leverage input signals up to frame t + C − 1 to better predict each of the C frames that are overlap-added to get the sub-frame output at frame t. This algorithm requires our frame-online DNN model to have C outputs. Alternatively, we can overlap-add the subframes marked by the black-frame rectangle (denoted as “full sub-frame summation”). This alternative could lead to better performance as it summates more sub-frame predictions.

We point out that our approach is different from studies [14], [16] that look ahead extra frames and add extra latency to the window size. It is also different from studies [32] that predict a symmetric window of frames of time-frequency masks at each frame, where the motivation was about output ensembling.

B. Synthesis Window Design

The proposed partial and full sub-frame summation methods

require us to design proper synthesis windows that can achieve

perfect reconstruction when used with an analysis window g ∈ RW . For the partial sub-frame summation, following [33] we can use a regular synthesis window l ∈ RW deﬁned as:

g[n]

l[n] =

,

(1)

W/H−1 g[eH + (n mod H)]2

e=0

where 0 ≤ n < W . When using the full sub-frame summation,

we design the synthesis window as follows:

l[n] =

g[n] , (2)
W e=/0H−1 (e + 1) × g[eH + (n mod H)]2

where the difference from Eq. (1) is that we have a weighting term for the W/H sub-frames in the denominator. The ra-

Output signal
Predicted signals at frame t

Sample n

2
Sub-frame at frame t

Add

Predicted signals at frame t+1

Predicted signals at frame t+2

Predicted signals at frame t+3

See Fig. 3

Inverse DFT & apply synthesis window Frame-Online DNN
Apply analysis window & DFT

Input signal at frame t

Input signal at frame t+1

Input signal at frame t+2

Input signal at frame t+3 Input signal

8 ms 32 ms
Time

Fig. 2: Overlapped-frame prediction. Best viewed in color.

tionale is that among all the sub-frames marked by the blackframe rectangle in Fig. 2, four are the last sub-frame in various predicted frames, three are the second-last sub-frame, and so on. We therefore need the weighting term in the denominator in order to have a pair of analysis and synthesis windows that can achieve perfect reconstruction.

C. DNN Conﬁgurations

Our STFT-domain DNN model is trained for complex spectral mapping [18], [34]–[37], where the real and imaginary (RI) components of the mixture Y are concatenated as input features for a DNN to predict the RI components of target speech S. The DNN architecture will be described in Section III-B.
When trained for overlapped-frame prediction, the DNN predicts K (= C) frames at each frame, essentially producing C estimated target spectrograms; and when trained for singleframe prediction, the DNN produces one estimated target spectrogram (i.e. K = 1). Let us denote the DNN-estimated RI components by Rˆ(k) and Iˆ(k), where k ∈ {1, 2, . . . , K} indexes the K spectrograms, and the enhanced speech by Sˆ(k) = Rˆ(k) + jIˆ(k), where j is the imaginary unit. We can deﬁne the loss function on the RI components and magnitudes of the estimated spectrograms:

LRI+Mag =
k

Rˆ(k) − Real(S) 1 + Iˆ(k) − Imag(S) 1

+ |Sˆ(k)| − |S| , (3)
1
where Real(·) and Imag(·) extract RI components, |·| computes

3

magnitude, and · 1 calculates the L1 norm. Based on the
described overlap-add mechanisms, an iSTFT is applied to resynthesize the estimated time-domain signal sˆ = iSTFT(Sˆ(:)), where Sˆ(:) includes all the K estimated spectrograms. Follow-
ing [38], we can train through the iSTFT and deﬁne the loss
on the re-synthesized signal and its magnitude:

LWav+Mag = sˆ − s 1 + |STFT(sˆ)| − |STFT(s)| , (4)
1
where STFT(·) extracts a complex spectrogram from a signal. When using this loss function with overlapped-frame prediction, at each time frame we essentially use signals up to frame t + C − 1 to predict the sub-frame at frame t (marked in the top of Fig. 2).
When using mapping based approaches with the loss functions in Eq. (3) or (4), the model needs to predict a spectrogram (or a signal) that has the same gain as the target. Although this may not be a problem for ofﬂine processing, as the model can observe the entire input mixture to produce a reasonably good gain, for frame-online processing this could be difﬁcult. We propose to ﬁrst compute a real-valued gain-equalization (GEQ) factor to balance the gain of the predicted signal with that of the target signal before loss computation:

LWav+Mag,geq = αˆsˆ − s 1+

|STFT(αˆsˆ)| − |STFT(s)| 1, (5)

where αˆ = argminα

αsˆ − s

2 2

= (sˆTs)/(sˆTsˆ).

Considering the implicit compensation between estimated

magnitude and phase [38], we always include in each loss

function a magnitude loss, which is known to improve metrics

that favor estimated signals with a good magnitude [38].

D. Extension to Time-Domain Models
The proposed algorithms can be used with time-domain models such as Conv-TasNet [13], by replacing the yellow blocks in Figs. 1 and 2 with time-domain models that use overlap-add for signal re-synthesis.

III. EXPERIMENTAL SETUP
We validate our algorithms on a simulated monaural speech enhancement task. This section describes the simulated dataset, DNN architectures, and miscellaneous conﬁgurations.

A. Dataset
The WSJCAM0 corpus [39] contains 7,861, 742, and 1,088 clean speech signals in its training, validation, and test sets, respectively. Using the split of the clean signals in WSJCAM0, we simulate 39,245 (∼77.7 h), 2,965 (∼5.6 h) and 3,260 (∼8.5 h) noisy-reverberant mixtures as our training, validation and test sets, respectively. We sample the clips in the development set of FSD50k [40] to simulate the noises for training and validation, and those in the evaluation set for testing. For each simulated mixture, we sample up to seven noise clips. The directions of each noise source and the target speaker to the microphone are independently sampled from the range [0, 2π). We treat each sampled clip as a point source, convolve each source with its corresponding room impulse response, and summate the convolved signals to create the mixture. After adding up all the spatialized noises, we scale the summated

Decoder Encoder

Reshape

384×%

384×%×1
3×3, 1,1 , 2,0 , 384
128×%×3
3×3, 1,2 , 2,0 , 128
64×%×7
3×3, 1,2 , 2,0 , 64

32×%×15
DenseBlock(32,32)
32×%×15
3×3, 1,2 , 2,0 , 32
32×%×31
DenseBlock(32,32)
32×%×31
3×3, 1,2 , 2,0 , 32
32×%×63
DenseBlock(32,32)
32×%×63
3×3, 1,2 , 2,0 , 32

32×%×127
DenseBlock(32,32)
32×%×127
3×3, 1,2 , 2,0 , 32

32×%×255
DenseBlock(32,32)
32×%×255
3×3, 1,1 , 2,0 , 32
2×$×257
Real % , Imag %

Causal TCN
Concatenate Concatenate Concatenate Concatenate
Concatenate
Concatenate
Concatenate
Concatenate

384×%

Reshape

384×T×1

(384 + 384)×%×1

1×3, 1,1 , 0,0 , 128

(128 + 128)×%×3
1×3, 1,2 , 0,0 , 64

(64 + 64)×%×7
1×3, 1,2 , 0,0 , 32

(32 + 32)×%×15
DenseBlock(32,64)
64×%×15
1×3, 1,2 , 0,0 , 32
(32 + 32)×%×31
DenseBlock(32,64)
64×%×31
1×3, 1,2 , 0,0 , 32
(32 + 32)×%×63
DenseBlock(32,64)
64×%×63
1×3, 1,2 , 0,0 , 32

(32 + 32)×%×127
DenseBlock(32,64)
64×%×127
1×3, 1,2 , 0,0 , 32

(32 + 32)×%×255
DenseBlock(32,64)
64×%×255
1×3, 1,1 , 0,0 , 2*

2" ×$×257
+*(,), … , +*(.) .-(,), … , .-(.)

ℒ/012304,4:;

Deconv2D+PReLU+BN Conv2D+PReLU+BN DenseBlock Conv2D+cauLN Deconv2D

64×%×255
3×3, 1,1 , 2,1 , 64
32×%×255
3×3, 1,1 , 2,1 , 32
32×%×255
Frequency Mapping
32×%×255
3×3, 1,1 , 2,1 , 32
32×%×255
3×3, 1,1 , 2,1 , 32
(32 + 32)×%×255

32×%×255
Reshape
255×%×32
1×1, 1,1 , 0,0 , 255
255×%×32
Reshape
32×%×255
1×1, 1,1 , 0,0 , 32
128×%×255

Fig. 3: Frame-online TCN-DenseUNet architecture adapted from [41]. During training, the tensor shape after each block in the encoder and decoder is denoted in the format: featureMaps×timeSteps×freqChannels. Each one of Conv2D+PReLU+BN, Deconv2D+PReLU+BN, Conv2D, and Deconv2D blocks is shown in the format: kernelTime×kernelFreq, (strideTime, strideFreq), (padTime, padFreq), featureMaps. Each DenseBlock(g1,g2) has ﬁve Conv2D+PReLU+BN blocks with growth rate g1 for the ﬁrst four layers and g2 for the last one.
reverberant noise such that the signal-to-noise ratio between the target direct-path speech and the summated reverberant noise is equal to a value sampled from the range [−8, 3] dB. The distance from each source to the microphone is drawn from the range [0.75, 2.5] m. The reverberation time is drawn from the range [0.2, 1.0] s. The sampling rate is 16 kHz.
B. DNN Architectures
Our STFT-domain frame-online DNN architecture is illustrated in Fig. 3. It is a modiﬁed version of the ofﬂine TCNDenseUNet architecture, which has shown strong performance in our previous studies [36], [41]–[43] on speech enhancement and speaker separation. The major modiﬁcations include using causal layer normalization (cauLN) layers, batch normalization (BN) layers, and causal one- and two-dimensional convolution and deconvolution. The network is a temporal convolution network (TCN) sandwiched by a U-Net, where the encoder performs down-sampling along frequency and the decoder performs up-sampling along frequency to recover the original frequency resolution. DenseNet blocks are inserted at multiple frequency scales in the encoder and decoder of the U-Net.

4

TABLE I SI-SDR (DB), PESQ, AND ESTOI RESULTS OF USING 32 AND 8 MS WINDOW
AND HOP SIZES FOR TCN-DENSEUNET.

ID Systems

Frameonline?

Loss function

Sub-frame SIsummation SDR PESQ eSTOI

0 Unprocessed

-

-

-

−6.2 1.44 0.411

1a Single-frame pred.

yes

1b Overlapped-frame pred. yes

1c Overlapped-frame pred. yes

RI+Mag RI+Mag RI+Mag

Partial Full

3.4 2.11 0.708 3.6 2.18 0.725 3.4 2.07 0.709

2a Single-frame pred.

yes Wav+Mag

-

2b Overlapped-frame pred. yes Wav+Mag Partial

2c Overlapped-frame pred. yes Wav+Mag

Full

2d Overlapped-frame pred. yes Wav+Mag,geq Full

3.5 2.17 0.719 4.0 2.23 0.733 4.0 2.26 0.738 4.2 2.28 0.743

3a Single-frame pred. 3b Single-frame pred. 3c Single-frame pred.

no

RI+Mag

-

no Wav+Mag

-

no Wav+Mag,geq

-

4.8 2.44 0.776 4.6 2.46 0.776 4.8 2.47 0.778

We choose this architecture as it shares many similarities with the architectures in many recent studies [17]–[20]. The RI components of different input (and output) signals are concatenated as feature maps in the network input (and output). The number of feature maps is 2 for the input tensor and is 2K for the output tensor. Clearly, using overlappedframe prediction with this architecture only introduces a slight amount of computation (only to the output layer) compared with using single-frame prediction. Linear activation units are used in the output layer to obtain the predicted RI components.
Conv-TasNet [13] is a representative time-domain model. We experiment with its default hyper-parameters reported in [13] for overlapped-frame prediction.
C. Miscellaneous Conﬁgurations
The direct-path signal of the target speaker is used as the target for model training and as the reference for metric computation. SI-SDR [44], PESQ [45], and eSTOI [46] are used as the evaluation metrics. For TCN-DenseUNet, we experiment with the commonly used 32/8 and 20/10 ms window/hop sizes. Giving a 16 kHz sampling rate, we use a 512-point discrete Fourier transform to obtain 257-dimensional STFT features at each frame for both setups. This way, the same DNN architecture can be used. The square-root Hann window is used as the analysis window. For Conv-TasNet, we experiment with 4/2 ms window/hop sizes. It should be noted that a longer window looks ahead more future samples and has a higher algorithmic latency, and would likely have more improvement when used with overlapped-frame prediction.
IV. EVALUATION RESULTS
Table I, II and III respectively report the results of using 32/8, 20/10 and 4/2 ms window/hop sizes. In each table, we provide the ofﬂine results obtained by using the ofﬂine versions of the DNN architectures. These ofﬂine results can be viewed as the performance upper bound of the frame-online models.
Let us ﬁrst look at Table I. Comparing 1a and 1b (or 2a and 2b), we observe that overlapped-frame prediction with partial sub-frame summation produces better performance than single-frame prediction. Using full sub-frame summation, 1c obtains worse performance than 1b, while 2c obtains better performance than 2b. This is likely because when we use the RI+Mag loss and do not train through iSTFT, the sub-frames summated by using full sub-frame summation contain sub-

TABLE II SI-SDR (DB), PESQ, AND ESTOI RESULTS OF USING 20 AND 10 MS WINDOW
AND HOP SIZES FOR TCN-DENSEUNET.

ID Systems

Frameonline?

Loss function

Sub-frame SIsummation SDR PESQ eSTOI

1a Single-frame pred.

yes

1b Overlapped-frame pred. yes

1c Overlapped-frame pred. yes

RI+Mag RI+Mag RI+Mag

Partial Full

2.8 2.01 0.692 3.0 2.05 0.699 2.3 1.81 0.660

2a Single-frame pred.

yes Wav+Mag

-

2b Overlapped-frame pred. yes Wav+Mag Partial

2c Overlapped-frame pred. yes Wav+Mag

Full

2d Overlapped-frame pred. yes Wav+Mag,geq Full

2.3 2.06 0.702 2.4 2.06 0.704 3.0 2.16 0.721 3.4 2.16 0.724

3a Single-frame pred. 3b Single-frame pred. 3c Single-frame pred.

no

RI+Mag

-

no Wav+Mag

-

no Wav+Mag,geq

-

4.5 2.46 0.770 4.0 2.47 0.778 4.5 2.49 0.784

TABLE III SI-SDR (DB), PESQ, AND ESTOI RESULTS OF USING 4 AND 2 MS WINDOW
AND HOP SIZES FOR CONV-TASNET.

ID Systems

Frameonline?

Loss function

Sub-frame SIsummation SDR PESQ eSTOI

2a Single-frame pred.

yes Wav+Mag

-

2b Overlapped-frame pred. yes Wav+Mag Partial

2c Overlapped-frame pred. yes Wav+Mag

Full

2d Overlapped-frame pred. yes Wav+Mag,geq Full

2.2 1.79 0.658 2.1 1.77 0.652 2.3 1.81 0.661 2.3 1.81 0.664

3b Single-frame pred. 3c Single-frame pred.

no Wav+Mag

-

no Wav+Mag,geq

-

3.8 2.28 0.754 3.9 2.26 0.754

frame predictions produced at earlier frames (see the blackframe rectangle in Fig. 2), which are not as good as the ones produced at the current frame. When we train through iSTFT and use the Wav+Mag loss, the model may ﬁgure out how to best predict each sub-frame and best summate all the sub-frames (in the black-frame rectangle) to optimize the loss. Overall, our proposed systems in 2c show noticeable improvement over 1a and 2a, by better leveraging the 32 ms future context information.
In Table II, similar trend as in Table I is observed. The relative gains of overlapped-frame prediction over singleframe prediction are smaller than those in Table I. This is as expected as there is less future context (i.e., 20 ms) to exploit. In Table III, the gains (see 2c vs. 2a) are very marginal as the future context (i.e., 4 ms) is even less.
Although the gains brought by overlapped-frame prediction depend on the allowed future context and are small when the allowed context is limited, we always observe consistent and steady improvements in the considered setups.
In all the tables, the proposed gain equalization leads to slightly better results in the online cases (see 2c vs. 2d).
V. CONCLUSION
We have proposed a novel overlapped-frame prediction techniquefor frame-online speech enhancement. It better leverages future context, without incurring extra algorithmic latency. The increased amount of computation only stems from the output layer and is negligible compared to that of the DNN backbone. The proposed technique can be easily modiﬁed for, or directly adopted by, numerous frame-online systems. It would likely yield better performance due to the better exploitation of the future context afforded by the algorithmic latency, no matter whether they are DNN- or non-DNN-based, whether they operate in the T-F domain or in the time domain, or whether they deal with speech enhancement or other related separation tasks, as long as they use overlap-add for signal re-synthesis.

5

REFERENCES
[1] D. Wang and J. Chen, “Supervised Speech Separation Based on Deep Learning: An Overview,” IEEE/ACM Trans. Audio, Speech, Lang. Process., vol. 26, pp. 1702–1726, 2018.
[2] Y. Wang and D. Wang, “Towards Scaling Up Classiﬁcation-Based Speech Separation,” IEEE Trans. Audio, Speech, Lang. Process., vol. 21, no. 7, pp. 1381–1390, 2013.
[3] C. K. Reddy, V. Gopal, R. Cutler, E. Beyrami, R. Cheng, H. Dubey, S. Matusevych, R. Aichner, A. Aazami, S. Braun, P. Rana, S. Srinivasan, and J. Gehrke, “The INTERSPEECH 2020 Deep Noise Suppression Challenge: Datasets, Subjective Speech Quality and Testing Framework,” in Proc. Interspeech, 2020, pp. 2492–2496.
[4] R. Cutler, A. Saabas, T. Parnamaa, M. Purin, H. Gamper, S. Braun, K. Sørensen, and R. Aichner, “ICASSP 2022 Acoustic Echo Cancellation Challenge,” in Proc. ICASSP, 2022.
[5] W. Rao, Y. Fu, Y. Hu, X. Xu, Y. Jv, J. Han, Z. Jiang, L. Xie, Y. Wang, S. Watanabe, Z.-H. Tan, H. Bu, T. Yu, and S. Shang, “INTERSPEECH 2021 ConferencingSpeech Challenge: Towards Farﬁeld Multi-Channel Speech Enhancement for Video Conferencing,” in arXiv preprint arXiv:2104.00960, 2021.
[6] “Clarity Challenge: Machine learning Challenges for Hearing Devices.” [Online]. Available: http://claritychallenge.org/
[7] J. Chen and D. Wang, “Long Short-Term Memory for Speaker Generalization in Supervised Speech Separation,” Journal of the Acoustical Society of America, vol. 141, pp. 4705–4714, 2017.
[8] G. Wichern and A. Lukin, “Low-Latency Approximation of Bidirectional Recurrent Networks for Speech Denoising,” in Proc. WASPAA, 2017, pp. 66–70.
[9] Y. Luo and N. Mesgarani, “TasNet: Time-Domain Audio Separation Network for Real-Time, Single-Channel Speech Separation,” in Proc. ICASSP, nov 2017, pp. 697–700.
[10] K. Wilson, M. Chinen, J. Thorpe, B. Patton, J. R. Hershey, R. A. Saurous, J. Skoglund, and R. F. Lyon, “Exploring Tradeoffs in Models for Low-Latency Speech Enhancement,” in Proc. IWAENC, 2018, pp. 366–370.
[11] S. Wisdom, J. R. Hershey, K. Wilson, J. Thorpe, M. Chinen, B. Patton, and R. A. Saurous, “Differentiable Consistency Constraints for Improved Deep Speech Enhancement,” in Proc. ICASSP, 2019, pp. 900–904.
[12] T. Higuchi, K. Kinoshita, N. Ito, S. Karita, and T. Nakatani, “Frame-byFrame Closed-Form Update for Mask-Based Adaptive MVDR Beamforming,” in Proc. ICASSP, 2018, pp. 531–535.
[13] Y. Luo and N. Mesgarani, “Conv-TasNet: Surpassing Ideal TimeFrequency Magnitude Masking for Speech Separation,” IEEE/ACM Trans. Audio, Speech, Lang. Process., vol. 27, no. 8, pp. 1256–1266, 2019.
[14] T. Yoshioka, Z. Chen, C. Liu, X. Xiao, H. Erdogan, and D. Dimitriadis, “Low-Latency Speaker-Independent Continuous Speech Separation,” in Proc. ICASSP, 2019, pp. 6980–6984.
[15] S. Chakrabarty and E. A. P. Habets, “Time-Frequency Masking Based Online Multi-Channel Speech Enhancement with Convolutional Recurrent Neural Networks,” IEEE Journal of Selected Topics in Signal Processing, 2019.
[16] S. Sonning, C. Schuldt, H. Erdogan, and S. Wisdom, “Performance Study of a Convolutional Time-Domain Audio Separation Network for Real-Time Speech Denoising,” in Proc. ICASSP, 2020, pp. 831–835.
[17] Y. Liu and D. Wang, “Causal Deep CASA for Monaural TalkerIndependent Speaker Separation,” IEEE/ACM Trans. Audio, Speech, Lang. Process., pp. 1270–1279, 2020.
[18] K. Tan and D. Wang, “Learning Complex Spectral Mapping With Gated Convolutional Recurrent Networks for Monaural Speech Enhancement,” IEEE/ACM Trans. Audio, Speech, Lang. Process., vol. 28, pp. 380–390, 2020.
[19] Y. Hu, Y. Liu, S. Lv, M. Xing, S. Zhang, Y. Fu, J. Wu, B. Zhang, and L. Xie, “DCCRN: Deep Complex Convolution Recurrent Network for Phase-Aware Speech Enhancement,” in Proc. Interspeech, 2020, pp. 2472–2476.
[20] A. Pandey and D. Wang, “Densely Connected Neural Network with Dilated Convolutions for Real-Time Speech Enhancement in The Time Domain,” in Proc. ICASSP, 2020, pp. 6629–6633.
[21] C. Han, Y. Luo, and N. Mesgarani, “Real-Time Binaural Speech Separation with Preserved Spatial Cues,” in Proc. ICASSP, 2020, pp. 6404–6408.
[22] Y. Xia, S. Braun, C. K. Reddy, H. Dubey, R. Cutler, and I. Tashev, “Weighted Speech Distortion Losses for Neural-Network-Based RealTime Speech Enhancement,” in Proc. ICASSP, 2020, pp. 871–875.

[23] A. De´fossez, G. Synnaeve, and Y. Adi, “Real Time Speech Enhancement in the Waveform Domain,” in Proc. Interspeech, 2020, pp. 3291–3295.
[24] S. Braun, H. Gamper, C. K. A. Reddy, and I. Tashev, “Towards Efﬁcient Models for Real-Time Deep Noise Suppression,” in Proc. ICASSP, 2021, pp. 656–660.
[25] X. Hao, X. Su, R. Horaud, and X. Li, “Fullsubnet: A Full-Band and Sub-Band Fusion Model for Real-Time Single-Channel Speech Enhancement,” in Proc. ICASSP, 2021, pp. 6633–6637.
[26] A. Li, W. Liu, X. Luo, C. Zheng, and X. Li, “ICASSP 2021 Deep Noise Suppression Challenge: Decoupling Magnitude and Phase Optimization with a Two-Stage Deep Network,” in Proc. ICASSP, 2021, pp. 6628– 6632.
[27] Z. Tu, J. Zhang, N. Ma, J. Barker, and C. Science, “A Two-Stage End-to-End System for Speech-in-Noise Hearing Aid Processing,” in Proceedings of Clarity, 2021, pp. 3–5.
[28] S. J. Yang, S. Wisdom, C. Gnegy, R. F. Lyon, and S. Savla, “Listening with Googlears : Low-Latency Neural Multiframe Beamforming and Equalization for Hearing Aids,” in Proceedings of Clarity, 2021.
[29] K. Zmolikova and J. H. Cernock, “BUT System for the First Clarity Enhancement Challenge,” in Proceedings of Clarity, 2021, pp. 1–3.
[30] X. Ren, X. Zhang, L. Chen, X. Zheng, C. Zhang, L. Guo, and B. Yu, “A Causal U-Net Based Neural Beamforming Network for Real-Time Multi-Channel Speech Enhancement,” in Proceedings of Interspeech, 2021, pp. 1832–1836.
[31] C. Li, L. Yang, W. Wang, and Y. Qian, “SkiM: Skipping Memory LSTM for Low-Latency Real-Time Continuous Speech Separation,” in Proc. ICASSP, 2022.
[32] J. Chen, Y. Wang, S. E. Yoho, D. Wang, and E. W. Healy, “LargeScale Training to Increase Speech Intelligibility for Hearing-Impaired Listeners in Novel Noises,” The Journal of the Acoustical Society of America, vol. 139, no. 5, pp. 2604–2612, 2016.
[33] D. W. Grifﬁn and J. S. Lim, “Signal Estimation from Modiﬁed ShortTime Fourier Transform,” IEEE Trans. Audio, Speech, Signal Process., vol. 32, no. 2, pp. 236–243, 1984.
[34] D. S. Williamson, Y. Wang, and D. Wang, “Complex Ratio Masking for Monaural Speech Separation,” IEEE/ACM Trans. Audio, Speech, Lang. Process., pp. 483–492, 2016.
[35] S.-W. Fu, T.-Y. Hu, Y. Tsao, and X. Lu, “Complex Spectrogram Enhancement By Convolutional Neural Network with Multi-Metrics Learning,” in Proc. MLSP, 2017, pp. 1–6.
[36] Z.-Q. Wang, P. Wang, and D. Wang, “Complex Spectral Mapping for Single-and Multi-Channel Speech Enhancement and Robust ASR,” IEEE/ACM Trans. Audio, Speech, Lang. Process., vol. 28, pp. 1778– 1787, 2020.
[37] Z.-Q. Wang and D. Wang, “Deep Learning Based Target Cancellation for Speech Dereverberation,” IEEE/ACM Trans. Audio, Speech, Lang. Process., vol. 28, pp. 941–950, 2020.
[38] Z.-Q. Wang, G. Wichern, and J. Le Roux, “On The Compensation Between Magnitude and Phase in Speech Separation,” IEEE Signal Process. Lett., vol. 28, pp. 2018–2022, 2021.
[39] T. Robinson, J. Fransen, D. Pye, J. Foote, and S. Renals, “WSJCAM0: A British English Speech Corpus for Large Vocabulary Continuous Speech Recognition,” in Proc. ICASSP, vol. 1, 1995, pp. 81–84.
[40] E. Fonseca, X. Favory, J. Pons, F. Font, and X. Serra, “FSD50K: An Open Dataset of Human-Labeled Sound Events,” IEEE/ACM Trans. Audio, Speech, Lang. Process., 2021.
[41] Z.-Q. Wang, G. Wichern, and J. Le Roux, “Leveraging Low-Distortion Target Estimates for Improved Speech Enhancement,” arXiv preprint arXiv:2110.00570, 2021.
[42] ——, “Convolutive Prediction for Monaural Speech Dereverberation and Noisy-Reverberant Speaker Separation,” IEEE/ACM Trans. Audio, Speech, Lang. Process., vol. 29, pp. 3476–3490, 2021.
[43] Z.-Q. Wang, P. Wang, and D. Wang, “Multi-Microphone Complex Spectral Mapping for Utterance-Wise and Continuous Speech Separation,” IEEE/ACM Trans. Audio, Speech, Lang. Process., vol. 29, pp. 2001– 2014, 2021.
[44] J. Le Roux, S. Wisdom, H. Erdogan, and J. R. Hershey, “SDR – HalfBaked or Well Done?” in Proc. ICASSP, 2019, pp. 626–630.
[45] A. Rix, J. Beerends, M. Hollier, and A. Hekstra, “Perceptual Evaluation of Speech Quality (PESQ)-A New Method for Speech Quality Assessment of Telephone Networks and Codecs,” in Proc. ICASSP, vol. 2, 2001, pp. 749–752.
[46] C. H. Taal, R. C. Hendriks, R. Heusdens, and J. Jensen, “An Algorithm for Intelligibility Prediction of Time–Frequency Weighted Noisy Speech,” IEEE Trans. Audio, Speech, Lang. Process., vol. 19, no. 7, pp. 2125–2136, 2011.

