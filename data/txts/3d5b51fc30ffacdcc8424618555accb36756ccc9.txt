arXiv:1902.03591v3 [math.OC] 7 May 2019

STOCHASTIC THREE POINTS METHOD FOR UNCONSTRAINED SMOOTH MINIMIZATION
EL HOUCINE BERGOU∗, EDUARD GORBUNOV† , AND PETER RICHTA´ RIK‡
Abstract. In this paper we consider the unconstrained minimization problem of a smooth function in Rn in a setting where only function evaluations are possible. We design a novel randomized derivative-free algorithm — the stochastic three points (STP) method — and analyze its iteration complexity. At each iteration, STP generates a random search direction according to a certain ﬁxed probability law. Our assumptions on this law are very mild: roughly speaking, all laws which do not concentrate all measure on any halfspace passing through the origin will work. For instance, we allow for the uniform distribution on the sphere and also distributions that concentrate all measure on a positive spanning set.
Although our approach is designed to not use explicitly derivatives, it covers some ﬁrst order methods. For instance if the probability law is chosen to be the Dirac distribution concentrated at the sign of the gradient then STP recovers the Signed Gradient Descent method. If the probability law is the uniform distribution on the coordinates of the gradient then STP recovers the Coordinate Descent Method.
Given a current iterate x, STP compares the objective function at three points: x, x + αs and x − αs, where α > 0 is a stepsize parameter and s is the random search direction. The best of these three points is the next iterate. We analyze the method STP under several stepsize selection schemes (ﬁxed, decreasing, estimated through ﬁnite diﬀerences, etc).
The complexity of STP depends on the probability law via a simple characteristic closely related to the cosine measure which is used in the analysis of deterministic direct search (DDS) methods. Unlike in DDS, where O(n) (n is the dimension of x) function evaluations must be performed in each iteration in the worst case, our method only requires two new function evaluations per iteration. Consequently, while DDS depends quadratically on n, our method depends linearly on n. In particular, in the nonconvex case, STP needs O(nε−2) function evaluations to ﬁnd a point at which the gradient of the objective function is below ε, in expectation. In the convex case, the complexity is O(nε−1). In the strongly convex case STP converges linearly, meaning that the complexity is O n log ε−1 .

1. Introduction. In this paper we consider the problem

(1.1)

min f (x),
x∈Rn

where f : Rn → R is a given smooth objective function. We assume that we do not have access to the derivatives of f and only have access to a function evaluation oracle. In other words, we assume that we work in the Derivative-Free Optimization (DFO) setting [3]. Optimization problems of this type appear in many industrial applications where usually the objective function is evaluated through a computer simulation process, and therefore derivatives cannot be directly evaluated; e.g., shape optimization in ﬂuid-dynamics problems [1, 10, 16].
Direct search methods of directional type [13, 3] are a popular class of methods for DFO and are among the ﬁrst algorithms proposed in numerical optimization [15]. These methods are characterized by evaluating the objective function over a number of (typically predetermined and ﬁxed) directions to ensure descent using a suﬃciently

∗King Abdullah University of Science and Technology (KAUST), Thuwal, Saudi Arabia. MaIAGE, INRA, Universit´e Paris-Saclay, 78350 Jouy-en-Josas, France (elhoucine.bergou@inra.fr). This author received support from the AgreenSkills+ fellowship programme which has received funding from the EU’s Seventh Framework Programme under grant agreement No FP7-609398 (AgreenSkills+ contract).
†Moscow Institute of Physics and Technology (MIPT), Moscow, Russian Federation (eduard.gorbunov@phystech.edu).
‡ King Abdullah University of Science and Technology (KAUST), Thuwal, Saudi Arabia. University of Edinburgh, Edinburgh, United Kingdom. Moscow Institute of Physics and Technology (MIPT), Moscow, Russian Federation (peter.richtarik@kaust.edu.sa).
1

small stepsize. The directions are typically required to form a positive spanning set (i.e. a set of vectors whose conic hull is Rn) in order to make sure that each point in Rn (and hence also the optimal solution) is achievable by a sequence of positive steps from any starting point.
For instance, the coordinate search method uses the coordinate (i.e., standard basic) directions, e1, e2, . . . , en, and their negatives, −e1, −e2, . . . , −en as the set of admissible directions. Clearly, {±ei, : i = 1, 2, . . . , n} forms a positive spanning set.
1.1. Stochastic Three Points method. In this paper, we study a very general randomized variant of direct search methods, which we call Stochastic Three Points (STP).
STP depends on two “parameters”: a distribution / probability law D from which we sample directions, and a stepsize selection rule. At iteration k of STP, we generate a random direction sk by sampling from D, and then choose the next iterate via
xk+1 = arg min {f (xk + αksk), f (xk − αksk), f (xk)} ,
where αk > 0 is an appropriately chosen stepsize. That is, we pick xk+1 as the best of the three points xk + αksk, xk − αksk and xk in terms of the function values.
We prove for such a scheme, with several diﬀerent choices of stepsizes, that the number of iterations suﬃcient to guarantee that min E [ ∇f (xk) D] ≤ ε
k=0,1,...,K
is O(nε−2), where · D is a norm dependent on D which we introduce in Section 3.1 and E [·] is the expectation. This complexity is global since no assumption is made on the starting point. If the objective function f is convex, then the number of iterations needed to get xk such that E [f (xk) − f∗] ≤ ε is O(nε−1) where f∗ is the optimal value of f . If in addition, f is strongly convex, then we have a global linear rate of convergence. This is an improvement on deterministic direct search (DDS) where the best known complexity bounds depend quadratically on n and the same way as our scheme in ε [14, 22, 5]. We propose also a parallel version for STP.
Despite our approach shares similarities with other randomized algorithmic approaches, the diﬀerences are signiﬁcant. In the sixties a random optimization approach was proposed in [15]. It was proposed to sample a point randomly around the current iterate and move to this new point if it decreases the objective function. This approach was generalized to cover constrained problems in [2]. The theoretical and numerical performances of this approach for nonconvex functions was studied in [7, 20]. More recently, the works in [4] and [9] use random searching directions, and impose a decrease condition to whether accept the step or reject it, like in DDS. They update the stepsize by increasing it if the step is accepted and decreasing it otherwise. Our approach is diﬀerent from these frameworks in the sense that at each iteration we generate a single direction, then we choose the stepsize independently from any decrease condition. In [9], the authors impose to the search direction some probabilistic property. In fact, they assume that at each iteration their random directions are probabilistic descent conditioned to the past. In other words, at a given iteration, independently from the past with a certain probability at least one of the directions is of descent type. The main result of [9] is the complexity bound O(rnε−2) to drive the gradient norm below ε with high probability, where r ≥ 2 is the number of the random directions at each iteration. Also [9] do not cover the cases when the objective function is convex or strongly convex. STP method gives similar complexity bound for non-convex problems (with r = 2).
More related to our work is the method proposed in [11, 12] for convex problems,
2

where at iteration k the step is updated as follows

xk+1 = xk + αku, where u is sampled uniformly from the uniform distribution on the unit sphere, and

αk = arg min f (xk + αu).
α∈R

The latter method was improved in two ways by [21]. In fact, the proposed method in [21] i) allows approximate line search, i.e., αk ≈ arg minα∈R f (xk + αu), ii) and allows discrete sampling from {±ei, i = 1, . . . , n} instead of sampling from the unit sphere. Our approach is diﬀerent from these methods in the sense that it did not perform any line search approximation to compute the stepsizes, and allows diﬀerent distributions (which include the uniform distribution over the unit sphere and the discrete sampling from the canonical basis of Rn) to sample the directions. The complexity bounds given in these works are worse than those obtained in this paper. Another method related to our work is the method discussed in [19, Section 3.4], a derivative-free approach based on forming an unbiased estimate of the gradient using Gaussian smoothing. The search direction in this method is distributed uniformly over the unit sphere and it is pre-multiplied by an approximation to the directional derivative along the direction itself. More precisely, this method updates the step at iteration k as follows

(1.2)

xk+1 = xk − αk f (xk + µku) − f (xk) u, µk

where µk ∈ (0, 1) is the ﬁnite diﬀerences parameter, αk is the stepsize, and u is a random vector distributed uniformly over the unit sphere. In this work, there is no explicit rules for choosing the parameters and there is no analysis of the worst case complexity. The paper [18] proposes other variants of this method by changing the way of approximating the directional derivative of f along u. Moreover, it gives the worst case complexity analysis of the method (1.2). The complexity bounds in [18] are similar to those of our STP approach. Our approach is diﬀerent from the method (1.2) and its variants porposed in [18], in our approach the search direction can follows a diﬀerent distribution from the uniform distribution over the unit sphere. For instance, we allow a distribution that has all its mass concentrated on a discrete set of vectors – which makes a direct connection with the (deterministic) direct search methods. Moreover, the proposed stepsizes in [18] depend on the Lipschitz constant of the gradient of the objective function. However, in our approach we proposed some stepsizes which can be easily computed in practice. The extention of the work [18] for an uncontrained problem of minimization of a smooth convex function which is only available through noisy observations of its values were studied in the recent work [8], where the authors proposed accelerated and non-accelerated zeroth-order method, which works in diﬀerent proximal-setups. They obtained almost dimensionindependent rate for the non-accelerated algorithm for the case of 1-proximal-setup and sparse vector x0 − x∗.
1.2. Outline. We organize this paper as follows. In Section 3 we present our stochastic three points method and give some of its properties. In Section 3.1 we give the main assumptions on the random direction to ensure the convergence of our method. Then, in Section 3.2 we introduce the key lemma for the analysis of the
3

complexity. Section 4 gives the analysis for the worst case complexity for non-convex problems. While Section 5 deals with the complexity analysis for the convex problems, and Section 6 gives the analysis of the complexity for strongly convex problems. Section 7 proposes a parallel version of STP and gives the corresponding complexity analysis. Numerical tests are illustrated and discussed in Section 8. Conclusions and future improvements are discussed in Section 9.
1.3. Notation. Throughout this paper D will denote a probability distribution over Rn. We use E [·] to denote the expectation and x, y = x y corresponds to the inner product of x and y. We denote also by · 2 the 2-norm, and by · D a norm dependent on D which we introduce in Section 3.1.
2. Summary of contributions. Here we highlight some of the contributions of this work.
A simple and ﬂexible algorithm. We study a novel variant of direct search based on random directions, which we call Stochastic Three Points (STP). It depends on at most three parameters: The starting point x0 for the iterate, the probability distribution D on Rn to sample the directions, and in some cases an α0 to deﬁne the stepsize. The probability distribution D may be iteration dependent as far as it satisﬁes the required assumption (see Assumption 5.4). In fact, Assumption 5.4 may be weakened by letting the probability distribution to depend on the iteration k in the following way
1. The quantity γDk d=ef Es∼Dk s 22 is positive and uniformly bounded away from inﬁnite.
2. There is a constant µD > 0 and norm · D (independent from k) on Rn such that

(2.1)

Es∼Dk | gk, s | ≥ µD gk D,

where gk = ∇f (xk). This assumption may be weakened even more by letting µD and norm · D to dependent on k and assuming i) the uniform boundness of µDk away from zero, ii) and that · Dk is uniformly equivalent to a norm independent from k. To avoid unnecessary notations and for the sake of clarity and simplicity of the presentation,
for the analysis we choose the probability distribution to be iteration independent in
this paper.
A general setting. Our approach covers some rather exotic ﬁrst order methods:
• Normalized Gradient Descent (NGD) method: at iteration k, s ∼ D means that s = ggkk 2 with probability 1.
• Signed Gradient Descent (SignGD) method: at iteration k, s ∼ D means that
s = sign (gk) with probability 1, where the sign operation is element wise sign.
• Normalized Randomized Coordinate Descent (NRCD) method (equivalently
this method can be called also Randomized Signed Gradient Descent): at iteration k, s ∼ D means that s = |ggkkii | ei if gki = 0 and s = 0 otherwise, with probability n1 , where gki is the i − th component of gk. • Normalized Stochastic Gradient Descent (NSGD) method: at iteration k,
s ∼ D means that s = gˆk where gˆk is the stochastic gradient satisfying E [gˆk] = ggkk 2 , and E gˆk 22 ≤ σ < ∞. The required assumption on D is satisﬁed in these cases (see Appendix B).
4

The probability distribution is also allowed to be either continuous or discrete, so that we cover many known strategies of choosing the directions in the DFO setting in the literature. For instance, if D is the uniform law on the unit sphere we recover the directions proposed in [11, 12, 19, 18]. If it is the discrete law on {±ei, i = 1, . . . , n} we recover the directions proposed in [21]. If it is the discrete law on {±di, i = 1, . . . , n} where di, i = 1, . . . , n form a basis of Rn, STP can be seen as a random variant of the Simpliﬁed Direct Search (SDS) method studied in [14].
One of the main goals of ﬂexibility in choosing the probability distribution D is the eﬃciency for solving some optimization problems which may have some speciﬁc properties like:
• The size of the problem to optimize is very large such that even the addition of two vectors may be unfeasible. For instance if the dimension of the problem (i.e., the size of x) is larger than the available memory, then updating all the components of x at each iteration is impossible. One is allowed to update only some components of x at each iteration.
• The objective function is not entirely deﬁned at the beginning of the optimization process, like in the streaming optimization. In other words the data describing the objective function arrives in real time during the optimization process. At a given iteration (time) we can not evaluate the objective function in all points of Rn. We can only evaluate the objective function in a set of directions (only some components of x can be updated).
• Even if we have the entire objective function at the beginning of the optimization process, for some problems the computation of the function value increases with the number of the perturbed variables. In other words, when perturbing all the components of x the evaluation of f takes a lot of time. However by perturbing only one parameter (or a set of parameters) the objective is evaluated in reasonable time.
• Some prior knowledge about Lipschitz constants in some directions is available.
For these kind of situations the choices of D to be a continuous law is prohibited. However the discrete choices of D are the most convenient in these cases.
Practicality. STP method is extremely simple to use in practice and its analysis is also simple compared to the state-of-the-art direct search methods based on random directions/stepsizes. In fact, the most related work to STP is the work in [18]. In the latter work, the proposed stepsizes depend on the Lipschitz constant of the gradient of the objective function, which may not be known in practice. However, for STP we proposed several stepsize selection schemes. Some of them can be easily computed in practice. Moreover, our preliminary numerical experiments show that our approach is competitive in practice.
Better bounds. We obtained compact worst case complexity bounds. These bounds are similar to those obtained in [18]. They depend linearly on the dimension of the considered problem, while this dependence is quadratic for deterministic direct search methods [22, 5, 14]. In Table 1 we summarize selected complexity results (bounds on the number of function evaluations) obtained in this paper for STP method. In all cases we assume that f is diﬀerentiable, bounded below (by f∗), with L-Lipschitz gradient. The assumptions listed in the ﬁrst column of the table are additional to this. The quantity R0 measures the size of a speciﬁc level set of f . The symbol ∝ means proportional. In fact, this symbol appears in the deﬁnition of the stepsizes, for instance αk ∝ √k1+1 means that αk is equal to some constant α0 (independent
5

from k) multiplied by √k1+1 . This constant α0 usually depends in the constants of the problem, like the Lipschitz constant and x0. More details about the deﬁnitions of
all these quantities are given in the main text.

Assumptions on f (additional to L-smoothness)

Stepsizes

Complexity Theorems

none

αk ∝ √k1+1 αk ∝ ε

O

n ε2

4.1, 4.2

convex, R0 ﬁnite

αk ∝ f (xk) − f (x∗)

αk

∝

|f (xk+tsk)−f (xk)| t

O nε

5.3, 5.5

λ-strongly convex

1
αk ∝ (f (xk) − f (x∗)) 2

αk

∝

|f (xk+tsk)−f (xk)| t

O n log 1ε

6.2 , 6.3

Table 1 Summary of the complexity results obtained in this paper for STP method. Column “Complexity” deﬁnes the number of iterations needed to guarantee mink E [ ∇f (xk) D] ≤ ε (second row) or E [f (xk) − f (x∗)] ≤ ε (third and fourth rows).

Parallel method. In Table 2 we summarize selected complexity results (bounds on the number of function evaluations) obtained in this paper for the parallel version of the STP method. More details about the deﬁnitions of all quantities appearing in the table are given in the main text. PSTP method gives the same rate as STP method with spherical setup but for wider range of distributions.

Assumptions on f (additional to L-smoothness)

Stepsizes

Complexity Theorems

none

αk ∝ √k1+1

O

n ε2

7.2

convex, R0 ﬁnite

αk ∝ f (xk) − f (x∗)

O nε

7.3

λ-strongly convex

1
αk ∝ (f (xk) − f (x∗)) 2

O n log 1ε

7.4

Table 2 Summary of the complexity results obtained in this paper for the parallel version of STP method. As before, column “Complexity” deﬁnes the number of iterations needed to guarantee mink E [ ∇f (xk) D] ≤ ε (second row) or E [f (xk) − f (x∗)] ≤ ε (third and fourth rows).

Experiments. We provide a number of experimental results, showing that our approach is a competitive algorithm in practice. In fact, we compared on a large set of problems our approach with the method (1.2) as well as with the coordinate search method (the DDS method which uses the 2n coordinate directions). The experiments show that the use of the random directions leads to a signiﬁcant improvement in
6

terms of the number of function evaluation. Indeed, our approach and method (1.2) outperform the DDS method. Moreover, our approach exhibits better performances than the other two methods. See Section 8 for a complete view on the experimental results.
3. Stochastic Three Points method. Our stochastic three points (STP) algorithm is formalized below as Algorithm 3.1.

Algorithm 3.1 Stochastic Three Points (STP)
Initialization Choose x0 ∈ Rn, stepsizes αk > 0, probability distribution D on Rn.
For k = 0, 1, 2, . . .
1. Generate a random vector sk ∼ D 2. Let x+ = xk + αksk and x− = xk − αksk 3. xk+1 = arg min{f (x−), f (x+), f (xk)}

Due to the randomness of the search directions sk for k ≥ 0, the iterates are also random vectors for all k ≥ 1. The starting point x0 is not random (the initial objective function value f (x0) is deterministic). Note that STP never moves to a point with a larger objective value. This monotonicity property does not depend on D or the properties of f . Let us formulate this simple observation as a lemma.
Lemma 3.1 (Monotonicity). STP produces a monotonic sequence of iterates, i.e., f (xk+1) ≤ f (xk) for all k ≥ 0. As a consequence,

(3.1)

E[f (xk+1) | xk] ≤ f (xk).

Throughout the paper, we assume that f is diﬀerentiable, bounded below and has L-Lipschitz gradient.

Assumption 3.2. The objective function f is L-smooth with L > 0 and bounded from below by f∗ ∈ R. That is, f has a Lipschitz continuous gradient with a Lipschitz constant L:
∇f (x) − ∇f (y) 2 ≤ L x − y 2, ∀x, y ∈ Rn
and f (x) ≥ f∗ for all x ∈ Rn.

3.1. Random Search Directions. Our analysis in the sequel of the paper will be based on the following key assumption.

Assumption 3.3. The probability distribution D on Rn has the following proper-

ties:

1. The quantity γD d=ef Es∼D

s

2 2

is

positive

and

ﬁnite.

2. There is a constant µD > 0 and norm · D on Rn such for all g ∈ Rn,

(3.2)

Es∼D | g, s | ≥ µD g D.

Note that since all norms in Rn are equivalent, the second part of the above assumption is satisﬁed if and only if

inf Es∼D | g, s | > 0.
g 2=1

However, as the next lemma illustrates, it will be convenient to work with norms that are allowed to depend on D. We now give some examples of distributions for which the above assumption is satisﬁed.
7

Lemma 3.4. Let g ∈ Rn. 1. If D is the uniform distribution on the unit sphere in Rn, then

(3.3)

1

γD = 1 and Es∼D | g, s | ∼ √

g 2.

2πn

Hence, D satisﬁes Assumption 3.3 with γD = 1, · D = · 2 and µD ∼ √21πn .

2. If D is the normal distribution with zero mean and identity over n as covari-

ance matrix. i.e., s ∼ N (0, nI ), then

(3.4)

√ 2
γD = 1 and Es∼D | g, s | = √ g 2. nπ

√
Hence, D satisﬁes Assumption 3.3 with γD = 1, · D = · 2 and µD = √n2π .

3. If D is the uniform distribution on {e1, . . . , en}, then

(3.5)

1 γD = 1 and Es∼D | g, s | = n g 1.

Hence, D satisﬁes Assumption 3.3 with γD = 1, · D = · 1 and µD = n1 . 4. If D is an arbitrary distribution on {e1, . . . , en} given by P (s = ei) = pi > 0,
then

(3.6)

n
γD = 1 and Es∼D | g, s | = g D d=ef pi|gi|.

i=1

Hence, D satisﬁes Assumption 3.3 with γD = 1 and µD = 1.
5. If D is a distribution on D = {d1, . . . , dn} where d1, . . . , dn form an orthonormal basis of Rn and P (s = di) = pi, then

(3.7)

n
γD = 1 and Es∼D | g, s | = g D d=ef pi|gi|.
i=1

Hence, D satisﬁes Assumption 3.3 with γD = 1 and µD = 1.

Proof. See Appendix A.

Without loss of generality, in the rest of this paper we assume that γD = 1. This can be achieved by considering distribution D instead, where s ∼ D is obtained
by ﬁrst sampling s from D and then either normalizing via i) s = s / s 2, or ii) s = s / Es ∼D s 22.

3.2. Key Lemma. Now, we establish the key result which will be used to prove the main properties of our Algorithm. Its similar result in the case of deterministic direct search (DDS) methods states that the gradient of the objective function for unsuccessful iterations is bounded by a constant multiplied by the stepsize. See for instance [14, Lemma 10].

Lemma 3.5. If Assumptions 3.2 and 3.3 hold, then for all k ≥ 0,

(3.8)

E [f (xk+1) | xk] ≤ f (xk) − µDαk ∇f (xk) D + L2 αk2,

and

(3.9)

θk+1 ≤ θk − µDαkgk + L2 αk2 ,

where θk = E[f (xk)] and gk = E[ ∇f (xk) D].

8

Proof. First we notice that from L-smoothness of f we have

f (xk + αksk) ≤ f (xk) +

∇f (xk), αksk

+ L2

αk sk

2 2

= f (xk) + αk ∇f (xk), sk + L2 αk2 sk 22,

and, similarly, f (xk − αksk) ≤ f (xk) − αk ∇f (xk), sk + L2 αk2 sk 22. Hence,

f (xk+1) ≤ min{f (xk + αksk), f (xk − αksk)} ≤ f (xk) − αk|

∇f (xk), sk

|

+

L 2

αk2

sk

22.

To conclude (3.8), we only need to take expectation in the above inequality with respect to sk ∼ D, conditional on xk, and use inequality (3.2). By taking the expectation in (3.8) we get (3.9).
Note that (3.8) can equivalently be written in the following form:

1 ∇f (xk) D ≤ µD

f (xk) − E [f (xk+1) | xk] + L αk .

αk

2

This form makes it possible to compare this result with a key result used in the analysis of DDS. Indeed, if we assume that the opposite of the following suﬃcient expected decrease condition holds

(3.10)

f (xk) − E [f (xk+1) | xk] ≥ cαk2,

for some c > 0, then we obtain

(3.11)

1

L

∇f (xk) D ≤ µD c + 2 αk.

In DDS, condition (3.10) is equivalent to the suﬃcient decrease condition f (xk) − f (xk+1) ≥ cαk2. If such condition does not hold than the step is declared unsuccessful. The inequality in (3.11) is similar with the result in [14, Lemma 10]. In DDS methods,
one can check the suﬃcient decrease condition, so this drives the analysis and allows
for simple stepsize update rules to be implemented. In STP, we typically cannot
evaluate E[f (xk+1) | xk] (we can if D has all its mass on a discrete set – but in that case we would need to do more work per iteration).

4. Non-convex Problems. In this section, we state our most general complexity result where we do not make any additional assumptions on f , besides smoothness and boundedness (see Assumption 3.2).
Theorem 4.1 (Decreasing stepsize). Let Assumptions 3.2 and 3.3 hold. Choose αk = √αk+0 1 , where α0 > 0. If

(4.1)

√

2

2 2(f (αx00)−f∗) + L2α0

K≥

µ2 ε2

,

D

then mink=0,1,...,K E [ ∇f (xk) D] ≤ ε.

Proof. We base the proof on the analysis of the recursion (3.9). In particular, it is useful to write it in the following form:

(4.2)

gk ≤ µ1D

θk−αθkk+1 + L2 αk

= µ1D
9

√
(θk−θkα+01) k+1 + 2√Lkα+0 1 .

We know from (3.1) and the assumption that f is bounded below that f∗ ≤ θk+1 ≤ θk ≤ f (x0) for all k. Letting l = K/2 , this implies that
2l
(θj − θj+1) = θl − θ2l+1 ≤ f (x0) − f∗ d=ef C,
j=l

from which we conclude that there must exist j ∈ {l, . . . , 2l} such that θj − θj+1 ≤ C/(l + 1). This implies that

(4.2)
gj ≤ µ1D ≤ µ1D

+ √
(θj −θj+1) j+1 α0
√
Cα0(2l+l+11) + 2√Lαl+0 1

√Lα0 2 j+1

≤ µ1D

√
αC0(lj++11) +

≤ √1
µD l+1

√
α20C + L2α0

√Lα0 2 j+1

≤ √1
µD K/2

√
α20C + L2α0

(4.1)
≤ ε.

Let us now give some insights into the above theorem.

• Sphere setup. If D is the uniform distribution on the Euclidean sphere,

then µD ∼ √21πn , and hence the above theorem gives a complexity guarantee

of the form

n O ε2 .

This is an improvement on DDS where the best known complexity bound is O(n2/ε2) [22, 14]. The same conclusion holds for the normal distribution

setup.

• Coordinate setup. If D is the uniform distribution on {e1, . . . , en}, then

µD = 1/n and hence the bound is of the form

n2 O ε2 .

However, this is for the 1 norm of th√e gradient of f , which is larger than the 2 norm. Indeed, for all x we have n ∇f (x) 2 ≥ ∇f (x) 1 ≥ ∇f (x) 2, and the ﬁrst inequality can be tight (for the vector of all ones, for instance). Hence, if we are interested to achieve ∇f (x) 2 ≤ ε , in certain√situations it may be suﬃcient to push the 1 norm of the gradient below ε = nε instead. So, the iteration bound can be as good as

n2

n

O (√nε )2 = O (ε )2 .

• Quality of the ﬁnal iterate. Theorem 4.1 does not guarantee the gradient of f at the ﬁnal point xK to be small (in expectation). Instead, it guarantees that the gradient of f at some point produced by the method will be small. Notice however, that the method is monotonic. Hence, all subsequent points produced by the method will have better functions values than the one which has gradient of minimum norm (in expectation). So, we can say that f (xK) ≤ f (xj) where E [ ∇f (xj) D] ≤ ε.
• Optimal stepsize. Note that the complexity depends on α0. The optimal choice (minimizing the complexity bound) is

α∗ = 81/4
10

f (x0) − f∗ , L

in which case the complexity bound (4.1) takes the form

(4.3)

√

4 2(f (x0) − f∗)L

µ2 ε2

.

D

Assume that the lower bound f∗ is achieved by some point x∗ ∈ Rn. Necessarily, ∇f (x∗) = 0. Moreover, since f is L-smooth, we can write

f (x0) ≤ f (x∗) + ∇f (x∗), x0 − x∗ + L2 x0 − x∗ 22. Hence, the optimal stepsize is no larger than

α∗ ≤ 21/4 x0 − x∗ 2.

Of course, we cannot use this optimal stepsize as we usually do not know L and/or f∗. So, we are paying for the lack of knowledge by an increased complexity bound. This makes intuitive sense: the stepsize should not be much larger than the distance of the initial point to an optimal point. On the other hand, there are examples of non-convex functions for which the ratio (f (x0) − f∗)/L is arbitrarily small, and the distance between x0 and x∗ arbitrarily high. This cannot happen for convex functions with bounded level sets or for strongly convex functions, as then f (x) − f (x∗) can be lower bounded by quantity proportional to x − x∗ 2 with some positive power. We now state a complexity theorem for STP used with a ﬁxed stepsize.
Theorem 4.2 (Fixed stepsize). Let f satisfy Assumption 3.2 and also assume that f is bounded below by f∗ ∈ R. Choose a ﬁxed stepsize αk = α with 0 < α < 2µDε/L. If

(4.4)

def f (x0) − f∗ K ≥ k(ε) = (µDε − L2 α)α − 1,

then mink=0,1,...,K E [ ∇f (xk) D] ≤ ε. In particular, if α = µDε/L, then

2L(f (x0) − f∗)

k(ε) =

µ2 ε2

− 1.

D

Proof. If gk ≤ ε for some k ≤ k(ε), then we are done. Assume hence by contradiction that gk > ε for all k ≤ k(ε). By taking expectation in Lemma 3.5, we get
θk+1 ≤ θk − µDαgk + L2 α2,
where θk = E[f (xk)] and gk = E[ ∇f (xk) D]. Hence,

(4.4)
f∗ ≤ θK+1 < θ0 − (K + 1) µDαε − L2 α2 ≤ θ0 − (f (x0) − f∗) = f∗,

which is a contradiction.
Here we give some comments about the STP for non-convex functions. • In some situations, when L is not available, it is impossible to compute optimal α = µLDε . • If we can guess α is close to the optimal, then the method depends linearly on n if 1/µ2D = O(n).
11

• Also, if we guess α right, we get complexity that depends on L(f (x0) − f∗), which is similar to the setup with variable stepsizes and optimal α0.
• As before, we only get guarantee on the best of the points in term of the gradient norm, not on the ﬁnal point.
5. Convex Problems. In this section we estimate the complexity of the STP in the case of convex f . In this case we need an additional technical assumption.
Assumption 5.1. We assume that f is convex, has a minimizer x∗, and has bounded level set at x0:

R0 d=ef max{

x − x∗

∗ D

:

f (x) ≤ f (x0)} < +∞,

where ξ ∗D d=ef max{ ξ, x | x D ≤ 1} deﬁnes the dual norm to · D.

Note that if the above assumption holds, then whenever f (x) ≤ f (x0), we get

f (x) − f (x∗) ≤

∇f (x), x − x∗

≤

∇f (x)

D

x − x∗

∗ D

≤

R0

∇f (x)

D.

That is,

(5.1)

∇f (x) D ≥ f (x) − f (x∗) . R0

Now, we state our main complexity result of this section. We start with the analysis of STP with constant stepsizes.
Theorem 5.2 (Constant stepsize). Let Assumptions 3.2, 3.3 and 5.1 be satisﬁed. Let 0 < ε < LµR2D02 and choose constant stepsize αk = α = LεµRD0 . If

(5.2)

LR02

2(f (x0) − f (x∗))

K ≥ µ2 ε log

ε

,

D

then E [f (xK ) − f (x∗)] ≤ ε. Proof. Let us substitute (5.1) into Lemma 3.5 and take expectations. We get

(5.3)

θk+1 ≤ θk − µRD0α (θk − f (x∗)) + L2 α2.

Let rk = θk − f (x∗) and c = 1 − µRD0α ∈ (0, 1). Subtracting f (x∗) from both sides of (5.3), we obtain

K −1

rK ≤ crK−1 + L2 α2 ≤ cK r0 + L2 α2

ci

i=0

≤ exp(−µDαK/R0)r0 + 2(L1α−2c) = exp(−µDαK/R0)r0 + 2ε (5≤.2) ε.

If µD ∼ √1n , then the above theorem gives a complexity guarantee of the form

n

1

O log

.

ε

ε

Comparing this to the best known complexity bound for DDS which is O( nε2 ) [5, 14], we improve the dependence on n but we deteriorate the dependence on ε because of the presence of the term log 1ε . In the next theorem we show how we get rid of the log 1ε term using variable stepsize.
12

Theorem 5.3 (Variable stepsize). Let Assumptions 3.2, 3.3 and 5.1 be satisﬁed. Let αk = α0 (f (xk) − f (x∗)), where 0 < α0 < R2µ0DL . Deﬁne a = µD Rα0 0 − L2α20 > 0. If k ≥ k(ε) d=ef a1 1ε − r10 , then E [f (xk) − f (x∗)] ≤ ε.
Proof. Let us substitute (5.1) into equation (3.8) of Lemma 3.5, and then substrate f (x∗) from both sides we get

E [f (xk+1) | xk] − f (x∗) ≤ f (xk) − f (x∗) − µDαk f(xk)R−0f(x∗) + L2 αk2 .

Let rk = E [f (xk)] − f (x∗). By using our choice of αk in the previous equation

and then taking the expectation we get rk+1 ≤ rk −

− µD α0
R

Lα20 2

rk2 = rk − ark2.

0

Therefore,

− = ≥ ≥ a. 1

1

rk+1

rk

rk −rk+1 rk rk+1

rk −rk+1 rk2

From this we have r1

≥ r1

+ ka and hence rk ≤

1 1

. It remains to notice that for

k

0

r0 +ka

k ≥ a1

1ε − r10

we have rk ≤

1 1

≤ ε.

r0 +ka

If α0 = Rµ0DL , then a is maximal as a function of α0, for which we get the optimal

bound

2R02L 1 1

k(ε) = µ2
D

−. ε r0

If µD ∼ √1n , then the above theorem gives a complexity guarantee of the form O

n ε

.

The stepsizes in the previous theorem depend on f (x∗). Of course, in practice we

cannot always use these stepsizes as we usually do not know f (x∗). Next theorem gives

a more practical stepsizes for which we get the same complexity as in the previous

theorem. We start by stating an extra assumption on the probability distribution D

and show that this assumption is satisﬁed for all the probability distributions given

in Lemma 3.4.

Assumption 5.4. The probability distribution D on Rn is such that for all s ∼ D are of unit Euclidean norm ( s 2 = 1) with probability 1.

Let CD be the positive constant such that for all x ∈ Rn the following inequality
holds: x 2 ≤ CD x D. Such constant exists due to the equivalence of the norms in Rn.

Theorem 5.5 (Solution-free stepsize). Let Assumptions 3.2, 3.3, 5.1 and 5.4 be satisﬁed. Let αk = |f(xk+tsLkt)−f(xk)| , where
√ 0 < t ≤ 2µDE [f (xK−1) − f∗] .
LR0

Deﬁne a = µ2D . If K ≥ k(ε) d=ef 1 1 − 1

4LR02

a ε r0

Proof. From Lemma 3.5 we have

, then E [f (xK ) − f (x∗)] ≤ ε.

(5.4)

f (xk+1) ≤ f (xk) − αk| ∇f (xk), sk | + Lα2 2k .

We know that αkopt = | ∇f(xLk),sk | minimizes the right hand side of (5.4). But it depends on ∇f (xk) which we can not compute exactly, because we have zeroth-order
13

oracle. Actually, we do not need to know the whole gradient, it is enough to know
directional derivative of f , which we can approximate by ﬁnite diﬀerence of function values of f . It is the main idea behind our choice of αkopt = |f(xk+tsLkt)−f(xk)| , which does not depends any more on f (x∗) and can be easily computed in practice. We can rewrite αk = |f (xk+tsLkt)−f (xk)| = | ∇f (xLk),sk | + |f (xk+tsLkt)−f (xk)| − | ∇f (xLk),sk | d=ef αkopt + δk. Therefore, we have

f (xk+1) ≤ f (xk) − | ∇f(xLk),sk |2 − δk| ∇f (xk), xk | + | ∇f(x2kL),xk |2 +δk| ∇f (xk), xk | + L2 (δk)2
= f (xk) − | ∇f (x2kL),sk |2 + L2 (δk)2

Next we estimate |δk| using L-smoothness of f :

|δk |

=

1 Lt

||f (xk

+ tsk) − f (xk)| − |

∇f (xk), tsk

||

≤

1 Lt

|f (xk

+

tsk )

−

f (xk)

−

∇f (xk), tsk

| ≤ L1t · L2

tsk

2 2

=

2t .

From this we obtain

(5.5)

f (xk+1) ≤ f (xk) − | ∇f (x2kL),sk |2 + L8t2 .

Taking mathematical expectation w.r.t. all randomness from the previous inequality we get

(5.6)

E[f (xk+1)] − f∗
rk+1

≤x E[f (xk)] − f∗ − µ2L2D E[ ∇f (xk) 2D] + L8t2
rk
≤y rk − 2Lµ2D R02 rk2 + L8t2 ,

where x is due to tower property of mathematical expectation and (3.2):

E[| ∇f (xk), sk |2] = E E[| ∇f (xk), sk |2 | xk] ≥ E (E[| ∇f (xk), sk | | xk])2

(3.2)
≥ µ2DE[ ∇f (xk) 2D];

y follows from Assumption 5.1: E[ ∇f (xk) 2D] ≥ E[(f(xRk)02−f∗)2] ≥ (E[f(xRk)02−f∗])2 = Rrk202 . From this and monotonicity of {f (xk)}k≥0 we have

(5.7)

− ≥ ≥ ≥ − 1

1

rk+1

rk

rk+1 −rk rk rk+1

µ2D 2 Lt2 2 rk− 8
2LR0 rk2

µ2D

L

2LR02

8

√
If k ≤ K − 1 and 0 < t ≤ 2µLDRr0K−1 , then we can write

2
rtk .

1 − 1 ≥ µ2D = a,

rk+1

rk

4LR02

since

rk

≤

rK−1.

Finally,

we

have

1 r

≥

1 r

+ ka

and

hence

rk

≤

1 1

. for all k ≤ K.

k

0

r0 +ka

Thus, if K ≥ a1

1ε − r1

, then rK ≤

1 1

≤ ε.

0

r0 +Ka

√

√

Actually, requirement t ≤ 2µDE[fL(RxK 0 −1)−f∗] could be replaced by t ≤ L2RµD0 ε if

we additionally require that for all k ≤ K we have rk ≥ ε.

14

6. Strongly Convex Problems. In this section we derive the complexity of the STP method in the case of strongly convex f .

Assumption 6.1. f is λ-strongly convex with respect to the norm · D.

In this section, we denote by x∗ the unique minimizer of f .

Theorem 6.2. Let Assumptions 3.2, 3.3 and 6.1 be satisﬁed. Let stepsize αk =

θk µD L

2λ(f (xk) − f (x∗)), for some θk ∈ (0, 2) such that θ d=ef infk 2θk − θk2 > 0. If

(6.1)

L

f (x0) − f (x∗)

K ≥ λµ2 θ log

ε

,

D

then E [f (xK ) − f (x∗)] ≤ ε.

Proof. By injecting αk into equation (3.8) of Lemma 3.5, and then substrate f (x∗) from both sides we get

√ E[f (xk+1) | xk] − f (x∗) ≤ f (xk) − f (x∗) − µ2Dθk 2λ(f(xk)−Lf(x∗)) ∇f(xk) D

+

µ2D

θ

2 k

λ

(f

(x

k

)−

f

(x

∗

))

.

L

From the strong convexity property of f we have

∇f (xk)

2 D

≥

2λ(f (xk) − f (x∗)),

therefore

E[f (xk+1)|xk] − f (x∗) ≤ f (xk) − f (x∗) − 2µ2Dθkλ(f (Lxk)−f (x∗)) + µ2Dθk2λ(f (xLk)−f (x∗)) ≤ f (xk) − f (x∗) − µ2Dλ(f(xLk)−f(x∗)) (2θk − θk2) ≤ f (xk) − f (x∗) − µ2Dθλ(f(xLk)−f(x∗)) ,

where we used the deﬁnition of θ. Let rk = E [f (xk)] − f (x∗). By taking the expectation of the last inequality we get rk+1 ≤ 1 − µ2DLθλ rk, and therefore

µ2 θλ k

rk ≤

1−

D
L

r0.

Hence if K satisﬁes (6.1), we get rK ≤ ε. From this theorem we conclude that if there exist 0 < θ1 ≤ θ2 < 2 such that

θ1µD L

2λ(f (xk) − f (x∗)) ≤ αk ≤ θ2µD L

2λ(f (xk) − f (x∗)),

then the sequence (rk)k converges linearly to zero. The stepsizes from the previous theorem depend on f (x∗). In practice, we cannot
always use these stepsizes as we usually do not know f (x∗). Next theorem gives the similar result for STP with stepsizes independent from f (x∗) under additional assumptions that for all s ∼ D we have s 2 = 1 with probability 1.
Theorem 6.3. Let Assumpti√ons 3.2, 3.3, 5.4 and 6.1 be satisﬁed. Let αk = |f (xk+tsLkt)−f (xk)| , for 1 < t ≤ 2µDL λε . If

(6.2)

L

2(f (x0) − f (x∗))

K ≥ λµ2 log

ε

,

D

then E [f (xK ) − f (x∗)] ≤ ε.
15

Proof. From (5.5) we have f (xk+1) ≤ f (xk) − | ∇f(x2kL),sk |2 + L8t2 . Taking mathematical expectation w.r.t. all randomness from the previous inequality we get

(6.3)

E[f (xk+1)] − f∗
rk+1

≤x E[f (xk)] − f∗ − µ2L2D E[ ∇f (xk) 2D] + L8t2
rk
≤y 1 − µ2D Lλ rk + L8t2 ,

where x is due to tower property of mathematical expectation and (3.2):

E[| ∇f (xk), sk |2] = E E[| ∇f (xk), sk |2 | xk] ≥ E (E[| ∇f (xk), sk | | xk])2

(3.2)
≥ µ2DE[ ∇f (xk) 2D];

y follows from λ-strong convexity of f :

∇f (xk)

2 D

≥ 2λ (f (xk) − f∗).

From (6.3)

we have

(6.4)

rk+1

≤ 1 − µ2D Lλ k+1 r0 + L8t2 k

µ2 λ i

1−

D
L

i=0

≤ 1 − µ2D Lλ k+1 r0 + 8Lµ22Dt2λ .

√

Hence

if

t

≤

2µD L

λε

and

K

satisﬁes

(6.2)

we

get

rK

≤

ε.

7. Parallel Stochastic Three Points Method. Consider the parallel version of STP proposed in Algorithm 7.1.

Algorithm 7.1 Parallel Stochastic Three Points (PSTP)

Initialization Choose x0 ∈ Rn, stepsizes αk > 0, parallelism parameter τ , diﬀerentiation stepsize t0.

For k = 0, 1, 2, . . .

1. For i = 1, 2, . . . , τ . Generate a random vector ski ∼ D.

τ

2.

Let

sk

=

1 τ

ski.

i=1

3. Let x+ = xk + αksk and x− = xk − αksk.

4. xk+1 = arg min{f (x−), f (x+), f (xk)}.

We start our analysis of the complexity in this section by stating an extra assumption on the probability distribution D which is satisﬁed for all the probability distributions given in Lemma 3.4.
Assumption 7.1. The probability distribution D on Rn satisﬁes the following properties.
1. If s1, s2 ∼ D are independent, then E[ s1, s2 ] = 0. 2. There exist a constant µ˜D > 0 and τ > 0 such that if s1, s2, . . . , sτ ∼ D are
independent and for all g ∈ Rn

1τ

µ˜D

E g, τ si ≥ √τ g 2.

i=1

16

If the ﬁrst part of the assumption does not hold for distribution D we can consider distribution D¯ such that Es∼D[s] = 0 by adding opposite vector for each vector from D and share the probability measure between opposite vectors in equal ratio. The second part of the assumption holds due to Central Limit Theorem for wide range of distributions (this range covers the examples in Lemma 3.4) and due to the second
part of Lemma 3.4 we can say that for big enough τ we have µ˜D ∼ π2n in the case when γD = 1.
In the next three subsections we will give the adaptation of the main complexity results obtained for STP for PSTP.

7.1. Non-convex Case. The following theorem is the adaptation of Theorem 4.1.
Theorem 7.2. Let Assumptions 3.2, 3.3, 5.4 and 7.1 hold. Choose αk = √αk+0 1 , where α0 > 0. If

(7.1)

√

2

2 2τ (f (αx00)−f∗) + 2L√ατ0

K≥

µ˜2 ε2

,

D

then

min E [ ∇f (xk) 2] ≤ ε.
k=0,1,...,K

Proof. By deﬁnition of x+ and x− we have

τ

x+

=

xk

+

αk τ

ski,

i=1

τ

x−

=

xk

−

αk τ

ski,

i=1

whence

f (x+) f (x−)

τ

≤

f (xk) − αk

∇f (xk),

1 τ

ski

i=1

τ

≤

f (xk) + αk

∇f (xk),

1 τ

ski

i=1

+ Lα2k
2τ 2
+ Lα2k
2τ 2

τ

2

ski ,

i=1

2

τ

2

ski .

i=1

2

Therefore

f (xk+1) ≤ min{f (x+), f (x−)}≤ f (xk) − αk

τ

∇f

(xk ),

1 τ

ski

i=1

+ Lα2k
2τ 2

τ

2

ski .

i=1

2

Taking conditional mathematical expectation E[ · | xk] from the both sides of previous inequality we have

(7.2)

E[f (xk+1) | xk] − f∗ ≤x f (xk) − f∗ − α√kµ˜τD ∇f (xk) 2 + L2ατ2k

where x is due to the ﬁrst part of Assumption 7.1 and Assumption 3.3:


τ

2 τ

τ

E  ski  = E[ ski 22] +

E[ ski, skj ] = τ.

i=1

2

i=1

i=j=1

17

Taking full expectation from the both sides of the inequality (7.2) and rearranging

the terms we obtain

(7.3)

√
gk ≤ µ˜Dτ

θk−αθkk+1 + 2Lτ αk

√
= µ˜Dτ

√
(θk−θkα+01) k+1 + 2τL√αk0+1 ,

where gk = ∇f (xk) 2. We know from (3.1) and the assumption that f is bounded below that f∗ ≤ θk+1 ≤ θk ≤ f (x0) for all k. Letting l = K/2 , this implies that
2l
θj − θj+1 = θl − θ2l+1 ≤ f (x0) − f∗ d=ef C,
j=l

from which we conclude that there must exist j ∈ {l, . . . , 2l} such that θj − θj+1 ≤ C/(l + 1). This implies that

(7.3) √
gj ≤ µ˜Dτ
√
≤ µ˜Dτ

+ √
(θj −θj+1) j+1 α0

L√α0 2τ j+1

√
≤ µ˜Dτ

√
αC0(lj++11) +

√
Cα0(2l+l+11) + 2τL√αl0+1

√
≤ √τ
µ˜D l+1

√
α20C + L2ατ0

L√α0 2τ j+1

≤ √1
µ˜D K/2

√
α2τ0C + 2L√ατ0

(7.1)
≤ ε.

√√ √ Note that α0 = 2 2τ √f(x0)−f∗ gives the optimal rate which does not depend on
L

τ and coincides with the rate for spherical setup in the STP method. It means that for

big

enough

τ

the

previous

theorem

gives

a

complexity

guarantee

of

the

form

O

(

n ε2

).

7.2. Convex Case. In this subsection we state the main complexity result when f is convex.

Theorem 7.3. Let Assumptions 3.2, 5.1 (with · D = · 2), 5.4 and 7.1 be satisﬁed. Let αk = α0 (f (xk) − f∗), where 0 < α0 ≤ 2Rτ0µ˜LD . Deﬁne a = √µ˜DτRα00 − L2ατ20 .
If k ≥ k(ε) d=ef a1 1ε − r10 , then E [f (xk) − f (x∗)] ≤ ε.

Proof. We have

(7.4)

E[f (xk+1) | xk] − f∗

≤x f (xk) − f∗ − α√kµ˜τD ∇f (xk) 2 + L2ατ2k ≤y f (xk) − f∗ − Rµ˜D0√αkτ (f (xk) − f∗) + L2ατ2k

where x follows from (7.2), and y follows from Assumption 5.1. Using our choice of αk = α0(f (xk) − f∗) and taking full mathematical expectation from the both sides of (7.2) we obtain

rk+1 ≤ rk −

− √µ˜D α0
τR

Lα20 2τ

rk2 = rk − ark2.

0

Therefore,

1

−

1

=

rk −rk+1

≥

rk −rk+1
2

≥ a.

From

this

we

have

1

≥

1

+ ka and

rk+1

rk

rk rk+1

rk

rk

r0

hence rk ≤

1 1

. Finally, if k ≥ a1

1ε − r1

, then rk ≤

1 1

≤ ε.

r0 +ka

0

r0 +ka

Note that α0 = √Rτ0µ˜LD maximizes the value a. The optimal value of a is 2Rµ˜02DL2 , which is proportional to πnR10L2 due to the second part of Lemma 3.4. It means that for big enough τ the above theorem gives an iteration complexity guarantee of the form

O( nε ).

18

7.3. Strongly Convex Case. In this subsection we state the main complexity result when f is strongly convex. The following theorem is an adaptation of Theorem 6.3.
Theorem 7.4. Let Assumptions 3.3, 3.2, 5.4, 6.1 and 7.1 be satisﬁed. Let αk = θkµ˜LD√τ 2λ(f (xk) − f (x∗)), for some θk ∈ (0, 2) such that θ d=ef infk 2θk − θk2 > 0. If

(7.5)

L

f (x0) − f (x∗)

K ≥ λµ˜2 θ log

ε

,

D

then E [f (xK ) − f (x∗)] ≤ ε.

Proof. By injecting αk into the ﬁrst inequality of (7.2) we get

√ E[f (xk+1)|xk] − f (x∗) ≤ f (xk) − f (x∗) − µ˜2Dθk 2λ(f(xk)−Lf(x∗)) ∇f(xk) 2

+

µ˜2D

θ

2 k

λ(f

(x

k

)

−f

(x

∗

))

.

L

From the strong convexity property of f we have

∇f (xk)

2 2

≥

2λ(f (xk) − f (x∗)).

Therefore

E[f (xk+1)|xk] − f (x∗) ≤ f (xk) − f (x∗) − 2µ˜2Dθkλ(f (Lxk)−f (x∗)) + µ˜2Dθk2λ(f (xLk)−f (x∗)) ≤ f (xk) − f (x∗) − µ˜2Dλ(f(xLk)−f(x∗)) (2θk − θk2) ≤ f (xk) − f (x∗) − µ˜2Dθλ(f (xLk)−f (x∗)) ,

by using the deﬁnition of θ. Let rk = E [f (xk)] − f (x∗). By taking the expectation of the last inequality we get rk+1 ≤ 1 − µ˜2DLθλ rk, hence

µ˜2 θλ k

rk ≤

1−

D
L

r0.

Therefore, if K satisﬁes (7.5), we get rK ≤ ε.
For big enough τ the above theorem gives an iteration complexity guarantee of the form O(n log 1ε ).
8. Numerical Results. In this section, we report the results of some preliminary experiments performed in order to assess the eﬃciency and the robustness of the proposed algorithms compared to the coordinate search method (this method will be called DDS) and the algorithm proposed in [18]. In the latter approach, at each iteration k, a random vector sk following the uniform distribution on the unit sphere is generated, then the next iterate is computed as follows

(8.1)

xk+1 = xk − αk f (xk + µksk) − f (xk) sk, µk

where µk ∈ (0, 1) is the ﬁnite diﬀerences parameter, and αk is the stepsize. This method generates a trial step similar to one of the trial steps in our method (x− = xk −αksk) when the probability distribution D is chosen to be the uniform distribution on the unit sphere up to the multiplication of the step by f(xk+µkµskk)−f(xk) . This method will be called RGF (Random Gradient free method). All the results presented here are averaged over 10 runs of the algorithms. We did all our experiments using Matlab.
19

To compare the performance of the algorithms we use performance proﬁles proposed by Dolan and Mor´e [6] over a variety of problems. Given a set of problems P (of cardinality |P|) and a set of algorithms (solvers) S, the performance proﬁle ρs(τ ) of an algorithm s is deﬁned as the fraction of problems where the performance ratio rp,s is at most τ

1 ρs(τ ) = |P| size{p ∈ P : rp,s ≤ τ }.

The performance ratio rp,s is in turn deﬁned by

rp,s =

tp,s ,

min{tp,s : s ∈ S}

where tp,s > 0 measures the performance of the algorithm s when solving problem p,

seen here as the number of function evaluation. Better performance of the algorithm s,

relatively to the other algorithms on the set of problems, is indicated by higher values

of ρs(τ ). In particular, eﬃciency is measured by ρs(1) (the fraction of problems

for which algorithm s performs the best) and robustness is measured by ρs(τ ) for τ

suﬃciently large (the fraction of problems solved by s). Following what is suggested

in [6] for a better visualization, we will plot the performance proﬁles in a log2-scale

(for which τ = 1 will correspond to τ = 0).

The distribution D used here for our random direction generation is the uniform

distribution on the unit sphere. We performed other experiments (not reported here)

with diﬀerent choices for distributions D. For instance, the distributions listed in

Lemma 3.4. We found similar performance as those reported here. The parameters

deﬁning the implemented algorithms are set as follows: For RGF we choose µk = 10−4,

and αk =

1 4(n+4)

where n is the problem dimension.

For this method the authors

proposed to use the stepsize αk = 4L(n1+4) , where L is the Lipschitz constant of the

gradient of the objective function. Since for our test problems we do not know this

constant, we ran RGF method with diﬀerent values for L, for instance 0.1, 1, 10, and

100. The best performance was found for L = 1. The stepsize in DDS is initialized

by α0 = 1, then it is updated dynamically with the iterations by multiplying it by 2

when the step is successful and dividing it by 2 otherwise.

For all algorithms, we counted the number of function evaluations taken to (i)

drive the function value below f ∗ + ε (f (x0) − f ∗), where f ∗ is a local minimal value of the objective function f , and ε is a tolerance. In our experiments ε = 10−1, 10−3

and 10−5, (ii) or the maximum number of iterations attains 100000.

8.1. Non-Convex Case. In this section, we report the results of comparison of our approach STP for non-convex problems with DDS and RGF. We will call our STP method when using the variable stepsize STP-vs, and STP-fs when we use a ﬁx stepsize. For STP-vs we choose αk = √k1+1 . For STP-fs we choose αk = α = 0.1ε.
We use the Mor´e/Garbow/Hillstrom 34 test problems [17] which are implemented in Matlab. All the test problems are smooth. The dimension n of the problems changes between n = 2 to n = 100, typically n = 2, 10, 50 and 100. We use the starting points and the values f ∗ suggested in [17] for all the problems.
Figure 1 depicts the performance proﬁles of the algorithms. It shows that our approach (the methods STP-vs and STP-fs) improves the eﬃciency of the DDS and RGF algorithms on the tested problems. In fact, the number of the function evaluations performance proﬁles show that the use of the random directions leads to a signiﬁcant

20

improvement on terms of the eﬃciency (for τ = 0, on about 40% of the tested problems
our approach performs the best, and less than 5% for RGF and DDS). From Figures 1(a) and 1(b), we see that the use of the random directions leads to a better robustness when a small precision is targeted (i.e. ε = 10−1 and ε = 10−3). However, when a big precision ( ε = 10−5) is targeted DDS becomes competitive. In fact, as shown in Figure 1(c), DDS is more robust than RGF approach and our method using ﬁx stepsize. Our method STP-vs still more robust than DDS.

s( ) s( )

1

0.8

0.6

0.4

0.2

0

0

2

4

6

8

(a) ε = 10−1

1

0.8

0.6

0.4

0.2

0

0

2

4

6

8 10 12

(b) ε = 10−3

1

0.8

0.6

0.4

0.2

0

0

2

4

6

8 10 12

(c) ε = 10−5

Fig. 1. Performance proﬁles on 34 optimization problems.

8.2. Convex Case. In this section, we report the results of comparison of two

STP methods for convex problems with DDS and RGF. The ﬁrst STP method is the one

using the variable stepsize αk =

1 Lt

(f

(xk

+

tsk )

−

f

(xk ))

,

where

t = 10−4.

We

will

call this method STP-vs. The second STP method is the one using the ﬁx stepsize

αk = α = 0.1ε. It will be called STP-fs.

We selected from the Mor´e/Garbow/Hillstrom problems those with a unique min-

imum. To have a large bed test, we create diﬀerent instances for problems by varying

the problem dimension n when it is possible. Our test bed in this section contains 40

problems.

In Figure 2, the performance proﬁles show that the random based methods ( RGF

method and our two methods STP-vs and STP-vs) outperform by far the DDS method.

Our method STP-vs gives the best performances for small precision (see Figures 2(a)

and 2(b)). For big precision (ε = 1e − 5), it gives almost similar performances as RGF

21

method ((see Figure 2(c)). Our method STP-fs is outperformed by RGF.

s( ) s( )
s( )

1

0.8

0.6

0.4

0.2

0

0

2

4

6

8

10 12

(a) ε = 10−1

1

0.8

0.6

0.4

0.2

0

0

2

4

6

8 10 12

(b) ε = 10−3

1

0.8

0.6

0.4

0.2

0

0

5

10

15

(c) ε = 10−5

Fig. 2. Performance proﬁles on 40 optimization problems.

8.3. First order methods. In this section, we report the results of comparison of gradient based methods that our approach cover, using the variable stepsize αk = √k1+1 and the ﬁx stepsize αk = 0.1ε. In fact to select these stepsizes, we run many experiments with diﬀerent values and found the best results for the chosen stepsizes. We denote with ngd-vs, and ngd-fs the Normalized Gradient Descent (NGD) methods using the variable stepsize, and the ﬁx stepsize respectively. With similar notation we denote by signgd-vs, and signgd-fs the Signed Gradient Descent (SignGD) methods and by nrcd-vs, and nrcd-fs Normalized Randomized Coordinate Descent (NRCD) methods using the variable stepsize, and the ﬁx stepsize respectively.
We use the Mor´e/Garbow/Hillstrom 34 test problems for which we add 20 problems by creating diﬀerent instances for problems by varying the problem dimension n when it is possible. Our test bed in this section contains 54 problems.
Figure 3 depicts the performance proﬁles of the algorithms. It shows that the use of the variable stepsize gives better performances than the ﬁx stepsize. As one may expect, the normalized gradient descent method ngd-vs exhibits performances better than the other methods, except for small precision (ε = 1e − 1), it is less eﬃcient than
22

signed gradient descent method signgd-vs. The latter method is more eﬃcient and less robust than normalized randomized coordinate descent method nrcd-vs.

s( ) s( )
s( )

1

0.8

0.6

0.4

0.2

0

0

5

10

15

(a) ε = 10−1

1

0.8

0.6

0.4

0.2

0

0

5

10

15

(b) ε = 10−3

1

0.8

0.6

0.4

0.2

0

0

5

10

15

(c) ε = 10−5

Fig. 3. Performance proﬁles on 54 optimization problems.

8.4. Experiments for PSTP. We considered the following function

1

1 n−1

1

f (x) = x21 +

(xi+1 − xi)2 + x2n − x1

22

2

i=1

and run PSTP with diﬀerent τ (see Figure 4). From the numerical results we see that the rate could be worse for small τ than for τ . It happens because the Assumption 5.4
does not have to be true for small τ with the parameter µ˜D ∼ π2n as we use in the experiments (recall that this parameter corresponds to the statement of Central Limit Theorem and, therefore, we need τ to be big enough). When τ is big enough the Assumption 7.1 will holds and we will obtain the improvement of the rate. We measure ff((xxk0))−−ff∗∗ on the y-axis and call it “Expected precision”.

8.5. STP vs RGF. We considered the following function

1

1 n−1

1

f (x) = x21 +

(xi+1 − xi)2 + x2n − x1

22

2

i=1

23

Expected precision Expected precision Expected precision

Expected precision Expected precision Expected precision

1.0

tau = 1

0.9

tau = 5

0.8

tau = 10

0.7

tau = 15 tau = 20

0.6

tau = 25

0.5

tau = 30

0.4

0.3
0 200Numb40e0r of ite6r0a0tions800 1000

(a) n = 25

1.0

tau = 1

0.9

tau = 5

0.8

tau = 10

0.7

tau = 15 tau = 20

0.6

tau = 25

0.5

tau = 30

0.4

0.3 0 200 400 600 800 1000
Number of iterations

(b) n = 50

1.0

tau = 1

0.9

tau = 5

0.8

tau = 10

0.7

tau = 15 tau = 20

0.6

tau = 25

0.5

tau = 30

0.4

0.3 0 200 400 600 800 1000
Number of iterations

(c) n = 75

1.0

tau = 1

0.9

tau = 5

0.8

tau = 10

0.7

tau = 15 tau = 20

0.6

tau = 25

0.5

tau = 30

0.4

0.3 0 200 400 600 800 1000
Number of iterations

(d) n = 100

1.0

tau = 1

0.9

tau = 5

0.8

tau = 10

0.7

tau = 15 tau = 20

0.6

tau = 25

0.5

tau = 30

0.4

0.3 0 200 400 600 800 1000
Number of iterations

(e) n = 125

1.0

tau = 1

0.9

tau = 5

0.8

tau = 10

0.7

tau = 15 tau = 20

0.6

tau = 25

0.5

tau = 30

0.4

0.3 0 200 400 600 800 1000
Number of iterations

(f) n = 150

Expected precision Expected precision

1.0

tau = 1

0.9

tau = 5

0.8

tau = 10

0.7

tau = 15 tau = 20

0.6

tau = 25

0.5

tau = 30

0.4

0.3 0 200 400 600 800 1000
Number of iterations

(g) n = 175

1.0

tau = 1

0.9

tau = 5

0.8

tau = 10

0.7

tau = 15 tau = 20

0.6

tau = 25

0.5

tau = 30

0.4

0.3 0 200 400 600 800 1000
Number of iterations

(h) n = 200

Fig. 4. Trajectories of PSTP for the diﬀerent n.

and run STP and RGF for diﬀerent n (see Figure 5). We measure ff((xxk0))−−ff∗∗ on the y-axis and call it “Expected precision”. One can notice that STP becomes more beneﬁcial then RGF when n is growing.
9. Conclusions. In this paper, we have proposed a very simple randomized algorithm — Stochastic Three Points (STP) method — for derivative free optimization (DFO). At each iteration, the proposed method try to decrease the objective function along a random direction sampled from a certain ﬁxed probability law. Under mild assumption on this law, we have given the properties of this method for non-convex, convex and strongly convex problems. In fact, we have derived diﬀerent practical rules for the stepsizes for which this method converges in expectation to a stationary point of the considered problem.
We have derived the worst case complexity of STP. In fact, in the non-convex case, we have shown that STP needs O(nε−2) function evaluations to ﬁnd a point at which the gradient of the objective function is below ε, in expectation. In the convex case, the number of iterations to ﬁnd a point at which the distance between the objective function and its optimal value in expectation is O(nε−1). STP is shown to converge linearly for the strongly convex problems, i.e. the complexity is O(n log(1/ε)). The complexity of STP depends linearly on the dimension of the considered problem, while this dependence is quadratic for deterministic direct search (DDS) methods. We have also proposed a parallel version for STP.
Our numerical experiments showed encouraging performance of the proposed STP
24

Expected precision Expected precision Expected precision

Expected precision Expected precision Expected precision

1.0

STP

0.8

RGF

0.6

0.4

0.2

0 200Numb40e0r of ite6r0a0tions800 1000

(a) n = 25

1.0

STP

0.9

RGF

0.8

0.7

0.6

0.5

0.4

0.3
0 200Numb40e0r of ite6r0a0tions800 1000

(b) n = 50

1.0

STP

0.9

RGF

0.8

0.7

0.6

0.5

0.4

0.3 0 200 400 600 800 1000
Number of iterations

(c) n = 75

1.0

STP

0.9

RGF

0.8

0.7

0.6

0.5

0.4

0.3 0 200 400 600 800 1000
Number of iterations

(d) n = 100

1.0

STP

0.9

RGF

0.8

0.7

0.6

0.5

0.4

0.3 0 200 400 600 800 1000
Number of iterations

(e) n = 125

1.0

STP

0.9

RGF

0.8

0.7

0.6

0.5

0.4

0.3 0 200 400 600 800 1000
Number of iterations

(f) n = 150

Expected precision Expected precision

1.0

STP

0.9

RGF

0.8

0.7

0.6

0.5

0.4

0.3 0 200 400 600 800 1000
Number of iterations

(g) n = 175

1.0 0.9 0.8 0.7 0.6 0.5 0.4 0.3
0

STP RGF

200 Numb4e00 r of iter60a0 tions 800

1000

(h) n = 200

Fig. 5. Trajectories of STP and RGF for the diﬀerent n.

algorithm. A number of issues need further investigation, in particular the best choice of probability law for choosing the random directions. Extending our results to the non smooth problems and/or the constrained problems remains an interesting topic for the future research. It would be also interesting to conﬁrm the potential of the proposed STP approach compared to the classical approaches in DFO using extensive numerical tests.
REFERENCES
[1] G. Allaire, Shape Optimization by the Homogenization Method, Springer, New York, USA, 2001.
[2] N. Baba, Convergence of a random optimization method for constrained optimization problems, Journal of Optimization Theory and Applications, 33 (1981), pp. 1–11.
[3] A. R. Conn, K. Scheinberg, and L. N. Vicente, Introduction to Derivative-Free Optimization, SIAM, Philadelphia, PA, USA, 2009.
[4] M. A. Diniz-Ehrhardt, J. M. Martinez, and M. Raydan, A derivative-free nonmonotone line-search technique for unconstrained optimization, Journal of Optimization Theory and Applications, 219 (2008), pp. 383–397.
[5] M. Dodangeh and L. N. Vicente, Worst case complexity of direct search, Mathematical Programming, 155 (2016), pp. 307–332.
[6] E. D. Dolan and J. J. More´, Benchmarking optimization software with performance proﬁles, Mathematical Programming, 91 (2002), pp. 201–213.
[7] C. Dorea, Expected number of steps of a random optimization method, Journal of Optimization Theory and Applications, 39 (1983), pp. 165–171.
25

[8] E. Gorbunov, P. Dvurechensky, and A. Gasnikov, An accelerated method for derivative-

free smooth stochastic convex optimization, arXiv preprint arXiv:1802.09022, (2018).

[9] S. Gratton, C. W. Royer, L. N. Vicente, and Z. Zhang, Direct search based on probabilistic

descent, SIAM Journal on Optimization, 25 (2015), pp. 1515–1541.

[10] J. Haslinger and R. Mckinen, Introduction to Shape Optimization: Theory, Approximation,

and Computation, SIAM, Philadelphia, PA, USA, 2003.

[11] V. G. Karmanov, Convergence estimates for iterative minimization methods, USSR Compu-

tational Mathematics and Mathematical Physics, 14 (1974), pp. 1–13.

[12]

, On convergence of a random search method in convex minimization problems, Theory

of Probability and its applications, 19 (1974), pp. 788–794.

[13] T. G. Kolda, R. M. Lewis, and V. Torczon, Optimization by direct search: New perspectives

on some classical and modern methods, SIAM Review, 45 (2003), pp. 385–482.

[14] J. Konecˇny´ and P. Richta´rik, Simple complexity analysis of simpliﬁed direct search, arXiv

preprint arXiv:1410.0390, (2014).

[15] J. Matyas, Random optimization, Automation and Remote Control, 26 (1965), pp. 246–253.

[16] B. Mohammadi and O. Pironneau, Applied Shape Optimization for Fluids, Clarendon Press,

Oxford, 2001.

[17] J. J. More´, B. S. Garbow, and K. E. Hillstrom, Testing unconstrained optimization soft-

ware, ACM Transactions on Mathematical Software, 7 (1981), pp. 17–41.

[18] Y. Nesterov and V. Spokoiny, Random gradient-free minimization of convex functions,

Foundations of Computational Mathematics, 17 (2017), pp. 527–566.

[19] B. T. Polyak, Introduction to Optimization, Optimization Software, Inc, New York, USA,

1987.

[20] M. Sarma, On the convergence of the Baba and Dorea random optimization methods, Journal

of Optimization Theory and Applications, 66 (1990), pp. 337–343.

[21] S. U. Stich, C. L. Muller, and B. Gartner, Optimization of convex functions with random

pursuit, arXiv preprint arXiv:1111.0194, (2011).

[22] L. N. Vicente, Worst case complexity of direct search, EURO Journal on Computational

Optimization, 1 (2013), pp. 143–153.

Appendix A. Proof of Lemma 3.4.

1. γD = E s 22 = An1(1)
n

s 22=1 s 22ds = An1(1)

s 2=1 ds = 1 where An(1) = 2

2π 2 n

is the air of the n − 1-unit sphere and Γ is the gamma function.

Γ( 2 )

Let ε1 = g/ g 2 and ε2, . . . , εn complete ε1 to an orthonormal basis of Rn

then

E| g, s | = An1(1)

s 2=1 | g, s |ds = g 2 An1(1) 2

|s1|ds

n i=2

s2i =1−s21

1
= g 2 An1(1) |s1|
−1

ds2:nds1

n i=2

s2i =1−s21

1

= g 2 An1(1)

|s1|An−1 1 − s21 ds1,

−1

where A

1 − s2 = 2π(n−1)/2(1−s21)n−2 is the volume of the n − 2 sphere

n−1

1

Γ((n−1)/2)

of radius 1 − s21, hence

E| g, s | = =

g 1 2π(n−1)/2
2 An(1) Γ((n−1)/2)

1

|s1|

1 − s21

n−2
ds1

−1

g 2 An1(1) Γ((n2−π1(n)/−21))(/n2−1) .

If n − 1 = 2p then

E| g, s | = g 2 22pπΓp(Γp()p2+π1p/√2π) = g 2 22p(+21p()p!!)2 ∼ 2√g π2p ,
26

since according to Stirling formula, p! ∼ ppe−p√2πp. If n − 1 = 2p + 1 then E| g, s | = g 2 2πp+21π(p2√p+π1Γ)(Γp(+p1+)1/2) = g 2 ((2pp!+)212)2!pπ ∼ √π(√2pp+1) ∼ 2√g π2p

In the both cases, E| g, s | ∼ 2√gπ2p ∼ √g2π2n .

2.

γD

=E

s

2 2

=

n1 E

x

2 2

= 1,

where

x ∼ N (0, I).

Note that s ∼ √1n N (0, I) implies g, s ∼ √1n N (0, g 22), hence

E| g, s | =

√1 g 2 2nπ

+∞

− x2

√

|x|e

2

g

2 2

dx

=

√

2

g 2.

nπ

−∞

3. γD =

n i=1

ei

22P (s = ei) = 1 and E|

g, s

| = n1

n i=1

|gi|

=

1 n

g

1.

4. γD =

n i=1

ei

22P (s

=

ei)

=

1 and E|

g, s

|

=

n i=1

|gi

|P

(s

=

ei)

=

n i=1

pi

|gi

|.

5. γD =

n i=1

di

22P (s = di) =

n i=1

pi

=1

and

E|

g, s

|=

n i=1

pi|gidi|

=

g D.

Appendix B. Proof that our approach covers some ﬁrst order methods.

• Normalized Gradient Descent (NGD) method: At iteration k, s ∼ Dk means that s = ggkk 2 with probability 1.

γDk = Es∼Dk

s

2 2

=

1,

Es∼Dk | gk, s | = gk 2.

• Signed Gradient Descent (SignGD) method: At iteration k, s ∼ Dk means that s = sign (gk) with probability 1, where the sign operation is element wise sign.

n

γDk = Es∼Dk

s

2 2

=

Es∼Dk

sign (gk)

2 2

≤

1 = n,

i=1

Es∼Dk | gk, s | = Es∼Dk | gk, sign (gk) | = gk 1.

• Normalized Randomized Coordinate Descent (NRCD) method (equivalently

this method can be called Randomized Signed Gradient Descent):

At

iteration

k,

s

∼

Dk

means

that

s

=

e gki
|gi | i

with

probability

n1 ,

where

gki

is

k

the i − th component of gk.

Es∼Dk | gk, s | = Ei∼U[1,...,n]

γDk = Es∼Dk gk, |ggkkii | ei = n1

n

s

2 2

=

1 n

i=1

n

|gki |

=

1 n

i=1

1=1 gk 1.

• Normalized Stochastic Gradient Descent (NSGD) method: At iteration k, s ∼ Dk means that s = gˆk where gˆk is the stochastic gradient satisfying E [gˆk] = ggkk 2 , and E gˆk 22 ≤ σ < ∞.

Es∼Dk | gk, s | = Es∼Dk | gk, gˆk | ≥ Es∼Dk gk, gˆk = gk 2.

27

