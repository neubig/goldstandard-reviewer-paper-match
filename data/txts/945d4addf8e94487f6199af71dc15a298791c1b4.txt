Age-of-information minimization via opportunistic sampling by an energy harvesting source

Akanksha Jaiswal, Arpan Chattopadhyay and Amokh Varma

arXiv:2201.02787v1 [eess.SY] 8 Jan 2022

Abstract‚ÄîHerein, minimization of time-averaged age-ofinformation (AoI) in an energy harvesting (EH) source equipped remote sensing setting is considered. The EH source opportunistically samples one or multiple processes over discrete time instants, and sends the status updates to a sink node over a time-varying wireless link. At any discrete time instant, the EH node decides whether to ascertain the link quality using its stored energy (called probing) and then, decides whether to sample a process and communicate the data based on the channel probe outcome. The trade-off is between the freshness of information available at the sink node and the available energy at the energy buffer of the source node. To this end, inÔ¨Ånite horizon Markov decision process (MDP) theory is used to formulate the problem of minimization of time-averaged expected AoI for a single energy harvesting source node. The following two scenarios are considered: (i) energy arrival process and channel fading process are independent and identically distributed (i.i.d.) across time, (ii) energy arrival process and channel fading process are Markovian. In the i.i.d. setting, after probing a channel, the optimal source node sampling policy is shown to be a threshold policy involving the instantaneous age of the process, the available energy in the buffer and the instantaneous channel quality as the decision variables. Also, for unknown channel state and energy harvesting characteristics, a variant of the Q-learning algorithm is proposed for the two-stage action taken by the source, that seeks to learn the optimal status update policy over time. For Markovian channel and Markovian energy arrival processes, the problem is again formulated as an MDP, and a learning algorithm is provided to handle unknown dynamics. Finally, numerical results are provided to demonstrate the policy structures and performance trade-offs.
Index Terms‚ÄîAge-of-information, remote sensing, energy harvesting, Markov decision process (MDP), Reinforcement learning.
I. INTRODUCTION
In recent years, the need for combining the physical systems with the cyber-world has attracted signiÔ¨Åcant research interest. These cyber-physical systems (CPS) are supported by ultralow power, low latency Internet of Things (IoT) networks, and encompass a large number of applications such as vehicle tracking, environment monitoring, intelligent transportation, industrial process monitoring, smart home systems etc. Such systems often require deployment of sensor nodes to monitor a physical process and send the real time status updates to a remote estimator over a wireless network. However, for such critical CPS applications, minimizing mean packet delay
Akanksha Jaiswal is with the Department of Electrical Engineering, Indian Institute of Technology, Delhi. Email: akanksha.jaiswal@ee.iitd.ac.in. Arpan Chattopadhyay is with the Department of Electrical Engineering and the Bharti School of Telecom Technology and Management, Indian Institute of Technology, Delhi. Email: arpanc@ee.iitd.ac.in. Amokh Varma is with the Department of Mathematics, Indian Institute of Technology, Delhi. Email: mt6180527@maths.iitd.ac.in.
This work was supported by the faculty seed grant and professional development allowance (PDA) of A.C. at IIT Delhi.
The preliminary version of this paper appeared in [1].

Fig. 1: Pictorial representation of a remote sensing system where an EH source samples one of ùëÅ number of processes at a time and sends the observation packet to a sink node.
without accounting for delay jitter can often be detrimental for the system performance. Also, mean delay minimization does not guarantee delivery of the observation packets to the sink node in the same order in which they were generated, thereby often resulting in unnecessarily dedicating network resources towards delivering outdated observation packets despite the availability of a freshly generated observation packet in the system. Hence, it is necessary to take into account the freshness of information of the data packets, apart from the mean packet delay.
Recently, a metric named Age of Information (AoI) has been proposed [2] as a measure of the freshness of the information update. In this setting, a sensor monitoring a system generates time stamped status update and sends it to the sink over a network. At time ùë°, if the latest monitoring information available to the sink node comes from a packet whose timestamped generation instant was ùë° , then the AoI at the sink node is computed as (ùë° ‚àí ùë° ). Thus, AoI has emerged as an alternative metric to mean delay [3].
However, timely delivery of the status updates is often limited by energy and bandwidth constraints in the network. Recent efforts towards designing EH source nodes (e.g., source nodes equipped with solar panels) have opened a new research paradigm for IoT network operations. The energy generation process in such nodes are very uncertain and typically modeled as a stochastic process. The harvested energy is stored in an energy buffer as energy packets, and used for sensing and communication as and when needed. This EH capability signiÔ¨Åcantly improves network lifetime and eliminates the need for frequent manual battery replacement, but poses a new challenge towards network operations due to uncertainty in the available energy at the source nodes at any given time.
Motivated by the above challenges, we consider the problem of minimizing the time-averaged expected AoI in a remote sensing setting, where a single EH source probes the channel state, samples one or multiple processes and sends the observation packets to the sink node over a fading channel. Energy generation process is modeled as a discrete-time

i.i.d. process, and a Ô¨Ånite energy buffer is considered. Two variants of the problem are considered: (i) single process with CSIT, (ii) multiple processes with CSIT. Channel state probing and process sampling for time-averaged expected AoI minimization is formulated as an MDP, and the threshold nature of the optimal policy is established analytically for each case. Next, we propose reinforcement learning (RL) algorithms to Ô¨Ånd the optimal policy when the channel statistics and energy harvesting characteristics are unknown. We also extend these works to the setting where the system has Markovian channel and Markovian energy harvesting process. Numerical results validate the theoretical results and intuitions.
A. Related work
Initial efforts towards optimizing AoI mostly involved the analysis of various queueing models; e.g., [2] for analysing a single source single server queueing system with FCFS service discipline, [4] for LCFS service discipline for M/M/1 queue, [5] and [6] for multi-source single sink system with M/M/1 queueing at each source, [7] for AoI performance analysis for multi-source single-sink inÔ¨Ånite-buffer queueing system where unserved packets are substituted by available newer ones, etc.
On the other hand, a number of papers have considered AoI minimization problem under EH setting: [8] for derivation of average AoI for a single source having Ô¨Ånite battery capacity, [9] for derivation of the minimal age policy for EH two hop network, [10] for average AoI expression for single source EH server, [11] for AoI minimization for wirelessly powered user, [12] for sampling, transmission scheduling and transmit power selection for a single source single sink system over inÔ¨Ånite time horizon where delay is dependent on the packet transmission energy. The authors in [13] considered two source nodes (power grid node and EH sensor node) sending different data packets to a common destination by using multiple access channel, and derived the delay and AoI of two source nodes respectively.
There have also been several other works on developing optimal scheduling policy for minimizing AoI for EH sensor networks [14]‚Äì[24]. For example, [14] has investigated optimal online policy for single sensor single sink system with a noiseless channel for inÔ¨Ånite, Ô¨Ånite and unit size battery capacity; for Ô¨Ånite battery size, it has provided energy aware status update policy. The paper [15] has considered a multisensor single sink system with inÔ¨Ånite battery size, and proposed a randomized myopic scheduling scheme. For an EH source with Ô¨Ånite battery and Poisson energy arrival process, the authors in [16] have provided age-energy tradeoff and optimal threshold policy for AoI minimization, but unlike our work, they do not consider probing/estimation of a fading channel while making a sampling decision. In [17], the optimal online status update policy to minimize the long run average AoI for EH source with updating erasures have been proposed. It has been shown that the best effort uniform updating policy is optimal when there is no feedback and besteffort uniform updating with re-transmission (BUR) policy is optimal when feedback is available to the source. The authors of [18] examined the problem of minimizing AoI under a constraint on the count of status updates. The authors of

[19] addressed AoI minimization problem for cognitive radio communication system with EH capability; they formulated optimal sensing and updating for perfect and imperfect sensing as a partially observable Markov decision process (POMDP). Information source diversity, i.e., multiple sources tracking the same process but with dissimilar energy cost, and sending status updates to the EH monitoring node with Ô¨Ånite battery size, has been considered in [20] with an MDP formulation, but no structure was provided for the optimal policy. In [21], reinforcement learning has been used to minimize AoI for a single EH sensor with HARQ protocol, but no clear intuition on the policy structure was provided. In [23], the authors have formulated optimal transmission schemes for an EH sensor which sends update packets to the BS by considering different transmission modes (controlled by power and error probability) and battery recovery setting as an MDP. The authors of [24] have developed a threshold policy for minimizing AoI for a single sensor single sink system with erasure channel and no channel feedback. For a system with Poisson energy arrival, unit battery size and error-free channel, it has shown that a threshold policy achieves average age lower than that of zero-wait policy; based on this, lower bound on average age for general battery size and erasure channel has been derived. The papers [25] and [26] have proposed threshold based sampling policies for AoI minimization in a multi-source system, and also use deep reinforcement learning to learn the optimal policy. However, these paper do not consider channel probing as done in our paper. A very similar claim applies to [27] which also proposes threshold-based scheduling policy.
B. Our contributions and organization
1) We formulate the problem of minimizing the timeaveraged expected AoI in an EH remote sensing system with a single source monitoring one or multiple processes, as an MDP with two stage action model, which is different from standard MDP in the literature. Under the assumptions of i.i.d. time-varying channel with CSIT, channel state probing capability at the source, and Ô¨Ånite battery size, we derive the optimal policy structures which turn out to be simple threshold policies. The source node, depending on the current age of a process, decides whether to probe the channel or not. Afterwards, based on the channel probe outcome, the source node decides whether to sample the process and send an observation packet, or to remain idle. Thus, the MDP involves taking action in two stages at each time instant.
2) We prove convergence of an analogue of value iteration for this two-stage MDP.
3) The threshold for multiple processes turns out to be a function of the relative age of the processes.
4) We prove certain interesting properties of various cost functions and some properties of the thresholds as a function of energy and age of the process, either analytically or numerically.
5) We also formulate the AoI minimization problem for a Markovian system as an MDP; the channel dynamics

and energy harvesting characteristics are modeled as Markov chains in this case. We provide optimality equations for the MDP. Certain conjectures regarding the threshold structure of the optimal policy for this MDP are validated numerically. 6) For unknown channel statistics and energy harvesting characteristics, we propose reinforcement learning algorithms that yield the optimal policies for the formulated MDPs. However, unlike standard Q-learning algorithm involving a single action at each time instant, we propose a variant of the Q-learning algorithm, that involves taking the action in two stages; Ô¨Årst the source decides whether to probe the channel quality, and if the channel quality turns out to be good, then it further decides whether to sample and communicate based on its available energy and age in sampling. 7) We numerically compare the AoI performance of our algorithm with the case where channel probing is not allowed. Surprisingly, it turns out that probing is sometimes better and sometimes worse than no probing; we provide intuitive justiÔ¨Åcation for this, and also assert that channel probing leads to smaller AoI in many practical scenarios.
The rest of the paper is organized as follows. The system model has been explained in Section II. AoI minimization for both single and multiple process case with i.i.d. system model is addressed in Section III, whereas AoI minimization for Markovian system model is considered in Section IV. Further, reinforcement learning for i.i.d. and Markovian system model is proposed in Section V. Lastly, numerical results are provided in Section VI, followed by the conclusions in Section VII. All proofs are provided in the appendices.
II. SYSTEM MODEL
We consider an EH source capable of sensing one out of ùëÅ different processes at a time, and reporting the observation packet to a sink node over a fading channel; see Figure 1. Time is discretized with the discrete time index ùë° ‚àà {0, 1, 2, 3, ¬∑ ¬∑ ¬∑}. At each time, the source node can decide whether to estimate the quality of the channel from the source to the sink, or not. If the source node decides to probe the channel state, it can further decide whether to sample a process and communicate the data packet to the sink or not, depending on the instantaneous channel quality. The source has a Ô¨Ånite energy buffer of size ùêµ units, where ùê∏ ùëù units of buffer energy is used to probe the channel once and ùê∏ùë† units of buffer energy is used in sensing and communication of one packet.
A. Channel model
We consider a fading channel between the source and the sink, where a particular fading state is characterized by the probability of success of a transmitted packet. We denote by ùëù(ùë°) ‚àà {ùëù1, ùëù2, ¬∑ ¬∑ ¬∑ , ùëùùëö} the probability of packet transmission success from the source to the sink node at time ùë°, and by ùê∂ (ùë°) ‚àà {ùê∂1, ùê∂2, ¬∑ ¬∑ ¬∑ , ùê∂ùëö} the corresponding channel state. The packet success probability corresponding to channel state ùê∂ ùëó is given by ùëù(ùê∂ ùëó ) = ùëù ùëó . Let us also denote by ùëü (ùë°) ‚àà {0, 1}

the indicator that the packet transmission from the source to the sink at time ùë° is successful. Obviously, P(ùëü (ùë°) = 1|ùê∂ (ùë°) = ùê∂ ùëó ) = ùëù ùëó for all ùëó ‚àà {1, 2, ¬∑ ¬∑ ¬∑ , ùëö}. It is assumed that the channel state ùê∂ (ùë°) is learnt perfectly via a channel probe. I.I.D. channel model: In this case, we assume that {ùëù(ùë°)}ùë° ‚â•0 is i.i.d. across ùë°, with P( ùëù(ùë°) = ùëù ùëó ) = ùëû ùëó for all ùëó ‚àà {1, 2, ¬∑ ¬∑ ¬∑ , ùëö}. Markovian channel model: Here the fading channel is modelled as Ô¨Ånite state Markov chain [28], where the onestep state transition probability from state ùê∂ùëñ to state ùê∂ ùëó is denoted by ùëûùëñ, ùëó and the ùë°-step transition probability is given by ùëûùëñ(,ùë°ùëó) = ùëÉ(ùê∂ (ùë°) = ùê∂ ùëó |ùê∂ (0) = ùê∂ùëñ) for all ùëñ, ùëó ‚àà {1, 2, ¬∑ ¬∑ ¬∑ , ùëö}.
B. Energy harvesting process
Let ùê¥(ùë°) denote the number of energy packet arrivals to the energy buffer at time ùë°, and ùê∏ (ùë°) denote the energy available to the source at time ùë°, for all ùë° ‚â• 0. We consider the following two models for energy harvesting. I.I.D. model: The energy packet generation process { ùê¥(ùë°)}ùë° ‚â•0 in the energy buffer is assumed to be an i.i.d. process with known mean ùúÜ > 0. Markov model: Here the EH state of the source is modelled as a two state Markov chain with states {ùêª1, ùêª2}, where ùêª1 is called the harvesting state and ùêª2 is called the non harvesting state. When the source node is in harvesting state, the energy packet generation process is considered to be an i.i.d. process with known mean ùúÜ > 0. When the source node is in nonharvesting state, then the energy packet generation rate is zero. The transition probability from state ùêª1 to state ùêª2 is ‚Ñé1,2 and from state ùêª2 to state ùêª1 is ‚Ñé2,1. Markov process for energy arrival has previously been used in [29], [30].
We assume that, in case the energy buffer at the source node is full, the newly generated energy packets will not be accommodated unless ùê∏ ùëù unit of energy is spent in probing.
C. Decision process and policy
At time ùë°, let ùëè(ùë°) ‚àà {0, 1} denote the indicator of deciding to probe the channel, and ùëé(ùë°) ‚àà {0, 1, ¬∑ ¬∑ ¬∑ , ùëÅ } denote the identity of the process being sampled, with ùëé(ùë°) = 0 meaning that no process is sampled, and ùëè(ùë°) = 0 meaning that the channel is not probed. Also, ùëè(ùë°) = 0 implies ùëé(ùë°) = 0. The set of possible actions or decisions is denoted by A = {{0, 0} ‚à™ {1 √ó {0, 1, 2, ¬∑ ¬∑ ¬∑ , ùëÅ }}}, where a generic action at time ùë° is denoted by (ùëè(ùë°), ùëé(ùë°)).
Let us denote by ùúèùëò (ùë°) sup{0 ‚â§ ùúè < ùë° : ùëé(ùúè) = ùëò, ùëü (ùúè) = 1} the last time instant before time ùë°, when process ùëò was sampled and the observation packet was successfully delivered to the sink. The age of information (AoI) for the ùëò-th process at time ùë° is given by ùëáùëò (ùë°) = (ùë° ‚àíùúèùëò (ùë°)). However, if ùëé(ùë°) = ùëò and ùëü (ùë°) = 1, then ùëáùëò (ùë°) = 0 since the current observation of the ùëò-th process is available to the sink node. A generic Markov scheduling policy is a collection of mappings {ùúáùë° }ùë° ‚â•0. where ùúáùë° maps (ùê∏ (ùë°), ùê∂ (ùë°), {ùëáùëò (ùë°)}1‚â§ùëò ‚â§ùëÅ ) to the action space A. If ùúáùë° = ùúá for all ùë° ‚â• 0, the policy is called stationary, else non-stationary.

We seek to Ô¨Ånd a stationary scheduling policy ùúá that minimizes the expected sum AoI which is averaged over time:

1

ùëá
‚àëÔ∏Å

ùëÅ
‚àëÔ∏Å

min

Eùúá (ùëáùëò (ùë°))

(1)

ùúáùëá

ùë°=0 ùëò=1

III. I.I.D. SYSTEM
In this section, we derive the optimal channel probing, source activation and data transmission policy for a single EH source sampling a single process (ùëÅ = 1) or multiple processes, when the channel and energy harvesting processes follow the i.i.d. model as described in Section II.

A. Single process (ùëÅ = 1)
Here, we formulate (1) as a long-run average cost MDP with state space S {0, 1, ¬∑ ¬∑ ¬∑ , ùêµ} √ó Z+ and an intermediate state space V = {0, 1, . . . .., ùêµ} √ó Z+ √ó {ùê∂1, ùê∂2, ¬∑ ¬∑ ¬∑ , ùê∂ùëö}. A generic state ùë† = (ùê∏, ùëá) means that the energy buffer has ùê∏ energy packets, and the source was last activated ùëá slots ago. A generic intermediate state ùë£ = (ùê∏, ùëá, ùê∂) additionally means that the current channel state obtained via probing is ùê∂. The action space is A = {{0, 0} ‚à™ {1 √ó {0, 1}}} with ùëé(ùë°), ùëè(ùë°) ‚àà {0, 1}. At each time, if the source node decides not to probe the channel (ùëè(ùë°) = 0) and consequently does not sample (ùëé(ùë°) = 0), the expected single-stage AoI cost is ùëá. However, if ùëè(ùë°) = 1, then the expected single-stage AoI cost is ùëá for ùëé(ùë°) = 0 and ùëá (1 ‚àí ùëù(ùê∂)) for ùëé(ùë°) = 1, where the expectation of the single stage cost is taken over packet success probability ùëù(ùê∂). We Ô¨Årst formulate the average-cost MDP problem as an ùõºdiscounted cost MDP problem with ùõº ‚àà (0, 1), and derive the optimal policy, from which the solution of the average cost minimization problem can be obtained by taking ùõº ‚Üí 1 [31, Section 4.1].
1) Optimality equation: Let ùêΩ‚àó (ùê∏, ùëá) be the optimal value function for state (ùê∏, ùëá) in the discounted cost problem, and let ùëä‚àó (ùê∏, ùëá, ùê∂) be the cost-to-go from an intermediate state (ùê∏, ùëá, ùê∂). The Bellman equations are given by:

ùêΩ ‚àó (ùê∏ ‚â• ùê∏ùëù + ùê∏ùë† , ùëá ) = ùëöùëñùëõ ùëá + ùõºEùê¥ ùêΩ ‚àó (ùëöùëñùëõ{ùê∏ + ùê¥, ùêµ }, ùëá + 1) ,

ùëâ ‚àó(ùê∏,ùëá )

ùëö
ùëâ ‚àó (ùê∏ , ùëá ) = ‚àëÔ∏Å ùëûùëó ùëä ‚àó (ùê∏ , ùëá , ùê∂ ùëó )

ùëó=1
ùëä ‚àó (ùê∏ , ùëá , ùê∂) = ùëöùëñùëõ{ùëá + ùõºEùê¥ ùêΩ ‚àó (ùëöùëñùëõ{ùê∏ ‚àí ùê∏ùëù + ùê¥, ùêµ }, ùëá + 1) , ùëá (1 ‚àí ùëù (ùê∂)) + ùõº ùëù (ùê∂)Eùê¥ ùêΩ ‚àó (ùëöùëñùëõ{ùê∏ ‚àí ùê∏ùëù ‚àí ùê∏ùë† + ùê¥, ùêµ }, 1) + ùõº(1 ‚àí ùëù (ùê∂))Eùê¥ ùêΩ ‚àó (ùëöùëñùëõ{ùê∏ ‚àí ùê∏ùëù ‚àí ùê∏ùë† +

ùê¥, ùêµ }, ùëá + 1) }

ùêΩ ‚àó (ùê∏ < ùê∏ùëù + ùê∏ùë† , ùëá ) = ùëá + ùõºEùê¥ ùêΩ ‚àó (ùëöùëñùëõ{ùê∏ + ùê¥, ùêµ }, ùëá + 1)

(2)

The Ô¨Årst expression in the minimization in the R.H.S. of the Ô¨Årst equation in (2) is the cost of not probing channel state (ùëè(ùë°) = 0), which includes single-stage AoI cost ùëá and an ùõº discounted future cost for a random next state (ùëöùëñùëõ{ùê∏ + ùê¥, ùêµ}, ùëá + 1), averaged over the distribution of the number of energy packet generation ùê¥. The quantity ùëâ‚àó (ùê∏, ùëá) is the expected cost of probing the channel state, which explains the second equation in (2). At an intermediate state (ùê∏, ùëá, ùê∂), if ùëé(ùë°) = 0, a single stage AoI cost ùëá is incurred and the next

state becomes (ùëöùëñùëõ{ùê∏ ‚àí ùê∏ ùëù + ùê¥, ùêµ}, ùëá + 1); if ùëé(ùë°) = 1, the expected AoI cost is ùëá (1 ‚àí ùëù(ùê∂)) (expectation taken over the packet success probability ùëù(ùê∂)), and the next random state becomes (ùëöùëñùëõ{ùê∏ ‚àí ùê∏ ùëù ‚àí ùê∏ùë† + ùê¥, ùêµ}, 1) and (ùëöùëñùëõ{ùê∏ ‚àí ùê∏ ùëù ‚àí ùê∏ùë† + ùê¥, ùêµ}, ùëá + 1) if ùëü (ùë°) = 1 and ùëü (ùë°) = 0, respectively. The last equation in (2) follows similarly since ùëè(ùë°) = 0, ùëé(ùë°) = 0 is the only possible action when ùê∏ < ùê∏ ùëù + ùê∏ùë†.
Substituting the value of ùëâ‚àó (ùê∏, ùëá) in the Ô¨Årst equation of
(2), we obtain the following Bellman equations:

ùêΩ‚àó(ùê∏ ‚â• ùê∏ùëù + ùê∏ùë†,ùëá)
= ùëöùëñùëõ ùëá + ùõºEùê¥ùêΩ‚àó (ùëöùëñùëõ{ùê∏ + ùê¥, ùêµ}, ùëá + 1), Eùê∂ ùëöùëñùëõ{ùëá + ùõºEùê¥ùêΩ‚àó (ùëöùëñùëõ{ùê∏ ‚àí ùê∏ ùëù + ùê¥, ùêµ}, ùëá + 1), ùëá (1 ‚àí ùëù(ùê∂)) + ùõºùëù(ùê∂)Eùê¥ùêΩ‚àó (ùëöùëñùëõ{ùê∏ ‚àí ùê∏ ùëù ‚àí ùê∏ùë† + ùê¥, ùêµ}, 1) + ùõº(1 ‚àí ùëù(ùê∂))Eùê¥ùêΩ‚àó (ùëöùëñùëõ{ùê∏ ‚àí ùê∏ ùëù ‚àí ùê∏ùë† + ùê¥, ùêµ}, ùëá + 1)}

ùêΩ‚àó(ùê∏ < ùê∏ùëù + ùê∏ùë†,ùëá)

= ùëá + ùõºEùê¥ùêΩ‚àó (ùëöùëñùëõ{ùê∏ + ùê¥, ùêµ}, ùëá + 1)

(3)

2) Policy structure: We Ô¨Årst provide the convergence proof of value iteration since we have a two-stage decision process as opposed to traditional MDP where a single action is taken.
Proposition 1. The value function ùêΩ (ùë°) (ùë†) converges to ùêΩ‚àó (ùë†) as ùë° ‚Üí ‚àû.

Proof. See Appendix A.

Next we provide some properties of the value function, which will help in proving the structure of the optimal policy.
Lemma 1. For ùëÅ = 1, ùêΩ‚àó (ùê∏, ùëá) is increasing in ùëá and ùëä‚àó (ùê∏, ùëá, ùê∂) is decreasing in ùëù(ùê∂).

Proof. See Appendix B.

Conjecture 1. For ùëÅ = 1, the optimal probing policy for the ùõº-discounted AoI cost minimization problem is a threshold policy on ùëá. For any ùê∏ ‚â• ùê∏ ùëù + ùê∏ùë†, the optimal action is to probe the channel state if and only if ùëá ‚â• ùëáùë°‚Ñé (ùê∏) for a threshold function ùëáùë°‚Ñé (ùê∏) of ùê∏.

This conjecture and many other conjectures later have been validated numerically in Section VI.

Theorem 1. For ùëÅ = 1, at any time, if the source decides to probe the channel, then the optimal sampling policy is a threshold policy on ùëù(ùê∂). For any ùê∏ ‚â• ùê∏ ùëù + ùê∏ùë† and probed channel state, the optimal action is to sample the source node if and only if ùëù(ùê∂) ‚â• ùëùùë°‚Ñé (ùê∏, ùëá) for a threshold function ùëùùë°‚Ñé (ùê∏, ùëá) of ùê∏ and ùëá.

Proof. See Appendix C.

The policy structure supports the following two intuitions. Firstly, given ùê∏ and ùëá, the source decides to probe the channel state if AoI is greater than some threshold value. Secondly, given ùê∏ and ùëá and probed channel state, if the channel quality is better than a threshold, then the optimal action is to sample the process and communicate the observation to the sink node. We will later numerically observe in Section VI

some intuitive properties of ùëáùë°‚Ñé (ùê∏) as a function of ùê∏ and ùúÜ, and ùëùùë°‚Ñé (ùê∏, ùëá) as a function of ùê∏, ùëá and ùúÜ.
3) No probing case: If we consider the case where the source samples the process directly without channel probing, then the Bellman equations for this case is given by:

ùêΩ ‚àó (ùê∏ ‚â• ùê∏ùë† , ùëá ) = ùëöùëñùëõ ùëá + ùõºEùê¥ ùêΩ ‚àó (ùëöùëñùëõ{ùê∏ + ùê¥, ùêµ }, ùëá + 1) ,

Eùê∂ ùëá (1 ‚àí ùëù (ùê∂)) + ùõº ùëù (ùê∂)Eùê¥ ùêΩ ‚àó (ùëöùëñùëõ{ùê∏ ‚àí ùê∏ùë† + ùê¥, ùêµ },

1) + ùõº(1 ‚àí ùëù (ùê∂))Eùê¥ ùêΩ ‚àó (ùëöùëñùëõ{ùê∏ ‚àí ùê∏ùë† + ùê¥, ùêµ }, ùëá + 1)

ùêΩ ‚àó (ùê∏ < ùê∏ùë† , ùëá ) = ùëá + ùõºEùê¥ ùêΩ ‚àó (ùëöùëñùëõ{ùê∏ + ùê¥, ùêµ }, ùëá + 1)

(4)

Here also we numerically observe an optimal threshold policy on ùëá; see Section VI.

B. Multiple processes(ùëÅ > 1)

Here we formulate the ùõº-discounted cost version of (1)

as an MDP with a generic state ùë† = (ùê∏, ùëá1, ùëá2, ¬∑ ¬∑ ¬∑ , ùëáùëÅ )

which means that the energy buffer has ùê∏ energy packets,

and the k-th process was last activated ùëáùëò slots ago. Also, a

generic intermediate state is ùë£ = (ùê∏, ùëá1, ùëá2, ¬∑ ¬∑ ¬∑ , ùëáùëÅ , ùê∂) which

additionally means that the current channel state ùê∂ is learnt

by probing, has packet success probability ùëù(ùê∂). The action

space A = {{0, 0} ‚à™ {1 √ó {0, 1, 2, ¬∑ ¬∑ ¬∑ , ùëÅ }}} with ùëè(ùë°) ‚àà {0, 1}

and ùëé(ùë°) ‚àà {0, 1, ¬∑ ¬∑ ¬∑ , ùëÅ }. At each time, if the source node

decides not to probe the channel state then it will not sample

any process, thus ùëè(ùë°) = 0, ùëé(ùë°) = 0 and the expected single

stage AoI cost is

ùëÅ ùëñ=1

ùëáùëñ .

However,

if

the

source

node

decides

to probe the channel state (ùëè(ùë°) = 1), the expected single stage

AoI cost is

ùëÅ
ùëñ=1 ùëáùëñ

for

ùëé(ùë°)

=

0

and

ùëñ‚â†ùëò ùëáùëñ +ùëáùëò (1 ‚àí ùëù(ùê∂)) for

ùëé(ùë°) = ùëò where the expectation is taken over packet success

probability ùëù(ùê∂).

1) Optimality equation: In this case, the Bellman equations

are given by:

ùêΩ ‚àó (ùê∏ ‚â• ùê∏ùëù + ùê∏ùë† , ùëá1, ùëá2, ¬∑ ¬∑ ¬∑ , ùëáùëÅ )

ùëÅ

=

ùëöùëñùëõ

‚àëÔ∏Å ùëáùëñ

+

ùõºEùê¥ ùêΩ ‚àó (ùëöùëñùëõ{ùê∏

+

ùê¥,

ùêµ }, ùëá1

+

1, ùëá2

+

1,

¬∑

¬∑

¬∑ , ùëáùëÅ

+

1) ,

ùëñ=1

ùëâ ‚àó (ùê∏ , ùëá1, ùëá2, ¬∑ ¬∑ ¬∑ , ùëáùëÅ )

ùëâ ‚àó (ùê∏ , ùëá1, ùëá2, ¬∑ ¬∑ ¬∑ , ùëáùëÅ )
ùëö
= ‚àëÔ∏Å ùëûùëó ùëä ‚àó (ùê∏ , ùëá1, ùëá2, ¬∑ ¬∑ ¬∑ , ùëáùëÅ , ùê∂ ùëó )
ùëó=1

ùëä ‚àó (ùê∏ , ùëá1, ùëá2, ¬∑ ¬∑ ¬∑ , ùëáùëÅ , ùê∂)

ùëÅ

=

‚àëÔ∏Å ùëöùëñùëõ{ ùëáùëñ

+

ùõºEùê¥ ùêΩ ‚àó (ùëöùëñùëõ{ùê∏

‚àí

ùê∏ùëù

+

ùê¥,

ùêµ }, ùëá1

+

1, ùëá2

+

1,

¬∑

¬∑

¬∑ , ùëáùëÅ

+

1) ,

ùëñ=1

min

‚àëÔ∏Å ùëá + ùëá (1 ‚àí ùëù(ùê∂)) + ùõºùëù(ùê∂)E

ùêΩ ‚àó (ùëöùëñùëõ{ùê∏ ‚àí ùê∏

‚àíùê∏

+ ùê¥,

ùëñùëò

ùê¥

ùëù

ùë†

1‚â§ùëò‚â§ùëÅ ùëñ‚â†ùëò

ùêµ }, ùëá1 + 1, ùëá2 + 1, ¬∑ ¬∑ ¬∑ , ùëá = 1, ùëáùëò+1 + 1, ¬∑ ¬∑ ¬∑ , ùëáùëÅ + 1) + ùõº(1 ‚àí ùëù (ùê∂))
ùëò

Eùê¥ ùêΩ ‚àó (ùëöùëñùëõ{ùê∏ ‚àí ùê∏ùëù ‚àí ùê∏ùë† + ùê¥, ùêµ }, ùëá1 + 1, ùëá2 + 1, ¬∑ ¬∑ ¬∑ , ùëáùëÅ + 1) }

ùêΩ ‚àó (ùê∏ < ùê∏ùëù + ùê∏ùë† , ùëá1, ùëá2, ¬∑ ¬∑ ¬∑ , ùëáùëÅ )

ùëÅ

=

‚àëÔ∏Å ùëáùëñ

+

ùõºEùê¥ ùêΩ ‚àó (ùëöùëñùëõ{ùê∏

+

ùê¥,

ùêµ }, ùëá1

+

1, ùëá2

+

1,

¬∑

¬∑

¬∑ , ùëáùëÅ

+

1)

(5)

ùëñ=1

The Ô¨Årst expression in the minimization in the R.H.S. of the

Ô¨Årst equation in (5) is the cost of not probing channel state

(ùëè(ùë°) = 0), which includes single-stage AoI cost

ùëÅ
ùëñ=1 ùëáùëñ

and

an ùõº discounted future cost with a random next state (min{ùê∏ +

ùê¥, ùêµ}, ùëá1 + 1, ùëá2 + 1, ¬∑ ¬∑ ¬∑ , ùëáùëÅ + 1), averaged over the distribution

of the number of energy packet generation ùê¥. The quantity ùëâ‚àó (ùê∏, ùëá1, ùëá2, ¬∑ ¬∑ ¬∑ , ùëáùëÅ ) is the optimal expected cost of probing

the channel state, which explains the second equation in (5).

At an intermediate state (ùê∏, ùëá1, ùëá2, ¬∑ ¬∑ ¬∑ , ùëáùëÅ , ùê∂), if ùëé(ùë°) = 0,

a single stage AoI cost

ùëÅ
ùëñ=1 ùëáùëñ

is

incurred

and

the

next

state

becomes (min{ùê∏‚àíùê∏ ùëù+ùê¥, ùêµ}, ùëá1+1, ùëá2+1, ¬∑ ¬∑ ¬∑ , ùëáùëÅ +1); if ùëé(ùë°) =

ùëò, the expected AoI cost is ùëñ‚â†ùëò ùëáùëñ +ùëáùëò (1‚àí ùëù(ùê∂)) (expectation

taken over the packet success probability ùëù(ùê∂)), and the next

random state becomes (min{ùê∏ ‚àí ùê∏ ùëù ‚àí ùê∏ùë† + ùê¥, ùêµ}, ùëá1 + 1, ùëá2 +

1, ¬∑ ¬∑ ¬∑ , ùëáùëò = 1, ùëáùëò+1 + 1, ¬∑ ¬∑ ¬∑ , ùëáùëÅ + 1) and (min{ùê∏ ‚àí ùê∏ ùëù ‚àí ùê∏ùë† + ùê¥, ùêµ}, ùëá1 + 1, ùëá2 + 1, ¬∑ ¬∑ ¬∑ , ùëáùëÅ + 1) if ùëü (ùë°) = 1 and ùëü (ùë°) = 0,

respectively. The last equation in (5) follows similarly since

ùëè(ùë°) = 0, ùëé(ùë°) = 0 is the only possible action when ùê∏ <

ùê∏ùëù + ùê∏ùë†. Substituting the value of ùëâ‚àó (ùê∏, ùëá1, ùëá2, ¬∑ ¬∑ ¬∑ , ùëáùëÅ ) in the Ô¨Årst

equation of (5), we obtain the Bellman equations (6).

2) Policy structure:

Lemma 2. For ùëÅ > 1, the value function ùêΩ‚àó (ùê∏, ùëá1, ùëá2, ¬∑ ¬∑ ¬∑ , ùëáùëÅ ) is increasing in each of ùëá1, ùëá2, ¬∑ ¬∑ ¬∑ , ùëáùëÅ and ùëä‚àó (ùê∏, ùëá1, ùëá2, ¬∑ ¬∑ ¬∑ , ùëáùëÅ , ùê∂) is decreasing in ùëù(ùê∂).
Proof. See Appendix D.

Let us deÔ¨Åne T = [ùëá1, ùëá2, ¬∑ ¬∑ ¬∑ , ùëáùëÅ ] and T‚àíùëò = [ùëá1, ùëá2, ¬∑ ¬∑ ¬∑ , ùëáùëò‚àí1, ùëáùëò+1, ¬∑ ¬∑ ¬∑ , ùëáùëÅ ].
Conjecture 2. For ùëÅ > 1, the optimal probing policy for the ùõº-discounted AoI cost minimization problem is a threshold policy on max1‚â§ùëò ‚â§ùëÅ ùëáùëò ùëáùëò‚àó . For any ùê∏ ‚â• ùê∏ ùëù + ùê∏ùë†, the optimal action is to probe the channel state if and only if max1‚â§ùëò ‚â§ùëÅ ùëáùëò ‚â• ùëáùë°‚Ñé (ùê∏, T‚àíùëò‚àó ) for a threshold function ùëáùë°‚Ñé (ùê∏, T‚àíùëò‚àó ) of ùê∏ and T‚àíùëò‚àó .

We next show that sampling the process with largest AoI is optimal.

Theorem 2. For ùëÅ > 1, after probing the channel state, the optimal source activation policy for the ùõº-discounted cost problem is a threshold policy on p(C). For any ùê∏ ‚â• ùê∏ ùëù + ùê∏ùë† and probed channel state, the optimal action is to sample the process arg max1‚â§ùëò ‚â§ùëÅ ùëáùëò if and only if ùëù(ùê∂) ‚â• ùëùùë°‚Ñé (ùê∏, T ) for a threshold function ùëùùë°‚Ñé (ùê∏, T ) of (ùê∏, T ).

Proof. See Appendix E.

We will later numerically demonstrate some intuitive properties of ùëáùë°‚Ñé (ùê∏, T‚àíùëò ) as a function of ùê∏, T‚àíùëò and ùúÜ and ùëùùë°‚Ñé (ùê∏, ùëá1, ùëá2, ¬∑ ¬∑ ¬∑ , ùëáùëÅ ) as a function of ùê∏, (ùëá1, ùëá2, ¬∑ ¬∑ ¬∑ , ùëáùëÅ ) and ùúÜ in Section VI.

IV. MARKOVIAN SYSTEM MODEL: SINGLE PROCESS
Here, we derive the optimal channel probing, source activation and data transmission policy for an EH source sampling a single process with Markovian channel and Markovian energy arrival process. We formulate the ùõºdiscounted cost version of (1) as an MDP with state space

ùëÅ

ùêΩ‚àó (ùê∏ ‚â• ùê∏ ùëù + ùê∏ùë†, ùëá1, ùëá2, ¬∑ ¬∑ ¬∑ , ùëáùëÅ )

=

ùëöùëñùëõ

‚àëÔ∏Å ùëáùëñ

+

ùõºE ùê¥ùêΩ ‚àó (ùëöùëñùëõ{ùê∏

+

ùê¥,

ùêµ}, ùëá1

+

1, ùëá2

+

1, ¬∑ ¬∑ ¬∑ , ùëáùëÅ

+

1),

ùëñ=1

ùëÅ

Eùê∂

‚àëÔ∏Å ùëöùëñùëõ{ ùëáùëñ

+

ùõºE ùê¥ùêΩ ‚àó (ùëöùëñùëõ{ùê∏

‚àí

ùê∏ùëù

+

ùê¥,

ùêµ}, ùëá1

+

1, ùëá2

+

1, ¬∑ ¬∑ ¬∑ , ùëáùëÅ

+

1),

ùëñ=1

min ‚àëÔ∏Å ùëáùëñ + ùëáùëò (1 ‚àí ùëù(ùê∂)) + ùõº ùëù(ùê∂)Eùê¥ùêΩ‚àó (ùëöùëñùëõ{ùê∏ ‚àí ùê∏ ùëù ‚àí ùê∏ùë† + ùê¥, ùêµ}, ùëá1 + 1,
1‚â§ùëò ‚â§ùëÅ ùëñ‚â†ùëò
ùëá2 + 1, ¬∑ ¬∑ ¬∑ , ùëáùëò = 1, ùëáùëò+1 + 1, ¬∑ ¬∑ ¬∑ , ùëáùëÅ + 1) + ùõº(1 ‚àí ùëù(ùê∂))Eùê¥ùêΩ‚àó (ùëöùëñùëõ{ùê∏ ‚àí ùê∏ ùëù ‚àí ùê∏ùë† + ùê¥, ùêµ},

ùëá1 + 1, ùëá2 + 1, ¬∑ ¬∑ ¬∑ , ùëáùëÅ + 1) }

ùëÅ

ùêΩ‚àó (ùê∏ < ùê∏ ùëù + ùê∏ùë†, ùëá1, ùëá2, ¬∑ ¬∑ ¬∑ , ùëáùëÅ )

=

‚àëÔ∏Å ùëáùëñ

+

ùõºE ùê¥ùêΩ ‚àó (ùëöùëñùëõ{ùê∏

+

ùê¥,

ùêµ}, ùëá1

+

1, ùëá2

+

1, ¬∑ ¬∑ ¬∑ , ùëáùëÅ

+

1)

(6)

ùëñ=1

S {0, 1, ¬∑ ¬∑ ¬∑ , ùêµ} √ó Z+ √ó Z+ √ó {ùê∂1, ùê∂2, ¬∑ ¬∑ ¬∑ , ùê∂ùëö} √ó {ùêª1, ùêª2} where a generic state ùë† = (ùê∏, ùëá, ùúè, ùê∂ùëùùëüùëíùë£ , ùêª1) means that the energy buffer has ùê∏ energy packets, the source was last activated ùëá slots ago, the channel state was last probed ùúè slots ago where ùúè ‚â§ ùëá, the previous channel state obtained via probing is ùê∂ùëùùëüùëíùë£ ‚àà {ùê∂1, ùê∂2, ¬∑ ¬∑ ¬∑ , ùê∂ùëö}, and the EH source is in harvesting state ùêª1. A generic state ùë† = (ùê∏, ùëá, ùúè, ùê∂ùëùùëüùëíùë£ , ùêª2) similarly means that the EH source is in non-harvesting state ùêª2. Also, V denotes intermediate state space where a generic state ùë£ = (ùê∏, ùëá, ùê∂, ùêª1) or ùë£ = (ùê∏, ùëá, ùê∂, ùêª2) is used for current channel state ùê∂ (obtained after probing). The action space A = {{0, 0} ‚à™ {1 √ó {0, 1}}} with ùëé(ùë°), ùëè(ùë°) ‚àà {0, 1}. At each time, if ùëè(ùë°) = 0, ùëé(ùë°) = 0, then the expected single-stage AoI cost is ùëá. However, if ùëè(ùë°) = 1, the expected single-stage AoI cost is ùëá and ùëá (1 ‚àí ùëù(ùê∂)) for ùëé(ùë°) = 0 and ùëé(ùë°) = 1, respectively.
A. Optimality equation
In this case, for state (ùê∏, ùëá, ùúè, ùê∂ùëùùëüùëíùë£ , ùêª1), the Bellman equations are given by:
ùêΩ ‚àó (ùê∏ ‚â• ùê∏ùëù + ùê∏ùë† , ùëá , ùúè, ùê∂ùëùùëüùëíùë£ , ùêª1)
= ùëöùëñùëõ ùëá + ùõºEùê¥|ùêª1,ùêªùëõùëíùë•ùë° |ùêª1 ùêΩ ‚àó (ùëöùëñùëõ {ùê∏ + ùê¥, ùêµ }, ùëá + 1, ùúè + 1, ùê∂ùëùùëüùëíùë£ ,
ùêªùëõùëíùë•ùë° ) , ùëâ ‚àó (ùê∏ , ùëá , ùúè, ùê∂ùëùùëüùëíùë£ , ùêª1)
ùëâ ‚àó (ùê∏ , ùëá , ùúè, ùê∂ùëùùëüùëíùë£ , ùêª1) = Eùê∂‚àºùëû ( ùúè) (¬∑|ùê∂ùëùùëüùëíùë£ ) ùëä ‚àó (ùê∏ , ùëá , ùê∂ , ùêª1)
ùëä ‚àó (ùê∏ , ùëá , ùê∂, ùêª1) = ùëöùëñùëõ {ùëá + ùõºEùê¥|ùêª1,ùêªùëõùëíùë•ùë° |ùêª1 ùêΩ ‚àó (ùëöùëñùëõ {ùê∏ ‚àí ùê∏ùëù + ùê¥, ùêµ }, ùëá + 1, 1, ùê∂,
ùêªùëõùëíùë•ùë° ) , ùëá (1 ‚àí ùëù (ùê∂)) + ùõº ùëù (ùê∂)Eùê¥|ùêª1,ùêªùëõùëíùë•ùë° |ùêª1 ùêΩ ‚àó (ùëöùëñùëõ {ùê∏ ‚àí ùê∏ùëù ‚àí ùê∏ùë† + ùê¥, ùêµ }, 1, 1, ùê∂, ùêªùëõùëíùë•ùë° ) + ùõº (1 ‚àí ùëù (ùê∂))Eùê¥|ùêª1,ùêªùëõùëíùë•ùë° |ùêª1 ùêΩ ‚àó (ùëöùëñùëõ {ùê∏ ‚àí ùê∏ùëù ‚àí ùê∏ùë† + ùê¥, ùêµ }, ùëá + 1, 1, ùê∂, ùêªùëõùëíùë•ùë° ) }
ùêΩ ‚àó (ùê∏ < ùê∏ùëù + ùê∏ùë† , ùëá , ùúè, ùê∂ùëùùëüùëíùë£ , ùêª1) = ùëá + ùõºEùê¥|ùêª1,ùêªùëõùëíùë•ùë° |ùêª1 ùêΩ ‚àó (ùëöùëñùëõ {ùê∏ + ùê¥, ùêµ }, ùëá + 1, ùúè + 1, ùê∂ùëùùëüùëíùë£ , ùêªùëõùëíùë•ùë° )
(7)
The Ô¨Årst expression in the minimization in the R.H.S. of the Ô¨Årst equation in (7) is the cost when ùëè(ùë°) = 0. The quantity ùëâ‚àó (ùê∏, ùëá, ùúè, ùê∂ùëùùëüùëíùë£ , ùêª1) is the expected cost when ùëè(ùë°) = 1,

which explains the second equation in (7). Here ùëû (ùúè) denote the ùúè-step transition probability of the channel dynamics, starting from ùê∂ùëùùëüùëíùë£ .
Similarly, in (7), if we consider ùêª2 for state (ùê∏, ùëá, ùúè, ùê∂ùëùùëüùëíùë£ , ùêª2) then the same Bellman equations hold with ùê¥ = 0. B. Policy structure
We conjecture the following properties regarding the value function and the optimal policy. Numerical validation of these conjectures is given in VI-C.
Conjecture 3. For Markovian system model and ùëÅ = 1, the value function ùêΩ‚àó (ùê∏, ùëá, ùúè, ùê∂ùëùùëüùëíùë£ , ùêª) is increasing in ùëá and ùëä‚àó (ùê∏, ùëá, ùê∂, ùêª) is decreasing in ùëù(ùê∂).
Conjecture 4. For Markovian system model and ùëÅ = 1, the optimal probing policy for the ùõº-discounted AoI cost minimization problem is a threshold policy on ùëá. For any ùê∏ ‚â• ùê∏ ùëù + ùê∏ùë†, the optimal action is to probe the channel state if and only if ùëá ‚â• ùëáùë°‚Ñé (ùê∏, ùúè, ùê∂ùëùùëüùëíùë£ , ùêª) for a threshold function ùëáùë°‚Ñé (ùê∏, ùúè, ùê∂ùëùùëüùëíùë£ , ùêª) of ùê∏, ùúè, ùê∂ùëùùëüùëíùë£ , ùêª.
Conjecture 5. For Markovian system model and ùëÅ = 1, at any time, if the source decides to probe the channel, then the optimal sampling policy is a threshold policy on ùëù(ùê∂). For any ùê∏ ‚â• ùê∏ ùëù + ùê∏ùë† and probed channel state, the optimal action is to sample the source node if and only if ùëù(ùê∂) ‚â• ùëùùë°‚Ñé (ùê∏, ùëá, ùêª) for a threshold function ùëùùë°‚Ñé (ùê∏, ùëá, ùêª) of ùê∏, ùëá and ùêª.
V. REINFORCEMENT LEARNING FOR AOI OPTIMIZATION: SINGLE PROCESS
If the channel statistics and the energy harvesting characteristics are not known at the source, then the source has to learn them with time. We adapt the Q-learning technique [32, Section 11.4.2] to Ô¨Ånd the optimal policy for our twostage action model. A. I.I.D. system
Here we assume that the energy generation rate ùúÜ and the channel state probabilities {ùëû ùëó }1‚â§ ùëó ‚â§ùëö are not known to the source. The packet success probabilities { ùëù ùëó }1‚â§ ùëó ‚â§ùëö are either known or unknown; the packet success indicator ùëü (ùë°) is used to estimate ùëù ùëó if it is unknown.

1) Optimality equation in terms of Q-function: The optimal Q function is denoted by ùëÑ‚àó (ùê∏, ùëá, ùëè) for a generic state-action pair (ùê∏, ùëá, ùëè), where the state is (ùê∏, ùëá) and the action is ùëè ‚àà {0, 1}. The quantity ùëÑ‚àó (ùê∏, ùëá, ùëè) denotes the expected cost obtained if the current state is (ùê∏, ùëá), current action chosen is ùëè, and an optimal policy is employed from the next time instant. Also, ùëÑ‚àó (ùê∏, ùëá, ùê∂, ùëé) corresponds to the intermediate state (ùê∏, ùëá, ùê∂) and a corresponding action ùëé ‚àà {0, 1}. It is important to note that, we have to maintain two different types of Q functions to handle states and intermediate states; this is not done in standard Q-learning. The optimal Q-value functions are given by:
ùëÑ‚àó (ùê∏ ‚â• ùê∏ ùëù + ùê∏ùë†, ùëá, ùëè = 0) = ùëá + ùõºEùê¥ùêΩ‚àó (ùëöùëñùëõ{ùê∏ + ùê¥, ùêµ}, ùëá + 1) = ùëá + ùõºEùê¥ min ùëÑ‚àó (ùëöùëñùëõ{ùê∏ + ùê¥, ùêµ}, ùëá + 1, ùëè) (8)
ùëè ‚àà {0,1 }
Here the terms in the R.H.S of the Ô¨Årst equality operator in (8) is the immediate cost of not probing channel state action (ùëè = 0) when state is (ùê∏ ‚â• ùê∏ ùëù + ùê∏ùë†, ùëá) and an ùõº discounted future cost Eùê¥ùêΩ‚àó (ùëöùëñùëõ{ùê∏ + ùê¥, ùêµ}, ùëá + 1) which is later substituted by Eùê¥ minùëè‚àà{0,1} ùëÑ‚àó (ùëöùëñùëõ{ùê∏ + ùê¥, ùêµ}, ùëá + 1, ùëè) (averaged over the distribution of A). On the other hand,
ùëö
ùëÑ‚àó (ùê∏ ‚â• ùê∏ùëù + ùê∏ùë† , ùëá , ùëè = 1) = ‚àëÔ∏Å ùëû ùëó ùëä ‚àó (ùê∏ , ùëá , ùê∂ ùëó )
ùëó=1 ùëö
= ‚àëÔ∏Å ùëûùëó min ùëÑ‚àó (ùê∏ , ùëá , ùê∂ ùëó , ùëé) (9)
ùëó=1 ùëé‚àà{0,1}
Now,
ùëÑ‚àó (ùê∏, ùëá, ùê∂, ùëé = 0) = ùëá + ùõºEùê¥ min ùëÑ‚àó (ùëöùëñùëõ{ùê∏ ‚àí ùê∏ ùëù + ùê¥, ùêµ}, ùëá + 1, ùëè) (10)
ùëè ‚àà {0,1 }
=ùêΩ ‚àó (ùëöùëñùëõ{ùê∏‚àíùê∏ùëù+ùê¥,ùêµ },ùëá +1)
and
ùëÑ‚àó (ùê∏ , ùëá , ùê∂, ùëé = 1) = ùëá (1 ‚àí ùëù (ùê∂)) + ùõº ùëù (ùê∂)Eùê¥ min ùëÑ‚àó (ùëöùëñùëõ{ùê∏ ‚àí ùê∏ùëù ‚àí ùê∏ùë† + ùê¥, ùêµ }, 1, ùëè)
ùëè‚àà{0,1}
+ùõº(1 ‚àí ùëù (ùê∂))Eùê¥ min ùëÑ‚àó (ùëöùëñùëõ{ùê∏ ‚àí ùê∏ùëù ‚àí ùê∏ùë† + ùê¥, ùêµ }, ùëá + 1, ùëè)
ùëè‚àà{0,1}
(11)
Similarly, the optimal Q-function for state (ùê∏ < ùê∏ ùëù + ùê∏ùë†, ùëá) is given by (12) in which only not probing (ùëè = 0) is a feasible action:
ùëÑ‚àó (ùê∏ < ùê∏ ùëù + ùê∏ùë†, ùëá, ùëè = 0) = ùëá + ùõºEùê¥ùêΩ‚àó (ùëöùëñùëõ{ùê∏ + ùê¥, ùêµ}, ùëá + 1) = ùëá + ùõºEùê¥ min ùëÑ‚àó (ùëöùëñùëõ{ùê∏ + ùê¥, ùêµ}, ùëá + 1, ùëè) (12)
ùëè ‚àà {0,1 }
2) Q-Learning algorithm: The optimal Q function is given by the zeros of (8)-(12). Here, we employ asynchronous stochastic approximation [33] to iteratively converge to the optimal Q function, starting from any arbitrary Q-function. Online learning is facilitated by sequentially observing ùê∂ (ùë°) (via channel probing), ùëü (ùë°) (via transmission ACK/NACK) and the number ùê¥(ùë°) of energy packets harvested at time ùë°.

In asynchronous stochastic approximation, we need step

size sequence ùëë (ùë°) (ùë° ‚â• 0) which satisÔ¨Åes the following

assumptions [34]:

Assumption 1. (i) 0 < ùëë (ùë°) ‚â§ ùëë¬Ø where ùëë¬Ø > 0,

(ii) ùëë (ùë° + 1) ‚â§ ùëë (ùë°) for all ùë° ‚â• 0,

(iii)

‚àû ùë° =0

ùëë(ùë°)

=

‚àû

and

‚àûùë°=0 (ùëë (ùë°))2 < ‚àû.

Let us denote by ùúàùë° (ùë†, ùëè) the number of occurrences of the state-action pair (ùë†, ùëè) up to iteration ùë° where ùë† ‚àà S and ùëè ‚àà {0, 1}. A similar notation is assumed for any generic intermediate state ùë£ and the corresponding action ùëé. The following assumption is required for the convergence of asynchronous stochastic approximation [34]. Assumption 2. lim infùë°‚Üí‚àû ùúàùë° (ùë†,ùëè) > 0 almost surely ‚àÄ(ùë†, ùëè),
ùë°
and lim infùë°‚Üí‚àû ùúàùë° (ùë£,ùëé) > 0 almost surely ‚àÄ(ùë£, ùëé).
ùë°
The proposed algorithm maintains a look-up table ùëÑùë° (¬∑, ¬∑) for various state-action pairs, and iteratively updates its entries depending on the current state, current action taken and the observed next state. However, Assumption 2 requires that all state-action pairs should be visited inÔ¨Ånitely and comparatively often. This is ensured by taking a random action (uniformly chosen) with probability ùúñ at each decision instant, and taking the action arg ùëöùëñùëõùëèùëÑ(ùë†, ùëè) or arg ùëöùëñùëõùëéùëÑ(ùë£, ùëé) with probability (1 ‚àí ùúñ).
One example of Q-value update is the following, aimed at convergence to the solution of (8):

ùëÑùë°+1 (ùê∏ ‚â• ùê∏ùëù + ùê∏ùë† , ùëá , ùëè = 0) = ùëÑùë° (ùê∏ , ùëá , ùëè = 0) + ùëë (ùúàùë° (ùê∏ , ùëá , ùëè = 0))1{ùë† (ùë°) = (ùê∏ , ùëá ) , ùëè (ùë°) = 0} [ùëá
+ùõº min ùëÑùë° (ùëöùëñùëõ{ùê∏ + ùê¥(ùë°) , ùêµ }, ùëá + 1, ùëè) ‚àí ùëÑùë° (ùê∏ , ùëá , ùëè = 0) ] (13)
ùëè‚àà{0,1}
Here 1{¬∑} is the indicator function. It is important to note that, this ùëÑ update can be performed only when the random next state (ùëöùëñùëõ{ùê∏ + ùê¥(ùë°), ùêµ}, ùëá + 1) is is observed.
In a similar fashion, equations (9)-(12) lead to the following Q-updates for various states and intermediate states:

ùëÑùë°+1 (ùê∏ ‚â• ùê∏ ùëù + ùê∏ùë†, ùëá , ùëè = 1)

= ùëÑùë° (ùê∏, ùëá, ùëè = 1) + ùëë (ùúàùë° (ùê∏, ùëá, ùëè = 1)1{ùë†(ùë°) = (ùê∏, ùëá), ùëè(ùë°) = 1}

[ min ùëÑùë° (ùê∏, ùëá, ùê∂, ùëé) ‚àí ùëÑùë° (ùê∏, ùëá, ùëè = 1)]

(14)

ùëé ‚àà {0,1 }

ùëÑùë°+1 (ùê∏, ùëá, ùê∂, ùëé = 0)

= ùëÑùë° (ùê∏, ùëá, ùê∂, ùëé = 0) + ùëë (ùúàùë° (ùê∏, ùëá, ùê∂, ùëé = 0))1{ùë£(ùë°) = (ùê∏, ùëá, ùê∂),

ùëé(ùë°) = 0}[ùëá + ùõº min ùëÑùë° (ùëöùëñùëõ{ùê∏ ‚àí ùê∏ ùëù + ùê¥(ùë°), ùêµ}, ùëá + 1, ùëè)
ùëè ‚àà {0,1 }

‚àíùëÑùë° (ùê∏, ùëá, ùê∂, ùëé = 0)]

(15)

ùëÑùë°+1 (ùê∏ , ùëá , ùê∂, ùëé = 1)

= ùëÑùë° (ùê∏ , ùëá , ùê∂, ùëé = 1) + ùëë (ùúàùë° (ùê∏ , ùëá , ùê∂, ùëé = 1))1{ùë£ (ùë°) = (ùê∏ , ùëá , ùê∂) ,

ùëé (ùë°) = 1} [ùëá (1 ‚àí ùëù (ùê∂)) + ùõº ùëù (ùê∂) min ùëÑùë° (ùëöùëñùëõ{ùê∏ ‚àí ùê∏ùëù ‚àí ùê∏ùë† + ùê¥(ùë°) ,
ùëè‚àà{0,1}

ùêµ }, 1, ùëè) + ùõº(1 ‚àí ùëù (ùê∂)) min ùëÑùë° (ùëöùëñùëõ{ùê∏ ‚àí ùê∏ùëù ‚àí ùê∏ùë† + ùê¥(ùë°) , ùêµ }, ùëá + 1,
ùëè‚àà{0,1}

ùëè) ‚àí ùëÑùë° (ùê∏ , ùëá , ùê∂, ùëé = 1) ]

(16)

ùëÑùë°+1 (ùê∏ < ùê∏ùëù + ùê∏ùë† , ùëá , ùëè = 0)

= ùëÑùë° (ùê∏ , ùëá , ùëè = 0) + ùëë (ùúàùë° (ùê∏ , ùëá , ùëè = 0))1{ùë† (ùë°) = (ùê∏ , ùëá ) , ùëè (ùë°) = 0} [ùëá +

ùõº min ùëÑùë° (ùëöùëñùëõ{ùê∏ + ùê¥(ùë°) , ùêµ }, ùëá + 1, ùëè) ‚àí ùëÑùë° (ùê∏ , ùëá , ùëè = 0) ]

(17)

ùëè‚àà{0,1}

In (16), we consider the case where packet success probability ùëù(ùê∂) for channel state ùê∂ is known to the source. However, if these probabilities are unknown, then one can replace ùëù(ùê∂) in (16) by ùëü (ùë°) to obtain a valid Q-update.
Q learning type algorithm for ùëÅ > 1 can also be formulated similarly.

ùëÑùë°+1 (ùê∏ ‚â• ùê∏ùëù + ùê∏ùë† , ùëá , ùúè, ùê∂ùëùùëüùëíùë£ , ùêª1, ùëè = 1)

= ùëÑùë° (ùê∏ , ùëá , ùúè, ùê∂ùëùùëüùëíùë£ , ùêª1, ùëè = 1) + ùëë (ùúàùë° (ùê∏ , ùëá , ùúè, ùê∂ùëùùëüùëíùë£ , ùêª1, ùëè = 1))

1{ùë† (ùë°) = (ùê∏ , ùëá , ùúè, ùê∂ùëùùëüùëíùë£ , ùêª1) , ùëè (ùë°) = 1} [ min ùëÑùë° (ùê∏ , ùëá , ùê∂, ùêª1, ùëé) ‚àí
ùëé‚àà{0,1}

ùëÑùë° (ùê∏ , ùëá , ùúè, ùê∂ùëùùëüùëíùë£ , ùêª1, ùëè = 1) ]

(24)

B. Markovian system
Here we adapt Q-learning for the Markovian model with unknown channel statistics and unknown energy harvesting characteristics to obtain the optimal solutions for (7).
1) Optimality equation in terms of Q-function: The optimal Q-value functions are given by:

ùëÑ‚àó (ùê∏ ‚â• ùê∏ ùëù + ùê∏ùë†, ùëá , ùúè, ùê∂ùëùùëüùëíùë£ , ùêª1, ùëè = 0) = ùëá + ùõºEùê¥|ùêª1,ùêªùëõùëíùë•ùë° |ùêª1 ùêΩ‚àó (ùëöùëñùëõ{ùê∏ + ùê¥, ùêµ}, ùëá + 1, ùúè + 1, ùê∂ùëùùëü ùëíùë£ ,

ùêªùëõùëíùë•ùë° )

= ùëá + ùõºEùê¥|ùêª ,ùêª |ùêª min ùëÑ‚àó (ùëöùëñùëõ{ùê∏ + ùê¥, ùêµ}, ùëá + 1, ùúè + 1,
1 ùëõùëíùë•ùë° 1 ùëè ‚àà {0,1}

ùê∂ùëùùëüùëíùë£ , ùêªùëõùëíùë•ùë° , ùëè)

(18)

On the other hand,

ùëÑ‚àó (ùê∏ ‚â• ùê∏ ùëù + ùê∏ùë†, ùëá , ùúè, ùê∂ùëùùëüùëíùë£ , ùêª1, ùëè = 1) = Eùê∂‚àºùëû(ùúè) ( ¬∑ |ùê∂ ) ùëä ‚àó (ùê∏, ùëá , ùê∂, ùêª1)
ùëùùëü ùëí ùë£
= Eùê∂‚àºùëû(ùúè) ( ¬∑|ùê∂ ) min ùëÑ‚àó (ùê∏, ùëá , ùê∂, ùêª1, ùëé) (19)
ùëùùëüùëíùë£ ùëé‚àà{0,1}
When ùëè(ùë°) = 1, for a current measured channel state ùê∂, the optimal ùëÑ‚àó functions for action ùëé = 0 and ùëé = 1 are given by:

ùëÑùë°+1 (ùê∏ , ùëá , ùê∂, ùêª1, ùëé = 0)

= ùëÑùë° (ùê∏ , ùëá , ùê∂, ùêª1, ùëé = 0) + ùëë (ùúàùë° (ùê∏ , ùëá , ùê∂, ùêª1, ùëé = 0))1{ùë£ (ùë°) =

(ùê∏ , ùëá , ùê∂, ùêª1) , ùëé (ùë°) = 0} [ùëá + ùõº min ùëÑùë° (ùëöùëñùëõ{ùê∏ ‚àí ùê∏ùëù + ùê¥(ùë°) , ùêµ },
ùëè‚àà{0,1}

ùëá + 1, 1, ùê∂, ùêªùëõùëíùë•ùë° , ùëè) ‚àí ùëÑùë° (ùê∏ , ùëá , ùê∂, ùêª1, ùëé = 0) ]

(25)

ùëÑùë°+1 (ùê∏ , ùëá , ùê∂, ùêª1, ùëé = 1) = ùëÑùë° (ùê∏ , ùëá , ùê∂, ùêª1, ùëé = 1) + ùëë (ùúàùë° (ùê∏ , ùëá , ùê∂, ùêª1, ùëé = 1))1{ùë£ (ùë°) =
(ùê∏ , ùëá , ùê∂, ùêª1) , ùëé (ùë°) = 1} [ùëá (1 ‚àí ùëù (ùê∂)) + ùõº ùëù (ùê∂) min ùëÑùë° (ùëöùëñùëõ{ùê∏ ‚àí
ùëè‚àà{0,1}
ùê∏ùëù ‚àí ùê∏ùë† + ùê¥(ùë°) , ùêµ }, 1, 1, ùê∂, ùêªùëõùëíùë•ùë° , ùëè) + ùõº(1 ‚àí ùëù (ùê∂)) min ùëÑùë° (ùëöùëñùëõ{ùê∏
ùëè‚àà{0,1}
‚àíùê∏ùëù ‚àí ùê∏ùë† + ùê¥(ùë°) , ùêµ }, ùëá + 1, 1, ùê∂, ùêªùëõùëíùë•ùë° , ùëè) ‚àí ùëÑùë° (ùê∏ , ùëá , ùê∂, ùêª1, ùëé = 1) ] (26)

ùëÑùë°+1 (ùê∏ < ùê∏ùëù + ùê∏ùë† , ùëá , ùúè, ùê∂ùëùùëüùëíùë£ , ùêª1, ùëè = 0) = ùëÑùë° (ùê∏ , ùëá , ùúè, ùê∂ùëùùëüùëíùë£ , ùêª1, ùëè = 0) + ùëë (ùúàùë° (ùê∏ , ùëá , ùúè, ùê∂ùëùùëüùëíùë£ , ùêª1, ùëè = 0))
1{ùë† (ùë°) = (ùê∏ , ùëá , ùúè, ùê∂ùëùùëüùëíùë£ , ùêª1) , ùëè (ùë°) = 0} [ùëá + ùõº min ùëÑùë° (ùëöùëñùëõ{ùê∏ + ùê¥(ùë°) ,
ùëè‚àà{0,1}
ùêµ }, ùëá + 1, ùúè + 1, ùê∂ùëùùëüùëíùë£ , ùêªùëõùëíùë•ùë° , ùëè) ‚àí ùëÑùë° (ùê∏ , ùëá , ùúè, ùê∂ùëùùëüùëíùë£ , ùêª1, ùëè = 0) ] (27)
Similarly, the Q-value function iteration updates are given for state (ùê∏, ùëá, ùúè, ùê∂ùëùùëüùëíùë£ , ùêª2) by taking ùê¥ = 0. Also, when ùëù(ùê∂) is not known, it can be replaced by ùëü (ùë°) in (26).

ùëÑ‚àó (ùê∏, ùëá, ùê∂, ùêª1, ùëé = 0) = ùëá + ùõºEùê¥|ùêª ,ùêª |ùêª min ùëÑ‚àó (ùëöùëñùëõ{ùê∏ ‚àí ùê∏ ùëù + ùê¥, ùêµ}, ùëá + 1,
1 ùëõùëíùë•ùë° 1 ùëè ‚àà {0,1}

1, ùê∂, ùêªùëõùëíùë•ùë° , ùëè)

(20)

ùëÑ‚àó (ùê∏ , ùëá , ùê∂, ùêª1, ùëé = 1) = ùëá (1 ‚àí ùëù (ùê∂)) + ùõº ùëù (ùê∂)Eùê¥|ùêª ,ùêª |ùêª min ùëÑ‚àó (ùëöùëñùëõ {ùê∏ ‚àí ùê∏ùëù ‚àí ùê∏ùë†
1 ùëõùëíùë•ùë° 1 ùëè‚àà{0,1}

+ùê¥, ùêµ }, 1, 1, ùê∂, ùêªùëõùëíùë•ùë° , ùëè) + ùõº (1 ‚àí ùëù (ùê∂))Eùê¥|ùêª1,ùêªùëõùëíùë•ùë° |ùêª1 min
ùëè‚àà{0,1}

ùëÑ‚àó (ùëöùëñùëõ{ùê∏ ‚àí ùê∏ùëù ‚àí ùê∏ùë† + ùê¥, ùêµ }, ùëá + 1, 1, ùê∂, ùêªùëõùëíùë•ùë° , ùëè) }

(21)

Also, the optimal Q-function for state (ùê∏ < ùê∏ ùëù + ùê∏ùë†, ùëá , ùúè, ùê∂ùëùùëüùëíùë£ , ùêª1) is given by:

ùëÑ‚àó (ùê∏ < ùê∏ ùëù + ùê∏ùë†, ùëá , ùúè, ùê∂ùëùùëüùëíùë£ , ùêª1, ùëè = 0) = ùëá + ùõºEùê¥|ùêª ,ùêª |ùêª min ùëÑ‚àó (ùëöùëñùëõ{ùê∏ + ùê¥, ùêµ}, ùëá + 1, ùúè + 1,
1 ùëõùëíùë•ùë° 1 ùëè ‚àà {0,1}

ùê∂ùëùùëüùëíùë£ , ùêªùëõùëíùë•ùë° , ùëè)

(22)

Similarly, the Q functions are given for state (ùê∏, ùëá , ùúè, ùê∂ùëùùëüùëíùë£ , ùêª2) by putting ùê¥ = 0.
2) Q-Learning algorithm: Here also we employ asynchronous stochastic approximation to Ô¨Ånd the solution of (18)-(22) as in V-A. The Q-updates for various states and intermediate states are given by following:

ùëÑùë°+1 (ùê∏ ‚â• ùê∏ùëù + ùê∏ùë† , ùëá , ùúè, ùê∂ùëùùëüùëíùë£ , ùêª1, ùëè = 0) = ùëÑùë° (ùê∏ , ùëá , ùúè, ùê∂ùëùùëüùëíùë£ , ùêª1, ùëè = 0) + ùëë (ùúàùë° (ùê∏ , ùëá , ùúè, ùê∂ùëùùëüùëíùë£ , ùêª1, ùëè = 0))
1{ùë† (ùë°) = (ùê∏ , ùëá , ùúè, ùê∂ùëùùëüùëíùë£ , ùêª1) , ùëè (ùë°) = 0} [ùëá + ùõº min ùëÑùë° (ùëöùëñùëõ{ùê∏ + ùê¥(ùë°) ,
ùëè‚àà{0,1}
ùêµ }, ùëá + 1, ùúè + 1, ùê∂ùëùùëüùëíùë£ , ùêªùëõùëíùë•ùë° , ùëè) ‚àí ùëÑùë° (ùê∏ , ùëá , ùúè, ùê∂ùëùùëüùëíùë£ , ùêª1, ùëè = 0) ] (23)

VI. NUMERICAL RESULTS
A. Single process, I.I.D. system, known dynamics
We consider Ô¨Åve channel states (ùëö = 5) with channel state occurrence probabilities q = [0.2, 0.2, 0.2, 0.2, 0.2] and the corresponding packet success probabilities p = [0.9, 0.7, 0.5, 0.3, 0.1]. Energy arrival process is i.i.d. ùêµùëíùëüùëõùëúùë¢ùëôùëôùëñ(ùúÜ) with energy buffer size ùêµ = 12, ùê∏ ùëù = 1 unit, ùê∏ùë† = 1 unit. Numerical exploration reveals that there exists a threshold policy on ùëá in decision making for channel state probing; see Figure 2(a) which substantiates our conjecture in III-A2. It is observed that this ùëáùë°‚Ñé (ùê∏) decreases with ùê∏ since higher available energy in the energy buffer allows the EH node to probe the channel state more aggressively. Similar reasoning explains the observation that ùëáùë°‚Ñé (ùê∏) decreases with ùúÜ. For probed channel state, Figure 2(b) shows the variation of ùëùùë°‚Ñé (ùê∏, ùëá) with ùê∏, ùëá, ùúÜ. It is observed that ùëùùë°‚Ñé (ùê∏, ùëá) decreases with ùê∏, since the EH node tries to sample the process more aggressively if more energy is available in the buffer. Similarly, higher value of ùëá results in aggressive sampling, and hence ùëùùë°‚Ñé (ùê∏, ùëá) decreases with ùëá. By similar arguments as before, we can explain the observation that this ùëùùë°‚Ñé (ùê∏, ùëá) decreases with ùúÜ.
B. Multiple processes (ùëÅ > 1), I.I.D. model, known dynamics
We choose ùëÅ = 3, ùêµ = 12, ùê∏ ùëù = 1, ùê∏ùë† = 1 and the same channel model and parameters as in Section VI-A. Figure 3(a) provides validation to our conjecture for optimal probing policy in III-B2 by demonstrating the variation on the threshold on ùëá1 for given ùëá2, ùëá3, for channel state

(a)

(a)

(b)
Fig. 2: Single process (ùëÅ = 1), I.I.D. model, known dynamics: (a) Variation of ùëáùë°‚Ñé (ùê∏) with ùê∏, ùúÜ and (b) Variation of ùëùùë°‚Ñé (ùê∏, ùëá) with ùê∏, ùëá, ùúÜ.

(b)
Fig. 3: ùëÅ = 3, I.I.D. model, known dynamics: (a) Variation of ùëáùë°‚Ñé (ùê∏, ùëá2, ùëá3) with ùê∏, ùëá2, ùëá3, ùúÜ and (b) Variation of ùëùùë°‚Ñé (ùê∏, ùëá1, ùëá2, ùëá3) with ùê∏, ùëá1, ùëá2, ùëá3, ùúÜ.

probing. It is observed that ùëáùë°‚Ñé (ùê∏, ùëá2, ùëá3) decreases with ùê∏ and ùúÜ. Extensive numerical work also demonstrated that this
threshold decreases with each of ùëá2, ùëá3. For probed channel state, Figure 3(b) shows that ùëùùë°‚Ñé (ùê∏, ùëá1, ùëá2, ùëá3) decreases with ùê∏ and ùúÜ. Further numerical analysis also demonstrated that
this threshold decreases with each of ùëá1, ùëá2, ùëá3 .

C. Markovian model, ùëÅ = 1, known dynamics
We consider a two state Markovian fading channel (ùëö = 2), where ùê∂1 is the good channel state and ùê∂2 is the bad channel state. The channel state transition probabilities are ùëû1,2 = 0.1 and ùëû2,1 = 0.1, and the corresponding packet transmission success probabilities are ùëù1 = 0.9, ùëù2 = 0.4. Also, for the Markovian energy harvesting process, we consider the transition probabilities ‚Ñé1,2 = 0.3 and ‚Ñé2,1 = 0.3. When the source node is in harvesting state, the energy packet generation process is i.i.d. ùêµùëíùëüùëõùëúùë¢ùëôùëôùëñ(ùúÜ) across time, and otherwise zero. We consider ùêµ = 9, ùê∏ ùëù = 1 unit, ùê∏ùë† = 1 unit, and ùúÜ = 0.4.
Our numerical exploration demonstrated the threshold nature of the optimal policy which veriÔ¨Åed conjectures predicted in IV-B. Figure 4 shows that, for a Ô¨Åxed ùúè, ùëáùë°‚Ñé (ùê∏, ùúè, ùê∂ùëùùëüùëíùë£ , ùêª) decreases with ùê∏, ùëáùë°‚Ñé (ùê∏, ùúè, ùê∂ùëùùëüùëíùë£ , ùêª1) ‚â§ ùëáùë°‚Ñé (ùê∏, ùúè, ùê∂ùëùùëüùëíùë£ , ùêª2) and ùëáùë°‚Ñé (ùê∏, ùúè, ùê∂1, ùêª) ‚â§ ùëáùë°‚Ñé (ùê∏, ùúè, ùê∂2, ùêª) (because probability of packet success in channel state ùê∂1 is higher than that in ùê∂2). Also, from Figure 5(a) it is observed that for probed channel state, ùëùùë°‚Ñé (ùê∏, ùëá, ùêª) decreases with ùê∏ and ùëá , and ùëùùë°‚Ñé (ùê∏, ùëá , ùêª1) ‚â§ ùëùùë°‚Ñé (ùê∏, ùëá , ùêª2).
Also, numerical exploration revealed that, after probing is done, the optimal sampling policy can also be represented as a threshold policy on ùëá instead of ùëù(ùê∂); in this case, sampling

(a)
(b)
Fig. 4: ùëÅ = 1, Markov model: (a) Variation of ùëáùë°‚Ñé (ùê∏, ùúè, ùê∂ùëùùëüùëíùë£ , ùêª) with ùê∏, ùúè, ùê∂ùëùùëüùëíùë£ = ùê∂1 and ùêª and (b) Variation of ùëáùë°‚Ñé (ùê∏, ùúè, ùê∂ùëùùëüùëíùë£ , ùêª) with ùê∏, ùúè, ùê∂ùëùùëüùëíùë£ = ùê∂2 and ùêª.
is done when ùëá ‚â• ùëáùë°‚Ñé (ùê∏, ùê∂, ùêª). The variation of ùëáùë°‚Ñé (ùê∏, ùê∂, ùêª) with ùê∏, ùê∂, ùêª is shown in Figure 5(b), which supports many

(a) (a)

(b)
Fig. 5: ùëÅ = 1, Markov model: (a) Variation of ùëùùë°‚Ñé (ùê∏, ùëá, ùêª) with ùê∏, ùëá, and ùêª (b) Variation of ùëáùë°‚Ñé (ùê∏, ùê∂, ùêª) with ùê∏, ùê∂, and ùêª.
intuitive justiÔ¨Åcations provided earlier in this section.
D. Comparison between probing and not probing
1) I.I.D. System: We consider ùëÅ = 1 and the same model as in Section VI-A, and compare the performance of our probing based threshold policy against the optimal policy that involves sampling mandatorily without probing (as in Section III-A3) based on ùëá and ùê∏. The results are summarised in Figure 6 (a); we note that AoI increases with ùê∏ ùëù and ùê∏ùë† since each action becomes more costly for the source. One interesting observation is that when the sensing energy is close to probing energy (ùê∏ùë† = 1, ùê∏ ùëù = 1), we see slightly higher time averaged AoI values for probing case compared to the case with ùê∏ùë† = 1 and no probing. This happens because the advantage offered by probing is being overshadowed by the extra amount of energy expended for it. However, when ùê∏ùë† = 5, ùê∏ ùëù = 1, we notice that probing yields a lower time averaged AoI. Obviously, the importance of probing is dependent on the trade off between the advantage offered by probing and the extra energy required for it. Typically, ùê∏ ùëù is much smaller than ùê∏ùë† since probing requires less energy as compared to sampling and transmitting a data packet. We have also observed numerically that, when there is sufÔ¨Åcient variation in the channel quality, probing improves the AoI performance. On the other hand, when channel variation is not predominant, spending energy in probing tends to hit the AoI performance and can sometimes render it worse than no probing.

(b)
Fig. 6: For ùëÅ = 1, Variation of time averaged AoI and comparison of a probing and non-probing system with change in ùúÜ, ùê∏ ùëù and ùê∏ùë† for (a) I.I.D. model, (b) Markovian model.
2) Markovian System: We consider the same system parameters as in Section VI-C. Here also Figure 6 (b) exhibits similar trade-off between the improvement offered by probing and the energy spent in it, as seen for the i.i.d. model.
E. Q-Learning We consider the same settings as Section VI-A and
Section VI-C, except that we set ùêµ = 5 and an upper bound on the age as ùëáùëöùëéùë• = 7, in order to reduce the number of possible states. We then run an online Q-learning scheme across two different sample paths. For the sake of comparison, we also plot the time averaged AoI of a uniform random strategy as well as that of the optimal policy derived through value iteration. Convergence of the time-averaged AoI under Q-learning to the optimal AoI for the i.i.d. channel system and the Markovian model can be seen in Figure 7 (a) and Figure 7 (b), respectively. We also observe signiÔ¨Åcant improvement from the initial policy which always picks an action uniformly at random; this demonstrates the necessity of the MDP based formulation for probing and sampling.
VII. CONCLUSIONS In this paper, we have derived optimal policy structures for minimizing the time-averaged expected AoI under an energy-harvesting source. We considered single and multiple processes, i.i.d. and Markovian time varying channels, time varying characteristics for energy harvesting, and channel probing capability at the source. The optimal source sampling policy often turned out to be a threshold policy. We have also proposed RL algorithms for unknown channel statistics and

Similarly, using ùêΩ (ùë°) (ùê∏, ùëá) ‚â• ùêΩ‚àó (ùê∏, ùëá) ‚àí ùëíùë° in (28), we obtain:

ùêΩ (ùë°+1) (ùê∏ ‚â• ùê∏ùëù + ùê∏ùë† , ùëá ) ‚â• ùëöùëñùëõ ùëá + ùõºEùê¥ ( ùêΩ ‚àó (ùëöùëñùëõ{ùê∏ + ùê¥, ùêµ }, ùëá + 1) ‚àí ùëíùë° ) ,

Eùê∂ ùëöùëñùëõ{ùëá + ùõºEùê¥ ( ùêΩ ‚àó (ùëöùëñùëõ{ùê∏ ‚àí ùê∏ùëù + ùê¥, ùêµ }, ùëá + 1) ‚àí ùëíùë° ) ,

ùëá (1 ‚àí ùëù (ùê∂)) + ùõº ùëù (ùê∂)Eùê¥ ( ùêΩ ‚àó (ùëöùëñùëõ{ùê∏ ‚àí ùê∏ùëù ‚àí ùê∏ùë† + ùê¥, ùêµ }, 1) ‚àí ùëíùë° )

+ùõº(1 ‚àí ùëù (ùê∂))Eùê¥ ( ùêΩ ‚àó (ùëöùëñùëõ{ùê∏ ‚àí ùê∏ùëù ‚àí ùê∏ùë† + ùê¥, ùêµ }, ùëá + 1) ‚àí ùëíùë° ) } (30)

(a)

ùêΩ ‚àó (ùê∏ ‚â• ùê∏ùëù + ùê∏ùë† , ùëá ) ‚àí ùõºùëíùë°

‚â§ ùêΩ (ùë°+1) (ùê∏ ‚â• ùê∏ùëù + ùê∏ùë† , ùëá ) ‚â§ ùêΩ ‚àó (ùê∏ ‚â• ùê∏ùëù + ùê∏ùë† , ùëá ) + ùõºùëíùë°

=‚áí | ùêΩ (ùë°+1) (ùê∏ ‚â• ùê∏ùëù + ùê∏ùë† , ùëá ) ‚àí ùêΩ ‚àó (ùê∏ ‚â• ùê∏ùëù + ùê∏ùë† , ùëá ) | ‚â§ ùõºùëíùë°

=‚áí max | ùêΩ (ùë°+1) (ùë†) ‚àí ùêΩ ‚àó (ùë†) | ‚â§ ùõºùëíùë°
ùë†‚ààS
=‚áí ùëíùë°+1 ‚â§ ùõºùëíùë°

=‚áí lim ùëíùë° = 0

(31)

ùë° ‚Üí‚àû

Hence, ùêΩ (ùë°) (ùë†) converges to ùêΩ‚àó (ùë†).

(b)
Fig. 7: Improvement in time averaged AoI exhibited by Qlearning, ùëÅ = 1, (a) I.I.D. model, (b) Markovian model.

energy harvesting characteristics. However, there are a number of issues that remain untouched, such as multi-source star topology and multi-hop network setting with multiple source and sink nodes. We plan to address these issues in our future research endeavours.

APPENDIX A PROOF OF PROPOSITION 1 We prove that ùëöùëéùë•ùë†‚ààS |ùêΩ (ùë°) (ùë†) ‚àí ùêΩ‚àó (ùë†)| ‚Üë 0 as ùë° ‚Üë ‚àû. Let us deÔ¨Åne the error ùëíùë° = maxùë†‚ààS |ùêΩ (ùë°) (ùë†) ‚àí ùêΩ‚àó (ùë†)| and ùêΩ (0) (ùë†) as initial estimate for ùêΩ‚àó (ùë†). For any state ùë†, we seek to establish relation between the error at time ùë° + 1 to the error at time ùë°.
ùêΩ (ùë°+1) (ùê∏ ‚â• ùê∏ùëù + ùê∏ùë† , ùëá )
= ùëöùëñùëõ ùëá + ùõºEùê¥ ùêΩ (ùë°) (ùëöùëñùëõ{ùê∏ + ùê¥, ùêµ }, ùëá + 1) ,

Eùê∂

ùëöùëñ ùëõ {ùëá

+

(ùë°)
ùõºE ùê¥ ùêΩ

( ùëöùëñ ùëõ { ùê∏

‚àí

ùê∏ùëù

+

ùê¥,

ùêµ}, ùëá

+

1) ,

ùëá (1 ‚àí ùëù (ùê∂)) + ùõº ùëù (ùê∂)Eùê¥ ùêΩ (ùë°) (ùëöùëñùëõ{ùê∏ ‚àí ùê∏ùëù ‚àí ùê∏ùë† + ùê¥, ùêµ }, 1) +

ùõº(1 ‚àí ùëù (ùê∂))Eùê¥ ùêΩ (ùë°) (ùëöùëñùëõ{ùê∏ ‚àí ùê∏ùëù ‚àí ùê∏ùë† + ùê¥, ùêµ }, ùëá + 1) }

(28)

We assume there exists an optimal value function ùêΩ‚àó (ùê∏, ùëá)
for the discounted cost problem. By using the fact that ùêΩ (ùë°) (ùê∏, ùëá) ‚â§ ùêΩ‚àó (ùê∏, ùëá) + ùëíùë° in (28), we obtain:

ùêΩ (ùë°+1) (ùê∏ ‚â• ùê∏ùëù + ùê∏ùë† , ùëá ) ‚â§ ùëöùëñùëõ ùëá + ùõºEùê¥ ( ùêΩ ‚àó (ùëöùëñùëõ{ùê∏ + ùê¥, ùêµ }, ùëá + 1) + ùëíùë° ) ,
Eùê∂ ùëöùëñùëõ{ùëá + ùõºEùê¥ ( ùêΩ ‚àó (ùëöùëñùëõ{ùê∏ ‚àí ùê∏ùëù + ùê¥, ùêµ }, ùëá + 1) + ùëíùë° ) , ùëá (1 ‚àí ùëù (ùê∂)) + ùõº ùëù (ùê∂)Eùê¥ ( ùêΩ ‚àó (ùëöùëñùëõ{ùê∏ ‚àí ùê∏ùëù ‚àí ùê∏ùë† + ùê¥, ùêµ }, 1) + ùëíùë° ) +ùõº(1 ‚àí ùëù (ùê∂))Eùê¥ ( ùêΩ ‚àó (ùëöùëñùëõ{ùê∏ ‚àí ùê∏ùëù ‚àí ùê∏ùë† + ùê¥, ùêµ }, ùëá + 1) + ùëíùë° ) } (29)

APPENDIX B PROOF OF LEMMA 1 We prove this result by value iteration:
ùêΩ (ùë°+1) (ùê∏ ‚â• ùê∏ùëù + ùê∏ùë† , ùëá )
= ùëöùëñùëõ ùëá + ùõºEùê¥ ùêΩ (ùë°) (ùëöùëñùëõ{ùê∏ + ùê¥, ùêµ }, ùëá + 1) ,

Eùê∂

ùëöùëñ ùëõ {ùëá

+

(ùë°)
ùõºE ùê¥ ùêΩ

( ùëöùëñ ùëõ { ùê∏

‚àí

ùê∏ùëù

+

ùê¥,

ùêµ}, ùëá

+

1) ,

ùëá (1 ‚àí ùëù (ùê∂)) + ùõº ùëù (ùê∂)Eùê¥ ùêΩ (ùë°) (ùëöùëñùëõ{ùê∏ ‚àí ùê∏ùëù ‚àí ùê∏ùë† + ùê¥, ùêµ }, 1) +

ùõº(1 ‚àí ùëù (ùê∂))Eùê¥ ùêΩ (ùë°) (ùëöùëñùëõ{ùê∏ ‚àí ùê∏ùëù ‚àí ùê∏ùë† + ùê¥, ùêµ }, ùëá + 1) }

ùêΩ (ùë°+1) (ùê∏ < ùê∏ùëù + ùê∏ùë† , ùëá )

=

ùëá

+

(ùë°)
ùõºE ùê¥ ùêΩ

( ùëöùëñ ùëõ { ùê∏

+

ùê¥,

ùêµ}, ùëá

+

1)

(32)

Let us start with ùêΩ (0) (ùë†) = 0 for all ùë† ‚àà ùëÜ. Clearly, ùêΩ (1) (ùê∏ ‚â• ùê∏ ùëù + ùê∏ùë†, ùëá) = ùëöùëñùëõ{ùëá, Eùê∂ (ùëöùëñùëõ{ùëá, ùëá (1 ‚àí ùëù(ùê∂))})} = ùëöùëñùëõ{ùëá, Eùê∂ (ùëá (1 ‚àí ùëù(ùê∂))} and ùêΩ (1) (ùê∏ < ùê∏ ùëù + ùê∏ùë†, ùëá) = ùëá. Hence, for any given ùê∏, the value function ùêΩ (1) (ùê∏, ùëá) is an
increasing function of ùëá. As induction hypothesis, we assume that ùêΩ (ùë°) (ùê∏, ùëá) is also increasing function of ùëá. Now,

ùêΩ (ùë°+1) (ùê∏ ‚â• ùê∏ùëù + ùê∏ùë† , ùëá ) = ùëöùëñùëõ ùëá + ùõºEùê¥ ùêΩ (ùë°) (ùëöùëñùëõ{ùê∏ + ùê¥, ùêµ }, ùëá + 1) ,

Eùê∂

ùëöùëñ ùëõ {ùëá

+

(ùë°)
ùõºE ùê¥ ùêΩ

( ùëöùëñ ùëõ { ùê∏

‚àí

ùê∏ùëù

+

ùê¥,

ùêµ}, ùëá

+

1) ,

ùëá (1 ‚àí ùëù (ùê∂)) + ùõº ùëù (ùê∂)Eùê¥ ùêΩ (ùë°) (ùëöùëñùëõ{ùê∏ ‚àí ùê∏ùëù ‚àí ùê∏ùë† + ùê¥, ùêµ }, 1) +

ùõº(1 ‚àí ùëù (ùê∂))Eùê¥ ùêΩ (ùë°) (ùëöùëñùëõ{ùê∏ ‚àí ùê∏ùëù ‚àí ùê∏ùë† + ùê¥, ùêµ }, ùëá + 1) }

(33)

We need to show that ùêΩ (ùë°+1) (ùê∏ ‚â• ùê∏ ùëù + ùê∏ùë†, ùëá ) is also increasing in ùëá. The Ô¨Årst term inside the minimization operation in (33) is increasing in ùëá, by the induction hypothesis and from the fact that expectation is a linear operation. On the other hand, the second term has linear expectation over channel state and another minimization operator. Also, the Ô¨Årst and second terms inside the second minimization operation in (33) are increasing in ùëá by the induction hypothesis and the linearity of expectation operation. Thus, ùêΩ (ùë°+1) (ùê∏ ‚â• ùê∏ ùëù + ùê∏ùë†, ùëá) is also increasing in ùëá . By

similar arguments, we can claim that ùêΩ (ùë°+1) (ùê∏ < ùê∏ ùëù + ùê∏ùë†, ùëá) is increasing in ùëá. Now, since ùêΩ (ùë°) (¬∑) ‚Üë ùêΩ‚àó (¬∑) as ùë° ‚Üë ‚àû by proof of Proposition 1, ùêΩ‚àó (ùê∏, ùëá) is also increasing in ùëá. Hence, the
Ô¨Årst part of the lemma is proved.
For currrent probed channel state, the value function ùëä (ùë°+1) (ùê∏, ùëá , ùê∂) is given by:

ùëä (ùë°+1) (ùê∏ , ùëá , ùê∂)

=

ùëöùëñ ùëõ {ùëá

+

(ùë°)
ùõºE ùê¥ ùêΩ

( ùëöùëñ ùëõ { ùê∏

‚àí

ùê∏ùëù

+

ùê¥,

ùêµ}, ùëá

+

1) ,

ùëá (1 ‚àí ùëù (ùê∂)) + ùõº ùëù (ùê∂)Eùê¥ ùêΩ (ùë°) (ùëöùëñùëõ{ùê∏ ‚àí ùê∏ùëù ‚àí ùê∏ùë† + ùê¥, ùêµ }, 1) +

ùõº(1 ‚àí ùëù (ùê∂))Eùê¥ ùêΩ (ùë°) (ùëöùëñùëõ{ùê∏ ‚àí ùê∏ùëù ‚àí ùê∏ùë† + ùê¥, ùêµ }, ùëá + 1) }

(34)

The Ô¨Årst term (cost of not sampling a source) inside the minimization operation in (34) is independent of ùëù(ùê∂), whereas the second term (cost of sampling a source) inside the minimization operation in (34) is given by:

ùëá (1 ‚àí ùëù (ùê∂)) + ùõº ùëù (ùê∂)Eùê¥ ùêΩ (ùë°) (ùëöùëñùëõ{ùê∏ ‚àí ùê∏ùëù ‚àí ùê∏ùë† + ùê¥, ùêµ }, 1) +

ùõº(1 ‚àí ùëù (ùê∂))Eùê¥ ùêΩ (ùë°) (ùëöùëñùëõ{ùê∏ ‚àí ùê∏ùëù ‚àí ùê∏ùë† + ùê¥, ùêµ }, ùëá + 1)

=

ùëá

(1

‚àí

ùëù (ùê∂) )

+

(ùë°)
ùõºE ùê¥ ùêΩ

( ùëöùëñ ùëõ { ùê∏

‚àí

ùê∏ùëù

‚àí

ùê∏ùë†

+

ùê¥,

ùêµ}, ùëá

+

1)

‚àí

ùëù (ùê∂) ùõº Eùê¥ ùêΩ (ùë°) (ùëöùëñùëõ{ùê∏ ‚àí ùê∏ùëù ‚àí ùê∏ùë† + ùê¥, ùêµ }, ùëá + 1) ‚àí

Eùê¥ ùêΩ (ùë°) (ùëöùëñùëõ {ùê∏ ‚àí ùê∏ùëù ‚àí ùê∏ùë† + ùê¥, ùêµ }, 1)

(35)

By the induction hypothesis and the linearity of expectation operation, Eùê¥ùêΩ (ùë°) (ùëöùëñùëõ{ùê∏ ‚àí ùê∏ ùëù ‚àí ùê∏ùë† + ùê¥, ùêµ}, ùëá + 1) ‚àí Eùê¥ùêΩ (ùë°) (ùëöùëñùëõ{ùê∏ ‚àí ùê∏ ùëù ‚àí ùê∏ùë† + ùê¥, ùêµ}, 1) is non-negative. Thus,
the second term inside the minimization operation in (34) is decreasing in ùëù(ùê∂). Now, since ùëä (ùë°) (¬∑) ‚Üë ùëä‚àó (¬∑) as ùë° ‚Üë ‚àû, ùëä‚àó (ùê∏, ùëá, ùê∂) is also decreasing in ùëù(ùê∂).

‚àíùê∏ùë† + ùê¥, ùêµ }, ùëá1 + 1, ùëá2 + 1, ¬∑ ¬∑ ¬∑ , ùëá = 1, ùëáùëò+1 + 1, ¬∑ ¬∑ ¬∑ , ùëáùëÅ + 1) +
ùëò
ùõº(1 ‚àí ùëù (ùê∂))Eùê¥ ùêΩ (ùë°) (ùëöùëñùëõ{ùê∏ ‚àí ùê∏ùëù ‚àí ùê∏ùë† + ùê¥, ùêµ }, ùëá1 + 1, ùëá2 + 1, ¬∑ ¬∑ ¬∑ ,
ùëáùëÅ + 1) }

ùêΩ (ùë°+1) (ùê∏ < ùê∏ùëù + ùê∏ùë† , ùëá1, ùëá2, ¬∑ ¬∑ ¬∑ , ùëáùëÅ )

ùëÅ

=

‚àëÔ∏Å ùëáùëñ

+

(ùë°)
ùõºE ùê¥ ùêΩ

( ùëöùëñ ùëõ { ùê∏

+

ùê¥,

ùêµ }, ùëá1

+

1, ùëá2

+

1,

¬∑

¬∑

¬∑ , ùëáùëÅ

+

1)

ùëñ=1

(36)

Let us start with ùêΩ (0) (ùë†) = 0 for all ùë† ‚àà ùëÜ.

Clearly, ùêΩ (1) (ùê∏ ‚â• ùê∏ ùëù + ùê∏ùë†, ùëá1, ùëá2, ¬∑ ¬∑ ¬∑ , ùëáùëÅ ) =

min{

ùëÅ ùëñ=1

ùëáùëñ

,

Eùê∂

(

ùëö

ùëñ

ùëõ

{

ùëÅ ùëñ=1

ùëáùëñ

,

min1

‚â§

ùëò

‚â§

ùëÅ

(

ùëñ‚â†ùëò ùëáùëñ

+ ùëáùëò (1

‚àí

ùëù(ùê∂)))}} and ùêΩ (1) (ùê∏ < ùê∏ ùëù + ùê∏ùë†, ùëá1, ùëá2, ¬∑ ¬∑ ¬∑ , ùëáùëÅ ) =

ùëÅ ùëñ=1

ùëáùëñ

.

Hence, for any given ùê∏, the value function

ùêΩ (1) (ùê∏, ùëá1, ùëá2, ¬∑ ¬∑ ¬∑ , ùëáùëÅ ) is an increasing function of

ùëá1, ùëá2, ¬∑ ¬∑ ¬∑ , ùëáùëÅ . As induction hypothesis, we assume

that ùêΩ (ùë°) (ùê∏, ùëá1, ùëá2, ¬∑ ¬∑ ¬∑ , ùëáùëÅ ) is also increasing function

of ùëá1, ùëá2, ¬∑ ¬∑ ¬∑ , ùëáùëÅ . Now,

ùêΩ (ùë°+1) (ùê∏ ‚â• ùê∏ùëù + ùê∏ùë† , ùëá1, ùëá2, ¬∑ ¬∑ ¬∑ , ùëáùëÅ )

ùëÅ

=

ùëöùëñùëõ

‚àëÔ∏Å ùëáùëñ

+

(ùë°)
ùõºE ùê¥ ùêΩ

( ùëöùëñ ùëõ { ùê∏

+

ùê¥,

ùêµ }, ùëá1

+

1, ùëá2

+

1,

¬∑

¬∑

¬∑ , ùëáùëÅ

+

1) ,

ùëñ=1

ùëÅ

Eùê∂

‚àëÔ∏Å ùëöùëñùëõ{ ùëáùëñ

+

ùõºE ùê¥ ùêΩ

(ùë°)

( ùëöùëñ ùëõ { ùê∏

‚àí

ùê∏ùëù

+

ùê¥,

ùêµ },

ùëá1

+

1,

ùëá2

+

1,

¬∑

¬∑

¬∑

,

ùëñ=1

ùëá + 1) , min

‚àëÔ∏Å ùëá +ùëá

(1 ‚àí ùëù(ùê∂)) + ùõºùëù(ùê∂)E

ùêΩ (ùë°) (ùëöùëñùëõ {ùê∏ ‚àí ùê∏

ùëÅ

ùëñùëò

ùê¥

ùëù

1‚â§ùëò‚â§ùëÅ ùëñ‚â†ùëò

‚àíùê∏ùë† + ùê¥, ùêµ }, ùëá1 + 1, ùëá2 + 1, ¬∑ ¬∑ ¬∑ , ùëá = 1, ùëáùëò+1 + 1, ¬∑ ¬∑ ¬∑ , ùëáùëÅ + 1) +
ùëò

ùõº(1 ‚àí ùëù (ùê∂))Eùê¥ ùêΩ (ùë°) (ùëöùëñùëõ{ùê∏ ‚àí ùê∏ùëù ‚àí ùê∏ùë† + ùê¥, ùêµ }, ùëá1 + 1, ùëá2 + 1, ¬∑ ¬∑ ¬∑ ,

ùëáùëÅ + 1) }

(37)

APPENDIX C PROOF OF THEOREM 1
From (3), it is obvious that for probed channel state the optimal decision for ùê∏ ‚â• ùê∏ ùëù + ùê∏ùë† is to sample the source if and only if the cost of sampling is lower than the cost of not sampling the source, i.e., ùëá + ùõºEùê¥ùêΩ‚àó (ùëöùëñùëõ{ùê∏ ‚àí ùê∏ ùëù + ùê¥, ùêµ}, ùëá + 1) ‚â• ùëá (1 ‚àí ùëù(ùê∂)) + ùõºEùê¥ùêΩ‚àó (ùëöùëñùëõ{ùê∏ ‚àí ùê∏ ùëù ‚àí ùê∏ùë† + ùê¥, ùêµ}, ùëá + 1) ‚àí
ùõº ùëù(ùê∂) Eùê¥ùêΩ‚àó (ùëöùëñùëõ{ùê∏ ‚àí ùê∏ ùëù ‚àí ùê∏ùë† + ùê¥, ùêµ}, ùëá + 1) ‚àí Eùê¥ùêΩ‚àó (ùëöùëñùëõ{ùê∏ ‚àí
ùê∏ ùëù ‚àí ùê∏ùë† + ùê¥, ùêµ}, 1) . Now, by Lemma 1, Eùê¥ùêΩ‚àó (ùëöùëñùëõ{ùê∏ ‚àí ùê∏ ùëù ‚àí
ùê∏ùë† + ùê¥, ùêµ}, ùëá + 1) ‚àí Eùê¥ùêΩ‚àó (ùëöùëñùëõ{ùê∏ ‚àí ùê∏ ùëù ‚àí ùê∏ùë† + ùê¥, ùêµ}, 1) is nonnegative. Thus the R.H.S. decreases with ùëù(ùê∂), whereas the L.H.S. is independent of ùëù(ùê∂). Hence, for probed channel state the optimal action is to sample if and only if ùëù(ùê∂) ‚â• ùëùùë°‚Ñé (ùê∏, ùëá) for some suitable threshold function ùëùùë°‚Ñé (ùê∏, ùëá).

APPENDIX D
PROOF OF LEMMA 2 The proof is similar to the proof of Lemma 1 and it follows from the convergence of value iteration as given below:

ùêΩ (ùë°+1) (ùê∏ ‚â• ùê∏ùëù + ùê∏ùë† , ùëá1, ùëá2, ¬∑ ¬∑ ¬∑ , ùëáùëÅ )

ùëÅ

=

ùëöùëñùëõ

‚àëÔ∏Å ùëáùëñ

+

(ùë°)
ùõºE ùê¥ ùêΩ

( ùëöùëñ ùëõ { ùê∏

+

ùê¥,

ùêµ }, ùëá1

+

1, ùëá2

+

1,

¬∑

¬∑

¬∑ , ùëáùëÅ

+

1) ,

ùëñ=1

ùëÅ

Eùê∂

‚àëÔ∏Å ùëöùëñùëõ{ ùëáùëñ

+

ùõºE ùê¥ ùêΩ

(ùë°)

( ùëöùëñ ùëõ { ùê∏

‚àí

ùê∏ùëù

+

ùê¥,

ùêµ },

ùëá1

+

1,

ùëá2

+

1,

¬∑

¬∑

¬∑

,

ùëñ=1

ùëá + 1), min

‚àëÔ∏Å ùëá +ùëá

(1 ‚àí ùëù(ùê∂)) + ùõºùëù(ùê∂)E

ùêΩ (ùë°) (ùëöùëñùëõ {ùê∏ ‚àí ùê∏

ùëÅ

ùëñùëò

ùê¥

ùëù

1‚â§ùëò‚â§ùëÅ ùëñ‚â†ùëò

We seek to show that ùêΩ (ùë°+1) (ùê∏ ‚â• ùê∏ ùëù + ùê∏ùë†, ùëá1, ùëá2, ¬∑ ¬∑ ¬∑ , ùëáùëÅ ) is also increasing in each of ùëá1, ùëá2, ¬∑ ¬∑ ¬∑ , ùëáùëÅ . The Ô¨Årst term inner to the minimization operation in (37) is increasing in each of ùëá1, ùëá2, ¬∑ ¬∑ ¬∑ , ùëáùëÅ , utilizing the induction hypothesis and linear property of expectation operation. On the other hand, the second term has expectation over channel state and another minimization operator. Also, the Ô¨Åst and second term of second minimization operator is increasing in each of ùëá1, ùëá2, ¬∑ ¬∑ ¬∑ , ùëáùëÅ by using induction hypothesis and the linearity of expectation operation. Thus, ùêΩ (ùë°+1) (ùê∏ ‚â• ùê∏ ùëù + ùê∏ùë†, ùëá1, ùëá2, ¬∑ ¬∑ ¬∑ , ùëáùëÅ ) is also increasing in each of ùëá1, ùëá2, ¬∑ ¬∑ ¬∑ , ùëáùëÅ . Similarly, we can assert that ùêΩ (ùë°+1) (ùê∏ < ùê∏ ùëù + ùê∏ùë†, ùëá1, ùëá2, ¬∑ ¬∑ ¬∑ , ùëáùëÅ ) is increasing in each of ùëá1, ùëá2, ¬∑ ¬∑ ¬∑ , ùëáùëÅ . Now, since ùêΩ (ùë°) (¬∑) ‚Üë ùêΩ‚àó (¬∑) as ùë° ‚Üë ‚àû, ùêΩ‚àó (ùê∏, ùëá1, ùëá2, ¬∑ ¬∑ ¬∑ , ùëáùëÅ ) is also increasing in each of ùëá1, ùëá2, ¬∑ ¬∑ ¬∑ , ùëáùëÅ . Hence, the Ô¨Årst part of the lemma is proved. Proof of second part of Lemma 2 is similar to the proof of second part of Lemma 1
APPENDIX E PROOF OF THEOREM 2
It is obvious that ùêΩ‚àó (¬∑) is invariant to any permutation of (ùëá1, ùëá2, ¬∑ ¬∑ ¬∑ , ùëáùëÅ ). Hence, by Lemma 2,
arg min1‚â§ùëò ‚â§ùëÅ ùëñ‚â†ùëò ùëáùëñ +ùëáùëò (1‚àí ùëù(ùê∂))+ùõº ùëù(ùê∂)Eùê¥ùêΩ (ùë°) (ùëöùëñùëõ{ùê∏ ‚àí
ùê∏ ùëù ‚àí ùê∏ùë† + ùê¥, ùêµ}, ùëá1 + 1, ùëá2 + 1, ¬∑ ¬∑ ¬∑ , ùëáùëò = 1, ùëáùëò+1 + 1, ¬∑ ¬∑ ¬∑ , ùëáùëÅ + 1) + ùõº(1 ‚àí ùëù(ùê∂))Eùê¥ùêΩ (ùë°) (ùëöùëñùëõ{ùê∏ ‚àí ùê∏ ùëù ‚àí ùê∏ùë† + ùê¥, ùêµ}, ùëá1 +
1, ùëá2 + 1, ¬∑ ¬∑ ¬∑ , ùëáùëÅ + 1) = arg max1‚â§ùëò ‚â§ùëÅ ùëáùëò , i.e., the best

process to activate is ùëò‚àó arg max1‚â§ùëò ‚â§ùëÅ ùëáùëò in case one process has to be activated. For probed channel state, it
is optimal to sample a process if and only if the cost
of sampling this process is less than or equal to the
cost of not sampling this process, which translates into ùëáùëò‚àó + ùõºEùê¥ùêΩ‚àó (ùëöùëñùëõ{ùê∏ ‚àí ùê∏ ùëù + ùê¥, ùêµ}, ùëá1 + 1, ùëá2 + 1, ¬∑ ¬∑ ¬∑ , ùëáùëÅ + 1) ‚â• ùëáùëò‚àó (1 ‚àí ùëù(ùê∂)) + ùõºEùê¥ùêΩ‚àó (ùëöùëñùëõ{ùê∏ ‚àí ùê∏ ùëù ‚àí ùê∏ùë† + ùê¥, ùêµ}, ùëá1 + 1, ùëá2 +
1, ¬∑ ¬∑ ¬∑ , ùëáùëÅ + 1) ‚àí ùõº ùëù(ùê∂) Eùê¥ùêΩ‚àó (ùëöùëñùëõ{ùê∏ ‚àí ùê∏ ùëù ‚àí ùê∏ùë† + ùê¥, ùêµ}, ùëá1 +
1, ùëá2 + 1, ¬∑ ¬∑ ¬∑ , ùëáùëÅ + 1) ‚àí Eùê¥ùêΩ‚àó (ùëöùëñùëõ{ùê∏ ‚àí ùê∏ ùëù ‚àí ùê∏ùë† + ùê¥, ùêµ}, ùëá1 +
1, ùëá2 + 1, ¬∑ ¬∑ ¬∑ , ùëáùëò = 1, ùëáùëò+1 + 1, ¬∑ ¬∑ ¬∑ , ùëáùëÅ + 1) . Now, by Lemma 2,
Eùê¥ùêΩ‚àó (ùëöùëñùëõ{ùê∏ ‚àí ùê∏ ùëù ‚àí ùê∏ùë† + ùê¥, ùêµ}, ùëá1 + 1, ùëá2 + 1, ¬∑ ¬∑ ¬∑ , ùëáùëÅ + 1) ‚àí Eùê¥ùêΩ‚àó (ùëöùëñùëõ{ùê∏ ‚àí ùê∏ ùëù ‚àí ùê∏ùë† + ùê¥, ùêµ}, ùëá1 + 1, ùëá2 + 1, ¬∑ ¬∑ ¬∑ , ùëá =
ùëò
1, ùëáùëò+1 + 1, ¬∑ ¬∑ ¬∑ , ùëáùëÅ + 1) is non negative. Thus, the R.H.S. is decreasing in ùëù(ùê∂) and the L.H.S. is independent of ùëù(ùê∂).
Hence, the threshold structure of the optimal sampling policy
is proved.
REFERENCES
[1] A. Jaiswal and A. Chattopadhyay, ‚ÄúMinimization of age-of-information in remote sensing with energy harvesting,‚Äù in 2021 IEEE International Symposium on Information Theory (ISIT). IEEE, 2021, pp. 3249‚Äì3254.
[2] S. Kaul, R. Yates, and M. Gruteser, ‚ÄúReal-time status: How often should one update?‚Äù in 2012 Proceedings IEEE INFOCOM. IEEE, 2012, pp. 2731‚Äì2735.
[3] R. Talak, S. Karaman, and E. Modiano, ‚ÄúCan determinacy minimize age of information?‚Äù arXiv preprint arXiv:1810.04371, 2018.
[4] S. K. Kaul, R. D. Yates, and M. Gruteser, ‚ÄúStatus updates through queues,‚Äù in 2012 46th Annual Conference on Information Sciences and Systems (CISS). IEEE, 2012, pp. 1‚Äì6.
[5] R. D. Yates and S. K. Kaul, ‚ÄúThe age of information: Real-time status updating by multiple sources,‚Äù IEEE Transactions on Information Theory, vol. 65, no. 3, pp. 1807‚Äì1827, 2018.
[6] S. K. Kaul and R. D. Yates, ‚ÄúTimely updates by multiple sources: The m/m/1 queue revisited,‚Äù in 2020 54th Annual Conference on Information Sciences and Systems (CISS). IEEE, 2020, pp. 1‚Äì6.
[7] A. Kosta, N. Pappas, A. Ephremides, and V. Angelakis, ‚ÄúAge of information performance of multiaccess strategies with packet management,‚Äù Journal of Communications and Networks, vol. 21, no. 3, pp. 244‚Äì255, 2019.
[8] S. Farazi, A. G. Klein, and D. R. Brown, ‚ÄúAge of information in energy harvesting status update systems: When to preempt in service?‚Äù in 2018 IEEE International Symposium on Information Theory (ISIT). IEEE, 2018, pp. 2436‚Äì2440.
[9] A. Arafa and S. Ulukus, ‚ÄúAge-minimal transmission in energy harvesting two-hop networks,‚Äù in GLOBECOM 2017-2017 IEEE Global Communications Conference. IEEE, 2017, pp. 1‚Äì6.
[10] S. Farazi, A. G. Klein, and D. R. Brown, ‚ÄúAverage age of information for status update systems with an energy harvesting server,‚Äù in IEEE INFOCOM 2018-IEEE Conference on Computer Communications Workshops (INFOCOM WKSHPS). IEEE, 2018, pp. 112‚Äì117.
[11] H. Hu, K. Xiong, Y. Zhang, P. Fan, T. Liu, and S. Kang, ‚ÄúAge of information in wireless powered networks in low snr region for future 5g,‚Äù Entropy, vol. 20, no. 12, p. 948, 2018.
[12] A. Arafa and S. Ulukus, ‚ÄúAge minimization in energy harvesting communications: Energy-controlled delays,‚Äù in 2017 51st Asilomar Conference on Signals, Systems, and Computers. IEEE, 2017, pp. 1801‚Äì1805.
[13] Z. Chen, N. Pappas, E. Bj√∂rnson, and E. G. Larsson, ‚ÄúAge of information in a multiple access channel with heterogeneous trafÔ¨Åc and an energy harvesting node,‚Äù in IEEE INFOCOM 2019-IEEE Conference on Computer Communications Workshops (INFOCOM WKSHPS). IEEE, 2019, pp. 662‚Äì667.
[14] X. Wu, J. Yang, and J. Wu, ‚ÄúOptimal status update for age of information minimization with an energy harvesting source,‚Äù IEEE Transactions on Green Communications and Networking, vol. 2, no. 1, pp. 193‚Äì204, 2017.
[15] J. Yang, X. Wu, and J. Wu, ‚ÄúOptimal scheduling of collaborative sensing in energy harvesting sensor networks,‚Äù IEEE Journal on Selected Areas in Communications, vol. 33, no. 3, pp. 512‚Äì523, 2015.

[16] B. T. Bacinoglu, Y. Sun, E. Uysal-Bivikoglu, and V. Mutlu, ‚ÄúAchieving the age-energy tradeoff with a Ô¨Ånite-battery energy harvesting source,‚Äù in 2018 IEEE International Symposium on Information Theory (ISIT). IEEE, 2018, pp. 876‚Äì880.
[17] S. Feng and J. Yang, ‚ÄúAge of information minimization for an energy harvesting source with updating erasures: With and without feedback,‚Äù arXiv preprint arXiv:1808.05141, 2018.
[18] B. T. Bacinoglu, E. T. Ceran, and E. Uysal-Biyikoglu, ‚ÄúAge of information under energy replenishment constraints,‚Äù in 2015 Information Theory and Applications Workshop (ITA). IEEE, 2015, pp. 25‚Äì31.
[19] S. Leng and A. Yener, ‚ÄúAge of information minimization for an energy harvesting cognitive radio,‚Äù IEEE Transactions on Cognitive Communications and Networking, vol. 5, no. 2, pp. 427‚Äì439, 2019.
[20] E. Gindullina, L. Badia, and D. G√ºnd√ºz, ‚ÄúAge-of-information with information source diversity in an energy harvesting system,‚Äù arXiv preprint arXiv:2004.11135, 2020.
[21] E. T. Ceran, D. G√ºnd√ºz, and A. Gy√∂rgy, ‚ÄúReinforcement learning to minimize age of information with an energy harvesting sensor with harq and sensing cost,‚Äù in IEEE INFOCOM 2019-IEEE Conference on Computer Communications Workshops (INFOCOM WKSHPS). IEEE, 2019, pp. 656‚Äì661.
[22] A. Arafa, J. Yang, and S. Ulukus, ‚ÄúAge-minimal online policies for energy harvesting sensors with random battery recharges,‚Äù in 2018 IEEE International Conference on Communications (ICC). IEEE, 2018, pp. 1‚Äì6.
[23] C. Tunc and S. Panwar, ‚ÄúOptimal transmission policies for energy harvesting age of information systems with battery recovery,‚Äù in 2019 53rd Asilomar Conference on Signals, Systems, and Computers. IEEE, 2019, pp. 2012‚Äì2016.
[24] B. T. Bacinoglu and E. Uysal-Biyikoglu, ‚ÄúScheduling status updates to minimize age of information with an energy harvesting sensor,‚Äù in 2017 IEEE International Symposium on Information Theory (ISIT). IEEE, 2017, pp. 1122‚Äì1126.
[25] M. A. Abd-Elmagid, H. S. Dhillon, and N. Pappas, ‚ÄúA reinforcement learning framework for optimizing age of information in rf-powered communication systems,‚Äù IEEE Transactions on Communications, vol. 68, no. 8, pp. 4747‚Äì4760, 2020.
[26] M. Hatami, M. Leinonen, and M. Codreanu, ‚ÄúAoi minimization in status update control with energy harvesting sensors,‚Äù arXiv preprint arXiv:2009.04224, 2020.
[27] A. Arafa, J. Yang, S. Ulukus, and H. V. Poor, ‚ÄúTimely status updating over erasure channels using an energy harvesting sensor: Single and multiple sources,‚Äù IEEE Transactions on Green Communications and Networking, 2021.
[28] G. Yao, A. M. Bedewy, and N. B. Shroff, ‚ÄúAge-optimal low-power status update over time-correlated fading channel,‚Äù in 2021 IEEE International Symposium on Information Theory (ISIT). IEEE, 2021, pp. 2972‚Äì2977.
[29] N. Michelusi, K. Stamatiou, and M. Zorzi, ‚ÄúTransmission policies for energy harvesting sensors with time-correlated energy supply,‚Äù IEEE Transactions on Communications, vol. 61, no. 7, pp. 2988‚Äì3001, 2013.
[30] B. Sombabu and S. Moharir, ‚ÄúAge-of-information aware scheduling under markovian energy arrivals,‚Äù in 2020 International Conference on Signal Processing and Communications (SPCOM). IEEE, 2020, pp. 1‚Äì5.
[31] D. P. Bertsekas, ‚ÄúDynamic programming and optimal control 3rd edition, volume ii,‚Äù Belmont, MA: Athena ScientiÔ¨Åc, 2011.
[32] S. Bhatnagar, H. Prasad, and L. Prashanth, Stochastic Recursive Algorithms for Optimization: Simultaneous Perturbation Methods. Springer, 2013.
[33] V. S. Borkar, ‚ÄúAsynchronous stochastic approximations,‚Äù SIAM Journal on Control and Optimization, vol. 36, no. 3, pp. 840‚Äì851, 1998.
[34] S. Bhatnagar, ‚ÄúThe borkar‚Äìmeyn theorem for asynchronous stochastic approximations,‚Äù Systems & control letters, vol. 60, no. 7, pp. 472‚Äì478, 2011.

