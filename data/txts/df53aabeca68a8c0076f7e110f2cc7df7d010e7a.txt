Deep Learning based Multi-Source Localization with Source Splitting and its Eﬀectiveness in Multi-Talker Speech Recognition
Aswin Shanmugam Subramaniana,∗, Chao Wengc, Shinji Watanabeb,a, Meng Yud, Dong Yud
aCenter for Language and Speech Processing, Johns Hopkins University, Baltimore, MD, USA bLanguage Technologies Institute, Carnegie Mellon University, Pittsburgh, PA, USA cTencent AI Lab, Shenzhen, China dTencent AI Lab, Bellevue, WA, USA

arXiv:2102.07955v2 [eess.AS] 28 Nov 2021

Abstract
Multi-source localization is an important and challenging technique for multi-talker conversation analysis. This paper proposes a novel supervised learning method using deep neural networks to estimate the direction of arrival (DOA) of all the speakers simultaneously from the audio mixture. At the heart of the proposal is a source splitting mechanism that creates source-speciﬁc intermediate representations inside the network. This allows our model to give source-speciﬁc posteriors as the output unlike the traditional multi-label classiﬁcation approach. Existing deep learning methods perform a frame level prediction, whereas our approach performs an utterance level prediction by incorporating temporal selection and averaging inside the network to avoid post-processing. We also experiment with various loss functions and show that a variant of earth mover distance (EMD) is very eﬀective in classifying DOA at a very high resolution by modeling inter-class relationships. In addition to using the prediction error as a metric for evaluating our localization model, we also establish its potency as a frontend with automatic speech recognition (ASR) as the downstream task. We convert the estimated DOAs into a feature suitable for ASR and pass it as an additional input feature to a strong multi-channel and multi-talker speech recognition baseline. This added input feature drastically improves the ASR performance and gives a word error rate (WER) of 6.3% on the evaluation data of our simulated noisy two speaker mixtures, while the baseline which doesn’t use explicit localization input has a WER of 11.5%. We also perform ASR evaluation on real recordings with the overlapped set of the MC-WSJ-AV corpus in addition to simulated mixtures.
Keywords: source localization, multi-talker speech recognition

1. Introduction
Direction of arrival (DOA) estimation is the task of estimating the direction of the sound sources with respect to the microphone array and it is an important aspect of source localization. This localization knowledge can aid many downstream applications. For example, they are used in robot audition systems [1, 2] to facilitate interaction with humans. Source localization is pivotal in making the human-robot communication more natural by rotating the robot’s head. The localization component in the robot audition pipeline also aids in source separation and recognition. Moreover, there is growing interest in using farﬁeld systems to process multi-talker conversations like meeting transcription [3]. Incorporating the source localization functionality can enrich such systems by monitoring the location of speakers and also potentially help improve the performance of the downstream automatic speech recognition (ASR) task. DOA estimation can also be used with smart speaker scenarios [4, 5] and potentially aid with better audio-visual fusion.

∗Corresponding Author. A part of the work was carried out by this author during an intership at Tencent AI Lab, Bellevue, USA.
Email addresses: aswin@jhu.edu (Aswin Shanmugam Subramanian), cweng@tencent.com (Chao Weng), shinjiw@ieee.org (Shinji Watanabe), raymondmyu@tencent.com (Meng Yu), dyu@tencent.com (Dong Yu)

Preprint submitted to Computer Speech & Language

November 30, 2021

Some of the earlier DOA estimation techniques are based on narrowband subspace methods [6, 7]. The simple wideband approximation can be achieved by using the incoherent signal subspace method (ISSM), which uses techniques such as multiple signal classiﬁcation (MUSIC) independently on narrowband signals of diﬀerent frequencies and average their results to obtain the ﬁnal DOA estimate. Broadband methods which better utilize the correlation between diﬀerent frequencies such as weighted average of signal subspaces (WAVES) [8] and test of orthogonality of projected subspaces (TOPS) [9] have shown promising improvements. Alternative cross-correlation based methods like steered response power with phase transform (SRP-PHAT) [10] have also been proposed. Most subspace methods are not robust to reverberations as they were developed under a free-ﬁeld propagation model [11]. Recently, there has been a shift in interest to supervised deep learning methods to make the estimation robust to challenging acoustic conditions.
The initial deep learning methods were developed for single source scenarios [12, 13, 14, 15, 16]. In [13] and [14], features are extracted from MUSIC and GCC-PHAT respectively. The learning is made more robust in [12, 15, 16] by encapsulating the feature extraction inside the neural network. While [14] treats DOA estimation as a regression problem, all the other methods formulate it as a classiﬁcation problem by discretizing the possible DOA angles. The modeling resolution which is determined by the output dimension of the network is quite low in [13], [12], and [16] at 5◦, 45◦, and 5◦ respectively. Although the modeling resolution is high in [15] with classes separated by just 1◦, the evaluation was performed only with a block size of 5◦ and it is also shown to be not robust to noise. In [17], it was shown that a classiﬁcation based system with a grid resolution of 10◦ and a regression based system gives similar performance.
Deep learning based techniques have been shown to be very eﬀective for joint sound event localization and detection (SELD) of overlapping sources consisting of both speech and non-speech events [18, 19]. In [18], SELD is solved using a multi-task learning approach with DOA estimation treated as a multi-output regression while in [19], detection and localization is solved jointly using a uniﬁed regression based loss. In this work, we focus only on localization of speech sources as we are interested in investigating the importance of DOA estimation as a frontend with speech recognition as the downstream task.
Time-frequency (TF) masks estimated from speech enhancement [20] or separation [21] systems have been used for DOA estimation to handle noise, and interference in [22, 23, 24, 25]. The preprocessing speech enhancement system in [22, 23] is used to select time-frequency regions in the single speech source input signal that exclude noise and other non-speech interference. In [25] and [26], the interference consists of a speech source so the preprocessing system takes an auxiliary input to extract the speaker of interest. In [25], a target speech separation system similar to [27] is used and this preprocessing system takes preenrollment speech corresponding to the speaker of interest as an auxiliary input. In [26], keywords were used to identify the target. Although, multiple source inputs are used in these methods, an independently trained preprocessing system is used to identify the target input and the source localization system performs a biased estimation based on this identiﬁer.
The speech enhancement systems used in single speech source scenario and target separation systems used in multi-talker scenario are typically trained with the groundtruth signals as the target and hence can be trained with only simulated data. Cascading with such a preprocessing system also increases the complexity of the overall DOA estimation framework. In [22], a pseudo multi task learning based end-to-end training approach was proposed for a single speech source input. In end-to-end training both the speech enhancement network and the DOA estimation network were jointly optimized with only the DOA label (and without using the target clean signal). It was also shown to give comparable performance to a typical multi-task learning approach. End-to-end methods are interesting as it has the scope for training with real data as the DOA labels can be obtained with infrared sensors as used in [28].
The deep learning models were extended to handle simultaneous estimation of multiple sources in an endto-end manner by treating it as a multi-label classiﬁcation problem in [29, 11, 30]. These approaches perform an unbiased estimation to obtain the DOAs of all the sources. The multi-label classiﬁcation approach still gives only a single vector as output with each dimension treated as a separate classiﬁcation problem. This procedure is not good at capturing the intricate inter-class relationship in our problem. So increasing the number of classes to perform classiﬁcation at a very high resolution will fail. During inference, the output vector is treated like a spatial spectrum and DOAs are estimated based on its peaks. In [30, 29], the number of peaks is determined based on a threshold. In [11], the number of sources are assumed to be known based
2

Figure 1: Multi-label Classiﬁcation Model for 2-source Scenario
on which the peaks are chosen at the output. In [29] and [11], the source locations are restricted to be only in a spatial grid with a step size of 10◦ and 15◦ respectively. To make the models work on realistic conditions, it is crucial to handle all possible source locations. Moreover, these models perform a frame level prediction in spite of the sources assumed to be stationary inside an utterance. Hence, post-processing is required to get the utterance level DOA estimate.
In this work, we propose a source splitting mechanism, which involves treating the neural network as two distinct and well deﬁned components. The ﬁrst component disentangles the input mixture by implicitly splitting them as source-speciﬁc hidden representations to enable tackling of multi-source DOA estimation via single-source DOA estimation. The second component then maps these features to as many DOA posteriors as the number of sources in the mixture. This makes the number of classiﬁcation problems to be only equal to the number of sources and not the number of DOA classes like the existing methods. With the added help of loss functions that can handle the inter-class relationship better, our methods can classify DOA reliably at a high resolution by performing a ﬁne-grid estimation. Although, source splitting formulates the network as two components, it is optimized end-to-end with only DOA estimation objective. Like [11], we assume the knowledge of number of sources in the utterance but we incorporate it directly in the architecture of the model instead of using it while post-processing.
We evaluate our proposed localization method both as a standalone task and also based on its eﬀectiveness as a frontend to aid speech recognition. Assuming the DOA to be known, the eﬀectiveness of using features extracted from the ground-truth location for target speech recognition was shown in [31, 32] as a proof of concept. The importance of localization for multi-talker speech recognition was also shown in our previous work [33]. In [33], we proposed a model named directional ASR, which can learn to predict DOA without explicit supervision by joint optimization with speech recognition. In the second part of this paper, we devise an approach that can use localization knowledge to improve multi-talker speech recognition. We use a strong multi-talker speech recognition system called MIMO-Speech [34, 35] as a baseline. MIMO-Speech doesn’t perform explicit localization of the sources. The DOAs estimated from our proposed models in the ﬁrst part are converted to angle features [31]. The angles features encode DOA information and act as a localization prior. These angles features are given as additional inputs to the MIMO-Speech model and tested for its eﬀectiveness in improving speech recognition. It is hard to obtain DOA labels for real datasets. We perform DOA estimation for real speech mixtures from multichannel wall street journal audio-visual (MC-WSJ-AV) corpus and evaluate the quality of localization solely based on the downstream ASR metric of word error rate (WER).
2. Deep Learning based Multi-Source Localization
Given a multi-channel input signal with multiple sources, a deep neural network can be trained to predict all the azimuth angles corresponding to each of the sources. Typically it is treated as a multi-label classiﬁcation problem [11, 29], as shown in Figure 1. Here, the network outputs only a single vector which is supposed to encode the DOA of all the sources. It is possible to have spurious peaks in angle classes that are in close proximity to any of the target angle classes, especially when the number of classes are increased.
3

Figure 2: Proposed Model with Source Splitting Mechanism for 2-source Scenario
As more than one peak needs to be selected from the vector during inference, it is possible a spurious peak will also be chosen at the expense of a diﬀerent source.
We propose an alternative modeling approach for multi-source localization. In this model, the network is divided into two well-deﬁned components with speciﬁc functionalities as shown in Figure 2. The ﬁrst component is the source splitter, which dissects the input mixed signal to extract source speciﬁc spatial features. These features are passed to the second component named source dependent predictors, which gives individual posterior vectors as output. As multiple outputs are obtained, each one has a well deﬁned target with just one target angle corresponding to a speciﬁc source.
As we will have N output predictions corresponding to the N sources in our proposed model, there will be source permutation ambiguity in how we assign the predictions to the targets. We handle it in two ways. First we can use permutation invariant training (PIT) [36] that are popular for source separation models. In PIT, all possible prediction-target assignment permutations are considered a valid solution and the one with the minimum loss is chosen during training. Alternatively, we ﬁx the target angles to be in ascending order and force only this permutation.
Although the sources are stationary in existing models like [11], a frame level prediction is performed and then time averaging is used as a post processing step during inference. It is common in tasks like speaker identiﬁcation, which involve converting a sequence of features into a vector to incorporate average pooling inside the network [37, 38]. We follow this approach in all our models to avoid post-processing.
We describe the diﬀerent deep learning architectures used in this work. First we describe an architecture based on multi-label classiﬁcation. Then we introduce two diﬀerent architectures based on our proposed source splitting mechanism. Let Y1, Y2, · · · , YM be the M -channel input signal in the short-time Fourier transform (STFT) domain, with Ym ∈ CT ×F , where T is the number of frames and F is the number of frequency components. The input signal is reverberated, consisting of N speech sources. There can be additive stationary noise but we assume that there are no point source noises. We also assume that N is known. In this work the array geometry is considered to be known at training time and only the azimuth angle is estimated.
2.1. Multi-label Classiﬁcation (MLC)
This model is designed to take the raw multi-channel phase as the feature. Let the phase spectrum of the multichannel input signal be represented as P ∈ [0, 2π]T ×M×F . This raw phase P is passed through the ﬁrst component of the localization network based on a convolutional neural network (CNN) given by LocNet-CNN(·) to extract phase feature Z by pooling the channels as follows:
4

Z = LocNet-CNN(P) ∈ RT ×Q,

(1)

where Q is the feature dimension. The LocNet-CNN(·) architecture is inspired from [11], which uses convolutional ﬁlters across the microphone-channel dimension to learn phase diﬀerence like features.
The phase feature Z from Eq. (1) is passed through a feedforward layer as follows,

W = ReLU(AﬃneLayer1(Z)),

(2)

where W ∈ RT ×Q is an intermediate feature. We consider that the sources are stationary within an utterance. So in the next step a simple time average is performed to obtain a summary vector.

1T

ξ(q) =

w(t, q),

(3)

T t=1

where ξ(q) is the summary vector at dimension q, and w(t, q) is the intermediate feature at time t and feature dimension q. The summary vector, represented in vector form as ξ ∈ RQ is passed through a learnable AﬃneLayer(·) to convert its dimension to 360/γ , where γ is the angle resolution in degrees to
discretize the DOA angle.

κ = σ(AﬃneLayer2(ξ)).

(4)

(5)

where, σ(·) is the sigmoid activation, and κ ∈ (0, 1) 360/γ is the multi-label classiﬁcation vector. The DOAs can be estimated by ﬁnding the indices in κ corresponding to the N largest peaks.

2.2. Convolutional Mapping based Source Splitting Mechanism (Map-Split-C)
This architecture is a simple extension of the MLC model to achieve source splitting. Similar to the MLC model in Section 2.1 raw phase is used as the input and the phase feature Z is extracted using Eq. (1). Z will have DOA information about all the sources in the input signal. Source splitting is achieved through a source dependent aﬃne transformation as follows,

W n = ReLU(AﬃneLayer1n(Z)),

(6)

Note that the only diﬀerence from Eq. (1) is that the aﬃne layer is made to be dependent on the source n to achieve source splitting. Here, W n are intermediate representations speciﬁc to source n. These
representations are averaged to create the source speciﬁc summary vectors in the following way,

ξn(q) = 1 T wn(t, q), (7) T
t=1
where ξn(q) is the summary vector for source n at dimension q. The summary vector, represented in vector form as ξn ∈ RQ is passed through a learnable feedforward
layer AﬃneLayer2(·), which acts as the predictor and converts the summary vector from dimension Q to the dimension 360/γ , where γ is the angle resolution in degrees to discretize the DOA angle. Based on this discretization, we can predict the DOA angle as a classiﬁer with the softmax operation. From this, we can get the source-speciﬁc posterior probability for the possible angle classes as follows,
5

[Pr(θn = αi|P)]i=3610/γ = Softmax(AﬃneLayer2(ξn)),

(8)

αi = ((γ ∗ i) − ((γ − 1)/2)) (π/180) ,

(9)

Note that the parameters of the predictor are shared across the sources in Eq. (8) but it is also possible to make this feedforward layer source dependent. The estimated DOA θˆn for source n is determined by
ﬁnding the peak from the corresponding posterior in Eq. (8) as follows,

θˆn = argmax Pr(θn = αi|P),

(10)

αi

2.3. Masking based Source Splitting Mechanism (Mask-Split) This architecture is inspired from the localization subnetwork used in directional ASR [33]. This model
also uses raw phase as the input and the phase feature Z is extracted in the same way as Section 2.1 using Eq. (1). Z will have DOA information about all the sources in the input signal. It is processed by the next component LocNet-Mask(·) which consists of BLSTM layers. Source splitting is achieved through this component which extracts source-speciﬁc ratio masks as follows,

[W n]Nn=1 = σ(LocNet-Mask(Z)),

(11)

where W n ∈ [0, 1]T ×Q is the feature mask for source n and σ(·) is the sigmoid activation. This mask segments Z into regions that correspond to each source. Note that LocNet-Mask(·) can also implicitly perform voiced activity detection as it outputs ratio masks.
The extracted phase feature mask from Eq. (11) is used to perform a weighted averaging of the phase feature from Eq. (1) to get source-speciﬁc summary vectors. Here, source splitting is performed by masking instead of a direct mapping. Note that this masking is performed in a latent space and not in the frequency domain like done in cascaded systems like [24, 25]. The summary vector will encode the DOA information speciﬁc to a source as the masks are used as weights to summarize information only from the corresponding source regions in the following way,

n

Tt=1 wn(t, q)z(t, q)

ξ (q) =

T wn(t, q) ,

(12)

t=1

where ξn(q) is the summary vector for source n at dimension q, wn(t, q) ∈ [0, 1] and z(t, q) ∈ R are the

extracted feature mask (for source n) and the phase feature, respectively, at time t and feature dimension q.
The summary vector, represented in vector form as ξn ∈ RQ is passed through a learnable sourcespeciﬁc AﬃneLayern(·), which acts as the predictor and converts the summary vector from dimension Q to

the dimension 360/γ , where γ is the angle resolution in degrees to discretize the DOA angle. Based on

this discretization, we can predict the DOA angle as a classiﬁer with the softmax operation. From this, we

can get the source-speciﬁc posterior probability for the possible angle classes as follows,

[Pr(θn = αi|P)]i=3610/γ = Softmax(AﬃneLayern(ξn)),

(13)

αi = ((γ ∗ i) − ((γ − 1)/2)) (π/180) ,

(14)

The estimated DOA θˆn for source n is determined by ﬁnding the peak from the corresponding posterior in Eq. (13) as follows,

θˆn = argmax Pr(θn = αi|P),

(15)

αi

In this architecture the predictor parameters are not shared across the sources in Eq. (13) but it also possible to share them and an experimental comparison is discussed in Section 4.2.1.

6

2.4. Recurrent Mapping based Source Splitting Mechanism (Map-Split-R)
In this model, inter-microphone phase diﬀerence (IPD) [39, 40] is computed and passed as a feature directly to the model. There are no CNN layers in this model as pre-deﬁned features are used. This model also performs a direct mapping like Section 2.2. The IPD features are calculated as,

p (t, f ) = 1 [cos ∠( yi1 (t, f ) ) + j sin ∠( yi1 (t, f ) )], i ∈ {1, . . . , I},

(16)

i

M

yi (t, f )

yi (t, f )

2

2

where ym(t, f ) is the input signal at channel m, time t and frequency f , i represents an entry in a microphone pair list deﬁned for calculating the IPD; and i1 and i2 are the index of microphones in each pair. We calculate IPD features for I pairs and then concatenate their real and imaginary parts together. The concatenated IPD feature is represented as P ∈ RT ×I×2F . The magnitude of the input signal at channel 1 i.e. |Y1| is also added as a feature to give the ﬁnal input feature Z ∈ RT ×(2IF +F ). This feature is passed to a BLSTM
component LocNet-BLSTM(·), which is the source splitter, as follows,

[W n]Nn=1 = σ(LocNet-BLSTM(Z)), (17)
where W n ∈ RT ×Q is the source-speciﬁc feature for source n. This model achieves source splitting by a direct mapping so a simple average is used to extract the summary vector as follows,

ξn(q) = 1 T wn(t, q), (18) T
t=1
The summary vector is used similar to Section 2.3 and the source speciﬁc DOAs are obtained by following Eq. (13) and Eq. (15).
2.5. Loss Functions
The multi-label classiﬁcation model in Section 2.1 is trained with the binary cross entropy (BCE) loss. The models in Section 2.3 and Section 2.4 give source-speciﬁc probability distributions at the output. So cross entropy (CE) loss will be the natural choice. This choice will be good for a typical classiﬁcation problem where the inter-class relationship is not important. As we are estimating the azimuth angles, we have classes that are ordered. So it is better to optimize by taking the class relationships into account as they are very informative.
Typically a 1-hot vector is given as target to the CE loss. In DOA estimation it is better to predict a nearby angle compared to a far away angle. But the typical penalty from the CE loss doesn’t take this into account. A loss function that takes the class ordering into account is the earth mover distance (EMD). EMD is the minimal cost required to transform one distribution to another [41]. As in our problem the classes are naturally in a sorted order, we can use the simple closed-form solution from [42]. The solution is given by the squared error between the cumulative distribution function(CDF) of the prediction and the target probability distributions.
Another way to induce inter-class relationship is by using a soft target probability distribution which is not as sparse as the 1-hot vector. We use the following target distribution:

0.4 i = ψ 



0.2 i = (ψ ± 1) mod 360/γ

χ(i) =

(19)

0.1 i = (ψ ± 2) mod 360/γ





0 elsewhere

where, χ(i) is the probability weight of the target distribution for class i and ψ is the index corresponding to the target angle class. When this soft target distribution is used with CE, we deﬁne it as the soft cross
7

entropy (SCE) loss. Similarly, it can also be used with EMD and we deﬁne it as soft earth mover distance (SEMD) loss. By assigning some probability weight to the angle classes in the neighbourhood of the target, the network can be potentially made to learn some inter-class relationship.

3. Integration of Localization with ASR
MIMO-Speech [34, 35] is an end-to-end neural network that can perform simultaneous multi-talker speech recognition by taking the mixed speech multi-channel signal as the input. MIMO-Speech like typical ASR systems don’t try to explicitly localize the sources. In this section we propose an extension in which MIMOSpeech uses source localization as a frontend. The azimuth angles of the all the sources estimated using the frontend is converted to a feature suitable for ASR and passed to MIMO-Speech as a localization prior.
The MIMO-Speech network consists of three main components: (1) The masking subnetwork, (2) differentiable minimum variance distortionless response (MVDR) beamformer, and (3) the ASR subnetwork. The masking network in MIMO-Speech is monaural and gives channel-dependent time-frequency masks for beamforming. For this only the magnitude of the corresponding channel is given as input to the masking network. We will modify this masking network to take composite multi-channel input and output a channel-independent mask which will be shared across all channels.
Firstly, it is possible to also get a multi-channel masking network baseline by using Z from Section 2.4 as the composite feature which is the concatenation of IPD features from Eq. (16) and the magnitude of the ﬁrst channel. For this to be further extended to take localization knowledge we need to concatenate it with an additional input feature that can encode the estimated azimuth angle. The groundtruth DOA is encoded as angle features in [31, 32, 40]. We follow the same procedure but use the estimated DOA from our models proposed in Section 2.
The steering vector dn(f ) ∈ CM for source n and frequency f is calculated from estimated DOA θˆn. In this work we have used uniform circular arrays (UCA) and the steering vector is calculated as follows,

τmn = rc cos(θˆn − ψm), m = 1 : M dn(f ) = [ej2πfτ1n , ej2πfτ2n , ..., ej2πfτM n ],

(20) (21)

where τmn is the signed time delay between the m-th microphone and the center for source n, ψi is the angular location of microphone m, r is the radius of the UCA and c is the speed of sound (343 m/s).
The calculated steering vector is used to compute the angle features with the following steps,

a˜n(t, f ) = |dn(f )Hy(t, f )|2,

(22)

an(t, f ) = a˜n(t, f ) ∗ I(a˜n(t, f ) − a˜s(t, f ))s=1:N ,

(23)

where an(t, f ) is the angle feature for source n at time t and frequency f , H is conjugate transpose, y(t, f ) ∈ CM is the multichannel input, N is the number of speakers, and I(.) is the indicator function that outputs 0 if the input diﬀerence is negative for any of the “s = 1 : N ” cases and 1 otherwise.
The computed angle features for all the sources are concatenated and included to the composite feature
input list that is fed to the multi-channel masking network. This subnetwork given by MaskNet(·) gives the source speciﬁc masks Ln ∈ (0, 1)T ×F as the output. Rest of the procedure is similar to the original
MIMO-Speech model. The masks are used to compute the source speciﬁc spatial covariance matrix (SCM), Φn(f ) as follows:

Φn(f ) = 1 T ln(t, f )y(t, f )y(t, f )H, (24) Tt=1 ln(t, f ) t=1

8

The interference SCM, Φnintf (f ) for source n is approximated as Ni=n Φi(f ) like [34] (we experiment only with N = 2, so no summation in that case). From the computed SCMs, the M -dimensional complex
MVDR beamforming ﬁlter [43] for source n and frequency f , bn(f ) ∈ CM is estimated as,

bn(f ) = [Φnintf (f ) + Φnoise(f )]−1Φn(f ) u, Tr([Φnintf (f ) + Φnoise(f )]−1Φn(f ))

(25)

where u ∈ {0, 1}M is a one-hot vector in which the index corresponding to the reference microphone is 1, Tr(·) denotes the trace operation, and Φnoise(f ) is the noise SCM. The noise SCM can be estimated by obtaining an additional mask from the masking network. However we consider only stationary noise in this study and we experimentally found that it was better to ignore the noise SCM by considering it as an all-zero matrix.
With the estimated MVDR ﬁlter, we can perform speech separation to obtain the n-th separated STFT signal, xn(t, f ) ∈ C as follows:

xn(t, f ) = bn(f )Hy(t, f ),

(26)

This separated signal for source n, represented in matrix form as Xn ∈ CT ×F is transformed to a fea-

ture suitable for speech recognition by performing log Mel ﬁlterbank transformation and utterance based

mean-variance normalization (MVN). The extracted feature On for source n is passed to the speech recog-

nition

subnetwork

ASR(·)

to

get

Cn

=

(

c

n 1

,

cn2

,

·

·

·

),

the

token

sequence

corresponding

to

source

n.

The

MIMO-Speech network is optimized with the reference text transcriptions [Crief ]Ni=1 as the target. The joint

connectionist temporal classiﬁcation (CTC)/attention loss [44] is used as the ASR optimization criteria.

Here again there is permutation ambiguity as there are multiple output sequences. We can solve it in

two ways. First, we can follow the PIT scheme similar to the original MIMO-Speech model to resolve the

prediction-target token sequence assignment problem. This takes additional computation time. When the

PIT scheme is used, the order of the sources in the angle features in chosen randomly. We propose to use

the DOA knowledge to resolve the permutation ambiguity in the following way. During training stage, we

can use the groundtruth DOA as input instead of the estimated DOA for computing the angle features. We

determine the permutation of the target sequence based on the order of sources in which the angle features

are concatenated and thereby can eliminate PIT.

4. Experiments
4.1. Data & Setup
We simulated 2-speaker mixtures from WSJCAM0 [45]. A uniform circular array (UCA) was used. Two types of mixtures with diﬀerent array radius, 5cm (UCA-5) and 10cm (UCA-10) respectively, were simulated to study the impact of array aperture. For UCA-10, background noise was added from REVERB corpus [46] as it matches the array geometry used for recording that corpus and also to match the conditions of the real data used for evaluation. The signal-to-noise ratio (SNR) was uniformly sampled from the range U(10dB, 20dB). The 5k vocabulary subset of the real overlapped data with stationary sources from MC-WSJ-AV was used just for evaluation with the models trained with the UCA-10 mixtures. Since the aperture cannot be changed on the real data we also experiment with choosing only the ﬁrst three microphones of UCA-10. We call this a quadrant array (QA-10) in which the aperture is reduced as only the ﬁrst quadrant from the circle is used. For each utterance, we mixed another utterance from a diﬀerent speaker within the same set, so the resulting simulated data is the same size as the original clean data. The SMS-WSJ [47] toolkit was used for creating the simulated data with maximum overlap. Image method [48] was used to create the room impulse responses (RIR). Room conﬁgurations with the size (length-width-height) ranging from 5m-5m-2.6m to 11m-11m-3.4m were uniformly sampled while creating the mixtures. Both sources and the array are always chosen to be at the same height. The other conﬁguration details used for simulation are shown in Table 1. The source positions are randomly sampled and hence not constrained to be on a grid like [49].
9

Table 1: The conﬁgurations used for simulating the 2-speaker mixtures

Simulation corpus ASR pretraining corpus Sampling rate
Num. of utterances
Num. of RIR
T60 Num. of channels Source distance from array

WSJCAM0
WSJ
16 KHz
Train - 7860 Dev - 742 Eval - 1088
Train - 2620 Dev - 742 Eval - 1088
U(0.25s, 0.7s)
8
U(1m, 2m)

A 25 ms window and a 10 ms frame shift with a Hanning window were used to compute STFT. For the eight microphone conﬁguration (UCA-10 and UCA-5) the following pair list: (1, 5), (2, 6), (3, 7), (4, 8), (1, 3), (3,5), (5,7), and (7, 1) were used to compute the IPD features deﬁned in Eq (16). The pair list for the three microphone subset (QA-10) was: (1, 2), (2, 3), and (1, 3). Three CNN blocks with rectiﬁed linear unit (ReLU) activation after each block followed by a feedforward layer were used as LocNet-CNN(·) deﬁned in Eq. (1). The CNN ﬁlters are applied across the microphone-frequency dimensions with a stride of 1. The kernel shapes were dependent on the number of input microphones. In the case of 8-mics (UCA-10 and UCA-5), kernels of shapes, 4 × 1, 3 × 3, and 3 × 3 were used for the ﬁrst, second, and third CNN block respectively. For the 3-mic QA-10, kernels of shapes, 2 × 1, 2 × 3, and 1 × 3 were used for the ﬁrst, second, and third CNN block respectively. Padding was not performed for the channel dimension so as to fuse them after the ﬁnal CNN block. The number of feature maps were 4 in the ﬁrst block, 16 in the second block, and 32 in the ﬁnal block. Q was ﬁxed as “2 × 360/γ ”. One output gate projected bidirectional long short-term memory (BLSTMP) layer with Q cells was used as LocNet-Mask(·) deﬁned in Eq. (11). All the DNN based multi-source localization were optimized with Adam [50] for 50 epochs and a learning rate of 10−3.
The masking network for MIMO-Speech was designed with two BLSTMP layers with 771 cells. The encoder-decoder ASR network was based on the Transformer architecture [51] and it was initialized with a pretrained model that used single speaker training utterances from both WSJ0 and WSJ1. The same architecture as [35] was used for the encoder-decoder ASR model. It has 12 layers in the encoder and 6 layers in the decoder. Before the Transformer encoder, the log Mel ﬁlterbank features of 80 dimensions are encoded by two CNN blocks. The CNN layers have a kernel size of 3 × 3 and the number of feature maps is 64 in the ﬁrst block and 128 in the second block. Attention/CTC joint ASR decoding was performed with score combination with a word-level recurrent language model from [52] trained on the text data from WSJ. Our implementation was based on the Pytorch backend of ESPnet [53].
The input signal was preprocessed with weighted prediction error (WPE) [54, 55] based dereverberation with a ﬁlter order of 10 and prediction delay 3, only during inference time . The second channel was ﬁxed as the reference microphone for MVDR in Eq. (25). We evaluate several localization models with the proposed DOA integration with MIMO-Speech. To avoid retraining every time, the groundtruth DOA was used while training the network and DOA estimation was performed only while inference. While PIT training, the order of the angle features was randomly permuted. Subspace DOA estimation was performed with “Pyroomacoustics” toolkit [56].

10

4.2. Multi-Source Localization Performance
Table 2: Mean absolute error (MAE) in degrees on both our UCA-5 and UCA-10 simulated 2-source mixtures comparing our proposed source-splitting methods with subspace methods, and multi-label classiﬁcation. The symbol “” implies that the option is not used and “” implies that it is used.

Row ID

Method

UCA-5 (M=8)

UCA-10 (M=8)

γ

Loss

PIT No WPE

WPE

No WPE

WPE

Function

Dev Test Dev Test Dev Test Dev Test

1

MUSIC

1

2 MUSIC-NAM 1

3

TOPS

1

4

MLC

5

5

MLC

10

BCE BCE

6 Map-Split-C 10

CE

7

Mask-Split 10

CE

8

Mask-Split 1

CE

9

Mask-Split 1

SCE

10

Mask-Split 1

EMD

11

Mask-Split 1 SEMD

12

Mask-Split 1 SEMD

13 Map-Split-C 1 SEMD

14 Map-Split-C 1 SEMD

15 Map-Split-R 1 SEMD

16 Map-Split-R 1 SEMD

- 32.9 28.3 21.0 16.8 17.0 14.1 11.0 8.7

-

8.7 6.8 6.8 4.8 2.5 2.2 2.1 1.1

-

7.9 6.9 7.4 6.3 3.1 2.6 1.9 1.8

- 18.1 18.2 22.0 20.0 19.9 21.6 20.8 20.7

- 10.5 10.9 13.1 12.7 7.8 8.4 9.1 9.0

 4.6 4.8 4.0 3.9 5.3 5.8 5.3 6.2  2.9 3.3 3.1 3.2 4.0 4.7 3.3 3.4  28.7 27.0 21.0 19.2 32.9 34.1 28.9 27.4  1.6 1.9 2.4 2.0 3.7 3.6 3.6 3.8  4.7 4.4 4.8 4.7 3.2 3.3 3.6 3.6  3.0 3.0 4.0 3.9 2.8 2.6 2.8 2.9  1.4 1.5 1.4 1.4 1.5 1.4 1.1 1.1  3.7 4.2 3.8 4.1 3.6 4.0 3.2 3.8  3.8 4.5 3.6 4.3 4.0 4.1 3.6 3.9  2.7 2.9 2.8 3.1 2.3 2.2 2.2 2.5  2.0 2.1 2.2 2.2 2.1 2.0 2.3 2.1

Table 3: Results on the simulated quadrant array (QA-10) 2-source mixtures comparing our proposed source-splitting methods with subspace methods, and multi-label classiﬁcation. QA-10 corresponds to using only the ﬁrst three microphones of the eight microphone UCA-10. MAE in degrees is the metric. The signals were preprocessed with WPE to remove late reverberations. The symbol “” implies that the option is not used and “” implies that it is used.

Row ID Method

QA-10 (M=3) γ Loss Function PIT Dev Test

1

MUSIC

1

2

MUSIC-NAM 1

3

TOPS

1

4

MLC

10

5

Mask-Split 1

6

Map-Split-C 1

7

Map-Split-R 1

-
BCE
SEMD SEMD SEMD

- 58.7 57.7

- 22.7 21.7

- 10.6

9.7

- 10.1 11.9

 2.5

2.8

 4.0

4.4

 2.8

3.5

We use three popular subspace-based signal processing methods MUSIC, MUSIC with normalized arithmetic mean fusion (MUSIC-NAM) [57], and TOPS as baselines. Frequencies from 100 Hz to 8000 Hz were used for the subspace methods. For all three, the spatial response is computed and the top two peaks are detected to estimate the DOA. The average absolute cyclic angle diﬀerence between the predicted angle and the ground-truth angle in degrees is used as the metric. The permutation of the prediction with the reference that gives the minimum error is chosen.
The results with and without WPE preprocessing for both UCA-5 and UCA-10 mixtures are given in Table 2. Most of the supervised deep learning models with diﬀerent conﬁgurations perform signiﬁcantly
11

better than the subspace methods for UCA-5 but for UCA-10 both MUSIC-NAM and TOPS give very strong results. The results show that the deep learning methods are robust to reverberations and give good results even without the WPE frontend.
The results of the multi-label classiﬁcation model from Section 2.1 are given in rows 4 & 5. We can see that a higher resolution of γ = 5 degrades the performance from γ = 10. This is because of spurious peaks in nearby angles when the resolution is increased. This shows the need for having separate output vectors for diﬀerent sources. Rows 6-16 gives the results of the models with the proposed source splitting mechanism. The results are given with both PIT and also ﬁxing the targets in ascending order. From the results we can see that ﬁxing the target order works better.
Rows 8-12 use the Mask-Split model proposed in Section 2.3 with phase feature masking. The CE loss with γ = 10 (row 7) works reasonably well and gives an improvement over the multi-label classiﬁcation model. Increasing the resolution to γ = 1 (rows 8) makes it poor because of its inability to learn inter-class relationship like explained in Section 2.5. Making the target distribution smoother with SCE loss alleviates the problem quite well and we can observe better results in row 9. Using EMD loss also has a similar eﬀect (row 10). Combining both ideas with the SEMD loss gives the best performance with a prediction error of less than or equal to 1.5◦ when PIT is not used (row 12) for both array conﬁgurations. The results of both Map-Split models (row 13-row16) are a bit worse compared to the Mask-Split model and amongst them Map-Split-R works better.
The results on the quadrant array with WPE preprocessing are shown in Table 3. This again shows that subspace methods degrade severely when the aperture is reduced. Here the proposed methods signiﬁcantly outperform both subspace methods and MLC.
4.2.1. Predictor Sharing
Table 4: MAE (degree) on the simulated UCA-10 2-source mixtures demonstrating the eﬀect of sharing the predictor parameters across both the sources on the three diﬀerent proposed source-splitting methods. The signals were preprocessed with WPE to remove late reverberations. The symbol “” implies that the option is not used and “” implies that it is used.

Method
Map-Split-C Map-Split-C Map-Split-C Map-Split-C
Map-Split-R Map-Split-R Map-Split-R Map-Split-R
Mask-Split Mask-Split Mask-Split Mask-Split

Predictor Sharing
   
   
   

PIT
   
   
   

UCA-10 Dev Test
3.2 3.8 3.6 3.9 5.2 5.5 3.5 4.1
5.7 5.4 5.7 5.2 2.2 2.5 2.3 2.1
6.0 6.0 2.3 2.2 2.8 2.9 1.1 1.1

In Table 4, a comparison is made between sharing the parameters of the predictor (source independent) and a source dependent predictor for all the three proposed source splitting methods. The results are shown for UCA-10 but a similar trend was observed for the other array conﬁgurations too. We can see that having a source dependent predictor beneﬁts Mask-Split and Map-Split-R but not Map-Split-C. So by default MaskSplit and Map-Split-R make the predictor feedforward layer dependent on source n as given in Eq (13) but Map-Split-C makes this layer independent of the source as given in Eq (8).

12

4.2.2. Impact of SNR
Table 5: Results on the simulated UCA-10 2-source mixtures demonstrating the eﬀect of SNR. MAE in degrees is the metric. The signals were preprocessed with WPE to remove late reverberations. PIT was not used for the source splitting methods in these experiments.

Row ID

Method

UCA-10

γ

Loss

PIT SNR - 0dB SNR - 30dB

Function

Test

Test

1

MUSIC

1

-

-

14.2

15.6

2 MUSIC-NAM 1

-

-

1.3

1.1

3

TOPS

1

-

-

2.3

1.6

4

MLC

10 BCE

-

8.9

10.3

5

Mask-Split 1 SEMD



1.5

1.8

6 Map-Split-C 1 SEMD



6.0

4.0

7 Map-Split-R 1 SEMD



2.8

2.2

Two additional test sets were simulated for the UCA-10 conﬁguration by ﬁxing the SNR as 0 dB and 30 dB respectively to see the impact of SNR on DOA estimation. We can see from Table 5 that SNR doesn’t play a signiﬁcant role in the DOA estimation performance for most of the DOA estimation methods. Only, Map-Split-C seems to have a notable degradation for the challenging 0 db SNR scenario. The background noise that we used from REVERB [46] consists mostly of stationary noise so both the baselines and the proposed methods are quite robust to it.
4.2.3. Impact of Angular Distance between Sources on DOA Estimation
Table 6: Results on the simulated UCA-5 2-source mixtures demonstrating the impact of angular distance between the two sources. The results here are divided into four subsets based on the angle diﬀerence. MAE in degrees is the metric. The signals were preprocessed with WPE to remove late reverberations. PIT was not used for the source splitting methods in these experiments.

Row ID

Method

UCA-5

γ

Loss

PIT

Dev

Test

Function

10-20 21-45 46-90 91-180 10-20 21-45 46-90 91-180

1

MUSIC

1

-

2 MUSIC-NAM 1

-

3

TOPS

1

-

-

42.8 23.9 13.9

6.7

36.0 16.8 14.2

7.0

-

67.0

8.0

0.4

0.5

65.3

4.7

0.5

0.5

-

67.6

8.3

1.3

1.1

67.3

9.2

1.6

1.1

4

MLC

10 BCE

-

25.6 12.9 11.0

12.5

19.9 15.4

9.2

12.7

5

Mask-Split 1 SEMD



1.6

1.4

1.3

1.4

1.9

1.4

1.2

1.5

6 Map-Split-C 1 SEMD



5.5

4.6

3.3

3.2

5.8

5.0

4.7

3.8

7 Map-Split-R 1 SEMD



3.6

2.5

1.9

2.1

3.4

2.9

2.2

1.9

In [32, 40], it was shown that it is very challenging to separate sources when the angular distance between the sources is very small. The results in Table 2 for the UCA-5 array showed that the proposed source splitting method signiﬁcantly outperforms the subspace methods. In Table 6, we further investigate the results on the UCA-5 conﬁguration by checking the performance based on the angular distance between the two sources (speakers). Both the development and test sets were divided into four subsets based on the angle diﬀerence in degrees: (1) “10-20”, (2) “21-45”, (3) “46-90”, and (4) “91-180”. We can see the performance of all three subspace methods (rows 1-3) are very poor in the most challenging scenario of “10-20” and they also have a signiﬁcant degradation in the “21-45” cases. The MLC baseline (row 4) also degrages when the angle diﬀerence is “10-20” but it is relatively better than the subspace methods. The
13

proposed source splitting methods (rows 5-7) are very robust to the angular distance between the sources and amongst them Mask-Split is the most robust.
4.3. ASR Performance

Table 7: ASR performance on the simulated 2-speaker mixtures comparing our proposed DOA integration method with vanilla MIMO-Speech. The input signal was preprocessed with WPE to remove late reverberations. Word error rate (WER) is used as the metric for comparison. For WER, lower the better. The symbol “” implies that the option is not used and “” implies that it is used.

Clean Input mixture (ch-1) Oracle binary Mask (IBM) Baseline MIMO-Speech MIMO-Speech + Oracle Angle Feature
MIMO-Speech + DOA Estimation

Row ID
1 2
3
4 5
6 7
8 9 10 11 12 13 14 15 16 17

DOA Method
-
-
-
-
MUSIC MUSIC-NAM
TOPS MLC Map-Split-C Map-Split-R Map-Split-R Map-Split-R Mask-Split Mask-Split

DOA PIT
-
-
-
-
     

IPD for ASR
 

 
 
         

ASR PIT
 

 
 
         

UCA-5 Dev Test

13.7 111.6
6.6
14.0 10.4
6.0 6.3
16.9 12.0 12.6 13.1 7.8 6.9 6.6 6.8 6.1 6.3

13.6 115.6
6.1
12.0 9.2
5.7 5.6
14.2 9.7 11.3 11.3 7.7 6.2 6.2 6.1 5.9 6.0

UCA-10 Dev Test

13.7 112.1
7.1
15.1 13.1
6.8 6.6
12.9 8.6 8.0 11.6 9.1 7.3 7.3 7.2 7.0 7.0

13.6 116.3
6.5
13.6 11.5
6.0 6.2
11.5 7.4 7.2 11.5 8.6 6.8 6.7 6.9 6.3 6.3

QA-10 Dev Test

13.7 112.1
12.7
24.8 24.0
11.9 12.0
27.2 25.5 17.3 15.5 14.6 12.4 12.6 12.7 12.5 13.0

13.6 116.3
11.8
22.2 22.0
10.7 11.1
25.2 24.1 15.7 15.6 13.3 11.5 11.8 12.1 11.7 11.8

The speech recognition performance of our proposed DOA integration compared with the vanilla MIMOSpeech baseline is given in Table 7. Word error rate (WER) is used as the metric for ASR and the results are shown for mixtures from all three array conﬁgurations. The results of the clean single-speaker data with the pretrained ASR model is given in row 1. Note that the results of the clean single-speaker data is not very good because it is from the British English corpus WSJCAM0 and the pretrained model was trained with American English data from WSJ. This accent diﬀerence is ﬁxed in the mixed speech models as the ASR network will be ﬁne tuned in that case with British English data. The ASR results of the simulated mixture using the single-speaker model is also shown (WER more than 100% because of too many insertion errors).
Oracle experiment by directly giving the reference ideal binary masks (IBM) to the beamformer is given in row 3. Row 4 shows the results of the baseline MIMO-Speech with the architecture originally proposed in [35]. Row 5 shows the modiﬁed baseline by adding IPD features and a channel independent masking network as mentioned in Section 3. This makes the baseline stronger and gives slightly better results. Oracle ASRDOA integration experiments were performed by using the groundtruth DOA while inference as a proof of concept and the results are shown in rows 6 & 7. We can see a very signiﬁcant improvement in this case with the word error rates reduced by close to a factor of two. The results are also slightly better than using the oracle binary masks. This proves the importance of feeding localization knowledge to a multi-speaker ASR system. We can also see that turning oﬀ PIT and ﬁxing the target sequence order based on the DOA order gives similar performance (row 7) with added beneﬁts. This not only saves computation time while training but also makes the inference more informative by associating the DOA and its corresponding transcription.
In rows 8-17 some of the DOA estimation methods are used to compute the angle features. All the methods except MLC here are high resolution and use γ = 1. For MLC γ = 10 was used. We can see that using MUSIC, MUSIC-NAM, TOPS, or MLC degrades the ASR performance from the baseline for the lower aperture scenarios of UCA-5 and QA-10. This shows that localization knowledge will be useful only if we can estimate with good precision and reliability. The results with the proposed deep learning based
14

DOA estimation and SEMD loss are shown in rows 12-17. With any of these methods we get results close to using the oracle DOA. We can see the Mask-Split model which gives the best localization performance in Table 2 also gives the best results here (row 14). This result almost matches the performance obtained with oracle masks in row 3.
4.3.1. Real Data

Figure 3: Layout of MC-WSJ-AV

The overlapped data from MC-WSJ-AV corpus [58] was also used for ASR evaluation. The positions of the speakers are mentioned to be stationary throughout the utterance. A schematic diagram of the recording layout redrawn from [58] is shown in Figure 3. There are six possible positions in the room that the speakers were positioned. As there are two speakers in the mixture, there are ﬁfteen possible position pairs. The real data has recordings from two arrays with the same conﬁguration placed in diﬀerent positions. From the schematic diagram we can see that four of the six positions are very close to array-1 and all positions are generally far from Array-2. The model trained with the simulated UCA-10 and QA-10 mixtures are used here as they follow similar conﬁgurations. Although trained with simulated data, it is crucial to see how our methods work on real data. As there are no groundtruth DOA labels, we cannot evaluate the angle prediction error.
Table 8: ASR performance on the real 2-speaker overlapped data from MC-WSJ-AV. The input signal was preprocessed with WPE to remove late reverberations. Word error rate (WER) is used as the metric for comparison. For WER, lower the better.

MIMO-Speech MIMO-Speech w/ IPD
MIMO-Speech + DOA Integration MIMO-Speech + DOA Integration
MIMO-Speech + DOA Integration MIMO-Speech + DOA Integration MIMO-Speech + DOA Integration

DOA Method
N/A N/A
MUSIC-NAM TOPS
Mask-Split Map-Split-R Map-Split-R (chunking)

UCA-10 (M=8) Array-1 Array-2

34.3 20.6
10.7 10.6
14.0 14.8 12.1

47.6 30.7
17.1 19.2
18.2 22.3 17.3

QA-10 (M=3) Array-1 Array-2

41.6 48.1
36.4 20.0
18.0 21.2 20.7

55.7 63.1
46.8 33.4
28.7 25.6 25.3

15

The ASR performance is shown in Table 8. The DOA estimation here is performed with γ = 1 and the source splitting models here are without PIT and with SEMD loss. We also experiment with a chunking scheme here for Map-Split-R. Here we splice the input audio into 100 ms chunks with 50% overlap. DOA estimation is performed on each of these chunks and their median output is chosen as the estimated DOA. This was done to remove the eﬀect of some bad chunks in the estimation process.
Using estimated DOAs from either the subspace methods or the source splitting models, ASR-DOA integration signiﬁcantly outperforms the MIMO-Speech baselines for this data. This further proves the importance of localization as a very pivotal frontend for ASR. As positions of the sources in this dataset were chosen to be favorable to array-1 it gives better results. For UCA-10, there is not much diﬀerence between the subspace methods and the proposed source splitting methods but for QA-10, the proposed method outperforms and the margin is signiﬁcant for array-2. This again shows the eﬀectiveness of the source splitting approach on challenging conditions and lower aperture.
5. Conclusion & Future Work
We proposed a novel deep learning based model for multi-source localization that can classify DOAs with a high resolution. An extensive evaluation was performed with diﬀerent choices of architectures, loss functions, classiﬁcation resolution, and training schemes that handle permutation ambiguity. Our proposed source splitting model was shown to have a signiﬁcantly lower prediction error compared to a multi-label classiﬁcation model. The proposed method also outperforms well known subspace methods. We also proposed a soft earth mover distance (SEMD) loss function for the localization task that models inter-class relationship well for DOA estimation and hence predicts near perfect DOA.
We have also devised a method to use the proposed DOA estimation as a frontend for multi-talker ASR. This integration greatly helps speech recognition and shows the importance of using localization priors with far-ﬁeld ASR models. Based on this ﬁnding, ASR performance was used as the metric of evaluation for DOA estimation in real data where DOA labels are not available to compute prediction errors.
In future, we would like to extend our methods to also handle source counting within the model to make it work for arbitrary number of sources. One possible approach would be to use a conditional chain model that is popular for source separation [59, 60]. The other important extension is adapting our method to work on more challenging and realistic data like CHiME-6 [61] which involves a complicated distributed array setup with moving sources.
References
[1] K. Nakadai, T. Takahashi, H. Okuno, H. Nakajima, Y. Hasegawa, H. Tsujino, Design and implementation of robot audition system ‘HARK’ - open source software for listening to three simultaneous speakers, Advanced Robotics 24 (5-6) (2010) 739–761.
[2] K. Nakadai, K. Hidai, H. Mizoguchi, H. Okuno, H. Kitano, Real-time auditory and visual multiple-object tracking for humanoids, in: International Joint Conferences on Artiﬁcial Intelligence (IJCAI), 2001, pp. 1425–1432.
[3] T. Yoshioka, I. Abramovski, C. Aksoylar, Z. Chen, M. David, D. Dimitriadis, Y. Gong, I. Gurvich, X. Huang, Y. Huang, A. Hurvitz, L. Jiang, S. Koubi, E. Krupka, I. Leichter, C. Liu, P. Parthasarathy, A. Vinnikov, L. Wu, X. Xiao, W. Xiong, H. Wang, Z. Wang, J. Zhang, Y. Zhao, T. Zhou, Advances in online audio-visual meeting transcription, in: IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), 2019, pp. 276–283.
[4] J. Barker, S. Watanabe, E. Vincent, J. Trmal, The ﬁfth CHiME speech separation and recognition challenge: Dataset, task and baselines, in: Interspeech, 2018, pp. 1561–1565.
[5] R. Haeb-Umbach, S. Watanabe, T. Nakatani, M. Bacchiani, B. Hoﬀmeister, M. L. Seltzer, H. Zen, M. Souden, Speech processing for digital home assistants: Combining signal processing with deep-learning techniques, IEEE Signal processing magazine 36 (6) (2019) 111–124.
[6] R. Schmidt, Multiple emitter location and signal parameter estimation, IEEE transactions on antennas and propagation 34 (3) (1986) 276–280.
[7] R. Roy, T. Kailath, ESPRIT-estimation of signal parameters via rotational invariance techniques, IEEE Transactions on Acoustics, Speech, and Signal Processing 37 (7) (1989) 984–995.
[8] E. D. Di Claudio, R. Parisi, WAVES: weighted average of signal subspaces for robust wideband direction ﬁnding, IEEE Transactions on Signal Processing 49 (10) (2001) 2179–2191.
[9] Yeo-Sun Yoon, L. M. Kaplan, J. H. McClellan, TOPS: new DOA estimator for wideband signals, IEEE Transactions on Signal Processing 54 (6) (2006) 1977–1989.
16

[10] J. DiBiase, A high-accuracy, low-latency technique for talker localization in reverberant environments using microphone arrays, PhD Thesis, Brown University.
[11] S. Chakrabarty, E. A. Habets, Multi-speaker DOA estimation using deep convolutional networks trained with noise signals, IEEE Journal of Selected Topics in Signal Processing 13 (1) (2019) 8–21.
[12] T. Hirvonen, Classiﬁcation of spatial audio location and content using convolutional neural networks, in: 138th Audio Engineering Society Convention, Vol. 2, 2015.
[13] R. Takeda, K. Komatani, Sound source localization based on deep neural networks with directional activate function exploiting phase information, in: IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 2016, pp. 405–409.
[14] F. Vesperini, P. Vecchiotti, E. Principi, S. Squartini, F. Piazza, A neural network based algorithm for speaker localization in a multi-room environment, in: 26th IEEE International Workshop on Machine Learning for Signal Processing (MLSP), 2016, pp. 1–6.
[15] N. Yalta, K. Nakadai, T. Ogata, Sound source localization using deep learning models, Journal of Robotics and Mechatronics 29 (2017) 37–48.
[16] S. Chakrabarty, E. A. Habets, Broadband DOA estimation using convolutional neural networks trained with noise signals, in: IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA), 2017, pp. 136–140.
[17] L. Perotin, A. D´efossez, E. Vincent, R. Serizel, A. Gu´erin, Regression versus classiﬁcation for neural network based audio source localization, in: IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA), 2019, pp. 343–347.
[18] S. Adavanne, A. Politis, J. Nikunen, T. Virtanen, Sound event localization and detection of overlapping sources using convolutional recurrent neural networks, IEEE Journal of Selected Topics in Signal Processing 13 (1) (2018) 34–48.
[19] K. Shimada, Y. Koyama, N. Takahashi, S. Takahashi, Y. Mitsufuji, ACCDOA: Activity-coupled cartesian direction of arrival representation for sound event localization and detection, in: IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2021, pp. 915–919.
[20] A. Narayanan, D. Wang, Ideal ratio mask estimation using deep neural networks for robust speech recognition, in: IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2013, pp. 7092–7096.
[21] Y. Wang, A. Narayanan, D. Wang, On training targets for supervised speech separation, IEEE/ACM Transactions on Audio, Speech, and Language Processing 22 (12) (2014) 1849–1858.
[22] W. Zhang, Y. Zhou, Y. Qian, Robust DOA estimation based on convolutional neural network and time-frequency masking, in: Interspeech, 2019, pp. 2703–2707.
[23] W. Mack, U. Bharadwaj, S. Chakrabarty, E. A. P. Habets, Signal-aware broadband DOA estimation using attention mechanisms, in: IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2020, pp. 4930– 4934.
[24] Z.-Q. Wang, X. Zhang, D. Wang, Robust speaker localization guided by deep learning-based time-frequency masking, IEEE/ACM Transactions on Audio, Speech, and Language Processing 27 (1) (2019) 178–188.
[25] Z. Wang, J. Li, Y. Yan, Target speaker localization based on the complex watson mixture model and time-frequency selection neural network, Applied Sciences 8 (11).
[26] S. Sivasankaran, E. Vincent, D. Fohr, Keyword based speaker localization: Localizing a target speaker in a multi-speaker environment, in: Interspeech, 2018, pp. 2703–2707.
[27] M. Delcroix, K. Zmolikova, K. Kinoshita, A. Ogawa, T. Nakatani, Single channel target speaker extraction and recognition with speaker beam, in: IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2018, pp. 5554–5558.
[28] H. W. L¨ollmann, C. Evers, A. Schmidt, H. Mellmann, H. Barfuss, P. A. Naylor, W. Kellermann, The locata Challenge data corpus for acoustic source localization and tracking, in: IEEE 10th Sensor Array and Multichannel Signal Processing Workshop, 2018, pp. 410–414.
[29] S. Adavanne, A. Politis, T. Virtanen, Direction of arrival estimation for multiple sound sources using convolutional recurrent neural network, in: European Signal Processing Conference (EUSIPCO), 2018, pp. 1462–1466.
[30] W. He, P. Motlicek, J. Odobez, Deep neural networks for multiple speaker detection and localization, in: IEEE International Conference on Robotics and Automation (ICRA), 2018, pp. 74–79.
[31] Z. Chen, X. Xiao, T. Yoshioka, H. Erdogan, J. Li, Y. Gong, Multi-channel overlapped speech recognition with location guided speech extraction network, in: IEEE Spoken Language Technology Workshop (SLT), 2018, pp. 558–565.
[32] A. S. Subramanian, C. Weng, M. Yu, S. Zhang, Y. Xu, S. Watanabe, D. Yu, Far-ﬁeld location guided target speech extraction using end-to-end speech recognition objectives, in: IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 2020, pp. 7299–7303.
[33] A. S. Subramanian, C. Weng, S. Watanabe, M. Yu, Y. Xu, S.-X. Zhang, D. Yu, Directional ASR: A new paradigm for e2e multi-speaker speech recognition with source localization, in: IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2021, pp. 8433–8437.
[34] X. Chang, W. Zhang, Y. Qian, J. Le Roux, S. Watanabe, MIMO-Speech: End-to-end multi-channel multi-speaker speech recognition, in: IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), 2019, pp. 237–244.
[35] X. Chang, W. Zhang, Y. Qian, J. Le Roux, S. Watanabe, End-to-end multi-speaker speech recognition with transformer, in: IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 2020, pp. 6134–6138.
[36] D. Yu, M. Kolbæk, Z. Tan, J. Jensen, Permutation invariant training of deep models for speaker-independent multi-talker speech separation, in: IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 2017, pp. 241–245.
[37] D. Snyder, D. Garcia-Romero, G. Sell, D. Povey, S. Khudanpur, X-Vectors: Robust DNN embeddings for speaker recog-
17

nition, in: IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 2018, pp. 5329–5333. [38] K. Vesely´, S. Watanabe, K. Zˇmol´ıkova´, M. Karaﬁ´at, L. Burget, J. H. Cˇ ernocky´, Sequence summarizing neural network
for speaker adaptation, in: IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 2016, pp. 5315–5319. [39] Z.-Q. Wang, J. Le Roux, J. R. Hershey, Multi-channel deep clustering: Discriminative spectral and spatial embeddings for speaker-independent speech separation, in: IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2018, pp. 1–5. [40] F. Bahmaninezhad, J. Wu, R. Gu, S.-X. Zhang, Y. Xu, M. Yu, D. Yu, A comprehensive study of speech separation: Spectrogram vs waveform separation, in: Interspeech, 2019, pp. 4574–4578. [41] E. Levina, P. Bickel, The earth mover’s distance is the mallows distance: some insights from statistics, in: IEEE International Conference on Computer Vision (ICCV), Vol. 2, 2001, pp. 251–256. [42] L. Hou, C.-P. Yu, D. Samaras, Squared earth mover’s distance-based loss for training deep neural networks, NeurIPS Workshop - Learning on Distributions, Functions, Graphs and Groups. [43] M. Souden, J. Benesty, S. Aﬀes, On optimal frequency-domain multichannel linear ﬁltering for noise reduction, IEEE Transactions on Audio, Speech, and Language Processing 18 (2) (2010) 260–276. [44] S. Kim, T. Hori, S. Watanabe, Joint CTC-attention based end-to-end speech recognition using multi-task learning, in: IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 2017, pp. 4835–4839. [45] T. Robinson, J. Fransen, D. Pye, J. Foote, S. Renals, WSJCAMO: a British English speech corpus for large vocabulary continuous speech recognition, in: IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), Vol. 1, 1995, pp. 81–84. [46] K. Kinoshita, M. Delcroix, S. Gannot, E. A. Habets, R. Haeb-Umbach, W. Kellermann, V. Leutnant, R. Maas, T. Nakatani, B. Raj, et al., A summary of the REVERB challenge: state-of-the-art and remaining challenges in reverberant speech processing research, EURASIP Journal on Advances in Signal Processing. [47] L. Drude, J. Heitkaemper, C. Boeddeker, R. Haeb-Umbach, SMS-WSJ: Database, performance measures, and baseline recipe for multi-channel source separation and recognition, arXiv preprint arXiv:1910.13934. [48] J. B. Allen, D. A. Berkley, Image method for eﬃciently simulating small-room acoustics, The Journal of the Acoustical Society of America 65 (4) (1979) 943–950. [49] S. Chakrabarty, E. A. Habets, Multi-speaker localization using convolutional neural network trained with noise, arXiv preprint arXiv:1712.04276. [50] D. P. Kingma, J. Ba, Adam: A method for stochastic optimization, in: International Conference on Learning Representations (ICLR), 2015. [51] S. Karita, N. Chen, T. Hayashi, T. Hori, H. Inaguma, Z. Jiang, M. Someki, N. E. Y. Soplin, R. Yamamoto, X. Wang, S. Watanabe, T. Yoshimura, W. Zhang, A comparative study on Transformer vs RNN in speech applications, in: IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), 2019, pp. 449–456. [52] T. Hori, J. Cho, S. Watanabe, End-to-end speech recognition with word-based RNN language models, in: IEEE Spoken Language Technology Workshop (SLT), 2018, pp. 389–396. [53] S. Watanabe, T. Hori, S. Karita, T. Hayashi, J. Nishitoba, Y. Unno, N. Enrique Yalta Soplin, J. Heymann, M. Wiesner, N. Chen, A. Renduchintala, T. Ochiai, ESPnet: End-to-end speech processing toolkit, in: Interspeech, 2018, pp. 2207– 2211. [54] T. Nakatani, T. Yoshioka, K. Kinoshita, M. Miyoshi, B. Juang, Speech dereverberation based on variance-normalized delayed linear prediction, IEEE Transactions on Audio, Speech, and Language Processing 18 (7) (2010) 1717–1731. [55] L. Drude, J. Heymann, C. Boeddeker, R. Haeb-Umbach, NARA-WPE: A Python package for weighted prediction error dereverberation in Numpy and Tensorﬂow for online and oﬄine processing, in: ITG Fachtagung Sprachkommunikation, 2018. [56] R. Scheibler, E. Bezzam, I. Dokmani´c, Pyroomacoustics: A python package for audio room simulation and array processing algorithms, in: IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 2018, pp. 351–355. [57] D. Salvati, C. Drioli, G. L. Foresti, Incoherent frequency fusion for broadband steered response power algorithms in noisy environments, IEEE Signal Processing Letters 21 (5) (2014) 581–585. [58] M. Lincoln, I. McCowan, J. Vepa, H. K. Maganti, The multi-channel wall street journal audio visual corpus (MC-WSJ-AV): speciﬁcation and initial experiments, in: IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU), 2005, pp. 357–362. [59] K. Kinoshita, L. Drude, M. Delcroix, T. Nakatani, Listening to each speaker one by one with recurrent selective hearing networks, in: IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 2018, pp. 5064–5068. [60] J. Shi, X. Chang, P. Guo, S. Watanabe, Y. Fujita, J. Xu, B. Xu, L. Xie, Sequence to multi-sequence learning via conditional chain mapping for mixture signals, in: Advances in Neural Information Processing Systems, 2020, pp. 3735–3747. [61] S. Watanabe, M. Mandel, J. Barker, E. Vincent, A. Arora, X. Chang, S. Khudanpur, V. Manohar, D. Povey, D. Raj, D. Snyder, A. S. Subramanian, J. Trmal, B. B. Yair, C. Boeddeker, Z. Ni, Y. Fujita, S. Horiguchi, N. Kanda, T. Yoshioka, N. Ryant, CHiME-6 challenge: Tackling multispeaker speech recognition for unsegmented recordings, in: 6th International Workshop on Speech Processing in Everyday Environments, 2020.
18

