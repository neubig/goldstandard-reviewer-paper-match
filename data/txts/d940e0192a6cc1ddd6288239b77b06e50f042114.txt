AN EXPLORATION OF SELF-SUPERVISED PRETRAINED REPRESENTATIONS FOR END-TO-END SPEECH RECOGNITION
Xuankai Chang1, Takashi Maekaku2∗, Pengcheng Guo3∗, Jing Shi4∗, Yen-Ju Lu5, Aswin Shanmugam Subramanian6, Tianzi Wang6, Shu-wen Yang7,
Yu Tsao5, Hung-yi Lee7, Shinji Watanabe1
1 Carnegie Mellon University, 2 Yahoo Japan Corporation, 3 Northwestern Polytechnical University 4 Institute of Automation, Chinese Academy of Sciences, 5 Academia Sinica, 6 Johns Hopkins University
7 National Taiwan University

arXiv:2110.04590v1 [cs.CL] 9 Oct 2021

ABSTRACT
Self-supervised pretraining on speech data has achieved a lot of progress. High-ﬁdelity representation of the speech signal is learned from a lot of untranscribed data and shows promising performance. Recently, there are several works focusing on evaluating the quality of self-supervised pretrained representations on various tasks without domain restriction, e.g. SUPERB. However, such evaluations do not provide a comprehensive comparison among many ASR benchmark corpora. In this paper, we focus on the general applications of pretrained speech representations, on advanced end-to-end automatic speech recognition (E2E-ASR) models. We select several pretrained speech representations and present the experimental results on various open-source and publicly available corpora for E2E-ASR. Without any modiﬁcation of the back-end model architectures or training strategy, some of the experiments with pretrained representations, e.g., WSJ, WSJ0-2mix with HuBERT, reach or outperform current state-of-the-art (SOTA) recognition performance. Moreover, we further explore more scenarios for whether the pretraining representations are effective, such as the cross-language or overlapped speech. The scripts, conﬁguratons and the trained models have been released in ESPnet to let the community reproduce our experiments and improve them.
Index Terms— Representation Learning, End-to-End Speech Recognition, ESPnet
1. INTRODUCTION
The performance of speech recognition systems have been improved a lot over the last decade. On the one hand, the rapid development of deep neural networks has dramatically pushed the limit of the models [1–6]. On the other hand, the increasing computing resources have enabled to train an automatic speech recognition (ASR) system with a large amount of transcribed data [7,8], leading to a better performance. It is known that the deep neural networks are data hungry, thus some researchers have been trying to improve the capacity of neural networks by incorporating more and more transcribed data [9]. However, using the transcribed data only is not efﬁcient because the untranscribed data is of great portion in all the data available. Motivated by this, researchers proposed to make use of the untranscribed data, known as unsupervised and semi-supervised learning. Recently, it has become a hot topic in speech recognition and can be roughly divided into two approaches. In [10–12],
∗Equal contribution.

a semi-supervised method, called pseudo-labelling, was proposed to use both transcribed and untranscribed data. A seed model is ﬁrst trained with the transcribed data in a supervised manner, which is then used to generate the pseudo-labels for the untranscribed data. After that, a model can be trained with all the data in a supervised manner.
Previous studies in computer vision (CV) and natural language processing (NLP) have investigated to learn representations from data, showing the advantages in the corresponding downstream tasks [13–15]. Similarly, another approach was proposed to pretrain models using a large amount of untranscribed data for learning high quality speech features. In this context, many pretrained speech representation models have been proposed, which are often referred as self-supervised learning representation (SSLR). These SSLRs can be categorized by their training objectives. To be speciﬁc, one direct way to learn the speech representations is to predict the future information given the history information. In [16, 17], the authors adopted an method similar to the autoregressive language models (LMs) to predict the future acoustic features (e.g. FBANK) conditioned on the past input features, called autoregressive predictive coding (APC). Instead of autoregressive modeling, some researchers proposed to use masking prediction techniques as in BERT-LM [14] to learn the speech representations, including Mockingjay [18], TERA [19] and NPC [20]. However, it is not necessary to learn the speech representations by reconstructing the acoustic features. In [21, 22], the models were optimized with a contrastive loss to distinguish the positive sample from negative samples in predictions of future. Later in [23, 24], a BERT Transformer model is concatenated after the encoder trained by the contrastive loss. Recently, a novel model, called HuBERT [25], was proposed to pretrain the representation model by a classiﬁcation tasks using pseudo-labels motivated by deep cluster models [26, 27].
All the proposed representations have shown promising results, however, we can hardly draw a conclusion about a suitable representation for various tasks because their experiments focused on a limited number of tasks and were performed independently. Recently, a benchmark, called Speech processing Universal PERformance Benchmark (SUPERB) [28], was proposed to provide a fair and standard evaluation of various speech representations, with a uniﬁed toolkit, S3PRL. SUPERB focuses on the shallow information of each representation. During evaluation, all the representation models are frozen and applied on several downstream tasks each of which uses a quite light-weight downstream model. For example, the ASR task is evaluated using a two-layer RNN-based connection-

ist temporal classiﬁcation (CTC) model. Such evaluation provides informative clues to compare the capacity and the concentration of information for each SSLR. Besides, it prepares the easy access to a lot of pretrained SSLR models. Thus, SUPERB is a very strong benchmark for evaluating the SSLRs without any doubt.
With that being said, it still remains a question that how well these SSLRs can perform in the advanced speech recognition systems. In this paper, we investigate the performance of end-to-end ASR (E2E-ASR) systems using the pretrained SSLRs. To achieve this, we incorporated the SSLRs from S3PRL, the toolkit used in SUPERB, to the ESPnet [29], a widely used E2E speech processing toolkit. Thus, we can easily evaluate the E2E-ASR performance of pretrained SSLRs available in S3PRL using the current state-ofthe-art (SOTA) neural network models, such as Transformers [4, 5] and Conformers [6]. We can also easily evalute the SSLRs in other downstream tasks, including speech translation (ST) [30] and speech enhancement (SE) [31]. It is also an interesting question in the air about the generalization ability of these SSLRs, given the fact that most of SSLRs were trained and tested mainly on LibriSpeech [7, 32]. In this project, we explored these pretrained SSLRs on various open-source and publicly available corpora as many as possible, considering different characteristics including read vs. spontaneous speech, single-speaker vs. multi-speaker, noisy/distant-talk environments, and the telephone channel. We show that some of the SSLRs can achieve much better results than the commonly used log-Mel Filterbank (FBANK) feature.
The contributions of this study include:
• We implement the use of pretrained SSLRs in advanced E2EASR models, based on which we compare the performance of different representations.
• The experimental results show that simply replacing the FBANK features with the SSLRs can surpass our current best E2E-ASR system. For some ASR benchmark corpora, our results get competitive results with the SOTA systems, such as WSJ, LibriSpeech and TEDLIUM2.
• We explore more scenarios with domain-mismatch from the raw speech data to train the pretraining representations. Some observations show the relationship between pretraining representations and their applicable scenarios.
• We provide reproducible benchmark results, recipes, setups and well-trained models on several publicly available corpora in our open source toolkit ESPnet.
2. END-TO-END ASR
In this section, we will brieﬂy describe the E2E-ASR models used in this paper, including the CTC and the attention-based encoderdecoder (AED) framework.
2.1. CTC
Giving an input speech feature X = (x1, . . . , xT ), where T means the number of frames, and the corresponding output label sequence Y = (y1, . . . , yU ), where yu ∈ V and V is a vocabulary, CTC [33] was proposed to estimate a frame-level input-output alignment π = (π1, . . . , πT ) by allowing repetitions of labels or emitting a special blank label , i.e., πt ∈ V { }. The CTC optimizes the model to maximize the probability distribution P (Y|X) over all possible

alignments, which can be formulated as:

PCTC(Y|X) =

P (π|X),

(1)

π∈Φ(Y)

where Φ(Y) refers all possible alignments of Y.

2.2. Joint CTC/Attention-based Encoder-Decoder
AED model directly maps an input speech feature into an label sequence of words or characters without any intermediate representations. It models the distribution of each label by conditioning on both previous estimated labels and the input feature as:

PAttn(Y|X) = P (yu|X, y1:u−1).

(2)

u

For better performance, we adopted the joint CTC/attentionbased encoder-decoder architecture [34] in this work. The loss is deﬁned as the sum of the negative log-likelihoods of CTC and AED:

L = − [λ ln PCTC(Y|X) + (1 − λ) ln PAttn(Y|X)] , (3)

where λ ∈ [0, 1] is a tunable hyper-parameter. In this work, we majorly use Transformer [4] and Conformer [6]
as the basic block to build our E2E-ASR models.

2.2.1. Transformer
Transformer [4] was proposed by Vaswani et al. and has become the dominant model in various E2E-ASR tasks [5, 35]. Both the encoder and decoder are multi-blocked architectures and each block is stacked by a multiheaded self-attention (MHSA) module and a positionwise feed-forward (FFN) module. For the decoder, another source-target attention module is inserted after the MHSA module to joint model the acoustic and linguistic information.

2.2.2. Conformer
In [6], Gulatiet et al. proposed a novel architecture with combination of self-attention and convolution in ASR models, which is named Conformer encoder. Speciﬁcally, it includes a MHSA module, a convolution (CONV) module, and a pair of FNN modules in the Macaron-Net style. While the MHSA learns the global context, the CONV module efﬁciently captures the local correlations synchronously. In this work, our model follows the same setups as in [36], which integrates the Conformer encoder with a normal Transformer decoder.

3. SPEECH REPRESENTATIONS FOR ASR
Basically, raw speech signal is much less efﬁcient in conveying information than text. Thus for ASR, speech representation extraction is an important module to condense the information of speech signal. Traditionally, many handcrafted features, such as log-Mel Filterbank (FBANK), Mel Frequency Cepstral Coefﬁcients (MFCCs), are used in ASR tasks. There are many SSLRs proposed in previous studies. In this project, we cover the following 8 typical methods to perform the evaluation, including APC, CPC, HuBERT, Mockingjay, NPC, TERA, VQ-APC, Wav2Vec2.01.
1We only evaluated Wav2Vec2.0 among the Wav2Vec series because we believe it can represent the best of Wav2Vec techniques.

Table 1. Information summary of the SSLRs used in this paper, including the data used in pretaining (Data), model architecture (Arch), number of parameters (#Params) and the stride of the features.

SSLR

Objective

Data

autoregressive masking contrastive pseudo-labelling

Arch

#Params Stride

APC



CPC

HuBERT

Mockingjay

NPC

TERA

VQ-APC



Wav2Vec2.0


  


LibriSpeech 960hr

3-GRU

4.1M 10ms

LibriLight 60K

5-Conv, 1-LSTM

1.8M 10ms



LibriLight 60K

7-Conv 24-Trans

316.2M 20ms

LibriSpeech 960hr

3-Trans

21.3M 10ms

LibriSpeech 960hr 4-Conv, 4-Masked Conv 19.4M 10ms

LibriSpeech 960hr

3-Trans

23.3M 10ms

LibriSpeech 960hr

3-GRU

4.6M 10ms

LibriSpeech 960hr

7-Conv 24-Trans

317.4M 20ms

3.1. Autoregressive-prediction based representations
One type of representation learning method is to learn to predict the future acoustic features given the past. APC. Autoregressive Predictive Coding (APC) was proposed in [16]. The idea comes from autoregressive LMs for text, which is typically a probability distribution over token sequences. APC uses an RNN for modeling the temporal information of raw speech signals to predict the features of the frame K steps ahead. The model is optimized by minimizing the L1 loss between the input speech signal and the predicted sequence. VQ-APC. VQ-APC [17] is a variant of APC that incorporates vector quantization (VQ) layers. A VQ layer in the middle of the APC network is used to control the amount of information from the previous layer. Therefore, the model is forced to learn better representations to predict future frames.
3.2. Masking-prediction based representations
Speech signal is context dependent. Using both the past and future information when learning the speech representation can be useful. Some researchers proposed to generate the current frame conditioned on both the past and future information. Mockingjay. Similar to BERT [14], the model is trained by recover the masked input features based on its left and right context frames. During training, a certain amount of input frames are selected to be dynamically masked by zero-masking, random value ﬁlling or leaving unchanged. The model is optimized by minimizing the reconstruction loss between the original feature sequence and the prediction using L1 loss. To avoid the model learning the local smoothness, the frames being masked are chosen as a consecutive frame subsequence. The MFCC feature is used as the acoustic features. A Transformer with multi-headed self-attention is used as the model. On top of it, a two-layer feed-forward network was used to predict the original feature. TERA. Transformer encoder representation from alteration (TERA) [19] is an extension of Mockingjay. Similar to Mockingjay, the model takes the manipulated acoustic features as input and minimizes the difference between the ground-truth and the prediction at masked portion of the input. In addition to the masking process along time axis used in Mockingjay, TERA employs two more alterations, including the masking process in frequency axis and the magnitude alterations by adding random noise. NPC. Nonautoregressive predictive coding (NPC) [20] shares the similar principle with Mockingjay and TERA. For NPC, it uses the convolutional neural networks. The model generates the acoustic feature at each time step t conditioned on both the future and past information within a certain range. To avoid the local smoothness

problem, the nearest frames within m steps are masked out. Thus, the input of the model is in the range [t − r, t + r] − [t − m, t + m].
3.3. Contrastive learning based representations
CPC. Instead of using a conditional generative model to reconstruct the original input signal, the contrastive predictive coding (CPC) [21] model learns the representation via maximizing the mutual information between the current context and future embeddings by minimizing the noise-contrastive estimation-based (NCE) loss [37]. We use an updated version of CPC model, called modiﬁed-CPC [38]. The model contains a 5-layer convolutional neural network (CNN) encoder to generate latent representation at lower temporal resolution and 1-layer LSTM to summarize the latent representation as a context representation. Wav2Vec2.0. Wav2Vec2.0 [24] is also a contrastive learning-based approach. It is composed of a CNN-based encoder network, a VQ module, and a Transformer-based context representation network. The masking idea is also applied in the model, the input to the Transformer being randomly masked. During the pretaining, Wav2Vec2.0 is optimized with a contrastive loss function. In addition, a regularization term is added to the loss function to increase the diversity of the codebook in the VQ module.
3.4. HuBERT
HuBERT Hidden-Unit BERT(HuBERT) [25] is another novel selfsupervised speech representation learning approach. During pretaining, iterative pseudo-labelling method is adopted. First, ofﬂine clustering method (e.g. k-Means) is applied on the acoustic features of untranscribed speech, such as MFCC. In this process, the discrete labels is generated for the speech data. There are two major modules in the pretained HuBERT model including a CNN-based encoder and a BERT encoder. The HuBERT model is trained to predict the distribution of discrete units. Similar to BERT-LM, the input feature to the BERT encoder is partially masked. The cross-entropy loss is computed on both the masked and unmasked frames. After training, the model can be used to generate the new pseudo-labels with higher quality. Such reﬁnement can be carried out for many times.
3.5. Application of SSLRs
All the SSLR models we use are pretained and can be accessed via S3PRL. For HuBERT and CPC, we used the pretained model with 60,000h Libri-Light [32]. For the rest of the SSLRs, we use the models pretained with LibriSpeech 960h [7]. The detailed information of SSLRs is described in Table 1. We follow the policies in SUPERB [28] to use the weighted-sum of multiple hidden states as

4.2. Performance on E2E-ASR tasks
In this part, we present the E2E-ASR results of various SSLRs using joint CTC/attention-based encoder-decoder system. The results on different corpora are shown in Table. 3.

Fig. 1. End-to-End speech processing with various speech representations. The framework can be used in speech recognition (ASR), speech translation (ST), speech enhancement (SE), etc.

the input to E2E-ASR. A weight parameter is trained to summarize all the hidden states for each SSLR model. The overall E2E-ASR is shown in Fig. 1. The input speech goes through a frontend to extract feature. Various of SSLRs can be extracted instead of conventional features, such as FBANK. Next, the feature sequence is fed into the ASR network to predict the hypothesis.
4. EXPERIMENTS
4.1. Setups
To evaluate the performance of various self-supervised learning representations, we conduct experiments on several open-source and publicly available corpora. Although the pretained representation models were learned on the LibriSpeech 960 or LibriLight data, we tested their performance on other datasets to verify their generalization ability. We conducted experiments on 7 ASR corpora 2. We try to cover various aspects in the ASR task, including read vs. spontaneous speech, noisy/distant-talk environment and the telephone channel.
Basically, we follow the recipes in the latest ESPnet [29] to conduct the experiments. The default FBANK feature extractor is replaced with the pretained self-supervised learning representation models. The downsampling layers are used to make all the SSLRs have a 40ms-stride in the encoder. For most of the corpora, we use the Conformer encoder and Transformer decoder as our network architecture, which showed promising results [36]. The hyperparameters of the ASR models are shown in Table 2.

Table 2. Hyperparameters of the ASR models for each dataset, in-
cluding data augmentations, encoder type and kernel size (kernel)
of Conformer encoder, number of encoder layers (nenc), number of decoder layers (ndec), number of attention heads (H), feed-forward dimension (dff), attention dimension (datt).

Dataset

DataAugment

EncoderType kernel nenc ndec H dff datt

SpeedPerturb SpecAug

AISHELL



CHiME4



LibriSpeech



Switchboard



TEDLIUM2



WSJ



WSJ0-2mix





Conformer

15

12 6 4 2048 256



Conformer

15

12 6 4 2048 256



Conformer

31

12 6 8 2048 512



Conformer

31

12 6 4 2048 256



Conformer

15

12 6 4 2048 256



Conformer

15

12 6 4 2048 256



Transformer

-

12 6 4 2048 256

2We did our best to work on as many datasets as possible. Due to the time limit, we couldn’t ﬁnish all the experiments before submission unfortunately. We are still working on some other corpora including but not limited to REVERB [39], AMI [40], VoxForge, HKUST [41], TEDLIUM3 [42].

4.2.1. Read English Speech
LibriSpeech [7] is a corpus of English speech extracted from audiobooks. Here we used all 960 hours of data as the training set. We can see that HuBERT, using 60,000h data, outperformed the baseline3. In particular, HuBERT achieved a relative improvement of as much as 23.8% and 26.2% for dev-other and test-other sets, respectively. On the other hand, no improvement was observed for other models pre-trained using LibriSpeech 960h. In this case, we used the same data for representation learning and ASR task without ﬁnetuning. Therefore, it is inferred that there was no room for growth in improvement compared to other corpora setups.
WSJ is a reading speech corpus drawn from the Wall Street Journal news text and the total amount of the training set is about 80 hours. From the Table 3, we can ﬁnd that most of SSLRs show similar results compared with the FBANK feature except HuBERT and Wav2Vec2.0. When simply using the speech representations learned from HuBERT (60kh) and Wav2Vec2.0 (960h), we obtain superior WERs of 3.0%/1.5% and 3.7%/2.1% on the dev93 and eval92 sets, respectively, reaching the SOTA results (1.3% on eval92 set) presented in [9], which is trained over 5000 hours English speech.
4.2.2. Spontaneous English Speech
We further evaluate the performance with TEDLIUM2 [43] dataset, a Semi-Spontaneous English Speech corpus. Using the HuBERT and Wav2Vec2.0 brings a signiﬁcant performance gain compared with the FBANK. HuBERT relatively improves the word error rates (WERs) on dev and test sets by 33% and 30%, respectively, while Wav2Vec2.0 achieves 20% and 17%. For other representations, they achieve better performance than FBANK. Among them, TERA is slightly better than the rest.
4.2.3. Noisy English Speech
CHiME4 [44] is a noisy multichannel ASR corpus. This corpus is challenging for self-supervised models because the representations are designed for encoding clean speech signals. We evaluate with both single-mic and multi-mic sets. The 5ch set is enhanced with delay and sum beamforming [45]. HuBERT and Wav2Vec2.0 gives signiﬁcant performance improvement over FBANK features on the multi-mic (5ch) scenario but the performance is similar to FBANK features on the single-mic (1ch) scenario. The other representations degrade the performance compared to FBANK in both scenarios. With Mockingjay, we noticed very serious overﬁtting issues and it seems inappropriate for this dataset.
4.2.4. Telephone Channel English Speech
Switchboard corpus consists of approximately 260 hours of telephone conversation speech and is collected at 8 KHz. Here, we upsample it to 16 KHz to make it suitable for the pretrained models. Since all SSLR models are trained on the reading English speech,
3For APC, NPC, and TERA, the number of gradient accumulation was set incorrectly, thus these results could be worse than they actually are. We are re-training the corresponding models.

Table 3. Performance (WERs / CERs) of joint CTC/attention-based encoder-decoder on various open source ASR corpora.

Dataset Vocab Metric

Evaluation Sets

FBANK

APC (960h)

CPC (60kh)

HuBERT (60kh)

Mockingjay (960h)

NPC (960h)

TERA (960h)

VQ-APC (960h)

Wav2Vec2.0 (960h)

AISHELL Char CER

dev / test

4.4 / 4.7 6.1 / 6.5 4.9 / 5.3 4.4 / 4.7

5.0 / 5.4

5.0 / 5.5 4.7 / 5.1 5.0 / 5.5

4.6 / 5.0

{dt05/et05}-1ch

13.6 / 23.2 16.8 / 27.4 16.2 / 25.8 11.6 / 22.8

-

16.8 / 28.0 16.8 / 28.0 17.1 / 27.4 13.5 / 26.1

CHiME4 Char WER

{dt05/et05}-5ch

9.4 / 15.8 11.1 / 18.3 10.5 / 17.1 5.0 / 10.2

-

11.0 / 18.8 10.9 / 18.1 11.0 / 18.0 6.3 / 12.5

LibriSpeech BPE WER

{dev / test}-clean {dev / test}-other

1.7 / 1.9 4.2 / 4.2

2.4 / 2.6 7.3 / 7.5

2.2 / 2.4 6.2 / 6.4

1.5 / 1.6 3.1 / 3.2

2.3 / 2.4 6.4 / 6.9

2.5 / 2.6 7.3 / 7.5

2.4 / 2.5 6.7 / 7.0

2.2 / 2.6 7.0 / 7.3

1.7 / 1.9 4.9 / 4.7

Switchboard BPE WER eval2000(callhm/swbd) 15.6 / 8.4 17.7 / 8.9 15.7 / 8.4 18.1 / 7.3 16.9 / 9.9 17.6 / 9.1 17.2 / 8.9 17.4 / 8.6 14.9 / 7.9

TEDLIUM2 BPE WER

dev / test

9.5 / 8.9 9.1 / 8.5 9.0 / 8.7 6.4 / 6.2

9.2 / 8.6

9.3 / 8.5 8.9 / 8.4 9.7 / 9.0

7.6 / 7.4

WSJ

Char WER

WSJ0-2mix Char WER

dev93/eval92 dev / test

6.6 / 4.4 17.7†

7.2 / 4.5 16.5

7.1 / 4.7 14.9

3.0 / 1.5 12.1

6.8 / 4.6 15.9

7.3 / 4.8 16.5

6.3 / 4.4 15.1

7.5 / 4.6 17.1

3.7 / 2.1 13.2

†: The FBANK result is obtained

there is no doubt that the data mismatches, like speech style, channel distortion, etc., will evidently hinder the performance. From the table, we can ﬁnd that while most of the SSLR models are susceptible to performance degradation, both HuBERT and Wav2Vec2.0 are robust to such domain mismatches, obtaining a slight improvement compared with FBANK feature.

respectively. It is worth mentioning that the raw speech data to training the pretained representation are all from single-speaker speech, which is quite different from the overlapped multi-speaker speech. We infer that because of this mismatch between the data distributions, although the back-end models are fully trained with the representation of CPC or Mockingjay, results are not satisfactory.

4.2.5. Non-English Speech
Another question remaining is whether these SSLR models are also suitable for cross-lingual scenarios, such Mandarin ASR task Thus, we also conduct experiments on the open-source Mandarin AISHELL corpus [46]. The AISHELL corpus contains about 178 hours Mandarin speech recorded in a clean environment. Due to the limitation of time and computation resource, we are only able to conduct experiments on a little SSLR models. From the character error rates (CERs) results, we can see that APC and the English language pretrained models are not able to generalize to other languages. Although the HuBERT achieves same results as FBANK feature, other models, like APC and Wav2Vec2.0 shows a severely performance degradation, yielding up to 30% relative increase of the CERs results.
4.2.6. Multi-speaker overlapped Speech
In the previous sections, results on several single-speaker speech corpora show the different performance of the pretaining models. To further explore whether the models, e.g., HuBERT and Wav2Vec2.0, that work well on a single-speaker dataset can be applied in the case of multi-speaker overlapped speech, in this section, we conduct the experiments on the WSJ0-2mix dataset [47], which is the de-facto benchmark dataset for speech separation or multi-speaker speech recognition systems. In WSJ0-2mix, the 30 hours training set and 10 hours validation set contain two-speaker overlapped mixtures. The 5 hours test set was similarly generated using utterances from another 18 speakers from the WSJ0 validation set and evaluation set which has not overlapped speaker with the training set.
As shown in the last line of Table 3, although with the identical architecture of transformer-based model, the results with different pretained representation show a wide range of differences in the term of WER. As shown in the last line of Table 3, although with the identical architecture of transformer-based model, the results with different pretained representation show a wide range of differences in the term of WER. To be speciﬁc, similar to the previous observation on the single-speaker dataset, the HuBERT and Wav2Vec2.0 also show obvious advantages with the WERs of 12.1% and 13.5%

Fig. 2. Validation accuracies of different self-supervised learning representations on WSJ ASR dataset.
4.3. Performance of E2E Non-autoregressive ASR
The pretained representations seem to be effective in E2E-ASR from the experiments above, especially the HuBERT and Wav2Vec2.0 representations. It is based on the joint CTC/attention-based encoder-decoder (AED) network. Recently, non-autoregressive (NAR) ASR has become popular. We use the simplest NAR model, namely the CTC-based ASR, to evaluate the performance of every SSLR. The results are shown in Table 4. When we decode without language model, namely the greedy search in CTC, the HuBERT and Wav2Vec2.0 still outperform all the other representations. The performance of Mockingjay is slightly worse than APC and CPC, different from the joint CTC/AED case. If we add the language model and using beamsearch, the results become much better. For the CTC models, it is surprising that the WERs of HuBERT without LM is even better than those of FBANK with LM.
4.4. Weighted-sum vs. Last layer vs. Finetuned Last layer
In this part, we explore different ways to use the pretained representations. Since the HuBERT and Wav2Vec2.0 achieve much better results than other representations, we mainly evaluate these two

Table 4. WERs of dev/test sets on WSJ ASR corpora. Comparison between CTC and joint CTC/attention-based encoder-decoder.

Frontend

CTC w/o LM w/ LM

Joint CTC & AED

FBANK APC CPC
HuBERT Mockingjay
NPC TERA VQ-APC Wav2Vec2.0

17.4 / 13.6 18.5 / 15.0 18.9 / 14.7 8.1 / 5.6 19.2 / 15.6 18.5 / 13.7 18.2 / 14.8 19.4 / 14.7 9.4 / 7.2

8.5 / 5.9 9.3 / 6.5 9.6 / 6.7 3.3 / 2.1 9.7 / 7.0 9.4 / 6.5 8.7 / 6.5 10.0 / 6.5 4.3 / 2.5

6.6 / 4.4 7.2 / 4.5 7.1 / 4.7 3.0 / 1.5 6.8 / 4.6 7.3 / 4.8 6.3 / 4.4 7.5 / 4.6 3.7 / 2.1

Fig. 3. The weights of the feature summarization after training on HuBERT and Wav2Vec2.0 representations.

models. We test three methods on the WSJ dataset for fast development and the results are presented in Table 5. To reduce the impact of language models, we show the results of both with LM and without LM. First, we can see that decoding results of both of the HuBERT and Wav2Vec2.0 without LM are better than FBANK baseline with LM in Table3. For Wav2Vec2.0 without LM, the weighted-sum feature performs better than the last-layer cases. This indicates that the hidden states in the middle of the Transformer encoder within Wav2Vec2.0 is also helpful. We further look at the weights of the feature summarization after training, shown in Fig. 3. It can be seen that HuBERT concentrates more on the last-layer. This phenomenon still remains discussion.

Table 5. WERs of dev/test sets on WSJ ASR corpus in w/ and w/o LM conditions. Using weighted-sum, last-layer and ﬁnetuned lastlayer outputs as ASR input feature.

Frontend LM Weighted-Sum Last-layer Finetuned Last-layer

HuBERT  
Wav2Vec2.0  

3.0 / 1.5 4.9 / 3.4 3.7 / 2.1 6.1 / 3.9

2.6 / 1.6 4.8 / 3.4 3.7 / 2.1 6.1 / 4.5

2.8 / 1.6 4.6 / 3.5 3.7 / 2.0 6.1 / 4.2

4.5. Discussions
According to the experiments, we ﬁnd that the self-supervised learning representations can improve the performance in some cases, especially using the HuBERT and Wav2Vec2.0 models. In Fig. 2, we show the validation accuracies of different SSLRs on the WSJ dataset. All the models use the same optimization parameters. We observed that the learning curve behaviour with the pretained representations are obviously better than FBANK at the ﬁrst few epochs, which shows the fast convergence properties of all SSLRs. The accuracies of HuBERT and Wav2Vec2.0 stay the leading position with large margins throughout the training process, which indicates that we can easily judge whether the pretained representations is correctly working or not without waiting for the entire training epochs.
We summarize the other training tips we have observed in our experiment:
• If the number of GPUs is insufﬁcient, the accumulating gradient strategy [48] can be employed to emulate a large minibatch.
• When a model suffers from over-ﬁtting, dropout of positional encoding and attentions in Transformer and Conformer block can be enabled.
• Strong self-supervised representation models, including HuBERT and Wav2Vec2.0, are robust to optimization parameters, such as the learning rate, batch size (or accumulating gradient), etc. Thus the hyperparameters used in HuBERT and Wav2Vec2.0 may not always ﬁt in others.
• Because HuBERT and Wav2Vec2.0 representations are robust, their training can be used as an upper bound for other representations to monitor the training trend.
• It takes a lot of time to compute the global normalization statistics on CPU. One option is to do it on GPU. For simplicity, we just use utterance-level normalization rather than global one.
5. CONCLUSION
In this paper, we explore the application of different pretained self-supervised learning representations in E2E-ASR with the joint CTC/attention-based encoder-decoder architecture. We conduct the experiments on several publicly available corpora. The results show that HuBERT and Wav2Vec2.0, which have the largest number of parameters, can dramatically improve the performance against the commonly used log-Mel Filterbank feature. To achieve better performance, we can directly re-use the existing representation models. We also ﬁnd that the pretained representations can generalize to other corpora, not restricted to a speciﬁc dataset. We plan to opensource all of our conﬁgurations and scripts in the ESPnet project, to help the community easily access and improve the performance. In the future, we will extend this project to more corpora and the speech processing tasks beyond ASR.
6. ACKNOWLEDGEMENT
This work used the Extreme Science and Engineering Discovery Environment (XSEDE) [49], which is supported by National Science Foundation grant number ACI-1548562. Speciﬁcally, it used the Bridges system [50], which is supported by NSF award number ACI-1445606, at the Pittsburgh Supercomputing Center (PSC).

7. REFERENCES
[1] Ossama Abdel-Hamid, Abdel-rahman Mohamed, Hui Jiang, and Gerald Penn, “Applying convolutional neural networks concepts to hybrid NN-HMM model for speech recognition,” in Proc. ICASSP. IEEE, 2012, pp. 4277–4280.
[2] Alex Graves, Abdel-rahman Mohamed, and Geoffrey Hinton, “Speech recognition with deep recurrent neural networks,” in Proc. ICASSP. IEEE, 2013, pp. 6645–6649.
[3] William Chan, Navdeep Jaitly, Quoc V Le, and Oriol Vinyals, “Listen, attend and spell,” in Proc. ICASSP. IEEE, 2016, pp. 4960–4964.
[4] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, et al., “Attention is all you need,” in Proc. NeurIPS, 2017, pp. 5998–6008.
[5] Linhao Dong, Shuang Xu, and Bo Xu, “Speech-Transformer: A no-recurrence sequence-to-sequence model for speech recognition,” in Proc. ICASSP. IEEE, 2018, pp. 5884–5888.
[6] Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, et al., “Conformer: Convolution-augmented Transformer for speech recognition,” in Proc. Interspeech. ISCA, 2020, pp. 5036–5040.
[7] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur, “Librispeech: An ASR corpus based on public domain audio books,” in Proc. ICASSP. IEEE, 2015, pp. 5206– 5210.
[8] Guoguo Chen, Shuzhou Chai, Guanbo Wang, Jiayu Du, WeiQiang Zhang, Chao Weng, Dan Su, Daniel Povey, Jan Trmal, Junbo Zhang, et al., “GigaSpeech: An evolving, multi-domain ASR corpus with 10,000 hours of transcribed audio,” in Proc. Interspeech. ISCA, 2021.
[9] William Chan, Daniel Park, Chris Lee, Yu Zhang, Quoc Le, and Mohammad Norouzi, “SpeechStew: Simply mix all available speech recognition data to train one large neural network,” in Proc. Interspeech. ISCA, 2021.
[10] Dong-Hyun Lee et al., “Pseudo-label: The simple and efﬁcient semi-supervised learning method for deep neural networks,” in Proc. ICML, 2013, p. 896.
[11] Gabriel Synnaeve, Qiantong Xu, Jacob Kahn, Tatiana Likhomanenko, Edouard Grave, Vineel Pratap, Anuroop Sriram, Vitaliy Liptchinsky, and Ronan Collobert, “End-to-end ASR: From supervised to semi-supervised learning with modern architectures,” in Proc. ICML, 2020.
[12] Jacob Kahn, Ann Lee, and Awni Hannun, “Self-training for end-to-end speech recognition,” in Proc. ICASSP. IEEE, 2020, pp. 7084–7088.
[13] Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan, “Show and tell: Lessons learned from the 2015 MSCOCO image captioning challenge,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 39, no. 4, pp. 652–663, 2016.
[14] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova, “BERT: Pre-training of deep bidirectional transformers for language understanding,” in Proc. NAACL. ACL, 2019, pp. 4171–4186.
[15] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever, “Improving language understanding by generative pre-training,” 2018.

[16] Yu-An Chung, Wei-Ning Hsu, Hao Tang, and James Glass, “An unsupervised autoregressive model for speech representation learning,” in Proc. Interspeech. ISCA, 2019, pp. 146–150.
[17] Yu-An Chung, Hao Tang, and James Glass, “Vector-quantized autoregressive predictive coding,” in Proc. Interspeech. ISCA, 2020, pp. 3760–3764.
[18] Andy T Liu, Shu-wen Yang, Po-Han Chi, Po-chun Hsu, and Hung-yi Lee, “Mockingjay: Unsupervised speech representation learning with deep bidirectional transformer encoders,” in Proc. ICASSP. IEEE, 2020, pp. 6419–6423.
[19] Andy T Liu, Shang-Wen Li, and Hung-yi Lee, “Tera: Selfsupervised learning of transformer encoder representation for speech,” arXiv preprint arXiv:2007.06028, 2020.
[20] Alexander H Liu, Yu-An Chung, and James Glass, “Nonautoregressive predictive coding for learning speech representations from local dependencies,” arXiv preprint arXiv:2011.00406, 2020.
[21] Aaron van den Oord, Yazhe Li, and Oriol Vinyals, “Representation learning with contrastive predictive coding,” arXiv preprint arXiv:1807.03748, 2018.
[22] Steffen Schneider, Alexei Baevski, Ronan Collobert, and Michael Auli, “wav2vec: Unsupervised pre-training for speech recognition,” in Proc. Interspeech. ISCA, 2019, pp. 3465—3469.
[23] Alexei Baevski, Steffen Schneider, and Michael Auli, “vqwav2vec: Self-supervised learning of discrete speech representations,” in Proc. ICLR, 2020.
[24] Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, and Michael Auli, “wav2vec 2.0: A framework for self-supervised learning of speech representations,” in Proc. NeurIPS, 2020.
[25] Wei-Ning Hsu, Yao-Hung Hubert Tsai, Benjamin Bolte, Ruslan Salakhutdinov, and Abdelrahman Mohamed, “HuBERT: How much can a bad teacher beneﬁt ASR pre-training?,” in Proc. ICASSP. IEEE, 2021, pp. 6533–6537.
[26] Mathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze, “Deep clustering for unsupervised learning of visual features,” in Proc. ECCV, 2018, pp. 132–149.
[27] Junyuan Xie, Ross Girshick, and Ali Farhadi, “Unsupervised deep embedding for clustering analysis,” in Proc. ICML. PMLR, 2016, pp. 478–487.
[28] Shu-wen Yang, Po-Han Chi, Yung-Sung Chuang, Cheng-I Jeff Lai, Kushal Lakhotia, Yist Y Lin, Andy T Liu, Jiatong Shi, Xuankai Chang, Guan-Ting Lin, et al., “SUPERB: Speech processing universal performance benchmark,” in Proc. Interspeech, 2021.
[29] Shinji Watanabe, Takaaki Hori, Shigeki Karita, Tomoki Hayashi, Jiro Nishitoba, Yuya Unno, Nelson Enrique Yalta Soplin, Jahn Heymann, Matthew Wiesner, Nanxin Chen, et al., “ESPnet: End-to-end speech processing toolkit,” in Proc. Interspeech. ISCA, 2018, pp. 2207–2211.
[30] Hirofumi Inaguma, Shun Kiyono, Kevin Duh, Shigeki Karita, Nelson Yalta, Tomoki Hayashi, and Shinji Watanabe, “ESPnetST: All-in-one speech translation toolkit,” in Proc. ACL, 2020, pp. 302–311.
[31] Chenda Li, Jing Shi, Wangyou Zhang, Aswin Shanmugam Subramanian, Xuankai Chang, Naoyuki Kamo, Moto Hira, Tomoki Hayashi, Christoph Boeddeker, Zhuo Chen, et al.,

“ESPnet-SE: End-to-end speech enhancement and separation toolkit designed for ASR integration,” in Proc. SLT. IEEE, 2021, pp. 785–792.
[32] Jacob Kahn, Morgane Rivie`re, Weiyi Zheng, Evgeny Kharitonov, Qiantong Xu, Pierre-Emmanuel Mazare´, Julien Karadayi, Vitaliy Liptchinsky, Ronan Collobert, Christian Fuegen, et al., “Libri-light: A benchmark for ASR with limited or no supervision,” in Proc. ICASSP. IEEE, 2020, pp. 7669–7673.
[33] Alex Graves, Santiago Ferna´ndez, Faustino Gomez, and Ju¨rgen Schmidhuber, “Connectionist temporal classiﬁcation: Labelling unsegmented sequence data with recurrent neural networks,” in Proc. ICML. PMLR, 2006, pp. 369–376.
[34] Suyoun Kim, Takaaki Hori, and Shinji Watanabe, “Joint CTCattention based end-to-end speech recognition using multi-task learning,” in Proc. ICASSP. IEEE, 2017, pp. 4835–4839.
[35] Shigeki Karita, Nanxin Chen, Tomoki Hayashi, Takaaki Hori, Hirofumi Inaguma, et al., “A comparative study on Transformer vs RNN in speech applications,” in Proc. ASRU. IEEE, 2019, pp. 449–456.
[36] Pengcheng Guo, Florian Boyer, Xuankai Chang, Tomoki Hayashi, Yosuke Higuchi, Hirofumi Inaguma, Naoyuki Kamo, Chenda Li, Daniel Garcia-Romero, Jiatong Shi, et al., “Recent developments on ESPnet toolkit boosted by Conformer,” in Proc. ICASSP. IEEE, 2021, pp. 5874–5878.
[37] Michael Gutmann and Aapo Hyva¨rinen, “Noise-contrastive estimation: A new estimation principle for unnormalized statistical models,” in Proc. AISTATS, 2010, pp. 297–304.
[38] Morgane Riviere, Armand Joulin, Pierre-Emmanuel Mazare´, and Emmanuel Dupoux, “Unsupervised pretraining transfers well across languages,” in Proc. ICASSP. IEEE, 2020, pp. 7414–7418.
[39] Keisuke Kinoshita, Marc Delcroix, Takuya Yoshioka, Tomohiro Nakatani, Emanuel Habets, Reinhold Haeb-Umbach, Volker Leutnant, Armin Sehr, Walter Kellermann, Roland Maas, et al., “The REVERB challenge: A common evaluation framework for dereverberation and recognition of reverberant speech,” in Proc WASPAA. IEEE, 2013, pp. 1–4.
[40] Iain McCowan, Jean Carletta, Wessel Kraaij, Simone Ashby, S Bourban, M Flynn, M Guillemot, Thomas Hain, J Kadlec, Vasilis Karaiskos, et al., “The AMI meeting corpus,” in Proceedings of the 5th International Conference on Methods and Techniques in Behavioral Research. Citeseer, 2005, vol. 88, p. 100.
[41] Yi Liu, Pascale Fung, Yongsheng Yang, Christopher Cieri, Shudong Huang, and David Graff, “HKUST/MTS: A very large scale mandarin telephone speech corpus,” in Proc. ISCSLP. Springer, 2006, pp. 724–735.
[42] Franc¸ois Hernandez, Vincent Nguyen, Sahar Ghannay, Natalia Tomashenko, and Yannick Esteve, “TED-LIUM 3: Twice as much data and corpus repartition for experiments on speaker adaptation,” in Proc. SPECOM. Springer, 2018, pp. 198–208.
[43] Anthony Rousseau, Paul Dele´glise, Yannick Esteve, et al., “Enhancing the TED-LIUM corpus with selected data for language modeling and more TED talks.,” in Proc. LREC, 2014, pp. 3935–3939.
[44] Emmanuel Vincent, Shinji Watanabe, Aditya Arie Nugraha, Jon Barker, and Ricard Marxer, “An analysis of environment,

microphone and data simulation mismatches in robust speech recognition,” Computer Speech & Language, vol. 46, pp. 535– 557, 2017.
[45] Xavier Anguera, Chuck Wooters, and Javier Hernando, “Acoustic beamforming for speaker diarization of meetings,” IEEE Transactions on Audio, Speech, and Language Processing, vol. 15, no. 7, pp. 2011–2022, 2007.
[46] Hui Bu, Jiayu Du, Xingyu Na, Bengu Wu, and Hao Zheng, “Aishell-1: An open-source mandarin speech corpus and a speech recognition baseline,” in Proc. O-COCOSDA. IEEE, 2017, pp. 1–5.
[47] John R Hershey, Zhuo Chen, Jonathan Le Roux, and Shinji Watanabe, “Deep clustering: Discriminative embeddings for segmentation and separation,” in Proc. ICASSP. IEEE, 2016, pp. 31–35.
[48] Myle Ott, Sergey Edunov, David Grangier, and Michael Auli, “Scaling neural machine translation,” in Proceedings of the Third Conference on Machine Translation: Research Papers, 2018, pp. 1–9.
[49] John Towns, Timothy Cockerill, Maytal Dahan, Ian Foster, Kelly Gaither, Andrew Grimshaw, Victor Hazlewood, Scott Lathrop, Dave Lifka, Gregory D Peterson, et al., “Xsede: Accelerating scientiﬁc discovery computing in science & engineering, 16 (5): 62–74, sep 2014,” URL https://doi. org/10.1109/mcse, vol. 128, 2014.
[50] Nicholas A Nystrom, Michael J Levine, Ralph Z Roskies, and J Ray Scott, “Bridges: A uniquely ﬂexible HPC resource for new communities and data analytics,” in Proc. XSEDE, 2015, pp. 1–8.

