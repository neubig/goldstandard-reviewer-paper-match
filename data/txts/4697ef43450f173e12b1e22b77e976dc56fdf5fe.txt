Compressive Sensing Based Adaptive Defence Against Adversarial Images
Akash Kumar Gupta, Arpan Chattopadhyay, Darpan Kumar Yadav

arXiv:2110.05077v1 [eess.IV] 11 Oct 2021

Abstract—Herein, security of deep neural network against adversarial attack is considered. Existing compressive sensing based defence schemes assume that adversarial perturbations are usually on high frequency components, whereas recently it has been shown that low frequency perturbations are more effective. This paper proposes a novel Compressive sensing based Adaptive Defence (CAD) algorithm which combats distortion in frequency domain instead of time domain. Unlike existing literature, the proposed CAD algorithm does not use information about the type of attack such as 0, 2, ∞ etc. CAD algorithm uses exponential weight algorithm for exploration and exploitation to identify the type of attack, compressive sampling matching pursuit (CoSaMP) to recover the coefﬁcients in spectral domain, and modiﬁed basis pursuit using a novel constraint for 0, ∞ norm attack. Tight performance bounds for various recovery schemes meant for various attack types are also provided. Experimental results against ﬁve state-of-the-art white box attacks on MNIST and CIFAR-10 show that the proposed CAD algorithm achieves excellent classiﬁcation accuracy and generates good quality reconstructed image with much lower computation.
Index Terms—Compressive sensing, Image classiﬁcation, Adversarial Image, CoSaMP, EXP3

videos and audio signals that are sparse in Fourier and wavelet domain. This also allows us to achieve lower complexity, lower power, smaller memory and less number of sensors, and provide theoretical performance guarantee for the image processing algorithms. These reasons motivate us to use compressive sensing to combat adversarial images.
In this paper, we propose a compressive sensing based adaptive defence (CAD) algorithm that can defend against all l0, l2, l∞ adversaries as well as gradient attacks. In order to identify the attack type and choose the appropriate recovery method, we use the popular exponential weight algorithm [10] adapted from the multi-armed bandit literature for exploration and exploitation decision, along with compressive sensing based recovery algorithms such as compressive sampling matching pursuit (CoSaMP) [11], standard basis pursuit and modiﬁed basis pursuit with novel constraints to mitigate l0, l∞ attack. Numerical results reveal that CAD is efﬁcient in classifying both grayscale and colored images, and that it does not suffer from clean data accuracy and gradient masking.

I. INTRODUCTION
T HE rapid development of Deep Neural Network (DNN) and Convolutional Neural Network (CNN) has resulted in the widening of computer vision applications such as object recognition, Covid-19 diagnosis using medical images [2], autonomous vehicles [3], face detection in security and surveillance systems [4] etc. In all these applications, images play a vital role. Recent studies have shown that smartly crafted, human imperceptible, small distortion in pixel values can easily fool these CNNs and DNNs [5]–[7]. Such adversarial images result in incorrect classiﬁcation or detection of an object or a face, leading to accidents on roads or by drones, trafﬁc jam, missed identiﬁcation of a criminal, etc. While many countermeasures have been proposed in recent years to tackle adversarial images, they are mostly based on heuristics and do not perform well against all classes of attacks. In this connection, the recent developments on compressive sensing [8], [9] allows signal recovery at subNyquist rate, which is suitable for application to images,
The authors are with the Electrical Engineering department, Indian Institute of Technology, Delhi. Email id: akgpt7@gmail.com, {arpanc, ee3180534}@ee.iitd.ac.in
AKG has contributed towards problem formulation, solution, coding and writing this paper. AC has contributed towards problem formulation, solution and writing. DKY has contributed towards coding.
This work was supported by the faculty seed grant, professional development allowance and professional development fund of Arpan Chattopadhyay, and the MHRD fellowship for Akash Kumar Gupta.
Codes for our numerical experiments are available in [1]

A. Related work
Existing research on adversarial images is broadly focused on two categories: attack design and defence algorithm design.
Attack design: Numerous adversarial attacks have been proposed in the literature so far. They can be categorized as white box attacks and black box attacks. In white box attacks, the attacker has full knowledge of trained classiﬁer its architectures, parameters and weights. Examples of white box attack include fast gradient sign method (FGSM [7]), projected gradient descent (PGD [12]), Carlini Wagner L2 (CW-L2) attack [13], basic iterative method (BIM [6]), Jacobian saliency map attack (JSMA [14]) etc. In black box attack, the attacker generates the adversarial perturbation without having any knowledge of the target model. Transfer-based attacks [15], gradient estimation attacks [16] and boundary attack [17]) are some examples of black box attack.
The adversarial attacks can also be divided into targeted attacks and non-targeted attacks. In targeted attacks, an attacker seeks to classify an image to a target class which is different from the original class. On the other hand, in nontargeted attack, the attacker’s goal is just to misclassify an image. Based on the nature of perturbation error, attacks are further grouped into various norm attacks, such as CW (L2) attack, L∞ BIM attack, etc.
Defence design: Adversarial image problem can be tackled either by (i) increasing the robustness of the classiﬁer by using either image processing techniques, or adversarial training,

or compressive sensing techniques (see [18]–[22]), or by (ii) distinguishing between clean and malicious images [23], [24].
Existing defense schemes based on compressive sensing [18], [19] assumes that normally images have heavy spectral strength at lower frequencies and little strength at higher frequencies, which allows the adversary to modify the high-frequency spectral components to fool the human eye. Usually, most of the adversarial attacks [13], [25], [17] work by searching the whole available attack space and are used to converge to high frequency perturbations to fool the classiﬁer. However, it has recently been observed that constraining attack to low-frequency perturbations and keeping small distortion bound in l∞ norm is more effective, and achieves high efﬁciency and transferability [26], [27].
The authors of [18] proposed a technique based on compressive sensing to combat l0 attack; the technique recovers low frequency components corresponding to 2D discrete cosine transform (DCT) basis. In this paper, the adversarial image vector y = x + e, where the original image x is k-sparse in Fourier domain and the injected noise e is t-sparse in time domain. This defense is based on the fact that usually the perturbation crafted by an attacker is on high frequency components, and hence it is not perceptible to human eye. Hence, the proposed defense works by just recovering the few top most DCT low frequency coefﬁcients and reconstructing images using those coefﬁcients only. Authors of [19] extended the same framework and proposed compressive recovery defense (CRD) to counter l2, l∞ attack. They proposed various algorithms for different perturbation attacks which require prior knowledge of the type of attack. However, they did not prescribe any choice of the recovery algorithm since the type of perturbation is not known apriori.
Another popular technique to counter malicious attacks is adversarial training based defense. Here the goal is to increase the robustness of the model by training the classiﬁer using several adversarial examples. The authors of [12] used projected gradient adversaries and clean images to train the network; though their proposed defense works well for datasets having grayscale images such as MNIST, it suffers from low classiﬁcation accuracy for datasets having colored images such as CIFAR-10. The authors of [20] used the same method and considered the properties of loss surface under various adversarial attacks in parameter and input domain. They showed that model robustness can be increased by using decision surface geometry as a parameter. The proposed defense has a very high computational complexity. The authors of [28] proposed collaborative multi-task training (CMT) to counter various attacks. They encoded training labels into label pairs which allowed them to detect adversarial images by determining the pairwise connections between actual output and auxiliary output. However, an enormous volume of nontargeted malicious samples is needed for determining the encoding format in [28]. Also, the proposed defense is only applicable for non-targeted attacks.
Several classical image processing techniques have been used earlier to combat adversarial attacks. The authors of [21] used Gaussian kernels with various intensities to

form multiple representations of the images in the dataset, and then fed these images to the classiﬁer. Classiﬁcation and attack detection were achieved by taking an average of multiple conﬁdence values given by the classiﬁer. The authors of [22] used pre-processing techniques; they altered the pixel values of images in the training and testing dataset block-wise by maintaining some common key. Using these image pre-processing techniques as a defense requires a lot of computations for each image in the dataset. Also, these papers did not establish any performance bound.
All the above papers deal with classiﬁcation based defense. Detection based defense has been proposed in [23], where the authors have proposed the adaptive perturbation based algorithm (APERT, a pre-processing algorithm) using principal component analysis (PCA), two-timescale stochastic approximation and sequential probability ratio test (SPRT [29]) to distinguish between clean and adversarial images.
B. Our Contributions
We have made following contributions in this paper:
• We propose a novel compressive sensing based adaptive defence (CAD) algorithm to combat l0, l2, l∞ norm attacks as well as gradient based attacks, with much lower computational complexity compared to existing works. The computational complexity is O(N 2) where N is the number of pixels in an image.
• CAD is the ﬁrst algorithm that can detect the type of attack if it falls within certain categories (such as l0, l2, l∞), and choose an appropriate classiﬁcation algorithm to apply on the potentially adversarial image. To this end, we have adapted the popular exponential weight algorithms [10], [30] from multi-armed bandit literature to our setting, which adaptively assigns a score to each attack type, thus guiding us in choosing the appropriate recovery algorithm (e.g., CoSaMP, basis pursuit etc.). The CAD algorithm does not require any prior knowledge of the adversary.
• We consider adversarial perturbation in the frequency domain instead of the time domain while formulating the problem, which allows us to counter both low as well as high frequency spectral components.
• We propose modiﬁed basis pursuit using a novel constraint to mitigate l0 and l∞ norm attacks, and establish its performance bound.
• Our work has the potential to trigger a new line of research where compressive sensing and multi-armed bandits can be used for detection and classiﬁcation of adversarial videos.
C. Organization
This paper is further arranged as follows. Description of various recovery algorithms and their performance bounds are established in Section II. The proposed CAD algorithm is described in Section III. Complexity analysis of CAD is provided in Section IV, followed by the numerical results in Section V and conclusions in Section VI.

II. BASIC MODEL AND VARIOUS RECOVERY ALGORITHMS
In this section, we deﬁne the basic problem and propose various recovery algorithms assuming that the attack type is known to the classiﬁer. It is noteworthy that here we propose modiﬁed versions of basis pursuit to combat l0 and l∞ attacks in the spectral domain, and provide performance bounds for these algorithms. The background theory provided in this section are prerequisites to understand the performance of the proposed CAD algorithm later under various circumstances.
A. Problem Formulation
Let us consider a clean, vectorized image x ∈ RN×1, and let us assume that it is k-sparse [9, Deﬁnition 2.1] in discrete Fourier transform domain. Let its Fourier coefﬁcients be xˆ = F x, where F ∈ CN×N is the DFT matrix. The adversary modiﬁes the image in spectral domain by adding an error vector e to xˆ, and the distorted image becomes y = F −1(xˆ + e). For l0 attack, e is assumed to be τ sparse, so that xˆ + e becomes at most (k + τ ) sparse in Fourier domain. Deﬁning A =. F −1 and β =. F −1e, the modiﬁed image becomes y = Axˆ + β. Our objective is to ﬁnd xˆ from y. We will solve this problem iteratively by using compressive sensing based adaptive defense (CAD) algorithm comprising compressive sampling matching pursuit (CoSaMP) and modiﬁed version of basis pursuit for various attacks, and an adapted version of the exponential weight algorithm for selecting the recovery algorithm.
B. Compressive Sampling Matching Pursuit, CoSaMP
We know that images are compressible signals as their coefﬁcients decay rapidly in Fourier domain when arranged according to their magnitudes. CoSaMP [11] iteratively recovers the approximate Fourier coefﬁcients of a compressible signal from noisy samples given that the signal is sparse in the Fourier domain; it is based on orthogonal matching pursuit (OMP), but provides stronger guarantee than OMP. The authors of [11] have shown that this algorithm produces a 2k-sparse recovered vector whose recovery error in L2 norm is comparable with the scaled approximation error in L1 norm. CoSaMP provides optimal error guarantee for sparse signal, compressible signal and arbitrary signal.
Since we do not know apriori whether the attack is l0, l2, l∞ or gradient-based, and since it is difﬁcult to infer the type of the attack initially, we use CoSaMP along with various versions of basis pursuit for Fourier coefﬁcient recovery. This is further motivated by the fact that CoSaMP is robust against arbitrary injected error [11]. However, our proposed CAD algorithm (described in Section III) also adaptively assigns a score to each recovery scheme via the exponential weight algorithm using the residue-based feedback for each algorithm, and probabilistically selects an algorithm in each iteration based on the assigned scores. The exponential weight algorithm is typically used to solve online learning problems that involve exploration and exploitation, and the robustness of CoSaMP facilitates exploration especially at the initial phase when the algorithm has not developed a strong belief about

the type of attack. In this connection, it is worth mentioning that CoSaMP has provably strong performance bounds in all cases and also works well for highly sparse signals.
Let us denote by xˆ0 the initialisation before applying CoSaMP algorithm (usually we take xˆ0 = 0). The quantity xˆh(k) is a k-sparse vector (i.e., its l0 norm is at most k) that consists of k largest entries (in terms of absolute values) of xˆ. We also deﬁne xˆt(k) = xˆ − xˆh(k). The iteration number in the CoSaMP algorithm is denoted by n.
The performance guarantee of CoSaMP is provided through the following theorem:
Theorem 1. Suppose that the 4kth restricted isometry constant of the matrix A ∈ CN×N satisﬁes δ4k < 0.47. Then, for xˆ ∈ CN , β ∈ CN , and S ⊂ [N ] with card(S) = k, the Fourier coefﬁcients xˆn deﬁned by CoSaMP with y = Axˆ + β satisﬁes:
xˆn − xˆh(k) 2 ≤ ρn xˆ0 − xˆh(k) 2 + τ Axˆt(k) + β 2 (1)
where the constant 0 < ρ < 1 and τ > 0 depend only on δ4k.
Proof. The proof is similar to that of [9, Theorem 6.27].

C. Combating l2 Attack using basis pursuit

Standard basis pursuit is chosen to counter l2 perturbation [31] since it minimizes the l1 norm of Fourier coefﬁcients while constraining the l2-norm of the injected error. Let us assume that the l2 perturbation satisﬁes ||F −1e||2 ≤ η for a small η, and hence is imperceptible to human eye. Since F −1 is an orthonormal matrix, we can
write it as ||e||2 ≤ η. Let σk(xˆ)1 =. min||z|| ≤k ||xˆ − z||1. Performance bound
0
for the standard basis pursuit algorithm is provided in the
following theorem:

Theorem 2. Suppose that the 2kth restricted isometry

constant of the matrix A ∈ CN×N satisﬁes δ2k < 0.624. Then, for any xˆ ∈ CN and y ∈ CN with ||Axˆ − y||2 ≤ η, a solution xˆ∗ of minz∈CN ||z||1 subject to ||Az − y||2 ≤ η

approximates the xˆ with errors

√

xˆ − xˆ∗ 1 ≤ Cσk(xˆ)1 + D kη

(2)

xˆ − xˆ∗

2≤

C √

σk(xˆ)1 + Dη

(3)

k

where the constants C, D > 0 depend only on δ2k.

Proof. The proof is similar to [9, Theorem 6.12]

From Theorem 2, it is clear that, in order to guarantee unique recovery of largest k Fourier coefﬁcients, sensing matrix A should satisfy restricted isometry property (RIP) [9, Deﬁnition 6.1] of order 2k. It has been observed that with high probability, random Gaussian and partial Fourier matrices satisfy RIP properties [32], which ensures that any 2k columns in matrix A are linearly independent. We can relate performance bound (3) in spectral domain with that in time domain, since F −1 is an orthonormal matrix.

D. Combating l0 Attack using basis pursuit

In Section III, we employ another modiﬁed version of basis pursuit to counter l0 attack; this involves a slightly different formulation. Let us assume that the perturbation error e is τ sparse, and let us arrange perturbations of error vector e in ascending order [e1, e2, ...eτ ...0]. In l0 attack, the attacker has constraints only on the number of Fourier coefﬁcients that can be perturbed. Since according to the uncertainty principal [33] any image cannot be simultaneously narrow in the pixel domain as well as in spectral domain, the l∞ norm of the injected error e under l0 attack should have small enough to remain imperceptible to the human eye, i.e., |e|∞ < η , for some constant η . Now, it is well known that e 2 ≤ e 1, and we also notice that e 1 = |e1| + |e2| + ... + |eτ | ≤ τ |eτ |, which yield e 2 ≤ τ |eτ | ≤ τ η .
The performance bound for the modiﬁed basis pursuit algorithm under l0 attack is provided in the following theorem:

Theorem 3. Suppose that the 2kth restricted isometry
constant of the matrix A ∈ CN×N satisﬁes δ2k < 0.624. Then, for any xˆ ∈ CN and y ∈ CN with ||Axˆ − y||2 ≤ τ η , a solution xˆ∗ of minz∈CN ||z||1 subject to ||Az − y||2 ≤ τ η

approximates the xˆ with errors

√

xˆ − xˆ∗ 1 ≤ Cσk(xˆ)1 + D kτ η

(4)

xˆ − xˆ∗

2≤

C √

σk(xˆ)1 + Dτ η

(5)

k

where the constants C, D > 0 depend only on δ2k.

Proof. This Theorem can be followed easily using Theorem 2 and the fact that e 2 ≤ τ |eτ | ≤ τ η as discussed earlier.

E. Combating l∞ Attack using basis pursuit
Let us assume that ||e||∞ < η . Now, since F is orthonormal,

||F −1e||22 = ||e||22 ≤ N max(|ei|2) = N ||e||2∞ (6)
i

and hence

√

√

||e||2 ≤ N ||e||∞ ≤ N η

(7)

The performance guarantee for modiﬁed basis pursuit under l∞ attack is provided in the following theorem:

Theorem 4. Suppose that the 2kth restricted isometry constant of the matrix A ∈ CN×N satisﬁes δ2k < 0.6√24. Then, for any xˆ ∈ CN and y ∈ CN with ||Axˆ − y||2 ≤ N√η , a solution xˆ∗ of minz∈CN ||z||1 subject to ||Az−y||2 ≤ N η

approximates the xˆ with errors

√

xˆ − xˆ∗ 1 ≤ Cσk(xˆ)1 + D kN η

(8)

xˆ − xˆ∗ 2 ≤ √C σk(xˆ)1 + D√N η (9) k

where the constants C, D > 0 depend only on δ2k.

Proof. The proof follows easily from Theorem 2 and (7).

F. Combating l1 attack using basis pursuit
If e is such that e 1 < η, then the error in the recovered image also satisﬁes F −1e 2 = e 2 ≤ e 1 ≤ η, and we can solve the same l1 minimization problem with the same constraint as in Section II-C for l2 attack. Similarly, its performance bound will be given by Theorem 2.
III. THE COMPRESSIVE SENSING BASED ADAPTIVE DEFENSE (CAD) ALGORITHM
In this section, we propose our main algorithm to combat adversarial images. Since the CAD algorithm does not have any prior knowledge on the type of attack, CAD algorithm employs an adaptive version of the exponential weight algorithm [10], [30] for exploration and exploitation to assign a score on each possible attack type, and chooses an appropriate recovery method based on the inferred nature of the injected error. In this paper, we consider four actions, i.e., four different ways to recover k-sparse Fourier coefﬁcients, corresponding to different types of perturbation:
• CoSaMP (Action 1): This greedy approach allow us to accurately approximate the Fourier coefﬁcients initially when we do not have any belief for the type of attack. As iterations progress, the algorithm explores other actions as well.
• Modiﬁed Basis pursuit L0 (Action 2): A modiﬁed form of basis pursuit with novel constraint e 2 ≤ τ η is used to tackle l0 perturbation attack.
• Standard Basis pursuit L1 and L2 (Action 3): Standard basis pursuit method is used to counter both l1 and l2 attack.
• Modiﬁed Basis pursuit L∞ (Action 4): Modiﬁed basis pursuit is used to tackle l∞ norm attack, using novel constraint given by (7).
In the next three subsections, we discuss three major aspects of our proposed CAD algorithm: (i) adaptive exponential weight algorithm for choosing an appropriate recovery scheme, (ii) actions and feedback, and (iii) stopping criteria.

A. The adaptive version of exponential weight for choosing the recovery scheme
Algorithm 1 summarizes the overall defence strategy. In each iteration t, the algorithm chooses randomly an action using a probability distribution pai (t) where ai, i ∈ {1, 2, 3, 4} denotes the action chosen.
The probability of choosing an action is given by exponential weighting:

pa (t) = (1 − γ) exp (σSai (t − 1))

γ +

(10)

i

4 m=1

exp

(σSam

(t

−

1))

4

where Sai (t − 1) =

t−1 τ =1

rai (τ )

is

the

total

score

up

for

the

action ai. Here σ and γ are tuning parameters such that σ > 0

and γ ∈ (0, 1). The reward for action ai at the t-th iteration, rai (t) is the following:



λ,

 

p

a

i

(t

)

rai (t) =

−1 ,
1−pai (t)



0

if fai (t) = 1 if fai (t) = 0 if ai is not chosen in the t-th iteration

(11)

Here fai (t) is a binary feedback that is obtained by checking

certain conditions for action ai in the t-th iteration; this

feedback signiﬁes the applicability of action ai. If action ai is

chosen in the t-th iteration and if its feedback fai (t) = 1,

the actual reward λ > 0 is divided by pai (t) so that an

unbiased estimate of the reward is obtained. On the other hand,

if fai (t) = 0, then a penalty of −1 is assigned for ai. However,

this penalty is divided by (1 − pai (t)) to ensure that, if pai (t)

is small because it has not been chosen frequently earlier, the

penalty incurred by ai in the t-th iteration remains small.

The action in each iteration is chosen in the following

way. With probability γ, one action is randomly chosen

from uniform distribution. This is done to ensure sufﬁcient

exploration of all recovery algorithms irrespective of the

reward accrued by them at the initial phase. On the other hand,

with probability (1 − γ), each action is chosen randomly with

a probability depending on its accumulated score.

B. Detailed discussion on actions and feedback

In action-1 CosaMP, the following steps are involved:
• Identiﬁcation: Steps 1 and 2 provide the signal proxy for the residual error vector and ﬁnd out the indices of largest 2k entries.
• Support Merger: Step 3 merges the set of new indices with set of indices of current Fourier coefﬁcients approximation.
• Estimation: Step 4 computes the least squares to obtain the approximate Fourier coefﬁcients on merged set R.
• Pruning: Steps 5 and 6 maintain only largest k Fourier coefﬁcients obtained from least square approximation.
Details of each action 2,3 and 4 are mentioned in the algorithm.
We choose the following feedback criterion i.e. fai = 1 for each action:
• Action 1: It is quite intuitive that if there is no attack then the l2 norm of the residual error will be upper bounded by just recovery error at the end of the algorithm. Hence, we set its upper bound equal to the parameter α. The maximum absolute value in the residual vector is upper bounded by the parameter m. If these inequalities are satisﬁed in each iteration, then the algorithm concludes that there is no attack, hence fai = 1. We can also calculate the Mahalanobis distance (MD) [34] using (12), between the residual error of a test image and that of the clean images. This is used as another alternative criterion to determine whether the image is malicious or not by comparing with some threshold parameter θ.

M D2 = (v − mˆ )T C−1(v − mˆ )

(12)

Algorithm 1: CAD algorithm

Input: The measurement matrix A = F −1, test image

vector y, dimension of image vector N , sparsity

parameters τ and k, perturbation levels η, η and η ,

Mahalanobis Distance (MD) threshold θ, stopping time

T , stopping time threshold parameters ∆ and δ, and

also α, β, m, γ ∈ (0, 1), λ > 0 σ > 0.

Initialisation: Set Cumulative score

Sai (0) = 0∀i ∈ {1, 2, 3, 4}, Fourier coefﬁcients xˆ0 = 0, residual error v0 = y and pai (1) = 1/4 for all
actions in A = {a1, a2, a3, a4}
Result: xˆ which is k sparse approximation of Fourier

coefﬁcients

Actions:

• a1: Action 1

1) z ← A∗vt−1

2) Ω ← supp(z2k) 3) R ← Ω ∪ supp(xˆt−1) 4) b|R ← A†Ry
5) b|Rc ← 0 6) Return: xˆt ← bk

• a2: Action 2 Return: xˆt ← arg min
z∈CN
• a3: Action 3 Return: xˆt ← arg min
z∈CN
• a4: Action 4 Return: xˆt ← arg min
z∈CN

z 1 s.t. z 1 s.t. z 1 s.t.

Az − y 2 < τ η
Az − y 2 < η √
Az − y 2 < N η

for t = 1, ..., T do

1) Select action ai, i ∈ {1, 2, 3, 4} with sampling distribution pai (t) using (10).
2) Perform some more number of initial iterations of

chosen action ai compared to the last time when ai

was chosen. 3) Find top k Fourier coefﬁcients i.e. xˆt = xˆh(k) using
the output in the previous step. 4) Calculate the residual error vt ← y − Axˆt. 5) Feedback fai (t) = 1 is set if following condition holds
for the chosen action:

• a1: • a2: • a3: • a4:

vt 2 < α Or M D < θ And vt ∞ < m vt 2 > α And vt 0 < τ vt 2 > α And m < vt ∞ < β vt 2 > α And vt ∞ > β

6) Calculate reward rai (t) using (11). 7) Update cumulative score

• Sa(t) = Sa(t − 1) + rai (t), a = ai • Sa(t) = Sa(t − 1), ∀a = ai
8) if pai (t) > ∆ Or vt 2 < δ then break

Recovery method chosen = arg max Sa(T )
a
if max Sa(T ) ≤ 0 then
a
Recovery method chosen = CoSaMP

where v is the residual error of test image and mˆ and C is the mean and covariance of residual error of clean images respectively. This is reminiscent of the popular χ2 detector used in anomaly detection. • Action 2: Under l0 attack, the number of non-zero entries in its perturbation vector should be upper bounded by some parameter τ . Hence, we use the conditions vt 2 > α and vt 0 < τ . This can be explained from the fact that vt includes perturbation error along with recovery error. • Action 3: Along with the previous condition vt 2 > α, here we assume that maximum absolute perturbation in case of l2 or l1 attack is upper bounded by β and lower bounded by m. • Action 4: Checking for l∞ attack additionally requires us to verify whether the maximum residual error component which acts as a proxy for the maximum perturbation is greater than β.
Choosing an action yields a feedback status which inﬂuences the reward values as in (11) and consequently the probabilities of choosing all actions.
We numerically observed, in addition to the above feedback criteria, that the residual vector contains a large number of nonzero entries for actions 3 and 4 for adversarial grayscale images such as the MNIST dataset. Hence, in our experiments in Section V, we additionally check whether vt 0 is above a threshold.
C. Stopping criteria
CAD can be run till the maximum limit T for the number of iterations is reached. However, if either of the two conditions pai (t) > ∆ for some i ∈ {1, 2, 3, 4} and vt 2 < δ is met before that for two given threshold parameters ∆ and δ, then the iteration will stop. The condition pai (t) > ∆ means that it is optimal to choose action i with high probability, and hence no further exploration is required. The condition vt 2 < δ means that most likely the test image is clean, and hence there is no need to investigate it further.
At the end, the appropriate recovery method is chosen according to the action which achieves the maximum cumulative score. However, if the maximum score is negative at this time, then it implies that CAD is unable to clearly identify the type of attack, and hence CoSaMP is chosen as a default recovery method due to its robustness.
IV. COMPLEXITY ANALYSIS
CoSaMP has the following ﬁve steps: forming signal proxy, identiﬁcation, support merger, least square estimation and pruning. The sensing matrix A = F −1 has dimension N × N and sparsity k. Hence, following standard matrix vector multiplication, time complexity for each of the ﬁve steps [11] are obtained as O(N 2), O(N ), O(k), O(kN ), O(k) respectively. Hence, CoSaMP has time complexity O(N 2) for each iteration t. For actions 2,3 and 4, we need to solve l1 minimization problem with different constraints, which can be solved efﬁciently by a standard convex optimization solver in polynomial time O(p(N )).

For any action, choosing the top k Fourier coefﬁcients is similar to the CoSaMP pruning step, and it can be done by a sorting algorithm in O(k log k) time. The number of operations required to calculate the residual error vt = y − Axˆt for a k sparse vector xˆt is O(kN ). Calculating various norms such as l2, l0 and l∞ require O(N ) each time. Also, the number of iterations is upper bounded by T .
Hence, the overall computational complexity of CAD will be of O(T N 2 + T p(N )).
V. EXPERIMENTS
We conducted our experiments on MNIST [35] and CIFAR10 [36] data sets for pixels lying in between [0, 1]. Discrete Cosine Transform (DCT) domain is used in experiments to get sparse coefﬁcients. We consider only white box attacks since the attacker in a black box attack has access to much less information than a white box attacker, and hence is less effective in general. All experiments were performed in Google Colab.
A. Attack setup
Foolbox [37] is an open source library available in python that can exploit the vulnerabilities of DNNs and generate various malicious attacks. All our evaluations are done using the 2.3.0 version of Foolbox library. We evaluate our compressive sensing based adaptive defense (CAD) against ﬁve major state-of-the-art white box adversarial attacks. They are projected gradient descent (PGD) [12], basic iterative method (BIM) [6], fast gradient sign method (FGSM) [7], Carlini Wagner(L2)(CW) attack [13] and Jacobian saliency map attack (JSMA) [14]. In the PGD attack, 40 iterations steps with random start are used in Foolbox. For the C&W attack, we use 10,000 iteration steps with a learning rate of 0.01. In the BIM attack, the number of iterations is set to 10 and limit on perturbation size is set to 0.3. We use the default parameters of foolbox library for the FGSM attack. In JSMA attack maximum iteration is set to 2000 and perturbation size in l0 norm is set to 20 and 35 for MNIST and CIFAR-10 respectively. All attacks used in this are bounded under l∞ norm with perturbation size = 0.3 and = 8/255 for MNIST and CIFAR-10 respectively.
As the authors of [26] observed that data sets such as MNIST (28 × 28) and CIFAR-10 (32 × 32) are too low dimensional to exhibit a diverse frequency spectrum. Hence, we do not test our algorithm against low frequency adversarial perturbation attacks.
B. Training and testing setup
For training, we use clean, compressed, reconstructed images using only top k DCT coefﬁcients. Then we test the DNN based classiﬁer against perturbed images (without any reconstruction) and note down its adversarial accuracy and loss. Then we employ our proposed CAD algorithm to reconstruct the adversarial images to obtain corrected classiﬁcation accuracy and loss for each attack.

The model architecture used for MNIST is described in Table I. We use an RMSprop optimizer in Keras with crossentropy loss for MNIST. For CIFAR-10 we use ResNet (32 Layers) [38] model having Adam optimizer with cross-entropy loss having batch size = 128 and epoch = 50. We randomly choose 7000 and 2050 images for MNIST and CIFAR-10 respectively from the training set, and train the classiﬁer with its reconstructed and compressed (reconstructed by taking top k DCT coefﬁcients of the image) images. In MNIST, we take 1000 corrupted images randomly from the test set for each attack. Since 3 channels are available in CIFAR-10, attacks are much expensive to execute and time complexity is O(3T N 2 + 3T p(N )). Hence, we choose only 250 images randomly from the test set for each attack to evaluate our CAD algorithm.

TABLE I: MNIST model architecture with batch size = 128, epochs = 15

Layer 1 2 3 4 5 6 7

Type Convolution Convolution Convolution Max pooling Fully connected Fully connected Fully connected

Properties 32 channels, 3x3 Kernel, activation=Relu 64 channels, 3x3 Kernel, activation=Relu 64 channels, 3x3 Kernel, activation=Relu
2x2, Dropout = 0.25, activation=Relu 128 neurons, activation=Relu
64 neurons, activation=Relu, Dropout = 0.5 10 neurons, activation = Softmax

Various parameters used in the algorithm are as follows:
• MNIST: k = 80, α = 8, β = 5, m = 1.8, τ = 15, θ = 65, γ = 0.07, σ = 1.01, λ = 1.25, different perturbation levels η = 0.3, η = 0.15 and η = 0.04.
• CIFAR-10: k = 300, α = 7, β = 2.8, m = 1.4, τ = 35, Mahalanobis distance threshold [34] parameter θ is equal to 3.3, 3, 3.2 respectively for each channel, γ = 0.45, σ = 1.0, λ = 5, different perturbation levels η = 0.5, η = 0.05 and η = 0.05.
Stopping criterion parameters are ∆ = 0.8 and δ = 2. For action-2, although we mention in the algorithm that the number of non-zero entries should be less than τ to satisfy the feedback condition, there will be some recovery error components in practice. Hence, we count the number of entries greater than a threshold 0.5 in the residual error vector, instead of exactly counting the number of non-zero entries. In CIFAR10, we run our algorithm channel-wise to obtain reconstructed coefﬁcients.

C. Clean data accuracy and reverse engineering attack
It has been observed that defenses that employ adversarial training suffer from the problem of clean data accuracy, i.e., the classiﬁers trained with adversarial images perform poorly for clean images. In order to address this problem, we evaluate the cross-entropy loss and classiﬁcation accuracy of our algorithm on 10,000 uncompressed, clean test images in the ﬁrst row of Table II and III for both MNIST and CIFAR-10. Our results show that our trained model using reconstructed and compressed images works effectively in classifying the uncompressed clean images, compared to the competing algorithms.

Reverse engineering attacks allow the attacker to determine the decision rule by monitoring the output of the classiﬁer for sufﬁcient number of query images [39]. CAD algorithm randomly chooses the recovery algorithm based on the nature of the recovery error, hence it is completely nondeterministic. In order to impart more uncertainty to the recovered coefﬁcients and confuse the attacker, one can randomly initialize xˆ0 instead of initializing it with all zero vectors. Hence, generating a reverse engineered attack for our proposed CAD algorithm becomes difﬁcult.
D. Classiﬁcation accuracy
For comparison, we take 5 state-of-the-art defenses proposed in recent years:
1) CRD [19]: We choose CRD defense for comparison since it is also based on compressive sensing. The authors of [19] provided different recovery methods of Fourier coefﬁcients for each norm attack, but did not test their defense against gradient based attacks like FGSM, PGD since these two attacks do not yield any norm condition.
2) Madry et al. defense [12]: It is min-max optimization based defense using adversarial training to combat adversarial attack.
3) Yu et al. defense [20]: This also is based on adversarial training, using decision surface geometry as parameter.
4) CMT [28]: This defense uses collaborative learning to increase the complexity in searching adversarial images for the attacker. It is applicable for non-targeted, blackbox and greybox attacks.
5) Bafna et al. defense [18]: This defense is based on compressive sensing techniques and is only applicable for l0 attack.
The classiﬁcation accuracy of the all defenses are computed for targeted white box adversarial images (since white box attack is more effective), except for the CMT [28] defense scheme which is evaluated for non-targeted black box adversarial images. Experimental results for cross-entropy loss and classiﬁcation accuracy of both adversarial and clean images for each attack are provided in Tables II and III. It is clear from the tabulated results that CAD algorithm is outperforming CRD except in CW(L2) attack in MNIST dataset where the performance is slightly worse. The defenses proposed in [12] and [20] perform well for data sets having grayscale images (e.g., MNIST), but exhibit very low classiﬁcation accuracy for data sets having colored images (e.g., CIFAR-10). Finally, we compare CAD algorithm with CMT [28]. Though white box attack usually performs better than black box attack, proposed CAD algorithm against white box attack achieves much better classiﬁcation accuracy compared to CMT under black box attack, for the MNIST data set. On the other hand, CAD algorithm against white box attack achieves comparable classiﬁcation accuracy compared to CMT under black box attack, for the CIFAR-10 data set. It is to be noted that CMT exhibits extremely poor classiﬁcation accuracy against CW(L2) attack for both MNIST and CIFAR10 data sets. Since PGD and BIM attacks are very similar,

CW-L2 FGSM PGD

BIM

JSMA

Fig. 1: Reconstruction quality of MNIST images against various attacks. The ﬁrst row shows the adversarial images and its reconstructed image is shown in second row.

CW-L2 FGSM PGD

BIM

JSMA

Fig. 2: Reconstruction quality of CIFAR-10 images against various attacks. The ﬁrst row shows the adversarial images and its reconstructed image is shown in second row.

many of the defence papers demonstrate their performance against only one of PGD and BIM, as seen in Tables II and III.
In Table IV, we compare the existing defenses against l0 norm attack JSMA. It can be observed that CAD algorithm signiﬁcantly outperforms others for MNIST. For CIFAR10, CAD algorithm achieves high classiﬁcation accuracy compared to CRD [19] but poorer accuracy compared to CMT [28]; however, one should remember that here CAD algorithm is evaluated against white box JSMA attack, while CMT is evaluated against black box JSMA attack.
We also illustrate the reconstruction quality of randomly selected images (after performing inverse DCT on recovered coefﬁcients) for each attack in Figures 1 & 2. It is observed that the reconstructed images have high classiﬁcation accuracy.
E. Obfuscated gradients
Most of recently proposed defenses are suffer from the problem of obfuscated gradients [40], [41]; the proposed defense often does not use accurate gradients while generating adversarial images for the testing phase. Here we argue that CAD algorithm does not cause gradient masking, the reasons being the following:
1) Iterative attacks are usually superior to single step attacks. In order to verify this, we randomly select 15 images on which foolbox can craft a perturbed image. We choose FGSM and PGD as single step attack and iterative attack respectively, evaluate each image separately on our model, and plot the cross-entropy loss for each image. From Figure 3 it can be seen clearly that, for each image, cross-entropy loss is always less

Fig. 3: Cross entropy loss under FGSM and PGD attack for each 15 Image indexes. Top: MNIST, Bottom: CIFAR-10
for PGD attack compared to FGSM attack , for both MNIST and CIFAR-10. This matches the well-known fact that iterative attack is superior to single step attack. 2) We apply unbounded distortion for both FGSM and PGD and observe that each image is misclassiﬁed. Hence, the attack exhibits 100% success rate, which is another desired condition.
VI. CONCLUSION
In this paper, we have proposed a compressive sensing based adaptive defense (CAD) scheme. CAD algorithm chooses an appropriate recovery algorithm in each iteration using the multi-armed bandit theory, based on the observed nature of the residual error. While the standard basis pursuit algorithm was previously used to mitigate l2 attack, we have proposed a modiﬁed basis pursuit with novel constraint to combat l0 and l∞ attacks, and also have provided their performance bounds. The proposed CAD algorithm achieves excellent classiﬁcation accuracy with low computational complexity and low memory requirement for both white box gradient attacks and norm attacks.
While our paper combines compressive sensing and multiarmed bandit techniques for adversarial image classiﬁcation, this approach can be adopted even for classifying and detecting adversarial videos. However, computation complexity will be a major challenge for videos, and that can be alleviated to some extent by opportunistically sampling frames and applying tools

TABLE II: Experimental results of various attack on MNIST and comparison with state of the art defenses

Attack
No Attack FGSM PGD BIM CW(L2)

Adversarial Acc.(%)
1.2 0.0 0.0 1.5

Adversarial Loss
0.94 1.02 1.054 2.21

CAD Corrected Acc.(%) 98.45 93.9 99.75 99.7 86.46

CAD Corrected Loss 0.386 0.69 0.001 0.014 0.914

CRD Acc.(%) [19]
99.17 74.7 92.4

Madry et al. Acc.(%) [12]
98.8 95.6 93.2 94

Yu et al. Acc.(%) [20]
98.4 91.6 88.1 89.2

CMT (Black Box Setting) Acc.(%) [28] 99.5 84.2 79.5 1.2

TABLE III: Experimental results of various attack on CIFAR-10 and comparison with state of the art defenses

Attack
No Attack FGSM PGD BIM CW(L2)

Adversarial Acc.(%)
0.0 0.0 0.0 0.45

Adversarial Loss
1.602 1.485 1.487 1.635

CAD Corrected Acc.(%) 84.41 75.33 76.0 78.66 75.53

CAD Corrected Loss 0.916 1.201 1.021 0.926 1.007

CRD Acc.(%) [19]
84.9 49.4 72.3

Madry et al. Acc.(%) [12]
87.3 56.1 45.8 46.8

Yu et al. Acc.(%) [20]
83.1 68.5 62.7 60.5

CMT (Black Box Setting) Acc.(%) [28] 80.1 81.8 80.2 4.5

Dataset
MNIST CIFAR-10

TABLE IV: Experimental result against l0 norm attack JSMA on MNIST and CIFAR-10

Perturbation size (t)
20 35

Adversarial Acc.(%)
0.0 12.4

Adversarial Loss
2.049 10.344

CAD Corrected Acc.(%) 94.9 71.6

CAD Corrected Loss 0.1483 7.01

CRD Acc.(%) [19]
55.9 67.3

Bafna et al. Acc.(%) [18]
90.8 -

CMT (Black Box Setting) Acc.(%) [28] 81.3 79.3

similar to this paper on them. Tools from restless bandit theory [11] can also be useful for videos. Thus, our paper opens the possibility of starting a new research domain on adversarial [12] image and video detection using theoretical tools, which has traditionally seen mostly DNN and heuristic based efforts.
[13]
REFERENCES

[1] Akash Kumar Gupta. https:// github.com/ AkashKumarGupta07/ [14]

Compressive-Sensing-Based-Adaptive-DefenceAgainst-Adversarial-Images.

git.

[2] Shuang Liang, Huixiang Liu, Yu Gu, Xiuhua Guo, Hongjun Li, Li Li,

Zhiyuan Wu, Mengyang Liu, and Lixin Tao. Fast automated detection [15]

of covid-19 from medical images using convolutional neural networks.

Communications Biology, 4(1):1–13, 2021.

[3] Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we ready for [16] autonomous driving? the kitti vision benchmark suite. In 2012 IEEE

Conference on Computer Vision and Pattern Recognition, pages 3354–

3361. IEEE, 2012.

[4] Edwin Jose, M Greeshma, Mithun TP Haridas, and MH Supriya. Face

recognition based surveillance system using facenet and mtcnn on jetson [17] tx2. In 2019 5th International Conference on Advanced Computing &

Communication Systems (ICACCS), pages 608–613. IEEE, 2019.

[5] Mingfu Xue, Chengxiang Yuan, Can He, Jian Wang, and Weiqiang Liu. [18] Naturalae: Natural and robust physical adversarial examples for object

detectors. Journal of Information Security and Applications, 57:102694,

2021.

[19]

[6] Alexey Kurakin, Ian Goodfellow, Samy Bengio, et al. Adversarial

examples in the physical world, 2016.

[7] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining

and harnessing adversarial examples. arXiv preprint arXiv:1412.6572,

2014.

[20]

[8] Emmanuel J Candes, Justin K Romberg, and Terence Tao. Stable

signal recovery from incomplete and inaccurate measurements. Communications on Pure and Applied Mathematics: A Journal Issued [21]

by the Courant Institute of Mathematical Sciences, 59(8):1207–1223,

2006. [9] Simon Foucart and Holger Rauhut. An invitation to compressive sensing. [22]

In A mathematical introduction to compressive sensing, pages 1–39.

Springer, 2013.

[10] Peter Auer, Nicolo Cesa-Bianchi, Yoav Freund, and Robert E Schapire. [23]

Gambling in a rigged casino: The adversarial multi-armed bandit

problem. In Proceedings of IEEE 36th Annual Foundations of Computer

Science, pages 322–331. IEEE, 1995.

[24]

Deanna Needell and Joel A Tropp. Cosamp: Iterative signal recovery from incomplete and inaccurate samples. Applied and computational harmonic analysis, 26(3):301–321, 2009.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083, 2017.
Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In 2017 ieee symposium on security and privacy (sp), pages 39–57. IEEE, 2017.
Nicolas Papernot, Patrick McDaniel, Somesh Jha, Matt Fredrikson, Z Berkay Celik, and Ananthram Swami. The limitations of deep learning in adversarial settings. In 2016 IEEE European symposium on security and privacy (EuroS&P), pages 372–387. IEEE, 2016.
Yanpei Liu, Xinyun Chen, Chang Liu, and Dawn Song. Delving into transferable adversarial examples and black-box attacks. arXiv preprint arXiv:1611.02770, 2016.
Pin-Yu Chen, Huan Zhang, Yash Sharma, Jinfeng Yi, and Cho-Jui Hsieh. Zoo: Zeroth order optimization based black-box attacks to deep neural networks without training substitute models. In Proceedings of the 10th ACM workshop on artiﬁcial intelligence and security, pages 15–26, 2017.
Wieland Brendel, Jonas Rauber, and Matthias Bethge. Decision-based adversarial attacks: Reliable attacks against black-box machine learning models. arXiv preprint arXiv:1712.04248, 2017.
Mitali Bafna, Jack Murtagh, and Nikhil Vyas. Thwarting adversarial examples: An l_0-robustsparse fourier transform. arXiv preprint arXiv:1812.05013, 2018.
Jasjeet Dhaliwal and Kyle Hambrook. Compressive recovery defense: Defending neural networks against 2, ∞, and 0 norm attacks. In 2020 International Joint Conference on Neural Networks (IJCNN), pages 1–8. IEEE, 2020.
Fuxun Yu, Chenchen Liu, Yanzhi Wang, Liang Zhao, and Xiang Chen. Interpreting adversarial robustness: A view from decision surface in input space. arXiv preprint arXiv:1810.00144, 2018.
Jiahuan Ji, Baojiang Zhong, and Kai-Kuang Ma. Multi-scale defense of adversarial images. In 2019 IEEE International Conference on Image Processing (ICIP), pages 4070–4074. IEEE, 2019.
MaungMaung AprilPyone and Hitoshi Kiya. Encryption inspired adversarial defense for visual classiﬁcation. arXiv preprint arXiv:2005.07998, 2020.
Darpan Kumar Yadav, Kartik Mundra, Rahul Modpur, Arpan Chattopadhyay, and Indra Narayan Kar. Efﬁcient detection of adversarial images. arXiv preprint arXiv:2007.04564, 2020.
Reuben Feinman, Ryan R Curtin, Saurabh Shintre, and Andrew B

Gardner. Detecting adversarial samples from artifacts. arXiv preprint arXiv:1703.00410, 2017. [25] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013. [26] Yash Sharma, Gavin Weiguang Ding, and Marcus Brubaker. On the effectiveness of low frequency perturbations. arXiv preprint arXiv:1903.00073, 2019. [27] Chuan Guo, Jared S Frank, and Kilian Q Weinberger. Low frequency adversarial perturbation. arXiv preprint arXiv:1809.08758, 2018. [28] Derui Wang, Chaoran Li, Sheng Wen, Surya Nepal, and Yang Xiang. Defending against adversarial attack towards deep neural networks via collaborative multi-task training. IEEE Transactions on Dependable and Secure Computing, 2020. [29] H Vincent Poor. An introduction to signal detection and estimation. Springer Science & Business Media, 2013. [30] Irched Chafaa, E Veronica Belmega, and Mérouane Debbah. Exploiting channel sparsity for beam alignment in mmwave systems via exponential learning. In 2020 IEEE International Conference on Communications Workshops (ICC Workshops), pages 1–6. IEEE, 2020. [31] David L Donoho and Michael Elad. On the stability of the basis pursuit in the presence of noise. Signal Processing, 86(3):511–532, 2006. [32] Mahdi Cheraghchi, Venkatesan Guruswami, and Ameya Velingker. Restricted isometry of fourier matrices and list decodability of random linear codes. SIAM Journal on Computing, 42(5):1888–1914, 2013. [33] Charles L Fefferman. The uncertainty principle. Bulletin (New Series) of the American Mathematical Society, 9(2):129–206, 1983. [34] Roy De Maesschalck, Delphine Jouan-Rimbaud, and Désiré L Massart. The mahalanobis distance. Chemometrics and intelligent laboratory systems, 50(1):1–18, 2000. [35] Yann LeCun. The mnist database of handwritten digits. http://yann. lecun. com/exdb/mnist/, 1998. [36] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009. [37] Jonas Rauber, Wieland Brendel, and Matthias Bethge. Foolbox: A python toolbox to benchmark the robustness of machine learning models. arXiv preprint arXiv:1707.04131, 2017. [38] Sasha Targ, Diogo Almeida, and Kevin Lyman. Resnet in resnet: Generalizing residual architectures. arXiv preprint arXiv:1603.08029, 2016. [39] Florian Tramèr, Fan Zhang, Ari Juels, Michael K Reiter, and Thomas Ristenpart. Stealing machine learning models via prediction apis. In 25th {USENIX} Security Symposium ({USENIX} Security 16), pages 601–618, 2016. [40] Anish Athalye, Nicholas Carlini, and David Wagner. Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples. In International conference on machine learning, pages 274–283. PMLR, 2018. [41] Nicholas Carlini, Anish Athalye, Nicolas Papernot, Wieland Brendel, Jonas Rauber, Dimitris Tsipras, Ian Goodfellow, Aleksander Madry, and Alexey Kurakin. On evaluating adversarial robustness. arXiv preprint arXiv:1902.06705, 2019.

