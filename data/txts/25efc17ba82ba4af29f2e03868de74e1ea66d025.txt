Multilingual Multimodal Pre-training for Zero-Shot Cross-Lingual Transfer of Vision-Language Models
Po-Yao Huang13∗, Mandela Patrick23∗, Junjie Hu1, Graham Neubig1, Florian Metze3, Alexander Hauptmann1
1School of Computer Science, Carnegie Mellon University 2Visual Geometry Group, University of Oxford 3Facebook AI
{poyaoh,junjieh,gneubig,alex}@cs.cmu.edu, {mandelapatrick,fmetze}@fb.com

arXiv:2103.08849v3 [cs.CV] 15 Apr 2021

Abstract
This paper studies zero-shot cross-lingual transfer of vision-language models. Specifically, we focus on multilingual text-tovideo search and propose a Transformerbased model that learns contextualized multilingual multimodal embeddings. Under a zero-shot setting, we empirically demonstrate that performance degrades signiﬁcantly when we query the multilingual text-video model with non-English sentences. To address this problem, we introduce a multilingual multimodal pre-training strategy, and collect a new multilingual instructional video dataset (MultiHowTo100M) for pre-training. Experiments on VTT show that our method signiﬁcantly improves video search in non-English languages without additional annotations. Furthermore, when multilingual annotations are available, our method outperforms recent baselines by a large margin in multilingual text-to-video search on VTT and VATEX; as well as in multilingual text-to-image search on Multi30K. Our model and Multi-HowTo100M is available at http://github.com/berniebear/ Multi-HT100M
1 Introduction
One of the key challenges at the intersection of computer vision (CV) and natural language processing (NLP) is building versatile vision-language models that not only work in English, but in all of the world’s approximately 7,000 languages. Since collecting and annotating task-speciﬁc parallel multimodal data in all languages is impractical, a framework that makes vision-language models generalize across languages is highly desirable.
One technique that has shown promise to greatly improve the applicability of NLP models to new languages is zero-shot cross-lingual transfer, where models trained on a source language are applied
∗Equal contribution.

as-is to a different language without any additional annotated training data (Ta¨ckstro¨m et al., 2012; Klementiev et al., 2012; Cotterell and Heigold, 2017; Chen et al., 2018; Neubig and Hu, 2018). In particular, recent techniques for cross-lingual transfer have demonstrated that by performing unsupervised learning of language or translation models on many languages, followed by downstream task ﬁne-tuning using only English annotation, models can nonetheless generalize to a non-English language (Wu and Dredze, 2019a; Lample and Conneau, 2019; Huang et al., 2019a; Artetxe et al., 2020; Hu et al., 2020). This success is attributed to the fact that many languages share a considerable amount of underlying vocabulary or structure. At the vocabulary level, languages often have words that stem from the same origin, for instance, “desk” in English and “Tisch” in German both come from the Latin “discus”. At the structural level, all languages have a recursive structure, and many share traits of morphology or word order.
For cross-lingual transfer of vision-language models, the visual information is clearly an essential element. To this end, we make an important yet under-explored step to incorporate visual-textual relationships for improving multilingual models (Devlin et al., 2019; Artetxe et al., 2020). While spoken languages could be different, all humans share similar vision systems, and many visual concepts can be understood universally (Sigurdsson et al., 2020; Zhang et al., 2020). For example, while is termed “cat” for an English speaker and “chat” for a French speaker; they understand similarly. We leverage this observation to learn to associate sentences in different languages with visual concepts for promoting cross-lingual transfer of visionlanguage models.
In this work, we focus on multilingual text-tovideo search tasks and propose a Transformerbased video-text model to learn contextual mul-

tilingual multimodal representations. Our vanilla model yields state-of-the-art performance in multilingual text→video search when trained with multilingual annotations. However, under the zero-shot setting, rather surprisingly, there is a signiﬁcant performance gap between English and non-English queries (see §5.5 for details). To resolve this problem, motivated by recent advances in large-scale language model (Artetxe et al., 2020) and multimodal pre-training (Lu et al., 2019; Miech et al., 2019; Patrick et al., 2020), we propose a multilingual multimodal pre-training (MMP) strategy to exploit the weak supervision from large-scale multilingual text-video data. We construct the Multilingual-HowTo100M dataset, that extends the English HowTo100M (Miech et al., 2019) dataset to contain subtitles in 9 languages for 1.2 million instructional videos.
Our method has two important beneﬁts. First, compared to pre-training on English-video data only, pre-training on multilingual text-video data exploits the additional supervision from a variety of languages, and therefore, enhances the search performance on an individual language. Second, by exploiting the visual data as an implicit “pivot” at scale, our methods learns better alignments in the multilingual multimodal embedding space (e.g., “cat”- -“chat”), which leads to improvement in zero-shot cross-lingual transfer (e.g., from “cat”-
to “chat”- ) of vision-language models.
In our experiments on VTT (Xu et al., 2016) and VATEX (Wang et al., 2019), our method yields state-of-the-art English→video search performance. For zero-shot cross-lingual transfer, the proposed multilingual multimodal pre-training improves English-video pre-training by 2 ∼ 2.5 in average R@1 across 9 languages. Additionally, when trained with in-domain multilingual annotations as other baselines, our method outperforms them by a large margin in multilingual text→video search on VATEX and text→image search on Multi30K (Elliott et al., 2016).
To summarize, we make the following contributions: (1) We propose a transformer-based videotext model that learns contextual multilingual multimodal representations (§3.1). (2) We empirically demonstrate that vision-language models, unlike NLP models, have limited zero-shot cross-lingual transferrability. (§5.5). (3) We introduce the multilingual multimodal pre-training strategy and construct a new Multi-HowTo100M dataset (§4) for

pre-training to improve zero-shot cross-lingual capability of vision-language models. (4) We demonstrate the effectiveness of our approach, by achieving state-of-the-art multilingual text→video search performance in both the zero-shot (§5.5) and fully supervised setup (§5.6).
2 Related Work
Cross-lingual representations. Early work on learning non-contextual cross-lingual representations used either parallel corpora (Gouws and Søgaard, 2015; Luong et al., 2015) or a bilingual dictionary to learn a transformation (Faruqui and Dyer, 2014; Mikolov et al., 2013). Later approaches reduced the amount of supervision using self-training (Artetxe et al., 2017). With the advances in monolingual transfer learning (McCann et al., 2017; Howard and Ruder, 2018; Peters et al., 2018; Devlin et al., 2019), multilingual extensions of pre-trained encoders have been proven effective in learning deep contextual cross-lingual representations (Eriguchi et al., 2017; Lample and Conneau, 2019; Wu and Dredze, 2019b; Siddhant et al., 2020; Pires et al., 2019; Pfeiffer et al., 2020). We extend prior work to incorporate visual context. Video-text representations. The HowTo100M dataset (Miech et al., 2019) has attracted significant interest in leveraging multimodal pre-training for text→video search (Korbar et al., 2020), captioning (Iashin and Rahtu, 2020), and unsupervised translation via image-based (Sur´ıs et al., 2020; Huang et al., 2020b) and video-based (Sigurdsson et al., 2020) alignment. This work studies a challenging and unexplored task: Zero-shot cross-lingual transfer of vision-language models. Unlike prior image/video-text work that utilizes RNN (Dong et al., 2019; Chen et al., 2020a; Burns et al., 2020; Kim et al., 2020) and inter-modal contrastive objectives (Sigurdsson et al., 2020; Liu et al., 2019; Huang et al., 2019b; Patrick et al., 2021), we employ Transformers to learn contextual multilingual multimodal representations and uniquely models cross-lingual instances. Moreover, we build Multi-HowTo100M, the largest text-video dataset for multilingual multimodal pre-training. Cross-lingual Transfer. Cross-lingual transfer has proven effective in many NLP tasks including dependency parsing (Schuster et al., 2019), named entity recognition (Rahimi et al., 2019), sentiment analysis (Barnes et al., 2019), document classiﬁcation (Schwenk and Li, 2018), and question an-

a

GRU

𝑐#

𝑐!|#

a man performs

Φ𝑥

Intra-modal

Transformer Pooling Transformer Pooling

AAAtAtttttetetenenntntititoioiononnn

AAAtAtttttetetenenntntititoioiononn1n11DDD--C-CCNNNNNN 111DDD--C-CCNNNNANANAtAtttttetetenenntntitiotioionoAnAnAntAtttttetetenenntntititoioiononnn AAAtAtttttetetenenntntitiotioiononnn Linear111DDD--C-CCNNNNNN

Linear

Linear

AAAAttttttetetenenntntititoioiononnn

Linear

AAAAttttttetetenenntntititoioiononnn

111DDD--C-CCNNNNNN

Attention

111DDD--C-CCNNNNNANAAttttteteennnttitioioAonAnAnAttttttetetenenntntititoioiononnn

AAAtAtttttetetenenntntitiotioiononnn 111DDD--C-CCNNNNNN

man shotGpRuaUt

mBERT TP GRU

𝑐

𝑐#

(TP)

(TP)

𝑒#

𝑥

𝑒!

!

Ψ𝑣

3D

AAAtAtttttetetenenntntitiotioiononnn

Linear

Linear

AAAtAtttttetetenenntntitiotioiononnn …

… …

a

GRU man GRU

Cross-lingual

CNN

shoutn homGbRreUrealiza

mBERT TP

𝑐" Inter-modal

TP 3D-CNN 3D

man

lanGzRaUmiento

put

GRU

de

bala 𝑦

Φ𝑦
𝑒"

𝑒#

𝑣CNN

time

shot

GRU

Contrastive attraction

Contrastive repulsio3nD

Time

put

GRU

CNN

shot FiGguRUre 1: An overview of our video-text model for learning contextual multilingual multimodal representations. Time

We utilize intra-modal, inter-modal, and conditional cross-lingual contrastive objectives to align (x, v, y) where

put x GanRdU y are the captions or transcriptions in different languages of a video v. TP: Transformer pooling head.

Time

swering (Lewis et al., 2020; Artetxe et al., 2020). Recently, XTREME (Hu et al., 2020) was proposed to evaluate the cross-lingual transfer capabilities of multilingual representations across a diverse set of NLP tasks and languages. However, a comprehensive evaluation of multilingual multimodal models on zero-shot cross-lingual transfer capabilities is still missing. To our best knowledge, we are the ﬁrst work that investigates and improves zero-shot cross-lingual transfer of vision-language models.
3 Method
We consider the problem of learning multilingual multimodal representations from a corpus C of video-text pairs {(xi, vi)}Ci=1, where vi is a video clip and xi is its corresponding text (caption or transcription) that is written in one of K languages. Our goal is to learn a shared multilingual text encoder cx = Φ(x) and a video encoder cv = Ψ(v), both of which project the input to a shared Ddimensional embedding space cv, ct ∈ RD, where semantically similar instances (i.e., paired (xi, vi)) are closer to each other than the dissimilar ones (i.e., (xi, vj), i = j). In the following, we denote a batch of multilingual text-video samples as B = {(xi, vi)}Bi=1} where B ⊂ C.
3.1 Multilingual Multimodal Transformers
Figure 1 gives an overview of the proposed method. Our text encoder consists of a multilingual Transformer (e.g. multilingual BERT (Devlin et al., 2019)) and a text Transformer pooling head (explained below). Similarly, our video encoder consists of a 3D-CNN (e.g. R(2+1)D network (Tran et al., 2018)) and a video Transformer pooling head. We use these multilingual multimodal Transformers to encode text and video for alignment.
Unlike prior multilingual text-image mod-

els (Gella et al., 2017; Kim et al., 2020; Huang et al., 2019b) that utilize word embeddings and RNNs, our multilingual text encoder is built on a multilingual Transformer that generates contextual multilingual representations ex ∈ RN×D to encode a sentence x containing N words. We employ an additional 2-layer Transformer which we will call a “Transformer pooling head (TP)” as it serves as a pooling function to selectively encode variablelength sentences and aligns them with the corresponding visual content. We use the ﬁrst output token of the second Transformer layer as the ﬁnal sentence representation. Precisely, we set cx = Trans(x2)(query=key=value=ex)[0] where Trans(x2) is a 2-layer stack of Transformers (Vaswani et al., 2017) with ex as the (query,key,value) in the multihead attention. Note that we use the same text encoder to encode sentences in all languages.
For encoding videos, our model uses pre-trained 3D-CNNs that encode spatial-temporal context in a video. For a M -second video v, we apply R(2+1)D (Tran et al., 2018) and S3D (Miech et al., 2020) networks to its frames, concatenate network outputs, and apply a linear layer to encode the visual input, ev ∈ RM×D, to our model. Similarly to the text part, we employ a two-layer Transformer as the pooling head to encode videos with different lengths into ﬁxed-length representations. Formally, we set cv = Trans(v2)(query=key=value=ev)[0]. Since videos are typically long and have a high frame rate (e.g., 30 fps), it is infeasible to update 3D-CNNs simultaneously and therefore, we use pre-extracted video features. Our model is parameterized by θ = θmBERT ∪ θTransx ∪ θTransv .
3.2 Multilingual Text-Video Alignment
For learning multimodal representations, the common practice is to minimize a contrastive objective

to map the associated (video, text) embeddings to be near to each other in a shared embedding space. The inter-modal max-margin triplet loss has been widely studied in video-text (Yu et al., 2018; Liu et al., 2019) and image-text (Kim et al., 2020; Burns et al., 2020; Huang et al., 2019b) research. In this work, we generalize and model all inter-modal, intra-modal, and cross-lingual instances with a noise contrastive estimation objective (NCE) (Gutmann and Hyva¨rinen, 2010; van den Oord et al., 2018; Chen et al., 2020b). Inter-modal NCE. Let X and V denote the subsets of the sampled sentences in multiple languages and videos in B, respectively. And let s(a, b) = aaT bb be the cosine similarity measure. We use an (intermodal) NCE objective deﬁned as:

1B

L(X , V) = −

log NCE(Φ(xi), Ψ(vi)), (1)

B

i=1

where

NCE(cx, cv) = es(cx,cv) +

es(cx ,cv )
(x ,v )∼N es(cx ,cv ) (2)

In inter-modal NCE, Linter = L(X , V), the noise

N is a set of “negative” video-text pairs sampled to

enforce the similarity of paired ones are high and

and those do not are low. Following Miech et al.

(2020), we set the negatives of (xi, vi) as other xj

and vj, j = i in B.

Intuitively, inter-modal NCE draws paired (se-

mantically similar) instances closer and pushes

apart non-paired (dissimilar) instances. Note that

we do not distinguish language types in X and the

sentences in all possible languages will be drawn

towards their corresponding videos in the shared

multilingual text-video embedding space.

Intra-modal NCE. Beyond cross-modality match-

ing, we leverage the intra-modal contrastive ob-

jective to learn and preserve the underlying struc-

ture within the video and text modality. For exam-

ple, Corgi should be closer to Husky than Balinese.

Prior image-text work (Gella et al., 2017; Huang

et al., 2019c) utilizes a triplet loss to maintain such

neighborhood relationships. Inspired by recent suc-

cess in self-supervised image and video represen-

tation learning (Yalniz et al., 2019; Ghadiyaram

et al., 2019), our model leverages intra-modal NCE

that constrains the learned representations to be

invariant against noise and to maintain the within-

modality structure simultaneously. We minimize

the following intra-modal NCE loss:

Lintra = L(X , X m) + L(V, Vm), (3)

where X m and Vm are the noised version of the original sentences and videos. For noising, we randomly mask 5% of the multilingual text tokens and video clips. We optimize our model by

min Linter + Lintra

(4)

θ

3.3 When Visually-Pivoted Multilingual Annotations Are Available

In many multilingual multimodal datasets, there

are sentences in different languages that describe a

shared visual context. For example, 10 English and

10 Chinese descriptions are available for each video

in VATEX. With these visually-pivoted (weakly

paralleled) sentences (x, y), we further revise the

contrastive objectives to leverage this additional

supervisory signal. Given a visually-pivoted cor-

pus Cp that contains all possible combination of

visually-pivoted pairs {(xi, vi, yi)}Ci=p0, we sample

batches

Bp

=

{

(x

i

,

vi

,

yi

)}

Bp i=1

,

B

p

⊂

Cp

and

re-

vise the contrastive objective as:

Linter = L(X , V) + L(Y, V)

(5)

Lintra = L(X , X m) + L(Y, Ym) + L(V, Vm)

(6)

Visual-pivoted Cross-lingual NCE. Inspired by Translation Language Modeling (TLM) in XLM (Lample and Conneau, 2019), we propose a multimodal TLM-like contrastive objective which promotes alignments of descriptions in different languages that describe the same video. We use the intuition that conditioned on a video, the descriptions (need not to be translation pairs) in different languages would likely be semantically similar. To this end, we set the cross-lingual NCE as:

Lcross = L(X |V, Y|V)

(7)

For visually-pivoted sentences, as shown in Fig. 1, we generate their representations conditioned on the video they describe. We extend the key and value of multihead attention with the additional visual content ev and generate new cx|v and cy|v for matching. Speciﬁcally, our model employs cx|v = Trans(x2)(query=ex, key=value=ex||ev)[0]. With the access to (visually-pivoted) multilingual annotations, we optimize our model by

min Linter + Lintra + Lcross

(8)

θ

00:00:37.160 --> 00:00:48.860 What it is, is a heat gun and I got this for ten bucks 是什么，它是热风枪，我花了十美元买了
00:11:36.380 --> 00:11:44.390

00:01:16.290 --> 00:01:21.210

and then pull it as tight as possible а затем потяните его как можно плотнее und dann ziehen Sie es so fest wie möglich

we just made our six-sided coaster so nous venons de faire notre caboteur à six côtés donc ce que

00:08:35.289 --> 00:08:39.300
It will also be accompanied with a little of french fries también la voy a acompañar con un poco de papas fritas von Pommes Frites können Sie es auch mit begleiten ya fries ya Kifaransa unaweza pia kuandamana nayo khoai tây chiên bạn cũng có thể đi kèm với nó

Figure 2: Video clips and the corresponding multilingual subtitles in Multi-HowTo100M.

At the inference time, we simply apply cx = Φ(x) and cv = Ψ(v) to encode multilingual text queries and videos. For text-to-video search, we sort videos according to their cosine similarity scores to the text query.
4 The Multilingual HowTo100M Dataset
As large-scale pre-training has been shown important in recent NLP and vision-language models, we construct the Multilingual HowTo100M dataset (Multi-HowTo100M) to facilitate research in multilingual multimodal learning. The original HowTo100M (Miech et al., 2019) dataset is a large-scale video collection of 1.2 million instructional videos (around 138 million clips/segments) on YouTube, along with their automatic speech recognition (ASR) transcriptions as the subtitles. For each video in HowTo100M, we crawl and collect the multilingual subtitles provided by YouTube, which either consist of user-generated subtitles or those generated by Google ASR and Translate in the absence of user-generated ones. Essentially, we collect video subtitles in 9 languages: English (en), German (de), French (fr), Russian (ru), Spanish (es), Czech (cz), Swahili (sw), Chinese (zh), Vietnamese (vi).
At the time of dataset collection (May 2020), there are 1.1 million videos available, each with subtitles in 7-9 languages. The video length ranges from 1 minute to more than 20 minutes. We utilize Multi-HowTo100M for multilingual multimodal pre-training to exploit the weak supervision from large-scale multilingual text-video data. In Fig. 2, we provide a visualization of few instances sampled in Multi-HowTo100M with the corresponding video frame, timestamp, and transcriptions in different languages. Please refer to Appendix for more details and dataset statistics.

5 Experiment
In this section, we ﬁrst describe our experimental setup (§5.1-5.3). In §5.4, we conduct ablation studies to validate the effectiveness of proposed multilingual text-video model . With the best models at hand, we investigate their zero-shot cross-lingual transferability in §5.5, where we showcase that the proposed multilingual multimodal pre-training serves as the key facilitator. We then verify the superior text→video search performance of our method under the monolingual, multilingual, and cross-modality settings in §5.6.
5.1 Evaluation Datasets
MSR-VTT (VTT) (Xu et al., 2016) contains 10K videos, where each video is annotated with 20 captions. Additionally, we created pseudomultilingual data by translating the English captions into 8 languages with off-the-shelf machine translation models.1 We use the ofﬁcial training set (6.5K videos) and validation set (497 videos). We follow the protocol in Miech et al. (2019); Liu et al. (2019) which evaluates on text→video search with the 1K testing set deﬁned by Yu et al. (2018). VATEX (Wang et al., 2019) is a multilingual (Chinese and English) video-text dataset with 35K videos. Five (en,zh) translation pairs and ﬁve nonpaired en and zh descriptions are available for each video. We use the ofﬁcial training split (26K videos) and follow the testing protocol in Chen et al. (2020a) to split the validation set equally into 1.5K validation and 1.5K testing videos. Multi30K (Elliott et al., 2016) is a multilingual extension of Flickr30K (Young et al., 2014). For each image, there are two types of annotations available: (1) One parallel (English,German,French,Czech) translation pair and (2) ﬁve English and ﬁve Ger-
1https://marian-nmt.github.io/

man descriptions collected independently. The training, validation, and testing splits contain 29K, 1K, and 1K images respectively.
5.2 Implementation Details
For the video backbone, we use a 34-layer, R(2+1)-D (Tran et al., 2018) network pre-trained on IG65M (Ghadiyaram et al., 2019) and a S3D (Miech et al., 2020) network pre-trained on HowTo100M. We pre-extract video features and concatenate the two 3D-CNN outputs to form ex ∈ RM×1024 as a video input.
For the text backbone, we use multilingual BERT (mBERT) (Devlin et al., 2019) or XLM-Robertalarge (XLM-R) (Artetxe et al., 2020), where the latter achieves near SoTA zero-shot cross-lingual transfer performance for NLP tasks. Following Hu et al. (2020), instead of using the top layer, we output the 12-th layer in XLM-R and mBERT. For vision-language tasks, we freeze layers below 9 as this setup empirically performs the best.
Our model employs a 2-layer Transformer with 4-head attention for the text and video transformer pooling (TP) modules. The embedding dimension D is set to 1024. We use the Adam (Kingma and Ba, 2015) optimizer and a 0.0002 learning rate to train our model for 16 (pre-training) and 10 (ﬁnetuning) epochs. The softmax temperature in all noise contrastive objectives is set to 0.1.
5.3 Experimental Setup
We use Multi-HowTo100M for multilingual multimodal pre-training (MMP). For each video, we randomly sample the start and end time to construct a video clip. For a video clip, we randomly sample one language type each time from 9 languages and use the consecutive ASR transcriptions that are closest in time to compose (text-video) pairs for training. For simplicity and speed purposes, we follow the training protocol of XLMR to pre-train on a multilingual corpus wihtout using translation pairs, i.e., we use multilingual text-video pairs (x, v) but no translation pairs from Multi-HowTo100M and utilize only inter- and intramodal NCE (Eq. 1-3) for MMP.
We ﬁne-tune our model on VTT, VATEX, and Multi30K to evaluate on text→video search tasks. In the zero-shot cross-lingual transfer experiments, we use only English-video data and ﬁne-tune with Eq. 1-3. We then test the model with non-English queries. When annotations in additional languages are available (by humans in VATEX and Multi30K;

Text-B Video-B R@1↑ R@5↑ R@10↑ XLM-R S3D 19.5 49.0 62.8 XLM-R R(2+1)D 19.0 49.5 63.2 XLM-R R+S 21.0 50.6 63.6 mBERT R+S 19.9 49.8 62.5
Table 1: Text and Video (B)ackbone comparison.

T layers V layers R@1↑ R@5↑ R@10↑

1

1 20.0 50.3 63.2

2

1 20.1 50.5 63.8

2

2 21.0 50.6 63.6

2∗

2∗ 20.7 50.5 63.3

4

4 20.8 50.4 63.8

Table 2: Architecture comparison. Number of multilingual multimodal transformer layers. *:Weight shar-
ing between video and text transformers.

Objective Inter Intra Cross R@1↑ R@5↑ R@10↑

Triplet Triplet NCE NCE NCE* NCE*

13.3 36.0 55.2 20.9 49.3 63.0 21.4 49.3 61.1 21.0 50.6 63.6
21.3 50.7 63.5 21.5 51.0 63.8

Table 3: Objective comparison. *Training with additional machine translated de-video and fr-video pairs.

by MT models (i.e., translate-train) in VTT), we utilize all available multilingual annotations (i.e., fully supervised) and iterate over all possible (x, v, y) pairs to train with Eq. 5-7 to demonstrate the strong performance target for evaluating zeroshot cross-lingual transfer on VTT and to compare fairly with other fully-supervised baselines in multilingual text→video search on VATEX and Multi30K. We report the standard recall at k (R@k) metrics (higher is better).
5.4 Comparison Experiments and Ablations
In this section, we ablate and compare different text/video encoders, Transformer model architectures, and learning objectives for English→video search on VTT. Text and Video Encoders. Table 1 compares different text and video encoder backbones. For the visual encoders, while R(2+1)D outperforms S3D, the simple concatenation (i.e., early-fusion) of their output features provides a 1.5 ∼ 2.0 improvement in R@1. For the text encoder, XLM-R signiﬁcantly outperforms mBERT. Transformer Pooling. Table 2 compares various conﬁgurations of the proposed Transformer pooling module. We observe that a simple 2-layer Transformer achieves the best performance. Weight

Model

en de f r cs zh ru vi sw es Avg↑

mBERT mBERT-MP mBERT-MMP XLM-R XLM-R-MP XLM-R-MMP

19.9 11.1 11.6 8.2 6.9 7.9 2.7 20.6 11.3 11.9 8.0 7.1 7.7 2.5 21.8 15.0 15.8 11.2 8.4 11.0 3.7 21.0 16.3 17.4 16.0 14.9 15.4 7.7 23.3 17.4 18.5 17.1 16.3 17.0 8.1 23.8 19.4 20.7 19.3 18.2 19.1 8.2

1.4 12.0 9.1 1.1 12.5 9.2 3.4 15.1 11.7 5.7 17.3 14.7 6.2 18.5 15.8 8.4 20.4 17.5

mBERT + translated VTT

19.6 18.2 18.0 16.9 16.2 16.5 8.4 13.0 18.5 16.1

mBERT-MMP + translated VTT 21.5 19.1 19.8 18.3 17.3 18.3 8.9 14.1 20.0 17.4

XLM-R + translated VTT

21.5 19.6 20.1 19.3 18.9 19.1 10.3 12.5 18.9 17.8

XLM-R-MMP + translated VTT 23.1 21.1 21.8 20.7 20.0 20.5 10.9 14.4 21.9 19.4

Table 4: Recall@1 of multilingual text→video search on VTT. Upper: Zero-shot cross-lingual transfer. Lower: Performance with synthesized pseudo-multilingual annotations for training. MMP: multilingual multimodal pretraining on Multi-HowTo100M. MP: Multimodal (English-Video) pre-training on HowTo100M.

XLM-R

mBERT

XLM-R

mBERT

24

20

23

18

22

16

21

14

20

12

19 None

10

en

en+de en+de+fr All Langs.

None

en

en+de en+de+fr All Langs.

(a) English→Video (b) Zero-shot German→Video

Figure 3: R@1 trends in languages used for multilingual multimodal pre-training. Left: English→video search. Right: Zero-shot German→video search.

sharing of the video and text Transformer slightly degrades the performance. Therefore, we choose to separate them. Learning Objective. From Table 3, the intramodal contrastive objective is important for both NCE and Triplet loss. In general, the NCE loss outperforms the Triplet loss. The proposed intermodal and intra-modal NCE objective achieves the best performance. When captions in multiple languages are available, cross-lingual NCE additionally provides a consistent improvement.
5.5 VTT Zero-Shot Cross-Lingual Transfer
Table 4 shows the multilingual text→video search results on VTT. With the best English-video models at hand (with either mBERT or XLM-R as the text backbone), we ﬁrst investigate how well these models transfer to other non-English languages under the zero-shot setting. We then analyze the beneﬁt of the proposed multilingual multimodal pre-training.
The upper section shows the zero-shot results. Unlike cross-lingual transfer in NLP tasks, employing multilingual Transformers in vision-language tasks apparently does not generalize well across languages. For example, there is a signiﬁcant drop in R@1 (19.9→11.1 (-44%) with mBERT,

21.0→16.3 (-24%) with XLM-R) when directly applying English-ﬁnetuned model to German→video search. For comparison, there is only a -10% degradation for XLM-R on en → de cross-lingual transfer in XNLI (Conneau et al., 2018). Multimodal (English-video) pre-training (MP) on HowTo100M only improves average R@1 (+0.1 or mBERT and +1.1 for XLM-R) compared to model-from-scratch. In contrast, our proposed multilingual multimodal pre-training (MMP) is shown to be the key facilitator for zero-shot cross-lingual transfer. MMP improves German→Video search (11.1→15.0, +35% for mBERT, and 16.3→19.4, +20% for XLM-R) and achieves 2.6 ∼ 2.8 improvement in average R@1. We attribute the effectiveness of MMP to learning improved alignments between multilingual textual and visual context in the shared embedding space, as relatively balanced improvements between English→video and non-English→video is observed with ﬁne-tuning.
Fig. 3 demonstrates the trend of R@1 while incrementally incorporating additional languages for MMP. For XLM-R, the improvement in R@1 asymptotically converges when pre-training with more multilingual text-video pairs. On the other hand, for zero-shot German→video search, pretraining with more languages keeps improving the search performance, even though the additional language (e.g., French) is different from the target language (i.e., German).
The lower section of Table 4 shows the results of models ﬁne-tuned with (synthesized) pseudomultilingual annotations. It can be regarded as the translate-train scenario, which serves as a strong performance target for evaluating zero-shot cross-lingual transfer, as discussed in (Lample and Conneau, 2019; Hu et al., 2020). Both mBERT and XLM-R yield better performance across non-

a soccer team walking out Rank on the field

человек жонглирует палками на вершине заснеженной горы

một người đàn ông đang nói về dự án không gian adam

一个男人在麦克风说话

✅ ✅ 1

1

1

1

(0.69)

(0.71)

(0.52)

✅ 2

2

2

2

(0.58)

(0.54)

(0.48)

3

3

3

3

(0.53)

(0.47)

(0.44)

(0.46)
✅ (0.45)
(0.42)

Figure 4: Qualitative multilingual (en, ru, vi, zh) text→video search results on VTT.

English languages with the in-domain translated pseudo-multilingual annotations. However, for English→video search, a 0.7 degradation is observed compared to the zero-shot setting. It is likely due to the noise in the translated captions. Notably, there is still a performance gap between zero-shot and translate-train settings for models with mBERT. In contrast, the gap is much smaller for models with XLM-R. In the following sections, we refer Ours-MMP as our best model with XLMR as the text backbone and compare it with other state-of-the-art methods. Qualitative Results Fig. 4 shows the multilingual text→video search results with Ours-MMP (VTT:en-only) on VTT under the zero-shot setup. Note that only one shared English-ﬁnetuned model is used for text→video search in all languages. As demonstrated, the proposed model successfully retrieves the correct videos with English (en) and Russian (ru) queries. The other top-ranked videos also share similar visual appearance to the correct one. For zero-shot transferring of the English-ﬁnetuned model to distant languages such as Vietnamese (vi) and Chinese (zh), we observe that there is still limitation for our zero-shot models to understand abstract concepts (e.g., “space project”) and associate small objects (e.g., “microphone”) with the text queries in distant languages.
5.6 Comparison to Supervised State of the Art
English→Video Search on VTT. Table 5 shows the comparison of English→video models on VTT. For a fair comparison to other baselines, our model ﬁne-tunes only with the original English annotations on VTT. The results show that our model outperforms other baselines by a large margin. Specifically, our model achieves 8.9 R@1 improvement over the original HowTo100M model (Miech et al., 2019) and other recent baselines with pre-training on HowTo100M. Using a smaller set of visual fea-

Model

R@1↑ R@5↑ R@10↑

JSFusion (Yu et al., 2018)

10.2 31.2 43.2

JPoSE (Wray et al., 2019)

14.3 38.1 53.0

VidTrans† (Korbar et al., 2020) 14.7 − 52.8

HT100M† (Miech et al., 2019) 14.9 40.2 52.8

Noise† (Amrani et al., 2020) 17.4 41.6 53.6

CE2 (Liu et al., 2019)

20.9 48.8 62.4

Ours(VTT:en-only)

21.0 50.6 63.6

Ours-MMP (VTT:en-only) 23.8 52.6 65.0

Table 5: English→video search performance on VTT. †: Models with pre-training on HowTo100M.

Model

English to Video Chinese to Video R@1↑ R@5↑ R10↑ R@1↑ R@5↑ R@10↑

VSE (Kiros et al., 2014)

28.0 64.3 76.9 -

-

-

VSE++ (Faghri et al., 2018) 33.7 70.1 81.0 -

-

-

Dual (Dong et al., 2019)

31.1 67.4 78.9 -

-

-

HGR (Chen et al., 2020a)

35.1 73.5 83.5 -

-

-

Ours (VATEX:en-only)

43.5 79.8 88.1 23.9 55.1 67.8

Ours-MMP (VATEX:en-only) 44.4 80.5 88.7 29.7 63.2 75.5

Ours-MMP (VATEX:en, zh) 44.3 80.7 88.9 40.5 76.4 85.9

Table 6: Multilingual text→video search on VATEX.

tures and training on a smaller (6,513 vs 9,000) training set2, our model also outperforms CE (Liu et al., 2019) with or without pre-training. Multilingual Text→Video Search on VATEX. Table 6 summarizes English→video and Chinese→video search performance on the VATEX dataset. Under the zero-shot setting where we train with only English-video pairs, our model already outperforms other baselines. However, a clear performance gap between English→video and Chinese→video search is observed, indicating that cross-lingual transfer to a distant language remains challenging even with XLM-R. With the proposed MMP, the gap is signiﬁcantly closed by 5.8/8.1/7.7 in R@1/5/10. When in-domain human-annotated Chinese captions are available, the performance of our model can further be improved for both languages and our model yields new state-of-the-art performance.
2CE uses 9,000 videos (VTT training and part of exclusive testing set) for training, while other baselines and our model in Table 5 are trained on the ofﬁcial VTT training set which contains 6,513 videos.

Model
OE (Vendrov et al., 2015) VSE++ (Faghri et al., 2018) Pivot (Gella et al., 2017) FB-NMT (Huang et al., 2020a) MULE (Kim et al., 2020) SMALR (Burns et al., 2020) MHA-D (Huang et al., 2019b) Ours (M30K:en-only) Ours-MMP (M30K:en-only) Ours-MMP (M30K:en, de, cs, f r)

M30K # lang.
2 2 2 2 4 10 2 1 1 4

English to Image R@1↑ R@5↑ R10↑
21.0 48.5 60.4 31.3 62.2 70.9 22.5 49.3 61.7 47.3 75.4 83.5 42.2 72.2 81.8 41.8 72.4 82.1 50.1 78.1 85.7 48.4 78.3 85.9 50.0 79.2 86.8 51.6 80.1 87.3

German to Image R@1↑ R@5↑ R@10↑
25.8 56.5 67.8 39.6 69.1 79.8 26.2 56.4 68.4 37.0 64.0 73.1 35.1 64.6 75.3 36.9 65.4 75.4 40.3 70.1 79.0 31.4 61.1 72.6 33.8 63.3 74.7 45.1 75.6 85.0

Czech to Image R@1↑ R@5↑ R@10↑

-

-

-

-

-

-

-

-

-

-

-

-

37.5 64.6 74.8

36.7 68.0 78.2

-

-

-

33.2 65.2 76.1

37.9 68.8 78.2

46.6 75.9 83.4

Table 7: Multilingual text→image search on Multi30K. MMP: Multilingual multimodal pre-training.

Cross-Modality Transfer to Multi30K: From Video-Text to Image-Text. To extend our study on zero-shot cross-lingual transfer for image-text tasks, we investigate the feasibility of transferring our video-text model across modalities. We replace the 3D-CNN in the original video-text model with a 2D-CNN to encode the image. In practice, following MHA-D (Huang et al., 2019b), we utilize the Faster-RCNN (Ren et al., 2015) pre-trained in Visual Genome (Krishna et al., 2016) to extract regional visual features. Essentially, an image is encoded as ev = RM×H where M = 36 is the maximum number of visual objects in an image. For models with MMP, we initialize their weights with the model pre-trained on Multi-HowTo100M. To tackle the feature mismatch between 2D-CNN and 3D-CNN, we leverage a linear layer with a doubled learning rate to map 2D-CNN features to the same dimension as 3D-CNN features.
Table 7 shows the results on Multi30K. For zero-shot cross-lingual transfer, when trained from scratch (M30K:en-only), our model achieves comparable performance to MHA-D but lags in German→image search since it only uses English annotations. In Ours-MMP, pre-training improves all recall metrics even with modality gap. The average R@1 improvement is 3.2. A larger gain for (relatively) low-resource language such as Czech is observed. Without using any Czech annotations, our zero-shot model with MMP achieves comparable Czech→image search performance to SMALR (Burns et al., 2020), which uses 10 languages including Czech. However, when transferring across modalities and using only English annotations, there are performance gaps between English→Image and German/Czech→Image search, implying that transferring models across modalities is feasible but remains challenging. We consider zero-shot crossmodal cross-lingual transfer as our future work.

For a fair comparison with other baselines, when trained with annotations in all 4 languages provided by Multi30K, our model greatly outperforms all baselines by large margins in multilingual text→image search.
6 Conclusion
We have presented a multilingual multimodal pretraining (MMP) strategy, the Multi-HowTo100M dataset, and a Transformer-based text-video model for learning contextual multilingual multimodal representations. The results in this paper have convincingly demonstrated that MMP is an essential ingredient for zero-shot cross-lingual transfer of vision-language models. Meanwhile, there are many remaining challenges, such as resolving the performance gap between zero-shot and training with in-domain non-English annotations; as well as techniques to transfer varieties of vision-language models (e.g., VQA (Goyal et al., 2017), TVQA (Lei et al., 2020)) or visually-enhanced NLP models such as unsupervised multimodal machine translation (Huang et al., 2020b). We believe the proposed methodology, and the corresponding resources we release, will be an important ﬁrst step towards spurring more research in this direction.
Acknowledgments
This work is supported by the DARPA grants funded under the AIDA program (FA875018-2-0018) and the GAILA program (award HR00111990063) (P.Y.). This work is also supported by EPSRC Centre for Doctoral Training in Autonomous Intelligent Machines & Systems [EP/L015897/1] (M.P.). The authors appreciate Prahal Arora, Shengxin Zha, Polina Kuznetsova, Xu Hu, and Geoffrey Zweig for their suggestions of this work. The authors would also like to thank the anonymous reviewers for their feedback.

References
Jean-Baptiste Alayrac, Piotr Bojanowski, Nishant Agrawal, Ivan Laptev, Josef Sivic, and Simon Lacoste-Julien. 2016. Unsupervised learning from narrated instruction videos. In Computer Vision and Pattern Recognition (CVPR).
Elad Amrani, Rami Ben-Ari, Daniel Rotman, and Alex Bronstein. 2020. Noise estimation using density estimation for self-supervised multimodal learning. arXiv preprint arXiv:2003.03186.
Mikel Artetxe, Gorka Labaka, and Eneko Agirre. 2017. Learning bilingual word embeddings with (almost) no bilingual data. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 451–462, Vancouver, Canada. Association for Computational Linguistics.
Mikel Artetxe, Sebastian Ruder, and Dani Yogatama. 2020. On the cross-lingual transferability of monolingual representations. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4623–4637, Online. Association for Computational Linguistics.
Jeremy Barnes, Lilja Øvrelid, and Erik Velldal. 2019. Sentiment analysis is not solved! assessing and probing sentiment classiﬁcation. In Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 12–23, Florence, Italy. Association for Computational Linguistics.
Andrea Burns, Donghyun Kim, Derry Wijaya, Kate Saenko, and Bryan A. Plummer. 2020. Learning to scale multilingual representations for visionlanguage tasks. In The European Conference on Computer Vision (ECCV).
David Chen and William Dolan. 2011. Collecting highly parallel data for paraphrase evaluation. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 190–200, Portland, Oregon, USA. Association for Computational Linguistics.
Shizhe Chen, Yida Zhao, Qin Jin, and Qi Wu. 2020a. Fine-grained video-text retrieval with hierarchical graph reasoning. In CVPR.
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. 2020b. A simple framework for contrastive learning of visual representations. In ICML.
Xilun Chen, Yu Sun, Ben Athiwaratkun, Claire Cardie, and Kilian Weinberger. 2018. Adversarial deep averaging networks for cross-lingual sentiment classiﬁcation. Transactions of the Association for Computational Linguistics, 6:557–570.

Alexis Conneau, Ruty Rinott, Guillaume Lample, Adina Williams, Samuel R. Bowman, Holger Schwenk, and Veselin Stoyanov. 2018. Xnli: Evaluating crosslingual sentence representations. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics.
Ryan Cotterell and Georg Heigold. 2017. Crosslingual character-level neural morphological tagging. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 748–759, Copenhagen, Denmark. Association for Computational Linguistics.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171–4186, Minneapolis, Minnesota. Association for Computational Linguistics.
Jianfeng Dong, Xirong Li, Chaoxi Xu, Shouling Ji, Yuan He, Gang Yang, and Xun Wang. 2019. Dual encoding for zero-example video retrieval. In CVPR.
Desmond Elliott, Stella Frank, Khalil Sima’an, and Lucia Specia. 2016. Multi30k: Multilingual englishgerman image descriptions. In Proceedings of the 5th Workshop on Vision and Language, pages 70–74. Association for Computational Linguistics.
Akiko Eriguchi, Yoshimasa Tsuruoka, and Kyunghyun Cho. 2017. Learning to parse and translate improves neural machine translation. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 72–78, Vancouver, Canada. Association for Computational Linguistics.
Fartash Faghri, David J Fleet, Jamie Ryan Kiros, and Sanja Fidler. 2018. Vse++: Improving visualsemantic embeddings with hard negatives. In BMVC.
Manaal Faruqui and Chris Dyer. 2014. Improving vector space word representations using multilingual correlation. In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 462–471, Gothenburg, Sweden. Association for Computational Linguistics.
Spandana Gella, Rico Sennrich, Frank Keller, and Mirella Lapata. 2017. Image pivoting for learning multilingual multimodal representations. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2839– 2845. Association for Computational Linguistics.
Deepti Ghadiyaram, Du Tran, and Dhruv Mahajan. 2019. Large-scale weakly-supervised pre-training for video action recognition. In CVPR.

Stephan Gouws and Anders Søgaard. 2015. Simple task-speciﬁc bilingual word embeddings. In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1386–1390, Denver, Colorado. Association for Computational Linguistics.
Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. 2017. Making the V in VQA matter: Elevating the role of image understanding in Visual Question Answering. In Conference on Computer Vision and Pattern Recognition (CVPR).
Michael Gutmann and Aapo Hyva¨rinen. 2010. Noisecontrastive estimation: A new estimation principle for unnormalized statistical models. In Proceedings of the Thirteenth International Conference on Artiﬁcial Intelligence and Statistics, pages 297–304.
Jeremy Howard and Sebastian Ruder. 2018. Universal language model ﬁne-tuning for text classiﬁcation. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 328–339, Melbourne, Australia. Association for Computational Linguistics.
Junjie Hu, Sebastian Ruder, Aditya Siddhant, Graham Neubig, Orhan Firat, and Melvin Johnson. 2020. Xtreme: A massively multilingual multi-task benchmark for evaluating cross-lingual generalization. CoRR, abs/2003.11080.
Haoyang Huang, Yaobo Liang, Nan Duan, Ming Gong, Linjun Shou, Daxin Jiang, and Ming Zhou. 2019a. Unicoder: A universal language encoder by pretraining with multiple cross-lingual tasks. arXiv preprint arXiv:1909.00964.
Po-Yao Huang, Xiaojun Chang, and Alexander Hauptmann. 2019b. Multi-head attention with diversity for learning grounded multilingual multimodal representations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLPIJCNLP), pages 1461–1467, Hong Kong, China. Association for Computational Linguistics.
Po-Yao Huang, Xiaojun Chang, Alexander Hauptmann, and Eduard Hovy. 2020a. Forward and backward multimodal nmt for improved monolingual and multilingual cross-modal retrieval. In Proceedings of the 2020 International Conference on Multimedia Retrieval, ICMR ’20, page 53–62, New York, NY, USA. Association for Computing Machinery.
Po-Yao Huang, Junjie Hu, Xiaojun Chang, and Alexander Hauptmann. 2020b. Unsupervised multimodal neural machine translation with pseudo visual pivoting. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8226–8237, Online. Association for Computational Linguistics.

Po-Yao Huang, Guoliang Kang, Wenhe Liu, Xiaojun Chang, and Alexander G. Hauptmann. 2019c. Annotation efﬁcient cross-modal retrieval with adversarial attentive alignment. In Proceedings of the 27th ACM International Conference on Multimedia, MM ’19, page 1758–1767, New York, NY, USA. Association for Computing Machinery.
Po-Yao Huang, Vaibhav, Xiaojun Chang, and Alexander G. Hauptmann. 2019d. Improving what crossmodal retrieval models learn through object-oriented inter- and intra-modal attention networks. In Proceedings of the 2019 on International Conference on Multimedia Retrieval, ICMR ’19, page 244–252, New York, NY, USA. Association for Computing Machinery.
Vladimir Iashin and Esa Rahtu. 2020. Multi-modal dense video captioning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, pages 958–959.
Donghyun Kim, Kuniaki Saito, Kate Saenko, Stan Sclaroff, and Bryan A. Plummer. 2020. MULE: Multimodal Universal Language Embedding. In AAAI Conference on Artiﬁcial Intelligence.
Diederik P Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. In ICLR.
Ryan Kiros, Ruslan Salakhutdinov, and Richard S. Zemel. 2014. Unifying visual-semantic embeddings with multimodal neural language models. CoRR, abs/1411.2539.
Alexandre Klementiev, Ivan Titov, and Binod Bhattarai. 2012. Inducing crosslingual distributed representations of words. In Proceedings of COLING 2012, pages 1459–1474.
Bruno Korbar, F. Petroni, Rohit Girdhar, and L. Torresani. 2020. Video understanding as machine translation. ArXiv, abs/2006.07203.
R. Krishna, Yuke Zhu, O. Groth, J. Johnson, K. Hata, J. Kravitz, Stephanie Chen, Yannis Kalantidis, L. Li, D. Shamma, Michael S. Bernstein, and Li Fei-Fei. 2016. Visual genome: Connecting language and vision using crowdsourced dense image annotations. International Journal of Computer Vision, 123:32– 73.
Guillaume Lample and Alexis Conneau. 2019. Crosslingual language model pretraining. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche´-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems 32, pages 7059– 7069. Curran Associates, Inc.
Jie Lei, Licheng Yu, Tamara Berg, and Mohit Bansal. 2020. TVQA+: Spatio-temporal grounding for video question answering. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8211–8225, Online. Association for Computational Linguistics.

Patrick Lewis, Barlas Oguz, Ruty Rinott, Sebastian Riedel, and Holger Schwenk. 2020. MLQA: Evaluating cross-lingual extractive question answering. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7315– 7330, Online. Association for Computational Linguistics.
Yang Liu, Samuel Albanie, Arsha Nagrani, and Andrew Zisserman. 2019. Use what you have: Video retrieval using representations from collaborative experts. In BMVC.
Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. 2019. Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, 814 December 2019, Vancouver, BC, Canada, pages 13–23.
Thang Luong, Hieu Pham, and Christopher D. Manning. 2015. Effective approaches to attention-based neural machine translation. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1412–1421, Lisbon, Portugal. Association for Computational Linguistics.
Jonathan Malmaud, Jonathan Huang, Vivek Rathod, Nicholas Johnston, Andrew Rabinovich, and Kevin Murphy. 2015. What’s cookin’? interpreting cooking videos using text, speech and vision. In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 143–152, Denver, Colorado. Association for Computational Linguistics.
Bryan McCann, James Bradbury, Caiming Xiong, and Richard Socher. 2017. Learned in translation: Contextualized word vectors. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems 30, pages 6294– 6305. Curran Associates, Inc.
Antoine Miech, Jean-Baptiste Alayrac, Lucas Smaira, Ivan Laptev, Josef Sivic, and Andrew Zisserman. 2020. End-to-End Learning of Visual Representations from Uncurated Instructional Videos. In CVPR.
Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand Tapaswi, Ivan Laptev, and Josef Sivic. 2019. Howto100M: Learning a text-video embedding by watching hundred million narrated video clips. In ICCV.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efﬁcient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.

Takashi Miyazaki and Nobuyuki Shimizu. 2016. Crosslingual image caption generation. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1780–1790. Association for Computational Linguistics.
Graham Neubig and Junjie Hu. 2018. Rapid adaptation of neural machine translation to new languages. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 875–880, Brussels, Belgium. Association for Computational Linguistics.
Aa¨ron van den Oord, Y. Li, and Oriol Vinyals. 2018. Representation learning with contrastive predictive coding. ArXiv, abs/1807.03748.
Mandela Patrick, Y. Asano, Ruth Fong, Joa˜o F. Henriques, G. Zweig, and A. Vedaldi. 2020. Multimodal self-supervision from generalized data transformations. ArXiv, abs/2003.04298.
Mandela Patrick, Po-Yao Huang, Yuki M. Asano, Florian Metze, Alexander Hauptmann, Joa˜o Henriques, and Andrea Vedaldi. 2021. Support-set bottlenecks for video-text representation learning. In International Conference on Learning Representations (ICLR).
Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word representations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 2227–2237, New Orleans, Louisiana. Association for Computational Linguistics.
Jonas Pfeiffer, Ivan Vulic´, Iryna Gurevych, and Sebastian Ruder. 2020. MAD-X: An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 7654–7673, Online. Association for Computational Linguistics.
Telmo Pires, Eva Schlinger, and Dan Garrette. 2019. How multilingual is multilingual BERT? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4996– 5001, Florence, Italy. Association for Computational Linguistics.
Afshin Rahimi, Yuan Li, and Trevor Cohn. 2019. Massively multilingual transfer for NER. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 151–164, Florence, Italy. Association for Computational Linguistics.
Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. 2015. Faster r-cnn: Towards real-time object detection with region proposal networks. In

Advances in neural information processing systems, pages 91–99.
Tal Schuster, Ori Ram, Regina Barzilay, and Amir Globerson. 2019. Cross-lingual alignment of contextual word embeddings, with applications to zeroshot dependency parsing. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 1599–1613, Minneapolis, Minnesota. Association for Computational Linguistics.
Holger Schwenk and Xian Li. 2018. A corpus for multilingual document classiﬁcation in eight languages. In Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018), Paris, France. European Language Resources Association (ELRA).
Aditya Siddhant, Ankur Bapna, Yuan Cao, Orhan Firat, Mia Chen, Sneha Kudugunta, Naveen Arivazhagan, and Yonghui Wu. 2020. Leveraging monolingual data with self-supervision for multilingual neural machine translation. arXiv preprint arXiv:2005.04816.
Gunnar A. Sigurdsson, Jean-Baptiste Alayrac, Aida Nematzadeh, Lucas Smaira, Mateusz Malinowski, Joa˜o Carreira, Phil Blunsom, and Andrew Zisserman. 2020. Visual grounding in video for unsupervised word translation. In CVPR.
D´ıdac Sur´ıs, Dave Epstein, and Carl Vondrick. 2020. Globetrotter: Unsupervised multilingual translation from visual alignment. arXiv preprint arXiv:2012.04631.
Oscar Ta¨ckstro¨m, Ryan McDonald, and Jakob Uszkoreit. 2012. Cross-lingual word clusters for direct transfer of linguistic structure. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 477– 487, Montre´al, Canada. Association for Computational Linguistics.
Du Tran, Heng Wang, Lorenzo Torresani, Jamie Ray, Yann LeCun, and Manohar Paluri. 2018. A closer look at spatiotemporal convolutions for action recognition. In CVPR.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in neural information processing systems, pages 5998–6008.
Ivan Vendrov, Ryan Kiros, Sanja Fidler, and Raquel Urtasun. 2015. Order-embeddings of images and language. arXiv preprint arXiv:1511.06361.
Xin Wang, Jiawei Wu, Junkun Chen, Lei Li, YuanFang Wang, and William Yang Wang. 2019. Vatex: A large-scale, high-quality multilingual dataset for video-and-language research. In Proceedings of the

IEEE International Conference on Computer Vision, pages 4581–4591.
Michael Wray, Diane Larlus, Gabriela Csurka, and Dima Damen. 2019. Fine-grained action retrieval through multiple parts-of-speech embeddings. In ICCV.
Shijie Wu and Mark Dredze. 2019a. Beto, bentz, becas: The surprising cross-lingual effectiveness of bert. arXiv preprint arXiv:1904.09077.
Shijie Wu and Mark Dredze. 2019b. Beto, bentz, becas: The surprising cross-lingual effectiveness of BERT. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 833–844, Hong Kong, China. Association for Computational Linguistics.
Jun Xu, Tao Mei, Ting Yao, and Yong Rui. 2016. MSRVTT: A large video description dataset for bridging video and language. In CVPR.
I. Zeki Yalniz, Herve´ Je´gou, Kan Chen, Manohar Paluri, and Dhruv Mahajan. 2019. Billion-scale semi-supervised learning for image classiﬁcation.
Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. 2014. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. Transactions of the Association for Computational Linguistics, 2:67–78.
Shoou-I Yu, Lu Jiang, and Alexander Hauptmann. 2014. Instructional videos for unsupervised harvesting and learning of action examples. In Proceedings of the 22nd ACM International Conference on Multimedia, MM ’14, page 825–828, New York, NY, USA. Association for Computing Machinery.
Youngjae Yu, Jongseok Kim, and Gunhee Kim. 2018. A joint sequence fusion model for video question answering and retrieval. In ECCV.
Zhuosheng Zhang, Kehai Chen, Rui Wang, Masao Utiyama, Eiichiro Sumita, Zuchao Li, and Hai Zhao. 2020. Neural machine translation with universal visual representation. In International Conference on Learning Representations.

A Appendix Overview
The Appendix is organized as follows: First we provide details about the Multilingual HowTo100M (Multi-HowTo100M) dataset for multilingual multimodal pre-training (MMP) in §B. Then we provide additional implementation details and experiment setup in §C. Additional ablation studies regarding choices of Transformer architecture are discussed in §D. Then we present additional cross-dataset transfer experiments in §E.
B The Multilingual HowTo100M Dataset
In this section we provide the detailed statistics of the Multilingual HowTo100M (MultiHowTo100M) dataset. We also provide a comparison to Sigurdsson et al. (2020) that also uses HowTo100M for unsupervised word translation.
The Multi-HowTo100M dataset is built upon the original English HowTo100M dataset (Miech et al., 2019) that contains 1.2 million instructional videos (138 million clips) on YouTube. We reuse the raw English subtitles in HowTo100M, where the subtitles in HowTo100M are either automatic speech recognition (ASR) transcriptions or user generated subtitles.
For Multi-HowTo100M, we use the same video collection as English HowTo100M. At the time of data collection (May 2020), there were 1.09 million videos accessible. We collect the subtitles provided by YouTube, which either consist of user-generated subtitles or those generated by Google ASR and Translate in the absence of user-generated ones. Essentially, we collect video subtitles in 9 languages: English (en), German (de), French (fr), Russian (ru), Spanish (es), Czech (cz), Swahili (sw), Chinese (zh), Vietnamese (vi). Table 8 summarizes the dataset statistics for each language. In most cases there are more than 1 billion tokens a language.
Fig. 5 further shows the number of tokens per video. There are typically lengthy narrations that contains several hundreds of tokens available in each instructional video. Fig. 6 shows the distribution of number of tokens in a subtitle. For each subtitle segment, which ranges from 0∼20 seconds, there are typically 15∼25 words. The most of the cases, subtitles are well aligned in time for nonEnglish languages. Fig. 2 visualizes a few examples in Multi-HowTo100M.
A similar HowTo100M variant has been recently reported in MUVE (Sigurdsson et al., 2020) that is created for unsupervised word translation.

Language videos #subtitle #tokens

English 1238911 138429877 German 1092947 69317890 French 1093070 69399097 Czech 1092717 68911940 Russian 1092802 69117193 Chinese 1092915 68939488 Swahili 1092302 68898800 Vietnamese 1092603 68887868 Spanish 1092649 70143503

1.18B 1.26B 1.33B 1.22B 1.25B 0.94B 1.22B 1.13B 1.16B

Table 8: Multi-HowTo100M statistics

Our Multi-HowTo100M differs from MUVE in the following perspectives: First, we collects 9 language for all videos in HowTo100M while MUVE only has 4 languages available (English, French, Japanese, and Korean) on HowTo100M. Also, MUVE divided HowTo100M into 4 nonoverlapped sections for each language, there are no parallel pairs for each subtitle. While in MultiHowTo100M, there are 7-9 languages for each subtitle. Essentially, There are more than 1 billion tokens in most languages in Multi-HowTo100M. To our best knowledge, our Multi-HowTo100M dataset is currently the largest multilingual textvideo collection.
Beyond scale, instructional videos in MultiHowTo100M are feasible pre-training resources for many downstream vision-language models. Demonstrators in instructional videos typically perform intentionally and explain the visual object or action explicitly. According to the inspection by (Miech et al., 2019), for around 51% of clips, at least one object or action mention in the caption can be visually seen. Prior work has shown that instructional videos are useful for event recognition (Yu et al., 2014), action localization model (Alayrac et al., 2016), cross-modal alignments (Malmaud et al., 2015). We expect the previous success in the intersection of natural language processing (NLP) and computer vision (CV) could be further translated into more languages to have a broaden impact.
The are great potentials of using our MultiHowTo100M dataset in related research ﬁeld such as multilingual multimodal representation learning (Huang et al., 2019b; Kim et al., 2020; Burns et al., 2020), multilingual multimodal translation (Huang et al., 2020b; Sur´ıs et al., 2020), multilingual image/video captioning (Miyazaki and Shimizu, 2016) ... etc. We expect the release of

60000 50000 40000 30000 20000 10000
00

Distribution of #tokens in
de fr cs ru zh vi sw es
50 100 150 200 250 300 350 400 450 500 550 600 650 700 750 800 850 900 950

Figure 5: Distribution of #tokens/video in MultiHowTo100M

1e7

3.0

de

fr

2.5

cs

ru

2.0

zh

1.5

vi

sw

1.0

es

0.5

Distribution of #token/subtitle in Mutli-HowTo100M

0.0 0

5

10

15

20

25

30

35

Figure 6: Distribution of #tokens/subtitle in MultiHowTo100M

Multi-HowTo100M will be a ﬁrst step towards spurring more research in these directions.
C Implementation and Experiment Details
Pre-Processing. For pre-possessing, we truncate the maximum length N of text to 192 for pretraining on Multi-HowTo100M. The maximum length is set to 96 for ﬁne-tuning VTT (Xu et al., 2016), VATEX (Wang et al., 2019) and Multi30K (Elliott et al., 2016). The maximum video length M is set to 128 for pre-training on Multi-HowTo100M and 36 for all ﬁne-tuning tasks.
Model Architecture. For the multilingual Transformers, either multilingual BERT (Devlin et al., 2019) or XLM-R-large (Artetxe et al., 2020), we use the pre-trained version provided by HuggingFace. 3 and use their corresponding tokenizers for tokenization. Detailed design choices regarding output layer and frozen layer is discussed in §D.
For the video backbone, we use a 34-layer, R(2+1)-D (Tran et al., 2018) network pre-trained on IG65M (Ghadiyaram et al., 2019) and a S3D (Miech et al., 2020) network pre-trained on HowTo100M (Miech et al., 2019). We apply a spatial-temporal average pooling over the last convolutional layer, resulting in a 512-dimensional vector for each 3D CNN network. We extract visual features at a rate of 1 feature per second. Since the 3D CNNs employs different size of input windows (e.g., 8 frames for R(2+1)D and 16 for S3D),
3https://github.com/huggingface/transformers

we re-sample videos to 30 fps and employs a window of size 8 or 30 that takes consecutive frames starting from the beginning of every second for encoding. We simply concatenate the two 3D-CNN outputs and use the 1024-dimension vector as the visual input stream to our model. Notably, instead of using 9 different types of visual features as in CE (Liu et al., 2019), we use only the above 2 features and achieve superior performance.
For the Transformer pooling head (TP) modules, we use a 2-layer Transformer with 4-head attention for each TP. The embedding dimension D is set to 1024. We do not use the positional embeddings in both text and video TP as we do not ﬁnd them beneﬁcial in our experiments. The softmax temperature in all NCE contrastive objectives is set to 0.1 as used in SimCLR (Chen et al., 2020b).
Note that unlike ViLBERT (Lu et al., 2019) or OAN (Huang et al., 2019d), our models does not employ cross-modality attention and keep the multi-head self-attention within the same modality. The main reason is to reduce the inference time complexity. For cross-modality attention, the complexity is O(T V ) to encode T text queries for V videos in a dataset before retrieval (since video and query representations depend on each other). It is clearly not scalable when the dataset contains millions of videos. To this end, our model keep self-attention within the same modality which results in a O(T + V ) complexity compared O(T V ) in prior work with cross-modality attention. In our preliminary experiments, we also incorporate crossmodality attention and achieved 0.3∼1.8 R@1 improvement. Considering the trade-off between performance and scalability, we choose the latter.
Training and Inference Details and Proﬁling. For the softmax temperature in NCE, we set to 0.1 as used in SimCLR (Chen et al., 2020b). We use the Adam (Kingma and Ba, 2015) optimizer with a initial learning rate 2 · 10−4 and clip gradients greater than 0.2 during the training phase. Dropout rate is 0.3. Since the video length and token length is longer in the pre-training phase, we use a 64 batch size for pre-training. For ﬁne-tuning, we use a batch size of 128.
Pre-training on the 1.2 million HowTo100M videos takes around 10 GPU hours (NVIDA V100) for 16 epochs. We speed up the pre-training process by distributing the workload over 8 GPUs on a single node of our server. We use 1 GPU for the ﬁne-tuning or training from scratch experiments.

For the MSR-VTT split, it takes 12 GPU hours to train our model on 180K video-text pairs for 20 epochs. For VATEX, it takes 32 GPU hours to train on 260K video-text pairs for 30 epochs. For inference, the encoding speed is around 250300 videos/sec and 200-250 text queries/sec. The overall text→video search speed on 1,000 videotext pairs (1,000 text queries over 1,000 videos) is around 6 seconds including video/text encoding and ranking their similarity scores.

Output layer Freeze lower en de

3

0

20.9 3.2

6

0

20.5 3.1

9

0

21.0 4.8

12

0

21.0 13.3

15

0

20.5 12.3

18

0

20.8 12.6

12

6

21.0 15.5

12

9

21.0 16.3

12

12 18.9 14.1

Table 9: Text→video R@1 of XLM-R output layers and layers to freeze on VTT

Experiment Details. Our experiment consider three types of pre-training: (1) Multilingual multimodal pre-training (MMP), (2) Multimodal pretraining (MP), and (3) no pre-training (from scratch). For (1) and (2), we pre-train 16 epochs and use the model weight at 16-th epoch for ﬁnetuning experiments.
For multimodal pre-training, we pre-train on the original English HowTo100M dataset. We iterate over all videos in HowTo100M. For each video, we randomly sample the start and end time to construct a video clip. For each clip, we locate the nearest consecutive ASR transcriptions in time and use it as to construct the (video, text) pair for training.
For multilingual multimodal pre-training (MMP), we use Multi-HowTo100M for pretraining. For each video, we follow the same strategy as MP. For a clip, we sample one language type each time from 9 languages and use the consecutive ASR transcriptions that are closest in time to compose (video, text) pairs for training.
After pre-training, we ﬁne-tune our model on VTT and VATEX to evaluate on text→video search tasks. In the zero-shot cross-lingual transfer experiments, we use only English-video data. We then directly test the model with non-English queries to report the zero-shot performance. When annotations in additional languages are available (by humans in VATEX and Multi30K; by MT models (i.e. translate-train) in VTT), we train our model with all available multilingual annotations (i.e. fully supervised) to compare fairly with other baselines in multilingual text→video search.
Since pre-trained model has a faster convergence rate, we ﬁne-tune for 10 epochs and use the model with best validation performance (summation of R@1, R@5, R@10) for testing. For models without pre-training (i.e., from-scratch), we train for 20 epochs under the same training protocol.

Output layer Freeze lower en de

3

0

19.2 2.5

6

0

19.5 2.0

9

0

19.3 5.8

12

0

19.6 8.8

12

6

19.3 10.5

12

9

19.9 11.1

12

12 18.9 9.8

Table 10: Text→video R@1 of mBERT output layers and layers to freeze on VTT

D Additional Ablation Studies
As has been investigated in XTREME (Hu et al., 2020), choosing different output layers will affect the zero-shot transferability of multilingual Transformers in various NLP tasks. For text→video search tasks, we conduct a series of experiments to identify the desirable choices of hyper-parameters in the proposed multilingual multimodal Transformer that lead to best performance in English-tovideo and (zero-shot) non-English-to-video search performance. Beyond our ablation studies in Sec. 5, in this part we highlight our trials in the choice of the output layer and the layers to be frozen in our multilingual Transformer backbone (i.e., mBERT and XLM-R). There are 24 layers in XLM-R (large) and 12 layers in mBERT. We perform grid-search on VTT to identify the best choice of these two hyper-parameters.
Choice of Output Layers Table 9 and Table 10 compare different choices of output layer and layers to freeze in multilingual Transformers. Our results suggest that the best output layer for mBERT and XLM-R is the 12-th layer. Surprisingly, while output layer does not affect English→video search signiﬁcantly, it greatly affects the zero-shot crosslingual transfer performance of video-text models. For both XLM-R and mBERT, the performance degrade signiﬁcantly if ﬁne-tuning all layers.

text→video
In-domain Out-of-domain

English

Non-English

Table 11: Coverage of our experiments

Choice of Layers to Freeze Similar to output layers, the choice of frozen layers greatly affects cross-lingual transferability. For both mBERT and XLM-R, it is desirable to freeze part of the lower layers and make the top-3 layers trainable for videotext models. We observe that when freezing all layers (i.e., using the pre-extracted contextual multilingual embeddings) does not lead to satisfactory results. For mBERT, R@1 drops from 19.9 to 18.9 in English→video search and 11.1 to 9.8 in German→video search. For XLM-R, R@1 drops from 21.0 to 18.9 in English→video search and 16.3 to 14.1 in German→video search. These results imply that text-only contextual multilingual embeddings along are likely to be infeasible to be applied to vision-language tasks without proper ﬁne-tuning.
An important observation is that the best English→video search performance corresponds to the best German→video performance. This trend implies that for model selection, the conﬁguration for the best English→video model usually translates to the best conﬁguration for (zero-shot) cross-lingual model. This shared trend justiﬁes the English→video ablation studies in the original paper. Note that we utilize the best English→video for all (zero-shot) cross-lingual experiment in our experiment section.
For multilingual text→video search, the best conﬁguration we found in our experiments is to output the 12-th layer and freeze the layers below 9 for both mBERT and XLM-R.

Model

R@1 R@5 R@10

VSE (Kiros et al., 2014) 10.1 29.4 41.5

VSE++ (Faghri et al., 2018) 14.4 35.7 46.9

Dual (Dong et al., 2019) 13.7 36.1 48.2

HGR (Chen et al., 2020a) 16.4 38.3 49.8

Ours-Full

24.0 50.5 62.1

Table 12: Zero-shot generalization on YouTube2Text with VTT-ﬁnetuned model.

E.1 Generalizability across English-Video Datasets
In this section. we provide additional experiment results regarding zero-shot generalization of the VTT-ﬁnetuned model on out-of-domain dataset. Speciﬁcally, we test on YouTube2Text (Chen and Dolan, 2011). The aim of this experiment is to test the cross-dataset generalizabilty of our model without using domain-speciﬁc training data.
Table 12 shows the comparison of English→video search results on the YouTube2Text testing set. Models in this table are only ﬁne-tuned on VTT and use no YouTube2Text training data. As can be observed, our model with MMP generalizes well on YouTube2Text, outperforming HGR (Chen et al., 2020a) by 7.6 and DualEncoder (Dong et al., 2019) by 10.3 in R@1.

E Additional Experimental Results
The coverage of our text→video search experiments is summarized in Table 11. Our experiments cover the following scenarios: In-domain, English: Table 5 (VTT) and Table 6 (VATEX) in the original paper. In-domain, non-English: Table 4 (VTT, 9 languages) and Table 6 (VATEX, Chinese). Out-of-domain, English: Additional (zero-shot) generalization results across datasets are in §E.1. Out-of-domain, non-English: We consider this as our future work.

