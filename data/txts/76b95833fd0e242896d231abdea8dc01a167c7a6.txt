Approximate Gaussian process inference for the drift of stochastic differential equations

Andreas Ruttor Computer Science, TU Berlin andreas.ruttor@tu-berlin.de

Philipp Batz Computer Science, TU Berlin philipp.batz@tu-berlin.de

Manfred Opper Computer Science, TU Berlin manfred.opper@tu-berlin.de

Abstract
We introduce a nonparametric approach for estimating drift functions in systems of stochastic differential equations from sparse observations of the state vector. Using a Gaussian process prior over the drift as a function of the state vector, we develop an approximate EM algorithm to deal with the unobserved, latent dynamics between observations. The posterior over states is approximated by a piecewise linearized process of the Ornstein-Uhlenbeck type and the MAP estimation of the drift is facilitated by a sparse Gaussian process regression.
1 Introduction
Gaussian process (GP) inference methods have been successfully applied to models for dynamical systems, see e.g. [1–3]. Usually, these studies have dealt with discrete time dynamics, where one uses a GP prior for modeling transition function and the measurement function of the system. On the other hand, many dynamical systems in the physical world evolve in continuous time and the noisy dynamics is described naturally in terms of stochastic differential equations (SDE). SDEs have also attracted considerable interest in the NIPS community in recent years [4–7]. So far most inference approaches have dealt with the posterior prediction of state variables between observations (smoothing) and the estimation of parameters contained in the drift function, which governs the deterministic part of the microscopic time evolution. Since the drift is usually a nonlinear function of the state vector, a nonparametric estimation using Gaussian process priors would be a natural choice, when a large number of data is available. A recent result by [8, 9] presented an important step in this direction. The authors have shown that GPs are a conjugate family to SDE likelihoods. In fact, if an entire path of dense observations of the state dynamics is observed, the posterior process over the drift is exactly a GP. Unfortunately, this simplicity is lost, when observations are not dense, but separated by larger time intervals. In [8] this sparse, incomplete observation case has been treated by a Gibbs sampler, which alternates between sampling complete state paths of the SDE and creating GP samples for the drift. A nontrivial problem is the sampling from SDE paths conditioned on observations. Second, the densely sampled hidden paths are equivalent to a large number of imputed observations, for which the matrix inversions required by the GP posterior predictions can become computationally costly. It was shown in [8] that in the univariate case for GP priors with precision operators (the inverses of covariance kernels) which are differential operators efﬁcient predictions can be realized in terms of the solutions of differential equations.
In this paper, we develop an alternative approximate expectation maximization (EM) method for inference from sparse observations, which is faster than the sampling approach and can also be applied to arbitrary kernels and multivariate SDEs. In the E-Step we approximate expectations over
1

state paths by those of a locally ﬁtted Ornstein-Uhlenbeck model. The M-step for computing the maximum posterior GP prediction of the drift depends on a continuum of function values and is thus approximated by a sparse GP.
The paper is organized as follows. Section 2 introduces stochastic differential equations and section 3 discusses GP based inference for completely observed paths. In section 4 our approximate EM algorithm is derived and its performance is demonstrated on a variety of SDEs in section 6. Section 7 presents a discussion.

2 Stochastic differential equations

We consider continuous-time univariate Markov processes of the diffusion type, where the dynamics of a d-dimensional state vector Xt ∈ Rd is given by the stochastic differential equation (SDE)

dXt = f (Xt)dt + D1/2dW.

(1)

The vector function f (x) = (f 1(x), . . . , f d(x)) deﬁnes the deterministic drift and W is a Wiener

process, which models additive white noise. D is the diffusion matrix, which we assume to be

independent of x. We will not attempt a rigorous treatment of probability measures over continuous

time paths here, but will mostly assume for our derivations that the process can be approximated

with a discrete time process Xt in the Euler-Maruyama discretization [10], where the times t ∈ G

are on a regular grid G = {0, ∆t, 2∆t, . . . } and where ∆t is some small microscopic time. The

discretized process is given by

√

Xt+∆t − Xt = f (Xt)∆t + D1/2 ∆t ǫt,

(2)

where ǫt ∼ N (0, I) is a sequence of i.i.d. Gaussian noise vectors. We will usually take the limit ∆t → 0 only in expressions where (Riemann) sums are over nonrandom quantities, i.e. where
expectations over paths have been carried out and can be replaced by ordinary integrals.

3 Bayesian Inference for dense observations

Suppose we observe a path of n d-dimensional observations X0:T = (Xt)t∈G over the time interval [0, T ]. Since for ∆t → 0, the transition probabilities of the process are Gaussian,

pf (X0:T |f ) ∝ exp − 1

||Xt+∆t − Xt − f (Xt)∆t||2 ,

(3)

2∆t t

the probability density for the path with a given drift function f =. (f (Xt))t∈G at these observations

can be written as the product

pf (X0:T |f ) = p0(X0:T )L(X0:T |f ),

(4)

where

p0(X0:T ) ∝ exp − 1

||Xt+∆t − Xt||2

(5)

2∆t t

is the measure over paths without drift, i.e. a discretized version of the Wiener measure, and a term

which we will call likelihood in the following,

L(X0:T |f ) = exp − 1 ||f (Xt)||2 ∆t + f (Xt), Xt+∆t − Xt .

(6)

2t t

Here we have introduced the inner product u, v =. u⊤D−1v and the corresponding squared norm ||u||2 =. u⊤D−1u to avoid cluttered notation.

To attempt a nonparametric Bayesian estimate of the drift function f (x), we note that the exponent in (6) contains the drift f at most quadratically. Hence it becomes clear that a conjugate prior
to the drift for this model is given by a Gaussian process, i.e. we assume that for each component f ∼ P0(f ) = GP(0, K), where K is a kernel [11], a fact which was recently observed in [8]. We denote probabilities over the drift f by upper case symbols in order to avoid confusion with path prob-
abilities. Although a more general model is possible, we will restrict ourselves to the case where the

2

Figure 1: The left ﬁgure shows a snippet of the double well sample path in black and observations as red dots. The right picture displays the estimated drift function for the double well model after initialization, where the red line denotes the true drift function and the black line the mean function with corresponding 95%-conﬁdence bounds (twice the standard deviation) in blue. One can clearly see that the larger distance between the consecutive points leads to a wrong prediction.

GP priors over the components f j(x), j = 1, . . . , d of the drift are independent (with usually different kernels) and we assume that we have a diagonal diffusion matrix D = diag(σ12, . . . , σd2). In this case, the GP posteriors of f j(x) are independent, too, and we can estimate drift components inde-

pendently by ordinary GP regression. We deﬁne data vectors by dj = ((Xtj+∆t −Xtj)/∆t)⊤t∈G\{T },

the

kernel

matrix

Kj

=

(Kj (Xs, Xt))s,t∈G,

and

the

test

vector

kj (x)

=

(

K

j

(

x

,

X

t

)

)

⊤ t∈

G

.

Then

a

standard calculation [11] shows that the posterior process over drift functions f has a posterior mean

and a GP posterior variance at an arbitrary point x is given by

f¯j(x) = kj(x)⊤

Kj + σj2 I −d1j , ∆t

σf2j (x) = Kj(x, x)−kj (x)⊤

Kj + σj2 I −k1j (x). (7) ∆t

Note that σj2/∆t plays the role of the variance of the observation noise in the standard regression case. In practice, the number of observations can be quite large for a ﬁne time discretization, and a fast computation of (7) could become infeasible. A possible way out of this problem—as suggested by [8]—could be a restriction to kernels for which the inverse kernel, the precision operator, is a differential operator. A well known machine learning approach, which is based on a sparse Gaussian process approximation, applies to arbitrary kernels and generalizes easily to multivariate SDE. We have resorted speciﬁcally to the optimal Kullback-Leibler sparsity [1,12], where the likelihood term of a GP model is replaced by another effective likelihood, which depends only on a smaller set of variables fs.

4 MAP Inference for sparse observations

The simple GP regression approach outlined in the previous section cannot be.applied when observations are sparse in time. In this setting, we assume that n observations yk = Xτk , k = 1, . . . , n are obtained at (for simplicity) regular intervals τk = kτ , where τ ≫ ∆t is much larger than the microscopic time scale. In this case, a discretization in (6), where the sum over the microscopic grid t ∈ G would be replaced by a sum over macroscopic times τk and ∆t by τ , would correspond to a discrete time dynamical model of the form (1) again replacing ∆t by τ . But this discretization
would give a bad approximation to the true SDE dynamics. The estimator of the drift would give some (approximate) estimation of the mean of the transition kernel over macroscopic times τ . How-
ever, this does usually not give a good approximation for the original drift. This can be seen in ﬁgure
1, where the red line corresponds to the true drift (of the so called double-well model [4]) and the black line to its prediction based on observations with τ = 0.2 and the naive estimation method.

To deal with this problem, we treat the process Xt for times t between consecutive observations kτ < t < (k + 1)τ as a latent unobserved random variable with a posterior path measure given by

n

p(X0:T |y, f ) ∝ p(X0:T |f ) δ(yk − Xkτ ),

(8)

k=1

3

where y is the collection of observations yk and δ(·) denotes the Dirac-distribution encoding the fact that the process is known perfectly at times τk. Our goal is to use an EM algorithm to compute the maximum posterior (MAP) prediction for the drift function f (x). Unfortunately, exact posterior
expectations are intractable and one needs to work with suitable approximations.

4.1 Approximate EM algorithm

The EM algorithm cycles between two steps

1. In the E-step, we compute the expected negative logarithm of the complete data likelihood

L(f , q) = −Eq [ln L(X0:T |f )] ,

(9)

where q denotes a measure over paths which approximates the intractable posterior p(X0:T |y, fold) for the previous estimate fold of the drift.

2. In the M-Step, we recompute the drift function as

fnew = arg min (L(f , q) − ln P0(f )) .

(10)

f

To compute the expectation in the E-step, we use (6) and take the limit ∆t → 0 at the end, when expectations have been computed. As f (x) is a time-independent function, this yields

−Eq[ln L(X0:T |f )] = lim 1 Eq ||f (Xt)||2∆t − 2 f (Xt), Xt+∆t − Xt ∆t→0 2 t

=

1

T
Eq ||f (Xt)||2 − 2 f (Xt), gt(Xt) dt

20

= 1 ||f (x)||2A(x)dx − f (x), y(x) dx.

(11)

2

Here qt(x) is the marginal density of Xt computed from the approximate posterior path measure q. We have also deﬁned the corresponding approximate posterior drift

gt(x) = lim 1 Eq[Xt+∆t − Xt|Xt = x],

(12)

∆t→0 ∆t

as well as the functions

T

T

A(x) = qt(x)dt and y(x) = gt(x)qt(x)dt.

(13)

0

0

There are two main problems for a practical realization of this EM algorithm:

1. We need to ﬁnd tractable path measures q, which lead to good approximations for marginal densities and posterior drifts given arbitrary prior drift functions f (x).
2. The M-Step requires a functional optimization, because (11) shows that L(f , q) − ln P0(f ) is actually a functional of f (x), i.e. it contains a continuum of values f (x), where x ∈ Rd.

4.2 Linear drift approximation: The Ornstein-Uhlenbeck bridge

For given drift f (·) and times t ∈ Ik in the interval Ik = [k τ ; (k + 1)τ ] between two consecutive observations, the exact posterior marginal pt(x) equals the density of Xt = x conditioned on the fact that Xkτ = yk and X(k+1)τ = yk+1. This can be expressed by the transition densities of the
homogeneous Markov diffusion process with drift f (x). We denote this quantity by ps(Xt+s|Xt) being the density of the random variable Xt+s at time t + s conditioned on Xt at time t. Using the
Markov property, this yields the representation

pt(x) ∝ p(k+1)τ−t(yk+1|x)pt−kτ (x|yk) for t ∈ Ik.

(14)

As functions of t and x, the second factor fulﬁlls a forward Fokker-Planck equation and the ﬁrst one
a Kolmogorov backward equation [13]. Both are partial differential equations. Since exact computations are not feasible for general drift functions, we approximate the transition density ps(x|xk) in each interval Ik by that of a process, where the drift f (x) is replaced by its local linearization

f (x) ≈ fou(x, t) = f (xk) − Γk(x − xk) with Γk = −∇f (xk).

(15)

4

This is equivalent to assuming that for t ∈ Ik the dynamics is approximated by the homogeneous Ornstein-Uhlenbeck process [13]

dXt = [f (yk) − Γk(Xt − yk)]dt + D1/2dW,

(16)

which is also used to build computationally efﬁcient hierarchical models [14, 15], as in this case

the marginal posterior can be calculated analytically. Here the transition density is a multivariate

Gaussian

qs(k)(x|y) = N x|αk + e−Γks(y − αk); Ss

(17)

where αk = yk + Γ−k 1f (yk) is the stationary mean and the variance Ss = AsBs−1 is calculated using the matrix exponential

As = exp

Γk

D
⊤

s

0.

(18)

Bs

0 −Γk

I

Then we obtain the Gaussian approximation qt(k)(x) = N (x|m(t); C(t)) of the marginal posterior for t ∈ Ik by multiplying the two transition densities, where

C(t) =

e−Γ⊤ k (tk+1−t)S−1

−1
e−Γk(tk+1−t) + S−1

tk+1 −t

t−tk

and

m(t) = C(t) e−Γ⊤ k (tk+1−t)St−k1+1−t yk+1 − αk + e−Γk(tk+1−t)αk

+ C(t) St−−1tk αk + e−Γk(t−tk)(yk − αk) .

By inspecting mean and variance we see that the distribution is a equivalent to a bridge between the points X = yk and X = yk+1 and collapses to point masses at these points.

Within this approximation, we can estimate parameters such as the diffusion D using the approxi-

mate evidence
n−1

p(y|f ) ≈ pou(y) = p(x1) qτ(k)(yk+1|yk)

(19)

j=1

Finally, in this approximation we obtain for the posterior drift

gt(x) = lim 1 E [Xt+∆t − Xt|Xt = x, Xτ = yk+1] ∆t→0 ∆t
= f (yk) − Γk(x − yk) + De−Γ⊤ k (tk+1−t)St−k1+1−t(yk+1 − αk − e−Γk(tk+1−t)(x − αk))

as shown in appendix A in the supplementary material.

4.3 Sparse M-Step approximation

To cope with the functional optimization, we resort to a sparse approximation for replacing the inﬁnite set f by a sparse set fs. Here the GP posteriors (for each component of the drift) is replaced by one that is closest in the KL sense. Following appendix B in the supplementary material, we ﬁnd
that in the sparse approximation the likelihood (11) is replaced by

Ls(f , q) = 1 ||E0[f (x)|fs]||2 A(x) dx − E0[f (x)|fs], y(x) dx,

(20)

2

where the conditional expectation is over the GP prior. In order to avoid cluttered notation, it should be noted that in the following results for a component f j, the quantities Λs, fs, ks, K−s 1, y(x), σ2, similar to (7) depend on the component j, but not A(x).

This is easily computed as

E0[f (x)|fs] = k⊤s (x)K−s 1fs.

(21)

Hence Ls(f , q) = 12 fs⊤Λsf s − fs⊤ds (22)
with

Λs = σ12 K−s 1 ks(x) A(x) k⊤s (x)dx K−s 1, ds = σ12 K−s 1 ks(x) y(x) dx. (23)

5

With these results, the approximate MAP estimate is

f¯s(x) = k⊤s (x)(I + ΛsKs)−1ds.

(24)

The integrals over x in (23) can be computed analytically for many kernels of interest such as polynomial and RBF ones. However, we have done this for 1-dimensional models only. For higher dimensions, we found it more efﬁcient to treat both the time integration in (13) and the x integrals by sampling, where time points t are drawn uniformly at random and x points from the multivariate Gaussian qt(x).
A related expression for the variance σs2(x) = K(x, x) − k⊤s (x)(I + ΛKs)−1Λsks(x) can only be viewed as a crude estimate, because it does not include the impact of the GP ﬂuctuations on the path probabilities.

5 A crude estimate of an approximation error

Unfortunately, there is no guarantee that this approximation to the EM algorithm will always incfrroemasethtehetheexOacrnt sltiekienl-iUhoholednbpe(yck|fa)p. prHoexriem, awtioenw(1il9l )dteovelolowpesat ocrrduedreinestthime adtieffehroewncep(δyf|(fX) td,itff)e=r.s f (Xt) − fou(Xt, t) between drift function and its approximation.

Our estimate is based on the exact expression

n

p(y|f ) = dp0(X0:T ) eln L(X0:T |f ) δ(yk − Xkτ )

(25)

k=1

where the Wiener measure p0 is deﬁned in (5) and the likelihood L(X0:T |f ) in (6). The OrnsteinUhlenbeck approximation (19) can expressed in a similar way: we just have to replace L(X0:T |f ) by a functional Lou(X0:T |f ) which in turn is obtained by replacing f (Xt) with the linearized drift fou(Xt, t) in (6). The difference in free energies (negative log evidences) can be expressed ex-
actly by an expectation over the posterior OU processes and then expanded (similar to a cumulant
expansion) in a Taylor series in ∆L = − ln(L/Lou). The ﬁrst two terms are given by

∆F =. − {ln p(y|f ) − ln pou(y)} = − ln Eq e−∆L ≈ Eq [∆L] − 1 Varq [∆L] ± . . . (26) 2

The computation of the ﬁrst term is similar to (11) and requires only the marginal qt and the posterior

gt. The second term contains the posterior variance and requires two-time covariances of the OU

process. We concentrate on the ﬁrst term which we further expand in the difference δf (Xt, t). This

yields

T

∆F ≈ Eq [∆L] ≈ Eq [ δf (Xt, t), fou(Xt, t) − gt(Xt) ] dt.

(27)

0

This expression could be evaluated in order to estimate the inﬂuence of nonlinear parts of the drift on the approximation error.

6 Experiments

In all experiments, we used different versions of the following general kernel, which is a linear combination of a RBF and a polynomial kernel,

K(x1, x2) = c σRBF exp − (x1 − x2)T (x1 − x2) + (1 − c) 1 + x⊤x2 p ,

(28)

2lR2 BF

1

where the hyperparameters σRBF and lRBF denote the variance and length scale of the RBF kernel and p denotes the order of the polynomial kernel.

Also, we determined the sparse points for the GP algorithm in each case by ﬁrst constructing a
histogram over the observations and then selecting the set of histogram midpoints of each histogram bin which contained at least a certain number bmin of observations. In our experiments, we chose bmin = 5.

6

Figure 2: The ﬁgures show the estimated drift functions for the double well model (left) and the periodic diffusion model (right) after completion of the EM algorithm. Again, the black and blue lines denote mean and 95%-conﬁdence bounds, while the red lines indicate the true drift functions.

6.1 One-dimensional toy models

First we test our algorithm on two toy data sets, the double well model with dynamics given by the

SDE

dx = 4(x − x3)dt + dW

(29)

and a diffusion model driven by a periodic drift

dx = sin(x)dt + dW.

(30)

For both models, we simulated a path of size M = 105 on a regular grid with width ∆t = 0.01 from the corresponding SDE and kept every 20th sample point as observation, resulting in N = 5000 data points. We initialized the EM Algorithm by running the sparse GP for the observation points
without any imputation and subsequently computed the expectation operators by analytically evalu-
ating the expressions on the same time grid as the simulated path and summing over the time steps.
An alternative initialization strategy which consists of generating a full trajectory of the same size as
the original path using Brownian bridge sampling between observations did not bring any noticeable
performance improvements. Since we cannot guarantee that the likelihood increases in every itera-
tion due to the approximation in the E-step, we resort to a simple heuristic by assuming convergence once L stabilizes up to some minor ﬂuctuation. In our experiments convergence was typically attained after a few (< 10) iterations. For the double well model we used an equal weighting c = 0.5 between kernels with hyperparameters σRBF = 1, lRBF = 0.5 and p = 5, whereas for the periodic model we used an RBF kernel (c = 1) with the same values for σRBF and lRBF.

6.2 Application to a real data set
As an example of a real world data set, we used the NGRIP ice core data (provided by NielsBohr institute in Copenhagen, http://www.iceandclimate.nbi.ku.dk/data/), which provides an undisturbed ice core record containing climatic information stretching back into the last glacial. Speciﬁcally, this data set as shown in ﬁgure 3 contains 4918 observations of oxygen isotope concentration δ18O over a time period from the present to roughly 1.23 · 105 years into the past. Since there are generally less isotopes in ice formed under cold conditions, the isotope concentration can be regarded as an indicator of past temperatures.
Recent research [16] suggest to model the rapid paleoclimatic changes exhibited in the data set by a simple dynamical system with polynomial drift function of order p = 3 as canonical model which allows for bistability. This corresponds to a meta stable state at higher temperatures close to marginal stability and a stable state at low values, which is consistent with other research on this data set, linking a stable state of oxygen isotopes to a baseline temperature and a region at higher values corresponding to the occurrence of rapid temperature spikes. For this particular problem we ﬁrst tried to determine the diffusion constant σ of the data. Therefore we estimated the likelihood of the data set for 40 ﬁxed values of σ in an interval from 0.3 to 11.5 by running the EM algorithm with a polynomial kernel (c = 0) of order p = 3 for each value in turn. The resulting drift function with the highest likelihood is shown in ﬁgure 3. The result seems to conﬁrm the existence of a metastable state of oxygen isotope concentration and a stable state at lower values.

7

Figure 3: The ﬁgure on the left displays the NGRIP data set, while the picture on the right shows the estimated drift in black with corresponding 95%-conﬁdence bounds denoting twice the standard deviation in blue for the optimal diffusion value σˆ = 2.9.

Figure 4: The left ﬁgure shows the empirical density for the two-dimensional model, together with the vector ﬁelds of the actual drift function given in blue and the estimated drift given in red. The right picture shows a snippet from the full sample in black together with the ﬁrst 20 observations denoted by red dots.

6.3 Two-dimensional toy model

As an example of a two dimensional system, we simulated from a process with the following SDE:

dx = (x(1 − x2 − y2) − y)dt + dW1,

(31)

dy = (y(1 − x2 − y2) + y)dt + dW2.

(32)

For this model we simulated a path of size M = 106 on a regular grid with width ∆t = 0.002 from the corresponding SDE and kept every 100th sample point as observation, resulting in N = 104 data points. In the inference shown in ﬁgure 4 we used a polynomial kernel (c = 0) of order p = 4.

7 Discussion
It would be interesting to replace the ad hoc local linear approximation of the posterior drift by a more ﬂexible time dependent Gaussian model. This could be optimized in a variational EM approximation by minimizing a free energy in the E-step, which contains the Kullback-Leibler divergence between the linear and true processes. Such a method could be extended to noisy observations and the case, where some components of the state vector are not observed. Finally, this method could be turned into a variational Bayesian approximation, where one optimizes posteriors over both drifts and over state paths. The path probabilities are then inﬂuenced by the uncertainties in the drift estimation, which would lead to more realistic predictions of error bars.
Acknowledgments This work was supported by the European Community’s Seventh Framework Programme (FP7, 2007-2013) under the grant agreement 270327 (CompLACS).

8

References [1] Michalis K. Titsias. Variational learning of inducing variables in sparse Gaussian processes. JMLR
WC&P, 5:567–574, 2009. [2] Marc Deisenroth and Shakir Mohamed. Expectation propagation in Gaussian process dynamical systems.
In P. Bartlett, F.C.N. Pereira, C.J.C. Burges, L. Bottou, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems 25, pages 2618–2626. 2012. [3] Jonathan Ko and Dieter Fox. GP-BayesFilters: Bayesian ﬁltering using Gaussian process prediction and observation models. Autonomous Robots, 27(1):75–90, July 2009. [4] Ce´dric Archambeau, Manfred Opper, Yuan Shen, Dan Cornford, and John Shawe-Taylor. Variational inference for diffusion processes. In J.C. Platt, D. Koller, Y. Singer, and S. Roweis, editors, Advances in Neural Information Processing Systems 20, pages 17–24. MIT Press, Cambridge, MA, 2008. [5] Jose´ Bento Ayres Pereira, Morteza Ibrahimi, and Andrea Montanari. Learning networks of stochastic differential equations. In J. Lafferty, C. K. I. Williams, J. Shawe-Taylor, R.S. Zemel, and A. Culotta, editors, Advances in Neural Information Processing Systems 23, pages 172–180. 2010. [6] Danilo J. Rezende, Daan Wierstra, and Wulfram Gerstner. Variational learning for recurrent spiking networks. In J. Shawe-Taylor, R.S. Zemel, P. Bartlett, F.C.N. Pereira, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems 24, pages 136–144. 2011. [7] Simon Lyons, Amos Storkey, and Simo Sarkka. The coloured noise expansion and parameter estimation of diffusion processes. In P. Bartlett, F.C.N. Pereira, C.J.C. Burges, L. Bottou, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems 25, pages 1961–1969. 2012. [8] Omiros Papaspiliopoulos, Yvo Pokern, Gareth O. Roberts, and Andrew M. Stuart. Nonparametric estimation of diffusions: a differential equations approach. Biometrika, 99(3):511–531, 2012. [9] Yvo Pokern, Andrew M. Stuart, and J.H. van Zanten. Posterior consistency via precision operators for Bayesian nonparametric drift estimation in SDEs. Stochastic Processes and their Applications, 123(2):603–628, 2013. [10] P. E. Kloeden and E. Platen. Numerical Solution of Stochastic Differential Equations. Springer, New York, corrected edition, June 2011. [11] C. E. Rasmussen and C. K. I. Williams. Gaussian Processes for Machine Learning. MIT Press, 2006. [12] Lehel Csato´, Manfred Opper, and Ole Winther. TAP Gibbs free energy, belief propagation and sparsity. In T. G. Dietterich, S. Becker, and Z. Ghahramani, editors, Advances in Neural Information Processing Systems 14, pages 657–663. MIT Press, 2002. [13] C. W. Gardiner. Handbook of Stochastic Methods. Springer, Berlin, second edition, 1996. [14] Manfred Opper, Andreas Ruttor, and Guido Sanguinetti. Approximate inference in continuous time Gaussian-jump processes. In J. Lafferty, C. K. I. Williams, J. Shawe-Taylor, R.S. Zemel, and A. Culotta, editors, Advances in Neural Information Processing Systems 23, pages 1831–1839. 2010. [15] Florian Stimberg, Manfred Opper, and Andreas Ruttor. Bayesian inference for change points in dynamical systems with reusable states—a Chinese restaurant process approach. JMLR WC&P, 22:1117–1124, 2012. [16] Frank Kwasniok. Analysis and modelling of glacial climate transitions using simple dynamical systems. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences, 371(1991), 2013.
9

