arXiv:1505.06449v3 [cs.LG] 2 Jul 2015

Efﬁcient Elastic Net Regularization for Sparse Linear Models

Zachary C. Lipton

Charles Elkan

University of California, San Diego {zlipton, elkan}@cs.ucsd.edu

May 27th, 2015

Abstract
This paper presents an algorithm for efﬁcient training of sparse linear models with elastic net regularization. Extending previous work on delayed updates, the new algorithm applies stochastic gradient updates to non-zero features only, bringing weights current as needed with closed-form updates. Closed-form delayed updates for the ℓ1, ℓ∞, and rarely used ℓ2 regularizers have been described previously. This paper provides closed-form updates for the popular squared norm ℓ22 and elastic net regularizers. We provide dynamic programming algorithms that perform each delayed update in constant time. The new ℓ22 and elastic net methods handle both ﬁxed and varying learning rates, and both standard stochastic gradient descent (SGD) and forward backward splitting (FoBoS). Experimental results show that on a bag-of-words dataset with 260, 941 features, but only 88 nonzero features on average per training example, the dynamic programming method trains a logistic regression classiﬁer with elastic net regularization over 2000 times faster than otherwise.
1 Introduction
For many applications of linear classiﬁcation or linear regression, training and test examples are sparse, and with appropriate regularization, a ﬁnal trained model can be sparse and still achieve high accuracy. It is therefore desirable to be able to train linear models using algorithms that require time that scales only with the number of non-zero feature values.
Incremental training algorithms such as stochastic gradient descent (SGD) are widely used to learn high-dimensional models from large-scale data. These methods process each example one at a time or in small batches, updating the model on the ﬂy. When a dataset is sparse and the loss function is not regularized, this sparsity can be exploited by updating only the weights corresponding to non-zero feature values for each example. However, to prevent overﬁtting to high-dimensional data, it is often useful to

1

apply a regularization penalty, and speciﬁcally to impose a prior belief that the true model parameters are sparse and small in magnitude. Unfortunately, widely used regularizers such as ℓ1 (lasso), ℓ22 (ridge), and elastic net (ℓ1 + ℓ22) destroy the sparsity of the stochastic gradient for each example, so they seemingly require most weights to be updated for every example.
This paper builds upon methods for delayed updating of weights ﬁrst described by [2], [11], and [6]. As each training example is processed, the algorithm updates only those weights corresponding to non-zero feature values in the example. The model is brought current as needed by ﬁrst applying closed-form constant-time delayed updates for each of these weights. For sparse data sets, the algorithm runs in time independent of the nominal dimensionality, scaling linearly with the number of non-zero feature values per example.
To date, constant-time delayed update formulas have been derived only for the ℓ1 and ℓ∞ regularizers, and for the rarely used ℓ2 regularizer. We extend previous work by showing the proper closed form updates for the popular ℓ22 squared norm regularizer and for elastic net regularization. When the learning rate varies (typically decreasing as a function of time), we show that the elastic net update can be computed with a dynamic programming algorithm that requires only constant-time computation per update.1
A straightforward experimental implementation of the proposed methods shows that on a representative dataset containing the abstracts of a million articles from biomedical literature, we can train a logistic regression classiﬁer with elastic net regularization over 2000 times faster than using an otherwise identical implementation that does not take advantage of sparsity. Even if the standard implementation exploits sparsity when making predictions during training, additionally exploiting sparsity when doing updates, via dynamic programming, still makes learning 1400 times faster.

2 Background and Deﬁnitions

We consider a data matrix X ∈ ÊÒ× where each row xi is one of n examples and

Ê each column, indexed by j, corresponds to one of d features. We desire a linear model

parametrized by a weight vector w ∈ that minimizes a convex objective function

F (w) expressible as

n i=1

Fi(w),

where

F

is

the

loss

with

respect

to

the

entire

dataset

X and Fi is the loss due to example xi.

In many datasets, the vast majority of feature values xij are zero. The-bag-of-words

representation of text is one such case. We say that such datasets are sparse. When

features correspond to counts or to binary values, as in bag-of-words, we sometimes

say say that a zero-valued entry xij is absent. We use p to refer to the average number of

nonzero features per example. Naturally, when a dataset is sparse, we prefer algorithms

that take time O(p) per example to those that require time O(d).

1The dynamic programming algorithms below for delayed updates with varying learning rates use time O(1) per update, but have space complexity O(T ) where T is the total number of stochastic gradient updates. If this space complexity is too great, that problem can be solved by allotting a ﬁxed space budget and bringing all weights current whenever the budget is exhausted. As the cost of bringing weights current is amortized across many updates, it adds negligibly to the total running time.

2

2.1 Regularization
To prevent overﬁtting, regularization restricts the freedom of a model’s parameters, penalizing their distance from some prior belief. Widely used regularizers penalize large weights with an objective function of the form

F (w) = L(w) + R(w).

(1)

Many commonly used regularizers R(w) are of the form λ||w|| where λ determines the strength of regularization and the ℓ0, ℓ1, ℓ22, or ℓ∞ norms are common choices for the penalty function. The ℓ1 regularizer is popular owing to its tendency to produce sparse models. In this paper, we focus on elastic net, a linear combination of ℓ1 and ℓ22 regularization that has been shown to produce comparably sparse models to ℓ1 while
often achieving superior accuracy [13].

2.2 Stochastic Gradient Descent
Gradient descent is a common strategy to learn optimal parameter values w. To minimize F (w), a number of steps T , indexed by t, are taken in the direction of the negative gradient:
n
w(t+1) := w(t) − η ∇Fi(w)
i=1
where the learning rate η may be a function of time t. An appropriately decreasing η ensures that the algorithm will converge to a vector w within distance ǫ of the optimal vector for any small value ǫ [2].
Traditional (or “batch”) gradient descent requires a pass through the entire dataset for each update. Stochastic gradient descent (SGD) circumvents this problem by updating the model once after visiting each example. With SGD, examples are randomly selected one at a time or in small so-called mini-batches. For simplicity of notation, without loss of generality we will assume that examples are selected one at a time. At time t + 1 the gradient ∇Fi(w(t)) is calculated with respect to the selected example xi, and then the model is updated according to the rule
w(t+1) := w(t) − η∇Fi(w(t)).
Because the examples are chosen randomly, the expected value of this noisy gradient is identical to the true value of the gradient taken with respect to the entire corpus.
Given a continuously differentiable convex objective function F (w), stochastic gradient descent is known to converge for learning rat√es η that satisfy t ηt = ∞ and
t ηt2 < ∞ [1]. Learning rates ηi ∝ 1/t and ηi ∝ 1/ t both satisfy these properties.2 For many objective functions, such as those of linear or logistic regression without regularization, the noisy gradient ∇Fi(w) is sparse when the input is sparse. In these cases, one needs only to update the weights corresponding to non-zero features in the
2 Some common objective functions, such as those involving ℓ1 regularization, are not differentiable when weights are equal to zero. However, forward backward splitting (FoBoS) offers a principled approach to this problem [11].

3

current example xi. These updates require time O(p), where p ≪ d is the average number of nonzero features in an example.
Regularization, however, can ruin the sparsity of the gradient. Consider an objective function as in Equation (1), where R(w) = ||w|| for some norm || · ||. In these cases, even when the feature value xij = 0, the partial derivative (∂/∂wj)Fi is nonzero owing to the regularization penalty if wj is nonzero. A simple optimization is to update a weight only when either the weight or feature is nonzero. Given feature sparsity and persistent model sparsity throughout training, not updating wj when wj = 0 and xij = 0 provides a substantial beneﬁt. But such an approach still scales with the size of the model, which may be far larger than p. In contrast, the algorithms below scale in time complexity O(p).

2.3 Forward Backward Splitting
Proximal algorithms are an approach to optimization in which each update consists of solving a convex optimization problem [9]. Forward Backward Splitting (FoBoS) [11] is a proximal algorithm that provides a principled approach to online optimization with non-smooth regularizers. We ﬁrst step in the direction of the negative gradient of the differentiable unregularized loss function. We then update the weights by solving a convex optimization problem that simultaneously penalizes distance from the new parameters and minimizes the regularization term.
In FoBoS, ﬁrst a standard unregularized stochastic gradient step is applied:

w

(

t+

1 2

)

=

w(t)

−

η(t)∇Li(w(t)).

Note that if (∂/∂wj)Li = 0 then wj(t+ 12 ) = wj(t). Then a convex optimization is solved, applying the regularization penalty. For elastic net the problem to be solved is

w(t+1) = argminw 12 ||w − w(t+ 12 )||22 + ηtλ1||w||1 + 21 ηtλ2||w||22 . (2)

The problems corresponding to ℓ1 or ℓ22 separately can be derived by setting the corresponding λ to 0.

3 Lazy Updates
The idea of lazy updating was introduced in [2], [6], and [11]. This paper extends the
Ê idea for the cases of ℓ22 and elastic net regularization. The essence of the approach is
given in Algorithm (1). We maintain an array ψ ∈ d in which each ψj stores the index of the last iteration at which the value of weight j was current. When processing example xi at time k, we iterate through its nonzero features xij . For each such nonzero feature, we lazily apply the k − ψj delayed updates collectively in constant time, bringing its weight wj current. Using the updated weights, we compute the prediction yˆ(k) with the fully updated relevant parameters from w(k). We then compute the gradient and update these parameters.

4

Algorithm 1 Lazy Updates
Require: ψ ∈ Êd
for t ∈ 1, ..., T do Sample xi randomly from the training set for j s.t. xij = 0 do wj ← Lazy(wj, t, ψj) ψj ← t end for w ← w − ∇Fi(w)
end for
When training is complete, we pass once over all nonzero weights to apply the delayed updates to bring the model current. Provided that we can apply any number k − ψj of delayed updates in O(1) time, the algorithm processes each example in O(p) time regardless of the dimension d.
To use the approach with a chosen regularizer, it remains only to demonstrate the existence of constant time updates. In the following subsections, we derive constanttime updates for ℓ1, ℓ22 and elastic net regularization, starting with the simple case where the learning rate η is ﬁxed during each epoch, and extending to the more complicated case when the learning rate is decreased every iteration as a function of time.3
4 Prior Work
Over the last several years, a large body of work has advanced the ﬁeld of online learning. Notable contributions include ways of adaptively decreasing the learning rate separately for each parameter such as AdaGrad [3] and AdaDelta [12], using small batches to reduce the variance of the noisy gradient [7], and other variance reduction methods such as Stochastic Average Gradient (SAG) [10] and Stochastic Variance Reduced Gradient (SVRG) [5].
Æ In 2008, Carpenter described an idea for performing lazy updates for stochastic
gradient descent [2]. With that method, we maintain a vector ψ ∈ d, where each ψi stores the index of the last epoch in which each weight was last regularized. We then perform periodic batch updates. However, as the paper acknowledges, the approach described results in updates that do not produce the same result as applying an update after each time step.
Langford et al. concurrently developed an approach for lazily updating ℓ1 regularized linear models [6]. They restrict attention to ℓ1 models. Additionally, they describe the closed form update only when the learning rate η is constant, although they suggest that an update can be derived when ηt decays as t grows large. We derive constant-time updates for ℓ22 and elastic net regularization. Our algorithms are applicable with both ﬁxed and varying learning rates.
3 The results hold for schedules of weight decrease that depend on time, but cannot be directly applied to AdaGrad [3] or RMSprop, methods where each weight has its own learning rate which is decreased with the inverse of the accumulated sum (or moving average) of squared gradients with respect to that weight.
5

In 2008 also, as mentioned above, Duchi and Singer described the FoBoS method
[4]. They share the insight of applying updates lazily when training on sparse highdimensional data. Their lazy updates hold for norms ℓq for q ∈ {1, 2, ∞}, However they do not hold for the commonly used ℓ22 squared norm. Consequently they also do not hold for mixed regularizers involving ℓ22 such as the widely used elastic net (ℓ1 + ℓ22).

5 Constant-Time Lazy Updating for SGD
In this section, we derive constant-time stochastic gradient updates for use when processing examples from sparse datasets. Using these, the lazy update algorithm can train linear models with time complexity O(p) per example. For brevity, we describe the more general case where the learning rate is varied. When the learning rate is constant the algorithm can be easily modiﬁed to have O(1) space complexity.

5.1 Lazy ℓ1 Regularization with Decreasing Learning Rate
The closed-form update for ℓ1 regularized models is [11]

wj(k) = sgn(wj(ψj )) |wj(ψj )| − λ1 (S(k − 1) − S(ψj − 1))
+

where S(t) is a function that returns the partial sum

t τ =0

η(τ ).

The

sum

t+n−1 τ =t

η(τ

)

can be computed in constant time using a caching approach. On each iteration t, we

compute S(t) in constant time given its predecessor as S(t) = η(t) + S(t − 1). The

base case for this recursion is S(0) = η(0). We then cache this value in an array for

subsequent constant time lookup.

When the learning rate decays with 1/t, the terms η(τ) follow the harmonic series,.

Each partial sum of the harmonic series is a harmonic number H(t) =

t i=1

1/t.

Clearly

t+n−1

η(τ) = η(0) (H(t + n) − H(t))

τ =t

where Hτ is the τth harmonic number. While there is no closed-form expression to calculate the τth harmonic number, there exist good approximations.
The O(T ) space complexity of this algorithm may seem problematic. However, this problem is easily dealt with by bringing all weights current after each epoch. The
cost to do so is amortized across all iterations and is thus negligible.

5.2

Lazy

ℓ2 2

Regularization

with

Decreasing

Learning

Rate

For a given example xi, if the feature value xij = 0 and the learning rate is varying, then the stochastic gradient update rule for an ℓ22 regularized objective is

wj(t+1) = wj(t) − η(t)λ2wj(t).

6

The decreasing learning rate prevents collecting successive updates as terms in a geometric series, as we could if the learning rate were ﬁxed. However, we can employ a dynamic programming strategy.
Lemma 1. For SGD with ℓ22 regularization, the constant-time lazy update to bring a weight current from iteration ψj to k is

w(k) = w(ψj ) P (k − 1)

j

j P (ψj − 1)

where P(t) is the partial product

t τ

=0(1

−

η(τ

)λ2

).

Proof. Rewriting the multiple update expression yields

wj(t+1) = wj(t)(1 − η(t)λ2) wj(t+n) = wj(t)(1 − η(t)λ2)(1 − η(t+1)λ2) · ... · (1 − η(t+n−1)λ2).

The products P (t) =

t τ

=0(1

−

η(τ

)λ2)

can be cached on each iteration in constant

time using the recursive relation

P (t) = (1 − η(t)λ2)P (t − 1).

The base case is P (0) = a0 = (1 − η0λ2). Given cached values P (0), ..., P (t + n), it is then easy to calculate the exact lazy update in constant time:

wj(t+n) = wj(t) P (Pt (+t −n −1)1) .

The claim follows.
As in the case of ℓ22 regularization with ﬁxed learning rate, we need not worry that the regularization update will ﬂip the sign of the weight wj, because P (t) > 0 for all t ≥ 0.

5.3 Lazy Elastic Net Regularization with Decreasing Learning Rate
Next, we derive the constant time lazy update for SGD with elastic net regularization. Recall that a model regularized by elastic net has an objective function of the form

F (w) = L(w) + λ1||w||1 + λ22 ||w||22. When a feature xj = 0, the SGD update rule is

wj(t+1) = sgn(wj(t)) |wj(t)| − η(t)λ1 − η(t)λ2|wj(t)|

+

(3)

= sgn(wj(t)) (1 − η(t)λ2)|wj(t)| − η(t)λ1

+

7

Theorem 1. To bring the weight wj current from time ψj to time k using repeated Equation (3) updates, the constant time update is

w(k) = sgn(w(ψj)) |w(ψj)| P (k − 1) − P (k − 1) · (B(k − 1) − B(ψj − 1))

j

j

j P (ψj − 1)

+

where P (t) = (1 − η(t)λ2) · P (t − 1) with base case P (−1) = 1 and B(t) =

t τ =0

η(τ )/P (τ

−

1)

with

base

case

B(−1)

=

0.

Proof. The time-varying learning rate prevents us from working out a simple expansion. Instead, we can write the following inductive expression for consecutive terms in the sequence:

wj(t+1) = sgn(wj(t)) (1 − η(t)λ2)|wj(t)| − η(t)λ1
+

Writing aτ = (1 − η(τ)λ2) and bτ = −η(τ)λ1 gives

w(t+1) = sgn(wj(t)) ...
w(t+n) = sgn(wj(t))
= sgn(wj(t))

at|w(t)| + bt
+

a(t+n−1)(...a(t+1) atwt − bt − b(t+1)...) − b(t+n−1) +

t+n−1

t+n−2

|wj(t) |

aτ +

bi

t+n−2
aq

+ b(t+n−1)

τ =t

τ =t

q=τ

+

The leftmost term

t+n−1 τ =t

aτ

can

be

calculated

in

constant

time

as

P (t+n−1)/P (t−

1) using cached values from the dynamic programming scheme discussed in the pre-

vious section. To cache the remaining terms, we group the center and rightmost terms

and apply the simpliﬁcation

t+n−2
bi
τ =t

t+n−2
aq
q=τ

+ bt+n−1

P (t + n − 2)

P (t + n − 2)

P (t + n − 2)

= bt P (t − 1) + bt+1 P (t) + ... + bt+n−1 P (t + n − 2)

η(t)

η(t+1)

η(t+n−1)

= −λ1P (t + n − 2) P (t − 1) + P (t) + ... + P (t + n − 2) .

We now add a new layer to the dynamic programming formulation. In addition to

precalculating all values P (t) as we go, we deﬁne a partial sum over inverses of partial

products

t

η(τ )

B(t) = P (τ − 1) .

τ =0

Given that P (t − 1) can be accessed in constant time at time t, B(t) can now be cached in constant time. With the base case B(−1) = 0, the dynamic programming here

8

depends upon the recurrence relation

B(t) = B(t − 1) + η(t) . P (t − 1)

Then, for SGD elastic net with decreasing learning rate, the update rule to apply any number n of consecutive regularization updates in constant time to weight wj is

w(t+n) = sgn(w(t)) |w(t)| P (t + n − 1) − λ1P (t + n − 1) (B(t + n − 1) − B(t − 1))

j

j P (t − 1)

+

6 Lazy Updates for Forward Backward Splitting

Here we turn our attention to FoBoS updates for ℓ22 and elastic net regularization. For ℓ22 regularization, to apply the regularization update we solve the problem from Equation (2) with λ1 set to 0. Solving for w∗ gives the update

(t+1)

wj(t)

wj = 1 + η(t)λ2

when xij = 0. Note that this differs from the standard stochastic gradient descent step. We can store the values Φ(t) = tτ=0 1+η1tλ2 . Then, the constant time lazy update for FoBoS with ℓ22 regularization to bring a weight current at time k from time ψj is

w(k) = w(ψj) Φ(k − 1)

j

j Φ(ψj − 1)

where Φ(t) = (1 + η(t)λ2)−1 · Φ(t − 1) with base case Φ(0) = 1+η10λ2 . Finally, in the case of elastic net regularization via forward backward splitting, we
solve the convex optimization problem from Equation (2). This objective also comes apart and can be optimized for each wj separately. Setting the derivative with respect to wj to zero yields the solution

(t+1)

(t) |wj(t)| − η(t)λ1

wj = sgn(wj ) ηtλ2 + 1

+

Theorem 2. A constant-time lazy update for FoBoS with elastic net regularization and decreasing learning rate to bring a weight current at time k from time ψj is

w(k) = sgn(w(ψj)) |w(ψj)| Φ(k − 1) − Φ(k − 1) · λ1 (β(k − 1) − β(ψj − 1))

j

j

j Φ(ψj − 1)

+

where Φ(t) = Φ(t−1)· 1+η1tλ2 with base case Φ(−1) = 1 and β(t) = β(t−1)+ Φ(ηt(−t)1) with base case β(−1) = 0.

9

Proof. Write at = (η(t)λ2 + 1)−1 and bt = −η(t)λ1. Note that neither at nor bt depends upon wj . Consider successive updates:

wj(t+1) = sgn(wj(t)) at(|wj(t)| + bt)
+



t+n−1

t+n−1

wj(t+n) = sgn(wj(t)) |wj(t)|

aβ +

β=t

τ =t

t+n−1

bτ

aα

α=τ

 .
+

Inside the square brackets, ΦΦ(t(+t−n−1)1) can be substituted for term can be expanded as

t+n−1 β=t

aβ

and

the

second

t+n−1 τ =t

t+n−1

bτ

aα

α=τ

t+n−1
=
τ =t

Φ(t + n − 1) bτ Φ(τ − 1)

t+n−1

= −Φ(t + n − 1) · λ1

τ =t

η(τ ) Φ(τ − 1)

Using the dynamic programming approach, for each time t, we calculate

β(t) = β(t − 1) + η(t) Φ(t − 1)

with the base cases β(0) = η(0) and β(−1) = 0. Then

w(t+n) = sgn(w(t)) |w(t)| Φ(t + n − 1) − Φ(t + n − 1) · λ1 (β(t + n − 1) − β(t − 1))

j

j

j Φ(t − 1)

+

7 Experiments
The usefulness of logistic regression with elastic net regularization is well-known. To conﬁrm the correctness and speed of the dynamic programming algorithm just presented, we implemented it and tested it on a bag-of-words representation of abstracts from biomedical articles indexed in Medline as described in [8]. The dataset contains exactly 106 examples, 260, 941 features and an average of 88.54 nonzero features per document.
We implemented algorithms in Python. Datasets are represented by standard sparse SciPy matrices. We implemented both standard and lazy FoBoS for logistic regression regularized with elastic net. We conﬁrmed on a synthetic dataset that the standard FoBoS updates and lazy updates output essentially identical weights. To make a fair comparison, we also report results where the non-lazy algorithm exploits sparsity when calculating predictions. Even when both methods exploit sparsity to calculate yˆ, lazy updates lead to training over 1400 times faster. Note that sparse data structures must
10

SGD FoBoS

Lazy Updates .0102 .0120

Dense Updates 21.377 22.511

Dense with Sparse Predictions 14.381 16.785

Table 1: Average time in seconds for each algorithm to process one example.

be used even with dense updates, because a dense matrix to represent the input dataset would use an unreasonable amount of memory.
Logistic regression with lazy elastic net regularization runs approximately 2000 times faster than with dense regularization updates for both SGD and FoBoS. In the absence of overhead, exploiting sparsity should yield a 2947× speedup. Clearly the additional dynamic programming calculations do not erode the beneﬁts of exploiting sparsity. While the dynamic programming strategy consumes space linear in the number of iterations, it does not present a major time penalty. Concerning space, storing two ﬂoating point numbers for each time step t is a modest use of space compared to storing the data itself. Further, if space ever were a problem, all weights could periodically be brought current. The cost of this update would be amortized across all iterations and thus would be negligible.
8 Discussion
Many interesting datasets are high-dimensional, and many high-dimensional datasets are sparse. To be useful, learning algorithms should have time complexity that scales with the number of non-zero feature values per example, as opposed to with the nominal dimensionality. This paper provides algorithms for fast training of linear models with ℓ22 or with elastic net regularization. Experiments conﬁrm the correctness and empirical beneﬁt of the method. In future work we hope to use similar ideas to take advantage of sparsity in nonlinear models, such as the sparsity provided by rectiﬁed linear activation units in modern neural networks.
Acknowledgments
This research was conducted with generous support from the Division of Biomedical Informatics at the University of California, San Diego, which has funded the ﬁrst author via a training grant from the National Library of Medicine. Galen Andrew began evaluating lazy updates for multilabel classiﬁcation with Charles Elkan in the summer of 2014. His notes provided an insightful starting point for this research. Sharad Vikram provided invaluable help in checking the derivations of closed form updates.
References
[1] Le´on Bottou. Stochastic gradient descent tricks. In Neural Networks: Tricks of the Trade, pages 421–436. Springer, 2012.

11

[2] Bob Carpenter. Lazy sparse stochastic gradient descent for regularized multinomial logistic regression. Alias-i, Inc., Tech. Rep, pages 1–20, 2008.
[3] John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic optimization. The Journal of Machine Learning Research, 12:2121–2159, 2011.
[4] John Duchi, Shai Shalev-Shwartz, Yoram Singer, and Tushar Chandra. Efﬁcient projections onto the l 1-ball for learning in high dimensions. In Proceedings of the 25th international conference on Machine learning, pages 272–279. ACM, 2008.
[5] Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance reduction. In Advances in Neural Information Processing Systems, pages 315–323, 2013.
[6] John Langford, Lihong Li, and Tong Zhang. Sparse online learning via truncated gradient. In Advances in neural information processing systems, pages 905–912, 2009.
[7] Mu Li, Tong Zhang, Yuqiang Chen, and Alexander J Smola. Efﬁcient mini-batch training for stochastic optimization. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 661– 670. ACM, 2014.
[8] Zachary C Lipton, Charles Elkan, and Balakrishnan Naryanaswamy. Optimal thresholding of classiﬁers to maximize f1 measure. In Machine Learning and Knowledge Discovery in Databases, pages 225–239. Springer, 2014.
[9] Neal Parikh and Stephen Boyd. Proximal algorithms. Foundations and Trends in optimization, 1(3):123–231, 2013.
[10] Mark Schmidt, Nicolas Le Roux, and Francis Bach. Minimizing ﬁnite sums with the stochastic average gradient. arXiv preprint arXiv:1309.2388, 2013.
[11] Yoram Singer and John C Duchi. Efﬁcient learning using forward-backward splitting. In Advances in Neural Information Processing Systems, pages 495–503, 2009.
[12] Matthew D Zeiler. Adadelta: an adaptive learning rate method. arXiv preprint arXiv:1212.5701, 2012.
[13] Hui Zou and Trevor Hastie. Regularization and variable selection via the elastic net. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 67(2):301–320, 2005.
12

