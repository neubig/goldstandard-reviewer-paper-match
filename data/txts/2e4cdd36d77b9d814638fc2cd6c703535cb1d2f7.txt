Input-Tuning: Adapting Unfamiliar Inputs to Frozen Pretrained Models
Shengnan An1∗, Yifei Li2∗, Zeqi Lin3, Qian Liu4, Bei Chen3, Qiang Fu3, Weizhu Chen5, Nanning Zheng1, Jian-Guang Lou3
1 Xi’an Jiaotong University; 2 Peking University; 3 Microsoft Research Asia; 4 Beihang University; 5 Microsoft Azure AI
{an1006634493@stu, nnzheng@mail}.xjtu.edu.cn
liyifei@stu.pku.edu.cn; qian.liu@buaa.edu.cn
{Zeqi.Lin, beichen, qifu, wzchen, jlou}@microsoft.com

arXiv:2203.03131v1 [cs.CL] 7 Mar 2022 Relative Performance w.r.t. Fine-Tuning (%)

Abstract
Recently the prompt-tuning paradigm has attracted signiﬁcant attention. By only tuning continuous prompts with a frozen pretrained language model (PLM), prompt-tuning takes a step towards deploying a shared frozen PLM to serve numerous downstream tasks. Although prompt-tuning shows good performance on certain natural language understanding (NLU) tasks, its effectiveness on natural language generation (NLG) tasks is still underexplored. In this paper, we argue that one of the factors hindering the development of prompt-tuning on NLG tasks is the unfamiliar inputs (i.e., inputs are linguistically different from the pretraining corpus). For example, our preliminary exploration reveals a large performance gap between prompt-tuning and ﬁne-tuning when unfamiliar inputs occur frequently in NLG tasks. This motivates us to propose input-tuning, which ﬁne-tunes both the continuous prompts and the input representations, leading to a more effective way to adapt unfamiliar inputs to frozen PLMs. Our proposed input-tuning is conceptually simple and empirically powerful. Experimental results on seven NLG tasks demonstrate that input-tuning is signiﬁcantly and consistently better than prompt-tuning. Furthermore, on three of these tasks, input-tuning can achieve a comparable or even better performance than ﬁne-tuning.
1 Introduction
Recently, there has been a surge of interest in prompt-tuning (Liu et al., 2021a; Jiang et al., 2020; Shin et al., 2020; Liu et al., 2021c; Lester et al., 2021). Unlike ﬁne-tuning that adapts pretrained language models (PLMs) to downstream tasks by tuning all parameters, prompt-tuning is based on the intuition that tasks can be adapted to a frozen PLM via proper task-speciﬁc contexts.
∗Work done during an internship at Microsoft Research. The ﬁrst two authors contributed equally to this paper.

105 100 98.2 (+2.9) 95 90

100.9 (+6.2)

101.2 (+7.1)

97.0 (+3.3)

91.9 (+6.1)

88.2 (+5.7)

91.8 (+11.8)

85

80

75 DART

E2E

ONR

ToTTo

Fr2En

De2En

Ro2En

Prompt-Tuning With Input-Tuning

Figure 1: Relative performance (w.r.t. ﬁne-tuning) of prompt-tuning and input-tuning on seven NLG tasks with unfamiliar inputs. The frozen PLM is T5-LARGE (∼770M parameters) (Raffel et al., 2020a). The performance of prompt-tuning lags behind that of ﬁne-tuning. Input-tuning (our method) can close this gap signiﬁcantly and consistently.

Concretely, for each task, prompt-tuning freezes PLM parameters, but optimizes a small continuous task-speciﬁc prompt (i.e., a vector sequence) to be prepended to the input text. Prompt-tuning is appealing, as it takes a step towards deploying a frozen PLM as a common cloud service that can serve many downstream tasks from customers.
The intuition behind prompt-tuning is to cast each downstream task into a language model format that ﬁts the pretraining objective. This leads to the remarkable success of prompt-tuning on natural language understanding (NLU) tasks (Liu et al., 2021c; Lester et al., 2021). However, many tasks, especially natural language generation (NLG) tasks, involve unfamiliar inputs, i.e., the input sequences are linguistically different from the pretraining corpus. For example, in a logic-to-text task, the input sequences are written in some domain-speciﬁc logic expressions (e.g. “Sort(datePublished asc of MusicRecording())[1];”) and the output sequences are their natural descriptions (e.g., “Re-

Figure 2: Input-tuning: comparing to prompt-tuning, we propose to add a lightweight trainable module between word embeddings and the bottom layer of the frozen PLM, thus affecting the encoding of unfamiliar inputs more directly and effectively.

trieve the ﬁrst song”). Moreover, for PLMs pretrained on English corpus, machine translation tasks with non-English inputs (e.g., French, German, and Romanian) can also be regarded as tasks with unfamiliar inputs. We can hardly cast such inputs to a format that ﬁts the pretraining objective by simply prompting. Therefore, it is natural to ask whether prompt-tuning can be useful at such tasks, but this has not yet been discussed in the prior work.
Our preliminary exploration shows that, on NLG tasks with unfamiliar inputs, the performance of prompt-tuning lags far behind that of ﬁne-tuning (Figure 1, detailed in Section 5). Though Lester et al. (2021) reported that, on NLU tasks, prompttuning can match the performance of ﬁne-tuning by using giant PLMs (>10 billion parameters), our preliminary results indicate that this success can hardly be replicated on tasks with unfamiliar inputs: for example, on the E2E task, changing the frozen PLM from T5-Large(∼770M parameters) to T511B only improves the BLEU score from 64.5 to 65.6, while ﬁne-tuning on T5-Large achieves 68.5.
In this paper, we ﬁrst conduct preliminary exploration to empirically show how the performance of prompt-tuning is limited by unfamiliar inputs. In particular, we ﬁnd that: the gap between prompttuning and ﬁne-tuning can be controlled by manually transforming the inputs towards/away the pretraining distribution. Therefore, to adapt unfamiliar inputs to the frozen PLM effectively, not only prompting, but also transforming their surface representations directly, is required.
This ﬁnding indicates the limitation of prompttuning and the direction to alleviate it, but it still remains challenging to effectively generalize prompttuning to tasks with unfamiliar inputs, since it is not easy to ﬁnd the best way that the unfamiliar

inputs should be transformed. To mitigate this, we propose input-tuning, which bridges the gap between prompt-tuning and ﬁne-tuning (Figure 1) by making the transformation learnable. Concretely, we add a lightweight trainable module between word embeddings and the bottom layer of the PLM, to adjust the encoding of unfamiliar inputs directly (i.e., the “input-adapter” module in Figure 2). For each task, the soft prompt and the input-adapter are the only two trainable modules, and they are optimized jointly. During inference, each input sequence is encoded by the input-adapter, concatenated to the soft prompt, and then sent to the frozen PLM to generate an output sequence.
Experimental results on seven NLG tasks demonstrate that input-tuning is signiﬁcantly and consistently better than prompt-tuning. Moreover, on three of these tasks, input-tuning can achieve comparable or even better performance than ﬁne-tuning. We further explore the effectiveness of input-tuning with different backbones, showing that it is applicable with both the encoder-decoder architecture and the auto-regressive language model (Section 6.1). For different data scales, input-tuning stably outperforms prompt-tuning and prefers low-resource scenarios (Section 6.2).

2 Background

2.1 Sequence-to-Sequence Learning
Sequence-to-sequence learning aims to model the conditional probability of the target sequence y = {y1, ..., ym} given the source sequence x = {x1, ..., xn}:

m

P (y|x) = P (yi|x, y<i),

(1)

i=1

where the factor P (yi|x, y<i) can be estimated by a neural model pφ(yi|x, y<i) parametrized by φ. Suppose that pφ is a pretrained language model, either autoregressive language model (such as GPT2 (Brown et al., 2020b) and UniLM (Dong et al.)) or encoder-decoder model (such as T5 (Raffel et al., 2020a) and BART (Lewis et al., 2020)).
In the ﬁne-tuning paradigm, given a train dataset D, the full set of the pretrained weights φ0 are updated by gradient descent methods to optimize the objective:

|y|

max

log (pφ(yi|x, y<i)). (2)

φ

(x,y)∈D i=1

2.2 Prompt-Tuning
Brown et al. (2020a) showed that prompt design is surprisingly effective at adapting multiple downstream tasks to a frozen GPT-3 model, by prepending a natural language task instruction and a few examples to the task input. Lester et al. (2021) proposed prompt-tuning, in which each prompt is not a text sequence manually designed, but a sequence of continous embeddings that can be optimized by learning from the train dataset D.
Given a PLM pφ and an input sequence x = {x1, ..., xn}, the ﬁrst thing pφ does is to embed x as a matrix X ∈ Rn×e, where e is the dimension of the embedding space. In prompt-tuning, a soft prompt can also be represented as a matrix C ∈ Rk×e, where k is a hyperparameter that controls the prompt length. This soft prompt is then concatenated to X , forming a single matrix [C ; X ] ∈ R(k+n)×e. This matrix is feed into the frozen PLM (from the bottom layer of the Transformer architecture) to generate the output sequence y:
m
P (y|x) = pφ(yi|[C ; X ], y<i). (3)
i=1

During training, PLM parameters are frozen (i.e., φ ≡ φ0), while the parameters of C (denoted as θC ) are updated by:

|y|

max

log(pφ0(yi|[C ; X ], y<i)). (4)

θC (x,y)∈D i=1

Existing work uses prompt-tuning to address tasks like SuperGLUE (Lester et al., 2021), while lacking further investigation on its effectiveness for more tasks such as machine translations.

Figure 3: Relative performance and input familiarity of machine translation and table-to-text tasks. It shows the trend that the relative performance of the prompttuning rises as the pretrained model’s familiarity with the task inputs increases. This trend indicates that alleviating the unfamiliarity by transforming the inputs can help prompt-tuning perform better.

Variant E2E E2E+ E2E− E2E−−

Input Example
name[The Punter], food[Indian], priceRange[cheap]
name is The Punter, food is Indian, priceRange is cheap.
nom[The Punter], nourriture[Indian], gamme de prix[cheap]
nom[Le Punter], nourriture[Indienne], gamme de prix[pas cher]

Table 1: Variants of E2E inputs. The + sufﬁx means that we increase the familiarity by manually transforming the inputs, while − and −− contra.

3 Preliminary Exploration
In this section, we start from an observation that on machine translation tasks there are performance gaps between prompt-tuning and ﬁnetuning, which are related to unfamiliar inputs (Section 3.1). We further ﬁnd that this gap can be controlled by manually transforming the inputs (Section 3.2), motivating the idea that we can use a trainable module to learn what is the best way to transform the unfamiliar inputs (Section 3.3).
3.1 Unfamiliar Inputs in Prompt-Tuning
We start by investigating the effectiveness of prompt-tuning on three machine translation tasks: Fr-En, De-En and Ro-En. We use the datasets from WMT 2014 (Bojar et al., 2014) for Fr-En and DeEn, and WMT 2016 (Bojar et al., 2016) for Ro-En.

Concretely, we use T5-Large (Raffel et al., 2020b) as the frozen PLM, and measure the gap between Prompt-Tuning (PT) and Fine-Tuning (FT) based on the Relative Performance (RP), which is calculated by:

RP = BLEUPT . (5) BLEUFT

Experimental results show that, on all machine translation tasks, prompt-tuning can only achieve 85.8% / 82.5% / 80.1% of the ﬁne-tuning performance, respectively. This unexpected phenomena leads to a natural follow-up research question: where does the gap come from?
Since T5-Large is mostly pretrained on English natural language corpus, a reasonable speculation to the question is that the frozen PLM lacks sufﬁcient knowledge of how to encode the non-English inputs in these three tasks, leading to the large gap between prompt-tuning and ﬁne-tuning. In other words, the frozen PLM is unfamiliar with the inputs. To make it more clear, we quantify the Familiarity (denoted by Fam) of a task’s inputs to a frozen PLM using the bi-gram frequencies in the pretraining corpus. Concretely, we deﬁne the Familiarity of task inputs as:

1

n−1 log BiC(xi:i+1)

Fam = |X|

n − 1 , (6)

x∈X i=1

where x is a token sequence from all inputs X in the task, and BiC(xi:i+1) returns the bi-gram counts of xi:i+1. The red points in Figure 3 shows the RP of translation tasks with different input familiarities. It shows the trend that the more familiar with inputs, the higher RP the prompt-tuning achieves, which implies that the performance gap partly comes from the unfamiliarity. Based on this observation, we suppose that alleviating the unfamiliarity by transforming the inputs can help prompt-tuning perform better.

3.2 Manually Transforming Inputs
Next step, to evaluate the effectiveness of transforming inputs, we conduct some controllable experiments based on a table-to-text task E2E (Novikova et al., 2017). The inputs of E2E are synthetic linearized tables, which are easy to manually transform. We consider two kinds of input transformations: more familiar transformation (denote with sufﬁx +) that changes the inputs more

like natural language, and less familiar transformation (denote with sufﬁx − and −−) that makes the bi-gram counts of input lower. Details of these transformations are listed in the Appendix A. Some input examples are shown in Table 1.
As shown in Figure 3 (painted in blue), the task with more familiar inputs (E2E+) achieve better performance than the original task while the tasks with less familiar inputs (E2E− and E2E−−) perform contra. We also conduct another controllable experiment on logic-to-text tasks and observe the similar performances (detailed in Appendix A). This trend is similar to it in machine translation tasks, which assists the speculation before. Furthermore, observing the gain from E2E to E2E+, it indicates that transforming inputs is a promising way to alleviate the unfamiliarity and improve the prompt-tuning.
3.3 Motivation
As discussed before that manually transforming the inputs can beneﬁt the prompt-tuning, but apparently, it is inefﬁcient and error-prone for nonsynthetic tasks such as translations. Moreover, separating the transforming processes with prompttuning could cause incompatibility between two modules.
Based on these concerns, our motivation is to design a transforming method that meets two requirements: ﬁrst, it should be universally applicable for different tasks and pretrained backbones. Second, it could be optimized automatically and jointly with prompt-tuning. Therefore, we proposed to utilize a neural-based adapter to transform the continuous input embeddings.
4 Input-Tuning
Based on our motivation, we extend the prompttuning with a neural-based adapter to transform the input embeddings, namely input-tuning. Figure 2 illustrates the basic idea of input-tuning. Given a frozen PLM pφ0 and an input sequence x, inputtuning generates the output sequence y by:
m
P (y|x) = pφ0(yi|F (x), y<i) (7)
i=1
where F(·) is a learnable function deciding how x should be “tuned” to activate pφ0 to generate proper y. Prompt-tuning (Equation 4) can be regarded as a special case of input-tuning, where F(·) is deﬁned

E2E DART ToTTo ONR Fr-En De-En Ro-En

Data Size
47.4K 81.6K 136.1K 13.7K ∼ 2M ∼ 2M ∼ 400K

Vocab Size
3.0K 33.2K 136K 1.3K 373K 586K 169K

Input Len (Avg)
30.3 30.2 151.3 56.9 28.8 26.2 26.8

Output Len (Avg)
21.7 21.1 18.0 9.3 27.5 27.4 26.8

Table 2: Task statistics.

Figure 4: The illustration of our Input-Adapter.

as the concatenation of C (the soft prompt) and X (the embedding matrix of x).
In this paper, we model F(·) as:

F(x) = [C ; T (X )],

(8)

T (X )i,∗ =X + σ(Xi,∗W1)W2, (9)
for each i = 1, ..., n.

Here σ is an element-wise nonlinear activation function (ReLU). W1 and W2 are learnable matrices. We denote T (·) as an input-adapter, since it plays the role that adapts the surface representations of x to better utilize the frozen PLM. Figure 4 illustrates this module.

Initialization. We initialize the soft prompt C by randomly picking from pretrained embeddings, as Lester et al. (2021) claimed that it is much better than totally random initialization. For parameters of the input-adapter, we initialize W2 as a zero matrix so that the T (X ) will be the same with pretrained embeddings at the beginning of training.

Token-wise or Sequence-wise. Input-adapter tunes the surface representation of each token in x respectively, without taking sequence contexts into consideration (e.g., model T (X ) using RNNs or Transformers). The key reason of this design choice is to prevent the input-adapter dominating the frozen PLM. Concretely, as RNNs and Transformers are already good sequence-to-sequence learners, using them to implement the inputadapter may lead to the result that the sequenceto-sequence task is mainly addressed in the inputadapter while the frozen PLM contributes little. Therefore, we chose a simple token-wise MLP to implement the input-adapter, which is far from sufﬁcient for sequence-to-sequence learning, thus activating the frozen PLM better. Our experimental results also support this design choice (Section 6.3).

5 Experiments
5.1 Experimental Setup
Tasks and Metrics We evaluate on seven NLG tasks: three table-to-text tasks, one logic-to-text task, and three machine translation tasks. Table 2 shows the statistics of these datasets.
The table-to-text tasks are: E2E (Novikova et al., 2017), DART (Nan et al., 2020), and ToTTo (Parikh et al., 2020). E2E is relatively simpler than the other two tasks, as it only has 1 domain (i.e., restaurant reviews). DART and ToTTo are both opendomain, using open-domain tables from Wikipedia. ToTTo is the most challenging task among them, as it increases the generalization challenge by reducing the vocabulary overlap between train set and dev/test set.
The logic-to-text task is ONR (OverNightReverse), which is constructed based on the textto-logic benchmark Overnight (Wang et al., 2015) (detailed in Appendix B).
The Machine translation tasks are: French to English (Fr-En), German to English (De-En) and Romanian to English (Ro-En). We use the parallel corpus from version 7 of Europarl corpus (Koehn et al., 2005) as the training data for all three translation tasks. For Fr-En and De-En, we take WMT newstest2013 newstest2014 (Bojar et al., 2014) as the development sets and test sets, respectively. For Ro-En, we use the development and test sets from WMT 2016 (Bojar et al., 2016).
For every tasks, we report the ofﬁcial evaluation metrics, including BLEU (Papineni et al., 2002), NIST (Belz and Reiter, 2006), METEOR (Lavie and Agarwal, 2007), ROUGE-L (Lin, 2004), CIDEr (Vedantam et al., 2015), TER (Snover et al., 2005), MoverScore (Zhao et al., 2019), BERTScore (Zhang et al., 2019), BLEURT (Sellam et al., 2020) and PARENT (Dhingra et al., 2019).

E2E

DART

ONR

BLEU NIST MET R-L CIDEr BLEU MET TER↓ Mov BERT BLRT BLEU NIST MET R-L CIDEr

T5-Large Prompt-Tuning 64.5 8.35 44.1 67.3 2.23 46.9 0.39 0.47 0.51 0.95 0.45 65.3 10.10 46.9 78.4 2.76 Input-Tuning 68.7 8.74 46.1 70.7 2.42 48.3 0.39 0.46 0.52 0.95 0.47 70.2 10.50 49.1 80.6 2.84

Fine-Tuning 68.1 8.67 46.6 71.3 2.44 49.2 0.40 0.44 0.53 0.95 0.48 69.4 10.41 49.4 81.4 2.87

GPT2-Large Prompt-Tuning 65.8 8.51 44.4 68.2 2.23 43.5 0.37 0.51 0.48 0.94 0.39 64.2 9.71 44.3 75.6 2.62 Input-Tuning 68.2 8.79 45.3 70.2 2.34 46.4 0.38 0.50 0.49 0.94 0.42 68.4 10.01 46.9 78.5 2.75

Fine-Tuning 68.5 8.78 46.0 69.9 2.45 47.0 0.39 0.46 0.51 0.94 0.40 67.0 9.58 45.7 77.6 2.67

Table 3: Results of table-to-text tasks E2E (left), DART (middle) and logic-to-text task ONR (right). The bold scores are the best results among all methods, and the underlined scores represent the best results in prompt-tuning and input-tuning. The ﬁne-tuning results of GPT2-Large on E2E and DART are reported by Li and Liang (2021). On all three tasks, our input-tuning signiﬁcantly outperforms the prompt-tuning. Impressively, it can achieve comparable or even better performance than ﬁne-tuning on these tasks.

Prompt-Tuning Input-Tuning
Fine-Tuning

ToTTo (PARENT on Dev Set) Overall Overlap Non-Overlap

56.0 59.3

52.9

58.0 61.3

54.7

59.8 63.4

56.3

Table 4: PARENT scores on ToTTo, with T5-Large as the backbone. Input-tuning outperforms prompttuning, and the relative gains on Overlap and NonOverlap subsets are on par (2.0 and 1.8, respectively), showing that it maintains a stable generalization ability.

Backbones and Hyperparameters We choose T5-Large (∼770M parameters) and GPT2-Large (774M parameters) as our backbones. Our implementation is based on the HuggingFace Transformer models (Wolf et al., 2020). The training hyperparameters mainly include the learning rate (1e-5∼5e-3), weight decay (1e-1 or 1e-2) and learning rate scheduler (linear or constant). The evaluation hyperparameters mainly include the beam size (1 or 5). All experimental results are produced under the same hyperparameter searching strategy except the cited results. The default prompt length is 100 (the best prompt length indicated by Lester et al. (2021)) and the hidden layer dimensions in the input-adapter are two times of the embedding dimensions. The reparameterization trick used by Li and Liang (2021) is applied. More hyperparameter details are listed in the Appendix C.
5.2 Main Results
Table 3, 4 and 5 show the main results on seven NLG tasks. In general, our input-tuning method notably outperforms prompt-tuning on all these tasks.

Prompt-Tuning Input-Tuning
Fine-Tuning

Machine Translation (BLEU)

Fr-En

De-En

Ro-En

Dev Test Dev Test Dev Test

26.9 29.6 24.4 24.5 26.7 26.5 28.5 31.7 25.9 26.2 31.3 30.4

30.9 34.5 29.0 29.7 34.4 33.1

Table 5: BLEU scores on Machine Translation tasks(Fr/De/Ro-En), with T5-Large as the backbone.

Moreover, on E2E, DART and ONR tasks (shown in Table 3), input-tuning achieves comparable or even better performance than ﬁne-tuning.
Table-to-Text With an additional lightweight input-adapter, the input-tuning notably outperforms prompt-tuning on three table-to-text tasks including E2E, DART and ToTTo (shown in Table 3 and 4). Surprisingly, it can be comparable or even better than ﬁne-tuning on E2E and DART tasks. For the ToTTo task, input-tuning still outperforms prompttuning and slightly lags ﬁne-tuning, and the relative gains on Overlap and Non-Overlap subsets are on par (2.0 and 1.8, respectively), showing that it maintains a stable generalization ability. These results show that a simple input-adapter can bring signiﬁcant beneﬁts.
Logic-to-Text Unlike table-to-text tasks, the inputs of logic-to-text tasks contain more nested and complex logic forms. On ONR task, the input-tuning also apparently outperforms prompttuning, and even surprisingly outperforms ﬁnetuning (shown in Table 3). These results indicate that the input-adapter can handle different input formats and keep a stable performance.

Machine Translation Compared with table-totext and logic-to-text, the translation tasks are much more challenging as they contain larger vocabularies and more training data, making them much harder for lightweight models. Table 5 shows that on all three translation tasks, input-tuning signiﬁcantly closes the gap between prompt-tuning and ﬁne-tuning. It demonstrates that input-tuning consistently outperforms prompt-tuning even on more challenging tasks.
6 Analysis
In this section, we further explore the effectiveness of input-tuning under different settings. Section 6.1 analyzes the performance with different PLM backbones. Section 6.2 explores the different data scale settings. Section 6.3 discusses the different designs of input-tuning. And Section 6.4 shows the performance with different lengths of soft prompts.
6.1 Different Backbones
We evaluate the effectiveness on different backbones from two aspects: backbone architectures (GPT2-Large and T5-Large) and backbone sizes (T5-Small/Base/Large). Note that as the inputadapter only modiﬁes the input embeddings, it can be universally applied to different types of backbones with word embeddings as input.
Backbone Architectures The results in Table 3 show that although the ﬁne-tuning performance of GPT2-Large and T5-Large vary, input-tuning maintains better results than prompt-tuning. It implies the potential of input-tuning to be effective for a variety of model architectures.
Backbone Sizes As shown in Figure 5, the inputtuning stably outperforms prompt-tuning with different sizes of T5 (Small, Base and Large). Furthermore, compared with ﬁne-tuning, the input-tuning beneﬁts more from the increasing of backbone size, indicating that it can be more effective with larger pretrained models.
6.2 Different Data Scales
To explore the impact of data scales, we conduct experiments on different scales of E2E and translation tasks. The experimental results in Figure 6 show that the input-tuning stably performs better than prompt-tuning under different data scales. Compared with ﬁne-tuning, the input-tuning consistently outperforms it in E2E, which is a relative low-resource task compared with translations.

>h

70

Ϯ

68

66

64

62

60
dϱͲ^ŵĂůů ;ϲϬDͿ

dϱͲĂƐĞ ;ϮϮϬDͿ

Input-Tuning

32

dƌĂŶƐůĂƚŝŽŶ;&ƌͲŶͿ

30

28

26

>h

24

22

20

18

16

dϱͲ>ĂƌŐĞ dϱͲ^ŵĂůů ;ϳϳϬDͿ ;ϲϬDͿ

dϱͲĂƐĞ ;ϮϮϬDͿ

dϱͲ>ĂƌŐĞ ;ϳϳϬDͿ

Prompt-Tuning

Fine-Tuning

Figure 5: The performance with different backbone sizes (T5-Small/Base/Large) on E2E task and translation task (Fr-En dev set). The input-tuning stably outperforms prompt-tuning and beneﬁts from the increasing of backbone size.

>h

70

Ϯ

35.0 dƌĂŶƐůĂƚŝŽŶ;ZŽͲŶͿ

65

32.5

60

30.0

27.5

55

Ϭ͘ϭ< Ϯ< ϴ͘ϰ< Ϯϭ< ϰϮ<

ϰϬ<

ĂƚĂ^ĐĂůĞ

ϴϬ< ϮϬϬ< ϰϬϬ< ĂƚĂ^ĐĂůĞ

dƌĂŶƐůĂƚŝŽŶ;&ƌͲŶͿ 30
28

30 dƌĂŶƐůĂƚŝŽŶ;ĞͲŶͿ 28 26

26

24

ϱϬ< ϭϬϬ< ϱϬϬ< ϮD

ϱϬ< ϭϬϬ< ϱϬϬ< ϮD

ĂƚĂ^ĐĂůĞ

ĂƚĂ^ĐĂůĞ

/ŶƉƵƚͲdƵŶŝŶŐ

WƌŽŵƉƚͲdƵŶŝŶŐ

&ŝŶĞͲdƵŶŝŶŐ

>h

Figure 6: Performance under different data scales on E2E and three translation tasks (dev sets). The inputtuning stably performs better than prompt-tuning and prefers low-resource scenarios.

And for translation tasks, the gaps between inputtuning and ﬁne-tuning are closed with the data scales decreasing. These results indicate that the input-tuning prefers low-resource scenarios.
6.3 Different Designs of Input-Tuning
Besides the original design described in Section 4 (the token-level input-adapter with soft prompts), we consider another two designs: the sequencewise input-adapter with soft prompts, and the tokenlevel input-adapter without soft prompts. Figure 7 shows the results on four tasks1.
Sequence-wise Input-Adapter The sequencewise input-adapter is implemented with a selfattention layer. As shown in Figure 7 (green bars), it always performs worse than the original one with a token-wise input-adapter (blue bars), supporting
1The results for more tasks are shown in the Appendix D.

Performance Improvement
>h

6

5 4.2 4.0 4

3

2.4

2

1

0

−1 E2E

1.4 1.1
-0.2 DART

Token-wise Input-Adapter w/o So Prompt

4.9

1.3 0.1

2.1 1.8 0.1

ONR

Fr-En

Sequence-wise Input-Adapter

Figure 7: The improvements of input-tuning (w.r.t. prompt-tuning) with different designs. We take BLEU scores for these four tasks. It shows that a simple tokenwise input-adapter can activate the frozen PLM better, and the soft prompts are indispensable in input-tuning.

70 68 66 64 62 60 58 56 54 52
Ϭ

Ϯ
ϭ ϱ ϮϬ ϭϬϬ ϮϬϬ ƉƌŽŵƉƚůĞŶŐƚŚ /ŶƉƵƚͲdƵŶŝŶŐ

30

dƌĂŶƐůĂƚŝŽŶ;&ƌͲŶͿ

28

26

24

22

20
Ϭ ϭ ϱ ϮϬ ϭϬϬ ϮϬϬ ƉƌŽŵƉƚůĞŶŐƚŚ
WƌŽŵƉƚͲdƵŶŝŶŐ

Figure 8: The performance of input-tuning and prompttuning with different prompt lengths. The 0 prompt length means that there is only an input-adapter (see Section 6.3). The input-tuning performs better than prompt-tuning with all prompt lengths.

our speculation in Section 4 that a stronger adapter may dominate the pretrained model and lead to the performance loss while a simple and lightweight module can activate the frozen PLM better.
Without Soft Prompts The purple bars in Figure 7 shows the results of input-tuning w/o soft prompts. With only a token-level input-adapter, the input-tuning loses much performance and sometimes even lags behind the prompt-tuning, indicating that soft prompts are indispensable in the input-tuning.
6.4 Different Lengths of Soft Prompts
As soft prompts are important for input-tuning, here we further explore the impact of different prompt lengths. We select prompt lengths from 1 to 200 and conduct experiments on E2E and Fr-En tasks.
As shown in Figure 8, the input-tuning is better than prompt-tuning with all prompt lengths. Moreover, it seems that different tasks prefer different input lengths. For the relative simple task E2E, it shows that a short soft prompt is enough while the longer prompt length may hurt the performance. For the more challenging task Fr-En, both inputtuning and prompt-tuning prefers longer inputs.
7 Related Work
Parameter-Efﬁcient Fine-Tuning Despite the strong ability of large-scale pretrained language models (Radford et al., 2018; Devlin et al., 2019; Liu et al., 2019; Raffel et al., 2020a), tuning hundreds of billions of parameters is extremely time- and space-consuming. Therefore, parameterefﬁcient ﬁne-tuning recently attracted more attention. There are two series of related works: invasive methods and non-invasive methods. Invasive methods, which are built on a strong assumption that

the inner structure (e.g., self-attention and feedforward layers) of the PLM can be modiﬁed, includes Preﬁx-Tuning (Li and Liang, 2021), Bitﬁt (Ben Zaken et al., 2021), Child-Tuning (Xu et al., 2021), P-Tuning v2 (Liu et al., 2021b), LoRA (Hu et al., 2021), UniﬁedSKG (Xie et al., 2022) and Adapter-based models (Rebufﬁ et al., 2017; Houlsby et al., 2019; Lin et al., 2020; He et al., 2021; Pfeiffer et al., 2021). Non-invasive methods, which only modify input embeddings and regard the inner structure as a black box, mostly are prompting methods (including our Input-Tuning).
Prompting Prompting means prepending instructions or a few examples to the task input and generating the output from the PLM. Recently, various kinds of hard prompt designing and searching methods were proposed (Jiang et al., 2020; Shin et al., 2020; Schick and Schütze, 2020; Gao et al., 2020). To overcome the shortcomings of hand-crafting, some works optimize continuous soft prompts such as prompt-tuning (Lester et al., 2021), P-Tuning (Liu et al., 2021c), Ppt (Gu et al., 2021) and SPoT (Vu et al., 2021). Recently, Sun et al. (2022) and Diao et al. (2022) explored blackbox tuning, which trains the prompt without gradient back-propagation. Compared with these work, we are the ﬁrst to study the effectiveness of soft prompts from the view of task inputs.
8 Conclusion
In this paper, we revealed that input unfamiliarity is an essential ingredient affecting the performance of prompt-tuning, and proposed input-tuning to alleviate this unfamiliarity with a neural-based inputadapter. We conducted experiments on seven NLG tasks with thorough analysis, showing that inputtuning performs stably better than prompt-tuning.

References
Anja Belz and Ehud Reiter. 2006. Comparing automatic and human evaluation of nlg systems. In 11th conference of the european chapter of the association for computational linguistics.
Elad Ben Zaken, Shauli Ravfogel, and Yoav Goldberg. 2021. Bitﬁt: Simple parameterefﬁcient ﬁne-tuning for transformer-based masked language-models. arXiv e-prints, pages arXiv–2106.
Ondˇrej Bojar, Christian Buck, Christian Federmann, Barry Haddow, Philipp Koehn, Johannes Leveling, Christof Monz, Pavel Pecina, Matt Post, Herve Saint-Amand, et al. 2014. Findings of the 2014 workshop on statistical machine translation. In Proceedings of the ninth workshop on statistical machine translation, pages 12–58.
Ondˇrej Bojar, Rajen Chatterjee, Christian Federmann, Yvette Graham, Barry Haddow, Matthias Huck, Antonio Jimeno Yepes, Philipp Koehn, Varvara Logacheva, Christof Monz, et al. 2016. Findings of the 2016 conference on machine translation. In Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers, pages 131–198.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020a. Language models are few-shot learners. In Advances in Neural Information Processing Systems, volume 33, pages 1877–1901. Curran Associates, Inc.
Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020b. Language models are few-shot learners. arXiv e-prints, pages arXiv–2005.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. Bert: Pre-training

of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171–4186.
Bhuwan Dhingra, Manaal Faruqui, Ankur Parikh, Ming-Wei Chang, Dipanjan Das, and William W Cohen. 2019. Handling divergent reference texts when evaluating table-to-text generation. arXiv preprint arXiv:1906.01081.
Shizhe Diao, Xuechun Li, Yong Lin, Zhichao Huang, and Tong Zhang. 2022. Black-box prompt learning for pre-trained language models. arXiv preprint arXiv:2201.08531.
Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao Ming Zhou, and Hsiao-Wuen Hon. Uniﬁed language model pre-training for natural language understanding and generation.
Tianyu Gao, Adam Fisch, and Danqi Chen. 2020. Making pre-trained language models better fewshot learners. arXiv preprint arXiv:2012.15723.
Yuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang. 2021. Ppt: Pre-trained prompt tuning for few-shot learning. arXiv preprint arXiv:2109.04332.
Ruidan He, Linlin Liu, Hai Ye, Qingyu Tan, Bosheng Ding, Liying Cheng, Jia-Wei Low, Lidong Bing, and Luo Si. 2021. On the effectiveness of adapter-based tuning for pretrained language model adaptation. arXiv preprint arXiv:2106.03164.
Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. 2019. Parameter-efﬁcient transfer learning for nlp. In International Conference on Machine Learning, pages 2790–2799. PMLR.
Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685.
Zhengbao Jiang, Frank F Xu, Jun Araki, and Graham Neubig. 2020. How can we know what

language models know? Transactions of the Association for Computational Linguistics, 8:423– 438.
Philipp Koehn et al. 2005. Europarl: A parallel corpus for statistical machine translation. In MT summit, volume 5, pages 79–86. Citeseer.
Alon Lavie and Abhaya Agarwal. 2007. Meteor: An automatic metric for mt evaluation with high levels of correlation with human judgments. In Proceedings of the second workshop on statistical machine translation, pages 228–231.
Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. The power of scale for parameter-efﬁcient prompt tuning.
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7871–7880, Online. Association for Computational Linguistics.
Xiang Lisa Li and Percy Liang. 2021. Preﬁx-tuning: Optimizing continuous prompts for generation. arXiv preprint arXiv:2101.00190.
Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out, pages 74–81.
Zhaojiang Lin, Andrea Madotto, and Pascale Fung. 2020. Exploring versatile generative language model via parameter-efﬁcient transfer learning. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings, pages 441–459.
Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. 2021a. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. arXiv preprint arXiv:2107.13586.
Xiao Liu, Kaixuan Ji, Yicheng Fu, Zhengxiao Du, Zhilin Yang, and Jie Tang. 2021b. P-tuning v2: Prompt tuning can be comparable to ﬁnetuning universally across scales and tasks. arXiv preprint arXiv:2110.07602.

Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and Jie Tang. 2021c. Gpt understands, too. arXiv preprint arXiv:2103.10385.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692.
Linyong Nan, Dragomir Radev, Rui Zhang, Amrit Rau, Abhinand Sivaprasad, Chiachun Hsieh, Xiangru Tang, Aadit Vyas, Neha Verma, Pranav Krishna, et al. 2020. Dart: Open-domain structured data record to text generation. arXiv preprint arXiv:2007.02871.
Jekaterina Novikova, Ondˇrej Dušek, and Verena Rieser. 2017. The e2e dataset: New challenges for end-to-end generation. In Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue, pages 201–206.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 311–318.
Ankur Parikh, Xuezhi Wang, Sebastian Gehrmann, Manaal Faruqui, Bhuwan Dhingra, Diyi Yang, and Dipanjan Das. 2020. Totto: A controlled table-to-text generation dataset. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1173–1186.
Jonas Pfeiffer, Aishwarya Kamath, Andreas Rücklé, Kyunghyun Cho, and Iryna Gurevych. 2021. Adapterfusion: Non-destructive task composition for transfer learning. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 487–503.
Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving language understanding by generative pre-training.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,

Yanqi Zhou, Wei Li, and Peter J Liu. 2020a. Exploring the limits of transfer learning with a uniﬁed text-to-text transformer. Journal of Machine Learning Research, 21:1–67.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020b. Exploring the limits of transfer learning with a uniﬁed text-to-text transformer. Journal of Machine Learning Research, 21(140):1–67.
Sylvestre-Alvise Rebufﬁ, Hakan Bilen, and Andrea Vedaldi. 2017. Learning multiple visual domains with residual adapters. In Proceedings of the 31st International Conference on Neural Information Processing Systems, pages 506–516.
Timo Schick and Hinrich Schütze. 2020. It’s not just size that matters: Small language models are also few-shot learners. arXiv preprint arXiv:2009.07118.
Thibault Sellam, Dipanjan Das, and Ankur P Parikh. 2020. Bleurt: Learning robust metrics for text generation. arXiv preprint arXiv:2004.04696.
Taylor Shin, Yasaman Razeghi, Robert L Logan IV, Eric Wallace, and Sameer Singh. 2020. Autoprompt: Eliciting knowledge from language models with automatically generated prompts. arXiv preprint arXiv:2010.15980.
Mathew Snover, Bonnie Dorr, Richard Schwartz, John Makhoul, Linnea Micciulla, and Ralph Weischedel. 2005. A study of translation error rate with targeted human annotation. In Proceedings of the 7th Conference of the Association for Machine Translation in the Americas (AMTA 06), pages 223–231.
Tianxiang Sun, Yunfan Shao, Hong Qian, Xuanjing Huang, and Xipeng Qiu. 2022. Black-box tuning for language-model-as-a-service. arXiv preprint arXiv:2201.03514.
Ramakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh. 2015. Cider: Consensus-based image description evaluation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4566–4575.
Tu Vu, Brian Lester, Noah Constant, Rami Al-Rfou, and Daniel Cer. 2021. Spot: Better frozen model

adaptation through soft prompt transfer. arXiv preprint arXiv:2110.07904.
Yushi Wang, Jonathan Berant, and Percy Liang. 2015. Building a semantic parser overnight. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 1332–1342.
Thomas Wolf, Julien Chaumond, Lysandre Debut, Victor Sanh, Clement Delangue, Anthony Moi, Pierric Cistac, Morgan Funtowicz, Joe Davison, Sam Shleifer, et al. 2020. Transformers: Stateof-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 38–45.
Tianbao Xie, Chen Henry Wu, Peng Shi, Ruiqi Zhong, Torsten Scholak, Michihiro Yasunaga, Chien-Sheng Wu, Ming Zhong, Pengcheng Yin, Sida I Wang, et al. 2022. Uniﬁedskg: Unifying and multi-tasking structured knowledge grounding with text-to-text language models. arXiv preprint arXiv:2201.05966.
Runxin Xu, Fuli Luo, Zhiyuan Zhang, Chuanqi Tan, Baobao Chang, Songfang Huang, and Fei Huang. 2021. Raise a child in large language model: Towards effective and generalizable ﬁnetuning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 9514–9528.
Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. 2019. Bertscore: Evaluating text generation with bert. arXiv preprint arXiv:1904.09675.
Wei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Christian M Meyer, and Steffen Eger. 2019. Moverscore: Text generation evaluating with contextualized embeddings and earth mover distance. arXiv preprint arXiv:1909.02622.
This is the Appendix for the paper: “InputTuning: Adapting Unfamiliar Inputs to Frozen Pretrained Models”.

A Preliminary Exploration
Here we show more details in our preliminary exploration.
A.1 Manually Transforming Inputs We take a table-to-text task E2E and logic-to-text task ONR. Details of these tasks are shown in Section 5.1. Here we describe how we transform the inputs of E2E and ONR manually.
E2E+ Replace the left brackets [ with is, remove the right brackets, and add a period at the end of the sentence.
E2E− Translate the attributes into French.
E2E−− Translate both the attributes and the values into French.
ONR+ We use the canonical utterances for every logic forms.
ONR− Convert the function names into Chinese Pinyin.
The examples of E2E inputs variants are listed in Table 1, and here we show the variants of ONR inputs in Table 6.

Variant ONR ONR+ ONR−

Input Example
call listvalue (call getproperty en.person.alice (string birthplace))
city that is birthplace of alice and that is birthplace of alice
diaoyong lieju (diaoyong dedao en.person.alice (zifuchuan birthplace))

Table 6: Variants of ONR inputs.

A.2 More Controllable Experiments
We list the relative performance of prompt-tuning and input familiarity in Table 7.

Variant
ONR ONR+ ONR−

Relative Performance(%)
94.1 97.1 89.3

Familiarity
3.2 11.8 1.1

It shows the similar performances with E2E variants that the ONR+, which with more familiar inputs, achieves better performance than original task ONR while ONR− performs contra. These results support our intuition that alleviating the unfamiliarity by transforming the inputs can help prompttuning perform better.
B Construction of ONR
In this section, we present how we construct the ONR (OverNight-Reverse) task based on the textto-logic benchmark Overnight (Wang et al., 2015).
The original task of Overnight dataset is semantic parsing, which converts natural language inputs into logic forms in 8 different domains. In our experiment, we mix all the domains and swap the input and output (i.e. use logic forms as inputs and natural language sentences as outputs), and merge different NL outputs with the same logic form input as its candidate outputs, in order to compute metrics (such as BLEU) more precisely. We then shufﬂe and divide them as new train (9030), dev (2262) and test (2390) set, using the original proportion as overnight. For the train set, we divide multiple candidates as multiple training data. For the test set, we use identical logic forms who have 5 candidate corresponding NL labels.
C Hyperparameters
In this section, we list the detailed hyperparameters for input-tuning used in our main experiments. The total training steps are 100K steps, and we save the checkpoints and evaluate for every 10k steps. The length penalty for evaluation is 1.0. The other details are listed in Table 8.
D More Results for Different Designs of Input-Tuning
Here we show the results on all seven tasks for different designs of input-tuning, as a supplementary to Section 6.3.
As shown in Figure 9, the sequence-wise inputadapter consistently performs worse the token-wise input-adapter, and without the soft prompts, the performance of input-tuning always drop. These results enhance that a simple token-wise inputadapter can activate the frozen PLM better, and the soft prompts are indispensable in input-tuning.

Table 7: Relative performance (w.r.t. ﬁne-tuning) and input familiarity of different ONR variants.

Task E2E
DART
ONR ToTTo Fr-En De-En Ro-En

Backbone T5-Large GPT2-Large T5-Large GPT2-Large T5-Large GPT2-Large T5-Large T5-Large T5-Large T5-Large

batch size 16 8 8 8 8 8 8 16 16 16

learning rate 5e-4 1e-3 5e-5 5e-4 5e-4 1e-3 5e-4 1e-4 1e-4 1e-4

weight decay 1e-2 1e-1 1e-2 1e-1 1e-2 1e-1 1e-2 1e-2 1e-2 1e-2

learning rate scheduler constant warmup constant warmup constant warmup constant constant constant constant

warmup ratio 0.1 0.1 0.1 -

beam size 1 5 5 1 1 5 1 5 5 5

no_repeat_ngram_size 3 0 3 0 3 0 3 0 0 0

Table 8: Hyperparameters for input-tuning in different tasks.

Performance Improvement

6

5

4.9

4.2 4.0

4

3

2.4

2

1

0

−1

−2

1.4 1.1
-0.2

2.0 1.3
0.1

2.1 1.8 0.1

-0.9 -1.5

−3 E2E

DART

ONR

ToTTo

Fr-En

Token-wise Input Adapter Sequence-wise Input-Adapter

1.7 1.3

3.9 3.7 1.6

-0.8

De-En

Ro-En

w/o So Prompt

Figure 9: The improvements of input-tuning (w.r.t. prompt-tuning) with different designs. We take the PARENT score for ToTTo and BLEU scores for others.

