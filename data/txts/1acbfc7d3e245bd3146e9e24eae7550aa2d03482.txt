arXiv:2003.01652v3 [stat.ML] 11 Jun 2020

Batch Normalization Provably Avoids Rank Collapse for Randomly Initialised Deep Networks
Hadi Daneshmand1∗, Jonas Kohler1∗, Francis Bach2, Thomas Hofmann1, Aurelien Lucchi1
June 15, 2020
Abstract
Randomly initialized neural networks are known to become harder to train with increasing depth, unless architectural enhancements like residual connections and batch normalization are used. We here investigate this phenomenon by revisiting the connection between random initialization in deep networks and spectral instabilities in products of random matrices. Given the rich literature on random matrices, it is not surprising to ﬁnd that the rank of the intermediate representations in unnormalized networks collapses quickly with depth. In this work we highlight the fact that batch normalization is an eﬀective strategy to avoid rank collapse for both linear and ReLU networks. Leveraging tools from Markov chain theory, we derive a meaningful lower rank bound in deep linear networks. Empirically, we also demonstrate that this rank robustness generalizes to ReLU nets.Finally, we conduct an extensive set of experiments on real-world data sets, which conﬁrm that rank stability is indeed a crucial condition for training modern-day deep neural architectures.
1 Introduction and related work
Depth is known to play an important role in the expressive power of neural networks [27]. Yet, increased depth typically leads to a drastic slow down of learning with gradient-based methods, which is commonly attributed to unstable gradient norms in deep networks [14]. One key aspect of the training process concerns the way the layer weights are initialized. When training contemporary neural networks, both practitioners and theoreticians advocate the use of randomly initialized layer weights with i.i.d. entries from a zero mean (Gaussian or uniform) distribution. This initialization strategy is commonly scaled such that the variance of the layer activation stays constant across layers [12, 13]. However, this approach can not avoid spectral instabilities as the depth of the network increases. For example,
[25] observes that for linear neural networks, such initialization lets all but one singular values of the last layers activation collapse towards zero as the depth increases.
Nevertheless, recent advances in neural architectures have allowed the training of very deep neural networks with standard i.i.d. initialization schemes despite the above mentioned shortcomings. Among these, both residual connections and normalization layers have proven particularly eﬀective and are thus in widespread use (see [16, 23, 13] to name just a few). Our goal here is to bridge the explanatory gap between these two observations by studying the eﬀect of architectural enhancements on the spectral properties of randomly initialized neural networks. We also provide evidence for a strong link of the latter with the performance of gradient-based optimization algorithms.
∗Shared ﬁrst authorship, 1ETH Zurich, 2INRIA Paris
1

One particularly interesting architectural component of modern day neural networks is Batch Normalization (Bn) [16]. This simple heuristics that normalizes the pre-activation of hidden units across a mini-batch, has proven tremendously eﬀective when training deep neural networks with gradient-based methods. Yet, despite of its ubiquitous use and strong empirical beneﬁts, the research community has not yet reached a broad consensus, when it comes to a theoretical explanation for its practical success. Recently, several alternatives to the original “internal covariate shift” hypothesis [16] have appeared in the literature: decoupling optimization of direction and length of the parameters [19], auto-tuning of the learning rate for stochastic gradient descent [3], widening the learning rate range [6], alleviating sharpness of the Fisher information matrix [17], and smoothing the optimization landscape [24]. Yet, most of these candidate justiﬁcations are still actively debated within the community. For example, [24] ﬁrst made a strong empirical case against the original internal covariate shift hypothesis. Secondly, they argued that batch normalization simpliﬁes optimization by smoothing the loss landscape. However, their analysis is on a per-layer basis and treats only the largest eigenvalue. Furthermore, even more recent empirical studies again dispute these ﬁndings, by observing the exact opposite behaviour of Bn on a ResNet20 network [33].
1.1 On random initialization and gradient based training
In light of the above discussion, we take a step back – namely to the beginning of training – to ﬁnd an interesting property that is provably present in batch normalized networks and can serve as a solid basis for a more complete theoretical understanding.
The diﬃculty of training randomly initialized, un-normalized deep networks with gradient methods is a long-known fact, that is commonly attributed to the so-called vanishing gradient eﬀect, i.e., a decreasing gradient norm as the networks grow in depth (see, e.g., [26]). A more recent line of research tries to explain this eﬀect by the condition number of the input-output Jacobian (see, e.g., [31, 32, 22, 6]). Here, we study the spectral properties of the above introduced initialization with a particular focus on the rank of the hidden layer activations over a batch of samples. The question at hand is whether or not the network preserves a diverse data representation which is necessary to disentangle the input in the ﬁnal classiﬁcation layer.
As a motivation, consider the results of Fig. 1, which plots accuracy and output rank when training batch-normalized and un-normalized neural networks of growing depth on the FashionMNIST dataset [30]. As can be seen, the rank in the last hidden layer of the vanilla networks collapses with depth and they are essentially unable to learn (in a limited number of epochs) as soon as the number of layers is above 10. The rank collapse indicates that the direction of the output vector has become independent of the actual input. In other words, the randomly initialized network no longer preserves information about the input. Batch-normalized networks, however, preserve a high rank across all network sizes and their training accuracy drops only very mildly as the networks reach depth 32.
The above example shows that both rank and optimization of even moderately-sized, unnormalized networks scale poorly with depth. Batch-normalization, however, stabilizes the rank in this setting and the obvious question is whether this eﬀect is just a slow-down or even simply a numerical phenomenon, or whether it actually generalizes to networks of inﬁnite depth.
In this work we make a strong case for the latter option by showing a remarkable stationarity aspect of Bn. Consider for example the case of passing N samples xi ∈ Rd arranged column-wise in an input matrix X ∈ Rd×N through a very deep network with fully-connected layers. Ideally, from an information propagation perspective, the network should be able to diﬀerentiate between
2Computed using torch.matrix_rank(), which regards singular values below σmax × d × 10−7 as zero. This is consistent with both Matlab and Numpy.
2

Rank of last hidden layer Rank

120 100 80 60 40 20
0 0

SGD w/o BN SGD w/ BN 5 10 15 20 25 30 Number of hidden layers

Final training accuracy

1.0 0.8 0.6 0.4 0.2
0

SGD w/o BN SGD w/ BN 5 10 15 20 25 30 Number of hidden layers

Figure 1: Eﬀect of depth on rank and learning, on the Fashion-MNIST dataset with ReLU multilayer
perceptrons (MLPs) of depth 1-32 and width 128 hidden units. Left: Rank2 after random initialization as in
PyTorch [21, 12]. Right: Training accuracy after training 75 epochs with SGD, batch size 128 and grid-searched
learning rate. Mean and 95% conﬁdence interval of 5 independent runs.

individual samples, regardless of its depth [26]. However, as can be seen in Fig. 2, the hidden representation of X collapses to a rank one matrix in vanilla networks, thus mapping all xi to the same line in Rd. Hence, the hidden layer activations and along with it the individual gradient directions become independent from the input xi as depth goes to inﬁnity. We call this eﬀect “directional” gradient vanishing (see Section 3 for a more thorough explanation).
Interestingly, this eﬀect does not happen in batch-normalized networks, which yield – as we shall prove in Theorem 2 – a stable rank for any depth, thereby preserving a disentangled representation of the input and hence allowing the training of very deep networks. These results substantiate earlier empirical observations made by [6] for random Bn-nets, and also validates the claim that Bn helps with deep information propagation [26].

30 25 20 15 10 5 00

Linear net

w/o BN w/ BN

10000 200D0e0pth (3L0)000 40000 50000

Rank

25 20 15 10 5 00

Relu net w/o BN w/ BN
10000 200D0e0pth (3L0)000 40000 50000

Figure 2: Rank comparison of last hidden activation: Log(rank) of the last hidden layer’s activation over total number of layers (blue for Bn- and orange for vanilla-networks) for Gaussian inputs. Networks are MLPs of width d = 32. (Left) Linear activations, (Right) ReLU activations. Mean and 95% conﬁdence interval of 10 independe√nt runs. While the rank quickly drops in depth for both networks, BN stabilizes the rank above d.

1.2 Contributions
In summary, the work at hand makes the following two key contributions: (i) We theoretically prove that Bn indeed avoids rank collapse for deep linear neural nets
under standard initialization and for any depth. In particular, we show that Bn can be seen as a
3

computationally cheap rank preservation operator, which may not yield hidden matrices with full rank but√still preserves suﬃcient modes of variation in the data to achieve a scaling of the rank with Ω( d), where d is the width of the network. Subsequently, we leverage existing results from random matrix theory [8] to complete the picture with a simple proof of the above observed rank collapse for linear vanilla networks, which interestingly holds regardless of the presence of residual connections (Lemma 3). Finally, we connect the rank to diﬃculties in gradient based training of deep nets by showing that a rank collapse makes the directional component of the gradients independent of the input.
(ii) We empirically show that the rank is indeed a crucial quantity for gradient-based learning. In particular, we show that both the rank and the ﬁnal training accuracy quickly diminish in depth unless Bn layers are incorporated in both simple feed-forward and convolutional neural nets. To take this reasoning beyond mere correlations, we actively intervene with the rank of networks before training and show that (a) one can break the training stability of Bn by initializing in a way that reduces its rank-preserving properties, and (b) a rank-increasing pretraining procedure for vanilla networks can recover their training ability even for large depth. Interestingly, our pre-training method allows vanilla SGD to outperform Bn on very deep MLPs. In all of our experiments, we ﬁnd that SGD updates preserve the order of the initial rank throughout optimization, which underscores the importance of the rank at initialization for the entire convergence behavior.

2 Background and Preliminaries

Network description. We consider a given input X ∈ Rd×N containing N samples in Rd. Let 1k ∈ Rk denote the k-dimensional all one vector and H(γ) denote the hidden representation of X in layer of a Bn-network with residual connections. The following recurrence summarizes
the network mapping

H(+γ)1 = Bn0,1d (H(γ) + γW H(γ)), H0(γ) = X,

(1)

where W ∈ Rd×d and γ regulates the skip connection strength (in the limit, γ = ∞ recovers a network without skip connection)3. Throughout this work, we consider the network weights W
to be initialized as follows.

Deﬁnition 1 (Standard weight initialization). The elements of weight matrices W are i.i.d.
samples from a distribution P that has zero-mean, unit-variance, and its density is symmetric around zero4. We use the notation µ for the probability distribution of the weight matrices.

We deﬁne the BN operator Bnα,β as in the original paper [16], namely

Bnα,β(H) = β ◦ (diag (M (H)))−1/2 H + α1N , M (H) :=

1 HH

,

(2)

N

where ◦ is a row-wise product. Both α ∈ Rd and β ∈ Rd are trainable parameters. Throughout this work we assume the initialization α = 0 and β = 1d, and also omit corrections of the mean activity. As demonstrated empirically in Fig. 5, and theoretically in App. C this simpliﬁcation does not change the performance of Bn in our settings.

3For the sake of simplicity, we here assume that the numbers of hidden units is equal across layers. In App. E we show how our results extend to nets with varying numbers of hidden units.
4Two popular choices for P are the Gaussian distribution N (0, 1) and the uniform distribution U([−1, 1]). The variance can be scaled with the choice of γ to match the prominent initializations from [13] and [12]. Note that the symmetry implies that the law of each element [W ]ij equates the law of −[W ]ij.

4

Rank notions. To circumvent numerical issues involved in rank computations we introduce a soft notion of the rank denoted by rankτ (H) (soft rank). Speciﬁcally, let σ1, . . . , σd be the singular values of H. Then, given a τ > 0, we deﬁne rankτ (H) as

d

rankτ (H) = 1(σi2/N ≥ τ ).

(3)

i=1

Intuit√ively, rankτ (H) indicates the number of singular values whose absolute values are greater than N τ . It is clear that rankτ (H) is less or equal to rank(H) for all matrices H. For analysis purposes, we need an analytic measure of the collinearity of the columns and rows of H. Inspired by the so-called stable rank (see, e.g., [28]), we thus introduce the following quantity

r(H) = Tr(M (H))2/

M (H)

2 F

,

M (H) = HH /N.

(4)

In contrast to the algebraic rank, r(H) is diﬀerentiable with respect to H. Furthermore, the next lemma proves that the above quantity lower-bounds the soft-rank for the hidden representations.
Lemma 1. For an arbitrary matrix H ∈ Rd×d, rank(H) ≥ r(H). For the sequence {H(γ)}∞=1 deﬁned in Eq. (2), rankτ (H(γ)) ≥ (1 − τ )2r(H(γ)) holds for τ ∈ [0, 1].

3 Batch normalization provably prevents rank collapse
Since our empirical observations hold equally for both non-linear and linear networks, we here focus on improving the theoretical understanding in the linear case, which constitutes a growing area of research [25, 18, 5, 2]. First, inspired by [9] and leveraging tools from Markov Chain theory, our m√ain result proves that the rank of linear batch-normalized networks scales with their width as Ω( width). Secondly, we leverage results from random matrix theory [7] to contrast our main result to unnormalized linear networks which we show to provably collapse to rank one, even in the presence of residual connections.

3.1 Main result
In the following we state our main result which proves that batch normalization indeed prevents the rank of all hidden layer activations from collapsing to one. Please see Appendix E for the more formal version of this theorem statement.

Theorem 2. [Informal] Suppose that the rank(X) = d and that the weights W are initialized in a standard i.i.d. zero-mean fashion (see Def. 1). Then, the following limits exist such that

lim 1 L rankτ (H(γ)) ≥ lim (1 − τ )2 L r(H(γ)) = Ω((1 − τ )2√d) (5)

L→∞ L

L→∞ L

=1

=1

holds almost surely for a suﬃciently small γ (independent of ) and any τ ∈ [0, 1), under some additional technical assumptions. Please see Theorem 14 in the Appendix for the formal statement.

Theorem 2 yields a non trivial width-dependency. Namely, by setting for example τ := 1/2,
the result states tha√t the average number of singular values with absolute value greater than N/2 is at least Ω( d) on average. To put this into context: If one were to replace diag(M )−1/2
by the full inverse (M )−1/2 in Eq. (2), then Bn would eﬀectively constitute a classical whitening operation such that all {H(γ)}L=1 would be full rank (equal to d). However, as noted in the original

5

Bn paper [16], whitening is obviously expensive to compute and furthermore prohibitively costly to incorporate in back-propagation. As such, Bn can be seen as a computationally inexpensive approximation of whitening, which does not yield full rank h√idden matrices but still preserves suﬃcient variation in the data to provide a rank scaling as Ω( d). Although the lower-bound in Thm. 2 is established on the average over inﬁnite depth (i.e., L → ∞), Corollary 15 (in App. E) proves that the same bound holds for all rank(H ) and rankτ (H ).

Necessary assumptions. The above result relies on two key assumptions: (i) First, the input X needs to be full rank. (ii) Second, the weights have to be drawn according to the standard initialization scheme. We believe that both assumptions are indeed necessary for Bn to yield a robust rank.
Regarding (i), we consider a high input rank a natural condition since linear neural nets cannot possibly increase the rank when propagating information through their layers. Of course, full rank is easily achieved by an appropriate data pre-processing. Yet, even when the matrix is close to low rank we ﬁnd that Bn is actually able to amplify small variations in the data (see Fig. 3.b).5 Regarding (ii), we derive – based on our theoretical insights – an adversarial initialization strategy that corrupts both the rank robustness and optimization performance of batch-normalized networks, thus suggesting that the success of Bn indeed relies heavily on the standard i.i.d. zero-mean initialization.

Experimental validation. In order to underline the validity of Theorem 2 we run multiple

simulations by feeding Gaussian data of dimensionality d = N into networks of growing size

and with diﬀerent residual strengths. For each network, we compute the mean and standard

dev√iation of the soft rank rankτ with τ = 0.5. As depicted in Fig. 3, the curves clearly indicate a

Ω( d) dependency for limL→∞

L =1

rankτ

(H

)/L,

just

as

predicted

in

the

Theorem.

Although

the established guarantee requires the weight on the parametric branch (i.e., γ) to be small, the

results of Fig. 3 indicate that the established lower bound holds for a much wider range including

the case where no residual connections are used at all (γ = ∞).

8

= 0.01

7

=1

6

=

5

dd

4

First ten eigenvalues of M
101 10 1 10 3 10 5 10 7

log2(rank)

3

10 9

1

3

5

7

9

2

10 11

2

4

6

8

10

3log2(num4 ber of h5idden un6its per la7yer (d))8 a)

0

20 hid40den layer6(0 ) 80

100

b)

Figure 3: a) Result of Theorem 2 for diﬀerent values of γ, where γ = ∞ depicts networks without skip connections.
Each point is the average rank1/2 over depth (L = 106) of nets of width d ∈ {8, 16, .., 256} an on x-axis. b) Top 10 singular values of H(γ) for increasing values of given nearly collinear inputs. As can be seen, Bn quickly
ampliﬁes smaller variations in the data while reducing the largest one.

5Intuitively this means that even if two data points are very close to each other in the input space, their hidden presentation can still be disentangled in batch-normalized networks (see Appendix E for more details)

6

3.2 Comparison with unnormalized networks
In order to stress the importance of the above result, we now compare the predicted rank of H with the rank of unnormalized linear networks, which essentially constitute a linear mapping in the form of a product of random matrices. The spectral distribution of products of random matrices with i.i.d. standard Gaussian elements has been studied extensively [6, 11, 20]. Interestingly, one can show that the gap between the top and the second largest singular value increases with the number of products (i.e., ) at an exponential rate6 [11, 20]. Hence, the matrix converges to a rank one matrix after normalizing by the norm. In the following, we extend this result to products of random matrices with a residual branch that is obtained by adding the identity matrices. Particularly, we consider the hidden states H of the following linear residual network:

H = B X, B := (I + γWk).

(6)

k=1

Since the norm of H is not necessarily bounded, we normalize as H = B X/ B . The next lemma characterizes the limit behaviour of {H }.

Lemma 3. Suppose that γ ∈ (0, 1) and assume the weights W to be initialized as in Def. 1 with element-wise distribution P. Then we have for linear networks, which follow recursion (6), that:

a. If P is standard Gaussian, then the sequence {H } converges to a rank one matrix. √√
b. If P is uniform[− 3, 3], then there exists a monotonically increasing sequence of integers 1 < 2, . . . such that the sequence {H k } converges to a rank one matrix.
This results stands in striking contrast to the result of Theorem 2 established for batchnormalized networks.7 Interestingly, even residual skip connections cannot avoid rank collapse for very deep neural networks, unless one is willing to incorporate a depth dependent down-scaling of the parametric branch as for example done in [1], who set γ = O( L1 ) . Remarkably, Theorem 2 shows that Bn layers provably avoid rank collapse without requiring the networks to become closer and closer to identity.

Implications of rank collapse on gradient based learning. In order to explain the severe consequence of rank collapse on optimization performance reported in Fig. 1, we study the eﬀect of rank one hidden-layer representations on the gradient of the training loss for distinct input samples. Let Li denote the training loss for datapoint i on a vanilla network as in Eq. (6). Furthermore, let the ﬁnal classiﬁcation layer be parametrized by WL+1 ∈ Rdout×d. Then, given that the hidden presentation at the last hidden layer L is rank one, the normalized gradients of the loss with respect to weights of individual neurons k ∈ 1, ..., dout in the classiﬁcation layer (denoted by ∇WL+1,k Li, where ∇WL+1,k Li = 1) are collinear for any two datapoints i and j, i.e. ∇WL+1,k Li = ∓∇WL+1,k Lj. A formal statement is presented in Prop. 19 in the Appendix alongside empirical validations on a VGG19 network (Fig. 10). This result implies that the commonly accepted vanishing gradient norm hypothesis is not descriptive enough since SGD does not take small steps into the right direction, but into a random direction that is independent from the input. In other words, deep neural networks are prone to directional gradient vanishing after initialization, which is caused by the collapse of the last hidden layer activations to a very small subspace (one line in Rd in the extreme case of rank one activations).
6The growth-rate of the i-th singular value is determined by the i-th Lyapunov exponent of the product of random matrices. We refer the reader to [11] for more details on Lyapunov exponents.
7According to the observations in Fig. 2, the result of part b holds for the usual sequence of indices { k = k}, which indicates that {Hk} converges to a rank one matrix even in the case of uniform initialization.

7

4 The important role of the rank
The preceding sections highlight that the rank of the hidden representations is a key diﬀerence between random vanilla and Bn networks. We now provide three experimental ﬁndings that substantiate the particular importance of the rank at the beginning of training: First, we ﬁnd that an unsupervised, rank-increasing pre-training allows SGD on vanilla networks to outperform Bn networks. Second, we show that the performance of Bn-networks is closely tied to a high rank at initialization. Third, we report that SGD updates preserve the initial rank magnitude throughout the optimization process.

Outperforming BN using a pre-training step. As discussed above, batch normalization layers are very eﬀective at avoiding rank collapse. Yet, this is of course not the only way to preserve rank. Based upon our theoretical insights, we leverage the lower bound established in Eq. (4) to design a pre-training step that not only avoids rank collapse but also accelerates the convergence of SGD. Our proposed procedure is both simple and computationally cheap. Speciﬁcally, we maximize the lower-bound r(H ) (in Eq. (4)) on the rank of the hidden presentation H in each layer . Since this function is diﬀerentiable with respect to its input, it can be optimized suﬃciently by just a few steps of (stochastic) gradient ascent (see Section G in the Appendix for more details).

Training loss Training loss Training loss

2.5 2.0 1.5 1.0 0.5
0

10 hidden layers
pre-trained SGD BatchNorm SGD
20 40Epochs60 80 100

2.5 2.0 1.5 1.0 0.5
0

30 hidden layers
pre-trained SGD BatchNorm SGD
20 40Epochs60 80 100

2.5

50 hidden layers

2.0

1.5

1.0

pre-trained SGD

0.5

BatchNorm SGD

0 20 40Epochs60 80 100

Figure 4: Pre-training versus Bn: Loss over epochs on CIFAR-10 for MLPs of increasing depth with 128
hidden units and ReLU activation. Trained with SGD (batchsize 64) and grid-searched stepsize. See Fig. 11 for the corresponding test loss and accuracy as well as Fig. 12 for FashionMNIST results.

Fig. 4 compares the convergence rate of SGD on pre-trained vanilla networks and Bn-networks. As can be seen, the slow down in depth is much less severe for the pre-trained networks. This improvement is, also, reﬂected both in terms of training accuracy and test loss (see Fig. 11 in Appendix). Interestingly, the pre-training is not only faster than Bn on deep networks, but it is also straight-forward to use in settings where the application of Bn is rather cumbersome such as for very small batch sizes or on unseen data [15, 29].

Breaking batch normalization. Some scholars hypothesize that the eﬀectiveness of Bn stems from a global landscape smoothing [24] or a certain learning rate tuning [3], that are thought to be induced by the normalization. Under these hypotheses, one would expect that SGD converges fast on Bn-nets regardless of the initialization. Yet, we here show that the way that networks are initialized does play a crucial role for the subsequent optimization performance of Bn-nets.

8

1.0 0.8 0.6 0.4 0.2
0

VGG19: Training Accuracy W U[ a, a] W U[0, 2a] W U[ a, a] no centering
1 2 3 Epo4chs 5 6 7 8

VGG19: Rank in last hidden layer

30

25

20

W U[ a, a]

15

W U[0, 2a] W U[ a, a] no centering

10

5

0 0 1 2 3 Epo4chs 5 6 7 8

Figure 5: Breaking Batchnorm: CIFAR-10 on VGG19 with standard PyTorch initialization as well as a
uniform initialization of same variance. (Left) training accuracy, (Right) Rank of last hidden layer computed using torch.matrix_rank(). Plot also shows results for standard initialization and Bn without mean deduction. Avg. and 95% CI of 5 independent runs. (See Fig. 13 in Appendix for similar results on ResNet-50).

Particularly, we train two MLPs with batchnorm, but cha√nge th√e initialization for the second net from√the standard PyTorch way Wl,i,j ∼ uniform −1/ dl, 1/ dl [21, 12] to Wl,i,j ∼ uniform 0, +2/ dl , where dl is the layer size. As can be seen to the right, this small change reduces the rank preserving quality of BN signiﬁcantly, which is reﬂected in much slower learning behaviour. Even sophisticated modern day architectures such as VGG and ResNet networks are unable to ﬁt the CIFAR-10 dataset after changing the initialization in this way (see Fig. 5).

Rank through the optimization process. The theoretical result of Theorem 2 considers the rank at random initialization. To conclude, we perform two further experiments which conﬁrm that the initial rank strongly inﬂuences the speed of SGD throughout the entire optimization process. In this regard, Fig. 6 reports that SGD preserves the initial magnitude of the rank to a large extent, regardless of the speciﬁc network type. This is particularly obvious when comparing the two Bn initializations. A further noteworthy aspect is the clear correlation between the level of pre-training and optimization performance on vanilla nets. Interestingly, this result does again not only hold on simple MLPs but also generalizes to modern day networks such as the VGG-19 (see Fig. 5) and ResNet50 architecture (see Appendix I).

Training loss Lower bound on rank

2.5 2.0 1.5 1.0 0.5
0.0 2.5 5.0 Ep7o.c5hs 10.0 12.5 15.0

101 100
0.0 2.5 5.0 Ep7o.c5hs 10.0 12.5 15.0

Figure 6: Pretraining: Fashion-MNIST on MLPs of depth 32 and width 128. (Left) Training accuracy, (Right)
Lower bound on rank. Blue line is a ReLU network with standard initialization. Other solid lines are pre-trained layer-wise with 25 (orange) and 75 (green) iterations to increase the rank. Dashed lines are batchnorm networks with standard and asymmetric initialization. Average and 95% conﬁdence interval of 5 independent runs.

9

References
[1] Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-parameterization. arXiv preprint arXiv:1811.03962, 2018.
[2] Sanjeev Arora, Nadav Cohen, Noah Golowich, and Wei Hu. A convergence analysis of gradient descent for deep linear neural networks. arXiv preprint arXiv:1810.02281, 2018.
[3] Sanjeev Arora, Zhiyuan Li, and Kaifeng Lyu. Theoretical analysis of auto rate-tuning by batch normalization. arXiv preprint arXiv:1812.03981, 2018.
[4] Boaz Barak and David Steurer. Proofs, beliefs, and algorithms through the lens of sum-ofsquares. Course notes: http://www. sumofsquares. org/public/index. html, 2016.
[5] Peter L. Bartlett, David P. Helmbold, and Philip M. Long. Gradient descent with identity initialization eﬃciently learns positive-deﬁnite linear transformations by deep residual networks. Neural computation, 31(3):477–502, 2019.
[6] Nils Bjorck, Carla P. Gomes, Bart Selman, and Kilian Q. Weinberger. Understanding batch normalization, 2018.
[7] Stéphane Boucheron, Gábor Lugosi, and Pascal Massart. Concentration inequalities: A nonasymptotic theory of independence. Oxford university press, 2013.
[8] Philippe Bougerol. Products of Random Matrices with Applications to Schrödinger Operators, volume 8. Springer Science & Business Media, 2012.
[9] Aymeric Dieuleveut, Alain Durmus, and Francis Bach. Bridging the gap between constant step size stochastic gradient descent and Markov chains. arXiv preprint arXiv:1707.06386, 2017.
[10] Randal Douc, Eric Moulines, Pierre Priouret, and Philippe Soulier. Markov Chains. Springer, 2018.
[11] Peter J. Forrester. Lyapunov exponents for products of complex Gaussian random matrices. Journal of Statistical Physics, 151(5):796–808, 2013.
[12] Xavier Glorot and Yoshua Bengio. Understanding the diﬃculty of training deep feedforward neural networks. In Proceedings of the thirteenth international conference on artiﬁcial intelligence and statistics, pages 249–256, 2010.
[13] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectiﬁers: Surpassing human-level performance on imagenet classiﬁcation. In Proceedings of the IEEE international conference on computer vision, pages 1026–1034, 2015.
[14] Sepp Hochreiter. The vanishing gradient problem during learning recurrent neural nets and problem solutions. International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems, 6(02):107–116, 1998.
[15] Sergey Ioﬀe. Batch renormalization: Towards reducing minibatch dependence in batchnormalized models. In Advances in neural information processing systems, pages 1945–1953, 2017.
[16] Sergey Ioﬀe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.
10

[17] Ryo Karakida, Shotaro Akaho, and Shun-ichi Amari. The normalization method for alleviating pathological sharpness in wide neural networks. In Advances in Neural Information Processing Systems, pages 6403–6413, 2019.
[18] Kenji Kawaguchi. Deep learning without poor local minima. In Advances in neural information processing systems, pages 586–594, 2016.
[19] Jonas Kohler, Hadi Daneshmand, Aurelien Lucchi, Ming Zhou, Klaus Neymeyr, and Thomas Hofmann. Exponential convergence rates for batch normalization: The power of lengthdirection decoupling in non-convex optimization. arXiv preprint arXiv:1805.10694, 2018.
[20] Dang-Zheng Liu, Dong Wang, and Lun Zhang. Bulk and soft-edge universality for singular values of products of ginibre random matrices. In Annales de l’Institut Henri Poincaré, Probabilités et Statistiques, volume 52, pages 1734–1762. Institut Henri Poincaré, 2016.
[21] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. In Advances in Neural Information Processing Systems, pages 8024–8035, 2019.
[22] Jeﬀrey Pennington, Samuel S Schoenholz, and Surya Ganguli. The emergence of spectral universality in deep networks. arXiv preprint arXiv:1802.09979, 2018.
[23] Tim Salimans and Diederik P. Kingma. Weight normalization: A simple reparameterization to accelerate training of deep neural networks. In Advances in Neural Information Processing Systems, pages 901–909, 2016.
[24] Shibani Santurkar, Dimitris Tsipras, Andrew Ilyas, and Aleksander Madry. How does batch normalization help optimization?(no, it is not about internal covariate shift). arXiv preprint arXiv:1805.11604, 2018.
[25] Andrew M. Saxe, James L. McClelland, and Surya Ganguli. Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. arXiv preprint arXiv:1312.6120, 2013.
[26] Samuel S. Schoenholz, Justin Gilmer, Surya Ganguli, and Jascha Sohl-Dickstein. Deep information propagation. arXiv preprint arXiv:1611.01232, 2016.
[27] Matus Telgarsky. Beneﬁts of depth in neural networks. arXiv preprint arXiv:1602.04485, 2016.
[28] Joel A. Tropp. An introduction to matrix concentration inequalities. arXiv preprint arXiv:1501.01571, 2015.
[29] Yuxin Wu and Kaiming He. Group normalization. In Proceedings of the European Conference on Computer Vision (ECCV), pages 3–19, 2018.
[30] Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms. arXiv preprint arXiv:1708.07747, 2017.
[31] Ge Yang and Samuel Schoenholz. Mean ﬁeld residual networks: On the edge of chaos. In Advances in neural information processing systems, pages 7103–7114, 2017.
[32] Greg Yang, Jeﬀrey Pennington, Vinay Rao, Jascha Sohl-Dickstein, and Samuel S. Schoenholz. A mean ﬁeld theory of batch normalization. arXiv preprint arXiv:1902.08129, 2019.
11

[33] Zhewei Yao, Amir Gholami, Kurt Keutzer, and Michael Mahoney. PyHessian: Neural networks through the lens of the Hessian. arXiv preprint arXiv:1912.07145, 2019.
12

Appendix

A Preliminaries
Recall that H(γ) denotes the hidden representations in layer . These matrices make a Markov chain that obeys the recurrence of Eq. (1), which we restate here

H(+γ)1 = Bn(H(γ) + γW H(γ)), H0γ = X,

(7)

where we use the compact notation Bn for Bn0,1d. Let M (γ) be second moment matrix of the
hidden representations H(γ), i.e. M (γ) := H(γ) H(γ) /N . Batch normalization ensures that √
the rows of H have the same norm N for > 0 –where N is the size of mini-batch. Let H be space of d × d-matrices that obey this propery. This property enforces two key characteristics on M (γ):

(p.1) its diagonal elements are one

(8)

(p.2) the absolute value of its oﬀ-diagonal elements is less than one

(9)

Property (p.1) directly yields that the trace of M (γ) (and hence the sum of its eigenvalues) is equal to d. We will repeatedly use these properties in our analysis.
Furthermore, the sequence {H(γ)}∞=1 constitute a Markov chain. Under mild assumptions, this chain admits an invariant distribution that is deﬁned bellow[10].
Deﬁnition 2. Distribution ν is an invariant distribution of the hidden representations {H(γ)}∞=1 if it obeys

Bn(H + γW H)µ(dW )ν(dH) = Bn(H)ν(dH)

(10)

where µ denotes the probability measure of random weights.
Later, we will see that the above invariance property allows us to determine the commutative behaviour of the sequence of hidden presentations.

B Lower bounds on the (soft) rank

Recall

that

we

introduced

the

ratio

r(H) = Tr(M (H))2/

M (H)

2 F

in

Eq.

(4)

as

a

lower

bound

on both the rank(H) as well as the soft rank rankτ (H) (stated in Lemma 1). This section

establishes these lower bounds.

Proof of Lemma 1 (part 1). We ﬁrst prove that rank(H) ≥ r(H). Let M := M (H) = HH /N .

Since the eigenvalues of H are obtained by a constant scaling factor of the squared singular values

of H, these two matrices have the same rank. We now establish a lower bound on rank(M ). Let

λ ∈ Rd contain the eigenvalues of matrix M , hence

λ 1 = Tr(M ) and

λ

2 2

=

M

2 F

.

Given λ,

we deﬁne the vector w ∈ Rd as

wi = 1/ λ 0 : λi = 0 (11)

0

: λi = 0.

13

To proof the assertion, we make use if a straightforward application of Cauchy-Schwartz

| λ, w | ≤ λ 2 w 2

(12)

=⇒

λ 1/ λ 0 ≤

λ

2/

λ

1/2 0

(13)

λ1

1/2

=⇒

≤ λ2

λ0

.

(14)

Replacing λ 2 = M F and λ 1 = Tr(M ) into the above equation concludes the result. Note that the above proof technique has been used in the planted sparse vector problem [4].

Proof of Lemma 1 (part 2). Now, we prove that rankτ (H(γ)) ≥ (1 − τ )2r(H(γ)). Let λ ∈ Rd+ be a vector containing the eigenvalues of the matrix M (γ) = M (H(γ)). Let σ ∈ Rd+ contain the singular values of H. Then, one can readily check that σi2/N = λi. Furthermore, λ 1 = d due
to (p.1) in Eq. (8). Furthermore, we have by deﬁnition that

d

d

rankτ (H(γ)) = hτ (λ) := 1(σi2/N ≥ τ ) = 1(λi ≥ τ ).

(15)

i=1

i=1

Let us now deﬁne a vector w ∈ Rd with entries

wi = 1/hτ (λ) : λi ≥ τ (16)

0

: otherwise.

Then, we use Cauchy-Schwartz to get

| λ, w | ≤ λ 2 w 2.

(17)

It is easy to check that w 2 = hτ (λ)−1/2 holds. Furthermore,

d

hτ (λ)| w, λ | =

|λi|

(18)

|λi|≥τ

≥ λ 1 − dτ

(19)

≥ (1 − τ ) λ 1,

(20)

where we used the fact that λ 1 = d in the last inequality. Replacing this result into the bound of Eq. (17) yields

rankτ (H(γ)) = hτ (λ) ≥ (1 − τ )2

λ

21/

λ

2 2

=

(1

−

τ )2r(H(γ)),

(21)

which conludes the proof.

C Initialization consequences
The particular weight initialization scheme consider through out this work (recall Def. 1), imposes an interesting structure in the invariant distribution of the sequence of hidden presentations (deﬁned in Def. 2).
Lemma 4. Suppose that the chain {H(γ)}∞=1 (deﬁned in Eq. 7) admits a unique invariant distribution νγ and H is drawn from νγ; then, the law of Hi: equates the law of −Hi: where Hi: denotes the ith row of matrix H.

14

Proof. Let S be a sign ﬁlliping matrix: it is diagonal and its diagonal elements are in {+1, −1}. Then SW =d W holds for a random matrix W whose elements are drawn i.i.d. from a symmetric distribution. Let H be drawn from the invariant distribution of the chain denoted by νγ; Leveraging the invariance property, we get

d

d

−1/2

H = H+ = diag(H1/2H1/2/N )

H1/2, H1/2 := H + γSW SH

(22)

By multiplying both sides with S, we get

SH =d SH+ =d diag H1/2H1/2/N

−1/2
H1/2,

H1/2 := SH + γW SH

(23)

Note that we use the fact that diagonal matrices commute in the above derivation. According to the deﬁnition, S2 = I holds. Considering this fact, we get

diag H1/2H1/2 = diag (H + γSW SH) (H + γSW SH)

(24)

= diag (SSH + γSW SH) (SSH + γSW SH)

(25)

= diag S (SH + γW SH) (SH + γW SH) S

(26)

= diag (SH + γW SH) (SH + γW SH)

(27)

= H1/2H1/2

(28)

Replacing the above result into Eq. (29) yields

SH =d SH+ =d diag−1/2 H1/2H1/2/N H1/2, H1/2 := SH + γW SH.

(29)

Hence the law of SH is invariant too. Since the invariant distribution is assumed to be unique, SH =d H holds and thus Hi: =d −Hi:.

Comment on Bn-centering Let νγ be the unique invariant distribution associated with Markov chain {H(γ)}. A straightforward implication of last Lemma is E [Hi] = 0 for H ∼ νγ, hence the rows of H(γ) are mean-zero, hence their average is close to zero 8 and the mean-zero operation in Bn is redundant. Although this theoretical argument is established for linear networks, we empirically observed that Bn without centering also works well on modern neural architectures. For example, Fig. 7 shows that the centering does not aﬀect the performance of Bn on a VGG net when training CIFAR-10.
D Main Theorem: warm-up analysis
As a warm-up analysis, the next lemma proves that rank(H(γ)) ≥ 2 holds. Later, we will prove a stronger result. Yet, this initial results provides valuable insights into our proof technique. Furthermore, we will use the following result in the next steps.
8When d is suﬃciently large and assuming that coordinates in one row are weakly dependent, the central limit theorem implies that the empirical average of the rows converges to zero.

15

VGG19: Training Accuracy
1.0

0.8

0.6

0.4

0.2 0

BN BN w/o centering 5000 10000 Iter1a50t0io0n 20000 25000 30000

Training Accuracy

3.50 3.25 3.00 2.75 2.50 2.25 2.00 1.75
0

VGG19: Lower bound on rank
BN BN w/o centering 5000 10000 Iter1a50t0io0n 20000 25000 30000
r(HL)

Figure 7: Centering for Bn. The experiment is conducted on a VGG network. The blue line indicates the original Bn network and the orange line is Bn without mean adaption. The vertical axis in the left plot is training accuracy. In the right plot it is r(HL), where HL is the data representation in the last hidden layer L. The horizontal axis indicates the number of iterations.

Lemma 5. Suppose that each element of the weight matrices is independently drawn from distribution P that is zero-mean, unit-variance, and its support lies in interval [−B, B]. If the Markov chain {H } ≥1 admits a unique invariant distribution, then

rank(H(γ)) ≥ 2

(30)

holds almost surely for all integers and γ ≤ 1/(8d).

Proof. Let the weights {W } be drawn from the distribution µ, deﬁned in Def. 1. Such a distribution obeys an important property: element-wise symmetricity. That is, [W ]ij is distributed as −[W ]ij. Such an initialization enforces an interesting structural property for the invariant distribution νγ that is stated in Lemma 4. It is easy to check that this implies

E [M (H(γ))]ij = −E [M (H(γ))]ij = 0,

(31)

for any i = j. Recall, M (H) = HH /N . The above property enforces [M (H)]2ij to be small and hence M (γ) 2F is small as well. Now, as rank(H(γ)) is proportional to 1/ M (γ) 2F (compare Eq. (4)), it must consequently stay large. The rest of the proof is based on this intuition. Given
the uniqueness of the invariant distribution, we can invoke Birkhoﬀ’s Ergodic Theorem for
Markov Chains (Theorem 5.2.1 and 5.2.6 [10]) which yields

1 L (γ)

lim L→∞ L

[M ]ij = EH∼νγ [[M (H)]ij] .

=1

(32)

This allows us to conclude the proof by a simple contradiction. Assume that rank(Hk(γ)) is indeed one. Then, as established in the following Lemma, in the limit all entries of M (H(γ)) are constant and either −1 or 1.
Lemma 6. Suppose the assumptions of Lemma 5 hold. If rank(Hk(γ)) = 1 for an integer k, then M (H(γ)) = M (Hk(γ)) holds for all > k. Furthermore, all elements of all matrices {M (H(γ))} ≥k have absolute value one, hence

1L

(γ)

lim

[M (H )]ij ∈ {1, −1}

(33)

L→∞ L

=1

holds.

16

As a result, leveraging the ergodicity established in (61), we get that then

EH∼νγ [[M (H)]ij] ∈ {+1, −1}

(34)

must also hold. However, this contradicts the consequence of the symmetricity (Eq. (31)) which
states that for any j = i we have EH∼νγ [[M (H)]ij] = −EH∼νγ [[M (H)]ij] = 0. Thus, the rank one assumption cannot hold, which proves the assertion.

To complete the proof of the last theorem, we prove Lemma 6.

Proof of Lemma 6. For the sake of simplicity, we omit all superscripts (γ) throughout the proof.
Suppose that rank(Hk) = 1, then rank(H ) = 1 for all ≥ k as the sequence {rank(H )} is non-increasing 9. Invoking the established rank bound from Lemma 1, we get

Tr(M )2

r(H ) = M 2 ≤ rank(H ) = 1.

(35)

F

The above inequality together with properties (p.1) and (p.2) (presented in Eqs 8 and 9) yield

Tr(M ) = d. Replacing this into the above equation gives that

M

2 F

≥

d2

must

hold

for

the

rank of H to be one. Yet, recalling property (p.2), this can only be the case if [M ]ij ∈ {+1, −1}

for all i, j. Replacing the deﬁnition M (H) = HH /N into updates of hidden presentation in

Eq. 1 obtains

M +1 = diag

M +1 2

−1/2

M +1 2

diag

M +1 2

−1/2

(36)

where

M + 1 = M + ∆M , ∆M := γW M + γM W + γ2W M W

(37)

2

We now prove that the sign of [M ]ij and [M +1]ij are the same for [M ]ij ∈ {+1, −1}. The above
update formula implies that the sign of [M +1]ij equates that of [M +1/2]ij. Furthermore, it is easy to check that |[∆M ]ij| ≤ 4γB. For γ ≤ 1/(8Bd), this bound yields |[∆M ]ij| ≤ 12 . Therefore, the sign of [M +1/2]ij is equal to the one of [M ]ij. Since furthermore [M +1]ij ∈ {1, −1} holds, we conclude that all elements of M remain constant for all ≥ k, which yields the limit stated
in Eq. 33 .

E Main theorem: Proof
√ In this section, we prove that Bn yields an Ω( d)-rank for hidden representation.
Proof sketch for Thm. 2. The proof is based on an application of ergodic theory (as detailed for example in Section 5 of [10]). In fact, the chain of hidden representations, denoted by H(γ) (1), constitutes a Markov chain in a compact space. This chain admits at least one invariant distribution ν for which the following holds

g(Bn0,1d(H + γW H))µ(dW )ν(dH) = g(H)ν(dH),

(38)

9Recall that the updates in Eq. (1) is obtained by matrix multiplications, hence it does not increase the rank.

17

for every bounded Borel function g : Rd×d → Rd. The above invariance property provides an interesting characterization of the invariant measure ν. Particularly, we show in Lemma 13 that

√

r(H)ν(dH) = Ω( d)

(39)

holds, where r(H) is the established lower-bound on the rank (see Lemma 1). Under weak assumptions, the chain obey Birkhoﬀ’s Ergodicity, which yields that the average behaviour of the hidden representations is determined by the invariant measure ν:

1

(γ)

(39) √

lim

r(H ) = r(H)ν(dH) = Ω( d).

(40)

L→∞ L

i=

Finally, the established lower bound in Lemma 1 allows us to directly extend this result to a lower bound on the soft rank itself.

Characterizing the change in Frobenius norm Recall the established lower bound on the rank denoted by r(H), for which

Tr(M )2

d2

r(H ) = M 2 = M 2

(41)

F

F

holds for all H

deﬁned in Eq. 1.10 Therefore,

M

2 F

directly

inﬂuences

rankτ (H

)

(and

also

rank(H )) according to Lemma 1. Here, we characterize the change in

M (H)

2 F

after

applying

one step of the recurrence in Eq. 7 to H, i.e. passing it trough one hidden layer. This yields

H+ = (diag(M (Hγ(W )))−1/2 Hγ(W ), Hγ(W ) = (I + γW )H.

(42)

Let M = M (H) and M+ = M (H+) for simplicity. The next lemma estimates the expectation (taken over the randomness of W ) of the diﬀerence between the Frobenius norms of M+ and M .

Lemma 7. If W ∼ µ (deﬁned in Def. 1), then

EW

M+

2 F

−

M

2 F

/(γ2) = 2d2 − 2 M

2 F

−

8Tr(M 3)

+

8Tr(diag(M 2)2) +O(γ)

(43)

δF (M )

holds as long as the support of distribution P (in Def. 1) lies in a ﬁnite interval [−B, B].

The proof of the above lemma is based on a Taylor expansion of the Bn non-linear operator. We postpone the detailed proof to the end of this section. While the above equation seems complicated at ﬁrst glance, it provides some interesting insights. Interlude: Intuition behind Lemma 7. In order to gain more understanding of the implications of the result derived in Lemma 7, we make the simplifying assumption that all the rows of matrix M have the same norm. We emphasize that this assumption is purely for intuition purposes and is not necessary for the proof of our main theorem. Under such an assumption, the next proposition shows that the change in the Frobenius norm directly relates to the spectral properties of matrix M .

Proposition 8. Suppose that all the rows of matrix M have the same norm. Let λ ∈ Rd contain the eigenvalues of matrix M . Then,

Tr(M 3) = λ 33, Tr(diag(M 2))2 = λ 4/d,

M

2 F

=

λ

2 2

(44)

holds and hence

δF (M ) = δF (λ) := 2d2 − 2

λ

2 2

−

8

λ

3 3

+

8

λ

4/d.

10Recall Tr(M ) = d holds due to property (p.2) in Eq. 9

(45)

18

We postpone the proof to the end of this section. This proposition re-expresses the polynomial of Lemma 7 in terms of the eigenspectrum of M .
Based on the above proposition, we can make sense of interesting empirical observation reported in Figure 3.b. This ﬁgure plots the evolution of the eigenvalues of M (H(γ)) after starting from a matrix M (H0) whose leading eigenvalue is large and all other eigenvalues are very small. We observe that a certain fraction of the small eigenvalues of M (H(γ)) grow quickly with , while the leading eigenvalue is decreases in magnitude. In the next example, we show that the result of the last proposition actually predicts this observation.

Example 9. Suppose that M is a matrix whose rows have the same norm. Let λ1 ≥ λ2, . . . , λd be

the eigenvalues associated with the matrix M such that λd = λd−1 = λ2 = γ2 and λ1 = d−γ2(d−1).

In this setting, Prop. 8 implies that EW

M+

2 F

<

M

2 F

− γ4d2

for

a

suﬃciently

small

γ.

This

change has two consequences in expectation:(i.) the leading eigenvalue of M+ is O(−γ4d) smaller

than the leading eigenvalue of M , and (ii.) some small eigenvalues of M+ are greater than those

of M (see Fig. 3.b).

We provide a more detailed justiﬁcation for the above statement at the end of this section.

This example illustrates that the change in Frobenius norm (characterized in Lemma 7) can

predict the change in the eigenvalues of M (H(γ)) (singular values of H(γ)) and hence the desired

rank. Inspired by this, we base the proof of Theorem 2 on leveraging the invariance property of

the unique invariant distribution with respect to Frobenius norm – i.e. setting g(H) =

M (H)

2 F

in Def. 2.

An observation: regularity of the invariant distribution We now return to the result de-

rived in Lemma 7 that characterizes the change in Frobenius norm of M (H) after the recurrence

of Eq. (7). We show how such a result can be used to leverage the invariance property with

respect to the Frobenius norm. First, we observe that the term Tr(M (H)3) in the expansion can

be shown to dominate the term Tr(diag(M (H)2)2) in expectation. The next deﬁnition states

this dominance formally.

Deﬁnition 3. (Regularity constant α) Let ν be a distribution over H ∈ H. Then the regularity constant associated with ν is deﬁned as the following ratio:

α = EH∼ν Tr diag(M (H)2)2 / EH∼ν Tr M (H)3 .

(46)

The next lemma states that the regularity constant α associated with the invariant distribution νγ is always less than one. Our analysis will in fact directly rely on α < 1.
Lemma 10. Suppose that the chain {H(γ)} admits the unique invariant distribution νγ (in Def. 2). Then, the regularity constant of νγ (in Def. 3) is less than one for a suﬃciently small γ.

Proof. We use a proof by contradiction where we suppose that the regularity constant of distribution νγ is greater than one. In this case, we prove that the distribution cannot be invariant with respect to the Frobenius norm.
If the regularity constant α is greater than one, then

EH∼νγ −Tr(M (H)3) + Tr(diag(M (H)2)2) ≥ 0

(47)

holds. According to Theorem 5, the rank of M (H) is at least 2. Since the sum of the eigenvalues is constant d, the leading eigenvalue is less than d. This leads to





M (H)

2 F

=

λ2i ≤ max λi  λj ≤ d max λi < d2.

i

i

i

j

19

Plugging the above inequality together with inequality 47 into the established bound in Lemma 7 yields

EW,H ∼νγ

M (H+)

2 F

−

M (H)

2 F

>0

(48)

for a suﬃciently small γ. Therefore, νγ does not obey the invariance property for g(H) =

M (H)

2 F

in

Def.

2.

We can experimentally estimate the regularity constant α using the Ergodicity of the chain. Assuming that the chain is Ergodic11,

1L

(γ)

lim L→∞ L

g(H ) = EH∼νγ [g(H)]

(49)

=1

holds almost surely for every Borel bounded function g : H → R. By setting g1(H) = Tr(M (H)3) and g2(H) = Tr(diag(M (H)2)2), we can estimate EH∼νγ [gi(H)] for i = 1, and 2. Given these estimates, α can be estimated. Our experiments in Fig. 8 show that the regularity constant of
the invariant distribution νγ is less than 0.9 for d > 10.

0.96

0.94

0.92

0.90

0.88

0.86

0.84

0.820

50 100 150 200 250 300

0.94

0.92

0.90

0.88

0.86

0.84

0.820

50 100 150 200 250 300

γ=1

γ = 0.1

Figure 8: Regularity constant of the invariant distribution. The vertical axis is the estimated regularity constant α and the horizontal axis is d. We use L = 105 (in Eq. (49)).

Interlude: intuition behind the regularity We highlight the regularity constant does by itself not yield the desired rank property in Theorem 2. This is illustrated in the next example that shows how the regularity constant relates to the spectral properties of M (H).
Example 11. Suppose that the support of distribution ν contains only matrices H ∈ H for which all rows of M (H) have the same norm. If the regularity constant of ν is greater than or equal to one, then all non-zero eigenvalues of matrix M (H) are equal.
A detailed justiﬁcation of the above statement is presented at the end of this section. This example shows that the regularity constant does not necessarily relate to the rank of H, but instead it is determined by how much non-zero eigenvalues are close to each other. We believe that a suﬃcient variation in non-zero eigenvalues of M (H) imposes the regularity of the law of H with a constant less than one (i.e. α < 1 in Def. 3). The next example demonstrates this.
Example 12. Suppose the support of distribution ν contains matrices H ∈ H for which all rows of M (H) have the same norm. Let λ ∈ Rd contain sorted eigenvalues of M (H). If λ1 = Θ(dβ) and λi = o(dβ) for i > 1 and β < 1,12 then the regularity constant α associated with ν is less than 0.9 for suﬃciently large d.
11The uniqueness of the invariant distribution implies Ergodicity (see Theorem 5.2.1 and 5.2.6 [10]). 12According to deﬁnition, limd→∞ o(dβ)/Θ(dβ) = 0

20

We later provide further details about this example. Invariance consequence The next lemma establishes a key result on the invariant distribution νγ .

Lemma 13. Suppose that the chain {H(γ)} (see Eq. 7) admits the invariant distribution νγ (see

Def. 2). If the regularity constant associated with νγ is α < 1 (deﬁned in Def. 3), then

EH ∼νγ

√

M (H)

2 F

≤ d3/2/

1−α

(50)

holds for a suﬃciently small γ.

Proof. Leveraging invariance property in Def. 2,

EW,H ∼νγ

M (H+)

2 F

−

M (H)

2 F

=0

(51)

holds where the expectation is taken with respect to the randomness of W and νγ.13 Invoking the result of Lemma 7, we get

EH ∼νγ

2d2 − 2

M (H)

2 F

− 8Tr(M (H)3) + 8Tr(diag(M (H)2)2)

+ O(γ) = 0.

(52)

Having a regularity constant less than one for νγ implies

0 ≤ 2d2 − EH∼νγ

2

M (H)

2 F

− 8(1 − α)Tr(M (H)3)

(53)

holds for suﬃciently small γ. Let λ ∈ Rd be a random vector containing the eigenvalues of the random matrix M (H).14 The eigenvalues of M 3 are λ3, hence the invariance result can be
written alternatively as

0 ≤ 2d2 − E

2

λ

2 2

−

8(1

−

α)

λ

3 3

.

(54)

The above equation leads to the following interesting spectral property:

E

λ

3 3

≤ d2/(1 − α).

(55)

A straightforward application of Cauchy-schwarz yields:

λ

2 2

=

λ2i =

λ1i/2λ3i/2 ≤

i

i

λi λ3i ≤

i

j

d

λ

3 3

(56)

Given (i) the above bound, (ii) an application of Jensen’s inequality, (iii) and the result of Eq. (55), we conclude with the desired result:

(i)

(ii)

EH∼νγ [M (H)] = E

λ

2 2

≤E

d

λ

3 3

≤

(iii)

√

dE

λ

3 3

≤

d3/2/

1−α

(57)

Notably, the invariant distribution is observed to have a regularity constant less than 0.9 (in Fig. 8) for suﬃciently large d. This implies that an upper-bound O d3/2 is achievable on the Frobenius norm. Leveraging Ergodicity (with respect to Frobenius norm in Eq. (49)), we experimentally validate the result of the last lemma in Fig. 9. Proof of the Main Theorem Here, we give a formal statement of the main Theorem that contains all required additional details (which we omitted for simplicity in the original statement).

13This result is obtained by setting g(H) =

M (H)

2 F

in

Def.

2.

14Note that H ∈ H is a random matrix whose law is νγ , hence λ ∈ Rd is also a random vector.

21

14

|M|_F^2

12

d**(1.5)

10

8

6

4

22

3

4

5

6

7

8

14

|M|_F^2

12

d**(1.5)

10

8

6

4

22

3

4

5

6

7

8

γ=1

γ = 0.1

Figure 9:

Dependency of Eνγ

M (H)

2 F

on d.

The horizontal axis is log2(d) and the vertical axis

shows log2( L1

L =1

M (H(γ))

2 F

)

for

L = 105.

The

green

dashed-line

plots

log2(d1.5).

Theorem 14 (Formal statement of Theorem 2). Suppose that rank(X) = d, γ is suﬃciently small, and all elements of the weight matrices {W } are drawn i.i.d. from a zero-mean, unit variance distribution whose support lies in [−B, B] and its law is symmertic around zero. Furthermore, assume that the Markov chain {H(γ)} (deﬁned in Eq. 1) admits a unique invariant distribution. Then, the regularity constant α > 0 associated with νγ (see Def. 3) is less than one and the following limits exist such that

lim 1 L rankτ (H(γ)) ≥ lim (1 − τ )2 L r(H(γ)) ≥ (1 − τ )2(1 − α)1/2√d (58)

L→∞ L

L→∞ L

=1

=1

holds almost surely for all τ ∈ [0, 1]. Assuming that the regularity co√nstant α√does not increase with respect to d, the above lower-bound is proportional to (1 − α)1/2 d = Ω( d).

Remarkably, we experimentally observed (in Fig. 8) that the regularity constant α is decreasing with respect to d. Examples 11 and 12 provide insights about the regularity constant. We believe that it is possible to prove that the constant α is non-increasing with respect to d.

Proof of Theorem 2 . Lemma 10 proves that the regularity constant α is less than one for the unique invariant distribution. Suppose that H ∈ H is a random matrix whose law is the one of the unique invariant distribution of the chain. For H ∈ H, we get Tr(M (H)) = d. A straightforward application of Jensen’s inequality yields the following lower bound on the expectation of r(H) (i.e. the lower bound on the rank):

E [r(H)] = E

Tr(M (H))2/

M (H)

2 F

=E

d2/

M (H)

2 F

≥ d2/E

M (H)

2 F

(59)

where the expectation is taken over the randomness of H (i.e. the invariant distribution). Invoking the result of Lemma 13, we get an upper-bound on the expectation of the Frobenius norm – in the right-side of the above equation. Therefore,

E [r(H)] ≥ (1 − α)d

(60)

holds. The uniqueness of the invariant distribution allows us to invoke Birkhoﬀ’s Ergodic Theorem for Markov Chains (Theorem 5.2.1 and 5.2.6 [10]) to get

1L

(γ)

lim

r(H ) = E [r(H)] ≥ (1 − α)d.

(61)

L→∞ L

=1

22

The established lower bound on rankτ (H(γ)) –in terms of r(H(γ))– in Lemma 1 concludes

lim 1 L rankτ (H(γ)) ≥ lim (1 − τ )2 L r(H(γ)) ≥ (1 − τ )2 (1 − α)d. (62)

L→∞ L

L→∞ L

=1

=1

As shown in the following corollary, one can extend the result of Theorem 14 for any ﬁnite . √
Corollary 15. Under the setting of Thm. 14, rank(H ) = Ω( d) holds almost surely for all ﬁnite integer . Assum√ing that {rankτ (H )} is a monotonically no-increasing sequence, then rankτ (H ) = Ω((1 − τ )2 d) holds almost surely for all ﬁnite .

Proof. The proof is based on the no-increasing property of the rank15. Next lemma presents a straightforward implication of this property.

Lemma 16. Consider a sequence of non-increasing bounded ﬁnite integers {yk}∞ k=1. If limN→∞ exists and is greater than α, then yk ≥ α for all ﬁnite k.

N k=1

yk

/N

The proof of the last lemma is provided at the end of this section. Replacing the result of Thm. 14 into the above lemma concludes the proof of the corollary.

A remark on the number of hidden units. The focus of our analysis was networks with the same number of hidden units in each layer. Yet, this result extends to more general architectures. Most of modern neural architectures consists of blocks in which the number of hidden units are constant. For example, VGG19-Nets and ResNets are consist of blocks convolutional layers with 64, 128, 256, and 512 channels where the number channels are equal in each block. An analogy of such an architecture is an MLP with diﬀerent blocks of hidden layers where the numbers of hidden units are the same in each block. According to Cor. 15, the rank preservation property holds in each block after applying BN. In this way, one can extend the established results of Thm. 14 and Cor. 15 to a general family of architectures with varying number of hidden units.

15Recall that the rank does not increases in updates of Eq. (7)
23

Postponed proofs.

Proof of Lemma 7. The proof is based on a Taylor expansion of the Bn non-linear recurrence function, which we restate here for simplicity:

H+ = (diag(M (Hγ)))−1/2 Hγ, Hγ = (I + γW )H

(63)

Consider the covariance matrices M = M (H) and M+ = M (H+) which obey

Mγ := M (Hγ) = M + ∆M, ∆M := γW M + γM W + γ2W M W

(64)

[M+]2ij = gij (Mγ ) = [Mγ ]2ij /[Mγ ]ii[Mγ ]jj

(65)

For the sake of simplicity, we use the compact notation g := gij for i = j. We further introduce the set of indices S = {ii, ij, jj}. A taylor expansion of g at M yields

EW [g(Mγ)] = g(M ) +
pq∈S

∂g(M ) ∂Mpq

EW [∆Mpq]

T1
+ 1 ∂2g(M ) EW [∆Mpq∆Mkm] +O(γ3). (66) 2 pq,km∈S ∂Mpq∂Mkm

T2
√√ Note that the choice of the element-wise uniform distribution over [− 3, 3] allows us to deterministically bound the Taylor remainder term by O(γ3). Now, we compute the derivatives
and expectations that appear in the above expansion individually. Let us start with the term T1. The ﬁrst-order partial derivative term in T1 is computed bellow.

∂g(M ) = −Mi2j/(Mi2iMjj) = −g(M ) pq = {ii, jj} (67)

∂Mpq

2Mij /(MiiMjj)

pq = {ij}.

The expectation term in T1 is

0

pq = {ij}

EW [∆Mpq] = γ2 dk=1 Mkk = γ2d pq = {ii, jj}. (68)

Given the above formula, we reach the following compact expression for T1:

T1 = −2γ2dg(M ).

(69)

The compute T2 we need to compute second-order partial derivatives of g and also estimate the following expectation:





EW

[∆Mpq ∆Mkm ]

=

γ2

 EW

[W M + M W

]pq[W M + M W

]km

 

+

O(γ

3

).

(70)





Kpq,km

We now compute Kpq,km in the above formula

  k Mk2j + n Mi2n α = {ij}, β = {ij}



Kα,β = 24 k MMk2jMki αα == {{iiji}},,ββ == {{iiii}} (71)

 

k ki



0

α = {ii}, β = {jj}

24

The second-order partial derivatives of g reads as



2

α = {ij}, β = {ij}



 ∂2g(M ) = −2Mij α = {ij}, β = {ii} (72) ∂Mα∂Mβ +2Mi2j α = {ii}, β = {ii}


 Mi2j

α = {jj}, β = {ii}

Now, we replace the computed partial derivatives and the expectations into T2:

T2 = Mk2j + Mi2n − 8 Mkj Mij Mki + 4 Mi2j Mk2i + 4 Mi2j Mk2j

k

n

k

k

k

(73)

Plugging terms T1 and T2 into the Taylor expansion yields

EW [gij(M+) − gij(M )] /(γ2)

= Mk2j + Mi2n − 2dgij (M ) − 8 Mkj Mij Mki + 4 Mi2j Mk2i + 4 Mi2j Mk2j + O(γ)

k

n

k

k

k

(74)

Summing over i = j concludes the proof (note that the diagonal elements are one for the both of matrices M and M+).
Proof of Proposition 8. Consider the spectral decomposition of matrix M as M = U diag(λ)U , then M k = U diag(λk)U . Since Tr(M k) is equal to the sum of the eigenvalues of M k, we get

d

Tr(M k) =

λki =

λ

k k

(75)

i=1

for k = 2 and k = 3. The sum of the squared norm of the rows in M is equal to the Frobenius norm of M . Assuming that the rows have equal norm, we get

d

dd

Mi2k =

Mi2k/d =

k=1

i=1 k=1

M

2 F

/d

=

λ 22/d.

(76)

Therefore,

d
Tr(diag(M 2)2) =
i=1

d

2

Mi2k = λ 42/d

k=1

(77)

holds.

Details of Example 9. Under the assumptions stated in Example 9, we get

λ

2 2

≈ d2 − 2γ2d,

λ

3 3

≈ d3 − 3γ2d2,

λ

4 2

≈ d4 − 4γ2d3

(78)

where the approximations are obtained by a ﬁrst-order Taylor approximation of the norms at λ = (d, 0, . . . , 0), and all small terms o(γ2) are omitted. Using the result of Proposition 8, we get

E

M+

2 F

−E

M

2 F

≈ γ2δF (λ) ≈ O(−γ4d2).

(79)

25

Let λ+ be the eigenvalues of the matrix M+, then

d

E[λ2+]i − λ2i = O(−γ4d2)

(80)

i=1

d

=⇒ max E[λ2+]i − λ21 ≤ O(−γ4d2) + λ2i ≤ O(−γ4d2) + γ4d = O(−γ4d2).

(81)

i

i=2

Let j = arg maxi E [λ+]2i . A straight-forward application of Jensen’s inequality yields

E [[λ+]j] ≤ E [λ+]2j ≤ λ1 − O(γ4d).

(82)

Hence the leading eigenvalue of M+ is smaller than the one of M . Since the sum of eigenvalues λ+ and λ are equal, some of the eigenvalues λ+ are greater than those of λ (in expectation) to compensate E[λ+]j < λ1.

Details of Example 11. Invoking Prop. 8, we get

E Tr(M (H)3) = λ 3, E diag(M (H)2)2 = λ 42/d,

(83)

where λ ∈ Rd contains the eigenvalues of M (H). Since H ∈ H, λ 1 = d. If the regularity constant is greater than or equal to one, then

λ

3 3

≤

λ 42/d =

λ 42/ λ 1.

(84)

A straightforward application of Cauchy-Schwartz yields:

dd

dd

λ

4 2

=

λ2i λ2j =

(λiλj )1/2(λiλj )3/2

i=1 j=1

i=1 j=1

≤









λiλj 

λ3i λ3j  =

λ

1

λ

3 3

i,j

i,j

(85)

The above result together with inequality 84 yields that

λ

3 3

=

λ 42/d =

λ 42/ λ 1.

(86)

Finally, the above equality is met only when all non-zero eigenvalues are equal.

Details of Example 12. Since λ1 = Θ(dβ) and λi>1 = o(dβ), we get

λ

3 3

= Θ(d3β),

λ

2 2

= Θ(d2β).

(87)

Thus, Prop. 8 yields

E Tr(M 3) = Θ(d3β), E Tr(diag(M 2)2) = λ 42/d = Θ(d4β−1)

(88)

Therefore,

E Tr(diag(M 2)2)

β−1

α = lim
d→∞

E [Tr(M 3)]

= O(d ) = 0.

(89)

As a result, α is less than 0.9 for suﬃciently large d.

26

Proof of Lemma 16. The proof is based on a contradiction. Suppose that there exits a ﬁnite n such that yn < α. Since the sequence is non-increasing, ym < α for holds for all m > n. This yields





N

N

lim yk/N = lim  yk/N + yk/N 

N →∞

N →∞

k=1

k>n

k≤n

(90)

(N − n)

<

α + lim yk/N

(91)

N

N →∞

k≤n

(N − n)

=

α,

(92)

N

where we used the fact that all yk are bounded. The above result contradicts the fact that

limn→∞

N k=1

yk/N

>

α.

F Analysis for Vanilla Linear Networks.

In this section, we prove Lemma 3 that states the rank vanishing problem for vanilla linear networks. Since the proof relies on existing results on products of random matrices (PRM) [8], we ﬁrst shortly review these results. Let T be the set of d × d matrices. Then, we review two notions for T : contractiveness and strong irreducibility.

Deﬁnition 4 (Contracting set [8]). T is contracting if there exists a sequence {Mn ∈ T, n ≥ 0} such that Mn/ Mn converges to a rank one matrix.

Deﬁnition 5 (Invariant union of proper subspaces [8]). Consider a family of ﬁnite proper linear subspace V1, . . . , Vk ⊂ Rd. The union of these subspaces is invariant with respect to T , if M v ∈ V1 or V2 or . . . or Vk holds for ∀v ∈ V1 or V2 or . . . or Vk and ∀M ∈ T .

Example 17. Consider the following sets









01 T = 1 0 , V1 = span([0, 1]) , V2 = span([1, 0]) ;

v1

v2

then, union of V1 and V2 is invariant with respect to T because αT v1 ∈ V2 and αT v2 ∈ V1 hold for α = 0.

Deﬁnition 6 (Strongly irreducible set [8]). The set T is strongly irreducible if there does not exist a ﬁnite family of proper linear subspaces of Rd such that their union is invariant with respect to T .

For example, the set T deﬁned in Example 17 is not strongly irreducible.

Lemma 18 (Thm 3.1 of [8]). Let W1, W2, . . . be random d×d matrices drawn independently from

a distribution µ. Let Bn =

n k=1

Wk

.

If

the

support

of

µ

is

strongly

irreducible

and

contracting,

then any limit point of {Bn/ Bn }∞ n=1 is a rank one matrix almost surely.

This result allows us to prove Lemma 3.

Proof of Lemma 3. Recall the structure of the random weight matrices as Wk√= I√+ γWk where the coordinates Wk are i.i.d. from (a.) standard Gaussian, (b.) uniform[− 3, 3] (i.e. with variance 1). One can readily check that for the Gaussian weights, the contracting and strong

27

irreducibility hold and one can directly invoke the result of lemma 18 to get part (a.) of Lemma 3.

Now, we prove part (b.). Let m be a random integer that obeys the law p(m = k) = 2−k. Given

the random variable m, we deﬁne the random matrix Y =

m k=1

Wk

and

use

the

notation

µ

for

its law. Let {Yi =

mi j=1

Wk

}ki=1

be

drawn

i.i.d.

from

µ

.

Then,

Ck

:=

Yk . . . Y2Y1

is

distributed

as

B k := W k . . . W2W1 for k =

k i=1

mi.

We prove that every limit point of {Ck/

Ck

} converges

to a rank one matrix, which equates the convergence of limit points of {B k / B k } to a rank one

matrix. To this end, we prove that the support of µ denoted by Tµ is contractive and strongly

contractive. Then, Lemma 18 implies that the limit points of {Ck/ Ck } are rank one. Contracting. Let e1 ∈ Rd be the ﬁrst standard basis vector. Since An := (I + γe1e1 )n ∈ Tµ

and its limit point {An/ An } converges to a rank one matrix, Tµ is contractive. Strong irreduciblity. Consider an arbitrary family of linear proper subspace of Rd as

{V1, . . . , Vq}. Let v be an arbitrary unit norm vector which belongs to one of the subspaces {Vi}qi=1. Given v, we deﬁne an indexed family of matrices {Mα ∈ Tµ |α ∈ Rd, |αi| ≤ 1} such that

γd Mα = I + d αieiv ∈ Tµ , (93)
i=1

where ei is the i-th standard basis16. Then, we get

γd Mαv = v + d αiei. (94)
i=1

Therefore, {Mαv||αi| ≤ 1} is not contained in any union of ﬁnite proper (m < k)-dimensional linear subspace of Rd, hence Tµ is strongly irreducible.

G Details: Pretraining algorithm

In Section 4, we introduced a pre-training method that eﬀectively obtains a better optimization
performance compared ot Bn. In this section, we provide more details about the pre-training step. Recall X ∈ Rd×N is a minibatch of d-dimensional inputs of size N . Let HL(X) ∈ Rd×N be the hidden representation of input X in the last layer of a MLP. Using gradient descent method,
we optimize r(HL(X)) –with respect to the parameters of networks– over diﬀerent minibatches X. Algorithm 1 presents our pretraining method. As can be seen, the procedure is very simple.

Algorithm 1 Pretraining

1: Input: Training set S, a network with parameters Θ and L layers, and constant N, M , and

T

2: for k = 1, 2, . . . , M do

3: Draw minibatch Xk of size N i.i.d. from S

4: for t = 1, 2, . . . , T do

5:

Take one GD step on r(HL(Xk)) w.r.t Θ.

6: end for

7: end for

8: return Θ.

16Notably, the absolute value of each element of d1 support of µ.

d i=1

αieiv

is less than 1, hence this matrix belongs to the

28

H Details: Why the rank matters for gradient based learning.

We now provide an intuitive explanation of why rank one hidden representations prevent randomly
initialized networks from learning. Particularly, we argue that these networks essentially map all inputs to a very small subspace17 such that the ﬁnal classiﬁcation layer can no longer disentangle
the hidden representations. As a result, the gradients of that layer also align, yielding a learning
signal that becomes independent of the input.
To make this claim more precise, consider training the linear network from Eq. (6) on a dataset X ∈ Rd×N , where xi ∈ Rd with dout targets yi ∈ Rdout, i = 1, . . . , N . Each column HL(γ,i) of the hidden representations in the last hidden layer HL(γ) is the latent representation of datapoint i, which is fed into a ﬁnal classiﬁcation layer parametrized by WL+1 ∈ Rdout×d. We optimize L(W), where W is a tensor containing all weights W1, . . . , WL+1 and HL(γ,i) is a function of W1, . . . , WL (as detailed in Eq. (6):

N
min L(W) =
W i=1

y

i

,

WL

+1

H

(γ) L,i

(W

1

,

...,

W

L

)

,

:=Li(W)

(95)

and : Rdout → R+ is a diﬀerentiable loss function. Now, if the the hidden representations
become rank one (as predicted by Lemma 3 and Fig. 2), one can readily check that the stochastic gradients of any neuron k in the last linear layer, i.e., ∇WL,[k,:]Li(W) = (∇ i)kHL(γ,i), align for both linear and ReLU networks.

Proposition 19. Consider a network with rank one hidden representations in the last layer

HL(γ)(W1, ..., WL), then for any neuron k and any two datapoints i, j with non-zero errors Li and

Lj we have

c(∇ i)k ∇WL+1,[k,:] Li(W) = (∇ j )k ∇WL+1,[k,:] Lj (W)

(96)

∈R

∀i, j. That is, all stochastic gradients of neuron k in the ﬁnal classiﬁcation layer align along one single direction in Rd.

Proof. The result follows directly from a simple application of the chain rule

∂Li(W) ∂ (yi, WL+1HL(γ,i)) ∂WL+1hL,i

(γ)

= ∂WL+1

∂ WL+1 HL(γ,i)

∂WL+1 = ∇WL+1HL(γ,i) (yi, WL+1hL,i)(HL,i )

 ∇ i,1HL(γ,i),1, . . . , ∇

 =

...



∇ i,dout HL(γ,i),1, . . . , ∇

i,1HL(γ,i),d  

 i,dout HL(γ,i),d

∈ Rdout×d

(97)

The same holds for j. Now, if HL(γ,i) = cHL(γ,i), c ∈ R \ {0} then

∂Li(W) ∂WL+1

= c ∇ i,k k,: ∇ j,k
∈R

∂ Lj (W) ∂WL+1 k,:

17A single line in Rd in the extreme case of rank one mappings

29

To validate this claim, we again train CIFAR-10 on the VGG19 network from Figure 5 (top).

VGG19: Training Accuracy
100
BN-SGD SGD SGD random SGD 100x
10 1
0 1 2 3 Epo4chs 5 6 7 8

VGG19: avg. pairwise gradient angles
1.0

0.9

0.8

BN

0.7

SGD SGD random

0.6

SGD 100x

0.5

0.4
0 1 2 3 Epo4chs 5 6 7 8

Figure 10: Directional gradient vanishing CIFAR-10 on a VGG19 network with BN, SGD, SGD with
100x learning rate and SGD on random data. Average and 95% conﬁdence interval of 5 independent runs.

As expected, the network shows perfectly aligned gradients without Bn (right hand side of Fig. 10), which renders it un-trainable. In a next step, we replace the input by images generated randomly from a uniform distribution between 0 and 255 and ﬁnd that SGD takes almost the exact same path on this data (compare log accuracy on the left hand side). Thus, our results suggest that the commonly accepted vanishing gradient norm hypothesis is not descriptive enough since SGD does not take small steps into the right direction- but into a random one after initialization in deep neural networks. As a result, even a 100x increase in the learning rate does not allow training. We consider our observation as a potential starting point for novel theoretical analysis focusing on understanding the propagation of information through neural networks, whose importance has also been highlighted by [6].

I Additional Experiments

Training Accuracy

0.8 0.7 0.6 0.5 0.4 0.3
0
2.4 2.2 2.0 1.8 1.6 1.4
0

10 hidden layers
pre-trained SGD BatchNorm SGD 20 40Epochs60 80 100
10 hidden layers
pre-trained SGD BatchNorm SGD
20 40Epochs60 80 100

Test loss

Training Accuracy

0.7 0.6 0.5 0.4 0.3 0.2
0
3.5 3.0 2.5 2.0 1.5
0

30 hidden layers
pre-trained SGD BatchNorm SGD 20 40Epochs60 80 100
30 hidden layers
pre-trained SGD BatchNorm SGD
20 40Epochs60 80 100

Test loss

Training Accuracy

0.5 0.4 0.3 0.2 0.1 0
2.4 2.2 2.0 1.8 1.6
0

50 hidden layers
pre-trained SGD BatchNorm SGD
20 40Epochs60 80 100 50 hidden layers pre-trained SGD BatchNorm SGD
20 40Epochs60 80 100

Figure 11: CIFAR-10: Same setting as Fig.4 but now showing accuracy and test loss

Test loss

Outperforming Bn The following Figure shows the result of the experiment of Fig. 4 that is repeated for FashionMNIST dataset.

30

Training loss

0.7 0.6 0.5 0.4 0.3 0.2 0.1
0
0.95 0.90 0.85 0.80 0.75
0

10 hidden layers
pre-trained SGD BatchNorm SGD
20 40Epochs60 80 100 10 hidden layers
pre-trained SGD BatchNorm SGD 20 40Epochs60 80 100

Training Accuracy

Training loss

1.2 1.0 0.8 0.6 0.4 0.2
0
0.9 0.8 0.7 0.6 0.5
0

30 hidden layers
pre-trained SGD BatchNorm SGD
20 40Epochs60 80 100 30 hidden layers
pre-trained SGD BatchNorm SGD 20 40Epochs60 80 100

Training Accuracy

Training loss

2.0
1.5
1.0
0.5
0
0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0

Figure 12: Results of Fig.4 for FashionMNIST

50 hidden layers
pre-trained SGD BatchNorm SGD
20 40Epochs60 80 100 50 hidden layers
pre-trained SGD BatchNorm SGD 20 40Epochs60 80 100

Training Accuracy

Breaking Bn In the following result, we repeated the experiment of Fig. 5 for ResNets.

ResNet50: Training Accuracy

1.0

W U[ a, a]

W U[0, 2a]

0.8

0.6

0.4

0.2

0 1 2 3Epoc4hs 5 6 7

ResNet50: Rank in last hidden layer

30

25

20

W U[ a, a]

15

W U[0, 2a]

10

5

0
0 1 2 3Epoc4hs 5 6 7

Figure 13: Breaking Batchnorm: CIFAR-10 on a ResNet-50 with standard PyTorch initialization as well as
a uniform initialization of same variance in R+. Average and 95% conﬁdence interval of 5 independent runs. This
plot also shows results for a Bn network without mean deduction/adaption, validating our claim from Section 2.

31

