Partial Counterfactual Identiﬁcation from Observational and Experimental Data
Junzhe Zhang,1 Jin Tian, 2 Elias Bareinboim 1
1 Columbia University 2 Iowa State University junzhez@cs.columbia.edu, jtian@iastate.edu, eb@cs.columbia.edu

arXiv:2110.05690v1 [cs.AI] 12 Oct 2021

Abstract
This paper investigates the problem of bounding counterfactual queries from an arbitrary collection of observational and experimental distributions and qualitative knowledge about the underlying data-generating model represented in the form of a causal diagram. We show that all counterfactual distributions in an arbitrary structural causal model (SCM) could be generated by a canonical family of SCMs with the same causal diagram where unobserved (exogenous) variables are discrete with a ﬁnite domain. Utilizing the canonical SCMs, we translate the problem of bounding counterfactuals into that of polynomial programming whose solution provides optimal bounds for the counterfactual query. Solving such polynomial programs is in general computationally expensive. We therefore develop effective Monte Carlo algorithms to approximate the optimal bounds from an arbitrary combination of observational and experimental data. Our algorithms are validated extensively on synthetic and real-world datasets.
Introduction
This paper studies the problem of inferring counterfactual queries from a combination of observations, experiments, and qualitative assumptions about the phenomenon under investigation. The assumptions are represented in the form of a causal diagram (Pearl 1995), which is a directed acyclic graph where arrows indicate the potential existence of functional relationships among corresponding variables; some variables are unobserved. This problem arises in diverse ﬁelds such as artiﬁcial intelligence, statistics, cognitive science, economics, and the health and social sciences. For example, when investigating the gender discrimination in college admission, one may ask “what would the admission outcome be for a female applicant had she been a male?” Such a counterfactual query contains conﬂicting information: in the real world the applicant is female, in the hypothetical world she was not. Therefore, it is not immediately clear how to design effective experimental procedures for evaluating counterfactuals, or how to compute them from observational data.
The problem of identifying counterfactual distributions from the combination of data and a causal diagram has been studied in the causal inference literature. First, there exists a complete proof system for reasoning about counterfactual queries (Halpern 1998). While such a system, in prin-
Preprint. Under review.

ciple, is sufﬁcient in evaluating any identiﬁable counterfactual expression, it lacks a proof guideline that determines the feasibility of such evaluation efﬁciently. There are algorithms to determine whether a counterfactual distribution is inferrable from all possible controlled experiments (Shpitser and Pearl 2007), or a special type of counterfactual distributions, called path-speciﬁc effects, from observational (Shpitser and Sherman 2018) and experimental data (Avin, Shpitser, and Pearl 2005). Finally, there exist an algorithm that decides whether any nested counterfactual is identiﬁable an arbitrary combination of observational and experimental distributions (Correa, Lee, and Bareinboim 2021).
In practice, however, the combination of quantitative knowledge and observed data does not always permit one to uniquely determine the target counterfactual query. In such cases, the counterfactual query is said to be non-identiﬁable. Partial identiﬁcation methods concern with deriving informative bounds over the target counterfactual probability in non-identiﬁable settings. Several algorithms have been developed to bound counterfactual probabilities from the combination of observational and experimental data (Manski 1990; Robins 1989; Balke and Pearl 1994, 1997; Evans 2012; Richardson et al. 2014; Kallus and Zhou 2018, 2020; Finkelstein and Shpitser 2020; Kilbertus, Kusner, and Silva 2020; Zhang and Bareinboim 2021).
In this work, we build on the approach introduced by Balke & Pearl in (Balke and Pearl 1994), which involves direct discretization of unobserved domains, also referred to as the canonical partitioning or the principal stratiﬁcation (Frangakis and Rubin 2002; Pearl 2011). Consider the causal diagram in Fig. 1a, where X, Y, Z are binary variables in {0, 1}; U is an unobserved variable taking values in an arbitrary continuous domain. (Balke and Pearl 1994) showed that domains of U could be discretized into 16 equivalent classes without changing the original counterfactual distributions and the graphical structure in Fig. 1a. For instance, suppose that values of U are drawn from an arbitrary distribution P ∗(u) over a continuous domain. It has been shown that the observational distribution P (x, y, z) could be reproduced by a generative model of the form P (x, y, z) =
u P (x|u, z)P (y|x, u)P (u)P (z), where P (u) is a discrete distribution over a ﬁnite domain {1, . . . , 16}.
Using the ﬁnite-state representation of unobserved variables, (Balke and Pearl 1997) derived tight bounds on treat-

ment effects under a set of constraints called instrumental variables (e.g., Fig. 1a). (Chickering and Pearl 1997; Imbens and Rubin 1997) applied the parsimony of ﬁnite-state representation in a Bayesian framework, to obtain credible intervals for the posterior distribution of causal effects in noncompliance settings. Despite the optimality guarantees in their treatments, these bounds were only derived for speciﬁc settings. A systematic strategy for partial identiﬁcation in an arbitrary causal diagram is still missing. There are signiﬁcant challenges in bounding any counterfactual query in an arbitrary causal diagram given an arbitrary collection of observational and experimental data.
Our goal in this paper is to overcome these challenges. We show that when inferring about counterfactual distributions (over ﬁnite observed variables) in an arbitrary causal diagram, one could restrict domains of unobserved variables to a ﬁnite space without loss of generality. This result allows us to develop novel partial identiﬁcation algorithms to bound unknown counterfactual probabilities from an arbitrary combination of observational and experimental data. In some way, this paper can be seen as closing a long-standing open problem introduced by (Balke and Pearl 1994), where they solve a special bounding instance in the case of instrumental variables. More speciﬁcally, our contributions are as follows. (1) We introduce a special family of discrete structural causal models, and show that it could represent all categorical counterfactual distributions (with ﬁnite support) in an arbitrary causal diagram. (2) Using this result, we translate the partial identiﬁcation task into an equivalent polynomial program. Solving such a program leads to bounds over target counterfactual probabilities that are provably optimal. (3) We develop an effective Monte Carlo algorithm to approximate optimal bounds from a ﬁnite number of observational and experimental data. Finally, our algorithms are validated extensively on synthetic and real-world datasets.
Preliminaries
We introduce in this section some basic notations and deﬁnitions that will be used throughout the paper. We use capital letters to denote variables (X), small letters for their values (x) and ΩX for their domains. For an arbitrary set X, let |X| be its cardinality. The probability distribution over variables X is denoted by P (X). For convenience, we consistently use P (x) as a shorthand for the probability P (X = x). Finally, the indicator function 1X=x returns 1 if an event X = x holds; otherwise, 1X=x is equal to 0.
The basic semantical framework of our analysis rests on structural causal models (SCMs) (Pearl 2000, Ch. 7). An SCM M is a tuple V , U , F , P where V is a set of endogenous variables and U is a set of exogenous variables. F is a set of functions where each fV ∈ F decides values of an endogenous variable V ∈ V taking as argument a combination of other variables in the system. That is, v ← fV (paV , uV ), PAV ⊆ V , UV ⊆ U . Exogenous variables U ∈ U are mutually independent, values of which are drawn from the exogenous distribution P (U ). Naturally, M induces a joint distribution P (V ) over endogenous variables V , called the observational distribution.
Each SCM M is also associated with a causal diagram

U1

U2

U1

U2

Z

X

Y

(a) IV

U1

U2

Z

X

Y

(b)
U

X

W

Y

(c) Frontdoor

X

Y

(d) Bow

Figure 1: Causal diagrams containing a treatment X, an outcome Y , an ancestor Z, a mediator W , and unobserved U s.

G (e.g., Fig. 1), which is a directed acyclic graph (DAG) where solid nodes represent endogenous variables V , empty nodes represent exogenous variables U , and arrows represent the arguments PAV , UV of each structural function fV . We will use graph-theoretic family abbreviations for graph-
ical relationships such as parents and children. For example, the set of parents of X in G is denoted by pa(X)G = ∪X∈X pa(X)G; ch are similarly deﬁned. The subscript G will be omitted when it is obvious from the context.
An intervention on an arbitrary subset X ⊆ V , denoted by do(x), is an operation where values of X are set to constants x, regardless of how they are ordinarily determined. For an SCM M , let Mx denote a submodel of M induced by intervention do(x). For any subset Y ⊆ V , the potential response Yx(u) is deﬁned as the solution of Y in the submodel Mx given U = u. Drawing values of exogenous variables U following the probability distribution P induces a counterfactual variable Yx. Speciﬁcally, the event Yx = y (for short, yx) can be read as “Y would be y had X been x”. For any subsets Y , . . . , Z, X, . . . , W ⊆ V , the distribution over counterfactuals Yx, . . . , Zw is deﬁned as:

P (yx, . . . , zw) =

1Yx(u)=y,...,Zw(u)=zdP (u). (1)

ΩU

Distributions of the form P (Yx) are called interventional distributions; when X = ∅, P (Y ) coincides with the observational distribution. Throughout this paper, we assume that endogenous variables V are discrete and ﬁnite; while exogenous variables U could take any (continuous) value. The counterfactual distribution P (Yx, . . . , Zw) deﬁned above is thus a categorical distribution. For a more detailed survey on SCMs, we refer readers to (Pearl 2000, Ch. 7).

Partial Counterfactual Identiﬁcation
We introduce the task of partial identiﬁcation of a counterfactual probability from a combination of observational and interventional distributions, which generalizes the previous partial identiﬁability settings that assume observational data are given (Balke and Pearl 1997; Imbens and Rubin 1997). Let Z = {zi}m i=1 be a ﬁnite collection of realizations zi for sets of variables Zi ⊆ V . We assume data are available from all of the interventional distributions in {P (Vz) | z ∈ Z}.

Note that Z = ∅ corresponds to the observational distri-
bution P (V ). Our goal is to ﬁnd a bound [l, r] for any
counterfactual probability P (yx, . . . , zw) from the collection {P (Vz) | z ∈ Z} and the causal diagram G.
Formally, let M (G) be the set of all SCMs associated with G, i.e., M (G) = {∀M | GM = G}1. The bound [l, r] is obtainable by solving the following optimization problem:

min / max PM (yx, . . . , zw)

M ∈M (G)

(2)

s.t. PM (vz) = P (vz) ∀v, ∀z ∈ Z

where PM (yx, . . . , zw) and PM (vz) are given in the form of Eq. (1). The lower l and upper bound r are minimum and maximum of the above equation respectively. By the formulation of Eq. (2), [l, r] must be the tight bound containing all possible values of the target counterfactual P (yx, . . . , zw).
Since we do not have access to the parametric forms of the underlying structural functions fV nor the exogenous distribution P (u), solving the optimization problem in Eq. (2) appears theoretically challenging. It is not clear how the existing optimization procedures can be used. Next we show the optimization problem in Eq. (2) can be reduced into a polynomial program by constructing an “canonical” SCM that is equivalent to the original SCM in representing the objective P (yx, . . . , zw) and all constraints P (Vz), ∀z ∈ Z.

Canonical Structural Causal Models
Our construction rests on the parametric family of discrete SCMs where values of each exogenous variable are drawn from a discrete distribution over a ﬁnite set of states.
Deﬁnition 1. An SCM M = V , U , F , P is said to be a discrete SCM if 1. For every exogenous U ∈ U , its values u are contained
in a discrete domain ΩU ; 2. For every endogenous V ∈ V , its values v are given by
a function v ← fV (paV , uV ) where for any paV , uV , fV (paV , uV ) is contained in a ﬁnite domain ΩV . For endogenous variables V , let P ∗ denote the collection of all possible counterfactual distributions over V , i.e.,
P ∗ = {P (Yx, . . . , Zw) | ∀Y , . . . , Z, X, . . . , W } . (3)
Recall that M (G) is the set of all SCMs compatible with a causal diagram G. Counterfactual distributions in G are deﬁned as {PM∗ : ∀M ∈ M (G)}. We will next show that discrete SCMs are indeed “canonical”, i.e., they could generates all counterfactual distributions in any causal diagram.
Our analysis utilizes a special type of clustering of endogenous variables in the causal diagram developed by (Tian and Pearl 2002), which we call confounded components.
Deﬁnition 2. For a causal diagram G, let U ∈ U be an arbitrary exogenous variable. A set of endogenous variables C(U ) ⊆ V (w.r.t. U ) is a c-component if for every V ∈ C(U ), there exists a sequence {U1, . . . , Un} ⊆ U such that: 1. U1 = U and Un ∈ UV ;
1We will use the subscript M to represent the restriction to an SCM M . Therefore, GM represents the causal diagram associated with M ; so does counterfactual distributions PM (yx, . . . , zw).

2. for every i = 1, . . . , n − 1, Ui and Ui+1 have a common child node, i.e., ch(Ui) ∩ ch(Ui+1) = ∅.
A c-component C(U ) in G is maximal if there exists no other c-component that strictly contains C(U ). For convenience, we will consistently use C(U ) to denote the maximal c-component w.r.t. every exogenous U ∈ U . For instance, Fig. 1a contains two c-components C(U1) = {Z} and C(U2) = {X, Y }; while exogenous variables U1, U2 in Fig. 1b share the same c-component C(U1) = C(U2) = {X, Y, Z} since they have a common child node Y .
Theorem 1. For a DAG G, consider following conditions2: 1. M (G) is the set of all SCMs compatible with G. 2. N (G) is the set of all discrete SCMs compatible with G
such that for every exogenous U ∈ U ,

|ΩU | =

|ΩPAV → ΩV | ,

(4)

V ∈C(U )

i.e., the number of functions mapping from domains of PAV to V for every endogenous V ∈ C(U ). Then, M (G), N (G) are counterfactually equivalent, i.e.,
{PM∗ : ∀M ∈ M (G)} = {PN∗ : ∀N ∈ N (G)} . (5)

Thm. 1 establishes the expressive power of discrete SCMs
in representing counterfactual distributions in a causal diagram G. Henceforth, we will refer to N (G) in Thm. 1 as the family of canonical SCMs for G. As an example, consider a causal diagram G in Fig. 1b where X, Y, Z are binary variables in {0, 1}. Since U1, U2 share the same c-component {X, Y, Z}, Eq. (4) implies that they also share the same cardinality d = |ΩZ | × |ΩZ → ΩX | × |ΩX → ΩY | = 32 in the canonical family N (G). It follows from Thm. 1 that the counterfactual distribution P (Xz , Yx ) in G could be generated by a SCM in N (G) and be written as follows:

d

P (xz , yx ) =

1fX (z ,u2)=x,fY (x ,u1,u2)=yP (u1)P (u2).

u1 ,u2 =1

More generally, Thm. 1 implies that counterfactual probabilities P (Yx, . . . , Zw) in any SCM M could be generically generated as follows: for dU = V ∈C(U) |ΩPaV → ΩV |,

P (yx, . . . , zw)

= 1Yx(u)=y,...,Zw(u)=z P (u). (6)

u

U ∈U

Among above quantities, P (U ) is a discrete distribution over a ﬁnite domain {1, . . . , dU }. Counterfactual variables Yx(u) = {Yx(u) | ∀Y ∈ Y } are recursively deﬁned as:

xY

if Y ∈ X

Yx(u) = fY ((PAY )x (u), uY ) otherwise (7)

where xY is the value assigned to variable Y in constants x.

2For every V ∈ V , we denote by ΩPAV → ΩV the set of all possible functions mapping from domains ΩPAV to ΩV .

Related work The discretization procedure in (Balke and Pearl 1994) was originally designed for the “IV” diagram in Fig. 1a, but it can not be immediately extended to other causal diagrams without loss of generality (see Appendix E for a detailed example). More recently, (Rosset, Gisin, and Wolfe 2018) applied a classic result of Carathe´odory theorem in convex geometry (Carathe´odory 1911) and showed that the observational distribution in any causal diagram could be represented using ﬁnitely many latent states. (Evans et al. 2018) proved a special case of Thm. 1 for interventional distributions in a restricted class of causal diagrams satisfying a running intersection property.
Thm. 1 generalizes existing results in several important ways. First, we prove that all counterfactual distributions could be generated using discrete exogenous variables with ﬁnite domains, which subsume both observational and interventional distributions. Second, Thm. 1 is applicable to any causal diagram, thus not relying on additional graphical conditions, e.g., IV constraints (Balke and Pearl 1994). More speciﬁcally, we introduce a general, canonical partitioning over exogenous domains in an arbitrary SCM. Any counterfactual distribution in this SCM could be written as a function of joint probabilities assigned to intersections of canonical partitions. This allows us to discretize exogenous domains while maintaining all counterfactual distributions and structures of the causal diagram. We refer readers to Appendix A for more details about the proof for Thm. 1.

Bounding Counterfactual Distributions
The expressive power of canonical SCMs in Thm. 1 suggests a natural algorithm for the partial identiﬁcation of counterfactual distributions. Recall that the canonical family N (G) for a causal diagram G consists of discrete SCMs with ﬁnite exogenous states. We derive a bound [l, r] over a counterfactual probability P (yx, . . . , zw) from an arbitrary collection of interventional distributions {P (Vz) | z ∈ Z} by solving the following optimization problem:

min / max PN (yx, . . . , zw)

N ∈N (G)

(8)

s.t. PN (vz) = P (vz) ∀v, ∀z ∈ Z

where PN (yx, . . . , zw) and PN (vz) are given in the form of Eq. (6). The optimization problem in Eq. (8) is generally
reducible to a polynomial program. To witness, for every U ∈ U , let parameters θu represent discrete probabilities P (U = u). For every V ∈ V , we represent the output of function fV (paV , uV ) given input paV , uV using an indicator vector µV(paV ,uV ) = µv(paV ,uV ) | ∀v ∈ ΩV such that

µv(paV ,uV ) ∈ {0, 1},

µv(paV ,uV ) = 1.
v∈ΩV

Doing so allows us to write any counterfactual probability
P (yx, . . . , zw) in Eq. (6) as a polynomial function of parameters µv(paV ,uV ) and θu. More speciﬁcally, the indicator function 1Yx(u)=y is equal to a product Y ∈Y 1Yx(u)=y.

For every Y ∈ Y , 1Yx(u)=y is recursively given by:

1y=xY 

1Yx(u)=y = 

µ(ypaY ,uY )1(PAY )x(u)=paY

pa Y

if Y ∈ X otherwise

For instance, consider again the causal diagram G in Fig. 1b. The counterfactual distribution P (Xz , Yx ) and the observational distribution P (X, Y, Z) of any discrete SCM in N (G) and be written as the following polynomial functions:

d

P (xz , yx ) =

µx(z ,u2)µY(x ,u1,u2)θu1 θu2 ,

(9)

u1 ,u2 =1

d

P (x, y, z) =

µz(u1)µx(z,u2)µ(yx,u1,u2)θu1 θu2 ,

u1 ,u2 =1

(10)

where µ(zu1), µ(xz ,u2), µ(yx ,u1,u2) are parameters taking values in {0, 1}; θui , i = 1, 2, are probabilities of the discrete distribution P (ui) over the ﬁnite domain {1, . . . , d}. One could derive a bound over P (xz , yx ) from P (X, Y, Z) by solving polynomial programs which optimize the objective Eq. (9) over parameters θu1 , θu2 , µz(u1), µ(xz,u2), µ(yx,u1,u2), subject to the constraints in Eq. (10) for all entries x, y, z.
We refer readers to Appendix D for additional examples
demonstrating how to reduce the original partial identiﬁca-
tion problem to an equivalent polynomial program. It follows immediately from Thm. 1 that the solution [l, r]
of the optimization program in Eq. (8) is guaranteed to be a
valid, tight bound over the target counterfactual probability.

Theorem 2. Given a DAG G and {P (Vz) | z ∈ Z}, the solution [l, r] of the polynomial program Eq. (8) is a tight bound over the counterfactual probability P (yx, . . . , zw).

Despite the soundness and tightness of its derived bounds, solving a polynomial program in Eq. (8) may take exponentially long in the most general case (Lewis 1983). Our focus here is upon the causal inference aspect of the problem and like earlier discussions we do not specify which solvers are used (Balke and Pearl 1994, 1997). In some cases of interest, effective approximate planning methods for polynomial programs do exist. Investigating these methods is an ongoing subject of research (Lasserre 2001; Parrilo 2003).

Bayesian Approach for Partial Identiﬁcation
This section describes an effective algorithm to approximate the optimal bound in Eq. (8) from ﬁnite samples drawn from interventional distributions {P (Vz) | z ∈ Z}, provided with prior distributions over parameters θu and µ(VpaV ,uV ) (possibly uninformative). Given space constraints, all proofs for results in this section are provided in Appendix B.
More speciﬁcally, the learner has access to a ﬁnite dataset v¯ = V (n) = v(n) | n = 1, . . . , N , where each V (n) is an independent sample drawn from an interventional distribution P (Vz) for some z ∈ Z. With a slight abuse of notation, we denote by Z(n) the set of variables Z that are intervened for generating the n-th sample; therefore, its realization z(n) = z. As an example, Fig. 2 shows a graphical representation of the data-generating process for a ﬁnite dateset

U1

U2

do(z(n) )

U1 (1)

U2 (1)

n = 1, . . . , N

Z

X

Y

Z(1)

X(1)

Y(1)

SCM M

V(1) ∼ P(v)

U1 (2)

U2 (2)

U1 (N)
...

U2 (N)

Z(2)

X(2)

Y(2)

Z(N)

X(N)

Y(N)

V(2) ∼ P(vz = 0)

V(N) ∼ P(vz = 1)

Figure 2: The data-generating process for a ﬁnite dateset {x(n), y(n), z(n)}Nn=1 in an SCM associated with Fig. 1b; the set Z = {∅, z = 0, z = 1} where the idle intervention do(∅) corresponds to the observational distribution.

{x(n), y(n), z(n)}Nn=1 associated with SCMs in Fig. 1b; the intervention set Z = {∅, z = 0, z = 1}.
We ﬁrst introduce effective Markov Chain Monte Carlo (MCMC) algorithms that sample the posterior distribution P (θctf | v¯) over an arbitrary counterfactual probability θctf = P (yx, . . . , zw). For every V ∈ V , ∀paV , uV , endogenous parameters µV(paV ,uV ) are drawn uniformly over the ﬁnite domain ΩV . For every U ∈ U , exogenous parameters θu are drawn from a Dirichlet distribution (Connor and Mosimann 1969). Formally,
(θ1, . . . , θdU ) ∼ Dir αU(1), . . . , αU(dU ) , (11)

where the cardinality dU = V ∈C(U) |ΩPaV → ΩV | and hyperparameters α1(u), . . . , αU(dU ) > 0.
Gibbs sampling is a well-known MCMC algorithm that allows one to sample posterior distributions. We ﬁrst introduce the following notations. Let parameters θ and µ be:
θ = {θu | ∀U ∈ U , ∀u} , µ = µV(paV ,uV ) | ∀V ∈ V , ∀paV , uV . (12)
We denote by U¯ = U (n) | n = 1, . . . , N exogenous variables affecting N endogenous variables V¯ =
V (n) | n = 1, . . . , N ; we use u¯ to represent its realization. Our blocked Gibbs sampler works by iteratively drawing values from the conditional distributions of variables as follows (Ishwaran and James 2001). Detailed derivations of complete conditionals are shown in Appendix B.1.
• Sampling P (u¯ | v¯, θ, µ). Exogenous variables U (n), n = 1, . . . , N , are mutually independent given parameters θ, µ. We could draw each U (n) | θ, µ, V¯ corresponding to the n-th sample induced by do(z(n)) independently. The complete conditional of U (n) is given by

P u(n) | v(n), θ, µ

∝ µ pa(Vn),u(Vn)
v(n) V ∈V \Z(n)

θu.
U ∈U

(13)

• Sampling P (µ, θ | v¯, u¯). Note that parameters µ, θ are mutually independent given V¯ , U¯ . Therefore, we will derive complete conditionals over µ, θ separately.
Consider ﬁrst endogenous parameters µ. For every V ∈ V , ﬁx paV , uV . If there exists an instance n = 1, . . . , N

such that V ∈ Z(n) and pa(Vn) = paV , u(Vn) = uV , the posterior over µV(paV ,uV ) is given by, for ∀v ∈ ΩV ,
P µv(paV ,uV ) = 1 | v¯, u¯ = 1v=v(n) . (14)

Otherwise, µV(paV ,uV ) is drawn uniformly from ΩV .

Consider now exogenous parameters θ. For every U ∈

U , ﬁx u. Let nu =

N n=1

1u(n) =u

be

the

number

of

instances in u(n) equal to u. By the conjugacy of the

Dirichlet distribution, the complete conditional of θu is,

(θ1, . . . , θdU ) ∼ Dir βU(1), . . . , βU(dU ) , (15)
where βU(u) = αU(u) + nu for u = 1, . . . , dU .

Doing so eventually produces values drawn from the posterior distribution over θ, µ, U¯ | V¯ . Given parameters
θ, µ, we compute the counterfactual probability θctf = P (yx, . . . , zw) following the three-step algorithm in (Pearl 2000) which consists of abduction, action, and prediction. Thus computing θctf from each draw θ, µ, U¯ eventually gives us the draw from the posterior distribution P (θctf | v¯).

Collapsed Gibbs Sampling
We also describe an alternative MCMC algorithm that applies to Dirichlet priors in Eq. (11). For n = 1, . . . , N , let U¯−n denote the set difference U¯ \ U (n); similarly, we write V¯−n = V¯ \ V (n). Our collapsed Gibbs sampler ﬁrst iteratively draws values from the conditional distribution over U (n) | V¯ , U¯−n for every n = 1, . . . , N as follows.
• Sampling P u(n) | v¯, u¯−n . At each iteration, draw U (n) from the conditional distribution given by

P u(n) | v¯, u¯−n

∝

P

V ∈V \Z(n)

v(n) | pa(Vn), uV(n), v¯−n, u¯−n

P u(n) | v¯−n, u¯−n .

(16)

U ∈U

Among quantities in the above equation, for every V ∈ V \ Z(n), if there exists an instance i = n such that V ∈ Z(i) and pa(Vi) = pa(Vn), u(Vi) = u(Vn),

P v(n) | pa(Vn), u(Vn), v¯−n, u¯−n = 1 . v(n)=v(i) (17)

Otherwise, the above probability is equal to 1/|ΩV |.
For every U ∈ U , let u¯−n be a set of exogenous samples u(1), . . . , u(N) \ {u(n)}. Let {u∗1, . . . , u∗K } denote K
unique values that samples in u¯−n take on. The conditional distribution over U (n) | V¯−n, U¯−n is given by
as follows, for αU = duU=1 αU(u),

P u(n) | v¯−n, u¯−n

(18)

 n∗k + αU(u∗k) 

 αU + N − 1

=

(u(n) )

  

αU



αU + N − 1

if u(n) = u∗k if u(n) ∈ {u∗1, . . . , u∗K }

where n∗k = i=n 1u(i)=u∗ , for k = 1, . . . , K, records k
the number of values u(i) ∈ u¯−n that are equal to u∗k.

Doing so eventually produces exogenous variables drawn from the posterior distribution of U¯ | V¯ . We then sample parameters from the posterior distribution of θ, µ | U¯ , V¯ ;
complete conditional distributions P (µ, θ | v¯, u¯) are given in Eqs. (14) and (15). Finally, computing θctf from each sample θ, µ gives a draw from the posterior P (θctf | v¯).
When the cardinality dU of exogenous domains is high, the collapsed Gibbs sampler described here is more computational efﬁcient than the blocked sampler, since it does not iteratively draw parameters θ, µ in the high-dimensional space. Instead, the collapsed sampler only draws θ, µ once after samples drawn from the distribution of U¯ | V¯ converge. On the other hand, when the cardinality dU is reasonably low, the blocked Gibbs sampler is preferable since it exhibits better convergence (Ishwaran and James 2001).

Credible Intervals over Counterfactuals
Given a MCMC sampler, one could bound the counterfactual probability θctf by computing credible intervals from the posterior distribution P (θctf | v¯).
Deﬁnition 3. Fix α ∈ [0, 1). A 100(1 − α)% credible interval [lα, rα] for θctf is given by
lα = sup {x | P (θctf ≤ x | v¯) = α/2} , (19) rα = inf {x | P (θctf ≤ x | v¯) = 1 − α/2} .
For a 100(1 − α)% credible interval [lα, rα], any counterfactual probability θctf that is compatible with observational data v¯ lies between the interval lα and rα with probability 1 − α. Credible intervals have been widely applied for computing bounds over counterfactuals provided with ﬁnite observations (Imbens and Manski 2004; Vansteelandt et al. 2006; Romano and Shaikh 2008; Bugni 2010; Todem, Fine, and Peng 2010). Let Nz denote the number of samples in v¯ that are drawn from an interventional distribution P (vz). Assume that the prior distribution over θctf has full support over Borel sets in [0, 1]. It follows from the law of large numbers that the 100% credible interval [l0, r0] converges to the optimal bound [l, r] in Eq. (8) as the sample size Nz grows (to inﬁnite) for all z ∈ Z (Chickering and Pearl 1997).

Algorithm 1: CREDIBLEINTERVAL
1: Input: Credible level α, tolerance level δ, . 2: Output: An credible interval [lα, hα] for θctf. 3: Draw T = 2 −2 ln(4/δ) samples θ(1), . . . , θ(T )
from the posterior distribution P (θctf | v¯). 4: Return interval ˆlα(T ), rˆα(T ) (Eq. (20)).

Let θ(t) Tt=1 be T samples drawn from P (θctf | v¯). One could compute the 100(1 − α)% credible interval for θctf
using following estimators (Sen and Singer 1994):

ˆlα(T ) = θ( (α/2)T ), rˆα(T ) = θ( (1−α/2)T ), (20)

where θ( (α/2)T ), θ( (1−α/2)T ) are the (α/2)T th smallest and the (1 − α/2)T th smallest of θ(t) 3.
Lemma 1. Fix T > 0 and δ ∈ (0, 1). Let function f (T, δ) = 2T −1 ln(4/δ). With probability at least 1 − δ, estimators
ˆlα(T ), rˆα(T ) for any α ∈ [0, 1) is bounded by

lα−f(T,δ) ≤ ˆlα(T ) ≤ lα+f(T,δ), rα+f(T,δ) ≤ rˆα(T ) ≤ rα−f(T,δ).

(21)

We summarize our algorithm, CREDIBLEINTERVAL, in Alg. 1. It takes a credible level α and tolerance levels δ, as inputs. In particular, CREDIBLEINTERVAL repeatedly draw T ≥ 2 −2 ln(4/δ) samples from P (θctf | v¯). It then computes estimates ˆlα(T ), hˆα(T ) from drawn samples following Eq. (20) and return them as the output.
Corollary 1. Fix δ ∈ (0, 1) and > 0. With probability at least 1 − δ, the interval [ˆl, rˆ] = CREDIBLEINTERVAL(α, δ, ) for any α ∈ [0, 1) is bounded by ˆl ∈ [lα− , lα+ ] and rˆ ∈ [rα+ , rα− ].
Corol. 1 implies that any counterfactual probability θctf compatible with the dataset v¯ falls between [ˆl, rˆ] =
CREDIBLEINTERVAL(α, δ, ) with P θctf ∈ [ˆl, rˆ] | v¯ ≈
1 − α ± . As the tolerance rate → 0, [ˆl, rˆ] converges to a 100(1 − α)% credible interval with high probability.

Simulations and Experiments
We demonstrate our algorithms on various synthetic and real datasets in different causal diagrams. Overall, we found that simulation results support our ﬁndings and the proposed bounding strategy consistently dominates state-of-art algorithms. When target probabilities are identiﬁable (Experiment 1), our bounds collapse to the actual counterfactual probabilities. For non-identiﬁable settings, our algorithm obtains sharp asymptotic bounds when the closed-form solutions already exist (Experiments 2); and obtains novel counterfactual bounds in other more general cases which consistently improve over existing strategies (Experiment 3 & 4).
3For any real α ∈ R, let α denote the smallest integer n ∈ Z larger than α, i.e., α = min{n ∈ Z | n ≥ α}.

(a) Frontdoor

(b) PNS

(c) IST

(d) Obs. + Exp.

Figure 3: Simulation results for Experiments 1-4. For all plots (a - d), ci represents our proposed algorithm; θ∗ is the actual counterfactual probability; opt is the optimal asymptotic bounds (if exists); nb stands for the natural bounds (Manski 1990).

In all experiments, we evaluate our proposed strategy using credible intervals (ci). We draw at least 4 × 103 samples from the posterior distribution P (θctf | v¯) over the target counterfactual. This allows us to compute 100% credible interval over θctf within error = 0.05, with probability at least 1 − δ = 0.95. As the baseline, we include the actual counterfactual probability θ∗. We refer readers to Appendix C for more details on simulations and additional experiments with other causal diagrams and datasets.
Experiment 1: Frontdoor Graph Consider the “Frontdoor” graph described in Fig. 1c where X, Y, W are binary variables in {0, 1}; U1, U2 ∈ R. In this case, any interventional probability P (yx) is identiﬁable from the observational distribution P (X, W, Y ) through the frontdoor adjustment (Pearl 2000, Thm. 3.3.4). We collect N = 104 observational samples v¯ = {x(n), y(n), w(n)}Nn=1 from a synthetic SCM instance. Fig. 3a shows samples drawn from the posterior distribution (P (Yx=0 = 1) | v¯). The analysis reveals that these samples collapse to the actual interventional probability P (Yx=0 = 1) = 0.5085, which conﬁrms the identiﬁability of P (yx) in the “frontdoor” graph.
Experiment 2: Probability of Necessity and Sufﬁciency (PNS) Consider the “Bow” diagram in Fig. 1d where X, Y ∈ {0, 1} and U ∈ R. We study the problem of evaluating the probability of necessity and sufﬁciency P (Yx=1 = 1, Yx=0 = 0) from the observational distribution P (X, Y ). The sharp bound for P (Yx=1 = 1, Yx=0 = 0) from P (X, Y ) was introduced in (Tian and Pearl 2000) (labelled as opt). We collect N = 103 observational samples v¯ = {x(n), y(n)}Nn=1 from a randomly generated SCM instance. Fig. 3c shows samples drawn from the posterior distribution over (P (Yx=1 = 1, Yx=0 = 0) | v¯). The analysis reveals that the 100% credible interval (ci) matches the optimal PNS bound l = 0, r = 0.6775 over the actual counterfactual probability P (Yx=1 = 1, Yx=0 = 0) = 0.1867.
Experiment 3: International Stroke Trials (IST) IST was a large, randomized, open trial of up to 14 days of antithrombotic therapy after stroke onset (Carolei et al. 1997). In particular, the treatment X is a pair (i, j) where i ∈ {0, 1} stands for aspirin allocation; j ∈ {0, 1, 2} stands for heparin allocation. The primary outcome Y ∈ {0, . . . , 3} is the health of the patient 6 months after the treatment. To emulate the presence of unobserved confounding, we ﬁlter

the experimental data following a procedure in (Kallus and Zhou 2018). Doing so allows us to obtain N = 103 synthetic observational samples v¯ = {x(n), y(n), z(n)}Nn=1 that are compatible with the “IV” diagram of Fig. 1a where Z ∈ {0, . . . , 9}. We are interested in evaluating the treatment effect E[Yx=(1,0)] for only assigning aspirin X = (1, 0). As a baseline, we also include the natural bound (Manski 1990) estimated at the 95% conﬁdence level (nb) (Zhang and Bareinboim 2021). The analysis (Fig. 3c) reveals that both algorithms achieve effective bounds containing target causal effect E[Yx=(1,0)] = 1.3418. The 100% credible interval is lci = 0.4363, rci = 2.3162, which improves over the existing strategy (lnb = 0.3050, rnb = 2.8686).
Experiment 4: Obs. + Exp. Consider the causal diagram in Fig. 1b where X, Y, Z ∈ {0, . . . , 9} and U1, U2 ∈ R. We are interested in evaluating counterfactual probabilities P (z, xz , yx ) from the observational distribution P (X, Y, Z) and a collection of interventional distributions P (Xz, Yz) induced by interventions do(z) for z = 0, . . . , 9. We collect N = 103 samples v¯ = {x(n), y(n), z(n)}Nn=1 from a SCM instance of Fig. 1b where each sample X(n), Y (n), Z(n) is an independent draw from P (X, Y, Z) or P (Xz, Yz). To address the challenge of the highdimensional exogenous domains, we apply the proposed collapsed Gibbs sampler to obtain samples from the posterior distribution (P (Z + Xz=0 + Yx=0 ≥ 14) | v¯). Simulation results are shown in Fig. 3d. The analysis reveals that our proposed approach is able to achieve an effective bound that contains the actual counterfactual probability P (Z + Xz=0 + Yx=0 ≥ 14) = 0.6378. The 100% credible interval (ci) is equal to l = 0.4949, r = 0.8482. To our best knowledge, no existing strategy is applicable for this setting.
Conclusion
This paper investigated the problem of partial identiﬁcation of counterfactual distributions, which concerns with bounding counterfactual probabilities from an arbitrary combination of observational and experimental data, provided with a causal diagram encoding qualitative assumptions about the data-generating process. We introduced a special parametric family of SCMs with discrete exogenous variables, taking values from a ﬁnite set of unobserved states, and showed that it could represent all counterfactual distributions (over

ﬁnite observed variables) in any causal diagram. Using this result, we reduced the partial identiﬁcation problem into a polynomial program and developed a novel algorithm to approximate the optimal asymptotic bounds over target counterfactual probabilities from ﬁnite samples obtained through arbitrary observations and experiments.
References
Avin, C.; Shpitser, I.; and Pearl, J. 2005. Identiﬁability of Path-Speciﬁc Effects. In Proceedings of the Nineteenth International Joint Conference on Artiﬁcial Intelligence IJCAI-05, 357–363. Edinburgh, UK: Morgan-Kaufmann Publishers.
Balke, A.; and Pearl, J. 1994. Counterfactual Probabilities: Computational Methods, Bounds, and Applications. In de Mantaras, R. L.; and Poole, D., eds., Uncertainty in Artiﬁcial Intelligence 10, 46–54. San Mateo, CA: Morgan Kaufmann.
Balke, A.; and Pearl, J. 1997. Bounds on treatment effects from studies with imperfect compliance. Journal of the American Statistical Association, 92(439): 1172–1176.
Bareinboim, E.; and Pearl, J. 2012. Causal inference by surrogate experiments: z-identiﬁability. In de Freitas, N.; and Murphy, K., eds., Proceedings of the Twenty-Eighth Conference on Uncertainty in Artiﬁcial Intelligence, 113–120. Corvallis, OR: AUAI Press.
Bauer, H. 1972. Probability theory and elements of measure theory. Holt.
Blackwell, D. A.; and Girshick, M. A. 1979. Theory of games and statistical decisions. Courier Corporation.
Bugni, F. A. 2010. Bootstrap inference in partially identiﬁed models deﬁned by moment inequalities: Coverage of the identiﬁed set. Econometrica, 78(2): 735–753. Carathe´odory, C. 1911. U¨ ber den Variabilita¨tsbereich der Fourier’schen Konstanten von positiven harmonischen Funktionen. Rendiconti Del Circolo Matematico di Palermo (1884-1940), 32(1): 193–217.
Carolei, A.; et al. 1997. The International Stroke Trial (IST): a randomized trial of aspirin, subcutaneous heparin, both, or neither among 19435 patients with acute ischaemic stroke. The Lancet, 349: 1569–1581.
Chickering, D.; and Pearl, J. 1997. A clinician’s tool for analyzing non-compliance. Computing Science and Statistics, 29(2): 424–431.
Connor, R. J.; and Mosimann, J. E. 1969. Concepts of independence for proportions with a generalization of the Dirichlet distribution. Journal of the American Statistical Association, 64(325): 194–206.
Correa, J.; Lee, S.; and Bareinboim, E. 2021. Nested counterfactual identiﬁcation from arbitrary surrogate experiments. In In Advances in Neural Information Processing Systems. Forthcoming.
Durrett, R. 2019. Probability: theory and examples, volume 49. Cambridge university press.
Evans, R. J. 2012. Graphical methods for inequality constraints in marginalized DAGs. In 2012 IEEE International

Workshop on Machine Learning for Signal Processing, 1–6. IEEE.
Evans, R. J.; et al. 2018. Margins of discrete Bayesian networks. The Annals of Statistics, 46(6A): 2623–2656.
Finkelstein, N.; and Shpitser, I. 2020. Deriving Bounds and Inequality Constraints Using Logical Relations Among Counterfactuals. In Conference on Uncertainty in Artiﬁcial Intelligence, 1348–1357. PMLR.
Frangakis, C.; and Rubin, D. 2002. Principal Stratiﬁcation in Causal Inference. Biometrics, 1(58): 21–29.
Galles, D.; and Pearl, J. 1998. An axiomatic characterization of causal counterfactuals. Foundation of Science, 3(1): 151– 182.
Halpern, J. 1998. Axiomatizing Causal Reasoning. In Cooper, G.; and Moral, S., eds., Uncertainty in Artiﬁcial Intelligence, 202–210. San Francisco, CA: Morgan Kaufmann. Also, Journal of Artiﬁcial Intelligence Research 12:3, 17– 37, 2000.
Imbens, G. W.; and Manski, C. F. 2004. Conﬁdence intervals for partially identiﬁed parameters. Econometrica, 72(6): 1845–1857.
Imbens, G. W.; and Rubin, D. B. 1997. Bayesian inference for causal effects in randomized experiments with noncompliance. The annals of statistics, 305–327.
Ishwaran, H.; and James, L. F. 2001. Gibbs sampling methods for stick-breaking priors. Journal of the American Statistical Association, 96(453): 161–173.
Kallus, N.; and Zhou, A. 2018. Confounding-robust policy improvement. In Advances in neural information processing systems, 9269–9279.
Kallus, N.; and Zhou, A. 2020. Confounding-robust policy evaluation in inﬁnite-horizon reinforcement learning. Advances in Neural Information Processing Systems.
Kilbertus, N.; Kusner, M. J.; and Silva, R. 2020. A Class of Algorithms for General Instrumental Variable Models. In Advances in Neural Information Processing Systems.
Lasserre, J. B. 2001. Global optimization with polynomials and the problem of moments. SIAM Journal on optimization, 11(3): 796–817.
Lewis, H. R. 1983. Computers and intractability. A guide to the theory of NP-completeness.
Manski, C. 1990. Nonparametric bounds on treatment effects. American Economic Review, Papers and Proceedings, 80: 319–323.
Parrilo, P. A. 2003. Semideﬁnite programming relaxations for semialgebraic problems. Mathematical programming, 96(2): 293–320.
Pearl, J. 1995. Causal diagrams for empirical research. Biometrika, 82(4): 669–710.
Pearl, J. 2000. Causality: Models, Reasoning, and Inference. New York: Cambridge University Press. 2nd edition, 2009.
Pearl, J. 2011. Principal Stratiﬁcation – A goal or a tool? The International Journal of Biostatistics, 7(1). Article 20, DOI: 10.2202/1557-4679.1322. Available at: <http://ftp.cs.ucla.edu/pub/stat ser/r382.pdf>.

Richardson, A.; Hudgens, M. G.; Gilbert, P. B.; and Fine, J. P. 2014. Nonparametric bounds and sensitivity analysis of treatment effects. Statistical science: a review journal of the Institute of Mathematical Statistics, 29(4): 596.
Robins, J. 1989. The analysis of randomized and nonrandomized AIDS treatment trials using a new approach to causal inference in longitudinal studies. In Sechrest, L.; Freeman, H.; and Mulley, A., eds., Health Service Research Methodology: A Focus on AIDS, 113–159. Washington, D.C.: NCHSR, U.S. Public Health Service.
Romano, J. P.; and Shaikh, A. M. 2008. Inference for identiﬁable parameters in partially identiﬁed econometric models. Journal of Statistical Planning and Inference, 138(9): 2786– 2807.
Rosset, D.; Gisin, N.; and Wolfe, E. 2018. Universal bound on the cardinality of local hidden variables in networks. Quantum Information & Computation, 18(11-12): 910–926.
Rubin, H.; and Wesler, O. 1958. A note on convexity in Euclidean n-space. Proceedings of the American Mathematical Society, 9(4): 522–523.
Sen, P. K.; and Singer, J. M. 1994. Large sample methods in statistics: an introduction with applications, volume 25. CRC press.
Shpitser, I.; and Pearl, J. 2007. What Counterfactuals Can Be Tested. In Proceedings of the Twenty-Third Conference on Uncertainty in Artiﬁcial Intelligence, 352–359. Vancouver, BC, Canada: AUAI Press. Also, Journal of Machine Learning Research, 9:1941–1979, 2008.
Shpitser, I.; and Sherman, E. 2018. Identiﬁcation of Personalized Effects Associated With Causal Pathways. In UAI.
Tian, J.; and Pearl, J. 2000. Probabilities of causation: Bounds and identiﬁcation. Annals of Mathematics and Artiﬁcial Intelligence, 28: 287–313.
Tian, J.; and Pearl, J. 2002. A general identiﬁcation condition for causal effects. In Proceedings of the Eighteenth National Conference on Artiﬁcial Intelligence, 567–573. Menlo Park, CA: AAAI Press/The MIT Press.
Todem, D.; Fine, J.; and Peng, L. 2010. A global sensitivity test for evaluating statistical hypotheses with nonidentiﬁable models. Biometrics, 66(2): 558–566.
Vansteelandt, S.; Goetghebeur, E.; Kenward, M. G.; and Molenberghs, G. 2006. Ignorance and uncertainty regions as inferential tools in a sensitivity analysis. Statistica Sinica, 953–979.
Zhang, J.; and Bareinboim, E. 2021. Bounding Causal Effects on Continuous Outcomes. In Proceedings of the 35nd AAAI Conference on Artiﬁcial Intelligence.

A. On the Expressive Power of Canonical
Structural Causal Models
In this section, we provide the proof for Thm. 1 which establishes the expressive power of discrete SCMs in representing counterfactual distributions in an arbitrary causal diagram containing observed variables with ﬁnite domains.
Recall that M (G) and N (G) in Thm. 1 are collections of all SCMs and discrete SCMs (thereafter, canonical) compatible with a causal diagram G respectively. Since N (G) ⊂ M (G), the reverse direction of Eq. (5) is self-evident. The main challenge here is to prove the other direction. That is, given an SCM M ∈ M (G) with arbitrary exogenous domains, we want to construct a discrete SCM N ∈ N (G) with ﬁnite exogenous domains such that N and M are both compatible with the same causal diagram G and induces the same set of counterfactual distributions P ∗.
To illustrate the idea of this constructive proof, consider as an example the “Bow” graph in Fig. 1d where X, Y are binary variables in {0, 1}; the exogenous variable U takes values in the real numbers R. Let domain ΩX be ordered by h(X1) = 0 and h(X2) = 1. We denote by ΩX → ΩY the set of all functions mapping from domains of X to Y , i.e.,

h(Y1)(x) = 0, h(Y2)(x) = x, (22) h(Y3)(x) = ¬x, h(Y4)(x) = 1.

Consider the following families of SCMs:
1. M is the set of all SCMs compatible with the “Bow” graph in Fig. 1d.
2. N is the set of all discrete SCMs compatible with the “Bow” graph in Fig. 1d with the cardinality |ΩU | = 8.
Our goal is to prove that M and N are counterfactually equivalent for binary X, Y ∈ {0, 1}. Since intervening on Y has no causal effect on X (Galles and Pearl 1998), it is sufﬁcient to show that for any SCM M ∈ M , one could construct a discrete SCM N ∈ N so that

PM (x, yx=0, yx=1) = PN (x, yx=0, yx=1) .

(23)

The construction procedure is described as follows. Let the exogenous variable U in N be a pair (UX , UY ) where UX ∈ {1, 2} and UY ∈ {1, . . . , 4}. Values of X and Y are given by the following functions, respectively,

x ← h(XuX ),

y ← h(YuY )(x). (24)

It is veriﬁable that in such N , the counterfactual distribution P (x, yx=0, yx=1) is given by, for ∀i, j, k ∈ {0, 1},

PN (X = i, Yx=0 = j, Yx=1 = k) (25)
= PN (UX = i + 1, UY = 2j + k + 1).

For any SCM M ∈ M, we deﬁne the exogenous distribution PN (uX , uY ) of the discrete SCM N as, for ∀i, j, k ∈ {0, 1},

PN (UX = i + 1, UY = 2j + k + 1) (26)
= PM (X = i, Yx=0 = j, Yx=1 = k).

It follows immediately from Eqs. (25) and (26) that M and N induce the same counterfactual distribution

P (x, yx=0, yx=1), i.e., the condition in Eq. (23) holds. This means that when inferring counterfactual distributions in the “Bow” graph of Fig. 1d with binary X, Y , we could assume that the exogenous variable U is discrete and takes values in the domain {1, . . . , 8}, without loss of generality.
Our goal is to generalize the construction described above to any SCMs compatible with an arbitrary causal diagram. The remainder of this section is organized as follows. In Appendix A.1, we introduce a general canonical partitioning (Balke and Pearl 1994) over exogenous domains for any SCMs with discrete endogenous variables. This allows us to write counterfactual distributions as functions of products of probabilities assigned to intersections of canonical partitions in every c-component. Appendix A.2 shows that probabilities over canonical partitions could be represented using discrete exogenous variables taking values in ﬁnite domains. This allows us to prove Thm. 1 for any causal diagram in the theoretical framework of measure-theoretic probability. Finally, we describe in Appendix A.3 a more ﬁne-grained decomposition for canonical partitions, which provides intuitive explanations for the discretization procedure.

A.1 Canonical Partitions of Exogenous Domains
For every endogenous variable V ∈ V , let ΩPAV → ΩV denote the hypothesis class containing all functions mapping from domains of PAV to V . Since V are discrete variables with ﬁnite domains, the cardinality of the class ΩPAV → ΩV must be also ﬁnite. Given any conﬁguration UV = uV , the induced function fV (·, uV ) must correspond to a unique element in the hypothesis class ΩPAV → ΩV . Such mappings lead to a ﬁnite partition over the exogenous domain ΩUV .
Deﬁnition 4. For an SCM M = V , U , F , P , for every V ∈ V , let functions in ΩPAV → ΩV be ordered by h(Vi) | i ∈ IV where IV = {1, . . . , mV }, mV =
|ΩPAV → ΩV |. A equivalence class UV(i) for function h(Vi), i = 1, . . . , mV , is a subset in ΩUV such that
UV(i) = uV ∈ ΩUV | fV (·, uV ) = h(Vi) . (27)

Deﬁnition 5 (Canonical Partition). For an SCM M = V , U , F , P , UV(i) | i ∈ IV is the canonical partition
over exogenous domain ΩUV for every V ∈ V .
Def. 5 extends the canonical partition in (Balke and Pearl 1994) which was designed for binary variables X, Y, Z ∈ {0, 1} in the “IV” diagram of Fig. 1a.
As exogenous variables UV vary along its domain, regardless of how complex the variation is, its only effect is to switch the functional relationship between PaV and V among elements in class ΩPAV → ΩV . Formally,
Lemma 2. For an SCM M = V , U , F , P , for each V ∈ V , function fV ∈ F could be decomposed as:

fV (paV , uV ) =

hV(i)(paV )1u . ∈U(i)

VV

i∈IV

(28)

Proof. By the deﬁnition of canonical partitions (Def. 5), for every i = 1, . . . , mV , ﬁx any uV ∈ UV(i).

We must have fV (·, uV ) = h(Vi)(·). This implies fV (paV , uV ) = h(Vi)(paV ) for any PAV = paV . Recall that
UV(i) | i = 1, . . . , mV forms a partition over exogenous
domains ΩUV . Given the same paV , uV , the r.h.s. of Eq. (28) must equate to h(Vi)(paV ), which completes the proof.

As an example, consider an SCM M associated with the “IV” graph of Fig. 1a where X, Y, Z are binary variables contained in {0, 1}; U1, U2 are continuous variables drawn uniformly from the interval [0, 3]. Values of X, Y, Z are decided by functions deﬁned as follows, respectively,

x ← fX (z, u2) = 1z≤u2≤z+2,

y ← fY (x, u2) = 1u2<x + 1u2>x+2,

(29)

z ← fZ (u1) = 1u1≤1.5,

We show in Fig. 4 the graphical representation of canonical partitions induced by functions fX , fY and fZ respectively. A detailed description is provided in Table 1. It follows from the decomposition of Lem. 2 that functions fX , fY , fZ in Eq. (29) could be written as follows:

fX (z, u2) = 1u2∈[0,1)¬z + 1u2∈[1,2]1 + 1u2∈(2,3]z fY (x, u2) = 1u2∈[0,1)x + 1u2∈[1,2]0 + 1u2∈(2,3]¬x,
fZ (u1) = 1u1∈[0,1.5]1 + 1u1∈(1.5,3]0.

Let I denote the product of indexing sets V ∈V IV . For any index i ∈ I, we use iV to represent the element in i restricted to V ∈ V . We omit the subscript V when it is obvious; therefore, UV(i) = UV(iV ), h(Vi) = hV(iV ). Our next result establishes a universal decomposition of counterfactual dis-
tributions in any SCM using canonical partitions.
Lemma 3. For an SCM M = V , U , F , P , for any Y , . . . , Z, X, . . . , W ⊆ V 4,

P (yx, . . . , zw)

=

1Yx(i)=y,...,Zw(i)=z P

UV(i) ,

i∈I

V ∈V

(30)

where variables of the form Yx(i) = {Yx(i) | ∀Y ∈ Y }; every Yx(i) is recursively deﬁned as:

xY

if Y ∈ X

Yx(i) = h(Yi) ((PAY )x (i)) otherwise (31)

Proof. We will ﬁrst prove the following claims: for arbitrary subsets Y , X ⊆ V , for any u, x, y,

1Yx(u)=y =

1Yx (i)=y

1 . uV ∈UV(i)

i∈I

V ∈V

(32)

Let GX be a subgraph obtained from the causal diagram G by removing all incoming arrows of X. We will prove Eq. (32) by induction on n = maxY ∈Y An(Y )GX .

4For an arbitrary subset U ⊆ ΩU , we will consistently use P (U) as a shorthand for the probability P (U ∈ U ).

UX(3) x ← ¬z

UX(4) x←1

UX(2) x←z

0

1

2

3 u2

(a) x ← fX (z, u2)

UY(2) y←x

UY(1) y←0

UY(3) y ← ¬x

0

1

2

3 u2

(b) y ← fY (x, u2)

UZ(2) z←1

UZ(1) z←0

0

1.5

3 u1

(c) z ← fZ (u1)

Figure 4: Canonical partitions for exogenous domains over U1, U2 induced by functions of X, Y, Z deﬁned in Eq. (29).

Z =0 Z =1

0 ≤ U2 < 1 X =1 X =0

1 ≤ U2 ≤ 2 X =1 X =1

(a) x ← fX (z, u2)

2 < U2 ≤ 3 X =0 X =1

X =0 X =1

0 ≤ U2 < 1 Y =0 Y =1

1 ≤ U2 ≤ 2 Y =0 Y =0

(b) y ← fY (x, u2)

2 < U2 ≤ 3 Y =1 Y =0

0 ≤ U1 < 1.5 1.5 ≤ U1 ≤ 3

Z =1

Z =0

(c) z ← fZ (u1)

Table 1: Canonical partitions for exogenous domains over U1, U2 induced by functions of X, Y, Z deﬁned in Eq. (29).

Base Case n = 1. Recall that an intervention do(x) set values of variables X as constants x. For any Y ∈ X ∩ Y , let xY be the values assigned to Y in x. It is veriﬁable that

1Yx(u)=y = 1y=xY

(33)

As for every variable Y ∈ Y \ X, we must have its parent nodes PAY = ∅ since n = 1. This implies

1Yx(u)=y = 1fY (uY )=y =

1 1 h(Yi)=y uY ∈UY(i)

i∈IY

(34)

The last step follows from the decomposition in Lem. 2. Eqs. (33) and (34) together imply that

1Yx (u)=y

= 1 1 1 y=xY

h(Yi) =y

uV ∈UV(i)

i∈I Y ∈Y ∩X

Y ∈(Y \X)

V ∈V

=

1Yx (i)=y

1 . uV ∈UV(i)

i∈I

V ∈V

The last step follows from the deﬁnition of variables Yx(i) in Eq. (31) given an index i ∈ I.
Induction Case n = k + 1. Assume that Eq. (32) holds for n = k. We will prove for the case n = k + 1. For every Y ∈ X ∩ Y , 1Yx(u)=y is given in Eq. (33). For every Y ∈ Y \ X, the decomposition in Lem. 2 implies:
1Yx (u)=y
= 1fY ((PAY )x(u),uY )=y

=1

y=

h(Yi)((PAY )x (u))1u ∈U(i)

YY

i∈IY

= 1 1 i∈IY h(Yi)((PAY )x(u))=y uY ∈UY(i)

= 1 1 1 . i∈IY paY h(Yi)(paY )=y (PAY )x(u)=paY uY ∈UY(i)
The last step hold by conditioning on events (PAY )x (u) = paY , ∀paY ∈ ΩPAY . Since we assume Eq. (32) holds for Case n = k, the above equation could be further written as

1 = 1 1 Yx(u)=y i∈IY paY h(Yi)(paY )=y uY ∈UY(i)

·

1(PAY )x(u)=paY

1uV ∈UV(i)

i∈I

V ∈V

A few simpliﬁcation gives:

1Yx (u)=y

= 1 1 1 i∈I paY

h(Yi)(paY )=y (PAY )x(u)=paY V ∈V

uV ∈UV(i)

= 1h(Yi)((PAY )x(u))=y

1 . uV ∈UV(i)

i∈I

V ∈V

Eqs. (33) and (35) together imply that

1Yx (u)=y



(35) 

=

1y=xY

1h(Yi)((PAY )x(u))=y 

i∈I Y ∈Y ∩X

Y ∈(Y \X)

· 1uV ∈UV(i)
V ∈V

=

1Yx (i)=y

1 . uV ∈UV(i)

i∈I

V ∈V

Again, the last step follows from the deﬁnition of variables Yx(i) in Eq. (31) given an index i ∈ I.
We are now ready to prove Eq. (30). The statement of Eq. (32) implies that for any Y , . . . , Z, X, . . . , W ⊆ V ,

P (yx, . . . , zw)

=

1Yx(u)=y,...,Zw(u)=z dP (u)

ΩU

=

1Yx (i)=y

1u ∈U (i) ∧

ΩU i∈I

V ∈V V V

···∧

1Zw (i)=z

1uV ∈UV(i)

i∈I

V ∈V

dP (u)

Simplifying the above equation gives: P (yx, . . . , zw)

=

1Yx(i)=y ∧ · · · ∧ 1Zw(i)=z

1u ∈U(i) dP (u)

ΩU i∈I

V ∈V V V

= 1Yx(i)=y ∧ · · · ∧ 1Zw(i)=z

1u ∈U(i) dP (u)

i∈I

ΩU V ∈V V V

=

1Yx(i)=y,...,Zw(i)=z P

UV(i) .

i∈I

V ∈V

In the above equations, the last two steps hold since variables Yx(i), . . . , Zw(i) are not functions of exogenous variables U . This completes the proof.

Let C(G) denote the collection of all maximal ccomponents (Def. 2) in a causal diagram G. For instance, in the “IV” diagram G of Fig. 1a, C(G) contains c-components C(U1) = {Z}, C(U2) = {X, Y }. The following proposition shows that probabilities over canonical partitions factorize over c-components in a causal diagram.
Lemma 4. For an SCM M = V , U , F , P , let G be the associated causal diagram. For any i ∈ I,

P UV(i) = P UV(i) .

V ∈V

C∈C(G)

V ∈C

(36)

Proof. For any c-compoment C ∈ C(G), let UC = ∪V ∈C UV the set of exogenous variables affecting (at least one of) endogenous variables in C. By the deﬁnition of
c-components (Def. 2), it is veriﬁable that for two differ-
ent c-compoments C1, C2 ∈ C(G), their corresponding exogenous variables UC1 , UC2 do not share any element, i.e., UC1 ∩ UC2 = ∅. We complete the proof by noting that exogenous variables in U are mutually independent.

As an example, consider again the SCM M compatible
with Fig. 1a deﬁned in Eq. (29). The event Z = 1, Xz=0 = 1, Yx=1 = 0 occurs if any only if U1 ∈ UZ(2) and U2 ∈
UX(3) ∪ UX(4) ∩ UY(1) ∪ UY(3) . This implies

P (Z = 1, Xz=0 = 1, Yx=1 = 0) = P UZ(2) ∩ (UX(3) ∪ UX(4)) ∩ (UY(1) ∪ UY(3)) = P UZ(2) P UX(3) ∪ UX(4)) ∩ (UY(1) ∪ UY(3)) .

The last step holds since {Z} and {X, Y } are two different c-components. It is veriﬁable from Fig. 4 that UZ(2) = {u1 ∈ [0, 1.5]}, UX(3) ∪ UX(4) ∩ UY(1) ∪ UY(3) = {u2 ∈ [1, 2]}. The above equation could be further written as:
P (Z = 1, Xz=0 = 1, Yx=1 = 0) 1
= P (U1 ∈ [0, 1.5]) P (U2 ∈ [1, 2]) = 6 .
The last step follows since variables U1, U2 are drawn uniformly at random over the interval [0, 3].

A.2 Bounding Cardinalities of Exogenous Domains
Lems. 3 and 4 together allow us to write any counterfactual
distribution in an SCM as a function of products of probabil-
ities assigned to the intersections of canonical partitions in
every c-component. To prove the counterfactual equivalence
in Thm. 1, it is thus sufﬁcient to construct a canonical SCM N from an arbitrary SCM M such that (1) M, N are compatible with the same causal diagram G; and (2) M, N generate
the same probabilities over canonical partitions. This section
will describe how to construct such a discrete SCM.
We start the discussion by introducing some necessary no-
tations and concepts. The probability distribution for every exogenous variable U ∈ U is characterized with a probability space. It is frequently designated ΩU , FU , PU where ΩU is a sample space containing all possible outcomes; FU is a σ-algebra containing subsets of ΩU ; PU is a probability measure on FU normalized by PU (ΩU ) = 1. Elements of FU are called events, which are closed under operations of set complement and unions of countably many sets. By means of PU , a real number PU (A) ∈ [0, 1] is assigned to every event A ∈ FU ; it is called the probability of event A.
For an arbitrary set of exogenous variables U , its realization U = u is an element in the Cartesian prod-
uct ×U∈U ΩU , represented by a sequence (u)U∈U . If now
AU ∈ ΩU , ∀U ∈ U , we may be interested in inferring whether a sequence of events U ∈ AU for every U ∈ U occurs. Such an event is represented by a sub-
set ×U∈U AU ⊆ ×U∈U ΩU . The products ×U∈U AU with
AU running through FU generate precisely the product σalgebra U∈U FU . The product measure U∈U PU is the only probability measure P with restrictions to U∈U FU that satisﬁes the following consistency condition

× P

AU =

PU (AU ) ,

(37)

U ∈U

U ∈U

for arbitrary AU ∈ FU . It is obvious that P is a probability measure. Consequently,

× ΩU , FU , PU

U ∈U

U ∈U

U ∈U

(38)

deﬁnes the product of probability spaces ΩU , FU , PU , U ∈ U . It is adequate to describe all “measurable events”
occurring to exogenous variables U .
Recall that for subsets X, Y ⊆ V , counterfactual random
variables (or potential responses) Yx(u) is deﬁned as the solution of Y in the submodel Mx induced by intervention do(x) given the conﬁguration U = u. For any y ∈ ΩY , let the inverse image Yx−1(y) be the set of values u generating the event Yx(u) = y, i.e.,

Yx−1(y) = {u ∈ ΩU | Yx(u) = y} .

(39)

Evidently, we are dealing with a U∈U FU -measurable mapping Yx : u → y. Because of this measurability, the inverse image Yx−1(y) is an event in U∈U FU for any realization y. Thus P Yx−1(y) is deﬁned as the probability of Yx taking on a value y. Similarly, for any subsets Y , . . . , Z,

X, . . . , W ⊆ V , the probability of a sequence of counterfactual events Yx = y, . . . , Zw = z is deﬁned as:
P (yx, . . . , zw) = P Yx−1(y) ∩ · · · ∩ Zw−1(z) .

We refer readers to (Durrett 2019; Bauer 1972) for a detailed
discussion on measure-theoretic probability concepts.
For a c-component C in a causal diagram G, we denote by UC = ∪V ∈C UV the union of exogenous variables UV affecting an endogenous variable V for every V ∈ C. Let exogenous variables in UC be ordered by U1, . . . , Um, m = |UC |. For convenience, we consistently write Ωi, Fi, Pi as the probability space of Ui, i = 1, . . . , m. The product of these probability spaces is thus written as

×m

m

m

Ωi, Fi, Pi .

i=1

i=1

i=1

(40)

For any SCM M compatible with the diagram G, the joint distribution over events deﬁned by canonical partitions UV(i) associated with variables V ∈ C is given by

m

P UV(i) =

1u d ∈U (i)

Pi .

V ∈C

×m i=1

Ωi

V

∈C

V

V

i=1

(41)

Our goal is to show that all correlations among events UV(i), V ∈ V , induced by exogenous variables described by arbitrary probability spaces could be produced by a “simpler” generative process with discrete exogenous domains.
Lemma 5. Any distribution P V ∈C UV(i) in Eq. (41) could be reproduced with a generic model of the form:

md

m

P UV(i) =

1u ∈U(i) P (uj ),

VV

V ∈C

j=1 uj =1 V ∈C

j=1

(42)

where every exogenous variable Uj ∈ U takes values in a ﬁnite domain {1, . . . , d}, d = V ∈C |ΩPAV → ΩV |.
(Rosset, Gisin, and Wolfe 2018, Prop. 2) applied a classic result of Carathe´odory theorem in convex geometry (Carathe´odory 1911) and showed that the observational distribution in any causal diagram could be generated using discrete exogenous variables, assuming that exogenous variables are drawn from distributions characterized with welldeﬁned probability density functions. We here present a constructive proof that applies to the general framework of measure-theoretic probability theory.

Proof of Lemma 5. Let P be a vector representing probabil-

ities of P

U (i)

. Note that for every V ∈ V ,

V ∈C V

i∈I

there are |ΩPAV → ΩV | equivalence classes UV(i). P is thus a vector with d = V ∈C |ΩPAV → ΩV | elements. Since

i P V ∈C UV(i) = 1, it only takes a vector with d − 1

dimensions to uniquely determine P . We could thus see P

as a point in the (d − 1)-dimensional real space. Similarly, P , 1 is vector in Rd where the d-th element is equal to 1
Fix an exogenous variable U1 ∈ UC. We deﬁne function Pu1 V ∈C UV(i) as the distribution over canonical partitions when U1 is ﬁxed as a constant u1 ∈ Ω1. That is,

Pu1 
=

UV(i)
V ∈C





m

1u ∈U(i) d  Pj 

×m j=2

Ωi

V

∈C

V

V

j=2

U1 =u1

(43)

The associativity of the product of probability spaces (Bauer 1972, Ch. 3.3) generally implies:





m

m

Fj = F1 ⊗  Fj ,

j=1

j=2





m

m

(44)

Pj = P1 ⊗  Pj .

j=1

j=2

Let Pu1 be a vector in Rd−1 representing probabilities of Pu1 V ∈C UV(i) and let Pu1 , 1 be vector in Rd where the d-th element is equal to 1. Applying Fubini’s Theorem (Durrett 2019, Thm. 1.7.2) implies that function u1 →
Pu1 , 1 is F1-measurable. That is, Ω1, F1, P1 yields a

probability measure for a set Pu1 , 1 | ∀u1 ∈ Ω1 with respective to Borel sets in real space Rd with average

P,1 =

Pu1 , 1 dP1.

(45)

Ω1

It can be shown that the probability vector P , 1 is a point

lying in the convex hull of a set Pu1 , 1 | ∀u ∈ ΩU (see
(Blackwell and Girshick 1979, Thm. 2.4.1) and its extension to arbitrary probability measures in (Rubin and Wesler 1958)). This means that there exists a ﬁnite set of vectors

Pu(1) , 1 , . . . , Pu(n) , 1 and a sequence of positive coef-

1

1

ﬁcients α1, . . . , αn > 0 such that

n

P , 1 = αk Pu(k) , 1 .

(46)

1

k=1

The above equation implies

n

P=

αkPu(k) , 1

k=1

n
and αk = 1 (47)
k=1

Indeed, we could further reduce the number of coefﬁcients n by removing linearly dependent vectors. If vectors

Pu(k) , 1 are not linearly independent, there exists a non1

trivial solution λ1, . . . λn such that k λk Pu(k) , 1 = 0. 1

It is veriﬁable that for any real value β > 0

n

(αk − βλk) Pu(k) , 1 1
k=1

n

n

= αk Pu(k) , 1 − β λk 1

k=1

k=1

n

= αk Pu(k) , 1 . 1 k=1

Pu(1k) , 1

(48) (49) (50)

The last step holds since k λk Pu(k) , 1 = 0. Therefore, 1
coefﬁcients αk − βλk, k = 1, . . . , n, satisfy

n

(αk − βλk) Pu(k) , 1 = P , 1 .

(51)

1

k=1

Let β be the largest value such that αk − βλk ≥ 0 for all k. Consequently, there must exist a coefﬁcient αk − βλk = 0.

We could then remove the corresponding vector Pu(k) , 1 1
from the base. This procedure continues until all remain-

ing vectors are linearly independent. Since Pu1 , 1 ∈ Rd,

there are at most d linearly independent vectors, i.e., n ≤ d. Finally, we replace the probability measure P1 with a dis-
crete distribution P U1 = u(1k) = wk over a ﬁnite dis-

crete domain Ω∗1 =

u(11)

,

.

.

.

,

u

(d) 1

. Doing so generated a

new SCM N ∗, with cardinality |Ω1| ≤ d, that reproduces

probabilities P V ∈C UV(i) over canonical partitions in

the original SCM M . Repeatedly applying this procedure for every exogenous U2, . . . , Um completes the proof.

Lems. 3 to 5 together yield a natural constructive proof for Thm. 1 in an arbitrary causal diagram G.
Theorem 1. For a DAG G, consider following conditions5: 1. M (G) is the set of all SCMs compatible with G. 2. N (G) is the set of all discrete SCMs compatible with G
such that for every exogenous U ∈ U ,

|ΩU | =

|ΩPAV → ΩV | ,

(4)

V ∈C(U )

i.e., the number of functions mapping from domains of PAV to V for every endogenous V ∈ C(U ). Then, M (G), N (G) are counterfactually equivalent, i.e.,
{PM∗ : ∀M ∈ M (G)} = {PN∗ : ∀N ∈ N (G)} . (5)
Proof. By the deﬁnition of c-components (Def. 2), it is veriﬁable that for two different c-compoments C1, C2 ∈ C(G), their corresponding exogenous variables UC1 , UC2 do not share any element, i.e., UC1 ∩ UC2 = ∅. Therefore, we could repeatedly apply the construction of Lem. 5 for every c-component C ∈ C(G). Doing so generates a discrete SCM N satisfying conditions as follows:

5For every V ∈ V , we denote by ΩPAV → ΩV the set of all possible functions mapping from domains ΩPAV to ΩV .

1. N is compatible with G; 2. N and M share the same set of structural functions F ; 3. N and M generate the same joint distribution over the in-
tersections of canonical partitions associated with every c-component.
It follows from Lems. 3 and 4 that M and N must coincide in all counterfactual distributions P ∗ over endogenous variables. This completes the proof.
A.3 Decomposing Canonical Partitions
This section provides a more ﬁne-grained decomposition for equivalence classes in canonical partitions. Such a decomposition provides new insights to the discretization procedure.

Deﬁnition 6 (Cell). For an SCM M = V , U , F , P , for each V ∈ V , a subset RV ⊆ ΩUV is a cell if RV =
U∈UV RV,U where RV,U ⊆ ΩU , for every U ∈ U .
Obviously, for |UV | = 1, any subset of ΩUV is a cell. However, the same is not necessarily true for |UV | ≥ 2. As an example, consider an SCM M associated with the causal diagram of Fig. 1b where X, Y, Z are binary variables in {0, 1}; U1, U2 are continuous variables drawn uniformly from the interval [0, 3]. More speciﬁcally,

4

y ← fY (x, u1, u2) = 1(u ,u )∈U(i) h(Yi)(x),

12

Y

i=1

4
x ← fX (z, u2) = 1u ∈U(i) h(Xi)(z), 2X i=1

2
z ← fZ (u1) = 1u ∈U(i) h(Zi) 1Z i=1

(52)

Canonical partitions UY(i), UX(j), UZ(k) are described in Fig. 5. For points on the boundary, we include them in the equivalence class with a higher indices i, j, k. As an example, consider the equivalence class UY(1), i.e.,
UY(1) = ([0, 2) × [0, 1)) ∪ ((2, 3] × (2, 3]) . (53)

It is a subset in the union of two cells R(Y1), R2Y) given by R(Y1) = [0, 2] × [0, 1], R(Y2) = [2, 3] × [2, 3]. (54)

However, one could not write the equivalence class UY(1) as a product of intervals in ΩU1 , ΩU2 , i.e., UY(1) is not a cell.
Our next result shows that each equivalence class in the canonical partition could be decomposed into a countable union of almost disjoint cells.
Deﬁnition 7 (Covering). For an SCM M = V , U , F , P , for every V ∈ V , let UV be an arbitrary subset of ΩUV . Consider the following conditions: 1. R(Vj) | j ∈ JV is a countable set of cells.
2. For any i = j, R(Vi) and R(Vj) are almost disjoint, i.e.,

P R(Vi) ∩ R(Vj) = 0. (55)

u2

3 UY(2)

y←x 2
UY(3)

y ← ¬x 1
UY(1)

y←0

0

1

UY(1) y←0

UY(4) y←1

UY(3)

y ← ¬x

2

3 u1

(a) y ← fY (x, u1, u2)

UX(3) x ← ¬z

UX(4) x←1

UX(2) x←z

0

1

2

3 u2

(b) x ← fX (z, u2)

UZ(2) z←1

UZ(1) z←0

0

1

2

3 u1

(c) z ← fZ (u1)

Figure 5: Canonical partitions of exogenous domains as-
sociated with X, Y, Z. Each equivalence class (e.g., UY(i)) is covered by a ﬁnite set of (almost) disjoint cells (e.g.,
UY(4) ⊆ [2, 3] × [0, 1]). For points on the boundary, we break the ties in favor of equivalence classes with higher indices.

3. UV is a subset for ∪j∈JV R(Vj). Then, R(Vj) | j ∈ JV is said to be a covering for UV .

Lemma 6. For an SCM M = V , U , F , P , for every
V ∈ V , let UV(i) be the equivalent class for an arbitrary function hV(i) ∈ ΩPAV → ΩV . There exists a covering
R(Vj) | j ∈ JV for UV(i) such that

P UV(i) =

P R(Vj) .

j∈JV

(56)

Proof. We ﬁrst consider a weaker version of the covering cells which does not require every pair of cells to be disjoint. That is, condition (2) in Def. 7 does not necessarily hold. For any A ⊆ ΩUV , deﬁne a set of coverings C (A):

C (A) = C ⊆ 2ΩUV | C is a covering for A . (57)

where 2ΩUV represents the set of all subsets of ΩUV . Recall that every U ∈ U is associated with a probability
space ΩU , FU , PU . The product measure U∈U PU is the only probability measure P with restrictions to U∈U FU which satisﬁes the independence restriction in Eq. (40). It
follows from the construction of product measures (Bauer
1972, Theorem 1.5.2) that such a probability measure P
must satisfy the following property: for any A ⊆ ΩUV ,

P UV(i) = inf

P (RV ) | ∀C ∈ C (UV ) . (58)

RV ∈C

Therefore, we could obtain a covering R(Vj) | j ∈ JV for an arbitrary equivalence class UV(i) such that

P UV(i) =

P R(Vj) .

j∈JV

(59)

What remains is to show that every pair R(Vi), R(Vj) are almost disjoint. This is equivalent to proving the following:





P

R(Vj) =

P R(Vj) .

j∈JV

j∈JV

(60)

By basic properties of probability measures,





P

R(Vj) ≤

P R(Vj) .

j∈JV

j∈JV

(61)

Therefore, it is sufﬁcient to show that





P

R(Vj) ≥

P R(Vj) .

j∈JV

j∈JV

(62)

Suppose now Eq. (62) does not hold. This means that there exists a covering C ∈ C ∪j∈JV R(Vj) such that





P

R(Vj) =

P (RV ) <

P

j∈JV

RV ∈C

j∈JV

R(Vj) . (63)

By the deﬁnition in Eq. (57), C is also a covering in C UV(i) . The property in Eq. (58) implies:

P UV(i) ≤

P (RV ) <

P R(Vj) ,

RV ∈C

j∈JV

(64)

which contradicts Eq. (59). This completes the proof.

Henceforth, we will consistently refer to a set of cells
as a covering if they satisfy conditions both in Def. 7 and Eq. (56). For instance, consider the equivalence class UY(1) in Fig. 5a and cells R(Y1), R(Y2) deﬁned in Eq. (54). Since UY(1) ⊆ R(Y1) ∪ R(Y1), R(Y1), R(Y2) forms a covering for

UY(1). By noting that ﬁnite segments in ΩU1 × ΩU2 (e.g., a line U1 = 2) has zero measure, we have

P UY(1)

= P ((U1, U2) ∈ ([0, 2] × [0, 1]) ∪ ((2, 3] × (2, 3]))

= P ((U1, U2) ∈ [0, 2] × [0, 1])

(65)

+ P ((U1, U2) ∈ [2, 3] × [2, 3])

= P R(Y1) + P R(Y2) .

The existence of covering cells also allows us to decompose probabilities over intersections of equivalence classes across canonical partitions. Formally,

Lemma 7. For an SCM M = V , U , F , P , for any i ∈ I, there exists a sequence of coverings R(Vj) | j ∈ JV for UV(i), ∀V ∈ V , such that





P UV(i)
V ∈V

= P  RV(j,)U 

j∈J U ∈U

V ∈ch(U )

(66)

Proof. For every V ∈ V , let RV(j) | j ∈ JV be a covering for UV(i) deﬁned in Lem. 6, i.e., it satisﬁes Eq. (57). We ﬁrst show that, for any subset A ⊆ ΩU ,

P UV(i) ∩ A = P R(Vi) ∩ A . (67)
j∈JV

Let A = Ω \ A. Since R(Vj) | j ∈ JV UV(i), we must have the following:

is a covering of

P UV(i) ∩ A ≤ P R(Vj) ∩ A , (68)
j∈JV

P UV(i) ∩ A ≤

P R(Vj) ∩ A . (69)

j∈JV

Next, we show that the above inequality relationships are both tight. Suppose at least one of inequalities in Eqs. (32) and (68) is strict. We must have

P UV(i) = P UV(i) ∩ A + P UV(i) ∩ A

< P R(Vj) ∩ A + P R(Vj) ∩ A .

j∈JV

j∈JV

The above equation implies

P UV(i) <

P R(Vj) ,

j∈JV

(70)

which contradicts Eq. (57). This means that the statement in Eq. (67) must hold, which implies, for any i ∈ I,

P UV(i) = P R(Vj) .

V ∈V

j∈J

V ∈V

(71)

I ⊆ {1, . . . , 4} AIU1
I ⊆ {1, . . . , 4} AIU1

∅ ∅
{2, 3} *(1, 2)

{1} ∅
{2, 4} *(2, 3]

{2} ∅
{3, 4} ∅

{3} ∅
{1, 2, 3} [1, 1]

{4} ∅
{1, 2, 4} ∅

{1, 2} ∅
{1, 3, 4} ∅

{1, 3} *[0, 1)
{2, 3, 4} [2, 2]

{1, 4} ∅
{1, 2, 3, 4} ∅

Table 2: Atoms generated by subsets R(Ui1), i = 1, . . . , 4 deﬁned in Eq. (79) which are contained in the domain of an exogenous variable U1 drawn uniformly from an interval [1, 3]. Atoms with positive probability measure is marked with a asterisk “*”.
.

Recall that each cell R(Vj) is a product U∈UV R(Vj,)U where R(Vj,)U is a subset in ΩU . Since exogenous variables U are mutually independent, we must have, for any j ∈ J ,





P R(Vj) = P 

R(Vj,)U  .

V ∈V

U ∈U

V ∈ch(U )

(72)

This completes the proof.

Consider again the SCM M described in Eq. (52). Note that only function in the hypothesis class D∅ → DZ compatible with event Z = 1 is h(Z2) ≡ z ← 1. Similarly, event Xz=0 = 1, Xz=1 = 0 corresponds to function h(X3) ≡ x ← ¬z; event Yx=0 = 0, Yx=1 = 0 corresponds to the function h(Y1)(x) ≡ y ← 0. The decomposition of Eq. (30) gives:
P (Z = 1, Xz=0 = 1, Xz=1 = 0, Yx=0 = 0, Yx=1 = 0)
= P UZ(1) ∩ UX(3) ∩ UY(1) (73)

Among above quantities, UY(1) is covered by cells R(Y1), R(Y2) deﬁned in Eq. (54). UZ(1) and UX(3) are cov-
ered by cells R(Z1) and R(X1), respectively, given by
R(Z1) = {u1 ∈ [0, 1]}, R(X1) = {u2 ∈ [0, 1]}. (74)
Applying the decomposition in Eq. (66) implies

P UZ(1) ∩ UX(3) ∩ UY(1)

= P R(Z1) ∩ R(X1) ∩ R(Y1) + P R(Z1) ∩ R(X1) ∩ R(Y2)

= P (U1 ∈ [0, 1]) P (U2 ∈ [0, 1]) .

(75)

Eqs. (73) and (75) together give the evaluation
1 P (Z = 1, Xz=0 = 1, Xz=1 = 0, Yx=0 = 0, Yx=1 = 0) = 9 .
One could verify the above equation from the parametrization in Eq. (52) using the three-step algorithm in (Pearl 2000) which consists of abduction, action, and prediction.

A.4 Decomposing Covering Cells
For an arbitrary cell RV = ×U∈UV RV,U , we will call every “side” RV,U the projection of RV onto domain ΩU , for every U ∈ UV . Observe that for disjoint cells, their projections onto the same domain may not necessarily be disjoint. As an

instance, equivalence classes UY2 and UY4 in Fig. 4a are covered by (almost) disjoint cells [1, 3] × [1, 2] and [0, 2] × [2, 3]
respectively. Their projections onto U1 are intervals [1, 3] and [0, 2], which overlap in the sub-interval [1, 2]. This ob-
servation suggests that every covering cell could be further
decomposed, which will be our focus in this section. The collection of all projections of covering cells R(Vj,)U
deﬁned in Lem. 7 onto an exogenous U ∈ U is given by

R(Vj,)U | ∀V ∈ ch(U ), ∀j ∈ J . (76)

Lems. 3 and 7 shows that all counterfactual distributions in

any SCM could be written as a function of probabilities over

intersections of above projections, i.e.,







 P

R(Vj,)U  | ∀V ∈ ch(U ), ∀j ∈ J  . (77)

 V ∈ch(U )



To prove the counterfactual equivalence of canonical SCMs,

it is thus sufﬁcient to show that probabilities in Eq. (77)

could be generated using a discrete distribution.

For convenience, we will slightly abuse the notation

and consistently represent Eq. (76) using a countable set

R(Uj) | j ∈ N . We will also utilize a special type of sub-

sets in domain ΩU generated by intersections over projections and their complements, which we call atoms.

Deﬁnition 8 (Atom). For an arbitrary U ∈ U , let

R(Uj) | j ∈ N be a countable collection of subsets in ΩU .

For any I ⊆ N, an atom AIU ⊆ ΩU is deﬁned as:

AIU = R(Ui) ∩

ΩU \ R(Ui) .

(78)

i∈I

i∈I

Observe that these atoms are pairwise disjoint, and that I⊆N AIU = Ω. For instance, consider again canonical partitions described in Fig. 5. Covering cells for UY(i), UZ(j) generates a collection of projections R(Ui1) | i = 1, . . . , 4 onto the exogenous domain of U1, i.e.,
R(U11) = [0, 1], R(U21) = [1, 3], (79) R(U31) = [0, 2], R(U41) = [2, 3].
For an indexing set I = {1, 3}, atom A{U11,3} is given by
A{U11,3} = R(U11) ∩ R(U31) ∩ ΩU1 \ R(U21) ∩ ΩU1 \ R(U41)

= [0, 1] ∩ [0, 2] ∩ [0, 1) ∩ [0, 2)

= [0, 1)

Table 2 shows atoms computed from all indexing sets I ⊆ {1, . . . , 4}. We obtain a set of atoms A(Ui1) | i = 1, . . . , 3

with positive probability measures, given by

A(U11) = [0, 1), A(U21) = (1, 2), A(U31) = (2, 3]. (80)

Evidently, one could write probabilities over any intersection of projections in R(Ui1) as a summation over some atoms A(Ui1). To witness, we show in Fig. 6 more ﬁne-grained partitions over exogenous domains associated with X, Y, Z fol-
lowing the decomposition of atoms A(Ui1) | i = 1, . . . , 3 .

In general, one could represent probabilities of any event generated by a ﬁnite set of projections
R(Ui1) | i = 1, . . . , N using the decomposition of atoms. However, as the number of projections N → ∞, there could exist uncountably many such atoms. Therefore, one could not immediately represent their measures as a discrete distribution. Next, we show that it sufﬁces to consider only a countable set of atoms with positive measures.

Lemma 8. For an SCM M = V , U , F , P , for every U ∈ U , there exists a countable set of atoms A(Ui) | i ∈ N

deﬁned in Eq. (78) such that
any j ∈ ×V ∈ch(U) JV ,

i∈N P A(Ui) = 1 and for





P

R(j)  =

I (i) (j) P A(i) .

V,U

AU ⊆RV,U

U

V ∈ch(U )

i∈N V ∈ch(U )

Proof. Formally, we deﬁne





 H=

A(UJ) | J ⊆ 2N countable or co-countable .

J ∈J



It is veriﬁable that H is a σ-algebra generated by projections R(Uj) | j ∈ N . Furthermore, the intersection
V ∈ch(U) R(Vj,)U is a measurable set in H. Therefore, it is sufﬁcient to show that for any event BU ∈ H,

P (BU ) = 1A(i)⊆B P A(Ui) .

U

U

i∈N

(81)

We ﬁrst show that there exists a countable set of atoms A(Ui) | i ∈ N that covers domain ΩU , i.e.,

P ΩU \ A(Ui) = 0.

(82)

i∈N

Take BU(1) = ΩU and deﬁne by induction, for all i ∈ N, if P BU(i) > 0, then let

BU(i+1) = BU(i) \ A(Ui)

(83)

where A(Ui) ⊆ BU(i) is an atom with the largest positive measure among all atoms contained in BU(i).
If we repeatedly apply the above construction, one of two

things may happen:

u2

3 UY(2)

UY(2)

UY(1)

y←x 2
UY(3)

y←x UY(4)

y←0 UY(4)

y ← ¬x 1
UY(1)

y←1 UY(1)

y←1 UY(3)

y←0

y ← 0 y ← ¬x

0

1

2

3 u1

(a) y ← fY (x, u1, u2)

UX(3) x ← ¬z

UX(4) x←1

UX(2) x←z

0

1

2

3 u2

(b) x ← fX (z, u2)

UZ(2) z←1

UZ(1) z←0

UZ(1) z←0

0

1

2

3 u1

(c) z ← fZ (u1)

Figure 6: More ﬁne-grained Partitions of exogenous domains associated with X, Y, Z based on atoms. Each equivalence class (e.g., UY(i)) is decomposed into a ﬁnite set of pairwise disjoint cells formed by atoms (e.g., [0, 1) × [1, 2]).

1. For some n ∈ N, P BU(i) = 0 and in this case, A(U1), . . . , A(Un−1) satisfy Eq. (82).
2. For all i ∈ N, P BU(i) > 0. In this case, we have
a countable set of atoms AU(i) | i ∈ N of positive measures. We now prove Eq. (82) by contradiction. If Eq. (82) does not hold, then there is an atom A ⊆ ΩUV \ i∈N AU(i) such that P (A) > 0. By our choice of A(Ui) at each step, we have that, for all i ∈ N,
P A(Ui) ≥ P (A). (84)
Therefore,

P A(Ui) = P A(Ui) = ∞. (85)

i∈N

i∈N

Contradiction, since the probability measure P is ﬁnite.

For any event BU ∈ H, since BU ⊆ ΩU , Eq. (82) implies

P BU \ A(Ui) ≤ P ΩU \ A(Ui) = 0. (86)

i∈N

i∈N

Therefore,

P (BU ) = P

BU ∩ A(Ui)
i∈N

=P
i∈N

BU ∩ A(Ui)

= 1A(Ui)⊆BU P
i∈N

A(Ui)

(87) (88) (89)

The last step holds since BU is a union of atoms and atoms are pairwise disjoint. This completes proof.

We are now ready to prove the counterfactual equivalence for canonical SCMs with discrete exogenous domains.
Lemma 9. For a DAG G, let M be an arbitrary SCM compatible with G. There exists a discrete SCM N compatible with G such that PM∗ = PN∗ , i.e., M and N coincide in all counterfactual distributions.

Proof. Let A(Ui) | i ∈ N the countable set of atoms deﬁned in Lem. 8 for every U ∈ U . We construct a discrete SCM N from M as follows.
1. For every U ∈ U , pick an arbitrary constant u(i) in each atom A(Ui).
2. Deﬁne distribution P (U ) for every U ∈ U in N as:

PN U = u(i) = PM A(Ui) .

(90)

Doing so generates a discrete SCM N satisfying conditions as follows:
1. N is compatible with G; 2. N and M share the same set of structural functions F ; 3. N and M generate the same distribution over the in-
tersections of projections of covering cells deﬁned in Eq. (77).
It follows from Lems. 3 and 7 that M and N must coincide in all counterfactual distributions P ∗ over endogenous variables. This completes the proof.

A mental image for the discretization procedure in Lem. 9
is described as follows. We ﬁrst partition the exogenous do-
main ΩU for each U ∈ U into a countable collection of atoms. By doing so, we obtain a partition over the product
× domain ΩUV = U∈UV ΩU for every V ∈ V . Such a par-
tition consists of countably many (almost) disjoint covering
cells (Def. 6) formed by products of atoms (Def. 8). Every
cell is assigned with a unique function hV in the hypothesis class ΩPAV → ΩV mapping from domains of input PAV to V . Given any conﬁguration U = u, for every V ∈ V , one
could ﬁnd the cell containing the constant uV and generate values of V following the associated function hV . As an

example, we show in Fig. 6 a graphical illustration for this discretization procedure for the SCM described in Eq. (52).
Finally, to construct a discrete SCM, it is sufﬁcient to pick an arbitrary constant u(i) in each atom, assign it with the probability measure over the corresponding atom, and replace the exogenous U with a variable drawn from a discrete distribution over constants u(i). Repeatedly applying this procedure for every exogenous U ∈ U results in a canonical SCM with discrete exogenous domains. Also, one could further reduce cardinalities of exogenous domains by shrinking the support of the constructed discrete distribution. This could be done by re-weighting probabilities assigned to constant u(i) in each atom while maintaining probabilities over canonical partitions. Indeed, it is possible to bound the total number of atoms with positive probabilities to a ﬁnite value. The existence of such probability measures is guaranteed by the classic result of Carathe´odory theorem (Carathe´odory 1911), following a similar procedure in the proof of Lem. 7.

B. Markov Chain Monte Carlo for Partial Counterfactual Identiﬁcation
In this section, we will show derivations for complete conditional distributions utilized in our proposed Gibbs samplers. We will also provide proofs for non-asymptotic bounds for empirical estimates of credible intervals used in Alg. 1.

B.1 Derivations of Complete Conditionals
Sampling P (u¯ | v¯, θ, µ). It is veriﬁable that variables U (n), V (n), n = 1, . . . , N , are mutually independent given parameters θ, µ. This implies

P (u¯ | v¯, θ, µ) = P
U ∈U
=P
U ∈U

u(n) | v¯, θ, µ u(n) | v(n), θ, µ

The complete conditional over U (n) | V (n), θ, µ , n = 1, . . . , N , is given by

P u(n) | v(n), θ, µ ∝ P u(n)v(n) | θ, µ

∝ P v(n) | pa(Vn), u(Vn), θ, µ
V ∈V
· P u(Vn) | θ, µ .
U ∈U

Among quantities in the above equation,

P v(n) | pa(n), u(n), θ, µ = µ , pa(Vn),u(n)

VV

v(n)

and P u(Vn) | θ, µ = θu for u = u(Vn).

Sampling P (µ, θ | v¯, u¯). For every exogenous variable U ∈ U , we denote by θU the set of parameters {θu | ∀u}. Similarly, for every endogenous variable V ∈ V , let µV =
µV(paV ,uV ) | ∀paV , uV . Obviously, parameters µV and
θU are mutually independent, and they do not directly determine values of a variable (exogenous or endogenous) simultaneously. We must have

P (µ, θ | v¯, u¯) = P (µV | v¯, u¯) P (θU | v¯, u¯) .

V ∈V

U ∈U

The above independence relationship implies that to draw
samples from the posterior distribution P (µ, θ | v¯, u¯), we could sample distributions over µV | V¯ , U¯ and θU | V¯ , U¯ for every V ∈ V and every U ∈ U separately.
Recall that for every V ∈ V , any paV , uV , µV(paV ,uV ) = µv(paV ,uV ) | ∀v ∈ ΩV is an indicator vector such that

µ(vpaV ,uV ) ∈ {0, 1},

µv(paV ,uV ) = 1.
v∈ΩV

The complete conditional distribution over µV | V¯ , U¯ , given by Eq. (14), follows from the fact that in any discrete SCM, the n-th observation of V ∈ V \ Z(n) is decided by
v(n) ← fV pa(Vn), u(Vn) = v,

where v is a unique element in ΩV such that µ(vpaV ,uV ) = 1. The complete conditional distribution over θU | V¯ , U¯ ,
given by Eq. (15), follows from the conjugacy of Dirichlet distributions with regard to categorical distributions (e.g., see (Ishwaran and James 2001, Sec. 5.2)).
Sampling P u(n) | v¯, u¯−n . At each iteration, draw U (n) from the conditional distribution given by

P u(n) | v¯, u¯−n

∝

P

V ∈V \Z(n)

v(n) | pa(Vn), uV(n), v¯−n, u¯−n

P u(n) | v¯−n, u¯−n .
U ∈U

Among quantities in the above equation, by expanding on valus of parameters µ(VpaV ,uV ), one could rewrite the posterior distribution P v(n) | pa(Vn), u(Vn), v¯−n, u¯−n for every
V ∈ V \ Z(n) as follows

P v(n) | pa(Vn), u(Vn), v¯−n, u¯−n

= µ 1 1 paV ,uV µ(paV ,uV ) V

(paV ,uV ) v(n)

pa V =pa V(n)

uV =u(Vn)

· P µV(paV ,uV ) | v¯−n, u¯−n .

(91)

The complete conditional over µ(VpaV ,uV ) | V¯−n, V¯−n , ∀paV , uV , follows from the deﬁnition of discrete SCMs. The n-th observation of V ∈ V \ Z(n) is decided by
v(n) ← fV paV(n), u(Vn) = v,

for a unique v ∈ ΩV such that µ(vpaV ,uV ) = 1. Formally, if there exists a sample i = n such that V ∈ Z(i) and pa(Vi) = paV , uV(i) = uV , the posterior over µ(VpaV ,uV ) is given by
P µ(vpaV ,uV ) = 1 | v¯, u¯ = 1v=v(i) .

Otherwise,

P µ(paV ,uV ) | v¯, u¯ =

1 .

V

|ΩV |

Marginalizing probabilities P µV(paV ,uV ) | v¯, u¯ over the
domain ΩV in Eq. (91) gives the complete conditional distribution over V (n) | PA(Vn), UV(n), U¯−n, U¯−n .
For every U ∈ U , the complete conditional over U (n) | V¯−n, U¯−n , given by Eq. (15), follows immediately from the Po´lya urn characterization of Dirichlet distributions (e.g., see (Ishwaran and James 2001, Sec. 4)).

B.2 Monte Carlo Estimation of Credible Intervals Recall that for samples θ(t) Tt=1 drawn from P (θctf | v¯), the empirical estimates for 100(1 − α)% credible interval over θctf are deﬁned as:
ˆlα(T ) = θ( (α/2)T ), rˆα(T ) = θ( (1−α/2)T ), (92)

where θ( (α/2)T ), θ( (1−α/2)T ) are the (α/2)T th smallest and the (1 − α/2)T th smallest of θ(t) . One could apply standard concentration inequalities to determine a sufﬁcient number of draws T required for obtaining accurate estimates of a 100(1 − α)% credible interval.
Lemma 1. Fix T > 0 and δ ∈ (0, 1). Let function f (T, δ) = 2T −1 ln(4/δ). With probability at least 1 − δ, estimators
ˆlα(T ), rˆα(T ) for any α ∈ [0, 1) is bounded by

lα−f(T,δ) ≤ ˆlα(T ) ≤ lα+f(T,δ), rα+f(T,δ) ≤ rˆα(T ) ≤ rα−f(T,δ).

(21)

Proof. Fix > 0. If ˆlα(T ) > lα+ , this means that there

are at most (α/2)T − 1 instances in smaller than or equal to lα+ . That is,

(t) T
θctf

that are

t=1

P ˆlα(T ) > lα+

T

≤P

1θc(ttf)≤lα+ ≤ (α/2)T − 1
t=1

T

≤P

1θc(ttf)≤lα+ ≤ (α/2)T
t=1

≤P

T
T1 1 t=1

θc(ttf ) ≤lα+

α+ ≤
2

− 2

T2

≤ exp −

.

2

The last step in the above equation follows from the standard

Hoeffding’s inequality.

If ˆlα(T ) < lα− , this implies that there are at least

(α/2)T instances in equal to lα+ . That is,

(t) T

θctf

that are larger than or

t=1

P ˆlα(T ) < lα−

T

≤P

1θc(ttf)≤lα− ≥ (α/2)T
t=1

T

≤P

1θc(ttf)≤lα− ≥ (α/2)T
t=1

≤P

T
T1 1 t=1

θc(ttf ) ≤lα−

α− ≥
2

+ 2

T2

≤ exp −

.

2

The last step follows from the standard Hoeffding’s inequal-

ity. Similarly, we could also show that
P hˆα(T ) < hα+ ≤ exp − T22 , P hˆα(T ) > hα− ≤ exp − T22 .

Finally, bounding the error rate by δ/4 gives:

exp − T 2 = δ ⇒ = 2T −1 ln(4/δ). (93)

2

4

Replacing the error rate with f (T, δ) = 2T −1 ln(4/δ) completes the proof.

As a corollary, it immediately follows from Lem. 1 that Algorithm CREDIBLEINTERVAL (Alg. 1) is guaranteed to from a sufﬁcient estimate of 100(1 − α)% credible intervals within the speciﬁed margin of errors.
Corollary 1. Fix δ ∈ (0, 1) and > 0. With probability at least 1 − δ, the interval [ˆl, rˆ] = CREDIBLEINTERVAL(α, δ, ) for any α ∈ [0, 1) is bounded by ˆl ∈ [lα− , lα+ ] and rˆ ∈ [rα+ , rα− ].

Proof. The statement follows immediately from Lem. 1 by setting 2T −1 ln(4/δ) ≤ .

C. Simulation Setups and Additional

Experiments

In this section, we will provide details on the simulation setups and preprocessing of datasets. We also conduct additional experiments on other more involved causal diagrams and using skewed hyperparameters for prior distributions. For all experiments, we will focus on Dirichlet priors in Eq. (11) with hyperparameters αU(u) = αU /dU for some real αU > 0. This is equivalent to drawing probabilities θu from a Dirichlet distribution deﬁned as follows:

αU

αU

(θ1, . . . , θdU ) ∼ Dirichlet

,··· ,

dU

dU

,

(94)

All experiments were performed on a computer with 32GB memory, implemented in MATLAB. We are migrating the source code to other open-source platforms (e.g., Julia), which will be released once the code migration is done.

Experiment 1: Frontdoor We study the problem of
evaluating interventional probabilities P (yx) from the observational distribution P (X, Y, W ) in the “Frontdoor” diagram of Fig. 1c. We collect N = 104 samples v¯ = {x(n), y(n), w(n)}Nn=1 from an SCM compatible with Fig. 1c. Detailed parametrization of the SCM is provided in
the following:

U1 ∼ Unif(0, 1),

U2 ∼ Normal(0, 1),

X ∼ Binomial(1, ρX ),

(95)

W ∼ Binomial(1, ρW ),

Y ∼ Binomial(1, ρY ),

where probabilities ρX , ρW , ρY are given by

ρX = U1,

1 ρW = 1 + exp(−X − U2) ,

1 ρY = 1 + exp(W − U1) .

Each observation x(n), y(n), w(n) is an independent draw from the observational distribution P (X, Y, W ). We set hyperparameters αU1 = dU1 = 8, αU1 = dU2 = 4.

Experiment 2: PNS We study the problem of evaluat-
ing the counterfactual probability P (yx, yx ) ≡ P (Yx = y, Yx = y ) for any x = x , y = y from the ob-
servational distribution P (X, Y ) in the “Bow” diagram of Fig. 1d. We collect N = 103 observational samples v¯ =
{x(n), y(n)}Nn=1 from an SCM compatible with Fig. 1d. Detailed parametrization of the SCM is deﬁned as follows:

U ∼ Normal(0, 1), X ∼ Binomial(1, ρX ), E ∼ Logistic(0, 1), (96) Y ← 1X−U+E+0.1>0, where probabilities ρX are given by
1 ρX = 1 + exp(U ) .

Each observation x(n), y(n) is an independent draw from the observational distribution P (X, Y ). In this experiment, we set hyperparameters αU = dU = 8.

Experiment 3: IST International Stroke Trials (IST) was a large, randomized, open trial of up to 14 days of antithrombotic therapy after stroke onset (Carolei et al. 1997). The aim was to provide reliable evidence on the efﬁcacy of aspirin and of heparin. The dataset is released under Open Data Commons Attribution License (ODC-By). In particular, the treatment X is a pair (i, j) where i = 0 stands for no aspirin allocation, 1 otherwise; j = 0 stands for no heparin allocation, 1 for median-dosage, and 2 for high-dosage. The primary outcome Y ∈ {0, . . . , 3} is the health of the patient 6 months after the treatment, where 0 stands for death, 1 for being dependent on the family, 2 for the partial recovery, and 3 for the full recovery.
To emulate the presence of unobserved confounding, we ﬁlter the experimental data with selection rules fX(Z), Z ∈ {0, . . . , 9}, following a procedure in (Zhang and Bareinboim 2021). More speciﬁcally, we are provided with a collection of IST samples {X(n), Y (n), U2(n)}Nn=1 where U2(n) is the age of the n-th patient. For each data point
X(n), Y (n), U2(n) , we introduce an instrumental variable
Z(n) ∈ {0, . . . , 9}. Values of the instrumental variable Z(n) for the n-th patient are decided by

Z(n) = 10 × U1 , where U1(n) ∼ Unif(0, 1). (97)

We then check if X(n) satisﬁes the following condition

X(n) = 6 × ρX ,

(98)

where parameter ρX is given by

1 ρX =
1 + exp −U2(n)/100 − Z(n)/10

If the above condition is satisﬁed, we keep the data point
X(n), Y (n), Z(n), U1(n), U2(n) in the dataset; otherwise,
the data point is dropped. After this data selection process is complete, we hide columns of variables U1(n), U2(n). Doing so allows us to obtain N = 1 × 103 synthetic observational samples V¯ = X(n), Y (n), Z(n) N that are compatible
n=1
with the “IV”’ diagram of Fig. 1a.
In this experiment, we set hyperparameters αU1 = 10 and αU2 = 1. As a baseline, we estimate the treatment effect E[Yx=(1,0)] = 1.3418 for only assigning aspirin X = (1, 0) from randomized trial data containing 1.9285×104 subjects.

Experiment 4: Obs. + Exp. We study the problem of evaluating counterfactual probabilities P (z, xz , yx ) from the combination of the observational distribution P (X, Y, Z) and interventional distributions P (Xz, Yz), ∀z ∈ ΩZ , in the causal diagram of Fig. 1b. We collect N = 103 samples v¯ = {x(n), y(n), z(n)}Nn=1 from an SCM compatible with

W U1

Z

U2

U3

X

Y

(a) Napkin

U1

U2

U1

U2

Z

X

Y

(b) Double Bow

U1

U2

Z

X

Y

(c) M+BD Graph

U1

U2

U3

U1

U2

U3

Z

W

X

Y

(d) Triple Bow

Figure 7: Causal diagrams for Experiment 5 (a), Experiment 6 (b), Experiment 7 (c), and Experiment 8 (d). Each diagram contains (not exclusively) a treatment X, an outcome Y , ancestors Z, W , and exogenous variables Ui, i = 1, 2, 3.

Fig. 1b, which we deﬁne as follows:

U1 ∼ Unif(0, 1),

U2 ∼ Unif(0, 1),

Z ← min { 15 · U1 , 9} ,

(99)

X ∼ Binomial(9, ρX ),

Y ∼ Binomial(9, ρY ),

where for any real α ∈ R, the operator α denotes the largest integer n ∈ Z smaller than α, i.e., α = min{n ∈ Z | n ≥ α}; probabilities ρX , ρY are given by
1 ρX = 1 + exp(−Z − U2) ,
1 ρY = 1 + exp(X/10 − U1 · U2) .
Each sample x(n), y(n), z(n) is an independent draw from the observational distribution P (X, Y, Z) or an interventional distribution P (Xz, Yz). To obtain a sample from P (Xz, Yz), we pick a constant z ∈ ΩZ uniformly at random, perform intervention do(Z = z) in the SCM described in Eq. (99) and observed subsequent outcomes. In this experiment, we set hyperparameters αU1 = 10 and αU2 = 10.

C.1 Additional Simulation Results
We also evaluate our algorithms on various simulated SCM instances in other more involved causal diagrams. Overall, we found that simulation results match our ﬁndings in the main manuscript. For identiﬁable settings (Experiment 5), our algorithms are able to recover the actual, unknown counterfactual probabilities. For non-identiﬁable settings, our algorithm consistently dominates existing bounding strategies: it achieves sharp bounds if closed-formed solutions exist (Experiments 6); otherwise, it improves over state-of-art bounds (Experiment 7). Finally, for other more challenging non-identiﬁable settings where existing strategies do not apply (Experiments 8), our algorithm is able to achieve effective bounds over unknown counterfactual probabilities.
In all experiments, we evaluate our proposed strategy using credible intervals (ci). In particular, we draw at least 4 × 103 samples from the posterior distribution P (θctf | v¯) over the target counterfactual. This allows us to compute 100% credible interval over θctf within error = 0.05, with probability at least 1 − δ = 0.95. As the baseline, we also include the actual counterfactual probability, labeled as θ∗.

Experiment 5: Napkin Graph Consider the “Napkin”
graph in Fig. 7a where X, Y, Z, W are binary variables in
{0, 1}; U1, U2, U3 take values in real R. The identiﬁability of interventional probabilities P (yx) from the observational distribution P (X, Y, Z, W ) could be derived by itera-
tively applying inference rules of “do-calculus” (Pearl 2000, Thm. 4.3.1). We collect N = 104 observational samples v¯ = {x(n), y(n), z(n), w(n)}Nn=1 from an SCM compatible with Fig. 7a, deﬁned as follows:

Ui ∼ Normal(0, 1), i = 1, 2, 3, W ∼ Binomial(1, ρW ), Z ∼ Binomial(1, ρZ ), X ∼ Binomial(1, ρX ), Y ∼ Binomial(1, ρY ),

(100)

where probabilities ρW , ρZ , ρX , ρY are given by:
1 ρW = 1 + exp(U1 − U2) ,
1 ρZ = 1 + exp(W − U3) ,
1 ρX = 1 + exp(−Z − U1) ,
1 ρY = 1 + exp(X − U2 − 0.5) .

Each observation x(n), y(n), z(n), w(n) is an independent
draw from the observational distribution P (X, Y, Z, W ). In this experiment, we set hyperparameters αU1 = dU1 = 32, αU2 = dU1 = 32, and αU3 = dU3 = 4. Fig. 8a shows a histogram containing samples drawn from the posterior distribution of (P (Yx=0 = 1) | v¯). Our analysis reveals that these samples converges to the actual interventional probability P (Yx=0 = 1) = 0.6098, which conﬁrms the identiﬁability of P (yx) in the napkin graph.

Experiment 6: Double Bow Consider the “Double Bow”
diagram in Fig. 7b where X, Y, Z ∈ {0, 1} and U1, U2 ∈ R. We study the problem of evaluating interventional probabilities P (yx) from the observational distribution P (X, Y, Z). We collect N = 103 observational samples v¯ = {x(n), y(n), z(n)}Nn=1 from an SCM compatible with Fig. 1b. The detailed parametrization of the SCM is deﬁned

(a) Napkin

(b) Double bow

(c) M+BD Graph

(d) Triple bow

Figure 8: Histogram plots for samples drawn from the posterior distribution over target counterfactual probabilities. For all plots
(a - d), ci represents our proposed algorithms; bp stands for Gibbs samplers using the representation of canonical partitions (Balke and Pearl 1994); θ∗ is the actual counterfactual probability; opt is the optimal asymptotic bounds (if exists); nb stands
for the natural bounds (Manski 1990).

as follows:

Ui ∼ Normal(0, 1), i = 1, 2, Z ∼ Binomial(1, ρZ ), X ∼ Binomial(1, ρX ), Y ∼ Binomial(1, ρY ),

(101)

where probabilities ρZ , ρX , ρY are given by:

1 ρZ = 1 + exp(−U1) ,
1 ρX = 1 + exp(−Z − U1 − U2) ,
1 ρY = 1 + exp(X − U2 + 0.5) .

Each observation x(n), y(n), z(n) is an independent draw
from the observational distribution P (X, Y, Z).
(Balke and Pearl 1997) introduced a closed-form bound over P (yx) from the observational distribution P (X, Y, Z) for the “IV” diagram in Fig. 1a with binary X, Y, Z ∈ {0, 1}. It is veriﬁable that such a bound is also applicable in Fig. 8b with binary endogenous domains, and is provably optimal (labeled as opt). To obtain a 100% credible intervals, we apply the collapsed Gibbs sampler with hyperparameters αU1 = dU1 = 32 and αU2 = dU1 = 32. Fig. 8b shows samples drawn from the posterior distribution of (P (Yx=0 = 1) | v¯). The analysis reveals that our algorithm derives a valid bound over the actual probability P (Yx=0 = 1) = 0.3954; the 100% credible interval converges to the optimal IV bound l = 0.1980, r = 0.6258.

Experiment 7: M+BD Graph Consider the “M+BD”
graph in Fig. 7c where X, Y, Z ∈ {0, 1} and U1, U2 ∈ R. In this case, interventional probabilities P (yx) are nonidentiﬁable from the observational distribution P (X, Y, Z)
due to the presence of the collider path X ← U1 → Z ← U2 → Y . We collect N = 103 observational samples v¯ = {x(n), y(n), z(n)}Nn=1 from an SCM compatible with Fig. 7c. The detailed parametrization of the SCM is provided

as follows:

Ui ∼ Normal(0, 1), i = 1, 2, Z ∼ Binomial(1, ρZ ), X ∼ Binomial(1, ρX ), Y ∼ Binomial(1, ρY ),

(102)

where probabilities ρZ , ρX , ρY are given by:

1 ρZ = 1 + exp(−U1) ,
1 ρX = 1 + exp(−Z − U1 − U2) ,
1 ρY = 1 + exp(X − Z − U2) .

Each observation x(n), y(n), z(n) is an independent draw
from the observational distribution P (X, Y, Z).
In this experiment, we set hyperparameters αU1 = dU1 = 32 and αU2 = dU1 = 32. Fig. 8c shows samples drawn from the posterior distribution of (P (Yx=0 = 1) | v¯). As a baseline, we also include the natural bounds introduced in (Robins 1989; Manski 1990) (nb). The analysis reveals that all algorithms achieve bounds that contain the actual, target causal effect P (Yx=0 = 1) = 0.5910. Our algorithm obtains a 100% credible interval lci = 0.4884, rci = 0.6519, which improves over the existing bounding strategy (lnb = 0.2230, rnb = 0.8296).

Experiment 8: Triple Bow Consider the “Triple Bow” di-
agram in Fig. 7d where X, Y, Z ∈ {0, 1} and U1, U2, U3 ∈ R. We are interested in evaluating the counterfactual probability P (Yx=1 = 1, Yx=0 = 0) from the combination of the observational distribution P (X, Y, Z, W ) and interventional
distributions P (Xz, Yz, Wz). To our best knowledge, existing bounding strategies are not applicable to this setting. We collect N = 103 samples v¯ = {x(n), y(n), z(n), w(n)}Nn=1 from an SCM compatible Fig. 7d. The detailed parametriza-

(a) Flat

(b) Skewed

(c) Flat

Figure 9: Prior distributions for (a, b) Experiment 9 and (c, d) Experiment 10.

(d) Skewed

(a) N = 10

(b) N = 102

(c) N = 103

(d) N = 104

(e) N = 10

(f) N = 102

(g) N = 103

(h) N = 104

Figure 10: Histogram plots for samples drawn from the posterior distribution over probability P (Yx=0 = 0) in “Frontdoor” graph of Fig. 1c using two priors. (a - d) shows the posteriors using the ﬂat prior and observational data of size N = 10, 102, 103 and 104 respectively; (e - h) shows the posetriors using the skewed prior and the same respective observational datasets.

tion of the SCM is provided in the following:

U1 ∼ Unif(0, 1), Ui ∼ Normal(0, 1), i = 2, 3, Z ∼ 1.5 · U1 , W ∼ Binomial(1, ρW ), X ∼ Binomial(1, ρX ), E ∼ Logistic(0, 1), Y ← 1X−U3+E+0.1>0,

(103)

where probabilities ρZ , ρW , ρX are given by:
1 ρZ = 1 + exp(−U1) ,
1 ρW = 1 + exp(−Z − U1 − U2) ,
1 ρX = 1 + exp(−W − U2 − U3) .

Each sample x(n), y(n), z(n), w(n) is an independent draw
from the observational distribution P (X, Y, Z, W ) or an interventional distribution P (Xz, Yz, Wz). To obtain a sample from P (xz, yz, wz), we pick a constant z ∈ ΩZ uniformly at random, perform intervention do(Z = z) in the SCM described in Eq. (103) and observed subsequent outcomes.

In this experiment, we set hyperparameters αU1 = dU1 = 32 and αU2 = dU1 = 32. Fig. 8d shows samples drawn from the posterior distribution of (P (Yx=0 = 1) | v¯). The analysis reveals that our proposed approach is able to achived an effective bound that contain the actual counterfactual probability P (Yx=1 = 1, Yx=0 = 0) = 0.1867. The 100% credible interval (ci) is equal to l = 0.1150, r = 0.3686.
C.2 The Effect of Sample Size and Prior Distributions
We will evaluate our algorithms using skewed prior distributions. We found that increasing the size of observational samples was able to wash away the bias introduced by prior distributions. That is, despite the inﬂuence of prior distributions, our algorithms eventually converge to sharp bounds over unknown counterfactual probabilities as the number of observational sample grows (to inﬁnite).
Experiment 9: Frontdoor Consider ﬁrst the “Frontdoor” graph in Fig. 1d where interventional probabilities P (yx) is identiﬁable from the observational distribution P (X, Y, W ). The detailed parametrization of the underlying SCM is described in Eq. (95). We present our results using two different priors. The ﬁrst is a ﬂat (uniform) distribution over probabilities of U1 and U2 respectively, i.e., αU1 = dU1 = 8 and αU1 = dU2 = 4. The second is skewed to present a

(a) N = 10

(b) N = 102

(c) N = 103

(d) N = 104

(e) N = 10

(f) N = 102

(g) N = 103

(h) N = 104

Figure 11: Histogram plots for samples drawn from the posterior distribution over probability P (Yx=0 = 0) in “IV” graph of Fig. 1a using two priors. (a - d) shows the posteriors using the ﬂat prior and observational data of size N = 10, 102, 103 and 104 respectively; (e - h) shows the posetriors using the skewed prior and the same respective observational datasets.

strong preference on the deterministic relationships between X and Y ; in this case, α1 = 300 × dUi , i = 1, 2, for prior distributions associated with both U1 and U2. Figs. 9a and 9b shows the distribution of P (Yx=0) induced by these two priors (in the absence of any observational data). We see that the skewed prior of Fig. 9b assigns almost all weights to deterministic events P (Yx=0 = 1) = 1 or P (Yx=0 = 0) = 1.
Fig. 7 shows posterior samples obtained by our Gibbs sampler when applied to observational data of various sizes, using both the ﬂat prior (Figs. 10a to 10d) and the skewed prior (Figs. 10e to 10h). Both priors eventually collapse to the actual, unknown probability P (Yx=0 = 1) = 0.5085. As expected, more observational data are needed for the skewed prior before the posterior distribution converges, since the skewed prior is concentrated further away from the value 0.5085 than the uniform prior.

Experiment 10: IV Consider the “IV” diagram in Fig. 1a where X, Y, Z are binary variables taking values in {0, 1}. Detailed parametrization of the SCM is provided as follows:

U1 ∼ Normal(0, 1), U2 ∼ Normal(0, 1), Z ∼ Binomial(1, ρZ ), X ∼ Binomial(1, ρX ), Y ∼ Binomial(1, ρY ),

(104)

where probabilities ρZ , ρX , ρY are given by 1
ρZ = 1 + exp(−U1) , 1
ρX = 1 + exp(−Z − U2) , 1
ρY = 1 + exp(X − U2 + 0.5) .

In this case, the counterfactual distribution P (yx) is not identiﬁable from the observational distribution P (X, Y, Z)

(Bareinboim and Pearl 2012). Sharp bounds over P (yx) from P (x, y, z) were derived in (Balke and Pearl 1994) (labelled as opt). We present our results using two different
priors. The ﬁrst is a ﬂat (uniform) distribution over probabilities of U1 and U2 respectively, i.e., αU1 = dU1 = 2 and αU1 = dU2 = 16. The second is skewed to present a strong preference on the deterministic relationships between X and Y ; in this case, α1 = 300 × dUi , i = 1, 2, for prior distributions associated with both U1 and U2. Figs. 9c and 9d shows distributions of P (Yx=0) induced by these two prior distributions (in the absence of any observational
data). We see that the skewed prior of Fig. 9d assigns almost all weights to deterministic events P (Yx=0 = 1) = 1 or P (Yx=0 = 0) = 1.
Fig. 11 shows posterior samples obtained by our Gibbs
sampler when applied to observational data of various sizes,
using both the ﬂat prior (Figs. 11a to 11d) and the skewed prior (Figs. 11e to 11h). Our analysis reveals that 100% cred-
ible intervals of both priors eventually converge to the sharp IV bound l = 0.1468, r = 0.6617 over the unknown interventional probability P (Yx=0 = 1) = 0.3954. It is interesting to note that, in this experiment, while the choice of prior
distribution does not inﬂuence the ﬁnal bound, it still has
an effect on the shape of posterior distributions given ﬁnite
samples of the observational data.

D. Polynomial Optimization for Bounding Counterfactual Probabilities
In this section, we will demonstrate how one could solve the optimization problem in Eq. (2) assuming access to a polynomial optimization oracle.
Recall that Thm. 1 implies that counterfactual distributions P (Yx, . . . , Zw) in any causal diagram G could be generated by a discrete SCM in the canonical family N (G) and be written as follows:

P (yx, . . . , zw) = 1Yx(u)=y · · · 1Zw(u)=z

θu.

u

U ∈U

Among above quantities, for every exogenous U ∈ U , parameters θu are discrete probabilities P (U = u) over a ﬁnite domain {1, . . . , dU } where the cardinality dU = V ∈C(U) |ΩPaV → ΩV |. That is, parameters {θu | ∀u ∈ ΩU } satisfy the following:

θu ∈ [0, 1],

θu = 1.
u∈ΩU

For every V ∈ V , we represent the output of function
fV (paV , uV ) given input paV , uV using an indicator vector µV(paV ,uV ) = µv(paV ,uV ) | ∀v ∈ ΩV such that

µv(paV ,uV ) ∈ {0, 1},

µv(paV ,uV ) = 1.
v∈ΩV

For subsets X, Y ⊆ V , ﬁx constants x, y, u. The indicator function 1Yx(u)=y could be written as a product

1Yx(u)=y =

1Yx(u)=y .

Y ∈Y

For every Y ∈ Y , 1Yx(u)=y is recursively given by:

1y=xY 

1Yx(u)=y = 

µ(ypaY ,uY )1PAY x=paY

pa Y

if Y ∈ X otherwise

The above equations allow us to write any counterfactual probability P (yx, . . . , zw) as a polynomial function of parameters µv(paV ,uV ) and θu. This means that the polynomial optimization in Eq. (2) could be reducible to an equivalent polynomial optimization program. For the remainder of this section, we will illustrate this reduction using various examples in different causal diagrams.

Example 1: IV Consider the “IV” diagram G in Fig. 1a.
We study the problem of bounding counterfactual probabilities P (yx , x, y) ≡ P (Yx = y , X = x, Y = y) from the observational distribution P (X, Y, Z). Formally, let M (G) denote the set of all SCMs compatible with the diagram G. One could obtain the tight bound over P (yx , x, y) from P (X, Y, Z) by solving the optimization problem as follows:

min / max
M ∈M (G)
s.t.

PM (yx , x, y) PM (x, y, z) = P (x, y, z), ∀x, y, z.

(105)

In the above optimization problem, it follows from Thm. 1 that the objective function could be written as

PM (yx , x, y)

d1
=

d2
µy(x ,u2)µ(yx,u2)

µx(z,u2)µz(u1)θu1 θu2 .

u1=1 u2=1

z

Similarly, the observational constraints could be written as:

d1 d2

PM (x, y, z) =

µz(u1)µx(z,u2)µ(yx,u2)θu1 θu2 .

u1=1 u2=1

The above equations imply that Eq. (105) could be reducible to an equivalent polynomial program as follows:

d1 d2

min / max

µ(yx ,u2)µ(yx,u2)

µx(z,u2)µ(zu1)θu1 θu2

u1=1 u2=1

z

subject to

d1 d2
µ(zu1)µ(xz,u2)µ(yx,u2)θu1 θu2
u1=1 u2=1

= P (x, y, z), ∀x, y, z

∀z, u1, µ(zu1) 1 − µ(zu1) = 0,

∀z, u1,

µ(zu1) = 1
z

∀x, z, u2, µx(z,u2) 1 − µx(z,u2) = 0

∀x, z, u2,

µx(z,u2) = 1
x

∀y, x, u2, µy(x,u2) 1 − µ(yx,u2) = 0

∀y, x, u2

µ(yx,u2) = 1
y

∀u1, 0 ≤ θu1 ≤ 1,

θu1 = 1

u1

∀u2, 0 ≤ θu2 ≤ 1,

θu2 = 1

u2

where cardinalities d1, d2 are equal to

d1 = |ΩZ | , d2 = |ΩZ → ΩX | × |ΩX → ΩY | .

Example 2 Consider the causal diagram in Fig. 1b. We study the problem of bounding counterfactual probabilities P (z, xz , yx ) from a combination of the observational distribution P (X, Y, Z) and the interventional distribution {P (Xz, Yz) | ∀z ∈ ΩZ }. That is,

min / max
M ∈M (G)
s.t.

PM (z, xz , yx ) PM (x, y, z) = P (x, y, z), ∀x, y, z

(106)

PM (xz, yz) = P (xz, yz), ∀x, y, z

Among quantities in the above equation, it follows from Thm. 1 that the objective function could be written as

PM (z, xz , yx )

d

=

µz(u1)µx(z ,u2)µy(x ,u1,u2)θu1 θu2 .

u1 ,u2 =1

Similarly, the observational constraints could be written as:

d

PM (x, y, z) =

µ(zu1)µ(xz,u2)µ(yx,u1,u2)θu1 θu2 ,

u1 ,u2 =1

and the interventional constraints imply:

d

PM (xz, yz) =

µ(xz,u2)µ(yx,u1,u2)θu1 θu2 .

u1 ,u2 =1

The above equations imply that one could obtain an optimal bound over P (z, xz , yx ) given by Eq. (106) by solving an equivalent polynomial program as follows:

min / max

d
µ(zu1)µ(xz ,u2)µ(yx ,u1,u2)θu1 θu2
u1 ,u2 =1

d

s.t.

µ(zu1)µ(xz,u2)µ(yx,u1,u2)θu1 θu2

u1 ,u2 =1

= P (x, y, z), ∀x, y, z

d
µ(xz,u2)µ(yx,u1,u2)θu1 θu2
u1 ,u2 =1

= P (xz, yz), ∀x, y, z

∀z, u1, µ(zu1) 1 − µ(zu1) = 0

∀z, u1,

µ(zu1) = 1
z

∀x, z, u1, u2, µ(xz,u1,u2) 1 − µ(xz,u1,u2) = 0

∀x, z, u1, u2,

µ(xz,u1,u2) = 1
x

∀y, x, u2, µ(yx,u2) 1 − µ(yx,u2) = 0

∀y, x, u2,

µ(yx,u2) = 1
y

∀u1, 0 ≤ θu1 ≤ 1,

θu1 = 1

u1

∀u2, 0 ≤ θu2 ≤ 1,

θu2 = 1

u2

where the cardinality d equates to

d = |ΩZ | × |ΩZ → ΩX | × |ΩX → ΩY | .

Example 3: Frontdoor Consider the “Frontdoor” diagram in Fig. 1c. We are interested in evaluating interventional probabilities P (yx) from the observational distribution P (X, Y, W ). That is,

min / max
M ∈M (G)
s.t.

PM (yx) (107)
PM (x, y, w) = P (x, y, w), ∀x, y, w

It follows from Thm. 1 that the objective function in the above optimization problem could be further written as

d1 d2

PM (yx) =

µ(yw,u1)µ(wx,u2)θu1 θu2 .

u1=1 u1=1 w

Similarly, the observational constraints could be written as:

d1 d2

PM (x, y, w) =

µ(xu)µ(yw,u1)µw(x,u2)θu1 θu2 .

u1=1 u1=1

The above equations imply that one could obtain the optimal solution in Eq. (107) by solving an equivalent polynomial optimization problem deﬁned as follows:

min / max subject to

d1 d2
µ(yw,u1)µw(x,u2)θu1 θu2
u1=1 u1=1 w
d1 d2
µ(xu)µ(yw,u1)µ(wx,u2)θu1 θu2
u1=1 u1=1
= P (x, y, w), ∀x, y, w

∀x, u1, µ(xu) 1 − µ(xu) = 0,

∀x, u1,

µx(u) = 1,
x

∀y, w, u1, µ(yw,u1) 1 − µ(yw,u1)

= 0,

∀y, w, u1,

µ(yw,u1) = 1,
y

∀w, x, u2, µ(wx,u2) 1 − µ(wx,uw) = 0,

∀w, x, u2,

µ(wx,uw) = 1,
w

∀u1, 0 ≤ θu1 ≤ 1,

θu1 = 1,

u1

∀u2, 0 ≤ θu2 ≤ 1,

θu2 = 1,

u2

where cardinalities d1, d2 equate to

d1 = |ΩX | × |ΩW → ΩY | , d2 = |ΩX → ΩW | .

Example 4: Bow Consider the “Bow” diagram in Fig. 1d.
We study the problem of bounding counterfactual probabil-
ities P (yx, yx ) ≡ P (Yx = y, Yx=x = y ) from a combination of the observational distribution P (X, Y, Z) and the
interventional distribution {P (Yx) | ∀x ∈ ΩX }, i.e.,

min / max
M ∈M (G)
s.t.

PM (yx, yx )
PM (x, y) = P (x, y), ∀x, y PM (yx) = P (yx), ∀x, y

(108)

Among quantities in the above equation, it follows from Thm. 1 that the objective function could be written as

d
PM (yx, yx ) = µ(yx,u)µ(yx ,u)θu.
u=1

Similarly, the observational constraints could be written as:

d
PM (x, y) = µx(u)µ(yx,u)θu,
u=1

and the interventional constraints are given by:

d
PM (yx) = µ(yx,u)θu.
u=1

The optimization problem deﬁned in Eq. (108) is thus reducible to an equivalent polynomial program as follows:

min / max

d
µ(yx,u)µ(yx ,u)θu
u=1

d

subject to

µ(xu)µ(yx,u)θu = P (x, y), ∀x, y

u=1

d
µ(yx,u)θu = P (yx),
u=1

∀x, y

∀x, u, µ(xu) 1 − µ(xu) = 0

∀x, u,

µ(xu) = 1
x

∀y, x, u, µ(yx,u) 1 − µ(yx,u) = 0

∀y, x, u,

µ(yx,u) = 1
y

∀u, 0 ≤ θu ≤ 1, θu = 1
u

where the cardinality d is equal to |ΩZ → ΩX |.

E. A Na¨ıve Generalization of (Balke and Pearl
1994)
In this section, we will describe a na¨ıve generalization of the discretization procedure introduced in (Balke and Pearl 1994) to the causal diagram of Fig. 12a. In particular, given any SCM M compatible with Fig. 12a, we will construct a discrete SCM N compatible with a different causal diagram described in Fig. 12b such that M and N coincide in all counterfactual distributions P ∗.
We ﬁrst introduce some useful notations. Let fZ , fX , fY denote functions associated with Z, X, Y in SCM M . Let constants h(Z1) = 0 and h(Z2) = 1. Note that given any U1 = u1, fZ (u1) must equate to a binary value in {0, 1}. Therefore, we could deﬁne a partition UZ(i), i = 1, 2, over domains of U1 such that u1 ∈ UZ(i) if and only if

fZ (u1) = h(Zi).

(109)

Given any u2, fX (·, u2) deﬁnes a function mapping from domains of Z to X. Let functions in the hypothesis class ΩZ → ΩX be ordered by

h(X1)(z) = 0, h(X3)(z) = ¬z,

h(X2)(z) = z, h(X4)(z) = 1.

(110)

Similarly, we could deﬁne a partition UX(i), i = 1, 2, 3, 4 over the domain ΩU2 such that u2 ∈ UX(i) if and only if the induced function fX (·, u2) = h(Xi). Finally, let functions in ΩX → ΩY mapping from domains of X to Y be ordered by

h(Y1)(x) = 0, h(Y3)(x) = ¬x,

h(Y2)(x) = x, h(Y4)(x) = 1.

(111)

For any u1, u2, the induced function fY (·, u1, u2) must co-
incide with only of the above elements in the hypothesis
class ΩX → ΩY . Let UY(i), i = 1, 2, 3, 4 be a subset of the product domain ΩU1 × ΩU2 such that (u1, u2) ∈ UY(i) if any only if fY (·, u1, u2) = h(Yi). It is veriﬁable that UY(i), i = 1, 2, 3, 4 must form a partition over ΩU1 × ΩU2 .
We now construct a discrete SCM N compatible with the
causal diagram of Fig. 12b. Let the exogenous variable U
in N be a tuple (UZ , UX , UY ), where UZ ∈ {1, 2}, UX ∈ {1, 2, 3, 4} and UY ∈ {1, 2, 3, 4}. For any uZ , values of Z
are decided by a function as follows:

z ← fZ (uz) = h(ZuZ),

(112)

where h(Z1) = 0 and h(Z2) = 1. Given any input z, uX , values of X are given by

x ← fX (z, uX ) = h(XuX)(z),

(113)

where h(Xi)(z), i = 1, 2, 3, 4, are deﬁned in Eq. (110). Similarly, given any x, uY , values of Y are given by

y ← fY (x, uY ) = h(YuY )(x),

(114)

U1

U2

U

Z

X

Y

(a)

Z

X

Y

(b)

Figure 12: Causal diagrams (a-b) containing a treatment X, an outcome Y , an ancestor Z, and unobserved U s.

where h(Yi)(x), i = 1, 2, 3, 4, are functions deﬁned in Eq. (111). Finally, we deﬁne the exogenous distribution
P (uZ , uX , uY ) in the discrete SCM N as the joint probability over partitions UZ(i), UX(j), UY(k), i = 1, 2, j = 1, 2, 3, 4, k = 1, 2, 3, 4. That is,

PN (UZ = i, UX = j, UY = k) = PM (U1, U2) ∈ UZ(i) ∧ UX(j) ∧ UY(k) .

(115)

It follows from the decomposition in Lem. 3 that N and M must coincide in all counterfactual distributions over binary X, Y, Z. The total cardinality of the exogenous domains in N is |ΩUZ | × |ΩUX | × |ΩUY | = 2 × 4 × 4 = 32.
However, the construction for the reverse direction does not hold true. That is, given an arbitrary discrete N compatible with the causal diagram in Fig. 12b, one may not be able to construct an SCM M compatible with the causal diagram in Fig. 12a such that M and N coincide in all counterfactual distributions. To witness, consider a discrete SCM N where P (UZ = UX ) = 1, i.e., variables UZ and UX are always the same, taking values in {1, 2}. Since in SCM N , values of Z(uZ ) and Xz=1(uX ) are given by
Z(uZ ) = h(ZuZ ) = 0 × 1uZ =1 + 1 × 1uZ =2,
Xz=1(uX ) = hX(uX )(1) = 0 × 1uX =1 + 1 × 1uX =2.
This means that values of counterfactual variables Z and Xz=0 must always coincide, i.e., P (Z = Xx=1) = 1. However, for any SCM M compatible with Fig. 12a, counterfactual variables Z and Xz must be independent due to the independence restriction (Pearl 2000, Ch. 7.3.2), i.e., Z ⊥⊥ Xz, which is a contradiction.

