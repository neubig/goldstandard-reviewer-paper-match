arXiv:1402.0555v2 [cs.LG] 14 Oct 2014

Taming the Monster: A Fast and Simple Algorithm for Contextual Bandits
Alekh Agarwal1, Daniel Hsu2, Satyen Kale3, John Langford1, Lihong Li1, and Robert E. Schapire1,4
1Microsoft Research 2Columbia University
3Yahoo! Labs 4Princeton University
October 15, 2014
Abstract
We present a new algorithm for the contextual bandit learning problem, where the learner repeatedly takes one of K actions in response to the observed context, and observes the reward only for that chosen action. Our method assumes access to an oracle for solving fully supervised cost-sensitive classiﬁcation problems and achieves the statistically optimal regret guarantee with only O˜( KT / log N ) oracle calls across all T rounds, where N is the number of policies in the policy class we compete against. By doing so, we obtain the most practical contextual bandit learning algorithm amongst approaches that work for general policy classes. We further conduct a proof-of-concept experiment which demonstrates the excellent computational and prediction performance of (an online variant of) our algorithm relative to several baselines.
1 Introduction
In the contextual bandit problem, an agent collects rewards for actions taken over a sequence of rounds; in each round, the agent chooses an action to take on the basis of (i) context (or features) for the current round, as well as (ii) feedback, in the form of rewards, obtained in previous rounds. The feedback is incomplete: in any given round, the agent observes the reward only for the chosen action; the agent does not observe the reward for other actions. Contextual bandit problems are found in many important applications such as online recommendation and clinical trials, and represent a natural half-way point between supervised learning and reinforcement learning. The use of features to encode context is inherited from supervised machine learning, while exploration is necessary for good performance as in reinforcement learning.
The choice of exploration distribution on actions is important. The strongest known results (Auer et al., 2002; McMahan and Streeter, 2009; Beygelzimer et al., 2011) provide algorithms that carefully control the exploration distribution to achieve an optimal regret after T rounds of
O KT log(|Π|/δ) ,
with probability at least 1 − δ, relative to a set of policies Π ⊆ AX mapping contexts x ∈ X to actions a ∈ A (where K is the number of actions). The regret is the diﬀerence between the cumulative reward of the best policy in Π and the cumulative reward collected by the algorithm. Because the bound has a mild logarithmic dependence on |Π|, the algorithm can compete with very large policy classes that are likely
1

to yield high rewards, in which case the algorithm also earns high rewards. However, the computational complexity of the above algorithms is linear in |Π|, making them tractable for only simple policy classes.
A sub-linear in |Π| running time is possible for policy classes that can be eﬃciently searched. In this work, we use the abstraction of an optimization oracle to capture this property: given a set of context/reward vector pairs, the oracle returns a policy in Π with maximum total reward. Using such an oracle in an i.i.d. setting (formally deﬁned in Section 2.1), it is possible to create ǫ-greedy (Sutton and Barto, 1998) or epoch-greedy (Langford and Zhang, 2007) algorithms that run in time O(log |Π|) with only a single call to the oracle per round. However, these algorithms have suboptimal regret bounds of O((K log |Π|)1/3T 2/3) because the algorithms randomize uniformly over actions when they choose to explore.
The Randomized UCB algorithm of Dud´ık et al. (2011a) achieves the optimal regret bound (up to logarithmic factors) in the i.i.d. setting, and runs in time poly(T, log |Π|) with O˜(T 5) calls to the optimization oracle per round. Naively this would amount to O˜(T 6) calls to the oracle over T rounds, although a doubling trick from our analysis can be adapted to ensure only O˜(T 5) calls to the oracle are needed over all T rounds in the Randomized UCB algorithm. This is a fascinating result because it shows that the oracle can provide an exponential speed-up over previous algorithms with optimal regret bounds. However, the running time of this algorithm is still prohibitive for most natural problems owing to the O˜(T 5) scaling.
In this work, we prove the following1:

Theorem 1. There is an algorithm for the i.i.d. contextual bandit problem with an optimal regret bound

requiring O˜

KT ln(|Π|/δ)

calls to the optimization oracle over T rounds, with probability at least 1 − δ.

Concretely, we make O˜( KT / ln(|Π|/δ)) calls to the oracle with a net running time of O˜(T 1.5 K log |Π|), vastly improving over the complexity of Randomized UCB. The major components of the new algorithm are (i) a new coordinate descent procedure for computing a very sparse distribution over policies which can be eﬃciently sampled from, and (ii) a new epoch structure which allows the distribution over policies to be updated very infrequently. We consider variants of the epoch structure that make diﬀerent computational trade-oﬀs; on one extreme we concentrate the entire computational burden on O(log T ) round√s with O˜( KT / ln(|Π|/δ)) oracle calls each time, while on the other we spread our computation over T rounds with O˜( K/ ln(|Π|/δ)) oracle calls for each of these rounds. We stress that in either case, the total number of calls to the oracle is only sublinear in T . Finally, we develop a more eﬃcient online variant, and conduct a proof-of-concept experiment showing low computational complexity and high reward relative to several natural baselines.

Motivation and related work. The EXP4-family of algorithms (Auer et al., 2002; McMahan and Streeter, 2009; Beygelzimer et al., 2011) solve the contextual bandit problem with optimal regret by updating weights (multiplicatively) over all policies in every round. Except for a few special cases (Helmbold and Schapire, 1997; Beygelzimer et al., 2011), the running time of such measure-based algorithms is generally linear in the number of policies.
In contrast, the Randomized UCB algorithm of Dud´ık et al. (2011a) is based on a natural abstraction from supervised learning—the ability to eﬃciently ﬁnd a function in a rich function class that minimizes the loss on a training set. This abstraction is encapsulated in the notion of an optimization oracle, which is also useful for ǫ-greedy (Sutton and Barto, 1998) and epoch-greedy (Langford and Zhang, 2007) algorithms. However, these latter algorithms have only suboptimal regret bounds.
Another class of approaches based on Bayesian updating is Thompson sampling (Thompson, 1933; Li, 2013), which often enjoys strong theoretical guarantees in expectation over the prior and good empirical performance (Chapelle and Li, 2011). Such algorithms, as well as the closely related upper-conﬁdence bound algorithms (Auer, 2002; Chu et al., 2011), are computationally tractable in cases where the posterior distribution over policies can be eﬃciently maintained or approximated. In our experiments, we compare to a strong baseline algorithm that uses this approach (Chu et al., 2011).
1Throughout this paper, we use the O˜ notation to suppress dependence on logarithmic factors in T and K, as well as log(|Π|/δ) (i.e. terms which are O(log log(|Π|/δ)).

2

To circumvent the Ω(|Π|) running time barrier, we restrict attention to algorithms that only access the policy class via the optimization oracle. Speciﬁcally, we use a cost-sensitive classiﬁcation oracle, and a key challenge is to design good supervised learning problems for querying this oracle. The Randomized UCB algorithm of Dud´ık et al. (2011a) uses a similar oracle to construct a distribution over policies that solves a certain convex program. However, the number of oracle calls in their work is prohibitively large, and the statistical analysis is also rather complex.2
Main contributions. In this work, we present a new and simple algorithm for solving a similar convex program as that used by Randomized UCB. The new algorithm is based on coordinate descent: in each iteration, the algorithm calls the optimization oracle to obtain a policy; the output is a sparse distribution over these policies. The number of iterations required to compute the distribution is small—at most O˜( Kt/ ln(|Π|/δ)) in any round t. In fact, we present a more general scheme based on epochs and warm start in which the total number of calls to the oracle is, with high probability, just O˜( KT / ln(|Π|/δ)) over all T rounds; we prove that this is nearly optimal for a certain class of optimization-based algorithms. The algorithm is natural and simple to implement, and we provide an arguably simpler analysis than that for Randomized UCB. Finally, we report proof-of-concept experimental results using a variant algorithm showing strong empirical performance.

2 Preliminaries
In this section, we recall the i.i.d. contextual bandit setting and some basic techniques used in previous works (Auer et al., 2002; Beygelzimer et al., 2011; Dud´ık et al., 2011a).

2.1 Learning Setting
Let A be a ﬁnite set of K actions, X be a space of possible contexts (e.g., a feature space), and Π ⊆ AX be a ﬁnite set of policies that map contexts x ∈ X to actions a ∈ A.3 Let ∆Π := {Q ∈ RΠ : Q(π) ≥ 0 ∀π ∈ Π, π∈Π Q(π) ≤ 1} be the set of non-negative weights over policies with total weight at most one, and let RA+ := {r ∈ RA : r(a) ≥ 0 ∀a ∈ A} be the set of non-negative reward vectors.
Let D be a probability distribution over X × [0, 1]A, the joint space of contexts and reward vectors; we assume actions’ rewards from D are always in the interval [0, 1]. Let DX denote the marginal distribution of D over X.
In the i.i.d. contextual bandit setting, the context/reward vector pairs (xt, rt) ∈ X × [0, 1]A over all rounds t = 1, 2, . . . are randomly drawn independently from D. In round t, the agent ﬁrst observes the context xt, then (randomly) chooses an action at ∈ A, and ﬁnally receives the reward rt(at) ∈ [0, 1] for the chosen action. The (observable) record of interaction resulting from round t is the quadruple (xt, at, rt(at), pt(at)) ∈ X × A × [0, 1] × [0, 1]; here, pt(at) ∈ [0, 1] is the probability that the agent chose action at ∈ A. We let Ht ⊆ X × A × [0, 1] × [0, 1] denote the history (set) of interaction records in the
ﬁrst t rounds. We use the shorthand notation Ex∼Ht [·] to denote expectation when a context x is chosen from the t contexts in Ht uniformly at random.
Let R(π) := E(x,r)∼D[r(π(x))] denote the expected (instantaneous) reward of a policy π ∈ Π, and let π⋆ := arg maxπ∈Π R(π) be a policy that maximizes the expected reward (the optimal policy). Let Reg(π) := R(π⋆) − R(π) denote the expected (instantaneous) regret of a policy π ∈ Π relative to the optimal policy. Finally, the (empirical cumulative) regret of the agent after T rounds4 is deﬁned as

T
rt(π⋆(xt)) − rt(at) .
t=1

2The paper of Dud´ık et al. (2011a) is colloquially referred to, by its authors, as the “monster paper” (Langford, 2014).

3Extension to VC classes is simple using standard arguments.

4We have deﬁned empirical cumulative regret as being relative to π⋆, rather than to the empirical reward maximizer

arg maxπ∈Π

T t=1

rt

(π(xt

)).

However, in the i.i.d. setting, the two do not diﬀer by more than O(

T ln(|Π|/δ)) with

probability at least 1 − δ.

3

2.2 Inverse Propensity Scoring

An unbiased estimate of a policy’s reward may be obtained from a history of interaction records Ht using

inverse propensity scoring (IPS; also called inverse probability weighting): the expected reward of policy

π ∈ Π is estimated as

1 t ri(ai) · 1{π(xi) = ai}

Rt(π) := t

pi(ai) .

(1)

i=1

This technique can be viewed as mapping Ht → IPS(Ht) of interaction records (x, a, r(a), p(a)) to context/reward vector pairs (x, rˆ), where rˆ ∈ RA+ is a ﬁctitious reward vector that assigns to the chosen action a a scaled reward r(a)/p(a) (possibly greater than one), and assigns to all other actions zero
rewards. This transformation IPS(Ht) is detailed in Algorithm 3 (in Appendix A); we may equivalently deﬁne Rt by Rt(π) := t−1 (x,rˆ)∈IPS(Ht) rˆ(π(x)). It is easy to verify that E[rˆ(π(x))|(x, r)] = r(π(x)), as
p(a) is indeed the agent’s probability (conditioned on (x, r)) of picking action a. This implies Rt(π) is an unbiased estimator for any history Ht.
Let πt := arg maxπ∈Π Rt(π) denote a policy that maximizes the expected reward estimate based
on inverse propensity scoring with history Ht (π0 can be arbitrary), and let Regt(π) := Rt(πt) − Rt(π)
denote estimated regret relative to πt. Note that Regt(π) is generally not an unbiased estimate of Reg(π), because πt is not always π⋆.

2.3 Optimization Oracle
One natural mode for accessing the set of policies Π is enumeration, but this is impractical in general. In this work, we instead only access Π via an optimization oracle which corresponds to a cost-sensitive learner. Following Dud´ık et al. (2011a), we call this oracle AMO5.
Deﬁnition 1. For a set of policies Π, the arg max oracle (AMO) is an algorithm, which for any sequence of context and reward vectors, (x1, r1), (x2, r2), . . . , (xt, rt) ∈ X × RA+, returns
t
arg max rτ (π(xτ )).
π∈Π τ =1

2.4 Projections and Smoothing

In each round, our algorithm chooses an action by randomly drawing a policy π from a distribution over

Π, and then picking the action π(x) recommended by π on the current context x. This is equivalent

to drawing an action according to Q(a|x) := π∈Π:π(x)=a Q(π), ∀a ∈ A. For keeping the variance of reward estimates from IPS in check, it is desirable to prevent the probability of any action from being

too small. Thus, as in previous work, we also use a smoothed projection Qµ(·|x) for µ ∈ [0, 1/K],

Qµ(a|x) := (1 − Kµ) Qµ(·|x).

π∈Π:π(x)=a Q(π) + µ, ∀a ∈ A. Every action has probability at least µ under

For technical reasons, our algorithm maintains non-negative weights Q ∈ ∆Π over policies that sum

to at most one, but not necessarily equal to one; hence, we put any remaining mass on a default policy π¯ ∈ Π to obtain a legitimate probability distribution over policies Q˜ = Q + 1 − π∈Π Q(π) 1π¯. We then pick an action from the smoothed projection Q˜µ(·|x) of Q˜ as above. This sampling procedure

Sample(x, Q, π¯, µ) is detailed in Algorithm 4 (in Appendix A).

3 Algorithm and Main Results
Our algorithm (ILOVETOCONBANDITS) is an epoch-based variant of the Randomized UCB algorithm of Dud´ık et al. (2011a) and is given in Algorithm 1. Like Randomized UCB, ILOVETOCONBANDITS solves
5Cost-sensitive learners often need a cost instead of reward, in which case we use ct = 1 − rt.

4

an optimization problem (OP) to obtain a distribution over policies to sample from (Step 7), but does so on an epoch schedule, i.e., only on certain pre-speciﬁed rounds τ1, τ2, . . .. The only requirement of the epoch schedule is that the length of epoch m is bounded as τm+1 − τm = O(τm). For simplicity, we assume τm+1 ≤ 2τm for m ≥ 1, and τ1 = O(1).
The crucial step here is solving (OP). Before stating the main result, let us get some intuition about this problem. The ﬁrst constraint, Eq. (2), requires the average estimated regret of the distribution Q over policies to be small, since bπ is a rescaled version of the estimated regret of policy π. This constraint skews our distribution to put more mass on “good policies” (as judged by our current information), and can be seen as the exploitation component of our algorithm. The second set of constraints, Eq. (3), requires the distribution Q to place suﬃcient mass on the actions chosen by each policy π, in expectation over contexts. This can be thought of as the exploration constraint, since it requires the distribution to be suﬃciently diverse for most contexts. As we will see later, the left hand side of the constraint is a bound on the variance of our reward estimates for policy π, and the constraint requires the variance to be controlled at the level of the estimated regret of π. That is, we require the reward estimates to be more accurate for good policies than we do for bad ones, allowing for much more adaptive exploration than the uniform exploration of ǫ-greedy style algorithms.
This problem is very similar to the one in Dud´ık et al. (2011a), and our coordinate descent algorithm in Section 3.1 gives a constructive proof that the problem is feasible. As in Dud´ık et al. (2011a), we have the following regret bound:
Theorem 2. Assume the optimization problem ( OP) can be solved whenever required in Algorithm 1. With probability at least 1 − δ, the regret of Algorithm 1 (ILOVETOCONBANDITS) after T rounds is
O KT ln(T |Π|/δ) + K ln(T |Π|/δ) .

Algorithm 1 Importance-weighted LOw-Variance Epoch-Timed Oracleized CONtextual BANDITS algorithm (ILOVETOCONBANDITS)

input Epoch schedule 0 = τ0 < τ1 < τ2 < · · · , allowed failure probability δ ∈ (0, 1). 1: Initial weights Q0 := 0 ∈ ∆Π, initial epoch m := 1.

Deﬁne µm := min{1/2K, ln(16τm2 |Π|/δ)/(Kτm)} for all m ≥ 0.

2: for round t = 1, 2, . . . do

3: Observe context xt ∈ X.

4: (at, pt(at)) := Sample(xt, Qm−1, πτm−1, µm−1).

5: Select action at and observe reward rt(at) ∈ [0, 1].

6: if t = τm then

7:

Let Qm be a solution to (OP) with history Ht and minimum probability µm.

8:

m := m + 1.

9: end if

10: end for

Optimization Problem (OP) Given a history Ht and minimum probability µm, deﬁne bπ := Rψegµtm (π) for ψ := 100, and ﬁnd Q ∈ ∆Π such that

Q(π)bπ ≤ 2K

(2)

π∈Π

∀π ∈ Π : Ex∼Ht Qµm (π1(x)|x) ≤ 2K + bπ. (3)

5

3.1 Solving (OP) via Coordinate Descent
We now present a coordinate descent algorithm to solve (OP). The pseudocode is given in Algorithm 2. Our analysis, as well as the algorithm itself, are based on a potential function which we use to measure progress. The algorithm can be viewed as a form of coordinate descent applied to this same potential function. The main idea of our analysis is to show that this function decreases substantially on every iteration of this algorithm; since the function is nonnegative, this gives an upper bound on the total number of iterations as expressed in the following theorem.
Theorem 3. Algorithm 2 (with Qinit := 0) halts in at most 4 ln(1/µ(mKµm)) iterations, and outputs a solution Q to ( OP).

Algorithm 2 Coordinate Descent Algorithm
Require: History Ht, minimum probability µ, initial weights Qinit ∈ ∆Π. 1: Set Q := Qinit. 2: loop 3: Deﬁne, for all π ∈ Π,

Vπ(Q) = Ex∼Ht [1/Qµ(π(x)|x)] Sπ(Q) = Ex∼Ht 1/(Qµ(π(x)|x))2 Dπ(Q) = Vπ(Q) − (2K + bπ).

4: if π Q(π)(2K + bπ) > 2K then

5:

Replace Q by cQ, where

c := 2K < 1. (4)

π Q(π)(2K + bπ)

6: end if

7: if there is a policy π for which Dπ(Q) > 0 then

8:

Add the (positive) quantity

απ(Q) = Vπ(Q) + Dπ(Q) 2(1 − Kµ)Sπ(Q)

to Q(π) and leave all other weights unchanged.

9: else

10:

Halt and output the current set of weights Q.

11: end if

12: end loop

3.2 Using an Optimization Oracle
We now show how to implement Algorithm 2 via AMO (c.f. Section 2.3).
Lemma 1. Algorithm 2 can be implemented using one call to AMO before the loop is started, and one call for each iteration of the loop thereafter.
Proof. At the very beginning, before the loop is started, we compute the best empirical policy so far, πt, by calling AMO on the sequence of historical contexts and estimated reward vectors; i.e., on (xτ , rˆτ ), for τ = 1, 2, . . . , t.
Next, we show that each iteration in the loop of Algorithm 2 can be implemented via one call to AMO. Going over the pseudocode, ﬁrst note that operations involving Q in Step 4 can be performed eﬃciently since Q has sparse support. Note that the deﬁnitions in Step 3 don’t actually need to be computed for all policies π ∈ Π, as long as we can identify a policy π for which Dπ(Q) > 0. We can identify such a policy using one call to AMO as follows.
6

First, note that for any policy π, we have

1 Vπ(Q) = Ex∼Ht Qµ(π(x)|x)

1t

1

= t Qµ(π(xτ )|xτ ) ,

τ =1

and

b = Regt(π) = Rt(πt) − 1

t
rˆ (π(x )).

π

ψµ

ψµ

ψµt

τ

τ

τ =1

Now consider the sequence of historical contexts and reward vectors, (xτ , r˜τ ) for τ = 1, 2, . . . , t, where

for any action a we deﬁne

1 ψµ

r˜τ (a) := t Qµ(a|xτ ) + rˆτ (a) .

(5)

It is easy to check that

1t

Rt(πt)

Dπ(Q) = ψµ r˜τ (π(xτ )) − 2K + ψµ .

τ =1

Since

2K

+

Rt (πt ) ψµ

is

a

constant

independent

of

π,

we

have

t

arg max Dπ(Q) = arg max r˜τ (π(xτ )),

π∈Π

π∈Π τ =1

and hence, calling AMO once on the sequence (xτ , r˜τ ) for τ = 1, 2, . . . , t, we obtain a policy that maximizes Dπ(Q), and thereby identify a policy for which Dπ(Q) > 0 whenever one exists.

3.3 Epoch Schedule
Recalling the setting of µm in Algorithm 1, Theorem 3 shows that Algorithm 2 solves (OP) with O˜( Kt/ ln(|Π|/δ)) calls to AMO in round t. Thus, if we use the epoch schedule τm = m (i.e., run Algorithm 2 in every round), then we get a total of O˜( KT 3/ ln(|Π|/δ)) calls to AMO over all T rounds. This number can be dramatically reduced using a more carefully chosen epoch schedule.
Lemma 2. For the epoch schedule τm := 2m−1, the total number of calls to AMO is O˜( KT / ln(|Π|/δ)).
Proof. The epoch schedule satisﬁes the requirement τm+1 ≤ 2τm. With this epoch schedule, Algorithm 2 is run only O(log T ) times over T rounds, leading to O˜( KT / ln(|Π|/δ)) total calls to AMO over the entire period.

3.4 Warm Start

We now present a diﬀerent technique to reduce the number of calls to AMO. This is based on the

observation that practically speaking, it seems terribly wasteful, at the start of a new epoch, to throw out

the results of all of the preceding computations and to begin yet again from nothing. Instead, intuitively,

we expect computations to be more moderate if we begin again where we left oﬀ last, i.e., a “warm-start”

approach. Here, when Algorithm 2 is called at the end of epoch m, we use Qinit := Qm−1 (the previously

computed weights) rather than 0. We can combine warm-start√with a diﬀerent epoch schedule to guarantee O˜(
calls to AMO, spread across O( T ) calls to Algorithm 2.

KT / ln(|Π|/δ)) total

Lemma 3. Deﬁne the epoch schedule (τ1, τ2) := (3, 5) and τm := m2 for m ≥ 3 (this satisﬁes τm+1 ≤ 2τm). With high probability,√the warm-start variant of Algorithm 1 makes O˜( KT / ln(|Π|/δ)) calls to AMO over T rounds and O( T ) calls to Algorithm 2.

7

3.5 Computational Complexity
So far, we have only considered computational complexity in terms of the number of oracle calls. However, the reduction also involves the creation of cost-sensitive classiﬁcation examples, which must be accounted for in the net computational cost. As observed in the proof of Lemma 1 (speciﬁcally Eq. (5)), this requires the computation of the probabilities Qµ(a|xτ ) for τ = 1, 2, . . . , t when the oracle has to be invoked at round t. According to Lemma 3, the support of the distribution Q at time t can be over at most O˜( Kt/ ln(|Π|/δ)) policies (same as the number of calls to AMO). This would suggest a computational complexity of O˜( Kt3/ ln(|Π|/δ)) for querying the oracle at time t, resulting in an overall computation cost scaling with T 2.
We can, however, do better with some natural bookkeeping. Observe that at the start of round t, the conditional distributions Q(a|xi) for i = 1, 2, . . . , t − 1 can be represented as a table of size K × (t − 1), where rows and columns correspond to actions and contexts. Upon receiving the new example in round t, the corresponding t-th column can be added to this table in time K · |supp(Q)| = O˜(K Kt/ ln(|Π|/δ)) (where supp(Q) ⊆ Π denotes the support of Q), using the projection operation described in Section 2.4. Hence the net cost of these updates, as a function of K and T , scales with as (KT )3/2. Furthermore, the cost-sensitive examples needed for the AMO can be obtained by a simple table lookup now, since the action probabilities are directly available. This involves O(Kt) table lookups when the oracle is invoked at time t, and again results in an overall cost scaling as (KT )3/2. Finally, we have to update the table when the distribution Q is updated in Algorithm 2. If we ﬁnd ourselves in the rescaling step 4, we can simply store the constant c. When we enter step 8 of the algorithm, we can do a linear scan over the table, rescaling and incrementing the entries. This also resutls in a cost of O(Kt) when the update happens at time t, resulting in a net scaling as (KT )3/2. Overall, we ﬁnd that the computational complexity of our algorithm, modulo the oracle running time, is O˜( (KT )3/ ln(|Π|/δ)).

3.6 A Lower Bound on the Support Size
An attractive feature of the coordinate descent algorithm, Algorithm 2, is that the number of oracle calls is directly related to the number of policies in the support of Qm. Speciﬁcally, for the doubling schedule of Section 3.3, Theorem 3 implies that we never have non-zero weights for more than 4 ln(1/µ(mKµm)) policies in epoch m. Similarly, the total number of oracle calls for the warm-start approach in Section 3.4 bounds the total number of policies which ever have non-zero weight over all T rounds. The support size of the distributions Qm in Algorithm 1 is crucial to the computational complexity of sampling an action (Step 4 of Algorithm 1).
In this section, we demonstrate a lower bound showing that it is not possible to construct substantially sparser distributions that also satisfy the low-variance constraint (3) in the optimization problem (OP). To formally deﬁne the lower bound, ﬁx an epoch schedule 0 = τ0 < τ1 < τ2 < · · · and consider the following set of non-negative vectors over policies:
Qm :={Q ∈ ∆Π : Q satisﬁes Eq. (3) in round τm}.
(The distribution Qm computed by Algorithm 1 is in Qm.) Recall that supp(Q) denotes the support of Q (the set of policies where Q puts non-zero entries). We have the following lower bound on |supp(Q)|.
Theorem 4. For any epoch schedule 0 = τ0 < τ1 < τ2 < · · · and any M ∈ N suﬃciently large, there exists a distribution D over X × [0, 1]A and a policy class Π such that, with probability at least 1 − δ,

inf inf |supp(Q)| = Ω
m∈N: Q∈Qm τm≥τM /2

K τM

.

ln(|Π|τM /δ)

The proof of the theorem is deferred to Appendix E. In the context of our problem, this lower bound shows that the bounds in Lemma 2 and Lemma 3 are unimprovable, since the number of calls to AMO is at least the size of the support, given our mode of access to Π.
8

4 Regret Analysis
In this section, we outline the regret analysis for our algorithm ILOVETOCONBANDITS, with details deferred to Appendix B and Appendix C.
The deviations of the policy reward estimates Rt(π) are controlled by (a bound on) the variance of each term in Eq. (1): essentially the left-hand side of Eq. (3) from (OP), except with Ex∼Ht [·] replaced by Ex∼DX [·]. Resolving this discrepancy is handled using deviation bounds, so Eq. (3) holds with Ex∼DX [·], with worse right-hand side constants.
The rest of the analysis, which deviates from that of Randomized UCB, compares the expected regret Reg(π) of any policy π with the estimated regret Regt(π) using the variance constraints Eq. (3): Lemma 4 (Informally). With high probability, for each m such that τm ≥ O˜(K log |Π|), each round t in epoch m, and each π ∈ Π, Reg(π) ≤ 2Regt(π) + O(Kµm).
This lemma can easily be combined with the constraint Eq. (2) from (OP): since the weights Qm−1 used in any round t in epoch m satisfy π∈Π Qm−1(π)Regτm−1(π) ≤ ψ · 2Kµτm−1, we obtain a bound on the (conditionally) expected regret in round t using the above lemma: with high probability,
Qm−1 Reg(π) ≤ O(Kµm−1).
π∈Π
Summing these terms up over all T rounds and applying martingale concentration gives the ﬁnal regret bound in Theorem 2.

5 Analysis of the Optimization Algorithm
In this section, we give a sketch of the analysis of our main optimization algorithm for computing weights Qm on each epoch as in Algorithm 2. As mentioned in Section 3.1, this analysis is based on a potential function.
Since our attention for now is on a single epoch m, here and in what follows, when clear from context, we drop m from our notation and write simply τ = τm, µ = µm, etc. Let UA be the uniform distribution over the action set A. We deﬁne the following potential function for use on epoch m:

Φm(Q) = τ µ Ex[RE (UA Qµ(· | x))] + π∈Π Q(π)bπ .

(6)

1 − Kµ

2K

The function in Eq. (6) is deﬁned for all vectors Q ∈ ∆Π. Also, RE (p q) denotes the unnormalized relative entropy between two nonnegative vectors p and q over the action space (or any set) A:

RE (p q) = (pa ln(pa/qa) + qa − pa).
a∈A
This number is always nonnegative. Here, Qµ(·|x) denotes the “distribution” (which might not sum to 1) over A induced by Qµ for context x as given in Section 2.4. Thus, ignoring constants, this potential function is a combination of two terms: The ﬁrst measures how far from uniform are the distributions induced by Qµ, and the second is an estimate of expected regret under Q since bπ is proportional to the empirical regret of π. Making Φm small thus encourages Q to choose actions as uniformly as possible while also incurring low regret — exactly the aims of our algorithm. The constants that appear in this deﬁnition are for later mathematical convenience.
For further intuition, note that, by straightforward calculus, the partial derivative ∂Φm/∂Q(π) is roughly proportional to the variance constraint for π given in Eq. (3) (up to a slight mismatch of constants). This shows that if this constraint is not satisﬁed, then ∂Φm/∂Q(π) is likely to be negative, meaning that Φm can be decreased by increasing Q(π). Thus, the weight vector Q that minimizes Φm satisﬁes the variance constraint for every policy π. It turns out that this minimizing Q also satisﬁes the

9

low regret constraint in Eq. (2), and also must sum to at most 1; in other words, it provides a complete solution to our optimization problem. Algorithm 2 does not fully minimize Φm, but it is based roughly on coordinate descent. This is because in each iteration one of the weights (coordinate directions) Q(π) is increased. This weight is one whose corresponding partial derivative is large and negative.
To analyze the algorithm, we ﬁrst argue that it is correct in the sense of satisfying the required constraints, provided that it halts.

Lemma 5. If Algorithm 2 halts and outputs a weight vector Q, then the constraints Eq. (3) and Eq. (2) must hold, and furthermore the sum of the weights Q(π) is at most 1.

The proof is rather straightforward: Following Step 4, Eq. (2) must hold, and also the weights must sum to 1. And if the algorithm halts, then Dπ(Q) ≤ 0 for all π, which is equivalent to Eq. (3).
What remains is the more challenging task of bounding the number of iterations until the algorithm
does halt. We do this by showing that signiﬁcant progress is made in reducing Φm on every iteration. To begin, we show that scaling Q as in Step 4 cannot cause Φm to increase.

Lemma 6. Let Q be a weight vector such that Φm(cQ) ≤ Φm(Q).

π Q(π)(2K + bπ) > 2K, and let c be as in Eq. (4). Then

Proof sketch. We consider Φm(cQ) as a function of c, and argue that its derivative (with respect to c) at the value of c given in the lemma statement is always nonnegative. Therefore, by convexity, it is nondecreasing for all values exceeding c. Since c < 1, this proves the lemma.
Next, we show that substantial progress will be made in reducing Φm each time that Step 8 is executed.

Lemma 7. Let Q denote a set of weights and suppose, for some policy π, that Dπ(Q) > 0. Let Q′ be a new set of weights which is an exact copy of Q except that Q′(π) = Q(π) + α where α = απ(Q) > 0.

Then

Φm(Q) − Φm(Q′) ≥ τ µ2 .

(7)

4(1 − Kµ)

Proof sketch. We ﬁrst compute exactly the change in potential for general α. Next, we apply a second-
order Taylor approximation, which is maximized by the α used in the algorithm. The Taylor approximation, for this α, yields a lower bound which can be further simpliﬁed using the fact that Qµ(a|x) ≥ µ
always, and our assumption that Dπ(Q) > 0. This gives the bound stated in the lemma. So Step 4 does not cause Φm to increase, and Step 8 causes Φm to decrease by at least the amount
given in Lemma 7. This immediately implies Theorem 3: for Qinit = 0, the initial potential is bounded by τ µ ln(1/(Kµ))/(1 − Kµ), and it is never negative, so the number of times Step 8 is executed is bounded
by 4 ln(1/(Kµ))/µ as required.

5.1 Epoching and Warm Start
As shown in Section 2.3, the bound on the number of iterations of the algorithm from Theorem 3 also gives a bound on the number of times the oracle is called. To reduce the number of oracle calls, one approach is the “doubling trick” of Section 3.3, which enables us to bound the total combined number of iterations of Algorithm 2 in the ﬁrst T rounds is only O˜( KT / ln(|Π|/δ)). This means that the average number of calls to the arg-max oracle is only O˜( K/(T ln(|Π|/δ))) per round, meaning that the oracle is called far less than once per round, and in fact, at a vanishingly low rate.
We now turn to warm-start approach of Section 3.4, where in each epoch m + 1 we initialize the coordinate descent algorithm with Qinit = Qm, i.e. the weights computed in the previous epoch m. To analyze this, we bound how much the potential changes from Φm(Qm) at the end of epoch m to Φm+1(Qm) at the very start of epoch m + 1. This, combined with our earlier results regarding how quickly Algorithm 2 drives down the potential, we are able to get an overall bound on the total number of updates across T rounds.

10

Table 1: Progressive validation loss, best hyperparameter values, and running times of various algorithm on RCV1.

Algorithm P.V. Loss Searched Seconds

ǫ-greedy 0.148 0.1 = ǫ 17

Explore-ﬁrst 0.081
2 × 105 ﬁrst 2.6

Bagging 0.059
16 bags 275

LinUCB
0.128 103 dim, minibatch-10
212 × 103

Online Cover 0.053
cover n = 1 12

Supervised 0.051
nothing 5.3

Lemma 8. Let M be the largest integer for which τM+1 ≤ T . With probability at least 1 − 2δ, for all T , the total epoch-to-epoch increase in potential is

M
(Φm+1(Qm) − Φm(Qm)) ≤ O˜
m=1

T ln(|Π|/δ) , K

where M is the largest integer for which τM+1 ≤ T .

Proof sketch. The potential function, as written in Eq. (6), naturally breaks into two pieces whose epoch-to-epoch changes can be bounded separately. Changes aﬀecting the relative entropy term on the left can be bounded, regardless of Qm, by taking advantage of the manner in which these distributions are smoothed. For the other term on the right, it turns out that these epoch-to-epoch changes are related to statistical quantities which can be bounded with high probability. Speciﬁcally, the total change in this term is related ﬁrst to how the estimated reward of the empirically best policy compares to the expected reward of the optimal policy; and second, to how the reward received by our algorithm compares to that of the optimal reward. From our regret analysis, we are able to show that both of these quantities will be small with high probability.
This lemma, along with Lemma 7 can be used to further establish Lemma 3. We only provide an intuitive sketch here, with the details deferred to the appendix. As we observe in Lemma 8, the total amount that the potential increases across T rounds is at most O˜( T ln(|Π|/δ)/K). On the other hand, Lemma 7 shows that each time Q is updated by Algorithm 2 the potential decreases by at least Ω˜ (ln(|Π|/δ)/K) (using our choice of µ). Therefore, the total number of updates of the algorithm totaled over all T rounds is at most O˜( KT / ln(|Π|/δ)). For instan√ce, if we use (τ1, τ2) := (3, 5) and τm := m2 for m ≥ 3, then the weight vector Q is only updated about T times in T rounds, and on each of those rounds, Algorithm 2 requires O˜( K/ ln(|Π|/δ)) iterations, on average, giving the claim in Lemma 3.

6 Experimental Evaluation
In this section we evaluate a variant of Algorithm 1 against several baselines. While Algorithm 1 is signiﬁcantly more eﬃcient than many previous approaches, the overall computational complexity is still at least O˜((KT )1.5) plus the total cost of the oracle calls, as discussed in Section 3.5. This is markedly larger than the complexity of an ordinary supervised learning problem where it is typically possible to perform an O(1)-complexity update upon receiving a fresh example using online algorithms.
A natural solution is to use an online oracle that is stateful and accepts examples one by one. An online cost-sensitive classiﬁcation (CSC) oracle takes as input a weighted example and returns a predicted class (corresponding to one of K actions in our setting). Since the oracle is stateful, it remembers and uses examples from all previous calls in answering questions, thereby reducing the complexity of each oracle invocation to O(1) as in supervised learning. Using several such oracles, we can eﬃciently track a distribution over good policies and sample from it. We detail this approach (which we call Online Cover) in the full version of the paper. The algorithm maintains a uniform distribution over a ﬁxed number n of policies where n is a parameter of the algorithm. Upon receiving a fresh example, it updates all n policies with the suitable CSC examples (Eq. (5)). The speciﬁc CSC oracle we use is a reduction to

11

squared-loss regression (Algorithms 4 and 5 of Beygelzimer and Langford (2009)) which is amenable to online updates. Our implementation is included in Vowpal Wabbit.6
Due to lack of public datasets for contextual bandit problems, we use a simple supervised-to-contextualbandit transformation (Dud´ık et al., 2011b) on the CCAT document classiﬁcation problem in RCV1 (Lewis et al., 2004). This dataset has 781265 examples and 47152 TF-IDF features. We treated the class labels as actions, and one minus 0/1-loss as the reward. Our evaluation criteria is progressive validation (Blum et al., 1999) on 0/1 loss. We compare several baseline algorithms to Online Cover; all algorithms take advantage of linear representations which are known to work well on this dataset. For each algorithm, we report the result for the best parameter settings (shown in Table 6).
1. ǫ-greedy (Sutton and Barto, 1998) explores randomly with probability ǫ and otherwise exploits.
2. Explore-ﬁrst is a variant that begins with uniform exploration, then switches to an exploit-only phase.
3. A less common but powerful baseline is based on bagging: multiple predictors (policies) are trained with examples sampled with replacement. Given a context, these predictors yield a distribution over actions from which we can sample.
4. LinUCB (Auer, 2002; Chu et al., 2011) has been quite eﬀective in past evaluations (Li et al., 2010; Chapelle and Li, 2011). It is impractical to run “as is” due to high-dimensional matrix inversions, so we report results for this algorithm after reducing to 1000 dimensions via random projections. Still, the algorithm required 59 hours7. An alternative is to use diagonal approximation to the covariance, which runs substantially faster (≈1 hour), but gives a worse error of 0.137.
5. Finally, our algorithm achieves the best loss of 0.0530. Somewhat surprisingly, the minimum occurs for us with a cover set of size 1—apparently for this problem the small decaying amount of uniform random sampling imposed is adequate exploration. Prediction performance is similar with a larger cover set.
All baselines except for LinUCB are implemented as a simple modiﬁcation of Vowpal Wabbit. All reported results use default parameters where not otherwise speciﬁed. The contextual bandit learning algorithms all use a doubly robust reward estimator instead of the importance weighted estimators used in our analysis Dud´ık et al. (2011b).
Because RCV1 is actually a fully supervised dataset, we can apply a fully supervised online multiclass algorithm to solve it. We use a simple one-against-all implementation to reduce this to binary classiﬁcation, yielding an error rate of 0.051 which is competitive with the best previously reported results. This is eﬀectively a lower bound on the loss we can hope to achieve with algorithms using only partial information. Our algorithm is less than 2.3 times slower and nearly achieves the bound. Hence on this dataset, very little further algorithmic improvement is possible.
7 Conclusions
In this paper we have presented the ﬁrst practical algorithm to our knowledge that attains the statistically optimal regret guarantee and is computationally eﬃcient in the setting of general policy classes. A remarkable feature of the algorithm is that the total number of oracle calls over all T rounds is sublinear— a remarkable improvement over previous works in this setting. We believe that the online variant of the approach which we implemented in our experiments has the right practical ﬂavor for a scalable solution to the contextual bandit problem. In future work, it would be interesting to directly analyze the Online Cover algorithm.
6http://hunch.net/~vw. The implementation is in the ﬁle cbify.cc and is enabled using --cover. 7The linear algebra routines are based on Intel MKL package.
12

Acknowledgements
We thank Dean Foster and Matus Telgarsky for helpful discussions. Part of this work was completed while DH and RES were visiting Microsoft Research.

References

Peter Auer. Using conﬁdence bounds for exploitation-exploration trade-oﬀs. Journal of Machine Learning Research, 3:397–422, 2002.

Peter Auer, Nicolo` Cesa-Bianchi, Yoav Freund, and Robert E. Schapire. The nonstochastic multiarmed bandit problem. SIAM Journal of Computing, 32(1):48–77, 2002.

Alina Beygelzimer and John Langford. The oﬀset tree for learning with partial labels. In KDD, 2009.

Alina Beygelzimer, John Langford, Lihong Li, Lev Reyzin, and Robert E. Schapire. Contextual bandit algorithms with supervised learning guarantees. In AISTATS, 2011.

Avrim Blum, Adam Kalai, and John Langford. Beating the holdout: Bounds for k-fold and progressive cross-validation. In COLT, 1999.

Olivier Chapelle and Lihong Li. An empirical evaluation of Thompson sampling. In NIPS, 2011.

Wei Chu, Lihong Li, Lev Reyzin, and Robert E. Schapire. Contextual bandits with linear payoﬀ functions. In AISTATS, 2011.

Miroslav Dud´ık, Daniel Hsu, Satyen Kale, Nikos Karampatziakis, John Langford, Lev Reyzin, and Tong Zhang. Eﬃcient optimal learning for contextual bandits. In UAI, 2011a.

Miroslav Dud´ık, John Langford, and Lihong Li. Doubly robust policy evaluation and learning. In ICML, 2011b.

David P. Helmbold and Robert E. Schapire. Predicting nearly as well as the best pruning of a decision tree. Machine Learning, 27(1):51–68, 1997.

John Langford.

Interactive machine learning,

http://hunch.net/~jl/projects/interactive/index.html.

January 2014.

URL

John Langford and Tong Zhang. The epoch-greedy algorithm for contextual multi-armed bandits. In NIPS, 2007.

David D Lewis, Yiming Yang, Tony G Rose, and Fan Li. Rcv1: A new benchmark collection for text categorization research. The Journal of Machine Learning Research, 5:361–397, 2004.

Lihong Li. Generalized Thompson sampling for contextual bandits. CoRR, abs/1310.7163, 2013.

Lihong Li, Wei Chu, John Langford, and Robert E. Schapire. A contextual-bandit approach to personalized news article recommendation. In WWW, 2010.

H. Brendan McMahan and Matthew Streeter. Tighter bounds for multi-armed bandits with expert advice. In COLT, 2009.

Richard S. Sutton and Andrew G. Barto. Reinforcement learning, an introduction. MIT Press, 1998.

William R. Thompson. On the likelihood that one unknown probability exceeds another in view of the evidence of two samples. Biometrika, 25(3–4):285–294, 1933.

13

Algorithm 3 IPS(H)
input History H ⊆ X × A × [0, 1] × [0, 1]. output Data set S ⊆ X × RA+.
1: Initialize data set S := ∅. 2: for each (x, a, r(a), p(a)) ∈ H do 3: Create ﬁctitious rewards rˆ ∈ RA+ with rˆ(a) = r(a)/p(a) and rˆ(a′) = 0 for all a′ ∈ A \ {a}. 4: S := S ∪ {(x, rˆ)}.
5: end for
6: return S.

Algorithm 4 Sample(x, Q, π¯, µ)
input Context x ∈ X, weights Q ∈ ∆Π, default policy π¯ ∈ Π, minimum probability µ ∈ [0, 1/K]. output Selected action a¯ ∈ A and probability p¯ ∈ [µ, 1].
1: Let Q˜ := Q + (1 − π∈Π Q(π))1π¯ (so π∈Π Q˜(π) = 1).
2: Randomly draw action a¯ ∈ A using the distribution

Q˜µ(a|x) := (1 − Kµ)

Q˜(π) + µ,

π∈Π: π(x)=a

∀a ∈ A.

3: Let p¯(a¯) := Q˜µ(a¯|x). 4: return (a¯, p¯(a¯)).

A Omitted Algorithm Details
Algorithm 3 and Algorithm 4 give the details of the inverse propensity scoring transformation IPS and the action sampling procedure Sample.

B Deviation Inequalities

B.1 Freedman’s Inequality

The following form of Freedman’s inequality for martingales is from Beygelzimer et al. (2011).

Lemma 9. Let X1, X2, . . . , XT be a sequence of real-valued random variables.

{1, 2, . . . , T }, Xt ≤ R and E[Xt|X1, . . . , Xt−1] = 0. Deﬁne S :=

T t=1

Xt

and

V

:=

For any δ ∈ (0, 1) and λ ∈ [0, 1/R], with probability at least 1 − δ,

Assume for all t ∈

T t=1

E[Xt2|X1

,

.

.

.

,

Xt−1].

S ≤ (e − 2)λV + ln(1/δ) . λ

B.2 Variance Bounds

Fix the epoch schedule 0 = τ0 < τ1 < τ2 < · · · . Deﬁne the following for any probability distribution P over Π, π ∈ Π, and µ ∈ [0, 1/K]:

1

V (P, π, µ) := Ex∼DX P µ(π(x)|x) ,

(8)

1

Vm(P, π, µ) := Ex∼Hτm P µ(π(x)|x) .

(9)

The proof of the following lemma is essentially the same as that of Theorem 6 from Dud´ık et al. (2011a).

14

Lemma 10. Fix any µm ∈ [0, 1/K] for m ∈ N. For any δ ∈ (0, 1), with probability at least 1 − δ,

V (P, π, µm) ≤ 6.4Vm(P, π, µm) + 75(1 − Kµm) ln |Π| + 6.3 ln(2|Π|2m2/δ)

µ2mτm

µmτm

for all probability distributions P over Π, all π ∈ Π, and all m ∈ N. In particular, if

µm ≥

ln(2|Π|m2/δ) , K τm

τm ≥ 4K ln(2|Π|m2/δ),

then V (P, π, µm) ≤ 6.4Vm(P, π, µm) + 81.3K.
Proof sketch. By Bernstein’s (or Freedman’s) inequality and union bounds, for any choice of Nm ∈ N and λm ∈ [0, µm] for m ∈ N, the following holds with probability at least 1 − δ:

V (P, π, µm) − Vm(P, π, µm) ≤ (e − 2)λmV (P, π, µm) + ln(|Π|Nm+12m2/δ)

µm

λmτm

all Nm-point distributions P over Π, all π ∈ Π, and all m ∈ N. Here, an N -point distribution over Π is

a

distribution

of

the

form

1 N

N i=1

1πi

for

π1, π2, . . . , πN

∈ Π.

We

henceforth

condition

on

this

≥ 1−δ

probability event (for choices of Nm and λm to be determined).

Using the probabilistic method (for more details, we refer the reader to the proof of Theorem 6 from

Dud´ık et al. (2011a)), it can be shown that for any probability distribution P over Π, any π ∈ Π, any

µm ∈ [0, 1/K], and any cm > 0, there exists an Nm-point distribution P over Π such that

V (P, π, µm) − V (P , π, µm) + cm Vm(P , π, µm) − Vm(P, π, µm) ≤ γNm,µm V (P, π, µm) + cmVm(P, π, µm)

where γN,µ := (1 − Kµ)/(N µ) + 3(1 − Kµ)/(N µ). Combining the displayed inequalities (using cm := 1/(1 − (e − 2)λm/µm)) and rearranging gives

V (P, π, µm) ≤ 1 + γNm,µm · Vm(P, π, µm) +

1

·

1

· ln(|Π|Nm+12m2/δ) .

1 − γNm,µm 1 − (e − 2) µλm m 1 − γNm,µm 1 − (e − 2) µλm m

λmτm

Using Nm := ⌈12(1 − Kµm)/µm⌉ and λm := 0.66µm for all m ∈ N gives the claimed inequalities. If µm ≥ ln(2|Π|m2/δ)/(Kτm) and τm ≥ 4K ln(2|Π|m2/δ), then µ2mτm ≥ ln(|Π|)/K and µmτm ≥
ln(2|Π|2m2/δ), and hence

75(1 − Kµm) ln |Π| + 6.3 ln(2|Π|2m2/δ) ≤ (75 + 6.3)K = 81.3K.

µ2mτm

µm τm

B.3 Reward Estimates
Again, ﬁx the epoch schedule 0 = τ0 < τ1 < τ2 < · · · . Recall that for any epoch m ∈ N and round t in epoch m,
• Qm−1 ∈ ∆Π are the non-negative weights computed at the end of epoch m − 1;
• Qm−1 is the probability distribution over Π obtained from Qm−1 and the policy πm−1 with the highest reward estimate through epoch m − 1;
• Qµmm−−11(·|xt) is the probability distribution used to choose at.

15

Let

m(t) := min{m ∈ N : t ≤ τm}

(10)

be the index of the epoch containing round t ∈ N, and deﬁne

Vt(π) := max {V (Qm, π, µm)}
0≤m≤m(t)−1

(11)

for all t ∈ N and π ∈ Π.

Lemma 11. For any δ ∈ (0, 1) and any choices of λm−1 ∈ [0, µm−1] for m ∈ N, with probability at least

1 − δ,

ln(4t2|Π|/δ) |Rt(π) − R(π)| ≤ Vt(π)λm−1 + tλm−1

for all policies π ∈ Π, all epochs m ∈ N, and all rounds t in epoch m.

Proof. Fix any policy π ∈ Π, epoch m ∈ N, and round t in epoch m. Then

1t Rt(π) − R(π) = t Zi
i=1

where Zi := rˆi(π(xi)) − ri(π(xi)). Round i is in epoch m(i) ≤ m, so

|Zi| ≤ µ

1

≤1

Q

m(i)−1
m(i)−1

(π

(xi

)|

xi

)

µm(i)−1

by the deﬁnition of the ﬁctitious rewards. Because the sequences µ1 ≥ µ2 ≥ · · · and m(1) ≤ m(2) ≤ · · · are monotone, it follows that Zi ≤ 1/µm−1 for all 1 ≤ i ≤ t. Furthermore, E[Zi|Hi−1] = 0 and

E[Zi2|Hi−1] ≤ E[rˆi(π(xi))2|Hi−1] ≤ V (Qm(i)−1, π, µm(i)−1) ≤ Vt(π)

for all 1 ≤ i ≤ t. The ﬁrst inequality follows because for var(X) ≤ E(X2) for any random variable

X; and the other inequalities follow from the deﬁnitions of the ﬁctitious rewards, V (·, ·, ·) in Eq. (8),

and Vt(·) in Eq. (11). Applying Freedman’s inequality and a union bound to the sums (1/t)

t i=1

Zi

and

(1/t) ti=1(−Zi) implies the following: for all λm−1 ∈ [0, µm−1], with probability at least 1−2·δ/(4t2|Π|),

1t

ln(4t2|Π|/δ)

t Zi ≤ (e − 2)Vt(π)λm−1 + tλm−1 .

i=1

The lemma now follows by applying a union bound for all choices of π ∈ Π and t ∈ N, since

δ ≤ δ. π∈Π t∈N 2t2|Π|

C Regret Analysis
Throughout this section, we ﬁx the allowed probability of failure δ ∈ (0, 1) provided as input to the algorithm, as well as the epoch schedule 0 = τ0 < τ1 < τ2 < · · · .

16

C.1 Deﬁnitions
Deﬁne, for all t ∈ N,

dt := ln(16t2|Π|/δ),

(12)

and recall that,

1 µm = min 2K ,

dτm . K τm

Observe that dt/t is non-increasing with t ∈ N, and µm is non-increasing with m ∈ N. Let

m0 := min m ∈ N : dτm ≤ 1 . τm 4K

Observe that τm0 ≥ 2. Deﬁne
ρ := sup
m≥m0
√ Recall that we assume τm+1 ≤ 2τm; thus ρ ≤ 2.

τm . τm−1

C.2 Deviation Control and Optimization Constraints
Let E be the event in which the following statements hold:

V (P, π, µm) ≤ 6.4Vm(P, π, µm) + 81.3K

(13)

for all probability distributions P over Π, all π ∈ Π, and all m ∈ N such that τm ≥ 4Kdτm (so µm = dτm /(Kτm)); and



max 3Vtdt , 2Vtdt if m ≤ m0,

|Rt(π) − R(π)| ≤

t

t

(14)

Vt(π)µm−1 + dt

if m > m0.

tµm−1

for all all policies π ∈ Π, all epochs m ∈ N, and all rounds t in epoch m. By Lemma 10, Lemma 11, and a union bound, Pr(E) ≥ 1 − δ/2.
For every epoch m ∈ N, the weights Qm computed at the end of the epoch (in round τm) as the solution to (OP) satisfy the constraints Eq. (2) and Eq. (3): they are, respectively:

Qm(π)Regτm(π) ≤ ψ · 2Kµm

(15)

π∈Π

and, for all π ∈ Π,

Vm(Qm, π, µm) ≤ 2K + Regτm (π) .

(16)

ψ · µm

√

Recall that ψ = 100 (as deﬁned in (OP), assuming ρ ≤ 2). Deﬁne θ1 := 94.1 and θ2 := ψ/6.4 (needed

for the next Lemma 12). With these settings, the proof o√f Lemma 13 will require that θ2 ≥ 8ρ, and hence ψ ≥ 6.4 · 8ρ; this is true with our setting of ψ since ρ ≤ 2.

17

C.3 Proof of Theorem 2

We now give the proof of Theorem 2, following the outline in Section 4. The following lemma shows that if Vt(π) is large—speciﬁcally, much larger than K—then the estimated
regret of π was large in some previous round.

Lemma 12. Assume event E holds. Pick any round t ∈ N and any policy π ∈ Π, and let m ∈ N be the

epoch achieving the max in the deﬁnition of Vt(π). Then  2K

if µm = 1/(2K),

Vt(π) ≤ θ1K + Regτm (π) θ2µm

if µm < 1/(2K).

Proof. Fix a round t ∈ N and policy π ∈ Π. Let m ≤ m(t) − 1 be the epoch achieving the max in the

deﬁnition of Vt(π) from Eq. (11), so Vt(π) = V (Qm, π, µm). If µm = 1/(2K), then V (Qm, π, µm) ≤ 2K.

So assume instead that 1/(2K) > µm = which holds in event E,

dτm /(Kτm). This implies that τm > 4Kdτm. By Eq. (13),

V (Qm, π, µm) ≤ 6.4Vm(Qm, π, µm) + 81.3K.

The probability distribution Qm satisﬁes the inequalities

Vm(Qm, π, µm) ≤ Vm(Qm, π, µm) ≤ 2K + Regτm (π) . ψµm

Above, the ﬁrst inequality follows because the value of Vm(Qm, π, µm) decreases as the value of Qm(πτm ) increases, as it does when going from Qm to Qm; the second inequality is the constraint Eq. (16) satisﬁed by Qm. Combining the displayed inequalities from above proves the claim.
In the next lemma, we compare Reg(π) and Regt(π) for any policy π by using the deviation bounds for estimated rewards together with the variance bounds from Lemma 12. Deﬁne t0 := min{t ∈ N : dt/t ≤ 1/(4K)}.
Lemma 13. Assume event E holds. Let c0 := 4ρ(1 + θ1). For all epochs m ≥ m0, all rounds t ≥ t0 in epoch m, and all policies π ∈ Π,
Reg(π) ≤ 2Regt(π) + c0Kµm;
Regt(π) ≤ 2 Reg(π) + c0Kµm.
Proof. The proof is by induction on m. As the base case, consider m = m0 and t ≥ t0 in epoch m. By deﬁnition of m0, µm′ = 1/(2K) for all m′ < m0, so Vt(π) ≤ 2K for all π ∈ Π by Lemma 12. By Eq. (14), which holds in event E, for all π ∈ Π,

|Rt(π) − R(π)| ≤ max

6Kdt , 4Kdt ≤ 6Kdt

t

t

t

where we use the fact that 4Kdt/t ≤ 1 for t ≥ t0. This implies

|Regt(π) − Reg(π)| ≤ 2 6Kt dt . √
by the triangle inequali√ty and optimality of πt and π⋆. Since t > τm0−1 and c0 ≥ 2 6ρ, it follows that |Regt(π) − Reg(π)| ≤ 2 6ρKµm0 ≤ c0Kµm0 .
For the inductive step, ﬁx some epoch m > m0. We assume as the inductive hypothesis that for all epochs m′ < m, all rounds t′ in epoch m′, and all π ∈ Π,
Reg(π) ≤ 2Regt′ (π) + c0Kµm′ ; Regt′ (π) ≤ 2 Reg(π) + c0Kµm′ .

18

We ﬁrst show that

Reg(π) ≤ 2Regt(π) + c0Kµm

(17)

for all rounds t in epoch m and all π ∈ Π. So ﬁx such a round t and policy π; by Eq. (14) (which holds in event E),

Reg(π) − Regt(π) = R(π⋆) − R(π) − Rt(πt) − Rt(π)

≤ R(π⋆) − R(π) − Rt(π⋆) − Rt(π)

≤ Vt(π) + Vt(π⋆) µm−1 + 2dt .

(18)

tµm−1

Above, the ﬁrst inequality follows from the optimality of πt. By Lemma 12, there exist epochs i, j < m such that

Vt(π) ≤ θ1K + Regτi (π) · 1{µi < 1/(2K)}, θ2µi
Regτj (π⋆) Vt(π⋆) ≤ θ1K + θ2µj · 1{µj < 1/(2K)}.

Suppose µi < 1/(2K), so m0 ≤ i < m: in this case, the inductive hypothesis implies

Regτi(π) ≤ 2 Reg(π) + c0Kµi ≤ c0K + 2 Reg(π)

θ2µi

θ2µi

θ2

θ2µm−1

where the second inequality uses the fact that i ≤ m − 1. Therefore,

Vt(π)µm−1 ≤ θ1 + c0 Kµm−1 + 2 Reg(π).

(19)

θ2

θ2

Now suppose µj < 1/(2K), so m0 ≤ j < m: as above, the inductive hypothesis implies

Regτj (π⋆) ≤ 2 Reg(π⋆) + c0Kµj = c0 K

θ2µj

θ2µj

θ2

since Reg(π⋆) = 0. Therefore,

Vt(π⋆)µm−1 ≤

θ1 + c0 θ2

K µm−1.

(20)

Combining Eq. (18), Eq. (19), and Eq. (20), and rearranging gives

Reg(π) ≤ 1

1

−

2 θ

Regt(π) + 2 θ1 + θc02 Kµm−1 + tµ2md−t 1 .

2

Since m ≥ m0+1, it follows that µm−1 ≤ ρµm by deﬁnition of ρ. Moreover, since t > τm−1, (dt/t)/µm−1 ≤ Kµ2m−1/µm−1 ≤ ρKµm Applying these inequalities to the above display, and simplifying, yields Eq. (17)

because c0 ≥ 4ρ(1 + θ1) and θ2 ≥ 8ρ.

We now show that

Regt(π) ≤ 2 Reg(π) + c0Kµm

(21)

for all π ∈ Π. Again, ﬁx an arbitrary π ∈ Π, and by Eq. (14),

Regt(π) − Reg(π) = Rt(πt) − Rt(π) − R(π⋆) − R(π)

≤ Rt(πt) − Rt(π) − R(πt) − R(π)

≤ Vt(π) + Vt(πt) µm−1 + 2dt

(22)

tµm−1

19

where the ﬁrst inequality follows from the optimality of π⋆. By Lemma 12, there exists an epoch j < m such
Regτj (πt) Vt(πt) ≤ θ1K + θ2µj · 1{µj < 1/(2K)}.
Suppose µj < 1/(2K), so m0 ≤ j < m: in this case the inductive hypothesis and Eq. (17) imply

Regτj (πt) ≤ 2 Reg(πt) + c0Kµj ≤ 2 2Regt(πt) + c0Kµm + c0Kµj = 3c0 K

θ2µj

θ2µj

θ2µj

θ2

(the last equality follows because Regt(πt) = 0). Thus

Vt(πt)µτ (t)−1 ≤

θ1 + 3c0 θ2

K µm−1 .

(23)

Combining Eq. (22), Eq. (23), and Eq. (19) gives

Regt(π) ≤

1+ 2 θ2

Reg(π) +

2θ1 + 4c0 θ2

Kµm−1 + 2dt . tµm−1

Again, applying the inequalities µm−1 ≤ ρµm and (dt/t)/µm−1 ≤ Kµm to the above display, and simplifying, yields Eq. (21) because c0 ≥ 4ρ(1 + θ1) and θ2 ≥ 8ρ. This completes the inductive step, and thus proves the overall claim.

The next lemma shows that the “low estimated regret guarantee” of Qt−1 (optimization constraint Eq. (15)) also implies a “low regret guarantee”, via the comparison of Regt(·) to Reg(·) from Lemma 13.
Lemma 14. Assume event E holds. For every epoch m ∈ N,

Qm−1(π) Reg(π) ≤ (4ψ + c0)Kµm−1
π∈Π
where c0 is deﬁned in Lemma 13.
Proof. Fix any epoch m ∈ N. If m ≤ m0, then µm−1 = 1/(2K), in which case the claim is trivial. Therefore assume m ≥ m0 + 1. Then

Qm−1(π) Reg(π) ≤ Qm−1(π) 2Regτm−1 (π) + c0Kµm−1

π∈Π

π∈Π

= 2 Qm−1(π)Regτm−1 (π)
π∈Π
≤ ψ · 4Kµm−1 + c0Kµm−1.

+ c0Kµm−1

The ﬁrst step follows from Lemma 13, as all rounds in an epoch m ≥ m0 + 1 satisfy t ≥ t0; the second step follows from the fact that Qm−1 is a probability distribution, that Qm−1 = Qm−1 + α1πτm−1 for
some α ≥ 0, and that Regτm−1(πτm−1 ) = 0; and the last step follows from the constraint Eq. (15) satisﬁed by Qm−1.

Finally, we straightforwardly translate the “low regret guarantee” from Lemma 14 to a bound on the cumulative regret of the algorithm. This involves summing the bound in Lemma 14 over all rounds t (Lemma 15 and Lemma 16) and applying a martingale concentration argument (Lemma 17).

Lemma 15. For any T ∈ N,

T
µm(t) ≤ 2
t=1

dτm(T ) τm(T ) . K

20

Proof. We break the sum over rounds into the epochs, and bound the sum within each epoch:

T

m(T ) τm

µm(t) ≤

µm

t=1

m=1 t=τm−1+1

m(T ) τm
≤
m=1 t=τm−1+1

dτm K τm

dτm(T ) m(T ) τm − τm−1

≤K

√τm

m=1

dτ

m(T ) τm dx

≤

m(T )

√=

K m=1 τm−1 x

dτm(T ) K

τm(T ) dx √ =2

τ0

x

dτm(T ) √τm(T ). K

Above, the ﬁrst step uses the fact that m(1) = 1 and τm(t)−1 + 1 ≤ t ≤ τm(t). The second step uses the deﬁnition of µm. The third step simpliﬁes the sum over t and uses the bound dτm−1 ≤ dτm(T) . The remaining steps use an integral bound which is then directly evaluated (recalling that τ0 = 0).

Lemma 16. For any T ∈ N,

T

τm

µm(t)−1 ≤

0
2K

+

t=1

8dτm(T ) τm(T ) . K

√ Proof. Under the epoch schedule condition τm+1 ≤ 2τm, we have µm(t)−1 ≤ 2µm(t) whenever m(t) >

m0; also, µm(t)−1 ≤ 1/(2K) whenever m(t) ≤ m0. The conclusion follows by applying Lemma 15.

Lemma 17. For any T ∈ N, with probability at least 1 − δ, the regret after T rounds is at most

C0 4Kdτm0−1 + 8Kdτm(T) τm(T ) + 8T log(2/δ)

where C0 := (4ψ + c0) and c0 is deﬁned in Lemma 13.

Proof. Fix T ∈ N. For each round t ∈ N, let Zt := rt(π⋆(xt)) − rt(at) − π∈Π Qm(t)−1 Reg(π). Since

E[rt(π⋆(xt)) − rt(at)|Ht−1] = R(π⋆) − Qm(t)−1(π)R(π) = Qm(t)−1 Reg(π),

π∈Π

π∈Π

it follows that E[Zt|Ht−1] = 0. Since |Zt| ≤ 2, it follows by Azuma’s inequality that

T
Zt ≤ 2
t=1

2T ln(2/δ)

with probability at least 1 − δ/2. By Lemma 10, Lemma 11, and a union bound, the event E holds with probability at least 1 − δ/2. Hence, by another union bound, with probability at least 1 − δ, event E holds and the regret of the algorithm is bounded by

T
Qm(t)−1(π) Reg(π) + 2
t=1 π∈Π

2T ln(2/δ).

The double summation above is bounded by Lemma 14 and Lemma 16:

T

T

Qm(t)−1(π) Reg(π) ≤ (4ψ + c0)K µm(t)−1 ≤ (4ψ + c0)

t=1 π∈Π

t=1

τm0 + 2

8K dτm(T ) τm(T ) .

By the deﬁnition of m0, τm0−1 ≤ 4Kdτm0−1 . Since τm0 ≤ 2τm0−1 by assumption, it follows that τm0 ≤ 8K dτm0−1 .

21

Theorem 2 follows from Lemma 17 and the fact that τm(T ) ≤ 2(T − 1) whenever τm(T )−1 ≥ 1. There is one last result implied by Lemma 12 and Lemma 13 that is used elsewhere.
Lemma 18. Assume event E holds, and t is such that dτm(t)−1 /τm(t)−1 ≤ 1/(4K). Then
Rt(πt) ≤ R(π⋆) + θ1 + c0 + c0 + 1 Kµm(t)−1. θ2
Proof. Let m′ < m(t) achieve the max in the deﬁnition of Vt(π⋆). If µm′ < 1/(2K), then m′ ≥ m0, and

Vt(π⋆) ≤ θ1K + ≤ θ1K +

Regτm′ (π⋆) θ2µm′
2 Reg(π⋆) + c0Kµm′ θ2µm′

= cK

for c := θ1 + c0/θ2. Above, the second inequality follows by Lemma 13. If µm′ = 1/(2K), then the same bound also holds. Using this bound, we obtain from Eq. (14),

Rt(π⋆) − R(π⋆) ≤ cKµm(t)−1 + dt . tµm(t)−1

To conclude,

Rt(πτm ) = R(π⋆) + Rt(π⋆) − R(π⋆) + Regt(π⋆) ≤ R(π⋆) + cKµm(t)−1 + tµmd(tt)−1 + Regt(π⋆) ≤ R(π⋆) + cKµm(t)−1 + dt + c0Kµm(t) tµm(t)−1
where the last inequality follows from Lemma 13. The claim follows because dt/t ≤ dτm(t)−1 /τm(t)−1 and µm(t) ≤ µm(t)−1.

D Details of Optimization Analysis

D.1 Proof of Lemma 5
Following the execution of Step 4, we must have

Q(π)(2K + bπ) ≤ 2K.

(24)

π

This is because, if the condition in Step 7 does not hold, then Eq. (24) is already true. Otherwise, Q is replaced by Q′ = cQ, and for this set of weights, Eq. (24) in fact holds with equality. Note that, since all
quantities are nonnegative, Eq. (24) immediately implies both Eq. (2), and that π Q(π) ≤ 1. Furthermore, at the point where the algorithm halts at Step 10, it must be that for all policies π,
Dπ(Q) ≤ 0. However, unraveling deﬁnitions, we can see that this is exactly equivalent to Eq. (3).

D.2 Proof of Lemma 6
Consider the function

g(c) = B0Φm(cQ),

22

where, in this proof, B0 = 2K/(τ µ), where we recall that we drop the subscripts on τm and µm. Let Qµc (a|x) = (1 − Kµ)cQ(a|x) + µ. By the chain rule, the ﬁrst derivative of g is:

g′(c) = B0 Q(π) ∂g(cQ) π ∂Q(π)

=

Q(π) (2K + bπ) − 2Ex∼H

1
µ

(25)

t Qc (π(x)|x)

π

To handle the second term, note that

1 Q(π)Ex∼Ht Qµc (π(x)|x)
π

1{π(x) = a}

=

Q(π)Ex∼Ht

Qµc (a|x)

π

a∈A

Q(π)1{π(x) = a}

= Ex∼Ht

Qµc (a|x)

a∈A π

Q(a|x)

= Ex∼Ht

Qµc (a|x)

a∈A

= 1 Ex∼H

cQ(a|x)

≤ K.

(26)

c t a∈A (1 − Kµ)cQ(a|x) + µ c

To see the inequality in Eq. (26), let us ﬁx x and deﬁne qa = cQ(a|x). Then a qa = c π Q(π) ≤ 1 by Eq. (4). Further, the expression inside the expectation in Eq. (26) is equal to

qa

= K· 1

1

a (1 − Kµ)qa + µ

K a (1 − Kµ) + µ/qa

≤ K·

1

(27)

(1 − Kµ) + Kµ/ a qa

≤ K·

1

= K.

(28)

(1 − Kµ) + Kµ

Eq. (27) uses Jensen’s inequality, combined with the fact that the function 1/(1 − Kµ + µ/x) is concave (as a function of x). Eq. (28) uses the fact that the function 1/(1 − Kµ + Kµ/x) is nondecreasing (in x), and that the qa’s sum to at most 1.
Thus, plugging Eq. (26) into Eq. (25) yields

g′(c) ≥ Q(π)(2K + bπ) − 2K = 0 πc

by our deﬁnition of c. Since g is convex, this means that g is nondecreasing for all values exceeding c. In particular, since c < 1, this gives

B0Φm(Q) = g(1) ≥ g(c) = B0Φm(cQ), implying the lemma since B0 > 0.

D.3 Proof of Lemma 7

We ﬁrst compute the change in potential for general α. Note that Q′µ(a|x) = Qµ(a|x) if a = π(x), and

otherwise

Q′µ(π(x)|x) = Qµ(π(x)|x) + (1 − Kµ)α.

23

Thus, most of the terms deﬁning Φm(Q) are left unchanged by the update. In particular, by a direct calculation:

2τKµ (Φm(Q) − Φm(Q′)) = 1 −2Kµ Ex∼Ht ln 1 + Qα(µ1(π−(xK)|µx)) − α(2K + bπ)

2

α(1 − Kµ) 1 α(1 − Kµ) 2

≥ 1 − Kµ Ex∼Ht Qµ(π(x)|x) − 2 Qµ(π(x)|x)

−α(2K + bπ)

(29)

= 2αVπ(Q) − (1 − Kµ)α2Sπ(Q) − α(2K + bπ)

= α(Vπ(Q) + Dπ(Q)) − (1 − Kµ)α2Sπ(Q)

(30)

(Vπ(Q) + Dπ(Q))2

= 4(1 − Kµ)Sπ(Q) .

(31)

Eq. (29) uses the bound ln(1 + x) ≥ x − x2/2 which holds for x ≥ 0 (by Taylor’s theorem). Eq. (31) holds by our choice of α = απ(Q), which was chosen to maximize Eq. (30). By assumption, Dπ(Q) > 0, which implies Vπ(Q) > 2K. Further, since Qµ(a|x) ≥ µ always, we have

1 Sπ(Q) = Ex∼Ht Qµ(π(x) | x)2
≤ µ1 · Ex∼Ht Qµ(π(1x) | x) = Vπµ(Q) .

Thus,

(Vπ(Q) + Dπ(Q))2 Vπ(Q)2

Vπ (Q)

Sπ (Q)

≥ Sπ(Q) = Vπ(Q) · Sπ(Q) ≥ 2Kµ.

Plugging into Eq. (31) completes the lemma.

D.4 Proof of Lemma 8
We break the potential of Eq. (6) into pieces and bound the total change in each separately. Speciﬁcally, by straightforward algebra, we can write

Φm(Q) = φam(Q) + φbm + φcm(Q) + φdm(Q)

where

φam(Q) =

τmµm Ex∼H − ln Qµ(a|x)

K(1 − Kµm)

t
a

φb = τmµm ln K

m

1 − Kµm

φcm(Q) = τmµm

Q(π) − 1

π

φd (Q) = τmµm

m

2K

Q(π)bπ .

π

We assume throughout that π Q(π) ≤ 1 as will always be the case for the vectors produced by Algorithm 2. For such a vector Q,

φcm+1(Q) − φcm(Q) = (τm+1µm+1 − τmµm)

Q(π) − 1 ≤ 0
π

24

since τmµm is nondecreasing. This means we can essentially disregard the change in this term. Also, note that φbm does not depend on Q. Therefore, for this term, we get a telescoping sum:

M
(φbm+1 − φbm) = φbM+1 − φb1 ≤ φbM+1 ≤ 2
m=1

T dT ln K K

since KµM+1 ≤ 1/2, and where dT , used in the deﬁnition of µm, is deﬁned in Eq. (12). Next, we tackle φam:

Lemma 19.

M
(φam+1(Qm) − φam(Qm)) ≤ 6
m=1

T dT ln(1/µM+1). K

Proof. For the purposes of this proof, let

Cm = µm . 1 − Kµm

Then we can write

φa (Q) = − Cm τm ln Qµm (a|x ).

m

K

t

t=1 a

Note that Cm ≥ Cm+1 since µm ≥ µm+1 and − ln Qµm (a|xt) ≥ 0. Thus,

φa (Q) − φa (Q) ≤ Cm+1 τm ln Qµm (a|x )

m+1

m

K

t

t=1 a

τm+1

−

ln Qµm+1 (a|xt)

t=1 a

Cm+1 τm

Qµm (a|xt)

=K

ln Qµm+1 (a|xt)

t=1 a

τm+1

−

ln Qµm+1 (a|xt).

t=τm+1 a

≤ Cm+1[τm ln(µm/µm+1) − (τm+1 − τm) ln µm+1].

(32)

Eq. (32) uses Qµm+1 (a|x) ≥ µm+1, and also

Qµm (a|x) = (1 − Kµm)Q(a|x) + µm ≤ µm , Qµm+1 (a|x) (1 − Kµm+1)Q(a|x) + µm+1 µm+1

using µm+1 ≤ µm. A sum over the two terms appearing in Eq. (32) can now be bounded separately. Starting with the one on the left, since τm < τm+1 ≤ T and Kµm ≤ 1/2, we have

Thus,

Cm+1τm ≤ 2τmµm+1 ≤ 2τm+1µm+1 ≤ 2

T dT . K

M

T dT M

Cm+1τm ln(µm/µm+1) ≤ 2 K

ln(µm/µm+1)

m=1

m=1

= 2 T dT ln(µ1/µM+1)) K

≤ 2 T dT (− ln(µM+1)). K

(33)

25

For the second term in Eq. (32), using µm+1 ≥ µM+1 for m ≤ M , and deﬁnition of Cm, we have

M

M

−Cm+1(τm+1 − τm) ln µm+1 ≤ −2(ln µM+1) (τm+1 − τm)µm+1

m=1

m=1

T

≤ −2(ln µM+1) µm(t)

t=1

≤ −4 T dT (ln µM+1) K

(34)

by Lemma 15. Combining Eqs. (32), (33) and (34) gives the statement of the lemma. Finally, we come to φdm(Q), which, by deﬁnition of bπ, can be rewritten as

φdm(Q) = B1τm Q(π)Regτm(π)
π

where B1 = 1/(2Kψ) and ψ is the same as appears in optimization problem (OP). Note that, conveniently,

τmRegτm (π) = Sm(πm) − Sm(π),

where Sm(π) is the cumulative empirical importance-weighted reward through round τm:
τm
Sm(π) = rˆt(π(xt)) = τmRτm (π).
t=1
From the deﬁnition of Q˜, we have that
φdm(Q˜) = φdm(Q)

+B1 1 − Q(π) τmRegτm (πm)
π
= φdm(Q)

since Regτm (πm) = 0. And by a similar computation, φdm+1(Q˜) ≥ φdm+1(Q) since Regτm+1 (π) is always nonnegative.
Therefore,

φdm+1(Qm) − φdm(Qm) ≤ φdm+1(Q˜m) − φdm(Q˜m)

= B1 Q˜m(π) Sm+1(πm+1) − Sm+1(π) − Sm(πm) − Sm(π)
π

= B1 Sm+1(πm+1) − Sm(πm)

−B1 τm+1

Q˜m(π)rˆt(π(xt)) .

t=τm+1 π

(35)

We separately bound the two parenthesized expressions in Eq. (35) when summed over all epochs. Beginning with the ﬁrst one, we have

M m=1

Sm+1(πm+1) − Sm(πm)

= SM+1(πM+1) − S1(π1) ≤ SM+1(πM+1).

26

But by Lemma 18 (and under the same assumptions),

SM+1(πM+1) = τM+1RτM+1 (πM+1) ≤ τM+1(R(π⋆) + D0KµM ) ≤ τM+1R(π⋆) + D0 KT dT ,

(36)

where D0 is the constant appearing in Lemma 18. For the second parenthesized expression of Eq. (35), let us deﬁne random variables

Zt = Q˜τ(t)(π)rˆt(π(xt)).
π

Note that Zt is nonnegative, and if m = τ (t), then

Zt =

Q˜ m (π)rˆt (π(xt ))

π

=

Q˜ m (a|xt )rˆt (a)

a

=

Q˜m(a|xt) rt(a)1{a = at}

a

Q˜µm (a|xt)

≤ rt(at) ≤ 2 1 − Kµm

since Q˜µm (a|x) ≥ (1 − Kµm)Q˜m(a|x), and since rt(at) ≤ 1 and Kµm inequality, with probability at least 1 − δ,

≤ 1/2.

Therefore, by Azuma’s

τM +1

τM +1

Zt ≥

E[Zt|Ht−1] −

t=1

t=1

2τM+1 ln(1/δ).

The expectation that appears here can be computed to be

E[Zt|Ht−1] = Q˜m(π)R(π)
π
so

R(π⋆) − E[Zt|Ht−1] =

Q˜m(π)(R(π⋆) − R(π))

π

=

Q˜m(π) Reg(π)

π

≤ (4ψ + c0)Kµm

by Lemma 14 (under the same assumptions, and using the same constants). Thus, with high probability,

τM +1

τM +1

(R(π⋆) − Zt) ≤ (4ψ + c0)K

µm(t) +

t=1

t=1

2τM+1 ln(1/δ)

≤ (4ψ + c0) 8KT dT + 2T ln(1/δ)

by Lemma 16. Combining the above bound with our earlier inequality Eq. (36), and applying the union bound, we
ﬁnd that with probability at least 1 − 2δ, for all T (and corresponding M ),

M
(φdm(Qm) − φdm+1(Qm)) ≤ O
m=1

T ln(T |Π|/δ) . K

Combining the bounds on the separate pieces, we get the bound stated in the lemma.

27

D.5 Proof of Lemma 3
We ﬁnally have all the pieces to establish our main bound on the oracle complexity with warm-start presented in Lemma 3. The proof is almost immediate, and largely follows the sketch in Section 3.4 apart from one missing bit of detail. Notice that we start Algorithm 1 with Q0 = 0, at which point the objective Φ0(Q0) = 0 since τ0 = 0. Initially, owing to the small values of τm, we might be in the regime where µ = 1/2K, where the decrease in the potential guaranteed by Lemma 7 is just O˜(τ /K2). However, in this regime, it is easy to check that Q0 = 0 remains a feasible solution to (OP). It clearly satisﬁes the regret constraint Eq. (2), and µ = 1/(2K) ensures that the variance constraints Eq. (3) are also met. Hence, we make no calls to the oracle in this initial regime and can focus our attention to τm large enough so that µm = dτm /(Kτm).
In this regime, we observe that τm2 µm = dτm/K, so that Lemma 7 guarantees that we decreaes the objective by at least dτm/(4K) (recalling Kµm ≥ 0). Hence, the total decrease in our objective after N calls to the oracle is at least N dτm/(4K), while the net increase is bounded by O˜( T dT /K. Since the potential is always positive, the number of oracle calls can be at most O˜( T K/ ln(|Π|/δ)), which completes the proof.

E Proof of Theorem 4
Recall the earlier deﬁnition of the low-variance distribution set Qm = {Q ∈ ∆Π : Q satisﬁes Eq. (3) in round τm}.
Fix δ ∈ (0, 1) and the epoch sequence, and assume M is large enough so µm = ln(16τm2 |Π|/δ)/τm for all m ∈ N with τm ≥ τM /2. The low-variance constraint Eq. (3) gives, in round t = τm,

Ex∼H

1

≤ 2K + Regτm (π) ,

t Qµm (π(x)|x)

ψµm

∀π ∈ Π.

Below, we use a policy class Π where every policy π ∈ Π has no regret (Reg(π) = 0), in which case Lemma 13 implies

Ex∼Ht Qµm (π1(x)|x) ≤ 2K + c0ψKµµmm = K 2 + cψ0 , ∀π ∈ Π.

Applying Lemma 10 (and using our choice of µm) gives the following constraints: with probability at least 1 − δ, for all m ∈ N with τm ≥ τM /2, for all π ∈ Π,

Ex∼D

1

≤ 81.3K + 6.4K 2 + c0 =: cK

(37)

X Qµm (π(x)|x)

ψ

(to make Q into a probability distribution Q, the leftover mass can be put on any policy, say, already in the support of Q). That is, with high probability, for every relevant epoch m, every Q ∈ Qm satisﬁes Eq. (37) for all π ∈ Π.
Next, we construct an instance with the property that these inequalities cannot be satisﬁed by a very sparse Q. An instance is drawn uniformly a√t random from N diﬀerent contexts denoted as {1, 2, . . . , N } (where we set, with foresight, N := 1/(2 2cKµM )). The reward structure in the problem will be extremely simple, with action K always obtaining a reward of 1, while all the other actions obtain a reward of 0, independent of the context. The distribution D will be uniform over the contexts (with these deterministic rewards). Our policy set Π will consist of (K − 1)N separate policies, indexed by 1 ≤ i ≤ N and 1 ≤ j ≤ K − 1. Policy πij has the property that

πij (x) = j if x = i, K otherwise.

28

In words, policy πij takes action j on context i, and action K on all other contexts. Given the uniform distribution over contexts and our reward structure, each policy obtains an identical reward

R(π) = 1 − 1 · 1 + 1 · 0 = 1 − 1 .

N

N

N

In particular, each policy has a zero expected regret as required. Finally, observe that on context i, πij is the unique policy taking action j. Hence we have that
Q(j|i) = Q(πij ) and Qµm(j|i) = (1 − Kµm)Q(πij ) + µm. Now, let us consider the constraint Eq. (37) for the policy πij . The left-hand side of this constraint can be simpliﬁed as

Ex∼DX

1 Qµm (π(x)|x)

1N

1

=

N x=1 Qµm (πij (x)|x)

=1

1

+1· 1

N x=i Qµm (πij (x)|x) N Qµm (j|i)

≥1· 1 . N Qµm (j|i)

If the distribution Q does not put any support on the policy πij , then Qµm(j|i) = µm, and thus

Ex∼DX

1 Qµm (π(x)|x)

≥ 1 · 1 = 1 ≥ √ 1 > cK

N Qµm (j|i) N µm

2N µM

√ (since N < 1/( 2cKµM )). Such a distribution Q violates Eq. (37), which means that every Q ∈ Qm must have Q(πij ) > 0. Since this is true for each policy πij , we see that every Q ∈ Qm has

|supp(Q)| ≥ (K − 1)N = √K − 1 = Ω 2 2cKµM
which completes the proof.

K τM ln(τM |Π|/δ)

F Online Cover algorithm
This section describes the pseudocode of the precise algorithm use √in our experiments (Algorithm 5). The minimum exploration probability µ was set as 0.05 min(1/K, 1/ tK) for our evaluation.
Two additional details are important in Step 9:
1. We pass a cost vector rather than a reward vector to the oracle since we have a loss minimization rather than a reward maximization oracle.
2. We actually used a doubly robust estimate Dud´ık et al. (2011b) with a linear reward function that was trained in an online fashion.

29

Algorithm 5 Online Cover

input Cover size n, minimum sampling probability µ.

1: Initialize online cost-sensitive minimization oracles O1, O2, . . . , On, each of which controls a policy π(1), π(2), . . . , π(n); U := uniform probability distribution over these policies.
2: for round t = 1, 2, . . . do

3: Observe context xt ∈ X.

4: (at, pt(at)) := Sample(xt, U, ∅, µ).

5: Select action at and observe reward rt(at) ∈ [0, 1].

6: for each i = 1, 2, . . . , n do

7:

Qi := (i − 1)−1 j<i 1π(j) .

8:

pi(a) := Qµ(a|xt).

9: Create cost-sensitive example (xt, c) where c(a) = 1 − prtt((aatt)) 1{a = at} − piµ(a) .

10:

Update π(i) = Oi(x, c)

11: end for

12: end for

30

