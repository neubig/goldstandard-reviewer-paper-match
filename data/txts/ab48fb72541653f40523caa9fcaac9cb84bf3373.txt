End-to-End Multi-speaker ASR with Independent Vector Analysis
Robin Scheibler1, Wangyou Zhang2, Xuankai Chang3, Shinji Watanabe3, Yanmin Qian2
1LINE Corporation, Tokyo, Japan 2Shanghai Jiao Tong University, Shanghai, China
3Carnegie Mellon University, Pittsburgh, USA
robin.scheibler@linecorp.com

arXiv:2204.00218v1 [eess.AS] 1 Apr 2022

Abstract
We develop an end-to-end system for multi-channel, multispeaker automatic speech recognition. We propose a frontend for joint source separation and dereverberation based on the independent vector analysis (IVA) paradigm. It uses the fast and stable iterative source steering algorithm together with a neural source model. The parameters from the ASR module and the neural source model are optimized jointly from the ASR loss itself. We demonstrate competitive performance with previous systems using neural beamforming frontends. First, we explore the trade-offs when using various number of channels for training and testing. Second, we demonstrate that the proposed IVA frontend performs well on noisy data, even when trained on clean mixtures only. Furthermore, it extends without retraining to the separation of more speakers, which is demonstrated on mixtures of three and four speakers. Index Terms: end-to-end, multi-speaker, automatic speech recognition, independent vector analysis, multichannel
1. Introduction
Automatic speech recognition (ASR) technology provides a natural interface for human-to-machine communication [1]. Despite tremendous progress in the last decade, ASR systems are still severely challenged by reverberation, overlapped speech, and noise [2]. Microphone arrays are a powerful tool to ﬁght these degradations. In particular, linear spatial ﬁltering, i.e., beamforming, has been shown to reliably decrease the word error rate (WER) of ASR systems [2]. While optimal beamforming formulations exist [3], e.g. the famous minimum variance distortionless response (MVDR) beamformer, their use has been traditionally limited by the difﬁculty of estimating the target and noise statistics. These limitations have been recently practically solved by using trained neural networks to estimate these statistics [4]. The resulting neural beamformers are highly effective [5]. However, training these networks requires a large amount of parallel speech data, e.g. reverberant mixtures and the isolated anechoic sources they contain. Such data is notoriously difﬁcult to collect. Instead, most works rely on simulation [6, 7]. However, the simulation is often insufﬁcient, and some fully unsupervised approaches have been proposed [8, 9].
The situation for ASR systems is much different since transcripts of actual recordings may be collected by skilled annotators [1]. Indeed, a large amount of annotated speech data has been collected for academic and commercial purposes [10]. One can thus bypass the necessity of parallel speech data by concatenating enhancement and ASR systems, and training directly from the ASR loss [11, 12]. Building on this approach, an end-to-end (E2E) paradigm for multi-channel, multi-speaker ASR called MIMO-speech [13] has been proposed. This approach has demonstrated not only competitive ASR, but also

decent separation performance, trained from the ASR loss only. It has been extended to include several advanced joint dereverberation and beamforming methods [14, 15]. Despite all these progresses, the challenge of domain mismatch remains. Neural beamformers for separation typically rely on estimating multiple sources, usually two, from a single spectrogram. If the test data is sufﬁciently different from the input data, this stage may fail, impeding the beamforming performance.
An alternative line of research builds upon independent vector analysis (IVA) [16, 17]. In addition to a statistical model of the sources, their mutual statistical independence is leveraged to help the separation. Vanilla IVA is a blind method, requiring no training data, that can be solved iteratively [18, 19], and rivals sophisticated neural beamformers [20]. Extensions to joint dereverberation and separation have been proposed [21]. In particular, time-decorrelation iterative source steering (T-ISS) [22] is a stable and fast algorithm that avoids matrix inversion. Recently, combining IVA with a neural source model has attracted attention [23, 24]. One particular approach proposes to train a neural source model end-to-end through T-ISS [19, 25]. Unlike in neural beamforming, the network models a single source. This allows to maintain the high performance of neural beamforming while being agnostic to the number of sources and channels, and robust to a fair amount of data mismatch [25].
In this work, we investigate the use of a T-ISS frontend for MIMO-speech E2E ASR. We extend T-ISS to the overdetermined case, where more channels than sources are present [26] We implement our model in ESPnet [27] with an E2E transformer-based ASR backend [28]. The whole system is trained E2E by joint CTC/attention loss [29]. We use variations of the spatialized WSJ1 [13] dataset, clean, and corrupted with several kinds of noise. We explore how the number of channels at training affects test performance (spoiler: more is better). We demonstrate the robustness of T-ISS to mismatch with training in terms of noise and number of sources.
2. Background
We use the following notation. Bold lower and upper case letters are for vector and matrices, respectively. Furthermore, A and AH denote the transpose and conjugate transpose, respectively, of matrix A. The norm of vector v is v = (vH v)1/2.
2.1. MIMO-Speech and Its Extensions
We are concerned with multi-channel, multi-speaker ASR systems. The MIMO-Speech method [13, 30] proposes a fully end-to-end framework that jointly optimizes the entire system with only the ﬁnal ASR criterion. The model consists of a beamforming-based frontend for speech separation and an E2E ASR backend. It takes M channels, as inputs, and outputs K text hypotheses, corresponding to K concurrent speakers. First,

update by backpropagation/gradient descent

M input signals

Frontend
shared
K extracted sources

PIT ASR Loss

AFTER THE CLOSE...
NOW WHATEVER THEIR...
K transcripts

Figure 1: Illustration of the MIMO-speech E2E system for K source and M microphones.

the frontend extracts K source signals from the input mixture. Second, each of the extracted signal is processed by the same E2E ASR backend in parallel. This produces K text hypotheses that are evaluated against the K reference transcripts with a permutation invariant training (PIT) loss [31]. The process is illustrated in Fig. 1.

2.2. Multichannel Speech Separation and Dereverberation

Physically, the signals from the K sources propagate and reﬂect on the walls of a room, and mix additively with various amplitudes and time delays at the M microphones. This process can be approximated in the short-time Fourier transform (STFT) domain as follows,

xfn = Af sfn + Zf x¯fn + bfn,

∈

M
C

,

(1)

where sfn ∈ CK is a vector containing the source signals. The matrix Af ∈ CM×K contains the transfer functions from sources to microphones in its entries. The term
x¯fn = xf,n−D, . . . , xf,n−D−L accounts for reﬂections from the room mixed by the matrix Zf ∈ CM×ML. The reverberation length is given by L, and D is a delay necessary due to the overlap of the STFT. Finally, bfn ∈ CM is a noise term The indices f and n are for frequency bins and STFT frames, and run from 1 to F and N , respectively. The role of the frontend is
to reduce the second and third terms, as much as possible, and invert Af , if feasible.

2.2.1. Neural Dereverberation and Beamforming
Conventionally, dereverberation and beamforming are done in distinct steps. While several dereverberation methods exist, WPE [32] has been widely adopted for ASR. Ignoring the noise term bfn, the dereverberated mixture Af sfn can be obtained if we know Zf . Provided with a neural network producing a mask uWfnPE hiding the target signal from the spectrogram, the dereverberation ﬁlters are given by the minimizer of n uWfnPE xfn − Zf x¯fn 2. The beamforming ﬁlters are computed from the spatial covariance matrices of target speech and noise. Since these are typically not available, a neural network is trained to estimate masks that extract these signals power from a spectrogram. Let ufn be one of these masks. Then, the corresponding spatial covariance matrix is Φf = 1/N n ufnx˜fnx˜Hfn, where x˜fn is the dereverberated input signal. For the separation of multiple sources, the neural network is trained to output masks for each of the sources. Several ways of combining or sharing masks between steps have been proposed and give rise to different neural beamformers such as MVDR, WPD [33], and wMPDR [34]. See [14] for the details. We emphasize here that after the masks are estimated, the beamforming ﬁlters are estimated independently. Thus, if the mask estimation fails, it cannot be corrected during the ﬁlter computation step.

2.2.2. Independence-based Dereverberation and Separation
This approach builds upon independent vector analysis (IVA) [16, 17]. Unlike, the neural beamforming approach, the foundational hypothesis of IVA is that multiple statistically independent sources are present. The approach has been extended to jointly optimize for dereverberation [21, 22] and include a trainable neural source model [19, 25]. We deﬁne the demixing and dereverberation matrices as W f ∈ CM×M and U f ∈ CM×ML, respectively. For convenience, we concatenate them into a uniﬁed dereverberation and separation matrix P f = [W f U f ]. Further let x˜fn be the concatenation of xfn and x¯fn. The kth row of P f , i.e. pHkf , extracts the kth target as ykfn = pHkf x˜fn. Maximum likelihood estimation of P f can be done by iteratively minimizing the cost function

L(P f ) = ufn(Yˆ k)|pHkf x˜fn|2 − 2 log | det W f | (2)
kf n

where pHkf is the kth row of P f . The F × N matrix Yˆ k is the current estimate of source k with entries (Yˆ k)fn = yˆkfn. The cost function (2) is derived from the likelihood function of the observed data xfn [21]. The change of variable from xfn to Y k allows to work on the source signals, rather than the mixture, but introduces the log-determinant term. Then, since the sources are independent, the joint probability density function (pdf) is the product of the marginals. Finally, we assume the log-pdf may be majorized by that of the Normal distribution to obtain (2) [18]. The function ufn(Y ) is derived from the majorization step, and guarantees decrease of (2). In [19, 25], this exact derivation is abandoned, together with the guarantees, and ufn(Y ) is replaced by a trainable neural network.
While minimization of (2) does not have a closed-form solution, algorithms to efﬁciently decrease its value exist [21, 22]. T-ISS [22] is particularly suitable for use in E2E training because of low computational cost and lack of matrix inversion. The algorithm proceeds by ﬁnding a sequence of = 1, . . . , M (L + 1) optimal rank-1 updates of the form

P f ← P f − v pHf , v = arg min L(P f − vpHf ). (3) v∈CM

For > M , we deﬁne p f = e , i.e., the vector with all zeros but a one at position . The closed form solution for v in (3) is

 1 −

n ufnN(Y ) |y fn|2 −1/2 , if q = ,

(v )q =

∗

(4)

 n uf n(Yq )yqf ny f n  , n ufn(Yq )|y fn|2

else.

where yqfn = pHqf x˜fn for q ≤ M , and yqfn = e x˜fn else. In contrast to the neural beamformer in [14, 15], spatial cues are taken into account when estimating the source masks. Because the neural network models a single source, the algorithm is easily extended to different numbers of sources. It was also shown to be robust to domain mismatch [25].

3. Proposed End-to-end Architecture
Our proposed system builds upon the latest methodology of MIMO-Speech [15]. We replace the WPD beamforming frontend by an IVA-based one that performs joint dereverberation and separation. During training, multiple iterations of T-ISS are run to obtain the separation matrix. The same single source mask network is used for all iterations and separation output. The proposed frontend is illustrated in Fig. 2. The system is trained E2E from the ASR loss as illustrated in Fig. 1.

input multi-channel signal

unrolled T-ISS iterations

mask

mask

separated spectrograms

Spatial Model Update (fixed) Spatial Model Update (fixed)

mask

mask

Trainable Source Model
257 x N
=
spectrogram

shared weights

257 x N
GLU 256 x N
GLU 256 x N
Dropout 256 x N GLU 256 x N
Conv1dT 257 x N

257 x N
mask

Figure 2: Block diagram of the proposed frontend and neural source model.

3.1. ASR Transformer Model
We adopt the joint connectionist temporal classiﬁcation (CTC)/attention-based encoder-decoder [29] as the ASR backend, which consists of four submodules: feature extraction, encoder, CTC, and attention-decoder. For each separated stream Yˆ k from the frontend, 80-dimensional log-Mel ﬁlterbank features Ok are ﬁrstly extracted via the feature extraction module MVN-LMF. The extracted feature is then fed into the ASR encoder to obtain hidden representations Hk, which are used in both CTC and attention-decoder submodules for recognition. The ASR procedure is summarized as follows:

Ok = MVN-LMF(Yˆ k), Rˆ (kctc) = CTC(Hk),

Hk = Enc(Ok), Rˆ (kdec) = AttentionDec(Hk),

where MVN-LMF denotes extracting log-Mel ﬁlterbank features and applying mean-variance normalization. Rˆ (kctc) and Rˆ (kdec) are recognition results from CTC and attention-decoder submodules, respectively. The ASR loss function is constructed
based on multi-task learning:

Lasr = αLctc + (1 − α)Ldec,

(5)

where 0 < α < 1 is an interpolation factor. Note that the permutation invariant training (PIT) [31] method is applied in the CTC submodule as in [13, 30] to solve the label permutation problem when processing multiple separated streams.

3.2. Overdetermined T-ISS Frontend
Our independence-based frontend is a new extension of the TISS [22] algorithm, described in this sub-section, that can be used when more channels are available than there are sources. When there are more channels than sources, i.e., K < M , the separation matrix W f is not square anymore and the algorithm of Section 2.2.2 is not sufﬁcient anymore. An extension of ISS to the overdetermined case has been proposed, but for separation only [26]. We extend it here to include dereverberation.
First, we write the overdetermined dereverberation and separation operation as a determined system, i.e., square,

yfn 

Wf

Uf  x

zfn  =  J f −IM−N

0  fn . (6)

x¯ f n

0

IML x¯ fn

The separated target sources are yfn ∈ CK , and zfn ∈ CM−K is a vector of background noise sources to make the system determined. The top part contains P f = [W f U f ], of Section 2.2.2, but with K rows, since we only wish to extract so
many sources. To complete the separation matrix, we add the

strict minimum of parameters J f ∈ CM−K×K . The zeros on the right of the middle block reﬂect that we do not need to dereverberate the background noise. Previous work [35, 36, 37] has shown that a necessary condition for optimality is that the target sources and the noise vector be orthogonal, i.e., E yfnzHfn = 0. From this and (6), we obtain an equation for J ,
((W f Rf + U Cf )E1)J H = ((W f Rf + U Cf )E2), (7)

where Rf = E xfnxHfn , Cf = E x¯fnxHfn , and E1, E2 are of the appropriate shape, and such that [E1 E2] = I. Following the methodology of [26], we update P f = [W f U f ] and J f in two steps.

1. We update P f = [W f U f ] with (3) and (4), but limit the size of v to K to match the size of W f , U f in (6).

2. Update J by solving (7).

The resulting algorithm maintains the low-complexity of T-ISS while allowing to use more channels for increased separation power. We note that one matrix inverse is introduced in step 2. However, the size of the matrix to invert is only K × K, with e.g., K = 2 for two sources. Despite this small size, we observed some stability issues. The system matrix is not Hermitian symmetric, and its eigenvalues not always positive. Thus, straight diagonal loading, as in [15], does not guarantee stability. Our solution is to replace the d × d system Ax = b by

(AH D−1A + I)x = AH D−1b,

(8)

where D is a diagonal matrix containing the square norms of the rows of A. The system matrix is now guaranteed positive deﬁnite. Clearly, if = 0, the solution is the same as the original system. Furthermore, normalizing the rows of A with D ensures the sum of the eigenvalues of the system matrix is d. This allows a numerically sensible choice of .

4. Experiments
We conducted several experiments to assess the performance of the proposed method. We investigate the impact of the number of channels, iterations, and the presence of noise.

4.1. Experimental Conditions
We evaluate the proposed method on three datasets derived from the WSJ1 corpus [38]. For training and test on clean and noisy mixtures, we use wsj1 spatialized [13], and a similar dataset [19] that includes noise from the CHiME3 challenge [39]. These two datasets have 8 and 6 channels, respectively. For noisy training, we concatenate the clean and noisy training sets. The third dataset is used for mismatched testing and remixes the clean test mixtures with simulated diffuse noise [6] created from the TUT environmental sound database [40] with SNR of 5 dB to 15 dB. Unlike previous work [14, 15], we did not do multi-condition training and used only the reverberant mixtures. All the input speech is sampled at 16 kHz. The STFT uses a 25 ms long Hann window with 10 ms shifts. The FFT is zero padded to length 512 producing 257-dimensional spectral feature vectors. After the frontend, spectrograms are converted to 80-dimensional log Melﬁlterbank features. The training was conducted on an NVidia V100 graphical processing unit (GPU) with 32 GB RAM.
All the models are implemented in ESPnet [27] using the PyTorch [41] backend. For the baseline, we use the WPD model described in [14]. It uses a bidirectional long-short term memory (BLSTM) network with 600 cells in each direction followed

Table 1: Performance in terms of WER (%) on the clean test set. Models are trained on clean data with M channels. The four columns on the right are for different number of channels at test time.

Algorithm

M 2ch 4ch 6ch 8ch

Best in [15]† 2ch 15.01

— 9.02

—

WPD T-ISS

8ch 25.71 12.56 10.00 9.57 2ch 13.71 23.40 28.88 31.46

T-ISS T-ISS

4ch 20.57 10.37 10.86 11.16 8ch 25.71 9.98 9.08 9.16

† Included for reference, training and parameters differ.

Table 2: Performance on matched and mismatched data.

Algo.

Train

Test

WER SIR SDR PESQ STOI

WPD 8ch / clean 8ch / clean 9.57 13.9 6.9 1.88 0.855 T-ISS 8ch / clean 8ch / clean 9.16 16.8 3.7 1.78 0.830

WPD 8ch / clean 6ch / noisy 17.12 12.3 8.7 1.70 0.890 T-ISS 8ch / clean 6ch / noisy 12.48 15.6 6.2 1.86 0.913

WPD 8ch / noisy 6ch / noisy 11.40 14.7 10.8 T-ISS 4ch / noisy 6ch / noisy 12.18 14.4 7.3

1.79 0.918 1.77 0.922

WPD 8ch / noisy 8ch / TUT 15.17 10.0 5.2 1.57 0.816 T-ISS 4ch / noisy 8ch / TUT 23.56 11.4 1.5 1.42 0.741 T-ISS 8ch / clean 8ch / TUT 14.55 13.7 2.1 1.45 0.787

by an output layer producing three masks per target speaker, i.e. 6 in our case. WPE is conﬁgured with L = 5 taps and delay D = 3, and runs for single iteration. The neural source model for T-ISS is the same as in [19]. It has three convolutional layers, with batch-norm, max pooling, and GLU activations. It has a 256 hidden dimension and dropout set to 0.2. For the models trained on clean data, we preprocessed the input with 5 iterations of AuxIVA-ISS with a non-trainable source model [42]. This is followed by 10 iterations of T-ISS with the neural source model. After separation, the scale and phase are aligned to a reference channel by projection back [43]. As it did not seem very effective, we did not apply this preprocessing when training on the noisy dataset. Instead we ran 15 iterations of T-ISS straight. We used demixing matrix checkpointing [25] to allow the model to ﬁt on GPU during training.
We used the Adam optimizer with warmup set to 25000 and 50000 steps on the clean and noisy datasets, respectively, and initial learning rate of 1. The WPD baseline was always trained with maximum M = 8 channels. We trained multiple T-ISS models on the clean dataset using M ∈ {2, 4, 8} channels. On the noisy dataset, we only trained on M = 4 channels due to time constraints. At test time, the number of T-ISS iterations was adjusted to achieve better performance. An external wordlevel recurrent neural network language model (RNNLM) [44] is applied as shallow fusion in the decoding stage.
4.2. Experimental Results
Effect of Number of Channels Table 1 reports the ASR evaluation results on the clean test set in terms of WER. Each row represents a different trained model. The performance with different numbers of channels at test time is reported in the four right-most columns. We observed that using more channels at training pays off. Models trained this way had lower WERs, even when testing with fewer channels. There is however an exception for T-ISS where the behavior differed if trained with two channels, or more. When trained on two-channel data, the performance was outstanding on two channels test data, even better than the best result from [15], but did poorly with more

Table 3: Performance of T-ISS trained with two speakers on mixtures K = 3, 4 speakers.

K

Train

WER SDR SIR PESQ STOI

3 8ch / clean 17.80 4ch / noisy 18.28

3.9 10.2 4.8 9.9

1.52 0.862 1.51 0.870

4 8ch / clean 33.06 1.1 5.8 1.34 0.792 4ch / noisy 30.66 2.2 6.1 1.34 0.804

channels. While not reported due to space constraints, separation metrics increased with the number of channels. This suggests that the ASR backend overﬁts the artefacts of the separation stage for two channels. Similarly, T-ISS models trained on more channels performed poorly on the two-channel test set. The best performing model was T-ISS trained on 8 channels.
Mismatched Conditions Table 2 reports the ASR performance under different training and test conditions. When trained and tested on clean data, both frontends achieved under 10 % WER, with T-ISS slightly better at 9.16 %. However, when trained on clean, but tested on noisy data, T-ISS signiﬁcantly outperformed WPD by 4.6 %. When trained on noisy data, the performance of WPD recovered. T-ISS did about 0.7 % worse than WPD, but still a little better than in the mismatched condition. Note that in this case, the noise was from the CHiME3 dataset both for training and testing. We thus further tested on the mismatched noisy mixture dataset (labeled TUT in the table). For WPD, the noisy training was effective at improving the robustness, and the WER did not increase as much as before. The T-ISS model trained on noisy data performed much worse on the mismatched noise. We conjecture that this may be due to the lack of a proper noise model, and overﬁtting to the noise in the source model. It was the T-ISS model trained on noiseless data only that performed best here. Table 2 also shows the regular separation metrics SDR, SIR, PESQ, and STOI. T-ISS had consistently high SIR, but otherwise somewhat lower metrics. The SDR in particular is much lower than that of WPD. This suggests that it achieves good separation, but at the expense of more target degradation.
Separation of 3 and 4 speakers Even though the model was trained on two speaker mixtures, the T-ISS algorithm can be used to separate more, provided that sufﬁciently many channels are available. We tested this on 3 and 4 speakers mixtures from the noisy dataset using 6 channels. Table 3 shows the results. We note that the problem becomes much harder than in the two speakers case since the per-speaker SNR drops significantly. Still, reasonable ASR performance was maintained in this challenging situation.
5. Conclusions
We have proposed the joint training of a MIMO-speech ASR system with an independent vector analysis frontend using the T-ISS algorithm. T-ISS is an iterative procedure performing joint separation and dereverberation with the help of a neural source model. We demonstrate that E2E training of this system, through the iterations, yields an ASR robust to data mismatch. The T-ISS frontend trained on clean data only, did best, or at least well enough, on all our test sets. In contrast, the neural beamformer baseline required noisy data in the training set in order to avoid a large performance drop.
Future work should concentrate on the inclusion of a noise model in T-ISS, e.g. [45]. Multi-condition training and curriculum learning are also promising research avenues.

6. References
[1] D. Yu and L. Deng, Automatic Speech Recognition, 1st ed., ser. Signals and Communication Technology. London: SpringerVerlag, 2015.
[2] R. Haeb-Umbach et al., “Far-ﬁeld automatic speech recognition,” Proc. IEEE, vol. 109, no. 2, pp. 124–148, Feb. 2021.
[3] H. L. Van Trees, Optimum Waveform Estimation. John Wiley & Sons, Ltd, 2002, ch. 6, pp. 428–709.
[4] H. Erdogan et al., “Improved MVDR beamforming using singlechannel mask prediction networks,” in INTERSPEECH, Sep. 2016, pp. 1981–1985.
[5] J. Heymann et al., “Neural network based spectral mask estimation for acoustic beamforming,” in ICASSP, Mar. 2016, pp. 196– 200.
[6] E. A. Habets et al., “Generating nonstationary multisensor signals under a spatial coherence constraint,” The Journal of the Acoustical Society of America, vol. 124, no. 5, pp. 2911–2917, 2008.
[7] R. Scheibler et al., “Pyroomacoustics: A Python package for audio room simulation and array processing algorithms,” in ICASSP, Apr. 2018, pp. 351–355.
[8] L. Drude et al., “Unsupervised training of a deep clustering model for multichannel blind source separation,” in ICASSP, May 2019, pp. 695–699.
[9] M. Togami et al., “Unsupervised training for deep speech source separation with Kullback-Leibler divergence based probabilistic loss function,” in ICASSP, Mar. 2020, pp. 56–60.
[10] V. Pratap et al., “MLS: A large-scale multilingual dataset for speech research,” in INTERSPEECH, Oct. 2020, pp. 2757–2761.
[11] J. Heymann et al., “Beamnet: End-to-end training of a beamformer-supported multi-channel ASR system,” in ICASSP, Mar. 2017, pp. 5325–5329.
[12] T. Ochiai et al., “Uniﬁed architecture for multichannel end-to-end speech recognition with neural beamforming,” IEEE J. Sel. Top. Signal Process., vol. 11, no. 8, pp. 1274–1288, 2017.
[13] X. Chang et al., “MIMO-Speech: End-to-end multi-channel multi-speaker speech recognition,” in ASRU, Dec. 2019, pp. 237– 244.
[14] W. Zhang et al., “End-to-end far-ﬁeld speech recognition with uniﬁed dereverberation and beamforming,” in INTERSPEECH, Oct. 2020, pp. 324–328.
[15] W. Zhang et al., “End-to-end dereverberation, beamforming, and speech recognition with improved numerical stability and advanced frontend,” in ICASSP, Jun. 2021, pp. 6898–6902.
[16] T. Kim et al., “Independent vector analysis: An extension of ICA to multivariate components,” in International conference on independent component analysis and signal separation. Springer, 2006, pp. 165–172.
[17] A. Hiroe, “Solution of permutation problem in frequency domain ICA, using multivariate probability density functions,” in ASIACRYPT 2016. Berlin, Heidelberg: Springer, Jan. 2006, vol. 3889, pp. 601–608.
[18] N. Ono, “Stable and fast update rules for independent vector analysis based on auxiliary function technique,” in WASPAA, Oct. 2011, pp. 189–192.
[19] R. Scheibler and M. Togami, “Surrogate source model learning for determined source separation,” in ICASSP, Jun. 2021, pp. 176– 180.
[20] C. Boeddeker et al., “A comparison and combination of unsupervised blind source separation techniques,” in Speech Communication; 14th ITG Conference. VDE, 2021, pp. 1–5.
[21] R. Ikeshita et al., “A unifying framework for blind source separation based on a joint diagonalizability constraint,” in EUSIPCO, Sep. 2019, pp. 1–5.
[22] T. Nakashima et al., “Joint dereverberation and separation with iterative source steering,” in ICASSP, Jun. 2021, pp. 216–220.
[23] N. Makishima et al., “Independent deeply learned matrix analysis for determined audio source separation,” IEEE/ACM Trans. Audio Speech Lang. Process., vol. 27, no. 10, pp. 1601–1615, 2019.

[24] H. Kameoka et al., “Supervised determined source separation with multichannel variational autoencoder,” Neural Computation, vol. 31, no. 9, pp. 1891–1914, 09 2019.
[25] K. Saijo and R. Scheibler, “Low-memory end-to-end training for iterative joint speech dereverberation and separation with a neural source model,” 2021.
[26] Y. Du et al., “Computationally-efﬁcient overdetermined blind source separation based on iterative source steering,” IEEE Signal Process. Lett., pp. 1–1, Dec. 2021.
[27] S. Watanabe et al., “ESPnet: End-to-end speech processing toolkit,” in INTERSPEECH, 2018, pp. 2207–2211.
[28] S. Karita et al., “A comparative study on transformer vs RNN in speech applications,” in ASRU, Dec. 2019, pp. 449–456.
[29] S. Kim et al., “Joint CTC-attention based end-to-end speech recognition using multi-task learning,” in ICASSP, Mar. 2017, pp. 4835–4839.
[30] X. Chang et al., “End-to-end multi-speaker speech recognition with transformer,” in ICASSP, 2020, pp. 6129–6133.
[31] M. Kolbaek et al., “Multitalker speech separation with utterancelevel permutation invariant training of deep recurrent neural networks,” IEEE/ACM Trans. Audio Speech Lang. Process., vol. 25, no. 10, pp. 1901–1913, Aug. 2017.
[32] T. Nakatani et al., “Speech Dereverberation Based on VarianceNormalized Delayed Linear Prediction,” IEEE Trans. Audio Speech Lang. Process., vol. 18, no. 7, pp. 1717–1731, Sep. 2010.
[33] T. Nakatani and K. Kinoshita, “A uniﬁed convolutional beamformer for simultaneous denoising and dereverberation,” IEEE Signal Process. Lett., vol. 26, no. 6, pp. 903–907, Jun. 2019.
[34] T. Nakatani et al., “Jointly optimal denoising, dereverberation, and source separation,” IEEE/ACM Trans. Audio Speech Lang. Process., vol. 28, pp. 2267–2282, Jul. 2020.
[35] R. Scheibler and N. Ono, “MM algorithms for joint independent subspace analysis with application to blind single and multisource extraction,” arXiv preprint arXiv:2004.03926, 2020.
[36] R. Ikeshita et al., “Overdetermined independent vector analysis,” in ICASSP, 2020, pp. 591–595.
[37] M. Togami and R. Scheibler, “Over-determined speech source separation and dereverberation,” in APSIPA, Dec. 2020, pp. 705– 710.
[38] Linguistic Data Consortium, and NIST Multimodal Information Group, CSR-II (WSJ1) Complete LDC94S13A, Linguistic Data Consortium, Philadelphia, 1994, web Download.
[39] J. Barker et al., “The third ‘CHiME’ speech separation and recognition challenge: Dataset, task and baselines,” in ASRU, Nov. 2015, pp. 504–511.
[40] A. Mesaros et al., “A multi-device dataset for urban acoustic scene classiﬁcation,” in DCASE, November 2018, pp. 9–13.
[41] A. Paszke et al., “PyTorch: An imperative style, high-performance deep learning library,” in Advances in Neural Information Processing Systems, H. Wallach et al., Eds., vol. 32. Curran Associates, Inc., 2019.
[42] R. Scheibler and N. Ono, “Fast and stable blind source separation with rank-1 updates,” in ICASSP, May 2020, pp. 236–240.
[43] N. Murata et al., “An approach to blind source separation based on temporal structure of speech signals,” Neurocomputing, vol. 41, no. 1-4, pp. 1–24, Oct. 2001.
[44] T. Hori et al., “End-to-end speech recognition with word-based RNN language models,” in Proc. IEEE SLT, 2018, pp. 389–396.
[45] Z. Koldovsky´ et al., “Orthogonally-Constrained Extraction of Independent Non-Gaussian Component from Non-Gaussian Background Without ICA,” in Latent Variable Analysis and Signal Separation. Cham: Springer, 2018, vol. 10891, pp. 161–170.

