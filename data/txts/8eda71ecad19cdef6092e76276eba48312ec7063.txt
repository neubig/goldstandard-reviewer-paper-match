Interpreting Dense Retrieval as Mixture of Topics

arXiv:2111.13957v1 [cs.IR] 27 Nov 2021

Jingtao Zhan1, Jiaxin Mao2, Yiqun Liu1★, Jiafeng Guo3, Min Zhang1, Shaoping Ma1
1 Department of Computer Science and Technology, Institute for Artificial Intelligence, Beijing National Research Center for Information Science and Technology, Tsinghua University, Beijing 100084, China
2 Beijing Key Laboratory of Big Data Management and Analysis Methods, Gaoling School of Artificial Intelligence, Renmin University of China, Beijing 100872, China
3 CAS Key Lab of Network Data Science and Technology, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China
jingtaozhan@gmail.com,maojiaxin@gmail.com,yiqunliu@tsinghua.edu.cn,guojiafeng@ict.ac.cn

ABSTRACT
Dense Retrieval (DR) reaches state-of-the-art results in first-stage retrieval, but little is known about the mechanisms that contribute to its success. Therefore, in this work, we conduct an interpretation study of recently proposed DR models. Specifically, we first discretize the embeddings output by the document and query encoders. Based on the discrete representations, we analyze the attribution of input tokens. Both qualitative and quantitative experiments are carried out on public test collections. Results suggest that DR models pay attention to different aspects of input and extract various high-level topic representations. Therefore, we can regard the representations learned by DR models as a mixture of high-level topics.
KEYWORDS
dense retrieval, neural ranking, explainability
1 INTRODUCTION
Dense Retrieval (DR) achieves state-of-the-art first-stage retrieval performance through representing text with dense embeddings and utilizing (approximate) nearest neighbor search to retrieve candidates [5, 7, 10, 11, 16, 17]. However, the mechanisms that contribute to its outstanding performance remain unclear.
We address this problem by conducting an interpretation study. Firstly, we propose RepMoT, which discretizes the output of query and document encoders. The discrete representations make interpretation study easier by modeling text relationships more explicitly than continuous embeddings. Secondly, we investigate DR models based on RepMoT with attribution techniques. We qualitatively analyze the word importance with respect to different discrete subvectors. Then we quantitatively measure the word importance by masking the input and observing the output change.
We conduct experiments on two widely-adopted benchmark datasets. Experimental results show RepMoT achieves ranking performance on par with state-of-the-art DR models and thus justifies analyzing DR models based on RepMoT. The analysis results suggest DR models pay attention to different aspects of the input and capture mixture of high-level topics. Therefore, the representations encoded by DR models can be regarded as mixture of high-level topic representations.
★Corresponding author

2 RELATED WORKS

DR models embed queries and documents to low-dimension dense

vectors and utilize their vector similarity to predict relevance [4, 7,

8, 16, 19]. Formally, let 𝑞 be the user query and 𝑑 be the document.

We use

𝑓

to denote the DR model. Thus, 𝒒

=

𝑓 (𝑞)

∈

𝐷
R

and

𝒅 = 𝑓 (𝑑) ∈ R𝐷 , where 𝐷 is the embedding dimension.

Previous DR models usually embed text to a continuous embed-

ding space, such as ANCE [14] and ADORE [16]. Recently, several

studies jointly optimize DR models with Product Quantization [9]

and produce discrete document representations, such as JPQ [15]

and RepCONC [17]. Our proposed RepMoT further discretizes query

representations based on RepCONC [17].

Here is a brief note about the term “Topic”. “Topic” in the context

of topic models usually refers to clusters of similar words and is

mined based on word statistics [2]. In this paper, the term “Topic”

is used in its idiomatic sense, and we investigate which words DR

models pay attention to. Future works may further explore the

relationships between RepMoT and topic models.

3 METHODOLOGY
3.1 Discrete Representations
It is difficult to directly interpret the DR models in the continuous embedding space because we need to manually set a threshold for the soft matching score to determine whether two pieces of texts are semantically similar or not. Inspired by Product Quantization [9], we propose to discretize the query and document representations. The discrete representations explicitly characterize the relationship between different texts. The queries or documents assigned to the same discrete sub-vectors are similar and those to different ones are dissimilar.
Now we describe how to discretize continuous representations. 1) We first split the output of DR models to 𝑀 sub-vectors of dimension 𝐷/𝑀. We use 𝑓𝑖 (text) ∈ R𝐷/𝑀 to denote the 𝑖-th sub-vector. 2) Next, we define 𝑀 embedding pools, each of which involves 𝐾
𝐷
embeddings of dimension 𝐷/𝑀. We use 𝒄𝑖,𝑗 ∈ R 𝑀 to denote the 𝑗-th embedding from the 𝑖-th pool. 3) Finally, we replace each subvector 𝑓𝑖 (text) with the nearest embedding in the corresponding pool {𝒄𝑖,𝑗 }. Let 𝒅^ be the discrete document representation. It is the concatenation of 𝑀 sub-vectors 𝒅^𝒊:

𝒅^ = 𝒅^1, 𝒅^2, ..., 𝒅^𝑀

𝒅^ = 𝒄 ( ) and 𝜑 (𝑑) = arg min ||𝑓 (𝑑) − 𝒄 || (1)

𝑖

𝑖,𝜑𝑖 𝑑

𝑖

𝑗𝑖

𝑖,𝑗

Table 1: Retrieval results on TREC 2019 Deep Learning Track. M denotes the number of discrete sub-vectors per passage/document.

Model

Discrete Query Doc M

ANCE [14]

\

ADORE [16]

\

OPQ [9] JPQ [15] RepCONC [17]

✓ 48 ✓ 48 ✓ 48

OPQ-SDC [9]

✓

✓ 48

RepMoT

✓

✓ 48

MARCO Passage

MRR@10

R@100

0.330 0.347

0.852 0.876

0.290 0.332 0.340

0.830 0.863 0.864

0.285 0.335

0.822 0.854

DL Passage NDCG@10 R@100

0.648 0.683

0.437 0.473

0.591 0.644 0.668

0.417 0.447 0.492

0.592 0.661

0.404 0.441

MARCO Doc

MRR@100

R@100

0.377 0.405

0.894 0.919

0.340 0.384 0.399

0.880 0.905 0.911

0.338 0.394

0.873 0.905

where commas denote concatenation. Query representations are defined in the same way. Each embedding pool splits the corpus into 𝐾 groups, and the texts in the same group are considered similar. The discrete representation is still very expressive because the total number of combinations is 𝐾𝑀 .
Inspired by RepCONC [17], we end-to-end optimize the dualencoders 𝑓 and embedding pools {𝒄𝑖,𝑗 } with the weighted sum of a ranking-oriented loss and a clustering loss. The ranking-oriented loss replaces the continuous embeddings in the common ranking loss functions [12, 14, 16] with the discrete embeddings:

e ⟨𝒒^,𝒅^+ ⟩

𝐿𝑟 = − log

(2)

e ⟨𝒒^,𝒅^+ ⟩ + 𝑑− e ⟨𝒒^,𝒅^− ⟩

where 𝑞^and 𝑑^are discrete query and document representations. The clustering loss is the MSE loss between the discrete and continuous embeddings:

𝐿 = 1 (∥ 𝑓 (𝑑) − 𝒅^∥2 + ∥ 𝑓 (𝑞) − 𝒒^∥2)

(3)

𝑚2

We utilize their weighed sum 𝐿𝑟 + 𝜆𝐿𝑚 as the final loss function. We also adopt the uniform clustering constraint [17] when discretizing document embeddings during training, which, according to RepCONC [17], helps maximize the distinguishability of discrete representations. We refer readers to RepCONC [17] for details.
We name our method RepMoT because the following sections will show it captures Mixture of high-level Topics.

3.2 Attribution Analysis

We aim to interpret DR on word level by quantifying the contri-

butions of each word to each discrete sub-vector by a scalar score.

For example, say the DR model takes the input of the sentence “his

food is tasty.". “food" and “tasty" are likely to have high contribution

scores because they are important for the semantic meanings. “is"

carries little information and thus has low score.

We adopt a popular attribution method, Integrated Gradients

(IG) [13], to measure the word importance. It interpolates the input

features from the baseline input to the actual input and computes

the partial derivatives of the model output with respect to the input.

Formally, let 𝑥 = (𝑥1, ..., 𝑥𝑛) ∈ R𝑛×𝑑𝑖𝑚 be the actual input word

embeddings,

and

𝑥′

=

(𝑥1′ ,

..., 𝑥 ′ )
𝑛

∈

R𝑛×𝑑𝑖𝑚

be

the

baseline

input.

The baseline can be a sequence of “〈unk〉" tokens 1 . Let 𝐹 be a neural model. The attribution of 𝑗-th word equals to:

′ ∫ 1 𝜕𝐹 (𝑥 ′ + 𝛼 (𝑥 − 𝑥 ′))

(𝑥 𝑗 − 𝑥 ) ·

𝑑𝛼

(4)

𝑗

𝛼 =0

𝜕𝑥 𝑗

Recall that RepMoT encodes the input text to the concatenation of multiple discrete sub-vectors. To study whether different discrete sub-vectors pay attention to different words, we investigate the attribution scores with respect to each output sub-vector. We use the following formula to compute the 𝑗-th word’s attribution for 𝑖-th output sub-vector.

′ ∫ 1 𝜕 − ||𝒅^𝒊 − 𝑓𝑖 (𝑥 ′ + 𝛼 (𝑥 − 𝑥 ′)||2

(𝑥 𝑗 − 𝑥 ) ·

𝑑𝛼 (5)

𝑗

𝛼 =0

𝜕𝑥 𝑗

The above attribution scores evaluate which words contribute to the selection of the discrete sub-vectors 𝒅^𝒊. We do not directly use

the output of 𝑓𝑖 but instead utilize the negative MSE. It is because

as shown in Eq. (1), MSE is important for selecting discrete rep-

resentations while the exact output of DR models is not. In other

words,

𝑓 (𝑥 ′
𝑖

+

𝛼 (𝑥

−

𝑥 ′))

is

equivalent

to

𝑓𝑖 (𝑥)

as

long

as

it

is

closer to 𝒅^𝒊 than to other embeddings from the pool. To model this

characteristic, we use the MSE to compute gradients in Eq. (5).

We compare the attribution distribution of the same text for
different 𝒅^𝒊 : 1 ≤ 𝑖 ≤ 𝑀, and also analyze the highly attributed words of different text for the same 𝒅^𝒊. Note, this paper focuses

on analyzing document representations and leaves investigating

query representations to future research.

4 EXPERIMENTS
This section presents our experimental results. We conduct experiments on TREC 2019 Deep Learning Track [1, 3]. The passage ranking task has a corpus of 8.8M passages, 0.5𝑀 training queries, 7𝑘 development queries (henceforth, MARCO Passage), and 43 test queries (DL Passage). The document ranking task has a corpus of 3.2𝑀 documents, 0.4𝑀 training queries, 5𝑘 development queries (MARCO Doc).
The following sections first justify our analysis based on RepMoT, and then show DR models capture high-level topics.

1According to Zhan et al. [18], removing stop words leads to unexpected behaviors of BERT. Thus, we keep all stop words for the baseline input.

(a) Energy

(b) Travel

(c) Art

(d) Geography

Figure 1: Word cloud cases based on RepMoT on MS MARCO Passage Ranking.

Table 2: The attribution results of one passage from MS MARCO Passage for different discrete embeddings. Topic-related words are manually selected from the highly attributed words of other passages assigned to the same discrete sub-vectors.

Passage (highly attributed words are marked bold.)
The law defines ‘full time" as 30 hours a week or more. “We work so hard for so little pay," he said. ... This month, the administration delayed the employer insurance requirement until January 2015.
The law defines “full time" as 30 hours a week or more. “We work so hard for so little pay," he said. ... This month, the administration delayed the employer insurance requirement until January 2015.
The law defines “full time" as 30 hours a week or more. “We work so hard for so little pay," he said. ... This month, the administration delayed the employer insurance requirement until January 2015.

Topic-Related Words
day, week, period, month
government, health, duty, insurance
salary, wage, work, pay

4.1 Discrete Representations
This section presents the ranking performance of RepMoT. We first introduce baselines and then discuss the experimental results.
4.1.1 Baselines. For continuous representations, we use ANCE [14] and ADORE [16] as baselines. For half-discrete representations, we employ OPQ [6], JPQ [15], and RepCONC [17], all of which use discrete document representations and continuous query representations. For discrete representations, we adopt OPQ-SDC [6], which unsupervisedly discretize representations with k-means.
4.1.2 Results. Table 1 shows the experimental results. RepMoT achieves performance on par with the state-of-the-art DR models. It outperforms ANCE [14] which employs continuous representations. It closes the gap with state-of-the-art ADORE [16] and RepCONC [17], both of which employ (half-) continuous representations. Therefore, discrete representations learned by RepMoT are good approximations for DR models.
4.2 Qualitative Analysis
We first investigate the passages assigned to the same discrete subvector. Figure 1 plots the word cloud examples for four sub-vectors. Clearly, the discrete sub-vector relates to a certain topic, e.g., energy, art, etc. Therefore, we can regard the discrete sub-vectors as topic representations.
Next, we investigate which words RepMoT pays attention to when it encodes the text to multiple discrete sub-vectors. Table 2 visualizes the important words for three example discrete sub-vectors. The topic-related words in Table 2 are highly-attributed words of other passages assigned to the same sub-vectors. Results suggest

that RepMoT pays attention to different aspects of the input to extract different high-level topic representations. Therefore, we can regard the learned representations as mixture of topics.
4.3 Quantitative Analysis
We conduct a quantitative experiment to validate that different sub-vectors pay attention to different parts of the input. We mask certain words according to different assumptions and measure the following MSE: ||𝒅^𝒊 − 𝑓𝑖 (mask(d))||. We report the average MSE for different 𝑖. Smaller MSE indicates the unmasked words are important for 𝒅^𝒊.
According to our finding that different sub-vectors pay attention to different parts of the input, we design a masking method called MoT. For different 𝒅^𝒊, MoT computes the attribution scores with Eq. (5) and thus can identify the important words specifically for a certain 𝒅^𝒊. It keeps the top attributed words and masks others. In the following, we first describe the baselines and then present the experimental results.
4.3.1 Baselines. All baselines assume the DR models pay attention to the same part of input for different sub-vectors. Position-based methods include “Tail", “Head", and “Rand" which only keep the tail, head, and random words while others are masked, respectively. Frequency-based methods involve “IDF", “TF", and "TF-IDF". Attribution-based methods involve “GlobalT" and “RandT". “GlobalT" computes the global attribution scores with respect to 𝒅^ instead of 𝒅^𝒊. “RandT" computes the attribution scores for another random discrete sub-vector 𝒅^𝒋.

Table 3: MSE error when only 5% words are retained. “12", “24", and “48" denote the number of sub-vectors. * denotes MoT performs significantly better than all baselines at 𝑝 < 0.01 level using two-tailed pairwise t-test. Best method is marked bold.

Method Setup
Position-based Tail Rand Head
Frequency-based IDF TF TF-IDF
Attribution-based RandT GlobalT MoT

MARCO Passage

12

24

48

4.20 4.18 3.52
4.17 3.63 3.46
3.36 3.08 2.73*

2.66 2.64 2.31
2.65 2.32 2.24
2.25 2.07 1.87*

1.82 1.80 1.60
1.82 1.58 1.54
1.59 1.45 1.26*

MARCO Document

12

24

48

3.06 3.22 2.09
3.40 2.01 1.96
1.90 1.50 1.39*

2.24 2.27 1.48
2.40 1.49 1.45
1.45 1.15 1.05*

1.14 1.16 0.82
1.23 0.80 0.78
0.81 0.64 0.54*

4.3.2 Results. Table 3 presents the experimental results. It clearly shows that MoT performs significantly better than all masking baselines, demonstrating the word importance is different for different discrete sub-vectors. Therefore, we validate the finding in our qualitative analysis that DR models pay attention to different aspects of input to extract different high-level representations.
5 CONCLUSION AND FUTURE WORK
Recently, DR models yield strong first-stage retrieval performance, but the internal mechanisms leading to its success remain unknown. Therefore, we propose RepMoT to discretize embeddings and then analyze DR models based on RepMoT. Results suggest DR models pay attention to different aspects of the input and capture different high-level topics. Therefore, we regard the representations encoded by DR models as mixture of topics.
We highlight several promising directions for building explainable DR. One is based on RepMoT, which can be regarded as rewriting the text with “discrete vector terms”. Researchers may manually design the connections between those “discrete vector terms” and “natural language terms” during training. The other is teaching DR models how to capture topics by designing attention patterns with some heuristics, e.g., topic models. We leave these to future research.
REFERENCES
[1] Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan Majumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen, et al. 2016. Ms marco: A human generated machine reading comprehension dataset. arXiv preprint arXiv:1611.09268 (2016).
[2] David M Blei. 2012. Probabilistic topic models. Commun. ACM 55, 4 (2012), 77–84.
[3] Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel Campos, and Ellen M. Voorhees. 2020. Overview of the TREC 2019 deep learning track. In Text REtrieval Conference (TREC). TREC.
[4] Luyu Gao and Jamie Callan. 2021. Unsupervised corpus aware language model pre-training for dense passage retrieval. arXiv preprint arXiv:2108.05540 (2021).
[5] Luyu Gao, Zhuyun Dai, Zhen Fan, and Jamie Callan. 2021. Complementing lexical retrieval with semantic residual embedding. In Proceedings of the 42nd European Conference in Information Retrieval.
[6] Tiezheng Ge, Kaiming He, Qifa Ke, and Jian Sun. 2013. Optimized product quantization. IEEE transactions on pattern analysis and machine intelligence 36, 4

(2013), 744–755. [7] Sebastian Hofstätter, Sheng-Chieh Lin, Jheng-Hong Yang, Jimmy Lin, and Allan
Hanbury. 2021. Efficiently Teaching an Effective Dense Retriever with Balanced Topic Aware Sampling. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR’21). 113–122. [8] Jui-Ting Huang, Ashish Sharma, Shuying Sun, Li Xia, David Zhang, Philip Pronin, Janani Padmanabhan, Giuseppe Ottaviano, and Linjun Yang. 2020. Embeddingbased Retrieval in Facebook Search. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. 2553–2561. [9] Herve Jegou, Matthijs Douze, and Cordelia Schmid. 2010. Product quantization for nearest neighbor search. IEEE transactions on pattern analysis and machine intelligence 33, 1 (2010), 117–128. [10] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense Passage Retrieval for OpenDomain Question Answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). [11] Yi Luan, Jacob Eisenstein, Kristina Toutanove, and Michael Collins. 2020. Sparse, dense, and attentional representations for text retrieval. arXiv preprint arXiv:2005.00181 (2020). [12] Yingqi Qu, Yuchen Ding, Jing Liu, Kai Liu, Ruiyang Ren, Wayne Xin Zhao, Daxiang Dong, Hua Wu, and Haifeng Wang. 2021. RocketQA: An optimized training approach to dense passage retrieval for open-domain question answering. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 5835–5847. [13] Mukund Sundararajan, Ankur Taly, and Qiqi Yan. 2017. Axiomatic attribution for deep networks. In International Conference on Machine Learning. 3319–3328. [14] Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul N. Bennett, Junaid Ahmed, and Arnold Overwijk. 2021. Approximate Nearest Neighbor Negative Contrastive Learning for Dense Text Retrieval. In International Conference on Learning Representations. [15] Jingtao Zhan, Jiaxin Mao, Yiqun Liu, Jiafeng Guo, Min Zhang, and Shaoping Ma. 2021. Jointly Optimizing Query Encoder and Product Quantization to Improve Retrieval Performance. In Proceedings of the 30th ACM International Conference on Information and Knowledge Management (CIKM’21). [16] Jingtao Zhan, Jiaxin Mao, Yiqun Liu, Jiafeng Guo, Min Zhang, and Shaoping Ma. 2021. Optimizing Dense Retrieval Model Training with Hard Negatives. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR’21). 1503–1512. [17] Jingtao Zhan, Jiaxin Mao, Yiqun Liu, Jiafeng Guo, Min Zhang, and Shaoping Ma. 2022. Learning Discrete Representations via Constrained Clustering for Effective and Efficient Dense Retrieval. In Proceedings of The 15th ACM International Conference on Web Search and Data Mining (WSDM’22). [18] Jingtao Zhan, Jiaxin Mao, Yiqun Liu, Min Zhang, and Shaoping Ma. 2020. An Analysis of BERT in Document Ranking. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval. 1941–1944. [19] Jingtao Zhan, Jiaxin Mao, Yiqun Liu, Min Zhang, and Shaoping Ma. 2020. RepBERT: Contextualized Text Embeddings for First-Stage Retrieval. arXiv preprint arXiv:2006.15498 (2020).

