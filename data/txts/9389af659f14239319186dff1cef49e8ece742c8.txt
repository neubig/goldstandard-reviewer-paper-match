OGB-LSC: A Large-Scale Challenge for Machine Learning on Graphs

arXiv:2103.09430v3 [cs.LG] 20 Oct 2021

Weihua Hu1, Matthias Fey2, Hongyu Ren1, Maho Nakata3, Yuxiao Dong4, Jure Leskovec1 1Department of Computer Science, Stanford University
2Department of Computer Science, TU Dortmund University 3RIKEN, 4Facebook AI
ogb-lsc@cs.stanford.edu

Abstract
Enabling effective and efﬁcient machine learning (ML) over large-scale graph data (e.g., graphs with billions of edges) can have a great impact on both industrial and scientiﬁc applications. However, existing efforts to advance large-scale graph ML have been largely limited by the lack of a suitable public benchmark. Here we present OGB Large-Scale Challenge (OGB-LSC), a collection of three real-world datasets for facilitating the advancements in large-scale graph ML. The OGB-LSC datasets are orders of magnitude larger than existing ones, covering three core graph learning tasks—link prediction, graph regression, and node classiﬁcation. Furthermore, we provide dedicated baseline experiments, scaling up expressive graph ML models to the massive datasets. We show that expressive models signiﬁcantly outperform simple scalable baselines, indicating an opportunity for dedicated efforts to further improve graph ML at scale. Moreover, OGB-LSC datasets were deployed at ACM KDD Cup 2021 and attracted more than 500 team registrations globally, during which signiﬁcant performance improvements were made by a variety of innovative techniques. We summarize the common techniques used by the winning solutions and highlight the current best practices in large-scale graph ML. Finally, we describe how we have updated the datasets after the KDD Cup to further facilitate research advances. The OGB-LSC datasets, baseline code, and all the information about the KDD Cup are available at https://ogb.stanford.edu/docs/lsc/.

1 Introduction

Machine Learning (ML) on graphs has attracted immense attention in recent years because of the prevalence of graphstructured data in real-world applications.

Table 1: Basic statistics of the OGB-LSC datasets used in KDD Cup 2021. Datasets marked by † has been updated to v2 after the KDD Cup (cf. Section 3).

Modern application domains include Webscale social networks (Ugander et al., 2011), recommender systems (Ying et al., 2018), hyperlinked Web documents (Kleinberg, 1999), knowledge graphs (KGs) (Bollacker et al., 2008; Vrandecˇic´ and Kro¨tzsch,

Task type Node-level Link-level

Dataset MAG240M
† WikiKG90M

#nodes: #edges:
#nodes: #edges:

Statistics
244,160,499 1,728,364,232
87,143,637 504,220,369

2014), as well as the molecule simulation data generated by the ever-increasing scien-

Graph-level

† PCQM4M

#graphs: #edges (total):

3,803,453 55,399,880

tiﬁc computation (Nakata and Shimazaki,

2017; Chanussot et al., 2021). All these domains involve large-scale graphs with billions of edges or

a dataset with millions of graphs. Deploying accurate graph ML at scale will have a huge practical im-

35th Conference on Neural Information Processing Systems (NeurIPS 2021) Track on Datasets and Benchmarks.

Node-level
MAG240M-LSC
Paper
cites

Subject area? Predict

Author

writes Institution
affiliated with

Link-level
WikiKG90M-LSC

Geoffrey Hinton

Canada

Predict
?

born in
is a Graduated from

affiliated with

located in

graduated from University of Toronto
Paul Martin

Person

King’s College, Cambridge

Graph-level
PCQM4M-LSC

Quantum calculation (DFT) ~hours

HOMO-LUMO gap? Predict << 1s

(a) MAG240M

(b) WikiKG90M

(c) PCQM4M

Figure 1: Overview of the three OGB-LSC datasets, covering node-, link-, and graph-level prediction tasks, respectively. (a) MAG240M is a heterogeneous academic graph, and the task is to predict the subject areas of papers situated in the heterogeneous graph (cf. Section 2.1). (b) WikiKG90M is a knowledge graph, and the task is to impute missing triplets (cf. Section 2.2). (c) PCQM4M is a quantum chemistry dataset, and the task is to predict an important molecular property—the HOMO-LUMO gap—of a given molecule (cf. Section 2.3).

pact, enabling better recommendation results, improved web document search, more comprehensive KGs, and accurate ML-based drug and material discovery.
However, community efforts to advance state-of-the-art in large-scale graph ML have been quite limited. In fact, most of graph ML models have been developed and evaluated on extremely small datasets (Yang et al., 2016; Morris et al., 2020; Bordes et al., 2013). Recently, the Open Graph Benchmark (OGB) has been introduced to provide a collection of larger graph datasets (Hu et al., 2020a), but they are still small compared to graphs found in the industrial and scientiﬁc applications.
Handling large-scale graphs is challenging, especially for state-of-the-art expressive Graph Neural Networks (GNNs) (Kipf and Welling, 2017; Hamilton et al., 2017; Velickovic et al., 2018) because they make predictions on each node based on the information from many other nodes. Effectively training these models at scale requires sophisticated algorithms that are well beyond standard SGD over i.i.d. data (Hamilton et al., 2017; Chen et al., 2018; Chiang et al., 2019; Zeng et al., 2020). More recently, researchers improve the model scalability by signiﬁcantly simplifying GNNs (Wu et al., 2019; Rossi et al., 2020; Huang et al., 2020), which inevitably limits their expressive power.
However, in deep learning, it has been demonstrated over and over again that one needs big expressive models and train them on big data to achieve the best performance (He et al., 2016; Russakovsky et al., 2015; Vaswani et al., 2017; Devlin et al., 2018; Brown et al., 2020). In graph ML, the trend has been the opposite—models get simpliﬁed and less expressive to be able to scale to large graphs (Wu et al., 2019). Thus, there is a massive opportunity to enable graph ML techniques to work with realistic and large-scale graph datasets, exploring the potential of expressive models for big graphs.
Here we present a large-scale graph ML challenge, OGB Large-Scale Challenge (OGB-LSC), to facilitate the development of state-of-the-art graph ML models for massive modern datasets. Speciﬁcally, we introduce three large-scale, realistic, and challenging datasets—MAG240M, WikiKG90M, and PCQM4M—that are unprecedentedly large in scale (see Table 1; the sizes are 10 to 100 times larger than the corresponding original OGB datasets1) and cover predictions at the level of nodes, links, and graphs, respectively. An overview of the datasets is provided in Figure 1.
Beyond providing the datasets, we perform an extensive baseline analysis on each dataset and implement both simple baseline models and advanced expressive models at scale. We ﬁnd that advanced expressive models—despite requiring more efforts to scale up—do beneﬁt from the large data and signiﬁcantly outperform simple baseline models that are easy to scale.
To facilitate the community engagement, we recently organized the ACM KDD Cup 2021 around the OGB-LSC datasets. The competition attracted more than 500 team registrations and 150 leaderboard submissions. Within the three-month duration of the competition (March 15 to June 15, 2021), we have already witnessed innovative methods being developed to provide impressive performance
1Speciﬁcally, MAG240M is 126 times larger than ogbn-mag in terms of the number of nodes, WikiKG90M is 35 times larger than ogbl-wikikg2 in terms of the number of nodes, and PCQM4M is 9 times larger than ogbg-molpcba in terms of the number of graphs.

2

gains2, further solidifying the value of the OGB-LSC datasets to advance state-of-the-art. We summarize the common techniques shared by the winning solutions, highlighting the current best practices of large-scale graph ML. Moreover, based on the lessons learned from the KDD Cup, we describe the future plan to update the datasets so that they can be further used to advance large-scale graph ML.
2 OGB-LSC Datasets, Baselines, and KDD Cup Summary
We describe the OGB-LSC datasets, covering three key task categories (node-, link-, and graph-level prediction tasks) of ML on graphs. We emphasize the practical relevance and data split for each dataset, making our task closely aligned to realistic applications. Through our extensive baseline experiments, we show that advanced expressive models tend to give much better performance than simple graph ML models, leaving room for further improvement. All the OGB-LSC datasets are available through the OGB Python package (Hu et al., 2020a). All the baseline and package code is available at https://github.com/snap-stanford/ogb.
In addition, we highlight the top 3 winning results from our KDD Cup 2021 that signiﬁcantly advance state-of-the-art and summarize common techniques used by the winning solutions. Note that while our baselines only used a single model for simplicity, all the winners used extensive model ensembling for their test submissions in order to maximize the performance. For a more direct comparison, we also report the winners’ self-reported validation accuracy in the main text, which still exhibits signiﬁcant accuracy improvement over our strong baselines.
2.1 MAG240M: Node-Level Prediction
Practical relevance and dataset overview. The volume of scientiﬁc publication has been increasing exponentially, doubling every 12 years (Dong et al., 2017). Currently, subject areas of ARXIV papers are manually determined by the paper’s authors and ARXIV moderators. An accurate automatic predictor of papers’ subject categories not only reduces the signiﬁcant burden of manual labeling, but can also be used to classify the vast number of non-ARXIV papers, thereby allowing better search and organization of academic papers.
MAG240M is a heterogeneous academic graph extracted from the Microsoft Academic Graph (MAG) (Wang et al., 2020). Given arXiv papers situated in the heterogeneous graph, whose schema diagram is illustrated in Figure 2, we aim to automatically annotate their topics, i.e., predicting the primary subject area of each ARXIV paper.
Graph. We extract 121M academic papers in English from MAG (version: 2020-11-23) to construct a heterogeneous academic graph. The resultant paper set is written by 122M author entities, who are afﬁliated with 26K institutes. Among these papers, there are 1.3 billion citation links captured by MAG. Each paper is associated with its natural language title and most papers’ abstracts are also available. We concatenate the title and abstract by period and pass it to a ROBERTA sentence encoder (Liu et al., 2019; Reimers and Gurevych, 2019), generating a 768-dimensional vector for each paper node. Among the 121M paper nodes, approximately 1.4M nodes are ARXIV papers annotated with 153 ARXIV subject areas, e.g., cs.LG (Machine Learning). On the paper nodes, we attach the publication years as meta information.
Prediction task and evaluation metric. The task is to predict the primary subject areas of the given ARXIV papers, which is cast as an ordinary multi-class classiﬁcation problem. The metric is the classiﬁcation accuracy.
To understand the relation between the prediction task and the heterogeneous graph structure, we analyze the graph homophily (McPherson et al., 2001)—tendency of two adjacent nodes to share the same labels—to better understand the interplay between heterogeneous graph connectivity and the prediction task. Homophily is normally analyzed over a homogeneous graph, but we extend the analysis to the heterogenous graph by considering meta-paths (Sun et al., 2011)—a path consisting of a sequence of relations deﬁned between different node types. Given a meta-path, we can say two nodes are adjacent if they are connected by the meta-path. Table 3 shows the homophily for different kinds of meta-paths with different levels of connection strength. Compared to the direct
2See the results at https://ogb.stanford.edu/kddcup2021/results/
3

citation connection (i.e., P-P), certain meta-paths (i.e., P-A-P) give rise to much higher degrees of homophiliness, while other meta-paths (i.e., P-A-I-A-P) provide much less homophily. As homophily is the central graph property exploited by many graph ML models, we believe that discovering essential heterogeneous connectivity is important to achieve good performance on this dataset.
Dataset split. We split the data according to time. Speciﬁcally, we train models on ARXIV papers published until 2018, validate the performance on the 2019 papers, and ﬁnally test the performance on the 2020 papers. The split reﬂects the practical scenario of helping the authors and moderators annotate the subject areas of the newly-published ARXIV papers.
Baseline. We benchmark a broad range of graph ML models in both homogeneous (where only paper to paper relations are considered) and full heterogeneous settings. For both settings, we convert the directed graph into an undirected graph for simplicity. First, for the homogeneous setting, we benchmark the simple baseline models: graph-agnostic MLP, Label Propagation, and the recently-proposed simpliﬁed graph methods: SGC (Wu et al., 2019), SIGN (Rossi et al., 2020) and MLP+C&S (Huang et al., 2020), which are inherently scalable by decoupling predictions from propagation. Furthermore, we benchmark state-of-the-art expressive GNNs trained with neighborhood sampling (NS) (Hamilton et al., 2017), where we recursively sample 25 neighbors in the ﬁrst layer and 15 neighbors in the second layer during training time. At inference time, we sample at most 160 neighbors for each layer. Here, we benchmark two types of strong models: the GRAPHSAGE (Hamilton et al., 2017) model (performing mean aggregation and utilizing skip-connections), and the more advanced GRAPH ATTENTION NETWORK (GAT) model (Velickovic et al., 2018). For the full heterogeneous setting, we follow Schlichtkrull et al. (2018) and learn distinct weights for each individual relation type (denoted by R-GRAPHSAGE and R-GAT, where “R” stands for “Relational”). We obtain the input features of authors and institutions by averaging the features of papers belonging to the same author and institution, respectively. The models are trained with NS. We note that the expressive GNNs trained with NS require more efforts to scale up, but are more expressive than the simple baselines.
Hyper-parameters. Hyper-parameters are selected based on their best validation performance. For all the models without NS, we tuned the hidden dimensionality ∈ {128, 256, 512, 1024}, MLP depth ∈ {1, 2, 3, 4}, dropout ratio ∈ {0, 0.25, 0.5}, propagation layers (for SGC, SIGN, and C&S) ∈ {2, 3}. For all the GNN models with NS, we use a hidden dimensionality of 1024. We make use of batch normalization (Ioffe and Szegedy, 2015) and ReLU activation in all models.
Discussion. Validation and test performances of all models considered are shown in Table 2. First, the graph-agnostic MLP and Label Propagation algorithm perform poorly, indicating that both graph structure and feature information are indeed important for the given task. Across the graph ML models operating on the homogeneous paper graph, GNNs with NS perform the best, with slight gains compared to their simpliﬁed versions. In particular, the advanced expressive graph attention aggregation is favourable compared to the uniform mean aggregation in GRAPHSAGE. Furthermore, considering all available heterogeneous relational structure in the heterogeneous graph setting yields signiﬁcant improvements, with performance gains up to 3 percentage points. Again, the advanced attention aggregation provides favorable performance. Overall, our experiments highlight the beneﬁts of developing and evaluating advanced expressive models on the larger scale.
KDD Cup 2021 summary. In Table 2, we show the results of the top 3 winners of the KDD Cup: BD-PGL Team (Shi et al., 2021), Academic Team (Addanki et al., 2021), and Synerise AI Team (Daniluk et al., 2021). All the solutions outperform our baselines signiﬁcantly, yielding 5–6% gain in test accuracy. For a more direct comparison, with a single model (no model ensembling), the BD-PGL Team reports a validation accuracy of 73.71% (Shi et al., 2021), improving our best R-GAT baseline by 3.7%.
Notably, all the winning solutions used the target labels as input to their models, which allows the models to propagate labels together with the features. Regarding the GNN architectures, the BD-PGL adopted the expressive Transformer-based UniMP architecture (Shi et al., 2020), while the Academic adopted the standard MPNN (Gilmer et al., 2017) but trained it with self-supervised contrastive learning on unlabeled paper nodes (Thakoor et al., 2021). These results suggest that expressive GNNs are indeed promising for this dataset. Finally, both the BD-PGL and Academic teams exploited the temporal aspect of the academic graph by using the publication years either as input positional encoding (Shi et al., 2021) or as a way to sample mini-batch subgraphs for
4

Paper 121,751,666 nodes

cites 1,297,748,926 edges

writes 386,022,720 edges

Author 122,383,112 nodes

affiliated with 44,592,586 edges

Institution 25,721 nodes

Figure 2: A schema diagram of MAG240M.

Table 2: Results of MAG240M measured by the accuracy (%).

Model

#Params Validation Test

MLP LABELPROP SGC SIGN MLP+C&S GRAPHSAGE (NS) GAT (NS)

0.5M 0
0.7M 3.8M 0.5M 4.9M 4.9M

52.67 58.44 65.82 66.64 66.98 66.79 67.15

52.73 56.29 65.29 66.09 66.18 66.28 66.80

R-GRAPHSAGE (NS) R-GAT (NS)

12.2M 12.3M

69.86 70.02

68.94 69.42

KDD 1ST: BD-PGL KDD 2ND: ACADEMIC KDD 3RD: SYNERISE AI

75.49 75.19 74.60

Table 3: Analysis of graph homophily for different meta-paths connecting 1,251,341 arXiv papers (only train+validation). Connection strength indicates the number of different possible paths along the template meta-path, e.g., meta-path “PaperAuthor-Paper (P-A-P)” with connection strength 3 means that at least 3 authors are shared for the two papers of interest. Homophily ratio is the ratio of two nodes having the same target labels.

Meta-path P-P P-A-P
P-A-I-A-P All pairs

Connect. strength
1
1 2 4 8 16
1 2 4 8 16
0

Homophily ratio (%)
57.80
46.12 57.02 64.03 66.65 70.46
3.83 4.61 5.69 6.85 7.70
1.99

#Edges
2,017,844
88,099,071 12,557,765
1,970,761 476,792 189,493
159,884,165,669 81,949,449,717 33,764,809,381 12,390,929,118 4,471,932,097
782,926,523,470

GNNs (Addanki et al., 2021). As real-world large-scale graphs are almost always dynamic, exploiting the temporal information is a promising direction of future research.
2.2 WikiKG90M: Link-Level Prediction
Practical relevance and dataset overview. Large encyclopedic Knowledge Graphs (KGs), such as Wikidata (Vrandecˇic´ and Kro¨tzsch, 2014) and Freebase (Bollacker et al., 2008), represent factual knowledge about the world through triplets connecting different entities, e.g., Hinton −c−it−iz−e−n−o→f Canada. They provide rich structured information about many entities, aiding a variety of knowledgeintensive downstream applications such as information retrieval, question answering (Singhal, 2012), and recommender systems (Guo et al., 2020). However, these large KGs are known to be far from complete (Min et al., 2013), missing many relational information between entities.
WikiKG90M is a Knowledge Graph (KG) extracted from the entire Wikidata knowledge base. The task is to automatically impute missing triplets that are not yet present in the current KG. Accurate imputation models can be readily deployed on the Wikidata to improve its coverage.
Graph. Each triplet (head, relation, tail) in WikiKG90M represents an Wikidata claim, where head and tail are the Wikidata items, and relation is the Wikidata predicate. We extracted triplets from the public Wikidata dump downloaded at three time-stamps: September 28, October 26, and November 23 of 2020, for training, validation, and testing, respectively. We retain all the entities and relations in the September dump, resulting in 87,143,637 entities, 1,315 relations, and 504,220,369 triplets in total.
In addition to extracting triplets, we provide text features for entities and relations. Speciﬁcally, each entity/relation in Wikidata is associated with a title and a short description, e.g., one entity is associated with the title ‘Geoffrey Hinton‘ and the description ‘computer scientist and psychologist‘. Similar to MAG240M, we provide ROBERTA embeddings (Reimers and Gurevych, 2019; Liu et al., 2019) as node and edge features.3
3We concatenate the title and description with comma, e.g., ‘Geoffrey Hinton, computer scientist and psychologist‘, and pass the sentence to a ROBERTA sentence encoder (Note that the ROBERTA model was trained before September 2020, so there is no obvious information leak). The title or/and description are sometimes missing, in which case we simply use the blank sentence to replace it.

5

Prediction task and evaluation metric. The task is the KG completion, i.e., given a set of training triplets, predict a set of test triplets. For evaluation, we follow the protocol similar to how KG completion is evaluated (Bordes et al., 2013). Speciﬁcally, for each validation/test triplet, (head, relation, tail), we corrupt tail with randomly-sampled 1000 negative entities, e.g., tail neg, such that (head, relation, tail neg) does not appear in the train/validation/test KG. The model is asked to rank the 1001 candidates (consisting of 1 positive and 1000 negatives) for each triplet and predict the top 10 entities that are most likely to be positive. The goal is to rank the ground-truth positive entity as high in the rank as possible, which is measured by Mean Reciprocal Rank (MRR). 4
Dataset split. We split the triplets according to time, simulating a realistic KG completion scenario of imputing missing triplets not present at a certain timestamp. Speciﬁcally, we construct three KGs using the aforementioned September, October, and November KGs, where we only retain entities and relation types that appear in the earliest September KG. We use the triplets in the September KG for training, and use the additional triplets in the October and November KGs for validation and test, respectively.
We analyze the effect of the time split. We ﬁnd that head entities of validation triplets tend to be less popular entities; on average, they only have 6.5 out-degrees in the training KG, which is less than a quarter of the out-degree averaged over training triplets (i.e., 28.0). This suggests that learning signals for predicting validation (and test) triplets are sparse. Nonetheless, even for the sparsely-connected triplets, we ﬁnd the textual information provides important clues, as illustrated in Table 4. Hence, we expect that advanced graph models that effectively incorporate textual information will be key to achieve good performance on the challenging time split.
Baseline. We consider two representative KG embedding models: TRANSE (Bordes et al., 2013) and COMPLEX (Trouillon et al., 2016). These models deﬁne their own decoders to score knowledge triplets using the corresponding entity and relation embeddings. For instance, TRANSE uses − h + r − t 2 as the decoder, where h, r, and t are embeddings of head, relation, and tail, respectively. For the encoder function (mapping each entity and relation to its embedding), we consider the following three options. Shallow: We use the distinct embedding for each entity and relation, as normally done in KG embedding models. RoBERTa: We use two MLP encoders (one for entity and another for relation) that transform the ROBERTA features into entity and relation embeddings. Concat: To enhance the expressive power of the previous encoder, we concatenate the shallow learnable embeddings into the ROBERTA features, and use the MLPs to transform the concatenated vectors to get the ﬁnal embeddings. This way, the MLP encoders can adaptively utilize the ROBERTA features and the shallow embeddings to ﬁt the large amount of triplet data. To implement our baselines, we utilize DGL-KE (Zheng et al., 2020).
Hyper-parameters. For the loss function, we use the negative sampling loss from Sun et al. (2019), where we pick margin γ from {1,4,8,10,100}. In order to balance the performance and the memory cost, we use the embedding dimensionality of 200 for all the models.
Discussion. Table 5 shows the validation and test performance of the six different models, i.e., combination of two decoders (TRANSE and COMPLEX) and three encoders (SHALLOW, ROBERTA, and CONCAT). Notably, in terms of the encoders, we see that the most expressive CONCAT outperforms both SHALLOW and ROBERTA, indicating that both the textual information (captured by the ROBERTA embeddings) and structural information (captured by node-wise learnable embeddings) are useful in predicting validation and test triplets. In terms of the decoders, TRANSE and COMPLEX show similar performance with the CONCAT encoder, while they show somewhat mixed results with the SHALLOW and ROBERTA encoders.
Overall, our experiments suggest that the expressive encoder that combines both textual information and structural information gives the most promising performance. In the KG completion literature, the design of the encoder has been much less studied compared to the decoder designs. Therefore, we believe there is a huge opportunity in scaling up more advanced encoders, especially GNNs (Schlichtkrull et al., 2018), to further improve the performance on this dataset.
KDD Cup 2021 summary. Table 5 shows the results of the top 3 winners of the KDD Cup: BD-PGL Team (Su et al., 2021), OhMyGod Team (Peng et al., 2021), and the GraphMIRAcles Team (Cai et al., 2021). All the winning solutions outperform our strong baselines signiﬁcantly,
4Note that this is more strict than the standard MRR since there is no partial score for positive entities being ranked outside of top 10.
6

Table 4: Textual representation of validation triplets whose head entities only appear once as head in the training WikiKG90M.

Head

Relation

Tail

Food and drink companies of Bulgaria combines topics Bulgaria

Performing arts in Denmark

combines topics performing arts

Anglicanism in Grenada

combines topics Anglicanism

Chuan Li

occupation researcher

Petra Junkova

given name Petra

Table 5: Results of WikiKG90M measured by Mean Reciprocal Rank (MRR).

Model

#Params Validation Test

TRANSE-SHALLOW COMPLEX-SHALLOW TRANSE-ROBERTA COMPLEX-ROBERTA TRANSE-CONCAT COMPLEX-CONCAT

17.4B 17.4B 0.3M 0.3M 17.4B 17.4B

0.7559 0.6142 0.6039 0.7052 0.8494 0.8425

0.7412 0.5883 0.6288 0.7186 0.8548 0.8637

KDD 1ST: BD-PGL KDD 2ND: OHMYGOD KDD 3RD: GRAPHMIRACLES

0.9727 0.9712 0.9707

achieving near-perfect test MRR score of 0.97. For a more direct comparison, with a single model (no model ensembling), the BD-PGL Team reports a validation MRR of 0.92 (Su et al., 2021), improving our best COMPLEX-CONCAT baseline by 0.07 points in validation MRR. Similar to our baselines, all the winners utilize the KG embedding approach as the backbone, and adopt the encoder that takes both shallow embedding and textual embeddings into account. Speciﬁcally, BD-PGL proposed the NOTE model (Su et al., 2021) which makes the ROTATE model more expressive, while OhMyGod adopted the ensemble of several existing KG embedding models. On the other hand, GraphMIRAcles explored different design choices for the encoder and found that adding residual connection for shallow embeddings signiﬁcantly improved the model performance.
In addition to the model advances, all the winners exploited some statistical property of candidate tail entities. Most notably, Yang et al. (2021) found that simply by sorting the candidate tails by the frequency they appear in the training KG, it was possible to achieve validation MRR of 0.75, rivaling our TRANSE-SHALLOW baseline. This highlights that our negative tail candidates are mostly rare entities that can be easily distinguished from the true tail entity. On the other hand, the practical KG completion presents a much harder challenge: the candidate tails are not provided, and a model needs to predict the true tail entity out of all the possible 87M entities. As the performance on WikiKG90M has already saturated, we have updated WikiKG90M to WikiKG90Mv2 to reﬂect the realistic setting in large-scale KG completion. See Section 3 for further details.
2.3 PCQM4M: Graph-Level Prediction
Practical relevance and dataset overview. Density Functional Theory (DFT) is a powerful and widely-used quantum physics calculation that can accurately predict various molecular properties such as the shape of molecules, reactivity, responses by electromagnetic ﬁelds (Burke, 2012). However, DFT is time-consuming and takes up to several hours per small molecule. Using fast and accurate ML models to approximate DFT enables diverse downstream applications, such as property prediction for organic photovaltaic devices (Cao and Xue, 2014) and structure-based virtual screening for drug discovery (Ferreira et al., 2015).
PCQM4M is a quantum chemistry dataset originally curated under the PubChemQC project (Nakata, 2015; Nakata and Shimazaki, 2017). Based on the PubChemQC, we deﬁne a meaningful ML task of predicting DFT-calculated HOMO-LUMO energy gap of molecules given their 2D molecular graphs. The HOMO-LUMO gap is one of the most practically-relevant quantum chemical properties of molecules since it is related to reactivity, photoexcitation, and charge transport (Grifﬁth and Orgel, 1957). Moreover, predicting the quantum chemical property only from 2D molecular graphs without their 3D equilibrium structures is also practically favorable. This is because obtaining 3D equilibrium structures requires DFT-based geometry optimization, which is expensive on its own.
To ensure the resulting models are practically useful, we limit the average inference budget per molecule (including both pre-processing and model inference) to be less than 0.1 second using a single GPU and CPU (multi-threading on a multi-core CPU is allowed). This means that expensive (quantum) calculations cannot be used to perform inference. As our test set contains 377,423 molecules, we require the all the prediction to be made within 12 hours. Note that this time constraint is quite generous for ordinary GNNs—each of our baseline GNN only took about 3 minutes to perform inference over the entire test data.
7

Graph. We provide molecules as the SMILES strings (Weininger, 1988), from which 2D molecule graphs (nodes are atoms and edges are chemical bonds) as well as molecular ﬁngerprints (handengineered molecular feature developed by the chemistry community) can be obtained. By default, we follow OGB (Hu et al., 2020a) to convert the SMILES string into a molecular graph representation, where each node is associated with a 9-dimensional feature (e.g., atomic number, chirality) and each edge comes with a 3-dimensional feature (e.g., bond type, bond stereochemistry), although the optimal set of input graph features remains to be explored.
Prediction task and evaluation metric. The task is graph regression: predicting the HOMOLUMO energy gap in electronvolt (eV) given 2D molecular graphs. Mean Absolute Error (MAE) is used as evaluation metric.
Dataset split. We split molecules by their PubChem ID (CID) with ratio 80/10/10. Our original intention was to provide the scaffold split (Hu et al., 2020a; Wu et al., 2018), but the provided data turns out to be split by the CID due to some pre-processing bug. The CID number itself does not indicate particular meaning about the molecule, but splitting by CID may provide a moderate distribution shift (most likely not as severe as the scaffold split). We empirically compared the CID and scaffold splits and found the model performances were consistent between the two splits.5
Baseline. We benchmark two types of models: a simple MLP over the Morgan ﬁngerprint (Morgan, 1965) and more advanced GNN models. For GNNs, we use the four strong models developed for graph-level prediction: Graph Convolutional Network (GCN) (Kipf and Welling, 2017) and Graph Isomorphism Network (GIN) (Xu et al., 2019), as well as their variants, GCN-VIRTUAL and GIN-VIRTUAL, which augment graphs with a virtual node that is bidirectionally connected to all nodes in the original graph (Gilmer et al., 2017). Adding the virtual node is shown to be effective across a wide range of graph-level prediction datasets in OGB (Hu et al., 2020a). Edge features are incorporated following Hu et al. (2020b). At inference time, the model output is clamped between 0 and 50 to avoid model’s anomalously large/small prediction.
Hyper-parameters. For the MLP over Morgan ﬁngerprint, we set the ﬁngerprint dimensionality to be 2048, and tune the ﬁngerprint radius ∈ {2, 3}, as well as MLP’s hyper-parameters: hidden dimensionality ∈ {1200, 1600}, number of hidden layers ∈ {2, 4, 6}, and dropout ratio ∈ {0, 0.2}. For GNNs, we tune hidden dimensionality, i.e., width ∈ {300, 600}, number of GNN layers, i.e., depth ∈ {3, 5}. Simple summation is used for graph-level pooling. For all MLPs (including GIN’s), we use batch normalization (Ioffe and Szegedy, 2015) and ReLU activation.
Discussion. The validation and test results are shown in Table 6. We see both the GNN models signiﬁcantly outperform the simple ﬁngerprint baseline. Expressive GNNs (GIN and GIN-VIRTUAL) outperform less expressive ones (GCN and GCN-VIRTUAL); especially, the most advanced and expressive GIN-VIRTUAL model signiﬁcantly outperforms the other GNNs. Nonetheless, the current performance is still much worse than the chemical accuracy of 0.043eV—an indicator of practical usefulness established by the chemistry community. In the same Table 6, we show our ablation, where we use only 10% of data to train the GIN-VIRTUAL model. We see the performance signiﬁcantly deteriorate, indicating the importance of training the model on large data. Finally, in Table 7, we show the relation between model sizes and validation performance. We see that the largest models always achieve the best performance.
Overall, we ﬁnd that advanced, expressive, and large GNN model gives the most promising performance on the PCQM4M dataset, although the performance still needs to be improved for practical use. We believe further advances in advanced modeling, expressive architectures, and larger model sizes could yield breakthrough in the large-scale molecular property prediction task.
KDD Cup 2021 summary. In Table 6, we show the results of the top 3 winners of the KDD Cup: Machine Learning Team (Ying et al., 2021b), SuperHelix Team (Zhang et al., 2021), and Quantum Team (Addanki et al., 2021). The winners have signiﬁcantly reduced the MAE compared our baselines, yielding around 0.03 points improvement in test MAE. For a more direct comparison, with a single model, the Machine Learning reports the validation MAE of 0.097 for their Graphormer model (Ying et al., 2021a), which is 0.04 points lower than our best GIN-VIRTUAL baseline.
5Detailed discussion can be found at https://github.com/snap-stanford/ogb/ discussions/162
8

Table 6: Results of PCQM4M measured by MAE [eV]. The lower, the better. Ablation study of using only 10% of training data is also shown. Chemical accuracy indicates the ﬁnal goal for practical usefulness.

Model

#Params Validation Test

MLP-FINGERPRINT GCN GCN-VIRTUAL GIN GIN-VIRTUAL

16.1M 2.0M 4.9M 3.8M 6.7M

0.2044 0.1684 0.1510 0.1536 0.1396

0.2070 0.1842 0.1580 0.1685 0.1494

MLP-FINGERPRINT (10% train) GIN-VIRTUAL (10% train)

6.8M 0.2708 0.2659 6.7M 0.1790 0.1892

KDD 1ST: MACHINELEARNING KDD 2ND: SUPERHELIX KDD 3RD: QUANTUM

0.1208 0.1210 0.1211

Chemical accuracy (goal)

–

0.0430

Table 7: Model size and the MAE performance [eV]. For both models, the width indicates the hidden dimensionality. For GINVIRTUAL, the depth represents the number of GNN layers, while for the MLP-FINGERPRINT, the depth represents the the number of hidden layers in MLP.

Model

Width Depth #Params Validation

1600 6 MLP-FINGERPRINT 1600 4
1600 2 1200 6

16.1M 11.0M
5.8M 9.7M

0.2044 0.2044 0.2220 0.2083

GIN-VIRTUAL

600 5 600 3 300 5 300 3

6.7M 3.7M 1.7M 1.0M

0.1410 0.1462 0.1442 0.1512

In terms of methodology, we ﬁnd that the winning solutions share three important components in common. (1) Their winning GNN models are indeed large and deep; the number of learnable parameters (single model) ranges from 50M up to 450M, while the number of GNN layers ranges from 11 up to 50, being signiﬁcantly larger than our baseline models. (2) All the GNNs perform global message passing at each layer, either through the virtual nodes (Gilmer et al., 2017) or fully-connected Transformer-style self-attention (Ying et al., 2021a). (3) All the winners utilize 3D structure of molecules to supervise their GNNs. As 3D structure was not provided at our KDD Cup, the winners generate the 3D structure themselves using RDkit (Landrum et al., 2006) or PySCF (Sun et al., 2020), both of which provide cheap but less accurate 3D structure of molecules.
As modeling 3D molecular graphs is a promising direction in graph ML (Schu¨tt et al., 2017; Klicpera et al., 2020; Sanchez-Gonzalez et al., 2020; Hu et al., 2021), we have updated PCQM4M to PCQM4Mv2 to include DFT-calculated 3D structures for training molecules. Details are provided in Section 3.

3 Updates after the KDD Cup
To facilitate further research advances, we have updated the datasets and leaderboards based on the lessons learned from our KDD Cup 2021. Here we brieﬂy describe our updates. More details are provided in Appendix C.
Updates on WikiKG90M. From the KDD Cup results, we learned that most of our provided negative entities in the large-scale WikiKG90M are “easy negatives”, and our current task gives overly-optimistic performance scores. In a realistic large-scale KG completion setting, ML models are required to predict the true tail entity from nearly 90M entities, which is much more challenging. To reﬂect this challenge, we have updated WikiKG90M to WikiKG90Mv2, where we do not provide any candidate entities for validation/test triples. Our initial experiments using the same set of baseline models, shows that WikiKG90Mv2 indeed provides a much harder challenge; our best model COMPLEX-CONCAT only achieves 0.1833 MRR on WikiKG90Mv2 as opposed to achieving 0.8637 MRR on WikiKG90M, leaving signiﬁcant room for further improvement.
Updates on PCQM4M. From the KDD Cup results, we saw that the winners effectively utilized (self-calculated) 3D structure of molecules. Modeling molecular graphs in 3D space is of great interest to the graph ML community; We therefore have updated PCQM4M to PCQM4Mv2, where we provide DFT-calculated 3D structure for training molecules. For validation and test molecules, 3D structures is not be provided, and ML models still need to make prediction based on the 2D molecular graphs. In updating to PCQM4Mv2, we are also ﬁxing subtle but important mismatch between some of the 2D molecular graphs and the corresponding 3D molecular graphs. Our preliminary experiments on PCQM4Mv2 suggest that the all the baseline models’ MAE is improved by ≈ 0.04 [eV] compared to PCQM4M, although the trends in model performance stay almost the same as PCQM4M.

9

Updates on leaderboards. We are introducing public leaderboards to facilitate further research advances after our KDD Cup. The test submissions of the KDD Cup 2021 were evaluated on the entire hidden test set. After the KDD Cup, we are randomly splitting the test set into two: “test-dev” and “test-challenge”. The test-dev set is be used for public leaderboards that evaluate test submissions any time during a year. The test-challenge set is be left for future competitions, which we plan to hold annually to facilitate community engagement. The leaderboards have been released together with the updated datasets.
4 Conclusions
Modern applications of graph ML involve large-scale graph data with billions of edges or millions of graphs. ML advances on large graph data have been limited due to the lack of a suitable benchmark. Here we present OGB-LSC, with the goal of advancing state-of-the-art in large-scale graph ML. OGB-LSC provides the three large-scale realistic benchmark datasets, covering the core graph ML tasks of node classiﬁcation, link prediction, and graph regression. We perform dedicated baseline analysis, scaling up advanced graph models to large graphs. We show that advanced and expressive models can signiﬁcantly outperform simpler baseline models, suggesting opportunities for further dedicated effort to yield even better performance.
We used our datasets for the recent ACM KDD Cup 2021, where we have attracted huge engagement from the community and have already witnessed signiﬁcant performance improvement. We summarize the winning solutions for each dataset, highliting the current best practices in large-scale graph ML. Finally, we describe how we have updated our datasets after the KDD Cup to further facilitate research advances. Overall, we hope OGB-LSC encourages dedicated community efforts to tackle the important but challenging problem of large-scale graph ML.
Acknowledgement
We thank Michele Catasta and Larry Zitnick for helpful discussion, Shigeru Maya for motivating the project, Adrijan Bradaschia for setting up the server for the project, and Amit Bleiweiss, Benjamin Braun and Hanjun Dai for providing helpful feedback on our baseline code, and the DGL Team for hosting our large datasets.
Stanford University is supported by DARPA under Nos. N660011924033 (MCS); ARO under Nos. W911NF-16-1-0342 (MURI), W911NF-16-1-0171 (DURIP); NSF under Nos. OAC-1835598 (CINES), OAC-1934578 (HDR), CCF-1918940 (Expeditions), IIS-2030477 (RAPID); Stanford Data Science Initiative, Wu Tsai Neurosciences Institute, Chan Zuckerberg Biohub, Amazon, JPMorgan Chase, Docomo, Hitachi, JD.com, KDDI, NVIDIA, Dell, Toshiba, Intel, and UnitedHealth Group. Weihua Hu is supported by Funai Overseas Scholarship and Masason Foundation Fellowship. Matthias Fey is supported by the German Research Association (DFG) within the Collaborative Research Center SFB 876 “Providing Information by Resource-Constrained Analysis”, project A6. Hongyu Ren is supported by Masason Foundation Fellowship and Apple PhD Fellowship. Jure Leskovec is a Chan Zuckerberg Biohub investigator.
Our baseline code and Python package are built on top of excellent open-source software, including NUMPY (Harris et al., 2020), PYTORCH (Paszke et al., 2017), PYTORCH GEOMETRIC (Fey and Lenssen, 2019), DGL (Wang et al., 2019), and DGL-KE (Zheng et al., 2020).
The HOKUSAI facility was used to perform some of the quantum calculations. This work was supported by the Japan Society for the Promotion of Science (JSPS KAKENHI Grant no. 18H03206). We are also grateful to Maeda Toshiyuki for helpful discussions.
References
Ravichandra Addanki, Peter W Battaglia, David Budden, Andreea Deac, Jonathan Godwin, Thomas Keck, Wai Lok Sibon Li, Alvaro Sanchez-Gonzalez, Jacklynn Stott, Shantanu Thakoor, et al. Large-scale graph representation learning with very deep gnns and self-supervision. arXiv preprint arXiv:2107.09422, 2021.
10

Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor. Freebase: a collaboratively created graph database for structuring human knowledge. In Special Interest Group on Management of Data (SIGMOD), pages 1247–1250. AcM, 2008.
Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana Yakhnenko. Translating embeddings for modeling multi-relational data. In Advances in Neural Information Processing Systems (NeurIPS), pages 2787–2795, 2013.
Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020.
K. Burke. Perspective on density functional theory. J. Chem. Phys., 136:150901, 2012. URL http://link.aip.org/link/doi/10.1063/1.4704546.
Jianyu Cai, Jiajun Chen, Taoxing Pan, Zhanqiu Zhang, and Jie Wang. Technical report of team graphmiracles in the wikikg90m-lsc track of ogb-lsc@ kdd cup 2021. 2021.
Weiran Cao and Jiangeng Xue. Recent progress in organic photovoltaics: Device architecture and optical design. Energy Environ. Sci., 7:2123–2144, 2014. doi: 10.1039/C4EE00260A. URL http://dx.doi.org/10.1039/C4EE00260A.
Lowik Chanussot, Abhishek Das, Siddharth Goyal, Thibaut Lavril, Muhammed Shuaibi, Morgane Riviere, Kevin Tran, Javier Heras-Domingo, Caleb Ho, Weihua Hu, Aini Palizhati, Anuroop Sriram, Brandon Wood, Junwoong Yoon, Devi Parikh, C. Lawrence Zitnick, and Zachary Ulissi. Open catalyst 2020 (oc20) dataset and community challenges. ACS Catal., 11:6059–6072, 2021. URL https://doi.org/10.1021/acscatal.0c04525.
Jie Chen, Tengfei Ma, and Cao Xiao. Fastgcn: fast learning with graph convolutional networks via importance sampling. arXiv preprint arXiv:1801.10247, 2018.
Wei-Lin Chiang, Xuanqing Liu, Si Si, Yang Li, Samy Bengio, and Cho-Jui Hsieh. Cluster-GCN: An efﬁcient algorithm for training deep and large graph convolutional networks. In ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD), pages 257–266, 2019.
Michał Daniluk, Jacek Dabrowski, Barbara Rychalska, and Konrad Gołuchowski. Synerise at kdd cup 2021: Node classiﬁcation in massive heterogeneous graphs. 2021. URL https: //ogb.stanford.edu/paper/kddcup2021/mag240m_SyneriseAI.pdf.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.
Yuxiao Dong, Hao Ma, Zhihong Shen, and Kuansan Wang. A century of science: Globalization of scientiﬁc collaborations, citations, and innovations. In ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD), pages 1437–1446. ACM, 2017.
Leonardo G. Ferreira, Ricardo N. Dos Santos, Glaucius Oliva, and Adriano D. Andricopulo. Molecular docking and structure-based drug design strategies. Molecules, 20(7):13384–13421, 2015. ISSN 1420-3049. doi: 10.3390/molecules200713384. URL https://www.mdpi.com/ 1420-3049/20/7/13384.
Matthias Fey and Jan Eric Lenssen. Fast graph representation learning with pytorch geometric. arXiv preprint arXiv:1903.02428, 2019.
Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural message passing for quantum chemistry. In International Conference on Machine Learning (ICML), pages 1273–1272, 2017.
JS Grifﬁth and LE Orgel. Ligand-ﬁeld theory. Quarterly Reviews, Chemical Society, 11(4):381–393, 1957.
Qingyu Guo, Fuzhen Zhuang, Chuan Qin, Hengshu Zhu, Xing Xie, Hui Xiong, and Qing He. A survey on knowledge graph-based recommender systems. IEEE Transactions on Knowledge and Data Engineering, 2020.
11

William L Hamilton, Rex Ying, and Jure Leskovec. Inductive representation learning on large graphs. In Advances in Neural Information Processing Systems (NeurIPS), pages 1025–1035, 2017.
Charles R Harris, K Jarrod Millman, Ste´fan J van der Walt, Ralf Gommers, Pauli Virtanen, David Cournapeau, Eric Wieser, Julian Taylor, Sebastian Berg, Nathaniel J Smith, et al. Array programming with numpy. Nature, 585(7825):357–362, 2020.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 770–778, 2016.
Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. In Advances in Neural Information Processing Systems (NeurIPS), 2020a.
Weihua Hu, Bowen Liu, Joseph Gomes, Marinka Zitnik, Percy Liang, Vijay Pande, and Jure Leskovec. Strategies for pre-training graph neural networks. In International Conference on Learning Representations (ICLR), 2020b.
Weihua Hu, Muhammed Shuaibi, Abhishek Das, Siddharth Goyal, Anuroop Sriram, Jure Leskovec, Devi Parikh, and C Lawrence Zitnick. Forcenet: A graph neural network for large-scale quantum calculations. arXiv preprint arXiv:2103.01436, 2021.
Qian Huang, Horace He, Abhay Singh, Ser-Nam Lim, and Austin R Benson. Combining label propagation and simple models out-performs graph neural networks. arXiv preprint arXiv:2010.13993, 2020.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International Conference on Machine Learning (ICML), pages 448–456, 2015.
Thomas N. Kipf and Max Welling. Semi-supervised classiﬁcation with graph convolutional networks. In International Conference on Learning Representations (ICLR), 2017.
Jon M Kleinberg. Authoritative sources in a hyperlinked environment. Journal of the ACM, 46(5): 604–632, 1999.
Johannes Klicpera, Shankari Giri, Johannes T. Margraf, and Stephan Gu¨nnemann. Fast and uncertainty-aware directional message passing for non-equilibrium molecules. In NeurIPS-W, 2020.
Greg Landrum et al. Rdkit: Open-source cheminformatics, 2006.
Jure Leskovec and Rok Sosicˇ. Snap: A general-purpose network analysis and graph-mining library. ACM Transactions on Intelligent Systems and Technology (TIST), 8(1):1–20, 2016.
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dolla´r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In eccv, pages 740–755. Springer, 2014.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019.
Miller McPherson, Lynn Smith-Lovin, and James M Cook. Birds of a feather: Homophily in social networks. Annual review of sociology, 27(1):415–444, 2001.
Bonan Min, Ralph Grishman, Li Wan, Chang Wang, and David Gondek. Distant supervision for relation extraction with an incomplete knowledge base. In North American Chapter of the Association for Computational Linguistics (NAACL), pages 777–782, 2013.
Harry L Morgan. The generation of a unique machine description for chemical structures-a technique developed at chemical abstracts service. Journal of Chemical Documentation, 5(2):107–113, 1965.
12

Christopher Morris, Nils M Kriege, Franka Bause, Kristian Kersting, Petra Mutzel, and Marion Neumann. Tudataset: A collection of benchmark datasets for learning with graphs. arXiv preprint arXiv:2007.08663, 2020.
Maho Nakata. the PubChemQC Project: A Large Chemical Database from the First Principle Calculations. AIP Conf. Proc., 1702:090058, 2015. doi: 10.1186/1758-2946-3-4. URL http: //dx.doi.org/10.1063/1.4938866.
Maho Nakata and Tomomi Shimazaki. Pubchemqc project: A large-scale ﬁrst-principles electronic structure database for data-driven chemistry. Journal of chemical information and modeling, 57(6): 1300–1308, 2017.
Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in pytorch. In NIPS-W, 2017.
Weihua Peng, Donghai Bian, Yanhui Huang, Guangzhi Sheng, and Jian Sun. Technical report of wikikg90m-lsc. 2021. URL https://ogb.stanford.edu/paper/kddcup2021/ wikikg90m_OhMyGod.pdf.
Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-networks. 11 2019. URL https://arxiv.org/abs/1908.10084.
Emanuele Rossi, Fabrizio Frasca, Ben Chamberlain, Davide Eynard, Michael Bronstein, and Federico Monti. Sign: Scalable inception graph neural networks. arXiv preprint arXiv:2004.11198, 2020.
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. International journal of computer vision, 115(3):211–252, 2015.
Alvaro Sanchez-Gonzalez, Jonathan Godwin, Tobias Pfaff, Rex Ying, Jure Leskovec, and Peter W Battaglia. Learning to simulate complex physics with graph networks. In International Conference on Machine Learning (ICML), 2020.
Michael Schlichtkrull, Thomas N Kipf, Peter Bloem, Rianne Van Den Berg, Ivan Titov, and Max Welling. Modeling relational data with graph convolutional networks. In European Semantic Web Conference, pages 593–607. Springer, 2018.
Kristof Schu¨tt, Pieter-Jan Kindermans, Huziel Enoc Sauceda Felix, Stefan Chmiela, Alexandre Tkatchenko, and Klaus-Robert Mu¨ller. Schnet: A continuous-ﬁlter convolutional neural network for modeling quantum interactions. In Advances in Neural Information Processing Systems (NeurIPS), pages 991–1001, 2017.
Yunsheng Shi, Zhengjie Huang, Wenjin Wang, Hui Zhong, Shikun Feng, and Yu Sun. Masked label prediction: Uniﬁed message passing model for semi-supervised classiﬁcation. arXiv preprint arXiv:2009.03509, 2020.
Yunsheng Shi, PGL Team, Zhengjie Huang, Weibin Li, Weiyue Su, and Shikun Feng. Runimp: Solution for kddcup 2021 mag240m-lsc. 2021. URL https://ogb.stanford.edu/paper/ kddcup2021/mag240m_BD-PGL.pdf.
Amit Singhal. Introducing the knowledge graph: things, not strings. Ofﬁcial google blog, 5:16, 2012.
Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu. Mpnet: Masked and permuted pre-training for language understanding. arXiv preprint arXiv:2004.09297, 2020.
Weiyue Su, Zeyang Fang, Hui Zhong, Huijuan Wang, Siming Dai, Zhengjie Huang, Yunsheng Shi, Shikun Feng, and Zeyu Chen. Note: Solution for kdd-cup 2021 wikikg90m-lsc. arXiv preprint arXiv:2107.01892, 2021.
Qiming Sun, Xing Zhang, Samragni Banerjee, Peng Bao, Marc Barbry, Nick S Blunt, Nikolay A Bogdanov, George H Booth, Jia Chen, Zhi-Hao Cui, et al. Recent developments in the pyscf program package. The Journal of chemical physics, 153(2):024109, 2020.
13

Yizhou Sun, Jiawei Han, Xifeng Yan, Philip S Yu, and Tianyi Wu. Pathsim: Meta path-based top-k similarity search in heterogeneous information networks. Proceedings of the VLDB Endowment, 4 (11):992–1003, 2011.
Zhiqing Sun, Zhi-Hong Deng, Jian-Yun Nie, and Jian Tang. Rotate: Knowledge graph embedding by relational rotation in complex space. In International Conference on Learning Representations (ICLR), 2019.
Shantanu Thakoor, Corentin Tallec, Mohammad Gheshlaghi Azar, Re´mi Munos, Petar Velicˇkovic´, and Michal Valko. Bootstrapped representation learning on graphs. arXiv preprint arXiv:2102.06514, 2021.
The´o Trouillon, Johannes Welbl, Sebastian Riedel, E´ ric Gaussier, and Guillaume Bouchard. Complex embeddings for simple link prediction. In International Conference on Machine Learning (ICML), pages 2071–2080, 2016.
Johan Ugander, Brian Karrer, Lars Backstrom, and Cameron Marlow. The anatomy of the facebook social graph. arXiv preprint arXiv:1111.4503, 2011.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. arXiv preprint arXiv:1706.03762, 2017.
Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph attention networks. In International Conference on Learning Representations (ICLR), 2018.
Denny Vrandecˇic´ and Markus Kro¨tzsch. Wikidata: a free collaborative knowledgebase. Communications of the ACM, 57(10):78–85, 2014.
Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. Glue: A multi-task benchmark and analysis platform for natural language understanding. arXiv preprint arXiv:1804.07461, 2018.
Kuansan Wang, Zhihong Shen, Chiyuan Huang, Chieh-Han Wu, Yuxiao Dong, and Anshul Kanakia. Microsoft academic graph: When experts are not enough. Quantitative Science Studies, 1(1): 396–413, 2020.
Minjie Wang, Lingfan Yu, Da Zheng, Quan Gan, Yu Gai, Zihao Ye, Mufei Li, Jinjing Zhou, Qi Huang, Chao Ma, Ziyue Huang, Qipeng Guo, Hao Zhang, Haibin Lin, Junbo Zhao, Jinyang Li, Alexander J Smola, and Zheng Zhang. Deep graph library: Towards efﬁcient and scalable deep learning on graphs. ICLR Workshop on Representation Learning on Graphs and Manifolds, 2019. URL https://arxiv.org/abs/1909.01315.
David Weininger. Smiles, a chemical language and information system. 1. introduction to methodology and encoding rules. Journal of chemical information and computer sciences, 28(1):31–36, 1988.
Felix Wu, Tianyi Zhang, Amauri Holanda de Souza Jr, Christopher Fifty, Tao Yu, and Kilian Q Weinberger. Simplifying graph convolutional networks. In International Conference on Machine Learning (ICML), 2019.
Zhenqin Wu, Bharath Ramsundar, Evan N Feinberg, Joseph Gomes, Caleb Geniesse, Aneesh S Pappu, Karl Leswing, and Vijay Pande. Moleculenet: a benchmark for molecular machine learning. Chemical science, 9(2):513–530, 2018.
Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? In International Conference on Learning Representations (ICLR), 2019.
Shuo Yang, Daixin Wang, Dingyuan Zhu, Yakun Wang, and Borui Ye. Team littleant’s solution of task2. 2021. URL https://ogb.stanford.edu/paper/kddcup2021/wikikg90m_ littleant.pdf.
Zhilin Yang, William W Cohen, and Ruslan Salakhutdinov. Revisiting semi-supervised learning with graph embeddings. In International Conference on Machine Learning (ICML), pages 40–48, 2016.
14

Chengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He, Yanming Shen, and Tie-Yan Liu. Do transformers really perform bad for graph representation? arXiv preprint arXiv:2106.05234, 2021a.
Chengxuan Ying, Mingqi Yang, Shuxin Zheng, Guolin Ke, Shengjie Luo, Tianle Cai, Chenglin Wu, Yuxin Wang, Yanming Shen, and Di He. Awardee solution of kdd cup 2021 ogb large-scale challenge graph-level track. arXiv preprint arXiv:2106.08279, 2021b.
Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L Hamilton, and Jure Leskovec. Graph convolutional neural networks for web-scale recommender systems. In ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD), pages 974–983, 2018.
Hanqing Zeng, Hongkuan Zhou, Ajitesh Srivastava, Rajgopal Kannan, and Viktor Prasanna. GraphSaint: Graph sampling based inductive learning method. In International Conference on Learning Representations (ICLR), 2020.
Shanzhuo Zhang, Lihang Liu, Sheng Gao, Donglong He, Xiaomin Fang, Weibin Li, Zhengjie Huang, Weiyue Su, and Wenjin Wang. Litegem: Lite geometry enhanced molecular representation learning for quantum property prediction. arXiv preprint arXiv:2106.14494, 2021.
Da Zheng, Xiang Song, Chao Ma, Zeyuan Tan, Zihao Ye, Jin Dong, Hao Xiong, Zheng Zhang, and George Karypis. Dgl-ke: Training knowledge graph embeddings at scale. arXiv preprint arXiv:2004.08532, 2020.
Checklist
1. For all authors...
(a) Do the main claims made in the abstract and introduction accurately reﬂect the paper’s contributions and scope? [Yes]
(b) Did you describe the limitations of your work? [Yes] See Appendix A (c) Did you discuss any potential negative societal impacts of your work? [Yes] See
Appendix A (d) Have you read the ethics review guidelines and ensured that your paper conforms to
them? [Yes]
2. If you are including theoretical results...
(a) Did you state the full set of assumptions of all theoretical results? [N/A] (b) Did you include complete proofs of all theoretical results? [N/A]
3. If you ran experiments (e.g. for benchmarks)...
(a) Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes] See Appendix A.
(b) Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes] See Section 2.
(c) Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [No] Our datasets are very large and offer hidden test sets; hence, we follow the convention of similar large-scale datasets such as ImageNet (Russakovsky et al., 2015), MS-COCO (Lin et al., 2014), GLUE Benchmark (Wang et al., 2018), where we report performance of a single run. Note that model performance is often very stable on the large datasets.
(d) Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] See Appendix A
4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...
(a) If your work uses existing assets, did you cite the creators? [Yes] See the references in Section 2.
(b) Did you mention the license of the assets? [Yes] See Appendix A
15

(c) Did you include any new assets either in the supplemental material or as a URL? [Yes] All of our relevant URLs are described in Appendix A.
(d) Did you discuss whether and how consent was obtained from people whose data you’re using/curating? [Yes] We are using public datasets and closely follow the license rules.
(e) Did you discuss whether the data you are using/curating contains personally identiﬁable information or offensive content? [Yes] Our datasets do not contain any private nor offensive information.
5. If you used crowdsourcing or conducted research with human subjects... (a) Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A] (b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A] (c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A]
16

A Key Information about OGB-LSC
Dataset documentation. All of our datasets as well as how to use them through our Python package are documented at https://ogb.stanford.edu/kddcup2021/. Our baseline code to reproduce all the results for each dataset is available at https://github.com/snap-stanford/ ogb/tree/master/examples/lsc.
Intended use. OGB-LSC is intended for machine learning and data scientists to develop ML models to tackle the challenge of large-scale graph ML.
Relevant URLs. OGB-LSC maintains the following:
• Ofﬁcial website (https://ogb.stanford.edu/kddcup2021/) is the main reference of OGB-LSC. It provides an overview of the OGB-LSC, descriptions of the datasets as well as detailed documentations of how to use the datasets through the OGB Python package. The subpage (https://ogb.stanford.edu/kddcup2021/results/) also contains the leaderboards during the KDD Cup 2021 as well as the technical reports and code provided by the winners.
• Github repository (https://github.com/snap-stanford/ogb) hosts the source code for the OGB Python package. OGB-LSC datasets and evaluation are all managed by the Python package. We also release all the baseline code that we used in our experiments.
• Datasets are extremely large (around 300GB in total) and are hosted under AWS with the help of the DGL Team. Our users do not need to directly interact with the URL, as the dataset download and processing are all managed by our Python package.
• Mailing list (https://groups.google.com/g/open-graph-benchmark) is used for making any announcements about OGB/OGB-LSC.
Hosting and maintenance plan. OGB-LSC’s Python package is hosted and version-tracked via Github. All the datasets are hosted under the AWS with the help of the DGL Team. We design the Python package to handle downloading and processing of the datasets. OGB is a community-driven initiative that has been actively maintained by our team members.
Licensing. The OGB Python package uses the MIT license. Each dataset has its own license. Speciﬁcally, MAG240M uses ODC-BY, WikiKG90M uses CC-0, and PCQM4M uses CC BY 4.0.
Author statement. We bear all responsibility in case of violation of rights, etc., and conﬁrmation of the data license.
Computing resources. We ran all the experiments on a server with 10 GeForce RTX 2080 GPUs and an Intel(R) Xeon(R) Gold 6148 CPU @ 2.40GHz.
Limitations. Large-scale graph ML has a wide variety of application domains and there are representative graphs that we cannot cover in the current OGB-LSC datasets. Examples include large-scale recommender systems, social networks, and ﬁnancial networks. These graphs are hard to obtain due to privacy and cooperative concerns, but we hope to include these realistic large graphs in the future if we have a chance. That being said, it is our hope that many methodological insights on our large graphs (training strategy, GNN architecture, regularization, etc) still transfer well to a variety of large-scale graphs. We leave the thorough investigation to future work.
Potential negative social impacts. All of our datasets are derived from practically-relevant tasks in the real world; hence, developing models and deploying them to the real-world could potentially produce predictions that are inﬂuenced by the bias in the datasets. For example, regarding the MAG240M dataset, we may use the resulting paper and author embeddings to perform a variety of downstream ML tasks such as searching for similar papers or recommending author collaboration and paper citations. Thus, it is critical to ensure there is no undesirable bias in the embeddings. There could be also misuse of highly accurate ML models. For instance, regarding the PCQM4M dataset, we
17

Table 8: Basic graph statistics of the OGB-LSC datasets. The last three graph statistics are calculated over the ‘standardized’ graphs, where the graphs are ﬁrst converted into undirected and unlabeled homogeneous graphs with duplicated edges removed. The SNAP library (Leskovec and Sosicˇ, 2016) is then used to compute the graph statistics. MAG240M (homo) represents the homogenized MAG240M graph with only paper nodes and citation links. Some graph statistics were omitted due to their high computational cost (the calculation did not complete in two weeks).

Model
MAG240M MAG240M (homo) WikiKG90M WikiKG90Mv2 PCQM4M PCQM4Mv2

#Graphs
1 1 1 1 3,803,453 3,746,619

Avg #nodes
244,160,499 121,751,666 87,143,637 91,230,610
14.15 14.14

Avg #edges
1,728,364,232 1,297,748,926
504,220,369 601,062,811
14.57 14.56

Avg deg
14.15 21.30 10.93 12.59 2.05 2.05

Avg clust. coeff.
0.033 0.031
— — 0.010 0.011

Avg diameter
— — — — 7.96 7.95

need to make sure that the trained molecular property predictor is used in the right way to develop useful drugs/materials rather than harmful ones.
B Basic Graph Statistics of the Datasets
The basic graph statistics of the OGB-LSC datasets are provided in Table 8.
C Details about Dataset Updates after the KDD Cup 2021
MAG240M updates. The MAG240M dataset itself has not been changed. The only update is on the test set. In Table 8, we report the test-dev accuracy of all the models.
WikiKG90Mv2 updates. The WikiKG90M dataset has been updated to WikiKG90Mv2. Below we summarize the updates we have applied to the dataset.
• No candidate tails provided. The most important update is that we do not provide any candidate tail entities for validation/test triples. Hence, a model needs to predict the target tail entity out of all the entities in Wikidata.
• Created from more recent Wikidata. The WikiKG90Mv2 is based on the public Wikidata dump downloaded at three time-stamps: May 17th, June 7th, and June 28th of 2021, for training, validation, and testing, respectively. We retain all the entities and relations in the September dump, resulting in 91,230,610 entities, 1,387 relations, and 601,062,811 triplets in total.
• A better text encoder used. The text features of WikiKG90Mv2 are obtained by using MPNet (Reimers and Gurevych, 2019; Song et al., 2020), which is shown to be signiﬁcantly better sentence encoder (Reimers and Gurevych, 2019).
• Balancing relation types in validation/test triples. On the new Wikidata dumps, we found the relation types of the raw validation/test triples are highly-skewed; the most frequent relation, “cites work (P2860)”, occupies 60% and 85% of the entire validation and test triples, respectively. To test a model’s capability to perform well across all types of relations, we subsample 15,000 triples from the entire validation/test triples such that the resulting relation counts are proportional to the cubic-root of the original relation counts.
In Table 10, we show head entities that have very sparse connection in the training KG. We see that textual features could provide important signals for predicting these triples.
We perform an extensive baseline analysis on WikiKG90Mv2. We used the same set of hyper-parameters and baseline models as our original WikiKG90M. Different from WikiKG90M, WikiKG90Mv2 does not provide any candidate tail entities. A na¨ıvely approach is to use the entire entities as the tail candidates. However, this approach does not scale well to a KG with nearly 90M entities because we need to predict scores for all the 90M entities for every triple. Nonetheless, in practice, most of the entities are obvious negatives: e.g., for the relation type “is located in”, any
18

entities that are not locations can be easily ﬁltered out as negatives. Based on the the above intuition, we consider the relation-speciﬁc tail candidate sets. Speciﬁcally, on training triples, we pre-compute 20K most frequent tail entities for each relation and treat them as candidate tail entities for that relation. At inference time, we use our KG model to score among those relation-speciﬁc candidates. The results are provided in Table 10. Overall, we observe that the relative trends are similar to the original WikiKG90Mv2. Especially the CONCAT encoder provides the best MRR performance. Different from WikiKG90M, the MRR score on the new WikiKG90Mv2 is far perfect score of 1 and leaves a lot of room for improvement. Overall, we believe it is promising to explore how to quickly generate a small number of high-quality candidate tail entities out of all the entities so that KG models only need to score a much fewer number of candidate entities.
PCQM4Mv2 updates. The PCQM4M dataset has been updated to PCQM4Mv2. Below we summarize the updates we have applied to the dataset.
• 3D molecular structures provided. We additionally provide 3D structures for training molecules. These structures are calculated by DFT and are obtained together with the HOMO-LUMO gap.
• SMILES strings are partly updated. In the process of preparing the 3D structures, we found a subtle mismatch between SMILES strings (i.e., 2D molecular graphs) and the HOMO-LUMO gap for about 10% of the entire molecules. Speciﬁcally, the SMILES strings can be changed in the course of DFT’s geometry optimization, but in PCQM4M, we provided the initial SMILES strings. In the updated PCQM4Mv2, we provide SMILES strings corresponding to the ﬁnal optimized 3D structures. Note that the HOMO-LUMO gap was calculated by DFT based on the ﬁnal 3D structures (Nakata and Shimazaki, 2017); hence, it makes more sense to correspond the HOMO-LUMO gap with the SMILES string associated with the ﬁnal 3D structures.
• Number of molecules decreased slightly. As a result of the SMILES update, some molecules can no longer be parsed by the commonly-used chemistry toolkit, i.e., rdkit (Landrum et al., 2006). As a result, the total number of molecules has been slightly reduced to 3,746,619.
• Split ratio changed. For PCQM4Mv2, we set the split ratio for train/validation/test-dev/testchallenge to 90/2/4/4. The split is still done by PubChem compound ID so that there is no test label leakage, i.e., all the test molecules in PCQM4Mv2 is in the test split of PCQM4M.
Similar to PCQM4M, we also provide our baseline analysis on the updated PCQM4Mv2 dataset. At inference time, we clamped the output values to be between 0 and 20, which prevents our models from predicting erroneous values for some test molecules. We show the results in Tables 12 and 13. We found that all the models were able to achieve lower MAE compared to PCQM4M, probably because we have ﬁxed the mismatch bug described above. Beyond the overall better MAE, we see that the trend in model performance is mostly preserved; larger and more expressive GNN models achieve better results. For the GNNs, we observe that the depth helps more than width. Interestingly, too-wide models often make unstable prediction on validation molecules.
19

Table 9: Results of MAG240M measured by the accuracy (%). R-GRAPHSAGE/-GAT utilize the full heterogeneous graph information, while the other models operate on the homogeneous paper citation graph. Test accuracy is evaluated on the test-dev set.

Model
MLP LABELPROP SGC SIGN MLP+C&S GRAPHSAGE (NS) GAT (NS)
R-GRAPHSAGE (NS) R-GAT (NS)
KDD 1ST: BD-PGL KDD 2ND: ACADEMIC KDD 3RD: SYNERISE AI

#Params Validation

0.5M 0
0.7M 3.8M 0.5M 4.9M 4.9M

52.67 58.44 65.82 66.64 66.98 66.79 67.15

12.2M 12.3M

69.86 70.02

Ensemble Ensemble Ensemble

Test-dev
52.76 56.38 65.30 66.03 66.05 66.21 66.71
68.78 69.31
75.39 75.07 74.57

Table 10: Textual representation of validation triplets whose head entities only appear once as head in the training WikiKG90Mv2.

Head
Herbert Hoover’s Inaugural Address Jussi Award for Best Sound Recording organ dose British Endurance Racing Team Knee bursae Churches in Dekanat Leuchtenberg web content management system Stephan von Divonne Minecraft mod beer pouring

Relation
country instance of calculated from country anatomical location is a list of model item given name depends on software uses

Tail
United States of America class of award absorbed dose United Kingdom knee church building workﬂow management system Stephan Minecraft beer engine

Table 11: Results of WikiKG90Mv2 measured by the Mean Reciprocal Rank (MRR).

Model

#Params Validation Test-dev

TRANSE-SHALLOW COMPLEX-SHALLOW TRANSE-MPNET COMPLEX-MPNET TRANSE-CONCAT COMPLEX-CONCAT

18.2B 18.2B 0.3M 0.3M 18.2B 18.2B

0.1103 0.1150 0.1128 0.1258 0.2060 0.2048

0.0824 0.0985 0.0860 0.0988 0.1761 0.1761

Table 12: Results of PCQM4Mv2 measured by MAE [eV]. The lower, the better. Ablation study of using only 10% of training data is also shown. Chemical accuracy indicates the ﬁnal goal for practical usefulness.

Model
MLP-FINGERPRINT GCN GCN-VIRTUAL GIN GIN-VIRTUAL
MLP-FINGERPRINT (10% train) GIN-VIRTUAL (10% train)
Chemical accuracy (goal)

#Params
16.1M 2.0M 4.9M 3.8M 6.7M
16.1M 6.7M
–

Validation Test-dev

0.1753 0.1379 0.1153 0.1195 0.1083

0.1760 0.1398 0.1152 0.1218 0.1084

0.2429 0.1442

0.2445 0.1446

0.0430

Table 13: Model size and the MAE performance [eV]. For both models, the width indicates the hidden dimensionality. For GINVIRTUAL, the depth represents the number of GNN layers, while for the MLP-FINGERPRINT, the depth represents the the number of hidden layers in MLP.

Model MLP-FINGERPRINT
GIN-VIRTUAL

Width
1600 1600 1600 1200
600 600 300 300

Depth
6 4 2 6
5 3 5 3

#Params
16.1M 11.0M 5.8M 9.7M
6.7M 3.7M 1.7M 1.0M

Validation
0.1753 0.1752 0.1954 0.1804
0.1083 0.1239 0.1100 0.1181

20

