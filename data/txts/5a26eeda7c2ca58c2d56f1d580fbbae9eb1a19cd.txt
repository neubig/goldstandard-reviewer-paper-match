arXiv:2103.02138v2 [cs.LG] 6 Jul 2021

Parametric Complexity Bounds for Approximating PDEs
with Neural Networks
Tanya Marwah, Zachary C. Lipton, Andrej Risteski Machine Learning Department, Carnegie Mellon University
{tmarwah, zlipton, aristesk}@andrew.cmu.edu
Abstract
Recent experiments have shown that deep networks can approximate solutions to high-dimensional PDEs, seemingly escaping the curse of dimensionality. However, questions regarding the theoretical basis for such approximations, including the required network size, remain open. In this paper, we investigate the representational power of neural networks for approximating solutions to linear elliptic PDEs with Dirichlet boundary conditions. We prove that when a PDE’s coeﬃcients are representable by small neural networks, the parameters required to approximate its solution scale polynomially with the input dimension d and proportionally to the parameter counts of the coeﬃcient networks. To this we end, we develop a proof technique that simulates gradient descent (in an appropriate Hilbert space) by growing a neural network architecture whose iterates each participate as sub-networks in their (slightly larger) successors, and converge to the solution of the PDE. We bound the size of the solution, showing a polynomial dependence on d and no dependence on the volume of the domain.
1 Introduction
A partial diﬀerential equation (PDE) relates a multivariate function deﬁned over some domain to its partial derivatives. Typically, one’s goal is to solve for the (unknown) function, often subject to additional constraints, such as the function’s value on the boundary of the domain. PDEs are ubiquitous in both the natural and social sciences, where they model such diverse processes as heat diﬀusion [Crank and Nicolson, 1947; O¨ zi¸sik et al., 2017], ﬂuid dynamics [Anderson and Wendt, 1995; Temam, 2001], and ﬁnancial markets [Black and Scholes, 1973; Ehrhardt and Mickens, 2008]. Because most PDEs of interest lack closed-form solutions, computational approximation methods remain a vital and an active ﬁeld of research [Ames, 2014]. For low-dimensional functions, dominant approaches include the ﬁnite diﬀerences and ﬁnite element methods [LeVeque, 2007], which discretize the domain. After partitioning the domain into a mesh, these methods solve for the function value at its vertices. However, these techniques scale exponentially with the input dimension, rendering them unsuitable for high-dimensional problems.
Following breakthroughs in deep learning for approximating high-dimensional functions in such diverse domains as computer vision [Krizhevsky et al., 2012; Radford et al., 2015] and natural language processing [Bahdanau et al., 2014; Devlin et al., 2018; Vaswani et al., 2017], a burgeoning line of research leverages neural networks to approximate solutions to PDEs. This line of work has produced promising empirical results for common PDEs such as the Hamilton-Jacobi-Bellman and Black-Scholes equations [Han et al., 2018; Grohs et al., 2018; Sirignano and Spiliopoulos, 2018]. Because they do not explicitly discretize the domain, and given their empirical success on high-dimensional problems, these methods appear not to suﬀer the curse of dimensionality. However, these methods are not well understood theoretically, leaving open questions about when they are applicable, what their performance depends on, and just how many parameters are required to approximate the solution to a given PDE.
Over the past three years, several theoretical works have investigated questions of representational power under various assumptions. Exploring a variety of settings, Kutyniok et al. [2019], Grohs et al. [2018], and
1

Jentzen et al. [2018], proved that the number of parameters required to approximate a solution to a PDE exhibits a less than exponential dependence on the input dimension for some special parabolic PDEs that admit straightforward analysis. Grohs and Herrmann [2020] consider elliptic PDEs with Dirichlet boundary conditions. However, their rate depends on the volume of the domain, and thus can have an implicit exponential dependence on dimension (e.g., consider a hypercube with side length greater than one).
In this paper, we focus on linear elliptic PDEs with Dirichlet boundary conditions, which are prevalent in science and engineering (e.g., the Laplace and Poisson equations). Notably, linear elliptic PDEs deﬁne the steady state of processes like heat diﬀusion and ﬂuid dynamics. Our work asks:
Question. How many parameters suﬃce to approximate the solution to a linear elliptic PDE up to a speciﬁed level of precision using a neural network?
We show that when the coeﬃcients of the PDE are expressible as small neural networks (note that PDE coeﬃcients are functions), the number of parameters required to approximate the PDE’s solution is proportional to the number of parameters required to express the coeﬃcients. Furthermore, we show that the number of parameters depends polynomially on the dimension and does not depend upon the volume of the domain.
2 Overview of Results
To begin, we formally deﬁne linear elliptic PDEs.
Deﬁnition 1 (Linear Elliptic PDE [Evans, 1998]). Linear elliptic PDEs with Dirichlet boundary condition can be expressed in the following form:
(Lu) (x) ≡ (−div (A∇u) + cu) (x) = f (x), ∀x ∈ Ω, u(x) = 0, ∀x ∈ ∂Ω,
where Ω ⊂ Rd is a bounded open set with a boundary ∂Ω. Further, for all x ∈ Ω, A : Ω → Rd×d is a matrix-valued function, s.t. A(x) ≻ 0, and c : Ω → R, s.t. c(x) > 0. 1
We refer to A and c as the coeﬃcients of the PDE. The divergence form in Deﬁnition 1 is one of two canonical ways to deﬁne a linear elliptic PDE [Evans, 1998] and is convenient for several technical reasons (see Section 4). The Dirichlet boundary condition states that the solution takes a constant value (here 0) on the boundary ∂Ω.
Our goal is to express the number of parameters required to approximate the solution of a PDE in terms of those required to approximate its coeﬃcients A and c. Our key result shows:
Theorem (Informal). If the coeﬃcients A, c and the function f are approximable by neural networks with at most N parameters, the solution u⋆ to the PDE in Deﬁnition 1 is approximable by a neural network with O (poly(d)N ) parameters.
This result, formally expressed in Section 5, may help to explain the practical eﬃcacy of neural networks in approximating solutions to high-dimensional PDEs with boundary conditions [Sirignano and Spiliopoulos, 2018; Li et al., 2020a]. To establish this result, we develop a constructive proof technique that simulates gradient descent (in an appropriate Hilbert space) through the very architecture of a neural network. Each iterate, given by a neural network, is subsumed into the (slightly larger) network representing the subsequent iterate. The key to our analysis is to bound both (i) the growth in network size across consecutive iterates; and (ii) the total number of iterates required.
Organization of the paper We introduce the required notation along with some mathematical preliminaries on PDEs in Section 4. The problem setting and formal statement of the main result are provided in Section 5. Finally, we provide the proof of the main result in Section 6.
1Here, div denotes the divergence operator. Given a vector ﬁeld F : Rd → Rd, div(F ) = ∇ · F = di=1 ∂∂Fxii
2

3 Prior Work
Among the ﬁrst papers to leverage neural networks to approximate solutions to PDEs with boundary conditions are Lagaris et al. [1998], Lagaris et al. [2000], and Malek and Beidokhti [2006]. However, these methods discretize the input space and thus are not suitable for high-dimensional input spaces. More recently, meshfree neural network approaches have been proposed for high-dimensional PDEs [Han et al., 2018; Raissi et al., 2017, 2019], achieving impressive empirical results in various applications. Sirignano and Spiliopoulos [2018] design a loss function that penalizes failure to satisfy the PDE, training their network on minibatches sampled uniformly from the input domain. They also provide a universal approximation result, showing that for suﬃciently regularized PDEs, there exists a multilayer network that approximates its solution. However, they do not comment on the complexity of the neural network or how it scales with the input dimension. Khoo et al. [2017] also prove universal approximation power, albeit with networks of size exponential in the input dimension. Recently, Grohs et al. [2018]; Jentzen et al. [2018] provided a better-than-exponential dependence on the input dimension for some special parabolic PDEs, for which the simulating a PDE solver by a neural network is straightforward.
Several recent works [Bhattacharya et al., 2020; Kutyniok et al., 2019; Li et al., 2020b,a] show (experimentally) that a single neural network can solve for an entire family of PDEs. They approximate the map from a PDE’s parameters to its solution, potentially avoiding the trouble of retraining for every set of coeﬃcients. Among these, only Kutyniok et al. [2019] provides theoretical grounding. However, they assume the existence of a ﬁnite low-dimensional space with basis functions that can approximate this parametric map—and it is unclear when this would obtain. Our work proves the existence of such maps, under the assumption that the family of PDEs has coeﬃcients described by neural networks with a ﬁxed architecture (Section 7).
In the work most closely related to ours, Grohs and Herrmann [2020] provides approximation rates polynomial in the input dimension d for the Poisson equation (a special kind of linear elliptic PDE) with Dirichlet boundary conditions. They introduce a walk-on-the-sphere algorithm, which simulates a stochastic diﬀerential equation that can be used to solve a Poisson equation with Dirichlet boundary conditions (see, e.g., Oksendal [2013]’s Theorem 9.13). The rates provided in Grohs and Herrmann [2020] depend on the volume of the domain, and thus depend, implicitly, exponentially on the input dimension d. Our result considers the boundary condition for the PDE and is independent of the volume of the domain. Further, we note that our results are deﬁned for a more general linear elliptic PDE, of which the Poisson equation is a special case.

4 Notation and Deﬁnitions

We now introduce several key concepts from PDEs and some notation. For any open set Ω ⊂ Rd, we denote its boundary by ∂Ω and denote its closure by Ω¯ := Ω ∪ ∂Ω. By C0(Ω), we denote the space of real-valued continuous functions deﬁned over the domain Ω. Furthermore, for k ∈ N, a function g belongs to Ck(Ω) if all partial derivatives ∂αg exist and are continuous for any multi-index α, such that |α| ≤ k. Finally, a function g ∈ C∞(Ω) if g ∈ Ck(Ω) for all k ∈ N. Next, we deﬁne several relevant function spaces:

Deﬁnition 2. For any k ∈ N ∪ {∞}, C0k(Ω) := {g : g ∈ Ck(Ω), supp(g) ⊂ Ω}.

Deﬁnition 3. For a domain Ω, the function space L2(Ω) consists of all functions g : Ω → R, s.t.
1
∞ where g L2(Ω) = Ω |g(x)|2dx 2 . This function space is equipped with the inner product

g L2(Ω) <

g, h L2(Ω) = g(x)h(x)dx.
Ω
Deﬁnition 4. For a domain Ω and a function g : Ω → R, the function space L∞(Ω) is deﬁned analogously, where g L∞(Ω) = inf{c ≥ 0 : |g(x)| ≤ c for almost all x ∈ Ω}.
Deﬁnition 5. For a domain Ω and m ∈ N, we deﬁne the Hilbert space Hm(Ω) as
Hm(Ω) := {g : Ω → R : ∂αg ∈ L2(Ω), ∀α s.t. |α| ≤ m}

3

Furthermore, Hm(Ω) is equipped with the inner product,
corresponding norm 

g, h Hm(Ω) = 1
2

g Hm(Ω) = 

∂αg 2L2(Ω) .

|α|≤m

|α|≤m Ω(∂αg)(∂αh)dx and the

Deﬁnition 6. The closure of C0∞(Ω) in Hm(Ω) is denoted by H0m(Ω).
Informally, H0m(Ω) is the set of functions belonging to Hm(Ω) that can be approximated by a sequence of functions φn ∈ C0∞(Ω). This also implies that if a function g ∈ H0m(Ω), then g(x) = 0 for all x ∈ ∂Ω. This space (particularly with m = 1) is often useful when analyzing elliptic PDEs with Dirichlet boundary conditions.

Deﬁnition 7 (Weak Solution). Given the PDE in Deﬁnition 1, if f ∈ L2(Ω), then a function u : Ω → R solves the PDE in a weak sense if u ∈ H01(Ω) and for all v ∈ H01(Ω), we have

(A∇u · ∇v + cuv) dx = f vdx

(1)

Ω

Ω

The left hand side of (1) is also equal to Lu, v L2(Ω) for all u, v ∈ H01(Ω) (see Lemma A.1), whereas, following the deﬁnition of the L2(Ω) norm, the right side is simply f, v L2(Ω). Having introduced these preliminaries, we now introduce some important facts about linear PDEs that feature prominently in our
analysis.

Proposition 1. For the PDE in Deﬁnition 1, if f ∈ L2(Ω) the following hold:

1. The solution to Equation (1) exists and is unique.

2. The weak solution is also the unique solution of the following minimization problem:

u⋆ = argmin J (v) := argmin 1 Lv, v L2(Ω) − f, v L2(Ω) .

(2)

v∈H 1 (Ω)

v∈H1(Ω) 2

0

0

This proposition is standard (we include a proof in the Appendix, Section A.1 for completeness) and states that there exists a unique solution to the PDE (referred to as u⋆), which is also the solution we get
from the variational formulation in (2). In this work, we introduce a sequence of functions that minimizes
the loss in the variational formulation.

Deﬁnition 8 (Eigenvalues and Eigenfunctions, Evans [1998]). Given an operator L, the tuples (λ, ϕ)∞ i=1, where λi ∈ R and ϕi ∈ H01(Ω) are (eigenvalue, eigenfunction) pairs that satisfy Lϕ = λϕ, for all x ∈ Ω. Since ϕ ∈ H01(Ω), we know that ϕ|∂Ω = 0. The eigenvalue can be written as

Lu, u L2(Ω)

λi = inf
u∈Xi

u2

,

(3)

L2(Ω)

where Xi := span{ϕ1, . . . , ϕi}⊥ = {u ∈ H01(Ω) : u, ϕj L2(Ω) = 0 ∀j ∈ {1, · · · , i}} and 0 < λ1 ≤ λ2 ≤ · · · . Furthermore, we deﬁne by Φk the span of the ﬁrst k eigenfunctions of L, i.e., Φk := span{ϕ1, · · · , ϕk}.
We note that since the operator L is self-adjoint and elliptic (in particular, L−1 is compact), the eigenvalues are real and countable. Moreover, the eigenfunctions form an orthonormal basis of H01(Ω) (see Evans [1998], Section 6.5).

5 Main Result
Before stating our results, we provide the formal assumptions on the PDEs of interest:

4

Assumptions:
(i) Smoothness: We assume that ∂Ω ∈ C∞. We also assume that the coeﬃcient A ∈ Ω → Rd×d is a symmetric matrix-valued function, i.e., A = (aij (x)) and aij (x) ∈ L∞(Ω) for all i, j ∈ [d] and the function c ∈ L∞(Ω) and c(x) ≥ ζ > 0 for all x ∈ Ω. Furthermore, we assume that aij, c ∈ C∞. We deﬁne a constant

C := (2d2 + 1) max max max ∂αaij L∞(Ω), max ∂αc L∞(Ω) .

α:|α|≤3 i,j

α:|α|≤2

Further, the function f ∈ L2(Ω) is also in C∞ and the projection of f onto Φk which we denote fspan satisﬁes for any multi-index α: ∂αf − ∂αfspan L2(Ω) ≤ ǫspan. 2

(ii) Ellipticity: There exist constants M ≥ m > 0 such that, for all x ∈ Ω and ξ ∈ Rd,

d

m ξ 2≤

aij (x)ξiξj ≤ M ξ 2.

i,j=1

(iii) Neural network approximability: There exist neural networks A˜ and c˜ with NA, Nc ∈ N parameters, respectively, that approximate the functions A and c, i.e., A − A˜ L∞(Ω) ≤ ǫA and c − c˜ L∞(Ω) ≤ ǫc, for small ǫA, ǫc ≥ 0. We assume that for all u ∈ H01(Ω) the operator L˜ deﬁned as,

L˜u = −div(A˜∇u) + c˜u.

(4)

is elliptic with (λ˜i, ϕ˜i)∞ i=1 (eigenvalue, eigenfunction) pairs. We also assume that there exists a neural network fnn ∈ C∞ with Nf ∈ N parameters such that for any multi-index α, ∂αf −∂αfnn L2(Ω) ≤ ǫnn. By Σ, we denote the set of all (inﬁnitely diﬀerentiable) activation functions used by networks A˜, c˜, and fnn. By Σ′, we denote the set that contains all the n-th order derivatives of the activation functions
in Σ, ∀n ∈ N0

Intuitively, ellipticity of L in a linear PDE Lu = f is analogous to positive deﬁniteness of a matrix Q ∈ Rd

in a linear equation Qx = k, where x, k ∈ Rd.

In (iii), we assume that the coeﬃcients A and c, and the function f can be approximated by neural

networks. While this is true for any smooth functions given suﬃciently large NA, Nc, Nf , our results are

most interesting when these quantities are small (e.g. subexponential in the input dimension d). For many

PDEs used in practice, approximating the coeﬃcients using small neural networks is straightforward. For

example, in heat diﬀusion (whose equilibrium is deﬁned by a linear elliptic PDE) A(x) deﬁnes the conductivity

of the material at point x. If the conductivity is constant, then the coeﬃcients can be written as neural

networks with O(1) parameters.

The part of assumption (i) that stipulates that f is close to fspan can be thought of as a smooth-

ness condition on f . For instance, if L = −∆ (the Laplacian operator), the Dirichlet form satisﬁes

Lu,u L2(Ω)
2

=

∇u L2(Ω) , so eigenfunctions corresponding to higher eigenvalues tend to exhibit a higher

u L2(Ω)

u L2(Ω)

degree of spikiness. The reader can also think of the eigenfunctions corresponding to larger k as Fourier

basis functions corresponding to higher frequencies.

Finally, in (i) and (iii), while the requirement that the function pairs (f , fnn) and (f , fspan) are close not

only in their values, but their derivatives as well is a matter of analytical convenience, our key results do not

necessarily depend on this precise assumption. Alternatively, we could replace this assumption with similar

(but incomparable) conditions: e.g., we can also assume closeness of the values and a rapid decay of the

L2 norms of the derivatives. We require control over the derivatives because our method’s gradient descent

iterations involve repeatedly applying the operator L to f —which results in progressively higher derivatives.

We can now formally state our main result:

2Since ∂Ω ∈ C∞ and the functions aij , c and f are all in C∞, it follows from Nirenberg [1955] (Theorem, Section 5) the eigenfuntions of L are also C∞. Hence, the function fspan is in C∞ as well.

5

Theorem 1 (Main Theorem). Consider a linear elliptic PDE satisfying Assumptions (i)-(iii), and let u⋆ ∈

H01(Ω) denote its unique solution. If there exists a neural network u0 ∈ H01(Ω) with N0 parameters, such that u⋆ − u0 L2(Ω) ≤ R, for some R < ∞, then for every T ∈ N such that T ≤ 20 min1(λk,1)δ , there exists a

neural network uT with size

O d2T (N0 + NA) + T (Nf + Nc)

such that u⋆ − uT L2(Ω) ≤ ǫ + ǫ˜ where

λ˜k − λ˜1 T ǫ := λ˜k + λ˜1 R, ǫ˜ := ǫsλp1an + λδ1 fγ −L2(δΩ) + δ u⋆ L2(Ω) + (max{1, T 2Cη})T ǫspan + ǫnn + 4 1 + γ −δ δ λTk f L2(Ω) ,

and η := λ˜1+2λ˜k , δ := max ǫmA , ǫζc . Furthermore, the activation functions used in uT belong to the set Σ ∪ Σ′ ∪ {ρ} where ρ(y) = y2 for all y ∈ R is the square activation function.

This theorem shows that given an initial neural network u0 ∈ H01(Ω) containing N0 parameters, we can recover a neural network that is ǫ close to the unique solution u⋆. The number of parameters in uǫ depend on how close the initial estimate u0 is to the solution u⋆, and N0. This results in a trade-oﬀ, where better

approximations may require more parameters, compared to a poorer approximation with fewer parameters.

Note that ǫ → 0 as T → ∞, while ǫ˜ is a “bias” error term that does not go to 0 as T → ∞. The ﬁrst

three terms in the expression for ˜ǫ result from bounding the diﬀerence between the solutions to the equations Lu = f and L˜u = fspan, whereas the third term is due to diﬀerence between f and fnn and the fact that

our proof involves simulating the gradient descent updates with neural networks. Further, if the constant ζ

is equal to 0 then the error term ǫc will also be 0, in which case the term δ will equal ǫA/m.

The fact that ǫ :=

λ˜k −λ˜1

T
R comes from the fact that we are simulating T steps of a gradient descent-

λ˜k +λ˜1

like procedure on a strongly convex loss. The parameters λ˜k and λ˜1 can be thought of as the eﬀective

Lipschitz and strong-convexity constants of the loss. Finally, to give a sense of what R looks like, we show

in Lemma 1 that if u0 is initialized to be identically zero then R ≤ f λL12(Ω) .

Lemma 1. If u0 = 0, then R ≤ f λL12(Ω) .
Proof. Given that u0 is identically 0, the value of R in Theorem 1 equals the inequality in (2), we have,

u⋆ − u0 L2(Ω) =

u⋆ L2(Ω) Using

=⇒

u⋆

2 L2(Ω)

≤

Lu⋆, u⋆ λ1

≤ 1 f, u⋆ L2(Ω) λ1

≤ 1 f L2(Ω) u⋆ L2(Ω) λ1

u⋆ L2(Ω) ≤ 1 f L2(Ω) λ1

We make few remarks about the theorem statement:

Remark 1. While we state our convergence results in L2(Ω) norm, our proof works for the H01(Ω) norm as well. This is because in the space deﬁned by the top-k eigenfunctions of the operator L, L2(Ω) and H01(Ω) norm are equivalent (shown in Proposition A.1). Further, note that even though we have assumed that
u⋆ ∈ H01(Ω) is the unique solution of (1) from the boundary regularity condition, we have that u⋆ ∈ H2(Ω) (see Evans [1998], Chapter 6, Section 6.3). This ensures that the solution u⋆ is twice diﬀerentiable as well.

6

Remark 2. To get a sense of the scale of λ1 and λk, when L = −∆ (the Laplacian operator), the eigenvalue

λ1 = inf u∈H1(Ω)

∇u u

L2 (Ω)

=

1 C

,

where

Cp

is

the

Poincar´e

constant

(see

Theorem

A.1

in

Appendix).

For

0

L2 (Ω)

p

geometrically well-behaved sets Ω (e.g. convex sets with a strongly convex boundary, like a sphere), Cp is

even dimension-independent. Further from the Weyl’s law operator (Evans [1998], Section 6.5) we have

lim λdk/2 = (2π)d k→∞ k vol(Ω)α(d)

where α(d) is the volume of a unit ball in d dimensions. So, if vol(Ω) ≥ 1/α(d), λk grows as O(k2/d), which is a constant so long as log k ≪ d.

6 Proof of Main Result
First, we provide some intuition behind the proof, via an analogy between a uniformly elliptic operator and a positive deﬁnite matrix in linear algebra. We can think of ﬁnding the solution to the equation Lu = f for an elliptic L as analogous to ﬁnding the solution to the linear system of equations Qx = k, where Q is a d × d positive deﬁnite matrix, and x and k are d-dimensional vectors. One way to solve such a linear system is by minimizing the strongly convex function Qx − b 2 using gradient descent. Since the objective is strongly convex, after O(log(1/ǫ)) gradient steps, we reach an ǫ-optimal point in an l2 sense.
Our proof uses a similar strategy. First, we show that for the operator L, we can deﬁne a sequence of functions that converge to an ǫ-optimal function approximation (in this case in the L2(Ω) norm) after O(log(1/ǫ) steps—similar to the rate of convergence for strongly convex functions. Next, we inductively show that each iterate in the sequence can be approximated by a small neural network. More precisely, we show that given a bound on the size of the t-th iterate ut, we can, in turn, upper bound the size of the (t+1)-th iterate ut+1 because the update transforming ut to ut+1 can be simulated by a small neural network (Lemma 7). These iterations look roughly like ut+1 ← ut − η(Lut − f ), and we use a “backpropagation” lemma (Lemma 8) which bounds the size of the derivative of a neural network.
6.1 Deﬁning a Convergent Sequence
The rough idea is to perform gradient descent in L2(Ω) [Neuberger, 2009; Farag´o and Kar´atson, 2001, 2002] to deﬁne a convergent sequence whose iterates converge to u⋆ in L2(Ω) norm (and following Remark 1, in H01(Ω) as well). However, there are two obstacles to deﬁning the iterates as simply ut+1 ← ut − η(Lut − f ): (1) L is unbounded—so the standard way of choosing a step size for gradient descent (roughly the ratio of the minimum and maximum eigenvalues of L) would imply choosing a step size η = 0, and (2) L does not necessarily preserve the boundary conditions, so if we start with ut ∈ H01(Ω), it may be that Lut − f does not even lie in H01(Ω).
We resolve both issues by restricting the updates to the span of the ﬁrst k eigenfunctions of L. More concretely, as shown in Lemma 2, if a function u in Φk, then the function Lu will also lie in Φk. We also show that within the span of the ﬁrst k eigenfunctions, L is bounded (with maximum eigenvalue λk), and can therefore be viewed as an operator from Φk to Φk. Further, we use fspan instead of f in our updates, which now have the form ut+1 ← ut − η(Lut − fspan). Since fspan belongs to Φk, for a ut in Φk the next iterate ut+1 will now remain in Φk. Continuing the matrix analogy, we can choose the usual step size of η = λ1+2λk . Precisely, we show:
Lemma 2. Let L be an elliptic operator. Then, for all v ∈ Φk it holds:
1. Lv ∈ Φk.
2. λ1 v L2(Ω) ≤ Lv, v L2(Ω) ≤ λk v L2(Ω)
3. I − λk+2 λk L u L2(Ω) ≤ λλkk−+λλ11 u L2(Ω)
7

Proof. Writing u ∈ Φk as u = i diϕi where di = u, ϕi L2(Ω), we have Lu = Lu ∈ Φ˜ k and Lu lies in H01(Ω), proving (1.).
Since v ∈ Φk, we use the deﬁnition of eigenvalues in (3) to get,

Lv, v L2(Ω)

Lv, v L2(Ω)

v L2(Ω)

≤ sup
v

v L2(Ω) = λk

=⇒

Lv, v

L2(Ω) ≤ λk

v

2 L2(Ω)

and similarly

Lv, v L2(Ω)

Lv, v L2(Ω)

v L2(Ω)

≥ inf
v

v L2(Ω) = λ1

=⇒

Lv, v

L2(Ω) ≥ λ1

v

2 L2(Ω)

k i=1

λi

di

ϕi

.

Therefore

In order to prove (2.) let us ﬁrst denote L¯ := I − λk+2 λ1 L . Note if ϕ is an eigenfunction of L with

corresponding eigenvalue λ, it is also an eigenfunction of L¯ with corresponding eigenvalue λkλ+kλ+1λ−12λ .

Hence, writing u ∈ Φk as u =

k i=1

diϕi,

where

di

=

u, ϕi , we have

L¯u 2

k λk + λ1 − 2λi

2

=

dϕ

≤ max λk + λ1 − 2λi 2

k

2

dϕ

(5)

L2(Ω)

i=1 λk + λ1

i i i∈k L2(Ω)

λk + λ1

ii

i=1

L2(Ω)

By the orthogonality of {ϕi}ki=1, we have

k
diϕi
i=1

2

k

= d2i =

L2(Ω) i=1

u

2 L2(Ω)

Since λ1 ≤ λ2 · · · ≤ λk, we have λk +λ1 −2λi ≥ λ1 −λk and λk +λ1 −2λi ≤ λk −λ1, so |λk +λ1 −2λi| ≤ λk −λ1.

2

2

This implies maxi∈k λk+λkλ+1−λ12λi ≤ λλ11−+λλkk . Plugging this back in (5), we get the claim we wanted.

In fact, we will use a slight variant of the updates and instead set ut+1 ← ut − η(L˜u − f˜span) as the iterates of the convergent sequence, where f˜span is the projections of f onto Φ˜ k. This sequence satisﬁes two important properties: (1) The convergence point of the sequence and u⋆, the solution to the original PDE, are not too far from each other; (2) The sequence of functions converges exponentially fast. In Section 6.2, we will see that updates deﬁned thusly will be more convenient to simulate via a neural network.
The ﬁrst property is formalized as follows:
Lemma 3. Assume that u˜⋆span is the solution to the PDE L˜u = f˜span, where f˜span : H01(Ω) → R is the projections of f onto Φ˜ k. Given Assumptions (i)-(iii), we have u⋆ − u˜⋆span L2(Ω) ≤ ǫ, such that ǫ = ǫsλp1an + λδ1 f γL−2δ(Ω) + δ u˜⋆span L2(Ω), where γ = λ1k − λk1+1 and δ = max ǫmA , ǫζc .

The proof for Lemma 3 is provided in the Appendix (Section B.1). Each of the three terms in the
ﬁnal error captures diﬀerent sources of perturbation: the ﬁrst term comes from approximating f by fspan; the second term comes from applying Davis-Kahan [Davis and Kahan, 1970] to bound the “misalignment” between the eigenspaces Φk and Φ˜ k (hence, the appearance of the eigengap between the k and (k + 1)-st eigenvalue of L−1); the third term is a type of “relative” error bounding the diﬀerence between the solutions to the PDEs Lu = f˜span and L˜u = f˜span.
The “misalignment” term can be characterized through the following lemma:

8

Lemma 4 (Bounding distance between fspan and f˜span). Given Assumptions (i)-(iii)and denoting the projection of f onto Φ˜ k by f˜span we have:

fspan − f˜span L2(Ω) ≤ f L2(Ω)δ

(6)

γ−δ

where δ = max

ǫA m

,

ǫc ζ

.

Proof. Let us write fspan =

k i=1

fiϕi

where

fi

=

f, ϕi L2(Ω). Further, we can deﬁne a function f˜span ∈ Φ˜ k

such that f˜span =

k i=1

f˜iϕ˜i

such

that

f˜i

=

f, ϕ˜i L2(Ω).

If Pkg :=

k i=1

g, ϕi

L2 (Ω) ϕi

and

P˜k g

:=

k i=1

g, ϕ˜i

L2 (Ω) ϕ˜i

denote

the

projection

of

a

function

g

onto

Φk and Φ˜ k, from Lemma C.1, we have:

fspan − f˜span

L2(Ω) =

k
f, ϕi L2(Ω)ϕi −
i=1

= Pkf − P˜kf
L2(Ω)
≤ Pk − P˜k f L2(Ω) δ
≤ γ − δ f L2(Ω)

f, ϕ˜i L2(Ω)ϕ˜i

L2(Ω)

where γ = λ1k − λk1+1 , and δ = max ǫmA , ǫζc .

The main technical tool for bounding the diﬀerence between the operators L and L˜ can be formalized through the lemma below. Note, the “relative” nature of the perturbation is because L and L˜ are not bounded operators.

Lemma 5 (Relative operator perturbation bound). Consider the operator L˜ deﬁned in (4), then for all u ∈ H01(Ω) we have the following:
1. (L˜ − L)u, u ≤ δ Lu, u

2.

(L−1L˜ − I)u, u

L2(Ω) ≤ δ

u

2 L2(Ω)

where δ = max

ǫA m

,

ǫc ζ

.

Proof.

(L˜ − L)u, u = (A˜ − A)∇u · ∇u + (c˜ − c)u2 dx
Ω

≤ max A˜ij − Aij L∞(Ω)
ij

∇u

2 L2(Ω)

+

c˜ − c

L∞(Ω)

u

2 L2(Ω)

≤ ǫA

∇u

2 L2(Ω)

+

ǫc

u

2 L2(Ω)

(7)

Further, note that

Lu, u = A∇u · ∇u + cu2dx

Ω

≥m

∇u

2 L2(Ω)

+

ζ

u

2 L2(Ω)

(8)

9

Using

the

inequality

a+b c+d

≥

min{ ac ,

db }

from

(7)

and

(8),

we

have

m

∇u

2 L2(Ω)

+

ζ

u

2 L2(Ω)

mζ

ǫA ∇u 2 + ǫc u 2 ≥ min ǫA , ǫc

(9)

L2(Ω)

L2(Ω)

Hence this implies that

(L˜ − L)u, u ≤ δ Lu, u

where δ = max

ǫA m

,

ǫc ζ

proving part (1.).

Further, for part (2.) we have for all u ∈ H01(Ω),

(L˜ − L)u, u L2(Ω) ≤ δ Lu, u L2(Ω) =⇒ (L˜L−1 − I)Lu, u L2(Ω) ≤ δ Lu, u L2(Ω) =⇒ (L˜L−1 − I)v, u L2(Ω) ≤ δ v, u L2(Ω) =⇒ (L˜L−1)v, u L2(Ω) ≤ (1 + δ) v, u L2(Ω)

(10)

where v = Lu. Therefore using (10) the following holds for all u ∈ H01(Ω),

(L˜L−1)u, u

L2(Ω) ≤ (1 + δ)

u

2 L2(Ω)

=(1⇒)

u, (L−1L˜)u L2(Ω) ≤ (1 + δ)

u

2 L2(Ω)

=(2⇒)

(L−1L˜ − I)u, u L2(Ω) ≤ δ

u

2 L2(Ω)

(11)

where we use the fact that the operators L˜ and L−1 are self-adjoint to get (1) and then bring the appropriate terms to the LHS in (2).

The second property of the sequence of functions is that they converge exponentially fast. Namely, we show:

Lemma 6 (Convergence of gradient descent in L2). Let u˜⋆span denote the unique solution to the PDE L˜u = f˜span, where f˜span ∈ Φ˜ k, and the operator L˜ satisﬁes the conditions in Lemma 2. For any u0 ∈ H01(Ω) such that u0 ∈ Φ˜ k, we deﬁne the sequence

ut+1 ← ut − 2 (L˜ut − f˜span) (t ∈ N)

(12)

λ˜1 + λ˜k

where for all t ∈ N, ut ∈ H01(Ω). Then for any t ∈ N, we have

ut − u˜⋆span L2(Ω) ≤

λ˜k − λ˜1 λ˜k + λ˜1

t−1
u0 − u˜⋆span L2(Ω)

The proof is essentially the same as the the analysis of the convergence time of gradient descent for strongly convex losses. Namely, we have:

Proof. Given that u0 ∈ H01(Ω) and u0 ∈ Φ˜ k the function L˜u0 ∈ H01(Ω) and L˜u0 ∈ Φ˜ k as well (from Lemma 2). As f˜span ∈ Φ˜ k, all the iterates in the sequence will also belong to H01(Ω) and will lie in the Φ˜ k. Now at a step t the iteration looks like,

2 ut+1 = un − λ˜k + λ˜1

L˜ut − f˜span

ut+1 − u˜⋆span =

I − 2 L˜ λ˜k + λ˜1

(ut − u˜⋆span)

10

Using the result from Lemma 2, part 3. we have,

ut+1 − u˜⋆span L2(Ω) ≤

λ˜k − λ˜1 λ˜k + λ˜1

ut − u˜⋆span L2(Ω)

⋆

λ˜k − λ˜1 t

⋆

=⇒ ut+1 − u˜span L2(Ω) ≤ λ˜k + λ˜1 u0 − u˜span

L2(Ω)

This ﬁnishes the proof.

Combining the results from Lemma 3 and Lemma 6 via triangle inequality, we have:

u⋆ − uT L2(Ω) ≤ u⋆ − u˜⋆span L2(Ω) + u˜⋆span − uT L2(Ω)

and the ﬁrst term on the RHS subsumes the ﬁrst three summands of ǫ˜ deﬁned in Theorem 1.

6.2 Approximating iterates by neural networks

In Lemma 6, we show that there exists a sequence of functions (12) which converge fast to a function close to u⋆. The next step in the proof is to approximate the iterates by neural networks.
The main idea is as follows. Suppose ﬁrst the iterates ut+1 = ut − η(L˜ut − f˜span) are such that f˜span is exactly representable as a neural network. Then, the iterate ut+1 can be written in terms of three operations performed on ut, a and f : taking derivatives, multiplication and addition. Moreover, if g is representable as a neural network with N parameters, the coordinates of the vector ∇g can be represented by a neural network
with O(N ) parameters. This is a classic result (Lemma 8), essentially following from the backpropagation
algorithm. Finally, addition or multiplication of two functions representable as neural networks with sizes
N1, N2 can be represented as neural networks with size O(N1 + N2) (see Lemma 9). Using these facts, we can write down a recurrence upper bounding the size of neural network approx-
imation ut+1, denoted by uˆt+1, in terms of the number of parameters in uˆt (which is the neural network approximation to ut). Formally, we have:

Lemma 7 (Recursion Lemma). Given the Assumptions (i)-(iii), consider the update equation

uˆt+1 ← uˆt − 2

L˜uˆt − fnn

(13)

λ˜1 + λ˜k

If at step t, uˆt : Rd → R is a neural network with Nt parameters, then the function uˆt+1 is a neural network with O(d2(NA + Nt) + Nt + Nf˜ + Nc) parameters.

Proof. Expand the update uˆt+1 ← uˆt − η L˜uˆt − fnn as follows:



d

d

uˆt+1 ← uˆt − η  a˜ij ∂ij uˆt +

i,j=1

j=1

d
∂ia˜ij
i=1

 ∂juˆt + c˜uˆt − fnn .

Using Lemma 8, ∂ij uˆt, ∂juˆt and ∂ia˜ij can be represented by a neural network with O(Nt), O(Nt) and O(NA) parameters, respectively. Further, ∂ia˜ij∂ju and a˜ij∂ij uˆ can be represented by a neural network with O(NA + Nt) parameters, and c˜uˆt can be represented by a network with O(Nt + Nc) parameters, from Lemma 9. Hence uˆt+1 can be represented in O(d2(NA + Nt) + Nf + Nc + Nt) parameters. Note that, throughout the entire proofs O hides independent constants.

Combining the results of Lemma 6 and Lemma 7, we can get a recurrence for the number of parameters required to represent the neural network uˆt:
Nt+1 ≤ d2Nt + d2NA + Nt + Nf˜ + Nc

11

Unfolding

this

recurrence,

we

get

NT

≤

d2T N0

+

d

2

(dT − d2−1

1)

N

A

+ T (Nf ) + Nc).

The formal lemmas for the diﬀerent operations on neural networks we can simulate using a new neural

network are as follows:

Lemma 8 (Backpropagation, Rumelhart et al. [1986]). Consider neural network g : Rm → R with depth l,

N parameters and diﬀerentiable activation functions in the set {σi}Ai=1. There exists a neural network of

size

O(l + N )

and

activation

functions

in

the

set

{

σi

,

σi′

}

A i=1

that

calculates

the

gradient

dg di

for

all

i ∈ [m].

Lemma 9 (Addition and Multiplication). Given neural networks g : Ω → R, h : Ω → R, with Ng and Nh parameters respectively, the operations g(x) + h(x) and g(x) · h(x) can be represented by neural networks of size O(Ng + Nh), and square activation functions.

Proof. For Addition, there exists a network h containing both networks f and g as subnetworks and an

extra layer to compute the addition between their outputs. Hence, the total number of parameters in such

a network will be O(Nf + Ng).

For

Multiplication,

consider

the

operation

f (x) · g(x)

=

1 2

(f (x) + g(x))2 − f (x)2 − g(x)2 . Then fol-

lowing the same argument as for addition of two networks, we can construct a network h containing both

networks and square activation function.

While the representation result in Lemma 9 is shown using square activation, we refer to Yarotsky [2017] for approximation results with ReLU activation. The scaling with respect to the number of parameters in the network remains the same.

Finally, we have to deal with the fact that f˜span is not exactly a neural network, but only approximately so. The error due to this discrepancy can be characterized through the following lemma:

Lemma 10 (Error using fnn). Consider the update equation in (13), where fnn is a neural network with Nf . Then the neural network uˆt approximates the function ut such that ut − uˆt L2(Ω) ≤ ǫ(ntn) where ǫ(ntn) is

O (max{1, t2ηeC})t ǫspan + ǫnn + 4 1 + γ −δ δ λtk f L2(Ω)

where δ = max ǫmA , ǫζc , γ = λ1k − λk1+1 , and α is a multi-index.
The proof for the lemma is deferred to Section B.2 of the Appendix. The main strategy to prove this lemma involves tracking the “residual” non-neural-network part of the iterates. Precisely, for every t ∈ N, we will write ut = uˆt + rt, s.t. uˆt is a neural network and bound rt L2(Ω). {uˆt}∞ t=0 is deﬁned such that

uˆ0 = u0, uˆt+1 = uˆt − η L˜uˆt − fnn

Correspondingly, as rt = ut − uˆt, we have:

r0 = 0, rt+1 = (I − ηL˜)rt − r

Unfolding the recurrence, we have rt = ηL˜)(i) L2(Ω). 3

ti=−01(I − ηL˜)(i)r, which reduces the proof to bounding (I −

3The reason we require that fnn is close to f not only in the L2 sense but also in terms of their higher order derivatives is since L˜(t)r involves 2t-order derivatives of r to be bounded at each step.

12

7 Applications to Learning Operators

A number of recent works attempt to simultaneously approximate the solutions for an entire family of PDEs

by learning a parametric map that takes as inputs (some representation of) the coeﬃcients of a PDE and

returns its solution [Bhattacharya et al., 2020; Li et al., 2020b,a]. For example, given a set of observations

that

{

a

j

,

u

j

}

N j=

1

,

where

each

aj

denotes

a

coeﬃcient

of

a

PDE

with

corresponding

solution

uj ,

they

learn

a neural network G such that for all j, uj = G(aj ). Our parametric results provide useful insights for why

simultaneously solving an entire family of PDEs with a single neural network G is possible in the case of

linear elliptic PDEs.

Consider the case where the coeﬃcients aj in the family of PDEs are given by neural networks with a

ﬁxed architecture, but where each instance of a PDE is characterized by a diﬀerent setting of the weights in

the models representing the coeﬃcients. Lemma 7 shows that each iteration of our sequence (12) constructs

a new network containing both the current solution and the coeﬃcient networks as subnetworks. We can

view our approximation as not merely approximating the solution to a single PDE but to every PDE in

the family, by treating the coeﬃcient networks as placeholder architectures whose weights are provided as

inputs. Thus, our construction provides a parametric map between the coeﬃcients of an elliptic PDE in this

family and its solution.

8 Conclusion and Future Work
We derive parametric complexity bounds for neural network approximations for solving linear elliptic PDEs with Dirichlet boundary conditions, whenever the coeﬃcients can be approximated by are neural networks with ﬁnite parameter counts. By simulating gradient descent in function spaces using neural networks, we construct a neural network that approximates the solution of a PDE. We show that the number of parameters in the neural network depends on the parameters required to represent the coeﬀcients and has a poly(d) dependence on the dimension of the input space, therefore avoiding the curse of dimensionality.
An immediate open question is related to the tightening our results: our current error bound is sensitive to the neural network approximation lying close to Φk which could be alleviated by relaxing (by adding some kind of “regularity” assumptions) the dependence of our analysis on the ﬁrst k eigenfunctions. Further, the dependencies in the exponent of d on R and κ in parametric bound may also be improvable. Finally, the idea of simulating an iterative algorithm by a neural network to derive a representation-theoretic result is broadly applicable, and may be a fertile ground for further work, both theoretically and empirically, as it suggest a particular kind of weight tying.

13

References
William F Ames. Numerical methods for partial diﬀerential equations. Academic press, 2014.
John David Anderson and J Wendt. Computational ﬂuid dynamics, volume 206. Springer, 1995.
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473, 2014.
Kaushik Bhattacharya, Bamdad Hosseini, Nikola B Kovachki, and Andrew M Stuart. Model reduction and neural networks for parametric pdes. arXiv preprint arXiv:2005.03180, 2020.
Fischer Black and Myron Scholes. The pricing of options and corporate liabilities. Journal of political economy, 81(3):637–654, 1973.
John Crank and Phyllis Nicolson. A practical method for numerical evaluation of solutions of partial diﬀerential equations of the heat-conduction type. In Mathematical Proceedings of the Cambridge Philosophical Society, volume 43, pages 50–67. Cambridge University Press, 1947.
Chandler Davis and William Morton Kahan. The rotation of eigenvectors by a perturbation. iii. SIAM Journal on Numerical Analysis, 7(1):1–46, 1970.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.
Matthias Ehrhardt and Ronald E Mickens. A fast, stable and accurate numerical method for the black– scholes equation of american options. International Journal of Theoretical and Applied Finance, 11(05): 471–501, 2008.
Lawrence C Evans. Partial Diﬀerential Equations. graduate studies in mathematics. american mathematical society, 1998. ISBN 9780821807729.
I Farag´o and J Kar´atson. The gradient-ﬁnite element method for elliptic problems. Computers & Mathematics with Applications, 42(8-9):1043–1053, 2001.
Istv´an Farag´o and J´anos Kar´atson. Numerical solution of nonlinear elliptic problems via preconditioning operators: Theory and applications, volume 11. Nova Publishers, 2002.
David Gilbarg and Neil S Trudinger. Elliptic partial diﬀerential equations of second order. 2001.
Philipp Grohs and Lukas Herrmann. Deep neural network approximation for high-dimensional elliptic pdes with boundary conditions. arXiv preprint arXiv:2007.05384, 2020.
Philipp Grohs, Fabian Hornung, Arnulf Jentzen, and Philippe Von Wurstemberger. A proof that artiﬁcial neural networks overcome the curse of dimensionality in the numerical approximation of black-scholes partial diﬀerential equations. arXiv preprint arXiv:1809.02362, 2018.
Jiequn Han, Arnulf Jentzen, and E Weinan. Solving high-dimensional partial diﬀerential equations using deep learning. Proceedings of the National Academy of Sciences, 115(34):8505–8510, 2018.
Arnulf Jentzen, Diyora Salimova, and Timo Welti. A proof that deep artiﬁcial neural networks overcome the curse of dimensionality in the numerical approximation of kolmogorov partial diﬀerential equations with constant diﬀusion and nonlinear drift coeﬃcients. arXiv preprint arXiv:1809.07321, 2018.
Yuehaw Khoo, Jianfeng Lu, and Lexing Ying. Solving parametric pde problems with artiﬁcial neural networks. arXiv preprint arXiv:1707.03351, 2017.
14

Alex Krizhevsky, Ilya Sutskever, and Geoﬀrey E Hinton. Imagenet classiﬁcation with deep convolutional neural networks. In F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems, volume 25, pages 1097–1105. Curran Associates, Inc., 2012.
Gitta Kutyniok, Philipp Petersen, Mones Raslan, and Reinhold Schneider. A theoretical analysis of deep neural networks and parametric pdes. arXiv preprint arXiv:1904.00377, 2019.
Isaac E Lagaris, Aristidis Likas, and Dimitrios I Fotiadis. Artiﬁcial neural networks for solving ordinary and partial diﬀerential equations. IEEE transactions on neural networks, 9(5):987–1000, 1998.
Isaac E Lagaris, Aristidis C Likas, and Dimitris G Papageorgiou. Neural-network methods for boundary value problems with irregular boundaries. IEEE Transactions on Neural Networks, 11(5):1041–1049, 2000.
Peter D Lax and Arthur N Milgram. Parabolic equations, volume 33 of annals of mathematics studies, 1954.
Randall J LeVeque. Finite diﬀerence methods for ordinary and partial diﬀerential equations: steady-state and time-dependent problems. SIAM, 2007.
Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew Stuart, and Anima Anandkumar. Fourier neural operator for parametric partial diﬀerential equations. arXiv preprint arXiv:2010.08895, 2020a.
Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew Stuart, and Anima Anandkumar. Neural operator: Graph kernel network for partial diﬀerential equations. arXiv preprint arXiv:2003.03485, 2020b.
Alaeddin Malek and R Shekari Beidokhti. Numerical solution for high order diﬀerential equations using a hybrid neural network—optimization method. Applied Mathematics and Computation, 183(1):260–271, 2006.
John Neuberger. Sobolev gradients and diﬀerential equations. Springer Science & Business Media, 2009.
Louis Nirenberg. Remarks on strongly elliptic partial diﬀerential equations. Communications on pure and applied mathematics, 8(4):648–674, 1955.
Bernt Oksendal. Stochastic diﬀerential equations: an introduction with applications. Springer Science & Business Media, 2013.
M Necati O¨ zi¸sik, Helcio RB Orlande, Marcelo J Colac¸o, and Renato M Cotta. Finite diﬀerence methods in heat transfer. CRC press, 2017.
Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.
Maziar Raissi, Paris Perdikaris, and George Em Karniadakis. Physics informed deep learning (part i): Data-driven solutions of nonlinear partial diﬀerential equations. arXiv preprint arXiv:1711.10561, 2017.
Maziar Raissi, Paris Perdikaris, and George E Karniadakis. Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial diﬀerential equations. Journal of Computational Physics, 378:686–707, 2019.
David E Rumelhart, Geoﬀrey E Hinton, and Ronald J Williams. Learning representations by backpropagating errors. nature, 323(6088):533–536, 1986.
Justin Sirignano and Konstantinos Spiliopoulos. Dgm: A deep learning algorithm for solving partial diﬀerential equations. Journal of computational physics, 375:1339–1364, 2018.
15

Roger Temam. Navier-Stokes equations: theory and numerical analysis, volume 343. American Mathematical Soc., 2001.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems, pages 5998–6008, 2017.
Dmitry Yarotsky. Error bounds for approximations with deep relu networks. Neural Networks, 94:103–114, 2017.

A Brief Overview of Partial Diﬀerential Equations

In this section, we introduce few key deﬁnitions and results from PDE literature. We note that the results in this section are standard and have been included in the Appendix for completeness. We refer the reader to classical texts on PDEs [Evans, 1998; Gilbarg and Trudinger, 2001] for more details.
We will use the following Poincar´e inequality throughout our proofs.

Theorem A.1 (Poincar´e inequality). Given Ω ⊂ Rd, a bounded open subset, there exists a constant Cp > 0 such that for all u ∈ H01(Ω)
u L2(Ω) ≤ Cp ∇u L2(Ω).

Corollary A.1. For the bounded open subset Ω ⊂ Rd, for all u ∈ H01(Ω), we deﬁne the norm in the Hilbert space H01(Ω) as

u H1(Ω) = ∇u L2(Ω). 0

(14)

Further, the norm in H01(Ω) is equivalent to the norm H1(Ω).

Proof. Note that for u ∈ H01(Ω) we have,

=⇒

u H1(Ω) = ≥
u H1(Ω) ≥

∇u L2(Ω) + ∇u L2(Ω) u H1(Ω).
0

u L2(Ω)

Where we have used the deﬁnition of the norm in H01(Ω) space. Further, using the result in Theorem A.1 we have

u

2 H1(Ω)

=

u

2 L2(Ω)

+

∇u

2 L2(Ω)

≤ Cp2 + 1

∇u

2 H1(Ω)

(15)

Therefore, combining the two inequalities we have

u H1(Ω) ≤ u H1(Ω) ≤ Ch u H1(Ω)

0

0

(16)

where Ch = (Cp2 + 1). Hence we have that the norm in H01(Ω) and H1(Ω) spaces are equivalent.
Proposition A.1 (Equivalence between L2(Ω) and H01(Ω) norms). If v ∈ Φk then we have that v L2(Ω) is equivalent to v H1(Ω).
0
Proof. We have from the Poincare inequality in Theorem A.1 that for all v ∈ H01(Ω), the norm in L2(Ω) is upper bounded by the norm in H01(Ω), i.e.,

v

2 L2(Ω)

≤

v

2 H1(Ω)

0

16

Further, using results from (18) and (17) (where b(u, v) := Lu, v L2(Ω)), we know that for all v ∈ H01(Ω) we have

m

v

2 H 1 (Ω)

≤

Lv, v L2(Ω) ≤ max{M, Cp

c

L∞(Ω)}

v

2 H1(Ω)

0

0

This implies that Lu, v L2(Ω) is equivalent to the inner product u, v H1(Ω), i.e., for all u, v ∈ H01(Ω), 0

m u, v H1(Ω) ≤ Lu, v L2(Ω) ≤ max M, Cp c L∞(Ω) 0
Further, since v ∈ Φk, we have from Lemma 2 that

u, v H1(Ω) 0

=⇒

Lv, v L2(Ω) ≤ λk

v H1(Ω) ≤ λk v

0

c1

v

2 L2(Ω)

2 L2(Ω)

Hence we have that for all v ∈ Φk v L2(Ω) is equivalent to v H1(Ω) and by Corollary A.1 is also equivalent 0
to v H1(Ω).

Now introduce a form for Lu, v L2(Ω) that is more amenable for the existence and uniqueness results. Lemma A.1. For all u, v ∈ H01(Ω), we have the following,
1. The inner product Lu, v L2(Ω) equals,

Lu, v L2(Ω) = (A∇u · ∇v + cuv) dx
Ω

2. The operator L is self-adjoint.

Proof. 1. We will be using the following integration by parts formula,

∂u

∂v

Ω ∂xi dx = −

Ω u ∂xi dx +

uvni∂Γ
∂Ω

Where ni is a normal at the boundary and ∂Γ is an inﬁnitesimal element of the boundary. Hence we have for all u, v ∈ H01(Ω),

Lu, v L2(Ω) = =

d

−

(∂i (A∇u)i)

Ω

i=1

A∇u · ∇vdx −

Ω

∂Ω

v + cuv dx
d
(A∇u)i ni
i=1

vdΓ + cuvdx
Ω

= A∇u · ∇vdx + cuvdx

Ω

Ω

(∵ v|∂Ω = 0)

2. To show that the operator L : H01(Ω) → H01(Ω) is self-adjoint, we show that for all u, v ∈ H01(Ω) we have Lu, v = u, Lv .
From Proposition A.1, for functions u, v ∈ H01(Ω) we have

Lu, v L2(Ω) = A∇u · ∇vdx + cuvdx

Ω

Ω

= A∇v · ∇udx + cvudx

Ω

Ω

= u, Lv

17

A.1 Proof of Proposition 1

We ﬁrst show that if u is the unique solution then it minimizes the variational norm.
Let u denote the weak solution, further for all w ∈ H01(Ω) let v = u + w. Using the fact that L is self-adjoint (as shown in Lemma A.1) we have

1 J (v) = J (u + w) = 2 L(u + w), (u + w) L2(Ω) − f, u + w L2(Ω)

1

1

= 2 Lu, u L2(Ω) + 2 Lw, w L2(Ω) + Lu, w L2(Ω) − f, u L2(Ω) −

1 = J (u) + 2 Lw, w L2(Ω) + Lu, w L2(Ω) − f, w L2(Ω)

≥ J(u)

f, w L2(Ω)

where we use the fact that Lu, u L2(Ω) > 0 and that u is a weak solution hence (1) holds for all w ∈ H01(Ω). To show the other side, assume that u minimizes J, i.e., for all λ > 0 and v ∈ H01(Ω) we have, J(u+λv) ≥
J (u),

J(u + λv) ≥ J(u)

1

1

2 L(u + λv), (u + λv) L2(Ω) − f, (u + λv) L2(Ω) ≥ 2

λ =⇒ 2 Lv, v L2(Ω) + Lu, v L2(Ω) − f, v L2(Ω) ≥ 0

Lu, u L2(Ω) −

f, u L2(Ω)

Taking λ → 0, we get

Lu, v L2(Ω) − f, v L2(Ω) ≥ 0

and also taking v as −v, we have

Lu, v L2(Ω) − f, v L2(Ω) ≤ 0

Together, this implies that if u is the solution to (2), then u is also the weak solution, i.e, for all v ∈ H01(Ω) we have
Lu, v L2(Ω) = f, v L2(Ω)

Proof for Existence and Uniqueness of the Solution

In order to prove for the uniqueness of the solution, we ﬁrst state the Lax-Milgram theorem.

Theorem A.2 (Lax-Milgram, Lax and Milgram [1954]). Let H be a Hilbert space with inner-product (·, ·) : H × H → R, and let b : H × H → R and l : H → R be the bilinear form and linear form, respectively. Assume that there exists constants C1, C2, C3 > 0 such that for all u, v ∈ H we have,

C1

u

2 H

≤

b(u,

u),

|b(u, v)| ≤ C2 u H v H,

and |l(u)| ≤ C3 u H.

Then there exists a unique u ∈ H such that,

b(u, v) = l(v) for all v ∈ H.

Having stated the Lax-Milgram Theorem, we make the following proposition,

Proposition A.2. Given the assumptions (i)-(iii), solution to the variational formulation in Equation 1 exists and is unique.
Proof. Using the variational formulation deﬁned in (1), we introduce the bilinear form b(·, ·) : H01(Ω) × H01(Ω) → R where b(u, v) := Lu, v . Hence, we prove the theorem by showing that the bilinear form b(u, v) satisﬁes the conditions in Theorem A.2.

18

We ﬁrst show that for all u, v ∈ H01(Ω) the following holds,

|b(u, v)| = (A∇u · ∇v + cuv) dx
Ω

≤ |(A∇u · ∇v + cuv)| dx
Ω

≤ |A∇u · ∇v| dx + |cuv| dx

Ω

Ω

≤ A L∞(Ω) ∇u L2(Ω) ∇v L2(Ω) + c L∞(Ω) u L2(Ω) v

≤ M ∇u L2(Ω) ∇v L2(Ω) + c L∞(Ω) u L2(Ω) v L2(Ω)

≤ max M, Cp c L∞(Ω) u H1(Ω) v H1(Ω)

0

0

L2(Ω

Now we show that the bilinear form a(u, u) is lower bounded.

(17)

b(v, v) = A∇v · ∇v + cv2 dx
Ω

≥ m ∇v 2dx = m v H1(Ω)

Ω

0

(18)

Finally, for v ∈ H01(Ω)

|(f, v)| =

f vdx ≤ f L2(Ω) v L2(Ω) ≤ Cp f L2(Ω) v H1(Ω)

Ω

0

Hence, we satisfy the assumptions in required in Theorem A.2 and therefore the variational problem deﬁned in (1) has a unique solution.

B Perturbation Analysis

B.1 Proof of Lemma 3

Proof. Using the triangle inequality the error between u⋆ and u˜⋆span, we have,

u⋆ − u˜⋆span L2(Ω) ≤ u⋆ − u⋆span L2(Ω) + u⋆span − u˜⋆span L2(Ω)

(19)

(I )

(I I )

where u⋆span is the solution to the PDE Lu = fspan. In order to bound Term (I), we use the inequality in (2) to get,

=⇒

u⋆ − u⋆span u⋆ − u⋆span

2
2

≤1

L (Ω) λ1

=1 λ1

≤1 λ1

1 L2(Ω) ≤ λ1

L(u⋆ − u⋆span), u⋆ − u⋆span L2(Ω) f − fspan, u⋆ − u⋆span L2(Ω) f − fspan L2(Ω) u⋆ − u⋆span L2(Ω) f − fspan L2(Ω) ≤ ǫspan
λ1

(20)

We now bound Term (II). First we introduce an intermediate PDE Lu = f˜span, and denote the solution u˜. Therefore, by utilizing
triangle inequality again Term (II) can be expanded as the following,

u⋆span − u˜⋆span L2(Ω) ≤ u⋆span − u˜ L2(Ω) + u˜ − u˜⋆span L2(Ω)

(21)

19

We will tackle the second term in (21) ﬁrst. Using u˜ = L−1f˜span and u˜⋆span = L˜−1f˜span,

u˜ − u˜⋆span L2(Ω) = (L−1 − L˜−1)f˜span L2(Ω) = (L−1L˜ − I)L˜−1f˜span L2(Ω)
=⇒ u˜ − u˜⋆span L2(Ω) = (L−1L˜ − I)u˜⋆span L2(Ω)

(22)

Therefore, using the inequality in Lemma 5 part (2.) we can upper bounded (22) to get,

u˜ − u˜⋆span L2(Ω) ≤ δ u˜⋆span L2(Ω)

(23)

where δ = max

ǫA m

,

ǫc ζ

.

Proceeding to the ﬁrst term in (21), using Lemma 4, and the inequality in (2), the term

can be upper bounded by,

u⋆span − u˜ L2(Ω)

=⇒

u⋆span − u˜ u⋆span − u˜

2
2

≤1

L (Ω) λ1

≤1 λ1

≤1 λ1

1 L2(Ω) ≤ λ1

L(u⋆span

−

u˜),

u

⋆ sp

an

−

u˜

L2(Ω)

fspan − f˜span, u⋆span − u˜ L2(Ω)

fspan − f˜span L2(Ω) u⋆span − u˜ L2(Ω)

fspan − f˜span L2(Ω) ≤ δ · λ1

f L2(Ω) γ−δ

(24)

Therefore Term (II), i.e., u⋆span − u˜⋆span L2(Ω) can be upper bounded by

u⋆span − u˜⋆span L2(Ω) ≤ u⋆span − u˜ L2(Ω) + u˜ − u˜⋆span L2(Ω) ≤ λǫˆf1 + δ u˜⋆span L2(Ω)

(25)

Putting everything together, we can upper bound (19) as

u⋆ − u˜⋆span

L2(Ω) ≤ u⋆ − u⋆span ≤ ǫspan + δ λ1 λ1

L2(Ω) + u⋆span − u˜⋆span L2(Ω) fγ −L2(δΩ) + δ u˜⋆span L2(Ω)

where γ = λ1k − λk1+1 and δ = max ǫmA , ǫζc .

B.2 Proof of Lemma 10
Proof. We deﬁne r = f˜span − fnn, therefore from Lemma C.2 we have that for any multi-index α,

L˜tr L2(Ω) ≤ (t!)2 · Ct (ǫnn + ǫspan) + 4

1+ δ γ−δ

λtk fspan L2(Ω).

For every t ∈ N, we will write ut = uˆt + rt, s.t. uˆt is a neural network and we (iteratively) bound rt L2(Ω). Precisely, we deﬁne a sequence of neural networks {uˆt}∞ t=0, s.t.

uˆ0 = u0, uˆt+1 = uˆt − η L˜uˆt − fnn

Since rt = ut − uˆt, we can deﬁne a corresponding recurrence for rt:

r0 = 0, rt+1 = (I − ηL˜)rt − r

20

Unfolding the recurrence, we get

t
rt+1 = (I − ηL˜)ir
i=0

(26)

Using the binomial expansion we can write:

t
(I − ηL˜)tr =
i=0

t (−1)i(ηL˜)ir i

=⇒ (I − ηL˜)(t)r L2(Ω) = t ti (−1)i(ηL˜)ir

i=0

L2(Ω)

t
≤
i=0

t ηi L˜ir L2(Ω) i

t
≤
i=0

tie i ηi L˜ir L2(Ω)

∵ t ≤ te i

i

i

(1) t
≤
i=0

te i iη

(i!)2Ci (ǫnn + ǫspan) + 4 1 + δ γ−δ

λik fspan L2(Ω)

t
≤
i=0

te η

i
(i!)2 C i

i

(ǫnn + ǫspan) + 4

1+ δ γ−δ

(i!λ)2ikCi fspan L2(Ω)

(2) t
≤
i=0

tie ηi2C i (ǫnn + ǫspan) + 4 1 + γ −δ δ

(i!λ)2ikCi fspan L2(Ω)

(3) t
≤
i=0

te ηi2C i i

δ (ǫnn + ǫspan) + 4 1 + γ − δ

λik fspan L2(Ω)

t
≤ (tieηC)i
i=0

(ǫnn + ǫspan) + 4

1+ δ γ−δ

λik fspan L2(Ω)

≤ t max{1, (t2eηC)t}

(ǫnn + ǫspan) + 4

1+ δ γ−δ

λtk fspan L2(Ω)

Here the inequality (1) follows by using the bound derived in Lemma C.2. Further, we use that all i ∈ N we

have

i!

≤

ii

in

(2)

and

the

inequality

(3)

follows

from

the

fact

that

1 (i!)2 C i

≤

1.

Hence we have the the ﬁnal upper bound:

rt L2(Ω) ≤ t2 max{1, (t2eηC)t}

ǫnn + ǫspan + 4

1+ δ γ−δ

λtk fspan L2(Ω)

C Technical Lemmas: Perturbation Bounds
In this section we introduce some useful lemmas about perturbation bounds used in the preceding parts of the appendix.
First we show a lemma that’s ostensibly an application of Davis-Kahan to the (bounded) operators L−1 and L˜−1.
21

Lemma C.1 (Subspace alignment). Consider linear elliptic operators L and L˜ with eigenvalues λ1 ≤ λ2 ≤ · · · and λ1 ≤ λ2 ≤ · · · respectively. Assume that γ := λ1k − λk1+1 > 0. For any function g ∈ H01(Ω), we

deﬁne Pkg :=

k i=1

g, ϕi

L2(Ω)ϕi

and

P˜k g

:=

k i=1

g, ϕ˜i

L2 (Ω) ϕ˜i

as

the

projection

of

g

onto

Φk

and

Φ˜ k,

respectively. Then we have:

Pkg − P˜kg L2(Ω) ≤ δ g L2(Ω)

(27)

γ−δ

where δ = max

ǫA m

,

ǫc ζ

.

Proof. We begin the proof by ﬁrst showing that the inverse of the operators L and L˜ are close. Using the

result from Lemma 5 with δ = max

ǫA m

,

ǫc ζ

, we have:

(L−1L˜ − I)u, u

L2(Ω) ≤ δ

u

2 L2(Ω)

=⇒

(L−1 − L˜−1)L˜u, u

L2(Ω) ≤ δ

u

2 L2(Ω)

=⇒

(L−1 − L˜−1)v, u

L2(Ω) ≤ δ

u

2 L2(Ω)

Now, the operator norm L−1 − L˜−1 can be written as,

L−1 − L˜−1

= sup
v∈H01 (Ω)

(L−1 − L˜−1)v, v L2(Ω)

v2

≤δ

L2(Ω)

(28)

Further note that, { λ1i }∞ i=1 and { λ˜1i }∞ i=1 are the eigenvalues of the operators L−1 and L˜−1, respectively. Therefore from Weyl’s Inequality and (28) we have:

sup 1 − 1 ≤ L−1 − L˜−1 ≤ δ

(29)

i λi λ˜i

Therefore, for all i ∈ N, we have that λ˜1i ∈ [ λ1i − δ, λ1i + δ], i.e., all the eigenvalues of L˜−1 are within δ of the eigenvalue of L−1. which therefore implies that the diﬀerence between kth eigenvalues is,

1

1

1

1

λ˜k − λk+1 ≥ λk − λk+1 − δ

Since the operators L−1, L˜−1 are bounded, the Davis-Kahan sin Θ theorem [Davis and Kahan, 1970] can be used to conclude that:

sin Θ(Φ , Φ˜ ) = P − P˜ ≤ L−1 − L˜−1 ≤ δ

(30)

kk

k

k

γ−δ

γ−δ

where · is understood to be the operator norm, and γ = λ1k − λk1+1 . Therefore for any function g ∈ H01(Ω) we have

Pkg − P˜kg L2(Ω) ≤ ≤

Pk − P˜k g L−1 − L˜−1
γ−δ

L2(Ω)
g L2(Ω)

By (30), we then get

Pkg − P˜kg

L2(Ω) ≤

δ γ−δ

g L2(Ω), which ﬁnishes the proof.

Finally, we show that repeated applications of L˜ to fnn − f have also bounded norms:

Lemma C.2 (Bounding norms of applications of L˜). The functions fnn and f satisfy:

22

1. L˜n(fnn − fspan) L2(Ω) ≤ (n!)2 · Cn(ǫspan + ǫnn)

2.

L˜n(fnn − f˜span) L2(Ω) ≤ (n!)2 · Cn(ǫspan + ǫnn) + 4

1

+

δ γ−δ

λnk f L2(Ω)

where δ = max

ǫA m

,

ǫc ζ

.

Proof. For Part 1, by Lemma D.4 we have that L˜n(fnn − fspan) L2(Ω) ≤ (n!)2 · Cn max ∂α(fnn − fspan) L2(Ω)
α:|α|≤n+2

(31)

From Assumptions (i)-(iii), for any multi-index α we have:

∂αfnn − ∂αfspan L2(Ω) ≤ ∂αfnn − ∂αf ≤ ǫnn + ǫspan

L2(Ω) +

∂αf − ∂αfspan L2(Ω)

(32)

Combining (31) and (32) we get the result for Part 1. For Part 2 we have,

L˜n(f˜span − fnn) L2(Ω) = ≤

L˜n(f˜span − fspan + fspan − fnn) L2(Ω) L˜n(f˜span − fspan) L2(Ω) + L˜n (fspan − fnn)

L2(Ω)

(33) (34)

Note that from Lemma 5 part (2.) we have that L−1L˜ − I ≤ δ (where · denotes the operator norm). This implies that there exists an operator Σ, such that Σ ≤ δ and we can express L˜ as:
L˜ = L(I + Σ)

We will show that there exists a Σ˜ , s.t. L−n := L−1 ◦ L−1 ◦ · · · L−1 and show that
n times

Σ˜ ≤ n2δ and L˜n = (I + Σ˜ )Ln. Towards that, we will denote

L−nL˜n ≤ 1 + n2δ

(35)

We have:

L−n L˜ n

= L−n (L(I + Σ))n





n

= L−n Ln + Lj−1 ◦ (L ◦ Σ) ◦ Ln−j + · · · + (L ◦ Σ)n

j=1

n
= I + L−n ◦ Lj−1 ◦ Σ ◦ Ln−j + · · · + L−n ◦ (L ◦ Σ)(n)
j=1

(1)
≤ 1+

n
L−n ◦ Lj−1 ◦ Σ ◦ Ln−j
j=1

(2)

n

≤ 1+

i=1

n δi i

= (1 + δ)n

(3)
≤ enδ

≤ 1 + 2nδ

+···+

L−n ◦ (L ◦ Σ)n

23

where (1) follows from triangle inequality, (2) follows from Lemma D.5, (3) follows from 1 + x ≤ ex, and the last part follows from nδ ≤ 1/10 and Taylor expanding ex. Next, since L and L˜ are elliptic operators, we have L−nL˜n = L˜nL−n . From this, it immediately follows that there exists a Σ˜ , s.t. L˜n = (I + Σ˜ )Ln with Σ˜ ≤ n2δ.
Plugging this into the ﬁrst term of (34), we have

L˜n(f˜span − fspan) L2(Ω) = = ≤ ≤ ≤

L˜nf˜span − L˜nfspan L2(Ω) L˜nf˜span − (I + Σ˜ )Lnfspan L2(Ω) L˜nf˜span − Lnfspan L2(Ω) + Σ˜ Lnfspan L2(Ω) L˜nf˜span − Lnfspan L2(Ω) + Σ˜ Lnfspan L2(Ω) L˜nf˜span − Lnfspan L2(Ω) + n2δλnk fspan L2(Ω)

(36)

The ﬁrst term in ﬁrst term in (36) can be expanded as follows:

L˜nf˜span − Lnfspan L2(Ω) = ≤

L˜nf˜span − Lnf˜span + Lnf˜span + Lnfspan L2(Ω) L˜nf˜span − Lnf˜span + Lnf˜span − Lnfspan L2(Ω)

(37)

We’ll consider the two terms in turn. For the ﬁrst term, the same proof as that of (35) shows that there exists an operator Σˆ , s.t.
and Ln = (I + Σˆ )L˜n. Hence, we have:

Σˆ ≤ 2nδ

L˜nf˜span − Lnf˜span

= L˜nf˜span − (I + Σˆ )L˜nf˜span = Σˆ L˜nf˜span ≤ λ˜nk Σˆ f˜span L2(Ω) ≤ 2nδλ˜nk f L2(Ω) (∵ f˜span

L2(Ω) ≤

f L2(Ω))

(38)

For the second term in (36) we have:

Ln(f˜span − fspan) L2(Ω) ≤

sup

v:v=v1−v2,v1∈Φk ,v2∈Φ˜ k

Lnv L2(Ω) v L2(Ω)

f˜span − fspan

L2(Ω)

(39)

To bound the ﬁrst factor we have:

Lnv

L2(Ω) = Ln(v1 − v2) L2(Ω) ≤ Lnv1 L2(Ω) + Lnv2 L2(Ω) = Lnv1 L2(Ω) + (I + Σˆ )L˜nv2 L2(Ω) ≤ λnk v1 L2(Ω) + λ˜nk I + Σˆ 2 v2 L2(Ω) ≤ (λnk + λ˜nk (1 + 2nδ)) v L2(Ω)

where we use the fact that v1 L2(Ω), v2 L2(Ω) ≤ v L2(Ω) and Σˆ ≤ 2nδ. Hence, we can bound

sup
v:v=v1−v2,v1∈Φk ,v2∈Φ˜ k

Lnv L2(Ω) ≤ (λn + λ˜n(1 + 2nδ))

v L2(Ω)

k

k

(40)

From (40) and Lemma 4 we have:

Ln(f˜span − fspan)

L2(Ω) ≤

sup

v:v=v1−v2,v1∈Φk ,v2∈Φ˜ k

Lnv L2(Ω) v L2(Ω)

f˜span − fspan

≤ (λnk + λ˜nk (1 + 2nδ)) γ −δ δ f L2(Ω)

L2(Ω)

(41)

24

Therefore from (38) and (41), we can upper bound L˜n(f˜span − fspan) L2(Ω) using (36) as follows:

L˜n(f˜span − fspan)

L2(Ω) ≤ L˜nf˜span − Lnfspan L2(Ω) + 2nδλnk f L2(Ω)

≤ 2nδλ˜nk f L2(Ω) + (λnk + λ˜nk (1 + 2nδ)) γ −δ δ f L2(Ω) + 2nδλnk f

(≤i) (1 + 2nδλk) (1 + (1 + 2nδ)) γ −δ δ λnk f L2(Ω) + 2nδλnk f L2(Ω)

(ii)
≤4

1+ δ γ−δ

λnk f L2(Ω)

L2(Ω)

Here in (i) we use the result from Lemma D.1 and write λλ˜nknk ≤ 1 + 2nδλk. In (iii), we use n ≤ T and the fact that 2T min(1, λk)δ ≤ 1/10 ≤ 1
Therefore, ﬁnally we have:

L˜nf˜span − Lnfspan L2(Ω) ≤ 4

δ +1 γ−δ

λnk f L2(Ω)

Combining with the result for Part 1, Therefore we have the following:

L˜n(f˜span − fnn) L2(Ω) ≤ (n!)2 · Cn(ǫspan + ǫnn) + 4

1+ δ γ−δ

λnk f L2(Ω)

D Technical Lemmas: Manipulating Operators

Before we state the lemmas we introduce some common notation used throughout this section. We denote Ln = L ◦ L ◦ · · · ◦ L. Further we use Lk to denote the operator with ∂kaij for all i, j ∈ [d] and ∂kc as

n times
coeﬃcients, that is:

d

d

Lku =

− (∂kaij ) ∂ij u −

∂k (∂iai) ∂ju + (∂kc)u

i,j=1

i,j=1

Similarly the operator Lkl is deﬁned as:

d

d

Lklu =

− (∂klaij ) ∂ij u −

∂kl (∂iai) ∂j u + (∂klc)u

i,j=1

i,j=1

Lemma D.1. Given ϕi and ϕ˜i for all i ∈ [k] are top k eigenvalues of operators L and L˜ respectively, such that L−1 − L˜−1 is bounded. Then for all n ∈ N we have that
λ˜ni ≤ (1 + eˆ)λni

where i ∈ [k] and |eˆ| ≤ 2nδλk and δ = max

ǫA m

,

ǫc ζ

.

Proof. From (28) and Weyl’s inequality we have for all i ∈ N

sup 1 − 1 ≤ L−1 − L˜−1 ≤ δ i λi λ˜i

25

From this, we can conclude that:

λ˜i − λi ≤ δλiλ˜i
=⇒ λ˜i(1 − δλi) ≤ λi =⇒ λ˜i ≤ λi
(1 − δλi) =⇒ λ˜i ≤ (1 + δλi)λi

Writing λ˜i = (1 + e˜i)λi (where e˜i = δλi), we have

λ˜ni − λni = |((1 + e˜i)λi)n − λni |

= |λni ((1 + e˜i)n − 1)|

(1)

n

≤ λni |e˜|i (1 + e˜i)j

j=1

(2)
≤ λni n|e˜i|en|e˜i|
(3)
≤ λni n|e˜i|(1 + |2ne˜i|) ≤ 2λni n|e˜i|

where (1) follows from the factorization an − bn = (a − b)(

n−1 i=0

aibn−i−i),

(2)

follows

from

1

+

x

≤

ex,

and

(3) follows from n|e˜i| ≤ 1/20 and Taylor expanding ex. Hence, there exists a eˆi, s.t. λ˜ni = (1 + eˆi)λni and

|eˆi| ≤ 2n|e˜i| (i.e., |eˆi| ≤ 2nδλi). Using the fact that λi ≤ λk for all i ∈ [k] completes the proof.

Lemma D.2 (Operator Chain Rule). Given an elliptic operator L, for all v ∈ C∞(Ω) we have the following

n

∇kLnu =

Ln−i ◦ Lk ◦ Li−1 (u) + Ln(∇ku)

i=1

(42)

∇kl(Lnu) =
i,j i<j
+
i,j i>j
+
i

Ln−i ◦ Lk ◦ Lj−i−1 ◦ Ll ◦ Lj−1 u Ln−j ◦ Lk ◦ Li−j−1 ◦ Ll ◦ Li−1 u Ln−i ◦ Lkl ◦ Li−1 u + Ln(∇klu)

where we assume that L(0) = I.

(43)

26

Proof. We show the proof using induction on n. To handle the base case, for n = 1, we have

∇k(Lu) = ∇k (−div(A∇u) + cu)





= ∇k − aij ∂ij u − ∂iaij ∂j u + cu

ij

ij





= − aij ∂ij (∂ku) − ∂iaij ∂j ∂ku + c∂ku

ij

ij





+ − ∂kaij ∂ij u − ∂i∂kaij ∂j u + ∂kcu

ij

ij

= L(∇ku) + Lku

(44)

Similarly n = 1 and k, l ∈ [d],

∇kl(Lu) = ∇kl (−div(A∇u) + cu)





= ∇kl − aij ∂ij u − ∂iaij ∂j u + cu

ij

ij





= − aij ∂ij (∂klu) − ∂iaij ∂j ∂klu + c∂klu

ij

ij





+ − ∂kaij ∂ij ∂lu − ∂i∂kaij ∂j ∂lu + ∂kc∂lu

ij

ij





+ − ∂laij ∂ij ∂ku − ∂i∂laij ∂j ∂ku + ∂lc∂ku

ij

ij





+ − ∂klaij ∂ij u − ∂i∂klaij ∂j u + ∂klcu

ij

ij

= L(∇klu) + Lk(∇lu) + Ll(∇ku) + Lklu

(45)

For the inductive case, assume that for all m < n, (42) and (43) hold. Then, for any k ∈ [d] we have:

∇k(Lnu) = ∇k L ◦ Ln−1(u)

= L ∇k(Ln−1u) + Lk Ln−1u

n−1

=L

Ln−1−i ◦ Lk ◦ Li−1 u + Ln−1(∇ku)

i=1

n

=

Ln−i ◦ Lk ◦ Li−1 (u) + Ln(∇ku)

i=1

+ Lk Ln−1 u

(46)

27

Similarly, for all k, l ∈ [d] we have:

∇kl(Lnu) = ∇kl L ◦ Ln−1(u)

= L ∇kl(Ln−1u) + Lk ∇l Ln−1u + Ll ∇k Ln−1u + Lkl Ln−1u

n−1

=L

Ln−1−i ◦ Lk ◦ Lj−i−1 ◦ Ll ◦ Lj−1 u

i,j i<j

n−1

+

Ln−1−j ◦ Lk ◦ Li−j−1 ◦ Ll ◦ Li−1 u

i,j i>j

n−1

+

Ln−1−i ◦ Lkl ◦ Li−1 u + Ln−1(∇klu)

i=1

n−1

+ Lk

Ln−1−i ◦ Ll ◦ Li−1 (u) + Ln−1(∇lu)

(from (46))

i=1

n−1

+ Ll

Ln−1−i ◦ Lk ◦ Li−1 (u) + Ln−1(∇ku)

(from (46))

i=1

+ Lkl Ln−1u

n
=

Ln−i ◦ Lk ◦ Lj−i−1 ◦ Ll ◦ Lj−1 u

i,j i<j

n
+

Ln−j ◦ Lk ◦ Li−j−1 ◦ Ll ◦ Li−1 u

i,j i>j

n
+

Ln−i ◦ Lkl ◦ Li−1 u + Ln(∇klu)

i

By induction, the claim follows.

Lemma D.3. For all u ∈ C∞(Ω) then for all k, l ∈ [d] the following upper bounds hold,

Lu L2(Ω) ≤ C max ∂αu L2(Ω)
α:|α|≤2

and where

∇k(Lu) L2(Ω) ≤ 2 · C max ∂αu L2(Ω)
α:|α|≤3
∇kl(Lu) L2(Ω) ≤ 4 · C max ∂αu L2(Ω)
α:|α|≤4

C := (2d2 + 1) max max max ∂αaij L∞(Ω), max ∂αc L∞(Ω) .

α:|α|≤3 i,j

α:|α|≤2

(47)
(48) (49) (50)

28

Proof. We ﬁrst show the upper bound on Lu L2(Ω):

Lu L2(Ω) ≤

d

d

−

aij ∂ij u −

∂iaij ∂j u + cu

i,j=1

i,j=1

L2(Ω)

≤(1) (2d2 + 1) max max ∂iaij L∞(Ω), max aij L∞(Ω), c L∞(Ω)

i,j

i,j

C1
≤ C1 max ∂αu L2(Ω)
α:|α|≤2

max ∂αu L2(Ω)
α:|α|≤2

(51)

where (1) follows by Ho¨lder. Proceeding to ∇k(Lu) L2(Ω), from Lemma D.4 we have

∇k(Lu) L2(Ω) ≤ ≤
+

Lk u L2(Ω) + L(∇ku) L2(Ω)

d

d

−

∂kaij ∂ij u −

∂ikaij ∂j u + ∂kcu

i,j=1

i,j=1

L2 (Ω)

d

d

−

aij ∂ijku −

∂iaij ∂jku + c∂ku

i,j=1

i,j=1

L2 (Ω)

≤ (2d2 + 1) max max max ∂αaij L∞(Ω), ∂kc L∞(Ω) α:|α|≤2 i,j

max ∂αu L2(Ω)
α:|α|≤2

+ (2d2 + 1) max max max ∂αaij L∞(Ω), c L∞(Ω) max ∂αu L2(Ω)

α:|α|≤1 i,j

α:|α|≤3

=⇒

∇k(Lu) L2(Ω) ≤ 2 · (2d2 + 1) max

max max ∂αaij L∞(Ω), max ∂αc L∞(Ω)

α:|α|≤2 i,j

α:|α|≤1

max ∂αu L2(Ω)
α:|α|≤3

C2
≤ 2 · C2 max ∂αu L2(Ω) α:|α|≤3

(52)

29

We use the result from Lemma D.2 (equation (45)), to upper bound the quantity ∇kl(Lu) L2(Ω)

=⇒

∇kl(Lu) L2(Ω) ≤ ≤ + + +

Lklu L2(Ω) + Lk(∇lu) L2(Ω) + Ll(∇ku) L2(Ω) + L(∇klu) L2(Ω)

d

d

−

∂klaij ∂ij u −

∂iklaij ∂j u + ∂klcu

i,j=1

i,j=1

L2 (Ω)

d

d

−

∂kaij ∂ij ∂lu −

∂i∂kaij ∂j ∂lu + ∂kc∂lu

i,j=1

i,j=1

L2 (Ω)

d

d

−

∂laij ∂ij ∂ku −

∂i∂laij ∂j ∂ku + ∂lc∂ku

i,j=1

i,j=1

L2 (Ω)

d

d

−

aij ∂ijklu −

∂iaij ∂jklu + c∂klu

i,j=1

i,j=1

L2 (Ω)

≤ (2d2 + 1) max max max ∂αaij L∞(Ω), ∂klc L∞(Ω) max ∂αu L2(Ω)

α:|α|≤3 i,j

α:|α|≤2

+ 2(2d2 + 1) max max max ∂αaij L∞(Ω), c L∞(Ω) max ∂αu L2(Ω)

α:|α|≤2 i,j

α:|α|≤3

+ (2d2 + 1) max max max ∂αaij L∞(Ω), c L∞(Ω) max ∂αu L2(Ω)

α:|α|≤2 i,j

α:|α|≤4

∇kl(Lu) L2(Ω) ≤ 4 · (2d2 + 1) max max max ∂αaij L∞(Ω), max ∂αc L∞(Ω) max ∂αu L2(Ω)

α:|α|≤3 i,j

α:|α|≤2

α:|α|≤4

C3
≤ 4 · C3 max ∂αu L2(Ω) α:|α|≤4

(53)

Since C1 ≤ C2 ≤ C3, we deﬁne C := C3 and therefore from equations (51), (52) and (53) the claim follows. Further, we note that from (52), we also have that

Lk(u) L2(Ω), L(∇ku) L2(Ω) ≤ C max ∂αu L2(Ω)
α:|α|≤3

(54)

and similarly from (53) we have that, Lkl(u) L2(Ω), Lk(∇lu) L2(Ω), Ll(∇ku) L2(Ω), L(∇klu) L2(Ω) ≤ C max ∂αu L2(Ω)
α:|α|≤4

(55)

Lemma D.4. For all u ∈ C∞(Ω) and k, l ∈ [d] then for all n ∈ N we have the following upper bounds,

Lnu L2(Ω) ≤ (n!)2 · Cn max ∂αu L2(Ω)
α:|α|≤n+2

(56)

∇k(Lnu) L2(Ω) ≤ (n + 1) · (n!)2 · Cn max ∂αu L2(Ω)
α:|α|≤n+2

(57)

∇kl(Lnu) L2(Ω) ≤ ((n + 1)!)2 · Cn max ∂αu L2(Ω)
α:|α|≤n+3

(58)

where C = (2d2 + 1) max maxα:|α|≤3 maxi,j ∂αaij L∞(Ω), maxα:|α|≤2 ∂αc L∞(Ω) .

Proof. We prove the Lemma by induction on n. The base case n = 1 follows from Lemma D.3, along with the fact that maxα:|α|≤2 ∂αu L2(Ω) ≤ maxα:|α|≤3 ∂αu L2(Ω).

30

To show the inductive case, assume that the claim holds for all m ≤ (n − 1). By Lemma D.3, we have

Lnu L2(Ω) = ≤

L(Ln−1u) L2(Ω)

d

d

−

aij ∂ij (Ln−1u) −

∂iaij ∂j (Ln−1u) + c(Ln−1u)

i,j=1

i,j=1

L2(Ω)

≤ C · max

Ln−1u L2(Ω), max ∇i(Ln−1u)
i

≤ C · (n!)2 · Cn−1 max

∂αu L2(Ω)

α:|α|≤(n−1)+3

L2(Ω), max
i,j

∇ij (Ln−1u)

L2(Ω)

Thus, we have

Lnu L2(Ω) ≤ (n!)2 · Cn max ∂αu L2(Ω)
α:|α|≤n+2

as we need. Similarly, for k ∈ [d], we have:

∇k (Ln u)

L2(Ω)

n

≤
i=1

Ln−i ◦ Lk ◦ Li−1 (u) L2(Ω) + Ln(∇ku) L2(Ω)

≤ (n) · (n!)2 · Cn max ∂αu L2(Ω) + (n!)2 · Cn max

α:|α|≤n+2

α:|α|≤n+2

≤ (n + 1) · (n!)2 · Cn max ∂αu L2(Ω)
α:|α|≤n+2

∂αu

L2(Ω)

(59)

Finally, for k, l ∈ [d] we have

∇kl(Lnu) L2(Ω) ≤
i,j i<j

Ln−i ◦ Lk ◦ Lj−i−1 ◦ Ll ◦ Lj−1 u L2(Ω)

+
i,j i>j

Ln−j ◦ Lk ◦ Li−j−1 ◦ Ll ◦ Li−1 u L2(Ω)

+

Ln−i ◦ Lkl ◦ Li−1 u L2(Ω) +

i

≤ n(n + 1) · (n!)2 · Cn max ∂αu
α:|α|≤n+2

Ln(∇klu)
L2(Ω)

L2(Ω)

+ n · (n!)2 · Cn max ∂αu L2(Ω) + Cn max ∂αu L2(Ω)

α:|α|≤n+2

α:|α|≤n+3

=⇒ ∇kl(Lnu) L2(Ω) ≤ ((n + 1)!)2 · Cn max ∂αu L2(Ω)
α:|α|≤n+3

(60)

Thus, the claim follows.

Lemma D.5. Let Ain, i ∈ [n] be deﬁned as a composition of (n − i) applications of L and i applications of L ◦ Σ (in any order), s.t. Σ ≤ δ. Then, we have:

L−nAin ≤ δi

(61)

Proof. We prove the above claim by induction on n. For n = 1 we have two cases. If A(1) = L ◦ Σ, we have:

L−1 ◦ L ◦ Σ ≤ δ

31

If A(1) = L we have:

L−1L = 1

Towards the inductive hypothesis, assume that for m ≤ n − 1 and i ∈ [n − 1] it holds that,

Ln−1Ain−1 ≤ δi
For n, we will have two cases. First, if Ain+1 = Ain−1 ◦ L ◦ Σ, by submultiplicativity of the operator norm, as well as the fact that similar operators have identical spectra (hence equal operator norm) we have:

L−n ◦ Ain+1

= L−1 ◦ L−(n−1) ◦ An(i−) 1 ◦ L ◦ Σ = L−(n−1) ◦ Ain−1 ◦ L ◦ Σ ◦ L−1 ≤ δ L−(n−1)Ai(−n−1 1) L ◦ Σ ◦ L−1 ≤ δiδ = δi+1

so the inductive claim is proved. In the second case, Ain = Ain−1L and we have, by using the fact that the similar operators have identical spectra:

L−n ◦ Ain ◦ L = L−(n−1) ◦ Ain−1 ◦ L ◦ L−1 = L−(n−1) ◦ Ain−1 ≤ δi

where the last inequality follows by the inductive hypothesis.

32

