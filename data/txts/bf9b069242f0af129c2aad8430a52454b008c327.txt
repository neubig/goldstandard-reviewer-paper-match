arXiv:2004.06977v1 [cs.LG] 15 Apr 2020

On Learning Rates and Schr¨odinger Operators

Bin Shi∗

Weijie J. Su†

Michael I. Jordan‡

April 16, 2020

Abstract
The learning rate is perhaps the single most important parameter in the training of neural networks and, more broadly, in stochastic (nonconvex) optimization. Accordingly, there are numerous eﬀective, but poorly understood, techniques for tuning the learning rate, including learning rate decay, which starts with a large initial learning rate that is gradually decreased. In this paper, we present a general theoretical analysis of the eﬀect of the learning rate in stochastic gradient descent (SGD). Our analysis is based on the use of a learning-rate-dependent stochastic diﬀerential equation (lr-dependent SDE) that serves as a surrogate for SGD. For a broad class of objective functions, we establish a linear rate of convergence for this continuous-time formulation of SGD, highlighting the fundamental importance of the learning rate in SGD, and contrasting to gradient descent and stochastic gradient Langevin dynamics. Moreover, we obtain an explicit expression for the optimal linear rate by analyzing the spectrum of the Witten-Laplacian, a special case of the Schr¨odinger operator associated with the lr-dependent SDE. Strikingly, this expression clearly reveals the dependence of the linear convergence rate on the learning rate— the linear rate decreases rapidly to zero as the learning rate tends to zero for a broad class of nonconvex functions, whereas it stays constant for strongly convex functions. Based on this sharp distinction between nonconvex and convex problems, we provide a mathematical interpretation of the beneﬁts of using learning rate decay for nonconvex optimization.
1 Introduction
Gradient-based optimization has been the workhorse algorithm powering recent developments in statistical machine learning. Many of these developments involve solving nonconvex optimization problems, which raises new challenges for theoreticians, given that classical theory has often been restricted to the convex setting.
A particular focus in machine learning is the class of gradient-based methods referred to as stochastic gradient descent (SGD), given its desirable runtime properties, and its desirable statistical performance in a wide range of nonconvex problems. Consider the minimization of a (nonconvex) function f deﬁned in terms of an expectation:
f (x) = Eζ f (x; ζ),
∗University of California, Berkeley. Email: binshi@berkeley.edu. †The Wharton School, University of Pennsylvania. Email: suw@wharton.upenn.edu. ‡University of California, Berkeley. Email: jordan@cs.berkeley.edu.

1

where the expectation is over the randomness embodied in ζ. A simple example of this is empirical risk minimization, where the loss function,
1n f (x) = n fi(x),
i=1

is averaged over n data points, where the datapoint-speciﬁc losses, fi(x), are indexed by i and

where x denotes a parameter. When n is large, it is computationally prohibitive to compute the

full gradient of the objective function, and SGD provides a compelling alternative. SGD is a

gradient-based update based on a (noisy) gradient evaluated from a single data point or a mini-

batch:

1 ∇f (x) := B ∇fi(x) = ∇f (x) + ξ,
i∈B

where the set B of size B is sampled uniformly from the n data points and therefore the noise term ξ has mean zero. Starting from an initial point x0, SGD updates the iterates according to

xk+1 = xk − s∇f (xk) = xk − s∇f (xk) − sξk,

(1.1)

where ξk denotes the noise term at the kth iteration. Note that the step size s > 0, also known as the learning rate, can either be constant or vary with the iteration [Bot10].

0.65

0.55

0.45

0.35

0.25

0.15 0

100

200

300

400

Figure 1: Training error using SGD with mini-batch size 32 to train an 8-layer convolutional neural network on CIFAR-10 [Kri09]. The ﬁrst 90 epochs use a learning rate of s = 0.006, the next 120 epochs use s = 0.003, and the ﬁnal 190 epochs use s = 0.0005. Note that the training error decreases as the learning rate s decreases and a smaller s leads to a larger number of epochs for SGD to reach a plateau. See [HZRS16] for further investigation of this phenomenon.
The learning rate plays an essential role in determining the performance of SGD and many of the practical variants of SGD [Ben12].1 The overall eﬀect of the learning rate can be complex. In convex optimization problems, theoretical analysis can explain many aspects of this complexity,
1Note that the mini-batch size as another parameter can be, to some extent, incorporated into the learning rate. See discussion later in this section.

2

but in the nonconvex setting the eﬀect of the learning rate is yet more complex and theory is lacking [Zei12, KB14]. As a numerical illustration of this complexity, Figure 1 plots the error of SGD with a piecewise constant learning rate in the training of a neural network on the CIFAR10 dataset. With a constant learning rate, SGD quickly reaches a plateau in terms of training error, and whenever the learning rate decreases, the plateau decreases as well, thereby yielding better optimization performance. This illustration exempliﬁes the idea of learning rate decay, a technique that is used in training deep neural networks (see, e.g., [HZRS16, BCN18, SS19]). Despite its popularity and the empirical evidence of its success, however, the literature stops short of providing a general and quantitative approach to understanding how the learning rate impacts the performance of SGD and its variants in the nonconvex setting [YLWJ19, LWM19]. Accordingly, strategies for setting learning rate decay schedules are generally adhoc and empirical.
In the current paper we provide theoretical insight into the dependence of SGD on the learning rate in nonconvex optimization. Our approach builds on a recent line of work in which optimization algorithms are studied via the analysis of their behavior in continuous-time limits [SBC16, Jor18, SDJS18]. Speciﬁcally, in the case of SGD, we study stochastic diﬀerential equations (SDEs) as surrogates for discrete stochastic optimization methods (see, e.g., [KY03, LTE17, KB17, COO+18, DJ19]). The construction is roughly as follows. Taking a small but nonzero learning rate s, let tk = ks denote a time step and deﬁne xk = Xs(tk) for some suﬃciently smooth curve Xs(t). Applying a Taylor expansion in powers of s, we obtain:

xk+1 = Xs(tk+1) = Xs(tk) + X˙s(tk)s + O(s2).

Let W be a standard Brownian motion and, for the time being, assume that the noise term ξk is approximately normally distributed with unit variance. Informally, this leads to2

√ − sξk

=

W (tk+1)

−

W (tk)

=

s

dW (tk)

+

O(s2).

dt

Plugging the last two displays into (1.1), we get

X˙s(tk) + O(s) = −∇f (Xs(tk)) + √s dW (tk) + O

3
s2

.

dt

Retaining both O(1) and O(√s) terms but ignoring smaller terms, we obtain a learning-rate-

dependent stochastic diﬀerential equation (lr-dependent SDE) that approximates the discrete-time

SGD algorithm:

√ dXs = −∇f (Xs)dt + sdW,

(1.2)

where the initial condition is the same value x0 as its discrete counterpart. This SDE has been shown to be a valid approximating surrogate for SGD in earlier work [KY03, CS18]. As an indication
of the generality of this formulation, we note that it can seamlessly take account of the mini-batch
size B; in particular, the eﬀective learning rate scales as O(s/B) in the mini-batch setting (see
more discussion in [SKYL17]). Throughout this paper we focus on (1.2) and regard s alone as the eﬀective learning rate.3

2Although a Brownian motion is not diﬀerentiable, the formal notation dW (t)/dt can be given a rigorous inter-
pretation [Eva12, Vil06]. 3Recognizing that the variance of ξk is inversely proportional to the mini-batch size B, we assume that the noise
term ξk has variance σ2/B. Under this assumption the resulting SDE reads dXs = −∇f (Xs)dt + σ s/BdW . In light of this, the eﬀective learning rate through incorporating the mini-batch size is O(σ2s/B).

3

101

101

100

100

10-1

10-1

10-2

10-2

10-3 0

10-3

250

500

750

1000

0

25

50

75

100

Figure 2: Illustrative examples showing distinct behaviors of GD, SGD, and SGLD. The y-axis displays the optimization error f (xk) − f (x⋆), where f (x⋆) denotes the minimum value of the objective and in the case of SGD and SGLD f (xk) denotes an average over 1000 replications. The objective function is f (x1, x2) = 5 × 10−2x21 + 2.5 × 10−2x22, with an initial point (8, 8), and the noise ξk in the gradient follows a standard normal distribution. Note that SGD with s = 1 is identical to SGLD with s = 1. As shown in the
right panel, taking time t = ks as the x-axis, the learning rate has little to no impact on GD and SGLD in
terms of optimization error.

Intuitively, a larger learning rate s gives rise to more stochasticity in the lr-dependent SDE (1.2), and vice versa. Accordingly, the learning rate must have a substantial impact on the dynamics of SGD in its continuous-time formulation. In stark contrast, this parameter plays a fundamentally diﬀerent role in gradient descent (GD) and stochastic gradient Langevin dynamics (SGLD) when one considers their limiting diﬀerential equations. In particular, consider GD:

xk+1 = xk − s∇f (xk),

which can be modeled by the following ordinary diﬀerential equation (ODE): X˙ = −∇f (X),

and the SGLD algorithm, which adds Gaussian noise ξk to the GD iterates: √
xk+1 = xk − s∇f (xk) + sξk,

and its SDE model:

dX = −∇f (X)dt + dW.

TanhdesreetdaiiﬀneirnegntOia(l1)eqaunadtioOn(s√asr)e tdeerrmivse.4d iWn hthilee stahme eSDwEayfoars (m1o.2d)e,linnagmSeGlyDbysetthsethTeayslqouraerxepraonostioonf the learning rate to be its diﬀusion coeﬃcient, both the GD and SGLD counterparts are completely free of this parameter. This distinction between SGD and the other two methods is reﬂected in their diﬀerent numerical performance as revealed in Figure 2. The right plot of this ﬁgure shows that the behaviors of both GD and SGLD in the time t = ks scale are almost invariant in terms of optimization error with respect to the learning rate. In striking contrast, the stationary optimization error of SGD decreases signiﬁcantly as the learning rate decays. As a consequence of this distinction, GD and SGLD do not exhibit the phenomenon that is shown in Figure 1.
4The coeﬃcients of the O(√s) terms turn out to be zero in both diﬀerential equations. See more discussion in Appendix A.1 and particularly Figure 12 therein.

4

1.1 Overview of contributions
The discussion thus far suggests that one may examine the eﬀect of the learning rate in SGD using the lr-dependent SDE (1.2). In particular, this SDE distinguishes SGD from GD and SGLD. Accordingly, in the current paper we study the lr-dependent SDE, and make the following contributions.

1. Linear convergence to stationarity. We show that, for a large class of (nonconvex)

objectives, the continuous-time formulation of SGD converges to its stationary distribution

at a linear rate.5 In particular, we prove that the solution Xs(t) to the lr-dependent SDE

obeys

E f (Xs(t)) − f ⋆ ≤ ǫ(s) + C(s)e−λst,

(1.3)

where f ⋆ denotes the global minimum of the objective function f , ǫ(s) denotes the risk at stationarity, and C(s) depends on both the learning rate and the distribution of the initial x0. Notably, we can show that ǫ(s) decreases monotonically to zero as s → 0. This bound can be carried over to the discrete case by a uniform approximation between SGD and the lr-dependent SDE (1.2). Speciﬁcally, the term C(s)e−λst becomes C(s)e−λsks, showing that the convergence is linear as well in the discrete regime. This is consistent with the numerical evidence from Figure 1 and Figure 2.

This convergence result sheds light on why SGD performs so well in many practical nonconvex problems. In particular, note that while GD can be trapped in a saddle point or a local minimum, SGD can eﬃciently escape saddle points, provided that the linear rate λs is not too small (this is the case if s is suﬃciently large; see the second contribution). This superiority of SGD in the nonconvex setting must be attributed to the noise in the gradient and this implication is consistent with earlier work showing that stochasticity in gradients signiﬁcantly accelerates the escape of saddle points for gradient-based methods [JGN+17, LSJR16].

2. Distinctions between convexity and nonconvexity. The ﬁrst contribution stops short

of saying anything about how λs depends on the learning rate s and the geometry of the

objective f . Such an analysis is fundamental to an explanation of the diﬀering eﬀects of

the learning rate in deep learning (nonconvex optimization) and convex optimization. In the

current paper we show that if the objective f is a nonconvex function and satisﬁes certain

regularity conditions, we have:6

λs

≍

e−

2H s

f

,

(1.4)

for a certain value Hf > 0 that only depends on f . This expression for λs enables a concrete interpretation of the eﬀect of learning rate in Figure 1. In brief, in the nonconvex setting, λs decreases to zero quickly as the learning rate s tends to zero. As a consequence, with a large learning rate s at the beginning, SGD converges rapidly to stationarity and the rate becomes smaller as the learning rate decreases.

For comparison, λs is equal to µ if f is µ-strongly convex for µ > 0, regardless of the learning rate s. As such, the convergence behaviors of SGD are necessarily diﬀerent between convex and nonconvex objectives. To appreciate this implication, we refer to Figure 3. Note that all

5Roughly speaking, stationarity refers to the distribution of Xs(t) in the limit t → ∞. See a more precise deﬁnition
in Section 3. 6We write am ≍ bm if there exist positive constants c and c′ such that cbm ≤ am ≤ c′bm for all m.

5

four plots show that a larger learning rate gives rise to a larger stationary risk, as predicted by the monotonically increasing nature of ǫ with respect to s in (1.3). The most salient part of this ﬁgure is, however, shown in the right panel. Speciﬁcally, the right panel, which uses time t as the x-axis, shows that in the (strongly) convex setting the linear rate of the convergence is roughly the same between the two choices of learning rate, which is consistent with the result that λs is constant in the case of a strongly convex objective. In the nonconvex case (bottom right), however, the rate of convergence is more rapid with the larger learning rate s = 0.1, which is implied by the fact that λ0.1 > λ0.05. In stark contrast, the two plots in the left panel, which use the number k of iterations for the x-axis, are observed to have a larger rate of linear convergence with a larger learning rate. This is because in the k scale the rate λss of linear convergence always increases as s increases no matter if the objective is convex or nonconvex.
The mathematical tools that we bring to bear in analyzing the lr-dependent SDE (1.2) are as follows. We establish the linear convergence via a Poincar´e-type inequality that is due to Villani [Vil09]. The asymptotic expression for the rate λs is proved by making use of the spectral theory of the Schro¨dinger operator or, more concretely, the Witten-Laplacian associated with the Fokker–Planck–Smoluchowski equation that governs the lr-dependent SDE. We believe that these tools will prove to be useful in theoretical analyses of other stochastic approximation methods.
1.2 Related work
Recent years have witnessed a surge of research devoted to explanations of the eﬀectiveness of deep neural networks, with a particular focus on understanding how the learning rate aﬀects the behavior of stochastic optimization. In [SKYL17, KMN+16], the authors uncovered various tradeoﬀs linking the learning rate and the mini-batch size. Moreover, [JKA+17, JKB+18] related the learning rate to the generalization performance of neural networks in the early phase of training. This connection has been further strengthened by the demonstration that learning rate decay encourages SGD to learn features of increasing complexity [LWM19, YLWJ19]. From a topological perspective, [DDC19] establish connections between the learning rate and the sharpness of local minima. Empirically, deep learning models work well with non-decaying schedules such as cyclical learning rates [LH16, Smi17] (see also the review [Sun19]), with recent theoretical justiﬁcation [LA19].
In a diﬀerent direction, there has been a ﬂurry of activity in using dynamical systems to analyze discrete optimization methods. For example, [SBC16, WWJ16, SDJS18] derived ODEs for modeling Nesterov’s accelerated gradient methods and used the ODEs to understand the acceleration phenomenon (see the review [Jor18]). In the stochastic setting, this approach has been recently pursued by various authors [COO+18, CS18, MHB16, LSJR16, CH19, LTE17] to establish various properties of stochastic optimization. As a notable advantage, the continuous-time perspective allows us to work without assumptions on the boundedness of the domain and gradients, as opposed to older analyses of SGD (see, for example, [HRB08]).
Our work is motivated in part by the recent progress on Langevin dynamics, in particular in nonconvex settings [Vil09, Pav14, HKN04, BGK05]. In relating to Langevin dynamics, s in the lr-dependent SDE can be thought of as the temperature parameter and, under certain conditions, this SDE has a stationary distribution given by the Gibbs measure, which is proportional to exp(−2f /s). Of particular relevance to the present paper from this perspective is a line of work that has considered the optimization properties of SGLD and analyzed its convergence
6

101 100 10-1 10-2
0 100

500

1000

1500

2000

101 100 10-1 10-2
0 100

25

50

75

100

10-1

10-1

10-2

0

10000

20000

30000

40000

50000

10-2 0

500

1000

1500

2000

2500

Figure 3: The dependence of the optimization dynamics of SGD on the learning rate diﬀers between convex
objectives and nonconvex objectives. The learning rate is set to either s = 0.1 or s = 0.05. The two top plots consider minimizing a convex function f (x1, x2) = 5 × 10−2x21 + 2.5 × 10−2x22, with an initial point (8, 8), and the bottom plots consider minimizing a nonconvex function f (x1, x2) = [(x1 + 0.7)2 + 0.1](x1 − 0.7)2 + (x2 + 0.7)2[(x2 − 0.7)2 + 0.1], with an initial point (−0.9, 0.9). The gradient noise is drawn from the
standard normal distribution. All results are averaged over 10000 independent replications.

rates [Hwa80, RRT17, ZLC17]. Compared to these results, however, the present paper is distinct in that our analysis provides a more concise and sharp delineation of the convergence rate based on geometric properties of the objective function.
1.3 Organization
The remainder of the paper is structured as follows. In Section 2 we introduce basic assumptions and techniques employed throughout this paper. Next, Section 3 develops our main theorems. In Section 4, we use the results of Section 3 to oﬀer insights into the beneﬁt of taking a larger initial learning rate followed by a sequence of decreasing learning rates in training neural networks. Section 5 formally proves the linear convergence (1.3) and Section 6 further speciﬁes the rate of convergence (1.4). Technical details of the proofs are deferred to the appendices. We conclude the paper in Section 7 with a few directions for future research.

7

2 Preliminaries

Throughout this paper, we assume that the objective function f is inﬁnitely diﬀerentiable in Rd; that is, f ∈ C∞(Rd). We use · to denote the standard Euclidean norm.

Deﬁnition 2.1 (Conﬁning condition [Pav14, MV99]). A function f is said to be conﬁning if it is

inﬁnitely diﬀerentiable and satisﬁes lim x →+∞ f (x) = +∞ and exp(−2f /s) is integrable for all

s > 0:

e−

2f

( s

x

)

dx

<

+∞.

Rd

This condition is quite mild and, indeed, it essentially requires that the function grows sufﬁciently rapidly when x is far from the origin. This condition is met, for example, when an ℓ2 regularization term is added to the objective function f or, equivalently, weight decay is employed in the SGD update.
Next, we need to show that the lr-dependent SDE (1.2) with an arbitrary learning rate s > 0 admits a unique global solution under mild conditions on the objective f . We will show in Section 3.3 that the solution to this SDE approximates the SGD iterates well. The formal description is shown rigorously in Proposition 3.5. Recall that the lr-dependent SDE (1.2) is
√ dXs = −∇f (Xs)dt + sdW,

where the initial point Xs(0) is distributed according to a probability density function ρ in Rd, independent of the standard Brownian motion W . It is well known that the probability density ρs(t, ·) of Xs(t) evolves according to the Fokker–Planck–Smoluchowski equation

∂ρs = ∇ · (ρs∇f ) + s ∆ρs,

∂t

2

(2.1)

with the boundary condition ρs(0, ·) = ρ. Here, ∆ ≡ ∇ · ∇ is the Laplacian. For completeness, in Appendix A.2 we derive this Fokker–Planck–Smoluchowski equation from the lr-dependent SDE (1.2) by Itˆo’s formula. If the objective f satisﬁes the conﬁning condition, then this equation admits a unique invariant Gibbs distribution that takes the form

µs =

1

e−

2f s

.

Zs

(2.2)

The proof of uniqueness is shown in Appendix A.3. The normalization factor is Zs = Rd e− 2sf dx.

Taking any initial probability density ρs(0, ·) ≡ ρ in L2(µ−s 1) (a measurable function g is said to

belong to L2(µ−s 1) if g µ−1 :=

Rd g2µ−s 1dx

1 2

< +∞), we have the following guarantee:

s

Lemma 2.2 (Existence and uniqueness of the weak solution). For any conﬁning function f and

any initial probability density ρ ∈ L2(µ−s 1), the lr-dependent SDE (1.2) admits a weak solution

whose probability density in C1

[0,

+

∞

),

L

2

(µ

− s

1

)

is the unique solution to the Fokker–Planck–

Smoluchowski equation (2.1).

The proof of Lemma 2.2 is shown in Appendix A.4. For more information, Lemma 5.2 in Section 5 shows that the probability density ρs(t, ·) converges to the Gibbs distribution as t → ∞.
Finally, we need a condition that is due to Villani for the development of our main results in the next section.

8

Deﬁnition 2.3 (Villani condition [Vil09]). A conﬁning function f is said to satisfy the Villani condition if ∇f (x) 2/s − ∆f (x) → +∞ as x → +∞ for all s > 0.
This condition amounts to saying that the gradient has a suﬃciently large squared norm compared with the Laplacian of the function. Strictly speaking, some loss functions used for training neural networks might not satisfy this condition. However, the Villani condition does not look as stringent as it appears since the SGD iterates in the training process are bounded and this condition is essentially concerned with the function at inﬁnity.

3 Main Results
In this section, we state our main results. In brief, in Section 3.1 we show linear convergence to stationarity for SGD in its continuous formulation, the lr-dependent SDE. In Section 3.2, we derive a quantitative expression of the rate of linear convergence and study the diﬀerence in the behavior of SGD in the convex and nonconvex settings. This distinction is further elaborated in Section 3.3 by carrying over the continuous-time convergence guarantees to the discrete case. Finally, Section 3.4 oﬀers an exposition of the theoretical results in the univariate case. Proofs of the results presented in this section are deferred to Section 5 and Section 6.

3.1 Linear convergence

In this subsection we are concerned with the expected excess risk, E f (Xs(t)) − f ⋆. Recall that f ⋆ = infx f (x).

Theorem 1. Let f satisfy both the conﬁning condition and the Villani condition. exists λs > 0 for any learning rate s > 0 such that the expected excess risk satisﬁes
E f (Xs(t)) − f ⋆ ≤ ǫ(s) + D(s)e−λst,

Then there (3.1)

for all t ≥ 0. Here ǫ(s) = ǫ(s; f ) ≥ 0 is a strictly increasing function of s depending only on the objective function f , and D(s) = D(s; f, ρ) ≥ 0 depends only on s, f , and the initial distribution ρ.

Brieﬂy, the proof of this theorem is based on the following decomposition of the excess risk:

E f (Xs(t)) − f ⋆ = E f (Xs(t)) − E f (Xs(∞)) + E f (Xs(∞)) − f ⋆,

where we informally use E f (Xs(∞)) to denote EX∼µs f (X) in light of the fact that Xs(t) converges weakly to µs as t → +∞ (see Lemma 5.2). The question is thus to quantify how fast E f (Xs(t)) − E f (Xs(∞)) vanishes to zero as t → ∞ and how the excess risk at stationarity E f (Xs(∞)) − f ⋆
depends on the learning rate. The following two propositions address these two questions. Recall that ρ ∈ L2(µ−s 1) is the probability density of the initial iterate in SGD.

Proposition 3.1. Under the assumptions of Theorem 1, there exists λs > 0 for any learning rate

s such that

|E f (Xs(t)) − E f (Xs(∞))| ≤ C(s) ρ − µs µ−1 e−λst, s

for all t ≥ 0, where the constant C(s) > 0 depends only on s and f , and where

ρ − µs µ−s 1 =

1
(ρ − µs)2 µ−s 1dx 2
Rd

measures the gap between the initialization and the stationary distribution.

9

Loosely speaking, it takes O(1/λs) time to converge to stationarity. In relating to Theorem 1,
D(s) can be set to C(s) ρ − µs µ−s 1. Notably, the proof of Proposition 3.1 shall reveal that C(s) increases as s increases.
Turning to the analysis of the second term, E f (Xs(∞)) − f ⋆, we write henceforth ǫ(s) := E f (Xs(∞)) − f ⋆.

Proposition 3.2. Under the assumptions of Theorem 1, the excess risk at stationarity, ǫ(s), is a strictly increasing function of s. Moreover, for any S > 0, there exists a constant A that depends only on S and f and satisﬁes ǫ(s) ≡ E f (Xs(∞)) − f ⋆ ≤ As,
for any learning rate 0 < s ≤ S.

The two propositions are proved in Section 5. The proof of Theorem 1 is a direct consequence of Proposition 3.1 and Proposition 3.2. More precisely, the two propositions taken together give

E f (Xs(t)) − f ⋆ ≤ O(s) + C(s)e−λst,

(3.2)

for a bounded learning rate s. Taken together, these results oﬀer insights into the phenomena observed in Figure 1. In par-
ticular, Proposition 3.1 states that, from the continuous-time perspective, the risk of SGD with a constant learning rate applied to a (nonconvex) objective function converges to stationarity at a linear rate. Moreover, Proposition 3.2 demonstrates that the excess risk at stationarity decreases as the learning rate s tends to zero. This is in agreement with the numerical experiments illustrated in Figures 1, 2, and 3. For comparison, this property is not observed in GD and SGLD.
The following result gives the iteration complexity of SGD in its continuous-time formulation.

Corollary 3.3. Under the assumptions of Proposition 3.2, for any ǫ > 0, if the learning rate s ≤ min{ǫ/(2A), S} and t ≥ λ1s log 2C(s) ρ−ǫ µs µ−s 1 , then
E f (Xs(t)) − f ⋆ ≤ ǫ.

3.2 The rate of linear convergence
We now turn to the key issue of understanding how the linear rate λs depends on the learning rate. In this subsection, we show that for certain objective functions, λs admits a simple expression that allows us to interpret how the convergence rate depends on the learning rate.
We begin by considering a strongly convex function. Recall the deﬁnition of strong convexity: for µ > 0, a function f is µ-strongly convex if
f (y) ≥ f (x) + ∇f (x), y − x + µ y − x 2, 2
for all x, y. Equivalently, f is µ-strong convex if all eigenvalues of its Hessian ∇2f (x) are greater than or equal to µ for all x (note that here f is assumed to be inﬁnitely diﬀerentiable). As is clear, a strongly convex function satisﬁes the conﬁning condition. In Appendix B.1, we prove the following proposition by making use of a Poincar´e-type inequality, the Bakry–Emery theorem [BGL13].7
7In fact, we can obtain a tighter log-Sobolev inequality for convergence of the probability densities in L1(Rd), as is shown in Appendix B.2.

10

Proposition 3.4. In addition to the assumptions of Theorem 1, assume that the objective f is a µ-strongly convex function. Then, λs in (3.1) satisﬁes λs = µ.

We turn to the more challenging setting where f is nonconvex. Let us refer to the objective f as a Morse function if its Hessian has full rank at any critical point x (that is, ∇f (x) = 0).8

Theorem 2. In addition to the assumptions of Theorem 1, assume that the objective f is a Morse function and has at least two local minima.9 Then the constant λs in (3.1) satisﬁes

λs

=

(α

+

o

(s

))e−

2H s

f

,

(3.3)

for 0 < s ≤ s0, where s0 > 0, α > 0, and Hf > 0 are constants that all depend only on f .

The proof of this result relies on tools in the spectral theory of Schro¨dinger operators and is deferred to Section 6. From now on, we call λs in (3.1) the exponential decay constant. To obviate any confusion, o(s) in Theorem 2 stands for a quantity that tends to zero as s → 0, and the precise expression for Hf shall be given in Section 6, with a simple example provided in Section 3.4. To leverage Theorem 2 for understanding the phenomena discussed in Section 1, however, it suﬃces to recognize the fact that Hf is completely determined by f . Moreover, we remark that while Theorem 1 shows that λs exists for any learning rate, the present theorem assumes a bounded learning rate.
The key implication of this result is that the rate of convergence is highly contingent upon the learning rate s: the exponential decay constant increases as the learning rate s increases. Accordingly, the linear convergence to stationarity established in Section 3.1 is faster if s is larger, and, by recognizing the exponential dependence of λs on s, the convergence would be very slow if the learning rate s is very small. For example, if Hf = 0.05, setting s = 0.1 and s = 0.001 gives

λ0.1 ≈ e−1 = 9.889 × 1042. λ0.001 e−100
Moreover,as we will see clearly in Section 6, λs is completely determined by the geometry of f . In particular, it does not depend on the probability distribution of the initial point or the dimension d given that the constant Hf has no direct dependence on the dimension d. For comparison, the linear rate in the nonconvex case is shown by Theorem 2 to depend on the learning rate s, while the linear rate of convergence stays constant regardless of s if the objective is strongly convex. This fundamental distinction between the convex and nonconvex settings enables an interpretation of the observation brought up in Figure 1, in particular the right panel of Figure 3. More precisely, with time t being the x-axis, SGD with a larger learning rate leads to a faster convergence rate in the nonconvex setting, while for the (strongly) convex setting the convergence rate is independent of the learning rate. For further in-depth discussion of the implications of Theorem 2 (see Section 4).

3.3 Discretization
In this subsection, we carry over the results developed from the continuous perspective to the discrete regime. In addition to assuming that the objective function f satisﬁes the Villani condition,
8See Section 6.2 for a discussion of Morse functions. Note that (inﬁnitely diﬀerentiable) strongly convex functions are Morse functions.
9We call x a local minimum of f if ∇f (x) = 0 and the Hessian ∇2f (x) is positive deﬁnite. By convention, in this paper a global minimum is also considered a local minimum.

11

satisﬁes the conﬁning condition, and is a Morse function, we also now assume f to be L-smooth; that is, f has L-Lipschitz continuous gradients in the sense that ∇f (x) − ∇f (y) ≤ L x − y for all x, y. Moreover, we restrict the learning rate s to be no larger than 1/L. The following proposition is the key theoretical tool that allows translation to the discrete regime.

Proposition 3.5. For any L-smooth objective f and any initialization Xs(0) drawn from a probability density ρ ∈ L2(µ−s 1), the lr-dependent SDE (1.2) has a unique global solution Xs in expectation; that is, E Xs(t) as a function of t in C1([0, +∞); Rd) is unique. Moreover, there exists B(T ) > 0 such that the SGD iterates xk satisfy

max |E f (xk) − E f (Xs(ks))| ≤ B(T )s,
0≤k≤T /s

for any ﬁxed T > 0.

We note that there exists a sharp bound on B(T ) in [BT96]. For completeness, we also remark that the convergence can be strengthened to the strong sense:

max E xk − Xs(ks) ≤ B′(T )s.
0≤k≤T /s

This result has appeared in [Mil75, Tal82, PT85, Tal84, KP92] and we provide a self-contained proof in Appendix B.3.
We now state the main result of this subsection.

Theorem 3. In addition to the assumptions of Theorem 1, assume that f is L-smooth. Then, the following two conclusions hold:

(a) For any T > 0, the iterates of SGD with learning rate 0 < s ≤ 1/L satisfy

E f (xk) − f ⋆ ≤ (A + B(T ))s + C ρ − µs µ−1 e−sλsk, s

(3.4)

for all k ≤ T /s, where λs is the exponential decay constant in (3.1), A as in Proposition 3.2 depends only on 1/L and f , C = C1/L is as in Proposition 3.1, and B(T ) depends only on the time horizon T and the Lipschitz constant L.

(b) If f is a Morse function with at least two local minima, with λs appearing in (3.4) being given by (3.3), and if f is µ-strongly convex then λs = µ.
Theorem 3 follows as a direct consequence of Theorem 1 and Proposition 3.5. Note that the second part of Theorem 3 is simply a restatement of Theorem 2 and Proposition 3.4. As earlier in the continuous-time formulation, we also mention that the dimension parameter d is not an essential parameter for characterizing the rate of linear convergence. In relating to Figure 3, note that its left panel with k being the x-axis shows a faster linear convergence of SGD when using a larger learning rate, regardless of convexity or nonconvexity of the objective. This is because the linear rate sλs in (3.4) is always an increasing function of s even for the strongly convex case, where λs itself is constant.

12

3.4 A one-dimensional example

In this section we provide some intuition for the theoretical results presented in the preceding
subsections. Our priority is to provide intuition rather than rigor. Consider the simple example of f presented in Figure 4, which has a global minimum x⋆, a local minimum x•, and a local maximum x◦.10 We use this toy example to gain insight into the expression (3.3) for the exponential decay
constant λs; deferring the rigorous derivation of this number in the general case to Section 6. From (3.1) it suggests that the lr-dependent SDE (1.2) takes about O(1/λs) time to achieve ap-
proximate stationarity. Intuitively, for the speciﬁc function in Figure 4, the bottleneck in achieving stationarity is to pass through the local maximum x◦. Now, we show that it takes about O(1/λs) time to pass x◦ from the local minimum x•. For simplicity, write

f (x) = θ (x − x•)2 + g(x), 2

where g(x) = f (x•) stays constant if x ≤ x◦ − ν for a very small positive ν and θ > 0. Accordingly, the lr-dependent SDE (1.2) is reduced to the Ornstein–Uhlenbeck process,

dXs

=

−θ(Xs

−

x•)dt

+

√ sdW,

before hitting x◦. Denote by τx◦ the ﬁrst time the Ornstein–Uhlenbeck process hits x◦. It is well known that the hitting time obeys

√

√

E τx◦ ≈

πs

√

e

2 s

·

1 2

θ

(x

◦

−

x

•

)2

≈

πs

√

2Hf
es,

(x◦ − x•)θ θ

(x◦ − x•)θ θ

(3.5)

where Hf := f (x◦) − f (x•) ≈ f (x◦) − g(x◦) = 21 θ(x◦ − x•)2. This number, which we refer to as the Morse saddle barrier, is the diﬀerence between the function values at the local maximum x◦ and the local minimum x• in our case. As an implication of (3.5), the continuous-time formulation of SGD takes time (at least) of the order e(1+o(1)) 2Hsf to achieve approximate stationarity. This is
consistent with the exponential decay constant λs given in (3.3).

x◦ Hf
x•
x⋆
Figure 4: A one-dimensional nonconvex function f . The height diﬀerence between x◦ and x• in this special case is the Morse saddle barrier Hf . See the formal deﬁnition in Deﬁnition 6.6.
10We can also regard x◦ as a saddle point in the sense that the Hessian at this point has one negative eigenvalue. See Section 6.2 for more discussion.

13

In passing, we remark that the discussion above can be made rigorous by invoking the theory of the Kramers escape rate, which shows that for this univariate case the hitting time satisﬁes

E τx◦ = (1 + o(1))

π e 2Hsf . −f ′′(x•)f ′′(x◦)

See, for example, [FW12, Pav14]. Furthermore, we demonstrate the view from the theory of viscosity solution and singular perturbation in Appendix B.4.

4 Why Learning Rate Decay?

As a widely used technique for training neural networks, learning rate decay refers to taking a large learning rate initially and then progressively reducing it during the training process. This technique has been observed to be highly eﬀective especially in the minimization of nonconvex objective functions using stochastic optimization methods, with a very recent strand of theoretical eﬀort toward understanding its beneﬁts [YLWJ19, LWM19]. In this section, we oﬀer a new and crisp explanation by leveraging the results in Section 3. To highlight the intuition, we primarily work with the continuous-time formulation of SGD.

1

1

1

0.5

0.5

0.5

0

0

0

-0.5

-0.5

-0.5

-1

-1

-0.5

0

1

0.5

1

-1

-1

-0.5

0

1

0.5

1

-1

-1

-0.5

0

1

0.5

1

0.5

0.5

0.5

0

0

0

-0.5

-0.5

-0.5

-1

-1

-0.5

0

0.5

1

-1

-1

-0.5

0

0.5

1

-1

-1

-0.5

0

0.5

1

Figure 5: Scatter plots of the iterates xk ∈ R2 of SGD for minimizing the nonconvex function in Figure 3. This function has four local minima, of which the bottom right one is the gloabl minimum. Each column corresponds to the same value of t = ks, and the ﬁrst row and second row correspond to learning rates 0.1 and 0.05, respectively. The gradient noise is drawn from the standard normal distribution. Each plot is based on 10000 independent SGD runs using the noise generator “state 1-10000” in Matlab2019b, starting from an initial point (−0.9, 0.9).

For purposes of illustration, Figure 5 presents numerical examples for this technique where the learning rate is set to 0.1 or 0.05. This ﬁgure clearly demonstrates that SGD with a larger learning rate converges much faster to the global minimum than SGD with a smaller learning rate. This comparison reveals that a large learning rate would render SGD able to quickly explore the landscape of the objective function and eﬃciently escape bad local minima. On the other hand,

14

a larger learning rate would prevent SGD iterates from concentrating around a global minimum, leading to substantial suboptimality. This is clearly illustrated in Figure 6. As suggested by the heuristic work on learning rate decay, we see that it is important to decrease the learning rate to achieve better optimization performance whenever the iterates arrive near a local minimum of the objective function.

1

1

0.5

0.5

0

0

-0.5

-0.5

-1

-1

-0.5

0

0.5

1

-1

-1

-0.5

0

0.5

1

Figure 6: The same setting as in Figure 5. Both plots correspond to the same value of t = ks = 1000.

Despite its intuitive plausibility, the exposition above stops short of explaining why nonconvexity
of the objective is crucial to the eﬀectiveness of learning rate decay. Our results in Section 3,
however, enable a concrete and crisp understanding of the vital importance of nonconvexity in this setting. Motivated by (3.2), we consider an idealized risk function of the form R(t) = as + be−λst, with λs set to e−c/s, where a, b, and c are positive constants for simplicity as opposed to the nonconstants in the upper bound in (3.1). This function is plotted in Figure 7, with two quite diﬀerent
learning rates, s1 = 0.1 and s2 = 0.001, as an implementation of learning rate decay. When the learning rate is s1 = 0.1, from the right panel of Figure 7, we see that rough stationarity is achieved at time t = ks ≈ 25; thus, the number of iterations k0.1 ≈ 25/s = 250. In the case of s = 0.001, from the left panel of Figure 7, we see now it requires ks ≈ 2.5 × 1044 to reach rough stationarity, leading to k0.001 ≈ 2.5 × 1047. This gives

k0.001 ≈ 1045. k0.1

In contrast, the sharp dependence of ks on the learning rate s is not seen for strongly convex

functions, because λs = µ stays constant as the learning rate s varies. Following the preceding

example, we have

k0.001 ≈ 102. k0.1

While a large initial learning rate helps speed up the convergence, Figure 7 also demon-
strates that a larger learning rate leads to a larger value of the excess risk at stationarity, ǫ(s) ≡ E f (Xs(∞))−f ⋆, which is indeed the claim of Proposition 3.2. Leveraging Proposition 3.1, we show below why annealing the learning rate at some point would improve the optimization performance. To this end, for any ﬁxed learning rate s, consider a stopping time Tsδ that is deﬁned as

Tsδ := inf {|E f (Xs(t)) − E f (Xs(∞))| ≤ δǫ(s)} ,
t

15

102

102

101

101

100

100

10-1

10-1

10-2

0

1

2

3

4

5

6

7

8

9

10

1044

10-2 0

10

20

30

40

50

60

70

80

90 100

c
Figure 7: Idealized risk function of the form R(t) = as + be−e s t with the identiﬁcation t = ks, which is
adapted from (3.2). The parameters are set as follows: a = 1, b = 100 − s, c = 0.1, and the learning rate is
s = 0.1 or 0.001. The right plot is a locally enlarged image of the left.

for a small δ > 0. In words, the lr-dependent SDE (1.2) at time Tsδ is approximately stationary since its risk E f (Xs(t)) − f ⋆ is mainly comprised of the excess risk at stationarity ǫ(s), with a total risk of no more than (1 + δ)ǫ(s). From Proposition 3.1 it follows that (recall that ρ is the initial
distribution):

δ 1 C(s) ρ − µs µ−1

e 2Hf s

C(s) ρ − µs µ−1

Ts ≤ λs log

δǫ(s) s = γ + o(s) log

δǫ(s) s .

(4.1)

In addition to taking a large s, an alternative way to make Tsδ small is to have an initial distribution ρ that is close to the stationary distribution µs. This can be achieved by using the technique of
learning rate decay. More precisely, taking a larger learning rate s1 for a while, at the end the
distribution of the iterates is approximately the stationary distribution µs1, which serves as the initial distribution for SGD with a smaller learning rate s2 in the second phase. Taking ρ ≈ µs1, the factor ρ − µs µ−s 1 in (4.1) for the second phase of learning rate decay is approximately

µs1 − µs2 µ−s21 =

1
(µs1 − µs2 )2µ−s21dx 2 =

µ2s1 dx − 1 21 . µs2

(4.2)

Both µs1 and µs2 are decreasing functions of f and, therefore, have the same modes. As a consequence, the integral of µ2s1/µs2 is small by appeal to the rearrangement inequality, thereby leading to fast convergence of SGD with learning rate s2 to the stationary risk ǫ(s2). In contrast, ρ − µs2 µ−s21 would be much larger for a general random initialization ρ. Put simply, SGD with learning rate
s2 cannot achieve a risk of approximately ǫ(s2) given the same number of iterations without the warm-up stage using learning rate s1. See Figure 8 for an illustration.

5 Proof of the Linear Convergence
In this section, we prove Proposition 3.1 and Proposition 3.2, leading to a complete proof of Theorem 1.

16

ρ large learning rate s1 ≈ µs1 small learning rate s2 ≈ µs2
Figure 8: Learning rate decay. The ﬁrst phase uses a larger learning rate s1, at the end of which the SGD iterates are approximately distributed as µs1 . The second phase uses a smaller learning rate s2 and at the end the distribution of the SGD iterates roughly follows µs2 .

5.1 Proof of Proposition 3.1
To better appreciate the linear convergence of the lr-dependent SDE (1.2), as established in Proposition 3.1, we start by showing the convergence to stationarity without a rate. In fact, this intermediate result constitutes a necessary step in the proof of Proposition 3.1.

Convergence without a rate. Recall that we use ρ to denote the initial probability density in the space L2(µ−s 1). Superﬁcially, it seems that the most natural space for probability densities is L1(Rd). However, we prefer to work in L2(µ−s 1) since this function space has certain appealing properties that allow us to obtain the proof of the desired convergence results for the lr-dependent SDE. Formally, the following result says that any (nonnegative) function in L2(µ−s 1) can be normalized to be a density function. The proof of this simple lemma is shown in Appendix C.1.

Lemma 5.1. Let f satisfy the conﬁning condition. Then, L2(µ−s 1) is a subset of L1(Rd).

The following result shows that the solution to the lr-dependent SDE converges to stationarity in terms of the dynamics of its probability densities over time.

Lemma 5.2. Let f satisfy the conﬁning condition and denote the initial distribution as ρ ∈

L2(µ−s 1). Then, the unique solution ρs(t, ·) ∈ C1

[0,

+

∞

),

L

2

(µ

− s

1

)

to the Fokker–Planck–Smoluchowski

equation (2.1) converges in L2(µ−s 1) to the Gibbs invariant distribution µs, which is speciﬁed

by (2.2).

Note that the existence and uniqueness of ρs(t, ·) is ensured by Lemma 2.2. The convergence guarantee on ρs(t, ·) in Lemma 5.2 relies heavily on the following lemma (Lemma 5.3). This preparatory lemma introduces the transformation

hs(t, ·) = ρs(t, ·)µ−s 1 ∈ C1 [0, +∞), L2(µs) ,

which allows us to work in the space L2(µs) in place of L2(µ−s 1) (a measurable function g is said to belong to L2(µs) if g µs := Rd g2dµs 21 < +∞11). It is not hard to show that hs satisﬁes the

following equation

∂hs = −∇f · ∇hs + s ∆hs,

∂t

2

(5.1)

with the initial distribution hs(0, ·) = ρµ−s 1 ∈ L2(µs). The linear operator

s Ls = −∇f · ∇ + 2 ∆

(5.2)

has a crucial property, as stated in the following lemma. Its proof is postponed to Appendix C.2.

11Here, dµs stands for the probability measure dµs ≡ µsdx = Z1s exp(−2f /s)dx.

17

Lemma 5.3. The linear operator Ls in (5.2) is self-adjoint and nonpositive in L2(µs). Explicitly, for any g1, g2, this operator obeys
s Rd (Lsg1)g2dµs = Rd g1Lsg2dµs = − 2 Rd ∇g1 · ∇g2dµs.

Proof of Lemma 5.2. We have

d

ρs(t, ·) − µs 2µ−1 = d

hs(t, ·) − 1

2 µ

dt

s dt

s

= d (hs(t, x) − 1)2 dµs dt Rd

= 2 (hs − 1)Ls(hs − 1)dµs,
Rd

where the last equality is due to (5.1). Next, we proceed by making use of Lemma 5.3:

2 (hs − 1)Ls(hs − 1)dµs = −s ∇(hs − 1) · ∇(hs − 1)dµs

Rd

Rd

= −s

∇hs 2dµs ≤ 0.

Rd

(5.3)

Thus,

ρs(t, ·) − µs

2 µ−1

is

a

strictly

decreasing

function,

decreasing

asymptotically

towards

the

s

equilibrium state

∇hs 2dµs = 0.
Rd

This equality holds, however, only if hs(t, ·) is constant. Because both ρs(t, ·) and µs are proba-

bility densities, this case must imply that hs(t, ·) ≡ 1; that is, ρs(t, ·) ≡ µs. Therefore, ρs(t, ·) ∈

C1

[0,

+

∞

),

L

2

(µ

− s

1

)

converges to the Gibbs invariant distribution µs in L2(µ−s 1).

Linear convergence. We turn towards the proof of linear convergence. We ﬁrst state a lemma which serves as a fundamental tool for us to prove a linear rate of convergence for Proposition 3.1.

Lemma 5.4 (Theorem A.1 in [Vil09]). If f satisﬁes both the conﬁning condition and the Villani

condition, then there exists λs > 0 such that the measure dµs satisﬁes the following Poincar´e-type

inequality

h2dµs −
Rd

2s hdµ ≤

∇h 2dµ ,

s Rd

2λs Rd

s

for any h such that the integrals above are well-deﬁned.

For completeness, we provide a proof of this Poincar´e-type inequality in Appendix C.3. For comparison, the usual Poincar´e inequality is put into use for a bounded domain, as opposed to the entire Euclidean space as in Lemma 5.4. In addition, while the constant in the Poincar´e inequality in general depends on the dimension (see, for example, [Eva10, Theorem 1, Chapter 5.8]), λs in Lemma 5.4 is completely determined by geometric properties of the objective f . See details in Section 6.
Importantly, Lemma 5.4 allows us to obtain the following lemma, from which the proof of Proposition 3.1 follows readily. The proof of this lemma is given at the end of this subsection.

18

Lemma 5.5. Under the assumptions of Proposition 3.1, ρs(t, ·) converges to the Gibbs invariant distribution µs in L2(µ−s 1) at the rate

ρs(t, ·) − µs µ−1 ≤ e−λst ρ − µs µ−1 .

s

s

(5.4)

Proof of Proposition 3.1. Using Lemma 5.5, we get

|E f (Xs(t)) − E f (X(∞))| = f (x) (ρs(t, x) − µs(x)) dx
Rd

= (f (x) − f ⋆) (ρs(t, x) − µs(x)) dx
Rd

1

1

≤

(f (x) − f ⋆)2µs(x)dx 2

(ρs(t, x) − µs(x))2 µ−s 1dx 2

Rd

Rd

≤ C(s)e−λst ρ − µs µ−1 , s

where the ﬁrst inequality applies the Cauchy-Schwarz inequality and

C(s) =

1
(f − f ⋆)2µsdx 2
Rd

is an increasing function of s.

We conclude this subsection with the proof of Lemma 5.5.

Proof of Lemma 5.5. It follows from (5.3) that

d

ρs(t, ·) − µs

2
−1

= −s

∇hs 2dµs.

dt

µs

Rd

Next, using Lemma 5.4 and recognizing the equality Rd hsdµs = Rd ρs(t, x)dx = 1, we get

d

ρs(t, ·) − µs

2
−1

≤ −2λs

dt

µs

h2sdµs −
Rd

2
hsdµs
Rd

= −2λs

h2sdµs − 1
Rd

= −2λs = −2λs

(hs − 1)2dµs
Rd

ρs(t, ·) − µs

2 µ−1

.

s

Integrating both sides yields (5.4), as desired.

19

5.2 Proof of Proposition 3.2

Next, we turn to the proof of Proposition 3.2. We ﬁrst state a technical lemma, deferring its proof to Appendix C.4.

Lemma 5.6. Under the assumptions of Proposition 3.2, the excess risk at stationarity ǫ(s) satisﬁes

dǫ(0) = 0. ds
Using Lemma 5.6, we now ﬁnish the proof of Proposition 3.2.

Proof of Proposition 3.2. Letting g = f − f ⋆, we write the excess risk at stationarity as

ǫ(s) = E f (Xs(∞)) − f ⋆ =

Rd ge− 2sg dx , Rd e− 2sg dx

which yields the following derivative:

2
dǫ(s) = s2 ds

Rd g2e− 2sg dx

Rd e− 2sg dx − s22 Rd e− 2sg dx 2

Rd ge− 2sg dx 2 .

Making use of the Cauchy-Schwarz inequality, the derivative satisﬁes

dǫ(s) ds

≥ 0 for all s > 0.

In

fact, the equality holds only in the case of a constant f is a constant, which contradicts both the

conﬁning condition and the Villani condition. Hence, the inequality can be strengthened to

dǫ(s) > 0, ds

for s > 0. Consequently, we have proven that the excess risk ǫ(s) at stationarity is a strictly increasing function of s ∈ [0, +∞).
Next, from Fatou’s lemma we get

ǫ(0) ≤ lim sup ǫ(s) ≤ lim gµsdx = f ⋆ − f ⋆ = 0

s→0+

Rd s→0+

ǫ(0) ≥ lim inf ǫ(s) ≥ lim gµsdx = f ⋆ − f ⋆ = 0.

s→0+

Rd s→0+

As a consequence, ǫ(0) = 0. Lemma 5.6 shows that for any S > 0, there exists A = AS such that

0≤

dǫ(s) ds

≤A

for

all

0 ≤ s ≤ S.

This

fact,

combined

with

ǫ(0) = 0,

immediately

gives

ǫ(s) ≤ As

for all 0 ≤ s ≤ S.

6 Geometrizing the Exponential Decay Constant
Having established the linear convergence to stationarity for the lr-dependent SDE, we now oﬀer a quantitative characterization of the exponential decay constant λs for a class of nonconvex objective functions. This is crucial for us to obtain a clear understanding of the dynamics of SGD and especially its dependence on the learning rate in the nonconvex setting.
20

6.1 Connection with a Schr¨odinger operator

We begin by deriving a relationship between the lr-dependent SDE (1.2) and a Schro¨dinger operator.

Recall that the probability density ρs(t, ·) of the SDE solution is assumed to be in L2(µ−s 1). Consider

the transformation

ψs(t, ·)

=

ρs(t, ·) √

∈

L2(Rd).

µs

This transformation allows us to equivalently write the Fokker–Planck–Smoluchowski equation (2.1)

as

∂ψs s

∇f 2 ∆f

−s∆ + Vs

∂t = 2 ∆ψs − 2s − 2 ψs = − 2 ψs,

(6.1)

with the initial condition ψs(0, ·) = √ρµs ∈ L2(Rd). This is a Schro¨dinger equation with the associated operator −s∆ + Vs, where the potential

∇f 2 Vs = s − ∆f

is positive for suﬃciently large x due to the Villani condition.
Now, we collect some basic facts concerning the spectrum of the Schro¨dinger operator −s∆+Vs. First, it is a positive semideﬁnite operator, as sh√own below. Recognizing the uniqueness of the Gibbs distribution (2.2), it is not hard to show that µs is the unique eigenfunction of −s∆ + Vs with a corresponding eigenvalue of zero. Using this fact, from the proof of Lemma 5.5, we get

(−s∆ + Vs)ψs(t, ·), ψs(t, ·)

= (−s∆ + Vs)(ψs(t, ·) − √µs), ψs(t, ·) − √µs

=−d dt
=−d dt

ψs(t, ·) − √µs, ψs(t, ·) − √µs

ρs(t, ·) − µs

2 µ−1

s

=s

∇

(ρs

(t

,

·)µ

− s

1

)

2dµs

Rd

≥ 0,

where ·, · denotes the standard inner product in L2(Rd). In fact, this inequality can be extended
to (−s∆ + Vs)g, g ≥ 0 for any g. This veriﬁes the positive semideﬁniteness of the Schro¨dinger operator −s∆ + Vs.
Next, making use of the fact that 1s Vs(x) → +∞ as x → +∞, we state the following wellknown result in spectral theory—that the Schro¨dinger operator has a purely discrete spectrum in L2(Rd) [HS12].

Lemma 6.1 (Theorem 10.7 in [HS12]). Assume that V is continuous, and V (x) → +∞ as x → +∞. Then the operator −∆ + V has a purely discrete spectrum.

Taken together, the positive semideﬁniteness of −s∆ + Vs and Lemma 6.1 allow us to order the eigenvalues of −s∆ + Vs in L2(Rd) as

0 = ζs,0 < ζs,1 ≤ · · · ≤ ζs,ℓ ≤ · · · < +∞.

21

A crucial fact from this representation is that the exponential decay constant λs in Theorem 5.5

can be set to 1

λs = 2 ζs,1.

(6.2)

To see this, note that ψs(t, ·) − √µs also satisﬁes (6.1) and is orthogonal to the null eigenfunction

√µs. Therefore, the norm of ψs(t, ·) − √µs must decay exponentially at a rate determined by half

of the smallest positive eigenvalue of Hs.12 That is, we have

ψs(t, ·) − √µs, ψs(t, ·) − √µs ≤ e−2 ζs2,1 t ψs(0, ·) − √µs, ψs(0, ·) − √µs = e−ζs,1t ψs(0, ·) − √µs, ψs(0, ·) − √µs ,

which is equivalent to

ρs(t, ·) − µs µ−1 ≤ e− ζs2,1 t ρ − µs µ−1 .

s

s

As such, we can take λs = 21 ζs,1 in the proof of Lemma 5.5. As a consequence of this discussion, we seek to study the Fokker–Planck–Smoluchowski equa-
tion (2.1) by analyzing the spectrum of the linear Schro¨dinger operator (6.1), especially its smallest positive eigenvalue δs,1. To facilitate the analysis, a crucial observation is that this Schro¨dinger operator is equivalent to the Witten-Laplacian,

∆sf := s(−s∆ + Vs) = −s2∆ + ∇f 2 − s∆f,

(6.3)

by a simple scaling. Denoting by the eigenvalues of the Witten-Laplacian as 0 = δs,0 < δs,1 ≤ · · · ≤ δs,ℓ ≤ · · · < +∞, we obtain the simple relationship

δs,ℓ = sζs,ℓ,

for all ℓ. The spectrum of the Witten-Laplacian has been the subject of a large literature [HN05, BGK05,
Nie04, AK99], and in the next subsection, we exploit this literature to derive a closed-from expression for the ﬁrst positive eigenvalue of the Witten-Laplacian, thereby obtaining the dependence of the exponential decay constant on the learning rate for a certain class of nonconvex objective functions [HHS11, Mic19].

6.2 The spectrum of the Witten-Laplacian: nonconvex Morse functions
We proceed by imposing the mild condition on the objective function that its ﬁrst-order and secondorder derivatives cannot be both degenerate anywhere. Put diﬀerently, the objective function is a Morse function. This allows us to use the theory of Morse functions to provide a geometric interpretation of the spectrum of the Witten-Laplacian.
12Here, the norm of ψs(t, ·) − √µs is induced by the inner product in L2(Rd). That is,
ψ(t, ·) − √µs L2(Rd) = ψ(t, ·) − √µs, ψ(t, ·) − √µs .

22

Basics of Morse theory. We give a brief introduction to Morse theory at the minimum level that is necessary for our analysis. Let f be an inﬁnitely diﬀerentiable function deﬁned on Rn. A point x is called a critical point if the gradient ∇f (x) = 0. A function f is said to be a Morse function if for any critical point x, the Hessian ∇2f (x) at x is nondegenerate; that is, all the eigenvalues of the Hessian are nonzero. The objective f is assumed to be a Morse function throughout Section 6.2. Note also that we refer to a point x as a local minimum if x is a critical point and all eigenvalues of the Hessian at x are positive.
Next, we deﬁne a certain type of saddle point. To this end, let η1(x) ≥ η2(x) ≥ · · · ≥ ηd(x) be the eigenvalues of the Hessian ∇2f (x) at x.13 A critical point x is said to be an index-1 saddle point if the Hessian at x has exactly one negative eigenvalue, that is, η1(x) ≥ · · · ≥ ηd−1(x) > 0, ηd(x) < 0. Of particular importance to this paper is a special kind of index-1 saddle point that will be used to characterize the exponential decay constant. Letting Kν := x ∈ Rd : f (x) < ν denote the sublevel set at level ν, for any index-1 saddle point x, it is not hard to show that the set Kf(x) ∩ {x′ : x′ − x < r} can be partitioned into two connected components, say C1(x, r) and C2(x, r), if the radius r is suﬃciently small. Using this fact, we give the following deﬁnition.
Deﬁnition 6.2. Let x be an index-1 saddle point and r > 0 be suﬃciently small. If C1(x, r) and C2(x, r) are contained in two diﬀerent (maximal) connected components of the sublevel set Kf(x), we call x an index-1 separating saddle point.
The remainder of this section aims to relate index-1 separating saddle points to the convergence rate of the lr-dependent SDE. For ease of reading, the remainder of the paper uses x◦ to denote an index-1 separating saddle point and writes X ◦ for the set of all these points. To give a geometric interpretation of Deﬁnition 6.2, let x•1 and x•2 denote local minima in the two maximal connected components of Kf(x◦), respectively. Intuitively speaking, the index-1 separating saddle point x◦ is the bottleneck of any path connecting the two local minima. More precisely, along a path connecting x•1 and x•2, by deﬁnition the function f must attain a value that is at least as large as f (x◦). In this regard, the function value at x◦ plays a fundamental role in determining how long it takes for the lr-dependent SDE initialized at x•1 to arrive at x•2. See an illustration in Figure 9.
As is assumed in this section, f is a Morse function and satisﬁes both the conﬁning and the Villani conditions; in this case, it can be shown that the number of the critical points of f is ﬁnite. Thus, denote by n◦ the number of index-1 separating saddle points of f and let n• denote the number of local minima.

H´erau–Hitrik–Sj¨ostrand’s generic case. To describe the labeling procedure, consider the set

of the objective values at index-1 separating saddle points V = {f (x◦) : x◦ ∈ X ◦}. This is a ﬁnite

set and we use I to denote the cardinality of this set. Write V = {ν1, . . . , νI } and sort these values

as

+ ∞ = ν0 > ν1 > · · · > νI ,

(6.4)

where by convention ν0 = +∞ corresponds to a ﬁctive saddle point at inﬁnity. Next, we follow [HHS11] and deﬁne a type of connected components of sublevel set.

Deﬁnition 6.3. A connected component E of the sublevel set Kν for some ν ∈ V is called a critical component if either ∂E ∩ X ◦ = ∅ or E = Rd, where ∂E is the boundary of E.
13Note that here we order the eigenvalues from the largest to the smallest, as opposed to the case of the Schr¨odinger operator previously.

23

Figure 9: The landscape of a two-dimensional nonconvex Morse function. Here, x•1 and x•2 denote two local minima. Both x◦ and x+ are index-1 saddle points, but only the former is an index-1 separating saddle point since f (x◦) < f (x•). In the two bottom plots, the deep blue regions form the sublevel sets at f (x◦) or f (x•). Note that the sublevel set induced by x◦ is the union of two connected components.
In this deﬁnition, the case of E = Rd applies only if ν = ν0 = +∞. If ν = νi for some 1 ≤ i ≤ I is only attained by one index-1 separating saddle point, the sublevel set Kνi has two critical components. See Deﬁnition 6.2 for more details.
With the preparatory notions above in place, we describe the following procedure for labeling index-1 separating saddle points and local minima [HHS11]. See Figure 10 for an illustration of this process.
1. Let E10 := Rd. Note that the global minimum x⋆ is contained in E10 and denote
x•0 := x⋆ = argmin f (x).
x∈E10
Let X0• denote the singleton set {x⋆}. 2. Let Ej1 for j = 1, . . . , m1 be the critical components of the sublevel set Kν1. Note that
E11 ∪ · · · ∪ Em1 1 is a (proper) subset of Kν1. Without loss of generality, assume x⋆ ∈ Em1 1 . Then, we select x•1,j1 as
x•1,j1 = argmin f (x).
x∈Ej11
Deﬁne X1• := {x•1,1, . . . , x•1,m1−1}.
24

3. For i = 2, . . . , I, let Eji for j = 1, . . . , mi be the critical components of the sublevel set Kνi. Without loss of generality, we assume that the critical components are ordered such that

there exists an integer ki ≤ mi satisfying





ki

i−1

 Eji

Xℓ• = ∅

j=1

ℓ=0

and Eji
for any j = ki + 1, . . . , mi. Set x•i,j to

i−1
Xℓ•
ℓ=0

= ∅,

x•i,j = argmin f (x),
x∈Eji

for j = 1, . . . , ki. Deﬁne Xi• := {x•i,1, . . . , x•i,ki}.

=

ν0 +∞
ν1

E10

E1

◦x◦1,1

1

ν2

E2 ◦x◦2,1

1

E2 ◦x◦2,2 E2

2

3

◦x◦2,3

ν3

•

E3 ◦x◦3,1
1

x◦3,2 ◦ E23

x•2,1 •

•

x•3,1

x•2,2

x••

x••
3,2

1,1

•

x•2,3

x•⋆

Figure 10: A generic one-dimensional Morse function. The labeling process gives rise to a one-to-one
correspondence between the local minimum x•ij and the index-1 separating saddle point x◦i,j (which are also local maxima) for all i, j.

To make the labeling process above valid, however, we need to impose the following assumption on the objective. This assumption is generic in the sense that it should be satisﬁed by a generic Morse function.
Assumption 6.4 (Generic case [HHS11]). For every critical component Eji selected in the labeling process above, where i = 0, 1, . . . , I, we assume that
• The minimum x•i,j of f in any critical component Eji is unique.

25

• If Eji ∩ X ◦ = ∅, there exists a unique x◦i,j ∈ Eji ∩ X ◦ such that f (x◦i,j) = max f (x). In
x∈Eji∩X ◦
particular, Eji ∩ Kf(x◦ ) is the union of two distinct critical components. i,j

The ﬁrst condition in this assumption requires that there exists a unique minimum of the objective f in every critical component Eji. In particular, the global minimum x⋆ is unique under this assumption. In addition, the second condition requires that among all index-1 separating saddle points in Eji, if any, f attains the maximum at exactly one of these points.
Under Assumption 6.4, the above labeling process includes all the local minima of f . Moreover,
it reveals a remarkable result: there exists a bijection between the set of local minima and the set of index-1 separating saddle points (including the ﬁctive one) X ◦ ∪ {∞}. As shown in the labeling process, for any local minimum x•i,j, we can relate it to the index-1 separating saddle point at which f attains the maximum in the critical component Eji. See Figure 10 for an illustrative example. Interestingly, this shows that the number of local minima is always larger than the number of index-1 separating saddle points by one; that is, n◦ = n• − 1.
In light of these facts, we can relabel the index-1 separating saddle points x◦ℓ for ℓ = 0, 1, . . . , n◦ with x◦0 = ∞, and the local minima x•ℓ for ℓ = 0, 1, . . . , n• − 1 with x•0 = x⋆, such that

f (x◦0) − f (x•0) > f (x◦1) − f (x•1) ≥ . . . ≥ f (x◦n•−1) − f (x•n•−1),

(6.5)

where f (x◦0) − f (x•0) = f (∞) − f (x⋆) = +∞. A detailed description of this bijection is given

in [HHS11, Proposition 5.2].

With

the

pairs

(x

◦ ℓ

,

x

• ℓ

)

in

place,

we

readily

state

the

following

fundamental

result

concerning

the ﬁrst n• − 1 smallest positive eigenvalues of the Witten-Laplacian ∆sf in (6.3). Recall that the

nonconvex Morse function f satisﬁes the conﬁning condition and the Villani condition.

Proposition 6.5 (Theorem 1.2 in [HHS11]). Under Assumption 6.4 and the assumptions of Theorem 2, there exists s0 > 0 such that for any s ∈ (0, s0], the ﬁrst n• − 1 smallest positive eigenvalues of the Witten-Laplacian ∆sf associated with f satisfy

δs,ℓ

=

s (γℓ

+

o(s))

e−

2(f

(x◦ℓ

)−f s

(x•ℓ

))

for ℓ = 1, 1, . . . , n• − 1, where

|ηd(x◦ℓ )|

det(∇2f (x•ℓ ))

1 2

γℓ = π

− det(∇2f (x◦)) ,

ℓ

(6.6)

and ηd(x◦ℓ ) is the unique negative eigenvalue of ∇2f (x◦ℓ ).

Using Proposition 6.5 in conjunction with the simple relationship between the exponential decay constant and the spectrum of the Schro¨dinger operator/Witten-Laplacian (6.2), it is a stone’s throw to prove Theorem 2 when f is generic. First, we give the deﬁnition of the Morse saddle barrier.

Deﬁnition 6.6. Let f satisfy the assumptions of Theorem 2. We call Hf = f (x◦1) − f (x•1) the Morse saddle barrier of f .

Proof of Theorem 2 in the generic case. By Proposition 6.5, we can set the exponential decay con-

stant to

1 λs = 2s δs,1 =

|ηd(x◦1)| 2π

det(∇2f (x•1))

−

d

et(∇

2

f

(x

◦ 1

))

1 2
+ o(s)

e−

2H s

f

26

in Theorem 2. Taking α = 12 |ηd2(πx◦1)| into the generic case.

det(∇2f (x•1 ))

−

det

(∇

2

f

(x

◦ 1

))

1
2 in (3.3), we complete the proof when f falls

However, the generic assumption for the labeling process is complex, leading to the lack of a geometric interpretation of the objective function required for the labeling process. To gain further insight, we present a simplifying assumption that is a special case of Assumption 6.4. This simpliﬁcation is due to [Nie04].

Assumption 6.7 (Simpliﬁed generic case [Nie04]). The objective functions f takes diﬀerent values

at its local minima and index-1 separating saddle points. That is, letting x1 be a local minimum

or an index-1 separating saddle point, and x2 likewise, then f (x1) = f (x2). Furthermore, the

diﬀerences

f

(x

◦ ℓ1

)

−

f

(x

• ℓ2

)

are

distinct

for

any

ℓ1

and

ℓ2.

The following result follows immediately from Proposition 6.5.

Corollary 6.8 (Theorem 3.1 in [Nie04]). Under Assumption 6.7 and the assumptions of Theorem 2, Proposition 6.5 holds. Therefore, Theorem 2 holds in this case.

Michel’s degenerate case. We say that a Morse function is degenerate if it satisﬁes the as-
sumptions of Theorem 2 but not Assumption 6.4. To violate the generic assumption, for example, we can change the objective value f (x•3,1) to f (x•1,1) or change f (x•3,2) to f (x•2,3) in Figure 10. In this situation, the ﬁrst condition in Assumption 6.4 is not satisﬁed. Alternatively, if the objective value at x◦3,1 is changed to f (x◦2,1), the second condition in Assumption 6.4 is not met. Figure 11 presents an example of a degenerate Morse function.
The main challenge in the degenerate case is the lack of uniqueness of the pairs (x◦ℓ , x•ℓ ) derived from the labeling process. Nevertheless, the uniqueness can be maintained if we work on the
function values. Explicitly, the labeling process can be adapted to the degenerate case and still yields unique pairs (f (x◦ℓ ), f (x•ℓ )) obeying
f (∞) − f (x⋆) = f (x◦0) − f (x•0) > f (x◦1) − f (x•1) ≥ . . . ≥ f (x◦n•−1) − f (x•n•−1).

In particular, the number of local minima remains larger than that of index-1 separating saddle points by one in this case. The following result extends Proposition 6.5 to the degenerate case, which is adapted from Theorem 2.8 of [Mic19].

Proposition 6.9 (Theorem 2.8 in [Mic19]). Assume that the assumptions of Theorem 2 are satisﬁed but not Assumption 6.4. Then, there exists s0 > 0 such that for any s ∈ (0, s0], the ﬁrst n• − 1 smallest positive eigenvalues of the Witten-Laplacian ∆sf associated with f satisfy

δs,ℓ

=

s

(γℓ

+

o(s))

e−

2Hf s

,

ℓ

,

for ℓ = 1, . . . , n• − 1, where f (x◦ℓ ) − f (x•ℓ ) ≤ Hf,ℓ ≤ f (x◦1) − f (x⋆). The constants Hf,ℓ and γℓ all depend only on the function f .

Taken together, Proposition 6.5 and Proposition 6.9 give a full proof of Theorem 2. As is
clear, the Morse saddle barrier in Deﬁnition 6.6 for the degenerate case is set to Hf = Hf,1. For completeness, we remark that this result applies to Assumption 6.4, in which case we conclude that Hf,ℓ = f (x◦ℓ ) − f (x•ℓ ) and γℓ is given the same as (6.6). As such, Proposition 6.5 is implied by Proposition 6.9.

27

=

ν0 +∞
ν1

E10

E1

◦x◦1,1

1

ν2

E2 x◦◦2,1E2 ◦x◦2,4

E2 ◦x◦2,2 E2

◦x◦2,3

1

4

2

3

x◦3,1

ν3

•

◦ E13

x•2,1

•

x•2,2

x•• x••

2,4

1,1

•

•

x•2,3 x•3,1

x•⋆

Figure 11: A degenerate one-dimensional Morse function. The labeling of its index-1 separating saddle
points x◦i,j and local minima x•i,j is not unique. Nevertheless, the labeling process gives a unique one-to-one correspondence between the function values at the two types of points. See Figure 10 for a comparison.

7 Discussion
In this paper, we have presented a theoretical perspective on the convergence of SGD in nonconvex optimization as a function of the learning rate. Introducing the notion of an lr-dependent SDE, we have leveraged modern tools for the study of diﬀusions, in particular the spectral theory of diﬀusion operators, to analyze the dynamics of SGD in a continuous-time model. Speciﬁcally, we have shown that the solution to the SDE converges linearly to stationarity under certain regularity conditions and we have presented a concise expression for the linear rate of convergence with transparent dependence on the learning rate for nonconvex Morse functions. Our results show that the linear rate is a constant in the strongly convex case, whereas it decreases rapidly as the learning rate decreases in the nonconvex setting. We have thus uncovered a fundamental distinction between convex and nonconvex problems. As one implication, we note that noise in the gradients plays a more determinative role in stochastic optimization with nonconvex objectives as opposed to convex objectives. We also note that our results provide a justiﬁcation for the use of a large initial learning rate in training neural networks.
We propose several directions for future research to consolidate and extend the framework for analyzing stochastic optimization methods via SDEs. A pressing question is to better characterize the gap between the stationary distribution of the lr-dependent SDE and that of the discrete SGD [Kro93, Pav14, DDB17]. Explicitly, can we improve the upper bound in Proposition 3.5? A related question is whether Theorem 3 can be improved to E f (xk) − f ⋆ ≤ O(s + (1 − λss)k), with the hidden coeﬃcients having less dependence on the time horizon ks. A possible approach to overcoming this diﬃculty in the discrete regime is to obtain a discrete version of the Poincar´e inequality in Rd (Lemma 5.4). From a diﬀerent angle, it is noteworthy that (s/2)∆ρs in the Fokker–Planck–Smoluchowski equation (2.1) corresponds to vanishing viscosity in ﬂuid mechanics. Appendix B.4 presents several open problems from this viewpoint. To widen the scope of this

28

framework, it is important to extend our results to the setting where the gradient noise is heavytailed [SSG19].
From a practical standpoint, our work oﬀers several promising avenues for future research in deep learning. First, a seemingly straightforward direction is to extend our SDE-based analysis to various learning rate schedules used in practice in training deep neural networks, such as diminishing learning rate and cyclical learning rates [BCN18, Smi17]. More broadly, it is of great interest to use SDEs to study and improve on practical variants of SGD, including RMSProp and Adam [TH12, KB14]. Second, our results would likely to be useful in guiding the choice of hyperparameters of deep neural networks from an optimization viewpoint. For instance, recognizing the essence of the exponential decay constant λs in determining the convergence rate of SGD, how to choose the neural network architecture and the loss function so as to get a small value of the Morse saddle barrier Hf ? Finally, we wonder if the lr-dependent SDE might give insights into generalization properties of neural networks such as local elasticity [HS20] and implicit regularization [ZBH+16, GLSS18].
Acknowledgments
We would like to thank Zhuang Liu and Yu Sun for helpful conversations about practical experience in deep learning. This work was supported in part by NSF through CAREER DMS-1847415, CCF-1763314, and CCF-1934876, and the Wharton Deans Research Fund. We also recognize support from the Mathematical Data Science program of the Oﬃce of Naval Research under grant number N00014-18-1-2764.

References

[AK99] [Arn12] [Arn13] [BCN18] [Ben12] [BGK05]
[BGL13] [Bot10] [BT96] [CEL84]

V. I. Arnol’d and B. A. Khesin. Topological Methods in Hydrodynamics, volume 125. Springer Science & Business Media, 1999.
V. Arnol’d. Geometrical Methods in the Theory of Ordinary Diﬀerential Equations. Springer Science & Business Media, 2012.
V. Arnol’d. Mathematical Methods of Classical Mechanics. Springer Science & Business Media, 2013.
L. Bottou, F. E. Curtis, and J. Nocedal. Optimization methods for large-scale machine learning. SIAM Review, 60(2):223–311, 2018.
Y. Bengio. Practical recommendations for gradient-based training of deep architectures. In Neural Networks: Tricks of the Trade, pages 437–478. Springer, 2012.
A. Bovier, V. Gayrard, and M. Klein. Metastability in reversible diﬀusion processes II: Precise asymptotics for small eigenvalues. Journal of the European Mathematical Society, 7(1):69–99, 2005.
D. Bakry, I. Gentil, and M. Ledoux. Analysis and Geometry of Markov Diﬀusion Operators, volume 348. Springer Science & Business Media, 2013.
L. Bottou. Large-scale machine learning with stochastic gradient descent. In Proceedings of COMPSTAT’2010, pages 177–186. Springer, 2010.
V. Bally and D. Talay. The law of the Euler scheme for stochastic diﬀerential equations: Ii. convergence rate of the density. Monte Carlo Methods and Applications, 2(2):93–128, 1996.
M. Crandall, L. Evans, and P.-L. Lions. Some properties of viscosity solutions of Hamilton-Jacobi equations. Transactions of the American Mathematical Society, 282(2):487–502, 1984.

29

[CF99]

G.-Q. Chen and H. Frid. Vanishing viscosity limit for initial-boundary value problems for conservation laws. Contemporary Mathematics, 238:35–51, 1999.

[CH19]

K. Caluya and A. Halder. Gradient ﬂow algorithms for density propagation in stochastic systems. IEEE Transactions on Automatic Control, 2019.

[CL83]

M. Crandall and P.-L. Lions. Viscosity solutions of Hamilton-Jacobi equations. Transactions of the American Mathematical Society, 277(1):1–42, 1983.

[CM90] A. Chorin and J. Marsden. A Mathematical Introduction to Fluid Mechanics. Springer, 1990.

[COO+18] P. Chaudhari, A. Oberman, S. Osher, S. Soatto, and G. Carlier. Deep relaxation: partial differential equations for optimizing deep neural networks. Research in the Mathematical Sciences, 5(3):30, 2018.

[CS04]

P. Cannarsa and C. Sinestrari. Semiconcave Functions, Hamilton-Jacobi Equations, and Optimal Control. Springer Science & Business Media, 2004.

[CS18]

P. Chaudhari and S. Soatto. Stochastic gradient descent performs variational inference, converges to limit cycles for deep networks. In 2018 Information Theory and Applications Workshop (ITA), pages 1–10. IEEE, 2018.

[DDB17]

A. Dieuleveut, A. Durmus, and F. Bach. Bridging the gap between constant step size stochastic gradient descent and Markov chains. arXiv preprint arXiv:1707.06386, 2017.

[DDC19]

D. Davis, D. Drusvyatskiy, and V. Charisopoulos. Stochastic algorithms with geometric step decay converge linearly on sharp functions. arXiv preprint arXiv:1907.09547, 2019.

[DJ19]

J. Diakonikolas and M. I. Jordan. Generalized momentum-based methods: A Hamiltonian perspective. arXiv preprint arXiv:1906.00436, 2019.

[DS01]

J.-D. Deuschel and D. Stroock. Large deviations, volume 342. American Mathematical Society, 2001.

[Eva80]

L. Evans. On solving certain nonlinear partial diﬀerential equations by accretive operator methods. Israel Journal of Mathematics, 36(3-4):225–247, 1980.

[Eva10]

L. Evans. Partial Diﬀerential Equations (Second Edition), volume 19. American Mathematical Society, 2010.

[Eva12]

L. Evans. An Introduction to Stochastic Diﬀerential Equations, volume 82. American Mathematical Society, 2012.

[FW12]

M. Freidlin and A. Wentzell. Random Perturbations of Dynamical Systems, volume 260. Springer Science & Business Media, 2012.

[Gas07] S. Gasiorowicz. Quantum Physics. John Wiley & Sons, 2007.

[GLSS18] S. Gunasekar, J. Lee, D. Soudry, and N. Srebro. Characterizing implicit bias in terms of optimization geometry. In International Conference on Machine Learning, pages 1832–1841, 2018.

[HHS11]

F. H´erau, M. Hitrik, and J. Sjo¨strand. Tunnel eﬀect and symmetries for Kramers–Fokker–Planck type operators. Journal of the Institute of Mathematics of Jussieu, 10(3):567–634, 2011.

[HKN04]

B. Helﬀer, M. Klein, and F. Nier. Quantitative analysis of metastability in reversible diﬀusion processes via a Witten complex approach. Mat. Contemp., 26:41–85, 2004.

[HN05]

B. Helﬀer and F. Nier. Hypoelliptic estimates and spectral theory for Fokker-Planck operators and Witten Laplacians. Springer, 2005.

[HRB08]

E. Hazan, A. Rakhlin, and P. Bartlett. Adaptive online gradient descent. In Advances in Neural Information Processing Systems, pages 65–72, 2008.

30

[HS12]

P. Hislop and I. Sigal. Introduction to Spectral Theory: With Applications to Schro¨dinger Operators, volume 113. Springer Science & Business Media, 2012.

[HS20]

H. He and W. J. Su. The local elasticity of neural networks. In International Conference on Learning Representations (ICLR), 2020.

[Hwa80]

C.-R. Hwang. Laplace’s method revisited: weak convergence of probability measures. The Annals of Probability, pages 1177–1182, 1980.

[HZRS16] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770–778, 2016.

[JGN+17]

C. Jin, R. Ge, P. Netrapalli, S. M. Kakade, and M. I. Jordan. How to escape saddle points eﬃciently. In Proceedings of the 34th International Conference on Machine Learning, pages 1724–1732. JMLR. org, 2017.

[JKA+17] S. Jastrzebski, Z. Kenton, D. Arpit, N. Ballas, A. Fischer, Y. Bengio, and A. Storkey. Three factors inﬂuencing minima in SGD. arXiv preprint arXiv:1711.04623, 2017.

[JKB+18]

S. Jastrzebski, Z. Kenton, N. Ballas, A. Fischer, Y. Bengio, and A. Storkey. On the relation between the sharpest directions of DNN loss and the SGD step length. arXiv preprint arXiv:1807.05031, 2018.

[Jor18]

M. I. Jordan. Dynamical, symplectic and stochastic perspectives on gradient-based optimization. In Proceedings of the International Congress of Mathematicians, Rio de Janeiro, volume 1, pages 523–550, 2018.

[KB14]

D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.

[KB17]

W. Krichene and P. L. Bartlett. Acceleration and averaging in stochastic descent dynamics. In Advances in Neural Information Processing Systems, pages 6796–6806, 2017.

[KCD08] P. Kundu, I. Cohen, and D. Dowling. Fluid Mechanics (Fourth Edition). Elsevier, 2008.

[KMN+16] N. S. Keskar, D. Mudigere, J. Nocedal, M. Smelyanskiy, P. Tang, and P. Tak. On largebatch training for deep learning: Generalization gap and sharp minima. arXiv preprint arXiv:1609.04836, 2016.

[KP92]

P. E. Kloeden and E. Platen. The approximation of multiple stochastic integrals. Stochastic Analysis and Applications, 10(4):431–441, 1992.

[Kri09]

A Krizhevsky. Learning multiple layers of features from tiny images. Master’s thesis, University of Toronto, 2009.

[Kro93]

A. S. Kronfeld. Dynamics of Langevin simulations. Progress of Theoretical Physics Supplement, 111:293–311, 1993.

[KY03]

H. Kushner and G. G. Yin. Stochastic approximation and recursive algorithms and applications, volume 35. Springer Science & Business Media, 2003.

[LA19]

Z. Li and S. Arora. An exponential learning rate schedule for deep learning. arXiv preprint arXiv:1910.07454, 2019.

[LH16]

I. Loshchilov and F. Hutter. SGDR: Stochastic gradient descent with warm restarts. arXiv preprint arXiv:1608.03983, 2016.

[Lio82]

P.-L. Lions. Generalized Solutions of Hamilton-Jacobi Equations, volume 69. London Pitman, 1982.

[LSJR16] J. D. Lee, M. Simchowitz, M. I. Jordan, and B. Recht. Gradient descent only converges to minimizers. In Conference on Learning Theory, pages 1246–1257, 2016.

31

[LTE17]
[LWM19]
[MHB16] [Mic19] [Mil75] [Mil86] [MV99]
[Nie04] [Pav14] [PT85] [RRT17]
[SBC16]
[SDJS18] [SKYL17] [Smi17] [SS19] [SSG19] [Sun19] [Tal82]

Q. Li, C. Tai, and W. E. Stochastic modiﬁed equations and adaptive stochastic gradient algorithms. In Proceedings of the 34th International Conference on Machine Learning, volume 70, pages 2101–2110. JMLR. org, 2017.
Y. Li, C. Wei, and T. Ma. Towards explaining the regularization eﬀect of initial large learning rate in training neural networks. In Advances in Neural Information Processing Systems, pages 11669–11680, 2019.
S. Mandt, M. Hoﬀman, and D. Blei. A variational analysis of stochastic gradient algorithms. In International Conference on Machine Learning, pages 354–363, 2016.
L. Michel. About small eigenvalues of Witten Laplacian. Pure and Applied Analysis, 1(2), 2019.
G. N. Mil’shtein. Approximate integration of stochastic diﬀerential equations. Theory of Probability & Its Applications, 19(3):557–562, 1975.
G. N. Mil’shtein. Weak approximation of solutions of systems of stochastic diﬀerential equations. Theory of Probability & Its Applications, 30(4):750–766, 1986.
P. A. Markowich and C. Villani. On the trend to equilibrium for the Fokker-Planck equation: An interplay between physics and functional analysis. In Physics and Functional Analysis, Matematica Contemporanea (SBM) 19, pages 1–29, 1999.
F. Nier. Quantitative analysis of metastability in reversible diﬀusion processes via a Witten complex approach. Journ´ees Equations aux D´eriv´ees Partielles, pages 1–17, 2004.
G. Pavliotis. Stochastic Processes and Applications: Diﬀusion Processes, the Fokker–Planck and Langevin Equations, volume 60. Springer, 2014.
E. Pardoux and D. Talay. Discretization and simulation of stochastic diﬀerential equations. Acta Applicandae Math, 3:23–47, 1985.
M. Raginsky, A. Rakhlin, and M. Telgarsky. Non-convex learning via stochastic gradient Langevin dynamics: A nonasymptotic analysis. In Conference on Learning Theory, pages 1674– 1703, 2017.
W. J. Su, S. Boyd, and E. Cand`es. A diﬀerential equation for modeling Nesterov’s accelerated gradient method: theory and insights. The Journal of Machine Learning Research, 17(1):5312– 5354, 2016.
B. Shi, S. Du, M. Jordan, and W. J. Su. Understanding the acceleration phenomenon via high-resolution diﬀerential equations. arXiv preprint arXiv:1810.08907, 2018.
S. L. Smith, P.-J. Kindermans, C. Ying, and Q. V. Le. Don’t decay the learning rate, increase the batch size. arXiv preprint arXiv:1711.00489, 2017.
L. N. Smith. Cyclical learning rates for training neural networks. In 2017 IEEE Winter Conference on Applications of Computer Vision (WACV), pages 464–472. IEEE, 2017.
M. Sordello and W. J. Su. Robust learning rate selection for stochastic optimization via splitting diagnostic. arXiv preprint arXiv:1910.08597, 2019.
U. Simsekli, L. Sagun, and M. Gurbuzbalaban. A tail-index analysis of stochastic gradient noise in deep neural networks. arXiv preprint arXiv:1901.06053, 2019.
R. Sun. Optimization for deep learning: theory and algorithms. arXiv preprint arXiv:1912.08957, 2019.
D. Talay. Analyse num´erique des ´equations diﬀ´erentielles stochastiques. PhD thesis, Universit´e Aix-Marseille I, 1982.

32

[Tal84]
[TH12] [Vil06] [Vil09] [WWJ16] [YLWJ19] [ZBH+16] [Zei12] [ZLC17]

D. Talay. Eﬃcient numerical schemes for the approximation of expectations of functionals of the solution of a SDE and applications. In Filtering and Control of Random Processes, pages 294–313. Springer, 1984.
T. Tieleman and G. Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural Networks for Machine Learning, 4(2):26–31, 2012.
C. Villani. Hypocoercive diﬀusion operators. In International Congress of Mathematicians, volume 3, pages 473–498, 2006.
C. Villani. Hypocoercivity. Memoirs of the American Mathematical Society, 202(950), 2009.
A. Wibisono, A. C. Wilson, and M. I. Jordan. A variational perspective on accelerated methods in optimization. proceedings of the National Academy of Sciences, 113(47):E7351–E7358, 2016.
K. You, M. Long, J. Wang, and M. I. Jordan. How does learning rate decay help modern neural networks? arXiv preprint arXiv:1908.01878, 2019.
C. Zhang, S. Bengio, M. Hardt, B. Recht, and O. Vinyals. Understanding deep learning requires rethinking generalization. arXiv preprint arXiv:1611.03530, 2016.
M. D. Zeiler. Adadelta: An adaptive learning rate method. arXiv preprint arXiv:1212.5701, 2012.
Y. Zhang, P. Liang, and M. Charikar. A hitting time analysis of stochastic gradient Langevin dynamics. In Conference on Learning Theory, pages 1980–2022, 2017.

33

A Technical Details for Sections 1 and 2

A.1 Approximating diﬀerential equations

Figure 12 presents a diagram that shows approximating surrogates for GD, SGD, and SGLD at

multiple scales. X˙ = −∇f (X),

In the case of SGD, for example, th√e inclusion of only O(1) terms leads to the whereas the inclusion of up to O( s) terms leads to the lr-dependent SDE

ODE (1.2).

√ For√GD and SGLD, O( s) terms are not found in the expansion as in the derivation of (1.2). The

O( s)-approximation, therefore, leads to the same diﬀerential equation as the O(1)-approximation

for both GD and SGLD.

GD: xk+1 = xk − s∇f (xk)

O(1)-approximation O(√s)-approximation

Gradient ﬂow: X˙ = −∇f (X)

O(1)-approximation

SGD: xk+1 = xk − s∇f (xk) + sξk

O(√s)-approximation

lr-dependent SD√E: dX = −∇f (X)dt + sdW

SGLD:

√

xk+1 = xk − s∇f (xk) + sξk

O(1)-approximation O(√s)-approximation

SDE: dX = −∇f (X)dt + dW

Figure 12: Diagram showin√g the relationship between three discrete algorithms and their O(1)approximating and O(1) + O( s)-approximating diﬀerential equations. Note that the inclusion of only O(1)-terms does not distinguish between GD and SGD.

A.2 Derivation of the Fokker–Planck–Smoluchowski equation

To derive the lr-dependent Fokker–Planck–Smoluchowski equation (2.1), we ﬁrst state the following lemma.
Lemma A.1 (Itˆo’s lemma). For any f ∈ C∞(Rd) and g ∈ C∞([0, +∞) × Rd), let Xs(t) be the solution to the lr-dependent SDE (1.2). Then, we have

∂g

s

√ d ∂g

dg(t, Xs(t)) = ∂t − ∇f · ∇g + 2 ∆g dt + s

∂xi dW.

i=1

From this lemma, we get

dE[g(t, Xs(t))|Xs(t′)] = ∂E[g(t, Xs(t))|Xs(t′)] − ∇f · ∇E[g(t, Xs(t))|Xs(t′)]

dt

∂t

(A.1)

34

+ s ∆E[g(t, Xs(t))|Xs(t′)], 2

(A.2)

for t ≥ t′. Setting vs(t′, x) = E[g(t, Xs(t))|Xs(t′) = x], from (A.2) we see that vs(t′, x) satisﬁes the

following diﬀerential equation:

 

∂vs

s

∂t′ = ∇f · ∇vs − 2 ∆vs

 vs(t, x) = g(t, x).

(A.3)

Recognizing the invariance of translation of time and letting us(t − t′, x) = vs(t′, x), we can reduce

(A.3) to the following backward Fokker–Planck–Smoluchowski equation:

 

∂us

s

∂t = −∇f · ∇us + 2 ∆us

 us(0, x) = g(t, x).

(A.4)

Next, from the Chapman–Kolmogorov equation, we get

ρs(t, x) = ρs(t, x|0, y)ρs(0, y)dy,
Rd
and by switching the order of the integration, we obtain

us(0, x)ρs(t, x)dx = g(x)ρs(t, x)dx

Rd

Rd

= g(x)

ρs(t, x|0, y)ρs(0, y)dy dx

Rd

Rd

= ρs(0, y)

g(x)ρs(t, x|0, y)dx dy

Rd

Rd

= ρs(0, y)us(t, y)dy
Rd

= ρs(0, x)us(t, x)dx.
Rd

(A.5)

Making use of the backward Fokker–Planck–Smoluchowski equation (A.4) and switching the order of integration (A.5), we get

us(0, x) ∂ρs(t, x) dx = d us(0, x)ρs(t, x)dx

Rd

∂t t=0

dt Rd

t=0

d = dt Rd us(t, x)ρs(0, x)dx t=0

= ∂us(t, x) ρs(0, x)dx Rd ∂t t=0

s = Rd −∇f (x) · ∇us(0, x) + 2 ∆us(0, x) ρs(0, x)dx

s = Rd us(0, x) ∇ · (ρs(0, x)∇f (x)) + 2 ∆ρs(0, x) dx.

Hence, we derive the forward Fokker–Planck–Smoluchowski equation at t = 0 for an arbitrary smooth function us(0, x) = g(t, x). Noting that t = 0 can be replaced by any time t, we complete the derivation of the Fokker–Planck–Smoluchowski equation.

35

A.3 The uniqueness of Gibbs invariant distribution
We begin by proving that the probability density µs is an invariant distribution of (2.1). Plugging 2
∇µs = − s (∇f ) µs into (2.1) gives

∇ · (µs∇f ) = ∇µs · ∇f + µs∆f = − 2 ∇f 2µs + (∆f )µs s

(A.6)

and ∆µs = − 2s ∇f · ∇µs − 2s µs∆f = s42 ∇f 2µs − 2s µs∆f.
Combining (A.6) and (A.7) yields

(A.7)

s ∇ · (µs∇f ) + 2 ∆µs = 0.

We now proceed to show that the probability density µs is unique. To derive a contradiction,

we assume that there exists another distribution ϑs satisfying the Fokker–Planck–Smoluchowski

equation:

s ∇ · (ϑs∇f ) + 2 ∆ϑs = 0.

(A.8)

Write ̟s = ϑsµ−s 1 and recall the operator Ls deﬁned in Section 5.1. We can rewrite (A.8) as

Ls̟s = 0.

Using Lemma 5.3, we have

0 = (Ls̟s)̟sdµs = − s

∇̟s 2dµs ≤ 0.

Rd

2 Rd

Hence, ̟s must be a constant on Rd. Furthermore, since both µs and ϑs are probability densities, it must be the case that ̟s ≡ 1. In other words, ϑs is identical to µs. The proof is complete.

A.4 Proof of Lemma 2.2
Recall that Section 6.1 shows that the transition probability density ρs(t, x) in C1([0, +∞), L2(µ−s 1)) governed by the Fokker–Planck–Smoluchowski equation (2.1) is equivalent to the function ψs(t, x) in C1([0, +∞), L2(Rd)) governed by (6.1). Moreover, in Section 6.1, we have shown that the spectrum of the Schro¨dinger operator −s∆ + Vs satisﬁes

0 = ζs,0 < ζs,1 ≤ · · · ≤ ζs,ℓ ≤ · · · < +∞.

Since L2(Rd) is a Hilbert space, there exists a standard orthogonal basis corresponding to the

spectrum of −s∆ + Vs:

µs = φs,0, φs,1, . . . , φs,ℓ, . . . ∈ L2(Rd).

36

Then, for any initialization ψs(0, x) ∈ L2(Rd), there exist constants cℓ (ℓ = 1, 2, . . .) such that

√

+∞

ψs(0, ·) = µs + cℓφs,ℓ.

ℓ=1

Thus, the solution to the partial diﬀerential equation (6.1) is

ψs(t, ·) = √µs + +∞ cℓe−ζs,ℓtφs,ℓ.
ℓ=1
Recognizing the transformation ψs(t, ·) = ρs(t, ·)/√µs, we recover

ρs(t, ·) = µs + +∞ cℓe−ζs,ℓtφs,ℓ√µs.
ℓ=1

Note that ζs,ℓ is positive for ℓ ≥ 1. Thus, the proof is ﬁnished.

B Technical Details for Section 3

B.1 Proof of Proposition 3.4

Here, we prove Proposition 3.4 using the Bakry–Emery theorem, which is a Poincar´e-type inequality for µ-strongly convex functions. As a direct consequence of this theorem, the exponential decay constant for strongly convex objectives does not depend on the learning rate s and the ambient dimension d.

Lemma B.1 (Bakry–Emery theorem). Let f be an inﬁnitely diﬀerentiable function deﬁned on Rd. If f is µ-strongly convex, then the measure dµs satisﬁes the Poincar´e-type inequality as in
Lemma 5.4 with λs = µ; that is, for any smooth function h with a compact support,

h2dµs −
Rd

2s hdµ ≤

∇h 2dµ .

s Rd

2µ Rd

s

Lemma B.1 serves as the main technical tool in the proof of Proposition 3.4. Its proof is in Appendix B.1.1. Now, we prove the following result using Lemma B.1.

Lemma B.2. Under the same assumptions as in Proposition 3.4, ρs(t, ·) converges to the Gibbs distribution µs in L2(µ−s 1) at the rate

ρs(t, ·) − µs µ−1 ≤ e−µt ρs − µs µ−1 .

s

s

(B.1)

Proof of Lemma B.2. It follows from (5.3) that

d

ρs(t, ·) − µs

2
−1

= −s

∇hs 2dµs.

dt

µs

Rd

Next, using Lemma B.1 and recognizing the equality Rd hsdµs = Rd ρs(t, x)dx = 1, we get

d

ρs(t, ·) − µs

2
−1

≤ −2µ

dt

µs

h2sdµs −
Rd

2
hsdµs
Rd

37

= −2µ

h2sdµs − 1
Rd

= −2µ = −2µ

(hs − 1)2dµs
Rd

ρs(t, ·) − µs

2 µ−1

.

s

Integrating both sides yields (B.1), as desired.

Leveraging Lemma B.2, we proceed to complete the proof of Proposition 3.4.

Proof of Proposition 3.4. Using Lemma B.2, we get

|E f (Xs(t)) − E f (X(∞))| = f (x) (ρs(t, x) − µs(x)) dx
Rd

= (f (x) − f ⋆) (ρs(t, x) − µs(x)) dx
Rd

1

1

≤

(f (x) − f ⋆)2µs(x)dx 2

(ρs(t, x) − µs(x))2 µ−s 1dx 2

Rd

Rd

≤ C(s)e−µt ρ − µs µ−1 , s

where the ﬁrst inequality applies the Cauchy-Schwarz inequality and

C(s) =

1
(f − f ⋆)2µsdx 2
Rd

is an increasing function of s.

B.1.1 Proof of Lemma B.1

We introduce two operators Γs and Γs,2 that are built on top of the linear operator Ls deﬁned in (5.2). For any g1, g2 ∈ L2(µs), let

1 Γs(g1, g2) = 2 [Ls(g1g2) − g1Lsg2 − g2Lsg1]

(B.2)

and 1 Γs,2(g1, g2) = 2 [LsΓs(g1, g2) − Γs(g1, Lsg2) − Γs(g2, Lsg1)] .
A simple relationship between the two operators is described in the following lemma.

(B.3)

Lemma B.3. Under the same assumptions as in Lemma B.1, for any g ∈ L2(µs) we have

Γs,2(g, g) ≥ µΓs(g, g).

Proof of Lemma B.3. Note that s
Ls(g1g2) = −g1(∇f · ∇g2) − g2(∇f · ∇g1) + 2 (g1∆g2 + g2∆g1 + 2∇g1 · ∇g2)

38

and

s

g1Lsh2 = −g1∇f · ∇g2 + 2 g1∆g2,

Then, the operator Γs must satisfy

s g2Lsg1 = −g2∇f · ∇g1 + 2 g2∆g1.

s Γs(g, g) = 2 (∇g · ∇g) .

(B.4)

Next, together with the equality

1 ∆( ∇g 2) = ∇g · ∇(∆g) + Tr[(∇2g)T (∇2g)], 2

we obtain that the operator Γs,2 satisﬁes

Γs,2(g, g) = s (∇g)T ∇2f (∇g) + s2 Tr[(∇2g)T (∇2g)],

2

4

(B.5)

where Tr is the standard trace of a squared matrix. Recognizing that the objective f is µ-strongly convex, a comparison between (B.4) and (B.5) completes the proof.

Recall that hs(t, ·) ∈ L2(µs) is the solution to the partial diﬀerential equation (5.1), with the initial condition hs(0, ·) = h. Deﬁne

Λ1,s(t) = h2s(t, ·)dµs.
Rd
The following lemma considers the derivatives of Λ1,s(t).
Lemma B.4. Under the same assumptions as in Lemma B.1, we have   Λ˙ 1,s(t) = −2 Γs(hs, hs)dµs
Rd
 Λ¨ 1,s(t) = 4 Γs,2(hs, hs)dµs.
Rd
Proof of Lemma B.4. Taking together (5.3) and (B.4), we have

(B.6) (B.7)

Γs(hs, hs)µsdµs = − hsLshsdµs.

Rd

Rd

Since hs(t, ·) ∈ L2(µs) is the solution to the partial diﬀerential equation (5.1), we get

Λ˙ 1,s(t) = 2 hsLshsdµs = −2 Γs(hs, hs)dµs.

Rd

Rd

Furthermore, by the deﬁnition of Γs,2 and integration by parts, we have14

Γs,2(hs, hs)dµs = (Lshs)2dµs.

Rd

Rd

14See the calculation in [BGL13].

39

From Lemma 5.3, we know that the linear operator Ls is self-adjoint. Then, we obtain the second derivative as

Λ¨ 1,s(t) = 2 (Lshs)2dµs + 2 hsLs2hsdµs = 4 Γs,2(hs, hs)dµs.

Rd

Rd

Rd

Finally, we complete the proof of Lemma B.1.

Proof of Lemma B.1. Using Lemma B.3 and Lemma B.4, we obtain the following inequality:

Λ¨ 1,s(t) ≥ −2µΛ˙ 1,s(t).

(B.8)

From the deﬁnition of Λ1,s(t), we have

Λ1,s(0) − Λ1,s(∞) = h2dµs −
Rd

2
hdµs ,
Rd

where the second term on the right-hand side follows from Lemma 5.2 and

hdµs = ρdx = 1.

Rd

Rd

By Lemma 5.2, we get hs(∞, ·) ≡ 1, which together with (B.7) gives

Λ˙ 1,s(0) − Λ˙ 1,s(∞) = −2 Γs(h, h)dµs = −s

∇h 2dµs.

Rd

Rd

The ﬁnal equality follows from (B.4). Integrating both sides of the inequality (B.8), we have

−2µ (Λ1,s(0) − Λ1,s(∞)) ≤ Λ˙ 1,s(0) − Λ˙ 1,s(∞),

which completes the proof.

B.2 Convergence in L1(Rd)
Lemma B.2 can be extended to the more general and natural function space L1(Rd). From Lemma 5.1, we know that L2(µ−s 1) ⊂ L1(Rd). This is formulated in the following lemma.

Lemma B.5. Under the same assumptions as in Proposition 3.4, ρs(t, ·) converges to the Gibbs distribution µs in L1(Rd) at the rate15

ρs(t, ·) − µs L1(Rd) ≤ √2e−µtH(ρ|µs),

(B.9)

where the relative entropy is

ρ

H(ρ|µs) = ρ log
Rd

µs

dx.

15Note that the L1(Rd) norm, ρs(t, ·) − µs L1(Rd), is deﬁned as

ρs(t, ·) − µs L1(Rd) = |ρs(t, ·) − µs|dx. Rd

40

Similarly, the proof of Lemma B.5 will be based on the following log-Sobolev type inequality.

Lemma B.6 (log-Sobolev inequality). Let f be an inﬁnitely diﬀerentiable function deﬁned on Rd. If f is µ-strongly convex, then the measure dµs satisﬁes a log-Sobolev inequality. That is,

Ent[h2] ≤ s

∇h 2dµs,

µ Rd

(B.10)

where the entropy is deﬁned as

Ent[h2] = h2 log h2dµs − h2dµs log h2dµs .

Rd

Rd

Rd

Proof of Theorem B.5. By the Csisz´ar–Kullback inequality, we have

ρs(t, ·) − µs

2 L1 (Rd )

≤

2H (ρs (t,

·)|µs)

=

2

hs log hsdµs.

Rd

(B.11)

From Lemma B.7, we have

dH(ρs(t, ·)|µs) = − hsΓs(log hs, log hs)dµs = −2s ∇ hs 2dµs.

dt

Rd

Rd

(B.12)

Finally, using Lemma B.6, we obtain the estimate for the derivative of the entropy (B.12) as

dH(ρs(t, ·)|µs) ≤ −2s (hs log hs)µsdx = −2µH(ρs(t, ·)|µs),

dt

Rd

which completes the proof.

B.2.1 Proof of Lemma B.6 We consider the integral

Λ2,s(t) = H(ρs(t, ·)|µs) = hs(t, ·) log(hs(t, ·)dµs,
Rd

(B.13)

with hs(0, x) = h2(x). We now proceed to ﬁnd the derivatives of Λ2,s(t) with respect to time t, which is formulated as the following lemma.

Lemma B.7. Under the same assumptions as in Lemma B.6, we have   Λ˙ 2,s(t) = − hsΓs(log hs, log hs) dµs
Rd
 Λ¨ 2,s(t) = 2 hs(Γs,2(log hs, log hs)dµs.
Rd

(B.14)

Proof of Lemma B.14. Recall the linear operator Ls deﬁned in (5.2). Using integration by parts,

we have

s Rd (1 + log hs) Lshsdµs = − 2 Rd ∇ (1 + log hs) · ∇hsdµs.

41

Since hs(t, ·) is the solution to the partial diﬀerential equation (5.1), we obtain the ﬁrst derivative as

Λ˙ 2,s(t) = (1 + log hs) Lshsdµs = − hsΓs(log hs, log hs)dµs.

Rd

Rd

Furthermore, recognizing the deﬁnitions of Γs,2, Γs and Ls, we have

  Ls

Γs(hs, hs) hs

= − Γs(hs, hs)Lshs + LsΓs(hs, hs) − Γs(hs, Γs(hs, hs)) + 2(Γs(hs, hs))2

h2s

hs

h2s

h3s

 Γs,2(log hs, log hs) = Γs,2(hs, hs) − Γs(hs, Γs(hs, hs) + (Γs,2(hs, hs))2 .

h2s

h3s

h4s

On the other hand, the integral with the measure dµs for the linear operator Ls acting on

γs(hs, hs)/hs is zero; that is,

Ls
Rd

Γs(hs, hs) hs

dµs = 0.

Then the ﬁrst equality above by the deﬁnitions can be written as

Γs(hs, hs)Lshs dµs =

Rd

h2s

Rd

LsΓs(hs, hs) Γs(hs, Γs(hs, hs)) 2(Γs(hs, hs))2

hs −

h2s

+

h3

s

dµs.

Finally, we obtain the second derivative as

Λ¨ 2,s(t) =

Γs(hs, hs)Lshs(t, x) − 2Γs(hs(t, x), Lshs(t, x)) dµs

Rd

(hs(t, x))2

hs(t, x)

=2

Γs,2(hs, hs) − Γs(hs, Γs(hs, hs))) + 2(Γs(hs, hs))2 dµs

Rd

hs

h2s

h3s

= 2 hs(Γs,2(log hs, log hs)dµs.
Rd

This proof is complete.

Next, we complete the proof of Lemma B.6.

Proof of Lemma B.6. Using Lemma B.3 and Lemma B.7, we obtain

Λ¨ 2,s(t) ≥ −2µΛ˙ 2,s(t).

Since Λ˙ 2,s(t) is positive, we get

Λ˙ 2,s(t) ≥ Λ˙ 2,s(0)e−2µt.

By the deﬁnition of Λ2,s(t), we have

Λ2,s(0) − Λ2,s(∞) = h2 log h2dµs −
Rd

h2dµs log
Rd

h2dµs ,
Rd

where the second term in the right-hand side follows from Lemma 5.2 and

hdµs = ρdx = 1.

Rd

Rd

42

(B.15)

By Lemma 5.2, we know that hs(∞, ·) ≡ 1. Plugging it into (B.14), we get

Λ˙ 2,s(0) = − h2Γs(log h2, log h2)dµs = −2s

∇h 2dµs.

Rd

Rd

where the last equality follows from (B.4). Integrating both sides of the inequality (B.15), we have

Λ2,s(0) − Λ2,s(∞) ≤ −Λ˙ 2,s(0) This concludes the proof.

+∞
e−2µt dt

≤

−

1

Λ˙ 2,s(0).

0

2µ

B.3 Proof of Proposition 3.5
By Lemma 2.2, let ρs(t, ·) ∈ C1([0, +∞), L2(µ−s 1)) denote the unique transition probability density of the solution to the lr-dependent SDE. Taking an expectation, we get

E[Xs(t)] = xρs(t, x)dx.
Rd
Hence, the uniqueness has been proved. Using the Cauchy–Schwarz inequality and Theorem 5.5, we obtain:

E[Xs(t)]

≤

x(ρs(t, ·) − µs)dx +

xµsdx

Rd

Rd

1

≤

x 2µsdx 2 ρs(t, ·) − µs µ−1 + 1

Rd

s

1

≤

x 2dµs 2 e−λst ρ − µs µ−1 + 1

Rd

s

< +∞,

where the integrability Rd x 2µs(x)dx is due to the fact that the objective f satisﬁes the Villani condition. The existence of a global solution to the lr-dependent SDE (1.2) is thus established.
For the strong convergence, the lr-dependent SDE (1.2) corresponds to the Milstein scheme in numerical methods. The original result is obtained by Milstein [Mil75] and Talay [Tal82, PT85], independently. We refer the readers to [KP92, Theorem 10.3.5 and Theorem 10.6.3], which studies numerical schemes for stochastic diﬀerential equation. For the weak convergence, we can obtain numerical errors by using both the Euler-Maruyama scheme and Milstein scheme. The original result is obtained by Milstein [Mil86] and Talay [PT85, Tal84] independently and [KP92, Theorem 14.5.2] is also a well-known reference. Furthermore, there exists a more accurate estimate of B(T ) shown in [BT96]. The original proofs in the aforementioned references only assume ﬁnite smoothness such as C6(Rd) for the objective function.

B.4 Connection with vanishing viscosity

Taking s = 0, the zero-viscosity steady-state equation of the Fokker–Planck–Smoluchowski equa-

tion (2.1) reads

∇ · (µ0∇f ) = 0.

(B.16)

43

A solution to this zero-viscosity steady-state equation takes the form

m
µ0(x) = ciδ(x − xi),
i=1

with

m
ci = 1,
i=1

(B.17)

where xi’s are critical points of the objective f . As is clear, the solution is not unique. However, we have shown previously that the invariant distribution µs is unique and converges to

µs→0(x) = δ(x − x⋆)

in the sense of distribution, which is a special case of (B.17). Clearly, when there exists more than one critical point, µs→0(x) is diﬀerent from µ0(x) in general. In contrast, µs→0(x) and µ0(x) must be the same for (strictly) convex functions. In light of this comparison, the correspondences between the case s > 0 and the case s = 0 are fundamentally diﬀerent in nonconvex and convex problems.
Next, we consider the rate of convergence in the convex setting. Let

f (x) = 1 θx2, 2

where θ > 0. Plugging into the Fokker-Planck-Smoluchowski equation (2.1), we have

 

∂ρs

∂(xρs) s ∂2ρs

∂t = θ ∂x + 2 ∂x2

 ρ(0, ·) = ρ ∈ L2( sπ/θeθx2/s)).

(B.18)

The solution to (B.18) is

ρs(t, x) =

θ

θ x − x0e−θt 2

πs (1 − e−2θt) exp − s 1 − e−2θt .

(B.19)

For any φ(x) ∈ L2( sπ/θeθx2/s), we have

ρs, φ =

θ πs (1 − e−2θt) exp

θ x − x0e−θt 2 − s 1 − e−2θt

, φ(x)

=

√1

e−

x2 2

,

φ

2π

→ φ x0e−θt =

s(1 − e−2θt) · x + x0e−θt 2θ
δ(x − x0e−θt), φ(x)

as s → 0, where δ(x − x0e−θt) denotes the solution to the following zero-viscosity equation

∂ρ0 = ∇ · (ρ0∇f ) . ∂t

(B.20)

Furthermore, using the following inequality

φ s(1 − e−2θt) · x + x0e−θt − φ x0e−θt

√ ≤O s ,

2θ

∞

44

we get ρ(t, x), ψ(x) → δ(x − x0e−θt), ψ(x) at the rate O (√s) for a test function ψ.

The phenomenon presented above is called singular perturbation. It appears in mathematical

models of boundary layer phenomena [CM90, Chapter 2.2, Example 1 and Example 2], WKB theory

for Schro¨dinger equations [Gas07, Supplement 4A], KAM theory for circle diﬀeomorphisms [Arn12,

Chapter 2, Section 11] and that for Hamilton systems [Arn13, Appendix 8]. Moreover, the sin-

gular perturbation phenomenon shows that there exists a fundamental distinction between the

O(1)-approximating ODE for SGD and the lr-dependent SDE (1.2). In particular, the learning

rate s → 0 in the Fokker–Planck–Smoluchowski equation (2.1) corresponds to vanishing viscosity.

The vanishing viscosity phenomenon was originally observed in ﬂuid mechanics [CM90, KCD08],

particularly in the degeneration of the Navier–Stokes equation to the Euler equation [CF99]. As

a milestone, the vanishing viscosity method has been used to study the Hamilton–Jacobi equa-

tion [CL83, Eva80, CEL84]. In fact, the Fokker–Planck–Smoluchowski equation (2.1) and its sta-

tionary equation are a form of Hamilton–Jacobi equation with a viscosity term, for which the

Hamiltonian is

H(x, ρ, ∇ρ) = ∆f ρ + ∇f · ∇ρ.

(B.21)

The Hamiltonian (B.21) is diﬀerent from the classical case [Lio82, CS04, Eva10], which is generally nonlinear in ∇ρ (cf. Burger’s equation). Although the Hamiltonian depends linearly on ρ and ∇ρ, the coeﬃcients depend on ∆f and ∇f . Hence, it is not reasonable to apply directly the wellestablished theory of Hamilton–Jacobi equations [CL83, Eva80, CEL84, Lio82, CS04, Eva10] to the Fokker–Planck–Smoluchowski equation (2√.1) and its stationary equation. Furthermore, for the aforementioned example, which proves the O( s) convergence for the Fokker–Planck–Smoluchowski equation with the quadratic potential f (x) = 2θ x2, is also a viscosity solution to the Hamilton–Jacobi equation [CL83], since the Hamiltonian (B.21) for the quadratic potential degenerates to

H(x, ρ, ∇ρ) = 2tr(A)ρ + 2Ax · ∇ρ,
where f (x) = xT Ax and A is positive deﬁnite and symmetric. Thus, we remark that the general theory of viscosity solutions to Hamilton–Jacobi equations cannot be used directly to prove the theorems in the main body of this paper.
In closing, we present several open problems.

• Consider the stationary solution µs(x) to the Fokker–Planck–Smoluchowski equation (2.1). For a convex or strongly convex objective f with Lipschitz √gradients, can we quantify the rate of convergence? Does the rate of convergence remain O( s)?
• Let T > 0 be ﬁxed and consider the solution ρs(t, x) to the Fokker–Planck–Smoluchowski equation (2.1) in [0, T ]. For a convex or strongly convex objective f with Lipschitz gradients, does the solution to the Fokker–Planck–Smoluchowski equation (2.1) con√verge to the solution to its zero-viscosity equation (B.20)? Is the rate of convergence still O( s)?
• Consider the solution ρs(t, x) to the Fokker–Planck–Smoluchowski equation (2.1) in [0, +∞). For a convex or strongly convex objective f with Lipschitz gradients, does the global solution to the Cauchy problem of the Fokker–Planck–Smoluchowski equation (2.1) con√verge to the solution of its zero-viscosity equation (B.20)? Is the rate of convergence still O( s)?

45

C Technical Details for Section 5

C.1 Proof of Lemma 5.1
From the Cauchy–Schwarz inequality, we get

|g(x)|dx =
Rd
≤

|g

(x)|e

f

(x) s

e−

f

(x) s

dx

Rd

1

g

2

(x)e

2f

(x) s

dx

2

Rd

< +∞.

1

e−

2f (x) s

dx

2

Rd

This completes the proof.

C.2 Proof of Lemma 5.3

Recall that the linear operator Ls in (5.2) is deﬁned as

s Ls = −∇f · ∇ + 2 ∆f.

Note that we have

s Rd (Lsg1) g2dµs = Rd −∇g1 · ∇f + 2 ∆g1 g2dµs

=− 1

(∇g1

·

∇f

)g2e−

2f s

dx

+

s

Zs Rd

2Zs

=− 1

(∇g1

·

∇f

)g2e−

2f s

dx

−

s

Zs Rd

2Zs

=− s

(∇g1

·

∇g2)e−

2f s

dx

2Zs Rd

s = − 2 Rd(∇g1 · ∇g2)dµs.

(∆g1)g2e−

2f s

dx

Rd

∇g1

·

∇(g2e−

2f s

)dx

Rd

Therefore, Ls is self-adjoint in L2(µs) and is non-positive.

C.3 Proof of Lemma 5.4
For completeness, we show below the original proof of Lemma 5.4 from [Vil09] in detail. Let Vs = ∇f 2/s − ∆f , then for any h ∈ Cc∞(Rd) with mean-zero condition

hdµs = 0,
Rd
we can obtain the following key inequality [DS01]

(C.1)

Vsh2dµs ≤ s

∇h 2µsdµs.

Rd

Rd

(C.2)

46

To show (C.2), note that

0≤

∇

he−

f s

Rd

2
dx

=

(∇

h)e−

f s

−

h

(∇

f

)e−

f s

2
dx

Rd

s

=

∇h

2 e−

2f s

dx

−

2

h∇h

·

∇f

e−

2f s

dx

+

Rd

s Rd

12

h2

∇f

2e−

2f s

dx

s

Rd

=

∇h

2 e−

2f s

dx

−

1

(∇h2

·

∇f

)e−

2f s

dx

+

Rd

s Rd

12

h2

∇f

2e−

2f s

dx

s

Rd

=

∇h

2 e−

2f s

dx

+

1

h2∇ ·

(∇

f

)e−

2f s

Rd

s Rd

dx +

12

h2

∇f

2e−

2f s

dx

s

Rd

=

∇h

2 e−

2f s

dx

+

1

(h2

∆f

)e−

2f s

dx

−

Rd

s Rd

12

h2

∇f

2e−

2f s

dx.

s

Rd

Recognizing

µs

∝

e−

2f s

,

this

proves

(C.2).

Let R0,s > 0 be large enough such that Vs(x) > 0 for

ǫs as

1 ǫs(Rs) := inf{Vs(x) : x

x ≥ R0,s. ≥ Rs} .

For Rs > R0,s, we can deﬁne (C.3)

Then ǫ(Rs) → 0 as Rs → ∞. Furthermore, we assume the Rs is large enough such that

1 x ≤Rs dµs ≥ 2 .

(C.4)

From the key inequality (C.2), we obtain that

h2dµs ≤ ǫ(Rs) s

∇h 2dµs − ( inf Vs(x)) h2dµs .

|x|≥Rs

Rd

x∈Rd

Rd

Let BRs be the ball centered at the origin of radius Rs in Rd and deﬁne

µs,Rs =

dµs
|x|≤Rs

−1
µs1|x|≤Rs .

(C.5)

Using the Poincar´e inequality in a bounded domain [Eva10, Theorem 1, Chapter 5.8], we get

h2dµs,Rs ≤ sC(Rs)

∇h 2µs,Rsdµs,Rs +

x∈Rd

x∈Rd

2
hdµs,Rs ,
x∈Rd

where C(Rs) is a constant depending on Rs. Furthermore, using the inequality (C.4), we obtain

h2dµs ≤ sC(Rs)
x ≤Rs

∇h 2dµs + 2
x ≤Rs

2
hdµs .
x ≤Rs

(C.6)

47

Making use of the mean-zero property of h, we have

2
hdµs =
x ≤Rs

2
hdµs ≤
x >Rs

h2dµs.
x >Rs

Combining (C.6) and (C.7), we get

(C.7)

h2dµs ≤ sC(Rs)

∇h 2dµs + 3

x∈Rd

x∈Rd

Taking (C.5) and (C.8) together, we obtain

h2dµs.
x ≥Rs

(C.8)

h2dµs ≤ s[C(Rs) + 3ǫ(Rs)] ∇h 2dµs − 3( inf Vs(x)).ǫs(Rs)

h2dµs

Rd

Rd

x∈Rd

x∈Rd

(C.9)

Apparently, from the deﬁnition of ǫs(x), we can select Rs > 0 large enough such that 1 + 3s( inf Vs(x))ǫ(Rs) > 0. Then, we can rewrite (C.9) as
x∈Rd

h2dµs ≤ s · 2(C(Rs) + 3ǫ(Rs))

Rd

2 1 + 3s( inf Vs(x))ǫ(Rs)

x∈Rd

x∈Rd

∇h 2dµs.

(C.10)

Finally, using h − Rd hdµs instead of h in the inequality (C.10), we prove the desired Poincar´e

inequality by taking

1 + 3s( inf Vs(x))ǫ(Rs)

λs =

x∈Rd

.

2(C(Rs) + 3ǫ(Rs))

C.4 Proof of Lemma 5.6

For convenience, we introduce a shorthand:

g

e−

2g s

Π s = e− 2sg dx .

Rd

Then, we can rewrite the derivative as

2
dǫ(s) = s2 ds

Rd g2e− 2sg dx

Rd

e−

2g s

dx

−

2 s2

Rd e− 2sg dx 2

Rd ge− 2sg dx 2

= 2 g 2 Π g dx − 2

Rd s

s

gg

2

Rd s · Π s dx .

Next, we assume that ζk(x) = xke−xα, where α < 1 is a ﬁxed positive constant and k = 1, 2. The facts that ζk(0) = 0 and lim ζk(x) = 0 give
x→+∞

gk g

g

0 ≤ lim s→0+ s

Π

s

≤ lim ζk
s→0+

s

= 0.

48

Then, by Fatou’s lemma, we get

0 ≤ lim inf dǫ(s) ≤ lim sup dǫ(s)

s→0+ ds

s→0+ ds

= lim sup 2

g 2 Π g dx − 2

s→0+

Rd s

s

g · Π g dx 2

Rd s

s

= 2 lim sup

g 2 Π g dx − 2 lim inf

s→0+ Rd s

s

s→0+

g · Π g dx 2

Rd s

s

≤ 2 lim sup g 2 Π g dx − 2

Rd s→0+ s

s

gg

2

lim inf · Π dx

Rd s→0+ s

s

= 2 lim g 2 Π g dx − 2

Rd s→0+ s

s

gg

2

lim · Π dx

Rd s→0+ s

s

= 0.

The proof is complete.

49

