Neural Controlled Differential Equations for Irregular Time Series

arXiv:2005.08926v2 [cs.LG] 5 Nov 2020

Patrick Kidger James Morrill James Foster Terry Lyons
Mathematical Institute, University of Oxford The Alan Turing Institute, British Library
{kidger, morrill, foster, tlyons}@maths.ox.ac.uk

Abstract
Neural ordinary differential equations are an attractive option for modelling temporal dynamics. However, a fundamental issue is that the solution to an ordinary differential equation is determined by its initial condition, and there is no mechanism for adjusting the trajectory based on subsequent observations. Here, we demonstrate how this may be resolved through the well-understood mathematics of controlled differential equations. The resulting neural controlled differential equation model is directly applicable to the general setting of partiallyobserved irregularly-sampled multivariate time series, and (unlike previous work on this problem) it may utilise memory-efﬁcient adjoint-based backpropagation even across observations. We demonstrate that our model achieves state-of-the-art performance against similar (ODE or RNN based) models in empirical studies on a range of datasets. Finally we provide theoretical results demonstrating universal approximation, and that our model subsumes alternative ODE models.

1 Introduction

Recurrent neural networks (RNN) are a popular choice of model for sequential data, such as a time series. The data itself is often assumed to be a sequence of observations from an underlying process, and the RNN may be interpreted as a discrete approximation to some function of this process. Indeed the connection between RNNs and dynamical systems is well-known [1, 2, 3, 4].
However this discretisation typically breaks down if the data is irregularly sampled or partially observed, and the issue is often papered over by binning or imputing data [5].
A more elegant approach is to appreciate that because the underlying process develops in continuous time, so should our models. For example [6, 7, 8, 9] incorporate exponential decay between observations, [10, 11] hybridise a Gaussian process with traditional neural network models, [12] approximate the underlying continuous-time process, and [13, 14] adapt recurrent neural networks by allowing some hidden state to evolve as an ODE. It is this last one that is of most interest to us here.

1.1 Neural ordinary differential equations

Neural ordinary differential equations (Neural ODEs) [3, 15], seek to approximate a map x → y by learning a function fθ and linear maps ℓ1θ, ℓ2θ such that

t

y ≈ ℓ1θ(zT ), where zt = z0 + fθ(zs)ds and z0 = ℓ2θ(x).

(1)

0

Note that fθ does not depend explicitly on s; if desired this can be included as an extra dimension in zs [15, Appendix B.2].

34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.

Neural ODEs are an elegant concept. They provide an interface between machine learning and the other dominant modelling paradigm that is differential equations. Doing so allows for the wellunderstood tools of that ﬁeld to be applied. Neural ODEs also interact beautifully with the manifold hypothesis, as they describe a ﬂow along which to evolve the data manifold.
This description has not yet involved sequential data such as time series. The t dimension in equation (1) was introduced and then integrated over, and is just an internal detail of the model.
However the presence of this extra (artiﬁcial) dimension motivates the question of whether this model can be extended to sequential data such as time series. Given some ordered data (x0, . . . , xn), the goal is to extend the z0 = ℓ2θ(x) condition of equation (1) to a condition resembling “z0 = ℓ(x0), . . . , zn = ℓ(xn)”, to align the introduced t dimension with the natural ordering of the data.
The key difﬁculty is that equation (1) deﬁnes an ordinary differential equation; once θ has been learnt, then the solution of equation (1) is determined by the initial condition at z0, and there is no direct mechanism for incorporating data that arrives later [4].
However, it turns out that the resolution of this issue – how to incorporate incoming information – is already a well-studied problem in mathematics, in the ﬁeld of rough analysis, which is concerned with the study of controlled differential equations.1 See for example [16, 17, 18, 19]. An excellent introduction is [20]. A comprehensive textbook is [21].
We will not assume familiarity with either controlled differential equations or rough analysis. The only concept we will rely on that may be unfamiliar is that of a Riemann–Stieltjes integral.
1.2 Contributions
We demonstrate how controlled differential equations may extend the Neural ODE model, which we refer to as the neural controlled differential equation (Neural CDE) model. Just as Neural ODEs are the continuous analogue of a ResNet, the Neural CDE is the continuous analogue of an RNN.
The Neural CDE model has three key features. One, it is capable of processing incoming data, which may be both irregularly sampled and partially observed. Two (and unlike previous work on this problem) the model may be trained with memory-efﬁcient adjoint-based backpropagation even across observations. Three, it demonstrates state-of-the-art performance against similar (ODE or RNN based) models, which we show in empirical studies on the CharacterTrajectories, PhysioNet sepsis prediction, and Speech Commands datasets.
We provide additional theoretical results showing that our model is a universal approximator, and that it subsumes apparently-similar ODE models in which the vector ﬁeld depends directly upon continuous data.
Our code is available at https://github.com/patrick-kidger/NeuralCDE. We have also released a library torchcde, at https://github.com/patrick-kidger/torchcde

2 Background

Let τ, T ∈ R with τ < T , and let v, w ∈ N. Let X : [τ, T ] → Rv be a continuous function of bounded variation; for example this is implied by X being Lipschitz. Let ζ ∈ Rw. Let f : Rw → Rw×v be continuous.

Then we may deﬁne a continuous path z : [τ, T ] → Rw by zτ = ζ and

t

zt = zτ + f (zs)dXs for t ∈ (τ, T ],

(2)

τ

where the integral is a Riemann–Stieltjes integral. As f (zs) ∈ Rw×v and Xs ∈ Rv, the notation “f (zs)dXs” refers to matrix-vector multiplication. The subscript notation refers to function

evaluation, for example as is common in stochastic calculus.

Equation (2) exhibits global existence and uniqueness subject to global Lipschitz conditions on f ; see [20, Theorem 1.3]. We say that equation (2) is a controlled differential equation (CDE) which is controlled or driven by X.

1Not to be confused with the similarly-named but separate ﬁeld of control theory.

2

Hidden state z

Hidden state z Path X

x1 x2 t1 t2

x3

xn

t3 · · · tn

Data x Time

x1 x2 t1 t2

x3

xn

t3 · · · tn

Data x Time

Figure 1: Some data process is observed at times t1, . . . , tn to give observations x1, . . . , xn. It is otherwise unobserved. Left: Previous work has typically modiﬁed hidden state at each observation, and perhaps continuously evolved the hidden state between observations. Right: In contrast, the
hidden state of the Neural CDE model has continuous dependence on the observed data.

3 Method

Suppose for simplicity that we have a fully-observed but potentially irregularly sampled time series x = ((t0, x0), (t1, x1), . . . , (tn, xn)), with each ti ∈ R the timestamp of the observation xi ∈ Rv, and t0 < · · · < tn. (We will consider partially-observed data later.)
Let X : [t0, tn] → Rv+1 be the natural cubic spline with knots at t0, . . . , tn such that Xti = (xi, ti). As x is often assumed to be a discretisation of an underlying process, observed only through x, then X is an approximation to this underlying process. Natural cubic splines have essentially the minimum regularity for handling certain edge cases; see Appendix A for the technical details.

Let fθ : Rw → Rw×(v+1) be any neural network model depending on parameters θ. The value w is a hyperparameter describing the size of the hidden state. Let ζθ : Rv+1 → Rw be any neural network model depending on parameters θ.

Then we deﬁne the neural controlled differential equation model as the solution of the CDE

t

zt = zt0 + fθ(zs)dXs for t ∈ (t0, tn],

(3)

t0

where zt0 = ζθ(x0, t0). This initial condition is used to avoid translational invariance. Analogous to RNNs, the output of the model may either be taken to be the evolving process z, or the terminal
value ztn, and the ﬁnal prediction should typically be given by a linear map applied to this output.

The resemblance between equations (1) and (3) is clear. The essential difference is that equation (3) is driven by the data process X, whilst equation (1) is driven only by the identity function ι : R → R. In this way, the Neural CDE is naturally adapting to incoming data, as changes in X change the local dynamics of the system. See Figure 1.

3.1 Universal Approximation
It is a famous theorem in CDEs that in some sense they represent general functions on streams [22, Theorem 4.2], [23, Proposition A.6]. This may be applied to show that Neural CDEs are universal approximators, which we summarise in the following informal statement. Theorem (Informal). The action of a linear map on the terminal value of a Neural CDE is a universal approximator from {sequences in Rv} to R.
Theorem B.14 in Appendix B gives a formal statement and a proof, which is somewhat technical. The essential idea is that CDEs may be used to approximate bases of functions on path space.

3.2 Evaluating the Neural CDE model

Evaluating the Neural CDE model is straightforward. In our formulation above, X is in fact not just of bounded variation but is differentiable. In this case, we may deﬁne

dX

gθ,X(z, s) = fθ(z) ds (s),

(4)

3

so that for t ∈ (t0, tn],

t

t

dX

t

zt = zt0 + t0 fθ(zs)dXs = zt0 + t0 fθ(zs) ds (s)ds = zt0 + t0 gθ,X (zs, s)ds. (5)

Thus it is possible to solve the Neural CDE using the same techniques as for Neural ODEs. In our experiments, we were able to straightforwardly use the already-existing torchdiffeq package [24] without modiﬁcation.

3.3 Comparison to alternative ODE models
For the reader not familiar with CDEs, it might instead seem more natural to replace gθ,X with some hθ(z, Xs) that is directly applied to and potentially nonlinear in Xs. Indeed, such approaches have been suggested before, in particular to derive a “GRU-ODE” analogous to a GRU [14, 25].
However, it turns out that something is lost by doing so, which we summarise in the following statement. Theorem (Informal). Any equation of the form zt = z0 + tt hθ(zs, Xs)ds may be represented
0
exactly by a Neural CDE of the form zt = z0 + tt fθ(zs)dXs. However the converse statement is 0
not true.
Theorem C.1 in Appendix C provides the formal statement and proof. The essential idea is that a Neural CDE can easily represent the identity function between paths, whilst the alternative cannot.
In our experiments, we ﬁnd that the Neural CDE substantially outperforms the GRU-ODE, which we speculate is a consequence of this result.

3.4 Training via the adjoint method
An attractive part of Neural ODEs is the ability to train via adjoint backpropagation, see [15, 26, 27, 28], which uses only O(H) memory in the time horizon L = tn − t0 and the memory footprint H of the vector ﬁeld. This is contrast to directly backpropagating through the operations of an ODE solver, which requires O(LH) memory.
Previous work on Neural ODEs for time series, for example [13], has interrupted the ODE to make updates at each observation. Adjoint-based backpropagation cannot be performed across the jump, so this once again requires O(LH) memory.
In contrast, the gθ,X deﬁned by equation (4) continuously incorporates incoming data, without interrupting the differential equation, and so adjoint backpropagation may be performed. This requires only O(H) memory. The underlying data unavoidably uses an additional O(L) memory. Thus training the Neural CDE has an overall memory footprint of just O(L + H).
We do remark that the adjoint method should be used with care, as some systems are not stable to evaluate in both the forward and backward directions [29, 30]. The problem of ﬁnite-time blow-up is at least not a concern, given global Lipschitz conditions on the vector ﬁeld [20, Theorem 1.3]. Such a condition will be satisﬁed if fθ uses ReLU or tanh nonlinearities, for example.

3.5 Intensity as a channel
It has been observed that the frequency of observations may carry information [6]. For example, doctors may take more frequent measurements of patients they believe to be at greater risk. Some previous work has for example incorporated this information by learning an intensity function [12, 13, 15].
We instead present a simple non-learnt procedure, that is compatible with Neural CDEs. Simply concatenate the index i of xi together with xi, and then construct a path X from the pair (i, xi), as before. The channel of X corresponding to these indices then corresponds to the cumulative intensity of observations.
As the derivative of X is what is then used when evaluating the Neural CDE model, as in equation (5), then it is the intensity itself that then determines the vector ﬁeld.

4

3.6 Partially observed data
One advantage of our formulation is that it naturally adapts to the case of partially observed data. Each channel may independently be interpolated between observations to deﬁne X in exactly the same manner as before.
In this case, the procedure for measuring observational intensity in Section 3.5 may be adjusted by instead having a separate observational intensity channel ci for each original channel oi, such that ci increments every time an observation is made in oi.
3.7 Batching
Given a batch of training samples with observation times drawn from the same interval [t0, tn], we may interpolate each x to produce a continuous X, as already described. Each path X is what may then be batched together, regardless of whether the underlying data is irregularly sampled or partially observed. Batching is thus efﬁcient for the Neural CDE model.
4 Experiments
We benchmark the Neural CDE against a variety of existing models.
These are: GRU-∆t, which is a GRU with the time difference between observations additionally used as an input; GRU-D [6], which modiﬁes the GRU-∆t with learnt exponential decays between observations; GRU-ODE [14, 25], which is an ODE analogous to the operation of a GRU and uses X as its input; ODE-RNN [13], which is a GRU-∆t model which additionally applies a learnt Neural ODE to the hidden state between observations. Every model then used a learnt linear map from the ﬁnal hidden state to the output, and was trained with cross entropy or binary cross entropy loss.
The GRU-∆t represents a straightforward baseline, the GRU-ODE is an alternative ODE model that is thematically similar to a Neural CDE, and the GRU-D and ODE-RNNs are state-of-the-art models for these types of problems. To avoid unreasonably extensive comparisons we have chosen to focus on demonstrating superiority within the class of ODE and RNN based models to which the Neural CDE belongs. These models were selected to collectively be representative of this class.
Each model is run ﬁve times, and we report the mean and standard deviation of the test metrics.
For every problem, the hyperparameters were chosen by performing a grid search to optimise the performance of the baseline ODE-RNN model. Equivalent hyperparameters were then used for every other model, adjusted slightly so that every model has a comparable number of parameters.
Precise experimental details may be found in Appendix D, regarding normalisation, architectures, activation functions, optimisation, hyperparameters, regularisation, and so on.
4.1 Varying amounts of missing data on CharacterTrajectories
We begin by demonstrating the efﬁcacy of Neural CDEs on irregularly sampled time series.
To do this, we consider the CharacterTrajectories dataset from the UEA time series classiﬁcation archive [31]. This is a dataset of 2858 time series, each of length 182, consisting of the x, y position and pen tip force whilst writing a Latin alphabet character in a single stroke. The goal is to classify which of 20 different characters are written.
We run three experiments, in which we drop either 30%, 50% or 70% of the data. The observations to drop are selected uniformly at random and independently for each time series. Observations are removed across channels, so that the resulting dataset is irregularly sampled but completely observed. The randomly removed data is the same for every model and every repeat.
The results are shown in Table 1. The Neural CDE outperforms every other model considered, and furthermore it does so whilst using an order of magnitude less memory. The GRU-ODE does consistently poorly despite being the most theoretically similar model to a Neural CDE. Furthermore we see that even as the fraction of dropped data increases, the performance of the Neural CDE remains roughly constant, whilst the other models all start to decrease.
Further experimental details may be found in Appendix D.2.
5

Table 1: Test accuracy (mean ± std, computed across ﬁve runs) and memory usage on CharacterTrajectories. Memory usage is independent of repeats and of amount of data dropped.

Model

30% dropped

Test Accuracy 50% dropped

70% dropped

Memory usage (MB)

GRU-ODE GRU-∆t GRU-D ODE-RNN

92.6% ± 1.6% 86.7% ± 3.9% 89.9% ± 3.7%

1.5

93.6% ± 2.0% 91.3% ± 2.1% 90.4% ± 0.8%

15.8

94.2% ± 2.1% 90.2% ± 4.8% 91.9% ± 1.7%

17.0

95.4% ± 0.6% 96.0% ± 0.3% 95.3% ± 0.6%

14.8

Neural CDE (ours) 98.7% ± 0.8% 98.8% ± 0.2% 98.6% ± 0.4%

1.3

Table 2: Test AUC (mean ± std, computed across ﬁve runs) and memory usage on PhysioNet sepsis prediction. ‘OI’ refers to the inclusion of observational intensity, ‘No OI’ means without it. Memory usage is independent of repeats.

Model

Test AUC

OI

No OI

Memory usage (MB)

OI

No OI

GRU-ODE GRU-∆t GRU-D ODE-RNN

0.852 ± 0.010 0.771 ± 0.024 454

273

0.878 ± 0.006 0.840 ± 0.007 837

826

0.871 ± 0.022 0.850 ± 0.013 889

878

0.874 ± 0.016 0.833 ± 0.020 696

686

Neural CDE (ours) 0.880 ± 0.006 0.776 ± 0.009 244

122

4.2 Observational intensity with PhysioNet sepsis prediction
Next we consider a dataset that is both irregularly sampled and partially observed, and investigate the beneﬁts of observational intensity as discussed in Sections 3.5 and 3.6.
We use data from the PhysioNet 2019 challenge on sepsis prediction [32, 33]. This is a dataset of 40335 time series of variable length, describing the stay of patients within an ICU. Measurements are made of 5 static features such as age, and 34 time-dependent features such as respiration rate or creatinine concentration in the blood, down to an hourly resolution. Most values are missing; only 10.3% of values are observed. We consider the ﬁrst 72 hours of a patient’s stay, and consider the binary classiﬁcation problem of predicting whether they develop sepsis over the course of their entire stay (which is as long as a month for some patients).
We run two experiments, one with observational intensity, and one without. For the Neural CDE and GRU-ODE models, observational intensity is continuous and on a per-channel basis as described in Section 3.6. For the ODE-RNN, GRU-D, and GRU-∆t models, observational intensity is given by appending an observed/not-observed mask to the input at each observation.23 The initial hidden state of every model is taken to be a function (a small single hidden layer neural network) of the static features.
The results are shown in Table 2. As the dataset is highly imbalanced (5% positive rate), we report AUC rather than accuracy. When observational intensity is used, then the Neural CDE produces the best AUC overall, although the ODE-RNN and GRU-∆t models both perform well. The GRU-ODE continues to perform poorly.
Without observational intensity then every model performs substantially worse, and in particular we see that the beneﬁt of including observational intensity is particularly dramatic for the Neural CDE.
As before, the Neural CDE remains the most memory-efﬁcient model considered.
Further experimental details can be found in Appendix D.3.
2As our proposed observational intensity goes via a derivative, these each contain the same information. 3Note that the ODE-RNN, GRU-D and GRU-∆t models always receive the time difference between observations, ∆t, as an input. Thus even in the no observational intensity case, they remain aware of the irregular sampling of the data, and so this case not completely fair to the Neural CDE and GRU-ODE models.
6

4.3 Regular time series with Speech Commands

Finally we demonstrate the efﬁcacy of Neural CDE models on regularly spaced, fully observed time series, where we might hypothesise that the baseline models will do better.

We used the Speech Commands dataset [34]. This consists of one-second audio recordings of both background noise and spoken words such as ‘left’, ‘right’, and so on. We used 34975 time series corresponding to ten spoken words so as to produce a balanced classiﬁcation problem. We preprocess the dataset by computing mel-frequency cepstrum coefﬁcients so that each time series is then regularly spaced with length 161 and 20 channels.

Table 3: Test Accuracy (mean ± std, computed across ﬁve runs) and memory usage on Speech Commands. Memory usage is independent of repeats.

Model
GRU-ODE GRU-∆t GRU-D ODE-RNN
Neural CDE (ours)

Test Accuracy
47.9% ± 2.9% 43.3% ± 33.9% 32.4% ± 34.8% 65.9% ± 35.6%
89.8% ± 2.5%

Memory usage (GB)
0.164 1.54 1.64 1.40
0.167

The results are shown in Table 3. We observed that the Neural CDE had the highest performance, whilst using very little memory. The GRU-ODE consistently failed to perform. The other benchmark models surprised us by exhibiting a large variance on this problem, due to sometimes failing to train, and we were unable to resolve this by tweaking the optimiser. The best GRU-∆t, GRU-D and ODE-RNN models did match the performance of the Neural CDE, suggesting that on a regularly spaced problem all approaches can be made to work equally well.

In contrast, the Neural CDE model produced consistently good results every time. Anecdotally this aligns with what we observed over the course of all of our experiments, which is that the Neural CDE model usually trained quickly, and was robust to choice of optimisation hyperparameters. We stress that we did not perform a formal investigation of this phenomenen.

Further experimental details can be found in Appendix D.4.

5 Related work
In [13, 14] the authors consider interrupting a Neural ODE with updates from a recurrent cell at each observation, and were in fact the inspiration for this paper. Earlier work [6, 7, 8, 9] use intra-observation exponential decays, which are a special case. [35] consider something similar by interrupting a Neural ODE with stochastic events.
SDEs and CDEs are closely related, and several authors have introduced Neural SDEs. [36, 37, 38] treat them as generative models for time series and seek to model the data distribution. [39, 40] investigate using stochasticity as a regularizer, and demonstrate better performance by doing so. [41] use random vector ﬁelds so as to promote simpler trajectories, but do not use the ‘SDE’ terminology.
Adjoint backpropagation needs some work to apply to SDEs, and so [42, 43, 44] all propose methods for training Neural SDEs. We would particularly like to highlight the elegant approach of [44], who use the pathwise treatment given by rough analysis to approximate Brownian noise, and thus produce a random Neural ODE which may be trained in the usual way; such approaches may also avoid the poor convergence rates of SDE solvers as compared to ODE solvers.
Other elements of the theory of rough analysis and CDEs have also found machine learning applications. Amongst others, [23, 45, 46, 47, 48, 49, ?, ?] discuss applications of the signature transform to time series problems, and [50] investigate the related logsignature transform. [51] develop a kernel for time series using this methodology, and [52] apply this kernel to Gaussian processes. [53] develop software for these approaches tailored for machine learning.
There has been a range of work seeking to improve Neural ODEs. [54, 55] investigate speedups to the training proecedure, [56] develop an energy-based Neural ODE framework, and [29] demonstrate potential pitfalls with adjoint backpropagation. [30, 57] consider ways to vary the network parameters over time. [55, 58] consider how a Neural ODE model may be regularised (see also the stochastic regularisation discussed above). This provides a wide variety of techniques, and we are hopeful that some of them may additionally carry over to the Neural CDE case.

7

6 Discussion
6.1 Considerations
There are two key elements of the Neural CDE construction which are subtle, but important. Time as a channel CDEs exhibit a tree-like invariance property [18]. What this means, roughly, is that a CDE is blind to speed at which X is traversed. Thus merely setting Xti = xi would not be enough, as time information is only incorporated via the parameterisation. This is why time is explicitly included as a channel via Xti = (xi, ti). Initial value networks The initial hidden state zt0 should depend on Xt0 = (x0, t0). Otherwise, the Neural CDE will depend upon X only through its derivative dX/dt, and so will be translationally invariant. An alternative would be to append another channel whose ﬁrst derivative includes translation-sensitive information, for example by setting Xti = (xi, ti, tix0).
6.2 Performance tricks
We make certain (somewhat anecdotal) observations of tricks that seemed to help performance.
Final tanh nonlinearity We found it beneﬁcial to use a tanh as a ﬁnal nonlinearity for the vector ﬁeld fθ of a Neural CDE model. Doing so helps prevent extremely large initial losses, as the tanh constrains the rate of change of the hidden state. This is analogous to RNNs, where the key feature of GRUs and LSTMs are procedures to constrain the rate of change of the hidden state. Layer-wise learning rates We found it beneﬁcial to use a larger (×10–100) learning rate for the linear layer on top of the output of the Neural CDE, than for the vector ﬁeld fθ of the Neural CDE itself. This was inspired by the observation that the ﬁnal linear layer has (in isolation) only a convex optimisation problem to solve.4
6.3 Limitations
Speed of computation We found that Neural CDEs were typically slightly faster to compute than the ODE-RNN model of [13]. (This is likely to be because in an Neural CDE, steps of the numerical solver can be made across observations, whilst the ODE-RNN must interrupt its solve at each observation.)
However, Neural CDEs were still roughly ﬁves times slower than RNN models. We believe this is largely an implementation issue, as the implementation via torchdiffeq is in Python, and by default uses double-precision arithmetic with variable step size solvers, which we suspect is unnecessary for most practical tasks. Number of parameters If the vector ﬁeld fθ : Rw → Rw×(v+1) is a feedforward neural network, with ﬁnal hidden layer of size ω, then the number of scalars for the ﬁnal afﬁne transformation is of size O(ωvw), which can easily be very large. In our experiments we have to choose small values of w and ω for the Neural CDE to ensure that the number of parameters is the same across models. We did experiment with representing the ﬁnal linear layer as an outer product of transformations Rw → Rw and Rw → Rv+1. This implies that the resulting matrix is rank-one, and reduces the number of parameters to just O(ω(v + w)), but unfortunately we found that this hindered the classiﬁcation performance of the model.
6.4 Future work
Vector ﬁeld design The vector ﬁelds fθ that we consider are feedforward networks. More sophisticated choices may allow for improved performance, in particular to overcome the trilinearity issue just discussed.
4In our experiments we applied this learning rate to the linear layer on top of every model, not the just the Neural CDE, to ensure a fair comparison.
8

Modelling uncertainty As presented here, Neural CDEs do not give any measure of uncertainty about their predictions. Such extensions are likely to be possible, given the close links between CDEs and SDEs, and existing work on Neural SDEs [36, 37, 38, 39, 40, 42, 43, 44, ?]. Numerical schemes In this paper we integrated the Neural CDE by reducing it to an ODE. The ﬁeld of numerical CDEs is relatively small – to the best of our knowledge [17, 59, 60, 61, 62, 63, 64, 65, 66] constitute essentially the entire ﬁeld, and are largely restricted to rough controls. Other numerical methods may be able to exploit the CDE structure to improve performance. Choice of X Natural cubic splines were used to construct the path X from the time series x. However, these are not causal. That is, Xt depends upon the value of xi for t < ti. This makes it infeasible to apply Neural CDEs in real-time settings, for which Xt is needed before xi has been observed. Resolving this particular issue is a topic on which we have follow-up work planned. Other problem types Our experiments here involved only classiﬁcation problems. There was no real reason for this choice, and we expect Neural CDEs to be applicable more broadly.
6.5 Related theories
Rough path theory The ﬁeld of rough path theory, which deals with the study of CDEs, is much larger than the small slice that we have used here. It is likely that further applications may serve to improve Neural CDEs. A particular focus of rough path theory is how to treat functions that must be sensitive to the order of events in a particular (continuous) way. Control theory Despite their similar names, and consideration of similar-looking problems, control theory and controlled differential equations are essentially separate ﬁelds. Control theory has clear links and applications that may prove beneﬁcial to models of this type. RNN theory Neural CDEs may be interpreted as continuous-time versions of RNNs. CDEs thus offer a theoretical construction through which RNNs may perhaps be better understood. Conversely, what is known about RNNs may have applications to improve Neural CDEs.
7 Conclusion
We have introduced a new class of continuous-time time series models, Neural CDEs. Just as Neural ODEs are the continuous analogue of ResNets, the Neural CDE is the continuous time analogue of an RNN. The model has three key advantages: it operates directly on irregularly sampled and partially observed multivariate time series, it demonstrates state-of-the-art performance, and it beneﬁts from memory-efﬁcient adjoint-based backpropagation even across observations. To the best of our knowledge, no other model combines these three features together. We also provide additional theoretical results demonstrating universal approximation, and that Neural CDEs subsume alternative ODE models.
Broader Impact
We have introduced a new tool for studying irregular time series. As with any tool, it may be used in both positive and negative ways. The authors have a particular interest in electronic health records (an important example of irregularly sampled time-stamped data) and so here at least we hope and expect to see a positive impact from this work. We do not expect any speciﬁc negative impacts from this work.
Acknowledgments and Disclosure of Funding
Thanks to Cristopher Salvi for many vigorous discussions on this topic. PK was supported by the EPSRC grant EP/L015811/1. JM was supported by the EPSRC grant EP/L015803/1 in collaboration with Iterex Therapuetics. JF was supported by the EPSRC grant EP/N509711/1. PK, JM, JF, TL were supported by the Alan Turing Institute under the EPSRC grant EP/N510129/1.
9

References
[1] K.-i. Funahashi and Y. Nakamura, “Approximation of dynamical systems by continuous time recurrent neural networks,” Neural Networks, vol. 6, no. 6, pp. 801 – 806, 1993.
[2] C. Bailer-Jones, D. MacKay, and P. Withers, “A recurrent neural network for modelling dynamical systems,” Network: Computation in Neural Systems, vol. 9, pp. 531–47, 1998.
[3] W. E, “A Proposal on Machine Learning via Dynamical Systems,” Commun. Math. Stat., vol. 5, no. 1, pp. 1–11, 2017.
[4] M. Ciccone, M. Gallieri, J. Masci, C. Osendorfer, and F. Gomez, “NAIS-Net: Stable Deep Networks from Non-Autonomous Differential Equations,” in Advances in Neural Information Processing Systems 31, pp. 3025–3035, Curran Associates, Inc., 2018.
[5] A. Gelman and J. Hill, Data Analysis Using Regression and Multilevel/Hierarchical Models. Cambridge University Press, 2007.
[6] Z. Che, S. Purushotham, K. Cho, D. Sontag, and Y. Liu, “Recurrent Neural Networks for Multivariate Time Series with Missing Values,” Scientiﬁc Reports, vol. 8, no. 1, p. 6085, 2018.
[7] W. Cao, D. Wang, J. Li, H. Zhou, L. Li, and Y. Li, “BRITS: Bidirectional Recurrent Imputation for Time Series,” in Advances in Neural Information Processing Systems 31 (S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, eds.), pp. 6775–6785, Curran Associates, Inc., 2018.
[8] H. Mei and J. M. Eisner, “The Neural Hawkes Process: A Neurally Self-Modulating Multivariate Point Process,” in Advances in Neural Information Processing Systems 30 (I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, eds.), pp. 6754–6764, Curran Associates, Inc., 2017.
[9] M. Mozer, D. Kazakov, and R. Lindsey, “Discrete Event, Continuous Time RNNs,” arXiv:1710.04110, 2017.
[10] S. C.-X. Li and B. M. Marlin, “A scalable end-to-end Gaussian process adapter for irregularly sampled time series classiﬁcation,” in Advances in Neural Information Processing Systems, pp. 1804–1812, 2016.
[11] J. Futoma, S. Hariharan, and K. Heller, “Learning to Detect Sepsis with a Multitask Gaussian Process RNN Classiﬁer,” in Proceedings of the 34th International Conference on Machine Learning, pp. 1174– 1182, 2017.
[12] S. N. Shukla and B. Marlin, “Interpolation-Prediction Networks for Irregularly Sampled Time Series,” in International Conference on Learning Representations, 2019.
[13] Y. Rubanova, T. Q. Chen, and D. K. Duvenaud, “Latent Ordinary Differential Equations for IrregularlySampled Time Series,” in Advances in Neural Information Processing Systems 32, pp. 5320–5330, Curran Associates, Inc., 2019.
[14] E. De Brouwer, J. Simm, A. Arany, and Y. Moreau, “GRU-ODE-Bayes: Continuous Modeling of Sporadically-Observed Time Series,” in Advances in Neural Information Processing Systems 32, pp. 7379–7390, Curran Associates, Inc., 2019.
[15] R. T. Q. Chen, Y. Rubanova, J. Bettencourt, and D. K. Duvenaud, “Neural Ordinary Differential Equations,” in Advances in Neural Information Processing Systems 31, pp. 6571–6583, Curran Associates, Inc., 2018.
[16] T. J. Lyons, “Differential equations driven by rough signals,” Revista Matema´tica Iberoamericana, vol. 14, no. 2, pp. 215–310, 1998.
[17] T. Lyons, “Rough paths, signatures and the modelling of functions on streams,” arXiv:1405.4537, 2014.
[18] B. M. Hambly and T. J. Lyons, “Uniqueness for the signature of a path of bounded variation and the reduced path group,” Annals of Mathematics, vol. 171, no. 1, pp. 109–167, 2010.
[19] I. Chevyrev and H. Oberhauser, “Signature moments to characterize laws of stochastic processes,” arXiv:1810.10971, 2018.
[20] T. Lyons, M. Caruana, and T. Levy, Differential equations driven by rough paths. Springer, 2004. E´ cole d’E´ te´ de Probabilite´s de Saint-Flour XXXIV - 2004.
[21] P. K. Friz and N. B. Victoir, “Multidimensional stochastic processes as rough paths: theory and applications,” Cambridge University Press, 2010.
[22] I. Perez Arribas, “Derivatives pricing using signature payoffs,” arXiv:1809.09466, 2018.
[23] P. Bonnier, P. Kidger, I. Perez Arribas, C. Salvi, and T. Lyons, “Deep Signature Transforms,” in Advances in Neural Information Processing Systems, pp. 3099–3109, 2019.
[24] R. T. Q. Chen, “torchdiffeq,” 2018. https://github.com/rtqichen/torchdiffeq.
10

[25] I. Jordan, P. A. Sokol, and I. M. Park, “Gated recurrent units viewed through the lens of continuous time dynamical systems,” arXiv:1906.01005, 2019.
[26] L. S. Pontryagin, E. F. Mishchenko, V. G. Boltyanskii, and R. V. Gamkrelidze, “The mathematical theory of optimal processes,” 1962.
[27] M. B. Giles and N. A. Pierce, “An Introduction to the Adjoint Approach to Design,” Flow, Turbulence and Combustion, vol. 65, pp. 393–415, Dec 2000.
[28] W. W. Hager, “Runge-Kutta methods in optimal control and the transformed adjoint system,” Numerische Mathematik, vol. 87, pp. 247–282, Dec 2000.
[29] A. Gholami, K. Keutzer, and G. Biros, “ANODE: Unconditionally Accurate Memory-Efﬁcient Gradients for Neural ODEs,” arXiv:1902.10298, 2019.
[30] T. Zhang, Z. Yao, A. Gholami, J. E. Gonzalez, K. Keutzer, M. W. Mahoney, and G. Biros, “ANODEV2: A Coupled Neural ODE Framework,” in Advances in Neural Information Processing Systems 32, pp. 5151– 5161, Curran Associates, Inc., 2019.
[31] A. Bagnall, H. A. Dau, J. Lines, M. Flynn, J. Large, A. Bostrom, P. Southam, and E. Keogh, “The uea multivariate time series classiﬁcation archive,” arXiv:1811.00075, 2018.
[32] M. Reyna, C. Josef, R. Jeter, S. Shashikumar, B. Moody, M. B. Westover, A. Sharma, S. Nemati, and G. Clifford, “Early Prediction of Sepsis from Clinical Data: The PhysioNet/Computing in Cardiology Challenge,” Critical Care Medicine, vol. 48, no. 2, pp. 210–217, 2019.
[33] Goldberger, A. L. and Amaral L. A. N. and Glass, L. and Hausdorff, J. M. and Ivanov P. Ch. and Mark, R. G. and Mietus, J. E. and Moody, G. B. and Peng, C.-K. and Stanley, H. E., “PhysioBank, PhysioToolkit and PhysioNet: Components of a New Research Resource for Complex Physiologic Signals,” Circulation, vol. 23, pp. 215–220, 2003.
[34] P. Warden, “Speech commands: A dataset for limited-vocabulary speech recognition,” arXiv:1804.03209, 2020.
[35] J. Jia and A. R. Benson, “Neural Jump Stochastic Differential Equations,” in Advances in Neural Information Processing Systems 32, pp. 9847–9858, Curran Associates, Inc., 2019.
[36] C. Cuchiero, W. Khosrawi, and J. Tiechmann, “A generative adversarial network approach to calibration of local stochastic volatility models,” arXiv:2005.02505, 2020.
[37] B. Tzen and M. Raginsky, “Theoretical guarantees for sampling and inference in generative models with latent diffusions,” COLT, 2019.
[38] R. Deng, B. Chang, M. Brubaker, G. Mori, and A. Lehrmann, “Modeling Continuous Stochastic Processes with Dynamic Normalizing Flows,” arXiv:2002.10516, 2020.
[39] X. Liu, T. Xiao, S. Si, Q. Cao, S. Kumar, and C.-J. Hsieh, “Neural SDE: Stabilizing Neural ODE Networks with Stochastic Noise,” arXiv:1906.02355, 2019.
[40] V. Oganesyan, A. Volokhova, and D. Vetrov, “Stochasticity in Neural ODEs: An Empirical Study,” arXiv:2002.09779, 2020.
[41] N. Twomey, M. Kozłowski, and R. Santos-Rodr´ıguez, “Neural ODEs with stochastic vector ﬁeld mixtures,” arXiv:1905.09905, 2019.
[42] X. Li, T.-K. L. Wong, R. T. Q. Chen, and D. K. Duvenaud, “Scalable Gradients and Variational Inference for Stochastic Differential Equations,” AISTATS, 2020.
[43] B. Tzen and M. Raginsky, “Neural Stochastic Differential Equations: Deep Latent Gaussian Models in the Diffusion Limit,” arXiv:1905.09883, 2019.
[44] L. Hodgkinson, C. van der Heide, F. Roosta, and M. Mahoney, “Stochastic Normalizing Flows,” arXiv:2002.09547, 2020.
[45] I. Chevyrev and A. Kormilitzin, “A primer on the signature method in machine learning,” arXiv:1603.03788, 2016.
[46] I. Perez Arribas, G. M. Goodwin, J. R. Geddes, T. Lyons, and K. E. A. Saunders, “A signaturebased machine learning model for distinguishing bipolar disorder and borderline personality disorder,” Translational Psychiatry, vol. 8, no. 1, p. 274, 2018.
[47] A. Fermanian, “Embedding and learning with signatures,” arXiv:1911.13211, 2019.
[48] J. Morrill, A. Kormilitzin, A. Nevado-Holgado, S. Swaminathan, S. Howison, and T. Lyons, “The Signature-based Model for Early Detection of Sepsis from Electronic Health Records in the Intensive Care Unit,” International Conference in Computing in Cardiology, 2019.
[49] J. Reizenstein, Iterated-integral signatures in machine learning. PhD thesis, University of Warwick, 2019. http://wrap.warwick.ac.uk/131162/.
11

[50] S. Liao, T. Lyons, W. Yang, and H. Ni, “Learning stochastic differential equations using RNN with log signature features,” arXiv:1908.08286, 2019.
[51] F. J. Kira´ly and H. Oberhauser, “Kernels for sequentially ordered data,” Journal of Machine Learning Research, 2019.
[52] C. Toth and H. Oberhauser, “Variational Gaussian Processes with Signature Covariances,” arXiv:1906.08215, 2019.
[53] P. Kidger and T. Lyons, “Signatory: differentiable computations of the signature and logsignature transforms, on both CPU and GPU,” arXiv:2001.00706, 2020.
[54] A. Quaglino, M. Gallieri, J. Masci, and J. Koutn´ık, “Snode: Spectral discretization of neural odes for system identiﬁcation,” in International Conference on Learning Representations, 2020.
[55] C. Finlay, J.-H. Jacobsen, L. Nurbekyan, and A. Oberman, “How to train your neural ODE,” arXiv:2002.02798, 2020.
[56] S. Massaroli, M. Poli, M. Bin, J. Park, A. Yamashita, and H. Asama, “Stable Neural ﬂows,” arXiv:2003.08063, 2020.
[57] S. Massaroli, M. Poli, J. Park, A. Yamashita, and H. Asama, “Dissecting Neural ODEs,” arXiv:2002.08071, 2020.
[58] H. Yan, J. Du, V. Y. F. Tan, and J. Feng, “On Robustness of Neural Ordinary Differential Equations,” arXiv:1910.05513, 2019.
[59] F. Castell and J. Gaines, “The ordinary differential equation approach to asymptotically efﬁcient schemes for solution of stochastic differential equations,” Annales de l’Institut Henri Poincare´. Probabilite´s et Statistiques, vol. 32, 1996.
[60] S. Malham and A. Wiese, “Stochastic Lie Group Integrators,” SIAM J. Sci. Comput., vol. 30, no. 2, pp. 597–617, 2007.
[61] L. G. Gyurko´, Numerical approximations for stochastic differential equations. PhD thesis, University of Oxford, 2008.
[62] A. Janssen, Order book models, signatures and numerical approximations of rough differential equations. PhD thesis, University of Oxford, 2011.
[63] Y. Boutaib, L. G. Gyurko´, T. Lyons, and D. Yang, “Dimension-free Euler estimates of rough differential equations,” Rev. Roumaine Math. Pures Appl., 2014.
[64] H. Boedihardjo, T. Lyons, and D. Yang, “Uniform factorial decay estimates for controlled differential equations,” Electronic Communications in Probability, vol. 20, no. 94, 2015.
[65] J. Foster, Numerical approximations for stochastic differential equations. PhD thesis, University of Oxford, 2020.
[66] J. Foster, T. Lyons, and H. Oberhauser, “An optimal polynomial approximation of Brownian motion,” SIAM J. Numer. Anal., vol. 58, no. 3, pp. 1393–1421, 2020.
[67] J. M. Varah, “A Lower Bound for the Smallest Singular Value of a Matrix,” Linear Algebra and its Applications, vol. 11, no. 1, pp. 3–5, 1975.
[68] A. Pinkus, “Approximation theory of the MLP model in neural networks,” Acta Numer., vol. 8, pp. 143– 195, 1999.
[69] P. Kidger and T. Lyons, “Universal Approximation with Deep Narrow Networks,” arXiv:1905.08539, 2019.
[70] D. Kingma and J. Ba, “Adam: A method for stochastic optimization,” ICLR, 2015. [71] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein,
L. Antiga, A. Desmaison, A. Kopf, E. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner, L. Fang, J. Bai, and S. Chintala, “PyTorch: An Imperative Style, High-Performance Deep Learning Library,” in Advances in Neural Information Processing Systems 32, pp. 8024–8035, Curran Associates, Inc., 2019.
12

Supplementary material
Appendix A discusses the technical considerations of schemes for constructing a path X from data. Appendix B proves universal approximation of the Neural CDE model, and is substantially more technical than the rest of this paper. Appendix C proves that the Neural CDE model subsumes alternative ODE models which depend directly and nonlinearly on the data. Appendix D gives the full details of every experiment, such as choice of optimiser, hyperparameter searches, and so on.

A Other schemes for constructing the path X
To evaluate the model as discussed in Section 3.2, X must be at least continuous and piecewise differentiable.

A.1 Differentiating with respect to the time points

However, there is a technical caveat in the speciﬁc case that derivatives with respect to the initial time t0 are required, and that training is done with the adjoint method. In this case the derivative with respect to t0 is computed using, and thus requires, derivatives of the vector ﬁeld with respect to t.
To be precise, suppose we have a Neural CDE as before:

t
zt = zt0 + fθ(zs)dXs
t0

for t ∈ (t0, tn].

Let L be some (for simplicity scalar-valued) function of ztn, for example a loss. Consider

dX gθ,X(z, s) = fθ(z) ds (s)

as before, and let

dL as = dzs ,

which is vector-valued, with size equal to the size of zs, the number of hidden channels.

Then, applying [15, Equation 52] to our case:

dL dL

t0

∂gθ,X

dt0 = dtn − t as · ∂s (zs, s)ds

n

dL

t0

d2X

= dtn − tn as · fθ(zs) ds2 (s)ds, (6)

where · represents the dot product.
In principle we may make sense of equation (6) when d2X/ds2 is merely measure valued, but in practice most code is only set up to handle classical derivatives. If derivatives with respect to t0 are desired, then practically speaking X must be at least twice differentiable.

A.2 Adaptive step size solvers
There is one further caveat that must be considered. Suppose X is twice differentiable, but that the second derivative is discontinuous. For example this would be accomplished by taking X to be a quadratic spline interpolation.
If seeking to solve equation (6) with an adaptive step-size solver, we found that the solver would take a long time to compute the backward pass, as it would have to slow down to resolve each jump in d2X/ds2, and then speed back up in the intervals in-between.

13

A.3 Natural cubic splines
This is then the reason for our selection of natural cubic splines: by ensuring that X is twice continuously differentiable, the above issue is ameliorated, and adaptive step size solvers operate acceptably. Cubic splines give essentially the minimum regularity for the techniques discussed in this paper to work ‘out of the box’ in all cases.
Other than this smoothness, however, there is little that is special about natural cubic splines. Other possible options are for example Gaussian processes [10, 11] or kernel methods [12]. Furthermore, especially in the case of noisy data it need not be an interpolation scheme – approximation and curve-ﬁtting schemes are valid too.

B Universal Approximation

The behaviour of controlled differential equations are typically described through the signature transform (also known as path signature or simply signature) [20] of the driving process X. We demonstrate here how a (Neural) CDE may be reduced to consideration of just the signature transform, in order to prove universal approximation.
The proof is essentially split into two parts. The ﬁrst part is to prove universal approximation with respect to continuous paths, as is typically done for CDEs. The second (lengthier) part is to interpret what this means for the natural cubic splines that we use in this paper, so that we can get universal approximation with respect to the original data as well.
Deﬁnition B.1. Let τ, T ∈ R with τ < T and let v ∈ N. Let V1([τ, T ]; Rv) represent the space of continuous functions of bounded variation. Equip this space with the norm

X→ X = X + X .

V

∞

BV

This is a somewhat unusual norm to use, as bounded variation seminorms are more closely aligned with L1 norms than L∞ norms.

Deﬁnition B.2. For any X ∈ V1([τ, T ]; Rv) let Xt = (Xt, t) ∈ V1([τ, T ]; Rv+1). We choose to use the notation of ‘removing the hat’ to denote this time augmentation, for consistency with the main text which uses X for the time-augmented path.

Deﬁnition B.3. For any N, v ∈ N, let κ(N, v) = Ni=0(v + 1)i.

Deﬁnition B.4 (Signature transform). For any k ∈ N and any y ∈ Rk, let M (y) ∈ Rk(v+1)×(v+1)

be the matrix

y1 0 0 · · · 0 

y2 0 0 · · · 0 

 ... ... ...

... 

yk 0 0 · · · 0 

 0 y1 0 · · · 0 

M (y) =  0 ... 0

..  .

 0 yk 0 · · · 0 

 

...

 

 0 0 0 · · · y1

 .. .

...

... · · ·

..  .

0 0 0 · · · yk

Fix N ∈ N and X ∈ V1([τ, T ]; Rv+1). Let y0,X,N : [τ, T ] → R be constant with yt0,X,N = 1.

For all i ∈ {1, . . . , N }, iteratively let yi,X,N : [τ, T ] → R(v+1)i be the value of the integral

yti,X,N = yτi,X,N +

t
M (ysi−1,X,N )dXs
τ

for t ∈ (τ, T ],

with yτi,X,N = 0 ∈ R(v+1)i .

14

Then we may stack these together:

yX,N = (y0,X,N , . . . , yN,X,N ) : [τ, T ] → Rκ(N,v),

M (yX,N ) = (0, M ◦ y0,X,N , . . . , M ◦ yN−1,X,N ) : [τ, T ] → Rκ(N,v)×v.

Then yX,N is the unique solution to the CDE

ytX,N = yτX,N + with yτX,N = (1, 0, . . . , 0).

t
M (yX,N )sdXs
τ

for t ∈ (τ, T ],

Then the signature transform truncated to depth N is deﬁned as the map

SigN : V1([τ, T ]; Rv+1) → Rκ(N,v), SigN : X → yTX,N .

If this seems like a strange deﬁnition, then note that for any a ∈ Rk and any b ∈ Rv that M (a)b is equal to a ﬂattened vector corresponding to the outer product a ⊗ b. As such, the signature CDE is
instead typically written much more concisely as the exponential differential equation

ytX,N = yτX,N +

t
ysX,N ⊗ dXs
τ

for t ∈ (τ, T ],

however we provide the above presentation for consistency with the rest of the text, which does not introduce ⊗.

Deﬁnition B.5. Let V01([τ, T ]; Rv) = X ∈ V1([τ, T ]; Rv) X0 = 0 .
With these deﬁnitions out of the way, we are ready to state the famous universal nonlinearity property of the signature transform. We think [22, Theorem 4.2] gives the most straightforward proof of this result. This essentially states that the signature gives a basis for the space of functions on compact path space. Theorem B.6 (Universal nonlinearity). Let τ, T ∈ R with τ < T and let v, u ∈ N. Let K ⊆ V01([τ, T ]; Rv) be compact. (Note the subscript zero.) Let SigN : V1([τ, T ]; Rv+1) → Rκ(N,v) denote the signature transform truncated to depth N .
Let J N,u = ℓ : Rκ(N,v) → Ru ℓ is linear .

Then is dense in C(K; Ru).

X → ℓ(SigN (X)) ℓ ∈ J N,u
N ∈N

With the universal nonlinearity property, we can now prove universal approximation of CDEs with respect to controlling paths X. Theorem B.7 (Universal approximation with CDEs). Let τ, T ∈ R with τ < T and let v, u ∈ N. For any w ∈ N let
F w = f : Rw → Rw×(v+1) f is continuous ,
Lw,u = {ℓ : Rw → Ru | ℓ is linear} , ξw = ζ : Rv+1 → Rw ζ is continuous .

For any w ∈ N, any f ∈ F w and any ζ ∈ ξw and any X ∈ V1([τ, T ]; Rv), let zf,ζ,X : [τ, T ] → Rw be the unique solution to the CDE

ztf,ζ,X = zτf,ζ,X +

t
f (zsf,ζ,X )dXs
τ

for t ∈ (τ, T ],

15

with zτf,ζ,X = ζ(Xτ ). Let K ⊆ V1([τ, T ]; Rv) be compact.

Then
w∈N
is dense in C(K; Ru).

X → ℓ(zTf,ζ,X )

f ∈ F w, ℓ ∈ Lw,u, ζ ∈ ξw

Proof. We begin by prepending a straight line segment to every element of K. For every X ∈ K, deﬁne X∗ : [τ − 1, T ] → Rv by

X∗ = (t − τ + 1)Xτ t ∈ [τ − 1, τ ),

t

Xt

t ∈ [τ, T ].

Similarly deﬁne X∗, so that a hat means that time is not a channel, whilst a star means that an extra straight-line segment has been prepended to the path.

Now let K∗ = X∗ X ∈ K . Then K∗ ⊆ V01([τ − 1, T ]; Rv) and is also compact. Therefore by

Theorem B.6,

X∗ → ℓ(SigN (X∗)) ℓ ∈ J N,u

N ∈N

is dense in C(K∗; Ru).

So let α ∈ C(K; Ru) and ε > 0. The map X → X∗ is a homeomorphism, so we may ﬁnd β ∈ C(K∗; Ru) such that β(X∗) = α(X) for all X ∈ K. Next, there exists some N ∈ N and ℓ ∈ JN,u such that γ deﬁned by γ : X∗ → ℓ(SigN (X∗)) is ε-close to β.
By Deﬁnition B.4 there exists f ∈ F κ(N,v) so that SigN (X∗) = yTX∗ for all X∗ ∈ K∗, where yX∗ is the unique solution of the CDE

ytX∗ = yτX−∗1 +

t

f

(y

X s

∗

)dX

∗ s

τ −1

for t ∈ (τ − 1, T ],

with yτX−∗1 = (1, 0, . . . , 0).

Now let ζ ∈ ξ be deﬁned by ζ(Xτ ) = yτX∗, which we note is well deﬁned because the value of ytX∗ only depends on Xτ for t ∈ [τ − 1, τ ].

Now for any X ∈ K let zX : [τ, T ] → Rw be the unique solution to the CDE

t
ztX = zτX + f (zsX )dXs
τ

for t ∈ (τ, T ],

with zτX = ζ(Xτ ).

Then by uniqueness of solution, ztX = ytX∗ for t ∈ [τ, T ], and so in particular SigN (X∗) = yTX∗ = zTX .

Finally it remains to note that ℓ ∈ J N,u = Lκ(N,v),u.

So let δ be deﬁned by δ : X → ℓ(zTX). Then δ is in the set which we were aiming to show density of (with w = κ(N, v), f ∈ F w, ℓ ∈ Lw,u and ζ ∈ ξ as chosen above), whilst for all X ∈ K,

δ(X) = ℓ(zTX) = ℓ(SigN (X∗)) = γ(X∗) is ε-close to β(X∗) = α(X). Thus density has been established.

Lemma B.8. Let K ⊆ C2([τ, T ], Rv) be uniformly bounded with uniformly bounded ﬁrst and
second derivatives. That is, there exists some C > 0 such that X + dX/dt ∞+ d2X/dt2 ∞ < C
∞
for all X ∈ K. Then K ⊆ V1([τ, T ]; Rv) and is relatively compact (that is, its closure is compact) with respect to · V .

16

Proof. K is bounded in C2([τ, T ], Rv) so it is relatively compact in C1([τ, T ], Rv). Furthermore for any X ∈ K,

X=
V
=
≤

X +X

∞

BV

X + dX ∞ dt
1

X + dX , ∞ dt
∞

and so the embedding C1([τ, T ], Rv) → V1([τ, T ]; Rv) is continuous. Therefore K is also relatively compact in V1([τ, T ]; Rv).

Next we need to understand how a natural cubic spline is controlled by the size of its data. We establish the following crude bounds.
Lemma B.9. Let v ∈ N. Let x0, . . . , xn ∈ Rv. Let t0, . . . , tn ∈ R be such that t0 < t1 < · · · < tn. Let X : [t0, tn] → Rv be the natural cubic spline such that X(ti) = xi. Let τi = ti+1 − ti for all i. Then there exists an absolute constant C > 0 such that

X + dX/dt ∞+ d2X/dt2 ∞ < C τ ∞ x ∞ (min τi)−2( τ ∞ + (min τi)−1).

∞

i

i

Proof. Surprisingly, we could not ﬁnd a reference for a fact of this type, but it follows essentially straightforwardly from the derivation of natural cubic splines.
Without loss of generality assume v = 1, as we are using the inﬁnity norm over the dimensions v, and each cubic interpolation is performed separately for each dimension.

Let the i-th piece of X, which is a cubic on the interval [ti, ti+1], be denoted Yi. Without loss of generality, translate each Yi to the origin so as to simplify the algebra, so that Yi : [0, τi] → R. Let Yi(t) = ai + bit + cit2 + dit3 for some coefﬁcients ai, bi, ci, di and i ∈ {0, . . . , n − 1}.
Letting Di = Yi′(0) for i ∈ {0, . . . , n − 1} and Dn = Yn−1(τn−1), the displacement and derivative conditions imposed at each knot are Yi(0) = xi, Yi(τi) = xi+1, Yi′(0) = Di and Yi′(τi) = Di+1. This then implies that ai = xi, bi = Di,

ci = 3τi−2(xi+1 − xi) − τi−1(Di+1 + 2Di),

(7)

di = 2τi−3(xi − xi + 1) + τi−2(Di+1 + Di).

(8)

Letting denote ‘less than or equal up to some absolute constant’, then these equations imply that

X = max Yi ∞

∞

i

dX

dt

= max Yi ∞
i

∞

d2X

dt2

= max Yi ∞
i

∞

max(|xi| + τi |Di|) ≤ x ∞ + τ ∞ D ∞ ,

(9)

i

max(τi−1 |xi| + |Di|) ≤ x ∞ (min τi)−1 + D ∞ ,

(10)

i

i

max(τi−2 |xi| + τi−1 |Di|) ≤
i

x ∞ (min τi)−2 +
i

D ∞ (min τi)−1.
i
(11)

Next, the second derivative condition at each knot is Yi′−′ 1(τi−1) = Yi′′(0) for i ∈ {1, . . . , n − 1}, and the natural condition is Y0′′(0) = 0 and Yn′′−1(τn−1) = 0. With equations (7), (8) this gives
T D = k,

17

where 2τ0−1  τ0−1 
T =  

τ0−1 2(τ0−1 + τ1−1)
τ1−1

τ1−1 2(τ1−1 + τ2−1)
...

τ2−1 ...
τn−−12

 D0
D =  ...  ,

Dn

 3τ0−2(x1 − x0) 

 3τ1−2(x2 − x1) + 3τ0−2(x1 − x0) 

k = 

...



3τn−−21(xn − xn−1) + 3τn−−22(xn−1 − xn−2)

3τn−−21(xn − xn−1).

... 2(τn−−12 + τn−−11)
τn−−11

    , τ −1 
n−1
2τn−−11

Let T −1 ∞ denote the operator norm and D ∞, k ∞ denote the elementwise norm. Now T is diagonally dominant, so the Varah bound [67] and HM-AM inequality gives

T −1 ∞ ≤ (min(τi−1 + τi−+11))−1
i

τ ∞.

Thus

D∞

τ∞ k∞

τ ∞ x ∞ (min τi)−2.
i

Together with equations (9)–(11) this gives the result.

Deﬁnition B.10 (Space of time series). Let v ∈ N. and τ, T ∈ R such that τ < T . We deﬁne the space of time series in [τ, T ] over Rv as
TS[τ,T ] (Rv) = {((t0, x0), . . . , (tn, xn)) | n ∈ N, ti ∈ [τ, T ], xn ∈ Rv, t0 = τ, tn = T, n ≥ 2} .

To our knowledge, there is no standard topology on time series. One option is to treat them as sequences, however it is not clear how best to treat sequences of different lengths, or how to incorporate timestamp information. Given that a time series is typically some collection of observations from some underlying process, we believe the natural approach is to treat them as subspaces of functions.
Deﬁnition B.11 (General topologies on time series). Let v ∈ N. and τ, T ∈ R such that τ < T . Let F denote some topological space of functions. Let ι : TS[τ,T ] (Rv) → F be some map. Then we may deﬁne a topology on TS[τ,T ] (Rv) as the weakest topology with respect to which ι is continuous.
Recall that we use subscripts to denote function evaulation.
Deﬁnition B.12 (Natural cubic spline topology). Let v ∈ N. and τ, T ∈ R such that τ < T . Let F = C([τ, T ]; Rv) equipped with the uniform norm. For all x = ((t0, x0), . . . , (tn, xn)) ∈ TS[τ,T ] (Rv), let ι : TS[τ,T ] (Rv) → F produce the natural cubic spline such that ι(x)ti = xi with knots at t0, . . . , tn. Then this deﬁnes a topology on TS[τ,T ] (Rv) as in the previous deﬁnition.
Remark B.13. In fact this deﬁnes a seminorm on TS[τ,T ] (Rv), by x = ι(x) ∞. This is only a seminorm as for example ((0, 0), (2, 2)) and ((0, 0), (1, 1), (2, 2)) have the same natural cubic spline interpolation. This can be worked around so as to instead produce a full norm, but it is a deliberate choice not to: we would often prefer that these time series be thought of as equal. (And if it they are not equal, then ﬁrst augmenting with observational intensity as in the main paper should distinguish them.)

18

Theorem B.14 (Universal approximation with Neural CDEs via natural cubic splines). Let τ, T ∈ R with τ < T and let v, u ∈ N. For any w ∈ N let
FNwN = f : Rw → Rw×(v+1) f is a feedforward neural network , Lw,u = {ℓ : Rw → Ru | ℓ is linear} , ξNwN = ζ : Rv+1 → Rw ζ is a feedforward neural network .

Let ι denote the natural cubic spline interpolation as in the previous deﬁnition, and recall that ‘removing the hat’ is our notation for augmenting with time. For any w ∈ N, any f ∈ F w and any ζ ∈ ξNwN and any x ∈ TS[τ,T ] (Rv), let zf,ζ,x : [τ, T ] → Rw be the unique solution to the CDE

ztf,ζ,x = zτf,ζ,x +

t
f (zsf,ζ,x)dι(x)s
τ

for t ∈ (τ, T ],

with zτf,ζ,x = ζ(ι(x)τ ). Let K ⊆ TS[τ,T ] (Rv) be such that there exists C > 0 such that

x ∞ (min(ti+1 − ti))−3 < C

(12)

i

for every x = ((t0, x0), . . . , (tn, xn)) ∈ K. (With C independent of x.)

Then

x → ℓ(zTf,ζ,x) f ∈ FNwN , ℓ ∈ Lw,u, ζ ∈ ξNwN
w∈N

is dense in C(K; Ru) with respect to the natural cubic spline topology on TS[τ,T ] (Rv).

Proof. Fix x = ((t0, x0), . . . , (tn, xn)) ∈ K. Let X = ι(x). Now τ ∞ ≤ T − τ is bounded so by Lemma B.9 and the assumption of equation (12), there exists a constant C1 > 0 independent of

x such that

dX

d2X

X ∞ + dt + dt2 < C1.

∞

∞

Thus by Lemma B.8, ι(K) is relatively compact in V1([τ, T ]; Rv).

Let K1 = ι(K), where the overline denotes a closure. Now by Theorem B.7, and deﬁning F w and ξw as in the statement of that theorem,

ι(x) → ℓ(zTf,ζ,x) f ∈ F w, ℓ ∈ Lw,u, ζ ∈ ξw
w∈N
is dense in C(K1, Ru).
For any f ∈ F w, any ζ ∈ ξw, any fNN ∈ FNwN and any ζNN ∈ ξNwN , the terminal values zTf,ζ,x and zTfNN ,ζNN ,x may be compared by standard estimates, for example as commonly used in the proof of Picard’s theorem. Classical universal approximation results for neural networks [68, 69] then yield that
ι(x) → ℓ(zTf,ζ,x) f ∈ FNwN , ℓ ∈ Lw,u, ζ ∈ ξNwN
w∈N
is dense in C(K1, Ru). By the deﬁnition of the natural cubic spline topology on TS[τ,T ] (Rv), then

w∈N
is dense in C(K, Ru).

x → ℓ(zTf,ζ,x)

f ∈ FNwN , ℓ ∈ Lw,u, ζ ∈ ξNwN

19

C Comparison to alternative ODE models
Suppose if instead of equation (4), we replace gθ,X(z, s) by hθ(z, Xs) for some other vector ﬁeld hθ. This might seem more natural. Instead of having gθ,X be linear in dX/ds, we take a hθ that is potentially nonlinear in the control Xs. Have we gained anything by doing so? It turns out no, and in fact we have lost something. The Neural CDE setup directly subsumes anything depending directly on X. Theorem C.1. Let τ, T ∈ R with τ < T , let v, w ∈ N with v + 1 < w. Let
F = f : Rw → Rw×(v+1) f is continuous , H = h : Rw−v−1 × Rv+1 → Rw−v−1 h is continuous , ξ = ζ : Rv+1 → Rw ζ is continuous , X = X : [τ, T ] → Rv X continuous and of bounded variation .

For any X ∈ X, let Xt = (Xt, t). Let π : Rw → Rw−v−1 be the orthogonal projection onto the ﬁrst w − v − 1 coordinates.
For any f ∈ F , any ζ ∈ ξ, and any X ∈ X, let zf,ζ,X : [τ, T ] → Rw be the unique solution to

ztf,ζ,X = zτf,ζ,X +

t
f (zsf,ζ,X )dXs
τ

for t ∈ (τ, T ],

with zτf,ζ,X = ζ(Xτ ).

Similarly for any h ∈ H, any ζ ∈ ξ, and any X ∈ X, let yf,X : [τ, T ] → Rw−v−1 be the unique solution to

yth,ζ,X = yτh,ζ,X +

t

h

(y

h s

,

ζ

,

X

,

X

s

)ds

τ

for t ∈ (τ, T ],

with yτh,ζ,X = π(ζ(Xτ )). Let Y = X → yh,ζ,X h ∈ H, ζ ∈ ξ

and Z =

X → π ◦ zf,ζ,X

f ∈ F, ζ ∈ ξ .

Then Y Z.

In the above statement, then a practical choice of f ∈ F or h ∈ H will typically correspond to some trained neural network.
Note the inclusion of time via the augmentation X → X. Without it, then the reparameterisation invariance property of CDEs [18], [23, Proposition A.7] will restrict the possible functions that CDEs can represent. This hypothesis is not necessary for the Y = Z part of the conclusion.
Note also how the CDE uses a larger state space of w, compared to w−v−1 for the alternative ODE. The reason for this is that whilst f has no explicit nonlinear dependence on X, we may construct it to have such a dependence implicitly, by recording X into v+1 of its w hidden channels, whereupon X is hidden state and may be treated nonlinearly. This hypothesis is also not necessary to demonstrate the Y = Z part of the conclusion.
This theorem is essentially an algebraic statement, and is thus not making any analytic claims, for example on universal approximation.

Proof. That Y = Z: Let zf,ζ,· ∈ Z for ζ ∈ ξ arbitrary and f ∈ F constant and such that
20





 1 0 0 ··· 0 

 0 0 0 · · · 0 

f (z) = .. .. ..

..  w

...

.

0 0 0 ··· 0

v+1

Then for any X ∈ X, the corresponding CDE solution in Z is

ztf,ζ,X = zτf,ζ,X +

t
f (zsf,ζ,X )dXs,
τ

and so the ﬁrst component of its solution is

ztf,ζ,X,1 = Xt1 − Xτ1 + ζ1(Xτ ),

whilst the other components are constant

ztf,ζ,X,i = ζi(Xτ )

for i ∈ {2, . . . , w}, where superscripts refer to components throughout.

Now suppose for contradiction that there exists yh,ζ,· ∈ Y for some Ξ ∈ ξ and h ∈ H such that yh,Ξ,X = π ◦ zf,ζ,X for all X ∈ X. Now yh,Ξ,X must satisfy

yth,Ξ,X = yτh,Ξ,X +

t

h

(y

h s

,

Ξ

,

X

,

X

s

)ds

,

τ

and so

t

(Xt1−Xτ1+ζ1(Xτ ), 0, . . . , 0) = π(Ξ(Xτ ))+

h

((X

1 s

−

X

1 τ

+

ζ

1

(X

τ

),

ζ

2

(X

τ

),

.

.

.

,

ζ

w

(X

τ

)),

X

s

)ds

.

τ

Consider those X which are differentiable. Differentiating with respect to t now gives

dX1 (t) = h1((X1 − X1 + ζ1(Xτ ), ζ2(Xτ ), . . . , ζw(Xτ )), Xt).

(13)

dt

s

τ

That is, h1 satisﬁes equation (13) for all differentiable X. This is clearly impossible: the right hand side is a function of t, Xt and Xτ only, which is insufﬁcient to determine dX1/dt(t).

That Y ⊆ Z: Let yh,Ξ,X ∈ Y for some Ξ ∈ ξ and h ∈ H. Let σ : Rw → Rv+1 be the orthogonal projection onto the last v + 1 coordinates. Let ζ ∈ ξ be such that π ◦ ζ = π ◦ Ξ and σ(ζ(Xτ )) = Xτ . Then let f ∈ F be deﬁned by





 0 0 · · · 0 h1(π(z), σ(z)) 

 . .

.

.



 .. ..

..

..



 0 0 · · · 0 hw−v−1(π(z), σ(z)) 

f (z) = 1 0 · · · 0

0

 

 0 1 · · · 0

0



 

...

...

...

...

...

 

 

0

0

···

1

0

 

0 0 ··· 0

1

w−v−1 v+1

v

1

21

Then for t ∈ (τ, T ],

ztf,ζ,X = ζ(Xτ ) + = ζ(Xτ ) +
= ζ(Xτ ) + = ζ(Xτ ) +

t

f (zsf,ζ,X )dXs

τ 0 0 ··· 0

h1(π(zf,ζ,X ), σ(zf,ζ,X )) 

s

s

 ... ...

...

...

  

0 0 · · · 0 hw−v−1(π(zf,ζ,X ), σ(zf,ζ,X )) dXs1

t 1 0 · · · 0

s
0

s

  ... 

τ 0..

1 ..

··· ..

0 ..

0 ..



dX

v s



 ds

0.

. 0

. ···

. 1

. 0



0 0 ··· 0

1

 h1(π(zf,ζ,X ), σ(zf,ζ,X ))ds 

s

s



...



t hw−v−1(π(zsf,ζ,X ), σ(zsf,ζ,X ))ds



dXs1



τ  

...

 



dXsv



ds

t

h

(π

(z

f s

,

ζ

,

X

),

σ

(z

f s

,

ζ

,

X

))ds

.

τ

dXs

Thus in particular

t
σ(ztf,ζ,X ) = σ(ζ(Xτ )) + dXs = σ(ζ(Xτ )) − Xτ + Xt = Xt.
τ

Thus π(ztf,ζ,X ) = π(ζ(Xτ )) +

t
h(π(zsf,ζ,X ), σ(zsf,ζ,X ))ds = π(Ξ(Xτ )) +
τ

t

h

(π

(z

f s

,

ζ

,

X

),

X

s

)ds

.

τ

Thus we see that π(zf,ζ,X ) satisﬁes the same differential equation as yh,Ξ,X. So by uniqueness of solution [20, Theorem 1.3], yh,Ξ,X = π(zf,ζ,X) ∈ Z.

D Experimental details

D.1 General notes

Code

Code to reproduce every experiment can be found at

https://github.com/patrick-kidger/NeuralCDE.

Normalisation Every dataset was normalised so that each channel has mean zero and variance one.

Loss Every binary classiﬁcation problem used binary cross-entropy loss applied to the sigmoid of the output of the model. Every multiclass classiﬁcation problem used cross-entropy loss applied to the softmax of the output of the model.

Architectures For both the Neural CDE and ODE-RNN, the integrand fθ was taken to be a feedforward neural network. A ﬁnal linear layer was always used to map from the terminal hidden state to the output.

Activation functions For the Neural CDE model we used ReLU activation functions. Following the recommendations of [13], we used tanh activation functions for the ODE-RNN model, who remark that for the ODE-RNN model, tanh activations seem to make the model easier for the ODE

22

solver to resolve. Interestingly we did not observe this behaviour when trying tanh activations and method=‘dopri5’ with the Neural CDE model, hence our choice of ReLU.
Optimiser Every problem used the Adam [70] optimiser as implemented by PyTorch 1.3.1 [71]. Learning rate and batch size varied between experiments, see below. The learning rate was reduced if metrics failed to improve for a certain number of epochs, and training was terminated if metrics failed to improve for a certain (larger) number of epochs. The details of this varied by experiment, see the individual sections. Once training was ﬁnished, then the parameters were rolled back to the parameters which produced the best validation accuracy throughout the whole training procedure. The learning rate for the ﬁnal linear layer (mapping from the hidden state of a model to the output) was typically taken to be much larger than the learning rate used elsewhere in the model; this is a standard trick that we found improved performance for all models.
Hyperparameter selection In brief, hyperparameters were selected to optimise the ODE-RNN baseline, and equivalent hyperparameters used for the other models.
In more detail:
We began by selecting the learning rate. This was selected by starting at 0.001 and reducing it until good performance was achieved for a small ODE-RNN model with batch size 32.
After this, we increased the batch size until the selected model trained at what was in our judgement a reasonable speed. As is standard practice, we increased the learning rate proportionate to the increase in batch size.
Subsequently we selected model hyperparameters (number of hidden channels, width and depth of the vector ﬁeld network) via a grid search to optimise the ODE-RNN baseline. A single run of each hyperparameter choice was performed. The equivalent hyperparameters were then used on the GRU-∆t, GRU-D, GRU-ODE baselines, and also our Neural CDE models, after being adjusted to produce roughly the same number of parameters for each model.
The grids searched over and the resulting hyperparameters are stated in the individual sections below. Weight regularisation L2 weight regularisation was applied to every parameter of the ODERNN, GRU-∆t and GRU-D models, and to every parameter of the vector ﬁelds for the Neural CDE and GRU-ODE models.
ODE Solvers The ODE components of the ODE-RNN, GRU-ODE, and Neural CDE models were all computed using the fourth-order Runge-Kutta with 3/8 rule solver, as implemented by passing method=‘rk4’ to the odeint adjoint function of the torchdiffeq [24] package. The step size was taken to equal the minimum time difference between any two adjacent observations.
Adjoint backpropagation The GRU-ODE, Neural CDE and the ODE component of the ODE-RNN are all trained via the adjoint backpropagation method [15], as implemented by odeint adjoint function of the torchdiffeq package.
Computing infrastructure All experiments were run on one of two computers; both used Ubuntu 18.04.4 LTS, were running PyTorch 1.3.1, and used version 0.0.1 of the torchdiffeq [24] package. One computer was equipped with a Xeon E5-2960 v4, two GeForce RTX 2080 Ti, and two Quadro GP100, whilst the other was equipped with a Xeon Silver 4104 and three GeForce RTX 2080 Ti.
D.2 CharacterTrajectories
The learning rate used was 0.001 and the batch size used was 32. If the validation loss stagnated for 10 epochs then the learning rate was divided by 10 and training resumed. If the training loss or training accuracy stagnated for 50 epochs then training was terminated. The maximum number of epochs allowed was 1000.
We combined the train/test split of the original dataset (which are of unusual proportion, being 50%/50%), and then took a 70%/15%/15% train/validation/test split.
The initial condition ζθ of the Neural CDE model was taken to be a learnt linear map from the ﬁrst observation to the hidden state vector. (Recall that is an important part of the model, to avoid translation invariance.)
23

The hyperparameters were optimised (for just the ODE-RNN baseline as previously described) by performing most of a grid search over 16 or 32 hidden channels, 32, 48, 64, 128 hidden layer size, and 1, 2, 3 hidden layers. (The latter two hyperparameters corresponding to the vector ﬁelds of the ODE-RNN and Neural CDE models.) A few option combinations were not tested due to the poor performance of similar combinations. (For example every combination with hidden layer size of 128 demonstrated relatively poor performance.) The search was done on just the 30% missing data case, and the same hyperparameters were used for the 50% and 70% missing data cases.
The hyperparameters selected were 32 hidden channels for the Neural CDE and ODE-RNN models, and 47 hidden channels for the GRU-∆t, GRU-D and GRU-ODE models. The Neural CDE and ODE-RNN models both used a feedforward network for their vector ﬁelds, with 3 hidden layers each of width 32. The resulting parameter counts for each model were 8212 for the Neural CDE, 8436 for the ODE-RNN, 8386 for the GRU-D, 8292 for the GRU-∆t, and 8372 for the GRU-ODE.
D.3 PhysioNet sepsis prediction
The batch size used was 1024 and learning rate used was 0.0032, arrived at as previously described. If the training loss stagnated for 10 epochs then the learning rate was divided by 10 and training resumed. If the training loss or validation accuracy stagnated for 100 epochs then training was terminated. The maximum number of epochs allowed was 200. The learning rate for the ﬁnal linear layer (a component of every model, mapping from the ﬁnal hidden state to the output) used a learning rate that 100 times larger, so 0.32.
The original dataset does not come with an existing split, so we took our own 70%/15%/15% train/validation/test split.
As this problem featured static (not time-varying) features, we incorporated this information by allowing the initial condition of every model to depend on these. This was taken to be a single hidden layer feedforward network with ReLU activation functions and of width 256, which we did not attempt a hyperparameter search over.
As this dataset is partially observed, then for the ODE-RNN, GRU-∆t, GRU-D models, which require something to be passed at each time step, even if a value is missing, then we ﬁll in missing values with natural cubic splines, for ease of comparison with the Neural CDE and ODERNN models. (We do not describe this as imputation as for the observational intensity case the observational mask is additionally passed to these models.) In particular this differs slightly from the usual implementation of GRU-D, which usually use a weighted average of the last observation and the mean. Splines accomplishes much the same thing, and help keep things consistent between the various models.
The hyperparameters were optimised (for just the ODE-RNN baseline as previously described) by performing most of a grid search over 64, 128, 256 hidden channels, 64, 128, 256 hidden layer size, and 1, 2, 3, 4 hidden layers. (The latter two hyperparameters corresponding to the vector ﬁelds of the ODE-RNN and Neural CDE models.)
The hyperparameters selected for the ODE-RNN model were 128 hidden channels, and a vector ﬁeld given by a feedforward neural network with hidden layer size 128 and 4 hidden layers. In order to keep the number of parameters the same between each model, this was reduced to 49 hidden channels and hidden layer size 49 for the Neural CDE model, and increased to 187 hidden channels for the GRU-∆t, GRU-D and GRU-ODE models. When using observational intensity, the resulting parameter counts were 193541 for the Neural CDE, 194049 for the ODE-RNN, 195407 for the GRUD, 195033 for the GRU-∆t, and 194541 for the GRU-ODE. When not using observational intensity, the resulting parameter counts were 109729 for the Neural CDE, 180097 for the ODE-RNN, 175260 for the GRU-D, 174886 for the GRU-∆t, and 174921 for the GRU-ODE. Note the dramatically reduced parameter count for the Neural CDE; this is because removing observational intensity reduces the number of channels, which affects the parameter count dramatically as discussed in Section 6.3.
D.4 Speech Commands
The batch size used was 1024 and the learning rate used was 0.0016, arrived at as previously described. If the training loss stagnated for 10 epochs then the learning rate was divided by 10 and
24

training resumed. If the training loss or validation accuracy stagnated for 100 epochs then training was terminated. The maximum number of epochs allowed was 200. The learning rate for the ﬁnal linear layer (a component of every model, mapping from the ﬁnal hidden state to the output) used a learning rate that 100 times larger, so 0.16. Each time series from the dataset is univariate and of length 16000. We computed 20 Melfrequency cepstral coefﬁcients of the input as implemented by torchaudio.transforms.MFCC, with logarithmic scaling applied to the coefﬁcients. The window for the short-time Fourier transform component was a Hann window of length 200, with hop length of 100, with 200 frequency bins. This was passed through 128 mel ﬁlterbanks and 20 mel coefﬁcients extracted. This produced a time series of length 161 with 20 channels. We took a 70%/15%/15% train/validation/test split. The hyperparameters were optimised (for just the ODE-RNN baseline as previously described) by performing most of a grid search over 32, 64, 128 hidden channels, 32, 64, 128 hidden layer size, and 1, 2, 3, 4 hidden layers. (The latter two hyperparameters corresponding to the vector ﬁelds of the ODE-RNN and Neural CDE models.) The hyperparameters selected for the ODE-RNN model were 128 hidden channels, and a vector ﬁeld given by a feedforward neural network with hidden layer size 64 and 4 hidden layers. In order to keep the number of parameters the same between each model, this was reduced to 90 hidden channels and hidden layer size 40 for the Neural CDE model, and increased to 160 hidden channels for the GRU-∆t, GRU-D and GRU-ODE models. The resulting parameter counts were 88940 for the Neural CDE model, 87946 for the ODE-RNN model, 89290 for the GRU-D model, 88970 for the GRU-dt model, and 89180 for the GRU-ODE model.
25

