arXiv:2204.10806v1 [cs.HC] 22 Apr 2022

A Unifying Framework for Combining Complementary Strengths of Humans and ML toward Better Predictive Decision-Making
Charvi Rastogi∗, Liu Leqi∗, Kenneth Holstein, Hoda Heidari
School of Computer Science, Carnegie Mellon University Email: {crastogi, leqil, kjholste, hheidari}@cs.cmu.edu
Abstract
Hybrid human-ML systems are increasingly in charge of consequential decisions in a wide range of domains. A growing body of work has advanced our understanding of these systems by providing empirical and theoretical analyses. However, existing empirical results are mixed, and theoretical proposals are often incompatible with each other. Our goal in this work is to bring much-needed organization to this ﬁeld by oﬀering a unifying framework for understanding conditions under which combining complementary strengths of human and ML leads to higher quality decisions than those produced by them individually—a state to which we refer to as human-ML complementarity. We focus speciﬁcally on the context of human-ML predictive decision-making systems and investigate optimal ways of combining human and ML-based predictive decisions, accounting for the underlying causes of variation in their judgments. Within this scope, we present two crucial contributions. First, drawing upon prior literature in human psychology, machine learning, and human-computer interaction, we introduce a taxonomy characterizing a wide variety of criteria across which human and machine decision-making diﬀer. Building on our taxonomy, our second contribution presents a unifying optimizationbased framework for formalizing how human and ML predictive decisions should be aggregated optimally. We show that our proposed framework encompasses several existing models of human-ML complementarity as special cases. Last but not least, the exploratory analysis of our framework oﬀers a critical piece of insight for future work in this area: the mechanism by which we combine human-ML judgments should be informed by the underlying causes of their diverging decisions.
1 Introduction
In recent years, we have witnessed a rapid growth in the deployment of Machine Learning (ML) models in decision-making systems across a wide range of domains, including healthcare (Patel et al., 2019; Rajpurkar et al., 2020; Tschandl et al., 2020; Bien et al., 2018), credit lending (Bussmann et al., 2021; Kruppa et al., 2013), criminal justice (Angwin et al., 2016; Kleinberg et al., 2017), and employment (Raghavan et al., 2020; Hoﬀman et al., 2017). For example, in the criminal justice system, algorithmic recidivism risk scores inform pre-trial bail decisions for defendants (Angwin et al., 2016). In credit lending, lenders routinely use credit-scoring models to assess the risk of default by applicants (Kruppa et al., 2013). The excitement around modern ML systems facilitating high-stakes decisions is fueled by the promise of these technologies to tap into large datasets, mine the relevant statistical patterns within them, and utilize those patterns to make more accurate
∗indicates equal contribution. The ordering was decided by a coin toss.
1

predictions at a lower cost and without suﬀering from the same cognitive biases and limitation as human decision-makers. Growing evidence, however, suggests that ML models are vulnerable to various biases (Angwin et al., 2016), instability (Finlayson et al., 2018), and opaqueness (Burrell, 2016). Additionally, they lack crucial human strengths such as contextual knowledge and commonsense reasoning (Holstein and Aleven, 2021; Lake et al., 2017; Miller, 2019). These observations have led to calls for human involvement in high-stakes decision-making systems—with the hope of combining and amplifying the respective strengths of human thinking and ML models through carefully designed hybrid decision-making systems. Such systems consist of ML models and human experts jointly making decisions, and they are common in practice—including in the domains mentioned above.
Researchers have proposed and tested various hybrid human-ML designs, ranging from human-in-theloop (Russakovsky et al., 2015) to algorithm-in-the-loop (De-Arteaga et al., 2020; Saxena et al., 2020; Brown et al., 2019; Green and Chen, 2019) arrangements. However, empirical ﬁndings regarding the success and eﬀectiveness of these proposals are mixed (Lai et al., 2021, and references therein). Simultaneously, a growing body of theoretical work has attempted to conceptualize and formalize these hybrid designs (Gao et al., 2021; Bordt and von Luxburg, 2020) and study optimal ways of aggregating human and ML judgments within them (Madras et al., 2018; Mozannar and Sontag, 2020; Wilder et al., 2020; Keswani et al., 2021; Raghu et al., 2019; Okati et al., 2021; Donahue et al., 2022; Steyvers et al., 2022). The existing theories, however, are hard to navigate and make sense of as a whole. The critical issue leading to this lack of coherence and organization is the wide range of (often implicit) idiosyncratic assumptions made in diﬀerent research articles—making it challenging to compare existing proposals and foresee the conditions under which one would outperform another in practice. Even within the same theoretical/conceptual framework, the empirical results are inconclusive and sensitive to the context, human expertise, and other situated factors (Lai et al., 2021).
While prior work has attempted to combine human and machine decisions optimally under various stylized settings, it has not systematically addressed the underlying causes of human-ML complementarity, the condition in which an aggregation mechanism can take the human and ML decisions as inputs and outputs decisions that outperform1 both human- and ML-based decision-making in isolation. We argue, therefore, that there is a clear need to form a deeper, more ﬁne-grained understanding of what types of human-ML systems lead to complementarity and under what conditions. In this work, we respond to this gap in the literature by building a taxonomy around sources of human-ML complementarity. This taxonomy aims to provide a shared understanding of the causes and conditions of complementarity so that researchers and practitioners can design more eﬀective hybrid systems—by ﬁrst investigating and enumerating the distinguishing characteristics of human vs. machine decision-making in their speciﬁc decision-making context.
We focus on a speciﬁc setting of hybrid human-ML decision-making, which we refer to as combining predictive decisions in static environments. This regime narrows down the scope of inquiry to: (a) Static environments: In particular, we set aside decision-making contexts with signiﬁcant externalities and those with sequential nature. (b) Predictive decisions: The decision at stake is solely based on predicting some outcome of interest (Mitchell et al., 2018). Henceforth, we use the terms ‘prediction’ and ‘decision’ interchangeably. Some examples of predictive decisions are diagnosis of diabetic retinopathy (Gulshan et al., 2016), predicting recidivism for pretrial, parole, sentencing decisions (Dressel and Farid, 2018), and consumer credit risk prediction (Bussmann et al., 2021; Kruppa et al., 2013). (c) Combining human and ML decisions: We consider
1“Outperforming” doesn’t necessarily refer to accuracy gains.
2

hybrid designs in which an independent third party combines human and ML predictions into a joint decision. Unlike algorithm-in-the-loop decision-making (Green and Chen, 2019), the human decision-maker does not have access to the ML prediction and is not inﬂuenced by it. Such designs are only sensible in domains where human and ML decisions are comparable and equally credible. This holds true for application domains such as image classiﬁcation (Beck et al., 2018; Russakovsky et al., 2015; Kerrigan et al., 2021), crowdsourcing (Kamar et al., 2012), and clinical radiology (Patel et al., 2019; Rajpurkar et al., 2020; Bien et al., 2018).
We emphasize that this setup does not capture many practical settings, but its stylized nature facilitates more rigorous investigations. Moreover, following traditions from cognitive science and computational social science (Lake et al., 2017; Marr and Poggio, 1977), we contrast human decisionmaking with the machine through a computational/quantitative lens. This computational view has several signiﬁcant limitations (e.g., including the fact that it ignores numerous critical nuances about human decision-making). However, as we will show in this work, with all these caveats in mind, avenues for further understanding of human-ML complementarity remain rich within the conﬁnes of our carefully-restricted scope.
To build our taxonomy of human-ML complementarity, we surveyed the literature on human behavior, cognitive and behavioral sciences, as well as psychology to understand the essential factors across which human and ML decision-making processes diﬀer. Our taxonomy provides a detailed list and characterization of the advantages and disadvantages of human and ML predictions in a static predictive decision-making regime (Section 2). We formalize and capture these factors within a unifying mathematical framework developed to analyze optimal ways of combining human and ML predictions according to their distinctive characteristics (Section 3). In this manner, we provide a generalized recipe (with the appropriate tools) to combine human and ML predictions optimally for a variety of contexts. We also demonstrate how our framework encapsulates many recent human-ML combination schemes, notably Mozannar and Sontag (2020) and Raghu et al. (2019) (Section 3.3). Last but not least, as a proof of concept we analyze a simple special case of our framework. In particular, we derive the optimal linear aggregation schemes under three distinct complementarity conditions: (1) human and ML models have access to diﬀerent features, (2) human decisions can be noisy/inconsistent, and (3) ML models are trained under biased target labels. Contrasting optimal aggregation under these special cases provides us with a critical piece of insight for future work in this area: that the mechanism by which we combine human-ML judgments should be informed by the unique strengths and weaknesses that each one brings to the table (Section 4).
In summary, we oﬀer a unifying taxonomy and formalization for human-ML complementarity. Our taxonomy characterizes major diﬀerences between human and machine predictions, and our unifying optimization-based framework formally characterizes optimal aggregation of human and machine decisions under various conditions. With this contribution, we hope to provide a common language and an organizational structure to inform future research in this increasingly important space for Human-AI research.
2 A Taxonomy of Human vs. ML Decision-making Capabilities
To investigate complementarity, we need to understand the respective strengths and drawbacks of the human decision-maker and the machine. If the human decision-maker and the machine have exactly the same set of knowledge and capabilities in a given application domain, then by our deﬁnition there is no room for complementarity. In many real-world settings, however, there are marked diﬀerences in the abilities of humans versus modern AI systems. For example, it has been
3

observed that while ML models draw inferences based on much larger bodies of data than humans could eﬃciently process (Jarrahi, 2018), human decision-makers bring rich contextual knowledge and common sense reasoning capabilities (Holstein and Aleven, 2021; Miller, 2019; Lake et al., 2017) to the decision-making process, which ML models are unable to replicate. Thus, we develop a taxonomy for human-ML decision-making that accounts for broad diﬀerences between human decision-makers and machine learning.
To inform this taxonomy, we draw from existing syntheses in human psychology, machine learning, AI, and human-computer interaction to understand distinguishing characteristics that have been observed between human decision-makers and ML in the context of predictive decision-making. In cognitive science, Lake et al. (2017) review major gaps between human and ML capabilities by synthesizing existing scientiﬁc knowledge about human abilities that have thus far deﬁed automation. In management science, Shrestha et al. (2019) identify characteristics of human and ML decisionmaking along four key axes: decision space, process and outcome interpretability, speed, and replicability, and discuss their combination for organizational decision-making. In human-computer interaction, Holstein et al. (2020) conceptually map distinct ways in which humans and AI can augment each others’ abilities in real-world educational contexts. More recently, Lai et al. (2021) surveyed empirical studies on human-AI decision-making to document trends in study design choices (e.g., decision tasks and evaluation metrics) and empirical ﬁndings. We draw upon this prior literature to summarize key diﬀerences in human and ML-based predictive decision-making across multiple domains, with an eye towards understanding opportunities to combine their strengths.
As with any modeling approach or analytic lens, computational-level explanations are inherently reductive, yet are often useful in making sense of complex phenomena for this very reason. Computational-level explanations often provide an account of a task an agent performs, the inputs that the agent takes in, the ways in which the agent processes and perceives these inputs, and the kinds of outputs that the agent produces. Accordingly, our taxonomy is organized into four elements: (1) task deﬁnition, (2) input, (3) internal processing, and (4) output. We expect, and in fact invite, future additions and modiﬁcations to our taxonomy, and hope that it serves as a stepping stone toward a uniﬁed understanding of human-ML complementarity under broader conditions.
2.1 Task Deﬁnition
We now describe distinguishing characteristics that have been observed in the task deﬁnitions used by the human versus machine decision-makers.
• Objective. Most machine learning models aim to only optimize the expected performance, e.g., minimize the expected loss for supervised learning models and maximize the expected cumulative rewards for reinforcement learning models. While there are some recent research on building models with respect to a more diverse set of objectives, including diﬀerent risk measures (Leqi et al., 2019; Khim et al., 2020), fairness deﬁnitions (Chouldechova and Roth, 2020) and interpretability notions (Lipton, 2018; Miller, 2019), these objectives are commonly far less complicated and comprehensive than the objectives that human decision-makers aim to optimize (Kleinberg et al., 2017). For example, when making a lending decision, in addition to diﬀerent risk factors, the bankers also care about the relationship with the clients and the lending practices in their organization (Tr¨onnberg and Hemlin, 2014).
• Misaligned construct of interest. ML models deployed in social contexts often involve unobservable theoretical constructs, such as socioeconomic status, teacher eﬀectiveness, and risk of recidivism, which cannot be measured directly. Instead they are inferred from measurements
4

of observable properties, and this process necessarily involves making assumptions. Jacobs and Wallach (2021) argue that several harms studied in the literature on fairness of sociotechnical systems are direct results of the mismatch between the construct of interests and the inferred measurements. For example, risk assessment tools in criminal justice predict the likelihood that an individual will be arrested for a new criminal oﬀense within some time window following their release. However, Fogliato et al. (2020, 2021b); Akpinar et al. (2021) found that there exist disparities in the likelihood of crime reporting and arrest, since not all crimes result in arrest, thus potentially leading to biases in the risk assessment tools. Furthermore, Obermeyer et al. (2019) traced the bias in risk scores for healthcare provision between black and white patients to the bias in the proxy target variable (cost of healthcare) utilized in the risk score model. On the other hand, doctors have a better intuitive understanding of the construct of interest (patients’ health outcomes), which cannot be easily quantiﬁed.
2.2 Input
We now describe the distinguishing characteristics that have been observed in the inputs used by the human and machine decision-maker.
• Access to diﬀerent information. From the input perspective, in many settings such as healthcare, criminal justice, humans and machines have access to both shared and non-overlapping information. This is because real-world decision-making contexts often contain features of importance that do not yield to codiﬁcation for the machine. For example, a doctor can see the physical presentation of a patient and understand their symptoms better, since this information is hard to codify and provide to the machine. Similarly, a judge learns about the predisposition of the defendant through interaction (Kleinberg et al., 2018).
• Nature of past experiences. The nature of embodied human experience over the course of a lifetime diﬀers substantially from the training datasets used by modern ML systems. For example, ML models are often trained using a large number of prior instances of a speciﬁc decision-making task, but for each instance, the training data contains a ﬁxed and limited set of information. This often does not reﬂect the richness of human experience. Humans make their decisions with reference to a lifetime of experiences across a range of domains, and it is diﬃcult to explicitly specify the information they take into account. By contrast, ML models may learn from training data that comprise narrow slices from a vast number of human decision-makers’ decisions, whereas humans typically learn only from their own experiences or from a small handful of other decision-makers.
2.3 Internal Processing
We now describe the distinguishing characteristics of the internal processes used by humans and ML systems.
• Models of the world. As is comprehensively overviewed in Lake et al. (2017), humans rely upon rich mental models and “theories” that encode complex beliefs about causal mechanisms in the world, not just statistical relationships. For example, starting from an early age, humans develop sophisticated systems of beliefs about the physical and social worlds (intuitive physics and intuitive psychology), which strongly guide how they perceive and make decisions in the world. In contrast to modern ML systems, humans’ mental models tend to be compositional and causal. In turn, these strong prior beliefs about the world can enable humans to learn rapidly in comparison to modern ML systems, and to make impressive inferential leaps based on very
5

limited data (e.g., one-shot and few-shot learning) (Gopnik and Wellman, 2012; Lake et al., 2017; Tenenbaum et al., 2011). On the other hand, the model class of the machine decision-maker has a more mathematically tractable form—whether it is a class of parametric or non-parametric models (Friedman, 2017). Though when designing these models (especially neural networks), researchers commonly encode domain knowledge through the model architecture, most machine learning models still suﬀer from distribution shift (Quin˜onero-Candela et al., 2009) and lack of interpretability (Gilpin et al., 2018), and require large sample sizes and long training time.
• Input processing and perception. The ways decision-makers perceive inputs is informed by their models of the world (Gentner and Stevens, 2014; Holstein et al., 2020). Following research in human cognition and ML, we highlight three sources of variation in input perception: (1) diﬀerences in mental/computational capacity, (2) diﬀerences in human versus machine biases, and (3) tendencies towards causal versus statistical perception. For instance, compared with ML systems, humans demonstrate less capacity to perceive small diﬀerences in numerical values (Amitay et al., 2013; Findling and Wyart, 2021). Furthermore, both humans and ML systems can bring in both adaptive and maladaptive biases, based on their experiences and models of the world, which in turn shape the ways they process and perceive new situations (Fitzgerald and Hurst, 2017; Wistrich and Rachlinski, 2017; Kleinberg et al., 2018; Gentner and Stevens, 2014). However, in some cases humans and ML systems may have complementary biases, opening room for each to help mitigate or compensate for the other’s limitations (Holstein et al., 2020; Tan et al., 2018). Finally, research on human cognition demonstrates that humans are predisposed to perceiving causal connections in the world, and drawing causal inferences based on their observations and interactions in the world (Gopnik and Wellman, 2012; Lake et al., 2017). While these abilities can sometimes be understood by analogy to the kinds of statistical learning that most modern ML systems are based upon (Tenenbaum et al., 2011), other aspects of human causal cognition appear to be fundamentally diﬀerent in nature Lake et al. (2017). As with bias, these abilities can be a double-edged sword. In some scenarios, human causal perception may lead to faulty inferences based on limited data. By contrast, ML systems will sometimes have an advantage in drawing more reliable inferences based on statistical patterns in large datasets. In other settings, human causal perception can help to overcome limitations of ML systems. For example, in many instances, human decision-makers have been observed to be better than ML systems at adapting to out-of-distribution instances, through the identiﬁcation and selection of causal features for decision-making (Lake et al., 2017).
• Choosing among models of the world. Given the task deﬁnition, a class of models of the world, and a set of data, when ﬁnding the model that optimizes the objective in the model class, modern machine learning models (e.g., neural networks) are commonly learned using ﬁrst-order methods and may require a huge amount of computational resource due to the size of the models (Bottou, 2010). On the other hand, humans may employ heuristics that can be executed in a relatively short amount of time (Simon, 1979). These simple strategies may have advantages over more complex models when the inherent uncertainty in the task is high. For a more comprehensive review on when and how such heuristics may be more preferable, we refer readers to Kozyreva and Hertwig (2021).
2.4 Output
We now describe the distinguishing characteristics of the outputs generated by humans and ML systems.
6

• Explaining the decision. Humans and ML have diﬀering abilities in communicating the reasoning behind their decisions. There has been extensive research in explainability (XAI) and interpretability for ML (Adadi and Berrada, 2018). Through research in cognitive and social psychology, it has been observed that humans are generally better than ML algorithms at generating coherent explanations that are meaningful to other humans. Furthermore, Miller (2019) argues that XAI research should move away from imprecise, subjective notions of “good” explanations and instead focus on reasons and thought processes that people apply for explanation selection. They ﬁnd that human explanations are contrastive, selected in a biased manner, and most importantly they are social and contextual. On the other hand, humans’ explanations may not have a correspondence to their actual underlying decision processes (Nisbett and Wilson, 1977), whereas with ML models we can always trace the precise computational steps that led to the output prediction (Hu et al., 2019).
• Uncertainty communication. With increasing research in uncertainty quantiﬁcation for machine learning, new methods have been devised for calibrating a ML model’s uncertainty in its prediction (Abdar et al., 2021). Moreover, methods have been developed to decompose the model uncertainty into aleatoric uncertainty and epistemic uncertainty (Hu¨llermeier and Waegeman, 2021), where aleatoric uncertainty signiﬁes the inherent randomness in an application domain and cannot be reduced, and epistemic uncertainty, also known as systematic uncertainty, signiﬁes the uncertainty due to lack of information or knowledge, and can be reduced. However, these uncertainty quantiﬁcation methods may not necessarily be well-calibrated (Abdar et al., 2021), and are an active research direction. Meanwhile, human decision-makers also ﬁnd it diﬃcult to calibrate their uncertainty or their conﬁdence in their decisions (Brenner et al., 2005), and tend to output discrete decisions instead of uncertainty scores. Moreover, diﬀerent people have diﬀerent scales for uncertainty calibration (Zhang and Maloney, 2012).
• Output consistency. We deﬁne a given decision-maker to have a consistent output when they always produce the same output for the same input. Therefore, we consider the inconsistency in decisions that are based on factors independent of the input, we call them extraneous factors. Some examples of extraneous factors are the time of the day, the weather, etc. Research in human behavior and psychology has shown that human judgments show inconsistency (Kahneman et al., 2016). More speciﬁcally, there is a positive likelihood of change in outcome by a given human decision-maker given the exact same problem description at two diﬀerent instances. Withinperson inconsistency in human judgments has been observed across many domains, including medicine (Koran, 1975; Kirwan et al., 1983), clinical psychology (Little, 1961), ﬁnance and management (Kahneman et al., 2016). This form of inconsistency is not exhibited by standard machine learning algorithms.2
• Time eﬃciency. In many settings, ML models in decision-support settings can generate larger volumes of decisions in less time than human decision-makers. In addition to potentially taking more time per decision, humans often have comparatively scarce time for decision-making overall.
2.5 Prior Work through the Lens of Our Taxonomy
As mentioned in the introduction, we believe the empirical and theoretical ﬁndings in existing human-ML collaboration literature can be hard to navigate as a whole, in part due to a wide range of (often implicit) assumptions made in diﬀerent research articles. To elaborate, in this section,
2There exists the special case of randomized models, we consider these outside the scope of our work and, further note that these models can be directly mapped to deterministic models with decision-based thresholds.
7

we attempt to contextualize prior literature on human-ML collaborative decision-making through the lens of our taxonomy. With this exercise we highlight ambiguities in existing literature about sources of human-AI complementarity through the lens of our taxonomy.
Much prior work has studied settings where the AI alone outperforms either the human alone or the human-AI team. These studies have often focused on tasks where there are no reasons to expect upfront that the human and the AI system will have complementary strengths (Bansal et al., 2021a; Holstein and Aleven, 2021; Lurie and Mulligan, 2020). For example, some research has studied human-AI collaboration on tasks where AI systems have clear advantages over humans under our taxonomy. Other studies have examined untrained crowdworkers’ performance on decisionmaking tasks that require strong domain expertise (Fogliato et al., 2021a; Lurie and Mulligan, 2020) (see Section 2.3). In the context of these study designs, there does not seem to be room for complementarity.
To systematically investigate HAI complementarity, Bansal et al. (2021b) artiﬁcially constrained the AI’s accuracy to be comparable to that of human crowdworkers in their study, in order to simulate conditions that can emerge in real deployment settings. Through experiments, the authors found that human accuracy was high on samples the AI got incorrect and vice versa. These disparities in accuracy, coupled with comparable overall accuracy, yielded complementary performance of the HAI team. In terms of our taxonomy, this work considers a setting where there is overall performance complementarity between the human and the AI, but the mechanisms behind this complementary performance are not well understood. In another recent work, Rastogi et al. (2022) attempt to achieve complementary performance between the human and the machine by providing each with access to diﬀerent task-relevant information. They also showed how training the machine on diﬀerent features led to complementarity in the accuracy of the human alone versus the AI alone. Despite this, the overall HAI team accuracy was similar to that of either the human or the AI alone, indicating that complementarity in input processing and perception was not suﬃcient to yield overall performance complementarity in this case.
Human-AI collaboration has been implemented and studied in real-world scenarios as well. For instance, in healthcare, Tschandl et al. (2020) and Patel et al. (2019) demonstrate better HAI team performance in skin cancer recognition and chest radiograph diagnosis respectively. Here, the ML model has access to a larger training dataset than an individual human decision-maker, including more training examples and access to ﬁner-grained pixel-level data. Tschandl et al. (2020) note that the beneﬁts of complementary performance increase, as the human collaborators’ expertise increases. In our taxonomy, this suggests that human decision-makers’ models of the world encode task relevant knowledge that the AI is missing, for example knowledge of causal mechanisms. Similarly, in the context of child maltreatment screening, Cheng et al. (2022) found that human workers were able to mitigate the impacts of racial disparities in recommendations from an ML-based decision support tool by systematically overriding the ML model’s recommendations in certain cases. Through both quantitative analyses of historical decision data and qualitative analyses of ﬁeld data from a child welfare agency, the authors found that workers were able to do so by taking advantage of complementary information to which they had access but the model did not, and by reasoning about causal mechanisms that might underlie their observations, not just the statistical patterns that the ML model took into account.
8

3 Human-ML Complementarity: An Optimization Framework

We begin by exploring an optimization-based framework for formalizing human-ML complementarity for predictive decision-making. As a running example, throughout the section, we consider a treatment prescription setting where the doctor interacts with an ML system to prescribe a treatment, as a reference to explain the notation. In our framework, we have access to two decisionmakers, namely, the human and the machine, denoted by H and M respectively. In our running example, H is the human doctor and M is the ML system. Our setup assumes a decision-making domain, wherein the covariate space is denoted by X , the space of decisions available is denoted by A and the outcome space observed is denoted by O. Thus, any given decision-maker considers an instance of covariates X ∈ X , then makes a decision A ∈ A, to observe an outcome O ∈ O.3 In a treatment prescription setting considered in Hammer et al. (1996), X is the covariate that describes a patient, A is the prescribed treatment (e.g., Zidovudine), and O is the outcome (e.g., T cell count) after drug prescription.
In predictive decision-making settings, both the human and ML make a decision in the same decision space, independently. In such settings, there is a straightforward transformation from the prediction (e.g., the eﬀect of prescribing zidovudine) to a decision (e.g., whether to prescribe the medicine). As we have discussed in Section 2, the covariate spaces of the two decision-makers may be diﬀerent. For example, the patient’s covariates for the doctor and ML system may diﬀer because the doctor also observes the patient’s facial expressions. Thus, we denote XH and XM (subspaces of X ) to be the covariate space for the human and machine. Now, the mechanism for making a decision given the covariates is given by each decision-maker’s policy, denoted by πM : XM → A for the machine, and πH : XH → A for the human. We denote the space these policies lie in by ΠM and ΠH, respectively. For the simplicity of explaining our general framework, for now, we do not separate X as XM and XH and use the common covariate space for the human and machine policy πH, πM : X → A. We note that the usage of the common X is without loss of generality, since one can always concatenate two diﬀerent space together, e.g., having X = XH × XM.
In hybrid decision-making settings, both humans and ML share a common overarching goal—to optimize their performance under some prescribed evaluation function F : Π → R. We denote the joint policy of the human and ML decision-maker by π ∈ Π where π : X → A. At a high-level, the evaluation function F (π) measures the overall quality of policy π. Importantly, it may account for considerations beyond predictive accuracy—e.g., ethical considerations, such as fairness or understandability, or logistics of limited resource. So F should be distinguished from the objective function used to obtain the human or machine’s policy πH and πM. However, they could be the same, e.g., in the treatment prescription setting, the evaluation function for π may be the same as the objective for obtaining πH and πM—the average number of CD4 T cell count under a policy, they may diﬀer as well. For example, the evaluation function F may also account for the cost of obtaining a human doctor’s decision in addition to the cell count outcome. We propose a general optimization framework for human-ML predictive decision-making: given a problem domain, our goal is to combine the two decision-makers policies to ﬁnd a joint policy π : X → A that maximizes the overall quality of the decisions. That is,

π ∈ arg max F (π),

(1)

π∈Π

where the joint policy space Π is a subset of functions that map from X to A. We assume the joint
3To keep the generality of our setup, we do not explicitly specify the domains of X , A, O. In Section 3.3, we provide more speciﬁcations.

9

policy is obtained by combing human and machine policies, ΠH and ΠM, through an aggregation function G : X × A × A → A. Given πH ∈ ΠH, πM ∈ ΠM, the joint policy π ∈ Π is given by

∀x ∈ X , π(x) = G(x, πH(x), πM(x)).

We note that the aggregation function depends on the covariate vector as well, so it may give higher or lower weight to the human and machine decision depending on the input. In settings where an algorithm is used to aggregate the decisions from both decision-makers (Raghu et al., 2019; Mozannar and Sontag, 2020), the aggregation function G commonly have a mathematically tractable form (in contrast to settings where the human decision-makers aggregate both their decisions and the machine’s). For the remaining of the paper, we consider the outcome space to be a subset of the reals O ⊆ R, e.g., CD4 T cell counts, and the aggregation functions to have the following semi-parametric form:

π(x) = G(x, πH(x), πM(x)) = wH(x)πH(x) + wM(x)πM(x),

(2)

for some weighting functions wH, wM : X → [0, 1]. In practice, the aggregation function may only have access to xM. In such cases, we have the joint decision to be π(x) = G(xM, πH(x), πM(xM)) = wH(xM)πH(x) + wM(xM)πM(xM), where the weighting functions wH, wM only take the machine’s covariates xM as inputs. In the following, we aim to establish human-ML complementarity in a simpliﬁed setting, where the aggregation function G and the weighting functions wH, wM have access to x. It is worth noting that G is a ﬂexible semi-parametric model in the sense that we do not specify the parametric form of the weighting function wH and wM. In addition, the weighting functions take values between 0 and 1 and sum up to 1, i.e., ∀x ∈ X , wH(x) + wM(x) = 1 and wH(x), wM(x) ∈ [0, 1]. In other words, the joint decision π(x) is a convex combination of the individual decisions πH(x) and πM(x). This condition ensures that the joint decision lies between the human’s and machine’s decision. For a decision-maker d ∈ {H, M}, one may interpret the weight wd(x) as the amount of contribution decision-maker d’s decision makes: when wd(x) = 0, the joint decision does not follow d’s decision at all on instance x, while wd(x) = 1 indicates that d’s decision is followed entirely.
There are many existing works that consider such an aggregation function (e.g., (Mozannar and Sontag, 2020; Gao et al., 2021)). In Section 3.3, we illustrate how some prior works ﬁt in our framework. Finally, we note that though it is possible to train the aggregation function G (and the machine policy πM), but it is not possible to have the same type of control over the human policy πH. Thus, by optimizing over π ∈ Π in (1), eﬀectively, we are maximizing over the machine policy (in cases we do have control over πM) and the aggregation function to obtain π . That is, depending on whether it is possible to change πM , we rewrite (1) as

max F (π) or max F (π).

(3)

πM,wM,wH

wM,wH

We further denote an optimal aggregation function and its corresponding optimal weighting functions by G , wH and wM, respectively.
To summarize, when instantiating the general optimization framework (1), we need to specify the following three components: (i) the evaluation function F ; (ii) the aggregation function G; and (iii) the machine policy πM. In particular, the evaluation function depends on both the joint policy π and the data generating process for the covariate, the decision and the outcome (X, G(X, πH(X), πM(X)), O). Depending on the application domain, F may be of diﬀerent forms. For example, in a treatment prescription setting where the overarching goal is the expected

10

outcome (i.e., expected CD4 cell count of the patients), the evaluation function is deﬁned to be F (π) := E[O] = EXEA[E[O|A = π(X), X]]. In the rest of the paper, we provide various examples on instantiating the general framework. We perform complementarity analysis of a binary classiﬁcation setting by analyzing the optimal aggregation function (Section 4).

3.1 Deﬁnition and Measures of Complementarity
We now deﬁne complementarity in human-ML combined decision-making using the evaluation F and provide metrics to quantify it. Deﬁnition 1. The joint policy π deﬁned in (2) exhibits complementarity if and only if
F (π) > max{F (πH), F (πM)}.

Deﬁnition 1 provides a binary indicator for the presence of complementarity for a given human-ML joint policy. We further quantify the extent of complementarity by introducing two diﬀerent measures of complementarity: across-instance complementarity and within-instance complementarity. In the application of human-ML combined decision-making, as proposed in our framework in (2), we see that for a given instance x we either have only one of human or ML making the ﬁnal decision (wM(x) = 0 or wH(x) = 0), or both decision-makers contributing to the ﬁnal decision partially (wM(x) = 0 and wH(x) = 0). These two types of combinations represent two modes of complementarity.

In the ﬁrst mode, there is no complementarity within a single instance, instead only the human or ML model decision gets used, however as the human and ML model provide the ﬁnal decision for diﬀerent instances, we call this across-instance complementarity. In the second mode, since both human and ML model contribute to the same instance x, we call this within-instance complementarity. These two metrics help distinguish between diﬀerent instance allocation strategies in human-ML teams described in Roth et al. (2019).

Formally, we propose two metrics to quantify complementarity in a human-ML collaboration setup. The two metrics rely on properties of the optimal aggregation function G and, in particular, on the optimal weighting functions wH, wM as deﬁned earlier in Section 3. The two metrics are deﬁned as follows.
• Across-instance complementarity quantiﬁes the variability of G (x, πH(x), πM(x)) across diﬀerent x ∈ X . More formally, we deﬁne

dacross(wM, wH) = EX (wM(X) − E[wM(X)])2 = EX (wH(X) − E[wH(X)])2 . (4)

This equality follows trivially from the constraint wM + wH = 1. In case of no variability across instances, that is if for d ∈ {H, M}, we have wd(x) is a constant for all x ∈ X , then dacross(wM, wH) = 0. The notion of across-instance complementarity is shown in Mozannar and Sontag (2020); Madras et al. (2018).
• Within-instance complementarity quantiﬁes how on average G (x, πH(x), πM(x)) balance between πH(x) and πM(x) for each x ∈ X . More formally, we deﬁne

dwithin(wH, wM) = EX 1 − (wH(X) − wM(X))2 .

(5)

We note that dwithin(wH, wM) simpliﬁes to 4EX [wH(X)wM(X)]. Importantly, the deﬁnition of within-instance complementarity satisﬁes some key properties as follows. Firstly,

11

dwithin(wH, wM) is maximized at wH = wM = 0.5 and minimized at wH ∈ {0, 1}. Thus, it is maximized when each decision-maker contributes equally and maximally to a problem instance and minimized when there is no contribution from one of the two decision-makers. Further, it increases monotonically as wH(x) and wM(x) get closer to each other in value, that is the two decision-makers’ contributions to the ﬁnal decision get closer. This notion of complementarity is demonstrated in Patel et al. (2019); Tschandl et al. (2020).
Examples. We provide some demonstrative toy examples to explain the two complementarity metrics introduced. Consider X = {x1, x2, x3, x4} where each covariate is equally likely.
1. If wH(x1) = wH(x2) = wH(x3) = wH(x4) = 0, then dwithin(wH, wM) = 0, and dacross(wH, wM) = 0.
2. If wH(x1) = wH(x2) = 0, wH(x3) = wH(x4) = 1, then dwithin(wH, wM) = 0, and dacross(wH, wM) = 0.75.
3. If wH(x1) = wH(x2) = wH(x3) = wH(x4) = 0.3, then dwithin(wH, wM) = 0.84, and dacross(wH, wM) = 0.
We note that in the second example, we have dwithin(wH, wM) = 0 and dacross(wH, wM) > 0, which is opposite to the third example. Both examples demonstrate complementarity in some form. This shows that across-instance complementarity captures aspects of human-ML complementarity, that are not captured by within-instance complementarity and vice versa.
3.2 Manifestations of the Taxonomy
As is discussed in Section 2, human and machine decision-makers possess distinguishing characteristics in their decision-making process. In this section, we illustrate how some of the characteristics can be captured in our framework. Throughout, we consider a supervised learning setting where the decision-makers aim to predict the label Y ∈ Y given the covariate X. That is, the decision-maker’s decision A is their predicted label and their space of decisions available is A = Y. Consequently, the outcome of a decision A is its incurred loss (A, Y ) where Y is the true label of X and is a loss function (e.g., the 0-1 loss.)
Task deﬁnition. First we discuss how to incorporate the distinguishing characteristics between the two decision-makers in our general framework, based on their task deﬁnition.
• Objective. Often, the objective function of the machine decision-maker M is to ﬁnd a policy πM that minimizes some notion of loss, for example, the expected squared loss E[(πM(X) − Y )2]. However, the human decision-maker H may incorporate other desiderata into their objective, and subsequently their choice of policy πH. (Recall again that the machine objective function can be diﬀerent from the general evaluation function F , as will be illustrated in Example 1).
• Misaligned construct of interest. When the data available to the machine has biased proxies of the underlying target label Y , we can represent the training data of the machine M, as {X, YM} where the biased label YM = Y + (X) for (X) that may depend on the covariates.
Input. Now we discuss how to incorporate the distinguishing characteristics between the two decision-makers based on their respective input in our general framework.
12

• Access to diﬀerent information. Access to diﬀerent information can be represented in our framework by restricting the domain of the policies as πM : XM → A and πH : XH → A where XM and XH are subspaces of X and XM = XH.

• Nature of past experiences. In general, it is hard to ﬁnd the optimal policies at the population

level. Often times, we replace the population level quantities by its empirical version that are

deﬁned on samples collected from past experiences. As we will see in Example 2, to learn

the machine policy πM, population risk is replaced with the empirical risk, i.e., given samples

{Xi, Yi}ni=1,

we

have

πM

∈

arg minπM∈ΠM

1 n

n i=1

(πM(Xi), Yi) where

is a loss function.

Internal processing. As mentioned in Section 2.5, the internal processes of human decisionmaking may be hard to specify, and in most cases are not controlled by people other than the decision-makers themselves. Thus, in this section, we mainly focus on the internal process of the machines, and describe how πM may be obtained.
• Models of the world. One may specify the machine’s model class through deﬁning its hypothesis class ΠM. Depending on the task, ΠM may range from parametric models (e.g., linear models, neural networks) to nonparametric models. On the other hand, the human’s models of the world ΠH is much harder for us to specify, both qualitatively and quantitatively.
• Input processing and perception. The two decision-makers may process or perceive the same covariate vector X ∈ X diﬀerently. For instance, consider X = {(A, 3.51), (A, 3.5)}. Now, due to their cognitive capacities, human experts may not be able to distinguish between (A, 3.51) and (A, 3.5). Thus, ∀x, x ∈ X , we have πH(x) = πH(x ).
• Choosing among models of the world. To specify the optimization procedure for the machine, one needs to choose the optimizer to solve the problem of ﬁnding πM. If the human and machine policies aim to optimize the same objective, then one may take into account the fact that humans may use “satisﬁcing” as a strategy and there may be an optimality gap for the human policy. For example, in settings given by Example 2, one may take this into account by adding the constraint that E[ (πM(X), Y )] = E[ (πH(X), Y )] − δ where δ > 0 is a gap between the machine and human policy in terms of their objective values. It is worth noting that though δ is impossible to obtain in real life, it can be treated as a hyper-parameter which may be ﬁne-tuned to obtain the optimal joint policy performance.

Output. Finally, we provide some illustrative ways of incorporating the diﬀerent output characteristics of the human and machine policy into the joint policy class.
• Explaining the decision. While human decision-makers are generally considered eﬀective at oﬀering post-hoc explanations and justiﬁcations for their decisions, diﬀerent families of ML models exhibit varying degrees of human understandability. For example, if we choose ΠM to be the class of all linear functions mapping XM to A, πM will be relatively more human interpretable4 than if ΠM is the class of deep neural nets.
• Uncertainty communication. Depending on the model used for πM, one may use diﬀerent uncertainty quantiﬁcation methods devised for ML. For example, if πM outputs a distribution over labels, the entropy of the distribution is used to quantify the uncertainty. Uncertainty communication plays an important role in decision-making and, hence, integrating uncertainty into our framework is a direction for future research.
4This is assuming that the feature space XM is easy to understand for humans.

13

• Outcome consistency. To model lack of consistency in decision making, for instance in humans, we consider the human decision-maker’s policy to be altered as πH(X) + ε(X) where ε(X) is a zero-mean additive random noise that may potentially depend on the covariates. We note that a special case of this is when the noise is covariate-independent.
• Time and cost eﬃciency. Considerations of time and cost eﬃciency can be captured by the evaluation function F .

3.3 Prior Work as Special Cases of Our Framework
Prior work in hybrid decision-making has considered two general collaborative strategies among others, one wherein the machine policy and the aggregation policy are optimized simultaneously Mozannar and Sontag (2020), and the other wherein the aggregation policy and the machine policy are optimized one after the other (Raghu et al., 2019). In this section, using Example 1 and Example 2, we illustrate that these two diﬀerent strategies ﬁt neatly into our framework (1) . In the binary classiﬁcation setting considered in Mozannar and Sontag (2020), for any given instance X ∈ X , the machine makes a choice to either defer the decision to the human expert or to make the decision itself. In either case the decision is the predicted binary label A = {0, 1}. The outcome for a given instance is determined by both the loss incurred by the decision. Formally, a loss function (·, ·) maps a pair of true label and decision to a non-negative real number (e.g., 0-1, logistic, hinge loss) and, thus for a joint-policy π(X) where X ∈ X and true label Y ∈ Y = {0, 1}, the outcome is given by O = (π(X), Y ). Finally, the cost of deferring a decision on X to the human is given by c(X, Y, π(X)) where c is a function that maps the covariate, the true label and the human’s decision to a real number.
Example 1 (Mozannar and Sontag (2020)). To instantiate our framework , we need to specify the evaluation function F and the policy space Π. In this case, the evaluation function F depends on both the expected outcome and a property of the aggregation function (i.e., whether the ﬁnal decision comes from the human decision-maker):

F (π) := −EX[E[ (π(X), Y ) + c(X, Y, π(X))wH(X)|X]],

(6)

where cwH (X) speciﬁes the cost incurred by deferring the ﬁnal decision to a human expert. We can equivalently write the optimization problem (1) as

min E[ (π(X), Y ) + c(X, Y, π(X))wH(X)]
wM,wH:X →{0,1}, πM∈ΠM
s.t. ∀x ∈ X , π(x) = wH(x)πH(x) + wM(x)πM(x), wH(x) + wM(x) = 1.
As shown in Mozannar and Sontag (2020), the learned joint policy outperforms the individual expert and the machine policy.
In settings such as the one studied by Raghu et al. (2019); Donahue et al. (2022), the machine model πM is learned ﬁrst and the aggregation function is optimized at a second stage. The next example shows how to instantiate our framework in such settings as a bi-level optimization program. We note that Example 2 can be thought of as a special case of Example 1 since one may impose the constraint that ΠM in Example 1 to only consist πM given below.
14

Example 2. Similar to Example 1, the evaluation function is deﬁned to be (6). The machine policy πM is learned to minimize the expected loss, under a model class ΠM. We rewrite (1) as
min E[ (π(X), Y ) + c(X, Y, π(X))wH(X)]
wM,wH:X →{0,1}
s.t. πM ∈ arg min E[ (πM(X), Y )],
πM∈ΠM
∀x ∈ X , π(x) = wH(x)πH(x) + wM(x)πM(x), wH(x) + wM(x) = 1.
Thus, our framework neatly captures two generalized settings in prior human-ML collaboration literature.
4 An Illustration of Complementarity Analysis
In this section, we analyze the optimal aggregation function in a special case of a binary classiﬁcation setting. Through this analysis, we demonstrate the use of our framework and show how the optimal aggregation function can vary depending on the source of complementarity. For all proofs in this section, we refer the readers to Appendix A.
We ﬁrst set up the binary classiﬁcation task where both the human and the machine have a ﬁnite covariate space. Each decision maker d ∈ {H, M}, given an instance X ∈ Xd, makes a decision A ∈ [0, 1] (predicted probability of the covariate being labelled positive), which incurs an outcome (a loss) measured by the squared error O = (A − Y )2 where Y ∈ {0, 1} is the true label of the instance. This implies that the action space is A = [0, 1] and the outcome space is O = R+. In the following complementarity analysis, we choose a commonly used objective in classiﬁcation problems to be the evaluation function—negative of the expected squared loss—F (π) = −E[(π(X) − Y )2]. Recall that the joint policy π : X → A where X = XH × XM. To simplify the exposition, for each decision-maker d ∈ {H, M}, we introduce a feature mapping sd : X → Xd that captures how the decision-makers have access to diﬀerent covariates, that is, it maps a vector x in the joint feature space X to the features that are accessible to the decision-maker d. We deﬁne πd : X → A wherein the decision-maker’s decision πd(x) relies only on the features sd(x) they have access to. More formally, we require that πd ∈ Πd, where Πd = {πd : X → A | ∀x, x ∈ X , sd(x) = sd(x ) =⇒ πd(x) = πd(x )}.
In our special case, we deal with settings where human decisions may be inconsistent and machines may have access to biased target labels. To model this, we ﬁrst introduce a notion of best-ﬁtting model and then consider the human and machine policies to be imperfect versions of the best-ﬁtting models. A decision-maker’s best-ﬁtting model is the optimal model under the expected mean squared error constrained on the decision-maker’s feature access. Formally, for a decision-maker d ∈ {H, M}, its best-ﬁtting model πd is deﬁned as πd ∈ arg minπd∈Πd E[(πd(X) − Y )2], where Πd is as deﬁned in the previous paragraph. In the following lemma, we provide a closed-form of πd. Lemma 1. For any decision-maker d ∈ {H, M}, its best-ﬁtting model is given by: ∀x ∈ X , πd(x) = E[Y |sd(X) = sd(x)]. Observe that the diﬀerence between πH and πM occurs due to diﬀerence in their access to features. Now, to account for the inconsistency in human decisions and target label bias in machines, with the help of our taxonomy and Section 3.2, we model the human and machine decision-makers πH, πM, using their best-ﬁtting models as follows:
15

• Inconsistency in human decisions: As discussed in Section 2, human decision-makers can be inconsistent in their decisions, which can be incorporated into our framework by deﬁning their policy to be πH(x) = πH(x) + ε(sH(x)) where ε(sH(x)) for all x ∈ X are independent zero-mean noises with bounded variance σ2(sH(x)). Note that, for simplicity, we assume for x, x ∈ X , the noises ε(sH(x)) and ε(sH(x )) share the same distribution if sH(x) = sH(x ).
• Target label bias for machines: We consider that the machine has access to biased labels and the bias B is independent from the label Y but can be potentially dependent on the covariates X. That is, the machine has access to data (X, Y + B) instead of (X, Y ). Here, the best-ﬁtting model is πM(x) = πM(x)+E[B|sM(X) = sM(x)] (for more details, see Appendix A). To simplify notations, we denote πM(x) = πM(x) + b(sM(x)) where the bias b(sM(x)) ∈ R.
Next, we present the optimal weighting function in this setup.
Proposition 1. Suppose that the evaluation function F and weighting function wH, wM are deﬁned as above. Deﬁne
w(x) = (πH(x) − πM(x) − b(sM(x)))(E[Y |X = x] − πM(x) − b(sM(x))) . (πH(x) − πM(x) − b(sM(x)))2 + σ2(sH(x))
The optimal weighting function wH(x) and wM(x) = 1 − wH(x) are given as follows: for all x ∈ X , • If πH(x) − πM(x) − b(sM(x)) = 0 and σ(sM(x)) = 0, then wH(x) can be any value in [0, 1]. • Otherwise, wH(x) is unique and deﬁned as wH(x) = min{1, max{0, w(x)}}.

Implications. In settings where the human and machine policies are πH and πM respectively, the optimal weight wH(x) has a simpler form: when πH(x) = πM(x), wH(x) can by any value in [0, 1]. when πH(x) = πM(x), we have

E[Y |X = x] − E[Y |sM(X) = sM(x)]

wH(x)

=

E[Y

|sH(X)

=

sH(x)]

−

E[Y

|sM(X)

=

. sM(x)]

(7)

In such a case, we have some direct observations of the optimal weights: we trust the human (or
machine) decision-maker fully, i.e., wH(x) = 1 (or wM(x) = 1) if and only if its decision πH(x) (or πM(x)) is the same as E[Y |X = x]. For example, if a decision-maker has access to all features (sd(x) = x), then we always trust that decision-maker (∀x ∈ X , wd(x) = 1). In settings where the human and machine both have access to a subset of the features, the optimal weight depends on
how “relevant” their features are to the decision. For example, as E[Y |sH(X) = sH(x)] gets closer
to E[Y |X = x], the weight wH(x) gets closer to 1. When the noise variance and the machine bias are nonzero, the optimal weight depends on their values. For example, when wH(x) = w(x) as the variance of the noise gets larger, wH(x) gets closer to 0, suggesting that the human decisions should contribute less to the ﬁnal decision.

Thus, we demonstrate the use of our framework to investigate human-ML complementarity in a speciﬁc binary classiﬁcation setting. Using Proposition 1, we show that complementarity exists, i.e., the optimal joint policy outperforms both the individual policies, in terms of the evaluation function. Speciﬁcally, for some x ∈ X , we see that wH(x) (or wM(x)) lies in (0, 1), thus the optimal joint decision does not rely on one decision-maker alone, and depending on the source of complementarity (e.g., feature access, inconsistency, biased target labels), the optimal weights (the form of complementarity) are diﬀerent. For further analysis, we compute across-instance (4) and within-instance complementarity (5) for the two cases considered in Figure 1. For case (1), we get

16

(a) E[Y |X = x].

(b) wH for case (1).

(c) wH for case (2).

Figure 1: We provide a toy example to illustrate the special case provided in Section 4. In this toy example (a), each grid point represents a distinct covariate value in X ∈ R2. The number on top of
the grid point gives E[Y |X = x]. For instance, E[Y |X = (−1, −1)] = 0.2. All covariate values are
equally probable. We consider two diﬀerent cases in Figures (b) and (c), and for each of these two
cases, we derive the optimal weighting function for human wH according to Proposition 1 and plot that as a greyscale heatmap, wherein wH(x) = 1 (black) indicates the entire weight for covariate x is given to the human. The two cases are as follows: (1) Human has access to X1 while machine has access to X2, and both use their best-ﬁtting model (Lemma 1). (2) Human has access to both X1 and X2, while machine has access to only X2. In addition to the decision-makers using their best-ﬁtting model, we assume human decision-maker is inconsistent, with variance for all covariates σH2 (x) = 0.04, as described in Section 4. Interestingly in case (2), even though the human has the true E[Y |X = x], due to their inconsistency, the optimal weights exhibit complementarity.

dacross(wH, wM) = 0.18 and dwithin(wH, wM) = 0.048, while for case (2) we get dacross(wH, wM) = 0.16 and dwithin(wH, wM) = 0.075. We observe that as we go from case (1) where the human and machine have access to diﬀerent features to case (2) where human has access to all features but is noisy, the within-instance complementarity increases and the across-instance complementarity decreases. The increase in within-instance complementarity may be attributed to the inconsistency in human decisions, since while human has all features their decisions are noisy and they will beneﬁt from within-instance collaboration. Thus, by measuring the within-instance and across-instance variability of the optimal weights assigned to the humans or ML, we quantify the extent of complementarity for a static human-ML predictive decision-making setting.
5 Discussion and Future Work
Our work oﬀered an initial yet critical contribution toward a more comprehensive knowledge of complementarity in human-ML decision-making. Our contribution aimed to bring much-needed organization to the growing line of work on human-ML collaborative decision-making. Towards this broader goal, we deliberately scoped our inquiry to “combining predictive decisions in static environments”. Within this scope, we presented a taxonomy characterizing diﬀerences between human and machine decision-making as well as a general optimization-based framework for optimal aggregation of human and machine decisions. Our proposed framework uniﬁed several existing approaches to combining human-ML decisions. Critically, the analysis of our framework suggested that the mechanism by which human-ML judgments are combined should be informed by the relative advantages and disadvantages they exhibit in the speciﬁc decision-making setting at hand.

17

Our optimization framework can be additionally utilized to generate hypotheses about the optimal aggregation schemes in practical settings. Testing such hypotheses in vivo is of great theoretical and practical interest.
The scoping considered in this work, while rich, is limited. In particular, it does not apply to settings where the human decision-maker makes the ﬁnal call or to cases where predictions do not straightforwardly translate to decisions. Our taxonomy and optimization framework only consider human-ML decision-making characteristics from a computational lens. They do not capture numerous important paradigms of human-ML collaboration, including those that focus on crucial factors beyond those covered by our work (e.g., sequential decisions under resource constraints). These limitations point to critical directions of future research that we hope the research community takes on toward a more comprehensive science of hybrid human-ML systems.
Acknowledgement
This work is supported by a J.P. Morgan AI research fellowship and a Open Philanthropy AI fellowship. The authors gratefully acknowledge members of the FEAT ML reading group at Carnegie Mellon University for their valuable discussions and feedback.
References
Abdar, M., Pourpanah, F., Hussain, S., Rezazadegan, D., Liu, L., Ghavamzadeh, M., Fieguth, P., Cao, X., Khosravi, A., Acharya, U. R., et al. (2021). A review of uncertainty quantiﬁcation in deep learning: Techniques, applications and challenges. Information Fusion.
Adadi, A. and Berrada, M. (2018). Peeking inside the black-box: A survey on explainable artiﬁcial intelligence (xai). IEEE Access, 6:52138–52160.
Akpinar, N.-J., De-Arteaga, M., and Chouldechova, A. (2021). The eﬀect of diﬀerential victim crime reporting on predictive policing systems. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, pages 838–849.
Amitay, S., Guiraud, J. A., Sohoglu, E., Zobay, O., Edmonds, B. A., xuan Zhang, Y., and Moore, D. R. (2013). Human decision making based on variations in internal noise: An eeg study. PLoS ONE, 8.
Angwin, J., Larson, J., Mattu, S., and Kirchner, L. (2016). Machine bias. propublica. See https://www. propublica. org/article/machine-bias-risk-assessments-in-criminal-sentencing.
Bansal, G., Nushi, B., Kamar, E., Horvitz, E., and Weld, D. S. (2021a). Is the most accurate ai the best teammate? optimizing ai for teamwork. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 35, pages 11405–11414.
Bansal, G., Wu, T., Zhou, J., Fok, R., Nushi, B., Kamar, E., Ribeiro, M. T., and Weld, D. (2021b). Does the whole exceed its parts? the eﬀect of ai explanations on complementary team performance. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems, pages 1–16.
Beck, M. R., Scarlata, C., Fortson, L. F., Lintott, C. J., Simmons, B., Galloway, M. A., Willett, K. W., Dickinson, H., Masters, K. L., Marshall, P. J., et al. (2018). Integrating human and machine intelligence in galaxy morphology classiﬁcation tasks. Monthly Notices of the Royal Astronomical Society, 476(4):5516–5534.
18

Bien, N., Rajpurkar, P., Ball, R., Irvin, J., Park, A., Jones, E., Bereket, M., Patel, B., Yeom, K., Shpanskaya, K., Halabi, S., Zucker, E., Fanton, G., Amanatullah, D., Beaulieu, C., Riley, G., Stewart, R., Blankenberg, F., Larson, D., Jones, R., Langlotz, C., Ng, A., and Lungren, M. (2018). Deep-learning-assisted diagnosis for knee magnetic resonance imaging: Development and retrospective validation of mrnet. PLoS Medicine, 15(11). Publisher Copyright: © 2018 Bien et al. http://creativecommons.org/licenses/by/4.0/.
Bordt, S. and von Luxburg, U. (2020). When humans and machines make joint decisions: A Non-Symmetric bandit model. arXiv preprint arXiv:2007.04800.
Bottou, L. (2010). Large-scale machine learning with stochastic gradient descent. In Proceedings of COMPSTAT’2010, pages 177–186. Springer.
Brenner, L., Griﬃn, D., and Koehler, D. (2005). Modeling patterns of probability calibration with random support theory: Diagnosing case-based judgment. Organizational Behavior and Human Decision Processes, 97:64–81.
Brown, A., Chouldechova, A., Putnam-Hornstein, E., Tobin, A., and Vaithianathan, R. (2019). Toward algorithmic accountability in public services: A qualitative study of aﬀected community perspectives on algorithmic decision-making in child welfare services. In CHI Conference on Human Factors in Computing Systems, page 41. ACM.
Burrell, J. (2016). How the machine ‘thinks’: Understanding opacity in machine learning algorithms. Big Data & Society, 3(1):2053951715622512.
Bussmann, N., Giudici, P., Marinelli, D., and Papenbrock, J. (2021). Explainable machine learning in credit risk management. Computational Economics, 57.
Cheng, H.-F., Stapleton, L., Kawakami, A., Sivaraman, V., Cheng, Y., Qing, D., Perer, A., Holstein, K., Wu, S. Z., and Zhu, H. (2022). How child welfare workers reduce racial disparities in algorithmic decisions.
Chouldechova, A. and Roth, A. (2020). A snapshot of the frontiers of fairness in machine learning. Communications of the ACM, 63(5):82–89.
De-Arteaga, M., Fogliato, R., and Chouldechova, A. (2020). A case for humans-in-the-loop: Decisions in the presence of erroneous algorithmic scores. In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems, pages 1–12.
Donahue, K., Chouldechova, A., and Kenthapadi, K. (2022). Human-algorithm collaboration: Achieving complementarity and avoiding unfairness. arXiv preprint arXiv:2202.08821.
Dressel, J. and Farid, H. (2018). The accuracy, fairness, and limits of predicting recidivism. Science advances, 4(1):eaao5580.
Findling, C. and Wyart, V. (2021). Computation noise in human learning and decision-making: origin, impact, function. Current Opinion in Behavioral Sciences, 38:124–132. Computational cognitive neuroscience.
Finlayson, S. G., Chung, H. W., Kohane, I. S., and Beam, A. L. (2018). Adversarial attacks against medical deep learning systems. arXiv preprint arXiv:1804.05296.
Fitzgerald, C. and Hurst, S. (2017). Implicit bias in healthcare professionals: a systematic review. BMC Medical Ethics, 18.
19

Fogliato, R., Chouldechova, A., and G’Sell, M. (2020). Fairness evaluation in presence of biased noisy labels. In International Conference on Artiﬁcial Intelligence and Statistics, pages 2325–2336. PMLR.
Fogliato, R., Chouldechova, A., and Lipton, Z. (2021a). The impact of algorithmic risk assessments on human predictions and its analysis via crowdsourcing studies. Proc. ACM Hum.-Comput. Interact., 5(CSCW2).
Fogliato, R., Xiang, A., Lipton, Z., Nagin, D., and Chouldechova, A. (2021b). On the validity of arrest as a proxy for oﬀense: Race and the likelihood of arrest for violent crimes. In Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society, AIES ’21, page 100–111, New York, NY, USA. Association for Computing Machinery.
Friedman, J. H. (2017). The elements of statistical learning: Data mining, inference, and prediction. springer open.
Gao, R., Saar-Tsechansky, M., De-Arteaga, M., Han, L., Lee, M. K., and Lease, M. (2021). Human-ai collaboration with bandit feedback. arXiv preprint arXiv:2105.10614.
Gentner, D. and Stevens, A. L. (2014). Mental models. Psychology Press.
Gilpin, L. H., Bau, D., Yuan, B. Z., Bajwa, A., Specter, M., and Kagal, L. (2018). Explaining explanations: An overview of interpretability of machine learning. In 2018 IEEE 5th International Conference on data science and advanced analytics (DSAA), pages 80–89. IEEE.
Gopnik, A. and Wellman, H. M. (2012). Reconstructing constructivism: Causal models, bayesian learning mechanisms, and the theory theory. Psychological bulletin, 138(6):1085.
Green, B. and Chen, Y. (2019). The principles and limits of algorithm-in-the-loop decision making. Proceedings of the ACM on Human-Computer Interaction, 3(CSCW):1–24.
Gulshan, V., Peng, L., Coram, M., Stumpe, M. C., Wu, D., Narayanaswamy, A., Venugopalan, S., Widner, K., Madams, T., Cuadros, J., Kim, R., Raman, R., Nelson, P. C., Mega, J. L., and Webster, D. R. (2016). Development and Validation of a Deep Learning Algorithm for Detection of Diabetic Retinopathy in Retinal Fundus Photographs. JAMA, 316(22):2402–2410.
Hammer, S. M., Katzenstein, D. A., Hughes, M. D., Gundacker, H., Schooley, R. T., Haubrich, R. H., Henry, W. K., Lederman, M. M., Phair, J. P., Niu, M., et al. (1996). A trial comparing nucleoside monotherapy with combination therapy in hiv-infected adults with cd4 cell counts from 200 to 500 per cubic millimeter. New England Journal of Medicine, 335(15):1081–1090.
Hoﬀman, M., Kahn, L. B., and Li, D. (2017). Discretion in hiring. The Quarterly Journal of Economics, 133(2):765–800.
Holstein, K. and Aleven, V. (2021). Designing for human-ai complementarity in k-12 education. arXiv preprint arXiv:2104.01266.
Holstein, K., Aleven, V., and Rummel, N. (2020). A conceptual framework for human–ai hybrid adaptivity in education. In Artiﬁcial Intelligence in Education, pages 240–254, Cham. Springer International Publishing.
Hu, X., Rudin, C., and Seltzer, M. (2019). Optimal sparse decision trees. Advances in Neural Information Processing Systems (NeurIPS).
20

Hu¨llermeier, E. and Waegeman, W. (2021). Aleatoric and epistemic uncertainty in machine learning: An introduction to concepts and methods. Machine Learning, pages 1–50.
Jacobs, A. Z. and Wallach, H. (2021). Measurement and fairness. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, pages 375–385.
Jarrahi, M. H. (2018). Artiﬁcial intelligence and the future of work: Human-ai symbiosis in organizational decision making. Business Horizons, 61.
Kahneman, D., Rosenﬁeld, A. M., Gandhi, L., and Blaser, T. (2016). Noise: How to overcome the high, hidden cost of inconsistent decision making. Harvard business review, 94(10):38–46.
Kamar, E., Hacker, S., and Horvitz, E. (2012). Combining human and machine intelligence in largescale crowdsourcing. In Proceedings of the 11th International Conference on Autonomous Agents and Multiagent Systems - Volume 1, page 467–474. International Foundation for Autonomous Agents and Multiagent Systems.
Kerrigan, G., Smyth, P., and Steyvers, M. (2021). Combining human predictions with model probabilities via confusion matrices and calibration. Advances in Neural Information Processing Systems, 34.
Keswani, V., Lease, M., and Kenthapadi, K. (2021). Towards unbiased and accurate deferral to multiple experts. ACM Conference on Artiﬁcial Intelligence, Ethics, and Society.
Khim, J., Leqi, L., Prasad, A., and Ravikumar, P. (2020). Uniform convergence of rank-weighted learning. In International Conference on Machine Learning, pages 5254–5263. PMLR.
Kirwan, J. R., de Saintonge, D. M. C., Joyce, C. R. B., and Currey, H. L. F. (1983). Clinical judgment in rheumatoid arthritis. i. rheumatologists’ opinions and the development of ’paper patients’. Annals of the Rheumatic Diseases, 42:644 – 647.
Kleinberg, J., Lakkaraju, H., Leskovec, J., Ludwig, J., and Mullainathan, S. (2017). Human decisions and machine predictions. The quarterly journal of economics, 133(1):237–293.
Kleinberg, J., Lakkaraju, H., Leskovec, J., Ludwig, J., and Mullainathan, S. (2018). Human decisions and machine predictions. The quarterly journal of economics, 133(1):237–293.
Koran, L. M. (1975). The reliability of clinical methods, data and judgments. New England Journal of Medicine, 293(14):695–701.
Kozyreva, A. and Hertwig, R. (2021). The interpretation of uncertainty in ecological rationality. Synthese, 198(2):1517–1547.
Kruppa, J., Schwarz, A., Arminger, G., and Ziegler, A. (2013). Consumer credit risk: Individual probability estimates using machine learning. Expert Systems with Applications, 40(13):5125–5131.
Lai, V., Chen, C., Liao, Q. V., Smith-Renner, A., and Tan, C. (2021). Towards a science of human-ai decision making: A survey of empirical studies. arXiv preprint arXiv:2112.11471.
Lake, B. M., Ullman, T. D., Tenenbaum, J. B., and Gershman, S. J. (2017). Building machines that learn and think like people. Behavioral and brain sciences, 40.
Leqi, L., Prasad, A., and Ravikumar, P. (2019). On human-aligned risk minimization. In Advances in Neural Information Processing Systems.
Lipton, Z. C. (2018). The mythos of model interpretability. Queue, 16(3):31–57.
21

Little, K. B. (1961). Conﬁdence and reliability. Educational and Psychological Measurement, 21(1):95–101.
Lurie, E. and Mulligan, D. K. (2020). Crowdworkers are not judges: Rethinking crowdsourced vignette studies as a risk assessment evaluation technique. Proceedings of the Workshop on Fair and Responsible AI at CHI 2020.
Madras, D., Pitassi, T., and Zemel, R. (2018). Predict responsibly: improving fairness and accuracy by learning to defer. In Advances in Neural Information Processing Systems, pages 6147–6157.
Marr, D. and Poggio, T. (1977). From understanding computation to understanding neural circuitry. Neurosciences research program bulletin, 15:470–488.
Miller, T. (2019). Explanation in artiﬁcial intelligence: Insights from the social sciences. Artiﬁcial Intelligence, 267:1–38.
Mitchell, S., Potash, E., Barocas, S., D’Amour, A., and Lum, K. (2018). Prediction-based decisions and fairness: A catalogue of choices, assumptions, and deﬁnitions. arXiv preprint arXiv:1811.07867.
Mozannar, H. and Sontag, D. (2020). Consistent estimators for learning to defer to an expert. In International Conference on Machine Learning, pages 7076–7087. PMLR.
Nisbett, R. E. and Wilson, T. D. (1977). Telling more than we can know: verbal reports on mental processes. Psychological review, 84(3):231.
Obermeyer, Z., Powers, B., Vogeli, C., and Mullainathan, S. (2019). Dissecting racial bias in an algorithm used to manage the health of populations. Science, 366(6464):447–453.
Okati, N., De, A., and Gomez-Rodriguez, M. (2021). Diﬀerentiable learning under triage. arXiv preprint arXiv:2103.08902.
Patel, B., Rosenberg, L., Willcox, G., Baltaxe, D., Lyons, M., Irvin, J., Rajpurkar, P., Amrhein, T., Gupta, R., Halabi, S., Langlotz, C., Lo, E., Mammarappallil, J., Mariano, A., Riley, G., Seekins, J., Shen, L., Zucker, E., and Lungren, M. (2019). Human–machine partnership with artiﬁcial intelligence for chest radiograph diagnosis. NPJ Digital Medicine, 2(1). Publisher Copyright: © 2019, The Author(s).
Quin˜onero-Candela, J., Sugiyama, M., Lawrence, N. D., and Schwaighofer, A. (2009). Dataset shift in machine learning. Mit Press.
Raghavan, M., Barocas, S., Kleinberg, J., and Levy, K. (2020). Mitigating bias in algorithmic hiring: Evaluating claims and practices. In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency, pages 469–481.
Raghu, M., Blumer, K., Corrado, G., Kleinberg, J., Obermeyer, Z., and Mullainathan, S. (2019). The algorithmic automation problem: Prediction, triage, and human eﬀort. arXiv preprint arXiv:1903.12220.
Rajpurkar, P., O’Connell, C., Schechter, A., Asnani, N., Li, J., Kiani, A., Ball, R., Mendelson, M., Maartens, G., Van Hoving, D., Griesel, R., Ng, A., Boyles, T., and Lungren, M. (2020). CheXaid: deep learning assistance for physician diagnosis of tuberculosis using chest x-rays in patients with HIV. NPJ Digital Medicine, 3:115.
22

Rastogi, C., Zhang, Y., Wei, D., Varshney, K. R., Dhurandhar, A., and Tomsett, R. (2022). Deciding fast and slow: The role of cognitive biases in ai-assisted decision-making. ACM CSCW.
Roth, E. M., Sushereba, C., Militello, L. G., Diiulio, J., and Ernst, K. (2019). Function allocation considerations in the era of human autonomy teaming. Journal of Cognitive Engineering and Decision Making, 13(4):199–220.
Russakovsky, O., Li, L.-J., and Fei-Fei, L. (2015). Best of both worlds: Human-machine collaboration for object annotation. In 2015 IEEE Conference on Computer Vision and Pattern Recognition, pages 2121–2131.
Saxena, D., Badillo-Urquiola, K., Wisniewski, P. J., and Guha, S. (2020). A human-centered review of algorithms used within the us child welfare system. In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems, pages 1–15.
Shrestha, Y. R., Ben-Menahem, S. M., and Von Krogh, G. (2019). Organizational decision-making structures in the age of artiﬁcial intelligence. California Management Review, 61(4):66–83.
Simon, H. A. (1979). Rational decision making in business organizations. The American economic review, 69(4):493–513.
Steyvers, M., Tejeda, H., Kerrigan, G., and Smyth, P. (2022). Bayesian modeling of human&#x2013;ai complementarity. Proceedings of the National Academy of Sciences, 119(11):e2111547119.
Tan, S., Adebayo, J., Inkpen, K., and Kamar, E. (2018). Investigating human+ machine complementarity for recidivism predictions. arXiv preprint arXiv:1808.09123.
Tenenbaum, J. B., Kemp, C., Griﬃths, T. L., and Goodman, N. D. (2011). How to grow a mind: Statistics, structure, and abstraction. science, 331(6022):1279–1285.
Tr¨onnberg, C.-C. and Hemlin, S. (2014). Lending decision making in banks: A critical incident study of loan oﬃcers. European Management Journal, 32(2):362–372.
Tschandl, P., Codella, N., Halpern, A., Puig, S., Apalla, Z., Rinner, C., Soyer, P., Rosendahl, C., Malvehy, J., Zalaudek, I., Argenziano, G., Longo, C., and Kittler, H. (2020). Human–computer collaboration for skin cancer recognition. Nature Medicine, 26.
Wilder, B., Horvitz, E., and Kamar, E. (2020). Learning to complement humans. arXiv preprint arXiv:2005.00582.
Wistrich, A. J. and Rachlinski, J. J. (2017). Implicit bias in judicial decision making how it aﬀects judgment and what judges can do about it. Chapter 5: American Bar Association, Enhancing Justice.
Zhang, H. and Maloney, L. (2012). Ubiquitous log odds: A common representation of probability and frequency distortion in perception, action, and cognition. Frontiers in Neuroscience, 6:1.
A Proofs
Proof of Lemma 1. Recall that for d ∈ {H, M}, the best-ﬁtting model is deﬁned to be
πd ∈ arg min E[(πd(X) − Y )2].
πd∈Πd
23

We denote Xd = {sd(x)|x ∈ X }, the set of unique covariates accessible by d. For a given covariate X, we use Xd to denote the features that are accessible by d, i.e., Xd = sd(X) and X¬d to denote the features that are not accessible by d. For example, X = (XH, X¬H). Rewriting the objective function, we have that

min E[(πd(X) − Y )2] = min P (X = x)EY [(πd(X) − Y )2|X = x]

πd∈Πd

πd∈Πd x∈X

= min
πd∈Πd

P (Xd = xd)EX¬d,Y [(πd(Xd) − Y )2|Xd = xd]

xd∈Xd

=

P (Xd = xd) min EX [(πd(Xd) − Y )2|Xd = xd],

πd(Xd)∈[0,1] ¬d,Y

xd∈Xd

where the last line is true due to the deﬁnition of Πd, i.e., the decision for x and x is the same if sd(x) = sd(x ) = xd. The proof completes by realizing that
EX¬d,Y [(πd(Xd) − Y )2|Xd = xd] =EX¬d,Y [(πd(Xd) − E[Y |Xd = xd])2|Xd = xd] + EX¬d,Y [(E[Y |Xd = xd] − Y )2|Xd = xd].

Thus, we have that πd(x) = πd(xd) = E[Y |Xd = xd] = E[Y |sd(X) = sd(x)], which completes the proof.

Target label bias for machines: In our discussion, we have claimed that when having access
to data (X, Y + B) (where B is independent of Y but dependent on X) instead of (X, Y ), the
machine’s best-ﬁtting model is πM(x) = πM(x) + E[B|sM(X) = sM(x)]. This is true because by replacing the label Y by Y + B in Lemma 1, we have its best-ﬁtting model to be

E[Y + B|sM(X) = sM(x)] = πM(x) + E[B|sM(X) = sM(x)].

Proof of Proposition 1. Recall that for all x ∈ X , we have that wH(x), wM(x) ∈ [0, 1] and wH(x) + wM(x) = 1. Thus, we can rewrite our objective (1) to

min E[(wH(X)πH(X) + (1 − wH(X))πM(X) − Y )2] = min E[(wH(X)(πH(X) − πM(X)) + πM(X) − Y )2]

wH

wH

= P(X = x) min EY,ε|X[(wH(X)(πH(X) − πM(X)) + πM(X) − Y )2|X = x].

x∈X

wH(x)∈[0,1]

Thus, our goal can be reduced to

min EY,ε|X[(wH(X)(πH(X) − πM(X)) + πM(X) − Y )2|X = x]
wH(x)∈[0,1]
= min EY,ε|X[(wH(X)(πH(X) + ε(sH(X)) − πM(X) − b(sM(X))) + πM(X) + b(sM(X)) − Y )2|X = x]
wH(x)∈[0,1]
= min wH2 (x)(πH(x) − πM(x) − b(sM(x)))2 + wH2 (x)σ2(sH(x)) + EY [(πM(x) + b(sM(x)) − Y )2]
wH(x)∈[0,1]
+ 2 (wH(x)(πH(x) − πM(x) − b(sM(x)))) (πM(x) + b(sM(x)) − E[Y |X = x]).

First, we note that the above objective is convex in wH(x), suggesting that it is suﬃcient to use the ﬁrst-order condition to ﬁnd a global optimum. Observe that when (πH(x) − πM(x) − b(sM(x)))2 = 0 and σ2(sH(x)) = 0, the optimal weighting function can be any value in [0, 1]. When (πH(x) −

24

πM(x) − b(sM(x)))2 + σ2(sH(x)) = 0, the objective function is quadratic in wH(x) and the optimum is unique. We obtain the following ﬁrst-order condition:
wH(x) (πH(x) − πM(x) − b(sM(x)))2 + σ2(sH(x)) =(πH(x) − πM(x) − b(sM(x)))(E[Y |X = x] − πM(x) − b(sM(x))). We denote w(x) = (πH(x) − πM(x) − b(sM(x)))(E[Y |X = x] − πM(x) − b(sM(x))) .
(πH(x) − πM(x) − b(sM(x)))2 + σ2(sH(x)) Given the geometry of our objective function, we have that the optimal weighting function wH to be: for all x ∈ X ,
• If w(x) ∈ [0, 1], then wH(x) = w(x). • If w(x) < 0, wH(x) = 0. • If w(x) > 1, wH(x) = 1.
25

