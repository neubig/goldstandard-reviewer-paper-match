Improving Neural Model Performance through Natural Language Feedback on Their Explanations
Aman Madaan ∗ , Niket Tandon ∗† , Dheeraj Rajagopal ∗ , Yiming Yang, Peter Clark†, Keisuke Sakaguchi†, Eduard Hovy
Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA, USA † Allen Institute for Artiﬁcial Intelligence, Seattle, WA, USA {dheeraj,amadaan,yiming,hovy}@cs.cmu.edu {nikett, peterc,keisukes}@allenai.org

arXiv:2104.08765v1 [cs.CL] 18 Apr 2021

Abstract
A class of explainable NLP models for reasoning tasks support their decisions by generating free-form or structured explanations, but what happens when these supporting structures contain errors? Our goal is to allow users to interactively correct explanation structures through natural language feedback. We introduce MERCURIE- an interactive system that reﬁnes its explanations for a given reasoning task by getting human feedback in natural language. Our approach generates graphs that have 40% fewer inconsistencies as compared with the off-the-shelf system. Further, simply appending the corrected explanation structures to the output leads to a gain of 1.2 points on accuracy on defeasible reasoning across all three domains.1
1 Introduction
Interactive Machine Learning allows humans to give feedback to the models, often leading to improved accuracy (Fails and Olsen, 2003; Raghavan, 2006; Settles, 2011). Interactive systems for NLP have used human-in-the-loop style interactions for helping refugee settlement (Brown and Grinter, 2016), aligning topic models (Yuan et al., 2018) and enhancing bilingual word embeddings (Yuan et al., 2020). Neural models have made advancements in explanation generation but are expensive to retrain. This paper aims to improve the model output through natural language feedback (e.g., on its explanation) without retraining.
One line of prior approaches (interactive semantic parsing approach) (Elgohary et al., 2021; Wang et al., 2016) parse natural language user feedback into a set of edit operations, which can then be
∗ authors contributed equally to this work. Ordering determined by dice rolling.
1We release a dataset of over 450k graphs for defeasible reasoning generated by our system at https://tinyurl. com/mercurie.

executed on the incorrect explanation structure, thereby correcting the explanation. In these approaches, the feedback is speciﬁc to a semantic parsing schema and has to be specialized, i.e., directly mapped to speciﬁc instructions or literals, limiting its generalizability. Moreover, the feedback is expected to be actionable, containing a speciﬁc set of edit operations expressed in natural language. However, real-world human feedback is often imprecise and not directly actionable. Another line of prior approaches (interactive reasoning approach) (Talmor et al., 2020) explore interactivity by enriching the context of an input sample through human feedback. However, for the human giving the feedback, the model is a black box – so the human does not know what the model’s internal belief is and how it will change based on the feedback.
These two lines of prior approaches inspire this paper – we provide more transparency to the human than the interactive reasoning approach as the model receives feedback on the explanation (similar to the interactive semantic parsing approach). We do this while relaxing the assumptions of the parsing approach – our feedback does not have a task-speciﬁc structure, and it is not assumed to be actionable (similar to the interactive reasoning approach).
We introduce MERCURIE, a pipeline system with two components, a previously trained neural model M and a graph corrector G. It takes as input any previously trained neural model M capable of generating an explanation structure. The second input is a natural language human feedback on the generated explanation structure (for example, that some nodes are inconsistent with the rest of the graph). As output, it produces a better explanation structure.
The contributions of this work are:

Figure 1: Our pipeline: the output generated by M is corrected by G using human feedback.

• We demonstrate a system that shows that an explainable NLP model output can be improved through natural feedback on their explanations. Experiments show that MERCURIE can improve the consistency of explanation structures by up to 40% (§4).
• We also show downstream task (defeasible inference (Rudinger et al., 2020)) improvement for all domains by at least 1.2 points on accuracy (§6).
Algorithm 1: MERCURIE algorithm to correct explanations through human feedback Given: M: x → y˜, {xi}Ni=1
Train G on DG: DG = ∅; for i ← 1, 2, . . . , N do y˜i = M(xi); Ii = feedback(y˜i); yi = human(y˜i); DG = DG ∪ (xi, Ii, y˜i, yi); end Train G on (x, I, y˜) → y ;
Inference: y˜ = M(x); while I : feedback(y˜) = ∅ do y˜ = G(x, I, y˜); end y = y˜;
2 Related work
Interactive Learning: Interactive learning involves a human in the loop, as opposed to learning from datasets collected ofﬂine. Relevant approaches in NLP are wide-ranging from active learning (Raghavan, 2006; Wu et al., 2019) to

training dialogue systems that adapt to user utterances, spanning diverse domains (Holzinger, 2016). There are various modes of interaction (through labels (Raghavan, 2006; Fails and Olsen, 2003), utterance (Radlinski et al., 2019), imitation (Brantley et al., 2020), and language (Elgohary et al., 2020)). Our work uses language as the mode of interaction.
Language-based interactions: Natural language interaction allows for expressive human feedback to correct a model. In language-based interactions, controlled settings (Mehta and Goldwasser, 2019; Wang et al., 2016) give a better handle and are easy to evaluate. However, they do not generalize to real-world settings– human feedback is rich, and it is not desirable to be restricted to a vocabulary. Finally, the model being taught is treated either as (i) a black box (as in machine teaching (Dasgupta et al., 2019), (Talmor et al., 2020)) or (ii) the beliefs of the model are in some form exposed to feedback (as in interactive semantic parsing (Elgohary et al., 2021)). This paper is uniquely positioned because we present the ﬁrst system, which has interaction through language by directly giving feedback on the model’s beliefs (explanation) in a real-world, open domain setting.
Interactive Semantic Parsing: The common theme in prior approaches to this task based on interactive semantic parsing (such as (Elgohary et al., 2021; Wang et al., 2016)) is that user feedback is mapped into structure edit commands, which can then be executed on the incorrect structures to ﬁx it. For example, (Elgohary et al., 2021) presented NL-EDIT to ﬁx SQL queries using human feedback such as: replace course id with program id.. However:
• the feedback are syntactic with a certain taskspeciﬁc formal structure, e.g., NL-EDIT is

known to struggle with natural feedback that does not describe an edit directly (Elgohary et al., 2021). • the feedback is expected to be actionable. Rather than highlighting a problem or error, it is expected to contain a solution to ﬁx the error. This feedback is then parsed using semantic parsing techniques into a set of structure edit commands.
Differences w.r.t. Interactive Semantic parsing Unlike NL-EDIT, we do not make assumptions about the structure of the feedback. Moreover, we assume that the feedback would be non-actionable (pointing out some local or global error without providing a solution to ﬁx the error). This should especially hold with the growing complexity of the structure to give feedback because it is simpler for a human to point to the problem rather than enumerate (in natural language) the edits that might be required. Therefore, semantic parsing techniques do not apply to our problem as the feedback is nonactionable (i.e., our feedback only highlights that something is wrong, not how to ﬁx it).
Interactive learning for reasoning tasks Our focus is a reasoning task that accounts for the context and requires commonsense to bridge between the feedback to a possible solution. In this, we are inspired by (Talmor et al., 2020) where the interaction is with a black box system (unlike this paper), and when the model incorrectly answers whether A whale has a belly button, then a user tells the model the explicit rule A mammal has a belly button, the model corrects its answer by combining the feedback with its implicit knowledge, e.g., that A whale is a mammal. Our work extends along this line of research by showing that a model can update a model’s explanation structure in a reasoning task setting.
3 Task and Dataset
We focus on the task of generating graphs for defeasible inference queries. After presenting the task, we describe the graph generator M that generates an inference graph for a defeasible inference query. Subsequently, we will use the feedback described in §4 to train G, a system that ﬁxes the output generated by M.

3.1 Task: Defeasible Inference
Defeasible inference (Rudinger et al., 2020) is a mode of reasoning in which given a premise P, a hypothesis H may be strengthened or weakened in light of new evidence. For example, given a premise ocean causes erosion, the hypothesis rocks become smaller will be strengthened by the situation waves are bigger, and weakened by the situation S no waves. We use PHS to refer to a defeasible query and T to the answer (strengthened or weakened).
This problem has been widely studied in cognitive science by supporting defeasible inference through argumentative frameworks (Pollock, 1987). Humans have found argumentations helpful in defeasible reasoning, and this insight has led to models that simulate argumentations through an inference graph, e.g., Pollock (2009) supplement defeasible queries PHS with an inference graph. An inference graph contains events as nodes and the causal relationship between the nodes as edges. The motivation behind using inference graphs is to provide additional context for each PHS query that might help the humans understand the nature of the effect that an update situation S has on the hypothesis. Being costly to construct by hand, inference graphs have only been studied at a small scale.
In the absence of a large repository of inference graphs for defeasible queries, we propose their automatic generation by learning from WIQA (Tandon et al., 2019) - a repository of graphs that are similar to an inference graph ( Section 3.1.2). The main challenge in learning from these graphs is that they are narrowly focused on the procedural text domain. In contrast, defeasible inference task has a wide scope– thus requiring the transfer technique that we present in 3.2. Given the central role that the WIQA dataset plays in our work, we provide a brief description next.
3.1.1 WIQA
WIQA comprises of 2107 pairs of (P, G) where P is a paragraph that describes a process (e.g., the spread of a virus). The inﬂuence graph G corresponding to P is a directed acyclic graph (DAG) that captures the interactions between the events and their inﬂuences within the context of the process described by P . Let G = (V, E), where V denotes the set of vertices and E the set of edges. The nodes n ∈ V are events relevant to the pro-

cess. Each node n is described by a sequence of text tokens. The edge set E contains two types of edges: helps and hurts, denoted by green and red arrows respectively. A helps edge between a source node nc and a target node ne signiﬁes that the source event nc positively inﬂuences the target event ne and a hurts edge stands for nc negatively inﬂuencing ne. Figure 2 shows an example inﬂuence graph for the process of “spread of a virus during a pandemic.”
Figure 2: A sample inﬂuence graph about spread of a virus during a pandemic
3.1.2 WIQA as a repository of inference graphs
We show that the nodes of an inﬂuence graph in WIQA are similar to the inference graph for defeasible reasoning proposed in (Pollock, 2009), by showing a semantic mapping between the components of a defeasible query and an inﬂuence graph.
• The premise of a defeasible query P and the passage in WIQA both play a similar role of providing more context for the inﬂuence graph.
• Each WIQA graph has two hypothesis nodes, which capture either the strengthening or weakening of a hypothesis. Thus, there is a natural correspondence between the hypothesis nodes in WIQA and the hypothesis in defeasible.
• Each inﬂuence graph consists of a node S, which contains an event grounded in P that signiﬁes a change. This is similar to the update S in the defeasible query.

3.2 Designing M for Defeasible Reasoning
Given these similarities, we train a graph-generator on WIQA and transfer it for defeasible reasoning. Our goal is to supplement each defeasible query PHS with an inference graph. We ﬁrst train a graph generator M using WIQA. As discussed, each example in WIQA consists of a (P, G) pair, where P is the passage, and G is the inﬂuence graph. We extract the hypothesis node H and the situation node S from G (using the last two nodes in Figure 2). We then train a sequence-to-sequence generation model (based on T5-11B), where the input is the string P H S and the output is the corresponding inﬂuence graph G encoded as a string. During inference, we obtain a graph for the defeasible query PHS by setting passage = P, hypothesis = H, and situation = S, as discussed. Figure 3 shows the details of the training process.
4 Human feedback on M
In this section, we propose a method to take feedback on the output of M.
4.1 Human feedback
We evaluate the graphs produced by M for defeasible reasoning using human evaluators. Two human judges evaluated 100 graphs produced by M. The judges found that all the graphs had the correct structure, but 70% of them had repeated nodes with the same information.
Each node in an inﬂuence graph plays a speciﬁc role (e.g., positive contextualizer or mediator). Thus, repeated nodes violate the semantic structure of a graph. Additionally, they also reduce the amount of information carried by each graph. For defeasible reasoning, we focus on reducing this repetition of nodes in each graph. We note that we do not utilize the edge structure of the graph for this work or take feedback on it. The structure of the graphs is assumed to be ﬁxed. Our intuition is that reducing the number of repeated nodes will improve the quality of these graphs, making them more useful for downstream tasks. To be consistent across tasks, we refer to such graphs with repeated nodes as being incorrect graphs.
4.2 Automating human-like feedback
We observed that humans found it cognitively challenging to look at multiple nodes and check for the consistency of nodes and repetition of content across multiple unrelated or opposite polarity

Figure 3: Training the graph generator M for defeasible reasoning.

Algorithm 2: Generating training data for

G using human feedback.

Given: Inference graphs G generated by M,

and G* generated by M *

Result: Training data for G

Init: D ← []

for i ← 1, 2, . . . , |M| do

FGi = f eedback(Gi) ; FGi∗ = f eedback(Gi∗); if FGi = ∅ and FGi∗ = ∅ then
/* Gi has problems, Gi∗

is good

*/

D = D ∪ (Gi, FGi, Gi∗);

else if FG = ∅ and FG∗ = ∅ then

/* Both Gi and Gi∗ are

good

*/

D=D∪

(Gi, No issues, looks good, Gi∗);

end

return D

nodes. In contrast, prior work on assembling structure edit commands relies on the relative simplicity of the structure (such as in an SQL query), allowing targeted feedback. This is not possible in our case, owing to the sizeable cognitive load of manually labeling each node while maintaining structural consistency. Therefore, using human annotations, we devised a simple rule-based system F that uses token-based overlap to detect repetitions while preventing spurious matches due to negation. Figures 8, 10, and 9 show examples of various kinds of inconsistencies and the corresponding feedback.
4.3 Automating expected corrected graph
Ideally, it would be desirable to have training data that provides a ﬁxed graph corresponding to each incorrect graph. However, we realized that manually ﬁxing incorrect graphs is not scalable, as it

requires identifying repeated nodes and then coming up with a label that would remove the repetitions across the graph. We circumvent this issue by training another version of graph generator M∗. The training process of M∗ closely follows that of M: we set the input to P H S T and the output to G. Note that the only difference here from M is that the generation is now additionally conditioned on the edges, leading to more diverse and possibly less noisy graphs.
During inference, we obtain a graph for the defeasible query PHS by setting passage = P, hypothesis = H, and situation = S, as discussed. Figure 3 shows the details of the training process.
We further note that such conditioning is not possible for the general case since the graph edges are not available with defeasible queries.
We use T5-11B (Raffel et al., 2020) as our graph generator M, feedback graph generator M∗, as well as graph corrector G.
5 Correcting explanation structure through human feedback
Can we make use of the feedback described §4? We show that we can train a model, G, that takes that feedback and improves M. That is, given PHS, M generates a potentially noisy graph (§3.2) - and G learns to correct this graph using the automatic human-like feedback (§4.2) and compute loss over the expected corrected graph (§4.3). First, we show this graph correction system G, followed by empirically measuring the effectiveness of G.
5.1 Training the graph corrector G
We now proceed to train the graph corrector G using Algorithm 2. The G is also trained as a sequence-to-sequence model. For a given query PHS, the graphs generated by M and M∗ are ﬁrst paired. From these pairs, we only retain those

Figure 4: C-, C+ and S,S- are overlapping.

Figure 5: C-,C+,S,S- and M-, M+, H+ are overlapping.

Figure 6: Incorrect graphs generated by M for SNLI (left) SOCIAL domains of Defeasible. The feedback on each graph is mentioned in caption, and we provide the ﬁxed versions of these graphs in the Appendix.

Figure 7: The graphs generated by M (left), M∗ (middle), and G (right).The input graph has repetitions for nodes {C−, S−}, {C+, H+}, and {M −, M +}. The corrected graph replaces the repetitions with meaningful labels.

cases where the M graph is incorrect, whereas M∗ graph is not, as identiﬁed by our rule-based feedback system F . We record each such example as (G , F (G ), G∗). We also retain pairs where both G and G are correct, and in those cases, the feedback F (G ) is set to no issues, looks good. This is then fed to our generation model, which is trained to generate G∗ from G , F (G ).
Training G completes our pipeline for obtaining high-quality graphs for each defeasible query. First, given a defeasible query PHS, we generate a potentially incorrect graph. G’ using M. We then use the feedback generator F to obtain feedback

F (G ) on G . The tuple (G , F (G )) is then fed to G to obtain a corrected graph G.
6 Results
In this section, we answer two questions: i) Does G reduce the inconsistencies in the graphs? ii) Does using graphs generated by G help the end task?
6.1 Does G reduce the inconsistencies in the graphs?
We evaluate the repetitions in the graphs using two metrics:

• rep. per graph: the average number of repeated nodes in the graphs produced by M and G.
• % with repetitions: the percentage of graphs with at least one repeated node.
As Table 1 shows, G reduces the average repetitions by 40% (2.11 to 1.25) and reduces the fraction of graphs with at least one repetition by 25.7 on average.

ATOMIC SNLI SOCIAL
Average

Metric (repetitions)
per graph % graphs
per graph % graphs
per graph % graphs
per graph % graphs

no feedback (M)
2.05 72 2.09 73 2.2 75 2.11 73.3

w/ feedback (G)
1.26 48 1.18 46 1.32 49 1.25 47.6

Table 1: G reduces the inconsistencies in the graphs. The number of repetitions on average per graph and percentage of graphs with some repetition, both improve.

6.2 Does using graphs generated by G help the end task?
We now evaluate the efﬁcacy of the graphs generated by M and corrected by G on the defeasible inference task. As mentioned in Section 3, the goal of the defeasible inference task is to classify an update S as a strengthener or weakener of a hypothesis H in the context of premise P.
Let M be the graph generated by M for the query PHS. The graph M and the feedback F on M are then supplied to G to obtain G. We overload the notation and use M and G to refer to the nodes of the graphs generated by M and G, respectively. Thus, given each defeasible query PHS, we obtain M : the set of nodes generated M and G: the set of nodes generated by G.
Following Rudinger et al. (2020), we preﬁx a given sequence of tokens T with a special beginning-of-sequence (BOS) token. T is then encoded using RoBERTa-base (Liu et al., 2019)2, and the hidden representation corresponding to BOS is passed to a classiﬁer (single-layer
2We use the implementation by (Wolf et al., 2019)

MLP). We train three classiﬁers, each following the above-described architecture with different inputs: (i) Baseline: T = P H S, (ii) M: T = P H M S, and (iii) G: T = P H G S. We report the results in Table 2, and observe that: (i) Despite the relative simplicity of our approach (concatenating nodes with the query), both M (concatenates noisy graph) and G (concatenates cleaner graph) improve over the baseline. This shows that these explanation structures help enrich the context in the defeasible reasoning task. (ii) G outperforms both the baseline and M, showing that reducing the inconsistencies and repetitions improves end task performance.

ATOMIC SNLI SOCIAL
average

Baseline
78.3 81.6 86.2
82.03

M
78.8 82.1 86.7
82.53

G
79.5 83.1 87.2
83.26*

Table 2: Results on Defeasible inference without using graphs (Baseline (Rudinger et al., 2020)), using graphs generated by M, and graphs corrected with feedback by G. * indicates statistical signiﬁcance

7 Discussion and Conclusion
We present MERCURIE, a system that improves the explanation structure (graphs) generated by a model without requiring expensive humanannotated feedback. Our approach generates graphs that have 40% fewer inconsistencies as compared with the off-the-shelf system. Further, simply appending the corrected explanation structures to the output leads to a gain of 1.2 points on accuracy on defeasible reasoning across all three domains.
This work paves a new path towards exciting future research direction of constantly improving explainable NLP models by applying human feedback.
Acknowledgments
This material is partly based on research sponsored in part by the Air Force Research Laboratory under agreement number FA8750-19-2-0200. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily

representing the ofﬁcial policies or endorsements, either expressed or implied, of the Air Force Research Laboratory or the U.S. Government. We would like to thank Google for providing the TPU machines for conducting experiments.
References
Kiante´ Brantley, Amr Sharaf, and Hal Daum’e. 2020. Active imitation learning with noisy guidance. In ACL.
D. Brown and R. Grinter. 2016. Designing for transient use: A human-in-the-loop translation platform for refugees. Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems.
S. Dasgupta, Daniel J. Hsu, Stefanos Poulis, and Xiaojin Zhu. 2019. Teaching a black-box learner. In ICML.
Ahmed Elgohary, Ahmed Hassan Awadallah, et al. 2020. Speak to your parser: Interactive text-to-sql with natural language feedback. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2065–2077.
Ahmed Elgohary, Christopher Meek, Matthew Richardson, Adam Fourney, Gonzalo Ramos, and Ahmed Hassan Awadallah. 2021. Nl-edit: Correcting semantic parse errors through natural language interaction. arXiv preprint arXiv:2103.14540.
Jerry Alan Fails and D. Olsen. 2003. Interactive machine learning. In IUI ’03.
Andreas Holzinger. 2016. Interactive machine learning for health informatics: when do we need the humanin-the-loop? Brain Informatics, 3:119 – 131.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692.
Ilya Loshchilov and Frank Hutter. 2017. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101.
Nikhil Mehta and Dan Goldwasser. 2019. Improving natural language interaction with robots using advice. In NAACL-HLT.
J. Pollock. 1987. Defeasible reasoning. Cogn. Sci., 11:481–518.
J. Pollock. 2009. A recursive semantics for defeasible reasoning. In Argumentation in Artiﬁcial Intelligence.
Filip Radlinski, K. Balog, B. Byrne, and K. Krishnamoorthi. 2019. Coached conversational preference elicitation: A case study in understanding movie preferences. In SIGdial.

Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of transfer learning with a uniﬁed text-to-text transformer. Journal of Machine Learning Research, 21:1–67.
H. Raghavan. 2006. Active learning with feedback on both features and instances. In JMLR ’06.
Rachel Rudinger, Vered Shwartz, Jena D. Hwang, Chandra Bhagavatula, Maxwell Forbes, Ronan Le Bras, Noah A. Smith, and Yejin Choi. 2020. Thinking like a skeptic: Defeasible inference in natural language. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4661–4675, Online. Association for Computational Linguistics.
Burr Settles. 2011. Closing the loop: Fast, interactive semi-supervised annotation with queries on features and instances. In EMNLP.
Alon Talmor, Oyvind Tafjord, P. Clark, Y. Goldberg, and Jonathan Berant. 2020. Teaching pre-trained models to systematically reason over implicit knowledge. NeurIPS.
Niket Tandon, Bhavana Dalvi, Keisuke Sakaguchi, Peter Clark, and Antoine Bosselut. 2019. Wiqa: A dataset for “what if...” reasoning over procedural text. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 6078–6087.
Sida I. Wang, Percy Liang, and Christopher D. Manning. 2016. Learning language games through interaction. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2368–2378, Berlin, Germany. Association for Computational Linguistics.
Thomas Wolf, L Debut, V Sanh, J Chaumond, C Delangue, A Moi, P Cistac, T Rault, R Louf, M Funtowicz, et al. 2019. Huggingface’s transformers: State-of-the-art natural language processing. ArXiv, abs/1910.03771.
Yuexin Wu, Yichong Xu, Aarti Singh, Yiming Yang, and Artur Dubrawski. 2019. Active learning for graph neural networks via node feature propagation. arXiv preprint arXiv:1910.07567.
Michelle Yuan, Benjamin Van Durme, and Jordan L. Ying. 2018. Multilingual anchoring: Interactive topic modeling and alignment across languages. In NeurIPS.
Michelle Yuan, Mozhi Zhang, Benjamin Van Durme, Leah Findlater, and Jordan L. Boyd-Graber. 2020. Interactive reﬁnement of cross-lingual word embeddings. In EMNLP.

A Appendix
A.1 Examples of errors in the explanation structures generated by M
• Figure 8 shows an example of incorrect graph generated for Defeasible SNLI data.
• Figure 9 shows an example of incorrect graph generated for Defeasible Social data.
• Figure 10 shows an example of incorrect graph generated for Defeasible ATOMIC data.
A.2 Reproducibility A.2.1 M, M∗, G T5-11B models has 11B parameters with 24-layers, 1024-hidden-state, 65,536 feed-forward hiddenstate, 128 attention heads. We use TPU (v3-8) on Google cloud platform. It takes 3 hours in average to train M and M∗, and 4 hours to train G.
A.2.2 Classiﬁer for defeasible tasks We build on the implementation by Wolf et al. (2019), using the default hyperparameters. For optimization, we use AdamW (Loshchilov and Hutter, 2017) with a learning rate of 2e-5, a batch size of 16, and a linear rate scheduler with warm up for the ﬁrst 3 (10%) of the epochs. We use accumulate gradients for two batches, and clip gradients at 1. We also experimented with a block size of 300 and a batch size of 2. All of our experiments were done on a single Nvidia GeForce RTX 2080 Ti.

Figure 8: Incorrect graph generated by M (left) and ﬁxed by G (right) for Defeasible-SNLI dataset. The feedback is ‘C-, C+ are overlapping, and S, S- are overlapping.’
Figure 9: Incorrect graph generated by M (left) and ﬁxed by G (right) for Defeasible-SOCIAL dataset. The feedback is ‘C-, C+,S,S- are overlapping, and M-, M+, H+ are overlapping.’

Figure 10: Incorrect graph generated by M (left) and ﬁxed by G (right) for Defeasible-ATOMIC dataset. The feedback is ‘S-, M+ are overlapping.;

