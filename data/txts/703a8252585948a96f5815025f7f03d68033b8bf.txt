Two Approaches to Building Collaborative, Task-Oriented Dialog Agents through Self-Play
Arkady Arkhangorodsky, Scot Fang, Victoria Knight, Ajay Nagesh, Maria Ryskina, Kevin Knight DiDi Labs 4640 Admiralty Way Marina del Rey, CA 90292

arXiv:2109.09597v1 [cs.CL] 20 Sep 2021

Abstract
Task-oriented dialog systems are often trained on human/human dialogs, such as collected from Wizard-of-Oz interfaces. However, human/human corpora are frequently too small for supervised training to be effective. This paper investigates two approaches to training agent-bots and user-bots through self-play, in which they autonomously explore an API environment, discovering communication strategies that enable them to solve the task. We give empirical results for both reinforcement learning and game-theoretic equilibrium ﬁnding.
1 Introduction
In this paper, we study dialog self-play, in which an agent-bot and user-bot develop communication and action strategies to solve task-oriented dialog problems.
We adopt a highly-simpliﬁed version of the tripbooking dialog domain of Arkhangorodsky et al. (2020), where the user (passenger) talks about their desired destination (“I want to go to Starbucks on Venice Boulevard”), and the agent (autonomous vehicle or personal assistant) uses Google Map APIs and utterances to propose and conﬁrm destinations. At the end of the dialog, we check whether the booked destination is correct, from the user’s point of view. A sample human/human dialog from this domain is shown in Figure 1. Only the verbal actions are shown. Not shown are the Google Map API calls that the agent uses, or what those calls return. The example highlights several challenging features of this task domain: millions of places, ambiguity of place names, calculation of distances, need for conﬁrmation, etc.
Given a human/human corpus, we can use machine learning to build an automatic agent that imitates a human agent. However, hundreds or thousands of human/human dialogs are often insufﬁ-

User: Agent: User: Agent:
User: Agent:

I want to go to Starbucks on Venice Boulevard. There is a Starbucks in Mar Vista. Are you okay with that one? Is it the one across from Coffee Connection? Starbucks is 141 feet away from Coffee Connection. It will take us 10 minutes to get there. Shall we go? Great, thanks. Great, we are going to Starbucks.

Figure 1: Sample human/human dialog in the tripbooking domain (Arkhangorodsky et al., 2020). A user (passenger) works with an agent (autonomous vehicle or personal assistant) to book a trip destination using a speech interface.

cient to train high-quality agents. This kind of training gap is commonly addressed with a user simulator (Schatzmann et al., 2007; Asri et al., 2016; Crook and Marin, 2017; Kreyssig et al., 2018; Gur et al., 2018; Shi et al., 2019). An automatic agent can interact with a user simulator to effectively generate many more positive examples of tasks completed. Reinforcement learning (RL) is commonly used (Sutton et al., 1998).
Indeed, in domains we are interested in, we can hand-build a user simulator. But in that case, we ﬁnd that we might as well instead just hand-build an agent simulator, and dispense with training. We can also train a user simulator on human/human dialogs, but we again run into data sparsity.
In this paper, we address data sparsity and limit handcrafting by investigating self-play, in which an agent-bot and a user-bot learn strategies from scratch. We supply randomly-generated destinations to the user-bot and reward both bots if (after the dialog) the agent-bot drives the user-bot to the correct place. We provide the user-bot with a pushbutton environment similar to the agent side.
In self-play for task-oriented dialog, we face three main challenges:
• Designing a learning scheme that allows the bots to obtain full rewards, by autonomously

solving the tasks given to them.
• Preventing the two bots from developing a secret language. After they are trained, we want the bots to be able to interact correctly with normal human users.
• Preventing the two bots from developing a single, narrow solution. The learned bots should be able to react correctly to a wide range of dialog situations.
2 Related Work and Contributions
Inspirational examples of self-play include competitive games like backgammon (Tesauro, 1995), Go (Silver et al., 2017), and poker (Brown and Sandholm, 2018), and collaborative games like Hanabi (Bard et al., 2020).
In collaborative, task-oriented dialog, self-play has been used primarily to create synthetic data (Shah et al., 2018b,a; Majumdar et al., 2019). In addition, it has been used to train agents in particular types of structured dialogs, such as recommendation (Kang et al., 2019) and negotiation (Lewis et al., 2017; Jang et al., 2020). Further, Liu and Lane (2018) use adversarial learning to supplement sparse reward signals. The closest work to ours is Section 6.2 of Wei et al. (2018), which brieﬂy describes a self-play formulation in an air-travel domain with a vast human/human dialog corpus.
The contributions of the present paper are:
• We implement fully-autonomous self-play in a collaborative, task-oriented dialog domain, addressing the challenges above.
• We compare two methods, one based on reinforcement learning methods, and one based on game-theoretic equilibrium-ﬁnding.
We do not address the entire trip-booking domain in this paper. Rather, we use a highlysimpliﬁed form of the problem to closely study the behavior of learning algorithms.
3 Simpliﬁed Trip-Booking
Our domain is characterized as follows.
• Desired destination: Randomly selected as either Starbucks or Peet’s.
• The user-bot (passenger) has two verbal actions: Say-Starbucks and Say-Peet’s.

• The agent-bot (driver) has two API calls: Drive-Starbucks and Drive-Peet’s. In this domain, the agent-bot does not talk.
• Reward: Both bots get -1.0 if agent-bot drives to the wrong destination, +1.1 if the agent-bot drives to the right destination (and the user-bot mentioned that destination), +1.0 if the agentbot drives to the right destination (despite the user-bot saying the wrong destination).
• Interaction: The user-bot performs a single action, the agent-bot performs a single action, and rewards are then assigned.
Note that the agent-bot can only partially observe the environment. It cannot see the desired destination. It is the user-bot’s job to relay that information to the agent-bot. We supply a stream of desired destinations to the user-bot, and we want the two bots to converge on strategies that yield good rewards.
RL solution. We ﬁrst attack this problem with RL. Initially, the interactions between the bots are nonsensical and low-reward. For example, Destination: Starbucks, User: Say-Starbucks, Agent: Drive-Peet’s, Reward: -1.0.
To improve the bots’ policies, we use the PG (policy gradient) and PPO (promixal policy optimization) algorithms as implemented in rllib (Liang et al., 2017) with default parameters. We run 90 iterations of the training loop with a batch size of 500 actions, resulting in 27,000 dialogs in a random restart. On top of that, we execute 100 restarts for each algorithm.
Figure 2 shows results. We note two things:
• Some proportion of random restarts result in bots that are unable to obtain the full +1.1 reward.
• The simpler PG algorithm achieves +1.1 reward in more restarts than the PPO algorithm.
Bots that achieve a suboptimal +1.0 reward do so by developing a secret language. When the destination is Starbucks, the user says Peet’s, yet the agent drives to Starbucks. We call this “opposite day.” We might expect the agents to switch to strategies that deliver the full reward (+1.1) if we manipulate exploration parameters, but in practice, we ﬁnd this difﬁcult.
Observation. We note that neither dialog bot explicitly models the possible strategies of the

Method
PG (policy gradient) PPO (promixal policy optimization) Game-theoretic equilibrium ﬁnding

restarts w/ +1.1 reward
97/100 84/100
1/1

restarts w/ +1.0 reward
3/100 16/100
0/1

avg time to converge (seconds) 63.4 sec 81.6 sec < 1 sec

Figure 2: Self-play results. For each restart in the RL experiments (ﬁrst two rows), we consider it to achieve a particular reward if at least 2700 of the last 3000 episodes of training obtain that reward. Time to converge for each restart is the ﬁrst time the reward, averaged over the last 300 episodes, exceeds +1.095; or, complete training time if the restart does not achieve the full reward.

Figure 3: Extensive game tree. There is one chance node (black), 2 user decision nodes (red), and 4 agent decision nodes grouped into 2 information sets (blue). The tree has 8 leaf nodes which store rewards.

other. Each bot simply tries to cope, during training, with an environment that appears to respond unpredictably and non-deterministically. In fact, the other bot is unwittingly causing this apparent mayhem as it updates its own policy.
Game theory solution. We use game theory to explicate these strategic interactions. Theoretical frameworks for the use of game theory in dialog include Hasida et al. (1995), Lewin and Lane (2000), and Caelen and Xuereb (2011), while Barlier et al. (2015) carry out experiments in adversarial-type dialogs. Here, we apply game theory to collaborative, task-oriented dialog, where an agent works with a user to solve the user’s problem.
Figure 3 shows an extensive tree for the domain. The ﬁrst node (black) is a chance node that selects the desired destination. At the next level, the user (red) selects a say-action. Finally, the agent (blue) selects a drive-action. Leaf nodes contain the relevant rewards.
The agent has imperfect information, because it cannot see the desired destination. It can only learn about it from the user. Thus, the agent cannot distinguish between certain nodes, and it must apply the same strategy in those cases. We model this with standard information sets (blue shaded ovals).

At the two lowest nodes, the agent only knows that the user said “Starbucks,” so it cannot plan to go left at one node and right at the other.
Like an RL policy, a strategy is a comprehensive, probabilistic map from environment to action. With games of perfect information, optimal strategies for both players can be obtained by simply backing up the rewards through the tree (backward induction or minimax search). With imperfect information, the job requires a more complex algorithm (Avis et al., 2010).
We ﬁrst compute equilibria. An equilibrium is a pair of strategies where neither player can unilaterally improve their reward. There are seven such equilibria, shown in Figure 4, which we compute with the gambit-enummixed method in the open-source Gambit software (McKelvey et al., 2014).
Equilibrium #1 represents the pair of strategies that obtain +1.1 reward (the user truthfully relates the destination, and the agent believes it). Equilibrium #2 represents opposite day, with +1.0 reward. Equilibrium #3 is “always say Starbucks, always drive to Starbucks.” The agent cannot improve things unilaterally, either by driving to Peet’s or ﬂipping a coin. Likewise, the user cannot improve

Equilibrium
#1 #2 #3 #4 #5 #6 #7

User Node 1

Say-S Say-P

1

0

0

1

1

0

0

1

20/21 1/21

0

1

20/41 21/41

User Node 2

Say-S Say-P

0

1

1

0

1

0

0

1

1

0

1/21 20/21

21/41 20/41

Agent Info Set 1

Drive-S Drive-P

1

0

0

1

1

0

0

1

20/21 1/21

0

1

20/41 21/41

Agent Info Set 2

Drive-S Drive-P

0

1

1

0

1

0

0

1

1

0

1/21 20/21

21/41 20/41

Expected Reward
+1.1 +1.0 +0.05 +0.05 +0.0476 +0.0476 +0.0249

Figure 4: Equilibria computed for the game tree in Figure 3, giving strategies for User and Agent. As an example, in Equilibrium #5, the user at Node 1 ﬂips a coin, electing Say-Starbucks with probability 20/21 and Say-Peet’s with probability 1/21. Likewise for Equilibrium #5, an agent in Information Set 2 (meaning “user said Peet’s, but actual destination is unknown”) will always elect Drive-Starbucks.

Figure 2 shows this method is effective, requiring less than one second to compute equilibria.

4 Conclusions and Future Work
We present results on self-play for task-oriented dialog with a highly-simpliﬁed form of trip-booking:

Figure 5: Three visualized equilibria. The x-axis gives mixed strategies for the user (1.0 = always faithfully relate desired destination, 0.0 always lie about destination). The y-axis gives mixed strategies for the agent (1.0 = always obey user, 0.0 always disobey user).
things unilaterally, as long as the agent is always driving to Starbucks.
Equilibrium #7 involves mixed strategies on both sides. The user mostly lies about the destination, and the agent mostly disobeys the user. Perhaps surprisingly, neither agent can improve things unilaterally, even though a small concerted change (by both) produces a higher reward and leads to a non-equilibrium.
Figure 5 illustrates that Equilbrium #7 is a saddle-point. To make a 3-dimensional ﬁgure, we reduce the bots’ options. Here, the x-axis records the user’s probability of telling the truth, which excludes strategies like “always say Starbucks” (see #3) from this plot.
In the game-theoretic framework, “self-play” is less play and more equation solving:
1. Enumerate all equilibria.
2. Choose the highest-reward equilibrium.
3. Record the user’s and agent’s strategies.
4. Apply those strategies on randomly-generated destinations.

• The bots are able to obtain full rewards autonomously, without human-produced training data. We give empirical results for both RL and game-theoretic equilibrium ﬁnding.
• We address secret languages by rewarding the user (passenger) for telling the truth, augmenting the standard reward for task completion.
• The game-theoretic solution not only locates the global maximum reward, it also avoids narrow solutions by computing a pair of complete strategies.
In future work, we would like to scale these approaches to self-play in complex, collaborative, task-oriented domains such as restaurant booking and full-scale trip booking. In such domains, extensive game trees will be quite large and likely require algorithms such as those used to solve complex games like poker (Brown and Sandholm, 2018).
References
Arkady Arkhangorodsky, Amittai Axelrod, Christopher Chu, Scot Fang, Yiqi Huang, Ajay Nagesh, Xing Shi, Boliang Zhang, and Kevin Knight. 2020. MEEP: An open-source platform for human-human dialog collection and end-to-end agent training. ArXiv 1910.03771.
Layla El Asri, Jing He, and Kaheer Suleman. 2016. A sequence-to-sequence model for user simulation in spoken dialogue systems. In Proc. INTERSPEECH.

D. Avis, G. Rosenberg, R. Savani, and B. von Stengel. 2010. Enumeration of Nash equilibria for twoplayer games. Economic Theory, 42(9-37).
Nolan Bard, Jakob N. Foerster, Sarath Chandar, Neil Burch, Marc Lanctot, H. Francis Song, Emilio Parisotto, Vincent Dumoulin, Subhodeep Moitra, Edward Hughes, Iain Dunning, Shibl Mourad, Hugo Larochelle, Marc G. Bellemare, and Michael Bowling. 2020. The Hanabi challenge: A new frontier for AI research. Artiﬁcial Intelligence, 280.
Merwan Barlier, Julien Perolat, Romain Laroche, and Olivier Pietquin. 2015. Human-machine dialogue as a stochastic game. In Proc. SIGDIAL, pages 2–11.
Noam Brown and Tuomas Sandholm. 2018. Superhuman ai for heads-up no-limit poker: Libratus beats top professionals. Science, 359(6374):418–424.
J. Caelen and A. Xuereb. 2011. Dialogue and game theory. In Proc. 6th Conference on Speech Technology and Human-Computer Dialogue (SpeD), pages 1–10.
Paul A. Crook and Alex Marin. 2017. Sequence to sequence modeling for user simulation in dialog systems. In Proc. INTERSPEECH.
Izzeddin Gur, Dilek Z. Hakkani-Tu¨r, Go¨khan Tu¨r, and Pararth Shah. 2018. User modeling for task oriented dialogues. In Proc. SLT.
K. Hasida, K. Nagao, and T. Miyata. 1995. A gametheoretic account of collaboration in communication. In Proc. 1st International Conference on Multiagent Systems (AAAI).
Youngsoo Jang, Jongmin Lee, and Kee-Eung Kim. 2020. Bayes-adaptive Monte-Carlo planning and learning for goal-oriented dialogues. In Proc. AAAI.
Dongyeop Kang, Anusha Balakrishnan, Pararth Shah, Paul Crook, Y-Lan Boureau, and Jason Weston. 2019. Recommendation as a communication game: Self-supervised bot-play for goal-oriented dialogue. In Proc. EMNLP-IJCNLP.
Florian Kreyssig, In˜igo Casanueva, Paweł Budzianowski, and Milica Gasic. 2018. Neural user simulation for corpus-based policy optimisation for spoken dialogue systems. In Proc. SIGDIAL.
Ian Lewin and Mill Lane. 2000. A formal model of conversational game theory. In Proc. 4th Workshop on the Semantics and Pragmatics of Dialogue (Gotalog), volume 69.
Mike Lewis, Denis Yarats, Yann Dauphin, Devi Parikh, and Dhruv Batra. 2017. Deal or no deal? End-to-end learning of negotiation dialogues. In Proc. EMNLP.
Eric Liang, Richard Liaw, Robert Nishihara, Philipp Moritz, Roy Fox, Joseph Gonzalez, Ken Goldberg, and Ion Stoica. 2017. Ray RLlib: A composable and scalable reinforcement learning library. CoRR, abs/1712.09381.

Bing Liu and Ian Lane. 2018. Adversarial learning of task-oriented neural dialog models. In Proc. SIGDIAL.
Sourabh Majumdar, Serra Sinem Tekiroglu, and Marco Guerini. 2019. Generating challenge datasets for task-oriented conversational agents through selfplay. In Proc. RANLP.
R. McKelvey, A. McLennan, and T. Turocy. 2014. Gambit: Software tools for game theory. Version 13.1.2. http://www.gambit-project.org.
Jost Schatzmann, Blaise Thomson, Karl Weilhammer, Hui Ye, and Steve J. Young. 2007. Agenda-based user simulation for bootstrapping a POMDP dialogue system. In Proc. HLT-NAACL.
Pararth Shah, Dilek Hakkani-Tu¨r, Bing Liu, and Gokhan Tu¨r. 2018a. Bootstrapping a neural conversational agent with dialogue self-play, crowdsourcing and on-line reinforcement learning. In Proc. NAACL.
Pararth Shah, Dilek Hakkani-T ur, Gokhan T ur, Abhinav Rastogi, Ankur Bapna, Neha Nayak, and Larry Heck. 2018b. Building a conversational agent overnight with dialogue self-play. ArXiv 1801.04871.
Weiyan Shi, Kun Qian, Xuewei Wang, and Zhou Yu. 2019. How to build user simulators to train RLbased dialog systems. In Proc. EMNLP/IJCNLP.
David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. 2017. Mastering the game of Go without human knowledge. Nature, 550(7676):354–359.
Richard S Sutton, Andrew G Barto, et al. 1998. Introduction to reinforcement learning, volume 135. MIT press Cambridge.
Gerald Tesauro. 1995. Temporal difference learning and TD-Gammon. Communications of the ACM, 38(3).
Wei Wei, Quoc V. Le, Andrew M. Dai, and Jia Li. 2018. AirDialogue: An environment for goal-oriented dialogue research. In Proc. EMNLP.

