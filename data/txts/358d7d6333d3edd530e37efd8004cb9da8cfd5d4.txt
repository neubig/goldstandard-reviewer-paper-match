A Benchmark for Structured Procedural Knowledge Extraction from Cooking Videos
Frank F. Xu1‚àó, Lei Ji2, Botian Shi3, Junyi Du4, Graham Neubig1, Yonatan Bisk1, Nan Duan2 1Carnegie Mellon University 2Microsoft Research Asia
3Beijing Institute of Technology 4University of Southern California {fangzhex,gneubig,ybisk}@cs.cmu.edu, {leiji,nanduan}@microsoft.com

arXiv:2005.00706v2 [cs.CL] 9 Oct 2020

Abstract

Watching instructional videos are often used to learn about procedures. Video captioning is one way of automatically collecting such knowledge. However, it provides only an indirect, overall evaluation of multimodal models with no Ô¨Åner-grained quantitative measure of what they have learned. We propose instead, a benchmark of structured procedural knowledge extracted from cooking videos. This work is complementary to existing tasks, but requires models to produce interpretable structured knowledge in the form of verb-argument tuples. Our manually annotated open-vocabulary resource includes 356 instructional cooking videos and 15,523 video clip/sentence-level annotations. Our analysis shows that the proposed task is challenging and standard modeling approaches like unsupervised segmentation, semantic role labeling, and visual action detection perform poorly when forced to predict every action of a procedure in structured form.

1 Introduction

Instructional videos are a convenient way to learn a new skill. Although learning from video seems natural to humans, it requires identifying and understanding procedures and grounding them to the real world. In this paper, we propose a new task and dataset for extracting procedural knowledge into a Ô¨Åne-grained structured representation from multimodal information contained in a largescale archive of open-vocabulary narrative videos with noisy transcripts. While there is a signiÔ¨Åcant amount of related work (summarized in ¬ß3 & 7), to our knowledge there is no dataset similar in scope, with previous attempts focusing only on a single

‚àó Work done during the Ô¨Årst author‚Äôs in-

ternship at Microsoft Research, Asia. Data and

code:

https://github.com/frankxu2004/

cooking-procedural-extraction.

Video V for Task R: Making Clam Chowder
ID Transcript T
i have my big giant tamali pot that i 'm going to use today for mike lamb, 1 chop suet clam shop is with my daughters used to call clam chowder
when they were little. 2 so clam shop soup, and i got all my ingredients here and then i 'll give
you exact measurements on my site. 3 and i 'm going to start with a cast iron skillet. 4 i 'm heating it up with a medium, medium high flame, and i 'm going to
put some bacon in there and fry it up. 5 and this is not a diet recipe. 6 sorry they making clam chowder. 7 an eye you can see the photo montage before this what i did cool. 8 somehow i fried some bacon. 9 icfhiorpepmeodvoertdhiecebda,ccoenlearfytearnwdaosnnioicnesaannddcthhreisntihaedndieaddadsetdicskoomfebutter.
10 i set a stick of butter, and i 'm going to add a quarter cup of cornstarch.

Key Clips & Utterances
Key clip:ùë£4(ùë°4)

cast iron skillet

with heated bacon skillet

Structured Procedural Knowledge S

ùëù1
heat

ùëù2

heat cast iron skillet;

fry fry bacon on heated skillet

Key clip:ùë£9(ùë°9)

bacon
ùëù3

skillet

diced celery

ùëù4
remove

onions add

remove bacon from skillet;
stick of add diced celery, onions, butter and stick of butter

quarter cup of cornstarch

Verb

add quarter cup of

Key clip:ùë£10(ùë°10) ùëù5

add

Argument cornstarch

Figure 1: An example of extracting procedures for task ‚ÄúMaking Clam Chowder‚Äù.

modality (e.g., text only (Kiddon et al., 2015) or video only (Zhukov et al., 2019; Alayrac et al., 2016)), using closed-domain taxonomies (Tang et al., 2019), or lacking structure in the procedural representation (Zhou et al., 2018a).
In our task, given a narrative video, say a cooking video on YouTube about making clam chowder as shown in Figure 1, our goal is to extract a series of tuples representing the procedure, e.g. (heat, cast iron skillet), (fry, bacon, with heated skillet), etc. We created a manually annotated, large test dataset for evaluation of the task, including over 350 instructional cooking videos along with over 15,000 English sentences in the transcripts spanning over 89 recipe types. This verb-argument structure using arbitrary textual phrases is motivated by open information extraction (Schmitz et al., 2012; Fader et al., 2011), but focuses on procedures rather than entity-entity relations.
This task is challenging with respect to both video and language understanding. For video, it requires understanding of video contents, with a spe-

cial focus on actions and procedures. For language, it requires understanding of oral narratives, including understanding of predicate-argument structure and coreference. In many cases it is necessary for both modalities to work together, such as when resolving null arguments necessitates the use of objects or actions detected from video contents in addition to transcripts. For example, the cooking video host may say ‚Äújust a pinch of salt in‚Äù, while adding some salt into a boiling pot of soup, in which case inferring the action ‚Äúadd‚Äù and its argument ‚Äúpot‚Äù requires visual understanding.
Along with the novel task and dataset, we propose several baseline approaches that extract structure in a pipelined fashion. These methods Ô¨Årst identify key clips/sentences using video and transcript information with unsupervised and supervised multimodal methods, then extract procedure tuples from the utterances and/or video of these key clips. On the utterances side, we utilize an existing state-of-the-art semantic role labeling model (Shi and Lin, 2019), with the intuition that semantic role labeling captures the verb-argument structures of a sentence, which would be directly related to procedures and actions. On the video side, similarly, we utilize existing state-of-the-art video action/object recognition model trained in kitchen settings to further augment utterance-only extraction results. The results are far from perfect, demonstrating that the proposed task is challenging and that structuring procedures requires more than just state-of-the-art semantic parsing or video action recognition.
2 Problem DeÔ¨Ånition
We show a concrete example of our procedural knowledge extraction task in Figure 1. Our ultimate goal is to automatically map unstructured instructional video (clip and utterances) to structured procedures, deÔ¨Åning what actions should be performed on which objects, with what arguments and in what order. We deÔ¨Åne the input to such an extraction system:
‚Ä¢ Task R, e.g. ‚ÄúCreate Chicken Parmesan‚Äù and instructional video VR describing the procedure to achieve task R, e.g. a video titled ‚ÄúChicken Parmesan - Let‚Äôs Cook with ModernMom‚Äù.1
‚Ä¢ A sequence of n sentences TR = {t0, t1, ..., tn} representing video VR‚Äôs corresponding transcript. According to the time stamps of the
1https://www.youtube.com/watch?v= nWGpCmDlNU4

General domain? Multimodal input? Use transcript? Use noisy text? Open extraction? Structured format?

Ours AR YC2 CT COIN How2 HAKE TACOS

Table 1: Comparison to current datasets.

transcript sentences, the video is also segmented into n clips VR = {v0, v1, ..., vn} accordingly to align with the sentences in the transcript TR.
The output will be:
‚Ä¢ A sequence of m procedure tuples SR = {s0, s1, ..., sm} describing the key steps to achieve task R according to instructional video VR.
‚Ä¢ An identiÔ¨Åed list of key video clips and corresponding sentences VR ‚äÜ VR, to which procedures in SR are grounded.
Each procedural tuple sj = (verb, arg1, ..., argk) ‚àà SR consists of a verb phrase and its arguments. Only the ‚Äúverb‚Äù Ô¨Åeld is required, and thus the tuple size ranges from 1 to k + 1. All Ô¨Åelds can be either a word or a phrase.
Not every clip/sentence describes procedures, as most videos include an intro, an outro, nonprocedural narration, or off-topic chit-chat. Key clips VR are clips associated with one or more procedures in PR, with some clips/sentences associated with multiple procedure tuples. Conversely, each procedure tuple will be associated with only a single clip/sentence.
3 Dataset & Analysis
While others have created related datasets, they fall short on key dimensions which we remedy in our work. SpeciÔ¨Åcally, In Table 1 we compare to AllRecipes (Kiddon et al., 2015) (AR), YouCook2 (Zhou et al., 2018b) (YC2), CrossTask (Zhukov et al., 2019) (CT), COIN (Tang et al., 2019), How2 (Sanabria et al., 2018), HAKE (Li et al., 2019) and TACOS (Regneri et al., 2013). Additional details about datasets are included in the Appendix A.2 In summary, none have both structured and open extraction annotations for the procedural knowledge extraction task, since most focus on either video summarization/captioning or action localization/classiÔ¨Åcation.
2A common dataset we do not include here is HowTo100M (Miech et al., 2019) as it does not contain any annotations.

Frequency

2000 1500 1429

1000

500

215 170 152 139 126 104 63 61 55 55 44 43 41

0

225500 208 185

200

155

115000 111 107 97 94 91 88 78 73 62 49 41 41 40 50

0

Frequency

Figure 3: Most frequent verbs (upper) and arguments (lower).

Figure 2: Annotation interface.

Total # Average # per key clip Average #words % directly from transcript % coreference (pronouns) % ellipsis

Verbs 4004 1.12 1.07 69.8 N/A 30.2

Arguments 6070 1.70 1.43 75.0 14.4 10.6

Table 2: Statistics of annotated verbs and arguments in procedures.

3.1 Dataset Creation
To address the limitations of existing datasets, we created our own evaluation dataset by annotating structured procedure knowledge given the video and transcript. Native English-speakers annotated four videos per recipe type (e.g. clam chowder, pizza margherita, etc.) in the YouCook2 dataset into the structured form presented in ¬ß2 (totaling 356 videos). Annotators selected key clips as important steps and extracted corresponding Ô¨Åelds to Ô¨Åll in verbs and arguments. Filling in the Ô¨Åelds with the original tokens was preferred but not required (e.g., in cases of coreference and ellipsis). The result is a series of video clips labeled with procedural structured knowledge as a sequence of steps sj and series of short sentences describing the procedure.
Figure 2 shows the user interface of annotation tool. The process is divided into 3 questions per clip: Q1: Determine if the video clip is a key step if: (1) the clip or transcript contains at least one action; (2) the action is required for accomplishing the task (i.e. not a self introduction); and (3) for if a clip duplicates a previous key clip, choose the one with clearer visual and textual signals (e.g. without coreference, etc.). Q2: For each key video

clip, annotate the key procedural tuples. We have annotators indicate which actions are both seen and mentioned by the instructor in the video. The actions should correspond to a verb and its arguments from the original transcript except in the case of ellipsis or coreference where they have to refer to earlier phrases based on the visual scene. Q3: Construct a short Ô¨Çuent sentence from the annotated tuples for the given video clip.
We have two expert annotators and a professional labeling supervisor for quality control and deciding the Ô¨Ånal annotations. To improve the data quality, the supervisor reviewed all labeling results, and applied several heuristic rules to Ô¨Ånd anomalous records for further correction. The heuristic is to check the annotated verb/arguments that are not found in corresponding transcript text. Among these anomalies, the supervisor checks the conÔ¨Çicts between the two annotators. 25% of all annotations were modiÔ¨Åed as a result. On average annotators completed task Q1 at 240 sentences (clips) per hour and task Q2 and Q3 combined at 40 sentences per hour. For Q1, we observe an inter-annotator agreement with Cohen‚Äôs Kappa of 0.83.3 Examples are shown in Table 3.
3.2 Dataset Analysis
Overall, the dataset contains 356 videos with 15,523 video clips/sentences, among which 3,569 clips are labeled as key steps. Sentences average 16.3 tokens, and the language style is oral English. For structured procedural annotations, there are 347 unique verbs and 1,237 unique objects in all. Statistics are shown in Table 2. Figure 3 lists the most commonly appearing verbs and entities. The action add is most frequently performed, and the entities
3We use the Jaccard ratio between the annotated tokens of two annotators for Q2‚Äôs agreement. Verb annotations have a higher agreement at 0.77 than that of arguments at 0.72.

Transcript sentence so we‚Äôve placed the dough directly into the caputo Ô¨Çour that we import from italy. we just give (ellipsis) a squish with our palm and make it Ô¨Çat in the center. so will have to rotate it every thirty to forty Ô¨Åve seconds ...

Procedure summary place dough in caputo Ô¨Çour
squish dough with palm Ô¨Çatten center of dough rotate pizza every 30-45 seconds

Verb place
squish Ô¨Çatten rotate

Arguments dough
dough center of dough pizza

caputo Ô¨Çour
with palm
every 30-45 seconds

Table 3: Annotations of structured procedures and summaries. Coreference and ellipsis are marked with italics and are resolved into referred phrases also linked back in the annotations. See Appendix (Table 6) for more examples.

Input

ùë°1

Hello everyone, today i am going to ...

ùë£1

ùë°2

Put some bacon in there and fry it up ‚Ä¶

ùë£2

...

ùë°ùëõ

You are good to go, thanks for watching!

ùë£ùëõ

Key Clip Prediction

Stage 1

Is key clip? If yes:

Procedural

Knowledge

Extraction

Stage 2

Extract tuples from key clips

ùë£2 ùë°2 <put, bacon, ‚Ä¶> ùë£2 ùë°2 <fry, bacon, ‚Ä¶>
...
ùë£ùëö ùë°ùëö <remove, bacon, ‚Ä¶>

Output

Figure 4: Extraction pipeline.

salt and onions are the most popular ingredients. In nearly 30% of annotations, some verbs and ar-
guments cannot be directly found in the transcript. An example is ‚Äú(add) some salt into the pot‚Äù, and we refer to this variety of absence as ellipsis. Arguments not mentioned explicitly are mainly due to (1) pronoun references, e.g. ‚Äúput it (Ô¨Åsh) in the pan‚Äù; (2) ellipsis, where the arguments are absent from the oral language, e.g. ‚Äúput the mixture inside‚Äù where the argument ‚Äúoven‚Äù is omitted. The details can be found in Table 2. The coreferences and ellipsis phenomena add difÔ¨Åculty to our task, and indicate the utility of using multimodal information from the video signal and contextual procedural knowledge for inference.
4 Extraction Stage 1: Key Clip Selection
In this and the following section, we describe our two-step pipeline for procedural knowledge extraction (also in Figure 4). This section describes the Ô¨Årst stage of determining which clips are ‚Äúkey clips‚Äù that contribute to the description of the procedure. We describe several key clip selection models, which consume the transcript and/or the video within the clip and decide whether it is a key clip.
4.1 Parsing-Based Heuristic Baselines
Given our unsupervised setting, we Ô¨Årst examine two heuristic parsing-based methods that focus on the transcript only, one based on semantic role labeling (SRL) and the other based on an unsuper-

vised segmentation model Kiddon et al. (2015). Before introducing heuristic baselines, we note
that having a lexicon of domain-speciÔ¨Åc actions will be useful, e.g., for Ô¨Åltering pretrained model outputs, or providing priors to the unsupervised model described later. In our cooking domain, these actions can be expected to consist mostly of verbs related to cooking actions and procedures. Observing recipe datasets such as AllRecipes (Kiddon et al., 2015) or WikiHow (Miech et al., 2019; Zhukov et al., 2019), we Ô¨Ånd that they usually use imperative and concise sentences for procedures and the Ô¨Årst word is usually the action verb like ‚Äúadd‚Äù, e.g., add some salt into the pot. We thus construct a cooking lexicon by aggregating the frequently appearing verbs as the Ô¨Årst word from AllRecipes, with frequency over a threshold of 5. We further Ô¨Ålter out words that have no verb synsets in WordNet (Miller, 1995). Finally we manually Ô¨Ålter out noisy or too general verbs like ‚Äúgo‚Äù. Note that when applying to other domains, the lexicon can be built following a similar process of Ô¨Årst Ô¨Ånding a domain-speciÔ¨Åc corpus with simple and formal instructions, and then obtaining the lexicon by aggregation and Ô¨Åltering. Semantic role labeling baselines. One intuitive trigger in the transcript for deciding whether the sentence is a key step should be the action words, i.e. the verbs. In order to identify these action words we use semantic role labeling (Gildea and Jurafsky, 2002), which analyzes natural language sentences to extract information about ‚Äúwho did what to whom, when, where and how?‚Äù The output is in the form of predicates and their respective arguments that acts as semantic roles, where the verb acts as the root (head) of the parse. We run a strong semantic role labeling model (Shi and Lin, 2019) included in the AllenNLP toolkit (Gardner et al., 2018) on each sentence in the transcript. From the output we get a set of verbs for each of the sentences.4 Because not all verbs in all sentences
4The SRL model is used in this stage only as a verb identiÔ¨Åer, with other output information used in stage 2.

represent actual key actions for the procedure, we additionally Ô¨Ålter the verbs with the heuristically created cooking lexicon above, counting a clip as a key clip only if at least one of the SRL-detected verbs is included in the lexicon. Unsupervised recipe segmentation baseline (Kiddon et al., 2015). The second baseline is based on the outputs of the unsupervised recipe sentence segmentation model in Kiddon et al. (2015). BrieÔ¨Çy speaking, the model is a generative probabilistic model where verbs and arguments, together with their numbers, are modeled as latent variables. It uses a bigram model for string selection. It is trained on the whole transcript corpus of YouCook2 videos iteratively for 15 epochs using a hard EM approach before the performance starts to converge. The count of verbs in the lexicon created in ¬ß4.1 is provided as a prior through initialization. We then do inference to parse the transcripts in our dataset using the trained model. Following the same heuristics as the SRL outputs, we treat sentences with non-empty parsed predicates after lexical Ô¨Åltering as key sentences, and those without as negatives.
4.2 Neural Selection Baseline
Next, we implement a supervised neural network model that incorporates visual information, which we have posited before may be useful in the face of incomplete verbal utterances. We extract the features of the sentence and each video frame using pretrained feature extractors respectively. Then we perform attention (Bahdanau et al., 2014) over each frame feature, using the sentence as a query, in order to acquire the representation of the video clip. Finally, we combine the visual and textual features to predict whether the input is a key clip. The model is trained on a general domain instructional key clip selection dataset with no overlap with ours, and our annotated dataset is used for evaluation only. Additional details about the model and training dataset are included in Appendix B.
5 Extraction Stage 2: Structured Knowledge Extraction
With the identiÔ¨Åed key clips and corresponding transcript sentences, we proceed to the second stage that performs clip/sentence-level procedural knowledge extraction from key clips. In this stage, the extraction is done from clips that are identiÔ¨Åed at Ô¨Årst as ‚Äúkey clips‚Äù.

5.1 Extraction From Utterances
We Ô¨Årst present two baselines to extract structured procedures using transcripts only, similarly to the key-clip identiÔ¨Åcation methods described in ¬ß4.1. Semantic role labeling. For the Ô¨Årst baseline, we use the same pretrained SRL model introduced in ¬ß4.1 to conduct inference on the sentences in key clips identiÔ¨Åed from stage 1. Because they consist of verb-argument structures, the outputs of the SRL model are well aligned with the task of extracting procedural tuples that identify actions and their arguments. However, not all outputs from the SRL model are the structured procedural knowledge we aim to extract. For example, in the sentence ‚Äúyou ‚Äôre ready to add a variety of bell peppers‚Äù from the transcript, the outputs from SRL model contains two parses with two predicates, ‚Äúare‚Äù and ‚Äúadd‚Äù, where only the latter is actually part of the procedure. To deal with this issue we Ô¨Årst perform Ô¨Åltering similar to that used in stage 1, removing parses with predicates (verbs) outside of the domain-speciÔ¨Åc action lexicon we created in ¬ß4.1. Next, we Ô¨Ålter out irrelevant arguments in the parse. For example, the parse from the SRL model for sentence ‚ÄúI add a lot of pepper because I love it.‚Äù after Ô¨Åltering out irrelevant verb ‚Äúlove‚Äù is ‚Äú[ARG0: I] [V: add] [ARG1: a lot of pepper] [ARGM-CAU: because I love it]‚Äù, some arguments such as ARG0 and ARGM-CAU are clearly not contributing to the procedure. We provide a complete list of the Ô¨Åltered argument types in Appendix C. Unsupervised recipe segmentation (Kiddon et al., 2015). The second baseline is to use the same trained segmentation model as in ¬ß4.1 to segment selected key transcript sentences into verbs and arguments. We treat segmented predicates in the key sentence as procedural verbs, and segmented predicate arguments plus preposition arguments as procedural arguments.
5.2 Extraction From Video
We also examine a baseline that utilizes two forms of visual information in videos: actions and objects. We predict both verbs and nouns of a given video clip via a state-of-the-art action detection model TSM (Lin et al., 2019),5 trained on the EpicKitchen (Damen et al., 2018a) dataset.6 For each video, we extract 5-sec video segments and feed
5https://github.com/epic-kitchens/ action-models
6https://epic-kitchens.github.io/2019

Acc P R F1

Parsing-based Heuristics

SRL w/o heur. 25.9 23.4 97.6 37.7

SRL w/ heur.

61.2 35.2 81.4 49.1

Kiddon et al. (2015) 67.3 33.5 42.7 37.6

Neural Model

Visual Only

43.8 27.2 85.9 41.3

Text Only

76.3 49.0 78.1 60.2

V+T (Full Model) 77.7 51.0 75.3 60.8

Table 4: Key clip selection results.

into the action detection model. The outputs of the models are in a predeÔ¨Åned set of labels of verbs (actions) and nouns (objects).7 We directly combine the outputs from the model on each video segment, aggregate and temporally align them with key clips/sentences, forming the Ô¨Ånal output.
5.3 Utterance and Video Fusion
Finally, to take advantage of the fact that utterance and video provide complementary views, we perform multimodal fusion of the results of both of these model varieties. We adopt a simple method of fusion by taking the union of the verbs/actions and arguments/objects respectively from the best performing utterance-only model and the visual detection model.
6 Evaluation
We propose evaluation metrics and provide evaluation results on our annotated dataset for both of the two stages: key clip selection and structured procedural extraction. Detailed reproducibility information about the experiments are in Appendix F. Besides quantitative evaluation and qualitative evaluations, we also analyze the key challenges of this task.
6.1 Extraction Stage 1: Key Clip Selection
In this section, we evaluate results of the key clip selection described in ¬ß4. We evaluate using the accuracy, precision, recall and F1 score for the binary classiÔ¨Åcation problem of whether a given clip in the video is a key clip. The results are shown in Table 4. We compare parsing-based heuristic models and supervised neural models, with ablations (model details in Appendix B). From the experimental results in Table 4, we can see that:
1. Unsupervised heuristic methods perform worse than neural models with training data. This is
7Notably, this contrasts to our setting of attempting to recognize into an open label set, which upper-bounds the accuracy of any model with a limited label set.

despite the fact that the dataset used for training neural models has a different data distribution and domain from the test set. 2. Among heuristic methods, pretrained SRL is better than Kiddon et al. (2015) even though the second is trained on transcript text from YouCook2 videos. One possible reason is that the unsupervised segmentation method was specially designed for recipe texts, which are mostly simple, concise and imperative sentences found in recipe books, while the transcript is full of noise and tends to have longer, more complicated, and oral-style English. 3. Post-processing signiÔ¨Åcantly improves the SRL model, showing that Ô¨Åltering unrelated arguments and incorporating the cooking lexicon helps, especially with reducing false positives. 4. Among neural method ablations, the model using only visual features performs worse than that using only text features. The best model for identifying key clips among proposed baselines uses both visual and text information in the neural model.
Besides quantitative evaluation, we analyzed key clip identiÔ¨Åcation results and found a number of observations. First, background introductions, advertisements for the YouTube channel, etc. can be relatively well classiÔ¨Åed due to major differences both visually and textually from procedural clips. Second, alignment and grounding between the visual and textual domains is crucial for key clip prediction, yet challenging. For example, the clip with the transcript sentence ‚Äúadd more pepper according to your liking‚Äù is identiÔ¨Åed as a key clip. However, it is in fact merely a suggestion made by the speaker about an imaginary scenario, rather than a real action performed and thus should not be regarded as a key procedure.
6.2 Extraction Stage 2: Structured Procedure Extraction
In this stage, we perform key clip-level evaluation for structured procedural knowledge extraction by matching the ground truth and predicted structures with both exact match and two fuzzy scoring strategies. To better show how stage 1 performance affects the whole pipeline, we evaluate on both ground truth (oracle) and predicted key clips. Similarly to the evaluation of key clip selection, we compare the parsing-based methods (¬ß5.1), as well as purposing the action detection results from

Model
Kiddon et al. (2015) SRL w/o heur. SRL w/ heur. Visual Fusion
Kiddon et al. (2015) SRL w/o heur. SRL w/ heur. Visual Fusion

Exact Match P R F1
12.0 10.9 11.4 19.4 54.7 28.6 38.7 51.6 44.3 4.1 6.7 5.1 19.9 55.2 29.3
7.0 6.3 6.6 11.2 31.7 16.6 22.5 29.9 25.7 2.4 3.9 3.0 11.5 32.0 17.0

Verbs

Fuzzy

Partial Fuzzy

Exact Match

P R F1 P R F1 P R F1

Using oracle key clips

18.8 17.2 18.0 20.2 18.4 19.3 0.4 0.9 0.5

25.3 70.1 37.2 26.6 73.8 39.1 1.3 5.4 2.0

45.2 60.3 51.7 46.9 62.6 53.6 1.6 3.3 2.2

17.9 27.8 21.7 19.3 30.1 23.5 0.9 1.1 1.0

28.6 73.3 41.2 31.2 78.6 44.7 1.1 3.8 1.6

Using predicted key clips

10.9 10.0 10.4 11.7 10.7 11.2 0.2 0.5 0.3

14.7 40.7 21.6 15.4 42.8 22.6 0.7 3.1 1.2

26.2 35.0 30.0 27.2 36.3 31.1 0.9 1.9 1.3

10.4 16.1 12.6 11.2 17.5 13.7 0.5 0.6 0.6

16.6 42.5 23.9 18.1 45.6 25.9 0.6 2.2 1.0

Arguments Fuzzy P R F1
10.4 19.3 13.5 14.1 53.6 22.3 21.2 39.8 27.7 17.8 25.8 21.1 16.9 50.0 25.2
6.1 11.2 7.9 8.2 31.1 13.0 12.3 23.1 16.1 10.3 15.0 12.2 9.8 29.0 14.6

Partial Fuzzy P R F1
16.4 30.2 21.3 22.0 81.8 34.6 32.3 59.5 41.9 24.2 36.2 29.0 24.4 72.5 36.5
9.5 17.5 12.3 12.7 47.4 20.1 18.8 34.5 24.3 14.1 21.0 16.8 14.1 42.1 21.2

Table 5: Clip/sentence-level structured procedure extraction results for verbs and arguments.

video signals for our task. Besides, we compare utterance-only and video-only baselines with our naive multi-modal fusion method.
We evaluate with respect to precision, recall and the F1 measure. Similarly to the evaluation method used for SRL (Carreras and Ma`rquez, 2004), precision (P) is the proportion of verbs or arguments predicted by a model which are correct, i.e. T P/#predicted where T P is the number of true positives. Recall (R) is the proportion of correct verbs or arguments which are predicted by a model, i.e. T P/#gold. The key here is how to calculate T P and we propose 3 methods: exact match, fuzzy matching, and partial fuzzy matching. The Ô¨Årst is straight forward, we count true positives if and only if the predicted phrase is an exact string match in the gold phrases. However, because our task lies in the realm of open phrase extraction without predeÔ¨Åned labels, it is unfairly strict to count only the exact string matches as T P . Also by design, the gold extraction results cannot always be found in the original transcript sentence (refer to ¬ß3.2), so we are also unable to use token-based metrics as in sequence tagging (Sang and De Meulder, 2003), or span-based metrics as in some question answering tasks (Rajpurkar et al., 2016). Thus for the second metric we call ‚Äúfuzzy‚Äù, we leverage edit distance to enable fuzzy matching and assign a ‚Äúsoft‚Äù score for T P . In some cases, the two strings of quite different lengths will hurt the fuzzy score due to the nature of edit distance, even though one string is a substring of another. To get around this, we propose a third metric, ‚Äúpartial fuzzy‚Äù to get the score of the best matching substring with the length of the shorter string in comparison. Note that this third metric will bias towards shorter, correct phrases and thus we should have a holistic view of all 3 metrics during the evaluation. Details of two fuzzy metrics are described in Appendix D. Table 5 illus-

trates evaluation results:
1. Argument extraction is much more challenging compared to verb extraction, according the results: arguments contain more complex types of phrases (e.g. objects, location, time, etc.) and are longer in length. It is hard to identify complex arguments with our current heuristic or unsupervised baselines and thus the need for better supervised or semi-supervised models.
2. Heuristic SRL methods perform better than the unsupervised segmentation model even though the second is trained on our corpus. This demonstrates the generality of SRL models, but the heuristics applied at the output of SRL models still improve the performance by reducing false positives.
3. The visual-only method performs the worst, mainly because of the domain gap between visual detection model outputs and our annotated verbs and arguments. Other reasons include: the closed label set predeÔ¨Åned in EpicKitchen; challenges in domain transferring from closed to open extraction; different video data distribution between EpicKitchen (for training) and our dataset (YouCook2, for testing); limited performance of video detection model itself.
4. Naive multimodal fusion leads to an overall performance drop to below the utterance-only model, partly due to the differences in video data distribution and domain, as well as the limitation of the predeÔ¨Åned set of verbs and nouns in the EpicKitchen dataset, implying the need for better multimodal fusion method. Unsurprisingly, the recall for verb extraction raises after the fusion, suggesting that action detection in videos helps with the coverage. The drop in argument extraction suggests the complexity of arguments in our open extraction setting: it

should be more than mere object detection.
Besides quantitative results, we also showcase qualitative analysis of example extraction outputs in Appendix E. From both, we suggest that there are two key challenges moving forward: Verb extraction: We Ô¨Ånd that verb ellipsis is common in transcripts. The transcript text contains sentences where key action ‚Äúverbs‚Äù do not have verb part-of-speech in the sentence. For example, in the sentence ‚Äúgive it a Ô¨Çip ...‚Äù with the annotation (‚ÄúÔ¨Çip‚Äù, ‚Äúpancake‚Äù), the model detects ‚Äúgive‚Äù as the verb rather than ‚ÄúÔ¨Çip‚Äù. Currently all our baselines are highly reliant on a curated lexicon for verb selection and thus such cases will get Ô¨Åltered out. How to deal with such cases with general verbs like make, give, do remains challenging and requires extracting from the contexts. Argument extraction: Speech-to-text errors are intrinsic in automatically acquired transcripts and cause problems during parsing that cascade. Examples are that ‚Äúadd Ô¨Çour‚Äù being recognized as ‚Äúadd Ô¨Çower‚Äù and ‚Äúsriracha sauce‚Äù being recognized as ‚Äúsarrah cha sauce‚Äù causing wrong extraction outputs. Coreference and ellipsis are also challenging and hurting current benchmark performance, as our baselines do not tackle any of these explicitly. Visual co-reference and language grounding (Huang et al., 2018, 2017) provides a feasible method for us to tackle these cases in the future.
7 Related Work
Text-based procedural knowledge extraction. Procedural text understanding and knowledge extraction (Chu et al., 2017; Park and Motahari Nezhad, 2018; Kiddon et al., 2015; Jermsurawong and Habash, 2015; Liu et al., 2016; Long et al., 2016; Maeta et al., 2015; Malmaud et al., 2014; Artzi and Zettlemoyer, 2013; Kuehne et al., 2017) has been studied for years on step-wise textual data such as WikiHow. Chu et al. (2017) extracted open-domain knowledge from how-to communities. Recently Zhukov et al. (2019) also studied to adopt the well-written how-to data as weak supervision for instructional video understanding. Unlike existing work on action graph/dependency extraction (Kiddon et al., 2015; Jermsurawong and Habash, 2015), our approach differs as we extract knowledge from the visual signals and transcripts directly, not from imperative recipe texts. Instructional video understanding. Beyond image semantics (Yatskar et al., 2016), unlike existing tasks for learning from instructional video (Zhou

et al., 2018c; Tang et al., 2019; Alayrac et al., 2016; Song et al., 2015; Sener et al., 2015; Huang et al., 2016; Sun et al., 2019b,a; Plummer et al., 2017; Palaskar et al., 2019), combining video & text information in procedures (Yagcioglu et al., 2018; Fried et al., 2020), visual-linguistic reference resolution (Huang et al., 2018, 2017), visual planning (Chang et al., 2019), joint learning of object and actions (Zhukov et al., 2019; Richard et al., 2018; Gao et al., 2017; Damen et al., 2018b), pretraining joint embedding of high level sentence with video clips (Sun et al., 2019b; Miech et al., 2019), our task proposal requires explicit structured knowledge tuple extraction.
In addition to closely related work (¬ß3) there is a wide literature (Malmaud et al., 2015; Zhou et al., 2018b; Ushiku et al., 2017; Nishimura et al., 2019; Tang et al., 2019; Huang et al., 2016; Shi et al., 2019; Ushiku et al., 2017) that aims to predict/align dense procedural captions given the video, which are the most similar works to ours. Zhou et al. (2018c) extracted temporal procedures and then generated captioning for each procedure. Sanabria et al. (2018) proposes a multimodal abstractive summarization for how-to videos with either human labeled or speech-to-text transcript. Alayrac et al. (2016) also introduces an unsupervised step learning method from instructional videos. Inspired by cross-task sharing (Zhukov et al., 2019), which is a weakly supervised method to learn shared actions between tasks, Ô¨Åne grained action and entity are important for sharing similar knowledge between various tasks. We focus on structured knowledge of Ô¨Åne-grained actions and entities.Visual-linguistic coreference resolution (Huang et al., 2018, 2017) is among one of the open challenges for our proposed task.
8 Conclusions & Open Challenges
We propose a multimodal open procedural knowledge extraction task, present a new evaluation dataset, produce benchmarks with various methods, and analyze the difÔ¨Åculties in the task. Meanwhile we investigate the limit of existing methods and many open challenges for procedural knowledge acquisition, including: to better deal with cases of coreference and ellipsis in visual-grounded languages; exploit cross-modalities of information with more robust, semi/un-supervised models; potential improvement from structured knowledge in downstream tasks (e.g., video captioning).

References
Jean-Baptiste Alayrac, Piotr Bojanowski, Nishant Agrawal, Josef Sivic, Ivan Laptev, and Simon Lacoste-Julien. 2016. Unsupervised learning from narrated instruction videos. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4575‚Äì4583.
Yoav Artzi and Luke Zettlemoyer. 2013. Weakly supervised learning of semantic parsers for mapping instructions to actions. Transactions of the Association for Computational Linguistics, 1:49‚Äì62.
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473.
Xavier Carreras and Llu¬¥ƒ±s Ma`rquez. 2004. Introduction to the conll-2004 shared task: Semantic role labeling. In Proceedings of the Eighth Conference on Computational Natural Language Learning (CoNLL-2004) at HLT-NAACL 2004, pages 89‚Äì97.
Chien-Yi Chang, De-An Huang, Danfei Xu, Ehsan Adeli, Li Fei-Fei, and Juan Carlos Niebles. 2019. Procedure planning in instructional videos. ArXiv, abs/1907.01172.
Cuong Xuan Chu, Niket Tandon, and Gerhard Weikum. 2017. Distilling task knowledge from how-to communities. In Proceedings of the 26th International Conference on World Wide Web, pages 805‚Äì814. International World Wide Web Conferences Steering Committee.
Dima Damen, Hazel Doughty, Giovanni Maria Farinella, Sanja Fidler, Antonino Furnari, Evangelos Kazakos, Davide Moltisanti, Jonathan Munro, Toby Perrett, Will Price, and Michael Wray. 2018a. Scaling egocentric vision: The epic-kitchens dataset. In European Conference on Computer Vision (ECCV).
Dima Damen, Hazel Doughty, Giovanni Maria Farinella, Sanja Fidler, Antonino Furnari, Evangelos Kazakos, Davide Moltisanti, Jonathan Munro, Toby Perrett, Will Price, et al. 2018b. Scaling egocentric vision: The epic-kitchens dataset. In Proceedings of the European Conference on Computer Vision (ECCV), pages 720‚Äì736.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248‚Äì255. Ieee.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.
Anthony Fader, Stephen Soderland, and Oren Etzioni. 2011. Identifying relations for open information extraction. In Proceedings of the conference on empirical methods in natural language processing, pages

1535‚Äì1545. Association for Computational Linguistics.
Daniel Fried, Jean-Baptiste Alayrac, Phil Blunsom, Chris Dyer, Stephen Clark, and Aida Nematzadeh. 2020. Learning to segment actions from observation and narration.
Jiyang Gao, Chen Sun, Zhenheng Yang, and Ram Nevatia. 2017. Tall: Temporal activity localization via language query. In Proceedings of the IEEE International Conference on Computer Vision, pages 5267‚Äì 5275.
Matt Gardner, Joel Grus, Mark Neumann, Oyvind Tafjord, Pradeep Dasigi, Nelson Liu, Matthew Peters, Michael Schmitz, and Luke Zettlemoyer. 2018. Allennlp: A deep semantic natural language processing platform. arXiv preprint arXiv:1803.07640.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic labeling of semantic roles. Computational linguistics, 28(3):245‚Äì288.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770‚Äì 778.
De-An Huang, Shyamal Buch, Lucio Dery, Animesh Garg, Li Fei-Fei, and Juan Carlos Niebles. 2018. Finding ‚Äúit‚Äù: Weakly-supervised, reference-aware visual grounding in instructional videos. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR).
De-An Huang, Li Fei-Fei, and Juan Carlos Niebles. 2016. Connectionist temporal modeling for weakly supervised action labeling. In European Conference on Computer Vision, pages 137‚Äì153. Springer.
De-An Huang, Joseph J Lim, Li Fei-Fei, and Juan Carlos Niebles. 2017. Unsupervised visual-linguistic reference resolution in instructional videos. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2183‚Äì2192.
Jermsak Jermsurawong and Nizar Habash. 2015. Predicting the structure of cooking recipes. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 781‚Äì786.
Chloe¬¥ Kiddon, Ganesa Thandavam Ponnuraj, Luke S. Zettlemoyer, and Yejin Choi. 2015. Mise en place: Unsupervised interpretation of instructional recipes. In EMNLP.
Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980.
Hilde Kuehne, Alexander Richard, and Juergen Gall. 2017. Weakly supervised learning of actions from transcripts. Computer Vision and Image Understanding, 163:78‚Äì89.

Yong-Lu Li, Liang Xu, Xijie Huang, Xinpeng Liu, Ze Ma, Mingyang Chen, Shiyi Wang, Hao-Shu Fang, and Cewu Lu. 2019. Hake: Human activity knowledge engine. arXiv preprint arXiv:1904.06539.
Ji Lin, Chuang Gan, and Song Han. 2019. Tsm: Temporal shift module for efÔ¨Åcient video understanding. In Proceedings of the IEEE International Conference on Computer Vision.
Changsong Liu, Shaohua Yang, Sari Saba-Sadiya, Nishant Shukla, Yunzhong He, Song-Chun Zhu, and Joyce Chai. 2016. Jointly learning grounded task structures from language instruction and visual demonstration. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1482‚Äì1492.
Reginald Long, Panupong Pasupat, and Percy Liang. 2016. Simpler context-dependent logical forms via model projections. arXiv preprint arXiv:1606.05378.
Hirokuni Maeta, Tetsuro Sasada, and Shinsuke Mori. 2015. A framework for procedural text understanding. In Proceedings of the 14th International Conference on Parsing Technologies, pages 50‚Äì60.
Jonathan Malmaud, Jonathan Huang, Vivek Rathod, Nicholas Johnston, Andrew Rabinovich, and Kevin Murphy. 2015. What‚Äôs cookin‚Äô? interpreting cooking videos using text, speech and vision. In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 143‚Äì152, Denver, Colorado. Association for Computational Linguistics.
Jonathan Malmaud, Earl Wagner, Nancy Chang, and Kevin Murphy. 2014. Cooking with semantics. In Proceedings of the ACL 2014 Workshop on Semantic Parsing, pages 33‚Äì38.
Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand Tapaswi, Ivan Laptev, and Josef Sivic. 2019. HowTo100M: Learning a Text-Video Embedding by Watching Hundred Million Narrated Video Clips. arXiv:1906.03327.
George A Miller. 1995. Wordnet: a lexical database for english. Communications of the ACM, 38(11):39‚Äì 41.
James Munkres. 1957. Algorithms for the assignment and transportation problems. Journal of the society for industrial and applied mathematics, 5(1):32‚Äì38.
Taichi Nishimura, Atsushi Hashimoto, Yoko Yamakata, and Shinsuke Mori. 2019. Frame selection for producing recipe with pictures from an execution video of a recipe. In Proceedings of the 11th Workshop on Multimedia for Cooking and Eating Activities, pages 9‚Äì16. ACM.

Shruti Palaskar, Jindrich Libovicky`, Spandana Gella, and Florian Metze. 2019. Multimodal abstractive summarization for how2 videos. arXiv preprint arXiv:1906.07901.
Hogun Park and Hamid Reza Motahari Nezhad. 2018. Learning procedures from text: Codifying how-to procedures in deep neural networks. In Companion Proceedings of the The Web Conference 2018, pages 351‚Äì358. International World Wide Web Conferences Steering Committee.
Bryan A Plummer, Matthew Brown, and Svetlana Lazebnik. 2017. Enhancing video summarization via vision-language embedding. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5781‚Äì5789.
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad: 100,000+ questions for machine comprehension of text. arXiv preprint arXiv:1606.05250.
Michaela Regneri, Marcus Rohrbach, Dominikus Wetzel, Stefan Thater, Bernt Schiele, and Manfred Pinkal. 2013. Grounding action descriptions in videos. Transactions of the Association for Computational Linguistics, 1:25‚Äì36.
Alexander Richard, Hilde Kuehne, and Juergen Gall. 2018. Action sets: Weakly supervised action segmentation without ordering constraints. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5987‚Äì5996.
Ramon Sanabria, Ozan Caglayan, Shruti Palaskar, Desmond Elliott, Lo¬®ƒ±c Barrault, Lucia Specia, and Florian Metze. 2018. How2: a large-scale dataset for multimodal language understanding. arXiv preprint arXiv:1811.00347.
Erik F Sang and Fien De Meulder. 2003. Introduction to the conll-2003 shared task: Languageindependent named entity recognition. arXiv preprint cs/0306050.
Michael Schmitz, Robert Bart, Stephen Soderland, Oren Etzioni, et al. 2012. Open language learning for information extraction. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 523‚Äì534. Association for Computational Linguistics.
Ozan Sener, Amir R Zamir, Silvio Savarese, and Ashutosh Saxena. 2015. Unsupervised semantic parsing of video collections. In Proceedings of the IEEE International Conference on Computer Vision, pages 4480‚Äì4488.
Botian Shi, Lei Ji, Yaobo Liang, Nan Duan, Peng Chen, Zhendong Niu, and Ming Zhou. 2019. Dense procedure captioning in narrated instructional videos. In Proceedings of the 57th Conference of the Association for Computational Linguistics, pages 6382‚Äì 6391.

Peng Shi and Jimmy Lin. 2019. Simple bert models for relation extraction and semantic role labeling. arXiv preprint arXiv:1904.05255.
Yale Song, Jordi Vallmitjana, Amanda Stent, and Alejandro Jaimes. 2015. Tvsum: Summarizing web videos using titles. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5179‚Äì5187.
Chen Sun, Fabien Baradel, Kevin Murphy, and Cordelia Schmid. 2019a. Learning video representations using contrastive bidirectional transformer.
Chen Sun, Austin Myers, Carl Vondrick, Kevin Murphy, and Cordelia Schmid. 2019b. Videobert: A joint model for video and language representation learning. arXiv preprint arXiv:1904.01766.
Yansong Tang, Dajun Ding, Yongming Rao, Yu Zheng, Danyang Zhang, Lili Zhao, Jiwen Lu, and Jie Zhou. 2019. Coin: A large-scale dataset for comprehensive instructional video analysis. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1207‚Äì1216.
Atsushi Ushiku, Hayato Hashimoto, Atsushi Hashimoto, and Shinsuke Mori. 2017. Procedural text generation from an execution video. In Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 326‚Äì335, Taipei, Taiwan. Asian Federation of Natural Language Processing.
Semih Yagcioglu, Aykut Erdem, Erkut Erdem, and Nazli Ikizler-Cinbis. 2018. RecipeQA: A challenge dataset for multimodal comprehension of cooking recipes. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1358‚Äì1368, Brussels, Belgium. Association for Computational Linguistics.
Mark Yatskar, Luke Zettlemoyer, and Ali Farhadi. 2016. Situation recognition: Visual semantic role labeling for image understanding. In Conference on Computer Vision and Pattern Recognition.
Luowei Zhou, Nathan Louis, and Jason J Corso. 2018a. Weakly-supervised video object grounding from text by loss weighting and object interaction. arXiv preprint arXiv:1805.02834.
Luowei Zhou, Chenliang Xu, and Jason J Corso. 2018b. Towards automatic learning of procedures from web instructional videos. In Thirty-Second AAAI Conference on ArtiÔ¨Åcial Intelligence.
Luowei Zhou, Yingbo Zhou, Jason J Corso, Richard Socher, and Caiming Xiong. 2018c. End-to-end dense video captioning with masked transformer. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 8739‚Äì8748.
Dimitri Zhukov, Jean-Baptiste Alayrac, Ramazan Gokberk Cinbis, David Fouhey, Ivan Laptev, and Josef Sivic. 2019. Cross-task weakly supervised learning

from instructional videos. In Computer Vision and Pattern Recognition (CVPR).

A Comparison with Existing Datasets
There are publicly available datasets related to understanding instructional videos:
‚Ä¢ AllRecipes (Kiddon et al., 2015) (AR). The authors collected 2,456 recipes from AllRecipes weibsite8. The sentences in the dataset are mostly simple imperative English describing concise steps to make a given dish, where the Ô¨Årst word is usually the verb describing the action. The ingredient list information is also available. In contrast, our task seeks to extract procedural information from more noisy, oral and erroneous languages in real life video context.
‚Ä¢ YouCook29 (Zhou et al., 2018b) (YC2). The procedure steps for each video are annotated with temporal boundaries in the video and described by human-written imperative English sentences. However, this dataset does not contain more Ô¨Åne-grained annotations in a structured form.
‚Ä¢ HowTo100M10(Miech et al., 2019). This is a large scale how-to videos dataset, searched on YouTube using the task taxonomy on WikiHow11 as a source. However, it does not contain any annotations although the domain is more general.
‚Ä¢ CrossTask12(Zhukov et al., 2019) (CT). Based on HowTo100M, this dataset is used for weakly supervised learning with 18 tasks fully labeled and 65 related tasks unlabeled. Although the dataset is annotated in a structured way by separating verbs and objects, the label space is closed with predeÔ¨Åned sets of verbs and objects. The dataset also does not allow multiple verbs or objects to be extracted for a single segment.
‚Ä¢ COIN13 (Tang et al., 2019). This contains instructional (how-to) videos, in a closed taxonomy of tasks and steps. The authors annotated time spans of steps in a video with pre-deÔ¨Åned
8https://www.allrecipes.com/ 9http://youcook2.eecs.umich.edu/ 10https://www.di.ens.fr/willow/ research/howto100m/ 11https://www.wikihow.com 12https://github.com/DmZhukov/CrossTask 13https://coin-dataset.github.io/

steps, however the biggest drawback is that it is unstructured and closed domain.
‚Ä¢ How214 (Sanabria et al., 2018). This dataset annotates ground truth transcript text to help abstractive summarization, a very different task than ours of structured data extraction.
‚Ä¢ HAKE15 (Li et al., 2019). Human Activity Knowledge Engine (HAKE) is a large-scale knowledge base of human activities, built upon existing activity datasets, and supplies human instance action labels and corresponding body part level atomic action labels. However, HAKE uses closed activity and part state classes. It also does not contain videos of activities accompanied with narrative transcripts.
‚Ä¢ TACOS16 (Regneri et al., 2013). This dataset considers the problem of grounding sentences describing actions in visual information extracted from videos in kitchen settings. The dataset contains expert annotations of low level activity tags, with a total of 60 different activity labels with numerous associated objects, and sequences of NL sentences describing actions in the kitchen videos. This dataset also does not support open extraction and the videos are provided using human annotated caption sentences, rather than transcript texts with noise.
B Neural Selection Model
Figure 5 presents the overall detailed structure of the neural selection model for combining utterance and video information for key clip selection.
Sentence token encoding Each input clip is accompanied with a sentence S = {t1, . . . , tk} which has k tokens. We use a pre-trained BERT (Devlin et al., 2018) model as the encoder and extract the sentence representation s.
Video frame features For each clip we uniformly sample T = 10 frames and use an ImageNet-pretrained (Deng et al., 2009) ResNet50 (He et al., 2016) to extract the feature vector of each frame as X = {x1, ¬∑ ¬∑ ¬∑ , xT }.
14https://github.com/srvk/how2-dataset 15http://hake-mvig.cn 16http://www.coli.uni-saarland.de/ projects/smile/page.php?id=tacos

Video Frame Features

x1

...

‚Ä¶

ResNet 50

xi

...

‚Ä¶

xT

Attention-based Frame Encoding
Attn(s,X)
as

Pre-trained BERT

‚Ä¶ I‚Äôm heating it up

with a medium

medium high flame

s

and I‚Äôm going to put

some bacon ...

Transcript Feature

√ó -1

1/0

Key Clip Prediction

MLP

Figure 5: Neural key clip selection model.

Attention-based frame encoding To model the interaction between the encoded sentence and the feature of each frame, we adopt an attention-based method. We Ô¨Årst calculate the attention weight as by a tensor product of sentence feature s with each video frame xi followed by a softmax layer. Then we perform a weighted sum on all frame features to get Attn(s, X).
Visual-utterance fusion Finally, we fuse the extracted transcript features s with the attended video features Attn(s, X) by a tensor product and Ô¨Çatten it into a vector. Then we use a non-linear activation layer to map these features into a real number, which represents the probability of the clip being a key clip.
Experiment details In the presented experiments, we use a pre-trained BERT (Devlin et al., 2018) model17 to extract the continuous representation of each sentence. During Ô¨Åne-tuning, the model is optimized by Adam optimizer (Kingma and Ba, 2014) with the starting learning rate of 1e ‚àí 4. The model is trained in a supervised fashion with a separate key clip/sentence classiÔ¨Åcation dataset that is not related to YouCook2. This auxiliary dataset will also be publicly released. All of them are general domain instructional videos harvested from from YouTube. Human annotators labeled whether it is a key clip when given a video clip-sentence pair. In the end, we have 1,034 videos (40,146 pairs) for training the classiÔ¨Åcation model. We split the dataset into two subset as 772 videos (28,519 pairs) and 312 videos (11,627 pairs)
17https://github.com/hanxiao/ bert-as-service

for training and validation (hyper-parameter tuning) respectively. The testing set is our proposed dataset with key clips and sentences annotated (see ¬ß3), containing 356 videos and 15,523 pairs. The testing set used is the same as all other compared methods.
C SRL Argument Filtering
The argument types that we deem to not contribute as the procedural knowledge for completing the task and Ô¨Ålter out include: ARG0 (usually refers to the subject, usually a person), AMMOD (modal verb), AM-CAU (cause), AM-NEG (negation marker), AM-DIS (discourse marker), AM-REC (reciprocal), AM-PNC/PRP (purpose), AM-EXT (extent), and R-ARG* (in-sentence references).
D Fuzzy Matching and Partial Fuzzy Matching
Fuzzy matching Denote the Levenshtein distance between string a and string b as d(a, b). We then deÔ¨Åne a normalized pairwise score between 0 to 1 as s(a, b) = d(a, b)/max{|a|, |b|} Given a set of n predicted phrases X = {x1, ..., xn} and a set of m ground truth phrases G = {g1, ..., gm}, we can Ô¨Ånd a set of min(n, m) string pairs between predicted X and ground truth G, as M = {(xi, gj)} that maximizes the sum of scores (xi,gj)‚ààM s(xi, gj). This assignment problem can be solved efÔ¨Åciently with KuhnMunkres (Munkres, 1957) algorithm18. Since this fuzzy pairwise score is normalized, it can be regarded as a soft version for calculating T P = max (xi,gj)‚ààM s(xi, gj ).
Partial fuzzy matching The only difference from ‚Äúfuzzy‚Äù matching is that the scoring function now follows the ‚Äúbest partial‚Äù heuristic that assuming the shorter string a is length |a|, and the longer string b is length |b|, we now calculate the score between shorter string and the best ‚Äúfuzzy‚Äù matching length-|a| substring.
s(a, b) = max{d(a, t)}/|a|,t ‚àà substring of b,
|t| = |a|, |a| < |b|
Both fuzzy metric implementations are based on FuzzyWuzzy19.
18http://software.clapper.org/munkres/ 19https://github.com/seatgeek/ fuzzywuzzy

E Example Extractions
In this section, we showcase some example extractions from our overall best-performing baseline model ‚ÄúSRL w/ heur.‚Äù in Table 6 (next page). We show both the annotated structured extractions as well as the model output, for a recipe of type ‚Äúpizza marghetta‚Äù.20 Note that the table includes only transcript sentences that are annotated as key steps. We can see that some sentences do no have extraction output from the model, while others tend to be over-extracted as long spans of text or incorrect due to the noisy nature of the transcript, degrading the extraction quality. Verbs are also relatively better extracted than arguments for the proposed model. From the extraction examples, we can also see that the model sometimes omits important action verbs in the extraction, and extract pronouns like ‚Äúit‚Äù, ‚Äúthis‚Äù, ‚Äúhere‚Äù, etc. as arguments. This suggests that the utterance-only model cannot handle coreference and ellipsis scenarios very well, which is one of the key difÔ¨Åculties of the proposed task (discussed in Section 3.2). This also partly implies the need for utilizing the visual information to extract actions and arguments that are not included in transcript, as well as visual co-reference and language grounding to help resolve what objects the pronouns in the extracted arguments are referring to.

The neural selection model (Section 4.2) takes 6 hours on average to train on both transcript and video data. The number of parameters for the neural model is the addition of those from BERT-base model, ResNet-50 model, an attention module and a MLP layer. More details are described in Appendix B.
The action and object detection model (Section 5.2) pretrained on EpicKitchen dataset is retrieved from https://github.com/ epic-kitchens/action-models. In our use case, we only need to perform the inference steps on our dataset, which costs 3 hours on average for all the video clips in our proposed dataset.

F Reproducibility Details
All the experiments in this paper are performed on a workstation with one 8-core Intel i7 processor, 32GB RAM and one NVIDIA Tesla K80 GPU.
The SRL-based models‚Äô runtime are bounded by the inference speed of the pretrained model, and take 30 minutes on average to complete the evaluation. The code is based on the semantic role labeling pretrained model from
https://github.com/allenai/allennlp-hub/
blob/master/allennlp_hub/pretrained/
allennlp_pretrained.py. Kiddon et al. (2015) models are trained from
scratch with default hyperparameter settings on our dataset using only CPU, and take 2 hours on average to complete training and evaluation. We directly use the released code (https://github. com/uwnlp/recipe-interpretation) to train the unsupervised model on our dataset for evaluation.
20https://www.youtube.com/watch?v= FHvZgt3ExDI

Table 6: Example extractions compared with gold annotations for a recipe of type ‚Äúpizza marghetta‚Äù. Only

transcript sentences that are annotated as key steps are included. Some long and incorrect extractions are quoted

with omission.

Transcript Sentence

Procedure

Gold

Gold Arguments

Extracted Extracted

Summary

Verbs

Verbs Arguments

so we ‚Äôve placed the dough directly into place dough in place dough, caputo Ô¨Çour place so, the dough,

the caputo Ô¨Çour that we import from

caputo Ô¨Çour

directly, into the

italy.

caputo Ô¨Çour that we

import from italy

and then we give it a Ô¨Çip as i ‚Äôve read in Ô¨Çip dough

Ô¨Çip

dough

some, some manuals for italian pizza

that are neapolitan style.

but that ‚Äôs what we do anyway, we

sprinkle the

sprinkle surface

sprinkle the surface

sprinkle the surface and then real quick. surface

we just give a squish with our palm and squish dough squish; dough, with palm; make just, it Ô¨Çat in the

make it Ô¨Çat in the center.

with palm;

Ô¨Çatten center of dough

center

Ô¨Çatten center

we dimple the rest of the pizza, moving dimple the rest dimple; pizza; pizza

move the pizza, around,

the pizza around deÔ¨Ånitely handmade, of pizza; move move

‚ÄúdeÔ¨Ånitely

deÔ¨Ånitely handmade, not trying your

the pizza

handmade,

best not to disturb the edge ‚Äôcause that around

deÔ¨Ånitely

‚Äôs where you ‚Äôre going to get a lot of

handmade‚Äù

natural bubbles from the fermentation.

once you ‚Äôve dimpled, you ‚Äôre going to stretch

stretch; dough; dough

rotate you

do a quick stretch, while you ‚Äôre

dough;rotate

rotate

rotating.

dough

and then we ‚Äôre going to put it, pick it pick up dough pick up; dough; dough,

put;

it; it; it, on the

up and put it on the backs of our hands and let it hang hang on backs of knuckles pick;

backs of our hands;

and just let it kind of hang down on the on backs of

put; let just, it kind of hang

backs of our knuckles.

knuckles

down on the backs

of our knuckles

do you want a quarter to two and a half put cheese on put on cheese

put

the cheese, on

ounces of sauce, and i ‚Äôm going to put

the cheese on.

olive oil actors come out of the oven, put on fresh

put on fresh basil

put

here

and we put on here.

basil

so that ‚Äôs all we do to the pizza and then place pizza in place; pizza, oven; pizza place it, directly on the

we go into the oven just going to place it oven, give it a shake

stone

directly on the stone and give it a shake. shake

so will have to rotate it every thirty to rotate pizza

rotate pizza, 30-45

rotate, it, every thirty to

forty Ô¨Åve seconds, your home oven will every 30-45

seconds

take

forty Ô¨Åve seconds;

take you about ten minutes.

seconds

will, you, about ten

minutes

we ‚Äôre going to put a little bit of extra put extra vigrin put on extra virgin olive put

a little bit of extra

virgin olive oil directly on.

olive oil on

oil

virgin olive oil,

directly, on

yeah , over the whole pizza and then in cut the pizza

cut

pizza

cut, cut in america, it, in; in

america we cut it, but i ‚Äôm told in, uh, in

italy, it, yourself

italy you cut it yourself.

and we start to knead, so we need about knead dough

knead dough, 15-20

though for this Ô¨Åfteen twenty minutes for 15-20

minutes

until we have a very nice texture.

minutes

you make a nice bowl with cover, the make bowl with make; bowl, with cover; make; a nice bowl, with

door to avoid crusting an you let it rise cover, rise

rise

dough, room

let

cover; it rise at

at room temperature for three four hour. dough at room

temperature, 3-4

room temperature

temperature for

hours

for three four hour

3-4 hours

one sourdough, as almost double is

make six small make six small balls

make one sourdough, as

volume we make six small balls.

balls

almost double is

volume, six small

balls

so we make nice round shape now.

make a round make round shape

make so, nice round

shape

shape, now

we cover again with Ô¨Ålm to avoid the door crafting an we elect, leave at room times or for a couple of hour or if you like you can place in a warm place for forty Ô¨Åve minutes around pizza dough is ready.

cover with Ô¨Ålm

cover

and we start making Ô¨Çat with our hands.
now we had a tomato and we spread all over our pizza dough.
we had the mozzarella cheese shredded little basil pizza is ready to be back.
now we should make pizza at around seven hundred Ô¨Åfty degrees.

make the dough Ô¨Çat with hands spread tomato over pizza dough add mozzarella cheese, shredded basil make pizza

make Ô¨Çat spread
add
make

fahrenheit four Ô¨Åve, seven Ô¨Ånish our pizza with basil and fresh olive oil.
OK , well, to make the margarita pizza we ‚Äôre going to start off with by stretching the dough and we do make gardell everyday fresh in our kitchen after that we ‚Äôre going to add some shredded mozzarella cheese.

Ô¨Ånish pizza with basil and olive oil stretch the dough, add shredded mozzarella cheese

Ô¨Ånish
stretch; add

then our version of a margarita pizza has four dollops of tomato sauce, along with some fresh, chopped tomato, then it goes into the wood burning oven, they certainly, we ‚Äôve been cooking with for Ô¨Åfteen years. after the margarita pizza comes out of the oven, we Ô¨Ånish it with some fresh, grated parmesan cheese. here you can Ô¨Ånd how i do it out on the website, and then i ‚Äôve also got my basic classic tomato sauce plus that i use for just about all my pizzas, an i ‚Äôm going to go ahead and lay that down in a, in a nice coat and i like you know, probably frankly i probably like a medium amount of sauce people, people have said little bit less a little bit more i kind of like it a little bit right in the middle use, the back of the spoon there, yeah, you spoon it out with the front, then just kind of use the back spread it out. and then i like to use our leave about half an inch all the way around the pizza. i ‚Äôm going to go ahead and lay this down. and then you can lay some basil down now.

put chopped tomato and tomato sauce into oven
Ô¨Ånish with more cheese, fresh basil lay down tomato sauce
leave half an inch around pizza lay down cheese lay basil down

put
Ô¨Ånish lay down
leave lay down lay down

Ô¨Ålm
dough, with hands tomato, pizza dough mozzarella cheese, shredded basil pizza, 750 degrees
pizza, with basil, olive oil dough; shredded mozzarella cheese
chopped tomato and tomato sauce, into oven
pizza, with more cheese, fresh basil tomato sauce
half an inch leaf, around pizza cheese basil

cover; leave; place
make spread shred make
make; stretch; make; add
oven; cook
lay; spoon; spread
lay lay

again, with Ô¨Ålm, ‚Äúto avoid the door ... pizza dough is ready‚Äù; at room times or for a couple of hour; in a warm place, for forty Ô¨Åve minutes, around Ô¨Çat, with our hands
all, over our pizza dough
pizza
now, should, pizza, at around seven hundred Ô¨Åfty degrees
‚Äúthe margarita pizza we ‚Äôre going to start off with‚Äù, by stretching the dough; the dough; gardell, everyday, fresh, in our kitchen, after that; some shredded mozzarella cheese wood burning; with, for Ô¨Åfteen years
that, down, ‚Äúin a , in a nice coat‚Äù; it, with the front; just, kind of, it
this, down
and, then, can, some basil, down, now

i like to press down a little bit to make sure that it stays down, stays down as it cooks if it stays down it.

press basil down

let ‚Äôs go ahead and pop this in the oven, pop pizza in

so in the interest of full disclosure.

oven

press down
pop

i actually kept it on the pizza stone.

keep it on pizza keep stone

basil pizza, oven pizza, pizza stone

press; make
let; pop
keep

down, a little bit, ‚Äúto make sure ... it stays down it‚Äù; ‚Äúsure ... it stays down it‚Äù ‚Äú‚Äôs go ahead and ... full disclosure‚Äù; this, in the oven, so in the interest of full disclosure actually, it, on the pizza stone

