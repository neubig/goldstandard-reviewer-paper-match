ESPNET2-TTS: EXTENDING THE EDGE OF TTS RESEARCH
Tomoki Hayashi1,2, Ryuichi Yamamoto3, Takenori Yoshimura4, Peter Wu5, Jiatong Shi5, Takaaki Saeki6, Yooncheol Ju7, Yusuke Yasuda2, Shinnosuke Takamichi6, Shinji Watanabe5 1Human Dataware Lab. Co., Ltd., 2Nagoya University, 3LINE Corp., 4Nagoya Institute of Technology, 5Carnegie Mellon University, 6The University of Tokyo, 7AIRS Company, Hyundai Motor Group

arXiv:2110.07840v1 [cs.CL] 15 Oct 2021

ABSTRACT
This paper describes ESPnet2-TTS, an end-to-end text-to-speech (E2E-TTS) toolkit. ESPnet2-TTS extends our earlier version, ESPnet-TTS, by adding many new features, including: on-theﬂy ﬂexible pre-processing, joint training with neural vocoders, and state-of-the-art TTS models with extensions like full-band E2E textto-waveform modeling, which simplify the training pipeline and further enhance TTS performance. The uniﬁed design of our recipes enables users to quickly reproduce state-of-the-art E2E-TTS results. We also provide many pre-trained models in a uniﬁed Python interface for inference, offering a quick means for users to generate baseline samples and build demos. Experimental evaluations with English and Japanese corpora demonstrate that our provided models synthesize utterances comparable to ground-truth ones, achieving state-of-the-art TTS performance. The toolkit is available online at https://github.com/espnet/espnet.
Index Terms— Text-to-speech, end-to-end, open-source, joint training, text-to-waveform
1. INTRODUCTION
Thanks to the improvements of deep learning techniques, endto-end text-to-speech (E2E-TTS) models have replaced the conventional statistical parametric speech synthesis (SPSS) systems based on hidden Markov models (HMMs) [1] and deep neural networks (DNNs) [2]. E2E-TTS models can alleviate complex text pre-processing and learn alignments between input and output sequences in a data-driven manner, achieving high-ﬁdelity speech comparable to professional recordings [3]–[5]. Recent E2E-TTS works have also introduced a range of desirable TTS features. For example, non-autoregressive (NAR) architectures enable us to generate speech faster than real-time [6]–[8]. Additionally, introducing learnable global or ﬁne-grained embeddings allows us to control speaker characteristics [9], [10] or speaking styles [11], [12].
To help researchers accelerate the development of TTS techniques and stay up to date with the state-of-the-art models, we ﬁrst developed an open-source E2E-TTS toolkit called ESPnet-TTS [13], which followed the great success of the speech recognition toolkits Kaldi [14] and ESPnet-ASR [15]. This toolkit provided Chainer [16] and PyTorch [17]-based neural network libraries and highly reproducible recipes. ESPnet-TTS also contributed to many research projects and development platforms for new applications like voice conversion [18], [19]. However, since the toolkit required a fair amount of ofﬂine processing, such as feature extraction and text frontend processing, there existed room for improvement in terms of scalability, ﬂexibility, and portability.
In this paper, we introduce our second generation TTS toolkit named ESPnet2-TTS. Compared to our previous toolkit [13], it includes many new features such as on-the-ﬂy pre-processing, the

Model Zoo, and state-of-the-art models with our own extensions. The contributions of this paper are summarized as follows:
• We introduce the ESPnet2-TTS toolkit, containing new features like a uniﬁed task design, ﬂexible on-the-ﬂy pre-processing, and a simple Python interface to quickly use many pre-trained models in the Model Zoo.
• We introduce our provided state-of-the-art models, including E2E text-to-waveform (T2W) models and joint training with neural vocoders, which simplify the training pipeline and enhance TTS performance. Furthermore, we provide extensions such as the Conformer architecture [20], full-band waveform modeling, and zero-shot adaptation with pre-trained speaker embeddings.
• We conduct experimental evaluations with English and Japanese corpora, and investigate the performance in various settings, including single speaker, multi-speaker, and adaptation. Evaluation results demonstrate that the model can achieve state-of-the-art performance comparable with the ground-truth.
2. RELATED WORKS
We observe a number of E2E-TTS toolkits whose functions are similar to ours, e.g., TensorFlow-TTS1, coqui-ai TTS2, OpenSeq2Seq [21], NeMo [22], and Fairseq S2 [23]. While Tensorﬂow-TTS and coquiai TTS focus on only TTS, the others support several speech processing tasks such as automatic speech recognition (ASR), machine translation (MT), and speech translation (ST). Compared with these toolkits, ESPnet2-TTS provides a number of reproducible recipes, including various languages and scenarios (e.g., single speaker, multi-speaker, and adaptation). This helps researchers quickly test state-of-the-art TTS models in major and minor languages, giving lots of insights for accelerating the development of new TTS research ideas. ESPnet2-TTS also supports various state-of-the-art TTS models, including E2E-T2W models and joint training with neural vocoders. Moreover, various of our own extensions are available, such as the Conformer architecture [24], full-band waveform generation, and zero-shot adaptation with pre-trained speaker embeddings. Furthermore, ESPnet2 supports various speech processing tasks, including ASR, speech enhancement, speech diarization, and self-supervised learning, enabling users to build such models through the same interface as TTS.
3. FEATURES OF ESPNET2-TTS
3.1. ESPnet2
ESPnet2 is a new framework for network training shared between various speech processing tasks. To enhance the scalability, ﬂexibil-
1https://github.com/TensorSpeech/TensorFlowTTS 2https://github.com/coqui-ai/TTS

ity and portability of our previous toolkit [13], ESPnet2 introduces multiple new features, as described below.
Uniﬁed task design Inspired by FairSeq [25], ESPnet2 has a uniﬁed task interface to deﬁne new speech processing tasks quickly. The task interface abstracts the data loader and the training iterations to handle complex task conﬁgurations. Speciﬁcally, this makes it easy to build a complicated model requiring various inputs and performing multi-node multi-GPU training for a more extensive network.
On-the-ﬂy ESPnet2 can perform pre-processing in an on-the-ﬂy manner during training. Pre-processing features include text cleaning, grapheme-to-phoneme (G2P) conversion, acoustic feature extraction, and data augmentation, and each pre-processing step can be easily customized (e.g., adding a new G2P function). The on-theﬂy pre-processing allows launching multiple training jobs without creating sizeable temporary ﬁles for each setting and makes it easy to deploy the model with a simple interface.
Model Zoo Inspired by Asteroid [26] and HuggingFace transformers [27], ESPnet2 has a Model Zoo that provides quick access to pre-trained models so that users can utilize pre-trained models with just a few lines of code.
3.2. Recipes
Based on a Kaldi-style ﬁle structure, our recipes are based on shell scripts that perform all the steps required to reproduce the results. To reduce maintenance costs and accelerate the creation of new recipes, ESPnet2 provides a template shared across all recipes. Namely, users can create a recipe by just modifying the script for the data preparation stage since the code for the other stages is the same for all recipes. ESPnet2-TTS provides 20+ recipes that cover 10+ languages, including single-speaker, multi-speaker, multi-language, and adaptation use cases.
3.3. Models
ESPnet2-TTS supports not only standard text-to-mel (T2M) and mel-to-waveform (M2W) models, but also state-of-the-art models such as E2E-T2W with our own extensions.
T2M models T2M models generate mel spectrograms from input text representations (e.g., phonemes). ESPnet2-TTS supports two autoregressive (AR) T2M models, Tacotron 2 [4] and TransformerTTS [5], and two non-autoregressive (NAR) T2M models, FastSpeech [6] and FastSpeech 2 [8] extended with techniques from FastPitch [28]. The extended FastSpeech 2 predicts token-averaged energy and pitch sequences instead of raw pitch and energy sequences. Furthermore, we extend these NAR models with the Conformer [20] architecture, dubbed Conformer-FastSpeech and Conformer-FastSpeech 2 [24]. For AR models, we use guided attention loss to help to learn diagonal attentions [29]. For NAR models, we use the duration of each input token calculated from the attention weights of AR models [6]. Although this requires building the AR models before training NAR models, we do not need to prepare the external aligner and use any input representations. Furthermore, we provide multi-speaker extensions using one-hot speaker embeddings, pre-trained X-vectors [30], and global style tokens (GSTs) [11] for all T2M models.
M2W models M2W models, also known as (neural) vocoders, generate waveforms from mel spectrograms. For M2W modelling, we support the Grifﬁn-Lim vocoder and NAR neural vocoders using generative adversarial networks (GANs). The latter includes

Parallel WaveGAN [31], MelGAN [32], StyleMelGAN [33], HiFiGAN [34], and their multi-band extensions [35]. Since GAN-based models consist of generators and discriminators, we provide support for training arbitrary combinations of generators and discriminators. Joint-T2W models ESPnet2-TTS provides a function to jointly train T2M and M2W models, which we refer to as the joint textto-waveform (Joint-T2W) model. For Joint-T2W models, we adopt a GAN-based training strategy using random windowed discriminators (RWDs) [36]. The Joint-T2W models are optimized to minimize the sum of the T2M model loss, calculated using whole sequence, and the M2W model loss, calculated from randomly windowed subsequences. The use of subsequences for the M2W loss alleviates the issue of huge memory consumption due to the length of the waveforms [37]. We can combine an arbitrary T2M model with an arbitrary neural M2W one. Also, we can use this joint training not only for ﬁne-tuning but also for training from scratch. This joint training can simplify the training scheme of TTS models and enhance their performance. E2E-T2W models E2E-T2W models perform TTS in a truly endto-end fashion, generating waveforms from input text representations. For E2E-T2W models, we support VITS [38], which utilizes a conditional variational autoencoder (CVAE) with normalizing ﬂows and GAN-based optimizations. We further extend VITS to make it possible to use the Conformer architecture for the text encoder, generate a full-band waveform (44.1 kHz), and perform zero-shot adaptation with pre-trained X-vectors.
3.4. Evaluation
To evaluate TTS model performance, ESPnet2-TTS provides three objective evaluation metrics: Mel-cepstral distortion (MCD), log-F0 root mean square error (F0 RMSE), and character error rate (CER) with pre-trained ESPnet2-ASR models. MCD and F0 RMSE reﬂect speaker, prosody, and phonetic content similarities, and CER can reﬂect the intelligibility of generated speech. For MCD and F0 RMSE, we apply dynamic time-warping (DTW) [39] to match the length difference between ground-truth speech and generated speech. While these objective metrics can estimate the quality of synthesized speech, it is still difﬁcult to fully determine human perceptual quality from these values, especially with high-ﬁdelity generated speech. Hence, we also provide instructions for subjective evaluations with mean opinion scores (MOS) based on webMUSHRA [40]. These instructions help researchers quickly launch a web-based subjective evaluation system and collect subjects through crowdsourcing services like Amazon Mechanical Turk.
4. EXPERIMENTAL EVALUATION
To study the performance of our TTS models, we conducted experimental evaluations with English and Japanese corpora. The objective evaluation metrics were MCD, F0 RMSE, and CER. The subjective evaluation metric was MOS for naturalness with a 5-point scale: 5 for excellent, 4 for good, 3 for fair, 2 for poor, and 1 for bad.
We made the pretrained weights and conﬁgurations of all models public for reproducibility. Audio samples are available online3.
4.1. English single speaker
First, we evaluated the performance of English single-speaker models with the LJSpeech dataset [41], which consists of 24 hours of
3https://espnet.github.io/icassp2022-tts

Table 1. Results on LJSpeech corpus, where “STD” represents standard deviation and “CI” represents 95 % conﬁdence intervals.

Method

MCD ± STD F0 RMSE ± STD CER MOS ± CI

GT Transformer [13]

N/A 6.97 ± 0.79

N/A 0.252 ± 0.042

1.1 4.15 ± 0.08 2.7 3.86 ± 0.08

CFS2 CFS2 (+ft) CFS2 (+joint-ft) CFS2 (+joint-tr) VITS

6.47 ± 0.58 6.51 ± 0.58 6.73 ± 0.62 6.80 ± 0.54 6.84 ± 0.65

0.214 ± 0.031 0.217 ± 0.032 0.221 ± 0.032 0.218 ± 0.032 0.232 ± 0.033

1.2 3.53 ± 0.09 1.2 4.00 ± 0.07 1.5 4.03 ± 0.07 1.5 3.92 ± 0.08 2.7 3.88 ± 0.08

The

I

The

F

B

I

(a) Generated spectrogram by VITS. (b) Generated spectrogram by CFS2.
Fig. 1. Generated example using the wrong G2P result. The input text is “The FBI” and g2p-en output is “DH AH0 B AY1”, which lacked phonemes corresponding to the character “F”.
speech recorded with 16 bits and a 22.05 kHz sampling rate. We followed the recipe in egs2/ljspeech/tts1, using 12,600 utterances for training, 250 for validation, and 250 for evaluation. Also, we used g2p-en4 without word separators as the G2P function. We compared the following models: Transformer [13] Our previous best model in [13], which con-
sists of Transformer-TTS and the mixture of logistics WaveNet vocoder [4]. To enhance the perceptual quality, the noise shaping technique was used to mask noise in the high frequency band [42].
CFS2 Conformer-FastSpeech2 (CFS2) + HiFi-GAN. Each of these parts was trained separately. The duration of each token was calculated from a Tacotron 2 teacher model.
CFS2 (+ft) Same as the above combination, but HiFi-GAN was ﬁne-tuned with ground-truth aligned outputs generated by CFS2.
CFS2 (+joint-ft) Same as the above combination, but the two networks were jointly ﬁne-tuned.
CFS2 (+joint-tr) Same as the above combination, but the two networks were jointly trained from scratch.
VITS The end-to-end text-to-waveform model VITS. We used 0.333 of the scaling factor for the standard deviation of the stochastic duration predictor and the prior distribution.
For the CER calculation, we used an ESPnet2-ASR pre-trained model5 trained on the LibriSpeech dataset [43]. Since the ASR model assumed that its speech inputs had sample rates of 16 kHz, we downsampled audio to 16 kHz before using the ASR model. For subjective evaluation, we randomly selected 60 utterances from the evaluation set and had 37 English speakers as listeners.
The experimental evaluation results are shown in Table 1. Our new best model outperformed our previous study’s best model, achieving comparable intelligibility and naturalness with the groundtruth. Moreover, since CFS2 and VITS consist of NAR architectures, they can perform inference much faster than our previous model, which requires slow AR computations. With joint training, CFS2 (+joint-tr) solves the mismatch of acoustic features between training and evaluation and signiﬁcantly outperforms CFS2 without joint training in terms of naturalness. The ﬁne-tuning of HiFi-GAN worked well, and joint ﬁne-tuning helped further reduce the metallic
4https://github.com/Kyubyong/g2p 5https://zenodo.org/record/4030677

Table 2. Results with different G2P functions on LJSpeech corpus, where “STD” represents standard deviation.

Method

MCD ± STD F0 RMSE ± STD CER

CFS2 (g2p-en)

6.47 ± 0.58 0.214 ± 0.031 1.2

CFS2 (espeak-ng) 6.51 ± 0.62 0.216 ± 0.033 1.1

VITS (g2p-en)

6.84 ± 0.65 0.232 ± 0.033 2.7

VITS (espeak-ng) 6.70 ± 0.58 0.228 ± 0.037 1.6

noise appearing in fricative consonant sounds such as /s/ and /z/, improving the naturalness slightly more. With VITS, there is a slight gap between the ground truth and the synthesized speech for both intelligibility and naturalness. We found that one of the reasons for this gap was the mispronunciation of words that included G2P errors. Fig. 1 shows an example of the generated spectrogram when the input contains G2P errors. Interestingly, CFS2 can generate correct pronunciations even with the wrong G2P results; however, VITS misses some characters. This implies that CFS2 can recover from incorrect G2P results through training thanks to the soft alignment derived from the teacher AR model. In contrast, VITS is more sensitive to G2P errors because of the hard monotonic alignment. To check this hypothesis, we investigated the performance when using a slower but more accurate G2P function (espeak-ng6) and the results are shown in Table 2. From these results, using a better G2P function improved the intelligibility of VITS while CFS2 had a similar intelligibility, reinforcing our hypothesis.

4.2. English multi-speaker
Next, we investigated the performance of multi-speaker VITS with two types of speaker embeddings using the VCTK dataset [44]. We followed the recipe in egs2/vctk/tts1 for pre-processing and training. To evaluate the models in seen and unseen speaker conditions, we randomly selected four speakers (two males and two females) as the seen speaker evaluation set and another four as the unseen speaker one. The number of evaluation utterances per speaker was set to 10 so that the total number of utterances was 40 for each condition. We downsampled audio to 22.05 kHz and used espeak-ng as the G2P function, which includes phoneme and stress symbols. We compared the following models: SID-VITS VITS with one-hot speaker ID (SID) embeddings. Since
this model cannot deal with unknown speakers, we trained it with all of the speakers.
X-VITS (Avg.) VITS with pre-trained X-vectors instead of one-hot speaker ID embeddings. This model was trained with all speakers except for the evaluation ones in the unseen speaker condition. For inference, we used X-vectors averaged over all the utterances of the target speaker except for the evaluation utterances.
X-VITS (Ran.) The same as the above model except it used Xvectors extracted from a single utterance of the target speaker. This datapoint was randomly selected from all utterances of the speaker excluding the evaluation utterances.
For CER computation, we used the same ASR model as the one described in Section 4.1. In total, 27 English speakers participated in the subjective evaluation listening test.
Tables 3 and 4 show the evaluation results for seen and unseen speaker conditions, respectively. In the former, both VITS models achieved naturalness comparable to the ground-truth, and using Xvectors extracted with more utterances improved speaker similarity since it yielded lower MCD and F0 RMSE. The results from the lat-
6https://github.com/espeak-ng/espeak-ng

Table 3. Results of the seen speaker condition on VCTK corpus, where “STD” represents standard deviation and “CI” represents 95 % conﬁdence intervals.

Method

MCD ± STD F0 RMSE ± STD CER MOS ± CI

GT

N/A

N/A

3.9 4.03 ± 0.08

SID-VITS X-VITS (Avg.) X-VITS (Ran.)

6.30 ± 0.82 6.35 ± 0.91 6.95 ± 1.09

0.242 ± 0.110 0.257 ± 0.119 0.289 ± 0.123

5.5 4.00 ± 0.08 6.3 3.99 ± 0.08 6.9 3.96 ± 0.08

Table 4. Results of the unseen speaker condition on VCTK corpus, where “STD” represents standard deviation and “CI” represents 95 % conﬁdence interval. Note that SID-VITS used all speakers.

Method

MCD ± STD F0 RMSE ± STD CER MOS ± CI

GT

N/A

N/A

2.8 4.04 ± 0.08

SID-VITS X-VITS (Avg.) X-VITS (Ran.)

6.91 ± 1.48 7.41 ± 0.83 7.89 ± 1.14

0.275 ± 0.101 0.274 ± 0.110 0.289 ± 0.128

7.0 3.92 ± 0.08 4.9 4.04 ± 0.08 8.0 3.97 ± 0.08

ter show that even for unknown speakers, X-VITS can generate natural speech and improve speaker similarity by using more reference utterances from the target speaker.

4.3. Japanese single speaker
Next, we evaluated the performance of Japanese single-speaker models with the JSUT corpus [45], which consists of 10 hours of speech recorded with 16 bits and a 48 kHz sampling rate. We followed the recipe in egs2/jsut/tts1, using 7,196 utterances for training, 250 for validation, and 250 for evaluation. We used the G2P function based on Open JTalk enhanced with prosody symbols [46] for all models. We compared the following architectures: Tacotron 2 Tacotron 2 + HiFi-GAN. Each model was separately
trained with a sampling rate of 24 kHz.
Transformer Transformer-TTS + HiFi-GAN. Each model was separately trained with a sampling rate of 24 kHz.
CFS2 Conformer-FastSpeech2 + HiFi-GAN. Each model was separately trained with a sampling rate of 24 kHz.
CFS2 (+ft) Same as the above model, but HiFi-GAN was ﬁne-tuned with ground-truth aligned mel spectrograms.
VITS VITS trained with a sampling rate of 22.05 kHz.
FB-VITS Full-band VITS trained with a sampling rate of 44.1 kHz. To calculate CER, we used an ESPnet2-ASR pre-trained model7 trained on the Corpus of Spontaneous Japanese (CSJ) [47]. For subjective evaluation, we randomly selected 60 utterances from the evaluation set. We also downsampled from a frequency of 24 kHz to 22.05 kHz in order to match model sampling rate assumptions. In total, 37 Japanese native speakers evaluated these models.
Table 5 contains the evaluation results. While CFS2 achieves the best intelligibility, VITS produces the best naturalness, comparable to the ground-truth. Interestingly, adding the full-band extension did not improve naturalness. However, since we observed that the listening equipment might affect the evaluation, we recommend listening to our samples and comparing the difference between 22.05 kHz and 44.1 kHz ones.

4.4. Japanese single speaker adaptation
Finally, we investigate the performance of VITS in speaker adaptation with a small amount of training data from the JVS corpus [48].
7https://zenodo.org/record/4037458

Table 5. Results on JSUT corpus, where “STD” represents standard deviation, “CI” represents 95 % conﬁdence interval, and “∗” represents different analysis conditions due to the sampling rate.

Method

MCD ± STD F0 RMSE ± STD CER MOS ± CI

GT (22k)

N/A

GT (44k)

N/A

N/A

6.0 4.02 ± 0.08

N/A

6.0 4.02 ± 0.08

Tacotron 2 Transformer CFS2 CFS2 (+ft) VITS
FB-VITS

6.62 ± 0.60 6.58 ± 0.62 6.26 ± 0.57 6.34 ± 0.57 6.37 ± 0.55
6.24 ± 0.38∗

0.177 ± 0.036 0.179 ± 0.038 0.158 ± 0.034 0.158 ± 0.032 0.157 ± 0.033
0.158 ± 0.031∗

7.0 3.54 ± 0.09 7.2 3.58 ± 0.09 6.3 3.79 ± 0.08 6.3 3.86 ± 0.08 6.9 4.00 ± 0.08
7.2 4.00 ± 0.08

Table 6. Evaluation results of VITS adaptation with JVS corpus, where “STD” represents standard deviation and “CI” represents 95 % conﬁdence interval. The “jvs001” and “jvs054” are male speakers, and the rest are female ones.

Method

MCD ± STD F0 RMSE ± STD CER MOS ± CI

GT (jvs001) VITS (jvs001)

N/A 5.75 ± 0.65

N/A 0.210 ± 0.055

7.2 4.73 ± 0.06 7.5 3.37 ± 0.13

GT (jvs054) VITS (jvs054)

N/A 5.30 ± 0.40

N/A 0.257 ± 0.042

5.9 4.49 ± 0.09 8.8 3.44 ± 0.13

GT (jvs010) VITS (jvs010)

N/A 5.89 ± 0.39

N/A 0.144 ± 0.026

4.3 3.91 ± 0.12 3.8 3.31 ± 0.13

GT (jvs092) VITS (jvs092)

N/A 5.49 ± 0.50

N/A 0.140 ± 0.034

7.3 4.05 ± 0.11 6.7 3.45 ± 0.11

We followed the recipe in egs2/jvs/tts1 and selected four speakers (two males and two females), using 100 utterances for adaptation and 30 for evaluation. For the pre-trained model, we used VITS trained on JSUT with 22.05 kHz audio. For the CER calculation, we used the same model as the one from Section 4.3. In total, 28 Japanese native speakers participated in the subjective evaluation.
Table 6 shows the evaluation results. We ﬁnd that adaptation with a small amount of training data also works for VITS, achieving comparable intelligibility and reasonable naturalness. Though there is a gap between the ground-truth in terms of naturalness, thanks to the E2E-T2W architecture, we do not need to perform adaptation with the T2M and M2W models separately. This makes the steps to perform adaption more straightforward than those for conventional E2E-TTS models that consist of T2M and M2W models. We observe that the naturalness gap was larger for male speakers than female ones. This is because the speaking style of males speakers is more emotional than that of females here, which can be also conﬁrmed from the scores of their ground-truth utterances.

5. SUMMARY
This paper introduces ESPnet2-TTS, an E2E-TTS toolkit extending ESPnet-TTS. ESPnet2-TTS enhances the ﬂexibility, scalability, and portability of ESPnet-TTS and provides new state-of-the-art TTS models and our own extensions of them. The experimental results demonstrate that our models can achieve TTS performance comparable to the ground-truth in both single-speaker and multi-speaker settings except for adaptation on small data.
In the future, we will work on training with noisy datasets, which include bad quality recordings, background noise, and transcription errors, as well as E2E speech-to-speech translation.

6. REFERENCES
[1] K. Tokuda, Y. Nankaku, T. Toda, et al., “Speech synthesis based on hidden Markov models,” Proceedings of the IEEE, vol. 101, no. 5, pp. 1234–1252, 2013.
[2] H. Zen and A. Senior, “Deep mixture density networks for acoustic modeling in statistical parametric speech synthesis,” in Proc. ICASSP, 2014, pp. 3844–3848.
[3] Y. Wang, R. Skerry-Ryan, D. Stanton, et al., “Tacotron: Towards endto-end speech synthesis,” in Proc. Interspeech, 2017, pp. 4006–4010.
[4] J. Shen, R. Pang, R. J. Weiss, et al., “Natural TTS synthesis by conditioning WaveNet on mel spectrogram predictions,” in Proc. ICASSP, 2018, pp. 4779–4783.
[5] N. Li, S. Liu, Y. Liu, et al., “Close to human quality TTS with Transformer,” arXiv preprint arXiv:1809.08895, 2018.
[6] Y. Ren, Y. Ruan, X. Tan, et al., “FastSpeech: Fast, robust and controllable text to speech,” in Proc. NeurIPS, 2019, pp. 3165–3174.
[7] K. Peng, W. Ping, Z. Song, et al., “Non-autoregressive neural text-tospeech,” in Proc. ICML, 2020, pp. 7586–7598.
[8] Y. Ren, C. Hu, X. Tan, et al., “FastSpeech 2: Fast and high-quality end-to-end text to speech,” in Proc. ICLR, 2021.
[9] Y. Jia, Y. Zhang, R. Weiss, et al., “Transfer learning from speaker veriﬁcation to multispeaker text-to-speech synthesis,” in Proc. NeurIPS, 2018, pp. 4480–4490.
[10] M. Chen, X. Tan, Y. Ren, et al., “MultiSpeech: Multi-speaker text to speech with Transformer,” arXiv preprint arXiv:2006.04664, 2020.

[25] M. Ott, S. Edunov, A. Baevski, et al., “Fairseq: A fast, extensible toolkit for sequence modeling,” in Proc. NAACL-HLT demonstration, 2019.
[26] M. Pariente, S. Cornell, J. Cosentino, et al., “Asteroid: The PyTorchbased audio source separation toolkit for researchers,” in Proc. Interspeech, 2020, pp. 2637–2641.
[27] T. Wolf, L. Debut, V. Sanh, et al., “Transformers: State-of-the-art natural language processing,” in Proc. EMNLP demonstration, 2020, pp. 38–45.
[28] A. Lan´cucki, “FastPitch: Parallel text-to-speech with pitch prediction,” in Proc. ICASSP, 2021, pp. 6588–6592.
[29] H. Tachibana, K. Uenoyama, and S. Aihara, “Efﬁciently trainable text-to-speech system based on deep convolutional networks with guided attention,” in Proc. ICASSP, 2018, pp. 4784–4788.
[30] D. Snyder, D. Garcia-Romero, G. Sell, et al., “X-vectors: Robust DNN embeddings for speaker recognition,” in Proc. ICASSP, 2018, pp. 5329–5333.
[31] R. Yamamoto, E. Song, and J.-M. Kim, “Parallel WaveGAN: A fast waveform generation model based on generative adversarial networks with multi-resolution spectrogram,” in Proc. ICASSP, 2020, pp. 6199–6203.
[32] K. Kumar, R. Kumar, T. de Boissiere, et al., “MelGAN: Generative adversarial networks for conditional waveform synthesis,” in Proc. NeurIPS, 2019, pp. 14 881–14 892.
[33] A. Mustafa, N. Pia, and G. Fuchs, “StyleMelGAN: An efﬁcient highﬁdelity adversarial vocoder with temporal adaptive normalization,” in Proc. ICASSP, 2021, pp. 6034–6038.

[11] Y. Wang, D. Stanton, Y. Zhang, et al., “Style tokens: Unsupervised style modeling, control and transfer in end-to-end speech synthesis,” in Proc. ICML, 2018, pp. 5180–5189.

[34] J. Kong, J. Kim, and J. Bae, “HiFi-GAN: Generative adversarial networks for efﬁcient and high ﬁdelity speech synthesis,” arXiv preprint arXiv:2010.05646, 2020.

[12] G. Sun, Y. Zhang, R. J. Weiss, et al., “Fully-hierarchical ﬁnegrained prosody modeling for interpretable speech synthesis,” in Proc. ICASSP, 2020, pp. 6264–6268.

[35] G. Yang, S. Yang, K. Liu, et al., “Multi-band MelGAN: Faster waveform generation for high-quality text-to-speech,” in Proc. SLT, 2021, pp. 492–498.

[13] T. Hayashi, R. Yamamoto, K. Inoue, et al., “ESPnet-TTS: Uniﬁed, reproducible, and integratable open source end-to-end text-to-speech toolkit,” in Proc. ICASSP, 2020, pp. 7654–7658.

[36] M. Bin´kowski, J. Donahue, S. Dieleman, et al., “High ﬁdelity speech synthesis with adversarial networks,” arXiv preprint arXiv:1909.11646, 2019.

[14] D. Povey, A. Ghoshal, G. Boulianne, et al., “The Kaldi speech recognition toolkit,” in Proc. ASRU, 2011.
[15] S. Watanabe, T. Hori, S. Karita, et al., “ESPnet: End-to-end speech processing toolkit,” arXiv preprint arXiv:1804.00015, 2018.
[16] S. Tokui, R. Okuta, T. Akiba, et al., “Chainer: A deep learning framework for accelerating the research cycle,” in Proc. KDD, 2019, pp. 2002–2011.
[17] A. Paszke, S. Gross, F. Massa, et al., “PyTorch: An imperative style, high-performance deep learning library,” in Proc. NeurIPS, 2019, pp. 8026–8037.
[18] Y. Zhao, W.-C. Huang, X. Tian, et al., “Voice Conversion Challenge 2020: Intra-lingual semi-parallel and cross-lingual voice conversion,” arXiv preprint arXiv:2008.12527, 2020.

[37] N. Chen, Y. Zhang, H. Zen, et al., “WaveGrad 2: Iterative reﬁnement for text-to-speech synthesis,” in Proc. Interspeech, 2021, pp. 3765– 3769.
[38] J. Kim, J. Kong, and J. Son, “Conditional variational autoencoder with adversarial learning for end-to-end text-to-speech,” arXiv preprint arXiv:2106.06103, 2021.
[39] S. Salvador and P. Chan, “Toward accurate dynamic time warping in linear time and space,” Intelligent Data Analysis, vol. 11, no. 5, pp. 561–580, 2007.
[40] M. Schoefﬂer, S. Bartoschek, F.-R. Sto¨ter, et al., “webMUSHRA – A comprehensive framework for web-based listening tests,” Journal of Open Research Software, vol. 6, no. 1, 2018.
[41] K. Ito, The LJ speech dataset, https://keithito.com/LJSpeech-Dataset/, 2017.

[19] W.-C. Huang, T. Hayashi, Y.-C. Wu, et al., “Voice Transformer network: Sequence-to-sequence voice conversion using Transformer with text-to-speech pretraining,” arXiv preprint arXiv:1912.06813, 2019.
[20] A. Gulati, J. Qin, C.-C. Chiu, et al., “Conformer: Convolutionaugmented Transformer for speech recognition,” arXiv preprint arXiv:2005.08100, 2020.
[21] O. Kuchaiev, B. Ginsburg, I. Gitman, et al., “OpenSeq2Seq: Extensible toolkit for distributed and mixed precision training of sequenceto-sequence models,” in Proc. NLP-OSS, 2018, pp. 41–46.

[42] K. Tachibana, T. Toda, Y. Shiga, et al., “An investigation of noise shaping with perceptual weighting for WaveNet-based speech generation,” in Proc. ICASSP, 2018, pp. 5664–5668.
[43] V. Panayotov, G. Chen, D. Povey, et al., “LibriSpeech: An ASR corpus based on public domain audio books,” in Proc. ICASSP, 2015, pp. 5206–5210.
[44] J. Yamagishi, C. Veaux, K. MacDonald, et al., CSTR VCTK corpus: English multi-speaker corpus for CSTR voice cloning toolkit (version 0.92), 2019.
[45] R. Sonobe, S. Takamichi, and H. Saruwatari, “JSUT corpus: Free

[22] O. Kuchaiev, J. Li, H. Nguyen, et al., “NeMo: A toolkit for building

large-scale Japanese speech corpus for end-to-end speech synthesis,”

AI applications using neural modules,” arXiv preprint arXiv:1909.09577,

arXiv preprint arXiv:1711.00354, 2017.

2019.

[46] K. Kurihara, N. Seiyama, and T. Kumano, “Prosodic features con-

[23] C. Wang, W.-N. Hsu, Y. Adi, et al., “Fairseq Sˆ2: A scalable and in-

trol by symbols as input of sequence-to-sequence acoustic modeling

tegrable speech synthesis toolkit,” arXiv preprint arXiv:2109.06912,

for neural TTS,” IEICE Transactions on Information and Systems,

2021.

vol. E104.D, no. 2, pp. 302–311, 2021.

[24] P. Guo, F. Boyer, X. Chang, et al., “Recent developments on ESPnet toolkit boosted by Conformer,” in Proc. ICASSP, 2021, pp. 5874– 5878.

[47] K. Maekawa, “Corpus of spontaneous Japanese: Its design and evaluation,” in Proceedings of SSPR, 2003, pp. 7–12.
[48] S. Takamichi, K. Mitsui, Y. Saito, et al., “JVS corpus: Free Japanese multi-speaker voice corpus,” arXiv preprint arXiv:1908.06248, 2019.

