Decision-Dependent Risk Minimization in Geometrically Decaying Dynamic Environments
Mitas Ray∗†, Dmitriy Drusvyatskiy‡, Maryam Fazel†, Lillian J. Ratliﬀ†
April 19, 2022
Abstract
This paper studies the problem of expected loss minimization given a data distribution that is dependent on the decision-maker’s action and evolves dynamically in time according to a geometric decay process. Novel algorithms for both the information setting in which the decision-maker has a ﬁrst order gradient oracle and the setting in which they have simply a loss function oracle are introduced. The algorithms operate on the same underlying principle: the decision-maker repeatedly deploys a ﬁxed decision over the length of an epoch, thereby allowing the dynamically changing environment to suﬃciently mix before updating the decision. The iteration complexity in each of the settings is shown to match existing rates for ﬁrst and zero order stochastic gradient methods up to logarithmic factors. The algorithms are evaluated on a “semi-synthetic” example using real world data from the SFpark dynamic pricing pilot study; it is shown that the announced prices result in an improvement for the institution’s objective (target occupancy), while achieving an overall reduction in parking rates.
1 Introduction
Traditionally, supervised machine learning algorithms are trained based on past data under the assumption that the past data is representative of the future. However, machine learning algorithms are increasingly being used in settings where the output of the algorithm changes the environment and hence, the data distribution. Indeed, online labor markets (Anagnostopoulos et al., 2018; Horton, 2010), predictive policing (Lum and Isaac, 2016), on-street parking (Dowling et al., 2020; Pierce and Shoup, 2018), and vehicle sharing markets (Banerjee et al., 2015) are all examples of real-world settings in which the algorithm’s decisions change the underlying data distribution due to the fact that the algorithm interacts with strategic users.
To address this problem, the machine learning community introduced the problem of performative prediction which models the data distribution as being decision-dependent thereby accounting for feedback induced distributional shift (Brown et al., 2020; Drusvyatskiy and Xiao, 2020; Mendler-Du¨nner et al., 2020; Miller et al., 2021; Perdomo et al., 2020). With the exception of (Brown et al., 2020), this work has focused on static environments.
In many of the aforementioned application domains, however, the underlying data distribution also may have memory or even be changing dynamically in time. When a decision-making mechanism is announced it may take time to see the full eﬀect of the decision as the environment and strategic data sources respond given their prior history or interactions.
For example, many municipalities announce quarterly a new quasi-static set of prices for on-street parking. In this scenario, the institution may adjust parking rates for certain blocks in order to to achieve a desired occupancy range to reduce cruising phenomena and increase business district vitality (Dowling et al., 2017; Fiez et al., 2018; Pierce and Shoup, 2013; Shoup, 2006). For instance, in high traﬃc areas, the institution may
∗mitasray@uw.edu †Department of Electrical and Computer Engineering, University of Washington, Seattle ‡Department of Mathematics, University of Washington, Seattle
1

announce increased parking rates to free up parking spots and redistribute those drivers to less populated blocks. However, upon announcing a new price, the population may react slowly, whether it be from initially being unaware of the price change, to facing natural inconveniences from changing one’s parking routine. This introduces dynamics into our setting; hence, the data distribution takes time to equilibrate after the pricing change is made.
Motivated by such scenarios, we study the problem of decision-dependent risk minimization (or, synonymously, performative prediction) in dynamic settings wherein the underlying decision-dependent distribution evolves according to a geometrically decaying process. Taking into account the time it takes for a decision to have the full eﬀect on the environment, we devise an algorithmic framework for ﬁnding the optimal solution in settings where the decision maker has access to diﬀerent types of gradient information.
For both information settings (gradient access and loss function access, via the appropriate oracle), the decision-maker deploys the current decision repeatedly for the duration of an epoch, thereby allowing the dynamically evolving distribution to approach the ﬁxed point distribution for that announced decision. At the end of the epoch, the decision is updated using a ﬁrst-order or zeroth-order oracle.
One interpretation of this procedure is that the environment is operating on a faster timescale compared to the update of the decision-maker’s action. For instance, consider the dynamically changing distribution as the data distribution corresponding to a population of strategic data sources. The phase during which the same decision is deployed for a ﬁxed number of steps can be interpreted as the population of agents adapting at a faster rate than the update of the decision. This in fact occurs in many practical settings such as on-street parking, wherein prices and policies more generally are quasi-static, meaning they are updated infrequently relative to actual curb space utilization.
1.1 Contributions
For the decision-dependent learning problem in geometrically decaying environments, we propose ﬁrst-order or zeroth-order oracle algorithms that converge to the optimal point under appropriate assumptions, which make the risk minimization problem strongly convex. We obtain the following iteration complexity guarantees:
• Zero Order Oracle (Algorithm 1, Section 3): We show that the sample complexity in the zeroth order setting is O˜( dε22 ) which matches the optimal rate for single query zeroth order methods in strongly convex settings up to logarithmic factors.
• First Order Oracle (Algorithm 2, Section 3): We show that the same complexity in the ﬁrst order setting is O˜( 1ε ) again matching the known rates for ﬁrst order stochastic gradient methods up to logarithmic factors.
The technical novelty arises from bounding the error between the expected gradient at the ﬁxed point distribution corresponding to the current decision and the stochastic gradient at the current distribution at time t.
The algorithms are applied to a set of semi-synthetic experiments using real data from the SFpark pilot study on the use of dynamic pricing to manage curbside parking (Section 4). The experiments demonstrate that optimizing taking into consideration feedback-induced distribution shift even in a dynamic environment leads to the institution—and perhaps surprisingly, the user as well—experiencing lower expected cost. Moreover, there are important secondary eﬀects of this improvement including increased access to parking—hence, business district vitality—and reduced circling for parking and congestion which not only saves users time, but also reduces carbon emissions (Shoup, 2006).
A more comprehensive set of experiments is contained in Appendix D, including purely synthetic simulations and other semi-synthetic simulations using the ‘Give Me Some Credit’ data set from Kaggle (2011).
2

1.2 Related work
Dynamic Decision-Dependent Optimization. As hinted above, dynamic decision-dependent optimization has been considered quite extensively in the stochastic optimization literature wherein the problem of recourse arises due to decision-makers being able to make a secondary decision after some information has been revealed (Goel and Grossmann, 2004; Jonsbr˚aten et al., 1998; Varaiya and Wets, 1988). In this problem, the goal of the institution is to solve a multi-stage stochastic program, in which the probability distribution of the population is a function of the decision announced by the institution. This multi-stage procedure models a dynamic process. Unlike the setting considered in this paper, the institution has the ability to make a recourse decision upon observing full or partial information about the stochastic components.
Reinforcement Learning. Reinforcement learning is a more closely related problem in the sense that a decision is being made over time where the environment dynamically changes as a function of the state and the decision-maker’s actions (Sutton and Barto, 2018). A subtle but important diﬀerence is that the setting we consider is such that the decision maker’s objective is to ﬁnd the action which optimizes the decisiondependent expected risk at the ﬁxed point distribution (cf. Deﬁnition 1, Section 2) induced by the optimal action and the environment dynamics. This is in contrast to ﬁnding a policy which is a state-dependent distribution over actions given an accumulated cost over time. Our setting can be viewed as a special case of the general reinforcement learning problem, however with additional structure that is both practically wellmotivated, and beneﬁcial to exploit in the design and analysis of algorithms. More concretely, we crucially exploit the assumed model of environment dynamics (in this case, the geometric decay), the distribution dependence, and convexity to obtain strong convergence guarantees for the algorithms proposed herein.
Performative prediction. As alluded to in the introductory remarks, the most closely related body of literature is on performative prediction wherein the decision-maker or optimizer takes into consideration that the underlying data distribution depends on the decision. A na¨ıve strategy is to re-train the model after using heuristics to determine when there is suﬃcient distribution shift. Under the guise that if retraining is repeated, eventually the distribution will stabilize, early works on performative prediction—such as the works of Perdomo et al. (2020) and Mendler-Du¨nner et al. (2020)—studied this equilibrium notion, and called these points performatively stable. Mendler-Du¨nner et al. (2020) and Drusvyatskiy and Xiao (2020) study stochastic optimization algorithms applied to the performative prediction problem and recover optimal convergence guarantees to the performatively stable point. Yet, performatively stable points may diﬀer from the optimal solution of the decision-dependent risk minimization problem as was shown in Perdomo et al. (2020). Taking this gap between stable and optimal points into consideration, Miller et al. (2021) characterize when the performative prediction problem is strongly convex, and devise a two-stage algorithm for ﬁnding the so-called performatively optimal solution—that is, the optimal solution to the decision-dependent risk minimization problem—when the decision-dependent distribution is from the location-scale family.
None of the aforementioned works consider dynamic environments. Brown et al. (2020) is the ﬁrst paper, to our knowledge, to investigate the dynamic setting for performative prediction. Assuming regularity properties of the dynamics, they show that classical retraining algorithms (repeated gradient descent and repeated risk minimization) converge to the performatively stable point of the expected risk at the corresponding ﬁxed point distribution. Counter to this, in this paper we propose algorithms for the dynamic setting which target performatively optimal points.
2 Preliminaries
We consider the problem of a single decision-maker facing a decision dependent learning problem in a geometrically decaying environment.
Towards formally deﬁning the optimization problem the decision-maker faces, we ﬁrst introduce some notation. Throughout, we let Rd denote a d–dimensional Euclidean space with inner product ·, · and induced norm x = x, x . The projection of a point y ∈ Rd onto a set X ⊂ Rd is denoted projX (y) =
3

argminx∈X x − y . We are interested in random variables taking values in a metric space. Given a metric space Z with metric d(·, ·) the symbol P(Z) denotes the set of Radon probability measures ν on Z with a
ﬁnite ﬁrst moment Ez∼ν[d(z, z )] < ∞ for some z ∈ Z. We measure the deviation between two measures ν, ν ∈ P(Z) using the Wasserstein-1 distance:

W1(ν, µ) = sup {EX∼ν [h(X)] − EY ∼µ[h(Y )]},
h∈Lip1

where Lip1 denotes the set of 1–Lipschitz continuous functions h : Z → R.

The decision-maker seeks to solve

min L(x)

(1)

x∈X

where L(x) = Ez∼D(x)[ (x, z)] is the expected loss. The decision-space X lies in the Euclidean space Rd, is closed and convex, and there exists constants r, R > 0 satisfying rB ⊆ X ⊆ RB where B is the unit ball in dimension d. The loss function is denoted : Rd × Z → R, and D(x) ∈ P(Z) is a probability measure that
depends on the decision x ∈ X .

Deﬁnition 1. For a given probability measure D(x) induced by action x ∈ X , the decision vector x∗ ∈ X is

optimal if

x∗ ∈ arg min L(x) = arg min E [ (z, x)].

x∈X

x∈X z∼D(x)

The main challenge to ﬁnding an optimal point is that the environment is evolving in time according to a geometrically decaying process. That is, the random variable z depends not only on the decision xt ∈ X at time t, but also explicitly on the time instant t. In particular, the random variable z is governed by the distribution pt which is the probability measure at time t generated by the process pt+1 = T (pt, xt) where

T (p, x) = λp + (1 − λ)D(x),

(2)

and λ ∈ [0, 1) is the geometric decay rate. Observe that given the geometrically decaying dynamics in (2), for any x ∈ X , the distribution D(x) is trivially a ﬁxed point—i.e., T (D(x), x) = D(x). Let T n := T ◦ · · · ◦ T denote the n-times composition of the map T so that, given the form in (2), we have T n(p, x) = λnp + (1 − λn)D(x).
One interpretation of this transition map is that it captures the phenomenon that for each time, a (1 − λ) fraction of the population becomes aware of the machine learning model x being used by the institution. Another interpretation is that the environment (and strategic data sources in the environment) has memory based on past interactions which is captured in the ‘state’ of the distribution, and the eﬀects of the past decay geometrically at a rate of λ. For instance, it is known in behavioral economics that humans often compare their decisions to a reference point, and that reference point may evolve in time and represent an accumulation of past outcomes (Kahneman and Tversky, 2013; Nar et al., 2017).
Throughout we use the notation ∇L to denote the derivative of L with respect to x. The notation ∇x and ∇z denotes the partial derivative of with respect to x and z, respectively. Further, let ∇x,z = (∇x , ∇z ) denote the vector of partial derivatives. We also make the following standing assumptions on the loss and the probability measure D(x).
Assumption 1 (Standing). The loss and distribution D satisfy the following:
a. The loss (x, z) is C1 smooth in x, and L-Lipschitz continuous in (x, z).
b. The map (x, z) → ∇x,z (x, z) is β-Lipschitz continuous.
c. The loss (x, z) is ξ-strongly convex in x.
d. There exists a constant γ > 0 such that

W1(D(x), D(x )) ≤ γ x − x ∀ x, x ∈ X .

4

The following assumption implies a convex ordering on the random variables on which the loss is dependent.

Assumption 2 (Mixture Dominance). The probability measure D(x) and loss satisfy mixture dominance— i.e., for any x ∈ X and s ∈ (0, 1),

E

[ (z, x)] ≤

E

[ (z, x)], ∀ v, w ∈ X .

z∼D(sv+(1−s)w)

z∼sD(v)+(1−s)D(w)

Under Assumptions 1 and 2, the expected loss L(x) is α := (ξ − 2γβ) strongly convex (cf. Theorem 3.1 Miller et al. (2021)), and so the optimal point is unique.
We make the following assumption on the regularity of the expected loss.
Assumption 3 (Smoothness). The map x → ∇L(x) is G-Lipschitz continuous, and the map x → ∇2L(x) is H-Lipschitz continuous.

An important class of distributions in the performative prediction literature that satisfy this assumption are location-scale distributions.

Assumption 4 (Parametric family). There exists a probability measure P and matrix A such that

z ∼ D(x) ⇐⇒ z = ζ + Ax,

and where ζ has mean µ := Eζ∼P [ζ] and co-variance Σ := Eζ∼P [(ζ − µ)(ζ − µ) ], respectively.
This class encompasses a broad set of distributions that are commonplace in the performative prediction literature. As observed in Miller et al. (2021), this class of probability measures is also γ-Lipschitz continuous and satisﬁes the mixture dominance condition when is convex.

Lemma 1 (Suﬃcient conditions for Assumption 3). Suppose that Assumption 4 holds and there exists constants β, ρ ≥ 0 such that the map (x, z) → ∇x,z (x, z) is β-Lipschitz continuous and has a ρ-Lipschitz continuous gradient. Then, Assumption 3 holds with constants

G :=

β2 max{1,

A

2 op

}

·

(1

+

A 2op),

H :=

ρ2 max{1,

A

4 op

}

·

(1

+

A 2op).

The proof is contained in Appendix A.

3 Algorithms & Sample Complexity Analysis
As alluded to in the introduction, the algorithms we propose for each of the information settings are similar in spirit: they each operate in epochs by holding ﬁxed a decision for n steps and querying the environment until the distribution dynamics have mixed suﬃciently towards the ﬁxed point distribution corresponding to the current decision.
3.1 Zero Order Stochastic Gradient Method
The most general information setting we consider is such that the decision-maker has only “bandit feedback”. That is, they only have access to a loss function evaluation oracle. This does not require the decisionmaker to have access to the decision-dependent probability measure D(x). This is a more realistic setting given that the form of D(·) may be a priori unknown. For example, if the data is generated by strategic data sources having their own private utility functions and preferences (e.g., as in strategic classiﬁcation or prediction, or incentive/pricing design problems), then the decision-maker does not necessarily have access to the distribution map D(x) in practice.

5

Algorithm 1: Epoch-Based Zeroth Order Algorithm

Initialization: epoch length nt, step-size ηt = t4α , initial point x1, query radius δ, horizon T , initial distribution p0;
for t = 1, 2, . . . , T do
// Step 1: Query-Mix

Sample vector vt from the unit sphere; Query with xt + δvt for nt steps, so that pt = T nt (pt−1, xt + δvt);

// Step 2: Update

Oracle reveals gˆt =

d δ

(z, xt + δvt)vt, z ∼ pt;

Update xt+1 = proj(1−δ)X (xt − ηtgˆt);

end

Algorithm 2: Epoch-Based First Order Algorithm
Initialization: epoch length nt, non-increasing step-size ηt, initial point x1, horizon T , initial distribution p0; for t = 1, 2, . . . , T do
// Step 1: Query-Mix Query with xt for nt steps, so that pt = T nt (pt−1, xt);
// Step 2: Update Oracle reveals gˆt = ∇ (xt, z), z ∼ pt; Update xt+1 = projX (xt − ηtgˆt); end

The zero-order stochastic gradient method proceeds as follows. Fix a parameter δ > 0. In each epoch t, Algorithm 1 samples vt, a unit vector, uniformly from the unit sphere S in dimension d, queries the environment for nt iterations with xt + δvt, and then receives feedback from the loss oracle which reveals (xt + δvt, zt) where zt ∼ λnt pt−1 + (1 − λnt )D(xt + δvt) which the decision maker uses to update xt as follows:
xt+1 = proj (xt − ηtgˆt) ,
(1−δ)X
where d
gˆt = δ (xt + δvt, zt)vt. (3)
This is a one-point gradient estimate of the expected loss at pt. It can be shown that (3) is an unbiased estimate of the gradient of the smoothed loss function

Lδt (x) = E E (x, z)
v∼S z∼pt
at time t (e.g., in the general setting without decision-dependence this follows from Flaxman et al. (2004)). The reason for projecting onto the set (1 − δ)X is to ensure that in the next iteration, the decision is in the feasible set.
Deﬁne the smoothed expected risk as follows:

Lδ(x) = E

E [ (x + δv, z)] .

v∼B z∼D(x+δv)

It is straightforward to show that Lδ is strongly convex with parameter (1 − c)α for some c ∈ (0, 1) in the regime where δ ≤ cα/H (cf. Lemma 6, Appendix B).
To obtain convergence guarantees we need the following additional assumption.

6

Assumption 5. The quantity ∗ := sup{| (x, z)| : x ∈ X , z ∈ Z} is ﬁnite.

The next lemma provides a crucial step in the proof of our main convergence result for the bandit feedback setting: it provides a bound on the bias due to the dynamics.
Lemma 2. Under Assumptions 1, 2, 3, and 5, the error between the gradient smoothed loss Lδt at pt and the gradient of the smoothed expected loss Lδ satisﬁes

∇Ev∼B[Ez∼pt [ (z, xt + δv)]] − ∇Lδ(xt) ≤ L · λnt W (p0) + λnt 4αγδd (1 λ− ∗λ)2

where pt = λnt pt−1 + (1 − λnt )D(xt + δvt), and W (p0) = maxx∈X W1(p0, D(x)).

We defer the proof to Appendix B.1. To obtain the convergence rate, let x¯δ be the optimal point for Lδ on (1 − δ)X .

Theorem

1.

Suppose

that

Assumptions

1,

2,

3,

and

5

hold.

Let

δ

≤ min{r, 2αH },

and

set

step

size

ηt

=

4 αt

and epoch length





W (p0)

+

4γd αδ

λ∗ (1−λ)2

1

nt ≥ log  



.

2d2 1/2  log(1/λ)

ηt

α L2

∗
4δ2

Then the estimate holds:

∗ 2 max{α2δ2 x1 − x¯δ 2, 16d2 2∗}

2

E xt − x ≤

tα2δ2

+ 2δ

G 1+
α

x∗ + G 2 . α

The following corollary states the convergence rate.

Corollary 1 (Main result for zero-order oracle). Suppose the assumptions of Theorem 1 hold. Fix a target accuracy
ε < 4r2((1 + Gα )R + Gα )2,
and set δ = α ε/4/((α + G)R + G) and ηt = 4/(αt). Then, the estimate E xt − x∗ 2 ≤ ε holds for all

t ≥ max{8α2εR2, 128((α + G)R + G)2 2∗d2} . α4ε2

In the proceeding corollary, the lower bound on t is in terms of the number of epochs that Algorithm 1

needs to be run to obtain the target accuracy. In terms of total iterations across all epochs (i.e.,

t k=1

nk

),

the rate is thus O dε22 log 1ε .

3.2 First Order Stochastic Gradient Method
In many situations, the decision maker has access to a parametric description of the decision-dependent probability measure D(x) in which case the decision-maker can employ a stochastic gradient method. The challenge of having the distribution changing in time still remains, and hence the novelty of the results in this section.
To this end, let the expected loss at time t be given by

Lt(x) = E (xt, z).

(4)

z∼pt

Under Assumption 4 and mild smoothness assumptions, diﬀerentiating (4) we see that the gradient of Lt is
simply ∇Lt(x) = E [∇x (x, z) + (1 − λn)A ∇z (x, z)].
z∼pt

7

Therefore, given a point x, the decision-maker may draw z ∼ pt and form the vector gˆt = ∇ (xt, z) = ∇x (xt, z) + (1 − λn)A ∇z (xt, z).

By deﬁnition, gˆt is an unbiased estimator of ∇Lt(x), that is

E [gˆt] = ∇Lt(x).
z∼pt

Algorithm 2 proceeds as follows. In round t, the decision maker queries the environment with xt for n steps so that pt = λnpt−1 + (1 − λn)D(xt). Then, the gradient oracle reveals gˆt as deﬁned above where z ∼ pt, and the decision maker updates xt using xt+1 = projX (xt − ηtgˆt).
The following lemma is completely analogous to Lemma 2, and provides a bound on the gradient error
due to the dynamics.

Lemma 3. Under Assumptions 1, 2, and 4, the gradient error satisﬁes

2

2

n

n L(1 + A op)λ 2

∇Ez∼pt [ (z, xt)]] − ∇L(xt) ≤ L · λ W 1(p0) + λ γη1 (1 − λ)2

where pt = λnpt−1 + (1 − λn)D(xt). We defer the proof to Appendix C.1.

Assumption 6 (Finite Variance). There exists a constant σ > 0 satisfying

E [ gˆt − E ∇ (x, z ) 2] ≤ σ2 ∀x ∈ X , ∀t ≥ 1.

z∼pt

z ∼pt

To justify the above assumption, we provide suﬃcient conditions for the above assumption to hold in terms of the variance of the partial gradients ∇x,z .
Lemma 4 (Suﬃcient Conditions for Assumption 6). Suppose there exists constants s1, s2 ≥ 0 such that for all x ∈ X the estimates hold:

E
z∼pt
E
z∼pt

∇x (x, z) − E ∇x (x, z ) 2 ≤ s21
z ∼pt
∇z (x, z) − E ∇z (x, z ) 2 ≤ s22
z ∼pt

Then Assumption 6 holds with σt2 = 2(s21 + A 2ops22).

Theorem

2.

Suppose

that

Assumptions

1,

2,

3,

and

4

hold.

For

step-size

η = ηt ≤

α 2G2

and

epoch

length

n ≥ log

W 1(p0) + γηL(1 +

A

op

)

λ (1−λ)

2

L (αη)1/2σ

1 ,
log(1/λ)

the estimate holds:

E xt+1 − x∗ 2 ≤ 1 E xt − x∗ 2 + 4η2σ2 .

1 + ηα

1 + ηα

We defer the proof to Appendix C.2. Applying a step-decay schedule on η yields the following corollary, the proof of which follows directly from the recursion in Theorem 2 and generic results on step decay schedules (see, e.g., Drusvyatskiy and Xiao (2020, Lemma B.2)).

8

Price (dollars) Price (dollars) Occupancy (%) Occupancy (%)

4.5

BEACH ST 600 FO

4.5

BEACH ST 600 ZO

90

BEACH ST 600 FO

90

BEACH ST 600 ZO

4.0

4.0

80

80

3.5

3.5

3.0

3.0

70

70

2.5

2.5

60

60

2.0

2.0

1.5

1.5

50

50

1.0 0 25 50 75 100 125 1.0 0 25 50 75 100 125 40 0 25 50 75 100 125 40 0 25 50 75 100 125

Iterations (weeks)

Iterations (weeks)

Iterations (weeks)

Iterations (weeks)

n=1,T=120

n=4,T=30

n=8,T=15

n=15,T=8

SF Park

PO Price

Desired Occupancy

Figure 1: Results of Algorithm 2 (ﬁrst and third plots) and Algorithm 1 (second and fourth plots) with diﬀerent (n, T ) pairs for 600 Beach ST and time window 1200–1500. Each marker represents a price announcement, and the plots show the prices and corresponding predicted occupancies. The SFpark prices and occupancies are far from the target and performative optimal price, whereas the proposed algorithms obtain both points up to theoretical error bounds.

Figure 2: Final prices announced by ﬁrst and zero order algorithms (Algorithms 2 and 1) run with (n, T ) = (8, 15) and (n, T ) = (1, 120), respectively, as compared to SFpark for streets depicted in the right graphic (color coded to the bar charts) during the 900–1200 time period. The center plot shows the corresponding predicted occupancies. The dotted lines represent performatively optimal price and target occupancy of 70%, in the left and center plots, respectively. The average price overall is lower for both proposed methods, the occupancy is better distributed, and the average occupancy closer to the desired range.

Corollary 2 (Main result for ﬁrst order oracle). Suppose the assumptions of Theorem 2 hold, and that

Algorithm 2 is run in super-epochs indexed by k = 1, . . . , K wherein each super-epoch is run for Tk epochs

with

constant

step-size

ηk

=

α 2G2

· 2−k,

and

such

that

the

last

iterate

of

super-epoch

k

is

used

as

the

ﬁrst

iterate in super-epoch k + 1. Fix a target accuracy ε > 0 and suppose R > x1 − x∗ 2 is available. Set

T1 = α2η1 log( 2εR ) , Tk = 2 lαoηgk(4) , for k ≥ 2,

and K =

1

+

log2(

2η1 σ2 αε

)

.

The

ﬁnal

iterate

x

produced

satisﬁes

E

x − x∗

2 ≤ ε, while the total number of

epochs is at most

G2

2R

σ2

O α2 log ε + α2ε .

It is straightforward to show that the total number of iterations is O Gα22 log 2εR + ασ22ε log( 1ε ) .

4 Numerical Experiments
In this section, we apply our aforementioned algorithms to a semi-synthetic example based on real data from the dynamic pricing experiment—namely, SFpark1—for on-street parking in San Francisco. Parking
1SFpark: tinyurl.com/dwtf7wwn

9

availability, location, and price are some of the most important factors when people choose whether or not to use a personal vehicle to make a trip (Fiez and Ratliﬀ, 2020; Shoup, 2006, 2021).2 The primary goal of the SFpark pilot project was to make it easy to ﬁnd a parking space. To this end, SFpark targeted a range of 60–80% occupancy in order to ensure some availability at any given time, and devised a controlled experiment for demand responsive pricing. Operational hours are split into distinct rate periods, and rates are adjusted on a block-by-block basis, using occupancy data from parking sensors in on-street parking spaces in the pilot areas. We focus on weekdays in the numerical experiments; for weekdays, distinct rate periods are 900–1200, 1200–1500, and 1500–1800. Excluding special events, SFpark adjusted hourly rates as follows: a) 80–100% occupancy, rates are increased by $0.25; b) 60–80% occupancy, no adjustment is made; c) 30–60% occupancy, rate is decreased by $0.25; d) occupancy below 30%, rate is decreased by $0.50. When a price change is deployed it takes time for users to become aware of the price change through signage and mobile payment apps (Pierce and Shoup, 2013).
Given the target occupancy, the dynamic decision-dependent loss is given by
Ez∼pt [ (x, z)] = Ez∼pt [ z − 0.7 · 1 2 + ν2 x 2],
where z is the vector of curb occupancies (which is between zero and one), x is the vector of changes in price from the nominal price at the beginning of the SFpark study for each curb, and ν is the regularization parameter. For the initial distribution p0, we sample from the data at the beginning of the pilot study where the price is at the nominal (or initial) price. The distribution D(x) is deﬁned as follows:
z ∼ D(x) ⇐⇒ z = ζ + Ax
where ζ follows the same distribution as p0 described above, and A is a proxy for the price elasticity which is estimated by ﬁtting a line to the ﬁnal and initial occupancy and price (cf. Appendix D.1).3
Comparing Performative Optimum to SFpark. We run Algorithms 1 and 2 for Beach ST 600, a representative block in the Fisherman’s Wharf sub-area, in the time window of 1200–1500 as depicted in Figure 1. Beach ST is frequently visited by tourists and local residents. For Beach ST 600, we compute A ≈ −0.157, which means that a $1.00 increase in the parking rate will lead to a 15% decrease in parking occupancy at the ﬁxed point distributions. Additionally, we use the data to compute the geometric decay rate of λ ≈ 0.959 (computations described in Appendix D). Since the initial price is $3 per hour for this block, we take X = [−3, 5], since the maximum price that SFpark charges is $8 per hour, and the minimum price is zero dollars. Additionally, we set the regularization parameter ν = 1e-3. The algorithms are run using parameters as dictated by Theorems 1 and 2, respectively, with the exception of epoch length. The epoch length we set to reasonable values as dictated by the parking application. In particular, the unit of time for an iteration is weeks, and we set the epoch length in terms of the number of weeks the price is held ﬁxed. For instance, the SFpark study changed prices every eight weeks.4
The ﬁrst and third plots in Figure 1 show prices announced and corresponding occupancy, respectively, for Algorithm 2, on 600 Beach Street, with diﬀerent choices of n and T ; and, they show the prices announced and corresponding occupanices by SFpark as compared to the performatively optimal point (computed oﬄine). Similarly, the second and fourth plots in Figure 1 show this same information for Algorithm 1. Since Algorithm 1 is zero order, convergence requires more time and has variance coming from the randomness of the query directions.
SFpark changed prices approximately every eight weeks. As observed in Figure 1, this choice of n is reasonable—the estimated λ value is close to one—and leads to convergence to the optimal price change for both the ﬁrst order and zero order algorithms. As n increases, the performance degrades, an observation that holds more generally for this curb. However, in our experiments, we found that diﬀerent curbs had diﬀerent optimal epoch lengths, thereby suggesting that a non-uniform price update schedule may lead to better outcomes. Appendix D.2 contains additional experiments.
2Code: https://github.com/ratliﬄj/D3simulator.git 3Price elasticity is the change in percentage occupancy for a given percentage change in price. 4In Appendix D, we run synthetic experiments wherein the epoch length is chosen according to the theoretical results.
10

Moreover, the prices under the optimal solution obtained by the proposed algorithms are lower than the SFpark solution for the entire trajectory, and the algorithms both reach the target occupancy while SFpark is far from it. The third and fourth plots of Figure 1 show the eﬀect of the negative price elasticity on the occupancy; an increased price causes a decreased occupancy. An interesting observation is that for Algorithm 2, a larger choice of n, and consequently a smaller choice of T , allows for convergence closer to the optimal price, but for Algorithm 1, a smaller choice of n, and consequently, a larger choice of T , allows for quicker (and with lower variance) convergence to the optimal price. This is due to the randomness in the query direction for the gradient estimator used in Algorithm 1, meaning that a larger T is needed to converge quickly to the optimal solution. This suggests that in the more realistic case of zero order feedback, the institution should make more price announcements.
Redistributing Parking Demand. In this semi-synthetic experiment, we set ν = 1e-3 and take X = [−3.5, 4.5] since the base distribution for these blocks has a nominal price of $3.50. We also use the estimated λ and A values (described in more detail in Appendix D.3). We run Algorithms 1 and 2 (using parameters as dictated by the corresponding sample complexity theorems) for a collection of blocks during the time period 900–1200 in a highly mixed use area (i.e., with tourist attractions, a residential building, restaurants and other businesses). The results are depicted in Figure 2.
Hawthorne ST 0 is a very high demand street; the occupancy is around 90% on average during the initial distribution and remains high for SFpark (cf. center, Figure 2). The performatively optimal point, on the other hand, reduces this occupancy to within the target range 60–80% for both the ﬁrst and zeroth order methods. This occupancy can be seen as being redistributed to the Folsom ST 500-600 block, as depicted in Figure 2 (center) for our proposed methods: the SFpark occupancy is much below the 70% target average for these blocks, while both the decision-dependent algorithms lead to occupancy at the target average. Interestingly, this also comes at a lower price (not just on average, but for each block) than SFpark.
Hawthorne ST 100 is an interesting case in which both our approach and SFpark do not perform well. This is because the performatively optimal price in the unconstrained case is $9.50 an hour which is well above the maximum price of $8 in the constrained setting we consider. In addition, the price elasticity is positive for this block; together these facts explain the low occupancy. Potentially other control knobs available to SFpark, such as time limits, can be used in conjunction with price to manage occupancy; this is an interesting direction of future work.
5 Discussion and Future Directions
This work is an important step in understanding performative prediction in dynamic environments. Moving forward there are a number of interesting future directions. In this work, we consider one class of wellmotivated dynamics. Another practically motivated class of dynamics are period dynamics; indeed, in many applications there is an external context which evolves periodically such as seasonality or other temporal eﬀects. Devising algorithms for such cases is an interesting direction of future work. As compared to classical reinforcement learning problems, in this work, we exploit the structure of the dynamics along with convexity to devise convergent algorithms. However, we only considered general conditions on the class of distributions D(x); it may be possible to exploit additional structure on D(x) in improving the sample complexity of the proposed algorithms or devising more appropriate algorithms that leverage this structure.
References
Aris Anagnostopoulos, Carlos Castillo, Adriano Fazzone, Stefano Leonardi, and Evimaria Terzi. Algorithms for hiring and outsourcing in the online labor market. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 1109–1118, 2018.
Siddhartha Banerjee, Carlos Riquelme, and Ramesh Johari. Pricing in ride-share platforms: A queueingtheoretic approach. Available at SSRN 2568258, 2015.
11

Gavin Brown, Shlomi Hod, and Iden Kalemaj. Performative prediction in a stateful world. In Advances in Neural Information Processing Systems, 2020.
Chase Dowling, Tanner Fiez, Lillian Ratliﬀ, and Baosen Zhang. Optimizing curbside parking resources subject to congestion constraints. In Proceedings of the IEEE 56th Annual Conference on Decision and Control (CDC), pages 5080–5085, 2017. doi: 10.1109/CDC.2017.8264412.
Chase P. Dowling, Lillian J. Ratliﬀ, and Baosen Zhang. Modeling Curbside Parking as a Network of Finite Capacity Queues. IEEE Transactions on Intelligent Transportation Systems, 21(3):1011–1022, 2020. doi: 10.1109/TITS.2019.2900642.
Dmitriy Drusvyatskiy and Lin Xiao. Stochastic optimization with decision-dependent distributions. arXiv preprint arXiv:2011.11173, 2020.
Tanner Fiez and Lillian J. Ratliﬀ. Gaussian Mixture Models for Parking Demand Data. IEEE Transactions on Intelligent Transportation Systems, 21(8):3571–3580, 2020. doi: 10.1109/TITS.2019.2939499.
Tanner Fiez, Lillian J Ratliﬀ, Chase Dowling, and Baosen Zhang. Data driven spatio-temporal modeling of parking demand. In 2018 Annual American Control Conference (ACC), pages 2757–2762. IEEE, 2018.
Abraham D Flaxman, Adam Tauman Kalai, and H Brendan McMahan. Online convex optimization in the bandit setting: gradient descent without a gradient. arXiv preprint cs/0408007, 2004.
James Glasnapp, Honglu Du, Christopher Dance, Stephane Clinchant, Alex Pudlin, Daniel Mitchell, and Onno Zoeter. Understanding dynamic pricing for parking in Los Angeles: Survey and ethnographic results. In International Conference on HCI in Business, pages 316–327. Springer, 2014.
Vikas Goel and Ignacio E Grossmann. A stochastic programming approach to planning of oﬀshore gas ﬁeld developments under uncertainty in reserves. Computers & chemical engineering, 28(8):1409–1429, 2004.
John J Horton. Online labor markets. In International workshop on internet and network economics, pages 515–522. Springer, 2010.
Tore W Jonsbr˚aten, Roger JB Wets, and David L Woodruﬀ. A class of stochastic programs withdecision dependent random elements. Annals of Operations Research, 82:83–106, 1998.
Kaggle. Give me some credit dataset. https://www.kaggle.com/c/GiveMeSomeCredit, 2011.
Daniel Kahneman and Amos Tversky. Prospect theory: An analysis of decision under risk. In Handbook of the fundamentals of ﬁnancial decision making: Part I, pages 99–127. World Scientiﬁc, 2013.
Kristian Lum and William Isaac. To predict and serve? Signiﬁcance, 13(5):14–19, 2016.
Celestine Mendler-Du¨nner, Juan Perdomo, Tijana Zrnic, and Moritz Hardt. Stochastic optimization for performative prediction. Advances in Neural Information Processing Systems, 33, 2020.
John Miller, Juan C. Perdomo, and Tijana Zrnic. Outside the echo chamber: Optimizing the performative risk. arXiv preprint arXiv:2102.08570, 2021.
Kamil Nar, Lillian J Ratliﬀ, and Shankar Sastry. Learning prospect theory value function and reference point of a sequential decision maker. In Proceedings of the IEEE 56th Annual Conference on Decision and Control (CDC), pages 5770–5775, 2017.
Juan Perdomo, Tijana Zrnic, Celestine Mendler-Du¨nner, and Moritz Hardt. Performative prediction. In Hal Daum´e III and Aarti Singh, editors, Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 7599–7609. PMLR, 13–18 Jul 2020.
12

Gregory Pierce and Donald Shoup. Getting the prices right: an evaluation of pricing parking by demand in san francisco. Journal of the american planning association, 79(1):67–81, 2013.
Gregory Pierce and Donald Shoup. Sfpark: Pricing parking by demand. Routledge, 2018. Donald C Shoup. Cruising for parking. Transport policy, 13(6):479–486, 2006. Donald C Shoup. The high cost of free parking. Routledge, 2021. Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018. Pravin Varaiya and RJ-B Wets. Stochastic dynamic optimization approaches and computation. IIASA
Working Paper (WP-88-087), 1988.
13

A Technical Lemmas and Notation

Notation. Throughout we will use the following derivative and partial derivative notation. For a given
function (x, z), the partial derivative of with respect to z is denoted ∇z (x, z) and the partial derivative with respect to x is denoted ∇x (x, z). For the expected risk Ez∼D(x)[ (x, z)], the total derivative with respect to x is denoted

∇ E [ (x, z)] = ∇
z∼D(x)

(x, z)px(z)dz = E [∇x (x, z)] + E [ (x, z)∇x log(px(z))]

z∼D(x)

z∼D(x)

where px(z) is the density function for D(x) and in the last equality we have applied the so-called ‘log trick’, which comes from the chain rule. Throughout, we use the notation · for the Euclidean norm.

Technical Lemmas. The following lemma is a direct consequence of dual form of the Wasserstein-1 distance.
Lemma 5. Let f : Rn → Rn be β-Lipschitz, and let X, X be random vectors with distributions p and p , respectively. Then,
E[f (X)] − E[f (X )] ≤ βW1(p, p ).
Lemma 1 (Suﬃcient conditions for Assumption 3). Suppose that Assumption 4 holds and there exists constants β, ρ ≥ 0 such that the map (x, z) → ∇x,z (x, z) is β-Lipschitz continuous and has a ρ-Lipschitz continuous gradient. Then, Assumption 3 holds with constants

G :=

β2 max{1,

A

2 op

}

·

(1

+

A 2op),

H :=

ρ2 max{1,

A

4 op

}

·

(1

+

A 2op).

Proof. Observe that we may write

I0

∇L(x) = E V
ζ∼P

∇x,z (z, ζ + Ax)

where V = 0

A.

Therefore, we deduce

∇L(x) − ∇L(y)

≤ V op E ∇x,z (z, ζ + Ax) − ∇x,z (y, ζ + Ay)
ζ∼P
≤ max{1, A op} · β · E (z, ζ + Ax) − (y, ζ + Ay)
ζ∼P
= max{1, A op} · β · x − y 2 + A(x − y) 2

≤ max{1, A op} · β · (1 + A 2op) · x − y .

Analogously, observe that

∇2L(x) = E V ∇2 (x, ζ + Ax)V
ζ∼P

where

∇2 (x, z) = ∇2x (x, z) ∇xz (x, z) ∇zx (x, z) ∇2z (x, z)

is the Hessian of with respect to (x, z). Therefore, we deduce

∇2L(x) − ∇2L(y)

≤

V

2 op

E

∇2 (z, ζ + Ax) − ∇2 (y, ζ + Ay)

ζ∼P

≤ max{1,

A

2 op

}

·

ρ

·

E

(z, ζ + Ax) − (y, ζ + Ay)

ζ∼P

= max{1,

A

2 op

}

·

ρ

·

x − y 2 + A(x − y) 2

≤ max{1,

A

2 op

}

·

ρ

·

(1 + A 2op) · x − y .

The proof is complete.

14

B Proofs for Zero Order Oracle Setting

B.1 Technical Lemmas

Recall that

Lδ(x) = Ev∼B[Ez∼D(x+δv)[ (z, x + δv)]].

Lemma 6. Suppose that Assumptions 1, 2, and 3 hold. Choose δ ≤ cα/H for some constant c ∈ (0, 1). Then the map Lδ is strongly convex over X with parameter (1 − c)α.

Proof. We ﬁrst estimate the Lipschitz constant of the diﬀerence map h(x) := ∇Lδ(x) − ∇L(x).

To this end, we compute

∇h(x) = Ew∼B[∇2L(x + δw) − ∇2L(x)].

Taking into account that the map x → ∇2L(x) is H-Lipschitz continuous, we deduce

∇h(x) op ≤ Ew∼B[ ∇2L(x + δw) − ∇2L(x) op] ≤ δHEw∼B w ≤ δH.

Thus the map h is Lipschitz continuous with parameter δH. We therefore compute ∇Lδ(x) − ∇Lδ(x ), x − x = ∇L(x) − ∇L(x ), x − x + h(x) − h(x ), x − x ≥ (α − Hδ) x − x 2,

which completes the proof.

Lemma 2. Under Assumptions 1, 2, 3, and 5, the error between the gradient smoothed loss Lδt at pt and the gradient of the smoothed expected loss Lδ satisﬁes
∇Ev∼B[Ez∼pt [ (z, xt + δv)]] − ∇Lδ(xt) ≤ L · λnt W (p0) + λnt 4αγδd (1 λ− ∗λ)2
where pt = λnt pt−1 + (1 − λnt )D(xt + δvt), and W (p0) = maxx∈X W1(p0, D(x)).
Proof of Lemma 2. Observe that using Jensen’s inequality along with Lemma 5, we deduce
∇Ev∼B[Ez∼pt [ (z, xt + δv)]] − ∇Lδ(xt) 2 ≤ Ev∼B ∇Ez∼pt [ (z, xt + δv)] − ∇Ez∼D(xt+δv)[ (z, xt + δv)] 2 ≤ Ev∼B[L2(W1(pt, D(xt + δv)))2].
Hence, we need an an upper bound on W1(pt, D(xt + δvt)) which is the Wasserstein-1 distance between the distribution at time t and the ﬁxed point distribution for the query point xt + δvt.
Upper bound on W1(pt, D(xt + δv)). Using the fact that pt = λnt pt−1 + (1 − λnt )D(xt + δv), we expand W1(D(xt + δv), pt) as follows:
W1(pt, D(xt + δv)) = W1(λnt pt−1 + (1 − λnt )D(xt + δv), D(xt + δv)) ≤ λnt W1(pt−1, D(xt + δv)) + (1 − λnt )W1(D(xt + δv), D(xt + δv)) = λnt W1(pt−1, D(xt + δv)) = λnt W1(λnt−1 pt−2 + (1 − λnt−1 )D(xt−1 + δv), D(xt + δv)) ≤ λnt · λ · W1(pt−2, D(xt + δv)) + λnt (1 − λnt )W1(D(xt−1 + δv), D(xt + δv)) ≤ λnt · λ · W1(pt−2, D(xt + δv)) + λnt (1 − λnt ) · γ · xt − xt−1 ≤ λnt · λ · W1(pt−2, D(xt + δv)) + λnt · γ · xt − xt−1 ,

15

where we have used the triangle inequality, Assumption 1(d), and the fact that λ > λnt for any t ≥ 1, λnt < λnt−i for any t ∈ {1, . . . , t − 1}, and 1 − λnt < 1. Continuing to unroll the recursion, we have that

EvW1(pt, D(xt + δv)) ≤ λnt · λEvW1(λnt−2 pt−3 + (1 − λnt−2 )D(xt−2 + δvt−2), D(xt + δvt)) + λnt · γ · xt − xt−1

≤ λnt λ2EvW1(pt−3, D(xt + δv)) + λλnt EvW1(D(xt + δv), D(xt−2 + δv))) + λnt · γ xt − xt−1

≤ λnt · λ2EvW1(pt−3, D(xt + δv)) + λnt · γEv( xt − xt−1 + λ · xt − xt−2 )

t−1

≤ λnt λt−1EvW1(D(xt + δv), p0) + λnt γ λ(i−1)Ev xt − xt−i .

(5)

i=1

Hence, we need a bound on

xt − xt−i

for

each

i

∈

{1,

.

.

.

,

t − 1}.

Using

the

fact

that

xt

=

xt−1

− ηt

d δ

(xt−1 +

δvt−1, zt−1)vt−1 where zt−1 ∼ pt−1, we have that

xt − xt−i

=

xt−1

−

ηt−1

d δ

(xt−1 + δvt−1, z)vt−1 − xt−i

d

d

= xt−2 − ηt−2 δ (xt−2 + δvt−2, zt−2)vt−2 − ηt−1 δ

d t−1

≤ δ η1

| (xj + δvj, zj)| vj

j=t−i

d ≤ η1 δ ∗(i − 1),

(xt−1 + δvt−1, zt−1)vt−1 − xt−i

where the penultimate inequality holds from the fact that the learning rate is non-increasing. Hence, we have that

t−1
EvW1(pt, D(xt + δv)) ≤ λnt λt−1EvW1(D(xt + δv), p0) + λnt γ λ(i−1) xt − xt−i

i=1

t−1

d

≤ λnt λt−1EvW1(D(xt + δv), p0) + λnt γ λ(i−1)η1 ∗(i − 1)

δ

i=1

≤ λnt W (p0) + λnt 4γd ∗λ , αδ (1 − λ)2

where the last inequality holds using the fact that

ti=−11 λ(i−1)(i − 1)

≤

λ (1−λ)2

.

Bounding gradient error. Using this bound, we deduce

∇Ev∼B[Ez∼pt [ (z, xt + δv)]] − ∇Lδ(xt) 2 ≤ Ev∼B[L2(W1(pt, D(xt + δv)))2]

2n

n 4γd λ ∗ 2

≤ L λ t W (p0) + λ t αδ (1 − λ)2 .

This concludes the proof.

Lemma 7. Suppose that Assumptions 1 and 3 hold. The loss Lδ(x) is diﬀerentiable and the map x → ∇Lδ(x) is G-Lipschitz continuous. Moreover, the estimate holds:

∇L(x) − ∇Lδ(x) ≤ Gδ ∀ x ∈ X .

Proof. For any point x, x ∈ X , we successively estimate ∇Lδ(x) − ∇Lδ(x ) ≤ E [ ∇L(x + δw) − ∇L(x + δw) ] ≤ G x − x
w∼B
16

Thus ∇Lδ is G-Lipschitz continuous. Next, we estimate

∇L(x) − ∇Lδ(x) ≤ E [ ∇L(x + δw) − ∇L(x) ] ≤ G · δ E w ≤ G · δ,

w∼B

w∼B

which concludes the proof.

Deﬁne the smoothed loss at pt as

Lδt (x) := E [ E [ (z, x + δv)].
v∼B z∼pt

Let x¯δ the optimal point of Lδ on (1 − δ)X , and xδ be the optimal point of Lδ on X . We have the following bound on the distance between the optimum of the performative prediction problem deﬁned by L on X and the optimum of the perturbed problem deﬁned by Lδ on (1 − δ)X .
The normal cone to a convex set X at x ∈ X , denoted by NX (x) is the set

NX (x) = {v ∈ Rd : v, y − x ≤ 0 ∀y ∈ X }.

Lemma 8. Choose δ < min{r, Hα }. Then the estimate holds:

x∗ − x¯δ ≤ δ

G 1+
α

x∗

G +

.

α

Proof. There are two sources of perturbation: one replacing X with (1 − δ)X and the other in replacing L with Lδ. We will deal with each one individually. To do so, set τ := 1 − δ and let x˜ be the optimal point for
L on the shrunken set τ X . Thus x˜ satisﬁes the inclusion 0 ∈ ∇L(x˜) + NτX (x˜) where NτX (x˜) denotes the normal cone to τ X at x˜. The triangle inequality directly gives

x∗ − x¯δ ≤ x∗ − x˜ + x˜ − x¯δ .

(6)

Let us bound the ﬁrst term on the right hand side of (6). To this end, since the map x → ∇L(x) + NτX (x) is α-strongly monotone, we deduce

α x˜ − τ x∗ ≤ dist(0, ∇L(τ x∗) + NτX (τ x∗)).

(7)

Let use estimate the right hand side of (7). Since x∗ is optimal, the inclusion 0 ∈ ∇L(x∗) + NX (x∗) holds. Taking into account the identity NτX (τ x∗) = NX (x∗), we deduce

dist(0, ∇L(τ x∗) + NτX (τ x∗)) = dist(0, ∇L(τ x∗) + NX (x∗)) ≤ ∇L(τ x∗) − ∇L(x∗) ≤ δ · G · x∗ ,

where the last inequality holds since ∇L is G-Lipschitz continuous. Appealing to (7) and using the triangle inequality, we therefore deduce

x∗ − x˜

≤

x˜ − τ x∗

+ δ x∗

≤δ

G 1+

x∗ .

α

It remains to upper bound x˜ − x∗ . Since x˜ is optimal, we have that

−∇L(x˜), x − x˜ ≤ 0, ∀ x ∈ τ X .

(8)

Analogously, since x¯δ is also optimal, we have that

−∇Lδ(x¯δ), x − x¯δ ≤ 0, ∀ x ∈ τ X .

(9)

17

Then, by strong convexity and estimates (8) and (9), we get that
α x˜ − x¯δ 2 ≤ ∇L(x˜) − ∇L(x¯δ), x˜ − x¯δ ≤ ∇Lδ(x¯δ) − ∇L(x¯δ), x˜ − x¯δ ≤ ∇Lδ(x¯δ) − ∇L(x¯δ) x˜ − x¯δ ≤ G · δ · x˜ − x¯δ

where the last inequality follows from Lemma 7.

The following lemma holds by a simple inductive argument.

Lemma 9. Consider a sequence Dt ≥ 0 for t ≥ 1 and constants t0 ≥ 0, a > 0 satisfying

Dt+1 ≤

2 1−
t + t0

a Dt + (t + t0)2 .

Then the estimate holds:

Dt ≤ max{(1 + t0)D1, a} ∀t ≥ 1. t + t0

B.2 Proof of Theorem 1

Theorem

1.

Suppose

that

Assumptions

1,

2,

3,

and

5

hold.

Let

δ

≤ min{r, 2αH },

and

set

step

size

ηt

=

4 αt

and epoch length





W (p0)

+

4γd αδ

λ∗ (1−λ)2

1

nt ≥ log  



.

2d2 1/2  log(1/λ)

ηt

α L2

∗
4δ2

Then the estimate holds:

∗ 2 max{α2δ2 x1 − x¯δ 2, 16d2 2∗}

2

E xt − x ≤

tα2δ2

+ 2δ

G 1+
α

x∗ + G 2 . α

Proof. Adding and subtracting appropriately, we have that

1 xt+1 − x∗ 2 ≤ xt+1 − x¯δ 2 + x¯δ − x∗ 2 2

≤ xt+1 − x¯δ 2 + δ2

G 1+
α

x∗ + G 2 . α

Now, to bound xt+1 − x¯δ , we have that

E[ xt+1 − x¯δ 2] ≤ E[ xt − x¯δ − ηtgˆt(xt) 2]

≤ E[ xt − x¯δ 2] − 2ηt E gˆt(xt), xt − x¯δ + ηt2 E gˆt(xt) 2

= E[ xt − x¯δ

2]

−

2ηt

E

∇L

δ t

(x

t

),

xt

−

x¯δ

+ ηt2 E

gˆt(xt) 2

where the last equality holds since E[gˆt(xt)] = ∇Lδt (xt). We rewrite the smoothed gradient of the loss at time t as
∇Lδt (xt) = ∇Lδ(xt) + ∇Lδt (xt) − ∇Lδ(xt).
Hence

E[ xt+1 − x¯δ

2] ≤ E[ xt − x¯δ − ηtgˆt(xt) 2]

≤ E[ xt − x¯δ 2] − 2ηt E ∇Lδ(xt), xt − x¯δ − 2ηt E ∇Lδt (xt) − ∇Lδ(xt), xt − x¯δ

δ2

δ

δ

δ

2 2∗d2

≤ (1 − ηtα) E[ xt − x¯ ] − 2ηt E ∇Lt (xt) − ∇L (xt), xt − x¯ + ηt 2δ2 ,

+ ηt2 E

gˆt(xt) 2

18

where we used the fact that the smoothed loss is (1 − c)α strongly convex for any c ∈ (0, 1) and we let c := 1/2. Using the fact that

E | ∇Lδ(xt) − ∇Lδ(xt), xt − x¯δ | ≤ 1 L2

t

2∆1

λnt W (p0) + λnt 4γd ∗ αδ (1 − λ)2

2 ∆1 E xt − x¯δ 2

+

,

2

we have that

δ2

δ2

2 2∗d2

E[ xt+1 − x¯ ] ≤ (1 − ηtα) E[ xt − x¯ ] + ηt 2δ2

+ 2ηt

1 L2 2∆1

λnt W (p0) + λnt 4γd ∗ αδ (1 − λ)2

2 ∆1 E xt − x¯δ 2 + 2

δ2

2 2∗d2

= (1 − ηt(α − ∆1)) E[ xt − x¯ ] + ηt 2δ2 + 2ηt

1 L2 2∆1

n

n 4γd ∗

2

λ t W (p0) + λ t αδ (1 − λ)2

α

δ2

2 2∗d2

L2 n

n 4γd ∗

2

≤ (1 − ηt 2 ) E[ xt − x¯ ] + ηt 2δ2 + 2ηt α λ t W (p0) + λ t αδ (1 − λ)2

where we use ∆1 := α/2. Now, since





W (p0)

+

4γd αδ

∗
(1−λ)2

1

nt ≥ log  



,

2d2 1/2  log(1/λ)

ηt

α L2

∗
4δ2

we have that so that

n

4γd ∗

α 2∗d2 1/2

λ t W (p0) + αδ (1 − λ)2 ≤ ηt L2 4δ2

,

L2 n

n 4γd ∗

2

2 2∗d2

2ηt α λ t W (p0) + λ t αδ (1 − λ)2

≤ ηt 2δ2 .

Therefore, we deduce

δ2

α

δ2

2 2∗d2

E[ xt+1 − x¯ ] ≤ 1 − ηt 2 E[ xt − x¯ ] + ηt δ2 .

Since ηt = 4/(αt), we apply Lemma 9 to deduce that

δ 2 max{α2δ2 x1 − x¯δ 2, 16 2∗d2}

E xt+1 − x¯ ≤

δ2α2t

∀t ≥ 1.

This concludes the proof.

B.3 Proof of Corollary 1
Corollary 1 (Main result for zero-order oracle). Suppose the assumptions of Theorem 1 hold. Fix a target accuracy
ε < 4r2((1 + Gα )R + Gα )2, and set δ = α ε/4/((α + G)R + G) and ηt = 4/(αt). Then, the estimate E xt − x∗ 2 ≤ ε holds for all
t ≥ max{8α2εR2, 128((α + G)R + G)2 2∗d2} . α4ε2
19

Proof. The assumed upper bound on ε directly implies that δ ≤ 2αG and δ < r. An application of Theorem 1

yields the estimate

∗ 2 max{δ2α2 x1 − x¯δ 2, 16d2 2∗} ε

E[ xt − x ] ≤

tα2δ2

+. 2

Setting the right side to ε, solving for t, and using the trivial upper bound x1 − x¯δ ≤ 2R completes the proof.

C Proofs for First Order Oracle Setting

C.1 Proof of Lemma 3

Lemma 3. Under Assumptions 1, 2, and 4, the gradient error satisﬁes

2

2

n

n L(1 + A op)λ 2

∇Ez∼pt [ (z, xt)]] − ∇L(xt) ≤ L · λ W 1(p0) + λ γη1 (1 − λ)2

where pt = λnpt−1 + (1 − λn)D(xt).

Proof. Observe that using Jensen’s inequality along with Lemma 5, we deduce

∇Ez∼pt [ (z, xt)]] − ∇L(xt) 2 = ∇Ez∼pt [ (z, xt)] − ∇Ez∼D(xt)[ (z, xt)] 2 ≤ L2(W1(pt, D(xt)))2.

The remainder of the proof is identical to the proof of Lemma 2. Indeed, we have that

t−1
W1(pt, D(xt)) ≤ λnλt−1W1(D(xt), p0) + λnγ λ(i−1) xt − xt−i .
i=1
Hence, we need a bound on xt − xt−i for each i ∈ {1, . . . , t − 1}. Recall that xt = xt−1 − ηt−1gˆt−1 where
gˆt−1 = ∇x (xt−1, zt−1) + (1 − λn)A ∇z (xt−1, zt−1), and zt−1 ∼ pt−1.

Moreover,

gˆt ≤ L(1 + A op)

since is L-Lipschitz continuous. Hence, we have the following bound:

xt − xt−i

= xt−1 − ηt−1gˆt−1 − xt−i

= xt−2 − ηt−2gˆt−2 − ηt−1gˆt−1 − xt−i

t−1

≤ η1

gˆj

j=t−i

≤ η1 · L · (1 + A op) · (i − 1),

where the penultimate inequality holds from the fact that the learning rate is non-increasing. Therefore, we deduce

t−1
W1(pt, D(xt)) ≤ λnλt−1W1(D(xt), p0) + λnt γ λ(i−1)η1 · L · (1 + A op) · (i − 1)

i=1

≤ λnλt−1W1(D(xt), p0) + λnγη1 · L · (1 + A op) ·

λ ,

(1 − λ)2

20

where the last inequality follows from the fact that

ti=−11 λ(i−1)(i − 1)

≤

λ (1−λ)2

.

Using

this

bound

on

the Wasserstein-1 distance between the current probability distribution pt at time t and the ﬁxed point

probability distribution D(xt) induced by xt, we have that

2

2

n

n

λ

2

∇Ez∼pt [ (z, xt)]] − ∇L(xt) ≤ L · λ W 1(p0) + λ γη1 · L · (1 + A op) · (1 − λ)2

since λt−1 ≤ 1. This concludes the proof.

C.2 Proof of Theorem 2

We restate the theorem for convenience.

Theorem

2.

Suppose

that

Assumptions

1,

2,

3,

and

4

hold.

For

step-size

η = ηt ≤

α 2G2

and

epoch

length

n ≥ log

W 1(p0) + γηL(1 +

A

op

)

λ (1−λ)

2

L (αη)1/2σ

1 ,
log(1/λ)

the estimate holds:

E xt+1 − x∗ 2 ≤ 1 E xt − x∗ 2 + 4η2σ2 .

1 + ηα

1 + ηα

Note that the gradient gˆt approximates the gradient G(x) := ∇L(x) = ∇ Ez∼D(x) (x, z).

xt+1 − xt = xt − ηgˆt − xt = xt − ηG(xt) − η(gˆt − G(xt)) − xt .

Noting

that

xt+1

is

the

minimizer

of

the

1–strongly

convex

function

x

→

1 2

xt − ηgˆt − x

2 over X , we deduce

1 xt+1 − x∗ 2 ≤ 1 xt − ηgˆt − x∗ 2 − 1 xt − ηgˆt − xt+1 2.

2

2

2

Expanding the squares on the right hand side and combining terms yields

1

xt+1 − x∗

2≤ 1

xt − x∗

2 − η gˆt, xt+1 − x∗

1 −

xt+1 − xt

2

2

2

2

1 =

xt − x∗

2 − η gˆt, xt − x∗

1 −

xt+1 − x∗

2 − η gˆt, xt+1 − xt .

2

2

Setting µt := Et[gˆt], we successively compute

1 Et

xt+1 − x∗

2≤ 1

xt − x∗

2 − η Etgˆt, xt − x∗

1 − Et

xt+1 − x∗

2 − ηEt gˆt, xt+1 − xt

2

2

2

1 ≤

xt − x∗

2 − η µt, xt − x∗

1 − Et

xt+1 − x∗

2 − ηEt gˆt, xt+1 − xt

2

2

1 =

xt − x∗

2 − ηEt G(xt+1), xt+1 − x∗

1 − Et

xt+1 − x∗

2

2

2

+ η Et gˆt − µt, xt − xt+1 +η Et[ µt − G(xt+1), x∗ − xt+1 ] .

P1

P2

Strong convexity of L(x) implies that G(xt+1), xt+1 − x∗ ≥ α xt+1 − x∗ 2 so that

1 + 2ηα Et

xt+1 − x∗

2≤ 1

xt − x∗

2

−

1 Et

xt+1 − xt

2 + η(P1 + P2).

2

2

2

21

Using Young’s inequality, we upper bound P1 as follows:

P1 ≤ 1 Et gˆt − µt 2 + ∆1Et xt+1 − xt 2

2∆1

2

≤ σ2 + ∆1Et xt+1 − xt 2

2∆1

2

using Assumption 6. Using Yong’s inequality again, we have that

Next observe that

Et µt − G(xt+1) 2 ∆2Et xt+1 − x∗ 2

P2 ≤

+

.

2∆2

2

Et µt − G(xt+1) 2 ≤ 2Et µt − G(xt) 2 + 2Et G(xt) − G(xt+1) 2 ≤ 2C2 + 2G2Et xt − xt+1 2,

where

2

2

n

n

λ

2

C := L · λ W 1(p0) + λ γη · L · (1 + A op) · (1 − λ)2 .

Therefore

2C2 + 2G2 xt − xt+1 2 ∆2Et xt+1 − x∗ 2

P2 ≤

+

.

(10)

2∆2

2

Now we have that

1 + η(2α − ∆2) Et xt+1 − x∗ 2 ≤ 1 xt − x∗ 2 + ησ2 + ηC2 − 1 − 2ηG2∆−2 1 − η∆1 Et xt+1 − xt 2.

2

2

2∆1 ∆2

2

(11)

Setting ∆2 = α and ∆1 = η1 − 2Gα2 ensures the last term on the right hand side is zero. We also have that

η ≤ α/(4G2) implies that ∆1 ≥ 21η . Rearranging (11) we get that

Et xt+1 − x∗ 2 ≤ 1 xt − x∗ 2 + 2η2σ2 + 2ηC2 .

1 + ηα

1 + ηα α(1 + ηα)

Next we verify that our choice of n is large enough so that Cα2 ≤ ησ2. Indeed, this is equivalent to

λnW 1(p0) + λnγ 4 · L · (1 + A op) · λ

αG2

(1 − λ)2

≤ α1/2 η1/2σ L

which is in turn equivalent to

n ≥ log

W

1

(p0

)

+

γ

4 αG2

L(1

+

A

op

)

(1

λ −λ)2

L (αη)1/2σ

1 .
log(1/λ)

Hence, for our choice of n, we have that

Et xt+1 − x∗ 2 ≤ 1 xt − x∗ 2 + 4η2σ2 .

1 + ηα

1 + ηα

Which completes the proof.

D Numerical Simulations
In this section, we start by describing the SFpark data and experiment set-up. Then we provide additional ﬁgures and details for each of the two experiments conducted in the main. Finally, we introduce a synthetic data example which abstracts strategic classiﬁcation in settings where agents have memory.

22

D.1 SFPark Data Description
In this section, we provide more details on our data cleaning strategies and our model for the SFpark dataset.

Data cleaning. We start by discussing our data cleaning strategy. Of the many features in the dataset, the key ones of interest to us were the street name, district name, total time available (number of parking spots multiplied by number of seconds per hour), total time occupied, and rate. Many of the rates were unavailable in the original dataset, but the rate charged for the day before and day after were. If we encountered a missing rate, we replaced it with the rate before and after, if those rates were equal. We only worked with blocks where we could successfully ﬁll in each of the missing rates. This process can be found in the accompanying code.

Estimating price sensitivity. The model we consider is explained in the main body. To provide more intuition and details, as an example, consider the 600 block of Beach Street (Beach ST 600) for the time window between 1200–1500. The initial distribution, d0, is sampled from the data at the initial price for parking along Beach ST 600, which in this case is x0 = $3 per hour. As described in Section 4, we assume that for an announced price diﬀerence of x = x˜ − x0, x˜ is the charged price and x is the variable of optimization. The occupancy follows a distribution of ζ + A(x˜ − x0), where ζ follows the same distribution as p0.
The price sensitivity A is a proxy for the price elasticity, in that it provides us a relationship between the change in price and the change in occupancy mapped to a (0, 1) scale. Indeed, recall that price elasticity is a change in the percentage occupancy for a given change in percentage price. Hence, price sensitivity as we have deﬁned it has the same sign as price elasticity except that it is in the right units of our mathematical abstraction for the problem, and is in this sense a proxy thereof. We compute A by considering the following:

a. The average occupancy for the initial price over every weekday in the beginning of the pilot study until the price is changed.
b. The average occupancy over every weekday in the ﬁnal week of the last price announcement.

As an example, for the 600 block of Beach ST, the initial price was $3.00 per hour and the average occupancy before a new price was announced was approximately 60.6%, the ﬁnal price announced during the pilot study was $4.25, and the average occupancy for the ﬁnal week was approximately 41.1%. Therefore, for the 600 block of Beach ST, we estimate that

0.411 − 0.606

A≈

= −0.156,

4.25 − 3

where occupancy percentage is mapped to the [0, 1] scale. It was shown in Pierce and Shoup (2018) that price elasticity is in general a small negative number on average for the SFpark pilot study and experiment. This is consistent with prior studies on price elasticity for on-street parking where information about price and location plays a crucial role (Fiez and Ratliﬀ, 2020; Glasnapp et al., 2014). However, for the SFpark pilot study, the price elasticity also depends highly on the block and neighborhood.

Estimating geometric decay parameter λ. We also use this data to estimate the geometric decay rate, λ. As described in Section 4, when a new rate is posted, the eﬀect on the occupancy is not immediate, and so the geometric decay rate, λ, in this context represents the speed at which this new announced price travels through the population (and consequently aﬀects the parking occupancy). We group the occupancy data by day of week, in order to account for diﬀerent traﬃc patterns on diﬀerent weekdays. We assume that the week before a new price is announced is the ﬁxed point distribution of the previous rate. For example, for the 600 block of Beach ST, a rate of $3.50 per hour was announced on February 14, 2012, which means that we assumed that the occupancies on February 7–13, 2012 were the ﬁxed point distributions of the previous rate $3.25. We now ﬁx a day of the week (e.g., Monday), a block (e.g., Beach ST 600), and a time window (e.g., 1200–1500). Suppose the prices {xi} are announced and D(xi) represents the ﬁxed point distribution

23

Price (dollars)

Price (dollars)

4.0

BEACH ST 500 FO

4.0

BEACH ST 500 ZO

BEACH ST 500 FO 100

BEACH ST 500 ZO 100

3.5 3.5 80 80

Occupancy (%)

Occupancy (%)

Price (dollars)

3.0

3.0

60

60

2.5

2.5

40

40

2.0

2.0

20

20

1.5 0 25 50 75 100 125 1.5 0 25 50 75 100 125

Iterations (weeks)

Iterations (weeks)

0 25 50 75 100 125 Iterations (weeks)

0 25 50 75 100 125 Iterations (weeks)

n=1,T=120

n=4,T=30

n=8,T=15

n=15,T=8

SF Park

PO Price

Desired Occupancy

5.0

BEACH ST 700 FO

5.0

BEACH ST 700 ZO

90

BEACH ST 700 FO

90

BEACH ST 700 ZO

4.5

4.5

80

80

Occupancy (%)

Occupancy (%)

Price (dollars)

70

70

4.0

4.0

60

60

3.5

3.5

50

50

3.0

3.0

40

40

2.5 0 25 50 75 100 125 2.5 0 25 50 75 100 125 30 0 25 50 75 100 125 30 0 25 50 75 100 125

Iterations (weeks)

Iterations (weeks)

Iterations (weeks)

Iterations (weeks)

n=1,T=120

n=4,T=30

n=8,T=15

n=15,T=8

SF Park

PO Price

Desired Occupancy

4.0

BEACH ST 800 FO

3.5

4.0

BEACH ST 800 ZO

3.5

BEACH ST 800 FO 100
90

BEACH ST 800 ZO 100
90

Occupancy (%)

Occupancy (%)

Price (dollars)

3.0

3.0

80

80

2.5

2.5

70

70

2.0

2.0

60

60

50

50

1.5 0 25 50 75 100 125 1.5 0 25 50 75 100 125

0 25 50 75 100 125

0 25 50 75 100 125

Iterations (weeks)

Iterations (weeks)

Iterations (weeks)

Iterations (weeks)

n=1,T=120

n=4,T=30

n=8,T=15

n=15,T=8

SF Park

PO Price

Desired Occupancy

Figure 3: Results of Algorithm 2 (ﬁrst and third plots of each row) and Algorithm 1 (second and fourth plots of each row) with diﬀerent (n, T ) pairs for the 500, 700 and 800 blocks of Beach ST for time window 1200–1500. Each marker represents a price announcement, and the plots show the prices and corresponding predicted occupancies. The SFpark prices and occupancies are far from the target and performatively optimal price, whereas the proposed algorithms obtain both points up to theoretical error bounds.

Price (dollars)

Figure 4: Map of Beach Street showing blocks 500 to 800. The tourist attractions Ghiradelli Square, Fisherman’s Wharf, and the Embarcadero are also depicted.
of announcing xi, where the price xi is in eﬀect for Ki weeks. Then, for the k-th week after announcing xi, we assume that the occupancy is represented by λkD(xi−1) + (1 − λk)D(xi). For each week k, and for price xi, the occupancy for the speciﬁed day is represented as zi,k. To ﬁnd the value of λ, for the speciﬁed day
24

and block, we solve the following optimization problem:

minimize
λ∈[0,1]

Ki
(λkD(xi−1) + (1 − λk)D(xi) − zi,k)2.
i k=1

We perform projected gradient descent to solve this problem. For the ﬁnal value of λ that we use for the speciﬁed block, we average the estimated values of delta for each day.

D.2 Comparing Performative Optimum to SFpark
Here, we provide experiments for other blocks on Beach Street (beyond just the 600 block in Section 4). Each row in Figure 3 shows prices and corresponding occupancies for Algorithm 2 and Algorithm 1 for the 500, 700, and 800 blocks of Beach ST, respectively. In each instance, we make similar observations to those in Section 4 for the 600 block on Beach ST, namely, that SFpark consistently overshot the price to reach the target occupancy, and that the choice of n = 8 is reasonable, in that a time period of 8 weeks is suﬃcient for the population to equilibriate before announcing a new price.
An interesting observation from Figure 3 comes from the fact that the 500 block of Beach ST has a price sensitivity of A ≈ −0.844, and the 800 block of Beach ST has a price sensitivity of A ≈ −0.424. Since both of these values have large magnitudes, we observe that for a small price reduction, the estimated occupancy increases to 100%. Therefore, for blocks where the magnitude of the price sensitivity is large, our experiments suggest using a smaller choice of n, and consequently a larger choice of T , in order to reduce the variance for the price announcements to prevent having large ﬂuctuations in occupancy. All four of the blocks on Beach Street have very similar estimated λ values. Table 1 indicates that each block adjusts to new price

Beach ST Block Number
500 600 700 800

(estimated) ≈ λ value
0.993 0.959 0.993 0.984

Table 1: Estimated decay rate λ for each block along Beach ST pictured in Figure 4.

announcements at similar rates. This makes sense given that each of the blocks are on the same street all next to each other as seen in Figure 4, and located near similar landmarks and attractions.

D.3 Redistributing Parking Demand
In this appendix subsection, we describe the details for the experiment on redistributing parking demand. The study includes the four connected blocks of Hawthorne ST 0, Hawthorne ST 100, Folsom ST 500, and Folsom ST 600 because the blocks are adjacent to one another as shown in Figure 2. Thus, we wanted to investigate whether price changes would redistribute the traﬃc such that each block had an occupancy closer to the target of 70%. An interesting note is that while Folsom ST 500 and Folsom ST 600 both have negative price sensitivity values of of A ≈ −0.399 and A ≈ −0.284 respectively, Hawthorne ST 0 and Hawthorne ST 100 have positive price sensitivity values of A ≈ 0.454 and A ≈ 0.044 respectively. Since Hawthorne ST has a very high initial average occupancy, SFpark should consider decreasing prices on this street in order to shift demand to the nearby streets. This is exactly what we see done by both Algorithms 1 and 2 so that both streets are closer to the target occupancy. Although the price sensitivity is very diﬀerent for these blocks, the estimated λ values are very similar. Hawthorne ST 0 has λ ≈ 0.853, Hawthorne ST 100 has λ ≈ 0.979, Folsom ST 500 has λ ≈ 0.996, and Folsom ST 600 has λ ≈ 0.793, so each block adjusts to new price announcements at similar rates.

25

D.4 Synthetic Data: Strategic Classiﬁcation in Dynamic Environments

In this appendix subsection, we apply our algorithm to a synthetic strategic classiﬁcation problem—which was considered in the dynamic setting in Brown et al. (2020) and in the static setting in Drusvyatskiy and Xiao (2020); Miller et al. (2021); Perdomo et al. (2020), e.g.—where there is memory in the agent population. For simplicity (and to support visualization of the classiﬁer performance), each data point contains a feature vector, φi ∈ R2, and a corresponding label, yi ∈ {−1, 1} where i ∈ {1, . . . , m} and m is the number of strategic users. The loss incurred by the institution is given by an 2-regularized logistic loss:

1m

ν

−yi x, φi + log(1 + exp( x, φi )) + x 2,

2

2

i=1

where we set m = 1000. The agents are non-strategic (meaning they do not perturb their true feature vector φ¯i) if they have label yi = 1, and otherwise ‘best respond’ to the announced classiﬁer according to the model

φi = arg max − x, w

1 −

w − φ¯i 2 = φ¯i − ˜x.

(12)

w

2˜

We take ˜ = 0.1, but the observations we make hold more generally with the exception of very large magnitude

perturbations for which the problem (even in the static setting) becomes untenable. We randomly select a

subset of the two features to treat as strategic. We also randomly generate a ground truth data set by

drawing m × 2 samples from a normal distribution, drawing the ground truth φgt from a (2 dimensional)

normal distribution and then assigning labels according to

yi = (sign(φi φgt + 0.1v) + 1)/2, v ∼ N (0, 1).

Speciﬁcally, agents are allowed to perturb in the x1 direction as can be seen in Figure 5. Moreover, we take the initial data distribution p0 to be far from the base distribution for users’ true preferences φ¯i even with performative eﬀects; speciﬁcally, p0 is a Gaussian distribution with a mean of 1.0 and scale (standard deviation) of 45. More details on the implementation can be found in the accompanying code.
We divide the data into a training and test set with a (2/3)–(1/3) split. We set the regularization parameter to ν = 1/mtrain where mtrain is the size of the training data set. For (12), the inner product can be interpreted as the utility of the agent and the norm diﬀerence as the cost of manipulation. We present results for a modest value of n = 20; similar or lower values are consistent with our observations and as our theory suggests, as n → ∞, the solution obtained by Algorithm 2 approaches the performatively optimal solution.

3

Classifer ( , n)=(0.5, 20)

3

Classifer ( , n)=(0.95, 20)

3

Classifer ( , n)=(0.99, 20)

3

Ground Truth

2

2

2

2

1.4

1

1

1

1

1.2

0

0

0

0

1.0

1

1

1

1

PO = 0.5, n = 20 = 0.95, n = 20 = 0.99, n = 20

2

2

2

2

0.8

32

x01

2 PO

32 y=1

x01 y= 1

2

3

( , n)=(0.5,20)

2

x01

( , n)=(0.95,20)

2

3 3 2 1 x01 1 2 3 0.6

( , n)=(0.99,20)

0

200 400 600 800 1000 iterations

Figure 5: Classiﬁers and losses for diﬀerent values of λ and n = 20. In order of appearance from left to right, the ﬁrst three plots show the learned classiﬁers with the data at the distribution D(x) induced by the learned classiﬁer for (λ, n) = (0.5, 20), (λ, n) = (0.95, 20), (λ, n) = (0.99, 20). The fourth plot from the left is the ground truth data distribution without performative eﬀects. The diﬀerences in the data distributions are subtle, but one can see that the diﬀerent learned classiﬁers evoke diﬀerent responses from the strategic users. The far right plot shows the losses as a function of iterations.

We explore diﬀerent values of λ and n—i.e., the mixing parameter of the geometric dynamics and the epoch length of Algorithm 2—on not just convergence but also on accuracy. The observations we report

x2 x2 x2 x2

26

( , n)=(0.5,20)

True Neg 120
40.68%

False Pos 14
4.75%

( , n)=(0.95,20)

True Neg 126
42.71%

False Pos 11
3.73%

( , n)=(0.99,20)

True Neg 133
45.08%

False Pos 3
1.02%

Performatively Optimal

True Neg 120
40.68%

False Pos 14
4.75%

False Neg 26
8.81%

True Pos 135
45.76%

0

1

Predicted label

Accuracy=0.864

False Neg 20
6.78%

True Pos 138
46.78%

0

1

Predicted label

Accuracy=0.895

False Neg 13
4.41%

True Pos 146
49.49%

0

1

Predicted label

Accuracy=0.946

False Neg 26
8.81%

True Pos 135
45.76%

0

1

Predicted label

Accuracy=0.864

Figure 6: Accuracy of the classiﬁers (via confusion matrix) learned for the data distribution and setting shown in Figure 5. For this randomly sampled data distribution, λ plays a signiﬁcant role on the generalization capability (as measured by accuracy on the test set). Surprisingly, accuracy improves as the mixing parameter λ increases (meaning longer time to mix) and this also has an impact on auxiliary but related metrics such as the false positive and false negative rates. This observation depends highly on the data distribution, but exposes interesting directions for future theoretical work on understanding how performative optimality translates to generalization and robustness guarantees.

actually lead to a number of interesting open questions for this ﬁeld including how performative optimality relates to generalization. We ﬁnd that depending on the skew of the data distribution and the strength of the perturbation power of the strategic agents—namely, ˜—that surprisingly, the performatively optimal point may not generalize very well as compared to the solution obtained by Algorithm 2 when the mixing parameter λ is large. The latter has better accuracy as can be seen in Figure 6; the loss value per iteration and the classiﬁers for diﬀerent λ values are shown in Figure 5.
In other settings (e.g., with diﬀerent ground truth data), the solution obtained by Algorithm 2, even with diﬀerent values of λ and diﬀerent choices of epoch length n, performs just as well as the performatively optimal solution as depicted in Figure 8, the data for which has original distribution depicted in Figure 7, which also contains the learned classiﬁers and losses per iteration for diﬀerent λ values.

3

2

1

0

1

2

2

1 x10

1

2

PO y=1 y= 1 ( , n)=(0.5,20) ( , n)=(0.95,20) ( , n)=(0.99,20)

1.4 1.2 1.0 0.8 0.6 0.4
0

PO = 0.5, n = 20 = 0.95, n = 20 = 0.99, n = 20
200 400 600 800 1000 iterations

Figure 7: Classiﬁers and losses for diﬀerent values of λ and n, for the given original data distribution shown in the far right plot. (left) Diﬀerent classiﬁers (as a function of λ and n) and the data distribution given the strategic best response at the performatively optimal point. (center) Losses for the diﬀerent (λ, n) pairs as a function of iteration. (right) original data distribution and ground truth classiﬁer.

These observations about the generalization performance of the obtained solution under our proposed algorithm (for diﬀerent values of the geometric process or mixing constant λ) as compared to the (performatively) optimal point, while highly dependent on the underlying data distribution, open up a number of interesting directions for future work on understanding precisely when the optimal point gives good generalization and robustness guarantees.

x2

27

2-distance from PO point

( , n)=(0.5,20)

True Neg 86
53.09%

False Pos 0
0.00%

( , n)=(0.95,20)

True Neg 86
53.09%

False Pos 0
0.00%

( , n)=(0.99,20)

True Neg 86
53.09%

False Pos 0
0.00%

Performatively Optimal

True Neg 86
53.09%

False Pos 0
0.00%

False Neg 4
2.47%

True Pos 72
44.44%

0

1

Predicted label

Accuracy=0.975

False Neg 4
2.47%

True Pos 72
44.44%

0

1

Predicted label

Accuracy=0.975

False Neg 4
2.47%

True Pos 72
44.44%

0

1

Predicted label

Accuracy=0.975

False Neg 4
2.47%

True Pos 72
44.44%

0

1

Predicted label

Accuracy=0.975

Figure 8: Accuracy of the classiﬁers (via confusion matrix) learned for the data distribution and setting shown in Figure 7. For this randomly sampled data distribution, the value of λ does not play a signiﬁcant role on the generalization capability (as measured by accuracy on the test set). Accuracy remains the same across the learned classiﬁers in each setting.

=0.01

=0.1

=1

=10

2.5

2.5

2.5

2.5

2.0

2.0

2.0

2.0

1.5

1.5

1.5

1.5

1.0

1.0

1.0

1.0

0.5

0.5

0.5

0.5

0.0

0.0

0.0

0.0

0 200 400 600 800 1000 0 200 400 600 800 1000 0 200 400 600 800 1000 0 200 400 600 800 1000

Iterations

Iterations

Iterations

Iterations

FO: n=1,T=1000

FO: n=5,T=200

FO: n=20,T=50

FO: n=100,T=10

RGD: n=100,T=10

RRM: n=100,T=10

Figure 9: ‘Give Me Some Credit’ Experiment 1: Results of Algorithm 2 called with diﬀerent (n, T ) pairs along with standard implementations of repeated risk minimization (RRM) and repeated gradient descent (RGD) wherein the dynamics and classiﬁer are updated at each iteration. Each marker represents a new x announcement, and the plots show the Euclidean distance from the performatively optimal point. Algorithm 2 converges to the performatively optimal point for each value of ε˜ while RRM and RGD converge to the performatively stable point. The latter may be far from the performatively optimal point for large perturbation values ˜ as indicated in the plot, going from left to right.

D.5 Semi-Synthetic Data: Strategic Classiﬁcation in Dynamic Environments
As a point of comparison to the existing literature, we perform additional numerical experiments on a strategic classiﬁcation simulator from the Kaggle Give Me Some Credit dataset discussed in Perdomo et al. (2020) and Brown et al. (2020). In this dataset, each data point contains a feature vector, φ ∈ Rd, which represents historical information about an individual, and the label, y ∈ {0, 1}, which represents whether or not the individual has defaulted on a loan. For more details on the dataset itself, see Appendix B.2 in Perdomo et al. (2020).
Let S be the subset of features that an individual can strategically manipulate. We assume that the best response of every individual to an announced x is given by φS − ε˜xS, where we use the notation xS to be the restriction of x to the subset S and similarly for φS. The remaining features of the individual stay the same as the original data.
We conduct two sets of experiments. In the ﬁrst set, we compare our algorithm on the total number of iterations—i.e., epochs n multiplied by T —to risk minimization (RRM) (Brown et al., 2020; Perdomo et al., 2020), and repeated gradient descent (RGD) (Perdomo et al., 2020)—implemented for the dynamic environment which was not considered in Perdomo et al. (2020)—both of which, notably update x at every iteration in [0, nT ] where as our approach (Algorithm 2) only updates at every n steps in that same interval.
In the second set of experiments, we compare our approach to an epoch based implementation of both

28

2-distance from PO point

2-distance from PO point

=0.01

=0.1

=1

2.5

2.5

2.5

=10 2.5

2.0

2.0

2.0

2.0

1.5

1.5

1.5

1.5

1.0

1.0

1.0

1.0

0.5

0.5

0.5

0.5

0.0

0.0

0.0

0.0

0 2000 4000 6000 8000 10000 0 2000 4000 6000 8000 10000 0 2000 4000 6000 8000 10000 0 2000 4000 6000 8000 10000

Epochs

Epochs

Epochs

Epochs

FO: n=10,T=10000

RGD: n=10,T=10000

RRM: n=10,T=10000

=0.01

=0.1

=1

=10

2.5

2.5

2.5

2.5

2.0

2.0

2.0

2.0

1.5

1.5

1.5

1.5

1.0

1.0

1.0

1.0

0.5

0.5

0.5

0.5

0.0

0.0

0.0

0.0

0 200 400 600 800 1000 0 200 400 600 800 1000 0 200 400 600 800 1000 0 200 400 600 800 1000

Epochs

Epochs

Epochs

Epochs

FO: n=100,T=1000

RGD: n=100,T=1000

RRM: n=100,T=1000

Figure 10: ‘Give Me Some Credit’ Experiment 2: Results of Algorithm 2 compared to epoch-based implementations of RRM and RGD—i.e., where in each epoch the dynamics are updated n times with the same classiﬁer deployed—each called with (n, T ) ∈ {(10, 1000), (100, 1000)}. Each marker represents a new x announcement, and the plots show the Euclidean distance from the performatively optimal point.

RRM and RGD where in these implementations the dynamics are also allowed to “mix” and the decision maker updates only every n steps as in our method. These later experiments are more comparable even though the epoch based implementations of RRM and RGD have not been studied theoretically. For both experiments, we plot the 2 distance to the optimal point.
Experiment 1: Comparison to Iteration-Based (Classical) RRM and RGD. Figure 9 shows the results of the ﬁrst set of experiments, for which we have taken λ = 0.9, which is relatively large meaning that the mixing time for the geometric process is large. Neither RRM nor RGD target the performatively optimal point, but instead the performatively stable point, i.e., the point at which repeated retraining will stabilize. As shown in Figure 9, a performatively stable point (the point RRM was shown to converge to in Brown et al. (2020)) may be far from the peformatively optimal point. Interestingly, we also observe that for small values of ε˜ (i.e. on the order of 1e-2), the performatively optimal point and the performatively stable point are very close, and so RGD behaves nearly identically to calling Algorithm 2 with n = 1. This seems to imply that when performative eﬀects (i.e., size of ε˜ in this set of experiments) are very low, the na¨ıve strategies of RRM or RGD suﬃce when trying to ﬁnd the optimal point. On the other hand, for values of ε˜ on the order of 1e-1 or larger, RRM and RGD do not converge to the performatively optimal point while Algorithm 2 does, albeit with worse iteration complexity to convergence to the stable point of the respective algorithm.
Experiment 2: Comparison to Epoch-Based RRM and RGD. Figure 10 shows the results of the second set of experiments. As noted above, in this set of experiments, we compare to epoch based implementations of RRM and RGD to Algorithm 2 which is also an epoch-based algorithm, the idea here being that these are more comparable algorithms in a sense. As can be seen in Figure 10, the observations are analogous to the ﬁrst set of experiments. Epoch-based RRM and RGD converge to the performatively stable point (as deﬁned in (Perdomo et al., 2020) and (Brown et al., 2020), for the dynamic setting). For ε˜ on the order of 1e-2, the performatively stable point is close to the performatively optimal point (although still not equal to it), and for ε˜ on the order of 1e-1 or larger, the performatively stable point is considerably farther away from
29

the performatively optimal point. On the other hand, Algorithm 2 converges to the performatively optimal point for all shown values of ˜, the size of the strategic perturbation.
We note that we did not compare to the zero-th order method since it has diﬀerent information than both the RRM and RGD and is thus less comparable. We expect the same observations about non-convergence of RRM and RGD for large ε˜ to persist and Algorithm 1 will converge as the theory predicts, albeit at a much slower rate than Algorithm 2 due to the bandit feedback.
30

