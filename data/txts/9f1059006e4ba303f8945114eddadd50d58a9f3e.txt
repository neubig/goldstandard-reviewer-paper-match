Neural Query Language: A Knowledge Base Query Language for Tensorﬂow

arXiv:1905.06209v1 [cs.LG] 15 May 2019

William W. Cohen Google Research wcohen@google.com

Matthew Siegler Google Research msiegler@google.com

Alex Hofer Google
rofer@google.com

Abstract
Large knowledge bases (KBs) are useful for many AI tasks, but are difﬁcult to integrate into modern gradient-based learning systems. Here we describe a framework for accessing soft symbolic database using only differentiable operators. For example, this framework makes it easy to conveniently write neural models that adjust conﬁdences associated with facts in a soft KB; incorporate prior knowledge in the form of hand-coded KB access rules; or learn to instantiate query templates using information extracted from text. NQL can work well with KBs with millions of tuples and hundreds of thousands of entities on a single GPU.
1 Introduction
Large knowledge bases (KBs) are useful for many AI tasks, but are difﬁcult to integrate into modern gradient-based learning systems. Here we describe the Neural Query Language (NQL), a framework for accessing soft symbolic databases using only differentiable operators from Tensorﬂow [1]. NQL is a dataﬂow language, implemented in Python and Tensorﬂow, that provides differentiable operations over (sets of) entities and relations in a KB. NQL makes it easy to conveniently write neural models that perform actions that are otherwise difﬁcult. For instance, a model can adjust conﬁdences associated with facts in a symbolic KB; incorporate prior knowledge in the form of hand-coded KB access rules; learn new KB access rules, thus implementing a variant of inductive logic programming.
NQL can also be used in a system that will learn to answer natural-language queries against a KB in a fully end-to-end manner, trained using examples consisting of a natural-language query input and a entity-set output. For example, a question like “who was the father of Queen Victoria’s husband?” might require the following steps to answer:
1. Find the KB entity e1 corresponding to “Queen Victoria” in a KB, and ﬁnd the KB relations r1 and r2 that correspond the “husband” and “father of”.
2. Use the KB to ﬁnd the entity or entities e2 that are related to e1 via the relationship r1, and then ﬁnd the entity or entities e3 that are related to e1 via the relationship r3.
Neural networks can be trained to perform the ﬁrst step above: ﬁnding e1 is an entity-linking task and ﬁnding r1 and r2 is a relation extraction task. Using NQL, the second step can also be performed with differentiable operators. This means that the loss between the predicted answer (in this case e3) and the desired answer can be backpropagated all the way to the entity-linking and relation extraction networks.
2 Related Work
NQL is closely related to TensorLog [2], a deductive database formalism which also can be compiled to Tensorﬂow. In fact, NQL was designed so that every expression in the target sublanguage used

by TensorLog can be concisely and readably written in NQL. TensorLog, in turn, has semantics derived from other “proof-counting” logics such as stochastic logic programs (SLP) [3]. TensorLog is also closely related to other differentiable ﬁrst-order logics such as the differentiable theorem prover (DTP) [5], in which a proof for an example is unrolled into a network. DPT includes representationlearning as a component, as well as a template-instantiation approach similar to the one used in NQL. TensorLog and NQL are more restricted than DPT but also more scaleable: the current version of NQL can work well with KBs with millions of tuples and hundreds of thousands of entities, even on a single GPU.
NQL however is not a logic, like TensorLog, but a dataﬂow language, similar in spirit to Pig [4] or Spark [7]. NQL also includes a number of features not found in TensorLog, notably the ability to have variables that refer to relations. NQL also makes it much easier for Tensorﬂow models to include pieces of NQL, or for NQL queries to call out to Tensorﬂow models.
NQL is one of many systems that have been built on top of Tensorlog or some other deep-learning platform. Perhaps the most similar of these in spirit is Edward [6], which like NQL, attempts to add a higher-level modeling language based on a rather different programming paradigm: most other packages are aimed at providing additional support for training, or combining existing Tensorﬂow operators into reusable fragments. In the case of Edward, the alternative paradigm being supported is probabilistic programming (e.g., variational autoencoder modes), while in Tensorlog, the alternative paradigm supported is dataﬂow operations on KGs.
3 NQL: A Neural Query Language
3.1 Preliminaries
NQL allows one to query a KB of entities and relations. An (typed) entity e has a type type(e), and an index i(e), which is an integer between 1 and Ntype(e), where Nτ is the number of entities of type τ . Types and entities both have names, which are readable strings describing them: the name of an entity e, for instance, will be written name(e) below. We assume that names and indices for entities are unique within a type, so if type(e) = type(e ) and either i(e) = i(e ) or name(e) = name(e ), then it must be that e = e .
A weighted relation π with domain type τ1 and range type τ2 is a weighted multiset of pairs of entities (e1, e2) such that type(e1) = τ1 and type(e2) = τ2. NQL currently supports only binary relations. Relations can be thought of as weighted edges from nodes of type τ1 to nodes of type τ2. A weighted relation π can be encoded as a (possibly sparse) matrix Mπ ∈ RNτ1 ×Nτ2 . Relations also have string names.
A KB is a pair (Π, E) where Π = π1, . . . , πNΠ is a set of relations, and E is a set of typed entities.
NQL also makes use of weighted multisets of typed entities. A weighted multiset σ of type τ is a mapping from entities of type τ to non-negative real numbers, which we will write in a Python-like notation, e.g. {blue:0.9, red:1.0}. Entities of type τ not explicitly listed in this notation are assumed to map to zero. A weighted multiset σ of type τ can be encoded as a (possibly sparse) vector vσ ∈ RNτ , where vσ[i(e)] = σ(e).
3.2 Simple NQL expressions
NQL is a simple KB query language embedded in Python. Some NQL expressions are produced using an NQL context object, which contains pointers to a KB. Below I will use the variable c for an NQL context object, and assume it has been initialized with a database derived from a widely-used example database of geneology information about European royal families1 from which we have derived 12 familial relations named aunt, brother, daughter, father, husband, mother, nephew, niece, sister, son, uncle, and wife. This KB has only one type, person_t, and all the relations/edges have unit weight.
One can create singleton, unit-weighted sets using a call to c.one, for example:
henry8 = c.one(‘Henry_VIII of house of Tudor’, ’person_t’)
1The dataset is widely distributed as an example of the GED format, for example under https://github.com/jdfekete/geneaquilt
2

NQL expression

Vector-matrix speciﬁcation Comments

s.rel() ≡ sMπ
s.rel(-1) ≡ sMTπ s | t ≡ s+t

s&t ≡ s t

is Hadamard product

s.follow(r) ≡ s

k i=1

r[i]Mπi

s.if_any(t) ≡ s||t||1

s * a ≡ sa

a is a Tensorﬂow scalar

Table 1: Matrix-vector implementation for NQL operators. Vectors s, t, r correspond to s, t, r respectively, r is over the relation group π1, . . . , πk, and Mπ corresponds to the relation rel.

Evaluating and printing the NQL expression henry8 would yield the multiset {’Henry_VIII of house of Tudor’: 1.0}. There are two other primitive set-constructions methods for contexts, c.none, which creates an empty set of a given type, and c.all, which creates a universal unit-weighted set of a given type.
Every relation π can be accessed by simply using the name of that relation as a method of any multisetvalued expression. For instance henry8.wife() would evaluate to a set of six people, {‘Anne of house of Boleyn’:1.0, ‘Anne of_Cleves’:1.0,, . . . , ‘Jane of house of Seymour’:1.0}. Relations can also be chained: for example, the set of sons of Henry VIII’s daughters could be written as henry8.daughter().son().
One can also reference a relation by a string “r” that names it with the syntax s.follow(‘r’): for instance henry8.follow(‘wife’) gives the same set as above. The inverse of a relation can by accessed by adding an argument -1 to a relation-name method: for example, Henry VIII’s parents could be found with the expression henry8.son(-1).
Unions and intersections of multisets of the same type can be computed using the operators “|” for union and “&” for intersection. For example, the set of henry VIII’s grandsons could be written as
(henry8.son() | henry8().daughter()).son()
Since the language is embedded in Python, one can use Python’s function deﬁnition construct to deﬁne NQL functions. As an example, this deﬁnition
def child(x): return son(x) | daughter(x)
would allow one to re-express the set deﬁned above as child(henry8).son(). (One can also deﬁne new multiset methods, so that the notation henry8() could be used, by subclassing the NQL context class.)
3.3 Conditionals, Predicate Variables, and Rule Templates
NQL also has a conditional construct. If s and t are multisets then s.if_any(t) returns exactly the set s if t is a singleton multiset with weight one on its only element, and returns the empty set of t is an empty set. More generally, s.if_any(t) will return a copy of s in which every element has been multiplied by a factor f , where f is the sum of all the weights of the members of t. This operation is best described using the vectors s and t corresponding to s and t respectively: the vector s corresponding to s.if_any(t) is simply s ≡ s||t||1. This deﬁnition, along with the deﬁnitions of

Figure 1: Example: Using NQL to compute the six wives (b) and twelve in-laws (c) of King Henry VIII (a). The variable sess is bound to a Tensorﬂow Session object.
3

the other NQL operators, is shown in Table 1. It is also possible to obtain a similar conditional effect with the notation s * a where a is a Tensorﬂow scalar.
NQL also includes a construct which allows one to construct variables which range over relations. Any set of relations π1, . . . , πk with the same domain and range types can be gathered together into relation group g. This creates a new type τg with Nτg = k elements, whose entity members have the same names as the relations π1, . . . , πk. One can then use the same NQL constructs to create weighted multisets of relations.
For example, the rel_t is a type for all of the relations in this KB, one could create the multiset
child = c.one(’daughter’, ’rel_t’) | c.one(’son’, ’rel_t’)
If r is a multiset of relation-naming entities, then the syntax s.follow(r) also lets you “follow” a group of relations. So in this example, henry8.follow(child) would evaluate to the set of all daughters and sons of Henry VIII. More generally, the weights associated with each relation in r are combined multiplicatively with any weights associated with the edges in the KB itself: a deﬁnition for this operator is also shown in Table 1.
4 Learning and Rule Templates
4.1 NQL and Tensorﬂow
NQL is tightly integrated with Tensorﬂow. Every NQL expression is attached to a context object c. The context object has sufﬁcient information to produce an appropriate Tensorﬂow compute-graph node which is an implementation of the NQL expression. These Tensorﬂow expressions are computed bottom-up.
If x is an NQL expression, one can access the underlying Tensorﬂow implementation using the syntax x.tf. If τ names an NQL type and c is an NQL context, and if w is a compatible Tensorﬂow Tensor or Variable object, then c.as_nql(w, τ ) converts w to an NQL expression. (By “compatible” here we mean that w contains a tensor of the right shape, i.e., it contains a minibatch of vectors in RNτ .) This makes it relatively simple to convert back and forth between NQL and Tensorﬂow, so models can easily include Tensorﬂow submodels (e.g., an LSTM to encode represent text) as well as NQL templates.
The current implementation of NQL can handle KBs with a few million tuples and types with a few hundred thousand entities on a single commodity GPU.
4.2 Learning with NQL
Having variables that can be bound to multisets of relations makes it possible to write relatively generic “template” queries. For instance, in the family-relations domain, many of the relations can be approximated with the union of a small number of queries that each chain together two other relations: e.g., x.father() is approximately the same as x.mother().husband(), and x.daughter() is approximately the same as x.daughter().sister() | x.son().sister().
If one wanted to learn to approximate a new familial relation with the ones in this dataset, one might pose the following learning problem: learn values for the multiset relation variables r1, r2, r3 and r4 for the query:
x.follow(r1).follow(r2) | x.follow(r3).follow(r4)
This “template” could be turned into an approximation of father by setting r1=r3={mother:1.0}, r2=r4={husband:1.0}, or into an approximation of daughter by setting r1={daughter:1.0},r3={son:1.0}, r2=r4={sister:1.0}. Templates reduce the difﬁcult problem of searching a discrete space of possible queries to the more tractable problem of searching a continuous space of weights inside a multiset.
Using this template, a very minimal approach to learning rules for an unknown predicate π∗ would be the following. (We assume the rules will take as input a domain entity x and output a set of things related to x via π∗.) First, let r1, . . . , r4 be NQL relation variables derived from Tensorﬂow Variables with shape k, where k is the number of relations. Second, let the models prediction y be deﬁned using the template above, i.e., let
4

Input: an entity x; Output: entity-set y so that y = {y : π(x, y)} def trainable_rel_var(): return c.as_nql(tf.Variable(tf.ones_initializer()[k])) r1 = trainable_rel_var() r2 = trainable_rel_var() r3 = trainable_rel_var() r4 = trainable_rel_var() y = x.follow(r1).follow(r2) | x.follow(r3).follow(r4) loss = (y.tf, target_labels)
Inputs: a question q containing an entity e; Output: entity-set y answering the question q. rel = c.as_nql(f (q)) y = e.follow(rel) loss = (y.tf, target_labels)
Inputs: a question q containing an entity e; Output: entity-set y answering the question q. r1 = c.as_nql(f1(q)) r2 = c.as_nql(f2(q)) switch1 = f3(q) switch2 = f4(q) y = e.follow(r1) * switch1 | e.follow(r1).follow(r2) * switch2 loss = (y.tf, target_labels)
Table 2: Some example learning based on NQL templates. In each of these the f ’s are differentiable functions of a textual query q, e.g., based on an encoder-decoder approach.
Input: an initial entity e and encoded state s; Output: entity-set y p = 1; y = tf.zeros(k) for i in range(MAX_HOPS): s, r, p_stop = f (s) e = e.follow(r) y += p * p_stop * e.as_tf() p = p * (1 - p_stop)
Table 3: An example learning using NQL without templates. Here f is a recurrent, differentiable function of a state vector s which returns a new state vector s, a distribution over relations r, and a probability of stopping p.
y = x.follow(r1).follow(r2) | x.follow(r3).follow(r4)
Third, deﬁne an appropriate loss function on y and train. This learns a deﬁnition of the predicate in terms of the values of r1, . . . , r4. This approach is summarized on the top of Table 2.
Another example use of templates is for question-answering against a KB. For example, consider simple questions of the form “Who was the father of Queen Victoria?” which ask for entities in some particular relation (e.g., father) to a speciﬁc “seed entity” e appearing in the question (e.g., ’Victoria of house of Hanover’). If there is an entity-linking system that can extract the appropriate entity e from a question q, then a simple question-answering system can be deﬁned using the model in the second panel of Table 2. Here f would be an arbitrary differentiable function of q, e.g., based on decoding an LSTM to the relation variable.
Clearly, these approaches could be combined to consruct models for more complex, multi-hop, compositional queries, like “who was the father of Queen Victoria’s husband?”, as shown in the bottom panel of Table 2. Here f1 . . . f4 are based on decoding q, and switch1 and switch2 are “switches” which select whether a one-hop or two-hop query was selected.
4.3 Learning without Templates
Consider the question answering task described at the end of Section 4.2 with a given seed entity e and potentially multiple relations necessary to reach the target. As shown above this task can be learned using templates. However, this can grow unwieldy as the depth of reasoning increases.
5

Instead, one could take advantage of the structure inherent in these templates and build a model which learns over an entire family of templates. Each of the templates mentioned in the question answering task represents a series of relations followed sequentially where the relations to follow and the order to follow them in are what the model is learning.
We can use a recurrent model to predict any chain of relations up to an arbitrary length. Consider a recurrent model si, ri, pi = f (si−1) where si is the state at step i, ri is a predicted relation for step i, pi is the probability of stopping at step i, and s0 is an encoding of the query.
Using this model a ﬁnal prediction can be calculated by weighting predictions from all number of steps up to some maximum as shown in Table 3.
Representing the problem in this way has the advantage that it allows for generalization to questions which require deeper reasoning than seen at training time. For example, a model trained on data which requires following up to 5 relations may successfully return answers requiring 6 or more relations to be followed.
This approach may be extended further to cover other more complex families of templates.
5 Advantages and Disadvantages of NQL
5.1 NQL vs. TensorLog
NQL’s implementation is fairly thin: it is implemented by having NQL expressions converted directly to Tensorﬂow computation graphs. During this conversion process NQL also enforces type checking (e.g., to ensure that the τ ’s for the domain and range of each relation is consistent). The eval method for NQL expressions makes use of backpointers to the context to allow conversion to the symbolic names of entities.
As Table 1 shows, NQL’s operations can be concisely speciﬁed with matrix-vector computations: it might be asked how much additional value the NQL abstraction provides over Tensorﬂow. We should note that the actual implementation of NQL’s operators are less concise than the speciﬁcation, for a number of reasons.2 There are also a number of plausible ways to implement the s.follow(r) operation, and using this higher-level notation allows one to choose between them easily as a conﬁguration option.
5.2 NQL vs. SQL (or SPARQL or OWL or ....)
Compared to more traditional KB query languages, NQL has a major limitation, in that it cannot construct and return tuples, only weighted sets of entities. This is a direct consequence of the decision to base NQL on differentiable vector-matrix operations, which do not support creating new objects (such as tuples).
This limitation seems to make it impossible for NQL to perform a number of familiar DB operations, such as joins. Consider two tables student and grade, student having ﬁelds id, program, and expected_degree and grade having ﬁelds student_id, course_id, letter_grade. Consider an SQL join query like
SELECT grade.id FROM student, grade WHERE student.id = grade.student_id AND student.expected_degree = ’PhD’ AND grade.letter_grade = ’C’
which asks for PhD students that have gotten a C in some course. Although this seems to need more power than NQL has, it can be emulated by incorporating into the KB new structures which act as indexes for the relations. In this case, one could construct a student_record type and a grade_record type, and deﬁne relations such as student_record_id, mapping student_record entities to the appropriate id value, and so on. The SQL query above could be emulated with the NQL code
2For instance, for even a small KB, it is essential that the Mπ matrices are stored as sparse matrices, but currently Tensorﬂow does not support sparse-sparse matrix multiplication, and multiplication of sparse matrix to a dense matrix is only supported in one order.
6

c_records = c.one(’C’, ’letter_grade_t’).grade_record_letter_grade(-1) records_of_students_with_Cs = c_records.grade_record_student_id().student_record_id(-1) records_of_phds = c.one(’PhD’, ’degree_t’).student_record_expected_degree(-1) result = (records_of_students_with_Cs & records_of_phds).student_record_id Although many join-like queries can be treated this way, SQL queries that return novel tuples clearly cannot be performed in NQL (e.g., if we modiﬁed the query above to SELECT both a course id and a student id.) However, NQL has an advantage over more expressive query languages in that it is differentiable, so it is possible to base a differentiable loss function on the result of executing an NQL query.
6 Conclusion
We have described NQL, a query language which makes it convenient to integrate queries on a KB into a neural model implemented in Tensorﬂow. NQL accesses data using only differentiable operators from Tensorﬂow, which allows a tight integration with gradient-based learning methods. NQL is available as open source: because of its many applications in NLP, code for NQL is available at https://github.com/google-research/language, the Google Research repository for NLP components.
References
[1] Martín Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al. Tensorﬂow: A system for large-scale machine learning. In 12th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 16), pages 265–283, 2016.
[2] William W Cohen, Fan Yang, and Kathryn Rivard Mazaitis. Tensorlog: Deep learning meets probabilistic dbs. arXiv preprint arXiv:1707.05390, 2017. To appear in JAIR.
[3] James Cussens. Parameter estimation in stochastic logic programs. Machine Learning, 44(3):245– 271, 2001.
[4] Alan F Gates, Olga Natkovich, Shubham Chopra, Pradeep Kamath, Shravan M Narayanamurthy, Christopher Olston, Benjamin Reed, Santhosh Srinivasan, and Utkarsh Srivastava. Building a high-level dataﬂow system on top of map-reduce: the pig experience. Proceedings of the VLDB Endowment, 2(2):1414–1425, 2009.
[5] Tim Rocktäschel and Sebastian Riedel. Learning knowledge base inference with neural theorem provers. In NAACL Workshop on Automated Knowledge Base Construction (AKBC), 2016.
[6] Dustin Tran, Alp Kucukelbir, Adji B Dieng, Maja Rudolph, Dawen Liang, and David M Blei. Edward: A library for probabilistic modeling, inference, and criticism. arXiv preprint arXiv:1610.09787, 2016.
[7] Matei Zaharia, Mosharaf Chowdhury, Michael J Franklin, Scott Shenker, and Ion Stoica. Spark: Cluster computing with working sets. HotCloud, 10(10-10):95, 2010.
7

