Breaking Down Multilingual Machine Translation
Ting-Rui Chiang1 Yi-Pei Chen2 Yi-Ting Yeh1 Graham Neubig1 1Carnegie Mellon University, 2The University of Tokyo {tingruic,yitingye,gneubig}@cs.cmu.edu ypc@g.ecc.u-tokyo.ac.jp

arXiv:2110.08130v2 [cs.CL] 3 Apr 2022

Abstract
While multilingual training is now an essential ingredient in machine translation (MT) systems, recent work has demonstrated that it has different effects in different multilingual settings, such as many-to-one, one-to-many, and many-to-many learning. These training settings expose the encoder and the decoder in a machine translation model with different data distributions. In this paper, we examine how different varieties of multilingual training contribute to learning these two components of the MT model. Speciﬁcally, we compare bilingual models with encoders and/or decoders initialized by multilingual training. We show that multilingual training is beneﬁcial to encoders in general, while it only beneﬁts decoders for low-resource languages (LRLs). We further ﬁnd the important attention heads for each language pair and compare their correlations during inference. Our analysis sheds light on how multilingual translation models work and enables us to propose methods to improve performance by training with highly related languages. Our many-to-one models for highresource languages and one-to-many models for LRL outperform the best results reported by Aharoni et al. (2019).
1 Introduction
Multilingual training regimens (Dong et al., 2015; Firat et al., 2016; Ha et al., 2016) are now a key element of natural language processing, especially for low-resource languages (LRLs) (Neubig and Hu, 2018; Aharoni et al., 2019). These algorithms are presumed to be helpful because they leverage syntactic or semantic similarities between languages, and transfer processing abilities across language boundaries.
In general, English is used as a central language due to its data availability, and three different multilingual training settings are considered: (1) one-tomany: training a model with languages pairs from English to many other languages. (2) many-to-one:

training a model with languages pairs from many languages to English (3) many-to-many: training a model with the union of the above two settings’ data. (1) and (3) can be used for English to other (En-X) translation, while (2) and (3) can be used for other to English (X-En) translation.
However, multilingual training has not proven equally helpful in every setting. Arivazhagan et al. (2019) showed that many-to-one training improves performance over bilingual baselines more than one-to-many does. In this paper, we consider this result from the point of view of the components of the MT model. In the many-to-one setting, the model’s inputs are from different language distributions so the encoder can be considered a multidomain model, whereas the decoder is trained on a single distribution. In the one-to-many setting, it is the opposite: the encoder shares data, and the decoder is multi-domain. While there are recent studies analyzing multilingual translation models (Kudugunta et al., 2019; Voita et al., 2019a; Aji et al., 2020; Mueller et al., 2020), in general, they do not (1) examine the impact of different multilingual training settings such as one-to-many and many-to-one, and (2) they do not examine the different components, such as the encoder and the decoder, separately.
This motivates us to ask “how do various types of multilingual training interact with learning of the encoder and decoder?” To answer this question, we set up controlled experiments that decouple the contribution to the encoder and the decoder in various training settings. We ﬁrst train multilingual models using many-to-one, one-to-many, or many-to-many training paradigms. We then compare training bilingual models with and without initializing the encoder or the decoder with parameters learned by multilingual training. We ﬁnd that, for LRLs, multilingual training is beneﬁcial to both the encoder and the decoder. However, surprisingly, for high-resource languages (HRL), we found mul-

Lang. az be gl sk ar de he it
Size (K) 6 5 10 61 214 168 212 205
Table 1: Training data size.
tilingual training only beneﬁcial to the encoder but not to the decoder.
To further analyze the result, we examine "to what degree are the learned parameters shared across languages?". We use the head importance estimation method proposed by Michel et al. (2019) as a tool to identify the important attention heads in the model, and measure the consistency between the heads sets that are important for different language pairs. The results suggest that the encoder does share parameters across different languages in all settings. On the other hand, the decoder can treat the representation from the encoder in a language-agnostic way for X-En translation, and less parameter sharing is observed for En-X translation. Our analyses on parameter sharing also provide a possible explanation to Kudugunta et al. (2019)’s observation that the representation from the encoder is target-language-dependent.
Our investigation of how multilingual training works leads us to a method for improving MT models. With the comprehensive experiments in multilingual settings, for translations in HRL (Ar-En, De-En, He-En, It-En), we discover that ﬁne-tuning multilingual model with target bilingual data outperforms the best results in Aharoni et al. (2019) by 2.99 to 4.63 BLEU score. With the analysis of the parameter sharing in the decoder, we are able to identify related languages. Fine-tuning jointly with the identiﬁed related languages boosts lowresource translation (En-Az, En-Be, En-Go, En-Sk) over the best results in Aharoni et al. (2019) by 1.66 to 4.44 BLEU score. Compared to Neubig and Hu (2018), our method does not require linguist knowledge, and thus may be more useful for less-studied low-resource languages.
In sum, our contributions are three-fold. First, our experiments can be used as a diagnostic tool for multilingual translation to investigate how an encoder and a decoder beneﬁt from multilingual training. Second, our results provide insights into how multilingual translation works. Third, we improve the translation models based on the ﬁndings from our analysis, showing a promising path for future research on multilingual machine translation.

2 Experimental Settings for Multilingual Training
Before stepping into our analysis, we ﬁrst explain our experimental setup. Following the setting in Aharoni et al. (2019) and Neubig and Hu (2018), we use the publicly available TED Talks Dataset (Qi et al., 2018) is used to train all our machine translation models. Following Neubig and Hu (2018), we break words into subwords with BPE jointly learned over all source languages using the sentencepiece toolkit. The vocabulary size is 32,000. We perform experiments with the Transformer architecture (Vaswani et al., 2017) using the hyperparameters same as in (Arivazhagan et al., 2019) 1. All models are implemented and trained using Fairseq 0.10.0 (Ott et al., 2019). We trained multilingual translation models with 60 different languages on the TED Talks Dataset with the three settings described in Section 1: one-to-many, manyto-one and many-to-many. For one-to-many and many-to-many settings, we add a special language token to the input of the encoder to indicate the target language. Following Aharoni et al. (2019), we evaluate our models with BLEU score (Papineni et al., 2002; Post, 2018) on the selected 8 languages. They are representative of different language families (Qi et al., 2018). The size of the training is shown in Table 1.
3 How Multilingual Training Beneﬁts Each Component
Previous studies have shown that the multilingual training results are generally stronger than the bilingual training (Arivazhagan et al., 2019). To understand how multilingual training beneﬁts NMT, we analyze the effect of multilingual training on different components of an NMT model, speciﬁcally, the encoder and decoder.
3.1 Experiments Design
To study how multilingual training beneﬁts each component, we train models on bilingual data with components initialized differently as follows:
• Bilingual Only: Models trained from scratch with no components initialized with parameters learned from multilingual training.
16 layers in both the encoder and the decoder, 8 attention head, state dimension=512, ffn dimension=2048, label smoothing=0.1

Model

→ en

az be gl

sk

ar de he

it

All-All (Aharoni et al., 2019) 12.8 21.7 30.7 29.5 28.3 33.0 33.2 35.1

All-En All-All

9.1 15.2 27.4 25.4 23.9 28.3 27.9 31.5 8.1 12.6 22.8 24.6 21.7 27.1 26.1 31.1

Bilingual Only

2.1 1.4 2.8 18.5 28.5 32.0 34.8 35.7

All-En

Load Enc. Load Dec. Freeze Enc. Freeze Dec. Load Both

2.8 1.8 5.9 18.1 30.6 35.5 36.9 35.7 2.5 1.8 5.7 17.8 27.2 30.3 33.2 35.7 5.0 6.0 19.3 26.3 28.4 33.0 33.6 36.4 3.4 4.1 16.9 24.7 28.1 31.4 33.4 33.6 11.5 19.0 29.9 28.00 30.4 33.1 36.2 36.7

All-All

Load Enc. Load Dec. Freeze Enc. Freeze Dec. Load Both

5.4 7.0 20.6 28.0 30.9 35.7 37.1 38.1 1.4 0.5 0.9 20.4 28.9 32.2 34.0 35.3 3.3 5.0 9.3 23.8 25.9 32.4 32.2 34.2 2.0 6.2 20.1 26.9 30.1 34.4 35.9 36.8 11.3 19.4 31.8 29.6 31.3 36.0 37.8 38.7

Table 2: Results of translating to English. All in the model name refers to using all 59 languages.

• Load encoder/decoder: Models with trainable parameters of either encoder or decoder initialized with parameters learned from multilingual training.
• Load both: Models with parameters of both encoder and decoder initialized with parameters learned from multilingual training. This can be seen as ﬁne-tuning the multilingual model on bilingual data.
The motivation for this paradigm is that if multilingual training is beneﬁcial to a component, then initializing the parameters of that component should result in improvements over random initialization and training on only bilingual data. If load encoder outperforms bilingual only, then we can say that multilingual training is beneﬁcial for the encoder, and if load decoder outperforms we can make the analogous conclusion for the decoder. Thus comparing these models reveals how each component beneﬁt from multilingual training.
We also consider a load and freeze setting (Thompson et al., 2018), where we initialize a component from a multilingual model and freeze its weights when ﬁne-tuning on bilingual data. For example, in the load decoder setting, we train the loaded decoder with a randomly initialized encoder. We suspect that learning with randomly initialized component might ruin the other component which is well-trained with multilingual data, especially in the beginning of the training. Thus, we additionally experiment with this load and freeze setting to ensure the multilingual-trained component is not deteriorated.

3.2 Results and Discussion
The overall results of X-En and En-X are shown in Table 2 and Table 3, respectively. The difference between the numbers reported in Aharoni et al. (2019) and ours is due to the different batch size and learning rate schedule we use. In the following section we will discuss the results of our study. Because they are highly dependent on the training data size (Table 1), we discuss the results in two groups: high-resource languages (HRL; referring to ar, de, he, and it) and low-resource languages (LRL; referring to az, be, gl, sk).2
3.2.1 Low-Resource Language Results
For LRLs, we ﬁnd that multilingual training is generally beneﬁcial to both the encoders and the decoders in all three multilingual models. Both load encoder and load and freeze decoder can achieve performance better than the bilingual baseline. This suggests that the parameters in the encoder and the decoder learned by multilingual training do contain information that is not effectively learned from the smaller bilingual data.
The results also suggest that multilingual training is more beneﬁcial for the encoders than decoders. In all cases, either load encoder or freeze encoder outperforms both load decoder and load and freeze decoder. However, multilingual training of the encoder and the decoder are complementary; loading both the encoder and the decoder can usually improve the performance over loading only one component.
2sk has intermediate size, and its behavior is not always consistent with the other LRL.

Model

en → az be gl sk ar de he it

All-En (Aharoni et al., 2019) 5.1 10.7 26.6 24.5 16.7 30.5 27.6 35.9

En-All All-All

4.9 9.0 24.2 21.9 15.1 27.9 24.1 33.3 3.1 6.2 20.5 18.4 12.7 24.5 21.1 30.5

Bilingual Baseline

1.3 1.9 3.9 13.1 15.6 27.1 25.4 32.0

En-All

Load Enc. Load Dec. Freeze Enc. Freeze Dec. Load All

3.0 5.6 16.7 21.7 17.2 30.0 27.5 34.6 1.3 2.0 8.1 17.4 16.0 26.7 25.8 32.6 2.7 4.6 14.7 21.1 9.7 24.4 22.6 33.4 1.9 3.7 14.5 17.6 16.2 28.0 25.9 33.3 6.4 14.7 26.9 23.5 17.1 31.1 28.2 34.9

All-All

Load Enc. Load Dec. Freeze Enc. Freeze Dec. Load All

2.4 5.0 16.9 21.4 16.9 29.8 27.4 34.4 1.1 2.2 7.0 17.5 16.0 28.1 25.6 32.5 2.1 0.5 12.6 19.4 10.2 24.4 24.3 33.1 0.9 4.7 15.0 18.8 15.1 27.5 24.9 32.4 6.1 13.0 26.4 23.2 17.0 30.3 27.9 34.6

Table 3: Results of translating from English. All in the model name refers to using all 59 languages.

3.2.2 High-Resource Language Results
On HRLs, we ﬁnd that multilingual training is generally beneﬁcial to the encoders in all three multilingual models, while it is not beneﬁcial for the decoders in some settings. Load encoder consistently outperforms the baseline models, but for the All-En model on X-En translation, and the All-All model on En-X translation, neither load decoder nor load and freeze decoder outperform the baseline model.
We also observe that multilingual training is generally more beneﬁcial to the encoders than decoders. In all cases, load encoder can achieve performance competitive to load both (better or less by within 1 BLEU score). However, in all cases, both load decoder and load and freeze decoder have worse performance than load both. Therefore, multilingual training is not as beneﬁcial to the decoders as to the encoders.
3.3 Discussion
For LRL, because the size of bilingual training data is small, it is not surprising that multilingual training is beneﬁcial for both the encoder and the decoder. However, our results are somewhat more surprising for HRL — it is not trivial that multilingual training is not as beneﬁcial. In the next section, we focus on explaining the phenomena observed on HRL by investigating how parameters are shared across languages.

4 How Multilingual Parameters are Shared in Each Component

Given the previous results, we are interested in exactly how parameters are shared among different language pairs. Given that we are using the Transformer architecture, for which multi-head attention is a fundamental component, we use the attention heads as a proxy to analyze how multilingual models work differently when translating between different languages. Speciﬁcally, we analyze our models by identifying the attention heads that are important when translating a language pair. Measuring the consistency between the sets of important attention heads for two language pairs gives us hints on the extent of parameter sharing.

4.1 Head Importance Estimation
First, we provide some background on head importance estimation, speciﬁcally the method proposed by Michel et al. (2019).
Given a set of multi-head attention modules, each of which can be written as
Nh
MHAtt(x) = ξhAttWq(h),Wk(h),Wv(h) (x), (1)
h=1
where Nh is the number of attention heads, and ξh = 1 for all h.
The importance of a head can be estimated as

I˜h = Ex∼X ∂L(x) .

(2)

∂ξh

given a loss function L and input X. Then, the importance score of each head in an attention module

is normalized

I˜h

Ih =

.

(3)

Nh i

Ih2

Note that when the input X is different, the estimated importance score can be different. Therefore, when different language pairs are fed in, the important heads identiﬁed can be different. We denote the set of attention head scores estimated on translation from language la to language lb as H(la, lb). We denote the scores of attention heads in a component by using superscript. For example, Henc represents the scores of the heads in a encoder.

4.2 Measuring Parameter Sharing by Correlation of Head Scores
With the attention head importance scores estimated by Equation 3, we can investigate how parameters are shared across languages. For each of the En-All, All-En, All-All multilingual models, we estimated a set of head-importance scores H(la, lb) for each language pair (la, lb) in the training setting. We calculate the head scores with the training loss function (MLE with label smoothing) and 100K randomly sampled sentences in the training set.
To investigate how much parameters are shared by two pairs of languages (la, lb) and (lc, ld), we measure the agreement between H(la, lb) and H(lc, ld). If a head is important for both of (la, lb) and (lc, ld), then important parameters for translating are shared. Thus high agreement suggests high parameter sharing.
To quantify the agreement between two score sets, we use Spearman’s rank correlation (Spearman, 1987). A rank-based correlation metric is used because the importance estimation was originally proposed to order attention heads in a model. Higher correlation implies higher agreement and thus implies higher parameter sharing. For each of the En-All, All-En, All-All models, we calculate the correlation between H(la, lb) and H(lc, ld) for all language pairs (la, lb) and (lc, ld) that are used to train the model. The detailed correlation computation process can be found in Appendix A. We plot the correlation matrices of the head scores (included in appendix) and summarize them in Table 4. We also compare the top-10 most important heads for every language pairs with F1 scores, and observe similar results. We include the statistics in appendix.

Model
All-En En-All All-All All-All

Lang. Pair
X-En En-X X-En En-X

H enc
.871 (.086) .806 (.153) .898 (.073) .813 (.126)

H dec
.973 (.023) .720 (.150) .967 (.029) .762 (.141)

Table 4: Correlation between the attention head scores when estimated using different language pairs.

4.3 How Multilingual Translation Models Share
Results in Table 4 combined with Section 3 provides the insights into how multilingual translation models work with respect to cross-lingual sharing:
Encoder for En-X: It is natural that the encoder from En-X likely beneﬁt from multilingual training because it can generate representations tailored for different target languages with shared parameters. En-X is a set of language pairs where the source language is always English. Therefore, if the prepended target language token is ignored, the inputs of the encoders for all pairs in En-X are from one identical distribution. This is in contrast to X-En pairs, where the inputs are in different languages. However, for the encoders, we observe from Table 4 that the average correlation scores of En-X pairs (0.806 and 0.813), are lower than the correlation scores of X-En pairs (0.871 and 0.898). Kudugunta et al. discovers that the representation of the encoder is target-language-dependent. Thus we conjecture that some parameters may be used to generate representation tailored for the target languages. At the same time, since the inputs are from a single distribution (English) for different target languages, a large portion of parameters may still be shareable across target languages. Therefore, in this case, multilingual training is beneﬁcial.
Encoder for X-En: For X-En language pairs, the input of the encoder is multilingual, which means the input from different X-En language pairs has distinct distribution. However, the correlation between different source languages is still high. It shows that high parameters sharing in the encoder is possible.
Decoder for En-X: The decoders for En-X have the lowest correlation. From the correlation matrix, we do see some parameter sharing between some language pairs. However, larger model capacity might be required for a model to be proﬁcient in

all the languages.
Decoder for X-En: The decoder have average correlation as high as 0.973 and 0.967 for All-En and All-All models respectively. This suggests that to decode intermediate representation encoded by the encoder, the decoder use almost the same set of parameters. However, Kudugunta et al. shows that the representation encoded by the encoder is not language-agnostic. A possible explanation is that the important parameters of the decoder are highly determined by the target output, which is always in English. Therefore, even though the encoder representation is not language-agnostic, it is still difﬁcult to learn parameters reﬂecting the difference. It suggests why multilingual training does not beneﬁt the decoder in the X-En setting. The set of English sentences is almost the same for all the HRL pairs in the TED Talks dataset, so multilingual training can hardly provide more unique English sentences than bilingual training does. If the decoder is dedicated for generation, multilingual training cannot expose the decoder to more diverse data. Therefore the multilingually trained decoder does not perform better than the bilingual one.
5 Improving Translation Based on the Degree of Parameter Sharing
Insights from the previous section provide us with a new way to choose languages for multilingual training. In previous work (Lin et al., 2019; Oncevay et al., 2020), choosing on languages with similar linguistic properties is a popular practice. However, Mueller et al. (2020) found the effect is highly language-dependent. Sometimes training with similar languages might be worse than training on a set of unrelated languages. Here we otherwise propose an entirely model-driven way to ﬁnd related languages to improve multilingual translation models. We explore choosing languages where parameters can be better shared.
5.1 Improving X-En by Related En-X Pairs
In the All-All model, we notice low parameter sharing between En-X and X-En pairs. The average correlation between Henc(En, X) and Henc(X, En) is 0.44 (std: 0.17). The average correlation between Hdec(En, X) and Hdec(X, En) is 0.49 (std: 0.13). It provides a possible explanation why training with both the En-X and the X-En pairs only brings little

improvement over training with only En-X alone or with X-En alone.
The low correlation combined with results in Section 3 motivate us to experiment on improving X-En with related En-X pairs. Section 3 shows that the multilingual decoder has less advantage than the encoder. This may suggest the inefﬁciency of parameter sharing in the decoder. Therefore we experiment on choosing a set of related languages based on the degree of parameter in the decoder. We choose the language set L such that for all l ∈ L, the average correlation 610 6li0=1 Corr(Hdec(En, l), Hdec(li, En)) is higher than 0.60.
Results are shown in Table 5. Even though ﬁnetuning on related languages improves the overall performance, it is not better than ﬁne-tuning on the All-En pairs only. Also, the average correlation between Hdec(En, la) and Hdec(lb, En) is not improved. Our experiment demonstrates the difﬁculty of sharing parameters between All-En pairs and EnAll pairs. We leave this problem for future work.
5.2 Improving En-X by Language Clusters
The low correlation between attention head scores of language pairs motivates us to improve the performance of En-X using related language pairs. As shown in Table 4, the decoders have the lowest correlation scores. We conjecture that it is due to the difﬁculty of sharing parameters between distant languages. Thus, we seek for ﬁnding related language sets, in each of which parameters can be shared.
Again, we resort to the attention head importance scores to ﬁnd the related languages. Our intuition is that related languages would share many parameters in between and training a model on related languages would be helpful. As a sanity check of our idea, we ﬁrst use t-SNE (Maaten and Hinton, 2008) to reduce the dimension of head-importance scores H(la, lb). We only focus on heads in the decoders, because the correlation score between H(En,lc) and H(En,ld) is lower in average for the decoders. The result visualized in Figure 1 illustrates that, the distance between H(En,lc) and H(En,ld) tend to be shorter if languages lc and ld are linguistically related. Hence, determining related languages with head score H(En,l) should be reasonable.
We then ﬁne-tune multilingual models on related language clusters. Related languages clusters are determined by k-mean++ (Arthur and Vassilvitskii,

Model

az be gl sk ar de he it

All-All

8.1 12.6 22.8 24.6 21.7 27.1 26.1 31.1

+ f.t. on All-En

10.5 17.5 29.7 28.1 25.9 31.3 30.5 34.0

+ f.t. on All-En & related 10.5 17.4 28.3 27.0 25.1 30.0 29.9 32.7

Table 5: Performance of All-All model ﬁne-tuned on All-En pairs and ﬁne-tuned on the union of All-En pairs and related En-All languages.

Model

az be gl sk ar de he it

En-All (Aharoni et al., 2019)

5.1 10.7 26.6 24.5 16.7 30.5 27.6 35.9

Bilingual Baseline

1.3 1.9 3.9 13.1 15.6 27.1 25.4 32.0

All-All

3.1 6.2 20.5 18.4 12.7 24.5 21.1 30.5

All-All w/ f.t. on related clusters 7.9 12.8 27.5 24.9 - 30.2 27.0 35.4

All-All w/ f.t. on random groups 6.9 13.3 22.5 24.3 -

- 27.5 35.2

En-All

4.9 9.00 24.2 21.9 15.1 27.9 24.1 33.3

En-All w/ f.t. on related clusters 7.9 13.9 21.0 26.2 16.7 30.4 27.1 35.4

En-All w/ f.t. on random groups 7.0 13.1 23.1 24.7 -

- 27.6 35.2

Load En-All w/ f.t. on closest 7.8 15.2 28.6

Table 6: Performance of En-All model without and with ﬁne-tuning on language clusters.

2007) with k = 5. We consider clusters that cover all of the four low-resource languages. For the AllAll model, one of the cluster we consider contains Be, Gl, De, He, It, and the other one contains Az. For the En-All model, we also experiment with two clusters. One includes Ar, De, He, It, and the other includes Az, Be, Gl, Sk. As a baseline, we also experiment with random groups. They are groups generated by randomly splitting the 59 target languages.
The results are shown in Table 6. For both the En-All and the All-All model, except En-Gl, ﬁnetuning on clusters can improve performance on all the considered language pairs consistently. For LRLs, ﬁne-tuning on related language clusters is also better than ﬁne-tuning on random groups in general. To verify whether this improvement is brought by increased parameter sharing in the decoders, we check the correlation between Hdec after ﬁne-tuning. The results shown in Table 7 shows improvements after ﬁne-tuning on the clusters.
For low-resource language pairs En-Az, En-Be, En-Sk on the En-All model, we notice that only few languages are highly correlated with them (with correlation > 0.80). Therefore, we also experiment with ﬁne-tuning the En-All model with only the language pairs with high correlation scores (> 0.80) for each of the three pairs , which boosts the performance of En-Be to 15.2 and En-Sk to 28.6.

Tamil

Afro-Asiatic

200

Bengali

Turkic

GeoMragriaanthi Mongolian
HindBi urmese Kazakh Armenian

Indo-European Constructed Uralic

100
Belarusian

Urdu Thai
Japanese

Language isolate Austronesian

MaceHdeobnrieawn

Korean

Ukrainian

Arabic

Vietnamese

RussiaBnulgarian Persian

Kurdish

IndMoanleasyian

Greek all

Turkish Azerbaijani

Polish

Japonic Kartvelian Koreanic

0

SlovSelonCviaaznkech Romanian

FinnishBasque

Lithuanian

Mongolic Sino-Tibetan

SerBbioasnnian

Estonian GalicianEsperanto

Dravidian

pt_bPrortuguese

Hungarian

Croatian

Tai–Kadai

Spanish

Italian fr_ca

Swedish

Austroasiatic

Norwegian Bokmål

-100

French

Danish

all

Dutch GermAalbnanian

-200 -200

zh_Ctwhinese zh_cn

-100

0

100

200

300

400

500

Figure 1: Visualization of the En-All decoder head scores of languages by t-SNE.

Model
All-All En-All (HL) En-All (LL)

Hdec w/o f.t.
.762 (.141) .855 (.066) .826 (.096)

Hdec w/ f.t.
.894 (.069) .866 (.065) .834 (.091)

Table 7: Correlation between the decoder attention head scores when estimated using the language pairs in the cluster. HL and LL represent the cluster that includes HRL and the one that includes LRL respectively.

6 Related Work
The early attempts of multilingual training for machine translation use a single model to translate between multiple languages (Dong et al., 2015; Firat et al., 2016; Ha et al., 2016). Those works ﬁnd multilingual NMT models are appealing because they not only give us a simple paradigm to handle mapping between multiple languages, but also improve performance on low and zero-resource languages pairs (Gu et al., 2018). However, how multilingual training contributes to components in the translation model still remains unknown.
There are some attempts at analyzing and explaining the translation models. Thompson et al. (2018) analyze the contribution of different components of NMT model to domain adaptation by freezing the weights of components during continued training. Arivazhagan et al. (2019) provide an comprehensive study on the state-of-the-art multilingual NMT model in different training and testing scenarios. Sachan and Neubig (2018) experiment with different parameter sharing strategies in Transformer models, showing that sharing parameters of embedding, key and query performs well for one-to-many settings. Artetxe et al. (2020) shows the strong transferability of monolingual representation to different languages. The intermediate representation of BERT can be language-agnostic if we freeze the embeddings during training. The deﬁciency of the one-to-many setting is explored in (Johnson et al., 2017). They ﬁnd only the many-toone setting consistently improves the performance across languages. Wang et al. (2018) also explore problems of the one-to-many setting, and show language-speciﬁc components are effective to improve the performance. Voita et al. (2019a) analyzes how generated sentences of NMT models are inﬂuenced by context in the encoder and decoder. The attempt to investigate encoder and decoder separately is similar to our work. Rothe et al.

(2020) explores how pretrained checkpoints can beneﬁt the encoder and the decoder in a translation model. Zhang et al. (2021) investigate the trade-off between language-speciﬁc and shared capacity of layers in a multilingual NMT model.
Multi-head attention has been shown effective in different NLP tasks. Beyond improving performance, multi-head attention can help with subjectverb agreement (Tang et al., 2018), and some heads are predictive of dependency structures (Raganato and Tiedemann, 2018). Htut et al. (2019) and Clark et al. (2019) report that heads in BERT attend signiﬁcantly more to words in certain syntactic position. They show some heads seem to specialize in certain types of syntactic relations. Michel et al. (2019), Voita et al. (2019b), and Behnke and Heaﬁeld (2020) study the importance of different attention heads in NMT models, and suggest that we can prune those attention heads which are less important. Brix et al. (2020) also shows pruning NMT models can improve the sparsity level to optimize the memory usage and inference speed.
However, all previous works do not directly investigate how encoder and decoder of NMT models beneﬁt from multilingual training, which is the key question of why multilingual training works. To our best knowledge, we are the ﬁrst to tackle the question, and our analysis can be used to further improve multilingual NMT models.
7 Conclusion
In this work, we have the following ﬁndings: 1) In Section 3, we examine how multilingual training contributes to each of the components in a machine translation model. We discover that, while multilingual training is beneﬁcial to the encoders, it is less beneﬁcial to the decoders. 2) In Section 4, our analysis of important attention heads provides insight into the behavior of multilingual components. Results suggest that the encoder in the EnAll model may generate target-language-speciﬁc representation, while the behavior of the decoder of the All-En model may be source-language-agnostic. In addition, in the All-All model, we observe indications of lower parameter sharing between X-En pairs and En-X pairs. 3) In Section 5, we explore approaches to improve the model based on our ﬁndings. On En-X translation, we outperform the best results in (Aharoni et al., 2019). With our proposed analysis as diagnostic tools, future work may further improve the multilingual systems.

References
Roee Aharoni, Melvin Johnson, and Orhan Firat. 2019. Massively multilingual neural machine translation. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 3874–3884, Minneapolis, Minnesota. Association for Computational Linguistics.
Alham Fikri Aji, Nikolay Bogoychev, Kenneth Heaﬁeld, and Rico Sennrich. 2020. In neural machine translation, what does transfer learning transfer? In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7701–7710, Online. Association for Computational Linguistics.
Naveen Arivazhagan, Ankur Bapna, Orhan Firat, Dmitry Lepikhin, Melvin Johnson, Maxim Krikun, Mia Xu Chen, Yuan Cao, George Foster, Colin Cherry, et al. 2019. Massively multilingual neural machine translation in the wild: Findings and challenges. arXiv preprint arXiv:1907.05019.
Mikel Artetxe, Sebastian Ruder, and Dani Yogatama. 2020. On the cross-lingual transferability of monolingual representations. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4623–4637, Online. Association for Computational Linguistics.
David Arthur and Sergei Vassilvitskii. 2007. Kmeans++: The advantages of careful seeding. In Proceedings of the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA ’07, page 1027–1035, USA. Society for Industrial and Applied Mathematics.
Maximiliana Behnke and Kenneth Heaﬁeld. 2020. Losing heads in the lottery: Pruning transformer attention in neural machine translation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2664– 2674, Online. Association for Computational Linguistics.
Christopher Brix, Parnia Bahar, and Hermann Ney. 2020. Successfully applying the stabilized lottery ticket hypothesis to the transformer architecture. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3909– 3915, Online. Association for Computational Linguistics.
Kevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher D Manning. 2019. What does bert look at? an analysis of bert’s attention. In Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP.
Daxiang Dong, Hua Wu, Wei He, Dianhai Yu, and Haifeng Wang. 2015. Multi-task learning for multiple language translation. In Proceedings of the

53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 1723–1732, Beijing, China. Association for Computational Linguistics.
Orhan Firat, Kyunghyun Cho, and Yoshua Bengio. 2016. Multi-way, multilingual neural machine translation with a shared attention mechanism. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 866–875, San Diego, California. Association for Computational Linguistics.
Jiatao Gu, Hany Hassan, Jacob Devlin, and Victor O.K. Li. 2018. Universal neural machine translation for extremely low resource languages. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 344–354, New Orleans, Louisiana. Association for Computational Linguistics.
Thanh-Le Ha, Jan Niehues, and Alexander Waibel. 2016. Toward multilingual neural machine translation with universal encoder and decoder. arXiv preprint arXiv:1611.04798.
Phu Mon Htut, Jason Phang, Shikha Bordia, and Samuel R Bowman. 2019. Do attention heads in bert track syntactic dependencies? arXiv preprint arXiv:1911.12246.
Melvin Johnson, Mike Schuster, Quoc V. Le, Maxim Krikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat, Fernanda Viégas, Martin Wattenberg, Greg Corrado, Macduff Hughes, and Jeffrey Dean. 2017. Google’s multilingual neural machine translation system: Enabling zero-shot translation. Transactions of the Association for Computational Linguistics, 5:339–351.
Sneha Kudugunta, Ankur Bapna, Isaac Caswell, and Orhan Firat. 2019. Investigating multilingual NMT representations at scale. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 1565–1575, Hong Kong, China. Association for Computational Linguistics.
Yu-Hsiang Lin, Chian-Yu Chen, Jean Lee, Zirui Li, Yuyan Zhang, Mengzhou Xia, Shruti Rijhwani, Junxian He, Zhisong Zhang, Xuezhe Ma, Antonios Anastasopoulos, Patrick Littell, and Graham Neubig. 2019. Choosing transfer languages for cross-lingual learning. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3125–3135, Florence, Italy. Association for Computational Linguistics.
Laurens van der Maaten and Geoffrey Hinton. 2008. Visualizing data using t-sne. Journal of machine learning research, 9(Nov):2579–2605.

Paul Michel, Omer Levy, and Graham Neubig. 2019. Are sixteen heads really better than one? In Advances in Neural Information Processing Systems, pages 14014–14024.
Aaron Mueller, Garrett Nicolai, Arya D. McCarthy, Dylan Lewis, Winston Wu, and David Yarowsky. 2020. An analysis of massively multilingual neural machine translation for low-resource languages. In Proceedings of the 12th Language Resources and Evaluation Conference, pages 3710–3718, Marseille, France. European Language Resources Association.
Graham Neubig and Junjie Hu. 2018. Rapid adaptation of neural machine translation to new languages. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 875–880, Brussels, Belgium. Association for Computational Linguistics.
Arturo Oncevay, Barry Haddow, and Alexandra Birch. 2020. Bridging linguistic typology and multilingual machine translation with multi-view language representations. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2391–2406, Online. Association for Computational Linguistics.
Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, and Michael Auli. 2019. fairseq: A fast, extensible toolkit for sequence modeling. In Proceedings of NAACL-HLT 2019: Demonstrations.
Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311–318, Philadelphia, Pennsylvania, USA. Association for Computational Linguistics.
Matt Post. 2018. A call for clarity in reporting BLEU scores. In Proceedings of the Third Conference on Machine Translation: Research Papers, pages 186– 191, Brussels, Belgium. Association for Computational Linguistics.
Ye Qi, Devendra Sachan, Matthieu Felix, Sarguna Padmanabhan, and Graham Neubig. 2018. When and why are pre-trained word embeddings useful for neural machine translation? In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 529–535, New Orleans, Louisiana. Association for Computational Linguistics.
Alessandro Raganato and Jörg Tiedemann. 2018. An analysis of encoder representations in transformerbased machine translation. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 287–297, Brussels, Belgium. Association for Computational Linguistics.

Sascha Rothe, Shashi Narayan, and Aliaksei Severyn. 2020. Leveraging pre-trained checkpoints for sequence generation tasks. Transactions of the Association for Computational Linguistics, 8:264–280.
Devendra Sachan and Graham Neubig. 2018. Parameter sharing methods for multilingual self-attentional translation models. In Proceedings of the Third Conference on Machine Translation: Research Papers, pages 261–271, Brussels, Belgium. Association for Computational Linguistics.
Charles Spearman. 1987. The proof and measurement of association between two things. The American journal of psychology, 100(3/4):441–471.
Gongbo Tang, Mathias Müller, Annette Rios, and Rico Sennrich. 2018. Why self-attention? a targeted evaluation of neural machine translation architectures. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4263–4272, Brussels, Belgium. Association for Computational Linguistics.
Brian Thompson, Huda Khayrallah, Antonios Anastasopoulos, Arya D. McCarthy, Kevin Duh, Rebecca Marvin, Paul McNamee, Jeremy Gwinnup, Tim Anderson, and Philipp Koehn. 2018. Freezing subnetworks to analyze domain adaptation in neural machine translation. In Proceedings of the Third Conference on Machine Translation: Research Papers, pages 124–132, Brussels, Belgium. Association for Computational Linguistics.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. Advances in neural information processing systems, 30:5998–6008.
Elena Voita, David Talbot, Fedor Moiseev, Rico Sennrich, and Ivan Titov. 2019a. Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5797–5808, Florence, Italy. Association for Computational Linguistics.
Elena Voita, David Talbot, Fedor Moiseev, Rico Sennrich, and Ivan Titov. 2019b. Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5797–5808, Florence, Italy. Association for Computational Linguistics.
Yining Wang, Jiajun Zhang, Feifei Zhai, Jingfang Xu, and Chengqing Zong. 2018. Three strategies to improve one-to-many multilingual translation. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2955– 2960, Brussels, Belgium. Association for Computational Linguistics.

Biao Zhang, Ankur Bapna, Rico Sennrich, and Orhan Firat. 2021. Share or not? learning to schedule language-speciﬁc capacity for multilingual translation. In International Conference on Learning Representations.

Code
ar az be bg bn bs cs da de el eo es et eu fa ﬁ fr fr-ca gl he hi hr hu hy id it ja ka

Name
Arabic Azerbaijani Belarusian Bulgarian Bengali Bosnian Czech Danish German Greek Esperanto Spanish Estonian Basque Persian Finnish French French Galician Hebrew Hindi Croatian Hungarian Armenian Indonesian Italian Japanese Georgian

Code
ku lt mk mn mr ms my nb nl pl pt pt-br ro ru sk sl sq sr sv ta th tr uk ur vi zh zh-cn zh-tw

Name

B Related Related Language Pairs

Kurdish Lithuanian Macedonian Mongolian Marathi Malay Burmese Norwegian Bokmål Dutch Polish Portuguese Portuguese Romanian Russian Slovak Slovenian Albanian Serbian Swedish Tamil Thai Turkish Ukrainian Urdu Vietnamese Chinese Chinese Chinese

The related language pairs used in Section 5 are: en-zh_cn en-it en-es en-vi en-zh_tw en-nl en-fr en-fr_ca en-th en-pt_br en-ru.
C Language Clusters
En-All model:
• en-ja en-ko en-zh en-zh-cn en-zh-tw
• en-az en-be en-bs en-cs en-da en-eo en-et eneu en-ﬁ en-gl en-hr en-hu en-lt en-mk en-nb en-pl en-sk en-sl en-sq en-sr en-sv en-tr en-uk
• en-bn en-hi en-hy en-ka en-ku en-mr en-my en-ta en-th en-ur
• en-ar en-bg en-de en-el en-es en-fa en-fr en-frca en-he en-id en-it en-ms en-nl en-pt en-pt-br en-ro en-ru en-vi
• en-kk en-mn
All-All:
• en-be, en-bg, en-bs, en-cs, en-de, en-el, en-es, en-fr, en-fr-ca, en-gl, en-he, en-hr, en-it, enlt, en-mk, en-pl, en-pt, en-pt-br, en-ro, en-ru, en-sk, en-sl, en-sq, en-sr, en-uk
• en-ar, en-fa, en-ja, en-ko, en-th, en-vi, en-zh, en-zh-cn, en-zh-tw

Table 8: Languages in the Ted Talk Dataset
A Correlation of Head Scores
Here we detail the computation of the correlation of head scores for two pairs of languages (la, lb) and (lc, ld). The steps are as follow:
1. The two language pairs’ head importance scores H(la, lb) and H(lc, ld) are estimated with Equation 3. Since there are many heads in a Transformer model, both H(la, lb) and H(lc, ld) are vectors.
2. We ﬂatten the scores in H(la, lb) and H(lc, ld) into two arrays of scalars. We treat the two arrays as the observations of two variables. Then, we use Spearman correlation to compute the correlation between the two variables. In other words, the input of the Spearman correlation function is the two arrays.

• en-bn, en-hi, en-hy, en-ka, en-ku, en-mr, enmy, en-ur
• en-az, en-da, en-eo, en-et, en-ﬁ, en-hu, en-id, en-ms, en-nb, en-nl, en-sv, en-tr
• en-eu, en-kk, en-mn, en-ta
D Random Clusters
• en-pt en-fa en-fr en-kk en-hi en-da en-hu ende en-nl en-ar en-hy en-zh-cn
• en-sr en-ﬁ en-be en-ko en-ru en-ur en-it en-id en-el en-eu en-sq en-zh en-bs en-bn en-sv enbg en-my en-ro en-ta en-sl en-et en-ku en-mn en-uk en-he en-tr
• en-mk en-mr
• en-ms en-pl en-pt-br en-cs en-zh-tw en-es
• en-vi en-eo en-hr en-nb en-fr-ca en-az en-sk en-ka en-lt en-th en-ja en-gl

1 0.9 0.8 0.7 0.6 0.5 0.4 0.3

all_all

zh_tw_en

zh_cn_en

zh_en

vi_en

ur_en

uk_en

tr_en

th_en

ta_en

sv_en

sr_en

sq_en

sl_en

sk_en

ru_en

ro_en

pt_br_en

pt_en

pl_en

nl_en

nb_en

my_en

ms_en

mr_en

mn_en

mk_en

lt_en

ku_en

ko_en

kk_en

ka_en

ja_en

it_en

id_en

hy_en

hu_en

hr_en

hi_en

he_en

gl_en

fr_ca_en

fr_en

ﬁ_en

fa_en

eu_en

et_en

es_en

eo_en

el_en

de_en

da_en

cs_en

bs_en

bn_en

bg_en

be_en

az_en

ar_en

en_zh_tw

en_zh_cn

en_zh

en_vi

en_ur

en_uk

en_tr

en_th

en_ta

en_sv

en_sr

en_sq

en_sl

en_sk

en_ru

en_ro

en_pt_br

en_pt

en_pl

en_nl

en_nb

en_my

en_ms

en_mr

en_mn

en_mk

en_lt

en_ku

en_ko

en_kk

en_ka

en_ja

en_it

en_id

en_hy

en_hu

en_hr

en_hi

en_he

en_gl

en_fr_ca

en_fr

en_ﬁ

en_fa

en_eu

en_et

en_es

en_eo

en_el

en_de

en_da

en_cs

en_bs

en_bn

en_bg

en_be

en_az

en_ar

en_ar en_az en_be en_bg en_bn en_bs en_cs en_da en_de en_el en_eo en_es en_et en_eu en_fa en_ﬁ en_fr _fr_ca en_gl en_he en_hi en_hr en_hu en_hy en_id en_it en_ja en_ka en_kk en_ko en_ku en_lt n_mk n_mn en_mr n_ms n_my en_nb en_nl en_pl en_pt _pt_br en_ro en_ru en_sk en_sl en_sq en_sr en_sv en_ta en_th en_tr en_uk en_ur en_vi en_zh zh_cn zh_tw ar_en az_en be_en bg_en bn_en bs_en cs_en da_en de_en el_en eo_en es_en et_en eu_en fa_en ﬁ_en fr_en ca_en gl_en he_en hi_en hr_en hu_en hy_en id_en it_en ja_en ka_en kk_en ko_en ku_en lt_en k_en n_en mr_en s_en y_en nb_en nl_en pl_en pt_en br_en ro_en ru_en sk_en sl_en sq_en sr_en sv_en ta_en th_en tr_en uk_en ur_en vi_en zh_en cn_en tw_en all_all

en

ee

ee

en

en_ en_

fr_

mm

mm

pt_

zh_ zh_

all_en zh_tw_en zh_cn_en
zh_en vi_en ur_en uk_en tr_en th_en ta_en sv_en sr_en sq_en sl_en sk_en ru_en ro_en pt_br_en pt_en pl_en nl_en nb_en my_en ms_en mr_en mn_en mk_en lt_en ku_en ko_en kk_en ka_en ja_en it_en id_en hy_en hu_en hr_en hi_en he_en gl_en fr_ca_en fr_en ﬁ_en fa_en eu_en et_en es_en eo_en el_en de_en da_en cs_en bs_en bn_en bg_en be_en az_en ar_en
ar_en az_en be_en bg_en bn_en bs_en cs_en da_en de_en el_en eo_en es_en et_en eu_en fa_en ﬁ_en fr_enfr_ca_en gl_en he_en hi_en hr_en hu_en hy_en id_en it_en ja_en ka_en kk_en ko_en ku_en lt_en mk_en mn_en mr_en ms_en my_en nb_en nl_en pl_en pt_epnt_br_en ro_en ru_en sk_en sl_en sq_en sr_en sv_en ta_en th_en tr_en uk_en ur_en vi_en zh_eznh_cn_eznh_tw_en all_en

en_all

en_zh_tw

en_zh_cn

en_zh

en_vi

en_ur

en_uk

en_tr

en_th

en_ta

en_sv

en_sr

en_sq

en_sl

en_sk

en_ru

en_ro

en_pt_br

en_pt

en_pl

en_nl

en_nb

en_my

en_ms

en_mr

en_mn

en_mk

en_lt

en_ku

en_ko

en_kk

en_ka

en_ja

en_it

en_id

en_hy

en_hu

en_hr

en_hi

en_he

en_gl

en_fr_ca

en_fr

en_ﬁ

en_fa

en_eu

en_et

en_es

en_eo

en_el

en_de

en_da

en_cs

en_bs

en_bn

en_bg

en_be

en_az

en_ar

en_ar en_az en_be en_bg en_bn en_bs en_cs en_da en_de en_el en_eo en_es en_et en_eu en_fa en_ﬁ en_fr _fr_ca en_gl en_he en_hi en_hr en_hu en_hy en_id en_it en_ja en_ka en_kk en_ko en_ku en_lt n_mk n_mn en_mr n_ms n_my en_nb en_nl en_pl en_pt _pt_br en_ro en_ru en_sk en_sl en_sq en_sr en_sv en_ta en_th en_tr en_uk en_ur en_vi en_zh zh_cn zh_tw en_all

en

ee

ee

en

en_ en_

Figure 2: Correlation matrix between language pairs. The top-left corner is the correlation between the encoder head scores Henc, while the bottom-right corner is the correlation between the decoder head scores Hdec. The top matrix is the correlation matrix of the All-All model, while the bottom-left and the bottom-right ones are the correlation matrices of the All-En and the En-All models respectively.

Model
All-En En-All All-All All-All

Lang. Pair
X-En En-X X-En En-X

H enc
.871 (.086) .806 (.153) .898 (.073) .813 (.126)

H dec
.973 (.023) .720 (.150) .967 (.029) .762 (.141)

H cross
.978 (.024) .662 (.204) .980 (.018) .677 (.236)

H self
.959 (.024) .771 (.115) .948 (.046) .810 (.101)

Table 9: Correlation between the attention head scores when estimated using different language pairs. Hcross is the scores for heads across the encoder and the decoder, and Hself is the scores for the self-attention head in the
decoder.

Model
All-En En-All All-All All-All

Lang. Pair
X-En En-X X-En En-X

H enc
.683 (.190) .839 (.187) .704 (.169) .664 (.213)

H dec
.925 (.064) .679 (.145) .803 (.124) .690 (.160)

H cross
.886 (.099) .585 (.207) .787 (.129) .545 (.216)

H self
.959 (.024) .771 (.115) .948 (.046) .810 (.101)

Table 10: The results of comparing language pairs by comparing their top-10 most important attention heads. Let S(a,b) and S(c,d) be the top-10 most important heads for language pair (la, lb), and S(c,d) respectively. We calculate the F1 score between S(a,b) and S(c,d) to measure their similarity. The number in the table is the average F1 scores.

Theses random clusters are generated by (1) shufﬂing the 59 languages, (2) randomly selecting positions. The results 5 segments separated by the 4 positions are the 5 clusters.
E Closest Languages
The closest languages used in Section 5.2 are:
• Az: en-az en-eu en-ﬁ en-tr
• Be: en-be en-it en-uk
• Gl: en-gl en-pt en-es en-lt en-it en-pt_br
F Experimental Details
• Infrastructure: All the experiments can be conducted on one single RTX 2080Ti GPU.
• Evaluation: We report the BLEU score calculated by FairSeq.
• Version of FairSeq: We use v0.10.0 (https://github.com/pytorch/ fairseq/tree/v0.10.0)
• Dataset: It can be downloaded from https://github.com/neulab/ word-embeddings-for-nmt.

1 0.9 0.8 0.7 0.6 0.5 0.4 0.3

en_uk en_sr en_sq en_sl en_sk en_ru en_ro en_pt_br en_pt en_pl en_mk en_lt en_it en_hr en_he en_gl en_fr_ca en_fr en_es en_el en_de en_cs en_bs en_bg en_be
en_been_bgen_bs en_csen_de en_el en_es en_efnr _fr_ca en_glen_he en_hr en_it en_lten_mk en_pl en_epnt_pt_br en_ro en_ru en_sk en_slen_sq en_sren_uk

1 0.9 0.8 0.7 0.6 0.5 0.4 0.3

en_vi en_ru en_ro en_pt_br en_pt en_nl en_ms en_it en_id en_he en_fr_ca en_fr en_fa en_es en_el en_de en_bg en_ar
en_aren_bgen_de en_elen_es en_fa en_efnr_fr_caen_he en_id en_iten_ms en_nl en_epnt_pt_br en_ro en_ru en_vi

1 0.9 0.8 0.7 0.6 0.5 0.4 0.3

en_uk en_tr en_sv en_sr en_sq en_sl en_sk en_pl en_nb en_mk en_lt en_hu en_hr en_gl en_ﬁ en_eu en_et en_eo en_da en_cs en_bs en_be en_az
en_azen_be en_bs en_cs en_da en_eo en_et en_eu en_ﬁ en_gl en_hr en_hu en_lten_mk en_nb en_pl en_sk en_sl en_sq en_sr en_sv en_tren_uk

Figure 3: Correlation matrix between language pairs after ﬁne-tuning on the languages clusters. The ﬁrst ﬁgure is the matrix of the ﬁne-tuned All-All model. The second and the third ones are the matrix of the En-All model ﬁne-tuned on the language clusters containing the high-resource and the LRL respectively. The topleft corner is the correlation between the encoder head scores Henc, while the bottom-right corner is the correlation between the decoder head scores Hdec.

