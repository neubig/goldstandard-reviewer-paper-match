Analyzing the Language of Food on Social Media

arXiv:1409.2195v2 [cs.CL] 11 Sep 2014

Daniel Fried, Mihai Surdeanu, Stephen Kobourov, Melanie Hingle, and Dane Bell University of Arizona, Tucson, AZ, USA
Email: {dfried, msurdeanu, kobourov, hinglem, dane}@email.arizona.edu

Abstract—We investigate the predictive power behind the language of food on social media. We collect a corpus of over three million food-related posts from Twitter and demonstrate that many latent population characteristics can be directly predicted from this data: overweight rate, diabetes rate, political leaning, and home geographical location of authors. For all tasks, our language-based models signiﬁcantly outperform the majorityclass baselines. Performance is further improved with more complex natural language processing, such as topic modeling. We analyze which textual features have most predictive power for these datasets, providing insight into the connections between the language of food, geographic locale, and community characteristics. Lastly, we design and implement an online system for real-time query and visualization of the dataset. Visualization tools, such as geo-referenced heatmaps, semantics-preserving wordclouds and temporal histograms, allow us to discover more complex, global patterns mirrored in the language of food.

state-level, to region-level), with our model signiﬁcantly outperforming the random baseline (e.g., 30 times better on the state-level).
3. In addition to examining the effectiveness of our models on these predictive tasks, we analyze which textual features have most predictive power for these datasets, providing insight into the connections between the language of food, geographic locale, and community characteristics.
4. Lastly, we show that visualizations of the language of food over geographical or temporal dimensions can be used to infer additional information such as the importance of various daily meals in different regions, the distribution of different foods and drinks over the course of days, weeks and seasons, as well as some migration patterns in the United States and worldwide.

I. INTRODUCTION
Our diets reﬂect our identities. The food we eat is inﬂuenced by our lifestyles, habits, upbringing, cultural and family heritage. In addition to reﬂecting our current selves, our diets also shape who we will be, by impacting our health and wellbeing. The purpose of this work is to understand if information about individuals’ diets, reﬂected in the language they use to describe their food, can convey latent information about a community, such as its location, likelihood of diabetes, and even political preferences. This information can be used for a variety of purposes, ranging from improving public health to better targeted marketing.
In this work we use Twitter as a source of language about food. The informal, colloquial nature of Twitter posts, as well as the ease of data access, make it possible to assemble a large corpus describing the type of food consumed and the context of the discussion. Over eight months, we collected such a corpus of meal-related tweets together with relevant meta data, such as geographic locations and time of posting. We construct a system for aggregating, annotating, and querying these tweets as a source for predictive models and interactive visualizations (Fig. 1). Building on this dataset and system, the contributions of this work are fourfold:
1. We analyze the predictive power of the language of food by predicting several latent population characteristics from the tweets alone (after ﬁltering out location-related words to avoid learning trivial correlations). We demonstrate that this data can be used to predict multiple characteristics, which are conceivably connected with food: a state’s percentage of overweight population, the rate of diagnosed diabetes, and even political voting history. Our results indicate that the languagebased model yields statistically-signiﬁcant improvements over the majority-class baseline in all conﬁgurations, and that more complex natural language processing (NLP), such as topic modeling, further improves results.
2. We demonstrate that the same data accurately predicts geographic home locale of the authors (from city-level, through

II. DATA
Twitter provides an accessible source of data with broad demographic penetration across ethnicities, genders, and income levels1, making it well-suited for examining the dietary habits of individuals on a large scale. To identify and collect tweets about food, we queried Twitter’s public streaming API2

Tweet Stream

Query Filters
#breakfast, #lunch, #dinner, #snack...

LDA Topic Annotator
I love waffles #breakfast

Solr Database
Tweet Text User Metadata Tweet Metadata

Location Normalizer
El Paso, Texas

LDA Topic

TOPIC_97

Normalized Location

TX

Tweets + Topics Metadata Filters

Predictive Models
Overweight Rate Diabetes Rate Political Tendency Locale (City, State, Region)

Online Visualizations
Distinctive Terms per State Geographic Heatmaps Temporal Histograms Parallel Wordclouds

Fig. 1: The main steps of the system are: collecting tweets from Twitter using a set of meal-related ﬁlters, loading the tweets and their meta data into a Lucene-backed Solr instance, annotating the tweets with topic model labels (Section IV-B) and normalizing locations (Section II), and then querying the tweets for use in the predictive models (Section III) or visualization systems (Section VI).

1http://www.pewinternet.org/2014/01/08/social-media-update-2013/ twitter-users/
2https://dev.twitter.com/docs/api/streaming. Note: Twitter caps the number of possible tweets returned by the streaming API to a fraction of the total number of tweets available at a given moment.

Term
#dinner #breakfast #lunch #brunch #snack #meal #supper Total

# of Tweets
1,156,630 979,031 931,633 287,305 139,136 94,266 32,235
3,498,749

# with normalized US Location 173,634 161,214 129,853 86,239 21,539 12,149 2,971 562,547

TABLE I: Hashtags used to collect tweets, and number of tweets containing each hashtag. “Normalized US location” indicates that we could extract at least the user’s state from the meta data. Since some tweets contain multiple meal hashtags, the total number of tweets (bottom row) is less than the column sum.

for posts containing hashtags related to meals (Table I). We collected approximately 3.5 million tweets containing at least one of these hashtags from the period between October 2, 2013 and May 29, 2014.
Tweets are very short texts, limited to 140 characters. In our collection, the average length of a tweet is 8.7 words, after ﬁltering out usernames, non-alphanumeric characters (hashtags excepted), and punctuation. The tweet collection contains a total of about 30 million words. Of these, there are around 1.5 million unique words.
Fig. 1 describes the system used to collect, annotate, and process the tweets for prediction and visualization. Along with the text of each tweet, we store the user’s self-reported location, time zone, and geotagging information, whenever these ﬁelds are available. This meta data is used to group tweets by the home location of the author, e.g., speciﬁed as city and/or state for those users located within the United States (US). For most experiments in this paper, geolocation normalization is performed using regular expressions, matching state names or postal abbreviations of one of the 50 US states or Washington, D.C. (e.g., Texas or TX), followed by matching city names or known abbreviations (e.g., New York City or NYC) within the author’s location ﬁeld. In case of ambiguities (e.g., LA stands for both Los Angeles and Louisiana) we used the user’s time zone to disambiguate. About 16% (562,547) of the collected tweets could be located within a state using this method (Table I). We chose to use the self-reported user location instead of the geotagging information because: (a) it is more common, (b) it tends to have a standard, easily parseable form for US addresses, and (c) to avoid potential biases introduced by travel. However, in Section VI, we extend our analysis to discover world-wide food-related patterns. In this context, because world addresses are considerably harder to parse than US addresses, we revert to geotagging information to identify the location of tweet authors.
Using this dataset, we can immediately see food-driven patterns. For example, Fig. 2 shows prominent food-related words that appeared in the tweets normalized to each state. Tweet text is ﬁltered using a list of approximately 800 foodrelated words (see Sec. IV-A). Terms are ranked using term frequency–inverse document frequency (tf-idf) [10] to discount words that occur frequently across all states, and give priority to those words that are highly representative of a state. Each state’s food word with the highest tf-idf ranking is displayed in the map. Regional trends can be seen, for example grits, a breakfast food made from ground corn, is a common dish in the

halibut soysauce
capers spuds
watercress

sauerkrautmillet caviar

vinegar

tamale mutton

ﬂan venison

durian venison

soybeans peach prune pickle

sauerkraut

maize

grits

prune

cod succotash

cloves

grits

lentils

pastry

clam

yolk gyro preserves

cobbler grits

succotash

rye guava

taco

blackberry grits

entree

grits

brisket

tangeringerits grits

grits

tarragon

grits

taro

Fig. 2: Most distinctive food word per state from the corpus of foodrelated tweets. Terms are ﬁltered by a list containing about 800 foodrelated terms (Section IV-A) and ranked using tf-idf. Note that “Prune” is the name of a popular restaurant.

southern states, and various types of seafood (halibut, caviar, cod, clam) are popular in the eastern and western coastal states.
III. TASKS
To understand the predictive power of the language of food, we implement several prediction tasks that use the tweets in the above dataset as their only input. We group these tasks into two categories: state-level characteristic prediction and locale prediction.
A. Predicting State-Level Characteristics
Here we predict three aggregate characteristics for US states, using features extracted from the tweets produced by individuals in each state:
1) Diabetes Rate: This is the percentage of adults in each state who have been told by a doctor that they have diabetes. Data in this set is taken from the Kaiser Commission on Medicaid and the Uninsured (KCMU)’s analysis of the Center for Disease Control’s Behavioral Risk Factor Surveillance System (BRFSS) 2012 survey.3 We convert this data into a binary dependent variable by considering whether a state’s rate of diabetes is above or below the national median. The median diabetes rate is 9.7%, and the range is 6.0% (7.0% in Alaska to 13.0% in West Virginia). For example, Alabama has a diabetes rate of 12.3%, which is above the national median of 9.7%, so it is labelled as high-diabetes, while Alaska, with a rate of 7.0%, is labelled as low-diabetes.
2) Overweight Rate: This is the percentage of adults within each state who reported having a Body Mass Index (BMI) of at least 25.0 kilograms per meter squared, placing them within the “overweight” or “obese” categories deﬁned by the National Institutes of Health.4 As with the diabetes rate dataset, data is taken from KCMU’s analysis of the BRFSS 2012 survey results5. Similarly, the corresponding binary dependent variable indicates if a state’s overweight rate is above/below the national median. The median overweight rate is 64.2%,
3The BRFSS is a random-digit-dialed telephone survey of adults age 18 and over. For more details see: http://kff.org/other/state-indicator/ adults-with-diabetes/
4http://www.nhlbi.nih.gov/guidelines/obesity/BMI/bmi-m.htm 5http://kff.org/other/state-indicator/adult-overweightobesity-rate/

and the range is 17.7% (51.9% in Washington, D.C. to 69.6% in Louisiana).
3) Political Tendency: This dataset measures historical voting history over a 5-year period: whether a state is more Democratic or Republican relative to the median US state, as measured by proportion of Democratic/Republican votes in general presidential, gubernatorial, and senatorial elections, in the interval from 2008 to 2013.6. For example, Alaska cast 554,565 total votes for Democratic candidates and 748,488 for Republican candidates in these three types of elections during the six-year period, for a fraction of 42.6% Democratic votes. This is below the median fraction of 51.6%, so Alaska is labelled as Republican. Votes are compared relative to the median because of a slight bias toward Democratic votes during this time period. The median fraction of Democratic votes is 51.6%, and the range is 65.4% (27.0% in Wyoming to 92.4% in Washington D.C.).
Because the above dependent variables are at state level, each state is treated as a single instance for these three tasks: all of the tweets produced within the state are aggregated into a single pool for feature extraction (detailed in the next section). We used Support Vector Machines (SVM) with a linear kernel [22] for classiﬁcation.
Although such a prediction task has many features (from all tweets in a given state), it has a small number of data points (51, one for each state plus Washington, D.C.). For this reason, we use leave-one-out cross-validation to evaluate the accuracy of the model. For each of the three data sets (overweight, diabetes, and political), we use the following process: Each state is held out in turn. The SVM is trained on features of tweets taken from the remaining 50 states, using the labels of the current data set. The SVM is then used to predict the current dataset’s label of the held-out state. The accuracy of the model on the label set is calculated as the number of correct predictions out of the total number of states. To avoid overﬁtting, we do not tune the classiﬁer’s hyper-parameters.
B. Predicting Locales
To examine the connection between the language of food and geographic location, we seek to predict the locale of a group of tweets, using only the text of the tweets as input. We predict locales at different levels: city, state, and region. It is important to note that, to focus our analysis on the predictive power of the language of food, we remove as many state and city names as possible from the tweets to avoid learning trivial correlations (see Sec. IV-A).
1) City: The locales in the city prediction task are the 15 most populous cities in the US.7
2) State: Locales in the state prediction task are the 50 US states, plus Washington, D.C. As discussed in the previous section, both city and state labels are assigned to tweets by parsing the self-reported author home location in the meta data.
3) Region: The ﬁnal variant of the locale prediction task is to predict the geographic region of the US containing the user’s state. We use four geographic regions, taken from the US Census Bureau: Midwest, West, Northeast, and South.8
Similar to the previous tasks, here we also aggregate tweets with the same locale for feature extraction, and use a linear
6http://uselectionatlas.org/ 7http://en.wikipedia.org/wiki/List of United States cities by population 8http://www.census.gov/geo/maps-data/maps/pdfs/reference/us regdiv.pdf

kernel SVM for classiﬁcation. However, the overall setup is different. Because the goal here is to predict locale itself, we divide the tweets from each locale into training and testing sets consisting of 80% and 20% of the tweets available for that locale, respectively. Tweets are sorted chronologically so that all training tweets for a given locale were posted before all the corresponding testing tweets. The overall training/testing corpora include training/testing tweets from all locales. Using this dataset, we train a one-against-many multi-class SVM classiﬁer for the locale as the dependent variable. For example, for the city detection task, the SVM classiﬁes a group of tweets as belonging to one of the 15 known cities.
The accuracy on each locale prediction task is the number of locales correctly identiﬁed, divided by the total number of locales. Thus, a baseline classiﬁer that randomly predicts a locale would achieve an accuracy of 1/51 = 1.96% on the state prediction task, 1/15 = 7% on the city task, and 1/4 = 25% for region prediction.
IV. FEATURE DESCRIPTIONS
We use two sets of features: lexical (from tweet words) and topical (sets of words appearing in similar contexts).
A. Lexical Features
We take the simple approach of representing each locale as a bag of words assembled from all the tweets in that group. Each word becomes a feature with value equal to the number of times it occurrs across all tweets for that locale. We tokenize the tweets using the Stanford CoreNLP software.9 An additional pre-processing step removes the following tokens: (a) tokens that do not contain alpha-numeric characters or punctuation (to reduce noise); (b) stopwords and words that occur a single time (to reduce data size); and, most importantly, (c) URLs, usernames (preceded by an @ symbol), and words and hashtags naming state and city locations10 (to avoid learning trivial correlations, such as #TX indicating a tweet from Texas).
We also experiment with open versus closed vocabularies. For open vocabularies, we use two conﬁgurations: all words produced by the above pre-processing step, or only hashtags. For a closed vocabulary experiment, we use a set of 809 words related to food, meals, and eating, obtained from the English portion of a Spanish-English food glossary11 and an online food vocabulary list12. These experiments will help us understand how much predictive power is contained in food words alone versus the full text (or hashtags) of the tweets, which capture a much broader context.
B. Topic Model Features
Topic models provide a method to infer the themes present in tweets, represented as clusters of words that tend to appear in similar contexts (e.g., a topic learned by the model, which we refer to as the American diet topic, contains chicken, baked, beans, and fried, among other terms). Using topics as features is beneﬁcial for a couple of reasons: (1) topics provide a
9http://nlp.stanford.edu/software/corenlp.shtml 10In addition of known state names and abbreviations we used a list of the 250 most populous cities in the US from http://en.wikipedia.org/wiki/List of United States cities by population, together with common nicknames, such as “#sanfran” for San Francisco. 11http://www.lingolex.com/spanishfood/a-b.htm 12http://www.enchantedlearning.com/wordlist/food.shtml

method to address the sparsity resulting from having very short documents (tweets are limited to 140 characters) by treating groups of related words as a single feature; (2) topical features aid in post-hoc analysis by allowing us to detect correlations that go beyond individual words.
We use Latent Dirichlet Allocation (LDA) [3] to learn a set of topics from the food tweets in an unsupervised fashion. LDA treats each tweet as a mixture of latent topics. Each topic is itself a probability distribution over words, and the words in the tweet are viewed as being sampled from this mixture of distributions. The LDA model (topic distributions and mixtures) is trained from all available tweets in the corpus, using the MALLET software package.13 We chose 200 as the number of topics for the model to learn. This number produced topics that seemed ﬁne-grained enough to capture speciﬁc patterns in diet, language, or lifestyle – clusters of foods of various nationalities, or speciﬁc diets such as vegetarian. For clarity in our analysis, we have manually assigned subject labels, such as American diet, to some of these topics based on the words contained in the topic.14 We use these assigned labels to refer to the topics in the remainder of this paper. Once the LDA topic model is trained, we use it to infer the mixture of topics for each tweet in the prediction tasks. The topic most strongly associated with the tweet (the topic with highest probability given the model and the tweet) is used as an additional feature for the tweet, similarly to the lexical features generated from the words of the tweet. Topics are counted across all tweets in a state in the same manner as the lexical features.
When applied in combination with the conﬁguration containing solely food word or hashtag vocabularies, the LDA topics are constructed using the corresponding ﬁltered versions of the tweets, i.e., with all non-food words or non-hashtag words removed.
To account for the large differences in the number of tweets available for each state (for example, the state with the most normalized tweets, New York has 83,670 tweets, while the state with the fewest, Wyoming, has 339), we scale all the features collected for each state. Each feature’s value within a state’s feature set is divided by the number of tweets collected for the state.
V. RESULTS
We present empirical results for both categories of tasks introduced in the previous section: predicting state-level characteristics and predicting locales. We also analyze the effectiveness of the language of food for these prediction tasks by examining the most important textual features in the classiﬁcation models, and investigating the importance of open versus closed vocabularies.
A. State-Level Characteristics
Table II shows classiﬁcation results on the state-level statistics prediction task (Section III-A) for varying feature sets. Since all three datasets are nearly evenly split between the binary classes (each dataset has either 25 or 26 states out of 51 in each of the two classes), a baseline that predicts the majority label achieves approximately 51% accuracy. We compare the performance of the tweet-based predictive models to this majority baseline, and evaluate how ﬁltering the lexical content
13http://mallet.cs.umass.edu/ 14But these topic labels are not visible to the classiﬁer.

majority baseline All Words All Words + LDA Hashtags Hashtags + LDA Food Food + LDA Food + Hashtags Food + Hashtags + LDA

overweight
50.98 76.47‡ 80.39‡ 72.55‡ 74.51‡ 70.59‡ 68.63† 64.71† 74.51‡++

diabetes
50.98 64.71 64.71 68.63† 68.63† 60.78 60.78 62.75 62.75

political
50.98 66.67‡ 68.63‡
60.78
62.75 68.63‡ 72.55‡ 64.71† 64.71†

average
50.98 69.28‡ 71.24‡ 67.32‡ 68.63‡ 66.67‡ 67.32‡ 64.05‡ 67.32‡+

TABLE II: Using features of tweets to predict state-level char-
acteristics: whether a given state is above or below the national
median for overweight rate, above or below the median diagnosed
diabetes rate, and the state’s historical political voting trend (D or
R). This table compares the effect of ﬁltering the lexical features to:
food words, hashtags, both, or keeping the entire text of the tweets;
as well as the effect of adding LDA topics. Throughout the paper, we mark results as follows: ‡denotes a signiﬁcant (p <= 0.05) and †a nearly-signiﬁcant (0.05 < p <= 0.10) improvements over the majority baseline. Similarly, ++denotes that the LDA model has a statistically signiﬁcant (p <= 0.05) and +a nearly statistically signiﬁcant (0.05 < p <= 0.10) improvement over the model without
LDA. Statistical signiﬁcance testing is implemented using one-tailed,
non-parametric bootstrap resampling with 10,000 iterations.

of the tweets and adding topical features affects accuracy on these prediction tasks. We draw several observations from this experiment:
(a) First and foremost, the language of food can indeed infer all the latent characteristics investigated: all conﬁgurations investigated statistically outperform the majority-class baseline. The best performance is obtained when the entire text of the tweets is used (All Words), which captures not only direct references to food, but also the context in which it is discussed. However, the performance of the closed vocabulary of food words (Food) is within 5% of the best performance, demonstrating that most of the predictive signal is captured by direct references to food.
(b) The classiﬁers achieve the highest accuracy on the overweight dataset. This is an intuitive result, which conﬁrms that there is a strong correlation between food and likelihood of obesity. However, the fact that this correlation can be detected solely from social media posts is, to our knowledge, novel and suggests potential avenues for better and personalized public health. A similar correlation with political preferences is also interesting, indicating potential marketing applications in the political domain.
(c) More complex NLP (topic modeling in our case) is beneﬁcial: the performance of the models that include LDA topics is, on average, better than that of the conﬁgurations without topics.15 We plan to use more informative representations of text, e.g., based on deep learning [21], in future work.
Table III shows the words and topical features assigned the greatest importance, i.e., largest magnitude weights, by the SVM training process, for each dataset and class. It is interesting to note that a dietary topic we have labeled as American Diet, containing terms such as chicken, baked, beans and fried, is an important feature for predicting both
15The improvement is not statistically signiﬁcant for most experiments, but this can be attributed to the small size of the dataset (51 data points).

Class overweight: + overweight: diabetes: + diabetes: -
Democrat Republican

Highest-weighted features
i, day, my, great, one, American Diet (chicken, baked, beans, fried), #snack, First-Person Casual (my, i, lol), cafe, Delicious (foodporn, yummy, yum), After Work (time, home, after, work), house, chicken, fried, Breakfast (day, start, off, right), #drinks, bacon, call, eggs, broccoli You, We (you, we, your, us), #rvadine, #vegan, make, photo, dinner, #meal, #pizza, Giveaway (win, competition, enter), new, Restaurant Advertising (open, today, come, join), #date, happy, #dinner, 10, jerk, check, #food, #bento, #beer
Mexican (mexican, tacos, burrito), American Diet (chicken, baked, beans, fried), #food, After Work (time, home, after, work), #pdx, my, lol, #fresh, Delicious (foodporn, yummy, yum), #fun, morning, special, good, cafe, #nola, fried, bacon, #cooking, all, beans #dessert, Turkish (turkish, kebab, istanbul), #foodporn, #paleo, #meal, Paleo Diet (paleo, chicken, healthy), i, Giveaway (win, competition, enter), I, You (i, my, you, your), your, new, today, #restaurant, Japanese (ramen, japanese, noodles), some, jerk, #tapas, more, Healthy DIY (salad, chicken, recipe), You, We (you, we, your, us)
#vegan, #yum, w, served, #brunch, Deli (cheese, sandwich, soup), photo, #rvadine, Restaurant Advertising (open, today, come, join), #breakfast, #bacon, delicious, #food, #dinner, 21dayﬁx, like, #ad, Giveaway (win, competition, enter), toast, 1 my, #lunch, i, Airport (airport, lounge, waiting), easy, #meal, tonight, #healthy, #easy, us, sunday, After Work (time, home, after, work), #party, #twye, First-Person Casual (my, i, lol), your, #snack, join, #delicious, house

TABLE III: Top 20 highest-weighted features in descending order of importance for each dataset, for both the positive and negative classes. For example, “overweight: +” indicates the most representative features for being overweight, whereas “overweight: -” shows the most indicative features for not being overweight. The features include LDA topics, with manually assigned names (italicized) for clarity, and a few of their most common words within parentheses.

that a state has higher rates of overweight and diabetes than normal, whereas other diets, such as #vegan and Paleo Diet are important predictors for the opposite. Note that pronouns have high weights in the overweight prediction task: the ﬁrstperson singular I and my are valuable for predicting that a state is overweight, while collective words such as the You, We topic cluster are valuable for predicting that a state is below the median. This is less surprising in view of prior work, such as Ranganath et al. [17], showing that the types of pronouns used by an individual are associated with a host of traits such as gender and intention. For the political afﬁliation task, we observe that features correlated with Republican states include those centered around work (the Airport topic) and home (the After Work topic, including words such as home, after, work). The most predictive feature for Democratic states is #vegan, and we also see topics associated with urban life and eating out, such as Deli, #brunch, promotions such as Restaurant

model
Random Baseline All Words All Words + LDA Food Food + LDA Hashtags Hashtags + LDA Food + Hashtags Food + Hashtags + LDA

accuracy (%)
6.67 66.67‡ 80.00‡+ 40.00‡ 40.00‡ 53.33‡ 66.67‡ 53.33‡ 86.67‡++

TABLE IV: City prediction accuracy (15 most populous US cities) for the various feature sets. Statistical signiﬁcance testing is performed similarly to Table II.

training fraction 0.2 0.4 0.6 0.8 1.0

0.2 6.66 13.33 20.00 33.33 46.66

testing fraction 0.4 0.6 0.8 6.66 6.66 6.66 13.33 13.33 13.33 26.66 26.66 26.66 46.66 33.33 53.33 53.33 60.00 66.66

1.0 6.66 20.00 40.00 53.33 80.00

TABLE V: Effects of varying the fraction of tweets used for training and testing on classiﬁcation accuracy in the city-prediction task, using All Words and LDA topics.

Advertising, and Eating Out.
B. City Prediction
For the ﬁrst locale prediction task we focus on city identiﬁcation. Table IV shows the accuracies of the various feature sets for this task. The input for this task is 15 cities, so the random-prediction baseline accuracy is 6.67%. As in the previous task, every set of features improves signiﬁcantly upon this baseline, ranging from 40% accuracy using only Food words to 86.67% accuracy using Food words, Hashtags, and LDA topics, demonstrating once again the predictive power of the language of food. The signiﬁcant improvement of the closed food vocabulary alone (Food) over the baseline indicates that the diets in each of these 15 cities are distinct enough to have some predictive power. However, diets alone are not enough to completely identify the cities, and we see that for this task more context is beneﬁcial: adding hashtags helps considerably (53.33% accuracy), and adding topical features to the food and hashtag ﬁltered set of lexical features improves performance even further (86.67%).
Table V, which measures performance as the size of training/testing sets varies, indicates that accuracy is greatly affected by the size of the data available for both training and testing. Indeed, when only 20% of the training set is used, the models achieve the same score as the baseline classiﬁer (6.67%). Performance continues to increase as we add more data, suggesting that we have not reached a performance ceiling yet.
Table VI lists the top ﬁve features for each city in this task. The table shows that variations in diet are clear: tacos are signiﬁcant in Austin, #vegetarian food is indicative of San Francisco, #brunch is representative of New York, etc. Using the context around food is clearly important. We see that several cities in California are associated with #foodie (Los

City Austin Chicago Columbus Dallas Houston Indianapolis Jacksonville Los Angeles New York City
Philadelphia Phoenix San Antonio San Diego
San Francisco San Jose

Highest-weighted features we, come, tacos, #tacos, Mixed Drinks (bottomless mimosas, bloody mary) Giveaway (win, competition, enter), jerk, #breakfast, #bbq, #foodie #breakfast, #asseenincolumbus, Directions (west, local, east), #cbus, #great #lunch, my, lunch, porch, come After Work (time, home, after, work), #lunch, #snack, i, #breakfast you, our, delicious, You, We (you, we, your, us), side #dinner, #ebaymobile, #food, kitchen, #yum my, #foodie, Directions (west, local, east), #timmynolans, #tolucalake #brunch, Mixed Drinks (bottomless mimosas, bloody mary), our, Eggs and Bacon (eggs, benedict, bacon), #sarabeths cafe, day, #ﬁshtown, shot, #byob #lunch, #easy, Wine (wine, 2014, today), st, we my, i, 1, bottomless, our Restaurant Advertising (open, today, come, join), #bottomless, Mixed Drinks (bottomless mimosas, bloody mary), Vacation (beach, hotel, vacation), your #vegetarian, #dinner, #foodie, brunch, Vacation (beach, hotel, vacation) #foodporn, #dinner, bill, #bacon, #goodeats

TABLE VI: Top ﬁve highest-weighted features for predicting each city from its tweets. Features include All Words and LDA topics. LDA topics are manually assigned names (italicized) for clarity, and a few of the most common words are displayed next to each topic.

model
Random Baseline All Words All Words + LDA Food Food + LDA Hashtags Hashtags + LDA Food + Hashtags Food + Hashtags + LDA

accuracy (%)
1.96 60.78‡ 66.67‡++ 33.33‡ 35.29‡ 62.75‡ 56.86‡ 56.86‡ 54.90‡

TABLE VII: State prediction accuracy for the various features sets.

Angeles and San Francisco) or eating while on Vacation (San Diego and San Francisco). First-person pronouns are highly weighted in cities in Texas (we in Austin, I in Houston, and my and I in San Antonio).
C. State Prediction
Table VII lists the results for the state prediction task. There are 51 possible locales in this task, from the 50 US states plus Washington D.C., so the random-prediction baseline achieves 1.96% accuracy. As in all previous experiments, the stateprediction model improves signiﬁcantly upon this baseline with every set of features that we tried. The model achieves its lowest accuracy, 33.33%, using the set of food words without topical features (Food). Unlike in the city prediction task but similar to the state-level characteristic prediction task, the model is most accurate when using the unﬁltered tweets

training fraction 0.2 0.4 0.6 0.8 1.0

0.2 11.76 19.60 25.49 39.21 43.13

testing fraction 0.4 0.6 0.8 11.76 5.88 9.80 17.64 17.64 17.64 29.41 35.29 41.17 41.17 43.13 50.98 58.82 54.90 62.74

1.0 15.68 25.49 47.05 52.94 64.70

TABLE VIII: Effects of varying the fraction of tweets used for training and testing on classiﬁcation accuracy in the state-prediction task, using All Words and LDA topics.

model

Random Baseline All Words All Words + LDA Food Food + LDA Hashtags Hashtags + LDA Food + Hashtags Food + Hashtags +

LDA

accuracy (%)
25 50 75 50 50 50 75 50 75

TABLE IX: Region prediction accuracy for the various feature sets.

with topical features (All Words + LDA), reaching 66.67% accuracy. This indicates that the closed food vocabulary is not sufﬁcient for optimal performance on this task, and the larger food-related context is required for optimal performance.
Table VIII analyzes the effect of the number of available tweets on prediction accuracy. Performance varies from 11.76% when using 20% of tweets in the training set and 20% in the testing set, to 64.7% when using all available tweets. Increasing the number of tweets in the training set has a larger positive effect on accuracy than increasing the number of tweets in the testing set. As in the city prediction task, performance continues to increase as we add more data, suggesting that the performance ceiling has not been reached.
Due to space consideration, we include the top features per state and the corresponding discussion in the supplemental material.16
D. Region Prediction
The ﬁnal locale prediction task predicts the four major US geographic regions: Midwest, Northeast, South, and West (Section III-B3) using tweets from each region. The high level of geographic granularity (each region contains about a dozen states, on average) simpliﬁes the tweet-based task in one sense, since there are now fewer possible classiﬁcation labels, but also makes the task more difﬁcult because of the variation in diet and tweet lexical content within these broad geographic regions. The random-prediction baseline in this task achieves 25% accuracy. Three of the feature sets, however, achieve 75% accuracy, only misclassifying a single region.17 We also see that for all of the feature sets except the closed food vocabulary (Food), lexical features give one more correct region classiﬁcation over the baseline, and adding
16https://sites.google.com/site/twitter4food/ 17Since there are only four data points in the testing set, measuring if improvements over the baseline are statistically signiﬁcant cannot be reliable, so it is skipped here.

#dinner

#breakfast

#brunch

#lunch

Fig. 3: The highest-weighted feature for the region prediction task in each of the four US Census geographic regions: West (blue), Midwest (orange), South (green), and Northeast (red).

Region Midwest
Northeast
South
West

Highest-weighted features #breakfast, i, #recipes, After Work (time, home, after, work), Recipe (recipe, easy, meal), your, #meals, breakfast, Promotional (free, off, today), I, You (i, my, you, your) #brunch, brunch, our, Mixed Drinks (bottomless mimosas, bloody mary), we, w, Roasted Meats (pork, chicken, special), Group Dining (our, us, join), you, new #lunch, Mixed Drinks (bottomless mimosas, bloody mary), After Work (time, home, after, work), American Diet (chicken, baked, beans, fried), chicken, #cltfood, mimosas, bottomless, us, my #dinner, #food, #foodporn, photo, dinner, w, #vegan, Mexican (mexican, tacos, burrito), #bomb, #pdx

TABLE X: Top 10 highest-weighted features for predicting each region from its tweets. Features include All Words and LDA topics. LDA topics are manually assigned names (italicized) for clarity, and a few of the most common words are displayed next to each topic.

topical features yields an additional correct classiﬁcation. The feature set consisting of all words and the LDA topics classiﬁes three out of four regions correctly, only misidentifying testing tweets from the Midwest as being from the Northeast.
Table X shows the most predictive features for each region. We see a clear preference for certain meal term hashtags in the table: #breakfast is a strong predictor for the Midwest, #brunch for the Northeast, #lunch for the South, and #dinner for the West; see Fig. 3. Brunch and mixed drinks are important features in the Northeast, likely because they were also highly weighted features for New York City, and New York City produced many of the tweets in this region. The West is a mixture of features that were important for California, such as #foodporn and #vegan, and the diet of other states in the region, such as the Mexican food topical feature. The Northeast and South are similar in that both have the Mixed Drink topic and a meat-related topic, but differ in other features, such as the Northeast’s #brunch and Group Dining, and the South’s #lunch and After Work topic.
VI. VISUALIZATION TOOLS
While the machine learning models described above are well-suited for prediction on predeﬁned tasks, we also constructed several visualization tools to discover previously unknown trends in the Twitter dataset. These tools aim to

allow aggregate analysis of tweet content in the context of geographic and temporal location.
A. Top Terms by State
The ﬁrst of these tools, the term visualizer (Fig. 2), does a simple keyword analysis of the tweets available for each US state. We extract all terms that are contained within a list of around 800 food-related words (see Sec. IV-A) and rank them using tf-idf, treating all tweets normalized to a given state as a single document: each term’s score is the number of times it occurred within a state, multiplied by the logarithm of the inverse proportion of the number of states it occurred in [10]. Ranking by tf-idf emphasizes words that are common in a particular state, but ensures that words used frequently in all states, such as food and eat, are not highly ranked. The term(s) with the highest ranking in each state are displayed on the state in the map. As discussed previously, this tool immediately highlights dietary patterns: grits in the Southern states, etc.
B. Temporal Histograms
Temporal histograms allow us to visualize the changing popularity of terms over the course of a day, week, or year. About 71% of the collected tweets (2,503,351) are from users who have listed their time zone. For these tweets, we compute the time local to the user when the tweet was posted. The temporal visualization tool (Fig. 4) allows querying these time-localized tweets by phrase and constructing histograms at varying time granularities: hour of day, day of the week, or month of the year. On the weekly scale, it is easy to see that while breakfast is more or less uniform all week long, brunch occurs much more frequently on weekends, particularly Sunday. On the daily scale, wine peaks around 8pm, while beer follows a bi-modal distribution, with two roughly equal-sized peaks around 1pm and 8pm.
C. Tweet Location Maps
About 10% of the collected tweets (362,978) have associated geolocation information – the user’s longitude and

4,000

4,000

3,000

3,000

2,000

2,000

1,000

1,000

Mon Tue Wed Thu Fri Sat Sun 0
(a) breakfast by day of post

Mon Tue Wed Thu Fri Sat Sun 0
(b) brunch by day of post

1,000

1,000

800

800

600

600

400

400

12am

6am

12pm

6pm

200 0 12am

12am

6am

12pm

6pm

200 0 12am

(c) wine by hour of post

(d) beer by hour of post

Fig. 4: Temporal histograms showing the popularity of the phrases breakfast and brunch by day of the week, and beer and wine by hour of the day. For each term, 10,000 tweets are sampled from users who have listed their time zone, which is then used to obtain the local time of the tweet.

(a) Heatmaps of 7,372 tweets from three Italian food (pasta, pizza, italian, carbonara, lasagna, ...) topics.
(b) Heatmaps of 1,032 tweets from a Vietnamese food (pho, vietnamese, ...) topic.
(c) Heatmaps of 2,226 tweets from a Full Breakfast (“bacon”, “eggs’, “sausage”, “biscuits”, “grits”, ...) topic. Fig. 5: Heatmaps showing migration patterns reﬂected in diet: Italian food has the largest concentration in the United States in New York City and Vietnamese food is highly concentrated in California, particularly Los Angeles and San Francisco.

Fig. 6: Tweet geolocation plot showing migration patterns reﬂected in diet: yellow dots mark the locations of 11,827 tweets matching ﬁve Spanish/Latin American food topics (tacos, burrito, salsa, pollo, arroz, paella, etc.).
latitude at the moment the tweet was posted. We use this meta-information to build a system for querying and plotting worldwide geographic maps of tweets. The interface allows searching by phrase or LDA topic and displays geographic plots or heatmaps showing the locations of all tweets matching the query.
This system allows the discovery of broad geographic trends in the data. For example, Fig. 5 shows heatmaps made from queries for several LDA topics for foods of various geographic origins. These topics are perhaps reﬂective of immigration patterns to the US or worldwide, e.g., the Italian food topic has high intensity in Italy and New York City (Fig. 5a) and the Vietnamese food topic has high intensity in Vietnam and in Southern California (Fig. 5b). The Full Breakfast topic, reﬂecting a traditional British and American breakfast of bacon, eggs, and sausage, is pronounced throughout all of English-speaking United Kingdom and the heavily populated regions of both the western and eastern United States (Fig. 5c). Similarly, Fig. 6 shows the prevalence of Spanish and Latin-American inﬂuenced food throughout the Spanishspeaking world, including portions of the United States and the Philippines.
D. Parallel Word Clouds
Word clouds offer a space-efﬁcient way to summarize text by highlighting important words. Semantics-preserving word clouds also add the feature that related words (e.g., those that frequently co-occur) are placed close to each other [2]. Our parallel word clouds further focus on comparing and contrasting two or more groups of texts: words are scaled by importance, related words are close to each other, and important words that occur in both groups are in the same locations in all clouds (citation hidden for review). Fig. 7 shows such parallel word clouds for weekday vs. weekend tweets, highlighting a different set of trends: family, brunch, and even breakfast are more prominent on weekends; work and tonight are common on weekdays (in red); and, ﬁnally, restaurant, and fun are present on weekends (in blue).
VII. RELATED WORK
Previous work has used textual analysis of Twitter posts to study diverse and global populations, including investigating temporal changes in mood [7] and correlations between religious expression and sentiment [19]. Several other works predict latent characteristics of individuals and communities

using social media posts and metadata. Rao et al. [18] predict gender, age, regional origin, and political orientation for individual Twitter users, using tweets and a set of hand-constructed linguistic features. Burger et al. [4] and Bamman et al. [1] predict users’ gender using their tweets and additional meta information, such as name, self description, and their social network. Jurafsky et al. [9] analyze a corpus of restaurant reviews and predict restaurant ratings using linguistic features such as sentiment, narrative, and self-portrayal.
Paul and Dredze [16] apply the Ailment Topic Aspect Model to 1.5 million health-related tweets and discover mentions of over a dozen ailments, including allergies and insomnia. Schwartz et al. [20] use Twitter to predict public health and well-being statistics on a state-wide level. The language used in 82 million geo-located tweets from 1,300 different US counties is used to predict the subjective well-being of people living in those counties. As in our study, LDA topics improved accuracy. Hingle et al. [8] use Twitter together with analytical software to capture real-time food consumption and diet-related behavior. While this study identiﬁes relationships between dietary and behavioral patterns the results were based on a small dataset (50 participants and 773 tweets). Nascimento et al. [13] evaluate self-reported migraine headache suffering using over 20,000 migrane-related tweets over a seven-day period, ﬁnding different peaking hours on weekdays and weekends. Yom-Tov et al. [23] show how Twitter can be used to discover possible outbreaks of communicable diseases at large public gatherings. Mysl´ın et al. [12] use machine classiﬁcation of tobacco-related Twitter posts to detect tobacco-relevant posts and sentiment towards tobacco products.
Previous work has modelled linguistic variation on Twitter in terms of demographic and geographic variables. O’Connor et al. [15] create a generative model of word use from demographic traits, and show clusters of Twitter users with common lexicons. Eisenstein et al. [5], [6] show that despite the global diffusion of social media, geographic regions have distinct word and topic use on Twitter.
Previous work has also developed systems for aggregating, processing, and visualizing tweets. McCreadie et al. [11] develop a system for detecting newsworthy events and clustering tweets in real-time. Nguyen et al. [14] produce geographic visualizations of tweet sentiment using machine learning classiﬁers and the tweets’ location metadata.
Our work builds upon these previous results, and is, to our knowledge, the ﬁrst to provide a large-scale, empirical analysis of the predictive power of the language of food.
VIII. CONCLUSION AND FUTURE WORK
This work empirically demonstrates that food and food discussion are a major part of who we are. We develop a system for collecting a large corpus of food-related tweets and using these tweets to predict many latent population characteristics: overweight and diabetes rates, political learning, and geographic location of authors. Furthermore, we integrate several visualization tools that summarize and query this data, allowing us to discover more complex geographical/temporal trends that are driven by the language of food, such as potential migration patterns. Our analysis indicates that the language of food alone is extremely powerful. For example, on most predictive tasks, a closed vocabulary of only 800 food words approaches the peak performance obtained when using the entire tweets. Perhaps most importantly, our analysis of the learned predictive models provides big-data-driven insights

Fig. 7: Parallel semantics-preserving word clouds showing the difference between two sets of tweets: one containing weekday tweets (left) and one containing weekend tweets (right). Words that appear frequently in both sets of tweets are black, those that appear frequently on weekdays are red, and those that are frequent on weekends are blue.

into connections between the language of food and the investigated population characteristics.
We note that our choice of populations (e.g., cities, states) for these tasks is purely practical (driven by the size of Twitter data at this granularity, and availability of dependent variables for the predictive tasks) and not a limitation of the proposed approach. In the future we would like to use our system to predict characteristics of individuals (e.g., propensity for diabetes), using the individuals’ food information. Given sufﬁcient amounts of available data, this can lead to non-trivial public health applications and, in a commercial and/or political space, to improved targeted marketing.
This paper is accompanied by a supplemental website, https://sites.google.com/site/twitter4food/, which includes a live version of all visualization tools presented.
REFERENCES
[1] D. Bamman, J. Eisenstein, and T. Schnoebelen. Gender identity and lexical variation in social media. Journal of Sociolinguistics, 18(2):135– 160, 2014.
[2] L. Barth, S. G. Kobourov, and S. Pupyrev. Experimental comparison of semantic word clouds. In SEA, pages 247–258, 2014.
[3] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent dirichlet allocation. The Journal of Machine Learning Research, 3:993–1022, 2003.
[4] J. D. Burger, J. Henderson, G. Kim, and G. Zarrella. Discriminating gender on twitter. In EMNLP, pages 1301–1309. Association for Computational Linguistics, 2011.
[5] J. Eisenstein, B. O’Connor, N. A. Smith, and E. P. Xing. A latent variable model for geographic lexical variation. In EMNLP, pages 1277–1287. Association for Computational Linguistics, 2010.
[6] J. Eisenstein, B. O’Connor, N. A. Smith, and E. P. Xing. Mapping the geographical diffusion of new words. arXiv preprint arXiv:1210.5268, 2012.
[7] S. A. Golder and M. W. Macy. Diurnal and seasonal mood vary with work, sleep, and daylength across diverse cultures. Science, 333(6051):1878–1881, 2011.
[8] M. Hingle, D. Yoon, J. F. S. G. Kobourov, M. Schneider, D. Falk, and R. Burd. Collection and visualization of dietary behavior and reasons for eating using a popular and free social media software application. Journal of Medical Internet Research (JMIR), 15(6):125–145, 2013.
[9] D. Jurafsky, V. Chahuneau, B. Routledge, and N. Smith. Narrative framing of consumer sentiment in online restaurant reviews. First Monday, 19(4), 2014.
[10] C. D. Manning, P. Raghavan, and H. Schu¨tze. Introduction to Information Retrieval. Cambridge University Press, New York, NY, USA, 2008.
[11] R. McCreadie, C. Macdonald, I. Ounis, M. Osborne, and S. Petrovic. Scalable distributed event detection for twitter. In Int. Conf. on Big Data, 2013, pages 543–549. IEEE, 2013.

[12] M. Mysl´ın, S.-H. Zhu, W. Chapman, and M. Conway. Using twitter to examine smoking behavior and perceptions of emerging tobacco products. J Med Internet Res, 15(8):e174, Aug 2013.
[13] D. T. Nascimento, F. M. DosSantos, T. Danciu, M. DeBoer, H. van Holsbeeck, R. S. Lucas, C. Aiello, L. Khatib, A. M. Bender, , J.-K. Zubieta, and F. A. DaSilva. Real-time sharing and expression of migraine headache suffering on Twitter: A cross-sectional infodemiology study. J Med Internet Res, 16(4):e96, Apr 2014.
[14] V. D. Nguyen, B. Varghese, and A. Barker. The royal birth of 2013: Analysing and visualising public sentiment in the uk using twitter. In Int. Conf. on Big Data, 2013, pages 46–54. IEEE, 2013.
[15] B. OConnor, J. Eisenstein, E. P. Xing, and N. A. Smith. A mixture model of demographic lexical variation. In Proc. of NIPS workshop on machine learning in computational social science, pages 1–7, 2010.
[16] M. J. Paul and M. Dredze. You are what you tweet: Analyzing Twitter for public health. In ICWSM, 2011.
[17] R. Ranganath, D. Jurafsky, and D. McFarland. It’s not you, it’s me: Detecting ﬂirting and its misperception in speed-dates. In EMNLP, pages 334–342, 2009.
[18] D. Rao, D. Yarowsky, A. Shreevats, and M. Gupta. Classifying latent user attributes in Twitter. In 2nd Intl. Workshop on Search and mining user-generated contents, pages 37–44. ACM, 2010.
[19] R. S. Ritter, J. L. Preston, and I. Hernandez. Happy tweets: Christians are happier, more socially connected, and less analytical than atheists on Twitter. Social Psychological and Personality Science, pages 243–249, 2013.
[20] H. Schwartz, J. Eichstaedt, M. Kern, L. Dziurzynski, M. Agrawal, G. Park, S. Lakshmikanth, S. Jha, M. Seligman, L. Ungar, et al. Characterizing geographic variation in well-being using tweets. In 7th Intl. AAAI ICWSM, 2013.
[21] R. Socher, A. Perelygin, J. Wu, J. Chuang, C. D. Manning, A. Ng, and C. Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In EMNLP. Association for Computational Linguistics, 2013.
[22] V. N. Vapnik. Statistical Learning Theory. John Wiley and Sons, Inc., New York, NY, USA, 1998.
[23] E. Yom-Tov, D. Borsa, J. I. Cox, and A. R. McKendry. Detecting disease outbreaks in mass gatherings using internet data. J Med Internet Res, 16(6):e154, Jun 2014.

