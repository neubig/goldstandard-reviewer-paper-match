Interpretable Neural Predictions with Differentiable Binary Variables

Jasmijn Bastings ILLC
University of Amsterdam bastings@uva.nl

Wilker Aziz ILLC
University of Amsterdam w.aziz@uva.nl

Ivan Titov ILLC, University of Amsterdam ILCC, University of Edinburgh
ititov@inf.ed.ac.uk

arXiv:1905.08160v2 [cs.CL] 19 Jun 2020

Abstract
The success of neural networks comes hand in hand with a desire for more interpretability. We focus on text classiﬁers and make them more interpretable by having them provide a justiﬁcation—a rationale—for their predictions. We approach this problem by jointly training two neural network models: a latent model that selects a rationale (i.e. a short and informative part of the input text), and a classiﬁer that learns from the words in the rationale alone. Previous work proposed to assign binary latent masks to input positions and to promote short selections via sparsityinducing penalties such as L0 regularisation. We propose a latent model that mixes discrete and continuous behaviour allowing at the same time for binary selections and gradient-based training without REINFORCE. In our formulation, we can tractably compute the expected value of penalties such as L0, which allows us to directly optimise the model towards a prespeciﬁed text selection rate. We show that our approach is competitive with previous work on rationale extraction, and explore further uses in attention mechanisms.
1 Introduction
Neural networks are bringing incredible performance gains on text classiﬁcation tasks (Howard and Ruder, 2018; Peters et al., 2018; Devlin et al., 2019). However, this power comes hand in hand with a desire for more interpretability, even though its deﬁnition may differ (Lipton, 2016). While it is useful to obtain high classiﬁcation accuracy, with more data available than ever before it also becomes increasingly important to justify predictions. Imagine having to classify a large collection of documents, while verifying that the classiﬁcations make sense. It would be extremely time-consuming to read each document to evaluate the results. Moreover, if we do not

look:
Classiﬁer
pours a dark amber color with decent head that does not recede much . it ’s a tad too dark to see the
carbonation , but fairs well . smells of roasted malts and mouthfeel is quite strong in the sense that you can get a good taste of it before you even swallow .
Rationale Extractor
pours a dark amber color with decent head that does not recede much . it ’s a tad too dark to see the
carbonation , but fairs well . smells of roasted malts and mouthfeel is quite strong in the sense that you can get a good taste of it before you even swallow .
Figure 1: Rationale extraction for a beer review.
know why a prediction was made, we do not know if we can trust it.
What if the model could provide us the most important parts of the document, as a justiﬁcation for its prediction? That is exactly the focus of this paper. We use a setting that was pioneered by Lei et al. (2016). A rationale is deﬁned to be a short yet sufﬁcient part of the input text; short so that it makes clear what is most important, and sufﬁcient so that a correct prediction can be made from the rationale alone. One neural network learns to extract the rationale, while another neural network, with separate parameters, learns to make a prediction from just the rationale. Lei et al. model this by assigning a binary Bernoulli variable to each input word. The rationale then consists of all the words for which a 1 was sampled. Because gradients do not ﬂow through discrete samples, the rationale extractor is optimized using REINFORCE (Williams, 1992). An L0 regularizer is used to make sure the rationale is short.
We propose an alternative to purely discrete selectors for which gradient estimation is possible without REINFORCE, instead relying on a repa-

rameterization of a random variable that exhibits both continuous and discrete behavior (Louizos et al., 2017). To promote compact rationales, we employ a relaxed form of L0 regularization (Louizos et al., 2017), penalizing the objective as a function of the expected proportion of selected text. We also propose the use of Lagrangian relaxation to target a speciﬁc rate of selected input text.
Our contributions are summarized as follows:1

1. we present a differentiable approach to extractive rationales (§2) including an objective that allows for specifying how much text is to be extracted (§4);

2. we introduce HardKuma (§3), which gives support to binary outcomes and allows for reparameterized gradient estimates;

3. we empirically show that our approach is competitive with previous work and that HardKuma has further applications, e.g. in attention mechanisms. (§6).

2 Latent Rationale
We are interested in making NN-based text classiﬁers interpretable by (i) uncovering which parts of the input text contribute features for classiﬁcation, and (ii) basing decisions on only a fraction of the input text (a rationale). Lei et al. (2016) approached (i) by inducing binary latent selectors that control which input positions are available to an NN encoder that learns features for classiﬁcation/regression, and (ii) by regularising their architectures using sparsity-inducing penalties on latent assignments. In this section we put their approach under a probabilistic light, and this will then more naturally lead to our proposed method.
In text classiﬁcation, an input x is mapped to a distribution over target labels:

Y |x ∼ Cat(f (x; θ)) ,

(1)

where we have a neural network architecture f (·; θ) parameterize the model—θ collectively denotes the parameters of the NN layers in f . That is, an NN maps from data space (e.g. sentences, short paragraphs, or premise-hypothesis pairs) to the categorical parameter space (i.e. a vector of class probabilities). For the sake of concreteness,
1Code available at https://github.com/ bastings/interpretable_predictions.

consider the input a sequence x = x1, . . . , xn . A target y is typically a categorical outcome, such as a sentiment class or an entailment decision, but with an appropriate choice of likelihood it could also be a numerical score (continuous or integer).
Lei et al. (2016) augment this model with a collection of latent variables which we denote by z = z1, . . . , zn . These variables are responsible for regulating which portions of the input x contribute with predictors (i.e. features) to the classiﬁer. The model formulation changes as follows:

Zi|x ∼ Bern(gi(x; φ)) (2)
Y |x, z ∼ Cat(f (x z; θ))

where an NN g(·; φ) predicts a sequence of n Bernoulli parameters—one per latent variable— and the classiﬁer is modiﬁed such that zi indicates whether or not xi is available for encoding. We can think of the sequence z as a binary gating mechanism used to select a rationale, which with some abuse of notation we denote by x z. Figure 1 illustrates the approach.
Parameter estimation for this model can be done by maximizing a lower bound E(φ, θ) on the loglikelihood of the data derived by application of Jensen’s inequality:2

log P (y|x) = log EP (z|x,φ) [P (y|x, z, θ)]

JI

(3)

≥ EP (z|x,φ) [log P (y|x, z, θ)] = E(φ, θ) .

These latent rationales approach the ﬁrst objective, namely, uncovering which parts of the input text contribute towards a decision. However note that an NN controls the Bernoulli parameters, thus nothing prevents this NN from selecting the whole of the input, thus defaulting to a standard text classiﬁer. To promote compact rationales, Lei et al. (2016) impose sparsity-inducing penalties on latent selectors. They penalise for the total number of selected words, L0 in (4), as well as, for the total number of transitions, fused lasso in (4), and approach the following optimization problem

n

n−1

min −E(φ, θ) + λ0 zi +λ1 |zi − zi+1| (4)

φ,θ

i=1

i=1

L0(z)

fused lasso

via gradient-based optimisation, where λ0 and λ1 are ﬁxed hyperparameters. The objective is however intractable to compute, the lowerbound, in

2This can be seen as variational inference (Jordan et al., 1999) where we perform approximate inference using a datadependent prior P (z|x, φ).

particular, requires marginalization of O(2n) binary sequences. For that reason, Lei et al. sample latent assignments and work with gradient estimates using REINFORCE (Williams, 1992).
The key ingredients are, therefore, binary latent variables and sparsity-inducing regularization, and therefore the solution is marked by nondifferentiability. We propose to replace Bernoulli variables by rectiﬁed continuous random variables (Socci et al., 1998), for they exhibit both discrete and continuous behaviour. Moreover, they are amenable to reparameterization in terms of a ﬁxed random source (Kingma and Welling, 2014), in which case gradient estimation is possible without REINFORCE. Following Louizos et al. (2017), we exploit one such distribution to relax L0 regularization and thus promote compact rationales with a differentiable objective. In section 3, we introduce this distribution and present its properties. In section 4, we employ a Lagrangian relaxation to automatically target a pre-speciﬁed selection rate. And ﬁnally, in section 5 we present an example for sentiment classiﬁcation.
3 Hard Kumaraswamy Distribution
Key to our model is a novel distribution that exhibits both continuous and discrete behaviour, in this section we introduce it. With non-negligible probability, samples from this distribution evaluate to exactly 0 or exactly 1. In a nutshell: i) we start from a distribution over the open interval (0, 1) (see dashed curve in Figure 2); ii) we then stretch its support from l < 0 to r > 1 in order to include {0} and {1} (see solid curve in Figure 2); ﬁnally, iii) we collapse the probability mass over the interval (l, 0] to {0}, and similarly, the probability mass over the interval [1, r) to {1} (shaded areas in Figure 2). This stretch-and-rectify technique was proposed by Louizos et al. (2017), who rectiﬁed samples from the BinaryConcrete (or GumbelSoftmax) distribution (Maddison et al., 2017; Jang et al., 2017). We adapted their technique to the Kumaraswamy distribution motivated by its close resemblance to a Beta distribution, for which we have stronger intuitions (for example, its two shape parameters transit rather naturally from unimodal to bimodal conﬁgurations of the distribution). In the following, we introduce this new distribution formally.3
3We use uppercase letters for random variables (e.g. K, T , and H) and lowercase for assignments (e.g. k, t, h). For a

2.5 Kuma(0.5, 0.5, ­0.1, 1.1) Kuma(0.5, 0.5)
2.0

1.5

1.0

0.5

0.0 0.0

0.2

0.4

0.6

0.8

1.0

Figure 2: The HardKuma distribution: we start from a Kuma(0.5, 0.5), and stretch its support to the interval (−0.1, 1.1), ﬁnally we collapse all mass before 0 to {0} and all mass after 1 to {1}.

3.1 Kumaraswamy distribution
The Kumaraswamy distribution (Kumaraswamy, 1980) is a two-parameters distribution over the open interval (0, 1), we denote a Kumaraswamydistributed variable by K ∼ Kuma(a, b), where a ∈ R>0 and b ∈ R>0 control the distribution’s shape. The dashed curve in Figure 2 illustrates the density of Kuma(0.5, 0.5). For more details including its pdf and cdf, consult Appendix A.
The Kumaraswamy is a close relative of the Beta distribution, though not itself an exponential family, with a simple cdf whose inverse
1/a
FK−1(u; a, b) = 1 − (1 − u)1/b , (5)

for u ∈ [0, 1], can be used to obtain samples

FK−1(U ; α, β) ∼ Kuma(α, β)

(6)

by transformation of a uniform random source U ∼ U(0, 1). We can use this fact to reparameterize expectations (Nalisnick and Smyth, 2016).

3.2 Rectiﬁed Kumaraswamy
We stretch the support of the Kumaraswamy distribution to include 0 and 1. The resulting variable T ∼ Kuma(a, b, l, r) takes on values in the open interval (l, r) where l < 0 and r > 1, with cdf

FT (t; a, b, l, r) = FK ((t − l)/(r − l); a, b) . (7)

We now deﬁne a rectiﬁed random variable, denoted by H ∼ HardKuma(a, b, l, r), by passing
random variable K, fK (k; α) is the probability density function (pdf), conditioned on parameters α, and FK (k; α) is the cumulative distribution function (cdf).

a sample T ∼ Kuma(a, b, l, r) through a hardsigmoid, i.e. h = min(1, max(0, t)). The resulting variable is deﬁned over the closed interval [0, 1]. Note that while there is 0 probability of sampling t = 0, sampling h = 0 corresponds to sampling any t ∈ (l, 0], a set whose mass under Kuma(t|a, b, l, r) is available in closed form:

P(H = 0) = FK r−−ll ; a, b .

(8)

That is because all negative values of t are deterministically mapped to zero. Similarly, samples t ∈ [1, r) are all deterministically mapped to h = 1, whose total mass amounts to

P(H = 1) = 1 − FK 1r−−ll ; a, b . (9)

See Figure 2 for an illustration, and Appendix A for the complete derivations.

3.3 Reparameterization and gradients
Because this rectiﬁed variable is built upon a Kumaraswamy, it admits a reparameterisation in terms of a uniform variable U ∼ U(0, 1). We need to ﬁrst sample a uniform variable in the open interval (0, 1) and transform the result to a Kumaraswamy variable via the inverse cdf (10a), then shift and scale the result to cover the stretched support (10b), and ﬁnally, apply the rectiﬁer in order to get a sample in the closed interval [0, 1] (10c).

k = FK−1(u; a, b) t = l + (r − l)k
h = min(1, max(0, t)) ,

(10a) (10b) (10c)

We denote this h = s(u; a, b, l, r) for short. Note that this transformation has two discontinuity points, namely, t = 0 and t = 1. Though recall, the probability of sampling t exactly 0 or exactly 1 is zero, which essentially means stochasticity circumvents points of non-differentiability of the rectiﬁer (see Appendix A.3).

4 Controlled Sparsity
Following Louizos et al. (2017), we relax nondifferentiable penalties by computing them on expectation under our latent model p(z|x, φ). In addition, we propose the use of Lagrangian relaxation to target speciﬁc values for the penalties.

Thanks to the tractable Kumaraswamy cdf, the expected value of L0(z) is known in closed form
n
Ep(z|x) [L0(z)] i=nd Ep(zi|x) [I[zi = 0]] n i=1 (11)
= 1 − P(Zi = 0) ,
i=1

where P(Zi = 0) = FK

−l r−l

;

ai

,

bi

. This quan-

tity is a tractable and differentiable function of the

parameters φ of the latent model. We can also

compute a relaxation of fused lasso by comput-

ing the expected number of zero-to-nonzero and

nonzero-to-zero changes:

n−1

Ep(z|x)

I[zi = 0, zi+1 = 0]

i=1

n−1

+ Ep(z|x)

I[zi = 0, zi+1 = 0]

i=1

(12)

n−1
i=nd P(Zi = 0)(1 − P(Zi+1 = 0))

i=1

+ (1 − P(Zi = 0))P(Zi+1 = 0) .

In both cases, we make the assumption that latent variables are independent given x, in Appendix B.1.2 we discuss how to estimate the regularizers for a model p(zi|x, z<i) that conditions on the preﬁx z<i of sampled HardKuma assignments.
We can use regularizers to promote sparsity, but just how much text will our ﬁnal model select? Ideally, we would target speciﬁc values r and solve a constrained optimization problem. In practice, constrained optimisation is very challenging, thus we employ Lagrangian relaxation instead:

max min −E(φ, θ) + λ (R(φ) − r) (13)
λ∈R φ,θ

where R(φ) is a vector of regularisers, e.g. expected L0 and expected fused lasso, and λ is a vector of Lagrangian multipliers λ. Note how this differs from the treatment of Lei et al. (2016) shown in (4) where regularizers are computed for assignments, rather than on expectation, and where λ0, λ1 are ﬁxed hyperparameters.
5 Sentiment Classiﬁcation
As a concrete example, consider the case of sentiment classiﬁcation where x is a sentence and y is a

5-way sentiment class (from very negative to very positive). The model consists of

Zi ∼ HardKuma(ai, bi, l, r) (14)
Y |x, z ∼ Cat(f (x z; θ))

where the shape parameters a, b = g(x; φ), i.e. two sequences of n strictly positive scalars, are predicted by a NN, and the support boundaries (l, r) are ﬁxed hyperparameters.
We ﬁrst specify an architecture that parameterizes latent selectors and then use a reparameterized sample to restrict which parts of the input contribute encodings for classiﬁcation:4

ei = emb(xi) hn1 = birnn(en1 ; φr) ui ∼ U (0, 1)

ai = fa(hi; φa) bi = fb(hi; φb) zi = s(ui; ai, bi, l, r)

where emb(·) is an embedding layer, birnn(·; φr) is a bidirectional encoder, fa(·; φa) and fb(·; φb) are feed-forward transformations with softplus outputs, and s(·) turns the uniform sample ui into the latent selector zi (see §3). We then use the sampled z to modulate inputs to the classiﬁer:

ei = emb(xi) h(ifwd) = rnn(h(i−fw1d), zi ei; θfwd) h(ibwd) = rnn(h(i+bw1d), zi ei; θbwd)
o = fo(h(nfwd), h(1bwd); θo)
where rnn(·; θfwd) and rnn(·; θbwd) are recurrent cells such as LSTMs (Hochreiter and Schmidhuber, 1997) that process the sequence in different directions, and fo(·; θo) is a feed-forward transformation with softmax output. Note how zi modulates features ei of the input xi that are available to the recurrent composition function.
We then obtain gradient estimates of E(φ, θ) via Monte Carlo (MC) sampling from

E(φ, θ) = EU(0,I) [log P (y|x, sφ(u, x), θ)] (15)

where z = sφ(u, x) is a shorthand for elementwise application of the transformation from uniform samples to HardKuma samples. This reparameterisation is the key to gradient estimation through stochastic computation graphs (Kingma and Welling, 2014; Rezende et al., 2014).
4We describe architectures using blocks denoted by layer(inputs; subset of parameters), boldface letters for vectors, and the shorthand v1n for a sequence v1, . . . , vn .

SVM (Lei et al., 2016) BiLSTM (Lei et al., 2016) BiRCNN (Lei et al., 2016)
BiLSTM (ours) BiRCNN (ours)

0.0154 0.0094 0.0087
0.0089 0.0088

Table 1: MSE on the BeerAdvocate test set.

Deterministic predictions. At test time we make predictions based on what is the most likely assignment for each zi. We arg max across conﬁgurations of the distribution, namely, zi = 0, zi = 1, or 0 < zi < 1. When the continuous interval is more likely, we take the expected value of the underlying Kumaraswamy variable.
6 Experiments
We perform experiments on multi-aspect sentiment analysis to compare with previous work, as well as experiments on sentiment classiﬁcation and natural language inference. All models were implemented in PyTorch, and Appendix B provides implementation details.
Goal. When rationalizing predictions, our goal is to perform as well as systems using the full input text, while using only a subset of the input text, leaving unnecessary words out for interpretability.
6.1 Multi-aspect Sentiment Analysis
In our ﬁrst experiment we compare directly with previous work on rationalizing predictions (Lei et al., 2016). We replicate their setting.
Data. A pre-processed subset of the BeerAdvocate5 data set is used (McAuley et al., 2012). It consists of 220,000 beer reviews, where multiple aspects (e.g. look, smell, palate) are rated. As shown in Figure 1, a review typically consists of multiple sentences, and contains a 0-5 star rating (e.g. 3.5 stars) for each aspect. Lei et al. mapped the ratings to scalars in [0, 1].
Model. We use the models described in §5 with two small modiﬁcations: 1) since this is a regression task, we use a sigmoid activation in the output layer of the classiﬁer rather than a softmax,6 and
5https://www.beeradvocate.com/ 6From a likelihood learning point of view, we would have assumed a Logit-Normal likelihood, however, to stay closer to Lei et al. (2016), we employ mean squared error.

Method
Attention (Lei et al.) Bernoulli (Lei et al.) Bernoulli (reimpl.) HardKuma

Look

% Precision % Selected

80.6

13

96.3

14

94.8

13

98.1

13

Smell

% Precision % Selected

88.4

7

95.1

7

95.1

7

96.8

7

Palate

% Precision % Selected

65.3

7

80.2

7

80.5

7

89.8

7

Table 2: Precision (% of selected words that was also annotated as the gold rationale) and selected (% of words not zeroed out) per aspect. In the attention baseline, the top 13% (7%) of words with highest attention weights are used for classiﬁcation. Models were selected based on validation loss.

2) we use an extra RNN to condition zi on z<i:

ai = fa(hi, si−1; φa) bi = fb(hi, si−1; φb) si = rnn(hi, zi, si−1; φs)

(16a) (16b) (16c)

For a fair comparison we follow Lei et al. by using RCNN7 cells rather than LSTM cells for encoding sentences on this task. Since this cell is not widely used, we veriﬁed its performance in Table 1. We observe that the BiRCNN performs on par with the BiLSTM (while using 50% fewer parameters), and similarly to previous results.
Evaluation. A test set with sentence-level rationale annotations is available. The precision of a rationale is deﬁned as the percentage of words with z = 0 that is part of the annotation. We also evaluate the predictions made from the rationale using mean squared error (MSE).
Baselines. For our baseline we reimplemented the approach of Lei et al. (2016) which we call Bernoulli after the distribution they use to sample z from. We also report their attention baseline, in which an attention score is computed for each word, after which it is simply thresholded to select the top-k percent as the rationale.
Results. Table 2 shows the precision and the percentage of selected words for the ﬁrst three aspects. The models here have been selected based on validation MSE and were tuned to select a similar percentage of words (‘selected’). We observe that our Bernoulli reimplementation reaches the precision similar to previous work, doing a little bit worse for the ‘look’ aspect. Our HardKuma managed to get even higher precision, and it extracted exactly the percentage of text that we spec-
7An RCNN cell can replace any LSTM cell and works well on text classiﬁcation problems. See appendix B.

0.013

0.012

0.011

MSE

0.010

0.009

0.0080%

20%

40%

60%

80% 100%

Selected Text

Figure 3: MSE of all aspects for various percentages of extracted text. HardKuma (blue crosses) has lower error than Bernoulli (red circles; open circles taken from Lei et al. (2016)) for similar amount of extracted text. The full-text baseline (black star) gets the best MSE.

iﬁed (see §4).8 Figure 3 shows the MSE for all aspects for various percentages of extracted text. We observe that HardKuma does better with a smaller percentage of text selected. The performance becomes more similar as more text is selected.
6.2 Sentiment Classiﬁcation
We also experiment on the Stanford Sentiment Treebank (SST) (Socher et al., 2013). There are 5 sentiment classes: very negative, negative, neutral, positive, and very positive. Here we use the HardKuma model described in §5, a Bernoulli model trained with REINFORCE, as well as a BiLSTM.
Results. Figure 4 shows the classiﬁcation accuracy for various percentages of selected text. We observe that HardKuma outperforms the Bernoulli model at each percentage of selected text. HardKuma reaches full-text baseline performance already around 40% extracted text. At that point, it obtains a test score of 45.84, versus 42.22 for Bernoulli and 47.4±0.8 for the full-text baseline.
8We tried to use Lagrangian relaxation for the Bernoulli model, but this led to instabilities (e.g. all words selected).

50%

45%

Accuracy

40%

35%

30%

0%

20%

40S%elected T6e0x%t

80%

100%

Figure 4: SST validation accuracy for various percentages of extracted text. HardKuma (blue crosses) has higher accuracy than Bernoulli (red circles) for similar amount of text, and reaches the full-text baseline (black star, 46.3 ± 2σ with σ = 0.7) around 40% text.

18231

Total HardKuma Bernoulli

146 119 112 992 603 489
3378 3806 1511 803 795 394 264 299

very negative negative neutral positive very positive
Figure 5: The number of words in each sentiment class for the full validation set, the HardKuma (24% selected text) and Bernoulli (25% text).
Analysis. We wonder what kind of words are dropped when we select smaller amounts of text. For this analysis we exploit the word-level sentiment annotations in SST, which allows us to track the sentiment of words in the rationale. Figure 5 shows that a large portion of dropped words have neutral sentiment, and it seems plausible that exactly those words are not important features for classiﬁcation. We also see that HardKuma drops (relatively) more neutral words than Bernoulli.
6.3 Natural Language Inference
In Natural language inference (NLI), given a premise sentence x(p) and a hypothesis sentence x(h), the goal is to predict their relation y which can be contradiction, entailment, or neutral. As our dataset we use the Stanford Natural Language Inference (SNLI) corpus (Bowman et al., 2015).
Baseline. We use the Decomposable Attention model (DA) of Parikh et al. (2016).9 DA does not make use of LSTMs, but rather uses attention to ﬁnd connections between the premise and the hy-
9Better results e.g. Chen et al. (2017) and data sets for NLI exist, but are not the focus of this paper.

pothesis that are predictive of the relation. Each word in the premise attends to each word in the hypothesis, and vice versa, resulting in a set of comparison vectors which are then aggregated for a ﬁnal prediction. If there is no link between a word pair, it is not considered for prediction.
Model. Because the premise and hypothesis interact, it does not make sense to extract a rationale for the premise and hypothesis independently. Instead, we replace the attention between premise and hypothesis with HardKuma attention. Whereas in the baseline a similarity matrix is softmax-normalized across rows (premise to hypothesis) and columns (hypothesis to premise) to produce attention matrices, in our model each cell in the attention matrix is sampled from a HardKuma parameterized by (a, b). To promote sparsity, we use the relaxed L0 to specify the desired percentage of non-zero attention cells. The resulting matrix does not need further normalization.
Results. With a target rate of 10%, the HardKuma model achieved 8.5% non-zero attention. Table 3 shows that, even with so many zeros in the attention matrices, it only does about 1% worse compared to the DA baseline. Figure 6 shows an example of HardKuma attention, with additional examples in Appendix B. We leave further explorations with HardKuma attention for future work.

Model

Dev Test

LSTM (Bowman et al., 2016) – 80.6

DA (Parikh et al., 2016)

– 86.3

DA (reimplementation)

86.9 86.5

DA with HardKuma attention 86.0 85.5

Table 3: SNLI results (accuracy).

<s> The man is walking his cat .

<s> 0 0 0 0 0 0 0 0 Young 0 0 0 0 0 0 0 0
man 0 0 77 21 0 0 0 0 walking 0 0 0 0 88 0 0 0
dog 0 0 0 0 0 0 86 0
Figure 6: Example of HardKuma attention between a premise (rows) and hypothesis (columns) in SNLI (cell values shown in multiples of 10−2).

7 Related Work
This work has connections with work on interpretability, learning from rationales, sparse structures, and rectiﬁed distributions. We discuss each of those areas.
Interpretability. Machine learning research has been focusing more and more on interpretability (Gilpin et al., 2018). However, there are many nuances to interpretability (Lipton, 2016), and amongst them we focus on model transparency.
One strategy is to extract a simpler, interpretable model from a neural network, though this comes at the cost of performance. For example, Thrun (1995) extract if-then rules, while Craven and Shavlik (1996) extract decision trees.
There is also work on making word vectors more interpretable. Faruqui et al. (2015) make word vectors more sparse, and Herbelot and Vecchi (2015) learn to map distributional word vectors to model-theoretic semantic vectors.
Similarly to Lei et al. (2016), Titov and McDonald (2008) extract informative fragments of text by jointly training a classiﬁer and a model predicting a stochastic mask, while relying on Gibbs sampling to do so. Their focus is on using the sentiment labels as a weak supervision signal for opinion summarization rather than on rationalizing classiﬁer predictions.
There are also related approaches that aim to interpret an already-trained model, in contrast to Lei et al. (2016) and our approach where the rationale is jointly modeled. Ribeiro et al. (2016) make any classiﬁer interpretable by approximating it locally with a linear proxy model in an approach called LIME, and Alvarez-Melis and Jaakkola (2017) propose a framework that returns input-output pairs that are causally related.
Learning from rationales. Our work is different from approaches that aim to improve classiﬁcation using rationales as an additional input (Zaidan et al., 2007; Zaidan and Eisner, 2008; Zhang et al., 2016). Instead, our rationales are latent and we are interested in uncovering them. We only use annotated rationales for evaluation.
Sparse layers. Also arguing for enhanced interpretability, Niculae and Blondel (2017) propose a framework for learning sparsely activated attention layers based on smoothing the max operator. They derive a number of relaxations to max,

including softmax itself, but in particular, they target relaxations such as sparsemax (Martins and Astudillo, 2016) which, unlike softmax, are sparse (i.e. produce vectors of probability values with components that evaluate to exactly 0). Their activation functions are themselves solutions to convex optimization problems, to which they provide efﬁcient forward and backward passes. The technique can be seen as a deterministic sparsely activated layer which they use as a drop-in replacement to standard attention mechanisms. In contrast, in this paper we focus on binary outcomes rather than K-valued ones. Niculae et al. (2018) extend the framework to structured discrete spaces where they learn sparse parameterizations of discrete latent models. In this context, parameter estimation requires exact marginalization of discrete variables or gradient estimation via REINFORCE. They show that oftentimes distributions are sparse enough to enable exact marginal inference.
Peng et al. (2018) propose SPIGOT, a proxy gradient to the non-differentiable arg max operator. This proxy requires an arg max solver (e.g. Viterbi for structured prediction) and, like the straight-through estimator (Bengio et al., 2013), is a biased estimator. Though, unlike ST it is efﬁcient for structured variables. In contrast, in this work we chose to focus on unbiased estimators.
Rectiﬁed Distributions. The idea of rectiﬁed distributions has been around for some time. The rectiﬁed Gaussian distribution (Socci et al., 1998), in particular, has found applications to factor analysis (Harva and Kaban, 2005) and approximate inference in graphical models (Winn and Bishop, 2005). Louizos et al. (2017) propose to stretch and rectify samples from the BinaryConcrete (or GumbelSoftmax) distribution (Maddison et al., 2017; Jang et al., 2017). They use rectiﬁed variables to induce sparsity in parameter space via a relaxation to L0. We adapt their technique to promote sparse activations instead. Rolfe (2017) learns a relaxation of a discrete random variable based on a tractable mixture of a point mass at zero and a continuous reparameterizable density, thus enabling reparameterized sampling from the half-closed interval [0, ∞). In contrast, with HardKuma we focused on giving support to both 0s and 1s.
8 Conclusions
We presented a differentiable approach to extractive rationales, including an objective that allows

for specifying how much text is to be extracted. To allow for reparameterized gradient estimates and support for binary outcomes we introduced the HardKuma distribution. Apart from extracting rationales, we showed that HardKuma has further potential uses, which we demonstrated on premise-hypothesis attention in SNLI. We leave further explorations for future work.
Acknowledgments
We thank Luca Falorsi for pointing us to Louizos et al. (2017), which inspired the HardKumaraswamy distribution. This work has received funding from the European Research Council (ERC StG BroadSem 678254), the European Union’s Horizon 2020 research and innovation programme (grant agreement No 825299, GoURMET), and the Dutch National Science Foundation (NWO VIDI 639.022.518, NWO VICI 277-89-002).
References
David Alvarez-Melis and Tommi Jaakkola. 2017. A causal framework for explaining the predictions of black-box sequence-to-sequence models. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 412– 421. Association for Computational Linguistics.
Yoshua Bengio, Nicholas Le´onard, and Aaron Courville. 2013. Estimating or propagating gradients through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432.
Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. 2015. A large annotated corpus for learning natural language inference. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 632–642. Association for Computational Linguistics.
Samuel R. Bowman, Jon Gauthier, Abhinav Rastogi, Raghav Gupta, Christopher D. Manning, and Christopher Potts. 2016. A fast uniﬁed model for parsing and sentence understanding. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1466–1477. Association for Computational Linguistics.
Qian Chen, Xiaodan Zhu, Zhen-Hua Ling, Si Wei, Hui Jiang, and Diana Inkpen. 2017. Enhanced lstm for natural language inference. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1657–1668. Association for Computational Linguistics.

Mark Craven and Jude W Shavlik. 1996. Extracting tree-structured representations of trained networks. In Advances in neural information processing systems, pages 24–30.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers). Association for Computational Linguistics.
Manaal Faruqui, Yulia Tsvetkov, Dani Yogatama, Chris Dyer, and Noah A. Smith. 2015. Sparse overcomplete word vector representations. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 1491–1500. Association for Computational Linguistics.
Leilani H Gilpin, David Bau, Ben Z Yuan, Ayesha Bajwa, Michael Specter, and Lalana Kagal. 2018. Explaining explanations: An overview of interpretability of machine learning. In 2018 IEEE 5th International Conference on Data Science and Advanced Analytics (DSAA), pages 80–89. IEEE.
Markus Harva and Ata Kaban. 2005. A variational bayesian method for rectiﬁed factor analysis. In Proceedings. 2005 IEEE International Joint Conference on Neural Networks, 2005., volume 1, pages 185–190. IEEE.
Aure´lie Herbelot and Eva Maria Vecchi. 2015. Building a shared world: mapping distributional to modeltheoretic semantic spaces. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 22–32. Association for Computational Linguistics.
Sepp Hochreiter and Ju¨rgen Schmidhuber. 1997. Long Short-Term Memory. Neural Computation, 9(8):1735–1780.
Jeremy Howard and Sebastian Ruder. 2018. Universal language model ﬁne-tuning for text classiﬁcation. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 328–339. Association for Computational Linguistics.
Eric Jang, Shixiang Gu, and Ben Poole. 2017. Categorical reparameterization with gumbel-softmax. International Conference on Learning Representations.
MichaelI. Jordan, Zoubin Ghahramani, TommiS. Jaakkola, and LawrenceK. Saul. 1999. An introduction to variational methods for graphical models. Machine Learning, 37(2):183–233.
Diederik P. Kingma and Max Welling. 2014. Autoencoding variational bayes. In International Conference on Learning Representations.

Ponnambalam Kumaraswamy. 1980. A generalized probability density function for double-bounded random processes. Journal of Hydrology, 46(12):79–88.
Tao Lei, Regina Barzilay, and Tommi Jaakkola. 2016. Rationalizing neural predictions. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 107–117. Association for Computational Linguistics.
Zachary Chase Lipton. 2016. The mythos of model interpretability. ICML Workshop on Human Interpretability in Machine Learning (WHI 2016).
Christos Louizos, Max Welling, and Diederik P Kingma. 2017. Learning sparse neural networks through l 0 regularization. arXiv preprint arXiv:1712.01312.
Chris J. Maddison, Andriy Mnih, and Yee Whye Teh. 2017. The concrete distribution: A continous relaxation of discrete random variables. International Conference on Learning Representations.
Andre Martins and Ramon Astudillo. 2016. From softmax to sparsemax: A sparse model of attention and multi-label classiﬁcation. In International Conference on Machine Learning, pages 1614–1623.
Julian McAuley, Jure Leskovec, and Dan Jurafsky. 2012. Learning attitudes and attributes from multiaspect reviews. In Data Mining (ICDM), 2012 IEEE 12th International Conference on, pages 1020– 1025. IEEE.
Eric Nalisnick and Padhraic Smyth. 2016. Stickbreaking variational autoencoders. arXiv preprint arXiv:1605.06197.
Vlad Niculae and Mathieu Blondel. 2017. A regularized framework for sparse and structured neural attention. In Advances in Neural Information Processing Systems, pages 3338–3348.
Vlad Niculae, Andre´ F. T. Martins, and Claire Cardie. 2018. Towards dynamic computation graphs via sparse latent structure. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 905–911. Association for Computational Linguistics.
Ankur Parikh, Oscar Ta¨ckstro¨m, Dipanjan Das, and Jakob Uszkoreit. 2016. A decomposable attention model for natural language inference. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2249–2255. Association for Computational Linguistics.
Hao Peng, Sam Thomson, and Noah A. Smith. 2018. Backpropagating through structured argmax using a spigot. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1863–1873. Association for Computational Linguistics.

Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word representations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 2227– 2237. Association for Computational Linguistics.
Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. 2014. Stochastic backpropagation and approximate inference in deep generative models. In Proceedings of the 31st International Conference on Machine Learning, volume 32 of Proceedings of Machine Learning Research, pages 1278–1286, Bejing, China. PMLR.
Marco Ribeiro, Sameer Singh, and Carlos Guestrin. 2016. “why should i trust you?”: Explaining the predictions of any classiﬁer. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations, pages 97–101. Association for Computational Linguistics.
Jason Tyler Rolfe. 2017. Discrete variational autoencoders. In ICLR.
Nicholas D. Socci, Daniel D. Lee, and H. Sebastian Seung. 1998. The rectiﬁed gaussian distribution. In M. I. Jordan, M. J. Kearns, and S. A. Solla, editors, Advances in Neural Information Processing Systems 10, pages 350–356. MIT Press.
Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1631–1642. Association for Computational Linguistics.
Sebastian Thrun. 1995. Extracting rules from artiﬁcial neural networks with distributed representations. In Advances in neural information processing systems, pages 505–512.
Ivan Titov and Ryan McDonald. 2008. A joint model of text and aspect ratings for sentiment summarization. In Proceedings of ACL.
Ronald J Williams. 1992. Simple statistical gradientfollowing algorithms for connectionist reinforcement learning. Machine learning, 8(3-4):229–256.
John Winn and Christopher M Bishop. 2005. Variational message passing. Journal of Machine Learning Research, 6(Apr):661–694.
Omar Zaidan and Jason Eisner. 2008. Modeling annotators: A generative approach to learning from annotator rationales. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 31–40, Honolulu, Hawaii. Association for Computational Linguistics.

Omar Zaidan, Jason Eisner, and Christine Piatko. 2007. Using “annotator rationales” to improve machine learning for text categorization. In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference, pages 260–267. Association for Computational Linguistics.
Ye Zhang, Iain Marshall, and Byron C. Wallace. 2016. Rationale-augmented convolutional neural networks for text classiﬁcation. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 795–804, Austin, Texas. Association for Computational Linguistics.

A Kumaraswamy distribution

2.5

(0.5, 0.5)

(5, 1)

2.0

(2, 2)

(2, 5)

(0.1, 0.1)

1.5

(1.0, 1.0)

1.0

0.5

0.00.0

0.2

0.4

0.6

0.8

1.0

Figure 7: Kuma plots for various (a, b) parameters.

A Kumaraswamy-distributed variable K ∼ Kuma(a, b) takes on values in the open interval (0, 1) and has density

fK (k; a, b) = abka−1(1 − ka)b−1 , (17)

where a ∈ R>0 and b ∈ R>0 are shape parameters. Its cumulative distribution takes a simple closed-form expression

k
FK(k; a, b) = fK(ξ|a, b)dξ
0
= 1 − (1 − ka)b ,

(18a) (18b)

with inverse
1/a
FK−1(u; a, b) = 1 − (1 − u)1/b . (19)
A.1 Generalised-support Kumaraswamy We can generalise the support of a Kumaraswamy variable by specifying two constants l < r and transforming a random variable K ∼ Kuma(a, b) to obtain T ∼ Kuma(a, b, l, r) as shown in (20, left).
t = l + (r − l)k k = (t − l)/(r − l) (20)

The density of the resulting variable is

fT (t; a, b, l, r) = fK rt−−ll ; a, b = fK rt−−ll ; a, b

dk dt
1 (r − l)

(21a) (21b) (21c)

where r − l > 0 by deﬁnition. This afﬁne transformation leaves the cdf unchanged, i.e.

t0

FT (t0; a, b, l, r) = fT (t; a, b, l, r)dt

−∞

t0 t−l

1

=

fK

r−l ; a, b

dt (r − l)

−∞

(22)

t0 −l

r−l

1

= −∞ fK (k; a, b) (r − l) (r − l)dk

= FK tr0−−ll ; a, b .

Thus we can obtain samples from this generalisedsupport Kumaraswamy by sampling from a uniform distribution U(0, 1), applying the inverse transform (19), then shifting and scaling the sample according to (20, left).

A.2 Rectiﬁed Kumaraswamy
First, we stretch a Kumaraswamy distribution to include 0 and 1 in its support, that is, with l < 0 and r > 1, we deﬁne T ∼ Kuma(a, b, l, r). Then we apply a hard-sigmoid transformation to this variable, that is, h = min(0, max(1, t)), which results in a rectiﬁed distribution which gives support to the closed interval [0, 1]. We denote this rectiﬁed variable by

H ∼ HardKuma(a, b, l, r)

(23)

whose distribution function is fH (h; a, b, l, r) = P(h = 0)δ(h) + P(h = 1)δ(h − 1) (24) + P(0 < h < 1) fT (h; a, b, l, r)1(0,1)(h) P(0 < h < 1)
where P(h = 0) = P(t ≤ 0) (25) = FT (0; a, b, l, r) = FK ( − l/(r − l); a, b)
is the probability of sampling exactly 0, where

P(h = 1) = P(t ≥ 1) = 1 − P(t < 1)

= 1 − FT (1; a, b, l, r)

(26)

= 1 − FK ((1 − l)/(r − l); a, b)

is the probability of sampling exactly 1, and

P(0 < h < 1) = 1 − P(h = 0) − P(h = 1) (27)

is the probability of drawing a continuous value in (0, 1). Note that we used the result in (22) to express these probabilities in terms of the tractable cdf of the original Kumaraswamy variable.

A.3 Reparameterized gradients
Let us consider the case where we need derivatives of a function L(u) of the underlying uniform variable u, as when we compute reparameterized gradients in variational inference. Note that

∂L ∂L ∂h ∂t ∂k = × × × , (28)
∂u ∂h ∂t ∂k ∂u

by

chain

rule.

The

term

∂L ∂h

depends

on

a

differen-

tiable observation model and poses no challenge;

the

term

∂h ∂t

is

the

derivative

of

the

hard-sigmoid

function, which is 0 for t < 0 or t > 1, 1 for

0 < t < 1, and undeﬁned for t ∈ {0, 1}; the

term

∂t ∂k

=

r−l

follows directly from (20, left);

the term

∂k ∂u

=

∂ ∂u

FK−1

(

u;

a,

b)

depends

on

the

Kumaraswamy inverse cdf (19) and also poses no

challenge. Thus the only two discontinuities hap-

pen for t ∈ {0, 1}, which is a 0 measure set under

the stretched Kumaraswamy: we say this reparam-

eterisation is differentiable almost everywhere, a

useful property which essentially circumvents the

discontinuity points of the rectiﬁer.

A.4 HardKumaraswamy PDF and CDF
Figure 8 plots the pdf of the HardKumaraswamy for various a and b parameters. Figure 9 does the same but with the cdf.

Figure 9: HardKuma cdf for various (a, b).

select the best models based on validation loss. For the MSE trade-off experiments on all aspects combined, we train for a maximum of 50 epochs.

Optimizer Learning rate Word embeddings Hidden size Batch size Dropout Weight decay Cell

Adam 0.0004 200D (Wiki, ﬁxed)
200 256 0.1, 0.2 1 ∗ 10−6 RCNN

Table 4: Beer hyperparameters.

Figure 8: HardKuma pdf for various (a, b).

For the Bernoulli baselines we vary L0 weight λ1 among {0.0002, 0.0003, 0.0004}, just as in the original paper. We set the fused lasso (coherence) weight λ2 to 2 ∗ λ1.
For the HardKuma models we set a target selection rate to the values targeted in Table 2, and optimize to this end using the Lagrange multiplier. We chose the fused lasso weight from {0.0001, 0.0002, 0.0003, 0.0004}.

B Implementation Details
B.1 Multi-aspect Sentiment Analysis
Our hyperparameters are taken from Lei et al. (2016) and listed in Table 4. The pre-trained word embeddings and data sets are available online at http://people.csail.mit.edu/ taolei/beer/. We train for 100 epochs and

B.1.1 Recurrent Unit
In our multi-aspect sentiment analysis experiments we use the RCNN of Lei et al. (2016). Intuitively, the RCNN is supposed to capture n-gram features that are not necessarily consecutive. We use the bigram version (ﬁlter width n = 2) used in

Lei et al. (2016), which is deﬁned as:
λt = σ(W λxt + U λht−1 + bλ) c(t1) = λt c(t−1)1 + (1 − λt) W1xt c(t2) = λt c(t−2)1 + (1 − λt) (c(t−1)1 + W2xt)
ht = tanh c(t2) + b
B.1.2 Expected values for dependent latent variables
The expected L0 is a chain of nested expectations, and we solve each term
Ep(zi|x,z<i) [I[zi = 0] | z<i] = 1 − FK r−−ll ; ai, bi (29)
as a function of a sampled preﬁx, and the shape parameters ai, bi = gi(x, z<i; φ) are predicted in sequence.
B.2 Sentiment Classiﬁcation (SST)
For sentiment classiﬁcation we make use of the PyTorch bidirectional LSTM module for encoding sentences, for both the rationale extractor and the classiﬁer. The BiLSTM ﬁnal states are concatenated, after which a linear layer followed by a softmax produces the prediction. Hyperparameters are listed in Table 5. We apply dropout to the embeddings and to the input of the output layer.

evaluate every 1000 updates, and select the best model based on validation loss. Figure 10 shows a correct and incorrect example with HardKuma attention for each relation type (entailment, contradiction, neutral).

Optimizer Learning rate Word embeddings Hidden size Batch size Dropout

Adam 0.0001 300D (Glove, ﬁxed)
200 64 0.2

Table 6: SNLI hyperparameters.

Optimizer Learning rate Word embeddings Hidden size Batch size Dropout Weight decay Cell

Adam 0.0002 300D Glove (ﬁxed)
150 25 0.5
1 ∗ 10−6 LSTM

Table 5: SST hyperparameters.

B.3 Natural Language Inference (SNLI)
Our hyperparameters are taken from Parikh et al. (2016) and listed in Table 6. Different from Parikh et al. is that we use Adam as the optimizer and a batch size of 64. Word embeddings are projected to 200 dimensions with a trained linear layer. Unknown words are mapped to 100 unknown word classes based on the MD5 hash function, just as in Parikh et al. (2016), and unknown word vectors are randomly initialized. We train for 100 epochs,

<s> The two dogs are black .
<s> Four people in a kitchen cooking .

<s> 0 0 0 0 0 0 0

<s> 0 0 0 0 0 0 0 0

Two 0 0 0 0 0 0 0 Four 0 89 0 0 0 0 0 0

black 0 0 0 0 0 100 0 people 0 0 53 0 0 0 0 0

dogs 0 0 0 90 0 0 0

in 0 0 0 0 0 0 0 0

running 0 0 0 23 0 0 0

a0 0 0 0 0 0 0 0

kitchen 0 0 0 0 0 100 74 0

(a) Entailment (correct)

(b) Entailment (incorrect, pred: neutral)

<s> Three cats race on a track .
<s> a couple on a motorcycle

<s> 0 0 0 0 0 0 0 0 Three 0 84 0 0 0 0 0 0 dogs 0 0 100 0 0 0 18 0 racing 0 0 0 87 0 0 43 0
on 0 0 0 0 0 0 0 0 racetrack 0 0 33 48 0 0 73 0
(c) Contradiction (correct)

<s> 0 0 0 0 0 0 A0 0 0 0 0 0
person 0 0 15 0 0 0 on 0 0 0 0 0 0 a0 0 0 0 0 0
motorcycle 0 0 0 0 0 89 .0 0 0 0 0 0
(d) Contradiction (incorrect, pred: entailment)

<s> They are in the desert .
<s> A dog found a bone

<s> 0 0 0 0 0 0 0

<s> 0 0 0 0 0 0 A0 0 0 0 0 0

People 0 0 0 0 0 0 0

dog 0 0 89 13 0 12

walking 0 0 0 0 0 0 0 gnawing 0 0 0 0 0 47

through 0 0 0 0 0 0 0

on 0 0 0 0 0 0

dirt 0 0 0 0 0 81 0

a0 0 0 0 0 0

.0 0 0 0 0 0 0

bone 0 0 12 14 0 76

.0 0 0 0 0 0

(e) Neutral (correct)

(f) Neutral (incorrect, pred: entailment)

Figure 10: HardKuma attention in SNLI for entailment, contradiction, and neutral.

