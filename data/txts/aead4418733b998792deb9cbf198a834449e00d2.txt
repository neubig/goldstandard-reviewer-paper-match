Symbolic Brittleness in Sequence Models: on Systematic Generalization in Symbolic Mathematics
Sean Welleck,1 2 Peter West,1 Jize Cao,1 Yejin Choi1 2
1 Paul G. Allen School of Computer Science & Engineering, University of Washington 2 Allen Institute for Artiﬁcial Intelligence wellecks@uw.edu

arXiv:2109.13986v2 [cs.LG] 24 Feb 2022

Abstract
Neural sequence models trained with maximum likelihood estimation have led to breakthroughs in many tasks, where success is deﬁned by the gap between training and test performance. However, their ability to achieve stronger forms of generalization remains unclear. We consider the problem of symbolic mathematical integration, as it requires generalizing systematically beyond the test set. We develop a methodology for evaluating generalization that takes advantage of the problem domain’s structure and access to a veriﬁer. Despite promising in-distribution performance of sequenceto-sequence models in this domain, we demonstrate challenges in achieving robustness, compositionality, and outof-distribution generalization, through both carefully constructed manual test suites and a genetic algorithm that automatically ﬁnds large collections of failures in a controllable manner. Our investigation highlights the difﬁculty of generalizing well with the predominant modeling and learning approach, and the importance of evaluating beyond the test set, across different aspects of generalization.1
1 Introduction
Despite their success, recent studies reveal undesirable properties of conventional neural sequence models, such as assigning high-probabilities to unrealistic sequences (Holtzman et al. 2020; Welleck et al. 2020), susceptibility to adversarial attacks (Wallace et al. 2019), and limited generalization on symbolic tasks (Saxton et al. 2019; Nogueira, Jiang, and Li 2021), even with very large models and datasets (Henighan et al. 2020). Despite these drawbacks, Lample and Charton (2019) recently demonstrated that a standard sequence-to-sequence model, which we call a neural sequence integrator, performs surprisingly well at symbolic integration, solving problems that are beyond traditional symbolic solvers and achieving near perfect test accuracy.
Recent studies suggest that achieving strong and systematic generalization is difﬁcult with vanilla sequence-tosequence methods, as they latch onto regularities in the training data, learning dataset-speciﬁc solutions that do not generalize beyond the training distribution (e.g. Agrawal, Batra, and Parikh (2016); Lake and Baroni (2018); Bahdanau et al. (2019); Hupkes et al. (2020)). Symbolic integration
1Code available at: https://github.com/wellecks/symbolic generalization

Input

Integral

Prediction

30 cos(39x) 17 cos(83x) 34 cos(77x)

1103 sin(39x) 1873 sin(83x) 3747 sin(77x)

1103 sin(39x) 

117 sin(83x) 

sin(77x)



x209

2110 x210

2110 x210 

x764

7615 x765

7615 x765 

x209 + x764 2110 x210 + 7615 x765 2015 x205 

−241 123x 4x + x465 + 1

−241x

123x

log(123)

x466 + x + 4x

466

log(4)

−239x − 14400  1+l1o2g3(x123)  x446666 + x + ex 

Table 1: Despite its impressive ability to integrate equations that are out of reach for traditional symbolic solvers, the neural sequence integrator shows deﬁciences in robustness (top) and compositionality (middle), and fails on adversarial problems discovered by SAGGA (bottom).

– ﬁnding the integral of a mathematical function – speciﬁcally requires these forms of generalization, as it involves an underlying structure that extends beyond this ﬁxed training distribution. For instance, the rule k = kx + C applies to
all constants k, and the sum rule f1 + f2 = (f1 + f2) means that integrating two functions correctly should imply integrating their sum correctly. Symbolic integration also offers a structured problem domain and a veriﬁer for evaluating whether a proposed solution is correct, making it an effective setting for evaluating generalization. As the neural sequence integrator relies on a common recipe– a large-scale transformer trained to maximize the likelihood of a training set of input-output sequences – it is especially interesting to study whether it generalizes systematically.
In this paper, we ﬁnd a discrepancy between the traditional notion of generalization captured by test set accuracy and the generalization needed in symbolic mathematics. While the model’s test accuracy is nearly perfect, we ﬁnd this breaks down when testing its robustness, compositionality, and out-of-distribution generalization (e.g. Table 1). We describe a methodology for evaluating these aspects, by constructing problem sets and developing a genetic

Robustness
2x42✓

Compositionality

✓

✓

✓

x2 cos(x)2 sin(x)2

Out-of-Distribution ✓
Training distribution

3x42✗

x2 + sin(x)2 ✗

5xx2✗
exploit
5x + 10 + cos(x)sin(x)2 + . . . ✗
long problem

Figure 1: Illustrating robustness, compositionality, and outof-distribution deﬁciencies in the neural sequence integrator.

algorithm, SAGGA (Symbolic Archive Generator with Genetic Algorithms), that automatically discovers diverse and targeted failures. We ﬁnd that successfully integrating an in-distribution problem does not imply success on nearby problems, despite being governed by the same underlying rule (robustness). Moreover, the model often succeeds on a collection of problems without being able to systematically compose those problems (compositionality), and struggles to generalize to longer problems, larger values, and functions not covered in training (out-of-distribution).
In addition to the model’s approximate mode being incorrect – i.e. the most probable sequence returned by beam search – the deﬁciencies are present deeper into its ranked list of candidate solutions, impacting the model’s effectiveness in a search-and-verify setting. Overall, our investigation highlights the difﬁculty of achieving robustness, compositionality, and out-of-distribution generalization with the predominant modeling and learning approach, and the importance of evaluating beyond the test set, across aspects of generalization that are required by the task at hand.

2 Problem Setup
Symbolic integration is the problem of ﬁnding the integral y of an input equation x. For instance, x2/2 is the integral of x, up to an additive constant.

Neural sequence integrator. Lample and Charton (2019)

frame symbolic integration as a sequence-to-sequence prob-

lem. In this view, input and output equations x and y are

preﬁx-notation sequences. The neural sequence integrator

uses a 6-layer transformer (Vaswani et al. 2017) to model

the distribution pθ(y|x) =

Ty t=1

pθ

(yt|y<t,

x)

by

train-

ing the model to maximize the log-likelihood of a set of

training problems, arg maxθ (x,y)∈D log pθ(y|x). Given

a trained model and input x, a set of predicted solutions

ranked by a model score is obtained by beam search, denoted

{yˆ1, . . . , yˆk} = fθ(x; k, b), where b is beam size and k is

the number of candidates saved for evaluation. For brevity

we omit b in the discussion unless necessary.

Evaluation. The standard practice is to evaluate a candidate yˆ by checking whether the derivative of yˆ is equivalent to x using a symbolic solver (e.g. Sympy). In the maximuma-posteriori (MAP) setting, the model’s output is considered correct if its top-ranked candidate yˆ1 is correct. This criterion is relaxed in the search-and-verify setting, where the

model’s output is considered correct if any of its k candidates {yˆ1, . . . , yˆk} is correct. In this view, the neural network narrows the search space to a small set of candidates that are checked, trading off correctness for search and veriﬁcation cost. We denote checking k candidate solutions as,
m(x, fθ(x; k)) = 01 xoth≡erwddxiysˆei.for any i ∈ 1 to k, (1)
In other words, m(·, ·) is 1 when the model fails to predict the correct integral, and 0 when the model succeeds. We measure the proportion of failures on problems X = {x1, . . . , xN } using k candidate solutions per problem as:
1 Fail@k(fθ, X) = N m(x, fθ(x; k)). (2)
x∈X
Fail@k is 0 when the model correctly integrates all of the problems in X, and increases towards 1 as it fails to integrate more problems. Evaluating a model’s performance in the MAP setting corresponds to evaluating Fail@1, while the search-and-verify setting with a budget of k > 1 candidates uses Fail@k. We omit k in fθ(x; k) unless necessary.
2.1 Experiment Structure
We structure our investigation into three parts (Figure 1). We begin close to the model’s training distribution, evaluating robustness to small perturbations of in-distribution problems and simple functions. We then ask whether learning to integrate a collection of functions implies that the model can integrate a composition of those functions. Finally we depart from the training distribution by studying extrapolation to larger problems and values, then by ﬁnding adversarial exploits that expose gaps in the training distribution.
Experimental setup. We use the implementation and pretrained model from Lample and Charton (2019) for all of our experiments, speciﬁcally the FWD+BWD+IBP model which obtained top-10 accuracies of 95.6%, 99.5%, and 99.6% on their publicly available test sets.2 Our evaluation is based on their code, we use their utilities for inputs and outputs, and by default use beam search with beam-size 10. Following the authors, we use Sympy to check whether the derivative of a prediction is equal to the original problem. We generously count the prediction as correct if a timeout occurs. See the Apppendix for additional details.
2.2 Automatic Problem Discovery with SAGGA
Automatically ﬁnding problems that expose deﬁciencies requires a non-differentiable cost (Equation 1), satisfying constraints for valid equations, and ﬁnding diverse problem sets to characterize each aspect of generalization. To address these challenges, we develop SAGGA (Symbolic Archive Generation with Genetic Algorithms), a gradient-free genetic algorithm which iteratively ﬁnds diverse failures.
At each iteration, SAGGA mutates a seed set of problems by modifying each problem’s equation tree, ensuring that the
2https://github.com/facebookresearch/SymbolicMathematics/, commit 4596d07.

Algorithm 1: SAGGA. Each seed problem denoted as xˆ, mutated problem as x˜, archived problem as x.

Parameters: Fitness F (fθ, x) → R, mutate and seed strategies, archive size N .

Output: Problem archive D = {x1, . . . , xN }.

D=∅

// initial archive

xˆ(10:M) = seed(D, ∅) while |D| < N do

// initial seed

// generate mutations

x˜(1i:)M = mutate(xˆ(1i:)M )

// select problems by fitness

x(1i:)M = select(F, x˜(1i:)M )

// archive selected problems

D = D ∪ x(1i:)M

// choose next seed

xˆ(1i:+M1) = seed(D, F, x˜(1i:)M )

resulting candidates are valid equations. The candidates are scored by a ﬁtness function – i.e. according to whether the neural sequence integrator fails to integrate the problem and other desired constraints – and the highest-ﬁtness candidates are saved in a problem archive. The next seed set is then formed to balance diversity and ﬁtness, by clustering candidates and selecting the highest-ﬁtness members of each cluster. SAGGA continues until the archive contains a target number of problems. Algorithm 1 summarizes SAGGA.
SAGGA offers control over the types of problems that it discovers through its seed problems, ﬁtness function, and mutation strategy. We detail our choices for each kind of generalization in their respective sections, and show default settings and further implementation details in the Appendix.

3 Robust or Brittle?
First, we study whether the model’s strong test-set performance adequately represents its robustness. Robustness tells us whether the integration model systematically solves all problems in a neighborhood governed by a generalizable pattern; for instance a model that solves 26x42 should solve 53x42. We study problems that are nearby to those from the original test distribution, as well as to simple primitive functions that offer ﬁne-grained, interpretable control.
A robust model is stable to small perturbations in input, meaning that it gets nearby problems x˜ correct when it gets a problem x correct. Formally, let X = {x1, . . . , xN } contain problems that the model gets correct, x∈X m(x, fθ(x)) = 0, and let Nd(x) be a set of problems that are nearby to x according to a distance d(x, x˜). We measure robustness by measuring failures on nearby problems,

Fail@k(fθ, XN ),

(3)

where XN = x∈X Nd(x). We measure this quantity by varying (i) the neighborhood Nd(x) used to generate nearby problems, and (ii) the seed problems X to consider. Below,
we will refer to a problem as x or f interchangeably.

Type coeff
coeff coeff +exp +ln

Test
k1 ln(k2x) k1x k1x42 k1 exp(k2x) k1 sin(k2x) k1 cos(k2x) k1 tan(k2x)
1/k · f k·f f + ex f + ln(x)

Fail@50
0.0 0.0 0.0 15.4 6.6 10.6 13.9
5.9 5.4 0.9 1.9

Fail@10
0.0 0.0 6.1 20.8 19.6 20.7 17.4
12.0 5.8 1.6 3.2

Fail@1
0.0 0.0 45.5 30.3 29.7 28.2 24.2
13.7 16.3
3.3 5.3

Table 2: Robustness results with simple primitives (top) and validation problems f which the model correctly integrates (bottom). Coefﬁcients are sampled from [1, 100].

Input

Integral

Prediction

30 cos(39x) 1130 sin(39x) 1103 sin(39x) 

17 cos(83x) 8137 sin(83x) 117 sin(83x) 

34 cos(77x) 7374 sin(77x) sin(77x)



26x42 88x42 53x42

4236 x43 8483 x43 4533 x43

2463 x43



8x43



(x44 + x)/x 

Table 3: Robustness examples. We show the model’s top prediction (beam search, size 10). Note that (x44 + x)/x = x43 + 1; its derivative is 43x42 and is hence incorrect.

3.1 Manually Testing Robustness
To deﬁne nearby problems, we ﬁrst consider manual templates which minimally perturb a problem f , e.g.
k · f, f + ln x, . . .
These problems are nearby f in the sense that a single operation is added to the problem’s equation tree, or a small number of node values are changed in the tree.
Brittleness on simple primitive functions. We ﬁrst investigate whether the neural sequence integrator is robust on simple primitive functions, since they make up more complicated functions and are frequently entered by real-world users. We use a manual neighborhood which yields,
XN = {k1 ln(k2x), k1 exp(k2x), k1x, k1x42, k1 sin(k2x), k1 cos(k2x), k1 tan(k2x)},
where k1 ∼ U (a, b) and k2 ∼ U (a, b) are randomly sampled coefﬁcients from a range (a, b). We use [0, 100] which is covered by the training distribution and evaluate on 1,000 k1, k2 pairs sampled without replacement for each primitive.
Table 2 shows the results. On a positive note, the neural sequence integrator is robust on the primitives k1x and k1 ln(k2x). The integral of k1x is k21 x2, so the model learned to divide by 2 for these cases. The integral of ln involves copying the coefﬁcients into a correct template (that

is, k1 ln(k2x) = k1x(ln(k2x) − 1)), and the neural sequence integrator learned this behavior.
On the other hand, the model is surprisingly brittle on
the other primitives. These require dividing coefﬁcients (e.g. k1 cos(k2x) = kk12 sin(k2x)). The failure rate shows that
the model has not perfectly learned the required division be-
havior. Moreover, despite learning a ‘division by 2’ rule for integrating k1x, the neural sequence integrator’s failures on k1x42 indicate that it did not perfectly learn an analogous ‘division by 43’ rule. Table 3 shows examples.

Test accuracy does not imply robustness. Next, we want to see whether the neural sequence integrator’s strong test accuracy implies that it is robust on test problems. We use the validation set, and perturb validation problems that the model correctly integrates using the neighborhoods,

1 XN = { f,

k · f },

XN = {f + ex,

f + ln(x)},

1

k

2

where k ∼ U(1, 100). The ﬁrst set multiplies the function by a constant, while the second adds a single primitive.
Table 2 shows the results. Despite achieving perfect accuracy on the original problems, the model frequently fails under the slight perturbations. The local neighborhood around validation examples reveals deﬁciencies in robustness that are not evident from validation performance alone, aligning with ﬁndings in NLP tasks (Gardner et al. 2020).

3.2 Automatically Finding Robustness Failures
Next, we use SAGGA to automatically discover robustness failures in the neighborhood of a seed set of problems.
Discovering brittleness near simple problems. First, we run SAGGA and only allow it to mutate leaves in a problem’s equation tree into a random integer. The problems are nearby in the sense that the tree’s structure is not changing, only a small number of its leaf values. We use SAGGA to mutate the leaves of seed sets of 9 polynomials Xpoly and 9 trigonometric functions Xtrig, which are listed in the Appendix. We run SAGGA until it discovers 1000 failing problems, then cluster these using k-means on SciBERT embeddings (Beltagy, Lo, and Cohan 2019) of each problem.
Table 4 shows three members from three discovered problem clusters, for the polynomial and trigonometric seeds. Intuitively, each cluster shows failures in a neighborhood around a prototypical problem – for instance, on 2x42+k the neural sequence integrator correctly integrates 2x42+21, but not the problems in Cluster 2 (e.g. 2x42 +22). See Appendix Table 16 for more prototypes and the model’s predictions.
Curiously, each problem in a neighborhood is governed by a common template – e.g. the problems {−104, −136, −33} are governed by k = kx + C, yet the failures suggest that the neural sequence integrator has either not inferred the template, or does not apply it across the neighborhood. To investigate this phenomenon, we show the raw model prediction in Table 5, along with its simpliﬁed version and derivative. Compared to the underlying template
k = kx + C the model’s raw output is long and complex. In contrast, the simpliﬁed version is short; we hypothesize this gap makes adhering to the template difﬁcult.

Seed Xpoly
Xtrig

Cluster 1
−104 −136 −33
13 cos 19x 13 cos 83x 17 cos 47x

Cluster 2
2x42 + 22 2x42 + 28 2x42 + 68
13 cos 83x − 59 17 cos 37x − 49 17 cos 41x − 45

Cluster 3
−47 + 2/x − 2/x71 −47 + 2/x − 31/x71 −71 + 36/x + 2/x71
10 sin 47x cos 2x 10 sin 90x cos 2x 19 sin 90x cos 2x

Table 4: Example robustness problems discovered by SAGGA which the neural sequence integrator fails to integrate. See Appendix Table 16 for model predictions.

x
−104 −136 −33

Raw
x2 + 2x − (x + 25)2 x2 − x(x + 130) + 2x x2 + x − (x + 16)2

Simpliﬁed
−48x − 625 −128x −31x − 256

Deriv.
−48 −128 −31

Table 5: The raw model predictions for the problems x and their simpliﬁed forms. Each prediction is incorrect since its derivative is not equal to x. The neural sequence integrator’s raw output is long and varied compared to the underlying integration rule k = kx + C.

Discovering brittleness near validation problems. Finally, we use SAGGA to discover difﬁcult problems that are close to a target set X – in our case validation problems – according to an explicit distance d(x, x˜). This allows for less hand-designing of the perturbations.
Speciﬁcally, we deﬁne a ﬁtness which is high whenever a candidate is close to any problem in a target set X,
−1
ﬁtness(x˜) = min d(x, x˜) · m(x˜, fθ(x˜)). (4)
x∈X
We randomly sample 10 validation problems to form X, set SAGGA’s initial seed to X, and use cosine similarity of SciBERT vectors to deﬁne the distance d(x, x˜). Since the distance now constrains the problems, we are free to use a wider set of mutations: changing a node’s operation, adding an argument to a node, and replacing the node with a random constant, symbol, or simple operation.
Table 6 shows example problems that SAGGA discovers around the successful validation problems, exposing a wider class of robustness failures than our preceding experiments.

4 Integrating Parts But Not The Whole
While the preceding section identiﬁed weaknesses in robustness – for instance, integrating 26x42 but not 88x42 – a remaining question is whether successfully integrating a collection of primitives implies that a composition of those primitives will be successfully integrated.
Compositionality refers to forming compound equations from known primitives and operations. A compositional model should correctly integrate equations of the form,

f = f1 ◦ f2 ◦ · · · ◦ fk,

(5)

Validation Problem −x2 + x + log(4) tan(x) √
3x + 3 − 2
tan(exp(2))/18x

Nearby Failures
−x2 + x + log(4) tan(17x2) −x2 + x + log(4) tan(2x2) −x2 + x + log(4) tan(63/x2)
−86x2 + 62/x − 40 14 + 62/x + 4 14 + 62/x − 2
tan(exp(2 + 71/x))/18x tan(exp(2 − 46x))/18x tan(exp(37x))/18x

Table 6: SAGGA discovers failures around successful validation problems, within a neighborhood deﬁned by an explicit distance. Model predictions are in Appendix Table 16.

Input

Prediction

x1/3 x1/606 x1/3 + x1/606

34 x4/3



x  606 660067
607

35 x 53 + 6163 x 661036 

x209 x764 x209 + x764

2110 x210



7165 x765



2105 x205



14 cos(58x) 46 cos(84x)

279 sin(58x)



2432 sin(84x)



14 cos(58x) + 46 cos(84x) sin(59x) cos(x) 

Table 7: Compositionality examples. We show the model’s top prediction (beam search, width 10). The model successfully integrates the individual primitives, but not their sum.

where f1, . . . , fk are equations that the model successfully integrates, and ◦ is a binary operation (e.g. addition). For instance, a system that integrates x2 and cos x and is capable of addition should successfully integrate x2 + cos x.
Formally, we say that a model is k-compositional with respect to equations X and operation ◦ when it successfully integrates any combination of k equations from X,
x∈X˜ m(x, fθ(x)) = 0, where X˜ = {f1◦· · ·◦fk|fi ∈ X}. We evaluate k-compositionality with respect to addition, using simple primitive functions and validation problems. As integration is linear, (f + g) = f + g, compositionality with respect to addition is a reasonable requirement.
Succeeding on simple primitives, failing on their sum. We collect simple primitives from the coefﬁcient robustness experiments that the model successfully integrates (coeff), and successful exponents xc or x1/c, c ∈ [0, 1000] (exp). We randomly sample 1000 compound equations f1+. . .+fk for k ∈ {2, 3, 4} and evaluate the failure rate. Table 8 shows the results. Adding two primitives gives failure rates of 29% and 85% for coefﬁcient and exponent primitives, respectively, despite failing 0% of the time on the individual prim-

Type
exp(1) exp(2) exp(3) exp(4)

Test
f1 f1 + f2 f1 + f2 + f3 f1 + . . . + f4

Fail@50
00.0 70.8 91.3 86.2

Fail@10
00.0 72.4 97.5 97.4

Fail@1
00.0 84.9 99.5 99.8

coeff(1) f1

00.0

00.0 00.0

coeff(2) f1 + f2

8.60

16.2 29.2

coeff(3) f1 + f2 + f3

23.8

37.5 61.0

coeff(4) f1 + . . . + f4

23.1

38.7 60.0

valid(1) f1

00.0

00.0 00.0

valid(2) f1 + f2

6.80

14.5 15.0

valid(3) f1 + f2 + f3

21.5

36.5 43.6

valid(4) f1 + . . . + f4

52.5

69.0 79.2

Table 8: Compositionality. Top: successful simple primitives from the robustness experiments (Table 2). Bottom: successful validation-set primitives. Despite integrating each primitive, the model struggles to integrate their sums.

Nodes
1-15 20 25 30 35

Fail@10
0.4 1.9 7.2 24.4 49.0

Fail@1
1.6 10.7 17.2 37.1 59.2

Table 9: Extrapolation to more operator nodes under the training data generation process. Training used 1-15 nodes.

itives. As the number of compositions increases, the failure rate increases towards 100%. Table 7 shows examples.
Succeeding on test problems, failing on their sum. We perform a similar experiment using successful validation-set functions. We ﬁlter examples longer than 20 tokens so that composed equations are within the training domain in terms of length, and sample 1000 compound equations f1 + . . . + fk for k ∈ {2, 3, 4}. As seen in Table 8, the failure rate grows as the number of compositions increases, similar to the simple primitives case. Maximizing the likelihood of a large training set did not yield a compositional model.
5 Departing Further From Training
The preceding experiments found problems that were nearby to, or composed directly from, in-distribution examples. In this section, we deliberately move from the model’s training distribution to evaluate its out-of-distribution generalization. First, we study extrapolation to longer equation sizes than those in its training distribution, and to integer ranges that are only sparsely covered in the training set. Then we use SAGGA to expose exotic failures and reveal problem classes that were not covered during training.
Longer problems are more difﬁcult. First, we use the same data-generating process as for training, but vary its parameters to depart from the training distribution. Speciﬁcally, we test extrapolation on number of operator nodes in

Cluster 1
119x 132x 136x

Cluster 2
−240x + 2 cos 2x −398x + 2 cos 2x −692x + 2 sin 2x

Cluster 3
−100xx −149xx −151xx

Cluster 4
158xx2 + 611 256xx2 + 191 332xx2 + 559

Table 11: SAGGA discovers many failures that involve x in an exponent. See Appendix Table 17 for model predictions.

Figure 2: Integer extrapolation. Failure rates for integrating simple primitives with coefﬁcients from the speciﬁed range.

Problem

Exploit

169 sin(4x)/x
−2 sin(42/x) −2 sin(185x2) cos(2x)
357x2 x + 2 sin(2x) 1/(x48(3x + 2)49)

Uses Si(·). Uses Ci(·). Uses Fresnel S, C integrals.
Uses incomplete gamma Γ(a, x). Decoding does not terminate.

Table 10: Exploits discovered by SAGGA whose integrals use out-of-domain functions. See Appendix Table 18 for model predictions.

each equation tree, using Lample and Charton (2019)’s data generation process and varying the max ops parameter. Table 9 shows performance when max ops is increased past the model’s training domain (1-15). The neural sequence integrator does show some extrapolation to equation trees with more operator nodes than it was trained on, but its failure rate increases substantially as the number of nodes increases.
Larger failures on larger digits. Next, we study performance as integer values increase, quickly going out-ofdomain. Considering a sample of 200,000 sequences from the training distribution, 99.4% of the positive integers were between 1 and 100. Other ranges were non-empty but sparsely represented; for instance, 0.2% of the integers were between 100 and 200, and 0.096% between 1,000 and 2,000. Figure 2 shows performance on primitive functions with coefﬁcients from the speciﬁed range. As in the robustness experiments, the x and ln primitives perform well, showing that there is some ability to use large numbers. However, performance severely degrades for the exp, sin, cos, tan primitives as the coefﬁcient magnitudes increase, reaching near 100% failure rates on large coefﬁcients.
Discovering unsupported functionality. Next, we run SAGGA in an unconstrained setting with all mutation types, favoring short problems using the ﬁtness, F (fθ, x) = m(x, fθ(x)) · |x1| , which is positive when the model returns an incorrect integral for x, and higher for shorter problems.
SAGGA discovers exploits based on the neural sequence integrator’s limited training distribution, such as problems whose integral is expressed using the Gamma function Γ(·),

Figure 3: Left: Average length of discovered problems in each iteration, using ﬁtness functions that promote the given length. SAGGA discovers problems of each target length. Right: SAGGA discovers longer problems at a higher rate.
or the cosine integral Ci, which are not included in its training data (Table 10).3 These examples are a reminder that the sequence-to-sequence paradigm determines which functions are ‘built in’ by inclusion in training data; omitted behavior is left unspeciﬁed, leaving it susceptible to exploits.
Finally, the last problem in Table 10 caused the neural sequence integrator to enter a non-terminating loop during decoding (Appendix Table 18), a known idiosyncrasy of autoregressive models with beam search (Welleck et al. 2020).
SAGGA also ﬁnds many clusters that indicate the neural sequence integrator struggles when x appears in an exponent. The discovered problems in Table 11 are a microcosm of our previous ﬁndings: For the ﬁrst cluster, we manually found a nearby problem, 30x, that the model gets correct; this cluster is a robustness failure. The second cluster shows how such failures cascade further as the function is composed. The ﬁnal two clusters involve xx or xx2 , which do not have analytical integrals;4 these clusters are exploits.
Finding problems with target properties. Finally, we generate failures of a target length, by running SAGGA to target length 10, 20, and 40 problems. As seen in Figure 3, SAGGA converges to ﬁnd problems of the target length. Based on our extrapolation experiments, we expect SAGGA to fail more often on longer equations. The right-hand plot in Figure 3 shows that it is also easier to ﬁnd failures for longer equations, in that the archive grows more quickly for longer target lengths. While we visually inspect short equations for interpretability, the growth rate is a reminder that the space of failures is vast for longer equations.
3https://en.wikipedia.org/wiki/Trigonometric integral. 4https://www.wolframalpha.com/input/?i=integral+of+x**x

Failures@500 Success@500

p@1
0.93912 0.91647

p@500
3.9×10−6 3.5×10−6

p@{1-500}
0.99435 0.99316

Table 12: Probability assigned to the top candidate (p@1), the 500’th candidate (p@500), and the mass covered by the top 500 candidates (p@{1-500}), in failures and successful cases (exp-robustness problems, width-500 beam search). Sequences outside of the top 500 are very improbable compared to the most probable candidate. In both cases, the top 500 candidates cover most of the probability mass.

k Unresolved@k

1

91.6%

10

65.2%

Table 13: Percentage of failures on the FWD validation set in which the ground truth y∗ is scored lower than the top beam candidate (Unresolved@1) or the bottom beam candidate (Unresolved@10), meaning that perfect search would leave the failures at level k unresolved.

6 Is it a search problem? Distinguishing
between model and search errors
Both the experiments of Lample and Charton (2019) and our own generate candidate solutions from a sequence-tosequence model using beam search. This raises the possibility that failures are due to search rather than the model: what if the highest scoring sequences are correct, but not found?
Speciﬁcally, we want to distinguish between search errors, which occur when pθ(y∗|x) p(y|x) but the search algorithm (e.g. beam search) does not return y∗, and model errors, meaning pθ(y|x) p(y∗|x). Here y∗ is any correct solution to problem x.

6.1 The model is deﬁcient: model errors.
We study simple-robustness and in-distribution problems, and ﬁnd evidence of model deﬁciencies that would remain unresolved with perfect search.
Robustness. First, we study the simple-primitive robustness problems (e.g. k1 exp(k2x), see Table 2), as these short problems resulted in a small number of timeouts, allowing us to scale up search and veriﬁcation. We increase the beam size to 500 candidates, and study the model’s probabilities on the 500 returned candidates and correct solutions. We refer to the candidates ranked by decreasing probability as yb(1ea)m, . . . , yb(5ea0m0) (i.e. yb(1ea)m has the highest probability).
When a correct solution is within the 500 returned candidates, the correct solution often has much lower probability than the top candidate, pθ(yb(1ea)m|x) p(y∗|x). Speciﬁcally, correct solutions often appear at the bottom of the candidates (Figure 4, orange), yet on average the bottom candidate yb(5ea0m0) has probability ≈ 0.0000035, while the top candidate yb(1ea)m has probability ≈ 0.92 (Table 12). These are

Figure 4: Probabilities assigned to the top-ranked beam candidate (max) versus correct candidates (correct) with a large search & veriﬁcation budget (500 candidates). The model often assigns very low probability to correct candidates that are found by increasing the search budget; search covers up underlying model deﬁciencies. (exp-robustness)
model deﬁciencies: the model is conﬁdently incorrect, assigning very high probability to an incorrect solution at the top, and very low probability to correct solutions.
When a correct solution is not within the top 500 candidates, the model is again conﬁdently incorrect, with the top candidate yb(1ea)m receiving ≈ 0.94 probability. Improving the search algorithm – e.g. by further increasing the search budget or using an alternative to beam search – would inevitably return a low probability solution, as the 500 candidates already cover more than 99.4% of the probability mass (Table 12). The ﬁndings again point to model errors.
In-distribution. Next, we study in-distribution problems from the FWD validation set of Lample and Charton (2019). On failure cases, we test if the ground-truth y∗ is scored above the top k beam candidates, meaning that failure@k might be resolved with perfect search–y∗ was scored more highly but was simply not found. As seen in Table 13, the majority of failures – 91.6% for failure@1 and 65.2% for failure@10 – would remain unresolved with perfect search, again pointing to model deﬁciencies.
6.2 Covering up deﬁciencies with search.
Our preceding experiments showed that the top of the model’s ranked solution list is often made up of incorrect solutions, while correct solutions deeper into the solution list are assigned very low probabilities. A natural question is whether we can simply ‘cover up’ the deﬁcient model by enumerating and verifying even more candidates, while ignoring the model’s probabilities.
On simple robustness problems (e.g. k1 exp(k2x)), we ﬁnd that large search budgets can alleviate failures, i.e. Fail@k decreases as k increases, for instance moving from roughly 30% failure@1 to 3% failure@500 on exp robustness (Figure 5). On more complex problems, verifying more candidates reduces the failure rate, yet our experiments do not indicate that failures approach zero for practical search

Candidates
1 10 50

Verify (hrs)
0.088 0.463 2.250

Worst-case (hrs)
0.833 8.333 41.667

Table 14: Runtime needed to verify the 3,000-problem exponent compositionality cases (exp, Table 8) with Sympy, using k candidates for k ∈ {1, 10, 50}.

budgets. For instance, in our compositionality exp experiments (Table 8), verifying 50 instead of 1 candidate reduces failures, but not below 90%. Larger search budgets quickly become impractical to verify: for compositionality exp, the veriﬁcation time increases substantially, from around 5 minutes for 1 candidate to over 2 hours for 50, with worst-case veriﬁcation time of 41.6 hours (Table 14). Looking ahead, developing methods that decrease search and veriﬁcation cost can help to further cover up a subset of model errors, yet improving the underlying model remains a core issue.
7 Related Work
In this work, we study systematic generalization in sequence models applied to symbolic integration, in terms of robustness, compositionality, and extrapolation, and develop a genetic algorithm for building adversarial problem sets.
Symbolic mathematics and sequence models. Several works study extrapolation to longer sequences and larger digits in synthetic arithmetic and basic mathematics tasks (Zaremba and Sutskever 2014; Trask et al. 2018; Saxton et al. 2019; Nogueira, Jiang, and Li 2021). Sequence models have also been applied to polynomial rewriting (Piotrowski et al. 2019; Agarwal, Aditya, and Goyal 2021), and differential system stability (Charton, Hayat, and Lample 2021). For symbolic integration, Davis (2019) argue that the neural sequence integrator’s test performance should be qualiﬁed, though without an empirical demonstration. These critiques motivate our focus on the neural sequence integrator (Lample and Charton 2019), whose performance we characterize and empirically study in terms of systematic generalization.
Systematic generalization. Several works identify difﬁculties with modern methods on synthetic tasks (e.g. Lake and Baroni (2018); Bahdanau et al. (2019); Hupkes et al. (2020); Kim and Linzen (2020)) and machine translation (Raunak et al. 2019), with a focus on compositionality and extrapolation. Some methods address systematicity with inductive biases in model structure (Andreas et al. 2016; Bahdanau et al. 2019), and others through the data (Hill et al. 2020; Andreas 2020) or learning procedure (Lake 2019; Vani et al. 2021). We focus on systematic generalization deﬁciencies in a state-of-the-art model in a new setting – symbolic integration – with additional aspects of generalization.
Robustness and adversaries in sequence models. Several works study robustness in NLP, including classiﬁcation (Tu et al. 2020), word substitutions (Jia et al. 2019), domain

shift in QA (Kamath, Jia, and Liang 2020) or topic distributions (Oren et al. 2019). Several methods ﬁnd adversarial examples in NLP (Morris et al. 2020). Alzantot et al. (2018) use genetic algorithms in a classiﬁcation setting, while we consider generation. Michel et al. (2019) constrain input sequences to be similar and use a gradient-based attack to swap tokens. We face a non-differentiable cost and generate large collections of failures with a wide class of mutations.
8 Conclusion
We study generalization in symbolic mathematics using the predominant modeling paradigm: a large-scale transformer trained with maximum likelihood. We ﬁnd deﬁciencies that are not captured by test accuracy, including brittleness to small perturbations, difﬁculty composing known solutions, and gaps in the training distribution. We offer speculations based on our results. Due to the large space of equations, practical empirical distributions do not provide a dense sampling of individual problem types (e.g. k1 cos(k2x)), and each empirical sample contains shared biases from the underlying data generator (e.g. integer values, lengths). Thus, sparse test sets do not adequately measure systematic generalization. From a learning perspective, generic networks trained with SGD do not necessarily favor the simplest hypothesis to explain the data; thus a sparse training set yields an underconstrained hypothesis space, with hypotheses that do not strongly generalize (e.g. Table 5), causing behavior that breaks simple rules (e.g. adhering to a template or following the sum rule). We suspect that inductive biases– e.g. encoded through the training distribution, architectural components, or learning algorithm– are needed to narrow the hypotheses to those that strongly generalize.
References
Agarwal, V.; Aditya, S.; and Goyal, N. 2021. Analyzing the Nuances of Transformers’ Polynomial Simpliﬁcation Abilities. arXiv:2104.14095.
Agrawal, A.; Batra, D.; and Parikh, D. 2016. Analyzing the Behavior of Visual Question Answering Models. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, 1955–1960. Austin, Texas: Association for Computational Linguistics.
Alzantot, M.; Sharma, Y.; Elgohary, A.; Ho, B.-J.; Srivastava, M.; and Chang, K.-W. 2018. Generating Natural Language Adversarial Examples. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, 2890–2896. Brussels, Belgium: Association for Computational Linguistics.
Andreas, J. 2020. Good-Enough Compositional Data Augmentation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, 7556–7566. Online: Association for Computational Linguistics.
Andreas, J.; Rohrbach, M.; Darrell, T.; and Klein, D. 2016. Neural module networks. In Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition. ISBN 9781467388504.

Bahdanau, D.; Murty, S.; Noukhovitch, M.; Nguyen, T. H.; De Vries, H.; and Courville, A. 2019. Systematic generalization: What is required and can it be learned? In 7th International Conference on Learning Representations, ICLR 2019.
Beltagy, I.; Lo, K.; and Cohan, A. 2019. SciBERT: A Pretrained Language Model for Scientiﬁc Text. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLPIJCNLP), 3615–3620. Hong Kong, China: Association for Computational Linguistics.
Charton, F.; Hayat, A.; and Lample, G. 2021. Learning advanced mathematical computations from examples. arXiv:2006.06462.
Davis, E. 2019. The Use of Deep Learning for Symbolic Integration: A Review of (Lample and Charton, 2019). arXiv:1912.05752.
Gardner, M.; Artzi, Y.; Basmov, V.; Berant, J.; Bogin, B.; Chen, S.; Dasigi, P.; Dua, D.; Elazar, Y.; Gottumukkala, A.; Gupta, N.; Hajishirzi, H.; Ilharco, G.; Khashabi, D.; Lin, K.; Liu, J.; Liu, N. F.; Mulcaire, P.; Ning, Q.; Singh, S.; Smith, N. A.; Subramanian, S.; Tsarfaty, R.; Wallace, E.; Zhang, A.; and Zhou, B. 2020. Evaluating Models’ Local Decision Boundaries via Contrast Sets. In Findings of the Association for Computational Linguistics: EMNLP 2020, 1307–1323. Online: Association for Computational Linguistics.
Henighan, T.; Kaplan, J.; Katz, M.; Chen, M.; Hesse, C.; Jackson, J.; Jun, H.; Brown, T. B.; Dhariwal, P.; Gray, S.; Hallacy, C.; Mann, B.; Radford, A.; Ramesh, A.; Ryder, N.; Ziegler, D. M.; Schulman, J.; Amodei, D.; and McCandlish, S. 2020. Scaling Laws for Autoregressive Generative Modeling. ArXiv, abs/2010.14701.
Hill, F.; Lampinen, A.; Schneider, R.; Clark, S.; Botvinick, M.; McClelland, J. L.; and Santoro, A. 2020. Environmental drivers of systematicity and generalization in a situated agent. In International Conference on Learning Representations.
Holtzman, A.; Buys, J.; Du, L.; Forbes, M.; and Choi, Y. 2020. The Curious Case of Neural Text Degeneration. In International Conference on Learning Representations.
Hupkes, D.; Dankers, V.; Mul, M.; and Bruni, E. 2020. Compositionality Decomposed: How do Neural Networks Generalise? Journal of Artiﬁcial Intelligence Research.
Jia, R.; Raghunathan, A.; Go¨ksel, K.; and Liang, P. 2019. Certiﬁed Robustness to Adversarial Word Substitutions. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), 4129–4142. Hong Kong, China: Association for Computational Linguistics.
Kamath, A.; Jia, R.; and Liang, P. 2020. Selective Question Answering under Domain Shift. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, 5684–5696. Online: Association for Computational Linguistics.

Kim, N.; and Linzen, T. 2020. COGS: A Compositional Generalization Challenge Based on Semantic Interpretation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), 9087–9105. Online: Association for Computational Linguistics.
Lake, B.; and Baroni, M. 2018. Still Not Systematic After All These Years: On the Compositional Skills of SequenceTo-Sequence Recurrent Networks. Iclr2018.
Lake, B. M. 2019. Compositional generalization through meta sequence-to-sequence learning. In Advances in Neural Information Processing Systems.
Lample, G.; and Charton, F. 2019. Deep learning for symbolic mathematics. arXiv preprint arXiv:1912.01412.
Michel, P.; Li, X.; Neubig, G.; and Pino, J. 2019. On Evaluation of Adversarial Perturbations for Sequence-to-Sequence Models. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), 3103–3114. Minneapolis, Minnesota: Association for Computational Linguistics.
Morris, J.; Liﬂand, E.; Yoo, J. Y.; Grigsby, J.; Jin, D.; and Qi, Y. 2020. TextAttack: A Framework for Adversarial Attacks, Data Augmentation, and Adversarial Training in NLP. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, 119–126. Online: Association for Computational Linguistics.
Nogueira, R.; Jiang, Z.; and Li, J. J. 2021. Investigating the Limitations of the Transformers with Simple Arithmetic Tasks. ArXiv, abs/2102.13019.
Oren, Y.; Sagawa, S.; Hashimoto, T. B.; and Liang, P. 2019. Distributionally Robust Language Modeling. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), 4227–4237. Hong Kong, China: Association for Computational Linguistics.
Piotrowski, B.; Urban, J.; Brown, C. E.; and Kaliszyk, C. 2019. Can Neural Networks Learn Symbolic Rewriting? CoRR, abs/1911.04873.
Raunak, V.; Kumar, V.; Metze, F.; and Callan, J. 2019. On Compositionality in Neural Machine Translation. ArXiv, abs/1911.01497.
Saxton, D.; Grefenstette, E.; Hill, F.; and Kohli, P. 2019. Analysing Mathematical Reasoning Abilities of Neural Models. In International Conference on Learning Representations.
Trask, A.; Hill, F.; Reed, S.; Rae, J.; Dyer, C.; and Blunsom, P. 2018. Neural arithmetic logic units. In Advances in Neural Information Processing Systems.
Tu, L.; Lalwani, G.; Gella, S.; and He, H. 2020. An Empirical Study on Robustness to Spurious Correlations using Pre-trained Language Models. Transactions of the Association for Computational Linguistics, 8: 621–633.
Vani, A.; Schwarzer, M.; Lu, Y.; Dhekane, E.; and Courville, A. 2021. Iterated learning for emergent systematicity in

{VQA}. In International Conference on Learning Representations.
Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. Attention is all you need. In Advances in Neural Information Processing Systems.
Wallace, E.; Feng, S.; Kandpal, N.; Gardner, M.; and Singh, S. 2019. Universal Adversarial Triggers for Attacking and Analyzing NLP. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), 2153–2162. Hong Kong, China: Association for Computational Linguistics.
Welleck, S.; Kulikov, I.; Kim, J.; Pang, R. Y.; and Cho, K. 2020. Consistency of a Recurrent Language Model With Respect to Incomplete Decoding. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), 5553–5568. Online: Association for Computational Linguistics.
Zaremba, W.; and Sutskever, I. 2014. Learning to Execute. CoRR, abs/1410.4615.

Figure 5: Fail@k with a large search-and-verify budget (500 candidates, beam search) on simple-primitive robustness problems. The model is ‘brittle’ in that the top of its ranked solution list is often made up of incorrect solutions; e.g. the top-ranked solution is incorrect around 25% of the time. However, the correct solution often exists within the model’s top 500 predictions on these problems.
Figure 6: Correct candidate’s ranking out of 500 candidates (beam search) on exp-primitive robustness problems.
A Additional Results
A.1 Large Search Budgets We study the simple-primitive robustness problems (from Table 2, top), as these problems resulted in a small number of Sympy timeouts, allowing for scaling of veriﬁcation. We increase the beam-width to 500 candidates, and show Fail@k for k ∈ {1, 10, 50, 200, 500} in Figure 5. The top of the solution list remains brittle with the larger beam size: that is, with beam size 500 the top-ranked solutions are still frequently incorrect, supporting the ﬁndings in Table 13. However, the correct solution often exists deeper into the ranked list, i.e. Fail@k decreases as k increases, ranging from 0.1 Fail@500 for cos to 3.7 Fail@500 for tan. As an illustration, Figure 6 shows the ranking of the correct solution for exp-robustness problems that had a solution in the top-500 beam candidates.
A.2 Alternate Decoding Strategies So far, we have used beam search, a deterministic search that approximately searches for the highest scoring output sequence. An alternative approach is decoding a set of sequences {yˆ1, . . . , yˆk} by sampling recursively from model-

Figure 7: Comparing Fail@k using sampling vs. beam search on exp-robustness. Beam search outperforms sampling, which we attribute to better exploration with beam search: namely, beam search returns unique candidates, while sampling returns many duplicates in this setting.

Iters Len Nodes Depth 1-term 2-term 3-term

General

3 15.4 7.3

General-Trig 3 21.8 11.1

Robust-Trig 4 16.4 7.7

Robust-Poly 4 23.6 12.6

Distance

3 30.0 15.0

3.6 64.8 24.7 10.5 4.9 55.7 36.6 7.7 4.5 69.9 30.1 0.0 3.9 17.0 24.1 43.7 5.4 32.6 34.2 20.2

Length-10 Length-20 Length-40

61 11.1 5.3 27 21.4 10.0 26 40.6 16.7

3.1 91.3 8.6 0.1 4.2 58.0 35.5 6.4 5.5 13.5 49.3 34.3

Table 15: Summary of the archives discovered with SAGGA for the target failure type shown on the left. Iters gives the number of SAGGA iterations needed to reach 1,000 failures, and 5,000 failures for the Length-X settings. We use the number of + operations plus 1 as a rough proxy for the number of ‘terms’ in a problem; e.g. x2 + 2 has 2 terms.

dependent per-token distributions, yt(i) ∼ q(yt|y<t, x, pθ), where each yˆi = (y1(i), . . . , yT(ii)). We use the common approach of temperature sampling.
Figure 7 shows Fail@k rates for sampling 500 candidates at temperatures {0.6, 0.8, 1.0}, and beam search with 500 candidates, for exp-robustness problems (other simpleprimitives gave similar results). Beam search outperforms sampling, which we attribute to better exploration with beam search: namely, beam search guarantees unique candidates, while sampling returns many duplicates in this setting (here, only 14 unique sequences out of 500 samples).
A.3 SAGGA – Quantitative Summary
Table 15 provides a quantitative summary of the archives discovered with SAGGA. Figure 8 shows the rate at which SAGGA ﬁlls its archive with failure cases under different mutation strategies and ﬁtness functions.

Figure 8: Cumulative failures found per iteration of SAGGA, for various mutation and ﬁtness settings.
B Additional Experiment Details
B.1 Experimental Setup
We use the implementation and pre-trained model from Lample and Charton (2019) for all of our experiments, speciﬁcally the FWD+BWD+IBP model which obtained top10 accuracies of 95.6%, 99.5%, and 99.6% on their publicly available test sets.5 Our evaluation is based on their code provided in beam integration.ipynb. We use their utilities for inputs and outputs, and by default use beam search with beam size 10. When computing Fail@50 we use beam size 50. Following Lample and Charton (2019), we use Sympy to check whether the derivative of a prediction is equal to the original problem, simplify(derivative - f) == 0. We generously count the prediction as correct if a timeout occurs. Since Sympy’s simplify function is imperfect, there is a possibility of false negatives, which is an inherent limitation of verifying whether a solution is correct. However, an answer that is not feasibly veriﬁable in a search-and-verify setting is incorrect for practical purposes. In preliminary experiments, we found that additionally simplifying the derivative and the function before subtracting tended to reduce timeouts, and found that Sympy’s ability to simplify hyperbolic functions (e.g. sinh, cosh) was limited, so we discard problems with hyperbolic functions from our analysis. Future work could verify with additional libraries, and we release all predictions for inspection.
B.2 Robustness Test accuracy does not imply robustness. For XN1 we sample 100 successful validation problems f and 10 values of k, while for XN2 we sample 1000 successful validation problems.
B.3 Compositionality Validation problems. Fail@50 valid had abnormally many timeouts compared to the other experiments; for this setting only we do not consider a timeout as correct.
B.4 Extrapolation More details of integer distribution. As a rough picture of integer values in the training distribution, we sample 100,000 sequences from each of the FWD and BWD train
5https://github.com/facebookresearch/SymbolicMathematics/, commit 4596d07.

sets, convert them to Sympy, and count Sympy equation tree nodes of type Integer. We ﬁnd that 99.4% of the positive integers were in the range [1, 100), and that other magnitude buckets were non-empty but sparsely represented – e.g. [100, 200) had 0.2%, and [1000, 2000) had 0.096% ,and [10000, 20000) had 0.005% of the positive integers in the problem sample.
B.5 Runtime vs. Candidates
We generously stop early if a successful candidate is found, and use a 1-second timeout for Sympy; thus the results are a lower-bound on the runtime. In the worst case, for N problems, without early stopping and with a t-second timeout, veriﬁcation takes O(ktN ) seconds, e.g. 41.6 hours to verify 50 candidates on 3,000 problems with a 1 second timeout.
C SAGGA
Default ﬁtness. Unless otherwise noted, we use a ﬁtness that favors short (and hence interpretable) problems,
1 F (fθ, x) = m(x, fθ(x; 1)) · |x| , (6)
which is positive when the model returns an incorrect integral of problem x, and higher for shorter problems.
Mutations. A mutation transforms a problem’s equation tree; i.e. h(Tx) → T where Tx is the equation tree of problem x. SAGGA supports mutations for internal tree nodes and leaf nodes.
The internal node mutations replace a node vi with:
• Constant: an integer k ∼ U (vmin, vmax). • Symbol: x. • Operation: vi ∼ {+, ∗, ∗∗} • Add-arg: vi(w1:j, wj+1), where w1:j are the previous ar-
guments to vi and wj+1 is a random simple operation (see below). For instance, sum(1, 2) → sum(1, 2, 3x2). If vi is a one-argument function, this mutation adds a new argument via a sum, e.g. exp(1) → exp(1 + 3x2).
The leaf node mutations replace a node vi with:
• Constant: an integer k ∼ U (vmin, vmax). • Symbol: kx where k ∼ U (vmin, vmax). • Simple-op: random simple operation (see below).
Each random simple operation is of the form k1 ◦ xk2, where k1 ∼ U (vmin, vmax), ◦ ∼ {∗, ∗∗, /}, k2 ∼ {1, 2}. For instance, 3x2, 5/x.
Default settings. Unless otherwise noted, we run SAGGA with the following default settings:
• Beam size: 10 • Evaluation m(x, ·): Fail@1 • Seed size: 100 • Generation size: 1000 • Seed selection: k-means, k = 10 • Fitness threshold τ : 0.01 • Target archive size: 1000

• Integer perturbation range vmin, vmax: [-1000, 1000] • Mutations: all • Seed problems: {1, x, x + 1, x2 + x + 1} Each generation (iteration) takes around 4 minutes on a single Quadro RTX 8000 GPU.
C.1 Robustness. Settings. • Integer perturbation range vmin, vmax: [-100, 100] • Mutations: only Constant mutations.
Seeds. Polynomial robustness:
Xpoly = {1, 2x, 2/x, 2x + 1, 2/x + 1, 2x2 + 2x + 1, 2x2 + 2/x + 1, 2x3 + 2x2 + 1, 2x42 + 2x3 + 2x2 + 1}
Trigonometric robustness:
Xtrig = {17 cos(83x), 17 cos(83x) + 1, 34 sin(77x), 34 sin(77x) + 1 2 cos(2x) + 2x, 2 cos(2x) + 2x + 1, 2 sin(2x) + 2x, 2 sin(2x) + 2x + 1 2 sin(2x) cos(2x)}
In these experiments, the structure of the seeds remains ﬁxed and the integers are varied. The seed elements with non-trivial integers were selected based on the manual neighborhood experiments. Note that this simply accelerates the experiment, as the algorithm could discover these.
C.2 Out-of-distribution. General failures. We run SAGGA in two settings. The ﬁrst is with default settings (see above). The second biases the search towards trigonometric functions, using the seed,
Xtrig-general = {2 cos(2x), 2 cos(2x) + 1 2 sin(2x), 2 sin(2x) + 1 2 cos(2x) + 2x, 2 cos(2x) + 2x + 1, 2 sin(2x) + 2x, 2 sin(2x) + 2x + 1 2 sin(2x) cos(2x)}
and set the default ﬁtness to zero unless the problem contains a trigonometric function.
Target lengths. We use the ﬁtness,
1 F (fθ, x) = m(x, fθ(x; 1)) · |x − | , (7)
where is the target length. To more clearly compare growth rates, compared to the
other experiments we use a smaller seed size of 50 and smaller generation size of 300, and generate 5,000 failures rather than 1,000, resulting in more iterations per algorithm setting.

x

hyp simpliﬁed

derivative

difference

-103 -104 -136 -33 2*x**(42)+21 2*x**(42)+22 2*x**(42)+28 2*x**(42)+68 -47 + 2/x - 2/x**70 -47 + 2/x - 2/x**71 -47 + 2/x - 31/x**71 -71 + 36/x - 2/x**71 13*cos(18*x) 13*cos(19*x) 13*cos(83*x) 17*cos(47*x) 13*cos(82*x) - 59 13*cos(83*x) - 59 17*cos(37*x) - 49 17*cos(41*x) - 45 10*sin(45*x)*cos(2*x) 10*sin(47*x)*cos(2*x) 10*sin(90*x)*cos(2*x) 19*sin(90*x)*cos(2*x) -x**2 + x + log(4)*tan(x) -x**2 + x + log(4)*tan(17*x**2) -x**2 + x + log(4)*tan(2*x**2) -x**2 + x + log(4)*tan(63/x**2) sqrt(3*x + 3) - 2 sqrt(-86**(x**2) + 62/x) - 40 sqrt(14 + 62/x) + 4 sqrt(14 + 62/x) - 2 tan(exp(2))/(18*x) tan(26*x + 2 + exp(2))/(18*x**75) tan(exp(-9504*x**2))/(18*x) tan(exp(-96*x))/(18*x)

-103*x -48*x - 625 -128*x -31*x - 256 x*(2*x**42 + 903)/43 2*x*(x**42 + 469)/43 2*x*(x**42 + 614)/43 2*x*(x**42 + 1502)/43 -47*x + 2*log(x) + 2/(69*x**69) -47*x + 2*log(x) + 2/(35*x**70) -47*x + log(x**2) + x**(-60) -71*x + 36*log(x) + 1/(17*x**70) 13*sin(18*x)/18 sin(19*x) sin(83*x)/17 sin(47*x) -59*x + 13*sin(82*x)/82 -59*x + sin(83*x)/13 -49*x + sin(37*x) -45*x + sin(41*x) -5*cos(43*x)/43 - 5*cos(47*x)/47 -255*cos(45*x)/2201 - 215*cos(49*x)/2201 -5*cos(43*x)/44 - 5*cos(47*x)/46 cos(2*x)**2*cos(90*x)/2 -x**3/3 + x**2/2 - log(4)*log(cos(x)) -x**3/3 + x**2/2 + log(2)*log(cos(17*x**2)... -x**3/3 + x**2/2 + log(2)*log(cos(2*x**2)... -x**3/3 + x**2/2 - log(2)*log(cos(63/x**2)... -2*x + 2*sqrt(3)*(x + 1)**(3/2)/3 -40*x + acosh(86**(-x**2/2)/x) (49*x**(3/2) + 217*sqrt(x) + sqrt(7*x + 31... (49*x**(3/2) + 217*sqrt(x) + (-14*x + 31*... log(x)*tan(exp(2))/18 [add, mul, div, INT-, 1, INT+, 1, 5, 1, 2, mul... -log(cos(exp(-9504*x**2))**(-2))/18144 -log(cos(exp(-96*x))**(-2))/1728

-103 -48 -128 -31 2*x**42 + 21 2*x**42 + 938/43 2*x**42 + 1228/43 2*x**42 + 3004/43 -47 + 2/x - 2/x**70 -47 + 2/x - 4/x**71 -47 + 2/x - 60/x**61 -71 + 36/x - 70/(17*x**71) 13*cos(18*x) 19*cos(19*x) 83*cos(83*x)/17 47*cos(47*x) 13*cos(82*x) - 59 83*cos(83*x)/13 - 59 37*cos(37*x) - 49 41*cos(41*x) - 45 5*sin(43*x) + 5*sin(47*x) 11475*sin(45*x)/2201 + 10535*sin(49*x)/2201 215*sin(43*x)/44 + 235*sin(47*x)/46 -2*sin(2*x)*cos(2*x)*cos(90*x) - 45*sin(90*x)*... -x**2 + x + log(4)*sin(x)/cos(x) -x**2 + 2*x*log(2)*sin(17*x**2)/cos(17*x**2) + x -x**2 + 4*x*log(2)*sin(2*x**2)/cos(2*x**2) + x -x**2 + x + 252*log(2)*sin(63/x**2)/(x**3*cos(... sqrt(3)*sqrt(x + 1) - 2 -40 + (-86**(-x**2/2)*log(86) - 86**(-x**2/2)/... (147*sqrt(x)/2 + (28 + sqrt(749)/(sqrt(x)*sqrt... (147*sqrt(x)/2 + (-14 + sqrt(854)/(2*sqrt(x)*s... tan(exp(2))/(18*x) – 44*x*exp(-9504*x**2)*sin(exp(-9504*x**2))/... exp(-96*x)*sin(exp(-96*x))/(9*cos(exp(-96*x)))

0 56 8 2 0 -8/43 24/43 80/43 0 -2/x**71 (31 - 60*x**10)/x**71 -36/(17*x**71) 0 6*cos(19*x) -138*cos(83*x)/17 30*cos(47*x) 0 -86*cos(83*x)/13 20*cos(37*x) 24*cos(41*x) 0 470*sin(45*x)/2201 - 470*si... 215*sin(43*x)/44 + 235*sin(47... -(43*sin(88*x)/2 + 19*sin... 0 (x - 1)*log(4)*tan(17*x**2) (2*x - 1)*log(4)*tan(2*x**2) (126 - x**3)*log(4)*tan(63/... 0 -sqrt(-86**(x**2) + 62/x) ... (-x*(7*x + 31)**(5/2)*sqrt(2... (x*(7*x + 31)**(5/2)*sqrt(6... 0 – 44*x*exp(-9504*x**2)*sin(... (2*x - exp(96*x))*exp(-9...

Table 16: SAGGA robustness. We show the problem x, the simpliﬁed prediction from the neural sequence integrator, its derivative, and the derivative’s difference with x. The prediction is incorrect when the difference is not zero. When the model’s prediction does not parse successfully, we show its unparsed inﬁx tokens. We show a nearby problem that the model gets correct in bold (either hand-selected or a validation example); all other problems are failures discovered by SAGGA.

x

hyp simpliﬁed

0 30**x

30**x/log(30)

1 119**x

119**(x - 1)

2 132**x

132**x/(1 + log(132))

3 136**x

exp(x*log(136))

4 -100*x**x

-50*x**2

5 -149*x**x

-149*x**2/2

6 -151*x**x

-151*x**2/2

7 158*x**(x**2) + 611 158*x**2/sqrt(x**2 + 1) + 611*x

8 256*x**(x**2) + 191 x*(191*x**2 + 256*x**(x**2) + 191)/(x**2 + 1)

9 332*x**(x**2) + 559 x*(559*x**2 + 332*x + 559)/(x**2 + 1)
10 -240**x + 2*cos(2*x) -exp(x*log(240)) + sin(2*x) 11 -398**x + 2*cos(2*x) -exp(x*log(398)) + sin(2*x) 12 -692**x + 2*sin(2*x) -exp(x*log(692)) - cos(2*x)

derivative

30**x

119**(x - 1)*log(119)

132**x*log(132)/(1 + log(132))

exp(x*log(136))*log(136)

-100*x

-149*x

-151*x

-158*x**3/(x**2 + 1)**(3/2) +

316*x/sqrt(x**2 + 1) + 611

-2*x**2*(191*x**2 + 256*x**(x**2)

+ 191)/(x**2 + 1)**2 + x*(382*x +

256*x**(x**2)*(2*x*log(x) + x))/(x**2

+ 1) + (191*x**2 + 256*x**(x**2) +

191)/(x**2 + 1)

-2*x**2*(559*x**2 + 332*x +

559)/(x**2 + 1)**2 + x*(1118*x +

332)/(x**2 + 1) + (559*x**2 + 332*x +

559)/(x**2 + 1)

-exp(x*log(240))*log(240)

+

2*cos(2*x)

-exp(x*log(398))*log(398)

+

2*cos(2*x)

-exp(x*log(692))*log(692) + 2*sin(2*x)

difference 0 119**(x - 1)*(-119 + log(119)) -132**x/(1 + log(132)) -136**x + exp(x*log(136))*log(136) -100*x + 100*x**x -149*x + 149*x**x -151*x + 151*x**x -158*x**3/(x**2 + 1)**(3/2) + 316*x/sqrt(x**2 + 1) - 158*x**(x**2) 512*x**(x**2 + 2)*(x**2*log(x) + log(x) - 1)/(x**4 + 2*x**2 + 1)
332*(2*x - x**(x**2) - 2*x**(x**2 + 2) - x**(x**2 + 4))/(x**4 + 2*x**2 + 1)
240**x - exp(x*log(240))*log(240)
398**x - exp(x*log(398))*log(398)
692**x - exp(x*log(692))*log(692)

Table 17: SAGGA OOD – g(x)x. We show the problem x discovered by SAGGA, the simpliﬁed prediction from the neural sequence integrator, its derivative, and the derivative’s difference with x. The prediction is incorrect when the difference is not zero. For the ﬁrst cluster, we manually found a nearby problem that the model gets correct in bold; thus this cluster can be seen as a robustness failure. The second two clusters involve xx or xx2 which to our knowledge do not have analytical integrals (e.g. see https://www.wolframalpha.com/input/?i=integral+of+x**x); these clusters can be seen as exploits.

x

hyp raw

Correct Integral (Wolfram Alpha)

0 169*sin(4*x)/x

169*log(x**2)/8 - 169*cos(4*x)/4

169 Si(4x)

1 -2*sin(42/x)

cos(42/x)/21

−2(−42Ci(42/x) + x sin(42/x))

2 -2*sin(185*x**2)*cos(2*x)

4*sin(2*x)*sin(185*x**2)/3421 + 370*cos(2*x)*cos(185*x**2)/3421 -(Sqrt[Pi/370] (Cos[1/185] FresnelS[Sqrt[2/(185

Pi)] (-1 + 185 x)] + Cos[1/185]

FresnelS[Sqrt[2/(185 Pi)] (1 + 185 x)] -

(FresnelC[Sqrt[2/(185 Pi)] (-1 + 185 x)]

+ FresnelC[Sqrt[2/(185 Pi)] (1 + 185 x)])

Sin[1/185]))

3 357**(x**(2**x)) + 2*sin(2*x) exp(x*log(2)) - cos(2*x)

No result

4 1/(x**48*(3*x+2)**49)

add, mul, div, INT-, 1, 0, 9, 0, 9, 1, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 5, See

https://www.wolframalpha.com/input/?

INT+, 1, 0, 2, 4, ln, x, add, mul, div, INT+, 1, 0, 9, 0, 9, 1, 9, 9, 9, 9, 9, i=integral+of+1%2F%28x**48*%283*x%2B2%29**49%29

9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, INT+, 1, 0, 2, 4, ln, add, div, INT+,

2, INT+, 3, x, mul, INT-, 1, mul, pow, add, mul, INT+, 1, 0, 0, 9, 6, 2,

9, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,

0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,

0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,

0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0

Table 18: SAGGA OOD – exploits. We show the problem x, and the neural sequence integrator’s prediction as either raw preﬁx tokens or (when possible) in its inﬁx form. All problems are failures discovered by SAGGA.

