arXiv:1906.00731v1 [math.OC] 30 May 2019

Convergence Analysis of Gradient-Based Learning with Non-Uniform Learning Rates in Non-Cooperative Multi-Agent Settings

Benjamin Chasnov Department of Electrical and Computer Engineering University of Washington

BCHASNOV@UW.EDU

Lillian J. Ratliff Department of Electrical and Computer Engineering University of Washington

RATLIFFL@UW.EDU

Eric Mazumdar Department of Electrical Engineering and Computer Sciences University of California, Berkeley

MAZUMDAR@EECS.BERKELEY.EDU

Samuel A. Burden

SBURDEN@UW.EDU

Department of Electrical and Computer Engineering

University of Washington

Abstract

Considering a class of gradient-based multi-agent learning algorithms in non-cooperative settings, we provide local convergence guarantees to a neighborhood of a stable local Nash equilibrium. In particular, we consider continuous games where agents learn in (i) deterministic settings with oracle access to their gradient and (ii) stochastic settings with an unbiased estimator of their gradient. Utilizing the minimum and maximum singular values of the game Jacobian, we provide ﬁnite-time convergence guarantees in the deterministic case. On the other hand, in the stochastic case, we provide concentration bounds guaranteeing that with high probability agents will converge to a neighborhood of a stable local Nash equilibrium in ﬁnite time. Different than other works in this vein, we also study the effects of non-uniform learning rates on the learning dynamics and convergence rates. We ﬁnd that much like preconditioning in optimization, non-uniform learning rates cause a distortion in the vector ﬁeld which can, in turn, change the rate of convergence and the shape of the region of attraction. The analysis is supported by numerical examples that illustrate different aspects of the theory. We conclude with discussion of the results and open questions.

1. Introduction
The characterization and computation of equilibria such as Nash equilibria and its reﬁnements constitutes a signiﬁcant focus in non-cooperative game theory. Several natural questions arises including “how do players ﬁnd such equilibria?” and “how should the learning process be interpreted?” With these questions in mind, a variety of ﬁelds have focused their attention on the problem of learning in games. This has, in turn, lead to a plethora of learning algorithms including gradient play, ﬁctitious play, best response, and multi-agent reinforcement learning among others [13].
From an applications point of view, a more recent trend is in the adoption of game theoretic models of algorithm interaction in machine learning applications. For instance, game theoretic tools are being used to improve the robustness and generalizability of machine learning algorithms; e.g., generative adversarial networks have become a popular topic of study demanding the use of game theoretic ideas to provide performance guarantees [12]. In other work from the learning community, game theoretic concepts are being leveraged to analyze the interaction of learning agents—see, e.g., [3, 15, 21, 23, 33]. Even more recently, convergence analysis to Nash equilibria has been called into question [27]; in its place is a proposal to consider game dynamics as the meaning of the game. This is an interesting perspective as it is well known that in general learning dynamics do not obtain an Nash equilibrium even asymptotically—see, e.g., [14]—and,

1

CHASNOV, RATLIFF, MAZUMDAR, AND BURDEN
perhaps more interestingly, many learning dynamics exhibit very interesting limiting behaviors including periodic orbits and chaos—see, e.g., [6, 7, 16, 17].
Despite this activity, we still lack a complete understanding of the dynamics and limiting behaviors of coupled, competing learning algorithms. One may imagine that the myriad results on convergence of gradient descent in optimization readily extend to the game setting. Yet, they do not since gradient-based learning schemes in games do not correspond to gradient ﬂows, a class of ﬂows that are guaranteed to converge to local minimizers almost surely. In particular, the gradient-based learning dynamics for competitive, multiagent settings have a non-symmetric Jacobian and as a consequence their dynamics may admit complex eigenvalues and non-equilibrium limiting behavior such as periodic orbits. In short, this fact makes it difﬁcult to extend many of the optimization approaches to convergence in single-agent optimization settings to multi-agent settings primarily due to the fact that steps in the direction of individual gradients of players’ costs do not guarantee that each agents cost decreases. In fact, in games, as our examples highlight, a player’s cost can increase when they follow the gradient of their own cost. Counterintuitively, agents can also converge to local maxima of their own costs despite descending their own gradient. These behaviors are due to the coupling between the agents.
Some of the questions that remain unaddressed and to which we provide partial answers include the derivation of error bounds and convergence rates. These are important for ensuring performance guarantees on the collective behavior and can help provide guarantees on subsequent control or incentive policy synthesis. We also investigate the question of how naturally arising features of the learning process for autonomous agents, such as their learning rates, impact the learning path and limiting behavior. This further exposes interesting questions about the overall quality of the limiting behavior and the cost accumulated along the learning path—e.g., is it better to be a slow or fast learner both in terms of the cost of learning and the learned behavior?
Contributions. We study convergence of a broad class of gradient-based multi-agent learning algorithms in non-cooperative settings by leveraging the framework of n-player continuous games along with tools from numerical optimization and dynamical systems theory. We consider a class of learning algorithms
x+i = xi − γigi(xi, x−i)
where xi is the choice variable or action of player i, γi is its learning rate, and gi is derived from the gradient of a function that abstractly represents the cost of player i. The key feature of non-cooperative settings is coupling of an agent’s cost through all other agents’ choice variables x−i.
We consider two settings: (i) agents have oracle access to gi and (ii) agents have an unbiased estimator for gi. The class of gradient-based learning algorithms we study encompases a wide variety of approaches to learning in games including multi-agent policy gradient, gradient-based approaches to adversarial learning, and multi-agent gradient-based online optimization. For both the deterministic (oracle gradient access) and the stochastic (unbiased estimators) settings, we provide convergence results for both uniform learning rates—i.e., where γi = γ for each player i ∈ {1, . . . , n}—and for non-uniform learning rates. The latter of which arises more naturally in the study of the limiting behavior of autonomous learning agents.
In the deterministic setting, we derive asymptotic and ﬁnite-time convergence rates for the coupled learning processes to a reﬁnement of local Nash equilibria known as differential Nash equilibria [28] (a class of equilibria that are generic amongst local Nash equilibria). In the stochastic setting, leveraging the results of stochastic approximation and dynamical systems, we derive asymptotic convergence guarantees to stable local Nash equilibria as well as high-probability, ﬁnite-time guarantees for convergence to a neighborhood of a Nash equilibrium. The analytical results are supported by several illustrative numerical examples. We also provide discussion on the effect of non-uniform learning rates on the learning path—that is, different learning rates warp the vector ﬁeld dynamics. Coordinate based learning rates are typically leveraged in gradient-based optimization schemes to speed up convergence or avoid poor quality local minima. In games,
2

however, the interpretation is slightly different since each of the coordinates of the dynamics corresponds to minimizing a different cost function along the respective coordinate axis. The resultant effect is a distortion of the vector ﬁeld in such a way that it has the effect of leading the joint action to a point which has a lower value for the slower player relative to the ﬂow of the dynamics given a uniform learning rate and the same initialization. In this sense, it seems that the answer to the question posed above is that it is most beneﬁcial for an agent to have the slower learning rate.
Organization. The remainder of the paper is organized as follows. We start with mathematical and gametheoretic preliminaries in Section 2 which is followed by the main convergence results for the deterministic setting (Section 3) and the stochastic setting (Section 4). Within each of the latter two sections, we present convergence results for both the case where agents have uniform and non-uniform learning rates. In Section 5, we present several numerical examples which help to illustrate the theoretical results and also highlight some directions for future inquiry. Finally, we conclude with discussion and future work in Section 6.

2. Preliminaries

Consider a setting in which at iteration k, each agent i ∈ I = {1, . . . , n} updates their choice variable xi ∈ Xi = Rdi by the process

xi,k+1 = xi,k − γi,kgi(xi,k, x−i,k).

(1)

where γi is agent i’s learning rate, x−i = (xj)j∈I/{i} ∈ j∈I/{i} Xj denotes the choices of all agents excluding the i-th agent, and (xi, x−i) ∈ X = i∈I Xi. Within the above setting, the class of learning algorithms we consider is such that for each i ∈ I, there exists a sufﬁciently smooth function fi ∈ Cq(X, R), q ≥ 2 such that gi is either Difi, where Di(·) denotes the derivative with respect to xi, or an unbiased estimator of Difi—i.e., gi ≡ Difi where E[Difi] = Difi.
The collection of costs (f1, . . . , fn) on X = X1 × · · · × Xn where fi : X → R is agent i’s cost function and Xi = Rdi is their action space deﬁnes a continuous game. In this continuous game abstraction, each player i ∈ I aims to selection an action xi ∈ Xi that minimizes their cost fi(xi, x−i) given the actions of all other agents, x−i ∈ X−i. That is, players myopically update their actions by following the gradient of their cost with respect to their own choice variable. For a symmetric matrix A ∈ Rd×d, let λd(A) ≤ · · · ≤ λ1(A) be its eigenvalues. For a matrix A ∈ Rd×d, let spec(A) = {λj(A)} be the spectrum of A.
Assumption 1. For each i ∈ I, fi ∈ Cr(X, R) for r ≥ 2 and ω(x) ≡ (D1f1(x) · · · Dnfn(x)) is L–Lipschitz.
Let Di2fi denote the second partial derivative of fi with respect to xi and Djifi denote the partial derivative of Difi with respect to xj. The game Jacobian—i.e., the Jacobian of ω—is given by
 D12f1(x) · · · D1nf1(x) J(x) =  ... . . . ...  .
Dn1fn(x) · · · Dn2 fn(x)
The entries of the above matrix are dependent on x, however, we drop this dependence where obvious. Note that each Di2fi is symmetric under Assumption 1, yet J is not. This is an important point and causes the subsequent analysis to deviate from the typical analysis of (stochastic) gradient descent.
The most common characterization of limiting behavior in games is that of a Nash equilibrium. The following deﬁnitions are useful for our analysis.

Deﬁnition 1. A strategy x ∈ X is a local Nash equilibrium for the game (f1, . . . , fn) if for each i ∈ I there
exists an open set Wi ⊂ Xi such that xi ∈ Wi and fi(xi, x−i) ≤ fi(xi, x−i) for all xi ∈ Wi. If the above inequalities are strict, x is a strict local Nash equilibrium.

3

CHASNOV, RATLIFF, MAZUMDAR, AND BURDEN

Deﬁnition 2. A point x ∈ X is said to be a critical point for the game if ω(x) = 0.
We denote the set of critical points as C = {x ∈ X| ω(x) = 0}. Analogous to single-player optimization settings, for each player, viewing all other players’ actions as ﬁxed, there are necessary and sufﬁcient conditions which characterize local optimality.
Proposition 1 ([28]). If x is a local Nash equilibrium of the game (f1, . . . , fn), then ω(x) = 0 and Di2fi(x) ≥ 0. On the other hand, if ω(x) = 0 and Di2fi(x) > 0, then x ∈ X is a local Nash equilibrium.
The sufﬁcient conditions in the above result give rise to the following deﬁnition of a differential Nash equilibrium.
Deﬁnition 3 ([28]). A strategy x ∈ X is a differential Nash equilibrium if ω(x) = 0 and Di2fi(x) > 0 for each i ∈ I.
Differential Nash need not be isolated. However, if J(x) is non-degenerate—meaning that det J(x) = 0—for a differential Nash x, then x is an isolated strict local Nash equilibrium. Non-degenerate differential Nash are generic amongst local Nash equilibria and they are structurally stable [29] which ensures they persist under small perturbations. This result also implies an asymptotic convergence result: if the spectrum of J is strictly in the right-half plane (i.e. spec(J(x)) ⊂ C◦+), then a differential Nash equilibrium x is (exponentially) attracting under the ﬂow of −ω [28, Proposition 2]. We say such equilibria are stable.

3. Deterministic Setting

The multi-agent learning framework we analyze is such that each agent’s rule for updating their choice

variable consists of the agent modifying their action xi in the direction of their individual gradient Difi. Let

us ﬁrst consider the setting in which each agent i has oracle access to gi. The learning dynamics are given

by

xk+1 = xk − Γω(xk)

(2)

where Γ = blockdiag(γ1Id1, . . . , γnIdn) with Idi denoting the di × di identity matrix. Within this setting we consider both the cases where the agents have a constant uniform learning rate—i.e., γi ≡ γ—and where
their learning rates are non-uniform, but constant—i.e., γi is not necessarily equal to γj for any i, j ∈ I,
j = i. Let S(x) = 21 (J(x) + J(x)T ) be the symmetric part of J(x). Deﬁne

α = min λd S(x)T S(x)
x∈Br (x∗ )

and β = max λ1(J(x)T J(x))
x∈Br (x∗ )
where Br(x∗) is a r–radius ball around x∗. For a stable differential Nash x∗, let Br(x∗) be a ball of radius r > 0 around the equilibrium x∗ that is contained in the region of attraction V(x∗) for x∗1. Let Br0(x∗) with 0 < r0 < ∞ be the largest ball contained in the region of attraction of x∗.
1. Many techniques exists for approximating the region of attraction; e.g., given a Lyapunov function, its largest invariant level set can be used as an approximation [30]. Since spec(J(x∗)) ⊂ C+◦ , the converse Lyapunov theorem guarantees the existence of a local Lyapunov function.

4

3.1 Uniform Learning Rates
With γi = γ for each i ∈ I, the learning rule (2) can be thought of as a discretized numerical scheme approximating the continuous time dynamics
x˙ = −ω(x).
With a judicious choice of learning rate γ, (2) will converge (at an exponential rate) to a locally stable equilibrium of the dynamics.
Proposition 2. Consider an n–player continuous game (f1, . . . , fn) satisfying Assumption 1. Let x∗ ∈ X be a stable differential Nash equilibrium. Suppose agents use the gradient-based learning rule xk+1 = xk − γω(xk) with learning rates 0 < γ < γ˜ where γ˜ is the smallest positive h such that maxj |1−hλj(J(x∗))| = 1. Then, for x0 ∈ Br(x∗) ⊂ V(x∗), xk → x∗ exponentially.
The above result provides a range for the possible learning rates for which (2) converges to a stable differential Nash equilibrium x∗ of (f1, . . . , fn) assuming agents initialize in a ball contained in the region of attraction of x∗. Note that the usual assumption in gradient-based approaches to single-objective optimization problems (in which case J is symmetric) is that γ < 1/L, where objective being minimized is L-Lipschitz. This is sufﬁcient to guarantee convergence since the spectral radius of a matrix is always less than any operator norm which, in turn, ensures that |1 − γλj| < 1 for each λj ∈ spec(J(x∗)). If the game is a potential game—i.e., there exists a function φ such that Difi = Diφ for each i which occurs if and only if Dijfi = Djifj—then convergence analysis coincides with gradient descent so that any γ < 1/L where L is the Lipschitz constant of ω results in local asymptotic convergence.
The convergence guarantee in Proposition 2 is asymptotic in nature. It is often useful, from both an analysis and synthesis perspective, to have non-asymptotic or ﬁnite-time convergence results. Such results can be used to provide guarantees on decision-making processes wrapped around the coupled learning processes of the otherwise autonomous agents. The next result, provides a ﬁnite-time convergence guarantee for gradient-based learning where agents uniformly use a ﬁxed step size.
Let Br(x∗) be deﬁned as before with the added condition that it be deﬁned to be the largest ball in the region of attraction such that on Br(x∗) the symmetric part of J—i.e., S ≡ 21 (J + JT )—is positive deﬁnite.
Theorem 1. Consider a game (f1, . . . , fn) on X = X1 × · · · × Xn satisfying Assumption 1. Let x∗ ∈ X be a stable differential Nash equilibrium. Suppose x0 ∈ B√r(x∗) and that α < β. Then, given ε > 0, the gradient-based learning dynamics with learning rate γ = α/β obtains an ε–differential Nash such that xk ∈ Bε(x∗) ⊂ Br(x∗) for all
βr k ≥ 2 log .
αε

Before we proceed to the proof, let us remark on the assumption that α < β. First, α ≤ β is always true; indeed, suppressing the dependence on x,

λd(ST S) ≤ λ1(ST S) ≤ σmax(J )2 = λ1(J T J )

where σmax(·) denotes the largest singular value of its argument. Thus, the condition that α < β is generally true; for equality to hold, the symmetric part of J(x) would have repeated eigenvalues, which is not generic. Hence, we include this assumption in Theorem 1, but note that it is not restrictive and is fairly benign.

Proof of Theorem 1. First, note that xk+1 − x∗ = g˜(xk) − g˜(x∗) where g˜(x) = x − γω(x). Now, given x0 ∈ Br(x∗), by the mean value theorem,

g˜(x0) − g˜(x∗) =

1 0

Dg˜(τ

x0

+

(1

−

τ

)x∗)(x0

−

x∗)dτ

≤ supx∈Br(x∗) Dg˜(x)

x0 − x∗ .

5

CHASNOV, RATLIFF, MAZUMDAR, AND BURDEN

Hence, it sufﬁces to show that for the choice of γ, the eigenvalues of I − γJ(x) are in the unit circle. Indeed, since ω(x∗) = 0, we have that

xk+1 − x∗ 2 = xk − x∗ − γ(ω(xk) − ω(x∗)) 2 ≤ supx∈Br(x∗) I − γJ (x) 2 xk − x∗ 2

If supx∈Br(x∗) I −γJ(x) 2 is less than one, then the d√ynamics are contracting. For notational convenience, we drop the explicit dependence on x. Since λd(S) ≥ α on Br(x∗),

(I − γJ )T (I − γJ ) ≤ (1 − 2γλd(S) + γ2λ1(J T J ))I ≤ (1 − αβ )I √
where the last inequality holds for γ = α/β. Hence,

xk+1 − x∗ 2 ≤ supx∈Br(x∗) I − γJ (x) 2 xk − x∗ 2 ≤ (1 − αβ )1/2 xk − x∗ 2.

Since α < β, we have that (1 − α/β) < exp(−α/β) so that

xT − x∗ 2 ≤ exp(−T α/(2β)) x0 − x∗ 2.

This, in turn, implies that xk ∈ Bε(x∗) for all k ≥ T = 2 αβ log(r/ε) .

Note that γ

=

√ α/β

is selected to minimize 1 − 2γλ1(S) + γ2λd(JT J).

Hence, this is the fastest

learning rate given the worst case eigenstructure of J over the ball Br(x∗) for the choice of operator norm

· 2. We note, however, that faster convergence is possible as indicated by Proposition 2 and observed

in the examples in Section 5. Indeed, we note that the spectral radius ρ(·) of a matrix is always less than

its maximum singular value—i.e. ρ(I − γJ) ≤ I − γJ 2—so it is possible to contract at a faster rate.

We remark that if J was symmetric (i.e., in the case of a potential game [24] or a single-agent optimization

problem), then ρ(I − γJ) = I − γJ 2. In games, however, J is not symmetric.

3.2 Non-Uniform Learning Rates
Let us now consider the case when agents have their own individual learning rate γi, yet still have oracle access to their individual gradients. This is, of course, more natural in the study of autonomous learning agents as opposed to efforts for computing Nash equilibria for a given game.
Proposition 3. Consider an n–player game (f1, . . . , fn) satisfying Assumption 1. Let x∗ ∈ X be a stable differential Nash equilibrium. Suppose agents use the gradient-based learning rule xk+1 = xk − Γω(xk) with learning rates γi such that ρ(I − ΓJ(x)) < 1 for all x ∈ V(x∗). Then, for x0 ∈ V(x∗), xk → x∗ exponentially.
The proof is a direct application of Ostrowski’s theorem [26]. We provide a simple proof via Lyapunov argument for posterity.
Mazumdar and Ratliff [21] show that (2) will almost surely avoid strict saddle points of the dynamics, some of which are Nash equilibria in non-zero sum games. Note that the set of critical points C contains more than just the local Nash equilibria. Hence, except on a set of measure zero, (2) will converge to a stable attractor of x˙ = −ω(x) which includes stable limit cycles and stable local non-Nash critical points.
Letting g˜(x) = x − Γω(x), since ω ∈ Cq for some q ≥ 1, g˜ ∈ Cq, the expansion
g˜(x) = g˜(x∗) + (I − ΓJ(x))(x − x∗) + R(x − x∗)
holds, where R satisﬁes limx→x∗ R(x − x∗) / x − x∗ = 0 so that given c > 0, there exists an r > 0 such that R(x − x∗) ≤ c x − x∗ for all x ∈ Br(x∗).

6

Proposition 4. Suppose that I − ΓJ(x) < 1 for all x ∈ Br0(x∗) ⊂ V(x∗) so that there exists r , r such that I − ΓJ(x) ≤ r < r < 1 for all x ∈ Br0(x∗). For 1 − r > 0, let 0 < r < ∞ be the largest r such that R(x − x∗) ≤ (1 − r ) x − x∗ for all x ∈ Br(x∗). Furthermore, let x0 ∈ Br∗(x∗), where
r∗ = min{r, r0}, be arbitrary. Then, given ε > 0, gradient-based learning with learning rates Γ obtains an ε–differential Nash equilibrium in ﬁnite time—i.e., xk ∈ Bε(x∗) for all k ≥ T = 1δ log (r∗/ε) where
δ=r −r.

The proof follows the proof of Theorem 1 in [2] with a few minor modiﬁcations; we provide it in Appendix A.1 for completeness.

Remark 1. We note that the proposition can be more generally stated with the assumption that ρ(I − ΓJ(x)) < 1, in which case there exists some δ deﬁned in terms of bounds on powers of I − ΓJ. We provide the proof of this in Appendix A.1. We also note that these results hold even if Γ is not a diagonal matrix as we have assumed as long as ρ(I − ΓJ(x)) < 1.

A perhaps more interpretable ﬁnite bound stated in terms of√the game structure can also be obtained. Consider the case in which players adopt learning rates γi = α/(βki) with ki ≥ 1. Given a stable differential Nash equilibrium x∗, let Br(x∗) be the largest ball of radius r contained in the region of attraction on which S˜ ≡ 21 (J˜T + J˜) is positive deﬁnite where ω˜ = (Difi/ki)i∈I so that J˜ ≡ Dω˜, and deﬁne
α˜ = minx∈Br(x∗) λd S˜(x)T S˜(x)

and β˜ = maxx∈Br(x∗) λ1(J˜(x)T J˜(x)).
Given a stable differential Nash equilibrium x∗, let Br(x√∗) be the largest ball contained in the region of attraction V(x∗) on which ST S is positive deﬁnite—i.e., α > 0.

Theorem 2. Suppose that Assu√mption 1 hol√ds and that x∗ ∈ X is a s√table differential Nash equilibrium. Let x0 ∈ Br(x∗), α < kminβ, α/kmin ≤ α˜, and for each i, γi = α/(βki) with ki ≥ 1. Then, given

ε > 0, the gradient-based learning dynamics with learning rates γi obtain an ε–differential Nash such that

xk ∈ Bε(x∗) for all

k ≥ 2 βkmin log r .

α

ε

Proof. First, note that xk+1 − x∗ = g˜(xk) − g˜(x∗) where g˜(x) = x − Γω(x). Now, given x0 ∈ Br(x∗), by the mean value theorem,

g˜(x0) − g˜(x∗) =

1 0

Dg˜(τ

x0

+

(1

−

τ

)x∗)(x0

−

x∗)dτ

≤ supx∈Br(x∗) Dg˜(x)

x0 − x∗ .

Hence, it sufﬁces to show that for the choice of Γ, the eigenvalues of I − ΓJ(x) live in the unit circle. Then an inductive argument can be made with the inductive hypothesis that xk ∈ Br(x∗). Let
Λ = diag (1/k1, . . . , 1/kn). Then we need to show that I − γΛJ has eigenvalues in the unit circle. Since ω(x∗) = 0, we have that

xk+1 − x∗ 2 = xk − x∗ − γΛ(ω(xk) − ω(x∗)) 2 ≤ supx∈Br(x∗) I − γΛJ (x) 2 xk − x∗ 2.

If supx∈Br(x∗) I − γΛJ(x) 2 is less than one, where the norm is the operator 2–norm, then the dynamics are contracting. For notational convenience, we drop the explicit dependence on x. Then,
(I − γΛJ )T (I − γΛJ ) ≤ (1 − 2γλd(S˜) + γ2λk12(JT J) )I ≤ (1 − 2γ√α/kmin + α/(βkmin))I min = (1 − α/(βkmin))I.

7

CHASNOV, RATLIFF, MAZUMDAR, AND BURDEN

The ﬁrst inequality holds since λ1(JT J/km2 in) ≥ λ1(JT Λ2J). Indeed, ﬁrst observe that the singular values of ΛJT JΛ are the same as those of JT Λ2J since the latter is positive deﬁnite symmetric. Thus, by noting

that

A 2 = σmax(A) and employing Cauchy-Schwartz, we get that

Λ

2 2

JTJ

2

≥

ΛJ T J Λ 2 and

hence, the inequality.

Using the above to bound supx∈Br(x∗) I − γΛJ (x) 2, we have xk+1 − x∗ 2 ≤ (1 − βkαmin )1/2 xk −

x∗ 2. Since α < kminβ, (1 − α/(βkmin)) < e−α/(βkmin) so that xk+1 − x∗ 2 ≤ e−T α/(2kminβ) x0 − x∗ 2. This, in turn, implies that xk ∈ Bε(x∗) for all k ≥ T = 2 βkαmin log(r/ε) .

Multiple learning rates lead to a scaling rows which can have a signiﬁcant effect on the eigenstructure of the matrix, thereby making the relationship between ΓJ and J difﬁcult to reason about. None-the-less, there are numerous approaches to solving nonlinear systems of equations (or differential equations expressed as a set of nonlinear system of equations) that employ preconditioning (i.e., coordinate scaling). The purpose of using a preconditioning matrix is to rescale the problem and achieve better or faster convergence. Many of these results directly translate to convergence guarantees for learning in games when the learning rates are not uniform; however, in the case of understanding convergence properties for autonomous agents learning an equilibrium—as opposed to computing an equilibrium—the preconditioner is not subject to design. Perhaps this reveals an interesting direction of future research in terms of synthesizing games or learning rules via incentivization or otherwise exogenous control policies for either coordinating agents or improving the learning process—e.g., using incentives to induce a particular equilibrium while also encouraging faster learning.

4. Stochastic Setting
In this section, we consider gradient-based learning rules for each agent where the agent does not have oracle access to their individual gradients, but rather has an unbiased estimator in its place. In particular, for each player i ∈ I, consider the noisy gradient-based learning rule given by

xi,k+1 = xi,k − γi,k(ω(xk) + wi,k+1)

(3)

where γi,k is the learning rate and wi,k is an independent identically distributed stochastic process. In order to prove a high-probability, ﬁnite sample convergence rate, we can leverage recent results for convergence of nonlinear stochastic approximation algorithms. The key is in formulating the the learning rule for the agents and in leveraging the notion of a stable differential Nash equilibrium which has analogous properties as a locally stable equilibrium for a nonlinear dynamical system. Making the link between the discrete time learning update and the limiting continuous time differential equation and its equilibria allows us to draw on rich existing convergence analysis tools.
In the ﬁrst part of this section, we provide convergence rate results for the case where the agents use a uniform learning rate—i.e. γi,k ≡ γk. In the second part of this section, we extend these results to the case where agents use non-uniform learning rates—that is, each agent has its own learning rate γi,k—by incorporating some additional assumptions and leveraging two-timescale analysis techniques from dynamical systems theory.
We require some modiﬁed assumptions in this section on the learning process structure.

Assumption 2. The gradient-based learning rule (3) satisﬁes the following: A2a. Given the ﬁltration Fk = σ(xs, w1,s, w2,s, s ≤ k), {wi,k+1}i∈I are conditionally independent. Moreovoer, for each i ∈ I, E[wi,k+1| Fk] = 0 almost surely (a.s.), and E[ wi,k+1 | Fk] ≤ ci(1 + xk ) a.s. for some constants ci ≥ 0.
A2b. For each i ∈ I, the stepsize sequence {γi,k}k contain positive scalars such that

8

(a) i k γi2,k < ∞; (b) k γi,k = ∞; (c) and, γ2,k = o(γ1,k). A2c. Each fi ∈ Cq(Rd, R) for some q ≥ 3 and each fi and ω are Li– and Lω–Lipschitz, respectively.

4.1 Uniform Learning Rates

Before concluding, we specialize to the case in which agents have the same learning rate sequence γi,k = γk for each i ∈ I.

Theorem 3. Suppose that x∗ is a stable differential Nash equilibrium of the game (f1, . . . , fn) and that Assumption 2 holds (excluding A2b.iii). For each k, let k0 ≥ 0 and

ζk = maxk0≤s≤k−1 exp(−λ

k−1 =s+1

γ

γs.

Fix any ε > 0 such that Bε(x∗) ⊂ Br(x∗) ⊂ V where V is the region of attraction of x∗. There exists constants C1, C2 > 0 and functions h1(ε) = O(log(1/ε)) and h2(ε) = O(1/ε) so that whenever T ≥ h1(ε) and k0 ≥ N , where N is such that 1/γk ≥ h2(ε) for all k ≥ N , the samples generated by the gradientbased learning rule satisfy

Pr (x¯(t) ∈ Bε(x∗) ∀t ≥ tk0 + T + 1| x¯(tk0) ∈ Br(x∗)) ≥ 1 − ∞ s=k0 C1 exp(−C2ε1/2/γs1/2) + C1 exp(−C2 min{ε, ε2}/ζs)

where the constants depend only on parameters λ, r, τL and the dimension d = i di. Then stochastic gradient-based learning in games obtains an ε–stable differential Nash x∗ in ﬁnite time with high probability.
The above theorem implies that xk ∈ Bε(x∗) for all k ≥ k0 + log(4K˜ /ε)λ−1 +1 with high probability for some constant K˜ that depends only on λ, r, τL, and d.
Proof. Since x∗ is a stable differential Nash equilibrium, J(x∗) is positive deﬁnite and Di2fi(x∗) is positive deﬁnite for each i ∈ I. Thus x∗ is a locally asymptotically stable hyperbolic equilibrium point of x˙ = −ω(x). Hence, the assumptions of Theorem 1.1 [32] are satisﬁed so that we can invoke the result which gives us the high probability bound for stochastic gradient-based learning in games.

The above theorem has a direct corollary specializing to the case where the gradient-based learning rule with uniform stepsizes is initialized inside a ball of radius r constained in the region of attraction—i.e., Br(x∗) ⊂ V.
Corollary 1. Let x∗ be a stable differential Nash equilibrium of (f1, . . . , fn) and suppose that Assumption 2 holds (excluding A2b.iii). Fix any ε > 0 such that Bε(x∗) ⊂ Br(x∗) ⊂ V. Let ζk, T , and h2(ε) be deﬁned as in Theorem 3. Suppose that 1/γk ≥ h2(ε) for all k ≥ 0 and that x0 ∈ Br(x∗). Then, with C1, C2 > 0 as in Theorem 3,

Pr (x¯(t) ∈ Bε(x∗) ∀t ≥ T + 1| x¯(tk0) ∈ Br(x∗))

≥1−

∞ s=0

C1 exp(−C2ε1/2/γs1/2) + C1 exp(−C2 min{ε, ε2}/ζs) .

9

CHASNOV, RATLIFF, MAZUMDAR, AND BURDEN

4.2 Non-Uniform Learning Rates
Consider now that agents have their own learning rates γi,k for each i ∈ I. In environments with several autonomous agents, as compared to the objective of computing Nash equilibria in a game, it is perhaps more reasonable to consider the scenario in which the agents have their own individual learning rate. For the sake of brevity, we show the convergence result in detail for the two agent case—that is, where I = {1, 2}. We note that the extension to n agents is straightforward. The proof leverages recent results from the theory of stochastic approximation presented in [9] and we note that our objective here is to show that they apply to games and provide commentary on the interpretation of the results in this context.
The gradient-based learning rules are given by

xi,k+1 = xi,k − γi,k(ω(xk) + wi,k+1)

(4)

so that with γ2,k = o(γ1,k), in the limit τ → 0, the above system can be thought of as approximating the singularly perturbed system

x˙ 1(t) = − D1f1(x1(t), x2(t))

(5)

x˙ 2(t)

τ D2f2(x1(t), x2(t))

Indeed, since limk→∞ γ2,k/γ1,k → 0—i.e., γ2,k → 0 at a faster rate than γ1,k—updates to x1 appear to be equilibriated for the current quasi-static x2 as the dynamics in (5) suggest.

4.2.1 ASYMPTOTIC CONVERGENCE IN THE NON-UNIFORM LEARNING RATE SETTING
Assumption 3. For ﬁxed x2 ∈ X2, the system x˙ 1(t) = −D1f1(x1(t), x2) has a globally asymptotically stable equilibrium λ(x2).

Lemma 1. Under Assumptions 2 and 3, conditioned on the event {supk {(λ(x2), x2)| x2 ∈ Rd2} almost surely.

i xi,k 2 < ∞}, (x1,k, x2,k) →

The above lemma follows from classical analysis (see, e.g., Borkar [10, Chapter 6] or Bhatnagar and

Prasad [8, Chapter 3]).

Deﬁne the continuous time accumulated after k samples of x2 to be tk =

k−1 l=0

γ2,k

and

deﬁne

x2(t, s, xs) for t ≥ s to be the trajectory of x˙ 2 = −D2f2(λ(x2), x2). Furthermore, deﬁne the event

E = {supk i xi,k 2 < ∞}.

Theorem 4. Suppose that Assumptions 2 and 3 hold. For any K > 0, conditioned on E,

limk→∞ sup0≤h≤K x2,k+h − x2(tk+h, tk, xk) 2 = 0.

Proof. The proof invokes Lemma 1 above and Proposition 4.1 and 4.2 of [5]. Indeed, by Lemma 1, (λ(x2,k) − x2,k) → 0 almost surely. Hence, we can study the sample path generated by
x2,k+1 = x2,k − γ2,k(D2f2(λ(x2,k), x2,k) + w2,k+1).
Since D2f2 ∈ Cq−1 for some q ≥ 3, it is locally Lipschitz and, on the event {supk i xi,k 2 < ∞}, it is bounded. It thus induces a continuous globally integrable vector ﬁeld, and therefore satisﬁes the assumptions of Proposition 4.1 of [5]. Moreover, under Assumption 2, the assumptions of Proposition 4.2 of [5] are satisﬁed. Hence, invoking said propositions, we get the desired result.

10

This result essentially says that the slow player’s sample path asymptotically tracks the ﬂow of

x˙ 2 = −D2f2(λ(x2), x2).

If we additionally assume that the slow component also has a global attractor, then the above theorem gives rise to a stronger convergence result.

Assumption 4. Given λ(·) as in Assumption 3, the system x˙ 2(t) = −τ D2f2(λ(x2(t)), x2(t)) has a globally asymptotically stable equilibrium x∗2.

Corollary 2. Under the assumptions of Theorem 4 and Assumption 4, conditioned on the event E, gradient-

based

learning

converges

almost

surely

to

a

stable

attractor

(x

∗ 1

,

x∗2

),

where

x∗1

=

λ(x∗2),

the

set

of

which

contains the stable differential Nash equilibria.

More generally, the process (x1,k, x2,k) will converge almost surely to the internally chain transitive set of the limiting dynamics (5) and this set contains the stable Nash equilibria. If the only internally chain transitive sets for (5) are isolated equilibria (this occurs, e.g., if the game is a potential game), then xk converges almost surely to a stationary point of the dynamics, a subset of which are stable local Nash equilibria.
It is also worth commenting on what types of games will satisfy these assumptions. To satisfy Assumption 3, it is sufﬁcient for the fastest player’s cost function to be convex in their choice variable.

Proposition 5. Suppose Assumption 2 and 4 hold and that f1(·, x2) is convex. Conditioned on the event E, the sample points of gradient-based learning satisfy (x1,k, x2,k) → {(λ(x2), x2)| x2 ∈ Rd2} almost surely. Moreover, (x1,k, x2,k) → (x∗1, x∗2) almost surely, where x∗1 = λ(x∗2).

Note

that

(

x

∗ 1

,

x∗2

)

could

still

be

a

spurious

stable

non-Nash

point

still

since

the

above

implies

that

D(D2f2(λ(·), ·))|x∗2 > 0, which does not necessarily imply that D22f2(λ(x∗2), x∗2) > 0.

Remark 2 (Relaxation to Local Asymptotic Stability.). Under relaxed assumptions on global asymptotic stability, we can obtain high-probability results on convergence to locally asymptotically stable attractors. If it is assumed that x0 is in the region of attraction for a locally asymptotically stable attractor, then the above results can be stated with only the assumption of a locally asymptotic stability. However, this is difﬁcult to ensure in practice. To relax the result to a local guarantee regardless of the initialization requires conditioning on an unveriﬁable event—i.e., the high-probability bound in this case is conditioned on the event {{x1,k} belongs to a compact set B, which depends on the sample point, of ∩x2R(λ(x2))} where R(λ(x2)) is the region of attraction of λ(x2). None-the-less, it is possible to leverage results from stochastic approximation [18], [10, Chapter 2] to prove local versions of the results for non-uniform learning rates. Further investigation is required to provide concentration bounds for not only games but stochastic approximation in general.

4.2.2 HIGH-PROBABILITY, FINITE-SAMPLE GUARANTEES WITH NON-UNIFORM LEARNING RATES

In the stochastic setting, the learning dynamics are stochastic approximation updates, and non-uniform

learning rates lead to a multi-timescale setting. The results leverage recent theoretical guarantees for two-

timescale analysis of stochastic approximation such as [9].

For a stable differential Nash equilibrium x∗ = (λ(x∗2), x∗2), using the bounds in Lemma 2 and Lemma 3

in Appendix A.2, we can provide a high-probability guarantee that (x1,k, x2,k) gets locked in to a ball around

(λ

(x

∗ 2

)

,

x∗2

)

.

Let x¯i(·) denote the linear interpolates between sample points xi,k and, as in the preceding sub-section,

let xi(·, ti,k, xk) denote the continuous time ﬂow of x˙ i with initial data (ti,k, xk) where ti,k =

k−1 l=0

γi,k

.

11

CHASNOV, RATLIFF, MAZUMDAR, AND BURDEN

Alekseev’s formula is a nonlinear variation of constants formula that provides solutions to perturbations of differential equations using a local linear approximation. We can apply it to the asymptotic pseudotrajectories x¯i(·) in each timescale. For these local approximations, linear systems theory lets us ﬁnd growth rate bounds for the perturbations, which can, in turn, be used to bound the normed difference between the
continuous time ﬂow and the asymptotic pseudo-trajectories. More detail is provided in Appendix A.2. Towards this end, ﬁx ε ∈ [0, 1) and let N be such that γ1,k ≤ ε/(8K) and τk ≤ ε/(8K) for all k ≥ N .
Deﬁne time sequences t1,k = t˜k and t2,k = tˆk which keep track of the time accumulated up to iteration k on each of the timescales. Let k0 ≥ N and, with K as in Lemma 2 (Appendix A.2), let T be such that

e−κ1(t˜k−t˜k0 )Hk0 ≤ ε/(8K)

for all k ≥ k0 + T where κ1 > 0 is a constant derived from Alekseev’s formula applied to x¯1(·). Analogously, with K¯ as in Lemma 3 (Appendix A.2), let

e−κ2(tˆk−tˆk0 )( x¯2(tˆk0 ) − x2(tˆk0 ) ≤ ε/(8K¯ ),

for all k ≥ k0 + T where κ2 > 0 is a constant derived from Alekseev’s formula applied to x¯2(·). Deﬁne constants

βk = maxk0≤s≤k−1 exp(−κ1(

k−1 i=s+1

γ1,i))γ1,s,

ηk

=

maxk0≤s≤k−1

exp(−κ2(

k−1 i=s+1

γ2,i))γ2,s

,

and τk = γ2,k/γ1,k.

Theorem 5. Suppose that Assumptions 2–4 hold and let γ2,k = o(γ1,k). Given a stable differential Nash

equilibrium x∗ = (λ(x∗2), x∗2), player 2’s sample path (generated by (4) with i = 1) will asymptotically track

zk = λ(x2,k). Moreover, given ε ∈ [0, 1), xk will get ‘locked in’ to a ε–neighborhood with high probability conditioned on reaching Br0(x∗) by iteration k0. That is, letting k¯ = k0 + T + 1, for some C1, C2 > 0,

P( x1,k − zk

≤ ε, ∀k ≥ k¯|x1,k0 , zk0 ∈ Br0 ) ≥1 − −

∞ C1 exp − C2√ε/√γ1,k

k=k0

√√

∞ k=k0 C2 exp − C2 ε/ τk

− ∞ k=k0 C1 exp − C2ε2/βk . (6)

Moreover, for some constants C˜1, C˜2 > 0,

P( x2,k − x2(tˆk)

≤ ε, ∀k ≥ k¯|xk0 , zk0 ∈ Br0 (x∗)) ≥1 + −

∞ k=k0 C˜1 exp − C˜2√ε/√γ1,k ∞ k=k0 C˜1 exp − C˜2√ε/√τk

− ∞ k=k0 C˜1 exp − C˜2ε2/βk

− ∞ k=k0 C˜1 exp − C˜2ε2/ηk . (7)

Corollary 3. Fix ε ∈ [0, 1) and suppose that γ1,n ≤ ε/(8K) for all n ≥ 0. With K as in Lemma 2 (Appendix A.2), let T be such that e−κ1(t˜n−t˜0)H0 ≤ ε/(8K) for all n ≥ T . Furthermore, with K¯ as in Lemma 3 (Appendix A.2), let e−κ2(tˆn−tˆ0)( x¯2(tˆ0) − x2(tˆ0) ≤ ε/(8K¯ ), ∀n ≥ T . Under the assumptions of Theorem 5, xk will will get ‘locked in’ to a ε–neighborhood with high probability conditioned on x0 ∈ Br0(x∗) where the high-probability bounds in (6) holds with k0 = 0.
Remark 3 (Relaxation to Locally Asymptotically Stable Attractors.). In fact, Corollary 3 holds under a relaxed assumption on the stability of x∗. Indeed, if x∗ is locally asymptotically stable and x0 ∈ Br0(x∗) ⊂ R(x∗) where R(x∗) is the region of attraction for x∗, then the high probability bound from Corollary 3 holds.
12

KK11

KK22

KK33

DDiissttaannccee ttoo NNaasshh

0.05.5

1.10.0

00..0005..05

0.08.8

0.05.5

0.06.6

00..0005..05

0.04.4

00..0005..05 0.05.5
101000 101101 101202 101303 101404 101505 101606 IteItreartaiotinosns

0.02.2 0.00.0 101000 101101 101202 101303 101404 101505 101606 IteItreartaiotinosns

(a)

(b)

Figure 1: Convergence of policy gradient in LQ dynamic games to the Nash policy. (a) Each player’s linear feedback gain matrix Ki converges to the unique Nash policies (dotted lines). (b) The black dashed line shows upper bound of the number of iterations required to converge within ε distance from Nash (2-norm). The actual convergence for this random initialization is shown as the solid line.

The key technique in proving the above theorem—the complete details are provided in Borkar and Pattathil [9] which, in turn, leverages results from Thoppe and Borkar [32]—is ﬁrst to compute the errors between the sample points from the stochastic learning rules and the continuous time ﬂow generated by initializing the continuous time limiting dynamics at each sample point and ﬂowing it forward for time tk+1 − tk, doing this for each x1,k and x2,k separately and in their own timescale, and then take a union bound over all the continuous time intervals deﬁned for k ≥ k0.

5. Numerical Examples
The results in the preceding sections provide convergence guarnatees for a class of gradient-based learning algorithms to a neighborhood of a stable Nash equilibrium under deterministic and stochastic gradient-based update rules with both uniform and non-uniform learning rates. In this section, we present several numerical examples that validate these theoretical results and highlight interesting aspects of learning in multi-agent settings.

5.1 Deterministic Policy Gradient in Linear Quadratic Dynamic Games

The ﬁrst example we explore is a linear quadratic (LQ) game with three players in the space of linear feedback policies. This game serves as a useful benchmark since it has a unique global equilibrium that we can compute via a set of coupled algebraic Riccati equations [4]. The gradient-based learning rule for each of the agents is a multi-agent version of policy gradient in which agents have oracle access to their gradients at each iteration.
Consider a four state discrete time linear dynamical system,

z(t + 1) = Az(t) + B1u1(t) + B2u2(t) + B3u3(t)

where z(t) ∈ R4 and, for each i ∈ {1, 2, 3}, ui(t) ∈ R is the control for player i. The policy for each player is parameterized by a linear feedback gain matrix, ui(t) = −Kiz(t). Moreover, each player seeks to minimize a quadratic cost

∞

fi(Ki, K−i) = Ez0∼D

z(t)T Qiz(t) +

t=0

n j=1

uj

(t)T

Rij

uj

(t)

13

CHASNOV, RATLIFF, MAZUMDAR, AND BURDEN

which is a function of the coupled state variable z(t), their own control ui(t) and all other agents’ control u−i(t) over an inﬁnite time horizon. In an effort to learn a Nash equilibrium, each agent employs policy gradient. In particular, they update their feedback policy via

Ki(t + 1) = Ki(t) − γi∇Ki fi(Ki, K−i).

It is fairly straightforward to compute the gradient of fi with respect to Ki, the feedback gain that parameterizes player i’s control input ui. Indeed,
∇Ki fi(Ki, K−i) = 2(RiiKi − BiT PiA)ΣK

where

∞

ΣK = Ez0∼D

z(t)z(t)T .

t=0

Hence, the collection of the agents’ individual gradients is given by

3
ω(K1, K2, K3) = 2(RiiKi − BiT PiA)ΣK
i=1

Remark 4. Note that ω can be zero at critical points or at points where

∞ t=0

z

(t)z

(t)T

drops rank.

To

prevent the latter possibility, we sample the initial condition from a distribution. That is, we take z0 ∼ D so

that Ez0∼Dz0z0T is full rank.

For a given joint policy (K1, K2, K3), the closed loop dynamics are A = A − B1K1 − B2K2 − B3K3.

The states z(t) are obtained from simulating the system. For each i, the Riccati matrix Pi is computed by

solving the Riccati equation

n
Pi = AT PiA + Qi + KjRijKj.

j=1

Note that this Riccati equation is only used to compute the gradient of the cost functions with respect to a

speciﬁc set of feedback gains. The system parameters used in this example are listed in Appendix B.1. For the purpose of validating convergence, we can compute the Nash policies (K1∗, K2∗, K3∗) by an

established method with coupled Riccati equations, explained in Appendix B.1. We use the learning rate

γi = γ deﬁned as in Theorem 1. To compute γ we ﬁrst compute the game Jacobian J(K1∗, K2∗, K3∗) at the Nash feedback gains and then ﬁnd the maximum eigenvalue of JT J and minimum eigenvalue of

(J T

+

J )T (J T

+

J)

in

a

neighborhood

of

(K

∗ 1

,

K2∗

,

K3∗

)

to

determine

the

constants

α

and

β

as

deﬁned

in

Section 3.

Figure 1 shows the convergence of the gradient updates to the Nash policies. The Ki are randomly initialized in a neighborhood of the known Nash equilibrium and such that A˜ is stable. The number of

iterations required to converge to an ε–differential Nash is bounded by the dashed black line in Figure 1b,

which shows the curve of (ε, T ) pairs determined by Theorem 1. However, this learning rate is not optimal,

as choosing a larger γ will result in faster convergence as empirically observed.

Remark 5 (Stochastic Policy Gradient). We note that stochastic policy gradient with an unbiased estimator has similar convergence properties. Here, e.g., the state dynamics may be subject to zero-mean, ﬁnitevariance noise. As long as the estimator for the gradient is unbiased, the theoretical guarantees of the proceeding sections apply.

14

1 1

0.5

0.5

0

0

0.5

10

uniform

0.5
faster

0

1

0

0.5

1

slower

Figure 2: Gradient dynamics of the matching pennies game where agents learning have different learning rates. The vector ﬁeld of the gradient dynamics are stretched along the faster agent’s coordinate.

5.2 Benchmark: matching pennies
The next example is again a multi-agent policy gradient example in which there are two players playing ‘matching pennies’, a classic bimatrix game in which agents have zero-sum costs associated with the matrices (A, B) deﬁned as follows:

A = 1 −1 , B = −1 1 .

−1 1

1 −1

In particular, the players aim to minimize their respective costs f1(x, y) = π(y)T Aπ(x) and f2(x, y) = π(x)T Bπ(y) where π(x) is player 1’s policy and π(y) is player 2’s policy. The class of policies the agents
are optimizing over are the so-called ‘softmax’ policies deﬁned by

π(z) =

e10z e10z + e10(1−z) ,

e10(1−z) e10z + e10(1−z) ,

and the update each player employs is a ‘smoothed best-response’ which in essence is a policy gradient update with respect to the softmax parameter and each agents individual cost. This game has been well studied in the game theory literature and we use it illustrate the fact that non-uniform learning rates result in a warping of the vector ﬁeld associated with the agents’ learning dynamics.
The mixed Nash equilibrium for this game is (x∗, y∗) = (0.5, 0.5), but the Jacobian of the gradient dynamics at this ﬁxed point is
J (x∗, y∗) = 0 100 −100 0
so that it has purely imaginary eigenvalues ±100i, and therefore admits a limit cycle. Regardless, we can visualize the effects of non-uniform learning rates to the gradient dynamics in Figure 2. We notice that the gradient ﬂow stretches along the axes of the faster agent (the agent with a larger learning rate), and the ﬁxed points of these dynamics remain constant.

5.3 Exploring the Effects of Non-uniform Learning Rates on the Learning Path
The examples presented so far all consider convergence (or non-convergence) to a single equilibrium. In the following two examples, we investigate the effect of non-uniform learning rates for more general nonconvex settings in which there are multiple equilibria. The following example is a two-player game in which the agents’ joint strategy space is a torus. That is, each player’s strategy space is the unit circle S1. For each

15

CHASNOV, RATLIFF, MAZUMDAR, AND BURDEN

0 /2

fast

fast
A B
A

slow
A B
A

0

( 1, k/ 2, k 0)
2=0 1=0
( 2, k/ 1, k 0)

2

/2

0

slow

B

B

0

0

(a)

/2 0

/2

1

(b)

Figure 3: The effects of non-uniform learning rates on the path of convergence to the equilibria. The zero lines for each player (D1f1 = 0 or D2f2 = 0) are plotted as the diagonal and curved lines, and the two stable Nash equilibria as circles (where D12f1 > 0 and D22f2 > 0). (a) In the deterministic setting, the region of attractions for each equilibrium can be computed numerically. Four scenarios are shown, with a combination of fast and slow agents. The region
of attractions for each Nash equilibrium are warped under different learning rates. (b) In the stochastic setting, the
samples (in black) approximate the singularly perturbed differential equation (in red). Two initializations and learning
rate conﬁgurations are plotted.

i ∈ {1, 2}, player i has cost fi : S1 × S1 → R given by

fi(θi, θ−i) = −αi cos(θi − φi) + cos(θi − θ−i)

where αi and φi are constants, and θi is player i’s choice variable. An interpretation of this game is that of a ‘location game’ in which each player wishes to be near location φi but far from each other. This game has many applications including those which abstract nicely to coupled oscillators.
The game form—i.e., collection of individual gradients—is given by

ω(θ1, θ2) = α1 sin(θ1 − φ1) − sin(θ1 − θ2) ,

(8)

α2 sin(θ2 − φ2) − sin(θ2 − θ1)

and the game Jacobian is composed of terms αi cos(θi − φi) − cos(θi − θ−i), i = 1, 2 on the diagonal and cos(θi − θ−i), i = 1, 2 on the off-diagonal.
The Nash equilibria of this game occur where ω(θ1, θ2) = 0 and where the diagonals of the game Jacobian are positive. The game has multiple Nash equilibria. We visualize the warping of the region of attraction of these equilibria under different learning rates, and the afﬁnity of the “faster” player to its own zero line.
In this example, we use constants φ = (0, π/8) and α = (1.0, 1.5). The joint strategy space can be viewed as a non-convex smooth manifold via an equivalence relationship, or equivalently, as players choosing θi ∈ R. There are two Nash equilibria, situated at (−1.063, 1.014) and (1.408, −0.325). These equilibria happen to also be stable differential Nash, and thus we expect the gradient dynamics to converge to them if initialized in the region of attraction. Which equilibrium it converges to, however, depends on the initialization and learning rates of agents.
To investigate how non-uniform learning rates affect the agents’ convergence to the two equilibria, we simulate agents learning at different rates, one fast and one slow. The fast agent’s learning rate is set to

16

γ1 = 0.171 and the slow γ2 = 0.017. Figure 3a shows the trajectory of agents’ learned strategies. Each of

the four squares depicts the full strategy space on the torus from −π to π for both agents’ actions, with θ1 on

the x-axis and θ2 on the y-axis. The labels “fast” and “slow” indicate the learning rate of the corresponding

agent. For example, in the bottom left square, agent 1 is the fast agent and agent 2 is the slow agent. Hence,

the non-uniform update equation for that square becomes θk+1 = θk − diag(γ1, γ2)ω(θk).

The white lines indicate the points x such that ωi(x) = 0, and the intersection of the white lines indicate

points x such that ω(x) = 0. The two intersections marked as circles are the stable differential Nash

equilibria. The unmarked intersections are either saddle points or other unstable equilibria. The black lines

show different paths of the update equations under the non-uniform update equation, with initial points

selected from a equally spaced 7 × 7 grid. We highlight two paths in green (labeled A and B) which begin

at (π/3, π/3) and (−π/3, −π/3).

In the case where agents both learn at the same rate, (γ1, γ1) and (γ2, γ2), paths A and B both converge

to the Nash equilibrium at (−1.063, 1.014). However, when agents learn at different rates, the equilibrium

to which the agents converge to, as well as the learning path, is no longer the same even starting at the same

initial points. This phenomena can also be captured by displaying the region of attraction for both Nash

equilibria. The red region corresponds to initializations that will converge to the equilibrium contained in

the red region (again indicated by a white circle). Analogously, the blue region corresponds to the region of

attraction of the other equilibria.

To provide an example of the stochastic setting in which agents have an unbiased estimator of their

individual gradients, we choose learning rates according to Assumption 2. In particular, we choose scaled

learning

rates

γ2,k

=

1 1+k log(k+1)

and

γ1,k

=

1 1+k

such

that

γ2,k /γ1,k

→

0

as

k

→

∞.

Figure

3b

shows

the

learning paths in this setting initialized at two different points, each with ﬂipped learning rate conﬁgurations.

The sample points approximate the singularly perturbed differential equation (shown in red) described in

Section 4.2.

In both deterministic and stochastic settings, we observe the afﬁnity of the faster agent to its own zero

line. For example, the bottom left square (in Figure 3a) and bottom left path (in Figure 3b) both have agent

1 as the faster agent, and the learning paths both tend to arrive to the line ω1 ≡ 0 before ﬁnally converging

to the Nash equilibrium. An interpretation of this is that the faster agent tries to be situated at the bottom

of the “valley” of its own cost function. The faster agent tends to be at its own minimum while it waits

for the slower agent to change its strategy. As a Stackelberg interpretation, where there are followers and

leaders, the slower agent would be the leader and faster agent the follower. In a sense, the slower agent has

an advantage.

5.4 Multi-agent control and collision avoidance

The ﬁnal example presents a practical use case for the gradient-based update. Consider a non-cooperative game between four collision-avoiding agents where they seek to arrive at a destination with minimum fuel while avoiding each other. We show that the scaling between agents’ learning rates dictates the equilibrium solution to which they converges. This can be useful in designing non-cooperative open-loop controllers where agents may choose to learn slower in order to deviate less from their initial plan, perhaps in an attempt to incur less ‘risk’.
Suppose there are four collision-avoiding particles traversing across a unit circle. Each particle follows discrete-time linear dynamics
zi(t + 1) = Azi(t) + Bui(t)

for t = 1, · · · , N where

A = I hI ∈ R4×4, B = h2I ∈ R4×2,

0I

hI

17

CHASNOV, RATLIFF, MAZUMDAR, AND BURDEN

(a)

(b)

(c)

(d)

Figure 4: Minimum-fuel particle avoidance control example. (a) Each particle seeks to reach the opposite side of the circle using minimum fuel while avoiding each other. The circles represent the approximate boundaries around each particle at time t = 5. (b) The joint strategy x = (u1, · · · , u4) is initialized to the minimum fuel solution ignoring interaction between particles. (c) Equilibrium solution achieved by setting the blue agent to have a slower learning rate. (d) Another equilibrium, where the red agent has the slower learning rate.

I is the identity matrix, and h = 0.1. These dynamics represent a typical discretized version of the continuous dynamics r¨i = ui in which ui ∈ R2 represents a force vector used to accelerate the particle, and the state zi = [ri, r˙i] represents the particles position and velocity. Let ui be the concatenated vector of control vectors for player i for all time—i.e., ui = (ui(1), · · · , ui(N )) and let u = (u1, · · · , un). Each particle i aims to minimize a cost deﬁned by

N
Ji(u) =
t=1

N +1

ui(t)

2 R

+

t=1

zi(t) − z¯i

2 Q

+

N +1

ρe−σ

zi(t)−zj (t)

2 S

j=i t=1

where

·

P denotes the quadratic norm—i.e.,

z

2 P

= zT P z with P

positive semi-deﬁnite.

The ﬁrst two

terms of the cost correspond to the minimum fuel objective and quadratic cost from desired ﬁnal state z¯i, a

typical setup for optimal control problems. We use R = diag(0.1, 0.1) and Q = diag(1, 1, 0, 0). The ﬁnal

term of the cost function is the sum of all pairwise interaction terms between the particles, modeled after the

shape of a Gaussian which encodes smooth boundaries around the particles. We use constants ρ = 10 and

σ = 100.

Figure 4 (a) visualizes the problem setup. Each particle’s initial position zi(0) is located on the left side

of a unit circle; they are separated by π/5, and their desired ﬁnal positions, z¯i for each i ∈ {1, . . . , 4},

are located directly opposite. The particles begin with zero velocity and must solve for a minimum control

solution that also avoids collision with other particles as described by the objectives Ji for each i.

To initialize the gradient-based learning algorithms in the game setting, we compute the optimal solution

for each agent ignoring the pairwise interaction terms, shown in Figure 4 (b). This can be computed using

classical discrete-time LQR methods or by gradient descent. Then, using this solution as the intialization

for the game setting, each agent descends their own gradient, i.e.

ui,k+1 = ui,k − γiDiJi(u),

with different learning rates γi. Just as the previous example shows, the relative learning rates of agents warp the region of attraction for the multiple equilibria. If we allow the red agent to learn slower, then the learning process converges to the equilibria shown in Figure 4 (c), whereas if the blue agent learns quicker, then we converge to Figure 4 (d). Hence, all else being equal, the learning rates adopted by players greatly impact the equilibrium to which they converge.

18

6. Discussion and Future Work
We analyze the convergence of gradient-based learning for non-cooperative agents with continuous costs. We leverage existing dynamical systems theory and stochastic approximation literature to provide convergence guarantees for agents that learn myopically—that is, only using information about their own gradient Difi to update their strategy. We provide guarantees for the case where agents are assumed to have oracle access to Difi and the case where they have sufﬁcient information to compute an unbiased estimator. We also study the effects of non-uniform learning rates.
By preconditioning the gradient dynamics by Γ, a diagonal matrix where the diagonals represent the agents’ learning rates, we can begin to understand how a changing learning rate relative to others can change the properties of the ﬁxed points of the dynamics. Moreover, players do not know how a change in others’ strategies affects its own cost (Djfi where j = i). A possible extension to this paper is to develop update schemes that use this to provide more robust convergence guarantees for full information continuous games. Different learning rates amongst agents also affects the region of attraction of the game, hence starting from the same initial condition, agents may converge to a different equilibria. Agents may use this to their beneﬁt, as shown in the last example. Such insights into the learning behavior of agents will be useful for providing guarantees on the design of control or incentive policies to coordinate agents. We also show through numerical examples that, counterintuitively, if an agent decides to learn slower, a stable differential Nash equilibrium can go unstable, resulting in learning dynamics that do not converge to Nash.
Beyond the the effects of learning rates, there are a number of avenues for future inquiry. For instance, the results as stated apply to continuous games with Euclidean strategy spaces. An interesting avenue to pursue is the study of learning in games where the agents decision spaces are constrained sets or Riemannian manifolds. The latter arises in a number of robotics applications and in this case, the update rule will need to be modiﬁed by the appropriately deﬁned retraction such as xk+1 = expxk (γk(ω(xk))) [31]. The former arises in a variety of applications where the learning rules are abstractions of agents learning in, e.g., physically constrained environments. The update rule in this case will also need to be deﬁned in terms of the appropriate proximal map thereby leading to potentially non-smooth dynamics [10, 19] which is even more challenging in the stochastic setting. Yet, such extensions will lead to a framework and set of analysis tools that apply to a broader class of multi-agent learning algorithms.
While we present the work in the context of gradient-based learning in games, there is nothing that precludes the results from applying to update rules in other frameworks. Our results will apply to many other settings where agents myopically update their decision using a process of the form xk+1 = xk − Γg(xk). In this paper, we consider the special case where g ≡ [D1f1 · · · Dnfn]. In the stochastic setting, variants of multi-agent Q-learning conform to this setting since Q-learning can be written as a stochastic approximation update.
Finally, as pointed out in [21], not all critical points of the dyanamics x˙ = −ω(x) that are attracting are necessarily Nash equilibria; one can see this simply by constructing a Jacobian with positive eigenvalues with at least one Di2fi with a non-positive eigenvalue. Understanding this phenomena will help us develop computational techniques to avoid them. Recent work has explored this in the context of zero-sum games [22], requiring coordination amongst the learning agents. However, when our objective is to study the learning behavior of autonomous agents seeking an equilibrium, an alternative perspective is needed.
Appendix A. Proofs
A.1 Deterministic Setting
The following proof follows nearly the same proof as the main result in [2] with a few minor modiﬁcations in the conclusion; we provide it here for posterity.
19

CHASNOV, RATLIFF, MAZUMDAR, AND BURDEN

Proof Proposition 4. Since I −ΓDω(x) < 1 for each x ∈ Br0(x∗), as stated in the proposition statement, there exists 0 < r < r < 1 such that I − ΓDω(x) ≤ r < r < 1 for all x ∈ Br(x∗). Since
lim R(x − x∗) / x − x∗ = 0,
x→x∗
for 0 < 1 − r < 1, there exists r˜ > 0 such that R(x − x∗) ≤ (1 − r ) x − x∗ , ∀ x ∈ Br˜(x∗).
As in the proposition statement, let r be the largest, ﬁnite such r˜. Note that for arbitrary c > 0, there exists r˜ > 0 such that the bound on R(x − x∗) holds; hence, we choose c = 1 − r and ﬁnd the largest such r˜ for which the bound holds. Combining the above bounds with the deﬁnition of g, we have that
g(x) − g(x∗) ≤ (1 − δ) x − x∗ , ∀ x ∈ Br∗(x∗) where δ = r − r and r∗ = min{r0, r}. Hence, applying the result iteratively, we have that
xt − x∗ ≤ (1 − δ)t x0 − x∗ , ∀ x0 ∈ Br∗(x∗).
Note that 0 < 1 − δ < 1. Using the approximation 1 − δ < exp(−δ), we have that xT − x∗ ≤ exp(−T δ) x0 − x∗
so that xt ∈ Bε(x∗) for all t ≥ T = δ−1 log(r∗/ε) .

As noted in the remark, a similar result holds under the relaxed assumption that ρ(I − ΓDω(x)) < 1
for all x ∈ Br0(x∗). To see this, we ﬁrst note that ρ(I − ΓDω(x)) < 1 implies there exists c > 0 such that ρ(I − ΓDω(x)) ≤ c < 1. Hence, given any > 0, there is a norm on Rd and a c > 0 such that I − ΓDω ≤ c + < 1 on Br0(x∗) [25, 2.2.8]. Then, we can apply the same argument as above using
r = c + ε.

A.2 Stochastic Setting
A key tool used in the ﬁnite-time two-timescale analysis is the nonlinear variation of constants formula of Alekseev [1], [9].

Theorem 6. Consider a differential equation

u˙ (t) = f (t, u(t)), t ≥ 0,

and its perturbation

p˙(t) = f (t, p(t)) + f˜(t, p(t)), t ≥ 0

where f, f˜ : R × Rd → Rd, f ∈ C1, and f˜ ∈ C. Let u(t, t0, p0) and p(t, t0, p0) denote the solutions of the above nonlinear systems for t ≥ t0 satisfying u(t0, t0, p0) = p(t0, t0, p0) = p0, respectively. Then,

t
p(t, t0, p0) = u(t, t0, p0) + Φ(t, s, p(s, t0, p0))f˜(s, p(s, t0, p0)) ds, t ≥ t0
t0

where Φ(t, s, u0), for u0 ∈ Rd, is the fundamental matrix of the linear system

∂f v˙(t) = ∂u (t, u(t, s, u0))v(t), t ≥ s (9)

with Φ(s, s, u0) = Id, the d–dimensional identity matrix.

20

Consider a locally asymptotically stable differential Nash equilibrium x∗ = (λ(x∗2), x∗2) ∈ X and let Br0(x∗) be an r0 > 0 radius ball around x∗ contained in the region of attraction. Stability implies that the Jacobian J(λ(x∗2), x∗2) is positive deﬁnite and by the converse Lyapunov theorem [30, Chapter 5] there exists local Lyapunov functions for the dynamics x˙ 2(t) = −τ D2f2(λ(x2(t)), x2(t)) and for the dynamics x˙ 1(t) = −D1f1(x1(t), x2), for each ﬁxed x2. In particular, there exists a local Lyapunov function V ∈ C1(Rd1) with lim x2 ↑∞ V (x2) = ∞, and ∇V (x2), D2f2(λ(x2), x2) < 0 for x2 = x∗2. For r > 0, let V r = {x ∈ dom(V ) : V (x) ≤ r}. Then, there is also r > r0 > 0 and 0 > 0 such that for < 0,
{x2 ∈ Rd2 | x2 − x∗2 ≤ } ⊆ V r0 ⊂ N 0 (V r0 ) ⊆ V r ⊂ dom(V )
where N 0(V r0) = {x ∈ Rd2| ∃x ∈ V r0 s.t. x − x ≤ 0}. An analogously deﬁned V˜ exists for the dynamics x˙ 1 for each ﬁxed x2.
For now, ﬁx n0 sufﬁciently large; we specify this a bit further down. Deﬁne the event
Ek = {x¯1(t) ∈ V r ∀t ∈ [t˜k0, t˜k]}

where

x¯1(t) = x1,k + t − t˜k (x1,k+1 − x1,k) γ1,k

are linear interpolates deﬁned for t ∈ (t˜k, t˜k+1) with t˜k+1 = t˜k + γ1,k and t˜0 = 0. The basic idea of the proof is to leverage Alekseev’s formula (Theorem 6) to bound the difference between the linearly interpolated trajectories (i.e., asymptotic psuedo-trajectories) and the ﬂow of the corresponding limiting differential equation on each continuous time interval between each of the successive iterates k and k + 1 by a number that decays asymptotically. Then, for large enough k, a union bound is used over all the remaining time intervals to construct a concentration bound. This is done ﬁrst for fast player (i.e. player 1), to show that x1,k tracks λ(x2,k), and then for the slow player (i.e., player 2).
Following Borkar and Pattathil [9], we can express the linear interpolates for any k ≥ k0 as x¯1(t˜k+1) = x¯1(t˜k0 ) − n=k0 γ1, (D1f1(x ) + w1, +1) where

γ1, D1f1(x ) =

t˜ +1
D1f1(x¯1(t˜ ), x2, )
t˜

and similarly for the w1, +1 term. Adding and subtracting

t˜n+1 ˜

D1f1(x¯1(s),

x2(s)),

Alekseev’s

formula

tn0

can be applied to get

t
x¯1(t) = x1(t) + Φ1(t, s, x¯1(t˜k0), x2(t˜k0))(x¯1(t˜k0) − x1(t˜k0)) + Φ2(t, s, x¯1(s), x2(s))ζ1(s) ds
t˜k0

where x2(t) ≡ x2 is constant (since x˙ 2 = 0), x1(t) = λ(x2),

ζ1(s) = −D1f1(x¯1(t˜k), x2(t˜k)) + D1f1(x¯1(s), x2(s)) + w1,k+1,

and where for t ≥ s, Φ1(·) satisﬁes linear system Φ˙ 1(t, s, x0) = J1(x1(t), x2(t))Φ1(t, s, x0),

with initial data Φ1(t, s, x0) = I and x0 = (x1,0, x2,0) and where J1 the Jacobian of −D1f1(·, x2). Given that x∗ = (λ(x∗2), x∗2) is a stable differential Nash equilibrium, J1(x∗) is positive deﬁnite. Hence,
as in [32, Lemma 5.3], we can ﬁnd M , κ1 > 0 such that for t ≥ s, x1,0 ∈ V r, Φ1(t, s, x1,0, x2,0) ≤

21

CHASNOV, RATLIFF, MAZUMDAR, AND BURDEN

M e−κ1(t−s); this result follows from standard results on stability of linear systems (see, e.g., Callier and Desoer [11, §7.2, Theorem 33]) along with a bound on
t
D12f1(x1, x2(τ, s, x˜0)) − D12f1(x∗) dτ
s
for x˜0 ∈ V r (see, e.g., [32, Lemma 5.2]). Consider zk = λ(x2,k)—i.e., where D1f1(x1,k, x2,k) = 0. Then, using a Taylor expansion of the
implicitly deﬁned λ, we get

zk+1 = zk + Dλ(x2,k)(x2,k+1 − x2,k) + δk+1

(10)

where δk+1 ≤ Lr x2,k+1 − x2,k 2 is the error from the remainder terms. Plugging in x2,k+1, we have

zk+1 = zk + γ1,k(−D1f1(zk, x2,k) + τkλ(x2,k)(w2,k+1 − D2f2(x1,k, x2,k)) + γ1−,k1δk+1).

The terms after −D1f1 are o(1), and hence asymptotically negligible, so that this z sequence tracks dynam-
ics as x1,k. We show that with high probability, they asymptotically contract to one another. Deﬁne constant Hk0 = ( x¯1(t˜k0 − x1(t˜k0) + z¯(t˜k0) − x1(t˜k0) ) and

k−1
S1,k =
=k0

t˜ +1
Φ1(t˜k, s, x¯1(t˜ ), x2(t˜ ))ds w2, +1.
t˜

Moreover, let τk = γ2,k/γ1,k.

Lemma 2. For any k ≥ k0, there exists K > 0 such that

x1,k − zk

≤K

S1,k + e−κ1(t˜k−t˜k0 )Hk0 + sup γ1, + sup γ1,

k0≤ ≤k−1

k0≤ ≤k−1

+ sup τ + sup τ w2, +1 2

k0≤ ≤k−1

k0≤ ≤k−1

w1, +1 2

conditioned on Ek.

In order to construct a high-probability bound for x2,k, we need a similar bound as in Lemma 2 can be

constructed

for

x2,k .

Deﬁne

the

event

Eˆk

=

{x¯2(t)

∈

V

r

∀t

∈

[tˆk0 ,

tˆk]}

where

x¯2(t)

=

x2,k +

t−tˆk γ

(x2,k+1 −

2,k

x2,k) is the linear interpolated points between the samples {x2,k}, tˆk+1 = tˆk + γ1,k, and tˆ0 = 0. Then as

above, Alekseev’s formula can again be applied to get

x¯2(t) = x2(t, tˆk0 , x2(tˆk0 )) + Φ2(t, tˆk0 , x¯2(tˆk0 ))(x¯2(tˆk0 ) − x2(tˆk0 )) + where x2(t) ≡ x∗2,

t
Φ2(t, s, x¯2(s))ζ1(s) ds
tˆk0

ζ1(s) = D2f2(λ(x2,k), x2,k) − D2f2(λ(x¯2(s)), x¯2(s)) + D2f2(xk) − D2f2(λ(x2,k), x2,k) + w2,k+1,

and Φ2 is the solution to a linear system with dynamics J2(λ(x∗2), x∗2), the Jacobian of −D2f2(λ(·), ·), and with initial data Φ2(s, s, x2,0) = I. This linear system, as above, has bound Φ2(t, s, x2,0) ≤ M2eκ2(t−1) for some M2, κ2 > 0. Deﬁne

k−1
S2,k =
=k0

tˆ +1
Φ2(tˆk, s, x¯2(tˆ ))ds w2, +1.
tˆ

22

Lemma 3. For any k ≥ k0, there exists K¯ > 0 such that

x¯2(tˆk) − x2(tˆk)

≤K¯ S2,k + supk0≤ ≤k−1 S1, + supk0≤ ≤k−1 γ1, + supk0≤ ≤k−1 γ1, w1, +1 2 + supk0≤ ≤k−1 τ + supk0≤ ≤k−1 τ w2, +1 2 + eκ2(tˆk−tˆk0 ) x¯2(tˆk0 ) − x2(tˆk0 ) + supk0≤ ≤k−1 τkHk0

conditioned on E˜k.

Using the above lemmas, we can get the desired guarantees on x1,k and x2,k as in [9].

Appendix B. Additional Examples
In this appendix, we include additional examples and information about examples contained in the main body of the text.

B.1 LQ game system parameters
The following are the system parameters and resulting Nash feedback gains computed using the coupled Riccatti equations:

 0.402 A = −0.021
 0.377

1.037 −0.990 1.105

−0.565 −0.584 0.698

0.115 

1

0

0

0.457 

1

1

0

1.192  , B1 = 0 , B2 = 1 B3 = 1

−0.177 −0.332 0.237 −0.286

0

0

1

0.48 0 0

0 Q1 =  0

0.64 0 0 0.74

0

0.01 0 0

0

0

0  , Q2 =  0

0.41 0 0 0.71

0

1.00 0 0

0

0

0  , Q3 =  0

0.55 0 0 0.86

0 0 0 0.71

0 0 0 0.44

000

R11 = 5.47 , R12 = 7.16 , R13 = 5.31 , R21 = 5.21 , R22 = 5.36 ,

R23 = 7.63 , R31 = 9.71 , R32 = 2.34 , R33 = 5.26 ,

0 0  , 0 0.63

and

 0.023 T

0.060T

0.033T

−0.201

0.029

0.082

K1 = −0.228 , K2 = 0.026 , K3 = 0.138 .

0.104

0.274

0.177

We use the following values for constants used in the LQ game: α = 19.4 and 2.90 × 105; hence, we use γ = 1.52 × 10−5.

B.2 Coupled Riccati equations

We require the following standard assumption adopted in LQ games.

√

√

Assumption 5. Either (A, B1, Q1) or (A, B2, Q2) is stabilizable-detectable.

√ Without loss of generality, we assume (A, B1, Q1) is stabilizable-detectable. We employ the following

iterative Lyapunov algorithm for ﬁnding the Nash equilibrium to the linear quadratic game [20]:

23

CHASNOV, RATLIFF, MAZUMDAR, AND BURDEN

step 1. Initialize P1(0) to be the unique positive deﬁnite solution to the Riccati equation,

P1(0)

=

AT P1(0)A

−

A

T

P

(0) 1

B1

R1 + B1T P1(0)B1

−1B1T P1(0)A + Q1,

(11)

and compute the corresponding gain matrix for player 1 by

K1(0)

=

(R1

+

B

T 1

P

(0) 1

B1

)−1

B1T

P

(0) 1

A.

(12)

Solve for P2(0) by

P2(0) = A¯T P2(0)A¯ − A¯T P2(0)B2 R2 + B2T P2(0)B2 −1B2T P2(0)A¯ + Q1

(13)

where A¯ = A − B1K1(0) and compute the corresponding gain matrix for player 2 by

K2(0)

=

(R2

+

B

T 2

P2(0)

B

2

)

−1

B

T 2

P

(0) 2

A − B1K1(0)

.

(14)

We note that initializing using this method ensures that the initial closed loop matrix A − B1K1(0) − B2K2(0) is stable. step 2. Given P1(k), P2(k), K1(k), and K2(k), update the feedback gains using the following update rules:

K1(k+1)

=

(R11

+

B1T

P

(k 1

)

B

1

)

−1

B1T

P

(k 1

)

(

A

−

B2K2(k))

(15)

K2(k+1)

=

(R22

+

B2T

P

(k 2

)

B

2

)

−1

B2T

P

(k 2

)

(

A

−

B1K1(k))

(16)

step 3. Update the cost-to-go matrices by solving the Lyapunov equations:
P1(k) = (A¯ − B2K2(k))T P1(k+1)(A¯ − B2K2(k)) + (K1(k))T R11K1(k) + (K2(k))T R12K2(k) + Q1 P2(k) = (A¯ − B2K2(k))T P2(k+1)(A¯ − B2K2(k)) + (K1(k))T R21K1(k) + (K2(k))T R22K2(k) + Q2

step 4. Repeat steps 2–3 until the gains converge.
The extension to n-players is fairly straightforward; more detail can be found in the seminal reference [4].

References
[1] V. M. Alekseev. An estimate for the perturbations of the solutions of ordinary differential equations. Vestnik Moskov. Univ. Ser. I. Mat. Meh., 2:28–36, 1961.
[2] I. K. Argyros. A generalization of ostrowski’s theorem on ﬁxed points. Applied Mathematics Letters, 12:77–79, 1999.
[3] David Balduzzi, Se´bastien Racaniere, James Martens, Jakob Foerster, Karl Tuyls, and Thore Graepel. The mechanics of n-player differentiable games. CoRR, abs/1802.05642, 2018.
[4] T. Basar and G. Olsder. Dynamic Noncooperative Game Theory. Society for Industrial and Applied Mathematics, 2nd edition, 1998. doi: 10.1137/1.9781611971132.
[5] Michel Bena¨ım. Dynamics of stochastic approximation algorithms. In Seminaire de Probabilites XXXIII, pages 1–68, 1999.

24

[6] Michel Benaim and Morris W. Hirsch. Mixed equilibria and dynamical systems arising from ﬁctitious play in perturbed games. Games and Economic Behavior, 29(1-2):36–72, 1999.
[7] Michel Bena¨ım, Josef Hofbauer, and Sylvain Sorin. Perturbations of set-valued dynamical systems, with applications to game theory. Dynamic Games and Applications, 2(2):195–205, 2012.
[8] S. Bhatnagar and H. L. Prasad. Stochastic Recursive Algorithms for Optimization. Springer, 2013.
[9] Vivek S. Borkar and Sarath Pattathil. Concentration bounds for two time scale stochastic approximation. arxiv:1806.10798, 2018.
[10] V.S. Borkar. Stochastic Approximation: A Dynamical Systems Viewpoint. Springer, 2008.
[11] F. Callier and C. Desoer. Linear Systems Theory. Springer, 1991.
[12] Constantinos Daskalakis, Andrew Ilyas, Vasilis Syrgkanis, and Haoyang Zeng. Traning GANs with Optimism. arxiv:1711.00141, 2017.
[13] Drew Fudenberg and David K Levine. The theory of learning in games, volume 2. MIT press, 1998.
[14] Sergiu Hart and Andreu Mas-Colell. Uncoupled dynamics do not lead to nash equilibrium. American Economic Review, 93(5):1830–1836, December 2003. doi: 10.1257/000282803322655581.
[15] J. Heinrich and D. Silver. Deep reinforcement learning from self-play in imperfect-information games. arxiv:1603.01121, 2016.
[16] Josef Hofbauer. Evolutionary dynamics for bimatrix games: A hamiltonian system? Journal of Mathematical Biology, 34(5):675, May 1996.
[17] Cars H. Hommes and Marius I. Ochea. Multiple equilibria and limit cycles in evolutionary games with logit dynamics. Games and Economic Behavior, 74(1):434 –441, 2012. doi: 10.1016/j.geb.2011.05. 014.
[18] Prasenjit Karmakar and Shalabh Bhatnagar. Two time-scale stochastic approximation with controlled markov noise and off-policy temporal-difference learning. Mathematics of Operations Research, 2018.
[19] H. J. Kushner and G. G. Yin. Stochastic Approximation and Recursive Algorithms and Applications. Springer, 2nd edition, 2003.
[20] T-Y. Li and Z. Gajic. Lyapunov iterations for solving coupled algebraic riccati equations of nash differential games and algebraic riccati equations of zero-sum games. In Geert Jan Olsder, editor, New Trends in Dynamic Games and Applications, pages 333–351, Boston, MA, 1995. Birkha¨user Boston. ISBN 978-1-4612-4274-1.
[21] E. Mazumdar and L. J. Ratliff. On the convergence of competitive, multi-agent gradient-based learning algorithms. arxiv:1804.05464, 2018.
[22] E. Mazumdar, M. Jordan, and S. S. Sastry. On ﬁnding local nash equilibria (and only local nash equilibria) in zero-sum games. arxiv:1901.00838, 2019.
[23] Panayotis Mertikopoulos and Zhengyuan Zhou. Learning in games with continuous action sets and unknown payoff functions. Mathematical Programming, 173(1–2):456–507, 2019.
[24] Dov Monderer and Lloyd S. Shapley. Potential games. Games and Economic Behavior, 14(1):124– 143, 1996. doi: 10.1006/game.1996.0044.
25

CHASNOV, RATLIFF, MAZUMDAR, AND BURDEN
[25] J. M. Ortega and W. C. Rheinboldt. Iterative Solutions to Nonlinear Equations in Several Variables. Academic Press, 1970.
[26] A. M. Ostrowski. Solution of Equations and Systems of Equations. Academic Press, 1966. [27] Christos H. Papadimitriou and G. Piliouras. Game dynamics as the meaning of a game. Sigecom,
2018. [28] L. J. Ratliff, S. A. Burden, and S. S. Sastry. On the Characterization of Local Nash Equilibria in
Continuous Games. IEEE Transactions on Automatic Control, 61(8):2301–2307, Aug 2016. doi: 10.1109/TAC.2016.2583518. [29] Lillian J. Ratliff, Samuel A. Burden, and S. Shankar Sastry. Generictiy and Structural Stability of Non–Degenerate Differential Nash Equilibria. In Proc. 2014 Amer. Controls Conf., 2014. [30] Shankar Sastry. Nonlinear Systems. Springer New York, 1999. doi: 10.1007/978-1-4757-3108-8. [31] Suhail M Shah. Stochastic approximation on riemannian manifolds. arXiv, November 2017. [32] G. Thoppe and V. S. Borkar. A concentration bound for stochastic approximation via alekseev’s formula. arXiv:1506.08657v3, 2018. [33] Karl Tuyls, Julien Pe´rolat, Marc Lanctot, Georg Ostrovski, Rahul Savani, Joel Z Leibo, Toby Ord, Thore Graepel, and Shane Legg. Symmetric decomposition of asymmetric games. Scientiﬁc Reports, 8(1):1015, 2018. doi: 10.1038/s41598-018-19194-4.
26

