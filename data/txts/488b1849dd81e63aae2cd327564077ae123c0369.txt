Distributed Methods with Absolute Compression and Error Compensation∗

Marina Danilova1,2

Eduard Gorbunov2

1Institute of Control Sciences of RAS, Moscow, Russia 2Moscow Institute of Physics and Technology, Moscow, Russia

arXiv:2203.02383v1 [math.OC] 4 Mar 2022

Abstract
Distributed optimization methods are often applied to solving huge-scale problems like training neural networks with millions and even billions of parameters. In such applications, communicating full vectors, e.g., (stochastic) gradients, iterates, is prohibitively expensive, especially when the number of workers is large. Communication compression is a powerful approach to alleviating this issue, and, in particular, methods with biased compression and error compensation are extremely popular due to their practical eﬃciency. Sahu et al. (2021) [29] propose a new analysis of Error Compensated SGD (EC-SGD) for the class of absolute compression operators showing that in a certain sense, this class contains optimal compressors for EC-SGD. However, the analysis was conducted only under the so-called (M, σ2)-bounded noise assumption. In this paper, we generalize the analysis of EC-SGD with absolute compression to the arbitrary sampling strategy and propose the ﬁrst analysis of EC-LSVRG [9] with absolute compression for (strongly) convex problems. Our rates improve upon the previously known ones in this setting. Our theoretical ﬁndings are corroborated by several numerical experiments.
1 Introduction
In the recent few years, distributed optimization methods has been receiving a lot of attention from various research communities and, especially, from the machine learning one. This can be explained by the need of training deep learning models with billions of parameters on the hundreds of gigabytes of data [4] (and sometimes even this is not enough [15]). Clearly, such problems cannot be solved in a reasonable time on a single yet powerful machine [22]. Next, distributed methods are literally the only possible choices in such applications like Federated Learning (FL) [18, 19, 14], where the data is privately stored on multiple devices.
Due to the huge dimensions of corresponding problems and large number of workers in the networks na¨ıve methods like centralized Parallel SGD [34] suﬀer from the so-called communication bottleneck. This phenomenon means that a method spends much more time on communication rounds than on computations. A natural and popular way of addressing this issue is communication compression [31] – a technique that uses special compression operators called compressors applied to the information that devices send through the network.
The works on distributed methods with compression usually focus either on unbiased compressors [1, 24, 13] like RandK or 2-quantization (e.g., see [3]) or on biased compressors [31, 32, 3, 9]
∗The research was supported by Russian Science Foundation (project No. 22-21-00707).
1

like TopK. Although the world of unbiased compressors has richer theory, the methods with biased compressors are very popular due to their eﬃciency in practice. However, to make them convergent one has to apply additional tricks on top of SGD, e.g., error-compensation [31, 32, 33, 3].
Error Compensated SGD (EC-SGD) was proposed in [31] where the authors demonstrated its eﬃciency in practical tasks, but the ﬁrst theoretical analysis of EC-SGD was given in [32] and tightened in [33]. This analysis was extended in various directions including (but not limited to) decentralized communications [17], arbitrary sampling and variance reduction (with the ﬁrst linearly convergent variants) [9], acceleration [27], and also some prominent alternatives were proposed [13, 28]. However, the compression operators in these papers are usually assumed to be δ-contractive.1
Recently, the authors of [29] developed a new analysis of EC-SGD with absolute compressors, i.e., such (stochastic) operators C that for some ∆ ≥ 0 the inequality E[ C(x) − x 2] ≤ ∆2 holds for all x ∈ Rd. In particular, they proved that this class contains special operators called hard-threshold sparsiﬁers that are optimal in view of total error minimization (a special quantity arising in the analysis of EC-SGD) for any ﬁxed sequence of errors. Moreover, the authors of [29] derived convergence rates for EC-SGD with absolute compressors under (M, σ2)-bounded noise assumption and illustrate the theoretical and practical beneﬁts of absolute compressors compared to δ-contractive ones. However, several fruitful directions were unexplored for EC-SGD with absolute compressor including more general analysis of the standard version of the method and variants with variance reduction.
1.1 Main Contributions
Uniﬁed analysis of EC-SGD with absolute compressors. We propose a generalized analysis of EC-SGD with absolute compression covering diﬀerent stochastic estimators under various assumptions. In particular, we consider the simpliﬁed version of the parametric assumption from [9] (Assumption 2.1) and derive a general result on the convergence of ECSGD with absolute compressors (Theorem 2.2). The considered assumption covers various setups including the one considered in [29] and the derived result gives sharp rates.
EC-SGD with absolute compression and arbitrary sampling. To illustrate the ﬂexibility of our approach, we propose the ﬁrst analysis of EC-SGD with absolute compression and arbitrary sampling. The derived bounds are superior to the ones from [29] under certain assumptions.
EC-LSVRG with absolute compression and arbitrary sampling. As a special case of our theoretical framework, we obtain the analysis of EC-LSVRG [9] with absolute compression. Moreover, in contrast to [9], we handle non-uniform sampling in EC-LSVRG. The derived rate for EC-LSVRG with absolute compression has the leading term proportional to 1/K2, while the leading term for EC-SGD is proportional to 1/K.
Numerical experiments. We conduct several numerical experiments to support our theory and compare the performance of EC-LSVRG with hard-threshold and TopK sparsiﬁers. The numerical results corroborate our theoretical ﬁndings and highlight the beneﬁts of using absolute compressors.
1The mapping (possibly stochastic) C : Rd → Rd is called δ-contractive compressor if there exists δ ∈ (0, 1] such that E[ C(x) − x 2] ≤ (1 − δ) x 2 for all x ∈ Rd.
2

Algorithm 1 Error-Compensated Stochastic Gradient Descent (EC-SGD)

Input: starting point x0, stepsize γ > 0, number of iterations K ≥ 0

1: Set e0i = 0 for all i = 1, . . . , n 2: for k = 0, . . . , K − 1 do

3: Server broadcasts xk to all workers

4: for i = 1, . . . , n in parallel do do 5: Compute stochastic gradient gik and send vik = γC eki +γγgik to the server

6:

Update error-vector: eki +1 = eki + γgik − vik

7: end for

8:

Server

gathers

v

k 1

,

v2k

,

.

.

.

,

v

k n

from

all

workers

and

computes

vk

=

1 n

n i=1

vik

9: Set xk+1 = xk − vk

10: end for

1.2 Preliminaries

Problem. We consider a classical centralized optimization problem

n

min f (x) = n1 fi(x) ,

(1)

x∈Rd i=1

where the information deﬁning diﬀerentiable functions f1, . . . , fn : Rd → R is distributed among n workers/clients/devices connected with parameter-server in a centralized way. In particular,
(stochastic) gradients of function fi are available to client i only. Throughout the work, we assume that the solution x∗ of problem (1) is unique and that the function f is convex and µ-quasi strongly
convex [25], where the later is a relaxation of strong convexity meaning that

∀x ∈ Rd f (x∗) ≥ f (x) + ∇f (x), x∗ − x + µ2 x − x∗ 2, µ ≥ 0.

(2)

Moreover, we assume that f (x) is L-smooth, i.e., ∇f (x) − ∇f (y) ≤ L x − y x, y ∈ Rd. Compression. In this work, we focus on absolute compression operators.

holds for all

Deﬁnition 1.1. The mapping (possibly stochastic) C : Rd → Rd is called absolute compression operator/absolute compressor if there exists ∆ ≥ 0 such that

E C(x) − x 2 ≤ ∆2, ∀x ∈ Rd.

(3)

An example of absolute compressor is hard-threshold sparsiﬁer [29] deﬁned as [CHT(x)]i = [x]i if |[x]i| ≥ λ and [CHT(x)]i = 0 otherwise for some λ ≥ 0, whe√re [·]i denotes the i-th component of the vector. One can show that CHT(x) satisﬁes (3) with ∆ = λ d. Other examples include (stochatsic) rounding schemes with bounded error [11] and scaled integer rounding [30].

2 Uniﬁed Analysis
In our analysis, we rely on a simpliﬁed version of Assumption 3.3 from2 [9].
2The uniﬁed analysis of stochastic ﬁrst-order methods was proposed in [7] for quasi strongly convex problems. After that, this idea was extended to the case of convex functions [16], methods with error feedback [9] and local

3

Assumption 2.1 (Key Parametric Assumption). For all k ≥ 0 the average of stochastic gradients

used in EC-SGD is an unbiased estimate of ∇f (xk), i.e., for gk = n1

n i=1

gik

we

have

Ek [g k ]

=

∇f (xk) for all k ≥ 0, where Ek[·] denotes an expectation w.r.t. the randomness coming from

iteration k. Moreover, there exist non-negative parameters A, B, C, D1, D2 ≥ 0, ρ ∈ (0, 1], and

sequence of (possibly random) variables {σk2}k≥0 such that for all k ≥ 0 the iterates produced by

EC-SGD and the objective function f satisfy

Ek gk 2 ≤ 2A f (xk) − f (x∗) + Bσk2 + D1,

(4)

Ek σk2+1 ≤ (1 − ρ)σk2 + 2C f (xk) − f (x∗) + D2.

(5)

As it is shown in [9], the above assumption is very general and covers various algorithms in
diﬀerent settings. In Section 3 and 4, we consider two particular examples when Assumption 2.1 is
satisﬁed. In all known special cases, parameters A and C are typically related to the smoothness properties of the problem, σk2 describes the variance reduction process (with “rate” ρ), D1 and D2 are remaining noises not handled by variance reduction, and B is some constant.
Under Assumption 2.1 we derive the following result in Appendix A.

Theorem 2.2. Let function f be convex, µ-quasi strongly convex (with unique solution x∗), Lsmooth, and Assumption 2.1 hold. Assume that 0 < γ ≤ 1/4(A+CF ), where F = 4B/3ρ. Then, for all K ≥ 0 the iterates produced by EC-SGD with absolute compression operator C (see Deﬁnition 1.1) satisfy

E f (xK ) − f (x∗) ≤ (1−η)K+γ12E[T0] +2γ D1 +F D2 + 3Lγ∆2 , if µ > 0, (6)

E f (xK ) − f (x∗) ≤ γ2(EK[T+01]) +2γ D1 + F D2 + 3Lγ∆2 , if µ = 0,

(7)

where xK = W1K Kk=0 wkxk, wk = (1 − η)−(k+1), η = min{γµ/2, ρ/4}, WK = T0 = x0 − x∗ 2 + F γ2σ02

K k=0

wk

,

and

Upper bounds (6) and (7) establish convergence to some neighborhood of the solution (in terms of the functional suboptimality). Applying Lemmas C.1 and C.2, we derive the convergence rates to the exact optimum.

Corollary 2.3. Let the assumptions of Theorem 2.2 hold. Then, there exist choices of the stepsize γ such that EC-SGD with absolute compressor guarantees E[f (xK) − f (x∗)] of the order

O (A+CF )E[Tˆ0] exp − min A+µCF , ρ K + D1+µFKD2 + µL2∆K22 , (µ > 0)

(8)

O (A+CF )R02 + R02√B√E[σ02] + R02(D1+F D2) + L1/3R04/3∆2/3 , (µ = 0) (9)

K

Kρ

K

K 2/3

when µ = 0, where R0 =

x0 − x∗

,

Tˆ0

=

R02

+

F 16(A+C

F

)2

σ02

.

This general result allows to obtain the convergence rates for all methods satisfying Assumption 2.1 via simple plugging of parameters A, B, C, D1, D2 and ρ in the upper bounds (8) and (9).
updates [8], and to the methods for solving variational inequalities and min-max problems [6, 2].

4

For example, due to such a ﬂexibility we recover the results from [29], where the authors assume

that each fi is convex and Li-smooth, f is µ-quasi strongly convex3, and stochastic gradients have

(M, σ2)-bounded noise, i.e., Ek[ gik − ∇fi(xk) 2] ≤ M ∇fi(xk) 2 + σ2 for all i ∈ [n]. In the proof of their main results (see inequality (24) in [29]), they derive an upper-bound for Ek[ gk 2] im-

plying that Assumption 2.1 is satisﬁed in this case with A = L + M maxni∈[n] Li , B = 0, σk2 ≡ 0,

D1

=

2M ζn∗2+σ2 ,

C

=

0,

D2

=

0,

where

ζ∗2

=

1 n

n i=1

∇fi(x∗) 2 measures the heterogeneity of local

loss functions at the solution. Plugging these parameters in Corollary 2.3 we recover4 the rates

from [29]. In particular, when µ > 0 the rate is

O

(L

+

M

maxi∈[n] n

Li

)R02

exp

−

M maµxi∈[n] Li K + 2Mµζn∗2K+σ2 + µL2∆K22 .

L+

n

Below we consider two other examples when Assumption 2.1 is satisﬁed.

(10)

3 Absolute Compression and Arbitrary Sampling

Consider

the

case

when

each

function

fi

has

a

ﬁnite-sum

form,

i.e.,

fi(x)

=

1 m

m j=1

fij (x),

which

is

a classical situation in distributed machine learning. Typically, in this case, workers sample (e.g.,

uniformly with replacement) some batch of functions from their local ﬁnite-sums to compute the

stochastic gradient. To handle a wide range of sampling strategies, we follow [10] and consider a

stochastic reformulation of the problem:

f (x) = Eξ∼D [fξ(x)] ,

n

fξ (x)

=

1 n

fξi (x),

i=1

m

fξi (x)

=

1 m

ξij fij (x),

j=1

(11)

where ξ = (ξ1 , . . . , ξn ) and random vector ξi = (ξi1, . . . , ξim) deﬁnes the sampling strategy with distribution Di such that E[ξij] = 1 for all i ∈ [n], j ∈ [m]. We assume that functions fξi(x) satisfy expected smoothness property [10, 9].

Assumption 3.1 (Expected Smoothness). Functions f1, . . . , fn are L-smooth in expectation w.r.t. distributions D1, . . . , Dn. That is, there exists constant L > 0 such that for all x ∈ Rd and
for all i = 1, . . . , n

Eξi∼Di ∇fξi (x) − ∇fξi (x∗) 2 ≤ 2L (fi(x) − fi(x) − ∇fi(x∗), x − x∗ ) .

One can show that this assumption (and reformulation itself) covers for a wide range of situations

[10]. For example, when all functions fij are convex and Lij-smooth, then for the classical uniform

sampling

we

have

P{ξi

=

mej }

=

1 m

and

L

=

LUS

=

maxi∈[n],j∈[m] Lij ,

where

ej

∈

Rm

denotes

the

j-th vector in the standard basis in Rm. Moreover, importance sampling P{ξi = mLLiji ej} = mLLiji ,

where

Li

=

1 m

m i=j

Lij ,

also

ﬁts

Assumption

3.1

with

L

=

LIS

=

maxi∈[n] Li,

which

can

be

signiﬁcantly smaller than LUS.

3Although the authors of [29] write in Section 3.1 that all fi are µ-strongly convex, in the proofs, they use convexity
of fi and quasi-strong monotonicity of f . 4When µ = 0, our result is tighter than the corresponding one from [29].

5

Next, it is worth mentioning that Assumption 3.1 and (M, σ2)-bounded noise assumption used
in [29] cannot be compared directly, i.e., in general, none of them is stronger than another. However, in contrast to (M, σ2)-bounded noise assumption, Assumption 3.1 is satisﬁed whenever fij(x) are convex and smooth.
Consider a special case of EC-SGD applied to the stochastic reformulation (11), i.e., let gik = ∇fξk (xk), where ξik is sampled from Di independently from previous steps and other workers. Since
i
this version of EC-SGD supports arbitrary sampling we will call it EC-SGD-AS. In this setup, we show5 that EC-SGD-AS ﬁts Assumption 2.1 (the proof is deferred to Appendix B).

Proposition 3.2. Let f be L-smooth, fi have ﬁnite-sum form, and Assumption 3.1 hold. Then

the iterates produced by EC-SGD-AS satisfy Assumption 2.1 with A = L + 2L/n, B = 0, D1 =

2nσ∗2 = n22

n i=1

E[

∇fξi (x∗) − ∇fi(x∗)

2],

σk2

≡ 0,

ρ = 1,

C

= 0,

D2

= 0.

Plugging the parameters from the above proposition in Theorem 2.2 and Corollary 2.3, one can derive convergence guarantees for EC-SGD-AS with absolute compression operator. In particular, our general analysis implies the following result.

Theorem 3.3. Let the assumptions of Proposition 3.2 hold. Then, there exist choices of stepsize 0 < γ ≤ (4L+8L/n)−1 such that EC-SGD-AS with absolute compressor guarantees E[f (xK)−f (x∗)] of the order

O L + Ln R02 exp − L+µL/ K + µσn∗2K + µL2∆K22 , n

O (L+LK/n)R02 +

+ , σ∗2R02
nK

L1/3 R04/3 ∆2/3 K 2/3

when µ > 0, when µ = 0.

(12) (13)

Consider the case when µ > 0 (similar observations are valid when µ = 0). In these settings, Assumption 3.1 is satisﬁed whenever fij are convex and smooth without assuming (M, σ2)-bounded noise assumption used in [29]. Moreover, our bound (12) has better O(1/K) decaying term than bound (10) from [29]. In particular, when σ∗2 = 0, i.e., workers compute full gradients our bound has O(1/K2) decaying leading term while for (10) the leading term decreases as O(1/K), when ζ∗2 = 0 (local functions has diﬀerent optima). Next, when µ > 0, the best-known bound for EC-SGD-AS
for δ-contractive compressors is (see [9])

O AR02 exp − Aµ K + µσn∗2K + L(δσµ∗22+Kζ∗22/δ) ,

(14)

√ where A = L+ maxi∈[n] Li+ δδ maxi∈[n] LiL (this parameter can be tightened using the independence of the samples on diﬀerent workers, which we use in our proofs). The second terms from (12) and (14)
are the same while the third terms are diﬀerent. Although, these results are derived for diﬀerent
classes of compressors, one can compare them for particular choices of compressions. In particular,
for hard-threshold and Top1 compressors the third term in (12) is proportional to dλ/K2, while the corresponding term from (14) is proportional to (dσ∗2+d2ζ∗2)/K2. When λ = O(1) (e.g., see Fig. 1) and ζ∗2 is large enough the bound (12) is more than d times better than (14).

5Proposition 3.2 is a reﬁned version of Lemma J.1 from [9].

6

4 Absolute Compression and Variance Reduction

In the same setup as in the previous section, we consider a variance-reduced version of EC-SGD called Error Compensated Loopless Stochastic Variance-Reduced Gradient (EC-LSVRG) from [9]. This method is a combination of LSVRG [12, 20] and EC-SGD and can be viewed as Algorithm 1 with

gik = ∇fξk (xk) − ∇fξk (wk) + ∇fi(wk)

i

i

(15)

wk+1 = xk, with probability p, w0 = x0, (16) wk, with probability 1 − p,

where ξik is sampled from Di independently from previous steps and other workers, and probability p of updating wk is usually taken as p ∼ 1/m. Such choice of p ensures that full gradients ∇fi(wk) are computed rarely meaning that the expected number of ∇fξi(x) computations per iteration is the same as for EC-SGD-AS (up to the constant factor). We point out that EC-LSVRG was studied for the contractive compressors and uniform sampling [9] (although it is possible to generalize the proofs from [9] to cover EC-LSVRG with arbitrary sampling as well). As we show next, EC-LSVRG with arbitrary sampling satisﬁes Assumption 2.1 (the proof is deferred to Appendix B).

Proposition 4.1. Let f be L-smooth, fi have ﬁnite-sum form, and Assumption 3.1 hold. Then the iterates produced by EC-LSVRG satisfy Assumption 2.1 with A = L + 2nL , B = n2 , D1 = 0, σk2 = 2L(f (wk) − f (x∗)), ρ = p, C = pL, D2 = 0.
Due to the variance reduction, noise terms D1 and D2 equal zero for EC-LSVRG allowing the method to achieve better accuracy with constant stepsize than EC-SGD-AS. Plugging the parameters from the above proposition in Theorem 2.2 and Corollary 2.3, one can derive convergence guarantees for EC-LSVRG with absolute compression operator. In particular, our general analysis implies the following result.

Theorem 4.2. Let the assumptions of Proposition 4.1 hold. Then, there exist choices of step-
size 0 < γ ≤ γ0 = (4L + 152L/3n)−1 such that EC-LSVRG with absolute compressor guarantees E[f (xK ) − f (x∗)] of the ordera

O L + Ln T0 exp − min L+µL/n , m1 K + µL2∆K22 ,

(L+L/n)R2

√ mLLR2

L1/3 R4/3 ∆2/3

O

K

0+

√ nK

0

+

0
K 2/3

,

when µ > 0, when µ = 0,

(17) (18)

where T0 = x0 − x∗ 2 + 634nm γ02L(f (x0) − f (x∗)).
aWe take into account that due to L-smoothness of f we have T0 = (1 + 32mLLγ2 n) x0 − x∗ 2.
3

x0 − x∗ 2 + 634nm γ2L(f (x0) − f (x∗)) ≤

Consider the case when µ > 0 (similar observations are valid when µ = 0). As expected, the bound (17) does not have terms proportional to any kind of variance of the stochastic estimator. Therefore, the leading term in the complexity bound for EC-LSVRG decreases as O(1/K2), while EC-SGD-AS has O(1/K) leading term. Next, in case of δ-contractive compression operator the

7

only known convergence rate for EC-LSVRG [9] has the leading term O( δ2Lµ2ζ∗K2 2 ). Using the same arguments as in the discussion after Theorem 3.3, one can that the leading term in the case of
hard-threshold sparsiﬁer can be more than d times better than the leading term in the case of Top1
compressor.

5 Numerical Experiments

We conduct several numerical experiments to support our theory, i.e., we tested the methods on the distributed logistic regression problem with 2-regularization:

nm

min f (x) = n1m

ln (1 + exp (−yi

aij, x

)) +

l2 2

x

2

,

x∈Rd

i=1 j=1

fij (x)

(19)

where vectors {aij}i∈[n],j∈[m] ∈ Rd are the columns of the matrix of features A , {yi}ni=1 ∈ {−1, 1} are labels, and l2 ≥ 0 is a regularization parameter. One can show that fij is l2-strongly convex and Lij-smooth, and f is L-smooth with Lij = l2 + aij 2/4 and L = l2 + λmax(A A)/4nm, where λmax(A A) is the largest eigenvalue of A A. In particular, we took 3 datasets – a9a (nm = 32000, d = 123)6, gisette (nm = 6000, d = 5000), and w8a (nm = 49700, d = 300) – from LIBSVM library [5]. Out code is based on the one from [9]. Each dataset was shuﬄed and equally split
between n = 20 workers. In the ﬁrst two experiments, we use only hard-threshold sparsiﬁer and
in the third experiment, we also consider TopK compressor. The results are presented in Fig. 1.
The methods were run for S epochs, where the values of S are given in the titles of corresponding
plots. We compare the methods in terms of total number of bits each worker sends to the server
(on average).

Experiment 1: EC-SGD with and without importance sampling. In this experiment, we
tested EC-SGD with hard-threshold sparsiﬁer and two diﬀerent sampling strategies: uniform sampling (US) and importance sampling (IS), described in Section 3. We chose l2 = 10−4 · maxi∈[n] Li. Stepsize was chosen as γUS = (L + n−1 maxi∈[n],j∈[m] Lij)−1 and γIS = (L + n−1 maxi∈[n] Li)−1 for the case of US and IS respectively, which are the multiples of the maximal stepsizes that
our theory allows for both cases. Following [29], we took parameter λ as λ = 5000 ε/d2γUS for ε = 10−3. We observe that EC-SGD behaves similarly in both cases for a9a (L ≈ 1.57, maxi∈[n] Li ≈ 3.47, maxi∈[n],j∈[m] Lij ≈ 3.5) and gisette (L ≈ 842.87, maxi∈[n] Li ≈ 1164.89, maxi∈[n],j∈[m] Lij ≈ 1201.51), while for w8a (L ≈ 0.66, maxi∈[n] Li ≈ 3.05, maxi∈[n],j∈[m] Lij ≈ 28.5) EC-SGD with IS achieves good enough accuracy much faster than with US. This is expected since
for w8a maxi∈[n] Li is almost 10 times smaller than maxi∈[n],j∈[m] Lij. That is, as our theory implies, importance sampling is preferable when maxi∈[n] Li maxi∈[n],j∈[m] Lij.

Experiment 2: EC-SGD vs EC-LSVRG. Next, we compare EC-SGD and EC-LSVRG to illustrate the beneﬁts of variance reduction for error-compensated methods with absolute compression. Both methods were run with stepsize γ = /1 maxi∈[n],j∈[m] Lij and λ = 5000 ε/d2γ for ε = 10−3. In all cases, EC-LSVRG achieves better accuracy of the solution than EC-SGD that perfectly corroborates our theoretical results.
6We take the ﬁrst 32000 and 49700 samples from a9a and w8a to get a multiple of n = 20.

8

f(xk) f(x * )

f(xk) f(x * )

101

a9a, S = 20, = 1.7
EC-SGD-US with HT

EC-SGD-IS with HT

100

10 1

10 2
0
101
100
10 1
10 2
10 3 0.0
101 100 10 1 10 2 10 3 10 4 10 5
0.0

20N0u0m0b0er4o0f0b0it0s0pe6r0w0o0r0k0er 800000 a9a, S = 50, = 2.41
EC-SGD HT EC-L-SVRG HT
0.N2umb0e.4r of b0it.s6per0w.8orker1.0 11e.62 a9a, S = 30, = 0.96
EC-L-SVRG Top-1 EC-L-SVRG HT
0.5Num1b.e0r of 1b.i5ts pe2r.w0ork2er.5 31.0e6

f(xk) f(x * )

f(xk) f(x * )

f(xk) f(x * )

101

gisette, S = 10, = 0.95
EC-SGD-US with HT

EC-SGD-IS with HT

100

10 1

10 2

10 3

0.00 0.25Nu0m.5b0er0.o7f5bi1ts.0p0er1w.2o5rk1e.r50 1.715e7

101

gisette, S = 150, = 1.1
EC-SGD HT

100

EC-L-SVRG HT

10 1

10 2

10 3

10 4

10 5

10 6 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00

Number of bits per worker 1e8

101

gisette, S = 150, = 5.48
EC-L-SVRG Top-50

100

EC-L-SVRG HT

10 1

10 2

10 3

10 4

10 5

10 6

0.0 0.2Nu0m.4ber0o.f6bit0s.p8er 1w.o0rke1r.2 11.4e8

f(xk) f(x * )

f(xk) f(x * )

f(xk) f(x * )

w8a, S = 3, = 0.76

101

EC-SGD-US with HT

EC-SGD-IS with HT

100

10 1

10 2
0
101 100 10 1 10 2 10 3 10 4 10 5
0.0
101 100 10 1 10 2 10 3 10 4 10 5 10 6 0

200N00um4b00e0r0of6b0i0t0s0pe8r00w0o0rk1e0r0000 120000 w8a, S = 150, = 2.81
EC-SGD HT EC-L-SVRG HT
0.N2umb0e.r4of bit0s.6per w0o.r8ker 1.0 1e6 w8a, S = 150, = 2.81
EC-L-SVRG Top-3 EC-L-SVRG HT
1 Num2ber o3f bits4per w5orker6 71e7

Figure 1: Trajectories of EC-SGD with uniform and importance samplings sampling and hardthreshold sparsiﬁer, EC-LSVRG with hard-threshold and TopK sparsiﬁers. The row i of plots corresponds to Experiment i described in Section 5.

f(xk) f(x * )

Experiment 3: EC-LSVRG with hard-threshold and TopK sparsiﬁers. Finally, to highlight the beneﬁts of hard-threshold (HT) sparsiﬁer compared to TopK sparsiﬁer we tested EC-LSVRG with both compressors. We chose stepsize as γ = /1 maxi∈[n],j∈[m] Lij, K ≈ d/100 (see the legends of plots in row 3 from Fig. 1), and λ = α ε/d2γ, where ε = 10−3 and α = 2000 for a9a, α = 25000 for gisette, α = 5000 for w8a. We observe that EC-LSVRG with HT achieves a reasonable accuracy (10−3 for a9a, 10−4 for gisette, and 10−5 for w8a) faster than EC-LSVRG with TopK for all datasets. In particular, EC-LSVRG with HT signiﬁcantly outperforms EC-LSVRG with TopK on w8a dataset.
References
[1] Dan Alistarh, Demjan Grubic, Jerry Li, Ryota Tomioka, and Milan Vojnovic. QSGD: Communication-eﬃcient SGD via gradient quantization and encoding. In Advances in Neural Information Processing Systems, pages 1709–1720, 2017.
[2] Aleksandr Beznosikov, Eduard Gorbunov, Hugo Berard, and Nicolas Loizou. Stochastic gradient descent-ascent: Uniﬁed theory and new eﬃcient methods. arXiv preprint arXiv:2202.07262, 2022.

9

[3] Aleksandr Beznosikov, Samuel Horv´ath, Peter Richt´arik, and Mher Safaryan. On biased compression for distributed learning. arXiv preprint arXiv:2002.12410, 2020.
[4] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877– 1901, 2020.
[5] Chih-Chung Chang and Chih-Jen Lin. LIBSVM: A library for support vector machines. ACM transactions on intelligent systems and technology (TIST), 2(3):1–27, 2011.
[6] Eduard Gorbunov, Hugo Berard, Gauthier Gidel, and Nicolas Loizou. Stochastic extragradient: General analysis and improved rates. arXiv preprint arXiv:2111.08611, 2021.
[7] Eduard Gorbunov, Filip Hanzely, and Peter Richt´arik. A uniﬁed theory of SGD: Variance reduction, sampling, quantization and coordinate descent. In The 23rd International Conference on Artiﬁcial Intelligence and Statistics (AISTATS 2020), 2020.
[8] Eduard Gorbunov, Filip Hanzely, and Peter Richt´arik. Local sgd: Uniﬁed theory and new eﬃcient methods. In International Conference on Artiﬁcial Intelligence and Statistics, pages 3556–3564. PMLR, 2021.
[9] Eduard Gorbunov, Dmitry Kovalev, Dmitry Makarenko, and Peter Richt´arik. Linearly converging error compensated sgd. Advances in Neural Information Processing Systems, 33, 2020.
[10] Robert Mansel Gower, Nicolas Loizou, Xun Qian, Alibek Sailanbayev, Egor Shulgin, and Peter Richt´arik. SGD: General analysis and improved rates. In International Conference on Machine Learning, pages 5200–5209, 2019.
[11] Suyog Gupta, Ankur Agrawal, Kailash Gopalakrishnan, and Pritish Narayanan. Deep learning with limited numerical precision. In International conference on machine learning, pages 1737– 1746. PMLR, 2015.
[12] Thomas Hofmann, Aurelien Lucchi, Simon Lacoste-Julien, and Brian McWilliams. Variance reduced stochastic gradient descent with neighbors. In Advances in Neural Information Processing Systems, pages 2305–2313, 2015.
[13] Samuel Horv´ath, Dmitry Kovalev, Konstantin Mishchenko, Sebastian Stich, and Peter Richt´arik. Stochastic distributed learning with gradient quantization and variance reduction. arXiv preprint arXiv:1904.05115, 2019.
[14] Peter Kairouz, H Brendan McMahan, Brendan Avent, Aur´elien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji, Keith Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, et al. Advances and open problems in federated learning. arXiv preprint arXiv:1912.04977, 2019.
[15] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeﬀrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020.
10

[16] Ahmed Khaled, Othmane Sebbouh, Nicolas Loizou, Robert M Gower, and Peter Richt´arik. Uniﬁed analysis of stochastic gradient methods for composite convex and smooth optimization. arXiv preprint arXiv:2006.11573, 2020.
[17] Anastasia Koloskova, Sebastian Stich, and Martin Jaggi. Decentralized stochastic optimization and gossip algorithms with compressed communication. In International Conference on Machine Learning, pages 3478–3487, 2019.
[18] Jakub Koneˇcny´, H Brendan McMahan, Felix X Yu, Peter Richt´arik, Ananda Theertha Suresh, and Dave Bacon. Federated learning: Strategies for improving communication eﬃciency. arXiv preprint arXiv:1610.05492, 2016.
[19] Jakub Koneˇcny´, H. Brendan McMahan, Felix Yu, Peter Richt´arik, Ananda Theertha Suresh, and Dave Bacon. Federated learning: strategies for improving communication eﬃciency. In NIPS Private Multi-Party Machine Learning Workshop, 2016.
[20] Dmitry Kovalev, Samuel Horv´ath, and Peter Richt´arik. Don’t jump through hoops and remove those loops: SVRG and Katyusha are better without the outer loop. In Proceedings of the 31st International Conference on Algorithmic Learning Theory, 2020.
[21] R´emi Leblond, Fabian Pedregosa, and Simon Lacoste-Julien. Improved asynchronous parallel optimization analysis for stochastic incremental methods. The Journal of Machine Learning Research, 19(1):3140–3207, 2018.
[22] Chuan Li. Openai’s gpt-3 language model: A technical overview. Blog Post, 2020.
[23] Horia Mania, Xinghao Pan, Dimitris Papailiopoulos, Benjamin Recht, Kannan Ramchandran, and Michael I Jordan. Perturbed iterate analysis for asynchronous stochastic optimization. SIAM Journal on Optimization, 27(4):2202–2229, 2017.
[24] Konstantin Mishchenko, Eduard Gorbunov, Martin Tak´aˇc, and Peter Richt´arik. Distributed learning with compressed gradient diﬀerences. arXiv preprint arXiv:1901.09269, 2019.
[25] Ion Necoara, Yu Nesterov, and Francois Glineur. Linear convergence of ﬁrst order methods for non-strongly convex optimization. Mathematical Programming, 175(1):69–107, 2019.
[26] Yurii Nesterov. Lectures on convex optimization, volume 137. Springer, 2018.
[27] Xun Qian, Peter Richt´arik, and Tong Zhang. Error compensated distributed sgd can be accelerated. arXiv preprint arXiv:2010.00091, 2020.
[28] Peter Richt´arik, Igor Sokolov, and Ilyas Fatkhullin. Ef21: A new, simpler, theoretically better, and practically faster error feedback. Advances in Neural Information Processing Systems, 34, 2021.
[29] Atal Sahu, Aritra Dutta, Ahmed M Abdelmoniem, Trambak Banerjee, Marco Canini, and Panos Kalnis. Rethinking gradient sparsiﬁcation as total error minimization. Advances in Neural Information Processing Systems, 34, 2021.
11

[30] Amedeo Sapio, Marco Canini, Chen-Yu Ho, Jacob Nelson, Panos Kalnis, Changhoon Kim, Arvind Krishnamurthy, Masoud Moshref, Dan Ports, and Peter Richtarik. Scaling distributed machine learning with in-network aggregation. In 18th USENIX Symposium on Networked Systems Design and Implementation (NSDI 21), pages 785–808, 2021.
[31] Frank Seide, Hao Fu, Jasha Droppo, Gang Li, and Dong Yu. 1-bit stochastic gradient descent and its application to data-parallel distributed training of speech dnns. In Fifteenth Annual Conference of the International Speech Communication Association, 2014.
[32] Sebastian U Stich, Jean-Baptiste Cordonnier, and Martin Jaggi. Sparsiﬁed SGD with memory. In Advances in Neural Information Processing Systems, pages 4447–4458, 2018.
[33] Sebastian U Stich and Sai Praneeth Karimireddy. The error-feedback framework: Better rates for sgd with delayed gradients and compressed updates. Journal of Machine Learning Research, 21:1–36, 2020.
[34] Martin Zinkevich, Markus Weimer, Lihong Li, and Alex Smola. Parallelized stochastic gradient descent. Advances in neural information processing systems, 23, 2010.

A Missing Proofs from Section 2

In the analysis, we use auxiliary iterates that are never computed explicitly during the work of the

method: x˜k = xk − ek, where ek = n1

n i=1

eki

.

These

iterates

are

usually

called

perturbed

or

virtual

iterates [21, 23]. They are used in many previous works on error feedback [32, 33, 9, 29]. One of

the key properties these iterates satisfy is the following recursion:

x˜k+1 = xk+1 − ek+1 = xk − vk − ek − γgk + vk = x˜k − γgk,

(20)

where we use ek+1 = ek + γgk − vk, which follows from eki +1 = eki + γgik − vik and deﬁnitions of ek, gk, and vk.

A.1 Proof of Theorem 2.2
Our proof is close to the ones from [9, 29]. Using the recursion (20) for the perturbed iterates, we obtain
x˜k+1 − x∗ 2 = x˜k − x∗ 2 − 2γ x˜k − x∗, gk + γ2 gk 2.
Next, we take conditional expectation Ek[·] from the above inequality and apply unbiasedness of gk and inequality (4) from Assumption 2.1:
(4)
Ek x˜k+1 − x∗ 2 ≤ x˜k − x∗ 2 − 2γ xk − x∗, ∇f (xk) + 2γ xk − x˜k, ∇f (xk) +γ2 2A f (xk) − f (x∗) + Bσk2 + D1 .

12

(2)
k − x∗, ∇f (xk) ≤ −(f (xk) − f (x∗)) − µ xk − x∗ 2

Since f is µ-quasi strongly convex, we have − x

2

implying

Ek x˜k+1 − x∗ 2 = x˜k − x∗ 2 − γµ xk − x∗ 2 + 2γ xk − x˜k, ∇f (xk)

−2γ(1 − Aγ) f (xk) − f (x∗) + Bγ2σk2 + γ2D1

(21)

To continue our derivation we need to upper bound −γµ xk − x∗ 2 and 2γ xk − x˜k, ∇f (xk) . Applying a − b 2 ≥ 21 a 2 − b 2, which holds for any a, b ∈ Rd, we get

−γµ xk − x∗ 2 ≤ − γ2µ x˜k − x∗ 2 + γµ x˜k − xk 2. (22)
To estimate the inner product, we use Fenchel-Young inequality a, b ≤ α2 a 2 + 21α b 2 holding for any a, b ∈ Rd, α > 0 together with standard inequality ∇f (xk) 2 ≤ 2L(f (xk) − f (x∗)), which holds for any L-smooth function f [26]:

2γ xk − x˜k, ∇f (xk) ≤ 2γL xk − x˜k 2 + 2γL ∇f (xk) 2

≤ 2γL x˜k − xk 2 + γ f (xk) − f (x∗) .

(23)

Plugging upper bounds (22) and (23) in (21), we derive

Ek x˜k+1 − x∗ 2 ≤ 1 − γ2µ x˜k − x∗ 2 − γ(1 − 2Aγ) f (xk) − f (x∗) +Bγ2σk2 + γ2D1 + γ(2L + µ) xk − x˜k 2
µ≤L
γµ x˜k − x∗ 2 − γ(1 − 2Aγ) f (xk) − f (x∗) ≤ 1− 2
+Bγ2σk2 + γ2D1 + 3Lγ ek 2.

Since the deﬁnition of ek+1 and Jensen’s inequality imply that for all k ≥ 0

ek+1 2 ≤

n
1 n

n
eki + γgik − vik 2 = γn2

eki +γγgik − C

eki +γgik γ

i=1

i=1

2
≤ γ2∆2,

we have (taking into account that e0 2 = 0 ≤ γ2∆2)

Ek x˜k+1 − x∗ 2 ≤ 1 − γ2µ x˜k − x∗ 2 − γ(1 − 2Aγ) f (xk) − f (x∗) +Bγ2σk2 + γ2D1 + 3Lγ3∆2.

Summing up the above inequality with F γ2-multiple of (5) and introducing new notation Tk = x˜k − x∗ 2 + F γ2σk2, we obtain

Ek [Tk+1] ≤

1 − γ2µ x˜k − x∗ 2 + F γ2 1 − ρ + BF σk2 −γ(1 − 2(A + CF )γ) f (xk) − f (x∗)

+γ2 D1 + F D2 + 3Lγ∆2
F =4B/3ρ
≤ (1 − η)Tk + γ2 D1 + F D2 + 3Lγ∆2 − γ2 f (xk) − f (x∗) ,

13

where in the last step we use 1 − γ2µ x˜k −x∗ 2 + 1 − ρ4 F γ2σk2 ≤ (1−η) x˜k − x∗ 2 + F γ2σk2 = (1 − η)Tk, where η = min{γµ/2, ρ/4}, and 0 < γ ≤ 1/4(A+CF ). Rearranging the terms and taking full
expectation, we derive

γ
2E

f (xk) − f (x∗)

≤ (1 − η)E[Tk] − E[Tk+1] + γ2

D1 + F D2 + 3Lγ∆2

.

Summing up the above inequality for k = 0, . . . , K with weights wk = (1 − η)−(k+1) and using

Jensen’s inequality f (xK ) ≤ W1K Kk=0 wkf (xk) for convex function f and point xK = W1K Kk=0 wkxk,

where WK =

K k=0

wk ,

and

wk

=

(1

−

η)wk−1,

we

get

E f (xK ) − f (x∗)

K
≤ γW2K (wk−1E[Tk] − wkE[Tk+1])
k=0

+2γ D1 + F D2 + 3Lγ∆2

≤ 2γEW[Tk0] + 2γ D1 + F D2 + 3Lγ∆2 ,

(24)

where in the last step we use

K k=0

(wk−1 E[Tk ]

−

wk E[Tk+1 ])

=

w−1E[T0]−wk+1E[TK+1]

≤

w−1E[T0]

=

E[T0]. Finally, it remains to notice that (24) implies (6) and (7). Indeed, when µ > 0, we have

WK ≥ wK = (1 − η)−(K+1), and when µ = 0, we have WK = K + 1, since η = 0.

B Missing Proofs from Sections 3 and 4

B.1 Proof of Proposition 3.2
To prove Proposition 3.2 we need to derive an upper bound for Ek[ gk 2]. Independence of ξ1k, . . . , ξnk for ﬁxed history, variance decomposition, and standard inequality ∇f (xk) 2 ≤ 2L(f (x) − f (x∗)) [26] imply

Ek gk 2

= Ek gk − ∇f (xk) 2 + ∇f (xk) 2


n

2

≤

Ek 

1 n

∇fξk (xk) − ∇fi(xk)  + 2L f (xk) − f (x∗) i

i=1

n

=

1 n2

Ek

i=1

∇fξk (xk) − ∇fi(xk) 2 + 2L f (xk) − f (x∗) i

n

≤

2 n2

Ek

i=1

∇fξk (xk) − ∇fξk (x∗) − (∇fi(xk) − ∇fi(x∗)) 2

i

i

n

+

2 n2

Ek

i=1

∇fξk (x∗) − ∇fi(x∗) 2 +2L f (xk) − f (x∗) , i

2σ∗2/n

14

where in the last step we use that a + b 2 ≤ 2 a 2 + 2 b 2 for all a, b ∈ Rd. Since the variance is upper-bounded by the second moment, we have

Ek gk 2

n

≤

2 n2

Ek

i=1

∇fξik (xk) − ∇fξik (x∗) 2 + 2L f (xk) − f (x∗) + 2nσ∗2

≤ 2 L + 2nL f (xk) − f (x∗) + 2nσ∗2 ,

where the second inequality follows from Assumption 3.1 and f (x) = n1

n i=1

fi

(x).

The de-

rived inequality implies that Assumption 2.1 holds with A = L + 2L/n, B = 0, D1 = 2nσ∗2 =

2 n2

n i=1

E[

∇fξi (x∗) − ∇fi(x∗)

2],

σk2

≡ 0,

ρ = 1,

C

= 0,

D2

= 0.

B.2 Proof of Proposition 4.1

We start with deriving an upper-bound for Ek[ gk 2]. Similarly to the proof of Proposition 3.2, we

use

independence

of

ξ

k 1

,

.

.

.

,

ξ

k n

for

ﬁxed

history,

variance

decomposition,

and

standard

inequality

∇f (xk) 2 ≤ 2L(f (x) − f (x∗)) [26]:

Ek gk 2

= Ek gk − ∇f (xk) 2 + ∇f (xk) 2


n

2

≤

Ek 

1 n

∇fξk (xk) − ∇fξk (wk) + ∇fi(wk) − ∇fi(xk) 

i

i

i=1

+2L f (xk) − f (x∗)

n

=

1 n2

Ek

i=1

2

∇fξk (xk) − ∇fξk (wk) − (∇fi(xk) − ∇fi(wk))

i

i

+2L f (xk) − f (x∗) .

Since the variance is upper-bounded by the second moment and a + b 2 ≤ 2 a 2 + 2 b 2 for all a, b ∈ Rd, we have

Ek gk 2

n

≤

1 n2

Ek

i=1

2

∇fξk (xk) − ∇fξk (wk) + 2L f (xk) − f (x∗)

i

i

n

≤

2 n2

Ek

i=1

2

∇fξk (xk) − ∇fξk (x∗)

i

i

n
+ n22 Ek
i=1

2

∇fξk (wk) − ∇fξk (x∗) + 2L f (xk) − f (x∗)

i

i

≤ 2 L + 2nL f (xk) − f (x∗) + 4L f (wk) − f (x∗) ,

(25)

2σk2/n

15

where the third inequality follows from Assumption 3.1 and f (x) = n1 deﬁnitions of σk2+1 and wk+1, we derive an upper bound for Ek[σk2+1]:

n i=1

fi

(x).

Using the

Ek[σk2+1] = 2LEk f (wk+1) − f (x∗)

= (1 − p) 2L f (wk) − f (x∗) +2pL f (xk) − f (x∗) .

(26)

σk2

Inequalities (25) and (26) imply that Assumption 2.1 holds with A = L + 2L/n, B = 2/n, D1 = 0, σk2 = 2L(f (wk) − f (x∗)), ρ = p, C = pL, D2 = 0.

C Auxiliary Results

We use the following lemmas to derive ﬁnal convergence rates.

Lemma C.1 (Lemma I.2 from [8]). Let sequence {rk}k≥0 satisfy rK ≤ (1 − η)K γa + c1γ + c2γ2 for all K ≥ 0, η = min{γµ/2, ρ/4}, where µ > 0, ρ ∈ (0, 1], with some constants a, c1, c2 ≥ 0 and
0 < γ ≤ 1/h. Then for all K > 0 such that

and for the stepsize

either or

ln(max{2,min{aµ2K2/4c1,aµ3K3/8c2}})

K

≤ρ

1 ≤ ln(max{2,min{aµ2K2/4c1,aµ3K3/8c2}})

h

µK

γ = min

1 , ln(max{2,min{aµ2K2/4c1,aµ3K3/8c2}})

h

µK

(27)

we have that

rK = O

ha exp − min

µh , ρ

K

+

c1 µK

+

c2 µ2 K 2

.

(28)

Lemma

C.2

(Lemma

I.3

from

[8]).

Let

sequence

{rk }k≥0

satisfy

rK

≤

a γK

+

bγ K

+ c1γ + c2γ

for

all K > 0 with some constants a, b, c1, c2 ≥ 0 and 0 < γ ≤ 1/h. Then for all K > 0 and

γ = min h1 , ab , c1aK , 3 c2aK

(29)

we have that

rK = O

√
ha+K ab +

√ aKc1 + 3Ka22/3c2 .

(30)

16

