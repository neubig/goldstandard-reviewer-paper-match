MICROSOFT SPEAKER DIARIZATION SYSTEM FOR THE VOXCELEB SPEAKER RECOGNITION CHALLENGE 2020
Xiong Xiao, Naoyuki Kanda, Zhuo Chen, Tianyan Zhou, Takuya Yoshioka Sanyuan Chen, Yong Zhao, Gang Liu, Yu Wu, Jian Wu, Shujie Liu, Jinyu Li, Yifan Gong
Microsoft, USA

arXiv:2010.11458v2 [eess.AS] 23 Oct 2020

ABSTRACT
This paper describes the Microsoft speaker diarization system for monaural multi-talker recordings in the wild, evaluated at the diarization track of the VoxCeleb Speaker Recognition Challenge (VoxSRC) 2020. We will ﬁrst explain our system design to address issues in handling real multi-talker recordings. We then present the details of the components, which include Res2Net-based speaker embedding extractor, conformer-based continuous speech separation with leakage ﬁltering, and a modiﬁed DOVER (short for Diarization Output Voting Error Reduction) method for system fusion. We evaluate the systems with the data set provided by VoxSRC challenge 2020, which contains real-life multi-talker audio collected from YouTube. Our best system achieves 3.71% and 6.23% of the diarization error rate (DER) on development set and evaluation set, respectively, being ranked the 1st at the diarization track of the challenge.
Index Terms— speaker diarization, speaker recognition, speech separation, system fusion.
1. INTRODUCTION
Speaker diarization is the task of determining “who spoke when” given a long audio signal [1]. It is an imporant component for audio analysis and has a wide range of application domains, such as broadcast news, meetings, and telephone conversations. It can also be used to improve automatic speech recognition in multi-speaker conversation scenarios [2, 3].
There have been tremendous efforts for improving speaker diarization systems. A speaker diarization system typically consists of several modules, including voice activity detection (VAD), speech segmentation, speaker embedding extraction, and speaker clustering. Each module has been extensively studied for different purposes such as speaker embedding [4–9] and speaker clustering [10–13]. There has also been an international effort to ﬁnd out best practices that would work for a diverse set of recordings [14, 15]. Despite these advances, speaker diarization for real recordings still remains to be challenging problem.
Difﬁculty of speaker diarization for real world recordings arises from (1) diversity of speaker characteristics and (2) adverse acoustic conditions, which often contain overlapping utterances (or simultaneously active speakers). Especially, the speech overlaps have sometimes been excluded from the system design as well as the evaluation metrics (e.g., [11–13]) due to the difﬁculty in handling them. However, speech overlaps are frequently observed in real conversations. The overlap ratio (the percentage of the time during which more than one person speaking) ranges from 10% to 30% for meetings [16], and it can become higher for daily conversations [2, 17, 18]. Recently proposed neural network-based diarization systems, such as

Fig. 1. System Diagram
end-to-end neural diarization [19, 20] and target-speaker voice activity detection [3], were shown to be effective for overlapped speech. However, they have a limitation that the maximum number of recognizable speakers is constrained by the number of output channels of the neural networks. It is also not clear whether these model-based diarization techniques generalize to unseen conditions.
With this as a background, we propose a speaker diarization system that consists of continuous speech separation (CSS), speaker embedding extraction, segmentation, speaker clustering, and system fusion as shown in Fig. 1. The prominent components of our system can be summarized as follows.
Conformer-based CSS We develop a highly optimized CSS system based on the conformer network [21, 22] to address the speaker overlap problem.
Res2Net-based speaker embedding extractor We incorporate Res2Net architecture [23] with additive margin Softmax loss [24] to train our speaker embedding extractor, which enables highly accurate speaker clustering.
Speaker clustering with leakage ﬁltering A speaker clustering with a leakage ﬁltering method is also proposed to reduce the false alarm due to the residual noise and music in the output channels of the speech separation. The leakage ﬁltering is essential to achieve a signiﬁcant improvement by speech separation.
System combination with a modiﬁed DOVER We fuse the outputs of multiple diarization systems by using a novel voting-based algorithm, called modiﬁed DOVER, which is an extension of DOVER [25] to handle overlapped speech.
Next section brieﬂy describes the dataset used in the diarization track of VoxSRC challenge 2020. Section 3 presents the proposed system. Section 4 reports the evaluation results which are the best result in the diarization track of VoxSRC 2020. The last section concludes the paper.

each module will be explained in the following subsections.
Note that our system can handle at most two simultaneous speakers by the design of the CSS module. In addition, in some conﬁguration of our systems, we do not apply speech separation. In such a case, the original mixed speaker signal is fed into the pipeline just by simply skipping the CSS module.

Fig. 2. Statistics of recordings in development set, (a) histogram of overlap ratios; (b) histogram of number of speakers.
2. VOXSRC CHALLENGE 2020 DATASET
At the VoxSRC Challenge 2020, the development data and the evaluation data were both monoaural multi-talker recordings provided by the organizer. The dataset is obtained from YouTube videos, consisting of multi-speaker audio from both professionally edited videos as well as more casual conversational multi-speaker audio. Throughout the audio, many artifacts are observed such as background noise, music, laughter, applause, and singing voices, which make the speaker diarization challenging. The audio also contains plenty amount of overlapped speech as shown in Fig. 2(a).
The reference time information was provided only for development set, and a participant can submit the result for evaluation data only up to 5 times, once per day. Therefore, we examined our system mainly based on the development set, and only show the results of evaluation set for our system submissions (we submitted 4 systems in total). The development set consists of 216 recordings (20.3 hours in total). The number of speakers in one recording varies from 1 to 20 as shown in Fig. 2(b), and the average overlap ratio of the recordings in the development set is 7.1% according to the reference time information. The evaluation set consists of 310 recordings (54.1 hours in total).
Evaluation was conducted based on the diarization error rate (DER) and the Jaccard error rate (JER) [14]. The DER was used for the primary metric of the challenge, and we also tuned our system based on it. More information about the challenge data set and evaluation metrics can be found in [26].
3. PROPOSED SPEAKER DIARIZATION SYSTEM
3.1. Overview
The proposed speaker diarization system is illustrated in Fig. 1. The input audio is ﬁrst processed by the CSS module that separates potential overlapped speech in a blockwise manner. The output is always two separated channels. For regions with only one active speaker, one of the channel is supposed to contain the speech while the other is supposed to be empty. The separated channels are independently processed by speaker embedding extraction and segmentation modules in sequence. The segments from both channels are then pooled together and fed into the clustering module where they are grouped into speaker clusters. Finally, diarization outputs from multiple systems are fused by the modiﬁed DOVER. The details of

3.2. Conformer based CSS
To handle the overlapped speech, the CSS framework [22, 27] is applied to each meeting due to its capability in handling arbitrary long sequence with various number of speakers in conversation. In this work, we assume the maximum number of simultaneously talking speakers is two. For each meeting, two channel outputs are estimated by separation module, where each channel only contains single active speaker. More detail can be found in [22, 27].
The frequency mask based approach were used for separation, where two masks were estimated for each frame of the input spectrogram. Following [28], we applied the conformer based separation network, which consists of 18 conformer encoder layers with 8 attention heads, 512 attention dimensions and 1024 FFN dimension. The model was trained with permutation invariant training objective with mean squared error between the magnitude spectrogram of the reference signal and masked mixture signal, with the mean and variance normalized spectrogram from mixture speech as input feature.
To train the network, we simulated 1500 hours of mixed training sample. For each sample, two clean speech utterances sampled from WSJ-1 and LibriSpeech [29] data sets were ﬁrstly convolved with room impulse response simulated with image method [30], then mixed with signal to noise ratio sampled between -5 ∼ 5 dB. We followed the mixing setup as in [28]. Meanwhile, we randomly removed one mixing source in 10% of training data to create single speaker mixtures.
3.3. Res2Net-based Speaker Embedding Extractor
Res2Net [23] structure was originally proposed for image classiﬁcation. It introduced a new dimension, called scale, to improve ResNet model’s representation power. In our previous work [31], we investigated the effectiveness of Res2Net architecture for text-independent speaker veriﬁcation (SV) task. Experimental results demonstrated that increasing scale is more efﬁcient than going deeper or wider. Res2Net model exhibits stronger capacity than conventional ResNet even with similar number of parameters. In addition, we veriﬁed Res2Net structure outperforms ResNet baseline for short utterances and mismatched scenarios due to its multi-scale feature representation ability, which could also beneﬁt subsequent clustering.
Given these promising results, in our diarization system, we adopted the Res2Net structure as our speaker embedding extractor. In order to further enhance speaker embedding’s discrimination, we applied the additive margin Softmax (AM-Softmax) loss [24] as our training criterion. The integration of Res2Net structure and AMSoftmax loss brought us a state-of-the-art speaker embedding extractor.
As shown in Table 1, we prepared three models with different conﬁgurations. The ﬁrst and third models were trained with VoxCeleb1&2, containing 7323 speakers in total (the VoxCeleb1-test part is excluded from training), while the second model was trained with VoxCeleb2-dev, which contains 5994 speakers. We also show the equal error rate (EER) and the minimum detection cost function (minDCF) of all three systems on the standard VoxCeleb1 test set.

Table 1. Evaluation results with different model structures. Nota-

tion for model: (w: base width; s: scale). EER (%) and minDCF

(p target=0.05) are reported on VoxCeleb1 test set.

ID Model

#spks

Loss

VoxCeleb1-test

EER minDCF

E1 Res2Net23-26w8s 7323 Softmax 1.16 0.0737

E2 Res2Net50-26w8s 5994 AM-Softmax 0.90 0.0509

E3 Res2Net50-26w8s 7323 AM-Softmax 0.83 0.0473

3.4. AHC-based Segmentation
Speaker segmentation module segments the continuous audio into multiple short segments so that each segment contains only one speaker. This is performed in two steps. First, voice activity detection (VAD) is applied to the input audio to extract each continual speech region where at least one person is active. Then, each region is further decomposed into the speaker-homogeneous segments by means of agglomerative hierarchical clustering (AHC). To do so, speaker embedding vectors are extracted at the rate of 12.5 Hz (i.e., one vector for every 80 ms). Each pair of two consecutive vectors is grouped to form an initial set of segments. For every neighboring segment pair, the degree of proximity between the two segments is estimated in the embedding space. The closest pair is then merged to form a new longer segment. The proximity is deﬁned as the cosine similarity between the mean embedding of the two segments. This process is repeated until the cosine similarity becomes less than a pre-determined threshold.
3.5. Speaker Clustering with Leakage Filtering
3.5.1. AHC-based speaker clustering
AHC is used to group speech segments into clusters. A high stopping threshold is used in the AHC to ensure high speaker purity of the clusters. As a result, the number of clusters is usually much larger than the number of actual speakers. Speaker clusters are chosen from the clusters according to a duration criterion. Speciﬁcally, only those clusters that are longer than a predeﬁned minimum speaker duration are considered as a valid speaker cluster. After that, a speaker embedding centroid vector is obtained for each speaker cluster, and the rest of the clusters are assigned to one of the speaker clusters via cosine similarity. The motivation of the above strategy is to increase the chance of the main speakers being diarized correctly while sacriﬁcing the performance on minor speakers. To avoid assigning minor speakers’ clusters, which failed to be treated as a valid speaker cluster, to other speaker clusters, a SV step is introduced. If the similarity between a cluster to be assigned and its most similar speaker cluster is lower than an SV threshold, the cluster will not be assigned to the speaker cluster but treated as unassigned cluster. Currently, all unassigned clusters in a session are treated as one single cluster during DER and JER computation. In practice, special treatment of these unassigned clusters are required which usually depends on the application of speaker diarization. The AHC stopping threshold, minimum speaker duration, and SV threshold are tuned on the development set and set to 0.55, 2.5 seconds, and 0.0, respectively.
3.5.2. Leakage ﬁltering
The CSS module occasionally produces residual noises. For example, when a single speaker is speaking with background music, one of the two separated signal may contain the active speaker’s voice,

Fig. 3. Example of the modiﬁed DOVER on three hypotheses. In the root hypothesis, there are three speakers called A, B, and C. Three hypotheses α, β, and γ are merged along with the root hypothesis.
while the other one may contains residual music. The VAD may incorrectly tag these residual noise/music as speech. One solution to this problem may be to use automatic speech recognition (ASR) as a VAD, but this makes the system language dependent and introduces high computational cost and latency. To handling this issue, we introduced a speaker embedding based segment ﬁltering step.
The diarization system is run on both with and without the CSS module independently. From the diarization output of the system without the CSS module, we obtain a set of speaker clusters and we assume that the centroids of these clusters contain all the speakers’ signature in the audio. These centroids are used to ﬁlter the speech segments from the system with the CSS module. Speciﬁcally, if the maximum cosine similarity of a segment to the centroids is below a predeﬁned threshold, the segment will be removed from the diarization output. We found that this approach is able to reduce the VAD errors on separated channels signiﬁcantly. The ﬁltering threshold is set to 0.2 which is tuned from development set.
3.6. Modiﬁed DOVER for System Combination
To further enhance robustness, we also fuse the outputs of multiple spekaer diarization systems by using a novel method, called modiﬁed DOVER. While widely utilized in other tasks such as speaker recognition (e.g., [32]) and automatic speech recognition

Table 2. The DER (%) and JER (%) of the proposed speaker diarization system. Speaker clustering “C1” uses stopping threshold of 0.6 and

minimum speaker duration of 4s, while speaker clustering “C2” uses 0.55 and 2.5s, respectively.

System Submission Speaker Speaker Speech Leakage

Dev

Test

embedding clustering separation ﬁltering DER (%) JER (%) DER (%) JER (%)

Baseline

-

-

-

-

-

-

-

21.75

51.89

1

1

E1

C1





5.63

25.65

8.87

21.08

2

2

E2

C1





5.06

24.47

8.54

20.58

3

-

E3

C1





5.04

23.97

-

-

4

-

E3

C1





4.91

23.33

-

-

5

3

E3

C1



(Sys. 3) 3.89

23.02

8.08

17.78

6

-

E3

C2





4.91

19.90

-

-

7

-

E3

C1



(Sys. 6) 3.80

18.69

-

-

8

4

Fusion of System 1, 2, 3, 7

3.71

18.74

6.23

21.52

(e.g., [33]), system fusion has been rarely explored for speaker diarization. Recently, [25] proposed a voting-based algorithm, called DOVER, where the multiple speaker diarization hypotheses are aligned in an iterative manner, and the speaker for each time step is estimated by the weighted-voting from all the hypotheses. While DOVER achieved signiﬁcant improvement in their experiment, it has a limitation that overlapping speech cannot be handled correctly.
To achieve system combination even for overlapping speech, we propose a modiﬁed DOVER algorithm. The modiﬁed DOVER works as followings. The example procedure with three hypotheses is also shown in Fig. 3.
1. Deﬁne a “root” hypothesis.
2. Align each hypothesis from a different speaker diarization system with the root hypothesis by ﬁnding the speaker permutation that maximizes the total duration of overlap with the root hypothesis.
3. For each speaker in the root hypothesis, each aligned hypothesis votes a weight for each time region for that speaker. If the total sum of voting weights exceeds the threshold, take that time region in the merged hypothesis. Note that a different hypothesis may have a different weight for voting.
In the modiﬁed DOVER algorithm, there are a few things that need to be taken care of. Firstly, if the number of speakers in a hypothesis is larger than that of the root hypothesis, there will be a speaker who does not have corresponding speaker of the root hypothesis after alignment in Step 2. In such a case, we simply discard such a speaker as exempliﬁed in Speaker 4’ of Hypothesis β in Fig. 3. Because of this, the number of speakers in the merged hypothesis never exceeds that of the root hypothesis. Secondly, there could be multiple choices for the root hypothesis. For example, we could use the hypothesis from the best system, or we could use the hypothesis that has maximum number of speakers. We could even use the fused hypothesis by other technique (such as the original DOVER) as the root hypothesis.
In our ﬁnal system, we combined four hypothesis. We used the hypothesis from the best system on development set as the root hypothesis. The voting weight was set to 1.0 for the best system, and 0.34 for the other three systems. Threshold was set to 1.0. Note that this setting eventually corresponds to keep using the hypothesis of the best system while appending the region where the results of remaining three systems were coincident.
4. RESULTS AND ANALYSIS
The diarization results of the proposed systems and the ofﬁcial baseline of the challenge are shown in Table 2. The difference between

systems 1-3 is in the speaker embedding extraction model. System 4 then introduced the CSS module to handle overlapped speech. However, due to the leakage, false alarm was increased while miss rate was reduced, and overall DER on development set was not signiﬁcantly improved. System 5 was the same as system 4 except that the leakage ﬁltering was additionally applied. The speaker clusters from system 3 was used as the centroids for the ﬁltering. It was observed that this ﬁltering signiﬁcantly reduced the DER on the development set. The system 5 also achieved signiﬁcantly better DER and JER on the test set.
In system 6, the AHC stopping threshold and minimum speaker duration were changed according to the grid search on the development set. Compared to system 3, DER was slightly reduced while JER was signiﬁcantly reduced from 23.97% to 19.90%. System 7 was the same as system 5 except that system 6 was used for ﬁltering out the leakage.
Finally, systems 1, 2, 3, and 7 were fused using the modiﬁed DOVER algorithm by setting the hypothesis of system 7 as the root hypothesis. Compared to the best single system 7, DER was slightly improved while JER was marginally degraded. When we compare the system 8 (4th submission) with system 5 (3rd submission), we observe a signiﬁcant improvement on the DER only in the test set. We also observe that the JER was signiﬁcantly degraded in the test set while it was signiﬁcantly improved in the development set. Due to the submission limit1, we could not do further analysis on these differences. Our best result of 6.23% DER was ranked 1st at the VoxSRC Challenge 2020.
5. CONCLUSION
This paper described the Microsoft speaker diarization system for monaural multi-talker recordings in the wild. We proposed the speaker diarization system consists of the state-of-the-art components such as Res2Net-based speaker embedding extractor, conformer-based speech separation system with the leakage ﬁltering, and the modiﬁed DOVER for the system fusion. We evaluated the proposed system with the data set provided by the VoxSRC challenge 2020, and ﬁnally achieved 3.71% and 6.23% of DERs on development set and evaluation set, respectively, being ranked 1st at the VoxSRC challenge 2020.
1We were able to submit only 4 systems due to the submission deadline while we were allowed to submit up to 5 systems.

6. REFERENCES
[1] S. E. Tranter and D. A. Reynolds, “An overview of automatic speaker diarization systems,” IEEE Trans. on ASLP, vol. 14, no. 5, pp. 1557–1565, 2006.
[2] N. Kanda, C. Boeddeker, J. Heitkaemper, Y. Fujita, S. Horiguchi, and R. Haeb-Umbach, “Guided source separation meets a strong asr backend: Hitachi/paderborn university joint investigation for dinner party asr,” Interspeech, 2019.
[3] I. Medennikov, M. Korenevsky, T. Prisyach, Y. Khokhlov, M. Korenevskaya, I. Sorokin, T. Timofeeva, A. Mitrofanov, A. Andrusenko, I. Podluzhny et al., “The STC system for the chime-6 challenge,” in CHiME 2020 Workshop, 2020.
[4] N. Dehak, P. J. Kenny, R. Dehak, P. Dumouchel, and P. Ouellet, “Front-end factor analysis for speaker veriﬁcation,” IEEE Trans. on ASLP, vol. 19, no. 4, pp. 788–798, 2010.
[5] D. Snyder, D. Garcia-Romero, G. Sell, D. Povey, and S. Khudanpur, “X-vectors: Robust dnn embeddings for speaker recognition,” in ICASSP, 2018, pp. 5329–5333.
[6] E. Variani, X. Lei, E. McDermott, I. L. Moreno, and J. Gonzalez-Dominguez, “Deep neural networks for small footprint text-dependent speaker veriﬁcation,” in ICASSP, 2014, pp. 4052–4056.
[7] H. Bredin, “Tristounet: triplet loss for speaker turn embedding,” in ICASSP, 2017, pp. 5430–5434.
[8] T. Zhou, Y. Zhao, J. Li, Y. Gong, and J. Wu, “CNN with phonetic attention for text-independent speaker veriﬁcation,” in ASRU, 2019.
[9] J. Wang, X. Xiao, J. Wu, R. Ramamurthy, F. Rudzicz, and M. Brudno, “Speaker diarization with session-level speaker embedding reﬁnement using graph neural networks,” in ICASSP, 2020, pp. 7109–7113.
[10] S. Meignier and T. Merlin, “LIUM SpkDiarization: an open source toolkit for diarization,” 2010.
[11] S. H. Shum, N. Dehak, R. Dehak, and J. R. Glass, “Unsupervised methods for speaker diarization: An integrated and iterative approach,” IEEE Trans. on ASLP, vol. 21, no. 10, pp. 2015–2028, 2013.
[12] D. Garcia-Romero, D. Snyder, G. Sell, D. Povey, and A. McCree, “Speaker diarization using deep neural network embeddings,” in ICASSP, 2017, pp. 4930–4934.
[13] Q. Wang, C. Downey, L. Wan, P. A. Mansﬁeld, and I. L. Moreno, “Speaker diarization with LSTM,” in ICASSP, 2018, pp. 5239–5243.
[14] N. Ryant, K. Church, C. Cieri, A. Cristia, J. Du, S. Ganapathy, and M. Liberman, “First DIHARD challenge evaluation plan,” 2018, tech. Rep., 2018.
[15] ——, “The second DIHARD diarization challenge: Dataset, task, and baselines,” arXiv preprint arXiv:1906.07839, 2019.
[16] O¨ . C¸ etin and E. Shriberg, “Analysis of overlaps in meetings by dialog factors, hot spots, speakers, and collection site: Insights for automatic speech recognition,” in Interspeech, 2006.
[17] J. Barker, S. Watanabe, E. Vincent, and J. Trmal, “The ﬁfth ’CHiME’ speech separation and recognition challenge: dataset, task and baselines,” in Interspeech, 2018, pp. 1561– 1565.

[18] S. Watanabe, M. Mandel, J. Barker, E. Vincent, A. Arora, X. Chang, S. Khudanpur, V. Manohar, D. Povey, D. Raj et al., “CHiME-6 challenge: Tackling multispeaker speech recognition for unsegmented recordings,” arXiv preprint arXiv:2004.09249, 2020.
[19] Y. Fujita, N. Kanda, S. Horiguchi, K. Nagamatsu, and S. Watanabe, “End-to-end neural speaker diarization with permutation-free objectives,” in Interspeech, 2019, pp. 4300– 4304.
[20] Y. Fujita, N. Kanda, S. Horiguchi, Y. Xue, K. Nagamatsu, and S. Watanabe, “End-to-end neural speaker diarization with selfattention,” in ASRU, 2019, pp. 296–303.
[21] A. Gulati, J. Qin, C.-C. Chiu, N. Parmar, Y. Zhang, J. Yu, W. Han, S. Wang, Z. Zhang, Y. Wu et al., “Conformer: Convolution-augmented transformer for speech recognition,” arXiv preprint arXiv:2005.08100, 2020.
[22] Z. Chen, T. Yoshioka, L. Lu, T. Zhou, Z. Meng, Y. Luo, J. Wu, X. Xiao, and J. Li, “Continuous speech separation: Dataset and analysis,” in ICASSP, 2020, pp. 7284–7288.
[23] S. Gao, M.-M. Cheng, K. Zhao, X.-Y. Zhang, M.-H. Yang, and P. H. Torr, “Res2net: A new multi-scale backbone architecture,” IEEE trans. on PAMI, 2019.
[24] F. Wang, J. Cheng, W. Liu, and H. Liu, “Additive margin softmax for face veriﬁcation,” IEEE Signal Processing Letters, vol. 25, no. 7, pp. 926–930, 2018.
[25] A. Stolcke and T. Yoshioka, “DOVER: A method for combining diarization outputs,” in ASRU, 2019, pp. 757–763.
[26] J. S. Chung, J. Huh, A. Nagrani, T. Afouras, and A. Zisserman, “Spot the conversation: speaker diarisation in the wild,” arXiv preprint arXiv:2007.01216, 2020.
[27] T. Yoshioka, I. Abramovski, C. Aksoylar, Z. Chen, M. David, D. Dimitriadis et al., “Advances in online audio-visual meeting transcription,” in ASRU, 2019, pp. 276–283.
[28] S. Chen, Y. Wu, Z. Chen, J. Li, C. Wang, S. Liu, and M. Zhou, “Continuous speech separation with conformer,” arXiv preprint arXiv:2008.05773, 2020.
[29] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, “LibriSpeech: an ASR corpus based on public domain audio books,” in ICASSP, 2015, pp. 5206–5210.
[30] J. B. Allen and D. A. Berkley, “Image method for efﬁciently simulating small-room acoustics,” The Journal of the Acoustical Society of America, vol. 65, no. 4, pp. 943–950, 1979.
[31] T. Zhou, Y. Zhao, and J. Wu, “ResNeXt and Res2Net structure for speaker veriﬁcation,” arXiv preprint arXiv:2007.02480, 2020.
[32] N. Brummer, L. Burget, J. Cernocky, O. Glembek, F. Grezl, M. Karaﬁat et al., “Fusion of heterogeneous speaker recognition systems in the stbu submission for the nist speaker recognition evaluation 2006,” IEEE Trans. on ASLP, vol. 15, no. 7, pp. 2072–2084, 2007.
[33] J. G. Fiscus, “A post-processing system to yield reduced word error rates: Recognizer output voting error reduction (ROVER),” in ASRU, 1997, pp. 347–354.

