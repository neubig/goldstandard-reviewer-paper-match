arXiv:2107.10121v1 [cs.GT] 21 Jul 2021

Peer Selection with Noisy Assessments
Omer Lev1, Nicholas Mattei2, Paolo Turrini3, and Stanislav Zhydkov3
1 Ben-Gurion University, Israel omerlev@bgu.ac.il
2 Tulane University, USA nsmattei@tulane.edu
3 University of Warwick, UK {p.turrini, s.zhydkov}@warwick.ac.uk
Abstract. In the peer selection problem a group of agents must select a subset of themselves as winners for, e.g., peer-reviewed grants or prizes. Here, we take a Condorcet view of this aggregation problem, i.e., that there is a ground-truth ordering over the agents and we wish to select the best set of agents, subject to the noisy assessments of the peers. Given this model, some agents may be unreliable, while others might be selfinterested, attempting to inﬂuence the outcome in their favour. In this paper we extend PeerNomination, the most accurate peer reviewing algorithm to date, into WeightedPeerNomination, which is able to handle noisy and inaccurate agents. To do this, we explicitly formulate assessors’ reliability weights in a way that does not violate strategyproofness, and use this information to reweight their scores. We show analytically that a weighting scheme can improve the overall accuracy of the selection signiﬁcantly. Finally, we implement several instances of reweighting methods and show empirically that our methods are robust in the face of noisy assessments.
1 Introduction
The results of the 2016 NeurIPS experiment [20] and other studies of bias in evaluative processes [24,21] have brought to the fore the extent of noisy assessments in conference reviewing. When agents evaluate each other to select a subset of themselves – the peer selection problem – various factors can come into play that hinder accurate assessment, including time pressure and strategic behaviour. Finding high quality mechanisms for peer review is a critical step in helping to ease reviewing in a number of areas, including large conferences [4], grant reviewing [17], and online courses [22].
Recently, researchers in algorithmic game theory and computational social choice have devised accurate and strategyproof algorithms for peer selection, such as Partition [2], Credible Subset [10], Exact Dollar Partition (EDP) [4] and most recently PeerNomination [15], each improving upon the state of the art. These algorithms take a Condorcet view on this aggregation problem, i.e., that there is a ground-truth over the agents, and we wish to select as many of the top ranked agents as possible [26]. However, none of the existing algorithms seek

2

O. Lev, N. Mattei, P. Turrini and S. Zhydkov

to alleviate the problem of noisy inputs in a uniﬁed, strategyproof mechanism. When earlier work did engage with noisy reports, it was limited to empirical testing with relatively low noise, e.g., a Mallows model with ϕ = 0.5 (see [4] or [15]), which yields fairly minor changes in agents’ reports. We are concerned with algorithms that can handle a signiﬁcant level of noise, while maintaining strategyproofness and high quality of selection, an important missing aspect in the peer reviewing literature.
Ideally, we want an algorithm that is capable of identifying inaccurate reviewers and reducing their inﬂuence on selection, using their own assessments as a guide. We could, for example, try and downgrade those reviewers that diﬀer too much from others. However, there are two problems with this approach: ﬁrst, the noise may be such that it is diﬃcult to establish what the consensus actually is; and second, that this meta-level reweghting can be exploited strategically. Simple reweghting is not strategyproof. Consider an agent a that is harshly reviewing agent b, with both a and b reviewing a third agent c. Agent b could beneﬁt by reviewing agent c in a way that would present agent a as one whom others disagree with, potentially lowering the impact of the report of agent a on agent b if weights are computed based on correlations to the evaluations of others, as done by [17]. On the other hand, if a mechanism is able to identify agent b as a source of noise, it can increase the overall quality of the selection. While one can reweigh agents without maintaining strategyproofness [22,25], we wish to achieve increased selection quality and strategyproofness.
Contribution. We extend PeerNomination, the most accurate peer reviewing algorithm to date, into WeightedPeerNomination, to handle noisy and inaccurate agents. To do so we explicitly formulate assessors’ reliability weights in a way that does not violate strategyproofness, and use this information to reweigh their scores. WeightedPeerNomination is able to handle signiﬁcant levels of noise, even when reviewers act adversarially. We show analytically that a weighting scheme can improve the overall accuracy of the selection signiﬁcantly. We then implement several instances of reweghting protocols and show empirically that our novel methods are able to signiﬁcantly improve the quality of peer selection over PeerNomination, under a variety of noise parameters.

2 Related Literature
Using the evaluations of peers to rank and select winners is a problem of broad interest beyond CS and AI, including numerous practical domains, e.g., conference, journal, and grant reviewing; large scale course grading, and group decision making. Brought to the fore by [17] to allocate telescope time, the problem is deeply rooted in economics, from the work of [7] on “dollar partition”, extended by [4] to the Dollar Raﬄe and Dollar Partition methods. Other notable algorithms include the Credible Subset method [10], where the protocol examines the possibility of manipulations and accounts for it. Despite strategyproofness, the system was shown to yield signiﬁcant number of cases where no proposal was funded in the end [3].

Peer Selection with Noisy Assessments

3

Two more prominent recent algorithms are Exact Dollar Partition (EDP) [4] which provides exactness at the cost of some randomness, while remaining strategyproof, and improving on earlier algorithms [4]. The second is PeerNomination [15], which improves upon EDP, while requiring reviewers to submit only approval based rankings, though at the cost of some exactness (details in Section 3).
Other developments in the multi-agent systems communities include voting rules to aggregate ranks, e.g., k-Partite [9], the Committee Rule [9], and Divideand-Rank [27] algorithms. Others focus on proving bounds on the quality of a given rank aggregation scheme under noisy and partial observations [6]. Yet other methods are approval-based but focus on single agent selection: Permutation [8] and Slicing [5].
A key application area for peer evaluation mechanisms is education, where the problems of reviewer reliability and bias have been extensively studied [18]. We are motivated by evidence from ﬁelded peer evaluation mechanisms showing that students are often unwilling to strictly rank assignments [1] and would rather rely on scores or pass/fail marks (approvals). Within the conference and journal reviewing ecosystem there is also growing interest in detecting strategic behaviour on the part of the reviewers [21,16] as well as de-biasing and calibrating diﬀerences in the scores of reviewers [23,11]. We go beyond calibration and debiasing, identifying suboptimal behaviour in agents’ populations and looking at the eﬀect of rescaling on the system as a whole.
Outside peer selection, there is extensive work in the machine learning, information retrieval, and preference learning communities on the learning to rank problem: inferring the most likely ranking from possibly noisy observations [12]. These works include learning noise models, e.g., the parameters of a Mallows model, for use in inferring latent preferences of agents [12,26]. This is of great practical interest in information retrieval, where one wishes to rank, e.g., webpages based oﬀ user clicks [19] and in combining labellings from multiple sources for the construction of datasets [25]. However, all of these systems do not concern themselves with strategyproofness, a key focus of our study.

3 Preliminaries
In our setup, a set of agents N = {1, 2, ..., n} aim to select a subset of themselves of size k. We assume that, if agents were to assess each other accurately, they would report the same ranking. We refer to this ranking as the ground truth, a standard assumption in Condorcet views of voting [26]. In practical applications, it is often not feasible for agents to review all others, therefore we assume, for simplicity, that each agent reviews m agents and is reviewed by m of them. We represent such an m-regular graph via an assignment function A : N → 2N and denote i’s review pool, the agents reviewed by i, as A(i). Let A−1(i) denote the set of agents that review agent i. In real-world settings, m is typically a small constant w.r.t. n.
Since we assume the ground truth to be a linear order, we also assume the belief of each reviewer over their review pool to be a strict ranking. Formally, we

4

O. Lev, N. Mattei, P. Turrini and S. Zhydkov

represent the underlying ranking of reviewer i as an injective function σi : A(i) → {1, ..., m}, where σi(j) is the rank given by reviewer i to agent j. The collection of these reviews is called a review proﬁle and is denoted by σ. However, as in [15],
we take a less demanding approach and require reviewers to only report approvals
to the mechanism, rather than a strict ranking. Formally, we instantiate each underlying belief σi by σiapp : A(i) → {0, 1, α}, where 1 represents approval and α ∈ [0, 1) is a constant representing “partial” approval. Similarly, σiapp(j) is the approval score given by reviewer i to reviewer j.

PeerNomination. In PeerNomination, each agent, independently and concurrently, nominates for certain nk m of their reviewees, i.e., setting σiapp(j) = 1. In addition, they probabilistically nominate exactly one other agent, with probability nk m − nk m , i.e., setting σiapp(j) = nk m − nk m . In other words, probabilistic nominations become certain nominations with probability nk m − nk m . These partial nominations are used to maintain the size requirements as discussed by
[15]. An agent j is then selected iﬀ they are nominated by the majority of their re-
viewers. Since each agent is considered independently for selection, the algorithm
is not guaranteed to return exactly k agents, though it closely approximates k
[15].

3.1 Noise Model
To model the inaccuracies in reviewers’ assessments, we assume that each agent receives a noisy observation of the ground truth according to a Mallows model [14]. Mallows models have been widely used to compare the performance of peer selection algorithms empirically [15,4], but so far only studied for very mild levels of noise, e.g., ϕ = 0.5.
The Mallows model is parameterised by a dispersion parameter ϕ ∈ [0, 1] and a reference linear ranking R. Given R and ϕ, the model induces a probability distribution over all permutations of R such that the probability of the linear order R is π(R ) ∝ ϕKT (R,R ), where KT (R, R ) is the Kendall-τ distance between R and R . The Kendall-τ distance counts the number of pairwise disagreements between two rankings. In other words, the probability to ﬁnd an additional pairwise disagreement from the reference ranking decreases exponentially. Note that, as we vary the dispersion parameter ϕ from 0 to 1, the probability distribution over all linear rankings moves from being concentrated on R to being uniform overall possible rankings.
In our simulations (Section 5), we take the ground truth as the reference ranking and sample a noisy ranking for each agent using the ϕ speciﬁed. An important feature of Mallows model is that it can be sampled eﬃciently [13,26], which allows us to generate a unique reviewer proﬁle for each experiment.
In addition, we test our weighting schemes in settings where some reviewers are not just random, but are actively adversarial. We thus extend the range of the dispersion parameter ϕ to [0, 2], where ϕ ∈ (1, 2] means that the ranking is sampled using the inverse of the ground truth as the reference ranking and

Peer Selection with Noisy Assessments

5

ϕ = 2 − ϕ . Thus, the distribution moves smoothly from being concentrated at the ground truth to the inverse ground truth while still being uniform around 1.
It is important to note that the Mallows model does not produce errors in reviews proportionally to ϕ. Suppose that agents have to nominate top 3 out of 9 individuals and their beliefs are given by the Mallows model. With ϕ = 0.5 only a small number of agents are even going to commit 1 error and we need to increase ϕ to around 0.95 to have any meaningful probability of getting 2/3 nominations wrong. Only under our adversarial extension we start seeing agents that get all 3 nominations wrong. Figure 1 illustrates this relationship of errors and ϕ.

Distribution of the number of errors

1.0 0 errors

0.8

1 error 2 errors

3 errors

0.6

0.4

0.2

0.0 0.8 0.9 1.0 1.1 1.2 Dispersion parameter,

Fig. 1: Nomination errors committed by agents as a function of ϕ. Each vertical slice shows the the distribution of the number of errors among the 200 agents in a simulation where each agent had to nominate 3 individuals out of 9.

3.2 Properties
Our mechanism, like most, maintains the properties of anonymity, permuting agents makes no diﬀerence; non-imposition, any set of k accepted papers is a possible output; and monotonicity, if agent j was selected, and some agent i increased their score for it, agent j will still be selected.
We focus on strategyproofness, i.e., no agent is better oﬀ by reporting a nontruthful review. We do so by showing no agent’s reviews have any inﬂuence over their own selection, and hence there is no incentive to misreport; and on ensuring the algorithm’s quality. We measure the quality by two key parameters: recall,

6

O. Lev, N. Mattei, P. Turrini and S. Zhydkov

Algorithm 1 WeightedPeerNomination Choice
Input: Assignment A, review proﬁle σ, target quota k, slack parameter ε, reviewer weights {w1, ..., wn}
Output: Accepting set S Set nomQuota := nk m + ε for all j in N do Initialise nomCount := 0 for all i ∈ A−1(j) do if σi(j) ≤ nomQuota then increment nomCount by wi else if σi(j) = nomQuota + 1 then increment nomCount by wi with probability nomQuota − nomQuota end if end for if nomCount ≥ ( i∈A−1(j) wi)/2 then S←j end if end for return S

the proportion of true positives in the selected set4; and size, i.e., the number of selected agents.
4 Weighted PeerNomination
WeightedPeerNomination extends PeerNomination [15] by adjusting a reviewer’s weight as a function of the overall quality of their reviews. This design choice is based on the intuition that in many cases, agents’ reviews are not independent when it comes to quality but, rather, correlated. PeerNomination is a special case of WeightedPeerNomination with weights (1, 1, . . . , 1).
The new algorithm can be separated into three parts:
Assignment. Determining which agents review each agent (Algorithm 2). Weights. Applying a reweghting protocol to adjust those reviews (Section 4.2). Choice. Choosing, based on reweighed reviews, which agents to select (Algo-
rithm 1).
As illustrated in Figure 2 (Left), the weighting scheme introduces the possibility of breaking strategyproofness: if agent a and agent b are both reviewing agent c, and agent a is also reviewing agent b, agent b may impact the weight given to agent a with their review to agent c, thus inﬂuencing agent a’s role in determining if agent b themself is selected. Therefore, we complement the modiﬁed selection algorithm (Algorithm 1), with a method to assign agents to
4 Accuracy, which is often paired with recall, is less relevant in our setting as we have a ﬁxed and often small proportion of positives (which is equal to k/n) in the population.

Peer Selection with Noisy Assessments

7

reviews that does not break strategyproofness, based on the Euler cycle (Figure 2 (Right)). This algorithm for assignment is presented as Algorithm 2. We separate the two algorithms as they do not depend on each other. Any assignment algorithm that does not create a case in which an agent can both review-with and be reviewed-by the same agent can run Algorithm 1 and maintain strategyproofness.

a

b

a

b

c

c

d

e

f

Fig. 2: Algorithm 2 avoids non strategyproof assignments (Left), outputting strategyproof instances (Right).

Weighting schemes may introduce various strategyproofness violations (as in [22]), so here we constrain them to be operators of the form wi : Rm × R(m−1)×m → R for each agent i. These functions, which in our case will be identical for each agent, associate to each agent i’s reviews the values of the other m − 1 reviews on those same reviewees. Under this condition we can state:
Theorem 1. WeightedPeerNomination is strategyproof.
Proof. As was also shown for PeerNomination [15], for any exogenous set of weights, Algorithm 1, is strategyproof – no agent can inﬂuence their own chance of being selected. To show that it is not possible for an agent to inﬂuence the weights given to their own reviewers, observe that, for each agent i, wi receives only the values of reviews on the same agents that agent i reviews. Thus, it is not possible for an agent j, who does not review a paper also reviewed by agent i, to change agent i’s weight. Consider now an agent j that is reviewing a paper also reviewed by agent i. Agent j has a reason to deviate form truthful reporting if changing agent i’s weight will beneﬁt the chance that agent j is selected. Since the only use of agent i’s weight in the outcome is on agents it reviews, agent j would beneﬁt from altering agent i’s weight only if agent i was reviewing agent j themself.
Such an occurrence is prevented by Algorithm 2. Thanks to the initial creation of a bi-partite graph, agent i would review agent j only if i ∈ X and j ∈ Y (or vice versa). But then there can be no third agent both of them review: agent i only reviews agents in Y , while agent j only reviews agents in X (see Figure 2 (Right)). Thus, the only case where agent j would ﬁnd it useful to not be truthful cannot happen.

8

O. Lev, N. Mattei, P. Turrini and S. Zhydkov

Algorithm 2 Euler-Based Assignment
Input: Set of n agents, review number m ≤ n/4. Output: Anti-transitive m-regular assignment A
Initialise G = (V, E) with V := [n], E := ∅ Partition V into X and Y such that |X| = |Y | = n/2 \\Make a 2m-regular bipartite graph G(V,E) for all x in X do
for all i in 1, ..., 2m do y∗ ← arg miny{deg(y) | y ∈ Y } E ← E ∪ {{x, y∗}}
end for end for \\Use Hierholzer’s algorithm to ﬁnd an Euler cycle \\Euler cycle exists since every node has even degree Set euler cycle = Hierholzers algorithm(G) Set A := ([n], EA), EA := ∅ for all (vi, vi+1) in euler cycle do
EA ← EA ∪ {(vi, vi+1)} end for return A

4.1 A Simple Theoretical Model
The constructive use of Algorithm 1 depends on the ability of identifying accurate and inaccurate reviewers, and using this identiﬁcation to weigh their reviews. Clearly, lacking the knowledge of the ground truth means any identiﬁcation of accurate/inaccurate agents has to depend on comparing agents to the reviews of the other agents. If all agents were accurate no reweghting of agents would be needed, but as the proportion of accurate agents drops the problem becomes more diﬃcult. Still, when a large majority of them are accurate, the correct opinion is usually the majority, as inaccurate agents will give random rankings. However, if the number of accurate agents is very low, or other agents are actively malicious, identiﬁcation becomes impossible, as ﬁnding a metric to evaluate the agents against becomes infeasible.
To provide some intuition to the conceptual underpinnings of our algorithm we now present a simpliﬁed setting, and show how our algorithm – even with a very simple, conservative, weighting scheme – is still able to improve over PeerNomination. We start with an m-regular assignment, where each agent has one of two types: A, meaning the agent is an accurate reviewer; or A, meaning the agent is inaccurate. Recall that in PeerNomination an agent is selected if a majority of their reviewers approve. We show that a very simple dynamic weighting scheme, only relying on knowing how many times an agent has been in a minority, has a good chance of ﬂipping a decision made by A agents to one made by A agents, improving on PeerNomination.
Let an A-agent be identiﬁed as inaccurate if they held the minority opinion in at least j reviews. We want to ﬁnd the probability of the following event: (1)

0.20

ProbabPileietrySoelfecCtioornrewcitthioNnoisy Assessments 9

dtherteeschtioolnd

0.15

j=5 j=6

j=7

0.10

j=8

Probability

0.05

0.000.0 0.2 0.4 0.6 0.8 1.0 Proportion of Inaccurate Reviewers, q
Fig. 3: The probability of identifying an inaccurate agent, when m = 9, and the threshold for identiﬁcation is j.

An agent’s reviewers have an A-majority and (2) Enough of the A-agents of that majority are identiﬁed as inaccurate.
Given the noise model, let q be the probability for an agent to be of type A. Then the probability that an agent is reviewed by a majority of A agents and that majority is of size k is:

qA,k := P[A-majority of size k] = mi qi(1 − q)m−i

where i = m/2 + k. In such a case we would like to identify at least k of the A agents in order to nullify their votes.
We also ﬁnd the probability that A-agents have a majority of any size:

m/2

m/2

qA := P[A-majority] =

qA,k =

k=1

i=0

m (1 − q)iqm−i i

Our simple weighting algorithm identiﬁes a A-agent if they are in minority for at least j of their other reviewed papers. The probability of this event, qdet, is given by the cumulative binomial probability, keeping in mind that the probability of A-majority in a set of reviews on an agent is conditioned on the fact that they contain at least one A-agent:

qA := P[A-majority | there is at least one A]

m/2
=
i=0 m−1
⇒qdet =
i=j

m − 1 (1 − q)iqm−1−i i
m −i 1 (qA)i(1 − qA)m−1−i

Correlation with Correlation with

10

O. Lev, N. Mattei, P. Turrini and S. Zhydkov

0.0

0.1

0.2

0.3

0.4

0.5

0.6

Majority

0.7

Distance Step

0.5 Propo0r.6tion of 0A.c7curate0A.8gents 0.9

(a) Agents produced from Mallows distribution with ϕ = 0.5 for the share shown, the rest with ϕ = 1.

0.0

0.1

0.2

0.3

0.4

0.5

0.6

Majority Distance

0.7

Step

0.1 Prop0o.r2t5ion of 0A.c5curate0.A7g5ents 0.9

(b) Agents produced from Mallows distribution with ϕ = 0.8 for the share shown, the rest with ϕ = 1.2.

Fig. 4: Spearman correlation of the weights from the diﬀerent weighting schemes with the underlying ϕ of each agent. The bars represent the mean and the standard deviation over 1000 simulations of the weights.

Notice that A may be not just inaccurate but adversarial, in which case we could ﬂip their review and only need to do so for k of them. However, we take the safer approach here, which means we need to detect at least 2k A-agents to correct the decision. The probability of this correcting event is given by the following expression:

m/2

 m/2 +k

P[correction event] =

qA,k ·

k=1

i=2k

m/2 + k i



qdi et(1

−

qdet)

m/2

+k−i


This produces the desired result, empirically shown in Figure 3. As can be seen, for a wide variety of q and j, even our very conservative weighting scheme produces a nice probability of improving some reviews. As we shall see with the weighting schemes below, examined by our simulations, even better results can be achieved.
It should be noted that one could produce analogous probabilities of the weighting scheme incorrectly identifying A-agents as A-agents. However, for a large enough j (say, j ≥ m/2), and a majority of A agents (i.e., q < n/2), this number will always be smaller, i.e., the beneﬁt from the reweghting will be positive.

4.2 Weighting Schemes
We present three main weighting schemes to evaluate the reliability of the reviewers, each based on the approval vote supplied to the algorithm, maintaining strategyproofness.

Recall

Peer Selection with Noisy Assessments

11

1.0

0.9

40

0.8

0.7

0.6

30

Size

0.5

0.4

20

0.3

0.2

10

0.1

0.0 0.1 0.25 0.5 0.75 0.9
Proportion of Accurate Reviewers

0 0.1 0.25 0.5 0.75 0.9
Proportion of Accurate Reviewers

Distance

Majority

Step

Unit

(a) Population’s ϕ ∈ {0.5, 1}.

1.0

0.9

0.8

40

0.7

0.6

30

Size

0.5

0.4

20

0.3

0.2

10

0.1

0.0 0.5 0.6 0.7 0.8 0.9 1.0
Proportion of Accurate Reviewers

0 0.5 0.6 0.7 0.8 0.9 1.0
Proportion of Accurate Reviewers

Distance

Majority

Step

Unit

(b) Population’s ϕ ∈ {0.8, 1.2}.

Fig. 5: Size and recall with n = 200, m = 7, k = 40 with noisy reviewers (a) and adversarial reviewers (b).

Recall

Distance. Distance directly computes the distance between the agent’s review

and those of other reviewers by simply averaging the individual diﬀerences be-

tween the reviewers. Formally, deﬁne the average distance of reviewer i to other

reviewers

as

di

=

1 m2

j∈A(i)

l∈A−1(j) |σiapp(j) − σlapp(j)|. Then the distance

weight is widist = (1−di)γ, where γ is an “aggression” parameter that exaggerates

for better discrimination between the agents.

Majority Errors. Let an approval of a reviewee be an “error” by a reviewer if it is a minority opinion, summing fractional nominations. Let
majσj = 1, if i∈A−1(j) σiapp(j) ≥ m/2 0, otherwise

12

O. Lev, N. Mattei, P. Turrini and S. Zhydkov

be the majority for agent j in proﬁle σ. Then deﬁne the number of errors of
reviewer i to be errσi = j∈A(i) 1σiapp(j)=majσj . Lastly, the weight is deﬁned as wimajerr = 1 − δ(errσi /m), where δ is an aggression parameter.

Step. Step applies a step function to the error rate errσi /m deﬁned above. We choose two thresholds, t1 and t2 such that if the error rate reaches t1, we reduce

the weight of the reviewer to 0.5; if the error rate reaches t2, we reduce the

weight to 0. Additionally, we scale each threshold by the nomination quota as

it plays a bigger role in error detection than just the size of the review pool, m.

Formally,

 1, 
wi = 0.5,
0,

if errσi / nk < t1 if t1 ≤ errσi / nk < t2
otherwise.

5 Setup and Performance Results
We test WeightedPeerNomination, using our weighting schemes, against PeerNomination, following the testing framework used in [15] and [4]. However, our setup diﬀers in a few signiﬁcant ways. In contrast to the previous experiments, we do not include any partition-based mechanisms, thus allowing us to use the assignment-generating procedure given in Algorithm 2 without worrying about forming a given number of clusters (the procedure does generate a 2-clustering).
To simulate the presence of noise, we use Mallows model, as described in Section 3.1. We assume that the population consists of accurate and inaccurate reviewers, which we represent by using two distinct values of the dispersion parameter ϕ. In Section 5.1 we assume that the accurate reviewers have ϕ = 0.5 and the inaccurate reviewers have ϕ = 1, i.e., their reviews are random. We vary the proportion of accurate reviewers between 0.1 and 0.9. Also recall that in Section 3.1 we extended the deﬁnition of the dispersion parameter to range between 0 and 2. In Section 5.2, we set ϕ = 0.8 for the accurate reviewers and ϕ = 1.2 for the adversarial reviewers, i.e., most of their reviews will contradict the ground truth. We vary the proportion of accurate reviewers between 0.5 and 1. Note that as the proportion of accurate reviewers drops to 0.5 and lower, it becomes impossible to recover the ground truth.
Figure 4 demonstrates that our metrics strongly correlate with the underlying ϕ in both setups, demonstrating their eﬀectiveness in singling out accurate reviewers.

5.1 Inaccurate Reviewers
Figure 5a shows that a high (90%) proportion of accurate reviewers results in barely any improvement over Unit (original PeerNomination). This indicates that the weighting schemes do not overﬁt in the search for non-existing noise.

Peer Selection with Noisy Assessments

13

However, as the proportion of random reviewers rises, all weighting schemes outperform Unit.
We also see that the weighting schemes are indeed much better at keeping the output size close to the desired k with Distance keeping the output size consistent across all levels of noise. In addition, the much greater recall of Distance and other weighting schemes indicates the additional selected agents are usually the deserving ones. Between the weighting schemes, Distance manages to gain more and more advantage as the noise levels increase as it is the most ﬁne-grained and aggressive one. This allows it to both identify the inaccurate reviewers and maintain consistent output size.
There are similar patterns for other settings of parameters. As k increases, the performance of all algorithms improves as not only can they be less selective, but the reviewers provide more data. The advantage over Unit, however, decreases as PeerNomination is capable of taking advantage of this as well. Increasing m, on the other hand, primarily beneﬁts the weighting schemes as they can calculate the weights more accurately, increasing their advantage over Unit.

5.2 Adversarial Reviewers
We see a similar story with the adversarial model. Again, at low noise levels, PeerNomination, as expected, performs well while the weighting schemes’ slight drop in performance can be attributed to overﬁtting. However, as the proportion of adversarial reviewers rises, we can observe a notable increase in performance for the weighting schemes. Even when half of the population is adversarial, Distance impressively achieves recall of over 40% as compared to the theoretical max of 50%.

6 Conclusion
In this paper we propose a novel strategyproof peer selection algorithm – WeightedPeerNomination, which weighs reviewers based on their perceived accuracy. The basis for this reweghting is the observation that in most cases, one’s reviews are correlated in quality. We develop several weighting methods, showing that even straightforward ones can reach high quality outcomes, with high level of noise. Our algorithm is constructed in a modular way, allowing for a variety of weighting and evaluation methods to be developed for particular settings or noise models. This modularity allows for multiple directions of future development, including developing additional weighting; matching weighting schemes to noise models; and optimising them using various techniques.

14

O. Lev, N. Mattei, P. Turrini and S. Zhydkov

References

1. de Alfaro, L., Shavlovsky, M.: Crowdgrader: a tool for crowdsourcing the evaluation of homework assignments. In: The 45th ACM Technical Symposium on Computer Science Education, SIGCSE. pp. 415–420. ACM (2014)
2. Alon, N., Fischer, F., Procaccia, A., Tennenholtz, M.: Sum of us: Strategyproof selection from the selectors. In: Proceedings of the 13th Conference on Theoretical Aspects of Rationality and Knowledge (TARK). pp. 101–110 (2011)
3. Aziz, H., Lev, O., Mattei, N., Rosenschein, J.S., Walsh, T.: Strategyproof peer selection: Mechanisms, analyses, and experiments. In: Schuurmans, D., Wellman, M.P. (eds.) AAAI. pp. 397–403. AAAI Press (2016)
4. Aziz, H., Lev, O., Mattei, N., Rosenschein, J.S., Walsh, T.: Strategyproof peer selection using randomization, partitioning, and apportionment. Artiﬁcial Intelligence 275, 295–309 (2019). https://doi.org/10.1016/j.artint.2019.06.004
5. Bousquet, N., Norin, S., Vetta, A.: A near-optimal mechanism for impartial selection. In: Proceedings of the 10th International Workshop on Internet and Network Economics (WINE). pp. 133–146. Lecture Notes in Computer Science (LNCS) (2014)
6. Caragiannis, I., Krimpas, G.A., Voudouris, A.A.: Aggregating partial rankings with applications to peer grading in massive online open courses. In: Proceedings of the 2015 International Conference on Autonomous Agents and Multiagent Systems, AAMAS. pp. 675–683. ACM (2015)
7. de Clippel, G., Moulin, H., Tideman, N.: Impartial division of a dollar. Journal of Economic Theory 139, 176–191 (2008)
8. Fischer, F., Klimm, M.: Optimal impartial selection. In: Proceedings of the 15th ACM Conference on Economics and Computation (ACM-EC). pp. 803–820 (2014)
9. Kahng, A., Kotturi, Y., Kulkarni, C., Kurokawa, D., Procaccia., A.: Ranking wily people who rank each other. In: Proceedings of the 32nd AAAI Conference on Artiﬁcial Intelligence (AAAI) (2018)
10. Kurokawa, D., Lev, O., Morgenstern, J., Procaccia, A.D.: Impartial peer review. In: Proceedings of the 24th International Conference on Artiﬁcial Intelligence. pp. 582– 588. IJCAI’15, AAAI Press (2015), http://dl.acm.org/citation.cfm?id=2832249. 2832330
11. Lian, J.W., Mattei, N., Noble, R., Walsh, T.: The conference paper assignment problem: Using order weighted averages to assign indivisible goods. In: Proceedings of the32ndAAAI Conference on Artiﬁcial Intelligence (AAAI). pp. 1138–1145 (2018)
12. Liu, T.Y.: Learning to rank for information retrieval. Springer Science & Business Media (2011)
13. Lu, T., Boutilier, C.: Learning mallows models with pairwise preferences. In: Proceedings of the 28th International Conference on International Conference on Machine Learning (ICML). pp. 145–152 (2011)
14. Mallows, C.L.: Non-null ranking models. I. Biometrika 44(1-2), 114–130 (June 1957)
15. Mattei, N., Turrini, P., Zhydkov, S.: PeerNomination: Relaxing exactness for increased accuracy in peer selection. In: Proceedings of the 29th International Joint Conference on Artiﬁcial Intelligence (IJCAI). pp. – (January 2020)
16. Meir, R., Lang, J., Lesca, J., Kaminski, N., Mattei, N.: A market-inspired bidding scheme for peer review paper assignment. In: Proceedings of the35thAAAI Conference on Artiﬁcial Intelligence (AAAI) (2021)

Peer Selection with Noisy Assessments

15

17. Merriﬁeld, M., Saari, D.: Telescope time without tears: a distributed approach to peer review. Astronomy and Geophysics 50(4), 4.16–4.20 (2009). https://doi.org/10.1111/j.1468-4004.2009.50416.x, http://dx.doi.org/10. 1111/j.1468-4004.2009.50416.x
18. Piech, C., Huang, J., Chen, Z., Do, C.B., Ng, A.Y., Koller, D.: Tuned models of peer assessment in moocs. In: Proceedings of the 6th International Conference on Educational Data Mining (EDM). pp. 153–160 (2013)
19. Schnabel, T., Swaminathan, A., Frazier, P.I., Joachims, T.: Unbiased comparative evaluation of ranking functions. In: Proceedings of the 2016 ACM International Conference on the Theory of Information Retrieval. pp. 109–118 (2016)
20. Shah, N.B., Tabibian, B., Muandet, K., Guyon, I., Von Luxburg, U.: Design and analysis of the NIPS 2016 review process. The Journal of Machine Learning Research 19(1), 1913–1946 (2018)
21. Stelmakh, I., Shah, N.B., Singh, A.: Catch me if I can: Detecting strategic behaviour in peer assessment. arXiv preprint arXiv:2010.04041 (2020)
22. Walsh, T.: The PeerRank method for peer assessment. In: Proceedings of the 21st European Conference on Artiﬁcial Intelligence (ECAI). pp. 909–914. Prague, Czech Republic (August 2014)
23. Wang, J., Shah, N.B.: Your 2 is my 1, your 3 is my 9: Handling arbitrary miscalibrations in ratings. In: Proceedings of the18thInternational Joint Conference on Autonomous Agents and Multi-Agent Systems (AAMAS). pp. 864–872 (2019)
24. Wang, J., Stelmakh, I., Wei, Y., Shah, N.B.: Debiasing evaluations that are biased by evaluations. arXiv preprint arXiv:2012.00714 (2020)
25. Whitehill, J., Wu, T.f., Bergsma, J., Movellan, J., Ruvolo, P.: Whose vote should count more: Optimal integration of labels from labelers of unknown expertise. In: Advances in Neural Information Processing Systems. pp. 2035–2043 (2009)
26. Xia, L.: Learning and Decision-Making from Rank Data. Synthesis Lectures on Artiﬁcial Intelligence and Machine Learning, Morgan and Claypool (January 2019)
27. Xu, Y., Zhao, H., Shi, X., Shah, N.B.: On strategyproof conference peer review. In: Proceedings of the 28th International Joint Conference on Artiﬁcial Intelligence (IJCAI). pp. 616–622. Macau (August 2019)

