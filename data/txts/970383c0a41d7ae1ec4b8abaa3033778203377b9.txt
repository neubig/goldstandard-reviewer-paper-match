Mitigating Noisy Inputs for Question Answering
Denis Peskov,1 Joe Barrow,1 Pedro Rodriguez,1 Graham Neubig,2 Jordan Boyd-Graber3
1University of Maryland Department of Computer Science and UMIACS 2Carnegie Mellon University Language Technology Institute
3University of Maryland Department of Computer Science, iSchool, UMIACS, and LSC
{dpeskov, jdbarrow, pedro}@cs.umd.edu, gneubig@cs.cmu.edu, jbg@umiacs.umd.edu

arXiv:1908.02914v1 [cs.CL] 8 Aug 2019

Abstract
Natural language processing systems are often downstream of unreliable inputs: machine translation, optical character recognition, or speech recognition. For instance, virtual assistants can only answer your questions after understanding your speech. We investigate and mitigate the effects of noise from Automatic Speech Recognition systems on two factoid Question Answering (QA) tasks. Integrating conﬁdences into the model and forced decoding of unknown words are empirically shown to improve the accuracy of downstream neural QA systems. We create and train models on a synthetic corpus of over 500,000 noisy sentences and evaluate on two human corpora from Quizbowl and Jeopardy! competitions.1
1. Introduction
Progress on question answering (QA) has claimed humanlevel accuracy. However, most factoid QA models are trained and evaluated on clean text input, which becomes noisy when questions are spoken due to Automatic Speech Recognition (ASR) errors. This consideration is disregarded in trivia match-ups between machines and humans: IBM Watson [1] on Jeopardy! and Quizbowl matches between machines and trivia masters [2] provide text data for machines while humans listen. A fair test would subject both humans and machines to speech input.
Unfortunately, there are no large spoken corpora of factoid questions with which to train models; text-tospeech software can be used as a method for generating training data at scale for question answering models (Section 2). Although synthetic data is less realistic than true human-spoken questions it easier and cheaper to collect at scale, which is important for training. These synthetic data are still useful; in Section 4.1, models trained on synthetic data are applied to human spoken data from Quizbowl tournaments and Jeopardy!
Noisy ASR is particularly challenging for QA systems (Figure 1). While humans and computers might know the title of a “revenge novel centering on Edmund Dantes by Alexandre Dumas”, transcription errors may mean deciphering “novel centering on edmond dance by alexander <unk>” instead. Dantes and Dumas are low-frequency words in the English language and hence likely to be mis-
1To appear at INTERSPEECH 2019

Figure 1: ASR errors on QA data: original spoken words (top of box) are garbled (bottom). While many words become into “noise”— frequent words or the unknown token—consistent errors (e.g., “clarendon” to “clarintin”) can help downstream systems. Additionally, words reduced to <unk> (e.g., “kermit”) can be useful through forced decoding into the closest incorrect word (e.g., “hermit” or even “car”).
interpreted by a generic ASR model; however, they are particularly important for answering the question. Additionally, the introduction of distracting words (e.g., “dance”) causes QA models to make errors [3]. Section 2.1 characterizes the signal in this noise: key terms like named entities are often missing, which is detrimental for QA.
Previous approaches to mitigate ASR noise for answering mobile queries [4] or building bots [5] typically use unsupervised methods, such as term-based information retrieval. Our datasets for training and evaluation can produce supervised systems that directly answer spoken questions. Machine translation [6] also uses ASR conﬁdences; we evaluate similar methods on QA.
Speciﬁcally, some accuracy loss from noisy inputs can be mitigated through a combination of forcing unknown words to be decoded as the closest option (Section 3.2), and incorporating the uncertainties of the ASR model directly in neural models (Section 3.3).
The forced decoding method reconstructs missing terms by using terms similar to the transcribed input. Word-level conﬁdence scores incorporate uncertainty from the ASR system into neural models. Section 4 compares these methods against baseline methods on our synthetic and human speech datasets for Jeopardy! and Quizbowl.

2. Spoken question answering datasets
Neural networks require a large training corpus, but recording hundreds of thousands of questions is not feasible. Crowd-sourcing with the required quality control (speakers who say “cyclohexane” correctly) is expensive. As an alternative, we generate a data-set with Google Textto-Speech on 96,000 factoid questions from a trivia game called Quizbowl [2], each with 4–6 sentences for a total of over 500,000 sentences.2 We then decode these utterances using the Kaldi chain model [7], trained on the FischerEnglish dataset [8] for consistency with past results on mitigating ASR errors in MT [6]. This model has a Word Error Rate (WER) of 15.60% on the eval2000 test set. The WER increases to 51.76% on our Quizbowl data, which contains out of domain vocabulary. The most BLEU improvement in machine translation under noisy conditions could be found in this middle WER range, rather than in values below 20% or above 80% [6]. Retraining the model on the Quizbowl domain would mitigate this noise; however, in practice one is often at the mercy of a pretrained recognition model due to changes in vocabularies or speakers. Intentional noise has been added to machine translation data [9, 10]. Alternate methods for collecting large scale audio data include Generative Adversarial Networks [11] and manual recording [12].
The task of QA requires the system to provide a correct answer out of many candidates based on the question’s wording. We test on two varieties of different length and framing. Quizbowl questions, which are generally four to six sentences, tests a user’s depth of knowledge; early clues are challenging and obscure but they progressively become easy and well-known. Competitors can answer these types of questions at any point. Computer QA is competitive with the top players [13]. Jeopardy! questions are single sentences and can only be answered after the question ends. To test this alternate syntax, we use the same method of data generation on a dataset of over 200,000 Jeopardy questions [14].
2.1. Why QA is challenging for ASR
ASR changes the features of the recognized text in several important ways: the overall vocabulary is quite different and important words are corrupted. First, it reduces the overall vocabulary. In our dataset, the vocab drops from 263,271 in the original data to a mere 33,333. This is expected, as ASR only has 42,000 words in its vocab, so the long tail of the Zipf’s curve is lost. Second, unique words—which may be central to answering the question— are lost or misinterpreted; over 100,000 of the words in the original data occur only once. Finally, ASR systems tend to deletes words which makes the sentences shorter; in our case, the average length decreases from 21.62 to 18.85 words per sentence.
The decoding system is able to express uncertainty by predicting <unk>. These account for slightly less than 10% of all our word tokens, but is a top-2 prediction for
2http://cloud.google.com/text-to-speech

30% of the 260,000 original words. For QA, words with a high TF-IDF measure are valuable. While some words are lost, others can likely be recovered: “hellblazer’ becomes “blazer”, “clarendon” becoming “claritin”. We evaluate this by ﬁtting a TF-IDF model on the Wikipedia dataset and then comparing the average TF-IDF per sentence between the original and the ASR data. The average TF-IDF score drops from 3.52 to 2.77 per sentence.
3. Mitigating noise
This section discusses two approaches to mitigating the effects of missing and corrupted information caused by ASR systems. The ﬁrst approach—forced decoding—exploits systematic errors to arrive at the correct answer. The second uses conﬁdence information from the ASR system to down-weight the inﬂuence of low-conﬁdence terms. Both approaches improve accuracy over a baseline DAN model and show promise for short single-sentence questions. An IR approach is more effective on long questions.
3.1. IR baseline
The IR baseline reframes Jeopardy! and Quizbowl QA tasks as document retrieval ones with an inverted search index. We create one document per distinct answer; each document has a text ﬁeld formed by concatenating all questions with that answer together. At test time questions are treated as queries, and documents are scored using BM25 [15, 16]. We implement this baseline with Elastic Search and Apache Lucene.
3.2. Forced decoding
We have systematically lost information. We could predict the answer if we had access to certain words in the original question and further postulate that wrong guesses are better than knowing that a word is unknown.
We explore commerical solutions—Bing, Google, IBM, Wit—with low transcription errors. However, their APIs ensure that an end-user often cannot extract anything more than one-best transcriptions, along with an aggregate conﬁdence for the sentence. Additionally, the proprietary systems are moving targets, harming reproducibility.
We use Kaldi [17] for all experiments. Kaldi is a commonly-used, open-source tool for ASR; its maximal transparency enables approaches that incorporate uncertainty into downstream models. Kaldi provides not only top-1 predictions, but also conﬁdences of words, entire lattices, and phones (Table 1). Conﬁdences are the same length as the text, range from 0.0 to 1.0 in value, and correspond to the respective word or phone in the sequence.
The typical end-use of an ASR system wants to know when when a word is not recognized. By default, a graph will have a token that represents an unknown; in Kaldi, this becomes <unk>. At a human-level, one would want to know that an out of context word happened.
However, when the end-user is a downstream model, a systematically wrong prediction may be better than a generic statement of uncertainty. So by removing all refer-

Table 1: As original data are translated through ASR, it degrades in quality. One-best output captures per-word conﬁdence. Full lattices provide additional words and phone data captures the raw ASR sounds. Conﬁdence models and forced decoding could be used for such data.

Clean 1-Best “Lattice” Phones

For 10 points, name this revenge novel centering on Edmond Dantes, written by Alexandre Dumas . . . for0.935 ten0.935 points0.871 same0.617 this1 . . . revenge novel centering on <unk> written by alexander <unk> . . . for0.935 [eps]0.064 pretend0.001 ten0.935 . . . pretend point points point name same named name names this revenge novel . . . f_B0.935 er_E0.935 t_B0.935 eh_I1 n_E0.935 . . . p_B oy_I n_I t_I s_E sil s_B ey_I m_E dh_B ih_I s_E r_B iy_I v_I eh_I n_I jh_E n_B aa_I v_I ah_I l_I . . .

ence to <unk> in the model’s Finite State Transducer, we force the system to decode “Louis Vampas” as “Louisiana” rather than <unk>. The risk we run with this method is introducing words not present in the original data. For example, “count” and “mount” are similar in sound but not in context embeddings. Hence, we need a method to downweight incorrect decoding.
3.3. Conﬁdence augmented DAN
We build on Deep Averaging Networks [18, DAN], assuming that deep bag-of-words models can improve predictions and be robust to corrupted phrases. The errors introduced by ASR can hinder sequence neural models as key phrases are potentially corrupted and syntactic information is lost.
The original Deep Averaging Network, or DAN, classiﬁer has three sections: a “neural-bag-of-words” (NBOW) encoder, which composes all the words in the document into a single vector by averaging the word vectors; a series of hidden transformations, which give the network depth and allow it to amplify small distinctions between composed documents; and a softmax predictor.
The encoded representation r is the averaged embeddings of input words. The word vectors exist in an embedding matrix E, from which we can look up a speciﬁc word w with E[w]. The length of the document is N . To compute the composed representation r, the DAN averages all of the word embeddings:
r = Ni E[wi] (1) N
The network weights W, consist of a weight-bias pair for each layer of transformations (W(hi), b(hi)) for each layer i in the list of layers L. To compute the hidden representations for each layer, the DAN linearly transforms the input and then applies a nonlinearity: h0 = σ(W(h0)r + b(h0)). Successive hidden representations hi are: hi = σ(W(hi)hi-1 +b(hi)). The ﬁnal layer in the DAN is a softmax

output: o = softmax(W(o)hL + b(o)). We modify the original DAN models to use word-level conﬁdences from the ASR system as a feature.
In increasing order of complexity, the variations are: a Conﬁdence Informed Softmax DAN, a Conﬁdence Weighted Average DAN, and a Word-Level Conﬁdence DAN. We represent the conﬁdences as a vector c, where each cell ci contains the ASR conﬁdence of word wi.
The simplest model averages the conﬁdence across the whole sentence and adds it as a feature to the ﬁnal output classiﬁer. For example in Table 1, “for ten points” averages to 0.914. We introduce an additional weight in the output Wc, which adjusts our prediction based on the average conﬁdence of each word in the question.
However, most words have high conﬁdence, and thus the average conﬁdence of a sentence or question level is high. To focus on which words are uncertain we weight the word embeddings by their conﬁdence attenuating uncertain words before calculating the DAN average.
Weighting by the conﬁdence directly removes uncertain words, but this is too blunt an instrument, and could end up erasing useful information contained in lowconﬁdence words, so we instead learn a function based on the raw conﬁdence from our ASR system. Thus, we recalibrate the conﬁdence through a learned function f :

f (c) = W(c)c + b(c)

(2)

and then use that scalar in the weighted mean of the DAN representation layer:

r** = Ni E[wi] ∗ f (ci) . (3) N
In this model, we replace the original encoder r with the new version r** to learn a transformation of the ASR conﬁdence that down-weights uncertain words and upweights certain words. This ﬁnal model is referred to in the results as “Conﬁdence Model”.
Architectural decisions are determined by hyperparameter sweeps. They include: having a single hidden layer of 1000 dimensionality for the DAN, multiple dropout, batch-norm layers, and a scheduled ADAM optimizer. Our DAN models train until convergence, as determined by early-stopping. Code is implemented in PyTorch [19], with TorchText for batching.3

4. Results
Achieving 100% accuracy on this dataset is not a realistic goal, as not all test questions are answerable (speciﬁcally, some answers do not occur in the training data and hence cannot be learned by a machine learning system). Baselines for the DAN (Table 2) establish realistic goals: a DAN trained and evaluated on the same train and dev set, only in the original non-ASR form, correctly predicts 54% of
3Code, data, and additional analysis available at https://
github.com/DenisPeskov/QBASR

Table 2: Both forced decoding (FD) and the best conﬁdence model improve accuracy for the DAN. Jeopardy only has an At-End-of-Sentence metric, as questions are one sentence in length. Combining the two methods leads to a further joint improvement. The IR and DAN accuracies on clean data are provided as a reference.

Quizbowl

Synth

Human

Method

Start End Start End

Methods Tested on Clean Data

IR

0.064 0.544 0.400

DAN

0.080 0.540 0.200

1.000 1.000

Methods Tested on Corrupted Data

IR

0.021 0.442 0.180

DAN

0.035 0.335 0.120

FD

0.032 0.354 0.120

Conﬁdence 0.036 0.374 0.120

FD+Conf 0.041 0.371 0.160

0.560 0.440 0.440 0.460 0.440

Jeopardy! Synth Human

0.190 0.236

0.050 0.033

0.079 0.097 0.102 0.095 0.109

0.050 0.017 0.033 0.033 0.033

the answers. Noise drops this to 44% with the best IR model and down to ≈ 30% with neural approaches.
The noisy data quality makes full recovery unlikely and we view any improvement over the neural model baselines as recovering valuable information. At the questionlevel, a strong IR model outperforms the DAN by around 10%. Since IR can avoid all the noise while beneﬁting from additional independent data points, it scales as the length of data increases. There is additional motivation to investigate this task at the sentence-level. Computers can beat humans at the game by knowing certain questions immediately; the ﬁrst sentence of the Quizbowl question serves as a proxy for this threshold. Our proposed combination of forced decoding with a neural model led to the highest test accuracy results and outperforms the IR one at the sentence level.
A strong TF-IDF IR model can top the best neural model at the multi-sentence question level in Quizbowl; multiple sentences are important because they progressively become easier to answer in competitions. However, our models improve accuracy on the shorter ﬁrst-sentence level of the question. This behavior is expected since textscir methods are explicitly designed to disregard noise and can pinpoint the handful of unique words in a long paragraph; conversely they are less accurate when they extract words from a single sentence.
4.1. Qualitative analysis & human data
The synthetic dataset facilitates large-scale machine learning, but ultimately we care about performance on human data. For Quizbowl we record questions read by domain experts at a competition. To account for variation in speech, we record ﬁve questions across ten different speakers, varying in gender and age; this set of ﬁfty questions is used as the human test data. Figure 3 has examples of variations. For Jeopardy! we manually parsed a complete episode by question.
The predictions of the regular DAN and the conﬁdence version can differ. For input about

Table 3: Variation in different speakers causes different transcriptions of a question on Oxford, which can lead to different DAN predictions.

Speaker Base S1 S2 S3

Text
John Deydras, an insane man who claimed to be Edward II, stirred up trouble when he seized this city’s Beaumont Palace.
<unk> an insane man who claimed to be the second <unk> trouble when he sees <unk> beaumont→ The Rivals
<unk> dangerous insane man who claim to be <unk> second third of trouble when he sees the city’s unk palace → Rome
<unk> and then say man you claim to be the second stir up trouble when he sees the city’s beaumont <unk> → London

The House on Mango Street, which contains words like “novel”, “character”, and “childhood” alongside a corrupted name of the author, the regular DAN predicts The Prime of Miss Jean Brodie, while our version predicts the correct answer.
4.2. Discussion & future work
Conﬁdences are a readily human-interpretable concept that may help build trust in the output of a system. Transparency in the quality of up-stream content can lead to downstream improvements in a plethora of NLP tasks.
Exploring sequence models or alternate data representations may lead to further improvement. Including full lattices may mirror past results for machine translation [6] for the task of question answering. Phone-level approaches work in Chinese [12], but our phone models had lower accuracies than the baseline, perhaps due to a lack of contextual representation. Using unsupervised approaches for ASR [20, 21] and training ASR models for decoding Quizbowl or Jeopardy! words are avenues for further exploration.
5. Conclusion
Question answering, like many NLP tasks are impaired by noisy inputs. Introducing ASR into a QA pipeline corrupts the data. A neural model that uses the ASR system’s conﬁdence outputs and systematic forced decoding of words rather than unknowns improves QA accuracy on Quizbowl and Jeopardy! questions. Our methods are task agnostic and can be applied to other supervised NLP tasks. Larger human-recorded question datasets and alternate model approaches would ensure spoken questions are answered accurately, allowing human and computer trivia players to compete on an equal playing ﬁeld.
6. Acknowledgments
This work was supported by NSF Grants IIS-1748663 and IIS-1748642. The views expressed in this paper are our own. We thank the reviewers, the Quizbowl and Kaldi communities, and Yogarshi Vyas for their help.

7. References
[1] D. A. Ferrucci, “Build Watson: an overview of DeepQA for the Jeopardy! challenge,” in 19th International Conference on Parallel Architecture and Compilation Techniques, 2010, pp. 1–2.
[2] J. Boyd-Graber, S. Feng, and P. Rodriguez, HumanComputer Question Answering: The Case for Quizbowl. Springer Verlag, 2018.
[3] R. Jia and P. Liang, “Adversarial examples for evaluating reading comprehension systems,” in Proceedings of Empirical Methods in Natural Language Processing, 2017, pp. 2021–2031.
[4] T. Mishra and S. Bangalore, “Qme!: A speech-based question-answering system on mobile devices,” in Human Language Technologies:, 2010, pp. 55–63.
[5] A. Leuski, R. Patel, D. Traum, and B. Kennedy, “Building effective question answering characters,” in Proceedings of the Annual SIGDIAL Meeting on Discourse and Dialogue, 2009, pp. 18–27.
[6] M. Sperber, G. Neubig, J. Niehues, and A. Waibel, “Neural lattice-to-sequence models for uncertain inputs,” in Proceedings of the Association for Computational Linguistics, 2017.
[7] V. Peddinti, G. Chen, V. Manohar, T. Ko, D. Povey, and S. Khudanpur, “Jhu aspire system: Robust lvcsr with tdnns, ivector adaptation and rnn-lms,” in Automatic Speech Recognition and Understanding (ASRU), IEEE Workshop on, 2015, pp. 539–546.
[8] C. Cieri, D. Miller, and K. Walker, “The ﬁsher corpus: a resource for the next generations of speech-to-text,” in Proceedings of the Language Resources and Evaluation Conference, 2004.
[9] P. Michel and G. Neubig, “Mtnt: A testbed for machine translation of noisy text,” in Proceedings of Empirical Methods in Natural Language Processing, 2018.
[10] Y. Belinkov and Y. Bisk, “Synthetic and natural noise both break neural machine translation,” in Proceedings of the International Conference on Learning Representations, 2017.
[11] C. Donahue, B. Li, and R. Prabhavalkar, “Exploring speech enhancement with generative adversarial networks for robust speech recognition,” in IEEE International Conference on Acoustics, Speech and Signal Processing, 2018, pp. 5024–5028.
[12] C.-H. Lee, S.-M. Wang, H.-C. Chang, and H.-Y. Lee, “Odsqa: Open-domain spoken question answering dataset,” in 2018 IEEE Spoken Language Technology Workshop (SLT). IEEE, 2018, pp. 949–956.
[13] I. Yamada, R. Tamaki, H. Shindo, and Y. Takefuji, “Studio Ousia’s quiz bowl question answering system,” in NIPS Competition: Building Intelligent Systems, 2018, pp. 181– 194.
[14] M. Dunn, L. Sagun, M. Higgins, V. U. Güney, V. Cirik, and K. Cho, “Searchqa: A new Q&A dataset augmented with context from a search engine,” CoRR, vol. abs/1704.05179, 2017.
[15] J. Ramos, “Using tf-idf to determine word relevance in document queries,” in Proceedings of the International Conference of Machine Learning, 2003.

[16] S. Robertson, H. Zaragoza et al., “The probabilistic relevance framework: Bm25 and beyond,” Foundations and Trends in Information Retrieval, vol. 3, no. 4, pp. 333–389, 2009.
[17] D. Povey, A. Ghoshal, G. Boulianne, N. Goel, M. Hannemann, Y. Qian, P. Schwarz, and G. Stemmer, “The Kaldi speech recognition toolkit,” in IEEE Workshop on Automatic Speech Recognition and Understanding, 2011.
[18] M. Iyyer, V. Manjunatha, J. Boyd-Graber, and H. Daumé III, “Deep unordered composition rivals syntactic methods for text classiﬁcation,” in Proceedings of the Association for Computational Linguistics, 2015.
[19] A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. DeVito, Z. Lin, A. Desmaison, L. Antiga, and A. Lerer, “Automatic differentiation in pytorch,” in Conference on Neural Information Processing Systems: Autodiff Workshop: The Future of Gradient-based Machine Learning Software and Techniques, 2017.
[20] F. Wessel and H. Ney, “Unsupervised training of acoustic models for large vocabulary continuous speech recognition,” IEEE Transactions on Speech and Audio Processing, vol. 13, no. 1, pp. 23–31, 2004.
[21] H. Lee, P. Pham, Y. Largman, and A. Y. Ng, “Unsupervised feature learning for audio classiﬁcation using convolutional deep belief networks,” in Proceedings of Advances in Neural Information Processing Systems, 2009, pp. 1096–1104.
[22] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu, “BLEU: a method for automatic evaluation of machine translation,” in Proceedings of the Association for Computational Linguistics, 2002, pp. 311–318.
[23] C. E. Shannon, “A mathematical theory of communication,” Bell system technical journal, vol. 27, no. 3, pp. 379–423, 1948.

Figure 2: A comparison of BLEU score distributions across human speakers (color-coded) to our artiﬁcial method, visualized by the step line. The distributions of BLEU scores are similar, with human data being slightly lower, justifying our weak supervision training approach.

Figure 3: Similarly a comparison of WER score distributions across human speakers (color-coded) to our artiﬁcial method, visualized by the step line. The distributions of WER scores are similar as well. Speakers are color-coded. The background step line is the WER of the automatic TTS approach.

A. Further Data Analysis
One potential concern with the synthetically-generated dataset is that ASR systems might be either better or worse at recognizing text-to-speech(TTS) speech. If the ASR system is trained on human data, then it might be an out-of-domain sample, or there might be systematic pronunciation issues that lower ASR accuracy. Alternatively, TTS-generated speech might prove more regular or cleaner than human speech, so an ASR system may produce a higher transcription accuracy on this data. Thus, we determine the distributional overlap between the ASR output on both the synthetic and natural data.
We compare BLEU scores [22] between the gold standard data and the decoded data for between the human and synthetic data variations. By using BLEU scores, which capture n-gram overlap between the target and source text, we can compare the variance in ASR between the two datasets. Figure 2 illustrates this variance. Additionally, Figure 3 shows the comparison of Word Error Rate (WER). Human data has more instances of higher WER and lower BLEU scores than the auto-generated data on the same questions; however, the two sources of speech data generally follow a similar distribution and our results are comparable in accuracy to our synthetic data. Therefore, we conclude that our method serves as a good approximation for the task, which allows weak supervision to work.
B. Negative Results
Alternative methods were applied to mitigate ASR-induced noise in the course of experimentation, including noisy channel techniques typically used in Information Retrieval and lattice-structured Recurrent Neural Networks. For completeness, we discuss the results of these two experiments in this section. While neither method provided an improvement on the question answering task, their discussion might prove useful for future research.

B.1. Noisy Channel Expansion
In both Information Retrieval and NLP it is often useful to model processes that induce noise using Shannon’s noisy channel model [23]. We know the answer would be predictable if we had access to certain words in the original question. The noisy channel model allows us to reconstruct the original data as cleanly as possible by modeling the process by which noise was induced, in this case the trip from text to speech and back to text. We propose two forms of query expansion based on this model, both of which are typically used in Cross Language Information Retrieval.
The ﬁrst model uses IBM Model 3 to generate an alignment table between the corrupted ASR data and the original text data. The alignment table serves as the underlying corruption model which we are aiming to reverse. We use our training data a second time and generate possible word candidates that were missed during decoding.
The second model uses a more robust version of the same Information Retrieval technique looks at twoway translations between ASR and original data based on (Xu, 2008). Whereas the ﬁrst model included many junk translations—stop-words such as “unk” or “the” would be mapped to a long tail of meaningful words—this version does not suffer from this problem: even if “the” maps to “Monte”, “Monte” does not map back to “the”.
In both cases, the reconstructed data was used to train the DAN model. That neither was able to improve over the conﬁdence modeling DAN indicates that the errors made by the ASR system were likely not recoverable with the translation models we used. This is unsurprising, as many low-frequency important words were mapped to a handful of high-frequency terms, collapsing the space and preventing simple recoverability.
B.2. Lattice-Structured RNN
The conﬁdence models are not calculate on a full lattice, and hence cannot not reconstruct alternate paths in situations with low conﬁdences. A more complex model can

ingest the entire lattice, and not the top word prediction. The lattice can update multiple words needed, as their relationships are preserved. “Leo Patrick” can now be reinterpreted as “Cleopatra”, as the lattice relationship allows alternate paths to be explored. The conﬁdence values provide additional value about what path to follow within a lattice.
We produce three variations:
1. A “lattice” LSTM that consumes the full lattice by linearizing the graphs with a topological sort and feeding it through a normal LSTM.
2. A lattice LSTM without conﬁdences. This network only sees the word vectors when consuming the lattice structure.
3. A lattice LSTM with conﬁdences integrated as features. The conﬁdences are concatenated to the word vector inputs.
This sequence demonstrates the gain from each part of the model. The ﬁrst tests the beneﬁt of additional data. The second tests the beneﬁt of the structure of this data. The third tests the importance of the conﬁdence of each item in the data.
Unfortunately, none of these experiments outperformed the conﬁdence augmented DAN. These may be due to instability or training issues, however.

