Linearly Converging Error Compensated SGD

arXiv:2010.12292v1 [math.OC] 23 Oct 2020

Eduard Gorbunov MIPT, Yandex and Sirius, Russia
KAUST, Saudi Arabia
Dmitry Makarenko MIPT, Russia

Dmitry Kovalev KAUST, Saudi Arabia
Peter Richtárik KAUST, Saudi Arabia

Abstract
In this paper, we propose a uniﬁed analysis of variants of distributed SGD with arbitrary compressions and delayed updates. Our framework is general enough to cover diﬀerent variants of quantized SGD, Error-Compensated SGD (EC-SGD) and SGD with delayed updates (D-SGD). Via a single theorem, we derive the complexity results for all the methods that ﬁt our framework. For the existing methods, this theorem gives the best-known complexity results. Moreover, using our general scheme, we develop new variants of SGD that combine variance reduction or arbitrary sampling with error feedback and quantization and derive the convergence rates for these methods beating the state-of-the-art results. In order to illustrate the strength of our framework, we develop 16 new methods that ﬁt this. In particular, we propose the ﬁrst method called EC-SGD-DIANA that is based on error-feedback for biased compression operator and quantization of gradient diﬀerences and prove the convergence guarantees showing that EC-SGD-DIANA converges to the exact optimum asymptotically in expectation with constant learning rate for both convex and strongly convex objectives when workers compute full gradients of their loss functions. Moreover, for the case when the loss function of the worker has the form of ﬁnite sum, we modiﬁed the method and got a new one called EC-LSVRG-DIANA which is the ﬁrst distributed stochastic method with error feedback and variance reduction that converges to the exact optimum asymptotically in expectation with a constant learning rate.

1 Introduction

We consider distributed optimization problems of the form

n

min f (x) = n1 fi(x) ,

(1)

x∈Rd

i=1

where n is the number of workers/devices/clients/nodes. The information about function
fi is stored on the i-th worker only. Problems of this form appear in the distributed or federated training of supervised machine learning models [42, 30]. In such applications, x ∈ Rd describes the parameters identifying a statistical model we wish to train, and fi is the (generalization or empirical) loss of model x on the data accessible by worker i. If worker
i has access to data with distribution Di, then fi is assumed to have the structure

fi(x) = Eξi∼Di [fξi (x)] .

(2)

Dataset Di may or may not be available to worker i in its entirety. Typically, we assume that worker i has only access to samples from Di. If the dataset is fully available, it is typically

34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.

ﬁnite, in which case we can assume that fi has the ﬁnite-sum form1:

m

fi(x)

=

1 m

fij (x).

(3)

j=1

Communication bottleneck. The key bottleneck in practical distributed [14] and federated [30, 21] systems comes from the high cost of communication of messages among the clients needed to ﬁnd a solution of suﬃcient qualities. Several approaches to addressing this communication bottleneck have been proposed in the literature.

In the very rare situation when it is possible to adjust the network architecture connecting the clients, one may consider a fully decentralized setup [6], and allow each client in each iteration to communicate to their neighbors only. One can argue that in some circumstances and in a certain sense, decentralized architecture may be preferable to centralized architectures [34]. Another natural way to address the communication bottleneck is to do more meaningful (which typically means more expensive) work on each client before each communication round. This is done in the hope that such extra work will produce more valuable messages to be communicated, which hopefully results in the need for fewer communication rounds. A popular technique of this type which is particularly relevant to Federated Learning is based in applying multiple local updates instead of a single update only. This is the main idea behind Local-SGD [43]; see also [4, 15, 22, 24, 29, 45, 49]. However, in this paper, we contribute to the line work which aims to resolve the communication bottleneck issue via communication compression. That is, the information that is normally exchanged—be it iterates, gradients or some more sophisticated vectors/tensors—is compressed in a lossy manner before communication. By applying compression, fewer bits are transmitted in each communication round, and one hopes that the increase in the number of communication rounds necessary to solve the problem, if any, is compensated by the savings, leading to a more eﬃcient method overall.

Error-feedback framework. In order to address these issues, in this paper we study a broad class of distributed stochastic ﬁrst order methods for solving problem (1) described by the iterative framework

n

xk+1 = xk − n1 vik,

(4)

i=1

eki +1 = eki + γgik − vik, i = 1, 2, . . . , n.

(5)

In this scheme, xk represents the key iterate, vik is the contribution of worker i towards the update in iteration k, gik is an unbiased estimator of ∇fi(xk) computed by worker i, γ > 0 is a ﬁxed stepsize and eki is the error accumulated at node i prior to iteration k (we set to e0i = 0 for all i). In order to better understand the role of the vectors vik and eki , ﬁrst consider the simple special case with vik ≡ γgik. In this case, eki = 0 for all i and k, and method (4)–(5) reduces to distributed SGD:

n

xk+1 = xk − nγ gik.

(6)

i=1

However, by allowing to chose the vectors vik in a diﬀerent manner, we obtain a more general update rule than what the SGD update (6) can oﬀer. Stich and Karimireddy [45], who studied (4)–(5) in the n = 1 regime, show that this ﬂexibility allows to capture several types of methods, including those employing i) compressed communication, ii) delayed gradients, and iii) minibatch gradient updates. While our general results apply to all these special cases and more, in order to not dilute the focus of the paper, in the main body of this paper we concentrate on the ﬁrst use case—compressed communication—which we now describe.

Error-compensated compressed gradient methods. Note that in distributed SGD (6),

each worker needs to know the aggregate gradient gk = n1

n i=1

gik

to

form

xk+1,

which

is needed before the next iteration can start. This can be achieved, for example, by each

1The implicit assumption that each worker contains exactly m data points is for convenience/simplicity only; all our results direct analogues in the general setting with mi data points on worker i.

2

worker i communicating their gradient gik to all other workers. Alternatively, in a parameter server setup, a dedicated master node collects the gradients from all workers, and broadcasts their average gk to all workers. Instead of communicating the gradient vectors gik, which is expensive in distributed learning in general and in federated learning in particular, and
especially if d is large, we wish to communicate other but closely related vectors which can
be represented with fewer bits. To this eﬀect, each worker i sends the vector

vik = C(eki + γgik), ∀i ∈ [n]

(7)

instead, where C : Rd → Rd is a (possibly randomized, and in such a case, drawn independently of all else in iteration k) compression operator used to reduce communication. We assume throughout that there exists δ ∈ (0, 1] such that the following inequality holds for all x ∈ Rd

E C(x) − x 2 ≤ (1 − δ) x 2.

(8)

For any k ≥ 0, the vector eki +1 =

k t=0

γgit

−

vit

captures

the

error

accumulated

by

worker

i up to iteration k. This is the diﬀerence between the ideal SGD update

k t=0

γgit

and

the

applied update

k t=0

vit.

As

we

see

in

(7),

at

iteration

k

the

current

error

eki

is

added

to

the

gradient update γgik—this is referred to as error feedback—and subsequently compressed,

which deﬁnes the update vector vik. Compression introduces additional error, which is added

to eki , and the process is repeated.

Compression operators. For a rich collection of speciﬁc operators satisfying (8), we refer the reader to Stich and Karimireddy [45] and Beznosikov et al. [7]. These include various unbiased or contractive sparsiﬁcation operators such as RandK and TopK, respectively, and quantization operators such as natural compression and natural dithering [18]. Several additional comments related to compression operators are included in Section B.

2 Summary of Contributions
We now summarize the key contributions of this paper.
General theoretical framework. In this work we propose a general theoretical framework for analyzing a wide class of methods that can be written in the the error-feedback form (4)-(5). We perform complexity analysis under µ-strong quasi convexity (Assumption 3.1) and L-smoothness (Assumption 3.2) assumptions on the functions f and {fi}, respectively. Our analysis is based on an additional parametric assumption (using parameters A, A , B1, B1, B2, B2, C1, C2, D1, D1, D2, D3, η, ρ1, ρ2, F1, F2, G) on the relationship between the iterates xk, stochastic gradients gk, errors ek and a few other quantities (see Assumption 3.4, and the stronger Assumption 3.3). We prove a single theorem (Theorem 3.1) from which all our complexity results follow as special cases. That is, for each existing or new speciﬁc method, we prove that one (or both) of our parametric assumptions holds, and specify the parameters for which it holds. These parameters have direct impact on the theoretical rate of the method. A summary of the values of the parameters for all methods developed in this paper is provided in Table 5 in the appendix. We remark that the values of the parameters A, A , B1, B1, B2, B2, C1, C2 and ρ1, ρ2 inﬂuence the theoretical stepsize.
Sharp rates. For existing methods covered by our general framework, our main convergence result (Theorem 3.1) recovers the best known rates for these methods up to constant factors.
Eight new error-compensated (EC) methods. We study several speciﬁc EC methods for solving problem (1). First, we recover the EC-SGD method ﬁrst analyzed in the n = 1 case by Stich and Karimireddy [45] and later in the general n ≥ 1 case by Beznosikov et al. [7]. More importantly, we develop eight new methods: EC-SGDsr, EC-GDstar, EC-SGD-DIANA, EC-SGDsr-DIANA, EC-GD-DIANA, EC-LSVRG, EC-LSVRGstar and EC-LSVRG-DIANA. Some of these methods are designed to work with the expectation structure of the local functions fi given in (2), and others are speciﬁcally designed to exploit the ﬁnite-sum structure (3). All these methods follow the error-feedback framework (4)–(5), with vik chosen as in (7). They diﬀer in how the gradient estimator gik is constructed (see Table 2 for a compact description

3

Table 1: Complexity of Error-Compensated SGD methods established in this paper. Symbols:
ε = error tolerance; δ = contraction factor of compressor C; ω = variance parameter of compressor Q; κ = L/µ; L = expected smoothness constant; σ∗2 = variance of the stochastic gradients in the solution; ζ∗2 = average of ∇fi(x∗) 2; σ2 = average of the uniform bounds for the variances of stochastic gradients of workers. EC-GDstar, EC-LSVRGstar and
EC-LSVRG-DIANA are the ﬁrst EC methods with a linear convergence rate without assuming that ∇fi(x∗) = 0 for all i. EC-LSVRGstar and EC-LSVRG-DIANA are the ﬁrst EC methods
with a linear convergence rate which do not require the computation of the full gradient ∇fi(xk) by all workers in each iteration. Out of these three methods, only EC-LSVRG-DIANA is practical. †EC-GD-DIANA is a special case of EC-SGD-DIANA where each worker i computes the full gradient ∇fi(xk).

Problem (1)+(3)

Method EC-SGDsr

Alg # Alg 3

Citation new

Sec # J.1

(1)+(2) (1)+(3)

EC-SGD EC-GDstar

Alg 4

[45]

J.2

Alg 5

new

J.3

(1)+(2)

EC-SGD-DIANA

Alg 6

new

J.4

(1)+(3)

EC-SGDsr-DIANA

Alg 7

new

J.5

(1)+(2)

EC-GD-DIANA†

Alg 6

new

J.4

(1)+(3)

EC-LSVRG

Alg 8

new

J.6

(1)+(3)

EC-LSVRGstar

Alg 9

new

J.7

(1)+(3)

EC-LSVRG-DIANA Alg 10

new

J.8

Rate (constants ignored)

√ L L+ δLL

σ∗2

O µ + δµ + nµε +

L(σ∗2 +ζ∗2/δ) √
µ δε

O κ + σ∗2 + δ nµε

L(σ∗2 +ζ∗2/δ) √
δµ ε

Opt. I: O Opt. II: O

O κδ log 1ε

√

ω + κ + σ2 + L√σ2

δ

nµε

δµ ε

√

1+ω + κ + σ2 + √Lσ2

δ

δ

nµε

µ δε

Opt. I: O

√

L

LL

σ∗2

ω + µ + δµ + nµε +

Lσ∗2 √ δµ ε

Opt. II: O

√

1+ω L

LL

σ∗2

δ + µ + δµ + nµε +

Lσ∗2 √ µ δε

O ω + κδ O m + κδ +

log 1ε Lζ∗2 √
δµ ε

O m + κδ log 1ε O ω + m + κδ log 1ε

of all these methods; formal descriptions can be found in the appendix). As we shall see, the existing EC-SGD method uses a rather naive gradient estimator, which renders it less eﬃcient in theory and practice when compared to the best of our new methods. A key property of our parametric assumption described above is that it allows for the construction and modeling of more elaborate gradient estimators, which leads to new EC methods with superior theoretical and practical properties when compared to prior state of the art.
First linearly converging EC methods. The key theoretical consequence of our general framework is the development of the ﬁrst linearly converging error-compensated SGD-type methods for distributed training with biased communication compression. In particular, we design four such methods: two simple but impractical methods, EC-GDstar and EC-LSVRGstar, with rates O κδ ln 1ε and O m + κδ ln 1ε , respectively, and two practical but more elaborate methods, EC-GD-DIANA, with rate O ω + κδ ln 1ε , and EC-LSVRG-DIANA, with rate O ω + m + κδ ln 1ε . In these rates, κ = L/µ is the condition number, 0 < δ ≤ 1 is the contraction parameter associated with the compressor C used in (7), and ω is the variance parameter associated with a secondary unbiased compressor2 Q which plays a key role in the construction of the gradient estimator gik. The complexity of the ﬁrst and third methods does not depend on m as they require the computation of the full gradient ∇fi(xk) for each i. The remaining two methods only need to compute O(1) stochastic gradients ∇fij(xk) on each worker i.
The ﬁrst two methods, while impractical, provided us with the intuition which enabled us to develop the practical variant. We include them in this paper due to their simplicity, because
2We assume that EQ(x) = x and E Q(x) − x 2 ≤ ω x 2 for all x ∈ Rd.
4

Table 2: Error compensated methods developed in this paper. In all cases, vik = C(eki + γgik). The full descriptions of the algorithms are included in the appendix.

Problem
(1) + (3)
(1) + (2) (1)

Method
EC-SGDsr
EC-SGD EC-GDstar

(1) + (2) EC-SGD-DIANA

gik
m
m1 ξij ∇fij (xk)
j=1
∇fξi (xk) ∇fi(xk) − ∇fi(x∗)
gˆik − hki + hk

(1) + (3) EC-SGDsr-DIANA

∇fξk (xk) − hki + hk i

(1) + (3)

EC-LSVRG

(1) + (3) EC-LSVRGstar

(1) + (3) EC-LSVRG-DIANA

∇fil(xk) − ∇fil(wik) + ∇fi(wik)
∇fil(xk) − ∇fil(wik) + ∇fi(wik) − ∇fi(x∗)
gˆik − hki + hk where
gˆik = ∇fil(xk) − ∇fil(wik) + ∇fi(wik)

Comment
E [ξij ] = 1 EDi ∇fξi (x) − ∇fξi (x∗) 2
≤ 2LDfi (x, x∗)
known ∇fi(x∗) ∀i E gˆik = ∇fi(xk) Ek gˆik − ∇fi(xk) 2 ≤ D1,i hki +1 = hki + αQ(gˆik − hki )
n
hk = n1 hki
i=1
E ∇fξk (xk) = ∇fi(xk) i
EDi ∇fξi (x) − ∇fξi (x∗) 2 ≤ 2LDfi (x, x∗)
hki +1 = hki + αQ(∇fξk (xk) − hki ) ni hk = n1 hki i=1 l chosen uniformly from [m] k+1 xk, with prob. p,
wi = wik, with prob. 1 − p l chosen uniformly from [m] k+1 xk, with prob. p,
wi = wik, with prob. 1 − p hki +1 = hki + αQ(gˆik − hki )
n
hk = n1 hki
i=1
l chosen uniformly from [m] k+1 xk, with prob. p, wi = wik, with prob. 1 − p

of the added insights they oﬀer, and to showcase the ﬂexibility of our general theoretical framework, which is able to describe them. EC-GDstar and EC-LSVRGstar are impractical since they require the knowledge of the gradients {∇fi(x∗)}, where x∗ is an optimal solution of (1), which are obviously not known since x∗ is not known.
The only known linear convergence result for an error compensated SGD method is due to Beznosikov et al. [7], who require the computation of the full gradient of fi by each machine i (i.e., m stochastic gradients), and the additional assumption that ∇fi(x∗) = 0 for all i. We do not need such assumptions, thereby resolving a major theoretical issue with EC methods.
Results in the convex case. Our theoretical analysis goes beyond distributed optimization and recovers the results from Gorbunov et al. [11], Khaled et al. [25] (without regularization) in the special case when vik ≡ γgik. As we have seen, in this case eki ≡ 0 for all i and k, and the error-feedback framework (4)–(5) reduces to distributed SGD (6). In this regime, the relation (19) in Assumption 3.4 becomes void, while relations (15) and (16) with σ22,k ≡ 0 are precisely those used by Gorbunov et al. [11] to analyze a wide array of SGD methods, including vanilla SGD [41], SGD with arbitrary sampling [13], as well as variance reduced methods such as SAGA [9], SVRG [20], LSVRG [17, 31], JacSketch [12], SEGA [16] and DIANA [37, 19]. Our theorem recovers the rates of all the methods just listed in both the convex case µ = 0 Khaled et al. [25] and the strongly-convex case µ > 0 Gorbunov et al. [11] under the more general Assumption 3.4.
5

DIANA with bi-directional quantization. To illustrate how our framework can be used even in the case when vik ≡ γgik, eki ≡ 0, we develop analyze a new version of DIANA called DIANAsr-DQ that uses arbitrary sampling on every node and double quantization3, i.e., unbiased compression not only on the workers’ side but also on the master’s one.
Methods with delayed updates. Following Stich [44], we also show that our approach covers SGD with delayed updates [1, 3, 10] (D-SGD), and our analysis shows the best-known rate for this method. Due to the ﬂexibility of our framework, we are able develop several new variants of D-SGD with and without quantization, variance reduction, and arbitrary sampling. Again, due to space limitations, we put these methods together with their convergence analyses in the appendix.

3 Main Result

In this section we present the main theoretical result of our paper. First, we introduce our assumption on f , which is a relaxation of µ-strong convexity.
Assumption 3.1 (µ-strong quasi-convexity). Assume that function f has a unique minimizer x∗. We say that function f is strongly quasi-convex with parameter µ ≥ 0 if for all x ∈ Rd
f (x∗) ≥ f (x) + ∇f (x), x∗ − x + µ2 x − x∗ 2. (9)

We allow µ to be zero, in which case f is sometimes called weakly quasi-convex in the literature (see [44] and references therein). Second, we introduce the classical L-smoothness assumption.

Assumption 3.2. L-smoothness We say that function f is L-smooth if it is diﬀerentiable and its gradient is L-Lipschitz continuous, i.e., for all x, y ∈ Rd

∇f (x) − ∇f (y) ≤ L x − y .

(10)

It is a well-known fact [38] that L-smoothness of convex function f implies that

∇f (x) − ∇f (y) 2 ≤ 2L(f (x) − f (y) − ∇f (y), x − y ) d=ef 2LDf (x, y).

(11)

We now introduce our key parametric assumption on the stochastic gradient gk. This is a generalization of the assumption introduced by Gorbunov et al. [11] for the particular class of methods described covered by the EF framework (4)–(5).

Assumption 3.3. For all k ≥ 0, the stochastic gradient gk is an average of stochastic gradients gik such that

n
gk = n1 gik,
i=1

E gk | xk = ∇f (xk).

(12)

Moreover, there exist constants A, A, A , B1, B2, B1, B2, B1, B2, C1, C2, G, D1, D1, D1, D2, D3 ≥ 0, and ρ1, ρ2 ∈ [0, 1] and two sequences of (probably random) variables {σ1,k}k≥0 and
{σ2,k}k≥0, such that the following recursions hold:

n
n1 E
i=1

n 1

g¯k 2

n

i

i=1

gk − g¯k 2 | xk

i

i

≤ 2A(f (xk) − f (x∗)) + B1σ12,k + B2σ22,k + D1, ≤ 2A(f (xk) − f (x∗)) + B1σ12,k + B2σ22,k + D1,

(13) (14)

E gk 2 | xk ≤ 2A (f (xk) − f (x∗)) + B1σ12,k + B2σ22,k + D1,

(15)

E σ12,k+1 | σ12,k, σ22,k ≤ (1 − ρ1)σ12,k + 2C1 f (xk) − f (x∗) + Gρ1σ22,k + D2,(16)

E σ22,k+1 | σ22,k ≤ (1 − ρ2)σ22,k + 2C2 f (xk) − f (x∗) ,

(17)

where g¯ik = E gik | xk .

3In the concurrent work (which appeared on arXiv after we have submitted our paper to NeurIPS) a similar method was independently proposed under the name of Artemis [40]. However, our analysis is more general, see all the details on this method in the appendix. This footnote was added to the paper during the preparation of the camera-ready version of our paper.

6

Let us brieﬂy explain the intuition behind the assumption and the meaning of the introduced parameters. First of all, we assume that the stochastic gradient at iteration k is conditionally unbiased estimator of ∇f (xk), which is a natural and commonly used assumption on the stochastic gradient in the literature. However, we explicitly do not require unbiasedness of gik, which is very useful in some special cases. Secondly, let us consider the simplest special case when gk ≡ ∇f (xk) and f1 = . . . = fn = f , i.e., there is no stochasticity/randomness in the method and the workers have the same functions. Then due to ∇f (x∗) = 0, we have that
(11)
∇f (xk) 2 ≤ 2L(f (xk) − f (x∗)),

which implies that Assumption 3.3 holds in this case with A = A = L, A = 0 and B1 = B2 = B1 = B2 = B1 = B2 = C1 = C2 = D1 = D1 = D1 = D2 = 0, ρ = 1, σ12,k ≡ σ22,k ≡ 0.

In general, if gk satisﬁes Assumption 3.4, then parameters A, A and A are usually connected
with the smoothness properties of f and typically they are just multiples of L, whereas
terms B1σ12,k, B2σ22,k, B1σ12,k, B2σ22,k, B1σ12,k, B2σ22,k and D1, D1, D1 appear due to the stochastic nature of gik. Moreover, {σ12,k}k≥0 and {σ22,k}k≥0 are sequences connected with variance reduction processes and for the methods; without any kind of variance reduction
these sequences contains only zeros. Parameters B1 and B2 are often 0 or small positive constants, e.g., B1 = B2 = 2, and D1 characterizes the remaining variance in the estimator gk that is not included in the ﬁrst two terms.

Inequalities (16) and (17) describe the variance reduction processes: one can interpret ρ1 and ρ2 as the rates of the variance reduction processes, 2C1(f (xk)−f (x∗)) and 2C2(f (xk)−f (x∗))
are “optimization” terms and, similarly to D1, D2 represents the remaining variance that is not included in the ﬁrst two terms. Typically, σ12,k controls the variance coming from compression and σ22,k controls the variance taking its origin in ﬁnite-sum type randomization
(i.e., subsampling) by each worker. In the case ρ1 = 1 we assume that B1 = B1 = C1 = G = 0, D2 = 0 (for ρ2 = 1 analogously), since inequality (16) becomes superﬂuous.

However, in our main result we need a slightly diﬀerent assumption.

Assumption 3.4. For all k ≥ 0, the stochastic gradient gk is an unbiased estimator of ∇f (xk):

E gk | xk = ∇f (xk).

(18)

Moreover, there exist non-negative constants A , B1, B2, C1, C2, F1, F2, G, D1, D2, D3 ≥ 0, ρ1, ρ2 ∈ [0, 1] and two sequences of (probably random) variables {σ1,k}k≥0 and {σ2,k}k≥0
such that inequalities (15), (16) and (17) hold and

K

K

3L

wkE ek 2

≤

1 4

wkE f (xk) − f (x∗) + F1σ12,0 + F2σ22,0 + γD3WK

k=0

k=0

for all k, K ≥ 0, where ek = n1

n i=1

eki

and

{WK }K≥0

and

{wk }k≥0

are

deﬁned

as

(19)

K
WK = wk,
k=0

wk = (1 − η)−(k+1),

η = min γ2µ , ρ41 , ρ42 .

(20)

This assumption is more ﬂexible than Assumption 3.3 and helps us to obtain a uniﬁed analysis of all methods falling in the error-feedback framework. We emphasize that in this assumption we do not assume that (13) and (14) hold explicitly. Instead of this, we introduce inequality (19), which is the key tool that helps us to analyze the eﬀect of error-feedback and comes from the analysis from [45] with needed adaptations connected with the ﬁrst three inequalities. As we show in the appendix, this inequality can be derived for SGD with error compensation and delayed updates under Assumption 3.3 and, in particular, using (13) and (14). As before, D3 hides a variance that is not handled by variance reduction processes and F1 and F2 are some constants that typically depend on L, B1, B2, ρ1, ρ2 and γ.
We now proceed to stating our main theorem.

7

Theorem 3.1. Let Assumptions 3.1, 3.2 and 3.4 be satisﬁed and γ ≤ 1/4(A . +C1M1+C2M2) Then for all K ≥ 0 we have

E f (x¯K ) − f (x∗) ≤ (1 − η)K 4(T 0+γF1σ12γ,0+γF2σ22,0) + 4γ (D1 + M1D2 + D3)

(21)

when µ > 0 and

E f (x¯K ) − f (x∗) ≤ 4(T 0+γF1σγ12K,0+γF2σ22,0) + 4γ (D1 + M1D2 + D3)

(22)

when

µ

=

0,

where

η

=

min {γµ/2, ρ1/4, ρ2/4},

Tk

def
=

M1 = 43Bρ11 , M2 = 4(B23+ρ243 G) .

x˜k − x∗ 2 + M1γ2σ12,k + M2γ2σ22,k and

All the complexity results summarized in Table 1 follow from this theorem; the detailed proofs are included in the appendix. Furthermore, in the appendix we include similar results but for methods employing delayed updates. The methods, and all associated theory is included there, too.

4 Numerical Experiments

To justify our theory, we conduct several numerical experimentson logistic regression problem with 2-regularization:

N

min f (x) = N1

log

(1

+

exp

(−yi

·

(Ax)i))

+

µ 2

x

2

,

(23)

x∈Rd

i=1

where N is a number of features, x ∈ Rd represents the weights of the model, A ∈ RN×d is a feature matrix, vector y ∈ {−1, 1}N is a vector of labels and (Ax)i denotes the i-th component of vector Ax. Clearly, this problem is L-smooth and µ-strongly convex with
L = µ + λmax(A A)/4N, where λmax(A A) is a largest eigenvalue of A A. The datasets were taken from LIBSVM library [8], and the code was written in Python 3.7 using standard
libraries. Our code is available at https://github.com/eduardgorbunov/ef_sigma_k.

We simulate parameter-server architecture using one machine with Intel(R) Core(TM) i7-9750 CPU 2.60 GHz in the following way. First of all, we always use such N that N = n · m and consider n = 20 and n = 100 workers. The choice of N for each dataset that we consider is stated in Table 3. Next, we shuﬄe the data and split in n groups of size m. To emulate

Table 3: Summary of datasets: N = total # of data samples; d = # of features.

a9a

w8a gisette mushrooms madelon phishing

N 32, 000 49, 700 6, 000

8, 000

2, 000

11, 000

d 123

300

5, 000

112

500

68

the work of workers, we use a single machine and run the methods with the parallel loop in series. Since in these experiments we study sample complexity and number of bits used for communication, this setup is identical to the real parameter-server setup in this sense.
In all experiments we use the stepsize γ = 1/L and 2-regularization parameter µ = 10−4λmax(A A)/4N. The starting point x0 for each dataset was chosen so that f (x0) − f (x∗) ∼ 10. In experiments with stochastic methods we used batches of size 1 and uniform sampling for simplicity. For LSVRG-type methods we choose p = 1/m.
Compressing stochastic gradients. The results for a9a, madelon and phishing can be found in Figure 1 (included here) and for w8a, mushrooms and gisette in Figure 3 (in the Appendix). We choose number of components for TopK operator of the order max{1, d/100}. Clearly, in these experiments we see two levels of noise. For some datasets, like a9a, phishing or mushrooms, the noise that comes from the stochasticity of the gradients dominates the noise coming from compression. Therefore, methods such as EC-SGD and EC-SGD-DIANA start to oscillate around a larger value of the loss function than other methods we consider. EC-LSVRG reduces the largest source of noise and, as a result, ﬁnds a better approximation of

8

the solution. However, at some point, it reaches another level of the loss function and starts to oscillate there due to the noise coming from compression. Finally, EC-LSVRG-DIANA reduces the variance of both types, and as a result, ﬁnds an even better approximation of the solution. In contrast, for the madelon dataset, both noises are of the same order, and therefore, EC-LSVRG and EC-SGD-DIANA behave similarly to EC-SGD. However, EC-LSVRG-DIANA again reduces both types of noise eﬀectively and ﬁnds a better approximation of the solution after a given number of epochs. In the experiments with w8a and gisette datasets, the noise produced by compression is dominated by the noise coming from the stochastic gradients. As a result, we see that the DIANA-trick is not needed here.

f(xk) f(x * ) f(x0) f(x * )

10 1 10 3

a9a, 20 workers EC-SGD top-1 EC-SGD-DIANA top-1 rand-1 EC-SGD-DIANA top-1 l2-quant EC-L-SVRG top-1 EC-L-SVRG-DIANA top-1 rand-1 EC-L-SVRG-DIANA top-1 l2-quant

10 5

10 7

0.00 0.25N0u.5m0b0e.r7o5f 1b.i0ts0p1e.2r 5w1o.r5k0er1.75 21.0e70

a9a, 20 workers

EC-SGD top-1

10 1

EC-SGD-DIANA top-1 rand-1 EC-SGD-DIANA top-1 l2-quant

EC-L-SVRG top-1

10 3

EC-L-SVRG-DIANA top-1 rand-1 EC-L-SVRG-DIANA top-1 l2-quant

10 5

10 7 0 Numb2e0r of pas4s0es throu6g0h the d8a0ta

f(xk) f(x * ) f(x0) f(x * )

f(xk) f(x * ) f(x0) f(x * )

100

madelon, 20 workers EC-SGD top-5

10 1

EC-SGD-DIANA top-5 rand-5 EC-SGD-DIANA top-5 l2-quant

10 2

EC-L-SVRG top-5 EC-L-SVRG-DIANA top-5 rand-5

10 3

EC-L-SVRG-DIANA top-5 l2-quant

10 4

10 5

10 6

0
100 10 1 10 2 10 3 10 4 10 5 10 6

500N0u0m0 b1e00r0o0f0b0i1ts50p0e0r00w2o0r0k0e0r00 2500000 madelon, 20 workers
EC-SGD top-5 EC-SGD-DIANA top-5 rand-5 EC-SGD-DIANA top-5 l2-quant EC-L-SVRG top-5 EC-L-SVRG-DIANA top-5 rand-5 EC-L-SVRG-DIANA top-5 l2-quant

0 Num20ber o4f 0passe6s0throu8g0h th1e0d0ata120

f(xk) f(x * ) f(x0) f(x * )

f(xk) f(x * ) f(x0) f(x * )

100

phishing, 20 workers
EC-SGD top-1

10 1

EC-SGD-DIANA top-1 rand-1 EC-SGD-DIANA top-1 l2-quant

10 2

EC-L-SVRG top-1 EC-L-SVRG-DIANA top-1 rand-1

10 3

EC-L-SVRG-DIANA top-1 l2-quant

10 4

10 5

10 6

10 7

0
100 10 1 10 2 10 3 10 4 10 5 10 6 10 7

1000N00u0m2b0e00r0o00f b3i0t0s00p0e0r 4w00o0r0k00er5000000 phishing, 20 workers
EC-SGD top-1 EC-SGD-DIANA top-1 rand-1 EC-SGD-DIANA top-1 l2-quant EC-L-SVRG top-1 EC-L-SVRG-DIANA top-1 rand-1 EC-L-SVRG-DIANA top-1 l2-quant

0 Numb5e0r of pas1s0e0s throu1g5h0the da2t0a0

f(xk) f(x * ) f(x0) f(x * )

Figure 1: Trajectories of EC-SGD, EC-SGD-DIANA, EC-LSVRG and EC-LSVRG-DIANA applied to solve logistic regression problem with 20 workers.

Compressing full gradients. In order to show the eﬀect of DIANA-type variance reduction itself, we consider the case when all workers compute the full gradients of their functions, see Figure 2 (included here) and Figures 4–7 (in the Appendix). Clearly, for all datasets except mushrooms, EC-GD with constant stepsize converges to a neighborhood of the solution only, while EC-GDstar and EC-GD-DIANA converge with linear rate asymptotically to the exact solution. EC-GDstar always show the best performance, however, it is impractical: we used a very good approximation of the solution to apply this method. In contrast, EC-DIANA converges slightly slower and requires more bits for communication; but it is practical and shows better performance than EC-GD. On the mushrooms datasets, EC-GD does not reach the oscillation region after the given number of epochs, therefore, it is preferable there.

f(xk) f(x * ) f(x0) f(x * )

a9a, 20 workers

EC-GD top-1

10 1

EC-GD top-2 EC-GD-star top-1

EC-GD-DIANA top-1 rand-1

10 3

EC-GD-DIANA top-1 l2-quant

10 5

10 7

0 100000N0u20m00b00e0r30o00f0b00it4s000p0e00r5w00o00r0k0e60r00000 7000000

a9a, 20 workers

EC-GD top-1

10 1

EC-GD top-2 EC-GD-star top-1

EC-GD-DIANA top-1 rand-1

10 3

EC-GD-DIANA top-1 l2-quant

10 5

10 7 0 Nu1m0b0e0r0of2p0a0s0s0es 3th0r0o0u0gh4th0e00d0ata50000

f(xk) f(x * ) f(x0) f(x * )

f(xk) f(x * ) f(x0) f(x * )

100

madelon, 20 workers
EC-GD top-5

EC-GD top-10

10 2

EC-GD-star top-5 EC-GD-DIANA top-5 rand-5

10 4

EC-GD-DIANA top-5 l2-quant

10 6

10 8

10 10 0.0
100 10 2 10 4

0N.5umbe1r .o0f bits1p.e5r wor2ke.0r 21.5e7 madelon, 20 workers
EC-GD top-5 EC-GD top-10 EC-GD-star top-5 EC-GD-DIANA top-5 rand-5 EC-GD-DIANA top-5 l2-quant

10 6

10 8

10 10

0 N5u00m0 b1e0r00o0f1p50a0s0s2e0s00t0hr2o50u0g0h30t0h0e0 3d5a00t0a 40000

f(xk) f(x * ) f(x0) f(x * )

f(xk) f(x * ) f(x0) f(x * )

100 10 2 10 4 10 6 10 8 10 10 10 12
100 10 2 10 4 10 6 10 8 10 10 10 12

phishing, 20 workers
EC-GD top-1 EC-GD top-2 EC-GD-star top-1 EC-GD-DIANA top-1 rand-1 EC-GD-DIANA top-1 l2-quant
0.0 0.2Num0.b4er 0of.6bits0p.8er w1o.0rker1.2 11.e47 phishing, 20 workers
EC-GD top-1 EC-GD top-2 EC-GD-star top-1 EC-GD-DIANA top-1 rand-1 EC-GD-DIANA top-1 l2-quant
0 Nu2m0b0e0r0of4p0a0s0s0es 6th0r0o0u0gh8th0e00d0at1a00000

f(xk) f(x * ) f(x0) f(x * )

Figure 2: Trajectories of EC-GD, EC-GD-star and EC-DIANA applied to solve logistic regression problem with 20 workers.

9

Broader Impact
Our contribution is primarily theoretical. Therefore, a broader impact discussion is not applicable.
Acknowledgments and Disclosure of Funding
The work of Peter Richtárik, Eduard Gorbunov and Dmitry Kovalev was supported by KAUST Baseline Research Fund. Part of this work was done while E. Gorbunov was a research intern at KAUST. The research of E. Gorbunov was also partially supported by the Ministry of Science and Higher Education of the Russian Federation (Goszadaniye) 075-00337-20-03 and RFBR, project number 19-31-51001.
References
[1] A. Agarwal and J. C. Duchi. Distributed delayed stochastic optimization. In Advances in Neural Information Processing Systems, pages 873–881, 2011.
[2] D. Alistarh, D. Grubic, J. Li, R. Tomioka, and M. Vojnovic. QSGD: Communicationeﬃcient SGD via gradient quantization and encoding. In Advances in Neural Information Processing Systems, pages 1709–1720, 2017.
[3] Y. Arjevani, O. Shamir, and N. Srebro. A tight convergence analysis for stochastic gradient descent with delayed updates. arXiv preprint arXiv:1806.10188, 2018.
[4] D. Basu, D. Data, C. Karakus, and S. Diggavi. Qsparse-local-SGD: Distributed SGD with quantization, sparsiﬁcation and local computations. In Advances in Neural Information Processing Systems, pages 14668–14679, 2019.
[5] J. Bernstein, J. Zhao, K. Azizzadenesheli, and A. Anandkumar. SignSGD with majority vote is communication eﬃcient and fault tolerant. In ICLR, 2019.
[6] D. P. Bertsekas and J. N. Tsitsiklis. Parallel and distributed computation: numerical methods, volume 23. Prentice hall Englewood Cliﬀs, NJ, 1989.
[7] A. Beznosikov, S. Horváth, P. Richtárik, and M. Safaryan. On biased compression for distributed learning. arXiv preprint arXiv:2002.12410, 2020.
[8] C.-C. Chang and C.-J. Lin. LIBSVM: A library for support vector machines. ACM transactions on intelligent systems and technology (TIST), 2(3):1–27, 2011.
[9] A. Defazio, F. Bach, and S. Lacoste-Julien. SAGA: A fast incremental gradient method with support for non-strongly convex composite objectives. In Advances in Neural Information Processing Systems, pages 1646–1654, 2014.
[10] H. R. Feyzmahdavian, A. Aytekin, and M. Johansson. An asynchronous mini-batch algorithm for regularized stochastic optimization. IEEE Transactions on Automatic Control, 61(12):3740–3754, 2016.
[11] E. Gorbunov, F. Hanzely, and P. Richtárik. A uniﬁed theory of SGD: Variance reduction, sampling, quantization and coordinate descent. In The 23rd International Conference on Artiﬁcial Intelligence and Statistics (AISTATS 2020), 2020.
[12] R. M. Gower, P. Richtárik, and F. Bach. Stochastic quasi-gradient methods: Variance reduction via Jacobian sketching. arXiv preprint arXiv:1805.02632, 2018.
[13] R. M. Gower, N. Loizou, X. Qian, A. Sailanbayev, E. Shulgin, and P. Richtárik. SGD: General analysis and improved rates. In International Conference on Machine Learning, pages 5200–5209, 2019.
[14] P. Goyal, P. Dollár, R. Girshick, P. Noordhuis, L. Wesolowski, A. Kyrola, A. Tulloch, Y. Jia, and K. He. Accurate, large minibatch SGD: training imagenet in 1 hour. arXiv preprint arXiv:1706.02677, 2017.
[15] F. Haddadpour and M. Mahdavi. On the convergence of local descent methods in federated learning. arXiv preprint arXiv:1910.14425, 2019.
10

[16] F. Hanzely, K. Mishchenko, and P. Richtárik. SEGA: Variance reduction via gradient sketching. In Advances in Neural Information Processing Systems, pages 2082–2093, 2018.
[17] T. Hofmann, A. Lucchi, S. Lacoste-Julien, and B. McWilliams. Variance reduced stochastic gradient descent with neighbors. In Advances in Neural Information Processing Systems, pages 2305–2313, 2015.
[18] S. Horváth, C.-Y. Ho, Ľudovít Horváth, A. N. Sahu, M. Canini, and P. Richtárik. Natural compression for distributed deep learning. arXiv preprint arXiv:1905.10988, 2019.
[19] S. Horváth, D. Kovalev, K. Mishchenko, S. Stich, and P. Richtárik. Stochastic distributed learning with gradient quantization and variance reduction. arXiv preprint arXiv:1904.05115, 2019.
[20] R. Johnson and T. Zhang. Accelerating stochastic gradient descent using predictive variance reduction. In Advances in Neural Information Processing Systems 26, pages 315–323, 2013.
[21] P. Kairouz, H. B. McMahan, B. Avent, A. Bellet, M. Bennis, A. N. Bhagoji, K. Bonawitz, Z. Charles, G. Cormode, R. Cummings, et al. Advances and open problems in federated learning. arXiv preprint arXiv:1912.04977, 2019.
[22] S. P. Karimireddy, S. Kale, M. Mohri, S. J. Reddi, S. U. Stich, and A. T. Suresh. Scaﬀold: Stochastic controlled averaging for federated learning. arXiv preprint arXiv:1910.06378, 2019.
[23] S. P. Karimireddy, Q. Rebjock, S. Stich, and M. Jaggi. Error feedback ﬁxes signSGD and other gradient compression schemes. In International Conference on Machine Learning, pages 3252–3261, 2019.
[24] A. Khaled, K. Mishchenko, and P. Richtárik. Tighter theory for local SGD on identical and heterogeneous data. In The 23rd International Conference on Artiﬁcial Intelligence and Statistics (AISTATS 2020), 2020.
[25] A. Khaled, O. Sebbouh, N. Loizou, R. M. Gower, and P. Richtárik. Uniﬁed analysis of stochastic gradient methods for composite convex and smooth optimization. arXiv preprint arXiv:2006.11573, 2020.
[26] S. Khirirat, H. R. Feyzmahdavian, and M. Johansson. Distributed learning with compressed gradients. arXiv preprint arXiv:1806.06573, 2018.
[27] A. Koloskova, S. Stich, and M. Jaggi. Decentralized stochastic optimization and gossip algorithms with compressed communication. In International Conference on Machine Learning, pages 3478–3487, 2019.
[28] A. Koloskova, T. Lin, S. U. Stich, and M. Jaggi. Decentralized deep learning with arbitrary communication compression. ICLR, page arXiv:1907.09356, 2020. URL https://arxiv.org/abs/1907.09356.
[29] A. Koloskova, N. Loizou, S. Boreiri, M. Jaggi, and S. U. Stich. A uniﬁed theory of decentralized SGD with changing topology and local updates. arXiv preprint arXiv:2003.10422, 2020.
[30] J. Konečný, H. B. McMahan, F. X. Yu, P. Richtárik, A. T. Suresh, and D. Bacon. Federated learning: Strategies for improving communication eﬃciency. arXiv preprint arXiv:1610.05492, 2016.
[31] D. Kovalev, S. Horváth, and P. Richtárik. Don’t jump through hoops and remove those loops: SVRG and Katyusha are better without the outer loop. In Proceedings of the 31st International Conference on Algorithmic Learning Theory, 2020.
[32] R. Leblond, F. Pedregosa, and S. Lacoste-Julien. Improved asynchronous parallel optimization analysis for stochastic incremental methods. The Journal of Machine Learning Research, 19(1):3140–3207, 2018.
[33] X. Lian, Y. Huang, Y. Li, and J. Liu. Asynchronous parallel stochastic gradient for nonconvex optimization. In Advances in Neural Information Processing Systems, pages 2737–2745, 2015.
11

[34] X. Lian, C. Zhang, H. Zhang, C.-J. Hsieh, W. Zhang, and J. Liu. Can decentralized algorithms outperform centralized algorithms? a case study for decentralized parallel stochastic gradient descent. In Advances in Neural Information Processing Systems, pages 5330–5340, 2017.
[35] X. Liu, Y. Li, J. Tang, and M. Yan. A double residual compression algorithm for eﬃcient distributed learning. arXiv preprint arXiv:1910.07561, 2019.
[36] H. Mania, X. Pan, D. Papailiopoulos, B. Recht, K. Ramchandran, and M. I. Jordan. Perturbed iterate analysis for asynchronous stochastic optimization. SIAM Journal on Optimization, 27(4):2202–2229, 2017.
[37] K. Mishchenko, E. Gorbunov, M. Takáč, and P. Richtárik. Distributed learning with compressed gradient diﬀerences. arXiv preprint arXiv:1901.09269, 2019.
[38] Y. Nesterov. Lectures on convex optimization, volume 137. Springer, 2018. [39] L. Nguyen, P. H. Nguyen, M. Dijk, P. Richtárik, K. Scheinberg, and M. Takáč. SGD
and Hogwild! convergence without the bounded gradients assumption. In International Conference on Machine Learning, pages 3750–3758, 2018. [40] C. Philippenko and A. Dieuleveut. Artemis: tight convergence guarantees for bidirectional compression in federated learning. arXiv preprint arXiv:2006.14591, 2020. [41] H. Robbins and S. Monro. A stochastic approximation method. In Herbert Robbins Selected Papers, pages 102–109. Springer, 1985. [42] O. Shamir, N. Srebro, and T. Zhang. Communication-eﬃcient distributed optimization using an approximate Newton-type method. In International conference on machine learning, pages 1000–1008, 2014. [43] S. U. Stich. Local SGD converges fast and communicates little. International Conference on Learning Representations (ICLR), page arXiv:1805.09767, 2019. URL https:// arxiv.org/abs/1805.09767. [44] S. U. Stich. Uniﬁed optimal analysis of the (stochastic) gradient method. arXiv preprint arXiv:1907.04232, 2019. [45] S. U. Stich and S. P. Karimireddy. The error-feedback framework: Better rates for SGD with delayed gradients and compressed communication. arXiv preprint arXiv:1909.05350, 2019. [46] S. U. Stich, J.-B. Cordonnier, and M. Jaggi. Sparsiﬁed SGD with memory. In Advances in Neural Information Processing Systems, pages 4447–4458, 2018. [47] H. Tang, C. Yu, X. Lian, T. Zhang, and J. Liu. DoubleSqueeze: Parallel stochastic gradient descent with double-pass error-compensated compression. In International Conference on Machine Learning, pages 6155–6165, 2019. [48] W. Wen, C. Xu, F. Yan, C. Wu, Y. Wang, Y. Chen, and H. Li. Terngrad: Ternary gradients to reduce communication in distributed deep learning. In Advances in Neural Information Processing Systems, pages 1509–1519, 2017. [49] B. Woodworth, K. K. Patel, S. U. Stich, Z. Dai, B. Bullins, H. B. McMahan, O. Shamir, and N. Srebro. Is local SGD better than minibatch SGD? arXiv preprint arXiv:2002.07839, 2020.
12

Appendix: Linearly Converging Error Compensated SGD

Contents

1 Introduction

1

2 Summary of Contributions

3

3 Main Result

6

4 Numerical Experiments

8

A Missing Plots

15

A.1 Compressing stochastic gradients . . . . . . . . . . . . . . . . . . . . . . . . . 15

A.2 Compressing full gradients . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16

B Compression Operators: Extra Commentary

20

B.1 Unbiased compressors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20

B.2 Biased compressors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20

C Further Notation and Deﬁnitions

21

C.1 Quantization operators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21

D Basic Inequalities, Identities and Technical Lemmas

22

D.1 Basic inequalities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22

D.2 Identities and inequalities involving random variables . . . . . . . . . . . . . . 22

D.3 Technical lemmas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22

E Proofs for Section 3

25

E.1 A lemma . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25

E.2 Proof of Theorem 3.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26

F SGD as a Special Case

27

G Distributed SGD with Compression and Error Compensation

28

H SGD with Delayed Updates

35

I Special Cases: SGD

40

I.1 DIANA with Arbitrary Sampling and Double Quantization . . . . . . . . . . . 40

I.2 Recovering Tight Complexity Bounds for VR-DIANA . . . . . . . . . . . . . . . 45

J Special Cases: Error Compensated Methods

48

13

J.1 EC-SGDsr . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48 J.2 EC-SGD . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50 J.3 EC-GDstar . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55 J.4 EC-SGD-DIANA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56 J.5 EC-SGDsr-DIANA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60 J.6 EC-LSVRG . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63 J.7 EC-LSVRGstar . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67 J.8 EC-LSVRG-DIANA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69

K Special Cases: Delayed Updates Methods

74

K.1 D-SGD . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74

K.2 D-QSGD . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77

K.3 D-QSGDstar . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80

K.4 D-SGD-DIANA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83

K.5 D-SGDsr . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 86

K.6 D-LSVRG . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 88

K.7 D-QLSVRG . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 90

K.8 D-QLSVRGstar . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 93

K.9 D-LSVRG-DIANA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 95

14

A Missing Plots

A.1 Compressing stochastic gradients

f(xk) f(x * ) f(x0) f(x * )

f(xk) f(x * ) f(x0) f(x * )

100 10 1 10 2 10 3

w8a, 20 workers EC-SGD top-3 EC-SGD-DIANA top-3 rand-3 EC-SGD-DIANA top-3 l2-quant EC-L-SVRG top-3 EC-L-SVRG-DIANA top-3 rand-3 EC-L-SVRG-DIANA top-3 l2-quant

10 4

10 5

10 6 0.0
100 10 1 10 2 10 3

0.N5umb1e.r0of bi1t.s5per w2.o0rker 2.5 1e37.0 w8a, 20 workers
EC-SGD top-3 EC-SGD-DIANA top-3 rand-3 EC-SGD-DIANA top-3 l2-quant EC-L-SVRG top-3 EC-L-SVRG-DIANA top-3 rand-3 EC-L-SVRG-DIANA top-3 l2-quant

10 4

10 5

10 6

0 Num5ber o1f0passe1s5thro2u0gh th2e5data30

100

w8a, 20 workers
EC-SGD identical

10 1

EC-SGD top-3 EC-SGD-DIANA top-3 rand-3

10 2

EC-SGD-DIANA top-3 l2-quant EC-L-SVRG top-3

EC-L-SVRG-DIANA top-3 rand-3

10 3

EC-L-SVRG-DIANA top-3 l2-quant

10 4

10 5

10 6 0.0
100 10 1 10 2 10 3

0.2Nu0m.4ber0o.f6bits0.p8er w1.o0rke1r.2 11.4e9 w8a, 20 workers
EC-SGD identical EC-SGD top-3 EC-SGD-DIANA top-3 rand-3 EC-SGD-DIANA top-3 l2-quant EC-L-SVRG top-3 EC-L-SVRG-DIANA top-3 rand-3 EC-L-SVRG-DIANA top-3 l2-quant

10 4

10 5

10 6 0 Num5ber o1f0passe1s5thro2u0gh th2e5data30

f(xk) f(x * ) f(x0) f(x * )

f(xk) f(x * ) f(x0) f(x * )

f(xk) f(x * ) f(x0) f(x * )

f(xk) f(x * ) f(x0) f(x * )

mushrooms, 20 workers

10 1

EC-SGD top-1 EC-SGD-DIANA top-1 rand-1

EC-SGD-DIANA top-1 l2-quant

10 3

EC-L-SVRG top-1 EC-L-SVRG-DIANA top-1 rand-1

EC-L-SVRG-DIANA top-1 l2-quant

10 5

10 7

10 9

0.0
10 1 10 3 10 5

0N.u5mber 1o.f0bits pe1r.5worker2.0 1e7 mushrooms, 20 workers
EC-SGD top-1 EC-SGD-DIANA top-1 rand-1 EC-SGD-DIANA top-1 l2-quant EC-L-SVRG top-1 EC-L-SVRG-DIANA top-1 rand-1 EC-L-SVRG-DIANA top-1 l2-quant

10 7

10 9

0 Nu1m0b0er o2f0p0asse3s00thro4u0g0h the50d0ata 600

mushrooms, 20 workers

10 1

EC-SGD identical EC-SGD top-1

EC-SGD-DIANA top-1 rand-1

10 3

EC-SGD-DIANA top-1 l2-quant EC-L-SVRG top-1

EC-L-SVRG-DIANA top-1 rand-1

10 5

EC-L-SVRG-DIANA top-1 l2-quant

10 7

10 9

0.0
10 1 10 3 10 5

0.2Num0b.e4r of b0i.t6s pe0r .w8orke1r.0 11.e29 mushrooms, 20 workers
EC-SGD identical EC-SGD top-1 EC-SGD-DIANA top-1 rand-1 EC-SGD-DIANA top-1 l2-quant EC-L-SVRG top-1 EC-L-SVRG-DIANA top-1 rand-1 EC-L-SVRG-DIANA top-1 l2-quant

10 7

10 9

0 Nu1m0b0er o2f0p0asse3s00thro4u0g0h the50d0ata 600

f(xk) f(x * ) f(x0) f(x * )

f(xk) f(x * ) f(x0) f(x * )

f(xk) f(x * ) f(x0) f(x * )

f(xk) f(x * ) f(x0) f(x * )

100 10 1 10 2

gisette, 20 workers
EC-SGD top-50 EC-SGD-DIANA top-50 rand-50 EC-SGD-DIANA top-1 l2-quant EC-L-SVRG top-50 EC-L-SVRG-DIANA top-50 rand-50 EC-L-SVRG-DIANA top-50 l2-quant

10 3

10 4

10

5
0.00

100 10 1 10 2

0.25Nu0m.5b0er0o.f7b5its1.p0e0r w1o.2rk5er1.50 11.e785 gisette, 20 workers
EC-SGD top-50 EC-SGD-DIANA top-50 rand-50 EC-SGD-DIANA top-1 l2-quant EC-L-SVRG top-50 EC-L-SVRG-DIANA top-50 rand-50 EC-L-SVRG-DIANA top-50 l2-quant

10 3

10 4

10 5 0 20 40 60 80

Number of passes through the data

100

gisette, 20 workers
EC-SGD identical

EC-SGD top-50

10 1

EC-SGD-DIANA top-50 rand-50 EC-SGD-DIANA top-1 l2-quant

10 2

EC-L-SVRG top-50 EC-L-SVRG-DIANA top-50 rand-50

EC-L-SVRG-DIANA top-50 l2-quant

10 3

10 4

10 5 0
100 10 1 10 2 10 3

Nu2mber of 4bits per w6orker 8 1e9 gisette, 20 workers
EC-SGD identical EC-SGD top-50 EC-SGD-DIANA top-50 rand-50 EC-SGD-DIANA top-1 l2-quant EC-L-SVRG top-50 EC-L-SVRG-DIANA top-50 rand-50 EC-L-SVRG-DIANA top-50 l2-quant

10 4

10 5 0 20 40 60 80 Number of passes through the data

f(xk) f(x * ) f(x0) f(x * )

f(xk) f(x * ) f(x0) f(x * )

Figure 3: Trajectories of EC-SGD, EC-SGD-DIANA, EC-LSVRG and EC-LSVRG-DIANA applied to solve logistic regression problem with 20 workers. EC-SGD identical corresponds to SGD with error compensation with trivial compression operator C(x) = x, i.e., it is just parallel SGD.

15

A.2 Compressing full gradients

f(xk) f(x * ) f(x0) f(x * )

100

w8a, 20 workers EC-GD top-3

EC-GD top-6

10 2

EC-GD-star top-3 EC-GD-DIANA top-3 rand-3

EC-GD-DIANA top-3 l2-quant

10 4

10 6

10 8

0.00 0.25N0u.m50be0r.7o5f b1it.0s0pe1r.2w5or1k.5er0 1.7512e.700

100

w8a, 20 workers
EC-GD top-3

EC-GD top-6

10 2

EC-GD-star top-3 EC-GD-DIANA top-3 rand-3

EC-GD-DIANA top-3 l2-quant

10 4

10 6

10 8

0 Nu1m0b0e0r0of2p0a0s0s0es 3th0r0o0u0gh4th0e00d0ata50000

f(xk) f(x * ) f(x0) f(x * )

f(xk) f(x * ) f(x0) f(x * )

100

mushrooms, 20 workers
EC-GD top-1

10 1

EC-GD top-2 EC-GD-star top-1

10 2

EC-GD-DIANA top-1 rand-1 EC-GD-DIANA top-1 l2-quant

10 3

10 4

10 5

10 6

10 7

0
100 10 1 10 2 10 3 10 4 10 5 10 6 10 7

Number of bits per worker 1000000 2000000 3000000 4000000 5000000 6000000 7000000 mushrooms, 20 workers
EC-GD top-1 EC-GD top-2 EC-GD-star top-1 EC-GD-DIANA top-1 rand-1 EC-GD-DIANA top-1 l2-quant

0 Nu1m0b0e0r0of2p0a0s0s0es 3th0r0o0u0gh4th0e00d0ata50000

f(xk) f(x * ) f(x0) f(x * )

f(xk) f(x * ) f(x0) f(x * )

100

gisette, 20 workers
EC-GD top-50

EC-GD top-100

10 2

EC-GD-star top-50 EC-DIANA top-50 l2-quant

EC-DIANA top-50 rand-50

10 4

10 6

10 8

0.0
100 10 2 10 4 10 6 10 8

0.5Num1b.0er of1b.5its p2e.r0wor2k.e5r 3.01e8 gisette, 20 workers
EC-GD top-50 EC-GD top-100 EC-GD-star top-50 EC-DIANA top-50 l2-quant EC-DIANA top-50 rand-50

0 Nu1m0b0e0r0of2p0a0s0s0es 3th0r0o0u0gh4th0e00d0ata50000

f(xk) f(x * ) f(x0) f(x * )

Figure 4: Trajectories of EC-GD, EC-GD-star and EC-DIANA applied to solve logistic regression problem with 20 workers.

16

f(xk) f(x * ) f(x0) f(x * )

f(xk) f(x * ) f(x0) f(x * )

a9a, 100 workers

EC-GD top-1

10 1

EC-GD top-2 EC-GD-star top-1

EC-GD-DIANA top-1 rand-1

10 3

EC-GD-DIANA top-1 l2-quant

10 5

10 7

0 100000N0u20m00b00e0r30o00f0b00it4s000p0e00r5w00o00r0k0e60r00000 7000000

a9a, 100 workers

EC-GD top-1

10 1

EC-GD top-2 EC-GD-star top-1

EC-GD-DIANA top-1 rand-1

10 3

EC-GD-DIANA top-1 l2-quant

10 5

10 7

0 Nu1m0b0e0r0of2p0a0s0s0es 3th0r0o0u0gh4th0e00d0ata50000

100

w8a, 100 workers EC-GD top-3

EC-GD top-6

10 2

EC-GD-star top-3 EC-GD-DIANA top-3 rand-3

EC-GD-DIANA top-3 l2-quant

10 4

10 6

10 8

0.00 0.25N0u.m50be0r.7o5f b1it.0s0pe1r.2w5or1k.5er0 1.7512e.700

100

w8a, 100 workers
EC-GD top-3

EC-GD top-6

10 2

EC-GD-star top-3 EC-GD-DIANA top-3 rand-3

EC-GD-DIANA top-3 l2-quant

10 4

10 6

10 8

0 Nu1m0b0e0r0of2p0a0s0s0es 3th0r0o0u0gh4th0e00d0ata50000

f(xk) f(x * ) f(x0) f(x * )

f(xk) f(x * ) f(x0) f(x * )

f(xk) f(x * ) f(x0) f(x * )

f(xk) f(x * ) f(x0) f(x * )

100

madelon, 100 workers
EC-GD top-5

EC-GD top-10

10 2

EC-GD-star top-5 EC-GD-DIANA top-5 rand-5

10 4

EC-GD-DIANA top-5 l2-quant

10 6

10 8

10 10 0.0
100 10 2 10 4

0N.5umbe1r .o0f bits1p.e5r wor2ke.0r 21.5e7 madelon, 100 workers
EC-GD top-5 EC-GD top-10 EC-GD-star top-5 EC-GD-DIANA top-5 rand-5 EC-GD-DIANA top-5 l2-quant

10 6

10 8

10 10

0 N5u00m0 b1e0r00o0f1p50a0s0s2e0s00t0hr2o50u0g0h30t0h0e0 3d5a00t0a 40000

100

mushrooms, 100 workers
EC-GD top-1

10 1

EC-GD top-2 EC-GD-star top-1

10 2

EC-GD-DIANA top-1 rand-1 EC-GD-DIANA top-1 l2-quant

10 3

10 4

10 5

10 6

10 7

0
100 10 1 10 2 10 3 10 4 10 5 10 6 10 7

Number of bits per worker 1000000 2000000 3000000 4000000 5000000 6000000 7000000 mushrooms, 100 workers
EC-GD top-1 EC-GD top-2 EC-GD-star top-1 EC-GD-DIANA top-1 rand-1 EC-GD-DIANA top-1 l2-quant

0 Num10b0e00r of p2a00s0s0es th30r0o0u0gh t4h0e00d0 ata 50000

f(xk) f(x * ) f(x0) f(x * )

f(xk) f(x * ) f(x0) f(x * )

f(xk) f(x * ) f(x0) f(x * )

f(xk) f(x * ) f(x0) f(x * )

100 10 2 10 4 10 6 10 8 10 10 10 12
100 10 2 10 4 10 6 10 8 10 10 10 12
100 10 2 10 4

phishing, 100 workers
EC-GD top-1 EC-GD top-2 EC-GD-star top-1 EC-GD-DIANA top-1 rand-1 EC-GD-DIANA top-1 l2-quant
0.0 0.2Num0.b4er 0of.6bits0p.8er w1o.0rker1.2 11.e47 phishing, 100 workers
EC-GD top-1 EC-GD top-2 EC-GD-star top-1 EC-GD-DIANA top-1 rand-1 EC-GD-DIANA top-1 l2-quant
0 Nu2m0b0e0r0of4p0a0s0s0es 6th0r0o0u0gh8th0e00d0at1a00000 gisette, 100 workers
EC-GD top-50 EC-GD top-100 EC-GD-star top-50 EC-DIANA top-50 l2-quant EC-DIANA top-50 rand-50

10 6

10 8

0.0
100 10 2 10 4 10 6 10 8

0.5Num1b.0er of1b.5its p2e.r0wor2k.e5r 3.01e8 gisette, 100 workers
EC-GD top-50 EC-GD top-100 EC-GD-star top-50 EC-DIANA top-50 l2-quant EC-DIANA top-50 rand-50

0 Nu1m0b0e0r0of2p0a0s0s0es 3th0r0o0u0gh4th0e00d0ata50000

f(xk) f(x * ) f(x0) f(x * )

f(xk) f(x * ) f(x0) f(x * )

Figure 5: Trajectories of EC-GD, EC-GD-star and EC-DIANA applied to solve logistic regression problem with 100 workers.

17

f(xk) f(x * ) f(x0) f(x * )

f(xk) f(x * ) f(x0) f(x * )

a9a, 20 workers

GD

10 1

EC-GD top-1 EC-GD top-2

EC-GD-star top-1

10 3

EC-GD-DIANA top-1 rand-1 EC-GD-DIANA top-1 l2-quant

10 5

10 7 0.0
10 1 10 3

0.5 Nu1m.0be1r.o5f b2it.0s pe2r.5wo3rk.0er 3.5 14e.80 a9a, 20 workers
GD EC-GD top-1 EC-GD top-2 EC-GD-star top-1 EC-GD-DIANA top-1 rand-1 EC-GD-DIANA top-1 l2-quant

10 5

10 7

0 Nu1m0b0e0r0of2p0a0s0s0es 3th0r0o0u0gh4th0e00d0ata50000

100

w8a, 20 workers
GD

EC-GD top-3

10 2

EC-GD top-6 EC-GD-star top-3

EC-GD-DIANA top-3 rand-3

10 4

EC-GD-DIANA top-3 l2-quant

10 6

10 8

0.0
100 10 2 10 4

0N.u2mber0o.f4bits p0e.r6worke0r.8 1e19.0 w8a, 20 workers
GD EC-GD top-3 EC-GD top-6 EC-GD-star top-3 EC-GD-DIANA top-3 rand-3 EC-GD-DIANA top-3 l2-quant

10 6

10 8

0 Nu1m0b0e0r0of2p0a0s0s0es 3th0r0o0u0gh4th0e00d0ata50000

f(xk) f(x * ) f(x0) f(x * )

f(xk) f(x * ) f(x0) f(x * )

f(xk) f(x * ) f(x0) f(x * )

f(xk) f(x * ) f(x0) f(x * )

100

madelon, 20 workers
GD

EC-GD top-5

10 2

EC-GD top-10 EC-GD-star top-5

10 4

EC-GD-DIANA top-5 rand-5 EC-GD-DIANA top-5 l2-quant

10 6

10 8

10 10 0.0
100 10 2 10 4

0.2Num0b.4er of0b.6its p0e.r8wor1ke.0r 1.21e9 madelon, 20 workers
GD EC-GD top-5 EC-GD top-10 EC-GD-star top-5 EC-GD-DIANA top-5 rand-5 EC-GD-DIANA top-5 l2-quant

10 6

10 8

10 10

0 N5u00m0 b1e0r00o0f1p50a0s0s2e0s00t0hr2o50u0g0h30t0h0e0 3d5a00t0a 40000

100

mushrooms, 20 workers
GD

10 1

EC-GD top-1 EC-GD top-2

10 2

EC-GD-star top-1 EC-GD-DIANA top-1 rand-1

10 3

EC-GD-DIANA top-1 l2-quant

10 4

10 5

10 6

10 7

0.0 100 10 1 10 2 10 3 10 4 10 5 10 6 10 7

0.5Nu1m.0ber1o.f5bits2.p0er w2.o5rke3r.0 31.5e8 mushrooms, 20 workers
GD EC-GD top-1 EC-GD top-2 EC-GD-star top-1 EC-GD-DIANA top-1 rand-1 EC-GD-DIANA top-1 l2-quant

0 Num10b0e00r of p2a00s0s0es th30r0o0u0gh t4h0e00d0 ata 50000

f(xk) f(x * ) f(x0) f(x * )

f(xk) f(x * ) f(x0) f(x * )

f(xk) f(x * ) f(x0) f(x * )

f(xk) f(x * ) f(x0) f(x * )

100

phishing, 20 workers
GD

10 2

EC-GD top-1 EC-GD top-2

10 4

EC-GD-star top-1 EC-GD-DIANA top-1 rand-1

10 6

EC-GD-DIANA top-1 l2-quant

10 8

10 10

10 12

0 Nu1mber of2bits per w3orker 4 1e8

100

phishing, 20 workers GD

10 2

EC-GD top-1 EC-GD top-2

10 4

EC-GD-star top-1 EC-GD-DIANA top-1 rand-1

10 6

EC-GD-DIANA top-1 l2-quant

10 8

10 10

10 12

0 Nu2m0b0e0r0of4p0a0s0s0es 6th0r0o0u0gh8th0e00d0at1a00000

100

gisette, 20 workers
GD

EC-GD top-50

10 2

EC-GD top-100 EC-GD-star top-50

EC-DIANA top-50 l2-quant

10 4

EC-DIANA top-50 rand-50

10 6

10 8

0.0
100 10 2 10 4 10 6 10 8

0.2 N0u.m4be0r.6of b0i.t8s p1e.r0wo1r.k2er 1.4 11e1.60 gisette, 20 workers
GD EC-GD top-50 EC-GD top-100 EC-GD-star top-50 EC-DIANA top-50 l2-quant EC-DIANA top-50 rand-50

0 Nu1m0b0e0r0of2p0a0s0s0es 3th0r0o0u0gh4th0e00d0ata50000

f(xk) f(x * ) f(x0) f(x * )

f(xk) f(x * ) f(x0) f(x * )

Figure 6: Trajectories of EC-GD, EC-GD-star, EC-DIANA and GD applied to solve logistic regression problem with 20 workers.

18

f(xk) f(x * ) f(x0) f(x * )

f(xk) f(x * ) f(x0) f(x * )

a9a, 100 workers

GD

10 1

EC-GD top-1 EC-GD top-2

EC-GD-star top-1

10 3

EC-GD-DIANA top-1 rand-1 EC-GD-DIANA top-1 l2-quant

10 5

10 7

0.0
10 1 10 3

0.5 Nu1m.0be1r.o5f b2it.0s pe2r.5wo3rk.0er 3.5 14e.80 a9a, 100 workers
GD EC-GD top-1 EC-GD top-2 EC-GD-star top-1 EC-GD-DIANA top-1 rand-1 EC-GD-DIANA top-1 l2-quant

10 5

10 7

0 Nu1m0b0e0r0of2p0a0s0s0es 3th0r0o0u0gh4th0e00d0ata50000

100

w8a, 100 workers
GD

EC-GD top-3

10 2

EC-GD top-6 EC-GD-star top-3

EC-GD-DIANA top-3 rand-3

10 4

EC-GD-DIANA top-3 l2-quant

10 6

10 8

0.0
100 10 2 10 4 10 6 10 8

0N.u2mber0o.f4bits p0e.r6worke0r.8 1e19.0 w8a, 100 workers
GD EC-GD top-3 EC-GD top-6 EC-GD-star top-3 EC-GD-DIANA top-3 rand-3 EC-GD-DIANA top-3 l2-quant

0 Nu1m0b0e0r0of2p0a0s0s0es 3th0r0o0u0gh4th0e00d0ata50000

f(xk) f(x * ) f(x0) f(x * )

f(xk) f(x * ) f(x0) f(x * )

f(xk) f(x * ) f(x0) f(x * )

f(xk) f(x * ) f(x0) f(x * )

100

madelon, 100 workers
GD

EC-GD top-5

10 2

EC-GD top-10 EC-GD-star top-5

10 4

EC-GD-DIANA top-5 rand-5 EC-GD-DIANA top-5 l2-quant

10 6

10 8

10 10 0.0
100 10 2 10 4

0.2Num0b.4er of0b.6its p0e.r8wor1ke.0r 1.21e9 madelon, 100 workers
GD EC-GD top-5 EC-GD top-10 EC-GD-star top-5 EC-GD-DIANA top-5 rand-5 EC-GD-DIANA top-5 l2-quant

10 6

10 8

10 10

0 N5u00m0 b1e0r00o0f1p50a0s0s2e0s00t0hr2o50u0g0h30t0h0e0 3d5a00t0a 40000

100

mushrooms, 100 workers
GD

10 1

EC-GD top-1 EC-GD top-2

10 2

EC-GD-star top-1 EC-GD-DIANA top-1 rand-1

10 3

EC-GD-DIANA top-1 l2-quant

10 4

10 5

10 6

10 7

0.0 100 10 1 10 2 10 3 10 4 10 5 10 6 10 7

0.5Nu1m.0ber1o.f5bits2.p0er w2.o5rke3r.0 31.5e8 mushrooms, 100 workers
GD EC-GD top-1 EC-GD top-2 EC-GD-star top-1 EC-GD-DIANA top-1 rand-1 EC-GD-DIANA top-1 l2-quant

0 Num10b0e00r of p2a00s0s0es th30r0o0u0gh t4h0e00d0 ata 50000

f(xk) f(x * ) f(x0) f(x * )

f(xk) f(x * ) f(x0) f(x * )

f(xk) f(x * ) f(x0) f(x * )

f(xk) f(x * ) f(x0) f(x * )

100

phishing, 100 workers
GD

10 2

EC-GD top-1 EC-GD top-2

10 4

EC-GD-star top-1 EC-GD-DIANA top-1 rand-1

10 6

EC-GD-DIANA top-1 l2-quant

10 8

10 10

10 12

0 Nu1mber of2bits per w3orker 4 1e8

100

phishing, 100 workers GD

10 2

EC-GD top-1 EC-GD top-2

10 4

EC-GD-star top-1 EC-GD-DIANA top-1 rand-1

10 6

EC-GD-DIANA top-1 l2-quant

10 8

10 10

10 12

0 Nu2m0b0e0r0of4p0a0s0s0es 6th0r0o0u0gh8th0e00d0at1a00000

100

gisette, 100 workers
GD

EC-GD top-50

10 2

EC-GD top-100 EC-GD-star top-50

EC-DIANA top-50 l2-quant

10 4

EC-DIANA top-50 rand-50

10 6

10 8

0.0
100 10 2 10 4 10 6 10 8

0.2 N0u.m4be0r.6of b0i.t8s p1e.r0wo1r.k2er 1.4 11e1.60 gisette, 100 workers
GD EC-GD top-50 EC-GD top-100 EC-GD-star top-50 EC-DIANA top-50 l2-quant EC-DIANA top-50 rand-50

0 Nu1m0b0e0r0of2p0a0s0s0es 3th0r0o0u0gh4th0e00d0ata50000

f(xk) f(x * ) f(x0) f(x * )

f(xk) f(x * ) f(x0) f(x * )

Figure 7: Trajectories of EC-GD, EC-GD-star, EC-DIANA and GD applied to solve logistic regression problem with 100 workers.

19

B Compression Operators: Extra Commentary
Communication eﬃcient distributed SGD methods based on the idea of communication compression exists in two distinct varieties: i) methods based on unbiased compression operators, and ii) methods based on biased compression operators. The ﬁrst class of methods is much mire developed than the latter since it is easier to theoretically analyze unbiased operators. The subject of this paper is the study of the latter and dramatically less developed and understood class.
B.1 Unbiased compressors
By unbiased compression operators we mean randomized mappings Q : Rd → R satisfying the relations
EQ(x) = x and E Q(x) − x 2 ≤ ω x 2, ∀x ∈ Rd
for some ω ≥ 0. While operators satisfying the above relations are often in the literature called quantization operators, this class includes compressors which perform sparsiﬁcation as well.
Among the ﬁrst methods using unbiased compressors developed in this ﬁeld are QSGD [2], TernGrad [48] and DQGD [26]. The ﬁrst analysis of QSGD and TernGrad without bounded gradients assumptions was proposed in [37], which contains the best known results for QSGD and TernGrad. However, existing guarantees in the strongly convex case for QGSD, TernGrad, and DQGD establish linear convergence to some neighborhood of the solution only, even if the workers quantize the full gradients of their functions. This problem was resolved by Mishchenko et al. [37], who proposed the ﬁrst method, called DIANA, which uses quantization for communication and enjoys the linear rate of convergence to the exact optimum asymptotically in the strongly convex case when workers compute the full gradients of their functions in each iteration. Unlike all previous approaches, DIANA is based on the quantization of gradient diﬀerences rather than iterates or gradients. In essence, DIANA is a technique for reducing the variance introduced by quantization. Horváth et al. [19] generalized the DIANA method to the case of more general quantization operators. Moreover, the same authors developed a new method called VR-DIANA specially designed to solve problems (1) with the individual functions having the ﬁnite sum structure (3).
B.2 Biased compressors
By biased compressors we mean (possibly) randomized mappings C : Rd → R satisfying the average contraction relation
E C(x) − x 2 ≤ (1 − δ) x 2, ∀x ∈ Rd
for some δ > 0.
Perhaps the most popular biased compression operator is TopK, which takes vector x as input and substitutes all coordinates of x by zero except the k components with the largest absolute values. However, such a greedy approach applied to simple distributed SGD and even distributed GD can break the convergence of the method even when applied to simple functions in small dimensions, and may even lead to exponential divergence [7]. The error-feedback framework described in [23, 45, 46] and studies in this paper can ﬁx this problem, and it remains the only known mechanism that does so for all compressors described in (8). This is one of the main motivations for the study of the error-feedback mechanism. For instance, error feedback can ﬁx convergence issues with methods like sign-SGD [5]. The analysis of error feedback by Karimireddy et al. [23], Stich and Karimireddy [45], Stich et al. [46] works either under the assumption that the second moment of the stochastic gradient is uniformly bounded or only for the single-worker case. Recently Beznosikov et al. [7] proposed the ﬁrst analysis of SGD with error feedback for the general case of multiple workers without bounded second moment assumption. There is another line of works [27, 28] where authors apply arbitrary compressions in the decentralized setup. This approach has better potential than a centralized one in terms of reducing the communication cost. However, in this paper, we study only centralized architecture.
20

C Further Notation and Deﬁnitions

In what follows it will be useful to denote
vk d=ef n1 i vik, gk d=ef n1 i gik, ek d=ef n1 i eki .
By aggregating identities (5) across all i, we get ek+1 = ek + γgk − vk. In our proofs we also use the perturbed iterates technique [32, 36] based on the analysis of the following sequence

x˜k = xk − ek.

(24)

This sequence satisﬁes very useful for the analysis relation:

x˜k+1

(24)
=

xk+1

−

ek+1

(4),(5)
=

xk

−

vk

−

(ek

+

γgk

−

vk)

=

xk

−

ek

−

γgk

(24)
=

x˜k

−

γgk.

(25)

C.1 Quantization operators

Deﬁnition C.1. We say that stochastic mapping Q(x) : Rd → Rd is a quantization operator if there exists such ω > 0 that for any x ∈ Rd

E [Q(x)] = x, E Q(x) − x 2 ≤ ω x 2.

(26)

Below we enumerate some classical compression and quantization operators (see more in [7]).

1. TopK sparsiﬁcation. This compression operator is deﬁned as follows:

K
C(x) = x(i)e(i)
i=1

where |x(1)| ≥ |x(2)| ≥ . . . ≥ |x(d)| are components of x sorted in the decreasing order of their absolute values, e1, . . . , ed is the standard basis in Rd and K is some number from [d]. Clearly, TopK is a biased compression operator. One can show that TopK satisﬁes (8) with δ = Kd [7].
2. RandK sparsiﬁcation operator is deﬁned as

d Q(x) = K xiei
i∈S

where S is a random subset of [d] sampled from the uniform distribution on the all subset of [d] with cardinality K. RandK is an unbiased compression operator satisfying (26) with ω = Kd .
3. p-quantization. By 2-quantization we mean the following random operator:

Q(x) = x psign(x) ◦ ξ

where x p = di=1 |xi|p 1/p is an p-norm of vector x, sign(x) is a component-wise sign of vector x, a ◦ b deﬁnes a component-wise product of vectors a and b and ξ = (ξ1, . . . , ξd) is a random vector such that

1, with probability |xi| ,

ξi =

xp |x |

0, with probability 1 − xip .

One

can

s√how

that

this

operator

satisﬁes

(26).
√

In

particular,

if

p

=

2

it

satisﬁes

(26)

with ω = d − 1 and if p = ∞, then ω = 1+2 d − 1 (see [37]).

We assume that C is any operator which enjoys the following contractive property: there exists a constant 0 < δ ≤ 1 such that

E x − C(x) 2 ≤ (1 − δ) x 2, ∀x ∈ Rd.

21

D Basic Inequalities, Identities and Technical Lemmas

D.1 Basic inequalities

For all a, b, x1, . . . , xn ∈ Rd, β > 0 and p ∈ (0, 1] the following inequalities hold

a2 βb2

a, b ≤

+

,

(27)

2β

2

a − b, a + b = a 2 − b 2,

(28)

1 a 2 − b 2 ≤ a + b 2,

(29)

2

a + b 2 ≤ (1 + β) a 2 + (1 + 1/β) b 2,

(30)

n

2

n

xn ≤ n

xi 2,

(31)

i=1

i=1

p −1

1−

≤ 1 + p,

(32)

2

p

p

1 + (1 − p) ≤ 1 − .

(33)

2

2

D.2 Identities and inequalities involving random variables

Variance decomposition. For a random vector ξ ∈ Rd and any deterministic vector x ∈ Rd the variance can be decomposed as

E ξ − Eξ 2 = E ξ − x 2 − Eξ − x 2

(34)

Tower property of mathematical expectation. For random variables ξ, η ∈ Rd we have

E [ξ] = E [E [ξ | η]]

(35)

under assumption that all expectations in the expression above are well-deﬁned.
Lemma D.1 (Lemma 14 from [45]). For any τ vectors a1, . . . , aτ ∈ Rd and ξ1, . . . , ξτ zeromean random vectors in Rd, each ξt conditionally independent of {ξi}ti=−11 for all 1 ≤ t ≤ τ the following inequality holds


τ

2

τ

E  (at + ξt)  ≤ τ

τ
at 2 + E ξt 2.

t=1

t=1

t=1

(36)

D.3 Technical lemmas

Lemma D.2 (see also Lemma 2 from [44]). Let {rk}k≥0 satisfy

rK ≤ a + c1γ + c2γ2

(37)

γWK

for all K ≥ 0 with some constants a, c2 > 0, c1 ≥ 0 where {wk}k≥0 and {WK }K≥0 are deﬁned in (20), γ ≤ d1 . Then for all K such that ln(max{2,min{aµ2KK2/c1,aµ3K3/c2}}) ≤ min{ρ1, ρ2} and

γ = min

1 ln max{2, min{aµ2K2/c1, aµ3K3/c2}}

,

d

µK

(38)

we have that

µ

c1

c2

rK = O da exp − min d , ρ1, ρ2 K + µK + µ2K2 .

(39)

22

Proof. Since WK ≥ wK = (1 − η)−(K+1) we have

rK ≤ (1 − η)K+1 a + c1γ + c2γ2 ≤ a exp (−η(K + 1)) + c1γ + c2γ2.

(40)

γ

γ

Next we consider two possible situations.

1. If 1

≥

( ) ln max{2,min{aµ2K2/c1,aµ3K3/c2}} then we choose γ

=

d

µK

( ) ln max{2,min{aµ2K2/c1,aµ3K3/c2}} and get that
µK

rK (4≤0) γa exp (−η(K + 1)) + c1γ + c2γ2

ln max{2, min{aµ2K2/c1, aµ3K3/c2}} = O aµK exp − min ρ1, ρ2, K K

+O c1 + c2 . µK µ2K2

Since ln(max{2,min{aµ2KK2/c1,aµ3K3/c2}}) ≤ min{ρ1, ρ2} we have

aµ2K2 aµ3K3

rK = O aµK exp − ln max 2, min

,

c1

c2

+O c1 + c2 µK µ2K2

= O c1 + c2 . µK µ2K2

2. If 1 ≤ ( ) ln max{2,min{aµ2K2/c1,aµ3K3/c2}} then we choose γ = 1 which implies that

d

µK

d

(40)

µ ρ1 ρ2

c1 c2

rK ≤ da exp − min d , 4 , 4 (K + 1) + d + d2

µ

c1

c2

= O da exp − min d , ρ1, ρ2 K + µK + µ2K2 .

Combining the obtained bounds we get the result.

Lemma D.3. Let {rk}k≥0 satisfy

rK ≤ γaK + bK1γ + b2Kγ2 + c1γ + c2γ2 (41)

for all K ≥ 0 with some constants a > 0, b1, b2, c1, c2 ≥ 0 where γ ≤ γ0. Then for all K and

γ = min γ0,

aa ,3 ,
b1 b2

a

a

,3

c1K c2K

we have that

√

√

√

a

ab1 3 a2b2

ac1 3 a2c2

rK = O γ0K + K + K + K + K2/3 .

(42)

Proof. We have rK ≤ ≤
=

γaK + bK1γ + b2Kγ2 + c1γ + c2γ2

min γ0,

a ba1 , 3 ba2 ,

c1aK , 3 c2aK

+ b1 · KK

a

a2

+c1 ·

+ c2 3

c1K

c2K

√

√

√

O a + ab1 + 3 a2b2 + ac1 + 3 a2c2

γ0K K

K

K

K 2/3

a + b2 · 3 a b1 K b2 .

23

24

E Proofs for Section 3
E.1 A lemma Lemma E.1 (See also Lemma 8 from [45]). Let Assumptions 3.1, 3.4 and 3.2 be satisﬁed and γ ≤ 1/4(A . +C1M1+C2M2) Then for all k ≥ 0 we have
γ2 E f (xk) − f (x∗) ≤ (1 − η)ET k − ET k+1 + γ2(D1 + M1D2) + 3LγE ek 2, (43) where T k d=ef x˜k − x∗ 2 + M1γ2σ12,k + M2γ2σ22,k and M1 = 43Bρ11 , M2 = 4(B23+ρ243 G) .

Proof. We start with the upper bound for E x˜k+1 − x∗ 2. First of all, by deﬁnition of x˜k we have

x˜k+1 − x∗ 2

(25)
=

=

=

x˜k − x∗ − γgk 2 x˜k − x∗ 2 − 2γ x˜k − x∗, gk + γ2 gk 2 x˜k − x∗ 2 − 2γ xk − x∗, gk + γ2 gk 2 + 2γ xk − x˜k, gk .

Taking conditional expectation E · | xk from the both sides of the previous inequality we get

E x˜k+1 − x∗ 2 | xk

(18),(15)
≤
(9)
≤

x˜k − x∗ 2 − 2γ xk − x∗, ∇f (xk) +γ2 2A (f (xk) − f (x∗)) + B1σ12,k + B2σ22,k + D1 +2γ xk − x˜k, ∇f (xk)

x˜k − x∗ 2 − γµ xk − x∗ 2 − γ(2 − 2A γ)(f (xk) − f (x∗))

+γ2B1σ12,k + γ2B2σ22,k + γ2D1

+2γ xk − x˜k, ∇f (xk) .

(44)

Next,

−

xk − x∗

2 = − x˜k − x∗ + xk − x˜k

2

(29)
≤

1 −

x˜k − x∗

2+

xk − x˜k

2.

(45)

2

Using Fenchel-Young inequality we derive an upper bound for the inner product from (44):

xk −x˜k, ∇f (xk)

(27)
≤L

xk −x˜k

2+

1

∇f (xk)

(11)
2≤L

xk −x˜k

2+ 1 (f (xk)−f (x∗)).

(46)

4L

2

Combining previous three inequalities we get

E x˜k+1 − x∗ 2 | xk

(44)−(46)
≤

γµ 1−
2

x˜k − x∗ 2 − γ (1 − 2A γ) (f (xk) − f (x∗))

+γ2B1σ12,k + γ2B2σ22,k + γ2D1

+γ(2L + µ) xk − x˜k 2.

(47)

Taking into account that T k = x˜k − x∗ 2 + M1γ2σ12,k + M2γ2σ22,k with M1 = 43Bρ11 and M2 = 4(B23+ρ234 G) , using the tower property (35) of mathematical expectation together with

25

γ ≤ 4(A +C1M11+C2M2) , we conclude

E T k+1 (4≤7) 1 − γ2µ E x˜k − x∗ 2 − γ (1 − 2A γ) E f (xk) − f (x∗) + M1γ2E σ12,k+1 M2γ2E σ22,k+1 + γ2B1σ12,k + γ2B2σ22,k + γ2D1 + γ(2L + µ)E xk − x˜k 2

(16),(17)
≤
≤

γµ 1−

E x˜k − x∗ 2 +

1 + B1 − ρ1

M1γ2E σ2

2

M1

1,k

+ 1 + B2 + M1Gρ1 − ρ2 M2γ2E σ2 + γ2(D + M1D2)

M2

2,k

1

−γ (1 − 2(A + C1M1 + C2M2)γ) E f (xk) − f (x∗) + γ(2L + µ)E xk − x˜k 2 1 − γ2µ E x˜k − x∗ 2 + 1 − ρ41 M1γ2E σ12,k + 1 − ρ42 M2γ2E σ22,k − γ2 E f (xk) − f (x∗) + γ(2L + µ)E xk − x˜k 2 + γ2(D1 + M1D2).

Since L ≥ µ, x˜k = xk − ek and η d=ef min{ γ2µ , ρ41 , ρ42 } the last inequality implies γ2 E f (xk) − f (x∗) ≤ (1 − η)ET k − ET k+1 + γ2(D1 + M1D2) + 3LγE ek 2,

which concludes the proof.

E.2 Proof of Theorem 3.1

Proof. Form Lemma E.1 we have γ2 E f (xk) − f (x∗) ≤ (1 − η)ET k − ET k+1 + γ2(D1 + M1D2) + 3LγE ek 2.
Summing up these inequalities for k = 0, . . . , K with weights wk = (1 − η)−(k+1) we get

1K wkE f (xk) − f (x∗)
2
k=0

≤
(19),(20)
≤

K wk(1 − η) k wk k+1

K

ET − ET

γ

γ

+ γ(D1 + M1D2) wk

k=0

k=0

K
+3L wkE ek 2

k=0

K wk−1 k wk k+1

2

2

ET − ET

γ

γ

+ F1σ1,0 + F2σ2,0

k=0

2

1K

k

∗

+γ (D1 + M1D2 + D3)WK + 4 wkE f (x ) − f (x ) .

k=0

Rearranging the terms and using x¯K = W1K obtain

K k=0

wk xk

together

with

Jensen’s

inequality

we

E f (x¯K ) − f (x∗)

4(T 0 + γF1σ12,0 + γF2σ22,0)

≤

γWK

+ 4γ (D1 + M1D2 + D3) .

Finally, using the deﬁnition of the sequences {WK }K≥0 and {wk}k≥0 we derive that if µ >, then WK ≥ wK ≥ (1 − η)−K and we get (21). In the case when µ = 0 we have wk = 1 and WK = K which implies (22).

26

F SGD as a Special Case

In this section we want to show that our approach is general enough to cover many existing methods of SGD type. Consider the following situation:

vk = γgk, e0 = 0.

(48)

It implies that ek = 0 for all k ≥ 0 and the updates rules (4)-(5) gives us a simple SGD:

xk+1 = xk − γgk.

(49)

The following lemma formally shows that SGD under general enough assumptions satisﬁes Assumption 3.4.
Lemma F.1. Let Assumptions 3.1 and 3.2 be satisﬁes and inequalities (18), (15), (16) and (17) hold. Then for the method (49) inequality (19) holds with F1 = F2 = 0 and D3 = 0 for all k ≥ 0.

Proof. Since ek = 0 and f (xk) ≥ f (x∗) for all k ≥ 0 we get

3L K wkE ek 2 = 0 ≤ 1 K wkE f (xk) − f (x∗)

4

k=0

k=0

which concludes the proof.

It implies that all methods considered in [11] ﬁt our framework. Moreover, using Theorem 3.1 we derive the following result.
Theorem F.1. Let Assumptions 3.1 and 3.2 be satisﬁed, inequalities (18), (15), (16), (17) hold and γ ≤ 1/4(A . +C1M1+C2M2) Then for the method (49) for all K ≥ 0 we have

E f (x¯K ) − f (x∗) ≤ 1 − min γµ , ρ1 , ρ2 2 44

K 4T 0 γ + 4γ (D1 + M1D2) ,

when µ > 0 and E

when

µ

=

0,

where

Tk

def
=

f (x¯K ) − f (x∗) ≤ 4γTK0 + 4γ (D1 + M1D2) xk −x∗ 2 +M1γ2σ12,k +M2γ2σ22,k and M1 = 43Bρ11 , M2 = 4(B23+ρ243 G) .

In particular, if σ22,k ≡ 0, then our assumption coincides with the key assumption from [11] and our theorem recovers the same rates as in [11] when µ > 0. The case when µ = 0 was not considered in [11], while in our analysis we get it for free.

27

G Distributed SGD with Compression and Error Compensation

In this section we consider the scenario when compression and error-feedback is applied in order to reduce the communication cost of the method, i.e., we consider SGD with error compensation and compression (EC-SGD) which has updates of the form (4)-(5) with

1n

gk =

gik

n

i=1

1n

vk =

vik, vik = C(eki + γgik)

n

i=1

k

1n k

k+1

k

k

k

k

k

k

k

e= n

ei , ei = ei + γgi − vi = ei + γgi − C(ei + γgi ).

i=1

(50) (51)

Moreover, we assume that e0i = 0 for i = 1, . . . , n. Lemma G.1. Let Assumptions 3.1 and 3.2 be satisﬁed, Assumption 3.3 holds and4













 

δ

δ

 

γ ≤ min ,

,

 4µ   

96L 2δA + A + 1−2ρ1 Cρ11 + ρ22(G1−Cρ22)

2Bδ 1 + B1 + 2C2ρ2(21Bδ−2ρ+2B) 2 

(52)
where M1 = 43Bρ11 and M2 = 4(B23+ρ234 G) . Then EC-SGD satisﬁes Assumption 3.3, i.e., inequality (19) holds with the following parameters:

24Lγ2 F1 = δρ1(1 − η)

2B1 + B1 , δ 6Lγ D3 = δ

24Lγ2 F2 = δρ2(1 − η)

2G 1 − ρ1

2B1 + B˜1 δ

D2 2B1 + B1 + 2D1 + D1 .

ρ1 δ

δ

+ 2B2 + B2 , δ (53)
(54)

Proof. First of all, we derive an upper bound for the second moment of eki +1:

E eki +1 2

(51),(35)
=
(8)
≤
(35),(34)
=
(30)
≤

E E eki + γgik − C(eki + γgik) 2 | eki , gik

(1 − δ)E eki + γgik 2

(1 − δ)E eki + γg¯ik 2 + (1 − δ)γ2E gik − g¯ik 2

(1 − δ)(1 + β)E ek 2 + (1 − δ)

1 1+

γ2E g¯k 2

i

β

i

+(1 − δ)γ2E gik − g¯ik 2.

Summing up these inequalities for i = 1, . . . , n we get

1n

k+1 2

1n

k2

n E ei

≤ (1 − δ)(1 + β) n

E ei

i=1

i=1

+(1 − δ)

1 1+
β

21 n

k2

21 n

k

k2

γ n

E g¯i

+ (1 − δ)γ n

E gi − g¯i(55.)

i=1

i=1

4When ρ1 = 1 and ρ2 = 1 one can always set the parameters in such a way that B1 = B1 =

B2 = B2 = C1 = C2 = 0, D2 = 0. In this case we assume that 1−2ρ1 Cρ11 + ρ22(G1−Cρ22) 2Bδ 1 + B1 +

2C2 2Bδ 2 +B2 ρ2 (1−ρ2 )

= 0.

28

Consider β = 2(1δ−δ) . For this choice of β we have

δ

δ

(1 − δ)(1 + β) = (1 − δ) 1 +

=1−

2(1 − δ)

2

1 (1 − δ) 1 +
β

2(1 − δ) (1 − δ)(2 − δ) 2(1 − δ)

= (1 − δ) 1 +

=

≤

.

δ

δ

δ

Using this we continue our derivations:

1n

k+1 2

n E ei

≤

i=1

(13),(14)
≤

δ 1−
2

1n

k 2 2γ2(1 − δ) 1 n

k2

n E ei + δ n E g¯i

i=1

i=1

21 n

k

k2

+(1 − δ)γ n

E gi − g¯i

i=1

δ 1−
2

1n

k2

2

n E ei + 2γ (1 − δ)

i=1

2A +A E
δ

f (xk) − f (x∗)

+γ2(1 − δ) 2Bδ 1 + B1 Eσ12,k + γ2(1 − δ) 2Bδ 2 + B2 Eσ22,k

+γ2(1 − δ) 2D1 + D1 .

(56)

δ

Unrolling the recurrence above we get

1n

(56)

k+1 2

2

2A k δ k−l l ∗

n E ei

≤ 2γ (1 − δ)

+A

δ

1− 2

E f (x ) − f (x )

i=1

l=0

2

2B1

k

δ k−l 2

+γ (1 − δ) δ + B1

1− 2

Eσ1,l

l=0

2

2B2

k

δ k−l 2

+γ (1 − δ) δ + B2

1− 2

Eσ2,l

l=0

+γ2(1 − δ) 2D1 + D1 k 1 − δ k−l

δ

2

l=0

(57)

which implies

K

K (51)

1n

2 (31)

K

1n

2

3L wkE ek 2 = 3L wkE

eki ≤ 3L wk

E eki

n

n

k=0

k=0

i=1

k=0

i=1

(57) 6Lγ2(1 − δ) 2A

Kk

δ k−l

≤

+A

wk 1 −

E f (xl) − f (x∗)

1 − 2δ δ k=0 l=0 2

3Lγ2(1 − δ) + 1− δ
2

2B1 + B1 δ

Kk
wk
k=0 l=0

δ 1−
2

k−l
Eσ12,l

3Lγ2(1 − δ) + 1− δ
2

2B2 + B2 δ

Kk
wk
k=0 l=0

δ 1−
2

k−l
Eσ22,l

3Lγ2(1 − δ) + 1− δ
2

2D1 + D1 δ

Kk
wk
k=0 l=0

δ k−l

1−

.

2

(58)

In the remaining part of the proof we derive upper bounds for three terms in the righthand side of the previous inequality. First of all, recall that wk = (1 − η)−(k+1) and

29

η = min γ2µ , ρ41 , ρ42 . It implies that for all 0 ≤ i < k we have

(32)
wk = (1 − η)−(k−j+1) (1 − η)−j ≤ wk−j (1 + 2η)j

j (52)

δj

≤ wk−j (1 + γµ) ≤ wk−j 1 + 4 ,

(32)
wk = (1 − η)−(k−j+1) (1 − η)−j ≤ wk−j (1 + 2η)j min{ρ1, ρ2} j
≤ wk−j 1 + 2 .

(59) (60)

For simplicity, we introduce new notation: rk d=ef E f (xk) − f (x∗) . Using this we get

Kk

δ k−l

(59) K k

δ k−l

δ k−l

wk 1 − 2

rl ≤

wlrl 1 + 4

1− 2

k=0 l=0

k=0 l=0

(33) K k

δ k−l

≤ wlrl 1 − 4

k=0 l=0

K

∞

δk

4K

≤

wk rk

1− 4

= δ wkrk.

k=0

k=0

k=0

(61)

Next, we apply our assumption on σ22,k and derive that

(17)
Eσ22,k+1 ≤ (1 − ρ2)Eσ22,k + 2C2 E f (xk) − f (x∗)
rk k
≤ (1 − ρ2)k+1σ22,0 + 2C2 (1 − ρ2)k−lrl,
l=0

(62)

hence

Kk

δ k−l 2

Kk

δ k−l

l2

wk 1 − 2

Eσ2,l ≤

wk 1 − 2

(1 − ρ2) σ2,0

k=0 l=0

k=0 l=0

2C2 K k l + 1 − ρ2 wk
k=0 l=0 t=0

δ 1−
2

k−l
(1 − ρ2)l−trt.

Using this and

wk 1 − δ k−l (1 − ρ2)l−t (5≤9) wl 1 + δ k−l 1 − δ k−l (1 − ρ2)l−t

2

4

2

(33),(60)
≤

1 − δ k−l 1 + ρ2 l−t (1 − ρ2)l−twt

4

2

(33)

δ k−l

ρ2 l−t

≤ 1 − 4 1 − 2 wt

30

we derive

Kk

δ k−l 2

Kk

δ k−l

ρ2 l

2

wk 1 − 2

Eσ2,l ≤

wk 1 − 4

1− 2

w0σ2,0

k=0 l=0

k=0 l=0

2C2 K k l

δ k−l

ρ2 l−t

+ 1 − ρ2

1− 4

1 − 2 wtrt

k=0 l=0 t=0

∞ 2

δk

≤ w0σ2,0

1− 4

k=0

∞

ρ2 k

1−

k=0 2

2C2 1 − ρ2

K
wk rk
k=0

∞

δk

1−

k=0 4

∞

ρ2 k

1−

k=0 2

8σ22,0

16C2 K

= δρ2(1 − η) + δρ2(1 − ρ2) wkrk. (63)

k=0

Similarly, we estimate σ12,k:

(16)
Eσ12,k+1 ≤ (1 − ρ1)Eσ12,k + 2C1 E f (xk) − f (x∗) +Gρ1Eσ22,k + D2

rk

k

k

≤ (1 − ρ1)k+1σ12,0 + 2C1 (1 − ρ1)k−lrl + Gρ1 (1 − ρ1)k−lEσ22,k

l=0

l=0

k
+D2 (1 − ρ1)l

l=0

k

k

≤ (1 − ρ1)k+1σ12,0 + 2C1 (1 − ρ1)k−lrl + Gρ1 (1 − ρ1)k−lEσ22,k

l=0

l=0

∞

+D2 (1 − ρ1)l

l=0

k

k

= (1 − ρ1)k+1σ12,0 + 2C1 (1 − ρ1)k−lrl + Gρ1 (1 − ρ1)k−lEσ22,k

l=0

l=0

+ D2 . (64) ρ1

Using this we get

Kk

δ k−l 2

Kk 2

δ k−l

l

wk 1 − 2

Eσ1,l ≤ σ1,0

wk 1 − 2

(1 − ρ1)

k=0 l=0

k=0 l=0

2C1 K k l + 1 − ρ1 wk
k=0 l=0 t=0

δ 1−
2

k−l
(1 − ρ1)l−trt

Gρ1 K k l + 1 − ρ1 wk
k=0 l=0 t=0

δ 1−
2

k−l
(1 − ρ1)l−tEσ22,t

D2 K k l + ρ1 wk
k=0 l=0 t=0

δ 1−
2

k−l
(1 − ρ1)l−t.

(65)

31

Moreover,

wk 1 − δ k−l (1 − ρ1)l−t (5≤9) wl 1 + δ k−l 1 − δ k−l (1 − ρ1)l−t

2

4

2

(33),(60)
≤

1 − δ k−l 1 + ρ1 l−t (1 − ρ1)l−twt

4

2

(33)

δ k−l

ρ1 l−t

≤ 1 − 4 1 − 2 wt,

hence

Kk

δ k−l 2 (65)

Kk 2

δ k−l

ρ1 l

wk 1 − 2

Eσ1,l ≤ w0σ1,0

1− 4

1− 2

k=0 l=0

k=0 l=0

2C1 K k l

δ k−l

ρ1 l−t

+ 1 − ρ1

1− 4

1 − 2 wtrt

k=0 l=0 t=0

Gρ1 K k l

δ k−l

ρ1 l−t

2

+ 1 − ρ1

1− 4

1− 2

wtEσ2,t

k=0 l=0 t=0

+ D2 ρ1

K
wk
k=0

∞

δk

1−

k=0 2

∞
(1 − ρ1)k
k=0

∞ 2

δk

≤ w0σ1,0

1− 4

k=0

∞

ρ1 k

1−

k=0 2

+ 2C1 1 − ρ1

K
wk rk
k=0

∞

δk

1−

k=0 4

∞

ρ1 k

1−

k=0 2

+ Gρ1 1 − ρ1

K
wk Eσ22,k
k=0

∞

δk

1−

k=0 4

∞

ρ1 k

1−

k=0 2

+ 2D2 WK δρ1

8σ12,0

16C1 K

8G K

2

=

+

δρ1(1 − η) δρ1(1 − ρ1)

wkrk + δ(1 − ρ1)

wk Eσ2,k

k=0

k=0

+ 2D2 WK . (66) δρ1

32

For the third term in the right-hand side of previous inequality we have

8G K

2 (62) 8Gσ22,0 K

k

δ(1 − ρ1) wkEσ2,k ≤ δ(1 − ρ1) wk(1 − ρ2)

k=0

k=0

+ 16GC2

Kk
wk(1 − ρ2)k−lrl

δ(1 − ρ1)(1 − ρ2) k=0 l=0

(6≤0) 8Gσ22,0w0 K 1 + ρ2 k (1 − ρ2)k δ(1 − ρ1) k=0 2

16GC2

Kk

+

δ(1 − ρ1)(1 − ρ2) k=0 l=0

1 + ρ2 2

k−l
(1 − ρ2)k−lwlrl

(33) 8Gσ22,0w0 ∞

ρ2 k

≤

1−

δ(1 − ρ1) k=0 2

16GC2

Kk

+

δ(1 − ρ1)(1 − ρ2) k=0 l=0

1 − ρ2 2

k−l
wlrl

16Gσ22,0w0

16GC2

K

≤

+

δρ2(1 − ρ1) δ(1 − ρ1)(1 − ρ2)

wk rk

k=0

∞

ρ2 k

1−

k=0 2

16Gσ22,0

32GC2

K

= δρ2(1 − ρ1)(1 − η) + δρ2(1 − ρ1)(1 − ρ2) wkrk (67)

k=0

Combining inequalities (66) and (67) we get

Kk

δ k−l 2

8σ12,0

16

C1

2GC2

wk 1 − 2

Eσ1,l ≤

+ δρ1(1 − η) δ(1 − ρ1)

+ ρ1 ρ2(1 − ρ2)

k=0 l=0

16Gσ22,0

2D2

+ δρ2(1 − ρ1)(1 − η) + δρ1 WK

Finally, we estimate the last term in the right-hand side of (58):

K
wk rk
k=0
(68)

Kk

δ k−l

wk 1 − 2 ≤

k=0 l=0

K
wk
k=0

∞

δk

1−

k=0 2

2 = δ WK .

(69)

Plugging inequalities (61), (63), (68), (69) and

1−δ
δ

≤ 1 in (58) we obtain

1− 2

K 24L 2δA + A + 1−2ρ1 Cρ11 + ρ22(G1−Cρ22) 3L wkE ek 2 ≤
k=0 δ

2B1 δ

+ B1

+ 2C2 2Bδ 2 +B2
ρ2 (1−ρ2 )

24Lγ2 +
δρ1(1 − η)

2Bδ 1 + B1 σ12,0

24Lγ2 +
δρ2(1 − η)

2G 1 − ρ1

2Bδ 1 + B˜1 + 2Bδ 2 + B2 σ22,0

6Lγ2 D2 2B1

2D1

+

+ B1 +

+ D1 WK .

δ ρ1 δ

δ

γ2 K wk rk
k=0

Taking into account that γ ≤

, 96L

2δA +A+ 1−2ρ1

δ Cρ11 + ρ22(G1−Cρ22)

2Bδ 1 +B1

2C2
+

2Bδ 2 +B2

ρ2 (1−ρ2 )

F1 =

24Lγ2 δ ρ1 (1−η )

2B1 δ

+ B1

,

F2

=

24Lγ2 δ ρ2 (1−η )

2G 1−ρ1

2B1 δ

+ B˜1

+

2B2 δ

+ B2

and D3 =

33

6Lδγ Dρ12 2Bδ 1 + B1 + 2Dδ 1 + D1 we get

K

k2

1K

2

2

3L wkE e

≤ 4

wkrk + F1σ1,0 + F2σ2,0 + γD3.

k=0

k=0

As a direct application of Lemma G.1 and Theorem 3.1 we get the following result.

Theorem G.1. Let Assumptions 3.1 and 3.2 be satisﬁed, Assumption 3.3 holds and

1

γ≤

,

4(A + C1M1 + C2M2)













 

δ

δ

 

γ ≤ min ,

,

 4µ   

96L 2δA + A + 1−2ρ1 Cρ11 + ρ22(G1−Cρ22)

2Bδ 1 + B1 + 2C2ρ2(21Bδ−2ρ+2B) 2 

where M1 = 43Bρ11 and M2 = 4(B23+ρ243 G) . Then for all K ≥ 0 we have

K

∗

K 4(T 0 + γF1σ12,0 + γF2σ22,0)

E f (x¯ ) − f (x ) ≤ (1 − η)

γ

+ 4γ (D1 + M1D2 + D3) ,

when µ > 0 and

K

∗

4(T 0 + γF1σ12,0 + γF2σ22,0)

E f (x¯ ) − f (x ) ≤

γK

+ 4γ (D1 + M1D2 + D3)

when

µ

=

0,

where

η

=

min {γµ/2, ρ1/4, ρ2/4},

Tk

def
=

x˜k − x∗ 2 + M1γ2σ12,k + M2γ2σ22,k and

24Lγ2 F1 = δρ1(1 − η)

2B1 + B1 , δ

24Lγ2 F2 = δρ2(1 − η)

2G 1 − ρ1

2B1 + B˜1 δ

+ 2B2 + B2 , δ

D3 = 6Lγ D2 2B1 + B1 + 2D1 + D1 .

δ ρ1 δ

δ

34

H SGD with Delayed Updates

In this section we consider the SGD with delayed updates (D-SGD) [1, 33, 10, 3, 45]. This method has updates of the form (4)-(5) with

1n

gk =

gik

n

i=1

k

1 n k k γgik−τ , if t ≥ τ,

v= n

vi , vi = 0,

if t < τ

i=1

k

1n k

k+1

k

k

k

τ k+1−t

e= n

ei , ei = ei + γgi − vi = γ

gi

,

i=1

t=1

(70) (71)

where the summation is performed only for non-negative indices. Moreover, we assume that e0i = 0 for i = 1, . . . , n.
For convenience we also introduce new constant:

Aˆ = A + Lτ.

(72)

Lemma H.1. Let Assumptions 3.1 and 3.2 be satisﬁed, inequalities (15), (16) and (17) hold

and5









 

1

1

 

γ ≤ min

,

,

 2τ µ 8 Lτ Aˆ + ρ12(B11−Cρ11) + ρ22(B12−Cρ22) + ρ2(14−Bρ11G)(C12−ρ2) 

(73)

where M1 = 43Bρ11 and M2 = 4(B23+ρ234 G) . Then D-SGD satisﬁes Assumption 3.4, i.e., inequality (19) holds with the following parameters:

6γ2LB1τ (2 + ρ1)

6γ2τ L(2 + ρ2) 2B1G

F1 =

ρ1 , F2 = ρ2

1 − ρ1 + B2 ,

(74)

D3 = 3γτ L D + 2B1D2 .

(75)

1

ρ1

Proof. First of all, we derive an upper bound for the second moment of eki :

E ek 2

(71)
=
(36)
≤
(34)
≤
(15),(11)
≤

 γ2E 

τ

2

gk−t 

t=1

τ
γ2τ E

τ
∇f (xk−t) 2 + γ2 E

t=1 τ
γ2τ E

t=1 τ
∇f (xk−t) 2 + γ2 E

t=1

t=1

τ

2γ2 (A + Lτ ) E f (xk−t) − f (x∗)

Aˆ

t=1

τ

+γ2B2 Eσ22,k−t + γ2τ D1

t=1

gk−t − ∇f (xk−t) 2
gk−t 2
τ
+ γ2B1 Eσ12,k−t
t=1

(76)

5When ρ1 = 1 and ρ2 = 1 one can always set the parameters in such a way that B1 = B1 = B2 = B2 = C1 = C2 = 0, D2 = 0. In this case we assume that ρ12(B11−Cρ11) = ρ22(B12−Cρ22) = 0.

35

which implies

K

(76)

Kτ

3L wkE ek 2 ≤ 6γ2LAˆ

wkE f (xk−t) − f (x∗)

k=0

k=0 t=1

K
+3γ2LB1

τ
wk Eσ12,k−t

k=0 t=1

K
+3γ2LB2

τ
wkEσ22,k−t + 3γ2τ LD1WK

k=0 t=1

(77)

In the remaining part of the proof we derive upper bounds for four terms in the righthand side of the previous inequality. First of all, recall that wk = (1 − η)−(k+1) and η = min γ2µ , ρ41 , ρ42 . It implies that for all 0 ≤ i < k and 0 ≤ t ≤ τ we have

(32)
wk = (1 − η)−(k−t+1) (1 − η)−t ≤ wk−t (1 + 2η)t

t (73)

1t

t

≤ wk−t (1 + γµ) ≤ wk−t 1 + 2τ ≤ wk−t exp 2τ ≤ 2wk−t, (78)

−(k−j+1) −j (32) j

min{ρ1, ρ2} j

wk = (1 − η)

(1 − η) ≤ wk−j (1 + 2η) ≤ wk−j 1 + 2

.(79)

For simplicity, we introduce new notation: rk d=ef E f (xk) − f (x∗) . Using this we get

Kτ

(78)

wkrk−t ≤

Kτ

K

2wk−trk−t ≤ 2τ wkrk

k=0 t=1

k=0 t=1

k=0

(80)

Similarly, we estimate the second term in the right-hand side of (79):

Kτ

Kτ

K

wkEσ12,k−t ≤

2wk−tEσ12,k−t ≤ 2τ wkEσ12,k

k=0 t=1

k=0 t=1

k=0

(64)

K 2

k 4C1τ K k

k−l

≤ 2τ σ1,0 wk(1 − ρ1) + 1 − ρ1

wk(1 − ρ1) rl

k=0

k=0 l=0

2Gρ1τ K k

k−l 2 2τ D2

+ 1 − ρ1

wk(1 − ρ1) Eσ2,l + ρ WK .

k=0 l=0

(81)

For the ﬁrst term in the right-hand side of previous inequality we have

K
2τ σ12,0 wk(1 − ρ1)k
k=0

(79)

K 2

ρ1 k+1

k

≤ 2τ σ1,0

1+ 2

(1 − ρ1)

k=0

(33)

ρ1

K 2

ρ1 k

≤

2τ 1 + 2

σ1,0

1− 2

k=0

∞ 2

ρ1 k 2τ (2 + ρ1) σ12,0

≤ τ (2 + ρ1) σ1,0

1−

≤

2

. ρ1

k=0

(82)

36

The second term in the right-hand side of (81) can be upper bounded in the following way:

4C1τ K k wk(1 − ρ1)k−lrl (7≤9) 4C1τ K k wlrl 1 + ρ1 k−l (1 − ρ1)k−l

1 − ρ1 k=0 l=0

1 − ρ1 k=0 l=0 2

(33) 4C1τ K k

ρ1 k−l

≤ 1 − ρ1

wlrl 1 − 2

k=0 l=0

4C1τ K

≤ 1 − ρ1

wk rk

k=0

∞

ρ1 k

1−

k=0 2

8C1τ K ≤ ρ1(1 − ρ1) wkrk. (83)
k=0

Repeating similar steps we estimate the third term in the right-hand side of (81):

2Gρ1τ K k

k−l 2

4Gτ K

2

1 − ρ1

wk(1 − ρ1) Eσ2,l ≤ 1 − ρ1 wkEσ2,k

k=0 l=0

k=0

(6≤2) 4Gτ σ22,0 K wk(1 − ρ2)k 1 − ρ1 k=0

+ 8GC2

Kk
wk(1 − ρ2)k−lrl

(1 − ρ1)(1 − ρ2) k=0 l=0

(7≤9) 4Gτ σ22,0 K 1 + ρ2 k+1 (1 − ρ2)k 1 − ρ1 k=0 2

8GC2τ

Kk

+

(1 − ρ1)(1 − ρ2) k=0 l=0

1 + ρ2 2

(33) 2Gτ (2 + ρ2)σ22,0 ∞ ≤
1 − ρ1 k=0

k−l
(1 − ρ2)k−lwlrl 1 − ρ2 k
2

8GC2τ

Kk

+

(1 − ρ1)(1 − ρ2) k=0 l=0

1 − ρ2 2

k−l
wlrl

4Gτ (2 + ρ2)σ22,0 ≤
ρ2(1 − ρ1)

+ 8GC2τ (1 − ρ1)(1 − ρ2)

K
wk rk
k=0

∞

ρ2 k

1−

k=0 2

4Gτ (2 + ρ2)σ22,0 =
ρ2(1 − ρ1)

16GC2τ

K

+ ρ2(1 − ρ1)(1 − ρ2) wkrk (84)

k=0

Combining inequalities (81), (82), (83) and (84) we get

Kτ 2

2τ (2 + ρ1) σ12,0

8τ C1

2GC2

K

wkEσ1,k−t ≤

ρ1 + 1 − ρ1 ρ1 + ρ2(1 − ρ2) wkrk

k=0 t=1

k=0

4Gτ (2 + ρ2)σ22,0 2τ D2 + ρ2(1 − ρ1) + ρ WK . (85)

37

Next, we derive

Kτ
wkEσ22,k−t ≤

Kτ

K

2wk−tEσ22,k−t ≤ 2τ wkEσ22,k

k=0 t=1

k=0 t=1

k=0

(62)

K

≤ 2τ σ22,0 wk(1 − ρ1)k

k=0

+ 4C2τ K k wk(1 − ρ2)k−lrl. 1 − ρ2 k=0 l=0

For the ﬁrst term in the right-hand side of previous inequality we have

(86)

K 2

(79) k

K 2

ρ2 k+1

k

2τ σ2,0 wk(1 − ρ2) ≤ 2τ σ2,0

1+ 2

(1 − ρ2)

k=0

k=0

(33)

ρ2

K 2

ρ2 k

≤

2τ 1 + 2

σ2,0

1− 2

k=0

∞ 2

ρ2 k 2τ (2 + ρ2) σ22,0

≤ τ (2 + ρ2) σ2,0

1−

≤

2

. ρ2

k=0

The second term in the right-hand side of (86) can be upper bounded in the following way:

4C2τ K k wk(1 − ρ2)k−lrl (7≤9) 4C2τ K k wlrl 1 + ρ2 k−l (1 − ρ2)k−l

1 − ρ2 k=0 l=0

1 − ρ2 k=0 l=0 2

(33) 4C2τ K k

ρ2 k−l

≤ 1 − ρ2

wlrl 1 − 2

k=0 l=0

4C2τ K

≤ 1 − ρ2

wk rk

k=0

∞

ρ2 k

1−

k=0 2

8C2τ K

≤ ρ2(1 − ρ2)

wk rk ,

k=0

hence

Kτ

2

(86) 2τ (2 + ρ2) σ22,0

8C2τ K

wkEσ2,k−t ≤

ρ2 + ρ2(1 − ρ2) wkrk.

k=0 t=1

k=0

(87)

Plugging inequalities (80), (85) and (87) in (77) we obtain

K
3L w E ek 2

≤

12γ2Lτ

Aˆ +

2B1C1

+

2B2C2

+

4B1GC2

k

ρ1(1 − ρ1) ρ2(1 − ρ2) ρ2(1 − ρ1)(1 − ρ2)

K
wk rk

k=0

k=0

+ 6γ2LB1τ (2 + ρ1) σ2 + 6γ2τ L(2 + ρ2)

ρ1

0

ρ2

2B1G + B σ2

1 − ρ1

2 2,0

+3γ2τ L D1 + 2Bρ1D2 WK .

Taking into account that γ ≤

1

, F1 =

4 4Lτ Aˆ+ 2B1C1 + 2B2C2 + 4B1GC2
ρ1(1−ρ1) ρ2 (1−ρ2 ) ρ2(1−ρ1)(1−ρ2)

6γ2LB1ρτ1(2+ρ1) , F2 = 6γρ22τ L

2B11G−(2ρ+1 ρ2) + B2

and D3 = 3γτ L

D1

+

2B1 D2 ρ

we get

K

k2

1K

2

2

3L wkE e

≤ 4

wkrk + F1σ1,0 + F2σ2,0 + γD3.

k=0

k=0

38

As a direct application of Lemma H.1 and Theorem 3.1 we get the following result.

Theorem H.1. Let Assumptions 3.1 and 3.2 be satisﬁed, inequalities (15), (16) and (17) hold and









 

1

1

1

 

γ ≤ min

,,

,

 4(A + C1M1 + C2M2) 2τ µ 8 Lτ Aˆ + ρ12(B11−Cρ11) + ρ22(B12−Cρ22) + ρ2(14−Bρ11G)(C12−ρ2) 

where M1 = 43Bρ11 and M2 = 4(B23+ρ243 G) . Then for all K ≥ 0 we have

K

∗

K 4(T 0 + γF1σ12,0 + γF2σ22,0)

E f (x¯ ) − f (x ) ≤ (1 − η)

γ

+ 4γ (D1 + M D2 + D3)

when µ > 0 and

K

∗

4(T 0 + γF1σ12,0 + γF2σ22,0)

E f (x¯ ) − f (x ) ≤

γK

+ 4γ (D1 + M D2 + D3)

when

µ

=

0,

where

η

=

min {γµ/2, ρ1/4, ρ2/4},

Tk

def
=

x˜k − x∗ 2 + M1γ2σ12,k + M2γ2σ22,k and

F1 = 6γ2LB1τ (2 + ρ1) , ρ1

6γ2τ L(2 + ρ2) F2 =
ρ2

2B1G + B , 1 − ρ1 2

D3 = 3γτ L D + 2B1D2 .

1

ρ1

39

Algorithm 1 DIANAsr with Double Compression (DIANAsr-DQ)

Input:

learning

rates

γ

> 0,

α ∈ (0, 1],

initial

vectors

x

0

,

h01

,

.

.

.

,

h

0 n

∈ Rd

1: Set h0 = n1

n i=1

h0i

2: for k = 0, 1, . . . do

3: Broadcast gk−1 to all workers

If k = 0, then broadcast x0

4: for i = 1, . . . , n in parallel do

5:

xk = xk−1 − γgk−1

Ignore this line if k = 0

6:

Sample gik,1 = ∇fξk (xk) satisfying Assumption I.1 independtently from other

i

workers

7:

∆ˆ ki = gik,1 − hki

8:

Sample ∆ki ∼ Q1(∆ˆ ki ) indepently from other workers

9:

gik,2 = hki + ∆ki

10:

hki +1 = hki + α∆ki

11: end for

12:

gk,2 = n1

n i=1

gik,2

=

hk

+

1 n

n i=1

∆ki

n

n

13: hk+1 = n1 hki +1 = hk + α n1 ∆ki

i=1

i=1

14: Sample gk ∼ Q2(gk,2)

15: xk+1 = xk − γgk−1

16: end for

I Special Cases: SGD
To illustrate the generality of our approach, we develop and analyse a new special case of SGD without error-feedback and show that in some cases, our framework recovers tighter rates than the framework from [11].

I.1 DIANA with Arbitrary Sampling and Double Quantization

In this section we consider problem (1) with f (x) being µ-quasi strongly convex and fi(x) satisfying (3) where functions fij(x) are diﬀerentiable, but not necessary convex. Following [13] we construct a stochastic reformulation of this problem:

1n

1m

f (x) = ED [fξ(x)] , fξ(x) = n fξi (x), fξi (x) = m ξijfij(x),

(88)

i=1

j=1

where ξ = (ξ1 , . . . , ξn ), ξi = (ξi1, . . . , ξim) is a random vector with distribution Di such that EDi [ξij] = 1 for all i ∈ [n], j ∈ [m] and the following assumption holds.

Assumption I.1 (Expected smoothness). We assume that functions f1, . . . , fn are L-smooth

in expectation w.r.t. distributions D1, . . . , Dn, i.e., there exists constant L = L(f, D1, . . . , Dn)

such that

EDi ∇fξi (x) − ∇fξi (x∗) 2 ≤ 2LDfi (x, x∗)

(89)

for all i ∈ [n] and x ∈ Rd.

To solve this problem, we consider DIANA [37, 19] — a distributed stochastic method using unbiased compressions or quantizations for communication between workers and master. We start with the formal deﬁnition of quantization. In [37, 19] DIANA was analyzed under the assumption that stochastic gradients gik have uniformly bounded variances which is not very practical.
Therefore, we consider a slightly diﬀerent method called DIANAsr-DQ which works with the stochastic reformulation (88) of problem (1)+(3), see Algorithm 1. Moreover, to illustrate the ﬂexibility of our approach, we consider compression not only on the workers’ side but also on the master side. To perform an update of DIANAsr-DQ master needs to gather quantized gradient diﬀerences ∆ki and the to broadcast quantized stochastic gradient gk to all workers. Clearly, in this case, only compressed vectors participate in communication.

40

In the concurrent work [40] the same method was independently proposed under the name of Artemis. However, our analysis is slightly more general: it is based on Assumption I.1 while in [40] authors assume L-cocoercivity of stochastic gradients almost surely. Next, a very similar approach was considered in [47], where authors present a method with error compensation on master and worker sides. Moreover, recently another method called DORE was developed in [35], which uses DIANA-trick on the worker side and error compensation on the master side. However, in these methods, compression operators are the same on both sides, despite the fact that gathering the information often costs much more than broadcasting. Therefore, the natural idea is in using diﬀerent quantization for gathering and broadcasting, and it is what DIANAsr-DQ does. Moreover, we do not assume uniform boundedness of the second moment of the stochastic gradient like in [47], and we also do not assume uniform boundedness of the variance of the stochastic gradient like in [35]. Assumption I.1 is more natural and always holds for the problems (1)+(3) when fij are convex and L-smooth for each i ∈ [n], j ∈ [m]. In contrast, in the same setup, there exist such problems that the variance of the stochastic gradients is not uniformly upper bounded by any ﬁnite constant.
We assume that Q1 and Q2 satisfy (26) with parameters ω1 and ω2 respectively.
Lemma I.1. Let Assumption I.1 be satisﬁed. Then, for all k ≥ 0 we have

E gk | xk E gk 2 | xk

= ∇f (xk), ≤ 2L(1 + ω2) 2 + 3ω1
n

(90) f (xk) − f (x∗) + 3ω1(1n+ ω2) σk2 + D1,(91)

where

σk2

=

1 n

n i=1

hki

−

∇f (x∗) 2

and

D1

=

n

(2+3ω1 )(1+ω2 ) n2

EDi ∇fξi (x∗) − ∇fi(x∗) 2 .

i=1

Proof. First of all, we show inbiasedness of gk:

E gk | xk

(35),(26)
=
(35),(26)
= =

E gk,2 | xk

1n

= hk +

E ∆ki | xk

n

i=1

hk + 1 n E ∆ˆ k | xk

n

i

i=1

hk + 1 n n
i=1

∇fi(xk) − hki

= ∇f (xk).

Next, to denote mathematical expectation w.r.t. the randomness coming from quantizations

Q1 and Q2 at iteration k we use EQk [·] and EQk [·] respectively. Using these notations and

1

2

the deﬁnition of quantization we derive

EQk [ gk 2] 2

(34),(26)
=
(26)
≤

gk,1 2 + EQk gk,2 − gk,1 2 2
(1 + ω2) gk,1 2.

41

Taking the conditopnal mathematical expectation EQk [·] from the both sides of previous

1

inequality

and

using

the

independence

of

∆

1 i

,

.

.

.

,

∆

n i

we

get

EQk1 ,Qk2 gk 2


n

2

(3=5) (1 + ω2)EQk1 gk,1 2 = (1 + ω2)EQk1  n1 (hki + ∆ki ) 

i=1

n

2


n

2

(34)

1

= (1 + ω2) n

hki + ∆ˆ ki

1

+ (1 + ω2)EQk  1

n

(∆ki − ∆ˆ ki ) 

i=1

i=1

n

2

1 = (1 + ω2) n

∇fξk (xk) − ∇fξk (x∗) + ∇fξk (x∗) − ∇fi(x∗)

i

i

i

i=1

(1 + ω2) n

+ n2

EQk 1

i=1

∆ki − ∆ˆ ki 2

(31),(26)
≤

2(1 + ω2) n n i=1

∇fξk (xk) − ∇fξk (x∗) 2

i

i

n

2

1 +2(1 + ω2) n

∇fξk (x∗) − ∇fi(x∗) i

i=1

ω1(1 + ω2) n + n2
i=1

∇fξk (xk) − hki 2 i

(31) 2(1 + ω2) n

k

∗2

≤

n

∇fξk (x ) − ∇fξk (x )

i

i

i=1

n

2

1 +2(1 + ω2) n

∇fξk (x∗) − ∇fi(x∗) i

i=1

3ω1(1 + ω2) n + n2
i=1

∇fξk (xk) − ∇fξk (x∗) 2

i

i

3ω1(1 + ω2) n + n2
i=1

∇fξk (x∗) − ∇fi(x∗) 2 i

3ω1(1 + ω2) n k

∗2

+ n2

hi − ∇fi(x ) .

i=1

Finally, we take conditional mathematical expectation E[· | xk] from the both sides of the inequality above and use the independece of ξ1k, . . . , ξnk:

E gk 2 | xk (8≤9) 2L(1 + ω2) 2 + 3nω1 (f (xk) − f (x∗)) + 3ω1(1n+ ω2) σk2

 +2(1 + ω2)E 

1n n i=1

∇fξk (x∗) − ∇fi(x∗) i

2 | xk

3ω1(1 + ω2) n

+ n2

EDi

i=1

∇fξi (x∗) − ∇fi(x∗) 2

= 2L(1 + ω2) 2 + 3nω1 (f (xk) − f (x∗)) + 3ω1(1n+ ω2) σk2

(1 + ω2)(2 + 3ω1) n

+ n2

EDi

i=1

∇fξi (x∗) − ∇fi(x∗) 2 .

42

Lemma I.2. Let fi be convex and L-smooth, Assumption I.1 holds and α ≤ 1/(ω1+1). Then, for all k ≥ 0 we have

E σk2+1 | xk ≤ (1 − α)σk2 + 2α(3L + 4L)(f (xk) − f (x∗)) + D2,

(92)

where

σk2

=

1 n

n i=1

hki − ∇fi(x∗)

2

and

D2

=

3α n

n i=1

EDi

∇fξi (x∗) − ∇fi(x∗) 2 .

Proof. For simplicity, we introduce new notation: h∗i d=ef ∇fi(x∗). Using this we derive an upper bound for the second moment of hki +1 − h∗i :

E hki +1 − h∗i 2 | xk

= E hki − h∗i + α∆ki 2 | xk

(26)
=
(26),(35)
≤

hki − h∗i 2 + 2α hki − h∗i , ∇fi(xk) − hki + α2E
hki − h∗i 2 + 2α hki − h∗i , ∇fi(xk) − hki +α2(ω1 + 1)E ∇fξk (xk) − hki 2 | xk .
i

∆ki 2 | xk

Using variance decomposition (34) and α ≤ 1/(ω1+1) we get

α2(ω1 + 1)EDi ∇fξk (xk) − hki 2 i

(34)
=
(31)
≤
(11),(89)
≤

α2(ω1 + 1)EDi ∇fξk (xk) − ∇fi(xk) 2 i +α2(ω1 + 1) ∇fi(xk) − hki 2

3αEDi ∇fξk (xk) − ∇fξk (x∗) 2

i

i

+3αEDi ∇fξk (x∗) − ∇fi(x∗) 2 i
+3α ∇fi(xk) − ∇fi(x∗) 2 +α ∇fi(xk) − hki 2

6α(L + L)Dfi (xk, x∗) + α ∇fi(xk) − hki 2 +3αEDi ∇fξk (x∗) − ∇fi(x∗) 2
i

Putting all together we obtain

E hki +1 − h∗i 2 | xk

≤ hki − h∗i 2 + α ∇fi(xk) − hki , fi(xk) + hki − 2h∗i

+6α(L + L)Dfi (xk, x∗) + 3αEDi ∇fξk (x∗) − ∇fi(x∗) 2 i

(28)
=

hki − h∗i 2 + α ∇fi(xk) − h∗i 2 − α hki − h∗i 2

+6α(L + L)Dfi (xk, x∗) + 3αEDi ∇fξk (x∗) − ∇fi(x∗) 2 i

(11)
≤ (1 − α) hki − h∗i 2 + α(6L + 8L)Dfi (xk, x∗)

+3αEDi ∇fξk (x∗) − ∇fi(x∗) 2 . i

Summing up the above inequality for i = 1, . . . , n we derive

1n E
n i=1

hki +1 − h∗i 2 | xk

1−α n k ∗ 2

k

∗

≤ n

hi − hi + α(6L + 8L)(f (x ) − f (x ))

i=1

3α n

+ n

EDi

i=1

∇fξk (x∗) − ∇fi(x∗) 2 . i

43

Theorem I.1. Assume that fi(x) is convex and L-smooth for all i = 1, . . . , n, f (x) is µ-quasi strongly convex and Assumption I.1 holds. Then DIANAsr-DQ satisﬁes Assumption 3.4 with

3ω1

3ω1(1 + ω2)

A = L(1 + ω2) 2 + n , B1 =

, n

(2 + 3ω1)(1 + ω2) n

D1 =

n2

EDi

i=1

∇fξi (x∗) − ∇fi(x∗) 2 ,

2

2 1n

σ1,k = σk = n

i=1

hki − ∇fi(x∗) 2,

B2 = 0,

σ22,k ≡ 0,

ρ1 = α,

ρ2 = 1,

C1 = α(3L + 4L),

C2 = 0,

3α n

D2 = n

EDi

i=1

∇fξi (x∗) − ∇fi(x∗) 2 ,

G = 0, F1 = F2 = 0, D3 = 0,

with γ and α satisfying

1 γ ≤ 4(1 + ω2) L 2 + 15nω1 + 16Lnω1 , and for all K ≥ 0

1

α≤

,

ω+1

M1 = 4ω1(1 + ω2) , nα

M2 = 0

E f (x¯K ) − f (x∗) ≤

1 − min

γµ α ,

24

K 4T 0 γ + 4γ (D1 + M1D2) ,

when µ > 0 and

E f (x¯K ) − f (x∗) ≤ 4γTK0 + 4γ (D1 + M1D2)

when

µ

=

0,

where

Tk

def
=

xk − x∗ 2 + M1γ2σ12,k.

In other words, if

1 γ = 4(1 + ω2) L 2 + 15nω1 + 16Lnω1 ,

1 α=
ω+1

and D1 = 0, i.e., ∇fξk (xk) = ∇fi(xk) almost surely, DIANAsr-DQ converges with the linear

i

rate

O ω1 + L (1 + ω2) 1 + ω1 ln 1

µ

n

ε

to the exact solution. Applying Lemma D.2 we establish the rate of convergence to ε-solution.

Corollary I.1. Let the assumptions of Theorem I.1 hold and µ > 0. Then after K iterations of DIANAsq-DQ with the stepsize

1 γ0 = 4(1 + ω2) L 2 + 15nω1 + 16Lnω1

 

ln max 2, µ2K2( x0−x∗ 2+M1γ02σ12,0)

 γ = min γ0,

D1 +M1 D2

 µK





 

4ω1(1 + ω2)

, M1 =

 nα



and α = ω+1 1 we have

E f (x¯K ) − f (x∗) = O A x0 − x∗ 2 exp − min

µ1 ,

K + D1 + M1D2 .

A ω1

µK

That is, to achive E f (x¯K ) − f (x∗) ≤ ε DIANAsq-DQ requires

L 1 + ωn1 (1 + ω2) (1 + ω1)(1 + ω2) n

∗

∗2

O ω1 +

µ

+ n2µε

EDi ∇fξi (x ) − ∇fi(x )

i=1

iterations.

44

Applying Lemma D.3 we get the complexity result in the case when µ = 0.

Corollary I.2. Let the assumptions of Theorem I.1 hold and µ = 0. Then after K iterations of DIANAsq-DQ with the stepsize

1 γ0 = 4(1 + ω2) L 2 + 15nω1 + 16Lnω1

γ = min γ0,

x0 − x∗ 2

x0 − x∗ 2

4ω1(1 + ω2)

M1σ2 , (D + M1D2)K , M1 =

nα

1,0

1

and α = ω+1 1 we have E f (x¯K ) − f (x∗) of order

LR02(1 + ω2) 1 + ω1

√ R0σ1,0(1 + ω1) 1 + ω2 R0

O

n+

√

+

K

nK

(1 + ω1)(1 + ω2)Dopt √ nK

where R0 =

n

x0 − x∗ 2, Dopt

=

1 n

EDi ∇fξi (x∗) − ∇fi(x∗) 2.

i=1

E f (x¯K ) − f (x∗) ≤ ε DIANAsq-DQ requires

That is, to achive

LR02(1 + ω2) 1 + ω1

√ R0σ1,0(1 + ω1) 1 + ω2

R2(1 + ω1)(1 + ω2)Dopt

O ε n + √nε + 0 nε2

iterations.

I.2 Recovering Tight Complexity Bounds for VR-DIANA

In this section we consider the same problem (1)+(3) and variance reduced version of DIANA called VR-DIANA [19], see Algorithm 2. For simplicity we assume that each fij is convex and L-smooth and fi is additionally µ-strongly convex.
Lemma I.3 (Lemmas 3, 5, 6 and 7 from [19]). Let α ≤ ω+1 1 . Then for all iterates k ≥ 0 of Algorithm 2 the following inequalities hold:

E gk | xk = ∇f (xk),

(93)

E Hk+1 | xk ≤ (1 − α) Hk + 2α Dk + 8αLn f (xk) − f (x∗) ,

(94)

m

E Dk+1 | xk ≤

1 1−

Dk + 2Ln f (xk) − f (x∗) ,

(95)

m

E gk 2 | xk

4ω + 2 ≤ 2L 1 +
n

f (xk) − f (x∗) + 2ω Dk + 2(ω + 1) Hk, (96)

n2 m

n2

n

nm

where Hk = hki − ∇fi(x∗) 2 and Dk =

∇fij (wikj ) − ∇fij (x∗) 2.

i=1

i=1 j=1

This lemma shows that VR-DIANA satisﬁes (15), (16) and (17). Applying Theorem F.1 we get the following result.

Theorem I.2. Assume that fij(x) is convex and L-smooth for all i = 1, . . . , n and fi(x) is µ-strongly convex for all i = 1, . . . , n. Then VR-DIANA satisﬁes Assumption 3.4 with

4ω + 2

2(ω + 1)

A =L 1+ n

, B1 = n , D1 = 0,

2

k 1n

σ1,k = H

= n

i=1

hki − ∇fi(x∗) 2,

2ω

B2 =

, n

2

k 1 nm

σ2,k = D

= nm

i=1 j=1

∇fij (wikj ) − ∇fij (x∗) 2,

ρ1 = α,

1 ρ2 = m ,

L C1 = 4αL, C2 = m , D2 = 0, G = 2, F1 = F2 = 0, D3 = 0,

45

Algorithm 2 VR-DIANA based on LSVRG (Variant 1), SAGA (Variant 2), [19]

Input:

learning

rates

α>0

and

γ

> 0,

initial

vectors

x

0

,

h01

,

.

.

.

,

h

0 n

,

h0

=

1 n

1: for k = 0, 1, . . . do

n i=1

h0i

2: Sample random uk = 1, with probability m1 0, with probability 1 − m1

only for Variant 1

3: Broadcast xk, uk to all workers

4: for i = 1, . . . , n in parallel do

5:

Pick jik uniformly at random from [m]

m

6:

µki

=

1 m

∇fij (wikj )

j=1

Worker side

7: gik = ∇fijik (xk) − ∇fijik (wikjik ) + µki

8:

∆ˆ ki = Q(gik − hki )

9:

hki +1 = hki + α∆ˆ ki

10:

for j = 1, . . . , m do

11:

wk+1 = xk, if uk = 1

ij

wikj , if uk = 0

uk = 1 12:

wikj+1 =

xk , wikj ,

j = jik j = jik

13:

end for

Variant 1 (L-SVRG): update epoch gradient if Variant 2 (SAGA): update gradient table

14: end for n 15: hk+1 = hk + αn ∆ˆ ki

Gather quantized updates

i=1 n
16: gk = n1 (∆ˆ ki + hki )
i=1
17: xk+1 = xk − γgk

18: end for

with γ and α satisfying

3

γ≤ L

41 + 52ω+35

,

3

n

and for all K ≥ 0

1

α≤

,

ω+1

8(ω + 1) M1 = 3nα ,

E f (x¯K ) − f (x∗) ≤

1 − min

γµ α 1 ,,

2 4 4m

when µ > 0 and

E

f (x¯K ) − f (x∗)

4T 0 ≤

γK

when

µ

=

0,

where

Tk

def
=

xk − x∗ 2 + M1γ2σ12,k + M2γ2σ22,k.

8ωm 32m M2 = 3n + 9
K 4T 0 ,
γ

In other words, if µ > 0 and

3

γ= L

41 + 52ω+35

,

3

n

then VR-DIANA converges with the linear rate

1

α=

,

ω+1

ω

1

O ω + m + κ 1 + ln

n

ε

to the exact solution which coincides with the rate obtained in [19]. We notice that the framework from [11] establishes slightly worse guarantee:

ω max{m, ω + 1} 1

O ω+m+κ 1+

ln

n

m

ε

46

This guarantee is strictly worse than our bound when m ≤ 1 + ω. The key tool that helps us to improve the rate is two sequences of {σ12,k}k≥0, {σ22,k}k≥0 instead of one sequence {σk2}k≥0 as in [11].

Applying Lemma D.3 we get the complexity result in the case when µ = 0.

Corollary I.3. Let the assumptions of Theorem I.2 hold and µ = 0. Then after K iterations of VR-DIANA with the stepsize

3

γ0 = L 41 + 52ω+35

3

n

x0 − x∗ 2

8(ω + 1)

8ωm 32m

γ = min γ0, M1σ2 + M2σ2 , M1 = 3nα , M2 = 3n + 9

1,0

2,0

and α = ω+1 1 we have E f (x¯K ) − f (x∗) of order

 LR02 1 + ωn
O K

R0 +

(1+ω)2 n

σ12,0

+

K

1 + ωn

 mσ22,0


where R0 = x0 − x∗ 2. That is, to achive E f (x¯K ) − f (x∗) ≤ ε VR-DIANA requires

 LR02 1 + ωn
O ε

R0 +

(1+ω)2 n

σ12,0

+

ε

1 + ωn

 mσ22,0


iterations.

47

Algorithm 3 EC-SGDsr

Input: learning rate γ > 0, initial vector x0 ∈ Rd 1: Set e0i = 0 for all i = 1, . . . , n 2: for k = 0, 1, . . . do

3: Broadcast xk to all workers

4: for i = 1, . . . , n in parallel do

5:

Sample gik = ∇fξi (xk)

6:

vik = C(eki + γgik)

7:

eki +1 = eki + γgik − vik

8: end for

9:

ek = n1

n i=1

eki

,

gk

=

1 n

n i=1

gik

,

vk

=

1 n

10: xk+1 = xk − vk

11: end for

n i=1

vik

J Special Cases: Error Compensated Methods

J.1 EC-SGDsr
In this section we consider the same setup as in Section I.1 and assume additionally that f1, . . . , fn are L-smooth.
Lemma J.1. For all k ≥ 0 we have

1n E gik
n
i=1

1n E
n i=1

gik − g¯ik

2 | xk 2 | xk

E gk 2 | xk

≤ 4L f (xk) − f (x∗) + 2 n ∇fi(x∗) 2, n
i=1

k ∗ 3n

∗

∗2

≤ 6(L + L) f (x ) − f (x ) + n

ED ∇fξi (x ) − ∇fi(x ) ,

i=1

k ∗ 2n

∗

∗2

≤ 4L f (x ) − f (x ) + n2 ED ∇fξi (x ) − ∇fi(x ) .

i=1

Proof. Applying straightforward inequality a + b 2 ≤ 2 a 2 + 2 b 2 for a, b ∈ Rd we get

1 n k2

1n

k

∗

∗2

n

g¯i

= n

∇fi(x ) − ∇fi(x ) + ∇fi(x )

i=1

i=1

(3≤1) 1 n ∇fi(xk) − ∇fi(x∗) 2 + 2 n ∇fi(x∗) 2

n

n

i=1

i=1

(1≤1) 4L f (xk) − f (x∗) + 2 n ∇fi(x∗) 2. n
i=1

48

(97)

Similarly we obtain

1n E
n i=1

gik − g¯ik 2 | xk

=
(31)
≤
(11),(89)
≤

1n n ED
i=1

∇fξi (xk) − ∇fi(xk) 2

3n n ED
i=1

∇fξi (xk) − ∇fξi (x∗) 2

3n + n ED
i=1

∇fξi (x∗) − ∇fi(x∗) 2

+ 3 n ∇fi(x∗) − ∇fi(xk) 2 n
i=1

6(L + L) f (xk) − f (x∗)

3n + n ED
i=1

∇fξi (x∗) − ∇fi(x∗) 2 .

Next,

using

the

independence

of

ξ

k 1

,

.

.

.

,

ξ

k n

we

derive

E gk 2 | xk


n

2

1 = E
n

∇fξk (xk) − ∇fξk (x∗) + ∇fξk (x∗) − ∇fi(x∗)

i

i

i

| xk

i=1

(31) 2 n

2

≤

E ∇fξk (xk) − ∇fξk (x∗) | xk

n

i

i

i=1

 +2E 

1n n i=1

∇fξk (x∗) − ∇fi(x∗) i

2 | xk

(89)

2n

≤ 4L f (xk) − f (x∗) +

ED

n2

i

i=1

∇fξi (x∗) − ∇fi(x∗) 2 .

Applying Theorem G.1 we get the following result.
Theorem J.1. Assume that f (x) is µ-quasi strongly convex, f1, . . . , fn are L-smooth and Assumption I.1 holds. Then EC-SGDsr satisﬁes Assumption 3.3 with

A = 2L, A = 3(L + L), A = 2L, B1 = B1 = B1 = B2 = B2 = B2 = 0,

2n D1 = n
i=1

∇fi(x∗) 2,

3n D1 = n ED
i=1

∇fξi (x∗) − ∇fi(x∗) 2 ,

σ12,k ≡ σ22,k ≡ 0,

2n D1 = n2 ED
i=1

∇fξi (x∗) − ∇fi(x∗) 2 ,

ρ1 = ρ2 = 1,

C1 = C2 = 0,

G = 0,

D2 = 0,

F1 = F2 = 0,

6Lγ D3 = δ

2D1 + D1 , δ

with γ satisfying

1

δ

γ ≤ min ,

8L 4 6L (4L + 3δ(L + L))

and for all K ≥ 0

E f (x¯K ) − f (x∗) ≤ 1 − γ2µ K 4 x0 −γ x∗ 2 + 4γ D1 + 12δL2 γ D1 + 6Lδ γ D1

49

when µ > 0 and

E f (x¯K ) − f (x∗) ≤ 4 x0K−γx∗ 2 + 4γ D1 + 12δL2 γ D1 + 6Lδ γ D1

when µ = 0.

In other words, EC-SGDsr converges with linear rate O

√
Lµ + L+µδδLL ln 1ε to the neigh-

bourhood of the solution when µ > 0. Applying Lemma D.2 we establish the rate of

convergence to ε-solution.

Corollary J.1. Let the assumptions of Theorem J.1 hold and µ > 0. Then after K iterations of EC-SGDsr with the stepsize







ln max 2, min x0−x∗ 2µ2K2 , δ x0−x∗ 2µ3K3



 

1

δ

D1



6L(2D1/δ+D1 )



γ = min ,

,

 8L 4 6L (4L + 3δ(L + L))

µK 









we have E f (x¯K ) − f (x∗) of order

√

O

L + δLL L+

x0 − x∗ 2 exp −

µ
√

K + D1 + L(D1 + D1/δ) .

δ L + L+ δδLL µK δµ2K2

That is, to achive E f (x¯K ) − f (x∗) ≤ ε EC-SGDsr requires



√

O  L + L + δLL + D1 +

µ

δµ

µε



L(D1 + D1/δ)

√

 iterations.

µ δε

Applying Lemma D.3 we get the complexity result in the case when µ = 0.
Corollary J.2. Let the assumptions of Theorem J.1 hold and µ = 0. Then after K iterations of EC-SGDsr with the stepsize

1

δ

γ0 = min 8L , 4 6L (4L + 3δ(L + L))

γ = min γ0,

x0 − x∗ 2

x0 − x∗ 2δ

,3

D1K

6L(2D1/δ + D1)K

we have E f (x¯K ) − f (x∗) of order

√

 R02 L + L+ δδLL

O

+

K



R2D

3 LR04(2D1/δ + D1)

0 1+



K

(δ K 2 )1/3

where R0 =

x0 − x∗ 2. That is, to achive E f (x¯K ) − f (x∗) ≤ ε EC-SGDsr requires

 R02 O

√
L + L+ δδLL ε

+ R02D1 + R02 ε2



L(2D1/δ + D1)

√



δε3

iterations.

J.2 EC-SGD
In this section we consider problem (1) with fi(x) satisfying (2) where functions fξi (x) are diﬀerentiable and L-smooth almost surely in ξi, i = 1, . . . , n.

50

Algorithm 4 EC-SGD

Input: learning rate γ > 0, initial vector x0 ∈ Rd 1: Set e0i = 0 for all i = 1, . . . , n 2: for k = 0, 1, . . . do

3: Broadcast xk to all workers

4: for i = 1, . . . , n in parallel do

5:

Sample gik = ∇fξi (xk) independently from other

6:

vik = C(eki + γgik)

7:

eki +1 = eki + γgik − vik

8: end for

9:

ek = n1

n i=1

eki

,

gk

=

1 n

n i=1

gik

,

vk

=

1 n

n i=1

vik

10: xk+1 = xk − vk

11: end for

workers

Lemma J.2 (See also Lemmas 1,2 from [39]). Assume that fξi (x) are convex in x for every ξi, i = 1, . . . , n. Then for every x ∈ Rd and i = 1, . . . , n

1 n ∇fi(x) 2 ≤ 4L (f (x) − f (x∗)) + 2 n ∇fi(x∗) 2,

n

n

i=1

i=1

1n

2

∗ 3n

∗

∗2

n

Eξi∼Di ∇fξi (x) − ∇fi(x)

≤ 12L (f (x) − f (x )) + n

E ∇fξi (x ) − ∇fi(x ) ,

i=1

i=1

n

2

1 Eξ ,...,ξ

∇fξ (x)

≤ 4L (f (x) − f (x∗)) + 2

1 nn

i

n2

i=1

E ∇fξi (x∗) − ∇fi(x∗) 2 .

If further f (x) is µ-strongly convex with µ > 0 and possibly non-convex fi, fξi , then for every x ∈ Rd and i = 1, . . . , n

1 n ∇fi(x) 2 ≤ 4Lκ (f (x) − f (x∗)) + 2 n ∇fi(x∗) 2,

n

n

i=1

i=1

1n

2

∗ 3n

∗

∗2

n

Eξi∼Di ∇fξi (x) − ∇fi(x)

≤ 12Lκ (f (x) − f (x )) + n

E ∇fξi (x ) − ∇fi(x ) ,

i=1

i=1

n

2

n

1 Eξ ,...,ξ

∇fξ (x)

≤ 4Lκ (f (x) − f (x∗)) + 2

E ∇fξ (x∗) − ∇fi(x∗) 2 .

1 nn

i

n2

i

i=1

i=1

where κ = Lµ .

Proof. We start with the case when functions fξi (x) are convex in x for every ξi. The ﬁrst inequality follows from (97). Next, we derive

1n

2 (31) 3 n

∗2

n Eξi∼Di ∇fξi (x) − ∇fi(x)

≤ n

Eξi∼Di ∇fξi (x) − ∇fξi (x )

i=1

i=1

3n

∗

∗2

+ n

Eξi∼Di ∇fξi (x ) − ∇fi(x )

i=1

+ 3 n ∇fi(x∗) − ∇fi(x) 2 n
i=1

(11)

∗ 3n

∗2

≤ 12L (f (x) − f (x )) + n

E ∇fξi (x ) .

i=1

51

Due

to

independence

of

ξ

k 1

,

.

.

.

,

ξ

k n

we

get

n

2

n

2

1 Eξ ,...,ξ

∇fξ (x)

1 = Eξ ,...,ξ

(∇fξ (x) − ∇fξ (x∗) + ∇fξ (x∗) − ∇fi(x∗))

1 nn

i

1 nn

i

i

i

i=1

i=1

(31) 2 n

∗2

≤ n

Eξi∼Di ∇fξi (x) − ∇fξi (x )

i=1

+2Eξ1,...,ξn

n

2

1 (∇fξ (x∗) − ∇fi(x∗))

n

i

i=1

(11)
≤

4L (f (x) − f (x∗)) + 2

n2

E ∇fξi (x∗) − ∇fi(x∗) 2 .

Next, we consider the second case: f (x) is µ-strongly convex with possibly non-convex fi, fξi . In this case

1 n ∇fi(x) 2 (3≤1) 2 n ∇fi(x) − ∇fi(x∗) 2 + 2 n ∇fi(x∗) 2

n

n

n

i=1

i=1

i=1

(1≤0) 2L2 x − x∗ 2 + 2 n ∇fi(x∗) 2 n
i=1

≤ 4L2 (f (x) − f (x∗)) + 2 n ∇fi(x∗) 2

µ

n

i=1

where the last inequality follows from µ-strong convexity of f . Similarly, we get

1n

n

Eξi ∼Di

i=1

∇fξi (x) − ∇fi(x) 2

(31) 3 n

∗2

≤ n

Eξi∼Di ∇fξi (x) − ∇fξi (x )

i=1

3n

+ n

Eξi ∼Di

i=1

∇fξi (x∗) − ∇fi(x∗) 2

+ 3 n ∇fi(x∗) − ∇fi(x) 2 n
i=1

(10)
≤ 6L2 x − x∗ 2

3n

+ n

Eξi ∼Di

i=1

∇fξi (x∗) − ∇fi(x∗) 2

≤ 12L2 (f (x) − f (x∗)) µ

3n

+ n

Eξi ∼Di

i=1

∇fξi (x∗) − ∇fi(x∗) 2 .

52

Finally,

using

independence

of

ξ

k 1

,

.

.

.

,

ξ

k n

we

derive

n

2

n

2

1 Eξ ,...,ξ

∇fξ (x)

1 = Eξ ,...,ξ

(∇fξ (x) − ∇fξ (x∗) + ∇fξ (x∗) − ∇fi(x∗))

1 nn

i

1 nn

i

i

i

i=1

i=1

(31) 2 n

∗2

≤ n

Eξi∼Di ∇fξi (x) − ∇fξi (x )

i=1

+2Eξ1,...,ξn

n

2

1 (∇fξ (x∗) − ∇fi(x∗))

n

i

i=1

(10)
≤

2L2 x − x∗ 2 + 2

n2

E ∇fξi (x∗) − ∇fi(x∗) 2

≤ 4L2 (f (x) − f (x∗)) + 2

µ

n2

E ∇fξi (x∗) − ∇fi(x∗) 2 .

Applying Theorem G.1 we get the following result.
Theorem J.2. Assume that fξ(x) is convex and L-smooth in x for every ξ and f (x) is µ-quasi strongly convex. Then EC-SGD satisﬁes Assumption 3.3 with

A = A = 2L, A = 6L, B1 = B1 = B1 = B2 = B2 = B2 = 0,

2n D1 = n
i=1

∇fi(x∗) 2,

2n D1 = n E
i=1

∇fξi (x∗) − ∇fi(x∗) 2 ,

σ12,k ≡ σ22,k ≡ 0,

2n D1 = n2 E
i=1

∇fξi (x∗) − ∇fi(x∗) 2 ,

ρ1 = ρ2 = 1,

C1 = C2 = 0,

G = 0,

D2 = 0,

F1 = F2 = 0,

6Lγ D3 = δ

2D1 + D1 , δ

with γ satisfying
and for all K ≥ 0 E f (x¯K ) − f (x∗) ≤

γ ≤ √δ 8L 6 + 9δ

γµ K 4 x0 − x∗ 2

1−

+ 4γ

2

γ

12Lγ

6Lγ

D1 + δ2 D1 + δ D1

when µ > 0 and

E

f (x¯K ) − f (x∗)

4 x0 − x∗ ≤

2
+ 4γ

Kγ

12Lγ

6Lγ

D1 + δ2 D1 + δ D1

when µ = 0. If further f (x) is µ-strongly convex with µ > 0 and possibly non-convex fi, fξi , then EC-SGD satisﬁes Assumption 3.3 with

A = A = 2Lκ, A = 6Lκ, B1 = B1 = B1 = B2 = B2 = B2 = 0,

2n D1 = n
i=1

∇fi(x∗) 2,

2n D1 = n E
i=1

∇fξi (x∗) − ∇fi(x∗) 2 ,

σ12,k ≡ σ22,k ≡ 0,

2n D1 = n2 E
i=1

∇fξi (x∗) − ∇fi(x∗) 2 ,

ρ1 = ρ2 = 1,

C1 = C2 = 0,

G = 0,

D2 = 0,

F1 = F2 = 0,

6Lγ D3 = δ

2D1 + D1 , δ

53

with γ satisfying

1

δ

γ ≤ min

,

8κL 8L 3κ(2 + 3δ)

and for all K ≥ 0

E f (x¯K ) − f (x∗) ≤ 1 − γ2µ K 4 x0 −γ x∗ 2 + 4γ D1 + 12δL2 γ D1 + 6Lδ γ D1 .

In other words, EC-SGD converges with linear rate O κδ ln 1ε to the neighbourhood of the solution when fξ(x) are convex for each ξ and µ > 0. Applying Lemma D.2 we establish the
rate of convergence to ε-solution.

Corollary J.3. Let the assumptions of Theorem J.2 hold, fξ(x) are convex for each ξ and µ > 0. Then after K iterations of EC-SGD with the stepsize







 

δ

ln max 2, min x0−x∗ 2µ2K2 , δ x0−x∗ 2µ3K3



D1



6L(2D1/δ+D1 )



γ = min √

,

 8L 6 + 9δ

µK 









we have

E f (x¯K ) − f (x∗) = O

L x0 − x∗ 2 exp

δµ −K

+ D1 + L(D1 + D1/δ)

.

δ

L

µK

δ µ2 K 2

That is, to achive E f (x¯K ) − f (x∗) ≤ ε EC-SGD requires

 O  L + D1 +
δµ µε



L(D1 + D1/δ)

√

 iterations.

µ δε

Corollary J.4. Let the assumptions of Theorem J.2 hold and f (x) is µ-strongly convex with µ > 0 and possibly non-convex fi, fξi . Then after K iterations of EC-SGD with the stepsize







ln max 2, min x0−x∗ 2µ2K2 , δ x0−x∗ 2µ3K3



 

1

δ

D1



6L(2D1/δ+D1 )



γ = min

,

,

 8κL 8L 3κ(2 + 3δ)

µK 









we have E f (x¯K ) − f (x∗) of order

√

O

Lκ Lκ +

x0 − x∗ 2 exp − min

δµ 1 √,

K + D1 + L(D1 + D1/δ) .

δ

L κ κ2

µK

δ µ2 K 2

That is, to achive E f (x¯K ) − f (x∗) ≤ ε EC-SGD requires

 O κ2 + κ3/2 + D1 +
δ µε



L(D1 + D1/δ)

√

 iterations.

µ δε

Applying Lemma D.3 we get the complexity result in the case when µ = 0.
Corollary J.5. Let the assumptions of Theorem J.2 hold, fξ(x) are convex for each ξ and µ = 0. Then after K iterations of EC-SGD with the stepsize

γ = min √ δ , 8L 6 + 9δ
we have E f (x¯K ) − f (x∗) of order

x0 − x∗ 2

x0 − x∗ 2δ

,3

D1K

6L(2D1/δ + D1)K

 LR02
O + δK



R2D

3 LR04(2D1/δ + D1)

0 1+



K

(δ K 2 )1/3

54

Algorithm 5 EC-GDstar (see also [11])

Input: learning rate γ > 0, initial vector x0 ∈ Rd 1: Set e0i = 0 for all i = 1, . . . , n 2: for k = 0, 1, . . . do

3: Broadcast xk to all workers

4: for i = 1, . . . , n in parallel do

5:

gik = ∇fi(xk) − ∇fi(x∗)

6:

vik = C(eki + γgik)

7:

eki +1 = eki + γgik − vik

8: end for

9:

ek = n1

n i=1

eki

,

gk

=

1 n

n i=1

gik

,

vk

=

1 n

10: xk+1 = xk − vk

11: end for

n i=1

vik

where R0 = x0 − x∗ 2. That is, to achive E f (x¯K ) − f (x∗) ≤ ε EC-SGD requires



LR02 R02D1 R02

O +

+

δε

ε2



L(2D1/δ + D1)

√



δε3

iterations.

J.3 EC-GDstar

We assume that i-th node has access to the gradient of fi at the optimality, i.e., to the ∇fi(x∗). It is unrealistic scenario but it gives some insights that we will use next in order to design the method that converges asymptotically to the exact solution.

Assume that f (x) is µ-quasi strongly convex and each fi is convex and L-smooth. By deﬁnition of gik it trivially follows that

1n

1n

gk =

gik =

∇fi(xk) − ∇fi(x∗) = ∇f (xk) − ∇f (x∗) = ∇f (xk),

n

n

i=1

i=1

gik = g¯ik, and

1 n k2

1n

k

∗2

n

gi

= n

∇fi(x ) − ∇fi(x )

i=1

i=1

(11) 2L n

≤

fi(xk) − fi(x∗) − ∇fi(x∗), xk − x∗ = 2L f (xk) − f (x∗) ,

n

i=1

gk 2 =

(11)
∇f (xk) 2 ≤ 2L f (xk) − f (x∗) .

Applying Theorem G.1 we get the following result.
Theorem J.3. Assume that fi(x) is convex and L-smooth for all i = 1, . . . , n and f (x) is µ-quasi strongly convex. Then EC-GDstar satisﬁes Assumption 3.3 with

A = A = L, A = 0, B1 = B2 = B1 = B2 = B1 = B2 = 0,

D1 = D1 = D1 = 0, σ12,k ≡ σ22,k ≡ 0,

ρ1 = ρ2 = 1, C1 = C2 = 0, G = 0, D2 = 0, F1 = F2 = 0, D3 = 0,

with γ satisfying and for all K ≥ 0

δ γ≤ √
8L 3

E f (x¯K ) − f (x∗)

≤

γµ 1−

K 4 x0 − x∗ 2 ,

2

γ

55

Algorithm 6 EC-SGD-DIANA

Input: learning rates γ > 0, α ∈ (0, 1], initial vectors

1: Set e0i = 0 for all i = 1, . . . , n

2: Set h0 = n1

n i=1

h0i

3: for k = 0, 1, . . . do

4: Broadcast xk, hk to all workers

5: for i = 1, . . . , n in parallel do

6:

Sample gˆik such that E[gˆik | xk] = ∇fi(xk)

independently from other workers

7:

gik = gˆik − hki + hk

8:

vik = C(eki + γgik)

9:

eki +1 = eki + γgik − vik

10:

hki +1 = hki + αQ(gˆik − hki )

11: end for

12:

ek = n1

n i=1

eki

,

gk

=

1 n

n i=1

gik

,

vk

=

1 n

n
α n1 Q(gˆik − hki )
i=1
13: xk+1 = xk − vk
14: end for

x

0

,

h01

,

.

.

.

,

h

0 n

and E gˆik −

n i=1

vik

,

hk+1

∈ Rd ∇fi(xk) 2 | xk
n
= n1 hki +1
i=1

≤ D1,i = hk +

when µ > 0 and

E

f (x¯K ) − f (x∗)

4 x0 − x∗ ≤

2

Kγ

when µ = 0.

In other words, EC-GDstar converges with linear rate O κδ ln 1ε to the exact solution when µ > 0 removing the drawback of EC-SGD and EC-GD. If µ = 0 then the rate of convergence is O L x0δ−εx∗ 2 . However, EC-GDstar relies on the fact that i-th node knows ∇fi(x∗) which
is not realistic.

J.4 EC-SGD-DIANA

In this section we present a new method that converges to the exact optimum asymptotically but does not need to know ∇fi(x∗) and instead of this it learns the gradients at the optimum. This method is inspired by another method called DIANA (see [37, 19]).
We notice that master needs to gather only C(eki + γgik) and Q(gˆik − hki ) from all nodes in order to perform an update.
Lemma J.3. Assume that fi(x) is convex and L-smooth for all i = 1, . . . , n. Then, for all k ≥ 0 we have

where

D1

=

1 n

E gk | xk = ∇f (xk),

1 n k2

k

∗

2

n g¯i ≤ 4L f (x ) − f (x ) + 2σk,

i=1

1n E
n i=1

gik − g¯ik 2 | xk

≤ D1,

E gk 2 | xk

≤ 2L f (xk) − f (x∗) + D1 n

n i=1

D1,i

and

σk2

=

1 n

n i=1

hki − ∇f (x∗) 2.

(98) (99) (100) (101)

56

Proof. First of all, we show unbiasedness of gk:

E gk | xk

1n

=

E gik | xk

n

i=1

1n =
n i=1

∇fi(xk) − hki + hk

= ∇f (xk).

Next, we derive the upper bound for g¯ik 2:

g¯ik 2 = ∇fi(xk) − hki − hk 2 (3≤1) 2 ∇fi(xk) − ∇fi(x∗) 2 + 2 hki − ∇fi(x∗) − hk + ∇f (x∗) 2

(11)
≤ 4L fi(xk) − ∇fi(x∗) − ∇fi(x∗), xk − x∗

+2 hki − ∇fi(x∗) − hk + ∇f (x∗) 2 .

Summing up previous inequality for i = 1, . . . , n we get

n

n

n

2

1

g¯k 2 ≤ 4L(f (xk) − f (x∗)) + 2

hk − ∇fi(x∗) − 1 (hk − ∇fi(x∗))

n

i

n

i

n

i

i=1

i=1

i=1

(34)

k ∗ 2n k

∗2

≤ 4L f (x ) − f (x ) + n

hi − ∇f (x ) .

i=1

(102)

Using the unbiasedness of gˆik we derive

1n E
n i=1

gik − g¯ik 2 | xk

1n

=

E

n i=1

gˆik − ∇fi(xk) 2 | xk

1n

≤ n

D1,i = D1.

i=1

Finally, we obtain the upper bound for the second moment of gk using the independence of gˆ1k, . . . , gˆnk:

E gk 2 | xk

(34)
=

∇f (xk) 2 + E gk − ∇f (xk) 2


n

2

(11)
≤ 2L(f (xk) − f (x∗)) + E

1

(gˆk − ∇f (xk)) | xk

 n

i

i



i=1

1n

2

= 2L(f (xk) − f (x∗)) + n2 E gˆik − ∇fi(xk) | xk

i=1

1n

≤ 2L(f (xk) − f (x∗)) +

D1,i.

n2

i=1

Lemma J.4. Let assumptions of Lemma J.3 hold and α ≤ 1/(ω+1). Then, for all k ≥ 0 we

have

E σk2+1 | xk ≤ (1 − α)σk2 + 2Lα(f (xk) − f (x∗)) + α2(ω + 1)D1,

(103)

where

σk2

=

1 n

n i=1

hki − ∇fi(x∗)

2

and

D1

=

1 n

n i=1

D1,i.

Proof. For simplicity, we introduce new notation: h∗i d=ef ∇fi(x∗). Using this we derive an upper bound for the second moment of hki +1 − h∗i :

E hki +1 − h∗i 2 | xk

= E hk − h∗ + αQ(gˆk − hk) 2 | xk

i

i

i

i

(26)
=
(26),(35)
≤

hki − h∗i 2 + 2α hki − h∗i , ∇fi(xk) − hki +α2E Q(gˆik − hki ) 2 | xk
hki − h∗i 2 + 2α hki − h∗i , ∇fi(xk) − hki +α2(ω + 1)E gˆik − hki 2 | xk .

57

Using variance decomposition (34) and α ≤ 1/(ω+1) we get

α2(ω + 1)E gˆik − hki 2 | xk Putting all together we obtain

(34)
=

α2(ω + 1)E

gˆik − ∇fi(xk) 2 | xk

+α2(ω + 1) ∇fi(xk) − hki 2

≤ α2(ω + 1)D1,i + α ∇fi(xk) − hki 2.

E hki +1 − h∗i 2 | xk

≤ hki − h∗i 2 + α ∇fi(xk) − hki , fi(xk) + hki − 2h∗i + α2(ω + 1)D1,i

(28)
=

hki − h∗i 2 + α ∇fi(xk) − h∗i 2 − α hki − h∗i 2 + α2(ω + 1)D1,i

(11)
≤ (1 − α) hki − h∗i 2 + 2Lα fi(xk) − fi(x∗) − ∇fi(x∗), xk − x∗

+α2(ω + 1)D1,i.

Summing up the above inequality for i = 1, . . . , n we derive

1n E
n i=1

hki +1 − h∗i 2 | xk

1−α n ≤
n i=1

k ∗2

k ∗ α2(ω + 1) n

hi −hi +2Lα(f (x )−f (x ))+ n

D1,i.

i=1

Applying Theorem G.1 we get the following result.

Theorem J.4. Assume that fi(x) is convex and L-smooth for all i = 1, . . . , n and f (x) is µ-quasi strongly convex. Then EC-SGD-DIANA satisﬁes Assumption 3.3 with

A = 2L,

A = 0,

A = L,

B1 = 2,

1n D1 = n D1,i,
i=1

2

2 1n

σ1,k = σk = n

i=1

hki − ∇fi(x∗) 2,

B1 = B2 = B2 = B1 = B2 = 0, σ22,k ≡ 0, ρ1 = α, ρ2 = 1, C1 = Lα, C2 = 0, D1 = 0,

D2 = α2(ω + 1)D1,

D1

D1 =

, n

G = 0,

96Lγ2

6Lγ 4α(ω + 1)

F1 = δ2α 1 − min γµ , α , F2 = 0, D3 = δ

δ + 1 D1,

24

with γ and α satisfying

√

1 δ 1−α

1

γ ≤ min 4L , 8L 6(3 − α) , α ≤ ω + 1 , M1 = M2 = 0

and for all K ≥ 0

E f (x¯K ) − f (x∗) ≤

1 − min

γµ α ,

24

K 4( x0 − x∗ 2 + γF1σ02)

γ

+ 4γ (D1 + D3) ,

when µ > 0 and

E f (x¯K ) − f (x∗) ≤ 4( x0 − x∗γK2 + γF1σ02) + 4γ (D1 + D3)

when µ = 0.

In other words, if

√

1 δ 1−α

11

γ = min ,

, α = min

,

4L 8L 6(3 − α)

ω+1 2

and D1 = 0, i.e., gˆik = ∇fi(xk) almost surely (this is the setup of EC-GD-DIANA), EC-SGD-DIANA converges with the linear rate
κ1 O ω + ln
δε
to the exact solution. Applying Lemma D.2 we establish the rate of convergence to ε-solution in the case when µ > 0.

58

Corollary J.6. Let the assumptions of Theorem J.4 hold and µ > 0. Then after K iterations

of EC-SGD-DIANA with the stepsize

√

1 δ 1−α γ = min ,

,

R = x0 − x∗ ,

F˜

=

784Lγ2 ,

0

4L 8L 6(3 − α)

0

1 7δ2α

 

ln max 2, min ( ) n R02+F˜1γ0σ12,0 µ2K2 , ( ) δ R02+F˜1γ0σ12,0 µ3K3

 







D1

6LD1 (4α(ω+1)/δ+1)



γ = min γ0, µK ,













and α ≤ ω+1 1 we have

E f (x¯K ) − f (x∗) = O Lδ R02 exp − min δLµ , α K + nDµK1 + LD1 (αδ(µω2+K1)2/δ + 1) .

That is, to achive E f (x¯K ) − f (x∗) ≤ ε EC-SGD-DIANA requires

 O  1 + L + D1 +
α δµ nµε



LD1 (α(ω+1)/δ + 1)

√

 iterations.

µ δε

In particular, if α = ω+1 1 , then to achive E f (x¯K ) − f (x∗) ≤ ε EC-SGD-DIANA requires





L D1

LD1

O ω + + + √  iterations,

δµ nµε δµ ε

and if α = ω+δ 1 , then to achive E f (x¯K ) − f (x∗) ≤ ε EC-SGD-DIANA requires





ω + 1 L D1

LD1

O

+ + + √  iterations.

δ δµ nµε µ δε

Applying Lemma D.3 we get the complexity result in the case when µ = 0.

Corollary J.7. Let the assumptions of Theorem J.4 hold and µ = 0. Then after K iterations

of EC-SGD-DIANA with the stepsize

√

1 δ 1−α

γ0 = min

,

, R0 = x0 − x∗ ,

4L 8L 6(3 − α)





 

3 R02δ2α 1 − min γ02µ , α4

nR02

δR02

 

γ = min γ0,
 

96Lσ02

,

,3

,

D1K 6LD1 4α(ωδ+1) + 1 K 

and α ≤ ω+1 1 we have E f (x¯K ) − f (x∗) of order

 O  LR02 + 3 L√R04σ02 +
 δK K 3 δ2α



R2D1 3 LR04D1 α(ωδ+1) + 1

0+

 .

nK

δK2



That is, to achive E f (x¯K ) − f (x∗) ≤ ε EC-SGD-DIANA requires



LR2 3 LR4σ2 R2D R02

 O

0+

√ 0 0+ 0 1+

 δε

ε 3 δ2α

nε2



LD1 α(ωδ+1) + 1

√

 

δε3



59

Algorithm 7 EC-SGDsr-DIANA

Input:

learning

rates

γ

> 0,

α ∈ (0, 1],

initial

vectors

x

0

,

h01

,

.

.

.

,

h

0 n

∈ Rd

1: Set e0i = 0 for all i = 1, . . . , n

2: Set h0 = n1

n i=1

h0i

3: for k = 0, 1, . . . do

4: Broadcast xk, hk to all workers

5: for i = 1, . . . , n in parallel do

6:

Sample gˆik = ∇fξk (xk) satisfying Assumption I.1 independtently from other

i

workers

7:

gik = gˆik − hki + hk

8:

vik = C(eki + γgik)

9:

eki +1 = eki + γgik − vik

10:

hki +1 = hki + αQ(gˆik − hki ) Q(·) is calculated independtly from other workers

11: end for n

12:

ek = n1

n i=1

eki

,

gk

=

1 n

n i=1

gik

,

vk

=

1 n

n i=1

vik

,

hk+1

=

1 n

hki +1 = hk +

i=1

n

α n1 Q(gˆik − hki )

i=1

13: xk+1 = xk − vk

14: end for

iterations. In particular, if α = ω+1 1 , then to achive E f (x¯K ) − f (x∗) ≤ ε EC-SGD-DIANA requires





LR02 3 LR04(ω + 1)σ02 R02D1 R02 LD1

O +

√

+

+ √  iterations,

δε

ε 3 δ2

nε2

δ ε3

and if α = ω+δ 1 , then to achive E f (x¯K ) − f (x∗) ≤ ε EC-SGD-DIANA requires





LR02 3 LR04(ω + 1)σ02 R02D1 R02 LD1 O  δε + δε + nε2 + √δε3  iterations.

J.5 EC-SGDsr-DIANA

In this section we consider the same setup as in Section I.1 and consider EC-SGD-DIANA adjusted to this setup. The resulting algorithm is called EC-SGDsr-DIANA, see

Lemma J.5. Let Assumption I.1 be satisﬁed and fi be convex and L-smooth for all i ∈ [n]. Then, for all k ≥ 0 we have

E gk | xk = ∇f (xk),

(104)

1 n k2

k

∗

2

n g¯i ≤ 4L f (x ) − f (x ) + 2σk,

i=1

1n E
n i=1

gik − g¯ik 2 | xk

≤ 6(L + L) f (xk) − f (x∗) + D1,

(105) (106)

E gk 2 | xk ≤ 4L f (xk) − f (x∗) + D1

(107)

where

σk2

=

1 n

n i=1

hki − ∇f (x∗)

2,

D1

=

3 n

n

D1

=

2 n2

EDi ∇fξi (x∗) − ∇fi(x∗) 2 .

i=1

n i=1

EDi

∇fξi (x∗) − ∇fi(x∗) 2 and

Proof. First of all, we show unbiasedness of gk:

E gk | xk

1n

=

E gik | xk

n

i=1

1n =
n i=1

∇fi(xk) − hki + hk

= ∇f (xk).

60

Following the same steps as in the proof of (102) we derive (105). Next, we establish (106):

1n E
n i=1

gik − g¯ik 2 | xk

1n

k

k2

=

n

EDi ∇fξk (x ) − ∇fi(x ) i

i=1

(31)

3n

k

∗2

≤

n

EDi ∇fξk (x ) − ∇fξk (x )

i

i

i=1

3n

+ n

EDi

i=1

∇fξk (x∗) − ∇fi(x∗) 2 i

+ 3 n ∇fi(x∗) − ∇fi(xk) 2 n
i=1

(11),(89)
≤ 6(L + L) f (xk) − f (x∗)

3n

+ n

EDi

i=1

∇fξi (x∗) − ∇fi(x∗) 2 .

Finally, we obtain the upper bound for the second moment of gk using the independence of

ξ

k 1

,

.

.

.

,

ξ

k n

:

E gk 2 | xk


n

2

1 = E
n

(∇fξk (xk) − ∇fξk (x∗) + ∇fξk (x∗) − ∇fi(x∗)) | xk

i

i

i

i=1

(31) 2 n

k

∗2 k

≤ n

E ∇fξk (x ) − ∇fξk (x ) | x

i

i

i=1


n

2

1 +2E 
n

(∇fξk (x∗) − ∇fi(x∗)) | xk i

i=1

(89)

2n

≤ 4L f (xk) − f (x∗) +

ED

n2

i

i=1

∇fξi (x∗) − ∇fi(x∗) 2 .

Lemma J.6. Let fi be convex and L-smooth, Assumption I.1 holds and α ≤ 1/(ω+1). Then, for all k ≥ 0 we have

E σk2+1 | xk ≤ (1 − α)σk2 + 2α(3L + 4L)(f (xk) − f (x∗)) + D2,

(108)

where

σk2

=

1 n

n i=1

hki − ∇fi(x∗) 2 and D2 = α2(ω + 1)D1.

Proof. The proof is identical to the proof of Lemma I.2 up to the following changes in the notation: ω1 = ω, ∆ki = Q(gˆik − hki ) and ∆ˆ ki = gˆik − hki .

Applying Theorem G.1 we get the following result.

Theorem J.5. Assume that fi(x) is convex and L-smooth for all i = 1, . . . , n, f (x) is µ-quasi

strongly convex and Assumption I.1 holds. Then EC-SGDsr-DIANA satisﬁes Assumption 3.3

with

A = 2L,

A = 3(L + L),

A = 2L,

B1 = 2,

3n D1 = n EDi
i=1

∇fξi (x∗) − ∇fi(x∗) 2 ,

2

2 1n

σ1,k = σk = n

i=1

hki − ∇fi(x∗) 2,

D1 = 0,

2 D1 = 3n D1,

D2 = α2(ω + 1)D1

B1 = B1 = B2 = B2 = B2 = 0, σ22,k ≡ 0, ρ1 = α, ρ2 = 1, C1 = 2α(3L + 4L), C2 = 0,

96Lγ2

6Lγ 4α(ω + 1)

G = 0, F1 = δ2α 1 − min γµ , α , F2 = 0, D3 = δ

δ + 1 D1,

24

61

with γ and α satisfying









 

1

δ

 

γ ≤ min ,

,

 4L 4 6L 4L + 3δ(L + L) + 16(31L−+α4L) 

1

α≤

,

ω+1

M1 = M2 = 0.

and for all K ≥ 0

E f (x¯K ) − f (x∗) ≤

1 − min

γµ α ,

24

K 4( x0 − x∗ 2 + γF1σ02)

γ

+ 4γ (D1 + D3) ,

when µ > 0 and

E f (x¯K ) − f (x∗) ≤ 4( x0 − x∗γK2 + γF1σ02) + 4γ (D1 + D3)

when µ = 0.

Applying Lemma D.2 we establish the rate of convergence to ε-solution in the case when µ > 0.
Corollary J.8. Let the assumptions of Theorem J.5 hold and µ > 0. Then after K iterations of EC-SGDsr-DIANA with the stepsize









 

1

δ

 

γ0 = min 4L , 

, 16(3L+4L) 

 

4 6L 4L + 3δ(L + L) + 1−α

 

R0 =

x0 − x∗ , F˜ =

96Lγ02 ,

1 δ2α 1 − min γ02µ , α4

 

ln max 2, min ( ) 3n R02+F˜1γ0σ12,0 µ2K2 , ( ) δ R02+F˜1γ0σ12,0 µ3K3

 

 
γ = min γ0,

2D1

6LD1 4α(ωδ+1) +1

 
,

 µK 









and α ≤ ω+1 1 we have E f (x¯K ) − f (x∗) of order



√

LL

µ

D

LD1 α(ωδ+1) + 1 

O  L + δ R02 exp − min L + √Lδ L , α K + nµK1 + δµ2K2 

That is, to achive E f (x¯K ) − f (x∗) ≤ ε EC-SGDsr-DIANA requires

 √
O  1 + L + LL + D1 +  α µ δµ nµε

LD1 α(ωδ+1) + 1 √
µ δε


  iterations. 

In particular, if α = ω+1 1 , then to achive E f (x¯K ) − f (x∗) ≤ ε EC-SGDsr-DIANA requires



√



L LL D1

LD1

O ω + +

+ + √  iterations,

µ δµ nµε δµ ε

and if α = ω+δ 1 , then to achive E f (x¯K ) − f (x∗) ≤ ε EC-SGDsr-DIANA requires



√



ω + 1 L LL D1

LD1

O

++

+ + √  iterations.

δ µ δµ nµε µ δε

62

Applying Lemma D.3 we get the complexity result in the case when µ = 0.
Corollary J.9. Let the assumptions of Theorem J.5 hold and µ = 0. Then after K iterations of EC-SGDsr-DIANA with the stepsize









 

1

γ0 = min 4L ,

δ  , R0 = x0 − x∗ , 16(3L+4L) 

 4 6L 4L + 3δ(L + L) + 1−α 





 

3 R02δ2α 1 − min γ02µ , α4

3nR02

δR02

 

γ = min γ0,
 

96Lσ02

,

,3

,

2D1K 6LD1 4α(ωδ+1) + 1 K 

and α ≤ ω+1 1 we have E f (x¯K ) − f (x∗) of order



√

O  LR02 + LLR02 + 3 L√R04σ02 +

K

δK

K 3 δ2α



R2D1 3 LR04D1 α(ωδ+1) + 1

0+

 .

nK

δK2



That is, to achive E f (x¯K ) − f (x∗) ≤ ε EC-SGDsr-DIANA requires



LR2

√ LLR2

3 LR4σ2 R2D

R02

 O

0+

0+ √ 0 0+ 0 1+

ε

δε

ε 3 δ2α

nε2



LD1 α(ωδ+1) + 1

√

 

δε3



iterations. In particular, if α = ω+1 1 , then to achive E f (x¯K ) − f (x∗) ≤ ε EC-SGDsr-DIANA requires



√

LR2

LLR2

3 LR4(ω + 1)σ2 R2D

 R02 LD1

O 0 +

0+

0√

0 + 0 1 + √  iterations,

ε

δε

ε 3 δ2

nε2

δ ε3

and if α = ω+δ 1 , then to achive E f (x¯K ) − f (x∗) ≤ ε EC-SGDsr-DIANA requires



√

LR2

LLR2

3 LR4(ω + 1)σ2 R2D

 R02 LD1

O 0 +

0+

0

0+ 0 1+ √

 iterations.

ε

δε

δε

nε2

δε3

J.6 EC-LSVRG

In this section we consider problem (1) with f (x) being µ-quasi strongly convex and fi(x) satisfying (3) where functions fij(x) are convex and L-smooth. For this problem we propose a new method called EC-LSVRG which takes for the origin another method called LSVRG (see
[17, 31]).

Lemma J.7. For all k ≥ 0, i ∈ [n] we have

g¯ik = E gik | xk = ∇fi(xk)

(109)

and

1 n k2

k

∗

n g¯i ≤ 4L f (x ) − f (x ) + D1,

i=1

1n E
n i=1

gik − g¯ik 2 | xk

≤ 12L f (xk) − f (x∗) + 3σk2,

E gk 2 | xk ≤ 4L f (xk) − f (x∗) + 2σk2

where

σk2

=

1 nm

n i=1

n j=1

∇fij (wik) − ∇fij (x∗)

2

and

D1

=

2 n

n i=1

∇fi(x∗) 2.

(110) (111) (112)

63

Algorithm 8 EC-LSVRG

Input: learning rate γ > 0, initial vector x0 ∈ Rd 1: Set e0i = 0 for all i = 1, . . . , n 2: for k = 0, 1, . . . do

3: Broadcast xk to all workers

4: for i = 1, . . . , n in parallel do

5:

Pick l uniformly at random from [m]

6:

Set gik = ∇fil(xk) − ∇fil(wik) + ∇fi(wik)

7:

vik = C(eki + γgik)

8:

eki +1 = eki + γgik − vik

9:

wk+1 = xk, with probability p,

i

wik, with probability 1 − p

10: end for

11:

ek = n1

n i=1

eki

,

gk

=

1 n

n i=1

gik

,

vk

=

1 n

n i=1

vik

12: xk+1 = xk − vk

13: end for

Proof. First of all, we derive unbiasedness of gik:

1m

E gik | xk =

∇fij (xk) − ∇fij (wik) + ∇fi(wik) = ∇fi(xk).

m

j=1

n
Next, we get an upper bound for n1 g¯ik 2:
i=1

1 n k2

1n

k2

n

g¯i

= n

∇fi(x )

i=1

i=1

(3≤1) 2 n ∇fi(xk) − ∇fi(x∗) 2 + 2 n ∇fi(x∗) 2

n

n

i=1

i=1

(1≤1) 4L f (xk) − f (x∗) + 2 n ∇fi(x∗) 2. n
i=1

Using (109) we establish the following inequality:

1n E
n i=1

gik − g¯ik 2 | xk

(31)
≤
(11),(34)
≤

3n E
n i=1

∇fil(xk) − ∇fil(x∗) 2 | xk

3n

+

E

n i=1

∇fil(wik) − ∇fil(x∗) − ∇fi(wik) − ∇fi(x∗) 2 | xk

+ 3 n ∇fi(x∗) − ∇fi(xk) 2 n
i=1

k ∗ 3 nm k

∗2

12L f (x ) − f (x ) + nm

∇fij(wi ) − ∇fij(x ) .

i=1 j=1

64

Finally, we derive (112):

E gk 2 | xk


n

2

1 = E
n

∇fil(xk) − ∇fil(wik) + ∇fi(wik) − ∇fi(x∗) | xk

i=1

(3≤1) 2 n E ∇fil(xk) − ∇fil(x∗) 2 | xk n
i=1

2n

+

E

n i=1

∇fil(wik) − ∇fil(x∗) − ∇fi(wik) − ∇fi(x∗) 2 | xk

= 2 n m ∇fij (xk) − ∇fij (x∗) 2 nm
i=1 j=1

2

2 +

nm

1m

∇fij (wik) − ∇fij (x∗) −

∇fij (wik) − ∇fij (x∗)

nm

m

i=1 j=1

j=1

(11),(34)
≤

4L f (xk) − f (x∗) + 2 n m ∇fij (wik) − ∇fij (x∗) 2 . nm
i=1 j=1

Lemma J.8. For all k ≥ 0, i ∈ [n] we have

where

σk2

=

1 nm

E σk2+1 | xk ≤ (1 − p)σk2 + 2Lp f (xk) − f (x∗) ,

n i=1

n j=1

∇fij (wik) − ∇fij (x∗) 2.

(113)

Proof. By deﬁnition of wik+1 we get

E σk2+1 | xk

1 nm

k+1

∗2 k

= nm

E ∇fij(wi ) − ∇fij(x ) | x

i=1 j=1

1−p n m k

∗2 p nm

= nm

∇fij(wi ) − ∇fij(x )

+ nm

i=1 j=1

i=1 j=1

(11)

2 2Lp n m

k∗

≤ (1 − p)σk + nm

Dfij (x , x )

i=1 j=1

= (1 − p)σk2 + 2Lp f (xk) − f (x∗) .

∇fij (xk) − ∇fij (x∗) 2

Applying Theorem G.1 we get the following result.

Theorem J.6. Assume that f (x) is µ-quasi strongly convex and functions fij are convex and L-smooth for all i ∈ [n], j ∈ [m]. Then EC-LSVRG satisﬁes Assumption 3.3 with

A = 2L,

A = 12L,

A = 2L,

B1 = B1 = B1 = B2 = 0,

2n D1 = n
i=1

∇fi(x∗) 2,

D1 = D1 = 0, B2 = 3, B2 = 2, σ12,k ≡ 0, C1 = 0,

2

2

1 nm

σ2,k = σk = nm

i=1 j=1

∇fij (wik) − ∇fij (x∗) 2,

ρ1 = 1,

ρ2 = p,

C2 = Lp,

D2 = 0,

72Lγ2

12Lγ

G = 0, F1 = 0, F2 = δp 1 − min γµ , p , D3 = δ2 D1,

24

65

with γ satisfying 



 

1

γ ≤ min

,

 24L 8L

δ 3 2 + 3δ 2 + 1−1 p





 

4

, M2 = p .







and for all K ≥ 0

E f (x¯K ) − f (x∗) ≤

1 − min

γµ p ,

24

K 4(T 0 + γF2σ02) 48Lγ2

γ

+ δ2 D1

when µ > 0 and E f (x¯K ) − f (x∗) ≤ 4(T 0 +γKγF2σ02) + 48δL2γ2 D1

when

µ

=

0,

where

Tk

def
=

xk − x∗ 2 + M2γ2σk2.

In other words, EC-LSVRG converges with linear rate O p1 + δ√κ1−p ln 1ε to the neighbourhood of the solution. If m ≥ 2 then taking p = m1 we get that in expectation the sample complexity of one iteration of EC-LSVRG is O(1) gradients calculations per node as for EC-SGDsr with standard sampling and the rate of convergence to the neighbourhood becomes O m + κδ ln 1ε . We notice that the size of this neighbourhood is typically smaller than for EC-SGDsr, but still the method fails to converge to the exact solution with linear rate. Applying Lemma D.2 we establish the rate of convergence to ε-solution in the case when µ > 0.

Corollary J.10. Let the assumptions of Theorem J.6 hold and µ > 0. Then after K iterations of EC-LSVRG with the stepsize









 

1

δ

 

γ0 = min

,

,

 24L 8L 3 2 + 3δ 2 + 1−1 p 

T˜0 =

x0 − x∗ 2 + M γ2σ2, F˜ =

72Lγ02 ,

20 0

2 δp 1 − min γ02µ , p4



δ2 T˜0+F˜2γ0σ02 µ3K3



 ln max 2, 48LD1 

γ = min γ0, µK ,













and p = m1 , m ≥ 2 we have

E f (x¯K ) − f (x∗) = O Lδ T˜0 + F˜2γ0σ02 exp − min δLµ , m1 K + δ2LµD2K1 2 .

That is, to achive E f (x¯K ) − f (x∗) ≤ ε EC-LSVRG requires

√

L O m+ +

L√D1

iterations.

δµ δµ ε

Applying Lemma D.3 we get the complexity result in the case when µ = 0.

Corollary J.11. Let the assumptions of Theorem J.6 hold and µ = 0. Then after K iterations of EC-LSVRG with the stepsize







 

1

δ

γ0 = min

,

 24L 8L 3 2 + 3δ 2 + 1−1 p

  
, R0 = x0 − x∗ ,
  


 γ = min γ0,


R02p 3 R02δp 1 − min γ02µ , p4

4σ2 ,

72Lσ2

0

0



,3

δ2R02

 ,

12LD1K 

66

Algorithm 9 EC-LSVRGstar

Input: learning rate γ > 0, initial vector x0 ∈ Rd 1: Set e0i = 0 for all i = 1, . . . , n 2: for k = 0, 1, . . . do

3: Broadcast xk to all workers

4: for i = 1, . . . , n in parallel do

5:

Pick l uniformly at random from [m]

6:

Set gik = ∇fil(xk) − ∇fil(wik) + ∇fi(wik) − ∇fi(x∗)

7:

vik = C(eki + γgik)

8:

eki +1 = eki + γgik − vik

9:

wk+1 = xk, with probability p,

i

wik, with probability 1 − p

10: end for

11:

ek = n1

n i=1

eki

,

gk

=

1 n

n i=1

gik

,

vk

=

1 n

n i=1

vik

12: xk+1 = xk − vk

13: end for

and p = m1 , m ≥ 2 we have E f (x¯K ) − f (x∗) of order



LR02 O +
δK

mR02σ02 + 3 L√R04mσ02 + 3 LR04 3

K

3 δK

(δ K )2/3

1n n i=1

 ∇fi(x∗) 2 .

That is, to achive E f (x¯K ) − f (x∗) ≤ ε EC-LSVRG requires

 LR02
O + δε

mR02σ02 + 3 L√R04mσ02 + R02

ε

3 δε

δε3/2

Ln n i=1

 ∇fi(x∗) 2

iterations.

J.7 EC-LSVRGstar

In the setup of Section J.6 we now assume that i-th node has an access to the ∇fi(x∗). Under this unrealistic assumption we construct the method called EC-LSVRGstar that asymptotically converges to the exact solution.

Lemma J.9. For all k ≥ 0, i ∈ [n] we have

E gk | xk = ∇f (xk)

(114)

and

where

σk2

=

1 nm

1n E
n i=1

E

n

n

i=1 j=1

1 n k2

k

∗

n g¯i ≤ 2L f (x ) − f (x ) ,

i=1

gik − g¯ik 2 | xk ≤ 4L f (xk) − f (x∗) + 2σk2,

gk 2 | xk ≤ 4L f (xk) − f (x∗) + 2σk2, ∇fij (wik) − ∇fij (x∗) 2.

(115) (116) (117)

Proof. First of all, we derive unbiasedness of gk:

E gk | xk

1n

=

E ∇fil(xk) − ∇fil(wik) + ∇fi(wik) − ∇fi(x∗) | xk

n

i=1

1 nm

=

∇fij (xk) − ∇fij (wik) + ∇fi(wik) − ∇fi(x∗)

nm

i=1 j=1

1n

= ∇f (xk) +

−∇fi(wik) + ∇fi(wik) − ∇f (x∗) = ∇f (xk).

n

i=1

67

n
Next, we get an upper bound for n1 g¯ik 2:
i=1

1 n k2 1 n

k

(11) ∗2

k

∗

n

g¯i

= n

∇fi(x ) − ∇fi(x ) ≤ 2L f (x ) − f (x ) .

i=1

i=1

Since the variance of random vector is not greater than its second moment we obtain:

1n E
n i=1

gik − g¯ik 2 | xk

(34)
≤
(31)
≤
(11),(34)
≤

1n E
n i=1

gik 2 | xk

2n E
n i=1

∇fil(xk) − ∇fil(x∗) 2 | xk

2n

+

E

n i=1

∇fil(wik) − ∇fil(x∗) − ∇fi(wik) − ∇fi(x∗)

k ∗ 2 nm k

∗2

4L f (x ) − f (x ) + nm

∇fij(wi ) − ∇fij(x ) .

i=1 j=1

2 | xk

Inequality (117) trivially follows from the inequality above by Jensen’s inequality and convexity of · 2.

Lemma J.10. For all k ≥ 0, i ∈ [n] we have

where

σk2

=

1 nm

E σk2+1 | xk ≤ (1 − p)σk2 + 2Lp f (xk) − f (x∗) ,

n i=1

n j=1

∇fij (wik) − ∇fij (x∗) 2.

(118)

Proof. The proof of this lemma is identical to the proof of Lemma J.8.

Applying Theorem G.1 we get the following result.
Theorem J.7. Assume that f (x) is µ-quasi strongly convex and functions fij are convex and L-smooth for all i ∈ [n], j ∈ [m]. Then EC-LSVRGstar satisﬁes Assumption 3.3 with

A = L, A = A = 2L, B1 = B1 = B1 = B2 = 0, B2 = B2 = 2, D1 = D1 = 0,

σ12,k ≡ 0, ,

C1 = 0,

2

2

1 nm

σ2,k = σk = nm

i=1 j=1

∇fij (wik) − ∇fij (x∗) 2,

ρ1 = 1,

ρ2 = p,

C2 = Lp,

D2 = 0,

G = 0,

F1 = 0,

48Lγ2(2 + p)

F2 =

, δp

D3 = 0,

with γ satisfying





 

3

γ ≤ min

,

 56L 8L

δ 3 1 + δ 1 + 1−2 p





 

8

, M2 = 3p .







and for all K ≥ 0

E f (x¯K ) − f (x∗) ≤

1 − min

γµ p ,

24

K 4(T 0 + γF2σ02) γ

when µ > 0 and

E f (x¯K ) − f (x∗) ≤ 4(T 0 + γF2σ02) γK

when

µ

=

0,

where

Tk

def
=

xk − x∗ 2 + M2γ2σk2.

68

In other words, EC-LSVRGstar converges with linear rate O p1 + δ√κ1−p ln 1ε exactly to
the solution when µ > 0. If m ≥ 2 then taking p = m1 we get that in expectation the sample complexity of one iteration of EC-LSVRGstar is O(1) gradients calculations per node as for EC-SGDsr with standard sampling and the rate of convergence becomes O m + κδ ln 1ε .

Applying Lemma D.3 we get the complexity result in the case when µ = 0.

Corollary J.12. Let the assumptions of Theorem J.7 hold and µ = 0. Then after K

iterations of EC-LSVRGstar with the stepsize







 

3

δ

γ0 = min

,

 56L 8L 3 1 + δ 1 + 1−2 p

  
, R0 = x0 − x∗ ,
  







3pR02 3 R02δp 1 − min γ02µ , p4 

γ = min γ0, 8σ2 ,

72Lσ2

,



0

0



and p = m1 , m ≥ 2 we have E f (x¯K ) − f (x∗) of order

O LR02 + δK

R02mσ02 + 3 L√R04mσ02 .

K

3 δK

That is, to achive E f (x¯K ) − f (x∗) ≤ ε EC-LSVRGstar requires

iterations.

O LR02 + δε

R02mσ02 + 3 L√R04mσ02

ε

3 δε

However, such convergence guarantees are obtained under very restrictive assumption: the method requires to know vectors ∇fi(x∗).

J.8 EC-LSVRG-DIANA

In the setup of Section J.6 we construct a new method called EC-LSVRG-DIANA which does not require to know ∇fi(x∗) and has linear convergence to the exact solution. As in
EC-SGD-DIANA the master needs to gather only C(eki + γgik) and Q(gˆik − hki ) from all nodes in order to perform an update.

Lemma J.11. Assume that fij(x) is convex and L-smooth for all i = 1, . . . , n, j = 1, . . . , m.

Then, for all k ≥ 0 we have

E gk | xk = ∇f (xk),

(119)

1 n k2

k

∗

2

n

g¯i ≤ 4L f (x ) − f (x ) + 2σ1,k,

i=1

1n E
n i=1

gik − g¯ik 2 | xk

≤ 6L f (xk) − f (x∗) + 3σ12,k + 3σ22,k,

(120) (121)

where

E gk 2 | xk

2

1n

σ1,k = n

i=1

hki − ∇f (x∗) 2,

≤ 4L f (xk) − f (x∗) + 2σ22,k

2

1 nm

σ2,k = nm

i=1 j=1

∇fij (wik) − ∇fij (x∗) 2.

(122)

Proof. First of all, we show unbiasedness of gk:

E gk | xk

1n

=

E gˆik − hki + hk | xk

n

i=1

1 nm

=

∇fij (xk) − ∇fij (wik) + ∇fi(wik) − hki + hk

nm

i=1 j=1

= ∇f (xk).

69

Algorithm 10 EC-LSVRG-DIANA

Input:

learning

rates

γ

> 0,

α ∈ (0, 1],

initial

vectors

x

0

,

h01

,

.

.

.

,

h

0 n

∈ Rd

1: Set e0i = 0 for all i = 1, . . . , n

2: Set h0 = n1

n i=1

h0i

3: for k = 0, 1, . . . do

4: Broadcast xk, hk to all workers

5: for i = 1, . . . , n in parallel do

6:

Pick l uniformly at random from [m]

7:

Set gˆik = ∇fil(xk) − ∇fil(wik) + ∇fi(wik)

8:

gik = gˆik − hki + hk

9:

vik = C(eki + γgik)

10:

eki +1 = eki + γgik − vik

11:

hki +1 = hki + αQ(gˆik − hki )

12:

wk+1 = xk, with probability p,

i

wik, with probability 1 − p

13: end for

14:

ek = n1

n i=1

eki

,

gk

=

1 n

n
α n1 Q(gˆik − hki )
i=1

n i=1

gik

,

vk

=

1 n

n

n i=1

vik

,

hk+1

=

1 n

hki +1 = hk +

i=1

15: xk+1 = xk − vk

16: end for

n
Next, we derive the upper bound for n1 g¯ik 2:
i=1

1 n k2

1n

k

k

k2

n

g¯i

=

n

∇fi(x ) − hi + h

i=1

i=1

(31) 2 n k

∗ 2 2n k

∗

k

∗2

≤

n

∇fi(x ) − ∇fi(x ) + n

hi − ∇fi(x ) − h − ∇f (x )

i=1

i=1

(11),(34)
≤

k ∗ 2n k

∗2

4L f (x ) − f (x ) + n

hi − ∇fi(x ) .

i=1

Since the variance of random vector is not greater than its second moment we obtain:

1n E
n i=1

gik − g¯ik 2 | xk

(34)

1n

k2 k

≤ n E gi | x

i=1

1n

k

k

k

k

k2 k

= n E ∇fil(x ) − ∇fil(wi ) + ∇fi(wi ) − hi + h | x

i=1

(31)

3n

2

≤

E ∇fil(xk) − ∇fil(x∗) | xk

n

i=1

3n

+

E

n i=1

∇fil(wik) − ∇fil(x∗) − ∇fi(wik) − ∇fi(x∗)

3n +

hki − ∇fi(x∗) − hk − ∇f (x∗) 2

n

i=1

(11),(34)
≤

k ∗ 3 nm k

∗2

6L f (x ) − f (x ) + nm

∇fij(wi ) − ∇fij(x )

i=1 j=1

3n +
n i=1

hki − ∇fi(x∗) 2 .

2 | xk

70

Finally, we obtain an upper boud for the second moment of gk:

E gk 2 | xk


n

2

1 = E
n

∇fil(xk) − ∇fil(wik) + ∇fi(wik) − ∇fi(x∗) | xk

i=1

(3≤1) 2 n E ∇fil(xk) − ∇fil(x∗) 2 | xk n
i=1

2n

+

E

n i=1

∇fil(wik) − ∇fil(x∗) − ∇fi(wik) − ∇fi(x∗) 2 | xk

= 2 n m ∇fij (xk) − ∇fij (x∗) 2 nm
i=1 j=1

2

2 +

nm

1m

∇fij (wik) − ∇fij (x∗) −

∇fij (wik) − ∇fij (x∗)

nm

m

i=1 j=1

j=1

(11),(34)
≤

4L f (xk) − f (x∗) + 2 n m ∇fij (wik) − ∇fij (x∗) 2 . nm
i=1 j=1

Lemma J.12. Assume that α ≤ 1/(ω+1). Then, for all k ≥ 0 we have

E σ12,k+1 | xk ≤ (1 − α)σ12,k + 6Lα(f (xk) − f (x∗)) + 2ασ22,k,

(123)

where

σ12,k

=

1 n

E σ22,k+1 | xk ≤ (1 − p)σk2,2 + 2Lp f (xk) − f (x∗)

(124)

n i=1

hki − ∇fi(x∗)

2

and

σ22,k

=

1 nm

n i=1

m j=1

∇fij (wik) − ∇fij (x∗) 2.

Proof. First of all, we derive an upper bound for the second moment of hki +1 − h∗i :

E hki +1 − h∗i 2 | xk

= E hk − h∗ + αQ(gˆk − hk) 2 | xk

i

i

i

i

(26)
=

hki − h∗i 2 + 2α hki − h∗i , ∇fi(xk) − hki

+α2E Q(gˆik − hki ) 2 | xk

(26),(35)
≤

hki − h∗i 2 + 2α hki − h∗i , ∇fi(xk) − hki +α2(ω + 1)E gˆik − hki 2 | xk .

Using variance decomposition (34) and α ≤ 1/(ω+1) we get

α2(ω + 1)E gˆik − hki 2 | xk

(34)
=

α2(ω + 1)E

gˆik − ∇fi(xk) 2 | xk + α2(ω + 1) ∇fi(xk) − hki 2

≤ αE gˆik − ∇fi(xk) 2 | xk + α ∇fi(xk) − hki 2

(3≤1) 2αE ∇fil(xk) − ∇fil(x∗) − ∇fi(xk) − ∇fi(x∗) 2 | xk

+2αE ∇fil(wik) − ∇fil(x∗) − ∇fi(wik) − ∇fi(x∗) 2 | xk

+α ∇fi(xk) − hki 2 (3≤4) 2αE ∇fil(xk) − ∇fil(x∗) 2 | xk

+2αE ∇fil(wik) − ∇fil(x∗) 2 | xk + α ∇fi(xk) − hki 2

(11)

k ∗ 2α m k

∗2

≤ 4LαDfi (x , x ) + m

∇fij(wi ) − ∇fij(x )

j=1

+α ∇fi(xk) − hki 2

71

Putting all together we obtain

E hki +1 − h∗i 2 | xk

≤ hki − h∗i 2 + α ∇fi(xk) − hki , fi(xk) + hki − 2h∗i

k ∗ 2α m k

∗2

+4LαDfi (x , x ) + m

∇fij(wi ) − ∇fij(x )

j=1

(28)
=

hki − h∗i 2 + α ∇fi(xk) − h∗i 2 − α hki − h∗i 2

k ∗ 2α m k

∗2

+4LαDfi (x , x ) + m

∇fij(wi ) − ∇fij(x )

j=1

(11)
≤ (1 − α) hki − h∗i 2 + 6LαDfi (xk, x∗)

2α m k

∗2

+ m

∇fij(wi ) − ∇fij(x ) .

j=1

Summing up the above inequality for i = 1, . . . , n we derive E σ12,k+1 | xk ≤ (1 − α)σ12,k + 6Lα(f (xk) − f (x∗)) + 2ασ22,k.

Similarly to the proof of Lemma J.8 we get

E σ22,k+1 | xk

1 nm

k+1

∗2 k

= nm

E ∇fij(wi ) − ∇fij(x ) | x

i=1 j=1

1−p n m k

∗2

= nm

∇fij(wi ) − ∇fij(x )

i=1 j=1

p +

nm
∇fij (xk) − ∇fij (x∗) 2

nm

i=1 j=1

(11)

2 2Lp n m

k∗

≤ (1 − p)σ2,k + nm

Dfij (x , x )

i=1 j=1

= (1 − p)σ22,k + 2Lp f (xk) − f (x∗) .

Applying Theorem G.1 we get the following result.
Theorem J.8. Assume that fij(x) is convex and L-smooth for all i = 1, . . . , n, j = 1, . . . , m and f (x) is µ-quasi strongly convex. Then EC-LSVRG-DIANA satisﬁes Assumption 3.3 with

A = A = 2L, B1 = B2 = 0, B1 = B2 = 2,

A = 3L,

B1 = B2 = 3,

2

1n

σ1,k = n

i=1

2

1 nm

σ2,k = nm

i=1 j=1

∇fij (wik) − ∇fij (x∗) 2,

D1 = D1 = D1 = D2 = D3 = 0, hki − ∇fi(x∗) 2, ρ1 = α,
ρ2 = p, C1 = 3Lα, C2 = Lp,

24Lγ2 4 + 3

24Lγ2 1−4α 4δ + 3 + 3

G = 2, F1 =

δ γµ α p

,

F2 =

γµ α p ,

δα 1 − min 2 , 4 , 4

δp 1 − min 2 , 4 , 4

with γ and α satisfying









 

9

δ

 

γ ≤ min

,

,

 296L 4L 6 4 + 3δ + 1−2α 3 + 1−4 p (4 + 3δ) + 16−δp 

1 α≤
ω+1

72

with

M1

=0

and

M2

=

8 3p

+

32 9p

and

for

al l

K

≥0

E f (x¯K ) − f (x∗) ≤

1 − min

γµ α p ,,

2 44

K 4(T 0 + γF1σ12,0 + γF2σ22,0) ,
γ

when µ > 0 and

E f (x¯K ) − f (x∗) ≤ 4(T 0 + γF1σ12,0 + γF2σ22,0) Kγ

when

µ

=

0,

where

Tk

def
=

xk − x∗ 2 + M2γ2σ22,k.

In other words, if p = 1/m, m ≥ 2 and









 

9

δ

 

11

γ = min

,

, α = min

,,

 296L 4L 6 4 + 3δ + 1−2α 3 + 1−4 p (4 + 3δ) + 16−δp 

ω+1 2

then EC-LSVRG-DIANA converges with the linear rate

κ1 O ω + m + ln
δε

to the exact solution when µ > 0.

Applying Lemma D.3 we get the complexity result in the case when µ = 0.
Corollary J.13. Let the assumptions of Theorem J.8 hold and µ = 0. Then after K iterations of EC-LSVRG-DIANA with the stepsize









γ0 = min  2996L , δ  , R0 = x0 − x∗ ,  4L 6 4 + 3δ + 1−2α 3 + 1−4 p (4 + 3δ) + 16−δp 


   γ = min γ0,
  

9pR02 , 56σ22,0 3





R02

 
,

24

L

(

4 δ

+3

)

σ2

+

24

L

(

4 1−

α

(

4 δ

+3

)+3

)

σ2

 

δ

α

(

1

−

min{

γ

0µ 2

,

α 4

,

p 4

})

1,0

δ

p

(1

−

min

{

γ0 µ 2

,

α 4

,

p 4

})

2,0 

and p = m1 , m ≥ 2, α = min ω+1 1 , 21 we have E f (x¯K ) − f (x∗) of order

 LR02
O + δK

R02mσ22,0 3 LR04((ω + 1)σ12,0 + mσ22,0)  K + δ2/3K  .

That is, to achive E f (x¯K ) − f (x∗) ≤ ε EC-LSVRG-DIANA requires

 LR02
O + δε

R02mσ22,0 3 LR04((ω + 1)σ12,0 + mσ22,0) 

+ ε

δ2/3ε



iterations.

73

Table 4: Complexity of SGD methods with delayed updates established in this paper.
Symbols: ε = error tolerance; δ = contraction factor of compressor C; ω = variance parameter of compressor Q; κ = L/µ; L = expected smoothness constant; σ∗2 = variance of the stochastic gradients in the solution; ζ∗2 = average of ∇fi(x∗) 2; σ2 = average of the uniform bounds for the variances of stochastic gradients of workers; M2,q = (ω + 1)σ2 + ωζ∗2; σq2 = (1 + ω) 1 + ωn σ2. †D-QGDstar is a special case of D-QSGDstar where each worker i computes the full gradient ∇fi(xk); ‡D-GD-DIANA is a special case of D-SGD-DIANA where each worker i computes the full gradient ∇fi(xk).

Problem (1)+(3)
(1)+(2)
(1)+(2)
(1)+(2) (1)+(2) (1)+(2) (1)+(2) (1)+(3) (1)+(3) (1)+(3) (1)+(3)

Method D-SGDsr
D-SGD
D-QSGD
D-QSGDstar D-QGDstar† D-SGD-DIANA D-GD-DIANA‡ D-LSVRG D-QLSVRG D-QLSVRGstar D-LSVRG-DIANA

Alg # Alg 15
Alg 11
Alg 12
Alg 13 Alg 13 Alg 14 Alg 14 Alg 16 Alg 17 Alg 18 Alg 19

Citation new
[45]
new
new new new new new new new new

Sec # K.5
K.1
K.2
K.3 K.3 K.4 K.4 K.6 K.7 K.8 K.9

Ra√te (constants ignor√ed)

O L+ L2τ 2+LLτ + σ∗2 +

Lτ σ∗2 √

µ

n√µε

µ nε

O τ κ + σ∗2 +

Lτ σ∗2 √

nµε

µ √nε

O κ τ + ω + M2,q +

Lτ M2,q √

n

nµε

µ nε

√

O κ τ + ω + σ2 + L√τσ2

n

nµε

µ nε

O κ τ + ωn log 1ε √

O ω + κ τ + ω + σ2 +

Lτ σq2 √

n

nµε

µ nε

O ω + κ τ + ωn log 1ε

O (m + κτ ) log 1ε √

O m + κ τ + ω + ζ∗2 +

Lτ ζ∗2 √

n

nµε

µ nε

O m + κ τ + ωn log 1ε O ω + m + κ τ + ωn log 1ε

Algorithm 11 D-SGD

Input: learning rate γ > 0, initial vector x0 ∈ Rd 1: Set e0i = 0 for all i = 1, . . . , n 2: for k = 0, 1, . . . do

3: Broadcast xk to all workers

4: for i = 1, . . . , n in parallel do

5:

Sample gik = ∇fξi (xk) − ∇fi(x∗)

6:

vk = γgik−τ , if k ≥ τ,

i

0,

if k < τ

7:

eki +1 = eki + γgik − vik

8: end for

9:

ek = n1

n i=1

eki

,

gk

=

1 n

10: xk+1 = xk − vk

11: end for

n i=1

gik

,

vk

=

1 n

n i=1

vik

=

1 n

n i=1

∇fξi

(xk−τ

)

K Special Cases: Delayed Updates Methods
K.1 D-SGD In this section we consider the same setup as in Section J.2. We notice that vectors eki appear only in the analysis and there is no need to compute them. Moreover, we use ∇fi(x∗) in the deﬁnition of gik which is problematic at the ﬁrt glance. Indeed, workers do not know ∇fi(x∗). However, since 0 = ∇f (x∗) = n1 ∇fi(x∗) and master node uses averages of gik for the updates one can ignore ∇fi(x∗) in gik in the implementation of D-SGD and get exactly the same method. We deﬁne gik in such a way only for the theoretical analysis.
74

Lemma K.1 (see also Lemmas 1,2 from [39]). Assume that fξi (x) are convex in x for every ξi, i = 1, . . . , n. Then for every x ∈ Rd and i = 1, . . . , n

k2 k

k ∗ 2n

∗

E g | x ≤ 4L(f (x ) − f (x )) + n2 Var [∇fξi (x )] .

i=1

(125)

If further f (x) is µ-quasi strongly convex with possibly non-convex fi, fξi and µ > 0, then for every x ∈ Rd and i = 1, . . . , n

k2 k

k ∗ 2n

∗

E g | x ≤ 4Lκ(f (x ) − f (x )) + n2 Var [∇fξi (x )] ,

i=1

(126)

where κ = Lµ .

Proof. By deﬁnition of gk we have

E gk 2 | xk



1n

= E

∇fξ (xk) − ∇fξ (x∗) + ∇fξ (x∗) − ∇fi(x∗)

n

i

i

i

i=1


n

2

(31)

1

∇f (xk) − ∇f (x∗) | xk

≤ 2E  n

ξi

ξi



i=1


n

2

1 +2 E 
n

(∇fξi (x∗) − ∇fi(x∗)) 

i=1

2 | xk

Var

n

1 n

∇fξi (x∗)

i=1

(31) 2 n

k

∗2 k

≤ n

E ∇fξi (x ) − ∇fξi (x ) | x

i=1

2n + n2 E
i=1

∇fξi (x∗) − ∇fi(x∗) 2 , Var[∇fξi (x∗)]

(127)

where in the last inequality we use independence of ∇fξi (x∗), i = 1, . . . , n. Using this we derive inequality (125):

E gk 2 | xk

(127),(11)
≤
=
=

4L n E
n i=1

Dfξi (xk, x∗) | xk

2n

+

Var [∇fξ (x∗)]

n2

i

i=1

4L n

2n

Df (xk, x∗) +

Var [∇fξ (x∗)]

n

i

n2

i

i=1

i=1

2n

4L f (xk) − f (x∗) +

Var [∇fξ (x∗)] .

n2

i

i=1

Next, if f (x) is µ-quasi strongly convex, but fi, fξi are not necessary convex, we obtain

E gk 2 | xk

(127),(10)
≤
(9)
≤

2L2 n n i=1

k ∗2 2 n

∗

x − x + n2 Var [∇fξi (x )]

i=1

4L2

2n

f (xk) − f (x∗) +

Var [∇fξ (x∗)] .

µ

n2

i

i=1

75

Theorem K.1. Assume that fξ(x) is convex in x for every ξ. Then D-SGD satisﬁes Assumption 3.4 with

A = 2L,

B1 = B2 = 0,

2n D1 = n2 Var [∇fξi (x∗)] ,
i=1

σ12,k ≡ σ22,k ≡ 0

ρ1 = ρ2 = 1, C1 = C2 = 0, D2 = 0

F1 = F2 = 0,

6γτ L n

D3 =

Var [∇fξ (x∗)]

n2

i

i=1

with γ satisfying

1 γ≤
8L 2τ (τ + 2)

and for all K ≥ 0

γµ K 4 x0 − x∗ 2 8γ

n

E f (x¯K ) − f (x∗) ≤ 1 −

+ (1 + 3Lγτ ) Var [∇fξ (x∗)]

2

γ

n2

i

i=1

when µ > 0 and

4 x0 − x∗ 2 8γ

n

E f (x¯K ) − f (x∗) ≤

+ (1 + 3Lγτ ) Var [∇fξ (x∗)]

γK

n2

i

i=1

when µ = 0. If further fi(x) are µ-strongly convex with possibly non-convex fξi and µ > 0, then D-SGD satisﬁes Assumption 3.4 with

A = 2κL,

B1 = B2 = 0,

2n D1 = n2 Var [∇fξi (x∗)] ,
i=1

σ12,k ≡ σ22,k ≡ 0,

ρ1 = ρ2 = 1, C1 = C2 = 0, D2 = 0, G = 0,

F1 = F2 = 0,

6γτ L n

D3 =

Var [∇fξ (x∗)]

n2

i

i=1

with γ satisfying

1

1

γ ≤ min

,

8κL 8L 2τ (τ + 2κ)

and for all K ≥ 0

γµ K 4 x0 − x∗ 2 8γ

n

E f (x¯K ) − f (x∗) ≤ 1 −

+ (1 + 3Lγτ ) Var [∇fξ (x∗)] .

2

γ

n2

i

i=1

In other words, D-SGD converges with linear rate O τ κ ln 1ε to the neighbourhood of the solution when µ > 0. Applying Lemma D.2 we establish the rate of convergence to ε-solution.

Corollary K.1. Let the assumptions of Theorem K.1 hold, fξ(x) are convex for each ξ and µ > 0. Then after K iterations of D-SGD with the stepsize

γ = min  1 , ln max 2, min x0−xD∗ 12µ2K2 , x0−3xτ∗LD2µ1 3K3 

 8L 2τ (τ + 2)

µK 

we have

E f (x¯K ) − f (x∗) = O

Lτ

x0 − x∗ 2 exp

µ −K

+ D1 + Lτ D1

.

τL

µK µ2K2

That is, to achive E f (x¯K ) − f (x∗) ≤ ε D-SGD requires

O τ L + D1 + L√τ D1 µ µε µ ε

iterations.

76

Corollary K.2. Let the assumptions of Theorem K.1 hold and f (x) is µ-strongly convex with µ > 0 and possibly non-convex fi, fξi . Then after K iterations of D-SGD with the stepsize

 1

1

ln max 2, min x0−xD∗ 2µ2K2 , x0−Lxτ∗D2µ3K3 

γ = min

,

,

1

1

 8κL 8L 2τ (τ + 2κ)

µK 

we have E f (x¯K ) − f (x∗) of order

√ O L κ+τ κ

x0 − x∗ 2 exp − min

µ1 √,

K + D1 + Lτ D1 .

τ L κ κ2

µK µ2K2

That is, to achive E f (x¯K ) − f (x∗) ≤ ε D-SGD requires

O κ2 + τ κ3/2 + D1 + L√τ D1 µε µ ε

iterations.

Applying Lemma D.3 we get the complexity result in the case when µ = 0.
Corollary K.3. Let the assumptions of Theorem K.1 hold, fξ(x) are convex for each ξ and µ = 0. Then after K iterations of D-SGD with the stepsize

1

γ = min

,

8L 2τ (τ + 2)

we have E f (x¯K ) − f (x∗) of order

x0 − x∗ 2 , 3 D1K

x0 − x∗ 2 3Lτ D1K

O τ LR02 + K

R02τ D1 + 3 LR04τ D1

K

K 2/3

where R0 = x0 − x∗ . That is, to achive E f (x¯K ) − f (x∗) ≤ ε D-SGD requires

O τ LR02 + R02D1 + R02 Lτ D1

ε

ε2

ε3/2

iterations.

K.2 D-QSGD

In this section we show how one can combine delayed updates with quantization using our scheme.

Lemma K.2. Assume that fi(x) is convex and L-smooth for all i = 1, . . . , n. Then, for all k ≥ 0 we have

E gk | xk E gk 2 | xk

= ∇f (xk), 2ω
≤ 2L 1 + n

k ∗ (ω + 1)D 2ω n

∗2

f (x ) − f (x ) + n + n2

∇fi(x )

i=1

where D = n1

n i=1

Di.

Proof. First of all, we show unbiasedness of gk:

E gk | xk

1n

1n

=

E gik | xk =

E EQ Q(gˆik) − ∇fi(x∗) | xk

n

n

i=1

i=1

(26) 1 n

=

∇fi(xk) − ∇fi(x∗) = ∇f (xk),

n

i=1

77

Algorithm 12 D-QSGD

Input: learning rate γ > 0, initial vector x0 ∈ Rd 1: Set e0i = 0 for all i = 1, . . . , n 2: for k = 0, 1, . . . do

3: Broadcast xk−τ to all workers

4: for i = 1, . . . , n do

5:

Sample gˆik−τ independently from other nodes such that E[gˆik−τ | xk−τ ] =

∇fi(xk−τ ) and E gˆik−τ − ∇fi(xk−τ ) 2 | xk−τ ≤ Di

6:

gik−τ = Q(gˆik−τ ) − ∇fi(x∗) (quantization is performed independently from other

nodes)

7:

vik = γgik−τ

8:

eki +1 = eki + γgik − vik

9: end for

10:

ek = n1

n i=1

eki

,

gk

=

1 n

γ n

n i=1

Q(gˆik−τ

)

11: xk+1 = xk − vk

n i=1

gik

,

vk

=

1 n

n i=1

vik

=

γ n

n i=1

gik−τ

=

12: end for

where EQ [·] denotes mathematical expectation w.r.t. the randomness coming only from the quantization. Next, we derive the upper bound for the second moment of gk:

EQ gk 2


n

2

1 = EQ

Q(gˆk) − ∇fi(x∗)

 n

i



i=1


n

2

n

2

(34)

1

= EQ

Q(gˆk) − gˆk

1 +

gˆk − ∇fi(x∗) . (128)

 n

i

i n

i

i=1

i=1

Since Q(gˆ1k), . . . , Q(gˆnk) are independent quantizations, we get

EQ gk 2

(128)
≤

1n n2 EQ
i=1

n

2

Q(gˆk) − gˆk 2 + 1

gˆk − ∇fi(x∗)

i

i

n

i

i=1

(26) ω n

1n

2

≤ n2

gˆik 2 + n

gˆik − ∇fi(x∗) .

i=1

i=1

Taking conditional expectation E · | xk from the both sides of the previous inequality we obtain

E gk 2 | xk

n


n

2

ω ≤

E gˆk 2 | xk + E 1

gˆk − ∇fi(x∗) | xk

n2

i

 n

i



i=1

i=1

(34) ω n

k2 ωn

k

k2 k

≤ n2

∇fi(x ) + n2 E gˆi − ∇fi(x ) | x

i=1

i=1

n

2



n

2

1 +

∇fi(xk) − ∇fi(x∗) +E 1

gˆk − ∇fi(xk) | xk .

n

 n

i



i=1

i=1

∇f (xk)−∇f (x∗) 2

78

It remains to estimate terms in the second and the third lines of the previous inequality:

ωn

k 2 (31) 2ω n

k

∗ 2 2ω n

∗2

n2

∇fi(x ) ≤ n2

∇fi(x ) − ∇fi(x ) + n2

∇fi(x )

i=1

i=1

i=1

(11) 4ωL

k

∗ 2ω n

∗2

≤ n f (x ) − f (x ) + n2

∇fi(x ) ,

i=1

ωn E
n i=1

gˆik − ∇fi(xk) 2 | xk

ωn

ωD

≤ n2 Di = n ,

i=1

(11)
∇f (xk) − ∇f (x∗) 2 ≤ 2L f (xk) − f (x∗) ,


n

2

n

1 E

gˆk − ∇fi(xk) | xk

1 =

E gˆk − ∇fi(xk) 2 | xk

 n

i



n2

i

i=1

i=1

1n

D

≤ n2 Di = n .

i=1

Putting all together we get

E gk 2 | xk

2ω ≤ 2L 1 +
n

k ∗ (ω + 1)D 2ω n

∗2

f (x ) − f (x ) + n + n2

∇fi(x ) .

i=1

Theorem K.2. Assume that fi(x) is convex and L-smooth for all i = 1, . . . , n and f (x) is µ-quasi strongly convex. Then D-QSGD satisﬁes Assumption 3.4 with

2ω

(ω + 1)D 2ω n

∗2

A =L 1+ n

,

B1 = B2 = 0,

D1 =

n

+ n2

∇fi(x ) ,

i=1

σ12,k ≡ σ22,k ≡ 0, ρ1 = ρ2 = 1, C1 = C2 = 0, D2 = 0

F1 = F2 = 0,

G = 0,

3γτ L D3 = n

(ω + 1)D + 2ω n ∇fi(x∗) 2 n
i=1

with γ satisfying

1

1

γ ≤ min

,

4L(1 + 2ω/n) 8L 2τ (τ + 1 + 2ω/n)

and for all K ≥ 0

E f (x¯K ) − f (x∗) ≤ 1 − γ2µ K 4 x0 −γ x∗ 2 + γ (D1 + D3)

when µ > 0 and when µ = 0.

E f (x¯K ) − f (x∗) ≤ 4 x0γ−Kx∗ 2 + γ (D1 + D3)

In other words, D-QSGD converges with the linear rate

ω

ω

1

O κ 1+ +κ τ τ +

ln

n

n

ε

to the neighbourhood of the solution when µ > 0. Applying Lemma D.2 we establish the rate of convergence to ε-solution.

79

Corollary K.4. Let the assumptions of Theorem K.2 hold, fξ(x) are convex for each ξ and µ > 0. Then after K iterations of D-QSGD with the stepsize

1

1

γ0 = min

,

, R0 = x0 − x∗ ,

4L(1 + 2ω/n) 8L 2τ (τ + 1 + 2ω/n)

 ln max 2, min R02µ2K2 , R02µ3K3



 γ = min γ0,

D1

3τ LD1



 µK 

we have E f (x¯K ) − f (x∗) of order





O LR02 1 + ωn + τ τ + ωn

µ

exp −

 ω+ τ τ+ω

L 1+ n

n

That is, to achive E f (x¯K ) − f (x∗) ≤ ε D-QSGD requires





K + D1 + Lτ D1  .  µK µ2K2 

O

L

ω

1+

L +

ω τ τ+

+ D1 +

L√τ D1

µ

nµ

n µε µ ε

iterations.

Applying Lemma D.3 we get the complexity result in the case when µ = 0.
Corollary K.5. Let the assumptions of Theorem K.2 hold and µ = 0. Then after K iterations of D-QSGD with the stepsize

1

1

γ0 = min 4L(1 + 2ω/n) , 8L 2τ (τ + 1 + 2ω/n) ,

γ = min γ0, we have E f (x¯K ) − f (x∗) of order

x0 − x∗ 2 , 3 D1K

x0 − x∗ 2 3Lτ D1K

 LR2 1 + ω

LR02 τ τ + ωn

O 0

n+

+

K

K

 R02KD1 + 3 LKR204/τ3 D1 

where R0 = x0 − x∗ . That is, to achive E f (x¯K ) − f (x∗) ≤ ε D-QSGD requires

 LR02 1 + ωn
O ε

LR02 +

τ τ + ωn ε



R02D1 R02 Lτ D1

+

+



ε2

ε3/2

iterations.

K.3 D-QSGDstar

As we saw in Section K.2 D-QSGD fails to converge to the exact optimum asymptotically even if gˆik = ∇fi(xk) for all i = 1, . . . , n almost surely, i.e., all Di = 0 for all i = 1, . . . , n. As for EC-GDstar we assume now that i-th worker has an access to ∇fi(x∗). Using this one can construct the method with delayed updates that converges asymptotically to the exact
solution when the full gradients are available.

Lemma K.3. Assume that fi(x) is convex and L-smooth for all i = 1, . . . , n. Then, for all k ≥ 0 we have

E gk | xk = ∇f (xk),

(129)

E gk 2 | xk

where D = n1

n i=1

Di.

ω ≤ 2L 1 +
n

f (xk) − f (x∗)

(ω + 1)D +

n

(130)

80

Algorithm 13 D-QSGDstar

Input: learning rate γ > 0, initial vector x0 ∈ Rd 1: Set e0i = 0 for all i = 1, . . . , n 2: for k = 0, 1, . . . do

3: Broadcast xk−τ to all workers

4: for i = 1, . . . , n do

5:

Sample gˆik−τ independently from other nodes such that E[gˆik−τ | xk−τ ] =

∇fi(xk−τ ) and E gˆik−τ − ∇fi(xk−τ ) 2 | xk−τ ≤ Di

6:

gik−τ = Q(gˆik−τ − ∇fi(x∗)) (quantization is performed independently from other

nodes)

7:

vik = γgik−τ

8:

eki +1 = eki + γgik − vik

9: end for

10:

ek = n1

n i=1

eki

,

gk

=

1 n

γ n

n i=1

Q(gˆik−τ

−

∇fi(x∗))

11: xk+1 = xk − vk

n i=1

gik

,

vk

=

1 n

n i=1

vik

=

γ n

n i=1

gik−τ

=

12: end for

Proof. First of all, we show unbiasedness of gk:

E gk | xk

1n

1n

=

E gik | xk =

E EQ Q(gˆik − ∇fi(x∗)) | xk

n

n

i=1

i=1

(26) 1 n

=

∇fi(xk) − ∇fi(x∗) = ∇f (xk),

n

i=1

where EQ [·] denotes mathematical expectation w.r.t. the randomness coming only from the quantization. Next, we derive the upper bound for the second moment of gk:

EQ gk 2


n

2

1 = EQ

Q gˆk − ∇fi(x∗)

 n

i



i=1


n

2

(34)

1

= EQ

Q gˆk − ∇fi(x∗) − gˆk − ∇fi(x∗)

 n

i

i



i=1

n

2

1 +

gˆk − ∇fi(x∗) .

n

i

i=1

(131)

Since Q gˆ1k − ∇f1(x∗) , . . . , Q gˆnk − ∇fn(x∗) are independent quantizations, we get

EQ gk 2

(131)
≤
(26)
≤

1n n2 EQ
i=1

Q gˆik − ∇fi(x∗) − gˆik − ∇fi(x∗) 2

n

2

1 +

gˆk − ∇fi(x∗)

n

i

i=1

n

n

2

ω

gˆk − ∇fi(x∗) 2 + 1

gˆk − ∇fi(x∗) .

n2

i

n

i

i=1

i=1

81

Taking conditional expectation E · | xk from the both sides of the previous inequality we obtain

E gk 2 | xk

n


n

2

ω ≤

E gˆk − ∇fi(x∗) 2 | xk + E 1

gˆk − ∇fi(x∗) | xk

n2

i

 n

i



i=1

i=1

(34) ω n k

∗2 ωn

k

k2 k

≤ n2

∇fi(x ) − ∇fi(x ) + n2 E gˆi − ∇fi(x ) | x

i=1

i=1

n

2



n

2

1 +

∇fi(xk) − ∇fi(x∗) +E 1

gˆk − ∇fi(xk) | xk .

n

 n

i



i=1

i=1

∇f (xk)−∇f (x∗) 2

It remains to estimate terms in the second and the third lines of the previous inequality:

ωn k

∗ 2 (11) 2ωL

k

∗

n2

∇fi(x ) − ∇fi(x )

≤

f (x ) − f (x ) ,

n

i=1

ωn E
n i=1

gˆik − ∇fi(xk) 2 | xk

ωn

ωD

≤ n2 Di = n ,

i=1

(11)
∇f (xk) − ∇f (x∗) 2 ≤ 2L f (xk) − f (x∗) ,


n

2

n

1 E

gˆk − ∇fi(xk) | xk

1 =

E gˆk − ∇fi(xk) 2 | xk

 n

i



n2

i

i=1

i=1

1n

D

≤ n2 Di = n .

i=1

Putting all together we get

E gk 2 | xk

ω ≤ 2L 1 +
n

f (xk) − f (x∗)

(ω + 1)D

+

.

n

Theorem K.3. Assume that fi(x) is convex and L-smooth for all i = 1, . . . , n and f (x) is µ-quasi strongly convex. Then D-QSGDstar satisﬁes Assumption 3.4 with

A =L with γ satisfying

ω 1+ ,
n

B1 = B2 = 0,

(ω + 1)D

D1 =

, n

σ12,k ≡ σ22,k ≡ 0,

ρ1 = ρ2 = 1, C1 = C2 = 0, D2 = 0, G = 0,

3γτ L(ω + 1)D F1 = F2 = 0, D3 = n

1

1

γ ≤ min

,

.

4L(1 + ω/n) 8L τ (τ + 1 + ω/n)

and for all K ≥ 0

E f (x¯K ) − f (x∗) ≤ 1 − γ2µ K 4 x0 −γ x∗ 2 + 4γ (D1 + D3)

when µ > 0 and when µ = 0.

E f (x¯K ) − f (x∗) ≤ 4 x0γ−Kx∗ 2 + 4γ (D1 + D3)

82

In other words, D-QSGDstar converges with the linear rate

ω

ω

1

O τ +κ 1+ +κ τ τ +

ln

n

n

ε

to the exact solution when µ > 0 and D = 0, i.e., gˆik = ∇fi(xk) for all i = 1, . . . , n almost surely. Applying Lemma D.2 we establish the rate of convergence to ε-solution.

Corollary K.6. Let the assumptions of Theorem K.3 hold and µ > 0. Then after K iterations of D-QSGDstar with the stepsize

1

1

γ0 = min

,

, R0 = x0 − x∗ ,

4L(1 + ω/n) 8L τ (τ + 1 + ω/n)

 ln max 2, min nR02Dµ2K2 , nR302τµL3DK3 

γ = min γ0, µK





we have E f (x¯K ) − f (x∗) of order





O LR02 1 + ωn + τ τ + ωn

µ

exp −

 ω+ τ τ+ω

L 1+ n

n

That is, to achive E f (x¯K ) − f (x∗) ≤ ε D-QSGDstar requires

√

L

ωL

ω

D

Lτ D

O

1+ + τ τ + + + √

µ

nµ

n nµε µ nε





D

Lτ D

K + nµK + nµ2K2  .

iterations.

Applying Lemma D.3 we get the complexity result in the case when µ = 0.
Corollary K.7. Let the assumptions of Theorem K.3 hold and µ = 0. iterations of D-QSGDstar with the stepsize

1

1

γ0 = min 4L(1 + 2ω/n) , 8L τ (τ + 1 + ω/n) ,

Then after K

γ = min γ0,

n x0 − x∗ 2 3 n x0 − x∗ 2

,

DK

3Lτ DK

we have E f (x¯K ) − f (x∗) of order

 LR2 1 + ω

LR02 τ τ + ωn

O 0

n+

+

K

K

 Rn02KD + 3n1L/3RK042τ/D3 

where R0 =

x0 − x∗ . That is, to achive E f (x¯K ) − f (x∗) ≤ ε D-QSGDstar requires

 LR02 1 + ωn
O ε

LR02 +

τ τ + ωn ε

√ + Rn02εD2 + R√02 nLε3τ/2D 

iterations.

K.4 D-SGD-DIANA

In this section we present a practical version of D-QSGDstar: D-SGD-DIANA.

Lemma K.4 (Lemmas 1 and 2 from [19]). Assume that fi(x) is convex and L-smooth for all i = 1, . . . , n and α ≤ 1/(ω+1). Then, for all k ≥ 0 we have

E gk | xk = ∇f (xk),

(132)

E gk 2 | xk

2ω ≤ 2L 1 +
n

f (xk) − f (x∗) + 2ωσk2 + (ω + 1)D

n

n

(133)

E σk2+1 | xk ≤ (1 − α)σk2 + 2Lα f (xk) − f (x∗) + αD

where

σk2

=

1 n

n i=1

hki − ∇fi(x∗)

2

and

D=

1 n

n i=1

Di.

(134)

83

Algorithm 14 D-SGD-DIANA

Input:

learning

rates

γ

> 0, α ∈ (0, 1],

initial

vectors

x

0

,

h01

,

.

.

.

,

h

0 n

∈ Rd

1: Set e0i = 0 for all i = 1, . . . , n

2: Set h0 = n1

n i=1

h0i

3: for k = 0, 1, . . . do

4: Broadcast xk−τ to all workers

5: for i = 1, . . . , n do

6:

Sample gˆik−τ independently from other nodes such that E[gˆik−τ

∇fi(xk−τ ) and E gˆik−τ − ∇fi(xk−τ ) 2 | xk−τ ≤ Di

7:

∆ˆ ki −τ = Q(gˆik−τ − hki −τ ) (quantization is performed independently

nodes)

8:

gik−τ = hki −τ + ∆ˆ ki −τ

9:

vik = γgik−τ

10:

eki +1 = eki + γgik − vik

11:

hki −τ +1 = hki −τ + α∆ˆ ki −τ

12: end for

13:

hk−τ = n1

n i=1

hki −τ

,

ek

=

1 n

n i=1

eki

,

gk

=

1 n

γ n

n i=1

gik−τ

=

γhk−τ

+

γ n

n i=1

∆ˆ ki −τ

14: xk+1 = xk − vk

15:

hk−τ +1 = hk−τ + αn

n i=1

∆ˆ ki −τ

16: end for

n i=1

gik

,

vk

=

1 n

| xk−τ ] = from other

n i=1

vik

=

Theorem K.4. Assume that fi(x) is convex and L-smooth for all i = 1, . . . , n and f (x) is µ-quasi strongly convex. Then D-SGD-DIANA satisﬁes Assumption 3.4 with

2ω

2ω

(ω + 1)D 2

2 1n k

∗2

A =L 1+ n

,

B1 =

, n

D1 =

n , σ1,k = σk = n

hi − ∇fi(x ) ,

i=1

B2 = 0, ρ1 = α, ρ2 = 1, C1 = Lα,

12γ2Lωτ (2 + α)

F1 =

, nα

F2 = 0,

α(ω + 1)D

C2 = 0, D2 =

, G = 0, n

4ω (ω + 1)D D3 = 3γτ L 1 + n n

with γ and α satisfying

1

1

1

8ω

γ ≤ min 4L(1 + 14ω/3n) , 8L 2τ (1 + τ + 2ω/n + 4ω/n(1−α)) , α ≤ ω + 1 , M1 = 3nα

and for all K ≥ 0

E f (x¯K ) − f (x∗) ≤

1 − min

γµ α ,

24

K 4(T 0 + γF1σ02)

γ

+ 4γ (D1 + M1D2 + D3)

when µ > 0 and

E f (x¯K ) − f (x∗) ≤ 4(T 0 +γKγF1σ02) + 4γ (D1 + M1D2 + D3)

when

µ

=

0,

where

Tk

def
=

x˜k − x∗ 2 + M1γ2σk2.

In other words, if

1

1

11

γ ≤ min

,

, α ≤ min

,

4L(1 + 14ω/3n) 8L 2τ (1 + τ + 10ω/n)

ω+1 2

then D-SGD-DIANA converges with the linear rate

ω

ω

1

O ω+κ 1+ +κ τ τ +

ln

n

n

ε

to the exact solution when µ > 0. Applying Lemma D.2 we establish the rate of convergence to ε-solution.

84

Corollary K.8. Let the assumptions of Theorem K.4 hold and µ > 0. Then after K iterations of D-SGD-DIANA with the stepsize

1

1

γ0 = min

,

, R0 = x0 − x∗ ,

4L(1 + 14ω/3n) 8L 2τ (1 + τ + 10ω/n)

F˜ = 12Lωτ (2 + α)γ02 , T˜0 = R2 + M γ2σ2,

1

nα

0

10 0







 

  

ln

max

 2,

min



(

T˜

0

+γ

0

F˜1

σ

2 0

)µ2

K

2

,

( ) T˜0+γ0F˜1σ02 µ3K3



  













 D1+M1D2

3τ L D1+ 2B1αD2  

γ = min γ0, µK





























and α ≤ min ω+1 1 , 21 we have E f (x¯K ) − f (x∗) of order

 O LR02

ω 1+ +
n

ω τ τ+
n





 
exp − min   L

µ 1 + ωn + τ

τ + ωn

 D + M1D2

τL

D1

+

B1 D2 α



+O  1

+

.

µK

µ2 K 2

 

1

 

,

K 

1 + ω  



That is, to achive E f (x¯K ) − f (x∗) ≤ ε D-SGD-DIANA requires



L

ωL

ω (ω + 1) 1 + ωn D

O ω + 1 + + τ τ + +

+

µ

nµ

n

nµε

Lτ (ω + 1) 1 + ωn D 

√



µ nε

iterations.
Applying Lemma D.3 we get the complexity result in the case when µ = 0. Corollary K.9. Let the assumptions of Theorem K.4 hold and µ = 0. Then after K iterations of D-SGD-DIANA with the stepsize

1

1

γ0 = min

,

, R0 = x0 − x∗ ,

4L(1 + 14ω/3n) 8L 2τ (1 + τ + 10ω/n)





 
γ = min γ0,
 

R02 , 3

R02nα

,

M1σ02 12Lωτ (2 + α)σ02

R02 , 3 (D1 + M1D2)K 3τ L

R02

D1

+

2B1 D2 α

 
K 

we have E f (x¯K ) − f (x∗) of order



L 1 + ωn R02 L

O

+

K

τ τ + ωn R02 +
K



R02ω√(1 + ω)σ02 + 3 R04Lτ√ω(1 + ω)σ02 

nK

3 nK

 +O 

(1 + ω) 1 + ω R2D

3 R4τ L(1 + ω)

1+ ω

 D

0

n

n 0+

.

nK

n1/3K 2/3

85

Algorithm 15 D-SGDsr

Input: learning rate γ > 0, initial vector x0 ∈ Rd 1: Set e0i = 0 for all i = 1, . . . , n 2: for k = 0, 1, . . . do

3: Broadcast xk−τ to all workers

4: for i = 1, . . . , n in parallel do

5:

Sample gik−τ = ∇fξi (xk−τ ) − ∇fi(x∗)

6:

vik = γgik−τ

7:

eki +1 = eki + γgik − vik

8: end for

9:

ek = n1

n i=1

eki

,

gk

=

1 n

n i=1

gik

,

vk

=

1 n

n i=1

vik

=

1 n

10: xk+1 = xk − vk

11: end for

n i=1

∇fξi

(xk−τ

)

That is, to achive E f (x¯K ) − f (x∗) ≤ ε D-SGD-DIANA requires



L 1 + ωn R02 L

O

+

ε

τ τ + ωn R02 +
ε



R02ω√(1 + ω)σ02 + 3 R04Lτ√ω(1 + ω)σ02 

nε

3 nε



(1 + ω) 1 + ωn R02D R02

+O 

+

nε2

τ L(1 + ω) 1 + ωn n1/2 ε3/2

 D


iterations.

K.5 D-SGDsr

In this section we consider the same settings as in Section J.1, but this time we consider delayed updates. Moreover, in this section we need slightly weaker assumption.
Assumption K.1 (Expected smoothness). We assume that function f is L-smooth in expectation w.r.t. distribution D, i.e., there exists constant L = L(f, D) such that

ED ∇fξ(x) − ∇fξ(x∗) 2 ≤ 2L (f (x) − f (x∗))

(135)

for all i ∈ [n] and x ∈ Rd.

Lemma K.5. For all k ≥ 0 we have E gk 2 | xk ≤ 4L f (xk) − f (x∗) + 2ED

∇fξ(x∗) 2 .

(136)

Proof. Applying straightforward inequality a + b 2 ≤ 2 a 2 + 2 b 2 for a, b ∈ Rd we get

E gk 2 | xk


n

2

1 = E
n

∇fξi (xk) − ∇fi(x∗) | xk

i=1

(31)
≤
(135)
≤

2ED ∇fξ(xk) − ∇fξ(x∗) 2 + 2ED ∇fξ(x∗) − ∇f (x∗) 2 4L f (xk) − f (x∗) + 2ED ∇fξ(x∗) 2 .

Theorem K.5. Assume that f (x) is µ-quasi strongly convex, L-smooth and Assumption K.1 holds. Then D-SGDsr satisﬁes Assumption 3.4 with
A = 2L, B1 = B2 = 0, D1 = 2ED ∇fξ(x∗) 2, σ12,k ≡ σ22,k ≡ 0 ρ1 = ρ2 = 1, C1 = C2 = 0, D2 = 0, G = 0, F1 = F2 = 0, D3 = 6γτ LED ∇fξ(x∗) 2
86

with γ satisfying

1

1

γ ≤ min ,

8L 8 Lτ (Lτ + 2L)

and for all K ≥ 0

E f (x¯K ) − f (x∗) ≤ 1 − γ2µ K 4 x0 −γ x∗ 2 + 8γ(1 + 3γτ L)ED ∇fξ(x∗) 2

when µ > 0 and

E f (x¯K ) − f (x∗) ≤ 4 x0γ−Kx∗ 2 + 8γ(1 + 3γτ L)ED ∇fξ(x∗) 2

when µ = 0.

In other words, D-SGDsr converges with linear rate O

√
Lµ + LLτµ+L2τ2 ln 1ε to the neigh-

bourhood of the solution when µ > 0. Applying Lemma D.2 we establish the rate of

convergence to ε-solution.

Corollary K.10. Let the assumptions of Theorem K.5 hold and µ > 0. Then after K iterations of D-SGDsr with the stepsize

1

1

γ0 = min

,

, R0 = x0 − x∗ ,

8L 8 Lτ (Lτ + 2L)

 ln max 2, min R02µ2K2 , R02µ3K3



 γ = min γ0,

D1

3τ LD1



 µK 

we have E f (x¯K ) − f (x∗) of order

O R02 L + L2τ 2 + LLτ exp − τµL K + ED ∇µfKξ(x∗) 2 + Lτ EDµ∇2Kfξ2(x∗) 2 .

That is, to achive E f (x¯K ) − f (x∗) ≤ ε D-SGDsr requires

√

O L + L2τ 2 + LLτ + ED ∇fξ(x∗) 2 +

µ

µε

Lτ ED ∇fξ(x∗) 2 √
µε

iterations.

Applying Lemma D.3 we get the complexity result in the case when µ = 0.
Corollary K.11. Let the assumptions of Theorem K.5 hold and µ = 0. Then after K iterations of D-SGDsr with the stepsize

1

1

γ = min ,

,

8L 8 Lτ (Lτ + 2L)

x0 − x∗ 2 , 3 D1K

x0 − x∗ 2 3Lτ D1K

we have E f (x¯K ) − f (x∗) of order

√

O LR02 + L2τ 2 + LLτ R02 +

K

K

R02τ ED

∇fξ (x∗ )

2
+

3 LR04τ ED

∇fξ (x∗ )

2

K

K 2/3

where R0 = x0 − x∗ . That is, to achive E f (x¯K ) − f (x∗) ≤ ε D-SGDsr requires

LR02

√ L2τ 2 + LLτ R02

R02ED ∇fξ(x∗) 2

R02 Lτ ED ∇fξ(x∗) 2

O ε + ε + ε2 + ε3/2

iterations.

87

Algorithm 16 D-LSVRG

Input: learning rate γ > 0, initial vector x0 ∈ Rd 1: Set e0i = 0 for all i = 1, . . . , n 2: for k = 0, 1, . . . do

3: Broadcast xk−τ to all workers

4: for i = 1, . . . , n in parallel do

5:

Pick l uniformly at random from [m]

6:

Set gik−τ = ∇fil(xk−τ ) − ∇fil(wik−τ ) + ∇fi(wik−τ )

7:

vik = γgik−τ

8:

eki +1 = eki + γgik − vik

9:

wk−τ+1 = xk−τ , with probability p,

i

wik−τ , with probability 1 − p

10: end for

11:

ek = n1

n i=1

eki

,

gk

=

1 n

n i=1

gik

,

vk

=

1 n

n i=1

vik

12: xk+1 = xk − vk

13: end for

K.6 D-LSVRG

In the same settings as in Section J.6 we now consider a new method called D-LSVRG which is another modiﬁcation of LSVRG that works with delayed updates.

Lemma K.6. For all k ≥ 0, i ∈ [n] we have

E gik | xk = ∇fi(xk)

(137)

and

E gk 2 | xk ≤ 4L f (xk) − f (x∗) + 2σk2,

where

σk2

=

1 nm

n i=1

n j=1

∇fij (wik) − ∇fij (x∗) 2.

(138)

Proof. First of all, we derive unbiasedness of gik:

E gik | xk

1m =
m j=1

∇fij (xk) − ∇fij (wik) + ∇fi(wik)

= ∇fi(xk).

Next, we estimate the second moment of gk:

E gk 2 | xk


n

2

1 = E
n

∇fil(xk) − ∇fil(wik) + ∇fi(wik) 

i=1


n

2

1 = E
n

∇fil(xk) − ∇fil(x∗) + ∇fil(x∗) − ∇fil(wik) + ∇fi(wik) − ∇fi(x∗) 

i=1

(3≤1) 2 n E ∇fil(xk) − ∇fil(x∗) 2 | xk n
i=1

2n

+

E

n i=1

∇fil(wik) − ∇fil(x∗) − ∇fi(wik) − ∇fi(x∗) 2 | xk

(34) 2 n m k

∗2 2

k

∗2 k

≤ nm

∇fij(x ) − ∇fij(x )

+E n

∇fil(wi ) − ∇fil(x )

|x

i=1 j=1

(11) 4L n m k ∗ 2 n m k

∗2

≤ nm

Dfij (x , x ) + nm

∇fij(wi ) − ∇fij(x )

i=1 j=1

i=1 j=1

= 4L f (xk) − f (x∗) + 2σk2.

88

Lemma K.7. For all k ≥ 0, i ∈ [n] we have

where

σk2

=

1 nm

E σk2+1 | xk ≤ (1 − p)σk2 + 2Lp f (xk) − f (x∗) ,

n i=1

n j=1

∇fij (wik) − ∇fij (x∗) 2.

(139)

Proof. The proof is identical to the proof of Lemma J.8.

Theorem K.6. Assume that f (x) is µ-quasi strongly convex and functions fij are convex and L-smooth for all i ∈ [n], j ∈ [m]. Then D-LSVRG satisﬁes Assumption 3.4 with

A = 2L,

B1 = 0,

B2 = 2,

D1 = 0,

2

2

1 nm

σ2,k = σk = nm

i=1 j=1

∇fij (wik) − ∇fij (x∗) 2,

σ12,k ≡ 0, ρ1 = 1, ρ2 = p, C1 = 0, C2 = Lp, D2 = 0,

G = 0,

F1 = 0,

12γ2Lτ (2 + p)

F2 =

, p

D3 = 0

with γ satisfying

3

1

8

γ ≤ min 56L , 8L τ (2 + τ + 4/(1−p)) , M2 = 3p

and for all K ≥ 0

E f (x¯K ) − f (x∗) ≤

1 − min

γµ p ,

24

K 4(T 0 + γF2σ02) γ

when µ > 0 and

when

µ

=

0,

where

Tk

def
=

E f (x¯K ) − f (x∗) ≤ 4(T 0 + γF2σ02) γK
x˜k − x∗ 2 + M2γ2σk2.

In other words, D-LSVRG converges with linear rate O p1 + κ τ τ + (1−1 p) ln 1ε to
the exact solution when µ > 0. If m ≥ 2 then taking p = m1 we get that in expectation the sample complexity of one iteration of D-LSVRG is O(1) gradients calculations per node as for D-SGDsr with standard sampling and the rate of convergence to the exact solution becomes O (m + κτ ) ln 1ε . Applying Lemma D.3 we get the complexity result in the case when µ = 0.
Corollary K.12. Let the assumptions of Theorem K.6 hold and µ = 0. Then after K iterations of D-LSVRG with the stepsize

3

1

γ = min

,

,

56L 8L τ (2 + τ + 4/(1−p))

x0 − x∗ 2

x0 − x∗ 2p

M σ2 , 3 12Lτ (2 + p)σ2

20

0

and p = m1 , m ≥ 2 we have E f (x¯K ) − f (x∗) of order

O Lτ R02 + K

R02mσ02 + 3 R04Lτ σ02

K

K

where R0 = x0 − x∗ . That is, to achive E f (x¯K ) − f (x∗) ≤ ε D-LSVRG requires

O Lτ R02 + ε

R02mσ02 + 3 R04Lτ σ02

ε

ε

iterations.

89

Algorithm 17 D-QLSVRG

Input: learning rate γ > 0, initial vector x0 ∈ Rd 1: Set e0i = 0 for all i = 1, . . . , n 2: for k = 0, 1, . . . do

3: Broadcast xk−τ to all workers

4: for i = 1, . . . , n in parallel do

5:

Pick l uniformly at random from [m]

6:

Set gˆik−τ = ∇fil(xk−τ ) − ∇fil(wik−τ ) + ∇fi(wik−τ )

7:

Set gik−τ = Q(gˆik−τ ) (quantization is performed independently

8:

vik = γgik−τ

9:

eki +1 = eki + γgik − vik

10:

wk−τ+1 = xk−τ , with probability p,

i

wik−τ , with probability 1 − p

11: end for

12:

ek = n1

n i=1

eki

,

gk

=

1 n

n i=1

gik

,

vk

=

1 n

n i=1

vik

13: xk+1 = xk − vk

14: end for

from

other

nodes)

K.7 D-QLSVRG

In this section we add a quantization to D-LSVRG. Lemma K.8. For all k ≥ 0, i ∈ [n] we have
E gik | xk = ∇fi(xk) and

E

gk 2 | xk ≤ 4L

2ω 1+

n

f (xk) − f (x∗) + 2

2ω 1+

n

where

σk2

=

1 nm

n i=1

n j=1

∇fij (wik) − ∇fij (x∗) 2.

2 2ω n σk + n2
i=1

∇fi(x∗) 2,

Proof.

First E

of all, we gik | xk

derive unbiasedness of gik:

(35)
=

E EQ

Q(gˆik )

| xk

(26)
=E

gˆk | xk

i

1m

=

∇fij (xk) − ∇fij (wik) + ∇fi(wik)

m

j=1

= ∇fi(xk).

Next, we estimate the second moment of gk:

EQ gk 2


n

2

1 = EQ

Q(gˆk )

 n

i

i=1


n

2

n

2

(34)

1

= EQ

Q(gˆk) − gˆk

1 +

gˆk .

 n

i

i n

i

i=1

i=1

Since quantization on nodes is performed independently we can decompose the ﬁrst term from the last row of the previous inequality into the sum of variances:

EQ gk 2

n

n

2

1 =

EQ Q(gˆk) − gˆk 2 + 1 gˆk

n2

i

i

n

i

i=1

i=1

(26) ω n

1n

2

≤ n2

gˆik 2 + n

gˆik − ∇fi(x∗)

i=1

i=1

(31) 2ω 1 n k

∗ 2 2ω n

∗2

≤ 1+ nn

gˆi − ∇fi(x ) + n2

∇fi(x ) .

i=1

i=1

90

Taking conditional mathematical expectation E · | xk from the both sides of previous inequality we get

E gk 2 | xk ≤

2ω 1+
n

2n E
n i=1

∇fil(xk) − ∇fil(x∗) 2 | xk

2ω + 1+
n

2n E
n i=1

∇fil(wik) − ∇fil(x∗) − ∇fi(wik) − ∇fi(x∗) 2 | xk

2ω n + n2
i=1

∇fi(x∗) 2

≤ 1 + 2ω 2 n m ∇fij (xk) − ∇fij (x∗) 2 n nm
i=1 j=1

2ω + 1+
n

2n E
n i=1

∇fil(wik) − ∇fil(x∗) 2 | xk

2ω n + n2
i=1

∇fi(x∗) 2

(11)

2ω 4L n m

≤ 1+

Df (xk, x∗)

n nm

ij

i=1 j=1

2ω + 1+
n

2 nm nm i=1 j=1

2 2ω n ∇fij (wik) − ∇fij (x∗) + n2
i=1

∇fi(x∗) 2

2ω = 4L 1 +
n

f (xk) − f (x∗) + 2

2ω 1+
n

2 2ω n σk + n2
i=1

∇fi(x∗) 2.

Lemma K.9. For all k ≥ 0, i ∈ [n] we have

where

σk2

=

1 nm

E σk2+1 | xk ≤ (1 − p)σk2 + 2Lp f (xk) − f (x∗) ,

n i=1

n j=1

∇fij (wik) − ∇fij (x∗) 2.

(140)

Proof. The proof is identical to the proof of Lemma J.8.

Theorem K.7. Assume that f (x) is µ-quasi strongly convex and functions fij are convex and L-smooth for all i ∈ [n], j ∈ [m]. Then D-QLSVRG satisﬁes Assumption 3.4 with

2ω

2ω

2ω n

∗2

2

A = 2L 1 + n

,

B1 = 0,

B2 = 2

1+ n

,

D1 = n2

∇fi(x ) , σ1,0 ≡ 0,

i=1

2

2

1 nm

σ2,k = σk = nm

i=1 j=1

∇fij (wik) − ∇fij (x∗) 2,

ρ1 = 1,

ρ2 = p,

C2 = Lp,

D2 = 0,

C1 = 0, G = 0, with γ satisfying

F1 = 0,

12γ2Lτ 1 + 2ω (2 + p)

F2 =

n

,

p

6γτ Lω n

∗2

D3 = n2

∇fi(x )

i=1

3

1

γ ≤ min

,

8 1 + 2ω

, M2 =

n

56L(1 + 2ω/n) 8L τ (τ + 2 (1 + 2ω/n) (1 + 2/(1−p)))

3p

and for all K ≥ 0

E f (x¯K ) − f (x∗) ≤

1 − min

γµ p ,

24

K 4(T 0 + γF2σ02)

γ

+ 4γ (D1 + D3)

when µ > 0 and

E f (x¯K ) − f (x∗) ≤ 4(T 0 +γKγF2σ02) + 4γ (D1 + D3)

when

µ

=

0,

where

Tk

def
=

x˜k − x∗ 2 + M2γ2σk2.

91

In other words, D-QLSVRG converges with linear rate

1

ω

ω

1

1

O

+κ 1+ +κ τ τ + 1+

1+

ln

p

n

n

(1 − p)

ε

to neighbourhood the solution when µ > 0. If m ≥ 2 then taking p = m1 we get that in expectation the sample complexity of one iteration of D-QLSVRG is O(1) gradients calculations per node as for D-QSGDsr with standard sampling and the rate of convergence to the neighbourhood of the solution becomes

ω

ω

1

O m+κ 1+ +κ τ τ +

ln .

n

n

ε

Applying Lemma D.2 we establish the rate of convergence to ε-solution.

Corollary K.13. Let the assumptions of Theorem K.7 hold, fξ(x) are convex for each ξ and µ > 0. Then after K iterations of D-QLSVRG with the stepsize

3

1

γ0 = min

,

, R0 = x0 − x∗ ,

56L(1 + 2ω/n) 8L τ (τ + 2 (1 + 2ω/n) (1 + 2/(1−p)))

 ln max 2, min R02µ2K2 , R02µ3K3



 γ = min γ0,

D1

3τ LD1



 µK 

and p = m1 , m ≥ 2 we have E f (x¯K ) − f (x∗) of order





O LR02 1 + ωn + τ τ + ωn

µ

exp −

 ω+ τ τ+ω

L 1+ n

n

That is, to achive E f (x¯K ) − f (x∗) ≤ ε D-QLSVRG requires





K + D1 + Lτ D1  .  µK µ2K2 

O

L

ω

1+

L +

ω τ τ+

+ D1 +

L√τ D1

µ

nµ

n µε µ ε

iterations.

Applying Lemma D.3 we get the complexity result in the case when µ = 0.
Corollary K.14. Let the assumptions of Theorem K.7 hold and µ = 0. Then after K iterations of D-QLSVRG with the stepsize

3

1

γ0 = min

,

, R0 = x0 − x∗ ,

56L(1 + 2ω/n) 8L τ (τ + 2 (1 + 2ω/n) (1 + 2/(1−p)))

γ = min γ0,

R02 , 3

R02p

,

M2σ02 12Lτ 1 + 2nω (2 + p)

R02 , 3 R02 D1K 3Lτ D1K

and p = m1 , m ≥ 2 we have E f (x¯K ) − f (x∗) of order



LR02 1 + ωn + τ τ + ωn

O



K



R02m 1 + ωn σ02 3 R04Lτ m 1 + ωn

+

+



K

K



+O R02D1 + 3 LR04τ D1 .

K

K 2/3

That is, to achive E f (x¯K ) − f (x∗) ≤ ε D-QLSVRG requires

 LR02
O 

1 + ωn + ε

τ τ + ωn



R02m 1 + ωn σ02 3 R04Lτ m 1 + ωn

+

+



ε

ε



iterations.

+O R02D1 + R02 Lτ D1

ε2

ε3/2

92

Algorithm 18 D-QLSVRGstar

Input: learning rate γ > 0, initial vector x0 ∈ Rd 1: Set e0i = 0 for all i = 1, . . . , n 2: for k = 0, 1, . . . do

3: Broadcast xk−τ to all workers

4: for i = 1, . . . , n in parallel do

5:

Pick l uniformly at random from [m]

6:

Set gˆik−τ = ∇fil(xk−τ ) − ∇fil(wik−τ ) + ∇fi(wik−τ )

7:

Set gik−τ = Q(gˆik−τ − ∇fi(x∗)) (quantization is performed

other nodes)

8:

vik = γgik−τ

9:

eki +1 = eki + γgik − vik

10:

wk−τ+1 = xk−τ , with probability p,

i

wik−τ , with probability 1 − p

11: end for

12:

ek = n1

n i=1

eki

,

gk

=

1 n

n i=1

gik

,

vk

=

1 n

n i=1

vik

13: xk+1 = xk − vk

14: end for

independently

from

K.8 D-QLSVRGstar
Now we assume that i-th node has an access to ∇fi(x∗) and modify D-QLSVRG in order to get convergence asymptotically to the exact optimum. Lemma K.10. For all k ≥ 0, i ∈ [n] we have

E gk | xk = ∇f (xk)

(141)

and

E

gk 2 | xk

≤ 2L

ω 1+

n

f (xk) − f (x∗) + 2 1 + ωn σk2,

where

σk2

=

1 nm

n i=1

n j=1

∇fij (wik) − ∇fij (x∗) 2.

(142)

Proof. First of all, we derive unbiasedness of gik:

E gk | xk

(35)

1n

(26)

1n

= E EQ

Q(gˆik − ∇fi(x∗)) | xk = E

gˆik − ∇fi(x∗) | xk

n

n

i=1

i=1

1 nm

=

∇fij(xk) − ∇fij(wik) + ∇fi(wik) = ∇f (xk).

nm

i=1 j=1

Next, we estimate the second moment of gk:

EQ gk 2


n

2

1 = EQ

Q(gˆk − ∇fi(x∗))

 n

i



i=1



(34)

1n

= EQ 

Q(gˆik − ∇fi(x∗)) − gˆik − ∇fi(x∗)

n

i=1

2 +

n

2

1 gˆk − ∇fi(x∗) .

n

i

i=1

93

Since quantization on nodes is performed independently we can decompose the ﬁrst term from the last row of the previous inequality into the sum of variances:

EQ gk 2

n

n

2

1 =

EQ Q(gˆk − ∇fi(x∗)) − gˆk − ∇fi(x∗) 2 + 1 gˆk − ∇fi(x∗)

n2

i

i

n

i

i=1

i=1

(26) ω n

1n 2

≤ n2

gˆik − ∇fi(x∗) 2 + n

gˆik − ∇fi(x∗)

i=1

i=1

(31) ω 1 n k

∗2

≤ 1+ nn

gˆi − ∇fi(x ) .

i=1

Taking conditional mathematical expectation E · | xk from the both sides of previous inequality and using the bound

1n E
n i=1

gˆik − ∇fi(x∗) 2 | xk ≤ 4L f (xk) − f (x∗) + 2σk2

implicitly obtained in the proof of Lemma K.8 we get (142).

Lemma K.11. For all k ≥ 0, i ∈ [n] we have

where

σk2

=

1 nm

E σk2+1 | xk ≤ (1 − p)σk2 + 2Lp f (xk) − f (x∗) ,

n i=1

n j=1

∇fij (wik) − ∇fij (x∗) 2.

(143)

Proof. The proof is identical to the proof of Lemma J.8.

Theorem K.8. Assume that f (x) is µ-quasi strongly convex and functions fij are convex and L-smooth for all i ∈ [n], j ∈ [m]. Then D-QLSVRGstar satisﬁes Assumption 3.4 with

A = 2L 1 + 2nω , B1 = 0, B2 = 2 1 + 2nω , D1 = 0, σ12,0 ≡ 0,

2

2

1 nm

σ2,k = σk = nm

i=1 j=1

∇fij (wik) − ∇fij (x∗) 2,

ρ1 = 1,

ρ2 = p,

C2 = Lp,

D2 = 0,

C1 = 0,

G = 0,

F1 = 0,

12γ2Lτ 1 + 2ω (2 + p)

F2 =

n

,

p

D3 = 0

with γ satisfying

3

1

γ ≤ min

,

8 1 + 2ω

, M2 =

n

56L(1 + 2ω/n) 8L τ (τ + 2 (1 + 2ω/n) (1 + 2/(1−p)))

3p

and for all K ≥ 0

E f (x¯K ) − f (x∗) ≤

1 − min

γµ p ,

24

K 4(T 0 + γF2σ02) γ

when µ > 0 and

E f (x¯K ) − f (x∗) ≤ 4(T 0 + γF2σ02) γK

when

µ

=

0,

where

Tk

def
=

x˜k − x∗ 2 + M2γ2σk2.

In other words, D-QLSVRGstar converges with linear rate

1

ω

ω

1

1

O

+κ 1+ +κ τ τ + 1+

1+

ln

p

n

n

(1 − p)

ε

94

to the exact solution when µ > 0. If m ≥ 2 then taking p = m1 we get that in expectation the sample complexity of one iteration of D-QLSVRGstar is O(1) gradients calculations per node

as for D-QSGDsr with standard sampling and the rate of convergence to the exact solution

becomes

ω

ω

1

O m+κ 1+ +κ τ τ +

ln .

n

n

ε

Applying Lemma D.3 we get the complexity result in the case when µ = 0.
Corollary K.15. Let the assumptions of Theorem K.8 hold and µ = 0. Then after K iterations of D-QLSVRGstar with the stepsize

3

1

γ0 = min

,

, R0 = x0 − x∗ ,

56L(1 + 2ω/n) 8L τ (τ + 2 (1 + 2ω/n) (1 + 2/(1−p)))

γ = min γ0,

R02 , 3

R02p

M2σ02 12Lτ 1 + 2nω (2 + p)

and p = m1 , m ≥ 2 we have E f (x¯K ) − f (x∗) of order



LR02 1 + ωn + τ τ + ωn

O



K



R02m 1 + ωn σ02 3 R04Lτ m 1 + ωn

+

+

.

K

K



That is, to achive E f (x¯K ) − f (x∗) ≤ ε D-QLSVRGstar requires

 LR02
O 

1 + ωn + ε

τ τ + ωn



R02m 1 + ωn σ02 3 R04Lτ m 1 + ωn

+

+



ε

ε



iterations.
However, such convergence guarantees are obtained under very restrictive assumption: the method requires to know vectors ∇fi(x∗).

K.9 D-LSVRG-DIANA

In the setup of Section K.6 we construct a new method with delayed updates and quantization called D-LSVRG-DIANA which does not require to know ∇fi(x∗) and has linear convergence to the exact solution.
Lemma K.12. Assume that fij(x) is convex and L-smooth for all i = 1, . . . , n, j = 1, . . . , m. Then, for all k ≥ 0 we have

E gk | xk E gk 2 | xk

= ∇f (xk), 2ω
≤ 4L 1 + n

f (xk) − f (x∗) + 2nω σ12,k + 2

2ω 1+
n

(144) σ22,k (145)

where

σ12,k

=

1 n

n i=1

hki − ∇f (x∗)

2

and

σ22,k

=

1 nm

n i=1

m j=1

∇fij (wik) − ∇fij (x∗) 2.

Proof. First of all, we show unbiasedness of gk:

E gk | xk

(35)

1n

= hk +

EE

∆ˆ k

| xk

(26)
=

hk

+

1

n
E gˆk − hk | xk

n

Qi

n

i

i

i=1

i=1

1 nm

=

∇fij(xk) − ∇fij(wik) + ∇fi(wik) = ∇f (xk).

nm

i=1 j=1

95

Algorithm 19 D-LSVRG-DIANA

Input:

learning

rates

γ

> 0,

α ∈ (0, 1],

initial

vectors

x

0

,

h01

,

.

.

.

,

h

0 n

∈ Rd

1: Set e0i = 0 for all i = 1, . . . , n

2: Set h0 = n1

n i=1

h0i

3: for k = 0, 1, . . . do

4: Broadcast xk−τ to all workers

5: for i = 1, . . . , n in parallel do

6:

Pick l uniformly at random from [m]

7:

Set gˆik−τ = ∇fil(xk−τ ) − ∇fil(wik−τ ) + ∇fi(wik−τ )

8:

∆ˆ ki −τ = Q(gˆik−τ − hki −τ ) (quantization is performed independently

nodes)

9:

gik−τ = hki −τ + ∆ˆ ki −τ

10:

vik = γgik−τ

11:

eki +1 = eki + γgik − vik

12:

hki −τ +1 = hki −τ + α∆ˆ ki −τ

13: end for n

14:

ek = n1

n i=1

eki

,

gk

=

1 n

n i=1

gik

=

hk +

1 n

∆ˆ ki ,

vk

=

1 n

n i=1

vik

i=1

γ n
15:

n i=1

∆ˆ ki −τ

n

n

hk−τ +1 = n1 hki −τ +1 = hk−τ + α n1 ∆ˆ ki −τ

i=1

i=1

16: xk+1 = xk − vk

17: end for

from other = γhk−τ +

Next, we derive the upper bound for the second moment of gk:

EQ gk 2



n

2

= EQ  hk + 1 n

∆ˆ ki 

i=1



(34)
=E

1 n ∆ˆ k − gˆk + hk

Q n

i

i

i

i=1

2 +

n

2

1 gˆk .

n

i

i=1

Since quantization on nodes is performed independently we can decompose the ﬁrst term from the last row of the previous inequality into the sum of variances:

EQ gk 2

≤
(26),(31)
≤
(31)
≤

1n n2 EQ
i=1

n

2

∆ˆ k − gˆk + hk 2 + 1

gˆk − ∇fi(x∗)

i

i

i

n

i

i=1

ωn n2
i=1

k k2 1 n

gˆi − hi

+ n

i=1

gˆik − ∇fi(x∗) 2

2ω 1+
n

1n n i=1

k

∗ 2 2ω n

gˆi − ∇fi(x ) + n2

i=1

hki − fi(x∗) 2.

Taking mathematical expectation E · | xk from the both sides of the previous inequality and using the bound

1n E
n i=1

k

∗2 k

k ∗ 2 nm k

∗2

gˆi − ∇fi(x )

|x

≤ 4L f (x ) − f (x ) + nm

∇fij(wi ) − ∇fij(x )

i=1 j=1

implicitly obtained in the proof of Lemma K.8 we get (145).

Lemma K.13. Assume that α ≤ 1/(ω+1). Then, for all k ≥ 0 we have

E σ12,k+1 | xk ≤ (1 − α)σ12,k + 6Lα(f (xk) − f (x∗)) + 2ασ22,k,

96

where

σ12,k

=

1 n

E σ22,k+1 | xk ≤ (1 − p)σk2,2 + 2Lp f (xk) − f (x∗)

n i=1

hki − ∇fi(x∗)

2

and

σ22,k

=

1 nm

n i=1

m j=1

∇fij (wik) − ∇fij (x∗) 2.

Proof. The proof is identical to the proof of Lemma J.12.

Theorem K.9. Assume that fij(x) is convex and L-smooth for all i = 1, . . . , n, j = 1, . . . , m and f (x) is µ-quasi strongly convex. Then D-LSVRG-DIANA satisﬁes Assumption 3.4 with

2ω

2ω

2ω

A = 2L 1 + n

,

B1 =

, n

B2 = 2

1+ n

,

D1 = 0,

2

1n

σ1,k = n

i=1

hki − ∇fi(x∗) 2,

2

1 nm

σ2,k = nm

i=1 j=1

∇fij (wik) − ∇fij (x∗) 2,

ρ1 = α, ρ2 = p, C1 = 3Lα, C2 = Lp, D2 = 0, G = 2,

12γ2Lωτ (2 + α)

12γ2τ L(2 + p) 4ω

2ω

F1 = nα , F2 = p

+1+

,

n(1 − α)

n

D3 = 0

with γ and α satisfying









 

1

1

 

γ ≤ min 8L 37 + 24ω ,

,

 9 3n 8L τ 2 + τ + 1−4 p + 4nω 1 + 1−3α + 1−2 p + (1−α)4(1−p) 

1

α≤

,

ω+1

8ω M1 = 3nα ,

8 7 + 6ω

M2 =

n.

9p

and for all K ≥ 0

E f (x¯K ) − f (x∗) ≤

1 − min

γµ α p ,,

2 44

K 4(T 0 + γF1σ12,0 + γF2σ22,0) γ

when µ > 0 and

E f (x¯K ) − f (x∗) ≤ 4(T 0 + γF1σ12,0 + γF2σ22,0) γK

when

µ

=

0,

where

Tk

def
=

x˜k − x∗ 2 + M1γ2σ12,k + M2γ2σ22,k.

In other words, if m ≥ 2, p = 1/m, α = min ω+1 1 , 12 and









 

1

1

 

γ ≤ min 8L 37 + 24ω ,

,

 9 3n 8L τ 2 + τ + 1−4 p + 4nω 1 + 1−3α + 1−2 p + (1−α)4(1−p) 

D-LSVRG-DIANA converges with the linear rate

ω

ω

1

O ω+m+κ 1+ +κ τ τ +

ln

n

n

ε

to the exact solution when µ > 0.

Applying Lemma D.3 we get the complexity result in the case when µ = 0.

Corollary K.16. Let the assumptions of Theorem K.9 hold and µ = 0. Then after K

iterations of D-LSVRG-DIANA with the stepsize









 

1

1

 

γ0 = min 8L 37 + 24ω ,

,

 9 3n 8L τ 2 + τ + 1−4 p + 4nω 1 + 1−3α + 1−2 p + (1−α)4(1−p) 





 
γ = min γ ,

R02

,

R02

 
,

0


M1σ12,0 + M2σ22,0

3
12τ L

ω(2+α) + 2+p

1 + 2ω +

4ω





nα

p

n n(1−α) 

97

where R0 = x0 − x∗ , α = min ω+1 1 , 21 of order



LR02 1 + ωn + τ τ + ωn

O

+



K

and p = m1 , m ≥ 2 we have E f (x¯K ) − f (x∗)

R02ω(ω + 1)σ12,0

√

+

nK

R02m

1 + ωn K

 σ22,0



 3 R04τ Lω(ω + 1)σ12,0

3 R04τ Lm

1 + ωn

 σ22,0

+O 

√

+



3 nK

K

That is, to achive E f (x¯K ) − f (x∗) ≤ ε D-LSVRG-DIANA requires

 LR02
O 

1 + ωn + ε

τ τ + ωn



R02ω(ω + 1)σ12,0

R02m 1 + ωn σ22,0

+

√

+



nε

ε



 3 R04τ Lω(ω + 1)σ12,0

3 R04τ Lm

1 + ωn

 σ22,0

+O 

√

+



3 nε

ε

iterations.

98

Table 5: The parameters for which the methods from Tables 1 and 4 satisfy Assumption 3.4. The meaning of the expressions appearing in the table, as well as their justiﬁcation is deﬁned in details in the Sections J and K. Symbols: ε = error tolerance; δ = contraction factor of compressor C; ω = variance parameter of compressor Q; κ = L/µ; L = expected smoothness constant; σ∗2 = variance of the stochastic gradients in the solution; ζ∗2 = average of ∇fi(x∗) 2; σ2 = average of the uniform bounds for the variances of stochastic gradients of workers.
99

Method

A

EC-SGDsr

2L

EC-SGD

2L

EC-GDstar

L

EC-SGD-DIANA

L

EC-SGDsr-DIANA

2L

EC-LSVRG

2L

EC-LSVRGstar

2L

EC-LSVRG-DIANA

2L

D-SGDsr D-SGD D-QSGD
D-QSGDstar D-QGDstar D-SGD-DIANA
D-LSVRG D-QLSVRG

2L 2L L 1 + 2nω L 1 + ωn L 1 + ωn L 1 + 2nω
2L
2L 1 + 2nω

D-QLSVRGstar D-LSVRG-DIANA

2L 1 + 2nω 2L 1 + 2nω

B1

B2

ρ1 ρ2

C1

C2

0

0

1

1

0

0

0

0

1

1

0

0

0

0

1

1

0

0

0

0

α

1

Lα

0

0

0

α

1 2α(3L + 4L)

0

0

2

1

p

0

Lp

0

2

1

p

0

Lp

0

2

α

p

3Lα

Lp

0

0

0

0

0

0

0

0

0

0

2nω 0

0

2

0 2 1 + 2nω

0 2 1 + 2nω 2nω 2 1 + 2nω

1

1

1

1

1

1

1

1

1

1

α

1

1

p

1

p

1

p

α

p

0

0

0

0

0

0

0

0

0

0

Lα

0

0

Lp

0

Lp

0

Lp

3Lα

Lp

F1, F2 0, 0

0, 0
0, 0 96Lγ2 δ2α(1−η) , 0
96Lγ2 δ2α(1−η) , 0 0, δ7p2(L1−γ2η)
0, 48δLpγ2 24Lγ2 4 +3
δ δα(1−η) , 24Lγ2 1−4α δ4 +3 +3 δp(1−η)
0, 0
0, 0
0, 0

0, 0 0, 0

12γ2 Lωτ (2+α)

nα

,0

12γ2Lτ (2+p)

0,

np

12γ2 Lτ 1+ 2nω τ (2+p)

0,

p

12γ2L 1+ 2nω τ (2+p)

0,

p

12γ2 Lωτ (2+α)

nα

,

12γ2τ L(2+p)

2ω(3−α)

p

1 + n(1−α)

G

D1, D2, D3

0

2σ∗2 , 0, 6Lγ 4ζ∗2 + 3σ2

n

δ

δ

∗

0

2σ∗2 , 0, 12Lγ 2ζ∗2 + σ2

n

δ

δ

∗

0

0, 0, 0

σ2 , α2(ω + 1)σ2,

0

n

6Lγ 4α(ω+1)

2

δ

δ

+1 σ

2σ∗2 , α2(ω + 1)σ2 ,

0

n

∗

18Lγ 4α(ω+1) + 1 σ2

δ

δ

∗

0 0, 0, 24δL2 γ ζ∗2

0

0, 0, 0

2

0, 0, 0

0

2σ∗2 , 0, 6Lτ γσ∗2

n

n

0

2σ∗2 , 0, 6Lτ γσ∗2

n

n

(ω+1)σ2 + 2ωζ∗2 , 0,

0

n

n

3γτ L n

(ω + 1)σ2 + 2ωζ∗2

(ω+1)σ2

3γτ L(ω+1)σ2

0

n , 0,

n

0

0, 0, 0

(ω+1)σ2

α(ω+1)σ2

0

n,

n

,

2

3γτ L 1 + 4nω

(ω+1)σ n

0

0, 0, 0

2ωζ∗2 , 0,

0

n2

6γτ Lωζ∗

n

0

0, 0, 0

0

0, 0, 0

