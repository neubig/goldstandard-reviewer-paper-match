arXiv:1707.00087v1 [math.PR] 1 Jul 2017

Sharp asymptotic and ﬁnite-sample rates of convergence of empirical measures in Wasserstein distance
Jonathan Weed∗,† and Francis Bach†
Massachusetts Institute of Technology INRIA – ENS
Abstract. The Wasserstein distance between two probability measures on a metric space is a measure of closeness with applications in statistics, probability, and machine learning. In this work, we consider the fundamental question of how quickly the empirical measure obtained from n independent samples from µ approaches µ in the Wasserstein distance of any order. We prove sharp asymptotic and ﬁnite-sample results for this rate of convergence for general measures on general compact metric spaces. Our ﬁnite-sample results show the existence of multi-scale behavior, where measures can exhibit radically diﬀerent rates of convergence as n grows.
Key words and phrases: Wasserstein metrics, quantization, optimal transport.
1. INTRODUCTION
The Wasserstein distance is a measure of the closeness of probability distributions on metric spaces which has proven extremely useful in data science and machine learning, particularly in the analysis of images [RTG00, SdGP+15, SL11] and text [KSKW15, ZLL+16]. This distance is especially useful in tasks such as classiﬁcation and clustering, since it captures geometric features of the underlying data. Moreover, unlike other measures of distance between distributions, such as the Kullback-Leibler divergence or total variation distance, the Wasserstein distance between two measures is generally ﬁnite even when neither measure is absolutely continuous with respect to the other, a situation that often arises when considering empirical distributions arising in practice.
Concretely, the Wasserstein distance measures how closely two measures can be coupled, where closeness is measured with respect to the underlying metric.
∗Supported in part by the Chaire E´conomie des nouvelles donn´ees, the data science Joint Research Initiative with the Fonds AXA pour la recherche, and the Initiative de Recherche “Machine Learning for Large-Scale Insurance” from the Institut Louis Bachelier
†Supported in part by NSF Graduate Research Fellowship 1122374
1

2

WEED AND BACH

For p ∈ [1, ∞), the Wasserstein distance of order p between two distributions µ and ν on a metric space (X, D) is deﬁned as

Wp(µ, ν) := inf
γ∈C(µ,ν)

1/p
D(x, y)pdγ(x, y) ,

where the inﬁmum is taken over all couplings γ of µ and ν, that is, distributions on X × X whose ﬁrst and second marginals agree with µ and ν, respectively [Kan42]. It can be shown that Wp is a metric on the space of probability measures on X [Vil08, Chapter 6].
In statistical contexts, direct access to a distribution of interest µ is generally not available; instead, the statistician has access to i.i.d. samples from µ, or, equivalently, to an empirical distribution µˆn. For µˆn to serve as a reasonable proxy to µ, we should insist that µˆn and µ are close in the Wasserstein sense. In the large-n limit, this is indeed the case: if X is compact and separable and µ is a Borel measure, then for any p ∈ [1, ∞),

Wp(µ, µˆn) → 0 µ-a.s.

This result follows from the fact that Wasserstein distances metrize weak conver-

gence [Vil08, Corollary 6.13] and the fact that empirical measure µˆn converges

weakly to µ almost surely [Var58].

This raises the question of quantifying the rate of convergence of µˆn to µ

in Wp distance either in expectation or with high probability. This question is

closely related to the optimal quantization problem [GL07], which asks how well

a given distribution µ can be approximated by a discrete distribution with ﬁnite

support, such as the empirical measure µˆn. This problem has wide applications

in information theory, under the name rate distortion [Sha60, CT12]; machine

learning [CR12, NE14]; and numerical methods [CP15]. Unfortunately, like many

statistics and optimization problems involving measures on Rd, the convergence

of µˆn to µ exhibits the so-called “curse of dimensionality” [Bel61]. In the high-

dimensional regime, the empirical distribution µˆn becomes less and less repre-

sentative as d becomes large [FHT01], so that in the convergence of µˆn to µ in

Wasserstein distance is slow.

This curse of dimensionality seems unavoidable. It was noted by Dudley [Dud68]

that any measure µ that is absolutely continuous with respect to the Lebesgue

measure on Rd satisﬁes

IE[W1(µ, µˆn)] n−1/d .

This lower bound is asymptotically tight: Dudley showed that, when d > 2, a compactly supported measure on Rd satisﬁes

IE[W1(µ, µˆn)] n−1/d .

These results have been sharpened over the years, culminating in a tight almost sure limit theorem due to Dobri´c and Yukich [DY95].
In short, these arguments establish that a d-dimensional measure yields a convergence rate in the W1 distance of exactly n−1/d. These results are in a sense disappointing, since they show that slow convergence is a necessary price to pay for high-dimensional data. However, they raise several questions about the behavior of the Wasserstein metric in practice:

SHARP RATES OF WASSERSTEIN CONVERGENCE

3

• When can faster rates be achieved for measures that are not absolutely continuous with respect to the Lebesgue measure?
• Under what conditions can sharper ﬁnite-sample (i.e., non-asymptotic) rates be obtained?
Our goal in this work is to answer the above questions in a very general sense. We consider a bounded metric space X subject to mild technical conditions and prove upper and lower bounds on the rate of convergence for Wp(µ, µˆn) for all p ∈ [1, ∞). Inspired by the bounds of [Dud68], we show essentially tight asymptotic convergence rates for a large class of measures. In particular, our upper and lower bounds improve on many existing results in the literature [Dud68, BLG14, DSS13, FG15], either in the generality with which they are applicable or the rates which are obtained. These results show that the rate of convergence of Wp(µ, µˆn) depends on a notion of the intrinsic dimension of the measure µ, which can be signiﬁcantly smaller than the dimension of the metric space on which µ is deﬁned.
Our second goal is to obtain ﬁnite-sample results which hold outside the asymptotic regime. A common phenomenon in practice is for a measure to exhibit different dimensional structure at diﬀerent scales; this so-called multi-scale behavior arises in a range of applications [LMR16, WDCB05, SMB98]. We show that the convergence of µˆn to µ in Wp for such measures can exhibit wildly diﬀerent rates as n increases. In particular, they can enjoy a much faster convergence rate when n is small than they do in the large-n limit. We illustrate this phenomenon via a number of examples inspired by measures that arise in practice.
In both of the above regimes, we consider exclusively the question of how the expectation IE[Wp(µ, µˆn)] behaves. Controlling this quantity suﬃces to understand the behavior of Wp(µ, µˆn) because the Wasserstein distance concentrates very well around its expectation, a fact which we prove in Section 6. Combining this observation with the bounds on we prove on IE[Wp(µ, µˆn)] yields sharp high-probability bounds.
We end by giving applications of our work to machine learning and statistics and sketch directions for future work.

2. PRELIMINARIES
In this section, we present the mild assumptions on X under which our results hold. We also give background on Wasserstein distances and compare our results to prior work.
2.1 Assumptions
We are concerned with measures on a compact metric space X. The ﬁrst assumption is entirely standard and allows us to avoid many measure-theoretic diﬃculties:

Assumption 1. The metric space X is Polish, and all measures are Borel.

Since we limit ourselves to the compact case, diam(X) is necessarily ﬁnite, and for normalization purposes we assume the following.

Assumption 2. diam(X) ≤ 1.

Assumption 2 can always be made to hold by a simple rescaling of the metric.

4

WEED AND BACH

2.2 Background on Wasserstein distances
Above, we deﬁned the Wasserstein p distance between two distributions µ and ν on (X, D) as

Wp(µ, ν) := inf
γ∈C(µ,ν)

1/p
D(x, y)pdγ(x, y) .

A second deﬁnition, due to Monge [Mon81, San15], reads as follows:

Wp(µ, ν) := inf
T :µ◦T −1=ν

1/p
D(x, T (x))pdµ(x) ,

where the inﬁmum is taken over all transports T : X → X such that the pushforward measure µ ◦ T −1 equals ν. In general, this inﬁmum in the Monge deﬁnition is not attained. This formulation has an easy geometric interpretation: the Wasserstein distance measures the cost of moving mass from the measure µ to the measure ν with respect to the metric of X.
The special case W1, which is also known as the Kantorovich-Rubinstein distance [Vil08] or earth mover distance [RTG00], has a particularly simple dual representation:

(1)

W1(µ, ν) = sup

f dµ − f dν ,

f ∈Lip(X)

where the supremum is taken over all 1-Lipschitz functions on X [KR58]. This dual representation makes W1 signiﬁcantly easier to bound [Vil08, Remark 6.6]. A more general dual formulation is also available for Wp for p = 1, but it is less simple to manipulate; more details appear in Section 6.
2.3 Related work
Our work generalizes several strands of work on the convergence rates of the empirical measure in Wasserstein distances. The ﬁrst strand, inaugurated by Dudley [Dud68], focuses on obtaining rates of convergence of µˆn to µ based on the inherent dimension of the measure µ. In that paper, Dudley obtained results matching the ones we present in Section 4 for the convergence of µˆn to µ in W1 distance, with a rate depending on the covering number of the support of µ. Dudley’s argument relied extensively on the dual characterization of W1 as a supremum over Lipschitz test functions, as in (1). As a result, his technique does not extend to Wp for p = 1.
An extension of Dudley’s techniques to other values of p appears in [BLG14]. Their approach is similar to ours, but our analysis is tighter: in the language of Section 4.2, they prove an upper bound based on the quantity dM whereas we obtain an upper bound based on the smaller quantity d∗p.
A second strand [FG15, DSS13] focuses on measures on Rd and obtains upper bounds on the rate of convergence of µˆn to µ in Wp for all p ∈ (0, ∞). The upper bounds arise from the construction of explicit couplings between µˆn and µ. The construction of these couplings depends on the fact that µ is a measure on Rd and does not extend easily to general metric spaces. Moreover, the rates obtained, while tight for measures which are absolutely continuous with respect

SHARP RATES OF WASSERSTEIN CONVERGENCE

5

to the Lebesgue measure on Rd, are not tight in general, as we show below. Nevertheless, the techniques employed in [FG15, DSS13] are very similar to those employed in [BLG14], and we follow the same approach.
We also note several other recent works [Boi11, BGV07] which have focused on obtaining tail bounds for the quantity Wp(µ, µˆn). The arguments of [BGV07] rely on transportation inequalities such as the celebrated Bobkov-G¨otze inequality [BG99]. These arguments were simpliﬁed in [Boi11], but, as noted in [BLG14], the analysis becomes much easier if the development of tail bounds is divided into two steps: an estimate of the expectation IE[Wp(µ, µˆn)] and a concentration bound showing how well Wp(µ, µˆn) concentrates near that expectation. This is the approach we adopt: bounds on the expected value appear in Section 4 and 5, and concentration bounds are obtained in Section 6.
2.4 Notation
The metric on X will always be denoted D(·, ·). Given a point x ∈ X and r > 0, denote by B(x, r) the open ball of radius r around x. The symbol log denotes the natural logarithm. The notation f (n) g(n) indicates that there exists a constant C, depending on f and g but not n, such that f (n) ≤ Cg(n) for all n.

3. DYADIC TRANSPORT
In order to prove upper bounds for the Wasserstein distance, we show how to construct an eﬃcient transport between two measures based on a recursive partitioning of the underlying space. By analogy with the dyadic intervals in R, we seek a sequence of partitions of a set such that each partition is a reﬁnement of the last, and such that the elements of the kth partition have diameter of order δk for some δ.
We formalize these requirements in the following deﬁnition [Dav88, Section A]. Denote by B(X) the Borel subsets of X.

Definition 1. A dyadic partition of a set S ⊆ X with parameter δ < 1 is a sequence {Qk}1≤k≤k∗ with Qk ⊆ B(X) possessing the following properties:
• The sets in Qk form a partition of S. • If Q ∈ Qk, then diam(Q) ≤ δk. • If Qk+1 ∈ Qk+1 and Qk ∈ Qk, then either Qk+1 ⊆ Qk or Qk+1 ∩ Qk = ∅.
That is, the (k + 1)th partition is a reﬁnement of the kth partition.
The following Proposition bounds Wpp(µ, ν) in terms of the mass µ and ν assign to elements of a dyadic partition.

Proposition 1. Let µ and ν be two Borel probability measures on X, and let S be a set such that µ(S) = ν(S) = 1. If {Qk}1≤k≤k∗ is a dyadic partition of S with parameter δ, then

k∗
Wpp(µ, ν) ≤ δk∗p + δ(k−1)p

|µ(Qki ) − ν(Qki )| .

k=1

Qki ∈Qk

6

WEED AND BACH

The upper bound in Proposition 1 arises from the explicit construction of a

coupling between µ and ν. Proposition 1 is not new and appears to have been

rediscovered many times. In particular, it is implicit in the proof of [BLG14,

Proposition 1.1], and similar results have appeared before in papers bounding the convergence of Wpp when X = Rd [DSS13, FG15]. An analogous bound has

also been used in the computer science community [IT03, BNNR11]. The idea of bounding the quantity Wpp(µ, ν) by considering the mass each measure assigns to

elements of a sequence of partitions is present also in [AKT84], where it is used to

obtain sharp results for the case X = [0, 1]2. We include a proof in Appendix A

for clarity and because we could not ﬁnd a suitably general version explicitly

stated in the literature. Proposition 1 is stated for Wpp, but can easily adapted to optimal transport
with a general cost c(·, ·) by replacing the requirement that diam(Q) ≤ δk in

Deﬁnition 1 by the requirement that supx,y∈Q c(x, y) ≤ δk.

Boissard and Le Gouic [BLG14] used a version of Proposition 1 to prove a

bound

on

W

p p

(µ

,

µˆ

n

)

based

on

the

covering

number

of

the

set

S,

a

deﬁnition

of

which appears in Section 4, below. However, their results are not sharp, and they

do not recover the rates obtained in [Dud68] for the case p = 1. In Section 4, we

show how to improve their argument to obtain sharper results, which extend the

rates from [Dud68] to all p ∈ [1, ∞).

4. ASYMPTOTIC UPPER AND LOWER BOUNDS
In this section, we show asymptotic upper and lower bounds for Wp that hold for all p ∈ [1, ∞). These bounds extend results of [Dud68] to the case p = 1 and improve the bounds of [BLG14] by focusing on a set S to which µ assigns mass of almost 1 rather than on the larger set supp(µ). We will also show a broad class of measures for which our bounds are asymptotically tight.
4.1 Deﬁnitions
To state our bounds we will deﬁne several notions of dimension of a measure.

Definition 2. Given a set S ⊆ X, the ε-covering number of S, denoted Nε(S), is the minimum m such that there exists m closed balls B1, . . . , Bm of diameter ε such that S ⊆ 1≤i≤m Bi. The ε-dimension of S is the quantity
dε(S) := log Nε(S) . − log ε
When working with measures instead of sets, it is convenient to be able to ignore a small fraction of the mass. The following deﬁnition appears in [Dud68], which notes a connection to the ε; δ entropy introduced by [PRR67].

Definition 3. Given a measure µ on X, the (ε, τ )-covering number is

Nε(µ, τ ) := inf{Nε(S) : µ(S) ≥ 1 − τ } and the (ε, τ )-dimension is
dε(µ, τ ) := log Nε(µ, τ ) . − log ε

SHARP RATES OF WASSERSTEIN CONVERGENCE

7

For convenience, let
Nε(µ) := Nε(µ, 0) , dε(µ) := dε(µ, 0) .
Note that Nε(µ) = Nε(supp(µ)), and that Nε(µ, τ ) and dε(µ, τ ) increase as τ decreases.
We now deﬁne our main notions of dimension of a measure.

Definition 4. The upper and lower Wasserstein dimensions are respectively

d∗p(µ)

=

inf{s

∈

(2p,

∞)

:

lim

sup

dε(µ,

ε

sp s−2p

)

≤

s}

,

ε→0

d∗(µ) = lim lim inf dε(µ, τ ) .
τ →0 ε→0

Note that the monotonicity of dε(µ, τ ) in τ implies that the limit in the definition of d∗(µ) exists. The deﬁnition of d∗p is complicated by the fact that the

behavior of the Wasserstein distance is very diﬀerent when the dimension is small.

For convenience we only treat the case where the dimension is larger than 2p. We

note that the monotonicity of dε(µ, τ ) in τ also implies that d∗ ≤ d∗p for all p.

Our deﬁnition of the upper Wasserstein dimension is new. Dudley [Dud68]

considered

measures

satisfying

a

bound

of

the

form

s
Nε(µ, ε s−2 )

≤

C ε−s

for

all

suﬃciently small ε; the deﬁnition of d∗p(µ) is the correct generalization to the

p = 1 case. The lower Wasserstein dimension was introduced by Young [You82],

who credits the idea to Ledrappier [Led81], in the context of dynamical systems.

The term Wasserstein dimension is ours, and is justiﬁed by Theorem 1 below.

4.2 Comparison with other notions of dimension

To make it easier to interpret the quantities d∗p(µ) and d∗(µ), we sketch here their relationship with two other well known notions of dimensions for the measure µ, the Minkowski dimension (also known as the Minkowski-Bouligand or box-counting dimension) and the Hausdorﬀ dimension. Both quantities have long been studied in fractal and metric geometry [Fal04].

Definition 5. The Minkowski dimension of a set S is the quantity

dimM (S) := lim sup dε(S) .
ε→0

The d-Hausdorﬀ measure of S is

∞

∞

Hd(S) = lim inf

rkd : S ⊆ B(xi, ri); rk ≤ ε ∀k ,

ε→0

k=1

k=1

and its Hausdorﬀ dimension is

dimH (S) := inf{d : Hd(S) = 0} .

Given a measure µ, the Minkowski and Hausdorﬀ dimensions of µ are respectively

dM (µ) := inf{dimM (S) : µ(S) = 1} , dH (µ) := inf{dimH (S) : µ(S) = 1} .

8

WEED AND BACH

We note that the quantities dM (µ) and dH (µ) are upper and lower bounds on the Wasserstein dimensions.

Proposition 2. If dM (µ) ≥ 2p, then

dH (µ) ≤ d∗(µ) ≤ d∗p(µ) . d∗p(µ) ≤ dM (µ) .

A proof appears in Appendix A. None of the inequalities in Proposition 2 can be replaced by equalities. Examples of measures µ for which dH (µ) < d∗(µ) are complicated; one appears in [KLP11, Remark 7.8]. It is much easier to ﬁnd examples in which d∗(µ), d∗p(µ), and dM (µ) do not agree. For instance, it is easy to see that d∗(µ) = 0 for any discrete measure, but the countable set S := {k−1}∞ k=1 d ⊂ [0, 1]d has Minkowski dimension d/2. By choosing d > 4p and choosing a measure µ supported on S with appropriately slow decay, one can ensure that d∗p(µ) is strictly less than d/2, and hence strictly between d∗(µ) and dM (µ).
4.3 Main result
With these deﬁnitions in place, we can state our main asymptotic bound.

Theorem 1. Let p ∈ [1, ∞). If s > d∗p(µ), then

IE[Wp(µ, µˆn)] n−1/s .

If t < d∗(µ), then

Wp(µ, µˆn) n−1/t .

The upper and lower bounds are proved below and are corollaries of more precise results with explicit constants (Propositions 5 and 6). Note that the lower bound does not merely hold in expectation. Indeed, such a lower bound holds for any discrete measure supported on at most n points.
Theorem 1 improves on several existing results. For the upper bound, Dudley [Dud68] showed that, if s > d∗1(µ), then
IE[W1(µ, µˆn)] n−1/s ,

but his proof technique applied only to p = 1. Boissard and Le Gouic [BLG14] extended this bound to all p, but only if s > dM (µ) ≥ 2p. Since d∗p(µ) ≤ dM (µ) with some measures exhibiting strict inequality, our result is sharper.
Dudley [Dud68] proved a lower bound for W1—and hence, by monotonicity of Wp in p, for Wp for all p ∈ [1, ∞)—based on the quantity

d1/2(µ) = lim inf log Nε(µ, 1/2) ,

ε→0

− log ε

which is easily seen to be smaller than d∗(µ), with strict inequality possible. Our argument is a simple extension of his.

SHARP RATES OF WASSERSTEIN CONVERGENCE

9

4.4 Proof of upper bound
The upper bound of Theorem 1 follows from Proposition 5, below. To apply the bound of Proposition 1, we need to show the existence of a suitable dyadic partition. The following Proposition is an extension of [BLG14, Lemma 2.1] and shows that we can choose a dyadic partition which provides an almost optimal covering of subsets of S.
Proposition 3. Fix S ∈ B(X). Let k∗ be any positive integer for which the covering number N3−(k∗+1)(S) is ﬁnite, and let {Sk}1≤k≤k∗ be a sequence of Borel subsets S. There exists a dyadic partition of S with parameter δ = 1/3 such that for 1 ≤ k ≤ k∗, the number of sets in Qk intersecting Sk is at most N3−(k+1)(Sk).
A proof appears in Appendix A. All the upper bounds we prove rely on the following fundamental estimate, which was used in [FG15] to provide bounds in the case where X = Rd.

Proposition 4. If S is any Borel set, then





IE 

|µ(Qki ) − µˆn(Qki )| ≤ 2(1 − µ(S)) +

Qki ∈Qk

|{i : Qki ∩ S = ∅}|/n .

Proof. Let Q(S) = {i : Qki ∩ S = ∅}, and write

S′ =

Qki .

i∈Q(S)

Since nµˆn(Qki ) is a Binomial random variable with parameters (n, µ(Qki )), we have the bound [BK13]

IE|µ(Qki ) − µˆn(Qki )| ≤ µ(Qki )/n ∧ 2µ(Qki ) .

Applying the ﬁrst bound on S′ and the second bound on X \ S′ yields





IE 

|µ(Qki ) − µˆn(Qki )| ≤ 2µ(X \ S′) +

Qki ∈Qk

i∈Q(S)

µ

(Q

k i

)/n

.

Since the second sum contains |Q(S)| terms and ﬁnal bound follows from Cauchy-Schwarz.

i∈Q(S) µ(Qki ) = µ(S′) ≤ 1, the

The key step in proving the upper bound of Theorem 1 is giving a bound for

IE[Wpp(µ,

µˆn

)]

in

terms

of

the

quantity

dε(µ,

ε

sp s−2p

),

which

appears

in

the

deﬁnition

of d∗p.

Proposition 5. such that

Let p ∈ [1, ∞). Suppose there exists an ε′ ≤ 1 and s > 2p
sp
dε(µ, ε s−2p ) ≤ s

10

WEED AND BACH

for all ε ≤ ε′. Then

IE[Wpp(µ, µˆn)] ≤ C1n−p/s + C2n−1/2 ,

where

C1

=

3

3sp s−2p

+

1

In particular,

1

3

s 2

−

p

−

1

+

3

IE[Wp(µ, µˆn)]

, and C2 = (27/ε′) 2s . n−1/s .

The assumption that s > 2p implies that the ﬁrst term in the above bound is asymptotically larger than the second term. Note also that C1 decreases as s increases, so that as long as s is bounded away from 2p, the constant C1 has no dependence on the dimension s. On the other hand, C2 does depend exponentially on s, even though the term C2n−1/2 is asymptotically negligible.
The presence of two terms in the upper bound of Proposition 5 is a consequence of the weakness of the assumption that the bound on dε holds only for ε suﬃciently small rather than for all ε. In Proposition 10, below, we remove the n−1/2 term by adopting a stronger assumption on dε.

Proof. If n < (27/ε′)s, then the second term is larger than 1, so the bound

holds

from

the

trivial

fact

that

W

p p

(µ

,

ν

)

≤

diam(X )

≤

1

for

any

measures

µ, ν

supported on X. We therefore assume that n ≥ (27/ε′)s.

For convenience, write α = sp/(s − 2p) and ℓ = ⌈ −lologg3ε′ ⌉. Let k∗ =

log n s log 3

− 2.

Let

k′

be

the

largest

integer

in

the

range

[ℓ, k∗]

satisfying

k′

≤

p α

·

slologgn3 ,

or

ℓ

if

no such integer exists.

Our assumptions imply that for all k ≥ ℓ,

N3−k (µ, 3−αk) ≤ 3ks . Hence for k ≥ k′, there exists a set Tk of mass at least 1 − 3−αk′ such that
N3−k (Tk) ≤ 3ks .
Applying Proposition 3 with Sk = Tk′ for k < k′ and Sk = Tk+1 for k ≥ k′ implies the existence of a dyadic partition {Qk}1≤k≤k∗ of X such that the number of sets of Qk intersecting Sk is at most N3−(k+1)(Sk).
Using this dyadic partition in Proposition 1 and applying Proposition 4 yields

k′−1
IE[Wpp(µ, µˆn)] ≤ 3−k∗p + 3−(k−1)p
k=1

N3−(k+1) (Tk′ ) n

k∗

+

3−(k−1)p

k=k′

N3−(k+1) (Tk+1) n

k∗
+ 2 · 3−αk′ 3−(k−1)p .

k=1

Since Nε(T ) increases as ε decreases, for k ≤ k′ − 1 we have the bound

N3−(k+1) (Tk′ ) ≤ N3−k′ (Tk′ ) ≤ 3k′s .

SHARP RATES OF WASSERSTEIN CONVERGENCE

11

By construction, the sets Tk also satisfy for k ≥ k′

N3−(k+1) (Tk+1) ≤ 3(k+1)s .

Combining these bounds with the bound

k∗ k=1

3−(k−1)p

≤

3/2

for

p

≥

1

yields

IE[Wpp(µ, µˆn)] ≤ 3−k∗p + 32 ≤ 3−k∗p + 3 2

3√k′s/2 + 2 · 3−αk′ n
3√k′s/2 + 2 · 3−αk′ n

+ 32p k∗ 3(k+√1)( 2s −p) k=k′ n

3−k∗p + 3 2s −p − 1

3(k∗+2)s . n

The choice of k∗ implies that 3(k∗+2)s ≤ n and that 3−k∗p ≤ 33pn−p/s, and the choice of k′ implies that αk′ > p slologgn3 − 3α, so that 3−αk′ < 33αn−p/s. Combining these estimates yields

IE[Wpp(µ, µˆn)] ≤

33p + 33p + 33α+1 3 2s −p − 1

n−p/s

+

3

·

3k′s/2 √

.

2n

The

deﬁnition

of

k′

implies

that

sk′

≤

m

ax

{s

ℓ

,

(

p α

·

lloogg n3 )},

so

3k′s/2 ≤ 3ℓs/2 + np/2α = 3ℓs/2 + n1/2n−p/s .

Plugging in the deﬁnitions of C1 and C2 then yields the claim.

Corollary 1. If s > d∗p(µ), then

IE[Wp(µ, µˆn)] n−1/s .

Proof.

If s > d∗p(µ), then there exists

an

ε′

such

that

dε(µ,

ε

sp s−2p

k

)

≤

s

for

all

ε ≤ ε′. Apply Proposition 5.

4.5 Proof of lower bound
Our asymptotic lower bounds involving d∗(µ) follow from a much simpler argument. One striking feature of this lower bound is that it actually holds not merely for the empirical measure µˆn but indeed for any measure ν supported on at most n atoms. That such lower bounds are often tight for empirical measures is a rather surprising fact, which has been noted several times, including in Dudley’s original paper [CR12, Klo12, DSS13, Dud68, BLG14].
The following Proposition is adapted from [Dud68] and forms the core of the lower bound.

Proposition that
for all ε ≤ ε′. If then

6. Suppose that there exist positive constants ε′, τ , and t such
Nε′(µ, τ ) ≥ ε−t n > ε′−t and ν is any measure supported on at most n points,
Wpp(µ, ν) ≥ τ 4−pn−p/t .

12

WEED AND BACH

Proof. Choose ε = n−1/t/2, and let S = x∈supp(ν) B(x, ε/2). Since Nε′(µ, τ ) ≥ ε−t > n, we must have µ(S) < 1 − τ . Therefore, if X ∼ µ, then D(X, supp(ν)) ≥ ε/2 with probability at least τ . Hence if (X, Y ) is any coupling of µ and ν,

IE[D(X, Y )p] ≥ IE[D(X, supp(ν))p] ≥ τ (ε/2)p = τ 4−pn−p/t .

Proposition 6 immediately implies the desired asymptotic lower bound.

Corollary 2. points, then

If t < d∗(µ) and ν is any measure supported on at most n Wp(µ, ν) n−1/t .

Proof. By the deﬁnition of d∗(µ), for any t < d∗(µ), there exist constants ε′ and τ as in the statement of Proposition 6. The claim follows.

4.6 Regular spaces
The remark after Proposition 2 establishes that d∗(µ) and d∗p(µ) do not agree in general. However, these dimensions do agree whenever the measure is suﬃciently well behaved. In this section, we give several broad classes of examples for which they do match, and for which our bounds are therefore sharp.
The following Proposition gives a simple condition under which this agreement occurs.

Proposition 7. Let Hd be the d-dimensional Hausdorﬀ measure on a closed set S. If µ ≪ Hd and supp(µ) ⊆ S, then for any p ∈ [1, d/2],
d ≤ d∗(µ) ≤ d∗p(µ) ≤ dM (S) .
In particular, if d = dM (S), then d∗(µ) = d∗p(µ) = d.
A proof appears in Appendix A. Proposition 7 immediately implies the result quoted in the Introduction (up to subpolynomial factors): since the set [0, 1]d satisﬁes d = dM ([0, 1]d), Theorem 1 implies that any measure µ absolutely continuous with respect to the Lebesgue measure on Rd (or, equivalently, to Hd) must satisfy
n−1/t IE[Wp(µ, µˆn)] n−1/s
for any t < d < s and p ∈ [1, d/2]. Limiting our attention to sets for which the Hausdorﬀ measure is well behaved
motivates the following deﬁnition, which appears in [GL07].

Definition 6. A set S is regular of dimension d if it is compact and there

exists constants c and r0 such that the d-dimensional Hausdorﬀ measure Hd on

S satisﬁes

1 rd ≤ Hd(B(x, r)) ≤ crd , c

for all x ∈ S.

SHARP RATES OF WASSERSTEIN CONVERGENCE

13

It is well known (see, e.g., [Mat99, Theorem 5.7]) that dM (S) = d if S is regular of dimension d. We therefore obtain the following simple characterization.

Proposition 8. If the support of µ is a regular set of dimension d and µ ≪ Hd, then for any p ∈ [1, d/2],
d∗(µ) = d∗p(µ) = d .
The following Proposition, which appears in [GL07], shows that many well behaved sets are regular, and so implies the existence of many examples for which our results are tight.

Proposition 9 ([GL07]). The following sets are regular of dimension d:
• Nonempty, compact convex sets spanned by an aﬃne space of dimension d, • Relative boundaries of nonempty, compact convex sets of dimension d + 1, • Compact d-dimensional diﬀerentiable manifolds, • Self-similar sets with similarity dimension d.
Moreover, regularity is preserved under ﬁnite unions and bi-Lipschitz maps.

5. FINITE-SAMPLE BOUNDS AND MULTISCALE BEHAVIOR
The results of Section 4 imply that for any suﬃciently regular d-dimensional measure µ, the empirical measure µˆn approaches µ in Wp at a rate of approximately n−1/d. For example, if µ is absolutely continuous with respect to the Lebesgue measure on [0, 1]d, Dudley showed that the slow n−1/d rate is unavoidable [Dud68]. Faster rates can be obtained if µ is singular: for instance, if µ is a sum of a ﬁnite number of Dirac masses, then Proposition 1 can be used to show that µˆn approaches µ at a much faster n−1/2p rate, independent of the ambient dimension.
However, what should one expect if µ is approximately a sum of Dirac masses (or, in general, approximately low dimensional)? Suppose for instance that µ is the convolution of a sum of Dirac masses with an isotropic Gaussian of small variance. Since µ has a density, Wp(µ, µˆn) must scale like n−1/d eventually, but it is possible that the convergence of µˆn to µ should improve due to the fact that µ is almost singular.
It turns out that this is indeed the case, as we show in this Section. We begin by proving a sharper version of Proposition 5 better suited to non-asymptotic results. In the second half of this Section, we show how this non-asymptotic bound can be used to prove faster convergence rates in the ﬁnite-sample regime for situations like the one described above.
5.1 Finite-sample behavior
The statement of Proposition 5 only assumes a bound on the quantity dε(µ, τ ) for suﬃciently small ε. It is therefore well suited to establishing results of an asymptotic nature. On the other hand, the resulting bound did not give any indication of the behavior in the small-n regime, since the bound was vacuous for n (ε′)−2.
If we have stronger control over dε(µ, τ ), then the proof of Proposition 5 can be modiﬁed to yield a ﬁnite-sample result. In particular, if we can control dε(µ, τ )

14

WEED AND BACH

for all ε larger than a certain threshold, we can prove an upper bound without the n−1/2 term present in Proposition 5.

Proposition 10. Fix p ∈ [1, ∞). Write d≥ε(µ, τ ) = supε′∈[ε,1/9] dε(µ, τ ), and let dn = infε>0 max{d≥ε(µ, εp), −lolgognε }. If dn > 2p, then

IE[Wpp(µ, µˆn)] ≤ C1n−p/dn ,

where

C1 = 27p 2 + 1

.

3 dn 2

−p

−

1

As in Proposition 5, the constant C1 is independent of the dimension as long as dn is bounded away from 2p.

Proof. Fix an arbitrary ε, and let d = max{d≥ε(µ, εp), −lolgognε }, where d > 2p.

If

n−1/d

≥

1/27,

then

the

bound

W

p p

(µ

,

µˆ

n

)

≤

C1n−p/d

is

trivial,

so

assume

that

n > 33d.

Let k∗ =

log n d log 3

− 2. As in the proof of Proposition 5, we can choose sets

S1, . . . , Sk∗ such that µ(Sk) ≥ 1 − εp and N3−(k+1) (Sk) = N3−(k+1) (µ, εp) for 1 ≤ k ≤ k∗. Applying Proposition 3 to construct an appropriate dyadic partition

and using Propositions 1 and 4 yields

k∗
IE[Wpp(µ, µˆn)] ≤ 3−k∗p + 3−(k−1)p
k=1
By the deﬁnition of k∗, for 1 ≤ k ≤ k∗,

N3−(k+1) (µ, εp) + 2εp k∗ 3−(k−1)p . n k=1

3(k+1)d ≤ n ,

so 3−(k+1) ≥ ε. Hence 3εp ≤ 3−k∗p and N3−(k+1) (µ, εp) ≤ 3(k+1)d for 1 ≤ k ≤ k∗,

and applying the bound

k∗ k=1

3−(k−1)p

≤

3/2

for

p

≥

1

yields

IE[W p(µ, µˆn)] ≤ 3−k∗p + 3−k∗p

p

3

d 2

−

p

−

1

3(k∗+2)d + 3εp n

≤ 2+ 1

3−k∗p

3 dn 2

−p

−

1

≤ C1n−p/d ,

where in the last step we have used the fact that d ≥ dn and

1
d

is decreasing

3 2 −p−1

in d.

Taking the inﬁmum over all possible choices of ε yields the bound.

Note in the proof of Proposition 10 that we in fact only needed control over dε′(µ, τ ) for ε′ of the form 3−k for k a positive integer, though for simplicity we have assumed that we can bound dε′(µ, τ ) for all ε′ ∈ [ε, 1/9].
The upper bound of Proposition 10 suggests that measures can have truly dif-
ferent rates of convergence at diﬀerent scales. The following Proposition shows
that Proposition 10 is essentially tight and that this multiscale behavior indeed

SHARP RATES OF WASSERSTEIN CONVERGENCE

15

can occur, in the sense that for any decreasing sequence δn satisfying mild conditions, there exists a measure µ such that n−1/dn ≈ δn and IE[Wp(µ, µˆn)] ≥ Cn−1/dn for all n. In other words, for any desired rate of decrease, there exists a measure such that IE[Wp(µ, µˆn)] converges to 0 at precisely that rate. Such measures can even be found when the underlying metric metric is induced by the ℓ∞ norm on real space. As with the lower bound proved in Proposition 6, above, the following bound in fact holds for any measure ν supported on at most n points.
A proof appears in Appendix A.

Proposition 11. Let δn be a nonincreasing sequence in (0, 1) with the following properties:
• the bound δn ≥ n−1 holds for all n ≥ 2 (i.e., δn does not decrease too quickly)
• the sequence −llooggnδn is nondecreasing (i.e., the rate of decrease of δn slows), and
• there exist constants c > 1 and α ∈ [−1, 0) such that 1c nα ≤ δn ≤ cnα for all n suﬃciently large (i.e., δn eventually decreases polynomially in n).
There exists a measure µ on X = ([0, 1]m, ℓ∞) for some m such that, if dn is deﬁned as in Proposition 10, then 14 δn ≤ n−1/dn ≤ 2δn and
IE[Wp(µ, ν)] ≥ 2−6n−1/dn
for all p ∈ [1, ∞), all n ≥ 1, and any measure ν supported on at most n points.

Proposition 10 only holds when dn > 2p, so for completeness we conclude this section by providing a second bound that can be used when Prposition 10 does not apply. The following bound is always valid and is sharper when the asymptotic dimension of µ is small.

Proposition 12. Let mn = infε>0 max{Nε(µ, εp), nε2p}. Then

where C1 = 9p + 3.

IE[Wpp(µ, µˆn)] ≤ C1

mn , n

Proof. Fix an arbitrary ε, and let m = max{Nε(µ, εp), nε2p}. If ε ≥ 1/9,

then the bound Wpp(µ, µˆn) ≤ C1

m n

is

trivial,

so

assume

ε

<

1/9.

Let k∗ =

− log ε log 3

− 1. Following the proof of Proposition 10, we have

k∗
IE[Wpp(µ, µˆn)] ≤ 3−k∗p + 3−(k−1)p
k=1

N3−(k+1) (µ, εp) + 2εp k∗ 3−(k−1)p . n k=1

The monotonicity of Nε(µ, τ ) implies that N3−(k+1)(µ, εp) ≤ Nε(µ, εp) ≤ m for all

k ≤ k∗. Plugging in this estimate and applying the bound

k∗ k=1

3−(k−1)p

≤

3/2

for p ≥ 1 yields

IE[Wpp(µ, µˆn)] ≤ 3−k∗p + 32 mn + 32 εp ≤ 3−k∗p + 3 mn .

16

WEED AND BACH

On the other hand, 3−k∗p = 9p3−(k∗+2)p < 9pεp ≤ 9p mn , so

IE[Wpp(µ, µˆn)] ≤ C1 mn .

Taking the inﬁmum over all possible choices of ε yields the bound.

5.2 Clusterable distributions
We now return to the situation described in the introduction to this Section and analyze the case where µ is like a sum of Dirac masses. This is the simplest example of where multiscale behavior can occur. We validate the intuition presented above: when µ is approximately discrete, in the sense that it is supported on balls of small radius, then the convergence of µˆn to µ enjoys the fast n−1/2p rate until n is large even µ is absolutely continuous with respect to the Lebesgue measure. We show that a similar phenomenon occurs when µ is the convolution of a discrete distribution with a small Gaussian, where we show that it is enough that most of the mass of µ is near a discrete distribution, even though the support is unbounded.

Definition 7. A distribution µ is (m, ∆)-clusterable if supp(µ) lies in the union of m balls of radius at most ∆.

Intuitively, the measure µ looks like a sum of m Dirac measures at “large scales,” with high-dimensional information arriving only when we consider scales smaller than ∆.
Proposition 13. If µ is (m, ∆) clusterable, then for all n ≤ m(2∆)−2p,

IE[Wpp(µ, µˆn)] ≤ (9p + 3) mn .
Proof. Since supp(µ) lies in the union of m balls of radius at most ∆, we have N2∆(µ) ≤ m. Therefore if n ≤ m(2∆)−2p, then
mn = inf max{Nε(µ, εp), nε2p} ≤ max{N2∆(µ), n(2∆)2p} ≤ m ,
ε>0
and the claim follows from Proposition 12.

We can apply the above result to “Diracs plus Gaussian” case described in the introduction to this Section. We ﬁrst require a simple Lemma, which allows us bound the mass of a Gaussian outside of a small ball.

Lemma 1. If Z ∼ N (0, Σ), then for any c ≥ 5,

P[

Z

2 2

>

c2Tr(Σ)]

≤

e−c2/4

.

A proof appears in Appendix A. Proposition 13 and Lemma 1 yield the following claim.

SHARP RATES OF WASSERSTEIN CONVERGENCE

17

Proposition 14. Let µ be a mixture of m Gaussian distributions in Rd

equipped with the ℓ2 norm, and let σ2 be an upper bound for the trace of the

covariance

matrix

of

each

mixture

component.

If

p

log

1 σ

≥

25/4,

then

for

all

n

≤

m(16σ2p

log

1 σ

)−p,

IE[Wpp(µ, µˆn)] ≤ (9p + 3) mn .

Since the measure µ is absolutely continuous with respect to the Lebesgue measure, if d > 2p, then asymptotically we have

IE

[W

p p

(µ

,

µˆ

n

)]

n−p/d ,

which is slower than the n−1/2 rate obtainable for small n.

Proof. Let c = 2 p log σ1 , which by assumption is at least 5. By Lemma 1,

all but at most e−c2/4 = σp mass lies within balls of radius cσ around the mixture

centers.

Therefore

N2cσ (µ,

(2cσ)p )

≤

N2cσ (µ,

σp)

≤

m.

If

n

≤

m(16σ2p

log

1 σ

)−p

=

m(2cσ)−2p, then we obtain

mn = inf max{Nε(µ, εp), nε2p} ≤ max{N2cσ(µ, (2cσ)p), n(2cσ)2p} ≤ m ,
ε>0

and the claim follows from Proposition 12.

5.3 Approximately low-dimensional sets
We now broaden considerably to the general case where µ is supported on an approximately low-dimensional set.

Definition 8 (See [Tal95]). For any S ⊆ X, the ε-fattening of S is

Sε := {y : D(y, S) ≤ ε} .
If S′ ⊂ Sε for some S, then S′ is close to S in the sense that every point of S′ is within ε of some point in S. In particular, S′ ⊂ Sε if the Hausdorﬀ distance between S′ and S is at most ε.
Measures supported on Sε are “close” to measures supported on S, and if S is low-dimensional, then we obtain correspondingly better ﬁnite-sample rates.

Proposition 15. Suppose supp(µ) ⊆ Sε for some ε > 0 and set S satisfying

Nε′(S) ≤ (3ε′)−d

for all ε′ ≤ 1/27 and for some d > 2p. Then for all n ≤ (3ε)−d,

IE[Wpp(µ, µˆn)] ≤ C1n−p/d ,

where

C1 = 27p 2 + 1

.

3

d 2

−

p

−

1

18

WEED AND BACH

In other words, µˆn converges to µ at the n−p/d rate until n is exponentially
large in d. In particular, if µ is absolutely continuous with respect to the Lebesgue measure on Rs where s ≫ d, then µˆn converges to µ much faster initially (at the rate n−p/d) than it does in the limit (at the rate n−p/s).

Proof. Given any covering of S by balls B1, . . . , Bm of diameter ε′, the εfattenings (B1)ε, . . . , (Bm)ε provide a covering of Sε by balls of diameter ε′ + 2ε. This implies for all ε′ ≥ ε that

N3ε′(µ) ≤ N3ε′(Sε) ≤ Nε′+2ε(Sε) ≤ Nε′(S) ≤ (3ε′)−s ,

and hence that Therefore, if n ≤ (3ε)−d, then

d≥3ε(Sε) ≤ s .

dn ≤ max d≥3ε(µ), log n ≤ d . − log 3ε

The claim follows from Proposition 10.

We can also relax the requirement that supp(µ) ⊆ Sε to the statement that µ is concentrated near S.

Proposition 16. Let S be a set satisfying

Nε(S) ≤ (3ε)−d

for all ε ≤ 1/27, for some d > 2p. Suppose there exists a positive constant σ such

that µ satisﬁes

µ(Sε) ≥ 1 − e−ε2/2σ2

for all ε > 0. If p log σ1 ≥ 118 , then for all n ≤ 18pσ2 log σ1 −d/2,

IE[Wpp(µ, µˆn)] ≤ C1n−p/d ,

where

C1 = 27p 2 + 1

.

3

d 2

−

p

−

1

In other words, we again get the fast n−p/d rate until n is of order approximately σ−d.

Proof. Let ε = tions imply that

2pσ2 log σ1 . As in the proof of Proposition 15, the assumpd≥3ε(Sε) ≤ d .

Since

µ(Sε) ≥ 1 − e−ε2/2σ2 = 1 − σp ≥ 1 − (3ε)p ,

we conclude that as long as n ≤ 18pσ2 log σ1 −d/2, then

dn ≤ max d≥3ε(µ, (3ε)p), log n ≤ d , − log 3ε

and the claim follows from Proposition 10.

SHARP RATES OF WASSERSTEIN CONVERGENCE

19

The condition appearing in Proposition 16 is notable because it resembles the guarantee of an isoperimetric inequality [Led05]. Such inequalities are an important topic in modern geometric probability theory, and possess a close connection to the W1 distance [BG99].
It is a striking fact about such inequalities that they are intimately connected to the concentration properties of 1-Lipschitz functions.

Proposition 17 (See [Led05, Proposition 1.3]). Given a function f : X → R, say that mf is a median of f if

P[f (X) ≥ mf ] ≥ 1/2 and P[f (X) ≤ mf ] ≥ 1/2 .

If for all 1-Lipschitz functions f : X → R and medians mf ,

(2)

P[f (X) ≥ mf + t] ≤ e−t2/2σ2 ,

then for any set A with µ(A) ≥ 1/2,

(3)

µ(Aε) ≥ 1 − e−ε2/2σ2 .

Conversely, if (3) holds for all sets A with µ(A) ≥ 1/2, then (2) holds for any 1-Lipschitz function f with median mf .

The conditions of Proposition 17 have been used recently to show concentration
bounds for the W1 distance [BGV07]. Here, we show that if µ possesses the property described above, then µˆn enjoys a fast rate of convergence to µ for any p, as long as µ assigns a constant fraction of mass to a low-dimensional set.

Proposition 18. Suppose that µ satisﬁes either of the two equivalent condi-

tions

of

Proposition

17

with

some

σ

satisfying

p log

1 σ

≥

118 .

If

S

is

a

set

satisfying

Nε(S) ≤ (3ε)−d

for all ε, for some d > 2p and µ(S) ≥ 1/2, then for all n ≤ 18pσ2 log σ1 −d/2,

IE[Wpp(µ, µˆn)] ≤ C1n−p/d ,

where

C1 = 27p 2 + 1

.

3

d 2

−

p

−

1

Proof. Combine Propositions 16 and 17.

6. CONCENTRATION In addition to proving bounds on the expected value of the quantity Wpp(µ, µˆn), we also show that it concentrates well around its expectation. Previous work [BGV07, Boi11] has sought to obtain tail bounds of the form
P[Wpp(µ, µˆn) ≥ t] ≤ ψn(t) ,

20

WEED AND BACH

where ψn(t) is some function exhibiting subgaussian decay. The results of [BGV07]

appear to obtain this rate, but the constants involved depend on n and the ambi-

ent dimension of the space in a way that makes the results diﬃcult to interpret.

We follow a diﬀerent approach, which more clearly emphasizes the dependence

of the tail on n and the dimension. The results of Sections 4 and 5, above, yield bounds on the expected value IE[Wpp(µ, µˆn)]. As we have seen, the convergence of

this quantity to 0 may be slow when the dimension is large. On the other hand,

we

show

below

that,

as

long

as

X

is

bounded,

the

quantity

W

p p

(µ

,

µˆ

n

)

concen-

trates well around its expectation independent of the dimension. The argument

is standard [Tal92] and is signiﬁcantly easier to obtain than the above bounds on

the expected value. We require the following dual formulation [RR90, R¨91].

Definition 9. Given a bounded continuous function f : X → R, the ctransform of f (with respect to D(·, ·)p) is the function f c : X → R deﬁned by
f c(y) = sup(f (x) − D(x, y)p) .
x∈X
The following claims are standard, and we provide a proof in Appendix A for completeness.

Proposition 19 (Kantorovich duality). Given any pair of probability measures µ and ν on X and any p ∈ [1, ∞), the following duality holds:

(4)

Wpp(µ, ν) = sup IEµf − IEνf c ,

f ∈Cb(X)

where the supremum is taken over all bounded continuous functions on X and f c is the c-transform of f with respect to D(·, ·)p. Moreover, if diam(X) ≤ 1, then
we can take 0 ≤ f (x) ≤ 1 for all x ∈ X.

We then obtain a concentration result via a standard bounded diﬀerence argument.

Proposition 20. For all n ≥ 0 and 0 ≤ p < ∞,

P[Wpp(µ, µˆn) ≥ IEWpp(µ, µˆn) + t] ≤ exp −2nt2 .

Proof. Let µˆn be the empirical distribution corresponding to the i.i.d. samples X1, . . . , Xn ∼ µ. We abbreviate Wpp(µ, µˆn) by W . By Proposition 19, we can
write W = sup IEµˆn f − IEµf c ,
0≤f ≤1

or, writing W explicitly as a function of X1, . . . Xn,

W (X , . . . , X ) = 1 sup

n
f (X ) − IE f c .

1

n n 0≤f ≤1

i

µ

i=1

SHARP RATES OF WASSERSTEIN CONVERGENCE

21

For any x1, . . . , xn, x′n ∈ X, we have

W (x1, . . . , xn) − W (x1, . . . , x′n) = n1

n
sup (f (xi) − IEµf c)
0≤f ≤1 i=1

n−1
− sup (f ′(xi) − IEµf ′c)
0≤f ′≤1 i=1

+f ′(x′n) − IEµf ′c

≤ n1 0≤sufp≤1 f (xn) − f (x′n) ≤ n1 .

Applying McDiarmid’s inequality [McD89] yields the bound.

7. APPLICATIONS
In this section, we sketch two applications of our work to machine learning and statistics.
7.1 Quadrature
Numerical integration, or quadrature, refers to the technique of approximating integrals by ﬁnite sums for the purpose of evaluating them at low computational cost. Given a measure µ, the goal is to choose points x1, . . . , xn ∈ X and weights α1, . . . , αn ∈ R+ such that the approximation
n
f (x)dµ(x) ≈ αif (xi)
i=1
is as good as possible for a wide class of functions f . This problem possesses close connections to optimal quantization, since the points x1, . . . , xn naturally serve as a ﬁnite approximation to the underlying measure [GL07, Pag98].
When only a single function f is considered, one can show that a Monte Carlo method which chooses x1, . . . , xn i.i.d. from µ with uniform weights αi = n−1 for 1 ≤ i ≤ n is asymptotically suboptimal [Nov88]. However, our results show that if we require that the approximation hold over the class of Lipschitz functions Lip(X), then this simple Monte Carlo scheme is asymptotically optimal for a wide class of measures.

Proposition 21. Denote by Lip(X) the class of 1-Lipschitz on X If µ is a

measure supported on a regular set of dimension d ≥ 2 and µ ≪ Hd, then for any

s > d,

IE sup
f ∈Lip(X)

1n f (x)dµ(x) − n f (Xi)
i=1

n−1/s ,

where X1, . . . , Xn ∼ µ are independent. On the other hand, for any t < d, x1, . . . , xn ∈ X, and αi, . . . , αn ∈ R+,

sup
f ∈Lip(X)

n
f (x)dµ(x) − αif (xi)
i=1

n−1/t .

22

WEED AND BACH

Proof. Recalling (1), we immediately see that the ﬁrst claim corresponds to the fact that IEW1(µ, µˆn) n−1/s, which follows from Corollary 1 and Proposition 8.
For the second claim, we ﬁrst note that by choosing f (x) = c to be a constant
function, we have

n

n

f (x)dµ(x) − αif (xi) = c 1 − αi .

i=1

i=1

Since such an f is Lipschitz, by choosing c arbitrarily large we obtain that

sup
f ∈Lip(X)

n
f (x)dµ(x) − αif (xi) = ∞
i=1

unless

n i=1

αi

=

1.

It

therefore

suﬃces

to

prove

the

claim

when

this case, setting
n

ν = αiδxi ,

i=1

n i=1

αi

=

1.

In

and again applying (1) yields that this claim is equivalent to the fact that W1(µ, ν) n−1/t. Since ν is supported on at most n points, this follows from
Corollary 2 and Proposition 8.

7.2 k-means clustering
The authors of [CR12] point out that many “unsupervised learning” techniques in machine learning involve constructing a simple approximation µ˜ to a measure µ such that W2(µ, µ˜) is small. One such example is the so-called k-means problem, where the goal is to ﬁnd a set S with |S| ≤ k minimizing the objective function
IED(X, S)2 ,

where X ∼ µ. It is not hard to see [CR12, Lemma 3.1] that this problem is equivalent to ﬁnding a measure µ˜ supported on at most k points such that W2(µ, µ˜) is as small as possible. Given such a measure, we obtain a clustering of µ into at most k pieces by constructing a Voronoi partition of supp(µ) based on the k points in supp(µ˜).
The authors of [CR12] show that for k suﬃciently large and for X a compact, smooth d-dimensional manifold, it is possible to ﬁnd a measure µ˜ with | supp(µ˜)| ≤ k satisfying

W2(µ, µ˜) ≤ C1τ k−1/d with probability 1 − e−τ2

on

the

basis

of

n

=

C

2

k

2+

4 d

samples.

Corollary

2

implies

that

this

dependence

on

k is asymptotically optimal.

Our results show that a much simpler procedure suﬃces in high dimensions.

As long as d ≥ 4, the empirical measure µˆk satisﬁes

IEW2(µ, µˆk) ≤ C1′ k−1/s

SHARP RATES OF WASSERSTEIN CONVERGENCE

23

for any s > d, and Proposition 20 then implies that
W2(µ, µˆk) ≤ C1′′τ k−1/s with probability 1 − e−τ4 .
This shows that clustering a measure µ into k pieces on the basis of k i.i.d. samples from µ is asymptotically optimal, and enjoys concentration properties even better than the ones implied by [CR12].

8. CONCLUSION AND FUTURE WORK
Our focus in this work has been to obtain sharper rates than previously available for the convergence of µˆn to µˆ in Wasserstein distance, both in asymptotic and ﬁnite-sample settings. Our results give theoretical support to a phenomenon observed in practice: even though Wp(µ, µˆn) can converge very slowly for measures supported on a high-dimensional metric space, many measures arising in applications are intrinsically low dimensional, at least approximately, and therefore enjoy reasonably fast rates of convergence.
Our work leaves open whether slightly diﬀerent versions of the Wasserstein distance can converge faster in general. Recently, a version of the Wasserstein distance with an entropic penalty has been proposed and shown to have attractive theoretical properties and practical performance [SdGP+15, Cut13, CDPS17, RCP16]. It is possible that these objects achieve better rates than the vanilla Wasserstein distance in the high-dimensional setting.
We also do not consider here empirical measures other than the simple µˆn. In practice, a technique known as importance sampling [Buc13] is often used to reduce the variance of estimates produced on the basis of random samples from a distribution. As noted in Section 7.1, if µ is suﬃciently regular, then no discrete measure on n points can achieve better asymptotic performance than the empirical measure µˆn. However, we conjecture that many reasonable sampling techniques should produce measures that are also asymptotically no worse than µˆn. We leave this question for future work.
9. ACKNOWLEDGMENTS
We thank Guillaume Carlier, Marco Cuturi, and Gabriel Peyr´e for discussions related to this work. JW would like to thank FB for his hospitality at INRIA, where this research was conducted.

APPENDIX A: OMITTED PROOFS

A.1 Proof of Proposition 1

We begin by giving an informal outline of the idea of the proof. Consider a partition {Qi}i∈I of X, for some index set I. The measures µ and ν both induce measures on each set in the partition. We will transport µ to ν by ﬁrst moving mass between sets in this partition, and then moving mass within each set in the partition. If µ(Qi) = ν(Qi) for one of the sets Qi, we we need to transport an amount of mass equal to |µ(Qi) − ν(Qi)| into or out of Qi. In total, we can transport the mass that µ assigns to each set in the partition to its proper set under ν for a total cost of

|µ(Qi) − ν(Qi)| diam(S) ≤ |µ(Qi) − ν(Qi)| ,

i∈I

i∈I

24

WEED AND BACH

where we use the fact that diam(S) ≤ diam(X) ≤ 1 by assumption. After the ﬁrst step of the transport plan, µ has been transported so that each
set in the partition contains the correct total amount of mass. It therefore suﬃces in the second step to properly arrange the mass within each set. Moving the mass within Qi cannot cost more than diam(Qi), so the total cost of arranging the mass within each set is at most

ν(Qi) diam(Qi) ≤ max diam(Qi) .
i∈I i∈I
We have obtained a transport of µ to ν for a total cost of approximately

max diam(Qi) + |µ(Qi) − ν(Qi)| .
i∈I i∈I

This “single scale” bound is generally not tight, but a more reﬁned bound

can be obtained by applying the above argument recursively: instead of naively

bounding the cost of moving the mass within Qi by the quantity diam(Qi), we

can partition Qi into smaller sets and estimate the cost of moving the mass within

Qi by ﬁrst moving it between the sets of the partition before moving it within each smaller set. Iterating the argument k∗ times yields the bound.

We now show how to make the above argument precise. Given two measures µ

and ν on X, write C(µ, ν) for the set of couplings between µ and ν; that is, for the

set of measures on X × X whose projection onto the ﬁrst and second coordinate

correspond to µ and ν respectively.

Fix a k∗ ≥ 1. We will deﬁne two sequences of measure πk and ρk on X for

1 ≤ k ≤ k∗ such that

k∗ k=1

πk

≤

µ

and

k∗ k=1

ρk

≤

ν.

Given

such

a

sequence,

we

set µ1 = µ and ν1 = ν and write

k−1

µk = µ − πℓ

ℓ=1

k−1

νk = ν − ρℓ

ℓ=1
for k ≤ k∗ + 1. Note that if γk ∈ C(πk, ρk) for 1 ≤ k ≤ k∗ and γk∗+1 ∈ C(µk∗+1, νk∗+1), then

k∗+1
γk ∈ C

k∗

k∗

πk + µk∗+1, ρk + νk∗+1

= C(µ, ν) ,

k=1

k=1

k=1

therefore
k∗
Wpp(µ, ν) ≤ Wpp(πk, ρk) + Wpp(µk∗+1, νk∗+1) .
k=1
For k ≥ 1, deﬁne

πk =
Qki ∈Qk µk (Qki )>0

1 − νk(Qki ) µk(Qki )

µk|Qki ,
+

ρk =
Qki ∈Qk νk(Qki )>0

1 − µk(Qki ) νk(Qki )

νk|Qki .
+

SHARP RATES OF WASSERSTEIN CONVERGENCE

25

Note that 0 ≤ πk ≤ µk and 0 ≤ ρk ≤ νk for all k, hence 0 ≤ µk ≤ µ and 0 ≤ νk ≤ ν for all k as well.

Lemma A.1. If Q ∈ Qk−1, then

πk(Q) = ρk(Q) .

Moreover,

πk(S) = ρk(S) ≤

|µ(Qki ) − ν(Qki )| .

Qki ∈Qk

Lemma A.2. If α and β are two measures on X such that

α(Q) = β(Q)

for all Q ∈ Qk, then

Wpp(α, β) ≤ δkpα(S) .

We can now obtain the ﬁnal bound. By Lemmas A.1 and A.2,

Wpp(πk, ρk) ≤ δ(k−1)p

|µ(Qki ) − ν(Qki )|

Qki ∈Qk

and Wpp(µk∗+1, µk∗+1) ≤ δk∗pµk∗+1(S) ≤ δk∗pµ(S) ≤ δk∗p .
The bound follows.

A.2 Proof of Proposition 2

We prove the inequalities in order. If d < dH (µ), then by [Fal97, Proposi-
tion 10.3] there exists a compact set K with positive mass and a r0 > 0 such
that µ(B(x, r)) ≤ rd

for all r ≤ r0 and all x ∈ K. (See also the proof of [GL07, Corollary 12.16].) Let τ < µ(K)/2. If S is any set with µ(S) ≥ 1 − τ , then µ(S ∩ K) > µ(K)/2.
If Nε(S) = N , then in particular there exists a covering of S ∩ K by at most N balls of radius ε whose centers all lie in K. Indeed, any set of diameter at most
ε which intersects S ∩ K is contained in a ball of radius ε whose center is in K. If ε ≤ r0, then each such ball satisﬁes µ(B(x, r)) ≤ εd, so

N ≥ ε−dµ(K)/2 .

We therefore have for all τ suﬃciently small,

lim inf

log

N

′ ε

(µ

,

τ

)

≥

d.

ε→0 − log ε

Thus d∗(µ) ≥ d. Since d < dH (µ) was arbitrary, we have dH (µ) ≤ d∗(µ), as

desired.

That d∗(µ) ≤ d∗p(µ) follows from the simple observation that for all positive α

and τ ,

lim inf dε(µ, τ ) ≤ lim inf dε(µ, εα) .

ε→0

ε→0

26

WEED AND BACH

Finally, if dM (µ) ≥ 2p, then setting s > dM (µ) yields

sp

lim sup dε(µ, ε s−2p ) ≤ lim sup dε(µ) = dM (µ) < s ,

ε→0

ε→0

so d∗p(µ) ≤ s. Since s > dM (µ) was arbitrary, we obtain d∗p(µ) ≤ dM (µ).

A.3 Proof of Proposition 3

Write

Nk

=

N3−(k+1) (Sk).

For

1

≤

k

≤

k∗,

let

Ck

=

{C

k 1

,

.

.

.

}

be

a

ﬁnite

covering of S by balls of diameter 3−(k+1) such that C1k, . . . , CNk k covers Sk. Such a

covering can always be found by choosing an optimal covering of Sk and extending

this covering to a covering of all of S. Since N3−(k∗+1) (S) < ∞, this requires only

a ﬁnite number of additional balls. We begin by constructing Qk∗. Let Qk1∗ = C1k∗, and for 1 < ℓ ≤ |Ck∗| let

ℓ−1

Qℓk∗ = Cℓk∗ \

Qkn∗ .

n=1

Let Qk∗ = {Qk1∗ , . . . }. Note that diam(Qkℓ ∗ ) ≤ diam(Cℓk∗ ) = 3−(k∗+1) < 3−k∗, that Qk∗ forms a partition of S, and that at most Nk∗ elements of Qk∗ intersect Sk∗.
We now show how to construct Qk from Qk+1 and Ck. Let

and for 1 < ℓ ≤ |Ck∗| let

Qk1 =

Q,

Q∈Qk+1 Q∩C1k =∅

Qkℓ =

Q\
Q∈Qk+1 Q∩Cℓk =∅

ℓ−1
Qkn .
n=1

Let Qk = {Qk1, . . . }. The sets in Qk clearly form a partition of S, and by construction at most Nk
elements of Qk intersect Sk Moreover, since diam(Cℓk) ≤ 3−(k+1) for all ℓ and diam(Q) ≤ 3−(k+1) for all Q ∈ Qk+1, the distance between any two points in Qkℓ is at most 3 · 3−(k+1) = 3−k, so each element of Qk has diameter at most 3−k. Finally, since each set in Qk is the union of sets in Qk+1, the partition Qk+1 reﬁnes Qk, as desired.

A.4 Proof of Proposition 7

The only inequality that does not follow from Proposition 2 is the ﬁrst. By

absolute continuity, for all τ > 0 there exists a σ > 0 such that any set T for

which µ(T ) ≥ 1 − τ satisﬁes Hd(T ) ≥ σ. If Hd(T ) ≥ σ, then in particular for any

covering {B(xi, ε)} of T by balls of radius ε, we must have such a covering contains at least σε−d balls, so

i εd ≥ σ. Therefore

log Nε(µ, τ ) ≥ d + log σ ,

− log ε

− log ε

and taking limits yields that d∗(µ) ≥ d, as desired.

SHARP RATES OF WASSERSTEIN CONVERGENCE

27

A.5 Proof of Proposition 11
For all integers k ≥ 0, denote by Nk the smallest positive integer n such that n is a power of two and δn ≤ 2−k. Such an integer always exists because the sequence δn decreases to 0. We require the following lemma, whose proof is deferred to Appendix B.

Lemma A.3. The sequence Nk+1/Nk is bounded.
Let m be an integer large enough that Nk+1/Nk ≤ 2m for all n. Let Q be the standard dyadic partition of [0, 1], with Qk being a partition of [0, 1]m consisting of 2km cubes of side length 2−k.
Our measure µ will satisfy N2−k (µ) = Nk−2 for all k ≥ 2. We will deﬁne a sequence of measures {µk}∞ k=2 iteratively and construct µ as their limit in the weak topology.
Let µ2 be the uniform distribution on [0, 1/4]m. For each positive integer k, the measure µk will be supported on Nk−2 cubes in Q, and will be uniform on its support. We will call a cube Qi ∈ Qk live if µk(Qi) = 0.
Fix an ordering x0, . . . , x2m−1 of the 2m elements of {0, 1}m. To produce µk+1 from µk, divide each live cube of µk into 2m cubes of side length 2−(k+1). The ordering of {0, 1}m induces an order on these 2m subcubes.
Given a live Q ∈ Qk, deﬁne the restriction µk+1|Q by requiring that µk+1(Q) = µk(Q) and that µk+1|Q be uniform on the union of the ﬁrst Nk+1/Nk subcubes of Q. Note that Nk+1/Nk is an integer because both Nk+1 and Nk are powers of 2, and by assumption Nk+1/Nk ≤ 2m, the total number of subcubes of Q. Since Qk forms a partition of [0, 1]m, combining the measures µk+1|Q for Q ∈ Qk yields a probability measure µk+1 on [0, 1]m. By Prokhorov’s theorem, this sequence of measures µk possesses a subsequence converging in distribution to some measure µ.
The following lemma collects necessary properties of µ. Its proof appears in Appendix B.

Lemma A.4. Moreover, and

If Nk ≤ n < Nk+1, then N2−k−4 (µ, 1/2) > n
2−(k+1) ≤ δn ≤ 2−k 2−k−4 ≤ n−1/dn ≤ 2−k .

We can now obtain the lower bound. Let ν be any measure supported on at most n points. If Nk ≤ n < Nk+1, then by Lemma A.4, if X ∼ µ, then
P[ min X − y ∞ ≤ 2−k−5] < 1/2 .
y∈supp(ν)
Markov’s inequality therefore implies for any coupling (X, Y ) of µ and ν that IE[ X −Y p∞]1/p ≥ 2−k−4P[ min X −y ∞ > 2−k−5]1/p ≥ 2−k−6 ≥ 2−6n−1/dn ,
y∈supp(ν)
as claimed.

28

WEED AND BACH

A.6 Proof of Proposition 19

Both claims are standard, and details can be found in [Vil08, Theorem 5.10]. The ﬁrst follows follows from the assumption that X is a bounded Polish space. For the second, we use the fact that the supremum is achieved by an f satisfying

(5)

f (x) = inf f c(y) + D(x, y)p ∀x ∈ X .

y∈X

Let f be a function achieving the supremum in (4) and satisfying (5). By adding a constant to f and f c, we can assume that supx∈X f (x) = 1. Then for all y ∈ X,

f c(y) = sup f (x) − D(x, y)p ≥ 0 ,
x∈X

and (5) then implies

f (x) ≥ 0 ∀x ∈ X ,

as claimed.

A.7 Proof of Lemma 1

Let Σ =

d i=1

λivivi⊤

be

an

eigendecomposition

of

Σ

with

λ1

≥

···

≥

λd

≥

0.

Then

Z

2 2

has

the

same

distribution

as

d i=1

λi

ξi2,

where

ξ1, . . . , ξd

are

i.i.d.

standard Gaussian random variables.

By [LM00, Lemma 1], for any positive t, we have


d
P  λi(ξi2 − 1) ≥ 2
i=1


d
t λ2i + 2λ1t ≤ exp(−t) .
i=1

Bounding both

d i=1

λ2i

1/2
and λ1 by Tr(Σ) yields

P

Z

2

≥

(1

+

√ 2t

+

2t)Tr(Σ)

≤ exp(−t) .

2

Finally,

setting

c2

=

√ 2( t

+

1)2,

we

obtain

√

P

Z

2 2

≥

c2Tr(Σ)

≤ exp(−(c −

2)2/2) ≤ exp(−c2/4)

for all c ≥ 5, as desired.

APPENDIX B: ADDITIONAL LEMMAS B.1 Proof of Lemma A.1
Proof. We ﬁrst show that for any ℓ < k, if Q ∈ Qℓ, then

µk(Q) = νk(Q) . Suppose ﬁrst that Q ∈ Qk−1. By deﬁnition, µk = µk−1 − πk−1. We obtain

µk(Q) = (µk−1 − πk−1)(Q) = min{µk−1(Q), νk−1(Q)} ,

and likewise

νk(Q) = min{µk−1(Q), νk−1(Q)} .

SHARP RATES OF WASSERSTEIN CONVERGENCE

29

Since Q is a dyadic partition, any Q ∈ Qℓ for ℓ < k can be written as a disjoint union of Q1, . . . , Qm ∈ Qk−1. Hence

m

m

µk(Q) = µk(Qi) = νk(Qi) = νk(Q) ,

i=1

i=1

as claimed. Note that this also implies

πk(Q) = µk(Q) − µk+1(Q) = νk(Q) − νk+1(Q) = ρk(Q) .

We now prove the bound on πK(S). By deﬁnition,

ρk(S) =

(νk(Qki ) − µk(Qki ))+ = 12

|νk(Qki ) − µk(Qki )| .

Qki ∈Qk

Qki ∈Qk

We now show that, for any P ∈ Qk−1, there exist scalars c1, c2 ∈ [0, 1] depending on P such that

µk|P = c1µ|P νk|P = c2ν|P .

We proceed by induction on k. By symmetry, it suﬃces to prove the claim for µk and µ. Since µ1 = µ, it holds for k = 1. Now assume µk−1|P = c1µ|P . We have

µk|P = µk−1|P − πk−1|P = min

νk−1(P ) , 1 µk−1(P )

µk−1|P = c′1µ|P ,

where c′1 = min µνkk−−11((PP)) , 1 c1. This proves the claim. Now, given such a P ∈ Qk−1 and c1, c2 ∈ [0, 1], we have µk(P ) = νk(P ), so

c1µ(P ) = c2ν(P ) . Summing over the elements of Qk contained in P , we obtain

|µk(Qki ) − νk(Qki )| =

|c1µ(Qki ) − c2ν(Qki )|

Qki ⊂P

Qki ⊂P

≤

c1|µ(Qki ) − ν(Qki )| +

ν(Qki )|c1 − c2|

Qki ⊂P

Qki ⊂P

=

c1|µ(Qki ) − ν(Qki )| + c2|µ(P ) − ν(P )|

Qki ⊂P

≤

(c1 + c2)|µ(Qki ) − ν(Qki )|

Qki ⊂P

≤2

|µ(Qki ) − ν(Qki )| .

Qki ⊂P

Finally, summing over all P ∈ Qk−1 yields

1 ρk(S) = 2

|νk(Qki ) − µk(Qki )| ≤

|µ(Qki ) − ν(Qki )| ,

Qki ∈Qk

Qki ∈Qk

as claimed.

30

WEED AND BACH

B.2 Proof of Lemma A.2

Let

α⊗β

γ=

α(Qk) .

Qk∈Qk

i

i

α(Qki )>0

Note that γ ∈ C(α, β). Indeed, for any measurable U ⊂ S, since Qk−1 is a partition of S, we have

γ(S, U ) =

α(Qki )β(Qki ∩ U ) = β(U ) .

Qk∈Qk

α(Qik−1)

i

On the other hand, by assumption, α(Qki ) = β(Qkk), so

γk(U, S) =

α(Qik−1 ∩ U )β(Qik−1) = α(U ) . β(Qk−1)

Qki ∈Qk

k

We have

D(x, y)pdγ(x, y) =

1

Qk∈Qk α(Qki )

i

D(x, y)pdα(x)dβ(y)
Qki

≤

β

(Q

k i

)

d

iam

(Q

k i

)p

Qki ∈Qk

≤ α(S)δkp .

B.3 Proof of Lemma A.3 By assumption, there exist constants c and α such that 1c nα ≤ δn ≤ cnα for
all n suﬃciently large. Let M = (2c2)−1/α. Then for n suﬃciently large,

δMn ≤ cM nα = 1 nα ≤ 1 δn .

2c

2

This implies that for k suﬃciently large, δNk ≤ 2−k implies that δMNk ≤ 2−k−1, so that Nk+1 ≤ M Nk. Hence Nk+1/Nk ≤ M for all k suﬃciently large, so Nk+1/Nk
is bounded for all k.

B.4 Proof of Lemma A.4

We ﬁrst show the key property of µ. For any x ∈ [0, 1]m and r > 0, denote by

B(x, r) the open ℓ∞ ball of radius r around x. We claim that for any x ∈ [0, 1]m

and ℓ ≥ 2,

µ(B(x, 2−ℓ−1)) ≤ 1 . Nℓ−2

The claim certainly holds when B(x, 2−ℓ−1) exactly coincides with one of the cubes in Qℓ, since each live cube in Qℓ has mass exactly 1/Nℓ−2 by construction.
For all other x, note that the restriction of µ to each live cube in Qℓ is the same measure. In general, the cube B(x, 2−ℓ−1) intersects 2m cubes cubes in Gℓ, so we

SHARP RATES OF WASSERSTEIN CONVERGENCE

31

can partition B(x, 2−ℓ−1) into 2m pieces which, via translation, exactly cover a cube of Qℓ. Each piece has mass at most the mass of the corresponding piece in
a live cube, hence the measure is at most the measure of a live cube.
This property immediately implies a bound on the number of balls needed to cover any set S such that µ(S) ≥ 1/2. Since each ball of diameter 2−ℓ has mass at
most 1/Nℓ−2, to cover a set of mass 1/2 requires at least Nℓ−2/2 balls. Therefore for all ℓ ≥ 2,

(6)

N2−ℓ (µ, 1/2) ≥ Nℓ−2/2 .

Since n ≤ Nk+1, we have by deﬁnition δn > 2−(k+1). Because −llooggnδn is nondecreasing and at least 1 for all n ≥ 2, we have as long as n ≥ 2 that

log 2n ≥ log n ≥ log 2n , − log δ2n − log δn − log δn/2

and therefore δ2n ≥ 12 δn > 2−(k+2). This implies Nk+2/2 > n. Choosing ℓ = k + 4 in (6) yields

N2−k−4 (µ, 1/2) > n .

This proves the ﬁrst claim.

If Nk ≤ n < Nk+1, then by deﬁnition of Nk and Nk+1 and the fact that δn is

nonincreasing in n,

δn ≤ δNk ≤ 2−k

and δn > 2−k−1 .

This proves the second claim. To prove the third claim, we ﬁrst note that the deﬁnition of dn implies that

n−1/dn

is nonincreasing as n increases. We can therefore prove an upper bound on n−1/dn by proving an upper bound on Nk−1/dNk .
Recall that

dN = inf max d≥ε(µ, εp), log Nk .

k ε>0

− log ε

Choosing ε = 2−(k+2) yields

dNk ≤ max{d≥2−(k+2) (µ), lokg+2 N2k } .

To bound the ﬁrst term, note that if ε′ ∈ [2−ℓ, 2−ℓ+1), then Nε′(µ) ≤ N2−ℓ(µ) =

Nℓ−2.

Therefore

d′ε

=

log Nε′ (µ) − log ε′

≤

log Nℓ−2 ℓ−1

.

As above, since −llooggnδn is non decreasing and at least 1 for all n ≥ 2, we have

that

δNℓ−2

≥

1 2

δN

ℓ

−

2

/2

>

2−ℓ+1.

Combining

this

with

the

above

bound

implies

that if ε′ ∈ [2−ℓ, 2−ℓ+1), then

d′ ≤ log Nℓ−2 . ε − log δNℓ−2

32

WEED AND BACH

The assumption that −llooggnδn is nonincreasing therefore implies

d≥2−k+2(µ) ≤ max log Nℓ−2 ≤ log Nk ≤ log2 Nk .

2≤ℓ≤k+2 − log δNℓ−2 − log δNk

k

We obtain

dN ≤ log2 Nk ,

k

k

so n−1/dn ≤ Nk−1/dNk ≤ 2−k. To obtain the lower bound, note that if ε ≤ 2−(k+4), then

d≥ε(µ, εp) ≥ d2−(k+4) (µ, 1/2) > lkog+2 4n ,

where we have used the fact proved above that N2−(k+4)(µ, 1/2) > n. If ε > 2−(k+4), then
log n > log2 n . − log ε k + 4 Combining these bounds yields

dn = inf max d≥ε(µ, εp), log n

ε>0

− log ε

> log2 n , k+4

so n−1/dn > 2−(k+4) ,

as claimed.

REFERENCES

[AKT84] [Bel61] [BG99] [BGV07] [BK13] [BLG14] [BNNR11] [Boi11]

M. Ajtai, J. Komlo´s, and G. Tusn´ady. On optimal matchings. Combinatorica, 4(4):259–264, 1984. R. Bellman. Adaptive control processes: a guided tour. Princeton University Press, 1961. S. G. Bobkov and F. Go¨tze. Exponential integrability and transportation cost related to logarithmic Sobolev inequalities. J. Funct. Anal., 163(1):1–28, 1999. F. Bolley, A. Guillin, and C. Villani. Quantitative concentration inequalities for empirical measures on non-compact spaces. Probab. Theory Related Fields, 137(3-4):541–593, 2007. D. Berend and A. Kontorovich. A sharp estimate of the binomial mean absolute deviation with applications. Statist. Probab. Lett., 83(4):1254–1259, 2013. E. Boissard and T. Le Gouic. On the mean speed of convergence of empirical and occupation measures in Wasserstein distance. Ann. Inst. Henri Poincar´e Probab. Stat., 50(2):539–563, 2014. K. D. Ba, H. L. Nguyen, H. N. Nguyen, and R. Rubinfeld. Sublinear time algorithms for Earth mover’s distance. Theory Comput. Syst., 48(2):428–442, 2011. E. Boissard. Simple bounds for the convergence of empirical and occupation measures in 1-wasserstein distance. Electron. J. Probab., 16:2296–2333, 2011.

SHARP RATES OF WASSERSTEIN CONVERGENCE

33

[Buc13] [CDPS17] [CP15] [CR12]
[CT12] [Cut13]
[Dav88] [DSS13] [Dud68] [DY95] [Fal97] [Fal04] [FG15] [FHT01] [GL07] [IT03] [Kan42] [Klo12]

J. Bucklew. Introduction to rare event simulation. Springer Science & Business Media, 2013. G. Carlier, V. Duval, G. Peyr´e, and B. Schmitzer. Convergence of entropic schemes for optimal transport and gradient ﬂows. SIAM J. Math. Anal., 49(2):1385–1418, 2017. S. Corlay and G. Pag`es. Functional quantization-based stratiﬁed sampling methods. Monte Carlo Methods Appl., 21(1):1–32, 2015. G. D. Can˜as and L. Rosasco. Learning probability measures with respect to optimal transport metrics. In P. L. Bartlett, F. C. N. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems 25: 26th Annual Conference on Neural Information Processing Systems 2012. Proceedings of a meeting held December 3-6, 2012, Lake Tahoe, Nevada, United States., pages 2501–2509, 2012. T. M. Cover and J. A. Thomas. Elements of information theory. John Wiley & Sons, 2012. M. Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. In C. J. C. Burges, L. Bottou, Z. Ghahramani, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013. Proceedings of a meeting held December 5-8, 2013, Lake Tahoe, Nevada, United States., pages 2292–2300, 2013. G. David. Morceaux de graphes lipschitziens et int´egrales singuli`eres sur une surface. Rev. Mat. Iberoamericana, 4(1):73–114, 1988. S. Dereich, M. Scheutzow, and R. Schottstedt. Constructive quantization: approximation by empirical measures. Ann. Inst. Henri Poincar´e Probab. Stat., 49(4):1183–1203, 2013. R. M. Dudley. The speed of mean Glivenko-Cantelli convergence. Ann. Math. Statist, 40:40–50, 1968. V. Dobri´c and J. E. Yukich. Asymptotics for transportation cost in high dimensions. J. Theoret. Probab., 8(1):97–118, 1995. K. Falconer. Techniques in fractal geometry, volume 3. Wiley Chichester (W. Sx.), 1997. K. Falconer. Fractal geometry: mathematical foundations and applications. John Wiley & Sons, 2004. N. Fournier and A. Guillin. On the rate of convergence in Wasserstein distance of the empirical measure. Probab. Theory Related Fields, 162(3-4):707–738, 2015. J. Friedman, T. Hastie, and R. Tibshirani. The elements of statistical learning, volume 1. Springer series in statistics Springer, Berlin, 2001. S. Graf and H. Luschgy. Foundations of quantization for probability distributions. Springer, 2007. P. Indyk and N. Thaper. Fast Image Retrieval via Embeddings. In 3rd International Workshop on Statistical and Computational Theories of Vision. ICCV, 2003. L. Kantorovitch. On the translocation of masses. C. R. (Doklady) Acad. Sci. URSS (N.S.), 37:199–201, 1942. B. Kloeckner. Approximation by ﬁnitely supported measures.

34

WEED AND BACH

[KLP11] [KR58] [KSKW15]
[Led81] [Led05] [LM00] [LMR16] [Mat99] [McD89]
[Mon81] [NE14] [Nov88] [Pag98] [PRR67] [R¨91]
[RCP16]
[RR90]

ESAIM Control Optim. Calc. Var., 18(2):343–359, 2012. V. A. Kaimanovich and V. Le Prince. Matrix random products with singular harmonic measure. Geom. Dedicata, 150:257–279, 2011. L. V. Kantoroviˇc and G. v. Rubinˇste˘ın. On a space of completely additive functions. Vestnik Leningrad. Univ., 13(7):52–59, 1958. M. J. Kusner, Y. Sun, N. I. Kolkin, and K. Q. Weinberger. From word embeddings to document distances. In F. R. Bach and D. M. Blei, editors, Proceedings of the 32nd International Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015, volume 37 of JMLR Workshop and Conference Proceedings, pages 957– 966. JMLR.org, 2015. F. Ledrappier. Some relations between dimension and Lyapounov exponents. Comm. Math. Phys., 81(2):229–238, 1981. M. Ledoux. The concentration of measure phenomenon. Number 89. American Mathematical Soc., 2005. B. Laurent and P. Massart. Adaptive estimation of a quadratic functional by model selection. Ann. Statist., 28(5):1302–1338, 2000. A. V. Little, M. Maggioni, and L. Rosasco. Multiscale geometric methods for data sets i: Multiscale svd, noise and curvature. Applied and Computational Harmonic Analysis, 2016. P. Mattila. Geometry of sets and measures in Euclidean spaces: fractals and rectiﬁability, volume 44. Cambridge university press, 1999. C. McDiarmid. On the method of bounded diﬀerences. In Surveys in combinatorics, 1989 (Norwich, 1989), volume 141 of London Math. Soc. Lecture Note Ser., pages 148–188. Cambridge Univ. Press, Cambridge, 1989. G. Monge. M´emoire sur la th´eorie des d´eblais et des remblais. Histoire de l’Acad´emie royale des sciences, 1:666–704, 1781. D. Nova and P. A. Est´evez. A review of learning vector quantization classiﬁers. Neural Computing and Applications, 25(3-4):511–524, 2014. E. Novak. Deterministic and stochastic error bounds in numerical analysis, volume 1349 of Lecture Notes in Mathematics. SpringerVerlag, Berlin, 1988. G. Pag`es. A space quantization method for numerical integration. J. Comput. Appl. Math., 89(1):1–38, 1998. E. C. Posner, E. R. Rodemich, and H. Rumsey, Jr. Epsilon entropy of stochastic processes. Ann. Math. Statist., 38:1000–1020, 1967. L. Ru¨schendorf. Fr´echet-bounds and their applications. In Advances in probability distributions with given marginals (Rome, 1990), volume 67 of Math. Appl., pages 151–187. Kluwer Acad. Publ., Dordrecht, 1991. A. Rolet, M. Cuturi, and G. Peyr´e. Fast dictionary learning with a smoothed wasserstein loss. In Proceedings of the 19th International Conference on Artiﬁcial Intelligence and Statistics, AISTATS 2016, Cadiz, Spain, May 9-11, 2016, pages 630–638, 2016. L. Ru¨schendorf and S. T. Rachev. A characterization of random variables with minimum L2-distance. J. Multivariate Anal., 32(1):48–

SHARP RATES OF WASSERSTEIN CONVERGENCE

35

[RTG00] [San15] [SdGP+15]
[Sha60] [SL11]
[SMB98] [Tal92] [Tal95] [Var58] [Vil08] [WDCB05] [You82] [ZLL+16]

54, 1990. Y. Rubner, C. Tomasi, and L. J. Guibas. The earth mover’s distance as a metric for image retrieval. International journal of computer vision, 40(2):99–121, 2000. F. Santambrogio. Optimal Transport for Applied Mathematicians, volume 87 of Calculus of Variations, PDEs, and Modeling. Birkh¨auser, 2015. J. Solomon, F. de Goes, G. Peyr´e, M. Cuturi, A. Butscher, A. Nguyen, T. Du, and L. J. Guibas. Convolutional wasserstein distances: eﬃcient optimal transportation on geometric domains. ACM Trans. Graph., 34(4):66:1–66:11, 2015. C. E. Shannon. Coding theorems for a discrete source with a ﬁdelity criterion. In Information and decision processes, pages 93–126. McGraw-Hill, New York, 1960. R. Sandler and M. Lindenbaum. Nonnegative matrix factorization with earth mover’s distance metric for image analysis. IEEE Transactions on Pattern Analysis and Machine Intelligence, 33(8):1590–1602, 2011. J.-L. Starck, F. Murtagh, and A. Bijaoui. Image processing and data analysis. Cambridge University Press, Cambridge, 1998. The multiscale approach. M. Talagrand. Matching random samples in many dimensions. Ann. Appl. Probab., 2(4):846–856, 1992. M. Talagrand. Concentration of measure and isoperimetric inequalities in product spaces. Inst. Hautes E´tudes Sci. Publ. Math., (81):73– 205, 1995. V. S. Varadarajan. On the convergence of sample probability distributions. Sankhy¯a, 19:23–26, 1958. C. Villani. Optimal transport: old and new, volume 338. Springer Science & Business Media, 2008. M. Wakin, D. Donoho, H. Choi, and R. G. Baraniuk. The multiscale structure of non-diﬀerentiable image manifolds. In Proc. SPIE. SPIE, 2005. L. S. Young. Dimension, entropy and Lyapunov exponents. Ergodic Theory Dynamical Systems, 2(1):109–124, 1982. M. Zhang, Y. Liu, H. Luan, M. Sun, T. Izuha, and J. Hao. Building earth mover’s distance on bilingual word embeddings for machine translation. In D. Schuurmans and M. P. Wellman, editors, Proceedings of the Thirtieth AAAI Conference on Artiﬁcial Intelligence, February 12-17, 2016, Phoenix, Arizona, USA., pages 2870–2876. AAAI Press, 2016.

Jonathan Weed Department of Mathematics Massachusetts Institute of Technology Cambridge, MA 02139, USA (jweed@mit.edu)

Francis Bach INRIA, D´epartement d’informatique de l’ENS CNRS PSL Research University 75005 Paris, France (francis.bach@inria.fr)

