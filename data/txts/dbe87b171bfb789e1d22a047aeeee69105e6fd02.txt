Sentence-T5: Scalable Sentence Encoders from Pre-trained Text-to-Text Models
Jianmo Ni, Gustavo Hernández Ábrego, Noah Constant, Ji Ma, Keith B. Hall, Daniel Cer, Yinfei Yang Google Research Mountain View, CA

arXiv:2108.08877v3 [cs.CL] 14 Dec 2021

Abstract
We provide the ﬁrst exploration of sentence embeddings from text-to-text transformers (T5). Sentence embeddings are broadly useful for language processing tasks. While T5 achieves impressive performance on language tasks cast as sequence-to-sequence mapping problems, it is unclear how to produce sentence embeddings from encoder-decoder models. We investigate three methods for extracting T5 sentence embeddings: two utilize only the T5 encoder and one uses the full T5 encoder-decoder model. To support our investigation, we establish a new sentence representation transfer benchmark, SentGLUE, which extends the SentEval toolkit to nine tasks from the GLUE benchmark (Wang et al., 2018). Our encoder-only models outperforms SentenceBERT (Reimers and Gurevych, 2019) and SimCSE (Gao et al., 2021) sentence embeddings on both SentEval and SentGLUE transfer tasks, including semantic textual similarity (STS). Scaling up T5 from millions to billions of parameters is found to produce consistent further improvements. Finally, our encoderdecoder method achieves a new state-of-theart on STS when using sentence embeddings.1
1 Introduction
Sentence embeddings providing compact meaning representations that are broadly useful for a variety of language processing tasks include classiﬁcation, question-answering, semantic retrieval, bitext mining, and semantic similarity tasks. Sentence embedding models have been trained using a variety of methods including: supervised tasks such as natural language inference (Conneau et al., 2017; Gao et al., 2021) or with semi-structured data such as question-answer pairs (Cer et al., 2018); translation pairs (Yang et al., 2020a; Feng et al., 2020); paraphrasing pairs (Wieting et al., 2016)
1Our models are released at https://tfhub.dev/ google/collections/sentence-t5/1.

Figure 1: Scaling up our ST5 model size improves performance on SentEval (left) and STS (right).

ST5-EncDec (11B params) ST5-Enc (11B params)
SimCSE-RoBERTa (large) (Gao et al., 2021) SBERT (large) (Reimers and Gurevych, 2019) USE (Cer et al., 2018) InferSent (Conneau et al., 2017)

Transfer 90.46 91.63 90.232 87.69 85.10 85.59

STS 84.94 84.96
83.76 76.55 71.22 65.01

Table 1: ST5 versus notable sentence embedding models on SentEval tasks. The reported numbers are the average of transfer tasks and STS tasks

and adjacent sentence pairs (Kiros et al., 2015; Logeswaran and Lee, 2018). Recent work has shown that scaling up model parameters and leveraging pre-trained models (Devlin et al., 2019; Liu et al., 2019) are two effective approaches to improve performance (Reimers and Gurevych, 2019, 2020; Yang et al., 2020b; Gao et al., 2021).
We explore sentence embeddings from a new family of pre-trained models: Text-to-Text Transfer Transformer (T5) (Raffel et al., 2020). Unlike encoder-only models, which use a transformer encoder to predict random masked tokens, T5 uses an encoder-decoder architecture and a generative span corruption pre-training task. T5 models can be scaled up to hundreds of billions of parameters (Fedus et al., 2021) and have achieved state-of-the-
2SimCSE-RoBERTa achieves the best performance on transfer tasks by adding an additional masked language model loss during training while ST5 and other models don’t.

Encoder Decoder Encoder Encoder Encoder Decoder

y1

y2

y3 ・

x1 x2

x3

x4

(a) T5 Encoder-Decoder

x1 x2

x3

x4

(b) ST5 Encoder-only (ST5-Enc) ﬁrst

x1 x2

x3

x4

(c) ST5 Encoder-only (ST5-Enc) mean

x1 x2

x3

x4

(d) ST5 Encoder-Decoder (ST5-EncDec) ﬁrst

Figure 2: Architecture diagrams for T5 and three ST5 variants to extract sentence representations from T5.

art performance on a broad range of NLP tasks including GLUE (Wang et al., 2018) and SuperGLUE (Wang et al., 2019). However, it is difﬁcult to efﬁciently apply T5 to some tasks such as retrieval or clustering. To score retrieval candidates, T5 would need to perform full inference with crossattention on each query-candidate pair. In contrast, sentence embeddings allow for efﬁcient retrieval and clustering (Gillick et al., 2018; Reimers and Gurevych, 2019; Yang et al., 2020a).
As shown in ﬁg. 2, we explore three ways of turning a pre-trained T5 encoder-decoder model into a sentence embedding model: (i) using the ﬁrst token representation of the encoder; (ii) averaging all token representations from the encoder; (iii) using the ﬁrst token representation from the decoder. We evaluate the quality of the resulting sentence embeddings on sentence transfer tasks using SentEval (Conneau and Kiela, 2018) and on semantic textual similarity (Agirre et al., 2012, 2013, 2014, 2015, 2016; Cer et al., 2017). We contrast raw representations from pre-trained T5 models with those learned through ﬁne-tuning on natural language inference (NLI) and Retrieval QuestionAnswering (ReQA) (Ahmad et al., 2019) using dual encoders and contrastive learning (Conneau et al., 2017; Cer et al., 2018; Yang et al., 2018; Gao et al., 2021). We introduce a multi-stage contrastive learning recipe involving ﬁne-tuning ﬁrst on ReQA and then on NLI. Finally, we investigate scaling our T5 sentence embedding model up to 11B parameters. As shown in ﬁg. 1, transfer tasks and STS both improve with increased model capacity.
To our knowledge, we are the ﬁrst to study using large-scale pre-trained text-to-text models for sentence representation learning and to scale sentence embedding models up to 11 billion parameters. We summarize our contributions as follows:

(i) even without ﬁne-tuning, encoder-only ST5 models perform well on sentence transfer tasks, outperforming state-of-the-art ﬁne-tuned models such as SimBERT and SimRoBERTa (Gao et al., 2021); (ii) encoder-decoder sentence embedding models achieve strong performance on STS, establishing a new state-of-the-art on sentence embedding based STS; (iii) contrastive learning is effective for ﬁne-tuning sentence encoders from T5-style pre-trained models, particularly using our proposed two-stage contrastive learning approach; (iv) training ST5 longer and with more data using a contrastive loss leads to consistent improvement on both sentence transfer and STS tasks; (v) creating a new sentence representation transfer benchmark ‘SentGLUE’ which extends the sentence evaluation toolkit (Conneau and Kiela, 2018) to nine tasks from GLUE (Wang et al., 2018) benchmark and evaluating ST5 and other state-of-the-art models on SentGLUE to compare their transfer performance on these challenging tasks. We name our model Sentence T5 (ST5).
2 Text-to-Text Transfer Transformers (T5)
Text-to-Text transfer transformers (T5) (Raffel et al., 2020) are gaining popularity due to their competitive performance and ease of use in solving a variety of tasks as simple text-to-text mapping problems. As shown in ﬁg. 2a, T5 consists of an encoder-decoder transformer model (Vaswani et al., 2017) pre-trained on an unsupervised span corruption task. Though T5 has been successfully applied to numerous NLP tasks, how to extract high quality text representations from T5 remains unexplored.

3 Sentence T5
3.1 Model Architecture
In this work we explore three strategies to extract sentence representations from T5, as shown in ﬁgs. 2b to 2d:
• Encoder-only ﬁrst (ST5-Enc ﬁrst): The encoder output of the ﬁrst token is taken as the sentence embedding.
• Encoder-only mean (ST5-Enc mean): The sentence embedding is deﬁned as the average of the encoder outputs across all input tokens.
• Encoder-Decoder ﬁrst (ST5-EncDec ﬁrst): The ﬁrst decoder output is taken as the sentence embedding. To obtain the decoder output, the input text is fed into the encoder, and the standard “start” symbol is fed as the ﬁrst decoder input.
The ﬁrst two are pooling strategies widely used in encoder-only pre-trained models such as BERT. Unlike BERT models, T5 models do not have a CLS token at the beginning of each sentence. For T5 encoder-decoder models, we assume the decoder is aware of the semantics of the entire input sentence when generating its ﬁrst token prediction; and if so, the ﬁrst decoder output embeddings (i.e. input to the softmax layer) might naturally capture the sentence semantics.
For sentence encoder training, we adopt a dual encoder architecture (Gillick et al., 2018; Cer et al., 2018; Reimers and Gurevych, 2019). As shown in ﬁg. 3, this architecture consists of two sharedweight transformer modules that encode the inputs. The transformer module can be either an encoderonly or encoder-decoder architecture. In our experiments, we initialize the transformer modules from the pre-trained T5 models. After each module computes a ﬁxed-length representation for its input sentence, a projection layer and L2 normalization are applied to the resulting embeddings. The projection layer transforms the output to a conﬁgurable ﬁxed dimensionality (i.e. the sentence embedding size). The embeddings from paired encoding towers can be scored for similarity tasks using a dotproduct3 or provide as input to additional layers layers for pairwise classiﬁcation tasks (e.g., NLI).
3Since L2 normalization is applied to the output of each tower, the dot-product between the embeddings will produce their cosine similarity.

Loss

Embedding 1 Projection & Norm
Transformer Encoder (optional Decoder)
Sentence 1

Embedding 2 Projection & Norm
Transformer Encoder (optional Decoder)
Sentence 2

Figure 3: Architecture of the dual encoder model.

3.2 Contrastive Learning
Applying contrastive learning to sentence embeddings improves the uniformity of the embeddings space, leading to better performance on downstream tasks such as STS (Gao et al., 2021). We apply contrastive learning to ﬁne-tune the T5 sentence representations.4

3.2.1 Contrastive Loss
Using a contrastive loss to train a sentence encoder requires paired examples D = {(vi, vi+)} as a training set, where vi is an input sentence and vi+ is a related sentence (e.g., that is semantically close). During training, vi+ is considered as a positive example for vi and all other examples in the batch are considered as negatives. The model should learn to pull the positive example closer to the input example while pushing away the negatives. We operationalize our contrastive loss using an in-batch sampled softmax (Henderson et al., 2017):

esim(vi,vi+)/τ

L=

+,

(1)

j∈B esim(vi,vj )/τ

The similarity scoring function is sim. B is a mini-
batch of examples and τ is the softmax temperature. When additional negatives vj− are provided for input example v, the loss can be computed as:

esim(vi,vi+)/τ

L=

+

− . (2)

j∈B esim(vi,vj )/τ + esim(vi,vj )/τ

4In preliminary experiments, we also explored ﬁne-tuning with the classiﬁcation loss used in InferSent (Conneau et al., 2017) and Sentence-BERT (Reimers and Gurevych, 2019). However we found ﬁne-tuning for classiﬁcation on an NLI dataset is inferior to contrastive learning as reported in (Gao et al., 2021)

3.3 Two-stage Training
To investigate the effect of additional training data, we explore two-stage training: (i) ﬁrst training on mined question-answering data from Community QA sites; (ii) then, ﬁne-tune the model on sentence pairs with human annotated NLI labels.
4 Experimental Setup
4.1 Training Corpus
For two-stage training, we use two datasets: one is collected from web forums while the other is from the Stanford Natural Language Inference (SNLI) dataset (Bowman et al., 2015). For the ﬁrst stage and similar to Cer et al. (2018), we collect 2 Billion question-answers pairs from community QA websites. During training, the associated answer is considered as the positive example for each input question. For the second stage, we utilize the contrastive version of the NLI dataset (Gao et al., 2021) containing 275K examples, where the positives are the ‘entailment’ hypothesis and premise pairs while the negatives are the ‘contradict’ pairs.
4.2 Evaluation
We evaluate using SentEval, which includes 7 transfer and 7 STS tasks (Conneau and Kiela, 2018). For the transfer tasks, sentence embeddings are evaluated by how well they perform as features for a linear classiﬁcation model. For STS, embeddings are evaluated by how well their cosine similarities correlate with human annotated similiarity scores.5
4.3 Conﬁgurations
Our models are implemented using JAX6 and trained on Cloud TPU-v8. We initialize the dual encoder modules from public T5 checkpoints 7. During training, we use Adafactor (Shazeer and Stern, 2018) as the optimizer and set the learning rate to 0.001. Linear decay is applied after 10% of the total number of training steps, reducing the learning rate to 0 by the end of training. To ﬁnetune on NLI we use a batch size of 512, while for the Community QA dataset the batch size is 2048. We use a softmax temperature τ of 0.01.
5Following SimCSE (Gao et al., 2021), we report Spearman’s correlation for the ‘all’ setting for all STS tasks which aggregates the data across different subsets.
6https://github.com/google/jax 7https://github.com/google-research/ text-to-text-transfer-transformer

5 Experimental Goals
Our experiments aim to answer the following:
• Q1: What is the best way to extract sentence representations from T5?
• Q2: How well do raw T5 sentence embeddings perform on downstream tasks?
• Q3: How much do contrastive sentence embedding tasks (e.g., NLI, QA) improve the T5 sentence embeddings.
• Q4: Can we beneﬁt from scaling up model capacity for better sentence representations?
With these goals, we study transfer and STS performance of T5 sentence embeddings using a variety of model and training conﬁgurations, comparing ST5 to state-of-the-art methods including SBERT/SRoBERTa (Reimers and Gurevych, 2019) and SimCSE (Gao et al., 2021).
6 Results
Table 2 and 3 provide performance on transfer and STS tasks, respectively. We compare ST5 models with two types of baselines: (ii) a model that extracts sentence embeddings from a pre-trained BERT model, listed in rows 1–2 of each table; (ii) the current state-of-the-art sentence embedding models ﬁne-tuned from BERT or RoBERTa, listed in rows 6–8 of each table.
6.1 Results for Raw T5 Sentence Embeddings
We ﬁrst evaluate the T5 sentence embeddings without ﬁne-tuning. We evaluate all three strategies from section 3.1: (i) Encoder-only ﬁrst token, (ii) Encoder-only mean, and (iii) Encoder-decoder start token. For all experiments, we use the encoder or decoder outputs from the T5 transformer directly, without doing any projection. This enables us to fully leverage the embedding capacity from the pre-trained models.
Transfer tasks Results for ST5 models using raw embeddings on transfer tasks are shown in rows 3–5 of table 2. Unlike BERT, T5’s ﬁrst token (either for encoder or decoder) is not reserved for a special placeholder (i.e. CLS) and there are no speciﬁc pre-training tasks using the ﬁrst token’s embeddings. Therefore, it is unlikely that without additional ﬁne-tuning the ﬁrst token’s representation would capture the semantics of the whole

Model
BERT (CLS-vector) BERT (mean) ♣ ST5-Enc ﬁrst ST5-Enc mean ST5-EncDec ﬁrst
SBERT-NLI ♣ SimCSE-BERT ♣ SimCSE-RoBERTa ♣
ST5-Enc mean ST5-EncDec ﬁrst ST5-Enc mean ST5-Enc-1.1 mean

Fine-tune data
N/A N/A N/A N/A N/A
NLI+MNLI NLI NLI
NLI NLI CommQA+NLI CommQA+NLI

MR
78.68 78.66 76.90 86.56 79.96
83.64 82.69 84.92
86.17 86.22 85.75 86.12

CR
84.85 86.25 86.38 91.31 77.93
89.43 89.25 92.00
91.71 91.60 92.08 92.50

SUBJ
94.21 94.37 90.93 96.01 91.02
94.39 94.81 94.11
94.70 94.05 94.58 94.73

MPQA
88.23 88.66 88.68 90.57 84.66
89.86 89.59 89.82
90.90 90.93 90.95 90.59

SST
84.13 84.40 80.01 90.77 86.27
88.96 87.31 91.27
90.44 90.72 91.76 92.15

TREC
91.4 92.80 94.40 94.60 84.00
89.60 88.40 88.80
90.00 92.60 96.40 95.80

MRPC
71.13 69.45 66.38 72.93 68.00
76.00 73.51 75.65
76.70 76.06 75.19 76.52

Avg
84.66 84.94 83.38 88.96 81.69
87.41 86.51 88.08
88.66 88.88 89.53 89.77

Table 2: Performance on transfer tasks on the SentEval benchmark. All models are using the Base architecture. ♣ results are from (Gao et al., 2021). For all tasks, a logistic regression classiﬁer is trained using the sentence
embeddings as features and the classiﬁcation accuracy on test sets are reported.

Model
BERT (CLS-vector) BERT (mean) ♣ ST5-Enc ﬁrst ST5-Enc mean ST5-EncDec ﬁrst
SBERT-NLI ♣ SimCSE-BERT ♣ SimCSE-RoBERTa ♣
ST5-Enc mean ST5-EncDec ﬁrst ST5-Enc mean ST5-Enc-1.1 mean

Fine-tune data
N/A N/A N/A N/A N/A
NLI+MNLI NLI NLI
NLI NLI CommQA+NLI CommQA+NLI

STS12
20.16 38.78 17.50 37.78 10.91
70.97 75.30 76.53
77.37 77.90 78.05 77.58

STS13
30.01 57.98 6.35 56.83 29.59
76.53 84.67 85.21
83.65 85.62 85.84 85.12

STS14
20.09 57.98 -20.70 49.37 14.90
73.19 80.19 80.95
80.41 82.24 82.19 81.46

STS15
36.88 63.15
2.29 65.48 28.91
79.09 85.40 86.03
86.04 86.81 87.46 87.14

STS16
38.08 61.06 21.87 64.68 30.61
74.30 80.82 82.57
81.70 82.13 84.03 82.89

STSb
16.50 46.35 16.71 57.51
9.45
77.03 84.25 85.83
84.49 84.98 86.04 85.82

SICK-R
42.63 58.40 28.60 60.11 39.31
72.91 80.39 80.50
79.79 79.97 79.75 80.18

Avg
29.19 54.81 10.37 55.97 23.38
74.89 81.57 82.52
81.92 82.81 83.34 82.88

Table 3: Spearman’s correlation coefﬁcient (×100) on STS tasks on the SentEval benchmark. All models are using the Base architecture. ♣ results are from (Gao et al., 2021).

sentence. Indeed, our experiments show the ﬁrst token’s representation from encoder or decoder are much worse on all SentEval tasks compared to the mean pooling of the encoder-only model.
When mean pooling is applied to the T5’s encoder outputs, it greatly outperforms the average embeddings of BERT. Notably, even without ﬁne-tuning, the average embeddings of the T5’s encoder-only outputs outperforms SimCSERoBERTa, which is ﬁne-tuned on NLI dataset. This may be due to the fact that T5 is trained on more data. The original T5 models also included downstream tasks (e.g. GLUE, SuperGLUE) during pretraining, and this multi-task setting may improve transfer performance. However we note that there are only two SentEval tasks (SST and MRPC) included in GLUE while the other ﬁve tasks are not. As shown in table 2, we observe signiﬁcant improvements on the ﬁve tasks that are not included.
STS tasks In contrast, we observe weak results on STS tasks using raw T5 sentence embeddings

as shown in rows 3–5 of table 3. The mean pooling of T5 embeddings achieves an average STS score of 55.97, slightly better than BERT mean pooling but still worse than models ﬁne-tuned on supervised tasks. This is similar to ﬁndings about the anisotropy phenomenon of contextual embeddings from other pre-trained language models such as BERT, RoBERTa (Ethayarajh, 2019; Gao et al., 2021). Embedding collapse prevents the model from performing well on distance-related metrics.
6.2 Results for Fine-Tuning T5 Sentence Embeddings
We next evaluate ST5 models that are ﬁne-tuned on NLI tasks using our contrastive loss, starting from pre-trained T5 models.
Given that mean pooling performs much better than the ﬁrst token when using encoder only, we opt to discard the ﬁrst token model when ﬁne-tuning ST5 models. The last three rows of table 2 show that the transfer performance of ST5 models is very consistent across different embedding extracting

strategies after ﬁne-tuning. The best ﬁne-tuned model is 0.57 better than the best raw T5 sentence embeddings.
In table 3, we see that ﬁne-tuning on the NLI dataset signiﬁcantly improves the STS task performance of ST5 compared to that without ﬁne-tuning. This supports the claim that contrastive learning is effective to mitigate embedding collapse for T5style models.
To investigate the impact of additional training data on contrastive learning, we experiment with the ST5 models ﬁrst trained on Community QA and then ﬁne-tuned on NLI. As shown in tables 2 and 3, ﬁne-tuning on an additional dataset brings a large performance boost for both transfer and STS tasks. This suggests that we may be able to improve sentence embedding quality further through the mining of additional semi-structured data for continued contrastive learning.
To exclude the effect of mixing in downstream tasks, we also trained a ST5 variant based on the T5 1.1 model which only pre-trained on the C4 dataset (Raffel et al., 2020). As shown on the last row of table 2 and table 3, it achieves comparable performance to the original T5 model, outperforming on most tasks but under-performing on STS.
6.3 Encoder-only vs. Encoder-decoder
In this section, we compare the performance of two architectures: encoder-only and encoder-decoder.
Better generalizability for T5’s encoder In table 2, we saw that the encoder-only Base model performs on-par with the encoder-decoder model on transfer tasks. When we scale the ST5 model up from Base to Large, 3B and 11B, the encoder-only models’ performance on transfer tasks consistently outperforms the encoder-decoder models as shown in table 5. This shows that building ST5 on top of the T5’s encoder gives strong transfer performance.
Recently, Chung et al. (2021) have shown that larger output embeddings (i.e. larger embedding size) effectively prevent the encoder from overspecializing to the pre-training task, thus making the encoder’s representations more general and more transferable. We hypothesize that the decoder in the encoder-decoder architecture can improve the generalizability of the encoder’s representation in a similar fashion, as the decoder focuses on optimizing for speciﬁc tasks.
Effectiveness of the decoder In the last two rows of table 3, we observe that the encoder-

# of params Model

Base Large

3B 11B

ST5-Enc

110M 335M 1.24B 4.8B

ST5-EncDec

220M 770M

3B 11B

Table 4: Number of parameters for different models.

decoder architecture outperforms encoder-only models for all STS tasks. As we scale up the ST5 model, we also observe improvement on STS tasks. As shown in table 5, the ST5 encoder-decoder Large model outperforms the state-of-the-art model SimCSE-RoBERTa Large, improving the Spearman’s correlation score from 83.76 to 84.11.
One explanation is that the additional parameters from the decoder are helpful for improving textual similarity tasks. Another possibility is that the decoder architecture itself helps to improve the sentence embedding quality. As shown in ﬁg. 2d, the decoder can be considered as an additional attention pooling layer on top of the encoder outputs. As the decoder’s weights are lifted from the pretrained T5 model, the decoder might learn a better way to add attention pooling over the encoder outputs than mean pooling.
7 Scaling up Sentence T5
We leverage the existing checkpoints from large T5 models to study the effect of scaling sentence encoders. The parameters of the T5 models are listed in table 4. Note however that ST5-EncDec doesn’t fully leverage the model parameters; the decoder’s learned self-attention is effectively ignored as only the start token is fed into the decoder.
7.1 Effect on Directly Using T5 Embeddings
As shown in table 5, the performance on the transfer tasks of directly using T5 embeddings consistently improves as T5 scales up. This corroborates that large pre-trained models can improve transfer performance of sentence embeddings.
On the other hand, increasing the model capacity alone is not enough to mitigate the embedding collapse. Even the embeddings from the T5 11B model still do worse on STS tasks than ﬁne-tuned models. One reason is that the pre-training snap corruption task of T5 does not require the model to avoid anisotropy (e.g., by using a contrastive loss or regularization). This highlights the importance of choosing ﬁne-tuning tasks that are aligned to the goal of similarity/retrieval performance.

Model
ST5-Enc mean (Large) ST5-Enc mean (3B) ST5-Enc mean (11B) SBERT-NLI Large ♣ SimCSE-RoBERTa Large ♣
ST5-Enc mean (Large) ST5-EncDec ﬁrst (Large) ST5-Enc mean (3B) ST5-EncDec ﬁrst (3B) ST5-Enc mean (11B) ST5-EncDec ﬁrst (11B) ST5-Enc mean (Large) ST5-Enc mean (3B) ST5-Enc mean (11B)
Model
ST5-Enc mean (Large) ST5-Enc mean (3B) ST5-Enc mean (11B) SBERT-NLI Large ♣ SimCSE-RoBERTa Large ♣
ST5-Enc mean (Large) ST5-EncDec ﬁrst (Large) ST5-Enc mean (3B) ST5-EncDec ﬁrst (3B) ST5-Enc mean (11B) ST5-EncDec ﬁrst (11B) ST5-Enc mean (Large) ST5-Enc mean (3B) ST5-Enc mean (11B)

Fine-tune data
N/A N/A N/A
NLI+MNLI NLI
NLI NLI NLI NLI NLI NLI CommQA+NLI CommQA+NLI CommQA+NLI
Fine-tune data
N/A N/A N/A
NLI+MNLI NLI
NLI NLI NLI NLI NLI NLI CommQA+NLI CommQA+NLI CommQA+NLI

MR
89.13 90.35 91.15
84.88 88.12
88.82 87.63 89.92 87.83 90.13 90.00 88.89 89.94 90.83
STS12
28.01 24.89 34.97
72.27 77.46
76.52 79.15 77.13 79.24 77.42 80.11 79.10 79.02 80.10

CR
92.69 92.77 93.33
90.07 92.37
93.43 92.85 93.27 92.85 93.85 93.94 93.46 94.09 94.44
STS13
52.60 51.49 60.19
78.46 87.27
85.75 87.42 86.73 87.80 87.50 88.78 87.32 88.80 88.75

SUBJ
97.06 97.43 97.55
94.52 95.11
95.73 94.32 96.19 94.75 96.02 95.01 95.38 95.85 96.33
STS14
41.35 41.09 47.59
74.90 82.36
81.01 83.61 82.53 83.95 82.51 84.33 83.17 84.33 84.70

MPQA
90.70 90.15 90.20
90.33 90.49
91.75 91.37 91.54 91.01 91.39 91.53 91.50 91.58 91.68
STS15
61.28 61.37 66.40
80.99 86.66
87.13 87.64 87.36 87.75 87.47 88.36 88.27 88.89 88.86

SST
92.92 93.85 94.07
90.66 92.75
93.08 91.98 94.18 93.14 93.96 93.85 94.23 94.84 94.84
STS16
63.58 64.51 70.62
76.25 83.93
83.26 83.92 84.51 84.60 84.88 85.55 84.36 85.31 85.17

TREC
93.60 95.60 94.40
87.40 91.80
94.00 93.00 94.20 93.60 95.20 92.20 96.20 96.20 95.40
STSb
56.31 52.57 62.83
79.23 86.70
85.45 86.35 85.71 86.62 85.61 86.82 86.73 86.25 86.77

MRPC
73.74 72.70 74.26
75.94 76.64
76.35 76.99 76.87 78.26 76.99 76.70 77.10 77.86 77.91
SICK-R
59.48 59.99 63.57
73.75 81.95
79.85 80.64 81.39 80.91 80.77 80.60 79.84 79.51 80.39

Avg
89.98 90.41 90.71
87.69 89.61
90.45 89.73 90.88 90.21 91.08 90.46 90.97 91.48 91.63
Avg
51.80 50.85 58.02
76.55 83.76
82.71 84.11 83.62 84.41 83.74 84.94 84.11 84.59 84.96

Table 5: Comparisons of models’ performance on SentEval benchmark when scaling up model size. ♣ results are from (Gao et al., 2021). The ﬁrst set of results are for the transfer task; the second set are for the similarity task.

7.2 Improving the ST5 Fine-tuning

measure the quality of the sentence embeddings:

As shown in table 5, we ﬁnd that scaling up model capacity leads to consistently better performance on all downstream tasks. For the ST5 11B model, the encoder-only model achieves an average score of 91.08 for transfer tasks which is better than 90.45 from the ST5 Large model; while the encoderdecoder model pushes the STS score to 84.94 that also outperforms the ST5 Large model. This inspires us to explore even larger model sizes to achieve better sentence embedding quality.
For STS tasks, we observe that the gain from increasing model size from 3B to 11B is smaller than that from Large to 3B. This might be due to the fact that the embedding sizes are ﬁxed for all models in our experiments. One potential exploration is to increase the sentence embedding size for larger models to fully leverage the model capacity.
We further compute the alignment loss and uniformity loss as deﬁned in Wang and Isola (2020) to

Lalign = − E f (v) − f (v+) (3)
v ,v + ∼ppos
Luniform = log E e−2 f(v)−f(w) , (4)
v,wi∼.i.dpdata
where ppos is all positive data and pdata is the data distribution. Lalign denotes the expected distance between embeddings of the positive pairs of data, while Luniform indicates how uniformly the embeddings are distributed. For both losses, lower numbers indicate better performance. As shown in ﬁg. 4, when models scale up, both the encoder and encoder-decoder models decrease the uniformity loss with only a slight increase in alignment loss.
We seek to investigate whether the effects of larger model size and more training data are additive for better sentence embeddings. As shown in the last two rows of table 5, when scaling up to Large and 3B parameters, ST5 further improves on downstream tasks by training on the Community QA dataset in addition to NLI.

Model
InferSent (Wang et al., 2018) SBERT (RoBERTa Base) ♣ SBERT (RoBERTa Large) ♣ SimCSE (RoBERTa Base) ♣ SimCSE (RoBERTa Large) ♣
ST5 Enc (Base) ST5 Enc (Large) ST5 Enc (3B) ST5 Enc (11B) ST5 Enc 1.1 (Base) ST5 Enc-Dec (Base)
T5 (Base) (Raffel et al., 2020)

Sent. Embed. Fine-tuning
NLI NLI+MNLI NLI+MNLI
NLI NLI
CommQA+NLI CommQA+NLI CommQA+NLI CommQA+NLI CommQA+NLI
NLI
-

Score
66.71 73.40 75.81 77.05 76.23
76.89 78.52 79.06 80.07 76.63 76.37
83.40

CoLA
8.60 21.22 20.69 35.87 40.11
22.73 29.46 34.78 43.91 21.59 17.46
53.84

SST-2
83.90 90.83 93.00 90.71 93.23
91.40 93.92 94.95 95.30 90.60 90.71
92.68

MRPC
76.50 73.34 73.39 76.47 70.23
76.88 77.26 78.71 78.46 76.66 76.44
88.92

STS-B
80.20 74.08 76.26 83.93 81.45
86.58 86.07 85.84 86.54 86.34 85.98
88.02

QQP
81.70 80.75 82.26 81.39 84.45
84.55 85.32 85.78 86.21 84.53 82.65
91.56

MNLI-m
67.80 77.21 79.46 70.74 73.44
69.73 72.20 72.38 73.46 70.40 70.49
84.24

MNLI-mm
78.13 80.18 72.05 73.56
70.00 72.44 73.10 74.42 70.76 70.96
84.57

QNLI
63.50 73.92 75.80 76.30 75.95
79.54 79.64 79.70 80.12 77.92 76.20
90.48

RTE
71.50 57.76 60.65 60.29 60.65
61.73 66.43 66.06 66.06 61.01 63.18
76.28

Table 6: Performance on transfer tasks on the Dev set of the GLUE benchmark. ♣ denotes that the models are released by HuggingFace. T5 (base) is a cross-attention model and other models are embedding based.

Enc-11B
EncDec11B

Enc-3B

Enc-Large Enc-Base

EncDec3B
Scaling up

EncDecLarge

EncDecBase

Figure 4: Alignment and uniformity losses for different model sizes. We consider the test split of the STS-B dataset. Lalign is calculated considering all pairs with score greater than 4. Luniform is computed using all sentences. The colormap denotes the models’ Spearman’s correlation score.
8 SentGLUE Evaluation
In this section we introduce a new sentence representation transfer benchmark – SentGLUE – which extends the sentence evaluation toolkit to nine challenge tasks from GLUE benchmark including: CoLA, SST-2, MRPC, STS-B, QQP, MNLI-m, MNLI-mm, QNLI, RTE 8. The GLUE benchmark has been widely adopted for measuring language understanding models. GLUE tasks are either single sentence or sentence pair classiﬁcation (e.g. NLI) or similarity (STS) tasks. The best models on the GLUE leaderboard are ﬁne-tuned crossattention models like BERT or T5. Such models change all the parameters in the underlying model during ﬁne-tuning and for the pairwise tasks they allow for early fusion of input features from both sentences being compared. For SentGLUE, we introduce the constraint that each input needs to
8We found the WNLI task from GLUE benchmark is too challenge for existing sentence embedding models, thus we exclude it in the current version.

be independently encoded into a ﬁxed embedding space representation that can then be feed to additional layers in order to make a prediction. We believe this best adapts the spirit of the original SentEval benchmark for sentence embeddings to the GLUE benchmark tasks.
From table 6, ST5-Enc Base outperforms both SBERT-RoBERTa Base and SimCSE-RoBERTa Base on all SentGLUE tasks except CoLA and MNLI.9 With its increased model capacity, ST5 Enc 11B’s sentence embeddings achieve the best overall performance. Notably, as model size is scaled up, aggregate performance using sentence embeddings approaches that of T5 base. This is remarkable given that T5 base makes use of full crossattention between sentence pairs and adjusts all of the parameters in the model during ﬁne-tuning.
9 Conclusion
In this paper, we study effective methods to build T5 sentence encoders (ST5) from pre-trained models. We propose three architectures and a two-stage contrastive learning method to ﬁne-tune ST5. We compare the difference between encoder-only and encoder-decoder architectures as sentence encoders and analyze their performance on downstream tasks. Through extensive experiments on the SentEval benchmark, we show that encoder-only models have strong transfer performance while encoderdecoder models perform better on textual similarity tasks. We also demonstrate the effectiveness of scaling up the model size, which greatly improves sentence embedding quality. These ﬁndings suggest that future improvements in scale and quality of pre-trained text-to-text models may translate into further gains for sentence encoder models.
9MNLI-m and MNLI-mm experiments for SBERT RoBERTa Large and SimCSE RoBERTa Base are still running at the time of submission.

Acknowledgments
We thank Zora Tung, Daniel Andor, Adam Roberts, Hyung Won Chung, Anselm Levskaya and Livio Baldini Soares for help with the JAX implementation, and Alexis Conneau and Chris Tar for feedback and suggestions.
References
Eneko Agirre, Carmen Banea, Claire Cardie, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei Guo, Iñigo Lopez-Gazpio, Montse Maritxalar, Rada Mihalcea, German Rigau, Larraitz Uria, and Janyce Wiebe. 2015. SemEval-2015 task 2: Semantic textual similarity, English, Spanish and pilot on interpretability. In SemEval 2015.
Eneko Agirre, Carmen Banea, Claire Cardie, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei Guo, Rada Mihalcea, German Rigau, and Janyce Wiebe. 2014. SemEval-2014 task 10: Multilingual semantic textual similarity. In SemEval 2014.
Eneko Agirre, Carmen Banea, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, Rada Mihalcea, German Rigau, and Janyce Wiebe. 2016. SemEval-2016 task 1: Semantic textual similarity, monolingual and cross-lingual evaluation. In SemEval-2016.
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor Gonzalez-Agirre. 2012. SemEval-2012 task 6: A pilot on semantic textual similarity. In *SEM 2012/SemEval 2012.
Eneko Agirre, Daniel Cer, Mona Diab, Aitor GonzalezAgirre, and Weiwei Guo. 2013. *SEM 2013 shared task: Semantic textual similarity. In *SEM.
Amin Ahmad, Noah Constant, Yinfei Yang, and Daniel Cer. 2019. ReQA: An evaluation for end-to-end answer retrieval models. In Workshop on Machine Reading for Question Answering.
Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. 2015. A large annotated corpus for learning natural language inference. In EMNLP.
Daniel Matthew Cer, Mona T. Diab, Eneko Agirre, I. Lopez-Gazpio, and Lucia Specia. 2017. Semeval2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation. In SemEval2016.
Daniel Matthew Cer, Yinfei Yang, Sheng yi Kong, Nan Hua, Nicole Limtiaco, Rhomni St. John, Noah Constant, Mario Guajardo-Cespedes, Steve Yuan, C. Tar, Yun-Hsuan Sung, B. Strope, and R. Kurzweil. 2018. Universal sentence encoder. In EMNLP.
Hyung Won Chung, Thibault Fevry, Henry Tsai, Melvin Johnson, and Sebastian Ruder. 2021. Rethinking embedding coupling in pre-trained language models. In ICLR.

Alexis Conneau and Douwe Kiela. 2018. Senteval: An evaluation toolkit for universal sentence representations. In LREC.
Alexis Conneau, Douwe Kiela, Holger Schwenk, Loïc Barrault, and Antoine Bordes. 2017. Supervised learning of universal sentence representations from natural language inference data. In EMNLP.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. Bert: Pre-training of deep bidirectional transformers for language understanding. In NAACL.
Kawin Ethayarajh. 2019. How contextual are contextualized word representations? comparing the geometry of bert, elmo, and gpt-2 embeddings. In EMNLP.
W. Fedus, Barret Zoph, and Noam M. Shazeer. 2021. Switch transformers: Scaling to trillion parameter models with simple and efﬁcient sparsity. ArXiv, abs/2101.03961.
Fangxiaoyu Feng, Yinfei Yang, Daniel Cer, Naveen Arivazhagan, and Wei Wang. 2020. Languageagnostic BERT sentence embedding. CoRR, abs/2007.01852.
Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021. Simcse: Simple contrastive learning of sentence embeddings. ArXiv, abs/2104.08821.
D. Gillick, A. Presta, and Gaurav Singh Tomar. 2018. End-to-end retrieval in continuous space. ArXiv, abs/1811.08008.
Matthew Henderson, Rami Al-Rfou, B. Strope, YunHsuan Sung, László Lukács, Ruiqi Guo, Sanjiv Kumar, Balint Miklos, and R. Kurzweil. 2017. Efﬁcient natural language response suggestion for smart reply. ArXiv, abs/1705.00652.
Ryan Kiros, Yukun Zhu, R. Salakhutdinov, R. Zemel, R. Urtasun, A. Torralba, and S. Fidler. 2015. Skipthought vectors. In NIPS.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, M. Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. ArXiv, abs/1907.11692.
Lajanugen Logeswaran and Honglak Lee. 2018. An efﬁcient framework for learning sentence representations. In ICLR.
Colin Raffel, Noam M. Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, W. Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a uniﬁed textto-text transformer. JMLR, 21/140.
Nils Reimers and Iryna Gurevych. 2019. Sentencebert: Sentence embeddings using siamese bertnetworks. In EMNLP/IJCNLP.

Nils Reimers and Iryna Gurevych. 2020. Making monolingual sentence embeddings multilingual using knowledge distillation. In EMNLP.
Noam M. Shazeer and Mitchell Stern. 2018. Adafactor: Adaptive learning rates with sublinear memory cost. In ICML.
Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Neurips.
Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. 2019. Superglue: A stickier benchmark for general-purpose language understanding systems. In NeurIPS.
Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. 2018. Glue: A multi-task benchmark and analysis platform for natural language understanding. In BlackboxNLP@EMNLP.
Tongzhou Wang and Phillip Isola. 2020. Understanding contrastive representation learning through alignment and uniformity on the hypersphere. In ICML.
John Wieting, Mohit Bansal, Kevin Gimpel, and Karen Livescu. 2016. Towards universal paraphrastic sentence embeddings. CoRR, abs/1511.08198.
Yinfei Yang, Daniel Matthew Cer, Amin Ahmad, Mandy Guo, Jax Law, Noah Constant, G. Ábrego, Steve Yuan, C. Tar, Yun-Hsuan Sung, B. Strope, and R. Kurzweil. 2020a. Multilingual universal sentence encoder for semantic retrieval. In ACL.
Yinfei Yang, Steve Yuan, Daniel Matthew Cer, Sheng yi Kong, Noah Constant, Petr Pilar, Heming Ge, Yun-Hsuan Sung, B. Strope, and R. Kurzweil. 2018. Learning semantic textual similarity from conversations. In Rep4NLP@ACL.
Ziyi Yang, Yinfei Yang, Daniel Cer, Jax Law, and Eric Darve. 2020b. Universal sentence representation learning with conditional masked language model.

We also report the speed on Nvidia Tesla V100 GPU and CPU. The ST5 11B model is able to run on 4 V100 GPUs with sequence length 128 and batch size 1024, achieving an inference speed of 27 examples per second. For CPU, with batch size 512, ST5 11B achieves 0.5 examples per second.
Although the speed on GPU and CPU are considerably slower than on TPU, the sentence embedding models are much faster than cross-attention based models whose computation time increases quadratically with the number of examples (e.g., clustering 1,000 sentences requires inference over 1 million sentence pairs).

A Model Inference
We run ST5 encoder-only on different platforms to investigate the computational cost of inference. Figure 5 summarizes the inference speed for different model sizes, sequence length, batch size and platforms. ST5 achieves the fastest inference speed on Cloud TPU-v8. As we increase the batch size, the inference speed can be further improved. For the 11B model, we are able to achieve a speed of 274 examples per second for sequence length 128 and batch size 1024. This shows the feasibility of deploying such large models on TPU hardware.

(a) TPU inference speed vs. sequence length.
(b) GPU inference speed vs. sequence length.
(c) CPU inference speed vs. sequence length. Figure 5: Comparison of inference speed for different model sizes on different platforms.

