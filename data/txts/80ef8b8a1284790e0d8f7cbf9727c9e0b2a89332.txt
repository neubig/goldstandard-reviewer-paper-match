Detecting and Correcting for Label Shift with Black Box Predictors

arXiv:1802.03916v3 [cs.LG] 26 Jul 2018

Zachary C. Lipton * 1 2 Yu-Xiang Wang * 2 3 Alexander J. Smola 2

Abstract
Faced with distribution shift between training and test set, we wish to detect and quantify the shift, and to correct our classiﬁers without test set labels. Motivated by medical diagnosis, where diseases (targets), cause symptoms (observations), we focus on label shift, where the label marginal p(y) changes but the conditional p(x|y) does not. We propose Black Box Shift Estimation (BBSE) to estimate the test distribution p(y). BBSE exploits arbitrary black box predictors to reduce dimensionality prior to shift correction. While better predictors give tighter estimates, BBSE works even when predictors are biased, inaccurate, or uncalibrated, so long as their confusion matrices are invertible. We prove BBSE’s consistency, bound its error, and introduce a statistical test that uses BBSE to detect shift. We also leverage BBSE to correct classiﬁers. Experiments demonstrate accurate estimates and improved prediction, even on high-dimensional datasets of natural images.
1. Introduction
Assume that in August we train a pneumonia predictor. Our features consist of chest X-rays administered in the previous year (distribution P ) and the labels binary indicators of whether a physician diagnoses the patient with pneumonia. We train a model f to predict pneumonia given an X-ray image. Assume that in the training set .1% of patients have pneumonia. We deploy f in the clinic and for several months, it reliably predicts roughly .1% positive.
Fast-forward to January (distribution Q): Running f on the last week’s data, we ﬁnd that 5% of patients are predicted to have pneumonia! Because f remains ﬁxed, the shift must owe to a change in the marginal p(x), violating the familiar
*Equal contribution 1Carnegie Mellon University, Pittsburgh, PA 2Amazon AI, Palo Alto, CA 3UC Santa Barbara, CA. Correspondence to: Zachary C. Lipton <zlipton@cmu.edu >, YuXiang Wang <yuxiangw@amazon.com>, Alexander J. Smola <smola@amazon.com>.
Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).

iid assumption. Absent familiar guarantees, we wonder: Is f still accurate? What’s the real current rate of pneumonia? Shouldn’t our classiﬁer, trained under an obsolete prior, underestimate pneumonia when uncertain? Thus, we might suspect that the real prevalence is greater than 5%.
Given only labeled training data, and unlabeled test data, we desire to: (i) detect distribution shift, (ii) quantify it, and (iii) correct our model to perform well on the new data. Absent assumptions on how p(y, x) changes, the task is impossible. However, under assumptions about what P and Q have in common, we can still make headway. Two candidates are covariate shift (where p(y|x) does not change) and label shift (where p(x|y) does not change). Schölkopf et al. (2012) observe that covariate shift corresponds to causal learning (predicting effects), and label shift to anticausal learning (predicting causes).
We focus on label shift, motivated by diagnosis (diseases cause symptoms) and recognition tasks (objects cause sensory observations). During a pneumonia outbreak, p(y|x) (e.g. ﬂu given cough) might rise but the manifestations of the disease p(x|y) might not change. Formally, under label shift, we can factorize the target distribution as
q(y, x) = q(y)p(x|y).
By contrast, under the covariate shift assumption, q(y, x) = q(x)p(y|x), e.g. the distribution of radiologic ﬁndings p(x) changes, but the conditional probability of pneumonia p(y|x) remains constant. To see how this can go wrong, consider: what if our only feature were cough? Normally, cough may not (strongly) indicate pneumonia. But during an epidemic, P(pneumonia|cough) might go up substantially. Despite its importance, label shift is comparatively underinvestigated, perhaps because given samples from both p(x) and q(x), quantifying q(x)/p(x) is more intuitive.
We introduce Black Box Shift Estimation (BBSE) to estimate label shift using a black box predictor f . BBSE estimates the ratios wl = q(yl)/p(yl) for each label l, requiring only that the expected confusion matrix is invertible 1. We estimate wˆ by solving a linear system Ax = b where A is the confusion matrix of f estimated on training data (from
1 For degenerate confusion matrices, a variant using soft predictions may be preferable.

Detecting and Correcting for Label Shift with Black Box Predictors

P) and b is the average output of f calculated on test samples (from Q). We make the following contributions:
1. Consistency and error bounds for BBSE.
2. Applications of BBSE to statistical tests for detecting distribution label shift
3. Model correction through importance-weighted Empirical Risk Minimization.
4. A comprehensive empirical validation of BBSE.
Compared to approaches based on Kernel Mean Matching (KMM) (Zhang et al., 2013), EM (Chan & Ng, 2005), and Bayesian inference (Storkey, 2009), BBSE offers the following advantages: (i) Accuracy does not depend on data dimensionality; (ii) Works with arbitrary black box predictors, even biased, uncalibrated, or inaccurate models; (iii) Exploits advances in deep learning while retaining theoretical guarantees: better predictors provably lower sample complexity; and (iv) Due to generality, could be a standard diagnostic / corrective tool for arbitrary ML models.
2. Prior Work
Despite its wide applicability, learning under label shift with unknown q(y) remains curiously under-explored. Noting the difﬁculty of the problem, Storkey (2009) proposes placing a (meta-)prior over p(y) and inferring the posterior distribution from unlabeled test data. Their approach requires explicitly estimating p(x|y), which may not be feasible in high-dimensional datasets. Chan & Ng (2005) infer q(y) using EM but their method also requires estimating p(x|y). Schölkopf et al. (2012) articulates connections between label shift and anti-causal learning and Zhang et al. (2013) extend the kernel mean matching approach due to (Gretton et al., 2009) to the label shift problem. When q(y) is known, label shift simpliﬁes to the problem of changing base rates (Bishop, 1995; Elkan, 2001). Previous methods require estimating q(x), q(x)/p(x), or p(x|y), often relying on kernel methods, which scale poorly with dataset size and underperform on high-dimensional data.
Covariate shift, also called sample selection bias, is wellstudied (Zadrozny, 2004; Huang et al., 2007; Sugiyama et al., 2008; Gretton et al., 2009). Shimodaira (2000) proposed correcting models via weighting examples in ERM by q(x)/p(x). Later works estimate importance weights from the available data, e.g., Gretton et al. (2009) propose kernel mean matching to re-weight training points.
The earliest relevant work to ours comes from econometrics and addresses the use of non-random samples to estimate behavior. Heckman (1977) addresses sample selection bias, while (Manski & Lerman, 1977) investigates estimating parameters under choice-based and endogenous stratiﬁed sampling, cases analogous to a shift in the label distribution.

Also related, Rosenbaum & Rubin (1983) introduce propensity scoring to design unbiased experiments. Finally, we note a connection to cognitive science work showing that humans classify items differently depending on other items they appear alongside (Zhu et al., 2010).
Post-submission, we learned of antecedents for our estimator in epidemiology (Buck et al., 1966) and revisited by Forman (2008); Saerens et al. (2002). These papers do not develop our theoretical guarantees or explore the modern ML setting where x is massively higher-dimensional than y, bolstering the value of dimensionality reduction.
3. Problem setup
We use x ∈ X = Rd and y ∈ Y to denote the feature and label variables. For simplicity, we assume that Y is a discrete domain equivalent to {1, 2, ..., k}. Let P, Q be the source and target distributions deﬁned on X × Y. We use p, q to denote the probability density function (pdf) or probability mass function (pmf) associated with P and Q respectively. The random variable of interest is clear from context. For example, p(y) is the p.m.f. of y ∼ P and q(x) is the p.d.f. of x ∼ Q. Moreover, p(y = i) and q(y = i) are short for PP (y = i) and PQ(y = i) respectively, where P(S) := E[1(S)] denotes the probability of an event S and E[·] denotes the expectation. Subscripts P and Q on these operators make the referenced distribution clear.
In standard supervised learning, the learner observes training data (x1, y1), (x2, y2), ..., (xn, yn) drawn iid from a training (or source) distribution P . We denote the collection of feature vectors by X ∈ Rn×d and the label by y. Under Domain Adaptation (DA), the learner additionally observes a collection of samples X = [x1; ...; xm] drawn iid from a test (or target) distribution Q. Our objective in DA is to predict well for samples drawn from Q.
In general, this task is impossible – P and Q might not share support. This paper considers 3 extra assumptions:
A.1 The label shift (also known as target shift) assumption
p(x|y) = q(x|y) ∀ x ∈ X , y ∈ Y.
A.2 For every y ∈ Y with q(y) > 0 we require p(y) > 0.2 A.3 Access to a black box predictor f : X → Y where the
expected confusion matrix Cp(f ) is invertible.
CP (f ) := p(f (x), y) ∈ R|Y|×|Y|
We now comment on the assumptions. A.1 corresponds to anti-causal learning. This assumption is strong but reasonable in many practical situations, including medical diagnosis, where diseases cause symptoms. It also applies
2Assumes the absolute continuity of the (hidden) target label’s distribution with respect to the source’s, i.e. dq(y)/dp(y) exists.

Detecting and Correcting for Label Shift with Black Box Predictors

when classiﬁers are trained on non-representative class distributions: Note that while visions systems are commonly trained with balanced classes (Deng et al., 2009), the true class distribution for real tasks is rarely uniform.

Assumption A.2 addresses identiﬁability, requiring that the target label distribution’s support be a subset of training distribution’s. For discrete Y, this simply means that the training data should contain examples from every class.

Assumption A.3 requires that the expected predictor outputs for each class be linearly independent. This assumption holds in the typical case where the classiﬁer predicts class yi more often given images actually belong to yi than given images from any other class yj. In practice, f could be a neural network, a boosted decision-tree or any other classiﬁer trained on a holdout training data set. We can verify at training time that the empirical estimated normalized confusion matrix is invertible. Assumption A.3 generalizes naturally to soft-classiﬁers, where f outputs a probability distribution supported on Y. Thus BBSE can be applied even when the confusion matrix is degenerate.

We wish to estimate w(y) := q(y)/p(y) for every y ∈

Y with training data, unlabeled test data and a pre-

dictor f . This estimate enables DA techniques under

the importance-weighted ERM framework, which solves

min

n i=1

wi

(yi, xi),

using

wi

=

q(xi, yi)/p(xi, yi).

Under the label shift assumption, the importance weight

wi = q(yi)/p(yi). This task isn’t straightforward because

we don’t observe samples from q(y).

4. Main results
We now derive the main results for estimating w(y) and q(y). Assumption A.1 has the following implication: Lemma 1. Denote by yˆ = f (x) the output of a ﬁxed function f : X → Y. If Assumption A.1 holds, then
q(yˆ|y) = p(yˆ|y).

The proof is simple: recall that yˆ depends on y only via x. By A.1, p(x|y) = q(x|y) and thus q(yˆ|y) = p(yˆ|y).
Next, combine the law of total probability and Lemma 1 and we arrive at
q(yˆ) = q(yˆ|y)q(y)
y∈Y
q(y) = p(yˆ|y)q(y) = p(yˆ, y) . (1)
y∈Y y∈Y p(y)
We estimate p(yˆ|y) and p(yˆ, y) using f and data from source distribution P , and q(yˆ) with unlabeled test data drawn from target distribution Q. This leads to a novel method-ofmoments approach for consistent estimation of the shifted label distribution q(y) and the weights w(y).

Without loss of generality, we assume Y = {1, 2, ..., k}. Denote by νy, νyˆ, νˆyˆ, µy, µyˆ, µˆ yˆ, w ∈ Rk moments of p, q, and their plug-in estimates, deﬁned via

[νy]i = p(y = i)

[νyˆ]i = p(f (x) = i)

[νˆyˆ]i =

j 1{f (xj) = i}
n

[µy]i = q(y = i)

[µyˆ]i = q(f (x) = i)

[µˆ yˆ]i =

j 1{f (xj) = i}
m

and [w]i = q(y = i)/p(y = i). Lastly deﬁne the covariance matrices Cyˆ,y, Cyˆ|y and Cˆ yˆ,y in Rk×k via

[Cyˆ,y]ij = p(f (x) = i, y = j) [Cyˆ|y]ij = p(f (x) = i|y = j)
[Cˆ yˆ,y]ij = 1 1{f (xl) = i and yl = j}
nl
We can now rewrite Equation (1) in matrix form:

µyˆ = Cyˆ|yµy = Cyˆ,yw

Using plug-in maximum likelihood estimates of the above quantities yields the estimators
wˆ = Cˆ −yˆ,1yµˆ yˆ and µˆ y = diag(νˆy)wˆ ,
where νˆy is the plug-in estimator of νy. Next, we establish that the estimators are consistent. Proposition 2 (Consistency). If Assumption A.1, A.2, A.3 are true, then as n, m → ∞, wˆ −a→.s. w and µˆ y −a→.s. µy.

The proof (see Appendix B) uses the First Borel-Cantelli Lemma to show that the probability that the entire sequence of empirical confusion matrices with data size n + 1, ..., ∞ are simultaneously invertible converges to 1, thereby enabling us to use the continuous mapping theorem after applying the strong law of large numbers to each component.

We now address our estimators’ convergence rates.

Theorem 3 (Error bounds). Assume that A.3 holds robustly.

Let σmin be the smallest eigenvalue of Cyˆ,y. There exists a

constant

C

>

0

such

that

for

all

n

>

80

log

(n

)σ

−2 min

,

with

probability at least 1 − 3kn−10 − 2km−10 we have

wˆ − w 2 ≤ C

w 2 log n k log m +

(2)

2 σm2 in

n

m

µˆ y − µy 2 ≤ C w n2 log n + νy 2∞ wˆ − w 22 (3)

The bounds give practical insights (explored more in Sec-
tion 7). In (2), the square error depends on the sample size and is proportional to 1/n (or 1/m). There is also a w 2

Detecting and Correcting for Label Shift with Black Box Predictors

term that reﬂects how different the source and target distributions are. In addition, σmin reﬂects the quality of the given classiﬁer f . For example, if f is a perfect classiﬁer, then σmin = miny∈Y p(y). If f cannot distinguish between certain classes at all, then Cyˆ,y will be low-rank, σmin = 0, and the technique is invalid, as expected.

We now parse the error bound of µˆ y in (3). The ﬁrst term w 2/n is required even if we observe the importance

weight w exactly. The second term captures the additional

error due to the fact that we estimate w with predictor f .

Note that

νy

2 ∞

≤ 1 and can be as small as 1/k2 when

p(y) is uniform. Note that when f correctly classiﬁes each

class with the same probability, e.g. 0.5, then νy 2/σm2 in

is a constant and the bound cannot be improved.

Proof of Theorem 3. Assumption A.2 ensures that w < ∞.

wˆ = Cˆ −yˆ,1yµˆ yˆ = (Cyˆ,y + E1)−1(µyˆ + E2) = w + [(Cyˆ,y + E1)−1 − C−yˆ,1y]µyˆ + (Cyˆ,y + E1)−1E2

By completing the square and Cauchy-Schwartz inequality,

wˆ − w

2 ≤ 2µTyˆ [(Cyˆ,y + E1)−1 − C−yˆ,1y]T [(Cyˆ,y + E1)−1 − C−yˆ,1y]µyˆ + 2E2[(Cyˆ,y + E1)−1]T (Cyˆ,y + E1)−1E2.

By Woodbury matrix identity, we get that

Cˆ −yˆ,1y = C−yˆ,1y + C−yˆ,1y[E1−1 + C−yˆ,1y]−1C−yˆ,1y.

Substitute into the above inequality and use (1) we get

T

wˆ − w 2 ≤2w [E1−1 + C−yˆ,1y]−1 [C−yˆ,1y]T ×

(4)

C−yˆ,1y[E1−1 + C−yˆ,1y]−1w + 2E2T [(Cyˆ,y + E1)−1]T (Cyˆ,y + E1)−1E2

We now provide a high probability bound on the Euclidean
norm of E2, the operator norm of E1, which will give us an operator norm bound of [E1−1 +C−yˆ,1y]−1 and (Cyˆ,y +E1)−1 under our assumption on n, and these will yield a high
probability bound on the square estimation error.

Operator norm of E1.

Note that Cˆ yˆ,y =

n1 ni=1 ef(xi)eTyi , where ey is the standard basis with 1 at

the index of y ∈ Y and 0 elsewhere. Clearly, Eef(xi)eTyi =

Cyˆ,y. Denote Zi := ef(xi)eTyi − Cyˆ,y. Check that Zi 2 ≤

Zi F ≤

Zi

1,1 ≤ 2, max{

E[ZiZTi ]

,

E[Z

T i

Z

i

]

}≤

1, by matrix Bernstein inequality (Lemma 7) we have for all

t ≥ 0:

P( E1

≥

t/n)

≤

2k

e−

t2 n+2t/

3

.

√

Take t = 20n log n and use the assumption that n ≥

4 log n/9 (which holds under our assumption on n since

σmin < 1). Then with probability at least 1 − 2kn−10

E1 ≤

20 log n .
n

Using the assumption on n, we have E1 ≤ σmin/2

[E1−1 + C−yˆ,1y]−1

≤ 2 E1

√ 2 20 log n ≤√ .
n

Also, we have (Cyˆ,y + E1)−1 ≤ σm2in .

Euclidean norm of E2.

Note that [E2]l =

1 m

m i=1

1(f

(xi

)

=

l) − q(f (xi)

=

l).

By the standard

Hoeffding’s inequality and union bound argument, we have

that with probability larger than 1 − 2km−10

√

10k log m

E2 = µyˆ − µˆ yˆ 2 ≤

√ m

Substitute into Equation 4, we get

wˆ − w

2 ≤ 80 log n

w

2

+

80k

log

m ,

(5)

σm2 inn

σm2 inm

which holds with probability 1 − 2kn−10 − 2km−10. We
now turn to µˆ y. Recall that µˆ y = diag(νˆy)wˆ . Let the estimation error of νˆy be E0.

µˆ y =µy + diag(E0)w + diag(νy)(wˆ − w) + diag(E0)(wˆ − w).

By Hoeffding’s inequality E0 ∞ ≤ 20 lnog n with probability larger than 1 − kn−10. Combining with (5) yields
µˆ y −µy 2 ≤ 20 w n2 log n + νy 2∞ wˆ −w 2 +O( n12 )
which holds with probability 1 − 3kn−10 − 2km−10.
5. Application of the results
5.1. Black Box Shift Detection (BBSD)
Formally, detection can be cast as a hypothesis testing problem where the null hypothesis is H0 : q(y) = p(y) and the alternative hypothesis is that H1 : q(y) = p(y). Recall that we observe neither q(y) nor any samples from it. However, we do observe unlabeled data from the target distribution and our predictor f . Proposition 4 (Detecting label-shift). Under Assumption A.1, A.2 and for each classiﬁer f satisfying A.3 we have that q(y) = p(y) if and only if p(yˆ) = q(yˆ).

Proof. Plug P and Q into (1) and apply Lemma 1 with assumption A.1. The result follows directly from our analysis in the proof of Proposition 2 that shows p(yˆ, y) is invertible under the assumptions A.2 and A.3.

Thus, under weak assumptions, we can test H0 by running two-sample tests on readily available samples from p(yˆ)

1.0

Uniform

KS-test with MLP

oracle KS-test

0.8

MMD B-test on p(x)

0.6

Detecting and Correcting for Label Shift with Black Box Predictors

1.0

1.0

KS-test with MLP (1 epoch)

KS-test with MLP (5 epoch)

MMD B-test on p(x)

0.8

0.8

Oracle KS-test

0.6

0.6

Power of the test (1- Type II error)

0.4

0.4

0.4

0.2

0.0

0.0

0.2

0.4

0.6

0.8

1.0

(a) CDF of p-value at δ = 0

0.2

Uniform

KS-test with MLP

oracle KS-test

0.0

MMD B-test on p(x)

0.0

0.2

0.4

0.6

0.8

1.0

(b) CDF of p-value at δ = 0.6

0.2

0.0

0.0

0.2

0.4

0.6

0.8

(c) Power at α = 0.05

Figure 1. Label-shift detection on MNIST. Pane 1a illustrates that Type I error is correctly controlled absent label shift. Pane 1b illustrates high power under mild label-shift. Pane 1c shows increased power for better classiﬁers. We compare to kernel two-sample tests (Zaremba et al., 2013) and an (infeasible) oracle two sample test that directly tests p(y) = q(y) with samples from each. The proposed test beats directly testing in high-dimensions and nearly matches the oracle.

and q(yˆ). Examples include the Kolmogorov-Smirnoff test, Anderson-Darling or the Maximum Mean Discrepancy. In all tests, asymptotic distributions are known and we can almost perfectly control the Type I error. The power of the test (1-Type II error) depends on the classiﬁer’s performance on distribution P , thereby allowing us to leverage recent progress in deep learning to attack the classic problem of detecting non-stationarity in the data distribution.
One could also test whether p(x) = q(x). Under the labelshift assumption this is implied by q(y) = p(y). The advantage of testing the distribution of f (x) instead of x is that we only need to deal with a one-dimensional distribution. Per theory and experiments in (Ramdas et al., 2015) two-sample tests in high dimensions are exponentially harder.
One surprising byproduct is that we can sometimes use this approach to detect covariate-shift, concept-shift, and more general forms of nonstationarity. Proposition 5 (Detecting general nonstationarity). For any ﬁxed measurable f : X → Y
P = Q =⇒ p(x) = q(x) =⇒ p(yˆ) = q(yˆ).
This follows directly from the measurability of f .
While the converse is not true in general, p(yˆ) = q(yˆ) does imply that for every measurable S ⊂ Y,
q(x ∈ f −1(S)) = p(x ∈ f −1(S)).
This suggests that testing Hˆ 0 : p(yˆ) = q(yˆ) may help us to determine if there’s sufﬁcient statistical evidence that domain adaptation techniques are required.
5.2. Black Box Shift Correction (BBSC)
Our estimator also points to a systematic method of correcting for label-shift via importance-weighted ERM. Speciﬁ-

cally, we propose the following algorithm:
Algorithm 1 Domain adaptation via Label Shift
input Samples from source distribution X, y. Unlabeled data from target distribution X . A class of classiﬁers F. Hyperparameter 0 < δ < 1/k. 1. Randomly split the training data into two X1, X2 ∈ Rn/2×d and y1, y2Rn/2. 2. Use X1, y1 to train the classiﬁer and obtain f ∈ F . 3. On the hold-out data set X2, y2, calculate the confusion matrix Cˆ yˆ,y. If , if σmin Cˆ yˆ,y ≤ δ then Set wˆ = 1. else Estimate wˆ = Cˆ −yˆ,1yµˆ yˆ . end if 4. Solve the importance weighted ERM on the X1, y1 with max(wˆ , 0) and obtain f˜.
output f˜
Note that for classes that occur rarely in the test set, BBSE may produce negative importance weights. During ERM, a ﬂipped sign would cause us to maximize loss, which is unbounded above. Thus, we clip negative weights to 0.
Owing to its efﬁcacy and generality, our approach can serve as a default tool to deal with domain adaptation. It is one of the ﬁrst things to try even when the label-shift assumption doesn’t hold. By contrast, the heuristic method of using logistic-regression to construct importance weights (Bickel et al., 2009) lacks theoretical justiﬁcation that the estimated weights are correct.
Even in the simpler problem of average treatment effect (ATE) estimation, it’s known that using estimated propen-

MSE

Detecting and Correcting for Label Shift with Black Box Predictors

Estimating error of w at = 0.1
101

Estimating error of w at = 1.0

Estimating error of w at = 10.0
100

100

100

10 1
500
0.98

BBSE-hard BBSE-soft KMM KMM timeout
1000 2000 40n00 8000 16000 32000 Target accuracy at = 0.1

0.96

0.94

0.92

0.90

Unweighted

BBSE-hard

0.88

BBSE-soft

0.86

KMM KMM timeout

500 1000 2000 40n00 8000 16000 32000

Target accuracy

MSE

10 1
10 2 500
0.98

BBSE-hard BBSE-soft KMM KMM timeout
1000 2000 40n00 8000 16000 32000 Target accuracy at = 1.0

0.96

0.94

0.92

0.90

Unweighted

BBSE-hard

0.88

BBSE-soft

KMM

0.86

KMM timeout

500 1000 2000 40n00 8000 16000 32000

Target accuracy

MSE

10 1

10 2
500
0.98

BBSE-hard BBSE-soft KMM KMM timeout
1000 2000 40n00 8000 16000 32000 Target accuracy at = 10.0

0.96

0.94

0.92

0.90

0.88

Unweighted

BBSE-hard

0.86

BBSE-soft

KMM

0.84

KMM timeout

500 1000 2000 40n00 8000 16000 32000

Figure 2. Estimation error (top row) and correction accuracy (bottom row) vs dataset size on MNIST data compared to KMM (Zhang et al., 2013) under Dirichlet shift (left to right) with α = {.1, 1.0, 10.0} (smaller α means larger shift). BBSE conﬁdence interval on 20 runs, KMM on 5 runs due to computation; n = 8000 is largest feasible KMM experiment.

Target accuracy

sity can lead to estimators with large variance (Kang & Schafer, 2007). The same issue applies in supervised learning. We may prefer to live with the biased solution from the unweighted ERM rather than suffer high variance from an unbiased weighted ERM. Our proposed approach offers a consistent low-variance estimator under label shift.
6. Experiments
We experimentally demonstrate the power of BBSE with real data and simulated label shift. We organize results into three categories — shift detection with BBSD, weight estimation with BBSE, and classiﬁer correction with BBSC. BBSE-hard denotes our method where f yields classiﬁcations. In BBSE-soft, f outputs probabilities.
Label Shift Simulation To simulate distribution shift in our experiments, we adopt the following protocols: First, we split the original data into train, validation, and test sets. Then, given distributions p(y) and q(y), we generate each set by sampling with replacement from the appropriate split. In knock-out shift, we knock out a fraction δ of data points from a given class from training and validation sets.

In tweak-one shift, we assign a probability ρ to one of the classes, the rest of the mass is spread evenly among the other classes. In Dirichlet shift, we draw p(y) from a Dirichlet distribution with concentration parameter α. With uniform p(y), Dirichlet shift is bigger for smaller α.
Label-shift detection We conduct nonparametric twosample tests as described in Section 5.1 using the MNIST handwritten digits data set. To simulate the label-shift, we randomly split the training data into a training set, a validating set and a test set, each with 20,000 data points, and apply knock-out shift on class y = 5 3. Note that p(y) and q(y) differ increasingly as δ grows large, making shift detection easier. We obtain f by training a two-layer ReLUactivated Multilayer Perceptron (MLP) with 256 neurons on the training set for ﬁve epochs. We conduct a two-sample test of whether the distribution of f (Validation Set) and f (Test Set) are the same using the Kolmogorov-Smirnov test. The results, summarized in Figure 1, demonstrate that BBSD (1) produces a p-value that distributes uniformly when δ = 0 4 (2) provides more power (less Type II error)
3Random choice for illustration, method works on all classes. 4Thus we can control Type I error at any signiﬁcance level.

MSE

Detecting and Correcting for Label Shift with Black Box Predictors

Estimating error of w at = 0.7

101 Estimating error of w at = 0.5

Estimating error of w at = 0.3

101 100

100

10 1
500
0.975

BBSE-hard BBSE-soft KMM KMM timeout 1000 2000 40n00 8000 16000 32000 Target accuracy at = 0.7

0.950

0.925

0.900

0.875

Unweighted

0.850

BBSE-hard

0.825

BBSE-soft KMM

0.800

KMM timeout

500 1000 2000 40n00 8000 16000 32000

Target accuracy

MSE

100

10 1
500
0.98

BBSE-hard BBSE-soft KMM KMM timeout
1000 2000 40n00 8000 16000 32000 Target accuracy at = 0.5

0.96

0.94

0.92

0.90

0.88

Unweighted

0.86

BBSE-hard

0.84

BBSE-soft

KMM

0.82

KMM timeout

500 1000 2000 40n00 8000 16000 32000

Target accuracy

MSE

10 1
10 2 500
0.98

BBSE-hard BBSE-soft KMM KMM timeout
1000 2000 40n00 8000 16000 32000 Target accuracy at = 0.3

0.96

0.94

0.92

0.90

0.88

Unweighted BBSE-hard

0.86

BBSE-soft

KMM

0.84

KMM timeout

500 1000 2000 40n00 8000 16000 32000

Figure 3. Label-shift estimation and correction on MNIST data with simulated tweak-one shift with parameter ρ.

Target accuracy

than the state-of-the-art kernel two-sample test that discriminates p(x) and q(x) at δ = 0.5, and (3) gets better as we train the black-box predictor even more.
Weight estimation and label-shift correction We evaluate BBSE on MNIST by simulating label shift and datasets of various sizes. Speciﬁcally, we split the training data set randomly in two, using ﬁrst half to train f and the second half to estimate w. We use then use the full training set for weighted ERM. As before, f is a two-layer MLP. For fair comparisons with baselines, the full training data set is used throughout (since they do not need f without data splitting). We evaluate our estimator wˆ against the ground truth w and by the prediction accuracy of BBSC on the test set. To cover a variety of different types of label-shift, we take p(y) as a uniform distribution and generate q(y) with Dirichlet shift for α = 0.1, 1.0, 10.0 (Figure 2).
Label-shift correction for CIFAR10 Next, we extend our experiments to the CIFAR dataset, using the same MLP and this time allowing it to train for 10 epochs. We consider both tweak-one and Dirichlet shift, and compare BBSE to the unweighted classiﬁer under varying degrees of shift (Figure 4). For the tweak-one experiment, we try ρ ∈ {0.0, 0.1, ..., 1.0}, averaging results over all 10 choices of the tweaked label, and plotting the variance. For the

Dirichlet experiments, we sample 20 q(y) for every choice of α in the range {1000, 100, ..., .001}. Because kernelbased baselines cannot handle datasets this large or highdimensional, we compare only to unweighted ERM.
Kernel mean matching (KMM) baselines We compare BBSE to the state-of-the-art kernel mean matching (KMM) methods. For the detection experiments (Figure 1), our baseline is the kernel B-test (Zaremba et al., 2013), an extension of the kernel max mean discrepancy (MMD) test due to Gretton et al. (2012) that boasts nearly linear-time computation and little loss in power. We compare BBSE to a KMM approach Zhang et al. (2013), that solves
min Cx|y(νy ◦ w) − µx 2H,
w
where we use operator Cx|y := E[φ(x)|ψ(y)] and function µx := EQ[φ(x)] to denote the kernel embedding of p(x|y) and pQ(x) respectively. Note that under the label-shift assumption, Cx|y is the same for P and Q. Also note that since Y is discrete, ψ(y) is simply the one-hot representation of y, so νy is the same as our deﬁnition before and Cx|y, νy and µx must be estimated from ﬁnite data. The proposal involves a constrained optimization by solving a Gaussian process regression with automatic hyperparameter choices through marginal likelihood.

Detecting and Correcting for Label Shift with Black Box Predictors

Accuracy

Black Box Shift Correction on CIFAR10

1.0

BBSE-hard

0.9

Unweighted

0.8

0.7

0.6

0.5

0.4

0.3

0.0

0.2

0.4

0.6

0.8

1.0

Black Box Shift Correction on CIFAR10

1.0

BBSE-hard

0.9

Unweighted

0.8

0.7

0.6

0.5

0.4

0.3

1000 100 10

1

0.1 0.01 0.001

Accuracy

Figure 4. Accuracy of BBSC on CIFAR 10 with (top) tweak-one shift and (bottom) Dirichlet shift.

For fair comparison, we used the original authors’ implementations as baselines 5 and also used the median trick to adaptively tune the RBF kernel’s hyperparameter. A key difference is that BBSE matches the distribution of yˆ rather than distribution of x like (Zhang et al., 2013) and we learn f through supervised learning rather than by specifying a feature map φ by choosing a kernel up front.
Note that KMM, like many kernel methods, requires the construction and inversion of an n × n Gram matrix, which has complexity of O(n3). This hinders its application to real-life machine learning problems where n will often be 100s of thousands. In our experiments, we ﬁnd that the largest n for which we can feasibly run the KMM code is roughly 8, 000 and that is where we unfortunately have to stop for the MNIST experiment. For the same reason, we cannot run KMM for the CIFAR10 experiments. The MSE curves in Figure 2 for estimating w suggest that the convergence rate of KMM is slower than BBSE by a polynomial factor and that BBSE better handles large datasets.
5https://github.com/wojzaremba/btest, http://people.tuebingen.mpg.de/kzhang/ Code-TarS.zip

7. Discussion
Constructing the training Set The error bounds on our estimates depend on the norm of the true vector w(y) := q(y)/p(y). This conﬁrms the common sense that absent any assumption on q(y), and given the ability to select classconditioned examples for annotations one should build a dataset with uniform p(y). Then it’s always possible to apply BBSE successfully at test time to correct f .
Sporadic Shift In some settings, p(y) might change only sporadically. In these cases, when no label shift occurs, applying BBSC might damage the classiﬁer. For these cases, we prose to combine detection and estimation, correcting the classiﬁer only when a shift has likely occurred.
Using known predictor In our experiments, f has been trained using a random split of the data set, which makes BBSEto perform worse than baseline when the data set is extremely small. In practice, especially in the context of web services, there could be a natural predictor f that is currently being deployed whose training data were legacy and have little to do with the two distributions that we are trying to distinguish. In this case, we do not lose that factor of 2 and we do not suffer from the variance in training f with a small amount of data. This could allow us to detect mild shift in distributions in very short period of time. Making it suitable for applications such as ﬁnancial market prediction.
BBSE with degenerate confusion matrices In practice, sometime confusion matrices will be degenerate. For instance, when a class i is rare under P , and the features are only partially predictive, we might ﬁnd that p(f (x) = i) = 0. In these cases, two straightforward variations on the black box method may still work: First, while our analysis focuses on confusion matrices, it easily extends to any operator f , such as soft probabilities. If each class i, even if i is never the argmax for any example, so long as p(yˆ = i|y = i) > p(yˆ = i|y = j) for any j = i, the soft confusion matrix will be invertible. Even when we produce and operator with an invertible confusion matrix, two options remain: We can merge c classes together, yielding a (k − c) × (k − c) invertible confusion matrix. While we might not be able to estimate the frequencies of those c classes, we can estimate the others accurately. Another possibility is to compute the pseudo-inverse.
Future Work As a next step, we plan to extend our methodology to the streaming setting. In practice, label distributions tend to shift progressively, presenting a new challenge: if we apply BBSE on trailing windows, then we face a tradeoff. Looking far back increases m, lowering estimation error, but the estimate will be less fresh. The use of propensity weights w on y makes BBSE amenable to doubly-robust estimates, the typical bias-variance tradeoff, and related techniques, common in covariate shift correction.

Detecting and Correcting for Label Shift with Black Box Predictors

Acknowledgments
We are grateful for extensive insightful discussions and feedback from Kamyar Azizzadenesheli, Kun Zhang, Arthur Gretton, Ashish Khetan Kumar, Anima Anandkumar, Julian McAuley, Dustin Tran, Charles Elkan, Max G’Sell, Alex Dimakis, Gary Marcus, and Todd Gureckis.
References
Bickel, S., Brückner, M., & Scheffer, T. (2009). Discriminative learning under covariate shift. Journal of Machine Learning Research, 10(Sep), 2137–2155.
Bishop, C. M. (1995). Neural networks for pattern recognition. Oxford university press.
Buck, A., Gart, J., et al. (1966). Comparison of a screening test and a reference test in epidemiologic studies. ii. a probabilistic model for the comparison of diagnostic tests. American Journal of Epidemiology.
Chan, Y. S., & Ng, H. T. (2005). Word sense disambiguation with distribution estimation. In Proceedings of the 19th international joint conference on Artiﬁcial intelligence, (pp. 1010–1015). Morgan Kaufmann Publishers Inc.
Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., & Fei-Fei, L. (2009). Imagenet: A large-scale hierarchical image database. In CVPR.
Elkan, C. (2001). The foundations of cost-sensitive learning. In IJCAI.
Forman, G. (2008). Quantifying counts and costs via classiﬁcation. Data Mining and Knowledge Discovery.
Gretton, A., Borgwardt, K. M., Rasch, M. J., Schölkopf, B., & Smola, A. (2012). A kernel two-sample test. Journal of Machine Learning Research, 13(Mar), 723–773.
Gretton, A., Smola, A. J., Huang, J., Schmittfull, M., Borgwardt, K. M., & Schölkopf, B. (2009). Covariate shift by kernel mean matching. Journal of Machine Learning Research.
Heckman, J. J. (1977). Sample selection bias as a speciﬁcation error (with an application to the estimation of labor supply functions).
Huang, J., Gretton, A., Borgwardt, K. M., Schölkopf, B., & Smola, A. J. (2007). Correcting sample selection bias by unlabeled data. In Advances in neural information processing systems.
Kang, J. D., & Schafer, J. L. (2007). Demystifying double robustness: A comparison of alternative strategies for estimating a population mean from incomplete data. Statistical science, 22(4), 523–539.

Manski, C. F., & Lerman, S. R. (1977). The estimation of choice probabilities from choice based samples. Econometrica: Journal of the Econometric Society.
Ramdas, A., Reddi, S. J., Póczos, B., Singh, A., & Wasserman, L. A. (2015). On the decreasing power of kernel and distance based nonparametric hypothesis tests in high dimensions. In AAAI, (pp. 3571–3577).
Rosenbaum, P. R., & Rubin, D. B. (1983). The central role of the propensity score in observational studies for causal effects. Biometrika, 70(1), 41–55.
Saerens, M., Latinne, P., & Decaestecker, C. (2002). Adjusting the outputs of a classiﬁer to new a priori probabilities: a simple procedure. Neural computation, 14(1), 21–41.
Schölkopf, B., Janzing, D., Peters, J., Sgouritsa, E., Zhang, K., & Mooij, J. (2012). On causal and anticausal learning. In International Coference on International Conference on Machine Learning (ICML-12), (pp. 459–466). Omnipress.
Shimodaira, H. (2000). Improving predictive inference under covariate shift by weighting the log-likelihood function. Journal of statistical planning and inference.
Storkey, A. (2009). When training and test sets are different: characterizing learning transfer. Dataset shift in machine learning.
Sugiyama, M., Nakajima, S., Kashima, H., Buenau, P. V., & Kawanabe, M. (2008). Direct importance estimation with model selection and its application to covariate shift adaptation. In Advances in neural information processing systems.
Zadrozny, B. (2004). Learning and evaluating classiﬁers under sample selection bias. In Proceedings of the twentyﬁrst international conference on Machine learning, (p. 114). ACM.
Zaremba, W., Gretton, A., & Blaschko, M. (2013). B-test: A non-parametric, low variance kernel two-sample test. In Advances in neural information processing systems, (pp. 755–763).
Zhang, K., Schölkopf, B., Muandet, K., & Wang, Z. (2013). Domain adaptation under target and conditional shift. In International Conference on Machine Learning, (pp. 819– 827).
Zhu, X., Gibson, B. R., Jun, K.-S., Rogers, T. T., Harrison, J., & Kalish, C. (2010). Cognitive models of test-item effects in human category learning. In ICML.

Detecting and Correcting for Label Shift with Black Box Predictors

A. Additional discussion
In this section we provide a few answers to some questions people may have when using our proposed techniques.
What if the label-shift assumption does not hold? In many applications, we do not know whether label-shift is a reasonable assumption or not. In particular, whenever there are unobserved variables that affects both x and y, then neither label-shift nor covariate-shift is true. However, label shift could still be a good approximation in the in the ﬁnite sample environment. Luckily, we can test whether the label-shift assumption is a good approximation in a datadriven fashion via the kernel two-sample tests. In particular, let φ : X → F be an arbitrary feature map that (possibly reduces the dimension of x) and k : F × F → R be the kernel function that induces a RKHS H. Let w = [q(y)/p(y)]y=1,...,k, then
Ep [w(y)k(φ(x), ·)] = Eq [k(φ(x), ·)] .

trained for ﬁve epochs, and the gap in the smallest singular values is predicative of the fact at least qualitatively.

0.450

min(CP(fhard))

0.425

min(CP(fsoft))

0.400

0.375

0.350

0.325

0.300

0.275

0.250
1 2 3 4 Epo5ch 6 7 8 9

The LHS can be estimated by plugging in wˆ and a stochastic approximation of the expectation using labeled data from the source domain and the RHS can be estimated by the sample mean using unlabeled data from the target domain. In particular, if label-shift assumption is true or a good approximation, then

1n

1m

[wˆ (yi)k(φ(xi), ·)] −

k(φ(xj), ·)

2 H

n

m

i=1

j=1

should be on the same order as the statistical error that we can calculate by m, n and the error of wˆ in estimating w.

Figure 5. The smallest singular value of the estimated confusion matrix: Cˆf under distribution p as a function of the number of epochs we train the classiﬁers on.
Is data splitting needed? Recall that we train the model f and estimate w using two independent splits of the labeled data set drawn from the same distribution. In practice, especially when n is large, using the same data to train f and to estimate w will be more data efﬁcient. This comes at a price of a small bias. It is unclear how to quantify that bias but the data-reuse version could be useful in practice as a heuristic.

Model selection criterion and the choice of f . Our analysis assumes that f is ﬁxed and given, but in practice, often we need to train f from the same data set. Given a number of choices, one may wonder which blackbox predictor f should we prefer out of a collection of F? Our theoretical results suggest a natural quantity: the smallest singular value of the confusion matrix, for choosing the blackbox predictors. Note that the smallest singular value is a quantity that can be estimated using only labeled data from the source domain. Therefore a practical heuristic to use is to the f that maximizes the smallest singular value of the corresponding Cˆf . Figure 5 plots the smallest singular value of the confusion matrices as the number of epochs of training f gets larger. The model we use is the same multi-layer perceptron that we used for our experiments and the source distribution is one that we knocks off 80% of the ﬁfth class. This is the same model and data set we used in Figure 1c. Referring to δ = 0.8 in Figure 1c, we see that the test power of f that is trained for only one epoch is much lower than the f that is

B. Proofs
We present the proofs of Lemma 1 and Proposition 2 in this Appendix.

Proof of Lemma 1. By the law of total probability

q(yˆ|y) = q(yˆ|x, y)q(x|y) = q(yˆ|x, y)p(x|y)

y∈Y

y∈Y

= pf (yˆ|x)p(x|y) = p(yˆ|x, y)p(x|y) = p(yˆ|y).

y∈Y

y∈Y

We applied A.1 to the second equality, and used the conditional independence yˆ ⊥⊥ y|x under P and Q together with p(yˆ|x) being determined by f , which is ﬁxed.

Proof of Proposition 2. A.2 ensures that w < ∞. By Assumption A.3, Cyˆ,y is invertible. Let δ > 0 be its smallest

Detecting and Correcting for Label Shift with Black Box Predictors
singular value. We bound the probability that Cˆ yˆ,y is not invertible:

P(Cˆ yˆ,y is not invertible) ≤ P(σmin(Cˆ yˆ,y) < δ/2)

≤ P( Cˆ yˆ,y − Cyˆ,y 2 ≥ δ/2) ≤ P( Cˆ yˆ,y − Cyˆ,y F ≥

δ √)

↑

2k

pigeon hole

≤ P(∃(i, j) ∈ [k]2, s.t.|[Cˆ yˆ,y]i,j − [Cyˆ,y]i,j | ≥

δ

)

≤

2

e

−

nδ2 4k3

.

↑

2k1.5 ↑

pigeon hole

Hoeffding

By the convergence of geometric series n P(Cˆ yˆ,y is not invertible) < +∞. This allows us
to invoke the First Borel-Cantelli Lemma, which shows

P(Cˆ yˆ,y is not invertible i.o.) = 0.

(6)

This ensures that as n → ∞, Cˆ yˆ,y is invertible almost surely.
By the strong law of large numbers (SLLN), as n → ∞ Cˆ yˆ,y −a→.s. Cyˆ,y and νˆy −a→.s. νy. Similarly, as m → ∞, µˆ yˆ −a→.s. µyˆ. Combining these with (6) and applying the continuous mapping theorem with the fact that the inverse
of an invertible matrix is a continuous mapping we get that

wˆ = [Cˆ yˆ,y]−1µˆ yˆ −a→.s. w, and µˆ y = diag(νˆy)wˆ −a→.s. µy.

C. Concentration inequalities

Lemma 6 (Hoeffding’s inequality). Let x1, ..., xn be in-

dependent random variables bounded by [ai, bi]. Then

x¯ = n1

n i=1

xi

obeys

for

any

t

>

0

2n2t2 P(|x¯ − E[x¯]| ≥ t) ≤ 2 exp − ni=1(bi − ai)2 .

Lemma 7 (Matrix Bernstein Inequality (rectangular case)). Let Z1, ..., Zn be independent random matrices with dimension d1 × d2 and each satisfy

EZi = 0 and Zi ≤ R

almost surely. Deﬁne the variance parameter

σ2 − max{

E[

Zi

Z

T i

]

,

i

Then for all t ≥ 0,

E[Z

T i

Zi

]

}.

i

−t2

P

Zi ≥ t ≤ (d1 + d2) · e σ2+Rt/3 .

i

