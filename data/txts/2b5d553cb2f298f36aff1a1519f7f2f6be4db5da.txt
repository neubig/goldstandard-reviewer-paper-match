STEAM: Self-Supervised Taxonomy Expansion with Mini-Paths

Yue Yu
Georgia Institute of Technology Atlanta, GA, USA yueyu@gatech.edu

Yinghao Li
Georgia Institute of Technology Atlanta, GA, USA
yinghaoli@gatech.edu

Jiaming Shen
University of Illinois at Urbana-Champaign Urbana, IL, USA js2@illinois.edu

arXiv:2006.10217v1 [cs.CL] 18 Jun 2020

Hao Feng
University of Electronic Science and Technology of China
Chengdu, Sichuan, China is.fenghao@gmail.com

Jimeng Sun
University of Illinois at Urbana-Champaign Urbana, IL, USA jimeng@illinois.edu

Chao Zhang
Georgia Institute of Technology Atlanta, GA, USA
chaozhang@gatech.edu

ABSTRACT
Taxonomies are important knowledge ontologies that underpin numerous applications on a daily basis, but many taxonomies used in practice suffer from the low coverage issue. We study the taxonomy expansion problem, which aims to expand existing taxonomies with new concept terms. We propose a self-supervised taxonomy expansion model named STEAM, which leverages natural supervision in the existing taxonomy for expansion. To generate natural self-supervision signals, STEAM samples mini-paths from the existing taxonomy, and formulates a node attachment prediction task between anchor mini-paths and query terms. To solve the node attachment task, it learns feature representations for query-anchor pairs from multiple views and performs multi-view co-training for prediction. Extensive experiments show that STEAM outperforms state-of-the-art methods for taxonomy expansion by 11.6% in accuracy and 7.0% in mean reciprocal rank on three public benchmarks. The implementation of STEAM can be found at https://github.com/yueyu1030/STEAM.
CCS CONCEPTS
• Computing methodologies → Information extraction.
KEYWORDS
Taxonomy Expansion, Mini-Paths, Self-supervised Learning
ACM Reference Format: Yue Yu, Yinghao Li, Jiaming Shen, Hao Feng, Jimeng Sun, and Chao Zhang. 2020. STEAM: Self-Supervised Taxonomy Expansion with Mini-Paths. In Proceedings of the 26th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD ’20), August 23–27, 2020, Virtual Event, CA, USA. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/3394486.3403145
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. KDD ’20, August 23–27, 2020, Virtual Event, CA, USA © 2020 Association for Computing Machinery. ACM ISBN 978-1-4503-7998-4/20/08. . . $15.00 https://doi.org/10.1145/3394486.3403145

1 INTRODUCTION
Concept taxonomies play a central role in a wide spectrum of applications. On a daily basis, e-commerce websites like Amazon heavily rely on their product taxonomies to support billions of product navigations, searches, and recommendations [46]; scientific taxonomies (e.g., MeSH1) make it much faster to identify relevant information from massive scientific papers, and concept taxonomies in knowledge bases (e.g., Freebase [5]) underpin many question answering systems [14]. Due to such importance, many taxonomies have been curated in general and specific domains, e.g., WordNet [28], Wikidata [41], MeSH [23], Amazon Product Taxonomy [17].
One bottleneck of many existing taxonomies is the low coverage problem. This problem arises mainly due to two reasons. First, many existing taxonomies are curated by domain experts. As the curation process is expensive and time-consuming, the result taxonomies often include only frequent and coarse-grained terms. Consequently, the curated taxonomies have high precision, but limited coverage. Second, domain-specific knowledge is constantly growing in most applications. New concepts arise continuously, but it is too tedious to rely on human curation to maintain and update the existing taxonomies. The low coverage issue can largely hurt the performance of downstream tasks, and automated taxonomy expansion methods are in urgent need.
Existing taxonomy construction methods follow two lines. One line is to construct taxonomies in an unsupervised way [24, 30, 42, 44]. This is achieved by hierarchical clustering [44], hierarchical topic modeling [24, 42], or syntactic patterns (e.g., the Hearst pattern [15]). The other line adopts supervised approaches [13, 19, 26], which first detect hypernymy pairs (i.e., term pairs with the “is-a” relation) and then organize these pairs into a tree structure. However, applying these methods for taxonomy expansion suffers from two limitations. First, most of them attempt to construct taxonomies from scratch. Their output taxonomies can rarely preserve the initial taxonomy structures curated by domain experts. Second, the performance of many methods relies on large amounts of annotated hypernymy pairs, which can be expensive to obtain in practice.
We propose a self-supervised taxonomy expansion model named STEAM2, which leverages natural supervision in the existing taxonomy for expansion. To generate natural self-supervision signals,
1
https://www.nlm.nih.gov/mesh/meshhome.html 2Short for Self-supervised Taxonomy ExpAnsion with Mini-Paths.

KDD ’20, August 23–27, 2020, Virtual Event, CA, USA

Yue Yu, Yinghao Li, Jiaming Shen, Hao Feng, Jimeng Sun, and Chao Zhang

Seed Taxonomy
Nuisance

dangerous substance

Pollutant

inﬂammable product

toxic substance

Atmospheric Pollutant

combustion gases

Greenhouse gas

Dust

New Concept

EMI

stratospheric

pollutant

economic noise

carcinogenic substance

Example of a situation where ExEaxEmaMmpIlpeisleoafoasf iasmistpuiltaeut,aiobtinuotnwfrwheehqreuereent ExEaMEmMIpiIsleisaoasf iasmismpniltpueuli,aesbta,iuonbtncufetrwe.frhqeeuqreuenetnt EMI is a simnpunliuesia,snbacunetc.fere. quent
nuisance. Corpus

Expanded Taxonomy
Nuisance

economic noise
carcinogenic substance

EMI

dangerous

Pollutant

substance

inﬂammable product

toxic substance

Atmospheric Pollutant

stratospheric pollutant

combustion gases

Greenhouse gas

Dust

Figure 1: Illustration of the taxonomy expansion problem. Given an existing taxonomy, the task is to insert new concept terms (e.g., EMI, stratospheric pollutant, economic noise, carcinogenic substance) into the correct positions in the existing taxonomy.

STEAM samples mini-paths from the existing taxonomy, and formulates a node attachment prediction task between mini-paths and query terms. The mini-paths, which contain terms in different layers (e.g. “Pollutant”–“Atmospheric Pollutant”–“Dust” in Figure 1), serve as candidate anchors for query terms and yield many training query-anchor pairs from the existing taxonomy. With these query-anchor pairs, we learn a model (Section 3.1) to pinpoint the correct position for a query term in the mini-path. Compared with previous methods [35, 38, 40] using single anchor terms, STEAM better leverages the existing taxonomy since the mini-paths contain richer structural information from different levels.
In cooperation with mini-path-based node attachment, STEAM extracts features for query-anchor pairs from multiple views, including: (1) distributed features that capture the similarity between terms’ distributed representations; (2) contextual features, i.e. information from two terms’ co-occurring sentences; (3) lexico-syntactic features extracted from the similarity of surface string names between terms. We find that different views can provide complementary information that is vital to taxonomy expansion. To fuse the three views more effectively, we propose a multi-view co-training procedure (Section 3.2). In this procedure, the three views lead to different branches for predicting the positions of the query term, and the predictions from these three views are encouraged to agree with each other.
We have conducted extensive experiments on three taxonomy construction benchmarks in different domains. The results show that STEAM outperforms state-of-the-art methods for taxonomy expansion by 11.6% in accuracy and 7.0% in mean reciprocal rank. Moreover, ablation studies demonstrate the effect of mini-path for capturing structural information from the taxonomy, as well as the multi-view co-training for harnessing the complementary signals from all views.
Our main contributions are: 1) a self-supervised framework that performs taxonomy expansion with natural supervision signals from existing taxonomies and text corpora; 2) a mini-path-based anchor format that better captures structural information in taxonomies for expansion; 3) a multi-view co-training procedure that integrates multiple sources of information in an end-to-end model; and 4) extensive experiments on several benchmarks verifying the efficacy of our method.

2 PROBLEM DESCRIPTION
We focus on the taxonomy expansion task for term-level taxonomies, which is formally defined as follows.
Definition 2.1 (Taxonomy). A taxonomy T = (V, E) is a tree structure where 1) V is a set of terms (words or phrases); and 2) E is a set of edges representing is-a relations between terms. Each directed edge ⟨vi , vj ⟩ ∈ E represents a hypernymy relation between term vi and term vj , where vi is the hyponym (child) and vj is the hypernym (parent).
The problem of taxonomy expansion (Figure 1) is to enrich an initial taxonomy by inserting new terms into it. These new terms are often automatically extracted and filtered from a text corpus. Formally, we define the problem as below:
Definition 2.2 (Taxonomy Expansion). Given 1) an existing taxonomy T0 = (V0, E0), 2) a text corpus D, and 3) a set of candidate terms C, the goal of taxonomy expansion is to insert the term q ∈ C into the existing taxonomy T0 and expand it into a more complete taxonomy T = (V, E) where V = V0 ∪ C, E = E0 ∪ R with R being the newly discovered relations between terms in C and V0.
3 THE STEAM METHOD
In this section, we describe our proposed method STEAM. We first give an overview of our method, and then detail the two key modules: mini-path-based prediction and multi-view co-training. Finally, we discuss the model learning and inference procedures.
3.1 Self-Supervised Learning by Mini-Path Attachment
The central task of taxonomy expansion is to attach a query term q ∈ C into the correct position in the existing taxonomy T0. STEAM learns to attach query terms using natural supervision signals from the seed taxonomy. Its self-supervised learning procedure aims to preserve the structure of the seed taxonomy by creating a learning task that pinpoints the anchor positions for the terms already seen in the seed taxonomy. The training data for this selfsupervised learning task can be easily obtained from the seed taxonomy, thereby facilitating learning a model that performs query attaching at inference time.

STEAM: Self-Supervised Taxonomy Expansion with Mini-Paths
3.1.1 Query-Anchor Matching with Mini-Paths. To instantiate the self-supervised learning paradigm [22, 35, 39], an intuitive idea is to find the best hypernym for the query term q. Most existing works [26, 35, 40] follow this idea and model the taxonomy expansion problem as finding the optimal hypernym pairs for test terms. They usually design a binary classifier trained by determining whether ⟨pi , pj ⟩ (pi , pj ∈ V0) is a hypernymy pair.
Unlike the binary classification formulation, STEAM learns to match query terms with anchors with richer structural information. The core of STEAM’s self-supervised learning procedure is mini-paths, which are snippets sampled from the seed taxonomy. These mini-paths, containing the term pairs from different layers of taxonomy, can preserve the hierarchical relations among different terms. Below, we introduce the notion of mini-path and formulate the self-supervised learning task based on mini-paths.
Definition 3.1 (Mini-path). A mini-path P = [p1, p2, . . . , pL] consists of several terms {p1, p2, . . . , pL } ⊂ V0, where L is the length of P. Each term pair ⟨pi , pi+1⟩ (1 ≤ i ≤ L − 1) corresponds to an edge in E0.

Nuisance A

A

A

A

dangerous B substance

Mini-paths

C Pollutant

B

B

C

inﬂammable D

toxic E

product

substance

F Atmospheric Pollutant

D

E

F

(a) An illustration of mini-paths.

Insertion
A

Situation:

carcinogenic substance

B

G

1? 2

A

G

A

?

B

G

B

3

4

A

A

？ None

B

G

B

?

D

D

D

D

G

D

(b) The classification target.

Figure 2: An illustration of the proposed self-supervised data structure, including the construction of mini-paths and the learning target during the term-insertion process.
The mini-paths are fixed-length branchless sub-graphs of the existing taxonomy T0, as shown in Figure 2(a), which maintain part of the parent-child relationships between terms. To keep the
hierarchical information in the taxonomy with the self-supervised
training set, we design the training task as a multi-class classifica-
tion problem. As shown in Figure 2(b), given a 3-terms mini-path as anchor and a new term as query, STEAM predicts the probabilities of the query being attached to the three terms or none of them.
Compared with the simple task of binary hypernymy classi-
fication, matching query terms with mini-paths has two major advantages: 1) When attaching a query term q into an anchor minipath P, we consider the collection of all terms pi ∈ P as a whole,

KDD ’20, August 23–27, 2020, Virtual Event, CA, USA

rather than attend to them separately. This not only provides richer information for query attachment but also results in larger training data for self-supervised learning. 2) Compared with the binary classification, this task is more challenging—the matching module needs to judge not only whether q should be matched to P but also which specific position to attach. Learning from this more challenging self-supervised task allows STEAM to better leverage the structural information of the existing taxonomy and perform better for anchor term deduction and taxonomy expansion.

3.1.2 Sampling Mini-Paths from the Seed Taxonomy. To facilitate the learning problem, we need to sample mini-paths from the exist-

ing taxonomies as anchors, as well as the query terms that should

be attached to different positions in anchor mini-paths. This can be

achieved by randomly sampling mini-paths in the seed taxonomy,

along with positive and negative query terms for each mini-path.

The detailed procedure for training data creation is described as

follows. Given one mini-path P ∈ P where P is the collection of

all mini-paths in the existing taxonomy, we first generate positive

training set Xpos by sampling all the child terms ai,l ∈ A of P ∈ P, where ai,l is the i-th child of the l-th anchor term pl ∈ P and A

contains all child terms attached to the mini-path, and a positive

pair

is

represented

as

pos
Xi,l

=

⟨ai,l , pj , l⟩.

Once

Xpos

is

obtained,

we

augment the training set by adopting the negative sampling strategy

to generate negative set Xneg by randomly selecting |Xneg| = r ×

|Xpos |

terms

{n

i

}i|

Xneg =1

|

with

sampling

ratio

r,

each

constituting

a negative pair with one term that is not its parent in a anchor.

Since these negative terms do not directly associate with the mini-
path P, we assign a relative position L + 1 for them to indicate no connection exists. Combining Xpos and Xneg together we obtain the final training set X.

After obtaining query-anchor pairs, we need to learn a model

using such data. Given the set of training pairs X, we denote each pair as X = ⟨q, P, l⟩ ∈ X where q is the query term, P is the minipath, and y is the relative position and aim to learn a model f (q, P |Θ)

to identify the correct position (represented by the true label y).

The training objective is to optimize the negative log likelihood

ℓ=−

X

L+1 i =1

y

i

log yˆi

where

yˆ

is

the

predicted

position.

3.2 Multi-View Co-Training with Mini-Paths
Now the question is how to obtain feature representations for each query-anchor pair (q, P). In STEAM, we learn feature representations for query-anchor pairs from three different views and integrate them with a multi-view co-training procedure.

3.2.1 Multi-View Feature Extraction. STEAM learns representations of query-anchor pairs from three views: (1) the distributed representation view, which captures their correlation from pre-trained word embeddings; (2) the contextual relation view, which captures their correlation from the sentences where the query term and anchor terms co-occur; and (3) the lexico-syntactic view, which captures their correlation from the linguistic similarities between the
query and the anchor.
Each of the three views has its own advantages and disadvantages: (1) Distributed features have a high coverage over the term vocabulary, but they do not explicitly model pair-wise relations between a query term and an anchor term; (2) Contextual features

KDD ’20, August 23–27, 2020, Virtual Event, CA, USA

Yue Yu, Yinghao Li, Jiaming Shen, Hao Feng, Jimeng Sun, and Chao Zhang

can capture the relation between two terms from their co-occurred
sentences, but have limited coverage over term pairs. For example,
only less than 15% of hypernym pairs have co-occurred in the scientific corpus of the SemEval dataset; (3) Lexico-Syntactic features encode linguistic information between terms and can work well
for matched term pairs, but these features are too rigid to cover all
the linguistic patterns, and may also have limited coverage. Given a query term q and an anchor mini-path P = [p1, p2, · · · , pL],
we describe the details of how we learn representations for the query-anchor pair (q, P) from the three different views. (1) Distributed Features. The first view extracts distributed features for both the query q and the anchor mini-path P. For the query term q and the anchor terms in the mini-path P, we use pre-trained BERT embeddings [9] to initialize their distributed representations
due to its superior expressive power [16, 21]. While it is feasible
to directly use such initial embeddings for similarity computation,
recent work [35] shows that the neighboring terms of an anchor
term are also useful for taxonomy expansion. We follow [35] to use
a position-enhanced graph attention network (PGAT) to propagate
the embeddings for the terms in the seed taxonomy by considering
the taxonomy as a directed graph—this will lead to updated embeddings for the anchor terms in the mini-path P. For each anchor term pl ∈ P, we use w(pl ) to denote its PGAT-propagated embedding and use w(q) to denote the embedding of the query term q, then we concatenate these embeddings and obtain the distributed representation for the query-anchor pair (q, P):

hd (q, P) = [w(q) ⊕ w(p1) ⊕ · · · ⊕ w(pL)].

(1)

(2) Contextual Features. When two terms co-occur in the same
sentence, the contexts of their co-occurrence can often indicate
the relation of the two terms. Our second view thus harvests the
sentences from the given corpus D to extract features for the query term q and the mini-path P. Given the query term q and any anchor term pl ∈ P, we fetch all the sentences where q and pl have cooccurred from corpus D. Similar to [38], we process these sentences to extract the dependency paths between q and pl in these sentences, denoted as Dq,pl . For each dependency path dq,pl ∈ Dq,pl , it is a sequence of context words that lead q to pl in the dependency tree:

dq,pl = {ve1 , ve2 , · · · , vek },

(2)

where k is the length of the dependency path. Each edge ve in the dependency path contains 1) the connecting term vl , 2) the partof-speech tag of the connecting term vpos , 3) the dependency label vdep , and 4) the edge direction between two subsequent terms vdir . Formally, each edge ve is represented as: ve = [vl , vpos , vdep , vdir ]. Now in order to encode each extracted dependency path dq,pl , we feed the multi-variate sequence dq,pl into an LSTM encoder.
The representation of the LSTM’s last hidden layer, denoted as
LSTM(dq,pl ), is then used as the representation the path dq,pl . As the set D(q, pl ) contains multiple dependency paths between q and pl , we aggregate them with the attention mechanism to compute

the weighted average of these path representations:

αˆd = uT tanh W · LSTM(dq,pl ) ,

αd =

exp (αˆd )

,

d′ ∈Dq,pl exp (αˆd′ ) (3)

d(q, pl ) =

αd · LSTM(dq,pl ),

d ∈ D(q,pl )

where αd denotes attention weight for the dependency path dq,pl ; W, u are trainable weights for the attention network.
The final contextual features between q and P is thus given by

hc (q, P) = [d(q, p1) ⊕ · · · ⊕ d(q, pL)].

(4)

(3) Lexical-Syntactic Features. Our third view extracts lexicalsyntactic features between terms. Such features encode the cor-

relations between terms based on their surface string names and syntactic information [26, 30, 45]. Given a term pair (x, y), we extract the following lexical-syntactic features between them:

• Ends with: Identifies whether y ends with x or not. • Contains: Identifies whether y contains x or not. • Suffix match: Identifies whether the k-length suffixes of x and

y match or not.
• LCS: The length of longest common substring of term x and y.
• Length Difference: The normalized length difference between
x and y. Let the length of term x and y be L(x) and L(y), then the
|L(x )−L(y)|
normalized length difference is calculated as max(L(x),L(y)) . • Normalized Frequency Difference: The normalized frequency
of (x, y) in corpus D with min-max normalization. Specifically, fol-

low [13], we consider two types of normalized frequency based

on the noisy hypernym pairs obtained in [30]: (1) the normal-
ized frequency difference. Given a term pair (x, y), their normalized frequency is defined as n f (x, y) = maxzf∈rVeqf(rxe,qy()x,z) where f req(x, y) defines the occurrence frequency between term (x, y)
in the hypernym pairs given by [30] and V = V0 ∪ C which is all

terms in the existing taxonomy and test set. Then the first normal-

ize frequence difference is defined as f (x, y) = n f (x, y)−n f (y, x). (2) the generality difference. For term x, the normalized generality score nд(x) = loд(1 + h), where h is defined as the logarithm of

the number of its distinct hyponyms. Then the generality differ-

ence of term pair д(x, y) is defined as the difference in generality between (x, y) as д(x, y) = nд(x) − nд(y).

Given the query term q and the mini-path P = [p1, p2, · · · , pL],
we compute the lexico-syntactic features for each pair (q, pl ) (1 ≤ l ≤ L), denoted as s(q, pl ) and concatenate the features derived from all the term pairs as the lexical-syntactic features for (q, P):

hs (q, P) = [s(q, p1) ⊕ · · · ⊕ s(q, pL)].

(5)

3.2.2 The Multi-View Co-Training Objective. As the three views provide complementary information to each other, it is important to aggregate the three views for the query-anchor matching. To this end, one may simply stack three different sets of features and train one unified classifier [26]. However, such feature-level integration can lead to suboptimal results due to two reasons: (1) one view can provide dominant signals over the other two, making it hard to fully unleash the discriminative power of each view; (2) the three views can have different dimensionality and distributions, making learning a unified classifier from concatenated features difficult.

STEAM: Self-Supervised Taxonomy Expansion with Mini-Paths

KDD ’20, August 23–27, 2020, Virtual Event, CA, USA

Nuisance

dangerous substance

Pollutant

toxic substance

Atmospheric Pollutant

Greenhouse gas

Dust

Corpus
surface nsanumarfwemaocered embedding

dependency path

query

lexicosyntactic features
EMI

GAT+ Linear

embedding emdbisetdridbiuntged view

LSTM+ attention

embedding emcboendtedxintugal view

emebmebdleeddxindicginogsyntactic view

MLP MLP MLP

prediction
L3 L3
prediction
L3
prediction

L2 ground
truth
L1
Integrate
prediction

Figure 3: Illustration of the proposed co-training model architecture. The grey terms in the existing taxonomy on the left is an anchor path to attach the new term to. L1, L2 and L3 corresponds to the log-likelihood loss and Euclidean loss calculated in Equation (8), (9) and (10) respectively.

To more effectively harvest the information from the three different views, we propose a multi-view co-training procedure. This co-training procedure (see Figure 3) uses the three views to learn three different classifiers and then derives an aggregated classifier from the three classifiers and also encourages their predictions to be consistent. The entire model can be trained in an end-to-end manner. Below, we first describe the base classifiers designed for the three different views, and then present the co-training objective.
Base Classifiers from Multiple Views. Based on three sets of feature hd , hc , hs derived from different views, we design three neural classifiers for the query-anchor matching task, i.e., the multiclass classification problem formulated in Section 3.1. For each of the three views, we use a multi-layer perceptron (MLP) with one hidden layer for this prediction task, denoted as fd , fs , and fr . Then the predictions from the three views are given by:
yd = fd (hd ) = Wd2 (σ (Wd1 hd + bd1 ) + bd2 ), yc = fc (hc ) = Wc2(σ (Wc1hc + bc1) + bc2), (6) ys = fc (hs ) = Ws2(σ (Ws1hs + bs1) + bs2), where {Wk , Wk , bk , bk } k ∈ {d, s, c} are trainable parameters for
1 212
the three MLP classifiers, and σ (·) is the activation function for which we use ReLU in our experiments.
The Co-Training Objective. Figure 3 shows the co-training model that integrates the three base classifiers. From the three base classifiers fd , fs , and fr , we design an aggregated classifier for the final output. This aggregated classifier, which we denote as fagg, integrates the three base classifiers by averaging over their predictions3:
yagg = fagg yd , yc , ys = softmax 1 yd + ys + yr . (7) 3
To jointly optimize the base classifiers as well as the aggregated classifier, we develop a co-training procedure that not only learns the classifiers to fit the self-supervised signals but also promotes
3
We have also tried to use attention mechanism to aggregate the score but didn’t see an obvious performance gain.

consistency among these classifiers. The co-training objective in-

volves three types of supervision, as detailed below.

The first loss ℓ1 is defines for the aggregated classifier f agg, which produces the final output. Let {(xi , yi }iN=1 be the training dataset, where xi is a query-anchor pair and yi is the label indicating

the correct position of the query term in the anchor mini-path. Then

ℓ1 is defined as the negative log-likelihood loss:

NC

ℓ1 = −

yi j log yiajgg,

(8)

i=1 j=1

where C = L + 1 is the number of labels for query-anchor matching. The second loss ℓ2 is defined for three base classifiers corre-

sponding to the three views:

NC

ℓ2 = −

yi j log yui j .

(9)

u ∈ {d,c,s } i=1 j=1

The third loss ℓ3 is a consistency loss that encourages the pre-

diction results from different views to agree with each other. We

use L2-distance to measure the difference between the classifiers

and define ℓ3 as:

N

ℓ3 =

yui − yvi 2 .

(10)

u,v ∈ {d,s,r } i=1

The overall objective of our model is then:

ℓ = ℓ1 + λℓ2 + µℓ3,

(11)

where λ > 0, µ > 0 are two pre-defined balancing hyper-parameters.

3.3 Model Learning and Inference
During training, we learn the model parameter Θ by minimizing the total loss ℓ using stochastic gradient optimizers such as Adam [18]. During inference, given a new query term q ∈ C, we traverse all the mini-paths P ∈ P and calculate the scores for all anchor terms p ∈ P based on the aggregated final prediction score yqP,p in Eq. (7). Specifically, for any anchor term pˆ, we calculate its score of

KDD ’20, August 23–27, 2020, Virtual Event, CA, USA
Table 1: The statistics of the three datasets for evaluation.

Dataset Environment Science Food

# of Terms

261

# of Edges

261

# of Layers

6

429 1486

452 1576

8

8

being the parent of query q as

ypˆ = |P1ˆ | yqP,pˆ, (12)
P ∈ Pˆ

where Pˆ is the set of mini-paths which contain term pˆ. Then, we rank all anchor terms and select the term p∗ with the highest score

as the predicted parent of the query q:

p∗ = arg max yp .

(13)

p ∈V0

3.4 Complexity Analysis
At the training stage, our model uses |P | training instances every epoch and thus scales linearly to the number of mini-paths in the existing taxonomy. From above we have listed the number of mini-paths in our training, and the number of such mini-paths is linear to O(|V0|) (i.e. the number of terms in the existing taxonomy). At the inference stage, for each query term, we calculate L|P | matching scores, where L is the length of the mini-path. To accelerate the computation, we use GPU for matrix multiplication and pre-calculate distributional and lexico-syntactic features and store the dependency paths for faster evaluation.

4 EXPERIMENTS
In this section, we evaluate the empirical performance of our pro-
posed STEAM method. Our experiments are designed to answer
the following three research questions:
• RQ1: How well does STEAM perform for the taxonomy expansion task compared with state-of-the-art methods?
• RQ2: How effective are the two key components in STEAM: mini-path-based prediction and multi-view co-training?
• RQ3: What are the effects of different parameters on the performance of STEAM?

4.1 Experiment Setup
4.1.1 Datasets. We evaluate the performance of our taxonomy construction method using three public benchmarks. These datasets come from the shared task of taxonomy construction in SemEval 2016 [6]. We use all the three English datasets in SemEval 2016, which correspond to three human-curated concept taxonomies from different domains: environment (EN), science (SCI), and food (Food). The statistics of these three benchmarks are presented in table 1. For each taxonomy, we start from the root term and randomly grow in a top-down manner until 80% terms are covered. We use the randomly-growed taxonomies as seed taxonomies for self-supervised learning, and the rest 20% terms as our test data. Our STEAM method and several baselines also require external text corpora to model the semantic relations between concept terms.
4.1.2 Baselines. We compare with the following baselines:

Yue Yu, Yinghao Li, Jiaming Shen, Hao Feng, Jimeng Sun, and Chao Zhang
• TAXI4 [30] is a taxonomy induction method that reached the first place in the SemEval 2016 task. It first extracts hypernym pairs based on substrings and lexico-syntactic patterns with domainspecific corpora and then organizes these terms into a taxonomy.
• HypeNet5 [38] is a strong hypernym extraction method, which uses an LSTM-CNN model to jointly model the distributional and relational information between term pairs.
• BERT+MLP is a distributional method for hypernym detection based on pre-trained BERT embeddings. For each term pair, it first obtains term embeddings from a pre-trained BERT model, and then feeds them into a Multi-layer Perceptron to predict whether they have the hypernymy relationship6.
• TaxoExpan7 [35] is state-of-the-art self-supervised method for taxonomy expansion. It adopts graph neural networks to encode the positional information and uses a linear layer to identify whether the candidate term is the parent of the query term. For a fair comparison, we also use BERT embeddings for TaxoExpan instead of the word embeddings as in the original paper.
4.1.3 Variants of STEAM. We also compare with several variants of STEAM to evaluate the effectiveness of its different modules: Concat directly concatenates the three features and feeds it into an MLP for prediction; Concat-D concatenates only the context and lexico-syntactic views; Concat-C concatenates the distributed and the lexico-syntactic features; Concat-L concatenates the distributed and the context features; STEAM-Co directly uses the aggregated classifier for prediction instead of the co-training objective (i.e., λ = µ = 0); STEAM-D co-trains without the distributed view; STEAM-C co-trains without the contextual view and STEAM-L co-trains without the lexico-syntactic view.
4.1.4 Implementation Details. All the baseline methods, except for BERT-MLP, are obtained from the code published by the original authors. The others (BERT-MLP, our model, and its variants) are all implemented in PyTorch. When learning our model, we use the ADAM optimizer [18] with a learning rate of 1e-3. On all the three datasets, we train the model for 40 epochs as we observe the model has converged after 40 epochs. To prevent overfitting, we used a dropout rate of 0.4 and a weight decay of 5e-4. For encoding context features, we follow [38] and set the dimensions for the POS-tag vector, dependency label vector, and edge direction vector, to 4, 5, and 1, respectively; and set the dimension for hidden units in the LSTM encoder to 200. For the three base MLP classifiers, we set the dimensions of the hidden layers to 50. For sampling negative mini-paths, we set the size of negative samples r = 4. In the co-training module, there are two key hyper-parameters: λ and µ for controlling the strength for training base classifiers and the consistency among classifiers. By default, we set λ = 0.1, µ = 0.1. We will study how these parameters affect the performance of our model later.
4.1.5 Evaluation Protocol. At test time, pinpointing the correct parent for a query term is a ranking problem. Specifically, given n test samples, let us use {yˆ1, yˆ2, · · · , yˆn } to denote their ground
4
https://github.com/uhh- lt/taxi
5
https://github.com/vered1986/HypeNET
6
For combining term embeddings, we experimented with Concat, Difference, and Sum as different fusing functions and report the best performance.
7
https://github.com/mickeystroller/TaxoExpan

STEAM: Self-Supervised Taxonomy Expansion with Mini-Paths

KDD ’20, August 23–27, 2020, Virtual Event, CA, USA

Table 2: Comparision of STEAM against the baseline methods on the three datasets (in %). To reduce the randomness, we ran all methods for three times and report the average performance. TAXI outputs an entire taxonomy instead of ranking lists, so we are unable to obtain the MRRs for it.

Dataset Metric

Environment

Science

Food

Acc MRR Wu&P Acc MRR Wu&P Acc MRR Wu&P

BERT+MLP 11.1 21.5 47.9 11.5 15.7 43.6 10.5 14.9 47.0 TAXI 16.7 – 44.7 13.0 – 32.9 18.2 – 39.2
HypeNet 16.7 23.7 55.8 15.4 22.6 50.7 20.5 27.3 63.2 TaxoExpan 11.1 32.3 54.8 27.8 44.8 57.6 27.6 40.5 54.2
STEAM 36.1 46.9 69.6 36.5 48.3 68.2 34.2 43.4 67.0

Table 3: Overall results of all variants of our methods on three datasets (in %).

Dataset Metric

Environment

Science

Food

Acc MRR Wu&P Acc MRR Wu&P Acc MRR Wu&P

Concat 25.0 40.3 64.2 20.4 25.8 51.1 15.5 23.8 49.6 Concat-D 30.6 38.6 63.7 11.1 20.1 48.1 23.1 28.9 55.4 Concat-C 27.7 37.4 57.8 13.5 25.7 53.3 25.3 31.2 58.3 Concat-L 11.1 31.4 57.7 13.5 23.7 39.1 8.30 13.4 40.1 STEAM-Co 25.0 41.0 66.3 32.7 45.3 64.4 31.1 40.7 65.1 STEAM-D 13.8 32.0 54.3 23.1 32.9 60.0 20.1 31.5 60.8 STEAM-C 11.1 26.8 49.2 32.7 44.5 67.2 19.3 29.7 59.3 STEAM-L 11.1 27.5 51.6 23.1 36.5 62.1 12.7 22.6 56.7
STEAM 36.1 46.9 69.6 36.5 48.3 68.2 34.2 43.4 67.0

truth positions, {y1, y2, · · · , yn } to denote model predictions. Follow existing works [25, 35, 40], we use multiple metrics as follows:

(1) Accuracy (Acc) measures the exact match accuracy for terms in the test set. It only counts the cases when the prediction equals

to the ground truth, calculated as

n

1

Acc =

I(yi = yˆi ).

n i=1

(2) Mean reciprocal rank (MRR) is the average of reciprocal ranks of a query concept’s true parent among all candidate terms.

Specifically, it is calculated as

n

1

1

MRR =

.

n i=1 rank(yi )

(3) Wu & Palmer similarity (Wu&P) calculates the semantic similarity between the predicted parent term y and the ground truth parent term yˆ as
ω (yˆ, y) = 2 × depth(LCA(yˆ, y)) depth(yˆ) + depth(y)
where “depth(·)” is the depth of a term in the taxonomy and “LCA(·, ·)”
is the least common ancestor of the input terms in the taxonomy.

Then, the overall Wu&P score is the mean Wu & Palmer similarity

for

all

terms

in

the

test

set

written

as

Wu&P

=

1 n

n i =1

ω

(y

i

,

yˆi

).

4.2 Experimental Results
4.2.1 Comparison with baselines. Table 2 reports the performance of STEAM and the baseline methods on the three benchmarks. From the results, we have the following observations: • STEAM consistently outperforms all the baselines by large margins on the three datasets. In particular, STEAM improves the performance of the state-of-the-art TaxoExpan model by 11.6%, 7.0% and 9.4% for Acc, MRR and Wu&P on average. Such improvements are mainly due to the mini-path-based prediction and the multi-view co-training designs in STEAM. • Among the baselines, TaxoExpan achieves the strongest overall performance. The key advantage of TaxoExpan compared with other baselines is that it propagates the embeddings among neighbors in the taxonomy via graph neural networks. From the results, we can see that embedding propagation is effective in improving the MRR, making it achieve close MRRs with STEAM. However,

TaxoExpan is largely outperformed by STEAM in accuracy. This phenomenon shows that while distributed features are useful for finding relevant concepts, contextual and lexico-syntactic features are important for pinpointing the exact hypernymy relationships. • Pre-trained BERT embeddings have remarkable expressive power. However, BERT embeddings alone can yield limited performance in the taxonomy expansion task since BERT does not well capture the contextual relations and between terms. STEAM is based on BERT embedding, but it integrates contextual and pattern information, which are highly useful for improving the performance. • TAXI underperforms other methods on all three datasets. The major drawback of TAXI and other taxonomy construction methods is that they fail to use self-supervision signals in the existing taxonomy. This hinders them from learning the hierarchical and semantic information. Moreover, they simply use lexico-syntactic patterns and neglect other distributional features, which is important for taxonomy expansion. • HypeNet outperforms BERT and TAXI since it combines the contextual and distributed features. However, it neglects the structural information during training and does not consider lexico-syntactic features, rendering it less effective than STEAM.
4.2.2 Ablations Studies. We perform ablation studies to study the effectiveness of the different components in STEAM: 1) mini-pathbased self-supervised learning; 2) the multi-view information; and 3) the co-training procedure. The Effect of Mini-Paths. To study the effectiveness of minipath-based self-supervised expansion, we vary the length L of minipaths. Note that, when L = 1, the model is reduced to performing hypernymy prediction. Figure 4 shows the performance of STEAM on the three datasets when L varies. Generally, when L is small, the performance of STEAM stably increases with L. Such results show that mini-paths can effectively capture the structural information in the seed taxonomy—apart from the ‘parent’ of the query term, the grandparents and siblings contain additional information to improve expansion performance. The mini-paths connect terms from different layers of the taxonomy and carry such information to make the model pinpoint the correct position. However, when L increases from 3 to 4, we observe slight performance drops. This is because the size of the training data shrinks for smaller taxonomies when L becomes larger. Take the environment dataset as an example:

KDD ’20, August 23–27, 2020, Virtual Event, CA, USA

L=1

0.6

L=2

L=3

0.4

L=4

0.2

0.0 Acc

MRR Wu&P

(a) Environment

L=1

0.6

L=2

L=3

0.4

L=4

0.2

0.0 Acc

MRR Wu&P

(b) Science

L=1

0.6

L=2

L=3

0.4

L=4

0.2

0.0 Acc

MRR Wu&P

(c) Food

Figure 4: The result for different length of mini-paths L over three datasets.

It contains 185 training samples when L = 3 while 83 when L = 4. As a result, the final performance decreases by 3.2% for accuracy. The Effect of Multi-view Information. We study the contributions of different views by comparing STEAM with its variants (STEAM-D, STEAM-C, STEAM-L). Table 3 shows the results on the three datasets. As shown, it is clear that all three types of features contribute significantly to the overall performance. When eliminating one of the three views, the average performance drops by 6.07%, 8.10% and 4.67% for the three metrics. The Effect of Co-training. Now we proceed to study the effectiveness of the co-training procedure. While integrating multiple views is important, how to integrate multi-view information is equally important. From the results in Table 2, one can see STEAM outperforms Concat by 15.3%, 16.2% and 13.3% for three metrics on average. This verifies the effectiveness of co-training compared with concatenation: the simple concatenation strategy cannot fully harvest the information from each view and could make the learning problem more difficult. Interestingly, the performance for Concat is even worse than Concat-D and Concat-C in accuracy on Food and Environment, which implies that simple concatenation can even hurt the performance with more views.
The co-training objective in STEAM involves two loss terms that encourage better learning of the base classifiers and the consistency among them. From Table 2, the performance gap between STEAM and STEAM-Co shows the effectiveness of these two terms. STEAM-Co only uses the aggregated classifier for prediction and underperforms STEAM by large margins. The reason is that these terms explicitly require every base classifier is sufficiently trained and mutually enhances each other; without them, certain views may not be fully leveraged, which limit the effectiveness in leveraging multi-view information for training.
4.2.3 Parameter Studies. In this subsection, we study the effects of different parameters on the performance of STEAM. We have already studied the effect of the path length in the ablation study, now we study the effects of two key parameters in the co-training procedure: 1) the weight of the prediction loss of the three base classifiers λ, and 2) the weight of the consistency loss µ. When evaluating one parameter, we fix other parameters to their default values and report the results. Due to the space limit, we only report the results on parameters on Science dataset as the tends and findings are similar for the three datasets. Effect of λ. Figure 5(a) shows the effect of λ on the Science dataset. We can observe that as λ increases, the performance improves for all

Yue Yu, Yinghao Li, Jiaming Shen, Hao Feng, Jimeng Sun, and Chao Zhang

Performance Performance

0.8

Acc

MRR

Wu&P

0.7

0.6

0.5

0.4

0.3 0.00 0.05 0.10 0.15 0.20 0.25

(a) λ

0.8

Acc

MRR

Wu&P

0.7

0.6

0.5

0.4

0.3 0 0.02 0.05 0.1 0.15 0.2

(b) µ

Figure 5: The performance of our model when varying different parameters.

Gold Parent: Physics
View Score Rank Distributed 0.812 11 Contextual 0.947 12 Lexico-syntactic 0.640 15 STEAM Output 0.799 1 (a) term Electrostatics (SCI)

Gold Parent: Medicine

View Distributed Contextual
Lexico-syntactic STEAM Output

Score 0.741 0.959 0.614 0.771

Rank 51 2 14 1

(d) term Podiatry (SCI)

Gold Parent: Fruit Juice

View

Score Rank

Distributed 0.720 25

Contextual 0.921 14

Lexico-syntactic 0.656 15

STEAM Output 0.765 1

(b) term Nectar (Food)

Gold Parent: Red Wine

View Distributed Contextual
Lexico-syntactic STEAM Output

Score 0.468 0.493 0.329 0.430

Rank 169 24 228 43

(e) term Chianti (Food)

Gold Parent: Mammal

View

Score Rank

Distributed 0.416 116

Contextual 0.987 1

Lexico-syntactic 0.615 31

STEAM Output 0.672 1

(c) term Whale Marine (EN)

Gold Parent: Sea Bed

View

Score Rank

Distributed 0.387 35

Contextual 0.568 22

Lexico-syntactic 0.483 127

STEAM Output 0.479 37

(f) term Inshore Grounds (EN)

Figure 6: Prediction result for several test terms from different datasets.
three metrics. This is because larger λ will add more weight to learning base classifiers and enforce each base classifier to achieve good prediction performance. As the base classifiers become stronger, the derived aggregated classifier can also become stronger. However, when λ ≥ 0.15, the performance decreases with λ. We suspect the reason is each single view can be one-sided and noisy to yield biased predictions, when λ is too large, the biased information from each single view can no longer be effectively eliminated during integration, which can hurt the overall performance. Effect of µ. Figure 5(b) shows the effect of µ. Similarly, as µ increases, the performance of STEAM first increases and then decreases when µ is too large. The reason for this phenomenon is that: 1) when µ is too small, the three models cannot regularize each other well, which hinders them from sharing the result with others; 2) when the µ is too large, then the output will be close to optimizing Equation 11. When one model does not perform well, it will negatively affect the other two models, which will deteriorate the performance of the overall model.
4.3 Case Studies and Error Analysis
Figure 6 shows multiple cases to illustrate the efficacy of STEAM. It reports the final prediction score of STEAM for the ground-truth parent, as well as the prediction scores from the three base classifiers. Based on the scores, we calculate the rank of the ground truth parent. From Figure 6(a), (b), we can find that there are cases when the predictions from all the three views are inadequate, but the final prediction can integrate the weak signals to rank the ground-truth

STEAM: Self-Supervised Taxonomy Expansion with Mini-Paths

KDD ’20, August 23–27, 2020, Virtual Event, CA, USA

to the top. Such cases verify the power of multi-view co-training in STEAM, which can utilize the complementary signals from all views and improve the final performance. Besides, Figure 6(c), (d) shows two cases when the predictions of one specific view are poor (e.g. Distributed view for term Whale Marine), yet STEAM can rectify the mistakes by leveraging the information from the other two views. Figure 6(e) and (f) show two random examples on which our model fails to provide the correct predictions. Through in-depth analysis, we found that one common case when our model cannot perform well is that all views cannot make accurate predictions. In such cases, the information from the three views is insufficient to capture the hypernymy relationships between the test term and its parent.
5 RELATED WORK
Taxonomy Construction. There have been many studies on automatic taxonomy construction. One line of works constructs taxonomies using cluster-based methods. They group terms into a hierarchy based on hierarchical clustering [1, 34, 44] or topic models [10, 24]. These methods can work in an unsupervised way. However, they cannot be applied to our taxonomy expansion problem, because they construct topic-level taxonomies where each node is a collection of topic-indicative terms instead of single terms. More relevant to our work are the methods developed for constructing term-level taxonomies. Focused on taxonomy induction, these methods organize hypernymy pairs into taxonomies. Graph optimization techniques [3, 8, 13, 19] have been proposed to organize the hypernymy graph into a hierarchical structure, and Mao et al. [26] utilize reinforcement learning to organize term pairs by optimizing a holistic tree metric over the training taxonomies. Very recently, Shang et al. [33] design a transfer framework to use the knowledge from existing domains for generating taxonomy for a new domain. However, all these methods attempt to construct taxonomies from scratch and cannot preserve the structure of the seed taxonomy. Hypernymy Detection. Hypernym detection aims at identifying hypernym-hyponym pairs, which is essential to taxonomy construction. Existing methods for hypernymy detection mainly fall into two categories: pattern-based methods and distributed methods. Pattern-based methods extract hypernymy pairs via pre-defined lexico-syntactic patterns [15, 30, 32]. One prominent work in this branch is the Hearst patterns [15], which extract hypernymy pairs based on a set of hand-crafted is-a patterns (e.g., “X is a Y”). Patternbased methods achieve good precision, but they suffer from low recall [43] and are prone to idiomatic expressions and parsing errors [20]. Distributed methods detect hypernymy pairs based on the distributed representations (e.g. word embeddings [9, 27, 31]) of terms. For a term pair ⟨x, y⟩, their embeddings are used for learning a binary classifier to predict whether it has the hypernymy relation [4, 7, 12, 37]. As embeddings are directly learned from the corpora, distributed methods eliminate the needs of designing hand-crafted patterns and have shown strong performance. However, their performance relies on a sufficient amount of labeled term pairs, which can be expensive to obtain. Taxonomy Expansion. Taxonomy expansion is less studied than taxonomy construction. Most existing works on taxonomy expansion aims to find new is-a relations and insert new terms to their

hypernyms. For example, Aly et al. [2] refine existing taxonomy by adopting hyperbolic embeddings [29] to better capture hierarchical lexical-semantic relationships, [36, 40] design various semantic patterns to determine the position to attach new concepts for expanding taxonomies, and Fauceglia et al. [11] use a hybrid method to take advantage of linguistic patterns, semantic web and neural networks for taxonomy expansion. However, the above methods only model the ‘parent-child’ relations and fail to capture the global structure of the existing taxonomy. To better exploit self-supervision signals, Manzoor et al. [25] study expanding taxonomies by jointly learning latent representations for edge semantics and taxonomy concepts. Recently, Shen et al. [35] propose position-enhanced graph neural networks to encode the relative position of terms and improve the overall quality of taxonomy. However, the above two approaches only consider distributional features such as word embeddings but neglect other types of relationships among terms. Compared with these methods, STEAM is novel in two aspects. First, it inserts new terms with mini-path-based classification instead of simple hypernym attachment, which models different layers to better preserve the holistic structure. Second, it considers multiple sources of features for expansion and integrates them with a multi-view co-training procedure.
6 CONCLUSION
We proposed STEAM, a self-supervised learning framework with novel mini-path-based prediction and a multi-view co-training objective. The self-supervised learning nature enables our model to optimize the utilization of the knowledge in the existing taxonomy without labeling efforts. Compared with the traditional node-tonode query-anchor pairs, the adoption of mini-paths captures more structural information thus facilitates the inference of a query’s attachment position. The multi-view co-training objective effectively integrates information from multiple input sources, including PGAT-propagated word embeddings, LSTM-embedded dependency paths and lexico-syntactic patterns. Comprehensive experiments on three benchmarks show that STEAM consistently outperforms all baseline models by large margins, which demonstrates its superiority for taxonomy expansion.
ACKNOWLEDGEMENT
This work was in part supported by the National Science Foundation award IIS-1418511, CCF-1533768 and IIS-1838042, the National Institute of Health award 1R01MD011682-01 and R56HL138415.
APPENDIX–EXTERNAL SOURCES OF CORPUS
STEAM (our method) and several baselines require external text corpora to model the semantic relations between concept terms. For all the three benchmarks, we collect the following public corpora: 1) the Wikipedia dump8, 2) the UMBC web-based corpus9; 3) the One Billion Word Language Modeling Benchmark10.
We directly match the terms with the corpus with tools available online (i.e. WikiExtractor11) and only preserve the sentences that
8
We use the 20190801 version of wikidump during our experiments.
9
https://ebiquity.umbc.edu/resource/html/id/351
10
https://www.statmt.org/lm-benchmark/
11
https://github.com/attardi/wikiextractor

KDD ’20, August 23–27, 2020, Virtual Event, CA, USA
term pairs co-occur. In this way, for each dataset, we obtain a
tailored corpus which preserves the co-occurrence between terms.
The information for these corpora are summarized as: • Environment: The corpus size is 824MB with 1.51M sentences. • Science: The corpus size is 1.36GB with 2.07M sentences. • Food: The corpus size is 2.00GB with 3.42M sentences.
REFERENCES
[1] Daniele Alfarone and Jesse Davis. 2015. Unsupervised Learning of an IS-A Taxonomy from a Limited Domain-Specific Corpus. In IJCAL. 1434–1441.
[2] Rami Aly, Shantanu Acharya, Alexander Ossa, Arne Köhn, Chris Biemann, and Alexander Panchenko. 2019. Every Child Should Have Parents: A Taxonomy Refinement Algorithm Based on Hyperbolic Term Embeddings. In ACL. 4811– 4817.
[3] Mohit Bansal, David Burkett, Gerard De Melo, and Dan Klein. 2014. Structured learning for taxonomy induction with belief propagation. In ACL. 1041–1051.
[4] Marco Baroni, Raffaella Bernardi, Ngoc-Quynh Do, and Chung-chieh Shan. 2012. Entailment above the word level in distributional semantics. In EACL. 23–32.
[5] Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor. 2008. Freebase: A Collaboratively Created Graph Database for Structuring Human Knowledge. In SIGMOD. ACM, 1247–1250.
[6] Georgeta Bordea, Els Lefever, and Paul Buitelaar. 2016. SemEval-2016 Task 13: Taxonomy Extraction Evaluation (TExEval-2). In SemEval-2016. ACL, 1081–1091.
[7] Haw-Shiuan Chang, Ziyun Wang, Luke Vilnis, and Andrew McCallum. 2018. Distributional Inclusion Vector Embedding for Unsupervised Hypernymy Detection. In NAACL. 485–495.
[8] Anne Cocos, Marianna Apidianaki, and Chris Callison-Burch. 2018. Comparing constraints for taxonomic organization. In NAACL. 323–333.
[9] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In NAACL-HLT. 4171–4186.
[10] Doug Downey, Chandra Bhagavatula, and Yi Yang. 2015. Efficient methods for inferring large sparse topic hierarchies. In the ACL. 774–784.
[11] Nicolas Rodolfo Fauceglia, Alfio Gliozzo, Sarthak Dash, Md Faisal Mahbub Chowdhury, and Nandana Mihindukulasooriya. 2019. Automatic Taxonomy Induction and Expansion. In EMNLP-IJCNLP Demo. 25–30.
[12] Ruiji Fu, Jiang Guo, Bing Qin, Wanxiang Che, Haifeng Wang, and Ting Liu. 2014. Learning semantic hierarchies via word embeddings. In ACL. 1199–1209.
[13] Amit Gupta, Rémi Lebret, Hamza Harkous, and Karl Aberer. 2017. Taxonomy induction using hypernym subsequences. In CIKM. 1329–1338.
[14] Sanda M Harabagiu, Steven J Maiorano, and Marius A Paşca. 2003. Open-domain textual question answering techniques. Natural Language Engineering 9, 3 (2003), 231–267.
[15] Marti A Hearst. 1992. Automatic acquisition of hyponyms from large text corpora. In COLING. ACL, 539–545.
[16] Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, and Tuo Zhao. 2020. SMART: Robust and Efficient Fine-Tuning for Pre-trained Natural Language Models through Principled Regularized Optimization. In ACL.
[17] Giannis Karamanolakis, Jun Ma, and Xin Luna Dong. 2020. TXtract: TaxonomyAware Knowledge Extraction for Thousands of Product Categories. arXiv preprint arXiv:2004.13852 (2020).
[18] Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980 (2014).
[19] Zornitsa Kozareva and Eduard Hovy. 2010. A semi-supervised method to learn and construct taxonomies using the web. In EMNLP. 1110–1118.
[20] Zornitsa Kozareva, Ellen Riloff, and Eduard Hovy. 2008. Semantic Class Learning from the Web with Hyponym Pattern Linkage Graphs. In ACL. 1048–1056.
[21] Chen Liang, Yue Yu, Haoming Jiang, Siawpeng Er, Ruijia Wang, Tuo Zhao, and Chao Zhang. 2020. BOND: Bert-Assisted Open-Domain Named Entity Recognition with Distant Supervision. In SIGKDD. ACM.
[22] Weixin Liang, James Zou, and Zhou Yu. 2020. Beyond User Self-Reported Likert Scale Ratings: A Comparison Model for Automatic Dialog Evaluation. arXiv preprint arXiv:2005.10716 (2020).
[23] Carolyn E Lipscomb. 2000. Medical subject headings (MeSH). Bulletin of the Medical Library Association 88, 3 (2000), 265.
[24] Xueqing Liu, Yangqiu Song, Shixia Liu, and Haixun Wang. 2012. Automatic taxonomy construction from keywords. In SIGKDD. 1433–1441.
[25] Emaad Manzoor, Dhananjay Shrouty, Rui Li, and Jure Leskovec. 2020. Expanding Taxonomies with Implicit Edge Semantics. In The Web Conference.
[26] Yuning Mao, Xiang Ren, Jiaming Shen, Xiaotao Gu, and Jiawei Han. 2018. Endto-end reinforcement learning for automatic taxonomy induction. In ACL. 2462– 2472.
[27] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Distributed Representations of Words and Phrases and Their Compositionality. In NIPS. 3111–3119.

Yue Yu, Yinghao Li, Jiaming Shen, Hao Feng, Jimeng Sun, and Chao Zhang
[28] George A. Miller. 1995. WordNet: A Lexical Database for English. Commun. ACM 38, 11 (Nov. 1995), 39–41.
[29] Maximillian Nickel and Douwe Kiela. 2017. Poincaré embeddings for learning hierarchical representations. In NIPS. 6338–6347.
[30] Alexander Panchenko, Stefano Faralli, Eugen Ruppert, Steffen Remus, Hubert Naets, Cédrick Fairon, Simone Paolo Ponzetto, and Chris Biemann. 2016. TAXI at SemEval-2016 Task 13: a Taxonomy Induction Method based on Lexico-Syntactic Patterns, Substrings and Focused Crawling. In SemEval-2016. ACL, 1320–1327.
[31] Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. Glove: Global Vectors for Word Representation. In EMNLP. ACL, 1532–1543.
[32] Stephen Roller, Douwe Kiela, and Maximilian Nickel. 2018. Hearst Patterns Revisited: Automatic Hypernym Detection from Large Text Corpora. In ACL. 358–363.
[33] Chao Shang, Sarthak Dash, Md Faisal Mahbub Chowdhury, Nandana Mihindukulasooriya, and Alfio Gliozzo. 2020. Taxonomy Construction of Unseen Domains via Graph-based Cross-Domain Knowledge Transfer. In ACL. ACL.
[34] Jingbo Shang, Xinyang Zhang, Liyuan Liu, Sha Li, and Jiawei Han. 2020. NetTaxo: Automated Topic Taxonomy Construction from Large-Scale Text-Rich Network. In The Web Conference.
[35] Jiaming Shen, Iris Shen, Chenyan Xiong, Chi Wang, Kuansan Wang, and Jiawei Han. 2020. TaxoExpan: Self-supervised Taxonomy Expansion with PositionEnhanced Graph Neural Network. In The Web Conference.
[36] Jiaming Shen, Zeqiu Wu, Dongming Lei, Chao Zhang, Xiang Ren, Michelle T Vanni, Brian M Sadler, and Jiawei Han. 2018. Hiexpan: Task-guided taxonomy construction by hierarchical tree expansion. In SIGKDD. 2180–2189.
[37] Yu Shi, Jiaming Shen, Yuchen Li, Naijing Zhang, Xinwei He, Zhengzhi Lou, Qi Zhu, Matthew Walker, Myunghwan Kim, and Jiawei Han. 2019. Discovering Hypernymy in Text-Rich Heterogeneous Information Network by Exploiting Context Granularity. In CIKM. ACM, 599–608.
[38] Vered Shwartz, Yoav Goldberg, and Ido Dagan. 2016. Improving Hypernymy Detection with an Integrated Path-based and Distributional Method. In ACL. ACL, 2389–2398.
[39] Hsiao-Yu Tung, Hsiao-Wei Tung, Ersin Yumer, and Katerina Fragkiadaki. 2017. Self-supervised learning of motion capture. In NIPS. 5236–5246.
[40] Nikhita Vedula, Patrick K Nicholson, Deepak Ajwani, Sourav Dutta, Alessandra Sala, and Srinivasan Parthasarathy. 2018. Enriching taxonomies with functional domain knowledge. In SIGIR. 745–754.
[41] Denny Vrandečiundefined. 2012. Wikidata: A New Platform for Collaborative Data Collection. In WWW Companion. ACM, 1063–1064.
[42] Chi Wang, Marina Danilevsky, Nihit Desai, Yinan Zhang, Phuong Nguyen, Thrivikrama Taula, and Jiawei Han. 2013. A phrase mining framework for recursive construction of a topical hierarchy. In SIGKDD. 437–445.
[43] Wentao Wu, Hongsong Li, Haixun Wang, and Kenny Q Zhu. 2012. Probase: A probabilistic taxonomy for text understanding. In SIGMOD. 481–492.
[44] Chao Zhang, Fangbo Tao, Xiusi Chen, Jiaming Shen, Meng Jiang, Brian Sadler, Michelle Vanni, and Jiawei Han. 2018. Taxogen: Unsupervised topic taxonomy construction by adaptive term embedding and clustering. In SIGKDD. 2701–2709.
[45] Hao Zhang, Zhiting Hu, Yuntian Deng, Mrinmaya Sachan, Zhicheng Yan, and Eric Xing. 2016. Learning Concept Taxonomies from Multi-modal Data. In ACL. 1791–1801.
[46] Yuchen Zhang, Amr Ahmed, Vanja Josifovski, and Alexander Smola. 2014. Taxonomy discovery for personalized recommendation. In WSDM. 243–252.

