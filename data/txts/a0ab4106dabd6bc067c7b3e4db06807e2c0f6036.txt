NLP From Scratch Without Large-Scale Pretraining: A Simple and Efﬁcient Framework
Xingcheng Yao1 Yanan Zheng2 Xiaocong Yang35 Zhilin Yang145 1 Institute for Interdisciplinary Information Sciences, Tsinghua University 2 Department of Computer Science and Technology, Tsinghua University
3 School of Economics and Management, Tsinghua University 4 Shanghai Qi Zhi Institute 5 Recurrent AI, Inc yxc18@mails.tsinghua.edu.cn, yangxc.18@sem.tsinghua.edu.cn,
{zyanan,zhiliny}@tsinghua.edu.cn

arXiv:2111.04130v1 [cs.CL] 7 Nov 2021 Perf.

Abstract
Pretrained language models have become the standard approach for many NLP tasks due to strong performance, but they are very expensive to train. We propose a simple and efﬁcient learning framework TLM that does not rely on large-scale pretraining1. Given some labeled task data and a large general corpus, TLM uses task data as queries to retrieve a tiny subset of the general corpus and jointly optimizes the task objective and the language modeling objective from scratch. On eight classiﬁcation datasets in four domains, TLM achieves results better than or similar to pretrained language models (e.g., RoBERTa-Large) while reducing the training FLOPs by two orders of magnitude. With high accuracy and efﬁciency, we hope TLM will contribute to democratizing NLP and expediting its development2.
1 Introduction
Pretrained language models (PLMs) have drawn much attention from the natural language processing (NLP) community. Neural networks based on the Transformer architecture (Vaswani et al., 2017) are trained on large general corpora for self-supervised language modeling tasks such as masked language modeling (Devlin et al., 2018; Liu et al., 2019; Raffel et al., 2019), autoregressive language modeling (Radford et al., 2018; Brown et al., 2020), permutation language modeling (Yang et al., 2020), etc, and then are ﬁnetuned on a small amount of labeled data for speciﬁc downstream tasks. This pretraining-ﬁnetuning framework has
1In the broadest sense, pretraining means training on some objectives before optimizing the target tasks. In contrast, throughout the paper, we use “pretraining” to only refer to task-agnostic training of language models on a large general corpus, such as BERT (Devlin et al., 2018).
2Our code, model checkpoints and datasets are publicly available at: https://github.com/yaoxingcheng/ TLM

86.0

85.5

85.0

84.5

84.0 TLM

83.5

BERT

RoBERTa

83.0

10 4 10 3 10 2 10 1

100

101

Relative FLOPs

Figure 1: Average performance on eight tasks v.s. relative FLOPs w.r.t. RoBERTa-Large (Liu et al., 2019). TLM slightly outperforms RoBERTa-Large while reducing FLOPs by two orders of magnitude.

signiﬁcantly improved the performance of many NLP tasks.
However, while considered effective, large-scale pretraining is usually computationally expensive. For example, RoBERTa-Large (Liu et al., 2019), a widely-used PLM, consumes a computational cost of 4.36 × 1021 FLOPs3. Larger PLMs such as GPT3 (Brown et al., 2020) consume 50 times more FLOPs for training than RoBERTa-Large. The expensiveness of large-scale pretraining prevents many research groups with limited budgets from pretraining customized language models, exploring new neural architectures, or improving pretraining loss functions. In contrast, a large number of NLP researchers resort to improving the ﬁnetuning algorithms, whose performance is largely upperbounded by the pretraining procedure. This creates a high barrier of NLP research and might not be ideal for the long-term development of the ﬁeld.
Even though there have been efforts devoted to
3It was pretrained with 1,000 V100 GPUs each with 32GB memory for approximately one day.

studying and improving the efﬁciency of language model pretraining (So et al., 2021; Tay et al., 2021; Chen et al., 2021; Clark et al., 2020), most of them focus on designing sample-efﬁcient self-supervised tasks or discovering efﬁcient Transformer architectures suitable for pretraining. Their improvements are limited, with a reduction of computational costs (in terms of FLOPs) less than one order of magnitude. Another line of works target reducing the sizes of PLMs using distillation (Jiao et al., 2019; Sanh et al., 2019) to improve the efﬁciency of inference, but these methods rely on pretraining a large PLM before distillation. Moreover, distilled models often do not perform as well as some of the best non-distilled PLMs such as RoBERTa-Large (Jiao et al., 2019; Sanh et al., 2019).
In this work, we explore alternatives to the standard pretraining-ﬁnetuning paradigm, aiming at more drastic efﬁciency improvement without performance drop. We propose a simple, efﬁcient, pretraining-free framework, Task-driven Language Modeling (TLM). Given a large general corpus and some labeled task data, TLM directly trains a model from scratch without relying on PLMs. TLM is motivated by two key ideas. First, humans master a task by using only a small portion of world knowledge (e.g., students only need to review a few chapters, among all books in the world, to cram for an exam). We hypothesize that there is much redundancy in the large corpus for a speciﬁc task. Second, training on supervised labeled data is much more data efﬁcient for downstream performance than optimizing the language modeling objective on unlabeled data. Based on these motivations, TLM uses the task data as queries to retrieve a tiny subset of the general corpus. This is followed by jointly optimizing a supervised task objective and a language modeling objective using both the retrieved data and the task data.
We evaluate TLM on eight different tasks covering the domains of news, review, computer science, and biomedical science, following the setting of Gururangan et al. (2020). TLM achieves results better than or similar to BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) while reducing the training FLOPs by two orders of magnitude4.
4This effectively reduces the cost from training on 1,000 GPUs for one day to training on 8 GPUs for 42 hours.

2 Related work
Pretrained Language Models Pretrained language models have become the de-facto solution to many of the NLP tasks (Devlin et al., 2018; Liu et al., 2019; Raffel et al., 2019; Radford et al., 2018; Brown et al., 2020; Yang et al., 2020). Those models are usually pretrained on a large-scale corpus in a self-supervised manner to learn a contextualized representation of tokens in natural language, and then are ﬁne-tuned with labeled data for speciﬁc tasks. BERT (Devlin et al., 2018), one of the most popular PLMs, is pretrained on a 16GB English corpus using a masked language modeling objective (i.e. predicting randomly masked tokens). RoBERTa (Liu et al., 2019) inherits the training objective of BERT, but is pretrained on a larger corpus consisting of 160GB English texts with larger batch size and dynamic token masking. In this work, we take both BERT and RoBERTa as our major baselines.
Efﬁcient Pretraining for NLP There is a line of work dedicated to improving the efﬁciency of pretraining language models. You et al. (2019) and Shoeybi et al. (2019) utilized the data and model parallelism across different computational devices to accelerate the pretraining process. However, accelerating through parallelism does not actually reduce computational costs in terms of FLOPs for training models at large scale. Chen et al. (2021) and So et al. (2021) tried to identify efﬁcient neural network architectures for language model pretraining, based on the lottery ticket hypothesis and neural architecture search. Such modiﬁcations on architecture can bring about 50% ∼ 70% reduction in computational costs. Clark et al. (2020) and He et al. (2020) incorporated manually designed mechanisms into language model pretraining, such as adversarial training and disentangled representation of content and position, which brings about 50% ∼ 75% reduction in computational costs. In this work, orthogonal to the aforementioned works, we investigate improving efﬁciency by reducing training data redundancy. Our approach also results in more drastic improvements.
Efﬁcient Inference of Pretrained Models Another line of work aims at improving inference efﬁciency of PLMs. Some works improve inference efﬁciency by distilling large PLMs into small-sized models and using the distilled models for inference, such as TinyBERT (Jiao et al.,

General corpus

encoder

LM Objective

Task data

encoder

Task Objective

Traditional Approach

Task data

I enjoy the movie Jaws, directed by Steven Spielberg in 1975.

Query
General corpus

An introduction to Spielberg...

Description of Retrieve Jaws, 1975...

I like the movies by Lucas...

Data Selection

A small subset of the corpus

Our Approach

encoder

Task Objective
+
LM Objective

Joint Learning

Figure 2: Comparison between the traditional pretraining-ﬁnetuning approach and our proposed framework TLM: instead of training a language model over the entire general corpus and then ﬁnetuning it on task data, we ﬁrst use task data as queries to retrieve a tiny subset of the general corpus, and then perform joint learning on both the task objective and self-supervised language modeling objective.

2019), DistilBERT (Sanh et al., 2019), MobileBERT (Sun et al., 2020), FastBERT (Liu et al., 2020), BORT (de Wynter and Perry, 2020), and BERT-of-Theseus (Xu et al., 2020). Other works speed up inference by quantizing PLMs with lowprecision representations during inference, such as Q8-BERT (Zafrir et al., 2019), Q-BERT (Shen et al., 2019), and I-BERT (Kim et al., 2021). Another type of works, such as (Wang et al., 2019; Michel et al., 2019; Gordon et al., 2020), adopt pruning by removing parts of PLMs to make it smaller and faster. However, these methods rely on large PLMs, and the performance after distillation, pruning, or quantization often decreases to a certain extent compared with some of the best PLMs (e.g., RoBERTa-Large). In contrast, our approach doesn’t rely on large-scale pre-training and achieves better or at least comparable performance.
Domain and Task Adaptation for Pretrained Models Domain-adaptive ﬁnetuning is a method that ﬁnetunes a pretrained model on in-domain data using a language modeling objective. It has been shown to be effective for domain and task adaptation (Gururangan et al., 2020; Zhang et al., 2019; Li et al., 2020; Lee et al., 2020). There are a few crucial differences between domain-adaptive ﬁnetuning and TLM. First, TLM is a general method to improve training efﬁciency that does not use any additional domain data. It only utilizes the general corpus as in BERT and RoBERTa. In comparison, domain-adaptive ﬁnetuning uses domain data to improve domain adaptation. Second, while previous works on domain-adaptive ﬁnetuning are built upon a model pretrained on the general cor-

pus, TLM learns from scratch without large-scale pretraining to substantially save computation costs.
3 Method
3.1 TLM: Task-Driven Language Modeling
It is an interesting phenomenon that humans are able to quickly master a certain task with limited time and effort by focusing only on pieces of relevant knowledge. For example, when students cram for exams, they review a few chapters instead of going through all books in the world. Following this observation, we conjecture that one of the key aspects of learning a task is to quickly and precisely locate task-relevant information. To this end, we develop TLM that ﬁrst automatically retrieves relevant training data from a general corpus and then learns on the retrieved data and task data combined.
Formally, given a general corpus D = {di}i where di is a document, and labeled task data T = {(xi, yi)}i where xi is text and yi ∈ Y is a label5, our goal is to train a model f to estimate the conditional probability for classiﬁcation f (x) = pˆ(y|x).
TLM consists of two steps as shown in Figure 2.
1. Retrieve data from a general corpus using task data as queries.
2. Train a model from scratch by jointly optimizing the task objective and the language modeling objective on the retrieved data and task data.
5While it is straightforward to extend our framework to generation tasks, we focus on classiﬁcation tasks in this work.

Retrieval From General Corpus For each example in the task data xi ∈ T , we retrieve a set of documents Si = {d˜i,1, d˜i,2, · · · } from the given general corpus D. The set Si represents the top-K similar documents to xi in D. Retrieved data for all examples xi are combined S = ∪iSi. Retrieved data S is a tiny subset of the general corpus D.
We use BM25 (Robertson and Zaragoza, 2009) for retrieval due to its efﬁciency. While using embedding-based dense retrievers (Karpukhin et al., 2020) might lead to better retrieval results, we do not consider these methods to keep our approach as simple as possible. Moreover, dense retrievers rely on pretraining, which might bring additional computational costs. The exploration of achieving a better tradeoff between efﬁciency and retrieval performance is left to future work. Moreover, for tasks with extremely long texts (e.g., Helpfulness (McAuley et al., 2015)), we ﬁnd it more efﬁcient to extract keywords (e.g., using the RAKE algorithm (Rose et al., 2010)) to form the queries for retrieval instead of using the entire input sequence. We call the retrieved data S external data and the task data T internal data.
Note that our data retrieval method is taskagnostic—it only depends on text x without dependency on y. Moreover, the retrieval procedure does not assume the availability of domain-speciﬁc data. It operates on a general corpus and has the same input as the pretraining-ﬁnetuning paradigm.
Joint Training Given both the internal and external data, we train a language model f from scratch. Let Lmlm(x) be the masked language modeling loss as in BERT (Devlin et al., 2018), and let Ltask(f (x), y) be the task loss function (e.g., cross entropy for classiﬁcation). TLM optimizes the following loss function:
Ex∼S [ρ1Lmlm(x)]
+Ex,y∼T [ρ2Lmlm(x) + Ltask(f (x), y)]
where ρ1 and ρ2 are hyperparameters. The network architecture we employ is identical to BERT, where we use a CLS head for classiﬁcation and an LM head for masked language modeling. TLM can also be extended to other architectures for nonclassiﬁcation tasks. Our implementation involves a two-stage training procedure. In the ﬁrst stage, we interleave one batch of internal data with ρ1 batches of external data for mini-batch stochastic gradient descent, where ρ1 is set as an integer. In the second stage, we set both ρ1 and ρ2 as zero to

only ﬁnetune the model on internal data with the task objective.
3.2 Comparison Between TLM and PLMs
Both TLM and pretraining-ﬁnetuning have two stages. In fact, the second stage of TLM equals the traditional ﬁnetuning stage. The main difference between the ﬁrst stage of TLM and pretraining (PLMs) is shown in Table 1. Unlike PLMs which learn as much task-agnostic knowledge as possible at an extremely high cost, TLM learns task-related knowledge for each task with very low costs.

Loss Function Training Data
Compute Cost Generality

TLM
Ltask and Lmlm A tiny subset of D
and task data T 8 GPUs 42 hours
Task-Driven

PLMs Lmlm
The entire D
1,000 GPUs one day
Task-Agnostic

Table 1: Comparison between TLM and PLMs. Here we provide qualitative comparison, while quantitative comparison in terms of training data size, FLOPs, and the number of parameters is available in Table 2.

Given the above difference between TLM and PLMs, we will discuss the pros and cons of TLM in detail.
Democratizing NLP In the pretrainingﬁnetuning paradigm, the ﬁnetuning performance is largely upper bounded by the pretrained model. However, due to the constraints of computational resources, the majority of NLP researchers cannot afford training large-scale language models and resort to studying the ﬁnetuning algorithms. Since only a small portion of researchers are working on the architectures, loss functions, and other design choices of PLMs, there is a risk that the development of the ﬁeld might be slowing down. On the other hand, TLM is efﬁcient and highly performant. As a result, TLM has the potential of democratizing NLP and expediting its development by allowing most researchers to freely explore the architectures, loss functions, algorithms, and other design choices in the neighborhood of a state-of-the-art solution.
Efﬁciency TLM improves over PLMs in terms of per-task FLOPs. In many cases when there are only a few target tasks, TLM is favorable. For example, a researcher might be interested in solving four tex-

tual entailment datasets, or an industrial team might want to improve a recommender system which can be viewed as one task. However, if the goal is to solve 1,000 tasks at once (e.g., building an NLP platform to serve multiple business units within a corporate), PLMs might still be preferred.
Flexibility Since TLM is task-driven, there is a larger degree of ﬂexibility. Researchers can use custom strategies for tokenization, sequence length, data representations, hyperparameter tuning, etc, which might improve performance and/or efﬁciency.
Generality PLMs learn task-agnostic general representations and can be used for few-shot and zero-shot learning (Brown et al., 2020). In comparison, TLM trades generality for efﬁciency by learning only task-speciﬁc representations. How to further improve TLM in terms of learning more general representations poses a challenge for future work. We believe multi-task learning might alleviate this issue given recent observations (Wei et al., 2021; Lee et al., 2021), especially for in-domain zero-shot generalization. It might also be possible to combine pretraining with TLM, e.g., using a small PLM with TLM to match a larger PLM, to achieve a better tradeoff between generality and efﬁciency.
4 Experiments
In this section, we conduct experiments to quantitatively compare TLM and pretraining-ﬁnetuning on different tasks.
4.1 Setup
Datasets Following the setting of (Gururangan et al., 2020), we conduct experiments on eight tasks over four domains, including biomedical science, computer science, news, and reviews (two tasks in each domain). The tasks can be categorized into high-resource and low-resource tasks. Highresource tasks has more than 5K task data, including AGNews (Zhang et al., 2015), IMDB (Maas et al., 2011), RCT (Dernoncourt and Lee, 2017), and Helpfulness (McAuley et al., 2015), while low-resource tasks include ChemProt (Kringelum et al., 2016), ACL-ARC (Jurgens et al., 2018), SciERC (Luan et al., 2018), and HyperPartisan (Kiesel et al., 2019). For the general training corpus, we collected two corpora that respectively match the original training corpora of BERT and RoBERTa.

We name them respectively Corpus-BERT (CBERT) and Corpus-RoBERTa (CRoBERTa). The size of CRoBERTa is 10 times larger than CBERT.
Baselines Our experiments focus on comparison with general PLMs. We ﬁnetuned both BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) of base and large scales as the baselines. Although TLM is a general method without using addition in-domain data, it even performs close to domainadaptive ﬁnetuning methods (Gururangan et al., 2020) (see Appendix A for detailed comparison).
Evaluation Strategy We report the average performance across three random seeds, together with the standard deviation. For evaluation metrics, following Beltagy et al. (2019) and Gururangan et al. (2020), we report the test micro-F1 for ChemProt and RCT, and macro-F1 for the rest of the datasets.
For fair comparison, we evaluate TLM of different training scales. The training scale is deﬁned by three factors, including the number of parameters, the size of the general corpus, and the number of total training tokens. The number of total training tokens is calculated as the product of training steps, batch size, and sequence length. We report TLM at three training scales as shown in Table B.1, namely small, medium, and large scales. Each scale of TLM is accordingly compared to the PLM baselines with an increasing computational cost.
Training Details For each experiment of TLM, while ﬁxing the training scale hyper-parameters (i.e., training steps, batch size and sequence length), we perform a grid search over ρ1 and ρ2. We listed the hyper-parameters used in Table B.1 in Appendix.
4.2 Main Results
Table 2 shows the main results that compare TLM of three different scales and the according PLM baselines. In conclusion, TLM can achieve results that are better than or comparable to the baselines with substantial reduction in FLOPs and the size of training data. Speciﬁcally, at a small scale, TLM achieves comparable results to BERT-Large with an average of 1/33 of FLOPs and 1/16 of the training corpus. At the medium and large scales, TLM improves the performance by 0.59 and 0.24 points on average respectively, while signiﬁcantly reducing both FLOPs and the training data size by two orders of magnitude or more. These results conﬁrm that TLM is highly accurate and much more

Model

#Param FLOPs1 Data2 AGNews Hyp. Help. IMDB ACL. SciERC Chem. RCT Avg.

BERT-Base3
BERT-Large3 TLM (small-scale)

109M 2.79E19 16GB 355M 9.07E19 16GB 109M 2.74E18 0.91GB

93.50
±0.15
93.51
±0.40
93.74
±0.20

91.93
±1.74
91.62
±0.69
93.53
±1.61

69.11
±0.17
69.39
±1.14
70.54
±0.39

93.77
±0.22
94.76
±0.09
93.08
±0.17

69.45
±2.90
69.13
±2.93
69.84
±3.69

80.98
±1.07
81.37
±1.35
80.51
±1.53

81.94
±0.38
83.64
±0.41
81.99
±0.42

87.00
±0.06
87.13
±0.09
86.99
±0.03

83.46 83.82 83.78

RoBERTa-Base3 125M 1.54E21 160GB

TLM (medium-scale)

109M 8.30E18 1.21GB

94.02
±0.15
93.96
±0.18

93.53
±1.61
94.05
±0.96

70.45
±0.24
70.90
±0.73

95.43
±0.16
93.97
±0.10

68.34
±7.27
72.37
±2.11

81.35
±0.63
81.88
±1.92

82.60
±0.53
83.24
±0.36

87.23
±0.09
87.28
±0.10

84.12 84.71

RoBERTa-Large3 355M 4.36E21 160GB

TLM (large-scale)

355M 7.59E19 3.64GB

94.30
±0.23
94.34
±0.12

95.16
±0.00
95.16
±0.00

70.73
±0.62
72.49
±0.33

96.20
±0.19
95.77
±0.24

72.80
±0.62
72.19
±1.72

82.62
±0.68
83.29
±0.95

84.62
±0.50
85.12
±0.85

87.53
±0.13
87.50
±0.12

85.50 85.74

1 The total training compute (FLOPs) is calculated by (6 × Total_Training_Tokens × Parameter_Size) as in (Brown et al., 2020). For
TLM, FLOPs are reported as the averaged result over eight tasks. 2 The size of data selected from general corpus that are actually used in training. For TLM, it is reported by averaging over eight tasks. 3 The BERT-Base and BERT-Large are pretrained by (Devlin et al., 2018) and RoBERTa-Base and RoBERTa-Large are pretrained
by (Liu et al., 2019). We ﬁnetuned them to obtain the results over the eight tasks.

Table 2: Evaluation results for TLM at three different training scales. For each task, we report the average F1 score across three random seeds with standard deviations as subscripts. We also list the number of parameters, the total training compute (FLOPs), and the size of training corpus for comparison.

efﬁcient than PLMs. Moreover, TLM gains more advantages in efﬁciency at a larger scale. This indicates that larger-scale PLMs might have been trained to store more general knowledge that is not useful for a speciﬁc task.
4.3 Ablation Study
4.3.1 Data Retrieval
Table 3 shows the comparison between different retrieval methods (i.e., BM25 and random retrieval) and different sizes of the general corpus. We ﬁnd that given the same general corpus, the results of BM25 signiﬁcantly outperform those of random retrieval by a large margin on all tasks, showing that using task-relevant data for joint training is crucial for the best performance. Speciﬁcally, BM25 shows an advantage of almost 1 point against random retrieval on high-resource tasks such as IMDB, and more signiﬁcant advantages on low-resource tasks such as SciERC and ChemProt by around 3-4 points. This is also aligned with our intuition that low-resource tasks rely more on external data.
By comparing the results of CBERT and CRoBERTa with BM25, we observe that increasing the size of the general corpus improves performance (by 0.5, 1.34, and 1.35 points on IMDB, SciREC, and ChemProt respectively). The gains of using 10 times more data are similar to the ones observed in PLMs (Yang et al., 2020; Liu et al., 2019). This

IMDB SciERC ChemProt

Random w/ CBERT w/ CRoBERTa
BM25 w/ CBERT w/ CRoBERTa

93.65±0.09 94.04±0.22
94.40±0.09 94.90±0.06

83.80±0.62 83.10±1.54
86.07±0.48 87.41±0.36

80.65±0.48 80.73±0.46
83.64±0.26 84.99±0.72

Table 3: Results on the development set using different retrieval methods and different general corpora on each task. We compared two data retrieval methods: random retrieval and the BM25 algorithm. We compare two source general corpora: the corpus used in BERT (CBERT) and the corpus used in RoBERTa (CRoBERTa). The size of CRoBERTa is 10 times larger than CBERT.

indicates that although TLM only uses a small amount of data, it is able to scale when a larger general corpus is available while maintaining efﬁciency. On the other hand, the gains of using a larger corpus diminish with random retrieval, showing that random retrieval, as a task-agnostic method, is not very sensitive to the general corpus size.
Data retrieval selects the top-K similar documents from the general corpus. Table 4 shows the results of different K values. We observe that high-resource tasks such as AGNews only need a small K value, while low-resource tasks such as SciREC and ChemProt require a large K to obtain the best performance. The observation is consis-

Only Task Data Top-50 Top-500 Top-5000

AGNews
93.41±0.10 94.51±0.15 94.32±0.05 94.42±0.10

SciERC
51.23±1.13 77.61±1.75 82.39±0.55 86.07±0.48

ChemProt
55.05±0.18 77.21±0.47 81.44±0.50 83.64±0.26

Table 4: Results on the development set with different values of K. The value K is the number of retrieved documents per task example. AGNews is a high-resource task, while SciREC and ChemProt are low-resource ones. Here we use ρ2 = 20 for all tasks. When there are external data available, we use ρ1 = 4 for AGNews and ρ1 = 1000 for SciERC and ChemProt.

tent with the above analysis that low-resource tasks rely more on external data to improve from joint training.
4.3.2 Language Modeling Weights ρ1 and ρ2 The hyperparameters ρ1 and ρ2 are the weights for the LM loss on external and internal data respectively. We conduct sensitivity analysis over ρ1 and ρ2. Results are shown in Table 5 and Table 6.

ρ1 =1 ρ1 =3 ρ1 =99 ρ1 =999 Ext only

Helpfulness
71.02±0.51 70.41±0.52 69.56±0.23 69.35±0.72 69.76±0.50

SciERC
80.72±3.32 80.01±0.72 84.95±0.57 86.07±0.48 85.66±1.58

ChemProt
73.27±0.30 79.43±1.03 83.30±0.30 83.64±0.26 82.50±0.27

Table 5: Results on the development set with different weights on external data (i.e., ρ1). We assign different values for ρ1 for the ﬁrst stage, and report the ﬁnal performance after two-stage joint learning. “Ext only” means using only external data for training (i.e., ρ1 = ∞). Helpfulness is a high-resource task, and the others are low-resource ones. For all tasks, we ﬁx ρ2 = 20.

For ρ1, we ﬁnd that high-resource tasks such as Helpfulness perform better with a smaller ρ1 (i.e., Helpfulness achieves best when ρ1 = 1) while low-resource tasks such as SciERC and ChemProt achieve their best when ρ1 is large (i.e., both tasks use ρ1 = 999). This is in line with conclusions in Section 4.3.1 that low-resource tasks rely more on external data. In addition, removing task data and only using external data for training (i.e., ρ1 =#CBERT), it performs worse than when incorporating the task data, proving the indispensability of small task data.
Results in Table 6 show that language modeling on internal data is necessary: consistently better

results are achieved when ρ2 is non-zero. Based on our observations, competitive performance can be achieved when ρ2 is set to a proper value between 20 and 1000.

RCT

SciERC ChemProt

ρ2 =0 ρ2 =20 ρ2 =100 ρ2 =1000

85.75±0.11 88.08±0.02 88.16±0.15 88.02±0.04

83.31±0.88 86.07±0.48 85.48±1.01 85.29±1.86

83.41±0.33 83.64±0.26 83.77±0.77 83.63±0.90

Table 6: Results on the development set with different language modeling weights on internal data (i.e., ρ2). Here we set ρ1 = 1000 for SciERC and ChemProt, and ρ1 = 4 for RCT

4.3.3 Second Stage of Training

two-stage wo/ stage-2

AGNews
94.51 93.98 ↓

IMDB
94.40 92.23↓

ChemProt
83.64 69.30↓

ACL-ARC
76.37 57.00↓

Table 7: Results on the development set of two-stage training and one-stage training (removing stage 2).
TLM contains two training stages—ﬁrst training on all three terms combined and then ﬁnetuning using only the task objective. To validate the effectiveness of the second stage of TLM, we compare the performance of two-stage training against using only stage one. Results are shown in Table 7. We ﬁnd that removing the second stage hurts the ultimate performance consistently, proving its indispensability. Particularly, the second stage has much more inﬂuence on low-resource tasks (with a huge decrease of 19.37 points on ACL-ARC and 14.34 points on ChemProt) than on high-resource tasks (with a performance decrease of 0.53 points on AGNews and 2.17 points on IMDB).
4.4 Analysis
4.4.1 Attention Weight Visualization
We also study the difference between the model behaviors of TLM and pretraining-ﬁnetuning by visualizing their attention weights. Voita et al. (2019) found that a speciﬁc kind of heads, referred to as "positional head" in which at least 90% of the maximum attention weights are assigned to adjacent tokens, have vital contributions to ﬁnal predictions of the model. Another sort of heads we are interested in are those in which most maximum attention weights are assigned to [CLS],[SEP] or the

H1 H2 H3 H4 H5 H6 H7 H8 H9 H10 H11 H12 L1 L2 L3 L4 L5 L6 L7 L8 L9 L10 L11 L12
(a) TLM (Medium scale)

H1 H2 H3 H4 H5 H6 H7 H8 H9 H10 H11 H12 L1 L2 L3 L4 L5 L6 L7 L8 L9 L10 L11 L12
(b) BERT-Base

H1 H2 H3 H4 H5 H6 H7 H8 H9 H10 H11 H12 L1 L2 L3 L4 L5 L6 L7 L8 L9 L10 L11 L12
(c) RoBERTa-Base

Figure 3: Attention visualization of TLM and pretraining-ﬁnetuning baselines, with "[CLS] crystallographic comparison with the structurally related. [SEP]" from ChemProt as the input. The positional heads (Voita et al., 2019) are highlighted in red boxes and vertical heads (Kovaleva et al., 2019) are masked in gray.

Task Hyp.

Task Data as Query
"A Republican student association at San Diego State University (SDSU) is facing backlash for sending a letter demanding Muslim students condemn last week’s terror attacks in Barcelona. ... "

Help.

Poor Quality. The case broke after dropping it on the tile ﬂoor. ...

ChemProt

FCEO signiﬁcantly inhibited « nitric oxide » (NO) and prostaglandin E2 (PGE2) by suppressing the protein expression of inducible nitric oxide synthase ([[ iNOS ]]) and cyclooxygenase (COX)-2, respectively.

SciERC

Image sequence processing techniques are used to study exchange , growth , and transport processes and to tackle key questions in [[ environmental physics ]] and « biology » .

Retrieved General Data
Example 1: "...The SDSU Aztecs intercollegiate water polo, swimming and diving teams are based at the Aztec Aquaplex..." Example 2: The Daily Aztec is a not-for-proﬁt, independent student newspaper serving San Diego State University (SDSU) and the surrounding College Area in San Diego, California. ...
Example 1: ...a collaborative algorithm will be able to recommend it, the quality of those recommendations will be poor. ... Example 2: ... Books that’re of poor quality will quickly cease to sell. ...
Example 1: ... They regulate the development of sperm by controlling their cell division and survival. Other immune factors found in the testis include the enzyme inducible nitric oxide synthase (iNOS) ... Example 2: These compounds have been shown "in vivo" to reduce two proteins that mediate inﬂammation, cyclooxygenase2 (COX-2) and inducible nitric oxide synthase (iNOS). ...
Example 1: ... Driving forces in signal processing for data parallelism are video encoding, image and graphics processing, wireless communications to name a few. Example 2: They have applications in many disciplines such as biology, chemistry, ecology, neuroscience, physics, image processing, ...

Table 8: Examples of retrieved data. The overlap between queries and retrieved data are highlighted in blue.

period token("."), which potentially encode less semantic or syntactic information (Kovaleva et al., 2019). In our experiments, if more than 90% maximum weights are assigned to [CLS], [SEP] or the period token, we categorize this head as a “vertical head”. Results in Figure 3 show that on the task ChemProt, more positional heads and less vertical heads are observed in TLM than in PLMs. We also observe similar patterns across various tasks (see Appendix C). These phenomena suggest that TLM learns different (probably more informative) attention patterns compared to PLMs.

4.4.2 Case Study of Retrieved Data
TLM retrieves relevant data from a general corpus using BM25 (Robertson and Zaragoza, 2009). Since BM25 is based on sparse features, it focuses more on lexical similarity instead of semantic similarity. This might be speciﬁcally beneﬁcial for professional domains, e.g., SciERC for computer science and ChemProt for biomedical science), since there are a large number of proper nouns in these domains. For other domains, it seems BM25 also performs reasonably well for retrieving related documents.

5 Conclusions
In this paper, we have proposed a simple, efﬁcient, pretraining-free framework, TLM. The core idea is to only use a tiny, task-relevant subset of the general corpus for language model training. Our experiments show that TLM achieves results similar to or even better than PLMs, with a reduction of training FLOPs by two orders of magnitude.
TLM opens the possibility of reducing the heavy reliance on large-scale PLMs and training a model from scratch in an efﬁcient manner, while not hurting the overall performance. We hope TLM will contribute to democratizing NLP and expediting its development by allowing most researchers to freely explore the architectures, loss functions, algorithms, and other design choices in the neighborhood of a state-of-the-art solution.
As discussed in Section 3.2, there are several potential directions for future work. It will be interesting to study how to use TLM to match the performance even larger-scale PLMs. Moreover, further extending and improving TLM for few-shot and zero-shot learning is a crucial problem.
References
Iz Beltagy, Kyle Lo, and Arman Cohan. 2019. Scibert: A pretrained language model for scientiﬁc text. In EMNLP/IJCNLP (1), pages 3613–3618. Association for Computational Linguistics.
Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. arXiv preprint arXiv:2005.14165.
Xiaohan Chen, Yu Cheng, Shuohang Wang, Zhe Gan, Zhangyang Wang, and Jingjing Liu. 2021. Earlybert: Efﬁcient bert training via early-bird lottery tickets. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers).
Kevin Clark, Minh-Thang Luong, Quoc V. Le, and Christopher D. Manning. 2020. Electra: Pretraining text encoders as discriminators rather than generators.
Franck Dernoncourt and Ji Young Lee. 2017. Pubmed 200k RCT: a dataset for sequential sentence classiﬁcation in medical abstracts. In IJCNLP(2), pages 308–313. Asian Federation of Natural Language Processing.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep

bidirectional transformers for language understanding.
Mitchell A. Gordon, Kevin Duh, and Nicholas Andrews. 2020. Compressing BERT: studying the effects of weight pruning on transfer learning. In RepL4NLP@ACL, pages 143–155. Association for Computational Linguistics.
Suchin Gururangan, Ana Marasovic, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey, and Noah A. Smith. 2020. Don’t stop pretraining: Adapt language models to domains and tasks. In ACL, pages 8342–8360. Association for Computational Linguistics.
Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. 2020. Deberta: Decoding-enhanced bert with disentangled attention.
Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, and Qun Liu. 2019. Tinybert: Distilling bert for natural language understanding. arXiv preprint arXiv:1909.10351.
David Jurgens, Srijan Kumar, Raine Hoover, Daniel A. McFarland, and Dan Jurafsky. 2018. Measuring the evolution of a scientiﬁc ﬁeld through citation frames. Trans. Assoc. Comput. Linguistics, 6:391–406.
Vladimir Karpukhin, Barlas Og˘uz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense passage retrieval for open-domain question answering. arXiv preprint arXiv:2004.04906.
Johannes Kiesel, Maria Mestre, Rishabh Shukla, Emmanuel Vincent, Payam Adineh, David P. A. Corney, Benno Stein, and Martin Potthast. 2019. Semeval2019 task 4: Hyperpartisan news detection. In SemEval@NAACL-HLT, pages 829–839. Association for Computational Linguistics.
Sehoon Kim, Amir Gholami, Zhewei Yao, Michael W. Mahoney, and Kurt Keutzer. 2021. I-BERT: integeronly BERT quantization. CoRR, abs/2101.01321.
Olga Kovaleva, Alexey Romanov, Anna Rogers, and Anna Rumshisky. 2019. Revealing the dark secrets of BERT. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 4365–4374, Hong Kong, China. Association for Computational Linguistics.
Jens Kringelum, Sonny Kim Kjærulff, Søren Brunak, Ole Lund, Tudor I. Oprea, and Olivier Taboureau. 2016. Chemprot-3.0: a global chemical biology diseases mapping. Database J. Biol. Databases Curation, 2016.
Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang. 2020. Biobert: a pre-trained biomedical language representation model for biomedical text mining. Bioinformatics, 36(4):1234–1240.

Ruiqi Lee, Zhong Kristy, Zheng Zhang, and Dan Klein. 2021. Adapting language models for zero-shot learning by meta-tuning on dataset and prompt collections.
Junlong Li, Zhuosheng Zhang, Hai Zhao, Xi Zhou, and Xiang Zhou. 2020. Task-speciﬁc objectives of pre-trained language models for dialogue adaptation. arXiv preprint arXiv:2009.04984.
Weijie Liu, Peng Zhou, Zhe Zhao, Zhiruo Wang, Haotang Deng, and Qi Ju. 2020. Fastbert: a selfdistilling BERT with adaptive inference time. CoRR, abs/2004.02178.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach.
Yi Luan, Luheng He, Mari Ostendorf, and Hannaneh Hajishirzi. 2018. Multi-task identiﬁcation of entities, relations, and coreference for scientiﬁc knowledge graph construction. In EMNLP, pages 3219– 3232. Association for Computational Linguistics.
Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. 2011. Learning word vectors for sentiment analysis. In ACL, pages 142–150. The Association for Computer Linguistics.
Julian J. McAuley, Christopher Targett, Qinfeng Shi, and Anton van den Hengel. 2015. Image-based recommendations on styles and substitutes. In SIGIR, pages 43–52. ACM.
Paul Michel, Omer Levy, and Graham Neubig. 2019. Are sixteen heads really better than one? CoRR, abs/1905.10650.
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2018. Language models are unsupervised multitask learners.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2019. Exploring the limits of transfer learning with a uniﬁed text-to-text transformer.
Stephen E. Robertson and Hugo Zaragoza. 2009. The probabilistic relevance framework: BM25 and beyond. Found. Trends Inf. Retr., 3(4):333–389.
Stuart Rose, Dave Engel, Nick Cramer, and Wendy Cowley. 2010. Automatic keyword extraction from individual documents. Text mining: applications and theory, 1:1–20.
Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2019. Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108.

Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao, Amir Gholami, Michael W. Mahoney, and Kurt Keutzer. 2019. Q-BERT: hessian based ultra low precision quantization of BERT. CoRR, abs/1909.05840.
Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. 2019. Megatron-lm: Training multi-billion parameter language models using model parallelism.
David R. So, Wojciech Man´ke, Hanxiao Liu, Zihang Dai, Noam Shazeer, and Quoc V. Le. 2021. Primer: Searching for efﬁcient transformers for language modeling.
Zhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming Yang, and Denny Zhou. 2020. Mobilebert: a compact task-agnostic BERT for resource-limited devices. CoRR, abs/2004.02984.
Yi Tay, Mostafa Dehghani, Jinfeng Rao, William Fedus, Samira Abnar, Hyung Won Chung, Sharan Narang, Dani Yogatama, Ashish Vaswani, and Donald Metzler. 2021. Scale efﬁciently: Insights from pre-training and ﬁne-tuning transformers.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need.
Elena Voita, David Talbot, Fedor Moiseev, Rico Sennrich, and Ivan Titov. 2019. Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5797–5808, Florence, Italy. Association for Computational Linguistics.
Ziheng Wang, Jeremy Wohlwend, and Tao Lei. 2019. Structured pruning of large language models. CoRR, abs/1910.04732.
Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. 2021. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652.
Adrian de Wynter and Daniel J. Perry. 2020. Optimal subarchitecture extraction for BERT. CoRR, abs/2010.10499.
Canwen Xu, Wangchunshu Zhou, Tao Ge, Furu Wei, and Ming Zhou. 2020. Bert-of-theseus: Compressing BERT by progressive module replacing. CoRR, abs/2002.02925.
Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V. Le. 2020. Xlnet: Generalized autoregressive pretraining for language understanding.

Yang You, Jing Li, Sashank Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh Bhojanapalli, Xiaodan Song, James Demmel, Kurt Keutzer, and Cho-Jui Hsieh. 2019. Large batch optimization for deep learning: Training bert in 76 minutes.
Oﬁr Zafrir, Guy Boudoukh, Peter Izsak, and Moshe Wasserblat. 2019. Q8BERT: quantized 8bit BERT. CoRR, abs/1910.06188.
Xiang Zhang, Junbo Jake Zhao, and Yann LeCun. 2015. Character-level convolutional networks for text classiﬁcation. In NIPS, pages 649–657.
Xuan Zhang, Pamela Shapiro, Gaurav Kumar, Paul McNamee, Marine Carpuat, and Kevin Duh. 2019. Curriculum learning for domain adaptation in neural machine translation. Proceedings of the 2019 Conference of the North.
A Compare to Domain Adaptation
Our work is different from domain adaptation such as Gururangan et al. (2020). While domain adaptation aims to address how to effectively adapt a pretrained LM into one domain-speciﬁc task with sufﬁcient domain data, this work targets to provide a method that is general enough to solve any task without domain data. Nevertheless, we still compare TLM with (Gururangan et al., 2020) as Table A.2 shows. We hope to ﬁgure out that, under the harsh but practical condition that no domain data is accessible, whether our proposed framework TLM can still match or even outperform the traditional domain adaptation methods with large pretrained language models as well as domain data.
From results in Table A.2, we have observations:
1. We reproduced the RoBERTa-Base results using the hyper-parameters reported by Gururangan et al. (2020) as well as our own hyperparameters. Results show that the baseline RoBERTa-Base results are underestimated in the paper with a gap of around 3 points. We list our hyper-parameters for ﬁne-tuning RoBERTa in Table A.1.
2. We also reproduced the DAPT+TAPT results using our own hyper-paraemters. Results show that DAPT+TAPT with new hyperparameters also performs slightly better than it was reported by Gururangan et al. (2020).
3. From the perspective of total training computes (FLOPs), DAPT+TAPT consumes a comparable FLOPs with TLM (large-scale), and TLM (large-scale) achieved comparable results with DAPT+TAPT (i.e., 85.70 vs

85.57). However, from the perspective of data usage, DAPT+TAPT uses large amounts of domain data, the amount of which for each domain almost equals the amount of BERT total training corpus. TLM does not rely on it.

Hyper-parameters
Epochs Training steps Patience Learning rate Batch size Max. grad. norm Weight decay

Ours
3e4
2e-5 32
0

Reported
3 or 10 3
2e-5 16 1 0.1

Table A.1: Comparison between the hyperparameters for ﬁne-tuning from our implementation and from Gururangan et al. (2020).

B Detailed Experiment Settings
Table B.1 lists the detailed hyperparameters for TLM at stage 1 of different scales for each task. At small and medium scales, for tasks with less than 5K training examples (HyperPartisan, ChemProt, SciERC, ACL-ARC), we set K = 5000; for tasks with more than 100K training examples (RCT, AGNews, Helpfulness), we set K = 50, for the rest of the tasks (IMDB), we set K = 500. At the large scale, K is doubled for each task. At each scale on every task, we conduct grid search for ρ1 ∈ {1, 3, 7, 19, 99, 499, 999, 1999} and ρ2 ∈ {20, 100, 1000}, and adjust training steps, batch size and sequence length to minimize the training cost while preserving competitive performance. We observe that for almost all the tasks, the larger the training scale, the more reliance on external data, indicated by the increasing trend of ρ1 and ρ2 as the total training tokens goes up.
C Attention visualization on other tasks
Besides ChemProt (Figure 3), we also experimented on RCT (Figure C.1) and SciERC (Figure C.2) to get attention visualizations. We ﬁnd TLM consistently contains more positional heads (in red box) and less vertical heads (in gray mask). These results reveal that the aforementioned pattern generally holds for TLM.

AGNews

Hyp.

Help.

IMDB

ACL.

SciERC

RoBERTa-Base1 RoBERTa-Base2 RoBERTa-Base3
DAPT1 DAPT+TAPT1 DAPT+TAPT3

93.90±0.20 93.97±0.13 94.02±0.15
93.90±0.20 94.60±0.10 94.07±0.07

86.60±0.90 88.50±4.18 93.53±1.61
88.20±5.90 90.00±6.60 93.59±0.00

65.10±3.40 67.45±0.49 70.45±0.24
66.50±1.40 68.70±1.80 71.44±0.99

95.00±0.20 95.43±0.07 95.43±0.16
95.40±0.10 95.60±0.10 95.65±0.14

63.00±5.80 63.87±1.24 68.34±7.27
75.40±2.50 75.60±3.80 75.62±1.77

77.30±1.90 79.97±1.29 81.35±0.63
80.80±1.50 81.30±1.80 82.06±0.90

TLM (large-scale)

94.32±0.07 95.16±0.00 72.49±0.33 95.77±0.24 72.19±1.72 83.29±0.95

1 Results reported by Gururangan et al. (2020) 2 Our reproduced results with the hyper-parameters reported by Gururangan et al. (2020) 3 Results obtained by our own hyper-parameters

Chem.
81.90±1.00 81.50±0.94 82.60±0.53
84.20±0.20 84.40±0.40 84.45±0.68
85.12±0.85

RCT
87.20±0.10 87.26±0.08 87.23±0.09
87.60±0.10 87.80±0.10 87.67±0.11
87.50±0.12

Avg.
81.25 82.24 84.12
84.00 84.75 85.57
85.74

Table A.2: Comparison results of TLM and Gururangan et al. (2020).

Hyper-Parameters AGNews Hyp. Help. IMDB ACL. SciERC Chem. RCT

Small Scale

Top-K ρ1 ρ2 Source Corpus2 Training Data Size3 Training Steps Batch Size
Sequence Length

50 1 100 CBERT 1.1GB 1E5 256 128

5000 99 20
CBERT 0.2GB
5E4 256 128

50 1 100 CBERT 0.5GB 1.5E5 256 128

500 19 100 CBERT 0.9GB 1.5E5 256 1281

5000 999 100 CBERT 1.5GB 1.5E5 256 128

5000 999 20 CBERT 1.6GB 1.5E5 256 128

5000 999 20 CBERT 0.7GB 1.5E5 256 128

50 3 20 CBERT 0.8GB 1E5 256 128

Medium Scale

Top-K ρ1 ρ2 Source Corpus2 Training Data Size3
Training Steps
Batch Size
Sequence Length

50 3 100 CBERT 1.1GB 3E5 256 128

5000 99 100
CBERT 0.2GB
1E5 256 128

50 1 1000 CBERT 0.5GB 3E5 256 128

500 99 100 CBERT 3.3GB 3E5 256 512

5000 999 20 CBERT 1.5GB 3E5 256 128

5000 999 20 CBERT 1.6GB 3E5 256 128

5000 999 100 CBERT 0.7GB 3E5 256 128

50 3 100 CBERT 0.8GB 3E5 256 128

Large Scale

Top-K ρ1 ρ2 Source Corpus2 Training Data Size3
Training Steps
Batch Size
Sequence Length

100 3
100 CRoBERTa 3.1GB
5E5 256 128

10000 499 20
CRoBERTa 0.9GB
3E5 512 128

100 7
100 CRoBERTa 1.7GB
5E5 512 128

1000 99
1000 CRoBERTa 11GB
5E5 512 512

10000 1999 20 CRoBERTa 3.5GB 5E5 512 128

10000 1999 20 CRoBERTa 4.2GB 3E5 512 128

10000 1999 20 CRoBERTa 2.5GB 5E5 256 128

100 7 100 CRoBERTa 2.2GB 5E5 256 128

1 At a small scale on IMDB, we use a sequence length of 512 for internal data and a sequence length of 128 for external data. 2 CBERT and CRoBERTa are our collected corpus that respectively match the original training corpus of BERT and RoBERTa. 3 TLM only uses a tiny subset of the source general corpus for training. We list the data size that are actually used for TLM
training.

Table B.1: Detailed hyper-parameters for TLM of different scales for each task.

H1 H2 H3 H4 H5 H6 H7 H8 H9 H10 H11 H12 L1 L2 L3 L4 L5 L6 L7 L8 L9 L10 L11 L12
(a) TLM (Medium scale)

H1 H2 H3 H4 H5 H6 H7 H8 H9 H10 H11 H12 L1 L2 L3 L4 L5 L6 L7 L8 L9 L10 L11 L12
(b) BERT-Base

H1 H2 H3 H4 H5 H6 H7 H8 H9 H10 H11 H12 L1 L2 L3 L4 L5 L6 L7 L8 L9 L10 L11 L12
(c) RoBERTa-Base

Figure C.1: task: RCT ; input: "[CLS] twenty-eight individuals from outpatient physiotherapy departments were randomized. [SEP]"

H1 H2 H3 H4 H5 H6 H7 H8 H9 H10 H11 H12 L1 L2 L3 L4 L5 L6 L7 L8 L9 L10 L11 L12
(a) TLM

H1 H2 H3 H4 H5 H6 H7 H8 H9 H10 H11 H12 L1 L2 L3 L4 L5 L6 L7 L8 L9 L10 L11 L12
(b) BERT-Base

H1 H2 H3 H4 H5 H6 H7 H8 H9 H10 H11 H12 L1 L2 L3 L4 L5 L6 L7 L8 L9 L10 L11 L12
(c) RoBERTa-Base

Figure C.2: task: SciERC ; input: "[CLS] multi-view constraints associated with groups of patches are combined. [SEP]"

