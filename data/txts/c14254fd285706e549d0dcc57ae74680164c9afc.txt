arXiv:1703.09842v3 [cs.LG] 21 Nov 2017

1
Inverse Risk-Sensitive Reinforcement Learning
Lillian J. Ratliff and Eric Mazumdar
Abstract
We address the problem of inverse reinforcement learning in Markov decision processes where the agent is risk-sensitive. We derive a risk-sensitive reinforcement learning algorithm with convergence guarantees that employs convex risk metrics and models of human decisionmaking deriving from behavioral economics. The risk-sensitive reinforcement learning algorithm provides the theoretical underpinning for a gradient-based inverse reinforcement learning algorithm that minimizes a loss function deﬁned on observed behavior of a risk-sensitive agent. We demonstrate the performance of the proposed technique on two examples: (i) the canonical Grid World example and (ii) a Markov decision process modeling ride-sharing passengers’ decisions given price changes. In the latter, we use pricing and travel time data from a ride-sharing company to construct the transition probabilities and rewards of the Markov decision process.
I. INTRODUCTION
T HE modeling and learning of human decision-making behavior is increasingly becoming important as critical systems begin to rely more on automation and artiﬁcial intelligence. Yet, in this task we face a number of challenges, not least of which is the fact that humans are known to behave in ways that are not completely rational. There is mounting evidence to support the fact that humans often use reference points—e.g., the status quo or former experiences or recent expectaions about the future that are otherwise perceived to be related to the decision the human is making [1], [2]. It has also been observed that their decisions are impacted by their perception of the external world (exogenous factors) and their present state of mind (endogenous factors) as well as how the decision is framed or presented [3].
The success of descriptive behavioral models in capturing human behavior has long been touted by the psychology community and, more recently, by the economics community. In the engineering context, humans have largely been modeled, under rationality assumptions, from the so-called normative point of view where things are modeled as they ought to be, which is counter to a descriptive as is point of view.
However, risk-sensitivity in the context of learning to control stochastic dynamical systems (see, e.g., [4], [5]) has been fairly extensively explored in engineering. Many of these approaches are targeted at mitigating risks due to uncertainties in controlling a system such as a plant or robot where risk-aversion is captured by leveraging techniques such as exponential utility functions or minimizing mean-variance criteria.
Complex risk-sensitive behavior arising from human interaction with automation is only recently coming into focus. Human decision makers can be at once risk-averse and risk-seeking
L. Ratliff is with the Department of Electrical Engineering, University of Washington, Seattle, WA, 98185. email: ratliffl@uw.edu.
E. Mazumdar is with the Department of Electrical Engineering and Computer Sciences at the University of California, Berkeley, Berkeley, CA 94720. email: mazumdar@berkeley.edu

2
depending their frame of reference. The adoption of diverse behavioral models in engineering— in particular, in learning and control—is growing due to the fact that humans are increasingly playing an integral role in automation both at the individual and societal scale. Learning accurate models of human decision-making is important for both prediction and description. For example, control/incentive schemes need to predict human behavior as a function of external stimuli including not only potential disturbances but also the control/incentive mechanism itself. On the other hand, policy makers and regulatory agencies, e.g., are interested in interpreting human reactions to implemented regulations and policies.
Approaches for integrating the risk-sensitivity in the control and reinforcement learning problems via behavioral models have recently emerged [6]–[10]. These approaches largely assume a risk-sensitive Markov decision process (MDP) formulated based on a model that captures behavioral aspects of the human’s decision-making process. We refer the problem of learning the optimal policy in this setting as the forward problem. Our primary interest is in solving the so-called inverse problem which seeks to estimate the decision-making process given a set of demonstrations; yet, to do so requires a well formulated forward problem with convergence guarantees.
Inverse reinforcement learning in the context of recovering policies directly (or indirectly via ﬁrst learning a representation for the reward) has long been studied in the context expected utility maximization and MDPs [11]–[13]. We may care about, e.g., producing the value and reward functions (or at least, characterize the space of these functions) that produce behaviors matching that which is observed. On the other hand, we may want to extract the optimal policy from a set of demonstrations so that we can reproduce the behavior in support of, e.g., designing incentives or control policies. In this paper, our focus is on the combination of these two tasks.
We model human decision-makers as risk-sensitive Q-learning agents where we exploit very rich behavioral models from behavioral psychology and economics that capture a whole spectrum of risk-sensitive behaviors and loss aversion. We ﬁrst derive a reinforcement learning algorithm that leverages convex risk metrics and behavioral value functions. We provide convergence guarantees via a contraction mapping argument. In comparison to previous work in this area [14], we show that the behavioral value functions we introduce satisfy the assumptions of our theorems.
Given the forward risk-sensitive reinforcement learning algorithm, we propose a gradient-based learning algorithm for inferring the decision-making model parameters from demonstrations— that is, we propose a framework for solving the inverse risk-sensitive reinforcement learning problem with theoretical guarantees. We show that the gradient of the loss function with respect to the model parameters is well-deﬁned and computable via a contraction map argument. We demonstrate the efﬁcacy of the learning scheme on the canonical Grid World example and a passenger’s view of ride-sharing modeled as an MDP with parameters estimated from real-world data.
The work in this paper signiﬁcantly extends our previous work [15] ﬁrst, by providing the proofs for the theoretical results appearing in the earlier work and second, by providing a more extensive theory for both the forward and inverse risk-sensitive reinforcement problems.
The remainder of this paper is organized as follows. In Section II, we overview the model we assume for risk-sensitive agents, show that it is amenable to integration with the behavioral models, and present our risk-sensitive Q-learning convergence results. In Section III, we formulate the inverse reinforcement learning problem and propose a gradient–based algorithm to solve it. Examples that demonstrate the ability of the proposed scheme to capture a wide breadth of risk-

3

sensitive behaviors are provided in Section IV. We comment on connections to recent related work in Section V. Finally, we conclude with some discussion in Section VI.
II. RISK-SENSITIVE REINFORCEMENT LEARNING
In order to learn a decision-making model for an agent who faces sequential decisions in an uncertain environment, we leverage a risk-sensitive Q-learning model that integrates coherent risk metrics with behavioral models. In particular, the model we use is based on a model ﬁrst introduced in [16] and later reﬁned in [7], [8].
The primary difference between the work presented in this section and previous work1 is that we (i) introduce a new prospect theory based value function and (ii) provide a convergence theorem whose assumptions are satisﬁed for the behavioral models we use. Under the assumption that the agent is making decisions according to this model, in the sequel we formulate a gradient–based method for learning the policy as well as parameters of the agent’s value function.

A. Markov Decision Process

We consider a class of ﬁnite MDPs consisting of a state space X, an admissible action space A(x) ⊂ A for each x ∈ X, a transition kernel P(x |x, a) that denotes the probability of moving from state x to x given action a, and a reward function2 r : X × A ×W → R where W is the space of bounded disturbances and has distribution Pr(·|x, a). Including disturbances allows us to model random rewards; we use the notation R(x , a) to denote the random reward having distribution Pr(·|x, a).
In the classical expected utility maximization framework, the agent seeks to maximize the expected discounted rewards by selecting a Markov policy π—that is, for an inﬁnite horizon MDP, the optimal policy is obtained by maximizing

J(x0) = maxπ E [

∞ t=1

γ

t

R

(xt

,

at

)]

(1)

where x0 is the initial state and γ ∈ (0, 1) is the discount factor. The risk-sensitive reinforcement learning problem transforms the above problem to account for
a salient features of the human decision-making process such as loss aversion, reference point dependence, and risk-sensitivity. Speciﬁcally, we introduce two key components, value functions and valuation functions, that allow for our model to capture these features. The former captures risk-sensitivity, loss-aversion, and reference point dependence in its transformation of outcome values to their value as perceived by the agent and the latter generalizes the expectation operator to more general measures of risk—speciﬁcally, convex risk measures.

B. Value Functions
Given the environmental and reward uncertainties, we model the outcome of each action as a real-valued random variable Y (i) ∈ R, i ∈ I where I denotes a ﬁnite event space and Y is the outcome of i–th event with probability µ(i) where µ ∈ ∆(I), the space of probability distributions
1For further details on the relationship the work in this paper and related works, including our previous work, see Section V.
2We note that it is possible to consider the more general reward structure r : X × A × X × W → R, however we exclude this case in order to not further bog down the notation.

4

on I. Analogous to the expected utility framework, agents make choices based on the value of the outcome determined by a value function v : R → R.
There are a number of existing approaches to deﬁning value functions that capture risksensitivity and loss aversion. These approaches derive from a variety of ﬁelds including behavioral psychology/economics, mathematical ﬁnance, and even neuroscience.
One of the principal features of human decision-making is that losses are perceived more signiﬁcant than a gain of equal true value. The models with the greatest efﬁcacy in capturing this effect are convex and concave in different regions of the outcome space. Prospect theory, e.g., is built on one such model [17], [18]. The value function most commonly used in prospect theory is given by
v(y) = ® k+(y − yo)ζ+, y > yo (2) −k−(yo − y)ζ−, y ≤ yo

where yo is the reference point that the decision-maker compares outcomes against in determining if the decision is a loss or gain. The parameters (k+, k−, ζ+, ζ−) control the degree of loss-aversion and risk-sensitivity; e.g.,

(i) 0 < ζ+, ζ− < 1 implies preferences that are risk-averse on gains and risk-seeking on losses (concave in gains, convex in losses);
(ii) ζ+ = ζ− = 1 implies risk-neutral preferences; (iii) ζ+, ζ− > 1 implies preferences that are risk-averse on losses and risk-seeking on gains
(convex in gains, concave in losses).

Experimental results for a series of one-off decisions have indicated that typically 0 < ζ+, ζ− < 1 thereby indicating that humans are risk-averse on gains and risk-seeking on losses.
In addition to the non-linear transformation of outcome values, in prospect theory the effect of under/over-weighting the likelihood of events that has been commonly observed in human behavior is modeled via warping of event probabilities [19]. Other concepts such as framing, reference dependence, and loss aversion—captured, e.g., in the (k+, k−) parameters in (2)—have also been widely observed in experimental studies (see, e.g., [20]–[22]).
Outside of the prospect theory value function, other mappings have been proposed to capture risk-sensitivity. Proposed in [8], the linear mapping

®

v(y) = (1 − κ)y, y > yo

(3)

(1 + κ)y, y ≤ yo

with κ ∈ (−1, 1) is one such example. This value function can be viewed as a special case of (2).
Another example is the entropic map which is given by

v(y) = exp(λ y)

(4)

where λ controls the degree of risk-sensitivity. The entropic map, however, is either convex or concave on the entire outcome space.
Motivated by the empirical evidence supporting the prospect theoretic value function and numerical considerations of our algorithm, we introduce a value function that retains the shape of the prospect theory value function while improving the performance (in terms of convergence speed) of the forward and inverse reinforcement learning procedures we propose. In particular, we deﬁne the locally Lipschitz-prospect ( -prospect) value function given by
v(y) = ® k+(y − yo + )ζ+ − k+ ζ+, y > yo (5) −k−(yo − y + )ζ− + k− ζ− , y ≤ yo

5

with k+, k−, ζ+, ζ− > 0 and > 0, a small constant. This value function is Lipschitz continuous on a bounded domain. Moreover, the derivative of the -prospect function is bounded away from zero at the reference point. Hence, in practice it has better numerical properties.
We remark that, for given parameters (k+, k−, ζ+, ζ−), the -prospect function has the same risk-sensitivity as the prospect value function with those same parameters. Moreover, as → 0 the -prospect value function approaches the prospect value function and thus, qualitatively speaking, the degree of Lipschitzness decreases as → 0.
The fact that each of these value functions are deﬁned by a small number of parameters that are highly interpretable in terms of risk-sensitivity and loss-aversion is one of the motivating factors for integrating them into a reinforcement learning framework. It is our aim to design learning algorithms that will ultimately provide the theoretical underpinnings for designing incentives and control policies taking into consideration salient features of human decision-making behavior.

C. Valuation Functions via Convex Risk Metrics
To further capture risk-sensitivity, valuation functions generalize the expectation operator, which considers average or expected outcomes,3 to measures of risk.
Deﬁnition 1 (Monetary Risk Measure [23]): A functional ρ : X → R ∪ {+∞} on the space X of measurable functions deﬁned on a probability space (Ω, F, P) is said to be a monetary risk measure if ρ(0) is ﬁnite and if, for all X, X ∈ X, ρ satisﬁes the following:
1) (monotone) X ≤ X =⇒ ρ(X) ≤ ρ(X ) 2) (translation invariant) m ∈ R =⇒ ρ(X + m) = ρ(X) + m
If a monetary risk measure ρ satisﬁes

ρ(λ X + (1 − λ )X ) ≤ λ ρ(X) + (1 − λ )ρ(X ),

(6)

for λ ∈ [0, 1], then it is a convex risk measure. If, additionally, ρ is positive homogeneous, i.e. if λ ≥ 0, then ρ(λ X) = λ ρ(X), then we call ρ a coherent risk measure. While the results apply to coherent risk measures, we will primarily focus on convex measures of risk, a less restrictive class, that are generated by a set of acceptable positions.
Denote the space of probability measures on (Ω, F) by M1(Ω, F). Deﬁnition 2 (Acceptable Positions): Consider a value function v, a probability measure P ∈ M1(Ω, F), and an acceptance level v0 = v(y¯) with y¯ in the domain of v. The set

A = {X ∈ X| EP[v(X)] ≥ v0}.

(7)

is the set of acceptable positions.

The above deﬁnition can be extending to the entire class of probability measures on (Ω, F) as

follows:

A = ∩P∈M1(Ω,F){X ∈ X| EP[v(X )] ≥ v(yP)}

(8)

with constants yP such that supP∈M1(Ω,F) yP < ∞. Proposition 1 ( [23, Proposition 4.7]): Suppose the class of acceptable positions A is a non-
empty subset of X satisfying
1) inf{m ∈ R|X + m ∈ A} > −∞, ∀ X ∈ X, and 2) given X ∈ A,Y ∈ X, Y ≥ X =⇒ Y ∈ A.

3In the case of two events, the valuation function can also capture warping of probabilities. Alternative approaches to reinforcement learning based on cumulative prospect theory for the more general case have been examined [6].

6

Then, A induces a monetary measure of risk ρA. If A is convex, then ρA is a convex measure

of risk. Furthermore, if A is a cone, then ρA is a coherent risk metric.

Note that a monetary measure of risk induced by a set of acceptable positions A ⊂ X is given

by

ρA(X) = inf{z ∈ R| z + X ∈ A}.

(9)

The following proposition is key for extending the expectation operator to more general measures of risk.
Proposition 2 ( [23, Proposition 4.104]): Consider

A = {X ∈ X| EP[v(X)] ≥ v0}

(10)

for a continuous value function v, acceptance level v0 = v(y¯) for some y¯ in the domain of v, and

probability measure P. Suppose that v is strictly increasing on (y¯ − ε, ∞) for some ε > 0. Then,

the corresponding ρA is a convex measure of risk which is continuous from below. Moreover,

ρA(X) is the unique solution to

EP[v(X − m)] = v0.

(11)

Proposition 2 also implies that for each value function, we can deﬁne an acceptance set which

in turn induces a convex risk metric ρ. Let us consider an example.

Example 1 (Entropic Risk Metric [23]): Consider the entropic value function v(y) = exp(λ y).

It has been used extensively in the ﬁeld of risk measures [23], in neuroscience to capture risk

sensitivity in motor control [9] and even more so in control of MDPs (see, e.g., [24]).

The entropic value function with an acceptance level v0 can be used to deﬁne the acceptance

set

A = {m ∈ R| E[exp(−λ (m +Y ))] ≤ v0}.

(12)

with corresponding risk metric

ρA(X) = inf{m ∈ R| E[exp(−λ (m +Y ))] ≤ v0}

(13)

= λ1 log E[exp(−λY )] − λ1 log(v0). (14)

The parameter λ ∈ R controls the risk preference; indeed, this can be seen by considering the Taylor expansion [23, Example 4.105].
As a further comment, this particular risk metric is equivalent (up to an additive constant) to the so called entropic risk measure which is given by

Å

1

ã

ρ(Y ) = sup EP [−Y ] − H(P |P)

(15)

P ∈M1(P)

λ

where M1(P) is the set of all measures on (Ω, F) that are absolutely continuous with respect to P and where H(·|·) is the relative entropy function.
Let us recall the concept of a valuation function introduced and used in [7], [23], [25]. Deﬁnition 3 (Valuation Function): A mapping V : R|I| × ∆(I) → R is called a valuation function if for each µ ∈ ∆(I), (i) V(Y, µ) ≤ V(Z, µ) whenever Y ≤ Z (monotonic) and (ii) V(Y + y1, µ) = V(Y, µ) + y for any y ∈ R (translation invariant). Such a map is used to characterize an agent’s preferences—that is, one prefers (Y, µ) to (Z, ν) whenever V(Z, ν) ≤ V(Y, µ).

7

We will consider valuation functions that are convex risk metrics induced by a value function v and a probability measure µ. To simplify notation, from here on out we will suppress the dependence on the probability measure µ.
For each state–action pair, we deﬁne V(Y |x, a) : R|I| × X × A → R a valuation map such that Vx,a ≡ V(·|x, a) is a valuation function induced by an acceptance set with respect to value function v and acceptance level v0.
If we let Vπx (Y ) = a∈A(x) π(a|x)Vx,a(Y ), the optimization problem in (1) generalizes to
J˜T (π, x0) =Vπx00 R[x0, a0] + γVπx11îR[x1, a1] + · · · + γVπxTT [R(xT , aT )] · · · ó (16)
where we deﬁne maxπ J˜(π, x0) = limT→∞ J˜T (π, x0).

D. Risk-Sensitive Q-Learning Convergence

In the classical reinforcement learning framework, the Bellman equation is used to derive a

Q-learning procedure. Generalizations of the Bellman equation for risk-sensitive reinforcement

learning—derived, e.g., in [8], [14]—have been used to formulate an action–value function or Q-

learning procedure for the risk-sensitive reinforcement learning problem. In particular, as shown

in [14], if V ∗ satisﬁes

V ∗(x0) = maxa∈A(x) Vx,a(R(x, a) + γV ∗),

(17)

then V ∗ = maxπ J˜(π, x0) holds for all x0 ∈ X; moreover, a deterministic policy is optimal if π∗(x) = arg maxa∈A(x) Vx,a(R + γV ∗) [14, Thm. 5.5]. The action–value function Q∗(x, a) = Vx,a(R + γV ∗)
is deﬁned such that (17) becomes

Q∗(x,

a)

=

Vx,a

Ä R

+

γ

maxa∈A(x

)

Q∗(x

,

ä a)

,

(18)

for all (x, a) ∈ X × A. Given a value function v and acceptance level v0, we use the coherent risk metric induced
state-action valuation function given by

Vx,a(Y ) = sup{z ∈ R| E[v(Y − z)] ≥ v0}

(19)

where the expectation is taken with respect to µ = P(x |x, a)Pr(w|x, a). Hence, by a direct application of Proposition 2, if v is continuous and strictly increasing, then Vx,a(Y ) = z∗(x, a) is the unique solution to E[v(Y − z∗(x, a))] = v0.
As shown in [7, Proposition 3.1], by letting Y = R + γV ∗, we have that z∗(x, a) corresponds to
Q∗(x, a) and, in particular,

ñÇ

åô

E v r(x, a, w) + γ max Q∗(x , a ) − Q∗(x, a) = v0

(20)

a ∈A(x )

where, again, the expectation is taken with respect to µ = P(x |x, a)Pr(w|x, a). The above leads naturally to a Q-learning procedure,

î

ó

Q(xt , at ) ←Q(xt , at ) + αt (xt , at ) v(yt ) − v0 ,

(21)

where the non-linear transformation v is applied to the temporal difference

yt = rt + γ max Q(xt+1, a) − Q(xt , at )
a

8

instead of simply the reward rt. Transformation of the temporal differences avoids certain pitfalls

of the reward transformation approach such as poor convergence performance. This procedure

has convergence guarantees even in this more general setting under some assumptions on the

value function v.

Theorem 1 (Q-learning Convergence [7, Theorem 3.2]): Suppose that v : Y → R is in C(Y, R),

is strictly increasing in y and there exists constants ε, L > 0 such that ε ≤ v(yy)−−yv(y ) ≤ L for all

y = y . Moreover, suppose that there exists a y¯ such that v(y¯) = v0. If the non-negative learning

rates αt(x, a) are such that

∞ t=0

αt

(x

,

a

)

=

∞

and

∞ t=0

αt2

(x

,

a)

<

∞,

∀(x, a)

∈

X

× A,

then

the

procedure in (21) converges to Q∗(x, a) for all (x, a) ∈ X × A with probability one.

The assumptions on αt are fairly standard and the core of the convergence proof is based on the

Robbins–Siegmund Theorem appearing in the seminal work [26].

We note that the assumptions on the value function v of Theorem 1 are fairly restrictive,

excluding many of the value functions presented in Section II-B. For example, value functions of the form ex and xζ do not satisfy the global Lipschitz condition.

We generalize the convergence result in Theorem 1 by modifying the assumptions on the value

function v to ensure that we have convergence of the Q-learning procedure for the -prospect

and entropic value functions. Assumption 1: The value function v ∈ C1(Y, R) satisﬁes the following:

(i) it is strictly increasing in y and there exists a y¯ such that v(y¯) = v0;

(ii) it is locally Lipschitz on any ball of ﬁnite radius centered at the origin;

Note that in comparison to the assumptions of Theorem 1, we have removed the assumption that

the derivative of v is bounded away from zero, and relaxed the global Lipschitz assumption on

v. We remark that the -prospect and entropic value functions satisfy these assumptions for all

parameters and MDPs.

Let X be a complete metric space endowed with the L∞ norm and let Q ⊂ X be the space of maps Q : X × A → R. Further, deﬁne v˜ ≡ v − v0. We then re-write the Q–update equation in the form

Å αt ã

αt Ä

ä

Qt+1(x, a) = 1 − Qt (x, a) + α(v(yt ) − v0) + Qt (x, a)

(22)

α

α

where α ∈ (0, min{L−1, 1}] and we have suppressed the dependence of αt on (x, a). This is a standard update equation form in, e.g., the stochastic approximation algorithm literature [27]–[29].

In addition, we deﬁne the map given by

îÄ

äó

(T Q)(x, a) =αEx ,w v˜ r(x, a, w) + γ max Q(x , a ) − Q(x, a) + Q(x, a)

(23)

a ∈A

which we will prove is a contraction. Theorem 2: Suppose that v satisﬁes Assumption 1 and that for each (x, a) ∈ X × A the reward
r(x, a, w) is bounded almost surely—that is, there exists 0 < M < ∞ such that |r| < M almost surely. Moreover, let α ∈ (0, min{1, L−1}], for L, the Lipschitz constant of v on BK(0).
(a) Let BK(0) ⊂ Q be a closed ball of radius K > 0 centered at zero. Then, T : Q → X is a contraction.
(b) Suppose K is chosen such that

max{|v˜(M)|, |v˜(−M)|}

< K min Dv˜(y)

(24)

(1 − γ)

y∈IK

where IK = [−M − K, M + K]. Then, T has a unique ﬁxed point in BK(0).

9

The proof of the above theorem is provided in Appendix A. The following proposition shows that the -prospect and entropic value functions satisfy the
assumption in (24). Moreover, it shows that the value functions which satisfy Assumption 1 also satisfy (24).
Proposition 3: Consider a MDP with reward r : X × A ×W → R bounded almost surely by M and γ ∈ (0, 1) and consider the condition

max{|v˜(M)|, |v˜(−M)|}

< K min Dv˜(y).

(25)

(1 − γ)

y∈IK

1) Suppose v satisﬁes Assumption 1 and that for some ε > 0, ε < v(yy)−−yv(y ) for all y = y . Then (25) holds.

2) Suppose v is an -prospect value function with arbitrary parameters (k−, k+, ζ−, ζ+) satisfying Assumption 1. Then there exists a K such that the -prospect value function satisﬁes (25).

3) Suppose that v is an entropic value function. Then there exists a C > 0 such that for any |λ | ∈ (0,C) where v satisﬁes Assumption 1, (25) holds with K = (λ )−1.

With Theorem 2 and Proposition 3, we can prove convergence of Q-learning for risk-sensitive

reinforcement learning.

Theorem 3 (Q-learning Convergence on BK(0)): Suppose that v satisﬁes Assumption 1 and that for each (x, a) ∈ X × A the reward r(x, a, w) is bounded almost surely—that is, there exists

0 < M < ∞ such that |r| < M almost surely. Moreover, suppose the ball BK(0) is chosen such

that (24) holds. If the non-negative learning rates αt(x, a) are such that

∞ t=0

αt

(x

,

a)

=

∞

and

∞ t=0

αt2

(x

,

a)

<

∞,

∀(x, a)

∈

X

× A,

then

the

procedure

in

(21)

converges

to

Q∗

∈

BK (0)

with

probability one.

Given Theorem 2, the proof of Theorem 3 follows directly the same proof as provided in [14]. The

key aspect of the proof is combining the ﬁxed point result of Theorem 2 with Robbins–Siegmund

Theorem [26].

Theorem 2, Proposition 3, and Theorem 3 extend the results for risk-sensitive reinforcement

learning presented in [14] by relaxing the assumptions on the value functions for which the

Q-learning procedure converges.

III. INVERSE RISK-SENSITIVE REINFORCEMENT LEARNING

We formulate the inverse risk-sensitive reinforcement learning problem as follows. First, we select a parametric class of policies, {πθ }θ , πθ ∈ Π and parametric value function {vθ }θ , vθ ∈ F where F is a family of value functions and θ ∈ Θ ⊂ Rd.
We use value functions such as those described in Section II-B; e.g., if v is the prospect theory value function deﬁned in (2), then the parameter vector is θ = (k−, k+, ζ−, ζ+, γ, β ). For mappings v and Q, we now indicate their dependence on θ —that is, we will write Q(x, a, θ ) and vθ (y) = v(y, θ ) where v : Y × Θ → R. Note that since y is the temporal difference it also depends on θ and we will indicate this dependence where it is not directly obvious by writing y(θ ).
It is common in the inverse reinforcement learning literature to adopt a smooth map G
that operates on the action-value function space for deﬁning the parametric policy space—e.g.,
Boltzmann policies of the form

exp(β Q(x, a, θ ))

Gθ (Q)(a|x) = a ∈A exp(β Q(x, a , θ ))

(26)

10

to the action-value functions Q where β > 0 controls how close Gθ (Q) is to a greedy policy which we deﬁne to be any policy π such that a∈A π(a|x)Q(x, a, θ ) = maxa∈A Q(x, a, θ ) at all states x ∈ X. We will utilize policies of this form. Note that, as is pointed out in [30], the beneﬁt
of selecting strictly stochastic policies is that if the true agent’s policy is deterministic, uniqueness
of the solution is forced. We aim to tune the parameters so as to minimize some loss (πθ ) which is a function of the
parameterized policy πθ . By an abuse of notation, we introduce the shorthand (θ ) = (πθ ).

A. Inverse Reinforcement Learning Optimization Problem

The optimization problem is speciﬁed by

min { (θ )| πθ = Gθ (Q∗), vθ ∈ F}

(27)

θ ∈Θ

Given a set of demonstrations D = {(xk, ak)}Nk=1, it is our goal to recover the policy and estimate the value function.

There are several possible loss functions that may be employed. For example, suppose we elect

to minimize the negative weighted log-likelihood of the demonstrated behavior which is given

by

(θ ) = (x,a)∈D w(x, a) log(πθ (x, a))

(28)

where w(x, a) may, e.g., be the normalized empirical frequency of observing (x, a) pairs in D, i.e. n(x, a)/N where n(x, a) is the frequency of (x, a).
Related to maximizing the log-likelihood, an alternative loss function is the relative entropy or Kullback-Leibler (KL) divergence between the empirical distribution of the state-action trajectories and their distribution under the learned policy—that is,

(θ ) = x∈Dx DKL(πˆ (·|x)||πθ (·|x))

(29)

where

DKL(P||Q) = i P(i) log (P(i)/Q(i))

(30)

is the KL divergence, Dx ⊂ D is the sequence of observed states, and πˆ is the empirical distribution on the trajectories of D.

B. Gradient–Based Approach

We propose to solve the problem of estimating the parameters of the agent’s value function and approximating the agent’s policy via gradient methods which requires computing the derivative of Q∗(x, a, θ ) with respect to θ . Hence, given the form of the Q-learning procedure where the temporal differences are transformed as in (21), we need to derive a mechanism for obtaining the optimal Q, show that it is in fact differentiable, and derive a procedure for obtaining the derivative.
Using some basic calculus, given the form of smoothing map Gθ in (26), we can compute the derivative of the policy πθ with respect to θk for an element of θ ∈ Θ:

Dθk πθ (a|x) = πθ (a|x)Dθk ln(πθ (a|x))

(31)

= πθ (a|x)β ÄDθk Q∗(x, a, θ ) − a ∈A πθ (a |x)Dθk Q∗(x, a , θ )ä. (32)

11

We show that Dθk Q∗θ can be calculated almost everywhere on Θ by solving ﬁxed-point equations similar to the Bellman-optimality equations.
To do this, we require some assumptions on the value function v. Assumption 2: The value function v ∈ C1(Y × Θ, R) satisﬁes the following conditions:
(i) v is strictly increasing in y and for each θ ∈ Θ, there exists a y¯ such that v(y¯, θ ) = v0; (ii) for each θ ∈ Θ, on any ball centered around the origin of ﬁnite radius, v is locally Lipschitz
in y with constant Ly(θ ) and locally Lipschitz on Θ with constant Lθ ; (iii) there exists ε > 0 such that ε ≤ v(y,θy)−−vy(y ,θ) for all y = y .
Deﬁne Ly = maxθ Ly(θ ) and L = maxθ {Ly(θ ), Lθ }. As before, let v˜ ≡ v − v0. We re-write the Q–update equation as

Qt+1(x, a, θ ) = Ä1 − ααt ä Qt (x, a, θ ) + ααt Äα(v(yt (θ ), θ ) − v0) + Qt (x, a, θ )ä (33)

where

yt (θ ) = rt + γ max Qt (xt+1, a, θ ) − Qt (xt , at , θ )
a

is the temporal difference, α ∈ (0, min{L−1, 1}] and we have suppressed the dependence of αt on (x, a). In addition, deﬁne the map T such that

(T Q)(x, a, θ ) = αEx ,wv˜(y(θ ), θ ) + Q(x, a, θ )

(34)

where y(θ ) = r(x, a, w) + γ maxa ∈A Q(x , a , θ ) − Q(x, a, θ ). This map is a contraction for each θ . Indeed, ﬁxing θ , when v satisﬁes Assumption 2, then for cases where v0 = 0, T was shown to be a contraction in [8] and in the more general setting (i.e. v0 = 0), in [7].
Our ﬁrst main result on inverse risk-sensitive reinforcement learning, which is the theoretical
underpinning of our gradient-based algorithm, gives us a mechanism to compute the derivative of Q∗θ with respect to θ as a solution to a ﬁxed-point equation via a contraction mapping argument.
Let Div˜(·, ·) be the derivative of v˜ with respect to the i–th argument where i = 1, 2. Theorem 4: Assume that v ∈ C1(Y ×Θ, R) satisﬁes Assumption 2. Then the following statements
hold:
(a) Q∗θ is locally Lipschitz continuous as a function of θ —that is, for any (x, a) ∈ X ×A, θ , θ ∈ Θ, |Q∗(x, a, θ ) − Q∗(x, a, θ )| ≤ C θ − θ for some C > 0;
(b) except on a set of measure zero, the gradient Dθ Q∗θ is given by the solution of the ﬁxed–point equation

φθ

(x,

a)

=α

Ex

î ,w D2v˜(y(θ

),

θ

)

+

D1v˜(y(θ

),

θ

)(γ

φθ

(x

,

a∗x

)

−

φθ

(x,

ó a))

+

φθ

(x,

a)

(35)

where φθ : X × A → Rd and a∗x is the action that maximizes π is any policy that is greedy with respect to Qθ .

a ∈A π(a|x )Q(x , a, θ ) where

We provide the proof in Appendix C. To give a high-level outline, we use an induction argument

combined with a contraction mapping argument on the map

(Sφθ

)(x,

a)

=α

Ex

î ,w D2v˜(y(θ

),

θ

)

+

D1v˜(y(θ

),

θ

)(γ

φθ

(x

,

a∗x

)

−

φθ

(x,

ó a))

+

φθ

(x,

a).

(36)

The almost everywhere differentiability follows from Rademacher’s Theorem (see, e.g., [31,
Thm. 3.1]).
Theorem 4 gives us a procedure—namely, a ﬁxed–point equation which is a contraction— to compute the derivative Dθk Q∗ so that we can compute the derivative of our loss function

12

Algorithm 1 Gradient-Based Risk-Sensitive IRL

1: procedure RISKIRL(D)

2: Initialize: θ ← θ0

3: while k < MAXITER & (θ ) − (θ−) ≥ δ do

4:

θ− ← θ

5:

ηk ← LINESEARCH( (θ−), Dθ (θ−))

6:

θ ← θ− − ηkDθ (θ−)

7:

k ←k+1

8: return θ

(θ ). Hence the gradient method provided in Algorithm 1 for solving the inverse risk-sensitive reinforcement learning problem is well formulated.
Remark 1: The prospect theory value function v given in (2) is not globally Lipschitz in y—in particular, it is not Lipschitz near the reference point yo—for values of ζ+ and ζ− less than one. Moreover, for certain parameter combinations, it may not even be differentiable. The -prospect
function, on the other hand, is locally Lipschitz and its derivative near the reference point is
bounded away from zero. This makes it a more viable candidate for numerical implementation. Its derivative, however, is not bounded away from zero as y → ∞.
This being said, we note that if the procedure for computing Q∗ follows an algorithm which implements repeated applications of the map T is initialized with Q0(x, a) being ﬁnite for all (x, a) and r is bounded for all possible (x, a, w) pairs, then the derivative of v˜ will always be bounded away from zero for all realized values of y in the procedure. An analogous statement can be made regarding the computation of Dθ Q∗. Hence, the procedures for computing Q∗ and Dθ Q∗ for all the value functions we consider (excluding the classical prospect value function) are guaranteed to converge (except on a set of measure zero).
Let us translate this remark into a formal result. Consider a modiﬁed version of Assumption 2: Assumption 3: The value function v ∈ C1(Y × Θ, R) satisﬁes the following:
(i) it is strictly increasing in y and for each θ ∈ Θ, there exists a y¯ such that v(y¯, θ ) = v0; (ii) for each θ ∈ Θ, it is Lipschitz in y with constant Ly(θ ) and locally Lipschitz on Θ with
constant Lθ .
Simply speaking, analogous to Assumption 1, we have removed the uniform lower bound on the derivative of v. Moreover, Theorem 2 gives us that T , as deﬁned in (53), is a contraction on a
ball of ﬁnite radius for each θ under Assumption 1. Theorem 5: Assume that v ∈ C1(Y × Θ, R) satisﬁes Assumption 3 and that the reward r :
X × A ×W → R is bounded almost surely by M > 0. Then the following statements hold. (a) For any ball BK(0), Q∗θ is locally Lipschitz-continuous on BK(0) as a function of θ —that is,
for any (x, a) ∈ X × A, θ , θ ∈ Θ, |Q∗(x, a, θ ) − Q∗(x, a, θ )| ≤ C θ − θ for some C > 0. (b) For each θ , let BK(0) be the ball with radius K satisfying

max{|v˜(M, θ )|, |v˜(−M, θ )|} < K min .Dv˜(y, θ ) (37)

1−γ

y∈IK

Except on a set of measure zero, the gradient Dθ Q∗θ (x, a) ∈ BK(0) is given by the solution of the ﬁxed–point equation

φθ

(x,

a)

=α

Ex

î ,w D2v˜(y(θ

),

θ

)

+

D1v˜(y(θ

),

θ

)(γ

φθ

(x

,

a∗x

)

−

φθ

(x,

ó a))

+

φθ

(x,

a)

(38)

13

where φθ : X × A → Rd and a∗x is the action that maximizes being any policy that is greedy with respect to Qθ .

a ∈A π(a|x )Q(x , a, θ ) with π

The proof (provided in Appendix D) of the above theorem follows the same techniques as in

Theorem 2 and Theorem 4.

Note that for each ﬁxed θ , condition (37) is the same as condition (24). Moreover, Proposition 3

shows that for the -prospect and entropic value functions, such a K must exist for any choice

of parameters.

C. Complexity
Small dataset size is often a challenge in modeling sequential human decision-making owing in large part to the frequency and time scale on which decisions are made in many applications. To properly understand how our gradient-based approach performs for different amounts of data, we analyze the case when the loss function, (θ ), is either the negative of the log-likelihood of the data—see (28) above—or the sum over states of the KL divergence between the policy under our learned value function and the the empirical policy of the agent—see (29) above. These are two of the more common loss functions used in the literature.
We ﬁrst note that maximizing the log-likelihood is equivalent to minimizing a weighted sum over states of the KL divergence between the empirical policy of the true agent, πˆn, and the policy under the learned value function, πθ . In particular, through some algebraic manipulation the weighted log-likelihood can be re-written as

(θ ) = x∈Dx w(x)DKL(πˆn(·|x)||πθ (·|x))

(39)

where w(x) is the frequency of state x normalized by |D| = N. This approach has the added beneﬁt that it is independent of θ and therefore will not be affected by scaling of the value functions [30].
Both cost functions are natural metrics for performance in that they minimize a measure of the divergence between the optimal policy under the learned agent and empirical policy of the true agent. While the KL-divergence is not suitable for our analysis, since it is not a metric on the space of probability distributions, it does provide an upper bound on the total variation (TV) distance via Pinsker’s inequality:

»

δ (πˆn(·|x), πθ (·|x)) ≤ 2DKL(πˆn(·|x)||πθ (·|x))

(40)

where δ (π(·|x), πθ (·|x)) is the TV distance between πˆn(·|x) and πθ (·|x), deﬁned as

δ (πˆn(·|x), πθ (·|x))

=

1 2

πθ (·|x) − πˆn(·|x)

1.

(41)

The TV distance between distributions is a proper metric. Furthermore, use of the two cost
functions described above will also translate to minimizing the TV distance as it is upper bounded
by the KL divergence. We ﬁrst note that, for each state x, we would ideally like to get a bound on δ (π(·|x), πθ (·|x)),
the TV distance between the agent’s true policy π(·|x) and the estimated policy πθ (·|x). However, we only have access to the empirical policy πˆn. We therefore use the triangle inequality to get an upper bound on δ (π(·|x), πθ (·|x)), in terms of values for which we can calculate explicitly or construct bounds. In particular, we derive the following bound:

δ (πθ (·|x), π(·|x)) ≤ δ (πˆn(·|x), πθ (·|x)) + δ (πˆn(·|x), π(·|x)).

(42)

14

Note that δ (πˆn(·|x), πθ (·|x)) is tantamount to a training error as metricized by the TV distance, and is upper bounded by a function of the KL divergence (which appears in the loss function) via (40).
The ﬁrst term in (42), δ (πˆn(·|x), π(·|x)), is the distance between the empirical policy and the true policy in state x. Using the Dvoretzky Kiefer-Wolfowitz inequality (see, e.g., [32], [33]), this term can be bounded above with high probability. Indeed,

Pr( π(·|x) − πˆn(·|x) 1 > ) ≤ 2|A|e−2n 2/|A|2, > 0

(43)

where n is the number of samples from the distribution π(·|x) and |A| is the cardinality of the action set. Combining this bound with (42), we get that, with probability 1 − ν,

Ç 2 2|A| å1/2 δ (πθ (·|x), π(·|x)) ≤|A| n log ν + δ (πˆn(·|x), πθ (·|x)). (44)

Supposing Algorithm 1 achieves a sufﬁciently small training error ε > 0, the second term above can be bounded above by a calculable small amount which we deﬁne notationally to be ε¯ > 0. Supposing ε¯ is also sufﬁciently small, the dominating term in the distance between π and πθ is the ﬁrst term on the right-hand side in (44). This gives us a O(n−1/2) convergence rate on the per state level. This rate is seen qualitatively in our experiments on sample complexity outlined in Section IV-A4.
We note that this bound is for each individual state x. Thus, for states that are visited more frequently by the agent, we have better guarantees on how well the policy under the learned value function approximates the true policy. Moreover, it suggests ways of designing data collection schemes to better understand the agent’s actions in less explored regions of the state space.

IV. EXAMPLES
Let us now demonstrate the performance of the proposed method on two examples. While we are able to formulate the inverse risk-sensitive reinforcement learning problem for parameter vectors θ that include γ and β , in the following examples we use γ = 0.95 and β = 4. The purpose of doing this is to explore the effects of changing the value function parameters on the resulting policy.
In all experiments, our optimization objective is the negative log-likelihood of the data, deﬁned in (28) and the valuation function we use is induced by an acceptance level set deﬁned for a value function that we specify and acceptance level of zero. Furthermore, for the prospect and -prospect value functions, we use a reference point of zero4. These choices are aimed at further deconﬂating our observations of the behavior—in terms of risk-sensitivity and loss-aversion—that results from different choices of the value function parameters from other characteristics of the MDP or learning algorithm.

A. Grid World In our ﬁrst test of the proposed gradient-based inverse risk-sensitive reinforcement learning
approach, we utilize data from agents operating on the canonical Grid World MDP. In the remainder, we describe the setup of the MDP, the three types of experiments we conduct, and qualitative results on sample complexity. The three experiments are described as follows:
4Individually, the acceptance level and the reference point can be recentered around zero without loss of generality.

15

(a)

(b)

Fig. 1: (a) Grid World layout showing the reward structure. (b) The ﬁve behavior proﬁles of risk-sensitive policies through the Grid World. These ﬁve paths correspond to the maximum likelihood paths of agents with various parameter combinations for their prospect, -prospect and entropic map value functions. To generate each behavior with the prospect and -prospect value functions, the following parameter combinations ({k−, k+, ζ−, ζ+}) were used: Behavior 1: {0.1, 1.0, 0.5, 1.5}; Behavior 2: {1.0, 1.0, 1.0, 1.0}; Behavior 3: {1.0, 1.0, 1.1, 0.9}; Behavior 4: {5.0, 1.0, 1.1, 0.8}; Behavior 5: {5.0, 1.0, 1.5, 0.7}. To generate the behaviors with the entropic map value function, we varied λ from 1 to −1.

1) Learning the value function of an agent with the correct model for the value function (e.g., learning a prospect value function when the agent also has a prospect value function);
2) Learning the value function of an agent with the wrong model for the value function (e.g., learning an entropic map value function when the agent has a prospect value function);
3) Exploring the dependence of our training error on the number of sample trajectories collected from the agent.
We measure the performance of the gradient-based approach via the TV norm, deﬁned in (41), of the difference between the policy in state x of the true agent and the policy in state x under the learned value function.
1) Setup: Our instantiation of Grid World is shown in Fig. 1a. An agent operating in this MDP starts in the blue box and aims to maximize their value function over an inﬁnite time horizon. Every square in the grid represents a state, and the action space is A = {N, NE, E, SE, S, SW,W, NW }. Each action corresponds to a movement in the speciﬁed direction (where we have used the usual abbreviations for directions). The black and green states are absorbing, meaning that once an agent enters that state they can never leave no matter their action. In all the other states, the agent moves in their desired direction with probability 0.93 and they move in any of the other seven directions with probability 0.01. To make the grid ﬁnite, any action taking the agent out of the grid has probability zero, and the other actions are re-weighted accordingly. The reward structure of our instantiation of the Grid World is shown in Fig. 1a as well. The agent gets a reward of −1 and +1 for being in the black and green states respectively. In the darker gray states, the agent gets a reward of −0.1. In all other states the agent is given a reward of

16

Value Function Behavior
Behavior 1 Behavior 2 Behavior 3 Behavior 4 Behavior 5

Prospect Mean Variance

1.9e-2 1.5e-2 2.0e-2 1.6e-2 4.7e-2

6.3e-4 2.0e-4 3.6e-4 2.0e-4 3.0e-3

-prospect Mean Variance

1.3e-2 1.0e-2 1.1e-2 1.2e-2 1.0e-2

2.3e-4 9.6e-5 1.3e-4 1.4e-4 3.4e-4

(a) Learning with the Correct Model

Entropic Mean Variance

1.6e-3 2.6e-4 2.2e-3 4.6e-4 6.6e-4

5.1e-6 1.4e-7 1.5e-5 1.8e-7 2.2e-7

Value Function
Prospect -prospect Entropic Map

Mean
1.5e-2 1.5e-2 5.4e-2

Variance
1.6e-4 1.6e-4 1.4e-2

(b) Learning with an Incorrect Model

TABLE I: Mean and variance of the TV distance across all states in the grid of the between the true policy and the policy under the learned value function. We note that we present the best of ﬁve randomly sampled initial sets of parameters. (a) Results for learning with the same type of value function as that of the agent. (b) Results learning with different models than the true agent: 10,000 trajectories are sampled from the policy of an agent with the prospect value function with {k−, k+, ζ−, ζ+} = {2.0, 1.0, 0.9, 0.7}; prospect, -prospect, and entropic map value functions are learned from this data.

+0.1. 2) Learning with the correct model of the value function: This experiment is intended to val-
idate our approach on a simple example. We trained agents with various parameter combinations of the four value functions described in Section II. The resulting policies of these agents are classiﬁed into ﬁve behavior proﬁles via their maximum likelihood path through the MDP. These behaviors are outlined in Fig. 1b. Each behavior corresponds to the maximum likelihood path resulting from a different risk proﬁle: Behavior 1 corresponds to a proﬁle that is risk-seeking on gains, Behavior 2 corresponds to a proﬁle that is risk neutral on gains and losses (this is also the behavior corresponding to the non-risk-sensitive reinforcement learning approach), and Behaviors 3-5 correspond to behaviors that are increasingly risk averse on losses and increasingly weigh losses more than gains.
We sampled 1,000 trajectories from the policies of these agents and used the data to learn the value function of the agent using our gradient-based approach. In this experiment, the learned value function is of the same type as that of the agent. For example, the data sampled from the policy of an agent having a prospect value function and exhibiting Behavior 1 is used to learn the parameters of a prospect value function. We note that due to the non-convexity of the problem, we use ﬁve randomly generated initial parameter choices.
The results we report are associated with the value function that achieves the minimum value of the objective. In Table Ia, we report the mean TV distance between the two policies across all states, as well as the variance in the TV distance across states. In all the cases considered in Table Ia, the learned value functions produce policies that correctly match the maximum likelihood path of the true agent.
We remark that the performance for learning a prospect value function was consistently worse than learning an -prospect function. This is most likely due to the fact that the prospect

17
value function is not Lipschitz around the reference point. Thus, we have no guarantees of differentiability of Q∗ with respect to θ for the prospect value function. This translates to numerical issues in calculating the gradient which, in turn, results in worse performance.
The entropic value function performs best of the four value functions, primarily due to the fact that there is only one parameter to learn, and the rewards and losses are all relatively small. In fact, in all the cases the learned entropic map value function coincided with the true value function of the agent, thereby indicating that the objective function was relatively convex around the parameter values we tried.
3) Learning with an incorrect model of the value function: The second experiment consists of learning different types of value functions from the same dataset. This is a more realistic experiment since the value function of human subjects will very likely be different than any model we could choose. The motivation for this experiment is to ensure that the results and risk-proﬁles learned were consistent across our choice of model.
The experiment uses 10,000 samples from an agent with a prospect value function and learned prospect, -prospect, and entropic map value functions. The mean TV distance between the policy of the true agent and the policies under the learned value functions are shown in Table Ib. The true agent’s value function has parameters {k−, k+, ζ−, ζ+} = {2.0, 1.0, 0.9, 0.7}—that is, it is risk-seeking in losses, risk-averse in gains, and loss averse.
Again, the learned value functions all have policies that replicated the maximum likelihood behavior of the true agent. We note that the -prospect and prospect functions perform as well as each other on this data, but the -prospect function showed none of the numerical issues that we encountered with the prospect function (see Section IV-C for further detail on numerical considerations). Further, learning with the -prospect function is markedly faster than with the prospect function. Again, this is most likely due to the fact that the prospect function is not locally Lipschitz continuous around the reference point. Thus, the values of α required to make the various contraction maps converge to their ﬁxed points are vanishingly small. This results in slow convergence.
The fact that the entropic value function does not perform as well is most likely due to the fact that it cannot accurately match the shape of the prospect function at these values; e.g., the entropic map is always either convex or concave.
4) Qualitative results on sample complexity: One of the challenges in modeling human decisionmaking is the lack of access to large datasets, particularly when it comes to sequential decisions that are made over longer periods of time. This is counter to the usual learning scenarios addressed in the much of the learning literature. For instance, if the focus is learning to control a robot, then it may be possible to generate a large number of demonstrations very quickly. This motivates our third experiment with the Grid World MDP—i.e. an experiment that allows us to better understand how the performance of our approach varies with the size of the dataset.
In this experiment, we ﬁrst train an agent with an entropic map value function and then create sets of sample trajectories from the agent’s policy varying between zero and 10,000 in size. Next, using each of these sample sets, we learn the value function via our approach and plot the mean TV distance across all states between the true policy of the agent and the policy under the learned value function. This is shown in Fig. 2.
First, we note that more data does translate to consistently better results. This matches our intuition that the better our data matches the policy of the true agent, the better we can learn a value function that would be associated with that policy. Of particular interest, though, is the rate at which the average TV across all the states decreases with the number of trajectories

18
Fig. 2: The mean TV distance across all states between the agent’s policy and the policy under the learned value function, as a function of the number of trajectories in the dataset. To construct each data point, we sample ﬁve different datasets of the same number of trajectories from the agent’s policy. We try ﬁve random initial parameter values per dataset and take the value function that achieves the minimum value of the objective. We calculate the mean TV distance between the policy of the agent and the policy under the learned value function for each dataset and then average these values. The bars show the 95% conﬁdence interval around the mean of the ﬁve datasets of the given size. We note that the trendline y = 0.04x−0.54 is the best ﬁt of the form y = axb to the data points, for constant terms a, b.
sampled. The rate, which is on the order of x−0.54, is very close to the asymptotic rate, derived in Section III-C, of O(x−1/2). This suggests that the dominating factor in the performance of our algorithm is how well our data matches the underlying policy, and not the non-convexity of the objective function. In fact, this provides empirical evidence that the second term in (44)— i.e. δ (πˆn(·|x), πθ (·|x))—must also be O(x−1/2).
B. A Passenger’s View of Ride-Sharing In addition to the Grid World example, we explore a ride-sharing example for which the MDP
is created from real-world data and we simulate agents with different risk preference and loss aversion proﬁles5.
Many ride-sharing companies set prices based on both supply of drivers and demand of passengers. From the passenger’s viewpoint, we model the ride-sharing MDP as follows. The action space is A = {0, 1} where 0 corresponds to ‘wait’ and 1 corresponds to ‘ride.’ The state space X = X × T ∪ {xf} where X is a ﬁnite set of surge price multipliers, T = {0, . . . , Tf} is the part of the state corresponding to the time index, and xf is a terminal state representing the completed ride that occurs when a ride is taken. At time t, the state is notationally given by (xt,t). The
5We adopt the surge pricing model here due to the availability of data even though ride-sharing services such as Uber are moving towards personalized pricing schemes that offer prices that the rider is willing to pay. This kind of pricing model motivates even more strongly the need for techniques that are considerate of how humans actually make decisions.

19

Value Function Preferences
RA Gains/RS Losses (Entropic: RA) Risk-Neutral
RS Gains/RA Losses (Entropic: RS)

Prospect Mean Variance

1.3e-2 0.6e-2 1.1e-2

3.5e-4 4.9e-5 1.7e-4

Entropic Mean Variance

0.9e-3 1.4e-3 1.1e-3

1.0e-6 2.0e-6 1.5e-6

Prospect/ -prospect Mean Variance

1.0e-2 6.6e-3 1.1e-2

1.4e-4 1.0e-4 1.1e-4

TABLE II: Averaged TV error and variance over 10 different initializations of the algorithm for different risk-preference proﬁles. The last column shows the error when using an -prospect agent with = 1e-2 to learn a prospect agent. We use the following abbreviations: Risk-Averse (RA); Risk-Seeking (RS).

reward rt is modeled as a random variable that depends on the current price as well as a random variable Z(t) for travel time. In particular, for any time t < Tf the reward is given by

®

R(xt, at) = r¯, at = 0 (’wait’)

(45)

r˜t, at = 1 (’ride’)

with r¯ < 0 a constant and r˜t = St − xt (pbase + pmileD + pminZ(t)) where D is the distance in miles, St is a time dependent satisfaction (we selected it to linearly decrease in time from some initial satisfaction level), and pbase, pmile, and pmin are the base, per mile, and per min prices, respectively.
At the ﬁnal time Tﬁnal, the agent is forced to take the ride if they have not selected to take a ride at a prior time. This reﬂects the fact that the agent presumably needs to get from their origin to their destination and the reward structure reﬂects the dissatisfaction the agent feels as a result of having to ultimately take the ride despite the potential desire to wait.
Using the Uber Movement6 platform for travel time statistics, base pricing data7 and surge pricing data8 for Washington D.C., we examined several locations and hours which have different characteristics in terms of travel time and price statistics. We generate the distribution for Z(t) from these data sets as well as the surge price intervals and transition probability matrix. Since the core risk-sensitive behaviors we observe are similar across the different locations, we report only on one.
Speciﬁcally, we report on a ride-sharing MDP generated with an origin and destination of GPS= (−77.027046, 38.926749) and GPS= (−76.935773, 38.885964)9, respectively, in Washington D.C. at 5AM.
The transition probability kernel P : X × A × X → [0, 1] is estimated from the ride-sharing data. The travel-time data is available on an hourly basis and the price change data is available on a three minute basis. Hence, we use the three minute price change data for each hour to derive a static transition matrix by empirically estimating the transition probabilities where we bin prices in the following way. For prices in [1.0, 1.2), x = 1.0; for prices in [1.2, 1.6), x = 1.4; for prices in [1.6, 2.0), x = 1.8; otherwise x = 2.2. Hence, X = {1.0, 1.4, 1.8, 2.2}. In the time periods we

6Uber Movement: https://movement.uber.com/cities 7The base, per min, and per mile prices can be found here: http://uberestimate.com/prices/
Washington-DC/ 8The surge pricing data we used was originally collected by and has been made publicly available here: https:
//github.com/comp-journalism/2016-03-wapo-uber. The data we use was collected over three minute intervals in period between November 14 to November 28, 2016.
9Note that these correspond to Uber Movement id’s 197 and 113, respectively.

20

price multiplier 1.0 1.4 1.8 2.2

true 0.27 0.34 0.46 0.46 0.96 1

0.45 0.53 0.49 0.6 0.98 1

price multiplier 1.0 1.4 1.8 2.2

0.62 0.6 0.66 0.74 0.99 1

1.0

0.74 0.67 0.7 0.75 0.99 1

0.8

learned 0.6

0.28 0.32 0.4 0.39 0.94 1

0.4

0.2 0.47 0.53 0.46 0.55 0.98 1
0.0 0.63 0.58 0.65 0.72 0.98 1

0.78 0.72 0.74 0.79 0.99 1 012345 time

(a) risk-seeking in gains/risk-averse in losses (convex/concave), ζ+ = ζ− = 1.5

price multiplier 1.0 1.4 1.8 2.2

price multiplier 1.0 1.4 1.8 2.2

true 0.34 0.35 0.39 0.42 0.96 1

0.62 0.45 0.57 0.63 0.99 1

0.72 0.62 0.66 0.65 0.99 1

1.0

0.68 0.7 0.62 0.72 0.99 1

0.8

learned 0.6

0.34 0.36 0.4 0.42 0.96 1

0.4

0.2 0.62 0.45 0.58 0.64 0.99 1
0.0 0.72 0.62 0.67 0.66 0.99 1

0.68 0.7 0.62 0.72 0.99 1 012345 time

(b) risk-neutral, ζ+ = ζ− = 1.0

price multiplier 1.0 1.4 1.8 2.2

true 0.49 0.51 0.56 0.46 0.98 1

0.64 0.49 0.64 0.63 0.98 1

price multiplier 1.0 1.4 1.8 2.2

0.73 0.58 0.68 0.55 0.99 1

1.0

0.66 0.65 0.62 0.58 0.99 1

0.8

learned 0.6

0.48 0.5 0.54 0.44 0.98 1

0.4

0.2 0.64 0.51 0.66 0.64 0.98 1
0.0 0.75 0.6 0.69 0.58 0.99 1

0.68 0.67 0.64 0.6 0.99 1 012345 time

(c) risk-averse in gains/risk-seeking in losses (concave/convex), ζ+ = ζ− = 0.5

Fig. 3: Plots showing the probabilities of taking a ride in each state under the true and learned optimal policies for true and learned agents with prospect value functions. The true agent has prospect gain parameters of k+ = 0.5 and k− = 1.0 for all three plots. The value function used for the right most graphic (Fig. 4c) is most representative of human decision-making since humans tend to be risk-averse in gains, risk-seeking in losses, are loss averse. In these plots, the trend we see is that the more risk-averse, the less likely they policy suggests taking the ride.

examine, the max price multiplier was 2.2. We set the reference point y0 and acceptance level v0 to be zero.
With this model, the transition matrix for the price multipliers is given by

0.876 0.099 0.017 0.008

0.347 0.412 0.167 0.074

P = 0.106 0.353 0.259 0.282

(46)





0.086 0.219 0.143 0.552

for each time. The travel time distribution is a standard normal distribution truncated to the upper and lower bounds speciﬁed by the Uber Movement data. Measured in seconds, we use location parameter 2371, scale parameter 100, and 1554 and 3619 as the upper and lower bound, respectively.
The graphics in Fig. 3 and Fig. 4 show the state space as a grid with the probability of taking a ride under the true and learned optimal policies overlaid on each state. For the examples depicted in these ﬁgures, we consider the true and learned agents to have prospect value functions.
In Fig. 3, we ﬁx the true agent’s parameters to show a range of behaviors from risk-seeking in losses/risk-averse in gains to risk-averse in losses/risk-seeking in gains. There is empirical evidence supporting the fact that humans are more like the former. Moreover, in these examples we use (k+, k−) = (0.5, 1) to capture that humans tend to be loss-averse—that is, for losses and gains of equal value, the loss is perceived as more signiﬁcant.
On the other hand, in Fig. 4 we ﬁx the true agent’s parameters to show a range of behaviors depending on the degree of loss-aversion. In particular, we ﬁx ζ− = ζ+ = 1 and vary the ratio of k− to k+, where a higher ratio corresponds to more loss averse preferences.

21

price multiplier 1.0 1.4 1.8 2.2

true 0.18 0.14 0.18 0.24 0.93 1

0.38 0.32 0.35 0.49 0.97 1

price multiplier 1.0 1.4 1.8 2.2

0.45 0.5 0.47 0.5 0.98 1

1.0

0.6 0.7 0.55 0.66 0.99 1

0.8

learned 0.6

0.18 0.14 0.16 0.2 0.92 1

0.4

0.2 0.38 0.32 0.33 0.46 0.97 1
0.0 0.46 0.51 0.47 0.49 0.98 1

0.61 0.71 0.56 0.67 0.99 1 012345 time

(a) less loss-averse k+ = 1.5, k− = 0.5

price multiplier 1.0 1.4 1.8 2.2

true 0.23 0.27 0.31 0.34 0.96 1

0.43 0.52 0.56 0.61 0.97 1

price multiplier 1.0 1.4 1.8 2.2

0.56 0.55 0.6 0.6 0.98 1

1.0

0.67 0.65 0.63 0.69 0.99 1

0.8

learned 0.6

0.24 0.27 0.31 0.34 0.96 1

0.4

0.2 0.43 0.51 0.55 0.59 0.97 1
0.0 0.57 0.55 0.6 0.59 0.98 1

0.64 0.63 0.6 0.67 0.99 1 012345 time

(b) no loss-aversion, k+ = k− = 1.0

price multiplier 1.0 1.4 1.8 2.2

true 0.41 0.46 0.54 0.41 0.97 1

0.59 0.58 0.63 0.62 0.99 1

price multiplier 1.0 1.4 1.8 2.2

0.7 0.67 0.67 0.66 0.99 1

1.0

0.73 0.66 0.72 0.7 0.99 1

0.8

learned 0.6

0.41 0.45 0.52 0.39 0.97 1

0.4

0.2 0.59 0.59 0.63 0.63 0.99 1
0.0 0.72 0.68 0.69 0.68 0.99 1

0.77 0.71 0.76 0.74 0.99 1 012345 time

(c) more loss-averse, k+ = 0.5, k− = 1.5

Fig. 4: Plots showing the probabilities of taking a ride in each state under the true and learned optimal policies for true and learned agents with prospect value functions. The true agent has prospect parameters of ζ− = ζ+ = 1.0 for all three plots, while we vary (k+, k−) to capture different degrees of loss-aversion. In these plots, the trend we see is that the more loss-averse the agent (under both the learned and true value functions), the more likely they are to take the ride.

0.4

E[R[xt,at=ride,xf]]

0.2

0.0

0.2

0.4 1.0

1.4

1.8

2.2

price multiplier

Fig. 5: Expected rewards for each time step in the ride-sharing example. Notice that the rewards can either be gains (positive values) or losses (negative values) given that we take the reference point to be yo = 0.

In each of the graphics in Fig. 3 and Fig. 4, we see that the learned policy is very close to the true policy. In addition, in Fig. 3, we observe that the more risk-averse the agent is (in gains or losses), the more likely they are to take the ride. This trend can be seen by noting the sign of the expected rewards—in Fig. 5, we see that the reward is positive for xt ∈ {1.0, 1.4} and is negative for xt ∈ {1.8, 2.2}—and examining the corresponding rows in Fig. 3 for negative and positive rewards. In Fig. 4, observe that the more loss averse the agent, the more likely they are to take the ride uniformly. This is reasonable as the satisfaction level is linear decreasing in time.
In Table II, we show the mean and variance of the total variation error for the ride-sharing example where we varied the risk preference proﬁles, holding (k−, k+) = (1, 1), using agents with prospect and entropic value functions. In addition, we show the error for different risk proﬁles

22
when we learn a true prospect agent with an -prospect agent. Recall that the prospect value function does not meet the requirements of our theorem whereas the -prospect value function does as it is Lipschitz.
C. Numerical Considerations
We end the experimental results section with some observations on the convergence speed and the implementation of Algorithm 1.
First, we note that the two contraction mappings (33) and (32) are sensitive to the learning rate α. A very small choice of α results in convergence of the sequence of Q-functions to the ﬁxed point being too slow to be practically useful. On the other hand, a large choice of α makes the sequence diverge. Thus, choosing α has a large effect on the runtime of the overall algorithm as the computation of Q∗ and Dθ Q∗ both depend on the choice of α.
We further remark that numerical observations suggest that the condition α ∈ (0, min{L−1, 1}] is fairly restrictive and that larger values of α give faster convergence. Hence, our implementation of Algorithm 1 includes an adaptive scheme to ﬁnd the largest possible α. In particular, if two consecutive iteration elements in the sequence are observed to diverge in the L∞ norm, we decrease α by a ﬁxed constant. As long successive elements in the sequence converge, we periodically increase α by another constant. This allows us to noticeably speed up the implementation of our algorithm. Adaptively choosing the step-size α also allows us to train the prospect function agents more accurately, since these were particularly susceptible to changes in the value of α due to the fact that the value function is non-Lipschitz around the reference point.
To speed up the gradient-descent algorithm, we also implement a back-tracking line search. We do this to address the computationally intensive gradient calculation. Speciﬁcally, the line-search allows us to exploit each gradient calculation fully. The backtracking line search also leads to a noticeable speed up in the implementation of our algorithm, which allows us to tackle larger MDPs.
V. RELATED WORK
Before concluding, we draw some comparisons between our work and that of others on related topics.
The primary motivation for most other works in this domain is to learn a prescriptive model for humans amidst autonomy. For example, in [7], their approach to learning the decision-making model is to parameterize unknown quantities of interest, sample the parameter space, and use a model selection criteria (i.e. Bayesian information criteria) to select parameters that best ﬁt the observed behavior. In contrast, we derive a well-formulated gradient-based procedure for ﬁnding the value function and policy best matching the observed behavior. Moreover, we introduce new value functions that satisfy our theorems for the forward and inverse problems while retain the salient features of the empirically observed behavioral models.
Another related work is [10]. The authors take a similar approach to ours in leveraging risk metrics to capture risk sensitivity. However, they focus their efforts on estimating the risk metric by leveraging the well-known representation theorem for coherent risk metrics [23]. They couple the resulting optimization problem with classical inverse reinforcement learning procedures for learning the reward (that is, they parameterize the reward function over a set of basis functions), yet their approach does not differentiate between the reward and the decision-making model. In contrast, we consider a broad class of risk metrics generated by value functions via acceptance

23

sets, formulate the MDP model based on the risk metric, and learn the parameters of the value function that generates the risk metric and results in a policy that best matches the agent’s observed behavior. The parameters of the value function, which ultimately drive the decisionmaking model, are highly interpretable in terms of the degree of risk sensitivity and loss aversion. Thus, our technique supports prescriptive and descriptive analysis, both of which are important for the design of incentives and policies that takes into consideration the nuances of human decision-making behavior.
Finally, as noted in the introduction, the results in this paper signiﬁcantly extend our previous work [15]. We (i) provide new convergence guarantees for the forward risk-sensitive reinforcement learning problem where behavioral-based value functions satisfy the assumptions and (ii) provide more extensive theoretical results on the gradient-based inverse risk-sensitive reinforcement learning problem including proofs for theorems appearing in the prior work.

VI. DISCUSSION
We present a new gradient based technique for learning risk-sensitive decision-making models of agents operating in uncertain environments. Speciﬁcally, we introduce a new forward risksensitive reinforcement learning procedure with convergence guarantees for which value functions retaining the fundamental shape of behavioral value functions satisfy the assumptions. Based on this learning algorithm, we introduce a well-formulated gradient-based inverse reinforcement learning algorithm to recover the parameters indicating the observed agent’s risk preferences. We demonstrate the algorithm’s performance for agents based on several types of behavioral models and do so on two examples: the canonical Grid World problem and a passenger’s view ride-sharing where the parameters of the ride-sharing MDP are learned from real-world data.
Looking forward, we are examining ways of designing mechanisms to adaptively produce the most informative demonstrations and to incentivize certain behaviors. This is challenging since some of the options in the state-action space may result in a very high risk to the agent and thus, the volatility introduced by inducing such state-action pairs may be viewed unfavorably by the agent causing them to completely opt-out.

APPENDIX

A. Proof of Theorem 2

The proof of Theorem 2 relies on the following ﬁxed point theorem.
Theorem 6 (Fixed Point Theorem [34, Theorem 2.2]): Let (X, d) be a complete metric space
and let Br(y) = {x ∈ X| d(x, y) < r} be a ball of radius r, where r > 0, centered at y ∈ X. Let f : Br(y) → X be a contraction map with contraction constant h < 1. Further, assume that d(y, f (y)) < r(1 − h). Then, f has a unqiue ﬁxed point in Br(y).
Proof of Theorem 2.a.: We claim that T is a contraction with constant α¯ = (1 − α(1 − γ)εK) where εK = min{Dv˜(y)| y ∈ IK}. Indeed, let y(Q(x, a)) = r(x, a, w) + γ maxa Q(x , a ) − Q(x, a) be the temporal difference and deﬁne g(x , a ) = maxa Q(x , a ). For any Q ∈ BK(0) we note that the temporal differences are bounded—in fact, y(Q(x, a)) ∈ IK = [−M − K, M + K]. Due to the monotonicity assumption on v, we have that for any y , y ∈ IK, v˜(y) − v˜(y ) = ξ (y − y ) for some ξ ∈ [εK, L]. Recall the contraction map deﬁned in (23):

îÄ

äó

(T Q)(x, a) =αEx ,w v˜ y(Q(x, a)) + Q(x, a)

(47)

24

Then, for any Q1 and Q2, we have that

(T Q1 − T Q2)(x, a) = αEx ,w[v˜(y(Q1(x, a))) − v˜(y(Q2(x, a)))] + Q1(x, a) − Q2(x, a) ≤ αEx ,w[ξx ,w(γ(g1(x , a ) + g2(x , a )) − Q1(x, a) + Q2(x, a))] + Q1(x, a) − Q2(x, a) ≤ αγEx ,w[ξx ,w(g1(x , a ) + g2(x , a ))] + (1 − αEx ,w[ξx ,w])(Q1(x, a) − Q2(x, a)).

Hence,

|(T Q1 − T Q2)(x, a)| ≤ (1 − α(1 − γ)Ex ,w[ξx ,w]) Q1 − Q2 ∞

≤ (1 − α(1 − γ)εK) Q1 − Q2 ∞.

We claim that the constant α¯ K = 1 − α(1 − γ)εK < 1. Indeed, recall that 0 < α ≤ min{1, L−1} so that if α = L−1, then α¯ K < 1 since L = maxy∈IK Dv˜(y) and εK = miny∈IK Dv˜(y). On the other hand, if α = 1, then 1 ≤ L−1 ≤ (εK)−1 so that εK ≤ 1 which, in turn, implies that α¯ K < 1. If 0 < α < min{1, L−1}, then α¯ K < 1 follows trivially from the implications in the above two cases.
Thus, T is a contraction on BK(0) with the constant α¯ K = (1 − α(1 − γ)εK) < 1.
Proof of Theorem 2.b.: Suppose K is chosen such that

max{|v˜(M)|, |v˜(−M)|}

< K min Dv˜(y).

(48)

1−γ

y∈IK

Now, we argue that T applied to the zero map, 0 ∈ BK(0), is strictly less than K(1 − α¯ K). Indeed, for any α ∈ (0, min{1, L−1}],

T (0) ≤ α max{|v(M)|, |v(−M)|} < (1 − γ)KεKα = K(1 − α¯ K)

Combinging the above fact with the fact that T is a contraction, the assumptions of Theorem 6 hold and, hence there is a unique ﬁxed point Q∗(x, a) ∈ BK(0) for each (x, a) ∈ X × A.

B. Proof of Proposition 3

Recall that v˜ ≡ v − v0. For different value functions, Proposition 3 claims that the condition of

Theorem 2.b,

max{|v˜(M)|, |v˜(−M)|}

< K min Dv˜(y),

(49)

(1 − γ)

y∈IK

holds. Proof of Proposition 3.a: Suppose v satisﬁes Assumption 1 and that for some ε > 0,
ε < v(yy)−−vy(y ) for all y = y . Then there exists a value of K, say K¯ , such that (49) holds for all K > K¯ . Indeed since minK>0 εK > ε, for all K satisfying
max{|v˜(M)|, |v˜(−M)|} < K, ε(1 − γ)

(49) must hold. Proof of Proposition 3.b: We now show that for the -prospect value function, (49) holds
for any choice of parameters (k−, k+, ζ−, ζ+). Indeed, for ζ+, ζ− ≥ 1 and any choice of k−, k+,

min εK > ε > 0
K>0

25

where ε = min{limy↑0 Dv˜(y), limy↓0 Dv˜(y)}. Therefore, with ζ+, ζ− ≥ 1, for any K such that

max{|v˜(M)|, |v˜(−M)|} < K, ε(1 − γ)

(49) must hold. For the case when either ζ+ < 1 or ζ− < 1 or both, we note that

®

´

min Dv˜(y) = min

min Dv˜(y), ε .

y∈IK

y∈{M+K,−M−K}

so that we need only show that for ζ+ < 1 and ζ− < 1, there exists a K such that

max{|v˜(M)|, |v˜(−M)|}

< KDv˜(K + M)

(50)

1−γ

and

max{|v˜(M)|, |v˜(−M)|}

< KDv˜(−K − M),

(51)

1−γ

respectively. Note that

® k+ζ+(y − y0 +

)ζ+−1,

Dv˜(y) = k−ζ−(y0 − y + )ζ−−1,

y ≥ y0 y < y0

Without loss of generality, we show (50) must hold for ζ+ < 1 and reference point y0 = 0 (the proof for ζ− < 1 follows an exactly analogous argument). Plugging Dv˜(K +M) in and rearranging, we get that we need to ﬁnd a K such that
max{|v˜(M)|, |v˜(−M)|} < K(K + M + )ξ+−1 (1 − γ)ξ+k+
Since the right-hand side above is a function of K that is zero at K = 0 and approaches inﬁnity as K → ∞, and the left-hand side is a ﬁnite constant, there is some K¯ such that for all K > K¯ , the above holds. Thus, for the -prospect value function, our assumptions are satisﬁed and there always exists a value of K to choose in Theorem 2.b.
Proof of Proposition 3.c: Suppose v is an entropic map. We note that, for the entropic map, miny∈IK Dv˜(y) must occur at either K + M or −K − M if λ < 0 or λ > 0, respectively. Without loss of generality, let λ > 0. First, consider that the derivative of v˜,
Dv˜(y) = (λ )−1eλy,

is minimized on IK at −M − K for any M and K. Moreover, |v˜(M)| > |v˜(−M)|. Hence, with K = λ −1, we can derive conditions on λ for which (49) holds. In other words, with the speciﬁed K, we use (49) to dertermine which values of λ are admissible. Indeed, from (49), we have
(1 − γ)−1eλ M < (λ )−1e−λ M−1
which reduces to λ e2λM < (1 − γ)e−1. Let x = 2λ M, so that xex < 2M(1 − γ)e−1.
Now, we can apply the Lambert W function which satisﬁes W (xex) = x for x ≥ 0, to get that x < W (2M(1 − γ)e−1),

so that

λ < (2M)−1W (2M(1 − γ)e−1).

Thus, if |λ | < (2M)−1W (2M(1 − γ)e−1), then for the choice K = 1 , (49) holds so that Theo-
λ
rem 2.b holds for the entropic map.

26

C. Proof of Theorem 4

We remark that the gradient algorithm and Theorem 4 are consistent with the gradient descent framework which uses the contravariant gradient for learning as introduced in [35] for Riemannian parameter spaces Θ. Of course, when Θ is Euclidean and the coordinate system is orthonormal, the gradient we normally use (covariant derivative) coincides with the contravariant gradient. However, using the covariant derivative does not generalize to admissible parameter spaces with more structure.
Moreover, as is pointed out in [30], the trajectories that result from the solution to the gradient algorithm are equivalent up to reparameterization through a smooth invertible mapping with a smooth inverse. Contravariant gradient methods have been shown to be asymptotically efﬁcient in a probabilistic sense and thus, they tend to avoid plateaus [35], [36].
Before we dive into the proof of Theorem 4, let us introduce some deﬁnitions and useful propositions.
Deﬁnition 4 (Fre´chet Subdifferentials): Let U be a Banach space and U∗ its dual. The Fre´chet subdifferential of f : U → R at u ∈ U, denoted by ∂ f (u) is the set of u∗ ∈ U∗ such that

lim inf h −1 ( f (u + h) − f (u) − u∗, u ) ≥ 0.

(52)

h→0 h=0

Proposition 4 ( [30], [37]): For a ﬁnite family ( fi)i∈I of real-valued functions (where I is a ﬁnite index set) deﬁned on U, let f (u) = maxi∈I fi(u). If u∗ ∈ ∂ fi(u) and fi(u) = f (u), then u∗ ∈ ∂ fi(u). If f1, f2 : U → R, α1, α2 ≥ 0, then α1∂ f1 + α2∂ f2 ⊂ ∂ (α1 f1 + α1 f2).
Proposition 5 ( [30], [38]): Suppose that ( fn)n∈N is a sequence of real-valued functions on U which converge pointwise to f . Let u ∈ U, u∗n ∈ ∂ fn(u) ⊂ U∗ and suppose that (u∗n) is weak∗– convergent to u∗ and is bounded. Moreover, suppose that at u, for any ε > 0, there exists an
N > 0 and δ > 0 such that for any n ≥ N, h ∈ BU (0, δ ), a δ –ball around 0, fn(u + h) ≥ fn(u) + u∗n, h − ε h . Then u∗ ∈ ∂ f (u).
We now provide the proof for parts (a) and (b) of Theorem 4.
Proof of Theorem 4.a.: Let Q0(x, a, θ ) ≡ 0. Then it is trivial that Q0(x, a, θ ) is locally
Lipschitz in θ on Θ. Supposing that Qt(x, a, θ ) is Lt–locally Lipschitz in θ , then we need to
show that T Qt(x, a, θ ) is locally Lipschitz which we recall is deﬁned by

(T Q)(x, a, θ ) = αEx ,wv˜(y(θ ), θ ) + Q(x, a, θ )

(53)

where y(θ ) = r(x, a, w) + γ maxa ∈A Q(x , a , θ ) − Q(x, a, θ ). Since v˜ ≡ v − v0, it also satisﬁes Assumption 2. Let Ly = max{Ly(θ )|θ ∈ Θ} and deﬁne
gt(x, θ ) = maxa Qt(x, a , θ ). Note that since Qt is assumed Lipschitz with constant Lt, so is gt. Surpressing the dependent of T Q on (x, a), we have that

T Qt(θ ) − T Qt(θ ) = αEx ,w[v˜(y(θ ), θ ) − v˜(y(θ ), θ )] + Qt(x, a, θ ) − Qt(x, a, θ ) = αEx ,w[v˜(y(θ ), θ ) − v˜(y(θ ), θ ) + v˜(y(θ ), θ ) − v˜(y(θ ), θ )] + Qt(θ ) − Qt(θ ).

Due to the monotonicity of v˜ in y, we know that for all y1, y2 there exists ξ ∈ [ε, Ly] such that

v˜(y1, θ ) − v˜(y2, θ ) = ξ (y1 − y2).

Hence,

Ex ,w[v˜(y(θ ), θ ) − v˜(y(θ ), θ ) + v˜(y(θ ), θ ) − v˜(y(θ ), θ )] = Ex ,w[ξx ,w(y(θ ) − y(θ )) + v˜(y(θ ), θ ) − v˜(y(θ ), θ )]

27

where we simply denote the dependence of ξ on x and w, the components subject to randomness. Then,

î

ó

T Qt(θ ) − T Qt(θ ) = αEx ,w ξx ,w(y(θ ) − y(θ )) + v˜(y(θ ), θ ) − v˜(y(θ ), θ )

+ Qt (θ ) − Qt (θ )

= αγEx ,w[ξx ,w(gt (x , θ ) − gt (x , θ ))] − αEx ,w[ξx ,w(Qt (θ ) − Qt (θ ))] + αEx ,w[v˜(y(θ ), θ ) − v˜(y(θ ), θ )] + Qt(θ ) − Qt(θ ) = αγEx ,w[ξx ,w(gt (x , θ ) − gt (x , θ ))] − αEx ,w[ξx ,w](Qt (θ ) − Qt (θ )) + αEx ,w[v˜(y(θ ), θ ) − v˜(y(θ ), θ )] + Qt(θ ) − Qt(θ ) = (1 − αEx ,w[ξx ,w])(Qt (θ ) − Qt (θ )) + αγEx ,w[ξx ,w(gt (x , θ ) − gt (x , θ ))] + αEx ,w[v˜(y(θ ), θ ) − v˜(y(θ ), θ )]

so that

T Qt(θ ) − T Qt(θ ) ≤((1 − α(1 − γ)ε) + αLθ )Lt θ − θ .

Hence, letting α¯ = 1 − α(1 − γ)ε, we have that T Qt(·, ·, θ ) is Lt+1–locally Lipschitz with Lt+1 = α¯ Lt + αLθ . With L0 = 0, by iterating, we get that

Lt+1 = (α¯ t + · · · + α¯ + 1)αLθ .

As stated in Section III-B, T is a contraction so that T nQ0 → Q∗θ = Q∗(·, ·, θ ) as n → ∞. Hence, by the above argument, Q∗θ is αLθ /(1 − α¯ )–Lipschitz continuous.
Proof of Theorem 4.b.: Consider a ﬁxed vector θ ∈ Rd. We now show that the operator S acting on the space of functions φθ : X × A → Rd and deﬁned by

(Sφθ

)(x,

a)

=

α

Ex

î ,w D2v˜(y(θ

),

θ

)

+

D1v˜(y(θ

),

θ

)(γ

φθ

(x

,

a∗x

)

−

φθ

(x,

ó a))

+

φθ

(x,

a)

(54)

is a contraction where a∗x is the action that maximizes policy π with respect to Qθ . Indeed,

a ∈A π(a|x)Q(x, a, θ ) for any greedy

(Sφθ

−

Sφθ

)(x,

a)

=

α

Ex

,w[D1v˜(y(θ

),

θ

Ä )γ

(φθ

(x

,

a∗x

)

−

φθ

(x

,

a∗x

))

ä − (φθ (x, a) − φθ (x, a)) ] + φθ (x, a) − φθ (x, a)

≤ (1 − α(1 − γ)Ex ,w[D1v˜(y(θ ), θ )]) φθ − φθ ∞

so that, by Assumption 2,

(Sφθ − Sφθ )(x, a) ≤ (1 − α(1 − γ)ε) φθ − φθ ∞.
Thus, α¯ is the required constant for ensuring S is a contraction. We remark that S operates on each of the d components of θ separately and hence, it is a contraction when restricted to each
individual component. Let π denote a greedy policy with respect to Q∗θ and let πn be a sequence of policies that are
greedy with respect to Qn = T nQ0 where ties are broken so that (x,a)∈X×A |π(a|x) − πn(a|x)| is minimized. Then for large enough n, πn = π. Denote by Sπn the map S deﬁned in (54) where πn is the implemented policy. Consider the sequence φθ,n such that φθ,0 = 0 and φθ,n+1 = Sπnφθ,n. For large enough n, φθ,n+1 = Sπ φθ,n. Applying the (local) contraction mapping theorem (see, e.g., [39, Theorem 3.18]) we get that limn→∞ Snφ0 converges to a unique ﬁxed point.

28

Moreover, by induction and Proposition 4, φθ,n(x, a) ∈ ∂θ Qn(x, a, θ ). Hence, by Proposition 5, the limit is a subdifferential of Q∗θ since v˜ is Lipschitz on Y and Θ and the derivatives of v˜ are uniformly bounded. Since by part (a), Q∗θ is locally Lipschitz in θ , Rademacher’s Theorem (see, e.g., [31, Thm. 3.1]) tells us it is differentiable almost everywhere (expcept a set of Lebesgue
measure zero). Since Q∗θ is differentiable, its subdifferential is its derivative.

D. Proof of Theorem 5

Given that the proof of Theorem 5 follows the same techniques as in Theorem 2 and Theorem 4,
we provide largely an outline, directing the reader to the particular analogous components of the
two proceeding theorems that are mimicked in creating the proof.
Proof of Theorem 5.a.: For each θ , the proof that T Q(x, a, θ ) is a contraction, and thus has a ﬁxed point Q∗θ ∈ BK(0), follows directly that of Theorem 2 where instead of Q1 and Q2 we have Q(θ ) and Q(θ ). Given that T is a contraction, the proof that Q∗θ ∈ BK(0) is Lipschitz with constant αLθ /(1 − α¯ K) follows a similar argument to Theorem 4.
Proof of Theorem 5.b.: The proof that S is a contraction on BK(0) follows a similar argument to that of Theorem 4, part (b). Indeed,

(Sφθ

−

Sφθ

)(x,

a)

=

α

Ex

,w[D1v˜(y(θ

),

θ

Ä )γ

(φθ

(x

,

a∗x

)

−

φθ

(x

,

a∗x

))

ä − (φθ (x, a) − φθ (x, a)) ] + φθ (x, a) − φθ (x, a)

≤ (1 − α(1 − γ)Ex ,w[D1v˜(y(θ ), θ )]) φθ − φθ ∞

so that, by Assumption 1,

(Sφθ − Sφθ )(x, a) ≤ (1 − α(1 − γ)εK) φθ − φθ ∞

where εK = min{D1v(y, θ )| y ∈ IK}. Note that α¯ K = 1 − α(1 − γ)εK < 1 for the same reasons as given in the proof of Theorem 2 since α ∈ (0, min{1, L−1}].
For each θ ∈ Θ, let BK(0) be the ball with radius K satisfying

max{|v˜(M, θ )|, |v˜(−M, θ )|} < K min Dv˜(y, θ ).

1−γ

y∈IK

Then, for each θ , S satisﬁes Theorem 6 so that it has a unique ﬁxed point in BK(0).
Following the same argument as in the proof of Theorem 4, part (b), by induction and
Proposition 4, φθ,n(x, a) ∈ ∂θ Qn(x, a, θ ). Hence, by Proposition 5, the limit is a subdifferential of Q∗θ . By part (a), Q∗θ is locally Lipschitz in θ so that Rademacher’s Theorem (see, e.g., [31, Thm. 3.1]) implies it is differentiable almost everywhere (expcept a set of Lebesgue measure zero). Since Q∗θ is differentiable, its subdifferential is its derivative.

REFERENCES
[1] B. Ko¨szegi and M. Rabin, “A model of reference-dependent preferences,” The Quarterly J. Economics, vol. 121, no. 4, pp. 1133–1165, 2006.
[2] A. Tversky and D. Kahneman, “Loss aversion in riskless choice: A reference-dependent model,” The Quarterly J. Economics, vol. 106, no. 4, pp. 1039–1061, 1991.
[3] ——, “Rational choice and the framing of decisions,” J. Business, vol. 59, no. 4, pp. pp. S251–S278, 1986. [4] P. Geibel and F. Wysotzki, “Risk-sensitive reinforcement learning applied to control under constraints,” J. Artiﬁcial
Intelligence Research, vol. 24, pp. 81–108, 2005.

29
[5] V. S. Borkar and S. P. Meyn, “Risk-sensitive optimal control for markov decision processes with monotone cost,” Mathematics of Operations Research, vol. 27, no. 1, pp. 192–209, 2002.
[6] P. L.A., C. Jie, M. Fu, S. Marcus, and C. Szepesva´ri, “Cumulative prospect theory meets reinforcement learning: Prediction and control,” in Proc. 33rd Intern. Conf. on Machine Learning, vol. 48, 2016.
[7] Y. Shen, M. J. Tobia, and K. Obermayer, “Risk-sensitive reinforcement learning,” Neural Computation, vol. 26, pp. 1298–1328, 2014.
[8] O. Mihatsch and R. Neuneier, “Risk-sensitive reinforcement learning,” Machine Learning, vol. 49, no. 2, pp. 267–290, 2002.
[9] A. J. Nagengast, D. A. Braun, and D. M. Wolpert, “Risk-sensitive optimal feedback control accounts for sensorimotor behavior under uncertainty,” PLOS Computational Biology, vol. 6, no. 7, pp. 1–15, 2010.
[10] A. Majumdar, S. Singh, A. Mandlekar, and M. Provone, “Risk-sensitive inverse reinforcement learning via coherent risk models,” in Robotics: Science and Systems, 2017.
[11] A. Y. Ng and S. Russell, “Algorithms for Inverse Reinforcement Learning,” in Proc. 17th Inter. Conf. Machine Learning, 2000, pp. 663–670.
[12] P. Abbeel and A. Y. Ng, “Apprenticeship learning via inverse reinforcement learning,” in Proc. 21st Inter. Conf. Machine Learning, 2004.
[13] N. D. Ratliff, J. A. Bagnell, and M. A. Zinkevich, “Maximum margin planning,” in Proc. 23rd Inter. Conf. Machine Learning, 2006, pp. 729–736.
[14] Y. Shen, W. Stannat, and K. Obermayer, “Risk-Sensitive Markov Control Processes,” SIAM J. Control Optimization, vol. 51, no. 5, pp. 3652–3672, 2013.
[15] E. Mazumdar, L. J. Ratliff, T. Fiez, and S. S. Sastry, “Gradient-based inverse risk-sensitive reinforcement learning,” in Proc. 56th IEEE Conf. Decision and Control, 2017.
[16] M. Heger, “Consideration of risk in reinforcement learning,” in Proc. 11th Inter. Conf. Machine Learning, 1994, pp. 105–111.
[17] D. Kahneman and A. Tversky, “Prospect theory: An analysis of decision under risk,” Econometrica, vol. 47, no. 2, pp. 263–291, 1979.
[18] A. Tversky and D. Kahneman, “Advances in prospect theory: Cumulative representation of uncertainty,” J. Risk and Uncertainty, vol. 5, no. 4, pp. 297–323, Oct 1992.
[19] R. Gonzalez and G. Wu, “On the shape of the probability weighting function,” Cognitive Psychology, vol. 38, no. 1, pp. 129–166, 1999.
[20] H. Simon, “Bounded rationality in social science: Today and tomorrow,” Mind & Society, vol. 1, no. 1, pp. 25–39, Mar. 2000.
[21] A. Tversky and D. Kahneman, “The framing of decisions and the psychology of choice,” Science, vol. 211, no. 4481, pp. 453–458, Jan. 1981.
[22] C. F. Camerer, “An experimental test of several generalized utility theories,” J. Risk and Uncertainty, vol. 2, no. 1, pp. 61–104, 1989.
[23] H. Fo¨llmer and A. Schied, “Convex measures of risk and trading constraints,” Finance and Stochastics, vol. 6, no. 4, pp. 429–447, 2002.
[24] S. P. Coraluppi and S. I. Marcus, “Mixed risk-neutral/minimax control of discrete-time, ﬁnite-state markov decision processes,” IEEE Trans. Autom. Control, vol. 45, no. 3, pp. 528–532, 2000.
[25] P. Artzner, F. Delbaen, J.-M. Eber, and D. Heath, “Coherent measures of risk,” Mathematical Finance, vol. 9, no. 3, pp. 203–228, 1999.
[26] H. Robbins and D. Siegmund, A Convergence Theorem for Non Negative Almost Supermartingales and Some Applications. Springer New York, 1985, pp. 111–135.
[27] H. Robbins and S. Monro, “A stochastic approximation method,” The Annals of Mathematical Statistics, vol. 22, no. 3, pp. 400–407, 1951.
[28] J. N. Tsitsiklis, “Asynchronous stochastic approximation and q-learning,” Machine Learning, vol. 16, no. 3, pp. 185–202, 1994.
[29] H. J. Kushner and G. G. Yin, Stochastic Approximation and Recursive Algorithms and Applications. Springer, 2003.
[30] G. Neu and C. Szepesva´ri, “Apprenticeship learning using inverse reinforcement learning and gradient methods,” in Proc. 23rd Conf. Uncertainty in Artiﬁcial Intelligence, 2007, pp. 295–302.
[31] J. Heinonen, “Lectures on Lipschitz Analysis,” 14th Jyva¨skyla¨ Summer School, 2004. [32] P. W. Millar, “Asymptotic minimax theorems for the sample distribution function,” Zeitschrift fu¨r Wahrschein-
lichkeitstheorie und Verwandte Gebiete, vol. 48, no. 3, pp. 233–252, 1979. [33] P. Massart, “The tight constant in the dvoretzky-kiefer-wolfowitz inequality,” The Annals of Probability, vol. 18,
pp. 1269–1283, 1990.

30
[34] A. Latif, Banach Contraction Principle and Its Generalizations. Springer International Publishing, 2014, pp. 33–64.
[35] S.-I. Amari, “Natural gradient works efﬁciently in learning,” Neural Computation, vol. 10, no. 2, pp. 251–276, 1998.
[36] J. Peters, S. Vijayakumar, and S. Schaal, “Natural actor-critic,” in Proc. 16th European Conf. Machine Learning, 2005, pp. 280–291.
[37] A. Y. Kruger, “On Fre´chet Subdifferentials,” J. Mathematical Sciences, vol. 116, no. 3, 2003. [38] J. Penot, “On the interchange of subdifferentiation and epi-convergence,” J. Mathematical Analysis and
Applications, vol. 196, no. 2, pp. 676–698, 1995. [39] S. S. Sastry, Nonlinear Systems. Springer, 1999.

