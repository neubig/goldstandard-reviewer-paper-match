END-TO-END MULTI-SPEAKER SPEECH RECOGNITION WITH TRANSFORMER
Xuankai Chang1, Wangyou Zhang2, Yanmin Qian2, Jonathan Le Roux3, Shinji Watanabe1
1Center for Language and Speech Processing, Johns Hopkins University, USA 2MoE Key Lab of Artiﬁcial Intelligence &
SpeechLab, Department of Computer Science and Engineering, Shanghai Jiao Tong University, China 3Mitsubishi Electric Research Laboratories (MERL), USA
xchang14@jhu.edu, {wyz-97, yanminqian}@sjtu.edu.cn, leroux@merl.com, shinjiw@jhu.edu

arXiv:2002.03921v2 [eess.AS] 13 Feb 2020

ABSTRACT
Recently, fully recurrent neural network (RNN) based endto-end models have been proven to be effective for multi-speaker speech recognition in both the single-channel and multi-channel scenarios. In this work, we explore the use of Transformer models for these tasks by focusing on two aspects. First, we replace the RNN-based encoder-decoder in the speech recognition model with a Transformer architecture. Second, in order to use the Transformer in the masking network of the neural beamformer in the multi-channel case, we modify the self-attention component to be restricted to a segment rather than the whole sequence in order to reduce computation. Besides the model architecture improvements, we also incorporate an external dereverberation preprocessing, the weighted prediction error (WPE), enabling our model to handle reverberated signals. Experiments on the spatialized wsj1-2mix corpus show that the Transformer-based models achieve 40.9% and 25.6% relative WER reduction, down to 12.1% and 6.4% WER, under the anechoic condition in single-channel and multi-channel tasks, respectively, while in the reverberant case, our methods achieve 41.5% and 13.8% relative WER reduction, down to 16.5% and 15.2% WER.
Index Terms— Transformer, end-to-end, overlapped speech recognition, neural beamforming, speech separation.
1. INTRODUCTION
Deep learning techniques have dramatically improved the performance of separation and automatic speech recognition (ASR) tasks related to the cocktail party problem [1], where the speech from multiple speakers overlaps. Two main scenarios are typically considered, single-channel and multi-channel. In single-channel speech separation, various methods have been proposed, among which deep clustering (DPCL) based methods [2] and permutation invariant training (PIT) based methods [3] are the dominant ones. For ASR, methods combining separation with single-speaker ASR as well as methods skipping the explicit separation step and building directly a multi-speaker speech recognition system have been proposed, using either the hybrid ASR framework [4–6] or the end-to-end ASR framework [7–9]. In the multi-channel condition, the spatial information derived from the inter-channel differences can help distinguish between speech sources from different directions, which makes the problem easier to solve. Several methods have been proposed for multi-channel speech separation, including DPCL-based
Wangyou Zhang and Yanmin Qian were supported by the China NSFC project No.U1736202.

methods using integrated beamforming [10] or inter-channel spatial features [11], and a PIT-based method using a multi-speaker mask-based beamformer [12]. For multi-channel multi-speaker speech recognition, an end-to-end system was proposed in [13], called MIMO-Speech because of the multi-channel input (MI) and multi-speaker output (MO). This system consists of a maskbased neural beamformer frontend, which explicitly separates the multi-speaker speech via beamforming, and an end-to-end speech recognition model backend based on the joint CTC/attention-based encoder-decoder [14] to recognize the separated speech streams. This end-to-end architecture is optimized via only the connectionist temporal classiﬁcation (CTC) and cross-entropy (CE) losses in the backend ASR, but is nonetheless able to learn to develop relatively good separation abilities.
Recently, Transformer models [15] have shown impressive performance in many tasks, such as pretrained language models [16,17], end-to-end speech recognition [18, 19], and speaker diarization [20], surpassing the long short-term memory recurrent neural networks (LSTM-RNNs) based models. One of the key components in the Transformer model is self-attention, which computes the contribution information of the whole input sequence and maps the sequence into a vector at every time step. Even though the Transformer model is powerful, it is usually not computationally practical when the sequence length is very long. It also needs adaptation for speciﬁc tasks, such as the subsampling operation in encoder-decoder based end-toend speech recognition. However, for signal-level processing tasks such as speech separation and enhancement, subsampling is usually not a good option, because these tasks need to maintain the original time resolution.
In this paper, we explore the use of Transformer models for endto-end multi-speaker speech recognition in both the single-channel and multi-channel scenarios. First, we replace the LSTMs in the encoder-decoder network of the speech recognition module with Transformers for both scenarios. Second, in order to also apply Transformers in the masking network of the neural beamforming module in the multi-channel case, we modify the self-attention layers to reduce their memory consumption in a time-restricted (or local) manner, as used in [5, 21, 22]. To the best of our knowledge, this work is the ﬁrst attempt to use the Transformer model for tasks such as speech enhancement/separation with such very long sequences. Another contribution of this paper is to improve the robustness of our model in reverberant environments. To do so, we incorporate an external dereverberation method, the weighed prediction error (WPE) [23], to preprocess the reverberated speech. The experiments show that this straightforward method can lead to a performance boost for reverberant speech.

X 1 < l a t e x i t s h a 1 _ b a s e 6 4 = " t + U 7 8 + Q a y k k x w p t J b p 8 H D m Q T n N g = " > A A A B 8 3 i c b V D L S s N A F L 2 p r 1 p f V Z d u B o v g q i R V 0 G X B j c s K 9 g F N K Z P p p B 0 6 m Y S Z G 6 G E / o Y b F 4 q 4 9 W f c + T d O 2 i y 0 9 c D A 4 Z x 7 u W d O k E h h 0 H W / n d L G 5 t b 2 T n m 3 s r d / c H h U P T 7 p m D j V j L d Z L G P d C 6 j h U i j e R o G S 9 x L N a R R I 3 g 2 m d 7 n f f e L a i F g 9 4 i z h g 4 i O l Q g F o 2 g l 3 4 8 o T o I w 6 8 2 H 3 r B a c + v u A m S d e A W p Q Y H W s P r l j 2 K W R l w h k 9 S Y v u c m O M i o R s E k n 1 f 8 1 P C E s i k d 8 7 6 l i k b c D L J F 5 j m 5 s M q I h L G 2 T y F Z q L 8 3 M h o Z M 4 s C O 5 l n N K t e L v 7 n 9 V M M b w e Z U E m K X L H l o T C V B G O S F 0 B G Q n O G c m Y J Z V r Y r I R N q K Y M b U 0 V W 4 K 3 + u V 1 0 m n U v a t 6 4 + G 6 1 m w U d Z T h D M 7 h E j y 4 g S b c Q w v a w C C B Z 3 i F N y d 1 X p x 3 5 2 M 5 W n K K n V P 4 A + f z B / X v k Z Q = < / l a t e x i t >

X 2 < l a t e x i t s h a 1 _ b a s e 6 4 = " M 5 l b 5 c t 8 n c c z u b 5 o M L v o y v x v W C 4 = " > A A A B 8 3 i c b V D L S s N A F L 2 p r 1 p f V Z d u B o v g q i R R 0 G X B j c s K 9 g F N K J P p p B 0 6 m Y R 5 C C X 0 N 9 y 4 U M S t P + P O v 3 H S Z q G t B w Y O 5 9 z L P X O i j D O l X f f b q W x s b m 3 v V H d r e / s H h 0 f 1 4 5 O u S o 0 k t E N S n s p + h B X l T N C O Z p r T f i Y p T i J O e 9 H 0 r v B 7 T 1 Q q l o p H P c t o m O C x Y D E j W F s p C B K s J 1 G c 9 + d D f 1 h v u E 1 3 A b R O v J I 0 o E R 7 W P 8 K R i k x C R W a c K z U w H M z H e Z Y a k Y 4 n d c C o 2 i G y R S P 6 c B S g R O q w n y R e Y 4 u r D J C c S r t E x o t 1 N 8 b O U 6 U m i W R n S w y q l W v E P / z B k b H t 2 H O R G Y 0 F W R 5 K D Y c 6 R Q V B a A R k 5 R o P r M E E 8 l s V k Q m W G K i b U 0 1 W 4 K 3 + u V 1 0 v W b 3 l X T f 7 h u t P y y j i q c w T l c g g c 3 0 I J 7 a E M H C G T w D K / w 5 h j n x X l 3 P p a j F a f c O Y U / c D 5 / A P d z k Z U = < / l a t e x i t >

X C < l a t e x i t s h a 1 _ b a s e 6 4 = " X M 9 W M 0 Q 8 / k j W e v 5 7 W c 5 z x 4 H h Y W w = " > A A A B 8 3 i c b V D L S s N A F L 3 x W e u r 6 t L N Y B F c l a Q K u i x 0 4 7 K C f U B T y m Q 6 a Y d O J m H m R i i h v + H G h S J u / R l 3 / o 2 T N g t t P T B w O O d e 7 p k T J F I Y d N 1 v Z 2 N z a 3 t n t 7 R X 3 j 8 4 P D q u n J x 2 T J x q x t s s l r H u B d R w K R R v o 0 D J e 4 n m N A o k 7 w b T Z u 5 3 n 7 g 2 I l a P O E v 4 I K J j J U L B K F r J 9 y O K k y D M e v N h c 1 i p u j V 3 A b J O v I J U o U B r W P n y R z F L I 6 6 Q S W p M 3 3 M T H G R U o 2 C S z 8 t + a n h C 2 Z S O e d 9 S R S N u B t k i 8 5 x c W m V E w l j b p 5 A s 1 N 8 b G Y 2 M m U W B n c w z m l U v F / / z + i m G d 4 N M q C R F r t j y U J h K g j H J C y A j o T l D O b O E M i 1 s V s I m V F O G t q a y L c F b / f I 6 6 d R r 3 n W t / n B T b d S L O k p w D h d w B R 7 c Q g P u o Q V t Y J D A M 7 z C m 5 M 6 L 8 6 7 8 7 E c 3 X C K n T P 4 A + f z B x F G k a Y = < / l a t e x i t >

O< l a t e x i t s h a 1 _ b a s e 6 4 = " w M w e F i N M s 6 v l T k E x G U 7 9 3 E e M x i 8 = " > A A A B 8 X i c b V D L S g M x F L 1 T X 7 W + q i 7 d B I v g q s x U Q Z c F N + 6 s Y B / Y D i W T 3 m l D M 5 k h y Q h l 6 F + 4 c a G I W / / G n X 9 j 2 s 5 C W w 8 E D u f c S 8 4 9 Q S K 4 N q 7 7 7 R T W 1 j c 2 t 4 r b p Z 3 d v f 2 D 8 u F R S 8 e p Y t h k s Y h V J 6 A a B Z f Y N N w I 7 C Q K a R Q I b A f j m 5 n f f k K l e S w f z C R B P 6 J D y U P O q L H S Y y + i Z h S E 2 d 2 0 X 6 6 4 V X c O s k q 8 n F Q g R 6 N f / u o N Y p Z G K A 0 T V O u u 5 y b G z 6 g y n A m c l n q p x o S y M R 1 i 1 1 J J I 9 R + N k 8 8 J W d W G Z A w V v Z J Q + b q 7 4 2 M R l p P o s B O z h L q Z W 8 m / u d 1 U x N e + x m X S W p Q s s V H Y S q I i c n s f D L g C p k R E 0 s o U 9 x m J W x E F W X G l l S y J X j L J 6 + S V q 3 q X V R r 9 5 e V u p v X U Y Q T O I V z 8 O A K 6 n A L D W g C A w n P 8 A p v j n Z e n H f n Y z F a c P K d Y / g D 5 / M H u t C Q 5 Q = = < / l a t e x i t >

EncMix < l a t e x i t s h a 1 _ b a s e 6 4 = " t H P Y 8 W F E w 4 g s 1 l 5 Q M 1 a 7 j E W l J 1 Y = " > A A A C A H i c b V D L S s N A F J 3 U V 6 2 v q A s X b g a L 4 K o k V d B l Q Q Q 3 Q g X 7 g D a E y X T S D p 0 8 m L m R l p C N v + L G h S J u / Q x 3 / o 3 T N A t t P T B w O O d e 7 p z j x Y I r s K x v o 7 S y u r a + U d 6 s b G 3 v 7 O 6 Z + w d t F S W S s h a N R C S 7 H l F M 8 J C 1 g I N g 3 V g y E n i C d b z x 9 c z v P D K p e B Q + w D R m T k C G I f c 5 J a A l 1 z z q A 5 t A e h P S z E 3 n / I 5 P s s w 1 q 1 b N y o G X i V 2 Q K i r Q d M 2 v / i C i S c B C o I I o 1 b O t G J y U S O B U s K z S T x S L C R 2 T I e t p G p K A K S f N A 2 T 4 V C s D 7 E d S v x B w r v 7 e S E m g 1 D T w 9 G R A Y K Q W v Z n 4 n 9 d L w L 9 y U h 7 G C T A d M T / k J w J D h G d t 4 A G X j I K Y a k K o 5 P q v m I 6 I J B R 0 Z x V d g r 0 Y e Z m 0 6 z X 7 v F a / v 6 g 2 r K K O M j p G J + g M 2 e g S N d A t a q I W o i h D z + g V v R l P x o v x b n z M R 0 t G s X O I / s D 4 / A E Y B Z d Q < / l a t e x i t >

E n c 1S D < l a t e x i t s h a 1 _ b a s e 6 4 = " Z E x 5 4 u 5 D h 9 n r k V q v e X 1 a T 1 D L f 2 g = " > A A A C A X i c b V D L S s N A F J 3 U V 6 2 v q B v B T b A I r k p S B V 0 W V H B Z 0 T 6 g j W E y n b R D J 5 M w c y O W E D f + i h s X i r j 1 L 9 z 5 N 0 7 T L r T 1 w M D h n H u 5 c 4 4 f c 6 b A t r + N w s L i 0 v J K c b W 0 t r 6 x u W V u 7 z R V l E h C G y T i k W z 7 W F H O B G 0 A A 0 7 b s a Q 4 9 D l t + c P z s d + 6 p 1 K x S N z C K K Z u i P u C B Y x g 0 J J n 7 n W B P k B 6 K U j m p R N + c 5 F l d 4 5 n l u 2 K n c O a J 8 6 U l N E U d c / 8 6 v Y i k o R U A O F Y q Y 5 j x + C m W A I j n G a l b q J o j M k Q 9 2 l H U 4 F D q t w 0 T 5 B Z h 1 r p W U E k 9 R N g 5 e r v j R S H S o 1 C X 0 + G G A Z q 1 h u L / 3 m d B I I z N 2 U i T o D q j P m h I O E W R N a 4 D q v H J C X A R 5 p g I p n + q 0 U G W G I C u r S S L s G Z j T x P m t W K c 1 y p X p + U a / a 0 j i L a R w f o C D n o F N X Q F a q j B i L o E T 2 j V / R m P B k v x r v x M R k t G N O d X f Q H x u c P O l u X U g = = < / l a t e x i t >

H1 < l a t e x i t s h a 1 _ b a s e 6 4 = " y l 9 n A Y w e 9 S 9 k W C h B V 2 J K v e q p 7 I A = " > A A A B 8 3 i c b V D L S g M x F L 2 p r 1 p f V Z d u g k V w V W a q o M u C m y 4 r 2 A d 0 x p J J M 2 1 o J j M k G a E M / Q 0 3 L h R x 6 8 + 4 8 2 / M t L P Q 1 g O B w z n 3 c k 9 O k A i u j e N 8 o 9 L G 5 t b 2 T n m 3 s r d / c H h U P T 7 p 6 j h V l H V o L G L V D 4 h m g k v W M d w I 1 k 8 U I 1 E g W C + Y 3 u V + 7 4 k p z W P 5 Y G Y J 8 y M y l j z k l B g r e V 5 E z C Q I s 9 b 8 0 R 1 W a 0 7 d W Q C v E 7 c g N S j Q H l a / v F F M 0 4 h J Q w X R e u A 6 i f E z o g y n g s 0 r X q p Z Q u i U j N n A U k k i p v 1 s k X m O L 6 w y w m G s 7 J M G L 9 T f G x m J t J 5 F g Z 3 M M + p V L x f / 8 w a p C W / 9 j M s k N U z S 5 a E w F d j E O C 8 A j 7 h i 1 I i Z J Y Q q b r N i O i G K U G N r q t g S 3 N U v r 5 N u o + 5 e 1 R v 3 1 7 W m U 9 R R h j M 4 h 0 t w 4 Q a a 0 I I 2 d I B C A s / w C m 8 o R S / o H X 0 s R 0 u o 2 D m F P 0 C f P 9 t g k Y E = < / l a t e x i t >

EncRe c < l a t e x i t s h a 1 _ b a s e 6 4 = " G w z c z o x H c I 3 j B 7 n U 9 s c t P 2 4 Z n 0 A = " > A A A C A H i c b V D L S s N A F J 3 4 r P U V d e H C T b A I r k p S B V 0 W R H B Z x T 6 g D W E y v W 2 H T h 7 M 3 I g l Z O O v u H G h i F s / w 5 1 / 4 z T N Q l s P D B z O u Z c 7 5 / i x 4 A p t + 9 t Y W l 5 Z X V s v b Z Q 3 t 7 Z 3 d s 2 9 / Z a K E s m g y S I R y Y 5 P F Q g e Q h M 5 C u j E E m j g C 2 j 7 4 6 u p 3 3 4 A q X g U 3 u M k B j e g w 5 A P O K O o J c 8 8 7 C E 8 Y n o d s s x L Z / w O W J Z 5 Z s W u 2 j m s R e I U p E I K N D z z q 9 e P W B J A i E x Q p b q O H a O b U o m c C c j K v U R B T N m Y D q G r a U g D U G 6 a B 8 i s E 6 3 0 r U E k 9 Q v R y t X f G y k N l J o E v p 4 M K I 7 U v D c V / / O 6 C Q 4 u 3 Z S H c Y K g I + a H B o m w M L K m b V h 9 L o G h m G h C m e T 6 r x Y b U U k Z 6 s 7 K u g R n P v I i a d W q z l m 1 d n t e q d t F H S V y R I 7 J K X H I B a m T G 9 I g T c J I R p 7 J K 3 k z n o w X 4 9 3 4 m I 0 u G c X O A f k D 4 / M H + Y S X P A = = < / l a t e x i t >

G1 < l a t e x i t s h a 1 _ b a s e 6 4 = " Z L Y A v C P O h z W s + 6 U e w e D d 7 W G C E h c = " > A A A B 8 3 i c b V D L S g M x F L 1 T X 7 W + q i 7 d B I v g q s x U Q Z c F F 7 q s Y B / Q G U s m z b S h m U x I M k I Z + h t u X C j i 1 p 9 x 5 9 + Y a W e h r Q c C h 3 P u 5 Z 6 c U H K m j e t + O 6 W 1 9 Y 3 N r f J 2 Z W d 3 b / + g e n j U 0 U m q C G 2 T h C e q F 2 J N O R O 0 b Z j h t C c V x X H I a T e c 3 O R + 9 4 k q z R L x Y K a S B j E e C R Y x g o 2 V f D / G Z h x G 2 e 3 s 0 R t U a 2 7 d n Q O t E q 8 g N S j Q G l S / / G F C 0 p g K Q z j W u u + 5 0 g Q Z V o Y R T m c V P 9 V U Y j L B I 9 q 3 V O C Y 6 i C b Z 5 6 h M 6 s M U Z Q o + 4 R B c / X 3 R o Z j r a d x a C f z j H r Z y 8 X / v H 5 q o u s g Y 0 K m h g q y O B S l H J k E 5 Q W g I V O U G D 6 1 B B P F b F Z E x l h h Y m x N F V u C t / z l V d J p 1 L 2 L e u P + s t Z 0 i z r K c A K n c A 4 e X E E T 7 q A F b S A g 4 R l e 4 c 1 J n R f n 3 f l Y j J a c Y u c Y / s D 5 / A H Z 2 Z G A < / l a t e x i t >

CTC
AttentionDecoder

E n c 2S D < l a t e x i t s h a 1 _ b a s e 6 4 = " / 1 B 3 S g + + 4 5 a r q j n e 5 e M f V V 5 b I N Y = " > A A A C A X i c b V D L S s N A F J 3 U V 6 2 v q B v B T b A I r k p S B V 0 W V H B Z 0 T 6 g j W E y n b R D J 5 M w c y O W E D f + i h s X i r j 1 L 9 z 5 N 0 7 T L r T 1 w M D h n H u 5 c 4 4 f c 6 b A t r + N w s L i 0 v J K c b W 0 t r 6 x u W V u 7 z R V l E h C G y T i k W z 7 W F H O B G 0 A A 0 7 b s a Q 4 9 D l t + c P z s d + 6 p 1 K x S N z C K K Z u i P u C B Y x g 0 J J n 7 n W B P k B 6 K U j m p R N + c 5 F l d 1 X P L N s V O 4 c 1 T 5 w p K a M p 6 p 7 5 1 e 1 F J A m p A M K x U h 3 H j s F N s Q R G O M 1 K 3 U T R G J M h 7 t O O p g K H V L l p n i C z D r X S s 4 J I 6 i f A y t X f G y k O l R q F v p 4 M M Q z U r D c W / / M 6 C Q R n b s p E n A D V G f N D Q c I t i K x x H V a P S U q A j z T B R D L 9 V 4 s M s M Q E d G k l X Y I z G 3 m e N K s V 5 7 h S v T 4 p 1 + x p H U W 0 j w 7 Q E X L Q K a q h K 1 R H D U T Q I 3 p G r + j N e D J e j H f j Y z J a M K Y 7 u + g P j M 8 f O 9 + X U w = = < / l a t e x i t >

H2 < l a t e x i t s h a 1 _ b a s e 6 4 = " c 9 S 6 D O j g Y J A R d V z K P p 6 u + x h e g L w = " > A A A B 8 3 i c b V D L S g M x F L 2 p r 1 p f V Z d u g k V w V W a q o M u C m y 4 r 2 A d 0 x p J J M 2 1 o J j M k G a E M / Q 0 3 L h R x 6 8 + 4 8 2 / M t L P Q 1 g O B w z n 3 c k 9 O k A i u j e N 8 o 9 L G 5 t b 2 T n m 3 s r d / c H h U P T 7 p 6 j h V l H V o L G L V D 4 h m g k v W M d w I 1 k 8 U I 1 E g W C + Y 3 u V + 7 4 k p z W P 5 Y G Y J 8 y M y l j z k l B g r e V 5 E z C Q I s 9 b 8 s T G s 1 p y 6 s w B e J 2 5 B a l C g P a x + e a O Y p h G T h g q i 9 c B 1 E u N n R B l O B Z t X v F S z h N A p G b O B p Z J E T P v Z I v M c X 1 h l h M N Y 2 S c N X q i / N z I S a T 2 L A j u Z Z 9 S r X i 7 + 5 w 1 S E 9 7 6 G Z d J a p i k y 0 N h K r C J c V 4 A H n H F q B E z S w h V 3 G b F d E I U o c b W V L E l u K t f X i f d R t 2 9 q j f u r 2 t N p 6 i j D G d w D p f g w g 0 0 o Q V t 6 A C F B J 7 h F d 5 Q i l 7 Q O / p Y j p Z Q s X M K f 4 A + f w D c 5 J G C < / l a t e x i t >

EncRe c < l a t e x i t s h a 1 _ b a s e 6 4 = " G w z c z o x H c I 3 j B 7 n U 9 s c t P 2 4 Z n 0 A = " > A A A C A H i c b V D L S s N A F J 3 4 r P U V d e H C T b A I r k p S B V 0 W R H B Z x T 6 g D W E y v W 2 H T h 7 M 3 I g l Z O O v u H G h i F s / w 5 1 / 4 z T N Q l s P D B z O u Z c 7 5 / i x 4 A p t + 9 t Y W l 5 Z X V s v b Z Q 3 t 7 Z 3 d s 2 9 / Z a K E s m g y S I R y Y 5 P F Q g e Q h M 5 C u j E E m j g C 2 j 7 4 6 u p 3 3 4 A q X g U 3 u M k B j e g w 5 A P O K O o J c 8 8 7 C E 8 Y n o d s s x L Z / w O W J Z 5 Z s W u 2 j m s R e I U p E I K N D z z q 9 e P W B J A i E x Q p b q O H a O b U o m c C c j K v U R B T N m Y D q G r a U g D U G 6 a B 8 i s E 6 3 0 r U E k 9 Q v R y t X f G y k N l J o E v p 4 M K I 7 U v D c V / / O 6 C Q 4 u 3 Z S H c Y K g I + a H B o m w M L K m b V h 9 L o G h m G h C m e T 6 r x Y b U U k Z 6 s 7 K u g R n P v I i a d W q z l m 1 d n t e q d t F H S V y R I 7 J K X H I B a m T G 9 I g T c J I R p 7 J K 3 k z n o w X 4 9 3 4 m I 0 u G c X O A f k D 4 / M H + Y S X P A = = < / l a t e x i t >

G2 < l a t e x i t s h a 1 _ b a s e 6 4 = " p C q 5 n Y a V Q a X K V j L 3 r g H d 3 o r U n I 0 = " > A A A B 8 3 i c b V D L S g M x F L 2 p r 1 p f V Z d u g k V w V W a q o M u C C 1 1 W s A / o j C W T Z t r Q T G Z I M k I Z + h t u X C j i 1 p 9 x 5 9 + Y a W e h r Q c C h 3 P u 5 Z 6 c I B F c G 8 f 5 R q W 1 9 Y 3 N r f J 2 Z W d 3 b / + g e n j U 0 X G q K G v T W M S q F x D N B J e s b b g R r J c o R q J A s G 4 w u c n 9 7 h N T m s f y w U w T 5 k d k J H n I K T F W 8 r y I m H E Q Z r e z x 8 a g W n P q z h x 4 l b g F q U G B 1 q D 6 5 Q 1 j m k Z M G i q I 1 n 3 X S Y y f E W U 4 F W x W 8 V L N E k I n Z M T 6 l k o S M e 1 n 8 8 w z f G a V I Q 5 j Z Z 8 0 e K 7 + 3 s h I p P U 0 C u x k n l E v e 7 n 4 n 9 d P T X j t Z 1 w m q W G S L g 6 F q c A m x n k B e M g V o 0 Z M L S F U c Z s V 0 z F R h B p b U 8 W W 4 C 5 / e Z V 0 G n X 3 o t 6 4 v 6 w 1 n a K O M p z A K Z y D C 1 f Q h D t o Q R s o J P A M r / C G U v S C 3 t H H Y r S E i p 1 j + A P 0 + Q P b X Z G B < / l a t e x i t >

CTC
AttentionDecoder

Lossctc < l a t e x i t s h a 1 _ b a s e 6 4 = " V M S g 1 q I g k 4 1 l t 9 P R v 6 n 8 V O 9 d N x Y = " > A A A C A X i c b V D L S s N A F J 3 4 r P U V d S O 4 G S y C q 5 J U Q Z c F N y 5 c V L A P a E O Y T C f t 0 M k k z N y I J c S N v + L G h S J u / Q t 3 / o 3 T N g t t P X D h z D n 3 M v e e I B F c g + N 8 W 0 v L K 6 t r 6 6 W N 8 u b W 9 s 6 u v b f f 0 n G q K G v S W M S q E x D N B J e s C R w E 6 y S K k S g Q r B 2 M r i Z + + 5 4 p z W N 5 B + O E e R E Z S B 5 y S s B I v n 3 Y A / Y A 2 U 2 s d e 5 n s w c F m u e + X X G q z h R 4 k b g F q a A C D d / + 6 v V j m k Z M A h V E 6 6 7 r J O B l R A G n g u X l X q p Z Q u i I D F j X U E k i p r 1 s e k G O T 4 z S x 2 G s T E n A U / X 3 R E Y i r c d R Y D o j A k M 9 7 0 3 E / 7 x u C u G l l 3 G Z p M A k n X 0 U p g J D j C d x 4 D 5 X j I I Y G 0 K o 4 m Z X T I d E E Q o m t L I J w Z 0 / e Z G 0 a l X 3 r F q 7 P a / U n S K O E j p C x + g U u e g C 1 d E 1 a q A m o u g R P a N X 9 G Y 9 W S / W u / U x a 1 2 y i p k D 9 A f W 5 w 8 t 4 Z f x < / l a t e x i t >

Permutation

⇡ˆ< l a t e x i t s h a 1 _ b a s e 6 4 = " I 4 D I R N + 1 k c 6 d B v S t a y u y d f H 4 Z 8 A = " > A A A B 8 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B g 5 S k C n o s e P F Y w X 5 I U 8 p m u 2 m X 7 i Z h d y K U 0 F / h x Y M i X v 0 5 3 v w 3 b t s c t P X B w O O 9 G W b m B Y k U B l 3 3 2 y m s r W 9 s b h W 3 S z u 7 e / s H 5 c O j l o l T z X i T x T L W n Y A a L k X E m y h Q 8 k 6 i O V W B 5 O 1 g f D v z 2 0 9 c G x F H D z h J e E / R Y S R C w S h a 6 d E f U c z 8 R E z 7 5 Y p b d e c g q 8 T L S Q V y N P r l L 3 8 Q s 1 T x C J m k x n Q 9 N 8 F e R j U K J v m 0 5 K e G J 5 S N 6 Z B 3 L Y 2 o 4 q a X z Q + e k j O r D E g Y a 1 s R k r n 6 e y K j y p i J C m y n o j g y y 9 5 M / M / r p h j e 9 D I R J S n y i C 0 W h a k k G J P Z 9 2 Q g N G c o J 5 Z Q p o W 9 l b A R 1 Z S h z a h k Q / C W X 1 4 l r V r V u 6 z W 7 q 8 q 9 Y s 8 j i K c w C m c g w f X U I c 7 a E A T G C h 4 h l d 4 c 7 T z 4 r w 7 H 4 v W g p P P H M M f O J 8 / G W 2 Q i A = = < / l a t e x i t >

Lossatt < l a t e x i t s h a 1 _ b a s e 6 4 = " L j F Y t h w d 5 4 5 p A Z X O C e / l j 6 s e S X o = " > A A A C A X i c b V D L S s N A F J 3 4 r P U V d S O 4 G S y C q 5 J U Q Z c F N y 5 c V L A P a E O Y T C f t 0 M k k z N y I J c S N v + L G h S J u / Q t 3 / o 3 T N g t t P X D h z D n 3 M v e e I B F c g + N 8 W 0 v L K 6 t r 6 6 W N 8 u b W 9 s 6 u v b f f 0 n G q K G v S W M S q E x D N B J e s C R w E 6 y S K k S g Q r B 2 M r i Z + + 5 4 p z W N 5 B + O E e R E Z S B 5 y S s B I v n 3 Y A / Y A 2 U 2 s d e 5 n s w c B y H P f r j h V Z w q 8 S N y C V F C B h m 9 / 9 f o x T S M m g Q q i d d d 1 E v A y o o B T w f J y L 9 U s I X R E B q x r q C Q R 0 1 4 2 v S D H J 0 b p 4 z B W p i T g q f p 7 I i O R 1 u M o M J 0 R g a G e 9 y b i f 1 4 3 h f D S y 7 h M U m C S z j 4 K U 4 E h x p M 4 c J 8 r R k G M D S F U c b M r p k O i C A U T W t m E 4 M 6 f v E h a t a p 7 V q 3 d n l f q T h F H C R 2 h Y 3 S K X H S B 6 u g a N V A T U f S I n t E r e r O e r B f r 3 f q Y t S 5 Z x c w B + g P r 8 w d E t 5 g A < / l a t e x i t >

Fig. 1. End-to-end single-channel multi-speaker model in the 2speaker case. The speaker-differentiating encoder (EncSD), recognition encoder (EncRec), and decoder are either RNNs or Transformers.

X 1 < l a t e x i t s h a 1 _ b a s e 6 4 = " t + U 7 8 + Q a y k k x w p t J b p 8 H D m Q T n N g = " > A A A B 8 3 i c b V D L S s N A F L 2 p r 1 p f V Z d u B o v g q i R V 0 G X B j c s K 9 g F N K Z P p p B 0 6 m Y S Z G 6 G E / o Y b F 4 q 4 9 W f c + T d O 2 i y 0 9 c D A 4 Z x 7 u W d O k E h h 0 H W / n d L G 5 t b 2 T n m 3 s r d / c H h U P T 7 p m D j V j L d Z L G P d C 6 j h U i j e R o G S 9 x L N a R R I 3 g 2 m d 7 n f f e L a i F g 9 4 i z h g 4 i O l Q g F o 2 g l 3 4 8 o T o I w 6 8 2 H 3 r B a c + v u A m S d e A W p Q Y H W s P r l j 2 K W R l w h k 9 S Y v u c m O M i o R s E k n 1 f 8 1 P C E s i k d 8 7 6 l i k b c D L J F 5 j m 5 s M q I h L G 2 T y F Z q L 8 3 M h o Z M 4 s C O 5 l n N K t e L v 7 n 9 V M M b w e Z U E m K X L H l o T C V B G O S F 0 B G Q n O G c m Y J Z V r Y r I R N q K Y M b U 0 V W 4 K 3 + u V 1 0 m n U v a t 6 4 + G 6 1 m w U d Z T h D M 7 h E j y 4 g S b c Q w v a w C C B Z 3 i F N y d 1 X p x 3 5 2 M 5 W n K K n V P 4 A + f z B / X v k Z Q = < / l a t e x i t > X 2 < l a t e x i t s h a 1 _ b a s e 6 4 = " M 5 l b 5 c t 8 n c c z u b 5 o M L v o y v x v W C 4 = " > A A A B 8 3 i c b V D L S s N A F L 2 p r 1 p f V Z d u B o v g q i R R 0 G X B j c s K 9 g F N K J P p p B 0 6 m Y R 5 C C X 0 N 9 y 4 U M S t P + P O v 3 H S Z q G t B w Y O 5 9 z L P X O i j D O l X f f b q W x s b m 3 v V H d r e / s H h 0 f 1 4 5 O u S o 0 k t E N S n s p + h B X l T N C O Z p r T f i Y p T i J O e 9 H 0 r v B 7 T 1 Q q l o p H P c t o m O C x Y D E j W F s p C B K s J 1 G c 9 + d D f 1 h v u E 1 3 A b R O v J I 0 o E R 7 W P 8 K R i k x C R W a c K z U w H M z H e Z Y a k Y 4 n d c C o 2 i G y R S P 6 c B S g R O q w n y R e Y 4 u r D J C c S r t E x o t 1 N 8 b O U 6 U m i W R n S w y q l W v E P / z B k b H t 2 H O R G Y 0 F W R 5 K D Y c 6 R Q V B a A R k 5 R o P r M E E 8 l s V k Q m W G K i b U 0 1 W 4 K 3 + u V 1 0 v W b 3 l X T f 7 h u t P y y j i q c w T l c g g c 3 0 I J 7 a E M H C G T w D K / w 5 h j n x X l 3 P p a j F a f c O Y U / c D 5 / A P d z k Z U = < / l a t e x i t >
X C < l a t e x i t s h a 1 _ b a s e 6 4 = " X M 9 W M 0 Q 8 / k j W e v 5 7 W c 5 z x 4 H h Y W w = " > A A A B 8 3 i c b V D L S s N A F L 3 x W e u r 6 t L N Y B F c l a Q K u i x 0 4 7 K C f U B T y m Q 6 a Y d O J m H m R i i h v + H G h S J u / R l 3 / o 2 T N g t t P T B w O O d e 7 p k T J F I Y d N 1 v Z 2 N z a 3 t n t 7 R X 3 j 8 4 P D q u n J x 2 T J x q x t s s l r H u B d R w K R R v o 0 D J e 4 n m N A o k 7 w b T Z u 5 3 n 7 g 2 I l a P O E v 4 I K J j J U L B K F r J 9 y O K k y D M e v N h c 1 i p u j V 3 A b J O v I J U o U B r W P n y R z F L I 6 6 Q S W p M 3 3 M T H G R U o 2 C S z 8 t + a n h C 2 Z S O e d 9 S R S N u B t k i 8 5 x c W m V E w l j b p 5 A s 1 N 8 b G Y 2 M m U W B n c w z m l U v F / / z + i m G d 4 N M q C R F r t j y U J h K g j H J C y A j o T l D O b O E M i 1 s V s I m V F O G t q a y L c F b / f I 6 6 d R r 3 n W t / n B T b d S L O k p w D h d w B R 7 c Q g P u o Q V t Y J D A M 7 z C m 5 M 6 L 8 6 7 8 7 E c 3 X C K n T P 4 A + f z B x F G k a Y = < / l a t e x i t >

mt,f,1
< l a t e x i t s h a 1 _ b a s e 6 4 = " x O r c V Z c E p C g R y g z X 4 D Q i 1 A M E T z s = " > A A A B + 3 i c b V D L S s N A F L 2 p r 1 p f s S 7 d B I v g o p S k C r o s u H F Z w T 6 g D W E y n b R D Z y Z h Z i K W k F 9 x 4 0 I R t / 6 I O / / G S d u F t h 4 Y O J x z L / f M C R N G l X b d b 6 u 0 s b m 1 v V P e r e z t H x w e 2 c f V r o p T i U k H x y y W / R A p w q g g H U 0 1 I / 1 E E s R D R n r h 9 L b w e 4 9 E K h q L B z 1 L i M / R W N C I Y q S N F N j V I U d 6 E k Y Z z 4 N M 1 6 O 6 l w d 2 z W 2 4 c z j r x F u S G i z R D u y v 4 S j G K S d C Y 4 a U G n h u o v 0 M S U 0 x I 3 l l m C q S I D x F Y z I w V C B O l J / N s + f O u V F G T h R L 8 4 R 2 5 u r v j Q x x p W Y 8 N J N F U r X q F e J / 3 i D V 0 Y 2 f U Z G k m g i 8 O B S l z N G x U x T h j K g k W L O Z I Q h L a r I 6 e I I k w t r U V T E l e K t f X i f d Z s O 7 b D T v r 2 q t + r K O M p z C G V y A B 9 f Q g j t o Q w c w P M E z v M K b l V s v 1 r v 1 s R g t W c u d E / g D 6 / M H 1 T C U O g = = < / l a t e x i t >
mt,f,2
< l a t e x i t s h a 1 _ b a s e 6 4 = " O O o C U 6 F w E R 6 8 u s V r O R C N m 5 4 x V R k = " > A A A B + 3 i c b V D L S s N A F L 2 p r 1 p f s S 7 d B I v g o p S k C r o s u H F Z w T 6 g D W U y n b R D Z y Z h Z i K W k F 9 x 4 0 I R t / 6 I O / / G S Z u F t h 4 Y O J x z L / f M C W J G l X b d b 6 u 0 s b m 1 v V P e r e z t H x w e 2 c f V r o o S i U k H R y y S / Q A p w q g g H U 0 1 I / 1 Y E s Q D R n r B 7 D b 3 e 4 9 E K h q J B z 2 P i c / R R N C Q Y q S N N L K r Q 4 7 0 N A h T n o 1 S X Q / r z W x k 1 9 y G u 4 C z T r y C 1 K B A e 2 R / D c c R T j g R G j O k 1 M B z Y + 2 n S G q K G c k q w 0 S R G O E Z m p C B o Q J x o v x 0 k T 1 z z o 0 y d s J I m i e 0 s 1 B / b 6 S I K z X n g Z n M k 6 p V L x f / 8 w a J D m / 8 l I o 4 0 U T g 5 a E w Y Y 6 O n L w I Z 0 w l w Z r N D U F Y U p P V w V M k E d a m r o o p w V v 9 8 j r p N h v e Z a N 5 f 1 V r 1 Y s 6 y n A K Z 3 A B H l x D C + 6 g D R 3 A 8 A T P 8 A p v V m a 9 W O / W x 3 K 0 Z B U 7 J / A H 1 u c P 1 r W U O w = = < / l a t e x i t >

Masking Network (Speeches and Noise)

m0t,f,C
< l a t e x i t s h a 1 _ b a s e 6 4 = " Y 3 N o 1 k k Y g 1 D c k q B W Y Q B M w O N w l a M = " > A A A B 9 H i c b V B N S w M x E J 2 t X 7 V + V T 1 6 C R b B Q y m 7 V d B j o R e P F e w H t G v J p t k 2 N M m u S b Z Q l v 4 O L x 4 U 8 e q P 8 e a / M W 3 3 o K 0 P B h 7 v z T A z L 4 g 5 0 8 Z 1 v 5 3 c x u b W 9 k 5 + t 7 C 3 f 3 B 4 V D w + a e k o U Y Q 2 S c Q j 1 Q m w p p x J 2 j T M c N q J F c U i 4 L Q d j O t z v z 2 h S r N I P p h p T H 2 B h 5 K F j G B j J V 8 8 p u 6 s n 5 p y W K 7 P + s W S W 3 E X Q O v E y 0 g J M j T 6 x a / e I C K J o N I Q j r X u e m 5 s / B Q r w w i n s 0 I v 0 T T G Z I y H t G u p x I J q P 1 0 c P U M X V h m g M F K 2 p E E L 9 f d E i o X W U x H Y T o H N S K 9 6 c / E / r 5 u Y 8 N Z P m Y w T Q y V Z L g o T j k y E 5 g m g A V O U G D 6 1 B B P F 7 K 2 I j L D C x N i c C j Y E b / X l d d K q V r y r S v X + u l Q r Z 3 H k 4 Q z O 4 R I 8 u I E a 3 E E D m k D g C Z 7 h F d 6 c i f P i v D s f y 9 a c k 8 2 c w h 8 4 n z 8 2 C J G p < / l a t e x i t >
m1t,f,C
< l a t e x i t s h a 1 _ b a s e 6 4 = " x 8 r z e d 6 j N o b 8 F X I D 6 T n Z r P X 0 S l w = " > A A A B 9 H i c b V B N S w M x E J 2 t X 7 V + V T 1 6 C R b B Q y m 7 V d B j o R e P F e w H t G v J p t k 2 N M m u S b Z Q l v 4 O L x 4 U 8 e q P 8 e a / M W 3 3 o K 0 P B h 7 v z T A z L 4 g 5 0 8 Z 1 v 5 3 c x u b W 9 k 5 + t 7 C 3 f 3 B 4 V D w + a e k o U Y Q 2 S c Q j 1 Q m w p p x J 2 j T M c N q J F c U i 4 L Q d j O t z v z 2 h S r N I P p h p T H 2 B h 5 K F j G B j J V 8 8 p t 6 s n 5 p y W K 7 P + s W S W 3 E X Q O v E y 0 g J M j T 6 x a / e I C K J o N I Q j r X u e m 5 s / B Q r w w i n s 0 I v 0 T T G Z I y H t G u p x I J q P 1 0 c P U M X V h m g M F K 2 p E E L 9 f d E i o X W U x H Y T o H N S K 9 6 c / E / r 5 u Y 8 N Z P m Y w T Q y V Z L g o T j k y E 5 g m g A V O U G D 6 1 B B P F 7 K 2 I j L D C x N i c C j Y E b / X l d d K q V r y r S v X + u l Q r Z 3 H k 4 Q z O 4 R I 8 u I E a 3 E E D m k D g C Z 7 h F d 6 c i f P i v D s f y 9 a c k 8 2 c w h 8 4 n z 8 3 l Z G q < / l a t e x i t >
m2t,f,C
< l a t e x i t s h a 1 _ b a s e 6 4 = " w + q I P G o 9 0 5 d 3 Y L J o R o 9 w 8 Y D 2 T G Q = " > A A A B 9 H i c b V B N S w M x E J 2 t X 7 V + V T 1 6 C R b B Q y m 7 V d B j o R e P F e w H t G v J p t k 2 N M m u S b Z Q l v 4 O L x 4 U 8 e q P 8 e a / M W 3 3 o K 0 P B h 7 v z T A z L 4 g 5 0 8 Z 1 v 5 3 c x u b W 9 k 5 + t 7 C 3 f 3 B 4 V D w + a e k o U Y Q 2 S c Q j 1 Q m w p p x J 2 j T M c N q J F c U i 4 L Q d j O t z v z 2 h S r N I P p h p T H 2 B h 5 K F j G B j J V 8 8 p t V Z P z X l s F y f 9 Y s l t + I u g N a J l 5 E S Z G j 0 i 1 + 9 Q U Q S Q a U h H G v d 9 d z Y + C l W h h F O Z 4 V e o m m M y R g P a d d S i Q X V f r o 4 e o Y u r D J A Y a R s S Y M W 6 u + J F A u t p y K w n Q K b k V 7 1 5 u J / X j c x 4 a 2 f M h k n h k q y X B Q m H J k I z R N A A 6 Y o M X x q C S a K 2 V s R G W G F i b E 5 F W w I 3 u r L 6 6 R V r X h X l e r 9 d a l W z u L I w x m c w y V 4 c A M 1 u I M G N I H A E z z D K 7 w 5 E + f F e X c + l q 0 5 J 5 s 5 h T 9 w P n 8 A O S K R q w = = < / l a t e x i t >

MVDR Filtering

Xet,nfh,1
< l a t e x i t s h a 1 _ b a s e 6 4 = " H g I H o 0 X 6 o r S I h J U K r Y x P 8 E m P s 8 8 = " > A A A C C X i c b V A 9 S w N B E N 2 L X z F + R S 1 t F o N g E c J d F L Q M 2 F g q G B N I Y t j b z C V L 9 v a O 3 T k x H N f a + F d s L B S x 9 R / Y + W / c S 1 L 4 9 W D g 8 d 4 M M / P 8 W A q D r v v p F B Y W l 5 Z X i q u l t f W N z a 3 y 9 s 6 1 i R L N o c k j G e m 2 z w x I o a C J A i W 0 Y w 0 s 9 C W 0 / P F Z 7 r d u Q R s R q S u c x N A L 2 V C J Q H C G V u q X a T d k O P K D t J 3 d p F 2 E O 0 x B j b I q 9 b J + i t U g 6 5 c r b s 2 d g v 4 l 3 p x U y B w X / f J H d x D x J A S F X D J j O p 4 b Y y 9 l G g W X k J W 6 i Y G Y 8 T E b Q s d S x U I w v X T 6 S U Y P r D K g Q a R t K a R T 9 f t E y k J j J q F v O / O 7 z W 8 v F / / z O g k G p 7 1 U q D h B U H y 2 K E g k x Y j m s d C B 0 M B R T i x h X A t 7 K + U j p h l H G 1 7 J h u D 9 f v k v u a 7 X v K N a / f K 4 0 q j O 4 y i S P b J P D o l H T k i D n J M L 0 i S c 3 J N H 8 k x e n A f n y X l 1 3 m a t B W c + s 0 t + w H n / A h A P m n s = < / l a t e x i t >

Feature Extractor

O 1 < l a t e x i t s h a 1 _ b a s e 6 4 = " 6 w / t 6 F Z s e I 7 W 3 c 3 v T b a p J 1 L E R j U = " > A A A B 9 X i c b V D L S s N A F L 2 p r 1 p f V Z d u B o v g Q k r S C r o s u H F n B f u A N i 2 T 6 a Q d O p m E m Y l S Q v 7 D j Q t F 3 P o v 7 v w b J 2 0 W 2 n p g 4 H D O v d w z x 4 s 4 U 9 q 2 v 6 3 C 2 v r G 5 l Z x u 7 S z u 7 d / U D 4 8 a q s w l o S 2 S M h D 2 f W w o p w J 2 t J M c 9 q N J M W B x 2 n H m 9 5 k f u e R S s V C 8 a B n E X U D P B b M Z w R r I w 3 6 A d Y T z 0 / u 0 k H i p M N y x a 7 a c 6 B V 4 u S k A j m a w / J X f x S S O K B C E 4 6 V 6 j l 2 p N 0 E S 8 0 I p 2 m p H y s a Y T L F Y 9 o z V O C A K j e Z p 0 7 R m V F G y A + l e U K j u f p 7 I 8 G B U r P A M 5 N Z S r X s Z e J / X i / W / r W b M B H F m g q y O O T H H O k Q Z R W g E Z O U a D 4 z B B P J T F Z E J l h i o k 1 R J V O C s / z l V d K u V Z 1 6 t X Z / W W l c 5 H U U 4 Q R O 4 R w c u I I G 3 E I T W k B A w j O 8 w p v 1 Z L 1 Y 7 9 b H Y r R g 5 T v H 8 A f W 5 w + w 3 p K Q < / l a t e x i t >

Xet,nfh,2
< l a t e x i t s h a 1 _ b a s e 6 4 = " o o J d N n M y 8 I y r 5 v I L 3 e U D y R h D X h Q = " > A A A C C X i c b V A 9 S w N B E N 3 z 2 / g V t b R Z D I J F C H d R 0 D J g Y 6 l g P i A X j 7 3 N n F n c 2 z t 2 5 8 R w X G v j X 7 G x U M T W f 2 D n v 3 H z U a j x w c D j v R l m 5 o W p F A Z d 9 8 u Z m 1 9 Y X F p e W S 2 t r W 9 s b p W 3 d 1 o m y T S H J k 9 k o j s h M y C F g i Y K l N B J N b A 4 l N A O b 8 9 G f v s O t B G J u s J h C r 2 Y 3 S g R C c 7 Q S k G Z + j H D Q R j l n e I 6 9 x H u M Q c 1 K K q 0 X g Q 5 V q M i K F f c m j s G n S X e l F T I F B d B + d P v J z y L Q S G X z J i u 5 6 b Y y 5 l G w S U U J T 8 z k D J + y 2 6 g a 6 l i M Z h e P v 6 k o A d W 6 d M o 0 b Y U 0 r H 6 c y J n s T H D O L S d o 7 v N X 2 8 k / u d 1 M 4 x O e 7 l Q a Y a g + G R R l E m K C R 3 F Q v t C A 0 c 5 t I R x L e y t l A + Y Z h x t e C U b g v f 3 5 V n S q t e 8 o 1 r 9 8 r j S q E 7 j W C F 7 Z J 8 c E o + c k A Y 5 J x e k S T h 5 I E / k h b w 6 j 8 6 z 8 + a 8 T 1 r n n O n M L v k F 5 + M b E Z q a f A = = < / l a t e x i t >

Feature Extractor

O 2 < l a t e x i t s h a 1 _ b a s e 6 4 = " G d M x f L n 7 T 8 4 S L C 9 l g 6 u 7 M / l f 5 D Y = " > A A A B 9 X i c b V D L S s N A F L 2 p r 1 p f V Z d u B o v g Q k r S C r o s u H F n B f u A N i 2 T 6 a Q d O p m E m Y l S Q v 7 D j Q t F 3 P o v 7 v w b J 2 0 W 2 n p g 4 H D O v d w z x 4 s 4 U 9 q 2 v 6 3 C 2 v r G 5 l Z x u 7 S z u 7 d / U D 4 8 a q s w l o S 2 S M h D 2 f W w o p w J 2 t J M c 9 q N J M W B x 2 n H m 9 5 k f u e R S s V C 8 a B n E X U D P B b M Z w R r I w 3 6 A d Y T z 0 / u 0 k F S S 4 f l i l 2 1 5 0 C r x M l J B X I 0 h + W v / i g k c U C F J h w r 1 X P s S L s J l p o R T t N S P 1 Y 0 w m S K x 7 R n q M A B V W 4 y T 5 2 i M 6 O M k B 9 K 8 4 R G c / X 3 R o I D p W a B Z y a z l G r Z y 8 T / v F 6 s / W s 3 Y S K K N R V k c c i P O d I h y i p A I y Y p 0 X x m C C a S m a y I T L D E R J u i S q Y E Z / n L q 6 R d q z r 1 a u 3 + s t K 4 y O s o w g m c w j k 4 c A U N u I U m t I C A h G d 4 h T f r y X q x 3 q 2 P x W j B y n e O 4 Q + s z x + y Y 5 K R < / l a t e x i t >

E2E ASR
E2E ASR

Masking Network and Neural Beamformer
Fig. 2. End-to-end multi-channel multi-speaker model in the 2speaker case. The masking network and end-to-end ASR network are based on either RNNs or Transformers.

2. END-TO-END MULTI-SPEAKER ASR
In this section, we review the end-to-end speech recognition models for both the single-channel [8, 9] and multi-channel [13] tasks. For both tasks, we denote by J the number of speakers in the input speech mixture.

2.1. Single-channel Multi-speaker ASR

In this subsection, we brieﬂy introduce the end-to-end singlechannel multi-speaker speech recognition model proposed in [8, 9], shown in Fig.1. The model is an extension of the joint CTC/attentionbased encoder-decoder framework [14] to recognize multi-speaker speech. The input O = {o1, . . . , oT } is the single-channel mixed speech feature. In the encoder, the input feature is separated and encoded as hidden states Gj, j = {1, . . . , J} for each speaker. The computation of the encoder can be divided into three submodules:

H = EncoderMix(O),

(1)

Hj = EncoderjSD(H), j = 1, . . . , J, (2)

Gj = EncoderRec(Hj ), j = 1, . . . , J.

(3)

EncoderMix ﬁrst maps the input O to some high dimensional representation H. Then J speaker-differentiating encoders EncoderjSD extract each speaker’s speech Hj. Finally, EncoderRec transforms each Hj into the embeddings Gj = {g1j , . . . , gLj }, L ≤ T with sub-

sampling. The attention-based decoder then takes these hidden rep-

resentations to generate the corresponding output token sequences

Yj

=

{

y1j

,

.

.

.

,

y

j N

}

.

For

each

embedding

sequence

Gj ,

the

recog-

nition process is formalized as follows:

cjn = Attention(ejn−1, Gj ),

(4)

ejn = Update(ejn−1, cjn−1, ynj −1),

(5)

ynj ∼ Decoder(ejn, ynj −1),

(6)

in which cjn denotes the context vector and ejn is the hidden state of the decoder at step n. To determine the permutation of the reference sequences Rj, permutation invariant training (PIT) is performed on
the CTC loss right after the encoder [8, 9]:

πˆ = argmin Lossctc(Zj , Rπ(j)), j = 1, . . . , J, (7)
π∈P j

where Zj is the sequence obtained from Gj by linear transform to compute the label posterior distribution, P is the set of all permutations on {1, . . . , J}, and π(i) is the i-th element of permutation π. The model is optimized with both CTC and cross-entropy losses:

L=
j

λLossctc(Zj , Rπˆ(j)) + (1 − λ)Lossatt(Yj , Rπˆ(j)) , (8)

where 0 ≤ λ ≤ 1 is an interpolation factor, and Lossatt is the crossentropy loss of the attention-decoder.

2.2. Multi-channel Multi-speaker ASR

In this subsection, we review the model architecture of the MIMOSpeech end-to-end multi-channel multi-speaker speech recognition system [13], shown in Fig.2. The model takes as input the microphone-array signals from an arbitrary number C of sensors. The model can be roughly divided into two modules, namely the frontend and the backend. The frontend is a maskbased multi-source neural beamformer. For simplicity of notation, we denote the noise as the 0-th source in the mixture signals. First, the monaural masking network estimates the masks Mjc for every source j = 0, 1, . . . , J on each channel c = 1, . . . , C from the complex STFT of the multi-channel mixture speech, Xc = (xt,f,c)t,f ∈ CT ×F , where 1 ≤ t ≤ T and 1 ≤ f ≤ F represent the time and frequency indices, as follows:

Mc = MaskNet(Xc),

(9)

where Mc = (mjt,f,c)t,f,j ∈ [0, 1]T ×F ×(J+1). Second, the multisource neural beamformer separates each source from the mixture
based on the MVDR formalization [24]. The estimated masks of
each source are used to compute the corresponding power spectral density (PSD) matrices Φj for j ∈ {0, . . . , J} [25–27]:

Φj(f ) =

1

T j

H

Tt=1 mjt,f t=1 mt,f xt,f xt,f

∈ CC×C ,

(10)

where xt,f

=

(xt,f ,c )c

∈

CC , mjt,f

=

1 C

C c=1

mjt,f,c

and

H

rep-

resents the conjugate transpose. The time-invariant ﬁlter coefﬁcients

gj(f ) for each speaker j are then computed from the PSD matrices:

(

Φi(f ))−1Φj (f )

gj(f ) =

i=j

u ∈ CC , (11)

Tr(( i=j Φi(f ))−1Φj (f ))

where 1 ≤ j ≤ J, and u ∈ RC is a vector representing the reference

microphone derived from an attention mechanism [28]. The beam-

forming

ﬁlters

gj

can

be

used

to

obtain

the

enhanced

signal

sˆj t,f

for

speaker j, which is further processed to get the log mel-ﬁlterbank

with global mean and variance normalization (LMF(·)):

sˆj t,f

=

(gj (f ))H xt,f

∈ C,

(12)

Oj = LMF(|Sˆj|)),

(13)

where Sˆj is the short-time Fourier transform (STFT) of sˆj.

The backend ASR module maps the speech feature Oj =

{oj1, . . . , ojT } of each speaker j into the output token sequences

Yj

=

{y

j 1

,

.

.

.

,

y

j N

}

.

The

computation

of

the

speech

recognition

is

very similar to the process for the single-channel case described in

Sec. 2.1, except that the encoder is a single path network and does

not have to separate the input feature using EncoderSD.

Similar to the single-channel model, the permutation order of the reference sequences Rj is determined by (7). The whole MIMO-
Speech model is optimized only with ASR loss as in (8).

3. TRANSFORMER WITH TIME-RESTRICTED SELF-ATTENTION

In this section, we describe one of the key components in the Transformer architecture, the multi-head self-attention [15], and the timerestricted modiﬁcation [22] for its application in the masking network of the frontend.
Transformers employ the dot-product self-attention for mapping a variable-length input sequence to another sequence of the same length, making them different from RNNs. The input consists of queries Q, keys K, and values V of dimension datt. The weights of the self-attention are obtained by computing the dot-product between the qu√ery and all keys and normalizing with softmax. A scaling factor datt is used to smooth the distribution:

QK T

Attention(Q, K, V ) = softmax √ V.

(14)

datt

To capture information from different representation subspaces, multi-head attention (MHA) is used by multiplying the original queries, keys, and values by different weight matrices:

MHA(Q, K, V ) = Concat([Hh]dhh=ea1d )W head,

(15)

where Hh = Attention(QWhq, KWhk, VhvWhv), (16)

where dhead is the number of heads, and W head ∈ R(dheaddatt)×datt and Whq, Whk, Whv ∈ Rdatt×datt are learnable parameters.
In general, the speech sequence length can be considerably long, making self-attention computationally difﬁcult. For tasks like speech separation and enhancement, the technique of subsampling is not practical as in speech recognition. Inspired by [21, 22], we adjust the self-attention of the Transformers in the masking network to be performed on a local segment of the speech, because those frames have higher correlation. This time-restricted self-attention for the query at time step t is formalized as:

QK T

Attention(Q, K , V ) = softmax √ V ,

(17)

datt

where the corresponding keys and values are K = Kt−l:t+r and V = Vt−l:t+r, respectively, with l and r here denoting the left and right context window sizes.

4. EXPERIMENTS
The proposed methods were evaluated on the same dataset as in [13], referred to as the spatialized wsj1-2mix dataset, where the number of speakers in an utterance is J = 2. The multi-channel speech signals were generated1 from the monaural wsj1-2mix speech used in [8, 9]. The room impulse responses (RIR) for the spatialization were randomly generated2, characterizing the room dimensions, speaker locations, and microphone geometry. The ﬁnal spatialized dataset contains two different environment conditions, anechoic and reverberant. In the anechoic condition, the room is assumed to be anechoic
1The spatialization toolkit is available at http://www.merl.com/ demos/deep-clustering/spatialize_wsj0-mix.zip
2The RIR generator script is available online at https://github. com/ehabets/RIR-Generator

and only the delays and decays due to the propagation are considered when generating the signals. In the reverberant condition, reverberation is also considered, with randomly drawn T60s from [0.2, 0.6] s. In total, the spatialized corpus under each condition contains 98.5 hr, 1.3 hr, and 0.8 hr in training, development, and evaluation sets respectively.
In the single-channel multi-speaker speech recognition task, we used the 1st channel of the training, development, and evaluation set to train, validate, and evaluate our model respectively. The input features are 80-dimensional log mel-ﬁlterbank coefﬁcients with pitch features and their delta and delta delta features. In the multi-channel multi-speaker speech recognition task, we also followed [13] in including the WSJ train si284 in the training set to improve the performance. The model takes the raw waveform audio signal as input and converts it to its STFT using a 25 ms-long Hann window with stride 10 ms. The spectral feature dimension is F = 257 due to zero-padding. After the frontend computation, 80-dimensional log ﬁlterbank features are extracted for each separated speech signal and global mean-variance normalization is applied, using the statistics of the single-speaker WSJ1 training set. All the multi-channel experiments were performed with C = 2 channels. However, the model can be extended to an arbitrary number of input channels as described in [28].
4.1. Experimental Setup
All the proposed end-to-end multi-speaker speech recognition models are implemented with the ESPnet framework [29] using the Pytorch backend. Some basic parts are the same for all the models. The interpolation factor λ of the loss function in (8) is set to 0.2. The word-level language model [30] used during decoding was trained with the ofﬁcial text data included in the WSJ corpus. The conﬁgurations of the RNN-based models are the same as in [9] and [13] for single-channel and multi-channel experiments, respectively.
In the Transformer-based multi-speaker encoder-decoder ASR model, there is a total of 12 layers in the encoder and 6 layers in the decoder as in [18]. Before the Transformer encoder, the log melﬁlterbank features are encoded by two CNN blocks. The CNN layers have a kernel size of 3 × 3 and the number of feature maps is 64 in the ﬁrst block and 128 in the second block. For the singlechannel multi-speaker model in Sec. 2.1, EncoderMix is the same as the CNN embedding layer, and EncoderSD and EncoderRec contain 4 and 8 Transformer layers, respectively. For all the tasks, the conﬁguration of each encoder-decoder layer is datt = 256, dff = 2048, dhead = 4. The masking network in the frontend has 3 layers similar to the encoder-decoder layer except dff = 768. The training stage of Transformer runs with the Adam optimizer and Noam learning rate decay as in [15]. Note that the backend ASR module is currently initialized with a pretrained model from the ESPnet recipe of WSJ corpus and kept frozen for the ﬁrst 15 epochs, for training stability.
4.2. Performance in Anechoic Condition
We ﬁrst provide in Table 1 the performance in anechoic condition of the single-channel multi-speaker end-to-end ASR models trained and evaluated on the original single-channel wsj1-2mix corpus used in [9, 30]. All the layers are randomly initialized. The result shows that using the Transformer model leads to a 40.9% relative word error rate (WER) improvement on the evaluation set, decreasing from 20.43% to 12.08% compared with the RNN-based model in [9].
The multi-channel multi-speaker speech recognition performance is shown in Table 2 using the spatialized anechoic wsj12mix dataset. The baseline multi-channel system is the RNN-based

Table 1. Performance in terms of average WER [%] on the singlechannel anechoic wsj1-2mix corpus.

Model

dev eval

RNN-based 1-channel Model [9]

24.90 20.43

Transformer-based 1-channel Model 17.11 12.08

Table 2. Performance in terms of average WER [%] on the spatialized two-channel anechoic wsj1-2mix corpus.

Model

dev eval

RNN-based MIMO-Speech [13] 13.54 8.62

+ Transformer backend ++ Transformer frontend

10.73 6.85 11.75 6.41

model from our previous study [13]. Before we move to the fully Transformer-based MIMO-Speech model, we ﬁrst replace the RNNs with Transformers in the backend ASR only. We see that using Transformers for the ASR backend can achieve 20.5% relative improvement against the RNN-based model in anechoic conditions.
We then also apply Transformers in the masking network of the frontend. Considering the feasibility of computing, in this preliminary study, the left and right context window sizes of the selfattention are set to l = 14 and r = 15. The parameters of the frontend are randomly initialized. Compared with using a Transformerbased model only for the backend, the fully Transformer-based model leads to a further improvement, achieving a WER of 6.41%. Compared against the whole sequence information available in the RNN-based model, such a small context window greatly limits the power of our model but shows its potential. Overall, the proposed fully Transformer-based model achieves a 25.6% relative WER improvement against the RNN-based model in the multi-channel case. We also see that the multi-channel system is better than the singlechannel system, thanks to the availability of spatial information.

4.3. Performance in Reverberant Condition
Even though our model can perform very well in anechoic condition, such ideal environments are rarely encountered in practice. It is thus crucial to investigate whether the model can be applied in more realistic environments. In this subsection, we describe preliminary efforts to process the reverberated signal.
We ﬁrst used a straightforward multi-conditioned training by adding reverberated utterances into the training set. The results of multi-speaker speech recognition on the multi-channel reverberant datasets are shown in Table 3. It can be observed that only using the Transformers for the backend is 6.6% better than the RNN-based model. In addition, the fully Transformer-based model achieves 13.2% relative WER improvement on the evaluation set, which is consistent with the anechoic case. However, comparing with the numbers for the anechoic condition in Table 2, a large performance degradation can be observed.
To alleviate this, we turned to an existing external dereverberation method to preprocess the input signals as a simple yet effective solution. Nara-WPE [31] is a widely used open source software for blind dereverberation of acoustic signals. The dereverberation is performed on the reverberated speech before it is added to the training dataset with anechoic data. Similarly, the reverberant test set is also preprocessed. Speech recognition performance on the multichannel reverberant speech after Nara-WPE is shown in Table 4. In general, the WERs are dramatically decreased with the dereverbera-

Table 3. Performance in terms of average WER [%] on the spatialized two-channel reverberant wsj1-2mix corpus.

Model

dev eval

RNN-based MIMO-Speech [13] 34.98 29.99

+ Transformer backend ++ Transformer frontend

32.95 28.01 31.93 26.02

Table 4. Performance in terms of average WER [%] on the spatialized two-channel reverberant wsj1-2mix corpus after Nara-WPE.

Model

dev eval

RNN-based MIMO-Speech 24.45 17.67

+ Transformer backend

19.17 15.24

++ Transformer frontend 20.55 15.46

tion method. For the RNN-based model, the WER on the evaluation set decreased by 41.1% relative, from 29.99% to 17.67%. Similar to the experiments under other conditions, the model with backend Transformer only is better than the RNN-based baseline model on the reverberant evaluation set by 13.8% relative WER. However, the Transformer-based frontend slightly degraded the performance. This may be due the window size of the attention being too small, as it only covers about 0.3 s of speech. Note that our systems are not trained through Nara-WPE, which is left for future work.
At last, we show results in the single-channel task with the 1st channel of the reverberated speech after Nara-WPE dereverberation in Table 5. Using the RNN-based model, the WER of the evaluation set is high, at 28.21%, which is inﬂuenced greatly by the reverberation, even when preprocessing with the dereverberation technique. However, the Transformer-based model can reach a ﬁnal WER of 16.50%, a 41.5% relative reduction, proving that the Transformerbased model is more robust than the RNN-based model.

Table 5. Performance in terms of average WER [%] on the 1st channel of the spatialized reverberant wsj1-2mix corpus after NaraWPE.

Model

dev eval

RNN-based 1-channel Model

31.21 28.21

Transformer-based 1-channel Model 20.44 16.50

5. CONCLUSION
In this paper, we applied Transformer models for end-to-end multispeaker ASR in both the single-channel and multi-channel scenarios, and observed consistent improvements. The RNN-based ASR module is replaced with the Transformers. To alleviate the fatal memory consumption issue when applying Transformers in the frontend with considerably long sequences, we modiﬁed the self-attention in the Transformers of the masking network by using a local context window. Furthermore, by incorporating an external dereverberation method, we largely reduced the performance gap between the reverberant condition and the anechoic condition, and hope to further reduce it in the future thanks to tighter integration of the dereverberation within our model.

6. REFERENCES
[1] E. C. Cherry, “Some experiments on the recognition of speech, with one and with two ears,” The Journal of the Acoustical Society of America, vol. 25, no. 5, pp. 975–979, 1953.
[2] J. R. Hershey, Z. Chen, J. Le Roux, and S. Watanabe, “Deep clustering: Discriminative embeddings for segmentation and separation,” in Proc. IEEE ICASSP, 2016, pp. 31–35.
[3] D. Yu, M. Kolbæk, Z.-H. Tan, and J. Jensen, “Permutation invariant training of deep models for speaker-independent multitalker speech separation,” in Proc. IEEE ICASSP, Mar. 2017.
[4] D. Yu, X. Chang, and Y. Qian, “Recognizing multi-talker speech with permutation invariant training,” in Proc. ISCA Interspeech, 2017, pp. 2456–2460.
[5] X. Chang, Y. Qian, and D. Yu, “Monaural multi-talker speech recognition with attention mechanism and gated convolutional networks,” in Proc. ISCA Interspeech, 2018.
[6] T. Menne, I. Sklyar, R. Schlu¨ter, and H. Ney, “Analysis of deep clustering as preprocessing for automatic speech recognition of sparsely overlapping speech,” in Proc. ISCA Interspeech, 2019, pp. 2638–2642.
[7] S. Settle, J. Le Roux, T. Hori, S. Watanabe, and J. R. Hershey, “End-to-end multi-speaker speech recognition,” in Proc. IEEE ICASSP, 2018, pp. 4819–4823.
[8] H. Seki, T. Hori, S. Watanabe, J. Le Roux, and J. R. Hershey, “A purely end-to-end system for multi-speaker speech recognition,” in Proc. ACL, Jul. 2018.
[9] X. Chang, Y. Qian, K. Yu, and S. Watanabe, “End-to-end monaural multi-speaker ASR system without pretraining,” in Proc. IEEE ICASSP, 2019, pp. 6256–6260.
[10] L. Drude and R. Haeb-Umbach, “Tight integration of spatial and spectral features for bss with deep clustering embeddings.” in Proc. ISCA Interspeech, 2017, pp. 2650–2654.
[11] Z.-Q. Wang, J. Le Roux, and J. R. Hershey, “Multi-Channel Deep Clustering: Discriminative spectral and spatial embeddings for speaker-independent speech separation,” in Proc. IEEE ICASSP, 2018, pp. 1–5.
[12] T. Yoshioka, H. Erdogan, Z. Chen, X. Xiao, and F. Alleva, “Recognizing overlapped speech in meetings: A multichannel separation approach using neural networks,” in Proc. ISCA Interspeech, 2018.
[13] X. Chang, W. Zhang, Y. Qian, J. L. Roux, and S. Watanabe, “MIMO-Speech: End-to-end multi-channel multi-speaker speech recognition,” arXiv preprint arXiv:1910.06522, 2019.
[14] S. Kim, T. Hori, and S. Watanabe, “Joint CTC-attention based end-to-end speech recognition using multi-task learning,” in Proc. IEEE ICASSP, Mar. 2017, pp. 4835–4839.
[15] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin, “Attention is all you need,” in Proc. NIPS, 2017.
[16] A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever, “Improving language understanding by generative pretraining,” 2018.
[17] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: Pre-training of deep bidirectional transformers for language understanding,” in Proc. NAACL, 2019.

[18] S. Karita, N. Enrique Yalta Soplin, S. Watanabe, M. Delcroix, A. Ogawa, and T. Nakatani, “Improving transformerbased end-to-end speech recognition with connectionist temporal classiﬁcation and language model integration,” in Proc. ISCA Interspeech, 2019, pp. 1408–1412.
[19] S. Karita, N. Chen, T. Hayashi, T. Hori, H. Inaguma, Z. Jiang, M. Someki, N. E. Y. Soplin, R. Yamamoto, X. Wang et al., “A comparative study on Transformer vs RNN in speech applications,” arXiv preprint arXiv:1909.06317, 2019.
[20] Y. Fujita, N. Kanda, S. Horiguchi, Y. Xue, K. Nagamatsu, and S. Watanabe, “End-to-end neural speaker diarization with selfattention,” arXiv preprint arXiv:1909.06247, 2019.
[21] M.-T. Luong, H. Pham, and C. D. Manning, “Effective Approaches to Attention-based Neural Machine Translation,” in Proc. EMNLP, 2015, pp. 1412–1421.
[22] D. Povey, H. Hadian, P. Ghahremani, K. Li, and S. Khudanpur, “A time-restricted self-attention layer for ASR,” in Proc. IEEE ICASSP, 2018, pp. 5874–5878.
[23] T. Yoshioka and T. Nakatani, “Generalization of multi-channel linear prediction methods for blind mimo impulse response shortening,” IEEE/ACM Trans. Audio, Speech, Language Process., vol. 20, no. 10, pp. 2707–2720, 2012.
[24] M. Souden, J. Benesty, and S. Affes, “On optimal frequencydomain multichannel linear ﬁltering for noise reduction,” IEEE/ACM Trans. Audio, Speech, Language Process., vol. 18, no. 2, pp. 260–276, 2009.
[25] T. Yoshioka, N. Ito, M. Delcroix, A. Ogawa, K. Kinoshita, M. Fujimoto, C. Yu, W. J. Fabian, M. Espi, T. Higuchi et al., “The NTT CHiME-3 system: Advances in speech enhancement and recognition for mobile multi-microphone devices,” in Proc. IEEE ASRU, Dec. 2015, pp. 436–443.
[26] J. Heymann, L. Drude, and R. Haeb-Umbach, “Neural network based spectral mask estimation for acoustic beamforming,” in Proc. IEEE ICASSP, Mar. 2016.
[27] H. Erdogan, J. R. Hershey, S. Watanabe, M. I. Mandel, and J. Le Roux, “Improved MVDR beamforming using singlechannel mask prediction networks,” in Proc. ISCA Interspeech, Sep. 2016, pp. 1981–1985.
[28] T. Ochiai, S. Watanabe, T. Hori, and J. R. Hershey, “Multichannel end-to-end speech recognition,” in Proc. ICML, 2017, pp. 2632–2641.
[29] S. Watanabe, T. Hori, S. Karita, T. Hayashi, J. Nishitoba, Y. Unno, N. E. Y. Soplin, J. Heymann, M. Wiesner, N. Chen et al., “ESPnet: End-to-End Speech Processing Toolkit,” in Proc. ISCA Interspeech, 2018, pp. 2207–2211.
[30] T. Hori, J. Cho, and S. Watanabe, “End-to-end speech recognition with word-based RNN language models,” in Proc. IEEE SLT, Dec. 2018, pp. 389–396.
[31] L. Drude, J. Heymann, C. Boeddeker, and R. Haeb-Umbach, “NARA-WPE: A Python package for weighted prediction error dereverberation in Numpy and Tensorﬂow for online and ofﬂine processing,” in ITG Fachtagung Sprachkommunikation (ITG), 2018.

