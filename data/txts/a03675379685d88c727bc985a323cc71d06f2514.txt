Unsupervised Learning of Syntactic Structure with Invertible Neural Projections
Junxian He Graham Neubig Taylor Berg-Kirkpatrick Language Technologies Institute School of Computer Science Carnegie Mellon University
{junxianh, gneubig, tberg}@cs.cmu.edu

arXiv:1808.09111v1 [cs.CL] 28 Aug 2018

Abstract
Unsupervised learning of syntactic structure is typically performed using generative models with discrete latent variables and multinomial parameters. In most cases, these models have not leveraged continuous word representations. In this work, we propose a novel generative model that jointly learns discrete syntactic structure and continuous word representations in an unsupervised fashion by cascading an invertible neural network with a structured generative prior. We show that the invertibility condition allows for efﬁcient exact inference and marginal likelihood computation in our model so long as the prior is well-behaved. In experiments we instantiate our approach with both Markov and tree-structured priors, evaluating on two tasks: part-of-speech (POS) induction, and unsupervised dependency parsing without gold POS annotation. On the Penn Treebank, our Markov-structured model surpasses state-of-the-art results on POS induction. Similarly, we ﬁnd that our tree-structured model achieves state-of-the-art performance on unsupervised dependency parsing for the difﬁcult training condition where neither gold POS annotation nor punctuation-based constraints are available.1
1 Introduction
Data annotation is a major bottleneck for the application of supervised learning approaches to many problems. As a result, unsupervised methods that learn directly from unlabeled data are increasingly important. For tasks related to unsupervised syntactic analysis, discrete generative models have dominated in recent years – for example, for both part-of-speech (POS) induction (Blunsom and Cohn, 2011; Stratos et al., 2016) and unsupervised dependency parsing (Klein and Manning,
1Code is available at https://github.com/jxhe/structlearning-with-ﬂow.

(a) Traditional pre-trained (b) Learned latent embedd-

skip-gram embeddings

ings from our approach

Figure 1: Visualization (t-SNE) of skip-gram embeddings (trained on one billion words with context window size equal to 1) and latent embeddings learned by our approach with a Markov-structured prior. Each node represents a word and is colored according to the most likely gold POS tag from the Penn Treebank (best seen in color).

2004; Cohen and Smith, 2009; Pate and Johnson, 2016). While similar models have had success on a range of unsupervised tasks, they have mostly ignored the apparent utility of continuous word representations evident from supervised NLP applications (He et al., 2017; Peters et al., 2018). In this work, we focus on leveraging and explicitly representing continuous word embeddings within unsupervised models of syntactic structure.
Pre-trained word embeddings from massive unlabeled corpora offer a compact way of injecting a prior notion of word similarity into models that would otherwise treat words as discrete, isolated categories. However, the speciﬁc properties of language captured by any particular embedding scheme can be difﬁcult to control, and, further, may not be ideally suited to the task at hand. For example, pre-trained skip-gram embeddings (Mikolov et al., 2013) with small context window size are found to capture the syntactic properties of language well (Bansal et al., 2014; Lin et al., 2015). However, if our goal is to separate syntactic categories, this embedding space is not ideal – POS categories correspond to overlap-

zi < l a t e x i t s h a 1 _ b a s e 6 4 = " A 0 v A n 9 B P w 9 k D X H S 9 r k u B 7 7 r p T A E = " > A A A C D n i c Z V D L T g I x F O 3 g C / G F u n T T S E x c E D J j T N Q d 0 Y 1 L T E Q w M C G d 0 o G G d j p p 7 5 j g h I 8 w b v U 7 X B m 3 / o K f 4 R / Y g V k I 3 K S 5 p + e e 2 5 y e I B b c g O v + O I W V 1 b X 1 j e J m a W t 7 Z 3 e v v H / w Y F S i K W t S J Z R u B 8 Q w w S P W B A 6 C t W P N i A w E a w W j m 2 z e e m L a c B X d w z h m v i S D i I e c E r D U 4 3 O P 4 6 7 h E v f K F b f m T g s v A y 8 H F Z R X o 1 f + 7 f Y V T S S L g A p i T M d z Y / B T o o F T w S a l b m J Y T O i I D F j H w o h I Z v x 0 a n i C T y z T x 6 H S 9 k S A p + z / j Z R I I w k M r T J r Z m 6 W M a C U M F W r g q H M W v b M 9 G 7 G M q g G s p q J t A n N g h E I L / 2 U R 3 E C L K I z H 2 E i M C i c h Y P 7 X D M K Y m w B o Z r b r 2 A 6 J J p Q s B G W b E b e Y i L L o H l W u 6 q 5 d + e V + n U e V h E d o W N 0 i j x 0 g e r o F j V Q E 1 E k 0 S t 6 Q + / O i / P h f D p f M 2 n B y X c O 0 V w 5 3 3 9 M a J 0 J < / l a t e x i t >

⇠ Syntax Model

z1 < l a t e x i t s h a 1 _ b a s e 6 4 = " j b t r + W 9 v p R f g 2 s a I 7 Y I m O b l k N i k = " > A A A C C H i c Z V D L S g M x F M 3 U V 6 2 v q k s 3 g 0 V w U c q M C O q u 6 M Z l R c c W 2 q F k 0 k w b m k y G 5 I 5 Q h 3 6 B u N X v c C V u / Q s / w z 8 w M 5 2 F b S + E e 3 L u u e H k B D F n G h z n x y q t r K 6 t b 5 Q 3 K 1 v b O 7 t 7 1 f 2 D R y 0 T R a h H J J e q E 2 B N O Y u o B w w 4 7 c S K Y h F w 2 g 7 G N 9 m 8 / U S V Z j J 6 g E l M f Y G H E Q s Z w W C o + + e + 2 6 / W n I a T l 7 0 M 3 A L U U F G t f v W 3 N 5 A k E T Q C w r H W X d e J w U + x A k Y 4 n V Z 6 i a Y x J m M 8 p F 0 D I y y o 9 t P c 6 t Q + M c z A D q U y J w I 7 Z / 9 v p F h o g W F k l F n T c 7 O M A S m 5 r h s V j E T W s m f y u 5 6 I o B 6 I e i Z S O t Q L R i C 8 9 F M W x Q n Q i M x 8 h A m 3 Q d p Z L P a A K U q A T w z A R D H z F Z u M s M I E T H g V k 5 G 7 m M g y 8 M 4 a V w 3 n 7 r z W v C 7 C K q M j d I x O k Y s u U B P d o h b y E E F D 9 I r e 0 L v 1 Y n 1 Y n 9 b X T F q y i p 1 D N F f W 9 x / u T Z q w < / l a t e x i t >

e
< l a t e x i t s h a 1 _ b a s e 6 4 = " 8 N C 3 L C M i r j + J o I 0 q d F Z B v o N w i U J L K d f b 8 r 8 h r + F W w N A r P h / P / g j X Q 8 = " > A A A C Q 3 i c Z V B N a S 9 w t M A x E B M 0 3 l 6 a b Z f O 2 4 q H e 3 v G Q a S Y F y E 9 G L x Q l i K G 0 l X x F k T i w 5 U J v I X U g c S Q R n a v u J C K W a 5 S Z 0 s b m g m y 2 W D E y K W v Z 1 J y Z l o 6 W y 6 q 7 x L W / 7 w o F 4 / I h r j 9 x F K 9 v K e f / k A z 9 p e t F b K 3 + 1 C P y + b T Y S H 0 P G w u b h C u v 7 H I n P z i J T s 2 y w 8 z K J B s X 3 c b g 5 O a + Z / l e 5 i V O S j W Y A + z M D T 3 k 8 1 H P a T + l s Z a n j Z x u 5 f t m b F 2 6 5 u 0 L n S T h 5 V 8 G 9 Z 3 p + q n x u F v l v V h D s 6 d K W i U K Y G H C z Z A 6 t w t F R n l A m Q 1 7 H C I r p V C j j M 5 h A I g s Z M I v P o S 5 8 t O D p 1 y L i 2 S + / Z y N q l 7 w f l + 5 / 9 7 B V P F W 2 V 6 u s S 0 b M s U J n j n T J g U l 8 Y U K X q R z S u i B F 5 w H w M i W o R 5 F K y u H 6 B d g x u p c m S r A e J p 9 I C L g E R V + i X g G S x K 4 4 T p q T B R z m I V o 9 w 0 v R w z t 6 5 T U q c y N q l J f q c 2 a / 7 p k q g L y p k z b b C + 6 K 5 C t a f K 9 L M q v j B X / a 0 f B c i g C 7 t I e d Z t q k s F H U X Q / L 2 c u D y B f 2 h X N 7 X 8 3 5 o h K s N A o X J K l b k d C P V / B F B Y j s r 0 h r t x + A C p u m 2 k c 1 a o O 6 B i U s M K S I k S p Z g I Y a F l k h 7 N z 6 p Q x L J r X u l 2 J M W C X G X S d m M X J Z H e D X h l Z n U g c d Z c v t U 0 7 c Z K U y N 0 e f e Q W n Y g M k c v m 0 1 5 c k a R 9 N A p 0 3 r P L p 3 W J 6 2 Z q L q 5 r 1 F X / M f M K X C 2 V y M D z 3 a q e g Q J a 1 2 1 l y 7 3 T 5 k S V 1 T 6 p c V u P o / F p s u m 2 a t E j 2 O k V T 9 m T z L + V L 8 Q y L J z j b c z 7 n u m 0 S C Z O s Y A n S S O S t 2 g K j s z k g J Q e G s h P Z k 2 e j e u S M U M J 1 a o u y I D d 6 J F W h N C h q O u E T M 0 5 F c 0 4 x A 7 y R I h 9 I w L p 1 h v E W 2 I p 9 o a Q v y x d 1 7 5 x D 3 9 n o U b b d T f s 3 y 6 C v o 6 Y p H 9 P e X o f A 9 D s W M I P K z r q a t B 3 V M t I 8 E t a e a A n c 7 1 c 0 A R g E R 6 H Q M S M 1 Z E n 0 M Q M N l 6 D Q I s D / B o D x f X y v A 0 n 3 / r A w r P + 7 B 3 7 M c g B H n f f G B G 3 M 7 8 l v 0 o L V V 3 j h M f 7 3 M z G 4 D q C t P m 7 0 9 = B < w / c l M a t O e c x = i < t / > l a t e x i t >

i

⇠ N (µzi , ⌃zi )

Neural Projector

xi < l a t e x i t s h a 1 _ b a s e 6 4 = " 2 P 7 u 0 N V s J s L p D G 7 j g p t c 0 c v 2 H S 7 e A V t m N k f D 4 W c f V r J 5 p a k b 6 2 V W A 0 = " > A A A C K X i c Z V C B 7 N T S i w s M x E J M 3 l 6 T b X f g 2 F q K e G v k Q s S H / h I J A I K U p b W S y h 9 g q V A t e c h K 6 Y M K W G j E g i r Q U C F S W C 5 R Z a s e m R r 0 X v B s Z b L D M X k K s 3 8 v W 2 y 6 7 k N a 0 r f 7 I f x k 7 Y V R u A / m / U A f k A 3 M r V 1 0 F P 3 I j F Q F s P 0 T n Y r C Q Q e W t P A k m a D w d 5 v c 3 + o a S M Z N F X y P a i C T G A / q D L 9 Y V f 2 g 9 c s j f I G y J O y j a U n 9 p M m T t k j 1 A X 3 Z v m 7 b C n 4 5 V h F e x q e i O 0 T s c n q V 1 u Z e T G V 8 q R b J T K U N p 0 u N z i m F O o A u x R q c w p M b H K w F Z D q y J s Z 8 k x S w G q g m j L X J C T q + y P L X f b d / 8 S / Y / N c l e z N F F Z T 9 o B 9 P x W n F 7 u G S O y 4 5 p h e H p n C B I K R w j V K F K B j c o b u 4 p U q y O u 1 8 b 4 F D / j z g I 9 S w 5 F C G 9 S J u o a s i J V X d 7 n a u w c n l y B n 5 L l T t Q b + A 4 f a F W D 1 b F / 1 i L D a w y L H / g B y 9 C U / f Q s e F M H T b r / D L X x X / 3 6 2 s 8 1 f 9 A n O Q A T w F q j r 1 6 Z 1 H u 0 5 V r S l y i G q K f g I g J x L l X x 2 U v / B Q F T m a 2 G C d m H o A Q q c W E B k 5 L o y Z v U t a 3 l P h K B M 6 s R k S t 7 6 Z w h c Y 8 U d x T k K c n y i 0 t s l 8 M E M t b O i d n 6 J y h T m A O d 6 H J S N t H s G X v A R x T 6 J w g v P y 0 c 6 y U I V o B 0 l k F 0 c L W V e K U l / 8 p y k f v n / m U N 8 A g K 1 W p H L K W V 3 g M V q d 7 6 C Z m L f 7 / Z D l e C D b 2 v v g Z z V L X o I S t w V 7 z k Y T u a M R T G e V y k P E R o T r D 1 Z 2 7 x R u Q M i k z B X R L b k T K 4 R v 5 u R k K e j S A S o o 7 C H b z e D N H 9 a I 4 V Z h B j d O G U 3 f g Q F c D o N M 7 8 S K m d Y Q d l o i k P m G F s K r y Q 7 u B V e t x R H d j d V S + R O / v 6 I C b + t U L 9 b m r v f + + K r f h W k q f 0 N M q 0 D j 4 B Y m x 0 B h S t u Z w R C p V v X Q R g L B q 1 q o h w Y g 3 E S c C Q 6 g o s i Y i X G M 3 E S N H 3 H M t F C 9 j c d B + 0 s 8 9 B e k y / B e y 2 1 A 6 E 5 n v z 3 O M L r M K O I 3 / C 4 F b 4 1 / A G 4 O i k Q q r t F Y A = < / l a t e x i t >

=f

(ei)

e 1 < l a t e x i t s h a 1 _ b a s e 6 4 = " W g i t C L n E S 1 r 4 T e J X e V D W d 7 c T v p M = " > A A A C D X i c Z V D L S g M x F M 3 U V 6 2 v q k s 3 w S K 4 K G V G B H V X d O O y g m M L b S m Z N N O G J p M h u S O U o f 8 g b v U 7 X I l b v 8 H P 8 A / M T G d h 2 w v h n p x 7 b j g 5 Q S y 4 A d f 9 c U p r 6 x u b W + X t y s 7 u 3 v 5 B 9 f D o y a h E U + Z T J Z T u B M Q w w S P m A w f B O r F m R A a C t Y P J X T Z v P z N t u I o e Y R q z v i S j i I e c E r B U p x f I l M 0 G 3 q B a c x t u X n g V e A W o o a J a g + p v b 6 h o I l k E V B B j u p 4 b Q z 8 l G j g V b F b p J Y b F h E 7 I i H U t j I h k p p / m f m f 4 z D J D H C p t T w Q 4 Z / 9 v p E Q a S W B s l V k z C 7 O M A a W E q V s V j G X W s m f y u 5 n K o B 7 I e i b S J j R L R i C 8 7 q c 8 i h N g E Z 3 7 C B O B Q e E s G z z k m l E Q U w s I 1 d x + B d M x 0 Y S C T b B i M / K W E 1 k F / k X j p u E + X N a a t 0 V Y Z X S C T t E 5 8 t A V a q J 7 1 E I + o k i g V / S G 3 p 0 X 5 8 P 5 d L 7 m 0 p J T 7 B y j h X K + / w A M u J z w < / l a t e x i t >

z2 < l a t e x i t s h a 1 _ b a s e 6 4 = " Z N K u R e 2 3 l z C J M N e s h 6 c R g w e K X J I = " > A A A C C H i c Z V D L S g M x F M 3 U V 6 2 v q k s 3 g 0 V w U c p M E d R d 0 Y 3 L i o 4 W 2 q F k 0 k w b m k y G 5 I 5 Q h 3 6 B u N X v c C V u / Q s / w z 8 w M 5 2 F b S + E e 3 L u u e H k B D F n G h z n x y q t r K 6 t b 5 Q 3 K 1 v b O 7 t 7 1 f 2 D B y 0 T R a h H J J e q E 2 B N O Y u o B w w 4 7 c S K Y h F w + h i M r 7 P 5 4 x N V m s n o H i Y x 9 Q U e R i x k B I O h 7 p 7 7 z X 6 1 5 j S c v O x l 4 B a g h o p q 9 6 u / v Y E k i a A R E I 6 1 7 r p O D H 6 K F T D C 6 b T S S z S N M R n j I e 0 a G G F B t Z / m V q f 2 i W E G d i i V O R H Y O f t / I 8 V C C w w j o 8 y a n p t l D E j J d d 2 o Y C S y l j 2 T 3 / V E B P V A 1 D O R 0 q F e M A L h h Z + y K E 6 A R m T m I 0 y 4 D d L O Y r E H T F E C f G I A J o q Z r 9 h k h B U m Y M K r m I z c x U S W g d d s X D a c 2 7 N a 6 6 o I q 4 y O 0 D E 6 R S 4 6 R y 1 0 g 9 r I Q w Q N 0 S t 6 Q + / W i / V h f V p f M 2 n J K n Y O 0 V x Z 3 3 / v 7 5 q x < / l a t e x i t >

e 2 < l a t e x i t s h a 1 _ b a s e 6 4 = " w B c N k 6 e u W G M r g + D a + m n o L S s 6 / F g = " > A A A C D X i c Z V D L S g M x F M 3 U V 6 2 v q k s 3 g 0 V w U c q 0 C O q u 6 M Z l B U c L 7 V A y a a Y N z W N I 7 g h l 6 D + I W / 0 O V + L W b / A z / A M z 0 1 n Y 9 k K 4 J + e e G 0 5 O G H N m w P N + n N L a + s b m V n m 7 s r O 7 t 3 9 Q P T x 6 N C r R h P p E c a W 7 I T a U M 0 l 9 Y M B p N 9 Y U i 5 D T p 3 B y m 8 2 f n q k 2 T M k H m M Y 0 E H g k W c Q I B k t 1 + 6 F I 6 W z Q G l R r X s P L y 1 0 F z Q L U U F G d Q f W 3 P 1 Q k E V Q C 4 d i Y X t O L I U i x B k Y 4 n V X 6 i a E x J h M 8 o j 0 L J R b U B G n u d + a e W W b o R k r b I 8 H N 2 f 8 b K R Z G Y B h b Z d b M w i x j Q C l u 6 l Y F Y 5 G 1 7 J n 8 b q Y i r I e i n o m 0 i c y S E Y i u g p T J O A E q y d x H l H A X l J t l 4 w 6 Z p g T 4 1 A J M N L N f c c k Y a 0 z A J l i x G T W X E 1 k F f q t x 3 f D u L 2 r t m y K s M j p B p + g c N d E l a q M 7 1 E E + I o i j V / S G 3 p 0 X 5 8 P 5 d L 7 m 0 p J T 7 B y j h X K + / w A O W p z x < / l a t e x i t >

f
< l a t e x i t s h a 1 _ b a s e 6 4 = " 8 J o b x C g P n J a 2 b f m H E A a G m Z X a C 8 M E 6 b E w L 6 W D d T h j W / T v r v r F t j 8 W c E = " > A A A C H H i c Z V A D 9 L S w g N M B x E F J M 3 z 4 2 r / K g 2 V P F q S r u h b y w M y 1 h i E F C O q F X i M o u 3 F Z F B 3 G R 0 T s c F u z K w 1 g h m b h a L E 3 v N J n p l J n k c 2 v N T J 1 k 2 M 5 y 4 Z R 1 w C 3 G G e 8 Z R b W x + K 2 3 t u b / a Q z F E x V J v W B 4 n F W P N 8 i O 7 F d r 0 s l 2 h 0 4 X s d A j y 2 b Q 9 r + g 8 n W 5 d 5 6 4 + b I T B o b 4 c X o c O q d b 9 B O c V b P 6 T s M p 7 e N W z V 8 1 w b u X L 1 p z a E X Y l 2 l t d 7 W m 1 8 v v Z r P G f l 3 V b G v J X p M s l y K n E S 1 i o h j 9 k H U R j D U D 8 B r I C + l Y n j A x a w 0 F B u A 4 0 4 1 b I o z a I J Q Y r e B J X z c W n v e c b F z 1 1 O j q 3 8 T P h q q d v J o M E B g n c x w 6 C 0 m h l y b G 4 / F G 7 Q A U f 4 E K Y W w 6 G p K a q 3 T w P m / 7 A a 7 D c m c T s a T j c v S s v 8 s y s 6 y o Q 5 p Z p N p l A + m t p 1 5 z 1 x 8 6 w l S 5 k R 7 7 4 n 7 / I Q X G g I T F s K F c h / c f r n x 5 z R 8 U v A r z M r P v d l b 7 3 e Z S P / 0 W U 1 S 1 y J C I K k k E g D x I t B w x 0 r v 3 B X g S 7 d K E d N H o I x q V W s B A Z I q p Z 0 0 m Y 2 F h W N k 6 a R Y 2 j 7 L Z A j P Y d U o R 0 k M c M x C 0 C 0 6 s n J Y + 8 5 t u p 5 / Z Y p J u 4 e b G p S 2 t r s 5 T U o 5 V g u R w g f j z 9 d n S / I G o z 0 E k W 2 W L m f D K o v G J 2 m X x a W 9 c M 6 w g s U Z s U L B U K r A k r t 7 G M B m X / 2 5 R M t 8 v X S d Z D 8 G V R 2 Q P C h 2 F Q f t y F R 2 C k T V m K g e k 3 j r G O B S 5 P 1 g U X h 7 7 R F j C F b o K Q I R D 0 n I 2 B E M i f X P B g R u t X 0 l H U a b a o V 9 R r 2 R l l y E l M K L g C I B 8 U M c w / E s Q V x l 8 / x a W J b J 9 h L R H t C o B C E W y b g U W W Z M O y R k O f 5 / A I P I 6 q g i f d 1 l 7 S 0 / L m z 9 q U 3 J D 6 G g s p B N t K m o E M H O q 0 t R C E A q Q I 2 h j e C d G o Z w y q D 6 D Q x V R V S U e Q I w Q T n F e 6 H B Y E e 9 n o D W f n r 3 w f X k q Y z S 3 q e 2 c P 0 i c X 4 T m J j m J u X 7 z s + o Q 5 t m i y i P K v W 8 s A < h / 3 l S a n t J e g x = i = t < > / l a t e x i t >

(e)

z3 < l a t e x i t s h a 1 _ b a s e 6 4 = " V w Y P d o e V j X k U U 9 1 2 k w m j d y m a S + E = " > A A A C C H i c Z V B L T s M w F H T 4 l v I r s G Q T U S G x q K o E k I B d B R u W R R B a q Y 0 q x 3 V a q 3 Y c 2 S 9 I J e o J E F s 4 B y v E l l t w D G 6 A k 2 Z B 2 y d Z b z x v n j W e I O Z M g + P 8 W E v L K 6 t r 6 6 W N 8 u b W 9 s 5 u Z W / / U c t E E e o R y a V q B 1 h T z i L q A Q N O 2 7 G i W A S c t o L R T T Z v P V G l m Y w e Y B x T X + B B x E J G M B j q / r l 3 1 q t U n b q T l 7 0 I 3 A J U U V H N X u W 3 2 5 c k E T Q C w r H W H d e J w U + x A k Y 4 n Z S 7 i a Y x J i M 8 o B 0 D I y y o 9 t P c 6 s Q + N k z f D q U y J w I 7 Z / 9 v p F h o g W F o l F n T M 7 O M A S m 5 r h k V D E X W s m f y u x 6 L o B a I W i Z S O t R z R i C 8 9 F M W x Q n Q i E x 9 h A m 3 Q d p Z L H a f K U q A j w 3 A R D H z F Z s M s c I E T H h l k 5 E 7 n 8 g i 8 E 7 r V 3 X n 7 r z a u C 7 C K q F D d I R O k I s u U A P d o i b y E E E D 9 I r e 0 L v 1 Y n 1 Y n 9 b X V L p k F T s H a K a s 7 z / x k Z q y < / l a t e x i t >

Syntax Model

Markov prior

z1 < l a t e x i t s h a 1 _ b a s e 6 4 = " j b t r + W 9 v p R f g 2 s a I 7 Y I m O b l k N i k = " > A A A C C H i c Z V D L S g M x F M 3 U V 6 2 v q k s 3 g 0 V w U c q M C O q u 6 M Z l R c c W 2 q F k 0 k w b m k y G 5 I 5 Q h 3 6 B u N X v c C V u / Q s / w z 8 w M 5 2 F b S + E e 3 L u u e H k B D F n G h z n x y q t r K 6 t b 5 Q 3 K 1 v b O 7 t 7 1 f 2 D R y 0 T R a h H J J e q E 2 B N O Y u o B w w 4 7 c S K Y h F w 2 g 7 G N 9 m 8 / U S V Z j J 6 g E l M f Y G H E Q s Z w W C o + + e + 2 6 / W n I a T l 7 0 M 3 A L U U F G t f v W 3 N 5 A k E T Q C w r H W X d e J w U + x A k Y 4 n V Z 6 i a Y x J m M 8 p F 0 D I y y o 9 t P c 6 t Q + M c z A D q U y J w I 7 Z / 9 v p F h o g W F k l F n T c 7 O M A S m 5 r h s V j E T W s m f y u 5 6 I o B 6 I e i Z S O t Q L R i C 8 9 F M W x Q n Q i M x 8 h A m 3 Q d p Z L P a A K U q A T w z A R D H z F Z u M s M I E T H g V k 5 G 7 m M g y 8 M 4 a V w 3 n 7 r z W v C 7 C K q M j d I x O k Y s u U B P d o h b y E E F D 9 I r e 0 L v 1 Y n 1 Y n 9 b X T F q y i p 1 D N F f W 9 x / u T Z q w < / l a t e x i t >

z2 < l a t e x i t s h a 1 _ b a s e 6 4 = " Z N K u R e 2 3 l z C J M N e s h 6 c R g w e K X J I = " > A A A C C H i c Z V D L S g M x F M 3 U V 6 2 v q k s 3 g 0 V w U c p M E d R d 0 Y 3 L i o 4 W 2 q F k 0 k w b m k y G 5 I 5 Q h 3 6 B u N X v c C V u / Q s / w z 8 w M 5 2 F b S + E e 3 L u u e H k B D F n G h z n x y q t r K 6 t b 5 Q 3 K 1 v b O 7 t 7 1 f 2 D B y 0 T R a h H J J e q E 2 B N O Y u o B w w 4 7 c S K Y h F w + h i M r 7 P 5 4 x N V m s n o H i Y x 9 Q U e R i x k B I O h 7 p 7 7 z X 6 1 5 j S c v O x l 4 B a g h o p q 9 6 u / v Y E k i a A R E I 6 1 7 r p O D H 6 K F T D C 6 b T S S z S N M R n j I e 0 a G G F B t Z / m V q f 2 i W E G d i i V O R H Y O f t / I 8 V C C w w j o 8 y a n p t l D E j J d d 2 o Y C S y l j 2 T 3 / V E B P V A 1 D O R 0 q F e M A L h h Z + y K E 6 A R m T m I 0 y 4 D d L O Y r E H T F E C f G I A J o q Z r 9 h k h B U m Y M K r m I z c x U S W g d d s X D a c 2 7 N a 6 6 o I q 4 y O 0 D E 6 R S 4 6 R y 1 0 g 9 r I Q w Q N 0 S t 6 Q + / W i / V h f V p f M 2 n J K n Y O 0 V x Z 3 3 / v 7 5 q x < / l a t e x i t >

z3 < l a t e x i t s h a 1 _ b a s e 6 4 = " V w Y P d o e V j X k U U 9 1 2 k w m j d y m a S + E = " > A A A C C H i c Z V B L T s M w F H T 4 l v I r s G Q T U S G x q K o E k I B d B R u W R R B a q Y 0 q x 3 V a q 3 Y c 2 S 9 I J e o J E F s 4 B y v E l l t w D G 6 A k 2 Z B 2 y d Z b z x v n j W e I O Z M g + P 8 W E v L K 6 t r 6 6 W N 8 u b W 9 s 5 u Z W / / U c t E E e o R y a V q B 1 h T z i L q A Q N O 2 7 G i W A S c t o L R T T Z v P V G l m Y w e Y B x T X + B B x E J G M B j q / r l 3 1 q t U n b q T l 7 0 I 3 A J U U V H N X u W 3 2 5 c k E T Q C w r H W H d e J w U + x A k Y 4 n Z S 7 i a Y x J i M 8 o B 0 D I y y o 9 t P c 6 s Q + N k z f D q U y J w I 7 Z / 9 v p F h o g W F o l F n T M 7 O M A S m 5 r h k V D E X W s m f y u x 6 L o B a I W i Z S O t R z R i C 8 9 F M W x Q n Q i E x 9 h A m 3 Q d p Z L H a f K U q A j w 3 A R D H z F Z s M s c I E T H h l k 5 E 7 n 8 g i 8 E 7 r V 3 X n 7 r z a u C 7 C K q F D d I R O k I s u U A P d o i b y E E E D 9 I r e 0 L v 1 Y n 1 Y n 9 b X V L p k F T s H a K a s 7 z / x k Z q y < / l a t e x i t >

e< l a t e x i t s h a 1 _ b a s e 6 4 = " L R g M i N i k z U H 7 R 9 m 9 h k H f O o K H E w E X b H w S q 6 V v R i 4 9 t X W i B R S Q 3 2 l t w = " > A A A C D X i c Z V D L T S g I M x F L M 2 3 D U L V 8 6 Q 2 X v a q u L D G z U W R A i R 4 X o J K Q Q y Q 1 R Y f W q 6 j K u 3 j X G j J s S g Q X i H J F E t C q w h U Z z N r J Q M 0 G E 5 4 p n M 7 h R u 0 S T O M U u I E b f + j g F 7 v j 9 V D X l 3 f D G r r S d l / w g J x f n o / M w b D / + 8 w E A / C M 4 D G P b t N w P r f Y 0 X 3 w H j O 0 b 5 0 9 + 9 N x H w g c h v v y 0 I v M G w 8 2 n O s 8 7 2 K 3 6 l t V r l 6 b R X 3 1 c j x f t y b m e 4 / W s t 7 u Z X 3 3 d D v x e 6 L M + i w j b V 2 l W d s a S q L E U 0 J k Z 2 J f L G 1 C f Z a 4 x y p O p r y I F U 1 b A B U m G p n B L m Y R j v R m b A H N w f O 3 W i 3 b 5 z o h 5 t P 0 T 3 B n u q u g w S n j s M c Z R 3 a s w E j 4 S o T p / 7 k A A g a 5 c A E F L j d G V A s w + V z L J v h r 4 i + 4 5 R F O N e 1 h / e w 9 S Y t s 6 m k p 3 O G F V n Q Z n y o 6 F A C 6 5 A a 6 j X 2 a 8 U w f g O A H 1 v W d 7 Y + / t G 9 7 1 3 T i N b J 7 Y c s v R S C S q x I o M C a I 2 R y j F r 2 T E t n V I J R w o I 5 v F w W Q y o c Y a 4 8 X e R G S R 6 Y M Q a O a S R Z p + i 1 M L 8 A I y B J 2 Z D K A a y T x T o P N y p O L 3 M V r P 8 L T 9 + N 9 x Q A w a f X T t u C Q d y C p f w s Q / 7 4 I 2 z E 9 S v C 5 M F J g D o q Q w W y G b o W V Z G u m l T j c K / o N l U D g B a F k q 5 8 L K p B s T V F D v A 6 U z a O U R u u f R y t e I 5 v 6 + L r P K y Y y i L r 8 Q q J p z S I O I l R A D L K x 4 i 6 B C 4 Q M + p j L G W F B l j I F p Q z E 6 M C y W 9 L R i H o E 3 D A Q Z b p t p 8 9 c n 1 Y o f y a h Y G o F A h T C 4 q 2 u A f B 2 P K F S z w F d E s E M 4 s o Q 2 K w E Z z z A N J q F L k y x Y G y 1 D c K V o E n l 5 o e F u 7 S X 1 r 7 m N u Z O n E U 2 D T 0 V 8 R r 1 C N M K Z 4 z + A O G 0 Z Q T k h 6 E Q i 1 p V w 0 B i 1 W W r o A F w j U W B Q L i / w A j K i b 6 8 A 6 k z 9 8 o + x 5 f 8 r O 0 J X 9 q T z a 3 c q a 2 Z P 7 q R T z R C n X z D X l Y f O f 0 9 V Y x q Z n X t 3 M 9 = b < g / q l C a P t < e / x l i a t > e x i t > 3

DMV prior
ztre e < l a t e x i t s h a 1 _ b a s e 6 4 = " d B j f + C p S w G r I 4 o b g B u Q o M 2 I I 8 Z M = " > A A A C F n i c Z V D L T g I x F O 3 4 R H y A u n Q z k Z i 4 I G Q w J u q O 6 M Y l J i I k Q E i n 3 I G G d j p p 7 x h x M h 9 i 3 O p 3 u D J u 3 f o Z / o E d Y C F w k + a e n n t u c 3 r 8 S H C D n v f j r K y u r W 9 s 5 r b y 2 z u 7 e 4 X i / s G D U b F m 0 G B K K N 3 y q Q H B Q 2 g g R w G t S A O V v o C m P 7 r J 5 s 1 H 0 I a r 8 B 7 H E X Q l H Y Q 8 4 I y i p X r F w n M v 6 S A 8 Y Y I a I E 1 7 x Z J X 8 S b l L o P q D J T I r O q 9 4 m + n r 1 g s I U Q m q D H t q h d h N 6 E a O R O Q 5 j u x g Y i y E R 1 A 2 8 K Q S j D d Z G I 8 d U 8 s 0 3 c D p e 0 J 0 Z 2 w / z c S K o 2 k O L T K r J m 5 W c a g U s K U r Q q H M m v Z M 5 O 7 G U u / 7 M t y J t I m M A t G M L j s J j y M Y o S Q T X 0 E s X B R u V l I b p 9 r Y C j G F l C m u f 2 K y 4 Z U U 4 Y 2 y r z N q L q Y y D J o n F W u K t 7 d e a l 2 P Q s r R 4 7 I M T k l V X J B a u S W 1 E m D M B K T V / J G 3 p 0 X 5 8 P 5 d L 6 m 0 h V n t n N I 5 s r 5 / g N o j a D p < / l a t e x i t >

xi < l a t e x i t s h a 1 _ b a s e 6 4 = " 4 X m k a a f D 4 E W e U 6 w r j e P k Y 8 q E 6 N o = " > A A A C E 3 i c Z V D L T g I x F O 3 g C / G F u n T T S E x c E D I Y E 3 V H d O M S E 0 d I G C S d 0 o G G d j p p 7 x j J h N 8 w b v U 7 X B m 3 f o C f 4 R / Y G V g I 3 K S 5 p + e e 2 5 y e I B b c g O v + O I W V 1 b X 1 j e J m a W t 7 Z 3 e v v H / w Y F S i K f O o E k q 3 A 2 K Y 4 B H z g I N g 7 V g z I g P B W s H o J p u 3 n p g 2 X E X 3 M I 5 Z V 5 J B x E N O C V j q 0 Q 9 k + j z p c e w b L n G v X H F r b l 5 4 G d R n o I J m 1 e y V f / 2 + o o l k E V B B j O n U 3 R i 6 K d H A q W C T k p 8 Y F h M 6 I g P W s T A i k p l u m r u e 4 B P L 9 H G o t D 0 R 4 J z 9 v 5 E S a S S B o V V m z c z N M g a U E q Z q V T C U W c u e y e 9 m L I N q I K u Z S J v Q L B i B 8 L K b 8 i h O g E V 0 6 i N M B A a F s 4 R w n 2 t G Q Y w t I F R z + x V M h 0 Q T C j b H k s 2 o v p j I M v D O a l c 1 9 + 6 8 0 r i e h V V E R + g Y n a I 6 u k A N d I u a y E M U a f S K 3 t C 7 8 + J 8 O J / O 1 1 R a c G Y 7 h 2 i u n O 8 / l w W f X A = = < / l a t e x i t >

⇠ Point mass at

f
< l a t e x i t s h a 1 _ b a s e 6 4 = " I S F M u i B W V v 5 r f v J n q q 9 u M 8 O s f v o = " > A A A C H n i c Z V B N S 8 N A E N 3 4 W e t X 1 Y M H L 8 E i V C g l E U G 9 F b 1 4 r G B s o S 1 h s 9 2 0 S 3 e z Y X c i l J A f I 1 7 1 d 3 g S r / o z / A d u 0 h 5 s O 7 D M 2 z d v h p k X x J x p c J w f a 2 V 1 b X 1 j s 7 R V 3 t 7 Z 3 d u v H B w + a Z k o Q j 0 i u V S d A G v K W U Q 9 Y M B p J 1 Y U i 4 D T d j C + y + v t Z 6 o 0 k 9 E j T G L a F 3 g Y s Z A R D I b y K 8 e h n / Y C k f b i E c u y W g 5 p 5 r N z v 1 J 1 G k 4 R 9 j J w Z 6 C K Z t H y K 7 + 9 g S S J o B E Q j r X u u k 4 M / R Q r Y I T T r N x L N I 0 x G e M h 7 R o Y Y U F 1 P y 0 O y O w z w w z s U C r z I r A L 9 n 9 H i o U W G E Z G m S c 9 V 8 s Z k J L r u l H B S O Q p H 1 P 8 9 U Q E 9 U D U c 5 H S o V 5 Y B M L r f s q i O A E a k e k e Y c J t k H Z u l j 1 g i h L g E w M w U c y c Y p M R V p i A s b R s P H I X H V k G 3 k X j p u E 8 X F a b t z O z S u g E n a I a c t E V a q J 7 1 E I e I i h D r + g N v V s v 1 o f 1 a X 1 N p S v W r O c I z Y X 1 / Q f n y q O 9 < / l a t e x i t >

(ei)

x1 < l a t e x i t s h a 1 _ b a s e 6 4 = " w P l w 8 y / 0 I q f H J r a z c S Q a H u w Q 0 7 k = " > A A A C D X i c Z V D L S g M x F M 3 4 r P V V d e k m W A Q X p c y I o O 6 K b l x W c G y h L S W T Z t r Q Z D I k d 8 Q y 9 B / E r X 6 H K 3 H r N / g Z / o G Z 6 S x s e y H c k 3 P P D S c n i A U 3 4 L o / z s r q 2 v r G Z m m r v L 2 z u 7 d f O T h 8 N C r R l P l U C a X b A T F M 8 I j 5 w E G w d q w Z k Y F g r W B 8 m 8 1 b T 0 w b r q I H m M S s J 8 k w 4 i G n B C z V 7 g Y y f Z 7 2 v X 6 l 6 t b d v P A y 8 A p Q R U U 1 + 5 X f 7 k D R R L I I q C D G d D w 3 h l 5 K N H A q 2 L T c T Q y L C R 2 T I e t Y G B H J T C / N / U 7 x q W U G O F T a n g h w z v 7 f S I k 0 k s D I K r N m 5 m Y Z A 0 o J U 7 M q G M m s Z c / k d z O R Q S 2 Q t U y k T W g W j E B 4 1 U t 5 F C f A I j r z E S Y C g 8 J Z N n j A N a M g J h Y Q q r n 9 C q Y j o g k F m 2 D Z Z u Q t J r I M / P P 6 d d 2 9 v 6 g 2 b o q w S u g Y n a A z 5 K F L 1 E B 3 q I l 8 R J F A r + g N v T s v z o f z 6 X z N p C t O s X O E 5 s r 5 / g M r 9 5 0 D < / l a t e x i t >

x2 < l a t e x i t s h a 1 _ b a s e 6 4 = " O U e 9 N Q 3 n y 9 I i l m H P U 3 O N Q 4 T t f R o = " > A A A C D X i c Z V D L S g M x F M 3 U V 6 2 v q k s 3 g 0 V w U c p M E d R d 0 Y 3 L C o 4 t t E P J p J k 2 N J k M y R 2 x D P 0 H c a v f 4 U r c + g 1 + h n 9 g Z j o L 2 1 4 I 9 + T c c 8 P J C W L O N D j O j 1 V a W 9 / Y 3 C p v V 3 Z 2 9 / Y P q o d H j 1 o m i l C P S C 5 V N 8 C a c h Z R D x h w 2 o 0 V x S L g t B N M b r N 5 5 4 k q z W T 0 A N O Y + g K P I h Y y g s F Q 3 X 4 g 0 u f Z o D m o 1 p y G k 5 e 9 C t w C 1 F B R 7 U H 1 t z + U J B E 0 A s K x 1 j 3 X i c F P s Q J G O J 1 V + o m m M S Y T P K I 9 A y M s q P b T 3 O / M P j P M 0 A 6 l M i c C O 2 f / b 6 R Y a I F h b J R Z 0 w u z j A E p u a 4 b F Y x F 1 r J n 8 r u e i q A e i H o m U j r U S 0 Y g v P J T F s U J 0 I j M f Y Q J t 0 H a W T b 2 k C l K g E 8 N w E Q x 8 x W b j L H C B E y C F Z O R u 5 z I K v C a j e u G c 3 9 R a 9 0 U Y Z X R C T p F 5 8 h F l 6 i F 7 l A b e Y g g j l 7 R G 3 q 3 X q w P 6 9 P 6 m k t L V r F z j B b K + v 4 D L Z m d B A = = < / l a t e x i t >

x3 < l a t e x i t s h a 1 _ b a s e 6 4 = " r 7 R T s V K D b Y X I R C y P j 9 s z u I n K M V A = " > A A A C D X i c Z V D L S g M x F M 3 U V 6 2 v q k s 3 g 0 V w U c q M C u q u 6 M Z l B c c W 2 q F k 0 k w b m k y G 5 I 5 Y h v 6 D u N X v c C V u / Q Y / w z 8 w M 5 2 F b S + E e 3 L u u e H k B D F n G h z n x y q t r K 6 t b 5 Q 3 K 1 v b O 7 t 7 1 f 2 D R y 0 T R a h H J J e q E 2 B N O Y u o B w w 4 7 c S K Y h F w 2 g 7 G t 9 m 8 / U S V Z j J 6 g E l M f Y G H E Q s Z w W C o T i 8 Q 6 f O 0 f 9 6 v 1 p y G k 5 e 9 D N w C 1 F B R r X 7 1 t z e Q J B E 0 A s K x 1 l 3 X i c F P s Q J G O J 1 W e o m m M S Z j P K R d A y M s q P b T 3 O / U P j H M w A 6 l M i c C O 2 f / b 6 R Y a I F h Z J R Z 0 3 O z j A E p u a 4 b F Y x E 1 r J n 8 r u e i K A e i H o m U j r U C 0 Y g v P J T F s U J 0 I j M f I Q J t 0 H a W T b 2 g C l K g E 8 M w E Q x 8 x W b j L D C B E y C F Z O R u 5 j I M v D O G t c N 5 / 6 i 1 r w p w i q j I 3 S M T p G L L l E T 3 a E W 8 h B B H L 2 i N / R u v V g f 1 q f 1 N Z O W r G L n E M 2 V 9 f 0 H L z u d B Q = = < / l a t e x i t >

z1 < l a t e x i t s h a 1 _ b a s e 6 4 = " j b t r + W 9 v p R f g 2 s a I 7 Y I m O b l k N i k = " > A A A C C H i c Z V D L S g M x F M 3 U V 6 2 v q k s 3 g 0 V w U c q M C O q u 6 M Z l R c c W 2 q F k 0 k w b m k y G 5 I 5 Q h 3 6 B u N X v c C V u / Q s / w z 8 w M 5 2 F b S + E e 3 L u u e H k B D F n G h z n x y q t r K 6 t b 5 Q 3 K 1 v b O 7 t 7 1 f 2 D R y 0 T R a h H J J e q E 2 B N O Y u o B w w 4 7 c S K Y h F w 2 g 7 G N 9 m 8 / U S V Z j J 6 g E l M f Y G H E Q s Z w W C o + + e + 2 6 / W n I a T l 7 0 M 3 A L U U F G t f v W 3 N 5 A k E T Q C w r H W X d e J w U + x A k Y 4 n V Z 6 i a Y x J m M 8 p F 0 D I y y o 9 t P c 6 t Q + M c z A D q U y J w I 7 Z / 9 v p F h o g W F k l F n T c 7 O M A S m 5 r h s V j E T W s m f y u 5 6 I o B 6 I e i Z S O t Q L R i C 8 9 F M W x Q n Q i M x 8 h A m 3 Q d p Z L P a A K U q A T w z A R D H z F Z u M s M I E T H g V k 5 G 7 m M g y 8 M 4 a V w 3 n 7 r z W v C 7 C K q M j d I x O k Y s u U B P d o h b y E E F D 9 I r e 0 L v 1 Y n 1 Y n 9 b X T F q y i p 1 D N F f W 9 x / u T Z q w < / l a t e x i t >

z2 < l a t e x i t s h a 1 _ b a s e 6 4 = " Z N K u R e 2 3 l z C J M N e s h 6 c R g w e K X J I = " > A A A C C H i c Z V D L S g M x F M 3 U V 6 2 v q k s 3 g 0 V w U c p M E d R d 0 Y 3 L i o 4 W 2 q F k 0 k w b m k y G 5 I 5 Q h 3 6 B u N X v c C V u / Q s / w z 8 w M 5 2 F b S + E e 3 L u u e H k B D F n G h z n x y q t r K 6 t b 5 Q 3 K 1 v b O 7 t 7 1 f 2 D B y 0 T R a h H J J e q E 2 B N O Y u o B w w 4 7 c S K Y h F w + h i M r 7 P 5 4 x N V m s n o H i Y x 9 Q U e R i x k B I O h 7 p 7 7 z X 6 1 5 j S c v O x l 4 B a g h o p q 9 6 u / v Y E k i a A R E I 6 1 7 r p O D H 6 K F T D C 6 b T S S z S N M R n j I e 0 a G G F B t Z / m V q f 2 i W E G d i i V O R H Y O f t / I 8 V C C w w j o 8 y a n p t l D E j J d d 2 o Y C S y l j 2 T 3 / V E B P V A 1 D O R 0 q F e M A L h h Z + y K E 6 A R m T m I 0 y 4 D d L O Y r E H T F E C f G I A J o q Z r 9 h k h B U m Y M K r m I z c x U S W g d d s X D a c 2 7 N a 6 6 o I q 4 y O 0 D E 6 R S 4 6 R y 1 0 g 9 r I Q w Q N 0 S t 6 Q + / W i / V h f V p f M 2 n J K n Y O 0 V x Z 3 3 / v 7 5 q x < / l a t e x i t >

z3 < l a t e x i t s h a 1 _ b a s e 6 4 = " V w Y P d o e V j X k U U 9 1 2 k w m j d y m a S + E = " > A A A C C H i c Z V B L T s M w F H T 4 l v I r s G Q T U S G x q K o E k I B d B R u W R R B a q Y 0 q x 3 V a q 3 Y c 2 S 9 I J e o J E F s 4 B y v E l l t w D G 6 A k 2 Z B 2 y d Z b z x v n j W e I O Z M g + P 8 W E v L K 6 t r 6 6 W N 8 u b W 9 s 5 u Z W / / U c t E E e o R y a V q B 1 h T z i L q A Q N O 2 7 G i W A S c t o L R T T Z v P V G l m Y w e Y B x T X + B B x E J G M B j q / r l 3 1 q t U n b q T l 7 0 I 3 A J U U V H N X u W 3 2 5 c k E T Q C w r H W H d e J w U + x A k Y 4 n Z S 7 i a Y x J i M 8 o B 0 D I y y o 9 t P c 6 s Q + N k z f D q U y J w I 7 Z / 9 v p F h o g W F o l F n T M 7 O M A S m 5 r h k V D E X W s m f y u x 6 L o B a I W i Z S O t R z R i C 8 9 F M W x Q n Q i E x 9 h A m 3 Q d p Z L H a f K U q A j w 3 A R D H z F Z s M s c I E T H h l k 5 E 7 n 8 g i 8 E 7 r V 3 X n 7 r z a u C 7 C K q F D d I R O k I s u U A P d o i b y E E E D 9 I r e 0 L v 1 Y n 1 Y n 9 b X V L p k F T s H a K a s 7 z / x k Z q y < / l a t e x i t >

Figure 2: Depiction of proposed generative model. The syntax model is composed of discrete random variables, zi. Each ei is a latent continuous embeddings sampled from Gaussian distribution conditioned on zi, while xi is the observed embedding, deterministically derived from ei. The left portion depicts how the neural projector maps the simple Gaussian to a more complex distribution in the output space. The right portion depicts two instantiations of the syntax model in our approach: one is Markov-structured and the other is DMV-structured. For DMV, ztree is the latent dependency tree structure.

ping interspersed regions in the embedding space, evident in Figure 1(a).
In our approach, we propose to learn a new latent embedding space as a projection of pretrained embeddings (depicted in Figure 1(b)), while jointly learning latent syntactic structure – for example, POS categories or syntactic dependencies. To this end, we introduce a new generative model (shown in Figure 2) that ﬁrst generates a latent syntactic representation (e.g. a dependency parse) from a discrete structured prior (which we also call the “syntax model”), then, conditioned on this representation, generates a sequence of latent embedding random variables corresponding to each word, and ﬁnally produces the observed (pre-trained) word embeddings by projecting these latent vectors through a parameterized non-linear function. The latent embeddings can be jointly learned with the structured syntax model in a completely unsupervised fashion.
By choosing an invertible neural network as our non-linear projector, and then parameterizing our model in terms of the projection’s inverse, we are able to derive tractable exact inference and marginal likelihood computation procedures so long as inference is tractable in the underlying syntax model. In §3.1 we show that this derivation corresponds to an alternate view of our approach whereby we jointly learn a mapping of observed word embeddings to a new embedding space that is more suitable for the syntax model, but include an additional Jacobian regularization term to prevent information loss.
Recent work has sought to take advantage of word embeddings in unsupervised generative

models with alternate approaches (Lin et al., 2015; Tran et al., 2016; Jiang et al., 2016; Han et al., 2017). Lin et al. (2015) build an HMM with Gaussian emissions on observed word embeddings, but they do not attempt to learn new embeddings. Tran et al. (2016), Jiang et al. (2016), and Han et al. (2017) extend HMM or dependency model with valence (DMV) (Klein and Manning, 2004) with multinomials that use word (or tag) embeddings in their parameterization. However, they do not represent the embeddings as latent variables.
In experiments, we instantiate our approach using both a Markov-structured syntax model and a tree-structured syntax model – speciﬁcally, the DMV. We evaluate on two tasks: part-of-speech (POS) induction and unsupervised dependency parsing without gold POS tags. Experimental results on the Penn Treebank (Marcus et al., 1993) demonstrate that our approach improves the basic HMM and DMV by a large margin, leading to the state-of-the-art results on POS induction, and state-of-the-art results on unsupervised dependency parsing in the difﬁcult training scenario where neither gold POS annotation nor punctuation-based constraints are available.
2 Model
As an illustrative example, we ﬁrst present a baseline model for Markov syntactic structure (POS induction) that treats a sequence of pre-trained word embeddings as observations. Then, we propose our novel approach, again using Markov structure, that introduces latent word embedding variables and a neural projector. Lastly, we extend our approach to more general syntactic structures.

2.1 Example: Gaussian HMM
We start by describing the Gaussian hidden Markov model introduced by Lin et al. (2015), which is a locally normalized model with multinomial transitions and Gaussian emissions. Given a sentence of length , we denote the latent POS tags as z = {zi}i=1, observed (pre-trained) word embeddings as x = {xi}i=1, transition parameters as θ, and Gaussian emission parameters as η. The joint distribution of data and latent variables factors as:
p(z, x; θ, η) = i=1 pθ(zi|zi−1)pη(xi|zi), (1)
where pθ(zi|zi−1) is the multinomial transition probability and pη(xi|zi) is the multivariate Gaussian emission probability.
While the observed word embeddings do inform this model with a notion of word similarity – lacking in the basic multinomial HMM – the Gaussian emissions may not be sufﬁciently ﬂexible to separate some syntactic categories in the complex pretrained embedding space – for example the skipgram embedding space as visualized in Figure 1(a) where different POS categories overlap. Next we introduce a new approach that adds ﬂexibility to the emission distribution by incorporating new latent embedding variables.
2.2 Markov Structure with Neural Projector
To ﬂexibly model observed embeddings and yield a new representation space that is more suitable for the syntax model, we propose to cascade a neural network as a projection function, deterministically transforming the simple space deﬁned by the Gaussian HMM to the observed embedding space. We denote the latent embedding of the ith word in a sentence as ei ∈ Rde, and the neural projection function as f , parameterized by φ. In the case of sequential Markov structure, our new model corresponds to the following generative process:
For each time step i = 1, 2, · · · , ,
• Draw the latent state zi ∼ pθ(zi|zi−1) • Draw the latent embedding ei ∼ N (µzi, Σzi) • Deterministically produce embedding
xi = fφ(ei)
The graphical model is depicted in Figure 2. The deterministic projection can also be viewed as sampling each observation from a point mass at

fφ(ei). The joint distribution of our model is:
p(z, e, x; θ, η, φ) (2)
= i=1[pθ(zi|zi−1)pη(ei|zi)pφ(xi|ei)],
where pη(·|zi) is a conditional Gaussian distribution, and pφ(xi|ei) is the Dirac delta function centered at fφ(ei):

pφ(xi|ei) = δ(xi − fφ(ei)) =

∞ xi = fφ(ei) 0 otherwise
(3)

2.3 General Structure with Neural Projector
Our approach can be applied to a broad family of structured syntax models. We denote latent embedding variables as e = {ei}i=1, discrete latent variables in the syntax model as z = {zk}Kk=1 (K ), where z1, z2, . . . , z are conditioned to generate e1, e2, . . . , e . The joint probability of our model factors as:

p(z, e, x; θ, η, φ) = i=1 pη(ei|zi)pφ(xi|ei)

· psyntax(z; θ),

(4)

where psyntax(z; θ) represents the probability of the syntax model, and can encode any syntactic structure – though, its factorization structure will determine whether inference is tractable in our full model. As shown in Figure 2, we focus on two syntax models for syntactic analysis in this paper. The ﬁrst is Markov-structured, which we use for POS induction, and the second is DMV-structured, which we use to learn dependency parses without supervision.
The marginal data likelihood of our model is:

p(x) = z psyntax(z; θ)

·

pη(ei|zi)pφ(xi|ei)dei .

i=1 ei

p(xi|zi)
(5)

While the discrete variables z can be marginalized out with dynamic program in many cases, it is generally intractable to marginalize out the latent continuous variables, ei, for an arbitrary projection f in Eq. (5), which means inference and learning may be difﬁcult. In §3, we address this issue by constraining f to be invertible, and show that this constraint enables tractable exact inference and marginal likelihood computation.

3 Learning & Inference
In this section, we introduce an invertibility condition for our neural projector to tackle the optimization challenge. Speciﬁcally, we constrain our neural projector with two requirements: (1) dim(x) = dim(e) and (2) fφ−1 exists. Invertible transformations have been explored before in independent components analysis (Hyva¨rinen et al., 2004), gaussianization (Chen and Gopinath, 2001), and deep density models (Dinh et al., 2014, 2016; Kingma and Dhariwal, 2018), for unstructured data. Here, we generalize this style of approach to structured learning, and augment it with discrete latent variables (zi). Under the invertibility condition, we derive a learning algorithm and give another view of our approach revealed by the objective function. Then, we present the architecture of a neural projector we use in experiments: a volume-preserving invertible neural network proposed by Dinh et al. (2014) for independent components estimation.
3.1 Learning with Invertibility
For ease of exposition, we explain the learning algorithm in terms of Markov structure without loss of generality. As shown in Eq. (5), the optimization challenge in our approach comes from the intractability of the marginalized emission factor p(xi|zi). If we can marginalize out ei and compute p(xi|zi), then the posterior and marginal likelihood of our Markov-structured model can be computed with the forward-backward algorithm. We can apply Eq. (3) and obtain :

p(xi|zi; η, φ) = pη(ei|zi)δ(xi − fφ(ei))dei.
ei
By using the change of variable rule to the integration, which allows the integration variable ei to be replaced by xi = fφ(ei), the marginal emission factor can be computed in closed-form when the invertibility condition is satisﬁed:

p(xi|zi; η, φ)

−1

∂fφ−1

= xi pη(fφ (xi)|zi)δ(xi − xi) det ∂xi dxi

−1

∂fφ−1

= pη(fφ (xi)|zi) det ∂xi ,

(6)

where pη(·|z) is a conditional Gaussian distribu-

tion, ∂fφ−1 is the Jacobian matrix of function f −1

∂xi

φ

∂f −1

at xi, and

det

φ
∂x

represents the absolute value

i

of its determinant. This Jacobian term is nonzero

and differentiable if and only if fφ−1 exists.

Eq. (6) shows that we can directly calculate the

marginal emission distribution p(xi|zi). Denote

the marginal data likelihood of Gaussian HMM as

pHMM(x), then the log marginal data likelihood of

our model can be directly written as:

log p(x) = log pHMM(fφ−1(x))

∂fφ−1

(7)

+

log det

,

i=1

∂xi

where fφ−1(x) represents the new sequence of embeddings after applying fφ−1 to each xi. Eq. (7) shows that the training objective of our model is
simply the Gaussian HMM log likelihood with an
additional Jacobian regularization term. From this
view, our approach can be seen as equivalent to reversely projecting the data through fφ−1 to another manifold e that is directly modeled by the
Gaussian HMM, with a regularization term. Intuitively, we optimize the reverse projection fφ−1 to modify the e space, making it more appropri-
ate for the syntax model. The Jacobian regular-
ization term accounts for the volume expansion or
contraction behavior of the projection. Maximiz-
ing it can be thought of as preventing information
loss. In the extreme case, the Jacobian determi-
nant is equal to zero, which means the projection
is non-invertible and thus information is being lost
through the projection. Such “information pre-
serving” regularization is crucial during optimiza-
tion, otherwise the trivial solution of always pro-
jecting data to the same single point to maximize likelihood is viable.2

More generally, for an arbitrary syntax model the data likelihood of our approach is:

p(x) = ·

z psyntax(z)

−1

∂fφ−1

i=1 pη(fφ (xi)|zi) det ∂xi

(8) .

If the syntax model itself allows for tractable inference and marginal likelihood computation, the same dynamic program can be used to marginalize out z. Therefore, our joint model inherits the tractability of the underlying syntax model.

2For example, all ei could learn to be zero vectors, leading to the trivial solution of learning zero mean and zero variance Gaussian emissions achieving inﬁnite data likelihood.

··

< l a t e x i t s h a 1 _ b a s e 6 4 = " 3 n d 3 u m u W a C + M i 7 m / 2 B 1 m K w n u d N 6 j M x o B / I u g l 9 J V R k O y + I F r 6 u G U Y 0 = " > A A A C C 3 i c Z V D L T g I x F O L 3 2 g D C L 8 c Q X 6 t L N R G L i g p D B j b o w E t 2 4 x M Q B E i C k U z p Q a a e T 9 o 4 J m I f S A R L + x g q X 1 G u r / 3 Q + V H X K x u q D 3 V x + E g 4 5 y / f h 4 H R 9 / g Y B G F V w g I I 3 3 a K e S 7 5 p p u + e e c e 2 2 p 5 y w e e I P B + T J f M o g e + d v 9 + O W Z L m m l l 5 5 Z Z X X U V t t u f 5 y 7 6 b v 2 b N G z 5 a t 3 b s 1 n T v 2 7 N t 2 W r M a S x j k R r l Q P j l 0 V i C u 6 V U R Z N A H D 2 B v M K 8 W Y U j g 5 9 y Y F M K B w p R M a 1 0 I Z U k C I 5 F / g T 9 h G j F + y 8 n T 8 u / e o N D B 0 6 4 o a 0 r k 6 + A E 6 d H j M C W L t a L E 0 b o g t f 4 s y o C A l R B D S I 9 a V q a t 0 K l v P Q g d u P 4 I W F i r m + 7 R Z N z y c l p 0 Z E B 5 J T U 9 p Q K u F P 5 z + K 2 X R T f w R C 6 Q 7 L d W d T 6 / x 2 Z l + 1 2 F z U 1 0 J k Y i k 5 F A D K I Y B k x y r z 3 7 a M q X 4 Y E H X h Q G S N r n I A A o R 2 T z s r d U 2 S O w 9 2 Y J 0 C w B m 6 S T I H + m 7 h R Z l G Y R I D g L F T 1 H Z k 0 3 k c c j z t t 0 2 j j y g 3 z T T d c U w G K l p 7 z I A n n Q B n y 7 d P j + / N G E w Z k F W G W E m u A x Y b G Z G d X r a M 9 z M C w x s l Z U U C B l K h r i k l t a G F B f Q Z O m R 2 t 9 v J S n Z J 7 3 K Q 5 x H l w U i A / x 5 k o M p R S V K p l E A 5 7 o 0 5 n I B x E i I e z t j U o c J 8 C i 6 h M N Y k a E E Z g 3 m 6 P C o B K P Y h O o y n C L d T N Z B N m w n u x 1 x 4 Q y l i w G E F c p G A Y q K O K b Y 2 + K Y y p 7 D t B E l 0 h 0 h o A 2 i v Y x / y 2 N 2 q R P U y m f U C 9 K k L E w X D g 8 n p 5 n f Z O e y 8 e W + 6 s 9 W Q q u 1 Y d J o p U Z n e l E 0 A g D A u 7 E R Y M y a n q A g K U F 1 b R i F B N K 6 v i h G A P 4 E R T 6 Q e P 4 X Q p V C e z n + S j f F n e z r X T l e 3 r P H q f b r S Y j y P L O N 3 W s d w O 8 d z f 5 T X R z T + 1 A u l c 1 f g e n o g e Y f = e < g / = l = a < t / e l x a i t e > x i t >·

e i,l < l a t e x i t s h a 1 _ b a s e 6 4 = " R K 1 3 y 4 7 C c x d R 2 g l S f 1 N v W y E I b a U = " > A A A C E n i c Z V D L S g M x F M 3 4 r P V V d e k m W A Q X p U x F U H d F N y 4 r O L b Q D i W T Z t r Q Z D I m d 4 Q y z G e I W / 0 O V + L W H / A z / A M z 0 y 5 s e y H c k 3 P P D S c n i A U 3 4 L o / z s r q 2 v r G Z m m r v L 2 z u 7 d f O T h 8 N C r R l H l U C a U 7 A T F M 8 I h 5 w E G w T q w Z k Y F g 7 W B 8 m 8 / b z 0 w b r q I H m M T M l 2 Q Y 8 Z B T A p b y e 4 F M W d Z P e Q 2 L r F + p u n W 3 K L w M G j N Q R b N q 9 S u / v Y G i i W Q R U E G M 6 T b c G P y U a O B U s K z c S w y L C R 2 T I e t a G B H J j J 8 W p j N 8 a p k B D p W 2 J w J c s P 8 3 U i K N J D C y y r y Z u V n O g F L C 1 K w K R j J v + T P F 3 U x k U A t k L R d p E 5 o F I x B e + S m P 4 g R Y R K c + w k R g U D g P C A + 4 Z h T E x A J C N b d f w X R E N K F g Y y z b j B q L i S w D 7 7 x + X X f v L 6 r N m 1 l Y J X S M T t A Z a q B L 1 E R 3 q I U 8 R N E T e k V v 6 N 1 5 c T 6 c T + d r K l 1 x Z j t H a K 6 c 7 z / 2 1 5 8 K < / l a t e x i t >

e i < l a t e x i t s h a 1 _ b a s e 6 4 = " R E k A E C s k l 6 v T P Q b 6 K r E 5 y d 3 H U v c = " > A A A C D X i c Z V D L S g M x F M 3 U V 6 2 v q k s 3 w S K 4 K G U q g r o r u n F Z w b G F d i i Z N N O G J p M h u S O U o f 8 g b v U 7 X I l b v 8 H P 8 A / M T G d h 2 w v h n p x 7 b j g 5 Q S y 4 A d f 9 c U p r 6 x u b W + X t y s 7 u 3 v 5 B 9 f D o y a h E U + Z R J Z T u B s Q w w S P m A Q f B u r F m R A a C d Y L J X T b v P D N t u I o e Y R o z X 5 J R x E N O C V i q 2 w 9 k y m Y D P q j W 3 I a b F 1 4 F z Q L U U F H t Q f W 3 P 1 Q 0 k S w C K o g x v a Y b g 5 8 S D Z w K N q v 0 E 8 N i Q i d k x H o W R k Q y 4 6 e 5 3 x k + s 8 w Q h 0 r b E w H O 2 f 8 b K Z F G E h h b Z d b M w i x j Q C l h 6 l Y F Y 5 m 1 7 J n 8 b q Y y q A e y n o m 0 C c 2 S E Q i v / Z R H c Q I s o n M f Y S I w K J x l g 4 d c M w p i a g G h m t u v Y D o m m l C w C V Z s R s 3 l R F a B d 9 G 4 a b g P l 7 X W b R F W G Z 2 g U 3 S O m u g K t d A 9 a i M P U S T Q K 3 p D 7 8 6 L 8 + F 8 O l 9 z a c k p d o 7 R Q j n f f 2 g o n S g = < / l a t e x i t >
f< l a t e x i t s h a 1 _ b a s e 6 4 = " w N k Q f D 9 P u i t K T G H U c H Q m F 9 b / f + v K S i 3 R V n q 5 9 w h z Z 6 Y H 9 v D h M r Z U U k = " > A A A C B n i c Z V D L T g I x F L O 3 j g E C / G F u t T S F Y R i G c L T i E g B p S D E B z j b b t o Q j d u 0 n Y E 1 J L i S S B M w k h Q A E U i I n 6 d p K Q C M h N n 7 U X 7 T a S O 3 y j Z E k h w E g 5 c Y Y u t j / F o v d 9 r D o l w d u G / l Q v 1 6 3 G / 3 o + J D / K Y P A 7 d A Y z C s N B y C k 4 u S a X f N n P n z t z u 2 c 3 n O i T A 1 W + 3 x K J D k n G f x T / s m r 2 q c 2 i v u r G a 5 + u s 5 b r + f c z 3 2 C z 1 u v 7 b e O f 7 u l H 5 g x 8 / M + G B o B R y F 1 P g m R U 6 y h W H U J b p g W b r E 6 M W M F E P j O 5 Q i u N o H B w A Z 0 q 6 x b Z k k a Q J G Y g + j J W w C 2 4 / W O 0 F 2 t b O z m w 8 y 8 b U b q i W K Z 7 D n O E 9 U h s F 4 N 4 G k O / w Y P i 2 H Q n B B Y K x 0 g V M D F 3 Q s 9 F 6 o B p Z e L 2 T Z s u X U J u y w l w 4 q G M 7 1 g C y s U n q n s z e W T f + w s C / g T 1 y i a 3 T 8 W t L n f u 6 K 2 J e p 5 J L F E S g A o U Z x A p O l N X a x 6 Y 5 u T y o k R R d C B O K n s g o B 3 F z O 7 x c 4 S V w 2 m r N G A m h E 6 y b R O D W 3 h a R c G v R A z E H A T u S q i O d 0 G l x m e d 2 G a y Z f n G h a s Z q n b B U 1 + K E Z 7 E o 4 T K 9 d v s 5 f E 8 S 3 a E S i T y B 0 g w V D V A m w z y c r z T N p M u g V a n V K E g q J Z R k c V l T 4 i 0 Q K W B c i u J e t m 6 d T z P N Z S X A Y a + l E Q X J / Y Z y F k O T R a U h p W H T e C g C F 4 I V x U B n c 5 d V R G I c W I R I j v H o Q 1 k E e x Y 9 C B B D e G V 3 m Q 4 d X p i p 9 K r H h a m P F K M U X q I A A j k w I z 1 A t R 1 D 9 H x z 6 F Y Z B s o M Q s t M F I G E l T 7 H c Q Z F V k R 5 Y G T 7 W m Q M b g + y R 8 f C m 4 6 q 7 1 N x V W t n V b j r c K w 6 r Q R d w P c K w o y m N c 0 Q i w s U 6 u R o i Q y p 5 3 R U F A d M 2 f h K G D v B I 4 Q h Q h R d S 4 9 d o Z F 6 f c 0 N Z + j f 1 d b + 7 Z 9 h a K H V 9 5 T z m Z V z 5 h q H z M Z l z f i P G 1 a B K + n v t r n D D / k I = m < n / Z l 8 a = t < e / x l i a t > e x i t >

=< l a t e x i t s h a 1 _ b a s e 6 4 = " t J t 7 7 1 s 0 H i f a Q w o j C l O D 2 t k 3 4 K Y c f c w h N 2 U y E C E + i l 7 H N T c g 9 m M c Q U = " > A A A C E 3 i c Z V D B N S g w M x E J M 6 3 t W f r 7 X p + V T 1 6 W S y C h 1 K 2 X t R D s e j F Y w V r C 2 + 0 1 t a 2 s T t T l b s h G i 5 a p b s J l Z m k R V W y K l E L v w / K h c n S h r V P / o 4 c Y n E T z x 5 A F D n 6 + G b / 2 8 C B 2 s 9 2 W 4 D N b t g B T 8 B K f 8 v v H l k m z m z P D n w 8 / S 5 H k C y D n 6 v / f 5 t Y Z u J a a X W l V l 1 d b W X 1 s / u L u q 5 9 j s c b 2 m t 1 7 v Z V 3 P 8 Y 7 3 t b 6 v d T U M b l G m E r N E o 6 j V k U L r r V p 8 E r 8 C M l E n D E 1 W k 0 d A O A Q 0 r 5 W b j s D a Q J j Y 0 + h J e w s 2 4 / Q c + F v V 0 V n m r 8 j + g U W K n W D Z V j X G i 5 L h o G 4 F h N 1 P J 4 O F m 7 H E P Q O k C Y U w o G K O X q u + 2 4 7 0 4 s S e P 6 T K O E S w N K i a W 2 V O c u T o d W f i 8 W E 3 r b e H J 4 N S x y F C U y P h 4 Q D U h L Y z v 7 P t X a O v U z R 6 A b G d r e d 7 / h E d + 9 7 O p I 2 E g k s i W a Y A h S U E E Y G 6 N 3 a b Z F S T / c C G T L k 8 I U 0 K c G i O r F Y 0 O Z N H e c O S D T Y W s N I M H B Z r I h + H a 2 1 w k Z Y G E W s F l D M t J p 5 e l O s t P R X 8 a 6 P R L Y N Q N I z n A l 6 M X q t 8 C 9 J G w d x s + P 7 8 7 j E x i U K I N L J D D H i 2 w j y z j J S K Z e m q V W r U K M o S F M L l C 1 F y K a 0 i K g B L z 7 J K N U 6 j Z R j n J / P s z 4 2 N o K 6 v M i t h i l K I t q I V m D M P H b O c L I Y h H G D d W e S y X q g I Y 4 x A c R h q C R O y t R 0 5 j h i w I h W 2 L Q y T k u 0 a d Q c E n z t B c F M C 4 f p C i h Z A A Z G g h o m Z t k t 5 T x X S D B o 8 g r m T l M C D 0 4 P a u B a u s P R K + v V O 5 O R L x I Z L B G / S a f R m 0 8 X 7 v N J 6 u 4 v x E d L o 1 l E m q k a Q R e h H Q a M B 4 D h d G I M w o q w 6 y B l T U V 4 0 R D p W q U o A w c Y K i G S p K 7 E h n B 9 V I 6 x d e J r + E f N r e z X c q + 3 p P t i K b M S 8 n 9 D e X z t D 2 z U P c h z f Y P X 0 3 C 9 D A O S C o h H p o Q x = k = < / l a t e x i t >

Inverse Projection

f
< l a t e x i t s h a 1 _ b a s e 6 4 = " T f 5 j O h 2 I W x 5 w f w d s B 5 e 8 9 G Y s V u b v R P k W g x 7 j t 5 B s W e S U 9 / q c h m G g 8 = " > A A A C I 3 i c Z V B A N 9 L T w 8 R M B w E K H 3 X x 4 b L X O 0 U t r j w i A Q g 6 S R E E Q K g y J 1 p o F w I L S b F s m L C F r k Y c G Q E i s s E W q v W S V 0 2 9 h t I j 5 O r 7 k q O n t J 2 9 n 0 F 1 k Y X j x O B Z V i l x I 8 U j f r g v x w i O B J n 3 8 F D x M c h P F A g T Y / G A F M j 9 Z u + w A 4 c + 4 K L u Q n M U f q J 1 e n v 3 O 7 l t U 0 v 7 S 6 q + W 4 w F 6 M P W t c v a 3 X s P D f g V 0 G P h D k I d 6 G N 5 h + 4 Y Z L W E J w y V a p n 2 q d m O 5 j + t b 3 t a h n c V U m z G L G R 8 N w F L a b I U 1 5 I j L 6 l j U l j U w i J S p 8 y g F Q t I E l a P M 0 O 8 C N 0 p E y S q u S K / R C c y B 6 p P i e j g 7 d Z 5 z f f 3 c 6 W J K V G W T a E y + e y g m U v + K j X F o t V C S 3 J w i R w s S Z g A 6 R K D q I w b u y x 7 m Z H X e Q j T F 1 T u e B T S D F u t i x K l C 2 7 X z Z j e a b B r Y t K Z + Z v t b 5 I f h Z T X r 5 Y b X M X u F 3 r 1 / 9 u 2 9 y I O P w 9 j B n 8 P A / 1 C W + 9 w j X Y p / l l 5 g 3 g 1 A 1 H + I / X H V u z o 2 + Z r b s n 0 6 z 2 x O B J J I m k g 1 E l R 4 C E O f t o W q 5 t 6 n b B g o z U t T F P C K t i g 0 h s N w O s T 2 y E m o 7 0 p j l T f H 8 p w 4 s Q G v E a K N m D 5 D b C e g e u + p K 2 g O q r w g 6 i p c k z 1 Y i M b 0 d 3 x F L C k q P c T y Y L n w x B M m 5 w V P V y Z d R S 7 L D L h T l A m 0 e D y X v K X P s O m l g f 1 v t Z L w W B n K A b o k 7 u q G k R z V l 0 N R 7 Z 3 7 a y d b l w V a U 1 i 7 1 o S u t g F B I k h b S 2 L z l + I L 6 Y 1 L H z 8 T W y g k X W C S / Z n s b g I T o 1 T t o 8 B j G z Z i L R h B H T m U H r A H S p F J s M Y 7 z 5 l n B S 2 Y H o a g D R M 4 C 3 H w c B K M Y F R D 1 O q n K O E K P S n L a F 8 S V Z 5 g F f P C x 0 1 a 5 j D 7 9 y o / b j N v V w 3 H 6 t / d 6 3 x y 8 Q 2 d o k f 9 + M j V E 4 G d C o 7 m A E M U a 0 x D D J A a N Q u 5 z v B I I Q R 3 x u B o A g x o j 5 c R w F Q d M U 8 Q w Q p d N f 3 o 7 F z t 1 2 7 j L B 9 + 5 v r G X e z r r K g e f r c Z / e M h w d 6 M / T w 6 3 n r l 8 l A C A v w 8 e J n 6 Q + Q w = K = D < u / q l m a q t < e / x l i a t > e x i t >

1(xi)

h(i,2l) < l a t e x i t s h a 1 _ b a s e 6 4 = " c J N e T q 2 N h p m 5 E 1 h w B l Q I Z x 9 c 5 V c = " > A A A C G n i c Z V D L S g M x F M 3 4 r P U 1 2 q W b Y B E q l D I t g r o r u n F Z w d p C W 0 s m z b S h y W R I 7 g h l m E 8 R t / o d r s S t G z / D P z D T d m H b C + G e n H t u O D l + J L g B z / t x 1 t Y 3 N r e 2 c z v 5 3 b 3 9 g 0 P 3 6 P j R q F h T 1 q R K K N 3 2 i W G C h 6 w J H A R r R 5 o R 6 Q v W 8 s e 3 2 b z 1 z L T h K n y A S c R 6 k g x D H n B K w F J 9 t 9 D 1 Z T J K + w k v Y 5 E + J a X a e d p 3 i 1 7 F m x Z e B d U 5 K K J 5 N f r u b 3 e g a C x Z C F Q Q Y z p V L 4 J e Q j R w K l i a 7 8 a G R Y S O y Z B 1 L A y J Z K a X T M 2 n + M w y A x w o b U 8 I e M r + 3 0 i I N J L A y C q z Z h Z m G Q N K C V O 2 K h j J r G X P T O 9 m I v 2 y L 8 u Z S J v A L B m B 4 K q X 8 D C K g Y V 0 5 i O I B Q a F s 6 D w g G t G Q U w s I F R z + x V M R 0 Q T C j b O v M 2 o u p z I K m j W K t c V 7 / 6 i W L + Z h 5 V D J + g U l V A V X a I 6 u k M N 1 E Q U T d A r e k P v z o v z 4 X w 6 X z P p m j P f K a C F c r 7 / A H 3 Y o V M = < / l a t e x i t >
+< l a t e x i t s h a 1 _ b a s e 6 4 = " 9 n q 3 0 M F P F B g z W o i P 8 j a 7 Q Q U / I d Q = " > A A A C E 3 i c Z V D N S s N A E N 7 U v 1 r / q h 6 9 B I s g W E o q g n o r e v F Y w d h C G 8 t m s 2 m X 7 m b D 7 k Q o o a 8 h X v U 5 P I l X H 8 D H 8 A 3 c p D n Y d m C Z b 7 / 5 Z p j 5 / J g z D Y 7 z Y 5 V W V t f W N 8 q b l a 3 t n d 2 9 6 v 7 B o 5 a J I t Q l k k v V 9 b G m n E X U B Q a c d m N F s f A 5 7 f j j 2 6 z e e a Z K M x k 9 w C S m n s D D i I W M Y D D U U 9 + X P N A T Y V J 6 N h 1 U a 0 7 D y c N e B s 0 C 1 F A R 7 U H 1 t x 9 I k g g a A e F Y 6 1 7 T i c F L s Q J G O J 1 W + o m m M S Z j P K Q 9 A y M s q P b S f O u p f W K Y w A 6 l M i 8 C O 2 f / d 6 R Y a I F h Z J R Z 0 n O 1 j A E p u a 4 b F Y x E l r I x + T 8 7 q O 6 L e i Z S O t Q L i 0 B 4 5 a U s i h O g E Z n t E S b c B m l n D t k B U 5 Q A n x i A i W L m F J u M s M I E j I 8 V 4 1 F z 0 Z F l 4 J 4 3 r h v O / U W t d V O Y V U Z H 6 B i d o i a 6 R C 1 0 h 9 r I R Q Q p 9 I r e 0 L v 1 Y n 1 Y n 9 b X T F q y i p 5 D N B f W 9 x 9 L X Z / K < / l a t e x i t >

h
< l a t e x i t s h a 1 _ b a s e 6 4 = " B d U 3 S 2 q e 0 J h N b B Y P Z t l z P K X I f i 9 y n l q e Q 1 L 1 O p e h l + C S 9 0 4 t 1 5 5 v W q V p c t c b J 4 y d v P N 1 i k + J T 5 O Y D t t h N o 1 f R E H y m M A u x f D J j B N W 5 u 0 M 4 s = " > A A A C B G 9 D n 3 i c Z V D L S T g M I x F M L O 3 U j 4 V s R 6 x 9 H 2 a y v r q h a 1 V L p r Z N d W 0 u b 0 g Y E k B h W F V N o c Q M U l K C M D G J q J X l M 1 G x C o j O + b q 4 o 6 u E T I 6 N 3 b M y L l Z 4 i x l r s i B O Y 4 W L G g s b 2 g L Q h J b D r I S i O 2 W m Z T U N y Z D N b t j O S p G h g 0 J e M 0 p Q h 0 M z m l h J S 7 u H O x S a 0 4 O E I R U M Z Y X 5 T Q l 5 t P O F u E M 3 / r W O Q X / p 5 6 0 3 X H O u 4 K V B s / 8 K f E a 3 4 L t b G / v f A z w 6 z / M B / D / m w P 8 W E 7 B k z A M X b D 2 t h L 4 r b A V 0 Y R t Q X u L 7 w 0 4 s j t R m 0 z 7 5 T c 5 9 u 4 9 8 6 a x 9 5 T w t 4 E c T e 2 s k T e J 8 4 C E Q k W C e w o 2 A i u 7 G C + A X H / d P 6 + X f + 2 H 2 t c y s r t a 7 K f 2 u 6 W t 3 N b f z 2 1 a x A y 3 u / W t 5 9 t w b I s k c 8 r 5 L a b x O / l d 7 v Z 2 t H 3 / 7 J d / + a v 4 w b e L f P p B F x + 0 w Y 5 d 6 n c P N V H x H u q o o K 1 V 2 A D K J u D I N p e W F Y 8 Z t l 6 M o p k q M S o Y 0 S X m w r T K m c b Z p D J 4 5 Y 4 I V p Y q L j J F E g H w v E r E I f I G U O m w C B c Q J g B W e 2 C Y 8 D s Y k t H k h W W b l D l F M G g V i p / S A C X t 8 9 j 4 F Y 2 O a y U n w x M 2 f / 7 6 g f 7 K 2 Z r e n v 8 f 7 P x u e W Y Z e M r G m 9 t y T O U t Z G z 0 c q z f R / D I Q A M + 8 B W T w J S h i h T P V H J c l r R V P S P H k T O c m I W U H M U i E e g E Q c K o 0 A N y 4 p 6 i J A 4 o W U 2 w K v Y b p 1 r V f i 6 w K 6 A 1 n W R 6 c u V o D L 4 s m 5 W n Y N 3 E 6 R Q + y 2 X y f k Y E s 9 E S p 4 W d r F W y Y t P G v W w R 0 4 x P K + O a a T w d l t 3 U 4 K O 7 Z N 8 z 2 M k / m 0 G q / l 1 W U 5 i H 0 j l Z A f t r F 8 3 7 s a p r X Z 4 T b F V w Y l X K D E g v T M D K x c m I B H o C D F I m e T z K b S m A V v 1 U H Z e 8 F r g E R / X W / 0 f / e 2 7 i k x s i P 0 D 3 N o R L U m W C s k P N V g J 0 j W Q R Z A q i J R C Z J X C s a E G 6 2 m N K w I P A 3 7 x i 8 n 3 k B x A t P h m s B 6 o Z L C q N i d G S Q H w i Z A u Y O q 8 5 B W K H c B V m u p j 5 K o E l 3 R p 1 c s x q S b q e w F 4 U m h 4 P N E 4 Z A 6 C E x J X R G k L 7 b P H z K W / r O s N Y h T w E R A o w G k i V R k r t z p S / P l Q 1 T e w s S M d a q j c f W q 4 k f y n M 4 2 J n l b 4 1 P V 4 l L Z m D y k g H Y k E C D g O g W b l t w d 7 D r 2 Y 0 a I k h d k A 4 C U T C q / 9 n a b 7 y v / f b R G y O k M U a m h z V R 0 1 V R k d R h g 3 T I C O H Y I Z T W 6 d l W v y k X M O 0 e m Z + z l b 2 M m s 9 I Y U W s Z G M Q Z K 0 w v i A o a 1 p J D t Y U S H W z O W p V Q n W q 3 w B m k r S A Y E O k F q Z s V W t 5 1 v / Y k H y 9 O M Z M T 9 6 7 N G d 2 G 4 1 b l n M i j K Z f z q F S Q y A r X H L v G q Z q D l C x Q 6 0 m u I X I j a m 8 I R 0 E 3 O C J s a s N g J 2 4 h S Q y G M E C M Q g m L X q D v t u V e A Z S w q i H s O J M H U o u U J B K e s h 5 c I b h x j S O m m i P f Y S q I 8 E J S g U J F X E w h H t K g C S B U + k w z i C H o 6 I h L i o Q C 7 M e A G h c 6 c e M 4 g E 0 Z 2 M o B C 5 i T K c I G 8 Q k x 7 B F g d y h F 5 o G D 3 x p N g w u 7 K X v V U y 4 c B F L w 1 s p H d T i R I A G F 4 1 h N u l C K 0 6 w N B O c g X I Z 4 q s Z C o u t z h 4 B Y t y l j 0 C 5 b j x y z x U 4 m 5 S R W s V D Q A d 4 e v A W u + 8 I i r o c f 1 g d l 3 S X 1 X 9 w 3 3 o 7 P w 3 9 u / w 1 L v o K c z w 4 v f R 2 1 v E Y m 0 c e H v w V l p y g Y Z k G e J E d H 1 c o a r A F M x H N T V X U V p c R E G A R E d t 6 e o N 6 u r a R k b A R z G 1 1 v D 0 d P C h I 0 b 5 c d z r a V C I q l G R I y 7 x k g x R o = 7 N m < L 0 q / 9 C B l 6 t X a H 6 9 t Q I e z + b x k / e i L O n t c i R > 8 / f W P n b h w p f / 3 D l o p 0 A f v / M m 8 + b r m S 7 a n + M D g 9 P W 8 f M p K o a 5 4 G / V F N y c < v r / v 7 l 8 / a A A t e H e v w x S 0 i h o t T V > g I = = < / l a t e x i t >

(1) i,l

=< l a t e x i t s h a 1 _ b a s e 6 4 = " t J t 7 7 1 s 0 H i f a Q w o j C l O D 2 t k 3 4 K Y c f c w h N 2 U y E C E + i l 7 H N T c g 9 m M c Q U = " > A A A C E 3 i c Z V D B N S g w M x E J M 6 3 t W f r 7 X p + V T 1 6 W S y C h 1 K 2 X t R D s e j F Y w V r C 2 + 0 1 t a 2 s T t T l b s h G i 5 a p b s J l Z m k R V W y K l E L v w / K h c n S h r V P / o 4 c Y n E T z x 5 A F D n 6 + G b / 2 8 C B 2 s 9 2 W 4 D N b t g B T 8 B K f 8 v v H l k m z m z P D n w 8 / S 5 H k C y D n 6 v / f 5 t Y Z u J a a X W l V l 1 d b W X 1 s / u L u q 5 9 j s c b 2 m t 1 7 v Z V 3 P 8 Y 7 3 t b 6 v d T U M b l G m E r N E o 6 j V k U L r r V p 8 E r 8 C M l E n D E 1 W k 0 d A O A Q 0 r 5 W b j s D a Q J j Y 0 + h J e w s 2 4 / Q c + F v V 0 V n m r 8 j + g U W K n W D Z V j X G i 5 L h o G 4 F h N 1 P J 4 O F m 7 H E P Q O k C Y U w o G K O X q u + 2 4 7 0 4 s S e P 6 T K O E S w N K i a W 2 V O c u T o d W f i 8 W E 3 r b e H J 4 N S x y F C U y P h 4 Q D U h L Y z v 7 P t X a O v U z R 6 A b G d r e d 7 / h E d + 9 7 O p I 2 E g k s i W a Y A h S U E E Y G 6 N 3 a b Z F S T / c C G T L k 8 I U 0 K c G i O r F Y 0 O Z N H e c O S D T Y W s N I M H B Z r I h + H a 2 1 w k Z Y G E W s F l D M t J p 5 e l O s t P R X 8 a 6 P R L Y N Q N I z n A l 6 M X q t 8 C 9 J G w d x s + P 7 8 7 j E x i U K I N L J D D H i 2 w j y z j J S K Z e m q V W r U K M o S F M L l C 1 F y K a 0 i K g B L z 7 J K N U 6 j Z R j n J / P s z 4 2 N o K 6 v M i t h i l K I t q I V m D M P H b O c L I Y h H G D d W e S y X q g I Y 4 x A c R h q C R O y t R 0 5 j h i w I h W 2 L Q y T k u 0 a d Q c E n z t B c F M C 4 f p C i h Z A A Z G g h o m Z t k t 5 T x X S D B o 8 g r m T l M C D 0 4 P a u B a u s P R K + v V O 5 O R L x I Z L B G / S a f R m 0 8 X 7 v N J 6 u 4 v x E d L o 1 l E m q k a Q R e h H Q a M B 4 D h d G I M w o q w 6 y B l T U V 4 0 R D p W q U o A w c Y K i G S p K 7 E h n B 9 V I 6 x d e J r + E f N r e z X c q + 3 p P t i K b M S 8 n 9 D e X z t D 2 z U P c h z f Y P X 0 3 C 9 D A O S C o h H p o Q x = k = < / l a t e x i t >

xi < l a t e x i t s h a 1 _ b a s e 6 4 = " Y T T h J c j f 7 T h L 3 C 1 a r I c 7 i Y N 4 i X U = " > A A A C D X i c Z V D L S g M x F M 3 4 r P V V d e k m W A Q X p U x F U H d F N y 4 r O L b Q D i W T Z t r Q Z D I k d 8 Q y 9 B / E r X 6 H K 3 H r N / g Z / o G Z 6 S x s e y H c k 3 P P D S c n i A U 3 4 L o / z s r q 2 v r G Z m m r v L 2 z u 7 d f O T h 8 N C r R l H l U C a U 7 A T F M 8 I h 5 w E G w T q w Z k Y F g 7 W B 8 m 8 3 b T 0 w b r q I H m M T M l 2 Q Y 8 Z B T A p b q 9 A K Z P k / 7 v F + p u n U 3 L 7 w M G g W o o q J a / c p v b 6 B o I l k E V B B j u g 0 3 B j 8 l G j g V b F r u J Y b F h I 7 J k H U t j I h k x k 9 z v 1 N 8 a p k B D p W 2 J w K c s / 8 3 U i K N J D C y y q y Z u V n G g F L C 1 K w K R j J r 2 T P 5 3 U x k U A t k L R N p E 5 o F I x B e + S m P 4 g R Y R G c + w k R g U D j L B g + 4 Z h T E x A J C N b d f w X R E N K F g E y z b j B q L i S w D 7 7 x + X X f v L 6 r N m y K s E j p G J + g M N d A l a q I 7 1 E I e o k i g V / S G 3 p 0 X 5 8 P 5 d L 5 m 0 h W n 2 D l C c + V 8 / w G H Z 5 0 7 < / l a t e x i t >

xi,l < l a t e x i t s h a 1 _ b a s e 6 4 = " v w q 6 y + P o H k b N q h + f p E P Z i l a E d a o = " > A A A C E n i c Z V D L T g I x F O 3 g C / G F u n T T S E x c T M h g T N Q d 0 Y 1 L T E R I Y E I 6 p Q M N 7 X R s 7 x j J Z D 7 D u N X v c G X c + g N + h n 9 g B 1 g I 3 K S 5 p + e e 2 5 y e I B b c g O f 9 O I W V 1 b X 1 j e J m a W t 7 Z 3 e v v H / w Y F S i K W t S J Z R u B 8 Q w w S P W B A 6 C t W P N i A w E a w W j m 3 z e e m L a c B X d w z h m v i S D i I e c E r C U 3 w 1 k + p z 1 U u 5 i k f X K F a / q T Q o v g 9 o M V N C s G r 3 y b 7 e v a C J Z B F Q Q Y z o 1 L w Y / J R o 4 F S w r d R P D Y k J H Z M A 6 F k Z E M u O n E 9 M Z P r F M H 4 d K 2 x M B n r D / N 1 I i j S Q w t M q 8 m b l Z z o B S w r h W B U O Z t / y Z y d 2 M Z e A G 0 s 1 F 2 o R m w Q i E l 3 7 K o z g B F t G p j z A R G B T O A 8 J 9 r h k F M b a A U M 3 t V z A d E k 0 o 2 B h L N q P a Y i L L o H l W v a p 6 d + e V + v U s r C I 6 Q s f o F N X Q B a q j W 9 R A T U T R I 3 p F b + j d e X E + n E / n a y o t O L O d Q z R X z v c f F o S f H Q = = < / l a t e x i t >

g< l a t e x i t s h a 1 _ b a s e 6 4 = " j H j 0 N q d Q + 8 7 f b Q S 5 p r P S 5 R H L n O U = " > A A A C B n i c Z V D L S g M x F M 3 4 r P V V d e k m W A Q X p U x F U H d F N y 5 b c G y h H U o m z b S h y W R I 7 g h l 6 A e I W / 0 O V + L W 3 / A z / A M z 0 1 n Y 9 k K 4 J + e e G 0 5 O E A t u w H V / n L X 1 j c 2 t 7 d J O e X d v / + C w c n T 8 Z F S i K f O o E k p 3 A 2 K Y 4 B H z g I N g 3 V g z I g P B O s H k P p t 3 n p k 2 X E W P M I 2 Z L 8 k o 4 i G n B C z V H g 0 q V b f u 5 o V X Q a M A V V R U a 1 D 5 7 Q 8 V T S S L g A p i T K / h x u C n R A O n g s 3 K / c S w m N A J G b G e h R G R z P h p b n S G z y 0 z x K H S 9 k S A c / b / R k q k k Q T G V p k 1 s z D L G F B K m J p V w V h m L X s m v 5 u p D G q B r G U i b U K z Z A T C G z / l U Z w A i + j c R 5 g I D A p n o e A h 1 4 y C m F p A q O b 2 K 5 i O i S Y U b H R l m 1 F j O Z F V 4 F 3 W b + t u + 6 r a v C v C K q F T d I Y u U A N d o y Z 6 Q C 3 k I Y o Y e k V v 6 N 1 5 c T 6 c T + d r L l 1 z i p 0 T t F D O 9 x + Y v 5 n 5 < / l a t e x i t > g< l a t e x i t s h a 1 _ b a s e 6 4 = " j H j 0 N q d Q + 8 7 f b Q S 5 p r P S 5 R H L n O U = " > A A A C B n i c Z V D L S g M x F M 3 4 r P V V d e k m W A Q X p U x F U H d F N y 5 b c G y h H U o m z b S h y W R I 7 g h l 6 A e I W / 0 O V + L W 3 / A z / A M z 0 1 n Y 9 k K 4 J + e e G 0 5 O E A t u w H V / n L X 1 j c 2 t 7 d J O e X d v / + C w c n T 8 Z F S i K f O o E k p 3 A 2 K Y 4 B H z g I N g 3 V g z I g P B O s H k P p t 3 n p k 2 X E W P M I 2 Z L 8 k o 4 i G n B C z V H g 0 q V b f u 5 o V X Q a M A V V R U a 1 D 5 7 Q 8 V T S S L g A p i T K / h x u C n R A O n g s 3 K / c S w m N A J G b G e h R G R z P h p b n S G z y 0 z x K H S 9 k S A c / b / R k q k k Q T G V p k 1 s z D L G F B K m J p V w V h m L X s m v 5 u p D G q B r G U i b U K z Z A T C G z / l U Z w A i + j c R 5 g I D A p n o e A h 1 4 y C m F p A q O b 2 K 5 i O i S Y U b H R l m 1 F j O Z F V 4 F 3 W b + t u + 6 r a v C v C K q F T d I Y u U A N d o y Z 6 Q C 3 k I Y o Y e k V v 6 N 1 5 c T 6 c T + d r L l 1 z i p 0 T t F D O 9 x + Y v 5 n 5 < / l a t e x i t >

< l a t e x i t s h a 1 _ b a s e 6 4 = " 3 n d 3 u m u W a C + M i 7 m / 2 B 1 m K w n u d N 6 j M x o B / I u g l 9 J V R k O y + I F r 6 u G U Y 0 = " > A A A C C 3 i c Z V D L T g I x F O L 3 2 g D C L 8 c Q X 6 t L N R G L i g p D B j b o w E t 2 4 x M Q B E i C k U z p Q a a e T 9 o 4 J m I f S A R L + x g q X 1 G u r / 3 Q + V H X K x u q D 3 V x + E g 4 5 y / f h 4 H R 9 / g Y B G F V w g I I 3 3 a K e S 7 5 p p u + e e c e 2 2 p 5 y w e e I P B + T J f M o g e + d v 9 + O W Z L m m l l 5 5 Z Z X X U V t t u f 5 y 7 6 b v 2 b N G z 5 a t 3 b s 1 n T v 2 7 N t 2 W r M a S x j k R r l Q P j l 0 V i C u 6 V U R Z N A H D 2 B v M K 8 W Y U j g 5 9 y Y F M K B w p R M a 1 0 I Z U k C I 5 F / g T 9 h G j F + y 8 n T 8 u / e o N D B 0 6 4 o a 0 r k 6 + A E 6 d H j M C W L t a L E 0 b o g t f 4 s y o C A l R B D S I 9 a V q a t 0 K l v P Q g d u P 4 I W F i r m + 7 R Z N z y c l p 0 Z E B 5 J T U 9 p Q K u F P 5 z + K 2 X R T f w R C 6 Q 7 L d W d T 6 / x 2 Z l + 1 2 F z U 1 0 J k Y i k 5 F A D K I Y B k x y r z 3 7 a M q X 4 Y E H X h Q G S N r n I A A o R 2 T z s r d U 2 S O w 9 2 Y J 0 C w B m 6 S T I H + m 7 h R Z l G Y R I D g L F T 1 H Z k 0 3 k c c j z t t 0 2 j j y g 3 z T T d c U w G K l p 7 z I A n n Q B n y 7 d P j + / N G E w Z k F W G W E m u A x Y b G Z G d X r a M 9 z M C w x s l Z U U C B l K h r i k l t a G F B f Q Z O m R 2 t 9 v J S n Z J 7 3 K Q 5 x H l w U i A / x 5 k o M p R S V K p l E A 5 7 o 0 5 n I B x E i I e z t j U o c J 8 C i 6 h M N Y k a E E Z g 3 m 6 P C o B K P Y h O o y n C L d T N Z B N m w n u x 1 x 4 Q y l i w G E F c p G A Y q K O K b Y 2 + K Y y p 7 D t B E l 0 h 0 h o A 2 i v Y x / y 2 N 2 q R P U y m f U C 9 K k L E w X D g 8 n p 5 n f Z O e y 8 e W + 6 s 9 W Q q u 1 Y d J o p U Z n e l E 0 A g D A u 7 E R Y M y a n q A g K U F 1 b R i F B N K 6 v i h G A P 4 E R T 6 Q e P 4 X Q p V C e z n + S j f F n e z r X T l e 3 r P H q f b r S Y j y P L O N 3 W s d w O 8 d z f 5 T X R z T + 1 A u l c 1 f g e n o g e Y f = e < g / = l = a < t / e l x a i t e > x i t >·

··

e< l a t e x i t s h a 1 _ b a s e 6 4 = " M n r 3 q c i j P h J T q J b w E 9 e t B l m L K d U u 3 5 L m a 9 A e 4 J L 8 y N Q N y i l B n I d q Y k = " > A A A C D n i c Z V D L T g I x F L O 2 3 D g L C 8 / Q G X F a m u r L h G x z M U 5 R G i Y 4 u o C K B Q k w c Y K 2 P 6 u I C 7 G p 5 x c C Q Y u k I I B g i h a 2 k S U g z c r a Q 2 0 u E m 4 k n v 7 W R N 0 C T J M n u y E E j c j a F v f 9 4 B c N q e 4 u 9 j R B f s c X + / A o / I + b g / R 8 1 E g / I s X D C O T w 5 E p L 6 h e J e c 2 0 5 / z P e P v b x c I 5 c P I X O 7 e E 9 m + Q 1 b k H V + l b b Z X y 1 K j 6 e t y r m 6 7 x m v t 5 7 z Z c 3 L d W v 9 f s z 7 + u w X b n 1 H R / s 4 a E a 7 s L T W p B V H Q q u E u s k m T l w a w v Q t P Y W U R 8 0 5 5 C C 6 t g a I M D N T C t P u S R F o 6 l z j h 4 D n 2 L / b S 8 e 0 e X O U J 6 a b c z N 1 V Q e p I Z e k j M i b H 2 U E k c 6 U Y U c / 8 g 4 Q J c S g g C p R R j 7 A a Y v 6 k r z 7 Y r u i M 4 t R z O 3 e X q z B Q K 3 5 m a T c c i p d O B V e v Q Y y K q l M a 1 P C a q z H y T M V A / V 2 L G v 5 9 3 s 3 9 Z E P r 0 / V n i b y 7 E k K s k S g C x h r k T A K 4 X 1 o r S p d T h d G S j L k w V E L q B y x A r E h U 0 4 b n F h h W E 6 6 s J a H Y 3 T W J s C j A A 9 k o k x p 8 l A O Q M C j 6 E q 8 9 d J k D 8 M t 8 0 s 3 U M 8 D N p 0 e 7 0 c J D 0 q Z c 2 w w J / w z c c 7 S Y I / o x 0 s k J O F L l D p K g t G J B m p 5 l W 2 c v q T g c U L s G I V U A r S Q q o 7 H L M R m g 3 V p D M k 5 b O b 7 0 G m U e m y / u 6 x M 8 t I i v K + t 6 I K m c M i A p t Q G O M 9 L I j I s R J C D C y 6 M 9 Y h m I Q V h R n D f D o Q I k Y U u x G 9 i B c z t G N 2 w Q 3 d B h 7 q X O j 3 K W I e Y K W E U u C B o j 5 A v z Y B r R L z h H 0 z Q F T J S k j O a s C M N A O E M T y Y o Z u p J R L d I T P G 6 R e Z e m C q e 5 V N 6 V 4 s q V T t c N w k r V S U w f c T w y w q m N c j Q d R I k L u O o U A B K V 3 d U o I B U q 6 U Q J Q D 3 w k A I q o / I w E 5 e j k w L 7 P 7 6 8 M 6 V H 6 8 t z N m 6 V s Z d p + z t Z j z K i s H 1 M Z l s f 5 P 1 1 D B N 2 F c f Z W n 1 x 0 / = s < Y / q l D a Z t < e / x l i a t > e x i t > i e i,r < l a t e x i t s h a 1 _ b a s e 6 4 = " I L R 7 g 1 A / j B s o H t Y q P 1 J E u i v g H Q 4 = " > A A A C E n i c Z V D L S g M x F M 3 4 r P V V d e k m W A Q X p U x F U H d F N y 4 r O L b Q D i W T Z t r Q Z D I m d 4 Q y z G e I W / 0 O V + L W H / A z / A M z 0 y 5 s e y H c k 3 P P D S c n i A U 3 4 L o / z s r q 2 v r G Z m m r v L 2 z u 7 d f O T h 8 N C r R l H l U C a U 7 A T F M 8 I h 5 w E G w T q w Z k Y F g 7 W B 8 m 8 / b z 0 w b r q I H m M T M l 2 Q Y 8 Z B T A p b y e 4 F M W d Z P e Q 3 r r F + p u n W 3 K L w M G j N Q R b N q 9 S u / v Y G i i W Q R U E G M 6 T b c G P y U a O B U s K z c S w y L C R 2 T I e t a G B H J j J 8 W p j N 8 a p k B D p W 2 J w J c s P 8 3 U i K N J D C y y r y Z u V n O g F L C 1 K w K R j J v + T P F 3 U x k U A t k L R d p E 5 o F I x B e + S m P 4 g R Y R K c + w k R g U D g P C A + 4 Z h T E x A J C N b d f w X R E N K F g Y y z b j B q L i S w D 7 7 x + X X f v L 6 r N m 1 l Y J X S M T t A Z a q B L 1 E R 3 q I U 8 R N E T e k V v 6 N 1 5 c T 6 c T + d r K l 1 x Z j t H a K 6 c 7 z 8 A u J 8 Q < / l a t e x i t >
+< l a t e x i t s h a 1 _ b a s e 6 4 = " 9 n q 3 0 M F P F B g z W o i P 8 j a 7 Q Q U / I d Q = " > A A A C E 3 i c Z V D N S s N A E N 7 U v 1 r / q h 6 9 B I s g W E o q g n o r e v F Y w d h C G 8 t m s 2 m X 7 m b D 7 k Q o o a 8 h X v U 5 P I l X H 8 D H 8 A 3 c p D n Y d m C Z b 7 / 5 Z p j 5 / J g z D Y 7 z Y 5 V W V t f W N 8 q b l a 3 t n d 2 9 6 v 7 B o 5 a J I t Q l k k v V 9 b G m n E X U B Q a c d m N F s f A 5 7 f j j 2 6 z e e a Z K M x k 9 w C S m n s D D i I W M Y D D U U 9 + X P N A T Y V J 6 N h 1 U a 0 7 D y c N e B s 0 C 1 F A R 7 U H 1 t x 9 I k g g a A e F Y 6 1 7 T i c F L s Q J G O J 1 W + o m m M S Z j P K Q 9 A y M s q P b S f O u p f W K Y w A 6 l M i 8 C O 2 f / d 6 R Y a I F h Z J R Z 0 n O 1 j A E p u a 4 b F Y x E l r I x + T 8 7 q O 6 L e i Z S O t Q L i 0 B 4 5 a U s i h O g E Z n t E S b c B m l n D t k B U 5 Q A n x i A i W L m F J u M s M I E j I 8 V 4 1 F z 0 Z F l 4 J 4 3 r h v O / U W t d V O Y V U Z H 6 B i d o i a 6 R C 1 0 h 9 r I R Q Q p 9 I r e 0 L v 1 Y n 1 Y n 9 b X T F q y i p 5 D N B f W 9 x 9 L X Z / K < / l a t e x i t >
h(i,2r) < l a t e x i t s h a 1 _ b a s e 6 4 = " P L D + o / n i m r k o i f A V q Z r G l S o / h i w = " > A A A C G n i c Z V D L S g M x F M 3 4 r P U 1 2 q W b Y B E q l D I t g r o r u n F Z w d p C W 0 s m z b S h y W R I 7 g h l m E 8 R t / o d r s S t G z / D P z D T d m H b C + G e n H t u O D l + J L g B z / t x 1 t Y 3 N r e 2 c z v 5 3 b 3 9 g 0 P 3 6 P j R q F h T 1 q R K K N 3 2 i W G C h 6 w J H A R r R 5 o R 6 Q v W 8 s e 3 2 b z 1 z L T h K n y A S c R 6 k g x D H n B K w F J 9 t 9 D 1 Z T J K + w k v Y 5 0 + J a X a e d p 3 i 1 7 F m x Z e B d U 5 K K J 5 N f r u b 3 e g a C x Z C F Q Q Y z p V L 4 J e Q j R w K l i a 7 8 a G R Y S O y Z B 1 L A y J Z K a X T M 2 n + M w y A x w o b U 8 I e M r + 3 0 i I N J L A y C q z Z h Z m G Q N K C V O 2 K h j J r G X P T O 9 m I v 2 y L 8 u Z S J v A L B m B 4 K q X 8 D C K g Y V 0 5 i O I B Q a F s 6 D w g G t G Q U w s I F R z + x V M R 0 Q T C j b O v M 2 o u p z I K m j W K t c V 7 / 6 i W L + Z h 5 V D J + g U l V A V X a I 6 u k M N 1 E Q U T d A r e k P v z o v z 4 X w 6 X z P p m j P f K a C F c r 7 / A I f O o V k = < / l a t e x i t >
=< l a t e x i t s h a 1 _ b a s e 6 4 = " t J t 7 7 1 s 0 H i f a Q w o j C l O D 2 t k 3 4 K Y c f c w h N 2 U y E C E + i l 7 H N T c g 9 m M c Q U = " > A A A C E 3 i c Z V D B N S g w M x E J M 6 3 t W f r 7 X p + V T 1 6 W S y C h 1 K 2 X t R D s e j F Y w V r C 2 + 0 1 t a 2 s T t T l b s h G i 5 a p b s J l Z m k R V W y K l E L v w / K h c n S h r V P / o 4 c Y n E T z x 5 A F D n 6 + G b / 2 8 C B 2 s 9 2 W 4 D N b t g B T 8 B K f 8 v v H l k m z m z P D n w 8 / S 5 H k C y D n 6 v / f 5 t Y Z u J a a X W l V l 1 d b W X 1 s / u L u q 5 9 j s c b 2 m t 1 7 v Z V 3 P 8 Y 7 3 t b 6 v d T U M b l G m E r N E o 6 j V k U L r r V p 8 E r 8 C M l E n D E 1 W k 0 d A O A Q 0 r 5 W b j s D a Q J j Y 0 + h J e w s 2 4 / Q c + F v V 0 V n m r 8 j + g U W K n W D Z V j X G i 5 L h o G 4 F h N 1 P J 4 O F m 7 H E P Q O k C Y U w o G K O X q u + 2 4 7 0 4 s S e P 6 T K O E S w N K i a W 2 V O c u T o d W f i 8 W E 3 r b e H J 4 N S x y F C U y P h 4 Q D U h L Y z v 7 P t X a O v U z R 6 A b G d r e d 7 / h E d + 9 7 O p I 2 E g k s i W a Y A h S U E E Y G 6 N 3 a b Z F S T / c C G T L k 8 I U 0 K c G i O r F Y 0 O Z N H e c O S D T Y W s N I M H B Z r I h + H a 2 1 w k Z Y G E W s F l D M t J p 5 e l O s t P R X 8 a 6 P R L Y N Q N I z n A l 6 M X q t 8 C 9 J G w d x s + P 7 8 7 j E x i U K I N L J D D H i 2 w j y z j J S K Z e m q V W r U K M o S F M L l C 1 F y K a 0 i K g B L z 7 J K N U 6 j Z R j n J / P s z 4 2 N o K 6 v M i t h i l K I t q I V m D M P H b O c L I Y h H G D d W e S y X q g I Y 4 x A c R h q C R O y t R 0 5 j h i w I h W 2 L Q y T k u 0 a d Q c E n z t B c F M C 4 f p C i h Z A A Z G g h o m Z t k t 5 T x X S D B o 8 g r m T l M C D 0 4 P a u B a u s P R K + v V O 5 O R L x I Z L B G / S a f R m 0 8 X 7 v N J 6 u 4 v x E d L o 1 l E m q k a Q R e h H Q a M B 4 D h d G I M w o q w 6 y B l T U V 4 0 R D p W q U o A w c Y K i G S p K 7 E h n B 9 V I 6 x d e J r + E f N r e z X c q + 3 p P t i K b M S 8 n 9 D e X z t D 2 z U P c h z f Y P X 0 3 C 9 D A O S C o h H p o Q x = k = < / l a t e x i t >
h(i,1r) < l a t e x i t s h a 1 _ b a s e 6 4 = " M 6 A X F m C U R F 5 o a g a a j 1 n 7 F J b N e k I = " > A A A C G n i c Z V D L S g M x F M 3 U V 6 2 v a p d u g k W o U M q M C O q u 6 M Z l B W s L b S 2 Z N N O G J p M h u S O U Y T 5 F 3 O p 3 u B K 3 b v w M / 8 B M 2 4 V t L 4 R 7 c u 6 5 4 e T 4 k e A G X P f H y a 2 t b 2 x u 5 b c L O 7 t 7 + w f F w 6 N H o 2 J N W Z M q o X T b J 4 Y J H r I m c B C s H W l G p C 9 Y y x / f Z v P W M 9 O G q / A B J h H r S T I M e c A p A U v 1 i 6 W u L 5 N R 2 k 9 4 F e v 0 K a l 4 Z 2 m / W H Z r 7 r T w K v D m o I z m 1 e g X f 7 s D R W P J Q q C C G N P x 3 A h 6 C d H A q W B p o R s b F h E 6 J k P W s T A k k p l e M j W f 4 l P L D H C g t D 0 h 4 C n 7 f y M h 0 k g C I 6 v M m l m Y Z Q w o J U z V q m A k s 5 Y 9 M 7 2 b i f S r v q x m I m 0 C s 2 Q E g q t e w s M o B h b S m Y 8 g F h g U z o L C A 6 4 Z B T G x g F D N 7 V c w H R F N K N g 4 C z Y j b z m R V d A 8 r 1 3 X 3 P u L c v 1 m H l Y e H a M T V E E e u k R 1 d I c a q I k o m q B X 9 I b e n R f n w / l 0 v m b S n D P f K a G F c r 7 / A I Y q o V g = < / l a t e x i t >
+< l a t e x i t s h a 1 _ b a s e 6 4 = " 9 n q 3 0 M F P F B g z W o i P 8 j a 7 Q Q U / I d Q = " > A A A C E 3 i c Z V D N S s N A E N 7 U v 1 r / q h 6 9 B I s g W E o q g n o r e v F Y w d h C G 8 t m s 2 m X 7 m b D 7 k Q o o a 8 h X v U 5 P I l X H 8 D H 8 A 3 c p D n Y d m C Z b 7 / 5 Z p j 5 / J g z D Y 7 z Y 5 V W V t f W N 8 q b l a 3 t n d 2 9 6 v 7 B o 5 a J I t Q l k k v V 9 b G m n E X U B Q a c d m N F s f A 5 7 f j j 2 6 z e e a Z K M x k 9 w C S m n s D D i I W M Y D D U U 9 + X P N A T Y V J 6 N h 1 U a 0 7 D y c N e B s 0 C 1 F A R 7 U H 1 t x 9 I k g g a A e F Y 6 1 7 T i c F L s Q J G O J 1 W + o m m M S Z j P K Q 9 A y M s q P b S f O u p f W K Y w A 6 l M i 8 C O 2 f / d 6 R Y a I F h Z J R Z 0 n O 1 j A E p u a 4 b F Y x E l r I x + T 8 7 q O 6 L e i Z S O t Q L i 0 B 4 5 a U s i h O g E Z n t E S b c B m l n D t k B U 5 Q A n x i A i W L m F J u M s M I E j I 8 V 4 1 F z 0 Z F l 4 J 4 3 r h v O / U W t d V O Y V U Z H 6 B i d o i a 6 R C 1 0 h 9 r I R Q Q p 9 I r e 0 L v 1 Y n 1 Y n 9 b X T F q y i p 5 D N B f W 9 x 9 L X Z / K < / l a t e x i t >

⇥< l a t e x i t s h a 1 _ b a s e 6 4 = " Z G n A d / H 1 D h G t I O B 7 u L R I m L t u j y m Q r p b 3 y + f W f Y Q 2 U 6 a 8 N R E q F p X k A = " > A A A C G n i c Z V B N T w I x E J O 3 1 i E F / + A L X 5 y e t h H l L I R z m H L x i Q g M Z j D i F R i b 3 0 o R j v e X v j G E I x i B S U t S E I N K g R t b X u W t h D o Q t b 5 j t f 2 t 1 r o H R G s z + 4 C Y E d e 4 j M F f 7 9 1 H 5 Z I 6 / M w V Z y L / x + 6 D 8 P W + f B 4 X C e 7 A S g 7 M c E B k C z Y r p 2 J / n e X t N D 2 M / v a i m A e U e 3 F 6 n H G k l / w z n t G r 8 6 r x t u 7 b K W 6 d t i r 5 6 f R 2 L N x n Q d 3 2 t z 7 8 Z o 3 H d h k 4 t 9 7 G + J 7 V d o a y R n o y p q Q h l N 0 K g t u g V B d g v m D e m M n R I 8 W 5 U C B h c Y K c 9 N a o M O y F E c C X w C Z 4 j 7 C T 6 l y j e S r / N T J e 6 u Y u N e V K 9 s E 1 9 k j c m A P N W x l S W H Q s Q C 8 D Z w B P T m g M p 4 b L q B F U U P u 1 d S Q u I e m t + J G P U t u C b x 0 M g C 5 n y p y A c h y N k U V T y / x q 7 l V i W l 8 N a z 7 s i r q C o X z Q U X G 0 5 G n K o + 1 f C H + w H G j g 3 0 9 S v v D + b d 7 v J q d K + J u p g J N F J S I A k U E x D p I l B 3 x z r Y 3 u a y k m 7 R I C f O Q n S g r k I 0 A K R n T c i S f w F m b N q A R R p G i b M C k 2 Y h D R 2 G n x H v w 3 A T C T b 6 X f 3 A p T J 9 N 9 v Q z y E f P T j d L U M 2 w p P 4 a I l 3 M S i n c 7 A v O y 2 M P l / 0 d k y i R C Y Q a 6 I v F M h k Z l J m R o p Z 0 Q n w O q 1 J l U A z E F p q u n a A 4 o a s F 5 Y Q x 9 E M m 7 t 1 J n W s 1 n U u C 6 W V c d l U E T 2 1 o V R S m k a t R K A 8 M X L B 7 g s H p / j r + J I e E w W I U I R y n A c B 4 m S Q J 6 c h F x G 9 5 x m G V 6 F S u d n G 2 m t U G P U m Y K w I t E I e F G R w z A u J 4 o p q L Z h V 0 W Q w T y i w t g b o O T g M v H W Y o W t j u U z f I 1 K R v U D e P W q g 1 X d t V S 7 O 8 6 5 8 6 5 d 1 Q 8 2 a z r y C M z E S x N n P M D A p Z A 1 h O + A g C Y 6 1 n d A E L p D a f q C A B r w 1 h E h Q e u 4 I Q i 3 h e G n z R + f g n F w v / V l p 0 P v 1 m r b v S 1 N Y W X f 1 e O U p 4 T K l F r c 1 L l 7 N / G A c O 2 n F O 9 p / C Q 4 E = A < L / q l Z a O t < e / x l i a t > e x i t >

xi,r < l a t e x i t s h a 1 _ b a s e 6 4 = " n y k e A u I u y i v a i + i p 1 2 G c 3 T W L d k 8 = " > A A A C E n i c Z V D L T g I x F O 3 g C / G F u n T T S E x c T M h g T N Q d 0 Y 1 L T E R I Y E I 6 p Q M N 7 X R s 7 x j J Z D 7 D u N X v c G X c + g N + h n 9 g B 1 g I 3 K S 5 p + e e 2 5 y e I B b c g O f 9 O I W V 1 b X 1 j e J m a W t 7 Z 3 e v v H / w Y F S i K W t S J Z R u B 8 Q w w S P W B A 6 C t W P N i A w E a w W j m 3 z e e m L a c B X d w z h m v i S D i I e c E r C U 3 w 1 k + p z 1 U u 5 i n f X K F a / q T Q o v g 9 o M V N C s G r 3 y b 7 e v a C J Z B F Q Q Y z o 1 L w Y / J R o 4 F S w r d R P D Y k J H Z M A 6 F k Z E M u O n E 9 M Z P r F M H 4 d K 2 x M B n r D / N 1 I i j S Q w t M q 8 m b l Z z o B S w r h W B U O Z t / y Z y d 2 M Z e A G 0 s 1 F 2 o R m w Q i E l 3 7 K o z g B F t G p j z A R G B T O A 8 J 9 r h k F M b a A U M 3 t V z A d E k 0 o 2 B h L N q P a Y i L L o H l W v a p 6 d + e V + v U s r C I 6 Q s f o F N X Q B a q j W 9 R A T U T R I 3 p F b + j d e X E + n E / n a y o t O L O d Q z R X z v c f I F a f I w = = < / l a t e x i t > xi < l a t e x i t s h a 1 _ b a s e 6 4 = " c q A l B O D u S c k O Q f a / w S l z 0 J c S 7 e u J B x 9 w u E 7 j + + x Z l u y l f g G K 6 r Y o A = " > A A A C D X i c Z V D L T g I x F L O 2 3 D g L C 8 / Q G X F a m u r L h G x z M U 5 R G i Y 4 u o C K B Q k w c Y K 2 P 6 u I C 7 G p 5 x c C Q Y u k I I C R U D x s I l p A 3 4 S 0 g t o N Z N 1 J O e 2 8 j d t I G J M v u y E D f c j a F v f 9 4 B c b q e 4 u 9 j R C v s c T + v A 8 / G + N g / R + 1 A g f I 2 X B C l T Y 5 C p N 6 y e k e u 2 a 5 f z n e n v t x u I c c H I j O / e i 9 T + I 1 P k j V f l F b u X 5 1 l j d e W y 1 m 9 7 Y m 3 t 8 7 Z Z m 3 F d r v e f 2 z d + 3 w r b 7 1 h R / s c a K a d s l T r p A V h Q 1 u i u e k R T S w t w X Q 2 P s W K R W 0 c 5 h C d t Y a E M B N p C + P 1 S I F U 6 S z x h 8 D T 2 l / v S + e 6 e D O q R d a t c + N 6 V p e 0 I k e y j G i t H z U C k O 6 q Y C c f 8 w 4 I J G S Q g B p I Z x p g t M X 1 y e Z 7 P 6 4 I y n 7 m v Y 5 9 g F t i e v y W Z H u I U q u T w l z b K 2 M 1 q C j o O H Q N K V l + 2 H 1 g P C x g h 2 r s / 3 X / P t R n q u / K 4 x 2 p + K 1 F L S E A g U s x a p A l u X F 2 Y I 6 u 0 w 7 k V R i C c O B n L g s o A 1 J z G 7 O d J i 0 w U i u N r A G h m 6 E b S O Y W j h P S K G A R d z A H 0 S M S s i q d P + a x S e z 2 O q / Z E n P h j s V o M b 3 U w + 6 I k 7 M o i T c 9 E v O 5 2 E P Q / a b S y T R B Y g a V I W F m h z a c J z R N p U 0 g 3 a O V z E l q A Z E o p V u T S i 4 Q b a F U Q u x f F m 2 d t z J N n S s P r p s F e X C x 7 Z / T s k i T 3 a I B q W U T j C r C Q w C W 0 U Y n g 4 u W P E Q U S I F w k v Y p x 1 0 E J c B Q M C f x Q e Q V x m t 2 0 b H j a 9 a r T h Z m 2 F n M y X l I K A g k I I 8 1 N t w 1 E 9 Q x 6 8 Y x B W o b Q D t L E H m C m B L E M y Z C l B R Z c N T R W d Q T b G 1 R 8 Z 9 e J C V e y V a 6 v 4 Z q r T K t 5 N h k W V l U k f 4 T h y h q M N 4 j g d z I J L c O Q U A B V u d o Q B p q 1 6 o Q C Q D 3 g k B I V o 7 I h 4 z e X k l L 2 P 3 6 p M 0 V P 6 5 t 3 N M 6 q s z d T + i t z j n K U s O 1 Y Z K s + 5 f 1 r D D N 0 F 2 f k W n 1 x w / = S < 7 / a l D a Y t < e / x l i a t > e x i t >

Figure 3: Depiction of the architecture of the inverse projection fφ−1 that composes multiple volume-preserving coupling layers, with which we parameterize our model. On the right, we schematically depict how the inverse projection transforms the observed word embedding xi to a point ei in a new embedding space.
3.2 Invertible Volume-Preserving Neural Net
For the projection we can use an arbitrary invertible function, and given the representational power of neural networks they seem a natural choice. However, calculating the inverse and Jacobian of an arbitrary neural network can be difﬁcult, as it requires that all component functions be invertible and also requires storage of large Jacobian matrices, which is memory intensive. To address this issue, several recent papers propose specially designed invertible networks that are easily trainable yet still powerful (Dinh et al., 2014, 2016; Jacobsen et al., 2018). Inspired by these works, we use the invertible transformation proposed by Dinh et al. (2014), which consists of a series of “coupling layers”. This architecture is specially designed to guarantee a unit Jacobian determinant (and thus the invertibility property).
From Eq. (8) we know that only fφ−1 is required for accomplishing learning and inference; we never need to explicitly construct fφ. Thus, we directly deﬁne the architecture of fφ−1. As shown in Figure 3, the nonlinear transformation from the observed embedding xi to h(i1) represents the ﬁrst coupling layer. The input in this layer is partitioned into left and right halves of dimensions, xi,l and xi,r, respectively. A single coupling layer is deﬁned as:

h(i,1l) = xi,l,

h(i,1r) = xi,r + g(xi,l), (9)

where g : Rdx/2 → Rdx/2 is the coupling function and can be any nonlinear form. This transformation satisﬁes dim(h(1)) = dim(x), and Dinh
et al. (2014) show that its Jacobian matrix is tri-

angular with all ones on the main diagonal. Thus the Jacobian determinant is always equal to one (i.e. volume-preserving) and the invertibility condition is naturally satisﬁed.
To be sufﬁciently expressive, we compose multiple coupling layers as suggested in Dinh et al. (2014). Speciﬁcally, we exchange the role of left and right half vectors at each layer as shown in Figure 3. For instance, from xi to h(i1) the left subset xi,l is unchanged, while from h(i1) to h(i2) the right subset h(i,1r) remains the same. Also note that composing multiple coupling layers does not change the volume-preserving and invertibility properties. Such a sequence of invertible transformations from the data space x to e is also called normalizing ﬂow (Rezende and Mohamed, 2015).
4 Experiments
In this section, we ﬁrst describe our datasets and experimental setup. We then instantiate our approach with Markov and DMV-structured syntax models, and report results on POS tagging and dependency grammar induction respectively. Lastly, we analyze the learned latent embeddings.
4.1 Data
For both POS tagging and dependency parsing, we run experiments on the Wall Street Journal (WSJ) portion of the Penn Treebank.3 To create the observed data embeddings, we train skip-gram word embeddings (Mikolov et al., 2013) that are found to capture syntactic properties well when trained with small context window (Bansal et al., 2014; Lin et al., 2015). Following Lin et al. (2015), the dimensionality dx is set to 100, and the training context window size is set to 1 to encode more syntactic information. The skip-gram embeddings are trained on the one billion word language modeling benchmark dataset (Chelba et al., 2013) in addition to the WSJ corpus.
4.2 General Experimental Setup
For the neural projector, we employ rectiﬁed networks as coupling function g following Dinh et al. (2014). We use a rectiﬁed network with an input layer, one hidden layer, and linear output units, the number of hidden units is set to the same as the number of input units. The number of coupling layers are varied as 4, 8, 16 for both tasks.
3Preprocessing is different for the two tasks, we describe the details in the following subsections.

We optimize marginal data likelihood directly using Adam (Kingma and Ba, 2014). For both tasks in the fully unsupervised setting, we do not tune the hyper-parameters using supervised data.
4.3 Unsupervised POS tagging
For unsupervised POS tagging, we use a Markovstructured syntax model in our approach, which is a popular structure for unsupervised tagging tasks (Lin et al., 2015; Tran et al., 2016).
Setup. Following existing literature, we train and test on the entire WSJ corpus (49208 sentences, 1M tokens). We use 45 tag clusters, the number of POS tags that appear in WSJ corpus. We train the discrete HMM and the Gaussian HMM (Lin et al., 2015) as baselines. For the Gaussian HMM, mean vectors of Gaussian emissions are initialized with the empirical mean of all word vectors with an additive noise. We assume diagonal covariance matrix for p(ei|zi) and initialize it with the empirical variance of the word vectors. Following Lin et al. (2015), the covariance matrix is ﬁxed during training. The multinomial probabilities are initialized as θkv ∝ exp(ukv), where ukv ∼ U [0, 1]. For our approach, we initialize the syntax model and Gaussian parameters with the pre-trained Gaussian HMM. The weights of layers in the rectiﬁed network are initialized from a uniform distribution with mean zero and a standard deviation of 1/nin, where nin is the input dimension.4 We evaluate the performance of POS tagging with both Many-to-One (M-1) accuracy (Johnson, 2007) and V-Measure (VM) (Rosenberg and Hirschberg, 2007). Given a model we found that the tagging performance is well-correlated with the training data likelihood, thus we use training data likelihood as a unsupervised criterion to select the trained model over 10 random restarts after training 50 epochs. We repeat this process 5 times and report the mean and standard deviation of performance.
Results. We compare our approach with basic HMM, Gaussian HMM, and several stateof-the-art systems, including sophisticated HMM variants and clustering techniques with handengineered features. The results are presented in Table 1. Through the introduced latent embeddings and additional neural projection, our approach improves over the Gaussian HMM by 5.4 points in M-1 and 5.6 points in VM. Neural HMM
4This is the default parameter initialization in PyTorch.

System

M-1

w/o hand-engineered features

Discrete HMM PYP-HMM (Blunsom and Cohn, 2011) NHMM (basic) (Tran et al., 2016) NHMM (+ Conv) (Tran et al., 2016) NHMM (+ Conv & LSTM) (Tran et al., 2016) Gaussian HMM (Lin et al., 2015)

62.7 77.5 59.8 74.1 79.1 75.4 (1.0)

Ours (4 layers) Ours (8 layers) Ours (16 layers)

79.5 (0.9) 80.8 (1.3) 73.2 (4.3)

w/ hand-engineered features

Feature HMM (Berg-Kirkpatrick et al., 2010) Brown (+ proto) (Christodoulopoulos et al., 2010) Cluster (word-based) (Yatbaz et al., 2012) Cluster (token-based) (Yatbaz et al., 2014)

75.5 76.1 80.2 79.5

VM
53.8 69.8 54.2 66.1 71.7 68.5 (0.5) 73.0 (0.7) 74.1 (0.7) 70.5 (2.1)
– 68.8 72.1 69.1

Table 1: Unsupervised POS tagging results on entire WSJ, compared with other baselines and state-of-the-art systems. Standard deviation is given in parentheses when available.

Others NNPS NNP NNS NN Others NNPS NNP NNS NN

0.33 0.00 0.02 0.00 0.65 0.13 0.48 0.02 0.00 0.37 0.01 0.00 0.35 0.06 0.59 0.01 0.18 0.50 0.19 0.12 0.01 0.00 0.00 0.00 0.98
NN NNS NNP NNPS Others

0.78 0.00 0.00 0.02 0.21 0.03 0.89 0.00 0.02 0.06 0.01 0.00 0.32 0.30 0.37 0.01 0.20 0.01 0.47 0.31 0.02 0.00 0.00 0.00 0.98
NN NNS NNP NNPS Others

(a) Gaussian HMM

(b) Our approach

Figure 4: Normalized Confusion matrix for POS tagging experiments, row label represents the gold tag.

(NHMM) (Tran et al., 2016) is a baseline that also learns word representation jointly. Both their basic model and extended Conv version does not outperform the Gaussian HMM. Their best model incorporates another LSTM to model long distance dependency and breaks the Markov assumption, yet our approach still achieves substantial improvement over it without considering more context information. Moreover, our method outperforms the best published result that beneﬁts from hand-engineered features (Yatbaz et al., 2012) by 2.0 points on VM.
Confusion Matrix. We found that most tagging errors happen in noun subcategories. Therefore, we do the one-to-one mapping between gold POS tags and induced clusters and plot the normalized confusion matrix of noun subcategories in Figure 4. The Gaussian HMM fails to identify “NN” and “NNS” correctly for most cases, and it often recognizes “NNPS” as “NNP”. In contrast, our approach corrects these errors well.

4.4 Unsupervised Dependency Parsing without gold POS tags
For the task of unsupervised dependency parse induction, we employ the Dependency Model with Valence (DMV) (Klein and Manning, 2004) as the syntax model in our approach. DMV is a generative model that deﬁnes a probability distribution over dependency parse trees and syntactic categories, generating tokens and dependencies in a head-outward fashion. While, traditionally, DMV is trained using gold POS tags as observed syntactic categories, in our approach, we treat each tag as a latent variable, as described in §2.3.
Most existing approaches to this task are not fully unsupervised since they rely on gold POS tags following the original experimental setup for DMV. This is partially because automatically parsing from words is difﬁcult even when using unsupervised syntactic categories (Spitkovsky et al., 2011a). However, inducing dependencies from words alone represents a more realistic experimental condition since gold POS tags are often unavailable in practice. Previous work that has trained from words alone often requires additional linguistic constraints (like sentence internal boundaries) (Spitkovsky et al., 2011a,b, 2012, 2013), acoustic cues (Pate and Goldwater, 2013), additional training data (Pate and Johnson, 2016), or annotated data from related languages (Cohen et al., 2011). Our approach is naturally designed to train on word embeddings directly, thus we attempt to induce dependencies without using gold POS tags or other extra linguistic information.
Setup. Like previous work we use sections 0221 of WSJ corpus as training data and evaluate on section 23, we remove punctuations and train the models on sentences of length 10, “headpercolation” rules (Collins, 1999) are applied to obtain gold dependencies for evaluation. We train basic DMV, extended DMV (E-DMV) (Headden III et al., 2009) and Gaussian DMV (which treats POS tag as unknown latent variables and generates observed word embeddings directly conditioned on them following Gaussian distribution) as baselines. Basic DMV and E-DMV are trained with Viterbi EM (Spitkovsky et al., 2010) on unsupervised POS tags induced from our Markov-structured model described in §4.3. Multinomial parameters of the syntax model in both Gaussian DMV and our model are initialized with the pre-trained DMV baseline. Other

System

10

w/o gold POS tags

DMV (Klein and Manning, 2004) E-DMV (Headden III et al., 2009) UR-A E-DMV (Tu and Honavar, 2012) CS∗ (Spitkovsky et al., 2013) Neural E-DMV (Jiang et al., 2016) CRFAE (Cai et al., 2017) Gaussian DMV

49.6 52.1 58.9 72.0∗ 55.3 37.2 55.4 (1.3)

Ours (4 layers) Ours (8 layers) Ours (16 layers)

58.4 (1.9) 60.2 (1.3) 54.1 (8.5)

all
35.8 38.2 46.1 64.4∗ 42.7 29.5 43.1 (1.2) 46.2 (2.3) 47.9 (1.2) 43.9 (5.7)

w/ gold POS tags (for reference only)

DMV (Klein and Manning, 2004) UR-A E-DMV (Tu and Honavar, 2012) MaxEnc (Le and Zuidema, 2015) Neural E-DMV (Jiang et al., 2016) CRFAE (Cai et al., 2017) L-NDMV (Big training data) (Han et al., 2017)

55.1 71.4 73.2 72.5 71.7 77.2

39.7 57.0 65.8 57.6 55.7 63.2

Table 2: Directed dependency accuracy on section 23 of WSJ, evaluating on sentences of length 10 and all lengths. Starred entries (∗) denote that the system beneﬁts from additional punctuation-based constraints. Standard deviation is given in parentheses when available.
parameters are initialized in the same way as in the POS tagging experiment. The directed dependency accuracy (DDA) is used for evaluation and we report accuracy on sentences of length 10 and all lengths. We train the parser until training data likelihood converges, and report the mean and standard deviation over 20 random restarts.
Comparison with other related work. Our model directly observes word embeddings and does not require gold POS tags during training. Thus, results from related work trained on gold tags are not directly comparable. However, to measure how these systems might perform without gold tags, we run three recent state-of-theart systems in our experimental setting: URA E-DMV (Tu and Honavar, 2012), Neural EDMV (Jiang et al., 2016), and CRF Autoencoder (CRFAE) (Cai et al., 2017).5 We use unsupervised POS tags (induced from our Markov-structured model) in place of gold tags.6 We also train basic DMV on gold tags and include several stateof-the-art results on gold tags as reference points.
Results. As shown in Table 2, our approach is able to improve over the Gaussian DMV by 4.8 points on length 10 and 4.8 points on all

5For the three systems, we use implementations from the original papers (via personal correspondence with the authors), and tune their hyperparameters on section 22 of WSJ.
6Using words directly is not practical because these systems often require a transition probability matrix between input symbols, which requires too much memory.

System

M-1

VM

Ours (4 layers)

78.2

71.2

Ours (8 layers)

72.5

69.7

Ours (16 layers)

67.2

69.2

Table 3: Unsupervised POS tagging results of our approach on WSJ, with random initialization of syntax model.
lengths, which suggests the additional latent embedding layer and neural projector are helpful. The proposed approach yields, to the best of our knowledge,7 state-of-the-art performance without gold POS annotation and without sentenceinternal boundary information. DMV, UR-A EDMV, Neural E-DMV, and CRFAE suffer a large decrease in performance when trained on unsupervised tags – an effect also seen in previous work (Spitkovsky et al., 2011a; Cohen et al., 2011). Since our approach induces latent POS tags jointly with dependency trees, it may be able to learn POS clusters that are more amenable to grammar induction than the unsupervised tags. We observe that CRFAE underperforms its goldtag counterpart substantially. This may largely be a result of the model’s reliance on prior linguistic rules that become unavailable when gold POS tag types are unknown. Many extensions to DMV can be considered orthogonal to our approach – they essentially focus on improving the syntax model. It is possible that incorporating these more sophisticated syntax models into our approach may lead to further improvements.

4.5 Sensitivity Analysis
Impact of Initialization. In the above experiments we initialize the structured syntax components with the pre-trained Gaussian or discrete baseline, which is shown as a useful technique to help train our deep models. We further study the results with fully random initialization. In the POS tagging experiment, we report the results in Table 3. While the performance with 4 layers is comparable to the pre-trained Gaussian initialization, deeper projections (8 or 16 layers) result in a dramatic drop in performance. This suggests that the structured syntax model with very deep projections is difﬁcult to train from scratch, and a simpler projection might be a good compromise in the random initialization setting.
Different from the Markov prior in POS tag-
7We tried to be as thorough as possible in evaluation by running top performing systems using our more difﬁcult training setup when this was feasible – but it was not possible to evaluate them all.

System

M-1

VM

Gaussian HMM

72.0

65.0

Ours (4 layers)

76.4

69.3

Ours (8 layers)

76.8

69.4

Ours (16 layers)

67.3

62.0

Table 4: Unsupervised POS tagging results on WSJ, with fastText vectors as the observed embeddings.

System

10

all

Gaussian DMV

53.6

41.3

Ours (4 layers)

56.9

43.9

Ours (8 layers)

57.1

42.3

Ours (16 layers)

52.9

39.5

Table 5: Directed dependency accuracy on section 23 of WSJ, with fastText vectors as the observed embeddings.
ging experiments, our parsing model seems to be quite sensitive to the initialization. For example, directed accuracy of our approach on sentences of length 10 is below 40.0 with random initialization. This is consistent with previous work that has noted the importance of careful initialization for DMV-based models such as the commonly used harmonic initializer (Klein and Manning, 2004). However, it is not straightforward to apply the harmonic initializer for DMV directly in our model without using some kind of pre-training since we do not observe gold POS.
Impact of Observed Embeddings. We investigate the effect of the choice of pre-trained embedding on performance while using our approach. To this end, we additionally include results using fastText embeddings (Bojanowski et al., 2017) – which, in contrast with skip-gram embeddings, include character-level information. We set the context windows size to 1 and the dimension size to 100 as in the skip-gram training, while keeping other parameters set to their defaults. These results are summarized in Table 4 and Table 5. While fastText embeddings lead to reduced performance with our model, our approach still yields an improvement over the Gaussian baseline with the new observed embeddings space.

4.6 Qualitative Analysis of Embeddings
We perform qualitative analysis to understand how the latent embeddings help induce syntactic structures. First we ﬁlter out low-frequency words and punctuations in WSJ, and visualize the rest words (10k) with t-SNE (Maaten and Hinton, 2008) under different embeddings. We assign each word with its most likely gold POS tags in WSJ and color them according to the gold POS tags.

Target come singing
cigars
newer
fanciest

Skip-gram go came follow coming sit dancing sing drumming dance dances cigarettes sodas champagne cigar rum ﬂashier fancier conventional low-end new-generation priciest up-scale loveliest fancier high-end

Markov Structure be go do give follow dancing drumming marching playing recording sodas bottles drinks pills cigarettes softer lighter thinner darker smoother liveliest priciest smartest best-run fastest-growing

Table 6: Target words and their 5 nearest neighbors, based on skip-gram embeddings and our learned latent embeddings with Markov-structured syntax model.

(subj) parents
smokers furriers
aides issuers folks

process dreams
error

(obj)
agenda plans

payments timetable

(subj)

actress

aide resident

owner attorney singer

Figure 5: Visualization (t-SNE) of learned latent embeddings with DMV-structured syntax model. Each node represents a word and is colored according to the most likely gold POS tag in the Penn Treebank (best seen in color).

For our Markov-structured model, we have displayed the embedding space in Figure 1(b), where the gold POS clusters are well-formed. Further, we present ﬁve example target words and their ﬁve nearest neighbors in terms of cosine similarity. As shown in Table 6, the skip-gram embedding captures both semantic and syntactic aspects to some degree, yet our embeddings are able to focus especially on the syntactic aspects of words, in an unsupervised fashion without using any extra morphological information.
In Figure 5 we depict the learned latent embeddings with the DMV-structured syntax model. Unlike the Markov structure, the DMV structure maps a large subset of singular and plural nouns to the same overlapping region. However, two clusters of singular and plural nouns are actually separated. We inspect the two clusters and the overlapping region in Figure 5, it turns out that the nouns in the separated clusters are words that can appear as subjects and, therefore, for which verb agreement is important to model. In contrast, the nouns

in the overlapping region are typically objects. This demonstrates that the latent embeddings are focusing on aspects of language that are speciﬁcally important for modeling dependency without ever having seen examples of dependency parses.
Some previous work has deliberately created embeddings to capture different notions of similarity (Levy and Goldberg, 2014; Cotterell and Schu¨tze, 2015), while they use extra morphology or dependency annotations to guide the embedding learning, our approach provides a potential alternative to create new embeddings that are guided by structured syntax model, only using unlabeled text corpora.
5 Related Work
Our approach is related to ﬂow-based generative models, which are ﬁrst described in NICE (Dinh et al., 2014) and have recently received more attention (Dinh et al., 2016; Jacobsen et al., 2018; Kingma and Dhariwal, 2018). This relevant work mostly adopts simple (e.g. Gaussian) and ﬁxed priors and does not attempt to learn interpretable latent structures. Another related generative model class is variational auto-encoders (VAEs) (Kingma and Welling, 2013) that optimize a lower bound on the marginal data likelihood, and can be extended to learn latent structures (Miao and Blunsom, 2016; Yin et al., 2018). Against the ﬂow-based models, VAEs remove the invertibility constraint but sacriﬁce the merits of exact inference and exact log likelihood computation, which potentially results in optimization challenges (Kingma et al., 2016). Our approach can also be viewed in connection with generative adversarial networks (GANs) (Goodfellow et al., 2014) that is a likelihood-free framework to learn implicit generative models. However, it is nontrivial for a gradient-based method like GANs to propagate gradients through discrete structures.
6 Conclusion
In this work, we deﬁne a novel generative approach to leverage continuous word representations for unsupervised learning of syntactic structure. Experiments on both POS induction and unsupervised dependency parsing tasks demonstrate the effectiveness of our proposed approach. Future work might explore more sophisticated invertible projections, or recurrent projections that jointly transform the entire input sequence.

References
Mohit Bansal, Kevin Gimpel, and Karen Livescu. 2014. Tailoring continuous word representations for dependency parsing. In Proceedings of ACL.
Taylor Berg-Kirkpatrick, Alexandre Bouchard-Coˆte´, John DeNero, and Dan Klein. 2010. Painless unsupervised learning with features. In Proceedings of HLT-NAACL, pages 582–590. Association for Computational Linguistics.
Phil Blunsom and Trevor Cohn. 2011. A hierarchical pitman-yor process hmm for unsupervised part of speech induction. In Proceedings of ACL.
Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. 2017. Enriching word vectors with subword information. Transactions of the Association of Computational Linguistics.
Jiong Cai, Yong Jiang, and Kewei Tu. 2017. Crf autoencoder for unsupervised dependency parsing. In Proceedings of EMNLP.
Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn, and Tony Robinson. 2013. One billion word benchmark for measuring progress in statistical language modeling. arXiv preprint arXiv:1312.3005.
Scott Saobing Chen and Ramesh A Gopinath. 2001. Gaussianization. In Advances in neural information processing systems.
Christos Christodoulopoulos, Sharon Goldwater, and Mark Steedman. 2010. Two decades of unsupervised pos induction: How far have we come? In Proceedings of EMNLP.
Shay B Cohen, Dipanjan Das, and Noah A Smith. 2011. Unsupervised structure prediction with nonparallel multilingual guidance. In Proceedings of EMNLP.
Shay B Cohen and Noah A Smith. 2009. Shared logistic normal distributions for soft parameter tying in unsupervised grammar induction. In Proceedings of HLT-NAACL.
Michael Collins. 1999. HEAD-DRIVEN STATISTICAL MODELS FOR NATURAL LANGUAGE PARSING. Ph.D. thesis, University of Pennsylvania.
Ryan Cotterell and Hinrich Schu¨tze. 2015. Morphological word-embeddings. In Proceedings of NAACL-HLT.
Laurent Dinh, David Krueger, and Yoshua Bengio. 2014. Nice: Non-linear independent components estimation. arXiv preprint arXiv:1410.8516.
Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. 2016. Density estimation using real nvp. arXiv preprint arXiv:1605.08803.

Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. Generative adversarial nets. In Proceedings of NIPS.
Wenjuan Han, Yong Jiang, and Kewei Tu. 2017. Dependency grammar induction with neural lexicalization and big training data. In Proceedings of EMNLP.
Luheng He, Kenton Lee, Mike Lewis, and Luke Zettlemoyer. 2017. Deep semantic role labeling: What works and whats next. In Proceedings of ACL.
William P Headden III, Mark Johnson, and David McClosky. 2009. Improving unsupervised dependency parsing with richer contexts and smoothing. In Proceedings of HLT-NAACL.
Aapo Hyva¨rinen, Juha Karhunen, and Erkki Oja. 2004. Independent component analysis, volume 46. John Wiley & Sons.
Jo¨rn-Henrik Jacobsen, Arnold Smeulders, and Edouard Oyallon. 2018. i-revnet: Deep invertible networks. arXiv preprint arXiv:1802.07088.
Yong Jiang, Wenjuan Han, and Kewei Tu. 2016. Unsupervised neural dependency parsing. In Proceedings of EMNLP.
Mark Johnson. 2007. Why doesnt em ﬁnd good hmm pos-taggers? In Proceedings of the EMNLPCoNLL.
Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980.
Diederik P Kingma and Prafulla Dhariwal. 2018. Glow: Generative ﬂow with invertible 1x1 convolutions. arXiv preprint arXiv:1807.03039.
Diederik P Kingma, Tim Salimans, Rafal Jozefowicz, Xi Chen, Ilya Sutskever, and Max Welling. 2016. Improved variational inference with inverse autoregressive ﬂow. In Advances in Neural Information Processing Systems, pages 4743–4751.
Diederik P Kingma and Max Welling. 2013. Autoencoding variational bayes. arXiv preprint arXiv:1312.6114.
Dan Klein and Christopher D Manning. 2004. Corpusbased induction of syntactic structure: Models of dependency and constituency. In Proceedings of ACL.
Phong Le and Willem Zuidema. 2015. Unsupervised dependency parsing: Let’s use supervised parsers. In Proceedings of NAACL-HLT.
Omer Levy and Yoav Goldberg. 2014. Dependencybased word embeddings. In Proceedings of ACL.
Chu-Cheng Lin, Waleed Ammar, Chris Dyer, and Lori Levin. 2015. Unsupervised pos induction with word embeddings. In Proceedings of the NAACL-HLT.

Laurens van der Maaten and Geoffrey Hinton. 2008. Visualizing data using t-sne. Journal of Machine Learning Research.
Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. 1993. Building a large annotated corpus of english: The penn treebank. Computational linguistics, 19(2):313–330.
Yishu Miao and Phil Blunsom. 2016. Language as a latent variable: Discrete generative models for sentence compression. In Proceedings of EMNLP.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efﬁcient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.
John K Pate and Sharon Goldwater. 2013. Unsupervised dependency parsing with acoustic cues. Transactions of the Association for Computational Linguistics.
John K Pate and Mark Johnson. 2016. Grammar induction from (lots of) words alone. In Proceedings of COLING.
Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word representations. In Proceedings of HLT-NAACL.
Danilo Jimenez Rezende and Shakir Mohamed. 2015. Variational inference with normalizing ﬂows. In Proceedings of ICML.
Andrew Rosenberg and Julia Hirschberg. 2007. Vmeasure: A conditional entropy-based external cluster evaluation measure. In Proceedings of EMNLPCoNLL.
Valentin I Spitkovsky, Hiyan Alshawi, Angel X Chang, and Daniel Jurafsky. 2011a. Unsupervised dependency parsing without gold part-of-speech tags. In Proceedings of EMNLP.
Valentin I Spitkovsky, Hiyan Alshawi, and Daniel Jurafsky. 2011b. Punctuation: Making a point in unsupervised dependency parsing. In Proceedings of CoNLL.
Valentin I Spitkovsky, Hiyan Alshawi, and Daniel Jurafsky. 2012. Capitalization cues improve dependency grammar induction. In Proceedings of NAACL-HLT Workshop on the Induction of Linguistic Structure.
Valentin I Spitkovsky, Hiyan Alshawi, and Daniel Jurafsky. 2013. Breaking out of local optima with count transforms and model recombination: A study in grammar induction. In Proceedings of EMNLP.
Valentin I Spitkovsky, Hiyan Alshawi, Daniel Jurafsky, and Christopher D Manning. 2010. Viterbi training improves unsupervised dependency parsing. In Proceedings of CoNLL.

Karl Stratos, Michael Collins, and Daniel Hsu. 2016. Unsupervised part-of-speech tagging with anchor hidden markov models. Transactions of the Association for Computational Linguistics.
Ke M Tran, Yonatan Bisk, Ashish Vaswani, Daniel Marcu, and Kevin Knight. 2016. Unsupervised neural hidden markov models. In Proceedings of the Workshop on Structured Prediction for NLP.
Kewei Tu and Vasant Honavar. 2012. Unambiguity regularization for unsupervised learning of probabilistic grammars. In Proceedings of EMNLPCoNLL.
Mehmet Ali Yatbaz, Enis Sert, and Deniz Yuret. 2012. Learning syntactic categories using paradigmatic representations of word context. In Proceedings of EMNLP-CoNLL.
Mehmet Ali Yatbaz, Enis Rıfat Sert, and Deniz Yuret. 2014. Unsupervised instance-based part of speech induction using probable substitutes. In Proceedings of COLING.
Pengcheng Yin, Chunting Zhou, Junxian He, and Graham Neubig. 2018. Structvae: Tree-structured latent variable models for semi-supervised semantic parsing. In Proceedings of ACL.

