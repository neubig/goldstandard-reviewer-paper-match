DISENTANGLING INFLUENCE: USING DISENTANGLED
REPRESENTATIONS TO AUDIT MODEL PREDICTIONS

arXiv:1906.08652v1 [cs.LG] 20 Jun 2019

Charles T. Marx Haverford College cmarx@haverford.edu

Richard Lanas Phillips Cornell University
rlp246@cornell.edu

Sorelle A. Friedler Haverford College sorelle@cs.haverford.edu

Carlos Scheidegger University of Arizona cscheid@cs.arizona.edu

Suresh Venkatasubramanian University of Utah
suresh@cs.utah.edu

June 21, 2019
ABSTRACT
Motivated by the need to audit complex and black box models, there has been extensive research on quantifying how data features inﬂuence model predictions. Feature inﬂuence can be direct (a direct inﬂuence on model outcomes) and indirect (model outcomes are inﬂuenced via proxy features). Feature inﬂuence can also be expressed in aggregate over the training or test data or locally with respect to a single point. Current research has typically focused on one of each of these dimensions. In this paper, we develop disentangled inﬂuence audits, a procedure to audit the indirect inﬂuence of features. Speciﬁcally, we show that disentangled representations provide a mechanism to identify proxy features in the dataset, while allowing an explicit computation of feature inﬂuence on either individual outcomes or aggregate-level outcomes. We show through both theory and experiments that disentangled inﬂuence audits can both detect proxy features and show, for each individual or in aggregate, which of these proxy features affects the classiﬁer being audited the most. In this respect, our method is more powerful than existing methods for ascertaining feature inﬂuence.
1 Introduction
As machine learning models have become increasingly complex, there has been a growing subﬁeld of work on interpreting and explaining the predictions of these models [17, 8]. In order to assess the importance of particular features to aggregated model predictions or outcomes for an individual instance, a variety of direct and indirect feature inﬂuence techniques have been developed. While direct feature inﬂuence [4, 9, 13, 18] focuses on determining the importance of features used directly by the model to determine an outcome, indirect feature inﬂuence techniques [1] report that a feature is important if that feature or a proxy had an inﬂuence on the model outcomes.
Feature inﬂuence methods can focus on the inﬂuence of a feature taken over all instances in the training or test set [4, 1], or on the local feature inﬂuence on a single individual item of the training or test set [18, 13] (both of which are different than the inﬂuence of a speciﬁc training instance on a model’s parameters [11]). Both the global perspective given by considering the impact of a feature on all training and/or test instances as well as the local, individual perspective can be useful when auditing a model’s predictions. Consider, for example, the question of fairness in an automated hiring decision: determining the indirect inﬂuence of gender on all test
Partially supported by the NSF under grants IIS-1513651, IIS-1633724, IIS-1633387 and DMR-1709351, the DARPA SD2 program, and the Arnold and Mabel Beckman Foundation. The Titan Xp GPU used for this research was donated by the NVIDIA Corporation. Code can be found at https://github.com/charliemarx/disentangling-influence

Disentangling Inﬂuence: Using disentangled representations to audit model predictions
outcomes could help us understand whether the system had disparate impacts overall, while an individual-level feature audit could help determine if a speciﬁc person’s decisions were due in part to their gender.1
Our Work. In this paper we present a general technique to perform both global and individual-level indirect inﬂuence audits. Our technique is modular – it solves the indirect inﬂuence problem by reduction to a direct inﬂuence problem, allowing us to beneﬁt from existing techniques.
Our key insight is that disentangled representations can be used to do indirect inﬂuence computation. The idea of a disentangled representation is to learn independent factors of variation that reﬂect the natural symmetries of a data set. This approach has been very successful in generating representations in deep learning that can be manipulated while creating realistic inputs [2, 3, 6, 12, 19]. Related methods use competitive learning to ensure a representation is free of protected information while preserving other information [5, 14].
In our context, the idea is to disentangle the inﬂuence of the feature whose (indirect) inﬂuence we want to compute. By doing this, we obtain a representation in which we can manipulate the feature directly to estimate its inﬂuence. Our approach has a number of advantages. We can connect indirect inﬂuence in the native representation to direct inﬂuence in the disentangled representation. Our method creates a disentangled model: a wrapper to the original model with the disentangled features as inputs. This implies that it works for (almost) any model for which direct inﬂuence methods work, and also allows us to use any future developed direct inﬂuence model.
Speciﬁcally, our disentangled inﬂuence audits approach provides the following contributions:
1. Theoretical and experimental justiﬁcation that the disentangled model and associated disentangled inﬂuence audits we create provides an accurate indirect inﬂuence audit of complex, and potentially black box, models.
2. Quality measures, based on the error of the disentanglement and the error of the reconstruction of the original input, that can be associated with the audit results.
3. An indirect inﬂuence method that can work in association with both global and individual-level feature inﬂuence mechanisms. Our disentangled inﬂuence audits can additionally audit continuous features and image data; types of audits that were not possible with previous indirect audit methods (without additional preprocessing).
2 Our Methodology
2.1 Theoretical background
Let P and X denote sets of attributes with associated domains P and X . P represents features of interest: these could be protected attributes of the data or any other features whose inﬂuence we wish to determine. For convenience we will assume that P consists of the values taken by a single feature – our exposition and techniques work more generally. X represents other attributes of the data that may or may not be inﬂuenced by features in P . An instance is thus a point (p, x) ∈ P × X . Let Y denote the space of labels for a learning task (Y = {+1, −1} for binary classiﬁcation or R for regression).
Disentangled Representation. Our goal is to ﬁnd an alternate representation of an instance (p, x). Speciﬁcally, we would like to construct x ∈ X that represents all factors of variation that are independent of P , as well as a mapping f such that f (p, x) = (p, x ). We will refer to the associated new domain as D = P × X . We can formalize this using the framework of [10]. For any (p, x), we can deﬁne a group action implicitly in terms of its orbits: speciﬁcally, we deﬁne an equivalence relation (p, x1) ≡ (p , x2) if in the underlying data, changing p to p would change x1 to x2. Note that this is an orbit with respect to the permutation group Sm on P (where m = |P| is the size of the domain P). Our goal is to ﬁnd an equivariant function f and an associated group action that yields the desired disentangled representation.
We can deﬁne a group action ◦ : Sm × D → D on the disentangled representation (p, x ) as the mapping π ◦ (p, x ) = (π(p), x ). Then, given f such that f (x) = x , it is equivariant (π ◦ (p, f (x)) = f (π ◦ (p, x)) and the representation (p, x ) satisﬁes the property of being disentangled. Formally, the group action is the product of Sm and the identity mapping on x , but for clarity we omit this detail.
1While unrelated to feature inﬂuence, the idea of recourse [21] also emphasizes the importance of individual-level explanations of an outcome or how to change it.
2

Disentangling Inﬂuence: Using disentangled representations to audit model predictions

(p, x) −−−π−→ (π(p), x)





f

f

(p, x ) −−−π−→ (π(p), x )

Direct and indirect inﬂuence Given a model M : D → Y , a direct inﬂuence measure quantiﬁes the degree

to which any particular feature inﬂuences the outcome of M on a speciﬁc input. In this paper, we use the SHAP

values proposed by [13] that are inspired by the Shapley values in game theory. For a model M and input

x, the inﬂuence of feature i is deﬁned as [13, Eq. 8] φi(M, x) =

z⊆x

|z|!(n−|z|−1)! n!

[Mx

(z)

−

Mx

(z

\

i)]

where z denotes the number of nonzero entries in z, z ⊆ x is a vector whose nonzero entries are a subset of

the nonzero entries in x, z \ i denotes z with the ith entry set to zero, and n is the number of features. Finally,

Mx(z) = E[M (z)|zS], the conditional expected value of the model subject to ﬁxing all the nonzero entries of

z (S is the set of nonzero entries in z).

Indirect inﬂuence attempts to capture how a feature might inﬂuence the outcome of a model even if it is not explicitly represented in the data, i.e its inﬂuence is via proxy features. The above direct inﬂuence measure cannot capture these effects because the information encoded in a feature i might be retained in other features even if i is removed. We say that the indirect inﬂuence of feature i on the outcome of model M on input x is the direct inﬂuence of some proxy for i, where a proxy for i consists of a set of features S and a function g that predicts i: i.e such that g(xS) x(i). Note that this generalizes in particular the notion of indirect inﬂuence deﬁned by [1]: in their work, indirect inﬂuence is deﬁned via an explicit attempt to ﬁrst remove any possible proxy for i and then evaluate the direct inﬂuence of i. Further, note that if there are no features that can predict xi, then the indirect and direct inﬂuence of i are the same (because the only proxy for i is itself).

Disentangled inﬂuence The key insight in our work is that disentangled representations can be used to compute indirect inﬂuence. Assume that we have an initial representation of a feature vector as (p, x) and train a model M on labeled pairs ((p, x), y). Our goal is to determine the indirect inﬂuence of p on the model outcome. Suppose we construct a disentangled representation (p, x ) as deﬁned above, with the associated encoding function f (x) = x and decoding function f −1.
Proposition 1. The indirect inﬂuence of p on the outcome of M on input x equals φp(M , x ), where M = f −1 ◦ M .

Proof. By the properties of the disentangled representation, there is no proxy for p in the components of x : if there were, then it would not be true that the f was equivariant (because we could not factor the action on p separately from the identity mapping on x ).
Thus, if we wished to compute the indirect inﬂuence of p on model M with outcome x, it is sufﬁcient to compute the direct inﬂuence of p on the model that ﬁrst converts from the disentangled representation back to the original representation and then applies M .

Dealing with errors. The above proposition holds if we are able to obtain a perfectly disentangled and invertible representation. In practice, this might not be the case and the resulting representation might introduce errors. In particular, assume that our decoder function is some g = f −1. While we do not provide an explicit formula for the dependence of the inﬂuence function parameters, we note that it is a linear function of the predictions, and so we can begin to understand the errors in the inﬂuence estimates by looking at the behavior of the predictor with respect to p.
Model output can be written as yˆ = (M ◦ g)(p, x ). Recalling that g(p, x ) = (p, xˆ), the partial derivative of yˆ with respect to p can be written as ∂∂yp = ∂(M∂p◦g) = ∂∂Mxˆ ∂∂xpˆ + ∂∂Mp = ∂∂Mxˆ ∂∂xxˆ ∂∂xp + ∂∂Mp . Consider the term ∂∂xp . If the disentangled representation is perfect, then this term is zero (because x is unaffected by p), and therefore we get ∂∂yp = ∂∂Mp which is as we would expect. If the reconstruction is perfect (but not necessarily the disentangling), then the term ∂∂xxˆ is 1. What remains is the partial derivative of M with respect to the latent encoding (x , p).
3

Disentangling Inﬂuence: Using disentangled representations to audit model predictions

2.2 Implementation
Our overall process requires two separate pieces: 1) a method to create disentangled representations, and 2) a method to audit direct features. In most experiments in this paper, we use adversarial autoencoders [15] to generate disentangled representations, and Shapley values from the shap technique for auditing direct features [13] (as described above in Section 2.1).

Disentangled representations via adversarial autoencoders We create disentangled representations by

training three separate neural networks, which we denote f , g, and h (see Figure 1). Networks f and

g are autoencoders: the image of f has lower dimensionality than the domain of f , and the training

process seeks for g ◦ f to be an approximate identity, through gradient descent on the reconstruction

error ||(g ◦ f )(x) − x||. Unlike regular autoencoders, g is also given direct access to the protected at-

tribute. Adversarial autoencoders [15], in addition, use an ancillary network h that attempts to recover

the protected attribute from the image of f , without access to p itself. (Note the slight abuse of nota-

tion here: h is assumed not to have access to p, while g does have access to it.) During the training of

f and g, we seek to reduce ||(g ◦ f )(x) − x||, but also to increase the error of the discriminator h ◦ f .

The optimization process of h tries to recover the

protected attribute from the code generated by f .

(h and f are the adversaries.) When the process

converges to an equilibrium, the code generated by f

will contain no information about p that is useful to h,

but g ◦ f still reconstructs the original data correctly: f disentangles p from the other features.
The loss functions used to codify this process are LEnc = MSE(x, xˆ) − β MSE(p, pˆ), LDec = MSE(x, xˆ), and LDisc = MSE(p, pˆ), where MSE

x

x

f

g

xˆ

M

encoder

decoder

model

yˆ

p

to be

audited

p

p

is the mean squared error and β is a hyperparame-

disentangled model: M

ter determining the importance of disentanglement relative to reconstruction. When p is a binary feature, LEnc and LDisc are adjusted to use binary cross entropy loss between p and pˆ.

x h pˆ discriminator

I feature inﬂuence
algorithm

Disentangled feature audits Concretely, our method works as follows, where the variable names match the diagram in Figure 1:

Figure 1: System diagram when auditing the indirect inﬂuence of feature p on the outcomes of model g for instance x using direct inﬂuence algorithm I.

DISENTANGLED-INFLUENCE-AUDIT(X, M )

1 for p in FEATURES(X)

2

(f , g, h) = DISENTANGLED-REPRESENTATION(X, p) // (h is not used)

3

M = g◦M

4

X = {f (x) for x in X}

5

SHAPp = DIRECT-INFLUENCE(X , p, M )

6 return {SHAPp for p in FEATURES(X)}

We note here one important difference in the interpretation of disentangled inﬂuence values when contrasted with regular Shapley values. Because the inﬂuence of each feature is determined on a different disentangled model, the scores we get are not directly interpretable as a partition of the model’s prediction. For example, consider a dataset in which feature p1 is responsible for 50% of the direct inﬂuence, while feature p2 is a perfect proxy for p1, but shows 0% inﬂuence under a direct audit. Relative judgments of feature importance remain sensible.

3 Experiments
In this section, we’ll assess the extent to which the disentangled inﬂuence audits is able to identify sources of indirect inﬂuence to a model and quantify its error. All data and code for the described method and below experiments is available at https://github.com/charliemarx/disentangling-influence.

4

Disentangling Inﬂuence: Using disentangled representations to audit model predictions

3.1 Synthetic x + y Regression Data

In order to evaluate whether the indirect inﬂuence calculated by the disentangled inﬂuence audits correctly captures all inﬂuence of individual-level features on an outcome, we will consider inﬂuence on a simple synthetic x + y dataset. It includes 5,000 instances two variables x and y drawn independently from a uniform distribution over [0, 1] that are added to determine the label x + y. It also includes proxy variables 2x, x2, 2y, and y2. A random noise variable c is also included that is drawn independently of x and y uniformly from [0, 1]. The model we are auditing is a handcrafted model that contains no hidden layers and has ﬁxed weights of 1 corresponding to x and y and weights of 0 for all other features (i.e., it directly computes x + y). We use shap as the direct inﬂuence delegate method [13].2
In order to examine the impact of the quality of the disentangled representation on the results, we considered both a handcrafted disentangled representation and a learned one. For the former, nine unique models were handcrafted to disentangle each of the nine features perfectly (see Appendix A for details). The learned disentangled representation is created according to the adversarial autoencoder methodology described in more detail in the previous section.

Direct Inﬂuence

Indirect Inﬂuence

Handcrafted DR

Learned DR

Figure 2: Synthetic x + y data direct shap (left) and indirect (right) feature inﬂuences using a handcrafted (top row) or learned disentangled representation (bottom row).
The results for the handcrafted disentangled representation (top of Figure 2) are as expected: features x and y are the only ones with direct inﬂuence, all x or y based features have the same amount of indirect inﬂuence, while all features including c have zero inﬂuence. Using the learned disentangled representation introduces the potential for error: the resulting inﬂuences (bottom of Figure 2) show more variation between features, but the same general trends as in the handcrafted test case.
Additionally, note that since shap gives inﬂuence results per individual instance, we can also see that (for both models) instances with larger (or, respectively, smaller) 2x or 2y values give larger (respectively, smaller) results for the label x + y, i.e., have larger absolute inﬂuences on the outcomes.
3.1.1 Error Analyses
There are two main sources of error for disentangled inﬂuence audits: error in the reconstruction of the original input x and error in the disentanglement of p from x such that the discriminator is able to accurately predict some pˆ close to p. We will measure the former error in two ways. First, we will consider the reconstruction error, which we deﬁne as x − xˆ. Second, we consider the prediction error, which is g(x) − g(xˆ) - a measure of the impact of the reconstruction error on the model to be audited. Reconstruction and prediction errors close to 0 indicate that the disentangled model M is similar to the model M being audited. We measure the latter form of error, the disentanglement error, as n1 ni=1(p − pˆ)2/var(p) where var(p) is the variance of p. A
2This method is available via pip install shap. See also: https://github.com/slundberg/shap
5

Disentangling Inﬂuence: Using disentangled representations to audit model predictions
disentanglement error of below 1 indicates that information about that feature may have been revealed, i.e., that there may be indirect inﬂuence that is not accounted for in the resulting inﬂuence score. In addition to the usefulness of these error measures during training time, they also provide information that helps us to assess the quality of the indirect inﬂuence audit, including at the level of the error for an individual instance.

Figure 3: Errors on the synthetic x + y data for the reconstruction error (left) when taken across inﬂuence audits for each feature, prediction error (middle), and disentanglement error (right).

These inﬂuence experiments on the x + y dataset demonstrate the importance of a good disentangled representation to the quality of the resulting indirect inﬂuence measures, since the handcrafted zero-error disentangled representation clearly results in more accurate inﬂuence results. Each of the error types described above are given for the learned disentangled representation in Figure 3. While most features have reconstruction and prediction errors close to 0 and disentanglement errors close to 1, a few features also have some far outlying instances. For example, we can see that the c, 2c, and c2 variables have high prediction error on some instances, and this is reﬂected in the incorrect indirect inﬂuence that they’re found to have on the learned representation for some instances.

3.2 dSprites Image Classiﬁcation

The second synthetic dataset is the dSprites dataset commonly used in the disentangled representations literature to disentangle independent factors that are sources of variation [16]. The dataset consists of 737, 280 images (64 × 64 pixels) of a white shape (a square, ellipse, or heart) on a black background. The independent latent factors are x position, y position, orientation, scale, and shape. The images were downsampled to 16 × 16 resolution and only the half of the data in which the shapes are largest were used due to the lower resolution. The binary classiﬁcation task is to predict whether the shape is a heart. A good disentangled representation should be able to separate the shape from the other latent factors.

Indirect Inﬂuence
Figure 4: dSprites data indirect latent factor inﬂuences on a model predicting shape.

Figure 5: The mean squared reconstruction error (left), absolute prediction error (middle), and absolute disentanglement error (right) of the latent factors in the dSprites data under an indirect inﬂuence audit.
6

Disentangling Inﬂuence: Using disentangled representations to audit model predictions
In this experiment we seek to quantify the indirect inﬂuence of each latent factor on a model trained to predict the shape from an image. Since shape is the label and the latent factors are independent, we expect the feature shape to have more indirect inﬂuence on the model than any other latent factor. Note that a direct inﬂuence audit is impossible since the latent factors are not themselves features of the data. Model and disentangled representation training information can be found in Appendix A.
The indirect inﬂuence audit, shown in Figure 4, correctly identiﬁes shape as the most important latent factor, and also correctly shows the other four factors as having essentially zero indirect inﬂuence. However, the audit struggles to capture the extent of the indirect inﬂuence of shape since the resulting shap values are small.
The associated error measures for the dSprites inﬂuence audit are shown in Figure 5. We report the reconstruction error as the mean squared error between x and xˆ for each latent factor. The prediction error is the difference between x and xˆ of the model’s estimate of the probability the shape is a heart. While the reconstruction errors are relatively low (less than 0.05 for all but y position) the prediction error and disentanglement errors are high. A high prediction error indicates that the model is sensitive to the errors in reconstruction and the indirect inﬂuence results may be unstable, which may explain the low shap values for shape in the indirect inﬂuence audit.
3.3 Adult Income Data

Figure 6: Ten selected features for Adult dataset. Direct (left) and indirect (right) inﬂuence are shown. For all features, see Supplemental Material. Low values indicate a one-hot encoded feature is false.

Finally, we’ll consider a real-world dataset containing Adult Income data that is commonly used as a test case in the fairness-aware machine learning community. The Adult dataset includes 14 features describing type of work, demographic information, and capital gains information for individuals from the 1994 U.S. census [20]. The classiﬁcation task is predicting whether an individual makes more or less than $50,000 per year. Preprocessing, model, and disentangled representation training information are included in Appendix A.
Direct and indirect inﬂuence audits on the Adult dataset are given in Figure 10 and in Appendix B. While many of the resulting inﬂuence scores are the same in both the direct and indirect cases, the disentangled inﬂuence audits ﬁnds substantially more inﬂuence based on sex than the direct inﬂuence audit - this is not surprising given the large inﬂuence that sex is known to have on U.S. income. Other important features in a fairness context, such as nationality, are also shown to have indirect inﬂuences that are not apparent on a direct inﬂuence audit. The error results (Figure 7 and Appendix B) indicate that while the error is low across all three types of errors for many features, the disentanglement errors are higher (further from 1) for some rare-valued features. This means that despite the indirect inﬂuence that the audit did ﬁnd, there may be additional indirect inﬂuence it did not pick up for those features.

3.4 Comparison to Other Methods

Here, we compare the disentangled inﬂuence audits results to results on the same datasets and models by the indirect inﬂuence technique introduced in [1], which we will refer to as BBA (black-box auditing).3 However,
this is not a direct comparison, since BBA is not able to determine feature inﬂuence for individual instances,
only inﬂuence for a feature taken over all instances. In order to compare to our results, we will thus take the

3This method is available via pip install BlackBoxAuditing. algofairness/BlackBoxAuditing

See also: https://github.com/

7

Disentangling Inﬂuence: Using disentangled representations to audit model predictions
Figure 7: The reconstruction error (left), prediction error (middle), and disentanglement error (right) of selected Adult Income features under an indirect inﬂuence audit; see the supplemental material for the complete ﬁgure.

Figure 8: Comparison on the synthetic x + y data of the disentangled inﬂuence audits using the handcrafted (left) or learned (middle) disentangled representation with the BBA approach of [1] (right).

mean over all instances of the absolute value of the per feature disentangled inﬂuence. BBA was designed to audit classiﬁers, so in order to compare to the results of disentangled inﬂuence audits we will consider the obscured data they generate as input into our regression models and then report the average change in mean squared error for the case of the synthetic x + y data. (BBA can’t handle dSprites image data as input.)

A comparison of the disentangled inﬂuence and BBA

results on the synthetic x + y data shown in ﬁgure

8 shows that all three variants of indirect inﬂuence

are able to determine that the c, 2c, c2 variables have

comparatively low inﬂuence on the model. The dis-

entangled inﬂuence with a handcrafted disentangled

representation shows the correct indirect inﬂuence

of each feature, while the learned disentangled repre-

sentation inﬂuence is somewhat more noisy, and the

BBA results suffer from relying on the mean squared

error (i.e., the amount of inﬂuence changes based on

the feature’s value).

Figure 9: Comparison on the Adult data of the disentan-

Figure 9 shows the mean absolute disentangled inﬂu- gled inﬂuence audits versus the BBA indirect inﬂuence ence per feature on the x-axis and the BBA inﬂuence approach of [1].

results on the y-axis. It’s clear that the disentangled

inﬂuence audits technique is much better able to ﬁnd

features with possible indirect inﬂuence on this dataset and model: most of the BBA inﬂuences are clustered

near zero, while the disentangled inﬂuence values provide more variation and potential for insight.

4 Discussion and Conclusion
In this paper, we introduce the idea of disentangling inﬂuence: using the ideas from disentangled representations to allow for indirect inﬂuence audits. We show via theory and experiments that this method works across a variety of problems and data types including classiﬁcation and regression as well as numerical, categorical, and image data. The methodology allows us to turn any future developed direct inﬂuence measures into indirect inﬂuence measures. In addition to the strengths of the technique demonstrated here, disentangled inﬂuence audits have the added potential to allow for multidimensional indirect inﬂuence audits that would, e.g., allow a fairness audit on both race and gender to be performed (without using a single combined race and gender feature [7]). We hope this opens the door for more nuanced fairness audits.

8

Disentangling Inﬂuence: Using disentangled representations to audit model predictions
References
[1] P. Adler, C. Falk, S. A. Friedler, T. Nix, G. Rybeck, C. Scheidegger, B. Smith, and S. Venkatasubramanian. Auditing black-box models for indirect inﬂuence. Knowledge and Information Systems, 54(1):95–122, 2018.
[2] A. A. Alemi, I. Fischer, J. V. Dillon, and K. Murphy. Deep variational information bottleneck. International Conference on Learning Representations, 2016.
[3] Y. Bengio, A. Courville, and P. Vincent. Representation learning: A review and new perspectives. IEEE transactions on pattern analysis and machine intelligence, 35(8):1798–1828, 2013.
[4] A. Datta, S. Sen, and Y. Zick. Algorithmic transparency via quantitative input inﬂuence: Theory and experiments with learning systems. In Proceedings of 37th IEEE Symposium on Security and Privacy, 2016.
[5] H. Edwards and A. Storkey. Censoring representations with an adversary. In Proceedings of the 33th International Conference on Machine Learning, 2016.
[6] B. Esmaeili, H. Wu, S. Jain, A. Bozkurt, N. Siddharth, B. Paige, D. H. Brooks, J. Dy, and J.-W. van de Meent. Structured disentangled representations. In K. Chaudhuri and M. Sugiyama, editors, Proceedings of Machine Learning Research, volume 89, pages 2525–2534. PMLR, 16–18 Apr 2019.
[7] S. A. Friedler, C. Scheidegger, S. Venkatasubramanian, S. Choudhary, E. P. Hamilton, and D. Roth. A comparative study of fairness-enhancing interventions in machine learning. In Proceedings of the Conference on Fairness, Accountability, and Transparency, pages 329–338. ACM, 2019.
[8] R. Guidotti, A. Monreale, S. Ruggieri, F. Turini, F. Giannotti, and D. Pedreschi. A survey of methods for explaining black box models. ACM computing surveys (CSUR), 51(5):93, 2018.
[9] A. Henelius, K. Puolamäki, H. Boström, L. Asker, and P. Papapetrou. A peek into the black box: exploring classiﬁers by randomization. Data Min Knowl Disc, 28:1503–1529, 2014.
[10] I. Higgins, D. Amos, D. Pfau, S. Racaniere, L. Matthey, D. Rezende, and A. Lerchner. Towards a deﬁnition of disentangled representations. arXiv preprint arXiv:1812.02230, 2018.
[11] P. W. Koh and P. Liang. Understanding black-box predictions via inﬂuence functions. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pages 1885–1894. JMLR. org, 2017.
[12] A. Kumar, P. Sattigeri, and A. Balakrishnan. Variational inference of disentangled latent concepts from unlabeled observations. International Conference on Learning Representations, 2017.
[13] S. M. Lundberg and S.-I. Lee. A uniﬁed approach to interpreting model predictions. In Advances in Neural Information Processing Systems, pages 4765–4774, 2017.
[14] D. Madras, E. Creager, T. Pitassi, and R. Zemel. Learning adversarially fair and transferable representations. In Proceedings of the 35th International Conference on Machine Learning, 2018.
[15] A. Makhzani, J. Shlens, N. Jaitly, I. Goodfellow, and B. Frey. Adversarial autoencoders. arXiv preprint arXiv:1511.05644, 2015.
[16] L. Matthey, I. Higgins, D. Hassabis, and A. Lerchner. dsprites: Disentanglement testing sprites dataset. https://github.com/deepmind/dsprites-dataset/, 2017.
[17] C. Molnar. Interpretable machine learning: A guide for making black box models explainable. Christoph Molnar, Leanpub, 2018.
[18] M. T. Ribeiro, S. Singh, and C. Guestrin. "Why Should I Trust You?": Explaining the Predictions of Any Classiﬁer. In Proc. ACM KDD, 2016.
[19] M. Tschannen, O. Bachem, and M. Lucic. Recent advances in autoencoder-based representation learning. arXiv preprint arXiv:1812.05069, 2018.
[20] I. M. L. R. University of California. Adult income dataset. https://archive.ics.uci.edu/ml/datasets/ adult.
[21] B. Ustun, A. Spangher, and Y. Liu. Actionable recourse in linear classiﬁcation. In Proceedings of the Conference on Fairness, Accountability, and Transparency, pages 10–19. ACM, 2019.
9

Disentangling Inﬂuence: Using disentangled representations to audit model predictions
A Implementation Details
Synthetic x + y model and disentangled representation information. In both our synthetic experiments with handcrafted and trained disentangled representations we audit a model with no hidden layers that computes x + y exactly from the features x and y. The handcrafted disentangled representation is created to map the features with no error. Suppose for example the protected feature, denoted p, was one of the features based on y (one of y, 2y, y2). The disentangled representation used in this case would be ([x, c], [p]). Here, we see that p will fully reveal the information relating to all of the features based on y, and X = [x, c] does not reveal any information about the protected feature. Thus, this representation satisﬁes the independence and preservation of information requirements. Tnahteurdaelcwoadye.rItfhfeonr emxaapmsptlheisp v=ecyto2,r tbhaecdketcoodtheer ﬁorrsitgcinoaml pfeuatetus r√e pvetoctcoarlc(xul,a2tex,yx, 2th, eyn, 2uys,eys 2th, izs,t2ozc,ozm2)p,uitnet2hye. All features relating to x and z are computed from x and z in the natural way as well. In the disentangled representation we train the encoder, decoder and discriminator each have two hidden layers of 10 hidden units each. We use a 4 dimensional latent vector. All layers in each model have ReLU activations except for the last layer of the decoder and discriminator which have sigmoid activations. We use β = 0.5 as the importance of disentanglement for the encoder. The minibatch size is 16 and we optimize for 10,000 train steps using SGD with a constant learning rate of 0.01.
dSprites model and disentangled representation information. The model we use to predict the shape from the image is a neural network with three layers of 128, 64, and 32 hidden units respectively, and achieves a 97% prediction accuracy on a held out test set. The test set was randomly drawn as 20% of the data. To generate the disentangled representation we use an encoder, decoder and discriminator each with a single hidden layer of 256, 256 and 64 hidden units respectively. We use a 16 dimensional latent vector. The minibatch size is 100 and we optimize for 10,000 train steps using SGD with a constant learning rate of 0.05. All layers in each model have ReLU activations except for the last layer of the decoder and discriminator which have sigmoid activations. We use β = 1 as the importance of disentanglement for the encoder.
Adult Income preprocessing, model, and disentangled representation information. During preprocessing, categorical features are one-hot encoded and numerical features are normalized to mean 0 and standard deviation 1. The “education_num" feature is dropped during preprocessing. For each categorical feature, values which occur in less than 1,000 instances are binned into “rare_value". We train a classiﬁer for the “income>=50K" label with binary cross entropy loss and no hidden layers. The classiﬁer achieves test loss of 0.326 and test accuracy of 84.9%. To generate the disentangled representation we use an encoder, decoder and discriminator which each have two hidden layers with 25 and 12 hidden units respectively. We use a 10 dimensional latent vector. We use β = 0.5 as the importance of disentanglement for the encoder. The models are trained for 4000 train steps with minibatch sizes of 16, using SGD with a constant learning rate of 0.01. We used the canonical train/test split.
Additional Information. All models for the synthetic x + y and dSprites experiments were trained on a MacBook Pro (Early 2015) with a 2.7GHz Processor and 8 GB of RAM. The models for the adult experiments were trained on an NVIDIA Titan Xp GPU. Hyperparameters were chosen via experimentation. Only architectures containing 2 or fewer hidden layers were considered for models used to disentangle the data. The minibatch sizes tested were between 16 and 100, and learning rates between 0.01 and 0.1 were tested. In each experiment, we used at least 5 and no more than 15 evaluation runs.
10

Disentangling Inﬂuence: Using disentangled representations to audit model predictions
B Full Results for Adult Income Dataset
B.1 Direct and Indirect Inﬂuence Results
Figure 10: The full inﬂuence results for the adult data direct (left) and indirect (right) feature inﬂuences.
11

Disentangling Inﬂuence: Using disentangled representations to audit model predictions B.2 Error Results
Figure 11: The full disentanglement (top), reconstruction (left) and prediction (right) error metrics for the adult data experiment.
12

