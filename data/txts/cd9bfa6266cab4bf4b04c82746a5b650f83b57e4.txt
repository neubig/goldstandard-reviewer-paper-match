arXiv:2012.06188v3 [math.OC] 26 Nov 2021

Recent Theoretical Advances in Non-Convex Optimization
Marina Danilova1,2, Pavel Dvurechensky3,4, Alexander Gasnikov2,4,5, Eduard Gorbunov2,4, Sergey Guminov4, Dmitry Kamzolov2,6, Innokentiy Shibaev2,4
Abstract Motivated by recent increased interest in optimization algorithms for nonconvex optimization in application to training deep neural networks and other optimization problems in data analysis, we give an overview of recent theoretical results on global performance guarantees of optimization algorithms for non-convex optimization. We start with classical arguments showing that general non-convex problems could not be solved efﬁciently in a reasonable time. Then we give a list of problems that can be solved efﬁciently to ﬁnd the global minimizer by exploiting the structure of the problem as much as it is possible. Another way to deal with non-convexity is to relax the goal from ﬁnding the global minimum to ﬁnding a stationary point or a local minimum. For this setting, we ﬁrst present known results for the convergence rates of deterministic ﬁrst-order methods, which are then followed by a general theoretical analysis of optimal stochastic and randomized gradient schemes, and an overview of the stochastic ﬁrst-order methods. After that, we discuss quite general classes of non-convex problems, such as minimization of αweakly-quasi-convex functions and functions that satisfy Polyak–Łojasiewicz condition, which still allow obtaining theoretical convergence guarantees of ﬁrst-order methods. Then we consider higher-order and zeroth-order/derivative-free methods and their convergence rates for non-convex optimization problems.
1Institute of Control Sciences RAS, Moscow, Russia 2Moscow Institute of Physics and Technology, Moscow, Russia 3Weierstrass Institute for Applied Analysis and Stochastics, Berlin, Germany 4HSE University, Moscow, Russia 5Institute for Information Transmission Problems RAS, Moscow, Russia 6 Mohamed bin Zayed University of Artiﬁcial Intelligence, Abu Dhabi, United Arab Emirates
1

2

Danilova, Dvurechensky, Gasnikov, Gorbunov, Guminov, Kamzolov, Shibaev

1

Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2

2

Preliminaries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3

2.1

Global Optimization is NP-hard . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4

2.2

Lower Complexity Bound for Global Optimization . . . . . . . . . . . . . . . . . . . 5

2.3

Examples of Non-Convex Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6

3

Deterministic First-Order Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11

3.1

Unconstrained Minimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11

3.2

Incorporating Simple Constraints . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13

3.3

Incorporating Momentum for Acceleration . . . . . . . . . . . . . . . . . . . . . . . . . . 14

4

Stochastic First-Order Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17

4.1

General View on Optimal Deterministic and Stochastic First-Order

Methods for Non-Convex Optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18

4.2

SGD and Its Variants . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26

4.3

Variance-reduced Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33

4.4

Adaptive Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37

5

First-Order Methods under Additional Assumptions . . . . . . . . . . . . . . . . . . . . . . . . . . . 40

5.1

Polyak–Łojasiewicz Condition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40

5.2

Star-convexity and α-weak-quasi-convexity . . . . . . . . . . . . . . . . . . . . . . . . . 43

6

Higher-Order Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46

6.1

Second-Order Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46

6.2

Stochastic Second-Order Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48

6.3

Tensor Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50

7

Zeroth-Order Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53

7.1

Random Directions Gradient Estimations . . . . . . . . . . . . . . . . . . . . . . . . . . . 54

7.2

Variance-Reduced Zeroth-Order Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . 61

8

Globalization Techniques . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65

8.1

Multistart Technique . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65

8.2

Multidimensional Bisection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66

8.3

Langevin Dynamics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67

References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68

1 Introduction
In this survey, we consider non-convex optimization problems in different settings, including stochastic optimization. We are mainly motivated by an increased interest in such problems in connection to applications in machine learning and data analysis, and our main focus is on the methods which possess theoretical guarantees for their global convergence rate or complexity. As we explain ﬁrst by providing classical examples [180, 186], there is no hope to have any theoretical guarantees for ﬁnding a global minimizer in a general non-convex optimization problem in a reasonable time. Despite the quite good practical performance of classical generalpurpose methods such as L-BFGS [195, 96], and proven local superlinear convergence, their global complexity is not well understood.
In the last 20 years, theoretical analysis of the global convergence rate or global complexity guarantees has become de facto a standard in the area of numerical optimization. Since the convexity of the problem allows for such an analysis, many global complexity and convergence results have been obtained in convex optimization [22, 41, 186, 155, 87, 84]. Recent advances in machine learning, which were

Recent theoretical advances in non-convex optimization

3

made possible by the application of neural networks, had lead to the optimization community changing focus to non-convex optimization and, especially to stochastic non-convex optimization. In this non-exhaustive survey, we attempt to highlight existing results on global performance guarantees of large-scale non-convex optimization methods. The large dimension of the decision variable in such problems motivates the use of ﬁrst-order methods, which possess a cheap iteration. Moreover, the large amount of data motivates to use randomized methods such as stochastic gradient descent, which does not require to look through the whole dataset to make one step of the optimization procedure, thus making the iteration even cheaper.
Since, in general, non-convex optimization problems cannot be made efﬁciently solved, we consider several ways to relax this challenging goal. The ﬁrst relaxation consists of ﬁnding problems with hidden convexity or in a convex reformulation of the problem. This requires exploitation of the problem structure as much as it is possible, which limits the generality of the approach, yet leading to a possibility to ﬁnd a global solution. Another way is to change the goal from ﬁnding the global solution to ﬁnding a stationary point or a local extremum. In this case, it is possible to obtain polynomial dependence of the complexity of ﬁrst-order methods on the dimension of the problem and desired accuracy. We consider this approach in the setting of deterministic and stochastic optimization. The third way is to deﬁne a class of nonconvex problems, which is, on the one hand, quite general, and on the other hand, allows to obtain a global performance guarantees of an algorithm. We consider a class of problems with objective satisfying Polyak–Łojasiewicz condition, which leads to global linear convergence rate, and the class of problems with α-weakly-quasiconvex objective, which leads to global sublinear convergence rate. In the above two approaches, we ﬁrst focus on ﬁrst-order methods. Then, motivated by several settings in machine learning such as reinforcement learning, black-box adversarial attacks on neural networks, as well as simulation optimization, in which the gradient of the objective is not available, we consider zeroth-order or derivative-free methods and their convergence rates for non-convex optimization problems. By no means we claim that our survey contains all the important results in this area since the literature is huge and we could miss some recent results. We would like to list here some other books [202, 65, 155, 97] and surveys [132, 66, 258, 58, 62, 236, 279] related to our paper1.

2 Preliminaries
The main challenges in non-convex optimization are caused either by nonconvexity of the feasible set or by non-convexity of the objective function. The ﬁrst case is tightly connected with discrete optimization when the decision variable can take only a discrete set of values. In the second case, yet the variable can take a continuum number of values, the non-convexity of the problem does not allow to
1 See also this webpage with the list of references being updated https://sunju.org/research/nonconvex/ .

4

Danilova, Dvurechensky, Gasnikov, Gorbunov, Guminov, Kamzolov, Shibaev

hope for ﬁnding a global solution in a reasonable amount of time. We start with two particular examples that illustrate the intractability of non-convex optimization in general. This intractability motivates different kinds of relaxations, such as changing the goal to the one consisting of ﬁnding an approximate stationary point instead of a global minimum, or introducing additional assumptions on the problem, or heavily using the structure of the problem, which lead to provable convergence to the global minimizer. Next, we present general non-convex optimization problems and some ways to classify them.

2.1 Global Optimization is NP-hard

Following [180], we consider an example which illustrates that the problem of

ﬁnding the exact global solution of a non-convex problem is NP-hard. To that end,

we consider the minimization problem

∑ 
xm∈Rinn  f (x) := n x4i − 1n
i=1

∑n

2

x2i +

i=1

n
∑ aixi
i=1



4



+ (1 − x1)4 ,

where xi is the i-th component of the vector x. Let A = I − 1n 11⊤, where I is the identity matrix of size n and 1 is a vector of n ones, and let [x]2 denote a vector with components [x]2i = x2i . In this notation, the objective takes the form

f (x) = A[x]2, [x]2 +

n
∑ aixi
i=1

4
+ (1 − x1)4 .

Since A is a positive semideﬁnite matrix, f (x) 0. One may also note that 0 is an eigenvalue of A with multiplicity 1 and that 1 is the corresponding eigenvector. With this in mind, it is not difﬁcult to see that f (x) = 0 if and only if x satisﬁes

n
∑ a1 + aixi = 0, i=2

xi = ±1, i = 2, . . . , n.

The problem of checking whether this equation has a solution is a form of the subset sum problem, which is known to be NP-complete. Since this problem has a solution if and only if the global minimum in the original optimization problem is exactly zero, this implies that the problem of ﬁnding even the value of a global minimum for a non-convex objective is NP-hard.

Recent theoretical advances in non-convex optimization

5

2.2 Lower Complexity Bound for Global Optimization

Following [186], we now derive a lower bound for the complexity of ﬁnding an approximate global minimum of a possibly non-convex objective. Consider the problem
min f (x) ,
x∈[0,1]n
where f is possibly non-convex and Lipschitz-continuous function, i.e., for some M > 0 and for all x, y ∈ [0, 1]n

| f (y) − f (x) | M y − x ∞ .
Such constant exists for all continuous functions f (x) on [0, 1]n, so this assumption is not restrictive. Let us set the desired accuracy in terms of the objective as ε, i.e., our goal is to ﬁnd a point xˆ such that f (xˆ) − f ∗ ε, where f ∗ is the global minimum of f on [0, 1]n. For simplicity, we assume ε to be equal to 1/N for some N ∈ N. Consider a family of continuous non-convex objectives fk(x), k = 1, . . . , Nn, constructed as follows: we divide the hypercube [0, 1]n into (MN/2)n non-intersecting hypercubes Ck with side length 2/(NM) and set

fk(x) = −Mdist∞(x, ∂Ck), 0,

x ∈ Ck, x ∈/ Ck,

where ∂Ck is the boundary of Ck and dist∞(x, ∂Ck) is the distance between x and ∂Ck in the · ∞-norm. Each fk has a minimum value of exactly −ε attained at the center of Ck, and the Lipschitz constant of fk is equal to M.
Any minimization method generating its trajectory based on the values of f (x) and its derivatives at the points of the trajectory would need to sample a point from each Ck to ﬁnd an approximate minimum of each fk(x). This gives us a lower bound on the number of iterations required: Ω ((MN)n) = Ω (Mnε−n). And this bound is attained by the algorithm which simply samples the objective values at the vertices
of a uniform grid and returns the point with the smallest value. This demonstrates that it is practically impossible to solve a high-dimensional non-convex minimiza-
tion problem with any reasonable accuracy unless some additional assumptions are
introduced. A similar complexity bound is proved in [185] for ﬁnding a point xˆ such that
∇ f (xˆ) ∞ ε and xˆ ∞ R. More precisely, for non-convex functions with Lipschitz continuous Hessian, such that there exists at least one point x∗ with ∇ f (x∗) = 0 and x∗ ∞ R, the lower complexity bound is Ω MR2/ε n/2 .

6

Danilova, Dvurechensky, Gasnikov, Gorbunov, Guminov, Kamzolov, Shibaev

2.3 Examples of Non-Convex Problems

In this subsection, we make a non-extensive overview of non-convex problem formulations and applications where they arise, with a focus on tractable problems. One possible way to classify such non-convex problems is to divide them into two groups:
• problems with hidden convexity or analytic solutions; • problems with provable global solution.
Let us consider formulations of a few concrete problems in each of these classes.

2.3.1 Problems with Hidden Convexity or Analytic Solutions

Firstly, it is worth noting a broad class of classical non-convex problems that in-

clude linear-fractional programs, geometric programs, problems with two quadratic

functions, handling convex equality constraints, convexifying constraint sets. Many

such problems are equivalent to convex problems via a simple transformation such

as convex relaxation and duality [38].

Next, a wide range of tasks in machine learning and statistics is reduced to eigen-

problems. Among these problems are the following principal component analysis,

classical multidimensional scaling, and other generalized eigenvalue problems [56].

In the context of non-convex optimization problems, one cannot but mention the

class of combinatorial optimization problems as graph problems. Basically, most of

these problems are NP-complete, but despite this, there are effective approaches and

ways to solve them. Let us consider a closer look at the MAX-CUT problem. This

is a bright example of convex reformulations. In some problems, the goal is to ﬁnd

a point with a value as small as possible (or as large as possible in the context of

maximization problems), but whether this point is close to the global minimum is

not that important. In this case, we can try to approximate the problem with a simpler

one and show that the exact solution to the approximate problem corresponds to a

good solution of the original problem. We will ﬁrst illustrate this idea on the MAX-

CUT problem

∑ max f (x) := 1 n,n Ai j (xi − x j)2 ,

x∈{−1,1}n

2 i, j=1,1

where A = Ai j in,,jn=1,1 (A = AT ). This is a discrete optimization problem. If we are interested only in the value of the functional and not in the cut itself, we can

approximate this problem with a computationally tractable one. Let us introduce

matrix

L = diag

n
∑ Ai j
j=1

n
− A,
i=1

which allows us to write

f (x) = x, Lx .

Recent theoretical advances in non-convex optimization

7

A simple observation: if ς is a random vector uniformly distributed on the Hamming cube {−1, 1}n, then
E ς , Lς ≥ 0.5 max x, Lx .
x∈{−1,1}n
In fact, we can do better due to the construction of Goemans and Williamson [105]

max x, Lx = max L, xxT

x∈{−1,1}n

x∈{−1,1}n

max L, X .
X ∈ Sn+ Xii = 1, i = 1, ..., n

This is an SDP problem. Let Σ be the solution of this SDP problem and let

ξ ∈ N (0, Σ ) , ς = sign (ξ ) .

Then

E ς , Lς ≥ αGW max x, Lx ,
x∈{−1,1}n

where αGW ≈ 0.878567, and this constant is unimprovable provided that P = NP and the Unique games conjecture is true [142].
Further, we would like to highlight the following subclasses of non-convex problems: non-convex proximal operators (Hard-thresholding [32], Potts minimization [145]), discrete problems (Binary graph segmentation, Discrete Potts minimization, Nearly optimal K-means), inﬁnite-dimensional problems (Smoothing splines, Locally adaptive regression splines, Reproducing kernel Hilbert spaces) and statistical problems.
Another important practical example we would like to mention in this part is Blind Deconvolution. Convolutional models arise in a wide range of problems in image processing and computer vision. The most basic convolutional data model – blind deconvolution aims to recover a convolution kernel a0 ∈ Rk and signal x0 ∈ Rm from their convolution
y = a0 ⊛ x0,

where y ∈ Rm and ⊛ is some kind of convolution. This problem is ill-posed in general — there are inﬁnitely many (a0, x0) that convolve to produce y. To overcome this issue, some low dimensional priors about a0 and x0 are necessary. As a result, it is essential to use additional constraints and regularization terms. Different priors
produce different non-convex optimization problems: Sparse Blind Deconvolution [204], Multi-channel Sparse Blind Deconvolution [224], Subspace blind deconvo-
lution [162], Convolutional dictionary learning [198].
The seen data in many settings in science and engineering are admixtures of several latent sources. Given the observations, we would normally wish to infer
the latent sources as well as the admixture distribution. The non-negative matrix
factorization (NMF) [100] mathematical framework offers a natural mathematical framework for modeling numerous mixing problems. In NMF, each row of observation matrix M ∈ Rn×m corresponds to a data-point in Rm. Next, the following assumptions are used: 1) there are r latent sources, encoded by the unobserved matrix W ∈ Rr×m, and 2) each observed data-point can be rewritten as a linear combina-

8

Danilova, Dvurechensky, Gasnikov, Gorbunov, Guminov, Kamzolov, Shibaev

tion of the r sources, the weights of combination are deﬁned via matrix A ∈ Rn×r. The goal is to ﬁnd such representation of matrix M that M = AW with the entries of M, A and W being non-negative. The number r is called the inner-dimension of the factorization, and the smallest possible r is the nonnegative rank of M.
Finally, in the part devoted to problems with Hidden Convexity or Analytical Solution we would like deal with Compressed Sensing and L1-optimization. A vector is said to be s-sparse if it has at most s non-zero elements. Consider solving Ax = b for x where A is an n × d matrix with n < d. The set of solutions to Ax = b is a subspace. However, if we restrict ourselves to s-sparse solutions, under certain conditions on A there is a unique sparse solution [30]. For instance, suppose that there were two s-sparse solutions x1 and x2. Then x1 − x2 would be a 2s-sparse solution to the homogeneous system Ax = 0, which would imply that some 2s columns of A are linearly dependent. Unless A has 2s linearly dependent columns, there can only be one s-sparse solution.
There are many areas in which the problem is to ﬁnd the unique sparse solution to a linear system. One is in plant breeding [30]. Assume we are given a number of apple trees and the strength of some desirable feature of each tree. If we wish to determine which genes are responsible for the feature, we may formulate a system of linear equations Ax = b in which each row of the matrix A corresponds to a tree and each column corresponds to a position on the genome. The vector b corresponds to the strength of the desired feature in each tree. The solution x tells us the positions on the genome corresponding to the genes that account for the feature.
The problem of ﬁnding a sparse solution can be stated as the optimization problem
min x 0 ,
Ax=b
where x 0 is the number of non-zero coordinates of x. This is an NP-hard problem, but it may sometimes be replaced by the convex problem

min x 1 .
Ax=b
What are the sufﬁcient conditions for

min x 0 ⇔ min x 1?

Ax=b

Ax=b

A matrix A is said to satisfy the s-restricted isometry property if for any s-sparse x there exists δs such that

(1 − δs)

x

2 2

Ax

2 2

(1 + δs)

x

2 2

.

The following theorems give sufﬁcient conditions for the equivalence mentioned above to hold [30, 44].
Theorem 1. Suppose A satisﬁes the s-restricted isometry property with δs+1 101√s . Suppose x0 is s-sparse and satisﬁes Ax0 = b. Then, x0 is the unique minimum 1-norm solution to Ax = b.

Recent theoretical advances in non-convex optimization

9

Theorem 2. Suppose A satisﬁes the k-restricted isometry property for k ∈ {s, 2s, 3s} with δs + δ2s + δ3s 1. Suppose x0 is s-sparse and satisﬁes Ax0 = b. Then, x0 is the unique minimum 1-norm solution to Ax = b.

Such results demonstrate the importance of matrices satisfying the restricted isometry property for practice. Fortunately, there is an easy way to obtain such matrices [20].
Theorem 3. Suppose A is an d × n matrix with elements sampled from the Gaussian distribution N (0, 1/d). Then, A satisﬁes the s-restricted isometry property for s < d with 0 < δs < 1 with probability ps satisfying
ps 1 − 2(12/δs)s exp − 3δs2 − δs3 d . 48

2.3.2 Problems with Convergence Results

In this section, we would like to give examples of non-convex optimization prob-

lems for which there are methods with proven convergence results. We start with

the Phase retrieval problem. The phase retrieval problem has been a topic of study

from at least the early 1980s. It is the recovery of a function given the magnitude of

its Fourier transform. This problem could be found in various engineering and scien-

tiﬁc applications such as optical imaging, electron microscopy, and crystallography, etc. [221]. We recover a d-dimensional signal vector x∗ ∈ Cd from its phaseless

measurements

yk = | ak, x |2, k = 1, . . . , M,

with ak denoting the measurement vectors. As a result, the phase-retrieval problem

can be formulated as the following least squares problem or empirical risk mini-

mization

∑M
min

y − | a , x |2 2 .

x

k

k

k=1

This problem is well-motivated by practical concerns, but unfortunately, this is a non-convex problem, and it is not clear how to ﬁnd a global minimum even if one exists. In recent literature, there are various approaches to handle this problem [259, 240, 59], also, algorithms with the provable convergence results were presented in the following papers [42, 267].
In the context of non-convex optimization problems with proven convergence result, one cannot but mention Low-Rank Matrix Completion. There are related problems: matrix completion and matrix sensing [28], which are present in big data problems with incompleteness and other machine learning problems. We would like to draw attention to the exact low-rank matrix completion. Given a matrix Y ∈ Rn×n, partially observed, over a set of indices Ω ⊆ {1, . . . , n}2. Consider the problem of ﬁnding the lowest-rank matrix matching X on the observed set

10

Danilova, Dvurechensky, Gasnikov, Gorbunov, Guminov, Kamzolov, Shibaev

min rank(X)
X
s.t. Xi j = Yi j, (i, j) ∈ Ω . This is a non-convex problem having a natural convex relaxation

min X tr
X

s.t. Xi j = Yi j, (i, j) ∈ Ω

In the paper [133] the ﬁrst results of global optimality of alternating minimization

were obtained for matrix completion and the related problem of matrix sensing.

Proofs of (nearly) linear convergence of gradient descent for Phase retrieval, Ma-

trix completion, Blind deconvolution can be found in the article [172]. Under some

assumptions, it can be shown that the solution to the convex problem is exactly

equal to the solution to the non-convex problem, with high probability over the sam-

pling model [45, 43]. So, this problem can also be attributed to statistical problems

with hidden convexity. Moreover, we we emphasise another relevant problem called

Low-Rank Matrix Recovery. This problem is also known to be non-convex but un-

der some assumptions has no spurious local minima (see [278, 273] and references

therein).

Deep Learning. In the era of AI, training of the deep neural networks [106] is one

of the most popular optimization problems with enormous amount of applications,

e.g., [147, 209, 143, 153, 237, 131, 75, 114, 229, 148]. The simplest example of such

problem [236] is training fully connected neural network for supervised learning

problem

∑ 1 m

min
W =(W ,...,W )

f (W ) := m ℓ(yi, fxi (W )) ,

1L
Wi∈Rni×ni−1 ,i=1,...,L

i=1

where {(xi, yi)}mi=1, xi ∈ Rn0 , yi ∈ Rny are training data points, W = (W1, . . . ,WL) are weights of the model, L is number of fully connected layers, ℓ(·, ·) is a loss function, e.g., quadratic loss or logistic loss, and

fxi (W ) = WLφ (WL−1φ . . . φ (W2φ (W1xi))) ,
where φ is a scalar2 function called an activation function. In general, training neural networks is NP-complete problem [31]. Deep neu-
ral networks have bad local minima both for non-smooth activation functions [239, 213] and smooth ones [165, 268] as well as ﬂat saddles [251]. Nevertheless, there exist positive results about training neural networks. First of all, under different assumptions it was shown that all local minima are global for 1-layer neural networks [232, 123, 94]. Next, one can show that GD/SGD converge under some assumptions to global minimum for linear networks [18, 135, 227] and sufﬁciently

2 By φ (a) where a = (a1, . . ., an)⊤ ∈ Rn is multidimensional vector we mean vector (φ (a1), . . ., φ (an))⊤.

Recent theoretical advances in non-convex optimization

11

wide over-parameterized networks [13]. The detailed summary of recent advances in optimization for deep learning can be found in [236].

2.3.3 Geometry of non-convex optimization problems
In one of the latest survey [280], the authors to distinguish a class of tractable non-convex problems, which have certain properties of symmetry. They highlight non-convex optimization problems with rotational symmetry and discrete symmetry. Problems with rotational symmetry include the previously described phase retrieval and related problems in low-rank matrix factorization and recovery. It turns out that the blind deconvolution and tensor decomposition problems have discrete symmetry.

3 Deterministic First-Order Methods

In this section we focus on the following optimization problem

min f (x) ,

(1)

x∈Q⊆Rn

where Q is a simple, closed, convex, set, and f is continuously differentiable function. The simplest method for this kind of problems is projected gradient descent,
which can be motivated by a simple continuous-time dynamics. For simplicity we start with the unconstrained case with Q = Rn.

3.1 Unconstrained Minimization

In the case Q = Rn, the trajectory of the continuous-time gradient method is the solution to the differential equation x˙ = −∇ f (x(t)). It is easy to see that W (x) = f (x(t)) is a Lyapunov function for this dynamical system. Indeed,

dW d(xt(t)) =

∇ f (x (t)) , dxd(tt)

= ∇ f (x (t)) , −∇ f (x (t))

=−

∇ f (x (t))

2 2

0.

This implies the convergence of the continuous-time gradient descent method to a stationary point.
The classic gradient descent method is then the Euler discretization of the above dynamics and has the form [202]

xk+1 = xk − hk∇ f xk ,

12

Danilova, Dvurechensky, Gasnikov, Gorbunov, Guminov, Kamzolov, Shibaev

where hk ≥ 0 is the stepsize of the method. One of the main assumptions in this
setting is that the function f is L-smooth, or, which is the same, its gradient is Lipschitz-continuous, i.e., for some starting point x0,

∀x, y ∈ x ∈ Rn : f (x) f x0

∇ f (y) − ∇ f (x) 2 L y − x 2 .

Then the stepsize h = 1/L guarantees

f (xk+1)

f (xk) − 1

2
∇ f (xk) .

2L

2

Summing up these inequalities, we obtain

f (xN ) − f (x0)

∑ − 1 N−1 ∇ f (xk) 2

2L k=0

2

− N min ∇ f (xk) 2.

2L k=0,...,N−1

2

Deﬁne f∗ = inf f (x) and assume that this value is ﬁnite. Then
x∈Rn

min ∇ f (xk) 2 2L( f (x0) − f∗) .

(2)

k=0,...,N−1

2

N

This proves that the complexity of ﬁnding an approximate stationary point, i.e. a

point xˆ such that ∇ f (xˆ) 2

ε is O

L( f (x0)− f∗) ε2

. This iteration complexity of ﬁnd-

ing an ε-stationary point N ∼ ε−2 is unimprovable in terms of its dependence on ε

and L for an arbitrary ﬁrst-order method applied to minimization of an L-smooth

objective.

On the one hand this bound is much better than the exponential in the dimension

bound for ﬁnding the global minimum, which was derived in Subsection 2.2. On

the other hand we can guarantee only an approximate stationary point, which could

be a saddle-point or even a maximum. This can be illustrated by the example of

minimization of the following objective [186]

f (x1, x2) = 1 (x1)2 + 1 (x2)4 − 1 (x2)2 .

2

2

2

If we set x0 = (1, 0)T , then xk converges to (0, 0)T as k → ∞, which is a saddle-point. The good news here is that gradient descent can be perturbed by adding some noise in the iterates in such a way that it converged to a local minimum for almost all initial points and escapes saddle-points [136].
It is important to note that, under additional smoothness assumptions that higherorder derivatives of the objective are Lipschitz continuous, i.e.

∀x, y ∈ x ∈ Rn : f (x) f x0

∇p f (y) − ∇p f (x) 2 Lp y − x 2 ,

[51, 50] obtain several lower complexity bounds for ﬁnding an approximate station-

ary

point.

If

this

inequality

holds

for

p

∈

{1, 2},

the

lower

bound

becomes

ε

−

12 7

,

Recent theoretical advances in non-convex optimization

13

and the additional assumption that the same holds for p = 3 gives the lower bound

to

ε

−

8 5

.

Surprisingly,

Lipschitz

continuity

of

derivatives

of

order

4

and

higher

gives

the same lower complexity bound.

3.2 Incorporating Simple Constraints

It is possible to generalize gradient method for the setting of composite optimization with simple convex constraints, i.e. for the problem

min{F(x) := f (x) + ψ(x)},

(3)

x∈Q

where Q is a closed convex set, ψ(x) is a simple convex function, e.g. x 1, and f

is L-smooth function. The standard approach for such problems uses prox-function

d(x) which is continuously differentiable and strongly convex on Q, i.e. d(y) −

d(x) −

∇d(x), y − x

≥

1 2

y−x

2 for any x, y ∈ Q. We deﬁne also the corresponding

Bregman divergence V [z](x) = d(x) − d(z) − d′(z), x − z , x, z ∈ Q. Then the step of

the gradient method from a point x with stepsize h is generalized [186, 103] to

x+ = arg min

∇ f (x), u

+

1 V

[x](u)

+

ψ

(u)

,

u∈Q

h

which

in

the

simplest

case

ψ (x)

≡

0,

d(x)

=

1 2

x

22,

V [z](x)

=

1 2

x−z

22, Q = Rn

coincides with the step of the gradient method. This generalized gradient step leads

to a generalized gradient, which is usually referred to as gradient mapping [186, 103] gQ(x) = 1h (x − x+). In this setting, the authors of [103] prove that

min gQ(xk) 2
k=0,...,N−1

2L(F(x0) − F∗) N

if h = 1/L. Here F∗ is a lower bound for F(x). In the described above simple situation this bound coincides with the bound (2). The authors of [68] prove that if
gQ(x) ε, then x+ is an approximately stationary point of the problem. More precisely, there exist p ∈ ∂ ψ(x+) such that
∇ f (x+) + p ∈ −NQ(x+) + B((1 + L(d)ε),
where NQ(x+) is the normal cone of Q at the point x+, B(r) = {v ∈ Rn : v ∗ r} – ball in the dual space deﬁned by the conjugate norm, and it is assumed that d is L(d)-smooth. Note that there is no contradiction with the exponential lower bound given in the end of Subsection 2.2 since non-necessarily the obtained point x+ has small norm of the gradient.
This approach was further generalized in [33, 82, 98] for the case of optimization with inexact oracle for the function f .

14

Danilova, Dvurechensky, Gasnikov, Gorbunov, Guminov, Kamzolov, Shibaev

Deﬁnition 1 We say that a function f (x) is equipped with an inexact ﬁrst-order
oracle on a set X if there exists δu > 0 and at any point x ∈ X for any number δc > 0 there exists a constant L(δc) ∈ (0, +∞) and one can calculate f˜(x, δc, δu) ∈ R and g˜(x, δc, δu) ∈ Rn satisfying

| f (x) − f˜(x, δc, δu)| δc + δu, f (y) − ( f˜(x, δc, δu) − g˜(x, δc, δu), y − x )

L(δc) x − y 2 + δc + δu, ∀y ∈ Q. 2

In this deﬁnition, δc represents the error of the oracle, which we can control and make as small as we would like to. On the opposite, δu represents the error, which
we can not control. The proposed for this setting method in [82] is adaptive to the constant L, works under inexact calculation of the point x+, and covers several dif-
ferent settings. In particular, smooth functions with Ho¨lder-continuous, i.e. satisfying, for some ν ∈ [0, 1], ∇ f (x) − ∇ f (y) ∗ Lν x − y ν, ∀x, y ∈ Q gradient satisfy this deﬁnition with δu = 0 and

L(δc) =

1−ν 2 1 + ν · δc

1−ν 1+ν 2
Lν1+ν .

As a corollary of the general method, [82] propose a universal method for such problems, which does not require the knowledge of the constants ν, Lν and gives the following convergence rate

min gQ(xk) 2
k=0,...,N−1

2 1+3ν 2ν

1 − ν 40 1+ν · ε

L 1−ν
2ν 1 ν
ν

F(x0) − F∗ + ε ,

N

2

1

or

the

following

complexity

estimate

Lνν (F(x0)−F∗)
1+3ν

to

ﬁnd

gQ (xk )

ε. Inexact

ε 2ν

oracle models for convex optimization can be useful in non-convex optimization

since in some settings a non-convex problem can be considered as a convex problem

with inexact oracle [235, 234].

3.3 Incorporating Momentum for Acceleration
The considered above dynamical system x˙ = −∇ f (x(t)) does not have any mechanical intuition behind it. In [203] the author proposed to consider the following dynamics
µx¨(t) = −∇ f (x(t)) − px˙(t). One of the ways to discretize it gives the so called heavy-ball method
xk+1 = xk − h∇ f (xk) + β (xk − xk−1),

Recent theoretical advances in non-convex optimization

15

where h > 0 is the stepsize and β > 0 is the momentum parameter. Due to the momentum term β xk − xk−1 the method avoids zigzagging for ill-conditioned problems, which leads to signiﬁcant efﬁciency in practice, especially in training neural networks. Despite practical efﬁciency, the theoretical guarantee for this method is
no better than for the gradient method. In particular, [119] considers the dynamical system
µ(t)x¨(t) = −∇ f (x(t)) − p(t)x˙(t),

where µ (t) ∼ ( f (x (t)) − c), c is an upper bound on the global minimum of f (x),

and p (t) = F (∇ f (x (t))). With a special choice of F (·), they show that x (t) converges to a local minimizer xloc such that f xloc c as t → +∞. In [77] it is shown

that for a discretization of a further generalization of the heavy-ball method one may

guarantee

min

∇ f (xk)

2 2

k=1,...,N

2L( f (x0) − f∗) , N

which coincides with the bound (2) for the gradient method. A different type of momentum was proposed in [183] for convex optimization,
which led to the Nesterov’s accelerated gradient method

x1 = x0 − h∇ f x0 ,

xk+1 = xk − h∇ f (xk + βk(xk − xk−1)) + βk(xk − xk−1).

The difference with the heavy-ball method is that the gradient is calculated in the

extrapolated point. This idea has been very fruitful and allowed to obtain many ac-

celerated algorithms for convex optimization. A variant of this method with a spe-

cial choice of the stepsize h and momentum term βk was shown in [102] to have

the same convergence rate (2) as the gradient method. This was further extended in

[104] for the case of objective with Ho¨lder-continuous gradients to obtain a bound

1

Lνν (F(x0)−F∗)
1+3ν

to

ﬁnd

gQ (xk )

ε in the general setting of composite optimiza-

ε 2ν

tion problem (3) with simple constraints. Importantly, this method is universal and

uniform, which means that it has best possible convergence rates for convex and

non-convex problems without knowing whether the problem is convex or not and

without knowing its smoothness parameters such as Ho¨lder exponent and Ho¨lder

constant.

It is possible to combine this idea with the idea of line-search, i.e. minimization in

the direction of the step. The papers [122, 187] propose a modiﬁcation of the accel-

erated gradient method which is listed as Algorithm 1. Instead of explicitly deﬁning

the stepsize h and the momentum term β , this method uses full one-dimensional re-

laxation and local information. This makes this method parameter-free and uniform

for convex and non-convex smooth optimization by providing optimal complexity

bound for the convex and non-convex case. At the same time, inexact line-search is

possible and its sufﬁcient accuracy for achieving the desired accuracy is estimated.

This method shares some similarities with nonlinear conjugate gradient methods

which were analyzed in [182].

16

Danilova, Dvurechensky, Gasnikov, Gorbunov, Guminov, Kamzolov, Shibaev

Algorithm 1 Accelerated Gradient Method with Small-Dimensional Relaxation (AGMsDR)

Ensure: xk 1: Set k = 0, A0 = 0, x0 = v0, ψ0(x) = V [x0](x) 2: for k 0 do
3: βk = arg min f vk + β (xk − vk) ,
β ∈[0,1]

yk = vk + βk(xk − vk).

4:

Let (∇ f (yk))# be such that

∇ f (yk), (∇ f (yk))#

=

∇ f (yk)

2 ∗

and

(∇ f (yk))#

2 = 1.

hk+1 = arg min f yk − h(∇ f (yk))# , xk+1 = yk − hk+1(∇ f (yk))#.
h0

Find ak+1 from equation f (yk) − 2(Aka+2k+a1k+1) ∇ f (yk) 2∗ = f (xk+1). 5: Set Ak+1 = Ak + ak+1. 6: Set ψk+1(x) = ψk(x) + ak+1{ f (yk) + ∇ f (yk), x − yk }. 7: vk+1 = arg minx∈Rn ψk+1(x), k = k + 1 8: end for

The above idea was further extended in [120] where an accelerated alternating minimization method was proposed and analyzed for convex and non-convex problems. The main assumption is that the set of coordinates is divided into n¯ disjoint subsets (blocks) Ip, p ∈ {1, . . ., n¯} and minimization in each block when the other variables are freezed can be made explicitly. The resulting accelerated alternating minimization algorithm is listed as Algorithm 2. This method is also parameter-free and uniform for convex and non-convex smooth optimization with optimal complexity bound for the convex and non-convex case.

Algorithm 2 Accelerated Alternating Minimization (AAM)

Require: Starting point x0.

Ensure: xk

1: Set A0 = 0, x0 = v0.

2: for k 0 do

3: Set βk = arg min f xk + β (vk − xk)
β ∈[0,1]

4: Set yk = xk + βk(vk − xk)

5:

Choose ik = arg max

∇i f (yk)

2 2

i∈{1,...,n¯}

6: Set xk+1 = arg min f (x), i.e. minimize f in the corresponding block.
x∈Sik (yk )
7: Find ak+1, Ak+1 = Ak + ak+1 from

f (yk) − a2k+1 ∇ f (yk) 2 = f (xk+1)

2Ak+1

2

8: Set vk+1 = vk − ak+1∇ f (yk) 9: end for

Recent theoretical advances in non-convex optimization

17

The sequence yk of this algorithm satisﬁes

min

∇ f (yk)

2 2

k=1,...,N

2n¯L( f (x0) − f∗) , N

i.e. there is an additional multiplier M – number of blocks. If the function turns out to be convex, then the same method generates the sequence xk which gives the decay
of the objective similar to accelerated gradient method:

f (xk) − f (x∗)

2n¯L

x0 − x∗

2
2,

N2

where x∗ is the closest to x0 global minimizer. By exploiting the idea of Nesterov’s acceleration and combining it with the no-
tion of negative curvature, the authors of [48] manage to accelerate ﬁrst-order meth-
ods for non-convex optimization under additional assumptions that second and third derivatives are Lipschitz continuous. More precisely, if L-smooth function has also Lipschitz continuous Hessian, they obtain complexity O ε−7/4 log(1/ε) to ﬁnd a point xˆ such that ∇ f (xˆ) 2 ε. Assuming additionally that the third derivative is Lipschitz, this bound is improved to O ε−5/3 log(1/ε) .

4 Stochastic First-Order Methods

In this section, we consider the same problem as in Section 3:

min f (x),

(4)

x∈Rn

where function f is a general non-convex L-smooth function with the uniform lower bound f∗, i.e., it is differentiable and
f (x) ≥ f∗ ∀x ∈ Rn, ∇ f (x) − ∇ f (y) 2 ≤ L x − y 2 ∀x, y ∈ Rn.

We are interested in two particular cases: expectation minimization

f (x) = Eξ [ f (x, ξ )],

(7)

and ﬁnite-sum minimization

∑ 1 m

f (x) =

fi(x).

(8)

m i=1

Such problems usually arise in applications of (deep) machine learning [106, 236] and mathematical statistics [233], and typically they are solved via stochastic ﬁrstorder methods.

18

Danilova, Dvurechensky, Gasnikov, Gorbunov, Guminov, Kamzolov, Shibaev

In general, the best one can expect to achieve is an approximate stationary point [250, 17]. To be speciﬁc, for this class of problems stochastic ﬁrst-order methods in the worst case can only ﬁnd such point xˆ that

E

∇ f (xˆ)

2 2

≤ ε2

(9)

For simplicity, we will call the point xˆ as ε-stationary point, but mean by this that inequality (9) holds.
Below we summarize recent results about ﬁnding ε-stationary point using stochastic ﬁrst-order methods. We start with presenting the general and uniﬁed approach to analyze optimal deterministic and stochastic ﬁrst-order methods for objectives of types (7) and (8) in the general settings. After that, we consider 3 big classes of stochastic ﬁrst-order methods with convergence guarantees: SGD and its variants, variance reduced methods, and adaptive stochastic methods.

4.1 General View on Optimal Deterministic and Stochastic First-Order Methods for Non-Convex Optimization
Assume that at each point x, we have access to the estimator g(x) of the gradient ∇ f (x). For now, it is not important to specify what properties g(x) satisﬁes. In these settings one can use Algorithm 3 in order to ﬁnd ε-stationary point.

Algorithm 3 General scheme of the optimal ﬁrst-order method for non-convex op-

timization

Require:

learning rates {hk}k≥0 satisfying hk ≤

1 2L

,

starting

point

x0

∈

Rn,

stopping

criterion

C

1: for k = 0, 1, 2, . . . do

2: Get gk = g(xk )

3: if C holds then

4:

xN = xk

5:

break

6: else

7:

xk+1 = xk − hkgk

8: end if

9: end for 10: return xN

Below we derive preliminary inequalities playing the central role in the analysis of optimal (stochastic) ﬁrst-order algorithms. From L-smoothness of f we have

Recent theoretical advances in non-convex optimization

19

f (xk+1) ≤ f (xk) + ∇ f (xk), xk+1 − xk + L xk+1 − xk 2

2

2

= f (xk) + gk, xk+1 − xk + ∇ f (xk) − gk, xk+1 − xk + L xk+1 − xk 2

2

2

≤

f (xk) − hk

gk

2 2

+

hk

∇ f (xk) − gk

2 2

+

1 +L

4hk 2

xk+1 − xk 22,

where in the last inequality we use Fenchel–Young inequality:

a, b

≤

1 2α

a

2 2

+

α2 b 22 with a = ∇ f (xk) − gk, b = xk+1 − xk and α = 21hk . Since hk ≤ 21L and xk+1 =

xk − hkgk we can continue our derivations:

f (xk+1) ≤ f (xk) − hk

gk

2 2

+

hk

∇ f (xk) − gk

22.

2

Now it is crucial to specify what we need to assume about g(x). We emphasize that

all 3 cases considered below are based on the tight bounds for

∇ f (xk) − gk

2 2

or

its

expectation.

4.1.1 Deterministic Case

In this case we assume that for all x ∈ Rn we have an access to such g(x) that

g(x) − ∇ f (x) 2 ≤ ε2 .

(11)

2 10

In other words, g(x) is good enough approximation of ∇ f (x). Consider the stopping

criterion C =

gk

2 2

≤

2ε 2 5

and

let

hk

=

1 2L

for

all

k

≥

0.

First

of

all,

if

Algorithm

3

stops, then

gN

2

≤

4ε 2 10

and

xN

satisﬁes

(11)

∇ f (xN )

2 2

=

∇ f (xN ) − gN + gN

2 2

≤

2

∇ f (xN ) − gN

2 2

+

2

gN

2 2

≤

ε2.

Next, we derive an upper bound for such N that Algorithm 3 stops after N iterations. Assume that, after N iterations the method has not stopped. Then for all k = 0, 1, . . . , T we have

f

(xk+1

)

(10),(11)
≤

f (xk) −

4hk ε 2

+

hk ε 2

=

f (xk) −

3ε2 .

10 10

20L

Unrolling the recurrence we obtain:

20

Danilova, Dvurechensky, Gasnikov, Gorbunov, Guminov, Kamzolov, Shibaev

f (xN+1) ≤ f (x0) − 3ε2 (N + 1) 20L

20L( f (x0) − f (xN+1))

20L( f (x0) − f∗)

N≤

3ε 2

−1≤

3ε 2

− 1.

Therefore, the methods stops after

N ≤ 20L( f (x0) − f∗) 3ε 2

iterations. This bound is optimal up to constant factors [51].

4.1.2 Stochastic Case: Uniformly Bounded Variance

In this case, we assume that for all x ∈ Rn we have E [g(x) | x] = ∇ f (x), E g(x) − ∇ f (x) 22 | x ≤ ε22 . (12)
For example, this situation appears when

f (x) = Eξ [ f (x, ξ )]

where ξ is a random variable with distribution D and g(x) is formed as

∑ 1 r

g(x) = ∇ fi(x, ξi)

(13)

r i=1

where ξ1, . . . , ξr are i.i.d. samples from D and

Eξ [∇ f (x, ξ )] = ∇ f (x),

Eξ

∇ f (x, ξ ) − ∇ f (x)

2 2

≤ σ2.

(14)

Indeed, if we choose r = max

1,

2σ 2 ε2

, then due to independence of ξ1, . . . , ξr we

have:

∑ E

g(x) − ∇ f (x)

2 2

|

x

=1

r
Eξ

∇ f (x, ξi) − ∇ f (x) 2 ≤ σ 2 ≤ ε2 .
2

r2 i=1 i

r2

Then, taking conditional expectation E · | xk from the both sides of (10) we derive

Recent theoretical advances in non-convex optimization

21

E f (xk+1) | xk

≤ f (xk) − hk E

gk

2 2

|

xk

+ hkE

gk − ∇ f (xk)

2 2

|

xk

2

= f (xk) − hk ∇ f (xk) 2 − hk E gk − ∇ f (xk) 2 | xk

2

22

2

+hkE

gk − ∇ f (xk)

2 2

|

xk

= f (xk) − hk 2

(12)
≤

f (xk) − hk

2

∇ f (xk) ∇ f (xk)

2 + hk E 22 2 + hkε2 . 24

gk − ∇ f (xk)

2 2

|

xk

After that, we take the full expectation from the both sides of the previous inequality,

choose

hk

≡

1 2L

and

sum

up

the

result

for

k

=

0, 1, . . . , N

− 1:

∑ 1 N−1
E N k=0

∇ f (xk)

2 2

∑ ≤ 4L N−1 E[ f (xk)] − E[ f (xk+1)]
N k=0

= 4L f (x0) − E[ f (xN )] + ε2

N

2

≤ 4L f (x0) − f∗ + ε2 .

N

2

+ ε2 2

Finally, we choose the output of the method xˆN uniformly at random from x0, x1, . . . , xN−1 which implies

E ∇ f (xˆN ) 2 ≤ 4L f (x0) − f∗ + ε2 .

2

N

2

Taking N = 8L( f (x0)− f∗) we obtain E ∇ f (xˆN ) 2 ≤ ε2. Moreover, the total number

ε2

2

of stochastic oracle calls (number of ∇ f (x, ξ )-calculations) is

∑N−1

8L f (x0) − f∗ 16L f (x0) − f∗ σ 2

k=0 rk = max ε2 , ε4 .

This bound is optimal up to constant factors for the case when the variance is uniformly upper bounded [17].

4.1.3 Stochastic Case: Finite Sum Minimization
In this case we assume that the objective function has a ﬁnite sum structure (8) with L-smooth summands. In fact, this smoothness constant L can be signiﬁcantly larger than the smoothness constant of f . It is essential for providing a fair comparison of different complexity results. It is possible to improve the dependence on L in the ﬁnal complexity bounds [163] using average smoothness assumption, but for simplicity we consider the case when all summands are L-smooth. Moreover, we as-

22

Danilova, Dvurechensky, Gasnikov, Gorbunov, Guminov, Kamzolov, Shibaev

sume that there exists constant σ 2 (possibly inﬁnite) such that for ξ taken uniformly at random from {1, . . . , m} and for all x ∈ Rn

Eξ

∇ fξ (x) − ∇ f (x)

2 2

≤ σ2.

(15)

We deﬁne rk and gk in the following way:

20σ 2 rk = r = max 1, ε2 ,

q = min {r, m} ,

r

 1r ∑ ∇ fξk,j (xk),

gk

=

j=1
∇ f (xk),

∇ fξk (xk) − ∇ fξk (xk−1) + gk−1,

1 hk = h = √ .
10L q

if r < m and r divides k,
if m ≤ r and m divides k, otherwise

Here, at iteration k random index ξk is sampled uniformly at random from {1, . . . , m} if k is not divisible by q and random indices ξk,1, . . . , ξk,r are i.i.d. samples from uniform distribution on {1, . . ., m} if q = r and r divides k. As the result, we obtain the
variant of SPIDER [90]. We notice that for k = aq + p, p ∈ {0, 1, . . ., q − 1} iteration
k requires 2 calculations of ∇ fξ (x) when p = 0 and q calculations of ∇ fξ (x) when p = 0. This implies that q iterations of the method requires only 3q calculations of
∇ fξ (x), so, if k ≥ q, then the number of stochastic ﬁrst-order oracle coincides with the number of iterations up to a constant factor 3.

Below we present a simpliﬁed approach to analyze SPIDER. As before, our goal

is to show that E

gk − ∇ f (xk)

2 2

can be upper-bounded by either something small

or something that can be controlled by other terms in (10). First of all, if k = aq, then



0,

 if q = m,

E

gk − ∇ f (xk)

2 2

2

=

E 

1

r
∑ ∇ fξ

(xk) − ∇ f (xk)

,

if q = r



r j=1 k, j

2



0,

if q = m,

=

1

r
∑E

r2 j=1

2
∇ fξk,j (xk) − ∇ f (xk) 2 , if q = r

(15) 0, if q = m, (16) 0, if q = m,

≤ σ2 , if q = r, ≤ ε2 , if q = r,

r

20

 where E 

r

1 r

∑

∇ fξk,j (xk) − ∇ f (xk)

j=1

2

=

1

r
∑E

r2 j=1

2

2
∇ fξk,j (xk) − ∇ f (xk) 2 due

to independence of ξk,1, . . . , ξk,r and in the third inequality we applied the tower

Recent theoretical advances in non-convex optimization

23

property: E[·] = E E · | xk . Secondly, if k = aq + p with p ∈ {1, . . . , q − 1} we have

E

2
gk − ∇ f (xk)

(=18) E

∇ fξ (xk) − ∇ fξ (xk−1) + gk−1 − ∇ f (xk) 2

2

k

k

2

=E

∇ fξ (xk) − ∇ fξ (xk−1) − ∇ f (xk) + ∇ f (xk−1) 2

k

k

2

+E gk−1 − ∇ f (xk−1) 2
2

where we use the variance decomposition3 Eξk

η

2 2

= Eξk

η − Eξk [η]

2 2

+

Eξk [η] for random vector η = ∇ fξk (xk) − ∇ fξk (xk−1) + gk−1 − ∇ f (xk) together

with the tower property E[·] = E Eξk [·] . Using the inequality above together with

a+b

2 2

≤

2

a

2 2

+

2

b

22, a, b ∈ Rn and L-smoothness of

f1,... , fm, f

we get

E

2
gk − ∇ f (xk) ≤ 2E

∇ fξ (xk) − ∇ fξ (xk−1) 2 + 2E

∇ f (xk) − ∇ f (xk−1) 2

2

k

k

2

2

+E gk−1 − ∇ f (xk−1) 2
2

≤ 4L2E

xk − xk−1

2 2

+E

gk−1 − ∇ f (xk−1) 2
2

= 4L2h2E

gk−1

2 2

+E

gk−1 − ∇ f (xk−1) 2
2

≤ 8L2h2E

∇ f (xk−1)

2 2

+

1 + 8L2h2

E

gk−1 − ∇ f (xk−1) 2 .
2

Unrolling the recurrence we derive

3 Here Eξk [·] is a mathematical expectation conditioned on everything despite ξk, i.e. expectation is taken w.r.t. the randomness coming only from ξk.

24

Danilova, Dvurechensky, Gasnikov, Gorbunov, Guminov, Kamzolov, Shibaev

2
E gk − ∇ f (xk)
2

∑p

≤

8L2h2

1 + 8L2h2 l−1 E

∇ f (xk−l )

2 2

l=1

+ 1 + 8L2h2 p E

gaq − ∇ f (xaq)

2 2

(20), p≤q
≤

∑p
1 + 8L2h2 q 8L2h2E
l=1

∇ f (xaq+l)

2 2

+ 1 + 8L2h2 q 0ε2, , iiff qq == mr ,
20

∑ (1+x)q≤eqx

p

≤ exp 8L2h2q 8L2h2E

l=1

∇ f (xaq+l)

2 2

+ exp 8L2h2q

0, if q = m, ε202 , if q = r.

Next, using the choice of the stepsize h = 1/(10L√q) we obtain

∑ E

2

p

gk − ∇ f (xk) ≤ 9L2h2E

∇ f (xaq+l) 2 + 11ε2 .

2

l=1

2 200

Finally, we put all the inequalities together. We start with modifying (10):

f (xk+1) ≤ f (xk) − hk

gk

2 2

+

hk

∇ f (xk) − gk

2 2

2

≤ f (xk) − h ∇ f (xk) 2 + 3h ∇ f (xk) − gk 2,

4

22

2

where we used that inequality

a+b

2 2

≥

1 2

a

2 2

−

b

2 2

holds

for

all

a, b

∈

Rn

(in

particular, we use a = ∇ f (xk) and b = gk − ∇ f (xk)). Next, we take the full mathe-

matical expectation from the both sides of previous inequality (taking into account

that k = aq + p):

E[ f (xaq+p+1)] ≤ E[ f (xaq+p)] − h E 4
≤ E[ f (xaq+p)] − h E 4
+ 33hε2 . 400

∇ f (xaq+p) 2 + 3h E 22

gaq+p − ∇ f (xaq+p)

2 2

∑ ∇ f (xaq+p)

2

+ 3h

p
9L2h2E

2 2 l=1

∇ f (xaq+l)

2 2

We notice that this inequality holds for all integers a ≥ 0 and p ∈ {0, . . . , q − 1}. Summing up these inequalities for p = 0, . . . , P and taking a = A where N = Aq + P, P ∈ {0, . . . , q − 1} we get

Recent theoretical advances in non-convex optimization

25

∑ ∑ 0 ≤ P E[ f (xAq+p)] − E[ f (xAq+p+1)] − h P E ∇ f (xAq+p) 2

p=0

4 p=0

2

∑ ∑ 27L2h3 P p

+

E

2 p=0 l=1

∇ f (xAq+l) 2 + 33hε2(P + 1)

2

400

P≤q−1
≤E

f (xAq)

−E

f (xAq+P+1)

−h

1 − 27L2h2q

4

2

P
∑E
p=0

∇ f (xAq+p)

2 2

+ 33hε2(P + 1) 400

∑ (=19) E f (xAq) − E f (xAq+P+1) − 23h P E ∇ f (xAq+p) 2 + 33hε2(P + 1) ,

200 p=0

2

400

hence
∑ 23h P
E 200 p=0

∇ f (xAq+p) 2 ≤ E f (xAq) − E f (xAq+P+1) + 33hε2(P + 1) .

2

400

These inequalities hold for all A and P. Then we can sum up these inequalities for (A, P) = (0, q − 1), (1, q − 1), . . . , (Aˆ, Pˆ) and get that for Nˆ = Aˆq + Pˆ and divide the result by 23h2(0Nˆ0+1) and get

∑ 1 Nˆ
E Nˆ + 1 k=0

∇ f (xk)

2 2

200 f (x0) − E f (xNˆ +1)

≤

23h(Nˆ + 1)

+ 33ε2 46

(19) 2000L√q f (x0) − f∗ 33ε2

≤

23(Nˆ + 1)

+ 46 .

Finally, taking xˆNˆ uniformly at random from x0, . . . , xNˆ we get

ˆ

2000L√q f (x0) − f∗ 33ε2

E

∇ f (xˆN )

2 2

≤

23(Nˆ + 1)

+ 46 .

This implies that after

4000L√q( f (x0) − f (x∗)) Nˆ = 13ε2

(16),(17) 4000L( f (x0) − f (x∗))

=

13ε 2

min

√ m, max

√ 20σ
1, ε

iterations we reach E

∇ f (xˆNˆ )

2 2

≤ ε2. Moreover, it requires

26

Danilova, Dvurechensky, Gasnikov, Gorbunov, Guminov, Kamzolov, Shibaev

L( f (x0) − f (x∗))

√

σ

O

ε2

min m, max 1, ε

σ2 + min m, max 1, ε2

calculations of ∇ fξ (x) which is optimal up to constant factors [90].

4.2 SGD and Its Variants

As it was shown in the previous section, SGD

xk+1 = xk − hkg(xk), E[g(x)] = ∇ f (x)

in the settings of Section 4.1.2 requires O L( f (xε02)− f∗ iterations with batch size

r =Θ

max

1,

σ2 ε2

to ﬁnd an ε-stationary point in expectation. The total number

of stochastic ﬁrst-order oracle calls equals

L( f (x0) − f∗)

σ2

O

ε2

max 1, ε2 .

(22)

We emphasize that we use large batch size for the sake of simplicity and uniﬁcation of the results in 3 different cases. In fact, it is possible to obtain the bound (22) using smaller stepsizes and constant batch sizes of the order O(1) [101].

4.2.1 Assumptions on the Stochastic Gradient

In addition to assumption (14), which is quite restrictive, there exist several other assumptions on the stochastic gradient studied in the literature. Recently in [141] it was proposed a simple and uniﬁed way to cover the most popular ones.
Assumption 4.1 (Expected Smoothness; Assumption 2 from [141]) The second moment of stochastic gradients satisﬁes

E

g(x)

2 2

≤ 2A ( f (x) − f∗) + B

∇ f (x)

2 2

+

C

(23)

for some A, B,C ≥ 0 and for all x ∈ Rn.

This assumption generalizes the notion of expected smoothness introduced and adjusted for convex problems in [117]. Moreover, the following assumptions are
stronger than Assumption 4.1 or can be seen as special cases of Assumption 4.1
(see more details and formal proofs in [141]). Uniformly upper-bounded variance (UV) assumption. Indeed, if A = 0, B = 1 and C = σ 2, then using variance decomposition inequality (23) implies (14):

(23)

E

g(x) − ∇ f (x)

2 2

=E

g(x)

2 2

−

∇ f (x)

2 2

≤ σ2.

Recent theoretical advances in non-convex optimization

27

Expected strong growth condition (E-SG). When A = C = 0 and B = α ≥ 1 inequality (23) transforms into so-called expected strong growth condition [231, 248]:

E

g(x)

2 2

≤α

∇ f (x)

22.

(24)

Maximal strong growth condition (M-SG) [245, 216] states that there exists such

α > 0 that

g(x)

2 2

≤

α

∇ f (x)

2 2

almost

surely

for

all

x

∈

Rn.

This condition implies E-SG (24) while known convergence results in expectation under M-SG assumption have no advantage in comparison with their counterparts under E-SG. Relaxed growth condition (RG) [37] can be seen as another special case of Assumption 4.1 with A = 0, B = α ≥ 1 and C = β ≥ 0 or as an extension of E-SG:

E

g(x)

2 2

≤α

∇ f (x)

2 2

+

β

.

(25)

However, there exist simple problems of type (4)+(7) that ﬁt the settings we are

interested in but do not satisfy (25) (see Proposition 1 from [141]).

Gradient confusion condition (GC) [214] was developed for the ﬁnite-sum case

(8). In particular, it states that there exists such η > 0 that for all i, j = 1, . . . , m and for all x ∈ Rn

∇ fi(x), ∇ f j(x) ≥ −η.

(26)

One can show (see Theorem 1, [141]) that inequality (26) implies (25) with α = m and β = η(m − 1), and, as a consequence, it is a special case of Assumption 4.1 with A = 0, B = m, and C = η(m − 1).
Sure-smoothness condition (SS) [159] is deﬁned for the case when the objective is represented as an expectation (7) and g(x) = ∇ f (x, ξ ) where ξ is sampled independently at each iteration of SGD. That is, sure-smoothness condition means that4 for all x, y ∈ Rn

∇ f (x, ξ ) − ∇ f (y, ξ ) 2 ≤ L x − y 2 and f (x, ξ ) ≥ 0 almost surely in ξ . (27)

Applying classical corollaries of L-smoothness one can derive inequality (23) with A = 2L, B = 0, and C = 2L f∗ from (27).
Next, Assumption 4.1 covers arbitrary sampling setup and distributed setup with quantization5. For simplicity, we mention only sampling with replacement
as a special case of arbitrary sampling (see more examples in [141]). In particu-
lar, consider the ﬁnite-sum optimization problem (4)+(8) and assume that fi is Lismooth and bounded from below by fi,∗ for all i = 1, . . . , m. Moreover, assume that g(x) = ∇ f j(x) where j = i with probability pi ≥ 0, i = 1, . . . , m, ∑mi=1 pi = 1. Then,

4 In the original paper [159], authors considered more general situation when stochastic realizations f (x, ξ ) have Ho¨lder-continuous gradients. 5 This technique is applied in distributed optimization to reduce the overall communication cost (e.g., see [5, 27, 108]). However, methods for distributed optimization are out of scope of our survey.

28

Danilova, Dvurechensky, Gasnikov, Gorbunov, Guminov, Kamzolov, Shibaev

one can prove [141] that Assumption 4.1 is satisﬁed in this case with A = maxi mLpi i ,

B

=

0,

and

C

=

2A∆∗

=

2A m

∑mi=1

( f∗

−

fi,∗).

That

is,

if

we

apply

uniform

sampling,

i.e.,

pi

=

1 m

for

all

i

=

1, . . . , m,

then

we

get

A

=

maxi Li,

B

=

0,

C

=

2 maxi Li∆∗,

and

if

importance

sampling

with

pi

=

Li ∑m

L

is applied, then Assumption 4.1 holds

l=1 l

with

A

=

L

=

1 m

∑mi=1

Li,

B

=

0,

and

C

=

2L∆∗.

Finally, under Assumption 4.1 Khaled and Richta´rik [141] derived the following

complexity bound: if h = min

√1 LAN

,

1 LB

,

ε 2LC

, then inequality

min E ∇ f (xk) 2 ≤ ε

(28)

0≤k≤N−1

is satisﬁed after

L( f (x0) − f∗)

A( f (x0) − f (x∗)) C

N=O

ε2

max B,

ε2

, ε2

(29)

iterations of SGD. It is worth to mention that this bound gives the sharpest rates for all known special cases. We summarize some of them in Table 1. We notice that (28) is weaker than (9), but it is easy to obtain the same bound (29) guaranteeing (9) instead of (28) based on the analysis given in [141].

Problem (4)+(7) (4)+(7)/(8) (4)+(8) (4)+(8) (4)+(8)

Settings UV (14) RG (25) GC (26) Uniform Sampling Importance Sampling

Citation [101]
[37, 248] [214] [141] [141]

Complexity

L∆0 ε2

max

1,

σ2 ε2

L∆0 max
ε2

α

,

β ε2

L∆0 ε2

max

m,

η (m−1) ε2

L maxi Li∆0 ε4

max

{∆0,

∆∗}

LL∆0 ε4

max {∆0,

∆∗}

Table 1: Summary of the complexity results for SGD under different assumptions

on the stochastic gradient. The column “Complexity” contains an overall number of

stochastic ﬁrst-order oracle calls needed to ﬁnd ε-stationary point neglecting con-

stant factors. Notation: ∆0 = f (x0) − f∗, σ 2 = a uniform bound for the variance of

the stochastic gradient (14), α, β = relaxed growth condition parameters, η = gra-

dient

confusion

parameter,

∆∗

=

1 m

∑mi=1( f∗

−

fi,∗ ),

maxi Li

=

maximal

smoothness

constant of fi in (8), L = averaged smoothness constant of fi in (8).

4.2.2 The Choice of the Stepsize
In practice, instead of using the constant stepsize for SGD it is popular to periodically decrease the stepsize by some factor [35, 152, 127] even for non-convex

Recent theoretical advances in non-convex optimization

29

problems. For strongly convex problems such a choice is natural: it is well-known [112] that if the stepsize equals h and strong convexity parameter equals µ, then SGD converges with linear rate O((hµ)−1) to the neighborhood of the solution with size proportional to h. Surprisingly, SGD enjoys similar behaviour even for nonconvex problems which was recently shown in [223].
In the neural networks training, “warmup” [118, 115] and cyclical stepsize [230, 170] schedules are also very popular and useful. The ﬁrst one refers to the strategy when, during several epochs of training, tiny stepsizes are used, and then they are increased. This technique was successfully applied for several deep learning problems like ResNet [127], large-batch training of Imagenet [118] and natural language problems [247, 76].
Cyclical stepsize schedule means that the stepsize is changing between some lower and upper bounds. There are different modiﬁcation of this technique including gradual decrease and increase during one epoch [230] and gradual decrease of the stepsize followed by the sudden increase [170]. However, the theoretical understanding of the success of “warmup” and cyclical schedules is very limited.
We also discuss different stepsize policies including adaptive ones (Section 4.4), Armijo line-search under expected strong growth assumption and stochastic Polyak stepsizes under relaxed growth assumption (Section 4.2.3) in the following subsections.

4.2.3 Over-Parameterized Models

In Section 2.3.2, we mentioned that over-parameterization [167, 190, 272, 194, 161, 13, 14], meaning that the last layer has more neurons than the number of samples in the training set, is a good property for neural networks from the optimization and generalization [173, 12, 11] point perspectives, but not a panacea: overparameterized neural networks have no spurious valleys, but still can have bad local minima [78].
In the papers, focusing mostly on the optimization aspects of over-parameterized models, it was shown that SGD converges with the same (up to the difference in the smoothness constants) rate as GD in terms of the iteration complexity in convex and strongly convex cases [248, 249, 168] under interpolation condition: for the ﬁnite-sum optimization problem (4)+(8) there exists such point x∗ ∈ Rn that

min fi(x) = fi(x∗) ∀i = 1, . . . , m.

(30)

x∈Rn

Furthermore, in this setting SGD converges with Armijo line-search [249], with stochastic Polyak stepsizes [168], and, if additionally expected strong growth condition (24) holds, SGD can be accelerated [248] and the accelerated version converges as good as Nesterov’s method [183] in terms of iteration complexity up to expected strong growth multiplicative factor α from (24).
In the general non-convex case, the following results exist. Constant stepsizes. In [248], it was shown that SGD with constant stepsize h = 1/αL

30

Danilova, Dvurechensky, Gasnikov, Gorbunov, Guminov, Kamzolov, Shibaev

ﬁnds ε-stationary point under expected strong growth condition (24) with the rate O αL( f (x0)− f∗)/ε2 matching the iteration complexity of GD up to the factor α. Armijo line-search. The idea that under interpolation condition/expected strong growth condition SGD and GD have similar properties was then strengthen in [249], where authors showed that SGD with Armijo line-search converges in these settings. In particular, the authors of [249] considered such stepsizes hk that

fik (xk − hk∇ fik (xk)) ≤ fik (xk) − chk ∇ fik (xk) 22,

(31)

where the index ik is sampled uniformly at random from the set {1, . . . , m}, the stochastic gradient gk is deﬁned as gk = ∇ fik (xk), and c > 0 is a hyper-parameter. Moreover, it is assumed that hk ∈ (0, hmax] for all k ≥ 0. Then SGD with Armijo line-search (31) with c > 1 − Lmax/(αL) and hmax ≤ 2/αL ﬁnds ε-stationary point under expected strong growth condition (24) with the rate O ( f (x0)− f∗)/(δε2) , where δ =
(hmax + 2(1−c)/Lmax) − α hmax − 2L(1m−axc) + Lh2max , Lmax is the maximal smoothness constant of summands fi, and f is the smoothness constant of f . Authors of [249] also considered the version with samples used for backtracking (31) independent
from those used for determining the stochastic gradient, and the version with non-
increasing stepsizes under additional assumption that the iterates lie in some ball with radius D. The rates are O max{Lmax,αL}( f (x0)− f∗)/ε2 and O max{Lmax,αL}LD2/ε2 respectively, and both complexity bounds hold with c = 1/2 and hmax = 1/(αL). Finally, in the numerical experiments from [249] the authors observed that the
method’s performance is robust to the choic of c and hmax. Stochastic Polyak stepsizes. Next, SGD under expected strong growth condition
converges with stochastic Polyak stepsizes introduced and analyzed in [168]:

h = min fik (xk) − fik,∗ , h ,

(32)

k

c ∇ fi (xk) 2 b

k

where the index ik is sampled uniformly at random from the set {1, . . . , m}, the stochastic gradient gk is deﬁned as gk = ∇ fik (xk), fi,∗ is uniform lower bound for fi(x), and c > 0 is a hyper-parameter. In particular, one can show [168] that SGD in
these settings with c > αL/4Lmax and hb ≤ max 2/(αL), hb ﬁnds ε-stationary point under expected strong growth condition (24) with the rate O ( f (x0)− f∗)/(δε2) , where
δ = (hb + β ) − α hb − β + Lh2b , β = min {1/(2cLmax), hb}, and

−(α − 1) + hb =

(α − 1)2 + 4L2αcL(αm+ax1) . 2Lα

4.2.4 Proximal Variants
In the previous subsections, all complexity results rely on the smoothness of the objective function. The natural question arises: is it possible to generalize these results to the non-smooth case? In the recent work [150], the authors give a negative

Recent theoretical advances in non-convex optimization

31

answer to this question for generally non-smooth non-convex functions, i.e., one cannot ﬁnd efﬁciently via ﬁrst-order methods near ε-stationary points. However, many complexity results that we mentioned before and will mention in the following subsections have generalizations to the composite optimization problems:

min {F(x) = f (x) + R(x)} ,
x∈Rn

where the function f is L-smooth, but, possibly, non-convex, while R(x), i.e., com-

posite term/regularizer, is a proper closed convex function which can be non-

smooth. Moreover, function R(x) is often chosen in such a way that the proximal

operator

prox (x) = argmin R(y) + 1 y − x 2

R y∈Rn

2

2

can be easily computed, and to make the solution of the problem satisfy certain

properties, e.g., sparsity; see [46, 63, 19] for the detailed discussion and examples

of regularizers.

In these settings, instead of SGD one can apply prox-SGD deﬁned by the follow-

ing recurrence:

xk+1 = proxhkR(xk − hkgk).

Moreover, to measure the progress of the method the generalized projected stochastic gradient is used: gk = (xk−xk+1)/hk. When the regularizer R(x) is a constant gk = gk. For proximal stochastic methods we say that the iterate xk is ε-stationary point if

E

gk

2 2

≤ ε2.

In [103], it was shown that prox-SGD under uniformly upper-bounded variance
assumption (14) converges with the rate given in (22). However, the analysis from
[103] works only in the large-batch setting, i.e., when batch sizes are of the order O(ε−2). For a long time, there was no analysis establishing the same bound without using O(ε−2) batches, and the problem was recently resolved in [69].

4.2.5 Momentum-SGD
As we already mentioned, SGD is optimal among stochastic ﬁrst-order methods for ﬁnding ε-stationary points under uniformly bounded variance assumption [17]. However, it does not imply that there is no sense in using different methods for such problems. In practice, different additional tricks are applied to improve the convergence of SGD, and, perhaps, the most popular one is momentum [203].
Momentum-SGD/Heavy Ball SGD can be written in different forms. Usually it is written as
mk+1 = βkmk + g(xk), xk+1 = xk − hkg(xk),

32

Danilova, Dvurechensky, Gasnikov, Gorbunov, Guminov, Kamzolov, Shibaev

where parameter βk ∈ [0, 1) is called momentum parameter. In the convex and strongly convex cases this method has some advantages in comparison to SGD like better last-iterate convergence guarantees [241, 242, 218], but does not have an accelerated rate [144]. In the non-convex case, Momentum-SGD has the same complexity guarantee (22) as SGD under uniformly bounded variance assumption [266, 70]. However, in practice, Momentum-SGD often works much better than SGD especially on computer vision problems [238], and also navigates ravines and escapes saddle points better than SGD.
Among other works on Momentum-SGD we emphasize the recent paper [70] establishing the tight convergence rates for Momentum-SGD in Stochastic Primal Averaging [241] form via Lyapunov functions analysis. In particular, [70] justiﬁes (theoretically and/or empirically) the following important insights about the behavior of Momentum-SGD: (i) Momentum-SGD is provably better than SGD during the early stage of the convergence, (ii) it is better to gradually reduce momentum parameter βk rather than the stepsize hk, and (iii) gradual changes of the parameters of Momentum-SGD are preferable than sudden changes.

4.2.6 Random Reshufﬂing
Before this subsection, we always assumed that stochastic gradients are sampled independently from previous iterations. However, in the context of ﬁnite sum optimization (4)+(8), the different sampling strategy called Random Reshufﬂing (or SGD with Without Replacement sampling) is often used: at each epoch (pass through the dataset) random permutation {i1, i2, . . . , im} of the set {1, 2, . . . , m} is generated deﬁning the order of gradients computations (see Algorithm 4). This strategy implies that stochastic gradient in RR is biased.

Algorithm 4 Random Reshufﬂing (RR)

Require: learning rates {hs,k}s,k≥0, starting point x0 ∈ Rn, batch size r ≥ 1, number of epochs S Set x00 = x0 for s = 0, 1, 2, . . .K − 1 do

Generate random permutation {is,1, . . ., is,m} of the set {1, . . ., m}

Set l = ⌈m/r⌉

for k = 0, 1, . . ., l − 1 do

Set rˆsk = min{r, m − kr}

rsk

Compute

gks

=

1 rk

∑ ∇ fis,kr+ j (xks )

s j=1

xks+1 = xks − hs,kgks

end for

x0s+1 = xls

end for

return xlK−1

Recent theoretical advances in non-convex optimization

33

While the superiority of RR to SGD was empirically discovered a long time ago [34, 36], the theoretical justiﬁcation of this phenomenon was developed only recently [125, 205, 193, 179]. In particular, authors of [193] proved that RR under uniformly bounded gradients assumption,

fi(x) 2 ≤ G ∀i = 1, . . . , m, ∀x ∈ Rn,

ﬁnds ε-stationary point with the rate O Lmaxm( f (x0) − f∗) ε−2 + Gε−3 , where Lmax is the maximal smoothness constant of summands f1, . . . , fm. Then, in [179] this result was generalized and tightened: under the assumption

∑ 1 m ∇ fi(x) − ∇ f (x) 2 ≤ 2A ( f (x) − f∗) + C,

(33)

m i=1

2

which is a special case of (23) with B = 1, authors of [179] derived the following

bound:

√

√

√

m A( f (x0) − f∗) + C

O Lmax m( f (x0) − f∗) ε2 +

ε3

. (34)

That is, under uniformly bound√ed variance assum√ption (14) this bound transforms (A = 0, C = σ 2) into O Lmax m( f (x0) − f∗) mε−2 + σ ε−3 wh√ich outperforms the corresponding complexity bound for SGD (22) whenever Lmax mε ≤ Lσ .

Next, one can show that for Lmax-smooth fi uniformly lower bounded by fi,∗,

i

=

1, . . . , m,

(33)

holds

with

A

=

Lmax

and

C

=

2Lmax∆∗

=

2Lmax m

∑mi=1( f∗

−

fi,∗ ),

and, as a consequence of (34), RR converges with the rate

√

√ m

O Lmax m( f (x0) − f∗) ε2 +

Lmax( f (x0) − f (x∗)) + √Lmax∆∗ ε3

w√hich is better th√an corres√ponding bound for SGD (see Table 1) when L f (x0) − f∗ ≥ ε Lmaxm and L ∆∗ ≥ ε Lmaxm.

4.3 Variance-reduced Methods
In this section, we discuss variance reduction for non-convex optimization – a special technique aimed at improving the convergence speed of SGD for ﬁnite-sum optimization problems (4)+(8). The typical behaviour of SGD with constant stepsize h and batch size r < m is as following: during the ﬁrst iterations the method converges rapidly to some neighbourhood of the solution or local minimum and then it starts to oscillate in this neighbourhood. Such oscillations of SGD are common even for strongly convex problems meaning that it is not a drawback of the problem. The size of the oscillation region is proportional to hσ2/r and this fact hints two

34

Danilova, Dvurechensky, Gasnikov, Gorbunov, Guminov, Kamzolov, Shibaev

simple and famous remedies: decreasing (gradually or suddenly) or small stepsizes

and large enough batch sizes. However, the ﬁrst option can make the convergence

too slow and the second option dramatically increases the iteration cost.

To remove these drawbacks one can apply variance-reduced methods like SAG

[215], SAGA [71], SVRG [139], Finito [73], MISO [174]. In particular, all of the

mentioned methods have O

(m

+

L/µ

)

ln

1 ε

convergence rate in the µ-strongly con-

vex case. What is more, they use constant stepsize and at each iteration (besides each

m-th iteration or besides the ﬁrst one) they require one computation of the stochastic

gradient with batch size r = 1 in the strongly convex case.

Among variance-reduced methods SAGA and SVRG are the most popular ones

(see Algorithm 5 and 6). In previous subsections, we already mentioned that to ﬁnd

Algorithm 5 SAGA [71, 208]

Require: learning rate h > 0, starting point x0 ∈ Rn, batch size r ≥ 1

Set φ 0j = x0 for each j ∈ [m]

m

v0 =

1 m

∑

∇

f

i

(φ

0 j

)

i=1

for k = 0, 1, 2, . . . do

Uniformly randomly pick sets Ik, Jk from {1, 2, . . ., m} (with replacement) such that |Ik| =

|Jk| = r

gk

=

1 r

∑

∇

f

i

(xk

)

−

∇

f

i

(φ

k i

)

+ vk

i∈Ik

xk+1 = xk − hgk

φ kj +1

= xk

for

j ∈ Jk

and

φ kj +1

=

φ

k j

for

j

∈ Jk

vk+1

=

vk

−

1 r

∑

j∈Jk

end for

∇

f

j

(φ

k j

)

−

∇

f

j

(φ

k j

+

1

)

Algorithm 6 SVRG [139, 208]

Require: learning rate h > 0, epoch length T , starting point x0 ∈ Rn, batch size r ≥ 1

φ0 = x00 = x0 for s = 0, 1, 2, . . . do

for k = 0, 1, 2, . . ., T − 1 do

Uniformly randomly pick set Ik from {1, . . ., m} (with replacement) such that |Ik| = r

gk

=

1 r

∑

∇ fi(xks ) − ∇ fi(φs) + ∇ f (φs)

i∈Ik

xks+1 = xks − hgk

end for

φs+1 = x0s+1 = xks

end for

ε-stationary GD and SGD require6 O mε−2 and O(ε−4) calculations of the gradients of the summands respectively. Despite the fact that SAGA and SVRG were initially analysed only in strongly convex cases, now their convergence in non-convex
6 For simplicity we neglect all parameters except m and ε, see the details in Table 2

Recent theoretical advances in non-convex optimization

35

case is also well-known due to [208, 206]. Unfortunately, when r = 1 both SAGA and SVRG guarantee only O(mε−2) convergence rate as simple GD. However, if r = m2/3, then SAGA and SVRG converges with the rate O(m2/3ε−2) which has m1/3 timHesowbeetvteerr,dtehpeenlodwenerceboonunmd tihsaΩn th√e mcoεm−p2 le[x9it0y, b1o6u3n]danfodr tGheDre. exist optimal al-
gorithms. Essentially, these methods are variations of SARAH [192]. However, in
the original paper on SARAH for non-convex problems authors did not prove com-
plexity bounds for the ﬁnite-sum optimization problems. After that, in [90] authors proposed the ﬁrst lower bounds in the small data regime m = O(L2( f (x0) − f ∗)ε−4)
together with the ﬁrst optimal method called SPIDER. Despite the theoretical optimality of the method, it requires very small stepsize (proportional to ε−1) that leads
to the poor behaviour in practice. Moreover, the original proof of the convergence
rate for SPIDER is technically tough and, because of it, it is hard to generalize the
method for the composite optimization problems. In recent works [252, 253], much
simpler optimal method called SpiderBoost was proposed (see Algorithm 7). Moreover, this method works with big constant stepsizes (of order L−1), can be easily
generalized for the composite optimization problems, and works well with heavy-
ball momentum.

Algorithm 7 SpiderBoost [252, 253]

Require: learning rate h > 0, epoch length T , starting point x0 ∈ Rn, batch size r ≥ 1, number of

iterations K

for k = 0, 1, 2, . . . do

if k mod T = 0 then

Compute gk = ∇ f (xk)

else

Uniformly randomly pick set Ik from {1, . . ., m} (with replacement) such that |Ik| = r

Compute gk =

1 r

∑

∇ fi(xk) − ∇ fi(xk−1) + gk−1

i∈Ik

end if

xk+1 = xk − hgk

end for

Pick ξ uniformly at random from {0, . . ., K − 1} return xξ

Next, in [163], the same lower bound Ω √mε−2 was derived without any assumptions on m. Furthermore, authors of [163] proposed a new optimal method called PAGE (see Algorithm 8) which is a variant of SPIDER with random length of the inner loop making the method easier to analyze.
However, in deep neural networks training, variance-reduced methods work typically worse than SGD or SGD with momentum [72]. This happens often due to the bad behaviour of variance-reduced methods with several widespread in deep learning tricks like batch normalization, data augmentation and dropout (see the details in [72]). Moreover, if the model is over-parameterized or, in particular, expected strong growth condition (24) or its relaxed version (25) with small noise level hold, SGD is as fast as GD in terms of iteration complexity, meaning that variance reduction is

36

Danilova, Dvurechensky, Gasnikov, Gorbunov, Guminov, Kamzolov, Shibaev

Algorithm 8 ProbAbilistic Gradient Estimator (PAGE) Algorithm [163]

Require: initial point x0, stepsize h, minibatch size r, r′ < r, probabilities {pk}k≥0 ∈ (0, 1] of

large-batch stochastic gradient computation, number of iterations K

g0 =

1 r

∑

∇ fi(x0), where I0 denotes indices in the minibatch, |I0| = r

i∈I0

for k = 0, 1, 2, . . ., K − 1 do

xk+1 = xk − hgk

 1r ∑ ∇ fi(xk+1)

gk+1

=

i∈Ik
gk +

1

∑

∇ f (xk+1) − ∇ f (xk)



r′ i∈I′ i

i

k

with probability pk, with probability 1 − pk, where |Ik| = r, |Ik′ | = r′

end for

return xˆK chosen uniformly from {xk}Kk=0

superﬂuous. That is, variance reduction trick is often not needed or gives worse rates than the rate of SGD for over-parameterized models from theoretical and practical perspectives. Nevertheless, when the problem is not over-parameterized, it makes sense to use variance-reduced methods.
We summarize the discussed above complexity bounds in Table 2. We also want

Method Lower bound
GD SGD, bounded var. SGD, unbounded var. SVRG, r = 1 SVRG, r = ⌈m2/3⌉ SAGA, r = 1 SAGA, r = ⌈m2/3⌉ SpiderBoost
SpiderBoost-M SPIDER PAGE

Citation [90, 163]
[101]
[141]
[208] [208] [208] [208] [252, 253] [253] [90] [163]

Complexi√ty L∆0 min{σ ε−3, mε−2}
mL∆0 ε −2
L∆0 max{ε−2, σ 2ε−4}
L2ε∆4 0 max {∆0, ∆∗}
mL∆0 ε −2 m2/3L∆0 ε −2 mL∆0 ε −2 m2/3L∆0 ε −2 m1/2L∆0 ε −2 m1/2L∆0 ε −√2 L∆0 min{σ ε−3, √mε−2} L∆0 min{σ ε−3, mε−2}

Table 2: Overview of the complexity results for different variance-reduced methods

applied to solve problem (4)+(8) with L-smooth summands. The column “Com-

plexity” contains an overall number of stochastic ﬁrst-order oracle calls needed

to ﬁnd ε-stationary point neglecting constant factors. Notation: ∆0 = f (x0) − f∗,

∆∗

=

1 m

∑mi=1( f∗

−

fi,∗),

σ2

=

a

uniform

bound

for

the

variance

of

the

stochastic

gradient (14) (can be ∞ for variance-reduced methods), r = batch size.

to mention some papers not presented in Table 2 but being highly relevant. In [164],

Recent theoretical advances in non-convex optimization

37

there was developed the generalization of the approach from [141] providing a uniﬁed analysis of different variants of SGD, non-optimal variance-reduced methods like SAGA or L-SVRG [129, 151], and some distributed methods with quantization [5] including DIANA-type variance reduction [178, 130] for non-convex optimization. Next, for the online case (4)+(7) with smooth stochastic trajectories the optimal rate O(ε−3) was shown for STOchastic Recursive Momentum (STORM) method [67], which does not require periodical large-batch stochastic gradient computations and is more robust to the parameters selection, and for its proximal variant [264]. These results shade a light on the role of momentum in the stochastic ﬁrstorder methods. Finally, it is optimal to generalize SPIDER and get similar rates for composition optimization problems [276, 61].

4.3.1 Convex and Weakly Convex Sums of Non-Convex Functions

There are also several results devoted to the case when the objective function f from (8) is (strongly) convex or almost convex, while the summands fi are smooth, but can be non-convex. In particular, [283] establishes the lower bounds for the cases when (i) f is µ-strongly convex with µ ≥ 0, (ii) f is α-weakly convex

f (x) − f (y) − ∇ f (y), x − y ≥ − α x − y 2,

2

2

and (iii) fi are α-weakly convex. Due to the additional assumptions on the structure of non-convexity in the problem the proposed lower bounds are tighter in these situations than the lower bound from [90, 163]. The lower bounds for the case (i) were further tightened in [260]. Moreover, there exist optimal and almost optimal methods for each case, see Table 3 for the details.

4.4 Adaptive Methods
One of the most signiﬁcant issues of the methods described above is that they require tuning of the stepsize and other parameters (e.g., batch size) when used in practice. It is often challenging and takes a lot of time, especially for training deep neural networks. That is why, in the recent few years, adaptive methods gained a lot of attention. Below we discuss the most popular ones – AdaGrad and Adam – as well as their variants. In fact, all of these methods depend on some parameters, but these algorithms are much more robust than other variants of SGD or variancereduced methods. Therefore, they are often called adaptive. One can ﬁnd PyTorch implementation of many popular adaptive ﬁrst-order methods together with with visualization of their convergence on Rosenbrock and Rastrigin functions in [1].

38

Danilova, Dvurechensky, Gasnikov, Gorbunov, Guminov, Kamzolov, Shibaev

Settings

Lower Bound

Upper Bound, Methods

f is µ-str. cvx. and L-smooth, { fi} are average L-smooth

(m + m3/4

L µ

)

log

∆0 ε

,

[260]

(m + m3/4

L µ

)

log

∆0 ε

,

Dual-Free SDCA [220],

KatyushaX [8]

f is cvx.

m + m3/4 LR20 ,

m + m3/4 LR20 ,

and L-smooth,

ε

ε

Dual-Free SDCA [220],

{ fi} are average L-smooth

[283]

KatyushaX [8]

∆0 min m3/4√αL, √mL , ∆0 min m3/4√αL, √mL ,

f is α-weakly cvx.

ε2

ε2

and L-smooth,

RepeatSVRG [49, 2],

{ fi}mi=1 are average L-smooth

[283]

SPIDER [90],

SNV√RG

[286] √

∆0 min

√ mαL, L

,

ε2

∆0 min
ε2

mαL, mL ,

{ fi}mi=1 are α-weakly cvx.

Natasha [6],

and L-smooth

[283]

RapGrad [156],

StagewiseKatyusha [60]

Table 3: Overview of the optimal convergence results for convex and weakly convex

sums of non-convex functions. Averaged L-smoothness of { fi}mi=1 means that for all

x, y

∈

Rn

the

following

inequality

holds:

1 m

∑mi=1

∇ fi(x) − ∇ fi(y)

2 2

≤

L2

x−y

22.

The column “Lower Bound” states for the number of stochastic ﬁrst-order oracle

calls needed to ﬁnd such xˆ that E[ f (xˆ) − f (x∗)] ≤ ε for the second and the third

rows and ε-stationary point for the fourth and the ﬁfth rows. Notation: R0 = distance

from x0 to the solutions set (for the third row), ∆0 = f (x0) − f∗.

4.4.1 AdaGrad and Adam

AdaGrad. As we mentioned above, SGD requires the tuning of the stepsize. The ﬁrst algorithm aiming to remove this drawback of SGD was AdaGrad [79]:

xki +1 = xki −

h gki , Gki + δ

where the subscript i denotes the i-th component of the vector, Gki = ∑tk=0(gti)2, and δ is some small positive number preventing from the division by zero and typically taken of the order 10−8. AdaGrad can be considered as a special case of SGD with
different per-coordinate stepsizes.
The main advantage of AdaGrad is in its robustness to the choice of h: in practice, it often works well with the default value h = 10−2. Moreover, AdaGrad was shown to work well with sparse data [80]. However, in the dense settings AdaGrad stepsizes
rapidly decrease which leads to the slow convergence of the method [257].

Recent theoretical advances in non-convex optimization

39

Adam. To resolve this issue of AdaGrad one can use exponential moving averages
instead of sums Gki leading to the method called RMSprop [243]. Then, based on RMSprop authors of [146] proposed one the most popular methods in deep learning – Adam7:

mki = β1mki −1 + (1 − β1)gki ,

mˆ k = mki , i 1 − (β1)k

vki = β2vki −1 + (1 − β2)(gki )2,

vˆk = vki , i 1 − (β2)2

xki +1 = xki −

h mˆ ki , vˆki + δ

i = 1, . . . , n,

δ is some small positive number preventing from the division by zero and typically taken of the order 10−8. Default values β1 = 0.9 and β2 = 0.999 from the original paper [146] often make Adam work well in practice. Adam was initially analyzed in the online convex case, but then authors of [207] found out the ﬂaw in the proof for Adam and proposed a convergent variant of Adam called AMSGrad. Convergence Guarantees. While the superiority of AdaGrad and Adam in comparison to SGD was noticed in many application [80, 154, 106], the best-known complexity bounds for AdaGrad, Adam, and their modiﬁcations are the same or even worse than ones for SGD [57, 285, 270, 256, 74]. Furthermore, these complexity results in non-convex case under more restrictive assumption, e.g., uniformly bounded second moment of the stochastic gradient, than their counterparts for SGD. Among other works providing complexity results for Adam and AdaGrad in the non-convex case we emphasize [74] because of the generality and the simplicity of the proofs. Moreover, the uniﬁed analysis of proximal variants of AdaGrad and Adam was proposed in [269]. Furthermore, we emphasize the recent work [225] where authors analyse RMSprop without assuming uniform boundedness of the gradients.
Next, in [275] the theoretical and empirical study why Adam sometimes behaves signiﬁcantly better than SGD was conducted. The authors of [275] empirically discovered that Adam performs better than SGD when stochastic gradients are heavy-tailed and the reason is that Adam does an “adaptive gradient clipping” [107, 109, 177, 200, 246, 110]. In the same work [275] authors showed that in such situations SGD can fail to converge while clipped-SGD (with general and coordinate-wise clipping operators) provably converges to ε-stationary point. Moreover, in [274] it was shown that Gradient Descent with clipping converges even under weaker assumption than L-smoothness in the non-convex case with the rate ∼ ε−2 while Gradient Descent in the same settings can converge arbitrary slower. Then, the bound from [274] was improved in [271]. Finally, it is known [107] that clipped-SGD works better than SGD in the vicinity of extremely steep cliffs. A very similar approach based on the normalization of Gradient Descent was also studied in [126, 160].

7 To distinguish exponents from superindexes we use braces (·) for exponents.

40

Danilova, Dvurechensky, Gasnikov, Gorbunov, Guminov, Kamzolov, Shibaev

4.4.2 Adaptive SGD

The approach described in Section 4.1.2 for general stochastic optimization prob-
lem (4) with the objective given as (7) was recently extended in [81] to obtain adap-
tive methods with Armijo-type line-search for stochastic non-convex optimization.
To do that they consider Algorithm 3 with the mini-batch stochastic gradient (13) and mini-batch size r = max{1, 8σ02/ε2}, where σ0 σ . In each iteration k of Algorithm 3 the stepsize is taken as hk = 1/Lk := 1/(2ik−1Lk−1) by increasing ik 0 until the inequality

f (xk+1) ≤ f (xk) +

∑ 1r r ∇ f (x, ξl ), xk+1 − xk l=1

+ L xk+1 − xk 2 + ε2

k

2 32Lk

is satisﬁed. This inequality is an inexact upper quadratic bound which follows for sufﬁciently large Lk from the L-smoothness and bounded variance. Thus, Lk plays the role of a guess of the Lipschitz constant L locally between the points xk and xk+1. The authors of [81] propose also methods for convex problems based on the same idea with the difference that in the convex case the mini-batch size r depends on the iteration counter k. Careful choice of this dependence allows to simultaneously adaptively choose both the stepsize hk and the mini-batch size rk. These methods have the same, up to logarithmic factors, iteration complexity and total number of stochastic oracle calls as their non-adaptive counterparts. In particular, for the nonconvex case the iteration complexity to obtain ε-stationary point is O L( f (x0)− f∗)/ε2 and the oracle complexity is O L f (x0) − f∗ max 1/ε2, σ2/ε4 . Moreover, empirically, the methods designed for convex problems turned out to be more efﬁcient on non-convex problems than the method designed for non-convex problems.

5 First-Order Methods under Additional Assumptions
In the previous parts of the paper, we focused on general non-convex problems. In this section, we consider two subclasses of non-convex objective functions which satisfy assumptions weaker than convexity and, at the same time, strong enough to obtain good global convergence rates of optimization algorithms. For simplicity, we consider an unconstrained optimization problem (1) with Q = Rn.

5.1 Polyak–Łojasiewicz Condition
A function f (x) is said to satisfy the Polyak–Łojasiewicz (PŁ) condition [201, 169] (or to be gradient dominated) if for all x ∈ Rn

Recent theoretical advances in non-convex optimization

41

f (x) − f (x∗) 21µ ∇ f (x) 22 . (36)

This condition implies that any stationary point of f (x) is a global minimum, although it is not necessarily unique. In particular, this property holds for strongly convex functions. It was ﬁrst shown in [201] that if the objective is also L-smooth, then gradient descent linearly converges to a global minimum, i.e.,

f (xk) − f (x∗)

exp − µ k L

f x0 − f (x∗) .

The Polyak–Łojasiewicz condition is naturally satisﬁed for the problems of solving nonlinear systems of equalities g(x) = 0, where g(x) is a vector-valued function. This problem can be equivalently reformulated as

min f (x) = 1 g (x) 2 .

x∈Rn

2

2

Assuming that, for all x ∈ Rn

λmin Jg(x)JgT (x) ≥ µ > 0,

where Jg(x) is the Jacobian matrix of g(x), one can show that

∇ f (x) 2 = JgT (x)g(x) 2 µ g(x) 2 = 2µ f (x),
which is exactly the Polyak–Łojasiewicz condition since g(x∗) = 0. An extensive survey of ﬁrst-order optimization methods under this condition, as well as its relationship with other classes of functions, can be found in [140]. An interesting example of the emergence of PŁ condition in Linear Feedback Control theory was recently described in [92] and in over-parameterized deep learning in [?].
Next, consider the convergence of gradient descent under the PŁ condition in terms of relative accuracy ∇ f (x)

∇ f (x) − ∇ f (x) 2 ≤ α ∇ f (x) 2, where α ∈ [0, 1). Let the stepsize h in gradient descent

xk+1 = xk − h∇ f (xk)

be computed using the following formula: 1 1−α
h = L (1 + α)2 . Combining this with the Lipschitz condition, we obtain

42

Danilova, Dvurechensky, Gasnikov, Gorbunov, Guminov, Kamzolov, Shibaev

f (xk+1) ≤ f (xk) − 1 (1 − α)2 ∇ f (xk) 2,

2L (1 + α)2

2

leading to

f (xN ) − f (x∗) ≤ 1 − µ (1 − α)2 N f (x0) − f (x∗) . L (1 + α)2

As a result, we achieve a linear convergence rate for the gradient descent under the PŁ condition.
In general case the main ingredient that guaranties global linear convergence under PŁ condition is an estimate like

∇ f (xN )

2 2

≤

θ

(N)

·

f (x0) − f (x∗)

,

where θ (N) – some decreasing function, i.e. (2). We assume that there exists such N(µ), that θ (N(µ)) ≤ µ, i.e. for (2) N(µ) = 2L/µ. In this case from PŁ condition

∇ f (xN (µ)) 2 ≤ 1 ∇ f (x0) 2.

22

2

By applying restarts we obtain oracle complexity O˜ (N(µ)).

5.1.1 Stochastic First-Order Methods under Polyak–Łojasiewicz Condition

The majority of the methods described in Section 4 are analyzed under PŁ condition as well. That is, one can ﬁnd the state-of-the-art results for different variants of SGD and non-accelerated variance reduced methods like SVRG and SAGA in [164], accelerated variance reduced methods like PAGE in [163], the tightest known analysis of Random Reshufﬂing under PŁ condition in [3], and the convergence results for SGD in the over-parameterized case with constant, Armijo-type, and stochastic Polyak’s stepsizes in [248], [249], and [168] respectively. The summary of known complexity results for the stochastic methods under PŁ condition is given in Table 4. We emphasize that the analysis from [116] is derived under so-called expected residual (ER) assumption on the stochastic gradient g(x): there exists such constant ρ > 0 that

E

g(x) − g(x∗) − (∇ f (x) − f (x∗))

2 2

≤ 2ρ ( f (x) − f (x∗)) .

(37)

Moreover, in the analysis of Random Reshufﬂing from [3] it is used that the norms of the gradients of individual functions from the sum (8) are uniformly upper by some constant G on the sublevel set:

∇ fi(x) 2 ≤ G, ∀i = 1, . . . , m, ∀x ∈ Rn : f (x) ≤ f (x0).

(38)

Recent theoretical advances in non-convex optimization

43

Rather simple introduction (close to the state of the art results) for SGD with bias under PŁ condition can be ﬁnd in [4].

5.2 Star-convexity and α-weak-quasi-convexity

A function f (x) is called star-convex if for some global minimizer x∗ and for all λ ∈ [0, 1] and x ∈ Rn
f (λ x + (1 − λ )x∗) λ f (x) + (1 − λ ) f (x∗).

While any interval connecting two points on the graph of a convex function lies not lower than the graph, for a star-convex functions this is assumed only for intervals connecting some ﬁxed global minimizer and any other point on the graph. This condition is considerably weaker than convexity, even for functions of one variable. For example, the function |x|(1 − e−|x|) is a non-convex star-convex function. The authors of [158] analyze a cutting plane method for minimization of this class of functions and obtain a polylogarithmic in ε and polynomial in n complexity bound using only function evaluations. The authors of [122, 187] prove that the same Algorithm 1 possesses the following convergence rate for star-convex L-smooth functions

min

∇ f (yk)

2 ∗

k=[N/2],...,N

64L2V [x0](x∗)

N3

,

f (xN ) − f (x∗)

4LV [x0](x∗) N2 .

A more general class of functions is the class of α-weakly-quasi-convex func-

tions satisfying

f (x) − f (x∗)

1 ∇ f (x) , x − x∗ α

for some α ∈ (0, 1] and some global minimizer x∗. Continuously differentiable

1-weakly-quasi-convex functions are exactly the star-convex functions. The authors of [121] propose an algorithm with iteration complexity O(α−1L1/2Rε−1/2), where R is an upper bound on the initial distance to the point x∗. A slightly worse
bound O(α−3/2L1/2Rε−1/2) is obtained in [187] by restarting Algorithm 1. Both

approaches require a line search for which the complexity also needs to be esti-

mated. The authors of [128] analyze this complexity and propose an algorithm with
O(α−1L1/2Rε−1/2) iteration complexity and the same up to a logarithmic factor in α−1ε−1 number of function and gradient evaluations. Moreover, they provide a

similar lower complexity bound, thus proving that their method is optimal. Further, they also consider a class of (α, µ)-strongly quasi-convex functions satisfying

f (x) − f (x∗)

1 ∇ f (x) , x − x∗ − µ x − x∗ 2

α

2

44

Danilova, Dvurechensky, Gasnikov, Gorbunov, Guminov, Kamzolov, Shibaev

Problem (4)
(4)+(7)

Method GD SGD
PAGE

Citation [201] [141, 140] [248, 141] [163]

Complexity

L µ

log

∆0 ε

L µ

log

∆0 ε

+

Lσ 2 µ2ε

αL µ

log

∆0 ε

+

Lβ µ2ε

σ2 µε

+

σ 2 Lavg µε µ

log

∆0 ε

(4)+(8)

GD [201]

m

L µ

log

∆0 ε

[141]

L µ

A µ

+

B

log

∆0 ε

+

C µε

SGD

[248, 141]

[141]

L µ

αL µ

log

∆0 ε

+

Lβ µ2ε

maxi µ

Li

log

∆0 ε

+

maxi Li∆∗ µε

[141]

L µ

L µ

log

∆0 ε

+

L∆∗ µε

[116]

L µ

ρ µ

+1

log

∆0 ε

+

σ∗2 µε

+ Armijo [249] line-search

αL µ

+

maxi µ

Li

log

∆0 ε

+ Polyak [168] stepsizes

maxi L2i log ∆0

µ2

ε

RR SVRG

[3] [206, 208]

mε∆0 + mL2G2µlo3gε3(ε−1) 1/2

m + m2/3 maxi Li log ∆0

µ

ε

L-SVRG SAGA [164, 208] PAGE [163]

m

+

m2/3 L µ

log

∆0 ε

b

+

√ b

Lavg

log

∆0

,

µ

ε

where

b

=

min{

σ2 µε

,

m}

Assumptions
UV (14) RG (25) UV (14), Avg. Lavg-smth.
ES (23) RG (25) Unif. sampl. Imp. sampl. ER (37) E-SG (24) Interpolation (30)
Bounded gradients (38)
Avg. Lavg-smth. UV (14)
with σ 2 ≤ +∞, Avg. Lavg-smth.

Table 4: Summary of the state-of-the-art complexity results for different stochastic

ﬁrst-order methods under assumption that f is L-smooth and satisﬁes PŁ condition

(36). Columns: “Complexity” – an overall number of stochastic ﬁrst-order oracle calls needed to ﬁnd such xˆ that E[ f (xˆ) − f (x∗)] ≤ neglecting constant factors; “As-

sumptions” – the assumptions used to derive the corresponding complexity bound

in addition to L-smoothness of f and PŁ condition (36) for f . For ﬁnite-sum case

(8) it is additionally assumed that each fi is Li-smooth, i = 1, . . . , m. Abbreviations:

UV – uniform variance bound assumption (14); RG – relaxed growth condition

(25); Avg. Lavg-smth. – averaged L-smoothness assumption meaning that there ex-

ist such L that E

∇ f (x, ξ ) − ∇ f (y, ξ )

2 2

≤ L2

x−y

2 2

in

the

online

case

(7),

and

E

∇ f j(x) − ∇ f j(y)

2 2

≤ L2

x−y

2 2

in

the

ﬁnite-sum

case,

where

j

is

sampled

uniformly at random from {1, . . . , m}; Unif. Sampl. and Imp. Sampl. denote the

sampling strategies described in Section 4.2.1. Notation: ∆0 = f (x0) − f∗; σ 2 = a

uniform bound for the variance of the stochastic gradient (14); α, β = relaxed growth

condition

parameters;

∆∗

=

1 m

∑mi=1( f∗

−

fi,∗);

maxi Li

=

maximal

smoothness

con-

stant of fi in (8); L = averaged smoothness constant of fi in (8); σ∗2 = E[ g(x∗) 22] –

the variance of the stochastic gradient at the solution.

Recent theoretical advances in non-convex optimization

45

and provide an algorithm which has iteration complexity O(α−1L1/2µ−1/2 log(α−1ε−1))

and requires up to a logarithmic factor the same number function and gradient evaluations. Similar optimal complexity bounds for accelerated gradient method for α-weakly-quasi-convex functions and (α, µ)-strongly quasi-convex functions were obtained in [39] by extending the estimating sequence technique.

5.2.1 Stochastic Methods and α-weak-quasi-convexity

The most general analysis of SGD under α-weak-quasi-convexity is provided in [116]. As it was mentioned earlier, authors of [116] consider ﬁnite-sum optimization problems8 (4)+(8) and derive complexity bounds for SGD under expected residual (37) assumption on the stochastic gradient for the α-weak-quasi-convex function and functions satisfying PŁ condition. In particular, for SGD in these settings the
following bound was established:

O (ρ + L)R20 + σ∗2R20 ,

α2ε

α2ε2

where σ∗2 = E[ g(x∗) 22] is the variance of the stochastic gradient at the solution. Note, that when interpolation condition (30) holds this bounds reduces to O (ρ+L)R20/(α2ε) . Moreover, under interpolation condition the authors of [116] also
derived that the generalized version of stochastic Polyak stepsize (32) for stochasti-
cally reformulated problem (4)+(8) converges with the rate

O L R20 , α2ε

where L is the expected smoothness constant of stochastic reformulation (see the

details in [117, 116]). In the full-batch case, i.e., when g(x) = ∇ f (x), we have L =

L, and in the importance sampling case, i.e., when g(x) = ∇ f j(x) where j = i with

probability Li/∑tm=1 Lt ,

we

have L

=L=

1 m

∑mi=1

Li.

5.2.2 Further Generalizations
A more wide class of functions that covers the class of α-weakly-quasi-convex functions referred to as approximately homogeneous functions satisfying the condition
N( f (x) − f (x∗)) ∂ f (x) , x − x∗ M( f (x) − f (x∗)),

8 In fact, most of the results from [116] do not rely on the ﬁnite-sum structure of f .

46

Danilova, Dvurechensky, Gasnikov, Gorbunov, Guminov, Kamzolov, Shibaev

where ∂ f (x) is a subgradient of f (x) and N, M are some constants. This class of functions was ﬁrst deﬁned in [228] and discussed in [202].
In general, if there exist good lower and upper convex models for non-convex target function, one can derive that complexity of such problem is similar to convex ones rather than non-convex (see [21] and references therein).

6 Higher-Order Methods

6.1 Second-Order Methods

Another branch of optimization incremental methods for solving (4) are methods
that use the second-order information about the function. This information is very
helpful to escape saddle-points by using a negative curvature. Next we deﬁne an (ε, δ )-second-order stationary point x∗ if

∇ f (x∗) 2 ≤ ε, λmin ∇2 f (x∗) ≥ −δ .

Next in this section we suppose that f (x) has L2-Lipschitz second-order derivative. The basic method for this class of problems is a Cubic Regularization method (CR) [188].

xk+1 = xk + argmin ∇ f (xk)⊤s + 1 s⊤∇2 f (xk)s + H s 3 ,

(39)

Cubic

s∈Rn

2

62

where H ≥ 0. It globally converges to the minimum for convex functions and converges to a (ε, δ )-second-order stationary point for non-convex function within O(ε−3/2) number of iterations. Note, that the subproblem (39) is also non-convex but in [188] authors proposed a method to solve this problem as a convex problem
via special choose of H and line-search for a dual problem. A related line of work considers trust region methods [64, 52, 54, 55, 53], where a classical Newton step is
calculated on a Euclidean ball of a carefully chosen radius. Both cubic regularized
Newton methods and trust region methods can be extended to work for constrained problems with linear and conic constraints [124, 86, 85]. In general, all these al-
gorithms work well for the problems in moderate dimensions. Unfortunately, for many large-scale Machine Learning problems it is hard to calculate the full Hes-
sian and the inverse such a large matrix. Recent work has therefore explored the use of Hessian-vector products ∇2 f (x) · s, which can be computed as efﬁciently as gradients in many cases including neural networks by using autogradient technique. By this Hessian-vector product we can efﬁciently ﬁnd xkC+ub1ic by variants of gradient descent [47]. Several algorithms incorporating Hessian-vector products [7, 9] have been shown to achieve faster convergence rates than gradient descent in the non-
stochastic setting. However, in the stochastic setting where we only have access to
stochastic Hessian-vector products, signiﬁcantly less progress has been made.

Recent theoretical advances in non-convex optimization

47

One of the improvement of this method was done in [255]. The authors introduce a momentum step and obtain faster convergence rate. This technique is widely used to speed up the ﬁrst order methods and also can speed up the second order method.

Algorithm 9 CRm
1: Input: Initialization x0 = y0 ∈ Rn, ρ < 1, H > L2. 2: for k = 0, 1, . . . do 3: Cubic step:

sk+1 = argmin ∇ f (xk)⊤s + 1 s⊤∇2 f (xk)s + H s 3 ,

s

2

62

yk+1 = xk + sk+1.

4: Momentum step:

βk+1 = min{ρ, ∇ f (yk+1) 2, yk+1 − xk 2}, zk+1 = yk+1 + βk+1(yk+1 − yk).

5: Monotone Step: 6: end for

xk+1 = argmin f (x).
x∈{yk+1 ,zk+1 }

Also, second-order methods that have access to the Hessian of f can exploit negative curvature to more effectively escape saddles and arrive at local minima. To show this concept we introduce one of such methods [258]. There are two types of steps: gradient steps and a step in a negative curvature for the Hessian. So
• If ∇ f (xk) 2 > ε, we do gradient step. • Otherwise, if λmin ∇2 f (xk) < −δ , choose sk to be the eigenvector correspond-
ing to λmin ∇2 f (xk) ) and do step xk+1 = xk + αksk.
There are different policies to αk and gradient steps. The main idea here is to use the ﬁrst-order methods as a cheap main method and switch to expensive secondorder methods when we reach local stationary point and want to escape it to ﬁnd a better local minimum. Methods with this idea are still developing. In [99, 136] it was proved that gradient methods with additive noise are able to escape from nondegenerate saddle points and ﬁnd approximate local minima. These ideas lead to the state of art ﬁrst-order methods to ﬁnd local minima with Hessian-vector product [49, 212, 9, 265, 10, 138, 90, 191]. In recent works [91, 137, 211] it was proved that stochastic gradient descent can escape from saddle point and converges to approximate local minima.

48

Danilova, Dvurechensky, Gasnikov, Gorbunov, Guminov, Kamzolov, Shibaev

6.2 Stochastic Second-Order Methods

Now we move to stochastic version of problem (3). Firstly, we speak about online version (7), where we minimize expectation of some stochastic function. In the work [244] authors propose a stochastic optimization method that utilizes stochastic gradients and Hessian-vector products to ﬁnd an (ε, δ )-second-order stationary point using only O(ε−3.5) oracle evaluations. This rate improves upon the O(ε−4) rate of stochastic gradient descent, and matches the best-known result for ﬁnding local minima without the need for any delicate acceleration or variance reduction techniques.

Algorithm 10 Stochastic Cubic Regularization

Require: mini-batch sizes r1, r2, initialization x0, number of iterations N, and ﬁnal tolerance ε.

1: for k = 0, . . ., N do 2: Sample S1 ← {ξi}ri=1 1, S2 ← {ξi}ri=2 1. 3: gk= r11 ∑ξi∈S1 ∇ f (xk; ξi) 4: Bk[·]= r12 ∑ξi∈S2 ∇2 f (xk, ξi)(·)

5:

sk = argmin

ψk (s)

=

s⊤ gk

+

1 2

s⊤ Bk s

+

L2 6

s

3 2

s

6: xk+1 ← xk + sk

7: end for

Ensure: The ﬁnal iterate xN+1.

This is a stochastic cubic regularization algorithm in Algorithm 10. To obtain
stochastic gradients and Hessians, we can sample independent batches of S1and S2 in each iteration, but they can also be connected so that S2 ⊆ S1. The average gradient is denoted by

∑ gk = 1 ∇ f (xk, ξi)
r1 ξi∈S1

and the average Hessian by

∑ Bk = 1 ∇2 f (xk, ξi),
r2 ξi∈S2

this implies a stochastic cubic submodel:

ψk(s) = s⊤gk + 1 s⊤Bks + L2 s 32.

2

6

This subproblem should be solved by special gradient-based subroutine. It is
written in details in [244]. Since only the gradient is used to solve the subproblem, we need to compute only a Hessian-vector product Bk[s] but not a full Hessian Bk. If our function can be represented by a computational tree, then we can use auto-

Recent theoretical advances in non-convex optimization

49

gradient techniques and compute Hessian-vector products as fast as we compute

gradients up to a small constant.

How many Hessians should we take? By concentration inequalities it is possible

to show that we need

|S2|= r2 = O ε−1 .

So in total, the method converges with O ε−3/2 iterations and O ε−5/2 Hessian calculations of the function.
In paper [16] this approach is improved by using special variance reduction technique. Authors get method that needs only O(ε−3) gradients and Hessian-vector products for ﬁnding second-order stationary point. Also, in this article authors prove lower bounds for higher-order stochastic problems.
What is the main advantage of such methods? We calculate fewer Hessians than in the full CR version and also do it in parallel if we have many cores for computing. The simplicity of the algorithms, both at fast rates and when escaping from saddle-points, leads us to very good optimization methods for non-convex stochastic problems.
Next we go to ofﬂine version that works with sum of functions (8).

f (x) =

1

m
∑

fi(x),

(40)

m i=1

where fi(x) has Lipschitz continuous Hessian. In this regime we have m functions and hence classic CR needs to compute O(mε−3/2) Hessians. To reduce it in papers [149, 262] authors used subsampled gradient and subsampled Hessian, which achieve O˜ (mε−3/2 ∧ ε−7/2) gradient complexity and O˜ (mε−3/2 ∧ ε−5/2) Hessian complexity similarly to the previous section. Next appears many articles with
different stochastic variance-reduced cubic(SVRC) methods. To collect this re-
sults in one place we add a table (see Table 5) with the convergence rates, where a ∧ b = min{a, b}.

Method CR [188] SCR [149, 262] SVRC1 [287] SVRC2 [254, 288] SVRC3 [277] STR [222] SRVRC [284] Lower bound [88]

Gradient O m · ε−3/2 O˜ m · ε−3/2 ∧ ε−7/2 O˜ m4/5 · ε −3/2 O˜ m · ε−3/2 O˜ m · ε−3/2 ∧ m2/3 · ε−5/2 O˜ m · ε−3/2 ∧ m1/2 · ε−2 O˜ m · ε−3/2 ∧ m1/2 · ε−2 ∧ ε−3 Ω m1/4 · ε −3/2

Hessian O m · ε−3/2 O˜ m · ε−3/2 ∧ ε−5/2 O˜ m4/5 · ε −3/2 O˜ m2/3 · ε −3/2 O˜ m2/3 · ε −3/2 O˜ m1/2 · ε−3/2 ∧ ε−2 O˜ m1/2 · ε−3/2 ∧ ε−2 Ω m1/4 · ε −3/2

Table 5: An Overview of the number of computations of gradients and Hessians of functions in (40).

50

Danilova, Dvurechensky, Gasnikov, Gorbunov, Guminov, Kamzolov, Shibaev

As a result, we have a method that not only works efﬁciently with the big sum by utilizing stochastic nature, but also employs Hessian information to escape saddles more effectively and arrive at to better local minimum. This statement is supported by the experiments described in [263, 196, 176, 199]. The authors of these papers experiment with various second-order methods and show how they compete with ﬁrst-order methods without any second-order information in practice. These papers’ main conclusions are that second-order methods ﬁnd deeper local minima and avoid saddle-points. They are more robust when hyperparameters are used. Subsampling speeds up computations and allows for the parallelization of such methods. As a result, second-order methods may be competitive with ﬁrst-order methods in practice.

6.3 Tensor Methods

Next, we present high-order or tensor methods for ﬁnding local minima of a highly smooth and non-convex objective function. High-order derivatives better describe functions and enable you to use curvature to improve convergence.
First, we lay out some standard assumptions about the smoothness of the function f . In the following, we will denote the directional derivative of the function f at x along the directions h j ∈ Rn, j = 1, . . . , p as

∇p f (x)[h1, . . . , hp].

For instance, ∇ f (x)[h] = ∇ f (x)⊤h and ∇2 f (x)[h]2 = h⊤∇2 f (x)h. The functions fi for each p = 0, . . . , 3 has Lp-Lipschitz-continuous derivatives,

∇p fi(x) − ∇p fi(y) 2 ≤ Lp x − y 2

for all x, y ∈ Rn. From this inequality we get next tensor method for p = 3,

xk+1 = xk + argmin ∇ f (xk)[s] + 1 ∇2 f (xk)[s]2 + 1 ∇3 f (xk)[s]3 + H s 4 ,

Tensor

s∈Rn

2

6

4! 2

In papers [29, 51, 50] it was proved that tensor p-order method with Taylor approximation is optimal, match lower bounds, and converges with the rate O(ε−(p+1)/p) for non-convex problems, hence for the third-order methods we get the rate O(ε−4/3) instead of O(ε−3/2) for the second-order methods. So, we get that third-order meth-
ods are faster than second-order methods in terms of iterations.
Another crucial motivation is that the second-order method could get stuck at
the so-called degenerate saddle point, where the Hessian matrix has nonnegative
eigenvalues with some eigenvalues equal to 0 [15].
In paper [289] it is shown how gradient descent and cubic regularization method stuck in such points for even small problems, like f (x, y) = x3 − 3xy2 in degenerate saddle point (0, 0). So, we should use third-order information to escape them.

Recent theoretical advances in non-convex optimization

51

This lead us to the third-order critical point. We deﬁne next critically measures

χ f ,1(xk) = ∇ f (xk) 2, χ f ,2(xk) = max 0, −λmin ∇2 f (xk) , χ f ,3(xk) = max ∇3 f (xk)[y]3 ,
y∈Zk+1
where Zk+1 is the kernel of ∇2 f (xk). Then, we deﬁne x∗ a (ε1, ε2, ε3)-third-order critical point if

χ f ,1(xk) ≤ ε1, χ f ,2(xk) ≤ ε2, χ f ,3(xk) ≤ ε3,

Third-order method converges to a (ε1, ε2, ε3)-third-order critical point with the rate O max ε1−4/3, ε2−2, ε3−4 .
But the calculation of the third-order derivative would be very computationally expensive. This problem leads us to stochastic tensor methods. The main idea of the stochastic method that by different concentration inequalities we can compute much fewer Hessians and third-order derivatives for sum type problems, than gradients. Correct proportions is written in (45). For example, if we have 200000 functions in sum, we may compute full gradient, only 10000 Hessians and 100 third-order derivatives and get the same speed as for full Hessian and full third-order derivatives.
In paper by [171] introduce such method that work with batch tensors and converges as fast as for full-batch methods. The optimization algorithm we consider is detailed in Algorithm 11. This algorithm uses sub-sampled derivatives instead of exact quantities and its implementation relies on tensor-vector products only. The proposed approach is shown to ﬁnd an (ε1, ε2, ε3)-third-order critical point in at most O max ε1−4/3, ε2−2, ε3−4 iterations, thereby matching the rate of deterministic approaches.
We construct an inexact Taylor approximation model and add a fourth-order regularization deﬁned as:

φk(s) = f (xk) + gk[s] + 1 Bk[s]2 + 1 T k[s]3,

2

6

ψk(s) = φk(s) + Hk s 42,

(41)

4

where gk, Bk and T k approximate the derivatives ∇ f (xk), ∇2 f (xk) and ∇3 f (xk) through sampling as follows. Three sample sets Sg, Sb and St are drawn and the
derivatives are then estimated as

∑ ∑ gk = 1 ∇ fi(xk), Bk = 1 ∇2 fi(xk),

|Sg| i∈Sg

|Sb| i∈Sb

∑ T k = 1 ∇3 fi(xk).
|St | i∈St

52

Danilova, Dvurechensky, Gasnikov, Gorbunov, Guminov, Kamzolov, Shibaev

Algorithm 11 Stochastic Tensor Method (STM)
1: Input: Starting point x0 ∈ Rn (e.g x0 = 0) 0 < γ1 < 1 < γ2 < γ3, 1 > η2 > η1 > 0, and H0 > 0, Hmin > 0
2: for k = 0, 1, . . ., until convergence do 3: Sample gradient gk, Hessian Bk and T k such that Eq. (42), Eq. (43) & Eq. (44) hold. 4: Obtain sk by solving ψk(sk) (Eq. (41)). 5: Compute f (xk + sk) and
f (xk) − f (xk + sk) ρk = f (xk) − φk(sk) .

6: Set

xk+1 =

xk + sk xk

if ρk ≥ η1 otherwise.

7: Set 8: end for

 [max{Hmin, γ1Hk}, Hk] Hk+1 = [[γH2kH, γk2, Hγ3kH] k]

if ρk > η2 (very successful iteration) if η2 ≥ ρk ≥ η1 (successful iteration) otherwise (unsuccessful iteration).

It is worth mentioning that the implementation of the algorithm does not re-
quire the computation of the Hessian or the third-order tensor, both of which would
demand signiﬁcant computational resources, but rather directly computes Tensorvector products with a complexity of order O(n).
We will make use of the following condition in order to reach an ε-critical point (where ε = ε1). For a given ε accuracy, one can choose the size of the sample sets Sg, Sb, St for sufﬁciently small κg, κb, κt > 0 such that:

gk − ∇ f (xk) 2 ≤ κgε,

(42)

(Bk − ∇2 f (xk))s 2 ≤ κbε2/3 s 2, ∀s ∈ Rn,

(43)

T k[s]2 − ∇3 f (xk)[s]2 2 ≤ κt ε1/3 s 22, ∀s ∈ Rn.

(44)

In practice, we can choose the size of the sample sets Sg, Sb and St as follows

r = O˜ L20 , r = O˜ L21 , r = O˜ L22 ,

(45)

g

κg2 ε 2

b

κb2ε4/3 t

κt2 ε 2/3

where O˜ hides poly-logarithmic factors and a polynomial dependency to n. We can see that due to the stochastic nature of the data and tensor concentration inequalities, we can use far fewer computations while still achieving the same convergence speed as a full-batch method.
As shown in [88], the lower bounds for sum type problem are still rather far from upper bound even for the second-order methods. Hence, further research in this

Recent theoretical advances in non-convex optimization

53

area may lead to new methods for sum-type problems by using variance reduction techniques. Another branch of possible research is a combination of tensor methods with ﬁrst or second-order methods.

7 Zeroth-Order Methods

Gradient free or zeroth-order optimization methods, which use only function val-

ues, are becoming increasingly important in machine learning problems, especially

in reinforcement learning [175], black-box adversarial attacks on deep neural net-

works [197] and other problems with structure making gradients difﬁcult or infea-

sible to obtain.

While there is a class of methods that does not have any connection to the gradi-

ent, for example, random search algorithms [217] (which are one of the ﬁrst meth-

ods of zeroth-order optimization, beside grid search), the Nelder–Mead algorithm

[181], the model-based methods (see Chapters 2-6 and 10-11 in [65]) or the re-

cent stochastic three points (STP) method [26] and its momentum variant STMP

[113] most zeroth-order optimization methods use gradient estimations, such as

g(x)

=

∑

n i=

1

[

(

f

(

x+

µ

ei

)

−

f

(

x)

)/µ

]

e

i

(where

ei

are

columns

of

n×n

identity

matrix

In,

i ∈ {1, . . . , n}), then for good enough functions ( f ∈ CL1,1 i.e. continuously differ-

entiable with Lipschitz√-continuous gradient) it can be shown, for example, that g(x) − ∇ f (x) 2 µL n. One then can consider some ﬁrst-order optimization

scheme, replace actual gradients with their estimations, and use bounds like this to

return to gradients from estimations in proofs, obtaining the results for the zeroth-

order case relatively easy.

While such deterministic zeroth-order schemes (like the GD with gradient esti-

mation of the same form as above) often suffer from the problem dimensionality

because of the number of oracle calls needed to reconstruct the gradient (n for the

estimation mentioned above, see also [24] for other examples), in a randomized ap-

proach one can use two- or one- point schemes of gradient approximation which

makes every iteration simpler, sometimes leading to better results in terms of oracle

calls [166]. Another beneﬁt of the stochastic approach is that such methods often

have good theoretical properties, for example, the Gaussian smoothing approach

[189] that gives a smoothed version of the initial function, for which the conver-

gence of stochastic zeroth-order algorithm can be easily proved, which can be later

used to show the convergence of the algorithm for the initial function. And there

are setups (for example online learning [40]) where one is limited to use only sev-

eral (or even one) oracle queries thus being unable to construct the full gradient

approximation, so the stochastic approach becomes the only option.

We begin with the formalization of these zeroth-order randomized schemes - we

have a problem with the form

min f (x)
x∈Q⊆Rn

54

Danilova, Dvurechensky, Gasnikov, Gorbunov, Guminov, Kamzolov, Shibaev

then stochastic zeroth-order methods generate {xk} s.t.

xk+1 = A fˆ, X , P, {xi}ki=0, {ui}ki=0

so the procedure A gives us xk+1 based on function values (obtained via oracle fˆ), history of {xk}, random vectors {uk}, and parameters P such as dimension n of X , Lν and ν – Ho¨lder parameters, etc. Function fˆ is not necessarily equal to f , we can, for example, use fˆ(x) = f (x) + ε(x) where |ε(x)| ≪ | f (x)|, or fˆ(x, u) = f (x) + ε(x, u) s.t. Eu[ fˆ(x, u)] = f (x).
In the subsections, we will discuss the characteristics of several zeroth-order
gradient estimations and then the zeroth-order methods for sum-minimization type
problems in a non-convex setup. Other information on gradient-free optimization
(such as structured objectives) can be found in the recent survey [157].

7.1 Random Directions Gradient Estimations

Let us start with the methods following the standard zeroth-order scheme of using gradient approximation to beneﬁt from the analysis of ﬁrst-order methods. In this section all methods have a form similar to the classic gradient descent

xk+1 = xk − hkg(xk, uk)

with only difference that instead of the true gradient we use the gradient approximation g(x, u). One way to build such gradient approximations is to use random directions to compute ﬁnite differences in the form

g(xk, uk) := fˆ(xk + µuk) − fˆ(xk) · uk µ

It makes sense to use centrally symmetric distributions for uk, for example uniformly distributed over the unit Euclidean sphere Sn−1 = {x ∈ Rn : x 2 = 1} (see [95, 111, 83]), or uk ∼ N (0, In) — so-called Gaussian smoothing introduced in
[189]. In this article, the authors proved Gaussian approximation

fµ (x) = 1

f

(x

+

µ

u

)e

−

1 2

u

2
2du

κ

Rn

(there κ = E e− u 22/2du = (2π)n/2) to have several good properties, such as convexity preservation (if f is convex then fµ is convex too), differentiability, and if f ∈ CL00,0 or f ∈ CL11,1 (i.e. Lipschitz-continuous function with constant L0 or function with Lipschitz-continuous gradient with L1 respectively) then the same holds for fµ with

Recent theoretical advances in non-convex optimization

55

L0( fµ )

L0(√f ) and L1( fµ )

L1( f ) respectively. It can be also shown that | fµ (x) −
0,0

f (x)| µL0 n for the case of f ∈ CL0 .

While in that paper the authors mostly discuss the convex case, there are some

results ([189][Section 7]) for a non-convex objective f too. They consider a process xk+1 = xk − hkg(xk, uk), with g deﬁned above, fˆ = f and uk ∼ N (0, In), and show that for the case of f ∈ CL11,1 this process converges in the sense of EU ∇ fµ (x) 2 (where U = {uk}Nk=−01):

∑ 1 N−1
EU N k=0

∇ fµ (xk)

2 2

fµ (x0) − f ∗ 3µ2(n + 4)

8(n + 4)L1

+

L1

N

32

then using the fact that ([189][Lemma 3]) ∇ fµ (x) − ∇ f (x) 2 [µL1/2](n + 3)3/2 we

obtain (from

∇ f (x)

2 2

2

∇ fµ (x) − ∇ f (x)

2 2

+

2

∇ fµ(x)

22)

∑ 1 N−1
EU N k=0

∇ f (xk)

2 2

2 µ2L21 (n + 3)3 4
+ 16(n + 4)L1

fµ (x0) − f ∗ 3µ2(n + 4)

+

L1

N

32

and choosing µ = O ε/[n3/2L1]

we

ensure

1 N

∑Nk=−01

EU

∇ f (xk)

2 2

upper bound for the expected number of steps N = O (n/ε2).

For the case of f ∈ CL00,0

ε2 with the

∑ 1 N−1
SN k=0 hkEU

∇ fµ (xk)

2 2

∑ 1

( fµ (x0) −

f ∗) +

1

N−1
n1/2(n + 4)2L3 h2

SN

µ

0

k

k=0

they show only that this process converges to the stationary point of fµ (x) – consider Q with diam(Q) R, then it can be shown that we need to make

N = O n(n + 4)2L50R ε4δ

steps

to

ensure

that

1 N

∑Nk=−01

EU

∇ fµ (xk)

2 2

ε2 keeping functional gap | fµ (x) −

f (x)| δ small. Authors also mention that with the hk → 0 and µ → 0 the conver-

gence in the sense of EU ∇ f (x) 2 can be proved too. These results can be extended [226] to the case of noisy fˆ i.e. | fˆ(x) − f (x)| δ

for f with Ho¨lder continuous gradient ( ∇ f (x) − ∇ f (y) 2 Lν x − y ν2 ) – it can be shown that for a small enough noise δ these convergence rates can be preserved.

More

speciﬁcally,

to

ensure

1 N

∑Nk=−01

EU

∇ f (xk)

2 2

ε2 one need to make

N=O

n

2

+

1−ν 2ν

ε

2 ν

steps under the assumption that noise δ = O

ε

3+ν 2ν

n 3+7ν 4ν

56

Danilova, Dvurechensky, Gasnikov, Gorbunov, Guminov, Kamzolov, Shibaev

where ν is a Ho¨lder parameter. For the convergence in the sense of smoothed func-

tion

gradient

norm

1 N

∑

N−1 k=0

EU

∇ fµ (xk)

2 2

ε2 it can be shown

N=O

7−3ν
n2 ε 2(3−ν)
1+ν

with δ = O

ε

5−ν 1+ν

n 13−3ν 4

with functional gap | fµ (x) − f (x)| = O ε/ n(1+ν)/2 . For the case of ν = 1 (i.e.

f ∈ CL11,1) these results can be improved to N = O (n/ε2) (n times better) achieving the same rate of convergence as in previous paper [189].

Such noisy setup is also interesting because it can be shown [210], that for a

non-convex function fˆ(x) s.t. | fˆ(x) − f (x)| ε f , where initial f is convex and 1Lipschitz and ε f ∼ max ε2/√n, ε/n there exists an algorithm which ﬁnds a point x˜

s.t. fˆ(x˜)

fˆ∗ + ε with complexity Poly

n,

1 ε

. The dependence ε f (ε) is optimal in

this class of algorithms.

This Gaussian smoothing technique was later used in works [101] (RSGF) and

[103] (RSPGF) to obtain complexity guarantees for stochastic zeroth-order opti-
mization. In the ﬁrst one ([101]), the unconstrained problem Q = Rn is considered, where fˆ = F(x, ξ ) s.t. Eξ [F(x, ξ )] = f (x) and F(·, ξ ) has a Lipschitz-continuous gradient with constant L1, ξ is a random variable whose distribution P is supported on Ξk ⊆ Rn. The procedure (7) has a form similar to the one proposed in [189]

xk+1 = xk − hkG(xk, ξ k, uk), G(xk, ξ k, uk) := fˆ(xk + µuk, ξµk) − fˆ(xk, ξ k) · uk,

and from Eξ [F(x, ξ )] = f (x) it follows that

Eξ,u [G(x, ξ , u)] = ∇ fµ (x).

The method then chooses the xk from generated {xk}Nk=1 as k = R where R is some random variable with a probability mass function PR supported on {1, . . . , N}.

The main goal to introduce this random iteration count R is to derive new complexity

results for non-convex stochastic optimization case.

For the case of f ∈ CL11,1, smoothing parameter µ, D f = 2( f (x1)− f ∗)/L, variance

σ 2 (Eξ

∇ fˆ(x, ξ ) − ∇ f (x)

2 2

σ 2) and the probability mass function

P (k) =

h

k

−

2

L(n

+

4

)h

2 k

R

N

∑ (hi − 2L(n + 4)h2i )

i=1

they obtain ([101][Theorem 3.2])

Recent theoretical advances in non-convex optimization

57

1 E ∇ f (xR) 2

L

2

D2f + 2µ2(n + 4)

1 + L(n + 4)2 ∑N h4k + Lh2k
k=1 N
∑ hk − 2L(n + 4)h2k
k=1

N
+ 2(n + 4)σ 2 ∑ h2k
k=1

where the expectation is taken with respect to R, {ξ k}. After choosing speciﬁc constant stepsizes hk = 1/[√n+4] · min 1/[4L√n+4], D˜/[σ√N] (note that this makes PR uniform on {1, . . . , N}) they get ([101][Corollary 3.3])

1 E ∇ f (xR) 2

L

2

1

2

(n

+

4

)LD

2 f

√ 2σ n + 4

+√

N

N

D˜ + D2f D˜

where D˜ > 0 is our estimation of D f (for example some upper bound). It can be

shown

that

to

ensure

P{

∇ f (xR)

2 2

ε}

1 − Λ (so-called (ε,Λ )-solution) the

total number of calls to the oracle fˆ can be bounded as

 O  nL2D2f + nL2
Λε Λ2

D˜ + D2f D˜

 2 σ2
ε2

Another method that is considered in [101] is a two-phase method (2-RSGF),
which uses the ﬁrst one (RSGF) S = log (2/Λ) times as a subroutine producing a list of candidates {x¯k}Sk=1 and then the output point x¯∗ is chosen in such a way that

∑ g(x¯∗) 2 = min g(x¯k) 2, g(x¯k) := 1 T G(x¯k, ξ k, uk)

k=1,...,S

T i=1

then it can be shown ([101][Theorem 3.4]) that (ε,Λ )-solution will be achieved after

taking





O  nL2D2f log (1/Λ ) + nL2

D˜ + D2f

2 σ2

n log2 (1/Λ)

log (1/Λ ) +

1+ σ2



ε

D˜ ε2

Λ

ε

calls to the fˆ which is better than the previous one in terms of Λ . A more general problem minx∈Q⊆Rn Ψ (x) = f (x) + h(x), where f ∈ CL1,1 and h(x)
is a simple convex and possibly non-smooth function is considered in [103]. They
use a mini-batched version of gradient estimation from the previous paper [101] and generalized projection obtaining ([103][Theorem 4, Corollaries 6-7]) similar
bounds for the gradient norm.
In [219], the authors use symmetric gradient estimations based on uniform distribution over the sphere to build a less dimension depending method. They consider the minimization problem minx∈Rn f (x) = Eξ [F(x, ξ )] = Eξ [ fˆ(x, ξ )] (note that in

58

Danilova, Dvurechensky, Gasnikov, Gorbunov, Guminov, Kamzolov, Shibaev

this paper authors consider both Rd and Rn with d ≪ n) where f (x) is L-Lipschitz, and µ-smooth, |F(x, ξ )| Ω and F variance is bounded by Vf . It was shown that using
g(xk, ξ k, uk) := n fˆ(xk + µuk, ξ k) − fˆ(xk − µuk, ξ k) · uk 2µ
where uk ∼ U Sn−1 (uniform distribution on the unit sphere Sn−1) and the process xk+1 = xk − αg(xk, ξ k, uk) after N steps

∑ 1 N
E N i=1

∇ f (xi)

2 2

=O

n n2/3 N1/2 + N1/3

Now consider the case when for a given ξ , F(x, ξ ) = g(r(x, θ ∗), ψ∗) (there g(·, ψ) and r(·, θ ) are parameterized function classes), where r(·, θ ∗) : Rn → Rd where d ≪ n. To put it simply, the authors consider the case when F(·, ξ ) : Rn → R while it is actually deﬁned on an d-dimensional manifold M for all ξ . That means that if one knows the manifold (i.e. θ ∗), and g and r are smooth the chain rule can be applied giving ∇ f (x) = J(x, θ ∗)∇rg(r, ψ) (where J(x, θ ∗) = ∂r(x,θ∗)/∂x) leading to
g(xk, ξ k, uk)) := d fˆ(xk + µJquk, ξ k)) − fˆ(xk − µJquk, ξ k)) · uk 2µ

where Jq is the orthonormalized J(xk, θ ∗) and uk ∼ U Sd−1 , and this gives

∑ 1 N
E N i=1

∇ f (xi)

2 2

=O

d d2/3 N1/2 + N1/3

which is much better than the previous one (because d ≪ n). However, this is impractical due to the fact that it requires the knowledge of θ ∗. Authors mix two previous estimations and estimate θ and ψ on every step, obtaining the method that
([219][Theorem 1]) after N steps ensures

∑ 1 N
E N i=1

∇ f (xi)

2 2

=O

n1/2 n1/2 + d + dn1/2 d2/3 + n1/2d2/3

N+

N 1/2

+

N 1/3

which is better than the initial bound for d n1/2. While such gradient estimates based on random directions are common it can
be shown that in terms of the number of samples required to the approximate gradient to ensure norm condition (or at least ensure it with some probability) random directions based methods lose to standard ﬁnite differences [24, 25, 23]. In these papers, authors consider an unconstrained optimization problem minx∈Rn f (x) where fˆ(x) = f (x) + ε(x) is computable, the noise ε is bounded uniformly: |ε(x)| ε f

Recent theoretical advances in non-convex optimization

59

and f (x) ∈ CL1,1 or f (x) ∈ CM2,2 (i.e. twice continuously differentiable function with M-Lipschitz continuous Hessian) .
The main idea in [24] is to compare the number of calls r (essentially a batch size) to the oracle fˆ(x) that will be enough to ensure norm condition

g(x) − ∇ f (x) 2 θ ∇ f (x) 2, θ ∈ [0, 1)

for zeroth-order gradient estimation g(x). This condition simpliﬁes the transition from gradient estimations to gradient when proving the convergence of algorithms. One of its implications is that g(x) is a descent direction for the function φ . In [25] the line-search method that uses such gradient approximations, ensuring the norm condition, is shown to converge.
They consider several methods of gradient estimation, deterministic (Forward and Central Finite Differences (FFD and CFD) and Linear Interpolation (LI) as generalization) and stochastic (Gaussian Smoothed Gradients (GSG and its centered version cGSG) and Sphere Smoothed Gradients (BSG and cBSG)), for the latter authors obtain the number of calls needed to ensure the norm condition with probability 1 − δ .

Name FFD CF D
LI GSG cGSG BSG cBSG

Gradient estimation g(x) form

∑n fˆ(x+µeµi)− fˆ(x) ei
i=1

∑n fˆ(x+µei)2−µfˆ(x−µei) ei
i=1
∑n fˆ(x+µuµi)− fˆ(x) ui, ui = [Q]i,
i=1
1r ∑r fˆ(x+µuµi)− fˆ(x) ui, ui ∼ N (0, In)
i=1

1r ∑r fˆ(x+µui)2−µfˆ(x−µui) ui, ui ∼ N (0, In)
i=1
nr ∑r fˆ(x+µuµi)− fˆ(x) ui, ui ∼ U Sn−1
i=1

nr ∑r fˆ(x+µui)2−µfˆ(x−µui) ui, ui ∼ U
i=1

Sn−1

Number of calls r n

n

n

12n δθ2

+

n+20 16δ

12n δθ2

+

n+30 144δ

8n θ2

+

8n 3θ

+

11n+104 24

log

n+1 δ

8n θ2

+

8n 3θ

+

9n+192 27

log

n+1 δ

∇√f (x) 2
2 nLε f

θ

√ 2n

3

Mε2

√f

3 6√θ

2 Q−1 nLε f

θ
6n√Lε f

θ

12 3 n7/2Mε2f θ
4n√Lε f
θ

4 3 n7/2Mε2
f
θ

Table 6: Bounds on number of fˆ calls r, and ∇ f (x) 2 that ensure the norm condition g(x) − ∇ f (x) 2 θ ∇ f (x) 2. For the GSG, cGSG, BSG and cBSG these are the results with probability 1 − δ . The gradient norm bound (last column) essentially means that for a noisy oracle fˆ we can ensure norm condition only for big enough
gradients. The LI method is basically FFD with directions given as columns of the
nonsingular matrix Q. When Q is orthonormal the g(x) takes a form from the table.

Let us take a look at two of these methods: FFD and GSG. For the ﬁrst one, the gradient estimation takes the form

60

Danilova, Dvurechensky, Gasnikov, Gorbunov, Guminov, Kamzolov, Shibaev

g(x) :=

n
∑

fˆ(x + µei) −

fˆ(x) ei

i=1

µ

where ei are the columns of In. It can be shown that for such g(x) the following

holds

g(x) − ∇ f (x) 2

µL√n 2ε f √n 2 + µ.

If there was no noise (ε f = 0) we could make this approximation as close to the
gradient as we want, so we would be able to ensure the norm condition in n calls to the fˆ. This is also true for a small enough noise (for example ev√en from this inequality we can take ε f = Lµ2/4 obtaining g(x) − ∇ f (x) 2 µL n). Authors provide such noise bound in form of lower bound on ∇ f (x) 2 for which the norm
condition can still be ensured

2 ε f µ θ ∇√f (x) 2 ⇒ 2 nLε f

L

nL

θ

∇ f (x) 2

In other word√s, that means that we can converge to the neighborhood where ∇ f (x) 2 ≈ 2 nLεf/θ. For the GSG they consider the mini-batched version of Gaussian smoothing from
[189]

g(x, {ui}) :=

1

r
∑

fˆ(x + µui) −

fˆ(x) ui,

ui

∼N

(0, In)

r i=1

µ

and prove that the norm condition will be ensured with probability 1 − δ after

r 3n √ n + (n + 4) + 1 = Ω 3n

δ θ 2 ( n − 1)2 16δ δ

θ 2δ

calls, which is while linear on n is still worse than the plain n in FFD, because of δ , and additional constants. However, this is a sufﬁcient number of calls, not a necessary, so authors derive the lower bound for r ([24][Section 2.3.1])
√ 1− δ r θ 2 (n + 1)
necessary to have probability P( g(x) − ∇ f (x) 2 θ ∇ f (x) 2) > 1 − δ . In their numerical experiments they show that to ensure the norm condition with θ < 1/2 with probability of at least 1/2 more than n oracle calls are needed, so this lower bound is weak.
The sufﬁcient lower bound can be improved using smoothing on a sphere for which they obtain Ω (n/θ2 · log [(n+1)/δ]), yet it is still worse than deterministic variants, and in practice its behavior is very similar to the Gaussian directions based approach.

Recent theoretical advances in non-convex optimization

61

There are also results for the case of f (x) ∈ CM2,2 (centered versions of the estimations), they can be found in Table 6.

7.2 Variance-Reduced Zeroth-Order Methods

One special case of the min f (x) problem is the ﬁnite sum minimization which was considered in previous sections for the ﬁrst-order methods. These problems in zeroth-order setup arise in reinforcement learning [93] (there as a minimization of a long-term cost which is essentially a sum of functions) and non-stationary online optimization problems [281].
Let us start with the ZO-SVRG from [166] – a zeroth-order version of SVRG from [139].
There a non-convex ﬁnite-sum problem of the form

∑ 1 m

min f (x) =

fi(x)

x∈Rn

m i=1

where fi ∈ CL1,1 i.e. ∇ fi(x) − ∇ fi(y) 2 L x − y 2 for any x, y ∈ Rn and i ∈ {1, . . . , m} is considered. Authors use the standard assumption that the variance of stochastic gradients is bounded

∑ 1 m ∇ fi(x) − ∇ fi(y) 2 σ 2

m i=1

2

and consider several different gradient estimates: two based on random directions on a unit sphere (in notation of [25] these are BSG with N = 1 and N = q (see Table 6), called RandGradEst and Avg-RandGradEst respectively), and one deterministic coordinate estimation (variant of CFD from Table 6 with possibly different µ j for each direction e j called CoordGradEst)

RandGradEst : ∇ˆ fi(x) = n [ fi(x + µui) − fi(x)]ui, µ

∑ Avg-RandGradEst : ∇ˆ f (x) = n

q
[ f (x + µui, j) − f (x)]ui, j,

i

µ q j=1 i

i

∑ CoordGradEst : ∇ˆ fi(x) = 1

n
[ fi(x + µ je j) − fi(x − µ je j)]e j

2µ j=1

there i ∈ {1, . . . , m}, µ > 0, and {e j}nj=1 are standard basis vectors (columns of In). For a mini-batch I ⊆ {1, . . . , m} of size r, authors denote

∑ ∇ˆ fI(x) = 1 ∇ˆ fi(x)
r i∈I

62

Danilova, Dvurechensky, Gasnikov, Gorbunov, Guminov, Kamzolov, Shibaev

Algorithm 12 ZO-SVRG [166]

Require: stepsizes {hks }, epoch length T , starting point x0 ∈ Rn, batch size r ≥ 1, smoothing parameter µ > 0, number of iterations N = S · T

φ0 = x00 = x0 for s = 0, 1, 2, . . ., S − 1 do

for k = 0, 1, 2, . . ., T − 1 do

Uniformly randomly pick set Ik from {1, . . ., m} such that |Ik| = r

gk

=

1 r

∑

∇ˆ fi(xks ) − ∇ˆ fi(φs) + ∇ˆ f (φs)

i∈Ik

xks+1 = xks − hks gk

end for

φs+1 = x0s+1 = xks end for

Pick ξ uniformly at random from {0, . . ., N − 1} return xξ

and the algorithm is the same as for SVRG (Algorithm 6), with the only difference that instead of true gradients update

xks+1

=

xks

−

h

k s

v

k s

,

vks

=

∇

f

Ik

(x

k s

)

−

∇

f

Ik

(x

0 s

)

+

∇

f

(x

0 s

)

they use gradient estimations

xks+1

=

xks

−

h

k s

vˆks

,

vˆks

=

∇ˆ

f

Ik

(x

k s

)

−

∇ˆ

f

Ik

(x

0 s

)

+

∇ˆ

f

(x

0 s

)

This estimation ∇ˆ f (x0s ) is no longer unbiased for zeroth-order gradient estimations, and that is the main problem for the convergence analysis of this method. They show that under assumptions mentioned above ZO-SVRG algorithm after N = S · T (there
S is a number of epochs) steps ensures that

RandGradEst : E Avg-RandGradEst : E
CoordGradEst : E

∇ f (x¯) 2 = O n + δn

2

Nr

∇ f (x¯) 2 = O n + δn

2

N r · min{n, q}

∇ f (x¯) 2 = O n

2

N

there n is a dimension, r = |I| – batch size, q is the number of directions used to estimate gradient via Avg-RandGradEst, x¯ is uniformly chosen from {xks }Ss,−k=1,0T−1, N = S · T is a total number of steps and

δn = 1,

if Ik draws samples from {1,. . . , m} with replacement

j(b < n), . . . without replacement

where j(b < n) = 1 if b < n and j(b < n) = 0 otherwise.

Recent theoretical advances in non-convex optimization

63

Basically, that means that CoordGradEst, the deterministic policy of gradient estimations, achieves the convergence rates of the original SVRG. In their tests, however, in terms of training loss versus function queries ZO-SVRG (the variant without mini-batching and with random directions on the sphere) beats ZO-SVRGAve (based on Avg-RandGradEst) and ZO-SVRG-Coord (based on CoordGradEst).

Algorithm 13 SpiderSZO [90]

Require: n0 ∈ [1, n1/2/6], Lipschitz constant L, epoch length T , starting point x0 ∈ Rn, outer batch size r1 ≥ 1, inner batch size r2 ≥ 1, number of iterations N = S · T for k = 0, 1, 2, . . ., N − 1 do if k mod T = 0 then Uniformly randomly pick set Ik from {1, . . ., m} (with replacement) such that |Ik| = r1

n
Compute gk = ∑

1 ∑ [ fi(xk +µe j )− fi(xk )] e j

j=1 r1 i∈Ik

µ

else

Create set of pairs Ik = {(i, ui)} where i uniformly randomly picked from {1, . . ., m} (with replacement) and independent ui ∼ N (0, In) such that |Ik| = r2

Compute

gk

=

1 r

∑

2 (i,ui)∈Ik

fi

(xk

+µ ui µ

)−

fi

(xk

)

ui

−

fi(xk−1+µuµi)− fi(xk−1) ui

+ gk−1

end if xk+1 = xk − hkgk where hk = min Ln0 εvk 2 , 2L1n0

end for

Pick ξ uniformly at random from {0, . . ., N − 1} return xξ

Another discussed above algorithm that can be used in the zeroth-order ﬁnite-
sum minimization setting is SPIDER [90]. The zeroth-order variant (Algorithm 13) of the algorithm blends stochastic and deterministic gradient estimations, using mini-batched FFD (Table 6) every p steps to reconstruct vk, which is later updated by mini-batched GSG.
The hk = min ε/[Ln0 vk 2], 1/[2Ln0] is a stepsize policy from Normalized Gradient Descent (NGD, [184]), where the stepsize is inverse-proportional to the norm of the gradient.
Authors show, that after N = O (1/ε2) iterations and O n min m1/2/ε2, 1/ε3 (there n is a dimension and m is a number of functions) IZO calls (i.e. calls of the oracle that returns the value of fi(x) given x and i) this algorithm ensures

E[ ∇ f (x¯) 2] 6ε

where x¯ is uniformly chosen from {xk}Nk=−01. This result is better than what follows directly from [189], at least by the factor of m1/2 (the direct application of the results

from [189] requires m calls on every step, and gives E[ ∇ f (x¯) 2] ε in O (n/ε2)

steps so the number of IZO calls would be O (nm/ε2)).

The results of two previously discussed papers [166, 90] were improved in the

recent work [134]. Authors show that ZO-SVRG-Coord actually has a better con-

vergence rate ([134][Theorem 2]) of E

∇ f (x¯)

2 2

= O (1/N) (n times better than

64

Danilova, Dvurechensky, Gasnikov, Gorbunov, Guminov, Kamzolov, Shibaev

the previous analysis). At ﬁrst they consider an intermediate variant of ZO-SVRG-
Coord and ZO-SVRG-Ave called ZO-SVRG-Coord-Rand, that uses CFD and BSG (Table 6) for ∇ˆ f (φs) and ∇ˆ fi(xks ) − ∇ˆ fi(φs) parts of

∑ gk = 1

∇ˆ

f

i

(x

k s

)

−

∇ˆ

f

i

(φs

)

+ ∇ˆ f (φs)

r i∈Ik

(from Algorithm 12) respectively, while variants in [166] used only one type of gra-

dient estimation at once. Then authors proof ([134][Corollary 1]) the convergence

rate E

∇ f (x¯)

2 2

= O (1/N) and show ([134][Lemmas 1-2]) that although the re-

placement of BSG with CFD requires n more oracle calls it achieves more accurate

gradient estimation so the convergence rate stays the same for the ZO-SVRG-Coord.

Another part of this work is devoted to SPIDER. Authors construct a new algo-

rithm (called ZO-SPIDER-Coord) in a way similar to the previous one – they use

CFD instead of GSG in Algorithm 13 and show that it has the same rate of conver-

gence, but with bigger stepsize hk = 1/[4L] (that doesn’t depend on ε), which is better

in practice.

One particular case of ﬁnite-sum minimization is considered in [281]. In this pa-

per, authors consider non-stationary online optimization problems, when the objec-

tive function being queried is time-varying, so one is limited to the use of one-point

estimators.

Such estimators can be constructed easily in the stochastic zeroth-order case. For

example we can consider GSG (Table 6) with N = 1 then

Eu(g(x)) = Eu f (x + µu) − f (x) u = Eu f (x + µu) u = ∇ fµ (x)

µ

µ

so we can chose g(x) := [ f (x+µu)/µ]u and obtain a reasonable one-point estimation. The problem is that the variance of such estimations explodes as µ → 0 (see [25]).
In this work, authors consider the residual feedback estimator
g˜k(xk) := uµk fk(xk + µuk) − fk−1(xk−1 + µuk−1)
where uk, uk−1 ∼ N (0, In). They show that (Lemma 2.4)
E[g˜k(xk)] = ∇ fµ,k(xk), ∀xk ∈ X and k
(there ∇ fµ,k is a gradient of smoothed fk). They consider the online bandit problem with regret function

∑T −1
RTg,µ = E
k=0

∇ fµ,k(xk)

2 2

and show ([281][Theorem 4.2]) that for xk+1 = ΠX xk − ηg˜k(xk) (where ΠX is the projection operator onto set X ) if f ∈ CL00,0

Recent theoretical advances in non-convex optimization

65





RT = O  n3/2L20 g,µ ε 3f/2

WT + W˜ T T −1

T

1/2

+

n3/2

L0

ε

1/2 f

T

1/2



and if additionally f ∈ CL11,1 ([281][Theorem 4.3])

∑T −1
RTg = E
k=0

∇ fk(xk)

2 2

=O

n4/3L0WT T 1/2 + n4/3L1L−0 1W˜ T

where WT and W˜ T are constants s.t.

T
∑ E [ fk(x) − fk−1(x)]
k=1
∑T
E | fk(x) − fk−1(x)|2
k=1

WT , ∀T, x W˜ T , ∀T, x.

That bound implies that RTg/T → 0 if WT = o T 1/2 and W˜ T = o (T ). Authors also consider ([281][Section 5]) the stochastic online optimization case where fˆt = Ft(x, ξt ) s.t. E [Ft(x, ξt )] = ft (x) and show that under the assumptions of the same form as above (with WT,ξ and W˜ T,ξ ) similar regret bounds can be achieved.
In their numerical experiments, authors compare conventional one-point and
two-point approaches with one-point residual feedback. Even though the latter
works worse than the two-point variant, it has lower variance and achieves better
results than the conventional one-point feedback, and can be used in practice, in
contrast to two-point feedback.

8 Globalization Techniques
In the previous sections we mainly considered guarantees for the methods to converge to a stationary point or local extremum. Global performance guarantees are available only for some subclasses of non-convex minimization problems. Despite that there are several practical techniques for convergence globalization for the local methods, which we brieﬂy describe next, following [282].

8.1 Multistart Technique
The ﬁrst approach involves using an algorithm which converges to a local minimum and running it multiple times from different starting points. This may result in the algorithm for ﬁnding multiple local minima of the objective, some of which might in fact be global solutions.

66

Danilova, Dvurechensky, Gasnikov, Gorbunov, Guminov, Kamzolov, Shibaev

To be more concrete, we consider the problem

min f (x) .
x∈[0,1]n

Let the initial points be sampled from the uniform distribution on [0, 1]n. If the Lebesgue measure of the attraction basin (the set of points, initialized at which the local algorithm converges to the global minimum) of the global minimum is µ > 0, then the expected number of points required to ﬁnd the global minimum is m = O˜ (1/µ). If the attraction basin is a ball of radius r, then µ ∼ rn. Hence, it is reasonable to expect that the number of initial points required depends on n exponentially. For that reason, this approach to global optimization becomes impractical as n grows.
The effectiveness of this approach also depends on the chosen initial points. The quality of a family of initial points x0,i mi=1 can be characterized by the quantity

dn x0,i mi=1 = max min x − x0,i 2 .
x∈[0,1]n i=1,...,m

One of the ways to iteratively generate the starting points x0,k mk=1 is called the quasi Monte Carlo scheme using low-discrepancy sequences, for example, the Van der Corput sequence. Let {pi}ni=1 be a sequence of distinct prime numbers, and let φi (k) be the k-th element of the Van der Corput sequence in base pi. Ex-

plicitly, φi(k) = l∑k,i a j pi− j−1, where lk,i is the length of the representation of k in
j=0

lk,i

base pi k = ∑ a j pij. Finally, set x0,k = (φ1 (k) , ..1., φn (k)), k = 1, ..., m. In this case

d

x0,i

j=0
m =O

√ n

m

−

1

/n

ln

m

, while the optimal value, which is achieved at

n

i=1

√

the uniform grid, is O nm−1/(2n) .

8.2 Multidimensional Bisection
The main shortcoming of the approach described above is that the family x0,k mk=1 is constructed without taking into account any properties of f (x). Assume now that, for all x, y ∈ [0, 1]n , | f (y) − f (x)| M y − x . Then, for any y, the function f (y) − M x − y is a minorant of f (x). Consequently, for any {yk}mk=1 the function max f (yk) − M x − yk is also a minorant of f (x). Then one may choose the
k=1,...,m
next initial point to be the minimizer of the minorant constructed using the previous initial points [89]:
x0,m+1 = arg min max f x0,k − M x − x0,k .
x k=1,...,m

Recent theoretical advances in non-convex optimization

67

In the one-dimensional case, each minorant is just a piecewise linear function, and its minimum is easy to compute explicitly. In higher-dimensions, this idea is more difﬁcult to implement, and the resulting algorithms also tend to become slower as n increases. This method also requires an estimate of the Lipschitz constant and is sensitive to the accuracy of this estimate.

8.3 Langevin Dynamics
The last but not least approach which we consider in this section is inspired by the Langevin dynamics, which is deﬁned by the stochastic differential equation
√ dx(t) = −∇ f (x(t))dt + 2T dW (t) ,
where W (t) is a Wiener process (also known as Brownian motion) and T is the temperature parameter. It has been shown that the distribution of x (t) converges to a distribution with density
exp − f (x) T exp − f (y) T dy
as t → ∞, and as T → 0+ this distribution concentrates around the global minima. To apply this in practice, the continuous dynamics has to be discretized. One of the ways to do that is as follows:
√ xk+1 = xk − h∇ f (xk) + 2hT εk,
where h > 0 is the stepsize and εk is standard gaussian random variable. Nonasymptotic results demonstrating the convergence of this method to an approximate global minimum were presented in the work [261]. In this paper, the temperature parameter T was assumed to be constant. However, other strategies are sometimes used in practice, for example,
c Tk = ln (2 + k) ,
which ensures Tk → 0+ as k → ∞.

Acknowledgements
The authors are grateful to A. Gornov, A. Nazin, Yu. Nesterov, B. Polyak and K. Scheinberg for fruitful discussions and their suggestions which helped to improve the quality of the text.

68

Danilova, Dvurechensky, Gasnikov, Gorbunov, Guminov, Kamzolov, Shibaev

The research was partially supported by the Ministry of Science and Higher Education of the Russian Federation (Goszadaniye) No.075-00337-20-03, project No. 0714-2020-0005.

References
1. Collection of optimizers for pytorch. https://github.com/jettify/pytorch-optimizer. 2. N. Agarwal, Z. Allen-Zhu, B. Bullins, E. Hazan, and T. Ma. Finding approximate local
minima faster than gradient descent. In Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing, pages 1195–1199, 2017. 3. K. Ahn, C. Yun, and S. Sra. Sgd with shufﬂing: optimal rates without component convexity and large epoch requirements. Advances in Neural Information Processing Systems, 33, 2020. 4. A. Ajalloeian and S. U. Stich. Analysis of sgd with biased gradient estimators. arXiv preprint arXiv:2008.00051, 2020. 5. D. Alistarh, D. Grubic, J. Li, R. Tomioka, and M. Vojnovic. Qsgd: Communication-efﬁcient sgd via gradient quantization and encoding. In Advances in Neural Information Processing Systems, pages 1709–1720, 2017. 6. Z. Allen-Zhu. Natasha: Faster non-convex stochastic optimization via strongly non-convex parameter. In International Conference on Machine Learning, pages 89–97, 2017. 7. Z. Allen-Zhu. How to make the gradients small stochastically: Even faster convex and nonconvex sgd. In Advances in Neural Information Processing Systems, pages 1157–1167, 2018. 8. Z. Allen-Zhu. Katyusha x: Simple momentum method for stochastic sum-of-nonconvex optimization. In International Conference on Machine Learning, pages 179–185, 2018. 9. Z. Allen-Zhu. Natasha 2: Faster non-convex optimization than sgd. In Advances in Neural Information Processing Systems, pages 2675–2686, 2018. 10. Z. Allen-Zhu and Y. Li. Neon2: Finding local minima via ﬁrst-order oracles. In Advances in Neural Information Processing Systems, pages 3716–3726, 2018. 11. Z. Allen-Zhu and Y. Li. Can sgd learn recurrent neural networks with provable generalization? In Advances in Neural Information Processing Systems, pages 10331–10341, 2019. 12. Z. Allen-Zhu, Y. Li, and Y. Liang. Learning and generalization in overparameterized neural networks, going beyond two layers. In Advances in neural information processing systems, pages 6158–6169, 2019. 13. Z. Allen-Zhu, Y. Li, and Z. Song. A convergence theory for deep learning via overparameterization. In International Conference on Machine Learning, pages 242–252. PMLR, 2019. 14. Z. Allen-Zhu, Y. Li, and Z. Song. On the convergence rate of training recurrent neural networks. In Advances in neural information processing systems, pages 6676–6688, 2019. 15. A. Anandkumar and R. Ge. Efﬁcient approaches for escaping higher order saddle points in non-convex optimization. In Conference on learning theory, pages 81–102. PMLR, 2016. 16. Y. Arjevani, Y. Carmon, J. C. Duchi, D. J. Foster, A. Sekhari, and K. Sridharan. Second-order information in non-convex stochastic optimization: Power and limitations. In Conference on Learning Theory, pages 242–299, 2020. 17. Y. Arjevani, Y. Carmon, J. C. Duchi, D. J. Foster, N. Srebro, and B. Woodworth. Lower bounds for non-convex stochastic optimization. arXiv preprint arXiv:1912.02365, 2019. 18. S. Arora, N. Cohen, N. Golowich, and W. Hu. A convergence analysis of gradient descent for deep linear neural networks. arXiv preprint arXiv:1810.02281, 2018. 19. F. Bach, R. Jenatton, J. Mairal, G. Obozinski, et al. Optimization with sparsity-inducing penalties. Foundations and Trends® in Machine Learning, 4(1):1–106, 2012.

Recent theoretical advances in non-convex optimization

69

20. R. Baraniuk, M. Davenport, R. DeVore, and M. Wakin. A simple proof of the restricted isometry property for random matrices. Constructive Approximation, 28(3):253–263, 2008.
21. A. Bazarova, A. Beznosikov, and A. Gasnikov. Linearly convergent gradient-free methods for minimization of symmetric parabolic approximation. arXiv preprint arXiv:2009.04906, 2020.
22. A. Ben-Tal and A. Nemirovski. Lectures on Modern Convex Optimization. Society for Industrial and Applied Mathematics, 2001.
23. A. S. Berahas, L. Cao, K. Choromanski, and K. Scheinberg. Linear interpolation gives better gradients than gaussian smoothing in derivative-free optimization, 2019.
24. A. S. Berahas, L. Cao, K. Choromanski, and K. Scheinberg. A theoretical and empirical comparison of gradient approximations in derivative-free optimization, 2020.
25. A. S. Berahas, L. Cao, and K. Scheinberg. Global convergence rate analysis of a generic line search algorithm with noise, 2019.
26. E. H. Bergou, E. Gorbunov, and P. Richta´rik. Stochastic three points method for unconstrained smooth minimization, 2019.
27. A. Beznosikov, S. Horva´th, P. Richta´rik, and M. Safaryan. On biased compression for distributed learning. arXiv preprint arXiv:2002.12410, 2020.
28. S. Bhojanapalli, A. Kyrillidis, and S. Sanghavi. Dropping convexity for faster semi-deﬁnite optimization. In Conference on Learning Theory, pages 530–582, 2016.
29. E. G. Birgin, J. Gardenghi, J. M. Mart´ınez, S. A. Santos, and P. L. Toint. Worst-case evaluation complexity for unconstrained nonlinear optimization using high-order regularized models. Mathematical Programming, 163(1-2):359–368, 2017.
30. A. Blum, J. Hopcroft, and R. Kannan. Foundations of data science. Cambridge University Press, 2016.
31. A. Blum and R. L. Rivest. Training a 3-node neural network is np-complete. In Advances in neural information processing systems, pages 494–501, 1989.
32. T. Blumensath and M. E. Davies. Iterative hard thresholding for compressed sensing. Applied and computational harmonic analysis, 27(3):265–274, 2009.
33. L. Bogolubsky, P. Dvurechensky, A. Gasnikov, G. Gusev, Y. Nesterov, A. M. Raigorodskii, A. Tikhonov, and M. Zhukovskii. Learning supervised pagerank with gradient-based and gradient-free optimization methods. In D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett, editors, Advances in Neural Information Processing Systems 29, pages 4914– 4922. Curran Associates, Inc., 2016. arXiv:1603.00717.
34. L. Bottou. Curiously fast convergence of some stochastic gradient descent algorithms. In Proceedings of the symposium on learning and data science, Paris, 2009.
35. L. Bottou. Large-scale machine learning with stochastic gradient descent. In Proceedings of COMPSTAT’2010, pages 177–186. Springer, 2010.
36. L. Bottou. Stochastic gradient descent tricks. In Neural networks: Tricks of the trade, pages 421–436. Springer, 2012.
37. L. Bottou, F. E. Curtis, and J. Nocedal. Optimization methods for large-scale machine learning. Siam Review, 60(2):223–311, 2018.
38. S. Boyd and L. Vandenberghe. Convex Optimization. NY Cambridge University Press, 2004. 39. J. Bu and M. Mesbahi. A note on Nesterov’s accelerated method in nonconvex optimization:
a weak estimate sequence approach. arXiv preprint arXiv:2006.08548, 2020. 40. S. Bubeck. Introduction to online optimization. 2011. 41. S. Bubeck. Convex optimization: Algorithms and complexity. Found. Trends Mach. Learn.,
8(3–4):231–357, nov 2015. 42. E. J. Candes, X. Li, and M. Soltanolkotabi. Phase retrieval via wirtinger ﬂow: Theory and
algorithms. IEEE Transactions on Information Theory, 61(4):1985–2007, 2015. 43. E. J. Cande`s and B. Recht. Exact matrix completion via convex optimization. Foundations
of Computational mathematics, 9(6):717, 2009. 44. E. J. Candes and T. Tao. Decoding by linear programming. IEEE transactions on information
theory, 51(12):4203–4215, 2005. 45. E. J. Cande`s and T. Tao. The power of convex relaxation: Near-optimal matrix completion.
IEEE Transactions on Information Theory, 56(5):2053–2080, 2010.

70

Danilova, Dvurechensky, Gasnikov, Gorbunov, Guminov, Kamzolov, Shibaev

46. E. J. Candes, M. B. Wakin, and S. P. Boyd. Enhancing sparsity by reweighted ℓ1 minimization. Journal of Fourier analysis and applications, 14(5-6):877–905, 2008.
47. Y. Carmon and J. C. Duchi. Gradient descent efﬁciently ﬁnds the cubic-regularized nonconvex newton step. arXiv preprint arXiv:1612.00547, 2016.
48. Y. Carmon, J. C. Duchi, O. Hinder, and A. Sidford. “Convex until proven guilty”: Dimensionfree acceleration of gradient descent on non-convex functions. volume 70 of Proceedings of Machine Learning Research, pages 654–663, International Convention Centre, Sydney, Australia, 06–11 Aug 2017. PMLR.
49. Y. Carmon, J. C. Duchi, O. Hinder, and A. Sidford. Accelerated methods for nonconvex optimization. SIAM Journal on Optimization, 28(2):1751–1772, 2018.
50. Y. Carmon, J. C. Duchi, O. Hinder, and A. Sidford. Lower bounds for ﬁnding stationary points II: ﬁrst-order methods. Mathematical Programming, Sep 2019.
51. Y. Carmon, J. C. Duchi, O. Hinder, and A. Sidford. Lower bounds for ﬁnding stationary points i. Mathematical Programming, 184(1):71–120, Nov 2020.
52. C. Cartis, N. I. Gould, and P. L. Toint. Adaptive cubic regularisation methods for unconstrained optimization. part i: motivation, convergence and numerical results. Mathematical Programming, 127(2):245–295, 2011.
53. C. Cartis, N. I. Gould, and P. L. Toint. Universal regularization methods: Varying the power, the smoothness and the accuracy. SIAM Journal on Optimization, 29(1):595–615, 2019.
54. C. Cartis, N. I. M. Gould, and P. L. Toint. Adaptive cubic regularisation methods for unconstrained optimization. part ii: worst-case function- and derivative-evaluation complexity. Mathematical Programming, 130(2):295–319, Dec 2011.
55. C. Cartis, N. I. M. Gould, and P. L. Toint. Improved second-order evaluation complexity for unconstrained nonlinear optimization using high-order regularized models. arXiv:1708.04044, 2018.
56. V. Charisopoulos, A. R. Benson, and A. Damle. Entrywise convergence of iterative methods for eigenproblems. arXiv preprint arXiv:2002.08491, 2020.
57. X. Chen, S. Liu, R. Sun, and M. Hong. On the convergence of a class of adam-type algorithms for non-convex optimization. arXiv preprint arXiv:1808.02941, 2018.
58. Y. Chen and Y. Chi. Harnessing structures in big data via guaranteed low-rank matrix estimation. arXiv preprint arXiv:1802.08397, 2018.
59. Y. Chen, Y. Chi, J. Fan, and C. Ma. Gradient descent with random initialization: Fast global convergence for nonconvex phase retrieval. Mathematical Programming, 176(1-2):5–37, 2019.
60. Z. Chen and T. Yang. A variance reduction method for non-convex optimization with improved convergence under large condition number. arXiv preprint arXiv:1809.06754, 2018.
61. Z. Chen and Y. Zhou. Momentum with variance reduction for nonconvex composition optimization. arXiv preprint arXiv:2005.07755, 2020.
62. Y. Chi, Y. M. Lu, and Y. Chen. Nonconvex optimization meets low-rank matrix factorization: An overview. arXiv preprint arXiv:1809.09573, 2018.
63. P. L. Combettes and J.-C. Pesquet. Proximal splitting methods in signal processing. In Fixed-point algorithms for inverse problems in science and engineering , pages 185–212. Springer, 2011.
64. A. Conn, N. Gould, and P. Toint. Trust Region Methods. Society for Industrial and Applied Mathematics, 2000.
65. A. Conn, K. Scheinberg, and L. Vicente. Introduction to Derivative-Free Optimization. Society for Industrial and Applied Mathematics, 2009.
66. F. E. Curtis and K. Scheinberg. Optimization methods for supervised machine learning: From linear models to deep learning. arXiv preprint arXiv:1706.10207, 2017.
67. A. Cutkosky and F. Orabona. Momentum-based variance reduction in non-convex sgd. In Advances in Neural Information Processing Systems, pages 15236–15245, 2019.
68. C. D. Dang and G. Lan. Stochastic block mirror descent methods for nonsmooth and stochastic optimization. SIAM J. on Optimization, 25(2):856–881, Apr. 2015.
69. D. Davis and D. Drusvyatskiy. Stochastic model-based minimization of weakly convex functions. SIAM Journal on Optimization, 29(1):207–239, 2019.

Recent theoretical advances in non-convex optimization

71

70. A. Defazio. Understanding the role of momentum in non-convex optimization: Practical insights from a lyapunov analysis. arXiv preprint arXiv:2010.00406, 2020.
71. A. Defazio, F. Bach, and S. Lacoste-Julien. Saga: A fast incremental gradient method with support for non-strongly convex composite objectives. In Proceedings of the 27th International Conference on Neural Information Processing Systems, NIPS’14, pages 1646– 1654, Cambridge, MA, USA, 2014. MIT Press.
72. A. Defazio and L. Bottou. On the ineffectiveness of variance reduced optimization for deep learning. In Advances in Neural Information Processing Systems, pages 1753–1763, 2019.
73. A. Defazio, J. Domke, et al. Finito: A faster, permutable incremental gradient method for big data problems. In International Conference on Machine Learning, pages 1125–1133, 2014.
74. A. De´fossez, L. Bottou, F. Bach, and N. Usunier. On the convergence of adam and adagrad. arXiv preprint arXiv:2003.02395, 2020.
75. V. Demin, D. Nekhaev, I. Surazhevsky, K. Nikiruy, A. Emelyanov, S. Nikolaev, V. Rylkov, and M. Kovalchuk. Necessary conditions for stdp-based pattern recognition learning in a memristive spiking neural network. Neural Networks, 134:64–75, 2021.
76. J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.
77. J. Diakonikolas and M. I. Jordan. Generalized momentum-based methods: A Hamiltonian perspective. arXiv preprint arXiv:1906.00436, 2019.
78. T. Ding, D. Li, and R. Sun. Spurious local minima exist for almost all over-parameterized neural networks. 2019.
79. J. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12(Jul.):2121–2159, 2011.
80. J. Duchi, M. I. Jordan, and B. McMahan. Estimation, optimization, and parallelism when data is sparse. In Advances in Neural Information Processing Systems, pages 2832–2840, 2013.
81. D. Dvinskikh, A. Ogaltsov, A. Gasnikov, P. Dvurechensky, and V. Spokoiny. On the linesearch gradient methods for stochastic optimization. IFAC-PapersOnLine, 53(2):1715–1720, 2020. 21th IFAC World Congress, arXiv:1911.08380.
82. P. Dvurechensky. Gradient method with inexact oracle for composite non-convex optimization. arXiv:1703.09180, 2017.
83. P. Dvurechensky, E. Gorbunov, and A. Gasnikov. An accelerated directional derivative method for smooth stochastic convex optimization. European Journal of Operational Research, 290(2):601 – 621, 2021.
84. P. Dvurechensky, S. Shtern, and M. Staudigl. First-order methods for convex optimization. EURO Journal on Computational Optimization, 9:100015, 2021. arXiv:2101.00935.
85. P. Dvurechensky and M. Staudigl. Hessian barrier algorithms for non-convex conic optimization. arXiv:2111.00100, 2021.
86. P. Dvurechensky, M. Staudigl, and C. A. Uribe. Generalized self-concordant hessian-barrier algorithms. arXiv:1911.01522, 2019. WIAS Preprint No. 2693.
87. P. E. Dvurechensky, A. V. Gasnikov, E. A. Nurminski, and F. S. Stonyakin. Advances in Low-Memory Subgradient Optimization, pages 19–59. Springer International Publishing, Cham, 2020. arXiv:1902.01572.
88. N. Emmenegger, R. Kyng, and A. N. Zehmakan. On the oracle complexity of higher-order smooth non-convex ﬁnite-sum optimization. arXiv preprint arXiv:2103.05138, 2021.
89. Y. G. Evtushenko. Numerical methods for ﬁnding global extrema (case of a non-uniform mesh). USSR Computational Mathematics and Mathematical Physics, 11(6):38–54, 1971.
90. C. Fang, C. J. Li, Z. Lin, and T. Zhang. Spider: Near-optimal non-convex optimization via stochastic path-integrated differential estimator. In Advances in Neural Information Processing Systems, pages 689–699, 2018.
91. C. Fang, Z. Lin, and T. Zhang. Sharp analysis for nonconvex sgd escaping from saddle points. In Conference on Learning Theory, pages 1192–1234, 2019.
92. I. Fatkhullin and B. Polyak. Optimizing static linear feedback: Gradient method. arXiv preprint arXiv:2004.09875, 2020.

72

Danilova, Dvurechensky, Gasnikov, Gorbunov, Guminov, Kamzolov, Shibaev

93. M. Fazel, R. Ge, S. M. Kakade, and M. Mesbahi. Global convergence of policy gradient methods for the linear quadratic regulator, 2019.
94. S. Feizi, H. Javadi, J. Zhang, and D. Tse. Porcupine neural networks:(almost) all local optima are global. arXiv preprint arXiv:1710.02196, 2017.
95. A. D. Flaxman, A. T. Kalai, and H. B. McMahan. Online convex optimization in the bandit setting: Gradient descent without a gradient. In Proceedings of the Sixteenth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA ’05, pages 385–394, Philadelphia, PA, USA, 2005. Society for Industrial and Applied Mathematics.
96. C. A. Floudas and P. M. Pardalos. Encyclopedia of optimization. Springer Science & Business Media, 2008.
97. A. Gasnikov. Universal gradient descent. MCCME, Moscow, 2021. 98. A. Gasnikov, P. Dvurechensky, M. Zhukovskii, S. Kim, S. Plaunov, D. Smirnov, and
F. Noskov. About the power law of the pagerank vector component distribution. part 2. the buckley–osthus model, veriﬁcation of the power law for this model, and setup of real search engines. Numerical Analysis and Applications, 11(1):16–32, 2018. 99. R. Ge, F. Huang, C. Jin, and Y. Yuan. Escaping from saddle points—online stochastic gradient for tensor decomposition. In Conference on Learning Theory, pages 797–842, 2015. 100. R. Ge and J. Zou. Intersecting faces: Non-negative matrix factorization with new guarantees. In International Conference on Machine Learning, pages 2295–2303. PMLR, 2015. 101. S. Ghadimi and G. Lan. Stochastic ﬁrst- and zeroth-order methods for nonconvex stochastic programming. SIAM Journal on Optimization, 23(4):2341–2368, 2013. arXiv:1309.5549. 102. S. Ghadimi and G. Lan. Accelerated gradient methods for nonconvex nonlinear and stochastic programming. Mathematical Programming, 156(1):59–99, 2016. 103. S. Ghadimi, G. Lan, and H. Zhang. Mini-batch stochastic approximation methods for nonconvex stochastic composite optimization. Mathematical Programming, 155(1):267–305, 2016. arXiv:1308.6594. 104. S. Ghadimi, G. Lan, and H. Zhang. Generalized uniformly optimal methods for nonlinear programming. Journal of Scientiﬁc Computing, 79(3):1854–1881, Jun 2019. 105. M. X. Goemans and D. P. Williamson. Improved approximation algorithms for maximum cut and satisﬁability problems using semideﬁnite programming. Journal of the ACM (JACM), 42(6):1115–1145, 1995. 106. I. Goodfellow, Y. Bengio, and A. Courville. Deep Learning. MIT Press, 2016. http://www.deeplearningbook.org . 107. I. Goodfellow, Y. Bengio, A. Courville, and Y. Bengio. Deep learning, volume 1. MIT press Cambridge, 2016. 108. E. Gorbunov, K. P. Burlachenko, Z. Li, and P. Richtarik. Marina: Faster non-convex distributed learning with compression. In M. Meila and T. Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 3788–3798. PMLR, 18–24 Jul 2021. 109. E. Gorbunov, M. Danilova, and A. Gasnikov. Stochastic optimization with heavy-tailed noise via accelerated gradient clipping. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 15042–15053. Curran Associates, Inc., 2020. 110. E. Gorbunov, M. Danilova, I. Shibaev, P. Dvurechensky, and A. Gasnikov. Near-optimal high probability complexity bounds for non-smooth stochastic optimization with heavy-tailed noise. arXiv:2106.05958, 2021. 111. E. Gorbunov, P. Dvurechensky, and A. Gasnikov. An accelerated method for derivativefree smooth stochastic convex optimization. arXiv preprint arXiv:1802.09022 (accepted to SIOPT), 2018. 112. E. Gorbunov, F. Hanzely, and P. Richta´rik. A uniﬁed theory of sgd: Variance reduction, sampling, quantization and coordinate descent. In International Conference on Artiﬁcial Intelligence and Statistics, pages 680–690, 2020. 113. E. A. Gorbunov, A. Bibi, O. Sener, E. H. Bergou, and P. Richta´rik. A stochastic derivative free optimization method with momentum. In ICLR, 2020.

Recent theoretical advances in non-convex optimization

73

114. A. Gorodetskiy, A. Shlychkova, and A. I. Panov. Delta schema network in model-based reinforcement learning. In B. Goertzel, A. I. Panov, A. Potapov, and R. Yampolskiy, editors, Artiﬁcial General Intelligence, pages 172–182, Cham, 2020. Springer International Publishing.
115. A. Gotmare, N. S. Keskar, C. Xiong, and R. Socher. A closer look at deep learning heuristics: Learning rate restarts, warmup and distillation. arXiv preprint arXiv:1810.13243, 2018.
116. R. Gower, O. Sebbouh, and N. Loizou. Sgd for structured nonconvex functions: Learning rates, minibatching and interpolation. In International Conference on Artiﬁcial Intelligence and Statistics, pages 1315–1323. PMLR, 2021.
117. R. M. Gower, N. Loizou, X. Qian, A. Sailanbayev, E. Shulgin, and P. Richta´rik. Sgd: General analysis and improved rates. In International Conference on Machine Learning, pages 5200– 5209, 2019.
118. P. Goyal, P. Dolla´r, R. Girshick, P. Noordhuis, L. Wesolowski, A. Kyrola, A. Tulloch, Y. Jia, and K. He. Accurate, large minibatch sgd: Training imagenet in 1 hour. arXiv preprint arXiv:1706.02677, 2017.
119. A. O. Griewank. Generalized descent for global optimization. Journal of optimization theory and applications, 34(1):11–39, 1981.
120. S. Guminov, P. Dvurechensky, N. Tupitsa, and A. Gasnikov. On a combination of alternating minimization and Nesterov’s momentum. In Proceedings of the 38th International Conference on Machine Learning, volume 145 of Proceedings of Machine Learning Research, Virtual, 18–24 Jul 2021. PMLR. arXiv:1906.03622, WIAS Preprint No. 2695.
121. S. Guminov and A. Gasnikov. Accelerated methods for alpha-weakly-quasi-convex problems. arXiv preprint arXiv:1710.00797, 2017.
122. S. V. Guminov, Y. E. Nesterov, P. E. Dvurechensky, and A. V. Gasnikov. Accelerated primaldual gradient descent with linesearch for convex, nonconvex, and nonsmooth optimization problems. Doklady Mathematics, 99(2):125–128, Mar 2019.
123. B. D. Haeffele and R. Vidal. Global optimality in neural network training. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 7331–7339, 2017.
124. G. Haeser, H. Liu, and Y. Ye. Optimality condition and complexity analysis for linearly-constrained optimization without differentiability on the boundary. Mathematical Programming, 178(1):263–299, Nov 2019.
125. J. Z. HaoChen and S. Sra. Random shufﬂing beats sgd after ﬁnite epochs. arXiv preprint arXiv:1806.10077, 2018.
126. E. Hazan, K. Levy, and S. Shalev-Shwartz. Beyond convexity: Stochastic quasi-convex optimization. In Advances in Neural Information Processing Systems, pages 1594–1602, 2015.
127. K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770– 778, 2016.
128. O. Hinder, A. Sidford, and N. Sohoni. Near-optimal methods for minimizing star-convex functions and beyond. In Conference on Learning Theory, pages 1894–1938. PMLR, 2020.
129. T. Hofmann, A. Lucchi, S. Lacoste-Julien, and B. McWilliams. Variance reduced stochastic gradient descent with neighbors. In Advances in Neural Information Processing Systems, pages 2305–2313, 2015.
130. S. Horva´th, D. Kovalev, K. Mishchenko, S. Stich, and P. Richta´rik. Stochastic distributed learning with gradient quantization and variance reduction. arXiv preprint arXiv:1904.05115, 2019.
131. S. A. Ilyuhin, A. V. Sheshkus, and V. L. Arlazarov. Recognition of images of Korean characters using embedded networks. In W. Osten and D. P. Nikolaev, editors, Twelfth International Conference on Machine Vision (ICMV 2019), volume 11433, pages 273 – 279. International Society for Optics and Photonics, SPIE, 2020.
132. P. Jain and P. Kar. Non-convex optimization for machine learning. Found. Trends Mach. Learn., 10(3–4):142–336, Dec. 2017.
133. P. Jain, P. Netrapalli, and S. Sanghavi. Low-rank matrix completion using alternating minimization. In Proceedings of the forty-ﬁfth annual ACM symposium on Theory of computing, pages 665–674, 2013.

74

Danilova, Dvurechensky, Gasnikov, Gorbunov, Guminov, Kamzolov, Shibaev

134. K. Ji, Z. Wang, Y. Zhou, and Y. Liang. Improved zeroth-order variance reduced algorithms and analysis for nonconvex optimization, 2019.
135. Z. Ji and M. J. Telgarsky. Gradient descent aligns the layers of deep linear networks. In 7th International Conference on Learning Representations, ICLR 2019, 2019.
136. C. Jin, R. Ge, P. Netrapalli, S. M. Kakade, and M. I. Jordan. How to escape saddle points efﬁciently. volume 70 of Proceedings of Machine Learning Research, pages 1724–1732, International Convention Centre, Sydney, Australia, 06–11 Aug 2017. PMLR.
137. C. Jin, P. Netrapalli, R. Ge, S. M. Kakade, and M. I. Jordan. On nonconvex optimization for machine learning: Gradients, stochasticity, and saddle points. Journal of the ACM (JACM), 68(2):1–29, 2021.
138. C. Jin, P. Netrapalli, and M. I. Jordan. Accelerated gradient descent escapes saddle points faster than gradient descent. In Conference On Learning Theory, pages 1042–1085. PMLR, 2018.
139. R. Johnson and T. Zhang. Accelerating stochastic gradient descent using predictive variance reduction. In Advances in neural information processing systems, pages 315–323, 2013.
140. H. Karimi, J. Nutini, and M. Schmidt. Linear convergence of gradient and proximal-gradient methods under the polyak-łojasiewicz condition. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 795–811. Springer, 2016.
141. A. Khaled and P. Richta´rik. Better theory for sgd in the nonconvex world. arXiv preprint arXiv:2002.03329, 2020.
142. S. Khot, G. Kindler, E. Mossel, and R. O’Donnell. Optimal inapproximability results for max-cut and other 2-variable csps? SIAM Journal on Computing, 37(1):319–357, 2007.
143. A. Khritankov. Hidden feedback loops in machine learning systems: A simulation model and preliminary results. In D. Winkler, S. Bifﬂ, D. Mendez, M. Wimmer, and J. Bergsmann, editors, Software Quality: Future Perspectives on Software Engineering Quality, pages 54– 65, Cham, 2021. Springer International Publishing.
144. R. Kidambi, P. Netrapalli, P. Jain, and S. Kakade. On the insufﬁciency of existing momentum schemes for stochastic optimization. In 2018 Information Theory and Applications Workshop (ITA), pages 1–9. IEEE, 2018.
145. L. Kiefer, M. Storath, and A. Weinmann. Iterative potts minimization for the recovery of signals with discontinuities from indirect measurements: The multivariate case. Foundations of Computational Mathematics, pages 1–46, 2020.
146. D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.
147. V. V. Kniaz, V. A. Knyaz, V. Mizginov, A. Papazyan, N. Fomin, and L. Grodzitsky. Adversarial dataset augmentation using reinforcement learning and 3d modeling. In B. Kryzhanovsky, W. Dunin-Barkowski, V. Redko, and Y. Tiumentsev, editors, Advances in Neural Computation, Machine Learning, and Cognitive Research IV, pages 316–329, Cham, 2021. Springer International Publishing.
148. V. V. Kniaz, S. Y. Zheltov, F. Remondino, V. A. Knyaz, and A. Gruen. Wire structure imagebased 3d reconstruction aided by deep learning. volume XLIII-B2-2020, pages 435 – 441, Go¨ttingen, 2020. Copernicus. XXIV ISPRS Congress 2020 (virtual); Conference Location: Online; Conference Date: August 31 - September 2, 2020; Due to the Corona virus (COVID19) the conference was conducted virtually.
149. J. M. Kohler and A. Lucchi. Sub-sampled cubic regularization for non-convex optimization. In International Conference on Machine Learning, pages 1895–1904, 2017.
150. G. Kornowski and O. Shamir. Oracle complexity in nonsmooth nonconvex optimization. arXiv preprint arXiv:2104.06763, 2021.
151. D. Kovalev, S. Horva´th, and P. Richta´rik. Don’t jump through hoops and remove those loops: Svrg and katyusha are better without the outer loop. In Algorithmic Learning Theory, pages 451–467, 2020.
152. A. Krizhevsky, G. Hinton, et al. Learning multiple layers of features from tiny images. 2009. 153. P. Kuderov. and A. Panov. Planning with hierarchical temporal memory for deterministic
markov decision problem. In Proceedings of the 13th International Conference on Agents

Recent theoretical advances in non-convex optimization

75

and Artiﬁcial Intelligence - Volume 2: ICAART,, pages 1073–1081. INSTICC, SciTePress, 2021. 154. T. Lacroix, N. Usunier, and G. Obozinski. Canonical tensor decomposition for knowledge base completion. In International Conference on Machine Learning, pages 2863–2872, 2018. 155. G. Lan. First-order and Stochastic Optimization Methods for Machine Learning. Springer, 2020. 156. G. Lan and Y. Yang. Accelerated stochastic algorithms for nonconvex ﬁnite-sum and multiblock optimization. SIAM Journal on Optimization, 29(4):2753–2784, 2019. 157. J. Larson, M. Menickelly, and S. M. Wild. Derivative-free optimization methods. Acta Numerica, 28:287–404, May 2019. 158. J. C. H. Lee and P. Valiant. Optimizing star-convex functions. In 2016 IEEE 57th Annual Symposium on Foundations of Computer Science (FOCS), pages 603–614, 2016. 159. Y. Lei, T. Hu, G. Li, and K. Tang. Stochastic gradient descent for nonconvex learning without bounded gradient assumptions. IEEE Transactions on Neural Networks and Learning Systems, 2019. 160. K. Y. Levy. The power of normalization: Faster evasion of saddle points. arXiv preprint arXiv:1611.04831, 2016. 161. D. Li, T. Ding, and R. Sun. Over-parameterized deep neural networks have no strict local minima for any continuous activations. arXiv preprint arXiv:1812.11039, 2018. 162. Y. Li, K. Lee, and Y. Bresler. Identiﬁability in blind deconvolution with subspace or sparsity constraints. IEEE Transactions on information Theory, 62(7):4266–4275, 2016. 163. Z. Li, H. Bao, X. Zhang, and P. Richta´rik. Page: A simple and optimal probabilistic gradient estimator for nonconvex optimization. arXiv preprint arXiv:2008.10898, 2020. 164. Z. Li and P. Richta´rik. A uniﬁed analysis of stochastic gradient methods for nonconvex federated optimization. arXiv preprint arXiv:2006.07013, 2020. 165. S. Liang, R. Sun, Y. Li, and R. Srikant. Understanding the loss surface of neural networks for binary classiﬁcation. In International Conference on Machine Learning, pages 2835–2843, 2018. 166. S. Liu, B. Kailkhura, P.-Y. Chen, P. Ting, S. Chang, and L. Amini. Zeroth-order stochastic variance reduction for nonconvex optimization, 2018. 167. R. Livni, S. Shalev-Shwartz, and O. Shamir. On the computational efﬁciency of training neural networks. In Advances in neural information processing systems, pages 855–863, 2014. 168. N. Loizou, S. Vaswani, I. H. Laradji, and S. Lacoste-Julien. Stochastic polyak step-size for sgd: An adaptive learning rate for fast convergence. In International Conference on Artiﬁcial Intelligence and Statistics, pages 1306–1314. PMLR, 2021. 169. S. Lojasiewicz. A topological property of real analytic subsets. Coll. du CNRS, Les e´quations aux de´rive´es partielles, 117:87–89, 1963. 170. I. Loshchilov and F. Hutter. Sgdr: Stochastic gradient descent with warm restarts. arXiv preprint arXiv:1608.03983, 2016. 171. A. Lucchi and J. Kohler. A stochastic tensor method for non-convex optimization. arXiv preprint arXiv:1911.10367, 2019. 172. C. Ma, K. Wang, Y. Chi, and Y. Chen. Implicit regularization in nonconvex statistical estimation: Gradient descent converges linearly for phase retrieval and matrix completion. In International Conference on Machine Learning, pages 3345–3354. PMLR, 2018. 173. S. Ma, R. Bassily, and M. Belkin. The power of interpolation: Understanding the effectiveness of sgd in modern over-parametrized learning. In International Conference on Machine Learning, pages 3325–3334. PMLR, 2018. 174. J. Mairal. Incremental majorization-minimization optimization with application to largescale machine learning. SIAM Journal on Optimization, 25(2):829–855, 2015. 175. D. Malik, A. Pananjady, K. Bhatia, K. Khamaru, P. L. Bartlett, and M. J. Wainwright. Derivative-free methods for policy optimization: Guarantees for linear quadratic systems, 2020. 176. J. Martens. Deep learning via hessian-free optimization. In International Conference on Machine Learning, volume 27, pages 735–742, 2010.

76

Danilova, Dvurechensky, Gasnikov, Gorbunov, Guminov, Kamzolov, Shibaev

177. T. Mikolov. Statistical language models based on neural networks. Presentation at Google, Mountain View, 2nd April, 80, 2012.
178. K. Mishchenko, E. Gorbunov, M. Taka´cˇ, and P. Richta´rik. Distributed learning with compressed gradient differences. arXiv preprint arXiv:1901.09269, 2019.
179. K. Mishchenko, A. Khaled, and P. Richta´rik. Random reshufﬂing: Simple analysis with vast improvements. arXiv preprint arXiv:2006.05988, 2020.
180. K. G. Murty and S. N. Kabadi. Some np-complete problems in quadratic and nonlinear programming. Mathematical Programming, 39(2):117–129, Jun 1987.
181. J. A. Nelder and R. Mead. A simplex method for function minimization. The computer journal, 7(4):308–313, 1965.
182. A. Nemirovski. Orth-method for smooth convex optimization. Izvestia AN SSSR, Transl.: Eng. Cybern. Soviet J. Comput. Syst. Sci , 2:937–947, 1982.
183. Y. Nesterov. A method of solving a convex programming problem with convergence rate o(1/k2). Soviet Mathematics Doklady, 27(2):372–376, 1983.
184. Y. Nesterov. Introductory Lectures on Convex Optimization: a basic course . Kluwer Academic Publishers, Massachusetts, 2004.
185. Y. Nesterov. How to make the gradients small. Optima, 88:10–11, 2012. 186. Y. Nesterov. Lectures on convex optimization, volume 137. Springer, 2018. 187. Y. Nesterov, A. Gasnikov, S. Guminov, and P. Dvurechensky. Primal-dual accelerated gradi-
ent methods with small-dimensional relaxation oracle. Optimization Methods and Software, pages 1–28, 2020. arXiv:1809.05895. 188. Y. Nesterov and B. Polyak. Cubic regularization of newton method and its global performance. Mathematical Programming, 108(1):177–205, 2006. 189. Y. Nesterov and V. Spokoiny. Random gradient-free minimization of convex functions. Found. Comput. Math., 17(2):527–566, Apr. 2017. First appeared in 2011 as CORE discussion paper 2011/16. 190. B. Neyshabur, S. Bhojanapalli, D. McAllester, and N. Srebro. Exploring generalization in deep learning. In Advances in neural information processing systems , pages 5947–5956, 2017. 191. L. M. Nguyen, J. Liu, K. Scheinberg, and M. Taka´cˇ. Sarah: A novel method for machine learning problems using stochastic recursive gradient. In International Conference on Machine Learning, pages 2613–2621, 2017. 192. L. M. Nguyen, J. Liu, K. Scheinberg, and M. Taka´cˇ. Stochastic recursive gradient algorithm for nonconvex optimization. arXiv preprint arXiv:1705.07261, 2017. 193. L. M. Nguyen, Q. Tran-Dinh, D. T. Phan, P. H. Nguyen, and M. van Dijk. A uniﬁed convergence analysis for shufﬂing-type gradient methods. arXiv preprint arXiv:2002.08246, 2020. 194. Q. Nguyen, M. C. Mukkamala, and M. Hein. On the loss landscape of a class of deep neural networks with no bad local valleys. arXiv preprint arXiv:1809.10749, 2018. 195. J. Nocedal and S. Wright. Numerical optimization. Springer Science & Business Media, 2006. 196. K. Osawa, Y. Tsuji, Y. Ueno, A. Naruse, R. Yokota, and S. Matsuoka. Second-order optimization method for large mini-batch: Training resnet-50 on imagenet in 35 epochs. arXiv preprint arXiv:1811.12019, 1:2, 2018. 197. N. Papernot, P. McDaniel, I. Goodfellow, S. Jha, Z. B. Celik, and A. Swami. Practical blackbox attacks against machine learning, 2017. 198. V. Papyan, Y. Romano, J. Sulam, and M. Elad. Convolutional dictionary learning via local processing. In Proceedings of the IEEE International Conference on Computer Vision, pages 5296–5304, 2017. 199. S. Park, S. H. Jung, and P. M. Pardalos. Combining stochastic adaptive cubic regularization with negative curvature for nonconvex optimization. Journal of Optimization Theory and Applications, 184(3):953–971, 2020. 200. R. Pascanu, T. Mikolov, and Y. Bengio. On the difﬁculty of training recurrent neural networks. In International conference on machine learning, pages 1310–1318, 2013. 201. B. Polyak. Gradient methods for the minimisation of functionals. USSR Computational Mathematics and Mathematical Physics, 3(4):864 – 878, 1963.

Recent theoretical advances in non-convex optimization

77

202. B. Polyak. Introduction to Optimization. New York, Optimization Software, 1987. 203. B. T. Polyak. Some methods of speeding up the convergence of iteration methods. USSR
Computational Mathematics and Mathematical Physics, 4(5):1–17, 1964. 204. Q. Qu, X. Li, and Z. Zhu. A nonconvex approach for exact and efﬁcient multichannel sparse
blind deconvolution. In Advances in Neural Information Processing Systems, pages 4015– 4026, 2019. 205. S. Rajput, A. Gupta, and D. Papailiopoulos. Closing the convergence gap of sgd without replacement. arXiv preprint arXiv:2002.10400, 2020. 206. S. J. Reddi, A. Hefny, S. Sra, B. Poczos, and A. Smola. Stochastic variance reduction for nonconvex optimization. In International conference on machine learning, pages 314–323, 2016. 207. S. J. Reddi, S. Kale, and S. Kumar. On the convergence of adam and beyond. arXiv preprint arXiv:1904.09237, 2019. 208. S. J. Reddi, S. Sra, B. Poczos, and A. J. Smola. Proximal stochastic methods for nonsmooth nonconvex ﬁnite-sum optimization. In Advances in Neural Information Processing Systems, pages 1145–1153, 2016. 209. A. Rezanov and D. Yudin. Deep neural networks for ortophoto-based vehicle localization. In B. Kryzhanovsky, W. Dunin-Barkowski, V. Redko, and Y. Tiumentsev, editors, Advances in Neural Computation, Machine Learning, and Cognitive Research IV, pages 167–174, Cham, 2021. Springer International Publishing. 210. A. Risteski and Y. Li. Algorithms and matching lower bounds for approximately-convex optimization. In NIPS, 2016. 211. A. Roy, K. Balasubramanian, S. Ghadimi, and P. Mohapatra. Escaping saddle-point faster under interpolation-like conditions. Advances in Neural Information Processing Systems, 33, 2020. 212. C. W. Royer and S. J. Wright. Complexity analysis of second-order line-search algorithms for smooth nonconvex optimization. SIAM Journal on Optimization, 28(2):1448–1477, 2018. 213. I. Safran and O. Shamir. Spurious local minima are common in two-layer relu neural networks. In International Conference on Machine Learning, pages 4433–4441. PMLR, 2018. 214. K. A. Sankararaman, S. De, Z. Xu, W. R. Huang, and T. Goldstein. The impact of neural network overparameterization on gradient confusion and stochastic gradient descent. arXiv preprint arXiv:1904.06963, 2019. 215. M. Schmidt, N. Le Roux, and F. Bach. Minimizing ﬁnite sums with the stochastic average gradient. Mathematical Programming, 162(1-2):83–112, 2017. 216. M. Schmidt and N. L. Roux. Fast convergence of stochastic gradient descent under a strong growth condition. arXiv preprint arXiv:1308.6370, 2013. 217. M. Schumer and K. Steiglitz. Adaptive step size random search. IEEE Transactions on Automatic Control, 13(3):270–276, June 1968. 218. O. Sebbouh, R. M. Gower, and A. Defazio. On the convergence of the stochastic heavy ball method. arXiv preprint arXiv:2006.07867, 2020. 219. O. Sener and V. Koltun. Learning to guide random search. In International Conference on Learning Representations, 2020. 220. S. Shalev-Shwartz. Sdca without duality, regularization, and individual convexity. In International Conference on Machine Learning, pages 747–754, 2016. 221. Y. Shechtman, Y. C. Eldar, O. Cohen, H. N. Chapman, J. Miao, and M. Segev. Phase retrieval with application to optical imaging: a contemporary overview. IEEE signal processing magazine, 32(3):87–109, 2015. 222. Z. Shen, P. Zhou, C. Fang, and A. Ribeiro. A stochastic trust region method for non-convex minimization. arXiv preprint arXiv:1903.01540, 2019. 223. B. Shi, W. J. Su, and M. I. Jordan. On learning rates and schr\” odinger operators. arXiv preprint arXiv:2004.06977, 2020. 224. L. Shi and Y. Chi. Manifold gradient descent solves multi-channel sparse blind deconvolution provably and efﬁciently. In ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 5730–5734. IEEE, 2020.

78

Danilova, Dvurechensky, Gasnikov, Gorbunov, Guminov, Kamzolov, Shibaev

225. N. Shi, D. Li, M. Hong, and R. Sun. Rmsprop converges with proper hyper-parameter. In International Conference on Learning Representations, 2021.
226. I. Shibaev, P. Dvurechensky, and A. Gasnikov. Zeroth-order methods for noisy Ho¨lder-gradient functions. Optimization Letters, 2021. (accepted), arXiv:2006.11857, doi:10.1007/s11590-021-01742-z.
227. Y. Shin. Effects of depth, width, and initialization: A convergence analysis of layer-wise training for deep linear neural networks. arXiv preprint arXiv:1910.05874, 2019.
228. N. Z. Shor. Generalized gradient descent with application to block programming. Kibernetika, 3(3):53–55, 1967.
229. A. Skrynnik, A. Staroverov, E. Aitygulov, K. Aksenov, V. Davydov, and A. I. Panov. Forgetful experience replay in hierarchical reinforcement learning from expert demonstrations. Knowledge-Based Systems, 218:106844, 2021.
230. L. N. Smith. Cyclical learning rates for training neural networks. In 2017 IEEE Winter Conference on Applications of Computer Vision (WACV), pages 464–472. IEEE, 2017.
231. M. V. Solodov. Incremental gradient algorithms with stepsizes bounded away from zero. Computational Optimization and Applications, 11(1):23–35, 1998.
232. M. Soltanolkotabi, A. Javanmard, and J. D. Lee. Theoretical insights into the optimization landscape of over-parameterized shallow neural networks. IEEE Transactions on Information Theory, 65(2):742–769, 2018.
233. V. Spokoiny et al. Parametric estimation. ﬁnite sample theory. The Annals of Statistics, 40(6):2877–2909, 2012.
234. F. Stonyakin, A. Tyurin, A. Gasnikov, P. Dvurechensky, A. Agafonov, D. Dvinskikh, M. Alkousa, D. Pasechnyuk, S. Artamonov, and V. Piskunova. Inexact model: A framework for optimization and variational inequalities. Optimization Methods and Software, 2021. (accepted), WIAS Preprint No. 2709, arXiv:2001.09013, arXiv:1902.00990, doi:10.1080/10556788.2021.1924714.
235. F. S. Stonyakin, D. Dvinskikh, P. Dvurechensky, A. Kroshnin, O. Kuznetsova, A. Agafonov, A. Gasnikov, A. Tyurin, C. A. Uribe, D. Pasechnyuk, and S. Artamonov. Gradient methods for problems with inexact model of the objective. In M. Khachay, Y. Kochetov, and P. Pardalos, editors, Mathematical Optimization Theory and Operations Research , pages 97–114, Cham, 2019. Springer International Publishing. arXiv:1902.09001.
236. R. Sun. Optimization for deep learning: theory and algorithms. arXiv preprint arXiv:1912.08957, 2019.
237. I. Surazhevsky, V. Demin, A. Ilyasov, A. Emelyanov, K. Nikiruy, V. Rylkov, S. Shchanikov, I. Bordanov, S. Gerasimova, D. Guseinov, N. Malekhonova, D. Pavlov, A. Belov, A. Mikhaylov, V. Kazantsev, D. Valenti, B. Spagnolo, and M. Kovalchuk. Noise-assisted persistence and recovery of memory state in a memristive spiking neuromorphic network. Chaos, Solitons & Fractals, 146:110890, 2021.
238. I. Sutskever, J. Martens, G. Dahl, and G. Hinton. On the importance of initialization and momentum in deep learning. In International conference on machine learning, pages 1139– 1147, 2013.
239. G. Swirszcz, W. M. Czarnecki, and R. Pascanu. Local minima in training of deep networks. 2016.
240. Y. S. Tan and R. Vershynin. Online stochastic gradient descent with arbitrary initialization solves non-smooth, non-convex phase retrieval. arXiv preprint arXiv:1910.12837, 2019.
241. W. Tao, Z. Pan, G. Wu, and Q. Tao. Primal averaging: A new gradient evaluation step to attain the optimal individual convergence. IEEE transactions on cybernetics, 50(2):835–845, 2018.
242. A. Taylor and F. Bach. Stochastic ﬁrst-order methods: non-asymptotic and computer-aided analyses via potential functions. In Conference on Learning Theory, pages 2934–2992, 2019.
243. T. Tieleman and G. Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural networks for machine learning, 4(2):26–31, 2012.

Recent theoretical advances in non-convex optimization

79

244. N. Tripuraneni, M. Stern, C. Jin, J. Regier, and M. I. Jordan. Stochastic cubic regularization for fast nonconvex optimization. In Advances in neural information processing systems, pages 2899–2908, 2018.
245. P. Tseng. An incremental gradient (-projection) method with momentum term and adaptive stepsize rule. SIAM Journal on Optimization, 8(2):506–531, 1998.
246. I. Usmanova. Robust solutions to stochastic optimization problems. Master Thesis (MSIAM); Institut Polytechnique de Grenoble ENSIMAG, Laboratoire Jean Kuntzmann, 2017.
247. A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin. Attention is all you need. In Advances in neural information processing systems, pages 5998–6008, 2017.
248. S. Vaswani, F. Bach, and M. Schmidt. Fast and faster convergence of sgd for overparameterized models and an accelerated perceptron. In The 22nd International Conference on Artiﬁcial Intelligence and Statistics, pages 1195–1204. PMLR, 2019.
249. S. Vaswani, A. Mishkin, I. Laradji, M. Schmidt, G. Gidel, and S. Lacoste-Julien. Painless stochastic gradient: Interpolation, line-search, and convergence rates. In Advances in Neural Information Processing Systems, pages 3732–3745, 2019.
250. S. A. Vavasis. Black-box complexity of local minimization. SIAM Journal on Optimization, 3(1):60–80, 1993.
251. R. Vidal, J. Bruna, R. Giryes, and S. Soatto. Mathematics of deep learning. arXiv preprint arXiv:1712.04741, 2017.
252. Z. Wang, K. Ji, Y. Zhou, Y. Liang, and V. Tarokh. Spiderboost: A class of faster variancereduced algorithms for nonconvex optimization. arXiv preprint arXiv:1810.10690, 2018.
253. Z. Wang, K. Ji, Y. Zhou, Y. Liang, and V. Tarokh. Spiderboost and momentum: Faster variance reduction algorithms. In Advances in Neural Information Processing Systems, pages 2403–2413, 2019.
254. Z. Wang, Y. Zhou, Y. Liang, and G. Lan. Stochastic variance-reduced cubic regularization for nonconvex optimization. In The 22nd International Conference on Artiﬁcial Intelligence and Statistics, pages 2731–2740. PMLR, 2019.
255. Z. Wang, Y. Zhou, Y. Liang, and G. Lan. Cubic regularization with momentum for nonconvex optimization. In Uncertainty in Artiﬁcial Intelligence, pages 313–322. PMLR, 2020.
256. R. Ward, X. Wu, and L. Bottou. Adagrad stepsizes: Sharp convergence over nonconvex landscapes. In International Conference on Machine Learning, pages 6677–6686. PMLR, 2019.
257. A. C. Wilson, R. Roelofs, M. Stern, N. Srebro, and B. Recht. The marginal value of adaptive gradient methods in machine learning. In Advances in neural information processing systems, pages 4148–4158, 2017.
258. S. J. Wright. Optimization algorithms for data analysis. The Mathematics of Data, 25:49, 2018.
259. F. Wu and P. Rebeschini. Hadamard wirtinger ﬂow for sparse phase retrieval. arXiv preprint arXiv:2006.01065, 2020.
260. G. Xie, L. Luo, and Z. Zhang. A general analysis framework of lower complexity bounds for ﬁnite-sum optimization. arXiv preprint arXiv:1908.08394, 2019.
261. P. Xu, J. Chen, D. Zou, and Q. Gu. Global convergence of langevin dynamics based algorithms for nonconvex optimization. In Advances in Neural Information Processing Systems, pages 3122–3133, 2018.
262. P. Xu, F. Roosta, and M. W. Mahoney. Newton-type methods for non-convex optimization under inexact hessian information. Mathematical Programming, 184(1):35–70, 2020.
263. P. Xu, F. Roosta, and M. W. Mahoney. Second-order optimization for non-convex machine learning: An empirical study. In Proceedings of the 2020 SIAM International Conference on Data Mining, pages 199–207. SIAM, 2020.
264. Y. Xu. Momentum-based variance-reduced proximal stochastic gradient method for composite nonconvex stochastic optimization. arXiv preprint arXiv:2006.00425, 2020.

80

Danilova, Dvurechensky, Gasnikov, Gorbunov, Guminov, Kamzolov, Shibaev

265. Y. Xu, R. Jin, and T. Yang. First-order stochastic algorithms for escaping from saddle points in almost linear time. In Advances in Neural Information Processing Systems, pages 5530– 5540, 2018.
266. Y. Yan, T. Yang, Z. Li, Q. Lin, and Y. Yang. A uniﬁed analysis of stochastic momentum methods for deep learning. In Proceedings of the 27th International Joint Conference on Artiﬁcial Intelligence, pages 2955–2961, 2018.
267. Z. Yang, L. F. Yang, E. X. Fang, T. Zhao, Z. Wang, and M. Neykov. Misspeciﬁed nonconvex statistical optimization for sparse phase retrieval. Mathematical Programming, 176(12):545–571, 2019.
268. C. Yun, S. Sra, and A. Jadbabaie. Small nonlinearities in activation functions create bad local minima in neural networks. arXiv preprint arXiv:1802.03487, 2018.
269. J. Yun, A. C. Lozano, and E. Yang. A general family of stochastic proximal gradient methods for deep learning. arXiv preprint arXiv:2007.07484, 2020.
270. M. Zaheer, S. Reddi, D. Sachan, S. Kale, and S. Kumar. Adaptive methods for nonconvex optimization. In Advances in neural information processing systems, pages 9793–9803, 2018.
271. B. Zhang, J. Jin, C. Fang, and L. Wang. Improved analysis of clipping algorithms for nonconvex optimization. Advances in Neural Information Processing Systems, 33, 2020.
272. C. Zhang, S. Bengio, M. Hardt, B. Recht, and O. Vinyals. Understanding deep learning (still) requires rethinking generalization. Communications of the ACM, 64(3):107–115, 2021.
273. H. Zhang, Y. Bi, and J. Lavaei. General low-rank matrix optimization: Geometric analysis and sharper bounds. arXiv preprint arXiv:2104.10356, 2021.
274. J. Zhang, T. He, S. Sra, and A. Jadbabaie. Why gradient clipping accelerates training: A theoretical justiﬁcation for adaptivity. In International Conference on Learning Representations, 2020.
275. J. Zhang, S. P. Karimireddy, A. Veit, S. Kim, S. Reddi, S. Kumar, and S. Sra. Why are adaptive methods good for attention models? Advances in Neural Information Processing Systems, 33, 2020.
276. J. Zhang and L. Xiao. Stochastic variance-reduced prox-linear algorithms for nonconvex composite optimization. arXiv preprint arXiv:2004.04357, 2020.
277. J. Zhang, L. Xiao, and S. Zhang. Adaptive stochastic variance reduction for subsampled newton method with cubic regularization. arXiv preprint arXiv:1811.11637, 2018.
278. R. Y. Zhang. Sharp global guarantees for nonconvex low-rank matrix recovery in the overparameterized regime. arXiv preprint arXiv:2104.10790, 2021.
279. Y. Zhang, Q. Qu, and J. Wright. From symmetry to geometry: Tractable nonconvex problems. arXiv preprint arXiv:2007.06753, 2020.
280. Y. Zhang, Q. Qu, and J. Wright. From symmetry to geometry: Tractable nonconvex problems. arXiv preprint arXiv:2007.06753, 2020.
281. Y. Zhang, Y. Zhou, K. Ji, and M. M. Zavlanos. Boosting one-point derivative-free online optimization via residual feedback, 2020.
282. A. Zhigljavsky and A. Zilinskas. Stochastic global optimization, volume 9. Springer Science & Business Media, 2007.
283. D. Zhou and Q. Gu. Lower bounds for smooth nonconvex ﬁnite-sum optimization. In International Conference on Machine Learning, pages 7574–7583, 2019.
284. D. Zhou and Q. Gu. Stochastic recursive variance-reduced cubic regularization methods. In International Conference on Artiﬁcial Intelligence and Statistics, pages 3980–3990. PMLR, 2020.
285. D. Zhou, Y. Tang, Z. Yang, Y. Cao, and Q. Gu. On the convergence of adaptive gradient methods for nonconvex optimization. arXiv preprint arXiv:1808.05671, 2018.
286. D. Zhou, P. Xu, and Q. Gu. Stochastic nested variance reduced gradient descent for nonconvex optimization. Advances in neural information processing systems, 2018.
287. D. Zhou, P. Xu, and Q. Gu. Stochastic variance-reduced cubic regularization methods. Journal of Machine Learning Research, 20(134):1–47, 2019.
288. D. Zhou, P. Xu, and Q. Gu. Stochastic variance-reduced cubic regularization methods. Journal of Machine Learning Research, 20(134):1–47, 2019.

Recent theoretical advances in non-convex optimization

81

289. X. Zhu, J. Han, and B. Jiang. An adaptive high order method for ﬁnding third-order critical points of nonconvex optimization. arXiv preprint arXiv:2008.04191, 2020.

