arXiv:1912.13188v1 [stat.ME] 31 Dec 2019

On Testing for Biases in Peer Review
Ivan Stelmakh, Nihar B. Shah and Aarti Singh
School of Computer Science Carnegie Mellon University {stiv,nihars,aarti}@cs.cmu.edu
Abstract We consider the issue of biases in scholarly research, speciﬁcally, in peer review. There is a long standing debate on whether exposing author identities to reviewers induces biases against certain groups, and our focus is on designing tests to detect the presence of such biases. Our starting point is a remarkable recent work by Tomkins, Zhang and Heavlin which conducted a controlled, large-scale experiment to investigate existence of biases in the peer reviewing of the WSDM conference. We present two sets of results in this paper. The ﬁrst set of results is negative, and pertains to the statistical tests and the experimental setup used in the work of Tomkins et al. We show that the test employed therein does not guarantee control over false alarm probability and under correlations between relevant variables coupled with any of the following conditions, with high probability, can declare a presence of bias when it is in fact absent: (a) measurement error, (b) model mismatch, (c) reviewer calibration. Moreover, we show that the setup of their experiment may itself inﬂate false alarm probability if (d) bidding is performed in non-blind manner or (e) popular reviewer assignment procedure is employed. Our second set of results is positive and is built around a novel approach to testing for biases that we propose. We present a general framework for testing for biases in (single vs. double blind) peer review. We then design hypothesis tests that under minimal assumptions guarantee control over false alarm probability and non-trivial power even under conditions (a)–(c) as well as propose an alternative experimental setup which mitigates issues (d) and (e). Finally, we show that no statistical test can improve over the non-parametric tests we consider in terms of the assumptions required to control for the false alarm probability.
1 Introduction
Past research in social sciences indicates that humans display various biases including gender, race and age biases in many critical domains such as hiring (Bertrand and Mullainathan, 2004), university admission (Thornhill, 2018), bail decisions (Arnold et al., 2018) and many others. Our focus is on fairness in academia and scholarly research, and speciﬁcally, on biases in peer review. Peer review is a backbone of scholarly research and is employed by a vast majority of journals and conferences. Due to the widespread prevalence of the Matthew eﬀect – rich get richer and poor get poorer – in academia (Thorngate and Chowdhury, 2014; Squazzoni and Gandelli, 2012), any biases in peer review can have far reaching consequences on career trajectories of researchers. Speciﬁcally, we follow the long-standing debate (Blank, 1991; Seeber and Bacchelli, 2017; Snodgrass, 2006; Largent and Snodgrass, 2016; Okike et al., 2016; Budden et al., 2008; Webb et al., 2008; Hill and J. Provost, 2003, and references therein) on whether the authors’ identities should be hidden from reviewers or not. The focus of this paper is on designing statistical tests to detect the presence of biases in peer review.
In a recent remarkable piece of work, Tomkins et al. (2017) conducted a large scale (semi-) randomized controlled trial during the peer review for the ACM International Conference on Web Search and Data Mining (WSDM) 2017. In their experiment, the entire pool of reviewers was partitioned uniformly at random into
1

False alarm probability
0.0 0.2 0.4 0.6 0.8 1.0
Probability of detection
0.0 0.2 0.4 0.6 0.8 1.0
Probability of detection
0.0 0.2 0.4 0.6 0.8 1.0

Previous work Disagreement test

200 400 600 800 1000
Number of papers
(a) Bias is absent. Any valid test must have false alarm probability below 0.05.

200 400 600 800 1000
Number of papers
(b) Bias is present. Higher probability of detection is better.

200 400 600 800 1000
Number of papers
(c) Bias is present. Higher probability of detection is better.

Figure 1: Synthetic simulations evaluating performance of the test in Tomkins et al. (2017) (“previous work”) and the test proposed in this paper (“Disagreement test”). Subﬁgures (a) and (b) are in presence of correlations and noisy estimates of true scores by double-blind reviewers; subﬁgure (c) has zero correlations and perfect estimate of true scores by double-blind reviewers. Details of the simulation setup are provided in Section 3. The error bars are too small to be visible.

two equal groups – single blind and double blind – and each paper was assigned to two reviewers from each of the groups. In this manner, the peer-review data contained both single-blind and double-blind reviews for each paper. The experiment allowed them to conduct a causal inference to test for biases, and conclude that the single-blind system induces a bias in favor of papers authored by (i) researchers from top-universities, (ii) researchers from top companies and (iii) famous authors. Interestingly, no bias against female-authored submissions was detected by their test, though a meta-analysis conﬁrmed the presence of such bias. The conclusions of this experiment have had a signiﬁcant impact. For instance, the WSDM conference itself completely switched to double-blind peer review starting 2018.
Testing for the presence of hypothesized phenomena is a common task in various branches of science including the biological, social, and physical sciences. The general approach therein is to impose a hard constraint on the probability of false alarm (claiming existence of the phenomenon when there is none; also called Type-I error) to some predeﬁned threshold called signiﬁcance level typically set as 0.05 or 0.01. The test would then aim to maximize the probability of detecting the phenomenon when it is actually present, while not violating the aforementioned hard constraint. The present paper also follows this general approach, for the speciﬁc setting of testing for biases using single versus double blind reviewing.
Contributions. In this paper, we study the problem of detecting bias in peer review, and present two sets of results.
(1) Detailed investigation into methodology of past work (Section 3) We ﬁrst analyze the testing procedure used by Tomkins et al. (2017), and show that under plausible conditions the statistical test employed therein does not control for false alarm probability. In other words, we show that under reasonable conditions, the test used by Tomkins et al. (2017) can, with probability as large as 0.5 or higher, declare the presence of a bias when the bias is in fact absent (even when the test is tuned to have a false alarm error rate below 0.05). Speciﬁcally, we show that in presence of correlations that are reasonable to expect, any of the following factors breaks their false alarm probability guarantees: (a) measurement error caused by noise or subjectivity of reviewers, (b) model mismatch caused by violation of strong parametric assumptions on reviewers’ behavior and (c) reviewer’s calibration if she/he reviews more than one paper. Figures 1a and 1b illustrate the eﬀect of measurement error on the false alarm probability and probability of detection of the test used by Tomkins et al. The issues we identify suggest that their test is at risk of committing Type-I

2

error in declaring biases in their analysis. Moving beyond the speciﬁc test used in Tomkins et al. (2017), we also study the eﬀect of their experimental
design, which is simply the standard peer-review procedure with an additional random partition of reviewers into single and double blind groups. We show that two factors – (d) asymmetrical bidding procedure and (e) non-random assignment of papers to referees – as is common in peer-review procedures today may introduce spurious correlations in the data, breaking some key independence assumptions and thereby violating the requisite guarantees on testing.
(2) Novel approach to testing for biases (Sections 4 - 6) We propose a general framework for the design of statistical tests to detect biases in this problem setting, that overcomes the aforementioned limitations. Speciﬁcally, our framework does not assume objectivity of reviewers and does not make any parametric assumptions on reviewers’ behaviour. Conceptually, we propose to think of this problem as an instance of a two-sample testing problem where single-blind and double-blind reviews form two samples and the test operates on these samples. (In contrast, Tomkins et al. (2017) study the problem under one-sample testing paradigm, operating on reviews of single-blind reviewers and using double-blind reviews to estimate some parameters in their parametric model).
We then design computationally-eﬃcient hypothesis testing procedures that under minimal assumptions guarantee a provable control over the false alarm probability under various conditions, including aforementioned conditions (a) - (c). We supplement these tests with an alternative design of the experimental setup which coupled with our tests mitigates issues (d) - (e) while not restricting the choice of assignment algorithm.
Our tests also have non-trivial power in that they have considerably higher probability of detection in hard cases where test used by Tomkins et al. fails, and a power comparable to that of Tomkins et al. when their assumptions are exactly met. The performance of one of these tests is illustrated in Figure 1. Additionally, we show that assumptions required by our tests to control for the Type-I error rate are essentially minimal in that they cannot be further relaxed without making reliable testing impossible.
We note that while the discussion in this paper focuses on testing for biases with respect to protected attributes, our experimental setup and statistical tests are not restricted to that alone. Instead of comparing the single versus double blind settings, our work can be used to test for eﬀects of aspects of a submission exogenous to the manuscript’s content, for instance, the eﬀects of the reviewer questionnaire or that of asking authors to provide extraneous information (such as prior submission history). Our work enables conducting such semi-randomized controlled trials while retaining the no-bias and veracity conditions (Tomkins et al., 2017), not requiring additional reviews, and having rigorous guarantees on the tests.
Related work. The problem of identifying biases in human decisions is commonly studied in social science and there are many works that design and conduct randomized ﬁeld experiments in various settings, including resume screening (Bertrand and Mullainathan, 2004), hiring in academia (Moss-Racusin et al., 2012), and peer review (Blank, 1991; Okike et al., 2016). However, the conference peer review setup we consider in this work does not comprise a fully randomized control trial (i.e., the reviewers are not assigned to submissions at random) and past approaches fail due to idiosyncrasies of the peer-review process. For example, a popular approach (Bertrand and Mullainathan, 2004; Moss-Racusin et al., 2012) is to assign author identities to (fabricated) documents (resumes, application packages or papers) uniformly at random and compare the outcomes for diﬀerent categories of authors. In our setup, random assignment of author identities to real (i.e., non-fabricated) submissions is problematic due to various logistical and ethical issues such as reviewers guessing actual authors thereby causing biases, and requrements of getting authors to agree to have their paper/name modiﬁed. Another approach (Okike et al., 2016) is to submit the same paper to multiple reviewers in both single-blind and double-blind conditions and test for the diﬀerence in the acceptance rates between conditions. However, such an approach necessitates a considerable additional reviewing load. Other approaches include observational studies, and we refer the interested readers to Tomkins et al. (2017) for a more in-depth literature review.
It is important to note that in this work, we do not aim to prove or disprove the existence of biases declared in the experiment by Tomkins et al. (2017). Instead, our focus is on the theoretical validity of the statistical procedures used to conduct such experiments and more generally on principled statistical approach towards
3

designing such experiments. Finally, the results and tests we discuss in this work are also applicable beyond peer review, and can be
used to test for biases in other domains such as admissions and hiring. The remainder of this paper is organized as follows. In Section 2 we present the problem setting formally
and describe the experimental setup of Tomkins et al. (2017). In Section 3 we uncover issues (a) - (e) with their test and setup and illustrate the detrimental eﬀect of such issues through simulations. Next, in Sections 4 and 5 we present a novel non-parametric approach to testing for biases and corresponding statistical tests as well as the alternative design of the experimental procedure. The detailed analysis is given in Section 6. We conclude the paper with a discussion in Section 7.

2 Preliminaries

The general peer-review setup we study for testing biases using single and double blind review is as considered

in Tomkins et al. (2017). We study a conference peer-review setup where n papers are submitted at once and

m independent reviewers are available to review submissions, where m is assumed to be an even number.

With a goal to test whether single-blind reviewing induces a bias against or in favor of some groups of authors,

we consider some pre-deﬁned set of k binary mutually non-exclusive properties pertaining to the author(s) of

any paper to be tested for bias. For example, a property could be “the ﬁrst author is female” or “majority of

authors

are

from

the

USA”.

Each

paper

j

∈

[n]

is

then

associated

with

k

indicator

variables

wj(1)

,

.

.

.

,

w

(k j

)

,

where wj( ) = 1 if paper j satisﬁes property and wj( ) = −1 otherwise. For each ∈ [k] we let J ⊆ [n] denote the set of papers that satisfy property and J = [n]\J denote its complement.1

For each property ∈ [k] we are interested in whether single-blind peer review setup induces a bias against

or in favor of papers that satisfy this property. For example, if we consider property “the ﬁrst author is

female”, then we aim at testing for the bias against or in favor of papers with female ﬁrst author. Note

that with respect to the properties, the study is observational in that we cannot assign author identities to

papers at random. Hence, the eﬀect of confounding is unavoidable and utmost care must be taken to address

presence of confounding factors.

For brevity, in the main text we consider the case of a single property of interest (k = 1) which captures

the complexity of our problem. For ease of notation we drop index from w( ) and J . In Appendix A we

generalize the results to k > 1. Let us now give details of the testing procedure used by Tomkins et al. (2017).

Experimental setup of Tomkins et al. The peer review process in their experiment is organized as follows. Reviewers are uniformly at random divided into two groups of equal sizes, corresponding to two conditions: (i) Double-Blind condition (DB) in which reviewers do not observe identities of papers’ authors; and (ii) Single-Blind condition (SB) in which reviewers observe identities of the papers’ authors. Next, each paper is assigned to λ reviewers from the SB group and λ reviewers from the DB group such that each reviewer reviews at most µ submissions, where λ and µ are predeﬁned constants. In both conditions, if any reviewer i ∈ [m] is assigned to any paper j ∈ [n], then she/he returns a binary accept/reject recommendation and possibly a numeric score that estimates a quality of the paper as perceived by reviewer, accompanied by a textual review.

Model and test used by Tomkins et al. We begin by introducing an idealized version of their model. They assume a parametric, logistic model for the binary decisions made by SB reviewers. Speciﬁcally, for each paper j ∈ [n], let Y1j, . . . , Yλj denote the binary accept/reject decisions given by the λ reviewers assigned to paper j in the SB setup. It is assumed that {Yrj}r∈[λ] are independent draws from a Bernoulli random variable with an expectation πj satisfying

log πj = β0 + β1q∗ + β2wj,

(1)

1 − πj

j

where qj∗ is a “true” underlying score of paper j, wj is an indicator of property satisfaction and {β0, β1, β2} are unknown coeﬃcients. In words, the model says that if there is a positive (respectively negative) bias

1Here, we adopt the standard notation [ν] = {1, 2, . . . , ν} for any positive integer ν.

4

with respect to a property of interest, then the fact that paper satisﬁes the property increases (respectively decreases) the log-odds of the probability of recommending acceptance by 2β2 as compared to the case if the same paper does not satisfy the property. The main diﬃculty with this model in the peer review setting lies in the fact that true scores {qj∗, j ∈ [n]} are unknown and hence standard tests for logistic regression model are not readily applicable.
In order to overcome the unavailability of true scores {qj∗, j ∈ [n]} in the model (1), Tomkins et al. (2017) use a plug-in estimate: they replace qj∗ with the mean qj of scores given by the DB reviewers to paper j, for every j ∈ [n]. Under this approximation and using q1, . . . , qn, they obtain maximum likelihood estimates of coeﬃcients {β0, β1, β2} and then use the standard Wald test (Weisberg, 2005) to test for signiﬁcance of the coeﬃcient β2. A bias is declared present if the coeﬃcient β2 is found signiﬁcant; the direction of the bias is determined as the sign of β2.
3 Problems with the past approach
In this section we identify several issues that should be taken into account when testing for biases in the setup we consider. Noting that the issues themselves are general, we motivate and discuss them in context of the prior work by Tomkins et al. (2017) and investigate possible consequences of these issues through synthetic simulations. In the simulations to follow, we juxtapose algorithm by Tomkins et al. (2017) to our Disagreement test introduced later in the paper. Complete details of all simulations are given in Appendix E.
3.1 Testing procedure
We begin from the issues that are pertinent to the testing procedure used by Tomkins et al. (2017). To this end, recall that with respect to the property of interest the experiment is observational. Hence we cannot assume independence between the indicator of property satisfaction w and the true score q∗. Moreover, a non-trivial amount of correlation between some properties is plausible. Consider for example a property “paper has author from top univeristy”. For this property a non-trivial correlation between true scores and indicator of property satisfaction is natural to expect. While correlation itself does not cause issues, we identify three conditions which coupled with correlation can be signiﬁcantly harmful.
(a) Measurement error. Tomkins et al. (2017) report low interreviewer agreement between DB reviewers which means that the estimates q1, . . . , qn of the true scores by the DB reviewers are noisy. It is known (Stefanski and Carroll, 1985; Brunner and Austin, 2009) that noisy covariate measurement coupled with correlation between some covariates may inﬂate the Type-I error rate of the Wald test for logistic regression. We now investigate the impact of measurement error on the Type-I error rate of the Tomkins et al. test through simulations. We consider absence of any bias, and assume that model (1) with β2 = 0 is correct for both DB and SB reviewers. We consider DB reviewers to report noisy estimates of true scores qj∗, and vary the correlation between q∗ and w. The level of noise was selected to keep correlation between the two DB reviewers assigned to each paper at the level of 0.6, which is much better than the actual interreviewer agreement observed by Tomkins et al. (2017) (correlation 0.37). We plot the Type-I error rates in Figure 2a for the test in Tomkins et al. (2017) and our proposed test, both tests are designed to restrict the Type-I error rate to 0.05.
Figure 2a indicates a strong detrimental eﬀect of measurement error on the validity of the test by Tomkins et al. (2017). Given that interreviewer agreement in the actual WSDM conference experiment was low, the fact that some properties considered by Tomkins et al. may lead to correlations between q∗ and w is concerning, because it could potentially undermine the validity of their ﬁndings.
The simulations in Section 1 follow the setup presented here: Figures 1a and 1b consider measurement error with correlation ﬁxed at 0.4 (Figure 1a) and 0.6 (Figure 1b) and show that (a) the negative eﬀect of measurement error on the Type-I error rate exacerbates as sample size grows and (b) measurement error may also hinder the power of the test. Figure 1c has zero correlation and no measurement error, satisfying all the assumptions of the test by Tomkins et al.
5

Type−I error rate
0.0 0.2 0.4 0.6 0.8 1.0
Type−I error rate
0.0 0.2 0.4 0.6 0.8 1.0
Type−I error rate
0.0 0.2 0.4 0.6 0.8 1.0

Previous work Disagreement test

0.0 0.1 0.2 0.3 0.4
Correlation coefficient
(a) Measurement error

0.0 0.1 0.2 0.3 0.4
Correlation coefficient
(b) Model mismatch

1 20 40 60 80 100
Reviewer load
(c) Reviewer calibration

Figure 2: Type-I error of the test from previous work (Tomkins et al. 2017) blows up under three diﬀerent setups: bias is absent in all simulations and the tests are designed to limit the Type-I error to at most 0.05. In contrast, our Disagreement test is robust to violations of modelling assumptions. Error bars are too small to be visible.

(b) Model mismatch. Model (1) assumes a speciﬁc parametric relationship, which may not hold in practice. In order to check the eﬀect of model mismatches, we consider a violation of the model (1) and suppose that the correct model for both SB and DB reviewers is

log πj = β0 + β1 q∗ 3 + β2wj ,

1 − πj

j

that is, instead of expected linear input, true scores of papers appear in the model raised to the power 3. To isolate the eﬀect of model mismatch, we assume that true scores qj∗, j ∈ [n], are known exactly to the test of Tomkins et al. and hence abstract out the impact of the measurement error. We again consider an absence of any bias and set β2 = 0 for both SB and DB reviewers. We then perform simulations similar to those in item (a). Figure 2b shows the results of the simulations.
(c) Reviewer calibration. The test employed by Tomkins et al. (2017) treats reviews given by the same reviewer as independent. In practice this assumption may be violated due to correlations introduced by reviewer’s calibration (Wang and Shah, 2018). While some easy calibrations such as harshness/leniency can be captured by simple parametric extensions of model (1), more subtle patterns are beyond the scope of this model. Suppose for example that the strength of reviewers’ input depends on paper’s clarity — the better the paper is written, the lower the contribution due to reviewers’ calibration. Assume also that we are given a set of papers such that true score of each paper is proportional to the clarity of the paper (we formalize construction in Appendix E.1.3). Coupled with the correlation between q∗ and w, this pattern is suﬃcient to break Type-I error guarantees of the test of Tomkins et al. Again, to isolate the impact of reviewers’ calibration, we assume that (i) true scores qj∗, j ∈ [n], are known to the test by Tomkins et al. and (ii) model (1) is marginally correct for each reviewer, that is, each reviewer follows model (1) for each paper she/he reviews, but her/his decisions for diﬀerent papers are correlated in a speciﬁc way.
Figure 2c shows a result of simulations in which we vary the number of papers per reviewer, keeping correlation between q∗ and w ﬁxed at 0.75 and the total number of papers ﬁxed at n = 1000. We simulate a wide range of reviewer load µ including small to medium loads of 5-15 papers typical in machine learning conferences like NeurIPS and larger loads of 40 or higher found in other smaller conferences.

3.2 Experimental setup
The issues discussed above pertain to the testing procedure and modelling assumptions made by Tomkins et al. (2017). We now issue a commentary regarding the experimental setup considered in their work which

6

Type−I error
0.0 0.2 0.4 0.6 0.8 1.0
Type−I error
0.0 0.2 0.4 0.6 0.8 1.0

Previous work Disagreement test

Non−blind

Blind

Bidding in SB condition

(a) Non-blind bidding

Tomkins et al.

Our work

Setup

(b) Reviewer assignment

Figure 3: The experimental setup from previous work (Tomkins et al. 2017) violates Type-I error guarantees of testing procedures. Bias is absent in all simulations and the tests are designed to limit the Type-I error to at most 0.05. Note that the issues which pertain to the experimental setup rather than the modelling break guarantees of both tests (leftmost columns). In contrast, our proposed setup with fully blind bidding procedure and careful management of assignment ensures Type-I error guarantees for both tests (rightmost columns). Error bars are too small to be visible.

comprises a random partition of reviewers into SB and DB groups within a standard peer review procedure. In particular, we show that the setup itself may create problems in controlling the Type-I error.
(d) Non-blind bidding. In the experiment by Tomkins et al. papers are allocated to reviewers based on preferences (“bids”) declared by reviewers (reviewers could indicate that they want to review some papers and do not want to review others). Importantly, the reviewers in the SB setup also get to see author identities in the bidding stage, which may act as a confounding factor in tests for bias in the acceptance/rejection of papers. This is indeed pointed out as a caveat by Tomkins et al. (2017) in their paper.
To illustrate the possible eﬀect, consider a property of interest “paper has a famous author” and suppose that among all reviewers there is a subset of lenient reviewers who additionally want to read papers from top authors with the hope of reading better papers. Then in DB setup such reviewers cannot use author identity information and hence make their bidding decisions based on title and abstract only; in contrast, in SB setup these reviewers tend to bid on papers authored by top authors. Given that reviewers who preferentially bid on papers with top authors in SB condition are by coincidence lenient, the diﬀerence in bidding behavior may result in structurally diﬀerent evaluations between conditions even when reviewers’ evaluations are unbiased, leading to a blow-up of the Type-I error rate of any reasonable test. Figure 3a shows a result of simulations (formal setup is in Appendix E.1.4) in which we compare non-blind and blind bidding conditions for SB reviewers and indicates a possible detrimental eﬀect of non-blind bidding.
(e) Reviewer assignment. One might imagine that a natural requirement to conduct the bidding in a double blind fashion for both DB and SB reviewers would ﬁx the issues with the setup of Tomkins et al. However, perhaps surprisingly, we show that even if both groups bid in a double blind fashion (or even if the bidding process is eliminated entirely), and even if the reviewers are assigned to DB or SB groups uniformly at random, the non-random assignment using algorithms such as TPMS (Charlin and Zemel, 2013) that assigns reviewers to papers maximizing some notion of “similarities” can still lead to a violation of the Type-I error guarantees. We give a formal construction in Appendix E.1.5; the intuition is as follows. Quoting Lamont (2009), “evaluators often deﬁne excellence as «what speaks to me» which is akin to «what is most like me»”, that is, a similarity between a paper and a reviewer may inﬂuence the decision. That said, we construct these similarities in a careful manner: our choice ensures that despite reviewers being allocated to DB or SB conditions at random, the popular TPMS assignment algorithm with high probability constructs
7

assignments that are in some sense structurally diﬀerent between SB and DB conditions, which in turn leads to structurally diﬀerent evaluations. Our construction, along with correlation between q∗ and w, introduces spurious correlations in the data, thereby violating some key independence assumptions and leading to the inﬂation of Type-I error.
Figure 3b shows a result of simulations in which we compare the setup of Tomkins et al. (2017) with our proposed experimental setup introduced in Section 5.2. Notably, under the setup of Tomkins et al. even the Disagreement test which is robust to various issues discussed in Section 3.1 is unable to control for the Type-I error. In contrast, observe that under our proposed experimental setup, both the Disagreement test and the test by Tomkins et al. control for the Type-I error rate at the desired level.
Importantly, we underscore that while our experimental procedure mitigates the issues with the experimental setup of Tomkins et al., their test is still susceptible to the issues we discussed in Section 3.1 even under our experimental setup. Finally, under the setup of Tomkins et al. the phenomenon of the Type-I guarantee violation is not restricted to the TPMS assignment and can occur in a much broader class of reviewer assignment algorithms.
4 Novel framework to test for biases
In Section 3 we identiﬁed ﬁve key limitations of the approach taken by Tomkins et al. (2017). Three of these limitations pertain to the testing procedure and the two limitations relate to the design of the experiment itself. In the next sections we design a set of tests and experimental setup with strong guarantees, and which overcome the aforementioned limitations. In this section we begin from principled deﬁnition of a bias testing problem that generalizes one made by Tomkins et al. and does not make any restrictive assumptions.
At a high level, our approach to testing for biases is diﬀerent from those proposed by Tomkins et al. in two ways. First, we relax two strict modelling assumptions: (i) instead of assuming existence of true qualities of submissions, we allow subjectivity in reviewer evaluations (Kerr et al., 1977; Ernst and Resch, 1994; Bakanic et al., 1987; Mahoney, 1977; Lamont, 2009; Noothigattu et al., 2018), and (ii) we do not assume any speciﬁc form of the relationship between a paper and its probability of acceptance by a reviewer. Instead, we allow these probabilities to be completely arbitrary and deﬁne the bias in terms of these probabilities. Second, we treat this problem conceptually diﬀerently from the work of Tomkins et al. The test therein treats the problem as that of one-sample testing and uses DB scores as a plugin estimate of true scores in SB model. In contrast, we approach this problem through the lenses of two-sample testing, where SB and DB reviews form the two samples, and the goal is to test whether they belong to the same distribution. This perspective helps us to avoid a number of issues discussed in Section 3.
Formally, let Πdb ∈ [0, 1]m×n be a matrix whose (i, j)th entry, denoted as πi(jdb), represents a probability that reviewer i would recommend acceptance of paper j if that paper is assigned to that reviewer in DB setup. Similarly, let matrix Πsb ∈ [0, 1]m×n be an analogous matrix in SB setup, and denote its (i, j)th entry as πi(jsb).
Let RSB be the set of reviewers allocated to the SB condition. Moreover, for each i ∈ RSB, let PSB(i) denote the set of papers assigned to reviewer i and let Yij ∈ {0, 1} denote the accept/reject decision given by reviewer i for paper j ∈ PSB(i). We similarly deﬁne set of DB reviewers RDB and their decisions {Xij : i ∈ RDB, j ∈ PDB(i)} . We are interested in testing for biases with respect to a property of interest. To this end, recall our notation J ⊆ [n] for the set of papers that satisfy a property of interest, and J as its complement.
With this notation in place, we now deﬁne two formulations of the bias testing problem — “absolute” and “relative”: The relative bias setting is strictly more general than the absolute bias setting, but also leads to more restrictive results.2 Importantly, the tests we will introduce in Section 5.1 are applicable to both formulations without additional modiﬁcations.
2An equivalent deﬁnition of the problem from the perspective of causal inference can be found in Appendix D.
8

4.1 Absolute bias problem

In the absence of bias, the knowledge of authors’ identities does not induce any diﬀerence in reviewers’ behaviour. In the biased hypothesis, there is a positive bias in favor of papers that satisfy a property of interest: reviewers in SB condition are more lenient towards papers from J and more harsh towards papers from J than they would be in DB condition. The following problem formalizes this intuition.

Problem 1 (Absolute bias problem). Given signiﬁcance level α ∈ (0, 1) and decisions of SB and DB reviewers, the goal is to test the following hypotheses:

H0 : ∀i ∈ [m] ∀j ∈ [n] πi(jsb) = πi(jdb)

π(sb)≥π(db) if j ∈ J

H1 : ∀i ∈ [m] ∀j ∈ [n]

ij (sb)

ij (db)

(2)

πij ≤πij if j ∈ J ,

where at least one inequality in the alternative hypothesis (2) is strict.

Note that one can deﬁne an alternative that represents a bias against papers from J simply by exchanging
the sets J and J in (2). Our goal is to design a testing procedure that controls for Type-I error and has non-trivial power for any pair of matrices Πsb, Πdb that fall under deﬁnition of Problem 1.
Non-trivial power. Informally, we say that the test has non-trivial power if for choices of Πsb and Πdb for
which the presence of bias is “obvious”, the test is able to detect the bias with probability that goes to 1 as number of papers in both J and J grows to inﬁnity. Formally, we say that matrices Πsb and Πdb satisfy
alternative hypothesis (2) with margin δ, if all inequalities in equation (2) are satisﬁed with margin δ > 0, that is, |πi(jsb) − πi(jdb)| > δ ∀ (i, j) ∈ [m] × [n]. Then we say that the testing procedure has non-trivial power if for any ε > 0 and for any δ > 0 there exists n0 = n0(ε, δ) such that if min{|J |, |J |} > n0, then for any Πsb and Πdb that satisfy alternative hypothesis (2) with margin δ, the power of testing procedure is at least 1 − ε.
For instance, if the logistic model (1) is correct for both SB and DB reviewers for some β0(sb) = β0(db) = β0, β1(sb) = β1(db) = β1 > 0, β2(db) = 0 and |β2(sb)| > 0, then the requirement of non-trivial power ensures that for any choice of true scores bounded in absolute value by a universal constant and any choice of property
satisfaction indicators, the test has power growing to 1 as min{|J |, |J |} goes to inﬁnity.

4.2 Relative bias problem

In Problem 1 we assumed that SB (or DB) condition itself does not cause any change in reviewers’ behaviour.
We now consider a generalization of Problem 1 which accommodates an additional confounding factor — a
bias in the reviewer simply due to her/his assignment in the SB or the DB group (and independent of the
paper or its characteristics). For example, reviewers may not have any bias with respect to the property
of interest, but just being placed in the SB condition may induce more harsh opinions than the reviewers in DB. Formally, recall the null hypothesis πi(jsb) = πi(jdb) ∀(i, j) ∈ [m] × [n] in Problem 1. Instead, under the null, we now allow πi(jsb) = f0(πi(jdb)), for some non-decreasing function f0 : [0, 1] → [0, 1]. Of course, one may not know the function f0 and the goal of this general problem is to design a test that is guaranteed to control over Type-I error and has non-trivial power uniformly for all functions f0 that belong to some set of non-decreasing functions F.

Problem 2 (Relative bias problem). Given signiﬁcance level α ∈ (0, 1), class of functions F and decisions of SB and DB reviewers, the goal is to test the following hypotheses:

H0 : ∀i ∈ [m] ∀j ∈ [n] πi(jsb) = f0(πi(jdb))

πi(jsb) ≥ f0(πi(jdb)) if j ∈ J

H1 : ∀i ∈ [m] ∀j ∈ [n] (sb)

(db)

,

(3)

πij ≤ f0(πij ) if j ∈/ J

where f0 is some unknown function from F and at least one inequality in the alternative hypothesis (3) is strict.

9

For example, if the logistic model (1) is correct for both SB and DB reviewers (with β2(db) = 0), but intercepts β0 in SB and in DB conditions are allowed to be diﬀerent, then the corresponding matrices Πsb and Πdb do not fall under the deﬁnition of Problem 1, but can be captured by Problem 2 with speciﬁc choice
of F as we will discuss in Section 6.2. The deﬁnition of non-trivial power transfers to the relative bias problem with the exception that all πi(jdb)
are substituted by f0(πi(jdb)) for f0 ∈ F . Our goal is to design a testing procedure that controls for Type-I error and has non-trivial power for any pair of matrices Πsb, Πdb that fall under deﬁnition of Problem 2 for
any function f0 ∈ F. Ideally, we would like to achieve this goal for a set of functions F that contains all
non-decreasing functions f : [0, 1] → [0, 1].

5 Proposed solution
We now introduce the proposed experimental setup as well as statistical tests we study in this work. We subsequently analyze them in the context of Problems 1 and 2 in Section 6.

5.1 Testing procedures
In order to avoid correlations introduced by reviews given by the same reviewer, our tests use at most one decision per reviewer. As we discuss in Section 5.2, we do so by ﬁrst matching reviewers into pairs, consisting of one SB and one DB reviewer who review a common paper. For the moment, assume that we are given a set of tuples T , where each tuple t ∈ T consists of a paper jt ∈ [n], decision of a SB reviewer for this paper Yjt , decision of a DB reviewer for this paper Xjt and indicator of property satisfaction wjt , with a constraint that each reviewer contributes her/his decision to at most one tuple. With this notation, we now present two tests we consider in this work. As we show subsequently, either of these tests would suﬃce for the absolute bias problem, but for the relative bias problem they cater to diﬀerent models of reviewers’ behaviour with non-intersecting areas of applicability. To provide intuition behind the tests, we deﬁne them in context of the absolute bias problem (Problem 1) and discuss their applicability to the relative bias problem later.
Disagreement-based test. A high-level idea of the test is as follows. Consider a pair of SB and DB reviewers who disagree in their decisions for some paper. Then under the null hypothesis, the events “SB accepts and DB rejects” and “SB rejects and DB accepts” are equally likely. In contrast, if the null hypothesis is violated, then depending on the property satisfaction and the direction of the bias, SB reviewer is more (or less) likely to vote for acceptance than her/his DB counterpart.

Test 1 Disagreement Input: Signiﬁcance level α ∈ (0, 1)
Set of tuples T , where each t ∈ T is of the form (jt, Yjt , Xjt , wjt ) for some paper j ∈ [n].

1. Initialize U and V to be empty arrays.

2. For each tuple t ∈ T , if Yj = Xj , append Yj to U if wjt = 1 .

t

t

t

V if wjt = −1

3. Run a permutation test (Fisher, 1935) at the level α to test if entries of U and V are exchangeable random variables, using the test statistic:

1

1

τ = |U | Ur − |V | Vr.

r∈[|U |]

r∈[|V |]

4. Reject the null if and only if the permutation test rejects the null. (If either of the arrays V and U is empty, the test keeps the null.)

10

We now formally present the Disagreement test as Test 1. In Step 1 two empty arrays U and V are initialized. Next, in Step 2 we focus on pairs of SB and DB reviewers disagreeing in their decisions for a paper they both review. For each of the corresponding tuples, we add the decision of SB reviewer to the array U if a paper satisﬁes the property of interest and to V otherwise. Finally, in Step 3 we deﬁne a test statistic τ . According to the aforementioned intuition, under the null hypothesis τ should be close to 0, but under the alternative it should be large in absolute value. Hence, to make a decision we run a permutation test and reject the null in Step 3 if this test suggests that |τ | is too large for a given signiﬁcance level α.
Counting-based test. The test is built on a simple intuition. Assume for the moment that SB setup induces a bias against papers from J and a bias in favor of papers from J . Then it is likely that papers from J will receive less number of positive recommendations in SB setup as compared to DB setup. Symmetrically, for papers from J we expect reviewers in SB to be more lenient than their DB counterparts. In contrast, if there is no bias at all, then we expect the aforementioned diﬀerences to be small.
We now formally present the Counting test as Test 2. In Step 1 two empty arrays U and V are created which in Step 2 are populated with diﬀerences between decisions of SB and DB reviewers for papers from J and J respectively. Importantly, in contrast to the Disagreement test, in the Counting test we do not condition on disagreeing pairs of reviewers. Noticing that mean value of entries of U (respectively V ) measures the change of attitude towards papers from J (respectively J ) between SB and DB conditions, in Step 3 we compute a test statistic γ which compares these changes. According to the aforementioned intuition, under correct null hypothesis the test statistic should be close to 0. Finally, in Step 4 we make a decision using concentration properties of the test statistic.

Test 2 Counting Input: Signiﬁcance level α ∈ (0, 1)
Set of tuples T , where each t ∈ T is of the form (jt, Yjt , Xjt , wjt ) for some paper j ∈ [n].

1. Initialize U and V to be empty arrays.

2. For each tuple t ∈ T , append (Yj − Xj ) to U if wjt = 1 .

t

t

V if wjt = −1

3. If either of the arrays V and U is empty, keep the null and terminate. Otherwise, set the test statistic γ as follows:

1

1

γ = |U | Ur − |V | Vr. (4)

r∈[|U |]

r∈[|V |]

4. Reject the null hypothesis if and only if |γ| >

2 (|U |−1 + |V |−1) log 2/α.

Eﬀect size. In Section 6 we will establish theoretical guarantees on Type-I error control for both Disagreement and Counting tests. In addition to these guarantees, both tests provide a natural measure of the eﬀect size: • Counting. The test statistic γ of the Counting test compares the within-subject diﬀerences in acceptance
rates for papers from J and J . Indeed, the ﬁrst term in equation (4) measures the diﬀerence between acceptance rates in SB and DB setups for papers from J . Similarly, the second term measures the same diﬀerence for papers from J . A positive value of the test statistics then indicates that papers from J beneﬁt from SB review more than papers from J .
• Disagreement. Slightly informally, the test statistic τ of the Disagreement test measures the diﬀerence in acceptance rates of “borderline” papers from J and J in the SB setup. Indeed, by conditioning on pairs

11

of disagreeing reviewers in Step 2 of Test 1, the test rules out “clear accept” and “clear reject” papers thus considering only the papers for which reviewers disagree (i.e., borderline papers). Overall, absolute values of the test statistics τ and γ are reasonable estimates of the eﬀect size and are in a similar vein to Cohen’s d and other popular eﬀect size measures (Cohen, 1992).
5.2 Setup of the experiment
We now propose the setup of the experiment to overcome the issues highlighted in Section 3.2 and discuss a construction of the set T used by the tests introduced above. At a higher level, the proposed setup has two main diﬀerences from one considered by Tomkins et al. (2017). First, bidding is performed in blind manner by both SB and DB reviewers (Step 1 below). Second and more importantly, to avoid issues caused by non-random reviewers’ assignment, we perform paper assignment and reviewer allocation to conditions jointly in a carefully selected manner (Steps 2-4 below).
Procedure 1 Design of the experiment Input: Paper load λ ≥ 1
Reviewer load µ ≥ 1 Assignment algorithm A 1. Reviewers bid on papers in blind manner
2. Depending on the relationship between number of papers (n) and reviewers (m):
(a) If m > 2n, select 2n reviewers uniformly at random and use algorithm A to assign each paper to 2 reviewers from the selected pool such that each reviewer is assigned to one paper
(b) If m < 2n, select m/2 papers such that proportions of papers from J and J are as close to each other as possible. Use algorithm A to assign each selected paper to 2 reviewers such that each reviewer is assigned to one paper
(c) If m = 2n, use algorithms A to assign each paper to 2 reviewers such that each reviewer is assigned to one paper
Denote the corresponding assignment as A∗
3. For each paper in assignment A∗, allocate one assigned reviewer to DB condition and another assigned reviewer to SB condition uniformly at random. If at this point there are reviewers who are not allocated to conditions, allocate half of them to SB and half to DB uniformly at random
4. Using algorithm A, complement assignment A∗ such that each paper is assigned to λ SB and λ DB reviewers and each reviewer reviews at most µ papers. Denote the corresponding assignment as A and begin review process according to this assignment
5. When the review process is ﬁnished, construct a set T as follows. For every paper j from the assignment A∗ and corresponding pair (i1, i2) of SB and DB reviewer, add tuple (j, Yi1j, Xi2j, wj) to the set T
6. Run statistical test on the set T
We now formally present the experimental procedure as Procedure 1. It takes as input parameters of paper and reviewer loads together with any assignment algorithm that operates on similarities and/or bids. In Step 1 reviewers bid on the papers in blind manner, that is, using only title and abstract of submissions. Notice that in contrast to the Tomkins et al. setup, bidding happens even before the reviewers are allocated to SB or DB conditions. In Step 2 we ﬁnd a partial assignment of papers to reviewers which satisﬁes (λ = 2, µ = 1)-load constraints. Depending on the relationship between n and m, we may include only a subset of papers or reviewers in this assignment. For example, in case 2b we do not have enough reviewers to respect the one paper per reviewer constraint and hence we select subset of papers of appropriate size such that it includes approximately equal number of papers from J and J and ﬁnd the assignment for selected papers only. The constraint on the number of papers from J and J is to ensure that the resulting set T is balanced which is
12

necessary for non-trivial power. The corresponding assignment A∗ is a building block for our tests which will use the reviews from this assignment only. Next, in Step 3 reviewers are allocated to conditions in a speciﬁc manner which is crucial for our statistical guarantees. In Step 4 we ﬁnd a full assignment A that is a completion of the partial assignment A∗, meaning that if reviewer i was assigned to paper j in assignment A∗, she/he is also assigned to this paper in A. Finally, in Step 5 we construct a set of tuples T that is used by the Disagreement and Counting algorithms in Step 6. Importantly, by construction we ensure that each reviewer contributes at most one decision to the set T .
As we show below, the experimental Procedure 1 overcomes the issues with the experimental setup we discussed in Section 3.2 and leads to provable control over Type-I error for our Disagreement and Counting algorithms. We underscore that (i) Disagreement and Counting tests are not tied to particular experimental procedure we introduce and can be applied under the setup of Tomkins et al. (2017) with caveats discussed in Section 3.2. For instance, in simulations (a)-(d) of Section 3 the Disagreement test was applied under the setup of Tomkins et al. More details on this remark are provided in Appendix B; (ii) as requested by the Disagreement and Counting tests, the set of tuples T constructed in Step 5 contains at most one decision of each reviewer. This requirement allows our tests to be agnostic to reviewer calibration which may otherwise undermine Type-I error guarantees as demonstrated in Section 3.1. However, if one treats reviews given by the same reviewer as independent, thereby ignoring issues with reviewer calibration, then in Step 5 of Procedure 1 one can construct a larger set T by using full assignment A and allowing each reviewer to contribute multiple decisions to the set T .
Finally, in addition to facilitating the experiment, in the interest of fairness in the review (Tomkins et al., 2017) the experimental procedure should ensure that in the eventual assignment each paper is reviewed by equal number of SB and DB reviewers. By construction, Procedure 1 satisﬁes this requirement: Step 4 ensures that in the ﬁnal assignment A each paper is assigned to λ SB reviewers and λ DB reviewers.
6 Analysis
We now present the analysis of the Counting and Disagreement tests in context of absolute and relative bias problems.
6.1 Absolute bias problem
We begin our analysis from the absolute bias problem and ﬁrst formulate the main theorem of this section.
Theorem 1. For any signiﬁcance level α ∈ (0, 1), under the setup of the absolute bias problem (Problem 1), let the experiment be organized according to Procedure 1. Then the Disagreement and Counting tests are guaranteed to control for Type-I error at the level α, and also satisfy the requirement of non-trivial power.
Remark. 1. As demonstrated in Figure 4, In practice, the Disagreement test has a higher power as compared to the Counting test and should be employed under conditions of the absolute bias problem.
2. Notice that the outcomes of the Disagreement and Counting tests depend on a set T provided to the tests as input. That is, for two diﬀerent sets T1 and T2 (that for example correspond to diﬀerent assignments A∗1 and A∗2 constructed in Step 2 of Procedure 1), the outcomes of the tests might be diﬀerent. Hence, one should ﬁx a set T before observing reviewers’ decisions to avoid chasing statistical signiﬁcance.
3. Finally, the Disagreement and Counting tests are also applicable to the experimental procedure used by Tomkins et al. (2017) and are guaranteed to be robust to issues (a)-(c) from Section 3.1. The formal statement is given in Appendix B.
We now discuss the issues (a)-(e) considered in Section 3 in the context of our Disagreement and Counting tests. • Noise. The Disagreement and Counting tests do not rely on any estimation of papers’ qualities made
by reviewers. Moreover, we do not even assume that there exists some objective quantity that can be estimated. Hence, our tests do not suﬀer from issues caused by noisy estimates of scores given by DB reviewers as illustrated by Figure 2a in case of the Disagreement test.
13

Power
0.0 0.2 0.4 0.6 0.8 1.0
Power
0.0 0.2 0.4 0.6 0.8 1.0

200 400 600 800 1000
Number of papers

Previous work Disagreement test Counting test
200 400 600 800 1000
Number of papers

(a) DB reviewers estimate true scores with some noise in presence of correlations.

(b) DB reviewers estimate true scores without noise and correlations are absent.

Figure 4: Synthetic simulations evaluating performance of the test in Tomkins et al. (2017) (“previous work”) and the tests introduced in this section (“Disagreement and Counting tests”) in presence of bias when logistic model (1) of reviewers is correct. Larger values are better. Details of the simulation setup are provided in Appendix E.2. Error bars are too small to be visible.

• Model mismatch. The only assumption we make is that under correct null hypothesis there is no diﬀerence in behavior of SB and DB reviewers. Hence, Theorem 1 guarantees that our tests are robust to violations of speciﬁc parametric model (1) as illustrated by Figure 2b.
• Reviewer calibration. We circumvent the detrimental eﬀect of correlations introduced by reviewers’ calibration by requiring that each reviewer contributes at most one review to the test. See Figure 2c for an illustration. Of course, such robustness comes at the cost of some power, but we notice that our matching procedures guarantee the use of at least a constant fraction of available data, thereby limiting reduction in the power.
• Non-blind bidding. The issue with bidding is straightforwardly resolved by requesting blind bidding from both SB and DB reviewers. As illustrated by Figure 3a, we abstract out possible confoundings due to diﬀerence in bidding behaviour and ensure that the observed diﬀerence in decisions (if any) is due to bias in evaluations and not in bidding.
• Reviewer assignment. The experimental design proposed in Procedure 1 allows to execute any assignment algorithm without breaking the guarantees of the tests as demonstrated in case of TPMS assignment by Figure 3b. The key part of the procedure that ensures such robustness are Steps 2 and 3 where we ﬁrst ﬁnd triples of two reviewers and a paper they review and then randomly allocate one reviewer from each triple to SB and one to DB. In this manner, we ensure that parts of the ﬁnal assignment A that are used for testing do not exhibit any structural diﬀerence caused by non-random assignment. We conclude the section with brief discussion of the power of the tests we introduced in this section. To
provide fair comparison of tests, the simulations are performed under the setup of the experiment by Tomkins et al. (2017) and the input to the Disagreement and Counting tests is computed according to procedure described in Appendix B. Figure 4 contrasts the powers of the tests under the logistic model (1) in two cases. In Figure 4a, DB reviewers estimate the true scores of the papers with some noise, which in presence of correlation dramatically decreases the power of the test by Tomkins et al. As discussed above, the Disagreement and Counting tests are robust to issues caused by measurement error, as seen in Figure 4a.
In contrast, Figure 4b considers the case when DB reviewers estimate true scores without noise, that is, true scores are known to the Tomkins et al. test. Although in this case their test has the highest power among 3 tests under consideration, we notice that the margin between their test and the Disagreement
14

test is not as large as in Figure 4a. Notice also that the test by Tomkins et al. gains power by overﬁtting to the strict model (1) which leads to higher power when the model is correct, but at the cost of not being able to control over Type-I error rate under reasonable violations of the modelling assumptions as discussed in Section 3.
Finally, notice that the Counting test relies on the sub-Gaussian approximation to deﬁne a threshold and consequently has lower power than the Disagreement test in both cases. While this suggests that for absolute bias problem the Disagreement test dominates the Counting test, we will show in the next section that under the relative bias problem these tests are incomparable.

6.2 Relative bias problem
In Section 6.1 we showed that if under the absence of bias the behaviour of reviewers in SB and DB conditions is the same, then the Disagreement and Counting tests control for Type-I error and have non-trivial power thus leading to a reliable testing procedure. In this section we relax that assumption and consider a relative bias problem with two speciﬁc choices of F that correspond to popular linear and logistic models. We ﬁrst show that for each of these choices either the Disagreement or the Counting test leads to reliable testing. We then conclude the analysis with a negative result saying that no test can control for Type-I error and have non-trivial power over both choices of F.
Let us now introduce these two choices of F that we consider throughout the remaining part of this section. To this end, for each j ∈ [n] we let qj ∈ R denote an unknown “representation” of the paper j, where by representation we imply any function of a paper’s content that deﬁnes reviewers’ perception of a paper. For example, it could be that qj = qj∗, where qj∗ is a true score as deﬁned by Tomkins et al. Generalized linear model. Under the generalized linear model, the SB condition itself induces a change in reviewers’ evaluations, making them more harsh (or lenient). Moreover, under absence of bias the change in behaviour between SB and DB conditions is described by a constant shift in probability of acceptance for all papers, irrespective of whether they satisfy a property of interest or not. Formally, consider a ﬁxed constant ∆ ∈ (0, 0.5). The generalized linear model assumes that (i) for every j ∈ [n] a corresponding representation qj belongs to the interval (∆, 1 − ∆) and (ii) under absence of bias for every (i, j) ∈ [m] × [n] the behavior of reviewer i if she/he reviews paper j is described by the following parametric equations:

DB: πi(jdb) = qj SB: πi(jsb) = ν + qj ,

(5a) (5b)

for some unknown constant ν ∈ (−∆, ∆). Provided that matrix Πdb was generated according to the model (5a) of DB reviewers, the generalized linear model corresponds to an instance of a relative bias problem with a set of functions F∆ associated to a ﬁxed constant ∆ and deﬁned as

F∆ = hν(t) : (∆, 1 − ∆) → [0, 1] ν ∈ (−∆, ∆) ,

(6)

where

hν(t) = t + ν, t ∈ (∆, 1 − ∆).

(7)

Indeed, observe that if for any (i, j) ∈ [n] × [m] the probability of acceptance πi(jdb) was generated according to the model (5a), then πi(jsb) deﬁned as πi(jsb) = hν (πi(jdb)) satisﬁes model (5b).
Generalized logistic model. Similar to the generalized linear model, under the generalized logistic model the SB condition also induces a change in reviewers’ evaluations, but the change now is described as a constant shift in space of log-odds of the acceptance probabilities. Formally, consider a ﬁxed constant ∆ > 0. The generalized logistic model assumes that (i) for every j ∈ [n] a corresponding representation qj belongs to the interval (−∆, ∆) and (ii) under absence of bias for every (i, j) ∈ [m] × [n] behaviour of reviewer i if she/he

15

reviews paper j is described by the following parametric equations:

DB: log πi(jdb) = β0 + β1qj 1 − πi(jdb)
SB: log πi(jsb) = β0 + ν + β1qj , 1 − πi(jsb)

(8a) (8b)

for some unknown constant ν ∈ (−∆, ∆), where unknown coeﬃcients β0 and β1 are also bounded in absolute value by ∆ and β1 > 0. Provided that matrix Πdb is generated according to the model (8a) of DB reviewers, one can verify that the generalized logistic model corresponds to an instance of a relative bias problem with a
set of functions F∆ associated to a ﬁxed constant ∆ and deﬁned as

F∆ = gν(t) : [0, 1] → [0, 1] ν ∈ (−∆, ∆) ,

(9)

where

teν

gν(t) = 1 − t + teν , t ∈ [0, 1].

(10)

Indeed, observe that if for some (i, j) ∈ [n] × [m] the probability that reviewer i accepts paper j in DB setup πi(jdb) is generated according to the model (8a), then setting πi(jsb) = gν (πi(jdb)) we ensure that πi(jsb) satisﬁes model (8b).
The models we deﬁned follow an objective parametric approach assumed by Tomkins et al. (2017) with
two diﬀerences: (i) we do not assume that qj has a known meaning or that it can be measured (for instance, it may be that qj = qj∗, or that qj = (qj∗)3, or qj may be a complex function of the content of the paper) and (ii) we do not assume that the bias is described by a linear shift in space of probabilities or log-odds, and
instead consider a non-parametric deﬁnition of the bias as speciﬁed in the alternative hypothesis (3).

6.2.1 Positive results
We now show that the Counting and Disagreement tests lead to reliable testing under the generalized linear and generalized logistic models respectively. Before we formulate the main result of this section, let us provide some intuition behind the models and the corresponding tests.
Generalized linear model. A natural strategy to test for biases under the generalized linear model is to estimate the shift in reviewers’ behaviour on papers that belong to J and to J separately and then compare the estimates. In fact, the Counting testing procedure introduced above as Test 2 follows this strategy and, as guaranteed by Theorem 2, leads to reliable testing under the generalized linear model.
Generalized logistic model. Intuitively, estimating a constant shift in reviewers’ behavior as done by the Counting test may not be the optimal strategy under the generalized logistic model, as under absence of bias the change of behaviour between SB and DB conditions is given by a constant shift in log-odds space and not in probability space. However, it turns out that the Disagreement test is able to capture the constant shift in log-odds space and hence we still can perform reliable testing under this model, using the Disagreement algorithm.
Theorem 2. For any signiﬁcance level α ∈ (0, 1), let the experiment be organized according to Procedure 1. Then
(a) Under the generalized linear model with any ∆ ∈ (0, 0.5), the Counting test is guaranteed to control for Type-I error at the level α, and also satisﬁes the requirement of non-trivial power.
(b) Under the generalized logistic model with any ∆ > 0, the Disagreement test is guaranteed to control for Type-I error at the level α, and also satisﬁes the requirement of non-trivial power.

16

0.0 0.2 0.4 0.6 0.8 0.0 0.2 0.4 0.6 0.8

Counting test Disagreement test

Type−I error

Power

Type−I error

Power

(a) Instance of generalized linear model.

(b) Instance of generalized logistic model.

Figure 5: Synthetic simulations evaluating performance of the Disagreement and Counting tests under setup of the relative bias problem when generalized linear (Figure 5a) or generalized logistic (Figure 5b) models is correct. For each model presence and absence of bias are simulated. Details of the simulation setup are provided in Appendix E.3. Error bars are too small to be visible.

Remark. 1. Result of Theorem 2(a) holds even for a subjective version of the generalized linear model in which for each (i, j) ∈ [n] × [m] we substitute qj with qij (that is, diﬀerent values across reviewers) in equations (5a) and (5b), thereby accounting for subjectivity of reviewers.
2. If the logistic model (1) assumed by Tomkins et al. is correct for both SB and DB reviewers with possibly diﬀerent intercepts, then Theorem 2(b) ensures that the Disagreement test provably controls for the Type-I error and can detect a bias with probability that goes to 1 as sample size grows, without requiring knowledge (neither exact nor approximate) of papers’ scores q1∗, . . . , qn∗ .
3. Notice that the Counting and Disagreement tests do not require the knowledge of ∆ and ∆ parameters to control for Type-I error and satisfy the requirement of non-trivial power.
Figure 5 compares the performance of the Disagreement and Counting tests under speciﬁc instances of the generalized linear and logistic models, illustrating the results of Theorem 2. Figure 5a shows that the Counting test controls for Type-I error and has a non-trivial power under the generalized linear model. Notice that the Disagreement test does not control for Type-I error in this instance, implying that Theorem 2(b) cannot be extended to guarantee the Type-I error control under the generalized linear model as well. Symmetrically, Figure 5b shows that while the Disagreement test leads to reliable testing under the generalized logistic model, the Counting test is unable to control for Type-I error under this model. Hence, we conclude that under the setup of the relative bias problem, none of the tests dominates another, each leading to reliable testing under the corresponding model.
We see in Figure 5 that neither the Disagreement nor the Counting test is suitable for both generalized linear and logistic models. In the next section we show that this is not a drawback of these speciﬁc tests, but rather a manifestation of a more general impossibility result.
6.2.2 Negative result
We conclude our analysis with a negative result that limits the complexity of the class F for which reliable testing in the relative bias problem is possible. Let us ﬁrst state the main result of this section.
Theorem 3. Suppose that there exist two functions g, h ∈ F and some 0 ≤ x1 < x2 ≤ 1 such that g(x1) < h(x1) and g(x2) > h(x2). Suppose also that there exists a testing procedure ψ operating on decisions of SB and DB reviewers that for any given α ∈ (0, 1) keeps Type-I error below α for all matrices Πsb, Πdb

17

that satisfy the null hypothesis of Problem 2 speciﬁed by some function f0 ∈ F. Then the testing procedure ψ cannot satisfy the requirement of non-trivial power.
The intuition behind Theorem 3 is as follows. If the class F contains “too many” functions, then some matrices Πsb and Πdb satisfy the null hypothesis of Problem 2 deﬁned by f0 ∈ F and simultaneously satisfy the alternative hypothesis (3) deﬁned by some other function f0 ∈ F with margin δ > 0. Hence, any testing procedure must either have high Type-I error rate or sacriﬁce the non-trivial power requirement over the class of functions F, implying that reliable testing is impossible.
Using Theorem 3 we can deduce that one can hope to control for the Type-I error rate and simultaneously have a non-trivial power only when functions contained in F are pointwise totally ordered, that is, for any two functions f, g ∈ F it must be the case that either f (x) ≥ g(x) for all x ∈ [0, 1] or f (x) ≤ g(x) for all x ∈ [0, 1].
Let us now illustrate the consequences of Theorem 3 for the generalized linear and logistic models.
Corollary 1. For any signiﬁcance level α ∈ (0, 1), let ψ1 and ψ2 be any testing procedures which operate on decisions of SB and DB reviewers. Suppose that under the generalized linear model with any ∆ ∈ (0, 0.5), procedure ψ1 controls for the Type-I error at the level α and satisﬁes the non-trivial power requirement. Suppose also that under the generalized logistic model with any ∆ > 0, procedure ψ2 controls for the Type-I error at the level α and satisﬁes the non-trivial power requirement. Then
(a) Under the generalized logistic model with any ∆ > 0, procedure ψ1 incurs a Type-I error rate strictly greater than α.
(b) Under the generalized linear model with any ∆ ∈ (0, 0.5), procedure ψ2 incurs a Type-I error rate strictly greater than α.
Corollary 1 shows that there does not exist a testing procedure that controls over Type-I error and has non-trivial power under both generalized linear and generalized logistic models. As a result, one needs to design diﬀerent procedures for these models as we did with the Disagreement and Counting tests. In case of the Disagreement and Counting tests, Corollary 1 is illustrated by Figure 5.
In Appendix C we discuss another application of Theorem 3 in context of the generalized logistic model that also suggests that generality of the Disagreement test cannot be further increased.
7 Discussion
Peer review is the backbone of academia but faces a number of challenges of unfairness, biases, and ineﬃciency. This work contributes to the growing literature (Shah et al., 2018; Kang et al., 2018; Gao et al., 2019; Wang and Shah, 2018; Stelmakh et al., 2018; Kobren et al., 2019; Noothigattu et al., 2018; Balietti et al., 2016; Xu et al., 2019; Fiez et al., 2019) in the domain of addressing these challenges in peer review, by designing a principled method to test for biases. We show that under various conditions the approach used by the prior work of Tomkins et al. does not control the Type-I error rate. We underscore that we do not aim at conﬁrming or disproving the presence of biases found in that work, but our focus is on the validity of testing methods. With this goal in mind, we propose a principled approach to testing for biases and design two statistical procedures that coupled with our novel experimental setup provably control for the Type-I error rate. Additionally, these procedures have non-trivial power under essentially a single assumption of no diﬀerence in the behavior of SB and DB reviewers when the bias is absent. We then show that this assumption cannot be relaxed in general and that to accommodate the aforementioned diﬀerence in behavior one needs to make some modelling assumptions, as we demonstrated with our tests and generalizations of popular linear and logistic models.
We presented the Disagreement and Counting tests in the context of peer review. However, we underscore that one can adapt our experimental setup (Procedure 1) to use our testing procedures (Test 1 and Test 2) in other applications. These applications include peer grading, university admission, and hiring where some protected attributes might be available to reviewers.
18

There are several open problems suggested by our work. The ﬁrst direction is associated with the statistical power of the testing procedures we propose. In this work, we show that our tests have power that going to one under certain conditions on the alternative. It is of interest to establish a bound on the statistical power of our tests in a ﬁnite sample setting and compare it with an upper bound on the maximum power that can be achieved by any computationally-eﬃcient testing procedure.
The second direction is related to the design of the experimental procedure. To accommodate tests for biases, one needs to deviate from the standard peer-review pipeline, thus introducing a trade-oﬀ between the quality of the peer-review process and the accuracy of the testing. Quantiﬁcation of such a trade-oﬀ may help to design a better setup and understand the cost of the experiment in terms of the peer-review quality. In this work, we designed a procedure that leads to the desired accuracy, but is suboptimal in terms of the TPMS objective. In contrast, the optimal TPMS assignment would not allow to perform reliable testing. Hence, an open problem is to design an experimental procedure that accommodates our statistical tests and subject to this maximizes the quality of the assignment in terms of the TPMS objective.
References
Arnold, D., Dobbie, W., and Yang, C. S. (2018). Racial bias in bail decisions*. The Quarterly Journal of Economics, 133(4):1885–1932.
Bakanic, V., McPhail, C., and Simon, R. J. (1987). The manuscript review and decision-making process. American Sociological Review, pages 631–642.
Balietti, S., Goldstone, R. L., and Helbing, D. (2016). Peer review and competition in the art exhibition game. Proceedings of the National Academy of Sciences, 113(30):8414–8419.
Bertrand, M. and Mullainathan, S. (2004). Are Emily and Greg more employable than Lakisha and Jamal? A ﬁeld experiment on labor market discrimination. American economic review, 94(4):991–1013.
Blank, R. M. (1991). The eﬀects of double-blind versus single-blind reviewing: Experimental evidence from the american economic review. American Economic Review, 81(5):1041–1067.
Brunner, J. and Austin, P. C. (2009). Inﬂation of Type I error rate in multiple regression when independent variables are measured with error. Canadian Journal of Statistics, 37(1):33–46.
Budden, A. E., Tregenza, T., Aarssen, L. W., Koricheva, J., Leimu, R., and Lortie, C. J. (2008). Double-blind review favours increased representation of female authors. Trends in Ecology and Evolution, 23(1):4 – 6.
Charlin, L. and Zemel, R. S. (2013). The Toronto Paper Matching System: An automated paper-reviewer assignment system. In ICML Workshop on Peer Reviewing and Publishing Models.
Cohen, J. (1992). A power primer. Psychological Bulletin, 112(1):155–159.
Ernst, E. and Resch, K.-L. (1994). Reviewer bias: a blinded experimental study. The Journal of laboratory and clinical medicine, 124(2):178–182.
Fiez, T., Shah, N., and Ratliﬀ, L. (2019). A SUPER* algorithm to optimize paper bidding in peer review. In ICML workshop on Real-world Sequential Decision Making: Reinforcement Learning And Beyond.
Fisher, R. A. (1935). The design of experiments. Oliver & Boyd, Oxford, England.
Gao, Y., Eger, S., Kuznetsov, I., Gurevych, I., and Miyao, Y. (2019). Does my rebuttal matter? Insights from a major NLP conference. CoRR, abs/1903.11367.
Hill, S. and J. Provost, F. (2003). The myth of the double-blind review? Author identiﬁcation using only citations. SIGKDD Explorations, 5:179–184.
19

Kang, D., Ammar, W., Dalvi, B., van Zuylen, M., Kohlmeier, S., Hovy, E. H., and Schwartz, R. (2018). A dataset of peer reviews (PeerRead): Collection, insights and NLP applications. CoRR, abs/1804.09635.
Kerr, S., Tolliver, J., and Petree, D. (1977). Manuscript characteristics which inﬂuence acceptance for management and social science journals. Academy of Management Journal, 20(1):132–141.
Kobren, A., Saha, B., and McCallum, A. (2019). Paper matching with local fairness constraints. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, KDD ’19, pages 1247–1257, New York, NY, USA. ACM.
Lamont, M. (2009). How professors think. Harvard University Press.
Largent, E. and Snodgrass, R. (2016). Blind peer review by academic journals. In Robertson, C. and Kesselheim, A., editors, Blinding as a Solution to Bias: Strengthening Biomedical Science, Forensic Science, and Law, pages 75–95. Cambridge.
Mahoney, M. J. (1977). Publication prejudices: An experimental study of conﬁrmatory bias in the peer review system. Cognitive therapy and research, 1(2):161–175.
Moss-Racusin, C. A., Dovidio, J. F., Brescoll, V. L., Graham, M. J., and Handelsman, J. (2012). Science faculty’s subtle gender biases favor male students. Proceedings of the National Academy of Sciences, 109(41):16474–16479.
Noothigattu, R., Shah, N., and Procaccia, A. (2018). Choosing how to choose papers. arXiv preprint arxiv:1808.09057.
Okike, K., Hug, K. T., Kocher, M. S., and Leopold, S. S. (2016). Single-blind vs double-blind peer review in the setting of author prestige. JAMA, 316(12):1315–1316.
Seeber, M. and Bacchelli, A. (2017). Does single blind peer review hinder newcomers? Scientometrics, 113(1):567–585.
Shah, N. B., Tabibian, B., Muandet, K., Guyon, I., and Von Luxburg, U. (2018). Design and analysis of the NIPS 2016 review process. The Journal of Machine Learning Research, 19(1):1913–1946.
Snodgrass, R. (2006). Single- versus double-blind reviewing: An analysis of the literature. SIGMOD Record, 35:8–21.
Squazzoni, F. and Gandelli, C. (2012). Saint Matthew strikes again: An agent-based model of peer review and the scientiﬁc community structure. Journal of Informetrics, 6(2):265–275.
Stefanski, L. A. and Carroll, R. J. (1985). Covariate measurement error in logistic regression. Ann. Statist., 13(4):1335–1351.
Stelmakh, I., Shah, N. B., and Singh, A. (2018). PeerReview4All: Fair and accurate reviewer assignment in peer review. arXiv preprint arXiv:1806.06237.
Thorngate, W. and Chowdhury, W. (2014). By the numbers: Track record, ﬂawed reviews, journal space, and the fate of talented authors. In Advances in Social Simulation, pages 177–188. Springer.
Thornhill, T. (2018). We want black students, just not you: How white admissions counselors screen black prospective students. Sociology of Race and Ethnicity, page 2332649218792579.
Tomkins, A., Zhang, M., and Heavlin, W. D. (2017). Reviewer bias in single- versus double-blind peer review. Proceedings of the National Academy of Sciences, 114(48):12708–12713.
Wang, J. and Shah, N. B. (2018). Your 2 is my 1, your 3 is my 9: Handling arbitrary miscalibrations in ratings. CoRR, abs/1806.05085.
20

Webb, T. J., O’Hara, B., and Freckleton, R. P. (2008). Does double-blind review beneﬁt female authors? Trends in Ecology and Evolution, 23(7):351 – 353.
Weisberg, S. (2005). Applied Linear Regression. Wiley, Hoboken NJ, third edition. Xu, Y., Zhao, H., Shi, X., and Shah, N. (2019). On strategyproof conference review. In Proceedings of the
International Joint Conferences on Artiﬁcial Intelligence.
Appendix
We provide supplementary materials and additional discussion.

A More than one property of interest

Throughout the main body of the paper, we considered the case of a single property of interest. We now

generalize some of the results to the case of more than one property of interest. First of all, we recall

some notation. Let k be the total number of properties, then for each paper j ∈ [n], we let variables

wj(1), wj(2), . . . , wj(k) indicate whether or not the paper satisﬁes the corresponding property. For each property

, the set J ⊆ [n] contains papers that satisfy a property with J being its complement.

We argue that when k > 1, one needs to think about the bias testing problem as of an instance of the

relative bias problem as deﬁned in Section 4.2. Indeed, consider for example the case of two properties of

interest (k = 2) and assume that we are interested in testing for biases with respect to the ﬁrst property.

Then even if there is no bias with respect to this property, the behavior of reviewers between SB and DB

conditions might be diﬀerent due to possible biases with respect to the second property.

The negative result of Theorem 3 we established in Section 6.2.2 also applies to the case of multiple

properties, implying that reliable testing is possible only under some restrictions on the diﬀerence in reviewers’

behavior between SB and DB conditions under the absence of bias. Following the relative bias problem

deﬁned as Problem 2, we now generalize it for k > 1. To this end, we consider a problem of testing for biases

with respect to the property ∈ [k] and introduce an additional piece of notation. For each paper j ∈ [n], let

wj

denote

a

vector

of

indicators

of

property

satisfaction:

wj

=

(

w

(1) j

,

wj(2)

,

.

.

.

,

w

(k j

)

)

and

let

wj(−

)

denote

the same vector but with

th

component

omitted,

that

is,

wj(−

)

=

(w

(1) j

,

.

.

.

,

w

( j

−1), wj(

+1)

,

.

.

.

,

w

( j

k

)

).

Following the deﬁnition of the relative bias problem (Problem 2), the set F contains functions that

under the absence of bias with respect to the property , specify the diﬀerence in behavior between DB

and SB conditions. In case of a single property of interest, F was a subset of all non-decreasing functions

f : [0, 1] → [0, 1]. However, when k > 1, even under the absence of the bias with respect to the property ,

the change in reviewers’ behavior between DB and SB conditions may be inﬂuenced by whether the paper

satisﬁes properties other than , due to possible biases with respect to these properties. Hence, under the

absence of bias with respect to the property , the change of behavior between SB and DB conditions is

described as follows:

∀(i, j) ∈ [n] × [m] : πi(jsb) = f0(πi(jdb), wj(− )),

where function f0 ∈ F is non-decreasing in its ﬁrst argument. Thus, the set F is a subset of all functions with domain [0, 1] × {0, 1}k−1 which are non-decreasing in their ﬁrst argument, that is,

F ⊆ f : [0, 1] × {0, 1}k−1 → [0, 1] f is non-decreasing in its ﬁrst argument .

Having deﬁned the necessary notation, we are ready to introduce the relative bias problem in case of multiple properties of interest.

21

Problem 3 (Relative bias problem for multiple properties.). Given signiﬁcance level α ∈ (0, 1), the property of interest , the class of functions F and decisions of SB and DB reviewers, the goal is to test the following hypotheses:

H0 : ∀i ∈ [m] ∀j ∈ [n] H1 : ∀i ∈ [m] ∀j ∈ [n]

πi(jsb) = f0(πi(jdb), wj(− ))
πi(jsb) ≥ f0(πi(jdb), wj(− )) πi(jsb) ≤ f0(πi(jdb), wj(− ))

if j ∈ J ,
if j ∈/ J

(11)

for some unknown f0 ∈ F , and where at least one inequality in the alternative hypothesis (11) is strict.

Intuitively, under the null hypothesis of absence of bias with respect to the property , the change of reviewers’ behaviour between SB and DB conditions is determined by (i) bias introduced by SB condition itself which is independent of papers’ authorship information and (ii) bias with respect to properties other than the property . The generalized linear and logistic models can be formulated in case of multiple properties of interest as follows.
Generalized linear model. Given a ﬁxed constant ∆ ∈ (0, 0.5), we follow the case of a single property and assume that each paper j ∈ [n] has some unknown representation qj ∈ (∆, 1 − ∆). The generalized linear model assumes that for each (i, j) ∈ [m] × [n], the behaviour of reviewer i if she/he reviews paper j is described by the following parametric equations:

DB: πi(jdb) = qj SB: πi(jsb) = qj + β0(sb) +

β(sb)wj( ),
∈[k]

(12a) (12b)

k
where unknown coeﬃcients are such that |β0(sb)| + |β(sb)| < ∆. Under the generalized linear model, a bias
=1
with respect to the property is present whenever β(sb) = 0.

Generalized logistic model. Given a ﬁxed constant ∆ > 0, the generalized logistic model assumes that (i) for every j ∈ [n], a corresponding representation qj belongs to the interval (−∆, ∆) and (ii) for each (i, j) ∈ [m] × [n], the behaviour of reviewer i if she/he reviews paper j is described by the following parametric equations:

DB: SB:

π(db)

log

ij (db)

= β0(db) + β1qj

1 − πij

π(sb)

log

ij (sb)

= β0(sb) + β1qj

+

1 − πij

β(+sb1)wj( ),
∈[k]

(13a) (13b)

where all coeﬃcients are bounded in absolute value by ∆ and β1 > 0. Under the generalized logistic model, a bias with respect to the property is present whenever β(+sb1) = 0.
Remark. 1. First, provided that matrix Πdb is generated according to the one of the introduced models, one can deﬁne a set of functions F that puts the corresponding model in the context of the relative bias problem for multiple properties of interest deﬁned as Problem 3.
2. The goal under each of the models introduced above is to test the signiﬁcance of the coeﬃcient in equation describing the behavior of SB reviewer that corresponds to the indicator of the property of interest. For example, if we are interested in testing for biases with respect to the property under the generalized logistic model, then we want to test the signiﬁcance of the coeﬃcient β(+sb1) in equation (13b).
3. Notice that in case of multiple properties, the models we introduced above describe reviewers’ behaviour both under the absence of bias and under the presence of bias. In this way they allow simultaneous testing for biases with respect to many properties of interest.

22

Observe that the relationships that describe the behaviour of SB reviewers in models (12b) and (13b) are reminiscent of the linear regression and logistic regression models respectively. As mentioned above, one cannot ﬁt decisions of SB reviewers to these models using existing methods, because one of the covariates (paper representation q) is unknown. In their work, Tomkins et al. employed DB reviewers to estimate this unknown covariate and used these estimates to ﬁt the logistic model. As we discussed in Section 3, this approach leads to an unreliable testing procedure under various realistic conditions.
We now show that using ideas of the Disagreement and Counting tests, one can use decisions of both SB and DB reviewers to eliminate the unknown covariate from the model, thereby enabling standard tools without the need to estimate any covariate.

Proposition 1. Let reviewers i and i be assigned to paper j in SB and DB setups correspondingly, then

(a) Under the generalized linear model the expectation of the quantity Yij −Xi j follows the linear model (12b) with qj = 0:

E [Yij − Xi j ] = β0(sb) +

β(sb)wj( ).

∈[k]

(14a)

(b) Under the generalized logistic model the expectation of the quantity Yij| (Yij = Xi j) follows the logistic model (13b) with qj = 0:

E [Yij | (Yij = Xi j )]

(sb) ( )

log 1 − E [Yij | (Yij = Xi j )] = β0 + β +1 wj ,

∈[k]

(14b)

where β0 = β0(sb) − β0(db).
Proposition 1 provides a mean to eliminate the unknown covariate from the models of SB decisions (12b) and (13b) using decisions of DB reviewers. For example, in case of the generalized logistic model it relies on the core idea of the Disagreement test and suggests conditioning on pairs (SB reviewer, DB reviewer) such that reviewers disagree in their decisions for some paper. After conditioning, decisions of SB reviewers follow model (14b) with all covariates known and hence standard test for logistic regression can be applied to evaluate signiﬁcance of coeﬃcients.
Proposition 1 also allows to avoid using noisy measurements and hence any test for signiﬁcance of the coeﬃcients applied to the models (14a) and (14b) will not be susceptible to issues caused by the use of noisy measurements and misspeciﬁcation of the meaning of q (issues (a) and (b) from Section 3). If one restricts each reviewer to input at most one decision to the testing procedure, then issue (c) will also be mitigated.

B Our tests under the setup of Tomkins et al.
In this section we give additional comments on the applicability of our testing procedures to the setup of Tomkins et al. (2017). To this end, recall that our tests take as input the set of tuples T such that (i) each tuple t ∈ T is of the form t = (jt, Yjt , Xjt , wjt ), where jt is a corresponding paper, Yjt , Xjt are decisions of SB and DB reviewers for this paper and wjt equals 1 if jt ∈ J and −1 otherwise; (ii) each reviewer contributes at most one decision to the set T . Potentially our tests can be coupled with any experimental procedure as long as this procedure enables a construction of such a set T . However, one needs to understand that while Procedure 1 is robust to issues we discussed in Section 3.2, other experimental setups may lead to an inﬂation of the Type-I error.
In the experiment conducted by Tomkins et al. (2017), reviewers were split into two groups (SB and DB) uniformly at random at the very beginning of the experiment. Then two assignments ASB and ADB were computed separately for each group of reviewers. As discussed in Section 3.2, even if both groups of reviewers bid in a blind manner, the design of Tomkins et al. may lead to inﬂated Type-I error. Nonetheless, we now show that even under the setup of Tomkins et al., one can employ the Disagreement and Counting tests to ﬁx issues (a)-(c) with the testing procedure discussed in Section 3.1.

23

B.1 Matching algorithms
Let us ﬁrst introduce two matching procedures that construct an input for our tests under the setup of Tomkins et al. (2017). Given assignment of papers to reviewers in both SB and DB conditions, we discuss two choices of matching algorithms depending on the relationship between parameters λ (required number of reviewers per paper in each condition) and µ (maximum number of papers per reviewer). Notice that our goal is not to maximize the size of T , but instead to maximize the minimum of the number of papers from J included in the set T and the number of papers from J included in the set T . This is because our statistical tests need decisions for papers from both J and J to maximize their power. Depending on the relationship between λ and µ we can solve this problem either exactly or approximately. Case 1 (λ ≥ µ). In this case, each paper can be matched to 1 SB reviewer and 1 DB reviewer by ﬁnding two separate maximum matchings (papers to SB reviewers and papers to DB reviewers) using the Hungarian matching algorithm. We formally present the matching procedure as Algorithm 1.
Algorithm 1 Exact matching algorithm Input: Assignments ASB, ADB of SB and DB reviewers to papers, respectively.
1. Construct a graph G that consists of 3 layers:
• Layer 1. One node for each SB reviewer • Layer 2. One node for each paper • Layer 3. One node for each DB reviewer
and add edges between reviewers and papers according to assignments ASB and ADB. Set T = ∅.
2. Using the Hungarian matching algorithm with uniform tie-breaking ﬁnd matchings MSB and MDB where MSB (respectively MDB) is a maximum 1-1 matching between SB (respectively DB) reviewers and papers (each reviewer is matched to at most 1 paper and each paper is matched to at most 1 reviewer).
3. Leave in graph G only those edges that correspond to matched pairs in MSB and MDB.
4. For any triple of (SB reviewer i1, paper j, DB reviewer i2) such that there is a path from a node that corresponds to reviewer i1 to a node that corresponds to reviewer i2 through a node that corresponds to paper j, add t = (j, Yi1j, Xi2j, wj) to T .
5. Return T .
Lemma 1. For any assignments of SB and DB referees to papers that satisfy (λ, µ)-load constraints with λ ≥ µ, the matching procedure in Algorithm 1 is guaranteed to construct a set of tuples T such that for each paper j ∈ [n] there is one tuple that corresponds to this paper.
Case 2 (λ < µ). In this case we cannot use the above idea, because there does not exist a matching such that each paper is matched to one SB and one DB reviewer, subject to a constraint that each reviewer is matched with at most one paper. While solving the exact optimization problem in this case might be hard, a simple greedy procedure constructs a suﬃciently large matching for the Disagreement and Counting tests to satisfy the non-trivial power requirement. The iterative greedy procedure in each iteration matches one paper from J and one paper from J to 1 SB and 1 DB reviewer and removes those reviewers from subsequent interations to maintain the constraint that each reviewer contributes at most one decision to the set T . We formally introduce the greedy procedure as Algorithm 2.
Lemma 2. For any assignments of SB and DB referees to papers that satisfy (λ, µ)-load constraints, the matching procedure in Algorithm 2 is guaranteed to construct a set of tuples T that for large enough min{|J |, |J |} contains at least c min{|J |, |J |} tuples corresponding to papers from J and at least c min{|J |, |J |} tuples corresponding to papers from J , where c is a constant that may depend only on λ and µ.
24

Remark. 1. If the set T constructed by the Algorithm 1 is such that there exist reviewers who do not contribute any of their decisions to this set, then one can run Algorithm 2 on assignments of these reviewers to papers and obtain the set T . Next, consider the updated set T ∗ = T ∪ T and observe that each reviewer contributes at most one decision to this set.
2. By construction both matching algorithms introduced in this section include at most one decision per reviewer in a set of tuples T .
Algorithm 2 Greedy matching algorithm Input: Assignments ASB, ADB of SB and DB reviewers to papers, respectively.
1. Construct a graph G that cosists of 3 layers:
• Layer 1. One node for each SB reviewer • Layer 2. One node for each paper • Layer 3. One node for each DB reviewer
and add edges between reviewers and papers according to assignments ASB and ADB. Set T = ∅.
2. Find a triple (SB reviewer i1, paper j ∈ J , DB reviewer i2) such that there is a path in graph G from a node corresponding to SB reviewer to a node corresponding to DB reviewer through a node corresponding to a paper. If there are many such triples, break ties uniformly at random. If such a triple exists, deﬁne t1 = (j, Yi1j, Xi2j, wj), otherwise set t1 = ∅.
3. Find a triple (SB reviewer i1 = i1, paper j ∈ J , DB reviewer i2 = i2) such that there is a path in graph G from a node corresponding to the SB reviewer to a node corresponding to the DB reviewer through a node corresponding to the paper. If there are many such triples, break ties uniformly at random. If such a triple exists, deﬁne t2 = (j , Yi1j , Xi2j , wj ), otherwise set t2 = ∅.
4. Update T = T ∪ {t1, t2}. If both t1 and t2 are empty, return T . Otherwise delete reviewers i1, i1, i2, i2 from the graph G together with the corresponding edges and go to Step 2.
Overall, let A denote a procedure that takes assignments ASB and ADB as input and depending on the relationship between λ and µ calls Algorithm 1 or Algorithm 2 to construct the set T .
B.2 Guarantees
Having deﬁned a procedure to construct input for the Disagreement and Counting tests, we are now ready to formulate corresponding theoretical guarantees. Recalling that the experimental setup of Tomkins et al. itself breaks the Type-I error guarantees, we abstract out these issues by assuming that instead of TPMS assignment algorithm or any other algorithm that computes assignments of papers to SB and DB reviewers, the assignment is selected uniformly at random from the set of all assignments satisfying (λ, µ)−constraints. For brevity, we only show the result for the absolute bias problem, but analogue of Theorem 2 also holds.
Proposition 2. For any given signiﬁcance level α ∈ (0, 1), under the setup of the absolute bias problem (Problem 1), let experiment be organized according to the procedure of Tomkins et al. with random assignment. Then the Disagreement and Counting tests coupled with procedure A (Algorithm 1 and Algorithm 2) are guaranteed to control for the Type-I error rate at the level α and also satisfy the requirement of non-trivial power.
Remark. 1. From theoretical standpoint, the requirement of random assignment can be substituted with the following conditions, under which any assignment algorithm can be used: (i) reviewers in both conditions bid blindly and (ii) reviewers’ evaluations are independent of similarities. That is, rows of matrices Πsb and Πdb are assigned to reviewers uniformly at random after the assignment is computed.
25

2. Proposition 2 ensures that the Counting and Disagreement tests, coupled with matching algorithms we introduced above, are robust to issues (a)-(c) discussed in Section 3.1. However, in practice even our robust tests may still be susceptible to issues caused by the experimental setup of Tomkins et al.

C Additional impossibility result for the generalized logistic model

In this section we formulate an additional impossibility result that highlights the generality of the Disagreement test.
As we demonstrated in Theorem 2, the Disagreement test leads to reliable testing under the generalized logistic model. Recalling the deﬁnition of papers representations qj, j ∈ [n], let us now consider an extended version of the generalized logistic model which is given by the following parametric equations describing the behaviour of DB and SB reviewers under the absence of bias

DB: log

π(db)

ij (db)

= β0(db) + β1(db)qj

1 − πij

SB: log

π(sb)

ij (sb)

= β0(sb) + β1(sb)qj ,

1 − πij

(15a) (15b)

where parameters β0(sb), β1(sb) > 0, β0(db), β1(db) > 0 as well as papers’ scores qj, j ∈ [n], are bounded in absolute value but otherwise are allowed to be arbitrary. This extended version specializes to the standard generalized logistic model if β1(sb) = β1(db). In words, under the extended version, the reviewers in SB and DB conditions under the absence of bias may have not only diﬀerent intercepts, but also diﬀerent coeﬃcients in front of q.
The presence of bias is then deﬁned as a violation of model (15b) where the direction of violation is diﬀerent
for papers from J and J .
Unfortunately, as we show in Corollary 2, reliable testing under this extension of the generalized logistic
model is not possible.

Corollary 2. For any signiﬁcance level α ∈ (0, 1), consider the extension of the generalized logistic model given by equations (15a) and (15b). Then no test operating on decisions of SB and DB reviewers can control for the Type-I error rate at the level α and simultaneously satisfy the non-trivial power requirement.

The negative result of Corollary 2 applies to the Disagreement test, implying that even our testing procedure which is robust to various issues discussed in Section 3 cannot handle the extension of the generalized logistic model speciﬁed by equations (15a) and (15b).

D Causal inference viewpoint

When testing for biases in peer review, we aim at discovering a causal relationship between paper’s authorship
information and reviewers’ perception of the paper. In this section, we describe a causal model under which
we approach the problem, and provide an equivalent formulation of the problem from the causal inference
viewpoint.
Recall that a decision of reviewer i for paper j if this reviewer is assigned to this paper in SB setup is denoted as Yij and is a Bernoulli random variable with expectation πi(jsb). Our ultimate goal is to evaluate whether the indicator variable wj ∈ {−1, 1} which encodes the property satisfaction has causal impact on the decisions of SB reviewers. To this end, we assume that for each reviewer i ∈ [m] and for each paper j ∈ [n], probability πi(jsb) can be expressed as:

πi(jsb) = ξ(ri, qj , wj ),

(16)

26

for some unknown function ξ with co-domain [0, 1], where qj is an anonymized content of a paper and ri is an arbitrary complex representation of a reviewer. That is, we assume that decisions of SB reviewers are determined by the paper content, reviewer identity and, possibly, authorship information.
In this notation, we can state a canonical formulation of the bias testing problem:

Problem 1 (Canonical formulation of the bias testing problem). Given signiﬁcance level α ∈ (0, 1), and decisions of SB reviewers that are distributed according to equation (16), the goal is to test the following hypotheses:

H0 : ∀i ∈ [m] ∀j ∈ [n] ξ(ri, qj, 1) = ξ(ri, qj, −1)

H1 : ∀i ∈ [m] ∀j ∈ [n] ξ(ri, qj, 1) ≥ ξ(ri, qj, −1)

(17)

where at least for one pair (i, j) ∈ [m] × [n] the inequality in the alternative hypothesis (17) is strict.

Unfortunately, the formulation of Problem 1 is too challenging and cannot be tested without further assumptions in the peer-review setup for the following reasons:
1. Fully randomized controlled experiments cannot be performed in peer-review settings, because we cannot randomize indicators wj, j ∈ [n], that is, we cannot randomize authors of the papers
2. To facilitate an observational study without further assumptions on function ξ, we need to have many papers with the same content but with diﬀerent authors which is also impossible
Tomkins et al. (2017) attempted to circumvent the aforementioned challenges by making the following assumptions: (i) for each paper j ∈ [n], representation qj is simply a true score of a paper qj∗ ∈ R; (ii) function ξ follows logistic model (1); (iii) function ξ is independent of reviewer identity r; and (iv) double blind reviewers can estimate true scores of submissions with reasonable accuracy. As we discussed in Section 3, even if assumptions (i)–(iv) are satisﬁed, the test used by Tomkins et al. (2017) is at risk of violating its Type-I error guarantees unless DB reviewers estimate true scores of submissions without any noise (which is not the case in conference settings). Of course, further violations of the assumptions exacerbate the issues.
In contrast, in our work we attempt the problem making fundamentally diﬀerent assumptions. Without loss of generality, we denote the probability of reviewer i recommending acceptance for paper j in DB condition as:

πi(jdb) = ξ(ri, qj , 0),

(18)

where the last argument of the function ξ is censored, indicating that DB reviewers do not have access to the authorship information. In this notation, our assumption is formulated as follows:

Assumption 1. Under absence of a bias, the behaviour of reviewers does not change between SB and DB conditions, that is, for any reviewer representation r, for any paper representation q and for any value of the indicator w ∈ {−1, 1}, we have

ξ(r, q, w) = ξ(r, q, 0).

Under Assumption 1, the presence of bias is deﬁned as a deviation of reviewers in SB condition from their behavior in DB condition such that the direction of the deviation is determined by the value of the indicator w. Given that, the canonical formulation of the bias testing problem (Problem 1 ) corresponds to the absolute bias problem (Problem 1).
Observe that Assumption 1 does not restrict the generality of representations q and r and also does not make strong parametric assumptions about function ξ. Instead, it essentially postulates that the condition in which a reviewer is put does not serve as a confounder, that is, under the absence of bias, the probability that reviewer i votes to accept paper j is independent of whether reviewer i reviews paper j in the SB or DB condition.
To accommodate an additional confounding factor — a distributional shift due to assignment of a reviewer in the SB or DB condition which is independent of papers’ characteristics — we substitute Assumption 1 with its less restrictive version.

27

Assumption 2. Under the absence of bias, the behaviour of any reviewer i in SB condition is connected to the behaviour of that reviewer in DB condition through a linking function f0, that is, for any reviewer representation r, for any paper representation q and for any value of the indicator w ∈ {−1, 1}, we have

ξ(r, q, w) = f0(ξ(r, q, 0)),

(19)

where f0 is an (unknown) member of a (known) family F of monotonic functions acting from [0, 1] to [0, 1].
First, observe that if we restrict F to be a singleton containing only the identity function, then Assumption 2 reduces to Assumption 1. However, richer choices of family F allow to incorporate various models of confoundings due to the setup. Second, if we again deﬁne the presence of bias as a deviation from (19), where the direction of the deviation is determined by indicator w, then the canonical formulation of the bias testing problem (Problem 1 ) reduces to the relative bias problem (Problem 2).
In this section we have formulated two assumptions that allow us to perform causal inference and lead to the absolute and relative bias testing problems deﬁned in Section 4. With this formulation, in Section 5 we introduce two statistical procedures to test for biases in the peer-review setup and provide their theoretical analysis in Section 6.

E Setup for simulations
In this section we describe setup for simulations we conducted in this work. Notice that in contrast to the test of Tomkins et al. (2017) which operates on accept/reject decisions of SB reviewers and scores provided by DB reviewers, the tests we introduce in this work operate on decisions of both SB and DB reviewers. Hence, to compare tests we need to specify (i) models of DB/SB reviewers’ decisions and (ii) models of DB reviewers’ scores. All simulations are run for 5000 iterations.

E.1 Simulations in Section 3
We now provide necessary details for the simulations in Section 3.

E.1.1 Measurement error (Figure 2a)

For this simulation we consider the following model of SB and DB reviewers:

DB: SB:

π(db)

log j

= β0 + β1qj∗

1 − πj(db)

π(sb) log ij = β0 + β1qj∗ + β2wj ,
1 − πi(jsb)

(20a) (20b)

that is, model (1) is correct and reviews given by the same reviewer for diﬀerent papers are independent.
Notice that under this model all reviewers are identical and hence issues with the setup do not manifest in
this case.
We set m = 2n = 1000 and µ = λ = 2. At each iteration we independently sample true scores of papers qj∗, j ∈ [n], from uniform distribution U [−2, 2] and assume that mean scores by two DB reviewers assigned to a paper j ∈ [n] estimates true score qj∗ with some Gaussian noise (σ = 0.7). We then sample values of wj, j ∈ [n], such that correlation between q∗ and w equals ϕ for values of ϕ between 0 and 0.5. To this end, we let each paper j ∈ [n] with the score qj∗ < 0 have wj = 1 with probability 0.5 − γ and wj = −1 otherwise. Similarly, each paper j ∈ [n] with the score qj∗ ≥ 0 has wj = 1 with probability 0.5 + γ and wj = −1 otherwise. We then vary the value of γ ∈ (0, 0.5) to achieve the necessary correlation. Finally, using models (20a) and (20b) with β0 = 1, β1 = 2 and β2 = 0 (no bias condition) we sample decisions of SB and DB reviewers and run the Disagreement test and the test used by Tomkins et al. (2017), setting the signiﬁcance level to be α = 0.05. We then compute a Type-I error as a fraction of iterations in which the null
hypothesis (β2 = 0) was rejected.

28

E.1.2 Model mismatch (Figure 2b)
For this simulation we consider a violation of model (1) and the following model of SB and DB reviewers with β2 = 0 (no-bias condition):

DB: SB:

π(db)

log j

= β0 + β1(qj∗)3

1 − πj(db)

π(sb) log ij = β0 + β1(qj∗)3 + β2wj .
1 − πi(jsb)

To abstract out the eﬀect of measurement error, in this section we assume that the true scores qj∗, j ∈ [n], are known, but the test used by Tomkins et al. (2017) ﬁts the model deﬁned by equation (20b). Besides
the change of correct model and availability of true scores {qj∗, j ∈ [n]}, the simulations follow scenario we described in Appendix E.1.1.

E.1.3 Reviewer calibration (Figure 2c)

In this simulation we model the eﬀect of correlations introduced by reviewer calibration. More concretely,
we construct a model of reviewer calibration under which the test by Tomkins et al. (2017) fails to control
for the Type-I error rate. In this section we assume that true scores of submissions are proportional to the
clarity of the writing. We then sample clarity scores ζj, j ∈ [n], from uniform distribution U[−1, 1] and deﬁne qj∗ = ζj for each j ∈ [n]. Eventually, we consider the following model of reviewer. For each i ∈ [m] and for each j ∈ [n]:

DB: SB:

πi(jdb) = πj(db) + i × I [ζj < 0.5] πi(jsb) = πj(sb) + i × I [ζj < 0.5] ,

where i is reviewers’ leniency which equals 0.4 with probability 0.5 and −0.4 otherwise and πj(db), πj(sb) are deﬁned by equations (20a) and (20b) with β0 = 0, β1 = 0.25 and β2 = 0 (no bias condition). Parameters are selected to ensure that 0 ≤ πi(jdb), πi(jsb) ≤ 1.
In words, the above model says that for papers with high quality of writing (ζ > 0.5) reviewers understand

their content well and follow models (20a) and (20b) exactly, but for papers with lower writing quality

their leniency parameter inﬂuences their decision. Notice that under this model it is natural to expect that

estimates of the true scores provided by DB reviewers are also inﬂuenced by their leniency and hence are

noisy. However, to isolate the eﬀect of reviewer identity we assume that the test used by Tomkins et al. (2017)

knows

true

scores

q

∗ j

,

j

∈

[n],

exactly.

Additionally,

notice

that

marginally

each

reviewer

follows

the

model

deﬁned by equations (20a) and (20b), and hence when µ = 1, the test by Tomkins et al. (2017) has control

over the Type-I error for any correlation between q∗ and w.

In this section we consider an extreme pattern of correlations between q∗ and w. Concretely, we assume

that for any paper j ∈ [n], we have wj = 1 if and only if qj∗ > 0.5 and wj = −1 otherwise. Notice that in practice such strong dependence is unlikely to happen, but we underscore that in practice the test by Tomkins

et al. (2017) also does not have access to noiseless true scores which will cause measurement errors and hence

will exacerbate the issue.

We then perform simulations as discussed above having n = 1000 and λ = 1 ﬁxed and varying the number

of papers per reviewer and using the modiﬁcation of the Wald test with factor variable for each reviewer

added (reviewer-depedent intercept).

E.1.4 Non-blind bidding (Figure 3a)
Formalizing the intuition we mentioned in Section 3.2, we consider a setting with n = 1000, m = 2000, λ = µ = 1 and consider a property of interest “paper has a famous author”. Suppose that during the bidding

29

procedure each reviewer i ∈ [m] gives a score bij ∈ {−1, 0, 1} to each paper j ∈ [n], where bij = 1 means that reviewer wants to review the paper, bij = −1 means that reviewer does not want to review the paper and bij = 0 is an intermediate between bij = 1 and bij = −1. Given the bids, the assignment is computed maximizing the total sum of the bids. Namely, for all (i, j) ∈ [m] × [n] let a binary indicator Aij equal 1 if reviewer i is assigned to paper j and 0 otherwise and let RSB ⊂ [m] be the set of reviewers allocated to SB condition. Then the assignment of SB reviewers to papers is computed maximizing the following objective
subject to the standard (λ, µ)-load constraints.

Aij bij .
i∈RSB j∈[n]
The same objective is used to assign DB reviewers to papers. Next, we suppose that for each paper j ∈ [n] there is a true score qj∗ ∈ [0, 0.9] and that all reviewers belong to one of the following personality types:
• Type A: Lenient reviewers who accept each paper j ∈ [n] assigned to them with probability qj∗ + 0.1 and want to read papers from top authors. If bidding is blind, they do not have any information about author identity and bid 0 on each paper, but if bidding is non-blind, then for each paper j ∈ J reviewer i of type A places a bid bij = 1 and for each paper j ∈ J she/he places a bid bij = −1.
• Type B: Accurate reviewers who accept each paper j ∈ [n] assigned to them with probability qj∗ and do not mind reviewing any paper. Independent of whether bidding is blind or not, reviewer i of type B places a bid bij = 0 on each paper j ∈ [n].
Notice that evaluations of reviewers of both types are unbiased — the probability of acceptance is not determined by author identities. The type of each reviewer is determined independently: reviewer i ∈ [m] is of type A with probability 0.3 and of type B with probability 0.7. Independently, each paper j ∈ [n] belongs to J with probability 0.3 and to J with probability 0.7.
Having deﬁned the setup, in each iteration we independently sample true scores of submissions from U[0, 0.9] (no correlation with indicator w) and compute two bidding matrices: (i) when SB reviewers observe author identities during bidding and (ii) when bidding is blind for both SB and DB reviewers. For each bidding matrix we compute assignments of SB and DB reviewers to papers and pass observed decisions to the Disagreement test and the test used by Tomkins et al. (2017). For the test of Tomkins et al., we assume that true scores qj∗, j ∈ [n], are known exactly.

E.1.5 Non-random assignment (Figure 3b)
In this section we construct a similarity matrix S and formalize the dependence of reviewer’s perception of a paper on similarity between paper and reviewer that leads to the eﬀect demonstrated in Figure 3b. We notice that the construction we provide here is artiﬁcial and serves as a proof of concept for our claim that non-random assignment may violate some key independence assumptions of statistical tests even if it is not based on reviewers’ bids. While in practice we do not expect to observe such speciﬁc similarity matrices, we can still observe some more subtle manifestations of issues caused by non-randomness of the assignment.
First, in this section we assume that assignment is performed using the TPMS algorithms (Charlin and Zemel, 2013), that is, given similarity matrix S between reviewers and papers, each paper is assigned to λ reviewers in a way that each reviewer is assigned to at most µ papers such that total sum similarity of the assignment is maximized.
Second, consider a similarity matrix S, deﬁned as follows. For each reviewer i ∈ [m] and for each paper j ∈ [n]:

Sij = (m + 1 − i) × (n + 1 − j).

(21)

Given that reviewers are allocated to conditions at random, similarity matrices SSB (SB condition) and SDB (DB condition) are constructed by random division of rows of S into two groups of equal size and stacking them into SSB and SDB correspondingly.

30

Third, we assume that each reviewer i ∈ [m] has some value of threshold zi such that if reviewer i is assigned to paper j ∈ [n] in either of setups, reviewer accepts the paper with probability πij given by:

0.9 if Sij ≥ zi πij = qj∗ if Sij < zi, (22)
where qj∗ ∈ [0, 0.9] is a true score of paper j. We also assume that reviewer i in DB condition returns πij as an estimate of qj∗.
Fourth, for every reviewer i we set a value of threshold as follows:

zi = (m + 1 − i) × (n − [(i−1)/2]),

(23)

where [x] is the integral part of x. Fifth and ﬁnally, we assume that true scores q∗ are independently sampled from U[0, 0.9] and sample
indicators w such that they are correlated with q∗, ﬁxing the value of correlation ϕ = 0.45. We also set µ = λ = 1 and m = 2n = 1000. Now we allocate half of reviewers to SB condition and half to DB condition uniformly at random. We then compare the performance of the Disagreement test and the test by Tomkins et al. under (i) experimental setup of Tomkins et al. and (ii) our experimental setup.
The intuition behind our construction of matrix S in equation (21) is that for any square submatrix of S, the TPMS algorithm with parameters µ = λ = 1 will compute an assignment that corresponds to the diagonal of this submatrix. Coupled with speciﬁc choice of thresholds (23), probabilities of acceptance (22) and correlation between q∗ and w at the level of 0.45, this choice of similarity matrix ensures that under the setup of Tomkins et al., with non-zero probability most of SB reviewers will receive papers with similarities above the corresponding threshold and most of DB reviewers will receive papers with similarities below the corresponding threshold or vice versa. Hence, the assignments will be structurally diﬀerent and, as demonstrated by Figure 3b, this diﬀerence will be confused with bias by both Tomkins et al. (2017) and Disagreement tests. In contrast, under our proposed setup the assignments of SB and DB reviewers to papers do not exhibit any structural diﬀerence and hence do not break the Type-I error guarantees of the tests.

E.2 Simulations in Section 1 and Section 6.1
The simulations in Section 1 and Section 6.1 were performed under the model of reviewers in (20a) and (20b) following the setup described in Appendix E.1.1 with small diﬀerences. Instead of varying the value of correlation ϕ between q∗ and w, we ﬁx the value of ϕ and vary the number of papers n. Moreover, we independently assign papers to the sets J and J as follows: each paper j such that qj∗ < 0 belongs to the set J with probability 0.5 − γ and otherwise belongs to the set J , similarly, each paper j with qj∗ > 0 belongs to the set J with probability 0.5 + γ and otherwise belongs to the set J . The value of γ is selected to achieve the required level of correlation ϕ between q∗ and w.
• For Figure 1a we set ϕ = 0.4 and perform simulations under β0 = 1, β1 = 2, β2 = 0 (no bias), λ = 2, µ = 1, where true scores are sampled from U[−1, 1]. We see that for the test used by Tomkins et al. (2017) a violation of Type-I error guarantees caused by measurement error coupled with correlations (see Appendix E.1.1 for details) exacerbates as sample size grows.
• For Figure 1b we set ϕ = 0.6 and perform simulations under β0 = 1, β1 = 2, β2 = −0.35 (bias against papers that satisfy the property), λ = 2, µ = 1, where true scores are sampled from U[−0.5, 0.5]. We see that in this case measurement error has strong harmful impact on the power of the test used by Tomkins et al. (2017).
• For Figure 1c we set ϕ = 0 and additionally assume that DB reviewers estimate true scores with no noise. In this case all parametric assumptions made by Tomkins et al. (2017) are satisﬁed. We then perform simulations under β0 = 1, β1 = 2, β2 = 0.35 (bias in favour of papers that satisfy the property), λ = 2, µ = 1, where true scores are sampled from U[−1, 1].

31

• Simulations in Section 6.1 follow the simulations in Figure 1b and Figure 1c with the exception that the Counting test is added for comparison.

E.3 Simulations in Section 6.2

In this section we illustrate that the Counting test designed to control for Type-I error under the generalized linear model does not lead to reliable testing under the generalized logistic model under which the Disagreement test is suitable, and vice versa. To this end, we design two instances of the relative bias problem under the generalized linear model — instance (i) with presence of bias and instance (ii) with absence of bias. Our construction ensures that the resulting matrices Πsb and Πdb simultaneously also fall in the relative bias problem under the generalized logistic model with the exception that instance (i) corresponds to absence of bias under the generalized logistic model and instance (ii) corresponds to the presence of bias under this model.
Instance (i) Under the generalized linear model, for each paper j ∈ J let qj∗ = 0.7 and for each paper j ∈ J let qj∗ = 0.5. Additionally, let ν = 0.175. This choice of parameters deﬁnes matrices Πs1b and Πd1b that are generated according to the equations (5a) and (5b) and fall under the null hypothesis of no bias.
Instance (ii) Under the generalized linear model, for each paper j ∈ J let qj∗ = 0.65 and for each paper j ∈ J let qj∗ = 0.25. Now let matrix Πd2b be deﬁned according to the model (5a) and matrix Πs2b be deﬁned as follows:

(sb)

qj∗ + ν1 if wj = 1

πij = qj∗ + ν2 if wj = −1 , (24)

where we carefully select ν2 > ν1 as explained below. This choice of parameters leads to a correct alternative hypothesis of presence of bias against papers that satisfy the property of interest.
We now simulate reviewers decisions with λ = 2, µ = 1, n = 1000, m = 4000, independently allocating
each paper to J with probability 0.5 and to J otherwise. We then apply the Counting and Disagreement tests for each of these instances, and present the results in Figure 5a. Instance (i) allows to compare Type-I
error rates, and instance (ii) allows to compare powers of the tests under the generalized linear model.
One can verify that the instances we constructed above under the generalized linear model also fall
under the generalized logistic model for some speciﬁc choice of parameters. Indeed, consider an instance of
the generalized logistic model speciﬁed by parameters β0 = −2.5 log 7/3, β1 = 5 log 7/3 and ν = 1. Then a straightforward veriﬁcation shows that matrix Πd1b satisﬁes equation (8a) which speciﬁes the behavior of DB reviewers under the generalized logistic model. Next, observe that for each reviewer i ∈ [m] and for each paper j ∈ J the corresponding entry of the matrix Πs1b is larger than prescribed by the model of SB reviewers under the absence of bias (8b). Similarly, for each paper j ∈ J the corresponding entry of the matrix Πs1b is smaller than it should be if the bias is absent (8b). Hence, the pair of matrices Πd1b, Πs1b satisﬁes the alternative hypothesis under the generalized logistic model.
Conversely, consider an instance of the generalized logistic model speciﬁed by parameters β0 = log 1/3 − 0.625 log 39/7, β1 = 2.5 log 39/7 and ν = 1.5. Then a straightforward veriﬁcation shows that matrix Πd2b satisﬁes equation (8a) which speciﬁes the behavior of DB reviewer under the generalized logistic model. Recall that at
this point we didn’t specify how we selected values ν1, ν2 in equation (24). In fact, we selected these values such that entries of the matrix Πs2b satisfy equation (8b) which speciﬁes the behavior of SB reviewers under the generalized logistic model when the bias is absent. Namely, we set

ν1 = −0.65 + (1 + exp {−β0 − ν − 0.65β1})−1 ν2 = −0.25 + (1 + exp {−β0 − ν − 0.25β1})−1

As a result, the pair of matrices Πd2b, Πs2b satisﬁes the null hypothesis under the generalized logistic model. Finally, the power of the Counting test in Figure 5a becomes the Type-I error rate under the instance
of the generalized logistic model with β0 = log 1/3 − 0.625 log 39/7, β1 = 2.5 log 39/7 and ν = 1. Similarly, the

32

Type-I error of the Counting test in Figure 5a becomes its power under the instance of the generalized logistic model with β0 = −2.5 log 7/3, β1 = 5 log 7/3 and ν = 1. The same applies to the Disagreement test and eventually we obtain Figure 5b by simply exchanging the bars in Figure 5a.

F Proofs of main results
In this section we give proofs of our main results.

F.1 Proof of Theorem 1
We prove Theorem 1 in two steps. First, we show the result for the Disagreement test and then for the Counting test. Before we delve into proofs, let us make two observations that we use in this section.
Observations: A For every paper j ∈ [n], if in assignment A∗ (Step 2 of Procedure 1) paper j is attributed to reviewers i1
and i2, then the events “reviewer i1 is allocated to SB and reviewer i2 is allocated to DB” and “reviewer i1 is allocated to DB and reviewer i2 is allocated to SB” are mutually exclusive and happen with probability 0.5 each. This is ensured by Step 3 of Procedure 1 where reviewers are allocated to conditions.
B By construction of Procedure 1, at least c min{|J |, |J |} papers each from sets J and J appear in assignment A∗ for some constant c that depends only on parameters λ and µ. Indeed, in cases (a) and (c) of Step 2, all papers are included into assignment A∗ and hence our claim holds with c = 1. In case (b) m2 ≥ µλ n ≥ 2 µλ min{|J |, |J |} papers are selected and hence our claim holds with c = µλ .

F.1.1 Proof for Disagreement test
The proof of Theorem 1 for the Disagreement test consists of two parts. First, we show that under the null hypothesis deﬁned in Problem 1, for any matrices Πdb and Πsb(= Πdb) and for any assignment A∗ constructed by Procedure 1 in Step 2, the test rejects the null with probability at most α. Second, we show that if the number of papers in both J and J is large enough, then the Disagreement test satisﬁes the requirement of non-trivial power.
We prove both parts conditioned on the assignment A∗. The unconditional statement of the theorem then follows from the law of total probability.

Control over Type-I error Let Πdb and Πsb(= Πdb) be arbitrary matrices that fall under the deﬁnition of null hypothesis in Problem 1. Consider arrays U and V constructed in Step 2 of the Disagreement test from the set of tuples T passed to the test by Procedure 1. If any of them is empty, the test keeps the null and hence does not commit the Type-I error. Now without loss of generality assume that both U and V are non-empty.
The idea of the proof is to show that under the null hypothesis, entries of arrays U and V are mutually independent and identically distributed. Assume for the moment that it is indeed the case. Then entries of arrays U and V are exchangeable random variables and hence the permutation test with statistic τ deﬁned in Step 3 of Test 1 is guaranteed to provide control over the Type-I error rate for any given signiﬁcance level α ∈ (0, 1) and hence the result for Type-I error control follows.
Consider any entry u of array U . Then u is a decision of SB reviewer for some paper jt ∈ J , where t is a tuple that corresponds to u. Corresponding SB and DB reviewers disagree in their decisions, that is, Yjt = Xjt . Recalling Observation A, we deduce that conditioned on assignment A∗, the symmetry of the null hypothesis guarantees that

Yjt | (Yjt = Xjt ) ∼ Bernoulli(0.5).

(25)

Indeed, given that both Yjt and Xjt are Bernoulli random variables, one can verify that

P [Yjt = 1, Xjt = 0] = P [Yjt = 0, Xjt = 1] ,

33

which coupled with the deﬁnition of condition probability implies (25). Hence, entries of array U are Bernoulli random variables with expectation 0.5. Provided that each reviewer
contributes at most one decision to T , entries of U are also independent. The same argument applies to entries of array V and hence we have shown that under the null hypothesis entries of U and V are independent Bernoulli random variables with probability of success 0.5 and thus are exchangeable.

Non-trivial power Consider any ﬁxed choice of δ > 0 and ε > 0 in the deﬁnition of non-trivial power. The goal now is to show that there exists n0 = n0(ε, δ) such that if min{|J |, |J |} > n0, then for any matrices Πdb and Πsb that satisfy the alternative hypothesis in Problem 1 with margin δ, the Disagreement test coupled with Procedure 1 is guaranteed to reject the null hypothesis with probability at least 1 − ε. Throughout the proof we use c to denote a universal constant and allow its value to change from line to line due to multiplications by some other universal constants. Recall that problem parameters λ, µ and α are treated as constants. For concreteness, throughout the proof we assume that the bias is in favor of papers from J . The same argument can be repeated in case of bias against papers from J .
Step 1. Cardinality of U and V . Let us ﬁrst show that arrays U and V will with high probability contain order n0 elements. To this end, recall that for tuple t ∈ T we add Yjt to U if (i) wjt = 1 and (ii) Yjt = Xjt . Observation B ensures that T will contain at least cn0 tuples that correspond to papers from J . Consider any such tuple, and let (jt, i1, i2) be a corresponding paper and two reviewers assigned to this paper in assignment A∗. Then conditioned on assignment A∗, P [Yjt = Xjt ] is lower bounded by:

1 P [Yjt = Xjt ] = 2
1 ≥
2
(i) 1 ≥
2

πi(1sbjt)(1 − πi(2djbt)) + πi(2djbt)(1 − πi(1sbjt))

1 +
2

πi(1sbjt)(1 − πi(2djbt))

1 +
2

πi(2sbjt)(1 − πi(1djbt))

δ2 + δ2 = δ2,

πi(2sbjt)(1 − πi(1djbt)) + πi(1djbt)(1 − πi(2sbjt))

where inequality (i) follows from the fact that for any reviewer i ∈ [m] and for any paper j ∈ [n] we have

δ ≤ πi(jsb) ≤ 1 and 0 ≤ πi(jdb) ≤ 1 − δ by the deﬁnition of non-trivial power requirement.

The same argument applies to tuples t ∈ T that correspond to papers from J . Hence, we conclude that

for any tuple t ∈ T we are guaranteed that Yjt = Xjt with probability at least δ2.

Now notice that |U | =

I [Yjt = Xjt ] and hence E [|U |] ≥ cn0δ2. Applying Hoeﬀding’s inequality,

t∈T : wjt =1

we

can

also

derive

that

for

large

enough

n0

with

probability

at

least

1−

ε 4

we

have

|U | > cn0δ2.

The same argument applies to V and hence we conclude that with probability at least 1 − 2ε we have

|U | > cn0δ2 and |V | > cn0δ2.

(26)

Step 2. Distribution.
Now we describe the distribution of components of U and V . By construction, the entries of these arrays
are independent, so it suﬃces to study a single component. Consider an entry u of array U and let (j, i1, i2) be a corresponding paper and two reviewers assigned to this paper in assignment A∗. For brevity, denote p = πi(1sbj ) ∈ (δ, 1], q = πi(2djb) ∈ [0, 1 − δ), γ1 = p − πi(1djb) and γ2 = πi(2sbj ) − q, where γ1 > δ and γ2 > δ by

34

deﬁnition of non-trivial power requirement. Then, we can derive the following chain of bounds:

2P [u = 1] − 1 = 2P [Yj = 1|Yj = Xj] − 1

= p(1 − q) + (q + γ2)(1 − p + γ1) − 1 p(1 − q) + q(1 − p) (q + γ2)(1 − p + γ1) + (p − γ1)(1 − q − γ2)

(i)

p(1 − q)

(q + δ)(1 − p + δ)

≥

+

−1

p(1 − q) + q(1 − p) (q + δ)(1 − p + δ) + (p − δ)(1 − q − δ)

1 p−q

p − q − 2δ

=

−

2 p + q − 2pq p + q − 2pq + 2δ(δ + q − p)

where inequality (i) holds due to monotonicity of the expression over γ1 and γ2 and lower bounds γ1 > δ, γ2 > δ.
Optimizing the last expression over p ∈ (δ, 1] and q ∈ [0, 1 − δ), we obtain

δ2 2P [u = 1] − 1 ≥ δ2 + (1 − δ)2 ,

and hence P [u = 1] ≥ 21 + 12 δ2+(δ12−δ)2 = 12 + γ. Similarly, we can show that P [v = 1] ≤ 21 − 12 δ2+(δ12−δ)2 = 12 − γ, where γ > 0 is a constant that depends on δ.
Step 3. Permutation.
At this point we are guaranteed that vectors V and U constructed in Step 2 of the Disagreement test, with probability 1 − 2ε , contain at least cn0δ2 elements and their entries are independent Bernoulli random variables. Moreover, the entries of U have expectations larger than 1/2 + γ and entries of V have expectations
smaller than 1/2 − γ, where γ is independent of n0. Conditioned on min{|V |, |U |} > cn0δ2, notice that as n0 grows, the permutation test for exchangeablility
of entries of V and U has power growing to 1. Hence, there exists n∗0 such that if n0 > n∗0, then the permutation test rejects the null with probability at least 1 − 2ε .
Finally, taking union bound over (i) probability that either of U and V has cardinality smaller than cn0δ2 and (ii) probability that the permutation test fails to reject the null given min{|V |, |U |} > cn0δ2, we deduce that conditioned on A∗, the requirement of non-trivial power is satisﬁed. It now remains to notice that the established fact holds for any A∗ that is constructed by Procedure 1 and hence Theorem 1(a) holds.

F.1.2 Proof for Counting test

Similar to the proof for theDisagreement test, the proof for the Counting test consists of two parts — control over Type-I error and non-trivial power. As in the proof for the Disagreement test, we prove both parts conditioned on the assignment A∗ computed in Step 2 of Procedure 1. The unconditional statement of
the theorem then follows from the law of total probability.
Control over Type-I error Let Πdb and Πsb(= Πdb) be arbitrary matrices that fall under the deﬁnition of the null hypothesis in
Problem 1. Consider arrays U and V constructed in Step 2 of the Counting test. If any of them is empty, the test keeps the null and hence does not commit the Type-I error. Now without loss of generality assume that both U and V are non-empty. By construction, conditioned on the assignment A∗, entries of arrays U and V are mutually independent and bounded by 1 in absolute value. Moreover, conditioned on A∗ the size
of arrays U and V is ﬁxed and is not a random variable. Next, we can show that expectation of any entry
of arrays U and V is zero. Indeed, consider any arbitrary entry u ∈ U and let (j, i1, i2) be a corresponding paper and reviewers assigned to this paper in assignment A∗. Then:

1 E [u] = 2

πi(1sbj ) − πi(2djb)

1 +
2

πi(2sbj ) − πi(1djb)

= 0,

where two terms correspond to two equiprobable allocations of reviewers i1 and i2 to conditions and the last equality follows from the fact that under the null hypothesis Πsb = Πdb. Hence, we conclude that the

35

expectation of test statistic γ equals 0. Independence and boundedness of entries of arrays V and U ensure that the test statistic γ is sub-Gaussian random variable with noise parameter σ given by
σ2 = |U |−1 + |V |−1.

Finally, applying Hoeﬀding’s inequality we deduce that

P |γ| >

2 |U |−1 + |V |−1 log 2/α 2 (|U |−1 + |V |−1) log 2/α ≤ 2 exp − 2 (|U |−1 + |V |−1)

= α,

which concludes the proof.

Non-trivial power

Consider any ﬁxed choice of δ > 0 and ε > 0 in the deﬁnition of non-trivial power. The goal now is to show

that there exists n0 > 0 such that if min{|J |, |J |} > n0, then for any matrices Πdb and Πsb that satisfy the

alternative hypothesis in Problem 1 with margin δ, the Counting test coupled with Procedure 1 rejects the

null hypothesis with probability at least 1 − ε. Throughout the proof we use c to denote a universal constant

and allow its value to change from line to line due to multiplications by some other universal constants.

Recall that problem parameters λ, µ and α are treated as constants. For concreteness, suppose that there is

a bias in favor of papers that satisfy the property of interest.

We now consider an arbitrary instance of the bias testing problem with matrices Πsb and Πdb that

fall under the deﬁnition of non-trivial power. First, Observation B ensures that the set T passed to the

Counting algorithm is such that the resulting vectors U and V contain at least cn0 elements each. Next, let

γ1

=

1 |U |

u

and

γ2

=

1 |V |

v, in this notation the test statistic is deﬁned as γ = γ1 − γ2. Conditioned

u∈U

v∈V

on the assignment A∗, we have:

1 E [γ1] = |U | E [u] ≥ δ.
u∈U

Indeed, for any arbitrary entry u of array U let (j, i1, i2) be corresponding paper and reviewers assigned to this paper in assignment A∗. Then requirement of non-trivial power guarantees that

1 E [u] = 2

πi(1sbj ) − πi(2djb)

1 +
2

πi(2sbj ) − πi(1djb)

1 ≥δ+
2

πi(1djb) − πi(2djb)

1 +
2

πi(2djb) − πi(1djb)

= δ.

Similarly,

1 E [γ2] = |V | E [v] ≤ −δ.
v∈V

Applying Hoeﬀding’s inequality we obtain:

δ2 P [γ1 − γ2 < δ] ≤ P [γ1 − γ2 < E [γ1 − γ2] − δ] ≤ exp − 2 (|V |−1 + |U |−1)

≤ exp −cδ2n0 .

On the other hand, the threshold for rejecting the null is such that

2 (|U |−1 + |V |−1) log 2/α ≤ c 1 . n0
Finally, setting n0 = c logδ21/ε , we ensure that if min{|J |, |J |} > n0, then the Counting algorithm with probability at least 1 − ε rejects the null for any matrices Πsb, Πdb that satisfy alternative hypothesis with margin δ.

F.2 Proof of Theorem 2
We prove Theorem 2 separately for the Disagreement test and for the Counting tests.

36

F.2.1 Proof for Disagreement test

Again, the proof is presented in two parts: control over Type-I error and non-trivial power. The conceptual diﬀerence from the proof of the corresponding result for absolute bias problem is that now the parametric relationships (8a) and (8b) allow us to avoid conditioning on the assignment A∗.
Control over Type-I error Let Πdb and Πsb be arbitrary matrices generated from the generalized logistic model under the absence of bias. Consider arrays U and V constructed in Step 2 of the Disagreement test from the set of tuples T passed to the test by Procedure 1. If any of them is empty, the test keeps the null and hence does not commit the Type-I error. Now without loss of generality assume that both arrays U and V are non-empty. Following the idea of the proof of Theorem 1, we need to show that entries of arrays U and V are exchangeable random variables. First, the mutual independence follows from construction of the set T . Second, using equations (8a) and (8b), we deduce that for any paper j ∈ [n] and for any reviewer i ∈ [m]:

log πi(jsb)(1 − πi(jdb)) = ν. πi(jdb)(1 − πi(jsb))

Noticing that πi(jsb) and πi(jdb) under the generalized logistic model are independent of reviewer’s identity, we drop index i from the above equation. Now we consider any entry u of array U together with a corresponding
tuple t = (jt, Yjt , Xjt , wjt ) and conclude that:

P [u = 1] = P [Yjt = 1|Yjt = Xjt ]
= πj(tsb)(1 − πj(tdb)) πj(tsb)(1 − πj(tdb)) + πj(tdb)(1 − πj(tsb))
= 1 πj(tdb)(1−πj(tsb)) 1 + πj(tsb)(1−πj(tdb)) 1
= 1 + e−ν .

(27)

Importantly, the value of the paper representation qj does not appear in equation (27), implying that entries of array U are identically distributed. Applying the same argument to entries of array V we deduce that entries of arrays U and V are exchangeable random variables and hence the permutation test with the test statistic τ deﬁned in Step 3 of Test 1 is guaranteed to control for the Type-I error rate at any given signiﬁcance level α ∈ (0, 1) which concludes the proof.

Non-trivial power Consider any ﬁxed choice of δ > 0 and ε > 0 in the deﬁnition of non-trivial power. The goal now is to show that there exists n0 = n0(ε, δ) such that if min{|J |, |J |} > n0, then for any matrices Πdb and Πsb generated from the generalized logistic model that satisfy the alternative hypothesis in Problem 2 with margin δ, the Disagreement test coupled with Procedure 1 is guaranteed to reject the null hypothesis with probability at least 1 − ε. Throughout the proof we use c to denote a universal constant and allow its value to change from line to line due to multiplications by some other universal constants. Recall that problem parameters λ, µ and α are treated as constants. For concreteness, throughout the proof we assume that the bias is in favor of papers from J . The same argument can be repeated in case of bias against papers from J .

Step 1. Cardinality of U and V . Consider any matrices Πsb and Πdb generated from the generalized logistic model that satisfy the alternative hypothesis in Problem 2 with margin δ. First, we notice that scores qj, j ∈ [n], and coeﬃcients β0, β1 are bounded in absolute value by some constant ∆, and hence using equation (8a) we conclude that for all (i, j) ∈ [n] × [m]

πi(jdb) ∈ ( , b) ∀j ∈ [n], (28)

37

where 0 < < b < 1 and values of and b are determined by ∆. Now consider any tuple t = (jt, Yi1jt , Xi2jt , wjt ) from the set of tuples T . Then

P [Yjt = Xjt ] = πi(1sbjt)(1 − πi(2djbt)) + πi(2djbt)(1 − πi(1sbjt)) ≥ min{πi(2djbt), 1 − πi(2djbt)} πi(1sbjt) + 1 − πi(1sbjt) = min{πi(2djbt), 1 − πi(2djbt)} ≥ min{ , 1 − b},

where the last inequality follows from equation (28). Applying Hoeﬀding’s inequality in the same way as we did in the proof of Theorem 1 to get the bound (26), we deduce that with probability at least 1 − 2ε , cardinalities of arrays U and V are at least cn0 for some constant c that may depend on δ and ∆.
Step 2. Distribution
By deﬁnition of non-trivial power requirement, it must be the case that for all (i, j) ∈ [m] × [n] we have |πi(jsb) − f0(πi(jdb))| > δ, where function f0 belongs to class F∆ deﬁned in (9) and for all (i, j) ∈ [m] × [n] satisﬁes:

f0(πi(jdb))

log

(db) = β0 + ν + β1qj

1 − f0(πij )

= log πi(jdb) + ν, 1 − πi(jdb)

(29a) (29b)

for some value of ν ∈ (−∆, ∆). Observe that values β0, ν, β1, qj in the RHS of equation (29a) are bounded in
absolute value by constant ∆. Next, recall that the deﬁnition of the non-trivial power requirement ensures that for each reviewer i ∈ [m] it must be the case that (a) for each paper j ∈ J we have f0(πi(jdb)) < 1 − δ and (b) for each paper j ∈ J we have f0(πi(jdb)) > δ. Finally, we are guaranteed that for any pair of reviewer i ∈ [m] and paper j ∈ [n] we have

f0(πi(jdb)) ∈ ( , min{b , 1 − δ}) if j ∈ J

(max{ , δ}, b )

if j ∈ J .

Notice that constants and b are such that 0 < < b < 1 and may be diﬀerent from equation (29a) we have additional term ν which is absent in (8a).
Let us now deﬁne two quantities d1 and d2 as

t+δ

t

d1 = t∈( ,miinn{fb ,1−δ}) log 1 − (t + δ) − log 1 − t

t

t−δ

d2 = t∈(maxin{ f ,δ},b ) log 1 − t − log 1 − (t − δ) .

and b, because in
(30a) (30b)

Notice that both quantities d1 and d2 are some functions of δ and ∆ and are strictly positive, because function log 1−xx is strictly increasing on the interval (0, 1) with its derivative being lower bounded by c > 0, where c is independent of problem parameters.
Putting together equations (29a) - (30b), we now show that for each reviewer i ∈ [m] the deﬁnition of
non-trivial power requirement ensures that for each paper j ∈ J

π(sb)

π(db)

log ij ≥ log ij + ν + d1,

1 − πi(jsb)

1 − πi(jdb)

(31)

38

and for each paper j ∈ J

π(sb)

π(db)

log ij ≤ ij + ν − d2.

1 − πi(jsb) 1 − πi(jdb)

(32)

Consider any arbitrary entry u of array U and the corresponding tuple (jt, Yi1jt , Xi2jt , wjt ). Then,

P [u = 1] = P [Yi1jt = 1|Yi1jt = Xi2jt ]
= πi(1sbjt)(1 − πi(2djbt)) πi(1sbjt)(1 − πi(2djbt)) + πi(2djbt)(1 − πi(1sbjt))
= 1 πi(2djbt)(1−πi(1sbj)t ) 1 + πi(1sbj)t (1−πi(2djbt)) 1
≥ 1 + e−ν−d1 ,

where the last inequality follows from (31). Similarly, using (32) we show that for each entry v of array V

1 P [v = 1] ≤ 1 + e−ν+d2 .

Step 3. Permutation.

At this point we are guaranteed that vectors V and U constructed in Step 2 of the Disagreement test, with probability 1 − 2ε , contain at least cn0 elements and their entries are independent Bernoulli random variables. Moreover, the entries of U have expectations larger than 1+1e−ν + γ and entries of V have expectations smaller

than

1 1+e−ν

− γ,

where

γ

is

independent

of

n0,

but

depends

on

δ

and

∆.

Conditioned on min{|V |, |U |} > cn0, notice that as n0 grows, the permutation test for exchangeablility of

entries of V and U has power growing to 1. Hence, there exists n∗0 such that if n0 > n∗0, then the permutation test rejects the null with probability at least 1 − 2ε .

Finally, taking union bound over (i) probability that either of U and V has cardinality smaller than cn0

and (ii) probability that the permutation test fails to reject the null given min{|V |, |U |} > cn0, we deduce

that the requirement of non-trivial power is satisﬁed.

F.2.2 Proof for Counting test
We give a proof for an extended version of the generalized linear model in which for each (i, j) ∈ [n] × [m] we substitute qj with qij, thus allowing subjectivity of reviewers. In the proof we will be using two observations we made in the beginning of Appendix F.1. As in the proof of Theorem 1, we prove the result conditioned on the assignment A∗ constructed in Step 2 of Procedure 1. The unconditional statement of the theorem then follows from the law of total probability.
Control over Type-I error Let Πdb and Πsb be arbitrary matrices generated under the generalized linear model that fall under the null hypothesis in Problem 2. Consider arrays U and V constructed in Step 2 of the Counting test. If any of them is empty, the test keeps the null and hence does not commit the Type-I error. Now without loss of generality assume that both U and V are non-empty. By construction, conditioned on the assignment A∗, entries of arrays U and V are mutually independent and bounded by 1 in absolute value. Moreover, conditioned on A∗ the size of arrays U and V is ﬁxed and is not a random variable. Next, for any arbitrary entry u ∈ U let (j, i1, i2) be a corresponding paper and reviewers assigned to this paper in assignment A∗.

39

Then,

1 E [u] = 2

πi(1sbj ) − πi(2djb)

1 +
2

πi(2sbj ) − πi(1djb)

1

1

= 2 (qi1j + ν − qi2j ) + 2 (qi2j + ν − qi1j )

= ν.

Similarly, it follows that for any arbitrary entry v ∈ V :

E [v] = ν.
Hence, we conclude that the expectation of the test statistic γ equals 0. Independence and boundedness of entries of arrays V and U ensure that the test statistic γ is sub-Gaussian random variable with noise parameter σ given by
σ2 = |U |−1 + |V |−1.

Finally, applying Hoeﬀding’s inequality we deduce that

P |γ| >

2 |U |−1 + |V |−1 log 2/α 2 (|U |−1 + |V |−1) log 2/α ≤ 2 exp − 2 (|U |−1 + |V |−1)

= α,

which concludes the proof.

Non-trivial power

Consider any ﬁxed choice of δ > 0 and ε > 0 in the deﬁnition of non-trivial power. The goal now is to show

that there exists n0 > 0 such that if min{|J |, |J |} > n0, then for any matrices Πdb and Πsb generated under

the generalized linear model that satisfy the alternative hypothesis in Problem 2 with margin δ, the Counting

test coupled with Procedure 1 rejects the null hypothesis with probability at least 1 − ε. Throughout the proof

we use c to denote a universal constant and allow its value to change from line to line due to multiplications

by some other universal constants. Recall that problem parameters λ, µ and α are treated as constants. For

concreteness, suppose that there is a bias in favor of papers that satisfy the property of interest.

We now consider an arbitrary instance of the bias testing problem with matrices Πsb and Πdb that fall

under the deﬁnition of non-trivial power. First, Observation B ensures that the set T passed to the Counting

algorithm is such that resulting vectors U

and V

contain at least cn0

elements each.

Next, let γ1 =

1 |U |

u

u∈U

and

γ2

=

1 |V |

v, in this notation the test statistic is deﬁned as γ = γ1 − γ2. Conditioned on the assignment

v∈V

A∗, we have:

1 E [γ1] = |U | E [u] ≥ ν + δ.
u∈U

Indeed, for any arbitrary entry u of array U let (j, i1, i2) be corresponding paper and reviewers assigned to this paper in assignment A∗. Then the deﬁnition of the non-trivial power guarantees that

1 E [u] = 2

πi(1sbj ) − πi(2djb)

1 +
2

πi(2sbj ) − πi(1djb)

1

1

≥ 2 (qi1j + ν + δ − qi2j ) + 2 (qi2j + ν + δ − qi1j )

= ν + δ.

Similarly,

1 E [γ2] = |V | E [v] ≤ ν − δ.
v∈V

40

Applying Hoeﬀding’s inequality we obtain: δ2
P [γ1 − γ2 < δ] ≤ P [γ1 − γ2 < E [γ1 − γ2] − δ] ≤ exp − 2 (|V |−1 + |U |−1)

≤ exp −cδ2n0 .

On the other hand, the threshold for acceptance is such that

2 (|U |−1 + |V |−1) log 2/α ≤ c 1 . n0
Finally, setting n0 = c logδ21/ε , we ensure that if min{|J |, |J |} > n0, then the Counting algorithm with probability at least 1 − ε rejects the null for any matrices Πsb, Πdb that satisfy the alternative hypothesis with margin δ.

F.3 Proof of Theorem 3
Assume that the premises of Theorem 3 are satisﬁed, that is, there exist functions g, h ∈ F and values 0 ≤ x1 < x2 ≤ 1 such that g(x1) < h(x1) and g(x2) > h(x2).
The high-level idea of the proof is to construct matrices Πdb and Πsb which simultaneously satisfy the null hypothesis of Problem 2 speciﬁed by some function f0 ∈ F and the alternative hypothesis of Problem 2 speciﬁed by another function f0 ∈ F with margin δ > 0. If such matrices exist, then there exist two instances of a bias testing problem — one with presence of bias and the other with absence of bias — such that the distributions of the reviewers’ decisions for these two instances coincide. Hence, any test that uniformly controls for the Type-I error rate at the level α for every f0 ∈ F must under the second instance have power upper bounded by α and thus violate the requirement of non-trivial power over the class of functions F.
We begin with building a matrix Πdb. For any reviewer i ∈ [m] and for any paper j ∈ [n] we let
πi(jdb) = x1 if wj = 1 x2 if wj = −1.
Next, we deﬁne Πsb as follows. For any reviewer i ∈ [m] and for any paper j ∈ [n]
πi(jsb) = h(πi(jdb)).
By construction matrices Πsb and Πdb satisfy the null hypothesis speciﬁed by function h ∈ F . On the other hand, notice that for each paper j ∈ J we have
πi(jsb) = h(x1) > g(x1) = g(πi(jdb)),
and for each paper j ∈ J we have
πi(jsb) = h(x2) < g(x2) = g(πi(jdb)).
Hence, matrices Πdb and Πsb also satisfy the alternative hypothesis speciﬁed by function g. Moreover, Πdb and Πsb satisfy this alternative with margin δ = min{|h(x1) − g(x1)|, |h(x2) − g(x2)|} > 0. We now conclude the proof by noting that our construction holds for any choice of parameters λ, µ, n, m and hence the requirement of non-trivial power must be violated by any testing algorithm that controls for Type-I error at the level α ∈ (0, 1).

F.4 Proof of Corollary 1
To prove Corollary 1, we consider any choice of parameters ∆ ∈ (0, 0.5) and ∆ > 0 and construct two functions f0 and f0 together with two numbers 0 ≤ x1 < x2 ≤ 1 such that

41

(i) Functions f0 and f0 describe the behavior of reviewers under the absence of bias under the generalized linear and generalized logistic models respectively, that is, f0 ∈ F∆, f0 ∈ F∆, where class F∆ is deﬁned by equation (6) and class F∆ is speciﬁed in equation (9)

(ii) Values x1 and x2 are such that:

(a)

One

can

select

parameters

q

∗ j

,

j

∈ [n],

that

fall

under

the

deﬁnition

of

the

generalized

linear

model

such that matrix Πdb generated according to the equation (5a) satisﬁes the following equation:

πi(jdb) = x1 if j ∈ J (33) x2 if j ∈ J .

(b) One can select parameters qj∗, j ∈ [n], and β0, β1 that fall under the deﬁnition of the generalized logistic model such that matrix Πdb generated according to the equation (8a) satisﬁes the
equation (33).

(iii) Functions f0 and f0 are such that

sign f0(x1) − f0(x1) × sign f0(x2) − f0(x2) = −1,

where sign(·) is the sign function. That is, at x1 the function f0 is strictly larger than f0 and at x2 the function f0 is strictly smaller than f0, or vice versa.
Assume for the moment that conditions (i)-(iii) are satisﬁed and consider the matrix Πdb whose entries are given by equation (33). Then one can select values of papers’ representations qj, j ∈ [n], such that Πdb satisﬁes the model of DB reviewers in the generalized linear model (5a). Similarly, there exists another choice of papers’ representations qj, j ∈ [n], and parameters β0, β1, such that the same matrix Πdb satisﬁes the model of DB reviewers in the generalized logistic model (8a). Now deﬁne matrix Πs1b whose entries for each (i, j) ∈ [m] × [n] are given by:
πi(jsb) = f0(πi(jdb))
and matrix Πs2b whose entries for each (i, j) ∈ [m] × [n] are given by:
πi(jsb) = f0(πi(jdb)).
Matrices Πdb, Πs1b satisfy the null hypothesis under the generalized linear model speciﬁed by the function f0. Moreover, condition (iii) ensures that they simultaneously satisfy the alternative hypothesis under the generalized logistic model speciﬁed by the function f0 with margin δ = min{|f0(x1)−f0(x1)|, |f0(x2)−f0(x2)|}. Hence, if the testing procedure ψ2 (which has a non-trivial power under the generalized logistic model) is given decisions of SB and DB reviewers sampled according to the pair of matrices Πdb, Πs1b, then it will reject the null hypothesis with probability that goes to 1 as the minimum of |J | and |J | grows. Finally, given that matrices Πdb and Πsb solely determine the distribution of observed reviewers’ decisions, our construction implies that under the generalized linear model procedure ψ2 does not control for the Type-I error rate at any level α < 1.
A similar argument applies to the pair of matrices Πdb, Πs2b, and it follows that under the generalized logistic model the procedure ψ1 does not control for the Type-I error rate at any level α < 1.
To conclude the proof it remains to ﬁnd x1, x2, f0, f0 that satisfy aforementioned conditions (i)-(iii). To this end, let us deﬁne quantities γ1, γ2:
−1
γ1 = max ∆, 1 + exp{∆ + ∆2}

−1

γ2 = min 1 − ∆, 1 + exp{−∆ − ∆2}

.

42

Notice that the value of γ1 is by deﬁnition smaller than 0.5. Moreover, for each pair (i, j) ∈ [m] × [n] it gives a lower bound on the value πi(jdb) that can be generated from both the generalized linear (with parameter ∆)
and generalized logistic (with parameter ∆) models. Likewise, the value of γ2 is at least 0.5 and gives the corresponding upper bound. Hence, any values of x1, x2 such that γ1 < x1 < x2 < γ2 satisfy the condition (ii).
Next, ﬁnd values ν ∈ (0, ∆) and ν ∈ (0, ∆) such that for functions hν ∈ F∆ and gν ∈ F∆ deﬁned in equations (6) and (9) respectively the following equality holds:

hν (0.5) = gν (0.5).

Observe that such values must exist because hν and gν are continuous functions of ν and ν respectively and

lim hν(0.5) = lim gν(0.5) = 0.5.

ν→+0

ν→+0

Consider now two possible cases:
Case 1. Functions hν and gν are such that there exist two points y ∈ (γ1, 0.5) and z ∈ (0.5, γ2) for which the following equation holds:

sign hν (y) − gν (y) × sign hν (z) − gν (z) = −1,

(34)

Observe that in this case conditions (i)-(iii) are satisﬁed by the choice f0 = hν, f0 = gν, x1 = y, x2 = z and hence the result of the theorem follows.
Case 2. Functions hν and gν are such that hν is a tangent line to gν at 0.5. This case reduces to the Case 1 by setting ν = ν − ε for a suﬃciently small ε ∈ (0, ν). Indeed, if ε is suﬃciently small, then due to strict
concavity and diﬀerentiability of the function gν, by shifting the tangent line down we ensure that there exist points y = 0.5 and z ∈ (0.5, γ2) such that

sign hν (y) − gν (y) × sign hν (z) − gν (z) = −1.

Hence, we can satisfy conditions (i)-(iii) by setting f0 = hν , f0 = gν, x1 = 0.5, x2 = z.
To conclude the proof, we notice that Cases 1 and 2 are complementary, because function gν is diﬀerentiable and strictly concave on the interval (0, 1) and function hν is a linear function.

F.5 Proofs of auxiliary results
In this section we give proofs for auxiliary results stated in appendix.

F.5.1 Proof of Proposition 1

We prove Lemma 1 by straightforward veriﬁcation. First, let Yij be generated from model (12b) and Xi j be generated from model (12a). Then

E [Yij − Xi j ] = qj + β0(sb) +

β(sb)wj( ) − qj = β0(sb) +

β(sb)wj( ).

∈[k]

∈[k]

Similarly, let Yij be generated from model (13b) and Xi j be generated from model (13a). Then

[Yij |Yij = Xi j ] =

πi(jsb)(1 − πi(djb))

E π(sb)(1 − π(db)) + π(db)(1 − π(sb))

ij

ij

ij

ij

= 1 + πi(djb)(1 − πi(jsb)) −1 πi(jsb)(1 − πi(djb))





−1

= 1 + exp − β0(sb) +

β(+sb1)wj( ) − β0(db) ,



∈[k]



43

and hence

E [Yij | (Yij = Xi j )]

(sb)

(db)

(sb) ( )

log 1 − E [Yij | (Yij = Xi j )] = β0 − β0 + β +1 wj .

∈[k]

F.5.2 Proof of Lemma 1
Consider any assignment of papers to SB reviewers that satisfy (λ, µ)−constraint with λ > µ. Then pick any subset of papers P ⊆ [n] and denote a set of SB reviewers who are assigned to at least one paper from P as RSB. Then one can notice that
λ|P | |RSB| ≥ µ ≥ |P|,
and hence by Hall’s theorem there exists a matching that maps each paper to one reviewer such that each reviewer is matched to at most one paper. This matching is computed in Step 2 of Algorithm 1.
The same argument applies to DB reviewers and hence, joining these two matchings, the algorithm in Step 4 constructs a set of tuples T where for each paper j ∈ [n] there exists a tuple that corresponds to this paper.

F.5.3 Proof of Lemma 2
Consider any assignments of papers to SB and DB reviewers that satisfy (λ, µ)−constranints. Let γ be a maximum integer that satisﬁes inequality

|J | |J |

γ ≤ min

,

.

4µ 4µ

Without loss of generality, assume that γ > 1. Given that µ and λ are treated as constants and that we only need to proof the result for large enough min{|J |, |J |}, we ignore the cases when min{|J |, |J |} is small.
Consider a graph G before the ﬁrst iteration of Steps 2 - 4 of Algorithm 2. Each paper in this graph is connected to λ SB and λ DB reviewers such that each reviewer is connected to at most µ papers.
Now let (i1, j, i2) and (i1, j , i2) be triples found in the ﬁrst iteration of the algorithm. These triples exists provided that γ > 1. Then in Step 4 we remove reviewers i1, i1, i2, i2 and corresponding edges from graph G. One can see that these reviewers are connected to at most 4µ papers in total and hence before the second iteration of Steps 2 - 4 graph G will have at least |J | − 4µ ≥ 4µ(γ − 1) papers from J and |J | − 4µ ≥ 4µ(γ − 1) papers from J that are connected to λ SB and λ DB remaining reviewers and each of the remaining reviewers (there must be at least 8λ(γ − 1) SB and 8λ(γ − 1) DB reviewers) will be connected to at most µ papers.
By induction we can show that in the ﬁrst γ iterations of Steps 2 - 4 the greedy algorithm will be able to ﬁnd non-empty triples in Steps 2 and 3. Hence the resulting set of tuples T will contain at least γ tuples that correspond to papers from J and at least γ tuples that correspond to papers from J . We then conclude the proof noticing that γ = c min{|J |, |J |}, where c is a constant that depends only on µ.

F.5.4 Proof of Proposition 2
The proof of Proposition 2 follows the idea of the proof of Theorem 1 with some changes which we now discuss. Consider any set of triples C such that (i) each triple c ∈ C is of the form (j, i1, i2) (one paper and two reviewers) and (ii) each reviewer i ∈ [m] appears in at most one triple. Let C denote a collection of all such sets of triples. Then any set of tuples T passed to the Disagreement or Counting tests as input corresponds to one member of C which is constructed as follows: for each t ∈ T let (jt, it, it) be a corresponding paper, SB reviewer and DB reviewer assigned to this paper, then C = (jt, it, it). Conversely,
t∈T
each member C ∈ C gives rise to a family of sets of tuples T(C) which contains 2|C| elements and each element

44

corresponds to a diﬀerent allocation of reviewers in each triple (j, i1, i2) ∈ C to SB and DB conditions. For example, let C = {(j, i1, i2), (j , i1, i2)}, then the family T(C) consists of four sets of tuples:
T1 = {(j, Yi1j , Xi2j , wj ), (j , Yi1j , Xi2j , wj )} T2 = {(j, Yi2j , Xi1j , wj ), (j , Yi1j , Xi2j , wj )} T3 = {(j, Yi1j , Xi2j , wj ), (j , Yi2j , Xi1j , wj )} T4 = {(j, Yi2j , Xi1j , wj ), (j , Yi2j , Xi1j , wj )}
Next, for concreteness assume that λ ≥ µ, that is, Algorithm 1 is used to construct a set T . Then conditioned on the fact that the set of tuples T constructed by the algorithm belongs to T(C), the randomness of the allocation of reviewers to conditions, the random assignment procedure used to assign reviewers to papers in each condition and randomness in the tie-breaking in the matching algorithm ensure that T ∈ U [T(C)], that is, all elements of T(C) are equally likely to be constructed and no other set of tuples can be constructed.
For each member C ∈ C, let P [C] be probability that Algorithm 1 constructs a set of tuples that belongs to T(C). Notice that for some C ∈ C we have P [C] = 0 which happens for example when |C| < n, because Lemma 1 ensures that |T | = n. Now, conditioning on any set C with P [C] > 0 (instead of conditioning on A∗) and using Lemma 1 (instead of Observation B), we repeat the proof of Theorem 1 for both Disagreement and Counting tests. The unconditional result then follows from the law of total probability. The same argument applies to the case when λ < µ and hence we conclude the proof.

F.5.5 Proof of Corollary 2
The high-level idea of the proof is to construct matrices Πdb and Πsb that simultaneously (for diﬀerent choices of β0(sb) and β1(sb) coeﬃcients) satisfy the null and the alternative hypotheses under the extended model given by equations (15a) and (15b).
We begin our construction from specifying values of qj, j ∈ [n]. For each paper j ∈ [n], let

qj = −1 if wj = 1 0 if wj = −1.

Then Πdb is generated from model (15a) with β0(db) = 0 and β1(db) = 1. In this way, for any reviewer i ∈ [m] and for any paper j ∈ [n], probability of acceprance πi(jdb) satisﬁes:
M0 : log πi(jdb) = qj . 1 − πi(jdb)
That is, for any reviewer i ∈ [m] and for any paper j ∈ [n] we have

π(db) =

1 1+e

if wj = 1

ij

0.5 if wj = −1.

We now consider two diﬀerent choices of coeﬃcients for SB reviewers which result into two diﬀerent models of behaviour of SB reviewers under the absence of bias:

M1 (β0(sb) = 1, β1(sb) = 1) : M2 (β0(sb) = 3/2, β1(sb) = 2) :

π(sb) log ij = 1 + qj
1 − πi(jsb)

πi(jsb)

3

log 1 − π(sb) = 2 + 2qj

ij

45

Consider a matrix Πsb whose components for each i ∈ [m] and j ∈ [n] are deﬁned as follows:

πi(jsb) =

0.5
1 1+e−1

if wj = 1 if wj = −1,

it is not hard to see that (i) entries of matrix Πsb satisfy the model M1 and (ii) for each paper j ∈ J corresponding entries of matrix Πsb are larger than prescribed by model M2 by δ > 0 and for each paper j ∈ J corresponding entries are smaller than those prescribed by M2 by δ > 0, where δ is some universal constant. Hence, depending on which model of SB reviewer under the absence of bias (M1 or M2) is correct, pair of matrices (Πdb, Πsb) corresponds to the absence or presence of bias.
Given that matrices Πdb and Πsb solely determine a distribution of reviewers’ decisions, we have shown
that reviewers’ decisions are identically distributed under both null and alternative hypotheses under the
extended version of the generalized logistic model. Hence, we conclude the proof by declaring that any
algorithm that operates on reviewers’ decision and keeps Type-I error below α must have power at most α under the alternative speciﬁed by models M0, M2 and matrices Πsb, Πdb for all values of min{|J |, |J |} and hence violates the non-trivial power requirement.

46

