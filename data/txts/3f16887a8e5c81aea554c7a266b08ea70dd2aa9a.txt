Balancing Rational and Other-Regarding Preferences in Cooperative-Competitive Environments

arXiv:2102.12307v1 [cs.LG] 24 Feb 2021

Dmitry Ivanov * 1 2 Vladimir Egorov * 1 2 Aleksei Shpilman 1 2

Abstract
Recent reinforcement learning studies extensively explore the interplay between cooperative and competitive behaviour in mixed environments. Unlike cooperative environments where agents strive towards a common goal, mixed environments are notorious for the conﬂicts of selﬁsh and social interests. As a consequence, purely rational agents often struggle to achieve and maintain cooperation. A prevalent approach to induce cooperative behaviour is to assign additional rewards based on other agents’ well-being. However, this approach suffers from the issue of multiagent credit assignment, which can hinder performance. This issue is efﬁciently alleviated in cooperative setting with such state-of-the-art algorithms as QMIX and COMA. Still, when applied to mixed environments, these algorithms may result in unfair allocation of rewards. We propose BAROCCO, an extension of these algorithms capable to balance individual and social incentives. The mechanism behind BAROCCO is to train two distinct but interwoven components that jointly affect each agent’s decisions. Our meta-algorithm is compatible with both Q-learning and ActorCritic frameworks. We experimentally conﬁrm the advantages over the existing methods and explore the behavioural aspects of BAROCCO in two mixed multi-agent setups.
1. Introduction
Human cooperation is considered an evolutionary puzzle in the economic literature (Axelrod & Hamilton, 1981; Fehr & Schmidt, 1999; Johnson et al., 2003; Colman, 2006; Rand & Nowak, 2013). Despite the predictions of the rational choice theory to act selﬁshly (Scott, 2000), people of different age, gender, culture, and socioeconomic status engage into co-
*Equal contribution 1HSE University, Russian Federation 2JetBrains Research. Correspondence to: Dmitry Ivanov <diivanov@hse.ru>, Vladimir Egorov <vsegorov@edu.hse.ru>.
The code for this paper is available here.

operation in a multitude of economic situations (Croson & Buchan, 1999; Henrich et al., 2001; Alvard, 2004; Benenson et al., 2007; Chen et al., 2013; Kettner & Waichman, 2016). A notable example of such situations is prisoner’s dilemma (Rapoport et al., 1965), where a rational agent chooses to defect despite his preference of mutual cooperation over mutual defection. One of the possible mechanisms to resolve the paradox implies that the agents take social and other-regarding preferences into account during decision making (Fehr & Schmidt, 1999; Fehr & Fischbacher, 2002).
The questions of emergence and maintenance of cooperation are mirrored in the Multi-Agent Reinforcement Learning (MARL) literature (Tan, 1993; Lowe et al., 2017; Sunehag et al., 2017; Rashid et al., 2018; Foerster et al., 2018; Peysakhovich & Lerer, 2018b). Numerous works have repeatedly demonstrated that purely rational agents are unable to maintain mutually beneﬁcial cooperation, unlike the agents guided by social incentives (Peysakhovich & Lerer, 2018b; Hughes et al., 2018; Jaques et al., 2019; Wang et al., 2019a). Despite this, training fully social agents can be undesirable when fairness is a concern.
As an example, consider the problem of coordination of autonomous vehicles. On the one hand, each car’s passengers have their own goals in terms of destination and desirable arrival time. Treating this problem as fully cooperative, as implied in (Cao et al., 2012; Rashid et al., 2018), may favor solutions where agents sacriﬁce these personal goals for the social good. For instance, a fully cooperative agent would be willing to let the other cars pass and stay on a crossroad indeﬁnitely as long as average arrival time decreases. In contrast, a selﬁsh agent would not. This example illustrates how fairness emerges from selﬁshness. On the other hand, it is still crucial for each agent to avoid creating inconvenient or dangerous situations for other cars. Therefore, this scenario falls in-between selﬁsh and social and requires agents to balance these preferences.
The simplest way to achieve such balance is to train agents on a mixture of selﬁsh and social rewards (Durugkar et al., 2020), which we refer to as Cooperative Reward Shaping (CRS). In this work, we deﬁne selﬁsh reward as the standard reward an agent receives in the environment, and social reward as some combination (e.g. sum) of selﬁsh rewards

Balancing Rational and Other-Regarding Preferences in Cooperative-Competitive Environments

(a) BAROCCO in Q-learning framework

(b) BAROCCO in Actor-Critic framework

Figure 1. BAROCCO. Solid lines represent parts that are used during both training and execution. Dashed lines represent parts that are
only used during training. a) Selﬁsh components predict selﬁsh Q-values Qi and are trained independently. Social components predict per-agent contributions QSi W , combined into social Q-value QSW through mixing network. Social components and mixing network are
trained end-to-end to approximate temporal difference target of social welfare (SW), deﬁned as a combination (e.g. sum) of Qi. Agents act according to combined Q-values Q⊕i , which are convex mixtures of Qi and QSi W . b) Selﬁsh components predict selﬁsh values Vi and are trained independently. Social components predict social Q-values QSi W and are trained to approximate temporal difference target of
social welfare (SW), deﬁned as a combination (e.g. sum) of Vi. Selﬁsh advantages Ai are estimated as temporal differences (TD) of Vi. Social advantages ASi W are estimated by subtracting counterfactual (CO) baselines from QSi W . Decentralized policies πi are trained via policy gradient on combined advantages A⊕i , which are convex mixtures of Ai and ASi W .

of all agents. However, CRS implies decentralized training and does not address several crucial issues of MARL, such as credit assignment, partial observability, and inherent non-stationarity (Agogino & Tumer, 2004; Hernandez-Leal et al., 2017; 2019). The two latter issues can be alleviated by considering global information and actions of other agents during training, as done in MADDPG (Lowe et al., 2017). Still, the combination of CRS and MADDPG does not address credit assignment of agents to the social welfare. On the other hand, all these issues are addressed by the techniques from fully cooperative MARL like QMIX (Rashid et al., 2018) or COMA (Foerster et al., 2018) that were shown to outperform decentralized training in such complex environments as StarCraft 2 (Vinyals et al., 2017). Still, these techniques are only concerned with team performance and ignore fairness.
In this paper we propose a meta-algorithm that extends techniques like QMIX and COMA to mixed environments with capability to balance the incentives. We refer to this meta-algorithm as BAROCCO, i.e. BAlancing Rational and Other-regarding preferences in Cooperative-COmpetitive environments. BAROCCO is based on the insight that in-

stead of relying on a single model to balance incentives via CRS, two distinct components, i.e. selﬁsh and social, can be trained concurrently and combined during decision making. While we show that mathematically the two approaches are equivalent, the latter approach allows us to train the social component via techniques that address credit assignment.
More speciﬁcally, BAROCCO is compatible with both Qlearning and Actor-Critic frameworks. In the case of Qlearning framework, for each agent we train selﬁsh Q-value via Rainbow (Hessel et al., 2018) and social Q-value via QMIX (Rashid et al., 2018). During decision making, the agents choose the action that maximizes the mixture of the two Q-values, and the importance of each Q-value is controlled via predeﬁned prosociality coefﬁcient. In the case of Actor-Critic framework, we train selﬁsh critic via a variant of MADDPG (Lowe et al., 2017) and social critic via COMA (Foerster et al., 2018). Then, the actor is trained via proximal policy gradient (Schulman et al., 2017) using a mixture of predictions of these two critics.
For both frameworks, we show that varying the prosociality coefﬁcient in BAROCCO results in trade-off of efﬁciency and fairness. In particular, we ﬁnd that fully social agents

Balancing Rational and Other-Regarding Preferences in Cooperative-Competitive Environments

may choose to concentrate all environment’s rewards in one particular agent, whereas agents with a non-zero selﬁsh component refuse to make such sacriﬁces. More surprisingly, in some cases we ﬁnd that less social agents are not only more fair but also more efﬁcient.
A crucial novelty of BAROCCO concerns the training of the social component. The natural approach would be to construct common reward as a combination of selﬁsh rewards. Instead, we directly combine selﬁsh values, omitting construction of common reward. We respectively refer to these approaches as short-term and long-term. While in certain cases the two approaches are mathematically equivalent, the long-term approach might be more suitable for mixed environments. We also formulate two qualitative advantages of the long-term approach: compatibility with a broader set of social welfare functions and applicability to a wider range of environments.
Finally, an alternative to achieving fairness through selﬁshness could be to train a fair centralized system by maximizing minimum of agents’ payoffs rather than sum. We show that such procedure can also be viable, but only if the system is trained via the long-term approach used in BAROCCO. In this case, the selﬁsh components are vital for efﬁciency, albeit are only used to estimate target and do not inﬂuence agents’ decisions directly.
2. Deﬁnitions and Background
2.1. Notations
A tuple G = (S, N, A, O, T, r) deﬁnes a temporallyextended Markov game (Littman, 1994), where:
• Let S be set of states s, N be number of players, Ai be set of actions a of player i. Let si = (s, a−i) denote concatenation of state s and actions of other agents a−i
• Let O : S × N → Rd be function that speciﬁes ddimensional observations available to each agent. Let Oi be set of observations oi of agent i.
• Let T : S × A1 × · · · × AN → ∆(S) be transition function, where ∆ is set of discrete probability distributions over S. Let ∆0(S) be the distribution of initial states.
• Let ri : S × A1 × · · · × AN → R be reward function for each player i.
∞
• Let Ri(s) = [γtri(st, a1t , . . . , aNt ) | s0 = s] be
t=0
return of player i in state s with discount factor γ ∈ [0, 1). Let πi : Oi → ∆(Ai) be policy of player i. Let πi(a | oi) be probability of taking action a in local state oi.

• Let Vi(s) = Eπi [Ri(s)] be state value function, Qi(s, a) = Eπi [Ri(s) | ai0 = a] be state-action value function, Ai(s, a) = Qi(s, a) − Vi(s) be advantage function. Subscript t will denote time-step, e.g. Vit = Vi(st). Bold font will denote vector, e.g. a = (a1, . . . , aN ), V(s) = (V1(s), . . . , VN (s)).
• Let SW : RN → R be social welfare function that evaluates well-being of all agents. The simplest example of its application is sum of agents’ rewards: SW (r) = i ri.
2.2. Single-Agent Reinforcement Learning
Deep Q-Learning. In Q-learning (Watkins & Dayan, 1992), the agent’s goal is to learn Q-values for each stateaction pair, and the agent’s policy is to choose actions that correspond to the highest Q-values. This approach has been successfully applied to such complex environments as Atari games when coupled with deep learning (Hessel et al., 2018; Badia et al., 2020). In Deep Q-Networks (DQN) (Mnih et al., 2015), the Q-values are no longer tabular and are instead approximated with a neural network trained with squared Temporal Difference (TD) loss function:

LT D(Qt, yt) = (yt − Qt)2

(1)

where yt = rt + γ maxat+1 Qt+1. The essential features of DQN are a replay buffer, which enables the reuse of past experiences, and a separate network for target estimation, which stabilises training. The performance of DQN was greatly improved in Rainbow (Hessel et al., 2018) by combining several modiﬁcations proposed in different papers.

Actor-Critic. In Actor-Critic framework (Mnih et al., 2016), the Actor’s goal is to learn a policy π(s) that maximizes agent’s long-term payoffs predicted by the Critic. A widely-used method is proximal policy optimization (PPO) (Schulman et al., 2017), where the Actor’s neural network is trained on the following loss:

Lπ(At) = − min(RtAt, clip(Rt, 1 − , 1 + )At) (2)
where Rt = πoπl(da(ta|ts|ts)t) denotes probability ratio of the policies after and before the update. Using this loss ensures that the agent’s policy stays within a trust region during the update. The advantage is deﬁned as At = yt − Vt, where yt = rt + γVt+1. The Critic’s neural network is independently trained to predict Vt by minimizing squared TD error LT D(Vt, yt).

Balancing Rational and Other-Regarding Preferences in Cooperative-Competitive Environments

2.3. Independent and Centralized Multi-Agent Reinforcement Learning
In Multi-Agent Reinforcement Learning (MARL), multiple agents learn and interact in the same environment. One of the simplest approaches in MARL is to train agents independently using unmodiﬁed single-agent RL techniques (Tan, 1993). Unfortunately, this naive approach invalidates convergence guarantees (Lowe et al., 2017) of QLearning (Watkins & Dayan, 1992) and Actor-Critic (Konda & Tsitsiklis, 2000). The reason for that is the inherent non-stationarity of multi-agent environments (Laurent et al., 2011; Hernandez-Leal et al., 2017). Furthermore, independent MARL does not address the issue of credit assignment in environments with common reward (Wolpert & Tumer, 2002; Agogino & Tumer, 2004). Nevertheless, this approach can be effective in both cooperative (Berner et al., 2019) and mixed (Leibo et al., 2017; Tampuu et al., 2017) setups. As the opposite extreme, the fully centralized approach reduces MARL to single-agent RL by controlling all agents simultaneously based on global information. Unfortunately, centralized MARL suffers from scalability issues due to exponential growth of the joint action space in the number of agents (Guestrin et al., 2002; Sunehag et al., 2017).
2.4. Centralized Training with Decentralized Execution
Centralized Training with Decentralized Execution (CTDE) is a compromise between independent and centralized MARL (Kraemer & Banerjee, 2016; Lowe et al., 2017; Sunehag et al., 2017; Rashid et al., 2018; Foerster et al., 2018; Son et al., 2019). Under this paradigm, training can be enhanced with the use of global information as long as it results in decentralized policies. Typically, CTDE techniques alleviate the issues of multi-agent credit assignment and/or non-stationarity while effectively dealing with the curse of dimensionality.
QMIX (Rashid et al., 2018) is an algorithm designed to train multiple Q-learning agents in cooperative environments. During training, it approximates the joint Q-value as a monotonic mixture of the individual Q-values: Q(s, a) = M(Q1(o1, a1), . . . , QN (oN , aN )). During execution, each agent acts according to its individual Q-value Qi(oi, ai), restricting the use of global information to the training phase. The function M is trained as a mixture network in an end-to-end fashion via TD loss LT D(Q(st, at), rt + γ maxat+1 Q(st+1, at+1)). By enforcing the monotonicity of M, the joint Q-value can be factorised in a way that preserves the order of actions. As a result, maximization over the joint action becomes tractable: maxaQ(s, a) = M(maxa1 Q1(o1, a1), . . . , maxaN QN (oN , aN )). To utilize global information s, the weights of the mixture network are predicted with a set of hypernetworks (Ha et al., 2017). QMIX is a direct extension of Value Decomposition

Networks (Sunehag et al., 2017), where the joint Q-value is simply approximated as a sum of agents contributions Qi rather than a monotonic mixture.
Counterfactual Multi-Agent policy gradient (COMA) (Foerster et al., 2018) is an adaptation of Actor-Critic framework to cooperative environments. COMA uses an efﬁciently designed centralized critic, which outputs Q-values Qi(s, ai | a−i) for a speciﬁed agent i based on the global state s and the actions of the other agents a−i. Furthermore, COMA estimates advantage for each agent by marginalising out the agent’s action while keeping actions of other agents ﬁxed: Ai(s, ai | a−i) = Qi(s, ai | a−i) − ai πi(ai | oi)Qi(s, ai | a−i). Decentralized policies πi(oi) are trained on these advantages via policy gradient.
Multi-Agent Deep Deterministic Policy Gradient (MADDPG) (Lowe et al., 2017) is a CTDE algorithm speciﬁcally designed for mixed environments. The core idea is to train DDPG agents using centralized critics conditioned on global state s and actions of all agents a. Similarly to the enhanced critic in COMA, this modiﬁcation reduces variance of policy gradient, as well as addresses non-stationarity and partial observability. However, MADDPG does not concern credit assignment. In our paper, we apply the same centralization of critic as in MADDPG to multi-agent PPO with discrete action spaces when training the selﬁsh components of ActorCritic agents.
2.5. Cooperative Reward Shaping
We broadly deﬁne Cooperative Reward Shaping (CRS) as reward shaping with respect to the behaviour of other agents, e.g. their rewards (Lerer & Peysakhovich, 2017; Peysakhovich & Lerer, 2018a;b; Hughes et al., 2018; Wang et al., 2019b), temporal differences (Hostallero et al., 2018), policies (Jaques et al., 2019), etc. CRS aims to learn cooperative yet not selﬂess policies in mixed environments. In this paper, we will only be concerned with a particular instance of CRS where agents’ rewards are mixed:

ri⊕ = (1 − λ)ri + λSW (r)

(3)

where λ is prosociality coefﬁcient and ri⊕ is combined reward. The agents are fully selﬁsh when λ = 0 and fully social when λ = 1. The social reward deﬁned as a sum of individual rewards is routinely used in MARL papers to train cooperative policies (Lerer & Peysakhovich, 2017; Peysakhovich & Lerer, 2018a;b; Wang et al., 2019b). The idea to train agents on a convex mixture of selﬁsh and social rewards similar to (3) is explored by Durugkar et al. (2020). While being simple, this approach is limited in its incapability to ﬁnd some of the Pareto optimal solutions, particularly the solutions that lie in concave regions of the Pareto front (Vamplew et al., 2008).

Balancing Rational and Other-Regarding Preferences in Cooperative-Competitive Environments

3. BAROCCO
3.1. Factorization of CRS
While CRS can achieve balance between selﬁsh and social incentives, it does not address multi-agent credit assignment, which can be crucial for performance. At the same time, CTDE algorithms like QMIX and COMA address credit assignment but are intended for cooperative environments, requiring agents to forgo selﬁsh incentives. As a middleground, we notice that the value Vi⊕ that CRS agents optimize can be factored as a mixture of selﬁsh and social values Vi and V SW respectively:

Vi⊕(s) = Eπi γt ((1 − λ)rit + λSW (rt)) | s0 = s
t

= (1 − λ)Eπi γtrit + λEπi γtSW (rt) | s0 = s

t

t

= (1 − λ)Vi(s) + λV SW (s) (4)

where expectations over policies of other agents π−i and over transition function T are omitted for brevity. Note that the same factorization can be applied to Q-value Q⊕i and advantage A⊕i .
The factorization (4) allows us to train the social component separately from the selﬁsh component via algorithms like QMIX and COMA that address credit assignment in cooperative environments. This technique forms the basis for BAROCCO. In Section 3.2, we take a more in-depth look on the social value and propose an alternative deﬁnition that is not based on common reward. Then, in Sections 3.3 and 3.4 we discuss the speciﬁcs of training and combining the two components within Q-learning and Actor-Critic frameworks. Pseudocode of BAROCCO is available in Appendix.
3.2. Assessing Social Welfare
In the previous subsection, we deﬁned social value based on common reward SW (r), which is a combination of individual rewards ri of all agents. We will denote this value as V SW S , where subscript S stands for ‘short-term‘. For convenience, this deﬁnition is repeated in (5). Training COMA critic or QMIX on TD loss based on this deﬁnition of value when the agents are fully social (i.e. λ = 1) is the most straightforward way to extend these algorithms to mixed environments that will be referred to as Vanilla.
The difference between BAROCCO and Vanilla algorithms is two-fold. First, BAROCCO agents can consider both selﬁsh and social motives during decision making, which is also reﬂected in the modiﬁed training procedure. This will be discussed in details in the following subsections. Second, BAROCCO utilizes an alternative deﬁnition of social value

V SW L , formulated in (6) and referred to as ‘long-term‘. Long-term value is not based on common reward SW (r) and is instead deﬁned as a combination of agents’ selﬁsh values Vi. Essentially, the two values V SW S and V SW L differ in the order in which expectation, sum, and social welfare function are applied. We experimentally conﬁrm that replacing V SW S with V SW L can increase performance. Additionally, we identify two qualitative advantages of the long-term value. We brieﬂy formulate these advantages below and verify them experimentally in Section 4.3. We also provide detailed examples in Appendix.

V SWS = E

γtSW (rt)

(5)

πt

V SWL = SW E

γtrt

πt

= SW (Vt) (6)

The ﬁrst limitation of V SWS is in the choice of social welfare functions SW . When SW is chosen as sum, V SWS is mathematically equivalent to V SWL due to commutativity of sum with expectation (although practical implementations of the algorithms still differ). However, this is not always the case. For instance, choosing SW as minimum can be a way to account for both efﬁciency and fairness (Rawls, 2009). In this case, maximizing V SWS requires fair reward distribution at each time-step, whereas to maximize V SWL the rewards should only be fairly distributed on average. While solving the ﬁrst task is sufﬁcient for solving the second, it is also unnecessarily constraining and might result in poor performance. Our experiments support this conjecture.
The second limitation of V SWS is inapplicability to environments where trajectory lengths are variable. As an example, consider an environment where the agents receive negative rewards upon termination. In such environment, an agent that maximizes V SWS might adopt two opposite strategies. The ﬁrst strategy is to prolong the episodes of all agents, thus postponing the negative rewards. The second strategy is to terminate own episode early, thus avoiding the negative rewards from other agents altogether. In contrast, an agent that optimizes V SWL anticipates termination of other agents regardless of witnessing it and therefore can only adopt the ﬁrst strategy. This issue is akin to the bias in rewards identiﬁed in generative adversarial imitation learning (Kostrikov et al., 2018).
As a side note, if simultaneous optimization of payoffs of multiple agents is viewed as multi-objective optimization, then the proposed long-term approach to MARL corresponds to the ‘scalarization of the expected return‘ approach to multi-objective RL (Roijers et al., 2013). It could also be interesting to explore the alternative ‘expectation of the

Balancing Rational and Other-Regarding Preferences in Cooperative-Competitive Environments

(a) Eldorado

(b) Harvest

Figure 2. Environments. Illustration of Harvest map is taken from (Hughes et al., 2018).

scalarized return‘ approach, which would imply changing the order of SW function and expectation in (6), but we leave this direction to the future work.
3.3. Combining Independent DQN and QMIX
Here we describe BAROCCO in Q-learning framework. The algorithm is schematically illustrated in Figure 1a.
When choosing an action, each agent maximizes the following convex combination of Q-values:
Q⊕i (oi, ai) = (1 − λ)Qi(oi, ai) + λQSi W (oi, ai) (7)
where Qi, QSi W , and Q⊕i denote selﬁsh, social, and combined Q-values, respectively. Equation (7) is, in essence, equation (4) rewritten for Q-values, but with one distinction: the social component is not common but is based on each agent’s contribution to social welfare. These contributions are disentangled via mixture network, as proposed in QMIX. Although the two Q-values Qi and QSi W are optimized separately, they still affect each other through the agent’s policy.
For each agent i, the selﬁsh Q-value Qi is trained via independent Q-learning (see Section 2.2). In particular, we use Rainbow architecture (Hessel et al., 2018), which is a modiﬁcation of DQN (Mnih et al., 2015). The only important distinction is that the agents do not act according to the estimated Q-values, i.e. they maximize Q⊕i rather than Qi. At the same time, Qi should be the expectation over the behavioural policy πi according to the deﬁnition of selﬁsh value in (4). To account for this discrepancy, the TD target is modiﬁed akin to double Q-learning. Speciﬁcally, maximization of Qi over actions is replaced with Qi of the action that maximizes Q⊕i :
yit = rit + γQi(oit+1 , argmaxait+1 Q⊕i (oit+1 , ait+1 )) (8)
The social component is based on QMIX. The common Q-value QSW is trained on TD loss and is disentangled into

agents’ individual contributions QSi W via mixture network (see Section 2.4). These individual contributions constitute
social components for each agent. We explore two alternative estimates of TD target ySW for QMIX that correspond
to two deﬁnitions of social values, discussed in Section 3.2.
The ﬁrst estimate (9) is based on the common reward and
the prediction of QMIX for the next state. As in the case of
the selﬁsh component, the target is modiﬁed with respect to the combined Q-values Q⊕i . When λ = 1, this target is equivalent to the target used in Vanilla QMIX. The second
estimate (10), used in BAROCCO, is based on TD targets
for the selﬁsh components.

ytSWS = SW (rt) + γQSW (st+1, argmaxa Q⊕(ot+1, a )) (9)

ytSWL = SW (yt)

(10)

In our implementation, both Vanilla QMIX and BAROCCO utilize noisy exploration (Fortunato et al., 2017).

3.4. Combining MADDPG, COMA, and PPO
Here we describe BAROCCO in Actor-Critic framework. The algorithm is schematically illustrated in Figure 1b.
Each agent acts according to its decentralized policy πi(oi) trained on PPO loss Lπ(A⊕i ), where A⊕i is a convex combination of selﬁsh and social advantages Ai and ASi W :

A⊕i (si, ai) = (1 − λ)Ai(si, ai) + λASi W (si, ai) (11)
For agent i, the selﬁsh advantage is estimated as TD of a critic that predicts the agent’s selﬁsh value: Ai = yit − Vit , where yit = rit + γVit+1 . The selﬁsh critic estimates value with respect to the behavioural policy πi, which corresponds to the deﬁnition of Vi in (4). So, no additional modiﬁcations of its target are required. Note that instead of using only local observations, the critic makes predictions based on concatenation of global state and actions of other agents si. Therefore, it is trained with a variation of MADDPG.

Balancing Rational and Other-Regarding Preferences in Cooperative-Competitive Environments

The social component is based on COMA. For each agent,

its social critic is trained on TD loss and predicts social

Q-value QSi W (si, ai).

Then,

the

advantage

A

S i

W

(si

,

ai

),

i.e. the effect of the agent’s actions on social welfare, is

estimated by subtracting counterfactual baseline from the

social Q-value (see Section 2.4). This advantage enters

(11) as the social component. We explore two alternative

estimates of TD target ySW for COMA that correspond to

two deﬁnitions of social values, discussed in Section 3.2.

The ﬁrst estimate (12) is based on the common reward and

the prediction of COMA for the next state. When λ = 1,

this target is equivalent to the target used in Vanilla COMA.

The second estimate (13), used in BAROCCO, is based on

TD targets for the selﬁsh components yit .

yiStWS = SW (rt) + γQSi W (sit+1 , ait+1 ) (12)

ytSWL = SW (rt + γV(st+1)) = SW (yt)

(13)

In our implementation, neither critics nor policies share weights.

4. Experiments
4.1. Modiﬁed Prisoner’s Dilemma
As a motivational example that illustrates importance of balance between selﬁsh and social incentives, we present modiﬁed prisoner’s dilemma (Table 1). In this 2 by 3 matrix game, both agents have access to ‘Cooperate‘ and ‘Defect‘ actions, but one of the agents can also ‘Sacriﬁce‘ his payoffs for the common good. As in the classic prisoner’s dilemma (Rapoport et al., 1965), defection is a dominant strategy for a selﬁsh agent. At the same time, mutual defection is Pareto dominated by mutual cooperation. As a result, selﬁsh agents are stuck with mutual defection, even though both agents would beneﬁt from mutual cooperation. In contrast, a social agent prefers to ‘Cooperate‘ than to ‘Defect‘.

Table 1. Modiﬁed Prisoner’s Dilemma Defect Cooperate Sacriﬁce

Defect

5, 5

15, 0

21, 0

Cooperate 0, 15 10, 10

21, 0

Now, consider the ‘Sacriﬁce‘ action of the column agent. While this action achieves the highest social welfare, it also ensures the worst individual payoff for the second agent. Nevertheless, a social agent always prefers ‘Sacriﬁce‘, regardless of how small the surplus of social welfare over the mutual cooperation is. Instead, an agent that is willing to cooperate but refuses to self-sacriﬁce might be preferable.
We report behaviour of agents trained to solve Modiﬁed Prisoner’s Dilemma with tabular Q-learning in Table 2. We

Table 2. Actions of Agents in Modiﬁed Prisoner’s Dilemma

λ

0-0.3 0.4-0.8 0.9-1

Row player action

D

C

C

Column player action D

C

S

vary λ in [0, 1], each time incrementing it by 0.1. Both agents Defect when λ is low and start to Cooperate when λ is as high as 0.4. The column agent further switches to Sacriﬁce when λ reaches 0.9. As we will see later in the paper, such sacriﬁcial behaviour is not unique to simple matrix games.
The agents were trained with tabular Q-learning for 100000 iterations. The learning rate was set to 0.1. The exploration rate was initialized at 1 and annealed to 0 over the course of training.
4.2. Environments
Eldorado. Eldorado (Fig. 2a) is based on the NeuralMMO environment (Suarez et al., 2019). Two agents navigate on a fully observable grid-like map, collecting two types of resources – water and food. Both water and food tiles provide 6 points of the corresponding resource. The water tile has inﬁnite supply, while the food tile has a recharge period of 6 turns. Each agent has limited capacity for the resources, as well as health pool limited to 10 points. Furthermore, both food and water supplies decrease each turn by 1. If some supply is absent, the health points also start to decrease. Conversely, the health regenerates when both supplies are above the threshold of 16. If an agent’s health reaches zero, the episode terminates with a unitary negative reward. However, if an agent successfully survives for a 1000 steps, its episode terminates with a unit of positive reward. Upon termination, an agent immediately respawns. Additionally, the agents can interact by attacking each other. This action has two effects. First, it decreases the health of the target by 1. With the small probability of 1/50, the damage is doubled. Second, it steals a unit of both resources. Attack is thus a very appealing action in the short terms. However, in order to successfully complete the task, the agents are required to coordinate their movement while refraining from combat.
Harvest. Harvest (Fig. 2b) is a popular environment (Perolat et al., 2017; Hughes et al., 2018; Jaques et al., 2019) where ﬁve agents collect apples on a partially observable grid-like map. Each episode lasts for a thousand steps. The regrowth rate of apples increases with the number of uncollected apples nearby. Therefore, the agents that harvest every apple in sight quickly exhaust the apple supplies. The optimal strategy for a group of agents is to balance harvesting and cultivating apples.

Balancing Rational and Other-Regarding Preferences in Cooperative-Competitive Environments

(a) Lifetime, all algorithms

(b) Gini, all algorithms

(c) Lifetime, BAROCCO with varying λ

(d) Gini, BAROCCO with varying λ

Figure 3. Experiments in Eldorado, Actor-Critic framework. ‘Lifetime‘ denotes sum of agents’ episode lengths, ‘Gini‘ is a metric of unfairness. ‘sum‘ and ‘min‘ denote the choice of SW function.

4.3. Results
We report experimental results for Eldorado and Harvest environments in Figures 3, 4 and 5, 6, respectively. We investigate how varying λ affects agents’ behaviour and performance, as well as compare BAROCCO to baselines, such as selﬁsh baseline, CRS, and Vanilla QMIX / COMA. CRS and Vanilla QMIX / COMA are deﬁned in sections 2.5 and 3.2, respectively. Selﬁsh baseline is deﬁned as BAROCCO without the social component, i.e. with λ = 0. For other algorithms, λ = 1 unless stated otherwise. The algorithms are compared by performance, deﬁned as sum of payoffs, and by fairness, deﬁned according to (Perolat et al., 2017) as unity minus Gini index. We repeat each experiment 3 times. Technical details and hyperparameters are reported in Appendix.
4.3.1. ACTOR-CRITIC AGENTS IN ELDORADO
• Selﬁsh agents are able to coordinate movement, but are unable to refrain from attacking, since this action

is very appealing in short terms. For this reason, they only achieve average lifetime of 800 (Fig. 3a).
• Unlike selﬁsh agents, prosocial ‘BAROCCO, sum‘ agents achieve higher average lifetime (Fig. 3a), but most of it is concentrated in a single agent that collects all resources (Fig. 3b). This illustrates how maximizing sum of agents’ payoffs can result in unfair reward distribution.
• ’BAROCCO, min’ agents manage to cooperate and successfully solve the environment, reaching average lifetime close to optimal (Fig. 3a). This illustrates how optimizing minimum of agents’ payoffs instead of sum favours the solutions where payoffs are distributed evenly.
• Increasing inﬂuence of the selﬁsh component is another way to reject solutions with uneven payoff distribution. When decreasing prosociality coefﬁcient λ, ‘BAROCCO, sum‘ agents are able to escape the lo-

Balancing Rational and Other-Regarding Preferences in Cooperative-Competitive Environments

(a) Lifetime, all algorithms

(b) Gini, all algorithms

(c) Lifetime, BAROCCO with varying λ

(d) Gini, BAROCCO with varying λ

Figure 4. Experiments in Eldorado, Q-learning framework. ‘Lifetime‘ denotes sum of agents’ episode lengths, ‘Gini‘ is a metric of unfairness. ‘sum‘ and ‘min‘ denote the choice of SW function.

cal optimum where one of the agents is exploited and learn to both successfully complete the task (Fig. 3c,d). This illustrates how fairness emerges from selﬁshness. The best performance is achieved when λ = 0.5. It might seem counter-intuitive that the decreasing agents’ prosociality positively affects performance, but similar results were reported by Durugkar et al. (2020).
• Finally, CRS and COMA agents perform abysmal in Eldorado (Fig. 3a). Since these algorithms optimize common reward that is always non-positive in Eldorado, each agent attempts to avoid the negative reward for termination of the other agent and races to terminate earlier, as discussed in Section 3.2.
4.3.2. Q-LEARNING AGENTS IN ELDORADO
• Selﬁsh Q-learning agents perform about as good as selﬁsh Actor-Critic agents, reaching average lifetime of 800 that is evenly distributed (Fig. 4a, b). These agents are unable to refrain from attacking.

• Procosial ‘BAROCCO, sum‘ and ‘BAROCCO, min‘ agents are able to cooperate and successfully survive in the environment (Fig. 4a, b). Unlike the case of ActorCritic agents, Q-learning ‘BAROCCO, sum‘ agents do not converge to a local optimum where one of the agents is exploited.
• CRS again underperforms compared to the selﬁsh baseline. QMIX outperforms the selﬁsh baseline but still performs slightly worse than BAROCCO (Fig. 4a).
• At ﬁrst, decreasing prosociality coefﬁcient λ has negative but slight effect on agents’ performance (Fig. 4c). While λ > 0.3, the agents refrain from attacking and manage to survive in the environment. However, once λ is at least as low as 0.3, survivability drops significantly as agents begin to combat. The existence of such threshold is consistent with the game-theoretic analysis of Durugkar et al. (2020), as well as with our toy experiment in Section 4.1.

Balancing Rational and Other-Regarding Preferences in Cooperative-Competitive Environments

(a) Apples, all algorithms

(b) Gini, all algorithms

(c) Apples, BAROCCO with varying λ

(d) Gini, BAROCCO with varying λ

Figure 5. Experiments in Harvest, Actor-Critic framework. ‘Apples‘ denotes total number of collected apples by all agents in an episode, ‘Gini‘ is a metric of unfairness. ‘sum‘ and ‘min‘ denote the choice of SW function.

4.3.3. ACTOR-CRITIC AGENTS IN HARVEST
• Selﬁsh agents quickly learn to naively harvest every apple in sight and exhaust the supplies long before episode ends. Unable to collude, they only gather about 200 apples per episode (Fig. 5a).
• ‘BAROCCO, sum‘ agents learn to alternate between harvesting and cultivating apples and manage to gather more than 800 apples per episode (Fig. 5a).
• ‘BAROCCO, min‘ agents collect slightly less apples than ‘BAROCCO, sum‘ agents (Fig. 5a), but distribute the apples signiﬁcantly more evenly among themselves (Fig. 5b). This result highlights that optimizing minimum instead of sum of agents’ payoffs might be preferable if fairness is a concern.
• ‘COMA, sum‘ also outperforms selﬁsh agents but is less stable than ‘BAROCCO, sum‘ (Fig. 5a), meaning that our modiﬁcations of the training procedure can be beneﬁcial.

• Decentralized ‘CRS, sum‘ performs better than all other algorithms (Fig. 5a), suggesting that additional complexity of centralized algorithms can hinder performance in some environments. This result contradicts the ﬁndings of the prior literature where centralization of training consistently improved performance (Rashid et al., 2018; Foerster et al., 2018). However, the algorithms suggested in this literature were not tested in complex mixed environments like Harvest before.
• Performance of CRS and COMA plummets when minimum is chosen as SW (Fig. 5a), which is consistent with our predictions formulated in Section 3.2 that optimizing minimum of agents rewards each step might be too restricting. This result also highlights ﬂexibility of BAROCCO in the choice of SW function.
• The effect of varying λ is monotonic: increasing λ improves performance (Fig. 5c) but can result in unfair reward allocation (Fig. 5d).

Balancing Rational and Other-Regarding Preferences in Cooperative-Competitive Environments

(a) Apples, all algorithms

(b) Gini, all algorithms

(c) Apples, BAROCCO with varying λ

(d) Gini, BAROCCO with varying λ

Figure 6. Experiments in Harvest, Q-learning framework. ‘Apples‘ denotes total number of collected apples by all agents in an episode, ‘Gini‘ is a metric of unfairness. ‘sum‘ and ‘min‘ denote the choice of SW function.

4.3.4. Q-LEARNING AGENTS IN HARVEST
• By and large, the results (Fig. 6) are similar to the case of Actor-Critic agents. Selﬁsh agents converge to a naive strategy of collecting every apple in sight. ‘BAROCCO, sum‘ agents outperform selfish agents by balancing harvesting and cultivating apples. ‘BAROCCO, min‘ performs a little worse than ‘BAROCCO, sum‘ but leads to a more even apple distribution. ‘QMIX, sum‘ is less stable than ‘BAROCCO, sum‘, highlighting that our training procedure is more suitable for mixed environments. ‘CRS, sum‘ performs better than all other algorithms. ‘CRS, min‘ and ‘QMIX, min‘ fail to outperform even selﬁsh agents, in contrast to BAROCCO that is ﬂexible in the choice of SW function.
• While the best team performance is achieved when prosociality coefﬁcient λ is maximal (Fig. 6c), this solution favours unfair apple allocation (Fig. 6d). Set-

ting λ = 0.9 leads to slower convergence and slightly lower ﬁnal team performance, but is a considerably less unfair solution.
5. Conclusion
In this paper, we present BAROCCO – a meta-algorithm for combining social and selﬁsh incentives in cooperativecompetitive environments. We conﬁrm the effectiveness of BAROCCO over the existing methods in two mixed multiagent environments for both Q-learning and Actor-Critic frameworks. Speciﬁcally, we ﬁnd that BAROCCO consistently improves over vanilla QMIX and COMA in all experiments, highlighting usefulness of the modiﬁcations that we propose for training these algorithms in mixed environments. Furthermore, we ﬁnd that varying the prosociality coefﬁcient λ results in unique mixtures of selﬁsh and selﬂess behaviour. While decreasing λ typically increases fairness at the expense of efﬁciency, in some cases both efﬁciency and fairness can beneﬁt from the inﬂuence of the

Balancing Rational and Other-Regarding Preferences in Cooperative-Competitive Environments

selﬁsh component. As an alternative way to achieve fairness, BAROCCO also allows to train fair cooperative agents by maximizing minimum of selﬁsh payoffs. An exciting extension of our work could be to train reciprocal agents that dynamically assess the cooperativeness of others and adapt their policies accordingly. We also note that BAROCCO is not limited to the algorithms utilized in this paper, i.e. DQN, PPO, MADDPG, COMA and QMIX. Rather, we propose a uniﬁed framework of two separate modules, which can be modiﬁed by other state-of-the-art techniques from single-agent, mixed, or cooperative setups.
This work contributes to the broader discussion of what constitutes cooperation. Most MARL papers that study mixed environments focus on efﬁciency, but we argue that this metric can be too limiting. Agents that act towards a single common goal are more reminiscent of a swarm system than a group of distinct individuals that could mutually beneﬁt from cooperation. We explore ways to incorporate the notion of fairness into such systems, either by preserving some individuality of the agents or by modifying the centralized objective. We hope that our work sparks further discussion regarding other desirable qualities of multi-agent systems and the means to achieve these qualities.
6. Acknowledgements
This research was supported in part through computational resources of HPC facilities at HSE University. Support from the Basic Research Program of the National Research University Higher School of Economics is gratefully acknowledged.
A. Assessing Social Welfare: Examples
In this section, we elaborate on the advantages of the longterm value approach formulated in Section 3.2.
Applicability of sum and minimum as SW . Consider the following toy environment. A centralized controller distributes positive unitary rewards between two agents for two time-steps. Furthermore, if the same agent is rewarded twice, the second reward is doubled. In this environment, there are 4 options to distribute rewards: 2 options to reward the same agent at both time-steps, and 2 options to reward one agent at the ﬁrst time-step and the other agent at the second time-step. We are interested how to distribute the rewards in order to maximize social welfare. We analyze two deﬁnitions of the prosocial value function, i.e. V SWS and V SWL , as well as two choices of SW function, i.e. sum and minimum. The results of the analysis are summarized in Table 3.
We can observe several patterns consistent with our experimental ﬁndings. First, when SW is chosen as sum, the

value functions V SWS and V SWL are equivalent. This is a consequence of commutativity of sum with itself: the order of summation over time-steps and over agents does not affect on resulting value function. Furthermore, maximization of the social welfare requires to sacriﬁce the interests of one of the agents by choosing either option 1 or 4. Second, when SW is chosen as minimum, all four options are equivalent from the standpoint of V SWS . This is a consequence of the environment design: the controller is unable to reward both agents at the same time-step and minimum of two rewards is always 0. In contrast, maximization of V SWL requires fair reward distribution on average, and thus options 2 and 3 are preferred. Therefore, if fairness is a concern, SW should be chosen as minimum and V SWL , i.e. the long-term approach to deﬁne value function that is used in BAROCCO, should be focused on.
Environments where trajectory lengths vary. Consider a two-agent environment where the only reward that each agent receives is a unitary negative reward upon termination. We are interested in the incentives that drive selﬁsh and prosocial agents in case of such reward structure. In this example, SW function will be chosen as sum. Let the trajectory lengths of the two agents be T1 and T2, respectively, and let T1 < T2. The agent that terminates earlier will be referred to as the ﬁrst agent, and vice versa. The selﬁsh values Vi and the prosocial values ViSWS and ViSWL of the two agents are estimated in Table 4 (expectation operator is omitted).
Depending on the value function that the agents optimize, they might learn different behaviour. First, each of the selfish agents is only incentivized to prolong its own trajectory. As was shown in the literature, such agents may struggle to achieve mutual beneﬁts of stable cooperation (Peysakhovich & Lerer, 2018b; Hughes et al., 2018; Jaques et al., 2019; Wang et al., 2019a). In contrast, the prosocial agents that optimize ViSWL are incentivized to prolong the trajectories of both agents and thus are willing to cooperate. However, this is not the only incentive that drives the prosocial agents that optimize ViSWS . While both such agents do beneﬁt from longer episodes, each agent also prefers to be the ﬁrst agent rather than the second, i.e. terminate earlier. This incentive emerges because the ﬁrst agent does not observe termination of the second agent. Moreover, by comparing values of such agents (Table 4, column 2) it is evident that the ﬁrst agent receives higher payoffs than the second regardless of how long the second agent survives, since −γT2 ≤ 0. Therefore, instead of cooperating to survive, such agents would compete for early termination.
A similar analysis can be performed for the opposite kind of environments where the termination reward is positive. In such environments, the agents are usually required to complete certain tasks. Instead, the agents that optimize

Balancing Rational and Other-Regarding Preferences in Cooperative-Competitive Environments

Table 3. Reward distributions and corresponding social welfare in the toy environment

Rewards of agent 1

Rewards of agent 2

SW is sum V SWS V SWL

SW is min V SWS V SWL

Option 1 Option 2 Option 3 Option 4

[1, 2γ] [1, 0] [0, γ] [0, 0]

[0, 0] [0, γ] [1, 0] [1, 2γ]

1 + 2γ 1 + 2γ 0

0

1+γ 1+γ

0

γ

1+γ 1+γ

0

γ

1 + 2γ 1 + 2γ 0

0

Table 4. Values of two agents in an environment with −1 reward upon termination (T1 < T2)

General formula

Selﬁsh agents
Ti
Vi = γtri
t=0

Prosocial agents

Ti
ViSWS = γt(r1 + r2)
t=0

T1

T2

ViSWL = γtr1 + γtr2

t=0

t=0

Value of agent 1 Value of agent 2

−γT1 −γT2

−γT1 −(γT1 + γT2 )

−(γT1 + γT2 ) −(γT1 + γT2 )

V SWS would delay task completion in attempts to observe termination of the others.
The two discussed environments with positive and negative termination rewards are extreme examples with two opposite artifacts. However, a combination of these artifacts may emerge in an environment with an arbitrary reward structure and varying episode length, which can result in unexpected and suboptimal behaviour.
B. Technical Details
Pseudocode of BAROCCO for Q-learning and Actor-Critic agents is presented in Algorithms 1 and 2, respectively. The choice of hyperparameters for the algorithms is reported in Table 5.
In Q-learning framework, the selﬁsh component is implemented via Rainbow (Hessel et al., 2018), and the prosocial component is implemented via QMIX (Rashid et al., 2018). Neither of the components utilizes parameter sharing for Qi(oi, ai) or QSi W (oi, ai) predictions. Both noisy (Fortunato et al., 2017) and -greedy explorations are applied. The rate of exploration is annealed to 0. This is an important detail, because for a given agent the hard-coded randomness of other agents’ actions can change its optimal policy (Wunder et al., 2010). Both Rainbow and QMIX use experience replay buffers. A well-known issue of experience replay is that it can be harmful in non-stationary environments (Lin, 1992). To address the inherent non-stationarity of multi-agent environments, we adopt the ﬁngerprint technique (Foerster et al., 2017) by adding the exploration rate to the state space. Vanilla QMIX additionally utilizes double Q-learning (Van Hasselt et al., 2016; Fu et al., 2019). Finally, we utilize multiprocessing to perform interaction with environment, update of the selﬁsh components, and

update of the prosocial components in parallel, similarly to APEX (Horgan et al., 2018).
In Actor-Critic framework, the selﬁsh component for each agent is a critic Vi that estimates the agent’s value function based on global information, and the prosocial component is a critic QSi W that estimates social welfare using COMA (Foerster et al., 2018). Again, neither selﬁsh nor prosocial critics share the parameters. The decentralized policies πi⊕ are trained on a combination of selﬁsh and social advantages Ai and ASi W via PPO (Schulman et al., 2017). The combined advantage A⊕i is normalized over batch. To enhance exploration, we apply entropy regularization (Mnih et al., 2016), annealed to 0 over the course of training. All weights of the networks use orthogonal initialization (Hu et al., 2020). Finally, neither of the components utilizes experience replay.
In Eldorado, both agents receive global information as inputs. The state space is a vector with 28 elements. It includes statuses of food tiles, as well as characteristics of both agents, such as their coordinates, health points, resources, and actions taken in the previous turn. The action space consists of 10 possible options, which include 4 movement options, an option to pass, and an option to attack (combined with movement and passing). The only reward that each agent receives is +1 upon surviving for 1000 steps or −1 upon earlier termination.
In Harvest, each agent’s local observation is restricted to a 15 by 15 part of the map, whereas the global state includes information about the whole 16 by 38 map. Both local and global states are 3-dimensional RGB images and are always preprocessed with a 6-channel CNN. The action space consists of 8 possible options, which include 4 movement options, 2 turn options, an option to pass, and an option to attack. The reward structure is the same as in the original

Balancing Rational and Other-Regarding Preferences in Cooperative-Competitive Environments

Table 5. Hyperparameters

Q-learning Eldorado Harvest

discount factor γ Adam learning rate learning rate decay batch size mini-batch size # epochs # FC layers # per-layer FC neurons # LSTM layers # CNN layers target network period exploration rate
decay noisy exploration σ0 entropy coefﬁcient β entropy decay buffer size prioritization exponent # quantiles (selﬁsh component) # steps in n-step returns

0.99

0.99

0.0005 0.0005

0.999995 0.999995

64

128

-

-

3

2

64

64

0

0

0

1

2K

2K

1→0 1→0

0.99999 0.999975

0.5

0.5

-

-

500K

250K

0.6

0.6

10

10

5

5

Actor-Critic Eldorado Harvest

0.99 0.0005 0.999998 2000
500 10 2 128 0 0
0.05 → 0 0.99998 1 1

0.99 0.001 0.9998 3000 500
3 3 64 1 1
0.05 → 0 0.998
1 1

implementation (Hughes et al., 2018): each agent receives +1 per collected apple, −50 for being attacked directly, and −1 for stepping into the ﬁre left after an attack.
References
Agogino, A. K. and Tumer, K. Unifying temporal and structural credit assignment problems. In Proceedings of the Third International Joint Conference on Autonomous Agents and Multiagent Systems-Volume 2, pp. 980–987. IEEE Computer Society, 2004.
Alvard, M. S. The ultimatum game, fairness, and cooperation among big game hunters. Foundations of human sociality, pp. 413–435, 2004.
Axelrod, R. and Hamilton, W. The evolution of cooperation. Science, 211(4489):1390–1396, 1981. ISSN 0036-8075. doi: 10.1126/science. 7466396. URL https://science.sciencemag. org/content/211/4489/1390.
Badia, A. P., Piot, B., Kapturowski, S., Sprechmann, P., Vitvitskyi, A., Guo, D., and Blundell, C. Agent57: Outperforming the atari human benchmark. arXiv preprint arXiv:2003.13350, 2020.
Benenson, J. F., Pascoe, J., and Radmore, N. Children’s altruistic behavior in the dictator game. Evolution and Human Behavior, 28(3):168–175, 2007.

Berner, C., Brockman, G., Chan, B., Cheung, V., De˛biak, P., Dennison, C., Farhi, D., Fischer, Q., Hashme, S., Hesse, C., et al. Dota 2 with large scale deep reinforcement learning. arXiv preprint arXiv:1912.06680, 2019.
Cao, Y., Yu, W., Ren, W., and Chen, G. An overview of recent progress in the study of distributed multi-agent coordination. IEEE Transactions on Industrial informatics, 9(1):427–438, 2012.
Chen, Y., Zhu, L., and Chen, Z. Family income affects children’s altruistic behavior in the dictator game. PloS one, 8(11), 2013.
Colman, A. M. The puzzle of cooperation. Nature, 440 (7085):744–745, 2006.
Croson, R. and Buchan, N. Gender and culture: International experimental evidence from trust games. American Economic Review, 89(2):386–391, 1999.
Durugkar, I., Liebman, E., and Stone, P. Balancing individual preferences and shared objectives in multiagent reinforcement learning. Good Systems-Published Research, 2020.
Fehr, E. and Fischbacher, U. Why social preferences matter– the impact of non-selﬁsh motives on competition, cooperation and incentives. The economic journal, 112(478): C1–C33, 2002.

Balancing Rational and Other-Regarding Preferences in Cooperative-Competitive Environments

Algorithm 1 BAROCCO for Q-learning framework

Initialize Replay buffers D and DSW for selﬁsh and prosocial components

Networks

θi,

θ

S i

W

,

θSW

that

predict

action-values

Qi,

QSi W ,

QSW

Hypernetwork θH that predicts weights of mixing network θSW

Target networks θi

while True do

for transition t = 0 . . . T do

Sample weights in noisy layers, reduce exploration rate

for agent i = 0 . . . N do

With probability sample random action ait Otherwise, select ait = argmaxait Q⊕i oit , ait ; θi, θiSW
where Q⊕i oit , ait ; θi, θiSW = (1 − λ)Qi (oit , ait ; θi) + λQSi W end for

Apply agents’ actions, observe rewards and next state Store transitions (ot, at, rt) to D, (s, ot, at, rt) to DSW

end for

oit , ait ; θiSW

for agent i = 0 . . . N do

Sample mini-batch of transitions Bi from D to update selﬁsh action-value Qi

for transition t = 0 . . . Bi do
n
yit = γkrit+k + γnQi
k=0
end for

oit+n , argmaxait+n Q⊕i (oit+n , ait+n ; θi, θiSW ); θi

Update θi via gradient descent on temporal difference loss LT D (Qi(oiB , aiB ; θi), yiB )

end for

Periodically, copy weights of online networks θi to target networks θi

Sample mini-batch B from DSW to update prosocial action-values QSi W and QSW

for transition t = 0 . . . B do

Utilize global state via hypernetworks θSW = θH (st)

QSW

ot

,

a

t

;

θ

S i

W

,

θ

S

W

,

θ

H

= θSW

QS1 W (o1t , a1t ; θ1SW ), . . . , QSNW (oNt , aNt ; θNSW )

for agent i = 0 . . . N do

yit = rit + γQi oit+1 , argmaxait+1 Q⊕i (oit+1 , ait+1 ; θi, θiSW ); θi

end for

ytSW = SW (y1t , . . . , yNt )

end for

Update

θ

S i

W

,

θSW ,

θH

via

gradient

descent

on

LT D

QS

W

(oB

,

a

B

;

θ

S i

W

,

θ

S

W

,

θ

H

end while

, yBSW )

Balancing Rational and Other-Regarding Preferences in Cooperative-Competitive Environments

Algorithm 2 BAROCCO for Actor-Critic framework

Initialize Critic networks θi, θiSW that predict values Vi, QSi W Actor networks ψi that predict policies πi

while True do

for transition t = 0 . . . T do

Sample agents’ actions ait from respective policies πi(oit ; ψi)

Apply agents’ actions, observe rewards and next state

Store transition (s, ot, at, rt, st+1, ot+1) to batch B

end for

Set πoldi = πi

for mini-batch b ∈ B do

for agent i = 0 . . . N do

for transition t ∈ b do
n
yit = γkrit+k + γnVi
k=0

st+n | a−it+n ; θi

Ai (sit , ait | a−it ; θi) = yit − Vi (st | a−it ; θi)

ASi W sit , ait | a−it ; θiSW =

QSi W st, ait | a−it ; θiSW −

a

πi(ai

|

oit )QSi W (st,

ai

|

a−it ;

θ

S i

W

)

i

R = πi(ait |oit ;ψi)

it

πoldi (ait |oit )

end for

Update θi via gradient descent on temporal difference loss LT D (Vi(sb | a−ib ; θi), yib ) Update ψi on PPO loss Lπi A⊕i sib , aib | a−ib ; θi, θiSW with entropy regularization
where A⊕i sib , aib | a−ib ; θi, θiSW = (1 − λ)Ai (sib , aib | a−ib ; θi) + λASi W sib , aib | a−ib ; θiSW
end for

ybSW = SW (y1b , . . . , yNb ) for agent i = 0 . . . N do

Update θiSW via gradient descent on temporal difference loss: LT D QSi W (sib , aib | a−ib ; θiSW ), ybSW
end for

end for

end while

Fehr, E. and Schmidt, K. M. A theory of fairness, competition, and cooperation. The quarterly journal of economics, 114(3):817–868, 1999.
Foerster, J., Nardelli, N., Farquhar, G., Afouras, T., Torr, P. H., Kohli, P., and Whiteson, S. Stabilising experience replay for deep multi-agent reinforcement learning. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pp. 1146–1155. JMLR. org, 2017.
Foerster, J. N., Farquhar, G., Afouras, T., Nardelli, N., and Whiteson, S. Counterfactual multi-agent policy gradients. In Thirty-second AAAI conference on artiﬁcial intelligence, 2018.
Fortunato, M., Azar, M. G., Piot, B., Menick, J., Osband, I., Graves, A., Mnih, V., Munos, R., Hassabis, D., Pietquin, O., et al. Noisy networks for exploration. arXiv preprint arXiv:1706.10295, 2017.

Fu, Z., Zhao, Q., and Zhang, W. Reducing overestimation in value mixing for cooperative deep multi-agent reinforcement learning. 2019.
Guestrin, C., Koller, D., and Parr, R. Multiagent planning with factored mdps. In Advances in neural information processing systems, pp. 1523–1530, 2002.
Ha, D., Dai, A., and Le, Q. V. Hypernetworks. In 5th International Conference on Learning Representations, ICLR, 2017.
Henrich, J., Boyd, R., Bowles, S., Camerer, C., Fehr, E., Gintis, H., McElreath, R., et al. Cooperation, reciprocity and punishment in ﬁfteen small-scale societies. American Economic Review, 91(2):73–78, 2001.
Hernandez-Leal, P., Kaisers, M., Baarslag, T., and de Cote, E. M. A survey of learning in multiagent environments: Dealing with non-stationarity. arXiv preprint arXiv:1707.09183, 2017.

Balancing Rational and Other-Regarding Preferences in Cooperative-Competitive Environments

Hernandez-Leal, P., Kartal, B., and Taylor, M. E. A survey and critique of multiagent deep reinforcement learning. Autonomous Agents and Multi-Agent Systems, 33(6):750– 797, 2019.
Hessel, M., Modayil, J., Van Hasselt, H., Schaul, T., Ostrovski, G., Dabney, W., Horgan, D., Piot, B., Azar, M., and Silver, D. Rainbow: Combining improvements in deep reinforcement learning. In Thirty-Second AAAI Conference on Artiﬁcial Intelligence, 2018.
Horgan, D., Quan, J., Budden, D., Barth-Maron, G., Hessel, M., Van Hasselt, H., and Silver, D. Distributed prioritized experience replay. arXiv preprint arXiv:1803.00933, 2018.
Hostallero, D. E., Kim, D., Son, K., and Yi, Y. Inducing cooperation via learning to reshape rewards in semicooperative multi-agent reinforcement learning. 2018.
Hu, W., Xiao, L., and Pennington, J. Provable beneﬁt of orthogonal initialization in optimizing deep linear networks. arXiv preprint arXiv:2001.05992, 2020.
Hughes, E., Leibo, J. Z., Phillips, M., Tuyls, K., DueñezGuzman, E., Castañeda, A. G., Dunning, I., Zhu, T., McKee, K., Koster, R., et al. Inequity aversion improves cooperation in intertemporal social dilemmas. In Advances in neural information processing systems, pp. 3326–3336, 2018.
Jaques, N., Lazaridou, A., Hughes, E., Gulcehre, C., Ortega, P., Strouse, D., Leibo, J. Z., and De Freitas, N. Social inﬂuence as intrinsic motivation for multi-agent deep reinforcement learning. In International Conference on Machine Learning, pp. 3040–3049, 2019.
Johnson, D. D., Stopka, P., and Knights, S. The puzzle of human cooperation. Nature, 421(6926):911–912, 2003.
Kettner, S. E. and Waichman, I. Old age and prosocial behavior: Social preferences or experimental confounds? Journal of Economic Psychology, 53:118–130, 2016.
Konda, V. R. and Tsitsiklis, J. N. Actor-critic algorithms. In Advances in neural information processing systems, pp. 1008–1014, 2000.
Kostrikov, I., Agrawal, K. K., Dwibedi, D., Levine, S., and Tompson, J. Discriminator-actor-critic: Addressing sample inefﬁciency and reward bias in adversarial imitation learning. arXiv preprint arXiv:1809.02925, 2018.
Kraemer, L. and Banerjee, B. Multi-agent reinforcement learning as a rehearsal for decentralized planning. Neurocomputing, 190:82–94, 2016.
Laurent, G. J., Matignon, L., Fort-Piat, L., et al. The world of independent learners is not markovian. International Journal of Knowledge-based and Intelligent Engineering Systems, 15(1):55–64, 2011.

Leibo, J. Z., Zambaldi, V., Lanctot, M., Marecki, J., and Graepel, T. Multi-agent reinforcement learning in sequential social dilemmas. In Proceedings of the 16th Conference on Autonomous Agents and MultiAgent Systems, pp. 464–473. International Foundation for Autonomous Agents and Multiagent Systems, 2017.
Lerer, A. and Peysakhovich, A. Maintaining cooperation in complex social dilemmas using deep reinforcement learning. arXiv preprint arXiv:1707.01068, 2017.
Lin, L.-J. Self-improving reactive agents based on reinforcement learning, planning and teaching. Machine learning, 8(3-4):293–321, 1992.
Littman, M. L. Markov games as a framework for multiagent reinforcement learning. In Machine learning proceedings 1994, pp. 157–163. Elsevier, 1994.
Lowe, R., Wu, Y., Tamar, A., Harb, J., Abbeel, O. P., and Mordatch, I. Multi-agent actor-critic for mixed cooperative-competitive environments. In Advances in neural information processing systems, pp. 6379–6390, 2017.
Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., Graves, A., Riedmiller, M., Fidjeland, A. K., Ostrovski, G., et al. Human-level control through deep reinforcement learning. Nature, 518(7540): 529, 2015.
Mnih, V., Badia, A. P., Mirza, M., Graves, A., Lillicrap, T., Harley, T., Silver, D., and Kavukcuoglu, K. Asynchronous methods for deep reinforcement learning. In International conference on machine learning, pp. 1928– 1937, 2016.
Perolat, J., Leibo, J. Z., Zambaldi, V., Beattie, C., Tuyls, K., and Graepel, T. A multi-agent reinforcement learning model of common-pool resource appropriation. In Advances in Neural Information Processing Systems (NIPS), pp. 3643–3652, Long Beach, CA, 2017.
Peysakhovich, A. and Lerer, A. Consequentialist conditional cooperation in social dilemmas with imperfect information. In International Conference on Learning Representations, 2018a. URL https://openreview.net/ forum?id=BkabRiQpb.
Peysakhovich, A. and Lerer, A. Prosocial learning agents solve generalized stag hunts better than selﬁsh ones. In Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems, pp. 2043– 2044. International Foundation for Autonomous Agents and Multiagent Systems, 2018b.

Balancing Rational and Other-Regarding Preferences in Cooperative-Competitive Environments

Rand, D. G. and Nowak, M. A. Human cooperation. Trends in cognitive sciences, 17(8):413–425, 2013.
Rapoport, A., Chammah, A. M., and Orwant, C. J. Prisoner’s dilemma: A study in conﬂict and cooperation, volume 165. University of Michigan press, 1965.
Rashid, T., Samvelyan, M., De Witt, C. S., Farquhar, G., Foerster, J., and Whiteson, S. Qmix: monotonic value function factorisation for deep multi-agent reinforcement learning. arXiv preprint arXiv:1803.11485, 2018.
Rawls, J. A theory of justice. Harvard university press, 2009.
Roijers, D. M., Vamplew, P., Whiteson, S., and Dazeley, R. A survey of multi-objective sequential decision-making. Journal of Artiﬁcial Intelligence Research, 48:67–113, 2013.
Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
Scott, J. Rational choice theory. Understanding contemporary society: Theories of the present, 129:671–85, 2000.
Son, K., Kim, D., Kang, W. J., Hostallero, D. E., and Yi, Y. Qtran: Learning to factorize with transformation for cooperative multi-agent reinforcement learning. arXiv preprint arXiv:1905.05408, 2019.
Suarez, J., Du, Y., Isola, P., and Mordatch, I. Neural mmo: A massively multiagent game environment for training and evaluating intelligent agents. arXiv preprint arXiv:1903.00784, 2019.
Sunehag, P., Lever, G., Gruslys, A., Czarnecki, W. M., Zambaldi, V., Jaderberg, M., Lanctot, M., Sonnerat, N., Leibo, J. Z., Tuyls, K., et al. Value-decomposition networks for cooperative multi-agent learning. arXiv preprint arXiv:1706.05296, 2017.
Tampuu, A., Matiisen, T., Kodelja, D., Kuzovkin, I., Korjus, K., Aru, J., Aru, J., and Vicente, R. Multiagent cooperation and competition with deep reinforcement learning. PloS one, 12(4), 2017.

Tan, M. Multi-agent reinforcement learning: Independent vs. cooperative agents. In Proceedings of the tenth international conference on machine learning, pp. 330–337, 1993.
Vamplew, P., Yearwood, J., Dazeley, R., and Berry, A. On the limitations of scalarisation for multi-objective reinforcement learning of pareto fronts. In Australasian joint conference on artiﬁcial intelligence, pp. 372–378. Springer, 2008.
Van Hasselt, H., Guez, A., and Silver, D. Deep reinforcement learning with double q-learning. In Thirtieth AAAI conference on artiﬁcial intelligence, 2016.
Vinyals, O., Ewalds, T., Bartunov, S., Georgiev, P., Vezhnevets, A. S., Yeo, M., Makhzani, A., Küttler, H., Agapiou, J., Schrittwieser, J., et al. Starcraft ii: A new challenge for reinforcement learning. arXiv preprint arXiv:1708.04782, 2017.
Wang, J. X., Hughes, E., Fernando, C., Czarnecki, W. M., Duéñez-Guzmán, E. A., and Leibo, J. Z. Evolving intrinsic motivations for altruistic behavior. In Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems, pp. 683–692. International Foundation for Autonomous Agents and Multiagent Systems, 2019a.
Wang, W., Hao, J., Wang, Y., and Taylor, M. Achieving cooperation through deep multiagent reinforcement learning in sequential prisoner’s dilemmas. In Proceedings of the First International Conference on Distributed Artiﬁcial Intelligence, pp. 1–7, 2019b.
Watkins, C. J. and Dayan, P. Q-learning. Machine learning, 8(3-4):279–292, 1992.
Wolpert, D. H. and Tumer, K. Optimal payoff functions for members of collectives. In Modeling complexity in economic and social systems, pp. 355–369. World Scientiﬁc, 2002.
Wunder, M., Littman, M. L., and Babes, M. Classes of multiagent q-learning dynamics with epsilon-greedy exploration. In Proceedings of the 27th International Conference on Machine Learning (ICML-10), pp. 1167–1174. Citeseer, 2010.

