Instance-based Transfer Learning for Multilingual Deep Retrieval

Andrew O. Arnold∗
AWS AI
anarnld@amazon.com

William W. Cohen
Google Research
wcohen@google.com

arXiv:1911.06111v3 [cs.CL] 15 Apr 2021

ABSTRACT
We focus on the problem of search in the multilingual setting. Examining the problems of next-sentence prediction and inverse cloze, we show that at large scale, instance-based transfer learning is surprisingly effective in the multilingual setting, leading to positive transfer on all of the 35 target languages and two tasks tested. We analyze this improvement and argue that the most natural explanation, namely direct vocabulary overlap between languages, only partially explains the performance gains: in fact, we demonstrate target-language improvement can occur after adding data from an auxiliary language even with no vocabulary in common with the target. This surprising result is due to the effect of transitive vocabulary overlaps between pairs of auxiliary and target languages.
KEYWORDS
multilingual, neural networks, transfer learning, information retrieval
ACM Reference Format: Andrew O. Arnold and William W. Cohen. 2021. Instance-based Transfer Learning for Multilingual Deep Retrieval. In Proceedings of Workshop on Multilingual Search, The Web Conference (Workshop on Multilingual Search). ACM, New York, NY, USA, 10 pages. https: //doi.org/10.1145/nnnnnnn.nnnnnnn
1 INTRODUCTION
In this paper we explore the problem of search in the multilingual setting. Specifically, we analyze the behavior of instance-based transfer learning on two very large-scale deep
∗Work done while at Google Research.
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. Workshop on Multilingual Search, WWW, April 2021 © 2021 Association for Computing Machinery. ACM ISBN 978-x-xxxx-xxxx-x/YY/MM. . . $15.00 https://doi.org/10.1145/nnnnnnn.nnnnnnn

retrieval tasks (next-sentence prediction and inverse cloze) across dozens of auxiliary languages. Motivated by the increasing availability of unlabelled data of various sizes across languages, we introduce a simple technique for combining vocabularies across languages. We show that this method is surprisingly effective, leading to positive transfer on all of the 35 target languages tested across both tasks, and relative improvements of up to 200%.
Analysis of this result reveals a number of influences on transfer-learning performance. Unsurprisingly, performance is improved more when the task is difficult, when the available target language data is limited and when there is a large overlap between target and auxiliary language vocabularies. However, analysis suggests that these effects are only a partial explanation of the effectiveness of instance-based transfer. To support this argument, we demonstrate that multilingual instance-based transfer can lead to target-language improvement after adding data from an auxiliary language with no vocabulary in common with the target: this surprising result is due to the effect of transitive vocabulary overlaps between pairs of auxiliary and target languages.
We conclude by contextualizing these results in the broader literature of multilingual search and transfer learning, and arguing that the results are important due to their generalizability across models and architectures, limited only to the wide class of settings where raw, unaligned text across languages is available.
2 BACKGROUND AND RELATED WORK
In deep retrieval (or deep document linking), the goal is to learn to retrieve relevant documents from a large corpus of candidates, based on similarity to a query document. In multilingual transfer learning, we wish to improve performance on some task by using data from auxiliary languages to improve performance on a designated target language. Perhaps the simplest type of multilingual transfer learning is instance-based transfer learning, in which, for example, data from the target language and the auxiliary languages may be pooled, and a single model learned from the pooled data [18]. It is not immediately obvious when instance-based transfer learning will improve performance in this multilingual setting: for instance, a plausible conjecture is that pooling data in this way would only improve performance if the amount

Workshop on Multilingual Search, WWW, April 2021
of auxiliary data was carefully balanced with the amount of target data, or if the auxiliary languages were carefully selected to be highly similar to the target.
2.1 Next-Sentence Prediction and Inverse Cloze
We examine deep retrieval on the tasks of next sentence prediction (NSP) and inverse cloze (IC). In NSP the goal is to identify the next sentence in a document from among a corpus of hundreds of millions of candidate sentences across dozens of languages, given only the current sentence as context. NSP has many important applications to areas such as question answering [28], language model training [7], summarization [19] and conversational modeling [36]. IC is a slight generalization of NSP, where instead of predicting the single next sentence given a query sentence, the model must now return the entire context surrounding that sentence. Considering the query sentence as a question and its context as a passage containing a potential answer, this task has a direct relationship to open domain question answering [17, 34].
While NSP and IC have been well studied from the perspective of sequence modeling [8] and binary classification [7], we follow an alternative line of work that models the problems as instances of deep retrieval in extremely large output spaces [9, 20, 29]. Specifically, we generate a shared set of unigram and bigram features representing the current and next (surrounding) sentences in an NSP (IC) pair, respectively. We then train a feed-forward neural network that learns to maximize the dot-product between these consecutive sentences (contexts), as represented by the learned embedding vectors of each of the shared vocabulary’s ngram tokens. We follow the architecture1 of Bromley et al. [3], Henderson et al. [9], Logeswaran and Lee [20], Reddi et al. [29], using a siamese network to learn the multilingual vocabulary embeddings end-to-end, along with our NSP and IC objective functions, with all embeddings across languages mapped into a single shared space.
2.2 Multilingual Deep Retrieval
This architecture allows us to efficiently use an extremely large vocabulary of simple unigrams and bigrams that more computationally intensive techniques do not currently permit. Using such large vocabularies, these models are able to identify and exploit tiny cross-lingual dependencies found among the many tail n-grams observed in the large unsupervised monolingual datasets available in many languages across the internet. This follows other work that leverages
1Further reproducibility details such as specific hyperparameter, optimization and inference settings are unfortunately unavailable due to change of employers.

Arnold and Cohen
the concatenation of monolingual corpora in the multilingual setting [7, 12], and is in contrast to other work that tries to learn multilingual embeddings via attention [6, 16, 20, 26, 40, 41], shared cross-lingual embedding spaces [2, 4, 5, 13, 21], cross-lingual mapping functions [15, 42], other outside structure [22, 32, 38] and domain knowledge [14, 27].
For language modelling and question answering, the NSP and IC objectives are interesting because they lead to difficult classification tasks for which labeled data is plentiful. The motivation for using these tasks here is similar: under the deep retrieval formulation, NSP and IC are difficult, because they require comparison against all possible candidate sentences and contexts in the corpus, forcing the model to learn very nuanced distinctions, and potentially generating embeddings that generalize well to downstream tasks.
2.3 Instance-based Transfer Learning
NSP and IC are attractive problems for unsupervised and semi-supervised transfer learning in part because of the large amount of otherwise unlabelled, yet ordered, language data available on the internet. Even for such unlabelled data, there is still more data available in certain languages than in others, making this problem also an attractive setting for studying transfer learning between high and low resource languages [25, 30].
We focus on instance-based transfer learning where we use examples drawn from a related but distinct auxiliary dataset to improve performance on a target dataset [37]. However, rather than trying to align embeddings learned across languages [1, 23, 31], we instead attempt the easier task of first aligning vocabulary items across languages (by simply matching identical tokens), and then learning a single shared embedding for each vocabulary item.
Instance-based transfer vs. fine-tuning
Instance-based transfer is similar to another widely-used neural transfer method, namely fine-tuning. In fine-tuning-based transfer learning [10, 11, 33, 35], the weights of a network are trained on one (large) set of auxiliary data, and then copied into a new network where they are further adjusted using the target data. So in fine-tuning the model is optimized twice—once on the auxiliary data, and once on the target data—whereas in instance-based transfer it is optimized once, jointly, on both auxiliary and target datasets.
While fine-tuning allows additional flexibility, since one can independently decide optimization hyperparameters for each pass, this flexibility comes with some costs. In our setting, with 35 potential target languages (each with 34 potential auxiliary languages), instance-based learning produces a single model; while fine-tuning would produce 35, one specialized for each language, and in practical settings each

Instance-based Transfer Learning for Multilingual Deep Retrieval
of these 35 different models would need to be separately stored, maintained, etc. The sequential nature of fine-tuning also means that there are many choices to make when there are multiple auxiliary languages: for instance, in learning an NSP model for Ukrainian, perhaps it is better to train first on English (the most frequent language), then Russian (a more frequent related language), and finally fine-tune on Ukrainian. In order to fully utilize all potential auxiliary languages, one would have to consider fine-tuning the model up to 34 different times! In this broader setting, fully exploring the space of fine-tuned transfer models is a substantial undertaking.
In contrast, with the instance-based transfer approach we train a single joint model which shows improvements across all languages. If necessary, this efficiently computed multi-domain instance-based transfer model could be further refined by more expensive target-dependent fine-tuning; however, we leave exploration of such approaches for future work, focusing here on careful study of the more efficient instance-based method, at large scale, across two tasks and many target languages.
3 EXPERIMENTS
3.1 Next Sentence Prediction
We extract approximately 720 million next sentence pairs from publicly available Wikipedia2, restricting our experiments to the top 35 languages3 which account for 90% of the data. We split this dataset into train, development, and evaluation splits of 90%, 5% and 5% respectively. Figure 1 shows the relative size of each language in the dataset.
For each section in each article in each language we extract a pair of consecutive sentences if both sentences have at least four words. We then use a bag-of-n-grams representation for each sentence, constructing a training example as the unigram and bigram features from each ⟨𝑐𝑢𝑟𝑟𝑒𝑛𝑡, 𝑛𝑒𝑥𝑡⟩ sentence pair, and aggregate all unique unigrams and bigrams into a shared vocabulary. This results in per-language vocabularies ranging from 4 to 200 million n-gram tokens. For the special collection all, containing the union of all tokens across all languages, we limit the vocabulary size to the top 350 million n-gram tokens across all languages, sorted by frequency.
These 35 languages include some pairs that are quite similar and some pairs that are quite different. To give a rough measure of this, we looked at the similarity of the vocabularies. Figure 2 shows the Jaccard index among the vocabularies
2Downloaded May 11 and December 2, 2019. 3We use the same language code abbreviations as the Wikipedia subdomains associated with each language (e.g., en.wikipedia.com for English), and the special code all for the combined dataset comprised of data from all languages. Chinese is excluded due to the lack of a suitable tokenizer.

Workshop on Multilingual Search, WWW, April 2021
Figure 1: Train sentence pairs per language (log scale). of the languages within the corpus, a measure of vocabulary similarity defined for a pair of language vocabularies 𝑉𝑎 and 𝑉 as |𝑉𝑎∩𝑉𝑏 | . The matrix has been sorted to emphasize clus-
𝑏 |𝑉𝑎 ∪𝑉𝑏 |
ters roughly corresponding to known language groups (e.g., the Romance languages ro-gl-ca-pt-it-fr-es). Other interesting structure observed includes the large overlap between Serbian and Serbo-Croatian (sr-sh); and the cluster of Cebuano and Waray (two Austronesian languages spoken in the Philippines) with Vietnamese and Swedish (ceb-war-visv)4.
Figure 2: Jaccard index of vocabularies across languages, with blocks highlighted for Austronesian/Lsjbot (green), Serbian/Serbo-Croatian (purple), and Romance (red) languages.
3.2 Inverse Cloze
We use the same Wikipedia corpus for the inverse cloze task, this time randomly sampling a single sentence from each section in each article in each language as our 𝑐𝑢𝑟𝑟𝑒𝑛𝑡 sentence, and concatenating the surrounding four sentences
4Most of the Cebuano and Waray articles were written by a computer program, Lsjbot, which has also written articles for Swedish Wikipedia, accounting for the large unexpected overlap in vocabularies.

Workshop on Multilingual Search, WWW, April 2021

Arnold and Cohen

Figure 3: Relative transfer improvement on each target language for NSP (left) and IC (right) tasks. Languages are sorted by number of training sentences, as in Figure 1.

(two preceding and two following) as our 𝑐𝑜𝑛𝑡𝑒𝑥𝑡, resulting in approximately 60 million ⟨𝑐𝑢𝑟𝑟𝑒𝑛𝑡, 𝑐𝑜𝑛𝑡𝑒𝑥𝑡⟩ pairs. Feature construction, model architecture, training and evaluation are the same as in NSP.
3.3 Method
Using the architecture described in Section 2.1, we train two different types of models: a per-language model trained on a corpus containing only sentence pairs obtained from a particular language’s Wikipedia subdomain; and a combined model trained on a corpus containing the aggregated training examples across all languages. We train one perlanguage model for each language, along with a single combined model, resulting in 36 models total. We evaluate each per-language model’s performance on that language’s heldout evaluation data, and we evaluate the single combined model’s performance likewise on each individual language’s evaluation data, resulting in 35 pairs of evaluation data. We evaluate the performance of these models using a sampled recall@k metric defined as the percent of query sentences for which the correct next sentence (context) is retrieved within the top-k results5. We define the relative transfer improvement as:

4 RESULTS
Figure 3 shows the relative transfer improvement6 for each language, for each task, on the recall@1 metric. Perhaps surprisingly, we observe that all languages, across both tasks, benefit from the addition of data from other languages (i.e., there is no observed negative transfer, in contrast with the observations of Arivazhagan et al. [2]), with most languages being improved by over 25%. This lack of negative transfer could be due to the manner in which transfer is being achieved, with only examples from the target domain that reinforce the source domain being picked-up by the model (see §§4.4 and 4.5).
We also see that the average relative benefit of transfer on the IC task (96%) is over three times greater than on the NSP task (31%). Despite this difference in the magnitude of the effect, the Pearson correlation coefficient of the relative effect for each language across tasks is quite high at .77. This suggests there are per-language effects, persisting across tasks, that at least somewhat influence the effectiveness of transfer. We investigate potential explanations for these effects below.

𝑟𝑒𝑐𝑎𝑙𝑙 @𝑘𝑐𝑜𝑚𝑏𝑖𝑛𝑒𝑑 − 𝑟𝑒𝑐𝑎𝑙𝑙 @𝑘𝑝𝑒𝑟-𝑙𝑎𝑛𝑔𝑢𝑎𝑔𝑒 𝑟𝑒𝑐𝑎𝑙𝑙 @𝑘𝑝𝑒𝑟 -𝑙𝑎𝑛𝑔𝑢𝑎𝑔𝑒
5To determine the top-k results we combine the true next sentence (context), drawn from the evaluation data, with a sample of 100,000 false next sentence (context) distractors, drawn from the training data. All 100,001 sentences (contexts) are embedded in the same space and their similarity computed relative to the test query, with the top-k most similar sentences (contexts) retrieved.

Figure 4: Performance on Ukrainian data as a function of the mixture ratio between target data (Ukrainian) and auxiliary data (all excluding Ukrainian) on NSP. Dashed line shows baseline performance using native Ukrainian mixture ratio of 2%.
6Baseline performance is shown in the x-axes of Figure 5.

Instance-based Transfer Learning for Multilingual Deep Retrieval

Workshop on Multilingual Search, WWW, April 2021

Figure 5: Relative transfer improvement for NSP (left) and IC (right) as a function of task difficulty (as measured by pre-transfer recall@1), along with linear fits ± one standard deviation (𝑅2 = 53% and 70%, respectively). Green,
purple and red points correspond to clusters in Figure 2, violet is Esperanto. The outlier in the IC plot (Serbo-
Croatian (sh), marked in black) has been removed from the linear fit to better show the trend.

4.1 Performance Improves for Many Mixture Ratios
These results are surprisingly strong for such a simple method. It is natural to ask if instance-based transfer can be improved, for instance by weighting the auxiliary data differently. In the experiments above, we built a combined model with relative language proportions unchanged from their original ratio in the raw Wikipedia data. Reasoning that some amount of transfer from the auxiliary data might be beneficial, but too much transfer might overwhelm the target language, Figure 4 shows the results for a particular language (Ukrainian) of adjusting this mixture ratio between the target language (Ukrainian) and auxiliary data (all languages except Ukrainian) on the NSP task. We observe that performance increases as the amount of target data used increases up to a point, and then starts to diminish, with an optimum mixture ratio of 10 - 20%. In all of these experiments, the total amount of data seen by the model is unchanged, only the ratio of the sources varies. In summary, gains are observed for a wide range of mixing ratios, but the chosen ratio is not optimal: there is a potential relative improvement of 17% over the native Ukrainian mixture ratio of 2% found in the original corpus, suggesting further improvement to the overall performance of the combined model could be achieved by optimizing all the mixture ratios across different languages. Even though there are clearly additional improvements to be obtained by tuning instance-based transfer, a more fundamental question to ask is why the method works so well. The following sections explore this question further, by investigating potential explanations for when and why this instance-based transfer improvement occurs.

4.2 Sample Size: Low-resource Languages Improve More
Figure 6: Relative transfer improvement for NSP ( , blue fit line) and IC (+, orange fit line) as a function of training data sample size, along with linear fit ± one standard deviation (𝑅2 = 14% and 18%, respectively). Green, purple and red points correspond to clusters in Figure 2, violet is Esperanto.
Figure 6 shows the relative transfer improvement on both tasks for each language on the recall@1 metric, as a function of that language’s share of the training data. Unsurprisingly, we observe that the languages with the smallest share of training data receive the largest improvement from this transfer. Interestingly, the language with the largest relative NSP improvement is Esperanto, a constructed language with vocabulary drawn from Romance and Germanic languages.

Workshop on Multilingual Search, WWW, April 2021
4.3 Task Difficulty: Harder Tasks Improve More
Figure 5 shows the relative transfer improvement for each language on both tasks, as a function of each language’s baseline, pre-transfer, performance on each task. In other words, for each language, we measure the performance of a model trained only on that language, and use that as a proxy for the difficulty of the task: the lower this baseline score, the harder the underlying task. Across both tasks we see a very strong positive relationship between pre-transfer task difficulty (measured as pre-transfer recall@1) and posttransfer relative improvement: the more difficult the initial task is, the more room there seems to be for improvement from transfer.
Combined with the results from § 4.2 relating transfer improvement and sample size, we see it is not just the lack of available data or the difficulty of the underlying task that makes transfer effective, but it is actually the combination of the two. Transfer seems to help when performance at a task can be improved by adding auxiliary data. When baseline performance is low, there is more room for improvement. Likewise, when baseline data is small, there is more room for adding information from other related sources. If the performance is already high, or if the target data is already sufficient to saturate the model, there is less opportunity for transfer to make a difference.
4.4 Direct Vocabulary Overlap

Arnold and Cohen

when there is larger overlap between language vocabularies. Using English as a reference language, Figure 7 shows the absolute transfer improvement in recall@1 for each language on both tasks as a function of that language’s vocabulary’s
|𝑉𝑡𝑎𝑟𝑔𝑒𝑡 ∩𝑉𝐸𝑛𝑔𝑙𝑖𝑠ℎ |
overlap with the English vocabulary |𝑉 | . The 𝑡 𝑎𝑟 𝑔𝑒 𝑡
larger the overlap, the larger the benefit from transfer. This makes sense in light of the model architecture, as it is the presence of overlapping tokens from different languages in the same training sentence pairs that allows information learned on one language’s data to impact another language. Combined with the significant joint overlap among language vocabularies shown in Figure 2, this helps demonstrate why transfer can occur even between seemingly unrelated languages.
Table 1 summarizes the explanatory effects of sample size, task difficulty and vocabulary overlap on the effectiveness of transfer across the NSP and IC tasks. These factors combined explain 62% and 78% (respectively) of the variance of transfer improvements across both tasks (𝑅2). Even accounting for these observed effects, however, there is still a significant amount of unexplained variance in the observed transfer improvement across languages, suggesting some other effect may be at work.

NSP

sample size

14%

task difficulty

53%

vocabulary overlap 23%

combined

62%

IC Average 18% 16% 70%7 62% 4% 14%
78% 70%

Table 1: Percent of observed relative transfer improvement variance explained by various factors (𝑅2) across
NSP and IC tasks.

Figure 7: Absolute transfer improvement for NSP ( , blue fit line) and IC (+, orange fit line) as a function of vocabulary overlap with en, along with linear fit ± one standard deviation (𝑅2 = 23% and 4%, respectively). Green, purple and red points correspond to clusters in Figure 2.
In addition to the observed relationships with inverse sample size and task difficulty, transfer also seems to improve

4.5 Transitive Vocabulary Overlap
We design an experiment to test this hypothesis and determine whether transfer requires direct overlap between the vocabularies of two languages, or whether transfer can still happen even if the vocabularies of the two languages are disjoint. For this experiment we focus on NSP and pick two languages with very little natural vocabulary overlap: Telugu (te) and Turkish (tr). We train one model, te+en, on sentence pairs drawn from the two languages Telugu (te) and English (en), in the same proportion as they occur naturally in the training data, and evaluate this model’s performance on held out Telugu data. We then similarly sample sentence pairs from an auxiliary language, Turkish (tr), whose vocabulary has been censored to contain no overlap with the Telugu vocabulary. Thus, there is no direct route by which the Turkish
7Serbo-Croatian removed.

Instance-based Transfer Learning for Multilingual Deep Retrieval

Workshop on Multilingual Search, WWW, April 2021

3.2%

te

10.1%

en

15.7%

tr

1.7%

te+en +tr % Change Recall@1 17.0 17.6 +3.5% Recall@10 23.9 25.0 +4.6% Recall@20 26.3 27.7 +5.3%

Figure 8: Overlap among the target, auxiliary and pivot language vocabularies. Target ⇌ auxiliary over-
lap numbers (dotted arrows) are measured before censoring the vocabularies to remove any overlap. Directed edges indicate direction of asymmetric percent
|𝑉𝑠𝑜𝑢𝑟𝑐𝑒 ∩𝑉𝑡𝑎𝑟𝑔𝑒𝑡 |
overlap |𝑉 | . 𝑠𝑜𝑢𝑟𝑐𝑒
data can influence the embedding of the Telugu vocabulary (see Figure 8 for details of all vocabulary overlaps). And yet, as shown in Table 2, the addition of the Turkish data does improve the (te+en+tr) model’s performance on the Telugu data, despite there being no direct connection between the two languages.
While, by construction, there is no direct overlap between the target (te) and auxiliary (tr) language vocabularies, there is indirect overlap via the pivot language, English. Thus, in a graphical sense, the influence of the auxiliary language is able to pass transitively through the chain of overlapping vocabularies of the pivot language to ultimately influence and improve the target language’s performance. If this link via the pivot language is removed, the performance of the target language is unchanged from its baseline, even in the presence of the auxiliary language.
Figure 9 shows a picture of how this indirect transfer might work, for a simplified setting where we must learn synonymy. In this example, the auxiliary language is needed to discover the relationship between (𝑢1, 𝑡1), while the pivot language is needed to map (𝑢2, 𝑢3) and (𝑡2, 𝑡3). Without both of these pieces of information, there would be no way to learn the implied edge between (𝑢4, 𝑡4) in the target language.
Generalizing from this example, we can see that the more auxiliary languages we have, the more potential information we have to learn about correlations among tokens and contexts. Likewise, the more pivot languages we have, the more strongly connected our token graph is and the more paths we have for spreading the information learned from the auxiliary languages to the target language. Of course, in the more realistic setting without artificial vocabulary censoring, languages can serve as both pivots and auxiliaries simultaneously, suggesting that the large scale multi-language transfer facilitated by our instance-based approach might

Table 2: Transfer improvement of Telugu (te) in the presence of Turkish (tr) data, even with no direct vocabulary overlap. English (en) serve as a pivot language.

𝐿𝑎𝑢𝑥𝑖𝑙𝑖𝑎𝑟 𝑦

𝐿𝑝𝑖 𝑣𝑜 𝑡

𝐿𝑡𝑎𝑟𝑔𝑒𝑡

𝑢1

𝑢2

𝑢3

𝑢4

𝑡1

𝑡2

𝑡3

𝑡4

Figure 9: Language 𝐿𝑎𝑢𝑥𝑖𝑙𝑖𝑎𝑟 𝑦 has tokens 𝑢1, 𝑢2, 𝑡1, 𝑡2 and evidence shows that (𝑢1, 𝑡1), (𝑢1, 𝑢2) and (𝑡1, 𝑡2) are synonyms (solid edges). 𝐿𝑝𝑖𝑣𝑜𝑡 has tokens 𝑢2, 𝑢3, 𝑡2, 𝑡3 and evidence that (𝑢2, 𝑢3) and (𝑡2, 𝑡3) are synonyms. Finally 𝐿𝑡𝑎𝑟𝑔𝑒𝑡 has tokens 𝑢3, 𝑢4, 𝑡3, 𝑡4 and evidence that (𝑢3, 𝑢4) and (𝑡3, 𝑡4) are synonyms. Here evidence from 𝐿𝑎𝑢𝑥𝑖𝑙𝑖𝑎𝑟 𝑦 influences 𝐿𝑡𝑎𝑟𝑔𝑒𝑡 , via 𝐿𝑝𝑖𝑣𝑜𝑡 , indicating that there is a relationship between (𝑢4, 𝑡4) (dashed edge), even though there is no direct lexical overlap among 𝐿𝑎𝑢𝑥𝑖𝑙𝑖𝑎𝑟 𝑦 and 𝐿𝑡𝑎𝑟𝑔𝑒𝑡 .
be a critical component towards maximizing the amount of transfer enabled by these overlapping token networks. In addition to these network effects, the added languages can also serve as regularizers, robustifying the models and leading to improved performance.
This effect seems to be enabled and amplified by the large sample size and diverse vocabulary of the pivot language, even when the amount of auxiliary data is relatively small, and is most easily seen in code-mixing around proper nouns and names (see Table 3 for an example involving Russian (ru), English (en) and Ukrainian (uk)).

Workshop on Multilingual Search, WWW, April 2021

Arnold and Cohen

6.1%

ru

9.4%

10.8%

ca

uk

22.7%

6.8%

en

2.6%

ca+en+ru +uk % Change

Recall@1

10.6 11.3 +6.6%

Recall@10 20.1 21.5 +7.0%

Recall@20 24.3 25.6 +5.3%

Figure 10: Overlap among the target, auxiliary and pivot language vocabularies. Target ⇌ auxiliary overlap numbers (dotted arrows) are measured before censoring the vocabularies to remove any overlap. Directed edges indicate direction of asymmetric percent
|𝑉𝑠𝑜𝑢𝑟𝑐𝑒 ∩𝑉𝑡𝑎𝑟𝑔𝑒𝑡 |
overlap |𝑉 | . 𝑠𝑜𝑢𝑟𝑐𝑒
To demonstrate that these results generalize across other language pairs, we show a similar effect with two other, similarly disjoint languages: Catalan (ca) and Ukrainian (uk). This time we use two pivot languages: English (en) and Russian (ru). We follow the same procedure as before, censoring the vocabularies of the four languages to ensure there is no overlap. Figure 10 shows the natural vocabulary overlaps between these four languages, while Table 4 summarizes the results of the experiment. Once again we see that the addition of a seemingly unrelated language (Ukrainian) improves the performance on another language (Catalan), despite the fact that there is no direct vocabulary overlap among the languages.
. . . английского названия Киева: в результате появилось слово Kyiv. . . ими реже, чем Kiev.8
Table 3: Example from Russian Wikipedia demonstrating code-mixing between Russian and English names for the city Kiev (underlined). Translation: ...English name of Kiev: as a result, the word Kyiv appeared...less often than Kiev.
5 CONCLUSION
We have shown that cross-language instance-based transfer learning can significantly improve performance across all 35 languages tested on both the next sentence prediction and inverse cloze tasks, when formulated as multilingual deep

Table 4: Transfer improvement of Catalan (ca) in the presence of Ukrainian (uk) data, even with no direct vocabulary overlap. English (en) and Russian (ru) serve as pivot languages.
retrieval problems. We have identified sample size, task difficulty and vocabulary overlap as three factors that contribute to this technique’s success, and demonstrated that transfer is possible even when there is only indirect transitive vocabulary overlap. We have also shown that varying the mixture ratio between target and auxiliary data can further improve transfer performance.
These very large-scale experiments with transfer between multiple languages have uncovered some regularities not easily seen in smaller-scale experiments involving only two or three languages, or only one task. This has been directly enabled by the highly scalable nature of the instance-based transfer method we describe, allowing more observations and the identification of subtler trends and relationships.
Looking forward, the results on direct vocabulary transfer show that transfer might be further improved by increasing the vocabulary overlap among languages, using features such as byte, character, subword and phoneme n-grams [24, 39, 43]. More surprisingly, the results on indirect vocabulary transfer suggest that researchers currently doing similar transfer experiments on a limited set of languages might see an immediate benefit from including a larger set of languages, even if they are seemingly unrelated, due to the transitive transfer effect. These direct and transitive vocabulary overlap effects seem to be distinct from more indirect transfer effects identified in related work [13, 22, 41]. It is an open question if these kinds of transitive effects would also occur under the fine-tuning transfer approach. Finally, further work is needed to investigate the impact that the improved transfer might have on downstream tasks.

Instance-based Transfer Learning for Multilingual Deep Retrieval
REFERENCES
[1] David Alvarez-Melis and Tommi Jaakkola. 2018. Gromov-Wasserstein Alignment of Word Embedding Spaces. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Brussels, Belgium, 1881–1890. https://www.aclweb.org/anthology/D18-1214
[2] Naveen Arivazhagan, Ankur Bapna, Orhan Firat, Dmitry Lepikhin, Melvin Johnson, Maxim Krikun, Mia Xu Chen, Yuan Cao, George Foster, Colin Cherry, Wolfgang Macherey, Zhifeng Chen, and Yonghui Wu. 2019. Massively Multilingual Neural Machine Translation in the Wild: Findings and Challenges. arXiv:1907.05019 [cs.CL]
[3] Jane Bromley, Isabelle Guyon, Yann LeCun, Eduard Säckinger, and Roopak Shah. 1993. Signature Verification Using a "Siamese" Time Delay Neural Network. In Proceedings of the 6th International Conference on Neural Information Processing Systems (Denver, Colorado) (NIPS’93). Morgan Kaufmann Publishers Inc., San Francisco, CA, USA, 737–744.
[4] Steven Cao, Nikita Kitaev, and Dan Klein. 2020. Multilingual Alignment of Contextual Word Representations. In International Conference on Learning Representations. https://openreview.net/forum? id=r1xCMyBtPS
[5] Xilun Chen and Claire Cardie. 2018. Unsupervised Multilingual Word Embeddings. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Brussels, Belgium, 261–270. https://www.aclweb. org/anthology/D18-1024
[6] Diego de Vargas Feijo and Viviane Pereira Moreira. 2020. Mono vs Multilingual Transformer-based Models: a Comparison across Several Language Tasks. arXiv:2007.09757 [cs.CL]
[7] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). Association for Computational Linguistics, Minneapolis, Minnesota, 4171–4186. https://doi.org/10.18653/v1/N19-1423
[8] Shalini Ghosh, Oriol Vinyals, Brian Strope, Scott Roy, Tom Dean, and Larry Heck. 2016. Contextual lstm (clstm) models for large scale nlp tasks. arXiv preprint arXiv:1602.06291 (2016).
[9] Matthew Henderson, Rami Al-Rfou, Brian Strope, Yun hsuan Sung, László Lukács, Ruiqi Guo, Sanjiv Kumar, Balint Miklos, and Ray Kurzweil. 2017. Efficient Natural Language Response Suggestion for Smart Reply. ArXiv e-prints (2017).
[10] Jeremy Howard and Sebastian Ruder. 2018. Universal Language Model Fine-tuning for Text Classification. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Computational Linguistics, Melbourne, Australia, 328–339. https://doi.org/10.18653/v1/P18-1031
[11] J. Huang, J. Li, D. Yu, L. Deng, and Y. Gong. 2013. Cross-language knowledge transfer using multilingual deep neural network with shared hidden layers. In 2013 IEEE International Conference on Acoustics, Speech and Signal Processing. 7304–7308. https://doi.org/10.1109/ ICASSP.2013.6639081
[12] Melvin Johnson, Mike Schuster, Quoc V. Le, Maxim Krikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat, Fernanda Viégas, Martin Wattenberg, Greg Corrado, Macduff Hughes, and Jeffrey Dean. 2017. Google’s Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation. Transactions of the Association for Computational Linguistics 5 (2017), 339–351. https://doi.org/10.1162/tacl_a_ 00065
[13] Kaliyaperumal Karthikeyan, Zihan Wang, Stephen Mayhew, and Dan Roth. 2019. Cross-lingual ability of multilingual bert: An empirical

Workshop on Multilingual Search, WWW, April 2021
study. In International Conference on Learning Representations. [14] Philipp Koehn and Kevin Knight. 2002. Learning a Translation Lexicon
from Monolingual Corpora. In Proceedings of the ACL-02 Workshop on Unsupervised Lexical Acquisition. Association for Computational Linguistics, Philadelphia, Pennsylvania, USA, 9–16. https://doi. org/10.3115/1118627.1118629 [15] Saurabh Kulshreshtha, José Luis Redondo-García, and Ching-Yun Chang. 2020. Cross-lingual Alignment Methods for Multilingual BERT: A Comparative Study. arXiv:2009.14304 [cs.CL] [16] Anne Lauscher, Vinit Ravishankar, Ivan Vulic, and Goran Glavas. 2020. From Zero to Hero: On the Limitations of Zero-Shot Cross-Lingual Transfer with Multilingual Transformers. CoRR abs/2005.00633 (2020). arXiv:2005.00633 https://arxiv.org/abs/2005.00633 [17] Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. 2019. Latent Retrieval for Weakly Supervised Open Domain Question Answering. CoRR abs/1906.00300 (2019). arXiv:1906.00300 http://arxiv.org/ abs/1906.00300 [18] Di Lin, Xing An, and Jian Zhang. 2013. Double-bootstrapping source data selection for instance-based transfer learning. Pattern Recognition Letters 34, 11 (2013), 1279–1285. [19] Jingyun Liu, Jackie CK Cheung, and Annie Louis. 2019. What comes next? Extractive summarization by next-sentence prediction. arXiv preprint arXiv:1901.03859 (2019). [20] Lajanugen Logeswaran and Honglak Lee. 2018. An efficient framework for learning sentence representations. In International Conference on Learning Representations. https://openreview.net/forum? id=rJvJXZb0W [21] Artetxe Mikel, Ruder Sebastian, and Yogatama Dani. 2020. On the Cross-lingual Transferability of Monolingual Representations. ACL (2020), 4623–4637. [22] Aaron Mueller, Garrett Nicolai, Arya D. McCarthy, Dylan Lewis, Winston Wu, and David Yarowsky. 2020. An Analysis of Massively Multilingual Neural Machine Translation for Low-Resource Languages. In Proceedings of the 12th Language Resources and Evaluation Conference. European Language Resources Association, Marseille, France, 3710–3718. https://www.aclweb.org/anthology/2020.lrec-1.458 [23] Ndapa Nakashole. 2018. NORMA: Neighborhood Sensitive Maps for Multilingual Word Embeddings. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Brussels, Belgium, 512–522. https: //www.aclweb.org/anthology/D18-1047 [24] Toan Q. Nguyen and David Chiang. 2017. Transfer Learning across Low-Resource, Related Languages for Neural Machine Translation. In Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers). Asian Federation of Natural Language Processing, Taipei, Taiwan, 296–301. https:// www.aclweb.org/anthology/I17-2050 [25] S. J. Pan and Q. Yang. 2010. A Survey on Transfer Learning. IEEE Transactions on Knowledge and Data Engineering 22, 10 (Oct 2010), 1345–1359. https://doi.org/10.1109/TKDE.2009.191 [26] Telmo Pires, Eva Schlinger, and Dan Garrette. 2019. How Multilingual is Multilingual BERT?. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics, Florence, Italy, 4996–5001. https: //doi.org/10.18653/v1/P19-1493 [27] Barbara Plank and Željko Agić. 2018. Distant Supervision from Disparate Sources for Low-Resource Part-of-Speech Tagging. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Brussels, Belgium, 614–620. https://www.aclweb.org/anthology/D181061

Workshop on Multilingual Search, WWW, April 2021
[28] Pranav Rajpurkar, Robin Jia, and Percy Liang. 2018. Know What You Don’t Know: Unanswerable Questions for SQuAD. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers). Association for Computational Linguistics, Melbourne, Australia, 784–789. https://www.aclweb. org/anthology/P18-2124
[29] Sashank J. Reddi, Satyen Kale, Felix Yu, Daniel Holtmann-Rice, Jiecao Chen, and Sanjiv Kumar. 2019. Stochastic Negative Mining for Learning with Large Output Spaces. In Proceedings of Machine Learning Research (Proceedings of Machine Learning Research, Vol. 89), Kamalika Chaudhuri and Masashi Sugiyama (Eds.). PMLR, 1940–1949. http://proceedings.mlr.press/v89/reddi19a.html
[30] Daniel L. Silver and Robert E. Mercer. 1998. The Parallel Transfer of Task Knowledge Using Dynamic Learning Rates Based on a Measure of Relatedness. Springer US, Boston, MA, 213–233. https://doi. org/10.1007/978-1-4615-5529-2_9
[31] Anders Søgaard, Sebastian Ruder, and Ivan Vulić. 2018. On the Limitations of Unsupervised Bilingual Dictionary Induction. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Computational Linguistics, Melbourne, Australia, 778–788. https://www.aclweb. org/anthology/P18-1072
[32] Dennis Spohr, Laura Hollink, and Philipp Cimiano. 2011. A Machine Learning Approach to Multilingual and Cross-Lingual Ontology Matching. In The Semantic Web – ISWC 2011, Lora Aroyo, Chris Welty, Harith Alani, Jamie Taylor, Abraham Bernstein, Lalana Kagal, Natasha Noy, and Eva Blomqvist (Eds.). Springer Berlin Heidelberg, Berlin, Heidelberg, 665–680.
[33] P. Swietojanski, A. Ghoshal, and S. Renals. 2012. Unsupervised crosslingual knowledge transfer in DNN-based LVCSR. In 2012 IEEE Spoken Language Technology Workshop (SLT). 246–251. https://doi.org/ 10.1109/SLT.2012.6424230
[34] Wilson L. Taylor. 1953. “Cloze Procedure”: A New Tool for Measuring Readability. Journalism Quarterly 30, 4 (1953), 415– 433. https://doi.org/10.1177/107769905303000401 arXiv:https://doi.org/10.1177/107769905303000401
[35] S. Thomas, S. Ganapathy, and H. Hermansky. 2012. Multilingual MLP features for low-resource LVCSR systems. In 2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). 4269– 4272. https://doi.org/10.1109/ICASSP.2012.6288862
[36] Oriol Vinyals and Quoc Le. 2015. A neural conversational model. arXiv preprint arXiv:1506.05869 (2015).
[37] Tianyang Wang, Jun Huan, and Michelle Zhu. 2018. Instance-Based Deep Transfer Learning. 2019 IEEE Winter Conference on Applications of Computer Vision (WACV) (2018), 367–375.
[38] Zhigang Wang, Zhixing Li, Juanzi Li, Jie Tang, and Jeff Z. Pan. 2013. Transfer Learning Based Cross-lingual Knowledge Extraction for Wikipedia. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Computational Linguistics, Sofia, Bulgaria, 641–650. https://www.aclweb.org/anthology/P13-1063
[39] Theresa Wilson and Stephan Raaijmakers. 2008. Comparing word, character, and phoneme n-grams for subjective utterance recognition. In Ninth Annual Conference of the International Speech Communication Association.
[40] Shijie Wu and Mark Dredze. 2019. Beto, Bentz, Becas: The Surprising Cross-Lingual Effectiveness of BERT. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 833–844.

Arnold and Cohen
[41] Shijie Wu and Mark Dredze. 2020. Are All Languages Created Equal in Multilingual BERT?. In Proceedings of the 5th Workshop on Representation Learning for NLP. Association for Computational Linguistics, Online, 120–130. https://doi.org/10.18653/v1/2020. repl4nlp-1.16
[42] Ruochen Xu, Yiming Yang, Naoki Otani, and Yuexin Wu. 2018. Unsupervised Cross-lingual Transfer of Word Embedding Spaces. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Brussels, Belgium, 2465–2474. https://www.aclweb.org/anthology/D181268
[43] Jinman Zhao, Sidharth Mudgal, and Yingyu Liang. 2018. Generalizing Word Embeddings using Bag of Subwords. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Brussels, Belgium, 601–606. https://www.aclweb.org/anthology/D18-1059

