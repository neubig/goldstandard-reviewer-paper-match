JTUBESPEECH: CORPUS OF JAPANESE SPEECH COLLECTED FROM YOUTUBE FOR SPEECH RECOGNITION AND SPEAKER VERIFICATION
Shinnosuke Takamichi1, Ludwig Ku¨rzinger2, Takaaki Saeki1, Sayaka Shiota3, Shinji Watanabe4
1The University of Tokyo, Japan, 2Technical University of Munich, Germany, 3Tokyo Metropolitan University, Japan, 4Carnegie Mellon University, USA

arXiv:2112.09323v1 [cs.SD] 17 Dec 2021

ABSTRACT In this paper, we construct a new Japanese speech corpus called “JTubeSpeech.” Although recent end-to-end learning requires largesize speech corpora, open-sourced such corpora for languages other than English have not yet been established. In this paper, we describe the construction of a corpus from YouTube videos and subtitles for speech recognition and speaker veriﬁcation. Our method can automatically ﬁlter the videos and subtitles with almost no languagedependent processes. We consistently employ Connectionist Temporal Classiﬁcation (CTC)-based techniques for automatic speech recognition (ASR) and a speaker variation-based method for automatic speaker veriﬁcation (ASV). We build 1) a large-scale Japanese ASR benchmark with more than 1,300 hours of data and 2) 900 hours of data for Japanese ASV.
Index Terms— automatic speech recognition, automatic speaker veriﬁcation, speech corpus, YouTube
1. INTRODUCTION
Powered by the development of deep learning, signiﬁcant progress has been made on various speech recognition tasks, e.g., automatic speech recognition (ASR) [3]–[5] and automatic speaker veriﬁcation (ASV) [6], [7]. Due to data hungriness of deep learning, massivesize speech corpora have been constructed and published. It is desirable to build and publish speech corpora of all languages for decentralizing the speech technologies. However, corpora in languages other than English are very poor. For example, while several thousands of hours of corpora have been published in English and Chinese [8]–[14], similar-size corpora are very limited for other languages.
Japanese, the target language in this paper, is also this example. The CSJ corpus [15] is the most frequently used corpus for Japanese ASR, but its size is relatively small (600 hours) compared to modern English corpora such as Common Voice [10] (1, 100 hours). Also, the modern size of ASV corpora is more than 1, 000 hours [14], [16], but there is no open-sourced corpus for Japanese ASV.
To construct a large-scale corpus, several studies have collected text-audio pairs from videos [11], [16]–[18]. For example, YouTube provides videos in diverse genres, recording environments, speakers, and language accents. There is no doubt that such in-the-wild data
We would like to thank Hiromasa Fujihara and the GigaSpeech team, especially Guoguo Chen and Shuzhou Chai for their valuable comments. This work used the HLTCOE cluster at Johns Hopkins University and the Extreme Science and Engineering Discovery Environment (XSEDE) [1], which is supported by National Science Foundation grant number ACI-1548562. Speciﬁcally, it used the Bridges system [2], which is supported by NSF award number ACI-1445606, at the Pittsburgh Supercomputing Center (PSC). This work is also supported by JSPS KAKENHI 19K20271, 21H04900, 21H05054, JST Moonshot R&D Grant Number JPMJPS2011, ROISDS-JOINT (030RP2021) to S. Shiota and the SECOM Science and Technology Foundation.

Target lang. (e.g., “Ja”)

Query to download

Query to search

Data collection Query to download

Cleansing for ASR

Wikipedia &

YouTube

Google Trends search engine

YouTube

CTC-based cleansing

words
List of search words

Video IDs
List of video IDs with subtitles

Text & audio
Text-audio pair

Variationbased
cleansing
Cleansing for ASV

Fig. 1. Procedure of corpus construction.

ASR corpus
ASV corpus

is useful for wide purpose of modern speech technology. Chen [19] proposed a strategy to collect English videos for ASR use. Also, Fan [14] manually selected Chinese celebrities and extracted videos for ASV use. Unlike these methods that require language-dependent and manual processes, this paper aims to develop a corpus with almost no language-dependent and no manual processes. The establishment of this method will be useful for building corpora of many languages, not limited to English and Chinese.
In this paper, we propose a mostly language-independent strategy to construct a speech corpus for both ASR and ASV, and then construct a Japanese speech corpus “JTubeSpeech” by using the proposed method. We ﬁrst crawl YouTube in order to generate candidate audio-text pair data. For ASR, the subtitles are aligned to the audio using a CTC-based ASR model using CTC segmentation [20]. This method calculates a conﬁdence score to ﬁlter the audio-text pair [20]–[22]. Unlike the conventional hidden Markov model (HMM) based cleaning [23], [24], this method does not require any language-dependent pre-processing thanks to the end-toend framework. Also, for ASV, the audio can be ﬁltered by calculating variation of speaker representation within a video. This paper applies the above techniques to a Japanese corpus as a case study. Experimental evaluation demonstrates 1) we designed a new Japanese ASR benchmark with more than 1,300 hours of training data and the ofﬁcial test sets, and 2) we also constructed 900 hours of Japanese ASV corpus. The contributions of this work are as follows:
• We construct a new modern-size corpus for Japanese ASR and ASV from YouTube videos. The video list is open-sourced in our project page1.
• Our process is applicable to many languages with high reproducibility. The repository also contains the script, which has been extended to support multiple languages for the data collection.

2. CORPUS CONSTRUCTION
All of our steps are performed with minimal language-dependent processing. Figure 1 shows the procedure.
1https://github.com/sarulab-speech/jtubespeech

2.1. Data collection
Creating search terms. The ﬁrst step is to create search terms to be entered into the video search engine. We extract the target language’s words with hyperlinks from HTML ﬁles of Wikipedia articles. Unlike Gigaspeech [19], categories are not speciﬁed, i.e., all articles are used. Also, we extract “sudden-rise search terms” in the past few years from Google Trends.
Obtaining video IDs that have subtitles. Next, we retrieve video IDs that have subtitles. Entering a search word into the video search engine, we obtain a list of video ID candidates. Then, for each video, we retrieve whether the video has subtitles. In this paper, we use only manual subtitles but also make the list for automatic (i.e., machine-generated) subtitles.
Downloading audio and caption. Finally, we download the audio and manual subtitles from videos. Since the number of channels and sampling frequency of the audio ﬁle varies from video to video, we reformat the audio to 16 kHz-sampled monaural WAV format.
2.2. Speciﬁc cleansing for speech recognition
The obtained audio data were already annotated with subtitles including timings. This dataset still included many bad samples, e.g., unspoken subtitles, English audio with Japanese subtitles, and other variations of audio–text mismatches. To sort-out these bad samples, we calculated a score of how well the audio segment ﬁts to the subtitle and then ﬁltered the utterances based on their score. Furthermore, as many subtitle timings were inaccurate, we fully re-aligned the subtitles to the audio. To calculate a score and to re-align the subtitles to the audio, we use CTC segmentation [20] as an alignment tool [25]. CTC segmentation utilizes CTC log-posteriors to determine utterance timings in the audio given a ground-truth text.
Text pre–processing. We apply minimal text pre–processing so that the ground truth text obtained from subtitles is composed of characters or tokens in the model dictionary. Numbers are replaced with their spoken equivalent using the num2words Python library [26]. UTF-16 characters are mapped to the Japanese character set. Automated subtitles are detected and ﬁltered out based on the average relative Levenshtein distance between subtitles.
Alignment. The onset and offset timings of an utterance are then estimated in three steps: (1) In a forward pass, transition probabilities are mapped into a trellis diagram of the ground-truth token sequence over all time steps. (2) Backtracking starts from the most probable timing of the last character and then determines the most probable path through the trellis. (3) A conﬁdence score is derived for each utterance from the per-token probabilities in the trellis. The score is determined by the L consecutive CTC output frames of the utterance with the lowest token probabilities; we chose L = 30 that relates to approximately 1s of audio. For a more in-depth description, readers could refer to [20]. We apply one further modiﬁcation that helps with the alignment of fragmented subtitle text: In the original publication, the algorithm was conﬁgured to skip preambles by setting transition cost to zero for the token that marks the start of the ﬁrst utterance. We extended this in our setup to all utterances in order to skip any unrelated audio segments.
CTC scoring. We also calculate a CTC score for each audio-text pair from its YouTube timings instead of fully aligning the subtitles to audio. For this, the audio segment to each subtitle is cut out to then derive its conﬁdence score as described above.
Cleaning. Bad samples are then eliminated based on this conﬁdence score. The conﬁdence score provides an estimated log-space probability of how well the subtitles ﬁt the audio data; this value is mainly inﬂuenced by the quality of ASR models, input data, and data pre-processing. Note that a score threshold θ of −0.3, used in one

Video

Speaker space

Variation shrinkage

TTS Multi speakers

… Data contamination

Single speaker

Fig. 2. Comparison of distributions in speaker space.

of our experiments, can be interpreted as a production probability of at least 75% each second.
Inference of long audio ﬁles. Alignment requires inference of the audio using the encoder and the CTC layer of a pre-trained ASR model. We found that many audio ﬁles are longer than three hours and their inference reaches a practical limitation: The memory complexity using a Transformer-based model is quadratic with audio length – a device with 64 GB memory is at most able to infer 500s of audio at once. Due to their rather linear memory complexity along longer audio data, RNN-based models require less memory and thus, it is possible to decode longer ﬁles. This can be scaled up to 2.7h of audio until the encoding reaches a memory limit of the Pytorch toolkit. To overcome this limitation, we partition long audio ﬁles.
Partitioning is done by splitting the audio into smaller blocks, performing inference on the parts, and then concatenating CTC posteriors. Maximum block sizes were chosen depending on the memory consumption of the model and available memory; the last block may be 25% longer to avoid too short blocks. As abrupt cuts in the audio cause distortions during inference, we add an overlap to each side of the block; overlapping posteriors are later omitted for scoring. By inspection of CTC posteriors, an overlap of at least 600ms is required to reduce impact of partitioning on scoring to an acceptable level. All lengths were chosen as multiples of the samples–to– posteriors ratio to preserve timing information and ensure the correct shape of the concatenated CTC posterior tensor.

2.3. Speciﬁc cleansing for speaker veriﬁcation
Unlike an ASR corpus, ASV requires high-quality speaker labels. Therefore, we propose an unsupervised method for only extracting monologue videos (i.e., single-speaker videos) from the obtained videos. Furthermore, we remove videos with text-to-speech (TTS) voices, which have different characteristics from natural speech.
Removing non-speech and too-short videos. First, we delete videos without speech. Unlike the corpus for ASR, there is no need to align subtitle and speech. Therefore, we simply use voice activity detection (VAD) here. We applied VAD to sections trimmed out based on the subtitles and used only sections mainly consisting of speech. Also, too-short videos are deleted. This is for robustly calculating the intra-video statistics described below.
Evaluating intra-video variation in speaker space. To extract single-speaker videos, we compute the speaker variability in the video. Figure 2 illustrates the concept. We use the d-vector [6], deep learning-based speaker representation, which is extracted using pre-trained models. The d-vector is ﬁrst calculated for each utterance. Then, the variance of the d-vectors is calculated within the video. We expect that the variance of TTS voices becomes smaller than the single-speaker voices because the TTS voice has no ﬂuctuation among utterances. Also, if the different speakers’ voices are contaminated (i.e., multi-speaker video), the variance will become larger than the single-speaker video. Therefore, we can eliminate TTS videos and multi-speaker videos by setting the appropriate threshold to the variance. For implementation, the d-vector is reduced to a lower dimension by t-SNE [27], and the determinant of

Table 1. Results of data collection. Videos with automatic subtitles

are not used in this paper, but the video ID is also opensourced.

Retrieved entity

Value

#search-terms

2.34M terms

#videos found in the search 11.9M videos

#videos with manual subtitles 0.11M videos

(#videos with auto subtitles) 4.96M videos

Table 2. Comparison of speech corpora of Japanese (upper half) and

other rich-resource languages (lower half).

Lang. Task

Corpus

Open-source Duration

Ja ASR/ASV

JNAS [28]

No

90

Ja

ASR

CSJ [15]

No

600

Ja

ASR LaboroTVspeech [29]

Yes

2,000

Ja

ASR Common Voice [10]

Yes

2

Ja

ASV

Liveness [30]

No

4

Ja ASR/ASV JTubeSpeech (ours)

Yes

1,300/900

En ASR

Librispeech [9]

Yes

982

En ASR Common Voice [10]

Yes

1,100

En ASR

SPGISpeech [11]

Yes

5,000

En ASR

GigaSpeech [19]

Yes

10,000

En ASV

VoxCeleb [16]

Yes

2,800

Zh ASR Common Voice [10]

Yes

12

Zh ASR

AISHELL-2 [31]

Yes

1,000

Zh ASV

CN-Celeb [14]

Yes

1,000

its covariance matrix is obtained. Grouping videos by YouTube Channel ID. It is necessary to
prevent the same speaker from being counted as different speakers. We obtain and use the YouTube Channel ID of each video. Singlespeaker videos with the same Channel ID are grouped together and considered as a unique speaker.
3. EXPERIMENTAL EVALUATION
3.1. Evaluation in data collection
The period of data collection was between February and April of 2021. From the data collection results listed in Table 1, we can describe 1) 5.09 videos are found for each search term and 2) 0.92 % of videos have manual subtitles, and 41.7 % have automatic subtitles. In the end, we obtained approximately 10, 000 hours of speech data from 110, 000 YouTube videos.
Table 2 shows a comparison with the existing corpus. The duration of our corpus is a subset used in the ASR or ASV experiments described below. Among ASR corpora, our corpus is similar in size to LaboroTVspeech (Ja) [29] and Common Voice (En) [10]. Also, among ASV corpora, it is the ﬁrst open-source Japanese corpus and is similar in size to CN-Celeb (Zh) [14].

3.2. Evaluation in speech recognition
Data cleaning. The most important data pre-processing is to prune utterances that have the wrong transcriptions and ﬁx the incorrect timing. We consistently employed CTC segmentation and CTC scoring, as described in Section 2.2. We used a pre-trained CTC model based on the ESPnet LaboroTVspeech [29] recipe [32]. Table 3 summarizes the statistic of the various training and test sets.
As a pilot study, we used two subsets of our corpus: (1) ”single speaker” (or ”ss” in short) based on the single speaker subset, as described in Section 2.3, and (2) ”top 15k” (or ”15k” in short) extracted based on top 15,000 videos in terms of the average score for each video. Note that top 15k is pruned only based on utterance conﬁdence scores and it may contain multi speaker videos, unlike single speaker. Also a part of these subsets is overlapped.

0.5 Transformer model 0.5

RNN model

0.4

0.4

Frequency

0.3

0.3

0.2

0.2

0.1

0.1

0.0 15 10 5 0 0.0 15 10 5 0

Score

Score

Fig. 3. Histograms of the score for different ASR models over all utterances of top 15k.

Table 3. Training and test data statistics for the ASR task. θ is a

threshold used to prune bad utterances based on the CTC score.

θ # videos

# utts hours

dev easy jun21 -0.3

110

785 0.7

eval easy jun21 -0.3

106

829 0.7

dev normal jun21 -1.0

128

1,036 1.1

eval normal jun21 -1.0

129

834 0.8

train single speaker -0.3 1,297 14,797 12.7

train single speaker -0.5 1,792 26,209 24.2

train single speaker -1.0 2,906 66,563 71.9

train single speaker -3.0 4,342 285,846 362.0

train top 15k -3.0 14,418 1,048,699 1087.1

train ss 15k -3.0 17,761 1,270,124 1376.9

These subsets are further decomposed into the training and test sets, which will be explained in the next section. We also prepare the various training sets by changing the threshold value θ of the conﬁdence score obtained by CTC segmentation for the single speaker and top 15k subsets. To determine the threshold value, we investigate the distribution of the conﬁdence score over all utterances in the top 15k subset, as shown in Figure 3. This distribution clearly shows that the peak of the distribution starts around -3.0 in both RNN and transformer based CTC models. Based on the observation, we regard the utterances located in the wide-based region as outlier data points, and use -3.0 as the lowest threshold for pruning in Table 3. The largest training set is obtained by combining the single speaker and top 15k subsets (train ss 15k).
Test set design. Focusing on the ”single speaker” videos reduces the cost of manual process of verifying audio and transcriptions. The rest of the procedure is as follows: (1) Select 1,621 videos from the ”single speaker” videos that include ”easy” utterances scored more than -0.3 threshold value based on CTC. (2) Randomly pick up 324 videos, around 20%, and use them as a test video set, which has 3,396 easy utterances in total. (3) Manually listen utterances and identify 1,614 utterances having correct transcriptions. (4) Split them into the development dev easy jun21 and evaluation eval easy jun21 sets, which have 785 and 829 utterances, respectively. We also made an additional test set with ”normal” utterances2 scored more than −1.0 threshold value. We ﬁxed to use the same test video set as we deﬁned before, and performed steps 3 and 4. We made dev normal jun21 and eval normal jun21 sets, which have 1,036 and 834 utterances, respectively.
Experimental results. We used an ESPnet state-of-the-art conformer model [5], [24] based on hybrid CTC/attention architectures [33]. The detailed conﬁguration can be found in the ESPnet JTubeSpeech recipe.
2The deﬁnition of ”easy” and ”normal” test sets are determined based on the CTC score objectively. Our listening process and later ASR experiments conﬁrmed that this categorization is reasonable.

CER(%)
Log determinant

dev_easy_jun21 30.0

eval_easy_jun21

dev_normal_jun21

eval_normal_jun21

20.0

10.0

0.0 ss_12h

ss_24h

ss_71h

ss_362h

ss_15k_1376h

Fig. 4. ASR performance for various amounts of training data (12, 24, 71, 362, and 1376 hours) by changing the score threshold and combining multi-speaker recordings.

Table 4. The effectiveness of CTC segmentation.

dev easy dev normal dev easy dev normal

original timing

11.5

16.5

9.2

15.7

CTC segmentation

9,2

14.3

6.9

13.6

Figure 4 shows the ASR performance for various amounts of training data by changing the score threshold θ. We conﬁrmed that the normal test set was more difﬁcult than the easy test set. This result validates our design of the test set by controlling the difﬁculties based on the score. As regards the amounts of training data, if we increase the amount of training data by reducing θ, the training data may contain more noisy transcriptions or distorted audio segments. Nevertheless, the ASR performance was improved, and the ﬁnal CERs were 5.2% in eval easy jun21 and 10.7% in eval normal jun21. These CER ranges are similar to other Japanese ASR benchmarks, e.g., 4 − 6% in CSJ [15] and 13% in LaboroTVspeech [29]. We also list the largest training data case (ss 15k 1376h) by combining the single speaker subset and top 15k subsets. The performance was improved in most cases except for dev easy jun21. This result conﬁrms that more training data is generally helpful to improve the performance of more challenging data.
Finally, we evaluate the effectiveness of CTC segmentation, as discussed in Section 2.2. We used train single speaker with −1.0 score threshold (71.9 hours) and prepared the corresponding training data based on the original YouTube timings only with CTC scoring. Table 4 shows that the CTC segmentation signiﬁcantly improved the performance and shows the effectiveness of the re-aligning for the YouTube audio data, as suggested by [19].

3.3. Evaluation in speaker veriﬁcation
3.3.1. Data cleansing
We used py-webrtcvad3 for VAD and a pre-trained model4 for extracting d-vectors. The variation in speaker space was computed from more than 10 utterances for each video. As a pilot study, we used randomly selected 35,000 videos (approx. 30% of total). Figure 5 shows two levels of rapid increase: near 0 and near 8 to 9 on the x-axis. Following the concept in Figure 2, we set a threshold around these levels and assign “TTS,” “single speaker,” or “multi speakers,” class to the videos.
We quantitatively evaluate the authenticity of this classiﬁcation. We randomly extracted 100 videos from each class and annotated a true class label. Multi-TTS videos and overdubbed-voice videos were annotated as “multi speakers.” To accurately ﬁnd TTS videos, a TTS specialist participated in this annotation. Table 5 shows that
3https://github.com/wiseman/py-webrtcvad 4https://github.com/yistLin/dvector

104

103

Multi speakers

102

Single speaker

101 TTS

100

0

5 10 15 20 25 30 35

Video ID index (x0.001)

Fig. 5. The intra-video variation in speaker space

Table 5. Comparison of classiﬁed and annotated video types.

Classiﬁed \Annotated TTS single speaker multi speakers

TTS

20

80

0

single speaker

5

95

0

multi speakers

1

36

63

each class has their suitable videos. Especially, the “single speaker” class does not contain multi-speaker videos. Therefore, our method works for choosing the single-speaker videos. Some single-speaker videos leak to the “TTS” class, but this impact is limited because the “TTS” class has only a few videos.

3.3.2. ASV dataset design and models
From the single-speaker videos, 92 unique speakers were selected as the enrollment and testing speakers. For the training and development datasets, 1,795 unique speakers were selected. In the training, 127,997 and 25,392 utterances were used for training and development datasets, respectively. In the testing, we designed an enrollment utterance and a testing one were picked up from the same video but different segments. In the testing, we performed 25,392 trial pairs, included 276 correct pairs and 25,116 incorrect pairs.
We used a speaker embedding network, which was built with four convolutional neural networks, pooling layers and two full connected layers. As the input feature, 40-order Mel-Frequency Cepstrum Coefﬁcients with 25-ms frame length and half overlapped shift were used. The speaker-embedding vector was 512 dimensions. The evaluation metric is equal error rate (EER) was used for our evaluation by comparing the cosine distance.

3.3.3. ASV performance
We performed ASV evaluations, and the EER of our ASV system with JTubeSpeech was 10.9%. Our system was regarded as the speaker embedding system based on the CNN-based model. Even though such a simple system, this result had the similar performance to the CNN-based model (VGG-M) trained with Voxceleb1 [16]. As a pilot study, we designed a simple benchmark. For example, the result was obtained with the small model and probabilistic linear discriminant analysis (PLDA) scoring and data augmentation techniques were not performed. However, from this simple benchmark, we could show the techniques of data cleaning for selecting a unique speakers and constructing ASV systems performed well. Consequently, we established the Japanese ASV systems with large-scale open source media in the ﬁrst time.
4. CONCLUSION
In this paper, we propose a speech corpus construction strategy and build 1, 300 and 900 hours data for Japanese ASR and ASV. One of our future directions is to extend the corpus to multiple languages.

References
[1] J. Towns, T. Cockerill, M. Dahan, et al., “Xsede: Accelerating scientiﬁc discovery computing in science & engineering, 16 (5): 62–74, sep 2014,” URL https://doi. org/10.1109/mcse, vol. 128, 2014.
[2] N. A. Nystrom, M. J. Levine, R. Z. Roskies, et al., “Bridges: A uniquely ﬂexible hpc resource for new communities and data analytics,” in Proceedings of the 2015 XSEDE Conference: Scientiﬁc Advancements Enabled by Enhanced Cyberinfrastructure, 2015, pp. 1–8.
[3] G. E. Dahl, D. Yu, L. Deng, et al., “Context-dependent pretrained deep neural networks for large-vocabulary speech recognition,” IEEE Transactions on Audio, Speech, and Language Processing, vol. 20, no. 1, pp. 30–42, 2012.
[4] A. Graves and N. Jaitly, “Towards end-to-end speech recognition with recurrent neural networks,” in International conference on machine learning, 2014, pp. 1764–1772.
[5] A. Gulati, J. Qin, C.-C. Chiu, et al., “Conformer: Convolutionaugmented Transformer for Speech Recognition,” in Proc. Interspeech, 2020, pp. 5036–5040.
[6] E. Variani, X. Lei, E. McDermott, et al., “Deep neural networks for small footprint text-dependent speaker veriﬁcation,” in ICASSP, 2014, pp. 4080–4084.
[7] D. Snyder, D. Garcia-Romero, G. Sell, et al., “X-vectors: Robust DNN embeddings for speaker recognition,” in ICASSP, 2018, pp. 5329–5333.
[8] C. Cieri, D. Miller, and K. Walker, “The ﬁsher corpus: A resource for the next generations of speech-to-text.,” in LREC, vol. 4, 2004, pp. 69–71.
[9] V. Panayotov, G. Chen, D. Povey, et al., “Librispeech: An asr corpus based on public domain audio books,” in ICASSP, 2015, pp. 5206–5210.
[10] R. Ardila, M. Branson, K. Davis, et al., “Common Voice: A massively-multilingual speech corpus,” in Proceedings of the 12th Language Resources and Evaluation Conference, 2020, pp. 4218–4222.
[11] P. K. O’Neill, V. Lavrukhin, S. Majumdar, et al., SPGISpeech: 5,000 hours of transcribed ﬁnancial audio for fully formatted end-to-end speech recognition, 2021.
[12] Y. Liu, P. Fung, Y. Yang, et al., “HKUST/MTS: A very large scale Mandarin telephone speech corpus,” in International Symposium on Chinese Spoken Language Processing, 2006, pp. 724–735.
[13] J. Du, X. Na, X. Liu, et al., “Aishell-2: Transforming mandarin asr research into industrial scale,” arXiv preprint arXiv:1808.10583, 2018.
[14] Y. Fan, J. Kang, L. Li, et al., CN-CELEB: A challenging Chinese speaker recognition dataset, 2019.
[15] K. Maekawa, H. Koiso, S. Furui, et al., “Spontaneous speech corpus of Japanese.,” in Proc. LREC, 2000, pp. 947–952.
[16] A. Nagrani, J. S. Chung, W. Xie, et al., “Voxceleb: Largescale speaker veriﬁcation in the wild,” Computer Science and Language, 2019.
[17] S. Abu-El-Haija, N. Kothari, J. Lee, et al., YouTube-8M: A large-scale video classiﬁcation benchmark, 2016.

[18] D. Galvez, G. Diamos, J. M. C. Torres, et al., “The People’s Speech: A large-scale diverse English speech recognition dataset for commercial usage,” 2021, https : / / openreview.net/forum?id=R8CwidgJ0yT.
[19] G. Chen, S. Chai, G. Wang, et al., “GigaSpeech: An evolving, multi-domain asr corpus with 10,000 hours of transcribed audio,” arXiv preprint arXiv:2106.06909, 2021.
[20] L. Ku¨rzinger, D. Winkelbauer, L. Li, et al., “Ctc-segmentation of large corpora for german end-to-end speech recognition,” in Speech and Computer, 2020, pp. 267–278.
[21] Y. Zhang, E. Bakhturina, K. Gorman, et al., “Nemo inverse text normalization: From development to production,” arXiv preprint arXiv:2104.05055, 2021.
[22] E. Bakhturina, V. Lavrukhin, B. Ginsburg, et al., Hi-ﬁ multispeaker english tts dataset, 2021.
[23] J. Trmal, M. Wiesner, V. Peddinti, et al., “The kaldi openkws system: Improving low resource keyword search.,” in Proc. Interspeech, 2017, pp. 3597–3601.
[24] P. Guo, F. Boyer, X. Chang, et al., “Recent developments on espnet toolkit boosted by conformer,” in ICASSP, 2021, pp. 5874–5878.
[25] L. Ku¨rzinger and D. Winkelbauer, Ctc-segmentation: Segment an audio ﬁle and obtain utterance alignments. (python package), https : / / github . com / lumaku / ctc segmentation, 2021.
[26] V. Dupras, M. Grigaitis, and T. Ogawa, Num2words: Modules to convert numbers to words. https : / / github . com/savoirfairelinux/num2words, 2021.
[27] L. Maaten and G. Hinton, “Visualizing data using t-SNE,” Journal of machine learning research, vol. 9, pp. 2579–2605, 2008.
[28] K. Itou, M. Yamamoto, K.Takeda, et al., “JNAS: Japanese speech corpus for large vocabulary continuous speech recognition research,” Journal of the Acoustical Society of Japan (E), vol. 20, no. 3, pp. 199–206, 1999.
[29] S. Ando and H. Fujihara, “Construction of a large-scale Japanese ASR corpus on TV recordings,” in ICASSP, 2021, pp. 6948–6952.
[30] S. Shiota, F. Villavicencio, J. Yamagishi, et al., “Voice liveness detection algorithms based on pop noise caused by human breath for automatic speaker veriﬁcation,” in Sixteenth annual conference of the international speech communication association, 2015.
[31] J. Du, X. Na, X. Liu, et al., “AISHELL-2: Transforming Mandarin ASR Research Into Industrial Scale,” ArXiv, 2018.
[32] V. authors, Espnet/egs2/laborotv/asr1 · espnet/espnet, https: //github.com/espnet/espnet/tree/master/ egs2/laborotv/asr1, 2021.
[33] S. Watanabe, F. Boyer, X. Chang, et al., “The 2020 ESPNet update: New features, broadened applications, performance improvements, and future plans,” arXiv preprint arXiv:2012.13006, 2020.

