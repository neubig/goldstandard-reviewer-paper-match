1–20

arXiv:2001.09377v1 [cs.LG] 26 Jan 2020

Constrained Upper Conﬁdence Reinforcement Learning

Liyuan Zheng, Lillian J. Ratliff Department of Electrical & Computer Engineering, University of Washington

LIYUANZ8, RATLIFFL@UW.EDU

Abstract

Constrained Markov Decision Processes are a class of stochastic decision problems in which the decision

maker must select a policy that satisﬁes auxiliary cost constraints. This paper extends upper conﬁdence rein-

forcement learning for settings in which the reward function and the constraints, described by cost functions,

are unknown a priori but the transition kernel is known. Such a setting is well-motivated by a number of

applications including exploration of unknown, potentially unsafe, environments. We present an algorithm

C-UCRL

and

show

that

it

achieves

sub-linear

regret

(O(T

3 4

log(T /δ))) with respect to the reward while

satisfying the constraints even while learning with probability 1 − δ. Illustrative examples are provided.

1. Introduction
Markov Decision Processes (MDPs) have been successfully utilized to model sequential decision-making problems in stochastic environments. In the typical approach to learning a policy, the decision-maker trades off between exploration and exploitation, gradually improving their performance at the task as learning progresses. Reinforcement learning, a standard paradigm of learning in MDPs, has shown exceptional success in a variety of domains such as video games [22], robotics [20, 19], recommender systems [26], autonomous vehicles [25], among many others.
However, in many of these real-world applications, there is often additional constraints, or speciﬁcations that lead to constraints, on the learning problem. For instance, a recommender system should avoid presenting offending items to users and autonomous vehicles must avoid crashing into others while navigating [14]. Building algorithms that respect safety constraints not only during normal operation, but also during the initial learning period, is a question of particular interest [18]. This problem is known as the safe exploration problem [23, 3]. In the standard MDP framework, an approach for baseline performance is risk-sensitive reinforcement learning [11, 14], where the optimization criterion is transformed in order to reﬂect a subjective measure balancing the return and the risk.
On the other hand, in a safety-critical environment, it is more reasonable to separate the return and the risk criterion, and enforce constraint satisfaction in the learning procedure. A standard formulation for an environment with safety constraints is the constrained MDPs (CMDPs) [2]. A decision-maker facing a CMDP aims to maximize the total reward while satisfying the constraints on costs in expectation over the whole trajectory.
In recent literature, policy gradient-based reinforcement learning algorithms have been proposed as a means to learn a policy for a CMDP. The following are two constrained policy search algorithms with stateof-the-art performance guarantees: Lagrangian-based actor-critic algorithm [5, 8] and Constrained Policy Optimization (CPO) [1]. However, for these policy gradient-based methods, safety is only approximately guaranteed after a sufﬁcient learning period. The fundamental issue is that without a model, safety must be learned via trial and error, which means it may be violated during initial learning interactions.
Model-based approaches have utilized Gaussian processes to model the state safety values or the dynamic uncertainties [4, 17, 27, 7] or utilized Lyapunov-based methods [9] to guarantee safety during learn-

1

CONSTRAINED UPPER CONFIDENCE REINFORCEMENT LEARNING
ing. Although these methods guarantee constraint satisfaction during learning, an arguably valuable analysis of the regret is lacking.
In unconstrained settings when the reward and transition kernel are unknown, upper conﬁdence based reinforcement learning algorithms have been proposed—namely, UCRL2 [15]—with sub-linear regret. The key idea is to build conﬁdence intervals on the reward and transition kernel and iteratively solve for policies using value iteration based methods.
In this work, we are not only interested in learning the optimal policy that satisﬁes the constraints via interacting with the stochastic environment, but also in ensuring performance guarantees on the learning algorithm during learning. With some practical scenarios in mind, we make the assumption that the rewards and constraint costs are unknown. For instance, consider a rover exploring the Mars landscape; here one can model the dynamics of the rover as known with some uncertainty and the reward and constraints which model the value of exploring the environment as unknown—e.g., constraints can be abstracted as costs which seek to limit the frequency of visiting a potentially hazardous states [13].
Motivated by upper conﬁdence reinforcement learning [15], we introduce the constrained upper conﬁdence reinforcement learning (C-UCRL) algorithm which combines elements of the classical UCRL2 algorithm with robust linear programming1. We deﬁne our goals as follows: (1) maintain constraint satisfaction throughout the learning process with high probability, and (2) achieve sub-linear regret comparing the rewards collected by the algorithm during learning with the reward of an optimal stochastic policy.
Contributions. The contributions can be summarized as follows. Building on UCRL2, we introduce the C-UCRL algorithm (Algorithm 1). We show that C-UCRL is guaranteed to satisfy constraints during learn-
3
ing with probability at least 1 − δ (Theorem 3) and achieves O(T 4 log(T /δ)) reward regret (Theorem 7). Of independent interest, we note that when the state space is trivial, the setting we consider subsumes stochastic multi-armed bandits with per-round budget constraints, where the optimal policy is a randomized policy across arms.
Organization. The rest of the paper is organized as follows. An overview of related work is provided in Section 2. Mathematical preliminaries and our algorithm are introduced in Section 3. Analysis of both constraints satisfaction and reward regret is provided in Section 4. Several illustrative examples are provided in Section 5. In those experiments we compare our proposed method to Risk-Sensitive UCRL2 algorithm and show that UCRL2 algorithm fails to converge to the optimal policy while our algorithm does. The paper is concluded in Section 6 with a brief summary and discussion of future directions.
2. Related Work
Recently, several policy gradient-based reinforcement learning algorithms have been proposed for learning policies for CMDPs. In particular, there are two noteable constrained policy search algorithms which enjoy state-of-the-art performance: a Lagrangian-based algorithm [5, 8] and Constrained Policy Optimization (CPO) [1]. The Lagrangian-based algorithm formulates the CMDP problem as a minimax problem and uses primal-dual gradient optimization to ﬁnd the saddle point solution. While this procedure will asymptotically converge to the saddle point solution, in general there is no guarantee on policies being safe during the learning procedure. On the other hand, CPO—a method that derives from an extension of trust-region policy optimization (TRPO)—guarantees monotonic performance improvements on the expected reward and a guarantee on constraint satisfaction throughout training. While this algorithm is safe during learning, analyzing its convergence is challenging and the regret analysis with respect to reward is lacking.
1. We remark that UCRL2 assumes the transition kernel is unknown a priori where we assume it is known; we leave extending our approach to unknown dynamics to future work.
2

CONSTRAINED UPPER CONFIDENCE REINFORCEMENT LEARNING
As an alternative to policy gradient reinforcement learning algorithms, linear programming based algorithms have been proposed. In [13], CMDPs with known reward, constraints, transition kernel but uncertain initial state distribution are considered. Linear programming based algorithms are proposed to solve for safe policies in this setting. In our setting, however, the reward and constraints are stochastic and considered unknown a priori, which the stochastic transition kernel is known.
Most similar to our approach is UCRL2; in particular, our approach can be viewed as an extension of UCRL2 [15], in some sense, by incorporating constraints; the one difference is that we assume the transition kernel is known while the classical UCRL2 algorithm does not. We leave extending our setting to unknown transition kernels to future work. As alluded to in the introduction, in UCRL2, the reward and transition kernel are approximated and the policy is obtained by value iteration based methods in a optimism in the face of uncertainty fashion. Further, the performance of UCRL2 is analyzed by bounding the regret with respect to the optimal deterministic policy. CMDPs, however, in general do not admit deterministic policies. In C-UCRL, the reward and constraints are approximated and the policy is obtained by solving a robust linear program. Performance is assessed by computing the reward regret with respect to the optimal randomized policy.
Finally, our work is related to the multi-armed bandit problem with constraints. Previous works, e.g., have considered the multi-armed bandit problem with an auxiliary cost in addition to the traditonal reward [12, 28]. The ‘game’ (between the player and the environment) ends when the sum of current costs associated with the played arms exceeds the remaining budget, which is ﬁxed and known to the player. The typical approach is to construct upper conﬁdence bounds for the reward-to-cost ratio and then utilize them in upper conﬁdence bound-based algorithms. On the other hand, in our approach, we use upper conﬁdence bounds for both reward and cost, and solve a linear program to obtain the policy policy. In related work, fairness constraints are incorporated into a multi-armed bandit setting; in particular, arms that are perceived to have less value/reward should never favored over better performing alternatives, despite a learning algorithm’s uncertainty over the true payoffs [16]. In such settings, the algorithm is forced to pick arms uniformly until the player has enough conﬁdence of the performance of arms. Connecting to this body of work, our problem reduces to a constrained multi-armed bandit problem when there is a single state. The main difference between our setting and that of the majority existing multi-armed bandit literature with constraints is that the optimal policy and policies obtainable by our algorithm can be a randomized or stochastic policy as opposed to a deterministic ‘best arm’ policy.
3. Constrained Upper Conﬁdence Reinforcement Learning Algorithm
An MDP is a tuple (S, A, P, r), where S is the set of states, A is the set of actions, P : S ×A×A → [0, 1] is the transition kernel such that P (s |s, a) is the probability of transitioning to state s given that the previous state was s and the agent took action a in s, and r : S ×A → [0, 1] is the reward function. A stationary policy π : S × A → [0, 1] is a map from states to a probability distribution over actions, with π(a|s) denoting the probability of selecting action a in state s. We consider the setting in which the transition kernel P (s |s, a) is known to the agent, but the reward and costs are stochastic and unknown. In the example of a rover exploring the surface of Mars, the agent (rover) is aware of the transition probability of next state based on its action, but the safety quality of each state is unknown. Let S = |S| and A = |A| where | · | is the cardinality of its argument. We use the notation [·] = {1, . . . , ·} for index sets.
3.1. Constrained Markov Decision Processes
A CMDP is an MDP augmented with ‘cost’ constraints that restrict the set of allowable policies for that MDP. For a given CMDP, we consider the performance measure to be the inﬁnite horizon average reward
3

CONSTRAINED UPPER CONFIDENCE REINFORCEMENT LEARNING

which is given by

J (π) = limT →∞ Eτ∼π

1 T

T −1 t=0

r(st

,

at

)

(1)

where τ denotes a trajectory τ = (s0, a0, s1, . . . ), and τ ∼ π is shorthand for indicating that the distribution

over trajectories depends on π: s0 ∼ p(s0), at ∼ π(·|st), st+1 ∼ P (·|st, at). Similarly, deﬁne the average

constraint costs by

Ci(π) = limT →∞ Eτ∼π

1 T

T −1 t=0

ci

(st

,

at

)

.

(2)

where {c1, . . . , cm} with ci : S × A → [0, 1] are the cost constraints. The CMDP is then deﬁned by

maxπ {J(π)| Ci(π) ≤ di, ∀ i ∈ [m]}

(3)

where {d1, . . . , dm} are upper bounds on the average constraint costs. Note that without loss of generality both the reward and costs are random variables with a distribution supported on [0, 1].
Denote the mean of reward and cost constraint functions as r¯(s, a) = E[r(s, a)], c¯i(s, a) = E[ci(s, a)] where the expectation is taken with respect to the distribution of the reward and cost function of that stateaction pair (s, a). If the transition kernel P (s |s, a), the mean of the reward function r¯(s, a), and mean cost functions c¯i(s, a) are all given, them we can solve the CMDP by solving the following linear program [2]:

max s,a r¯(s, a)y(s, a)
y
s.t. a y(s , a ) = s,a P (s |s, a)y(s, a) s,a y(s, a) = 1, y(s, a) ≥ 0 s,a c¯i(s, a)y(s, a) ≤ di, i ∈ [m]

To simplify notation, we write the above linear program in matrix form as follows:

maxy{ r¯ y | Ioy = P y, 1 y = 1, y ≥ 0, c¯ y ≤ d}

(4)

where r¯ ∈ RSA, y ∈ RSA, c¯ ∈ RSA×m, d ∈ Rm, P ∈ RS×SA, and Io ∈ RS×SA is a sparse matrix built by placing S row blocks of length A in a block diagonal fashion, where each row block consists of all ones. Here, y ∈ RS×A represents the steady-state occupation measure deﬁned by

y(s, a) = limT →∞ Eτ∼π

1 T

T −1 t=0

1{st

=

s, at

=

a}

.

(5)

With y¯ the solution of this linear program, the optimal stationary policy is

π¯(a|s) = y¯(s, a)/( a∈A y¯(s, a)).

(6)

Remark. It is worth noting that unlike in tabular MDPs without constraints, where the optimal policy is always deterministic, the optimal policy in CMDPs could be stochastic [24]. It is, in fact, trivial to solve the CMDP if the optimal policy in CMDPs is deterministic because that means the constraints are not active.

3.2. Constrained Upper Conﬁdence Reinforcement Learning Algorithm
Since the reward and constraint cost functions are unknown, motivated by UCRL2, we introduce C-UCRL (Algorithm 1). In general, the C-UCRL algorithm follows a principle of “optimism in the face of reward uncertainty; pessimism in the face of cost uncertainty.” That is, it deﬁnes conﬁdence intervals for the reward and cost of each state-action pair given the observations so far, and solves for the optimistic policy that satisﬁes the constraints. More speciﬁcally, in C-UCRL, given the current conﬁdence interval estimates, we

4

CONSTRAINED UPPER CONFIDENCE REINFORCEMENT LEARNING

Algorithm 1 Constrained UCRL (C-UCRL) algorithm

Input: safety parameter δ ∈ (0, 1), baseline policy π0(a|s), episode length h.

Initialization: set t = 1, observe the initial state s1

for episodes k = 1, 2, . . . , K do

tk = t ;

// initialize start time of episode k

while t ≤ tk + h;

// Execute baseline policy h times for exploration

do

Draw action at ∼ π0(·|st)

Observe reward rt, costs ci,t, and the next state st+1

t←t+1

end

Nk(s, a) = Rk(s, a) = Ci,k(s, a) =

t t

=1

1(st

= a, at

= a),

∀(s, a) ∈ S × A ;

t t

=1

rt

1(st

= a, at

= a);

t t

=1

ci,t

1(st

= a, at

= a);

rˆk(s, a) = maxR{1k,(Nsk,a()s,a)} , cˆi,k(s, a) = maxC{i1,,kN(sk,(as),a)} ;

// set the state-action count // compute cumulative reward
// compute the cumulative costs // compute estimates

y˜k ← arg max of (RLP) using r˜k(s, a) and c˜i,k(s, a) in (7) and (8), resp.

π˜k ← y˜k(s, a)/( a∈A y˜k(s, a)) ;

// recover policy

while t ≤ tk + kh ;

// Execute π˜k policy (k − 1)h times

do

Draw action at ∼ π˜k(·|st)

Observe reward rt, costs ci,t, and the next state st+1

t←t+1

end

end

use a robust linear program [21] formulation to ﬁnd a policy using the conﬁdence intervals as determined at the current iteration.
In particular, in episode k, we start by executing the baseline policy π0 for a constant h number of iterations2. It is common to assume a initial safe baseline policy [1] and without loss of generality, we assume under such policy, the Markov chain resulting from the CMDP is irreducible and aperiodic [6]. This baseline policy could, e.g., be obtained by some prior information about which states are safe to start the conservative exploration3. After executing π0, we deﬁne estimates of the reward and costs by
rˆk(s, a) = maxR{1k,(Nsk,a()s,a)}
and cˆi,k(s, a) = maxC{i1,,kN(sk,(as),a)} ,
respectively, where Nk(s, a), Rk(s, a), and Ci,k(s, a) are the state-action count, and cumulative reward and costs, respectively, as deﬁned in Algorithm 1. The visitation frequency random variable Nk(s, a) is deﬁned to be the sum of indicators of whether or not the state-action pair (s, a) was visited in each iteration over all episodes. The corresponding reward Rk(s, a) and constraint costs Ci,k(s, a) are deﬁned similarly.
2. The heuristic for choosing h is based on the mixing time of the Markov chain induced by π0 given the known transition kernel for the CMDP.
3. Choosing π0 is an important component of C-UCRL. In Section 5, we provide some intuitive choices for the simple examples we present, while we leave further development on how to select π0, either heuristically or theoretically, to future work.

5

CONSTRAINED UPPER CONFIDENCE REINFORCEMENT LEARNING

Using these estimates, we deﬁne

r˜k(s, a) = min rˆk(s, a) + log2(SmAa(xm{1+,N1)kπ(s2,ta3k)/}3δ) 1/2, 1 (7)

and c˜i,k(s, a) = min cˆi,k(s, a) + log2(SmAa(xm{1+,N1)kπ(s2,ta3k)/}3δ) 1/2, 1 , (8)

where

log(SA(m+1)π2t3k/3δ) 1/2 2 max{1,Nk(s,a)}

deﬁnes the conﬁdence interval as we show in Section 4. We then use (7) and (8) to deﬁne the following robust linear program:

maxy{ r˜k y | Ioy = P y, 1 y = 1, y ≥ 0, c˜k y ≤ d}.

(RLP)

A few comments here on guaranteeing that the feasible set is non-trivial are warranted. Our analysis results
are predicated on π0 and h being chosen such that in each episode the robust linear program we solve has at least one feasible solution. The duration h is chosen based on the mixing time of the induced Markov chain
under the baseline policy with the goal of ensuring with high probability that the feasible set is not empty; for
instance, ‘sufﬁcient’ exploration will guarantee that c˜1 y ≤ d for some y ∈ {Ioy = P y, 1 y = 1, y ≥ 0}. It is possible that in the ﬁrst episode, even after h iterations of executing the baseline policy, that there is no y such that c˜1 y ≤ d4. A heuristic we use in practice is to run the baseline policy π0 for as many iterations as it takes for y0 ∈ {Ioy = P y, 1 y = 1, y ≥ 0, c˜1 y ≤ d}. Then, we are guaranteed that in all future episodes, y0 is always in the feasible set of (RLP). We leave further exploration of theoretically guaranteeing that the (RLP) has a non-trivial feasible set in the ﬁrst episode to future work.
Returning to the description of the algorithm, in episode k, the solution y˜k to the robust linear program is then used to construct the policy π˜k via (6). This policy is executed for a linearly increasing number of iterations (k − 1)h where k is the episode index and h is the ﬁxed duration used for executing the baseline
policy. To summarize, for each episode of C-UCRL, we execute the baseline policy for h steps, estimate the
reward and costs, and then execute π˜k for a linearly increasing (in the number of epochs) number of steps (k − 1)h, making kh the total duration of episode k.

4. Analysis: Regret Bounds and High-Probability Safety Guarantees
In this section, we summarize our analysis results. We ﬁrst show that C-UCRL has guarantees on constraint satisfaction during learning. Then, we provide regret analysis with respect to the reward, showing that the regret is sub-linear.
4.1. Constraint/Safety Guarantees To capture constraint satisfaction, we leverage the notion of δ-safety.
Deﬁnition 1 (δ-safe) An algorithm is δ-safe if, with probability at least 1 − δ, for all time steps t, the policy executed by the algorithm satisﬁes Ci(πt) ≤ di, ∀i ∈ [m].
4. e.g., if c˜i,1(s, a) = 1 for each state-action pair and constraint i ∈ [m], then clearly the feasible set is empty if d > 1.

6

CONSTRAINED UPPER CONFIDENCE REINFORCEMENT LEARNING

Following [15], we deﬁne the set of plausible CMDPs by the conﬁdence intervals for the reward and each of the constraint costs. In particular, at episode k, let Mk be the set of plausible CMDPs with states and actions as in the underlying true CMDP M , deﬁne by all such CMDPs satisfying the following:

|rˆk(s, a) − r¯(s, a)| ≤ |cˆi,k(s, a) − c¯i(s, a)| ≤

log(S A(m+1)π2 t3k /3δ) 2 max{1,Nk(s,a)}
log(S A(m+1)π2 t3k /3δ) 2 max{1,Nk(s,a)}

1/2, 1/2, i ∈ [m]

(9) (10)

for all state-action pairs (s, a) ∈ S × A. Let M be the set of plausible for all episodes k.

Lemma 2 For any ﬁxed k ≥ 1, the probability that the true CMDP M is not contained in the set of plausible CMDPs Mk at episode k is at most 6δ/(π2t2k). Furthermore, with probability at least 1 − δ, for every state-action pair (s, a), cost ci and episode k, C-UCRL satisﬁes the following:

|rˆk(s, a) − r¯(s, a)| ≤ |cˆi,k(s, a) − c¯i(s, a)| ≤

log(S A(m+1)π2 t3k /3δ) 2 max{1,Nk(s,a)}
log(S A(m+1)π2 t3k /3δ) 2 max{1,Nk(s,a)}

1/2,
1/2

(11) (12)

Hence, the probability that the true CMDP M is not in the set of all plausible CMDPs for any episode k is at most δ—that is, Pr{M ∈/ M} ≤ δ.

Proof Consider any ﬁxed state-action pair (s, a) and its visitation frequency Nk(s, a) up to episode k. If the state-action pair (s, a) has not been visited, then (11) and (12) trivially hold since Nk(s, a) = 0 by deﬁnition and the right-hand sides of (11) and (12) are greater than one when Nk(s, a) = 0.
On the other hand, if Nk(s, a) is not zero, meaning the state-action pair has been visited, then since for each (s, a) pair, the reward and constraint costs are all supported on [0, 1] and independent identically
distributed (iid) real-valued random variables, we can apply Hoeffding’s inequality to get a bound on the deviation between the true mean r¯(s, a) (respectively, c¯i(s, a)) and the empirical mean rˆk(s, a) (respectively, cˆi,k(s, a)) given n iid samples of the state-action pair (s, a):

Pr {|rˆk(s, a) − r¯(s, a)| ≥ } ≤ 2 exp(−2n 2)

(13)

Consider

= 21n log SA(m+3δ1)π2t3k

1/2
,

then

Pr |rˆk(s, a) − r¯(s, a)| ≥ 21n log SA(m+3δ1)π2t3k

1/2 ≤ 2 exp −2n 1 log SA(m+1)π2t3k

2n

3δ

= SA(m+6δ1)π2t3k

Similarly, for each state-action pair (s, a) and constraint cost indexed by i,

Pr |cˆi,k(s, a) − c¯i(s, a)| ≥ 21n log SA(m+3δ1)π2t3k

1/2
≤ SA(m+6δ1)π2t3k .

(14)

Noting that from the above argument, the conﬁdence intervals hold with probability one when (s, a) has not be visited, taking a union bound over all possible values of n ∈ {1, . . . , tk} gives

Pr

∪tnk=1

|rˆk(s, a) − r¯(s, a)| ≥

log(SA(m+1)π2t3k/3δ) 1/2 2 max{1,N (s,a)}

k

≤

tk

6δ

n=1 SA(m+1)π2t3

=

6δ SA(m+1)π2t2

k

k

7

CONSTRAINED UPPER CONFIDENCE REINFORCEMENT LEARNING

and Pr

∪tnk=1

|cˆi,k(s, a) − c¯i(s, a)| ≥

log(SA(m+1)π2t3k/3δ) 1/2 2 max{1,Nk(s,a)}

≤

tk

6δ

n=1 SA(m+1)π2t3

=

m6δ SA(m+1)π2t2

k

k

where we have now written Nk(s, a) for the number of visits in (s, a) up to episode k. This proves (11) and (12).
Now, further union bounding over all state-action pairs (s, a) gives

Pr ∪tnk=1 ∪s,a |rˆk(s, a) − r¯(s, a)| ≥ r(n)

≤

tk n=1

6δ s,a SA(m+1)π2t3

=

6δ (m+1)π2t2

k

k

(15)

for the reward. Analogously, taking a further union bound over all state-action pairs (s, a) and all constraint costs i ∈ [m], gives

Pr ∪tnk=1 ∪s,a,i |cˆi,k(s, a) − c¯i(s, a)| ≥ r(n)

≤

tk n=1

6δ s,a,i SA(m+1)π2t3

=

m6δ (m+1)π2t2

k

k

(16)

for the constraint costs. Summing (15) and (16), we get the ﬁrst claim of the lemma—i.e.,

Pr{M ∈ Mk} ≤ π62δt2k

Now, since

∞1 =1 2

=

π62 , if in (15) and (16), we additionally union bounded over all episodes k

∈

{1, . . . , ∞}, we get that

Pr ∪∞ tk=1 ∪tnk=1 ∪s,a |rˆk(s, a) − r¯(s, a)| ≥ r(n)

≤

∞ tk =1

tk n=1

6δ s,a SA(m+1)π2t3

=

δ m+1

k

and

Pr ∪∞ tk=1 ∪tnk=1 ∪s,a,i |cˆi,k(s, a) − c¯i(s, a)| ≥ r(n)

≤

∞ tk =1

tk n=1

6δ s,a,i SA(m+1)π2t3

=

mδ m+1

k

so that

Pr{M ∈ M} ≤ δ

which proves the ﬁnal statement in the lemma.

Given that, for each episode, we can bound the gaps between the estimated reward (respectively, costs) and the mean reward (respectively, mean costs), with probability 1 − δ, we can provide an assurance on C-UCRL being δ-safe.
Theorem 3 C-UCRL is δ-safe.
Proof According to Lemma 2, with probability at least 1 − δ, c¯i(s, a) ≤ c˜i,k(s, a). The occupation measure y˜k obtained at each episode via (RLP) satisﬁes s,a c˜i,k(s, a)y˜k(s, a) ≤ di. Hence, Ci(π˜k) =
s,a c¯i(s, a)y˜k(s, a) ≤ di with probability 1 − δ.

8

CONSTRAINED UPPER CONFIDENCE REINFORCEMENT LEARNING

4.2. Regret Analysis of C-UCRL
Given that we have shown that C-UCRL is δ-safe, we now analyze the reward regret. In episode k of C-UCRL, we execute a baseline policy π0 for h times and policy π˜k for (k − 1)h times. The pseudo-regret of episode k is given by

∆k = h[J(π¯) − J(π0)] + (k − 1)h[J(π¯) − J(π˜k)] = hr¯ (y¯ − y0) + (k − 1)hr¯ (y¯ − y˜k).

We ﬁrst upper bound the per-step pseudo-regret of executing policy π˜k, r¯ (y¯ − y˜k), where the ﬁrst term is the expected average reward under the optimal policy π¯ and the second term is the sub-optimal expected average reward under policy π˜k.
Using the conﬁdence bounds in Lemma 2, deﬁne

r(s, a) = log2(SmAa(xm{1+,N1)kπ(s2,ta3k)/}3δ) 1/2,

(17)

and c(s, a) = r(s, a) for each state-action pair and let r and c denote the vectors containing the values across all state-action pairs5. Deﬁne the following two linear programs:

maxy{r y|Ay = 0, 1 y = 1, y ≥ 0, c y ≤ d}

(18)

maxy{(r + r) y|Ay = 0, 1 y = 1, y ≥ 0, (c + c) y ≤ d}.

(19)

where 0 ≤ r ≤ 1, 0 ≤ c ≤ 1, r ≥ 0, and c ≥ 0 hold element wise.

Lemma 4 Assuming the domains of (18) and (19) are not empty, let y1 and y2 be solutions for each of the

problems, respectively. If, for some constant α > 0 and β > 0, there exist y0 ∈ {y|Ay = 0, 1 y = 1, y ≥

0, (c + c) y ≤ d} such that r (y1 − y0) = α > 0 and c (y1 − y0) = β > 0, then r (y1 − y2) ≤

2α β

c 1+

r 1.

Proof Let

y3 = arg max{r y|Ay = 0, 1 y = 1, y ≥ 0, (c + c) y ≤ d}.
y

We ﬁrst ﬁnd the upper bound of r (y1 − y3) where we note that y3 and y1 are the solutions of same linear program over different domains. Since the domain of y3 is smaller than y1, we know that r (y1 − y3) ≥ 0. First, consider the trivial case that y1 satisﬁes (c + c) y1 ≤ d. In this case, y1 = y3 and r (y1 − y3) = 0. Now we only consider the case such that (c + c) y1 > d. Note that (c + c) y0 ≤ d. Hence, there exists a γ ∈ [0, 1) such that y4 = y0 + γ(y1 − y0) and (c + c) y4 = d—i.e., γ = (d − (c + c) y0)/((c + c) (y1 − y0)). Further, we have

y1 − y4 = y1 − y0 − γ(y1 − y0) = (1 − γ)(y1 − y0),

so that c (y1 − y4) = (1 − γ)β > 0 and

c (y1 − y4) = (c + c) (y1 − y4) − c (y1 − y4)

(20)

= c y1 + c y1 − d − c (y1 − y4)

(21)

≤ d − d + c y1 − c (y1 − y4)

(22)

≤ c 1 y1 ∞ + c 1 y1 − y4 ∞

(23)

=2 c 1

(24)

5. We note that it is possibel to deﬁne separate conﬁdence bounds for the reward and constraint costs, however, for simplicity of the statement and proof of Lemma 2, we deﬁne them to be the same.

9

CONSTRAINED UPPER CONFIDENCE REINFORCEMENT LEARNING

Combining this bound with r (y1 − y4) = r (y1 − y0) = α , (25) c (y1 − y4) c (y1 − y0) β

we have that

0<r

(y1

−

y4)

≤

2

α β

c 1.

Since the domain for each of these problems is convex, we know that

y4 ∈ {y|Ay = 0, 1 y = 1, y ≥ 0, (c + c) y ≤ d}.

Due to optimality, r y3 ≥ r y4 so that

r

(y1

−

y3)

≤

2

α β

c 1.

We leverage the bounud on r (y1 − y3) to obtain a bound on r (y3 − y2). Note that y3 and y2 are the

solutions of two linear programs with different objectives but the same domain. According to optimality of

the solutions, we know that r y3 ≥ r y2 and (r + r) y2 ≥ (r + r) y3. Combining these facts, we have

that

0 ≤ r (y3 − y2) ≤ r (y2 − y3) ≤ r 1 y2 − y3 ∞ ≤ r 1

(26)

Now, combining the bounds on r (y3 − y2) and r (y1 − y3), we have that

r

(y1 − y2) = r

(y1 − y3) + r

(y3

−

y2)

≤

2α β

c 1+

r 1.

(27)

We can use the preceding lemma to get a bound on the pseudo-regret.

Proposition 5 Denote Y = {y|(Io − P )y = 0, 1 y = 1, y ≥ 0}. If there exists y0 ∈ Y such that r¯ (y¯ − y0) = α > 0, c¯ (y¯ − y0) = β > 0, then with probability at least 1 − δ,

r¯ (y¯ − y˜k) ≤ 2( 2αβm + 1) s,a log2(SmAa(xm{1+,N1)kπ(s2,ta3k)/}3δ) 1/2.

(28)

Proof By deﬁnition

y¯ = arg max{r¯ y|(Io − P )y = 0, 1 y = 1, y ≥ 0, c¯i y ≤ di, i ∈ [m]}
y

and y˜k = arg max{r˜k y|(Io − P )y = 0, 1 y = 1, y ≥ 0, c˜i,ky ≤ di, i ∈ [m]}.
y

Deﬁne a sequence of subproblems by adding the conﬁdence value to one additional constraint at a time as follows:

y(1) = arg max{r¯
y
y(2) = arg max{r¯
y
... y(m) = arg max{r¯
y

y|(Io − P )y = 0, 1 y|(Io − P )y = 0, 1
y|(Io − P )y = 0, 1

y = 1, y ≥ 0, c˜1,ky ≤ d1, c¯i y ≤ di, i ∈ {2, . . . , m}} y = 1, y ≥ 0, c˜1,ky ≤ d1, c˜2,ky ≤ d2, c¯i y ≤ di, i ∈ {3, . . . , m}}
y = 1, y ≥ 0, c˜i,ky ≤ di, i ∈ {1, . . . , m}}

10

CONSTRAINED UPPER CONFIDENCE REINFORCEMENT LEARNING

Using the same proof technique as for that of Lemma 4, we obtain the bounds for each of the subprob-

lems

r¯ (y¯ − y(1)), r¯ (y(1) − y(2)), . . . , r¯ (y(m−1) − y(m)), r¯ (y(m) − y˜k).

Combining each of the bounds and the fact that

|r˜k(s, a) − r¯(s, a)| ≤ 2 log2(SmAa(xm{1+,N1)kπ(s2,ta3k)/}3δ) ,

and

|c˜i,k(s, a) − c¯i(s, a)| ≤ 2

log(SA(m+1)π2t3k/3δ) ,
2 max{1,Nk(s,a)}

we have that

r¯

(y¯ − y˜k) = r¯

(y¯ − y(1)) + · · · + r¯

(y(m)

−

y˜k)

≤

m

2α β

c˜k − c¯ 1 +

r˜k − r¯ 1

≤ 2( 2αβm + 1) s,a log2(SmAa(xm{1+,N1)kπ(s2,ta3k)/}3δ)

which completes the proof.

Note that according to Proposition 5, with probability at least 1 − δ, the per-step pseudo-regret of executing policy π˜k depends on the conﬁdence intervals of reward and costs of all state-action pairs. This is intuitive since in order for the policy π˜k to be close to the optimal policy π¯, we need to have good approximations of the reward and costs for all state-action pairs. To ensure this, we need to constantly explore the CMDP so that Nk(s, a) is not ‘too small’ for any state-action pair. Since the Markov chain resulting from the baseline policy is irreducible and aperiodic, the steady state occupation measure y0(s, a) corresponding to the baseline policy π0(a|s) has the property that y0(s, a) > 0, ∀s, a. Due to this universal exploration demand, we execute the baseline policy π0 for a constant number of times in each linear increasing episode in the C-UCRL algorithm.
To have a upper bound on the regret derived in Proposition 5, we need to have a lower bounds on Nk(s, a). Given our assumptions on the baseline policy as discussed above, deﬁne ρ > 0 such that y0(s, a) ≥ ρ > 0 for all state-action pairs (s, a) ∈ S × A. The following lemma gives a lower bound on the number of times each state-action pair is visited in episode k.

Lemma 6 Given a ﬁxed total number of episodes K, with probability at least 1 − δ, for every state-action pair (s, a) and episode k ∈ [K],

Nk(s, a) ≥ (k − 1)ρh − (k − 1) 72ξρh log

ϕ·SAK δ

1/2

(29)

where ξ the mixing time of the Markov chain induced by policy π0, ρ > 0 is such that y0(s, a) ≥ ρ > 0 for all state-action pairs (s, a) ∈ S × A, and ϕ = s,a yy0((ss,a,a))2 , where y is the initial state action distribution and yo is the steady state action distribution under the baseline policy.

Proof Consider the exploration phase (when the baseline policy π0 is executed) of the k-th episode in

Algorithm 1. For a given episode and for a ﬁxed state-action pair (s, a), let X ,1, . . . , X ,h be the indicator

variables of whether state-action pair (s, a) has be selected at each step within the episode . Let Y =

h i=1

X

,i

and

thus

E[Y

]

=

y0(s, a)h.

Applying

the

Chernoff-Hoeffding

bound

in

[10,

Theorem

3],

gives

Pr{E[Y ] − Y ≥ y0(s, a)h} ≤ ϕ · exp − 2y07(2sξ,a)h . (30)

11

CONSTRAINED UPPER CONFIDENCE REINFORCEMENT LEARNING

Setting = y0(7s2,ξa)h log( ϕSδAK ), (31)
the above bound becomes

Pr Y ≤ y0(s, a)h −

72ξ

y0

(

s,

a)

h

log

(

ϕSAK δ

)

≤ SAδK

(32)

Using the assumption that y0(s, a) ≥ ρ > 0, ∀(s, a), the union bound over all state-action pairs (s, a) and episodes k ∈ [K] is given by

Pr

(s,a), Y ≤ ρh − 72ξρh log( ϕSδAK )

≤

K =1

s,a

δ SAK

=δ

(33)

Now, we note that

k−1 =1

Y

≤ (k − 1)

ρh −

72ξρh log( ϕSδAK )

⊂

k−1 =1

Y ≤ ρh −

72ξρh log( ϕSδAK )

and Nk(s, a) ≥

k−1 =1

Y

since in each episode π˜

is executed h

times after the baseline policy so that

N (s, a) may be larger. Hence,

Nk(s, a) ≥ (k − 1)ρh − (k − 1) 72ξρh log( ϕSδAK )

(34)

holds with probability at least 1 − δ.

Combining Proposition 5 and Lemma 6 and summing over K episodes, we obtain the total regret bound for C-UCRL.
Theorem 7 Suppose that δ ≤ ϕSAK exp(− 2ρ8h8ξ ). Under the assumptions of Proposition 5, with probabil-
3
ity at least 1 − δ, C-UCRL has total pseudo-regret ∆(T ) = O(T 4 log(T /δ)).

Proof According to Proposition 5, the total regret of K episodes is

K k=1

∆k

=

K k=1

hr¯

(y¯ − y0) + (k − 1)hr¯

(y¯ − y˜k)

= hKr¯ (y¯ − y0) + h Kk=2(k − 1)r¯ (y¯ − y˜k)

≤ 2hK + 2( 2αβm + 1)h Kk=2(k − 1) s,a log2(SmAa(xm{1+,N1)kπ(s2,ta3k)/}3δ)

Let

ζ = ρh −

72ξρh log( ϕSAK )

1/2
.

δ

Since δ ≤ ϕSAK exp(− 2ρ8h8ξ ), we have that

ϕSAK 1/2 1

72ξρh log(

) ≥ ρh

δ

2

so that ζ ≤ 12 ρh.

12

CONSTRAINED UPPER CONFIDENCE REINFORCEMENT LEARNING

Combining this with Lemma 6, we have that

∆(T ) =

K k=1

∆k

≤

2hK

+

2( 2αβm

+

1)h

Kk=2(k − 1)

log(SA(m+1)π2t3k/3δ) 1/2 s,a 2 max{1,Nk(s,a)}

≤ 2hK + 2( 2αβm + 1)hSA Kk=2(k − 1) log(SA(2m(k+−11))πζ2t3k/3δ) 1/2

≤ 2hK + 2( 2αβm + 1)hSA = 2hK + 2( 2αβm + 1)hSA ≤ 2hK + 2( 2αβm + 1)hSA ≤ 2hK + 2( 2αβm + 1)hSA

log(SA(m + 1)π2T 3/3δ)

K −1 k=1

k 2ζ

log(SA(m+1)π2T 3/3δ) 2ζ

Kk=−11 √k

log(SA(m+1)π2T 3/3δ) ρh

Kk=−11 √k

log(SA(m+ρh1)π2T 3/3δ) (K − 1) K2 1/2

= O(K) + O(K K log(T /δ))
3
≤ O(T 4 log(T /δ))

where the second to last inequality follows from Jensen’s inequality and the ﬁnal step follows from T =

K k=1

kh

=

K(K2−1) h

so

that

K

<

(2T /h)1/2.

Remark. Adding constants related to the dimension of the CMDP, we have the regret bound

3

∆(T ) ≤ O(mSAT 4 log(mSAT /δ)).

(35)

4.3. Specializing to the Constrained Multi-Armed Bandit Setting

Constrained Multi-Armed Bandits (CMABs) can be viewed as a special case of CMDPs, where there is only

one state, S = 1 and the transition kernel is trivially staying in that state with all actions. The policy in a

CMAB is a probabilistic distribution over actions/arms y(a) and the goal is to solve the following linear

program:

max{r¯ y | 1 y = 1, y ≥ 0, c¯ y ≤ d}.

(36)

y

Similarly, the per-step pseudo-regret is deﬁned as r¯ (y¯ − y˜k) where y¯ is the optimal randomized policy and y˜k is the policy execute in episode k of C-UCRL. Running C-UCRL with S = 1, the following corollaries hold.

Corollary 8 In CMABs, C-UCRL is δ-safe.

Corollary 9 In CMABs, Under the assumptions of Proposition 5, with probability at least 1 − δ, C-UCRL
3
has total pseudo-regret ∆(T ) = O(T 4 log(T /δ)).

The proofs of the above two corollaries follow directly from the corresponding results in the preceding section.

5. Experiments
The goal of this section is to explore a few illustrative examples which highlight different features of our approach.

13

CONSTRAINED UPPER CONFIDENCE REINFORCEMENT LEARNING

Mean reward and cost Average pulling of arm 1
Cumulative Regret

1.0

Constraint

0.8

reward

0.6

cost

0.4

0.2

0.0

Arm 1

Arm2

(a)

0.7 0.6 0.5 0.4 0.3 0.2 0.1 0.0
0

50000 100000 150000 200000
Time Step (b)

8000 6000 4000 2000
0 0

50000 100000 150000 200000
Time Step (c)

Figure 1: Two armed bandit with per-round budget constraint: (a) mean reward and cost of each arm as well as the per-round constraint; (b) average number of times arm one is pulled; (c) the cumulative regret of C-UCRL.

5.1. Two Armed Bandit with per Round Budget Constraints
We ﬁrst consider a simple two arms bandit example. As stated before, the CMDP reduces to a constrained multi-armed bandit problem when |S| = 1. The reward and cost of each arm are unknown and stochastic. In our simulation, the reward and cost is draw from a binomial distribution, with the mean shown in Figure 1(a). Even though arm one has a better reward, we cannot pull arm one all the time since the constraint is set to be less than the mean cost of arm one. The optimal policy is to pull arm one with probability 0.75 and arm two with probability 0.25. The baseline policy we use to start exploration is pulling the two arms uniformly at random. Figure 1(b) and 1(c) show the average number of times arm one is pulled and the cumulative regret of C-UCRL, respectively. The average pull count of arm one never exceeds 0.75.
5.2. Three State CMDP
To demonstrate the performance of C-UCRL, we consider a simple three state CMDP. As show in Figure 2(a), the CMDP we consider has three states and two actions. An agent can take either a risky exploratory action in which the navigate to another state or they can take the safe action and remain in the current state. There is no reward or cost for staying in the current state but there will be a stochastic reward and cost if the agent navigates. In the simulation, the reward and cost of each state-action pair are each draw from a binomial distribution, with the means deﬁned in the labels on edges in Figure 2(a). Obviously, without this constraint, the optimal policy is to navigate in each of the states. In this problem, we consider the constraint that in expectation, the average cost should be less than 0.2. This constraint prevents the agents from continuously navigating between the three states. In particular, as shown in Figure 2(b), the constrained optimal policy is a randomized policy that has positive probability on the safe action in each state. The relatively conservative baseline policy we use in C-UCRL for exploration is staying in the current state with probability 0.8 and navigate to the next state with probability 0.2.
We compare our approach with the UCRL2 algorithm. However, UCRL2 does not allow for constraints or multiple reward/cost criteria. Hence, we leverage the idea of risk sensitive reinforcement learning [18], where we treat a linear combination of reward and cost—i.e., r−λc—as the reward for the UCRL2 algorithm (Algorithm 2). The hyperparameter λ represents the trade off between the reward and cost, the combination of which represents the reward in the classical implementation of UCRL2; we refer to risk-sensitive UCRL2 by RS-UCRL2. Figure 2(c) shows the constraint violation probability in 30 training episodes by RS-UCRL2 algorithm with different λ. Figure 3(a) shows the cumulative regret and average cost of the
14

CONSTRAINED UPPER CONFIDENCE REINFORCEMENT LEARNING

Algorithm 2 risk-sensitive UCRL2 (RS-UCRL2) algorithm

Input: safety parameter δ ∈ (0, 1), baseline policy π0(a|s), episode length h, risk sensitive parameter λ.

Initialization: set t = 1, observe the initial state s1

for episodes k = 1, 2, . . . , K do

tk = t ;

// initialize start time of episode k

while t ≤ tk + h;

// Execute baseline policy h times for exploration

do

Draw action at ∼ π0(·|st)

Observe reward rt, costs ci,t, and the next state st+1

t←t+1

end

Nk(s, a) = Rk(s, a) =

t t

=1

1(st

= a, at

= a),

∀(s, a) ∈ S × A ;

t t

=1(rt

−λ

ct )1(st

= a, at

= a);

// set the state-action count // cumulative reward cost trade-off

rˆk(s, a) = maxR{1k,(Nsk,a()s,a)} , r˜k(s, a) = rˆk(s, a) + 27mlaoxg{(21S,NAkt(ks/,δa))} ;

// compute estimates

y˜k ← arg max{r˜k y|Ioy = P y, 1 y = 1, y ≥ 0} π˜k ← y˜k(s, a)/( a∈A y˜k(s, a)) ; while t ≤ tk + kh ; do
Draw action at ∼ π˜k(·|st)
Observe reward rt, costs ci,t, and the next state st+1
t←t+1 end

// recover policy // Execute π˜k policy (k − 1)h times

end

r¯ = 1.0 c¯ = 0.6

r¯ = 0 c¯ = 0
r¯ = 0.3 c¯ = 0.1

r¯ = 0 c¯ = 0

r¯ = 0.5 c¯ = 0.2

(a)

r¯ = 0 c¯ = 0

Optimal policy Probability of constraint violation

1.0

1.0

stay

0.8

0.8

navigate

0.6

0.6

0.4

0.4

0.2

0.2

0.0 State 1 State 2 State 3 without constraint

0.0 State 1 State 2 State 3 with constraint

(b)

1.0 0.8 0.6 0.4 0.2 0.0
1.6 1.8 2.0 2.2 2.4
(c)

Figure 2: Simple CMDP. (a) CMDP structure; (b) optimal policy computed with the true mean reward and mean cost, with and without the constraint on cost, d = 0.2; (c) probability of constraint violation in 30 training episodes by risk-sensitive UCRL2 (RS-UCRL2).

C-UCRL and RS-UCRL2 algorithms. As we can see, when the cost value is underestimated (λ = 1.9), applying RS-UCRL2 directly leads to a ‘good’ reward (i.e., the regret is negative as it gets more reward than the optimal randomized policy), yet the constraints are violated. On the other hand, when the costs are overestimated (λ = 2.1), RS-UCRL2 is too conservative about the cost and, thus, receives high regret. We can observe that C-UCRL does not violate the constraint during learning though in this experiment, δ is set to be 0.1, meaning that with probability at least 0.9, the constraint will not be violated in all episodes.

15

CONSTRAINED UPPER CONFIDENCE REINFORCEMENT LEARNING

Policy Cumulative Regret Average Cost

120000 100000 80000 60000 40000 20000
0 20000 40000
0

100000 200000 300000
Time Step

0.25 0.20 0.15 0.10 0.05
0 100000 200000 300000
Time Step (a)

C-UCRL= 1.9
UCRL
UCRL = 2.0 UCRL = 2.1
Constraint

1.0

0.8

0.6

0.4

0.2

0.0

State 1 UC

RSLtate=2

1S.9tate

3

1.0

0.8

0.6

0.4

0.2

0.0

State 1 UC

Stat RL

e =2

2S.1ta

te

3

(b)

1.0 0.8 0.6 0.4 0.2
0.0 State 1 State 2 State 3 C-UCRL

stay navigate

Figure 3: C-UCRL vs. RS-UCRL2: (a) Cumulative regret and average cost for C-UCRL and risk sensitive UCRL2; (b) Policy learned by C-UCRL and RS-UCRL2.

The fundamental problem with RS-UCRL2 is that with only one criterion, the policy it learns will always be a deterministic policy, while in this CMDP, the optimal policy is randomized. Figure 3(b) shows the policy learned by C-UCRL and RS-UCRL2. When λ = 1.9, RS-UCRL2 learn the optimal policy as there is no constraint, which leads to constraint violation. When λ = 2.1, the policy learned by RS-UCRL2 is to stay in one state forever. On the contrary, the policy learned by C-UCRL algorithm converges to the optimal randomized policy.
5.3. Grid World with Safety Constraints
Motivated by the goal of ensuring safety in reinforcement learning safety, we validate our algorithms using a 2D grid-world exploration problem [18, 2.24]. This example also represents a crude abstraction of rovers exploring the surface of Mars as described in [27].
Figure 4(a) shows the CMDP structure. The green color in each state represents the mean cost of that state, and the darker the color, the higher the cost is. In the Mars exploration problem, those darker states are the states with large slope that the agents want to avoid. The constraint we enforce is the upper bound of the per-step probability of step into those state with large slope—i.e., the more risky or potentially unsafe states to explore. The agent starts from the origin state ‘O’ and receives reward 1 if it reaches the destination state ‘D’ after which it returns to the origin. In the simulation, the cost of each state is draw from a binomial distribution, with the mean shown in the ﬁgure. At each time step, the agent can take action to move into any of its four neighboring states. Due to the stochastic environment, transitions are stochastic (i.e., even if the agents action is to go “North, the environment can send the vehicle with a small probability to “East).
Without safety constraints, the optimal policy is obviously to always choose the orange route in Figure 4(a). However, with constraints, as we can see in Figure 4(b), the optimal policy is a randomized policy that use both blue and orange routes with some probabilities. The relatively conservative baseline policy we
16

CONSTRAINED UPPER CONFIDENCE REINFORCEMENT LEARNING

Policy at initial state

1.0

left

D

0.8

up

0.6

0.4

O

0.2

0.0 optimal C-UCRL UCRL = 7.5 UCRL = 7.0

(a)

(b)

Figure 4: Grid World with Safety Constraints. (a) Grid world structure: the states with darker green color have larger mean cost, and ‘O’ and ‘D’ are the origin and destination states, respectively; (b) Policy learned by different algorithms: the blue column represents the probability of going ‘West’ (choose blue route) and orange column represents the probability of going ‘North’ (choose orange route).

Cumulative Regret Average Cost

400000 300000 200000 100000
0 0 1000000 2000000 3000000
Time Step

0.35 0.30 0.25 0.20 0.15 0.10 0.05 0.00
0

1000000 2000000 3000000
Time Step

C-UCRL= 7.0
UCRL
UCRL = 7.5
Constraint

Figure 5: Cumulative regret and average cost of C-UCRL and RS-UCRL2.

use in C-UCRL for exploration is choose both routes uniformly at random. Figure 5 show the cumulative regret and average cost of the C-UCRL and RS-UCRL2 algorithm and Figure 4(b) shows the policy learned by them. As we can see, RS-UCRL2 either learns to only choose orange or blue route respectively, causing either constraint violation or large reward regret, while C-UCRL converges to the optimal policy.
Figure 6(a) shows the structure of another larger scale safety grid world example. The green states in the ﬁgure have mean cost 1 and the others have zero cost. The blue state is the origin state and the red state is the destination state, which has reward 1. Figure 6(b) shows the cumulative regret and average cost of the C-UCRL algorithm and RS-UCRL2 algorithm. The RS-UCRL2 algorithm is able to learn a policy that does not violate the constraint if we choose a conservative λ, however, with much larger reward regret as compared to C-UCRL.
6. Conclusion
We formulate the problem of safe reinforcement learning when the transition kernel is known but the reward and constraint costs are unknown a priori as a CMDP and propose a C-UCRL algorithm to learn the optimal policy. Theoretically, we show that C-UCRL algorithm is guaranteed to satisfy the constraints during learn-
3
ing with probability at least 1 − δ and achieves O(T 4 log(T /δ)) reward regret. Empirically, we provide examples which demonstrate two key properties relative to comparable algorithms: 1) C-UCRL is able to

17

CONSTRAINED UPPER CONFIDENCE REINFORCEMENT LEARNING

Cumulative Regret Average Cost

350000 300000 250000 200000 150000 100000 50000
0 0

1234
Time Step 1e6

0.30 0.25 0.20 0.15 0.10 0.05
0

1234
Time Step 1e6

C-UCRL
UCRL = 0.3
Constraint

(a)

(b)

Figure 6: Grid World with Safety Constraints. (a) CMDP grid world structure: the states with green color have mean cost equals to 1 and others have no cost; the blue state is the origin state and the red state is the destination state. (b) Cumulative regret and average reward of C-UCRL and RS-UCRL2.

learn the optimal policy which in general is a randomized policy as opposed to a deterministic policy, and 2) C-UCRL has high-probability guarantees on remaining safe while learning.
Let us comment brieﬂy on some of the limitations of our approach and avenues for future research. First, we remark that artful selection of the baseline policy π0 and the duration h for executing it in each episode is required. We choose h based on the mixing time of the Markov chain induced by π0. The choice of these two facets is really central to the algorithm as it deﬁnes the exploration phase and hence, the robust linear program that we solve for ﬁnding π˜k. The baseline and duration need to be chosen such that in each episode the linear program has a non-trivial feasible set. Our results are predicated on this being case; as noted in Section 3, in practice, however, it may not be. To handle this, we suggest the heuristic of executing the baseline policy in episode k = 1 until c˜1 y0 ≤ d. A better understanding of how to ensure that in each episode the feasible set remains non-trivial is an avenue of future work.
We note also that it is likely that C-UCRL has a much worse sample complexity as compared to approaches which do not impose any criteria on safe learning during the exploration period. Better understanding of this trafeoff is an avenue for future work. Furthermore, our approach requires knowledge of the transition kernel. It is not immediately obvious how to extend classical approaches such as UCRL2, without further exacerbating sample complexity issues, due to the fact that central proof technique we employ is the robust linear programming formulation in each episode which is used to obtain a policy based on the conﬁdence bounds. Alternative approaches may be better suited if the transition kernel is unknown. Another interesting direction that arose in our study of CMDPs is that there is potential to extend the theoretical results of UCRL2 to RS-UCRL2 through a primal-dual lens for capturing the hyper-parameter λ; investigating this direction may lead to an alternative for addressing the unknown transition kernel setting, however, the issue of the optimal policy being non-deterministic for the true underlying CMDP and the fact that UCRL2 seeks out deterministic policies remains.
References
[1] Joshua Achiam, David Held, Aviv Tamar, and Pieter Abbeel. Constrained policy optimization. arXiv preprint arXiv:1705.10528, 2017.
[2] Eitan Altman. Constrained Markov decision processes, volume 7. CRC Press, 1999.

18

CONSTRAINED UPPER CONFIDENCE REINFORCEMENT LEARNING
[3] Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Mane´. Concrete problems in ai safety. arXiv preprint arXiv:1606.06565, 2016.
[4] Felix Berkenkamp, Matteo Turchetta, Angela Schoellig, and Andreas Krause. Safe model-based reinforcement learning with stability guarantees. In Advances in Neural Information Processing Systems, pages 908–918, 2017.
[5] Shalabh Bhatnagar and K Lakshmanan. An online actor–critic algorithm with function approximation for constrained markov decision processes. Journal of Optimization Theory and Applications, 153(3): 688–708, 2012.
[6] Shalabh Bhatnagar, Richard S Sutton, Mohammad Ghavamzadeh, and Mark Lee. Natural actor–critic algorithms. Automatica, 45(11):2471–2482, 2009.
[7] Richard Cheng, Ga´bor Orosz, Richard M Murray, and Joel W Burdick. End-to-end safe reinforcement learning through barrier functions for safety-critical continuous control tasks. arXiv preprint arXiv:1903.08792, 2019.
[8] Yinlam Chow, Mohammad Ghavamzadeh, Lucas Janson, and Marco Pavone. Risk-constrained reinforcement learning with percentile risk criteria. Journal of Machine Learning Research, 18(167):1–51, 2018.
[9] Yinlam Chow, Oﬁr Nachum, Edgar Duenez-Guzman, and Mohammad Ghavamzadeh. A lyapunovbased approach to safe reinforcement learning. arXiv preprint arXiv:1805.07708, 2018.
[10] Kai-Min Chung, Henry Lam, Zhenming Liu, and Michael Mitzenmacher. Chernoff-hoeffding bounds for markov chains: Generalized and simpliﬁed. arXiv preprint arXiv:1201.0559, 2012.
[11] Stefano P Coraluppi and Steven I Marcus. Risk-sensitive and minimax control of discrete-time, ﬁnitestate markov decision processes. Automatica, 35(2):301–309, 1999.
[12] Wenkui Ding, Tao Qin, Xu-Dong Zhang, and Tie-Yan Liu. Multi-armed bandit with budget constraint and variable costs. In Twenty-Seventh AAAI Conference on Artiﬁcial Intelligence, 2013.
[13] Mahmoud El Chamie, Yue Yu, Behc¸et Ac¸ıkmes¸e, and Masahiro Ono. Controlled markov processes with safety state constraints. IEEE Transactions on Automatic Control, 64(3):1003–1018, 2019.
[14] Javier Garcıa and Fernando Ferna´ndez. A comprehensive survey on safe reinforcement learning. Journal of Machine Learning Research, 16(1):1437–1480, 2015.
[15] Thomas Jaksch, Ronald Ortner, and Peter Auer. Near-optimal regret bounds for reinforcement learning. Journal of Machine Learning Research, 11(Apr):1563–1600, 2010.
[16] Matthew Joseph, Michael Kearns, Jamie H Morgenstern, and Aaron Roth. Fairness in learning: Classic and contextual bandits. In Advances in Neural Information Processing Systems, pages 325–333, 2016.
[17] Torsten Koller, Felix Berkenkamp, Matteo Turchetta, and Andreas Krause. Learning-based model predictive control for safe exploration. In 2018 IEEE Conference on Decision and Control (CDC), pages 6059–6066. IEEE, 2018.
[18] Jan Leike, Miljan Martic, Victoria Krakovna, Pedro A Ortega, Tom Everitt, Andrew Lefrancq, Laurent Orseau, and Shane Legg. Ai safety gridworlds. arXiv preprint arXiv:1711.09883, 2017.
19

CONSTRAINED UPPER CONFIDENCE REINFORCEMENT LEARNING
[19] Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel. End-to-end training of deep visuomotor policies. The Journal of Machine Learning Research, 17(1):1334–1373, 2016.
[20] Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971, 2015.
[21] David G Luenberger, Yinyu Ye, et al. Linear and nonlinear programming, volume 2. Springer, 1984. [22] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare,
Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529, 2015. [23] Teodor Mihai Moldovan and Pieter Abbeel. Safe exploration in markov decision processes. arXiv preprint arXiv:1205.4810, 2012. [24] Martin L Puterman. Markov decision processes: discrete stochastic dynamic programming. John Wiley & Sons, 2014. [25] Ahmad EL Sallab, Mohammed Abdou, Etienne Perot, and Senthil Yogamani. Deep reinforcement learning framework for autonomous driving. Electronic Imaging, 2017(19):70–76, 2017. [26] Guy Shani, David Heckerman, and Ronen I Brafman. An mdp-based recommender system. Journal of Machine Learning Research, 6(Sep):1265–1295, 2005. [27] Akifumi Wachi, Yanan Sui, Yisong Yue, and Masahiro Ono. Safe exploration and optimization of constrained mdps using gaussian processes. In Thirty-Second AAAI Conference on Artiﬁcial Intelligence, 2018. [28] Datong P Zhou and Claire J Tomlin. Budget-constrained multi-armed bandits with multiple plays. In Thirty-Second AAAI Conference on Artiﬁcial Intelligence, 2018.
20

