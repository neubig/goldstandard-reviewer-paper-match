The Trade-offs of Domain Adaptation for Neural Language Models

David Grangier Google, Mountain View, CA grangier@google.com

Dan Iter∗ Stanford, Palo Alto, CA daniter@stanford.edu

arXiv:2109.10274v2 [cs.CL] 21 Mar 2022

Abstract
This work connects language model adaptation with concepts of machine learning theory. We consider a training setup with a large outof-domain set and a small in-domain set. We derive how the beneﬁt of training a model on either set depends on the size of the sets and the distance between their underlying distributions. We analyze how out-of-domain pretraining before in-domain ﬁne-tuning achieves better generalization than either solution independently. Finally, we present how adaptation techniques based on data selection, such as importance sampling, intelligent data selection and inﬂuence functions, can be presented in a common framework which highlights their similarity and also their subtle differences.
1 Introduction
Neural Language Models (LMs) trained on large generic training sets – over a billion sentences (Kaplan et al., 2020; Roziewski and Kozłowski, 2021) – have been shown to be effective at adapting to smaller, speciﬁc target domains for language modeling and other downstream tasks (Bommasani et al., 2021). Neural LM adaptation is commonly performed via ﬁne tuning (Devlin et al., 2018; Liu et al., 2019; Raffel et al., 2019; Radford et al., 2019), data selection (van der Wees et al., 2017) or their combination (Wang et al., 2018; Aharoni and Goldberg, 2020; Gururangan et al., 2020). However, the tradeoffs between ﬁne-tuning and reweighting of pretraining data is not well understood and a theoretical framework for reasoning about the generalization performance of these methods is needed.
In this paper, we connect language model adaptation with concepts of machine learning theory. Our derivations support past empirical observations: it has been observed that the size of the out-of-domain pre-training set is important for in
∗Work performed while interning at Google.

domain generalization (Raffel et al., 2019; Devlin et al., 2018) or that domain adaptation is more effective on domains which are well represented in the the pre-training data (Radford et al., 2019). Our study consider a training setup with a large out-of-domain set and a small in-domain set. As a ﬁrst contribution, we derive how the beneﬁt of training a model on either set depends on the size of the sets and the distance between their underlying distribution. We also expose how ﬁne-tuning can be viewed as a regularization method that can achieve a better trade-off than training only on either set.
The research on data selection for LM adaption originates mainly from intelligent selection (Moore and Lewis, 2010; Axelrod et al., 2011). This method examines the out-of-domain training data to emphasize a subset deemed more likely by an in-domain model than by an out-ofdomain model. Although intuitive, the connection of this method with statistical estimation is unclear, which makes studying its impact on generalization error difﬁcult. Another family of selection methods stems from inﬂuence functions (Koh and Liang, 2017; Wang et al., 2021) which estimate whether the model updates from out-of-domain training examples are aligned with the in-domain updates. This approach is more principled and its impact on the generalization error is easier to study. In this work, as a second contribution, we show how intelligent selection and inﬂuence function methods are linked in the case of neural LMs. In particular, we show that they both can be derived from importance sampling (Owen, 2013), a classical, well-studied statistical estimation technique.
The rest of our paper is organized as follows. We ﬁrst presents the theoretical trade-offs between in-domain and out-of-domain training. We highlight the importance of the relative sizes of indomain and out-of-domain training sets along with

the distance between their underlying distributions. We also present how ﬁne-tuning with a limited number of updates can be seen as a training method regularized with respect to the out-ofdomain prior. Finally, we present data selection methods under a unifying framework.

2 Neural Language Modeling

Language modeling refers to the generative modeling of natural language (Manning and Schutze, 1999). Commonly, natural language is represented as a sequence of symbols, tokens, from a ﬁnite vocabulary. For instance, language can be represented as a sequence of characters, a sequence of words or alternative units. A neural language model (LM) decomposes the estimates the log probability of a text y = (y1, . . . yn), as

n

log P (y; θ) =

log

P

(yi

|y

i− 1

1

;

θ

)

i=1

where Pθ maps a parameter vector θ along with a sequence of past tokens y1i−1 onto a probability distribution over the vocabulary. Different types of neural architectures have been used for neural language modeling. Most architectures used for LMs re-use intermediate computations from the previous steps for the next steps when estimating probabilities for successive tokens in the same sequence. Popular architectures include recurrent neural networks (Mikolov et al., 2010; Sundermeyer et al., 2012), convolutional networks (Dauphin et al., 2017) and transformer networks (Vaswani et al., 2017; Radford et al., 2019).
The parameter vector θ ∈ Θ of a neural LM is identiﬁed by maximizing the log likelihood over a training set D sampled from the true distribution D using variants of stochastic gradient descent. The log likelihood of a held-out set, sampled from the same distribution, can evaluate model quality. One often reports perplexity, the exponentiated negative average log likelihood per token.
Conditional LMs model the distribution of a text y given a conditioning input x.
n
log P (y|x; θ) = log P (yi|y1i−1, x; θ)
i=1

This type of model is used for translation where (x, y) pairs are sentences in the source and target language (Koehn, 2009; Bahdanau et al., 2015) or

summarization where (x, y) pairs are corresponding articles and summaries (See et al., 2017).
For both conditional and regular LMs, the size of the training data is important to achieve a low held-out perplexity. This is an obstacle for domains with limited available training data. This issue has led to various model adaptation approaches. These methods leverage large amounts of generic training data D along with a small amount of target domain training data T from the domain of interest. Fine tuning is a popular domain adaptation method which trains a neural language model in two phases, ﬁrst maximizing the likelihood of the generic set D (pre-training) before optimizing the likelihood of the target domain set T (ﬁne-tuning). As an alternative to ﬁne-tuning, some methods consider leveraging the small target-domain training set to identify and emphasize similar data in the larger generic training set. These emphasis methods can be used individually or in conjunction with ﬁne-tuning. Emphasis methods include importance sampling, contrastive data selection and inﬂuence functions. This paper shows that these methods – although proposed in different context – can be presented in a uniﬁed way which allows light to be cast on their subtle differences.

3 Training Strategies
This section ﬁrst examines in-domain training, i.e. when the training and test data are sampled from the same distribution. It then studies out-ofdomain training, i.e. when the training and test data distribution differs. Finally, it examines outof-domain pre-training followed by in-domain ﬁne tuning. For the three cases, we decompose the loss relying on classical concepts from learning theory and study the trade-offs involved in each setup.

3.1 In-Domain Training
Given a training set D sampled from a distribution D, learning an LM typically aims at minimizing the negative log-likelihood of D, also referred to as the cross-entropy loss i.e.

1

L(θ; D) = − |D|

log P (y|θ) = E [− log P (y|θ)].
y∼D

y∈D

This empirical risk is the average over the ﬁnite set D, which acts as a proxy for the expectation

over the true, unavailable distribution P (y|D),
L(θ; D) = − log P (y|θ)P (y|D)
y∈Ω
= E [− log P (y|θ)],
y∼D
where the distribution’s support Ω is the set of all ﬁnite sequences. The true expected loss is bounded by the entropy of the distribution P (·|D), i.e.
L(θ; D) ≥ LH (D) = H(P (·|D))
since H(P (·|D)) = minq Ey∼D[− log q(y)]. The gap between the best likelihood from a neural network with the chosen parameterization and the entropy is called the approximation error
Lapp(D, Θ) = min L(θ; D) − H(P (·|D)).
θ∈Θ
This gap accounts for the fact that P (·|D) generally cannot be represented by a parameterized function from the chosen family spanned by Θ. In addition to the approximation error, one should consider the estimation error to account that one relies on the empirical risk from the ﬁnite set D,
Lest(D, Θ, D) = L(θD; D) − min L(θ; D)
θ
with θD = arg minθ∈Θ L(θ; D). Therefore, the loss of θD over D decomposes as (Bottou and Bousquet, 2007)
L(θD; D) = LH (D) + Lapp(D, Θ) + Lest(D, Θ, D) (1)
where the three terms accounts for the intrinsic uncertainty of D, the chosen neural architecture and the ﬁnite training set D respectively.
The approximation error Lapp(D, Θ) depends on the selected model family Θ. It can be reduced by selecting a more expressive family, i.e. a neural architecture with more capacity, a larger Θ, e.g. architectures with more, wider layers. The estimation error Lest(D, Θ, D) depends both on the selected model family Θ and the size of the training data D. Increasing model capacity will result in a higher estimation error for the same training set size, but training over a larger training set will decrease estimation error. Therefore, for a given training set size, capacity needs to be chosen to identify a good trade-off between the two error types.

Two important properties of neural networks need to be kept in mind when examining this trade-off. The universal approximation property (Lecun, 1987; Funahashi, 1989) means that for any approximation error and any distribution D, there exists a capacity setting C( , D) at which a neural network θ ∈ C( , D) whose error is below , i.e.
∀ > 0, ∃ C s.t. Lapp(D, C) ≤ .
In layman terms, the universal approximation property means that for sufﬁciently large capacity settings, the approximation error can become arbitrary low. The statistical consistency property means that for any , > 0, there exist a training set size N ( , D) such that sampling a training set of size N ( , , D) from D will result in an estimation error less than with probability 1 − , ∀ , > 0, ∃ N s.t ,
P (D ∼ DN : Lest(D, Θ, D) < ) = 1 −
In layman terms, the statistical consistency property means that for sufﬁciently large training sets, the probability to get an estimation error below any positive value can be arbitrary close to 1.
Universal approximation and consistency implies that, in the asymptotic case (i.e. as the size of D tends to inﬁnity), the last two terms in Eq. 1 can be arbitrary close to zero with the appropriate model capacity (with high probability). In that case, the likelihood L(θD; D) amounts to the intrinsic entropy of D with the appropriate model capacity.
3.2 Out-of-Domain Training
This section considers a setup where one needs a specialized language model in a domain T and two training sets are available: a small training set T sampled from T and a large training set D sampled from D, a generic domain different from the specialized domain.
In that context, the simplest options are either to train a model over T or D alone. Training only on the small set T results in the generalization loss
L(θT ; T ) = LH (T ) + Lapp(T , Θ) + Lest(T , Θ, T )
with θT = arg minθ∈Θ L(θ; T ) as in the previous section. Training on the larger set D results in
L(θD; T ) = LH (T ) + Lapp(T , Θ) + Lest(T , Θ, D).

Two factors are important to compare these two options: the size of the specialized set T relative to the size of the generic set D and the similarity between T and D distributions.
When the T and D distributions are identical, D and T are sampled from the same distribution and training a model on the larger training set D is advantageous. For a constant capacity, this option will get a lower estimation error. When varying capacity, one might identify a setting with an even better trade-off in the compound loss of Eq. (1) with the larger training set D.
When the distributions T and D differ and the size of D is ﬁxed, the size of T determines which option to prefer. Statistical consistency means that Lest(T , Θ, T ) will converge to zero in probability as the size of T grows. This means that when the size of T is greater than N ( , Lest(T , Θ, D), D), the probability that training on T results in a better generalization loss than training on D is above 1− .
When the distributions T and D differ, the Kullback–Leibler (KL) divergence between the two distributions plays a key role. Theorem 1 The generalization of the loss of θD over T is upper bounded as
∀ > 0, ∃N s.t. ∀D ∼ Dn,
L(θD; T ) ≤ H(T ) + KL(T , D) + (2)
with probability 1 − . This bound justiﬁes the intuition that, if given the choice between two generic domains D and D , training over the one with the lowest KL divergence to T will result in a better asymptotic behaviour. The proof of this bound is presented in Appendix A.
3.3 Fine-Tuning & Multitask Learning
Fine-tuning for domain adaptation trains a model on a small in-domain set initializing optimization from the parameters of a model trained on a large out-of-domain set. Formally, ﬁnetuning minimizes L(θ; T ) the loss over T for a few steps, starting the optimization from θD = arg minθ∈Θ L(θ; D). This strategy implicitly targets a trade-off between the empirical losses over T and D. This trade-off is controlled by the number of ﬁne tuning steps nft. Few steps means that the identiﬁed parameters θft achieve a low loss over D, while many steps expresses that the parameters achieve a low loss over T . This strategy leverages the regularization effect of

early stopping (Caruana et al., 2001), i.e. the solution found by gradient descent is guaranteed to be in an Euclidean ball centered around the initialization whose radius grows with the number of steps (Grangier and Bengio, 2008), i.e.

θft − θD 2 ≤ λ nft gmax

where λ refers to the (maximum) learning rate and gmax to an upper bound on the update norm. The small distance between θft and θD guarantees that the loss L(θft; D) is close to the optimum L(θD; D) when θ → L(θ; D) is a smooth function, e.g. a Lipschitz function.
For the basic ﬁne-tuning setup, several variants have been introduced. Some approaches (Devlin et al., 2018; Liu et al., 2019; Raffel et al., 2019) consider leaving some parameters un-tuned or frozen which is the extreme case of regularization for these weights, penalizing any deviation from initialization. Other approaches consider introducing novel (unregularized) weights for ﬁne tuning, often referred as adapter layers (Houlsby et al., 2019; Stickland et al., 2019; Pfeiffer et al., 2020). Other forms of regularization, such as dropout, have also been considered in conjunction with ﬁne tuning (Miceli Barone et al., 2017).
The selection of the regularization strength in ﬁne-tuning is computationally efﬁcient since it successively visits an optimization path from the most regularized model (θD trained only on D, Sec. 3.2) to the unregularized θT (Sec. 3.1). This is more efﬁcient compared to explicit regularization methods, including multitask learning (Caruana, 1998; Collobert and Weston, 2008; Pilault et al., 2021), i.e. optimizing Lmulti(θ; T, D, α) = L(θ; T ) + αL(θ; D).

4 Data Selection

Data selection aims to improve out-of-domain training by selecting or giving stronger weights to some data points. The identiﬁcation of these points aims to emphasize out-of-domain examples which have an impact on the model similar to the impact of the in-domain training examples. We study three independently proposed selection methods, importance sampling, contrastive data selection and inﬂuence functions. We show that these methods all train models through weighted log-likelihood training,

1

L(θ; D, T, w) = −

w(y; T , D) log P (y|θ)

|D| y∈D

but introduce their weights w(y; T , D) with different justiﬁcations. Despite these differences, we show that these methods result in surprisingly similar selection weights in the speciﬁc case of neural language models.
Data selection is particularly suited when the out-of-domain training distribution and the test distribution have a large KL divergence but the out-of-domain training set is large. In that case, the generalization of a model trained on out-ofdomain data is poor due to the large KL divergence between T and D, see Eq. (2). When this KL divergence is large but out-of-domain data is abundant, data selection methods propose to select a subset of the out-of-domain data DT ⊂ D. Ideally, the training loss over such a subset L(θ, DT ) would be a better proxy for the generalization loss over T , L(θ, T ), than the training loss over the full set D, L(θ, D).
Selection involves a delicate trade-off though. One one hand, data selection is attractive since it replaces the training set with another set closer to the test domain. On the other hand, this training set is smaller, which increases the impact of estimation errors. Additionally, data selection is imperfect since the target domain distribution T is only known through a small target training set T .
This section successively presents importance sampling, contrastive data selection and inﬂuence functions and connect them into a single framework.
4.1 Importance Sampling
Although intelligent selection also called contrastive data selection is more common (Moore and Lewis, 2010; Wang et al., 2018), we ﬁrst examine importance sampling since this method will guide our understanding of other selection methods.
Importance sampling is a generic statistical technique (Owen, 2013). In our case, it can be used to estimate the expectation of the crossentropy loss over T while having access to sam-

ples from D. It relies on the identity

L(θ; T ) = =
= =

E [− log P (y|θ)]
y∼T

− log P (y|θ)P (y|T )

y∈Ω

P (y|T )

− log P (y|θ)

P (y|D)

y∈Ω P (y|D)

E [−w(y; T , D) log P (y|θ)]
y∼D

where w(y; T , D) = PP ((yy||DT )) , assuming full support on D, i.e. ∀y ∈ Ω, P (y|D) > 0. In practice, one has not access to T and D but to ﬁnite samples T and D. With importance sampling, we can consider two alternative estimators of L(θ; T ), either the empirical risk over T ,

1

L(θ; T ) = −

log P (y|θ)

|T | y∈T

or the mean of the importance weighted cross entropy over D, i.e.

1 Limp(θ; D, T, wˆ) = −

wˆ(y; T , D) log P (y|θ)

|D|

y∈D

where wˆ estimates of the weights w from the training sets D and T . The trade-off between these two estimators depends on the relative size of T and D, the imbalance of the weights w and the quality of their estimate wˆ.
Importance sampling is interesting when the generalization error L(θimp(D,T ); T ) of the model

θimp(D,T ) = arg min Limp(θ; D, T, wˆ)
θ
is less than the generalization error of θT selected by minimizing L(θ; T ), i.e. classical empirical risk minimization. This error decomposes as,

L(θimp(D,T ); T ) = LH (T ) + Lapp(T , Θ) + Liemstp(T , Θ, D, T ).
We further decompose the estimation error in two terms,
Liemstp(T , Θ, D, T ) = Lest/w(T , D, Θ, D) + Lest/wˆ (T , Θ, D, T )
where Lest/w(T , D, Θ, D) refers to the estimation error resulting from the ﬁnite size of D, assuming access to the true importance weights, and

Lest/wˆ (T , Θ, D, T ) isolate the residual error resulting from the estimation of w. We have

Lest/w(T , D, Θ, D) = L(θimp(D,D); D) − min L(θ; T ),
θ
Lest/wˆ (T , Θ, D, T ) = L(θimp(D,T ); D) − L(θimp(D,T ); D)

with θimp(D,D) = arg minθ Limp(θ; D, T, wˆ) The ﬁrst term depends on the size of D and
the imbalance of the weights. For instance, if the
weights are mostly concentrated over a small subset of D, this estimation error will be high. If this subset is smaller than T , estimation errors from Limp(θ; D, T, wˆ) will be higher than from L(θ; T ). The notion of effective sample size has
been deﬁned to quantify this effect (Kish, 1965).
It is deﬁned by examining the variance of the weighted sum of n independent random variables Zi with mean µZ and variance σZ2 , Sw = i iwwiZi i . This variance is

σS2w = (

i wi2 σ2 i w)2 Z

which can be compared to σS2 = n1 σZ2 in the unweighted case. This means that the weighted sum

variance matches the variance of an unweighted

case with

( ne =

i w)2 . i wi2

Assuming that losses over D and T have compa-

rable means and variances, the expected loss esti-

mate with importance weighting over D has lower

variance than the mean over T only when,

(w)2

ne =

|D| |T |

w2

where w

=

1 |D|

y∈D w(y) and w2

=

1 |D|

y∈D w2(y) are the sample mean and vari-

ance of the weights over D. This means

that the ﬁrst term in the estimation error is

Lest/w(T , Θ, D, T ) advantageous compared to

the estimation error from classical empirical risk

minimization over T when T is small.

Unfortunately, the second estimation error

term Lest/wˆ (T , Θ, D, T ) gets larger as T gets

smaller since estimating the importance weights
w(y; T , D) = PP ((yy||DT )) from data is challenging when T is small. One can remark that language

modeling is actually the very problem of identifying a model to estimate the probabilities in this ratio, P (y|T ) and P (y|D), from ﬁnite samples from the distributions T and D. Discriminative classiﬁers are also relevant to estimate this ratio since

P (T |y)

w(y; T , D) ∝

.

P (D|y)

In fact the multiplying constant (prior ratio) does not matter since multiplying the weighted loss by a positive constant has no impact on optimization.
When importance weights are estimated with an LM, one can estimate P (·|T ) by ﬁne tuning on T a model pre-trained on D. The number of tuning steps nft gives controls on θft − θD . When nft = 0, wˆ = 1 and the importance sampling loss corresponds to L(θ, D). As nft grows, the estimate P (y|θft) could overﬁt and assigns most of the probability mass to a small neighborhood around samples in T . The weights wˆ will in turn be concentrated in this small neighborhood, making the minimizer of the importance sampling loss close to the minimizer of the empirical loss over T . Therefore, ﬁne-tuning a language model for estimating the importance weights allow to progressively transition between the in-domain and the out-of-domain empirical loss minimizers seen in Section 3.2. In the next sections, we refer to the estimated importance sampling weights as

wDim,Tp (y) = wˆ(y; T, D).

Importance sampling has been used for model training for various application: either to improve training speed (Johnson and Guestrin, 2018; Katharopoulos and Fleuret, 2018) or to adapt to a changing training distribution (Mahmood et al., 2014; Metelli et al., 2018). Importance sampling has rarely been used to modify the training distribution of language models (Foster et al., 2010; Fernandez and Downey, 2018) as intelligent selection methods are more common.

4.2 Intelligent Selection
Intelligent selection (Moore and Lewis, 2010; Axelrod et al., 2011) and contrastive data selection, its extension to neural networks (van der Wees et al., 2017; Wang et al., 2018), have been introduced in the language modeling literature. We show that these methods are closely related to importance sampling, even if their original papers does not mention this link.

Intelligent selection selects training samples from an out-of-domain dataset according to the log-odd between an in-domain LM and an out-ofdomain LM. Typically, a binary decision is taken per sentence by comparing the average log-odd to a threshold τ ,

LIntSel(θ, D, T ) = − bIDn,tTSel(y) log P (y|θ)
y∈D

where

bIDn,tTSel(y)

is

deﬁned

as

I {log P (y|θT ) − log P (y|θD) > τ }.

Com-

pared to importance sampling, the weights are

binarized, i.e.

bIDn,tTSel(y) = I log wDim,Tp (y) > τ .

The binarization decision was certainly driven by convenience as most n-gram LM training packages did not support weighted likelihood optimization when intelligent selection was introduced. Binarization also has the advantage of down-weighting extremely positive weight values from large log P (y|θT ) due to over-ﬁtting on the small set T .
More recently, intelligent selection has been extended to neural models (van der Wees et al., 2017; Wang et al., 2018). Contrastive data selection (Wang et al., 2018) suggests to ﬁne tune the in-domain model log P (y|θT ) from log P (y|θD) and also observes that selection scores can efﬁciently be estimated from a model with a much smaller capacity than the ﬁnal trained model. Dynamic selection (van der Wees et al., 2017) proposes to increase the selection threshold τt as training progresses, gradually transitioning from generic to in-domain training. This gradual adaptation of neural network is related to curriculum learning (Bengio et al., 2009) which studies the ordering of examples and tasks during model training.
Intelligent selection methods have been applied both for unconditional models (language modeling) and conditional models (machine translation). In the conditional case, intelligent selection computes

bIDn,tTSel(x, y) = I log wDIn,tTSel(x, y) > τ

with wIntSel(x, y) = P (y|x, θT ) .

D,T

P (y|x, θD)

This ratio of conditional probabilities is different from the ratio of joint probabilities stemming from importance sampling, i.e.

Limp(θ; D, T, wˆ) =

1

P (x, y|T )

−

log P (y|x, θ).

|D| y∈D P (x, y|D)

The two ratios match when P (x|T ) = P (x|D) since

wDim,Tp (x, y) = =

P (x, y|T ) P (x, y|D) PP ((xx||DT )) wDIn,tTSel(x, y).

The formulation of intelligent selection therefore neglects the domain mismatch from the input distribution in the conditional case. This formulation aligns with the denoising goal (Wang et al., 2018) which assumes that D contains label noise, i.e. mistranslation in that case.

4.3 Inﬂuence Functions
As mentioned above, importance sampling and intelligent selection weights can be estimated by contrasting the log probabilities from a base model with those from a ﬁne-tuned model. This use of ﬁne-tuning connects intelligent selection to inﬂuence function and gradient alignment techniques. Inﬂuence functions (Koh and Liang, 2017; Pruthi et al., 2020) have been used as a diagnostic tool to identify the training instances which support or contradict with a given test label. This task is related to the selection of training data when the objective is to ﬁnd instances in a generic training set D whose training updates increase the likelihood of a set T from a different domain.
The inﬂuence of a training point y on a test point y is deﬁned as

∂ I(y, y ) = −

(y ; θ)

H−1 ∂

(y; θ)

∂θ

θ ∂θ

where (y, θ) refers to the loss at y for a model with parameters θ and Hθ refers to the Hessian of the model loss at θ. This quantity can be derived by considering the impact of reducing the weight of point y during training on the test loss at y . If we increase the weight of a training example by ,

1

θD, = min θ |D|

(z; θ) +

z∈D

(y; θ)

From (Cook and Weisberg, 1982), we derive

∂θD, ∂

= −H−1 ∂ (y; θ)

.

=0 θ ∂θ θ=θD

Composing with the test loss on (x , y ), we get

∂ (y ; θD, ) = − ∂ (y ; θ)

∂

=0

∂θ

Hθ−1 ∂ (∂yθ; θ)

θ=θD

θ=θD

which matches the expression of inﬂuence introduced above.
We now connect inﬂuence with the precedent sections on importance sampling and contrastive data selection. We consider an LM with weights θD, trained on the generic training set D. Its ﬁrst order Taylor expansion at θD is

log P (y|θD + ∆θ) = log P (y|θD) + ∆θ g(y; θD) + O ∆θ 2 (3)

where g(y; θD) = ∂∂θ log P (y|θ) θ=θD . If the model pre-trained on D is ﬁne-tuned on T by performing a single step of gradient descent with learning rate λ, we get

∂

θT = θD − λ L(T ; θ)

∂θ

θ=θD

= θD + λ E [g(y; θD)] .
y∼T

In that case, the log-odd of the two models therefore has the following Taylor expansion,

log P (y|θT ) − log P (y|θD)
= λ E g(y ; θD) g(y; θD)
y ∼T
+ O θD − θT 2 .
If we assume that the model’s Hessian is the iden-
tity, Hθ = 1, we therefore have

log P (y|θT ) − log P (y|θD) = − λ E I(y, y ) + O θD − θT 2 .
y ∼T

The Hessian assumption might be dropped when the model is ﬁne-tuned with a Newton-style update (Boyd and Vandenberghe, 2014). The above relation means that the negative mean inﬂuence of a point y ∈ D over the set T also corresponds to the log of the estimated importance weights introduced in Section 4.1, i.e.

log wDim,Tp (y) = − λ E I(y, y ) + O
y ∼T

θD − θT 2 .

Of course, this relation holds only in the case where a single gradient step is performed for ﬁnetuning. This relation allows estimating the reduction in test loss (here over T ) when removing training samples from D with positive inﬂuence which is also the goal of intelligent data selection. This strategy has been applied to label noise ﬁltering (Koh and Liang, 2017), class rebalancing (Ren et al., 2018) and domain adaptation (Wang et al., 2021).
4.4 Comparing Data Selection Methods
Our analysis connects importance sampling, contrastive data selection and inﬂuence functions. In practice, contrastive data selection is the most popular approach. Unlike inﬂuence functions, contrastive data selection weights rely on ﬁne tuning the generic model for more than one step on the in-domain data T . This has two effects. On one hand the contrastive data selection weights can be more reliable, closer to the ideal weights w(y; T , D) = PP ((yy||DT )) . On the other hand, multiple steps increase the risk of over-ﬁtting to T . In the case where one ﬁrst trains with data selection before ﬁne tuning on T , it might actually be helpful to limit the inﬂuence of T on selected data, to increase the complementary effect of ﬁne tuning (Iter and Grangier, 2021).
When comparing contrastive data selection with importance sampling, the weight binarization is the main difference. This binarization might also have two opposite effects. On the positive side, it acts has a regularizer since binary weights are less likely to reﬂect statistics speciﬁc to T compared to unquantized ones. On the negative side, it cancels low weights which might collectively represent most of the weighted cross entropy. This interpretation of contrastive data selection as a regularized version of importance sampling opens the door to exploring more sophisticated regularization alternative to regularization, e.g. using a lower capacity model or different input features to estimate selection weights.
5 Conclusions
This work focuses on domain adaptation for neural language modeling. It compares the generalization properties of a model trained over a large outof-domain corpus as opposed to a model trained over a small in-domain corpus. It shows how ﬁnetuning, the most common approach for neural LM

adaptation can achieve better trade-offs than either solution. We then focus on adaptation via data selection techniques, i.e. techniques to emphasize in-domain data in an out-of-domain training set. We show that common techniques, contrastive data selection and inﬂuence function selection, can both be derived from importance sampling.
Our analysis currently assumes a pure language modeling setup, i.e. an auto-regressive model trained aiming high log-likelihood, both for outof-domain and in-domain data. In the future, we want to extend our analysis of domain adaptation techniques to the popular setting (Bommasani et al., 2021) where model training combines language modeling over out-of-domain data and a different ﬁnal task on in-domain data.
Our theoretical work also raises empirical questions. The binarization of importance sampling weights in intelligent selection is a simple variance reduction technique and more sophisticated alternative might be beneﬁcial empirically. The link between inﬂuence functions and importance sampling suggests that examples with importance sampling weights lower than one have only a negative effect on the in-domain likelihood, which is not a typical observation in practice. This contradiction suggests expanding inﬂuence scores to take into account effects beyond a single update.
Acknowledgements
We thanks Wei Wang, Bowen Liang, Kelvin Guu and Nicolas Le Roux for their suggestions and comments.
References
Roee Aharoni and Yoav Goldberg. 2020. Unsupervised domain clusters in pretrained language models. arXiv preprint arXiv:2004.02105.
Amittai Axelrod, Xiaodong He, and Jianfeng Gao. 2011. Domain adaptation via pseudo in-domain data selection. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 355–362, Edinburgh, Scotland, UK. Association for Computational Linguistics.
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural machine translation by jointly learning to align and translate. In 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings.

Yoshua Bengio, Jérôme Louradour, Ronan Collobert, and Jason Weston. 2009. Curriculum learning. In Proceedings of the 26th Annual International Conference on Machine Learning, ICML 2009, Montreal, Quebec, Canada, June 14-18, 2009, volume 382 of ACM International Conference Proceeding Series, pages 41–48. ACM.
Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, Erik Brynjolfsson, Shyamal Buch, Dallas Card, Rodrigo Castellon, Niladri Chatterji, Annie S. Chen, Kathleen Creel, Jared Quincy Davis, Dorottya Demszky, Chris Donahue, Moussa Doumbouya, Esin Durmus, Stefano Ermon, John Etchemendy, Kawin Ethayarajh, Li Fei-Fei, Chelsea Finn, Trevor Gale, Lauren Gillespie, Karan Goel, Noah D. Goodman, Shelby Grossman, Neel Guha, Tatsunori Hashimoto, Peter Henderson, John Hewitt, Daniel E. Ho, Jenny Hong, Kyle Hsu, Jing Huang, Thomas Icard, Saahil Jain, Dan Jurafsky, Pratyusha Kalluri, Siddharth Karamcheti, Geoff Keeling, Fereshte Khani, Omar Khattab, Pang Wei Koh, Mark S. Krass, Ranjay Krishna, Rohith Kuditipudi, and et al. 2021. Foundation models. CoRR, abs/2108.07258.
Léon Bottou and Olivier Bousquet. 2007. The tradeoffs of large scale learning. In Advances in Neural Information Processing Systems 20, Proceedings of the Twenty-First Annual Conference on Neural Information Processing Systems, Vancouver, British Columbia, Canada, December 3-6, 2007, pages 161–168. Curran Associates, Inc.
Stephen P. Boyd and Lieven Vandenberghe. 2014. Convex Optimization. Cambridge University Press.
Rich Caruana. 1998. Multitask learning. In Sebastian Thrun and Lorien Y. Pratt, editors, Learning to Learn, pages 95–133. Springer.
Rich Caruana, Steve Lawrence, and Lee Giles. 2001. Overﬁtting in neural nets: Backpropagation, conjugate gradient, and early stopping. Advances in neural information processing systems, pages 402–408.
Ronan Collobert and Jason Weston. 2008. A uniﬁed architecture for natural language processing: Deep neural networks with multitask learning. In Proceedings of the 25th international conference on Machine learning, pages 160–167.
R Dennis Cook and Sanford Weisberg. 1982. Residuals and inﬂuence in regression. New York: Chapman and Hall.
Yann N. Dauphin, Angela Fan, Michael Auli, and David Grangier. 2017. Language modeling with gated convolutional networks. In Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, volume 70 of Proceedings of Machine Learning Research, pages 933–941. PMLR.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. BERT: pre-training of deep bidirectional transformers for language understanding. CoRR, abs/1810.04805.
Jared Fernandez and Doug Downey. 2018. Sampling informative training data for RNN language models. In Proceedings of ACL 2018, Student Research Workshop, pages 9–13, Melbourne, Australia. Association for Computational Linguistics.
George Foster, Cyril Goutte, and Roland Kuhn. 2010. Discriminative instance weighting for domain adaptation in statistical machine translation. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 451– 459, Cambridge, MA. Association for Computational Linguistics.
Ken-Ichi Funahashi. 1989. On the approximate realization of continuous mappings by neural networks. Neural networks, 2(3):183–192.
D. Grangier and S. Bengio. 2008. A discriminative kernel-based model to rank images from text queries. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI).
Suchin Gururangan, Ana Marasovic, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey, and Noah A. Smith. 2020. Don’t stop pretraining: Adapt language models to domains and tasks. CoRR, abs/2004.10964.
Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. 2019. Parameter-efﬁcient transfer learning for NLP. CoRR, abs/1902.00751.
Dan Iter and David Grangier. 2021. On the complementarity of data selection and ﬁne tuning for domain adaptation. arXiv, 2109.07591.
Tyler B Johnson and Carlos Guestrin. 2018. Training deep models faster with robust, approximate importance sampling. Advances in Neural Information Processing Systems, 31:7265–7275.
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361.
Angelos Katharopoulos and François Fleuret. 2018. Not all samples are created equal: Deep learning with importance sampling. In International conference on machine learning, pages 2525–2534. PMLR.
Leslie Kish. 1965. Survey sampling. new york: John wesley & sons.
Philipp Koehn. 2009. Statistical machine translation. Cambridge University Press.

Pang Wei Koh and Percy Liang. 2017. Understanding black-box predictions via inﬂuence functions. In Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pages 1885–1894, International Convention Centre, Sydney, Australia. PMLR.
Yann Lecun. 1987. Modeles connexionnistes de l’apprentissage. Universite Pierre et Marie Curie (Paris 6).
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692.
Ashique Rupam Mahmood, Hado Van Hasselt, and Richard S Sutton. 2014. Weighted importance sampling for off-policy learning with linear function approximation. In NIPS, pages 3014–3022.
Christopher Manning and Hinrich Schutze. 1999. Foundations of statistical natural language processing. MIT press.
Alberto Maria Metelli, Matteo Papini, Francesco Faccio, and Marcello Restelli. 2018. Policy optimization via importance sampling. arXiv preprint arXiv:1809.06098.
Antonio Valerio Miceli Barone, Barry Haddow, Ulrich Germann, and Rico Sennrich. 2017. Regularization techniques for ﬁne-tuning in neural machine translation. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1489–1494, Copenhagen, Denmark. Association for Computational Linguistics.
Tomás Mikolov, Martin Karaﬁát, Lukás Burget, Jan Cernocký, and Sanjeev Khudanpur. 2010. Recurrent neural network based language model. In INTERSPEECH 2010, 11th Annual Conference of the International Speech Communication Association, Makuhari, Chiba, Japan, September 26-30, 2010, pages 1045–1048. ISCA.
Robert C. Moore and William Lewis. 2010. Intelligent selection of language model training data. In Proceedings of the ACL 2010 Conference Short Papers, pages 220–224, Uppsala, Sweden. Association for Computational Linguistics.
Art B. Owen. 2013. Monte Carlo theory, methods and examples. Stanford.
Jonas Pfeiffer, Aishwarya Kamath, Andreas Rücklé, Kyunghyun Cho, and Iryna Gurevych. 2020. Adapterfusion: Non-destructive task composition for transfer learning.
Jonathan Pilault, Amine El hattami, and Christopher Pal. 2021. Conditionally adaptive multi-task learning: Improving transfer learning in {nlp} using fewer parameters & less data. In International Conference on Learning Representations.

Garima Pruthi, Frederick Liu, Satyen Kale, and Mukund Sundararajan. 2020. Estimating training data inﬂuence by tracing gradient descent. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 612, 2020, virtual.
Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners (gpt).
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2019. Exploring the limits of transfer learning with a uniﬁed text-to-text transformer.
Mengye Ren, Wenyuan Zeng, Bin Yang, and Raquel Urtasun. 2018. Learning to reweight examples for robust deep learning. In International Conference on Machine Learning, pages 4334–4343. PMLR.
Szymon Roziewski and Marek Kozłowski. 2021. Languagecrawl: a generic tool for building language models upon common crawl. Language Resources and Evaluation, pages 1–29.
Abigail See, Peter J. Liu, and Christopher D. Manning. 2017. Get to the point: Summarization with pointergenerator networks. CoRR, abs/1704.04368.
Asa Cooper Stickland, Iain Murray, someone, and someone. 2019. BERT and PALs: Projected attention layers for efﬁcient adaptation in multi-task learning. volume 97 of Proceedings of Machine Learning Research, pages 5986–5995, Long Beach, California, USA. PMLR.
Martin Sundermeyer, Ralf Schlüter, and Hermann Ney. 2012. LSTM neural networks for language modeling. In INTERSPEECH 2012, 13th Annual Conference of the International Speech Communication Association, Portland, Oregon, USA, September 9-13, 2012, pages 194–197. ISCA.
Marlies van der Wees, Arianna Bisazza, and Christof Monz. 2017. Dynamic data selection for neural machine translation. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1400–1410, Copenhagen, Denmark. Association for Computational Linguistics.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. CoRR, abs/1706.03762.
Wei Wang, Taro Watanabe, Macduff Hughes, Tetsuji Nakagawa, and Ciprian Chelba. 2018. Denoising neural machine translation training with trusted data and online data selection. In Proceedings of the Third Conference on Machine Translation: Research Papers, pages 133–143, Brussels, Belgium. Association for Computational Linguistics.

Xinyi Wang, Ankur Bapna, Melvin Johnson, and Orhan Firat. 2021. Gradient-guided loss masking for neural machine translation.

A Proof of Theorem 1

When the distributions T and D differ, the Kullback–Leibler (KL) divergence between the two distributions is considered. We show that the generalization of the loss of θD over T is upper bounded

∀ > 0, ∃N s.t. ∀D ∼ Dn,
L(θD; T ) ≤ H(T ) + KL(T , D) + (4)
with probability 1 − This bound justiﬁes intuition that if given the choice between two generic domain D and D , training over the one with the lowest KL divergence to T will result a in better asymptotic behaviour.

Proof. We consider the asymptotic case for the size of D. For any > 0, the universal approximation property allows us to consider a model capacity large enough such that

Lapp(D, Θ) < 2 Using consistency, we can also consider a training set D large enough such that

Lest(D, Θ, D) < 2 with probability 1 − . With the same probability,

L(θD; D) < LH (D) +
which can be rewritten as a bound on the Kullback-Leibler divergence,

KL(P (·|D), P (·|θD)) = L(θD; D)−LH (D) < .

This bound can help connecting the generalization loss of θD over T with the Kullback-Leibler divergence of T and D,

L(θD; T )

=

P (y|T ) log P (y|θD)

y∈Ω

=

P (y|T ) log(P (y|D) + P (y|θD) − P (y|D))

y∈Ω

≤

P (y|T ) log(P (y|D) + |P (y|D) − P (y|θD)|)

y∈Ω

≤

P (y|T ) log(P (y|D) + 2 2)

(5)

y∈Ω

≤

P (y|T ) log(P (y|D)) + log(1 + 2m 2)

y∈Ω

≤ H(T ) + KL(T , D) + log(1 + 2m 2)

where m = 1/ miny P (y|D) assumes that P (·|D) has full support, and (5) relies on
Pinsker’s inequality, i.e. maxy |P (y) − Q(y)| < 2KL(Q, Y )2.

