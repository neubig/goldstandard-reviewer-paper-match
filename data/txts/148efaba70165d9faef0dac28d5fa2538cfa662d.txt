arXiv:2010.07938v2 [cs.HC] 4 Apr 2022

Deciding Fast and Slow: The Role of Cognitive Biases in AI-assisted Decision-making
Charvi Rastogi∗1 , Yunfeng Zhang2, Dennis Wei3, Kush R. Varshney3, Amit Dhurandhar3, Richard Tomsett4
1Carnegie Mellon University, 2Twitter, 3IBM Research, 4Onﬁdo
Abstract
Several strands of research have aimed to bridge the gap between artiﬁcial intelligence (AI) and human decision-makers in AI-assisted decision-making, where humans are the consumers of AI model predictions and the ultimate decision-makers in high-stakes applications. However, people’s perception and understanding are often distorted by their cognitive biases, such as conﬁrmation bias, anchoring bias, availability bias, to name a few. In this work, we use knowledge from the ﬁeld of cognitive science to account for cognitive biases in the human-AI collaborative decisionmaking setting, and mitigate their negative eﬀects on collaborative performance. To this end, we mathematically model cognitive biases and provide a general framework through which researchers and practitioners can understand the interplay between cognitive biases and human-AI accuracy. We then focus speciﬁcally on anchoring bias, a bias commonly encountered in human-AI collaboration. We implement a time-based de-anchoring strategy and conduct our ﬁrst user experiment that validates its eﬀectiveness in human-AI collaborative decision-making. With this result, we design a time allocation strategy for a resource-constrained setting that achieves optimal human-AI collaboration under some assumptions. We, then, conduct a second user experiment which shows that our time allocation strategy with explanation can eﬀectively de-anchor the human and improve collaborative performance when the AI model has low conﬁdence and is incorrect.
1 Introduction
It should be a truth universally acknowledged that a human decision-maker in possession of an AI model must be in want of a collaborative partnership. Recently, we have seen a rapid increase in the deployment of machine learning (ML) models in decision-making systems, where the AI models serve as helpers to human experts in many high-stakes settings. Examples of such tasks can be found in healthcare, ﬁnancial loans, criminal justice, job recruiting, and fraud monitoring. Speciﬁcally, judges use risk assessments to determine criminal sentences, banks use models to manage credit risk, and doctors use image-based ML predictions for diagnosis, to list a few.
The emergence of AI-assisted decision-making in society has raised questions about whether and when to rely on the AI model’s decisions. These questions can be viewed as problems of communication between AI and humans, and research in interpretable, explainable, and trustworthy machine learning as eﬀorts to improve aspects of this communication. However, a key component of human-AI communication that is often sidelined is the human decision-makers themselves. Humans’ perception of the communication received from AI is at the core of this communication gap. Research in communication exempliﬁes the need to model receiver characteristics, thus implying the need to understand and account for human cognition in collaborative decision-making.
∗Corresponding author. Email: crastogi@cs.cmu.edu
1

Figure 1. Three constituent spaces to capture diﬀerent interactions in human-AI collaboration. The interactions of the perceived space, representing the human decision-maker, with the observed space and the prediction space may lead to cognitive biases. The deﬁnition of the diﬀerent spaces is partially based on ideas of Yeom and Tschantz (2018).
As a step towards studying human cognition in AI-assisted decision-making, our work focuses on the role of cognitive biases in this setting. Cognitive biases, introduced in the seminal work by Tversky and Kahneman (1974), represent a systematic pattern of deviation from rationality in judgment wherein individuals create their own "subjective reality" from their perception of the input. An individual’s perception of reality, not the objective input, may dictate their behavior in the world, thus, leading to distorted and inaccurate judgment. While cognitive biases and their eﬀects on decision-making are well known and widely studied, we note that AI-assisted decision-making presents a new decision-making paradigm and it is important to study their role in this new paradigm, both analytically and empirically.
Our ﬁrst contribution is illustrated partially in Figure 1. In a collaborative decision-making setting, we deﬁne the perceived space to represent the human decision-maker. Here, we posit that there are two interactions that may lead to cognitive biases in the perceived space – (1) interaction with the observed space which consists of the feature space and all the information the decision-maker has acquired about the task, (2) interaction with the prediction space representing the output generated by the AI model, which could consist of the AI decision, explanation, etc. In Figure 1, we associate cognitive biases that aﬀect the decision-makers’ priors and their perception of the data available with their interaction with the observed space. Conﬁrmation bias, availability bias, the representativeness heuristic, and bias due to selective accessibility of the feature space are mapped to the observed space. On the other hand, anchoring bias and the weak evidence eﬀect are mapped to the prediction space. Based on this categorization of biases, we provide a model for some of these biases using our biased Bayesian framework.
To focus our work in the remainder of the paper, we study and provide mitigating strategies for anchoring bias in AI-assisted decision-making, wherein the human decision-maker forms a skewed perception due to an anchor (AI decision) available to them, which limits the exploration of alternative hypotheses. Anchoring bias manifests through blind reliance on the anchor. While poorly calibrated reliance on AI has been studied previously from the lens of trust (Zhang et al., 2020; Tomsett et al., 2020; Okamura and Yamada, 2020), in this work we analyse reliance miscalibration due to anchoring bias which has a diﬀerent mechanism and, hence, diﬀerent mitigating strategies.
Tversky and Kahneman (1974) explained that anchoring bias manifests through the anchoring-andadjustment heuristic wherein, when asked a question and presented with any anchor, people adjust away insuﬃciently from the anchor. Building on the notion of bounded rationality, previous work (Lieder et al., 2018) attributes the adjustment strategy to a resource-rational policy wherein the insuﬃcient adjustment is a rational trade-oﬀ between accuracy and time. To test this in our setting, we conduct an experiment with human participants on Amazon Mechanical Turk to study whether allocating more resources — in this case, time — alleviates anchoring bias. This bias manifests through the rate of
2

agreement with the AI prediction. Thus, by measuring the rate of agreement with the AI prediction in several carefully designed trials, we validate that time indeed is a useful resource that helps the decision-maker suﬃciently adjust away from the anchor when needed. We note that the usefulness of time in remediating anchoring bias is an intuitive idea discussed in previous works Tversky and Kahneman (1974), but one that has not been empirically validated to our knowledge. Thus, it is necessary to conﬁrm this ﬁnding experimentally, especially in the AI-assisted setting.
As our ﬁrst experiment conﬁrms that more time helps reduce bounded-rational anchoring to the AI decision, one might suggest that giving more time to all decisions should yield better decision-making performance from the human-AI team. However, this solution does not utilize the beneﬁts of highquality AI decisions available in many cases. Moreover, it does not account for the limited availability of time. Thus, we formulate a novel resource (time) allocation problem that factors in the eﬀects of anchoring bias and the variance in AI accuracy to maximize human-AI collaborative accuracy. We propose a time allocation policy and prove its optimality under some assumptions. We also conduct a second user experiment to evaluate human-AI team performance under this policy in comparison with several baseline policies. Our results show that while the overall performance of all policies considered is roughly the same, our policy helps the participants de-anchor from the AI prediction when the AI is incorrect and has low conﬁdence.
The time allocation problem that we study is motivated by real-world AI-assisted decision-making settings. Adaptively determining the time allocated to a particular instance for the best possible judgement can be very useful in multiple applications. For example, consider a (procurement) fraud monitoring system deployed in multinational corporations, which analyzes and ﬂags high-risk invoices (Dhurandhar et al., 2015). Given the scale of these systems, which typically analyze tens of thousands of invoices from many diﬀerent geographies daily, the number of invoices that may be ﬂagged, even if a small fraction, can easily overwhelm the team of experts validating them. In such scenarios, spending a lot of time on each invoice is not admissible. An adaptive scheme that takes into account the biases of the human and the expected accuracy of the AI model is highly desirable to produce the most objective decisions. Our work is also applicable to the other aforementioned domains, such as in criminal proceedings where judges have to look over many diﬀerent case documents and make quick decisions, often in under a minute.
In summary, we make the following contributions:
• We provide a biased Bayesian framework for modeling biased AI-assisted decision making. Based on the source of the cognitive biases, we situate some well-known cognitive biases within our framework.
• Focusing on anchoring bias in AI-assisted decision-making, we show with human participants that allocating more time to a decision reduces anchoring in this setting.
• We formulate a time allocation problem to maximize human-AI team accuracy that accounts for the anchoring-and-adjustment heuristic and the variance in AI accuracy.
• We propose a conﬁdence-based allocation policy and identify conditions under which it achieves optimal team performance.
• Through a carefully designed human subject experiment, we evaluate the real-world eﬀectiveness of the conﬁdence-based time allocation policy, showing that when conﬁdence-based information is displayed, it helps humans de-anchor from incorrect and low-conﬁdence AI predictions.
2 Related work
Cognitive biases are an important factor in human decision-making (Barnes JR., 1984; Das and Teng, 1999; Ehrlinger et al., 2016), and have been studied widely in decision-support systems research (Arnott,
3

2006; Zhang et al., 2015; Solomon, 2014; Phillips-Wren et al., 2019). Cognitive biases also show up in many aspects of collaborative behaviours (Silverman, 1992; Janssen and Kirschner, 2020; Bromme et al., 2010). More speciﬁcally, there exists decades-old research on cognitive biases (Tversky and Kahneman, 1974) such as conﬁrmation bias (Nickerson, 1998; Klayman, 1995; Oswald and Grosjean, 2004), anchoring bias (Furnham and Boo, 2011; Epley and Gilovich, 2006, 2001), automation bias Lee and See (2004), availability bias (Tversky and Kahneman, 1973), etc.
Recently, as AI systems are increasingly embedded into high stakes human decisions, understanding human behavior, and reliance on technology have become critical, “Poor partnerships between people and automation will become increasingly costly and catastrophic” (Lee and See, 2004). This concern has sparked crucial research in several directions, such as human trust in algorithmic systems, interpretability, and explainability of machine learning models (Arnold et al., 2019; Zhang et al., 2020; Tomsett et al., 2020; Siau and Wang, 2018; Doshi-Velez and Kim, 2017; Lipton, 2018; Adadi and Berrada, 2018; Preece, 2018).
In parallel, research in AI-assisted decision-making has worked on improving the human-AI collaboration Lai et al. (2020); Lai and Tan (2019); Bansal et al. (2021, 2019); Green and Chen (2019); Okamura and Yamada (2020). These works experiment with several heuristic-driven AI explanation techniques that do not factor in all the characteristics of the human at the end of the decision-making pipeline. Speciﬁcally, the experimental results in (Bansal et al., 2021) show that explanations supporting the AI decision tend to exacerbate over-reliance on the AI decision. In contrast, citing a body of research in psychology, philosophy, and cognitive science, Miller Miller (2019) argues that the machine learning community should move away from imprecise, subjective notions of "good" explanations and instead focus on reasons and thought processes that people apply for explanation selection. In agreement with Miller, our work builds on literature in psychology on cognitive biases to inform modeling and eﬀective de-biasing strategies. Our work provides a structured approach to addressing problems, like over-reliance on AI, from a cognitive science perspective. In addition, we adopt a two-step process, wherein we inform our subsequent de-biasing approach (Experiment 2) based on the results of our ﬁrst experiment, thus, paving the pathway for experiment-driven human-oriented research in this setting.
Work on cognitive biases in human-AI collaboration is still rare, however. Recently, Fürnkranz et al. (2020) evaluated a selection of cognitive biases to test whether minimizing the complexity or length of a rule yields increased interpretability of machine learning models. Kliegr et al. (2018) review twenty diﬀerent cognitive biases that aﬀect the interpretability and associated de-biasing techniques. Both these works (Fürnkranz et al., 2020; Kliegr et al., 2018) are speciﬁc to rule-based ML models. Baudel et al. (2020) address complacency/authority bias in using algorithmic decision aids in business decision processes. Wang et al. (2019) propose a conceptual framework for building explainable AI based on the literature on cognitive biases. Building on these works, our work provides novel mathematical models for the AI-assisted setting to identify the role of cognitive biases. Contemporaneously, Buçinca et al. (2021) studies the use of cognitive forcing functions to reduce over-reliance in human-AI collaboration.
The second part of our work focuses on anchoring bias. The phenomenon of anchoring bias in AI-assisted setting has also been studied as a part of automation bias Lee and See (2004) wherein the users display over-reliance on AI due to blind trust in automation. Previous experimental research has also shown that people do not calibrate their reliance on AI based on its accuracy (Green and Chen, 2019). Several studies suggest that people are unable to detect algorithmic errors (Poursabzi-Sangdeh et al., 2018), are biased by irrelevant information (Englich et al., 2006), rely on algorithms that are described as having low accuracy, and trust algorithms that are described as accurate but present random predictions (Springer et al., 2018). These behavioural tendencies motivate a crucial research question — how to account for these heuristics, often explained by cognitive biases such as anchoring bias. In this direction, our work is the ﬁrst to empirically and analytically study a time-based de-biasing strategy to remediate anchoring bias in the AI-assisted setting.
Lastly, the work by Park et al. (Park et al., 2019) considers the approach of forcing decision-makers to spend more time deliberating their decision before the AI prediction is provided to them. In this work, we consider the setting where the AI prediction is provided to the decision-maker beforehand
4

which may lead to anchoring bias. Moreover, we treat time as a limited resource and accordingly provide optimal allocation strategies.

3 Problem setup and modeling
We consider a collaborative decision-making setup, consisting of a machine learning algorithm and a human decision-maker. First, we precisely describe our setup and document the associated notation. Following this, we provide a Bayesian model for various human cognitive biases induced by the human-AI collaborative process.
Our focus in this paper is on the AI-assisted decision-making setup, wherein the objective of the human is to correctly classify the set of feature information available into one of two categories. Thus, we have a binary classiﬁcation problem, where the true class is denoted by y∗ ∈ {0, 1}. To make the decision/prediction, the human is presented with feature information, and we denote the complete set of features available pertaining to each sample by D. In addition to the feature information, the human is also shown the output of the machine learning algorithm. Here, the AI output could consist of several parts, such as the prediction, denoted by y ∈ {0, 1}, and the machine-generated explanation for its prediction. We express the complete AI output as a function of the machine learning model, denoted by f (M ). Finally, we denote the decision made by the human decision-maker by y˜ ∈ {0, 1}.
We now describe the approach towards modeling the behavior of human decision-makers when assisted by machine learning algorithms.

3.1 Bayesian decision-making
Bayesian models for human cognition have become increasingly prominent across a broad spectrum of cognitive science (Tenenbaum, 1999; Griﬃths and Tenenbaum, 2006; Chater et al., 2006). The Bayesian approach is thoroughly embedded within the framework of decision theory. Its basic tenets are that opinions should be expressed in terms of subjective or personal probabilities, and that the optimal revision of such opinions, in the light of relevant new information, should be accomplished via Bayes’ theorem.
First, consider a simpler setting, where the decision-maker uses the feature information available, D, and makes a decision y˜ ∈ {0, 1}. Let the decision variable be denoted by Y˜ . Based on literature in psychology and cognitive science (Griﬃths and Tenenbaum, 2006; Chater et al., 2006), we model a rational decision-maker as Bayes’ optimal. That is, given a prior on the likelihood of the prediction, Ppr(Y˜ ) and the data likelihood distribution P(D|Y˜ ), the decision-maker picks the hypothesis/class with the higher posterior probability. Formally, the Bayes’ theorem states that

P(Y˜ = i|D) =

P(D|Y˜ = i)Ppr(Y˜ = i) ,

(1)

j∈{0,1} P(D|Y˜ = j)Ppr(Y˜ = j)

where i ∈ {0, 1} and the human decision is given by y˜ = argmaxi∈{0,1} P(Y˜ = i|D). Now, in our setting, in addition to the feature information available, the decision-maker takes into account the output of
the machine learning algorithm, f (M ), which leads to following Bayes’ relation

P(Y˜ |D, f (M )) ∝ P(D, f (M )|Y˜ )Ppr(Y˜ ).

(2)

We assume that conditioned on the decision-maker’s decision Y˜ , they perceive the feature information and the AI output independently, which gives

P(Y˜ |D, f (M )) ∝ P(D|Y˜ )P(f (M )|Y˜ )Ppr(Y˜ ),

(3)

5

where P(f (M )|Y˜ ) indicates the conditional probability of the AI output perceived by the decision-maker. The assumption in (3) is akin to a naive Bayes’ assumption of conditional independence, but we only assume conditional independence between D and f (M ) and not between components within D or components within f (M ). This concludes our model for a rational decision-maker assisted by a machine learning model.
In reality, the human decision-maker may behave diﬀerently from a fully rational agent due to their cognitive biases. In some studies (Matsumori et al., 2018; Payzan-LeNestour and Bossaerts, 2011, 2012), such deviations have been explained by introducing exponential biases (i.e. inverse temperature parameters) on Bayesian inference because these were found useful in expressing bias levels. We augment the modeling approach in (Matsumori et al., 2018) to a human-AI collaborative setup. Herein we model the biased Bayesian estimation as

P(Y˜ |D, f (M )) ∝ P(D|Y˜ )αP(f (M )|Y˜ )βPpr(Y˜ )γ,

(4)

where α, β, γ are variables that represent the biases in diﬀerent factors in the Bayesian inference.
Equation (4) allows us to understand and model several cognitive biases arising in AI-assisted
decision making. To facilitate the following discussion, we take the ratio between (4) evaluated for Y˜ = 1 and (4) for Y˜ = 0:

P(Y˜ = 1|D, f (M )) P(D|Y˜ = 1) α P(f (M )|Y˜ = 1) β Ppr(Y˜ = 1) γ P(Y˜ = 0|D, f (M )) = P(D|Y˜ = 0) P(f (M )|Y˜ = 0) Ppr(Y˜ = 0) . (5)

The human decision is thus Y˜ = 1 if the ratio is greater than 1 and Y˜ = 0 otherwise. The ﬁnal ratio is a product of the three ratios on the right-hand side raised to diﬀerent powers. We can now state the following:
1. In anchoring bias, the weight put on AI prediction is high, i.e., β > 1 and the corresponding ratio in (5) contributes more to the ﬁnal ratio, whereas the weight on prior and data likelihood reduces.
2. By contrast, in conﬁrmation bias the weight on the prior is high, γ > 1, and the weight on the data and machine prediction reduces in comparison.
3. Selective accessibility is a phenomena used to explain the mechanism of cognitive biases wherein the data that supports the decision-maker is selectively used as evidence, while the rest of the data is not considered. This distorts the data likelihood factor in (4). The direction of distortion α > 1 or α < −1 depends on the cognitive bias driven decision.
4. The weak evidence eﬀect (Fernbach et al., 2011) suggests that when presented with weak evidence for a prediction, the decision-maker would tend to choose the opposite prediction. This eﬀect is modeled with β < −1.
To focus our approach, we consider a particular cognitive bias — anchoring bias, which is speciﬁc to the nature of human-AI collaboration and has been an issue in previous works (Lai and Tan, 2019; Springer et al., 2018; Bansal et al., 2021). In the next section, we summarise the ﬁndings about anchoring bias in the literature, explain proposed de-biasing technique and conduct an experiment to validate the technique.

4 Anchoring bias
AI-assisted decision-making tasks are prone to anchoring bias, where the human decision-maker is irrationally anchored to the AI-generated decision. The anchoring-and-adjustment heuristic, introduced by Tversky and Kahneman in Tversky and Kahneman (1974) and studied in (Epley and Gilovich, 2006;

6

Lieder et al., 2018) suggests that after being anchored, humans tend to adjust insuﬃciently because adjustments are eﬀortful and tend to stop once a plausible estimate is reached. Lieder et al. (Lieder et al., 2018) proposed the resource rational model of anchoring-and-adjustment which explains that the insuﬃcient adjustment can be understood as a rational trade-oﬀ between time and accuracy. This is a consequence of the bounded rationality of humans (Simon, 1956, 1972), which entails satisﬁcing, that is, accepting sub-optimal solutions that are good enough, rather than optimizing solely for accuracy. Through user studies, Epley et al. (Epley and Gilovich, 2006) argue that cognitive load and time pressure are contributing factors behind insuﬃcient adjustments.
Informed by the above works viewing anchoring bias as a problem of insuﬃcient adjustment due to limited resources, we aim to mitigate the eﬀect of anchoring bias in AI-assisted decision-making, using time as a resource. We use the term de-anchoring to denote the rational process of adjusting away from the anchor. With this goal in mind, we conducted two user studies on Amazon Mechanical Turk. Through the ﬁrst user study (Experiment 1), we aim to understand the eﬀect of diﬀerent time allocations on anchoring bias and de-anchoring in an AI-assisted decision-making task. In Experiment 2, we use the knowledge obtained about the eﬀect of time in Experiment 1 to design a time allocation strategy and test it on the experiment participants.
We now describe Experiment 1 in detail.

4.1 Experiment 1

In this study, we asked the participants to complete an AI-assisted binary prediction task consisting of a number of trials. Our aim is to learn the eﬀect of allocating diﬀerent amounts of time to diﬀerent trials on participants with anchoring bias.
To quantify anchoring bias and thereby the insuﬃciency of adjustments, we use the probability P(y˜ = y) that the human decision-maker agrees with the AI prediction y, which is easily measured. This measure can be motivated from the biased Bayesian model in (4). In the experiments, the model output f (M ) consists of only a predicted label y. In this case, (4) becomes

P(Y˜ = y|D, f (M )) ∝ P(D|Y˜ = y)αP(Y = y|Y˜ = y)βPpr(Y˜ = y)γ.

(6)

Let us make the reasonable assumption that the decision-maker’s decision Y˜ positively correlates with the AI prediction Y , speciﬁcally that the ML model’s probability P(Y = y|Y˜ = y) is larger when y = y than when y = y. Then as the exponent β increases, i.e., as anchoring bias strengthens, the likelihood that y = y maximizes (6) and becomes the human decision y˜ also increases. In the limit β → ∞, we have agreement y˜ = y with probability 1. Conversely, for β = 1, the two other factors in (6) are weighed appropriately and the probability of agreement assumes a natural baseline value. We conclude that the probability of agreement is a measure of anchoring bias. It is also important to ensure that this measure is based on tasks where the human has reason to choose a diﬀerent prediction.
Thus, given the above relationship between anchoring bias and agreement probability (equivalently disagreement probability), we tested the following hypothesis to determine whether time is a useful resource in mitigating anchoring bias:

• Hypothesis 1 (H1): Increasing the time allocated to a task alleviates anchoring bias, yielding a higher likelihood of suﬃcient adjustment away from the AI-generated decision when the decision-maker has the knowledge required to provide a diﬀerent prediction.

Participants. We recruited 47 participants from Amazon Mechanical Turk for Experiment 1, limiting the pool to subjects from within the United States with a prior task approval rating of at least 98% and a minimum of 100 approved tasks. 10 participants were between Age 18 and 29, 26 between Age 30 and 39, 6 between Age 40 and 49, and 5 over Age 50. The average completion time for this user study was 27 minutes, and each participant received compensation of $4.5 (roughly equals an hourly wage of $10). The participants received a base pay of $3.5 and a bonus of $1 (to incentivize accuracy).

7

Task and AI model. We designed a performance prediction task wherein a participant was asked to predict whether a student would pass or fail a class, based on the student’s characteristics, past performance, and some demographic information. The dataset for this task was obtained from the UCI Machine Learning Repository, published as the Student Performance Dataset (Cortez and Silva, 2008). This dataset contains 1044 instances of students’ class performances in 2 subjects (Mathematics and Portuguese), each described by 33 features. To prepare the dataset, we binarized the target labels (‘pass’, ‘fail’), split the dataset into training and test sets (70/30 split). To create our AI, we trained a logistic regression model on the standardized set of features from the training dataset. Based on the feature importance (logistic regression coeﬃcients) assigned to each feature in the dataset, we retained the top 10 features for the experiments. These included — mother’s and father’s education, mother’s and father’s jobs, hours spent studying weekly, interest in higher education, hours spent going out with friends weekly, number of absences in the school year, enrolment in extra educational support, and number of past failures in the class.
Study procedure Since we are interested in studying decision-makers’ behavior when humans have prior knowledge and experience in the prediction task, we ﬁrst trained our participants before collecting their decision data for analysis. The training section consists of 15 trials where the participant is ﬁrst asked to provide their prediction based on the student data and is then shown the correct answer after attempting the task. These trials are the same for all participants and are sampled from the training set such that the predicted probability (of the predicted class) estimated by the AI model is distributed uniformly over the intervals [0.5, 0.6], (0.6, 0.7], · · · , (0.9, 1]. Taking predicted probability as a proxy for diﬃculty, this ensures that all levels of diﬃculty are represented in the task. To help accelerate participants’ learning, we showed bar charts that display the distributions of the outcome across the feature values of each feature. These bar charts were not provided in the testing section to ensure stable performance throughout the testing section and to emulate a real-world setting.
To induce anchoring bias, the participant was informed at the start of the training section that the AI model was 85% accurate (we carefully chose the training trials to ensure that the AI was indeed 85% accurate over these trials), while the model’s actual accuracy is 70.8% over the entire training set and 66.5% over the test set. Since our goal is to induce anchoring bias and the training time is short, we stated a high AI accuracy. Moreover, this disparity between stated accuracy (85%) and true accuracy (70.8%) is realistic if there is a distribution shift between the training and the test set, which would imply that the humans’ trust in AI is misplaced. In addition to stating AI accuracy at the beginning, we informed the participants about the AI prediction for each training trial after they have attempted it so that they can learn about AI’s performance ﬁrst-hand.
The training section is followed by the testing section which consists of 36 trials sampled from the test set and was kept the same (in the same order) for all participants. In this section, the participants were asked to make a decision based on both the student data and AI prediction. They were also asked to describe their conﬁdence level in their prediction as low, medium, or high.
To measure the de-anchoring eﬀect of time, we included some trials where the AI is incorrect but the participants have the requisite knowledge to adjust away from the incorrect answer. That is, we included trials where the participants’ accuracy would be lower when they are anchored to the AI prediction than when they are not. We call these trials — probe trials, which help us probe the eﬀect of time on de-anchoring. On the ﬂip side, we could not include too many of these trials because participants may lose their trust in the AI if exposed to many apparently incorrect AI decisions. To achieve this balance, we sampled 8 trials of medium diﬃculty where the AI prediction is accurate (predicted probability ranging from 0.6 to 0.8) and ﬂip the AI prediction shown to the participants. The remaining trials, termed unmodiﬁed trials are sampled randomly from the test set while maintaining a uniform distribution over the AI predicted probability (of the predicted class). Here, again, we use the predicted probability as a proxy for the diﬃculty of the task, as evaluated by the machine learning model. We note that the accuracy of the AI predictions shown to the participants is 58.3% which is far lower than the 85% accuracy shown in the training section.
8

Average disagreement Density

0.7

Time

0.175

10s

0.6

0.150

15s

20s

0.5

0.125

25s

Probe

0.4

Unmodified (All)

0.100

0.3

Unmodified (y = y * )

Unmodified (y y * ) 0.075

0.2

0.050

0.1

0.025

10

15

20

Time allocated per task (s)

25

0.000

0 5 10 15 20 25

Time taken to answer (s)

(a)

(b)

Figure 2. Results of experiment 1. (a) Average disagreement with the AI prediction for diﬀerent time allocations in experiment 1. (b) Distribution of time taken by the participants to provide their answer under the four diﬀerent time conditions, {10,15,20,25} seconds in Experiment 1. For illustration purposes, we use kernel density estimation to estimate the probability density function shown. The actual answering time lies between 0 and 25 seconds.

Time allocation. To investigate the eﬀect of time on the anchoring-and-adjustment heuristic in AI-assisted decision-making, we divide the testing section into four blocks for each participant based on the time allocation per trial. To select the allocated time intervals, we ﬁrst conducted a shorter version of the same study to learn the amount of time needed to solve the student performance prediction task, which suggested that the time intervals of 10s, 15s, 20s and 25s captured the range from necessary to suﬃcient. Now, with these four time intervals, we divided the testing section into four blocks of 9 trials each, where the time allocated per trial in each block followed the sequence [t1, t2, t3, t4] and for each participant this sequence was a random permutation of [10, 15, 20, 25]. The participants were not allowed to move to the next trial till the allocated time ran out. Furthermore, each block was comprised of 2 probe trials and 7 unmodiﬁed trials, randomly ordered. Recall that each participant was provided the same set of trials in the same order.
Now, with the controlled randomization of the time allocation, independent of the participant, their performance, and the sequence of the tasks, we are able to identify the eﬀect of time on de-anchoring. It is possible that a participant that disagrees with the AI prediction often in the ﬁrst half, is not anchored to the AI prediction in the latter half. Our study design allows us to average out such participant-speciﬁc eﬀects, through the randomization of time allocation interval sequences across participants.
The main results of Experiment 1 are illustrated in Figure 2(a). We see that the probe trials served their intended purpose, since the average disagreement is much higher for probe trials compared to unmodiﬁed trials for all time allocations. This suggests that the participants had learned to make accurate predictions for this task, otherwise they would not be able to detect the AI’s errors in the probe trials, more so in the 10-second condition. We also observe that the likelihood of disagreement for unmodiﬁed trials is low (close to 0.1) for all time allocations. This suggests that the participants’ knowledge level in this task is roughly similar to or less than that of the AI since the participants are unable to oﬀer any extra knowledge in the unmodiﬁed trials.
Anchoring-and-adjustment. The results on the probe trials in Figure 2(a) suggest that the participants’ likelihood of suﬃciently adjusting away from the incorrect AI prediction increased as the time allocated increased. This strengthens the argument that the anchoring-and-adjustment heuristic is a resource-rational trade-oﬀ between time and accuracy (Lieder et al., 2018). Speciﬁcally, we observe that the average disagreement percentage in probe trials increased from 48% in the 10-second condition to 67% in the 25-second condition. We used the bootstrap method with 5000 re-samples to estimate

9

the coeﬃcient of a linear regression ﬁt on average disagreement vs. time allocated for probe trials. This resulted in a signiﬁcantly positive coeﬃcient of 0.01 (bootstrap 95% conﬁdence interval [0.001, 0.018]). This result is consistent with our Hypothesis 1 (H1) that increasing time for decision tasks alleviates anchoring bias. We note that the coeﬃcient is small in value because the scales of the independent and dependent variables of the regression (time and average disagreement) have not been adjusted for the regression, so the coeﬃcient of 0.01 yields a 0.15 increase in average disagreement between the 10s and the 25s time condition.
Time adherence. Figure 2(b) suggests that the participants adhere reasonably to the four diﬀerent time conditions used. We note that this time reﬂects the maximum time taken to click the radio button (in case of multiple clicks), but the participants may have spent more time thinking over their decision. In the survey at the end of the study, we asked the participants how often they used the entire time available to them in the trials, and obtained the following distribution of answers — Frequently 15, Occasionally 24, Rarely 6, Never 2.

5 Optimal resource allocation in human-AI collaboration
In Section 4, we see that time is a useful resource for de-anchoring the decision-maker. More generally, there are many works that study de-biasing techniques to address the negative eﬀects of cognitive biases. These de-biasing techniques require resources such as time, computation and explanation strategies. Thus, in this section, we model the problem of mitigating the eﬀect of cognitive biases in the AI-assisted decision-making setting as a resource allocation problem, where our aim is to eﬃciently use the resources available and improve human-AI collaboration accuracy.

5.1 Resource allocation problem

From Experiment 1 in Section 4.1, we learnt that given more time, the decision-maker is more likely to

adjust away from the anchor (AI decision) if the decision-maker has reason to believe that the correct

answer is diﬀerent. This is shown by change in their probability of agreement with the AI prediction,

denoted by Pa = P(y˜ = y). The results of Experiment 1 indicate that, ideally, decision-makers should

be provided with ample time for each decision. However, in practice, given a ﬁnite resource budget

T, we also have the constraint

N i=1

Ti

=

T.

Thus,

we

formulate

a

resource

allocation

problem

that

captures the trade-oﬀ between time and accuracy. More generally, this problem suggests a framework

for optimizing human-AI team performance using constrained resources to de-bias the decision-maker.

In our setup, the human decision-maker has to provide a ﬁnal decision y˜i for N total trials with AI

assistance, speciﬁcally in the form of a predicted label yi. The objective is to maximize the average

accuracy over the trials, denoted by R, of the human-AI collaboration: E[R] = 1 N

N i=1

E[Ri],

where

Ri is an indicator of human-AI correctness in trial i.

We ﬁrst relate collaborative accuracy E[R] to the anchoring-and-adjustment heuristic. Intuitively, if

we know the AI to be incorrect in a given trial, we should aim to facilitate adjustment away from the

anchor as much as possible, whereas if AI is known to be correct, then anchoring bias is actually beneﬁcial.

Based on this intuition, E[Ri] can be rewritten by conditioning on AI correctness/incorrectness as

follows:

E[Ri] = P(y˜i = yi | yi = yi∗) P(yi = yi∗) + 1 − P(y˜i = yi | yi = yi∗) (1 − P(yi = yi∗)) .

(7)

Pari

Pawi

We see therefore that human-AI correctness depends on the probability of agreement Pari conditioned on AI being correct and the probability of agreement Pawi conditioned on AI being incorrect. Recalling from Section 4 the link established between agreement probability and anchoring bias, (7) shows the

10

1.0

Accuracy

AI

Human

0.0 CL

Human+AI CH

AI Confidence

Figure 3. An ideal case for human-AI collaboration, where (1) we correctly identify the set of tasks with low and high AI conﬁdence, (2) the AI accuracy is perfectly correlated with its conﬁdence, (3) human accuracy is higher than AI in the low conﬁdence region, CL, and lower than AI in the high conﬁdence region CH .

eﬀect of anchoring bias on human-AI accuracy. Speciﬁcally in the case of (7), the eﬀect is through the two conditional agreement probabilities Pari and Pawi .
We consider time allocation strategies to modify agreement probabilities and thus improve collabo-
rative accuracy, based on the relationship established in Experiment 1.
We denote the time used in trial i as Ti, which impacts correctness Ri (7) as follows:

E[Ri | Ti] = Pari (Ti)P(yi = yi∗) + Pawi (Ti) (1 − P(yi = yi∗)) . (8)

The allocation of time aﬀects only the human decision-maker, making the agreement probabilities

functions of Ti, whereas the probability of AI correctness is unaﬀected. The resource allocation problem

would then be to maximize the average of (8) over trials subject to the budget constraint

n i=1

Ti

=

T.

The challenge with formulation (8) is that it requires identifying the true probability of AI correctness,

which is a non-trivial task (Guo et al., 2017). Instead, we operate under the more realistic assumption

that the AI model can estimate its probability of correctness from the class probabilities that it predicts

(as provided for example by a logistic regression model). We refer to this estimate as AI conﬁdence

and denote it as Ci. We may then consider a decomposition of human-AI correctness as in (7), (8) but

conditioned on Ci. In keeping with the two cases in (7), (8) and to simplify the allocation strategy,

we binarize Ci into two intervals, low conﬁdence Ci ∈ CL, and high conﬁdence Ci ∈ CH . The time

allocated is then Ti(Ci) = tL for Ci ∈ CL and Ti(Ci) = tH for Ci ∈ CH . Thus we have

E[Ri] = P(Ci ∈ CL)E[Ri | Ci ∈ CL, Ti = tL] + P(Ci ∈ CH )E[Ri | Ci ∈ CH , Ti = tH ].

(9)

The quantities E[Ri | Ci ∈ C, Ti], C = CL, CH , are not pure agreement probabilities as in (8) because
the low/high-conﬁdence events Ci ∈ CL, Ci ∈ CH generally diﬀer from the correctness/incorrectness events yi = yi∗, yi = yi∗. Nevertheless, since we expect these events to be correlated, E[Ri | Ci ∈ C, Ti] is related to the agreement probabilities in (8).
Figure 3 presents an ideal scenario that one hopes to attain in (9). In presence of anchoring bias, our aim is to achieve the human-AI team accuracy shown. This approach capitalises on human expertise where AI accuracy is low. Speciﬁcally, by giving human decision-makers more time, we encourage them to rely on their own knowledge (de-anchor from the AI prediction) when the AI is less conﬁdent,
Ci ∈ CL. Usage of more time in low AI conﬁdence tasks, implies less time in tasks where AI is more conﬁdent, Ci ∈ CH , where anchoring bias has lower negative eﬀects and is even beneﬁcial. Thus,

11

this two-level AI conﬁdence based time allocation policy allows us to mitigate the negative eﬀects of anchoring bias and achieve the “best of both worlds”, as illustrated in Figure 3.
We now formally write the assumption under which the optimal time allocation policy is straightforward to see. Assumption 1. For any t1, t2 ∈ R+, if t1 < t2, then
E[Ri | Ci ∈ CL, Ti = t1] ≤ E[Ri | Ci ∈ CL, Ti = t2], and (10)
E[Ri | Ci ∈ CH , Ti = t1] ≥ E[Ri | Ci ∈ CH , Ti = t2].
We provide explanation for Assumption 1 (10) in Appendix A. Under Assumption 1, the optimal strategy is to maximize time for low-AI-conﬁdence trials and minimize time for high-conﬁdence trials, as is stated formally below.
Proposition 1. Consider the AI-assisted decision-making setup discussed in this work with N trials where the total time available is T . Suppose Assumption 1, stated in (10), holds true for human-AI accuracy. Then, the optimal conﬁdence-based allocation is as follows,

Ti = tH = tmin if Ci ∈ CH

(11)

tL = tmax if Ci ∈ CL,

where tmin is the minimum allowable time, and tmax is the corresponding maximum time such that tmaxP(Ci ∈ CL) + tminP(Ci ∈ CH ) = NT .

Proposition 1 gives us the optimal time allocation strategy by eﬃciently allocating more time for adjusting away from the anchor in the tasks that yield a lower probability of accuracy if the human is anchored to the AI predictions. We note that, although in an ideal scenario as shown in Figure 3, we should set tmin = 0, in real world implementation Ci is an approximation of the true conﬁdence, and hence, it would be helpful to have human oversight with tmin > 0 in case the AI conﬁdence is poorly calibrated.
To further understand the optimality of the conﬁdence-based time allocation strategy, we compare it with two baseline strategies that obey the same resource constraint, namely, Constant time and Random time strategies, deﬁned as –

•

Constant

time:

For

all

i,

Ti

=

T N

.

• Random time : Out of the N trials, N × P(Ci ∈ CL) trials are selected randomly and allocated time tL. The remaining trials are allocated time tH .
Constant time is the most natural baseline allocation, while Random time assigns the same values tL and tH as the conﬁdence-based policy but does so at random. Both are evaluated in the experiment described in Section 5.2.

5.2 Experiment 2: Dynamic time allocation for human-AI collaboration
In this experiment, we implement our conﬁdence-based time allocation strategy for human-AI collaboration in a user study deployed on Amazon Mechanical Turk. Based on the results of Experiment 1 shown in Figure 2(a), we assign tL = 25s and tH = 10s.
In addition, we conjecture that giving the decision-maker the reasoning behind the time allocation, that is, informing them about AI conﬁdence and then allocating time accordingly, would help improve the collaboration further. This conjecture is supported by ﬁndings in (Chambon et al., 2020), where the authors observe that choice tips the balance of learning: for the same action and outcome, the brain learns diﬀerently and more quickly from free choices than forced ones. Thus, providing valid reasons for time allocation would help the decision-maker make an active choice and hence, learn to collaborate with AI better.
In this experiment, we test the following hypotheses.

12

• H2: Anchoring bias has a negative eﬀect on human-AI collaborative decision-making accuracy when AI is incorrect.
• H3: If the human decision-maker has complementary knowledge then allocating more time can help them suﬃciently adjust away from the AI prediction.
• H4 : Conﬁdence-based time allocation yields better performance than Human alone and AI alone.
• H5: Conﬁdence-based time allocation yields better human-AI team performance than constant time and random time allocations.
• H6: Conﬁdence-based time allocation with explanation yields better human-AI team performance than the other conditions.
We now describe the diﬀerent components of Experiment 2 in detail.
Participants. In this study, 479 participants were recruited in the same manner as described in Section 4.1. 83 participants were between ages 18 and 29, 209 between ages 30 and 39, 117 between ages 40 and 49, and 70 over age 50. The average completion time for this user study was 30 minutes, and participants received compensation of $5.125 on average (roughly equals an hourly wage of $10.25). The participants received an average base pay of $4.125 and bonus of $1 (to incentivize accuracy).
Task and AI model. The binary prediction task in this study is the same as the student performance prediction task used before. In this experiment, our goal is to induce optimal human-AI collaboration under the assumptions illustrated in Figure 3. In real-world human-AI collaborations, it is not uncommon for the decision-maker to have some domain expertise or complementary knowledge that the AI does not, especially in ﬁelds where there is not enough data such as social policy-making and design. To emulate this situation where the participants have complementary knowledge, we reduced the information available to the AI, given the unavailability of human experts and the limited training time in our experiment. We train the assisting AI model over 7 features, while the participants have access to 3 more features, namely, hours spent studying weekly, hours spent going out with friends weekly, and enrollment in extra educational support. These 3 features were the second to fourth most important ones as deemed by a full model.
To implement the conﬁdence-based time allocation strategy, we had to identify trials belonging to classes CL and CH . Ideally, for this we require a machine learning algorithm that can calibrate its conﬁdence correctly. As discussed in Section 5.1, we use the AI’s predicted probability Ci (termed as AI conﬁdence) and choose the threshold for CH as Ci ≥ 0.75. This study has 40 questions in the testing section, from which 20 belong to CL and 20 belong to CH .
Study procedure. As in Experiment 1, this user study has two sections, the training section and the testing section. The training section is exactly the same as before where the participants are trained over 15 examples selected from the training dataset. To induce anchoring bias, as in Experiment 1, we reinforce that the AI predictions are 85% accurate in the training section.
The testing section has 40 trials, which are sampled randomly from the test set such that the associated predicted probability values (of the predicted class) estimated by the machine learning algorithm are distributed uniformly. While the set of trials in the testing section is ﬁxed for all participants, the order they were presented in was varied randomly.
To test hypotheses H2, H3, H4, H5 and H6, we randomly assigned each participant to one of ﬁve groups:
1. Human only: In this group, the participants were asked to provide their prediction without the help of the AI prediction. The time allocation for each trial in the testing section is ﬁxed at 25
13

seconds. This time is judged to be suﬃcient for humans to make a prediction on their own, based on the results of Experiment 1 (for example the time usage distributions in Figure 2).

2. Constant time: In this group, the participants were asked to provide their prediction with the

help of the AI prediction. The time allocation for each trial in the testing section is ﬁxed as

tL +tH 2

= 17.5

seconds.

We

rounded

this

to

18

seconds

when

reporting

it

to

the

participants.

3. Random time: This group has all factors the same as the constant time group except for the time allocation. For each participant, the time allocation for each trial is chosen uniformly at random from the set {10, 25} such that the average time allocated per trial is 17.5 seconds.

4. Conﬁdence-based time: This is our treatment group, where we assign time according to conﬁdence-based time allocation, tL = 25 seconds and tH = 10 seconds, described in Section 5.1.

5. Conﬁdence-based time with explanation: This is our second treatment group, where in addition to conﬁdence-based time allocation, we provide the AI conﬁdence (“low” or “high”) corresponding to each trial.

Out of the 479 participants, 95 were in "Human only", 109 were in "Constant time", 95 were in "Random time", 85 were in "Conﬁdence-based time" and 96 were in "Conﬁdence-based time with explanation". In the groups where participants switch between 10-second and 25-second conditions the accuracy would likely be aﬀected by rapid switching between the two time conditions. Hence, we created blocks of 5 trials with the same time allocation for both groups. The complete testing section contained 8 such blocks. This concludes the description of Experiment 2.

5.3 Results
Figure 4 shows that our eﬀort to create a scenario where the AI knowledge is complementary to human knowledge is successful because the AI only and "Human only" conditions have similar overall accuracy (around 60%, black diamonds), and yet humans only agreed with the AI in 62.3% of the trials. Moreover, on trials where AI is incorrect, "Human only" has accuracy of 61.8% on trials in CL, and 29.8% on trials in CH . Thus, the participants showed more complementary knowledge in trials in CL compared to CH .
Given this successful setup of complementary knowledge between humans and AI, there is good potential for the human-AI partnership groups, especially the "Conﬁdence-based time" group and the "Conﬁdence-based time with explanation" group, to outperform the AI only or "Human only" groups (H4). In Figure 4(a), we see that the mean accuracy of the human-AI team is 61% in "Conﬁdence-based time" and 61.9% in "Conﬁdence-based time with explanation" while the accuracy of "Human only" is 61.9% and the accuracy of the AI model is 60%. Thus, regarding H4, the results suggest that the accuracy in "Conﬁdence-based time" is greater than AI alone (p = 0.06, t(183) = 1.52), whereas they do not provide suﬃcient evidence for "Conﬁdence-based time" being better than "Human only" (p = 0.58, t(92) = −0.21). Similarly, regarding H6, the results suggest that the accuracy in "Conﬁdencebased time with explanation" is better than AI alone (p = 0.004, t(194) = 2.66), whereas for "Human only" the results are not statistically signiﬁcant (p = 0.5, t(189) = −0.02).
However, we see that anchoring bias aﬀected overall team performance negatively when the AI is incorrect (H2). Figure 4(b) shows evidence of anchoring, the agreement percentage in the "Human only" group is much lower than those in the collaborative conditions (p < 0.001, t(184) = 6.73). When the AI was incorrect (red triangles and green circles), this anchoring bias clearly reduced team accuracy when compared to the "Human only" accuracy (p < 0.001, t(370) = −6.68). Although, it is important to note that the "Human only" group received longer time (25s) than the collaborative conditions on average. Nevertheless, if we just compare "Human only" and "Conﬁdence-based time" within the low conﬁdence trials (red triangles), where both were assigned the same amount of time(25s), we observe similar disparity in agreement percentages (p < 0.001, t(92) = 4.97) and accuracy (p < 0.001, t(92) = −4.74). Hence, the results are consistent with H2.

14

AI only
Conf-based time+Explanation
Conf-based time
Random time

Constant time

Human only
0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40 0.45 0.50 0.55 0.60 0.65 0.70 0.75 0.80 0.85 0.90 0.95 1.00
Accuracy

(a) Average accuracy

Conf-based time+Explanation
Conf-based time
Random time
Constant time

All
CL, y y * CL, y = y * CH, y y * CH, y = y *

Human only
0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40 0.45 0.50 0.55 0.60 0.65 0.70 0.75 0.80 0.85 0.90 0.95 1.00
Agreement
(b) Average agreement with AI Figure 4. Average accuracy and agreement ratio of participants in Experiment 2 across the four diﬀerent conditions, marked on the y-axis. We note that the error bars in (a) for ’All’ trials (black diamonds) are smaller than the marker size.

15

Regarding H3, we see that while "Conﬁdence-based time" alone did not lead to suﬃcient adjustment away from the AI when it was incorrect, "Conﬁdence-based time with explanation" showed signiﬁcant reduction in anchoring bias in the low conﬁdence trials (red triangles) compared to the other conditions (p = 0.003, t(383) = 2.70), which suggests that giving people more time along with an explanation for the time helped them adjust away from the anchor suﬃciently, in these trials (H3). This de-anchoring also led to higher accuracy in these trials(red triangles) for "Conﬁdence-based time with explanation" (43.8%) when compared to the other three collaborative conditions with "Random time" at 36.2%, "Constant time"at 36.4% and "Conﬁdence-based time" at 37.5%. Note that the set of conditions chosen in our experiment does not allow us to separately quantify the eﬀect of the time-based allocation strategy and the conﬁdence-based explanation; we discuss this in Section 6.
Next, we examine the diﬀerences between the four collaborative groups. Figure 4(a) shows that the average accuracy over all trials (black diamonds) is highest for "Conﬁdence-based time with explanation" at 61.9% with "Conﬁdence-based time" at 61%, "Random time" at 61.1% and "Constant time" at 61.5%. Regarding H5, we see that "Conﬁdence-based time" does not have signiﬁcantly diﬀerent accuracy from the other collaborative groups. Finally, regarding H6, we observe that "Conﬁdence-based time with explanation" has the highest accuracy, although the eﬀect is not statistically signiﬁcant (p = 0.19, t(383) = 0.84). We note that the outcomes in all collaborative conditions are similar in all trials except trials where AI is incorrect and has low conﬁdence, and in these trials our treatment group has signiﬁcantly higher accuracy. This implies that in settings prone to over-reliance on AI, "Conﬁdence-based time with explanation" helps improve human-AI team performance.
The reason that the overall accuracy of "Conﬁdence-based time" is not signiﬁcantly better than the other two collaborative conditions is likely because of the relatively low accuracy and low agreement percentage in trials in CH (green circles, purple pentagons). Based on the results of Experiment 1, we expected that the agreement percentage for the 10-second trials would be high and since these align with the high AI conﬁdence trials for "Conﬁdence-based time", we expected these trials to have a high agreement percentage and hence high accuracy. Instead, we observed that "Conﬁdence-based time" has low agreement percentage (84%) in CH , compared to "Random time" (87.9%), and "Constant time"(88.1%), both having an average time allocation of 17.5 seconds. This lower agreement percentage translates into lower accuracy (86%) when AI is correct (purple pentagons). In the next section, we discuss how this points to possible distrust of AI in these high conﬁdence trials and its implications. For "Conﬁdence-based time with explanation" we observe that the participants in this group are able to de-anchor from incorrect low conﬁdence AI predictions, to give higher mean accuracy than other collaborative conditions, albeit the diﬀerence is not statistically signiﬁcant.
6 Discussion
Lessons learned. We now discuss some of the lessons learned from the results obtained in Experiment 2. As noted in Section 5.3, we see that "Conﬁdence-based time" has a low agreement rate on trials in CH where the time allocated is 10 seconds and the AI prediction is 70% accurate. Moreover, we see that the agreement rate is lower than "Human only" and "Constant time" on trials in CH , where the AI prediction is correct as well as where the AI prediction is incorrect. This behavior suggests that the participants in "Conﬁdence-based time" may have grown to distrust the AI, as they disagreed more with the AI on average and spent more time on the trials where they disagreed. The distrust may be due to "Conﬁdence-based time" assigning longer times (25s) only to low-AI-conﬁdence trials, perhaps giving the impression that the AI is worse than it really is. However, these eﬀects are reduced by providing an explanation for the time allocation in "Conﬁdence-based time with explanation". Our observations highlight the importance of accounting for human behaviour in such collaborative decision-making tasks.
Another insight gained from Experiment 2 is that the model should take into account the sequentiality of decision-making where the decision-maker continues to learn and build their perception of the AI as
16

the task progresses, based on their interaction with the AI. Dynamic Markov models have been studied previously in the context of human decision-making (Busemeyer et al., 2020; Lieder et al., 2018). We believe that studying dynamic cognitive models that are cognizant of the changing interaction between the human and the AI model would help create more informed policies for human-AI collaboration.
Limitations. One limitation of our study is that our participants are not experts in student assessment. To mitigate this problem we ﬁrst trained the participants in the task and showed them the statistics of the problem domain. We also showed more features to the human users, compared to the AI, to give them complementary knowledge. The fact that human-only accuracy in Experiment 2 is roughly the same as the AI-only accuracy suggests that these domain-knowledge enhancement measures were eﬀective. Secondly, we proposed a time-based strategy and conducted Experiment 1 to validate our hypothesis and select the appropriate time durations (10s, 25s) for our second experiment. Due to limited resources, we did not extend our search space beyond four settings – (10s, 15s, 20s, 25s). Although it is desirable to conduct the experiment with real experts, this can be extremely expensive. Our approach can be considered as "human grounded evaluation" Doshi-Velez and Kim (2017), a valid approach by using lay people as a "proxy" to understand the general behavioral patterns. We used a non-critical decision-making task where the participants would not be held responsible for the consequences of their decisions. This problem was mitigated by introducing an outcome-based bonus reward which motivates optimal decision-making. Our work considers the eﬀect of our time allocation strategy with and without the conﬁdence-based explanation through the treatment groups in experiment 2. While this helps us investigate the beneﬁts of the time allocation strategy, we cannot separate out the independent eﬀect of the conﬁdence-based explanation strategy. Lastly, our work focuses on a single decision-making task. Additional work is needed to examine if the eﬀects we observe generalize across domains and settings. However, prior research provides ample evidence that even experts making critical decisions resort to heuristic thinking, which suggests that our results will generalize broadly.
Conclusions. In this work, we foreground the role of cognitive biases in the human-AI collaborative decision-making setting. Through literature in cognitive science and psychology, we explore several biases and present mathematical models of their eﬀect on collaborative decision-making. We focus on anchoring bias and the associated anchoring-and-adjustment heuristic that is important towards optimizing team performance. We validate the use of time as an eﬀective strategy for mitigating anchoring bias through a user study. Furthermore, through a time-based resource allocation formulation, we provide an optimal allocation strategy that attempts to achieve the "best of both worlds" by capitalizing on the complementary knowledge presented by the decision-maker and the AI model. Using this strategy, we obtain human-AI team performance that is better than the AI alone, as well as better than having only the human decide in cases where the AI predicts correctly. When the AI is incorrect, the information it provides the human distracts them from the correct decision, thus reducing their performance. Giving them information about the AI conﬁdence as explanation for the time allocation alleviates some of these issues and brings us closer to the ideal Human-AI team performance shown in Figure 3.
Future work. Our work shows that a time-based strategy with explanation, built on the cognitive tendencies of the decision-maker in a collaborative setting, can help decision-makers adjust their decisions correctly. More generally, our work showcases the importance of accounting for cognitive biases in decision-making, where in the future we would want to study other important biases such as conﬁrmation bias or weak evidence eﬀect. This paper opens up several directions for future work where explanation strategies in this collaborative setting are studied and designed based on the cognitive biases of the human decision-maker. Another interesting direction is to utilize the resource allocation framework for other cognitive biases based on their de-biasing strategies.
Acknowledgements. The work of Charvi Rastogi was supported in part by NSF CIF 1763734.
17

References
Adadi, A. and Berrada, M. (2018). Peeking inside the black-box: A survey on explainable artiﬁcial intelligence (xai). IEEE Access, 6:52138–52160.
Arnold, M., Bellamy, R. K., Hind, M., Houde, S., Mehta, S., Mojsilović, A., Nair, R., Ramamurthy, K. N., Olteanu, A., Piorkowski, D., et al. (2019). Factsheets: Increasing trust in ai services through supplier’s declarations of conformity. IBM Journal of Research and Development, 63(4/5):6–1.
Arnott, D. (2006). Cognitive biases and decision support systems development: a design science approach. Information Systems Journal, 16(1):55–78.
Bansal, G., Nushi, B., Kamar, E., Weld, D. S., Lasecki, W. S., and Horvitz, E. (2019). Updates in human-ai teams: Understanding and addressing the performance/compatibility tradeoﬀ. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 33, pages 2429–2437.
Bansal, G., Wu, T., Zhou, J., Fok, R., Nushi, B., Kamar, E., Ribeiro, M. T., and Weld, D. (2021). Does the whole exceed its parts? the eﬀect of ai explanations on complementary team performance. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems, pages 1–16.
Barnes JR., J. H. (1984). Cognitive biases and their impact on strategic planning. Strategic Management Journal, 5(2):129–137.
Baudel, T., Verbockhaven, M., Roy, G., Cousergue, V., and Laarach, R. (2020). Addressing cognitive biases in augmented business decision systems. arXiv preprint arXiv:2009.08127.
Bromme, R., Hesse, F. W., and Spada, H. (2010). Barriers and Biases in Computer-Mediated Knowledge Communication: And How They May Be Overcome. Springer Publishing Company, Incorporated, 1st edition.
Buçinca, Z., Malaya, M. B., and Gajos, K. Z. (2021). To trust or to think: Cognitive forcing functions canreduce overreliance on ai in ai-assisted decision-making. Proc. ACM Hum.-Comput. Interact. 5, CSCW1, 5:21.
Busemeyer, J. R., Kvam, P. D., and Pleskac, T. J. (2020). Comparison of markov versus quantum dynamical models of human decision making. Wiley Interdisciplinary Reviews: Cognitive Science.
Chambon, V., Théro, H., Vidal, M., Vandendriessche, H., Haggard, P., and Palminteri, S. (2020). Information about action outcomes diﬀerentially aﬀects learning from self-determined versus imposed choices. Nature Human Behavior.
Chater, N., Tenenbaum, J. B., and Yuille, A. (2006). Probabilistic models of cognition: Conceptual foundations. Trends in Cognitive Sciences, 10(7):287 – 291. Special issue: Probabilistic models of cognition.
Cortez, P. and Silva, A. M. G. (2008). Using data mining to predict secondary school student performance. EUROSIS-ETI.
Das, T. and Teng, B.-S. (1999). Cognitive biases and strategic decision processes: An integrative perspective. Journal of Management Studies, 36(6):757–778.
Dhurandhar, A., Graves, B., Ravi, R. K., Maniachari, G., and Ettl, M. (2015). Big data system for analyzing risky procurement entities. In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Sydney, NSW, Australia, August 10-13, 2015, pages 1741–1750. ACM.
18

Doshi-Velez, F. and Kim, B. (2017). Towards a rigorous science of interpretable machine learning. arXiv preprint arXiv:1702.08608.
Ehrlinger, J., Readinger, W., and Kim, B. (2016). Decision-making and cognitive biases. Encyclopedia of Mental Health.
Englich, B., Mussweiler, T., and Strack, F. (2006). Playing dice with criminal sentences: The inﬂuence of irrelevant anchors on experts’ judicial decision making. Personality and Social Psychology Bulletin, 32(2):188–200.
Epley, N. and Gilovich, T. (2001). Putting adjustment back in the anchoring and adjustment heuristic: Diﬀerential processing of self-generated and experimenter-provided anchors. Psychological science, 12(5):391–396.
Epley, N. and Gilovich, T. (2006). The anchoring-and-adjustment heuristic: Why the adjustments are insuﬃcient. Psychological Science, 17(4):311–318. PMID: 16623688.
Fernbach, P., Darlow, A., and Sloman, S. (2011). When good evidence goes bad: The weak evidence eﬀect in judgment and decision-making. Cognition, 119:459–67.
Furnham, A. and Boo, H. (2011). A literature review of the anchoring eﬀect. The Journal of Socio-Economics, 40:35–42.
Fürnkranz, J., Kliegr, T., and Paulheim, H. (2020). On cognitive preferences and the plausibility of rule-based models. Machine Learning, 109(4):853–898.
Green, B. and Chen, Y. (2019). The principles and limits of algorithm-in-the-loop decision making. Proceedings of the ACM on Human-Computer Interaction, 3(CSCW):1–24.
Griﬃths, T. L. and Tenenbaum, J. B. (2006). Optimal predictions in everyday cognition. Psychological Science, 17(9):767–773.
Guo, C., Pleiss, G., Sun, Y., and Weinberger, K. Q. (2017). On calibration of modern neural networks. In International Conference on Machine Learning, pages 1321–1330.
Janssen, J. and Kirschner, P. (2020). Applying collaborative cognitive load theory to computersupported collaborative learning: towards a research agenda. Educational Technology Research and Development, pages 1–23.
Klayman, J. (1995). Varieties of conﬁrmation bias. In Psychology of learning and motivation, volume 32, pages 385–418. Elsevier.
Kliegr, T., Bahník, Š., and Fürnkranz, J. (2018). A review of possible eﬀects of cognitive biases on interpretation of rule-based machine learning models. arXiv preprint arXiv:1804.02969.
Lai, V., Liu, H., and Tan, C. (2020). Why is ‘chicago’ deceptive? towards building model-driven tutorials for humans. In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems, pages 1–13.
Lai, V. and Tan, C. (2019). On human predictions with explanations and predictions of machine learning models: A case study on deception detection. In Proceedings of the Conference on Fairness, Accountability, and Transparency, FAT* ’19, page 29–38, New York, NY, USA. Association for Computing Machinery.
Lee, J. D. and See, K. A. (2004). Trust in automation: Designing for appropriate reliance. Human Factors, 46(1):50–80. PMID: 15151155.
19

Lieder, F., Griﬃths, T. L., Huys, Q. J. M., and Goodman, N. D. (2018). The anchoring bias reﬂects rational use of cognitive resources. Psychonomic Bulletin and Review, 25:322–349.
Lipton, Z. C. (2018). The mythos of model interpretability. Queue, 16(3):31–57.
Matsumori, K., Koike, Y., and Matsumoto, K. (2018). A biased bayesian inference for decision-making and cognitive control. Frontiers in Neuroscience, 12:734.
Miller, T. (2019). Explanation in artiﬁcial intelligence: Insights from the social sciences. Artiﬁcial Intelligence, 267:1–38.
Nickerson, R. S. (1998). Conﬁrmation bias: A ubiquitous phenomenon in many guises. Review of General Psychology, 2:175 – 220.
Okamura, K. and Yamada, S. (2020). Adaptive trust calibration for human-ai collaboration. PLOS ONE, 15(2):1–20.
Oswald, M. and Grosjean, S. (2004). Conﬁrmation bias. In R. F. Pohl (Ed.). Cognitive Illusions. A Handbook on Fallacies and Biases in Thinking, Judgement and Memory, pages 79–96. Hove and N.Y.: Psychology Press.
Park, J. S., Barber, R., Kirlik, A., and Karahalios, K. (2019). A slow algorithm improves users’ assessments of the algorithm’s accuracy. Proc. ACM Hum.-Comput. Interact., 3(CSCW).
Payzan-LeNestour, E. and Bossaerts, P. (2011). Risk, unexpected uncertainty, and estimation uncertainty: Bayesian learning in unstable settings. PLoS Comput Biol, 7(1):e1001048.
Payzan-LeNestour, E. and Bossaerts, P. (2012). Do not bet on the unknown versus try to ﬁnd out more: Estimation uncertainty and “unexpected uncertainty” both modulate exploration. Frontiers in Neuroscience, 6:150.
Phillips-Wren, G., Power, D. J., and Mora, M. (2019). Cognitive bias, decision styles, and risk attitudes in decision making and dss. Journal of Decision Systems, 28(2):63–66.
Poursabzi-Sangdeh, F., Goldstein, D. G., Hofman, J. M., Vaughan, J. W., and Wallach, H. (2018). Manipulating and measuring model interpretability. arXiv preprint arXiv:1802.07810.
Preece, A. (2018). Asking ‘why’in ai: Explainability of intelligent systems–perspectives and challenges. Intelligent Systems in Accounting, Finance and Management, 25(2):63–72.
Siau, K. and Wang, W. (2018). Building trust in artiﬁcial intelligence, machine learning, and robotics. Cutter Business Technology Journal, 31:47–53.
Silverman, B. G. (1992). Human-computer collaboration. Human–Computer Interaction, 7(2):165–196.
Simon, H. A. (1956). Rational choice and the structure of the environment. Psychological review, 63(2):129.
Simon, H. A. (1972). Theories of bounded rationality. Decision and Organization, 1(1):161–176.
Solomon, J. (2014). Customization bias in decision support systems. In Proceedings of the SIGCHI conference on human factors in computing systems, pages 3065–3074.
Springer, A., Hollis, V., and Whittaker, S. (2018). Dice in the black box: User experiences with an inscrutable algorithm. arXiv preprint arXiv:1812.03219.
Tenenbaum, J. B. (1999). Bayesian modeling of human concept learning. In Advances in neural information processing systems, pages 59–68.
20

Tomsett, R., Preece, A., Braines, D., Cerutti, F., Chakraborty, S., Srivastava, M., Pearson, G., and Kaplan, L. (2020). Rapid trust calibration through interpretable and uncertainty-aware ai. Patterns, 1(4):100049.
Tversky, A. and Kahneman, D. (1973). Availability: A heuristic for judging frequency and probability. Cognitive psychology, 5(2):207–232.
Tversky, A. and Kahneman, D. (1974). Judgment under uncertainty: Heuristics and biases. Science, 185(4157):1124–1131.
Wang, D., Yang, Q., Abdul, A., and Lim, B. Y. (2019). Designing theory-driven user-centric explainable ai. In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems, CHI ’19, page 1–15, New York, NY, USA. Association for Computing Machinery.
Yeom, S. and Tschantz, M. C. (2018). Discriminative but not discriminatory: A comparison of fairness deﬁnitions under diﬀerent worldviews. arXiv preprint arXiv:1808.08619.
Zhang, Y., Bellamy, R. K., and Kellogg, W. A. (2015). Designing information for remediating cognitive biases in decision-making. In Proceedings of the 33rd annual ACM conference on human factors in computing systems, pages 2211–2220.
Zhang, Y., Liao, Q. V., and Bellamy, R. K. (2020). Eﬀect of conﬁdence and explanation on accuracy and trust calibration in ai-assisted decision making. In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency, pages 295–305.

Appendix

A Additional details of optimal resource allocation
Following from the discussion in Section 5.1, in this section we provide additional details about Assumption 1 and the optimality of the conﬁdence-based time allocation policy proposed thereafter.

Reasoning for Assumption 1. To see how Assumption 1 (10) might hold, we refer ﬁrst to Figure 3,
which assumes that human accuracy is higher than AI accuracy when conﬁdence is low, Ci ∈ CL, and lower when Ci ∈ CH . (Human accuracy does not have to be uniformly higher/lower in CL/CH as Figure 3 suggests.) At t = 0, E[Ri | Ci ∈ C, Ti = 0] is equal to AI accuracy conditioned on C = CL, CH , and by giving the human more time to de-anchor, we might expect E[Ri | Ci ∈ CL, Ti = t] to increase and E[Ri | Ci ∈ CH , Ti = t] to decrease. A second way to understand Assumption 1 is to break down the conditional accuracy into two parts:

E[Ri | Ci ∈ C, Ti = t] = P(y˜i = yi | yi = yi∗, Ci ∈ C, Ti = t)P(yi = yi∗ | Ci ∈ C)

+ P(y˜i = yi | yi = yi∗, Ci ∈ C, Ti = t)P(yi = yi∗ | Ci ∈ C),

(12)

for C = CL, CH . The results of Section 4.1 indicate that the disagreement probability in the second RHS term in (12) increases or stays the same with time t, and the agreement probability in the ﬁrst term decreases or stays the same with t. For C = CL, assuming positive correlation between low conﬁdence Ci ∈ CL and low accuracy (not necessarily the perfect correlation in Figure 3), the probability P(yi = yi∗ | Ci ∈ CL) tends to be larger and the second term dominates, resulting in the

21

LHS increasing with t. For C = CH , the ﬁrst term tends to dominate, leading to decrease with t.

To show why the conﬁdence-based policy has higher accuracy, we provide the following corollary.
Corollary 1. Consider the two time allocation strategies deﬁned above - (1) Constant time and (2) Random time, in AI-assisted decision-making where we have total N trials and time T. Suppose Assumption 1 stated in (10) holds. Then the human-AI accuracy of the conﬁdence-based time allocation policy is greater than or equal to the accuracy of the constant time allocation and random time allocation strategies.
Proof. Let the two-level conﬁdence based allocation policy be denoted by π. Now, we have that the accuracy for each round under this policy,

Eπ[Ri] = E E[Ri | Ci, Ti = π(Ci)]

= P(Ci ∈ CL)E[Ri | Ci ∈ CL, Ti = tL] + P(Ci ∈ CH )E[Ri | Ci ∈ CH , Ti = tH ].

(13)

For the constant allocation policy, we get

T Econst[Ri] = E[Ri | Ti = N ]

= P(Ci ∈ CL)E[Ri | Ci ∈ CL, Ti = T ] + P(Ci ∈ CH )E[Ri | Ci ∈ CH , Ti = T ]. (14)

N

N

Now,

according

to

assumption

1

in

(10),

we

have

E[Ri | Ci

∈

CL, Ti

=

tL]

≥

E[Ri | Ci

∈

CL, Ti

=

T N

],

and

E[Ri | Ci

∈

CH , Ti

=

tH ]

≥

E[Ri | Ci

∈

CH , Ti

=

T N

].

Thus,

Eπ [Ri ]

≥

Econst[Ri].

Similarly

for

random

allocation, we have

Erand[Ri] = P(Ti = tL)E[Ri | Ti = tL] + P(Ti = tH )E[Ri | Ti = tH ]

= P(Ci ∈ CL)P(Ti = tL)E[Ri | Ci ∈ CL, Ti = tL]

+ P(Ci ∈ CH )P(Ti = tL)E[Ri | Ci ∈ CH , Ti = tL]

+ P(Ci ∈ CL)P(Ti = tH )E[Ri | Ci ∈ CL, Ti = tH ]

+ P(Ci ∈ CH )P(Ti = tH )E[Ri | Ci ∈ CH , Ti = tH ].

(15)

Using the assumptions (10) as stated for constant allocation, we prove that Eπ[Ri] ≥ Erand[Ri].

22

