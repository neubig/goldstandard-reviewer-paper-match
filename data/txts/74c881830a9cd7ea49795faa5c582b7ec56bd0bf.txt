An Investigation of End-to-End Multichannel Speech Recognition for Reverberant and Mismatch Conditions
Aswin Shanmugam Subramanian1, Xiaofei Wang1, Shinji Watanabe1 Toru Taniguchi2, Dung Tran2, Yuya Fujita2
1Center for Language and Speech Processing, Johns Hopkins University, Baltimore, MD, USA 2Yahoo Japan Corporation, Tokyo, Japan
{aswin, xiaofeiwang, shinjiw}@jhu.edu, {ttaniguc, tdung, yuyfujit}@yahoo-corp.jp

arXiv:1904.09049v3 [eess.AS] 28 Apr 2019

Abstract
Sequence-to-sequence (S2S) modeling is becoming a popular paradigm for automatic speech recognition (ASR) because of its ability to jointly optimize all the conventional ASR components in an end-to-end (E2E) fashion. This report investigates the ability of E2E ASR from standard close-talk to far-ﬁeld applications by encompassing entire multichannel speech enhancement and ASR components within the S2S model. There have been previous studies on jointly optimizing neural beamforming alongside E2E ASR for denoising. It is clear from both recent challenge outcomes and successful products that far-ﬁeld systems would be incomplete without solving both denoising and dereverberation simultaneously. This report uses a recently developed architecture for far-ﬁeld ASR by composing neural extensions of dereverberation and beamforming modules with the S2S ASR module as a single differentiable neural network and also clearly deﬁning the role of each subnetwork. The original implementation of this architecture was successfully applied to the noisy speech recognition task (CHiME-4), while we applied this implementation to noisy reverberant tasks (DIRHA and REVERB). Our investigation shows that the method achieves better performance than conventional pipeline methods on the DIRHA English dataset and comparable performance on the REVERB dataset. It also has additional advantages of being neither iterative nor requiring parallel noisy and clean speech data. Index Terms: speech recognition, far-ﬁeld, end-to-end, neural dereverberation, neural beamformer
1. Introduction
Sequence-to-sequence (S2S) neural network models for automatic speech recognition (ASR) [1] are rapidly gaining a lot of attention and popularity because of their property to jointly optimize all the conventional ASR components in an end-to-end (E2E) fashion. It is seen as a competitive alternate to stateof-the-art hidden Markov model (HMM)-deep neural network (DNN) based hybrid automatic speech recognition (ASR) systems [2] as it has achieved comparable performance on tasks with a very large amount of training data [3, 4]. The legacy hybrid ASR system has multiple components optimized independently and works in a soft pipeline fashion where the (probabilistic) output of the preceding component is fed as an input to the succeeding component. On the other hand, the E2E model just composes of a single network, which is trained to map a sequence of speech features directly to a text sequence, by optimizing all the different components in the ASR pipeline jointly.
This report investigates the above joint optimization ability of E2E ASR from standard close-talk to far-ﬁeld applications

by encompassing entire multichannel speech enhancement and ASR components within the S2S model. Far-ﬁeld ASR systems often utilize input from multiple microphones and have frontend enhancement components to handle distortions caused by both noise and reverberation [5]. Outcomes of recent challenges like REVERB [6] and CHiME-5 [7] show that both denoising and dereverberation components are indispensable for handling far-ﬁeld speech. Typically for hybrid ASR, a multichannel dereverberation component followed by a beamforming component is used as an additional pipeline [8]. Instead of using these techniques as a pipeline for E2E models or simply extending E2E models to allow multichannel speech features [9, 10], it is straightforward to include carefully designed sub-networks for beamforming and dereverberation within the E2E model to take advantage of the fact that they can be jointly trained.
Currently, neural beamforming techniques for denoising [11, 12] have given state-of-the art results in robust ASR tasks like the CHiME-4 challenge [13–16]. In these techniques, a neural network is employed to estimate speech and noise masks, which in turn are used to compute the power spectral density (PSD) matrices needed to estimate the beamforming ﬁlter. The neural network is often trained with simulated data and the ground truth mask as the target. A neural beamforming mechanism as a differentiable component was proposed in [17, 18] to allow the joint optimization of multichannel speech recognition and enhancement within the E2E system only based on the ASR objective. The masks are made as latent variables in the end-toend training. Hence, parallel clean and noisy speech data are not needed in this approach. Joint training of a neural beamformer with a hybrid ASR acoustic model is also proposed in [19–23], although these require frame-level alignments, while the E2E system does not.
Weighted prediction error (WPE) [24, 25] is a technique based on variance normalized long term linear prediction popularly used for dereverberation of wet (reverberant) signals. It has been very effective in successful commercial products like Google Home [26]. This technique requires an estimate of the time-varying variance of the desired dry signal and hence the conventional WPE method is iterative. A non-iterative method DNN-WPE was proposed in [27, 28] where a neural network was trained to estimate the magnitude spectrum of the desired signal from the observed signal’s magnitude spectrum. We can introduce a mask as a hidden state vector similar to [17] for estimation of the magnitude spectrum of the desired signal without parallel data. The WPE ﬁltering solution being differentiable, we can train this also in an E2E framework and optimize it only based on the ASR objective.
We use the implementation developed by NTT which has

neural extensions of WPE and MVDR with E2E ASR 1. The original implementation was applied on noisy data CHiME4 without including the dereverberation component. We investigate to jointly train both WPE based dereverberation and minimum variance distortionless response (MVDR) based beamforming along with ASR using noisy and reverberant data and also test it on mismatch conditions. Other major differences from the original implementation are as follows: (1) we jointly train both dereveberation and beamforming by passing the input through both subnetworks while the original implementation only passes through one of them, (2) we also investigate applying a speech activity detection type mask in the beamforming subnetwork and different activations for the mask in the dereverberation subnetwork. The parameters to be estimated by the neural network for the front-end are channel-independent masks. This makes the trained system to generalize for input signals with arbitrary number and order of channels like [17].

2. Multi-channel end-to-end ASR

2.1. Dereverberation subnetwork

This section explains WPE based dereverberation method [6,
24, 25], which cancels late reverberations using variance nor-
malized delayed linear prediction (NDLP). WPE estimates the desired M -channel (dereverberated) signal d(t, b) ∈ CM in the short-time Fourier transform (STFT) domain at time frame t, frequency bin b using the following vector-form equation:

d(t, b) = y(t, b) − GH(b)y˜(t − ∆, b),

(1)

y(t, b) ∈ CM is the observed multichannel signal in the STFT domain, ∆ is the prediction delay. GH(b) ∈ CML×M and y˜(t − ∆, b) ∈ CML are the stacked representations of the prediction ﬁlter coefﬁcients and the delayed multichannel observations with the ﬁlter order L, respectively. H denotes the conjugate transpose.
WPE assumes the desired signal d(t, b) in Eq. (1) is a realization of a zero-mean complex Gaussian N c(·) with an unknown channel independent time-varying variance λ(t, b) ∈ R>0, as follows:
p(d(t, b); λ(t, b)) = N c(d(t, b); 0, λ(t, b)I). (2)

The prediction ﬁlter G(b) in Eq. (1) obtained based on maximum likelihood estimation yields the following iterative solution with the previously estimated desired signal d¯(t, b, m):

λ(t, b) = 1 d¯(t, b, m) 2 ,

(3)

Mm

y˜(t − ∆, b)y˜H(t − ∆, b)

R(b) =

λ(t, b) , (4)

t

P(b) = y˜(t − ∆, b)yH(t, b) ∈ CML×M , (5) λ(t, b)
t

G(b)

=

R(b)−1P(b)

∈

M L×M
C

,

(6)

where m is the channel index and R(b) ∈ CML×ML is the correlation matrix. In conventional WPE, the estimated desired signal d¯(t, b, m) in Eq. (3) is initialized with the observed signal y(t, b, m) to estimate the variance λ(t, b) in the ﬁrst iteration. This iterative process makes this algorithm slow and also loses online processing capabilities.

1https://github.com/espnet/espnet/pull/596

Figure 1: E2E Multichannel ASR architecture.

Instead, [27] uses a DNN to estimate the magnitude spectrum |d¯(t, b, m)| in Eq. (3) from the magnitude spectrum of the observed signals y(:, :, m) for every channel m2. This network gives a good estimate of the variance λ(t, b), and it was shown that the performance obtained with one-shot ﬁlter estimation with Eqs. (4)–(6) can match that of WPE without iterations. This method is called DNN-WPE. The drawback in this method is we need to simulate parallel data to train the DNN.
We use this WPE-based dereverberation as a sub-network of our E2E framework (described in Section 2.3). This subnetwork processing (deﬁned as the operation WPE(·)) is based on the sequence of the ﬁlter estimation steps based on Eqs.(3)– (6), and the ﬁnal dereverberation based on Eq. (1). As all the operations are easily differentiable, we can incorporate it into a computational graph for joint training. In our joint training approach, we propose to estimate the desired power spectrum d(t, b, m) 2 via the following masking network MaskNetD(·) that produces a mask w(t, b, m) ∈ [0, 1]:

w(:, :, m) = MaskNetD(y(:, :, m)),

(7)

d(t, b, m) 2 = w(t, b, m) y(t, b, m) 2 ,

(8)

Since the domain of the mask is bounded within [0, 1], it is easily estimated from a neural network compared with the direct prediction of the desired power/magnitude spectrum.

2.2. Beamforming subnetwork

We use a beamforming subnetwork similar to the one proposed in [17]. Like the dereverberation subnetwork, another set of two masking networks MaskNetS and are MaskNetN are used to produce the speech mask wS(t, b, m) ∈ [0, 1] and the noise mask wN(t, b, m) ∈ [0, 1] given the output d(t, b, m) from WPE(·), as follows:

wv(:, :, m) = MaskNetv(d(:, :, m)) where v ∈ {S, N}. (9)

These masks are averaged over channels (e.g., wv(t, b) =
1/M m wv(t, b, m)) and used to compute the power spectral density (PSD) matrices of speech and noise ΦS(b) ∈ CM×M and ΦN(b) ∈ CM×M at frequency bin b as follows:

T
Φv(b) = wv(t, b)d(t, b)dH(t, b) where v ∈ {S, N}, (10)
t=1

From these PSD matrices, the M -dimensional complex MVDR beamforming ﬁlter fMVDR(b) ∈ CM is estimated by solving the
following optimization problem:

ΦN(b)−1ΦS(b)

fMVDR(b) = Tr(ΦN(b)−1ΦS(b)) u

(11)

2We use the notation f (:) to denote all elements. For example, y(:, : , m) denotes the observation STFT signal of a channel m for all frames and frequency bins.

where u ∈ {0, 1}M is a one-hot vector to choose a reference microphone and the beamformer estimates the speech image at the reference microphone. Tr(·) denotes the trace operation. We use the MVDR formulation based on reference selection (Eq. (11)) given in [29] instead of the widely-used steering vector estimation based formulation [30] to make the operation more easily differentiable. Note that all the masking networks in beamforming (and dereverberation) are trained without any signal-level supervision but with the ASR objective.
Once we obtain the beamforming ﬁlter fMVDR(b), we can perform speech denoising to obtain an enhanced STFT signal x(t, b) ∈ C as follows:

x(t, b) = fMHVDR(b)d(t, b).

(12)

Similar to the WPE operation, we deﬁne this MVDR ﬁlter estimation using Eqs. (10) and (11) along with the denoising equation (12) as MVDR(·).

2.3. Joint dereverberation & beamforming
The beamforming subnetwork is placed after the dereverberation subnetwork. The output of the beamforming network goes to the ASR. This whole network is trained solely based on the ASR objective. The architecture is shown in Figure 1.
We summarize the three stages in terms of operations deﬁned before as:
• Dereverberation: Eqs. (7) → (8) → (3)–(6) → (1)

w(:, :, m) = MaskNetD(y(:, :, m)). (13)

D = WPE(W, Y ).

(14)

• Beamforming: Eqs. (9) → (10) → (11) → (12)
wS(:, :, m) = MaskNetS(d(:, :, m)). (15) wN(:, :, m) = MaskNetN(d(:, :, m)). (16)
X = MVDR(WS, WN, D). (17)

• Feature extraction & recognition:

F = MVN(Log(MelFilterbank(|X|))) (18)

C = ASR(F ).

(19)

where W , Y , and D denote the mask, observation and dereverberated STFT signals for all frames, frequency bins, and channels, respectively. WS and WN denote the averaged speech and noise masks over the channels and X denotes the beamformed STFT signal for all frames and frequency bins. Log Mel Filterbank transformation is applied on the magnitude of X and utterance based mean-variance normalization (MVN) is performed to produce an input that is suitable for ASR F . All these operations are still differentiable. C = (c1, c2, · · · ) is the character sequence that represents the text output of E2E ASR (ASR(·)).
One of the most important beneﬁts of this architecture is that the entire network is represented as a differentiable computational graph and their parameters are jointly trained using back propagation, as shown in Figure 2. This architecture also clearly deﬁnes the role of each subnetwork by careful design of their architecture which makes it possible to interpret their intermediate outputs D and X as dereverberated and denoised signals respectively.

3. Experiments
3.1. Setup
We evaluated the effectiveness of the method described in the previous section by using the REVERB [6] and DIRHA English WSJ [33] datasets in the following way.
• Training - 2-channel simulation data from REVERB and clean data from wall street journal (WSJ) corpus [34] (both WSJ0 and WSJ1)
• Validation - REVERB 8-channel real and simulation development sets.
• Evaluation - (1) REVERB 8-channel real and simulation evaluation sets, (2) DIRHA-WSJ 6-channel real recordings from the living room’s circular ceiling array.
A hybrid combination of connectionist temporal classiﬁcation (CTC) and attention-based encoder-decoder model [35, 36] was used for E2E speech recognition. The ESPnet toolkit [37] was used for the E2E ASR experiments. The baseline E2E ASR uses the 80-dimensional log Mel ﬁlterbank energies as the feature. The encoder consists of two initial blocks of convolution layers followed by three output gate projected bidirectional long short-term memory (BLSTMP) layers with 1024 units. The location based attention mechanism was used. The decoder consists of a single LSTM layer with 1024 units followed by a linear layer with a number of output units corresponding to the number of distinct characters. The CTC-attention interpolation weight was ﬁxed as “0.5”. The word based RNN language model proposed in [38] was used.
An open source implementation of WPE [39] was used. We also implemented the DNN-WPE model [27], and made it publicly available as an open source toolkit 3. The WPE ﬁlter order L and the prediction delay ∆, which are introduced in Section 2.1, were ﬁxed as “5” and “3” respectively for all the dereverberation methods. The number of iterations was ﬁxed as “3” for WPE. In DNN-WPE, the architecture given in [27] was used for the neural network predicting magnitude spectrum. Both dereverberation subnetwork’s and beamforming subnetwork’s masking networks in Section 2.3 consist of two BLSTMP layers followed by an additional feedforward layer. The dereverberation subnetwork uses clipped rectiﬁed linear unit (ReLU) with a max clamp at “1” as the activation and the beamforming subnetwork uses sigmoid as the activation.
We used two methods for the reference microphone selection (u in Eq (11)) in beamforming subnetwork: 1) ﬁxing channel 2 as the reference, 2) attention based soft reference selection proposed in [17] that includes reference selection inside the network using the state vectors of the masking network and the speech PSD matrix. BeamformIt [40] - a weighted delay and sum beamformer was used for the conventional pipeline method. The reference channel selection in BeamformIt is performed using a metric named X-Corr which is based on the average cross-correlation of one channel and all other channels.
Since the system can generalize to any number of channels, we only choose two channels while training to be memory efﬁcient. All eight channels were used while testing. The batchsize was ﬁxed as “12” for all the experiments. The single channel E2E ASR baseline is trained by randomly choosing a channel when the batch consists of the REVERB data.
To regularize the ASR network when training the E2E frontends, we randomly choose whether to pass through the frontend subnetworks or directly to the ASR encoder by also
3https://github.com/sas91/jhu-neural-wpe

Table 1: WER (%) on REVERB and DIRHA-WSJ (LA array) evaluation sets comparing the performance of pipeline & E2E frontend techniques.

Method Challenge baseline [6]
E2E Baseline
Pipeline
E2E
Tachioka et. al. [31] Alam et. al. [32] Wang et. al. [10]

Dereverberation
-
WPE DNN-WPE
WPE DNN-WPE
WPE -
WPE WPE WPE
Spectral subtraction Iterative deconvolution
-

Beamformer

Method
-
BeamformIt BeamformIt BeamformIt
MVDR MVDR MVDR MVDR MVDR
Delay-sum -

Reference
-
X-Corr X-Corr X-Corr
Ch 2 Ch 2 Ch 2 Ch 2 Attention
-

Mask
Type
-
-
TF SAD TF SAD SAD
-

REVERB Simulated

Room 1

Room 2

Room 3

Near Far Near Far Near Far

16.2 18.7 20.5 32.5 24.8 38.9 5.4 7.1 7.6 12.9 9.7 16.1

6.0 6.6 7.1 9.8 8.0 11.2 5.7 6.0 7.5 9.3 7.8 10.1 5.8 6.1 5.8 8.5 6.9 10.2 6.6 5.9 6.1 7.0 6.8 8.2 6.3 5.8 6.4 6.8 6.6 7.7

6.3 6.7 6.7 8.9 7.4 10.6 5.7 6.1 5.6 8.2 6.2 10.2 7.2 7.2 6.4 8.6 7.1 12.1 5.5 5.7* 5.3* 6.6* 6.5 7.6* 8.3 7.8 6.9 7.0 7.6 8.6 6.4 6.3 5.9 6.8 6.3* 7.6*

5.0 5.6 5.6 8.2 5.7 10.5

6.7 7.3 8.0 11.1 8.1 12.1

-

-

-

-

-

-

REVERB Real

Room 1

Near Far

50.1 47.6 23.9 26.8

17.7 18.4 16.4 18.5 14.6 16.1 11.3 11.9 11.0 10.8*

17.0 19.8 12.6 17.3 16.0 20.5 10.7 13.7 10.8 13.9 8.7* 12.4

16.9 20.3

21.4 22.0

-

-

DIRHA LA
55.3
42.3 41.3 39.2 30.7 31.3
42.3 42.3 45.3 35.4 31.6 29.1*
35.1

choosing a random channel. Also, while jointly training dereverberation-beamforming subnetworks, we randomly also skip the dereverberation part and give the input directly to the beamforming subnetwork. We tried two types of masks for the beamforming subnetwork: a) Standard time-frequency (TF) mask b) One (time) dimensional mask like speech activity detection (SAD). Dereverberation subnetwork always uses a TF mask.
Figure 2: Sample real eval ﬁle - 1) Magnitude spectrum of the input reverberated noisy speech (ch1), 2) Output of dereverberation subnetwork after WPE ﬁltering (ch1 magnitude), 3) The ﬁnal enhanced signal output (magnitude) of frontend subnetworks after beamforming.
3.2. Results & discussion The ASR results comparing the performance of the pipeline frontend with the E2E frontend is given in Table 1. The E2E baseline results are shown in the top row. It is compared with the results of the HMM-GMM baseline given as a part of the challenge [6] and the E2E baseline performs better. Table 1 shows that there is signiﬁcant gain using dereverberation and/or beamforming in both pipeline and E2E frontends. When we focus on the performance of solo E2E dereverberation or beamforming with ASR, they are better than their pipeline counterparts on some of the simulation and real near sets of REVERB, while there is a slight degradation on the real far sets of REVERB and

DIRHA data. This degradation could come from the mismatch in their conditions because the condition of the REVERB simulation training data is relatively less reverberant, and stationary compared with those of the REVERB real far and DIRHA data.
The combination of E2E dereverberation and beamforming with TF mask gives the best results in most of the simulation test sets. The “room1” simulated condition is relatively very clean compared to the other sets and we can infer that joint training does not distort the clean signal much when we use a TF mask. Finally, the combination of E2E dereverberation and beamforming with SAD mask along with attention based reference selection for the beamforming part is convincing overall on challenging real conditions. Interestingly this model gives 5.2% relative improvement over the best pipeline method on the mismatched DIRHA set. This suggests that joint dereverberation and beamforming is well generalized even for more challenging mismatch data by considering the fact that it was trained only with the REVERB simulation data.
Finally, we compared the investigated method with the reference results using the same test set. We chose [31] and [32] from the submissions in the REVERB challenge ofﬁcial data track, which almost matches our training data conditions. We also refer an end-to-end ASR system from [10], which was trained with the DIRHA training data. Joint dereverberation and beamforming with SAD mask attention based reference selection for the beamforming part often outperforms these results especially for the challenging REVERB real and DIRHA data conditions.
With these results, we can conclude that that it is possible to realize robust far-ﬁeld ASR within an end-to-end manner by integrating dereverberation, beamforming, and ASR. Note that this framework has extra beneﬁts compared with the conventional pipeline methods as it is working without parallel data nor an iterative process. Another beneﬁt is that we can also generate enhanced signals from the output of each subnetwork, as discussed in Section 2.3. Interestingly, Figure 2 shows that the dereverberated signal has less smearing in time and the effect of denoising is very clear in the ﬁnal enhanced spectrogram from a clearer background. This result indicates the investigated E2E multi-channel ASR system can perform dereverberation and denoising without any signal-level objective but with the ASR objective.

4. Summary
In this report, we investigated the ability of a recently developed multichannel end-to-end ASR model to work on noisy reverberant and mismatched environments. The investigated model jointly optimizes both beamforming and dereverberation components along with the ASR network only with the end-to-end ASR objective. We showed that this model is robust to mismatch conditions and gives comparable or better performance compared to existing pipeline methods.
5. References
[1] D. Bahdanau, J. Chorowski, D. Serdyuk, P. Brakel, and Y. Bengio, “End-to-end attention-based large vocabulary speech recognition,” in ICASSP, 2016, pp. 4945–4949.
[2] G. Hinton et al., “Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups,” IEEE Signal processing magazine, vol. 29, no. 6, pp. 82–97, 2012.
[3] D. Amodei et al., “Deep speech 2: End-to-end speech recognition in english and mandarin,” in ICML, 2016, pp. 173–182.
[4] C.-C. Chiu et al., “State-of-the-art speech recognition with sequence-to-sequence models,” in ICASSP, 2018, pp. 4774–4778.
[5] J. Li, L. Deng, Y. Gong, and R. Haeb-Umbach, “An overview of noise-robust automatic speech recognition,” IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 22, no. 4, pp. 745–777, April 2014.
[6] K. Kinoshita et al., “A summary of the REVERB challenge: stateof-the-art and remaining challenges in reverberant speech processing research,” EURASIP Journal on Advances in Signal Processing, 2016.
[7] J. Barker, S. Watanabe, E. Vincent, and J. Trmal, “The ﬁfth CHiME speech separation and recognition challenge: Dataset, task and baselines,” in Interspeech, 2018, pp. 1561–1565.
[8] L. Drude et al., “Integrating neural network based beamforming and weighted prediction error dereverberation,” in Interspeech, 2018, pp. 3043–3047.
[9] S. Braun, D. Neil, J. Anumula, E. Ceolini, and S.-C. Liu, “Multichannel attention for end-to-end speech recognition,” in Interspeech, 2018, pp. 17–21.
[10] X. Wang et al., “Stream attention-based multi-array end-to-end speech recognition,” in ICASSP, 2019.
[11] J. Heymann, L. Drude, and R. Haeb-Umbach, “Neural network based spectral mask estimation for acoustic beamforming,” in ICASSP, 2016, pp. 196–200.
[12] H. Erdogan, J. R. Hershey, S. Watanabe, M. I. Mandel, and J. Le Roux, “Improved MVDR beamforming using single-channel mask prediction networks.” in Interspeech, 2016, pp. 1981–1985.
[13] J. Du et al., “The USTC-iFlytek system for CHiME-4 challenge,” in CHiME-4 workshop, 2016.
[14] T. Menne et al., “The RWTH/UPB/FORTH system combination for the 4th CHiME challenge evaluation,” in CHiME-4 workshop, 2016.
[15] E. Vincent, S. Watanabe, A. A. Nugraha, J. Barker, and R. Marxer, “An analysis of environment, microphone and data simulation mismatches in robust speech recognition,” Computer Speech & Language, vol. 46, pp. 535–557, 2017.
[16] S.-J. Chen, A. S. Subramanian, H. Xu, and S. Watanabe, “Building state-of-the-art distant speech recognition using the CHiME-4 challenge with a setup of speech enhancement baseline,” in Interspeech, 2018, pp. 1571–1575.
[17] T. Ochiai, S. Watanabe, T. Hori, and J. R. Hershey, “Multichannel end-to-end speech recognition,” in ICML, 2017.
[18] T. Ochiai, S. Watanabe, T. Hori, J. R. Hershey, and X. Xiao, “Uniﬁed architecture for multichannel end-to-end speech recognition with neural beamforming,” IEEE Journal of Selected Topics in Signal Processing, vol. 11, no. 8, pp. 1274–1288, 2017.

[19] X. Xiao et al., “Deep beamforming networks for multi-channel speech recognition,” in ICASSP, 2016, pp. 5745–5749.
[20] B. Li, T. N. Sainath, R. J. Weiss, K. W. Wilson, and M. Bacchiani, “Neural network adaptive beamforming for robust multichannel speech recognition,” in Interspeech, 2016, pp. 1976–1980.
[21] J. Heymann, L. Drude, C. Boeddeker, P. Hanebrink, and R. HaebUmbach, “Beamnet: End-to-end training of a beamformersupported multi-channel ASR system,” in ICASSP, 2017, pp. 5325–5329.
[22] T. Menne, R. Schlu¨ter, and H. Ney, “Speaker adapted beamforming for multi-channel automatic speech recognition,” in IEEE SLT Workshop, 2018, pp. 535–541.
[23] W. Minhua, K. Kumatani, S. Sundaram, N. Strom, and B. Hoffmeister, “Frequency domain multi-channel acoustic modeling for distant speech recognition,” in ICASSP, 2019.
[24] T. Nakatani, T. Yoshioka, K. Kinoshita, M. Miyoshi, and B. Juang, “Speech dereverberation based on variance-normalized delayed linear prediction,” IEEE Transactions on Audio, Speech, and Language Processing, vol. 18, no. 7, pp. 1717–1731, 2010.
[25] T. Yoshioka and T. Nakatani, “Generalization of multi-channel linear prediction methods for blind mimo impulse response shortening,” IEEE Transactions on Audio, Speech, and Language Processing, vol. 20, no. 10, pp. 2707–2720, 2012.
[26] B. Li et al., “Acoustic modeling for Google Home,” in Interspeech, 2017, pp. 399–403.
[27] K. Kinoshita, M. Delcroix, H. Kwon, T. Mori, and T. Nakatani, “Neural network-based spectrum estimation for online WPE dereverbertion,” in Interspeech, 2017, pp. 384–388.
[28] J. Heymann, L. Drude, R. Haeb-Umbach, K. Kinoshita, and T. Nakatani, “Frame-online DNN-WPE dereverberation,” in WAENC, 2018, pp. 466–470.
[29] M. Souden, J. Benesty, and S. Affes, “On optimal frequencydomain multichannel linear ﬁltering for noise reduction,” IEEE Transactions on audio, speech, and language processing, vol. 18, no. 2, pp. 260–276, 2010.
[30] S. Gannot, E. Vincent, S. Markovich-Golan, and A. Ozerov, “A consolidated perspective on multimicrophone speech enhancement and source separation,” IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 25, no. 4, pp. 692–730, April 2017.
[31] Y. Tachioka, T. Narita, F. J. Weninger, and S. Watanabe, “Dual system combination approach for various reverberant environments with dereverberation techniques,” in IEEE REVERB Workshop, 2014.
[32] M. J. Alam, V. Gupta, P. Kenny, and P. Dumouchel, “Use of multiple front-ends and i-vector-based speaker adaptation for robust speech recognition,” IEEE REVERB Workshop, 2014.
[33] M. Ravanelli et al., “The DIRHA-English corpus and related tasks for distant-speech recognition in domestic environments,” in IEEE ASRU Workshop, 2015, pp. 275–282.
[34] D. B. Paul and J. M. Baker, “The design for the wall street journalbased CSR corpus,” in Proceedings of the workshop on Speech and Natural Language. Association for Computational Linguistics, 1992, pp. 357–362.
[35] S. Watanabe, T. Hori, S. Kim, J. R. Hershey, and T. Hayashi, “Hybrid CTC/attention architecture for end-to-end speech recognition,” IEEE Journal of Selected Topics in Signal Processing, vol. 11, no. 8, pp. 1240–1253, 2017.
[36] S. Kim, T. Hori, and S. Watanabe, “Joint CTC-attention based end-to-end speech recognition using multi-task learning,” in ICASSP, 2017, pp. 4835–4839.
[37] S. Watanabe et al., “ESPnet: End-to-end speech processing toolkit,” in Interspeech, 2018, pp. 2207–2211.
[38] T. Hori, J. Cho, and S. Watanabe, “End-to-end speech recognition with word-based RNN language models,” in IEEE SLT Workshop, 2018, pp. 389–396.

[39] L. Drude, J. Heymann, C. Boeddeker, and R. Haeb-Umbach, “NARA-WPE: A Python package for weighted prediction error dereverberation in Numpy and Tensorﬂow for online and ofﬂine processing,” in ITG Fachtagung Sprachkommunikation, 2018.
[40] X. Anguera, C. Wooters, and J. Hernando, “Acoustic beamforming for speaker diarization of meetings,” IEEE Transactions on Audio, Speech, and Language Processing, vol. 15, no. 7, pp. 2011–2022, 2007.

