arXiv:1809.00794v2 [cs.CL] 4 Jul 2019

Texar: A Modularized, Versatile, and Extensible Toolkit for Text Generation
Zhiting Hu‚àó, Haoran Shi, Bowen Tan, Wentao Wang, Zichao Yang, Tiancheng Zhao, Junxian He, Lianhui Qin, Di Wang, Xuezhe Ma, Zhengzhong Liu,
Xiaodan Liang, Wangrong Zhu, Devendra Singh Sachan, Eric P. Xing Carnegie Mellon University, Petuum Inc. zhitinghu@gmail.com‚àó
Abstract We introduce Texar, an open-source toolkit aiming to support the broad set of text generation tasks that transform any inputs into natural language, such as machine translation, summarization, dialog, content manipulation, and so forth. With the design goals of modularity, versatility, and extensibility in mind, Texar extracts common patterns underlying the diverse tasks and methodologies, creates a library of highly reusable modules, and allows arbitrary model architectures and algorithmic paradigms. In Texar, model architecture, inference, and learning processes are properly decomposed. Modules at a high concept level can be freely assembled and plugged in/swapped out. The toolkit also supports a rich set of large-scale pretrained models. Texar is thus particularly suitable for researchers and practitioners to do fast prototyping and experimentation. The versatile toolkit also fosters technique sharing across different text generation tasks. Texar supports both TensorFlow and PyTorch, and is released under Apache License 2.0 at https://www.texar.io.
1 Introduction
Text generation spans a broad set of natural language processing tasks that aim to generate natural language from input data or machine representations. Such tasks include machine translation (Brown et al., 1990; Vaswani et al., 2017), dialog systems (Williams and Young, 2007; Serban et al., 2016; Tang et al., 2019), text summarization (Hovy and Lin, 1998; See et al., 2017), data description (Wiseman et al., 2017; Li et al., 2018), text paraphrasing and manipulation (Hu et al., 2017a; Madnani and Dorr, 2010; Lin et al., 2019), image captioning (Vinyals et al., 2015b; Karpathy and Fei-Fei, 2015), and more. Recent years have seen rapid progress of this active area, in part due to the integration of modern deep learning approaches in many of the tasks. On the other hand, considerable research efforts are still needed in order to improve techniques and enable real-world applications.
A few remarkable open-source toolkits have been developed in support of text generation applications (sec 4). Those toolkits, however, are largely designed for one or a small number of speciÔ¨Åc tasks, particularly machine translation (e.g., Britz et al., 2017; Klein et al., 2017) and conversation systems (e.g., Miller et al., 2017), and usually support a narrow set of machine learning algorithms such as supervised learning. Emerging new applications and approaches instead are often developed by individual teams in a more ad-hoc manner, which can easily result in hard-to-maintain custom code and duplicated efforts across the disjoint projects.
The variety of text generation tasks indeed have many common properties. For instance, two central goals are shared across most of them, namely 1) generating well-formed, grammatical, and readable text, and 2) realizing in the generated text all desired information inferred from the inputs. To this end, a set of key techniques are increasingly widely-used, such as neural encoderdecoders (Sutskever et al., 2014), attention (Bahdanau et al., 2014; Luong et al., 2015; Vaswani et al., 2017), memory networks (Sukhbaatar et al., 2015), adversarial methods (Goodfellow et al., 2014; Lamb et al., 2016), reinforcement learning (Ranzato et al., 2015; Bahdanau et al., 2016; Tan et al., 2018), structured supervision (Hu et al., 2018; Yang et al., 2018), as well as optimization
1

A

Prior

M

0/1

C

ùë•

E

D

ùë¶ùë•

E

ùëß

D

ùë¶ùë•

E

D

ùë¶ùë•

E

D

ùë¶

(a)

(b)

(c)

(d)

ùë•1

E1

ùë•2

E2

ùë•3

E3

D1

ùë¶1

D2

ùë¶2

D3

ùë¶3

ùë•

E

C

ùë•

E

ùëß

D

ùë¶

D1

ùë¶1

C

0/1

D2

ùë¶2

(e)

(f)

(g)

Figure 1: An example of various model architectures in recent text generation literatures. E denotes encoder, D denotes decoder, C denotes classiÔ¨Åer (i.e., binary discriminator). (a): The canonical

encoder-decoder, sometimes with attention A (Sutskever et al., 2014; Bahdanau et al., 2014; Luong

et al., 2015; Vaswani et al., 2017) or copy mechanisms (Gu et al., 2016; Vinyals et al., 2015a; Gulcehre et al., 2016). (b): Variational encoder-decoder (Bowman et al., 2015; Yang et al., 2017). (c): Encoder-decoder augmented with external memory (Sukhbaatar et al., 2015; Bordes et al., 2016). (d): Adversarial model using a binary discriminator C, with or without reinforcement learning (Liang et al., 2017; Zhang et al., 2017; Yu et al., 2017). (e): Multi-task learning with multiple encoders and/or decoders (Luong et al., 2016; Firat et al., 2016). (f): Augmenting with cyclic loss (Hu et al., 2017a). (g): Learning to align with adversary, either on samples y or hidden

states (Lamb et al., 2016; Lample et al., 2017; Shen et al., 2017).

real data

model

model

samples w/ soft approx. discriminator

0/1 classification

real data cross entropy (a)

model

samples

real data reward

model

(b) samples

real data discriminator

0/1 classification

policy gradient

policy gradient

(c)

(d)

Figure 2: An example of various learning paradigms for text generation models. Inference pro-

cesses are denoted as blue solid arrows; learning signals and gradient propagation are denoted as

red dashed arrows. (a): Maximum likelihood learning with a cross-entropy loss (Mikolov et al.,

2010). (b): Adversarial learning that propagates gradient through samples with continuous approx-

imation (soft approx.) (Hu et al., 2017a; Yang et al., 2018). A discriminator is trained jointly with

the target model. (c): Reinforcement learning with a speciÔ¨Åed reward function (e.g., BLEU score)

and policy gradient (Ranzato et al., 2015; Rennie et al., 2017). (d): Combination of adversarial

learning and reinforcement learning by using a learnable discriminator as the reward function and

updating the target model with policy gradient (Fedus et al., 2018; Yu et al., 2017). Other learning

algorithms can include reward-augmented maximum likelihood (Norouzi et al., 2016), learning-

to-search (Hal Daum√© et al., 2009; Wiseman and Rush, 2016), interpolation algorithm (Tan et al.,

2018), etc.

2

stack
Library APIs Training
Models Architectures

Applications Evaluation
Losses

Model templates + ConÔ¨Åg Ô¨Åles Prediction

Data

Trainer

MonoText PairedText Executor Optimizer

Encoder Decoder Embedder ClassiÔ¨Åer (Seq) MaxLikelihood Adversarial Dialog Numerical Seq/Episodic RL Agent

Memory Connector Policy

QNet Rewards RL-related Regularize Multi-Ô¨Åeld/type Parallel lr decay / grad clip / ...

. . .

. . .

. . .

. . .

Figure 3: The stack of main modules and functionalities in Texar. Many of the low-level compo-

nents are omitted.

techniques, data pre-processing and result post-processing procedures, evaluations, etc. These techniques are often combined together in various ways to tackle different problems. Figures 1 and 2 summarize examples of various model architectures and learning paradigms, respectively.
It is therefore highly desirable to have an open-source platform that uniÔ¨Åes the development of the diverse yet closely-related applications, backed with clean and consistent implementations of the core algorithms. Such a platform would enable reuse of common components and functionalities; standardize design, implementation, and experimentation; foster reproducibility; and, importantly, encourage technique sharing among different text generation tasks so that an algorithmic advance developed for a speciÔ¨Åc task can quickly be evaluated and generalized to many other tasks.
We introduce Texar, a general-purpose text generation toolkit aiming to support popular and emerging applications in the Ô¨Åeld, by providing researchers and practitioners a uniÔ¨Åed and Ô¨Çexible framework for building their models. Texar has two versions, building upon TensorFlow (tensorflow. org) and PyTorch (pytorch.org), respectively, with the same uniform design.
Underlying the core of Texar‚Äôs design is principled anatomy of extensive machine learning models and algorithms (Hu et al., 2017b; Tan et al., 2018), which subsumes the diverse cases in Figures 1, 2 and beyond, enabling a uniÔ¨Åed formulation and consistent implementation. Texar emphasizes three key properties:
‚Ä¢ Versatility: Texar contains a wide range of features and functionalities for 1) arbitrary model architectures as a combination of encoders, decoders, embedders, discriminators, memories, and many other modules; and 2) different modeling and learning paradigms such as sequenceto-sequence, probabilistic models, adversarial methods, and reinforcement learning. Based on these, both workhorse and cutting-edge solutions to the broad spectrum of text generation tasks are either already included or can be easily constructed.
‚Ä¢ Modularity: Texar is designed to be highly modularized, by decoupling solutions of diverse tasks into a set of highly reusable modules. Users can construct their model at a high conceptual level just like assembling building blocks. It is convenient to plug in or swap out modules, conÔ¨Ågure rich options of each module, or even switch between distinct modeling paradigms. For example, switching between maximum likelihood learning and reinforcement learning involves only minimal code changes (e.g., Figure 7). Modularity makes Texar particularly suitable for fast prototyping and experimentation.
‚Ä¢ Extensibility: The toolkit provides interfaces of multiple functionality levels, ranging from simple conÔ¨Åguration Ô¨Åles to full library APIs. Users of different needs and expertise are free to choose different interfaces for appropriate programmability and internal accessibility. The library APIs are fully compatible with the native TensorFlow/PyTorch interfaces, which allows seamless integration of user-customized modules, and enables the toolkit to take advantage of the vibrant open-source community by effortlessly importing any external components as needed.

3

learning
Maximum likelihood Adversarial
Reinforcement Reward-augmented
‚Ä¶

decoding
inference
Teacher-forcing Sample Greedy
Beam-search Gumbel-softmax
Top-k sample ‚Ä¶

decoder
architecture
Basic RNN Attention RNN Transformer
‚Ä¶

called as subroutines provide uniform interfaces
Figure 4: The learning‚Äìinference‚Äìarchitecture anatomy in Texar, taking decoder for example. SpeciÔ¨Åcally, a sequence decoder in a model can have an arbitrary architecture (e.g., basic RNN decoder); all architectures expose uniform interfaces for specifying one of the tens of decoding strategies (e.g., teacher-forcing decoding) to generate samples or infer probabilities; a learning procedure repeated calls arbitrary speciÔ¨Åed inference procedure during training. Learning can be totally agnostic to the model architecture.

Furthermore, Texar puts much emphasis on well-structured high-quality code of uniform design patterns and consistent styles, along with clean documentations and rich tutorial examples. Other useful features such as distributed GPU training are also supported.

2 Structure and Design

Figure 3 shows the stack of main modules and functionalities in Texar. Building upon the lower level deep learning platforms, Texar provides a comprehensive set of building blocks for model construction, training, evaluation, and prediction. In the following, we Ô¨Årst present the design principles that lead to the attainment of these goals (sec 2.1), and then describe the detailed structure of Texar with running examples to demonstrate the properties of the toolkit (sec 2.2-2.4).

2.1 The Design of Texar

The broad variation of the many text generation tasks and the fast-growing new models and algorithms have posed unique challenges to designing a versatile toolkit. We tackle the challenges by principally decomposing the modeling and experimentation pipeline, developing an extensive set of ready-to-assemble modules, and providing user interfaces of varying abstract levels.
UniÔ¨Åed Anatomy. We break down the complexity of the rich text generation tasks into three dimensions of variations, namely, the varying data types and formats for different use cases, the arbitrary combinational model architectures and associated inference procedures (e.g., Figure 1), and the distinct learning algorithms such as maximum likelihood learning, reinforcement learning, and combinations thereof (e.g., Figure 2). Within the uniÔ¨Åed abstraction, all learning paradigms are each specifying one or multiple loss functions (e.g., cross-entropy loss, policy gradient loss), along with an optimization procedure that improves the losses:

minŒ∏ L(fŒ∏, D)

(1)

where f Œ∏ is the model that deÔ¨Ånes the model architecture and the inference procedure; D is the data; L is the learning objectives (losses); and min denotes the optimization procedure. Note that the above can have multiple losses imposed on different model parts (e.g., adversarial learning).
Further, as illustrated in Figure 4, we decouple learning, inference, and model architecture to the maximum extent, forming abstraction layers of learning ‚Äì inference ‚Äì architecture, as illustrated in Figure 4. That is, different architectures implement the same set of inference procedures and provide the same interfaces, so that learning algorithms can call proper inference procedures as subroutines while staying agnostic to the underlying architecture and implementation details. For example, maximum likelihood learning uses teacher-forcing decoding (Mikolov et al., 2010); a policy gradient algorithm can invoke stochastic or greedy decoding (Ranzato et al., 2015); and adversarial

4

Model architecture Encoder UnidirectionalRNNEncoder BidirectionalRNNEncoder HierarchicalRNNEncoder ConvEncoder TransformerEncoder ‚Ä¶
Model loss

Decoder

Embedder

BasicRNNDecoder
AttentionRNNDecoder
BahdananAttn LuongAttn MonotonicAttn TransformerDecoder Greedy/Sample/BeamSearch/ GumbelSoftmax/‚Ä¶ Decoding
‚Ä¶ Trainer

WordEmbedder (one-hot / soft)
PositionEmbedder
Parametrized Sinusoids
‚Ä¶

Classifier/ Discriminator
RNNClassifier
ConvClassifier
HierarchicalClassifier
‚Ä¶

Connector
MLPTransformer Stochastic ReparameterizedStochastic Concat Forward
...

. . .

Data

Loss
MLE Loss
(Sequence) Cross-entropy
‚Ä¶
Adversarial Loss Binary Adversarial Loss
‚Ä¶
Rewards
‚Ä¶

RL Agent
Seq RL Agent Seq Policy Gradient Agent
‚Ä¶
Episodic RL Agent Policy Gradient Agent DQN Agent Actor-critic Agent
‚Ä¶

Optimization Optimizer Adam/SGD/‚Ä¶ Learning Rate Decay Piecewise/Exp/‚Ä¶ ‚Ä¶

Data
MonoText PairedText MultiAligned Dialog Numerical
...

. . .

Figure 5: The catalog of a subset of modules for model construction and learning. Other modules, such as memory network modules, and those for evaluation and prediction, are omitted due to space limitations.

learning can use either stochastic decoding for policy gradient-based updates (Yu et al., 2017) or Gumbel-softmax reparameterized decoding (Jang et al., 2016) for direct gradient back-propagation. Users can effortlessly switch between different learning algorithms for the same model, by simply specifying the corresponding inference strategy and plugging into the new learning module, without adapting the model architecture (see section 2.3 for a running example).
The uniÔ¨Åed anatomy has underlay the strong modularity of the toolkit. It helps maximize the opportunities for reuse, enable free combinations of different parts, and greatly improve the cleanness of code structure.
Module Assembly. The fast evolution of modeling and learning methodologies has led to sophisticated models that go beyond the canonical (attentional) sequence-to-sequence alike forms and introduce many new composite architectures. (Figure 1 summarizes several model architectures developed in recent literature for different tasks.) To versatilely support all these diverse approaches, we break down the complex models and extract a set of frequently-used modules (e.g., encoders, decoders, embedders, classifiers, etc). Figure 5 shows the catelog of a subset of modules. Crucially, Texar allows free concatenation between these modules in order to assemble arbitrary model architectures. Such concatenation can be done by directly interfacing two modules, or through an intermediate connector module that provides general functionalities of shape transformation, reparameterization (e.g., Kingma and Welling, 2013; Jang et al., 2016), sampling, and others.
User Interfaces. It is crucial for the toolkit to be Ô¨Çexible enough to allow construction of simple and advanced models, while at the same time providing proper abstractions to relieve users from overly concerning about low-level implementations. In particular, Texar provides two major types of user interfaces: 1) YAML conÔ¨Åguration Ô¨Åles that instantiate pre-deÔ¨Åned model templates, and 2) full Python library APIs. The former is simple, clean, straightforwardly understandable for non-expert users, and is also adopted by other toolkits (Britz et al., 2017; Klein et al., 2017), while the latter allows maximal Ô¨Çexibility, full access to internal states, and essentially unlimited customizability. The libray APIs also provide interfaces at different abstract levels for key functionalities, allowing users to select and trade off between readily usability and customizability. Examples are provided in the following sections.
2.2 Assemble Arbitrary Model Architectures
Figure 6 shows an example of specifying an attentional sequence-to-sequence model through either a YAML conÔ¨Åguration Ô¨Åle (left panel), or concise Python code (right panel), respectively.
‚Ä¢ The conÔ¨Åguration Ô¨Åle passes hyperparameters to the model template which instantiates the

5

1 source_embedder: WordEmbedder

2 source_embedder_hparams:

3 dim: 300

4 encoder: UnidirectionalRNNEncoder

5 encoder_hparams:

6 rnn_cell:

7

type: BasicLSTMCell

8

kwargs:

9

num_units: 300

10

num_layers: 1

11

dropout:

12

output_dropout: 0.5

13

variational_recurrent: True

14 embedder_share: True

15 decoder: AttentionRNNDecoder

16 decoder_hparams:

17 attention:

18

type: LuongAttention

19 beam_search_width: 5

20 optimization: ‚Ä¶

1 # Read data

2 dataset = PairedTextData(data_hparams)

3 batch = DataIterator(dataset).get_next()

4

5 # Encode

6 embedder = WordEmbedder(dataset.vocab.size, hparams=embedder_hparams) 7 encoder = TransformerEncoder(hparams=encoder_hparams)

8 enc_outputs = encoder(embedder(batch['source_text_ids']),

9

batch['source_length'])

10

11 # Decode

12 decoder = AttentionRNNDecoder(memory=enc_outputs,

13

hparams=decoder_hparams)

14 outputs, length, _ = decoder(inputs=embedder(batch['target_text_ids']),

15

seq_length=batch['target_length']-1)

16

17 # Loss

18 loss = sequence_sparse_softmax_cross_entropy( 19 labels=batch['target_text_ids'][:,1:], logits=outputs.logits, seq_length=length)

20

Figure 6: Two ways of specifying an attentional sequence-to-sequence model. Left: Snippet of an example YAML conÔ¨Åguration Ô¨Åle of the sequence-to-sequence model template. Only those hyperparameters that the user concerns are speciÔ¨Åed explicitly in the particular Ô¨Åle, while the remaining many hyperparameters can be omitted and will take default values. Right: Python code assembling the sequence-to-sequence model using the Texar library APIs. Modules are created as Python objects, and then called as functions to add TensorFlow ops to the computation graph and return output tensors. Other code such as optimization is omitted.

model for subsequent training and evaluation (which are also conÔ¨Ågured through YAML). Text highlighted in blue in the Ô¨Ågure speciÔ¨Åes the names of modules to use. Module hyperparameters are speciÔ¨Åed under *_hparams in the conÔ¨Åguration hierarchy (for example, source_embedder_hparams for the source_embedder module). Note that most of the hyperparameters have sensible default values, and users only have to specify a small subset of them. Hyperparameters taking default values can be omitted in the conÔ¨Åguration Ô¨Åle.
‚Ä¢ The library APIs enable users to efÔ¨Åciently build any desired pipelines at a high conceptual level, without worrying too much about the low-level implementations. Power users are also given the option to access the full internal states for native programming and low-level manipulations.
Texar modules have multiple features for ease of use, including 1) Convenient variable reuse: Each module instance creates its own sets of variables, and automatically re-uses its variables on subsequent calls. Hence TensorFlow variable scope is transparent to users; 2) ConÔ¨Ågurable through hyperparameters: Each module deÔ¨Ånes allowed hyperparameters and default values. Hyperparameter values are conÔ¨Ågured by passing the hparams argument to the module constructor, which precisely corresponds to the *_hparams sections in YAML conÔ¨Åguration Ô¨Åles; 3) PyTorch-alike function calls: As in Figure 6, after a module is created as an object, it can be called as a function which performs the module logic on input tensors and returns output tensors. Both the Texar TensorFlow and PyTorch versions have the same interfaces.
2.3 Plug-in and Swap-out Modules
Texar builds a shared abstraction of the broad set of text generation tasks. It is convenient to switch between different application contexts, or change from one modeling paradigm to another, by simply plugging in/swapping out a single or few modules, or even merely changing a conÔ¨Åguration parameter, while keeping all other parts of the modeling pipeline unchanged.
For example, given the base code of the sequence-to-sequence model in Figure 6 (right panel), Figure 7 illustrates how Texar can easily support switching between different learning algorithms, by changing only the relevant code snippet (i.e., Line.14‚Äì19 in Figure 6 right panel). In particular,

6

(a) Maximum likelihood learning

Cross entropy loss

!1

!2

‚Ä¶

Decoder

<BOS> (b) Adversarial learning
!

Discriminator

!"1

!"2

0/1 ‚Ä¶

outputs, length, _ = decoder(

# Teacher-forcing greedy decoding

inputs=embedder(batch['target_text_ids']),

seq_length=batch['target_length']-1,

decoding_strategy='train_greedy')

loss = sequence_sparse_softmax_cross_entropy( labels=data['target_text_ids'][:,1:], logits=outputs.logits, seq_length=length)

helper = GumbelSoftmaxTrainingHelper(

# Gumbel-softmax decoding

start_tokens=[BOS]*batch_size, end_token=EOS, embedding=embedder)

outputs, _, _ = decoder(helper=helper)

discriminator = Conv1DClassifier(hparams=conv_hparams)

Decoder
<BOS> (c) Reinforcement learning

G_loss, D_loss = binary_adversarial_losses( embedder(data[‚Äòtarget_text_ids‚Äô][:, 1:]), embedder(soft_ids=softmax(outputs.logits)), discriminator)

Policy Gradient Agent

!

!"1

!"2

‚Ä¶

!"

BLEU

outputs, length, _ = decoder(

# Random sample decoding

start_tokens=[BOS]*batch_size, end_token=EOS,

embedding=embedder, decoding_strategy=‚Äòinfer_sample')

agent = SeqPGAgent( samples=outputs.sample_id, logits=outputs.logits, seq_length=length)

<BOS> Decoder

Rewards

for _ in range(STEPS): samples = agent.get_samples() rewards = BLEU(batch[‚Äòtarget_text_ids‚Äô], samples) agent.observe(rewards) # Train the policy (decoder)

Figure 7: Switching between different learning paradigms of a decoder involves only modiÔ¨Åcation of Line.14-19 in the right panel of Figure 6. In particular, the same decoder is called with different decoding strategies, and discriminator or reinforcement learning agent is added as needed with simple API calls. (Left): Module structure of each paradigm; (Right): The respective code snippets. For adversarial learning in (b), continuous Gumbel-softmax approximation (Jang et al., 2016) to the generated samples (with GumbelSoftmaxTrainingHelper) is used to enable gradient propagation from the discriminator to the decoder.

Figure 7 shows three major learning paradigms, including maximum-likelihood based supervised learning, adversarial learning, and reinforcement learning, each of which invokes different decoding (inference) methods of the decoder, namely teacher-forcing decoding, Gumbel-softmax decoding, and random-sample decoding, respectively.
The convenient module switch can be useful for fast exploration of different algorithms for a speciÔ¨Åc task, or quick experimentation of an algorithm‚Äôs generalization on different tasks.

2.4 Customize with Extensible Interfaces

Texar emphasizes heavily on extensibility, and allows easy addition of customized or external modules through various interface1s, without editing the Texar codebase.

With the YAML conÔ¨Åguratio2n Ô¨Åle, users can directly insert their own modules by providing the

Python importing path to the3 module. For example, to use an externally implemented RNN cell

in the sequence-to-sequence4model encoder, one can simply change Lines.6-9 in the left panel of

Figure 6 to the following: 5

6

7 rnn_cell:

8 type: path.to.MyCell

9 kwargs:

10

my_kwarg_1: 123

11

my_kwarg_2: ‚Äòxyz‚Äô

12

‚Ä¶

7

Task: VAE language modeling

Dataset

Metrics

Yahoo (Yang et al., 2017)

Test PPL Test NLL

PTB (Bowman et al., 2015) Test PPL Test NLL

VAE-LSTM
68.31 337.36
105.27 102.06

VAE-Transformer
61.26 328.67
102.46 101.46

Table 1: Deployment of Transformer on the task of VAE language modeling (Bowman et al., 2015). Both test set perplexity (PPL) and sentence-level negative log likelihood (NLL) are evaluated (The lower the better). The model with a Transformer decoder consistently outperforms the one with a conventional LSTM decoder. For fair comparison, both models have the same parameter size.

Task: Conversation generation

Metrics

HERD-GRU

BLEU-3 prec 0.281 BLEU-3 recall 0.256

BLEU-4 prec 0.228 BLEU-4 recall 0.205

HERD-Transformer
0.289 0.273
0.232 0.214

Table 2: Comparison of Transformer decoder and GRU RNN decoder within the conversation model HERD (Serban et al., 2016) for response generation, on the Switchboard dataset (Zhao et al., 2017).

as long as the MyCell class is accessible by Python, and its interface is compatible to other parts of the model.
Incorporating customized modules with Texar library APIs is even more Ô¨Çexible and straightforward. As the library APIs are designed to be coherent with the native TensorFlow/PyTorch programming interfaces, any externally-deÔ¨Åned modules can directly be combined with Texar components as needed.
3 Case Studies
In this section, we conduct several case studies to show that Texar can greatly reduce implementation efforts and easily enable technique sharing among different tasks.
3.1 One Technique on Many Tasks: Transformer
Transformer, as Ô¨Årst introduced in (Vaswani et al., 2017), has greatly improved the machine translation results and created other successful models, such as BERT for text embedding (Devlin et al., 2019), GPT-2 for language modeling (Radford et al., 2018), text inÔ¨Ålling (Zhu et al., 2019), etc. Texar supports easy construction of these models and Ô¨Åne-tuning pretrained weights. On Texar, we can easily deploy the Transformer components to various other tasks and get improved results.
The Ô¨Årst task we explore is the variational autoencoder (VAE) language modeling (Bowman et al., 2015). LSTM RNN has been widely-used in VAE for decoding sentences. We follow the experimental setting in previous work (Bowman et al., 2015; Yang et al., 2017), and test two models, one with the traditional LSTM RNN decoder, and the other with the Transformer decoder. All other model conÔ¨Ågurations are the same in the two models. Notably, with Texar, changing the decoder from an LSTM to a Transformer is achieved by modifying only 3 lines of code. Table 1 shows the results. We see that the Transformer VAE consistently improves over the conventional LSTM VAE, showing that the Transformer architecture can beneÔ¨Åt tasks beyond machine translation.
It is worth noting that, building the VAE language model (including data reading, model construction and optimization speciÔ¨Åcation) on Texar uses only 70 lines of code (with the length of each line < 80 chars). As a (rough) reference, a popular public TensorFlow code (Li, 2017) of the same model has used around 400 lines of code for the same part (without line length limit).

8

Models
LSTM RNN with MLE (Zaremba et al., 2014) LSTM RNN with seqGAN (Yu et al., 2017) Memory Network LM (Sukhbaatar et al., 2015)

Test PPL
74.23 74.12 94.82

Lines of Model Code
42 115 39

Table 3: Comparison of the three models on the task of language modeling, using the PTB dataset (Zaremba et al., 2014). The lower the test perplexity (PPL), the better. We also report the lines of code for implementing the respective model (including model construction, and loss/optimization speciÔ¨Åcation, excluding data reading). In comparison, the ofÔ¨Åcial implementation of SeqGAN on TensorFlow involves 480 lines of model code (Yu et al., 2017). Texar implementations have limited the maximum length of each line to 80 chars.

Models
Shen et al. (2017)
Shen et al. (2017) on Texar Hu et al. (2017a) on Texar

Accuracy
79.5
82.5 88.6

BLEU
12.4
13.0 38.0

Lines of Model Code
485
136 105

Table 4: Text style transfer on the Yelp data (Shen et al., 2017). The Ô¨Årst row is the original opensource implementation by the authors of (Shen et al., 2017). The subsequent two rows are Texar implementations of the two work. Texar implementations have limited the maximum length of each line to 80 chars.

The second task is to generate a response given conversation history. Following (Serban et al., 2016), the conversation history is encoded with a Texar module HierarchicalRNNEncoder which is followed with a decoder to generate the response. Similar as above, we study the performance of a Transformer decoder in comparison with a more conventional GRU RNN decoder. Table 2 shows the results. Again, we see that the Transformer model generalizes well to the conversation generation setting, and consistently outperforms the GRU RNN counterpart. Regarding implementation efforts, our implementation based on Texar has around 100 lines of code, while the reference code (Zhao et al., 2017) based on TensorFlow involves over 600 lines for constructing the same part.

3.2 One Task with Many Techniques: Language Modeling
We next showcase how Texar can support investigation of diverse techniques on a single task. This can be valuable for research community to standardize experimental conÔ¨Ågurations and foster fair, reproducible comparisons. As a case study, we choose the standard language modeling task (Zaremba et al., 2014). Note that this is different from the VAE language modeling task above, due to different data partition strategies conventionally adopted in respective research lines.
We compare three models as shown in Table 3. The LSTM RNN trained with maximum likelihood estimation (MLE) (Zaremba et al., 2014) is the most widely used model for language modeling, due to its simplicity and prominent performance. We use the exact same architecture as generator and setup a (seq)GAN (Yu et al., 2017) framework to train the language model with adversarial learning. (The generator is pre-trained with MLE.) From Table 3 we see that adversarial learning (almost) does not improve in terms of perplexity (which can be partly because of the high variance of the policy gradient in seqGAN learning). We further evaluate a memory network-based language model (Sukhbaatar et al., 2015) which has the same number of parameters with the LSTM RNN model. The test set perplexity is signiÔ¨Åcantly higher than the LSTM RNNs in our experiments, which is not unexpected because LSTM RNN models are well studied for language modeling and a number of optimal modeling and optimization choices are already known.
Besides the benchmark performance of the various models, Table 3 also reports the amount of code for implementing each of the models. Texar makes the development very efÔ¨Åcient.

3.3 Composite Model Architectures: Text Style Transfer
To further demonstrate the versatility of Texar for composing complex model architectures, we next choose the newly-emerging task of text style transfer (Hu et al., 2017a; Shen et al., 2017). The task aims to manipulate the text of an input sentence to change from one attribute to another (e.g., from

9

positive sentiment to negative), given only non-parallel training data of each style. The criterion is that the output sentence accurately entails the target style, while preserving the original content and other properties well.
We use Texar to implement the models from both (Hu et al., 2017a) and (Shen et al., 2017), whose model architectures fall in the category (f) and (g) in Figure 1, respectively. Experimental settings mostly follow those in (Shen et al., 2017). Following previous setting, we use a pre-trained sentiment classiÔ¨Åer to evaluate the transferred style accuracy. For evaluating how well the generated sentence preserves the original content, we measure the BLEU score between the generated sentence and the original one (the higher the better) (Yang et al., 2018). Table 4 shows the results. Our reimplementation of (Shen et al., 2017) replicates and slightly surpasses the original results, while the implementation of (Hu et al., 2017a) provides better performance in terms of the two metrics. Implementations on Texar use fewer lines of code for the composite model construction.
4 Related Work
Text generation is a broad research area with rapid advancement. There exist several toolkits that focus on one or a few speciÔ¨Åc tasks. For example, for neural machine translation and alike, there are Google Seq2seq (Britz et al., 2017) and Tensor2Tensor (Vaswani et al., 2018) on TensorFlow, OpenNMT (Klein et al., 2017) on (Py)Torch, XNMT (Neubig et al., 2018) on DyNet, Nematus (Sennrich et al., 2017) on Theano, MarianNMT (Junczys-Dowmunt et al., 2018) on C++, and others. For dialogue, ParlAI (Miller et al., 2017) is a software platform specialized for research in the Ô¨Åeld. Differing from these highly task-focusing toolkits, Texar aims to cover as many text generation tasks as possible. The goal of versatility poses unique challenges to the design, requiring high modularity and extensibility.
On the other end of spectrum, there are libraries and tools for more general natural language processing (NLP) or deep learning applications. For example, AllenNLP (AllenAI, 2018), QickNLP (Pytorch, 2018), GluonNLP (DMLC, 2018) and others are designed for the broad NLP tasks in general, while Keras (Chollet et al., 2017) is for high conceptual-level programming without speciÔ¨Åc task focuses. In comparison, though extensible to broader techniques and applications, Texar has a proper focus on the text generation sub-area, and provide a comprehensive set of modules and functionalities that are well-tailored and readily-usable for relevant tasks. For example, Texar provides rich text docoder modules with optimized interfaces to support over ten decoding methods (see section 2.3 for an example), all of which can be invoked and interact with other modules conveniently.
It is also notable that some platforms have been developed for speciÔ¨Åc types of algorithms, such as OpenAI Gym (Brockman et al., 2016), DeepMind Control Suite (Tassa et al., 2018), and ELF (Tian et al., 2017) for reinforcement learning in game environments. Texar has drawn inspirations from these toolkits when designing relevant speciÔ¨Åc algorithm supports.
5 Conclusion
This paper has introduced Texar, an open-source, general-purpose toolkit on both TensorFlow and PyTorch, that supports the broad set of machine learning, especially text generation, applications and algorithms. The toolkit is modularized to enable easy replacement of components, and extensible to allow seamless integration of any external or customized modules. We are excited to further enrich the toolkit to support a broader set of natural language processing and machine learning applications.
References
AllenAI. AllenNLP. 2018. URL http://allennlp.org.
D. Bahdanau, K. Cho, and Y. Bengio. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473, 2014.
D. Bahdanau, P. Brakel, K. Xu, A. Goyal, R. Lowe, J. Pineau, A. Courville, and Y. Bengio. An actor-critic algorithm for sequence prediction. arXiv preprint arXiv:1607.07086, 2016.
10

A. Bordes, Y.-L. Boureau, and J. Weston. Learning end-to-end goal-oriented dialog. arXiv preprint arXiv:1605.07683, 2016.
S. R. Bowman, L. Vilnis, O. Vinyals, A. M. Dai, R. Jozefowicz, and S. Bengio. Generating sentences from a continuous space. arXiv preprint arXiv:1511.06349, 2015.
D. Britz, A. Goldie, T. Luong, and Q. Le. Massive exploration of neural machine translation architectures. arXiv preprint arXiv:1703.03906, 2017.
G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, and W. Zaremba. OpenAI gym. arXiv preprint arXiv:1606.01540, 2016.
P. F. Brown, J. Cocke, S. A. D. Pietra, V. J. D. Pietra, F. Jelinek, J. D. Lafferty, R. L. Mercer, and P. S. Roossin. A statistical approach to machine translation. Computational linguistics, 16(2): 79‚Äì85, 1990.
F. Chollet et al. Keras (2015), 2017.
J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. NAACL, 2019.
DMLC. GluonNLP. 2018. URL https://github.com/dmlc/gluon-nlp.
W. Fedus, I. Goodfellow, and A. M. Dai. Maskgan: Better text generation via Ô¨Ålling in the _. In ICLR, 2018.
O. Firat, K. Cho, and Y. Bengio. Multi-way, multilingual neural machine translation with a shared attention mechanism. arXiv preprint arXiv:1601.01073, 2016.
I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Generative adversarial nets. In NeurIPS, pages 2672‚Äì2680, 2014.
J. Gu, Z. Lu, H. Li, and V. O. Li. Incorporating copying mechanism in sequence-to-sequence learning. arXiv preprint arXiv:1603.06393, 2016.
C. Gulcehre, S. Ahn, R. Nallapati, B. Zhou, and Y. Bengio. Pointing the unknown words. arXiv preprint arXiv:1603.08148, 2016.
I. Hal Daum√©, J. Langford, and D. Marcu. Search-based structured prediction as classiÔ¨Åcation. Journal Machine Learning, 2009.
E. Hovy and C.-Y. Lin. Automated text summarization and the SUMMARIST system. In Proceedings of a workshop on held at Baltimore, Maryland: October 13-15, 1998, pages 197‚Äì214. Association for Computational Linguistics, 1998.
Z. Hu, Z. Yang, X. Liang, R. Salakhutdinov, and E. P. Xing. Toward controlled generation of text. In ICML, 2017a.
Z. Hu, Z. Yang, R. Salakhutdinov, and E. P. Xing. On unifying deep generative models. In ICLR, 2017b.
Z. Hu, Z. Yang, R. Salakhutdinov, X. Liang, L. Qin, H. Dong, and E. Xing. Deep generative models with learnable knowledge constraints. In NeurIPS, 2018.
E. Jang, S. Gu, and B. Poole. Categorical reparameterization with Gumbel-softmax. arXiv preprint arXiv:1611.01144, 2016.
M. Junczys-Dowmunt, R. Grundkiewicz, T. Dwojak, H. Hoang, K. HeaÔ¨Åeld, T. Neckermann, F. Seide, U. Germann, A. F. Aji, N. Bogoychev, A. F. T. Martins, and A. Birch. Marian: Fast neural machine translation in C++. arXiv preprint arXiv:1804.00344, 2018.
A. Karpathy and L. Fei-Fei. Deep visual-semantic alignments for generating image descriptions. In CVPR, pages 3128‚Äì3137, 2015.
D. P. Kingma and M. Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.
11

G. Klein, Y. Kim, Y. Deng, J. Senellart, and A. M. Rush. OpenNMT: Open-source toolkit for neural machine translation. arXiv preprint arXiv:1701.02810, 2017.
A. M. Lamb, A. G. A. P. GOYAL, Y. Zhang, S. Zhang, A. C. Courville, and Y. Bengio. Professor forcing: A new algorithm for training recurrent networks. In NeurIPS, pages 4601‚Äì4609, 2016.
G. Lample, L. Denoyer, and M. Ranzato. Unsupervised machine translation using monolingual corpora only. arXiv preprint arXiv:1711.00043, 2017.
C. Y. Li, X. Liang, Z. Hu, and E. P. Xing. Hybrid retrieval-generation reinforced agent for medical image report generation. arXiv preprint arXiv:1805.08298, 2018.
Z.-Y. Li. A TensorÔ¨Çow implementation of VAE. 2017. URL https://github.com/ Chung-I/Variational-Recurrent-Autoencoder-Tensorflow.
X. Liang, Z. Hu, H. Zhang, C. Gan, and E. P. Xing. Recurrent topic-transition GAN for visual paragraph generation. In ICCV, volume 2, 2017.
S. Lin, W. Wang, Z. Yang, H. Shi, F. Xu, X. Liang, E. Xing, and Z. Hu. Towards unsupervised text content manipulation. arXiv preprint arXiv:1901.09501, 2019.
M.-T. Luong, H. Pham, and C. D. Manning. Effective approaches to attention-based neural machine translation. arXiv preprint arXiv:1508.04025, 2015.
M.-T. Luong, Q. V. Le, I. Sutskever, O. Vinyals, and L. Kaiser. Multi-task sequence to sequence learning. In ICLR, 2016.
N. Madnani and B. J. Dorr. Generating phrasal and sentential paraphrases: A survey of data-driven methods. Computational Linguistics, 36(3):341‚Äì387, 2010.
T. Mikolov, M. KaraÔ¨Å√°t, L. Burget, J. CÀá ernocky`, and S. Khudanpur. Recurrent neural network based language model. In Eleventh Annual Conference of the International Speech Communication Association, 2010.
A. H. Miller, W. Feng, A. Fisch, J. Lu, D. Batra, A. Bordes, D. Parikh, and J. Weston. ParlAI: A dialog research software platform. arXiv preprint arXiv:1705.06476, 2017.
G. Neubig, M. Sperber, X. Wang, M. Felix, A. Matthews, S. Padmanabhan, Y. Qi, D. S. Sachan, P. Arthur, P. Godard, et al. XNMT: The eXtensible neural machine translation toolkit. arXiv preprint arXiv:1803.00188, 2018.
M. Norouzi, S. Bengio, N. Jaitly, M. Schuster, Y. Wu, D. Schuurmans, et al. Reward augmented maximum likelihood for neural structured prediction. In NeurIPS, pages 1723‚Äì1731, 2016.
Pytorch. QuickNLP. 2018. URL https://github.com/outcastofmusic/quick-nlp.
A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever. Language models are unsupervised multitask learners. Technical report, Technical report, OpenAI, 2018.
M. Ranzato, S. Chopra, M. Auli, and W. Zaremba. Sequence level training with recurrent neural networks. arXiv preprint arXiv:1511.06732, 2015.
S. J. Rennie, E. Marcheret, Y. Mroueh, J. Ross, and V. Goel. Self-critical sequence training for image captioning. In CVPR, pages 7008‚Äì7024, 2017.
A. See, P. J. Liu, and C. D. Manning. Get to the point: Summarization with pointer-generator networks. arXiv preprint arXiv:1704.04368, 2017.
R. Sennrich, O. Firat, K. Cho, A. Birch, B. Haddow, J. Hitschler, M. Junczys-Dowmunt, S. L√§ubli, A. V. M. Barone, J. Mokry, et al. Nematus: a toolkit for neural machine translation. arXiv preprint arXiv:1703.04357, 2017.
I. V. Serban, A. Sordoni, Y. Bengio, A. C. Courville, and J. Pineau. Building end-to-end dialogue systems using generative hierarchical neural network models. 2016.
12

T. Shen, T. Lei, R. Barzilay, and T. Jaakkola. Style transfer from non-parallel text by crossalignment. In NeurIPS, pages 6833‚Äì6844, 2017.
S. Sukhbaatar, J. Weston, R. Fergus, et al. End-to-end memory networks. In NeurIPS, pages 2440‚Äì 2448, 2015.
I. Sutskever, O. Vinyals, and Q. V. Le. Sequence to sequence learning with neural networks. In NeurIPS, pages 3104‚Äì3112, 2014.
B. Tan, Z. Hu, Z. Yang, R. Salakhutdinov, and E. Xing. Connecting the dots between MLE and RL for sequence generation. arXiv preprint arXiv:1811.09740, 2018.
J. Tang, T. Zhao, C. Xiong, X. Liang, E. P. Xing, and Z. Hu. Target-guided open-domain conversation. In ACL, 2019.
Y. Tassa, Y. Doron, A. Muldal, T. Erez, Y. Li, D. d. L. Casas, D. Budden, A. Abdolmaleki, J. Merel, A. Lefrancq, et al. Deepmind control suite. arXiv preprint arXiv:1801.00690, 2018.
Y. Tian, Q. Gong, W. Shang, Y. Wu, and C. L. Zitnick. ELF: An extensive, lightweight and Ô¨Çexible research platform for real-time strategy games. In NeurIPS, pages 2656‚Äì2666, 2017.
A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, ≈Å. Kaiser, and I. Polosukhin. Attention is all you need. In NeurIPS, pages 6000‚Äì6010, 2017.
A. Vaswani, S. Bengio, E. Brevdo, F. Chollet, A. N. Gomez, S. Gouws, L. Jones, ≈Å. Kaiser, N. Kalchbrenner, N. Parmar, et al. Tensor2Tensor for neural machine translation. arXiv preprint arXiv:1803.07416, 2018.
O. Vinyals, M. Fortunato, and N. Jaitly. Pointer networks. In NeurIPS, pages 2692‚Äì2700, 2015a.
O. Vinyals, A. Toshev, S. Bengio, and D. Erhan. Show and tell: A neural image caption generator. In CVPR, pages 3156‚Äì3164. IEEE, 2015b.
J. D. Williams and S. Young. Partially observable Markov decision processes for spoken dialog systems. Computer Speech & Language, 21(2):393‚Äì422, 2007.
S. Wiseman and A. M. Rush. Sequence-to-sequence learning as beam-search optimization. In EMNLP, pages 1296‚Äì1306, 2016.
S. Wiseman, S. M. Shieber, and A. M. Rush. Challenges in data-to-document generation. arXiv preprint arXiv:1707.08052, 2017.
Z. Yang, Z. Hu, R. Salakhutdinov, and T. Berg-Kirkpatrick. Improved variational autoencoders for text modeling using dilated convolutions. In ICML, 2017.
Z. Yang, Z. Hu, C. Dyer, E. P. Xing, and T. Berg-Kirkpatrick. Unsupervised text style transfer using language models as discriminators. In NeurIPS, 2018.
L. Yu, W. Zhang, J. Wang, and Y. Yu. SeqGAN: Sequence generative adversarial nets with policy gradient. In AAAI, pages 2852‚Äì2858, 2017.
W. Zaremba, I. Sutskever, and O. Vinyals. Recurrent neural network regularization. arXiv preprint arXiv:1409.2329, 2014.
Y. Zhang, Z. Gan, K. Fan, Z. Chen, R. Henao, D. Shen, and L. Carin. Adversarial feature matching for text generation. arXiv preprint arXiv:1706.03850, 2017.
T. Zhao, R. Zhao, and M. Eskenazi. Learning discourse-level diversity for neural dialog models using conditional variational autoencoders. In ACL, 2017. URL https://github.com/ snakeztc/NeuralDialog-CVAE.
W. Zhu, Z. Hu, and E. Xing. Text inÔ¨Ålling. arXiv preprint arXiv:1901.00158, 2019.
13

