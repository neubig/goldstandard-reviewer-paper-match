Factor Graph Grammars

arXiv:2010.12048v1 [cs.LG] 22 Oct 2020

David Chiang University of Notre Dame
dchiang@nd.edu

Darcey Riley University of Notre Dame darcey.riley@nd.edu

Abstract
We propose the use of hyperedge replacement graph grammars for factor graphs, or factor graph grammars (FGGs) for short. FGGs generate sets of factor graphs and can describe a more general class of models than plate notation, dynamic graphical models, case–factor diagrams, and sum–product networks can. Moreover, inference can be done on FGGs without enumerating all the generated factor graphs. For ﬁnite variable domains (but possibly inﬁnite sets of graphs), a generalization of variable elimination to FGGs allows exact and tractable inference in many situations. For ﬁnite sets of graphs (but possibly inﬁnite variable domains), a FGG can be converted to a single factor graph amenable to standard inference techniques.
1 Introduction
Graphs have been used with great success as representations of probability models, both Bayesian and Markov networks (Koller and Friedman, 2009) as well as latent-variable neural networks (Schulman et al., 2015). But in many applications, especially in speech and language processing, a ﬁxed graph is not sufﬁcient. The graph may have substructures that repeat a variable number of times: for example, a hidden Markov model (HMM) depends on the number of words in the string. Or, part of the graph may have several alternatives with different structures: for example, a probabilistic context-free grammar (PCFG) contains many trees for a given string.
Several formalisms have been proposed to ﬁll this need. Plate notation (Buntine, 1994), plated factor graphs (Obermeyer et al., 2019), and dynamic graphical models (Bilmes, 2010) address the repeated-substructure problem, but only for sequence models like HMMs. Case–factor diagrams (McAllester et al., 2008) and sum–product networks (Poon and Domingos, 2011) address the alternative-substructure problem, so they can describe PCFGs, but only for ﬁxed-length inputs.
More general formalisms like probabilistic relational models (Getoor et al., 2007) and probabilistic programming languages (van de Meent et al., 2018) address both problems successfully, but because of their generality, tractable exact inference in them is often not possible.
Here, we explore the use of hyperedge replacement graph grammars (HRGs), a formalism for deﬁning sets of graphs (Bauderon and Courcelle, 1987; Habel and Kreowski, 1987; Drewes et al., 1997). We show that HRGs for factor graphs, or factor graph grammars (FGGs) for short, are expressive enough to solve both the repeated-substructure and alternative-substructure problems, and constrained enough allow exact and tractable inference in many situations. We make three main contributions:
• We deﬁne FGGs and show how they generalize the constrained formalisms mentioned above (§3).
• We deﬁne a conjunction operation that enables one to modularize a FGG into two parts, one which deﬁnes the model and one which deﬁnes a query (§4).
• We show how to perform inference on FGGs without enumerating the (possibly inﬁnite) set of graphs they generate. For ﬁnite variable domains, we generalize variable elimination to FGGs
34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.

(§5.1). For some FGGs, this is exact and tractable; for others, it gives a sequence of successive approximations.
For inﬁnite variable domains, we show that if a FGG generates a ﬁnite set, it can be converted to a single factor graph, to which standard graphical model inference methods can be applied (§5.2). But if a FGG generates an inﬁnite set, inference is undecidable (§5.3).

2 Background

In this section, we provide some background deﬁnitions for hypergraphs (§2.1), factor graphs (§2.2), and HRGs (§2.3). Our deﬁnitions are mostly standard, but not entirely; readers already familiar with these concepts may skip these subsections and refer back to them as needed.

2.1 Hypergraphs
Assume throughout this paper the following “global” structures. Let LV be a ﬁnite set of node labels and LE be a ﬁnite set of edge labels, and assume there is a function type : LE → (LV )∗, which says for each edge label what the number and labels of the endpoint nodes must be. Deﬁnition 1. A hypergraph (or simply a graph) is a tuple (V, E, att, labV , labE), where
• V is a ﬁnite set of nodes.
• E is a ﬁnite set of hyperedges (or simply edges). • att : E → V ∗ maps each edge to zero or more endpoint nodes, not necessarily distinct.
• labV : V → LV assigns labels to nodes.
• labE : E → LE assigns labels to edges.
• For all e, |att(e)| = |type(labE(e))|, and if att(e) = v1 · · · vk and type(labE(e)) = 1 · · · k, then labV (vi) = i for i = 1, . . . , k.
Although the elements of V and E can be anything, we assume in our examples that they are natural numbers. If a node v has label , we draw it as a circle with v inside it. We draw a hyperedge as a square with lines to its endpoints. In principle, we would need to indicate the ordering of the endpoints somehow, but we omit this to reduce clutter.

2.2 Factor graphs

Deﬁnition 2. A factor graph (Kschischang et al., 2001) is a hypergraph (V, E, att, labV , labE) together with mappings Ω and F , where
• Ω maps node labels to sets of possible values. For brevity, we write Ω(v) for Ω(labV (v)).
• F maps edge labels to functions. For brevity, we write F (e) for F (labE(e)). For every edge e with att(e) = v1 · · · vk, F (e) is of type Ω(v1) × · · · × Ω(vk) → R≥0.
A node v together with its domain Ω(v) is called a variable. An edge e together with its function F (e) is called a factor.

We draw a factor e as a small square, but instead of writing its label, we write F (e) next to it, as an expression in terms of its endpoints. As shorthand, we often write Boolean expressions, which are implicitly converted to real numbers (true = 1 and false = 0).
Example 3. Although HMMs are deﬁned for sentences of arbitrary length, factor graphs force us to choose a ﬁxed length; below is a HMM for sentences of length 3. (Here, T and W are node labels, Ω(T) is the set of possible tags, and Ω(W) is the set of possible words.)

T0 = BOS

p(T1 | T0)

p(T3 | T1)

p(T5 | T3)

p(T7 | T5)

T7 = EOS

T0

T1

T3

T5

T7

p(W2 | T1)

p(W4 | T3)

p(W6 | T5)

W2

W4

W6

2

Deﬁnition 4. If H is a factor graph, deﬁne an assignment ξ of H to be a mapping from nodes to values: ξ(v) ∈ Ω(v). We write ΞH for the set of all assignments of H. The weight of an assignment ξ is given by

wH (ξ) =

F (e)(ξ(v1), . . . , ξ(vk)).

edges e with att(e) = v1 · · · vk

In a factor graph with no factors, every assignment has weight 1. A factor graph with no variables has exactly one assignment.

Factor graphs are general enough to represent Bayesian networks and Markov networks. They can also represent stochastic computation graphs (SCGs), introduced by Schulman et al. (2015) for latent-variable neural networks.

2.3 Hyperedge Replacement Graph Grammars
Hyperedge replacement graph grammars (HRGs) were introduced by Bauderon and Courcelle (1987) and Habel and Kreowski (1987), and surveyed by Drewes et al. (1997). They generate graphs by using a context-free rewriting mechanism that replaces nonterminal-labeled edges with graphs. In this section, we provide a brief deﬁnition of HRGs, with a minor extension for node labels. Deﬁnition 5. A hypergraph fragment is a tuple (V, E, att, labV , labE, ext), where
• (V, E, att, labV , labE) is a hypergraph,
• ext ∈ V ∗ is a sequence of zero or more external nodes.
In our ﬁgures, we draw external nodes as black nodes. In principle, we would need to indicate their ordering somehow, but we omit this to reduce clutter. Deﬁnition 6. A hyperedge replacement graph grammar (HRG) is a tuple (N, T, P, S), where
• N ⊆ LE is a ﬁnite set of nonterminal symbols.
• T ⊆ LE is a ﬁnite set of terminal symbols, such that N ∩ T = ∅.
• P is a ﬁnite set of rules of the form (X → R), where
• X ∈ N.
• R is a hypergraph fragment with edge labels in N ∪ T .
• If R has external nodes x1 · · · xk, then type(X) = labV (x1) · · · labV (xk).
• S ∈ N is a distinguished start nonterminal symbol with type(S) = .
Although a left-hand side X is formally just a nonterminal symbol, we draw it as a hyperedge labeled X inside, with replicas of the external nodes as its endpoints. On right-hand sides, we draw an edge e with nonterminal label X as a square with Xe inside. If R is the empty graph, we write ∅.
Intuitively, a HRG generates graphs by starting with a hyperedge labeled S and repeatedly selecting an edge e labeled X and a rule X → R and replacing e with R. (See Figure 1a for an example, where H = R.) Replacement stops when there are no more nonterminal-labeled edges.
As with a CFG, we can abstract away from the ordering of replacement steps using a derivation tree, in which the nodes are labeled with HRG rules, and an edge from parent π1 to child π2 has a label indicating which edge in the right-hand side of π1 is replaced with the right-hand side of π2. Deﬁnition 7. Let G be a HRG. For all nonterminals X, deﬁne the set D(G, X) of X-type derivation trees (or simply X-type derivations) of G to be the smallest set containing all ﬁnite, unordered, edge-labeled trees of the form shown in Figure 1b, where π = (X → R) is a rule in G, R has nonterminal-labeled edges e1, . . . , ek with labels X1, . . . , Xk, and for i = 1, . . . , k, Di is an Xi-type derivation. We simply write derivation for S-type derivation, and we let D(G) = D(G, S).
The derived graph of a derivation D is the graph formed as follows. If D is as shown in Figure 1b, then for i = 1, . . . k, let Hi be the derived graph of Di. In (a copy of) R, replace ei with Hi, making the jth endpoint of ei and the jth external node of Hi into the same node (for j = 1, . . . , |att(ei)|). The resulting node is external iff the jth endpoint was. All other nodes are kept distinct. (Again, see Figure 1a for an example with X = Xi and H = Hi.)

3

X

H

=r=ep=la=c⇒e

H

π

e1

ek

···

D1

Dk

(a)

(b)

Figure 1: (a) Example of replacing a hyperedge labeled X with a hypergraph fragment H. Here |type(X)| = 3, but in general, there could be any number of endpoint/external nodes, including zero. (b) A derivation tree.

T1 = BOS

S −π→1

T1

X2

p(T2 | T1)

T1 X −π→2 T1

T2

X4

p(W3 | T2)

W3

p(T2 | T1)

T2 = EOS

T1 X −π→3 T1

T2

π1
X2
π2
X4
π2
X4
π2
X4
π3

(a)

(b)

Figure 2: (a) A FGG generating the inﬁnite set of unrollings of a HMM, one for each sequence length. Each rule is labeled πi for use in the derivation tree. (b) Derivation tree of the factor graph of Example 3. An edge from parent π with label X to child π means that the right-hand side of π replaces the edge labeled X in the right-hand side of π.

From now on, when we mention a derivation D in a context where a graph would be expected, the derived graph of D is to be understood.
3 Factor Graph Grammars
Deﬁnition 8. A HRG for factor graphs, or a factor graph grammar (FGG) for short, is a HRG together with mappings Ω and F , as in the deﬁnition of factor graphs (Deﬁnition 2), except that F is deﬁned on terminal edge labels only. Example 9. Figure 2 shows a FGG which is equivalent to a HMM. It generates an inﬁnite number of graphs, one for each string length. Also shown is the derivation tree of the factor graph of Example 3.
Example 19 in Appendix A shows how to simulate a PCFG in Chomsky normal form as a FGG. The graphs generated by a FGG can be viewed, together with Ω and F , as factor graphs, each of which deﬁnes a (not necessarily normalized) distribution over assignments. Moreover, the whole language of the FGG deﬁnes a (not necessarily normalized) distribution over derivations and assignments to the variables in them. If D ∈ D(G), then
wG(D, ξ) = wD(ξ).
FGGs can simulate several other formalisms for dynamically-structured models. As mentioned above (§1), they can solve two problems that previous formalisms have addressed separately. FGGs can generate repeated substructures like plate notation (Buntine, 1994; Obermeyer et al., 2019) and dynamic graphical models (Bilmes, 2010) can. There are some structures that plate notation can describe that a FGG cannot – like the set of all restricted Boltzmann machines, which have two
4

G

Gw

T1 = BOS

S −→

T1 X2

p(T2 | T1)

T1 X −→ T1

T2 X4

p(W3 | T2)

W3

S −→ T1 (0)2 T1 (i − 1) −→ T1 T2 (i)4
W3

p(T2 | T1) T2 = EOS

T1 X −→ T1

T2

W3 = wi T1 (n) −→ T1 T2

G Gw

T1 = BOS

S, S −→

T1

X, (0)2

p(T2 | T1)

T1 X, (i − 1) −→ T1

T2 X, (i)4

p(W3 | T2)

W3

W3 = wi

p(T2 | T1) T2 = EOS

T1 X, (n) −→ T1

T2

Figure 3: Illustration of the conjunction operation (Example 12). In rules with i in their nonterminals, i ranges from 1 to n where n = |w|.

fully-connected layers of nodes. But these are the same structures that Obermeyer et al. (2019) try to avoid, because inference on them is (believed) intractable. FGG rules these structures out naturally. FGGs can generate alternative substructures like case–factor diagrams (McAllester et al., 2008) and valid sum–product networks (Poon and Domingos, 2011) can; in particular, they can simulate PCFGs in such a way that inference is equivalent to the cubic-time inside and Viterbi algorithms. Theorem 10. All of the following can be converted into an equivalent FGG:
1. Plated factor graphs for which the sum–product algorithm of Obermeyer et al. (2019) succeeds. 2. Dynamic graphical models. 3. Case–factor diagrams. 4. Valid sum–product networks.
Proof. See Appendix B.

4 Conjunction

The preceding examples show how to use FGGs to model the probability of all tagged strings or all trees generated by a grammar. But it’s common for queries to constrain some variables to ﬁxed values, sum over some variables, and get the distribution of the remaining variables. How do such queries generalize to FGGs? For example, in a HMM, how do we compute the probability of all taggings of a given string? Or, how do we compute the marginal distribution of the second-to-last tag?

To answer such questions, we need to be able to specify a set of nodes across the graphs of a graph language, like the second-to-last tag. Our only means of doing this is to specify a particular node in a particular right-hand side, which could correspond to zero, one, or many nodes in the derived graphs. And we can modify a FGG so that a particular node in a particular right-hand side is always (say) the second-to-last tag. But we propose to factor such modiﬁcations into a separate FGG, keeping the FGG describing the model unchanged. Then the modiﬁcations can be applied using a conjunction operation, which we describe in this section.

Conjunction is closely related to synchronous HRGs (Jones et al., 2012), and, because HRG derivation trees are generated by regular tree grammars, to intersection/composition of ﬁnite tree automata/transducers (Comon et al., 2007). It is also similar to the PRODUCT operation on weighted logic programs (Cohen et al., 2011).
Deﬁnition 11. Two FGG rules are conjoinable if they can be written in the form

X1 → R1 X2 → R2

R1 = (V, EN ∪ E1, att N ∪ att 1, labV , labE1 , ext ) R2 = (V, EN ∪ E2, att N ∪ att 2, labV , labE2 , ext ),

5

where

• EN contains only nonterminal edges, and attN is deﬁned on EN . • E1, E2 contain only terminal edges, and att1, att2 are deﬁned on E1, E2, respectively. • type(X1) = type(X2), and for e ∈ EN , type(labE1 (e)) = type(labE2 (e)). Then their conjunction is

X1, X2 → R

R = (V, EN ∪ E1 E2, att, labV , labE, ext)

where means that all edges in E1 and E2 are kept distinct while taking their union, and



lab

E 1

(

e)

,

lab

E 2

(e

)



labE(e) = labE1 (e)

labE2 (e)

if e ∈ EN if e ∈ E1 if e ∈ E2

attN (e)  att(e) = att1(e) att 2 (e)

if e ∈ EN if e ∈ E1 if e ∈ E2.

The conjunction of two FGGs G1 and G2, written as G1 G2, is the FGG containing the conjunction of all conjoinable pairs of rules from G1 and G2.
Example 12. Our FGG for HMMs (Example 9) is repeated in Figure 3 as G. We can constrain the W variables to an observed string w using another FGG, Gw, which has the same variables as G but different factors; its nonterminal edges are the same as G but with different labels. This FGG generates just one graph, whose W nodes spell out the string w. The conjunction of these two FGGs is shown in the last column (G Gw). It combines the factors and nonterminal labels of G and Gw and generates just one graph, the HMM for string w.
Example 13. To compute the distribution of the second-to-last tag, we need a way of identifying the variable for the second-to-last tag across all graphs. We can do this by conjoining with the FGG:

S −π→1 T1 X2

T1 X −π→2 T1 T2 X4

T1 X −π→3 T1 T2 Y4

T1 Y −π→4 T1 T2

W3

W3

Then the second-to-last tag is always node T1 in the right-hand side of rule π3. The methods of the following section can then be used to compute the distribution of this node.

Example 20 in Appendix A shows how to use conjunction to constrain a PCFG to a single input string.

5 Inference

Given a FGG G, we want to be able to efﬁciently compute its sum–product,

ZG =

wG(D, ξ).

D∈D(G) ξ∈ΞD

We can answer a wide variety of queries by using the conjunction operation to constrain variables based on observations, and then computing the sum–product in various semirings: ordinary addition and multiplication would sum over assignments to the remaining variables, and the expectation semiring (Eisner, 2002) would compute expectations with respect to them. The Viterbi (max–product) semiring would ﬁnd the highest-weight derivation and assignment, not necessarily the highest-weight graph and assignment, which is NP-hard (Lyngsø and Pedersen, 2002).
We consider three cases below: ﬁnite variable domains, but possibly inﬁnite graph languages (§5.1); ﬁnite graph languages, but possibly inﬁnite variable domains (§5.2); and inﬁnite variable domains and graph languages (§5.3). To help characterize these cases and their subcases, we introduce the following deﬁnitions.
Deﬁnition 14. A FGG is recursive if it has an X-type derivation that contains an X-type derivation as a proper subtree; otherwise, it is nonrecursive. A nonrecursive FGG generates a ﬁnite set of graphs; this is a common case, because the conjunction of any FGG with a nonrecursive FGG (e.g., one describing a ﬁnite-sized observation) is nonrecursive.

6

A recursive FGG is nonlinearly recursive if it has an X-type derivation that contains two disjoint X-type derivations as proper subtrees; otherwise, it is linearly recursive.
A FGG is nonreentrant if no derivation contains two different X-type derivations as subtrees. Every nonreentrant FGG is nonrecursive, and any nonrecursive FGG can be made nonreentrant by duplicating rules and renaming nonterminals (though this may cause an exponential blowup in the size of the grammar).

5.1 Finite variable domains
When a HRG generates a graph, the derivation tree is isomorphic to a tree decomposition of the graph: each derivation tree node π = (X → R) corresponds to a bag of the tree decomposition containing the nodes in R. It follows that a HRG whose right-hand sides have at most (k + 1) nodes generates graphs with treewidth at most k (Bodlaender, 1998, Theorem 37). So if a FGG G generates a graph H, computing the sum–product of H by variable elimination (VE) takes time linear in the size of H and exponential in k.
In this section, we generalize VE to compute the sum-product of all graphs generated by G without enumerating them. If G is nonrecursive, this is (like VE) linear in the size of G and exponential in k; in the envisioned typical use-case, we have a ﬁxed FGG G representing the model and different FGGs G representing different observations; since conjunction cannot increase k, we may regard k as ﬁxed, so computing the sum–product of G G takes time linear in the size of G G .
Theorem 15. Let G = (N, T, P, S) be a FGG such that for all v in G, |Ω(v)| ≤ m. Let |G| be the number of rules in G, and let k be such that every right-hand side in G has at most (k + 1) nodes. Then ZG is the least solution of a monotone system of polynomial equations, and in particular:
1. If G is nonrecursive, ZG can be computed in O(|G|mk+1) time.
2. If G is linearly recursive, ZG can be computed in O(|G|3m3(k+1)) time in the worst case.

Proof. The computation of the sum–product is closely analogous to the sum–product of a PCFG (Stolcke, 1995; Nederhof and Satta, 2008). We introduce some shorthand for assignments. If ξ is an assignment and v1 · · · vl is a sequence of nodes, we write ξ(v1 · · · vl) for ξ(v1) · · · ξ(vl). If X is a nonterminal and type(X) = 1 . . . k, we deﬁne ΞX = Ω( 1) × · · · × Ω( k), the set of assignments to the endpoints of an edge labeled X.
Next, we deﬁne a system of equations whose solution gives the desired sum–product. The unknowns are ψX (ξ) for all X ∈ N and ξ ∈ ΞX , and τR(ξ) for all rules (X → R) and ξ ∈ ΞX . For all X ∈ N , let P X be the rules in P with left-hand side X. For each ξ ∈ ΞX , add the equation

ψX (ξ) =

τR(ξ).

(X→R)∈P X

For each right-hand side R = (V, EN ∪ ET , att, labV , labE, ext), where EN contains only nonterminal edges and ET contains only terminal edges, and for each ξ ∈ ΞX , add the equation

τR(ξ) =

F (e)(ξ (att(e)))

ψlabE(e)(ξ (att (e))).

ξ ∈ΞR e∈ET ξ (ext)=ξ

e∈EN

Then ξ∈ΞX ψX (ξ) represents the sum–product of all X-type derivations. In particular, the sum– product of the FGG is ψS().
To solve these equations, construct a directed graph over nonterminals with an edge from X to Y iff there is a rule X → R where R contains an edge labeled Y . For each connected component C of this graph in reverse topological order:
1. If C = {X}, compute ψX and substitute it into the other equations.
2. Else if the equations for ψX and τR where X ∈ C and (X → R) ∈ P are linear, solve them and substitute into the other equations (Stolcke, 1995; Goodman, 1999).
3. Else, the equations can be approximated iteratively (Goodman, 1999; Nederhof and Satta, 2008).

7

If G is nonrecursive, the graph of nonterminals is acyclic, so case (1) always applies. The total running time is O(|G|mk+1).
If G is linearly recursive, then case (2) may also apply. In the worst case, the nonterminal graph is one connected component, corresponding to O(|G|mk+1) unknowns. Solving the equations could involve inverting a matrix of this size, which takes O(|G|3m3(k+1)) time.
If G is nonlinearly recursive, any of the three cases may apply. For case (3), each iteration takes O(|G|mk+1) time (ﬁxed-point iteration method) or O(|G|3m3(k+1)) time (Newton’s method), but the number of iterations depends on G.

Finally, we note that we can reduce the sizes of the right-hand sides of a FGG by a process analogous to binarization of CFGs (Gildea, 2011; Chiang et al., 2013): Proposition 16. For any hypergraph fragment R, let R¯ be the hypergraph formed by adding a hyperedge connecting R’s external nodes. Let G be a HRG, nG be the total number of nodes in its right-hand sides, and k be such that for every right-hand side R, the treewidth of R¯ is at most k. Then there is an equivalent HRG with at most nG rules whose right-hand sides have at most (k + 1) nodes.

Proof. See Appendix C.

5.2 Finite graph languages
Next, we show that a nonrecursive FGG can also be converted into an equivalent factor graph, such that the sum–product of the factor graph is equal to the sum–product of the FGG. This makes it possible to use standard graphical model inference techniques for reasoning about the FGG, even with inﬁnite variable domains. However, the conversion increases treewidth in general, so when the method of Section 5.1 is applicable, it should be preferred.
The construction is similar to constructions by Smith and Eisner (2008) and Pynadath and Wellman (1998) for dependency parsers and PCFGs, respectively. Their constructions and ours encode a set of possible derivations as a graphical model, using hard constraints to ensure that every assignment to the variables corresponds to a valid derivation.
Theorem 17. Let G = (N, T, P, S) be a nonreentrant FGG. Let nG and mG be the total number of nodes and edges in the right-hand sides of G respectively. Then G can be converted into a factor graph with O(nG) variables and O(nG + mG) factors which gives the same sum–product.

Proof. We construct a factor graph that encodes all derivations of G. (Example 31 in Appendix D shows an example of this construction for a toy FGG.) First, we add binary variables (with label B where Ω(B) = {true, false}) that switch on or off parts of the factor graph (somewhat like the gates of Minka and Winn (2008)). For each nonterminal X ∈ N , we add BX , indicating whether X is used in the derivation, and for each rule π ∈ P , we add Bπ, indicating whether π is used.

Next, we create factors that constrain the B variables so that only one derivation is active at a time. We write P X for the set of rules with left-hand side X, and P →X for the set of rules which have a
right-hand side edge labeled X. Deﬁne the following function:

CondOnel(B, B1, . . . , Bl) = ∃! i ∈ {1, . . . , l} . Bi if B = true ¬(B1 ∨ · · · ∨ Bl) if B = false

Then we add these factors, which ensure that if one of the rules in P →X is used (or X = S), then exactly one rule in P X is used; if no rule in P →X is used (and X = S), then no rule in P X is used.

• For the start symbol S, add a factor e with att(e) = BS and F (e)(BS) = (BS = true).
• For X ∈ N \ {S}, let P →X = {π1, . . . , πl} and add a factor e with att(e) = BX Bπ1 · · · Bπl and F (e) = CondOnel.
• For X ∈ N , let P X = {π1, . . . , πl} and add a factor e with att(e) = BX Bπ1 · · · Bπl and F (e) = CondOnel.

Next, deﬁne the function:

Cond(B, x) = x if B = true 1 otherwise.

8

For each rule π ∈ P , where π = (X → R) and R = (V, EN ∪ ET , att, labV , labE), we construct a “cluster” Cπ of variables and factors:
• For each v ∈ V , add a variable v with the same label to Cπ. Also, add a factor with endpoints Bπ and v and function CondNormalizev (Bπ, v ), deﬁned to equal Cond(¬Bπ, p(v )), where p is any probability distribution over Ω(v ). This ensures that if π is not used, then v will sum out of the sum–product neatly.
• For each e ∈ ET where att(e) = v1 · · · vk, add a new edge e with att(e ) = Bπ v1 · · · vk and function CondFactore (Bπ, v1, . . . , vk), deﬁned to equal Cond(Bπ, F (e)(v1, . . . , vk)).
Next, for each X ∈ N , let l = |type(X)|. We create a cluster CX containing variables vX,i for i = 1, . . . , l, which represent the endpoints of X, such that labV (vX,i) = type(X)i. We give each an accompanying factor with endpoints Bπ and vX,i and function CondNormalizevX,i . These clusters are used by the factors below, which ensure that if two variables are identiﬁed during rewriting, they have the same value. Deﬁne CondEquals(B, v, v ) = Cond(B, v = v ).
• For each π ∈ P →X , let v1, . . . , vl be the endpoints of the edge in π labeled X. (By nonreentrancy, there can be only one such edge.) For i = 1, . . . , l, create a factor e where att(e) = Bπ vX,i vi and F (e) = CondEquals.
• For each π ∈ P X , let ext be the external nodes of π. For i = 1, . . . , l, create a factor e where att(e) = Bπ vX,i exti and F (e) = CondEquals.
The resulting graph has |N | + |P | binary variables, nG variables in the clusters Cπ, and X∈N |type(X)| ≤ nG variables in the clusters CX , so the total number of variables is in O(nG).
It has mG CondFactore factors, nG + X∈N |type(X)| ≤ 2nG CondNormalizev factors, 2|N | CondOnel factors, and 2 (X→R)∈P |extR| ≤ 2nG CondEquals factors, so the total number of factors is in O(nG + mG). Appendix D contains more information on this construction, including an example, a detailed proof that the sum–product is preserved, and a discussion of inference on the resulting graph.
5.3 Inﬁnite variable domains, inﬁnite graph languages
Finally, if we allow both (countably) inﬁnite domains and inﬁnite graph languages, then computing the sum–product is undecidable. This has already been observed even for single factor graphs with inﬁnite variable domains (Dreyer and Eisner, 2009), but we show further that this can be done using a minimal inventory of factors. Theorem 18. Let G be a FGG whose variable domains are N and whose factors only use the successor relation and equality with zero. It is undecidable whether the sum–product of G is zero.
Proof. By reduction from the halting problem for Turing machines. See Appendix E.
6 Conclusion
Factor graph grammars are a powerful way of deﬁning probabilistic models that permits practical inference. We plan to implement the algorithms described in this paper as differentiable operations and release them as open-source software. We will also explore techniques for optimizing inference in FGGs, for example, by automatically modifying rules to reduce their treewidth (Bilmes, 2010) or reducing the cost of matrix inversions in Theorem 15 (Nederhof and Satta, 2008). Another important direction for future work is the development of approximate inference algorithms for FGGs.
Broader Impact
This research is of potential beneﬁt to anyone working with structured probability models, including latent-variable neural networks. As this research is purely theoretical, we are not aware of any direct negative impacts.
9

Acknowledgments and Disclosure of Funding
We would like to thank the anonymous reviewers, especially Reviewer 3, for making numerous suggestions for improvement. We also thank Antonis Anastasopoulos, Justin DeBenedetto, Wes Filardo, Chung-Chieh Shan, and Xing Jie Zhong for their feedback.
This material is based upon work supported by the National Science Foundation under Grant No. 2019291. Any opinions, ﬁndings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reﬂect the views of the National Science Foundation.
References
Michel Bauderon and Bruno Courcelle. 1987. Graph expressions and graph rewriting. Mathematical Systems Theory, 20:83–127.
Jeff Bilmes. 2010. Dynamic graphical models. IEEE Signal Processing Magazine, 27(6):29–42.
Jeff Bilmes and Chris Bartels. 2003. On triangulating dynamic graphical models. In Proc. UAI, pages 47–56.
Hans L. Bodlaender. 1993. A linear time algorithm for ﬁnding tree-decompositions of small treewidth. In Proc. STOC, pages 226–234.
Hans L. Bodlaender. 1998. A partial k-arboretum of graphs with bounded treewidth. Theoretical Computer Science, 209:1–54.
Wray L. Buntine. 1994. Operations for learning with graphical models. J. Artiﬁcial Intelligence Research, 2:159–225.
David Chiang, Jacob Andreas, Daniel Bauer, Karl Moritz Hermann, Bevan Jones, and Kevin Knight. 2013. Parsing graphs with hyperedge replacement grammars. In Proc. ACL, volume 1, pages 924–932.
Shay B. Cohen, Robert J. Simmons, and Noah A. Smith. 2011. Products of weighted logic programs. Theory and Practice of Logic Programming, 11(2–3):263–296.
H. Comon, M. Dauchet, R. Gilleron, C. Lo¨ding, F. Jacquemard, D. Lugiez, S. Tison, and M. Tommasi. 2007. Tree automata techniques and applications. Release October, 12th 2007.
Frank Drewes, Hans-Jo¨rg Kreowski, and Annegret Habel. 1997. Hyperedge replacement graph grammars. In Grzegorz Rozenberg, editor, Handbook of Graph Grammars and Computing by Graph Transformation, pages 95–162. World Scientiﬁc.
Markus Dreyer and Jason Eisner. 2009. Graphical models over multiple strings. In Proc. EMNLP, pages 101–110.
Jason Eisner. 2002. Parameter estimation for probabilistic ﬁnite-state transducers. In Proc. ACL, pages 1–8.
Lise Getoor, Nir Friedman, Daphne Koller, Avi Pfeffer, and Ben Taskar. 2007. Probabilistic relational models. In Lise Getoor and Ben Taskar, editors, Introduction to Statistical Relational Learning, pages 129–174. MIT Press.
Daniel Gildea. 2011. Grammar factorization by tree decomposition. Computational Linguistics, 37(1):231–248.
Joshua Goodman. 1999. Semiring parsing. Computational Linguistics, 25(4):573–606.
Annegret Habel and Hans-Jo¨rg Kreowski. 1987. May we introduce to you: Hyperedge replacement. In Proc. Third International Workshop on Graph Grammars and Their Application to Computer Science, volume 291 of Lecture Notes in Computer Science, pages 15–26. Springer.
Bevan Jones, Jacob Andreas, Daniel Bauer, Karl Moritz Hermann, and Kevin Knight. 2012. Semanticsbased machine translation with hyperedge replacement grammars. In Proc. COLING, pages 1359–1376.
10

Daphne Koller and Nir Friedman. 2009. Probabilistic Graphical Models: Principles and Techniques. MIT Press.
Frank R. Kschischang, Brendan J. Frey, and Hans-Andrea Loeliger. 2001. Factor graphs and the sum-product algorithm. IEEE Trans. Information Theory, 47(2):498–519.
Rune B. Lyngsø and Christian N. S. Pedersen. 2002. The consensus string problem and the complexity of comparing hidden Markov models. J. Computer and System Sciences, 65:545–569.
David McAllester, Michael Collins, and Fernando Pereira. 2008. Case-factor diagrams for structured probabilistic modeling. J. Computer and System Sciences, 74(1):84–96.
Mazen Melibari, Pascal Poupart, Prashant Doshi, and George Trimponias. 2016. Dynamic sum product networks for tractable inference on sequence data. In Proc. International Conference on Probabilistic Graphical Models, pages 345–355.
Tom Minka and John Winn. 2008. Gates. In Proc. NeurIPS, pages 1073–1080. Mark-Jan Nederhof and Giorgio Satta. 2008. Computing partition functions of PCFGs. Research on
Language and Computation, 6:139–162. Fritz Obermeyer, Eli Bingham, Martin Jankowiak, Justin Chiu, Neeraj Pradhan, Alexander Rush, and
Noah Goodman. 2019. Tensor variable elimination for plated factor graphs. In Proc. ICML, pages 4871–4880. Hoifung Poon and Pedro Domingos. 2011. Sum-product networks: A new deep architecture. In Proc. UAI, pages 337–346. David V. Pynadath and Michael P. Wellman. 1998. Generalized queries on probabilistic context-free grammars. Trans. Pattern Analysis and Machine Intelligence, 20(1):65–77. John Schulman, Nicolas Heess, Theophane Weber, and Pieter Abbeel. 2015. Gradient estimation using stochastic computation graphs. In Proc. NeurIPS. David A. Smith and Jason Eisner. 2008. Dependency parsing by belief propagation. In Proc. EMNLP, pages 145–156. Andreas Stolcke. 1995. An efﬁcient probabilistic context-free parsing algorithm that computes preﬁx probabilities. Computational Linguistics, 21(2):165–201. Andreas Stuhlmu¨ller and Noah D. Goodman. 2012. A dynamic programming algorithm for inference in recursive probabilistic programs. In Proc. International Workshop on Statistical Relational AI (StarAI). Jan-Willem van de Meent, Brooks Paige, Hongseok Yang, and Frank Wood. 2018. An introduction to probabilistic programming. ArXiv:1809.10756.
11

A Simulating PCFGs
Example 19. Below is a FGG for derivations of a PCFG in Chomsky normal form. The start symbol of the FGG is S and the start symbol of the PCFG is S. Random variables N range over nonterminal symbols of the PCFG, and random variables W range over terminal symbols.

N1 = S

N1

N1

N1

N1

S −→ N1

X −→

p(N1 → N2N3)

X −→ p(N1 → W2)

X2

N2

N3

W2

X4

X5

Example 20. We can conjoin the FGG of Example 19 with the following FGG to constrain it to an input string w, with n = |w|, 0 ≤ i < j < k ≤ n, and 1 ≤ l ≤ n:

N1

N1

N1

N1

(0, n) −→ N1

(i, j) −→ N2

N3

(l − 1, l) −→ W2

(0, n)2

(i, k)4 (k, j)5

W2 = wl

The resulting rules have a total of O(n3) variables in their right-hand sides. The largest right-hand
side has 3 variables, so k = 2. The variables range over nonterminals, so m = |N | where N is
the CFG’s nonterminal alphabet. Therefore, running the algorithm of Theorem 15 on this FGG takes O(nGmk+1) = O(|N |3n3) time, which is the same as the CKY algorithm. This construction generalizes easily to CFGs not in Chomsky normal form; applying Lemma 16 would keep the inference complexity down to O(n3) (or O(n2) for a linear CFG).

B Relationship to other formalisms
B.1 Plate diagrams
Plate diagrams are extensions of graphs that describe repeated structure in Bayesian networks (Buntine, 1994) or factor graphs (Obermeyer et al., 2019). A plate is a subset of variables/factors, together with a count M , indicating that the variables/factors inside the plate are to be replicated M times. But there cannot be edges between different instances of a plate. Deﬁnition 21. A plated factor graph or PFG (Obermeyer et al., 2019) is a factor graph H = (V, E) together with a ﬁnite set B of plates and a function P : V ∪ E → 2B that assigns each variable and factor to a set of plates. If b ∈ P (v) and e is incident to v, then b ∈ P (e).
The unrolling of H by M : B → N is the factor graph that results from making M (b) copies of every node v such that b ∈ P (v) and every edge e such that e ∈ P (e).
Obermeyer et al. (2019) give an algorithm for computing the sum–product of a PFG. It only succeeds on some PFGs. An example for which it fails is the set of all restricted Boltzmann machines (fullyconnected bipartite graphs); one of their main results is to characterize the PFGs for which their algorithm succeeds. Below, we show how to convert these PFGs to FGGs. Proposition 22. Let H be a PFG. If the sum–product algorithm of Obermeyer et al. (2019) succeeds on H, then there is a FGG G such that for any M : B → N, there is a FGG GM such that G GM generates one graph, namely the unrolling of H by M .

Proof. We just describe how to construct G GM directly; hopefully, it should be clear how to construct G and GM separately (G has factors but not counts; GM has counts but not factors). Algorithm 1 converts H and M to G GM . It has the same structure as the sum–product algorithm of Obermeyer et al. (2019) and therefore works on the same class of PFGs.

12

Algorithm 1 Procedure for converting a PFG H and count assignment M to a FGG.
while E = ∅ do let e = arg maxe∈E |P (e)| (breaking ties arbitrarily) and L = P (e) let HL be the subgraph of nodes and edges of H whose plate set is L for each connected component Hc of HL do let Vf be the variables not in Hc but incident to factors in Hc let L = ∪v∈Vf P (v) if L = L then error let X be a fresh nonterminal let n = b∈L\L M (b) replace Hc with an edge with label Xn and endpoints Vf for i ← n, . . . , 1 do create rule Xi → R where R has: • internal nodes and edges from Hc • external nodes Vf • an edge with label Xi−1 and endpoints Vf
create rule X0 → R where R has external nodes Vf and no other nodes/edges
create rule S → H

If the algorithm of Obermeyer et al. (2019) fails on a PFG, there might not be an equivalent FGG. In particular, FGGs cannot generate the set of RBMs, because a m × n RBM has treewidth min(m, n), so the set of all RBMs has unbounded treewidth and can’t be generated by a HRG.
Although, in this respect, FGGs are less powerful than PFGs, we view this as a strength, not a weakness. Because FGGs inherently generate graphs of bounded treewidth, our sum–product algorithm (Theorem 15) works on all FGGs, and no additional constraints are needed to guarantee efﬁcient inference.
Example 23. The following PFG is from Obermeyer et al. (2019):

F

H

G

X

Y

J

I

Converting to a FGG produces the following rules (in order of their construction by the above algorithm):

H

X1 Aj Y2 −→ X1

Y2

Aj3−1

1≤j≤J

X1 A0 Y2 −→ X1

Y2

G X1 Bi −→ X1 AJ3 Y2
B4i−1

1≤i≤I

X1 B0 −→ X1

F S −→

X1 BI2

13

B.2 Dynamic graphical models For simplicity, we only consider binary factors, which we draw as directed edges, and we ignore edge labels. Deﬁnition 24. A dynamic graphical model or DGM (Bilmes, 2010) is a tuple (H1, H2, H3, E12, E22, E23), where the Hi = (Vi, Ei) are factor graphs and the Eij ⊆ Vi × Vj are sets of edges from Hi to Hj. A DGM speciﬁes how to construct, for any length n ≥ 2, a factor graph
Hn = (V1 ∪ V2 × {1, . . . , n} ∪ V3, E), where E is deﬁned by:
• If (u, v) ∈ E12, add an edge from u to (v, 1). • If (u, v) ∈ E22, add an edge from (u, i − 1) to (v, i) for all 1 < i ≤ n. • If (u, v) ∈ E23, add an edge from (u, n) to v. Proposition 25. Given a DGM D = (H1, H2, H3, E12, E22, E23), there is a FGG G such that for any count n ≥ 2, there is another FGG Gn such that G Gn generates exactly one graph, the unrolling of D by n.
Proof. Again, we give an algorithm for constructing G Gn, and hopefully, it should be clear how to construct G and Gn separately. Create the following rules:
• S → R, where R contains • Nodes and edges from H1, H2, and E12 • An edge labeled An−1 and endpoints {u | (u, v) ∈ E22}.
• Ai → R, where R contains • Nodes and edges from H2 • If (u, v) ∈ E22, R has an external node u • For each (u, v) ∈ E22, an edge from u to v • An edge labeled Ai−1 and endpoints {u | (u, v) ∈ E22}.
• A1 → R, where R contains • Nodes and edges from H2, H3, and E23 • If (u, v) ∈ E22, R has an external node u • For each (u, v) ∈ E22, an edge from u to v.
Example 26. Bilmes (2010) give the following example of a DGM. All factors have two endpoints, and we draw them as directed edges instead of the usual squares. We draw the edges in E22 with dotted lines.
H1 H2 H3
The resulting FGG:
14

S −→

An−1

Ai −→

Ai−1

A1 −→

where, in the middle rule, 1 < i < n.
Running the algorithm of Theorem 15 would not be guaranteed to achieve the same time complexity as that of (Bilmes and Bartels, 2003), which searches through alternative ways of dividing the unrolled factor graph into time slices.

B.3 Case–factor diagrams and sum–product networks
Case–factor diagrams (McAllester et al., 2008) and sum–product networks (Poon and Domingos, 2011) are compact representations of probability distributions over assignments to Boolean variables. They generalize both Markov networks and PCFGs. Both formalisms represent models as rooted directed acyclic graphs (DAGs), with edges directed away from the root, in which some nodes mention variables. If D is a DAG, for any node v ∈ D, let scope(v) be the set of variables mentioned in v or any descendant of v.
Deﬁnition 27. A case–factor diagram (CFD) model is a pair (D, Ψ), where D is a rooted DAG with root r, each of whose nodes is one of the following:
• case(x) with two children v1 and v2, where x is a variable not in scope(v1) ∪ scope(v2). • factor with two children v1 and v2, where scope(v1) ∩ scope(v2) = ∅. • unit with no children. • empty with no children. And Ψ : scope(r) → R≥0 assigns a cost to each variable in scope(r). A CFD model deﬁnes a probability distribution over assignments to its variables. We compute quantities q(v, ξ) and Z(v) for each node v as follows. Let v1, v2 be the children of v, if any.

v = case(x)
v = factor v = unit v = empty

q(v, ξ) = e−Ψ(x) q(v1, ξ) q(v2, ξ)
q(v, ξ) = q(v1, ξ) q(v2, ξ) q(v, ξ) = 1 q(v, ξ) = 0

if ξ(x) = 1 if ξ(x) = 0

Z(v) = e−Ψ(x) Z(v1) + Z(v2)
Z(v) = Z(v1) Z(v2) Z(v) = 1 Z(v) = 0

Deﬁne q(ξ) = q(r, ξ) and Z = Z(r). Then P (ξ) = q(ξ)/Z.
Proposition 28. If (D, Ψ) is a CFD model, there is a FGG G such that (D, Ψ) and G have the same sum–product, and for any assignment ξ of (D, Ψ), there is a FGG Gξ such that the sum–product of G ∧ Gξ equals q(ξ).
15

Proof. Given a CFD, we can construct a FGG where each node v of the CFD becomes a different nonterminal symbol Dv:

node

G

Gξ

v = case(x)

Dv −→

x

Dv1

e−Ψ(x)

x=1

Dv −→

x Dv1

x = ξ(x)

Dv −→ x

Dv2

Dv −→

x Dv2

x=0

x = ξ(x)

v = factor v = unit

D −→ Dv1 Dv2 unit −→ ∅

D −→ Dv1 Dv2 unit −→ ∅

We do not create any rule with left-hand side empty, so that any derivations that generate empty fail.

The number of rules in G is the number of nodes in D. Computing its sum–product is linear in the number of rules, just as computing the sum–product of D is linear in the number of nodes.
Deﬁnition 29. A valid sum–product network (SPN) is a rooted DAG whose nodes are each either:
• sum(λ1, λ2) with two children v1 and v2, where scope(v1) = scope(v2). • product with two children v1 and v2 such that no variable appears in one and negated in the
other.
• x or x¯ with no children.
A valid SPN deﬁnes a distribution over assignments to its variables. For each node v, let v1, v2 be the children of v, if any.

v = sum(λ1, λ2) v = product v=x v = x¯

q(v, ξ) = λ1q(v1, ξ) + λ2q(v2, ξ) q(v, ξ) = q(v1, ξ) q(v2, ξ) q(v, ξ) = ξ(x) q(v, ξ) = 1 − ξ(x)

Converting a valid SPN to a FGG is straightforward, but the resulting FGG has a separate node for each occurrence of a variable x. The syntactic constraints in the deﬁnition of valid SPN ensure that in any graph with nonzero weight, all occurrences of x have the same value.
Proposition 30. Any valid SPN S can be converted into a FGG G such that S and G have the same sum–product, and for any assignment ξ of S, there is a FGG Gξ such that the sum–product of G ∧ Gξ equals q(ξ).
16

Proof. We construct a FGG where each node v becomes a different nonterminal symbol Dv:

node

G

Gξ

v=x

Dv −→ x x = 1

Dv −→ x x = ξ(x)

v = x¯

Dv −→ x x = 0

Dv −→ x x = ξ(x)

v = sum(λ1, λ2) v = product

Dv −→

Dv1

λ1

Dv −→

Dv2

λ2

Dv −→ Dv1 Dv2

Dv −→ Dv1 Dv −→ Dv2
Dv −→ Dv1 Dv2

The number of rules in G is the number of nodes in S. Computing its sum–product is linear in the number of rules, just as computing the sum–product of S is linear in the number of nodes.
Further variations of SPNs have been proposed, in particular to generate repeated substructures (Stuhlmu¨ller and Goodman, 2012; Melibari et al., 2016). Factored SPNs (Stuhlmu¨ller and Goodman, 2012) are especially closely related to FGGs, in that they allow one part of a SPN to “reference” another, which is analogous to a nonterminal-labeled edge in a FGG.
CFDs and SPNs present a rather different, lower-level view of a model than the other formalisms surveyed here do. Whereas factor graphs and the other formalisms represent the model’s variables and the dependencies among them, CFDs and SPNs (including factored SPNs) represent the computation of the sum-product. For instance, converting a factor graph H to a CFD or SPN requires forming a tree decomposition of H (McAllester et al., 2008), and the resulting CFD/SPN’s structure is that of the tree decomposition, not of H.
FGGs, in a sense, combine both points of view. Their derived graphs represent a model’s variables and dependencies, while their derivation trees represent the computation of the sum-product. Thus, a factor graph H can be trivially converted into a FGG S → H, and, as can be seen in the translations given above, a CFD or SPN can also be converted to a FGG while preserving its structure.
C Proof of Proposition 16
Let H = (V, E) be a hypergraph. Recall that a tree decomposition of H is a tree whose nodes are called bags, to each of which is associated a set of nodes, VB ⊆ V , and (nonstandardly) a set of edges, EB ⊆ E. The bags must satisfy the properties:
• Node cover: B VB = V . • Edge cover: for every edge e ∈ E, there is exactly one bag B such that e ∈ EB and att(e) ⊆ VB.
• Running intersection: if v ∈ VB1 and v ∈ VB2 , then for every bag B between B1 and B2, v ∈ VB.
The width of a tree decomposition is maxB |VB| − 1, and the treewidth of H is the minimum width of any tree decomposition of H. A tree decomposition can always be made to have at most n nodes without changing its width (Bodlaender, 1993).
Chiang et al. (2013) give a parsing algorithm for HRGs that matches right-hand sides incrementally using their tree decompositions. They observe that this is related to the concept of binarization of context-free grammars. Here, we make this connection explicit by showing how to factorize a HRG.
17

For every rule (X → R), where R¯ has nR nodes and treewidth at most k, form a tree decomposition of R¯ with nR − k ≤ nR bags. Let the root of the tree decomposition be the bag containing all the external nodes of R. For each bag B, construct a rule XB → RB as follows.

• If B is the root bag, XB = X; otherwise, XB is a fresh nonterminal symbol.

• Add all nodes in VB and edges in EB to RB.

• If B is the root bag, RB’s external nodes are the same as R’s; if B has parent P , let RB’s external nodes be VP ∩ VB.

• For each child bag Bi, add a hyperedge with label XBi and endpoints VB ∩ VBi .

This new FGG generates the same language as G. The number of rules is at most nG. Every right-hand side has at most (k + 1) nodes.

(X→R)∈G nR =

D Supplement to Theorem 17

D.1 An example
Example 31. We show how to construct the factor graph corresponding to the following simple, nonreentrant FGG:

A1

B2

S −π→1

X3

A4

S −π→2 A1

Y3

B2

A1

B2

A1

X −π→3 f (A1, A4)

B2 Y3

A4

A4

g(A1, B2)

A1

Y

B2 −π→4 A1

B2

This grammar generates just two graphs:

g(A1, B2)

A1

B2

A1 f (A1, A4)

B2 g(A4, B2)

A4

Applying the construction from Theorem 17 gives the factor graph shown in Figure 4.

18

A1

B2

A4
π1

A1

B2

A4
X

A1

B2

f

A4

π3

A

B

Y

g

A

B

π4

BS = true
BS S
CondOne(BS, Bπ1 , Bπ2 )

Bπ1

Bπ2

A1

B2

π2 CondOne(BX, Bπ1 )

BX

CondOne(BX, Bπ3 )

Bπ3

CondOne(BY, Bπ2 , Bπ3 )

BY CondOne(BY, Bπ4 )
Bπ4

Figure 4: The FGG of Example 31, converted to a single factor graph using the construction of Theorem 17. Some detail has been omitted to reduce clutter. The edges from clusters to B variables are “metatentacles” that stand for a tentacle from every factor inside the cluster to the B variable. We draw CondNormalize and CondEquals factors as smaller squares and omit their names. Lastly, rather than writing out “CondFactor”, we use the name of the original factor function (f or g).

19

D.2 Complexity of inference
As noted in Section 5.2, the purpose of this conversion to a single factor graph is to make inference possible with inﬁnite variable domains; after converting to a factor graph, existing, possibly approximate, inference methods can be applied. But with ﬁnite variable domains, an algorithm like variable elimination would not be appropriate because this conversion has the potential to increase treewidth dramatically.
In the proof of Theorem 15, we constructed the nonterminal graph, which has a node for every nonterminal and an edge from X to Y iff there is a rule X → R where R has an edge labeled Y . For a nonreentrant FGG, the nonterminal graph is always a DAG. If, for each X ∈ N \ S, X appears in the right-hand side of exactly one rule, then the nonterminal graph is a tree.
When the nonterminal graph is a tree, we can construct a tree decomposition by making one bag for each cluster, and one bag for each CondOne factor. The bag for a cluster C contains all the variables in C, along with BC and all the CondNormalize and CondFactor edges associated with C. The bag for a CondOne factor will contain all the B variables used by that CondOne factor, all the CondEquals edges connecting clusters involved in that CondOne factor, and all the variables connected to those CondEquals edges.
Exact inference on this tree decomposition is very similar to the algorithm described in Theorem 15. However, a na¨ıve application of variable elimination will still be less efﬁcient than that algorithm, since the CondOne factors connect |P X | + 1 binary variables, requiring a loop over 2|P X|+1 assignments. All but |P X | + 1 of these assignments have zero weight, so in fact we can process these factors much faster; modifying the variable elimination algorithm to account for this and the CondEquals constraints would give us something almost identical to the algorithm of Theorem 15.
In the DAG case, this simple tree decomposition is not possible. The factor graph H has the nonterminal graph as a minor, so the treewidth of the nonterminal graph is a lower bound on the treewidth of H (Bodlaender, 1998, Lemma 16). In the worst case, this could be |N |.

D.3 Detailed proof of correctness

If G is a FGG and H is the factor graph that results from the construction of Theorem 17, we can show that they have the same sum–product ZG = ZH .

The sum–product ZH can be computed in the usual way, by summing over all assignments to the variables and, for each assignment, taking the product over all of the factors:

ZH =

F (e)(ξ(e)).

ξ∈ΞH e∈H

The summation over assignments ξ includes many possible settings of the B variables. But the CondOne factors tell us that, if the assignment to the B variables does not give us a valid derivation, then the weight of that assignment will be 0. Therefore, we only need to sum over assignments to the B variables which represent a valid derivation, and so we can express the sum–product using a sum over derivations rather than a sum over assignments to B variables. Let ξB represent the assignment to the B variables. Then:

ZH =

F (e)(ξ(e)).

D∈D(G)

ξ∈ΞH

e∈H

ξB consistent with D

(Note that the product over e ∈ H can ignore all CondOne factors, since when the assignment to the B variables is consistent with some derivation, they all have value 1.)

We can associate a derivation D with the subset of clusters in H corresponding to the nonterminals
and rules which were used in the derivation; call this CD. For any D, all the variables in H are divided into three parts: those that belong to clusters in CD (call this VD), those that belong to clusters not in CD (call this VD), and the B variables (which don’t belong to any cluster). Let ξB,D be the unique assignment to the B variables that is consistent with D. Let ΞD be the set of all assignments extending ξB,D with assignments to VD, and let ΞD be the set of all assignments extending ξB,D with assignments to VD.

20

Let ED be the set of factors involving a variable in VD, and let ED be the set of factors involving a variable in VD. Because any factors between VD and VD are CondEquals factors with value 1 (since their B variable is false), we can ignore them. Similarly, the only factors which don’t involve either
VD or VD are the CondOne factors, which we are already ignoring. This allows us to rewrite the sum–product as

ZH =

F (e)(ξ(e))

D∈D ξ∈ΞD e∈ED

ZD

F (e)(ξ(e)) .
ξ∈ΞD e∈ED ZD

Consider ZD ﬁrst. All CondFactor and CondEquals factors in ED have value 1 and can be ignored, leaving only CondNormalize factors. Because these place a probability distribution pv on each variable v in an unused cluster, those variables all sum out:

ZD =

CondNormalizev(BX , v)

CondNormalizev(Bπ, v)

ξ∈ΞD CX ∈CD v∈CX

Cπ ∈CD v∈Cπ

=

pv (ξ (v))

pv (ξ (v))

ξ∈ΞD CX ∈CD v∈CX

Cπ ∈CD v∈Cπ









=



pv (x)



pv (x)

CX ∈CD v∈CX x∈Ω(v)

Cπ ∈CD v∈Cπ x∈Ω(v)

= 1.

Now consider ZD. All CondNormalize factors in ED have value 1 and can be ignored, leaving only CondEquals and CondFactor factors. Let HD be the derived graph of D. We can think of the derivation as merging pairs of nodes in VD, so that a single node v ∈ HD may correspond to several “copies” in VD. However, the CondEquals constraints ensure that all copies of v have the same value. Therefore, instead of summing over the assignments to VD, we can simply sum over the assignments to HD (and omit CondEquals factors):

ZD =

CondFactore(Bπ, ξ(att(e)))

ξ∈ΞHD Cπ ∈CD e∈π

=

F (e)(ξ(att(e)))

ξ∈ΞHD Cπ ∈CD e∈π

=

F (e)(ξ(att(e))).

ξ∈ΞHD e∈HD

So, ﬁnally, the sum–product of H can be rewritten as:

ZH =

F (e)(ξ(att(e)))

D∈D(G) ξ∈ΞHD e∈HD

=

wG(D, ξ)

D∈D(G) ξ∈ΞHD

= ZG.

E Proof of Theorem 18
Let Γ be a ﬁnite alphabet containing a blank symbol ( ), and let k = |Γ|. Number the symbols in Γ as γ0 = , γ1, γ2, . . . , γk−1. Deﬁne an encoding for strings over Γ:
=0 γiw = i + k · w . Note that strings that differ only in the number of trailing blanks have the same encoding.

21

We write x // k for x/k and x % k = x − x // k · k.
Let M be a Turing machine with doubly-inﬁnite tape, input alphabet Σ, tape alphabet Γ, start state q0, transition function δ, accept state qaccept, and reject state qreject. For any input string w ∈ Σ∗, construct the following rules, where the q nodes track the Turing machine’s state, the u nodes track the reverse of the tape to the left of the head, and the v nodes track the tape from the head rightward:

u1 = 0

q2 = q0

v3 = w

S −→ u1

q2

v3

T

u1 q2 v3

u1

q2

v3

T

−→

q2 ∈ {qaccept, qreject}

For each transition δ(q, a) = (r, b, L):

u1 q2 T

v3

u1

q2

−→ u5 = u1 // k

u4

q5

q2 = q v3 v3 % k = a v6 = u1 % k + b · k + v3 // k · k2
q5 = r v6

T

For each transition δ(q, a) = (r, b, R):

u1 q2 v3

u1

q2 q2 = q v3 v3 % k = a

T

−→ u4 = b + u1 · k

v6 = v3 // k

u4

q5 q5 = r v6

T

The sum-product of this FGG is 1 if M halts on w, 0 otherwise. Therefore, computing the sum-product of an FGG is undecidable. The operations +, ·, //, % and = can be further reduced to just the successor relation and equality with zero, as shown below.
22

x3 = x1 + 1

x3 = x2 + 1

x1

=

x2 −→ x1

x3

x2

x2 x1 = x3 + 1

x1

>

x2 −→ x1

x3

+

x4

x1

+

x3 −→

x2

x2 = 0

x1 = x3
x2

x1

x1

+

x3 −→

x3 = x5 + 1

+

x5

x3

x2

x2

x4

x2 = x4 + 1

x1

x1

·

x3 −→

x2

x2

x2 = 0

x3 = 0 x3

x1

x1

·

x3 −→

x2

x2

+

x3

·

x5

x4

x2 = x4 + 1

Integer division and remainder can both be computed using the rule:

x7

x1

x1

·

x5

D

→

+

x7

x2

x3

x2

>

x3

where x7 is the dividend, x2 is the divisor, x1 is the quotient, and x3 is the remainder.

23

