arXiv:1906.02922v4 [cs.LG] 18 May 2020

Non-Stationary Reinforcement Learning: The Blessing of (More) Optimism
Wang Chi Cheung
Department of Industrial Systems Engineering and Management, National University of Singapore isecwc@nus.edu.sg
David Simchi-Levi
Institute for Data, Systems, and Society, Massachusetts Institute of Technology, Cambridge, MA 02139 dslevi@mit.edu
Ruihao Zhu
Institute for Data, Systems, and Society, Massachusetts Institute of Technology, Cambridge, MA 02139 rzhu@mit.edu
We consider un-discounted reinforcement learning (RL) in Markov decision processes (MDPs) under temporal drifts, i.e., both the reward and state transition distributions are allowed to evolve over time, as long as their respective total variations, quantiﬁed by suitable metrics, do not exceed certain variation budgets. This setting captures endogeneity, exogeneity, uncertainty, and partial feedback in sequential decision-making scenarios, and ﬁnds applications in various online marketplaces, epidemic control, and transportation. We ﬁrst develop the Sliding Window Upper-Conﬁdence bound for Reinforcement Learning with Conﬁdence Widening (SWUCRL2-CW) algorithm, and establish its dynamic regret bound when the variation budgets are known. In addition, we propose the Bandit-over-Reinforcement Learning (BORL) algorithm to adaptively tune the SWUCRL2-CW algorithm to achieve the same dynamic regret bound, but in a parameter-free manner, i.e., without knowing the variation budgets. Finally, we conduct numerical experiments to show that our proposed algorithms achieve superior empirical performance compared with existing algorithms.
Notably, the interplay between endogeneity and exogeneity presents a unique challenge, absent in existing (stationary and non-stationary) stochastic online learning settings, when one applies the conventional Optimism in Face of Uncertainty (OFU) principle to design algorithms with provably low dynamic regret for RL in non-stationary MDPs. We overcome this challenge by a novel conﬁdence widening technique that incorporates additional optimism into our learning algorithms to ensure low dynamic regret bounds. To extend our theoretical ﬁndings, we demonstrate, in the context of single item inventory control with ﬁxed cost, how one can leverage special structures on the state transition distributions to bypass the diﬃculty of exploring time-varying environments.
Key words : reinforcement learning, data-driven decision making, revenue management, conﬁdence widening
1. Introduction
Consider a general sequential decision-making framework, where a decision-maker (DM) interacts with an initially unknown environment iteratively. At each time step, the DM ﬁrst observes the current state of the environment, and then chooses an available action. After that, she receives an
1

2
instantaneous random reward, and the environment transitions to the next state. The DM aims to design a policy that maximizes its cumulative rewards, while facing the following challenges: • Endogeneity: At each time step, the reward follows a reward distribution, and the subsequent
state follows a state transition distribution. Both distributions depend (solely) on the current state and action, which are inﬂuenced by the policy. Hence, the environment can be fully characterized by a discrete time Markov decision process (MDP). • Exogeneity: The reward and state transition distributions vary (independently of the policy) across time steps, but the total variations are bounded by the respective variation budgets. • Uncertainty: Both the reward and state transition distributions are initially unknown to the DM. • Bandit/Partial Feedback: The DM can only observe the reward and state transition resulted by the current state and action in each time step.
1.1. Motivating Examples It turns out that many applications, such as vehicle remarketing in used-car sales and real-time bidding in advertisement (ad) auctions, can be captured by this framework.
Example 1 (Vehicle Remarketing in Used-Cars Sales). An automobile company disposes of continually arriving oﬀ-lease vehicles (i.e., leasing vehicles that have reached the end of their ﬁxed term) via daily wholesale vehicle auctions (Manheim 2020, Vehicle Remarketing 2020). At the beginning of each auction, the company observes the number of on-hand vehicles (the “state”), and decides the number of oﬀ-lease vehicles to be listed (the “action”). Then, the car dealers bid for the purchases via a ﬁrst-price auction. The sales of vehicles generate revenue to the company while unsold vehicles incur holding cost to the company (the “reward” and “state transition”). The company aims at maximizing proﬁt by designing a policy that dynamically decides the vehicles to be listed in each auction. However, the dealers’ bidding behaviors are aﬀected by many unpredictable (and thus exogenous) factors (e.g., real-time customer demands, vehicles’ depreciation, and inter-dealer competitions) in addition to the company’s decisions (i.e., the vehicles listed), and can vary across time.
Example 2 (Real-Time Bidding in Ads Auctions). Advertisers repeatedly competes for ad display impressions via real-time online auctions (Google 2011, Cai et al. 2017, Flajolet and Jaillet 2017, Balseiro and Gur 2019, Guo et al. 2019). Each advertiser begins with a budget. Upon the arrival of a user, an impression is generated, and the advertisers submit bids (the “action”) for it subject to her remaining budget (the “state”). The winning advertiser acquires the impression to display her ad to the user, and observes the user click or no-click behavior (the “reward”). For each slot won, the advertiser has to make the payment (determined by the auction mechanism) using her

3
remaining budget, and the budget is periodically reﬁlled (the “state transition”). Each advertiser wants to maximize the number of clicks on her advertisement subject to her own (continuously evolving) budget constraint. Nevertheless, the competitiveness of each auction exhibits exogeneity as the participating advertisers and the arriving users are diﬀerent from time to time. Moreover, the popularity of an ad can change due to endogenous reasons. For instance, displaying the same ad too frequently in a short period of time might reduce its freshness, and results in a tentatively low number of clicks (i.e., we can incorporate both the remaining budget and the number of times that the ad is shown within a given window size into the state of the MDP to model endogenous dynamics). Besides, this framework can be used to model sequential decision-making problems in transportation (Zhang and Wang 2018, Qin et al. 2019), wireless network (Zhou and Bambos 2015, Zhou et al. 2016), consumer choice modeling (Xu and Yun 2020), healthcare operations (Shortreed et al. 2010), epidemic control (Nowzari et al. 2016, Kiss et al. 2017), and inventory control (Huh and Rusmevichientong 2009, Bertsekas 2017, Zhang et al. 2018, Agrawal and Jia 2019, Chen et al. 2019a).
1.2. Diﬀerent Scenarios of Sequential Decision-Making There exists numerous works in sequential decision-making that considered part of the four challenges (Please refer to Table 1 for a summary and comparison). The traditional stream of research (Auer et al. 2002b, Bubeck and Cesa-Bianchi 2012, Lattimore and Szepesv´ari 2018) on stochastic multi-armed bandits (MAB) focused on the interplay between uncertainty and bandit feedback (i.e., challenges 3 and 4), and (Auer et al. 2002b) proposed the classical Upper Conﬁdence Bound (UCB) algorithm. Starting from (Burnetas and Katehakis 1997, Tewari and Bartlett 2008, Jaksch et al. 2010), a volume of works (see Section 3) have been devoted to reinforcement learning (RL) in MDPs (Sutton and Barto 2018), which further involves endogeneity. RL in MDPs incorporate challenges 1,3,4, and stochastic MAB is a special case of MDPs when there is only one state. In the absence of exogeneity, the reward and state transition distributions are invariant across time, and these three challenges can be jointly solved by the Upper Conﬁdence bound for Reinforcement Learning (UCRL2) algorithm (Jaksch et al. 2010).
The UCB and UCRL2 algorithms leverage the optimism in face of uncertainty (OFU) principle to select actions iteratively based on the entire collections of historical data. However, both algorithms quickly deteriorate when exogeneity emerge since the environment can change over time, and the historical data becomes obsolete. To address the challenge of exogeneity, (Garivier and Moulines 2011) considered the piecewise-stationary MAB environment where the reward distributions remain unaltered over certain time periods and change at unknown time steps. Later on, there is a line of

4

Endogeneity Exogeneity Uncertainty Bandit feedback

Stationary MAB









RL in stationary MDPs









Non-stationary MAB









RL in non-stationary MDPs









Table 1 Summary of diﬀerent sequential decision-making settings. Among them, RL in non-stationary MDPs is

the only setting that addresses all four challenges.

research initiated by (Besbes et al. 2014) that studied the general non-stationary MAB environment (Besbes et al. 2014, Cheung et al. 2019b,a), in which the reward distributions can change arbitrarily over time, but the total changes (quantiﬁed by a suitable metric) is upper bounded by a variation budget (Besbes et al. 2014). The aim is to minimize the dynamic regret, the optimality gap compared to the cumulative rewards of the sequence of optimal actions. Both the (relatively restrictive) piecewise-stationary MAB and the general non-stationary MAB settings consider the challenges of exogeneity, uncertainty, and partial feedback (i.e., challenges 2, 3, 4), but endogeneity (challenge 1) are not present.
In this paper, to address all four above-mentioned challenges, we consider RL in non-stationary MDPs where bot the reward and state transition distributions can change over time, but the total changes (quantiﬁed by suitable metrics) are upper bounded by the respective variation budgets. We note that in (Jaksch et al. 2010), the authors also consider the intermediate RL in piecewisestationary MDPs. Nevertheless, we ﬁrst demonstrate in Section 4.1, and then rigorously show in Section 6 that simply adopting the techniques for non-stationary MAB (Besbes et al. 2014, Cheung et al. 2019b,a) or RL in piecewise-stationary MDPs (Jaksch et al. 2010) to RL in non-stationary MDPs may result in poor dynamic regret bounds.
1.3. Summary of Main Contributions Assuming that, during the T time steps, the total variations of the reward and state transition distributions are bounded (under suitable metrics) by the variation budgets Br (> 0) and Bp (> 0), respectively, we design and analyze novel algorithms for RL in non-stationary MDPs. Let Dmax, S, and A be respectively the maximum diameter (a complexity measure to be deﬁned in Section 2), number of states, and number of actions in the MDP. Our main contributions are: • We develop the Sliding Window UCRL2 with Conﬁdence Widening (SWUCRL2-CW) algorithm.
When the variation budgets are known, we prove it attains a O˜ Dmax(Br + Bp)1/4S2/3A1/2T 3/4 dynamic regret bound via a budget-aware analysis.

5
• We propose the Bandit-over-Reinforcement Learning (BORL) algorithm that tunes the SWUCRL2-CW algorithm adaptively, and retains the same O˜ Dmax(Br + Bp)1/4S2/3A1/2T 3/4 dynamic regret bound without knowing the variation budgets.
• We identify an unprecedented challenge for RL in non-stationary MDPs with conventional optimistic exploration techniques: existing algorithmic frameworks for non-stationary online learning (including non-stationary bandit and RL in piecewise-stationary MDPs) (Jaksch et al. 2010, Garivier and Moulines 2011, Cheung et al. 2019b) typically estimate unknown parameters by averaging historical data in a “forgetting” fashion, and construct the tightest possible conﬁdence regions/intervals accordingly. They then optimistically search for the most favorable model within the conﬁdence regions, and execute the corresponding optimal policy. However, we ﬁrst demonstrate in Section 4.1, and then rigorously show in Section 6 that in the context of RL in non-stationary MDPs, the diameters induced by the MDPs in the conﬁdence regions constructed in this manner can grow wildly, and may result in unfavorable dynamic regret bound. We overcome this with our novel proposal of extra optimism via the conﬁdence widening technique. A summary of the algorithmic frameworks for stationary and non-stationary online learning settings are provided in Table 2.

Stationary

Non-stationary

MAB OFU (Auer et al. 2002b) OFU + Forgetting (Besbes et al. 2014, Cheung et al. 2019a)

RL OFU (Jaksch et al. 2010)

Extra optimism + Forgetting (This paper)

Table 2 Summary of algorithmic frameworks of stationary and non-stationary online learning settings.

• As a complement to this ﬁnding, suppose for any pair of initial state and target state, there always exists an action such that the probability of transiting from the initial state to the target state by taking this action is lower bounded uniformly over the entire time horizon, the DM can attain low dynamic regret without widening the conﬁdence regions. We demonstrate that in the context of single item inventory control with ﬁxed cost (Yuan et al. 2019), a mild condition on the demand distribution is suﬃcient for this extra assumption to hold.
1.4. Paper Organization The rest of the paper is organized as follows: in Section 2, we describe the non-stationary MDP model of interest. In Section 3, we review related works in non-stationary online learning and reinforcement learning. In Section 4, we introduce the SWUCRL2-CW algorithm, and analyze its performance in terms of dynamic regret. In Section 5, we design the BORL algorithm that can attain

6

the same dynamic regret bound as the SWUCRL2-CW algorithm without knowing the total variations. In Section 6, we discuss the challenges in designing learning algorithms for reinforcement learning under drift, and manifest how the novel conﬁdence widening technique can mitigate this issue. In Section 7, we discuss the alternative approach without widening the conﬁdence regions in inventory control problems. In Section 8, we conduct numerical experiments to show the superior empirical performances of our algorithms. In Section 9, we conclude our paper.

2. Problem Formulation
In this section, we introduce the notations to be used throughout paper, and introduce the learning protocol for our problem of RL in non-stationary MDPs.

2.1. Notation Throughout the paper, all vectors are column vectors, unless speciﬁed otherwise. We deﬁne [n] to be the set {1, 2, . . . , n} for any positive integer n. We denote 1[·] as the indicator function. For p ∈ [1, ∞], we use x p to denote the p-norm of a vector x ∈ Rd. We denote x ∨ y and x ∧ y as the maximum and minimum between x, y ∈ R, respectively. We adopt the asymptotic notations O(·), Ω(·), and Θ(·) (Cormen et al. 2009). When logarithmic factors are omitted, we use O˜(·), Ω˜ (·), Θ˜ (·), respectively. With some abuse, these notations are used when we try to avoid the clutter of writing out constants explicitly.

2.2. Learning Protocol Model Primitives: An instance of non-stationary MDP is speciﬁed by the tuple (S, A, T, r, p). The set S is a ﬁnite set of states. The collection A = {As}s∈S contains a ﬁnite action set As for each state s ∈ S. We say that (s, a) is a state-action pair if s ∈ S, a ∈ As. We denote S = |S|, A = ( s∈S |As|)/S. We denote T as the total number of time steps, and denote r = {rt}Tt=1 as the sequence of mean rewards. For each t, we have rt = {rt(s, a)}s∈S,a∈As, and rt(s, a) ∈ [0, 1] for each state-action pair (s, a). In addition, we denote p = {pt}Tt=1 as the sequence of state transition distributions. For each t, we have pt = {pt(·|s, a)}s∈S,a∈As, where pt(·|s, a) is a probability distribution over S for each state-action pair (s, a). Exogeneity: The quantities rt’s and pt’s vary across diﬀerent t’s in general. Following (Besbes et al. 2014), we quantify the variations on rt’s and pt’s in terms of their respective variation budgets Br, Bp (> 0):

T −1

Br = Br,t, where Br,t = max |rt+1(s, a) − rt(s, a)| , s∈S ,a∈As t=1

T −1

Bp = Bp,t, where Bp,t = max pt+1(·|s, a) − pt(·|s, a) 1 .

(1)

s∈S ,a∈As

t=1

7

We emphasize although Br and Bp might be used as inputs by the DM, individual Br,t’s and Bp,t’s

are unknown to the DM throughout the current paper. We also refer to Remark 2 for the choice

of inﬁnity-norm and 1-norm in eqn. (1).

Endogeneity: The DM faces a non-stationary MDP instance (S, A, T, r, p). She knows S, A, T ,

but not r, p. The DM starts at an arbitrary state s1 ∈ S. At time t, three events happen. First,

the DM observes its current state st. Second, she takes an action at ∈ Ast. Third, given st, at,

she stochastically transits to another state st+1 which is distributed as pt(·|st, at), and receives a

stochastic reward Rt(st, at), which is 1-sub-Gaussian with mean rt(st, at). In the second event, the

choice of at is based on a non-anticipatory policy Π. That is, the choice only depends on the current

state st and the previous observations Ht−1 := {sq, aq, Rq(sq, aq)}qt−=11. Dynamic Regret: The DM aims to maximize the cumulative expected reward E[

T t=1

rt

(st

,

at

)],

despite the model uncertainty on r, p and the dynamics of the learning environment. To measure

the convergence to optimality, we consider an equivalent objective of minimizing the dynamic regret

(Besbes et al. 2014, Jaksch et al. 2010)

T

Dyn-RegT (Π) = {ρ∗t − E[rt(st, at)]} .

(2)

t=1

In the oracle

T t=1

ρ∗t

,

the

summand

ρ∗t

is

the

optimal

long-term

average

reward

of

the

stationary

MDP with state transition distribution pt and mean reward rt. The optimum ρ∗t can be computed

by solving linear program (15) provided in Section A.1. We note that the same oracle is used for

RL in piecewise-stationary MDPs (Jaksch et al. 2010).

Remark 1 (Comparisons with Non-Stationary MAB). When S = 1, eqn. (2) reduces to

the deﬁnition (Besbes et al. 2014) of dynamic regret for non-stationary K-armed bandit. Diﬀerent

from the bandit case, however, the oracle

T t=1

ρ∗t

does

not

equal

to

the

expected

optimum

for

the

non-stationary MDP problem in general. Nevertheless, we justify this choice in Proposition 1.

Remark 2 (Deﬁnition of Variation Budgets). For brevity of exposition, we choose to

deﬁne the variation budgets (see eqn. (1) ) for reward and state transition distributions with the

inﬁnity norm and 1 norm, respectively. One can also deﬁne them with respect to other commonly

used metrics, such as the 2 norm (Cheung et al. 2019b), and the this would only aﬀect the depen-

dence on S and A for the established dynamic regret bounds in the subsequent sections.

Next, we review relevant concepts on MDPs, in order to stipulate an assumption that ensures

learnability and justiﬁes our oracle.

Definition 1 (Communicating MDPs and Diameter (Jaksch et al. 2010)). Consider

a set of states S, a collection A = {As}s∈S of action sets, and a state transition distribution

p¯ = {p¯(·|s, a)}s∈S,a∈As. For any s, s ∈ S and stationary policy π, the hitting time from s to s under π

8

is the random variable Λ(s |π, s) := min {t : st+1 = s , s1 = s, sτ+1 ∼ p¯(·|sτ , π(sτ )) ∀τ } , which can be inﬁnite. We say that (S, A, p¯) is a communicating MDP iﬀ D := maxs,s ∈S minstationary π E [Λ(s |π, s)] is ﬁnite. The quantity D is the diameter associated with (S, A, p¯).
Remark 3 (Diameter and RL in MDPs). As shown in (Jaksch et al. 2010), “diameter” plays a fundamental role in characterizing the complexity of RL in MDPs. Intuitively, in order to make informative decisions, the DM has to have accurate estimates of the quantities rt(s, a)’s and pt(·|s, a).’s. In other words, she must visit every state s ∈ S and choose each of its available actions a ∈ As frequently enough to collect relevant samples. Consequently, the harder to transition from a state s to another state s , the more the DM would suﬀer during the learning process, and the diameter of a MDP captures the “hardness” of transitioning between states in this MDP. With the above remark, we make the following assumption.

Assumption 1 (Bounded Diameters). For each t ∈ [T ], the tuple (S, A, pt) constitutes a communicating MDP with diameter at most Dt. We denote the maximum diameter as Dmax = maxt∈{1,...,T } Dt.

The following proposition justiﬁes our choice of oracle

T t=1

ρ∗t

.

Proposition 1. Consider an instance (S, A, T, p, r) that satisﬁes Assumption 1 with maximum diameter Dmax, and has variation budgets Br, Bp for the rewards and transition distributions respectively. In addition, suppose that T ≥ Br + 2DmaxBp > 0, then it holds that

T

T

ρ∗t ≥ max E

rt

(sΠt

,

a

Π t

)

Π

t=1

t=1

− 4(Dmax + 1) (Br + 2DmaxBp)T .

The

maximum

is

taken

over

all

non-anticipatory

policies

Π’s.

We

denote

{(

sΠt

,

a

Π t

)

}Tt=1

as

the

trajectory under policy Π, where aΠt ∈ AsΠ is determined based on Π and Ht−1 ∪ {sΠt }, and sΠt+1 ∼ t

p

t

(

·|sΠt

,

a

Π t

)

for

each

t.

The Proposition is proved in section A.2 of the appendix. In fact, our dynamic regret bounds (see

the forthcoming Theorems 1 and 2) are larger than the error term 4(Dmax + 1) (Br + 2DmaxBp)T ,

thus justifying the choice of

T t=1

ρ∗t

as

the

oracle.

It

turns

out

that

the

oracle

T t=1

ρ∗t

is

more

convenient for analysis than the expected optimum, since the former can be decomposed to sum-

mations across diﬀerent intervals, unlike the latter where the summands are intertwined due to

endogenous

dynamics,

i.e.,

sΠt+1

∼

p

t

(

·|sΠt

,

a

Π t

).

3. Related Works
3.1. RL in Stationary MDPs
RL in stationary (discounted and un-discounted reward) MDPs has been widely studied in (Bur-
netas and Katehakis 1997, Bartlett and Tewari 2009, Jaksch et al. 2010, Agrawal and Jia 2017,

9

Fruit et al. 2018a,b, Sidford et al. 2018b,a, Wang 2019, Zhang and Ji 2019, Fruit et al. 2019,
Wei et al. 2019). For the discounted reward setting, the authors of (Sidford et al. 2018b, Wang
2019, Sidford et al. 2018a) proposed (nearly) optimal algorithms in terms of sample complexity.
For the un-discounted reward setting, the authors of (Jaksch et al. 2010) established a minimax √
lower bound Ω( DmaxSAT ) on the regret when both the reward and state transition distributions
are time-invariant. They also designed the UCRL2 algorithm and showed that it attains a regret √
bound O˜(DmaxS AT ). The authors of (Fruit et al. 2019) proposed the UCRL2B algorithm, which
is an improved version of the UCRL2 algorithm. The regret bound of the UCRL2B algorithm is O˜(S√DmaxAT + Dm2 axS2A). The minimax optimal algorithm is provided in (Zhang and Ji 2019) although it is not computationally eﬃcient.

3.2. RL in Non-Stationary MDPs

In a parallel work (Ortner et al. 2019), the authors considered a similar setting to ours by applying

the “forgetting principle” from non-stationary bandit settings (Garivier and Moulines 2011, Cheung

et al. 2019a) to design a learning algorithm. To achieve its dynamic regret bound, the algorithm

by (Ortner et al. 2019) partitions the entire time horizon [T ] into time intervals I = {Ik}Kk=1, and

crucially requires the access to

max Ik−1 t=min I

Br,t

and

max Ik−1 t=min I

Bp,t,

i.e.,

the

variations

in

both

reward

k

k

and state transition distributions of each interval Ik ∈ I (see Theorem 3 in (Ortner et al. 2019)). In

contrast, the SWUCRL2-CW algorithm and the BORL algorithm require signiﬁcantly less information

on the variations. Speciﬁcally, the SWUCRL2-CW algorithm does not need any additional knowledge

on the variations except for Br and Bp, i.e., the variation budgets over the entire time horizon

as deﬁned in eqn. (1), to achieve its dynamic regret bound (see Theorem 1). This is similar to

algorithms for the non-stationary bandit settings, which only require the access to Br (Besbes et al.

2014). More importantly, the BORL algorithm (built upon the SWUCRL2-CW algorithm) enjoys the

same dynamic regret bound even without knowing either of Br or Bp (see Theorem 2).

There also exists some settings that are closely related to, but diﬀerent than our setting (in terms

of exogeneity and feedback). (Jaksch et al. 2010, Gajane et al. 2018) proposed solutions for the

RL in piecewise-stationary MDPs setting. But as discussed in Section 1.2, simply applying their

techniques to the general RL in non-stationary MDPs may result in undesirable dynamic regret

bounds (see Section 6 for more details). In (Yu et al. 2009, Neu et al. 2010, Arora et al. 2012,

Dick et al. 2014, Jin et al. 2019, Cardoso et al. 2019), the authors considered RL in MDPs with

changing reward distributions but ﬁxed transition distributions. The authors of (Even-Dar et al.

2005, Yu and Mannor 2009, Neu et al. 2012, Abbasi-Yadkori et al. 2013, Rosenberg and Mansour

2019, Li et al. 2019) considered RL in non-stationary MDPs with full information feedback.

10
3.3. Non-Stationary MAB For online learning and bandit problems where there is only one state, the works by (Auer et al. 2002a, Garivier and Moulines 2011, Besbes et al. 2014, Keskin and Zeevi 2016) proposed several “forgetting” strategies for diﬀerent non-stationary MAB settings. More recently, the works by (Karnin and Anava 2016, Luo et al. 2018, Cheung et al. 2019b,a, Chen et al. 2019b) designed parameter-free algorithms for non-stationary MAB problems. Another related but diﬀerent setting is the Markovian bandit (Kim and Lim 2016, Ma 2018), in which the state of the chosen action evolve according to an independent time-invariant Markov chain while the states of the remaining actions stay unchanged. In (Zhou et al. 2020), the authors also considered the case when the states of all the actions are governed by the same (uncontrollable) Markov chain.
4. Sliding Window UCRL2 with Conﬁdence Widening Algorithm
In this section, we ﬁrst describe a unique challenge in RL in non-stationary MDPs, and then present the SWUCRL2-CW algorithm, which incorporates our novel conﬁdence widening technique and sliding window estimates (Garivier and Moulines 2011) into UCRL2 (Jaksch et al. 2010).
4.1. Design Challenge: Failure of Naive Sliding Window UCRL2 Algorithm For stationary MAB problems, the UCB algorithm (Auer et al. 2002b) suggests the DM should iteratively execute the following two steps in each time step: 1. Estimate the mean reward of each action by taking the time average of all observed samples. 2. Pick the action with the highest estimated mean reward plus the conﬁdence radius, where the
radius scales inversely proportional with the number of observations (Auer et al. 2002b). The UCB algorithm has been proved to attain optimal regret bounds for various stationary MAB settings (Auer et al. 2002b, Kveton et al. 2015). For non-stationary problems, (Garivier and Moulines 2011, Keskin and Zeevi 2016, Cheung et al. 2019a) shown that the DM could further leverage the forgetting principle by incorporating the sliding-window estimator (Garivier and Moulines 2011) into the UCB algorithms (Auer et al. 2002b, Kveton et al. 2015) to achieve optimal dynamic regret bounds for a wide variety of non-stationary MAB settings. The sliding window UCB algorithm with a window size W ∈ R+ is similar to the UCB algorithm except that the estimated mean rewards are computed by taking the time average of the W most recent observed samples.
As noted in Section 1, (Jaksch et al. 2010) proposed the UCRL2 algorithm, which is a UCB-alike algorithm with nearly optimal regret for RL in stationary MDPs. It is thus tempting to think that one could also integrate the forgetting principle into the UCRL2 algorithm to attain low dynamic regret bound for RL in non-stationary MDPs. In particular, one could easily design a naive slidingwindow UCRL2 algorithm that follows exactly the same steps as the UCRL2 algorithm with the exception that it uses only the W most recent observed samples instead of all observed samples to

11
estimate the mean rewards and the state transition distributions, and to compute the respective conﬁdence radius.
Under non-stationarity and bandit feedback, however, we show in Proposition 3 of the forthcoming Section 6 that the diameter of the estimated MDP produced by the naive sliding-window UCRL2 algorithm with window size W can be as large as Θ(W ), which is orders of magnitude larger than Dmax, the maximum diameter of each individual MDP encountered by the DM. Consequently, the naive sliding-window UCRL2 algorithm may result in undesirable dynamic regret bound. In what follows, we discuss in more details how our novel conﬁdence widening technique can mitigate this issue.
4.2. Design Overview The SWUCRL2-CW algorithm ﬁrst speciﬁes a sliding window parameter W ∈ N and a conﬁdence widening parameter η ≥ 0. Parameter W speciﬁes the number of previous time steps to look at. Parameter η quantiﬁes the amount of additional optimistic exploration, on top of the conventional optimistic exploration using upper conﬁdence bounds. The latter turns out to be helpful for handling the temporal drifts in the state transition distributions (see Section 6).
The algorithm runs in a sequence of episodes that partitions the T time steps. Episode m starts at time τ (m) (in particular τ (1) = 1), and ends at the end of time step τ (m + 1) − 1. Throughout an episode m, the DM follows a certain stationary policy π˜τ(m). The DM ceases the mth episode if at least one of the following two criteria is met: • The time index t is a multiple of W. Consequently, each episode last for at most W time steps.
The criterion ensures that the DM switches the stationary policy π˜τ(m) frequently enough, in order to adapt to the exogenous dynamics. • There exists some state-action pair (s, a) such that ντ(m)(s, a), the number of time step t’s with (st, at) = (s, a) within episode m, is at least as many as the total number of counts for it within the W time steps prior to τ (m), i.e., from (τ (m) − W ) ∨ 1 to (τ (m) − 1). This is similar to the doubling criterion in (Jaksch et al. 2010), which ensures that each episode is suﬃciently long so that the DM can focus on learning. The combined eﬀect of these two criteria allows the DM to learn a low dynamic regret policy with historical data from an appropriately sized time window and conﬁdence widening parameter. One important piece of ingredient is the construction of the policy π˜τ(m), for each episode m. To allow learning under endogenous and exogenous dynamics, the SWUCRL2-CW algorithm computes the policy π˜m based on the history in the W time steps prior to the current episode m, i.e., from round (τ (m) − W ) ∨ 1 to round τ (m) − 1. The construction of π˜τ(m) involves the Extended Value Iteration (EVI) (Jaksch et al. 2010), which requires the conﬁdence regions Hr,τ(m), Hp,τ(m)(η) for

12

rewards and state transition distributions as the inputs, in addition to an precision parameter . The conﬁdence widening parameter η ≥ 0 is capable of ensuring the MDP output by the EVI has a bounded diameter most of the time.

4.3. Policy Construction

To describe SWUCRL2-CW algorithm, we ﬁrst deﬁne for each state-action pair (s, a) and each time t

in episode m,

t−1

Nt(s, a) =

1((sq, aq) = (s, a)), Nt+(s, a) = max{1, Nt(s, a)}.

(3)

q=(τ (m)−W )∨1

4.3.1. Conﬁdence Region for Rewards For each state-action pair (s, a) and each time t in

episode m, we consider the empirical mean estimator





1

t−1

rˆt(s, a) = Nt+(s, a) q=(τ(m)−W )∨1 Rq (s, a) 1(sq = s, aq = a) ,

which serves to estimate the average reward





1

t−1

r¯t(s, a) = Nt+(s, a) q=(τ(m)−W )∨1 rq(s, a)1(sq = s, aq = a) .

The conﬁdence region Hr,t = {Hr,t(s, a)}s∈S,a∈As is deﬁned as

Hr,t(s, a) = {r˙ ∈ [0, 1] : |r˙ − rˆt(s, a)| ≤ rad-r,t(s, a)} ,

(4)

with conﬁdence radius rad-r,t(s, a) = 2

2

log

(

S

AT

/δ

)

/N

+ t

(

s,

a

)

.

4.3.2. Conﬁdence Widening for State Transition Distributions. For each state-action

pair s, a and each time step t in episode m, we consider the empirical mean estimator





1

t−1

pˆt(s |s, a) = Nt+(s, a) q=(τ(m)−W )∨1 1(sq = s, aq = a, sq+1 = s ) ,

which serves to estimate the average transition probability

1

t−1

p¯t(s |s, a) = Nt+(s, a) q=(τ(m)−W )∨1 pq(s |s, a)1(sq = s, aq = a). (5)

Diﬀerent from the case of estimating reward, the conﬁdence region Hp,t(η) = {Hp,t(s, a; η)}s∈S,a∈As

for the transition probability involves a widening parameter η ≥ 0:

Hp,t(s, a; η) = p˙ ∈ ∆S : p˙(·|s, a) − pˆt(·|s, a) 1 ≤ rad-p,t(s, a) + η ,

(6)

with conﬁdence radius rad-p,t(s, a) = 2 2S log (SAT /δ) /Nt+(s, a). With η > 0, the DM can explore state transition distributions that deviate from the sample average, and the exploration is crucial

for learning MDPs under endogenous and exogenous dynamics. In a nutshell, the incorporation of

η provides an additional source of optimism. We treat η as a hyper-parameter at the moment, and

provide a suitable choice of η when we discuss our main results (see Theorem 1).

13

Algorithm 1 SWUCRL2-CW algorithm
1: Input: Time horizon T , state space S, and action space A, window size W , conﬁdence widening parameter η.
2: Initialize t ← 1, initial state s1. 3: for episode m = 1, 2, . . . do 4: Set τ (m) ← t, ντ(m)(s, a) ← 0, and Nτ(m)(s, a) according to eqn (3), for all s, a. 5: Compute the conﬁdence regions Hr,τ(m), Hp,τ(m)(η) according to eqns (4, 6). 6: Compute a (1/ τ (m))-optimal optimistic policy π˜τ(m):

EVI(Hr,τ(m), Hp,τ(m)(η); 1/ τ (m)) → (π˜τ(m), r˜τ(m), p˜τ(m), ρ˜τ(m), γ˜τ(m)).

7: while t is not a multiple of W and νm(st, π˜τ(m)(st)) < Nτ+(m)(st, π˜τ(m)(st)) do

8:

Choose action at = π˜τ(m)(st), observe reward Rt(st, at) and the next state st+1.

9:

Update ντ(m)(st, at) ← ντ(m)(st, at) + 1, t ← t + 1.

10:

if t > T then

11:

The algorithm is terminated.

12:

end if

13: end while

14: end for

4.3.3. Extended Value Iteration (EVI) (Jaksch et al. 2010). The SWUCRL2-CW algorithm relies on the EVI, which solves MDPs with optimistic exploration to near-optimality. We extract and rephrase a description of EVI in Section A.3 of the appendix. EVI inputs the conﬁdence regions Hr, Hp for the rewards and the state transition distributions. The algorithm outputs an “optimistic MDP model”, which consists of reward vector r˜ and state transition distribution p˜ under which the optimal average gain ρ˜ is the largest among all r˙ ∈ Hr, p˙ ∈ Hp: • Input: Conﬁdence regions Hr for r, Hp for p, and an error parameter > 0. • Output: The returned policy π˜ and the auxiliary output (r˜, p˜, ρ˜, γ˜). In the latter, r˜, p˜, and ρ˜
are the selected “optimistic” reward vector, state transition distribution, and the corresponding long term average reward. The output γ˜ ∈ RS+ is a bias vector (Jaksch et al. 2010). For each s ∈ S, the quantity γ˜(s) is indicative of the short term reward when the DM starts at state s and follows the optimal policy. By the design of EVI, for the output γ˜, there exists s ∈ S such that γ˜(s) = 0. Altogether, we express
EVI(Hr, Hp; ) → (π˜, r˜, p˜, ρ˜, γ˜).
Combining the three components, a formal description of the SWUCRL2-CW algorithm is shown in Algorithm 1.
4.4. Performance Analysis: The Blessing of More Optimism We now analyze the performance of the SWUCRL2-CW algorithm. First, we introduce two events Er, Ep, which state that the estimated reward and state transition distributions lie in the respective

14

(un-widened) conﬁdence regions.

Er = {r¯t(s, a) ∈ Hr,t(s, a) ∀s, a, t}, Ep = {p¯t(·|s, a) ∈ Hp,t(s, a; 0) ∀s, a, t}.

We prove that Er, Ep hold with high probability.

Lemma 1. We have Pr[Er] ≥ 1 − δ/2, Pr[Ep] ≥ 1 − δ/2.

The proof of Lemma 1 is provided in Section B of the appendix. In deﬁning Ep, the widening parameter η is set to be 0, since we are only concerned with the estimation error on p. Next, we bound the dynamic regret of each time step, under certain assumptions on Hp,t(η). To facilitate our discussion, we deﬁne the following variation measure for each t in an episode m:

t−1

varr,t =

Br,q ,

q=(τ (m)−W )∨1

t−1

varp,t =

Bp,q .

q=(τ (m)−W )∨1

Proposition 2. Consider an episode m. Condition on events Er, Ep, and suppose that there exists a state transition distribution p satisfying two properties: (1) ∀s ∈ S ∀a ∈ As, we have p(·|s, a) ∈ Hp,τ(m)(s, a; η), and (2) the diameter of (S, A, p) at most D. Then, for every t ∈ {τ (m), . . . , τ (m + 1) − 1} in episode m, we have

ρ∗t − rt(st, at) ≤

pt(s |st, at)γ˜τ(m)(s ) − γ˜τ(m)(st)

(7)

s ∈S

1

+

+ [2varr,t + 4D(varp,t + η)] + 2rad-r,τ(m)(st, at) + 4D · rad-p,τ(m)(s, a) . (8)

τ (m)

The proof of Proposition 2 is provided in Section C of the appendix. Next, we state our ﬁrst main result, which provides a dynamic regret bound assuming the knowledge of Br, Bp to set W, η:

Theorem 1. Assuming S > 1, the SWUCRL2-CW algorithm with window size W, conﬁdence widen-

ing parameter η > 0, and δ = T −1 satisﬁes the dynamic regret bound

√

√

O˜ BpηW + BrW + √SWAT + Dmax BpW + S√WAT + T η + SWAT + √T .

If we further put W = W ∗ = S2/3A1/2T 1/2(Br + Bp)−1/2 and η = η∗ := O˜ Dmax(Br + Bp)1/4S2/3A1/2T 3/4 .

BpW ∗T −1, this is

The proof of Theorem 1 is provided in Section D of the appendix. Remark 4 (Conﬁdence Widening). Similar to the regret analysis of the UCRL2 algorithm
(Section 4 of (Jaksch et al. 2010)) and the UCRL2B algorithm (Lemma 3 and eqn. (10) of (Fruit et al. 2019)), Proposition 2 states that, if the conﬁdence region Hp,τ(m)(η) contains a state transition

15

distribution with diameter at most D, then the EVI provided with Hp,τ(m)(η) returns a policy

with dynamic regret bound that grows at most linearly with D during episode m. However, as

shown in Section 6 later, the parameter η has to be carefully chosen for D to be small as the

worst case diameter of every state transition distribution in Hp,τ(m)(0) (i.e., setting η = 0) can grow as Ω˜ (√W ), and might result in unfavorable dynamic regret bound. Here, the parameter η is the

keystone of our novel conﬁdence widening technique and the resulting dynamic regret bound: As

η increases, the conﬁdence region Hp,τ(m)(s, a; η) becomes larger for each state-action pair (s, a).

Consider the ﬁrst time step τ (m) of each episode m : if pτ(m)(·|s, a) ∈ Hp,τ(m)(s, a; η) for all state-

action pair (s, a), then Proposition 2 can be leveraged; otherwise, the widened conﬁdence region

enforces that a considerable amount of variation budget is consumed.

Remark 5 (Connections with Dynamic Regret Bounds for Non-Stationary MAB).

When S = {s}, our problem becomes the non-stationary bandit problem studied by (Besbes et al.

2014),

and

we

have

Dmax

=

0

and

Bp

=

0.

By

choosing

W

=

W∗

=

A1/

3

T

2/

3

/B

2/ r

3

,

our

algorithm

has

dynamic

regret

O˜

(B

1/ r

3

A1

/

3

T

2/

3

),

matching

the

minimax

optimal

dynamic

regret

bound

by

(Besbes et al. 2014) when Br ∈ [A−1, A−1T ].

Remark 6 (Connections with Regret Bounds for RL in Stationary MDPs). When

r1 = . . . = rT and p1 = . . . = pT , our problem becomes the RL in stationary MDPs problem studied

by (Jaksch et al. 2010), and the SWUCRL2-CW algorithm with W = T and η = 0 can recover the √
regret bound O˜(DmaxS AT ) of the UCRL2 algorithm studied in (Jaksch et al. 2010).

Remark 7 (Dynamic Regret Bound without Knowing Br and Bp). Similar to (Cheung

et

al.

2019b,a),

if

Bp, Br

are

not

known,

we

can

set

W

and

η

obliviously

as

W

=

S

2 3

A

1 2

T

1 2

,

η=

W/T

=

S

2 3

A

1 2

T

−

1 2

to obtain a dynamic regret bound O˜

Dmax(Br + Bp + 1)S2/3A1/2T 3/4

.

5. Bandit-over-Reinforcement Learning Algorithm: Towards Parameter-Free
As pointed out by Remark 7, in the case of unknown Br and Bp, the dynamic regret of SWUCRL2-CW algorithm scales linearly in Br and Bp. However, by Theorem 1, we are assured a ﬁxed pair of parameters (W ∗, η∗) can ensure low dynamic regret. For the bandit setting, (Cheung et al. 2019a,b) propose the bandit-over-bandit framework that uses a separate copy of EXP3 algorithm to tune the window size. Inspired by it, we develop a novel Bandit-over-Reinforcement Learning (BORL) algorithm with parameter-free O˜ Dmax(Br + Bp + 1)1/4S2/3A1/2T 3/4 dynamic regret here.
5.1. Design Overview Following a similar line of reasoning as (Cheung et al. 2019a), we make use of the SWUCRL2-CW algorithm as a sub-routine, and “hedge” (Bubeck and Cesa-Bianchi 2012) against the (possibly

16
adversarial) changes of rt’s and pt’s to identify a reasonable ﬁxed window size and conﬁdence widening parameter.
As illustrated in Fig. 1, the BORL algorithm divides the whole time horizon into T /H blocks of equal length H rounds (the length of the last block can ≤ H), and speciﬁes a set J from which each pair of (window size, conﬁdence widening) parameter are drawn from. For each block i ∈ [ T /H ], the BORL algorithm ﬁrst calls some master algorithm to select a pair of (window size, conﬁdence widening) parameters (Wi, ηi) (∈ J), and restarts the SWUCRL2-CW algorithm with the selected parameters as a sub-routine to choose actions for this block. Afterwards, the total reward of block i is fed back to the master, and the “posterior” of these parameters are updated accordingly.
One immediate challenge not presented in the bandit setting (Cheung et al. 2019b) is that the starting state of each block is determined by previous moves of the DM. Hence, the master algorithm is not facing a simple oblivious environment as the case in (Cheung et al. 2019b), and we cannot use the EXP3 (Auer et al. 2002a) algorithm as the master. Nevertheless, fortunately the state is observed before the starting of a block. Thus, we use the EXP3.P algorithm for multi-armed bandit against an adaptive adversary (Auer et al. 2002a, Bubeck and Cesa-Bianchi 2012) as the master algorithm. We follow the exposition in Section 3.2 in (Bubeck and Cesa-Bianchi 2012) for adapting the EXP3.P algorithm.

Figure 1 Structure of the BORL algorithm

5.2. Design Details We are now ready to state the details of the BORL algorithm. For some ﬁxed choice of block length H (to be determined later), we ﬁrst deﬁne a couple of additional notations:

H = 3S 32 A 12 T 21 , Φ = 1 1 , ∆W = ln H , ∆η = ln Φ−1 , ∆ = (∆W + 1)(∆η + 1), (9) 2T 2

JW =

1
H 0, H ∆W

,...,H

11
, Jη = S 3 A 4 ×

1
Φ0, Φ ∆η , . . . , Φ

, J = {(W, η) : W ∈ JW , η ∈ Jη} .

Here, JW and Jη are all possible choices of window size and conﬁdence widening parameter, respectively, and J is the Cartesian product of them with |J| = ∆. We also let Ri(W, η, s) be the total

17

rewards for running the SWUCRL2-CW algorithm with window size W and conﬁdence widening parameter η for block i starting from state s,
The EXP3.P algorithm treats each element of J as an arm. It begins by initializing

ln ∆

ln ∆

∆ ln ∆

α = 0.95 ∆ T /H , β = ∆ T /H , γ = 1.05 T /H , q(j,k),1 = 0 ∀ (j, k) ∈ M, (10)

where M = {(j , k ) : j ∈ {0, 1, . . . , ∆W }, k ∈ {0, 1, . . . , ∆η}}. At the beginning of each block i ∈

[ T /H ] , the BORL algorithm ﬁrst sees the state s(i−1)H+1, and computes

∀ (j, k) ∈ M,

u(j,k),i = (1 − γ)

exp(αq(j,k),i)

γ +.

(j ,k )∈M exp(αq(j ,k ),i) ∆

(11)

Then it sets (ji, ki) = (j, k) with probability u(j,k),i ∀ (j, k) ∈ M. The selected pair of parameters are

thus Wi = Hji/∆W and ηi = Φki/∆η . Afterwards, the BORL algorithm starts from state s(i−1)H+1,

selects actions by running the SWUCRL2-CW algorithm with window size Wi and conﬁdence widening

parameter ηi for each round t in block i. At the end of the block, the BORL algorithm observes the

total rewards R Wi, ηi, s(i−1)H+1 . As a last step, it rescales R Wi, ηi, s(i−1)H+1 by dividing it by

H so that it is within [0, 1], and updates

∀ (j, k) ∈ M

q(j,k),i+1 = q(j,k),i + β + 1(j,k)=(ji,ki) · Ri Wi, ηi, s(i−1)H+1 /H . u(j,k),i

(12)

The formal description of the BORL algorithm (with H deﬁned in the next subsection) is shown in

Algorithm 2.

Algorithm 2 BORL algorithm

1: Input: Time horizon T , state space S, and action space A, initial state s1.

2: Initialize H, Φ, ∆W , ∆η, ∆, JW , Jη according to eqn. (9), and α, β, γ according to eqn. (10).

3: M ← {(j , k ) : j ∈ {0, 1, . . . , ∆W }, k ∈ {0, 1, . . . , ∆η}}, q(j,k),1 ← 0 ∀(j, k) ∈ M.

4: for i = 1, 2, . . . , T /H do

5: Deﬁne distribution (u(j,k),i)(j,k)∈M according to eqn. (11), and set (ji, ki) ← (j, k) with prob-

ability u(j,k),i.

6:

Wi ← H ji/∆W , ηi ← Φki/∆η .

7: for t = (i − 1)H + 1, . . . , i · H ∧ T do

8:

Run the SWUCRL2-CW algorithm with window size Wi and blow up parameter ηi, and

observe the total rewards R Wi, ηi, s(i−1)H+1 .

9: end for

10: Update q(j,k),i+1 according to eqn. (12).

11: end for

5.3. Performance Analysis The dynamic regret guarantee of the BORL algorithm can be presented as follows
Theorem 2. Assume S > 1, with probability 1 − O(δ), the dynamic regret bound of the BORL algorithm is O˜ Dmax(Br + Bp + 1)1/4S2/3A1/2T 3/4 The proof of Theorem 2 is provided in Section E of the appendix.

18

6. The Perils of Drift in Learning Markov Decision Processes

In stochastic online learning problems, one usually estimates a latent quantity by taking the time

average of observed samples, even when the sample distribution varies across time. This has been

proved to work well in stationary and non-stationary bandit settings (Auer et al. 2002b, Garivier

and Moulines 2011, Cheung et al. 2019a,b). To extend to RL, it is natural to consider the sample

average transition distribution pˆt, which uses the data in the previous W rounds to estimate the time average transition distribution p¯t to within an additive error O˜(1/ Nt+(s, a)) (see Section

4.3.3 and Lemma 1). In the case of stationary MDPs, where ∀ t ∈ [T ] pt = p, one has p¯t = p.

Thus, the un-widened conﬁdence region Hp,t(0) contains p with high probability (see Lemma 1).

Consequently, the UCRL2 algorithm by (Jaksch et al. 2010), which optimistic explores Hp,t(0), has

a regret that scales linearly with the diameter of p.

The approach of optimistic exploring Hp,t(0) is further extended to RL in piecewise-stationary

MDPs

by

(Jaksch

et

al.

2010,

Gajane

et

al.

2018).

The

latter

establishes

a

O(

1/

3

D

2/3 max

S

2/

3

A1/

3

T

2/

3

)

dynamic regret bounds, when there are at most changes. Their analyses involve partitioning

the T -round horizon into C · T 1/3 equal-length intervals, where C is a constant dependent on

Dmax, S, A, . At least CT 1/3 − intervals enjoy stationary environments, and optimistic exploring

Hp,t(0) in these intervals yields a dynamic regret bound that scales linearly with Dmax. Bounding

the dynamic regret of the remaining intervals by their lengths and tuning C yield the desired

bound.

In contrast to the stationary and piecewise-stationary settings, optimistic exploration on Hp,t(0)

might lead to unfavorable dynamic regret bounds in non-stationary MDPs. In the non-stationary

environment where pt−W , . . . , pt−1 are generally distinct, we show that it is impossible to bound

the diameter of p¯t in terms of the maximum of the diameters of pt−W , . . . , pt−1. More generally,

we demonstrate the previous claim not only for p¯t, but also for every p˜ ∈ Hp,t(0) in the following

Proposition. The Proposition showcases the unique challenge in exploring non-stationary MDPs

that is absent in the piecewise-stationary MDPs, and motivates our notion of conﬁdence widening

with η > 0. To ease the notation, we put t = W + 1 without loss of generality.

Proposition 3. There exists a sequence of non-stationary MDP transition distributions p1, . . . , pW such that • The diameter of (S, A, pn) is 1 for each n ∈ [W ]. • The total variations in state transition distributions is O(1). Nevertheless, under some deterministic policy, 1. The empirical MDP (S, A, pˆW +1) has diameter Θ(W ) 2. Further, for every p˜ ∈ Hp,W +1(0), the MDP (S, A, p˜) has diameter Ω( W/ log W )

19
Proof. The sequence p1, . . . , pW alternates between the following 2 instances p1, p2. Now, deﬁne the common state space S = {1, 2} and action collection A = {A1, A2}, where A1 = {a1, a2}, {A2} = {b1, b2}. We assume all the state transitions are deterministic, and a graphical illustration is presented in Fig. 2. Clearly, we see that both instances have diameter 1.

Figure 2 Example MDPs. Since the transitions are deterministic, the probabilities are omitted.

Now, consider the following two deterministic and stationary policies π1 : π1(1) = a1, π1(2) = b2, and π2 : π2(1) = a2, π2(2) = b1. Since the MDP is deterministic, we have pˆW +1 = p¯W +1.
In the following, we construct a trajectory where the DM alternates between policies π1, π2 during time {1, . . . , W } while the underlying transition distribution alternates between p1, p2. In the construction, the DM is almost always at the self-loop at state 1 (or 2) throughout the horizon, no matter what action a1, a2 (or b1, b2) she takes. Consequently, it will trick the DM into thinking that pˆW +1(1|1, ai) ≈ 1 for each i ∈ {1, 2}, and likewise pˆW +1(2|2, bi) ≈ 1 for each i ∈ {1, 2}. Altogether, this will lead the DM to conclude that (S, A, pˆW+1) constitute a high diameter MDP, since the probability of transiting from state 1 to 2 (and 2 to 1) are close to 0.
The construction is detailed as follows. Let W = 4τ . In addition, let the state transition distributions be

p1 = . . . = pτ = p1, pτ+1 = . . . = p2τ = p2, p2τ+1 = . . . = p3τ = p1, p3τ+1 = . . . = p4τ = p2.

The DM starts at state 1. She follows policy π1 from time 1 to time 2τ , and then policy π2 from 2τ + 1 to 4τ .
Under the speciﬁed MDP models and policies, it can be readily veriﬁed that the DM takes action a1 from time 1 to τ + 1, action b2 from time τ + 2 to 2τ , action b1 from time 2τ + 1 to 3τ + 1, and action a2 from time 3τ + 2 to 4τ . As a result, the DM is at state 1 from time 1 to τ + 1, state 2 from time τ + 2 to 3τ + 1, and eventually state 1 from time 3τ + 2 to 4τ as depicted in Fig. 3. We thus have:

τ pˆW +1(1|1, a1) = τ + 1 ,

1 pˆW +1(2|1, a1) = τ + 1 ,

pˆW +1(1|1, a2) = 1,

pˆW +1(2|1, a2) = 0

20

Figure 3 Illustration of the latent MDPs, policies, and state visits.

τ pˆW +1(2|2, b1) = τ + 1 ,

1 pˆW +1(1|2, b1) = τ + 1 ,

pˆW +1(2|2, b2) = 1,

pˆW +1(1|2, b2) = 0,

and It can be readily veriﬁed that the diameter of (S, A, pˆW+1) is τ + 1 = Θ(W ). Finally, for the conﬁdence region Hp,W +1(0) = {Hp,W +1(s, a; 0)}s,a constructed without conﬁdence widening, for any p˜ ∈ Hp,W +1(0) we have

p˜(2|1, a1) = p˜(1|2, b1) = O

log W τ + 1 , p˜(2|1, a2) = p˜(1|2, b2) = O

log W τ −1

respectively. Since the stochastic conﬁdence radii Θ loτg+W1 and Θ loτg−W1 dominate the sample

mean

1 τ +1

and

0.

Therefore,

for

any

p˜ ∈ Hp,W +1(0),

the

diameter

of

the

MDP

constructed

by

(S, A, p˜) is at least Ω loW g W .

Remark 8. In Proposition 3, there are two reasons for the discrepancy between the individual

MDPs p1, . . . , pW and the MDPs in the un-widened conﬁdence region Hp,W +1(0):

• First, due to the bandit feedback, the samples used to construct pˆW+1 come from diﬀerent state-

action pairs at diﬀerent time. As a result, pˆW +1 and p¯W +1 can be very diﬀerent than each of the

individual state transition probability distributions p1, . . . , pW .

• Second, the number of visits to each state-action pair is roughly W/4, which means we would have very “narrow” conﬁdence regions (of the order O˜(1/√W )) if we follow standard optimistic

exploration techniques based on concentration inequalities (i.e., the conﬁdence regions shrink as

the number of samples grows).

Critically, as shown in Proposition 2 (as well as Section 4 of (Jaksch et al. 2010)) as well as Lemma

3 and eqn. (10) of (Fruit et al. 2019)), the minimum diameter of the MDPs in the conﬁdence

regions play a key role in leading to low (dynamic) regret bounds. We thus believe the caveat in

learning non-stationary MDPs via conventional optimistic exploration is fundamental in general. In

the current paper, we leverage our novel conﬁdence widening technique to prevent the conﬁdence

regions from becoming too narrow even if we have lots of samples.

Remark 9. Inspecting the prevalent OFU guided approach for stochastic MAB and RL in

MDPs settings (Auer et al. 2002b, Abbasi-Yadkori et al. 2011, Jaksch et al. 2010, Bubeck and

21
Cesa-Bianchi 2012, Lattimore and Szepesv´ari 2018), one usually concludes that a tighter design of conﬁdence region can result in a lower (dynamic) regret bound. In (Abernethy et al. 2016), this insights has been formalized in stochastic K-armed bandit settings via a potential function type argument. Nevertheless, Proposition 3 (together with Theorem 1) demonstrates that using the tightest conﬁdence region in learning algorithm design may not be enough to ensure low dynamic regret bound for RL in non-stationary MDPs.
7. Alternative for Conﬁdence Widening with Application in Inventory Control
As demonstrated in previous sections, running the proposed algorithms with the widened conﬁdence regions can help the DM to attain provably low dynamic regret in general RL in non-stationary MDPs. Nevertheless, conﬁdence widening is not always necessary if the state transition distributions bear a special structure. In particular, we consider the following assumption on the state transition distributions p1, . . . , pT .
Assumption 2. There exists a positive quantity (not necessarily known to the DM) ζ ∈ R+, such that for any pair of states s, s ∈ S, there is an action a(s,s ) ∈ As that satisﬁes pt s |s, a(s,s ) ≥ ζ for all t ∈ [T ].
We can now analyze the dynamic regret bound of the SWUCRL2-CW algorithm under Assumption 2. Here, we follow the notations introduced in Section 2 for consistency. In general, Assumption 2 ensures that for every time step t ∈ [T ], there exists a state transition distribution p ∈ Hp,t(0) such that the induced diameter of the MDP (S, A, p) is upper bounded by the constant D¯ := 1/ζ with high probability.
Proposition 4. Under Assumption 2 and conditioned on the event Ep, there exists a state transition distribution p in the conﬁdence region Hp,t(0), such that the induced diameter of the MDP (S, A, p) is at most D¯ := 1/ζ for all t ∈ [T ].
The proof of Proposition 4 is provided in Section F of the appendix. The proposition indicates that the DM can achieve a bounded dynamic regret by implementing the SWUCRL2-CW algorithm with η = 0. To analyze its dynamic regret bound, we provide a variation of Proposition 2 as follows.
Proposition 5. Consider an episode m. Conditioning on events Er, Ep, then for every t ∈ {τ (m), . . . , τ (m + 1) − 1} in episode m, we have

ρ∗t − rt(st, at) ≤

pt(s |st, at)γ˜τ(m)(s ) − γ˜τ(m)(st)

s ∈S

+ 1 + 2varr,t + 4D¯ varp,t + 2rad-r,τ(m)(st, at) + 4D¯ · rad-p,τ(m)(s, a) . τ (m)

22

The proof is similar to that of Proposition 2 with Dτ(m) replaced by D¯ and η set to 0, respectively. We are now ready to state the dynamic regret bound of the SWUCRL2-CW algorithm when Assumption 2 holds.

Theorem 3. Under Assumption 2 and assuming S > 1, the SWUCRL2-CW algorithm with window

size W, conﬁdence widening parameter η = 0, and δ = T −1 satisﬁes the dynamic regret bound

Dyn-RegT (SWUCRL2-CW) = O˜

S√AT SAT √

√+

+T

BrW + D¯ BpW +

W

W

If we further put W = W ∗ = S2/3A1/2T 2/3(Br + Bp + 1)−2/3, this dynamic regret bound is O˜ D¯ (Br + Bp + 1)1/3S2/3A1/2T 2/3 .

We omit the proof since it is similar to that of Theorem 1.

7.1. An Application to Inventory Control In this subsection, we ﬁrst elaborate on Assumption 2 in the context of single non-perishable item inventory control problem with zero lead time, ﬁxed cost, and lost sales similar to (Yuan et al. 2019), and then demonstrate how to implement the SWUCRL2-CW algorithm for this problem. For each time step t ∈ [T ] of the inventory control problem (with some abuse of notations), the following sequence of events happens: 1. The seller ﬁrst observes her stock level st, and decides the quantity at to order. 2. If at > 0, a ﬁxed cost f and a c per-unit ordering cost are incurred, and the order arrives
instantaneously. The stock level then becomes st + at. 3. The demand Xt is realized, and the seller observes the censored demand Yt = min{Xt, st + at}.
The DM faces non-stationary demands, in the sense that the demand distributions X1, . . . , XT at time steps 1, . . . T are independent but not identically distributed. 4. Unfulﬁlled demand incurs a l per-unit lost sales cost, while excess inventory leads to a h per-nit holding cost. The total cost for time step t is

Ct(st, at) = f · 1[at > 0] + c · at + l · [Xt − st − at]+ + h · [st + at − Xt]+.

(13)

Due to demand censoring, the cost is not observable.

The seller’s objective is to minimize the cumulative total cost

T t=1

Ct

(st

,

at

).

To

map

this

into

the non-stationary MDP model we described in Section 2, we represent the level of stock at the

beginning of each time step as the state. Same as (Yuan et al. 2019) (and similar to (Huh and

Rusmevichientong 2009, Zhang et al. 2018, Agrawal and Jia 2019)), we assume the DM has a

limited shelf capacity, and she can hold at most S units of inventory at any time. Consequently,

23
S = {0, . . . , S}, and As = {0, . . . , S −s} for each s ∈ S. We also deﬁne the reward and state transition distributions for all t ∈ [T ], s, s ∈ S, and a ∈ As as follows,
Rt(s, a) = −Ct(s, a) and pt(s |s, a) = Pr (s + a − min{s + a, Xt} = s ) .
However, it is worth emphasizing that, diﬀerent than our setup in Section 2, Rt(s, a) is not observable as Ct(s, a) is not observable. Nevertheless, we shall demonstrate in Section 7.1.2 that one could use the technique of pseudo-reward proposed in (Agrawal and Jia 2019) to bypass this issue.
Following Assumption 2, we make the strictly positive probability mass function (PMF) assumption on X1, . . . , XT .
Assumption 3 (Strictly Positive PMF). There is a ζ > 0 such that Pr(Xt = s) ≥ ζ > 0 for all t ∈ [T ] and s ∈ {0, . . . , S}.
Remark 10. It can be readily veriﬁed that if the demands satisfy the strictly positive PMF assumption, the underlying inventory control problem satisﬁes Assumption 2. Indeed, the DM could transit from a state s ∈ S to another state s ∈ S with probability at least ζ by ordering S − s units of the item, since then pt (s |s, S − s) = Pr(Xt = S − s ) ≥ ζ.
7.1.1. Comparisons to Existing Inventory Control Models We ﬁrst compare our setting and existing ones on single non-perishable item inventory control problem with lost sales.
Similar to (Huh and Rusmevichientong 2009, Zhang et al. 2018, Yuan et al. 2019, Agrawal and Jia 2019), the model presented in this section studies the single non-perishable item inventory control problem with lost sales. However, there are several key diﬀerences between ours and the existing works in terms of cost functions, demand distributions, and lead time: • Cost Functions: In (Huh and Rusmevichientong 2009), the authors assume a linear purchasing
cost function without ﬁxed cost, linear lost sales and holding cost functions. In (Yuan et al. 2019), the authors additionally allow ﬁxed cost. In (Zhang et al. 2018, Agrawal and Jia 2019), the authors assume the lost sales cost function and the holding cost function are linear, and there is no purchasing cost. In our setting, our cost function is the same as that of (Yuan et al. 2019). • Demand Distributions: In (Huh and Rusmevichientong 2009, Zhang et al. 2018, Yuan et al. 2019, Agrawal and Jia 2019), the authors assume stationary demand distributions, but they admit both continuous or discrete demand distributions. In contrast, we allow non-stationary demand distributions, but we impose that the demand distribution has to be discrete, and satisﬁes the strictly positive PMF assumption described above. • Lead Time: In (Zhang et al. 2018, Agrawal and Jia 2019), the authors allow the lead time to be positive; while in (Huh and Rusmevichientong 2009, Yuan et al. 2019) and our setting, we assume the lead time is zero. A summary of the comparisons is provided in Table 3.

24

Cost functions

Demand distributions Lead time

linear purchasing cost

Huh and Rusmevichientong without ﬁxed cost, lin- stationary, continuous zero

(2009)

ear lost sales and hold- or discrete

ing cost

Zhang et al. (2018), Agrawal no purchasing cost, lin- stationary,

and Jia (2019)

ear lost sales and hold- or discrete

ing cost

continuous positive

linear purchasing cost

Yuan et al. (2019)

with ﬁxed cost, linear stationary, continuous zeros lost sales and holding or discrete

cost

Ours

linear purchasing cost non-stationary, discrete, with ﬁxed cost, linear with strictly positive zero lost sales and holding PMF cost

Table 3 Comparisons between our inventory control model and existing works’

7.1.2. Implementation of the SWUCRL2-CW algorithm As pointed out in Section 7.1, diﬀerent than the model we present in Section 2, the reward in each time step t is not directly observable due to the censored demand. Nevertheless, we can follow the pseudo-reward technique proposed in (Agrawal and Jia 2019) to implement the SWUCRL2-CW algorithm on a sequence of suitably designed pseudo-reward distributions.
In particular, we deﬁne the pseudo-reward following (Agrawal and Jia 2019) for each time step t ∈ [T ], every state s, and every action a ∈ As as

Rtpseudo(s, a) := Rt(s, a) + l · Xt = −f · 1[a > 0] − c · at − h · [s + a − Yt]+ + l · Yt,

where we recall Yt = min{s + a, Xt} is the censored demand. We note that the pseudo-reward is perfectly observable. We also deﬁne the mean pseudo-reward or each time step t ∈ [T ], every state s, and every action a ∈ As as

rtpseudo(s, a) := E Rtpseudo(s, a) = E [Rt(s, a) + l · Xt] = rt(s, a) + l · E[Xt].

(14)

This indicates regardless of state and action, the mean pseudo-reward of a time step t can be

obtained from shifting the corresponding mean reward uniformly by l · E[Xt]. Without loss of gen-

erality, we assume for all t ∈ [T ], s ∈ S, and a ∈ As, the mean pseudo-reward is bounded, i.e., rtpseudo(s, a) ∈ [0, 1], and the pseudo-reward Rtpseudo(s, a) is 1-sub-Gaussian with mean rtpseudo(s, a). Deﬁning ρ∗t pseudo as the optimal long-term average reward of the stationary MDP with state transition distribution pt and mean reward rtpseudo = {rtpseudo(s, a)}s∈S,a∈As , we can show that for any

policy Π, the dynamic regret of the non-stationary MDP instance speciﬁed by the tuple M =

(S, A, T, r, p) and the dynamic regret of the non-stationary MDP instance speciﬁed by the tuple

Mpseudo

=

(S, A, T, rpseudo

=

{r

pseudo t

}Tt=1

,

p

)

are

the

same.

25

Proposition 6. For any policy Π, we denote the sample path for following Π

on M as {st(M), at(M)}Tt=1, and the sample path for following Π on Mpseudo as

{

st

(

M

pseudo

)

,

a

t

(M

pseudo

)

}

T t=1

,

we

have

T

T

{ρ∗t − E[rt(st(M), at(M))]} =

ρ∗t pseudo − E[rtpseudo(st(Mpseudo), at(Mpseudo))] .

t=1

t=1

The proof of Proposition 6 is provided in Section G in the appendix. Together with Theorem 3,

we have the following dynamic regret bound guarantee for the SWUCRL2-CW algorithm on the the single non-perishable item inventory control problem with zero lead time, ﬁxed cost, and lost sales.

Theorem 4. For the inventory control model in Section 7.1, under Assumption 3 and assuming S > 1, the SWUCRL2-CW algorithm with window size W, conﬁdence widening parameter η = 0, and δ = T −1 satisﬁes the dynamic regret bound
Dyn-RegT (SWUCRL2-CW) = O˜ BrW + D¯ BpW + S√23WT + SW2T + √T
If we further put W = W ∗ = ST 2/3(Br + Bp + 1)−2/3, this dynamic regret bound is O˜ D¯ (Br + Bp + 1)1/3ST 2/3 .
Remark 11. To interpret the dynamic regret bound of the SWUCRL2-CW algorithm in the context of inventory control, we note that in Theorem 4, we normalize the cost functions so that the cost incurs in each time period is in [0, 1]. This is slightly diﬀerent than the setups in (Huh and Rusmevichientong 2009, Zhang et al. 2018, Yuan et al. 2019, Agrawal and Jia 2019), where the upper bound of the cost functions are of order O(S).
8. Numerical Experiments
As a complement to our theoretical results, we conduct numerical experiments on synthetic datasets to compare the dynamic regret performances of our algorithms with the UCRL2 algorithm (Jaksch et al. 2010), which is one of the most widely used benchmarks for RL in MDPs due to its nearlyoptimal regret bound in stationary environments (Wei et al. 2019), and also the restarting UCRL2 (denoted as UCRL2.S) algorithm for RL in piecewise-stationary MDPs (Jaksch et al. 2010) Setup: We consider a MDP with 2 states {s1, s2} and 2 actions {a1, a2}, and set T = 5000. The rewards are deterministically set to
rt(s1, a1) = 0.2 + 3 cos (5Vrπt/T ) , rt(s1, a2) = 0.2 + cos (5Vrπt/T ) , rt(s2, a1) = 0.2 − cos (5Vrπt/T ) , rt(s2, a2) = 0.2 − 3 cos (5Vrπt/T ) .

26

1.0

0.5

Rewards

0.0

0.5

Figure 4

0 1000 2000 3000 4000 5000 Time step
Illustrations of mean rewards rt(s2, a2) (the mean rewards of other state-action pairs are similar)

The total variations in mean rewards is thus Br = 15Vr = Θ(Vr). An illustration of the reward process of state s2 and action a2 is provided in Fig. 4 (the mean rewards of other (state,action) pairs are similar). The state transition distributions are set to

pt(s1|s1, a1) = 1, pt(s1|s2, a1) = 0,

pt(s2|s1, a1) = 0, pt(s2|s2, a1) = 1,

pt(s1|s1, a2) = 1 − βt, pt(s1|s1, a2) = βt, pt(s1|s2, a2) = βt, pt(s1|s2, a2) = 1 − βt.

where βt is governed by the following process:

βt = 0.5 + 0.3 sin (5Vpπt/T ) .

The total variations in the state transition distributions is thus Bp = 12Vp = Θ(Vp). In this simulation, we allow both Vr and Vp to take values from {T 0.2, T 0.5} to evaluate the performances of the algorithms in low and high variations scenarios. Here, we assume the SWUCRL2-CW algorithm knows the variation budgets, and the UCRL2.S algorithm restarts the UCRL2 algorithm every T 2/3 time steps. All the results are averaged over 50 runs. Results: The cumulative rewards of the algorithms under various variation budgets are shown in Fig. 5. The results show that both the SWUCRL2-CW algorithm and the BORL algorithm are able to collect at least 20% more rewards then the UCRL2 algorithm and the UCRL2.S algorithm except for the case when Bp = Θ(T 0.5) and Br = Θ(T 0.2), the percentage improvement is 12%. Comparing the results in Figs. 5(a), 5(b), and 5(c), we can see that both the SWUCRL2-CW algorithm and the BORL algorithm are more robust to variations in the state transition distributions than that in reward distributions. This demonstrate the power of our conﬁdence widening technique. Interestingly, we can see that in Figs. 5(a), 5(b), and 5(c), the cumulative rewards of the BORL algorithm (does not know the variation budgets) are higher than those of the SWUCRL2-CW algorithm (knows the

27

Cumulative rewards

3000 SWUCRL2-CW

2500

BORL

2000

UCRL2

UCRL2.S

1500

1000

500

0 0 1000 2000 3000 4000 5000 Time step

(a) Bp = Θ(T 0.2), Br = Θ(T 0.2)

Cumulative rewards

1500

SWUCRL2-CW

1250

BORL

UCRL2

1000

UCRL2.S

750

500

250

0 0 1000 2000 3000 4000 5000 Time step

(b) Bp = Θ(T 0.2), Br = Θ(T 0.5)

Cumulative rewards

Cumulative rewards

3000

SWUCRL2-CW

2500

BORL

2000

UCRL2

UCRL2.S

1500

SWUCRL2-CW

1500

BORL

UCRL2

1000

UCRL2.S

1000

500

500

0 0 1000 2000 3000 4000 5000 Time step

0 0 1000 2000 3000 4000 5000 Time step

(c) Bp = Θ(T 0.5), Br = Θ(T 0.2)

(d) Bp = Θ(T 0.5),

Figure 5 Cumulative rewards of the algorithms

Br = Θ(T 0.5)

variation budgets). This indeed has no contradiction to our theoretical results. Theorems 1 and 2 state that the SWUCRL2-CW algorithm and the BORL algorithm enjoy the same (in the sense of O˜(·)) worst case dynamic regret bound. Nevertheless, the environments we construct in Fig. 4 are not the worst case scenario, and the results indicate that the adaptive master algorithm (i.e., the EXP3.P algorithm) of the BORL algorithm is able to leverage this more benign environment to attain higher rewards.
9. Conclusion
In this paper, we study the problem of un-discounted reinforcement learning in a gradually changing environment. In this setting, the parameters, i.e., the reward and state transition distributions, can be diﬀerent from time to time as long as the total changes are bounded by some variation budgets, respectively. We ﬁrst incorporate the sliding window estimator and the novel conﬁdence widening

28
technique into the UCRL2 algorithm to propose a SWUCRL2-CW algorithm with low dynamic regret when the variation budgets are known. We then design a parameter-free BORL algorithm that allows us to enjoy the same dynamic regret bound as the SWUCRL2-CW algorithm without knowing the variation budgets. The main ingredient of the proposed algorithms is the novel conﬁdence widening technique, which injects extra optimism into the design of learning algorithms, and thus ensure low dynamic regret bounds. This is in contrast to the widely held believe that optimistic exploration algorithms for (stationary and non-stationary) stochastic online learning settings should employ the lowest possible level of optimism. To extend this ﬁnding, we also use the problem of singleitem inventory control with ﬁxed cost as an example to demonstrate how one can leverage special structures in the state transition distributions to attain low dynamic regret bound without widening the conﬁdence region.
Acknowledgments
The authors would like to express sincere gratitude to Dylan Foster, Negin Golrezaei, and Mengdi Wang, as well as various seminar attendees for helpful discussions and comments.
References
Abbasi-Yadkori, Yasin, Peter L Bartlett, Varun Kanade, Yevgeny Seldin, Csaba Szepesv´ari. 2013. Online learning in markov decision processes with adversarially chosen transition probability distributions. Advances in Neural Information Processing Systems 26 (NIPS).
Abbasi-Yadkori, Yasin, David P´al, Csaba. Szepesv´ari. 2011. Improved algorithms for linear stochastic bandits. Advances Neural Information Processing Systems 25 (NIPS).
Abernethy, Jacob, Kareem Amin, Ruihao Zhu. 2016. Threshold bandits, with and without censored feedback. Advances in Neural Information Processing Systems 29 (NIPS).
Agrawal, Shipra, Randy Jia. 2017. Optimistic posterior sampling for reinforcement learning: worst-case regret bounds. Advances in Neural Information Processing Systems 30 (NIPS). Curran Associates, Inc., 1184–1194.
Agrawal, Shipra, Randy Jia. 2019. Learning in structured mdps with convex cost functions: Improved regret bounds for inventory management. Proceedings of the ACM Conference on Economics and Computation (EC).
Arora, Raman, Ofer Dekel, Ambuj Tewari. 2012. Deterministic mdps with adversarial rewards and bandit feedback. Proceedings of the Twenty-Eighth Conference on Uncertainty in Artiﬁcial Intelligence. 93– 101.
Auer, P., N. Cesa-Bianchi, Y. Freund, R. Schapire. 2002a. The nonstochastic multiarmed bandit problem. SIAM Journal on Computing, 2002, Vol. 32, No. 1 : pp. 48–77 .

29
Auer, Peter, Nicolo Cesa-Bianchi, Paul Fischer. 2002b. Finite-time analysis of the multiarmed bandit problem. Machine learning, 47, 235–256 .
Balseiro, Santiago, Yonatan Gur. 2019. Learning in repeated auctions with budgets: Regret minimization and equilibrium. Management Science.
Bartlett, Peter L., Ambuj Tewari. 2009. REGAL: A regularization based algorithm for reinforcement learning in weakly communicating mdps. UAI 2009, Proceedings of the Twenty-Fifth Conference on Uncertainty in Artiﬁcial Intelligence, Montreal, QC, Canada, June 18-21, 2009 . 35–42.
Bertsekas, Dimitri. 2017. Dynamic Programming and Optimal Control . Athena Scientiﬁc. Besbes, Omar, Yonatan Gur, Assaf Zeevi. 2014. Stochastic multi-armed bandit with non-stationary rewards.
Advances in Neural Information Processing Systems 27 (NIPS). Bubeck, S., N. Cesa-Bianchi. 2012. Regret Analysis of Stochastic and Nonstochastic Multi-armed Bandit
Problems. Foundations and Trends in Machine Learning, 2012, Vol. 5, No. 1: pp. 1–122. Burnetas, Apostolos N., Michael N. Katehakis. 1997. Optimal adaptive policies for markov decision processes.
Mathematics of Operations Research, vol. 22. 222–255. Cai, Han, Kan Ren, Weinan Zhang, Kleanthis Malialis, Jun Wang, Yong Yu, Defeng Guo. 2017. Real-
time bidding by reinforcement learning in display advertising. Proceedings of the ACM International Conference on Web Search and Data Mining (WSDM). Cardoso, Adrian Rivera, He Wang, Huan Xu. 2019. Large scale markov decision processes with changing rewards. Advances in Neural Information Processing Systems 32 (NeurIPS). Chen, Weidong, Cong Shi, Izak Duenyas. 2019a. Optimal learning algorithms for stochastic inventory systems with random capacities. SSRN Preprint 3287560 . Chen, Yifang, Chung-Wei Lee, Haipeng Luo, Chen-Yu Wei. 2019b. A new algorithm for non-stationary contextual bandits: Eﬃcient, optimal, and parameter-free. Proceedings of Conference on Learning Theory (COLT). Cheung, Wang Chi, David Simchi-Levi, Ruihao Zhu. 2019a. Hedging the drift: Learning to optimize under non-stationarity. arXiv:1903.01461 . URL https://arxiv.org/abs/1903.01461. Cheung, Wang Chi, David Simchi-Levi, Ruihao Zhu. 2019b. Learning to optimize under non-stationarity. Proceedings of International Conference on Artiﬁcial Intelligence and Statistics (AISTATS). Cormen, Thomas H., Charles E. Leiserson, Ronald L. Rivest, Cliﬀord Stein. 2009. Introduction to algorithms. MIT Press. Dick, Travis, Andr´as Gy¨orgy, Csaba Szepesv´ari. 2014. Online learning in markov decision processes with changing cost sequences. Proceedings of the International Conference on Machine Learning (ICML). Even-Dar, Eyal, Sham M Kakade, , Yishay Mansour. 2005. Experts in a markov decision process. Advances in Neural Information Processing Systems 18 (NIPS).

30

Flajolet, Arthur, Patrick Jaillet. 2017. Real-time bidding with side information. Advances in Neural Information Processing Systems 30 (NeurIPS).

Fruit, Ronan, Matteo Pirotta, Alessandro Lazaric. 2018a. Near optimal exploration-exploitation in noncommunicating markov decision processes. Advances in Neural Information Processing Systems 31 . Curran Associates, Inc., 2998–3008.

Fruit, Ronan, Matteo Pirotta, Alessandro Lazaric. 2019. https://rlgammazero.github.io/ .

Improved analysis of ucrl2b.

Fruit, Ronan, Matteo Pirotta, Alessandro Lazaric, Ronald Ortner. 2018b. Eﬃcient bias-span-constrained exploration-exploitation in reinforcement learning. Proceedings of the 35th International Conference on Machine Learning, Proceedings of Machine Learning Research, vol. 80. PMLR, 1578–1586.

Gajane, Pratik, Ronald Ortner, Peter Auer. 2018. A sliding-window algorithm for markov decision processes with arbitrarily changing rewards and transitions.

Garivier, Aur´elien, Eric Moulines. 2011. On upper-conﬁdence bound policies for switching bandit problems. Algorithmic Learning Theory. Springer Berlin Heidelberg, 174–188.

Google. 2011. The arrival of real-time bidding. URL https://www.rtbchina.com/wp-content/uploads/ 2012/03/Google-White-Paper-The-Arrival-of-Real-Time-Bidding-July-2011.pdf.

Guo, Xin, Anran Hu, Renyuan Xu, Junzi Zhang. 2019. Learning mean-ﬁeld games. Advances in Neural Information Processing Systems 32 (NeurIPS).

Hoeﬀding, Wassily. 1963. Probability inequalities for sums of bounded random variables. Journal of the American statistical association, vol. 58. Taylor & Francis Group, 13–30.

Huh, Woonghee Tim, Paat Rusmevichientong. 2009. A nonparametric asymptotic analysis of inventory planning with censored demand. Mathematics of Operations Research.

Jaksch, Thomas, Ronald Ortner, Peter Auer. 2010. Near-optimal regret bounds for reinforcement learning. J. Mach. Learn. Res., vol. 11. JMLR.org, 1563–1600.

Jin, Chi, Tiancheng Jin, Haipeng Luo, Suvrit Sra, Tiancheng Yu. 2019. Learning adversarial markov decision processes with bandit feedback and unknown transition.

Karnin, Z., O. Anava. 2016. Multi-armed bandits: Competing with optimal sequences. Advances in Neural Information Processing Systems 29 (NIPS).

Keskin, N., A. Zeevi. 2016. Chasing demand: Learning and earning in a changing environments. Mathematics of Operations Research, 2016, 42(2), 277–307 .

Kim, Michael Jong, Andrew E.B. Lim. 2016. Robust multiarmed bandit problems. Management Science.

Kiss, Istvan Z., Joel C. Miller, Peter L. Simon. 2017. Mathematics of epidemics on networks. Springer .

Kveton, Branislav, Zheng Wen, Azin Ashkan, Csaba Szepesv´ari. 2015. Tight regret bounds for stochastic combinatorial semi-bandits. AISTATS .

31
Lattimore, T., C. Szepesv´ari. 2018. Bandit Algorithms. Cambridge University Press. Li, Yingying, Aoxiao Zhong, Guannan Qu, Na Li. 2019. Online markov decision processes with time-varying
transition probabilities and rewards. ICML workshop on Real-world Sequential Decision Making. Luo, H., C. Wei, A. Agarwal, J. Langford. 2018. Eﬃcient contextual bandits in non-stationary worlds.
Proceedings of Conference on Learning Theory (COLT). Ma, Will. 2018. Improvements and generalizations of stochastic knapsack and markovian bandits approxi-
mation algorithms. Mathematics of Operations Research. Manheim. 2020. Online. URL https://www.manheim.com/. [Last accessed February 10, 2020]. Neu, Gergely, Andras Antos, Andr´as Gyo¨rgy, Csaba Szepesv´ari. 2010. Online markov decision processes
under bandit feedback. Advances in Neural Information Processing Systems 23 (NIPS). 1804–1812. Neu, Gergely, Andras Gyorgy, Csaba Szepesvari. 2012. The adversarial stochastic shortest path problem with
unknown transition probabilities. Proceedings of the Fifteenth International Conference on Artiﬁcial Intelligence and Statistics, vol. 22. PMLR, 805–813. Nowzari, Cameron, Victor M. Preciado, George J. Pappas. 2016. Analysis and control of epidemics: A survey of spreading processes on complex networks. IEEE Control Systems Magazine. Ortner, Ronald, Pratik Gajane, Peter Auer. 2019. Variational regret bounds for reinforcement learning. Proceedings of Conference on Uncertainty in Artiﬁcial Intelligence (UAI). Puterman, Martin L. 1994. Markov Decision Processes: Discrete Stochastic Dynamic Programming. 1st ed. John Wiley & Sons, Inc., New York, NY, USA. Qin, Zhiwei (Tony), Jian Tang, Jieping Ye. 2019. Deep reinforcement learning with applications in transportation. Tutorial of the 33rd AAAI Conference on Artiﬁcial Intelligence (AAAI-19). Rosenberg, Aviv, Yishay Mansour. 2019. Online convex optimization in adversarial Markov decision processes. Proceedings of the 36th International Conference on Machine Learning, vol. 97. PMLR, 5478– 5486. Shortreed, Susan, Eric Laber, Daniel Lizotte, Scott Stroup, Joelle Pineau Susan Murphy. 2010. Informing sequential clinical decision-making through reinforcement learning: an empirical study. Machine Learning . Sidford, Aaron, Mengdi Wang, Xian Wu, Lin F. Yang, Yinyu Ye. 2018a. Near-optimal time and sample complexities for solving discounted markov decision process with a generative model. Advances in Neural Information Processing Systems 31 (NeurIPS). Sidford, Aaron, Mengdi Wang, Xian Wu, Yinyu Ye. 2018b. Variance reduced value iteration and faster algorithms for solving markov decision processes. Proceedings of the Annual ACM-SIAM Symposium on Discrete Algorithms (SODA). Sutton, Richard S., Andrew G. Barto. 2018. Reinforcement Learning: An Introduction. A Bradford Book.

32
Tewari, Ambuj, Peter L. Bartlett. 2008. Optimistic linear programming gives logarithmic regret for irreducible mdps. Advances in Neural Information Processing Systems 20 (NIPS). 1505–1512.
Vehicle Remarketing. 2020. Online. URL https://www.vehicleremarket.com/. [Last accessed February 10, 2020].
Wang, Mengdi. 2019. Randomized linear programming solves the markov decision problem in nearly-linear (sometimes sublinear) running time. Mathematics of Operations Research.
Wei, Chen-Yu, Mehdi Jafarnia-Jahromi, Haipeng Luo, Hiteshi Sharma, Rahul Jain. 2019. Model-free reinforcement learning in inﬁnite-horizon average-reward markov decision processes. arXiv:1910.07072 .
Xu, Kuang, Se-Young Yun. 2020. Reinforcement with fading memories. Mathematics of Operations Research. Yu, Jia Yuan, Shie Mannor. 2009. Online learning in markov decision processes with arbitrarily changing
rewards and transitions. Proceedings of the International Conference on Game Theory for Networks. Yu, Jia Yuan, Shie Mannor, Nahum Shimkin. 2009. Markov decision processes with arbitrary reward pro-
cesses. Mathematics of Operations Research, vol. 34. 737–757. Yuan, Hao, Qi Luo, Cong Shi. 2019. Marrying stochastic gradient descent with bandits: Learning algorithms
for inventory systems with ﬁxed costs. SSRN Preprint 3329611 . Zhang, Anru, Mengdi Wang. 2018. Spectral state compression of markov processes. arXiv:1802.02920 . Zhang, Huanan, Xiuli Chao, Cong Shi. 2018. Closing the gap: A learning algorithm for the lost-sales inventory
system with lead times. Management Science. Zhang, Zihan, Xiangyang Ji. 2019. Regret minimization for reinforcement learning by evaluating the optimal
bias function. Advances in Neural Information Processing Systems 32 (NeurIPS). Zhou, Xiang, Ningyuan Chen, Xuefeng Gao, Yi Xiong. 2020. Regime switching bandits. arXiv:2001.09390 . Zhou, Zhengyuan, Nicholas Bambos. 2015. Wireless communications games in ﬁxed and random environ-
ments. IEEE Conference on Decision and Control (CDC). Zhou, Zhengyuan, Peter Glynn, Nicholas Bambos. 2016. Repeated games for power control in wireless
communications: Equilibrium and regret. IEEE Conference on Decision and Control (CDC).

33

Appendix. Supplementary
A. Supplementary Details about MDPs A.1. Linear Program Formulations The optimal long term reward ρ∗t is equal to the optimal value of the linear program P(rt, pt) (Puterman 1994). For a reward vector r and a transition distribution p, we deﬁne

P(r, p) : max

r(s, a)x(s, a)

s∈S ,a∈As

s.t. x(s, a) =

p(s|s , a )x(s , a )

a∈As

s ∈S,a ∈As

x(s, a) = 1

s∈S ,a∈As
x(s, a) ≥ 0

(15) ∀s ∈ S
∀s ∈ S, a ∈ As

Throughout our analysis, it is useful to consider the following dual formulation D(r, p) of the optimization problem P(r, p):

D(r, p) : min ρ
s.t. ρ + γ(s) ≥ r(s, a) + p(s |s, a)γ(s )
s ∈S
φ, γ(s) free

(16) ∀s ∈ S, a ∈ As ∀s ∈ S.

The following Lemma shows that any feasible solution to D(r, p) is essentially bounded if the underlying MDP is communicating, which will be crucial in the subsequent analysis.
Lemma 2. Let (ρ, γ) be a feasible solution to the dual problem D(r, p), where (S, A, p) consititute a communicating MDP with diameter D. We have

max {γ(s) − γ(s )} ≤ 2D.
s,s ∈S

The Lemma is extracted from Section 4.3.1 of (Jaksch et al. 2010), and it is more general than (Lattimore and Szepesv´ari 2018), which requires (ρ, γ) to be optimal instead of just feasible.

A.2. Proof of Proposition 1

We

begin

with

invoking

Lemma

2,

which

guarantees

that

for

each

t

there

is

an

optimal

solution

(

ρ

∗ t

,

γt∗

)

of

D(rt, pt) that satisﬁes 0 ≤ γt∗(s) ≤ 2Dmax for all s ∈ S. Recall for each t:

Br,t = max |rt+1(s, a) − rt(s, a)| , Bp,t = max pt+1(·|s, a) − pt(·|s, a) 1 .

s∈S ,a∈As

s∈S ,a∈As

Consider two time indexes t ≤ τ . We ﬁrst claim the following two inequalities:

τ −1
ρ∗τ ≥ ρ∗t − (Br,q + 2DmaxBp,q)
q=t

ρ∗t ≥ rτ (sτ , aτ ) +

pτ (s |sτ , aτ )γt∗(s ) − γt∗(sτ )

s ∈S

τ −1
− (Br,q + 2DmaxBp,q) .
q=t

(17)
(18) (19)

34

The proofs of inequalities (18, 19) are deferred to the end. Now, combining (18, 19) gives

ρ∗τ ≥ rτ (sτ , aτ ) +

pτ (s |sτ , aτ )γt∗(s ) − γt∗(sτ )
s ∈S

τ −1
− 2 (Br,q + 2DmaxBp,q) .
q=t

(20)

Let positive integer W ≤ T be a window size, which is speciﬁed later. Summing (20) over τ = t, . . . , t + W − 1

and

taking

expectation

over

{

(sτ

,

aτ

)}

t+W τ =t

−1

yield

t−W +1

t−W +1

t−W

ρ∗τ ≥ E

rτ (sτ , aτ ) + E

pτ (s |sτ , aτ )γt∗(s ) − γt∗(sτ+1)

τ =t

τ =t

τ =t

t−W +1 τ −1

+E

pt−W +1(s |st−W +1, at−W +1)γt∗(s ) − γt∗(st) − 2

(Br,q + 2DmaxBp,q)

s ∈S

τ =t q=t

t−W +1

t+W −1

≥E

rτ (sτ , aτ ) − 2Dmax − 2W

(Br,q + 2DmaxBp,q) .

τ =t

q=t

(21) (22) (23)

To arrive at (23), note that the second expectation in (21), which is a telescoping sum, is equal to 0, since sτ+1 is distributed as p(·|sτ , aτ ). In addition, we trivially lower bound the ﬁrst expectation in (22) by −2Dmax by applying Lemma 2. Next, consider partitioning the horizon of T steps into intervals of W time steps, where last interval could have less than W time steps. That is, the ﬁrst interval is {1, . . . , W }, the second is {W + 1, . . . , 2W }, and so on. Applying the bound (23) on each interval and summing the resulting bounds together give

T
ρ∗t ≥ E
t=1
≥E

T
rt(st, at)
t=1 T
rt(st, at)
t=1

T

T

− 2 W Dmax − 2W (Br,t + 2DmaxBp,t)

t=1

− 4T Dmax − 2W (Br + 2DmaxBp). W

(24)

Choosing W to be any integer in [ T /(Br + 2DmaxBp), 2 T /(Br + 2DmaxBp)] yields the desired inequality in the Theorem. Finally, we go back to proving inequalities (18,19). These inequalities are clearly true when t = τ , so we focus on the case t < τ .
Proving inequality (18). It suﬃces to show that the solution (ρ∗τ + qτ=−t1(Br,q + 2DmaxBp,q), γτ∗) is feasible to the linear program D(rt, pt). To see the feasibility, it suﬃces to check the constraint of D(rt, pt) for each state-action pair s, a:

τ −1
ρ∗τ + (Br,q + 2DmaxBp,q)

q=t

τ −1

≥ rτ (s, a) +

Br,q

+

−

γ

∗ τ

(

s)

+

τ −1
pτ (s |s, a)γτ∗(s ) + 2DmaxBp,q .

q=t

s ∈S

q=t

The feasibility is proved by noting that

τ −1
|rτ (s, a) − rt(s, a)| ≤ Br,q,
q=t

(25)

pτ (s |s, a)γτ∗(s ) − pt(s |s, a)γτ∗(s ) ≤ pτ (·|s, a) − pt(·|s, a) 1 γτ∗ ∞

s ∈S

s ∈S

35

τ −1
≤ Bp,q(2Dmax).
q=t

(26)

Proving inequality (19). We have

ρ∗t ≥ rt(sτ , aτ ) + pt(s |sτ , aτ )γt∗(s ) − γt∗(sτ )

s ∈S

τ −1

≥ rτ (sτ , aτ ) + pt(s |sτ , aτ )γt∗(s ) − γt∗(sτ ) − Br,s

s ∈S

s=t

τ −1

τ −1

≥ rτ (sτ , aτ ) + pτ (s |sτ , aτ )γt∗(s ) − γt∗(sτ ) − Br,s − 2Dmax Bp,s,

s ∈S

s=t

s=t

where steps (27, 28) are by inequalities (25, 26). Altogether, the Proposition is proved.

(27) (28)

A.3. Extended Value Iteration (EVI) by (Jaksch et al. 2010)

Algorithm 3 EVI(Hr, Hp; ), mostly extracted from (Jaksch et al. 2010) 1: Initialize VI record u0 ∈ RS as u0(s) = 0 for all s ∈ S. 2: for i = 0, 1, . . . do 3: For each s ∈ S, compute VI record ui+1(s) = maxa∈As Υ˜ i(s, a), where

Υ˜ i(s, a) = max {r˙(s, a)} + max

r˙(s,a)∈Hr (s,a)

p˙ ∈H p (s,a)

ui(s )p˙(s ) .

s ∈S

4: Deﬁne stationary policy π˜ : S → As as π˜(s) = argmaxa∈AsΥ˜ i(s, a).

5: Deﬁne optimistic reward r˜ = {r˜(s, a)}s,a with r˜(s, a) ∈ argmax {r˙(s, a)}.
r˙(s,a)∈Hr (s,a)
6: Deﬁne optimistic distribution p˜ = {p˜(·|s, a)}s,a with p˜(·|s, a) ∈ argmax s ∈S ui(s )p˙(s ) . p˙ ∈H p (s,a)
7: Deﬁne optimistic dual variables ρ˜ = maxs∈S {ui+1(s) − ui(s)}, γ˜(s) = ui(s) − mins∈S ui(s).

8: if maxs∈S {ui+1(s) − ui(s)} − mins∈S {ui+1(s) − ui(s)} ≤ then

9:

Break the for loop.

10: end if

11: end for

12: Return policy π˜.

13: Auxiliary output: optimistic reward and state transition distributions (r˜, p˜), optimistic dual

variables (ρ˜, γ˜).

We provide the pseudo-codes of EVI(Hr, Hp; ) proposed by (Jaksch et al. 2010) in Algorithm 3. By (Jaksch et al. 2010), the algorithm converges in ﬁnite time when the conﬁdence region Hp contains a transition distribution p such that (S, A, p) constitutes a communicating MDP. The output (π˜, r˜, p˜, ρ˜, γ˜) of the EVI(Hr, Hp; ) satisﬁes the following two properties (Jaksch et al. 2010).

36

Property 1 The dual variables (ρ˜, γ˜) are optimistic, i.e.,

ρ˜+ γ˜(s) ≥ max {r˙(s, a)} + γ˜(s ) max {p˙(s |s, a)}.

r˙(s,a)∈Hr (s,a)

s ∈S

p˙∈Hp (s,a)

Property 2 For each state s ∈ S, we have

r˜(s, π˜(s)) ≥ ρ˜+ γ˜(s) − p˜(s |s, π˜(s))γ˜(s ) − .
s ∈S
Property 1 ensures the feasibility of the output dual variables (ρ˜, γ˜), with respect to the dual program D(r˙, p˙) for any r˙, p˙ in the conﬁdence regions Hr, Hp. The feasibility facilitates the bounding of maxs∈S γ˜(s), which turns out to be useful for bounding the regret arise from switching among diﬀerent stationary policies. To illustrate, suppose that Hp is so large that it contains a transition distribution p˙ under which (S, A, p˙) has diameter D. By Lemma 2, we have 0 ≤ maxs∈S γ˜(s) ≤ 2D.
Property 2 ensures the near-optimality of the dual variables (ρ˜, γ˜) to the (r˜, p˜) optimistically chosen from Hr, Hp. More precisely, the deterministic policy π˜ near-optimal for the MDP with time homogeneous reward function r˜ and time homogeneous transition distribution p˜, under which the policy π˜ achieves a long term average reward is at least ρ˜∗ − . B. Proof of Lemma 1
We employ the self-normalizing concentration inequallity (Abbasi-Yadkori et al. 2011). The following inequality is extracted from Theorem 1 in (Abbasi-Yadkori et al. 2011), restricted to the case when d = 1.

Proposition 7 ((Abbasi-Yadkori et al. 2011)). Let {Fq}Tq=1 be a ﬁltration. Let {ξq}Tq=1 be a real-

valued stochastic process, such that ξq is Fq-measurable, and ξq is conditionally R-sub-Gaussian, i.e. for all λ ≥ 0, it holds that E[exp(λξq)|Fq−1] ≤ exp(λ2R2/2). Let {Yq}Tq=1 be a non-negative real-valued stochastic

process such that Yq is Fq−1-measurable. For any δ ∈ (0, 1), it holds that

t q=1

ξq

Yq

log(T /δ )

Pr max{1, tq=1 Yq2} ≤ 2R max{1, tq=1 Yq2} for all t ∈ [T ] ≥ 1 − δ .

In particular, if {Yq}Tq=1 be a {0, 1}-valued stochastic process, then for any δ ∈ (0, 1), it holds that

t q=1

ξq

Yq

log(T /δ )

Pr max{1, tq=1 Yq} ≤ 2R max{1, tq=1 Yq} for all t ∈ [T ] ≥ 1 − δ . (29)

The Lemma is proved by applying Proposition 7 with suiatable choices of FqT=1, {ξq}Tq=1, {Yq}Tq=1, δ. We divide

the proof into two parts.

B.1. Proving Pr[Er] ≥ 1 − δ/2 It suﬃces to prove that, for any ﬁxed s ∈ S, a ∈ As, t ∈ [T ], it holds that

Pr rˆ(s, a) − r¯t(s, a) ≤ rad-r,t(s, a)



1

t−1

= Pr  Nt+(s, a) q=(τ(m)−W )∨1 [Rq(s, a) − rq(s, a)] · 1(sq = s, aq = a) ≤ 2



log(2SAT 2/δ)

N

+ t

(s,

a)



δ

≥1 −

.

(30)

2SAT

37

since then Pr[Er] ≥ 1 − δ/2 follows from the union bound over all s ∈ S, a ∈ As, t ∈ [T ]. Now, the trajectory

of

the

online

algorithm

is

expressed

as

{sq

,

aq

,

Rq

}

T q=1

.

Inequality

(30)

directly

follows

from

Proposition

7,

with {Fq}Tq=1, {ξq}Tq=1, {Yq}Tq=1, δ deﬁned as

Fq = {(s , a , R )}q=1 ∪ {(sq+1, aq+1)},

ξq = Rq(s, a) − rq(s, a),

Yq = 1 (sq = s, aq = a, ((t − W ) ∨ 1) ≤ q ≤ t − 1) ,

δ

δ=

.

2SAT

Each ξq is conditionally 2-sub-Gaussian, since −1 ≤ ξq ≤ 1 with certainty. Altogether, the required inequality is shown.

B.2. Proving Pr[Ep] ≥ 1 − δ/2 We start by noting that, for two probability distributions p, {p(s)}s∈S, p = {p (s)}s∈S on S, it holds that

p − p 1 = θ∈{m−a1,x1}S θ(s) · (p(s) − p (s)). Consequently, to show Pr[Ep] ≥ 1−δ/2, it suﬃces to show that, for any ﬁxed s ∈ S, a ∈ As, t ∈ [T ], θ ∈ {−1, 1}S, it holds that

Pr

θ(s) · (pˆt(s |s, a) − p¯t(s |s, a)) ≤ rad-p,t(s, a)

s ∈S



1

t−1

≤ Pr  Nt+(s, a) q=(τ(m)−W )∨1

θ(s )1(sq = s, aq = a, sq+1 = s )
s ∈S

−

θ(s )pq(s |s, a) · 1(sq = s, aq = a) ≤ 2

s ∈S

log(2SAT 22S/δ) Nt+(s, a)

δ

≥1 −

,

(31)

2SAT 2S

since then the required inequality follows from a union bound over all s ∈ S, a ∈ As, t ∈ [T ], θ ∈ {−1.1}S. Similar to the casea of Er, (31) follows from Proposition 7, with {Fq}Tq=1, {ξq}Tq=1, {Yq}Tq=1, δ deﬁned as

Fq = {(s , a )}q=+11,

ξq =

θ(s )1(sq = s, aq = a, sq+1 = s ) −

θ(s )pq(s |s, a) ,

s ∈S

s ∈S

Yq = 1 (sq = s, aq = a, ((t − W ) ∨ 1) ≤ q ≤ t − 1) ,

δ

δ=

.

2SAT 2S

Each ξq is conditionally 2-sub-Gaussian, since −1 ≤ ξq ≤ 1 with certainty. Altogether, the required inequality is shown.

38

C. Proof of Proposition 2 In this section, we prove Proposition 2. Throughout the section, we impose the assumptions stated by the Proposition. That is, the events Er, Ep hold, and there exists p with (1) p ∈ Hp,τ(m)(η), (2) (S, A, p) has diameter at most D. We begin by recalling the following notations:

Br,t = max |rt+1(s, a) − rt(s, a)|, s∈S ,a∈As

t−1

varr,t =

Br,q ,

q=τ (m)−W

We then need the following auxiliary lemmas

Bp,t = max pt+1(·|s, a) − pt(·|s, a) 1 , s∈S ,a∈As

t−1

varp,t =

Bp,q .

q=τ (m)−W

Lemma 3. Let t be in episode m. For every state-action pair (s, a), we have

rt(s, a) − r¯τ(m)(s, a) ≤ varr,t, pt(·|s, a) − p¯τ(m)(·|s, a) 1 ≤ varp,t Lemma 4. Let t be in episode m. We have
ρ˜τ(m) ≥ ρ∗t − varr,t − 2D · varp,t.

Lemma 5. Let t be in episode m. For every state-action pair (s, a), we have

p˜τ(m)(s |s, a)γ˜τ(m)(s ) − pt(s |s, a)γ˜τ(m)(s ) ≤ 2D varp,t + 2rad-p,τ(m)(s, a) + η .

s ∈S

s ∈S

Lemmas 3, 4, 5 are proved in Sections C.1, C.2, and C.3, respectively.

C.1. Proof of Lemma 3 We ﬁrst provide the bound for rewards:

rt(s, a) − r¯τ(m)(s, a) ≤ rt(s, a) − rτ(m)(s, a) + rτ(m)(s, a) − r¯τ(m)(s, a)

t−1

1W

≤ |rq+1(s, a) − rq(s, a)| + W rτ(m)(s, a) − rτ(m)−w(s, a) .

q=τ (m)

w=1

By the deﬁnition of Br,q, we have

t−1

t−1

|rq+1(s, a) − rq(s, a)| ≤

Br,q ,

q=τ (m)

q=τ (m)

and

1W

1W w

W rτ(m)(s, a) − rτ(m)−w(s, a) ≤ W

rτ(m)−i+1(s, a) − rτ(m)−i(s, a)

w=1

w=1 i=1

1WW

≤ W

rτ(m)−i+1(s, a) − rτ(m)−i(s, a)

w=1 i=1

W

W

=

rτ(m)−i+1(s, a) − rτ(m)−i(s, a) ≤ Br,τ(m)−i.

i=1

i=1

Next, we provide a similar analysis on the transition distribution.

pt(s, a) − p¯τ(m)(s, a) 1 ≤ pt(s, a) − pτ(m)(s, a) 1 + pτ(m)(s, a) − p¯τ(m)(s, a) 1

39

t−1

1W

≤

pq+1(s, a) − pq(s, a) 1 + W

pτ(m)(s, a) − pτ(m)−w(s, a) 1 .

q=τ (m)

w=1

By the deﬁnition of Bp,q, we have

t−1 q=τ (m)

pq+1(s, a) − pq(s, a)

t−1

1≤

Bp,q ,

q=τ (m)

and

1W W w=1

pτ(m)(s, a) − pτ(m)−w(s, a)

1W w 1≤ W
w=1 i=1

pτ(m)−i+1(s, a) − pτ(m)−i(s, a) 1

1WW ≤
W w=1 i=1

pτ(m)−i+1(s, a) − pτ(m)−i(s, a) 1

W

W

=

pτ(m)−i+1(s, a) − pτ(m)−i(s, a) 1 ≤ Bp,τ(m)−i.

i=1

i=1

Altogether, the lemma is shown.

C.2. Proof of Lemma 4 We ﬁrst demonstrate two immediate consequences about the dual solution (ρ˜τ(m), γ˜τ(m)) by the Proposition’s assumptions:

0 ≤ γ˜τ(m)(s) ≤ 2D
ρ˜τ(m) + γ˜τ(m)(s) ≥ r¯τ(m)(s, a) + γ˜τ(m)(s )p¯τ(m)(s |s, a)
s ∈S
To see inequality (32), ﬁrst observe that

for all s ∈ S,

(32)

for all s ∈ S, a ∈ As.

(33)

ρ˜τ(m) + γ˜τ(m)(s) ≥

max

{r˙(s, a)} + γ˜τ(m)(s ) max {p˙(s |s, a)}

r˙ (s,a)∈Hr,τ (m) (s,a)

s ∈S

p˙∈Hp,τ (m)(s,a;η)

≥

max

{r˙(s, a)} + γ˜τ(m)(s )p(s |s, a).

r˙ (s,a)∈Hr,τ (m) (s,a)

s ∈S

(34) (35)

Step (34) is by Property 1 of the output from EVI, which is applied with conﬁdence regions

Hr,τ(m), Hp,τ(m)(η). Step (35) is because of the assumption that p ∈ Hp,τ(m)(η). Altogether, the solution (ρ˜τ(m), γ˜τ(m)) is feasible to D(r˙, p) for any r˙ ∈ Hr,τ(m). Now, by Lemma 2, we have maxs,s ∈S |γ˜τ(m)(s) − γ˜τ(m)(s )| ≤ 2D. Finally, inequality (32) follows from the fact that the bias vector γ˜τ(m) returned by EVI is component-wise non-negative, and there exists s ∈ S such that γ˜τ(m) = 0.
To see inequality (33), observe that

ρ˜τ(m) + γ˜τ(m)(s) ≥

max

{r˙(s, a)} + γ˜τ(m)(s ) max {p˙(s |s, a)}

r˙ (s,a)∈Hr,τ (m) (s,a)

s ∈S

p˙∈Hp,τ (m)(s,a;η)

≥ r¯τ(m)(s, a) + γ˜τ(m)(s )p¯τ(m)(s |s, a).
s ∈S

(36) (37)

Step (36) is again by Property 1 of the output from EVI, and step (37) is by the assumptions that r¯τ(m) ∈ Hr,τ(m), and p¯τ(m) ∈ Hp,τ(m)(0) ⊂ Hp,τ(m)(η).

40

Now, we claim that (ρ˜τ(m) + varr,t + 2D · varp,t, γ˜τ(m)) is a feasible solution to the tth period dual problem D(rt, pt), which immediately implies the Lemma. To demonstrate the claim, for every state-action pair (s, a) we have

r¯τ(m)(s, a) ≥ rt(s, a) − varr,t

(38)

γ˜τ(m)(s )pτ(m)(s |s, a) ≥ γ˜τ(m)(s )pt(s |s, a) − γ˜τ(m) ∞ pt(·|s, a) − p¯τ(m)(·|s, a) 1

s ∈S

s ∈S

≥ γ˜τ(m)(s )pt(s |s, a) − 2D · varp,t, .
s ∈S

(39)

Inequality (38) is by Lemma 3 on the rewards. Step (39) is by inequality (32), and by Lemma 3 which shows pt(·|s, a) − p¯τ(m)(·|s, a) 1 ≤ varp,t. Altogether, putting (38), (39) to inequality (33), our claim is shown, i.e., for all s ∈ S and a ∈ As,

ρ˜τ(m) + varr,t + 2D · varp,t + γ˜τ(m)(s) ≥ rt(s, a) + γ˜τ(m)(s )pt(s |s, a).
s ∈S
Hence, the lemma is proved.
C.3. Proof of Lemma 5 We have

p˜τ(m)(s |s, a) − pt(s |s, a) γ˜τ(m)(s )

s ∈S





≤

γ˜τ (m)

· ∞

p˜τ(m)(·|s, a) − p¯τ(m)(·|s, a)

1+

p¯τ(m)(·|s, a) − pt(·|s, a)

1 .

(a)

(b)

(c)

(40)

In step (40), we know that

• (a) ≤ 2D by inequality (32),

• (b) ≤ 2rad-p,τ(m)(s, a) + η, by the facts that p˜τ(m)(·|s, a) ∈ Hp,τ(m)(s, a; η) and p¯τ(m)(·|s, a) ∈ Hp,τ(m)(s, a; 0), • (c) ≤ varp,t by Lemma 3 on the bound on p. Altogether, the Lemma is proved.

C.4. Finalizing the Proof Now, we have

rt(st, at)

≥r¯τ(m)(st, at) − varr,t

≥r˜τ(m)(st, at) − varr,t − 2 · rad-r,τ(m)(st, at)

≥ρ˜τ(m) + γ˜τ(m)(st) −

p˜τ(m)(s |st, at)γ˜τ(m)(s ) −

s ∈S

− varr,t − 2 · rad-r,τ(m)(st, at)

1 τ (m)

≥ρ∗t + γ˜τ(m)(st) −

pt(s |st, at)γ˜τ(m)(s ) −

s ∈S

1 τ (m)

(41) (42)
(43)

41

− 2 varr,t + rad-r,τ(m)(st, at) − 2D 2 · varp,t + 2 · rad-p,τ(m)(s, a) + η .

(44)

Step (41) is by Lemma 3 on t. Step (42) is by conditioning that event Er holds. Step (43) is by Property 2 for the output of EVI. In step (44), we upper bound ρ˜τ(m) by Lemma 4 and we upper bound
s ∈S p˜τ(m)(s |st, at)γ˜τ(m)(s ) by Lemma 5. Rearranging gives the Proposition. D. Proof of Theorem 1

To facilitate the exposition, we denote M (T ) as the total number of episodes. By abusing the notation we, let τ (M (T ) + 1) − 1 = T . Episode M (T ), containing the ﬁnal round T , is interrupted and the algorithm is forced to terminate as the end of time T is reached. We can now rewrite the dynamic regret of the SWUCRL2-CW algorithm as the sum of dynamic regret from each episode:

T

M (T ) τ (m+1)−1

Dyn-RegT (SWUCRL2-CW) = (ρ∗t − rt(st, at)) =

(ρ∗t − rt(st, at))

t=1

m=1 t=τ (m)

(45)

To proceed, we deﬁne the set

U = {m ∈ [M (T )] : pτ(m)(·|s, a) ∈ Hp,τ(m)(s, a; η) ∀(s, a) ∈ S × As}.

For each episode m ∈ [M (T )], we distinguish two cases: • Case 1. m ∈ U. Under this situation, we apply Proposition 2 to bound the dynamic regret during the
episode, using the fact that pτ(m) satisﬁes the assumptions of the proposition with D = Dτ(m) ≤ Dmax.
• Case 2. m ∈ [M (T )] \ U . In this case, we trivially upper bound the dynamic regret of each round in episode m by 1. For case 1, we bound the dynamic regret during episode m by summing the error terms in (7, 8) across
the rounds t ∈ [τ (m), τ (m + 1) − 1] in the episode. The term (7) accounts for the error by switching policies. In (8), the terms rad-r,τ(m), rad-p,τ(m) accounts for the estimation errors due to stochastic variations, and the term varr,t, varp,t accounts for the estimation error due to non-stationarity.
For case 2, we need an upper bound on m∈[M(T )]\U tτ=(mτ(+m1))−1 1, the total number of rounds that belong to an episode in [M (T )] \ U . The analysis is challenging, since the length of each episode may vary, and one can only guarantee that the length is ≤ W . A ﬁrst attempt could be to upper bound as
m∈[M(T )]\U τt=(mτ(+m1))−1 1 ≤ W m∈[M(T )]\U 1, but the resulting bound appears too loose to provide any meaningful regret bound. Indeed, there could be double counting, as the starting time steps for a pair of episodes in case 2 might not even be W rounds apart!
To avoid the trap of double counting, we consider a set QT ⊆ [M (T )] \ U where the start times of the episodes are suﬃciently far apart, and relate the cardinality of QT to m∈[M(T )]\U τt=(mτ(+m1))−1 1. The set QT ⊆ [M (T )] is constructed sequentially, by examining all episodes m = 1, . . . , M (T ) in the time order. At the start, we initialize QT = ∅. For each m = 1, . . . , M (T ), we perform the following. If episode m satisﬁes both criteria: 1. There exists some s ∈ S and a ∈ As such that pτ(m)(·|s, a) ∈/ Hp,τ(m)(s, a; η);
2. For every m ∈ QT , τ (m) − τ (m ) > W,

42

Figure 6 Both episodes mi and mi+4 belong to QT (and thus Q˜T ) because pτ(mi) ∈/ Hp,τ(mi)(η) and pτ(mi+4) ∈/ Hp,τ(mi+4)(η). mi+1 is added to Q˜T (but not QT ) because τ (mi+1) − τ (mi) ∈ [0, W ]. mi+2 and mi+3
belong to neither of QT nor Q˜T as pτ(mi+2) ∈ Hp,τ(mi+2)(η) and pτ(mi+3) ∈ Hp,τ(mi+3)(η).

then we add m into QT . Afterwards, we move to the next episode index m + 1. The process terminates once we arrive at episode M (T ) + 1. The construction ensures that, for each episode m ∈ [M (T )], if τ (m) − τ (m ) ∈/ [0, W ] for all m ∈ QT , then ∀s ∈ S ∀a ∈ As pτ(m)(·|s, a) ∈ Hp,τ(m)(s, a); otherwise, m would have been added into QT .
We further construct a set Q˜T to include all elements in QT and every episode index m such that there exists m ∈ QT with τ (m) − τ (m ) ∈ [0, W ]. By doing so, we can prove that every episode m ∈ [M (T )] \ Q˜T satisﬁes pτ(m)(·|s, a) ∈ Hp,τ(m)(s, a) ∀s ∈ S ∀a ∈ As. The procedures for building Q˜T (initialized to QT ) are described as follows: for every episode index m ∈ [M (T )], if there exists m ∈ QT , such that τ (m) − τ (m ) ∈ [0, W ], then we add m to Q˜T . Formally,
Q˜T = QT ∪ {m ∈ [M (T )] : ∃m ∈ QT τ (m) − τ (m ) ∈ [0, W ]} .
We can formalize the properties of QT and Q˜T as follows.
Lemma 6. Conditioned on Ep, |QT | ≤ Bp/η. Lemma 7. For any episode m ∈/ Q˜T , we have pτ(m)(·|s, a) ∈ Hp,τ(m)(s, a; η) for all s ∈ S and a ∈ As.
The proofs of Lemmas 6 and 7 are presented in Sections D.1 and D.2, respectively. Together with eqn. (45), we can further decompose the dynamic regret of the SWUCRL2-CW algorithm as

Dyn-RegT (SWUCRL2-CW)

τ (m+1)−1

τ (m+1)−1

=
m∈Q˜T

t=τ (m)

(ρ∗t − rt(st, at)) +
m∈[MT ]\Q˜T

t=τ (m)

(ρ∗t − rt(st, at))

τ (m+1)−1

≤
m∈Q˜T

t=τ (m)

(ρ∗t − rt(st, at))

τ (m+1)−1
+
m∈[MT ]\Q˜T t=τ (m)

pt(s |st, at)γ˜τ(m)(s ) − γ˜τ(m)(st) +
s ∈S

1 τ (m)

τ (m+1)−1

+

(2varr,t + 4Dmax · varp,t + 2Dmaxη)

m∈[MT ]\Q˜T t=τ (m)

τ (m+1)−1

+

2rad-r,τ(m)(st, at) + 4Dmax · rad-p,τ(m)(st, at) ,

m∈[MT ]\Q˜T t=τ (m)

(♠) (♣) (♦) (♥)

where the last step makes use of Lemma 7 and Proposition 2. We accomplish the promised dynamic regret

bound by the following four Lemmas that bound the dynamic regret terms (♠, ♣, ♦, ♥).

43

Lemma 8. Conditioned on Ep, we have (♠) = O BpW . η
Lemma 9. Conditioned on events Er, Ep, we have with probability at least 1 − O(δ) that (♣) = O˜(Dmax[M (T ) + √T ]) = O˜ Dmax SWAT + √T .
Lemma 10. With certainty,
(♦) = O ((Br + DmaxBp)W + DmaxT η) .

Lemma 11. With certainty, we have

√

(♥) = O˜

DmaxS AT √

.

W

The proofs of Lemmas 8, 9, 10, and 11 are presented in Sections D.3, D.4, D.5, and D.6, respectively. Putting

all these pieces together, we have the dynamic regret of the SWUCRL2-CW algorithm is upper bounded as √
O˜ BpηW + BrW + Dmax BpW + S√WAT + T η + SWAT + √T ,

and by setting W and η accordingly, we can conclude the proof.

D.1. Proof of Lemma 6

We ﬁrst claim that, for every episode m ∈ QT , there exists some state-action pair (s, a) and some time step tm ∈ [(τ (m) − W ∨ 1), τ (m) − 1] such that

pτ(m)(·|s, a) − ptm (·|s, a) 1 > η.

(46)

For contradiction sake, suppose the otherwise, that is, pτ(m)(·|s, a) − pt(·|s, a) 1 ≤ η for every state-action

pair s, a and every time step t ∈ [(τ (m) − W ∨ 1), τ (m) − 1]. For each state-action pair (s, a), consider the

following cases on Nτ(m)(s, a) =

τ (m)−1 q=(τ (m)−W )∨1

1(sq

=

s,

aq

=

a):

• Case 1: Nτ(m)(s, a) = 0. Then pˆτ(m)(·|s, a) = 0S, and clearly we have

pτ(m)(·|s, a) − pˆτ(m)(·|s, a) 1 = 1 < rad-pτ(m)(s, a) < rad-pτ(m)(s, a) + η.

• Case 2: Nτ(m)(s, a) > 0. Then we have the following inequalities:

pτ(m)(·|s, a) − p¯τ(m)(·|s, a) 1

1 τ (m)−1

= N + (s, a)

pτ(m)(·|s, a) − pq(·|s, a) · 1(sq = s, aq = a)

τ (m) q=(τ (m)−W )∨1 1

1 τ (m)−1

≤ N + (s, a)

τ (m)

q=(τ (m)−W )∨1

pτ(m)(·|s, a) − pq(·|s, a)

1 · 1(sq = s, aq = a) ≤ η.

(47) (48)

Step (47) is by the deﬁnition of p¯τ(m)(·|s, a), and step (48) is by the triangle inequality. Consequently, we

have

pτ(m)(·|s, a) − pˆτ(m)(·|s, a) 1

≤ p¯τ(m)(·|s, a) − pˆτ(m)(·|s, a) 1 + pτ(m)(·|s, a) − p¯τ(m)(·|s, a) 1

(49)

≤rad-pτ(m)(s, a) + η.

Step (49) is true since we condition on the event Ep,

44

Combining the two cases, we have shown that pτ(m)(·|s, a) ∈ Hp,τ(m)(s, a, η) for all s ∈ S, a ∈ As, contradicting to the fact that m ∈ QT . Altogether, our claim on inequality (46) is established.
Finally, we provide an upper bound to |QT | using (46):

Bp =

max { pt+1(·|s, a) − pt(·|s, a) 1}

s∈S ,a∈As

t∈[T −1]

τ (m)−1

≥
m∈QT

q=tm

max
s∈S ,a∈As

pq+1(·|s, a) − pq(·|s, a) 1





τ (m)−1





≥

max

s∈S ,a∈As

m∈QT



(pq+1(·|s, a) − pq(·|s, a))

q=tm


1

>|QT |η.

(50) (51) (52)

Step (50) follows by the second criterion of the construction of QT , which ensures that for distinct m, m ∈ QT , the time intervals [tm, τ (m)], [tm , τ (m )] are disjoint. Step (51) is by applying the triangle inequality, for each m ∈ QT , on the state-action pair (s, a) that maximizes the term τq=(mtm)−1(pq+1(·|s, a) − pq(·|s, a)) 1 = (pτ(m)(·|s, a) − ptm (·|s, a)) 1. Step (52) is by applying the claimed inequality (46) on each m ∈ QT . Altogether, the Lemma is proved.
D.2. Proof of Lemma 7 We prove by contradiction. Suppose there exists an episode m ∈/ Q˜T , a state s ∈ S, and an action a ∈ As such that pτ(m)(·|s, a) ∈/ Hp,τ(m)(s, a; η), then m should have been added to QT . To see this, we ﬁrst note that episode m trivially satisﬁes criterion 1 in the construction of QT . Moreover, at the time when m is examined, we know that any m has been added to QT should satisfy τ (m) − τ (m ) > W as otherwise m would have been added to Q˜T . Therefore, we have prove m ∈ QT ⊆ Q˜T , which is clearly a contradiction.
D.3. Proof of Lemma 8 Denote QT = {m1, . . . , m|QT |}. By construction, for every element m ∈ Q˜T , there exists an unique m ∈ QT such that

τ (m) − τ (m ) ∈ [0, W ].

(53)

We can thereby partition the elements of Q˜T into |QT | disjoint subsets Q˜T (m1), . . . , Q˜T (m|QT |) such that 1. Each element m ∈ Q˜T belongs to exactly one Q˜T (m ) for some m ∈ QT .

2. Each element m ∈ Q˜T (m ) satisﬁes τ (m) − τ (m ) ∈ [0, W ]. We bound (♠) from above as

τ (m+1)−1
(ρ∗t − rt(st, at))
m ∈QT m∈Q˜T (m ) t=τ (m)

τ (m+1)−1

≤

1

m ∈QT m∈Q˜T (m ) t=τ (m)

=

(τ (m + 1) − τ (m))

m ∈QT m∈Q˜T (m )

(54)

45

≤
m ∈QT

max τ (m + 1) − τ (m )
m∈Q˜T (m )

=

max (τ (m + 1) − τ (m) + τ (m)) − τ (m )

m ∈QT m∈Q˜T (m )

≤

max (τ (m + 1) − τ (m)) + max τ (m) − τ (m )

m ∈QT m∈Q˜T (m )

m∈Q˜T (m )

≤

[W + W ]

m ∈QT
=2W |QT |

≤2BpW/η.

(55) (56)

Here, inequality (54) holds by boundedness of rewards, inequality (55) follows from the fact that episodes are mutually disjoint, inequality (56) makes the observations that each episode can last for at most W time steps (imposed by the SWUCRL2-CW algorithm) as well as criterion 2 of the construction of Q˜T (m )’s, and the last step uses Lemma 6.
D.4. Proof of Lemma 9 We ﬁrst give an upper bound for M (T ), the total number of the episodes.
Lemma 12. Conditioned on events Er, Ep, we have M (T ) ≤ SA(2 + log2 W )T /W = O˜ (SAT /W ) with certainty.

First, to demonstrate the bound for M (T ), it suﬃces to show that there are at most SA(2 + log2 W )

many episodes in each of the following cases: (1) between time steps 1 and W , (2) between time steps jW

and (j + 1)W , for any j ∈ {1, . . . , T /W − 1}, (3) between time steps T /W · W and T . We focus on case

(2), and the edge cases (1, 3) can be analyzed similarly.

Between time steps jW and (j + 1)W , a new episode m + 1 is started when the second criterion

νm(st, π˜m(st)) < Nτ+(m)(st, π˜m(st)) is violated during the current episode m. We ﬁrst illustrate how the second criterion is invoked for a ﬁxed state-action pair (s, a), and then bound the number of invocations due to (s, a).

Now, let’s denote m1, . . . , mL as the episode indexes, where jW ≤ τ (m1) < τ (m2) < . . . < τ (mL) < (j + 1)W ,

and the second criterion for (s, a) is invoked during m for 1 ≤ ≤ L. That is, for each ∈ {1, . . . , L}, the DM

breaks the while loop for episode m because νm (s, a) = Nτ+(m )(s, a), leading to the new episode m + 1.

To demonstrate our claimed bound for M (T ), we show that L ≤ 2 + log2 W as follows. To ease the notation,

let’s denote ψ

= νm

(s, a).

We

ﬁrst

observe

that

ψ1

=

N+ τ (m1

)

(s,

a)

≥

1.

Next,

we

note

that

for

∈ {2, . . . L},

we have ψ ≥ k−=11 ψk. 1 Indeed, we know that for each we have (τ (m + 1) − 1) − τ (m1) ≤ W , by our

assumption on m1, . . . , m . Consequently, the counting sum in Nτ(m )(s, a), which counts the occurrences

of (s, a) in the previous W time steps, must have counted those occurrences corresponding to ψ1, . . . , ψ −1.

The worst case sequence of ψ1, ψ2, . . . , ψL that yields the largest L is when ψ1 = ψ2 = 1, ψ3 = 2, and more

generally ψ = 2 −2 for ≥ 2. Since ψ ≤ W for all W , we clearly have L ≤ 2 + log2 W , proving our claimed

bound on L. Altogether, during the T time steps, there are at most (SAT (2 + log2 W ))/W episodes due to

the second criterion and T /W due to the ﬁrst, leading to our desired bound on M (T ).

46

Next, we establish the bound for (♣). By Lemma 7, we know that γ˜τ(m)(s) ∈ [0, 2Dmax] for all m ∈ [M (T )] \ Q˜T and s. For each episode m ∈ [M (T )] \ Q˜T , we have

τ (m+1)−1 t=τ (m)

pt(s |st, at)γ˜τ(m)(s ) − γ˜τ(m)(st)
s ∈S

τ (m+1)−1

= −γ˜τ(m)(sτ(m)) + γ˜τ(m)(sτ(m)+1) +

pt(s |st, at)γ˜τ(m)(s ) − γ˜τ(m)(st+1) .

≤2Dmax

t=τ (m) s ∈S

=Yt

Summing (57) over m ∈ [M (T )] \ Q˜T we get

(57)

τ (m+1)−1

pt(s |st, at)γ˜τ(m)(s ) − γ˜τ(m)(st)

m∈[M (T )]\Q˜T t=τ (m) s ∈S

≤2Dmax · M (T ) − |Q˜T | +

τ (m+1)−1
Yt.

(58)

m∈[M (T )]\Q˜T t=τ (m)

Deﬁne the ﬁltration Ht−1 = {(sq, aq, Rq(sq, aq))}tq=1. Now, we know that E[Yt|Ht−1] = 0, Yt is σ(Ht)-

measurable, and |Yt| ≤ 2Dmax. Therefore, we can apply the Hoeﬀding inequality (Hoeﬀding 1963) to show

that

τ (m+1)−1 m∈[M (T )]\Q˜T t=τ (m)

pt(s |st, at)γ˜τ(m)(s ) − γ˜τ(m)(st)
s ∈S

=O

Dmax · M (T ) + Dmax

1 T log
δ

with probability 1 − O(δ), where we use the facts that M (T ) − |Q˜T | ≤ M (T ) and m∈[M(T )]\Q˜ tτ=(mτ(+m1))−1 1 ≤ T

T. Finally, note that

τ (m+1)−1
m∈[M (T )]\Q˜T t=τ (m)
Hence, the Lemma is proved.

1 M(T ) τ (m+1)−1 ≤
τ (m) m=1 t=τ(m)

1 T /W W

√

≤

√ = O( T ).

τ (m) i=1 iW

D.5. Proof of Lemma 10

We ﬁrst note that

τ (m+1)−1

T

(2varr,t + 4Dmax · varp,t + 2Dmaxη) ≤ (2varr,t + 4Dmax · varp,t + 2Dmaxη) ,

m∈[MT ]\Q˜T t=τ (m)

t=1

since varr,t ≥ 0 and varp,t ≥ 0 for all t. We can thus work with the latter quantity.

We ﬁrst bound

T t=1

varr,t

.

Now,

recall

the

deﬁnition

that,

for

time

t

in

episode

m,

we

have

deﬁned

varr,t =

t−1 q=τ (m)−W

Br,q .

Clearly,

for

iW

≤ q < (i + 1)W ,

the

summand

Br,q

only

appears

in

varr,t

for

iW

≤

q < t ≤ (i + 2)W , since each episode is contained in {i W, . . . , (i + 1)W } by our episode termination criteria

(t is a multiple of W ) of the SWUCRL2-CW algorithm. Altogether, we have

T

T −1

2 varr,t ≤ 2 Br,tW = 2BrW.

(59)

t=1

t=1

Next, we bound

T t=1

varp,t

.

Now,

we

know

that

τ (m + 1) − τ (m) ≤ W

by

our

episode

termination

criteria

(t is a multiple of W ) of the SWUCRL2-CW algorithm. Consequently,

T

T −1

4Dmax varp,t ≤ 4Dmax Bp,tW = 4DmaxBpW.

(60)

t=1

t=1

Finally, combining (59, 60) with 2Dmax Tt=1 η, the Lemma is established.

47

D.6. Proof of Lemma 11

Due to non-negativity of rad-rt(s, a)’s and rad-pt(s, a)’s, we have

τ (m+1)−1

M (T ) τ (m+1)−1

rad-rτ(m)(st, at) ≤

rad-rτ(m)(st, at),

m∈[MT ]\Q˜T t=τ (m)

m=1 t=τ (m)

τ (m+1)−1

M (T ) τ (m+1)−1

rad-pτ(m)(st, at) ≤

rad-pτ(m)(st, at)

m∈[MT ]\Q˜T t=τ (m)

m=1 t=τ (m)

We thus ﬁrst show that, with certainty,

M (T ) τ (m+1)−1

M (T ) τ (m+1)−1

√

rad-rτ(m)(st, at) =

2 2 ln(SAT /δ) = O˜ N + (st, at)

SAT

√

,

W

m=1 t=τ (m)

m=1 t=τ (m)

τ (m)

M (T ) τ (m+1)−1

M (T ) τ (m+1)−1

√

rad-pτ(m)(st, at) =

2

2S log (AT W/δ) = O˜

S AT √

.

N + (st, at)

W

m=1 t=τ (m)

m=1 t=τ (m)

τ (m)

(61) (62)
(63) (64)

We analyze by considering the dynamics of the algorithm in each consecutive block of W time steps, in a way similar to the proof of Lemma 9. Consider the episodes indexes m0, m1 . . . , m T/W , m T/W +1, where τ (m0) = 1, and τ (mj) = jW for j ∈ {1, . . . , T /W }, and m T/W +1 = m(T ) + 1 (so that τ (m T/W +1 − 1) is the time index for the last episode in the horizon).
To prove (63, 64), it suﬃces to show that, for each j ∈ {0, 1, . . . , T /W }, we have

mj+1−1 τ (m+1)−1 m=mj t=τ (m)

1

√

= O SAW .

N

+ τ (m

)

(st

,

at

)

(65)

Without loss of generality, we assume that j ∈ {1, . . . , T /W − 1}, and the edge cases of j = 0, T /W can be analyzed similarly.
Now, we ﬁx a state-action pair (s, a) and focus on the summands in (65):

mj+1−1 τ (m+1)−1
1((st, at) = (s, a))

mj +1 −1

=

m=mj t=τ (m)

N

+ τ (m

)

(st

,

at

)

m=mj

νm(s, a) Nτ+(m)(s, a)

(66)

It is important to observe that:

1. It holds that νmj (s, a) ≤ Nτ(mj)(s, a), by the episode termination criteria of the SWUCRL2-CW algorithm,

2. For m ∈ {mj + 1, . . . , mj+1 − 1}, we have

m−1 m =m

νm (s, a) ≤ Nτ(m)(s, a) . Indeed, we know that episodes

j

mj, . . . , mj+1 − 1 are inside the time interval {jW, . . . , (j + 1)W }. Consequently, the counts of “(st, at) =

(s, a)” associated with {νm (s, a)}m m−=1mj are contained in the W time steps preceding τ (m), hence the

desired inequality.

With these two observations, we have

(66) ≤

νmj (s, a)

mj +1 −1
+

max{νmj (s, a), 1} m=mj+1

max{

νm(s, a)

m−1 m =m

νm (s, a), 1}

j





√

mj +1 −1





≤ max{νmj (s, a), 1} + ( 2 + 1) max

νm(s, a), 1

(67)

 m=mj



48

√ ≤ ( 2 + 2)

max

(j+1)W −1
1((st, at) = (s, a)), 1 .
t=jW

(68)

Step (67) is by Lemma 19 in (Jaksch et al. 2010), which bounds the sum in the previous line. Step (68) is by the fact that episodes mj, . . . , mj+1 − 1 partitions the time interval jW, . . . , (j + 1)W − 1, and νm(s, a) counts the occurrences of (st, at) = (s, a) in episode m. Finally, observe that (66) = 0 if νm(s, a) = 0 for all m ∈ {mj, . . . , mj+1 − 1}. Thus, we can reﬁne the bound in (68) to

√ (66) ≤ ( 2 + 2)

(j+1)W −1
1((st, at) = (s, a)).
t=jW

(69)

The required inequality (65) is ﬁnally proved by summing (69) over s ∈ Sa ∈ A and applying Cauchy

Schwartz:

mj+1−1 τ (m+1)−1 m=mj t=τ (m)

1 mj+1−1

=

N

+ τ (m

)

(st

,

at

)

s∈S,a∈As m=mj

νm(s, a) Nτ+(m)(s, a)

√ ≤( 2 + 2)

s∈S ,a∈As

√

√

≤( 2 + 2) SAW .

(j+1)W −1
1((st, at) = (s, a))
t=jW

Altogether, the Lemma is proved.

E. Proof of Theorem 2

To begin, we consider the following regret decomposition, for any choice of (W †, η†) ∈ J, we have





T /H

i·H ∧T

Dyn-RegT (BORL) =

E

ρ∗t − Ri Wi, ηi, s(i−1)H+1 

i=1

t=(i−1)H +1





T /H

i·H ∧T

=

E

ρ∗t − Ri W †, η†, s(i−1)H+1 

i=1

t=(i−1)H +1

T /H

T /H

+

E

Ri W †, η†, s(i−1)H+1 − Ri Wi, ηi, s(i−1)H+1 .

i=1

i=1

(70)

For the ﬁrst term in eqn. (70), we can apply the results from Theorem 1 to each block i ∈ T /H , i.e.,

i·H ∧T

√ ρ∗t − R W †, η†, s(i−1)H+1 =O˜ Bp(i)W † + Br(i)W † + Dmax Bp(i)W † + S√ AH + Hη† + SAH + √H ,

η†
t=(i−1)H +1

W†

W†

(71)

where we have deﬁned





i·H ∧T

Br(i) = 

Br,t ,

t=(i−1)H +1





i·H ∧T

Bp(i) = 

Bp,t

t=(i−1)H +1

for brevity. For the second term, it captures the additional rewards of the DM were it uses the ﬁxed parameters (W †, η†) throughout w.r.t. the trajectory on the starting states of each block by the BORL algorithm, i.e., s1, . . . , s(i−1)H+1, . . . , s( T/H −1)H+1, and this is exactly the regret of the EXP3.P algorithm when it is applied

49

to a ∆-arm adaptive adversarial bandit problem with reward from [0, H]. Therefore, for any choice of (W †, η†),

we can upper bound this by

√ O˜ H ∆T /H = O˜ T H

as ∆ = O ln2 T . Summing these two, the regret of the BORL algorithm is √
Dyn-RegT (BORL) = O˜ BpηW† † + BrW † + Dmax BpW † + S√WA†T + T η† + SWA†T + √T H . (72)

By virtue of the EXP3.P, the BORL algorithm is able to adapt to any choice of (W †, η†) ∈ J. Note that

3S

2 3

A

1 2

T

1 2

3T

1 2

H ≥W∗=

1≥

1 ≥ 1,

(Br + Bp + 1) 2 (3T ) 2

11

∗

(Bp

+

1)

1 2

S

1 3

A

1 4

S

1 3

A

1 4

S3A4 ≥η =

1 1≥

1

(Br + Bp + 1) 4 T 4 2T 2

11
= S 3 A 4 Φ.

(73) (74)

Therefore, there must exists some j† and k† such that

H j†/∆W ≤ W ∗ ≤ H (j†+1)/∆W ,

S

1 3

A

1 4

Φk†

/∆η

≥

η∗

≥

S

1 3

A

1 4

Φ(k†

+1)/∆η

(75)

By adapting W † to Hj†/∆W and η† to Φk†/∆η , we further upper bound eqn. (72) as

Dyn-RegT (BORL) = O˜

12 1 3
Dmax(Br + Bp + 1) 4 S 3 A 2 T 4

.

where we use H1/∆W = exp(1) and Φ1/∆η = exp(−1) in the last step. F. Proof of Proposition 4 We ﬁrst show the following lemma.

Lemma 13. Conditioned on Ep, there exists a state transition distribution p ∈ Hp,t(0) such that for every pair of states s, s ∈ S,
p(s |s, a(s,s )) ≥ ζ

for every time step t ∈ [T ].

The proof of the lemma is provided in Section F.1. We then consider the state transition distribution p ∈ Hp,t(0) speciﬁed in Lemma 13. For an arbitrary state s ∈ S, we consider the policy π such that π(s) = a(s,s ) for all s ∈ S (see Assumption 2 for the deﬁnition of a(s,s )). Starting from an arbitrary state s ∈ S, the policy either hits state s in the next step, which happens with probability at least ζ, or it transits to another state s = s , which would then hit state s in the next step with probability at least ζ. Therefore, the hitting process stochstically dominates the Bernoulli process with success probability ζ, and thus the expected hitting time is at most 1/ζ.
F.1. Proof of Lemma 13 First, we recall the deﬁnition of deﬁntion of conﬁdence region Hp,t(s, a; 0) in eqn. 6,

Hp,t(s, a; 0) = p˙ ∈ ∆S : p˙(·|s, a) − pˆt(·|s, a) 1 ≤ rad-p,t(s, a) .

For every pair of states s, s ∈ S, we construct p by distinguishing the following two cases:

50

• If Nt(s, a(s,s )) = 0, then by deﬁnition, rad-p,t(s, a(s,s )) ≥ 1, therefore every probability distribution p¯ on S belongs to Hp,t(s, a; 0). Setting p(·|s, a(s,s )) = pt(·|s, a(s,s )) for any t satisﬁes the requirement in the Lemma.

• If Nt(s, a(s,s )) > 0, then we know from Assumption 2 and eqn. 5 that

1

t−1

p¯t(s |s, a(s,s )) = Nt+(s, a(s,s )) q=(τ(m)−W )∨1 pq(s |s, a(s,s ))1(sq = s, aq = a(s,s )) ≥ ζ.

By conditioning on Ep, we know that p¯t(·|s, a(s,s )) ∈ Hp,t(s, a(s,s ); 0), and we can thus set p(·|s, a(s,s )) = p¯t(·|s, a(s,s )). Combining the above cases, the transition probability distribution p satisﬁes the stated inequality in the

Lemma, and we conclude the proof.

G. Proof of Proposition 6

We ﬁrst show that for any time step t ∈ [T ], we have ρ∗t − ρ∗t pseudo = −l · E[Xt]. From Section A.1, we have ρ∗t is equal to the optimal value of the following linear program P(rt, pt); while ρ∗t pseudo is equal to the optimal value of the following linear program P(rtpseudo, pt). The two linear programs has the same set of constraints, and follows from eqn. (14), the only diﬀerence is that the objective value of P(rtpseudo, pt) is l · E[Xt] more than that of P(rt, pt) (note that the summation of x(s, a) over s ∈ S and a ∈ As is 1 from the second constraint of
the linear program (15)). Therefore, we have

T

T

ρ∗t − ρ∗t pseudo = −l · E[Xt].

t=1

t=1

(76)

Next, conditioned on any demand realizations X1, . . . , XT , we can show by induction that the trajectory

generated by Π on M and Mpseudo are exactly the same as they use the same sequence of state transition

distributions. Therefore,

T

T

T

E[rt(st(M), at(M))|{Xs}ts=1] − E[rtpseudo(st(Mpseudo), at(Mpseudo))|{Xs}ts=1] = −l · Xt.

t=1

t=1

t=1

(77)

Taking expectation on both sides of eqn. (77), and combining this with eqn. (76), we can conclude the

statement.

