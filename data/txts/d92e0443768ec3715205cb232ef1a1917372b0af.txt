ESPNET-SLU: ADVANCING SPOKEN LANGUAGE UNDERSTANDING THROUGH ESPNET
Siddhant Arora1, Siddharth Dalmia1, Pavel Denisov2, Xuankai Chang1, Yushi Ueda1, Yifan Peng1, Yuekai Zhang3, Sujay Kumar1, Karthik Ganesan1, Brian Yan1, Ngoc Thang Vu2, Alan W Black1, Shinji Watanabe1
1Carnegie Mellon University, 2University of Stuttgart, 3Zoom Video Communications

arXiv:2111.14706v2 [cs.CL] 3 Mar 2022

ABSTRACT
As Automatic Speech Processing (ASR) systems are getting better, there is an increasing interest of using the ASR output to do downstream Natural Language Processing (NLP) tasks. However, there are few open source toolkits that can be used to generate reproducible results on different Spoken Language Understanding (SLU) benchmarks. Hence, there is a need to build an open source standard that can be used to have a faster start into SLU research. We present ESPnet-SLU, which is designed for quick development of spoken language understanding in a single framework. ESPnet-SLU is a project inside end-to-end speech processing toolkit, ESPnet, which is a widely used open-source standard for various speech processing tasks like ASR, Text to Speech (TTS) and Speech Translation (ST). We enhance the toolkit to provide implementations for various SLU benchmarks that enable researchers to seamlessly mix-and-match different ASR and NLU models. We also provide pretrained models with intensively tuned hyper-parameters that can match or even outperform the current state-of-the-art performances. The toolkit is publicly available at https://github.com/espnet/espnet.
Index Terms— open-source, spoken language understanding
1. INTRODUCTION
Spoken Language Understanding (SLU) is the task of inferring the semantic meaning of spoken utterances. SLU is an essential component of voice assistants, social bots, and intelligent home devices [1, 2] which have to map speech signals to executable commands every day. Recent advances have driven the commercial success of voice assistants including but not limited to Alexa, Google Home, Siri and Cortana. SLU comprises widespread applications of semantic understanding from spoken utterances. Some examples include recognizing the intent [3, 4] and their associated entities [4, 5] of a user’s command to take appropriate action, or even understanding the emotion behind a particular utterance [6], and engaging in conversations with a user by modeling the topic of a conversation [7, 8].
Conventional SLU systems consist of a pipeline approach, where a Speech Recognition (ASR) system ﬁrst maps a spoken utterance into an intermediate text representation, followed by the Natural Language Understanding (NLU) module that extracts the intent from the text representation. Recently, many end-to-end (E2E) SLU [9–11] approaches have been introduced to avoid the error propagation seen in cascaded models. Moreover, these models typically have a smaller carbon footprint [9] compared to the pipeline-based approaches, making them of particular interest to perform SLU on devices. E2E architectures can also capture nonphonemic speech signals such as pauses, phrasing of words, and intonation which can help provide additional cues towards the semantics that a text-based system cannot capture. These models

are also useful for low resource languages [12] where there is not enough training data or access to reliable transcripts to separately train ASR and NLU components.
With the increase in SLU datasets and methodologies proposed [3, 9, 13], there is a growing need for an open-source SLU toolkit which would help standardize the pipelines involved in building an SLU model like data preparation, model training, and its evaluation. Our goal is to provide an open-source standard where researchers can easily incorporate previously proposed technologies, compare and contrast new ideas with the existing methodologies. In this work, we introduce a new E2E-SLU toolkit built on an already existing open-source speech processing toolkit ESPnet [14–16]. ESPnet supports a variety of speech processing tasks ranging from front-end processing like enhancement and separation to recognition and translation. Having ESPnet-SLU would help users build systems for realworld scenarios where many speech processing steps need to be applied before running the downstream task. ESPnet also provides an easy access to other speech technologies being developed like dataaugmentation [17], encoder sub-sampling [14], and speech-focused encoders like conformers [18]. They also support many pretrained ASR [19–22] and NLU systems [23, 24] that can be used as feature extractors in a SLU framework. The contributions of ESPnet-SLU are summarized below: • We provide recipes that covers all experiment processes for intent
classiﬁcation [3, 13], slot ﬁlling [4], emotion recognition [6] and dialogue acts classiﬁcation [8] datasets. The toolkit also contains implementations in non English languages [12, 27–29].
• This toolkit incorporates the use of pretrained ASR models like HuBERT, Wav2vec2 and pretrained NLU models like BERT, MPNet that can be used as feature extractors for ASR and NLU submodules inside the E2E-SLU framework.
• It also contains implementations of various speech processing tasks that can be used in a pipeline manner, thus replicating realworld scenarios where speech processing frontend need to be applied before performing a downstream task1.
• We release an open-source toolkit and provides easy access to the trained models that match or even signiﬁcantly outperform the state-of-the-art performance on these benchmarks.
2. DESIGN
This section brieﬂy describes the design for recipes that include all procedures to complete model training and evaluation on a given dataset. The recipes have been carefully designed to follow a uniﬁed approach with stage-by-stage processing as described in [30]. Table 1 summarises the features supported by our toolkit and other
1The interactive demo on - https://espnet-slu.github.io

Table 1. Comparison with other open-source End to End Spoken Language Understanding toolkits in September 2021
Alexa[9] Lugosch[3] CoraJung [25] SpeechBrain[26] ESPnet-SLU
BiLSTM based encoder Transformer based encoder Conformer based encoder
Classiﬁer RNN based decoder Transformer based decoder
Supports multi tasking with ASR? Supports multi tasking with NLU? Supports using pretrained ASR model? Supports using pretrained NLU model? Supports other task? Supports SLU on languages besides English? Supports using context from previous utterances? Supports using tasks in pipeline manner? Provide pretrained model

popular E2E-SLU toolkits to the best of our knowledge. We compare with 4 well maintained frameworks, i.e. Alexa (alexa-end-toend-slu) [9], Lugosch (lorenlugosch/end-to-end-SLU) [3], CoraJung (CoraJung/ﬂexible-input-slu) [25] and SpeechBrain [26]. 2
Recipes We provide various recipes in order to implement a strong baseline across a variety of datasets. We broadly categorize our implementation into 3 types of data regimes; (1) Mid resource datasets, which comprises of majority of the SLU datasets like Fluent Speech Commands (FSC) [3], and Snips SmartLight (Snips) [13]. Usually, they do not contain sufﬁcient data to train an ASR module from scratch, but a pretrained ASR model can help improve acoustic modeling. Multi-tasking SLU with ASR transcripts can further improve model performance. (2) High resource SLU datasets like SLURP [4] provide both intent and transcript for a large number of audio ﬁles. Models on these datasets can utilize the ASR transcripts for multi-tasking for improved performance, but they do not usually beneﬁt from pretrained models. (3) Most SLU datasets on non-English languages can be described as low resource like the Dutch Grabo dataset [12]. They often lack speech data and hence multilingual pretrained ASR models can help as feature extractors. In these scenarios, transcripts are often unavailable or unreliable to perform ASR multi-tasking. By providing recipes for each of these datasets, ESPnet-SLU facilitates researchers to understand what methodologies work in different data regimes.
Tasks ESPnet supports various speech processing tasks such as ASR [14], TTS [15], ST [16], SE [30] and Voice Conversion (VC) [34]. We believe that to perform downstream understanding tasks on real-world audio, these tasks need to be applied in conjunction with SLU. By having multiple tasks in a single uniﬁed implementation, ESPnet allows the use of different speech tasks in a pipeline manner that can have widespread applications, as shown in Section 4.4.
ASR Multi-task learning Since SLU requires both acoustic and semantic understanding, it is often regarded as a more challenging task than ASR and NLU. Multi-task learning-based approaches [9, 25, 35] have become popular to strengthen the training of SLU systems. Hence, we allow the option to add auxiliary ASR objectives by making the model generate both intent and transcript.
ASR and NLU pretraining Recent work [36, 37] has advanced the state-of-the-art SLU performance by building the architecture on self-supervised ASR and NLU models. Inspired by this work,
2There are many excellent toolkits[31–33] that support only NLU task.

Table 2. Supported tasks and datasets in ESPnet-SLU along with their reported performance in the original paper and our toolkit. We show the metrics used in the original paper. We match or outperform SOTA performance across a variety of SLU benchmarks.

Task Dataset

Metric

Paper Results ESPnet-SLU

SLURP [4]

Acc.

FSC [3]

F1

FSC Unseen (S) [3, 40]

Acc.

FSC Unseen (U) [3, 40] Acc.

FSC Challenge (S) [3, 40] Acc.

IC FSC Challenge (U) [3, 40] Acc.

SNIPS [13]

F1

HarperValleyBank [41]

Acc

Grabo [12, 42]

Acc.

CAT-SLU MAP [27, 43] Acc.

Speech Commands [44] Acc.

78.3

86.3

98.8

99.6

94.2

98.6

88.3

86.4

92.3

97.5

78.3

78.5

91.7

91.7

45.5

47.1

94.5

97.2

79.8

78.9

88.2

98.4

SF SLURP [4]

SLU-F1

70.8

71.9

DA Switchboard [45, 46] Acc.

HarperValleyBank [41]

Acc.

68.7

67.5

45.5

47.1

ER IEMOCAP [6, 47]

5-fold Acc.

67.6

69.4

we also support options to use our framework’s pretrained ASR and NLU models as feature extractors. More details are in section 3.
Low resource Multilingual SLU The toolkit also contains recipes for languages such as Japanese [28], Dutch [12], Tamil [29], Sinhala[29] and Mandarin [27]. With these recipes, we want to facilitate research in SLU technologies and ensure that they are available to a wide variety of users, going beyond English-speaking users.
Combining context from previous utterances Human interactions are usually in the form of spoken conversations, where the semantic meaning of a given utterance depends on the context [11, 38, 39] in which it was spoken. Hence, we support using dialogue history to perform classiﬁcation on each conversation turn.

3. EXAMPLE MODELS
To provide a glimpse into various models supported within our SLU toolkit, we brieﬂy describe the construction of an example E2E-SLU model. The library is written in python using PyTorch as the main neural network library. The following sections describe the general details without going into the dataset-speciﬁc customizations.

Table 3. Intent Classiﬁcation accuracy on FSC [3] for models using ASR multitasking, pretrained ASR and data augmentation methods. SpeechBrain [26] results are accessed on September 2021.

Model

IC (% Acc)

Baseline

E2E-SLU [3]

96.6

+ Pretraining ASR [3]

98.8

Pretrained E2E-SLU + data augmentation [26]

99.6

Tsf. Encoder w/ Full Intent Decoding

93.5

ESPnet-SLU + SpecAug Data Augmentation

98.9

+ ASR Multi-tasking

99.4

+ Pretrained ASR HuBERT

99.6

Ablations

ESPnet-SLU w/ Character Decoding

98.3

for Intent

w/ Slot Decoding

97.8

Decoding

w/ Full Intent Decoding

98.9

Table 4. Intent Classiﬁcation F1 score on Snips [13] where we experiment with ﬁnetuning the frontend pretrained ASR models.

Model

IC (F1)

Pipeline ASR + NLU [13]

91.7

ESPnet-SLU w/ Pretrained HuBERT 87.4

+ Finetuning HuBERT

89.1

+ ASR Multi-tasking

91.7

Encoder Decoder Model We build the SLU model as a Transformer-based hybrid CTC/attention framework [48]. The transformer architecture [49] usually consists of 12 self-attention blocks in the transformer encoder and 6 self-attention blocks in the decoder. We also experiment with Conformer [18] encoders.
Using pretrained ASR models as pre-encoder We support using pretrained ASR models as feature extractors for our encoder architecture. We use the s3prl [47] and fairseq [50] toolkit to access a variety of self-supervised learning representations as frontend in our SLU architecture. These pretrained ASR models are inserted before the Encoder such that Encoder takes in the output of these ASR models as acoustic features extracted from the input audio ﬁle.
Using pretrained NLU models as post-encoder We integrate the HuggingFace Transformers library [51], which allows usage of numerous generic and task-speciﬁc pretrained NLU models. Pretrained self-attention blocks of the NLU model can be inserted into any sequence-to-sequence model between the Encoder and Decoder components and therefore we name this component post-encoder NLU. In this conﬁguration, hidden states from Encoder output are passed to the ﬁrst self-attention block of NLU instead of NLU token embeddings, and Decoder consumes the output of the last NLU self-attention block instead of Encoder output. This way, the output of Encoder gets processed by NLU and may have more information about its linguistic properties, e.g. semantics.

4. EXPERIMENTS
In this section, we demonstrate how models from our toolkit described in Section 3 perform on benchmark spoken language understanding datasets i.e. intent classiﬁcation (IC) [3, 4, 13, 41, 44]; slot ﬁlling (SF) [4]; emotion recognition (ER) [6] and dialogue acts (DA) classiﬁcation [7] corpora. As discussed in Section 2, we also perform experiments on low resource non-English datasets [12, 27]. The detailed comparison with the results in the original dataset paper is shown in Table 2. All the results are reported on the splits provided by the original paper’s authors unless stated otherwise.

Table 5. Intent Classiﬁcation accuracy on the SLURP Dataset [4] where we perform comparison between different pretrained ASR and NLU systems as feature extractors. SpeechBrain [26] results are accessed on September 2021.

Model

IC (F1)

Baseline

Pipeline ASR+NLU w/ synthetic data [4]

74.6

+ Additional ASR data [4]

78.3

E2E-SLU w/ Pretraining + synthetic data [26] 75.1

E2E-SLU w/ Conformer Encoder

76.4

ESPnet-SLU

+ Pretrained ASR HuBERT [19]

77.0

+ synthetic data

86.3

+ VQ-APC [22]

82.1

Ablations for

+ HuBERT [19]

83.3

Pretrained ASR

+ Wav2vec2 [20]

83.3

+ TERA [21]

83.5

Ablations for

+ MPNET [24]

82.5

Pretrained NLU

+ BERT [23]

85.7

4.1. Intent Classiﬁcation (IC) and Slot Filling (SF)
The intent classiﬁcation task is modeled as a conditional prediction task where we decode the intent as one word. Slot ﬁlling is modeled similarly where we ﬁrst predict intent followed by entity label and lexical ﬁller, separated by separator tokens.
FSC (IC) [3] tests a model’s ability to predict intents from commands used with an intelligent home voice assistant. Table 3 shows the result of different model architectures on this benchmark. We observe that a transformer-based SLU system with SpecAug [17] data augmentation can outperform the published results achieved by using a pretrained ASR system [3] on this dataset. Also, instead of decoding the intent as a whole word, we tried decoding intent by character and by each slot which was found to hurt the intent classiﬁcation performance. We experimented with multitasking with ASR transcripts as discussed in Section 2, gaining further improvements. Finally, using the pretrained ASR model HuBERT as a feature extractor improved the acoustic modeling, achieving SOTA performance [26] on this dataset. We also show results on the recently proposed Challenge and Unseen split [40] in Table 2 where we are able to outperform baselines in unseen speaker(S) test set and match baselines for unseen utterance(U) test set.
Snips (IC) [13] is another popular SLU benchmark whose results are shown in table 4. We perform our experiments on random split constructed using the approach followed in [9]. We observe that ﬁnetuning pretrained ASR models can further help in improving performance. Unlike FSC, Snips had unseen utterances in the test set that were not observed during training. Hence, we performed byte pair encoding of the transcript before concatenating with the intent to reduce the mismatch in the vocabulary of training and test transcripts and match the baseline performance.
SLURP (IC, SF) [4] has been recently proposed as a substantially larger and more linguistically diverse SLU dataset. It consists of prompts for an in-home personal robot assistant. Unlike FSC and Snips, pretrained ASR systems did not signiﬁcantly improve performance on this larger SLU dataset. Including the provided synthetic dataset (SLURP-synth) into our training set, as done in [4], achieved a signiﬁcant 8% performance gain over the previous state-of-the-art on this benchmark which is a pipeline model that uses additional ASR training data. We also analyzed using different pretrained ASR systems as feature extractors and observed that they did not help improve performance over FBANK. Thus, in contrast to results in SUPERB [47] benchmark, the pretrained ASR systems do not always

Table 6. Emotion Recognition accuracy of ESPnet-SLU models on the IEMOCAP Dataset [6] with different pretrained ASR systems. We report results on 1 out of 5 folds for development. SpeechBrain [26] results are accessed on September 2021.

Model

ER (% Acc)

E2E-SLU [26]

65.7

ESPnet-SLU w/ Conformer Enc. + ASR Multi-task

57.5

+ Pretrained ASR Wav2vec2 [20]

67.6

+ Pretrained ASR HuBERT [19]

70.0

Table 7. Dialogue Act Classiﬁcation accuracy results on the SWB Dataset [7] showing the impact of using spoken dialog contexts.

Model

DA (% Acc)

Pretrained ASR + NLU w/ 2 utt. context [53]

68.7

Baseline E2E-SLU [7]

50.9

ESPnet-SLU w/ Conformer

52.9

+ 3 utterance context

54.9

+ Pretrained ASR HuBERT [19]

67.5

improve performance when used as feature extractors in an E2E SLU system. We also analyzed the impact of pretrained NLU systems to incorporate semantic information. However, we observed no gains in performance. This analysis shows that researchers can use our toolkit to compare the utility of different pretrained ASR and NLU systems as feature extractors (See Section 2) for intent classiﬁcation. We also perform the Slot Filling (Entity Classiﬁcation) task on SLURP [4] dataset. As shown in Table 2, we outperform the previous best SLU-F1 [4] performance.
Non English SLU (IC) i.e., for Dutch (Grabo Dataset [12]) and Mandarin (CAT-SLU MAP [27]). For CAT-SLU, we use multilingual pretrained ASR model XLSR-53 [52] as the frontend, whereas we do not use any pretrained ASR models for the Grabo dataset. To simulate a low resource setting discussed in Section 2 in the Grabo dataset, we do not concatenate the transcript with intent and are still able to outperform the no pretrained ASR results reported in [42].
Other Corpora (IC) The performance for other intent classiﬁcation datasets is shown in Table 2, demonstrating the broad coverage of our system. Google Speech Commands [44] is a dataset used to train a limited domain ASR system on which we were able to outperform prior best performance. We were able to match intent classiﬁcation results on the HarperValleyBank corpus [41] which is a corpus of spoken dialog between an agent and a customer of a bank.

4.2. Emotion Recognition (ER)
Emotion Recognition is also modeled as a conditional prediction task where we infer the ﬁrst word as the emotion class. We conduct our experiments on IEMOCAP [6] dataset using the four classes (neutral, happy, sad, angry). Model comparisons were made based on the split using Sessions 1–4 as a training set and Session 5 as a test set. As seen in Table 6, using a pretrained HuBERT model before the conformer encoder performs the best. Next, we compare the accuracy of this model with the one reported in [47] based on 5-fold cross-validation in Table 2, and achieve competitive performance.
4.3. Dialogue Act Classiﬁcation (DA)
Dialogue Act classiﬁcation is modeled similar to the intent classiﬁcation task. Given an utterance, the system has to classify it to

100

90

Noisy speech accuracy Enhanced speech accuracy

80

FSC accuracy

70

60

50

0

2

4 SNR (in D6B)

8

10

Fig. 1. Intent classiﬁcation accuracy on the FSC dataset against the

Signal-to-Noise Ratio (SNR) of noisy speech. This plot indicates

that applying Speech Enhancement (SE) before running our SLU

model reduces the performance drop with no-noise speech.

one of the DA classes, such as statement, question, etc. We conduct our experiments on NXT-format Switchboard Corpus that annotates Switchboard telephone speech corpus [45] with 42 DA classes [46]. Since it has been reported that context is important for DA classiﬁcation [54, 55], we also extend each utterance by simple concatenation with the acoustic signal from 3 preceding utterances to provide context (see Section 2) which improves the accuracy by 2%. Furthermore, using pretrained HuBERT ASR models increases the accuracy to 67.7%, which is close to baseline accuracy on this dataset.
4.4. Noisy Intent Classiﬁcation (IC) with Speech Enhancement
As discussed in Section 2, ESPnet already has the implementation of various speech processing tasks like ASR, SE, and many more. This experiment explores the effectiveness of supporting numerous tasks in a single toolkit by using multiple ASR tasks in a pipeline manner. We test our hypothesis on the Fluent Speech Command dataset. We ﬁrst convert the audio ﬁles into noisy speech by adding real-world noise [56]. We computed the intent classiﬁcation performance using our already trained model on clean audio ﬁles and observed a signiﬁcant drop in performance in Figure 1. The noise ﬁles were then passed through a speech enhancement model trained on CHIME4 [57] dataset. We observe a signiﬁcant improvement in intent classiﬁcation performance on these enhanced audio ﬁles, highlighting the advantage of having multiple tasks in a uniﬁed toolkit.
5. CONCLUSION
We present ESPnet-SLU, a new open-source E2E-SLU toolkit, with the objective of facilitating fast research and development of SLU systems through standardized recipes for various benchmarks containing data preparation, training, and model evaluation. ESPnetSLU contains recipes for over 10 diverse SLU corpora, encompassing multiple languages and task types, with performance nearing or exceeding the prior state-of-the-art. Furthermore, our design is a modular extension of the popular ESPnet toolkit with access to the entire pre-existing infrastructure of various speech processing tasks, models and architectures. In the future, we will support more corpora and implement more SLU systems like NLU multi-tasking to further advance the performance of our SLU systems.
6. ACKNOWLEDGEMENTS
This work used the Extreme Science and Engineering Discovery Environment (XSEDE), which is supported by NSF grant number ACI1548562. Speciﬁcally, it used the Bridges system, supported by NSF grant ACI-1445606, at the PSC.

7. REFERENCES
[1] D. Yu, M. Cohn, Y. M. Yang, et al., “Gunrock: A social bot for complex and engaging long conversations,” in Proc. EMNLP, 2019.
[2] A. Coucke, A. Saade, A. Ball, et al., “Snips voice platform: An embedded spoken language understanding system for private-by-design voice interfaces,” CoRR, vol. abs/1805.10190, 2018.
[3] L. Lugosch, M. Ravanelli, P. Ignoto, et al., “Speech model pretraining for end-to-end slu,” in Proc. Interspeech, 2019.
[4] E. Bastianelli, A. Vanzo, P. Swietojanski, et al., “SLURP: A spoken language understanding resource package,” in Proc. EMNLP, 2020.
[5] M. Del Rio, N. Delworth, R. Westerman, et al., “Earnings-21: A Practical Benchmark for ASR in the Wild,” in Proc. Interspeech, 2021.
[6] C. Busso, M. Bulut, C.-C. Lee, et al., “IEMOCAP: interactive emotional dyadic motion capture database,” ELRA, pp. 335–359, 2008.
[7] D. Ortega and N. T. Vu, “Lexico-acoustic neural-based models for dialog act classiﬁcation,” in Proc. ICASSP, 2018, pp. 6194–6198.
[8] A. Stolcke, K. Ries, N. Coccaro, et al., “Dialogue act modeling for automatic tagging and recognition of conversational speech,” Computational Linguistics, vol. 26, no. 3, pp. 339–371, 2000.
[9] B. Agrawal, M. Mu¨ller, M. Radfar, et al., “Tie your embeddings down: Cross-modal latent spaces for end-to-end spoken language understanding,” arXiv preprint arXiv:2011.09044, 2020.
[10] M. Saxon, S. Choudhary, J. P. McKenna, et al., “End-to-End Spoken Language Understanding for Generalized Voice Assistants,” in Proc. Interspeech, 2021, pp. 4738–4742.
[11] J. Ganhotra, S. Thomas, H.-K. J. Kuo, et al., “Integrating Dialog History into End-to-End Spoken Language Understanding Systems,” in Proc. Interspeech, 2021, pp. 1254–1258.
[12] V. Renkens, S. Janssens, B. Ons, et al., “Acquisition of ordinal words using weakly supervised NMF,” in Proc. SLT, 2014, pp. 30–35.
[13] A. Saade, A. Coucke, A. Caulier, et al., “Spoken language understanding on the edge,” vol. abs/1810.12735, 2018.
[14] S. Watanabe, T. Hori, S. Karita, et al., “Espnet: End-to-end speech processing toolkit,” in Proc. Interspeech, 2018, pp. 2207–2211.
[15] T. Hayashi, R. Yamamoto, K. Inoue, et al., “Espnet-tts: Uniﬁed, reproducible, and integratable open source end-to-end text-to-speech toolkit,” in Proc. ICASSP, 2020, pp. 7654–7658.
[16] H. Inaguma, S. Kiyono, K. Duh, et al., “Espnet-st: All-in-one speech translation toolkit,” in ACL System Demonstrations, 2020.
[17] D. S. Park, W. Chan, Y. Zhang, et al., “Specaugment: A simple data augmentation method for automatic speech recognition,” in Proc. Interspeech, 2019, pp. 2613–2617.
[18] A. Gulati, J. Qin, C.-C. Chiu, et al., “Conformer: Convolutionaugmented transformer for speech recognition,” in Proc. Interspeech, 2020, pp. 5036–5040.
[19] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, et al., “Hubert: Self-supervised speech representation learning by masked prediction of hidden units,” CoRR, vol. abs/2106.07447, 2021.
[20] A. Baevski, Y. Zhou, A. Mohamed, et al., “Wav2vec 2.0: A framework for self-supervised learning of speech representations,” in Proc. NeurIPS, 2020.
[21] A. T. Liu, S.-W. Li, and H.-y. Lee, “TERA: self-supervised learning of transformer encoder representation for speech,” IEEE ACM Trans. Audio Speech Lang. Process., vol. 29, pp. 2351–2366, 2021.
[22] Y.-A. Chung, H. Tang, and J. R. Glass, “Vector-quantized autoregressive predictive coding,” in Proc. Interspeech, 2020, pp. 3760–3764.
[23] J. Devlin, M.-W. Chang, K. Lee, et al., “BERT: pre-training of deep bidirectional transformers for language understanding,” in Proc. NAACL, 2019, pp. 4171–4186.
[24] K. Song, X. Tan, T. Qin, et al., “Mpnet: Masked and permuted pretraining for language understanding,” in Proc. NeurIPS, 2020.
[25] S. Cha, W. Hou, H. Jung, et al., “Speak or chat with me: End-to-end spoken language understanding with ﬂexible inputs,” arXiv, 2021.
[26] M. Ravanelli, T. Parcollet, P. Plantinga, et al., “Speechbrain: A general-purpose speech toolkit,” CoRR, vol. abs/2106.04624, 2021.
[27] X. Zhang and L. He, “End-to-end cross-lingual spoken language understanding with multilingual pretraining,” Proc. Interspeech, 2021.

[28] K. Yoshino, H. Tanaka, K. Sugiyama, et al., “Japanese dialogue corpus of information navigation and attentive listening annotated with extended ISO-24617-2 dialogue act tags,” in Proc. LREC 2018, 2018.
[29] Y. Karunanayake, U. Thayasivam, and S. Ranathunga, “Transfer learning based free-form speech command classiﬁcation for lowresource languages,” in Proc. ACL, 2019, pp. 288–294.
[30] C. Li, J. Shi, W. Zhang, et al., “Espnet-se: End-to-end speech enhancement and separation toolkit designed for ASR integration,” in Proc. SLT, 2021, pp. 785–792.
[31] S. Ultes, L. M. Rojas Barahona, P.-H. Su, et al., “PyDial: A Multidomain Statistical Dialogue System Toolkit,” in ACL, 2017.
[32] Uber Technologies Inc., Plato Research Dialogue System, 2022.
[33] Rasa Technologies, Rasa, 2022.
[34] W.-C. Huang, T. Hayashi, S. Watanabe, et al., “The sequence-tosequence baseline for the voice conversion challenge 2020: Cascading ASR and TTS,” CoRR, vol. abs/2010.02434, 2020.
[35] M. Li, X. Liu, W. Ruan, et al., “Multi-task learning of spoken language understanding by integrating n-best hypotheses with hierarchical attention,” in Proc. COLING, 2020, pp. 113–123.
[36] C.-I. Lai, Y.-S. Chuang, H.-Y. Lee, et al., “Semi-supervised spoken language understanding via self-supervised speech and language model pretraining,” in Proc. ICASSP, 2021, pp. 7468–7472.
[37] P. Wang and H. V. hamme, Pre-training for low resource speech-tointent applications, 2021.
[38] S. Kim, S. Dalmia, and F. Metze, “Gated embeddings in e2e speech recognition for conversational-context fusion,” in Proc. ACL, 2019.
[39] ——, “Cross-Attention End-to-End ASR for Two-Party Conversations,” in Proc. Interspeech, 2019, pp. 4380–4384.
[40] S. Arora, A. Ostapenko, V. Viswanathan, et al., “Rethinking end-toend evaluation of decomposable tasks: A case study on spoken language understanding,” in Proc. Interspeech, 2021.
[41] M. Wu, J. Nafziger, A. Scodary, et al., “Harpervalleybank: A domainspeciﬁc spoken dialog corpus,” CoRR, vol. abs/2010.13929, 2020.
[42] Y. Tian and P. J. Gorinski, “Improving end-to-end speech-to-intent classiﬁcation with reptile,” in Proc. Interspeech, 2020, pp. 891–895.
[43] S. Zhu, Z. Zhao, T. Zhao, et al., “CATSLU: the 1st chinese audiotextual spoken language understanding challenge,” in ICMI, 2019.
[44] P. Warden, “Speech commands: A dataset for limited-vocabulary speech recognition,” arXiv preprint arXiv:1804.03209, 2018.
[45] J. J. Godfrey, E. C. Holliman, and J. McDaniel, “SWITCHBOARD: Telephone speech corpus for research and development,” in Proc. ICASSP, vol. 1, 1992, pp. 517–520.
[46] D. Jurafsky and E. Shriberg, “Switchboard swbd-damsl shallowdiscourse-function annotation coders manual,” 1997.
[47] S.-w. Yang, P.-H. Chi, Y.-S. Chuang, et al., “SUPERB Speech Processing Universal PERformance Bench.,” in Proc. Interspeech, 2021.
[48] S. Watanabe, T. Hori, S. Kim, et al., “Hybrid ctc/attention architecture for end-to-end speech recognition,” JSTSP, 2017.
[49] S. Karita, X. Wang, S. Watanabe, et al., “A comparative study on transformer vs RNN in speech applications,” in Proc. ASRU, 2019.
[50] M. Ott, S. Edunov, A. Baevski, et al., “Fairseq: A fast, extensible toolkit for sequence modeling,” in Proc. NAACL-HLT Demo., 2019.
[51] T. Wolf, L. Debut, V. Sanh, et al., “Transformers: State-of-the-art natural language processing,” in Proc. EMNLP, 2020, pp. 38–45.
[52] A. Conneau, A. Baevski, R. Collobert, et al., “Unsupervised crosslingual representation learning for speech recognition,” CoRR, vol. abs/2006.13979, 2020.
[53] D. Ortega, C.-Y. Li, G. Vallejo, et al., “Context-aware neural-based dialog act classiﬁcation on automatically generated transcriptions,” in Proc. ICASSP, 2019, pp. 7265–7269.
[54] J. Y. Lee and F. Dernoncourt, “Sequential short-text classiﬁcation with recurrent and convolutional neural networks,” in Proc. NAACL, 2016.
[55] D. Ortega and N. T. Vu, “Neural-based context representation learning for dialog act classiﬁcation,” in Proc. SIGDIAL, 2017, pp. 247–252.
[56] G. Wichern, J. Antognini, M. Flynn, et al., “Wham!: Extending speech separation to noisy environments,” in Proc. Interspeech, 2019.
[57] E. Vincent, S. Watanabe, A. A. Nugraha, et al., “An analysis of environment, microphone and data simulation mismatches in robust speech recognition,” CSL, 2017.

