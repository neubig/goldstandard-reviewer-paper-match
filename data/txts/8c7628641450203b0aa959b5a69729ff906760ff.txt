JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

1

Encoder-Decoder Based Attractors for End-to-End Neural Diarization
Shota Horiguchi, Member, IEEE, Yusuke Fujita, Member, IEEE, Shinji Watanabe, Senior Member, IEEE, Yawen Xue, Paola Garc´ıa, Member, IEEE,

arXiv:2106.10654v2 [eess.AS] 28 Mar 2022

Abstract—This paper investigates an end-to-end neural diarization (EEND) method for an unknown number of speakers. In contrast to the conventional cascaded approach to speaker diarization, EEND methods are better in terms of speaker overlap handling. However, EEND still has a disadvantage in that it cannot deal with a ﬂexible number of speakers. To remedy this problem, we introduce encoder-decoder-based attractor calculation module (EDA) to EEND. Once frame-wise embeddings are obtained, EDA sequentially generates speaker-wise attractors on the basis of a sequence-to-sequence method using an LSTM encoder-decoder. The attractor generation continues until a stopping condition is satisﬁed; thus, the number of attractors can be ﬂexible. Diarization results are then estimated as dot products of the attractors and embeddings. The embeddings from speaker overlaps result in larger dot product values with multiple attractors; thus, this method can deal with speaker overlaps. Because the maximum number of output speakers is still limited by the training set, we also propose an iterative inference method to remove this restriction. Further, we propose a method that aligns the estimated diarization results with the results of an external speech activity detector, which enables fair comparison against cascaded approaches. Extensive evaluations on simulated and real datasets show that EEND-EDA outperforms the conventional cascaded approach.
Index Terms—Speaker diarization, EEND, EDA
I. INTRODUCTION
S PEAKER diarization is a task of estimating multiple speakers’ speech activities from input audio (sometimes referred to as the “who spoke when” problem) [1]. It can be placed as a downstream task of automatic speech recognition (ASR), in which speaker information is tagged to each transcribed utterance [2]–[4]. It can also be used as a prior step to speech separation and the following ASR. For example, in guided source separation [5], speech activities are used as constraints to update time-frequency masks of a complex angular central Gaussian mixture model. The speech-activitydriven speech-extraction neural network [6] takes acoustic features and a target speaker’s speech activity to perform fully neural speech separation.
Classical cascaded methods treat speaker diarization as a partition problem. Given a set of time frames, they ﬁrst detect
S. Horiguchi and Y. Xue are with Hitachi, Ltd. Y. Fujita is with LINE corporation. This work had been done during he was with Hitachi, Ltd. S. Watanabe is with Carnegie Mellon University. P. Garc´ıa is with Johns Hopkins University. ©2022 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works.

speaker-active frames and then divide them into clusters by using speaker embeddings extracted with a sliding window. The number of clusters, which represents the number of speakers, is determined in the clustering step during inference. Eigen value analysis on the graph Laplacian of a similarity matrix calculated from frame-wise embeddings is one way to estimate the number of speakers explicitly [7], [8]. If agglomerative hierarchical clustering is employed as a clustering algorithm, a threshold value is usually preset, and the number of clusters, i.e., the number of speakers, is dynamically determined by the threshold value [9]. Either way, the number of clusters can be set ﬂexibly during inference. However, there is one fundamental problem that it basically cannot handle speaker overlaps because each speech frame is usually assigned to one speaker.
Some neural-network-based end-to-end methods, in comparison, naturally handle speaker overlap with a single network. For example, the Recurrent Selective Attention Network (RSAN) [10], [11] decodes speech activity for each speaker one by one until a stopping condition is satisﬁed. However, it requires clean speech to be trained as a mask-based speech separation model. End-to-end neural diarization (EEND) [12]– [14], which estimates multiple speakers’ speech activities at once from input audio, does not require such clean speech for training. The limitation is that the original EEND ﬁxes the output number of speakers; thus, knowing the number of speakers in advance is a requirement.
In our previous study [15], we introduced an encoderdecoder-based attractor calculation module (EDA) as part of the self-attentive EEND model [13] to handle unknown numbers of speakers (EEND-EDA). It calculates attractors from frame-wise embeddings using a sequence-to-sequence method with an LSTM encoder-decoder; thus, the number of attractors can be ﬂexible. In general, sequence-to-sequence methods require a stopping criterion in their decoding process. To decide when to stop the attractor calculation, EDA also estimates whether each calculated attractor really corresponds to a speaker. The diarization results are calculated as dot products between the attractors and frame-wise embeddings. Despite being designed for the diarization of ﬂexible numbers of speakers, it also has performed better than the original EEND under ﬁxed-number-of-speakers conditions. Compared with other EEND extensions for unknown numbers of speakers [16], [17], it performed the best on various datasets including the CALLHOME and DIHARD III datasets [18]. Several studies have also proposed extensions to EEND-EDA to allow online processing [19], [20].

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

2

In this paper, we revisit EEND-EDA with more comprehensive discussions and formulations and propose several extensions from the original EEND-EDA presented in [15]. The modiﬁcations from the original EEND-EDA study are summarized as follows:
• We discuss the relationship between the original EEND and EEND-EDA, which explains EEND-EDA’s better performance in a ﬁxed-number-of-speakers evaluation.
• We also propose reﬁning the training strategy of EENDEDA, which resulted in a 2.41 % DER improvement on the CALLHOME dataset from the original paper [15].
• In the history of diarization studies, it has been difﬁcult to compare the results of cascaded approaches and EENDbased approaches because the former ones are often evaluated with an oracle speech activity detection (SAD), while EENDs operate SAD and diarization simultaneously. To conduct fair comparisons between cascaded and EEND-based approaches, this paper introduces SAD post-processing to align diarization results from EENDEDA with external SAD results.
• We also propose an iterative inference for handling the problem of the number of outputs of EEND-EDA being empirically limited by its training dataset.
• We conduct thorough evaluations and analyses on simulated and real datasets including CALLHOME, CSJ, AMI, DIHARD II, and DIHARD III.
II. RELATED WORK
A. Speaker diarization
Conventional diarization methods are typically a cascade of four modules: 1) speech activity detection (SAD), 2) speaker embedding extraction, 3) embedding clustering, and 4) overlap handling as an optional process. Some methods also include an ASR module [21], [22]. Most studies mainly focus on 2) speech embedding extraction and 3) embedding clustering. For speaker embeddings, i-vectors [23], [24], x-vectors [25]–[27], and d-vectors [7], [28] have been explored. For embedding clustering, earlier works used traditional clustering algorithms, e.g., K-means clustering [29], [30], agglomerative hierarchical clustering (AHC) [9], [31], [32], mean-shift clustering [23], and spectral clustering [7], [33]. Recently, better clustering methods have been proposed, such as variational Bayes hidden Markov model clustering (VBx) [34], [35], auto-tuning spectral clustering [8], or fully supervised clustering [28], [36]. They are usually used for hard clustering, so most cascaded methods (with some exceptions [37]) cannot deal with speaker overlap. To make them able to treat speaker overlap, 4) overlap handling should be considered; however, it has sometimes been excluded from methods and evaluations even in very recent studies [7], [8], [24], [28], [36]. Moreover, 1) speech activity detection has often been ignored in evaluations of cascaded approaches that use oracle speech activities [7], [8], [24], [28], [36].
Neural-network-based methods that directly produce diarization results from audio are emerging [10], [11]. One strength of such methods is that they require no extra modules for SAD or overlap handling. For some methods, models have

been trained for speech separation, and diarization results have been obtained as byproducts [10], [11]. Such models have been trained on the basis of clean speech (or time-frequency masks calculated from clean speech); thus, they cannot be trained on real mixtures like DIHARD datasets [38], [39]. However, EEND-based models are trained to output multiple speakers’ speech activities; they do not require clean speech for training and real mixtures can be used. The original EEND [12]–[14] can output diarization results for a ﬁxed number of speakers. To extend the EEND for an unknown number of speakers, two approaches have been investigated. One is an attractor-based approach [15], [19], and the other is a speaker-wise conditional EEND (SC-EEND) [16], [17]. In this paper, we investigate the attractor-based EEND because it showed better performance compared to SC-EEND.
B. Speech processing based on neural networks for unknown numbers of speakers
While some methods have achieved promising results with a ﬁxed number of output speakers in diarization [12], [13], [40] and speech separation [41]–[44] contexts, it is challenging to make them able to deal with unknown numbers of speakers. The difﬁculty of neural-network-based speech processing for unknown numbers of speakers is that we cannot ﬁx the output dimension.
One possible approach is to determine the maximum number of speakers to decode. In this case, the number of outputs is set to a sufﬁciently large value. Some methods treat a ﬂexible number of speakers by outputting null speech activities if the number of outputs is smaller than the network capacity [45]. However, this approach did not work well with EEND (see [16]). In other methods, the number-of-speaker-wise output branches are trained independently, and the most probable is used during inference [46]. In this case, we have to know the maximum number of speakers. One of the strengths of EEND is that it can be ﬁnetuned using a target domain dataset from a pretrained model, but we usually cannot access the maximum number of speakers of the target domain beforehand. Therefore, a method that does not require that the maximum number of speakers be deﬁned would be preferable.
Another approach is to decode speakers one by one until a stopping condition is satisﬁed, like SC-EEND [16]. For speech separation, RSAN [10], [11] and one-and-rest permutation invariant training (OR-PIT) [47] can be used. The key difference between speech separation and diarization is whether or not the residual output can be deﬁned. RSAN uses a mask-based approach, in which each time-frequency bin is softly assigned to each speaker so that the process ﬁnishes when all the elements of the residual mask become zero. OR-PIT is time-domain speech separation by which residual output is determined as a mixture that contains other speakers rather than the target speaker. Both require clean recordings to determine oracle masks or signals. However, they are not always accessible in the diarization context, in which only multi-talker recordings and speech segments are provided.
In this paper, we adopted an attractor-based approach like deep attractor networks (DANet) [45], [48]. While the number

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

3

of speakers [48] or maximum number of speakers [45] is ﬁxed for the original DANet, in this paper, we calculated a ﬂexible number of attractors without deﬁning them.

C. Neural-network-based representative vector calculation
There have been several efforts to calculate representative vectors from a sequence of embeddings in an end-to-end trainable fashion. For example, Set Transformer [49] enables setto-set transformation, which can be used to calculate cluster centroids from a set of embeddings. However, the number of outputs has to be known in advance, so it cannot be used for our purpose. Meier et al. proposed an end-to-end clustering framework [50], in which clustering for all possible number of clusters K ∈ {1, . . . , Kmax} is performed and the result of the most probable number of clusters is used. The framework performs the clustering of a ﬂexible number of clusters in an end-to-end manner, but the maximum number of clusters is limited by Kmax. EDA in this paper, in comparison, determines a ﬂexible number of attractors from an input embedding without prior knowledge of the number of speakers. Thus, we can use datasets of the different maximum number of speakers during pretraining and ﬁnetuning.
III. METHOD
In this section, we ﬁrst introduce the conventional EEND in Section III-A followed by an explanation of a natural extension of the method called attractor-based EEND in Section III-B. We also provide novel inference techniques in Section III-C.

A. Conventional end-to-end neural diarization
End-to-end neural diarization (EEND) [12], [13] is a method for estimating multiple speakers’ speech activities simultaneously from an input recording. Given frame-wise F dimensional acoustic features (xt)Tt=1, where t ∈ {1, . . . , T } is a frame index, EEND estimates speech activities (yt)Tt=1. Here, yt := [y1,t, . . . , ys,t, . . . , yS,t]T denotes speech activities of S speakers at t deﬁned as

0 (Speaker s is inactive at t)

ys,t =

.

(1)

1 (Speaker s is active at t)

EEND assumes that ys,t is conditionally independent given the acoustic features, namely,

TS

P (y1, . . . , yT | x1, . . . , xT ) =

P (ys,t | x1, . . . , xT ) .

t=1 s=1
(2)

With this assumption, speaker diarization can be regarded as a multi-label classiﬁcation problem and can thus be easily modeled using a neural network fEEND as

(p1, . . . , pT ) = fEEND (x1, . . . , xT ) ,

(3)

where pt := [p1,t, . . . , pS,t]T ∈ (0, 1)S is the posterior
probabilities of S speakers’ speech activities at frame index t. The estimation of speech activities (yˆt)Tt=1 is

yˆ1, . . . , yˆT = arg max P (y1, . . . , yT | x1, . . . , xT ) , (4) y1,...,yT

= (1 (ps,t > 0.5)) 1≤s≤S ,

(5)

1≤t≤T

where 1 (cond) is an indicator function that returns 1 if cond is satisﬁed and 0 otherwise. Note that the threshold value in
(5) is always set to 0.5 in this paper for simplicity. The conventional EEND is implemented as a composition
of an embedding part g : RF ×T → RD×T and a classiﬁcation part h : RD×T → (0, 1)S×T , i.e.,

fEEND = h ◦ g.

(6)

The ﬁrst embedding part g converts input acoustic features into D-dimensional frame-wise embeddings. It is implemented with N -stacked encoders, each of which converts a ﬂexible length of embedding sequence (e(tn−1))Tt=1 into the same length of embedding sequence (e(tn))Tt=1 as

e(1n), . . . , e(Tn) = g(n) e(1n−1), . . . , e(Tn−1) ,

(7)

e(t0) = xt (1 ≤ t ≤ T ),

(8)

where g(n) is the n-th encoder layer. As examples of encoders,
bi-directional long short-term memories (BLSTM) [12] and
Transformers [13] are exploited in the conventional studies.
In this paper, we used Transformer encoders but without posi-
tional encodings to prevent the outputs from being affected by
the absolute position of the frames. Hereafter, for simplicity, we use et to denote the embeddings from the last encoder, i.e., et := et(N) for t ∈ {1, . . . , T }.
Then, the classiﬁcation part h in (6) converts the embeddings (et)Tt=1 to posteriors of speech activities (pt)Tt=1 in (3). It is implemented by using a fully connected layer and an element-wise sigmoid function σ(·) that takes a tensor as an
argument:

[p1, . . . , pT ] = h(e1, . . . , eT ; Wcls, bcls) = σ WcTls [e1, . . . , eT ] + bcls1TD

(9) ∈ (0, 1)S×T ,
(10)

where (·)T denotes the matrix transpose, 1D is D-dimensional all-one vector, and Wcls ∈ RD×S and bcls ∈ RS are the weight and bias of the fully connected layer, respectively.
EEND outputs posteriors of multiple speakers simultane-
ously but without any conditions to decide the order of the
speakers. Such a network is optimized by using a permutation-
free objective [41], [51], which was originally proposed for
multi-talker speech separation. It computes the loss for all possible speaker assignments between predictions (pt)Tt=1, as introduced in (3), and groundtruth labels (yt)Tt=1, and it picks the minimum one for backpropagation as follows.

1

T φ

Ldiar =

min

T S φ∈Φ(S)

H yt , pt ,

(11)

t=1

where Φ (S) is a set of all possible permutations of the sequence (1, . . . , S), φ := (φ1, . . . , φS) is the permuted sequence, yφt := [yφ1,t, . . . , yφS,t]T ∈ {0, 1}S is the permuted groundtruth labels using φ, and H (·, ·) is the binary cross
entropy deﬁned as

S
H (yt, pt) := {−ys,t log ps,t − (1 − ys,t) log (1 − ps,t)} .
s=1
(12)

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

4

Labels

...

Posteriors of

...

speech activity

Sigmoid

××

×

LSTM encoder

...

Order shuffling

Embeddings Acoustic features

...
SA-EEND
...

1 1 ... 1 0 Labels

...

Attractor existence

probabilities

Linear + Sigmoid
...

Attractors

...

LSTM decoder

...

Zero vectors

Encoder-Decoder Based Attractor Calculation

Fig. 1. EEND with encoder-decoder-based attractor calculation (EENDEDA).

Compared with cascaded approaches, EEND has two significant strengths. One is that the cascaded approaches conduct diarization by dividing frame-wise speaker embeddings, so they require SAD as pre-processing and overlap detection and assignment as post-processing. In contrast, EEND estimates each speaker’s speech activities independently, so no extra modules for speech activity detection and overlap detection are needed. The other strength is that the EEND model can be adapted to the desired domain’s dataset, while cascaded approaches typically tune only probabilistic linear discriminant analysis (PLDA) parameters to optimize intra- and interspeaker similarity between speaker embeddings [9], [18], [52].
B. Attractor-based end-to-end neural diarization
The limitation of the conventional EEND is in the classiﬁcation part h in (6); the number of output speakers S is ﬁxed by the fully connected layer as in (10). One possible way to treat a ﬂexible number of speakers with this ﬁxed-output architecture is to set the number of outputs to be large enough. However, as discussed in Section II-B, it requires knowing the maximum number of speakers in advance, and it has been already veriﬁed that such a strategy results in poor performance (see [16]). It is also a problem that the calculation cost of the permutationfree loss increases if we set a large number of speakers to be output. Therefore, a signiﬁcant research question is how to output diarization results for a ﬂexible number of speakers.
In this paper, we extend the conventional EEND to handle a ﬂexible number of speakers. We assume that the embedding part g in (6) is implemented in the same manner as the conventional EEND described in Section III-A. Given frame-wise D-dimensional embeddings {et}Tt=1, our goal is to produce posteriors for a ﬂexible number of speakers in the classiﬁcation part h. To achieve this goal, we propose a method to calculate a ﬂexible number of speaker-wise attractors from embeddings and then calculate diarization results on the basis of attractors and embeddings. The proposed method is depicted in Figure 1.
1) EDA: Encoder-decoder-based attractor calculation: EDA converts frame-wise embeddings into speaker-wise attractors using a sequence-to-sequence method with an LSTM

encoder-decoder. The LSTM encoder henc takes the framewise embeddings as input and updates its hidden state hetnc and cell state cetnc as

hetnc, cetnc = henc et, het−nc1, cet−nc1

(t = 1, . . . , T ) . (13)

The hidden and cell states of the encoder are initialized with zero vectors, i.e., he0nc = ce0nc = 0. The LSTM decoder hdec
estimates speaker-wise attractors as

hdsec, cdsec = hdec 0, hds−ec1, cds−ec1

(s = 1, 2, . . . ) . (14)

We treat the hidden state at each step hdsec =: as ∈ (−1, 1)D as speaker s’s attractor, whose dimensionality D is the same as that of the frame-wise embeddings et. The hidden and cell states of the decoder are initialized by the ﬁnal hidden and
cell states of the encoder as

hd0ec = heTnc,

(15)

cd0ec = ceTnc,

(16)

which is shown as a right arrow from the LSTM encoder to the LSTM decoder in Figure 1. In general applications of a sequence-to-sequence method, e.g., speech recognition or machine translation, the output is sentences, i.e., a sequence of words, so the order of output is ﬁxed. However, EDA cannot determine the order of output speakers in advance because this order is determined by minimizing cross entropy as in (11). Even if the order could be predetermined, it would not be possible to determine the optimal attractor outputs. Thus, the well-known strategy of teacher forcing, for which the optimal outputs with their order have to be known in advance, cannot be used. Furthermore, the s-th attractor can correspond to any speaker that is not contained in the ﬁrst (s − 1) attractors. To make this attractor calculation procedure fully order-free, we input a zero vector as input at each step as in (14). Using zero vectors as inputs provides ﬂexibility to change the number of output speakers across pretraining and ﬁnetuning rather than using, for example, trainable parameters. This is why we chose an LSTM-based encoder-decoder rather than Transformer encoder-decoder, which requires input queries rather than zero vectors.
Here, the input order to the EDA encoder affects the output attractors because EDA is based on a sequence-to-sequence method. To investigate the effect of the input order, we tried two types of input orders: chronological and shufﬂed orders. In the chronological order setting, embeddings are input in the order of frame indexes as in (13). In the shufﬂed order setting, we use the following instead of (13) :

hetnc, cetnc = henc eψt , het−nc1, cet−nc1

(t = 1, . . . , T ) , (17)

where (ψ1, . . . , ψT ) is a randomly chosen permutation of (1, . . . , T ).
The diarization results pt in (3) are calculated on the basis of the dot product of the frame-wise embeddings and speakerwise attractors (⊗ in Figure 1):

pt = σ ATet ∈ (0, 1)S ,

(18)

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

5

where A := [a1, . . . , aS] are the speaker-wise attractors. The posteriors are optimized by using (11) in the same manner as the conventional EEND. This posterior calculation no longer depends on the fully connected layer, which determines the output number of speakers as in (10); therefore, EDA-based diarization can vary the output number of speakers.
Comparing (10) and (18), the conventional EEND can also be regarded as using ﬁxed attractors Wcls (with bias bcls). In comparison, EDA calculates attractors from an input sequence of embeddings, which makes attractors adaptive to the embeddings. This makes EEND-EDA more accurate even under the ﬁxed-number-of-speakers condition (see Table III).
2) Attractor existence probability: As in (14), we can obtain an inﬁnite number of attractors. To decide when to stop the attractor calculation, we calculate the attractor existence probabilities from the calculated attractors by using a fully connected layer followed by sigmoid activation:

qs = σ wTexistas + bexist ,

(19)

where wexist ∈ RD and bexist ∈ R are trainable weights and bias parameters of the fully connected layer, respectively.
During training, we know the oracle number of speakers S, so the training objective of the attractor existence probabilities is based on the ﬁrst (S +1)-th attractors using the binary cross entropy deﬁned in (12):

1 Lexist = S + 1 H (l, q) , (20)

where

l := [1, . . . , 1, 0]T,

(21)

S

q := [q1, . . . , qS+1]T .

(22)

The total loss is deﬁned as the weighted sum of Ldiar in (11) and Lexist in (20) with the weighting parameter α ∈ R+ as

L = Ldiar + αLexist.

(23)

In this paper, we use α = 1. This multi-task loss aims to optimize frame- and speaker-wise posteriors with Ldiar and attractor existence probabilities with Lexist.
While (23) was used for the network optimization in our previous study [15], we found that the optimization of Lexist inhibits the minimization of Ldiar during the training of a model with a ﬂexible number of speakers, which is more important for improving diarization accuracy. Therefore, when a ﬂexible number of speakers’ dataset is used for training, we use Lexist to update only the fully connected layer parameterized by wexist and bexist in (19). This can be implemented by cutting the graph before the fully connected layer to disable backpropagation to the preceding layers.
During inference, we cannot access the oracle number of speakers; thus, it is estimated using qs in (19) as follows.

Sˆ = min {s | s ∈ Z+ ∧ qs+1 < τ } ,

(24)

where τ ∈ (0, 1) is a thresholding parameter, which is set to 0.5 in this paper. We then use the ﬁrst Sˆ attractors to calculate
posteriors as in (18).

C. Inference methodology
1) SAD post-processing: Diarization methods, especially cascaded ones, are sometimes evaluated with oracle speech segments. When evaluated in such a way, the comparison between cascaded methods and EEND-methods becomes hard, mainly because EEND-based methods perform SAD and diarization simultaneously. One reason evaluations of cascaded approaches are mainly based on oracle speech segments is to consider speaker errors and SAD errors separately. It is reasonable to use oracle speech segments to focus on reducing speaker errors. However, such segments are not accessible in real scenarios, and the existence of SAD errors may worsen the clustering performance, which directly affects the diarization accuracy. Thus, we believe that SAD errors should also be considered in the context of cascaded methods. However, it is hard to say how accurate the SAD should be for a fair comparison between cascaded and EEND-based methods. Therefore, to align with the the cascaded methods, we introduce SAD post-processing for evaluating EEND. With this method, we can conduct a fair comparison between cascaded and EENDbased methods with the same SAD. Note that it can be used to improve the diarization performance by eliminating false alarm speech and recovering missed speech when an accurate external SAD system is given.
The SAD post-processing algorithm is described in Algorithm 1. Here, we assume that we have SAD results z1, . . . , zT in addition to frame- and speaker-wise posteriors p1, . . . , pT . We ﬁrst estimate speech activities as usual by using (5) (line 1). However, this estimation is not always consistent with SAD results. Thus, we ﬁrst ﬁlter false alarms (FA) by using SAD results. For each frame (line 2), if it is estimated that some speakers are active while the speech activity should be zero (line 3), we update the estimations with a zero vector (line 4). This procedure will always improve DER if z1, . . . , zT are the oracle speech activities. We also recover missed frames (MI) if no speaker is estimated as active while the speech activity is one (line 5). For each of such frames, we treat the speaker with the highest posterior as an active speaker (line 6–line 7). Including the oracle SAD as input will also improve the DER because missed-frame errors are replaced by correct estimation or at least speaker errors.
2) Iterative inference: Even if the model is trained to output a ﬂexible number of speakers, the output number of speakers is empirically limited by the maximum number of speakers in a recording observed during pre-training (see Table VII). How to output the results of more than N speakers even if the model is trained on at most N -speaker mixtures is still an open question. In this paper, we propose an iterative inference method to produce results for more than N speakers by applying EEND decoding with iterative frame selection.
Preliminarily, we ﬁrst reveal the characteristics of the EEND models that consist of stacked Transformer encoders and EDA. A Transformer encoder involves neither recurrence nor convolutional calculation, and we do not use positional encoding in this paper; thus, the embedding part g in (6) is an orderfree transformation. EDA contains an LSTM encoder-decoder, but if the order of the input sequence to EDA is shufﬂed, we

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

6

Algorithm 1: SAD post-processing.

Input : (p1, . . . , pT ) ∈ (0, 1)S×T

// Frame-wise

posteriors
(z1, . . . , zT ) ∈ {0, 1}T Output: (yˆ1, . . . , yˆT ) ∈ {0, 1}S×T

// SAD results // Speech activities

1 Compute yˆ1, . . . , yˆT using (5)

// Initial results

2 foreach t ∈ {1, . . . , T } do

3 if yˆt 1 > 0 ∧ zt = 0 then

4

yˆt ← [0, . . . , 0]T

// Filter FA

5 else if yˆt 1 = 0 ∧ zt = 1 then

6

s∗ ← arg maxs∈1,...,S pt

// Recover MI

7

yˆt ← [0, . . . , 0, 1 , 0, . . . , 0]T ∈ {0, 1}S

∧

s∗

Algorithm 2: Iterative inference.

Input : Output:

x1, . . . , xT fEEND Smax ∈ N // Max Yˆ ∈ {0, 1}S×T

#Speakers

// Acoustic features // EEND model
that EEND can output

1 T ← {1, . . . , T }

// Frame set

2 for n ← 1 to ∞ do

3 Compute Yˆ (n) by (25), (26), and (5)

// Decoding

4 Update T by (27)

// Silence frame selection

5 if S(n) < Smax ∨ |T | = 0 then

6

break

Yˆ (1)  7 Yˆ ←  ... 
Yˆ (n)

can say that EDA does not depend on the input order, so the EDA’s classiﬁcation part h in (6) is also an order-free function. Therefore, EEND-EDA does not depend on the order of the input features, which makes it possible to process features that are not extracted at equal intervals along the time axis, as in EEND as post-processing [53]. The proposed iterative inference also utilizes this characteristic.
Algorithm 2 shows the algorithm of iterative inference. In the algorithm, two processes are iteratively conducted: decoding and silence frame selection. Each process at the n-th iteration is described as follows.
1) Decoding (line 3): Acoustic features xt of the selected frames T are fed into EEND, and the corresponding posteriors p(tn) ∈ (0, 1)S(n) are obtained as

p(tn)

← fEEND (xt)t∈T ,

t∈T

(25)

where S(n) ∈ {0, . . . , Smax} is the number of decoded speakers. The posteriors of the frames that are not in T are set to zero as

p(tn) ← [0, . . . , 0]T (t ∈ {1, . . . , T } \ T ) . (26)

S(n)

1st iteration
Speaker 1 Speaker 2 Speaker 3
2nd iteration
Speaker 4 Speaker 5 Speaker 6

Decoding Decoding

Silence frame selection Silence frame selection

...

Fig. 2. Iterative inference in the case of Smax = 3.

With the posteriors p(tn) for t ∈ {1, . . . , T }, diarization

results Yˆ (n) =

yˆ

(n) 1

,

.

.

.

,

yˆ

(n T

)

are computed using

(5). Note that Yˆ (n) corresponds to the speech activities

of the ((n−1)Smax +1)-th through ((n−1)Smax +S(n))-

th speakers.

2) Silence frame selection (line 4): Given the diarization

results decoded at the n-th iteration, we select the frames

in which no speaker is active to update T as

T ← t t ∈ T , yˆ(tn) = 0 .

(27)

1

The above processes start with the initial value of T as the set of all frames {1, . . . , T } (line 1), and last until T becomes the empty set or when it is assumed that all the speakers are decoded (line 5–line 6). Here, we assume that all the speakers are decoded if the number of output speakers S(n) is smaller than the maximum output of EEND Smax.
After the iterative process is ﬁnished, the ﬁnal results Yˆ are obtained by concatenating the results calculated at each iteration (line 7). With iterative inference, the number of speakers to be decoded is no longer limited by the training dataset. The iterative inference workﬂow when Smax = 3 is also illustrated in Figure 2.
3) Iterative inference with DOVER-Lap (or iterative inference+): Despite iterative inference being able to produce more than Smax speakers’ speech activities, it has a potential problem in that the speech activities of two speakers decoded at different iterations never overlap. For example, the (Smax + 1)th speaker’s speech activities never overlap with those of the ﬁrst Smax speakers. This is because the frames in which the ﬁrst Smax speakers are active will not be processed in the second iteration. To ease this problem, we introduce DOVERLap [54], which is the extension of DOVER [55]. Both of them are methods for combining multiple diarization results on the basis of majority voting, but unlike DOVER, DOVER-Lap take speaker overlap into account. We used a modiﬁed version of DOVER-Lap presented in [18], in which the speaker assignment strategy when multiple speakers were ranked equally was slightly different from the original DOVER-Lap [54]. Note that we did not use a hypothesis-wise weighting of DOVER-Lap, which is also introduced in [18].
The algorithm of iterative inference incorporated with DOVER-Lap is shown in Algorithm 3. In this paper, we refer to this inference as iterative inference+. The difference from the iterative inference in Algorithm 2 is that we limit

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

7

Algorithm 3: Iterative inference with DOVER-Lap (or

iterative inference+).

Input : Output:

x1, . . . , xT fEEND Smax ∈ N // Max Yˆ ∈ {0, 1}S×T

#Speakers

// Acoustic features // EEND model
that EEND can output

1 for Slimit = 1 to Smax do

2 T ← {1, . . . , T }

// Frame set

3 for n ← 1 to ∞ do

4

Compute Yˆ (n) by (25), (26), (5)

// Decoding

5

if n = 1 then

6

Limit the number of speakers in Yˆ (n) by (28)

7

Update T by (27)

// Silence frame selection

8

if S(n) < Smax ∨ |T | = 0 then

9

break

Yˆ (1)  10 YˆSlimit ←  ... 
Yˆ (n)
11 Yˆ ← DOVER-Lap Yˆ1, . . . , YˆSmax

the number of speakers to decode at the ﬁrst iteration with
Slimit(≤ Smax) (line 5–line 6). After the decoding step at the ﬁrst iteration using (25), (26), and (5), we choose at most the ﬁrst Slimit speakers’ speech activities from Yˆ (1) := (yˆs,t)s,t as

Yˆ (1) ← (yˆs,t) 1≤s≤min(S(1),Slimit) .
1≤t≤T

(28)

The other procedures are the same as those in Algorithm 2, and
ﬁnally, we obtain Slimit-wise diarization results YSlimit (line 10). In iterative inference+, Slimit is varied from 1 to Smax
(line 1), which results in Smax diarization results for each recording. We then combine them by using DOVER-Lap to obtain the ﬁnal result Yˆ (line 11). With this procedure, the k-
th speaker’s speech activities can be overlapped with those of
the max (1, (k − Smax + 1))-th to (k + Smax − 1)-th speakers.

IV. EXPERIMENTS
A. Datasets
1) Simulated datasets: To train the EEND-EDA model, we created simulated speech mixtures from single-speaker recordings of the following corpora.
• Switchboard-2 (Phase I & II & III) • Switchboard Cellular (Part 1 & 2) • NIST Speaker Recognition Evaluation (2004 & 2005 &
2006 & 2008) Note that these corpora are compatible with the Kaldi CALLHOME x-vector recipe1.
We used the following simulation protocol to create multitalker mixtures from single-speaker recordings:
1) Select N speakers,
1https://github.com/kaldi-asr/kaldi/tree/master/egs/callhome diarization/v2

TABLE I DATASETS OF SIMULATED MIXTURES.

Dataset Split #Spk #Mixtures β Overlap ratio (%)

Sim1spk Train 1

100,000 2

0.0

Test

1

100,000 2

0.0

Sim2spk Train 2

100,000 2

34.1

Test

2

500

2

34.4

Test

2

500

3

27.3

Test

2

500

5

19.1

Sim3spk Train 3

100,000 5

34.2

Test

3

500

5

34.7

Test

3

500

7

27.4

Test

3

500

11

19.2

Sim4spk Train 4

100,000 9

31.5

Test

4

500

9

32.0

Sim5spk Train 5

100,000 13

30.3

Test

5

500

13

30.7

2) For each speaker, randomly sample speech segments and concatenate them with silences that are interlaid between speech segments,
3) For each of the N long recordings created, randomly select a room impulse response and convolve it with the recording,
4) Mix the N long recordings and a noise signal with a randomly determined signal-to-noise ratio.
The detailed algorithm for creating simulated mixtures can be found in [12]. In the second process, we assume that the occurrence of an utterance is a Poisson process, so the duration of the silence between speech segments follows the exponential distribution β1 exp − βx , where β is the mean value. β can be used to control the overlap ratio of the mixtures. To obtain a similar overlap ratio among various numbers of speakers, we varied β according to the number of speakers as summarized in Table I.
2) Real datasets: For real datasets, we employed ﬁve multitalker datasets below.
• CALLHOME [56]: A dataset that consists of telephone conversations whose average duration is two minutes. We used the splits provided in the Kaldi x-vector recipe1, which are denoted as Part 1 and Part 2, respectively. Two- and three-speaker subsets were used in the ﬁxednumber-of-speakers evaluations, which are denoted as CALLHOME-2spk and CALLHOME-3spk.
• CSJ [57]: A dataset that consists of monologues and dialogues of Japanese speech. In this paper, we used the dialogue part of the dataset. The average duration of the recordings is about 13 minutes. Following [58], we used 54 dialogue recordings out of 58.
• AMI headset mix [2]: A meeting dataset that consists of 100 hours of multi-modal meeting recordings. Each meeting session is about 30 minutes. We used headset mix recordings, which were obtained by mixing the headset recordings of all the participants. We used the split and reference RTTMs provided in the VBx paper [35].
• DIHARD II [38]: A dataset used in the second DIHARD challenge. We used single-channel audio, which is used

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

8

TABLE II DATASETS OF REAL RECORDINGS.

Dataset CALLHOME-2spk [56] CSJ [57] CALLHOME-3spk [56] CALLHOME [56] AMI headset mix [2]
DIHARD II [38] DIHARD III [39]

Split
Part 1 Part 2
—
Part 1 Part 2
Part 1 Part 2
Train Dev Test
Dev Test
Dev Test (Core) Test (Full)

#Spk
2 2
2
3 3
2–7 2–6
3–5 4
3–4
1–10 1–9
1–10 1–9 1–9

#Mixtures
155 148
54
61 74
249 250
136 18 16
192 194
254 184 259

Overlap ratio (%)
14.0 13.1
20.1
19.6 17.0
17.0 16.7
13.4 14.1 14.6
9.8 8.9
10.7 8.8 9.2

for tracks 1 and 2. The dataset consists of recordings from 11 domains (including telephone data) with an average duration of about 7 minutes. • DIHARD III [39]: A dataset used in the third DIHARD challenge. It also consists of recordings from 11 domains (including telephone data) with an average duration of about 8 minutes. The test set has two evaluation conditions called core and full. The core set is a subset of the full set, in which the recordings are selected to balance the duration of each domain. In terms of the number of speakers, the full set contains more recordings of two speakers than the core set.
Their statistics are summarized in Table II. Note that the recordings in CSJ, AMI, DIHARD II, and DIHARD III were sampled at 16 kHz, so we downsampled them to 8 kHz to be aligned with those of the simulated datasets. We also note that the recordings of the CSJ corpus are in stereo, so we mixed them to create monaural recordings.
B. Training
For the embedding part g in (6) of the proposed EENDEDA, we used four-stacked Transformer encoders with four attention heads without positional encodings, each of which outputs 256-dimensional frame-wise embeddings. The inputs for the model were log-scaled Mel-ﬁlterbank-based features. We ﬁrst extracted 23-dimensional log-scaled Mel-ﬁlterbanks with a frame length of 25 ms and frame shift of 10 ms. Each of them was then concatenated with those of the preceding and following seven frames, followed by subsampling with a factor of 10. As a result, a 345 (= 23 × 15) dimensional acoustic feature was extracted for each 100 ms.
In this paper, we evaluated EEND-EDA for both ﬁxednumbers-of-speakers and unknown-numbers-of-speakers conditions; thus, a model was trained for each purpose. For the ﬁxed-number-of-speakers evaluation, the model was ﬁrst trained on the Simkspk training set for 100 epochs and evaluated on the Simkspk test set. We also adapted the model to CALLHOME-kspk for another 100 epochs to evaluate the

model on real recordings. We used k ∈ {2, 3} in this paper. For the unknown-number-of-speakers evaluation, the model that was trained on Sim2spk was ﬁnetuned by using the concatenation of Sim{1,2,3,4}spk or Sim{1,2,3,4,5}spk for 50 epochs. The model was also adapted to each target dataset for another 500 epochs.
For network training using simulated mixtures, we used the Adam optimizer [59] with the Noam scheduler [60] with 100,000 warm-up steps. For adaptation, we also used the Adam optimizer but with a ﬁxed learning rate of 1 × 10−5. For efﬁcient batch processing during training, we split each recording into 500 frames when using Simkspk and 2000 frames when using the adaptation sets. The batch size for training was set to 64. Note that an entire recording is fed into the network without splitting during inference.

C. Evaluation

As an evaluation metric, we used diarization error rates (DERs) deﬁned as

DER = TMI + TFA + TCF , (29) TSpeech

where TSpeech, TMI, TFA, and TCF denote the duration of total speech, missed speech, false alarm speech, and speaker confusion, respectively. Following the prior work in [12], [61], we used 0.25 s of collar tolerance at each speech boundary for the Simkspk, CALLHOME, and CSJ evaluation. For AMI, DIHARD II, and DIHARD III, we allowed no collar tolerance and used a subsampling factor of 5 during inference, which results in acoustic features extracted every 50 ms, to obtain more ﬁne-grained results. We emphasize that speaker overlaps were NOT excluded from the evaluations.
We also report Jaccard error rates (JERs) in addition to DERs. To calculate JER, ﬁrst, the optimal assignment between reference and system speakers is calculated. JER is the average score of each reference speaker deﬁned as

JER = 1 Sref TF(As) + TM(sI) , Sref s=1 TU(sn)ion

(30)

where Sref is the number of reference speakers, and TM(sI) and TF(As) are the duration of the missed and false alarm speech calculated between speech activities of the s-th reference speaker and the paired system speaker, respectively. TU(sn)ion is the time duration in which at least one of the s-th reference
speakers of a paired system speaker is active.

V. RESULTS
A. Fixed numbers of speakers
1) Two-speaker experiment: First, we evaluated our method under the two-speaker condition. In this case, the model was ﬁrst trained on Sim2spk and then adapted to CALLHOME2spk Part 1. For the EEND-based methods, we used the model trained on Sim2spk to evaluate the simulated datasets and the one adapted to CALLHOME-2spk Part 1 to evaluate CALLHOME-2spk Part 2 and CSJ. For EEND-EDA, we used the ﬁrst two output attractors for speech activity calculation.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

9

TABLE III DERS (%) FOR TWO-SPEAKER EVALUATIONS. 0.25 s OF COLLAR
TOLERANCE WAS ALLOWED.

Method
i-vector + AHC x-vector (TDNN) + AHC BLSTM-EEND [12] SA-EEND [13] EEND-EDA (Chronol.) EEND-EDA (Shufﬂed)

Simulated

β=2 β=3 β=5

33.74 28.77 12.28 4.56 3.07 2.69

30.93 24.46 14.36 4.50 2.74 2.44

25.96 19.78 19.69 3.85 3.04 2.60

Real
CALLHOME-2spk
12.10 11.53 26.03 9.54 8.24 8.07

CSJ
27.99 22.96 39.33 20.48 18.89 16.27

TABLE IV DERS (%) FOR THREE-SPEAKER EVALUATIONS. 0.25 s OF COLLAR
TOLERANCE WAS ALLOWED.

6LOHQFH 6SN 6SN 2YHUODS $WWUDFWRU

6LOHQFH 6SN 6SN 2YHUODS $WWUDFWRU

(a) Conventional EEND [13]
6LOHQFH 6SN 6SN 2YHUODS $WWUDFWRU

6LOHQFH 6SN 6SN 2YHUODS $WWUDFWRU

Method
x-vector (TDNN) + AHC SA-EEND [13] EEND-EDA (Chronol.) EEND-EDA (Shufﬂed)

Simulated

β=2 β=3 β=5

31.78 8.69 13.02 8.38

26.06 7.64 11.65 7.06

19.55 6.92 10.41 6.21

Real
CALLHOME-3spk
19.01 14.00 15.86 13.92

Table III shows the results of the two-speaker evaluation. We observed that the proposed method with the shufﬂed order setting achieved the best DERs. Despite EEND-EDA being designed to deal with ﬂexible numbers of speakers, it outperformed the conventional EENDs, i.e., BLSTM-EEND and SA-EEND, which output diarization results for ﬁxed numbers of speakers. This is because the conventional EEND can be regarded as a ﬁxed-attractor-based method, while EENDEDA is an adaptive-attractor-based method as described in the last paragraph of Section III-B. This ﬂexibility of attractors makes the proposed method more accurate even in ﬁxednumber-of-speakers evaluations. In terms of the order of the input to EDA, shufﬂed sequences always performed better than chronologically ordered sequences. It indicates that the global context is more important than the temporal context to calculate attractors.
2) Three-speaker experiment: We also evaluated the method under the three-speaker condition. We ﬁrst trained the model on Sim3spk and then adapted it to CALLHOME3spk Part 1. We validated the performance on Sim3spk using the model trained on Sim3spk and that on CALLHOME3spk Part 2 using the model adapted to CALLHOME-3spk Part 1. We used the ﬁrst three attractors to evaluate EENDEDA’s performance. As shown in Table IV, EEND-EDA with sequence shufﬂing performed best on both simulated and real datasets.
3) Effect of input order: For a better understanding of EDA, we tried various types of sequences as inputs to the models, each of which was trained on chronologically ordered sequences and shufﬂed sequences. We evaluated matched and unmatched conditions of orders, and we also evaluated the effect of reducing the sequence length by subsampling or using the last 1/N part of the sequences. Table V shows the results on Sim2spk (β = 2). The EEND-EDA that was trained on chronologically ordered sequences performed well on chronologically ordered sequences but did poorly on

(b) EEND-EDA
Fig. 3. Visualization of embedding and attractors within each recording. For conventional EEND, weights of last fully connected layer Wcls were visualized instead of attractors.
shufﬂed sequences. It was also affected by subsampling, while it was slightly inﬂuenced by using the last 1/N part. These results indicate that the length of each utterance is an important factor to decide the output attractors for the model trained on chronologically ordered sequences. On the other hand, when the model was trained on shufﬂed sequences, it was not that affected by the order of sequences nor subsampling. However, when the last 1/N of the sequences were used, its performance degradation was worse than the model trained on chronologically ordered sequences. These results indicate that EDA trained on shufﬂed sequences captured the distribution of embeddings; thus, subsampling did not affect the performance that much, while using the last 1/N , i.e., biased sampling, degraded the DERs.
4) Embedding visualization: For intuitive understanding of the behavior of EDA, we visualized the embeddings et and attractors as within a two-speaker mixture from Sim2spk (β = 2) in Figure 3b. They were projected to two-dimensional space by using principal component analysis (PCA). We observed that the embeddings of two speakers were well distinguished from those of silence frames, and those of overlapped frames were distributed between the areas of the two speakers. For EEND-EDA, two attractors were calculated for each of the two speakers successfully as in Figure 3b. In Figure 3a, in comparison, the ﬁxed attractors Wcls of the conventional EEND were not well separated compared with the attractors calculated using EDA.
To understand the characteristics of attractors from EDA, we also visualized the inter-mixture relationship of attractors. For visualization, we ﬁrst chose an anchor speaker and then selected mixtures that contained the anchor speaker. We calculated two attractors from each mixture by using EENDEDA and mapped them onto a two-dimensional space using PCA. The speaker assignment from the calculated attractors to speaker identiﬁers was based on the groundtruth labels.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

10

TABLE V DERS FOR SIM2SPK (OVERLAP RATIO: 34.4 %) USING VARIOUS TYPES OF SEQUENCES.

Method
EEND-EDA (Train: Chronol.) EEND-EDA (Train: Shufﬂed)

Using whole sequence

Chronol. Shufﬂed

3.07

30.04

2.69

2.69

N =2
3.54 2.70

Subsample 1/N

N = 4 N = 8 N = 16

7.32 14.48 21.13

2.68

2.79

3.09

N = 32
27.18 5.08

N =2
3.67 3.36

Using the last 1/N

N = 4 N = 8 N = 16

4.97

5.40

6.11

5.92

7.46

8.59

N = 32
7.68 10.65

TABLE VI DERS (%) OF CROSS EVALUATIONS OF TWO- AND THREE-SPEAKER
EEND-EDA. 0.25 s OF COLLAR TOLERANCE WAS ALLOWED.

Fig. 4. Visualization of attractors across recordings. Selected speakers’ attractors are marked by dots, and their interference speakers’ attractors are marked by crosses. Colors of crosses correspond to speaker identities within each ﬁgure. Each pair of attractors from same mixture are connected with gray line.
Figure 4 shows the attractors of two-speaker mixtures that contain the same anchor speaker. It clearly shows that the each anchor speaker’s attractors were not distributed near each other.
From these results, the embeddings and attractors were calculated only to separate speakers in each mixture. We can also say that the attractors were not suited for speaker identiﬁcation. This also supports the idea that attractors are adaptively calculated from input embeddings. A similar observation on attractors from DANet [48] in speech separation was provided in Section 5 of [62] that attractors cannot be used for speaker identiﬁcation or tracing.
5) Evaluation on the mismatched number of speakers: We also evaluated two-speaker EEND-EDA on three-speaker datasets, and three-speaker EEND-EDA on two-speaker datasets. We used the model trained on Sim2spk or Sim3spk for the evaluation on the simulated datasets, and used the model adapted to CALLHOME-2spk or CALLHOME-3spk for the evaluation on the real datasets. The order of the embeddings is shufﬂed before being fed into EDA. The results are shown in Table VI. It is clearly observed that the DERs degraded when the number of speakers during training and inference was different. It is worth mentioning that threespeaker EEND-EDA did not work well on the two-speaker datasets; this indicates that the larger number of speakers during training does not serve the smaller number of speakers during inference.
B. Unknown numbers of speakers
1) Simulated mixtures: To train EEND-EDA to output ﬂexible numbers of speakers’ results, we ﬁnetuned the model from the two-speaker model for at most 50 epochs using Sim1spk to Sim4spk or Sim1spk to Sim5spk. Table VII shows the step-by-step improvement of the model. Note that the results on the top row correspond to our previous paper [15].

Model
Two-speaker EEND-EDA Three-speaker EEND-EDA

Two-speaker datasets

Sim2spk CALLHOME

(β = 2)

-2spk

2.69

8.07

15.12

9.95

Three-speaker datasets

Sim3spk CALLHOME

(β = 5)

-3spk

28.79 8.38

20.80 13.92

First, disabling backpropagation from the attractor existence loss Lexist to update only wexist and bexist improved the DERs for Sim1spk to Sim4spk. However, we observed that the model still did not perform well on Sim5spk, which was not included in the training set. Adding Sim5spk to the training set solved the problem as shown in the third row, which shows DERs that improved for Sim5spk from 23.08 % to 13.70 %. This indicates that EEND-EDA’s number of output speakers was empirically limited by its training datasets, even though it does not limit the number of output speakers with its network architecture. Increasing the number of training epochs further improved the DERs as shown in the last row. We also showed the DERs computed by SA-EEND [13] trained on a ﬂexible number of speakers’ dataset in the last two rows. In each case, the model’s output number of speakers was set to the maximum number of speakers in the dataset, i.e., four or ﬁve, and the model was trained to output null speech activities if a recording of a fewer number of speakers was input. EEND-EDA outperformed SAEEND in all datasets. Hereafter, we use the EEND-EDA model of the fourth row (k ∈ {1, . . . , 5}, 50 epochs, using Lexist to update only wexist and bexist during training) and the SA-EEND model of the sixth row (k ∈ {1, . . . , 5}, 50 epochs).
2) CALLHOME: Since the CALLHOME dataset does not include an ofﬁcial dev/eval split, we used the split provided in the Kaldi recipe and performed cross-validation. For comparison with the prior work on EEND, we also report the results obtained for Part 2 of the dataset using the model adapted to Part 1. For SAD post-processing described in Section III-C1, we used the TDNN-based SAD provided in the Kaldi ASpIRE recipe2 and oracle speech segments.
We show the number-of-speakers-wise results of crossvalidation in Table VIIIa. We also show the results for only evaluated single speaker regions in brackets. For this purpose, we chose up the most probable speakers from each time frame of the EEND-EDA results for fair comparison with xvector-based methods. EEND-EDA outperformed the state-ofthe-art x-vector-based methods in total DERs. One reason is that EEND-EDA can handle speaker overlap, but it showed
2https://github.com/kaldi-asr/kaldi/tree/master/egs/aspire/s5

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

11

TABLE VII STEP-BY-STEP IMPROVEMENT ON SIMULATED DATASETS. FOR SIM2SPK AND SIM3SPK, WE USED β = 2 AND β = 5, RESPECTIVELY. IN LEXIST COLUMN,
WE SHOW WHICH PARAMETERS WERE UPDATED USING LEXIST DURING TRAINING. RESULTS ON TOP ROW CORRESPOND TO ORIGINAL SETTING [15].

Model EEND-EDA SA-EEND

Training data
k ∈ {1, . . . , 4} k ∈ {1, . . . , 4} k ∈ {1, . . . , 5} k ∈ {1, . . . , 5}
k ∈ {1, . . . , 4} k ∈ {1, . . . , 5}

#Epochs
25 25 25 50
50 50

Lexist
Update all the parameters in fEEND Update only wexist and bexist Update only wexist and bexist Update only wexist and bexist
N/A N/A

k=1
0.39 0.25 0.21 0.36
0.60 0.50

Simkspk
k=2 k=3
4.33 8.94 4.06 7.68 4.22 8.25 3.65 7.70
4.39 9.40 3.95 9.18

k=4
13.76 10.12 10.75 9.97
13.56 12.24

k=5
N/A 23.08 13.70 11.95
25.22 17.42

TABLE VIII DERS (%) OF CALLHOME. 0.25 s OF COLLAR TOLERANCE WAS ALLOWED. TDNN-BASED X-VECTOR RESULTS WERE OBTAINED WITH KALDI RECIPE. DERS OF SINGLE-SPEAKER REGIONS ARE REPORTED IN BRACKETS. AHC: AGGLOMERATIVE HIERARCHICAL CLUSTERING, VB: VARIATIONAL BAYES
RESEGMENTATION [34], VBX: VARIATIONAL BAYES HMM CLUSTERING [35].

(a) Results of cross-validation.

Method
SA-EEND EEND-EDA
X-vector (TDNN) + AHC X-vector (TDNN) + AHC + VB SA-EEND EEND-EDA
X-vector (TDNN) + AHC X-vector (TDNN) + AHC + VB X-vector (ResNet101) + AHC + VBx [35] SA-EEND EEND-EDA

SAD
-
TDNN TDNN TDNN TDNN
Oracle Oracle Oracle Oracle Oracle

2
8.51 8.18
14.66 11.68 7.42 6.79
13.68 10.94 9.83 6.02 5.50

3
19.84 15.05
18.42 17.22 18.10 13.74
17.04 15.85 15.23 16.28 12.17

#Speakers

4

5

26.16 36.82 16.54 27.29

20.46 19.71 21.80 15.53

31.40 30.24 31.69 25.25

17.89 17.40 14.29 20.26 12.86

29.96 29.23 19.24 30.42 23.17

6
48.52 31.40
32.62 32.07 44.61 27.65
32.55 33.97 25.76 43.51 27.96

7
38.24 37.23
46.43 46.49 35.09 34.49
45.20 42.69 36.25 35.09 34.08

Total
19.82 (13.38) 14.81 (8.68)
19.48 (10.25) 17.80 (8.29) 17.41 (10.66) 13.36 (7.12)
18.04 (8.54) 16.57 (6.63) 14.21 (4.42) 15.90 (8.99) 11.72 (5.29)

(b) Results on CALLHOME Part 2.

Method
SA-EEND SC-EEND [16] SAD-OD-ﬁert SC-EEND [17] EEND-EDA (From [15]) EEND-EDA
X-vector (TDNN) + AHC X-vector (TDNN) + AHC + VB SA-EEND EEND-EDA
X-vector (TDNN) + AHC X-vector (TDNN) + AHC + VB X-vector (ResNet101) + AHC + VBx [35] SA-EEND EEND-EDA

SAD
-
TDNN TDNN TDNN TDNN
Oracle Oracle Oracle Oracle Oracle

DER
21.19 15.75 15.32 15.29 12.88
19.43 17.61 19.85 13.84
17.02 15.57 13.33 16.79 10.46

a competitive DER (5.29 %) even when speaker overlaps were excluded from the evaluation. Considering the number of speakers in a mixture, EEND-EDA did especially better than the x-vector-based methods with VBx clustering when the number of speakers was small (#Speakers=2,3,4), while it was worse or on par when the number of speakers was large (#Speakers=5,6,7). One reason is that the pretraining was based on mixtures with at most ﬁve speakers, and another reason is that mixtures of a larger number of speakers are rare in the CALLHOME dataset. Compared to SA-EEND, EENDEDA achieved better DERs on all the cases. Table VIIIb shows the results on CALLHOME Part2. It clearly shows that EENDEDA outperformed the other EEND-based methods [16], [17] by over two percent of absolute DER.
Table IX shows confusion matrices for the speaker counting of x-vector (TDNN) + AHC, x-vector (ResNet101) + AHC + VBx [35], SC-EEND [16], and EEND-EDA on CALLHOME Part 2. Our method achieved a higher speaker counting accuracy than the other methods by a large margin.
3) AMI headset mix: We next evaluated our method on the AMI headset mix, which has a different domain from the pretraining data (telephone conversation vs. meeting). We trained the model on the training set for 500 epochs and evaluated it on the dev and eval sets. The oracle speech segments were also used for SAD post-processing.
The results are shown in Table X. EEND-EDA outperformed the x-vector-based methods on both the dev and

eval sets with the oracle SAD. Note that the x-vector-based methods tuned the PLDA parameters on the dev set, so the superiority of EEND-EDA was smaller on the dev set than the eval set. EEND-EDA also outperformed SA-EEND with and without the oracle SAD. We also note that the average duration of the recordings in the AMI headset mix test set is over 30 min. The performance of EEND-EDA showed that EEND-EDA generalized well to such long recordings while using 200 s segments during adaptation.
4) DIHARD II & DIHARD III: Finally, we evaluated our method on the DIHARD II and III datasets, which contain recordings from multiple domains. In this evaluation, we used iterative inference with and without DOVER-Lap, each of which are described in Section III-C2 and Section III-C3, respectively, to deal with large numbers of speakers. For SAD post-processing, we used oracle segments and the system used in the Hitachi-JHU submission to the DIHARD III challenge [18].
The results are shown in Tables XI and XII. We can see that iterative inference with DOVER-Lap (iterative inference+) consistently improved DERs. Compared with the x-vectorbased methods, EEND-EDA performed best on DIHARD III full, while the x-vector-based methods were better on DIHARD II and DIHARD III core.
We show the number-of-speakers-wise DERs and JERs on DIHARD III in Table XIII. Our method performed better when the number of speakers was small and worse when the number

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

12

TABLE IX CONFUSION MATRICES FOR SPEAKER COUNTING ON CALLHOME PART
2. X-VECTOR-BASED RESULTS WERE OBTAINED WITH ORACLE SAD, WHILE EEND-BASED RESULTS WERE OBTAINED WITHOUT EXTERNAL
SAD.

(a) X-vector (TDNN) + AHC (b) X-vector (ResNet101) + AHC +

(Accuracy=56.4 %)

VBx [35] (Accuracy=72.0 %)

Pred. #Speakers

Ref. #Speakers 1 2 3 4 56
10 2 1 0 00 2 0 87 19 3 0 0 3 0 59 51 14 3 2 40 2 4 3 21 50 0 0 0 00 60 0 0 0 00

Pred. #Speakers

Ref. #Speakers 1 2 3 4 56
1 0 21 3 0 0 0 2 0 122 22 2 0 0 3 0 3 44 7 0 0 4 0 2 5 10 2 1 50 0 0 1 30 60 0 0 0 01 70 0 0 0 01

(c)

SC-EEND

(Accuracy=76.4 %)

[16] (d) EEND-EDA (Accuracy=84.4 %)

Pred. #Speakers

Ref. #Speakers 1 2 3 4 56
10 1 0 0 00 2 0 134 20 4 0 0 3 0 13 51 10 4 2 40 0 3 6 11 50 0 0 0 00 60 0 0 0 00

Pred. #Speakers

Ref. #Speakers 1 2 3 4 56
10 1 0 0 00 2 0 142 7 1 0 0 3 0 5 54 4 0 0 4 0 0 13 14 4 1 50 0 0 1 12 60 0 0 0 00

TABLE X DERS AND JERS (%) FOR AMI HEADSET MIX. NO COLLAR TOLERANCE
WAS ALLOWED.

Method
SA-EEND EEND-EDA
X-vector (ResNet101) + AHC X-vector (ResNet101) + AHC + VBx [35] SA-EEND EEND-EDA

SAD
-
Oracle Oracle Oracle Oracle

Dev

DER JER

31.66 39.20 21.93 25.86

19.61 16.33 23.95 15.69

23.90 20.57 35.64 22.19

Eval

DER JER

27.70 37.50 21.56 29.99

21.43 18.99 20.88 15.80

25.50 24.57 34.38 26.68

of speakers was large. This is why EEND-EDA performed well on DIHARD III full and worse on DIHARD II and DIHARD III eval. We also observed that the proposed iterative inference+ improved the performance, especially in terms of JERs on a large number of speaker cases, but it was still worse than the x-vector method. Handling a large number of speakers with EEND is left for future work.
VI. CONCLUSION
In this paper, we proposed an end-to-end speaker diarization method for unknown numbers of speakers using an encoderdecoder-based attractor calculation module called EENDEDA. In EEND-EDA, frame-wise embeddings are ﬁrstly calculated from an input acoustic feature sequence, then speakerwise attractors are calculated from the embeddings using EDA, and ﬁnally diarization results are obtained by the dot product of the embeddings and attractors. We also proposed to improve the performance of the diarization by shufﬂing the order of the embeddings before input to EDA and limiting the scope of backpropagation of the attractor existence loss.

To conduct fair comparisons between EEND-based methods and cascaded methods under the same SAD condition, we introduced SAD post-processing for EEND-based methods. We also proposed iterative inference to cope with the problem of EEND-EDA’s number of outputs being empirically limited by its training dataset. The evaluations on both simulated and real datasets showed that the proposed EEND-EDA performed well in both ﬁxed-number-of-speakers and ﬂexible-number-ofspeakers evaluations.
One possible future direction of this research is to train EEND-EDA with simulated data of a larger number of speakers. Preparing a large amount of data in advance for training increments the storage usage. Therefore, we will need a method to prepare simulated mixtures on the ﬂy during training as recently studied in [64]. In addition, to create a simulated mixture, we ﬁrst create N recordings each of which contains one speaker, and then mix them to be an N -speaker mixture. To control the overlap ratio, we increased the value of β as the number of speakers in the mixture increased, but this leads to an increase in the duration of silence in the mixture. An investigation of a better simulation protocol is also left for future work.
Even if EEND-EDA is trained with datasets of a large number of speakers, it would still limit the maximum number of speakers by the datasets as shown in Table VII. One reason is that EEND-EDA decides the number of speakers by using a neural network trained in a fully supervised manner. One of our later works has shown that unsupervised clustering can be introduced into EEND-EDA to remove the limitation on the output number of speakers caused by the training dataset [65].
Another direction is the network architecture. Currently, EDA employs a vanilla LSTM encoder-decoder, but an attention-based LSTM or Transformer encoder-decoder may be possible alternatives. Transformer encoders to extract frame-wise embeddings from input features can be also replaced with other architectures such as Conformers [66] or time-dilated convolutional neural networks [64].
REFERENCES
[1] T. J. Park, N. Kanda, D. Dimitriadis, K. J. Han, S. Watanabe, and S. Narayanan, “A review of speaker diarization: Recent advances with deep learning,” Comput. Speech Lang., vol. 72, p. 101317, 2022.
[2] J. Carletta, “Unleashing the killer corpus: experiences in creating the multi-everything AMI Meeting Corpus,” Lang. Resour. and Eval., vol. 41, no. 2, pp. 181–190, 2007.
[3] S. Watanabe, M. Mandel, J. Barker, E. Vincent, A. Arora, X. Chang, S. Khudanpur, V. Manohar, D. Povey, D. Raj, D. Snyder, A. S. Subramanian, J. Trmal, B. B. Yair, C. Boeddeker, Z. Ni, Y. Fujita, S. Horiguchi, N. Kanda, T. Yoshioka, and N. Ryant, “CHiME-6 Challenge: Tackling multispeaker speech recognition for unsegmented recordings,” in Proc. 6th Int. Workshop Speech Process. Everyday Environ. (CHiME-6), 2020.
[4] Z. Chen, T. Yoshioka, L. Lu, T. Zhou, Z. Meng, Y. Luo, J. Wu, X. Xiao, and J. Li, “Continuous speech separation: Dataset and analysis,” in Proc. IEEE Int. Conf. Acoust., Speech, Signal Process., 2020, pp. 7284–7288.
[5] C. Boeddeker, J. Heitkaemper, J. Schmalenstoeer, L. Drude, J. Heymann, and R. Haeb-Umbach, “Front-end processing for the CHiME-5 dinner party scenario,” in Proc. 5th Int. Workshop Speech Process. Everyday Environ. (CHiME-5), 2018.
[6] M. Delcroix, K. Zmolikova, T. Ochiai, K. Kinoshita, and T. Nakatani, “Speaker activity driven neural speech extraction,” in Proc. IEEE Int. Conf. Acoust., Speech, Signal Process., 2021, pp. 6099–6103.
[7] Q. Wang, C. Downey, L. Wan, P. Andrew Mansﬁeld, and I. Lopez Moreno, “Speaker diarization with LSTM,” in Proc. IEEE Int. Conf. Acoust., Speech, Signal Process., 2018, pp. 5239–5243.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

13

TABLE XI DERS AND JERS FOR DIHARD II EVAL. NO COLLAR
TOLERANCE WAS ALLOWED.

Method
SA-EEND EEND-EDA EEND-EDA (Iterative inference) EEND-EDA (Iterative inference+)
X-vector (TDNN) + AHC + VBx [52] SA-EEND EEND-EDA EEND-EDA (Iterative inference) EEND-EDA (Iterative inference+)
DIHARD II baseline [38] X-vector (TDNN) + AHC + VBx [52] X-vector (ResNet101) + AHC [35] X-vector (ResNet101) + AHC + VBx [35] SA-EEND EEND-EDA EEND-EDA (Iterative inference) EEND-EDA (Iterative inference+)

SAD
-
BUT [52] BUT [52] BUT [52] BUT [52] BUT [52]
Oracle Oracle Oracle Oracle Oracle Oracle Oracle Oracle

DER
32.14 29.57 29.41 28.52
27.11 32.01 30.48 29.80 29.09
28.81 18.21 23.59 18.55 23.25 20.54 21.00 20.24

JER
54.32 51.50 49.61 49.77
49.07 54.66 51.78 49.99 50.45
50.12 N/A 43.93 43.91 50.30 46.92 45.30 45.62

TABLE XII DERS AND JERS FOR DIHARD III EVAL. NO COLLAR TOLERANCE WAS ALLOWED.

Method
SA-EEND EEND-EDA EEND-EDA (Iterative inference) EEND-EDA (Iterative inference+)
X-vector (TDNN) + AHC + VBx [18] X-vector (TDNN) + AHC + VBx + OVL [18] SA-EEND EEND-EDA EEND-EDA (Iterative inference) EEND-EDA (Iterative inference+)
DIHARD III baseline [39] X-vector (TDNN) + AHC + VBx [18] X-vector (TDNN) + AHC + VBx + OVL [18] X-vector (ResNet152) + AHC + VBx [63] SA-EEND EEND-EDA EEND-EDA (Iterative inference) EEND-EDA (Iterative inference+)

SAD
-
Hitachi-JHU [18] Hitachi-JHU [18] Hitachi-JHU [18] Hitachi-JHU [18] Hitachi-JHU [18] Hitachi-JHU [18]
Oracle Oracle Oracle Oracle Oracle Oracle Oracle Oracle

Core

DER JER

27.49 25.94 25.76 24.77

49.64 47.76 45.35 45.18

22.99 24.58 25.79 23.96 24.41 23.43

42.44 42.02 49.20 46.82 44.70 44.93

20.65 16.89 18.20 16.56 20.21 18.38 18.87 17.86

47.74 38.49 38.42 38.72 46.17 43.69 41.58 41.69

Full

DER JER

22.64 21.55 21.40 20.69

43.14 41.15 39.09 39.07

21.48 21.47 21.29 20.03 20.30 19.53

38.73 37.83 42.68 40.31 38.47 38.78

19.25 15.83 15.65 15.79 16.19 14.91 15.21 14.42

42.45 34.27 33.71 34.46 39.44 36.93 35.08 35.30

TABLE XIII BREAKDOWN RESULTS OF DIHARD III EVAL FOR EACH NUMBER OF
SPEAKERS WITH ORACLE SPEECH SEGMENTS.

(a) DER (%)

#Speakers

Method

12 3 4 5 6 7 8 9

X-vector (TDNN) + AHC + VBx 1.30 11.43 16.76 23.09 44.99 26.43 25.61 35.57 2.03

EEND-EDA

2.80 7.52 15.79 25.63 47.66 31.73 35.47 38.19 18.73

EEND-EDA (Iterative inference+) 1.47 6.98 15.55 26.32 47.48 31.44 34.79 38.26 14.99

(b) JER (%)

#Speakers

Method

12 3 4 5 6 7 8 9

X-vector (TDNN) + AHC + VBx 2.40 16.99 44.68 44.70 66.17 53.32 56.05 56.71 8.01

EEND-EDA

3.37 11.77 38.70 48.37 67.40 64.85 67.77 69.00 57.60

EEND-EDA + iterative inference+ 3.31 11.34 39.60 48.76 68.46 62.41 62.65 65.36 41.23

[8] T. J. Park, K. J. Han, M. Kumar, and S. Narayanan, “Auto-tuning spectral clustering for speaker diarization using normalized maximum eigengap,” IEEE Signal Process. Lett., vol. 27, pp. 381–385, 2020.
[9] G. Sell and D. Garcia-Romero, “Speaker diarization with PLDA i-vector scoring and unsupervised calibration,” in Proc. IEEE Spoken Lang. Technol. Workshop, 2014, pp. 413–417.
[10] K. Kinoshita, L. Drude, M. Delcroix, and T. Nakatani, “Listening to each speaker one by one with recurrent selective hearing networks,” in Proc. IEEE Int. Conf. Acoust., Speech, Signal Process., 2018, pp. 5064–5068.
[11] T. von Neumann, K. Kinoshita, M. Delcroix, S. Araki, T. Nakatani, and R. Haeb-Umback, “All-neural online source separation, counting, and diarization for meeting analysis,” in Proc. IEEE Int. Conf. Acoust., Speech, Signal Process., 2019, pp. 91–95.
[12] Y. Fujita, N. Kanda, S. Horiguchi, K. Nagamatsu, and S. Watanabe, “End-to-end neural speaker diarization with permutation-free objectives,” in Proc. Interspeech, 2019, pp. 4300–4304.
[13] Y. Fujita, N. Kanda, S. Horiguchi, Y. Xue, K. Nagamatsu, and S. Watanabe, “End-to-end neural speaker diarization with self-attention,” in Proc. IEEE Autom. Speech Recognit. Understanding Workshop, 2019, pp. 296– 303.
[14] Y. Fujita, S. Watanabe, S. Horiguchi, Y. Xue, and K. Nagamatsu, “Endto-end neural diarization: Reformulating speaker diarization as simple multi-label classiﬁcation,” arXiv:2003.02966, 2020.
[15] S. Horiguchi, Y. Fujita, S. Wananabe, Y. Xue, and K. Nagamatsu, “Endto-end speaker diarization for an unknown number of speakers with encoder-decoder based attractors,” in Proc. Interspeech, 2020, pp. 269– 273.
[16] Y. Fujita, S. Watanabe, S. Horiguchi, Y. Xue, J. Shi, and K. Naga-

matsu, “Neural speaker diarization with speaker-wise chain rule,” arXiv:2006.01796, 2020. [17] Y. Takashima, Y. Fujita, S. Watanabe, S. Horiguchi, P. Garcia, and K. Nagamatsu, “End-to-end speaker diarization conditioned on speech activity and overlap detection,” in Proc. IEEE Spoken Lang. Technol. Workshop, 2021, pp. 849–856. [18] S. Horiguchi, N. Yalta, P. Garcia, Y. Takashima, Y. Xue, D. Raj, Z. Huang, Y. Fujita, S. Watanabe, and S. Khudanpur, “The HitachiJHU DIHARD III system: Competitive end-to-end neural diarization and x-vector clustering systems combined by DOVER-Lap,” in Proc. 3rd DIHARD Speech Diarization Challenge Workshop, 2021. [19] E. Han, C. Lee, and A. Stolcke, “BW-EDA-EEND: Streaming end-toend neural speaker diarization for a variable number of speakers,” in Proc. IEEE Int. Conf. Acoust., Speech, Signal Process., 2021, pp. 7193– 7197. [20] Y. Xue, S. Horiguchi, Y. Fujita, Y. Takashima, S. Watanabe, P. Garcia, and K. Nagamatsu, “Online streaming end-to-end neural diarization handling overlapping speech and ﬂexible numbers of speakers,” in Proc. Interspeech, 2021, pp. 3116–3120. [21] M. A` . India Massana, J. A. Rodr´ıguez Fonollosa, and F. J. Hernando Perica´s, “Lstm neural network-based speaker segmentation using acoustic and language modelling,” in Proc. Interspeech, 2017, pp. 2834– 2838. [22] T. J. Park, K. J. Han, J. Huang, X. He, B. Zhou, P. Georgiou, and S. Narayanan, “Speaker diarization with lexical information,” in Proc. Interspeech, 2019, pp. 391–395. [23] M. Senoussaoui, P. Kenny, T. Stafylakis, and P. Dumouchel, “A study of the cosine distance-based mean shift for telephone speech diarization,” IEEE/ACM Trans. Audio, Speech, Lang. Process., vol. 22, no. 1, pp. 217–227, 2014. [24] G. Sell, D. Snyder, A. McCree, D. Garcia-Romero, J. Villalba, M. Maciejewski, V. Manohar, N. Dehak, D. Povey, S. Watanabe, and S. Khudanpur, “Diarization is hard: Some experiences and lessons learned for the JHU team in the inaugural DIHARD challenge,” in Proc. Interspeech, 2018, pp. 2808–2812. [25] D. Snyder, D. Garcia-Romero, G. Sell, D. Povey, and S. Khudanpur, “X-vectors: Robust DNN embeddings for speaker recognition,” in Proc. IEEE Int. Conf. Acoust., Speech, Signal Process., 2018, pp. 5329–5333. [26] M. Diez, L. Burget, S. Wang, J. Rohdin, and J. Cˇ ernocky`, “Bayesian HMM based x-vector clustering for speaker diarization,” in Proc. Interspeech, 2019, pp. 346–350. [27] X. Xiao, N. Kanda, Z. Chen, T. Zhou, T. Yoshioka, S. Chen, Y. Zhao, G. Liu, Y. Wu, J. Wu, S. Liu, J. Li, and Y. Gong, “Microsoft speaker diarization system for the VoxCeleb speaker recognition challenge 2020,” in Proc. IEEE Int. Conf. Acoust., Speech, Signal Process., 2021, pp. 5824–5828. [28] A. Zhang, Q. Wang, Z. Zhu, J. Paisley, and C. Wang, “Fully supervised speaker diarization,” in Proc. IEEE Int. Conf. Acoust., Speech, Signal Process., 2019, pp. 6301–6305. [29] S. H. Shum, N. Dehak, R. Dehak, and J. R. Glass, “Unsupervised methods for speaker diarization: An integrated and iterative approach,”

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

14

IEEE Trans. Audio, Speech, Lang. Process., vol. 21, no. 10, pp. 2015– 2028, 2013.
[30] D. Dimitriadis and P. Fousek, “Developing on-line speaker diarization system,” in Proc. Interspeech, 2017, pp. 2739–2743.
[31] D. Garcia-Romero, D. Snyder, G. Sell, D. Povey, and A. McCree, “Speaker diarization using deep neural network embeddings,” in Proc. IEEE Int. Conf. Acoust., Speech, Signal Process., 2017, pp. 4930–4934.
[32] M. Maciejewski, D. Snyder, V. Manohar, N. Dehak, and S. Khudanpur, “Characterizing performance of speaker diarization systems on far-ﬁeld speech using standard methods,” in Proc. IEEE Int. Conf. Acoust., Speech, Signal Process., 2018, pp. 5244–5248.
[33] D. Raj, Z. Huang, and S. Khudanpur, “Multi-class spectral clustering with overlaps for speaker diarization,” in Proc. IEEE Spoken Lang. Technol. Workshop, 2021, pp. 582–589.
[34] M. Diez, L. Burget, F. Landini, and J. Cˇ ernocky´, “Analysis of speaker diarization based on bayesian HMM with eigenvoice priors,” IEEE/ACM Trans. Audio, Speech, Lang. Process., vol. 28, pp. 355–368, 2020.
[35] F. Landini, J. Profant, M. Diez, and L. Burget, “Bayesian HMM clustering of x-vector sequences (VBx) in speaker diarization: theory, implementation and analysis on standard tasks,” Comput. Speech Lang., vol. 71, p. 101254, 2022.
[36] Q. Li, F. L. Kreyssig, C. Zhang, and P. C. Woodland, “Discriminative neural clustering for speaker diarisation,” in Proc. IEEE Spoken Lang. Technol. Workshop, 2021, pp. 574–581.
[37] Z. Huang, S. Watanabe, Y. Fujita, P. Garc´ıa, Y. Shao, D. Povey, and S. Khudanpur, “Speaker diarization with region proposal network,” in Proc. IEEE Int. Conf. Acoust., Speech, Signal Process., 2020, pp. 6514– 6518.
[38] N. Ryant, K. Church, C. Cieri, A. Cristia, J. Du, S. Ganapathy, and M. Liberman, “The Second DIHARD Diarization Challenge: Dataset, task, and baselines,” in Proc. Interspeech, 2019, pp. 978–982.
[39] N. Ryant, P. Singh, V. Krishnamohan, R. Varma, K. Church, C. Cieri, J. Du, S. Ganapathy, and M. Liberman, “The Third DIHARD Diarization Challenge,” in Proc. Interspeech, 2021, pp. 3570–3574.
[40] I. Medennikov, M. Korenevsky, T. Prisyach, Y. Khokhlov, M. Korenevskaya, I. Sorokin, T. Timofeeva, A. Mitrofanov, A. Andrusenko, I. Podluzhny, A. Laptev, and A. Romanenko, “Target-speaker voice activity detection: a novel approach for multi-speaker diarization in a dinner party scenario,” in Proc. Interspeech, 2020, pp. 274–278.
[41] D. Yu, M. Kolbæk, Z.-H. Tan, and J. Jensen, “Permutation invariant training of deep models for speaker-independent multi-talker speech separation,” in Proc. IEEE Int. Conf. Acoust., Speech, Signal Process., 2017, pp. 241–245.
[42] Y. Luo, Z. Chen, J. R. Hershey, J. Le Roux, and N. Mesgarani, “Deep clustering and conventional networks for music separation: Stronger together,” in Proc. IEEE Int. Conf. Acoust., Speech, Signal Process., 2017, pp. 61–65.
[43] Y. Luo and N. Mesgarani, “TasNet: Time-domain audio separation network for real-time, single-channel speech separation,” in Proc. IEEE Int. Conf. Acoust., Speech, Signal Process., 2018, pp. 696–700.
[44] ——, “Conv-TasNet: Surpassing ideal time–frequency magnitude masking for speech separation,” IEEE/ACM Trans. Audio, Speech, Lang. Process., vol. 27, no. 8, pp. 1256–1266, 2019.
[45] Y. Luo, Z. Chen, and N. Mesgarani, “Speaker-independent speech separation with deep attractor network,” IEEE/ACM Trans. Audio, Speech, Lang. Process., vol. 26, no. 4, pp. 787–796, 2018.
[46] N. Zeghidour and D. Grangier, “Wavesplit: End-to-end speech separation by speaker clustering,” IEEE/ACM Trans. Audio, Speech, Lang. Process., vol. 29, pp. 2840–2849, 2021.
[47] N. Takahashi, S. Parthasaarathy, N. Goswami, and Y. Mitsufuji, “Recursive speech separation for unknown number of speakers,” in Proc. Interspeech, 2019, pp. 1348–1352.
[48] Z. Chen, Y. Luo, and N. Mesgarani, “Deep attractor network for singlemicrophone speaker separation,” in Proc. IEEE Int. Conf. Acoust., Speech, Signal Process., 2017, pp. 246–250.
[49] J. Lee, Y. Lee, J. Kim, A. R. Kosiorek, S. Choi, and Y. W. Teh, “Set Transformer: A framework for attention-based permutation-invariant neural networks,” in Proc. Int. Conf. Mach. Learn., 2019, pp. 3744– 3753.
[50] B. B. Meier, I. Elezi, M. Amirian, O. Du¨rr, and T. Stadelmann, “Learning neural models for end-to-end clustering,” in Proc. IAPR Workshop Artif. Neural Netw. Pattern Recognit., 2018, pp. 126–138.
[51] J. R. Hershey, Z. Chen, J. Le Roux, and S. Watanabe, “Deep clustering: Discriminative embeddings for segmentation and separation,” in Proc. IEEE Int. Conf. Acoust., Speech, Signal Process., 2016, pp. 31–35.

[52] F. Landini, S. Wang, M. Diez, L. Burget, P. Mateˇjka, K. Zˇ mol´ıkova´, L. Mosˇner, A. Silnova, O. Plchot, O. Novotny`, H. Zeinali, and J. Rohdin, “BUT system for the Second DIHARD Speech Diarization Challenge,” in Proc. IEEE Int. Conf. Acoust., Speech, Signal Process., 2020, pp. 6529–6533.
[53] S. Horiguchi, P. Garcia, Y. Fujita, S. Watanabe, and K. Nagamatsu, “End-to-end speaker diarization as post-processing,” in Proc. IEEE Int. Conf. Acoust., Speech, Signal Process., 2021, pp. 7188–7192.
[54] D. Raj, L. P. Garcia-Perera, Z. Huang, S. Watanabe, D. Povey, A. Stolcke, and S. Khudanpur, “DOVER-Lap: A method for combining overlapaware diarization outputs,” in Proc. IEEE Spoken Lang. Technol. Workshop, 2021, pp. 881–888.
[55] A. Stolcke and T. Yoshioka, “DOVER: A method for combining diarization outputs,” in Proc. IEEE Autom. Speech Recognit. Understanding Workshop, 2019, pp. 757–763.
[56] “2000 NIST Speaker Recognition Evaluation,” https://catalog.ldc.upenn. edu/LDC2001S97.
[57] K. Maekawa, “Corpus of spontaneous Japanese: Its design and evaluation,” in Proc. ISCA & IEEE Workshop on Spontaneous Speech Process. Recognit., 2003, pp. 7–12.
[58] N. Kanda, C. Boeddeker, J. Heitkaemper, Y. Fujita, S. Horiguchi, K. Nagamatsu, and R. Haeb-Umbach, “Guided source separation meets a strong ASR backend: Hitachi/Paderborn University joint investigation for dinner party scenario,” in Proc. Interspeech, 2019, pp. 1248–1252.
[59] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,” in Proc. Int. Conf. Learn. Representations, 2015.
[60] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. u. Kaiser, and I. Polosukhin, “Attention is all you need,” in Proc. Advances Neural Inf. Process. Syst., 2017, pp. 5998–6008.
[61] N. Kanda, S. Horiguchi, Y. Fujita, Y. Xue, K. Nagamatsu, and S. Watanabe, “Simultaneous speech recognition and speaker diarization for monaural dialogue recordings with target-speaker acoustic models,” in Proc. IEEE Autom. Speech Recognit. Understanding Workshop, 2019, pp. 31–38.
[62] L. Drude, T. von Neumann, and R. Haeb-Umbach, “Deep attractor networks for speaker re-identiﬁcation and blind source separation,” in Proc. IEEE Int. Conf. Acoust., Speech, Signal Process., 2018, pp. 11–15.
[63] F. Landini, A. Lozano-Diez, L. Burget, M. Diez, A. Silnova, K. Zˇ mol´ıkova´, O. Glembek, P. Mateˇjka, T. Stafylakis, and N. Bru¨mmer, “BUT system description for the Third DIHARD Speech Diarization Challenge,” in Proc. 3rd DIHARD Speech Diarization Challenge Workshop, 2021.
[64] S. Maiti, H. Erdogan, K. Wilson, S. Wisdom, S. Watanabe, and J. R. Hershey, “End-to-end diarization for variable number of speakers with local-global networks and discriminative speaker embeddings,” in Proc. IEEE Int. Conf. Acoust., Speech, Signal Process., 2021, pp. 7183–7187.
[65] S. Horiguchi, P. Garc´ıa, S. Watanabe, Y. Xue, Y. Takashima, and Y. Kawaguchi, “Towards neural diarization for unlimited numbers of speakers using global and local attractors,” in Proc. IEEE Autom. Speech Recognit. Understanding Workshop, 2021, pp. 98–105.
[66] Y. C. Liu, E. Han, C. Lee, and A. Stolcke, “End-to-end neural diarization: From transformer to conformer,” in Proc. Interspeech, 2021, pp. 3081–3085.

