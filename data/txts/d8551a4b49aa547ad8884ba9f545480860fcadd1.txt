arXiv:2203.08205v1 [cs.LG] 15 Mar 2022

Learning Deep Implicit Fourier Neural Operators (IFNOs) with Applications to Heterogeneous Material Modeling
Huaiqian Youa, Quinn Zhanga, Colton J. Rossb, Chung-Hao Leeb, Yue Yua,∗
aDepartment of Mathematics, Lehigh University, Bethlehem, PA 18015, USA bSchool of Aerospace and Mechanical Engineering, The University of Oklahoma, Norman, OK 73019, USA
Abstract
Constitutive modeling based on continuum mechanics theory has been a classical approach for modeling the mechanical responses of materials. However, when constitutive laws are unknown or when defects and/or high degrees of heterogeneity are present, these classical models may become inaccurate. In this work, we propose to use data-driven modeling, which directly utilizes high-ﬁdelity simulation and/or experimental measurements to predict a material’s response without using conventional constitutive models. Speciﬁcally, the material response is modeled by learning the implicit mappings between loading conditions and the resultant displacement and/or damage ﬁelds, with the neural network serving as a surrogate for a solution operator. To model the complex responses due to material heterogeneity and defects, we develop a novel deep neural operator architecture, which we coin as the Implicit Fourier Neural Operator (IFNO). In the IFNO, the increment between layers is modeled as an integral operator to capture the long-range dependencies in the feature space. As the network gets deeper, the limit of IFNO becomes a ﬁxed point equation that yields an implicit neural operator and naturally mimics the displacement/damage ﬁelds solving procedure in material modeling problems. To obtain an eﬃcient implementation, we parameterize the integral kernel of this integral operator directly in the Fourier space and interpret the network as discretized integral (nonlocal) diﬀerential equations, which consequently allow for the fast Fourier transformation (FFT) and accelerated learning techniques for deep networks. We demonstrate the performance of our proposed method for a number of examples, including hyperelastic, anisotropic and brittle materials. As an application, we further employ the proposed approach to learn the material models directly from digital image correlation (DIC) tracking measurements, and show that the learned solution operators substantially outperform the conventional constitutive models in predicting displacement ﬁelds. Keywords: Operator-Regression Neural Networks, Fourier Neural Operator (FNO), Data-Driven Material Modeling, Deep Learning, Brittle Fracture, Implicit Networks
∗Corresponding author Email addresses: huy316@lehigh.edu (Huaiqian You), quz222@lehigh.edu (Quinn Zhang), cjross@ou.edu (Colton J.
Ross), ch.lee@ou.edu (Chung-Hao Lee), yuy214@lehigh.edu (Yue Yu)

Preprint submitted to Elsevier

March 17, 2022

Contents

1 Introduction

2

2 Background and Related Work

5

2.1 Problem statement: Learning solution operators . . . . . . . . . . . . . . . . . . . . . . . . . 6

2.2 Three relevant integral neural operator architectures . . . . . . . . . . . . . . . . . . . . . . . 8

3 Implicit Fourier Neural Operators (IFNOs)

12

3.1 The network architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13

3.2 Universal approximation properties . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15

4 Numerical Examples

19

4.1 The ﬂow through a porous medium . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20

4.2 The deformation of a hyperelastic and anisotropic ﬁber-reinforced material . . . . . . . . . . 24

4.3 The brittle fracture mechanics in glass-ceramics . . . . . . . . . . . . . . . . . . . . . . . . . . 28

5 Application: Learning From Digital Image Correction (DIC) Measurements

32

5.1 Digital Image Correction (DIC) and biaxial mechanical testing . . . . . . . . . . . . . . . . . 32

5.2 Constitutive modeling for comparisons with the IFNO . . . . . . . . . . . . . . . . . . . . . . 34

5.3 Results and discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35

6 Conclusion

38

Appendix A Detailed Numeric Results

39

1. Introduction
In science and engineering, predicting and monitoring heterogeneous material responses are ubiquitous in many applications [1–11]. In these materials, the microstructure, in terms of the geometric distribution of phases, constituent properties, and interfacial bonding attributes inﬂuences the deformation and failure behavior, which needs to be accurately captured to guarantee reliable and trustworthy predictions and inform decision making. Conventionally, constitutive models based on continuum mechanics have been commonly employed for modeling heterogeneous material responses. When the material microstructures are known, constitutive models in conjunction with other ﬁeld equations (e.g., balance of linear momentum) are often built in the form of partial diﬀerential equations (PDEs), and the material responses are obtained by approximating the PDE solutions with classical numerical methods such as ﬁnite elements.
However, fundamental challenges are still present in utilizing the constitutive models and numerical simulations to provide a comprehensive physical and functional description of heterogeneous material responses [12]. First, in the constitutive modeling theory the choice of governing laws (such as the strain energy density
2

function) is often determined a priori and the free parameters are often tuned to obtain agreement with experimental stress-strain data. This fact makes the rigorous calibration and validation process challenging. Second, although new experimental technologies and testing procedures have been designed to observe much smaller microstructure patterns and monitor defects in a faster manner [4, 13–18], it remains diﬃcult to fully quantify the microstructure and responses for individual material samples, due to variability and measurement noises from diﬀerent microstructure geometries, properties, and operating environments. In addition to these challenges, many microstructure characterization methods require the use of destructive methods that could alter the observed microstructural properties, such as optical clearing and histological processing [19, 20]. Therefore, in the application scenarios where the material response of a particular material sample is of interests, such as the non-destructive evaluation and damage prediction problems, conventional constitutive models may suﬀer from errors stemmed from its functional form assumption and the measurement noises, leading to limited predictivity.
To address these challenges, data-driven computing has been considered as an alternative to the conventional constitutive modeling. In recent years, there has been signiﬁcant progress in the development of deep neural networks (NNs), focusing on learning the hidden physics of a complex system [21–35]. Among these works, several studies that use neural networks in modeling heterogeneous materials have been conducted [36–41]. In [37, 38], physics-informed NN models [42] were developed, where the material responses were modeled as the solution of a known PDE by a deep NN with weights and biases learned together with the PDE’s unknown parameter ﬁelds (e.g., permeability). In [41], a symbolic regression method [43–46] was developed to learn the microstructure-dependent plasticity from data, where the constitutive models were generated using interpretable machine learning as symbolic expressions. In [36], data-driven approaches were employed for the homogenization procedure, where the information from multiple sub-scales can be used to sequentially generate the macroscopic prediction in a cost-eﬃcient manner. In [39, 40], representative volume elements (RVE) were employed to build the material law for heterogeneous materials, and a homogenized model was then discovered based on the RVE database. To the authors’ best knowledge, most of the stateof-the-art NN developments for heterogeneous material modeling either focus on the homogenized behavior of the material or rely on (partially) known physics laws, which limits their applicability to problems where the unknown heterogeneous behavior of each individual sample is of interest.
More recently, the use of NNs has been extended to learning maps between inputs of a dynamical system and its state, so that the network serves as a surrogate for a solution operator [47–51]. This approach, which can be referred as neural operators, ﬁnds applicability when the constitutive laws are unknown. Representative works in this direction include the integral neural operator architectures [49–54] and the DeepONet architectures [47, 48, 55]. Comparing with the classical NNs, the most notable advantages of neural operators are resolution independence and generalizability to diﬀerent input instances. The former implies that the accuracy of the prediction is invariant with respect to the resolution of input parameters such as loading conditions and material properties. This fact is in stark contrast with the classical ﬁnite-
3

dimensional approaches that build the NN models between ﬁnite-dimensional Euclidean spaces, so that their accuracy is tied to the resolution of input [56–60]. Furthermore, being generalizable with respect to diﬀerent input parameter instances renders another computing advantage: once the neural operator is trained, solving for a new instance of the input parameter only requires a forward pass of the network. This unique property is in contrast with traditional PDE-constrained optimization techniques [61] and some other NN models that directly parameterize the solution [42, 62–65], as all these methods only approximate the solution for a single instance of the input. In [55, 66, 67], neural operators have been successfully applied to model the unknown physics law of homogeneous materials. In [49–51, 68], neural operators are employed as a solution surrogate for the Darcy’s ﬂow in a heterogeneous porous medium, when the microstructure ﬁeld is known.
In this work, we propose to advance the current data-driven methods on heterogeneous material modeling by designing deep neural operators to model heterogeneous material responses without using any predeﬁned constitutive models or microstructure measurements. Speciﬁcally, through learning the solution operator directly from high-ﬁdelity simulation and/or experimental measurements, we integrate material identiﬁcation, modeling procedures, and material response prediction. The material microstructure properties are learned implicitly from the data and naturally embedded in the network parameters. The heterogeneous material responses can thus be obtained without assumptions on microstructure or governing laws. To capture the complex and possibly nonlinear material responses, deep NNs are necessary to learn multiple levels of abstraction for representations of the raw input data. To achieve this goal, we pursue a new integral neural operator architecture that, 1) is stable in the limit of deep layers with ﬁxed memory costs, 2) has guaranteed universal approximation capability, and 3) is independent of the input resolution and generalizable to unseen input function instances. Our proposed architecture can be interpreted as a data-driven surrogate of the ﬁxed point procedure, in the sense that the increment of ﬁxed point iterations are modeled as increment between layers. As such, a forward pass through a very deep network is analogous to obtaining the PDE solution as an implicit problem, and the universal approximation capability is guaranteed as far as there exists a convergent ﬁxed point equation1. To further accelerate the learning, we identify iterative layers with time instants such that the proposed network can be interpreted as discretized autonomous integral (non-local) diﬀerential equations, and consequently allows for the shallow-to-deep initialization technique [52, 74, 75] where optimal parameters learned on shallow networks are considered as (quasi-optimal) initial guesses for deeper networks. Since the proposed architecture is built as a modiﬁcation of the Fourier Neural Operator method (FNO), it also parameterizes the integral kernel directly in the Fourier space and utilizes the fast Fourier transformation (FFT) to eﬃciently evaluate the integral operator. As such, our network inherits the advantages of FNOs on resolution independence and superior eﬃciency. Because it preserves the similar properties to both the implicit neural networks and the FNOs, we refer to our proposed network as implicit Fourier neural operators (IFNOs).
1Here, we point out that the idea of using constant parameters across layers and formulating the NNs as a ﬁxed point equation was also proposed in implicit networks [69–73] such that the deep network can be trained with ﬁxed memory costs.
4

We summarize our major contributions as follows.
1. We introduce a novel deep neural operator by parameterizing the layer increment as an integral operator, referred to as IFNO, which learns the mapping between loading conditions and material responses as a solution operator while preserving the accuracy across resolutions.
2. By resembling the network architecture as a ﬁxed point method, the IFNOs can be interpreted as a numerical solver for an implicit problem with unknown material properties/microstructure, and the universal approximation property is guaranteed as far as there exists a converging ﬁxed point equation for this implicit problem.
3. By identifying the layers with time instants, the IFNOs can also be interpreted as discretized nonlocal time-dependent equations, which allows for accelerated learning techniques for deep networks, such as the shallow-to-deep technique [74].
4. In a variety of complex material response learning tasks, the IFNOs demonstrate not only stability but also improved accuracy in the deep network limit: in complex learning tasks, the IFNOs outperform the best FNOs with reduced memory costs and halved prediction errors.
5. Our proposed method integrates material identiﬁcation, modeling procedures, and material response prediction into one learning framework, which makes it particularly promising for learning complex material responses without explicit constitutive models and/or microstructure measurements. To demonstrate this capability, we learn the mechanical responses of a latex glove sample directly from digital image correlation (DIC) tracking measurements. Comparing with the conventional constitutive models, our method reduces the prediction error by 10 times.
The remainder of this paper is organized as follows. In Section 2, we introduce three integral neural operator architectures that inspired our work and highlight their advantages and limitations. In Section 3, we introduce the IFNOs as inspired by an implicit problem solver, and discuss its universal approximation capability. In Section 4, we show the stability and convergence of the IFNOs for a number of benchmarks, including heterogeneous, hyperelastic, anisotropic and brittle fracture material problems, that illustrate the eﬃcacy of our network compared to the baseline networks. Next, in Section 5 we further demonstrate the applicability of our data-driven approach to learn the unknown mechanical responses directly from DIC tracking measurements, providing evidence that the scheme yields accurate predictions for practical engineering problems. In Section 6, we provide a summary of our achievements and concluding remarks. In the appendix, we provide additional numerical results.
2. Background and Related Work
This section provides the necessary background for the rest of the paper by formally stating the problem of neural operator learning, providing succinct reviews on the three integral neural operator learning approaches
5

Model
GKN NKN FNO IFNO

Layer-Independent Parameters
–

Eﬃciency Through FFT
– –

Continuous in Depth (Time)
–
–

Stability in Deep Networks
–
–

Ref
[49, 50] [52] [51]

Table 1: List of the properties for the graph kernel networks (GKNs), nonlocal kernel networks (NKNs), Fourier neural operators (FNOs), and the proposed implicit Fourier neural operators (IFNOs).

recently proposed in the literature that inspired the proposed IFNOs, and highlighting their properties, as summarized in Table 1.

2.1. Problem statement: Learning solution operators

The main application considered in this work is the modeling of complex material responses under diﬀerent loading conditions. Formally, consider a s-dimensional body occupying the domain Ω ⊂ Rs (s = 1, 2 or 3),

which deforms under external loading. Without prior knowledge of the material properties or constitutive

laws, our ultimate goal is to identify the best surrogate solution operator, that accurately predicts the material

mechanical responses in terms of the resultant displacement ﬁeld u(x) and/or damage ﬁeld given new and

unseen material property or loading scenarios. In this context, diﬀerent types of loading scenarios are

considered, such as a displacement-type loading applied on the subject’s boundary, a body force applied on

the whole domain Ω, a traction loading applied on part of its boundaries or a combination of the above.

Denoting the whole boundaries of domain Ω as ∂Ω, we consider general mixed boundary conditions: ∂Ω = ∂ΩD ∂ΩN and (∂ΩD)o (∂ΩN )o = ∅, where ∂ΩD and ∂ΩN are the Dirichlet and Neumann boundaries,

respectively. To apply the displacement-type loading on the boundary, we assume that u(x) = uD(x) are

provided on ∂ΩD, while the traction t(x) is applied on the boundary ∂ΩN .

In this work, we propose to learn the surrogate solution operator as a mapping between functions, namely,

the microstructure and/or loading and the resultant displacement/damage ﬁeld, given a collection of observed

function pairs. Mathematically, let Kb be the unknown diﬀerential operator associated with the momentum

balance equation and Nb be the unknown operator associated with the traction, both depending on the

material microstructure parameter ﬁeld b(x). Given a body force g(x), the momentum balance equation

and boundary conditions write:

Kb[u](x) = g(x), x ∈ Ω,

u(x) = uD(x), x ∈ ∂ΩD,

(2.1)

Nb[u](x) = t(x), x ∈ ∂ΩN .

To solve the displacement ﬁeld, we consider the problem of learning a general solution operator, with its input being a concatenated vector function f (x) of x, b(x), g(x), uD(x), t(x) and its output being the displacement ﬁeld u(x), for all x ∈ Ω. Here, we notice that uD(x) and t(x) are only deﬁned on the displacement boundary ∂ΩD and the traction boundary ∂ΩN , respectively. To make them well-deﬁned

6

on the whole domain, we employ the zero-padding strategy proposed in [68], namely, we deﬁne f (x) := [x, b(x), g(x), u˜D(x), t˜(x)] where


 uD(x), u˜D(x) =
 0,

if x ∈ ∂ΩD ,
if x ∈ Ω\∂ΩD



 t˜(x) =

t(x),

 0,

if x ∈ ∂ΩN .
if x ∈ Ω\∂ΩN

(2.2)

In what follows, we denote the input and output function spaces as F = F (Ω; RdF ) and U = U (Ω; Rdu ),

respectively.

Let

{

fj

,

uj

}

N j=1

be

a

set

of

observations

where

the

input

{fj }

⊂

F

is

a

sequence

of

independent

and identically distributed random ﬁelds from a known probability distribution µ on F , and G†[fj](x) =

uj(x) ∈ U, possibly noisy, is the output of the solution map G† : F → U. With neural operator learning, we

aim to build an approximation of G† by constructing a nonlinear parametric map

G[· ; θ] : F × Θ → U,

in the form of a neural network (NN), for some ﬁnite-dimensional parameter space Θ. Here, θ ∈ Θ is the set of parameters in the network architecture to be inferred by solving the following minimization problem

N

min Ef∼µ[C(G[f ; θ], G†[f ])] ≈ min [C(G[fj; θ], uj)],

θ∈Θ

θ∈Θ

j=1

(2.3)

where C denotes a properly deﬁned cost functional C : U × U → R. Although fj and uj are (vector) functions deﬁned on a continuum, with the purpose of doing numerical simulations, we assume that they are deﬁned on a discretization of the domain deﬁned as χ = {x1, · · · , xM } ⊂ Ω. With such a discretization to establish learning governing laws, a popular choice of the cost functional C is the mean square error, i.e.,

C(G[fj; θ], uj) := ||G[fj; θ](xi) − uj(xi)||2.
xi ∈χ
In this context, we have formulated the material response modeling problem as to learn the solution operator G of an unknown PDE system from data. To emphasize the importance and challenges of learning the solution operator rather than a particular solution u, we notice that when the operators Kb and Nb are known, existing methods, ranging from the classical discretization of PDEs with known coeﬃcients to modern machine learning (ML) approaches such as the basic version of physics-informed neural networks [42], lead to ﬁnding the solution u ∈ U for a single instance of the material parameter and loading f ∈ F. However, when constitutive laws are unknown or when defects and/or high degrees of heterogeneity are present such that the classical constitutive models may become inaccurate, the operators Kb and Nb can not be predeﬁned.
Thus, our goal is to provide a neural operator, i.e., an approximated solution operator G[·; θ] : f → u that delivers solutions of the system for any input f . This is a more challenging task for several reasons. First, in contrast to the classical NN approaches where the solution operator is parameterized between ﬁnite-

7

dimensional Euclidean spaces [56–60], the neural operators are built as mappings between inﬁnite-dimensional spaces, and they are resolution independent. As the consequence, no further modiﬁcation or tuning will be required for diﬀerent resolutions in order to achieve the same level of solution accuracy [49, 51, 52]. Second, for every new instance of material microstructure and/or loading scenarios f , the neural operators require only a forward pass of the network, which implies that the optimization problem (2.3) only needs to be solved once and the resulting NN can be utilized to solve for multiple instances of the input parameter. This property is in contrast to the classical numerical PDE methods [76–78] and some ML approaches [42, 62– 65], where the optimization problem needs to be solved for every new instance of the input parameter of a known governing law. Finally, of fundamental importance is the fact that the neural operators can ﬁnd solution maps regardless of the presence of an underlying PDE and only require the observed data pairs {(fj, uj)}Nj=1. Therefore, learning a data-driven neural operators would be particularly promising when the mechanical responses are provided by experimental measurements such as the displacement tracking data from DIC (see Section 5) or molecular dynamics simulations [79, 80] for which the material governing equations are not available.
2.2. Three relevant integral neural operator architectures We now discuss the network architecture of three relevant integral neural operator learning methods,
namely, the GKNs [49, 50], NKNs [52], and FNOs [51]. To provide a consistent description of all three networks and illustrate their connections with the proposed IFNO architecture, we describe each model following a formulation similar to the one presented in [52].
Lifting Layer. In integral neural operator models, we ﬁrst lift the input f (·) ∈ F to a representation (feature) h(·, 0) that corresponds to the ﬁrst network layer (also known as the lifting layer, see, e.g., [81]). In this section, we identify the ﬁrst argument of h with space (the set of nodes) and the second argument with time (the set of layers). Given an input vector ﬁeld f (x) : Rs → RdF , we deﬁne the ﬁrst network layer as

h(x, 0) = P[f ](x) := P (x)f (x) + p(x).

Here, P (x) ∈ Rd×dF and p(x) ∈ Rd deﬁne an aﬃne pointwise mapping. In practice, P (x) and p(x) are often taken as constant parameters, i.e., P (x) ≡ P and p(x) ≡ p.
Iterative Kernel Integration Layers. Then, we formulate the NN architecture in an iterative manner:

h(·, l∆t) = Ll[h(·, (l − 1)∆t)], l = 1, · · · , L,

(2.4)

where h(·, j∆t), j = 0, · · · , L := T /∆t, is a sequence of functions representing the values of the network at each hidden layer, taking values in Rd. L1, · · · , LL are the nonlinear operator layers deﬁned via the action of the sum of a local linear operator (i.e., a nonlocal integral kernel operator) and a bias function. Within each

8

layer, we treat the nodes within a layer as a continuum so that we have an inﬁnite number of nodes, i.e., a layer has an inﬁnite width. As such, each layer representation can be seen by a function of the continuum set of nodes Ω ⊂ Rs. Then, we denote the l-th network representation by h(x, l∆t) : Rs × N+ → Rd, or, equivalently, h(x, l∆t) = h(x, t) : Rs × (0, T ] → Rd. Here, l = 0 (or equivalently, t = 0) denotes the ﬁrst hidden layer, whereas t = L∆t (or t = T ) for the last hidden layer. The use of the symbol t stems from the relationship that can be established between the network update and a time stepping scheme.
Projection Layer. Third, the output u(·) ∈ U is obtained through a projection layer. In particular, we project the last hidden layer representation h(·, T ) onto U as:

u(x) = Q[h(·, T )](x) := Q2(x)σ(Q1h(x, T ) + q1(x)) + q2(x).

Here, Q1(x) ∈ RdQ×d, Q2(x) ∈ Rdu×dQ , q1(x) ∈ RdQ and q2(x) ∈ Rdu are the appropriately sized matrices and vectors that are part of the parameter set that we aim to learn. σ is an activation function. Unless otherwise stated, in this work we choose σ to be the popular rectiﬁed linear unit (ReLU) function:


 0, ReLU(x) :=
 x,

for x ≤ 0; for x > 0.

(2.5)

Similarly as for the lifting layer, Q1(x), Q2(x), q1(x) and q2(x) are also often taken as constant parameters, which will be denoted as Q1, Q2, q1 and q2, respectively.
To sum up, the integral neural operators can be written as mappings of the form:

G[f ; θ] = Q ◦ LL ◦ LL−1 ◦ · · · ◦ L1 ◦ P[f ].

(2.6)

The architectures of the GKNs, FNOs, NKNs, and our IFNOs mainly diﬀer in the design of their iterative layer update rules in (2.4), which will be elaborated in more detail for each method below. We also summarize their beneﬁts and limitations in Table 1, to highlight that the proposed IFNOs are designed in such a way that all the beneﬁts of these approaches are preserved, while the limitations are overcome.
Graph Kernel Networks (GKNs). As the ﬁrst integral neural operator, the GKNs introduced in [49] have the foundation in the representation of the solution of a PDE by the Green’s function. In the GKNs, it is assumed that the iterative kernel integration part is invariant across layers, i.e.,
L1 = L2 = · · · = LL := LGKN ,

with the update of each layer network given by

h(x, (l + 1)∆t) = LGKN [h(x, l∆t)] := σ

ˆ
W h(x, l∆t) + κ(x, y, f (x), f (y); v)h(y, l∆t)dy + c
Ω

. (2.7)

9

Here, σ is an activation function, W ∈ Rd×d and c ∈ Rd are the learnable tensors, and κ ∈ Rd×d is a tensor kernel function that takes the form of a (usually shallow) NN whose parameters v are to be learned. The GKN resembles the original ResNet block [82], where the usual discrete aﬃne transformation is substituted by a continuous integral operator. Therefore, the learnt network parameters are resolution-independent: the learned W , c, and v are close to optimal even when used with diﬀerent resolutions, i.e., with diﬀerent partitions/discretizations of the domain Ω. However, despite its advantage on resolution-independence, in the presence of complex learning tasks the applicability of the GKNs may become compromised by two factors. First, in the most general version of the GKNs, the integral in (2.7) is realized through a message passing graph neural network architecture on a fully-connected graph. Therefore, the GKNs are generally much more expensive than other integral neural operators, say, FNOs, making the GKNs less favorable for large-scale problems. Second, although single-layer and shallow GKNs have been shown to be successful in learning governing equations, e.g., the Darcy [49] and Burgers [50] equations, it was found in [52] that the GKNs may become unstable when the number of its layers increases. As the GKN becomes deeper, either there is no gain in accuracy or increasing values of the loss function occur.
Nonlocal Kernel Networks (NKNs). As a deeper and stabilized modiﬁcation of the GKNs, the NKNs are introduced in [52] to handle both learning governing equations and classifying images tasks. The NKN stems from the interpretation of the neural network as a discrete nonlocal diﬀusion reaction equation that, in the limit of inﬁnite layers, is equivalent to a parabolic nonlocal equation. Therefore, its stability in the deep layer limit can be analyzed via nonlocal vector calculus. In the NKNs, the iterative kernel integration is also assumed to be layer-independent. Diﬀers from the GKNs where the next layer representation is deﬁned via a nonlinear operator, the increment of each layer network representation is deﬁned as a nonlinear operator in the NKNs. In particular, the network hidden layer update is given as

h(x, (l + 1)∆t) = LNKN [h(x, l∆t)] ˆ
:= h(x, l∆t) + ∆t κ(x, y, f (x), f (y); v)(h(y, l∆t) − h(x, l∆t))dy − W (x; w)h(x, l∆t) + c
Ω

. (2.8)

As for the GKNs, the kernel tensor function κ ∈ Rd×d is modeled by a NN parameterized by v. The reaction term W ∈ Rd×d is modeled by another NN parameterized by w. The NKN architecture preserves the continuous, integral treatment of the interactions between nodes that characterizes the GKNs, and hence enables resolution independence with respect to the inputs. On the other hand, by modeling the layer representation increment and identifying the number of layers with the number of time steps in a time-discretization scheme, the training of deep NNs in the NKNs is accelerated via the shallow-to-deep technique [83]. In particular, it is obvious to see that by diving both sides of (2.8) by ∆t, the term (h(·, (l + 1)∆t) − h(·, l∆t))/∆t corresponds to the discretization of a ﬁrst-order derivative so that this architecture can be interpreted as a nonlinear diﬀerential equation in the limit of deep layers, i.e., as ∆t → 0. Thus, the optimal parameters (v, w and c) of a shallow network are interpolated and will be reused in a deeper one

10

as initial guesses. In [52], it is found that the NKNs generalize well to diﬀerent resolutions and stays stable when the network is getting deeper.
Similarly to the GKNs, since the building blocks of the NKNs are integral operators characterized by space dependent kernels with minimal assumptions, they come at the price of a higher computational cost compared to other networks whose kernels have a convolutional structure (e.g., the standard CNN and FNO). Hence, the NKNs are computationally more expensive than the FNOs, and generally less favorable in large-scale learning tasks.
Fourier Neural Operators (FNOs). The Fourier neural operator (FNO) was ﬁrst proposed in [51], where the integral kernel κ is parameterized in the Fourier space. In particular, the FNO drops the dependence of kernel κ on the input b and assumes that κ(x, y; v) := κ(x − y; v). The integral operator in (2.7) then becomes a convolution operator so that κ can be parameterized directly in the Fourier space. The corresponding l−th layer update is then given by
h(x, (l + 1)∆t) = LFl+N1O[h(x, l∆t)] := σ Wlh(x, l∆t) + F −1[F [κ(·; vl)] · F [h(·, l∆t)]](x) + cl(x) , (2.9)
where F and F −1 denote the Fourier transform and its inverse, respectively. In practice, F and F −1 are computed using the the FFT algorithm and its inverse to each component of h separately, with the highest modes truncated and keeping only the ﬁrst k modes. cl(x) deﬁnes a pointwise bias, which is often taken as a constant bias cl(x) ≡ cl (see, e.g., [81, 84]). Therefore, F [h(·, l∆t)] has the shape d × k, and the trainable parameters for each hidden layer will be cl ∈ Rd, Wl ∈ Rd×d, and F [κ(·; vl)] := Rl ∈ Cd×d×k. Here, we use Wl, cl and vl to highlight the fact that in the FNOs, each layer has diﬀerent parameters (i.e., diﬀerent kernels, weights and biases). This is diﬀerent from the layer-independent kernel in the GKNs and NKNs, and makes the total number of trainable parameters in the FNOs as DOF F NO := [d(1 + dF )] + [L(d + d2 + 2d2k)] + [dQ(d + du + 1) + du]. Here, the ﬁrst part is the number of parameters associated with the lifting layer, the second part is associated with the L iterative kernel integration layers, and the last part comes from the projection layer. As the network gets deeper, the second part dominates the total number of parameters, and therefore, the number of trainable parameters in the FNOs grows almost linearly with the increase of L.
Comparing with the GKN and NKN, the FNO has superior eﬃciency because one can use the FFT to compute (2.9). Moreover, in [84], Kovachiki et al. have proved that with suﬃciently large depth L, the FNOs are universal in the sense that they can approximate any continuous operator to a desired accuracy. However, the number of trainable parameters in the FNOs increases as the network gets deeper, which makes the training process of the FNOs more challenging and potentially prone to over-ﬁtting. In [52], it was found that when the network gets deeper, the training error decreases in the FNO while the test error becomes much larger than the training error, indicating that the network is overﬁtting the training data. Furthermore, if one further increases the number of hidden layer L, training the FNOs becomes challenging
11

Figure 1: The architecture of IFNO: start from input f (x), then 1) Lift to a high dimensional feature space by the lifting layer P and obtain the ﬁrst hidden layer representation h(x, 0); 2) Apply L iterative layers with the formulation proposed in (3.4); 3) Project the last hidden layer representation h(x, L∆t) back to the target dimension by a shallow network Q.
due to the vanishing gradient phenomenon. On the other hand, as reported in [68], the vanilla version of the FNO is generally restricted to simple geometries and structured data. Although the FNO has superior eﬃciency and is a theoretically proved universal approximator, its application is generally limited to the cases when the data is structured and less complex such that a shallow network would be suﬃcient.
3. Implicit Fourier Neural Operators (IFNOs) To overcome the limitations of the architectures mentioned in Section 2.2, we propose Implicit Fourier
Neural Operators (IFNOs), an eﬃcient, deep, and stable integral neural operator for solution operator learning problems. In particular, we ﬁrst formulate the solution operator as an implicitly deﬁned mapping, and then propose to model it as a ﬁxed point, not via an explicit mapping. Based on this idea, we provide the hidden layer network formulation for the IFNO and illustrate the shallow-to-deep training technique. While the former reduces the number of trainable parameters and memory cost, the latter aims to resolve the diﬃculty of network training in the limit of deep layers. Finally, we discuss the expressiveness of the IFNOs by showing that as far as there exists a converging ﬁxed point equation for the target implicit problem, the IFNOs would be universal. In the present study, we assume that the datum are structured so the FFT can be employed, and we also note that when the problem domain Ω and the discretization χ are not structured, one might employ the nonlinear mapping extension technique developed in [68] to obtain a structured datum so that the IFNO, based on the following discussion, is still applicable.
12

3.1. The network architecture We now propose an IFNO for the solution of the problem outlined in (2.1). To see a guiding principle
for our architecture, let us consider the following boundary displacement example:

Kb[u](x) = g(x), x ∈ Ω, u(x) = uD(x), x ∈ ∂Ω,

(3.1)

where Kb is a diﬀerential operator depending on the (possibly nonlinear) material constitutive law, and uD is the prescribed displacement on the boundary. Given a discretization of the domain deﬁned as χ = {x1, · · · , xM }, the desired network output, or equivalently the numerical solution of (2.1), is then U = [U1, U2, · · · , UM ] ≈ [u(x1), · · · , u(xM )]. Here, we assume, without loss of generality, that the ﬁrst β number of points are on ∂Ω, and therefore the solution U on these points is prescribed by the displacement boundary condition uD. With proper discretization methods, such as the ﬁnite diﬀerence method, for the diﬀerential operator Kb and an instance of input vector F = [b(x1), · · · , b(xM ), g(x1), · · · , g(xM ), u(x1), · · · , u(xβ), 0, · · · , 0], the numerical solution U is determined from the following implicit system of equations:


   H(U ; F ) :=    

U1 − uD(x1) ...
Uβ − uD(xβ) Kbh(U ) − G


    = 0.   

(3.2)

Herein, G := [g(xβ+1), · · · , g(xM )] is the loading term, and Kbh is the discretized operator. To solve for U from the nonlinear system in (3.2), one can employ ﬁxed-point iteration methods, such as its special case – the Newton-Raphson method. In particular, with an initial guess of the solution (denoted as U 0), the process is repeated to produce successively better approximations to the roots of (3.2) following:

U l+1 = U l − (∇H(U l; F ))−1H(U l; F ) := U l + R(U l, F ),

(3.3)

until a suﬃciently precise value is reached. Here, we noticed that for each implicit problem, there are inﬁnite numbers of the corresponding ﬁxed point equations, and (3.3) is just one example. In fact, the ﬁxed point method solves the implicit system as long as there exists one ﬁxed point equation with a convergent and unique solution.
Guided by the representation in (3.3), we argue that the desired network output is more aptly described implicitly, and propose to develop a network architecture to model the operator R and mimic the ﬁxed point method by design. Using the same notations of Section 2, we propose the following iterative network update

13

formulation
h(x, (l + 1)∆t) =LIF NO[h(x, l∆t)] :=h(x, l∆t) + ∆tσ W h(x, l∆t) + F −1[F [κ(·; v)] · F [h(·, l∆t)]](x) + c(x) .

(3.4)

Note that although the FFT is still applied to each component of h separately with the highest modes truncated as for the FNOs, the hidden layer parameters are taken to be layer-independent, which is distinctly diﬀerent from the FNOs. Following the conventions in the FNOs, we also take the bias cl(x) as a constant bias (c(x) ≡ c) in all the subsequent numerical tests. Therefore, the set of trainable parameters in our IFNOs are P ∈ Rd×dF and p ∈ Rd for the lifting layer, Q1 ∈ RdQ×d, Q2 ∈ Rdu×dQ , q1 ∈ RdQ and q2 ∈ Rdu for the projection layer, and c ∈ Rd, W ∈ Rd×d and F (κ(·; v)) = R ∈ Cd×d×k for the hidden layers. The total number of trainable parameters is DOF IF NO := [d(1 + dF )] + [d + d2 + 2d2k] + [dQ(d + du + 1) + du], which is independent of the number of hidden layers L, alleviating the major bottleneck of the overﬁtting issue encountered by the original FNOs with a deeper network. Moreover, this feature also enables the straightforward application of the shallow-to-deep initialization technique.
As the layer becomes deep (∆t → 0), (3.4) can be seen as an analog of a discretized ordinary diﬀerential equations (ODEs). This allows us to exploit the shallow-to-deep learning technique described in Section 2 for the NKNs. Similarly to in (2.8), we can reinterpret the network update as the time discretization of a diﬀerential equation and use the optimal parameters obtained with L layers as the initial guesses for deeper networks. Speciﬁcally, let W , c and R be the optimal network parameters obtained by training an IFNO of depth L. For further improving the accuracy of the network, we can increase the number of layers (or equivalently, time steps), and train a new network of depth L > L. The idea of the shallow-to-deep technique is to perform interpolation in time (or across layers) over the optimal parameters obtained at depth L and to scale them in such a way that the ﬁnal time of the diﬀerential equation remains unchanged. In our speciﬁc setting, due to the fact that the network parameters are not time dependent, this technique simply corresponds to the initialization of the (deeper) L-layer network by W , c and R.
As a further note, we point out that although the idea of using repeated hidden layers has not been explored for the FNOs, resembling ﬁxed-point methods is not new for neural networks. In [69–72], implicit networks are introduced as an analog to a forward pass through an “inﬁnite depth” network, without storing the intermediate quantities of the forward pass for back-propagation, and hence can be trained using constant memory costs with respect to depth. One can see that our IFNO architecture requires only constant memory cost, similar to implicit networks. Moreover, it preserves the continuous, integral treatment of the interactions between nodes that characterizes integral neural operators. Therefore, the IFNO provides a new and eﬃcient implicit-type neural operator architecture – that is why it is named “implicit”.
Table 1 summarizes relevant properties of the IFNOs in comparison with other integral neural operators. In summary, being a resemblance of an implicit equation solver and stable in the limit of deep layers make

14

the proposed IFNO’s architecture a viable tool for modeling problems with complex material responses, since these problems can be considered as PDE solution operator learning tasks.
3.2. Universal approximation properties In this section, we show that the IFNOs are universal solution ﬁnding operators, in the sense that they
can approximate a ﬁxed point method to a desired accuracy. Without loss of generality, we consider a 1D domain Ω ⊂ R, f (x) := [x, fˆ(x)] ∈ R2 and u(x) ∈ R. The function u ∈ C(Ω) is evaluated at uniformly distributed nodes χ = {x1, x2, . . . , xM }. Let us denote U ∗ := [u(x1), u(x2), . . . , u(xM )] as the solution we seek, U 0 := [x1, · · · , xM ] as the initial guess, C = [c(x1), · · · , c(xM )] as the collection of pointwise bias vectors c(xi), and F := [fˆ(x1), fˆ(x2), . . . , fˆ(xM )] as the loading vector. We aim to show that for any desired accuracy ε > 0, one can ﬁnd a suﬃciently large L > 0 and a set of parameters θε = {P, p, Q1, Q2, q1, q2, C, W, R}, such that the resultant IFNO model satisﬁes
Q ◦ (LIF NO)L ◦ P([U 0, F ]T) − U ∗ ≤ ε, ∀F ∈ RM .

Here, the matrix and vector parameters in the lifting and projection layers are taken as pointwise functions. With a slight abuse of notation, we denote P ∈ RdM×dF M as the collection of the pointwise weight matrices at each discretization point in χ, and a similar convention applies for other matrix and vector parameters in the lifting and projection layers. Hence, the dimension of all trainable parameters are: C ∈ RdM , W ∈ Rd×d, R ∈ Cd×d×k, P ∈ RdM×dF M , p ∈ RdM , Q1 ∈ RdQM×dM , Q2 ∈ RduM × dQM , q1 ∈ RdQM and q2 ∈ RduM . With the assumption that f (x) ∈ R2 and u(x) ∈ R, we note that dF = 2 and du = 1. As will be seen in the proof below, we will further take dQ = d. For the simplicity of notation, in this section we organize the feature vector H ∈ RdM in a way such that the components corresponding to each discretization point are adjacent, i.e., H = [H(x1), · · · , H(xM )] and H(xi) ∈ Rd. For simplicity, we further assume that the Fourier coeﬃcient is not truncated, and all available frequencies will be used. We point out that under this circumstance, we have k = M and the (discretized) iterative layer can be written as
LIF NO[H(l∆t)] =H(l∆t) + ∆tσ W˜ H(l∆t) + Re(F∆−x1(R · F∆x(H(l∆t)))) + C =H(l∆t) + ∆tσ V IF NOH(l∆t) + C ,

with











V

IF NO

:=

 Re 







M −1

Rn+1 + W

n=0

M −1

Rn+1

exp(

2iπ∆xn M

)

n=0

...

M −1

Rn+1

exp(

2iπ∆xn M

)

n=0

M −1

Rn+1 + W

n=0
...

M −1



...

Rn+1

exp(

2iπ(M −1)∆xn M

)

n=0 

M −1



. . . Rn+1 exp( 2iπ(MM−2)∆xn )

n=0

.

...

...

 



M −1

M −1



Rn+1

exp(

2iπ(M −1)∆xn M

)

Rn+1

exp(

2iπ(M −2)∆xn M

)

...

n=0

n=0

M −1



Rn+1 + W



n=0

15

Here, R ∈ CM×d×d with Ri ∈ Cd×d being the component associated with each discretization point xi ∈ χ, V IF NO ∈ RdM×dM , C ∈ RdM , W˜ := W ⊕ W ⊕ · · · ⊕ W is a dM × dM block diagonal matrix formed by W ∈ Rd×d, F∆x and F∆−x1 denote the discrete Fourier transform and its inverse, respectively. By further taking R2 = · · · = RM = W = 0, a d × d matrix with all its elements being zero, it suﬃces to show the universal approximation property for an iterative layer as follows:
LIF NO(H(l∆t)) := H(l∆t) + ∆tσ V˜ H(l∆t) + C
where V˜ := 1[M,M] ⊗ V with V ∈ Rd×d and 1[m,n] being an m by n all-ones matrix. Before stating our main theoretical results, we need the following assumptions on U ∗ and R:
Assumption 1. There exists a ﬁxed point equation, U = U + R(U , F ) for the implicit problem (3.2), such that R : R2M → RM is a continuous function satisfying R(U ∗, F ) = 0 and ||R(Uˆ , F ) − R(U˜ , F )||l2(RM ) ≤ m||Uˆ − U˜ ||l2(RM ) for any two vectors Uˆ , U˜ ∈ RM . Here, m > 0 is a constant independent of F . Assumption 2. With the initial guess U 0 := [x1, · · · , xM ], the ﬁxed-point iteration
U l+1 = U l + R(U l, F ), l = 0, 1, . . . .
converges, i.e., for any given ε > 0, there exists an integer L such that
||U l − U ∗||l2(RM ) ≤ ε, ∀l > L,
for all possible input instances F ∈ RM and their corresponding solutions U ∗. Next, we prove that the IFNOs are universal, i.e., give a ﬁxed point method and solution U ∗ satisfying
Assumptions 1-2, one can ﬁnd an IFNO whose output approximates U ∗ to a desired accuracy, ε > 0. To be more precise, we will prove the following theorem: Theorem 1 (Universal approximation). Let U ∗ = [u(x1), u(x2), . . . , u(xM )] be the ground-truth solution that satisﬁes Assumptions 1-2, the activation function σ for all iterative kernel integration layers be the ReLU function, and the activation function in the projection layer be the identity function. Then for any ε > 0, there exist suﬃciently large layer number L > 0 and feature dimension number d > 0, such that one can ﬁnd a parameter set θε = {P, p, Q1, Q2, q1, q2, C, V } with P ∈ RdM×2M , p ∈ RdM , Q1 ∈ RdM×dM , Q2 ∈ RM×dM , q1 ∈ RdM , q2 ∈ RM , C ∈ RdM , V ∈ Rd×d with the corresponding IFNO model satisﬁes
Q ◦ (LIF NO)L ◦ P([U 0, F ]T) − U ∗ ≤ ε, ∀F ∈ RM .
Before proceeding to the proof of this main theorem, we ﬁrst show the approximation property of a shallow neural network:
16

Lemma 1. Given a continuous function T : R2M → RM , and a non-polynomial and continuous activation function σ, for any constant ε > 0 there exists a shallow neural network model Tˆ := Sσ (BX + A) such that

||T (X) − Tˆ (X)||l2(RM ) ≤ ε, ∀X ∈ R2M ,

for suﬃciently large feature dimension d˜ > 0. Here, S ∈ RM×d˜M , B ∈ Rd˜M×2M , and A ∈ Rd˜M are matrices/vectors which are independent of X.

Proof. As shown in [85], when σ is non-polynomial and continuous, span(σ(r · X + a)) is dense in C(R2M ), where r ∈ R1×2M , and a ∈ R. Therefore, denoting T (X) = [T1(X), . . . , TM (X)], for each Ti(X) ∈ R there exist si ∈ R1×d˜, Bi ∈ Rd˜×2M , and ai ∈ Rd˜, such that

ε |Ti(X) − siσ(BiX + ai)| ≤ √ ,
M

∀X ∈ R2M .

Let





s1 0 . . . 0

 B1

 a1



 0 s2 . . .

S

:=

 

..

.

... . . .





0 

M ×d˜M

..

∈R 

,

.



 B2  d˜M×2M

B :=  

..

∈R 

,

.



 a2  d˜M

A= 

..

∈R 

,

.









0 0 . . . sM

BM

aM

we then obtain

||T (X) − Sσ (BX + A) ||l2(RM ) =

M
|Ti(X) − siσ(BiX + ai)|2 ≤
i=1

ε2 M × = ε,
M

for all X ∈ R2M .
We now proceed to the proof of Theorem 1:
Proof. Since U ∗ satisﬁes Assumptions 1-2, for any ε > 0, we ﬁrst pick a suﬃciently large integer L such that ||U L − U ∗||l2(RM ) ≤ 2ε . As shown in Lemma 1, with suﬃciently large feature dimension d˜ > 0, one can ﬁnd S ∈ RM×(d˜−1)M , B ∈ R(d˜−1)M×2M , and A ∈ R(d˜−1)M , such that Rˆ (U , F ) := Sσ(B[U , F ]T + A) satisﬁes
||R(U , F ) − Rˆ(U , F )||l2(RM ) = ||R(U , F ) − Sσ(B[U , F ]T + A)||l2(RM ) ≤ 2(1 m+εm)L ,

where m is the contraction parameter of R, as deﬁned in Assumption 1. By this construction, we know that S has independent rows. Hence, there exists the right inverse of S, which we denote as S+ ∈ R(d˜−1)M×M , such that

SS+ = IM , S+S := I˜(d˜−1)M ,

17

where IM is the M by M identity matrix, I˜(d˜−1)M is a (d˜− 1)M by (d˜− 1)M block matrix with each of its element being either 1 or 0. Hence, for any vector Z ∈ R(d˜− 1)M , we have σ(I˜(d˜−1)M Z) = I˜(d˜−1)M σ(Z). Moreover, we note that S has a very special structure: from the ((i − 1)(d˜− 1) + 1)-th to the (i(d˜− 1))-th column of S, all nonzero elements are on its i-th row. Correspondingly, we can also choose S+ to have a special structure: from the ((i − 1)(d˜− 1) + 1)-th to the (i(d˜− 1))-th row of S+, all nonzero elements are on its i-th column. Hence, when multiplying S+ with U , there will be no entanglement between diﬀerent components of U . That means, S+ can be seen as a pointwise weight function.
We now construct the IFNO as follows. In this construction, we choose the feature dimention as d := d˜M . With the input [U 0, F ] ∈ R2M , for the lift layer we set





T

S+ 0

S+ 0 S+ 0 · · · S+ 0

P := 1[M,1] ⊗ 

=

 ∈ RdM×2M , and p := 0 ∈ RdM .

0 IM

0 IM 0 IM · · · 0 IM

repeated for M times

As such, the initial layer of feature is then given by

H0 = P([U 0, F ]T) = 1[M,1] ⊗ [S+U 0, F ]T ∈ RdM .

Here, we point out that P and p can be seen as pointwise weight and bias functions, respectively. Next we construct the iterative layer LINF O, by setting





I˜(d˜−1)M B/M S/∆t

V := 



0

0


0  , V˜ := 1[M,M] ⊗ V, IM /∆t





I˜(d˜−1)M A/∆t

and C := 1[M,1] ⊗ 

.

0

Note that V˜ falls into the formulation of V IF NO, by letting R1 = V and R2 = R2 = · · · = RM = W = 0. For the l + 1-th layer of feature vector, we then arrive at

H((l + 1)∆t) = H(l∆t) + ∆tσ V˜ H(l∆t) + C





S+S =H(l∆t) + IM ⊗ 

0

 

  



0

B/M

S

 σ 1[M,1] ⊗ 

 1[1,M] ⊗ 

IM

0

0



 

0

A

 H(l∆t) + 1[M,1] ⊗   ,

IM

0

where H(l∆t) = [hˆ l1∆t, hˆ l2∆t, . . . , hˆ l2∆Mt−1, hˆ l2∆Mt]T denotes the (spatially discretized) hidden layer feature at the l−th iterative layer of the IFNO. Subsequently, we note that the second part of the feature vector, hˆ l2∆j t ∈ RM , satisﬁes

hˆ (2lj+1)∆t = hˆ l2∆j t = · · · = hˆ 02j = F , ∀l = 0, · · · , L − 1, ∀j = 1, · · · , M

18

Problem Porous medium I Porous medium II
Fiber-reinforced material
Glass-ceramics fracture Latex glove sample (palm region)

Data collected from Darcy’s equation
Darcy’s equation
Holzapfel-Gasser-Odgen (HGO) model
Quasi-static linear peridynamic solid model Digital Image Correlation (DIC) displacement tracking

Input function Permeability ﬁeld
Source ﬁeld & boundary condition
Boundary condition
Boundary displacement & previous damage ﬁeld Boundary condition &
previous displacement ﬁeld

Output function Pressure ﬁeld Pressure ﬁeld
Displacement ﬁeld
Damage ﬁeld
Displacement ﬁeld

Table 2: Setup for the three numerical examples in Section 4 and the application in Section 5.

Hence, the ﬁrst part of the feature vector, hˆ l2∆j−t 1 ∈ R(d˜−1)M , satisﬁes the following iterative rule:

hˆ (2lj+−11)∆t = hˆ l2∆j−t 1 + S+Sσ(B[Shˆ l2∆j−t 1, F ]T + A), ∀l = 0, · · · , L − 1, ∀j = 1, · · · , M,

and hˆ (1l+1)∆t = hˆ (3l+1)∆t = · · · = hˆ (2lM+1−)1∆t.
Finally, for the projection layer Q, we set the activation function in the projection layer as the identity function, Q1 := IdM (the identity matrix of size dM ), Q2 := [S, 0] ∈ RM×dM , q1 := 0 ∈ RdM , and q2 := 0 ∈ RM . Denoting the output of IFNO as UIF NO := Q ◦ (LIF NO)L ◦ P([U 0, F ]T), we now show that UIF NO can approximate U ∗ with a desired accuracy ε:

||UIF NO − U ∗|| ≤ ||UIF NO − U L||l2(RM ) + ||U L − U ∗||l2(RM )

≤ ||Shˆ L1 ∆t − U L||l2(RM ) + 2ε (by Assumption 2) ≤ ||Shˆ (1L−1)∆t − U L−1||l2(RM ) + ||Rˆ (Shˆ (1L−1)∆t, F ) − R(U L−1, F )||l2(RM ) + 2ε

≤ ||Shˆ (1L−1)∆t − U L−1||l2(RM ) + ||Rˆ (Shˆ (1L−1)∆t, F ) − R(Shˆ 1(L−1)∆t, F )||l2(RM )

+ ||R(Shˆ (1L−1)∆t, F ) − R(U L−1, F )||l2(RM ) + 2ε ≤ (1 + m)||Shˆ (1L−1)∆t − U L−1||l2(RM ) + 2(1 m+εm)L + 2ε

(by Lemma 1 and Assumption 1)

≤ mε (1 + (1 + m) + (1 + m)2 + · · · + (1 + m)L−1) + ε

2(1 + m)L

2

εε ≤ + = ε.
22

4. Numerical Examples In this section, we illustrate the performance of the proposed IFNOs on three benchmark material mod-
eling problems: (i) the ﬂow through a porous medium, (ii) the deformation of a hyperelastic and anisotropic ﬁber-reinforced material, and (iii) the brittle fracture mechanics in glass-ceramics. The detailed settings of
19

each example, including the choices of high-ﬁdelity (ground-truth) training/testing data generation, input function, and output function, are provided in Table 2. For all numerical experiments, we compare the IFNO to the FNO as the baseline approach, since the other two integral neural operators (i.e., the GKNs and the NKNs) are computationally much more expensive and hence not feasible given the data size and our computational resources. All our numerical experiments were performed on a machine with 2.8 GHz 8-core CPU and a single Nvidia RTX 3060 GPU. For the implementation of the IFNOs and the FNOs, we used the Pytorch package provided in [51]. The optimization was performed with the Adam optimizer. To conduct a fair comparison, for each method, we tuned the hyperparameters, including the learning rates, the decay rates and the regularization parameters, to minimize the training loss. Furthermore, for each example and each method, we repeated the numerical experiment for ﬁve diﬀerent random initializations, and reported the averaged relative mean squared errors and their standard errors. For a compact presentation of the results, we reported the relative mean squared errors in plots, as functions of the number of hidden layers (L), with error bars representing the standard errors over ﬁve simulations. A more detailed error comparison is provided in the appendix.
4.1. The ﬂow through a porous medium We consider the modeling problem of two-dimensional sub-surface ﬂows through a porous medium with
heterogeneous permeability ﬁeld. Following the settings in [49], the high-ﬁdelity synthetic simulation data for this example are described by the Darcy’s ﬂow. Here, the physical domain is D = [0, 1]2, b(x) is the permeability ﬁeld, and the operator Kb is then an elliptic operator associated with b(x). In particular, the Darcy’s equation has the form:
−∇ · (b(x)∇u(x)) = g(x), x ∈ Ω, u(x) = uD(x), x ∈ ∂Ω.
In this context, our goal is to learn the solution operator of the Darcy’s equation and compute the pressure ﬁeld u(x). In this example two study scenarios are considered, corresponding to two diﬀerent real-world application scenarios:
1. (Porous medium I, see Figure 3) Considering a ﬁxed source ﬁeld g(x) = 1 and Dirichlet boundary condition uD(x) = 0, we aim to obtain the pressure ﬁeld u(x) for each permeability ﬁeld b(x). Therefore, the neural operators are employed to learn the mapping from f (x) := [x, b(x)] to u(x). This setting corresponds to a scenario that the same lab test protocols are applied to heterogeneous material samples with diﬀerent microstructures, and our learning goal is to predict the material response for a new and unseen sample. Note that this setting is also the benchmark problem considered in a series of integral neural operator studies [49–52].
2. (Porous medium II, see Figure 4) Considering a ﬁxed permeability ﬁeld b(x), we aim to estimate the
20

pressure ﬁeld u(x) subject to diﬀerent source ﬁelds g(x) and Dirichlet boundary conditions uD(x). That means, the neural operators are employed to learn the mapping from f (x) := [x, g(x), u˜D(x)] to u(x). This setting corresponds to a scenario that diﬀerent lab tests are available for a given material sample with unknown microstructure, and our learning goal is to predict the mechanical response of this sample under a new and unseen loading. We note that this setting reﬂects the typical material mechanical testing experiments, see, e.g., [28], where one representative material sample is tested under several loading protocols and the responses, such as the displacement ﬁelds and/or stretch-stress curves, are provided.

Model FNO, setting I IFNO, setting I FNO, setting II IFNO, setting II

L=1 171.42k 171.42k 300.48k 300.48k

L=2 338.37k 171.42k 596.45k 300.48k

L=4 672.26k 171.42k 1.19M 300.48k

L=8 1.34M 171.42k 2.37M 300.48k

L = 16 2.68M 171.42k 4.74M 300.48k

L = 32 5.35M 171.42k 9.48M 300.48k

Table 3: Example 1: the ﬂow of a ﬂuid through a porous medium. Number of trainable parameters for each model.

Figure 2: The ﬂow of a ﬂuid through a porous medium (example 1). Comparison of relative mean squared errors of pressure ﬁeld from FNOs and IFNOs. (a) Results from setting I, where neural operators are employed to solve for the corresponding pressure ﬁeld for each giving permeability ﬁeld. (b) Results from setting II, where neural operators are employed to solve for the corresponding pressure ﬁeld with each given pair of source ﬁeld g(x) and Dirichlet boundary condition uD(x).
Setting and results of porous medium I. As standard simulations of subsurface ﬂow, the permeability b(x) is modeled as a two-valued piecewise constant function with random geometry such that the two values have a ratio of 4. Speciﬁcally, we generated 1, 100 samples of b(x) according to b ∼ ψ#N (0, (−∆ + 9I)−2), where ψ takes a value of 12 on the positive part of the real line and a value of 3 on the negative. For cross-validation, the total dataset were divided into a training set with 1, 000 samples and a test dataset with 100 samples. Then, for each sample the high-ﬁdelity solution of u was generated by using a second-order ﬁnite diﬀerence scheme to solve the Darcy’s equation on a 241 × 241 grid solution, and both the input and output functions were down-sampled to a structured grid χ with grid size ∆x = 1/30. In this experiment, for both the FNOs and IFNOs, we set the dimension of h as d = 32, and the number of truncated Fourier modes as k = 9 × 9. For each depth L, we trained the network for 500 epochs with a learning rate of 1e−3, then decrease the
21

Figure 3: The ﬂow of a ﬂuid through a porous medium setting I, prediction of pressure ﬁeld from diﬀerent permeability ﬁeld b(x) and ﬁxed source ﬁeld g(x) = 1 and boundary condition uD(x) = 0 (example 1). A visualization of FNO and IFNO performances on two instances of permeability parameter b(x). Here, the best IFNO results (L = 32), best FNO results (L = 4), and the deepest FNO results (L = 32) are reported.
learning rate with a ratio 0.5 every 100 epochs. For the IFNOs, the network was trained with the shallowto-deep training procedure: we initialized the L−layer network parameters from the (L/2)−layer IFNOs model. This strategy was also employed for other examples in this paper.
In Figure 2(a) we report the averaged relative mean squared errors from setting I as a function of iterative layer number L; the number of trainable parameters for each model is provided in Table 3. We can observe that as we increases L to 8 and 16, the FNO reaches a relatively low level of error on the training dataset (O(10−4)). However, the test error of the FNOs deteriorates with the increase of L, and reaches O(10−2) when L = 8 or 16. This indicates that the network is overﬁtting the training data. Moreover, for L ≥ 32, the training of the FNOs becomes challenging due to the vanishing gradient phenomenon [86]. In contrast, the IFNOs trained with the shallow-to-deep initialization are robust and not subject to the overﬁtting issues: the test error improves as one increases L, and stays at a similar magnitude as the training error. Comparing with
22

Figure 4: The ﬂow of a ﬂuid through a porous medium setting II, prediction of pressure ﬁeld on a ﬁxed permeability ﬁeld b(x) and diﬀerent source ﬁeld g(x) and boundary condition uD(x) (example 1). A visualization of FNO and IFNO performances on two instances of g(x) and uD(x). Here, the best IFNO results (L = 32), best FNO results (L = 2), and the deepest FNO results (L = 32) are reported.
the FNOs with the same number of layers, the IFNOs have a much smaller number of trainable parameters and lower test errors in all L > 1 cases. Speciﬁcally, the IFNO reaches its best performance when L = 32, where the averaged (relative) test error is 1.02%. On the other hand, the lowest error for the FNO is 1.19%, achieved when L = 4. In Figure 3, we show the plots of solutions obtained with the best IFNO, the best FNO, and the deepest FNO, in correspondence of two instances of permeability parameter b(x). Both the solutions and the errors are plotted, showing that the FNO loses accuracy when the layer gets deeper (L = 32), while all other solutions are visually consistent with the ground-truth solutions.
Setting and results of porous medium II. In setting II, we considered a ﬁxed realization of permeability ﬁeld b(x), which was generated following the same procedure as described in setting I. In this context, our goal is to predict the pressure ﬁeld driven by diﬀerent source ﬁelds g(x) and Dirichlet boundary conditions
23

uD. To generate each sample, we set the source ﬁeld as g(x) = cos(2πaxx) cos(2πayy). Here, ax and ay are the constant coeﬃcients randomly generated as ax, ay ∼ U(0.5, 2), the uniform distribution on [0.5, 2]. To generate the boundary condition uD, we set the pressure on the top edge of the domain as uD(x, 1) = U0(t1 sin(2πx) + t2 sin(4πx))/(t1 + t2), where U0 ∼ U (−0.001, 0.001), and t1, t2 ∼ U (0, 1). On the rest of boundaries, the pressure was prescribed as uD(x, y) = U0. For training and cross-validation, we generated 600 samples in total and split it as a training set with 500 samples and a test set with 100 samples. Similar to setting I, the training and test measurements of the pressure ﬁelds u were also generated by solving the Darcy’s equation and down-sampling to a M = 31 × 31 grid. In this experiment, for both the FNOs and IFNOs, we set the dimension of h as d = 32, and the number of truncated Fourier modes as k = 12 × 12. For each depth L, we trained the network for 500 epochs with a learning rate of 3e−3, then decrease the learning rate with a ratio 0.5 every 100 epochs.
In Figures 2(b), we report the relative mean squared errors from each model, with hidden layer number L from 1 to 64. The number of trainable parameters for each model is provided in Table 3. Similarly to the porous medium setting I, when increasing the number of layers, the relative test errors of the FNOs deteriorates for L > 2, after initially decreasing. In contrast, the accuracy of the IFNOs monotonically improves for increasing values of L. Also in this case, the FNOs suﬀer from the vanishing gradient: the training becomes challenging when L > 16. In Figure 4, we depict both solutions and prediction errors obtained with the best IFNO, the best FNO, and the deepest FNO, in correspondence of two pairs of source ﬁeld g(x) and boundary condition uD(x). In particular, in this setting, the IFNO reaches its best test error, 0.49%, when L = 32. For the FNO, the best performance is achieved when L = 2, where the test error is 0.53%. When L > 2, The IFNOs consistently outperforms the FNOs in the testing experiments.
4.2. The deformation of a hyperelastic and anisotropic ﬁber-reinforced material
Figure 5: Problem setup of example 2: the deformation of a hyperelastic and anisotropic ﬁber-reinforced material. (a) A unit square subject to biaxial stretching with Dirichlet-type boundary conditions. (b) A unit square subject to uniaxial tension with Neumann-type boundary condition.
We now consider the modeling problem of a hyperelastic, anisotropic, ﬁber-reinforced material, and seek to ﬁnd its displacement ﬁeld u : [0, 1]2 → R2 under diﬀerent boundary loadings. To generate training and test samples, the Holzapfel-Gasser-Odgen (HGO) model [88] was employed to describe the constitutive
24

Parameter c10

K

k1 k2 α κ

Value 0.3846 0.8333 0.1 1.5 π/2 0

Table 4: Parameter values of the HGO model for data generation in example 2.

Set ID 1 2 3 4 5 6 7 8 9

Protocol Biaxial Stretch 1 : 1 Biaxial Stretch 0.66 : 1 Biaxial Stretch 0.5 : 1 Biaxial Stretch 0.33 : 1 Biaxial Stretch 1 : 0.66 Biaxial Stretch 1 : 0.5 Biaxial Stretch 1 : 0.33 Uniaxial Stretch in x Uniaxial Stretch in y

max Ux on the right edge 0.4 0.4 0.2 0.2 0.6 0.4 0.6 0.4 0

max Uy on the top edge 0.4 0.6 0.4 0.6 0.4 0.2 0.2 0 0.4

Table 5: Nine protocols of the synthetic biaxial mechanical testing on a 1 × 1 hyperelastic and anisotropic ﬁber-reinforced material sample. All simulations are generated with FEniCS [87] based on the HGO model.

behavior of the material in this example, with its strain energy density function given as:

η = c10 (I1 − 3) − c10 ln(J ) + k1 2 (exp (k2 Ei 2) − 1) + K 2 2k2 i=1 2

J2 − 1 − ln J .
2

Here, · denotes the Macaulay bracket, and the ﬁber strain of the two ﬁber groups is deﬁned as:

Ei = κ(I1 − 3) + (1 − 3κ)(I4i − 1), i = 1, 2,

where k1 and k2 are ﬁber modulus and the exponential coeﬃcient, respectively, c10 is the moduli for the non-ﬁbrous ground matrix, K is the bulk modulus, and κ is the ﬁber dispersion parameter. Moreover, I1 = tr(C) is the is the ﬁrst invariant of the right Cauchy-Green tensor C = FT F, F is the deformation gradient, and J is related with F such that J = det F. For the i−th ﬁber group with angle direction αi from the reference direction, I4i = nTi Cni is the fourth invariant of the right Cauchy-Green tensor C, where ni = [cos(αi), sin(αi)]T . In our simulations, we considered a material with ﬁber reinforcement in the vertical direction, and set the orientation for both ﬁber groups as αi = π/2. All parameter values are summarized in Table 4.
In this example, our goal is to learn the solution operator of the HGO model, and predict the displacement ﬁeld u(x) subject to diﬀerent boundary conditions. As depicted in Figure 5, two types of boundary conditions are considered: (i) the Dirichlet-type boundary condition where a uniform uniaxial displacement loading was applied on the right and top edges of the plate (see Figure 5(a)); and (ii) the Neumann-type boundary loading where we applied a uniaxial tension t(x) on the top edge (see Figure 5(b)). For both cases, to generate the high-ﬁdelity (ground-truth) dataset, we solved the displacement ﬁeld on the entire domain by minimizing potential energy using the ﬁnite element method implemented in FEniCS [87]. In particular, the displacement ﬁled was approximated by continuous piecewise linear ﬁnite elements with triangular mesh, and the grid size

25

was taken as 0.025. Then, the ﬁnite element solution was interpolated onto χ, a structured 41 × 41 grid which will be employed as the discretization in our neural operators.
Figure 6: The deformation of a hyperelastic and anisotropic ﬁber-reinforced material (example 2). (a-b) Comparison of relative mean squared errors of displacement ﬁeld predictions driving by displacement boundary conditions. (a) Results from in-distribution tests, where the testing boundary conditions are inside the training region. (b) Results from out-of-distribution tests, where the testing boundary conditions are outside the training region. (c-d) Results of displacement ﬁeld predictions driving by traction conditions. (c) Comparison of relative mean squared errors. (d) A visualization of L = 32 IFNO performances on two instances of traction loads on the top edge.
Learning material responses from displacement boundary conditions. We ﬁrst studied the performance of the IFNOs as a solution operator under Dirichlet-type boundary conditions. To mimic the real-world mechanical test settings (see, e.g. [28]), we generated 9 diﬀerent biaxial loading protocol sets as listed in Table 5, with 100 samples for each set. For each sample, a uniform uniaxial displacement boundary condition uD = (Ux, 0) was applied on the right edge of the plate, and another uniform uniaxial displacement uD = (0, Uy) was prescribed on the top edge. The other two edges were set as clamped on the tangential direction. Based on this boundary condition, we generated the displacement ﬁeld solution u(x) using FEniCS, serving as the highﬁdelity solution. Then, the neural operators were employed to learn the mapping from f (x) := [x, U˜x, U˜y] to u(x) := [ux(x), uy(x)], where U˜x and U˜y are the padded boundary conditions, as described in (2.2). Two study scenarios were considered to evaluate the in-distribution prediction capability and the out-ofdistribution generalizability of the proposed neural operators:
1. We randomly selected 100 samples as the test dataset from the 900 total samples, and used all other samples to form the training dataset. In this scenario, we note that the boundary conditions of test samples are inside the training region. 26

2. We used protocol set #4 (0.33 : 1 biaxial tension) as the test dataset, and all other sets as the training dataset. Note that that the 0.33 : 1 biaxial tension protocol is not covered in any of other sets. Therefore, with this scenario we aim to study the generalizability of the proposed method by testing with boundary conditions outside the training region.

For both study scenarios, we set the dimension of h as d = 32 and the number of truncated Fourier modes as k = 8 × 8 in all neural operator models. For this example, we trained the network for 500 epochs with a learning rate of 5e − 3, then decreased the learning rate with a ratio of 0.5 every 100 epochs.
In Figure 6(a-b), we provide the averaged relative mean squared errors as functions of hidden layer numbers L. In Figure 6(a), we depict the results from scenario 1. We observed that when L > 4, both the training and testing errors from the FNOs start to increase, due to the vanishing gradient issue. A similar phenomenon is observed in Figure 6(b), where the results from scenario 2 are provided. In contrast, the accuracy of the IFNOs monotonically improves for increasing values of L. For the in-distribution test scenario, the IFNOs reaches its lowest test error (0.21%) at L = 32, which almost halved the optimal error from FNOs (0.35% at L = 4). Similarly, for the out-of-distribution scenario, the best performance for the IFNO is obtained at L = 32, and the test error is 0.21%. In the mean time, the optimal FNO is still with L = 4, and achieved a slightly larger test error (0.24%). When comparing between the IFNO and the FNO with the same depth, the IFNO again achieves a better accuracy whenever the network is deeper than 4. Diﬀerent from example 1, in this example we did not observe much overﬁtting problem, possibly due to the fact that the material microstructure and loading settings have low complexity: the material is assumed to be homogeneous, and the displacement-type boundary conditions are uniform. All these facts are anticipated to reduce the complexity of this learning task, so the material responses in testing datasets do not vary much from the responses in the training dataset, even in the out-of-distributing prediction scenario.

Learning material responses from traction boundary conditions. In the previous examples and experiments,

we have investigated the performance of integral neural operators on predicting material responses driven by

Dirichlet-type boundary conditions. Here, we further studied the material deformation driven by Neumann-

type boundary conditions, as depicted in Figure 5(b). With the IFNOs, we aim to learn the solution

operator which predicts the resultant displacement ﬁeld u(x) driven by diﬀerent traction boundary conditions t(x) = [0, Ty(x)]. In this context, the input function is f (x) := [x, T˜y(x)], where T˜y(x, y) := Ty(x, 1) is the

padded function of Ty(x) onto the whole domain Ω. The output function is the displacement ﬁeld. To

generate the training/testing dataset, we sampled 1, 000 diﬀerent vertical traction conditions Ty(x) on the

top edge from a random ﬁeld, following the algorithm in [67, 89]. In particular, Ty(x) is taken as the

restriction of a 2D random ﬁeld, φ(x) = F −1(γ1/2F (Γ ))(x), on the top edge. Here, Γ (x) is a Gaussian

white

noise

random

ﬁeld

on

R2,

γ

=

(w12

+

w22

)−

5 4

represents

a

correlation

function,

and

w1,

w2

are

the

wave numbers on x and y directions, respectively. Then, for each sampled traction loading, we performed a

FEniCS simulation based on the HGO model, to obtain the solutions in the entire domain and collect the

27

corresponding solutions of displacement ﬁelds of in Ω. Among these 1, 000 samples, 800 cases were employed as the training data while the rest was kept as testing data. In this setting, we train the network for 500 epochs with a learning rate of 5e-3, then decreased the learning rate with a ratio of 0.5 every 100 epochs.
In Figure 6(c), we show the relative mean squared errors from each neural operator model with respect to diﬀerent hidden layer numbers L. Similarly to the Dirichlet-type boundary cases, the IFNO achieves its best performance at L = 32, while the FNO suﬀers from vanishing gradient when L > 8. In Figure 6(d), we compare the horizontal displacement ux and vertical displacement uy in Ω between the FEniCS ground truth and the L = 32 IFNO prediction, together with the prediction errors. To illustrate the network generalizability to diﬀerent traction boundary conditions, the results on two instances of Ty(x) among the test samples are illustrated. We observe that the IFNO predictions match well with the ground truth solution, demonstrating the capability of our proposed method in predicting material responses driven by unseen traction conditions.
4.3. The brittle fracture mechanics in glass-ceramics

Figure 7: Problem setup of pre-cracked glass-ceramics experiment with randomly distributed material property ﬁelds and the plate microstructure considered in example 3, following [90]. Here, dark grey represents the crystalline and light grey represents the glassy matrix. This microstructure represents a glass-ceramic sample where the crystals occupy 20% of the volume.

Glass Crystal

Young’s modulus E1 =80 GPa E2 =133 GPa

Poisson ratio 0.25 0.25

Fracture energy
G1 =6.59 J/m2 G2 =86.35 J/m2

Fracture Tou√ghness 0.75 MPa·√ m 3.5 MPa· m

Table 6: Material parameters used for generating the high-ﬁdelity solution in the pre-cracked glass-ceramics experiment, following [90].

In this example, we study the problem of brittle fracture in a glass-ceramic material, as a prototypical exemplar on the heterogeneous material damage ﬁeld prediction. A glass-ceramic material is the product of controlled crystallization of a specialized glass composition, which results in the creation of a microstructure composing of one of more crystalline phases within the residual amorphous glass [91–95]. In glass-ceramics,

28

the material has enhanced strength and toughness compared to pure glass, while the microstructure and phase assemblage of each material sample play a vital role in determining material strength and toughness.
We considered a pre-notched idealized microstructural realization which is subject to displacement boundary conditions on its top and bottom boundaries. As demonstrated in Figure 7, a plate of dimensions 800 µm by 400 µm was considered, with an initial crack of length 100 µm, and a gradually increasing uniform displacement loading UD applied on the top and bottom of the sample. All other boundaries, including the new boundaries created by cracks, were treated as free surfaces. This microstructure realization is composed of randomly distributed crystals embedded in a glassy matrix, such that the crystals occupy 20% of the volume. Similarly to [90, 91, 95], we generated the center location (Cx, Cy) and rotation angle Cη of each crystal as random variables, satisfying Cx ∼ U (0, 800), Cy ∼ U (0, 400), and Cη ∼ U (0, 2π). All crystals are identical ellipses with semi-major and semi-minor axes being 12 µm and 7.5 µm, respectively, with an aspect ratio of 1.6. The mechanical properties of glass and crystalline phases are summarized in Table 6. This material was studied experimentally in [90] and numerically in [91, 95] for diﬀerent crystallized volume fractions. Here, we adopted the setting in [95] and employed the quasi-static linear peridynamic solid (LPS) model to generate the high-ﬁdelity simulation data. In particular, for each microstructure realization, we used R(x) to denote the microstructure, such that


0 R(x) =
1

if the material point x is glass, if the material point x is crystal.

(4.1)

For this microstructure sample, the ﬁeld of Young’s modulus E(x) and fracture energy G(x) can be represented as linear transformations of R:

E(x) = R(x)(E2 − E1) + E1, G(x) = R(x)(G2 − G1) + G1,

where E1, E2 are the Young’s modulus of glass and crystal, respectively, and G1, G2 are their repsective fracture energy. The high-ﬁdelity material responses and crack propagation simulations in this sample are calculated using the LPS model proposed in [95]:

KRu(x, τ ) = 0, u(x, τ ) = uD(x, τ ),

x∈Ω x ∈ BBΩD

(4.2)

where BBΩD denotes the nonlocal boundary layer on the top and the bottom edges of the plate, the instant τ denotes the indexes for (incrementally increasing) loading. In particular, we set uD(x, τ ) = [0, UD(τ )] on the top edge, and uD(x, τ ) = [0, −UD(τ )] on the bottom edge. To perform quasi-static simulations of crack propagation, we gradually increase UD from 0.12 µm to 0.26 µm, and simulate the propagation of the crack starting from the pre-crack tip till it reaches the right boundary of the domain. At each quasi-static step, we increased UD by 0.002 µm, performed subiterations until no new broken bonds are detected, and then

29

Figure 8: The glass-ceramic crack propagation problem (example 3). Comparison of relative mean squared errors for quasistatic damage ﬁeld prediction on a ﬁxed microstructure ﬁeld and increasing boundary displacement loading.
proceeded to the next step. For spatial discretization, we employed uniform grids with grid size ∆x = 2 µm. Therefore, the whole computational domain Ω ∪ BBΩD has 87, 969 grid points in total. To generate the training and testing samples, we employed the meshfree method proposed in [95] to solve for the displacement ﬁeld u(x, τ ) and the damage ﬁeld d(x, τ ). For the detailed formulation of the LPS operator KR and the numerical method, we refer interested readers to [95].
Setting and results of the glass-ceramics fracture problem. In this context, our goal is to learn the solution operator of the quasi-static LPS equation, and compute the damage ﬁeld for a given plate. As shown in Figure 7, we considered a plate with a ﬁxed microstructure ﬁeld R(x), and the goal is to estimate the evolution of crack in the left half of this plate, by predicting the damage ﬁeld d(x, τ ) subject to increasing Dirichlet-type boundary loadings UD(τ ). That means, the neural operator were employed to learn the mapping from f (x) := [x, d(x, τ − ∆τ ), U˜D(τ )] to d(x, τ ), where d(x, τ − ∆τ ) stands for the damage ﬁeld corresponding to the last quasi-static loading step. With this setting, we aim to predict the crack propagation of one particular material sample under a new and unseen loading scenario. In particular, we generate 70 numbers of samples corresponding to UD(τ ) ∈ [0.12 µm, 0.26 µm] with an increment of ∆UD = 0.002 µm for each quasi-static step, such that the ﬁrst 55 steps/samples (corresponding to UD(τ ) ∈ [0.12 µm, 0.23 µm]) are employed for training, and the last 15 steps/samples (corresponding to UD(τ ) ∈ [0.232 µm, 0.26 µm]) are for testing. This setting reﬂects to the material defect monitoring scenario where some cracks are detected on a material sample with a unknown microstructure, and the learning goal is to predict and monitor the future crack growth. Therefore, this is an out-of-distribution test problem: the longer the prediction period (corresponding to larger UD) is, the harder the prediction task will be. For the purpose of training, we choose the loss function as the accumulated error of the damage ﬁeld d(x, τ ) within ﬁve successive quasistatic steps. Speciﬁcally, we used the neural operator to map [x, d(x, τ − ∆τ ), U˜D(τ )] to d(x, τ ), then used [x, d(x, τ ), U˜D(τ +∆τ )] as the input to obtain d(τ +∆τ ), and repeat till an approximated damage ﬁeld for the next ﬁve steps are obtained. Then, we train the network by minimizing the averaged error of d(x, τ + k∆τ ), k = 0, · · · , 4. A similar setting can be found, e.g., in [51]. In this example, a structured 200 × 200 grid is employed as the discretization in our neural operators. For both FNO and IFNO, we set the dimension of
30

Figure 9: The glass-ceramic crack propagation problem (example 3). A visualization of FNO and IFNO performances on the 1st, 5th and 15th prediction steps. Here, the best IFNO results (L = 8) and best FNO results (L = 4) are reported.
h as 48, and the truncated fourier modes k = 20 × 20. For each depth L, we trained the neural network for 500 epochs with a learning rate of 5e − 3, then the learning rate was decreased by 0.5 every 100 epochs.
In Figure 8 we report the averaged relative mean squared errors as a function of iterative layer number L. In particular, we show the averaged prediction errors for a relatively short term (over 5 prediction steps) and longer term (over 15 prediction steps), respectively. When considering the short term prediction error, we observe that as we increase L from 1 to 8, the prediction error from IFNO has a monotonic and drastic decrease from 20.1% to 6.8%, reaching a similar level as the averaged training error. Hence, in this example using deeper layer is necessary to obtain a suﬃciently expressive IFNO. In contrast, FNO reaches its best performance at L = 4, and obtains only 14.2% prediction error. For the longer term prediction error, a 12.8% averaged prediction error is obtained for IFNO at L = 8, while the error from the best FNO is only 25.7%. Similarly to previous examples, when increasing L the performance of FNOs starts to get polluted by the network instability issue caused by overﬁtting and vanishing gradient problems, which limits FNOs performance on deeper layers. In Figure 9, we show the predicted damage ﬁelds obtained with the best IFNO and the best FNO, in correspondence of the 1st, 5th and 15th prediction steps, which are the 56th, 60th and 70th steps among all samples. Both the solutions and the errors are plotted, showing that the FNO starts to mistakenly predict a subcrack since the 1st prediction step, and this crack grows over time, eventually leads to the large long-term prediction error in FNOs. From this example, we ﬁnd that developing a stable and deep NN is important, especially for a complex learning task like the heterogeneous material damage
31

Figure 10: Problem setup of the DIC data acquisition in the latex glove sample modeling problem. (a) An image of the speckle-patterned specimen subject to biaxial stretch loading. (b) A sample subject to Dirichlet-type boundary conditions, as the corresponding numerical setting of (a).
problem.
5. Application: Learning From Digital Image Correction (DIC) Measurements
Having illustrated the performances of our learned neural operators on high-ﬁdelity synthetic simulation datasets in Section 4, we now consider a problem of learning the material response of a latex glove sample from DIC displacement tracking measurements as a prototypical exemplar. The main objective of this section is to provide a proof-of-principle demonstration that the framework introduced thus far applies to learning tasks where the constitutive equations and material microstructure are both unknown, and the dataset has unavoidable measurement noise. Besides the FNOs, in this application we further compare our proposed IFNO against two conventional approaches that use constitutive modeling with parameter ﬁtting to demonstrate the advantages of neural operator models and the importance of considering the heterogeneity of material microstructures.
5.1. Digital Image Correction (DIC) and biaxial mechanical testing In this section, we ﬁrst introduce the experimental sample and data acquisition procedure. For material
sample acquisition, the central, palm region of a standard nitrile glove (Dealmed, NY, USA) was sectioned into a 7.5 mm ×7.5 mm specimen. Then, an optics-based laser thickness measurement device (Keyence, IL, USA) was used to measure the thickness of the specimen before application of a speckle pattern. Following the common procedures from previous research works [96–98], we used an airbrush to generate a random speckling texture on the surface of the specimen. Then, speckle-patterned specimens were mounted to a biaxial mechanical testing device (CellScale Biomaterials Testing Co., Canada) using ﬁve BioRake tines that pierced the specimen at each edge (Figure 10(a)). Biaxial characterizations of the specimen was conducted with 3 loading/unloading cycles, targeting an arbitrary force of 750 mN in each direction. Throughout the
32

test, the load cell force readings and actuator positions were recorded at a frequency of 5 Hz, which were subsequently used to calculate the stresses and the stretches for the constitutive model ﬁtting approach as one of the baselines. Meanwhile, a CCD camera captured images throughout the biaxial test at a frequency of 5 Hz. The recorded images were tracked using the digital image correlation (DIC) module of the CellScale LabJoy software. The central 6 mm ×5.5 mm region of the specimen was selected for the DIC tracking, as the speckling pattern was more random and less susceptible to tracking errors. A 20 × 20 node grid was constructed, and the tracked coordinates were exported.
Based on the tracked coordinates, we constructed two datasets: (i) an original dataset obtained directly from the experimental measurement, and (ii) a smoothed dataset where a moving least-squares (MLS) algorithm was used to calculate the smoothed nodal displacements. To generate the displacement ﬁeld uori(x) for original samples, we subtracted each material point location with its initial location on the ﬁrst sample, and the boundary displacement loading was obtained by restricting uori(x) on the boundary nodes. To create a structured grid for FNOs and IFNOs, we further applied a cubic spline interpolation to the displacement ﬁeld on a structured 21 × 21 node grid. Our goal was then to predict the displacement ﬁeld in the current loading step, given the displacement on the previous step and the current boundary displacement. To construct the smoothed samples for the jth material point, xj = (xj, yj), we employed a two-dimensional MLS shape function Ψj to reconstruct the smoothed displacement ﬁeld:

NP

NP

u(x, y) = Ψj(x, y)uj = φ(x − xj, y − yj; w)HT (0, 0)M−1(x, y)H(x − xj, y − yj)uj,

j=1

j=1

where uj = [uxj, uyj]T is the displacement vector of the jth point, φ(x, y; w) is the window function with a

support of w, H(x, y) = [1, x, y]T is the monomial basis function of linear order, M(x, y) :=

NP k=1

φk (x

−

xk, y − yk)H(x − xk, y − yk)HT (x − xk, y − yk) is the moment matrix, and N P is the set of discrete points

used to represent the region of interest [99, 100].

For this study, we chose N P = 9, a cubic B-spline function with a support of w = 5 for φ(x, y; w), and a

14 × 14 query point grid. The MLS shape functions were used to obtain the smoothed nodal displacements

usm. Both the smoothed and the original datasets have 877 total time instants (samples), denoted as

Dsm = {(uD)sjm, usjm}8j=771 and Dun = {(uD)ojri, uojri}8j=771, respectively. For training and cross-validation, we randomly select 177 samples from the each dataset as test samples, and use the rest as training samples.

On each dataset, these training samples were employed for parameter ﬁtting in the constitutive modeling

approaches, and used to train for the best neural operators for the IFNO and FNO. In that context, we

used common datasets for the constitutive modeling approaches and neural operator learning approaches,

to provide a fair comparison between the two diﬀerent approaches.

33

Figure 11: An illustration of the constitutive model ﬁtting approach which optimizes the generalized Mooney Rivlin (GMR) model parameters from the stress-stretch curve for a latex glove sample.
5.2. Constitutive modeling for comparisons with the IFNO In this section, we provide details for two constitutive modeling approaches, one uses constitutive model
ﬁtting to the stress-stretch data and the other uses ﬁnite element modeling of the DIC-tracked node displacements, for comparisons with the proposed IFNO method. For both approaches, a generalized Mooney-Rivlin (GMR) hyperelastic model was considered, with its strain energy density function given by:
η(I1, I2) = c10(I1 − 3) + c01(I2 − 3) + c20(I1 − 3)2 + c02(I2 − 3)2 + c11(I1 − 3)(I2 − 3).
Here, I1 = tr(C) and I2 = 12 [tr(C)2 − tr(C2)] represent the ﬁrst and second invariants of the right CauchyGreen deformation tensor C, and cij are the model-speciﬁc parameters. Based on this pre-assumed constitutive model, we aim to ﬁnd the optimal parameters of cij from the training samples, and these parameters will then be used for displacement ﬁeld predictions on the test samples.
In the ﬁrst modeling approach, constitutive model parameters were obtained by ﬁtting the ﬁnal unloading portion of the biaxial stress-stretch data. In particular, the ﬁrst Piola-Kirchhoﬀ stresses in the x- and ydirections were determined using the specimen thickness t, the undeformed edge lengths Lx and Ly, and the measured forces Fx and Fy as Pxx = Fx/tLy and Pyy = Fy/tLx. Meanwhile, the stretches in the two directions were calculated as the ratio of the deformed edge lengths to the undeformed length. Both stressstretch curves in the x- and y-directions are shown in Figure 11. To obtain the optimal parameters for the GMR model, we used a diﬀerential evolution optimization framework to minimize the residual errors in stress predictions between the experimental and model predicted data. Then, using the determined model parameters, ﬁnite element modeling was performed using the DIC-tracked nodes and the relative errors of displacement ﬁelds are evaluated by comparing the result from this ﬁnite element solver and the displacement measurements from DIC. In the following contents, we will refer to this approach as the “GMR model ﬁtting” method.
As the second modeling approach, we optimized the constitutive model parameters by minimizing the
34

displacement error from the ﬁnite element solver directly. In particular, the structured nodal locations were imported to Abaqus [101] to construct a 21 × 21 node domain composed of plane stress elements. Then, we solved for the displacement ﬁeld based on the GMR model using Abaqus, and calculated its relative error with respect to the experimentally-retrieved displacements of each node. The optimal model parameters were obtained by minimizing the total relative displacement error on all training samples. In the following contents, we refer to this approach as the “GMR inverse analysis” method. 5.3. Results and discussion
Figure 12: A latex glove sample modeling from DIC measurements. Error comparisons of each model. Upper plots: results from the original dataset. Bottom plots: results from the smoothed dataset. Left column: relative mean squared errors for quasi-static displacement ﬁeld prediction on the training and test datasets. Right column: sample-wise error comparison on all samples.
In this section, we introduce the settings of our neural operator learning models and report the comparison results. Because the time instance between two subsequent loading steps is relatively long, we employed a quasi-static model. In this context, we aim to predict the displacement ﬁeld u(x) based on a given boundary displacement loading uD(x) and the displacement ﬁeld from the last loading step (denoted as ulast(x)). Therefore, the neural operators were employed to learn the mapping from f (x) := [x, ulast(x), u˜D(x)] to u(x), where u˜D is the zero-padded boundary condition, as described in (2.2). In this example, for both the FNOs and IFNOs, we set the dimension of h as d = 16, and the number of truncated Fourier modes as k = 8 × 8. For each depth L, we train the neural network for 1,000 epochs with a learning rate of 1e − 3, then decrease the learning rate with a ratio of 0.7 every 100 epochs.
35

Figure 13: A latex glove sample modeling from DIC measurements. A visualization of GMR and IFNO performances on a test sample in the original dataset.
In Figure 12, we report the relative mean squared errors from both the original dataset (see Figure 12(a)) and the smoothed dataset (see Figure 12(c)), as functions of the number of hidden layers L from 3 to 24. The sample-wise error for each model are also provided in Figure 12(b) for the original dataset and in Figure 12(d) for the smoothed dataset. Unsurprisingly, when comparing the results from the original dataset and the smoothed dataset, one can observe that the smoothing procedure improves the prediction accuracy for all models. That is because the DIC measurements may contain noise-induced errors, and the nonlocal smoothing procedure we employed performs as an eﬀective ﬁlter [102] for the measurement noise. When comparing the prediction accuracy from diﬀerent models, similar to the previous examples, the FNO suﬀers from overﬁtting and vanishing gradient issues when L > 2, especially in the original (more noisy) dataset. This ﬁnding is consistent with the results reported in [68, 103], where the performance of the FNOs was found to be deteriorated on noisy datasets. In contrast, the accuracy of the IFNOs monotonically improves with the increase of L. Both neural operator models outperforms the conventional constitutive modeling approaches by around one order of magnitude. Among all the models, the deep IFNO (L = 24) performs
36

Figure 14: A latex glove sample modeling from DIC measurements. A visualization of GMR and IFNO performances on a test sample in the smoothed dataset.
the best in both datasets. On the original dataset which features noise, it achieves a 3.3% prediction error. On the smoothed dataset, the IFNO has an 1.18% prediction error. On the other hand, the GMR model ﬁtting and GMR inverse analysis approaches have obtained 33.0% and 29.1% prediction errors on the original dataset, respectively. On the smoothed dataset, the prediction error for these two GMR models are slightly smaller, as 30.5% and 27.3%, respectively. To provide further insights into this comparison, in Figures 13-14 we depict both solutions and prediction errors obtained with the best IFNO and the two GMR models on two test samples which correspond to the large deformation (t = 113.25 s) and small deformation (t = 16.18 s) representatives, respectively. From the ground-truth data pattern of uy(x), we can see that the glove sample is in fact heterogeneous, since a large deformation region is observed in the middle of the sample. Both GMR models fail to capture the material heterogeneity and hence obtained large prediction errors. This observation again conﬁrms the importance of capturing the material heterogeneity and veriﬁes the capability of IFNOs in heterogeneous material modeling.
37

6. Conclusion
With the objective of predicting material responses under unseen loading conditions, in this work we have proposed a novel data-driven computing paradigm for material modeling, which integrates material identiﬁcation, modeling procedures, and material responses prediction into one uniﬁed learning framework. In particular, a data-driven model has been developed, which learns the mapping from loading conditions to the corresponding material responses as a solution operator. To this end, a new integral neural operator has been proposed, which we refer to as the implicit Fourier neural operator (IFNO). In the IFNO, the increment between layers are modeled by integral operators, so the resultant architecture can be interpreted as a ﬁxed point method for the unknown governing laws. Furthermore, by identifying its layers with time instants, the IFNO can be reinterpreted as time-dependent equations, which enables the use of eﬃcient initialization techniques that enhances the network stability in the deep layer limit. Our results have shown that, in all learning tasks, the IFNOs outperform baseline methods in stability and prediction accuracy for unseen loading conditions. Both the universal approximation theorem and numerical results demonstrate that, in complex learning tasks, a stable deep layer architecture is necessary to achieve a satisfactory prediction accuracy. Last but not least, we have, for the ﬁrst time, leveraged the application of neural operators to learning the material responses directly from DIC displacement tracking measurements, where the constitutive equations and material microstructure are both unknown, and measurement noise is present. Numerical results have conﬁrmed the advantage of neural operator learning approaches against the conventional constitutive modeling approaches: the former does not require a pre-assumed material model, and is able to capture the material heterogeneity. Hence, the proposed neural operator models have outperformed the conventional generalized Mooney Rivlin (GMR) model in prediction accuracy by at least one order of magnitude. When comparing with another neural operator model, i.e., the FNOs, our proposed IFNOs have been shown to be less prone to the overﬁtting issue and hence achieve a better performance on noisy experimental datasets.
Although the IFNO requires a much smaller number of trainable parameters comparing with its counterpart, FNOs, we did not observe a decrease of the computational time because the ﬁxed point procedure of the IFNO comes with the price of using an iterative algorithm. Therefore, an important next step is to combine the IFNO with faster training techniques of implicit networks [73] to improve its eﬃciency. Moreover, we point out that the IFNO provides a general and ﬂexible solution operator for unknown governing laws, which is not restricted to material modeling tasks. As another natural extension, we will consider the application of the IFNO on other complex learning tasks, such as image classiﬁcation problems. The implicit neural operator architecture we proposed here can also be combined with other recent integral neural operator architectures, e.g., the multiwavelet-based operator [54] and the integral autoencoder-based network (IAE-Net) [53], which would be another interesting future direction.
38

Acknowledgements
The authors would like to thank Mr. Minglang Yin for sharing his FEniCS codes and for the helpful discussions. H. You and Y. Yu would like to acknowledge support by the National Science Foundation under award DMS 1753031. Portions of this research were conducted on Lehigh University’s Research Computing infrastructure partially supported by NSF Award 2019035. We also thank the Presbyterian Health Foundation Team Science Grant, and the National Science Foundation Graduate Research Fellowship Program (GRF2020307284).

Appendix A. Detailed Numeric Results

In this section we provide the detailed numerical results of each task in Sections 4-5, as the supplementary results of the training and test errors plotted in Figures 2, 6, 8 and 12 of the main text. The full results for porous medium pressure ﬁeld learning I, porous medium pressure ﬁeld learning II, ﬁber-reinforced material displacement ﬁeld learning, glass-ceramics damage ﬁeld learning, and DIC measurements of latex glove displacement ﬁled learning are provided in Tables A1, A2, A3, A4 and A5, respectively. To reduce the impact of initialization in neural operator models, for each task we run ﬁve simulations for each network using diﬀerent random seeds, and report the mean and the standard error among these ﬁve simulations. For each model, we use the bold case to highlight the architecture with the best prediction accuracy.

Model/dataset

L=1

L=2

L=4

L=8

L = 16

L = 32

IFNO train 1.67e-2±1.16e-4 7.79e-3±5.58e-5 6.48e-3±6.16e-5 5.84e-3±6.58e-5 5.46e-3±6.79e-5 5.21e-3±6.98e-5 test 1.77e-2±1.18e-4 1.23e-2±9.46e-5 1.10e-2±6.90e-5 1.05e-2±5.72e-5 1.04e-2±3.72e-5 1.02e-2±5.77e-5

FNO train 1.65e-2±4.94e-5 4.13e-3±3.16e-4 7.94e-3±1.65e-5 6.83e-4±5.09e-6 8.34e-4±1.55e-5 2.84e-1±2.57e-6 test 1.76e-2±9.40e-5 1.30e-2±5.11e-5 1.19e-2±8.98e-5 1.56e-2±1.85e-4 2.80e-2±1.19e-3 2.90e-1±1.07e-4

Table A1: Numerical results for the learning task of porous medium I. Bold numbers highlight the case with the best error for each model.

Model/dataset IFNO train
test FNO train
test

L=1 9.81e-3±9.90e-5 1.10e-2±1.16e-4 1.00e-2±9.52e-5 1.13e-2±1.05e-4

L=2 4.38e-3±9.30e-5 5.75e-3±1.17e-4 3.43e-3±8.54e-5 5.26e-3±8.21e-5

L=4 3.93e-3±7.19e-5 5.23e-3±1.05e-4 3.27e-3±8.53e-5 6.07e-3±1.68e-4

L=8 3.89e-3±7.60e-5 5.10e-3±1.16e-4 3.77e-3±2.65e-5 8.59e-3±5.14e-5

L = 16 3.90e-3±8.36e-5 5.07e-3±1.51e-4 3.91e-3±2.54e-5 1.26e-2±3.26e-4

L = 32 3.98e-3±9.57e-5 5.04e-3±1.61e-4 9.86e-1±2.20e-5 9.89e-1±2.03e-4

L = 64 3.93e-3±1.01e-4 4.89e-3±2.22e-4 9.86e-1±2.19e-5 9.89e-1±2.03e-4

Table A2: Numerical results for the learning task of porous medium II. Bold numbers highlight the case with the best error for each model.

References [1] T. Zohdi, D. Steigmann, The toughening eﬀect of microscopic ﬁlament misalignment on macroscopic ballistic fabric response, International journal of fracture 118 (4) (2002) 71–76. [2] P. Wriggers, G. Zavarise, T. Zohdi, A computational study of interfacial debonding damage in ﬁbrous composite materials, Computational Materials Science 12 (1) (1998) 39–56.
39

Model/dataset IFNO train
test FNO train
test
Model/dataset IFNO train
test FNO train
test
Model/dataset IFNO train
test FNO train
test

Dirichlet boundary condition with in-distribution test

L=1

L=2

L=4

L=8

L = 16

7.96e-3±6.60e-5 3.82e-3±6.10e-5 3.07e-3±1.36e-4 2.68e-3±6.40e-5 2.52e-3±4.91e-5

7.70e-3±1.66e-4 4.10e-3±6.17e-4 4.05e-3±4.25e-4 4.86e-3±7.12e-4 2.96e-3±2.78e-4

7.81e-3±8.77e-5 3.65e-3±8.59e-5 3.34e-3±4.58e-5 4.98e-3±7.51e-4 4.93e-1±2.60e-1

7.46e-3±3.79e-4 5.78e-3±9.90e-4 3.50e-3±3.26e-4 5.24e-3±4.70e-4 4.78e-1±2.56e-1

Dirichlet boundary condition with out-of-distribution test

L=1

L=2

L=4

L=8

L = 16

7.58e-3±8.20e-5 3.74e-3±6.09e-5 2.92e-3±6.52e-5 2.69e-3±7.10e-5 2.44e-3±3.88e-5

6.78e-3±7.18e-5 3.42e-3±3.44e-4 2.95e-3±1.60e-4 2.51e-3±1.74e-4 2.34e-3±1.23e-4

7.61e-3±2.07e-4 3.63e-3±1.13e-4 3.68e-3±4.46e-4 3.89e-3±1.77e-4 2.78e-1±2.36e-1

6.56e-3±2.62e-4 3.25e-3±4.33e-4 2.43e-3±2.09e-4 3.18e-3±1.98e-4 2.33e-1±1.83e-1

Neumann boundary condition

L=1

L=2

L=4

L=8

L = 16

4.33e-2±1.26e-4 1.42e-2±2.40e-5 1.08e-2±8.91e-5 9.32e-3±7.34e-5 9.03e-3±4.48e-5

5.45e-2±6.35e-4 2.15e-2±1.44e-4 1.75e-2±8.42e-5 1.54e-2±8.60e-5 1.44e-2±1.30e-4

4.47e-2±5.61e-4 1.37e-2±4.88e-5 8.18e-3±7.51e-5 7.96e-3±1.36e-4 2.21e-1±1.78e-1

5.19e-2±8.17e-4 1.90e-2±6.55e-4 1.40e-2±5.80e-3 1.42e-2±3.86e-4 2.26e-1±1.74e-1

L = 32 2.40e-3±2.42e-5 2.12e-3±4.42e-5 1.23e0±2.73e-3 1.24e0±3.67e-2
L = 32 2.14e-3±2.33e-5 2.06e-3±5.92e-5 8.34e-1±2.44e-1 7.03e-1±1.85e-1
L = 32 9.04e-3±7.01e-5 1.41e-2±3.70e-5 5.81e-1±1.87e-1 6.01e-1±1.76e-1

Table A3: Numerical results for the learning task of ﬁber-reinforced material displacement ﬁeld. Bold numbers highlight the case with the best error for each model.

Model/dataset train
IFNO 5-step test 15-step test train
FNO 5-step test 15-step test

L=1 8.01e-2±9.96e-3 2.01e-1±1.58e-2 3.10e-1±1.54e-2 6.85e-2±1.85e-3 2.06e-1±1.64e-2 3.12e-1±1.69e-2

L=2 8.48e-2±1.96e-2 1.60e-1±3.25e-2 2.51e-1±3.28e-2 5.52e-2±9.74e-4 1.91e-1±3.41e-2 2.84e-1±2.72e-2

L=4 4.87e-2±2.05e-3 7.65e-2±8.46e-3 1.58e-1±1.42e-2 4.16e-2±3.33e-4 1.42e-1±1.34e-2 2.57e-1±1.85e-2

L=8 4.48e-2±1.62e-3 6.86e-2±7.08e-3 1.28e-1±2.04e-2 1.28e-1±4.11e-3 2.36e-1±3.68e-4 3.30e-1±1.74e-3

Table A4: Numerical results for the learning task of glass-ceramics damage ﬁeld. Bold numbers highlight the case with the best error for each model.

Model/dataset IFNO, original FNO, original GMR model ﬁtting, original GMR inverse analysis, original IFNO, smoothed FNO, smoothed GMR model ﬁtting, smoothed GMR inverse analysis, smoothed

train test train test train test train test train test train test train test train test

L=3 3.26e-2±1.08e-4 3.43e-2±4.96e-4 2.88e-2±1.23e-4 3.40e-2±4.09e-4
1.33e-2±1.61e-4 1.43e-2±2.99e-4 1.14e-2±3.28e-5 1.25e-2±2.25e-4

L=6

L = 12

3.13e-2±1.30e-4 3.06e-2±1.08e-4

3.34e-2±4.53e-4 3.32e-2±4.41e-4

2.25e-2±8.68e-5 1.66e-2±9.94e-4

3.84e-2±4.21e-4 4.66e-2±1.47e-3

3.16e-1

3.30e-1

2.66e-1

2.91e-1

1.16e-2±8.10e-5 1.09e-2±4.91e-5

1.26e-2±2.20e-4 1.21e-2±2.28e-4

1.01e-2±9.28e-5 9.83e-3±3.02e-4

1.23e-2±1.98e-4 1.49e-2±1.15e-4

2.87e-1

3.05e-1

2.52e-1

2.73e-1

L = 24 3.00e-2±1.24e-4 3.30e-2±4.63e-4 8.47e-1±4.72e-3 8.61e-1±2.70e-2
1.05e-2±6.01e-5 1.18e-2±2.21e-4 8.49e-1±3.50e-3 8.73e-1±1.87e-2

Table A5: Numerical results for the learning task of DIC measurements of latex glove displacement ﬁled, compared with the generalized Mooney-Rivlin (GMR) model. Bold numbers highlight the case with the best error for each model.

[3] Y. Kok, X. P. Tan, P. Wang, M. Nai, N. H. Loh, E. Liu, S. B. Tor, Anisotropy and heterogeneity of microstructure and mechanical properties in metal additive manufacturing: A critical review, Materials

40

& Design 139 (2018) 565–586.
[4] R. Bostanabad, Y. Zhang, X. Li, T. Kearney, L. C. Brinson, D. W. Apley, W. K. Liu, W. Chen, Computational microstructure characterization and reconstruction: Review of the state-of-the-art techniques, Progress in Materials Science 95 (2018) 1–41.
[5] Z. Su, L. Ye, Y. Lu, Guided lamb waves for identiﬁcation of damage in composite structures: A review, Journal of sound and vibration 295 (3-5) (2006) 753–780.
[6] 2014 technical strategic plan, Tech. rep., the Air Force Oﬃce of Scientiﬁc Research (2014).
[7] R. Talreja, J. Varna, Modeling damage, fatigue and failure of composite materials, Elsevier, 2015.
[8] J. Sori´c, P. Wriggers, O. Allix, Multiscale modeling of heterogeneous structures, Springer, 2018.
[9] G. Pijaudier-Cabot, F. Dufour, Damage mechanics of cementitious materials and structures, John Wiley & Sons, 2013.
[10] C. Mourlas, G. Markou, M. Papadrakakis, Accurate and computationally eﬃcient nonlinear static and dynamic analysis of reinforced concrete structures considering damage factors, Engineering Structures 178 (2019) 258–285.
[11] G. Markou, R. Garcia, C. Mourlas, M. Guadagnini, K. Pilakoutas, M. Papadrakakis, A new damage factor for seismic assessment of deﬁcient bare and frp-retroﬁtted rc structures, Engineering Structures 248 (2021) 113152.
[12] E. A. Lindgren, US Air Force perspective on validated NDE–past, present, and future, in: AIP Conference Proceedings, Vol. 1706, AIP Publishing LLC, 2016, p. 020002.
[13] E. Lindgren, J. Brausch, C. Buynak, P. Kobryn, M. Leonard, The state of nondestructive evaluation and structural health monitoring, in: Aircraft Structural Integrity Program Conference, 2013.
[14] M. HDBK, Nondestructive evaluation system reliability assessment, Department of Defense Handbook 7.
[15] J. D. Achenbach, Quantitative nondestructive evaluation, International Journal of Solids and Structures 37 (1-2) (2000) 13–27.
[16] K. Jones, J. Brausch, W. Fong, B. Harris, Probing the future: Better f-16 inspections using conformal eddy current inspection tools, in: Proceedings of 2015 Aircraft Airworthiness & Sustainment Conference, Baltimore, Maryland, 2015.
[17] B. Pan, L. Yu, Q. Zhang, Review of single-camera stereo-digital image correlation techniques for fullﬁeld 3d shape and deformation measurement, Science China Technological Sciences 61 (1) (2018) 2–20.
41

[18] K. Shukla, P. C. Di Leoni, J. Blackshire, D. Sparkman, G. E. Karniadakis, Physics-informed neural network for ultrasound nondestructive quantiﬁcation of surface breaking cracks, arXiv preprint arXiv:2005.03596.
[19] M. Misfeld, H.-H. Sievers, Heart valve macro-and microstructure, Philosophical Transactions of the Royal Society B: Biological Sciences 362 (1484) (2007) 1421–1436.
[20] J. Rieppo, J. Hallikainen, J. S. Jurvelin, I. Kiviranta, H. J. Helminen, M. M. Hyttinen, Practical considerations in the use of polarized light microscopy in the analysis of the collagen network in articular cartilage, Microscopy research and technique 71 (4) (2008) 279–287.
[21] J. Ghaboussi, D. A. Pecknold, M. Zhang, R. M. Haj-Ali, Autoprogressive training of neural network constitutive models, International Journal for Numerical Methods in Engineering 42 (1) (1998) 105– 126.
[22] J. Ghaboussi, J. Garrett Jr, X. Wu, Knowledge-based modeling of material behavior with neural networks, Journal of engineering mechanics 117 (1) (1991) 132–153.
[23] G. Carleo, I. Cirac, K. Cranmer, L. Daudet, M. Schuld, N. Tishby, L. Vogt-Maranto, L. Zdeborov´a, Machine learning and the physical sciences, Reviews of Modern Physics 91 (4) (2019) 045002.
[24] G. E. Karniadakis, I. G. Kevrekidis, L. Lu, P. Perdikaris, S. Wang, L. Yang, Physics-informed machine learning, Nature Reviews Physics 3 (6) (2021) 422–440.
[25] L. Zhang, J. Han, H. Wang, R. Car, E. Weinan, Deep potential molecular dynamics: a scalable model with the accuracy of quantum mechanics, Physical Review Letters 120 (14) (2018) 143001.
[26] S. Cai, Z. Mao, Z. Wang, M. Yin, G. E. Karniadakis, Physics-informed neural networks (PINNs) for ﬂuid mechanics: A review, Acta Mechanica Sinica (2022) 1–12.
[27] D. Pfau, J. S. Spencer, A. G. Matthews, W. M. C. Foulkes, Ab initio solution of the many-electron schr¨odinger equation with deep neural networks, Physical Review Research 2 (3) (2020) 033429.
[28] Q. He, D. W. Laurence, C.-H. Lee, J.-S. Chen, Manifold learning based data-driven modeling for soft biological tissues, Journal of Biomechanics 117 (2021) 110124.
[29] G. Besnard, F. Hild, S. Roux, “ﬁnite-element” displacement ﬁelds analysis from digital images: application to portevin–le chˆatelier bands, Experimental mechanics 46 (6) (2006) 789–803.
[30] R. Iban˜ez, D. Borzacchiello, J. V. Aguado, E. Abisset-Chavanne, E. Cueto, P. Ladeveze, F. Chinesta, Data-driven non-linear elasticity: constitutive manifold construction and problem discretization, Computational Mechanics 60 (5) (2017) 813–826.
42

[31] R. Ibanez, E. Abisset-Chavanne, J. V. Aguado, D. Gonzalez, E. Cueto, F. Chinesta, A manifold learning approach to data-driven computational elasticity and inelasticity, Archives of Computational Methods in Engineering 25 (1) (2018) 47–57.
[32] L. Stainier, A. Leygue, M. Ortiz, Model-free data-driven methods in mechanics: material data identiﬁcation and solvers, Computational Mechanics 64 (2) (2019) 381–393.
[33] T. Kirchdoerfer, M. Ortiz, Data-driven computational mechanics, Computer Methods in Applied Mechanics and Engineering 304 (2016) 81–101.
[34] Y. Heider, K. Wang, W. Sun, So (3)-invariance of informed-graph-based deep neural network for anisotropic elastoplastic materials, Computer Methods in Applied Mechanics and Engineering 363 (2020) 112875.
[35] J. N. Fuhg, N. Bouklas, On physics-informed data-driven isotropic and anisotropic constitutive models through probabilistic machine learning and space-ﬁlling sampling, arXiv preprint arXiv:2109.11028.
[36] K. Wang, W. Sun, A multiscale multi-permeability poroplasticity model linked by recursive homogenizations and deep learning, Computer Methods in Applied Mechanics and Engineering 334 (2018) 337–380.
[37] Q. He, D. Barajas-Solano, G. Tartakovsky, A. M. Tartakovsky, Physics-informed neural networks for multiphysics data assimilation with application to subsurface transport, Advances in Water Resources 141 (2020) 103610.
[38] A. M. Tartakovsky, C. O. Marrero, P. Perdikaris, G. D. Tartakovsky, D. Barajas-Solano, Physicsinformed deep neural networks for learning parameters and constitutive relationships in subsurface ﬂow problems, Water Resources Research 56 (5) (2020) e2019WR026731.
[39] Z. Liu, C. Wu, M. Koishi, A deep material network for multiscale topology learning and accelerated nonlinear modeling of heterogeneous materials, Computer Methods in Applied Mechanics and Engineering 345 (2019) 1138–1168.
[40] H. Yang, X. Guo, S. Tang, W. K. Liu, Derivation of heterogeneous material laws via data-driven principal component expansions, Computational Mechanics 64 (2) (2019) 365–379.
[41] K. Garbrecht, M. Aguilo, A. Sanderson, A. Rollett, R. M. Kirby, J. Hochhalter, Interpretable machine learning for texture-dependent constitutive models with automatic code generation for topological optimization, Integrating Materials and Manufacturing Innovation 10 (3) (2021) 373–392.
[42] M. Raissi, P. Perdikaris, G. E. Karniadakis, Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial diﬀerential equations, Journal of Computational Physics 378 (2019) 686–707.
43

[43] J. Bongard, H. Lipson, Automated reverse engineering of nonlinear dynamical systems, Proceedings of the National Academy of Sciences 104 (24) (2007) 9943–9948.
[44] M. Schmidt, H. Lipson, Distilling free-form natural laws from experimental data, science 324 (5923) (2009) 81–85.
[45] S.-M. Udrescu, M. Tegmark, Ai feynman: A physics-inspired method for symbolic regression, Science Advances 6 (16) (2020) eaay2631.
[46] G. Bomarito, T. Townsend, K. Stewart, K. Esham, J. Emery, J. Hochhalter, Development of interpretable, data-driven plasticity models with symbolic regression, Computers & Structures 252 (2021) 106557.
[47] L. Lu, P. Jin, G. E. Karniadakis, Deeponet: Learning nonlinear operators for identifying diﬀerential equations based on the universal approximation theorem of operators, arXiv preprint arXiv:1910.03193.
[48] L. Lu, P. Jin, G. Pang, Z. Zhang, G. E. Karniadakis, Learning nonlinear operators via deeponet based on the universal approximation theorem of operators, Nature Machine Intelligence 3 (3) (2021) 218–229.
[49] Z. Li, N. Kovachki, K. Azizzadenesheli, B. Liu, K. Bhattacharya, A. Stuart, A. Anandkumar, Neural operator: Graph kernel network for partial diﬀerential equations, arXiv preprint arXiv:2003.03485.
[50] Z. Li, N. Kovachki, K. Azizzadenesheli, B. Liu, A. Stuart, K. Bhattacharya, A. Anandkumar, Multipole graph neural operator for parametric partial diﬀerential equations, Advances in Neural Information Processing Systems 33.
[51] Z. Li, N. B. Kovachki, K. Azizzadenesheli, K. Bhattacharya, A. Stuart, A. Anandkumar, et al., Fourier neural operator for parametric partial diﬀerential equations, in: International Conference on Learning Representations, 2020.
[52] H. You, Y. Yu, M. D’Elia, T. Gao, S. Silling, Nonlocal kernel network (NKN): a stable and resolutionindependent deep neural network, arXiv preprint arXiv:2201.02217.
[53] Y. Z. Ong, Z. Shen, H. Yang, IAE-NET: Integral autoencoders for discretization-invariant learningdoi: 10.13140/RG.2.2.25120.87047/2.
[54] G. Gupta, X. Xiao, P. Bogdan, Multiwavelet-based operator learning for diﬀerential equations, in: A. Beygelzimer, Y. Dauphin, P. Liang, J. W. Vaughan (Eds.), Advances in Neural Information Processing Systems, 2021. URL https://openreview.net/forum?id=LZDiWaC9CGL
44

[55] S. Goswami, M. Yin, Y. Yu, G. E. Karniadakis, A physics-informed variational deeponet for predicting crack path in quasi-brittle materials, Computer Methods in Applied Mechanics and Engineering 391 (2022) 114587.
[56] X. Guo, W. Li, F. Iorio, Convolutional neural networks for steady ﬂow approximation, in: Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining, 2016, pp. 481–490.
[57] Y. Zhu, N. Zabaras, Bayesian deep convolutional encoder–decoder networks for surrogate modeling and uncertainty quantiﬁcation, Journal of Computational Physics 366 (2018) 415–447.
[58] J. Adler, O. O¨ ktem, Solving ill-posed inverse problems using iterative deep neural networks, Inverse Problems 33 (12) (2017) 124007.
[59] S. Bhatnagar, Y. Afshar, S. Pan, K. Duraisamy, S. Kaushik, Prediction of aerodynamic ﬂow ﬁelds using convolutional neural networks, Computational Mechanics 64 (2) (2019) 525–545.
[60] Y. Khoo, J. Lu, L. Ying, Solving parametric pde problems with artiﬁcial neural networks, European Journal of Applied Mathematics 32 (3) (2021) 421–435.
[61] J. C. De los Reyes, Numerical PDE-constrained optimization, Springer, 2015.
[62] E. Weinan, B. Yu, The deep ritz method: A deep learning-based numerical algorithm for solving variational problems, Communications in Mathematics and Statistics 6 (1).
[63] L. Bar, N. Sochen, Unsupervised deep learning algorithm for pde-based forward and inverse problems, arXiv preprint arXiv:1904.05417.
[64] J. D. Smith, K. Azizzadenesheli, Z. E. Ross, Eikonet: Solving the eikonal equation with deep neural networks, IEEE Transactions on Geoscience and Remote Sensing.
[65] S. Pan, K. Duraisamy, Physics-informed probabilistic learning of linear embeddings of nonlinear dynamics with guaranteed stability, SIAM Journal on Applied Dynamical Systems 19 (1) (2020) 480–509.
[66] M. Yin, E. Ban, B. V. Rego, E. Zhang, C. Cavinato, J. D. Humphrey, G. Em Karniadakis, Simulating progressive intramural damage leading to aortic dissection using deeponet: an operator–regression neural network, Journal of the Royal Society Interface 19 (187) (2022) 20210670.
[67] M. Yin, E. Zhang, Y. Yu, G. E. Karniadakis, Interfacing ﬁnite elements with deep neural operators for fast multiscale modeling of mechanics problems (2022). arXiv:2203.00003.
[68] L. Lu, X. Meng, S. Cai, Z. Mao, S. Goswami, Z. Zhang, G. E. Karniadakis, A comprehensive and fair comparison of two neural operators (with practical extensions) based on fair data, arXiv preprint arXiv:2111.05512.
45

[69] L. El Ghaoui, F. Gu, B. Travacca, A. Askari, A. Tsai, Implicit deep learning, SIAM Journal on Mathematics of Data Science 3 (3) (2021) 930–958.
[70] S. Bai, J. Z. Kolter, V. Koltun, Deep equilibrium models, in: Proceedings of the 33rd International Conference on Neural Information Processing Systems, 2019, pp. 690–701.
[71] E. Winston, J. Z. Kolter, Monotone operator equilibrium networks, Advances in Neural Information Processing Systems 33 (2020) 10718–10728.
[72] S. Bai, V. Koltun, J. Z. Kolter, Multiscale deep equilibrium models, Advances in Neural Information Processing Systems 33.
[73] S. W. Fung, H. Heaton, Q. Li, D. McKenzie, S. Osher, W. Yin, Jfb: Jacobian-free backpropagation for implicit networks, arXiv preprint arXiv:2103.12803.
[74] E. Haber, L. Ruthotto, E. Holtham, S.-H. Jun, Learning across scales—multiscale methods for convolution neural networks, in: Proceedings of the AAAI Conference on Artiﬁcial Intelligence, Vol. 32, 2018.
[75] J. Modersitzki, FAIR: ﬂexible algorithms for image registration, SIAM, 2009.
[76] R. J. LeVeque, Finite diﬀerence methods for ordinary and partial diﬀerential equations: steady-state and time-dependent problems, SIAM, 2007.
[77] O. C. Zienkiewicz, R. L. Taylor, P. Nithiarasu, J. Zhu, The ﬁnite element method, Vol. 3, McGraw-hill London, 1977.
[78] G. Karniadakis, S. Sherwin, Spectral/hp element methods for computational ﬂuid dynamics, OUP Oxford, 2005.
[79] M. Kim, N. Winovich, G. Lin, W. Jeong, Peri-net: Analysis of crack patterns using deep neural networks, Journal of Peridynamics and Nonlocal Modeling 1 (2) (2019) 131–142.
[80] H. You, Y. Yu, S. Silling, M. D’Elia, A data-driven peridynamic continuum model for upscaling molecular dynamics, Computer Methods in Applied Mechanics and Engineering 389 (2022) 114400.
[81] N. Kovachki, Z. Li, B. Liu, K. Azizzadenesheli, K. Bhattacharya, A. Stuart, A. Anandkumar, Neural operator: Learning maps between function spaces, arXiv preprint arXiv:2108.08481.
[82] K. He, X. Zhang, S. Ren, J. Sun, Deep residual learning for image recognition, IEEE Conference on Computer Vision and Pattern Recognition.
[83] L. Ruthotto, E. Haber, Deep neural networks motivated by partial diﬀerential equations, Journal of Mathematical Imaging and Vision (2019) 1–13.
46

[84] N. Kovachki, S. Lanthaler, S. Mishra, On universal approximation and error bounds for fourier neural operators, Journal of Machine Learning Research 22 (2021) Art–No.
[85] A. Pinkus, Approximation theory of the mlp model in neural networks, Acta numerica 8 (1999) 143– 195.
[86] S. Hochreiter, The vanishing gradient problem during learning recurrent neural nets and problem solutions, International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems 6 (02) (1998) 107–116.
[87] M. Alnæs, J. Blechta, J. Hake, A. Johansson, B. Kehlet, A. Logg, C. Richardson, J. Ring, M. E. Rognes, G. N. Wells, The fenics project version 1.5, Archive of Numerical Software 3 (100).
[88] G. A. Holzapfel, T. C. Gasser, R. W. Ogden, A new constitutive framework for arterial wall mechanics and a comparative study of material models, Journal of Elasticity and the Physical Science of Solids 61 (1) (2000) 1–48.
[89] A. Lang, J. Potthoﬀ, Fast simulation of gaussian random ﬁelds, Monte Carlo Methods and Applications 17 (3) (2011) 195–214. doi:doi:10.1515/mcma.2011.009. URL https://doi.org/10.1515/mcma.2011.009
[90] F. Serbena, I. Mathias, C. Foerster, E. Zanotto, Crystallization toughening of a model glass-ceramic, Acta Materialia 86 (2015) 216–228.
[91] N. Prakash, B. Deng, R. J. Stewart, C. M. Smith, J. T. Harris, Investigation of microscale fracture mechanisms in glass-ceramics using peridynamics simulations, Journal of American Ceramic Society.
[92] F. C. Serbena, E. D. Zanotto, Internal residual stresses in glass-ceramics: A review, Journal of NonCrystalline Solids 358 (6-7) (2012) 975–984.
[93] W. Holand, G. H. Beall, Glass-ceramic technology, John Wiley & Sons, 2019.
[94] Q. Fu, G. H. Beall, C. M. Smith, Nature-inspired design of strong, tough glass-ceramics, MRS Bulletin 42 (3) (2017) 220–225.
[95] Y. Fan, H. You, X. Tian, X. Yang, X. Li, N. Prakash, Y. Yu, A meshfree peridynamic model for brittle fracture in randomly heterogeneous materials, arXiv preprint arXiv:2202.06578.
[96] D. S. Zhang, D. D. Arola, Applications of digital image correlation to biological tissues, Journal of Biomedical Optics 9 (4) (2004) 691–699.
[97] G. Lionello, L. Cristofolini, A practical approach to optimizing the preparation of speckle patterns for digital-image correlation, Measurement Science and Technology 25 (10) (2014) 107001.
47

[98] M. Palanca, G. Tozzi, L. Cristofolini, The use of digital image correlation in the biomechanical area: a review, International Biomechanics 3 (1) (2016) 1–21.
[99] T. Belytschko, Y. Krongauz, D. Organ, M. Fleming, P. Krysl, Meshless methods: An overview and recent developments, Computer Methods in Applied Mechanics and Engineering 139 (1-4) (1996) 3–47.
[100] J.-S. Chen, C. Pan, C.-T. Wu, W. K. Liu, Reproducing kernel particle methods for large deformation analysis of non-linear structures, Computer Methods in Applied Mechanics and Engineering 139 (1-4) (1996) 195–227.
[101] G. Abaqus, Abaqus 6.11, Dassault Systemes Simulia Corporation, Providence, RI, USA. [102] R. B. Lehoucq, P. L. Reu, D. Z. Turner, A novel class of strain measures for digital image correlation,
Strain 51 (4) (2015) 265–275. [103] G. Kissas, J. Seidman, L. F. Guilhoto, V. M. Preciado, G. J. Pappas, P. Perdikaris, Learning operators
with coupled attention, arXiv preprint arXiv:2201.01032.
48

