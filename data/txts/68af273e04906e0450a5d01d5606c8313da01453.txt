Optimal Sensing and Data Estimation in a Large Sensor Network
Arpan Chattopadhyay, Urbashi Mitra

arXiv:1707.08074v2 [cs.IT] 11 Sep 2017

Abstract—An energy efﬁcient use of large scale sensor networks necessitates activating a subset of possible sensors for estimation at a fusion center. The problem is inherently combinatorial; to this end, a set of iterative, randomized algorithms are developed for sensor subset selection by exploiting the underlying statistics. Gibbs sampling-based methods are designed to optimize the estimation error and the mean number of activated sensors. The optimality of the proposed strategy is proven, along with guarantees on their convergence speeds. Also, another new algorithm exploiting stochastic approximation in conjunction with Gibbs sampling is derived for a constrained version of the sensor selection problem. The methodology is extended to the scenario where the fusion center has access to only a parametric form of the joint statistics, but not the true underlying distribution. Therein, expectation-maximization is effectively employed to learn the distribution. Strategies for iid time-varying data are also outlined. Numerical results show that the proposed methods converge very fast to the respective optimal solutions, and therefore can be employed for optimal sensor subset selection in practical sensor networks.
Index Terms—Wireless sensor networks, active sensing, data estimation, Gibbs sampling, stochastic approximation, expectation maximization.
I. INTRODUCTION
A wireless sensor network typically consists of a number of sensor nodes deployed to monitor some physical process. The sensor data is often delivered to a fusion center via wireless links. The fusion center, based on the gathered data from the sensors, infers the state of the physical process and makes control decisions if necessary.
Sensor networks have widespread applications in various domains such as environmental monitoring, industrial process monitoring and control, localization, tracking of mobile objects, system parameter estimation, and even in disaster management. However, severe resource constraints in such networks necessitates careful design and control strategies in order to attain a reasonable compromise between resource usage and network performance. One major restriction is that the nodes are battery constrained, which limits the lifetime of the network. Also, low capacity of wireless channels due to transmit power constraint, heavy interference and unreliable link behaviour restricts the amount of data that can be sent to the fusion center per unit time. In some special cases, such
Arpan Chattopadhyay and Urbashi Mitra are with Ming Hsieh Department of Electrical Engineering, University of Southern California, Los Angeles, USA. Email: {achattop,ubli}@usc.edu
This work was funded by the following grants: ONR N00014-15-1-2550, NSF CNS-1213128, NSF CCF-1410009, AFOSR FA9550-12-1-0215, and NSF CPS-1446901, and also by the UK Leverhulme Trust, the UK Royal Society of Engineers and the Fulbright Foundation.

as mobile crowdsensing applications (see [1]), a certain cost might be necessary in order to engage a sensor owned by a third party. All these constraints lead us to the fundamental question: how to select a small subset of sensors so that the observations made by these sensors are most informative for the efﬁicient inference of the state of the physical process under measurement?
Recent results have focused on optimal sequential sensor subset selection in order to monitor a random process modeled as Markov chain or linear dynamical system; see e.g. [2]–[7]. Sensor subset selection using these control-theoretic resullts are typically computationally expensive, and the lowcomplexity approximation schemes proposed in some of these papers (such as [3] and [7]) are not optimal. On the other hand, there appears to be limited work on optimal subset selection when sensor data is static and its distribution is known either absolutely or in parametric form; the major challenge in this problem is computational ( [8]), where the computational burden arises for two reasons: (i) ﬁnding the optimal subset requires a search operation over all possible subsets of sensors, thereby requiring exponentially many number of computations, and (ii) for each subset of active sensors, computing the estimation error conditioned on the observation made by active sensors requires exponentially many computations. In [8], the problem of minimizing the minimum mean squared error (MMSE) of a vector signal using samples collected from a given number of sensors chosen by the network operator is considered; a tractable lower bound to the MMSE is employed in a certain greedy algorithm to obviate the complexity in MMSE computation and the combinatorial problem of searching over all possible subsets of sensors. In contrast, our paper deals with a general error metric (which could potentially be the MMSE or even the lower bound to MMSE as in [8]), and proposes Gibbs sampling based techniques for the optimal subset selection problem, in order to minimize a linear combination of the estimation error and the expected number of activated sensors. To the best of our knowledge, ours is the ﬁrst paper to use Gibbs sampling for optimal sensor subset selection with low complexity in the context of active sensing. We also provide an algorithm based on Gibbs sampling and stochastic approximation, which is provably optimal and which minimizes the expected estimation error subject to a constraint on the expected number of activated sensors; this technique can be employed to solve

many other constrained combinatorial optimization problems.1
A. Organization and our contribution
The paper is organized as follows. The system model is described in Section II. In Section III, we propose Gibbs sampling based algorithms to minimize a linear combination of data estimation error and the number of active sensors. We prove convergence of these algorithms, and also provide a bound on the convergence speed of one algorithm. Section IV provides algorithm for minimizing the estimation error subject to a constraint on the mean number of active sensors. We propose a novel algorithm based on Gibbs sampling and stochastic approximation, and prove its convergence to the desired solution. To the best of our knowledge, this is a novel technique that can be used for other constrained combinatorial optimization problems as well. We also discuss how the Gibbs sampling algorithm can be used when we have a hard constraint on the number of activated sensors. Section V discusses expectation maximization (EM) based algorithms when data comes from a parameterized distribution with unknown parameters. Numerical results on computational gain and performance improvement by using some of the proposed algorithms are presented in Section VI. Finally, we conclude in Section VII. All proofs are provided in the appendices.
We have also discussed in various sections how the proposed algorithms with minor modiﬁcations can be used for data varying in time in an i.i.d. fashion.
II. SYSTEM MODEL AND NOTATION
A. The network and data model
We consider a large connected single or multi-hop sensor network, whose sensor nodes are denoted by the set N = {1, 2, · · · , N }. Each node k ∈ N is associated with a (possibly vector-valued) data Xk, and we denote by X = {Xk}k∈N the set of data which has to be reconstructed. A fusion center determines the set of activated sensors, and estimates the data in each node given the limited observations only from the activated sensors.
While our methods assume static data from the sensors; these methods can be employed with good performance for data that varies in an iid fashion with respect to time.
B. Reconstruction of sensor data
We denote the activation state of a sensor by 1 if it is active, and by 0 otherwise. We call B := {0, 1}N the set of all possible conﬁgurations in the network, and denote a generic conﬁguration by B. Specifying a conﬁguration is equivalent to selecting a subset S of active sensors. We denote by B−j ∈ {0, 1}N−1 the conﬁguration B with its j-th entry removed.
1In this connection, we would like to mention that Gibbs sampling based algorithms were used in wireless caching [9], but to solve an unconstrained problem. In the current paper, we combine Gibbs sampling and stochastic approximation to solve a constrained optimization problem; this technique is general and can iteratively solve many other constrained combinatorial optimization problems optimally with very small computation per iteration, while the approximation algorithms are not guaranteed to achieve optimality.

The estimate of X at the fusion center is denoted by Xˆ . The

corresponding expected error under conﬁguration B ∈ B is

denoted by EdB(X, Xˆ ) =

N k=1

EdB (X k ,

Xˆk ).

Speciﬁcally,

the mean squared error (MSE) yields EdB(X, Xˆ ) = E(||X −

Xˆ ||2) =

N k=1

E(||X

k

−

Xˆk ||2 ).

Let

us

denote

the

cost

of activating a sensor by λ. Heterogeneous sensor classes

with different priorities or weights can be straightforwardly

accommodated and thus are not presented herein.

1) The unconstrained problem: Given a conﬁguration B ∈ B, the associated network cost is given by:

h(B) := EdB(X, Xˆ ) + λ||B||1

(1)

In the context of stastistical physics, one can view h(B) as the potential under conﬁguration B. Our goal herein is to solve the following optimization problem:

min h(B)
B∈B

(UP)

2) The constrained problem: Problem (UP) is a relaxed version of the constrained problem below:

min EdB(X, Xˆ ) s.t. E||B||1 ≤ N¯

(CP)

B∈B

Here the expectation in the constraint is over any possible

randomization in choosing the conﬁguration B. The cost of

activating a sensor, λ, can be viewed as a Lagrange multiplier

used to relax this constrained problem.

Theorem 1 relates solution of (UP) to (CP).

Theorem 1: Consider problems (CP) and (UP). If there exists a Lagrange multiplier λ∗ ≥ 0 and a B∗ ∈ B, such that an optimal conﬁguration for (UP) under λ = λ∗ is B∗,

and the constraint in (CP) is satisﬁed with equality under the pair (B∗, λ∗), then B∗ is an optimal conﬁguration for (CP).

In case there exist multiple conﬁgurations B1∗, B2∗, · · · , Bm∗ , a multiplier λ∗ ≥ 0, and a probability mass function

(p1, p2, · · · , pm) such that (i) each of B1∗, B2∗, · · · , Bm∗ is opti-

mal for problem (UP) under λ∗, and (ii)

m i=1

pi||Bi∗||1

=

N¯ ,

then an optimal solution for (CP) is to choose one conﬁgu-

ration

from

B

∗ 1

,

B2∗

,

·

·

·

, Bm∗

with

probability

mass

function

(p1, p2, · · · , pm).

Proof: See Appendix A.

Remark 1: Theorem 1 allows us to obtain a solution for

(CP) from the solution of (UP) by choosing an appropriate λ∗; this will be elaborated upon in Section IV.

III. GIBBS SAMPLING APPROACH TO SOLVE THE
UNCONSTRAINED PROBLEM
In this section, we will provide algorithms based on Gibbs sampling to compute the optimal solution for (UP).

A. Basic Gibbs sampling

Let us denote the distribution πβ(·) over B as follows:

πβ(B) :=

e−βh(B)

e−βh(B)

:=

B∈B e−βh(B)

Zβ

Choose any initial B(0) ∈ {0, 1}N . At each discrete time instant t = 0, 1, 2, · · · , pick a random sensor jt ∈ N independently and uniformly. For sensor jt, choose Bjt (t) = 1 with probability p := and choose e−βh(B−jt (t−1),1)
e−βh(B−jt (t−1),1)+e−βh(B−jt (t−1),0)
Bjt (t) = 0 with probability (1 − p). Choose Bk(t) = Bk(t − 1) for all k = jt.
Algorithm 1: BASICGIBBS algorithm
. Motivated by the theory of statistical physics, we call the parameter β the inverse temperature, and Zβ the partition function. Clearly, limβ↑∞ B∈arg minA∈B h(A) πβ(B) = 1. Hence, if we can choose a conﬁguration B with probability πβ(B) for a large β > 0, we can approximately solve (UP).
Computing Zβ will require 2N addition operations, and hence it is computationally prohibitive for large N . As an alternative, we provide an iterative algorithm based on Gibbs sampling, which requires many fewer computations in each iteration. Gibbs sampling runs a discrete-time Markov chain {B(t)}t≥0 whose stationary distribution is πβ(·).
The BASICGIBBS algorithm (Algorithm 1) simulates the Markov chain {B(t)}t≥0 for any β > 0. The fusion center runs the algorithm to determine the activation set; as such, the fusion center must create a virtual network graph.
Theorem 2: The Markov chain {B(t)}t≥0 has a stationary distribution πβ(·) under the BASICGIBBS algorithm.
Proof: Follows from the theory in [10, Chapter 7]). Remark 2: Theorem 2 tells us that if the fusion center runs BASICGIBBS algorithm and reaches the steady state distribution of the Markov chain {B(t)}t≥0, then the conﬁguration chosen by the algorithm will have distribution πβ(·). For very large β > 0, if one runs {B(t)}t≥0 for a sufﬁciently long, ﬁnite time T , then the terminal state BT will belong to arg minB∈B h(B) with high probability.
B. The exact solution
BASICGIBBS is operated with a ﬁxed β; but, in practice, the optimal soultion of the unconstrained problem (UP) is obtained with β ↑ ∞; this is done by updating β at a slower time-scale than the iterates of BASICGIBBS. This new algorithm is called MODIFIEDGIBBS (Algorithm 2).
Theorem 3: Under MODIFIEDGIBBS algorithm, the Markov chain {B(t)}t≥0 is strongly ergodic, and the limiting probability distribution satisﬁes limt→∞ A∈arg minC∈B h(C) P(B(t) = A) = 1.
Proof: See Appendix C. We have used the notion of weak and strong ergodicity of time-inhomogeneous Markov chains from [10, Chapter 6, Section 8]), which is provided in Appendix B. The proof is similar to the proof of one theorem in [9], but is given here for completeness.
Remark 3: Theorem 3 shows that we can solve (UP) exactly if we run MODIFIEDGIBBS algorithm for inﬁnite time, in contrast with BASICGIBBS algorithm which provides an

This algorithm is same as BASICGIBBS algorithm except that at time t, we use β(t) := β(0) log(1 + t) to compute the update probabilities, where β(0) > 0, β(0)N ∆ < 1, and ∆ := maxB∈B,A∈B |h(B) − h(A)|.
Algorithm 2: MODIFIEDGIBBS algorithm
approximate solution. Remark 4: For i.i.d. time varying {X(t)}t≥0 with known
joint distribution, we can either: (i) ﬁnd the optimal conﬁguration B∗ using MODIFIEDGIBBS and use B∗ for ever, or (ii) run MODIFIEDGIBBS at the same timescale as t, and use the running conﬁguration B(t) for sensor activation; both schemes will minimize the time-average expected cost.
C. Convergence rate of BASICGIBBS
Let µt denote the probability distribution of B(t) under BASICGIBBS. Let us consider the transition probability matrix P of the Markov chain {X(l)}l≥0 with X(l) = B(lN ), under the BASICGIBBS algorithm. Let us recall the deﬁnition of the Dobrushin’s ergodic coefﬁcient δ(P ) from [10, Chapter 6, Section 7] for the matrix P ; using a method similar to that of the proof of Theorem 3, we can show that δ(P ) ≤ (1 − e−NβNN∆ ). Then, by [10, Chapter 6, Theorem 7.2], we can say that under BASICGIBBS algorithm, we have
l
dV (µlN , πβ) ≤ dV (µ0, πβ) 1 − e−NβNN∆ . We can prove similar bounds for any t = lN + k, where 0 ≤ k ≤ N − 1.
Unfortunately, we are not aware of such a bound for MODIFIEDGIBBS.
Remark 5: Clearly, under BASICGIBBS algorithm, the convergence rate decreases as β increases. Hence, there is a trade-off between convergence rate and accuracy of the solution in this case. Also, the rate of convergence decreases with N . For MODIFIEDGIBBS algorithm, the convergence rate is expected to decrease with time.
IV. GIBBS SAMPLING AND STOCHASTIC APPROXIMATION
BASED APPROACH TO SOLVE THE CONSTRAINED PROBLEM
In Section III, we presented Gibbs sampling based algorithms for (UP). In this section, we provide an algorithm that updates λ with time in order to meet the constraint in (CP) with equality, and thereby solves (CP) (via Theorem 1).
Lemma 1: The optimal mean number of active sensors, E|B|1, for the unconstrained problem (UP), decreases with λ. Similarly, the optimal error, EdB(X, Xˆ ), increases with λ.
Proof: See Appendix D. Lemma 1 provides an intuition about how to update λ in BASICGIBBS or in MODIFIEDGIBBS in order to solve (CP). We seek to provide one algorithm which updates λ(t) in each iteration, based on the number of active sensors in the previous iteration. In order to maintain the necessary timescale difference between the {B(t)}t≥0 process and the λ(t) update process, we use stochastic approximation ( [11]) based update rules for λ(t).

Choose any initial B(0) ∈ {0, 1}N and λ(0) ≥ 0. At

each discrete time instant t = 0, 1, 2, · · · , pick a random

sensor jt ∈ N independently and uniformly. For sensor

jt, choose Bjt (t) = 1 with probability p := e−βhλ(t)(B−jt (t−1),1)
e−βhλ(t)(B−jt (t−1),1)+e−βhλ(t)(B−jt (t−1),0)

and choose

Bjt (t) = 0 with probability (1 − p). For k = jt, we

choose Bk(t) = Bk(t − 1).

After this operation, before the (t + 1) decision instant,

update λ(t) at each node as follows.

λ(t + 1) = [λ(t) + a(t)(||B(t − 1)||1 − N¯ )]cb
The stepsize {a(t)}t≥1 constitutes a positive sequence such that ∞ t=1 a(t) = ∞ and ∞ t=1 a2(t) = ∞. The nonnegative projection boundaries b and c for the λ(t) iterates are such that λ∗ ∈ (b, c) where λ∗ is deﬁned in Assumption 1.
Algorithm 3: GIBBSLEARNING algorithm

Choose any initial estimate θ1. Sample the sensor
j1 = arg minj∈N E dB:Bj=1,||B||1=1(X, Xˆ ) θ1 . In
general, after sampling nodes j1, j2, · · · , jk and observing the partial data Xj1 = xj1 , · · · , Xjk = xjk , obtain a new estimate θk+1 by completely running the EM algorithm using the available partial data and starting from the initial estimate θk. Once θk+1 is obtained, sample

jk+1 = arg

min

EB

j∈N ,j∈/{j1,··· ,jk}

dB(X, Xˆ ) Xj1 = xj1 , · · · , Xjk = xjk ; θk+1

where B is such that Bj = Bj1 = · · · = Bjk = 1, and ||B||1 = k + 1. Continue this process until the N -th
sensor is sampled. Algorithm 4: EMSTATIC algorithm

Remark 6: The optimal mean number of active sensors, E||B||1, for the unconstrained problem (UP) is a decreasing staircase function of λ, where each point of discontinuity is associated with a change in the optimizer B∗(λ).
The above remark tells us that the optimal solution of the constrained problem (CP) requires us to randomize between two values of λ in case the optimal λ∗ as in Theorem 1 belongs to the set of such discontinuities. However, this randomization will require us to update a randomization probability at another timescale; having stochastic approximations running in multiple timescales leads to very slow convergence and hence is not a very practical solution for (CP). Hence, instead of using a varying β(t), we use a ﬁxed, but large β and update λ(t) in an iterative fashion using stochastic approximation.
Before proposing the algorithm, we provide a result analogous to that in Lemma 1.
Lemma 2: Under BASICGIBBS algorithm for any given β > 0, the mean number of active sensors E||B||1 is a continuous and decreasing function of λ.
Proof: See Appendix E.
Let us ﬁx any β > 0. We make the following feasibility assumption for (CP), under the chosen β > 0.
Assumption 1: There exists λ∗ ≥ 0 such that the constraint in (CP) under λ∗ and BASICGIBBS is met with equality.
Remark 7: By Lemma 2, E||B||1 continuously decreases in λ. Hence, if N¯ is feasible, then such a λ∗ must exist by the intermediate value theorem. Let us deﬁne: hλ(t)(B) := EdB(X, Xˆ ) + λ(t)||B||1.
Our proposed algorithm GIBBSLEARNING (Algorithm 3) updates λ(t) iteratively in order to solve (CP).
Discussion of GIBBSLEARNING algorithm:
• If ||B(t−1)||1 is more than N¯ , then λ(t) is increased with the hope that this will reduce the number of active sensors in subsequent iterations, as suggested by Lemma 2.

• The B(t) and λ(t) processes run on two different timescales; B(t) runs in the faster timescale whereas λ(t) runs in a slower timescale. This can be understood from the fact that the stepsize in the λ(t) update process decreases with time t. Here the faster timescale iterate will view the slower timescale iterate as quasi-static, while the slower timescale iterate will view the faster timescale as almost equilibriated. This is reminiscent of two-timescale stochastic approximation (see [11, Chapter 6]).
Let πβ|λ∗ (·) denote πβ(·) under λ = λ∗. Theorem 4: Under GIBBSLEARNING algorithm and Assumption 1, we have λ(t) → λ∗ almost surely, and the limiting distribution of {B(t)}t≥0 is πβ|λ∗ (·).
Proof: See Appendix F.
This theorem says that GIBBSLEARNING produces a conﬁguration from the distribution πβ|λ∗ (·) under steady state.

A. A hard constraint on the number of activated sensors

Let us consider the following modiﬁed constrained problem:

min EdB(X, Xˆ ) s.t. ||B||1 ≤ N¯
B∈B

(MCP)

It is easy to see that (MCP) can be easily solved using similar Gibbs sampling algorithms as in Section III, where the Gibbs sampling algorithm runs only on the set of conﬁgurations which activate N¯ number of sensors. Thus, as a by-product, we have also proposed a methodology for the problem in [8], though our framework is more general than [8].
Remark 8: The constraint in (CP) is weaker than (MCP).
Remark 9: If we choose β very large, then the number of sensors activated by GIBBSLEARNING will have very small variance. This allows us to solve (MCP) with high probability.

V. EXPECTATION MAXIMIZATION BASED ALGORITHM FOR
PARAMETERIZED DISTRIBUTION OF DATA
In previous sections, we assumed that the joint distribution of X is completely known to the fusion center. In case this joint distribution is not known but a parametric form p(x|θ) of the distribution is known with unknown parameter θ, selecting all active sensors at once might be highly suboptimal, and a better approach would be to sample sensor nodes sequentially and reﬁne the estimate of θ using the data collected from a newly sampled sensor. We use standard expectation maximization (EM) algorithm (see [12, Section 5.2]) to reﬁne the estimate of θ. Hence, we present a greedy algorithm EMSTATIC (Algorithm 4) to solve (MCP):
Remark 10: This algorithm is based on heuristics, and it does not have any optimality guarantee because (i) EM algorithm yields a parameter value which corresponds to only a local maximum of the log-likelihood function of the observed data, and (ii) the greedy algorithm to pick the nodes is suboptimal.
The performance of EMSTATIC algorithm depends on the initial value θ1, since θ1 will determine {θk}k=1,2,··· ,N¯ and the chosen subset of activated sensors. If θ1 happens to be initialized at a favourable value, then EMSTATIC algorithm might even yield the same optimal subset of sensors as in MCP with θ known apriori. One trivial example for this case would be when N = 1 and we set θ1 = θ.
In case X(t) ∼ p(x|θ) varies in time t in an i.i.d. fashion, we can employ the EMSEQUENTIAL algorithm (Algorithm 5) to ﬁnd the optimal subset of sensors at each discrete time slot t.
Remark 11: The performance of EMSEQUENTIAL algorithm depends on the initial estimate θ1. Also, the maximization operation B(1) = arg minB(1)∈B:||B(1)||1=N E dB(1)(X(1), Xˆ (1)) θ1 can be efﬁciently done by employing Gibbs sampling algorithms as in Section III; this shows the potential use of Gibbs sampling in solving sensor subset selection problem for parameterized distribution of data with unknown parameters. However, since this is not the main focus of our paper, we will only consider known data distribution from now on.
VI. NUMERICAL RESULTS
A. Performance of BASICGIBBS algorithm
For the sake of illustration, we consider N = 18 sensors which are supposed to sense X = {X1, X2, · · · , X18}, where X is a jointly Gaussian random vector with covariance matrix M . Sensor k has access only to Xk. The matrix M is chosen as follows. We generated a random N × N matrix A whose elements are uniformly and independently distributed over the interval [−1, 1], and set M = AT A as the covariance matrix of X. We set sensor activation cost λ = 2.3, and seek to solve (UP) with MMSE as the error metric. We assume that sensing

Choose any initial estimate θ1. In slot t = 1, choose the conﬁguration B(1) of sensors B(1) =
arg minB(1)∈B:||B(1)||1=N E dB(1)(X(1), Xˆ (1)) θ1 . Then update the parameter to θ2 using EM algorithm with the partial observation XB(1)(1) = xB(1)(1) and with initial estimate θ1. Use θ2 to choose B(2) = arg minB(2)∈B:||B(2)||1=N E dB(2)(X(2), Xˆ (2)) θ2 in slot t = 2. Continue this procedure for all t.
Algorithm 5: EMSEQUENTIAL algorithm
at each node is perfect,2 and that the fusion center estimates Xˆ from the observation {Xi}i∈S =: XS as E(X|XS), where S is the set of active sensors. Under such an estimation scheme, the conditional distribution of XSc is still a jointly Gaussian random vector with mean E(XSc |XS) and the covariance matrix M (Sc, Sc) − M (Sc, S)M (S, S)−1M (S, Sc) (see [12, Proposition 3.4.4]), where M (S, Sc) is the restriction of M to the rows indexed by S and the columns indexed by Sc. The trace of this covariance matrix gives the MMSE when the subset S of sensors are active.
In this scenario, in Figure 1, we compare the cost for the following four algorithms:
• OPTIMAL: Here we consider the minimum possible cost for (UP).
• BASICGIBBS under steady state: Here we assume that the conﬁguration B ∈ B is chosen according to the distribution πβ(·) deﬁned in Section III. This is done for several values of β.
• BASICGIBBS with ﬁnite iteration: Here we run BASICGIBBS algorithm for 100 iterations. This is done independently for several values of β, where for each β the iteration starts from an independent random conﬁguration. Note that, we have simulated only one sample path of BASICGIBBS for each β; if the algorithm is run again independently, the results will be different.
• GREEDY: Start with an empty set S, and ﬁnd the cost if this subset of sensors are activated. Then compare this cost with the cost in case sensor 1 is added to this set. If it turns out that adding sensor 1 to this set S reduces the cost, then add sensor 1 to the set S; otherwise, remove sensor 1 from set S. Do this operation serially for all sensors, and activate the sensors given by the ﬁnal set S.
It turns out that, under the optimal conﬁguration, 12 sensors are activated and the optimal cost is 32.3647. On the other hand, GREEDY activates 14 sensors and incurred a cost of 35.9663. However, we are not aware of any monotonicity or supermodularity property of the objective function in (UP); hence, we cannot provide any constant approximation ratio guarantee for the problem (UP). On the other hand, we have already proved that BASICGIBBS performs near optimally
2However, our analysis can be extended where there is sensing error, but the distribution of sensing error is known to the fusion center.

Cost Estimation error

50

48

46

OPTIMAL

GREEDY

44

BASICGIBBS with finite iteration

BASICGIBBS under steady state

42

40

38

36

34

32

0

2

4

6

8

10

Fig. 1: Comparison among OPTIMAL, BASICGIBBS under steady state, GREEDY, and BASICGIBBS with ﬁnite iterations, for solving problem (UP). For each β, BASICGIBBS with ﬁnite iterations stops after 100 iterations. Details are provided in Section VI-A.
for large β. Hence, we choose to investigate the performance of BASICGIBBS, though it might require more number of iterations compared to N = 18 iterations for GREEDY. It is important to note that, (UP) is NP-hard, and BASICGIBBS allows us to avoid searching over 2N possible conﬁgurations.
In Figure 1, we can see that for β ≥ 2, the steady state distribution πβ(·) of BASICGIBBS achieves better expected cost than GREEDY, and the cost becomes closer to the optimal cost as β increases. On the other hand, for each β ≥ 2, BASICGIBBS after 100 iterations yielded a conﬁguration that achieves near-optimal cost. Hence, BASICGIBBS with reasonably small number of iterations can be used to ﬁnd the optimal subset of active sensors when N is large.
B. Performance of Gibbs sampling applied to problem (MCP) Here we seek to solve problem (MCP) with N¯ = 10 under
the same setting as in Section VI-A except that a new sample of the covariance matrix M is chosen. Here we compare the estimation error for the following three cases:
• OPTIMAL: Here we choose an optimal subset for (MCP). • BASICGIBBS under steady state: Here we assume that the
conﬁguration B is chosen according to the steady-state distribution πβ(·) deﬁned in Section III, but restricted only to the set {B ∈ B : ||B||1 = N¯ }. This is done by putting h(B) = EdB(X, Xˆ ) if ||B||1 = N¯ and h(B) = ∞ otherwise. This is done for several values of β. • NEWGREEDY: Start with an empty set S, and ﬁnd the estimation error if this subset of sensors are activated. Then ﬁnd the sensor j1 which, when added to S, will result in the minimum estimation error. If the estimation error for S ∪ {j1} is less than that of S, then do S =

45

40

35

OPTIMAL

30

NEWGREEDY

BASICGIBBS under steady state

25

20

15

10

0

2

4

6

8

10

Fig. 2: Comparison among OPTIMAL, BASICGIBBS under steady state, and NEWGREEDY, for solving problem (MCP). Details are provided in Section VI-B.

S ∪ {j1}. Now ﬁnd the sensor j2 which, when added to S, will result in the minimum estimation error. If the estimation error for S ∪ {j2} is less than that of S, then do S = S ∪ {j2}. Repeat this operation until we have |S| = N¯ , and activate the set of N¯ sensors given by the ﬁnal set S. A similar greedy algorithm is used in [8].
The performances for these three cases are shown in Figure 2. It turns out that, the estimation error for OPTIMAL and NEWGREEDY are 12.9741 and 15.4343, respectively. BASICGIBBS outperforms NEWGREEDY for β ≥ 2, and becomes very close to OPTIMAL performance for β ≥ 5.
C. Convergence speed of GIBBSLEARNING algorithm
We ﬁrst demonstrate the convergence speed of GIBBSLEARNING algorithm, for one speciﬁc sample path.
We consider a setting similar to that of Section VI-A, except that we ﬁx β = 5. The covariance matrix M is generated using the same method, but the realization of M here is different from that in Section VI-A. Under this setting, for λ∗ = 2, BASICGIBBS algorithm yields the MMSE 3.5680, and the expected number of sensors activated by BASICGIBBS algorithm becomes 12.7758. Now, let us consider problem (CP) with the constraint value N¯ = 12.7758. Clearly, if GIBBSLEARNING algorithm is employed to ﬁnd out the solution of problem (CP) with N¯ = 12.7758, then λ(t) should converge to λ∗ = 2.
The evolution of λ(t) against the iteration index t is shown in the top plot in Figure 3. We can see that, starting from λ(0) = 4 and and using the stepsize sequence a(t) = 1t , the iterate λ(t) becomes very close to λ∗ = 2 within 100 iterations. At t = 200, we found that λ(200) = 2.0318. The resulting conﬁguration yielded by GIBBSLEARNING algorithm at t = 200 achieves MMSE 5.0308 and activates 12 sensors; it is important to remember that these results are

(t)

4

3.5

3

2.5

2

1.5

1

0.5

0

0

50

100

150

200

t

Fig. 3: Illustration for convergence speed of λ(t) in the GIBBSLEARNING algorithm. Top plot: Result for a single sample path. Details can be found in Section VI-C. Bottom plot: Average result over 1000 independent sample paths. Details can be found in Section VI-C.
for one speciﬁc realization of the sample path. On the other hand, under λ(200) = 2.0318, the steady state distribution of BASICGIBBS, πβ(·), yields MMSE 3.6524 and mean number of active sensors 12.7354, which are very close to the respective target values 3.5680 and N¯ = 12.7758.
However, the top plot in Figure 3 is only for a speciﬁc sample path of GIBBSLEARNING algorithm. In the bottom plot in Figure 3, we demonstrate convergence speed of λ(t) averaged over multiple independent sample paths of GIBBSLEARNING algorithm. Here we generate a different covariance matrix M , set λ∗ = 2, and follow the same procedure as before to set N¯ . Then we run GIBBSLEARNING algorithm independently 1000 times, starting from λ(0) = 4. The bottom plot of Figure 3 shows the variation of λ(t) (averaged over 1000 sample paths) with t. We can again see that the average λ(t) is very close to λ∗ = 2 for t ≥ 100.
Thus, our numerical illustration shows that GIBBSLEARN-

ING algorithm has reasonably fast convergence rate for practical active sensing.
VII. CONCLUSION
In this paper, we have presented Gibbs sampling, stochastic approximation and expectation maximization based algorithms for efﬁcient data estimation in the context of active sensing. We ﬁrst proposed Gibbs sampling based algorithms for unconstrained optimization of the estimation error and the mean number of active sensors, proved convergence of these algorithms, and provided a bound on the convergence speed. Next, we proposed an algorithm based on Gibbs sampling and stochastic approximation, in order to solve a constrained version of the above unconstrained problem, and proved its convergence. Finally, we proposed expectation maximization based algorithms for the scenario where the sensor data is coming from a distribution with known parametric distribution but unknown parameter value. Numerical results demonstrate the near-optimal performance of some of these algorithms with small number of computations.
As our future research endeavours, we seek to develop distributed sensor subset selection algorithms to efﬁciently track the data varying in time according to a stochastic process.
REFERENCES
[1] F. Schnitzler, J.Y. Yu, and S. Mannor. Sensor selection for crowdsensing dynamical systems. In International Conference on Artiﬁcial Intelligence and Statistics (AISTATS), pages 829–837, 2015.
[2] D.S. Zois, M. Levorato, and U. Mitra. Active classiﬁcation for pomdps: A kalman-like state estimator. IEEE Transactions on Signal Processing, 62(23):6209–6224, 2014.
[3] D.S. Zois, M. Levorato, and U. Mitra. Energy-efﬁcient, heterogeneous sensor selection for physical activity detection in wireless body area networks. IEEE Transactions on Signal Processing, 61(7):1581–1594, 2013.
[4] V. Krishnamurthy and D.V. Djonin. Structured threshold policies for dynamic sensor schedulinga partially observed markov decision process approach. IEEE Transactions on Signal Processing, 55(10):4938–4957, 2007.
[5] W. Wu and A. Arapostathis. Optimal sensor querying: General markovian and lqg models with controlled observations. IEEE Transactions on Automatic Control, 53(6):1392–1405, 2008.
[6] V. Gupta, T.H. Chung, B. Hassibi, and R.M. Murray. On a stochastic sensor selection algorithm with applications in sensor scheduling and sensor coverage. Automatica, 42:251–260, 2006.
[7] A. Bertrand and M. Moonen. Efﬁcient sensor subset selection and link failure response for linear mmse signal estimation in wireless sensor networks. In European Signal Processing Conference (EUSIPCO), pages 1092–1096. EURASIP, 2010.
[8] D. Wang, J. Fisher III, and Q. Liu. Efﬁcient observation selection in probabilistic graphical models using bayesian lower bounds. In Proceedings of the Thirty-Second Conference on Uncertainty in Artiﬁcial Intelligence (UAI), pages 755–764. ACM, 2016.
[9] A. Chattopadhyay and B. Baszczyszyn. Gibbsian on-line distributed content caching strategy for cellular networks. https:// arxiv.org/ abs/ 1610.02318, 2016.
[10] P. Bremaud. Markov Chains, Gibbs Fields, Monte Carlo Simulation, and Queues. Springer, 1999.
[11] Vivek S. Borkar. Stochastic approximation: a dynamical systems viewpoint. Cambridge University Press, 2008.
[12] B. Hajek. An Exploration of Random Processes for Engineers. Lecture Notes for ECE 534, 2011.

APPENDIX A PROOF OF THEOREM 1
We will prove only the ﬁrst part of the theorem where there exists a unique B∗. The second part of the theorem can be proved similarly.
Let us denote the optimizer for (CP) by B, which is possibly different from B∗. Then, by the deﬁnition of B∗, we have EdB∗(X, Xˆ ) + λ∗||B∗||1 ≤ EdB(X, Xˆ ) + λ∗||B||1. But ||B||1 ≤ K and ||B∗||1 = K. Hence, EdB∗(X, Xˆ ) ≤ EdB(X, Xˆ ). This completes the proof.

APPENDIX B WEAK AND STRONG ERGODICITY
Consider a discrete-time Markov chain (possibly not timehomogeneous) {B(t)}t≥0 with transition probability matrix (t.p.m.) P (m; n) between t = m and t = n. We denote by D the collection of all possible probasbility distributions on the state space. Let dV (·, ·) denote the total variation distance between two distributions in D. Then {B(t)}t≥0 is called weakly ergodic if, for all m ≥ 0, we have limn↑∞ supµ,ν∈D dV (µP (m; n), νP (m; n)) = 0.
The Markov chain {B(t)}t≥0 is called strongly ergodic if there exists π ∈ D such that, limn↑∞ supµ∈D dV (µT P (m; n), π) = 0 for all m ≥ 0.

APPENDIX C PROOF OF THEOREM 3

We will ﬁrst show that the Markov chain {B(t)}t≥0 in weakly ergodic.

Let us deﬁne ∆ := maxB∈B,A∈B |h(B) − h(A)|.

Consider the transition probability matrix (t.p.m.) Pl for
the inhomogeneous Markov chain {X(l)}l≥0 (where X(l) :=
B(lN )). The Dobrushin’s ergodic coefﬁcient δ(Pl) is given
by (see [10, Chapter 6, Section 7] for deﬁnition) δ(Pl) =
1 − infB ,B ∈B B∈B min{Pl(B , B), Pl(B , B)}. A sufﬁcient condition for the Markov chain {B(t)}t≥0 to be weakly ergodic is ∞ l=1(1 − δ(Pl)) = ∞ (by [10, Chapter 6, Theorem 8.2]).

Now, with positive probability, activation states for all nodes

are updated over a period of N slots. Hence, Pl(B , B) > 0 for
all B , B ∈ B. Also, once a node jt for t = lN + k is chosen
in MODIFIEDGIBBS algorithm, the sampling probability for any activation state in a slot is greater than e−β(lN2 +k)∆ . Hence, for independent sampling over N slots, we have, for all pairs

B , B:

N −1 e−β(lN +k)∆

Pl(B , B) >

>0 2N

k=0

Hence,
= ≥ ≥ = ≥ =

∞

(1 − δ(Pl))

l=0 ∞

inf

min{Pl(B , B), Pl(B , B)}

l=0 B ,B ∈B B∈B

∞

N −1

2N

l=0

k=0

e−β(0) log(1+lN +k)×∆ 2N

∞ N −1 e−β(0) log(1+lN +N )×∆

l=0 k=0
1∞

N 1

NN

(1 + lN )β(0)N∆

l=1

1

∞

1

N N+1

(1 + i)β(0)N∆

i=N +1

∞

(2)

Here the ﬁrst inequality uses the fact that the cardinality of B is 2N . The second inequality follows from replacing k by

N in the numerator. The third inequality follows from lower-

bounding

1 (1+lN )β(0)N∆

by

1 N

ilN=l+NN −1 (1+i)β1(0)N∆ . The last

equality follows from the fact that

∞1 i=1 ia

diverges

for

0<

a < 1.

Hence, the Markov chain {B(t)}t≥0 is weakly ergodic.

In order to prove strong ergodicity of {B(t)}t≥0, we

invoke [10, Chapter 6, Theorem 8.3]. We denote the t.p.m.

of {B(t)}t≥0 at a speciﬁc time t = T0 by Q(T0), which is

a given speciﬁc matrix. If {B(t)}t≥0 evolves up to inﬁnite

time with ﬁxed t.p.m. Q(T0), then it will reach the stationary

distribution πβ (B) = e−βT0 h(B) . Hence, we can claim that

T0

ZβT0

Condition 8.9 of [10, Chapter 6, Theorem 8.3] is satisﬁed.

Next, we check Condition 8.10 of [10, Chapter 6, Theo-

rem 8.3]. For any B ∈ arg minB ∈B h(B ), we can argue
that πβT0 (B) increases with T0 for sufﬁciently large T0; this can be veriﬁed by considering the derivative of πβ(B) w.r.t.

β. For B ∈/ arg minB ∈B h(B ), the probability πβT0 (B) decreases with T0 for large T0. Now, using the fact that

any monotone, bounded sequence converges, we can write ∞ T0=0 B∈B |πβT0+1 (B) − πβT0 (B)| < ∞.
Hence, by [10, Chapter 6, Theorem 8.3], the Markov chain

{B(t)}t≥0 is strongly ergodic. It is straightforward to verify

the claim regarding the limiting distribution.

APPENDIX D PROOF OF LEMMA 1
Let λ1 > λ2 > 0, and the corresponding optimal error and mean number of active sensors under these multiplier values be (d1, n1) and (d2, n2), respectively. Then, by deﬁnition, d1+ λ1n1 ≤ d2 + λ1n2 and d2 + λ2n2 ≤ d1 + λ2n1. Adding these two inequalities, we obtain λ1n1 + λ2n2 ≤ λ1n2 + λ2n1, i.e., (λ1 − λ2)n1 ≤ (λ1 − λ2)n2. Since λ1 > λ2, we obtain n1 ≤ n2. This completes the ﬁrst part of the proof. The second

part of the proof follows using similar arguments.

APPENDIX E

PROOF OF LEMMA 2

Let us denote E||B||1 =: f (λ) =

B∈B ||B||1e−βh(B) . It is
Zβ

straightforward to see that E||B||1 is continuously differen-

tiable in λ.

Let us denote Zβ by Z for simplicity, and let h(B) = dB + λ||B||1 be the linear combination of the error (here we have written EdB(·, ·) as dB for simplicity in notation) and number of active sensors under conﬁguration B. Then the derivative
of f (λ) w.r.t. λ is given by:

f (λ) −Z β =

B∈B ||B||21e−β(dB +λ||B||1) − Z2

B∈B

||B||1e−β(dB +λ||B||1)

dZ dλ

Now, it is straightforward to verify that ddZλ = −βZf (λ). Hence,

f (λ) −Zβ B∈B ||B||21e−β(dB +λ||B||1) + B∈B ||B||1e−β(dB +λ||B||1)βZf (λ) =
Z2

Now, f (λ) ≤ 0 is equivalent to

f (λ) ≤

B∈B ||B||21e−β(dB+λ||B||1) B∈B ||B||1e−β(dB+λ||B||1)

Noting that E||B||1 =: f (λ) and dividing the numerator and
denominator of R.H.S. by Z, the condition is reduced to E||B||1 ≤ EE||||BB||||211 , which is true since E||B||21 ≥ (E||B||1)2. Hence, E||B||1 is decreasing in λ for any β > 0.

APPENDIX F
PROOF OF THEOREM 4
Let the distribution of B(t) under GIBBSLEARNING algorithm be µt(·). Since limt→∞ a(t) = 0, it follows that limt→∞ dV (µt−1, πβ|λ(t−1)) = 0 (where dV (·, ·) is the total variation distance), and limt→∞(Eµt−1 ||B||1 − Eπβ|λ(t−1)||B||1) := limt→∞ e(t) = 0. Now, we can rewrite the λ(t) update equation as follows:
λ(t + 1) = [λ(t) + a(t)(Eπβ|λ(t−1)||B||1 − N¯ + Mt + et)]cb (3)
Here Mt := ||B(t−1)||1−Eµt−1 ||B(t−1)||1 is a Martingale difference noise sequence, and limt→∞ et = 0. It is easy to see that the derivative of Eπβ|λ||B||1 w.r.t. λ is bouned for λ ∈ [b, c]; hence, Eπβ|λ||B||1 is a Lipschitz continuous function of λ. It is also easy to see that the sequence {Mt}t≥0 is bounded. Hence, by the theory presented in [11, Chapter 2] and [11, Chapter 5, Section 5.4], λ(t) converges to the unique zero of Eπβ|λ||B||1 − N¯ almost surely. Hence, λ(t) → λ∗ almost surely. Since limt→∞ dV (µt−1, πβ|λ(t−1)) = 0 and πβ|λ is continuous in λ, the limiting distribution of B(t) becomes πβ|λ∗ .

