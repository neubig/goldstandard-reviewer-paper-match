Do We Know What We Don’t Know? Studying Unanswerable Questions beyond SQuAD 2.0
Elior Sulem, Jamaal Hay and Dan Roth Department of Computer and Information Science, University of Pennsylvania
eliors,jamaalh,danroth@seas.upenn.edu

Abstract

Understanding when a text snippet does not

provide a sought after information is an essential part of natural language understanding.

Context: John was born in New York.

Recent work (SQuAD 2.0, Rajpurkar et al., 2018) has attempted to make some progress in this direction by enriching the SQuAD dataset

Q1: Where did John marry? Answer: IDK - Competitive

for the Extractive QA task with unanswerable questions. However, as we show, the performance of a top system trained on SQuAD 2.0

Q2: When was John born? Answer: IDK - Non-competitive

drops considerably in out-of-domain scenarios,

limiting its use in practical situations. In or-

der to study this we build an out-of-domain corpus, focusing on simple event-based questions and distinguish between two types of

Figure 1: Examples of a competitive (Q1) and a noncompetitive (Q2) IDK questions.

IDK questions: competitive questions, where

the context includes an entity of the same type as the expected answer, and simpler, noncompetitive questions where there is no entity

identify that a given information is not in the text, a witness of understanding in human comprehension.

of the same type in the context. We ﬁnd that

SQuAD 2.0-based models fail even in the case of the simpler questions. We then analyze the similarities and differences between the IDK phenomenon in Extractive QA and the Recognizing Textual Entailments task (RTE, Dagan et al., 2013) and investigate the extent to which the latter can be used to improve the performance.1

The ability to answer "IDK" allows one to address more realistic situations in reading comprehension, both as an end task and as an intermediary step for other NLP applications, such as QA-based event extraction (Chen et al., 2020; Lyu et al., 2021) or QA-based summarization evaluation (Deutsch et al., 2021).

1 Introduction

To begin addressing this important phenomenon, Rajpurkar et al. (2018) added unanswerable ques-

Extractive Question Answering (Extractive QA) tions to SQuAD 1.1 (Rajpurkar et al., 2016), pro-

has attracted a lot of interest in recent years with viding a useful resource for identifying IDK cases

the creation of large-scale datasets (Rajpurkar et al., in the Extactive QA case (SQuAD 2.0). How-

2016, 2018) and has seen large improvements with ever, as we show, the performance of a top sys-

the use of contextualized language models such tem trained on SQuAD 2.0 considerably drops on

as BERT (Devlin et al., 2019) and RoBERTa (Liu out-of-domain simple questions.

et al., 2019). However, the ability to extract infor- In this paper, we show that SQuAD 2.0 alone

mation from text only addresses one aspect of the is not sufﬁcient to address IDK questions in prac-

expectations we have from a comprehension sys- tical situations. For this purpose, we introduce a

tem. Another main aspect concerns the ability to new evaluation dataset of very simple questions

1The new datasets along with all the other artifacts generated here are available at http://cogcomp.org/page/ publication_view/955.

on single-sentence contexts that we compile based on an event extraction corpus (ACE, Walker et al., 2006). In particular, we propose to separately eval-

4543

Findings of the Association for Computational Linguistics: EMNLP 2021, pages 4543–4548 November 7–11, 2021. ©2021 Association for Computational Linguistics

Corpus MNLI SQuAD 2.0

Split # Examples IDK (%)

Task

Data Annotation

Existing Corpora

train

392,702

33

dev

9,815

32

RTE

"entailment", "contradiction", "neutral"

train

130,319

dev

11,873

33

Extractive QA

50

extracted span, "[]"

Has answer

test

ACE-whQA

Compet. IDK

test

non-Compet. IDK test

New Corpora

238

0

250

100

Extractive QA

246

100

extracted span, "[]"

Table 1: Statistics and properties of existing corpora we use (top) and the newly introduced corpus (bottom).

uate the performance of a QA model on two types et al. (2020) by experimenting in an out-of-domain

of IDK questions: (i) cases where the context in- setting and by preserving the QA format. We also

cludes an entity of the same type as the expected distinguish between easier IDK cases when there

answer such as Q1 in Figure 1 where “New York" is no entities of the argument type expected by the

is a location appearing in the context 1. We call question and harder cases where an entity of the

this type of questions competitive IDK questions same type appears in the sentence (see Section 3).

and (ii) cases where the context includes no entity of the same type as the expected answer such as Q2 in Figure 1 where the question expects a time mention while the context does not include time.
Evaluating on the new dataset, we ﬁnd that a top SQuAD 2.0 model obtains low scores. even in the case of the simpler, non-competitive IDK questions, only reaching 28.46 F1 (Section 4).

Rajpurkar et al. (2018) enriched the SQuAD 1.1 corpus by including unanswerable questions for the same paragraphs via crowdsourcing, resulting in SQuAD 2.0, that we are using in this paper for the Extractive QA task. We show that training on SQuAD 2.0 is not sufﬁcient to address IDK in out-of-domain settings (focusing on simple, eventbased questions) and that the RTE data can be use-

We then explore the use of another Natural Lan- ful to address a particular type of IDK questions.

guage Understanding (NLU) task that also includes an IDK option. We focus on the Recognizing Textual Entailments (RTE, Dagan et al., 2013) task and ﬁnd that leveraging it considerably improves the results in the case of non-competitive IDK questions but is not sufﬁcient for reaching a good performance in the competitive IDK cases.

Rajpurkar et al. (2018) experimented on SQuAD 2.0 using the BiDAF-No-Answer (BNA) model proposed by Levy et al. (2017) and the DocumentQA No-Answer (DocQA) model from Clark and Gardner (2018). These models learn to predict the probability that a question is unanswerable, in addition to a distribution over answer choices. This also

2 Related Work

holds in the BERT implementation we use here. An alternative way for training and prediction

Unanswerable questions have been ﬁrst addressed in the context of the annual TREC competition for open-domain QA (Voorhees, 2002), where the subtask of span extraction has some similarities with Extractive QA, although in the former the goal is to answer a question from a large collection of documents. In Extractive QA, a system being able to answer "I don’t know" has been proposed by Levy et al. (2017) in the framework of the relation extraction task which is formulated in QA terms. Another example is the use of QA systems for event extrac-

in the case of unanswerable questions has been advanced by Tan et al. (2018) who proposed to ﬁrst predict whether there is an answer in the context. Tan et al. (2018) also used a predict+validate approach, which is also explored by Hu et al. (2019) who added a separately trained answer veriﬁer for no-answer detection. We do not modify the training and prediction used in the BERT paper approach but rather explore the performance in outof-domain scenarios as well as the use of RTE to improve the performance.

tion, as recently proposed by Chen et al. (2020) The selective question answering task in out-of-

who modiﬁed a BERT-based QA system to predict domain settings (Kamath et al., 2020) is related to

an argument role in a clozed test format. In this the identiﬁcation of unanswerable questions. How-

work we evaluate our Extract QA systems on event- ever, it targets the ability of a system to refrain from

based questions questions derived from the ACE answering in some of the cases in order to avoid er-

corpus (Walker et al., 2006), focusing on the loca- rors in out-of-domain settings, independently from

tion and time argument types. We differ from Chen the presence of the answer in the context. The au-

4544

IDK questions where there is an entity in the con-

text that has the same type as the expected answer;

this creates competition and makes the prediction

harder (Compet. IDK). For creating this type of

examples, we manually modify the context sen-

tence to add an entity that has the same type as the

expected answer. We choose the entity randomly

from a set of time/locations entities appearing in

Figure 2: Examples of RTE hypotheses (left) and whquestions (right) given a premise/context.

the dataset. For example, given the context “She went to Mexico after she lost her seat in the 1997

election", a Compet. IDK question is “Where is the

thors show that selective prediction methods do not identify unanswerable questions, suggesting that an explicit labeling of IDK in the training data is necessary in our case.
In the RTE task (Dagan et al., 2013), the IDK option is instantiated by the "neutral" category. In some of the RTE works (Bentivogli et al., 2009; Wang et al., 2018), “contradiction" and “neutral" are uniﬁed in a “non-entailed" joint category. Demszky et al. (2018) proposed a conversion of Extractive QA datasets to 2-label RTE format. We instead leverage the RTE task for Extractive QA via additional pretraining and compare between the

loss?". The second type of questions (non-Compet. IDK) concerns cases where there is no candidate of the same type in the sentence. In this case too, we use manual modiﬁcations. For example, given the context “He was arrested for his crimes", a noncompet. IDK question is “When was the arrest?". The resulting corpus, called ACE-whQA includes three sub-corpora: “Has Answer", “Compet. IDK" and “non-Compet. IDK" with 238, 250 and 246 examples respectively. More examples are presented in Figure 3.
4 Training on SQuAD 2.0 is Not Sufﬁcient

presence and the absence of an IDK label in the RTE data (See Section 5).
3 Test Datasets

We ﬁnetune the BERT-LARGE-CASED representation on the SQuAD 2.0 dataset and evaluate on ACE-whQA.3 We also report the score on the SQuAD 2.0 dev set (80.96 F1).

We leverage the ACE event extraction (Walker et al., 2006)2 dataset to derive questions asking about the argument that participates in an event, given the trigger. This allow us to experiment on IDK answers that result from the fact that one of the event arguments is missing. For this purpose, we ﬁrst select sentence fragments that include a location or a time mention according to the ACE annotation. To generate the wh-questions, we automatically generate candidate questions based on

The evaluation on the ACE-whQA dataset is presented in The ﬁrst column of Table 3. We ﬁnd that for "Has Answer" the performance of the baseline trained on SQuAD 2.0 drops, compared to the indomain setting but still achieves acceptable performance. However, in the case of IDK, we observe that even in the case of easy questions, with no competition from an entity of the same type (nonCompet. IDK), the performance of the baseline system is very low (28.46).

the event structure by asking both where and when did T happen, where T is the event trigger. The answer is labeled “I don’t know” when the entity type is missing.
To generate additional IDK questions, we select more sentences from the ACE dataset that do not necessarily include time/location mentions. All the questions are manually validated to ensure both grammatical and logical correctness. We compile two types of IDK questions. The ﬁrst concerns
2https://catalog.ldc.upenn.edu/ LDC2006T06

5 Exploring the use of the RTE task
Similarities and Differences The Recognizing Textual Entailment (RTE) task (Dagan et al., 2013) consists of classifying a sentence pair composed of a premise p and a hypothesis h into three classes, according to the relation between the two sentences: “entailment", “contradiction" and “neutral", which corresponds to the IDK option. Although the instances of IDK in RTE and Extractive QA share
3For training on SQuAD 2.0, we use two train epochs and ﬁne-tune for the learning rate (3e-5 and 5e-5) and the batch size (24 and 48).

4545

Train → Test ↓ All Has answer IDK

SQuAD 2.0
80.96 83.53 78.40

MNLI + SQuAD 2.0
81.92∗ 84.63 79.23∗

c(MNLI) + SQuAD 2.0
82.60∗ 84.12 81.09∗

Table 2: F1 scores of the different systems, tested on SQuAD 2.0 Dev for the Extractive QA task. The rows represent the training strategies. The columns represent the test datasets. In all the cases the trained representation is BERT-LARGE-CASED. In each line the highest score is presented in bold. The scores signiﬁcantly higher (using a one-sided t-test, p < 0.05) than the baseline (the ﬁrst column) appear with a star (∗).

Train → Test ↓ Has answer Compet. IDK non-Compet. IDK

SQuAD 2.0
62.39 20.8 28.46

MNLI + SQuAD 2.0
71.68 46.40∗ 75.61∗

c(MNLI)+ SQuAD 2.0
78.13 26.00 47.15∗ ◦

Table 3: F1 scores of the different systems, tested on the ACE-whQA out-of-domain test set for the Extractive
QA task. In all the cases the trained representation is BERT-LARGE-CASED. In each line the highest score
is presented in bold. The scores signiﬁcantly higher (using a one-sided t-test, p < 0.05) than the baseline (the ﬁrst column) appear with a star (∗). Scores that are signiﬁcantly higher than the baseline and in the same time, signiﬁcantly lower than the top system, are presented with a circle (◦).

✁✂✄☎✆✝✞✟✠ ✡☛☞✌ ✍☞✎☞ ✏☛☞ ✑✒✏✓✔ ✕✝✞☎✂✖☎✟✠ ✗✌ ✘✙✙✚✛ ✜✒✢✣☞✏ ✑✒✏✓ ☞✤✥✦✥✌✧✏☞✢ ✏☛☞ ★✩✜ ✩✪ ✫✏✧✏☞ ✬✎✑☛✧☞✩✤✩✣✥✓✏✭ ✮✞✄✯✂✰✟✠ ✱✲✲✳ ✁✂✄☎✆✝✞✴✠ ✡☛☞✌ ✍✧✓ ✏☛☞ ✑✩✌✵✥✑✏✥✩✌✔ ✕✝✞☎✂✖☎✴✠ ✗✶✵☞ ✜☞☞✌ ✑✩✌✵✥✑✏☞✢ ✪✩✎ ✤✧✓✏ ✷☞✧✎✶✓ ✑✎✥✦☞✓✸ ✮✞✄✯✂✰✴✠ ✹✺✻ ✁✂✄☎✆✝✞✼✠ ✡☛☞✎☞ ✍✧✓ ✓☛☞ ✓☞✌✏☞✌✑☞✢✔ ✕✝✞☎✂✖☎✼✠ ✫☛☞ ✍✧✓ ✓☞✌✏☞✌✑☞✢ ✏✩ ✘✽ ✷☞✧✎✓ ✪✩✎ ✏☛☞ ✦✒✎✢☞✎✸ ✮✞✄✯✂✰✼✠ ✹✺✻

used by the community.
Experimental Setting Here we consider Extractive QA as a target task. RTE is the auxiliary task. Our baseline system consists in the BERT-LARGECASED representation ﬁne-tuned on the SQuAD 2.0 train corpus. We experiment with the following systems: (i) MNLI + SQuAD 2.0 where we ﬁrst ﬁnetune BERT-LARGE on MNLI, remove the clas-

Figure 3: Examples of (1) Has-answer, (2) Competitive IDK and (3) Non-competitive questions from the ACEwhQA dataset.

siﬁcation layer and further ﬁnetune on SQuAD 2.0. (ii) c(MNLI) + SQuAD 2.0: 2-label pretraining on
MNLI, where we only consider the "contradiction" and "non-contradiction" classes.4 In all cases we

evaluate the system on the SQuAD 2.0 dev as well

a common idea, there are also considerable differ- as the three sub-corpora of ACE-whQA introduced

ences. First, the format of a wh-question is miss- in Section 3: questions that have an answer (Has

ing some content which is already present in a answer), questions that do not have an answer but

corresponding RTE hypothesis; for example, the there is an entity in the sentence of the same type as

location entity in a “where" question. Therefore, the expected answer (Competitive IDK) and ques-

a wh-question cannot be directly converted to an tions that do not have an answer and there is no

RTE hypothesis, independently from the context. entity of the same type (non-competitive. IDK).

This format difference is also related to the fact that RTE can be seen as a classiﬁcation task, while Extract QA involves span extraction. Second, a conversion between the formats will not always preserve the IDK label, as illustrated in H1 and Q1 in Figure 2. In particular, an IDK instance in Extractive QA can correspond also to a "contradiction" in RTE. Finally, while short paragraphs are used in SQuAD 2.0, the premises in the MNLI corpus for the RTE task are single sentences. While this is not inherent in the deﬁnition of the respective tasks, the available datasets impact the models

For training on MNLI with the BERT-LARGECASED representation, we use batch size of 32 and 3 training epochs. We ﬁne tune over three possible learning rate values: 2e-5, 3e-5 and 5e5. For training on SQuAD 2.0, we use the same hyperparameters as in Section 4. For each of the training settings, we choose the hyperparameter combination that maximizes the accuracy for the
4We chose this binary version for the experiments (the other versions being "entailment"/"non-entailment" and "neutral"/"non-neutral") since it achieved the highest score on the corresponding binary MNLI dev set (92.50 accuracy).

4546

target task on the SQuAD 2.0 dev set.

data for IDK and the improvement of the ability to

face adversarial IDK questions. Results The evaluation on the SQuAD 2.0 dev

set is presented in Table 2, where we report the F1 Acknowledgements
scores. We observe that the use of MNLI for addi-

tional pretraining is helpful, siginiﬁcantly improv- We thank Qing Lyu and the members of the Coging both the overall and the IDK scores5 SQuAD nitive Computation Group for their insightful feed-

2.0 where the additional pretraining is done on the back ad well as the reviewers of the paper for their

binary MNLI train corpus, which achieves the best useful suggestions. This work was supported by

performance but is not signiﬁcantly better than the Contracts FA8750-19-2-1004 and FA8750-19-2-

use of the 3-label MNLI.

0201 with the US Defense Advanced Research

The evaluation on the ACE-whQA dataset is pre- Projects Agency (DARPA). Approved for Public

sented in Table 3. We ﬁnd that for "Has Answer" Release, Distribution Unlimited. This research is

the performance of the baseline trained on SQuAD also based upon work supported in part by the

2.0 drops, compared to the in-domain setting but Ofﬁce of the Director of National Intelligence

still achieves acceptable performance. The best per- (ODNI), Intelligence Advanced Research Projects

formance is obtained where c(MNLI) is used for Activity (IARPA), via IARPA Contract No. 2019-

pretraining, reaching an F1 score of 78.13. How- 19051600006 under the BETTER Program. The

ever, in the case of IDK, we observe that even in the views and conclusions contained herein are those

case of easy questions, with no competition from of the authors and should not be interpreted as

an entity of the same type (non-Compet. IDK), necessarily representing the ofﬁcial policies, ei-

the performance of the baseline system is very low ther expressed or implied, of ODNI, IARPA, the

(28.46). The use of MNLI for additional pretrain- Department of Defense, or the U.S. Government.

ing greatly improves the performance, achieving The U.S. Government is authorized to reproduce

an F1 score of 75.61. For the harder IDK ques- and distribute reprints for governmental purposes

tions (where there is an entity of the same type notwithstanding any copyright annotation therein.

in the context), the performance signiﬁcantly im- This material is also based upon work supported by

proves as well when using MNLI (p < 0.05) but Google Cloud (TFRC Program).

it only reaches a score of 46.40, leaving room for

additional research.
We also observe that the best model in the in- References

domain setting that uses the binary MNLI corpus (with the same amount of data), achieves low results on IDK cases (and signiﬁcantly lower with respect to the 3-label MNLI) showing the importance

L. Bentivogli, I. Dagan, H. T. Dang, D. Giampiccolo, and B. Magnini. 2009. The ﬁfth PASCAL recognizing textual entailment challenge. In Proc. of TAC Workshop.

of training on the three labels to address eventbased IDK questions.
6 Conclusion
We studied the IDK phenomenon, which is essential in language comprehension, in Extractive QA, going beyond the evaluation on SQuAD 2.0. We designed an out-of-domain evaluation dataset, composed of two main types of IDK questions. We show that IDK in Extractive QA is a major challenge for current NLP systems. We further explore the use of the RTE dataset and observe a considerable improvement in the case of non-competitive

Yunmo Chen, Tongfei Chen, Seth Ebner, and Benjamin Van Durme. 2020. Reading the manual: Event extraction as deﬁnition comprehension. In Proceedings of the Fourth Workshop on Structured Prediction for NLP, pages 74–83.
Christopher Clark and Matt Gardner. 2018. Simple and effective multi-paragraph reading comprehension. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers).
Ido Dagan, Dan Roth, Mark Sammons, and Fabio Massimo Zanzoto. 2013. Recognizing Textual Entailment: Models and Applications.

questions. Future work concerns the use of additional Natural Language Understanding tasks and
5one-sided t-test, p < 0.05

Dorottya Demszky, Kelvin Guu, and Percy Liang. 2018. Transforming question answering datasets into natural language inference datasets. ArXiv:1809.02922 [cs.CL].

4547

Daniel Deutsch, Tania Bedrax-Weiss, and Dan Roth. 2021. Towards question-answering as an automatic metric for evaluating the content quality of a summary. TACL, 9:774–789.

Christopher Walker, Stephanie Strassel, Julie Medero, and Kazuaki Maeda. 2006. Ace 2005 multilingual training corpus. Linguistic Data Consortium, Philadelphia, 57.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171–4186.

Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2018. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 353–355.

Minghao Hu, Furu Wei, Yuxing Peng, Zhen Huang, Nan Yang, and Dongsheng Li. 2019. Read + verify: Machine reading comprehension with unanswerable questions. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, page 6529–6537.

Amita Kamath, Robin Jia, and Percy Liang. 2020. Selective question answering under domain shift. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5684– 5696.

Omer Levy, Minjoon Seo, Eunsol Choi, and Luke Zettlemoyer. 2017. Zero-shot relation extraction via reading comprehension. In Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017), pages 333–342.

Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach.

Qing Lyu, Hongming Zhang, Elior Sulem, and Dan Roth. 2021. Zero-shot event extraction via transfer learning: Challenges and insights. In ACL 2021, page 322–332.

Pranav Rajpurkar, Robin Jia, and D. Percy Liang. 2018. Know what you don’t know: Unanswerable questions for SQuAD. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 784–789.

Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383–2392.

Chuanqi Tan, Furu Wei, Qingyu Zhou, Nan Yang, Weifeng Lv, and Ming Zhou. 2018. I know there is no answer: Modeling answer validation for machine reading comprehension. In Natural Language Processing and Chinese Computing, pages 85–97.

E. Voorhees. 2002. Overview of the TREC-2002 question answering track. In The Eleventh TREC Conference.
4548

