Why don’t people use character-level machine translation?
Jindrˇich Libovický1 and Helmut Schmid2 and Alexander Fraser2 1 Faculty of Mathematics and Physics, Charles Univeristy, Prague, Czech Republic
2 Center for Information and Speech Processing, LMU Munich, Germany libovicky@ufal.mff.cuni.cz {schmid, fraser}@cis.lmu.de

arXiv:2110.08191v2 [cs.CL] 27 Apr 2022

Abstract
We present a literature and empirical survey that critically assesses the state of the art in character-level modeling for machine translation (MT). Despite evidence in the literature that character-level systems are comparable with subword systems, they are virtually never used in competitive setups in WMT competitions. We empirically show that even with recent modeling innovations in characterlevel natural language processing, characterlevel MT systems still struggle to match their subword-based counterparts. Character-level MT systems show neither better domain robustness, nor better morphological generalization, despite being often so motivated. However, we are able to show robustness towards source side noise and that translation quality does not degrade with increasing beam size at decoding time.
1 Introduction
The progress in natural language processing (NLP) brought by deep learning is often narrated as removing assumptions about the input data and letting the models learn everything end-to-end. One of the assumptions about input data that seems to resist this trend is (at least partially) linguistically motivated segmentation of input data in machine translation (MT) and NLP in general.
For NMT, several papers have claimed parity of character-based methods with subword models, highlighting advantageous features of such systems. Very recent examples include Gao et al. (2020); Banar et al. (2020); Li et al. (2021). Despite this, character-level methods are rarely used as strong baselines in research papers and shared task submissions, suggesting that character-level models might have drawbacks that are not sufﬁciently addressed in the literature.
In this paper, we examine what the state of the art in character-level MT really is. We survey existing methods and conduct a meta-analysis of the

input segmentation methods used in WMT shared task submissions. We then systematically compare the most recent character-processing architectures, some of them taken from general NLP research and used for the ﬁrst time in MT. Further, we propose an alternative two-step decoder architecture that unlike standard decoders does not suffer from a slow-down due to the length of character sequences. Following the recent ﬁndings on MT decoding, we evaluate different decoding strategies in the character-level context.
Many previous studies on character-level MT drew their conclusions from experiments on rather small datasets and focused only on quantitatively assessed translation quality without further analysis. To compensate for this, we revisit and systematically evaluate the state-of-the-art approaches to character-level neural MT and identify their major strengths and weaknesses on large datasets.
2 Character-Level Neural MT
Character-level processing was hardly possible within the statistical MT paradigm that assumed the existence of phrases consisting of semantically rich tokens that roughly correspond to words. Neural sequence-to-sequence models (Sutskever et al., 2014; Bahdanau et al., 2015; Vaswani et al., 2017) do not explicitly work with this assumption. In theory, they can learn to transform any sequence into any sequence.
The original sequence-to-sequence models used word-based vocabularies of a limited size and which led to a relatively frequent occurrence of out-of-vocabulary tokens. A typical solution to that problem is subword segmentation (Sennrich et al., 2016; Kudo and Richardson, 2018), which keeps frequent tokens intact and splits less frequent ones into smaller units.
Modeling language on the character level is attractive because it can help overcome several problems of subword models. One-hot representations

ofMwoodrdelsinogr lsaunbgwuoargdes odno tnhoet crehﬂareaccttesryslteevmelatiisc acthtararacctitveer-lbevecelauresleatiiot ncsabnethweelpenowveorrcdos,mpeotseenvtiearlalyl phraormblienmgsmoofrpshuoblwogoircdalmlyordicehlsl.anOgnuea-gheost. rWepitrhesseunb-twatoirodns,omf winoorrdtsyoprossuobnwtohredssoduorcneostidreeﬂleecatdstyostreamdi-actailclychdaifrfaecrteenrt-lienvpeultsriempirleasreitniteastiboentswreeesnulwtinogrdisn, lpoowtreonbtuiasltlnyehssartmowinagrdmsosropuhrocleo-gsiicdaellnyoriisceh (lPanrogvuialgkeosv. Wetiathl.,s2u0b2w0o; rLdisb,omviicnkoýratnydpoFsraosner,th2e02s0o)u.rce side leaMd otoderlsaduiscianlglyredcuifrfreernetnnt eiunrpaul tnertewporerksesn(tRatNioNnss) rsehsouwlteindgeairnlyloswuccreosbsuwstinthescshatorawcaterdr-slesvoeul rsceeg-msiednentaotiisoen(Pornovtihlekodveectoadle.,r2s0i2d0e; (LCibhouvnigckeýt aanld., F2r0a1se6r),. 2U0s2i0n)g. character-level processing on the encoder sidMe opdroevlsedushianrgderercwuhrriecnhtwneaus raatltrnibeutwteodrktos t(hReNfNea)stuhroewseodfetahrelyastutecncteisosnwmitehcchhaanriascmtewr-lheivcehl sceagnmperne-tsautmioanbloynbtehneeﬁdtefcroodmersesmidaent(iCcahlulyngriceht uanl.i,ts2(0s1u6c)h. Uassisnugbwchoarrdasc)teinr-ltehveeelnpcroodceers.siFnoglloonwitnhge tehnicsoldineer soifdethpinrokvinedg,hLaredeeertwahl.ic(h20w1a7s) aitnttrriobudtuecdedto1tDhecfoean-tvuorleustioofnsthwe iathttemntaixo-npomoelcinhgantihsamt pwreh-ipchroccaenssptrheescuhmaraabcltyerbseenqeuﬁetnfcroeminstoema asnetqiucaelnlyceriocfhlautneintst (wsuocrdhaliskesusbtawteosr.dCs)oiunpltehde wenitchoadecrh.arFaocltleorw-leinvgeltdheiscoldineer, othfetyhicnlakiimnge,dLtoeemeattcahl. t(h2e0s1t7a)tei-notfr-othdeu-caertds1uDbwcoornd-vboalsuetdiomnosdwelist.hEmveanx-tphoooulgihngthtihsaatrcphriet-epcrtourceeswsotrhkes cwhealrlacotnerthseeqcuhenarcaecitnetrolaevseelq,uietndcoeeosf nlaottengtewneorradl-liizkee fsutarttheesrthtoat twheasbuysteedleavselan(Cinopstuat-jtuossthàeesttaanl.-, d2a0r1d7)e.nHcoydberird. Capopurpolaecdhewsicthoma bcihnainragcttoekr-elnevizealtidoencinotdoewr,otrhdesywciltahimtheedcotompmuatatctihonthoef scthaatrea-cotfe-rt-hbea-saerdt swuobrwdorredp-rbeasseendtamtioodneslws. eErevesnucthcoesusgfhultlhyisusaercdhwiteitchtRuNreNwso(rLkusownegllaonnd tMheanchnainragc, t2e0r1le6v;eGl rföonr rEouorsoepteaaln., l2a0n1g7u;agAetsa,mitadnoeest naol.t,g2e0n1e9ra)l.izLe afuterrt,heCrhtoertrhye ebtyatel. l(e2v0e1l8()Coshstoaw-jeudssàtheattaRl.,N2N01s7p).erHfoyrbmridoanpppraoracwhieths csoumbwboinridnmg toodkeelnsiwzaittihoonutincthoawngoirndgs tfhoellomwoeddelbayrcthheicteocmtupruetiafttihoenmofodcehlasraarcetesru-fbﬁacsieedntlwyolardrgree.pKrereseuntzteartainodnsSwokeorelosvuc(2c0es1s8f)uslluypupsoerdt twhiisthbyRNshNows i(nLguothnagt aRnNdNMmanondienlgs,w2h0i1c6h; lGearrönnrsoeogsmeetnatal.t,io2n01jo7i;nAtltyamwiatnh etht ealr.e,s2t 0o1f9th).e Lmaotedre,l Carheecrrlyoseettaol.ch(2a0ra1c8te)rs-hleovwele.d thaCt hwairtahctReNr-lNevse, lsmufoﬁdceielinntglywliatrhgTe rmanosdfoelrsmdeorsnaoptnpeeadrsatrochbietemctourreedmifoﬁdciuﬁlct.aGtiounptaanedt pael.rf(o2r0m19o)nupseadr wTriathnspuabrwenotrdAmtteondteiolsn. (Bapna et al., 2018) to train deCephacrhaacrtaecr-tleerv-leelvmelomdeolidneglswainthdTnreaendsefodrumpertso a3p2plaeyaersrstotobcelomsoertehdeifgﬁacpublte.tGwuepentatehteaBl.P(E20a1n9d) uchseadrTacratenrspmaroednetlAs,ttwenhtiicohn m(Baakpensatehteaml.,o2d0e1l8t)otoo ltarargine dfoerepprcahcatriaccatleru-slee.velLmiboodveiclskýanadnndeeFdreadseur p(2to02302) lnaayrerroswtoedcltohsee gthaep gbaeptwbeetewneseunbtwheorBdPaEndancdhcahraacratecrtmeromdeoldineglsu. sLinibgocvuicrkriýcualnudmFlreaasrenri(n2g02b0y)ﬁnnaertruonwiendg tshuebwgaoprdbemtwodeeelns stoubcwhaorradcatenrd-lecvhealr.acter modeling usiGngaocuertriaclu. l(u2m02l0e)arpnrionpgobseydﬁandedtuinnginag cthoenvsoulbuwtionrdalmsoudbe-lasyteorcihnatrhaectTer-alnevsfeol.rmer layers. At the coGstaoof eat 3a0l.%(2i0n2c0re)apsreopinospeadraamddetienrgcaoucnotn,vtohleuytmioannaalgseudbt-olanyaerrroinwththeeTgraapnsbfeotrwmeeernlsauybewrso.rAd-t athned ccohsatraocftear-3b0a%sedinmcroedaesles obfymhaoldf.elBpaanraarmeet taelr. (c2o0u2n0t), trheeuysemdatnhaegceodntvoonluatriroonwalthperegparpocbeestswineegnlasyuebrwworitdhacnodnscthanart-ascitzeer-sbeagsmedenmtsodoeflsLebey ehtalafl.. B(2a0n1a7r)eitnaal.

Research papers
WMT System Description papers
2016 2017 2018 2019 2020 2021 Transformer BERT
• RNN MT • Transformer MT ◦ Transformer repr.
Figure 1: A timeline of research interest in characterlevel MT. Months of arXiv pre-print publication of rtehseeparacpherasncditesdysitnemSecdteisocnrsip2tiaondp3a.pTerrsancsiftoedrmienr Sreepcr-. tmioenasns2parned-tr3a.ineTdragnesnfoeraml-eprurreppors.erseefenrtsentcoeprreep-rteraseinnetadgtieonne,rnaol-tpMurTpomseodsenlst.ence representation, not MT models.
Transformer model for translation into English. (W20it2h0o)urtecuhsaendgtihnegctohnevdoelucotidoenra,ltphreeyprreoaccehsesdincgolmaypearwraibthle,cbountstuasnutalsltyepslisgehgtmlyewntosrsoef, tLraenesleattiaoln. (q2u0a1li7ty) icnoma pTarraendstfoorBmPeEr-bmaosdedelmfodr etlrsa.nslation into EnglisShh.ahTahmeyamndadLeenvoy c(2h0an2g1eas) rtoevtihseiteddeccohdaerracatnedrraenadchbeydtec-loemveplaMraTbloe,nursauthaellryssmliaglhl tIlWy wSLoTrsde,atarasentss-. lTahtieoinr qreusauliltys asshothwe tBhPaEt -cbhaasreadctmero-ldeevles.l and bytelevSehlamhaomdelasnadreLuevsuya(ll2y02w1oar)seretvhiasniteBdPcEhmaroadcteelrs-, abnudt byte-lbeavseeldMmToodnelrsawthietrhsomutaellmIWbeSdLdTindgaltaysetrss. Tohfteeinr roeustupletsrfsohromwBthPaEt -abltahsoeudgmh ochdaerlascitnert-hlevoeulta-nodfbEyntge-lileshvedlimreocdtieolsn.arUesuisnugalsliymwiloarslye tshmanalBl dPaEtamseotds-, eLlis,ebt yalt.e(-2b0a2se1d) cmlaoidmeltshawticthoaruatcetemr-blevdedlinmgoldaeyleinrgs ofuttepneorfuotrpmersfoBrmPEBwPEhe-bnatsreadnsmlaotdinegls iinntothfeuosuiot-noafl-, Eagngglluistihnadtiirveec,tiaonnd. inUtsroinﬂgexsivmeialarnlygusamgaelsl. datasets, Li Netiaklo.l(o2v021e)t claali.m (t2h0at1c8h)areaxctpeer-rliemvenl tmedodewliintgh ocuhtaprearcftoerm-lesvBelPEmowdheelsn tfroarnsrloamtinagniiznetdo fCuhsionneasel,. aTghgelsuetinmaotidvel,sanpderifnotrmoﬂeedxicvoemlapnagraubagleest.o models usiNnigkolologvogreatphailc. si(g2n0s1,8b)utesxipgenriiﬁmceantelyd wworitshe cthhaanramctoedr-ellesvueslinmgosdueblws ofrodrs. rZohmaanngizaendd KCohminaecshei. H(2o0w18ev) ear,gtuheedirthraetsusligtsnws ienrelocgoomgrpaaprhaibclelatnogusaignegs lcoagrorygrtaopohmicuscighnisnafonrdmsaigtinoinﬁcaanndtlwy ewreorasbeltehaton iumsipnrgovseubtwheortdrasn. sZlahtainogn aqnudalKitoymbaychsei g(2m0e1n8t)inagrgCuehditnheastesiagnnds Jinaploagnoegseraipnhtoicsluanbg-cuhaagreascctearruyntiotos wmhuiclhe iknefeoprimngatsioubnwaonrddwseegremaebnlteattionimonprtohveeEtnhgelitsrhanssidlae-. tionLiqttulealitiys bkynsoewgmn eanbtionugtCohtihnerseparnodpeJartpieasnesoef icnhtaorascutbe-rc-lheavrealcMterTubneiytsonwdhtihle okveerpailnlgtrasnusblwatoiordn sqeugamlietyn.tatSioennornicthhe(2E0n1g7li)shprseipdaer.ed a set of contraLstiitvtleeEnisgliksnho-Gwenrmaabnosuetntoetnhceer papirrospaenrdtietessteodf cthheamracutseirn-glevsheal lMloTwbReNyoNn-dbathseedomveordalellstr.aTnhsleaytioobnqseuravlietdy.thSatencnhraircahct(e2r-0b1a7s)edprmepoadreelds tarasnestliotefrcaotendtbreatstteivr,e bEuntgcliaspht-uGreedrmmanorspehnotesnycnetapcatiircs aagnrdeetemsteendt twhoemrseu.sinLgibsohvailclokwý RanNdNF-braasseedr m(2o0d2e0l)s.eTvhaleuyaotebdsTerravnesdfotrhmatecr-hbaarsaecdtecr-hbaarsaecdtemr-loedveells mtraondsellisteurastiendg bMetotrepr,hEbvuat lcaanpdtucraemd emtoormphixoesdyncotancctliucsiaognrse.ement woGrsuep. taLeitboavl.ic(k2ý01a9n)danFdraLseibro(v2i0c2k0ý)aenvdalFuraatseedr T(2r0an2s0f)omrmaekre-bcalasiemdscahbaoruact ttehre-lnevoeislemroobduesltsneusssinogf MtheorcphhaErvaacltearn-dlevcaeml me otodemlsixuesdincgonscylnutshioentisc. noise. LiGeut patla. (e2t0a2l1. )(2e0v1a9lu)aatendd dLoibmoaviinckrýobaunsdtnFesrsasbeyr (t2ra0i2n0in)gmmakoedecllsaiomnssmabaolul tdtohmeanioni-ssepercoibﬁucstdnaetsasseotfs tahnedcehvaarlaucatteinr-gletvheelmmoondeulnsreulsaitnegd dsyonmthaeintisc, cnloaiisme-.

iLngi etht easl.up(2e0ri2o1ri)tyevoaflcuhaateradctdeor-mleavienl mroobduesltsneinssthbisy steratuinpi.nOgnmtohdeeoltshoenr hsamnadl,lGdoumptaaient-sapl.e(c2iﬁ01c9d)aetvasaelutsaatnedd ethvealduoamtinaginthroebmusotnneusns rienlaatemdodreomnaatiunrsa,lcsleatiumpainngd tdhiedsnuopteoribosreitryveofhcihgahrearcrteorb-ulesvtnelemssowdehlesninevthailsusaettiunpg. gWeneearragludeotmhaatinthmisoidsealsvoenryduonmnaaitnu-rsaplesceitﬁucp taensdtsrcaothmepraerveadlutoatBe PthEe.domain robustness by evaluaAtinngotgheenrecroanl smidoedrealtsioonnidsolmonagine-rstpreaciniﬁincgteasntdseints-. fIenresuncche atimseetus.p,Cthhaerraecstuelrt-sleovfeGl usypstateemt asla. r(e20s1ig9n)idﬁocnaonttlsyhoslwowaecrledaureatdovtahnetaingcereoafscehdasreaqcuteernmceoldeenlginthg. LoivbeorvBicPkEýinanEdnFglriasshe-rto(-2G0e2r0m) arneptroarntesdlataio5n..6-fold slowDduoewton tahtetrianicnrienagsetidmseeqanudenac4e.l7e-nfogltdh,sclohwardaocwtenralet vineflesryesntceemtsimaerecsoimgnpiaﬁrceadnttolysusblowwoerrd. mLiobdoevlsic. ký anRdeFcreansterre(s2e0a2rc0h) roenpocrtheadraact5e.r6-l×evsellowmdoodwelninagt gtroaeisnibnegyotinmdeManTd. aPr4e.-7tr×ainsleodwmdouwltinlinagt uinaflerreepnrceesteimnteatcioonmspaarreeda ptoarstuicbuwlaorrldy macotidveelsa.rea. Clark et al. (20R21e)cepnrot proesseeaCrcAhNoINnEc.hTahraecmteor-dleelvsehl rimnkosdeclhianrgagcoteesr sbeeqyuoenndceMs Tin.toPfreew-terraihniedddemnuslttaitleinsg(usaiml irleaprtroesLeenetaettioanl.s, a2r0e1a7)p. aTrthiecyuluasrleyloacctailvseealfr-eaat.tenCtliaornk aentdasl.tr(id2e0d21c)onpvroolpuotisoenssC(iAnsNteIaNdEo,f ahnigharwcahyitelacytuerrse afnodr smharixn-kpionoglicnhgaarascitnerLseeeq’suewnocreks).inTthoeliersms ohdidedl eisn esittahteerstr(asiinmeidlaurstinogLtheee metaaskl.e,d2-0la1n7g)u.agTeh-emyoudseelinlogocbajlescetlivf-eat(tDenevtiloinn eatnadl.s,t2ri0d1e9d)cwointvhosluubtiwonosrdinssutpeeardvoifsihoing,howraiyn laanyeernscaondderm-daexc-opdoeorlsinegtup(assiminilLaer et’os Rwaofrfke)l.etTahle.i(r2m02o0d)e.l Bisottrhaimneedthuosdisngreathceh ma areskpered-sleanntgautiaogne-qmuoadlietylincgomobpjaercatibvlee (tDo esvimlinilaert aslu.,b2w0o1r9d) mwoitdheslus.bword supervision or in an encoder-decoder setBuypTs5im(XilauretoetTa5l.,(R20a2ff1eal)eat nadl.,C2h0a2r0f)o,rwmietrh(bToatyh erteaaclh.,in2g02a1s)imarilearbraesperdesoennttahtieonmqTu5almityodtoelsi(mXiulaer emt oald.,el2s0b2u1iblt) ownhsicuhbwusoersdss.equence-to-sequence denoiBsiynTg5pr(eX-tureaientinagl..,W20h2e1reaa)sabnydTC5hoanrflyorumseesr b(Tyatey seetqaule.,nc2e0s21in)sateraedboafsesdubownotrhdes manTd5dmiffoedreslin(Xhuyepeet rapla.,r2a0m2e1tbe)rsw, hCihcharufoserms seerquuseenscec-otonv-soelquutieonnceanddecnoomisbiningepsrceh-tarraaicntienrgb. lWochkesrteoasobbtyaTin5laotnelnytussuebswboyrtde rseepqrueseenncteastioinnsst.eTadheosfe smuobdweolsrdmsoasntldy dreifafcehrssiimnihlayrrpeesruplatsratmo estuebr-sw, Corhdarmfoordmeelsr,uoscecsacsoionnvaollluytioountpaenrdfcoormmbiningesa cfhewaraocftethr ebmlo,ckins tthoeocbatasienolaftCenhtasrufobrwmoerdr wreipthreosuetnatastiigonnisﬁ. cTahnetsselomwoddoewlsnm. ostly reach similar
3rfeosrumWltisnMgtosTosmusbue-bwomfotrihdsesmmioo,ndisneltsh,eoccacsaesioofnCalhlyarofourtmpeerr-
TwhiethCouotnafesriegnnciﬁecoanntMslaocwhdinoewTn.ranslation (WMT) organizes annual shared tasks in various use cases
o3f MWT. MThTe sshuabremd itsassikosnusbmissions focus on trans-
lTathieonCqounafelirteynrcaethoenr tMhaanchthineenTorvaenltsylaotifopnre(WsenMteTd) iodregaasn,izaessmanonstuaolthshearrreedsetaasrkcshipnavpaerrisoudsou. seTchaesreesfoofreM, Tw. eThaesssuhmareedthtaast,k isfubcmhairsasciotenrs-lfeovceuls menotdireellsy woneretraanfusllalyti-oﬂnedqgueadliatlyterrantahteivretthoansutbhweonrdovmeoltdyelosf, atht elepasret sseonmteedsyidseteams,sassumbmositttoedthteor trheesesahracrhedpatapsekrss wdoo.ulTdhuesreefcohraer,awcteera-lsesvueml me tohdaetl,si.f character-level moWdeelasnwneorteataedfurlelyc-eﬂnetdsgyesdteamltedrensactirvipettioonsupbawpeorrsd wmiothdetlhse, aintpluetasatnsdomouetpsuytstseemgms esunbtamtiiotntedmteothtohde tshheayreudsetdas. kWs ewfooucludseudseocnhianrfaocrtmera-tlieovnelabmooudteelxs.per-

Vocabulary size

60k

40k

20k

0 2018

2019

2020

FFiigguurree 22:: AA bbooxxpplloott ooff vvooccaabbuullaarryy ssiizzeess ooff WWMMTT ssyyss-tteemmss ffrroomm 22001188––22002200,, tthhee mmeeddiiaann iiss ddeennootteedd wwiitthh tthhee oorraannggee lliinnee..

imeWnetsawnintohtactheadraacltlerre-lceevnetlsmysotdeemlsd. eSsicnrcieptwioenapraeppreirmsawriiltyhiwnthearet sitnepduitnatnhde oTuratpnustfosremgmereanrtcahtiotenctuhreey tuhsaetdb.ecWame efotchuessetdanodnaridnfaofrtmera2ti0o1n7a, bwoeutonexlypeinri-cmluednetds wsyistthemchdareasctreipr-tiloevneplampeordseflrso.mSi2n0c1e8w–2e0a2r0e (pBriomjaarrielyt ianlt.e, r2e0st1e8d;inBtahreraTurlatnestfoarlm.,e2r0a1rc9h,it2e0c2tu0r)e. Tthrant sbfeocrmameres twhersetaunseddaridn a8f1t%er,28071%7, awned o9n7l%y oinftchleudseydstseymstsemin dtheescrreipsptieocntipvaepyeersarfsro. mW2e0i1n8c–lu2d0e2d0 t(hBeomjaarinettaaslk., o2n01W8M; BTa, rnreawulst terat nasll.a, ti2o0n1,9a,n2d0t2w0o). mTriannosrfotarsmkesrws wheerree cuhsaerdacinte8r-1l%ev,e8l7m%e,thaonds9m7%ighotf htheelps:ytsrtaenmslsatiinonthreorbeussptencetsisve(Lyieeatrsa.l.W, 2e0i1n9c;luSdpeedcthiae emt aailn.,ta2s0k20on) aWndMtTr,anselwatsiotrnanbseltawtieoenn, asnimd tiwlaor lmaningouragtaessk(sibwidh.e)r.e character-level modeling might beAulsmeofustl:altlrasnysltaemtiosnusroebaussutnbewsosrd(L-biaestedalv.,o2c0ab1u9-; lSapryec(iBaPeEt :al8.,12%0,2701)%an,d66tr%ansinlathioenrebseptweceteivnesyimeairlas;r SlaenngteunacgeePs.iece: None in 2018, 9% and 25% in the follAowlminogstoanlelss)y. sPteumreslyuwseosrdu-bbwasoerd-(bnaosneedivno2c0a1b8u,2la%rya(nBdPE3%: 81in%t,h7e1%lat,e6r6y%eairns)thoerremsoperpchtioveloygeicarasl; sSeegnmteennctaetPioienc(e4:%N,o2n%e ,in3%20i1n8,th9e%reasnpdec2ti5v%e yienatrhse) afroellorawreinlyg ounseds).. TPuhreelayvewroargde-bvaosceadb(unlaornye siniz2e0d1e8-, c2r%easaensdov3e%r timn eth(eseleatFeirguyreear2s))woirthmaomrpehdoialongsiiczael rseemgmaiennintagtioant 3(42%k ,in2%th,e3%lasitntwthoe ryeesapresc.tivTeheyeraeras-) saorne froaretlhye udseecdre.aTsihneg average ivsopcraobbualbalryyashizieghdeerpcroepasoerstiovneorftismysete(smees Ffoigr ulorew2-r)ewsoituhrcaemlaendgiaunagsiezse, wrehmeraeinainsmg altle3r2vkoicnabtuhlearlyaslteatwdsotyoebaertst.erTtrhaensrelaa-tsion bqeuhailnitdyd(eScernenarsiicnhg aanvderZagheanisg,p2ro0b1a9b)l.y a higher proApmorotniogntohfes1y4st5emansnfootraltoewd -sryestoeumrcedelascnrgiupatigoens pwahpeerres,smthaelrlerwvoecreabounlalyrytlweaodsthtaotbuestteedr tcrhanarsalacttieornlqeuvaelitsyeg(Smeennntraitcihona.ndMZahhaantga,e2t0a1l9.)(.2018) used a chaAramctoenr-gletvheel m14o5dealnfnoortFaitnednisshy-sttoe-mEndgelisschritpratinosnlpaatipoenr.s,Tthhiesresywsteerme ,ohnolywtewveor,thmaat kuessedmaancyhasruabcotepr-tliemveall dseegsimgnencthaotiiocne.s aMndaheantadeedt uapl. a(s20th1e8)laustseodnea icnhathraecmtera-nleuvaell emvaolduealtfioonr .FiSnnchisehr-rteor-Eent galli.sh(2t0ra1n9s)elxatpieorni.mTehnitsedsywsittehmchhaorwacetveerr-lmevaeklessysmteamnys ufonrussiuma-l ialanrdlasnugbuoapgtiemtraalndselastiigonn cahnodiocebssearnvdedetnhdaetdchuapraacsttehres olausttpoernfeorimn tohtehemr asengumaleenvtaatliuoantsiofno.r SSpcahneirsrherPeot ratul.gu(2e0se19tr)aenxslpaetrioimn,enbtuetdnowtitfhorcChazreaccht-ePr-olleivsehl. Ksynsotwemless feotr asli.m(i2l0ar20la)negxupaegreimtreanntseldatwiointhanddiffoebr-esnetrvseudbwthoartdcvhoacraabcutelarsryosuitzpeesrffoorrmEnogthlieshr -sIengumkteinkutatttriaonnsslaftoiornSapnadnirseha-cPhoerdtuthgeuebseesttrreasnuslltastiuosnin, gbuatsnubo-t wCozredchv-oPcoabliuslha.ry of size 1k, which makes it close to theKcnhoarwalcetserelteavle.l(.2M02o0s)t oexf ptheerimpaepnetresddwointhotdeifvfeenr-

mention character-level segmentation as a viable alternative they would like to pursue in future work (7% in 2018, 2% in 2019, none in 2020).
Character-level methods were more frequently used in WMT17 with RNN-based systems, especially for translation of Finnish (Escolano et al., 2017; Östling et al., 2017) and less successfully for Chinese (Holtz et al., 2017) and the automatic post-editing task (Variš and Bojar, 2017).
On the other hand, Figure 1 shows that the research interest in character-level methods remains approximately the same, or may have slightly increased. For practical solutions in WMT systems, we clearly show that system designers in the WMT community have avoided character-level models.
We speculate that the main reasons for not considering character-level modeling are its lower efﬁciency and the fact that the literature shows no clear improvement of translation quality. Most of the submissions use back-translation (85%, 82%, and 94% in the respective years), often iterated several times (11%, 20%, 16%), which requires both training and inference on large datasets. With the approximately 5-fold slowdown, WMT-scale experiments on character models are not easily tractable.
4 Evaluated Models
We evaluate several Transformer-based architectures for character-level MT. A major issue with character-level sequence processing is the sequence length and low information density compared to subword sequences. Architectures for characterlevel sequence processing typically address this issue by locally processing and shrinking the sequences into latent word-like units. In our experiments, we explore several ways to do this.
First, we directly use character embeddings as input to the Transformer. Second, following Banar et al. (2020), we use the convolutional character processing layers proposed by Lee et al. (2017). Third, we replace the convolutions with local selfattention as proposed in the CANINE model (Clark et al., 2021). Finally, we use the recently proposed Charformer architecture (Tay et al., 2021).
Lee-style encoding. Lee et al. (2017) process the sequence of character embeddings with convolutions of different kernel sizes and number of output channels. In the original paper, this was followed by 4 highway layers (Srivastava et al., 2015). In our preliminary experiments, we observed that a too deep stack of highway layers leads to diminishing

gradients, and we replaced the second two Highway layers with feedforward sublayers as used in the Transformer architecture (Vaswani et al., 2017).
CANINE. Clark et al. (2021) experiment with character-level pre-trained sentence representations. The character-processing architecture is in principle similar to Lee et al. (2017) but uses more modern building blocks. Character embeddings are processed by a Transformer layer with local selfattention which only allows the states to attend to states in their neighborhood. This is followed by downsampling using strided convolution.
Originally, CANINE used a local self-attention span as long as 128 characters. In the case of MT, this would usually span the entire sentence, so we use signiﬁcantly shorter spans.
Charformer. Unlike previous approaches, Charformer (Tay et al., 2021) does not apply a nonlinearity on the embeddings and gets latent subword representations by repeated averaging of character embeddings. First, it processes the sequence using a 1D convolution, so the states are aware of their mutual local positions in local neighborhoods. Second, non-overlapping character n-grams of length up to N are represented by averages of the respective character embeddings. This means that for each character, there is a vector that represents the character as a member of n-grams of length 1 to N . In the third step, the character blocks are scored with a scoring function (a linear transformation), which can be interpreted as attention over the N different n-gram lengths. The attention scores are used to compute a weighted average over the n-gram representations. Finally, the sequence is downsampled using mean-pooling with window size and stride size N (i.e., the maximum n-gram size).
Whereas Lee-style encoding allows using lowdimensional character embeddings and keeps most parameters in the convolutional layers, CANINE and Charformer need the character representation to have the same dimension as the following Transformer layer stack.
Two-step decoding. The architectures mentioned above allow the Transformer layers to operate more efﬁciently with a shorter and more information-dense sequence of states. However, while decoding, we need to generate the target character sequence in the original length, by outputting a block of characters in each decoding step. Our

The_cat_sleeps
character embedding processing downsampling

#####Die_Ka
char. proc. downsampling

Transformer encoder

Transformer decoder

Lightweight LSTM decoder #

Die

_Ka

FFigiguurere33: :EEnnccooddeer-rd-deeccooddeerraarcrchhitietecctuturerewwitihthcchhaararaccteter-rpprorocceessssiningg llaayyeerrss bayndanad atwtow-sot-esptepdedceocdoedrer wwitihth lilgighhtwtweeigighht tLLSSTTMMfoforroouutptpuut tccoohheererennccee. .

parcetleimr sienqauryenecxepienritmheeonrtisgsinhaolwleendgtthh,atbygeonuetprauttitningg balobclkosckofocfhcahraacratecrtsernsoinn-aeuatcohredgerceossdiivneglystleepad. sOtour inpcreolhimeriennatryouetxppuet.riTmheenrtesfoshreo,wweed pthroapt ogseeneartawtion-g sbtelpocdkescoodficnhgaarracchteitrescntuorne-wauhteorreegthreessstiavceklyofleTardasnst-o foinrcmoehrelraeynetros uotppeurta. tiTnhgeorvefeor rteh,ewdeowprnospamospeleadtwseo-qsuteenpcdeecisofdoinllgowarecdhibteycatulrieghwthweeriegthhteLsStaTcMk oafuTtroarnes-gfroersmsivere ldaeyceorsdeorp(esreaetiFngigouvreer3t)h. e downsampled sequence is followed by a lightweight LSTM autoregrTehsseivinepduetctoodtehre(sLeSeTFMigudreec3o)d.er is a concatenationTohfethinepeumt btoedtdhiengligohfttwheeipgrhetvdioeucsoldyegreinsearacteodncchaatreancatteironanodf tahpereomjebcetidodninogfothf ethTerparnesvfioorumsleyr gdeen-ceordaeterdoucthpaurtascttaetre.anAdt ianfperroejneccetitoinmeo,fththeeLTSTraMnsdfeocromdeerr dgeecnoedraetreosuatpbulot cstkatoef. cAhtarinafceterersncaendtiminep,utthse thliegmhtwtoeitghhet cdheacroadcetrerg-elenveeraltpesroacbelsoscinkgoflacyhearr.acTtheres, Twrahnicsfhoarmreetrhednecpordoevridceodmapsuatnesinapnuot utotptuhtescthaateratchtaetrthleeveLlSpTroMcedsseicnogdlearyuers,easntdotgheenTerraantsefoarnmotehredrecchoadre-r awctheircbhlcoocmk.pMutoesreandeotuatiplsuat rsetaitne Athpapt ethnedliixghAt.weight LSMToMdifdyeicnogdCerhaursfeosrmtoegrefnoerrtahtee tawnoot-hsteerpbdloecckodo-f incghawraocuteldrsr.eMquoirree adeltoanilgspaareddininAg papt ethnedibxeAgi.nning of thFeirsset,quweencceocnaduuscintgatlhl eoduerceoxdperertiomdeivnetsrgoen. Bthe-e csamusaelloIfWthSatL,TwedautsaeseLtes.e-sSteycleonednc, owdeinegvoanlutahtee dthe-e cmodoesrt psirdoemwishinegn aurscihnigteCcthuarrefocormmebrininatitohneseonncoladregre. r daFtiarssett,s.we conduct all our experiments on the small IWSLT datasets. Then we evaluate the most
p5romEisxinpgearricmhietnecttsuorens oSnmlarlgl eDr adtaatasets.
We implement the models using Huggingface
5TraEnsxfporemriemrse(nWtsoolfneSt mala.,ll2D02a0t)a. We take the
CANINE layer from Huggingface Transformers Wanedimuspeleamn einndt etpheendmenotdeimlspluesminegntaHtiuogngionfgCfahcaerTfroarnmsfeorr1m. Herysp(eWrpoalrfaemt eatle.,rs20a2n0d)o. tWheer teaxkpeetrhime eCnAta-l NdIeNtaEilslaayreerinfrothme AHpupgegnindgixfaBc.e Transformers and use an independent implementation of Charformer1. O5u.1r soEuxrcpeerciomdenitsaalvSaeitluabple on Github.2 HyperpWareameveatleurasteanthdeomthoedreelsxpoenritmraennsltaatliodnetbaeiltswceaenn bEenfogulinshd oinnAonpepesniddeixanBd. German, French, and Arabic on the other side using the IWSLT 2017 datasets (C12ehhttttttpopssl::o////ggeiittthhauublb...,ccoo2mm0//1ljul7icb)idowrvaiicintkhsy/c/ahatrrfaoirnmienr-gpydtaortcahsize of
char-1nhmttpt-st:w//goi-tshtuepb.-cdoemco/lduecridrains/charformer-pytorch

5a.1rounEdx2p0e0rkimseenntteanlcSeestfuopr each language pair (see

WAepepveanlduiaxteBthfeormdoedtaeillss)o.n translation between En-

glisFhopratihreedswubitwh oGredrmmaond,eFlsr,enwceh,toaknednAizreabtihce(winipthut

Eunsginligshthase bMotohsiensputot kaenndizoeurtp(uKt)oueshingethaelI.,W2S0L0T7)

2a0n1d7thdeantafsuerttshe(Cr septtloitlothetwaol.r,d2s0in1t7o) swubitwhoardtruaninit-s

inugsindgatBa PsEize(Soefnnariocuhnedt 2al0.,02k0s1e6n)tewnicthes1f6okrmeaecrghe

laonpgeuratgioenpsa.irFo(sreteheAcphpaernadcitxerBmfodr edlest,awilse)l.imit the

voFcoarbtuhlearsyutbow3o0r0d UmToFd-e8lsc, hwaeratcotkeersn.ize the input

usinWgethueseMothses Ttorkanensfiozermr e(Kr oBehanseetaracl.h,it2e0c0tu7r)e

a(nVdatshweannfiuerthaelr.,sp2l0i1t 7th)einwoalrldsexinpteorismubewntosr.dWuneidtso

unsointgmBakPeEa(nSyenchnarincgheestinalt.h, e20s1u6b)wworidtha1n6dkbmaseerlginee

ocphearraaticotenrs.exFpoerrtihmeecnhtsa.raWcteeremploadceelst,hwe emlibmeidtdthineg

vloocoakbuuplawryitthot3h0e0chUaTraFc-t8ercphraorcaecstesrins.g architectures

inWthee luaster etxhpeerTimraennstfso. rFmoerrtheBLaseee-satryclheietencctoudrer, (Vwaescwhaonsie estimaill.a, r2h0y1p7e)rpianraamlleteexrpsearsimreelnates.d wWorek m(Bakaenanroecthaal.n,g2e0s2t0o).it in the subword and baseline

chaFraocrtexr peexrpimereimntsenwtsit.h ICnhtahrefolramteerr eaxnpdeCriAmNenINtsE,

wmeordeepllsa,cweethsetethmebheydpdeinrpgarloamokeuteprswsiuthchththeact hthaer-y

accotevrerprtohceesasimnge acrhcahriatcetcetrursepsa.nFboerfothre Ldoeew-nstsyalme -

epnlciondgear,s wtheecLheoes-estsyimleileanrchoydpeer,rpwahraicmhectearussaes rteh-e

lamteoddewlsortok h(Bavaenaferweteralp.a, r2a0m2e0t)e.rsFothraenxapeLreime-esntytsle

weinthcoCdhear.rfoNrmoteer ahnodwCevAeNr ItNhaEt mfoordbelost,hwteheseCt thhaer-

hfyopremrpearranmdettehres sCuAchNtIhNatEthmeyocdoevlse,r the snaummebcehraor-f

apcaterramspeatenrsbeisfoarlemdooswt innsdaempepnlidnegnat sofththeeLcehea-rsatyclter

ewncinodoewr, whidicthh. cFaourseasll tthhereme ocdhealrsacttoerhpavroecfeeswsienrg

paarrcahmiteetcetrusreths,anwea eLxepee-rsitmyleentenwciothdedro. wNnosatemhpoliwn-g

efvaecrtothrastofof r3baonthdth5e(Ca h1a6rkfoBrmPeEr avnodcathbeulCarAyNcIoNrrEe-

mspoodneldss, tthoeanudmowbenrsaomf palirnagmefatecrtos risoaflmaboosut tin4dei-n

pEengdleinsht )o.f the character window width. For all

three character processing architectures, we experim5e.2nt wTirthandsolwatnisoanmQpluinaglitfyactors of 3 and 5 (a 16k

BWPeEevvoalcuaabtueltahreytcraonrsrleastpioonndqsuatolitay udsoiwngnsthame BplLinEgU fasccotorreof(Paabpoiunte4niineEt nagll.i,sh2)0. 02), the chrF score

5(P.P2oospt,oTv2ri0ac´n1,8s2l)a0, t1ia5on)nd(QatshueiamlCiptOyleMmeEnTtesdcionreSa(cRreeiBeLtEaUl.;,

W2e02ev0a).luaWteethruentraenasclhateioxnpeqruiamlietyntu4sintigmthees BanLdEUre-

spcorteth(ePampeinanenvialeute anl.d, s2t0an0d2a),rdtdhevicahtiroFn.score

(PoTpohveirc´e,s2u0lt1s5a)r(eapsriemsepnletemdeinteTdabinleS1a.cErexBceLpEt Ufo;r Ptorasnt,sl2a0ti1o8n)i,n3toanAdratbhiec C(wOhMichETis csocnosreist(eRnet iweithatlh.,e 2ﬁ0n2d0i)n. gWs eofruSnheaahcahmeaxnpderLimeveyn,t 240t2im1aesanadndLrieeptoarlt., th2e02m1e),anwhvaelrueechanardasctaenr dmaerdthdoedvsiaotuiotpne.rform BPEs,

suTbhweordesmulettshoadres aprreesaelwntaeyds binetTtearbtlhean1.chEarxacceteprts.

for TtrhaenscloatmiopnariinstoonAorfatbhiec,cwhahrearcetecrhapraoccteesrsminegtha-rocdhsitoeuctpuererfsosrhmowBsPtEhsat(wthheicLheeis-sctyolneseisntceondt ewriothuttpheerﬁfnodrminsgsthoeftwShoamhaomre arencdeLntemvye,th2o0d2s1anadntdheLimeetthalo.d, 2o0f2u1s)i,nsgutbhweocrhdarmacetehroedms baereddailnwgasydsirbeectttleyr. Cthhaanrcfhoarrmacetrerpse.rforms similarly to using character em-

be3dBdLiEnUgs

directly, CANINE is signiﬁcantly worse.
score signature nrefs:1|case:mixed|

eTfhfe:nroe|sutoltks:a1r3eam|somsotloythc:oenxspis|tvenetrsaicorons:s2.th0e.0lan-

cghruFasgceorpe asiigrnsa.ture nrefs:1|case:mixed|eff:yes|

nc:I6n|cnrwea:s0i|nsgpathcee:dnoow|nvsearmsipolinn:g2.fr0o.m0 3 to 5 de-

Model

Lee-style

Enc. Dec.

downsample BPE 16k Vanilla char.

3

—

5

—

3

3

5

5

3

—

5

—

3

3

5

5

3

—

5

—

3

3

5

5

Char. proc. params
16516 658
9672 9672 9646 9646
1320 1320 1165 1165
6446 7470 6291 7444

BLEU
11.2 ±0.2
13.5 ±0.4
13.1
±0.5
12.5 ±0.1
11.0 ±0.2 9.4 ±0.5
13.3 ±0.3
12.2 ±0.3
10.3 ±0.5 8.4 ±0.2
12.6 ±0.3
11.2 ±0.2
10.3 ±0.5 6.9 ±0.4

ar
chrF
.436
±.002
.447
±.004
.448
±.002
.439
±.002
.432
±.002
.418
±.003
.448
±.002
.435
±.002
.431
±.004
.402
±.003
.440
±.002
.421
±.001
.425
±.004
.373
±.007

COMET
.258
±.011
.267
±.016
.274
±.009
.245
±.013
.143
±.013
.006
±.015
.261
±.011
.179
±.020
.000
±.000
-.121 ±.023
.195
±.019
.045
±.005
.064
±.023
-.355 ±.041

From English

de

BLEU chrF COMET

27.7
±0.3
25.6 ±0.7

.555
±.002
.550
±.005

.254
±.005
.165
±.034

25.9 ±0.7
25.0
±0.4
23.4
±0.4
21.8
±0.3

.552
±.001
.545
±.002
.541
±.002
.524
±.002

.200
±.023
.140
±.013
.065
±.028
-.106 ±.021

25.9
±0.5
24.2
±0.6
23.2
±0.5
19.9 ±0.2

.550
±.004
.535
±.003
.540
±.004
.510
±.002

.167
±.026
.060
±.027
.037
±.034
-.250 ±.027

25.4
±0.5
22.5
±0.4
22.4
±0.3
19.1
±0.4

.547
±.002
.524
±.004
.534
±.003
.498
±.005

.121
±.024
-.095 ±.027
-.034 ±.023
-.417 ±.036

BLEU
36.4
±0.3
34.6 ±0.7
35.2
±0.4
33.2 ±0.1
31.7
±0.5
28.7
±1.7
32.9
±0.3
31.3
±0.4
30.6
±0.4
27.4 ±0.7
33.2
±0.6
30.5
±0.5
30.2
±0.5
27.9
±0.6

fr
chrF
.619
±.002
.611
±.002
.613
±.002
.602
±.003
.603
±.002
.584
±.011
.607
±.003
.591
±.003
.601
±.003
.575
±.005
.606
±.004
.584
±.004
.595
±.004
.540
±.001

COMET
.408
±.008
.350
±.020
.383
±.010
.303
±.017
.277
±.012
.094
±.096
.300
±.018
.171
±.026
.192
±.031
-.039 ±.029
.269
±.024
.273
±.029
.139
±.027
.078
±.019

BLEU
29.7
±0.2
27.7
±0.8
28.0
±0.4
24.9
±4.4
25.6
±0.3
23.7
±0.3
27.3
±0.5
25.1
±0.6
24.5
±0.4
18.4 ±3.1
26.1
±0.5
22.1
±0.6
23.7
±0.9
15.4 ±0.3

ar
chrF
.521
±.001
.518
±.006
.521
±.002
.491
±.042
.509
±.001
.492
±.001
.520
±.002
.500
±.002
.506
±.003
.448
±.029
.512
±.004
.477
±.001
.499
±.006
.413
±.003

COMET
.325
±.147
.238
±.034
.257
±.015
.090
±.228
.170
±.016
.033
±.015
.229
±.028
.103
±.022
.125
±.021
-.248 ±.173
.137
±.024
-.121 ±.023 .034 ±.030
-.499 ±.030

Into English

de

BLEU chrF COMET

31.6
±0.3
29.4 ±0.7

.554
±.001
.545
±.005

.379
±.008
.327
±.029

30.2
±0.5
28.9
±0.3
28.0
±0.3
25.5
±0.3

.551
±.003
.543
±.002
.537
±.002
.519
±.003

.345
±.022
.311
±.019
.262
±.019
.131
±.019

29.9
±0.3
28.1
±0.4
27.5
±0.5
23.5
±0.5

.548
±.001
.535
±.003
.538
±.003
.511
±.003

.327
±.008
.227
±.022
.225
±.021
.018
±.029

29.1
±0.4
27.3
±0.3
25.9
±1.0
23.2
±0.2

.546
±.002
.528
±.001
.527
±.008
.504
±.001

.273
±.020
.115
±.022
.127
±.043
-.082 ±.010

BLEU
36.2
±0.3
34.7
±0.4
35.3
±0.2
34.4
±0.3
33.3
±0.4
30.9
±0.5
35.1
±0.3
33.7
±0.2
32.6
±0.3
29.2 ±0.7
34.5
±0.4
32.5
±0.5
32.5
±0.3
27.9
±0.6

fr
chrF
.592
±.003
.585
±.003
.588
±.001
.583
±.002
.577
±.001
.561
±.004
.588
±.002
.577
±.002
.576
±.001
.552
±.002
.583
±.003
.566
±.004
.575
±.002
.540
±.001

COMET
.527
±.005
.487
±.012
.506
±.013
.483
±.016
.440
±.015
.335
±.018
.495
±.013
.428
±.012
.425
±.014
.228
±.035
.448
±.014
.273
±.029
.368
±.008
.078
±.019

Charformer

Canine

Table 1: Translation quality of the models on the IWSLT data. The fourth column shows the size of the characterprocessing layers expressed as the vocabulary size of Transformer Base having the same number of parameters in the embeddings.

The Lee-style encoder outperforms the two more recent methods and the method of using the character embeddings directly. Charformer performs similarly to using character embeddings directly, CANINE is signiﬁcantly worse. The results are mostly consistent across the language pairs.
Increasing the downsampling rate from 3 to 5 degrades the translation quality for all architectures. Employing the two-step decoder matches the decoding speed of subword models. However, the overall translation quality is much worse.
The three metrics that we use give consistent results in most cases. Often, relatively small differences in BLEU and chrF scores correspond to much bigger differences in the COMET score.
5.3 Inference
Inference algorithms for neural MT have been discussed extensively (Meister et al., 2020; Massarelli et al., 2020; Shi et al., 2020; Shaham and Levy, 2021b) for the subword models. Subword translation quality quickly degrades beyond a certain beam width unless heuristically deﬁned length normalization is applied.
As an alternative, Eikema and Aziz (2020) recently proposed Minimum Bayes Risk (MBR; Goel and Byrne 2000) estimation as an alternative. Assuming that similar sentences should be similarly probable, they propose repeatedly sampling from the model and selecting a sentence that is most sim-

ilar to other samples. With subword models, MBR performs comparably to beam search.
Intuitive arguments about the inference algorithms are often based on the properties of the subword output distribution. On average, character models will produce distributions with lower perplexity and thus likely suffer more from the exposure bias which might harm sampling from the model. Therefore, there is a risk that these empirical ﬁndings do not apply to character-level models.
We explore what decoding strategies are best suited for the character-level models. We compare the translation quality of beam search decoding with different degrees of length normalization.4 Further, we compare length-normalized beam search decoding with MBR (with 100 samples), greedy decoding, and random sampling. We use the chrF as a comparison metric which allows pre-computing the character n-grams and thus faster sentence pair comparison than the originally proposed METEOR (Denkowski and Lavie, 2011).
Figure 4 shows the translation quality of the selected models for different beam sizes. The dotted lines denoting the translation quality without length normalization show that the quality of the subword models quickly deteriorates without length normalization, whereas vanilla and Lee-style character-
4As we increase beam size, the number of search errors is decreasing, but here we are evaluating modeling errors, not search errors.

chrF

BBPPEE

--00..9944

0.55

CChhaarr ddiirreecctt 00..0044

LLeeee--ssttyyllee 00..0000

CChhaarrffoorrmmeerr --00..9911

0.54

CCAANNIINNEE 00..1155

1 2 3 4 5 7 10 20 40 80 Beam size
Figure 4: chrF scores for IWSLT en-de translation for different models and beam sizes. The dotted lines are without length normalization, the solid lines are with length normalization. All character processing architectures use a downsampling window of size 3. The legend tabulates the Pearson correlation of the beam size (asntadrtihnegcfhrroFmsc5o)raen. d the chrF score.

Model

Enc. Dec. Sample Greedy Beam MBR
METEOdoRwnmsaemtrpilce (Denkowski and Lavie, 2011). BFPigEu1r6ek4 shows th0e.4t8ra2nsl0a.t5i4o5n qu0.a5l5i5ty o0f.5th54e se-
lected models for d-i0f.f1e3r2ent0b.1e9a9m s0i.2z6e2s. T0.h1e87dottedValinnilelascdhearn. oting th0e.44tr8ans0l.a5t3i7on q0u.5a3l7ity 0w.5i3th8out length normalization-0s.4h4o6w th0.a1t1w7 he0r.e1a6s5 the0.q0u86ality olefntghtehsnuob3rwmoarldiz—matoiodne-00,l..sc34hq46a01uriacck00tl..ey51r34-d92leevteer00li..o52m50rao20tdeesl00sw..51di40to46honuott

Lee-style

seem to 5suffer —from 0th.4i5s4pro0b.5le2m7 . W0.5e4h5ypo0.t5h3e7size this is because of the-0s.3ig71niﬁc0a.0n8t2ly d0if.1fe4r0ent0s.i0z5e1s of

omuotdpeultsdl3iesatdribtout3simonasl-l00we..64rh53vi70cahlu-i00en..s05t12oh53ne acva00e..s50re46a05ogfe.s-00u..b5120w65ord Table 52 pres5ents -t00h..93e1985tran-00s..l24a59t95ion-00q..15u02a64lity-00f..53o04r01dif-
ferent decoding methods. In all cases, beam search is the bes3t strat—egy. -S10a..43m9005plin00g..50f36r01om00c..51h44a79rac-t00e..48r-43l81evel models leads to very0p.2o9o1r tra0n.5s1l5ation0.5q3u6alit0y.4th42at in turn also5inﬂue—nces-1th.4e80MB-0R.01d7eco0d.0in69g th-0a.t85le4ads tgoesmtsutchha3twcohrasrea3crteesru-ll-te10sv..72et22hl07amno-b00d..e44ea26lms42 lesae00ra..n50r43cc06ho.rrT-e01hc..40itsl19y20stuhge-

Charformer

mode

of

the
5

dis5tribut0io.2n1s5,

bu0t.4fa7i8l

to0l.e5a1r0n

go0.o3d71con-

ditional distribution-s1,.7s9o0th-e0.i7n3i3tial-0a.s2s3u0m-p1t.i2o9n0 be-

hind MB3R dec—oding0.t3h0a7t Ei0k.5e3m1a a0n.d54A7 ziz0.(4250620) made for the subwo-r1d.5m00ode0ls.0d51oes 0n.1o2t1ho-l0d.8o3n8 the

Canine

character5 level—.

0.301 0.509 0.524 0.440 -1.530 -0.163 -0.095 -0.968

Moreover, we found that MBR decoding per-

forms co3mpara3bly -w10..i62t85h03be-a00..m059176sea-r00c..05h3344in -t01e..r41m1330s of

chrF score for our best models, but gets much

worse C5OMET5 sco-r10e..72s51. 09Th-i00s..54r27a45ise-s00..44c19o78nce-01r..n33s7710that

the previous results of MBR decoding reaching par-

Titayblwe it2h: beachmrFsea(yrcelhlomwi-gghretehnavsecableee)n aanndarCtiOfaMctEoTf (ny-egllroawm-rbedasescdalme)etsrcicosr.es for decoding methods for models trained on en-de systems.

6 Experiments on WMT Data

lBevaesel dmoondetlhsedroesnuolttsseoefmthteo esuxfpfeerrimfroenmtsthwisitphrothbelIeWmS. LT data, we further experimented only with the LeTea-sbtlyele2epnrceosdeenrtsutshinegtraadnoslwatnisoanmqpulainligtyfafcotrordioff-

Model

ferent dEecnco.dinDgemc.ethSoadmsp.leInGarlelecdayseBse,abmeamMsBeRarch is the bedsotwsntsraamtepgley. Sampling from character-level models leads to very poor translation quality that in tBuPrEn 1a6lsko inﬂuenc-e00s..14t38h22e M00B..5149R59 dec00..o52d56i52ng 00le..51a58d47ing to mVauncilhlawchoarr.se result0s.4t4h8an b0e.5a3m7 se0a.r5c3h7. 0.538
Our experiments-0s.h4o46w th0.a1t17bea0m.16s5earc0h.08w6 ith lfeonrgcthhanroa3rcmtearl-ilze—avteiol nm-00ios..34dt46he01elsb. e00Ts..51th34ie92nyfear00les..n52o50c20eseaelgm00o..51rt40io46thbme

Lee-style

more resi5lient t—oward0s.4t5h4e be0a.5m27sea0r.c5h45cur0s.e53c7ompared to subword m-o0d.3e7l1s. 0.082 0.140 0.051

6

Exp3erime3nts

0.430 0.523 0.540
o-0n.65W7 M-0T.01D5 at0a.065

0.526 -0.105

Based on5 the re5sult-s00..o93f1985the-00e..24x59p95eri-m00..15e02n64ts w-00i..35th4010the ILWeeS-sLtTylde3aetnac, owd—eerfuurs-t10ihn..43eg90r05aexdpo00ew..50ri36nm01saemn00tp..o51li44nn79lgy f-wa00c..i84tth34o18rthoef

Charformer

3 on the s5ource—side.0A.29d1ditio0.n5a1l5ly, 0w.5e3e6xpe0r.4im42ent with hybrid systems-1w.4i8t0h a-0s.u01b7wor0d.0e6n9co-d0.e8r54and

character3 decod3er. W0.2e27trai0n.4t6r2ans0la.5t4io0n s0y.4s1te2ms of competitive qual-i1ty.72o0n t-w0.4o24high0-.0r3e6sou-1rc.0e90lan-

guage pa5irs, and perform

aEnn5gexlitseh-n10-sC..72iv91ze05ecehv-00aa..l74nu37da38tEion-n00g...25li31s00h-G-10e..23r97m01 an,

6.1

Exp3erim—ental-10S..53e00t07up

0.531 0.051

0.547 0.456 0.121 -0.838

Canine

For Eng5lish-to—-Cze-c10..h533001tran-00s..l15a60t39ion-,00..05w9254e -u00s..94e6480the

CzEng 2.0 corpus (K0.o25c3mi 0e.t51a6l., 200.52304b) 0th.4a1t3ag-

gregates 3and cu3rates-1a.6l8l 0sou-0r.c0e9s7 fo-r0.t0h3i4s la-1n.g13u0age

pair. pairs

aWned5u50seMalb5la6c6kM-10tr..72aa51nu09stlhae-t00en..d54ti27cC45zpea-c00rha..44l19sle78elnts-ee10nn..33ct77ee01sn.ce

For the English-to-German translation, we use aTasbulebse2t: ofchthrFe t(ryaeilnloinwg-gdreaetan ussceadle)byanCdheCnOeMt EaTl. ((2ye0l2lo1w).-reTdhescdaalet)a sccoonresisstfsorofde6c6oMdinagutmheetnhtoicdssefnortmenocdeelpsatirrasinﬁeldteornedenfr-odme styhseteamvsa.ilable data for WMT

and 52M back-translated German sentences from

N3 eownsthCeraswoulr2c0e2s0id. e. Additionally, we experiment wiWthehtyabgritdhesybsatcekm-tsrawnsitlhataiosnudbawtaor(dCeasnwcoedlleertaanld., 2c0h1ar9a)c. teWr edeucsoedtehre. TWraenstrfaoirnmterranBsilgataiorcnhsityesctteumres foofr caollmepxepteitriivme eqnutsalwityithonhytpweorphairgahm-reetseorsurfcoellolawn-ignugaPgeoppealirasn:dEBngoljiasrh(-2C0z1e8c)h. aFnodr tEhnegLlieseh--sGtyelremeanncaonddepr,ewrfoerdmouabnleexttheenhsiivdedeenvallauyaetriosniz. es compared to the IWSLT experiments (following the hidden

s6i.z1e inEcxrepaesreimbeentwtaeleSnetthuepTransformer Base and BoFfoigerxapErcenhrgiimltiesechnt-uttsroe,-swC).zeeIuncshceoFtnraatirnrassseltaqttoi(oOtnht,et ewptreeavl.ui,os2ue0s1ts9he)et. OCuzEr cnogd2e.0is caovrapiluasbl(eKooncmGiitehtuabl5.., S2y0s2t0ebm) othuatpt uatgsagrreegaatttaecshaenddtocuthraetepsapaellr ainvatihleabAleCsLoaunrctheoslfoogryt.his lanWgueaegvealpuaaitre. thWeesyussteemalsl n6o6tMonaluythoenntWicMpTar2a0ltleesltsseentstebnucteaplsaoirosnanddata50thMatboafctekn-tmraontsivlaatteedd Cthzeercehsseeanrtcehncoefsc. haracter-level methods. We evaluate the outF-oorf-tdhoemEaninglpisehr-ftoor-mGaenrmceaonftrthanesmlaotidoenl,swone uthsee Na HsuSbsteesttosfetthferotmraitnhiengWdMatTa1u7seBdiobmyeCdihceanl Tetasakl. (2021). The data consists of 66M authentic senten5chettppsa:i//rgsitﬁhlutbe.rceodmf/jrloibmovtihckeya/cvhaairl-anbmlet-fdaairtsaeqfor WMT

en-cs

BPE 16k BPE to char. Vanilla char. Lee-style enc.
BPE 16k BPE to char. Vanilla char. Lee-style enc.

BLEU 30.8
±0.8
28.4
±0.8
27.7
±0.7
28.8
±0.8
31.5
±0.9
29.1
±0.8
27.8
±0.8
29.1
±0.8

News chrF .585
±.006
.570
±.006
.563
±.006
.568
±.006
.603
±.006
.589
±.006
.578
±.006
.588
±.006

COMET .672
±.022
.597
±.024
.550
±.026
.609
±.024
.418
±.021
.360
±.022
.321
±.023
.363
±.022

BLEU 34.5
±1.3
31.4
±1.2
30.0
±1.2
31.7
±1.3
45.6
±1.3
46.5
±1.3
45.3
±1.3
46.5
±1.3

IT chrF .623
±.008
.603
±.008
.589
±.008
.606
±.008
.701
±.009
.703
±.008
.698
±.008
.710
±.008

COMET .889
±.022
.821
±.025
.778
±.028
.849
±.024
.622
±.021
.617
±.021
.600
±.022
.619
±.022

BLEU 26.4
±1.4
23.6
±1.3
23.3
±1.3
24.3
±1.3
38.7
±1.6
36.0
±1.4
35.6
±1.4
36.5
±1.4

Medical chrF .519
±.010
.499
±.010
.492
±.010
.506
±.010
.640
±.010
.621
±.009
.618
±.009
.623
±.009

COMET .734
±.037
.674
±.039
.663
±.039
.696
±.038
.569
±.034
.513
±.035
.496
±.036
.500
±.037

Gender Acc.
71.3 68.9 70.2 65.6
66.5 71.2 71.2 74.0

Avg. Morpheval 86.6 87.0 86.4 86.6
90.6 91.3 91.4 91.5

Recall of novel

Forms

Lemmas

33.7
vs. 63.7
34.3
vs.
34.4
vs. 61.0
34.1
vs. 61.7

48.5
vs. 71.1
47.4
vs.
47.4
vs. 68.7
48.5
vs. 69.2

40.2
vs. 72.3
45.1
vs. 71.1
50.7
vs. 64.3
44.5
vs. 77.1

51.0
vs. 67.0
50.8
vs. 65.5
45.1
vs. 70.2
50.8
vs. 65.5

Noisy set chrF .436
±.002
.436
±.001
.493
±.001
.497
±.001
.464
±.002
.465 .504
±.001
.515
±.001

Table 3: Results of the WMT-scale experiments.

en-de

(Jimeno Yepes et al., 2017) and on the WMT16 IT Domain test set (Bojar et al., 2016). We use the same evaluation metrics as for the IWSLT experiments. We estimate the conﬁdence intervals using bootstrap resampling (Koehn, 2004).
We also assess the gender bias of the systems (Stanovsky et al., 2019; Kocmi et al., 2020a), using a dataset of sentence pairs with stereotypical and non-stereotypical English sentences. We measure the accuracy of gendered nouns and pronouns using word alignment and morphological analysis.
Morphological generalization is often mentioned among the motivations for character-level modeling. Therefore, we evaluate our models using MorphEval (Burlot and Yvon, 2017; Burlot et al., 2018). Similar to the gender evaluation, MorphEval also uses contrastive sentence pairs that differ in exactly one morphological feature. Accuracy on the sentences is measured. Besides, we assess how well the models handle lemmas and forms that were unseen at training time. We tokenize and lemmatize all data with UDPipe (Straka and Straková, 2017). On the WMT20 test set, we compute the recall of test lemmas that were not in the training set and the recall of word forms that were not in the training data, but forms of the same lemma were. Note that not generating a particular lemma or form is not necessarily an error. Therefore, we report the recall in contrast with the recall of lemmas and forms that were represented in the training data.
Character-level models are also supposed to be more robust towards source-side noise. We evaluate the noise robustness of the systems using synthetic noise. We use TextFlint (Wang et al., 2021) to generate synthetic noise in the source text with simulated typos and spelling errors. We generate 20 noisy versions of the WMT20 test set and report

the average chrF score.
6.2 Results
The main results are presented in Table 3. The main trends in the translation quality are the same as in the case of IWSLT data: subword models outperform character models. Using Lee-style encoding narrows the quality gap and performs similarly to models with subword tokens on the source side. Although domain robustness often motivates character-level experiments, our experiments show that the trends are domain-independent, except for English-German IT Domain translation.
The similar performance of the subword encoder and the Lee-style encoder suggests that the hidden states of the Lee-style encoder can efﬁciently emulate the subword segmentation. We speculate that the main weaknesses remain on the decoder side.
In the English-to-Czech direction, the characterlevel models perform worse in gender bias evaluation, although they better capture grammatical gender agreement according to the MorphEval benchmark. On the other hand, character-level models make more frequent errors in the tense of coordinated verbs. There are no major differences in recall of novel forms and lemmas.
For the English-to-German translation, characterlevel methods reach better results on the gender benchmark. We speculate that getting gender correct in German might be easier because unlike Czech it does not require subject-verb agreement. The average performance on the MorphEval benchmark is also slightly better for character models. Detailed results on MorphEval are in Tables 7 and 8 in the Appendix. The higher recall of novel forms also suggests slightly better morphological generalization.
The only consistent advantage of the character-

level models is their robustness towards source side noise. Here, the character-level models outperform both the fully subword model and the subword encoder.
7 Conclusions
In our extensive literature survey, we found evidence that character-level methods should reach comparative translation quality as subword methods, typically at the expense of much higher computation costs. We speculate that the computational cost is the reason why virtually none of the recent WMT systems used character-level methods or mentioned them as a reasonable alternative.
Recently, most innovations in character-level modeling were introduced in the context of pretrained representations. In our comparison of character processing architectures (two of them used for the ﬁrst time in the context of MT), we showed that 1D convolutions followed by highway layers still deliver the best results for MT.
Character-level systems are still mostly worse than subword systems. Moreover, the recent character-level architectures do not show advantages over vanilla character models, other than improved speed.
To overcome efﬁciency issues, we proposed a two-step decoding architecture that matches the speed of subword models, however at the expense of a further drop in translation quality.
Furthermore, we found that conclusions of recent literature on decoding in MT do not generalize for character models. Character models do not suffer from the beam search curse and decoding methods based on sampling perform poorly, here.
Evaluation on competitively large datasets showed that there is still a small quality gap between character and subword models. Character models do not show better domain robustness, and only slightly better morphological generalization in German, although this is often mentioned as important motivation for character-level modeling. The only clear advantage of character models is high robustness towards source-side noise.
In contrast to earlier work on character-level MT, which claimed that decoding is straightforward and which focused on the encoder part of the model, our conclusions are that Lee-style encoding is comparable to subword encoders. Even now, most modeling innovations focus on encoding. Character-level decoding which is both accurate and efﬁcient remains

an open research question.
Acknowledgement
Many thanks to Martin Popel for comments on the pre-print of this paper and to Lukas Edman for discovering a bug in the source code and for a fruitful discussion on the topic of the paper. The work at LMU Munich was supported by was supported by the European Research Council (ERC) under the European Union’s Horizon 2020 research and innovation programme (No. 640550) and by the German Research Foundation (DFG; grant FR 2829/4-1). The work at CUNI was supported by the European Commission via its Horizon 2020 research and innovation programme (No. 870930).
References
Duygu Ataman, Orhan Firat, Mattia A. Di Gangi, Marcello Federico, and Alexandra Birch. 2019. On the importance of word boundaries in character-level neural machine translation. In Proceedings of the 3rd Workshop on Neural Generation and Translation, pages 187–193, Hong Kong. Association for Computational Linguistics.
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural machine translation by jointly learning to align and translate. In 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings.
Nikolay Banar, Walter Daelemans, and Mike Kestemont. 2020. Character-level transformer-based neural machine translation. In NLPIR 2020: 4th International Conference on Natural Language Processing and Information Retrieval, Seoul, Republic of Korea, December 18-20, 2020, pages 149–156. ACM.
Ankur Bapna, Mia Chen, Orhan Firat, Yuan Cao, and Yonghui Wu. 2018. Training deeper neural machine translation models with transparent attention. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3028–3033, Brussels, Belgium. Association for Computational Linguistics.
Loïc Barrault, Magdalena Biesialska, Ondˇrej Bojar, Marta R. Costa-jussà, Christian Federmann, Yvette Graham, Roman Grundkiewicz, Barry Haddow, Matthias Huck, Eric Joanis, Tom Kocmi, Philipp Koehn, Chi-kiu Lo, Nikola Ljubešic´, Christof Monz, Makoto Morishita, Masaaki Nagata, Toshiaki Nakazawa, Santanu Pal, Matt Post, and Marcos Zampieri. 2020. Findings of the 2020 conference on machine translation (WMT20). In Proceedings of

the Fifth Conference on Machine Translation, pages 1–55, Online. Association for Computational Linguistics. Loïc Barrault, Ondˇrej Bojar, Marta R. Costa-jussà, Christian Federmann, Mark Fishel, Yvette Graham, Barry Haddow, Matthias Huck, Philipp Koehn, Shervin Malmasi, Christof Monz, Mathias Müller, Santanu Pal, Matt Post, and Marcos Zampieri. 2019. Findings of the 2019 conference on machine translation (WMT19). In Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1), pages 1–61, Florence, Italy. Association for Computational Linguistics. Ondˇrej Bojar, Rajen Chatterjee, Christian Federmann, Yvette Graham, Barry Haddow, Matthias Huck, Antonio Jimeno Yepes, Philipp Koehn, Varvara Logacheva, Christof Monz, Matteo Negri, Aurélie Névéol, Mariana Neves, Martin Popel, Matt Post, Raphael Rubino, Carolina Scarton, Lucia Specia, Marco Turchi, Karin Verspoor, and Marcos Zampieri. 2016. Findings of the 2016 conference on machine translation. In Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers, pages 131–198, Berlin, Germany. Association for Computational Linguistics. Ondˇrej Bojar, Christian Federmann, Mark Fishel, Yvette Graham, Barry Haddow, Philipp Koehn, and Christof Monz. 2018. Findings of the 2018 conference on machine translation (WMT18). In Proceedings of the Third Conference on Machine Translation: Shared Task Papers, pages 272–303, Belgium, Brussels. Association for Computational Linguistics. Franck Burlot, Yves Scherrer, Vinit Ravishankar, Ondˇrej Bojar, Stig-Arne Grönroos, Maarit Koponen, Tommi Nieminen, and François Yvon. 2018. The WMT’18 morpheval test suites for English-Czech, English-German, English-Finnish and Turkish-English. In Proceedings of the Third Conference on Machine Translation: Shared Task Papers, pages 546–560, Belgium, Brussels. Association for Computational Linguistics. Franck Burlot and François Yvon. 2017. Evaluating the morphological competence of machine translation systems. In Proceedings of the Second Conference on Machine Translation, pages 43–55, Copenhagen, Denmark. Association for Computational Linguistics. Isaac Caswell, Ciprian Chelba, and David Grangier. 2019. Tagged back-translation. In Proceedings of the Fourth Conference on Machine Translation (Volume 1: Research Papers), pages 53–63, Florence, Italy. Association for Computational Linguistics. Mauro Cettolo, Marcello Federico, Luisa Bentivogli, Niehues Jan, Stüker Sebastian, Sudoh Katsuitho, Yoshino Koichiro, and Federmann Christian. 2017. Overview of the iwslt 2017 evaluation campaign. In International Workshop on Spoken Language Translation, pages 2–14.

Pinzhen Chen, Jindˇrich Helcl, Ulrich Germann, Laurie Burchell, Nikolay Bogoychev, Antonio Valerio Miceli Barone, Jonas Waldendorf, Alexandra Birch, and Kenneth Heaﬁeld. 2021. The University of Edinburgh’s English-German and English-Hausa submissions to the WMT21 news translation task. In Proceedings of the Sixth Conference on Machine Translation, pages 104–109, Online. Association for Computational Linguistics.
Colin Cherry, George Foster, Ankur Bapna, Orhan Firat, and Wolfgang Macherey. 2018. Revisiting character-based neural machine translation with capacity and compression. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4295–4305, Brussels, Belgium. Association for Computational Linguistics.
Junyoung Chung, Kyunghyun Cho, and Yoshua Bengio. 2016. A character-level decoder without explicit segmentation for neural machine translation. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1693–1703, Berlin, Germany. Association for Computational Linguistics.
Jonathan H. Clark, Dan Garrette, Iulia Turc, and John Wieting. 2021. CANINE: pre-training an efﬁcient tokenization-free encoder for language representation. CoRR, abs/2103.06874.
Marta R. Costa-jussà, Carlos Escolano, and José A. R. Fonollosa. 2017. Byte-based neural machine translation. In Proceedings of the First Workshop on Subword and Character Level Models in NLP, pages 154–158, Copenhagen, Denmark. Association for Computational Linguistics.
Michael Denkowski and Alon Lavie. 2011. Meteor 1.3: Automatic metric for reliable optimization and evaluation of machine translation systems. In Proceedings of the Sixth Workshop on Statistical Machine Translation, pages 85–91, Edinburgh, Scotland. Association for Computational Linguistics.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171–4186, Minneapolis, Minnesota. Association for Computational Linguistics.
Bryan Eikema and Wilker Aziz. 2020. Is MAP decoding all you need? the inadequacy of the mode in neural machine translation. In Proceedings of the 28th International Conference on Computational Linguistics, pages 4506–4520, Barcelona, Spain (Online). International Committee on Computational Linguistics.
Carlos Escolano, Marta R. Costa-jussà, and José A. R. Fonollosa. 2017. The TALP-UPC neural machine

translation system for German/Finnish-English using the inverse direction model in rescoring. In Proceedings of the Second Conference on Machine Translation, pages 283–287, Copenhagen, Denmark. Association for Computational Linguistics. Yingqiang Gao, Nikola I. Nikolov, Yuhuang Hu, and Richard H.R. Hahnloser. 2020. Character-level translation with self-attention. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1591–1604, Online. Association for Computational Linguistics. Vaibhava Goel and William J. Byrne. 2000. Minimum bayes-risk automatic speech recognition. Comput. Speech Lang., 14(2):115–135. Stig-Arne Grönroos, Sami Virpioja, and Mikko Kurimo. 2017. Extending hybrid word-character neural machine translation with multi-task learning of morphological analysis. In Proceedings of the Second Conference on Machine Translation, pages 296– 302, Copenhagen, Denmark. Association for Computational Linguistics. Rohit Gupta, Laurent Besacier, Marc Dymetman, and Matthias Gallé. 2019. Character-based NMT with transformer. CoRR, abs/1911.04997. Chester Holtz, Chuyang Ke, and Daniel Gildea. 2017. University of Rochester WMT 2017 NMT system submission. In Proceedings of the Second Conference on Machine Translation, pages 310–314, Copenhagen, Denmark. Association for Computational Linguistics. Antonio Jimeno Yepes, Aurélie Névéol, Mariana Neves, Karin Verspoor, Ondˇrej Bojar, Arthur Boyer, Cristian Grozea, Barry Haddow, Madeleine Kittner, Yvonne Lichtblau, Pavel Pecina, Roland Roller, Rudolf Rosa, Amy Siu, Philippe Thomas, and Saskia Trescher. 2017. Findings of the WMT 2017 biomedical translation shared task. In Proceedings of the Second Conference on Machine Translation, pages 234–247, Copenhagen, Denmark. Association for Computational Linguistics. Rebecca Knowles, Darlene Stewart, Samuel Larkin, and Patrick Littell. 2020. NRC systems for the 2020 Inuktitut-English news translation task. In Proceedings of the Fifth Conference on Machine Translation, pages 156–170, Online. Association for Computational Linguistics. Tom Kocmi, Tomasz Limisiewicz, and Gabriel Stanovsky. 2020a. Gender coreference and bias evaluation at WMT 2020. In Proceedings of the Fifth Conference on Machine Translation, pages 357–364, Online. Association for Computational Linguistics. Tom Kocmi, Martin Popel, and Ondˇrej Bojar. 2020b. Announcing czeng 2.0 parallel corpus with over 2 gigawords. CoRR, abs/2007.03006.

Philipp Koehn. 2004. Statistical signiﬁcance tests for machine translation evaluation. In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing, pages 388– 395, Barcelona, Spain. Association for Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondˇrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions, pages 177–180, Prague, Czech Republic. Association for Computational Linguistics.
Julia Kreutzer and Artem Sokolov. 2018. Learning to segment inputs for NMT favors character-level processing. In Proceedings of the 15th International Conference on Spoken Language Translation, pages 166–172, Brussels. International Conference on Spoken Language Translation.
Taku Kudo and John Richardson. 2018. SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 66–71, Brussels, Belgium. Association for Computational Linguistics.
Jason Lee, Kyunghyun Cho, and Thomas Hofmann. 2017. Fully character-level neural machine translation without explicit segmentation. Transactions of the Association for Computational Linguistics, 5:365–378.
Jiahuan Li, Yutong Shen, Shujian Huang, Xinyu Dai, and Jiajun Chen. 2021. When is char better than subword: A systematic study of segmentation algorithms for neural machine translation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers), pages 543–549, Online. Association for Computational Linguistics.
Xian Li, Paul Michel, Antonios Anastasopoulos, Yonatan Belinkov, Nadir Durrani, Orhan Firat, Philipp Koehn, Graham Neubig, Juan Pino, and Hassan Sajjad. 2019. Findings of the ﬁrst shared task on machine translation robustness. In Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1), pages 91–102, Florence, Italy. Association for Computational Linguistics.
Jindˇrich Libovický and Alexander Fraser. 2020. Towards reasonably-sized character-level transformer NMT by ﬁnetuning subword systems. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages

2572–2579, Online. Association for Computational Linguistics. Minh-Thang Luong and Christopher D. Manning. 2016. Achieving open vocabulary neural machine translation with hybrid word-character models. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1054–1063, Berlin, Germany. Association for Computational Linguistics. Sainik Kumar Mahata, Dipankar Das, and Sivaji Bandyopadhyay. 2018. JUCBNMT at WMT2018 news translation task: Character based neural machine translation of Finnish to English. In Proceedings of the Third Conference on Machine Translation: Shared Task Papers, pages 445–448, Belgium, Brussels. Association for Computational Linguistics. Luca Massarelli, Fabio Petroni, Aleksandra Piktus, Myle Ott, Tim Rocktäschel, Vassilis Plachouras, Fabrizio Silvestri, and Sebastian Riedel. 2020. How decoding strategies affect the veriﬁability of generated text. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 223–235, Online. Association for Computational Linguistics. Clara Meister, Ryan Cotterell, and Tim Vieira. 2020. If beam search is the answer, what was the question? In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2173–2185, Online. Association for Computational Linguistics. Nikola I. Nikolov, Yuhuang Hu, Mi Xue Tan, and Richard H.R. Hahnloser. 2018. Character-level Chinese-English translation through ASCII encoding. In Proceedings of the Third Conference on Machine Translation: Research Papers, pages 10–16, Brussels, Belgium. Association for Computational Linguistics. Robert Östling, Yves Scherrer, Jörg Tiedemann, Gongbo Tang, and Tommi Nieminen. 2017. The Helsinki neural machine translation system. In Proceedings of the Second Conference on Machine Translation, pages 338–347, Copenhagen, Denmark. Association for Computational Linguistics. Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, and Michael Auli. 2019. fairseq: A fast, extensible toolkit for sequence modeling. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations), pages 48–53, Minneapolis, Minnesota. Association for Computational Linguistics. Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311–318, Philadelphia, Pennsylvania, USA. Association for Computational Linguistics.

Martin Popel and Ondˇrej Bojar. 2018. Training Tips for the Transformer Model. The Prague Bulletin of Mathematical Linguistics, 110:43–70.
Maja Popovic´. 2015. chrF: character n-gram F-score for automatic MT evaluation. In Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 392–395, Lisbon, Portugal. Association for Computational Linguistics.
Matt Post. 2018. A call for clarity in reporting BLEU scores. In Proceedings of the Third Conference on Machine Translation: Research Papers, pages 186– 191, Brussels, Belgium. Association for Computational Linguistics.
Ivan Provilkov, Dmitrii Emelianenko, and Elena Voita. 2020. BPE-dropout: Simple and effective subword regularization. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1882–1892, Online. Association for Computational Linguistics.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a uniﬁed text-totext transformer. Journal of Machine Learning Research, 21(140):1–67.
Ricardo Rei, Craig Stewart, Ana C Farinha, and Alon Lavie. 2020. COMET: A neural framework for MT evaluation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2685–2702, Online. Association for Computational Linguistics.
Yves Scherrer, Raúl Vázquez, and Sami Virpioja. 2019. The University of Helsinki submissions to the WMT19 similar language translation task. In Proceedings of the Fourth Conference on Machine Translation (Volume 3: Shared Task Papers, Day 2), pages 236–244, Florence, Italy. Association for Computational Linguistics.
Rico Sennrich. 2017. How grammatical is characterlevel neural machine translation? assessing MT quality with contrastive translation pairs. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 376–382, Valencia, Spain. Association for Computational Linguistics.
Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Neural machine translation of rare words with subword units. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1715– 1725, Berlin, Germany. Association for Computational Linguistics.
Rico Sennrich and Biao Zhang. 2019. Revisiting lowresource neural machine translation: A case study. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 211– 221, Florence, Italy. Association for Computational Linguistics.

Uri Shaham and Omer Levy. 2021a. Neural machine translation without embeddings. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 181–186, Online. Association for Computational Linguistics.
Uri Shaham and Omer Levy. 2021b. What do you get when you cross beam search with nucleus sampling? CoRR, abs/2107.09729.
Xing Shi, Yijun Xiao, and Kevin Knight. 2020. Why neural machine translation prefers empty outputs. CoRR, abs/2012.13454.
Lucia Specia, Zhenhao Li, Juan Pino, Vishrav Chaudhary, Francisco Guzmán, Graham Neubig, Nadir Durrani, Yonatan Belinkov, Philipp Koehn, Hassan Sajjad, Paul Michel, and Xian Li. 2020. Findings of the WMT 2020 shared task on machine translation robustness. In Proceedings of the Fifth Conference on Machine Translation, pages 76–91, Online. Association for Computational Linguistics.
Rupesh Kumar Srivastava, Klaus Greff, and Jürgen Schmidhuber. 2015. Highway networks. CoRR, abs/1505.00387.
Gabriel Stanovsky, Noah A. Smith, and Luke Zettlemoyer. 2019. Evaluating gender bias in machine translation. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1679–1684, Florence, Italy. Association for Computational Linguistics.
Milan Straka and Jana Straková. 2017. Tokenizing, POS tagging, lemmatizing and parsing UD 2.0 with UDPipe. In Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies, pages 88–99, Vancouver, Canada. Association for Computational Linguistics.
Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014. Sequence to sequence learning with neural networks. In Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems 2014, December 8-13 2014, Montreal, Quebec, Canada, pages 3104–3112.
Yi Tay, Vinh Q. Tran, Sebastian Ruder, Jai Prakash Gupta, Hyung Won Chung, Dara Bahri, Zhen Qin, Simon Baumgartner, Cong Yu, and Donald Metzler. 2021. Charformer: Fast character transformers via gradient-based subword tokenization. CoRR, abs/2106.12672.
Dušan Variš and Ondˇrej Bojar. 2017. CUNI system for WMT17 automatic post-editing task. In Proceedings of the Second Conference on Machine Translation, pages 661–666, Copenhagen, Denmark. Association for Computational Linguistics.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all

you need. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 49, 2017, Long Beach, CA, USA, pages 5998–6008.
Xiao Wang, Qin Liu, Tao Gui, Qi Zhang, Yicheng Zou, Xin Zhou, Jiacheng Ye, Yongxin Zhang, Rui Zheng, Zexiong Pang, Qinzhuo Wu, Zhengyan Li, Chong Zhang, Ruotian Ma, Zichu Fei, Ruijian Cai, Jun Zhao, Xingwu Hu, Zhiheng Yan, Yiding Tan, Yuan Hu, Qiyuan Bian, Zhihua Liu, Shan Qin, Bolin Zhu, Xiaoyu Xing, Jinlan Fu, Yue Zhang, Minlong Peng, Xiaoqing Zheng, Yaqian Zhou, Zhongyu Wei, Xipeng Qiu, and Xuanjing Huang. 2021. TextFlint: Uniﬁed multilingual robustness evaluation toolkit for natural language processing. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations, pages 347–355, Online. Association for Computational Linguistics.
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. 2020. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 38–45, Online. Association for Computational Linguistics.
Linting Xue, Aditya Barua, Noah Constant, Rami AlRfou, Sharan Narang, Mihir Kale, Adam Roberts, and Colin Raffel. 2021a. Byt5: Towards a tokenfree future with pre-trained byte-to-byte models. CoRR, abs/2105.13626.
Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. 2021b. mT5: A massively multilingual pre-trained text-to-text transformer. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 483–498, Online. Association for Computational Linguistics.
Longtu Zhang and Mamoru Komachi. 2018. Neural machine translation of logographic language using sub-character level information. In Proceedings of the Third Conference on Machine Translation: Research Papers, pages 17–25, Brussels, Belgium. Association for Computational Linguistics.
A Two-step decoder
Here, we describe details of the architecture of the two step decoder shown in Figure 3. The input of the decoder are hidden states of the character processing architecture, i.e., for a downsampling

factor s, a sequence that is s times shorter than the input sequence. The output of the Transformer stack is a sequence of the same length.
For each Transformer decoder state hi, the decoder needs to produce s characters. This is done by a light-weight autoregressive LSTM decoder. In each step, it has two inputs: the embedding of the previously decoded character and a projection of the decoder state hi. There are s different linear projections for each of the output character generated from a single Transformer state.
At inference time, the LSTM decoder gets one Transformer state and generates s output characters. The characters are fed to the character processing architecture, which is in turn used to generate the next Transformer decoder state.
B IWSLT Experiments
B.1 Dataset details
We used the tst2010 part of the dataset for validation and tst2015 for testing and did not use any other test sets. The data sizes are presented in Table 4.
B.2 Model Hyperparameters
All models are trained with initial learning rate: 5 · 10−4 with 4k warmup steps. The batch size is 20k tokens for both BPE and character experiments with update after 3 batches. Label smoothing is set to 0.1.
Lee-style. The character embedding dimension is 64. The original paper used kernel sizes from 1 to 8. For ease of implementation, we only use even-sized kernels up to size 9. The encoder uses 1D convolutions of kernel size 1, 3, 5, 7, 9 with 128, 256, 512, 512, 256 ﬁlters. Their output is concatenated and projected to the model dimension, followed by 2 highway layers and 2 Transformer feed-forward layers.
CANINE. The local self-attention span in the encoder is 4× the downsampling factor, in the encoder, equal to the downsampling factor.
Two-step decoder. The decoder uses character embeddings with dimension of 64, which is also the size of the projection of the Transformer decoder state. The hidden state size of the LSTM is 128.
B.3 Validation Performance
The validation BLEU and chrF scores and training and inference times are in Table 5. The training

times were measured on machines with GeForce GTX 1080 Ti GPUs and with Intel Xeon E5– 2630v4 CPUs (2.20GHz), a single GPU was used.
Note that the experiments on IWSLT were not optimized for speed and are thus not comparable with the times reported on the larger datasets.
C WMT Experiments
C.1 Training Details
We use the Transformer Big architecture as deﬁned FairSeq’s standard transformer_wmt_en_de_big_t2t. The Lee-style encoder uses ﬁlters sizes 1, 3, 5, 7, 9 of dimensions 256, 512, 1024, 1024, 512. The other parameters remains the same as in the IWSLT experiments.
We set the beta parameters of the Adam optimizer to 0.9 and 0.998 and gradient clipping to 5. The learning rate is 5 · 10−4 with 16k warmup steps. Early stopping is with respect to negative log likelihood with patience 10. We save 5 best checkpoints and do checkpoint averaging before evaluation. The maximum batch size is 1800 tokens for the BPE experiments and 500 for character-level experiments. We train the models on 4 GPUs, so the effective batch size is 4 times bigger.
C.2 Validation Performance
During training, we evaluated the models by measuring the cross-entropy on the validation set. After model training, we use grid search to estimate the best value of length normalization on the validation set. The translation quality on the validation data is tabulated in Table 6.
C.3 Detailed Results
The detailed results on the MorphEval benchmark are in Tables 7 (Czech) and 8 (German). The details of the noise evaluation are in Table 9.

en-ar en-de en-fr

Sent. 232k 206k 232k

Train
Char. src 22.5M 19.9M 22.6M

Char. tgt 32.8M 21.7M 25.5M

Sent. 1.3k 1.3k 1.3k

Validation
Char. src 119k 117k 119k

Char. tgt 179k 132k 140k

Sent. 1.2k 1.1k 1.2k

Test
Char. src 116k 109k 116k

Char. tgt 164k 100k 129k

Table 4: IWSLT data statistics in terms of number of parallel sentences and number of characters.

Model

Lee-style

Enc. Dec.

downsample BPE 16k Vanilla char.

3

—

5

—

3

3

5

5

3

—

5

—

3

3

5

5

3

—

5

—

3

3

5

5

Train
8.9
±1.6
14.5 ±5.5
13.0 ±9.5
16.5 ±6.8
15.4 ±3.2
13.7 ±3.9
16.4 ±2.4
14.0 ±1.9
15.5 ±1.6
14.0 ±1.9
14.8 ±2.2
13.9 ±7.5
17.3 ±2.5
17.1
±8.3

ar

Valid BLEU

19.4 ±1.0
203.2 ±3.9

13.8 ±0.2
11.4 ±0.2

232.8 ±3.3
223.2 ±6.9
81.5 ±2.1
41.0 ±0.9

11.5 ±0.1
11.0 ±0.2
10.0 ±0.2 8.4 ±0.1

232.0 ±8.4
63.0
±7.0
81.2
±1.5
63.0
±7.0

11.3 ±0.2 7.4 ±0.1
10.0 ±0.2 7.4 ±0.1

300.8 ±6.8
249.2 ±5.0
91.5 ±1.1
72.0 ±6.7

10.7 ±0.3
9.4
±0.2
9.4
±0.3
6.1
±0.2

chrF
.411
±.002
.417
±.003
.420
±.002
.411
±.002
.398
±.002
.377
±.002
.417
±.002
.359
±.003
.398
±.001
.359
±.003
.407
±.004
.386
±.002
.390
±.003
.332
±.005

Train
8.2
±0.9
13.7 ±5.5
16.6 ±9.2 9.4 ±7.4
15.7 ±3.1
13.1
±5.2
16.4 ±2.7
12.2 ±1.0
14.9 ±2.3
12.2 ±1.0
19.1
±2.3
13.5 ±7.3
18.6 ±2.8
15.2 ±4.4

From English

de

Valid BLEU

23.8
±8.6
293.5 ±5.8

26.1
±0.3
24.7
±0.5

331.0 ±7.2
313.8 ±4.9
103.0 ±6.0
46.4 ±0.8

24.8 ±0.1
23.6
±0.2
22.5
±0.3
19.5 ±0.3

342.2 ±7.1
80.8
±15.4
102.8 ±3.1
80.8
±15.4

24.0
±0.4
18.2 ±0.2
22.5
±0.3
18.2 ±0.2

481.0
±51.2
366.8 ±2.8
138.5 ±11.9
85.5
±9.6

24.1
±0.2
21.6
±0.4
21.6
±0.4
17.3 ±0.3

chrF
.523
±.001
.516
±.005
.519
±.002
.510
±.002
.502
±.002
.474
±.003
.510
±.004
.456
±.002
.497
±.003
.456
±.002
.513
±.002
.489
±.005
.493
±.001
.450
±.004

Train
6.8
±1.0
17.0 ±2.0
11.1 ±9.1
18.7 ±2.0
17.1
±2.9
10.7 ±3.4
17.2 ±1.5
13.8 ±3.2
16.2 ±1.1
13.8 ±3.2
20.0
±3.3
20.1
±4.2
18.4 ±1.8
16.2 ±1.8

fr

Valid BLEU

20.6
±1.0
318.7 ±3.8

35.8
±0.3
34.9
±0.3

358.2 ±7.0
347.5 ±3.9
106.0 ±0.7
44.2
±11.1

34.9
±0.4
32.6
±0.4
33.0
±0.2
28.0
±1.9

363.8 ±8.3
76.2 ±7.4
119.2 ±9.0
76.2 ±7.4

33.7
±0.1
27.8
±0.5
32.2
±0.4
27.8
±0.5

494.8
±13.8
395.5 ±5.4
132.2 ±15.9
89.0
±5.4

33.9
±0.6
31.2 ±0.7
31.6
±0.6
27.1
±0.3

chrF
.594
±.002
.590
±.002
.591
±.003
.576
±.002
.579
±.000
.545
±.013
.582
±.002
.536
±.005
.571
±.003
.536
±.005
.582
±.003
.558
±.005
.567
±.004
.529
±.003

Train
10.4 ±0.7
16.2 ±5.2
9.6
±9.0
9.2
±7.6
14.2 ±8.3
11.6 ±6.8
15.4 ±7.0
11.5 ±3.7
14.8 ±3.8
11.5 ±3.7
19.7 ±3.3
17.7 ±4.8
14.1
±4.9
20.9 ±1.1

ar

Valid BLEU

19.8 ±0.7
241.3
±28.1

27.9 ±0.1
26.8 ±0.7

321.0 ±1.2
237.0
±120.7
102.5 ±2.2
47.2 ±0.4

27.0 ±0.1
23.7
±4.7
24.6
±0.3
22.1
±0.2

363.0
±40.0
62.5
±8.0
104.2 ±4.8
62.5
±8.0

27.1
±0.3
18.1 ±2.7
24.8
±0.3
18.1 ±2.7

368.8 ±3.8
363.2 ±8.9
115.2 ±1.8
81.8
±1.9

26.1
±0.3
22.6 ±0.1
23.9
±0.6
15.7 ±0.4

chrF
.501
±.002
.499
±.005
.502
±.002
.472
±.043
.484
±.001
.461
±.002
.500
±.002
.419
±.027
.482
±.003
.419
±.027
.493
±.003
.458
±.001
.474
±.004
.391
±.005

Train
8.9
±0.2
15.6 ±3.3
16.5 ±8.2
21.3 ±1.7
16.2 ±2.0
10.8 ±1.1
16.7 ±1.0
11.6 ±1.4
13.4 ±0.7
11.6 ±1.4
18.5 ±2.3
12.9 ±7.5
12.9 ±2.4
15.7 ±3.9

Into English

de

Valid BLEU

16.2 ±1.0
203.5
±29.6

30.2 ±0.1
29.0 ±0.7

275.2 ±1.1
257.0 ±2.9
90.8
±2.9
43.4 ±0.5

29.6
±0.3
28.5
±0.4
27.3
±0.2
24.1
±0.2

276.0 ±4.4
64.2
±2.9
89.0
±2.5
64.2
±2.9

29.4
±0.3
23.0
±0.3
27.6
±0.2
23.0
±0.3

318.2
±10.2
300.8
±10.8
104.5 ±4.0
75.0 ±2.4

28.8
±0.4
26.7
±0.2
26.2
±0.8
22.5
±0.2

chrF
.534
±.001
.527
±.004
.533
±.003
.524
±.003
.513
±.002
.489
±.002
.531
±.001
.480
±.003
.516
±.002
.480
±.003
.526
±.003
.508
±.002
.505
±.006
.473
±.001

Train
9.3
±0.7
17.9 ±2.7
17.4 ±7.7
10.8 ±9.6
14.8 ±2.2 8.9 ±2.0
17.9 ±3.2
13.0 ±5.5
15.7 ±2.4
13.0 ±5.5
13.3 ±6.5
16.9 ±2.5
14.2 ±5.9
13.1 ±3.1

fr

Valid BLEU

17.4 ±0.5
230.8
±29.4

37.9
±0.3
36.9
±0.5

301.5 ±3.4
287.8 ±9.0
94.8 ±3.7
46.4 ±0.8

37.6
±0.3
36.4
±0.2
35.3
±0.2
31.8
±0.4

306.2 ±8.3
72.5 ±9.1
100.2 ±9.8
72.5 ±9.1

37.1
±0.3
30.6
±0.3
35.7
±0.1
30.6
±0.3

347.5
±10.1
312.2 ±3.7
118.0 ±4.1
84.5
±5.0

36.7
±0.4
34.4
±0.5
35.0 ±0.1
29.4 ±0.1

chrF
.591
±.003
.583
±.003
.589
±.002
.580
±.002
.574
±.001
.549
±.003
.587
±.001
.541
±.002
.576
±.001
.541
±.002
.583
±.003
.564
±.003
.572
±.001
.529
±.002

Charformer

Canine

Table 5: Training time (hours), inference time on the validation set (seconds) and translation quality in terms of BLUE and chrF scores on the validation data.

en-cs

BLEU chrF COMET

BPE 16k

24.4 .524 .753

BPE to char 22.9 .513 .687

Vanilla char. 22.3 .506 .654

Lee-style enc. 23.1 .514 .698

BPE 16k

47.8 .708 .651

BPE to char 43.7 .683 .594

Vanilla char. 42.7 .675 .569

Lee-style enc. 43.7 .684 .595

Len. norm. 0.8 1.2 1.4 1.0
1.2 1.2 1.4 1.6

en-de

Table 6: Translation quality on the validation data and the value of length normalization that led to the best quality.

comparative conditional coordverb-number coordverb-person coordverb-tense coref-gender future negation noun number past preposition pron2coord pron2nouns-case pron2nouns-gender pron2nouns-number pron fem pron plur pron relative-gender pron relative-number superlative NOUN case ADJ gender ADJ number ADJ case VERB number VERB person VERB tense VERB negation Average

BPE 78.2% 59.8% 85.4% 85.2% 81.8% 71.7% 86.2% 96.2% 79.4% 87.2% 96.0% 100.0% 95.8% 95.2% 95.6% 94.0% 92.0% 78.9% 80.1% 93.0% .102 .198 .198 .204 .117 .091 .113 .081 88.6%

BPE2char 78.2% 65.8% 81.2% 82.0% 78.4% 74.8% 85.8% 97.4% 81.0% 89.0% 96.6% 100.0% 95.6% 95.2% 95.6% 94.6% 92.0% 81.8% 83.1% 91.4% .108 .194 .190 .198 .103 .083 .109 .077 87.0%

char 79.6% 71.2% 77.4% 78.0% 74.0% 76.5% 84.0% 98.0% 80.8% 89.4% 96.1% 99.6% 94.4% 93.6% 94.4% 93.8% 92.0% 81.8% 82.8% 91.0% .105 .211 .213 .220 .101 .085 .108 .075 86.4%

lee 80.4% 68.4% 80.0% 80.0% 75.2% 75.9% 85.8% 98.2% 81.4% 86.8% 95.9% 100.0% 94.6% 93.8% 94.6% 93.2% 91.4% 81.5% 82.6% 92.0% .100 .202 .202 .207 .104 .084 .110 .075 86.6&

Table 7: Detailed MorphEval results for English-Czech translation.

dj strong comparative compounds syns conditional coordverb-number coordverb-person coordverb-tense coref-gender future negation noun number past pron2nouns-gender pron2nouns-number pron plur pron relative-gender pron relative-number superlative verb position ADJ gender ADJ number NOUN case VERB number VERB person VERB tense/mode Average

BPE 97.9% 96.9% 65.9% 90.5% 98.0% 98.3% 98.0% 94.5% 87.3% 98.8% 67.0% 94.7% 100.0% 100.0% 99.2% 69.4% 69.4% 99.8% 96.0% .006 .004 .018 .022 .010 .046 90.6

BPE2char 98.7% 96.8% 66.0% 95.4% 98.7% 99.1% 98.7% 93.2% 90.8% 98.8% 69.3% 97.1% 100.0% 100.0% 99.2% 69.1% 69.1% 99.8% 95.2% .002 .001 .011 .017 .010 .041 91.3

Char 99.6% 95.6% 65.4% 97.0% 99.1% 99.5% 99.3% 95.1% 87.6% 99.4% 71.5% 96.0% 100.0% 100.0% 98.6% 68.8% 68.8% 99.8% 95.2% .002 .002 .013 .015 .006 .049 91.4

Lee 99.2% 96.3% 66.7% 97.0% 99.3% 99.8% 99.3% 91.9% 88.9% 99.4% 68.4% 96.5% 100.0% 100.0% 98.2% 71.0% 71.0% 99.6% 95.8% .003 .001 .011 .020 .008 .050 91.5

Table 8: Detailed MorphEval results for EnglishGerman translation.

en-cs

BPE 16k BPE to char Vanilla char. Lee-style enc. BPE 16k BPE to char Vanilla char. Lee-style enc.

BLEU 15.1 ±0.2 14.4 ±0.2 19.5 ±0.2 20.2 ±0.2 16.0 ±0.2 15.5 ±0.2 18.5 ±0.1 19.6 ±0.1

chrF .436 ±.002 .436 ±.001 .493 ±.001 .497 ±.001 .464 ±.002 .465 ±.001 .504 ±.001 .515 ±.001

COMET -.863 ±.010 -.836 ±.009 -.307 ±.009 -.308 ±.009 -1.127 ±.012 -1.112 ±.008 -.742 ±.013 -.743 ±.014

en-de

Table 9: Detailed results on the datasets with generated noise. Average and standard deviation for 20 evaluations.

