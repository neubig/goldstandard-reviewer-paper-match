Ranking and Tuning Pre-trained Models: A New Paradigm of Exploiting Model Hubs

Kaichao You1,2 ∗

youkaichao@gmail.com

Yong Liu1 ∗

liuyong21@mails.tsinghua.edu.cn

Ziyang Zhang2

zhangziyang11@huawei.com

Jianmin Wang1

jimwang@tsinghua.edu.cn

Michael I. Jordan3

jordan@cs.berkeley.edu

Mingsheng Long1 †

mingsheng@tsinghua.edu.cn

1 School of Software, BNRist, Tsinghua University, Beijing 100084, China.

2 Advanced Computing and Storage Lab, Huawei Technologies Co. Ltd

3 Division of Computer Science and Department of Statistics, UC Berkeley, CA 94720-1776, USA

arXiv:2110.10545v2 [cs.LG] 28 Mar 2022

Abstract
Model hubs with many pre-trained models (PTMs) have been a cornerstone in deep learning. Although built at a high cost, they remain under-exploited : practitioners usually pick one PTM from the provided model hub by popularity and then ﬁne-tune the PTM to solve the target task. This na¨ıve but common practice poses two obstacles to suﬃcient exploitation of pre-trained model hubs: (1) the PTM selection by popularity has no optimality guarantee; (2) only one PTM is used while the rest PTMs are ignored. Ideally, to exploit pre-trained model hubs maximally, trying all combinations of PTMs and extensively ﬁne-tuning each PTM combination are required, which incurs exponential combinations and an unaﬀordable computational budget. In this paper, we propose a new paradigm of exploiting model hubs by ranking and tuning pre-trained models: (1) Our conference paper (You et al., 2021) proposed LogME to estimate the maximum value of label evidence given features extracted by pre-trained models, which can rank all the PTMs in a model hub for various types of PTMs and tasks before ﬁne-tuning. (2) The best ranked PTM can be ﬁne-tuned and deployed if we have no preference for the model’s architecture, or the target PTM can be tuned by top-K ranked PTMs via the proposed B-Tuning algorithm. The ranking part is based on the conference paper, and we complete its theoretical analyses in this paper, including the convergence proof of the heuristic evidence maximization procedure and the inﬂuence of feature dimension. The tuning part introduces a novel Bayesian Tuning (B-Tuning) method for tuning multiple PTMs, which surpasses specialized methods designed for tuning homogeneous PTMs and sets up a new state of the art for tuning heterogeneous PTMs. The new paradigm of exploiting PTM hubs can be interesting to a large audience across the machine learning community. Keywords: Pre-trained Model Hub, Model Ranking, Model Tuning, Transfer Learning
1. Introduction
Deep neural networks (He et al., 2015, 2016; Devlin et al., 2019) trained by large-scale data (Deng et al., 2009; Russakovsky et al., 2015; Merity et al., 2017) and specialized computational devices (Jouppi et al., 2017) has achieved better performance than human
∗. The ﬁrst two authors contribute equally to the paper. †. Mingsheng Long is the corresponding author.
©2021 Kaichao You et al..
License: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/.

Kaichao You et al.

on many recognition tasks in both computer vision and natural language processing. Lots of researchers (Donahue et al., 2014; Girshick et al., 2014; Devlin et al., 2019) found that deep neural networks trained on a large-scale pre-training task (Yang et al., 2019; Clark et al., 2020; Brown et al., 2020) can produce generic representations (Donahue et al., 2014) that beneﬁt downstream tasks such as object detection (Girshick et al., 2014) and language understanding (Wang et al., 2019). These trained neural networks are known as pre-trained models (PTMs). Readers can refer to dedicated surveys (Han et al., 2021; Qiu et al., 2020; Bommasani et al., 2021) for a holistic overview of pre-trained models. The amazing power (Brown et al., 2020) of PTMs, together with the transfer learning paradigm of “pretraining → ﬁne-tuning” to exploit PTMs, has revolutionized both vision (Kornblith et al., 2019) and language (Devlin et al., 2019) communities, and PTMs’ inﬂuence is spreading to more communities like geometric learning (Hu et al., 2020).
The cost of training PTMs varies from hundreds of GPU hours (He et al., 2016) to hundreds of GPU days (Devlin et al., 2019), which is too expensive for individual researchers and academic labs to aﬀord. Most pre-trained models are provided by technical tycoons, including PyTorch Hub 1, TensorFlow Hub 2, and HuggingFace Transformer Models 3. A collection of pre-trained models is called a “pre-trained model hub” (PTM Hub), which has become an indispensable ingredient in practitioners’ daily development. Take the HuggingFace Transformer library (Wolf et al., 2020) for example, the most popular BERT model (Devlin et al., 2019) is downloaded over 80 million times every month!
Although technical tycoons spent enormous resources on providing rich PTM Hubs for the public, it turns out that practitioners usually pick the most popular PTM, meaning that the whole PTM Hub is insuﬃciently exploited. Figure 1 analyzes the monthly downloads of PTMs in the HuggingFace Transformer hub. Except for several popular models, the rest PTMs in the hub are seldom downloaded. The statistics in PyTorch Hub and TensorFlow Hub are quite the same: several popular PTMs dominate the rest.

83

118

143 151

downloads (millions)

Top 1 (BERT) Top 10 Top 100 Top 1000
Figure 1: Monthly download statistics (in millions) for top popular models in the HuggingFace Transformer library. The most popular PTM (BERT) takes up more than a half downloads, while other PTMs are much less exploited.
Na¨ıvely picking the most popular PTM is far from optimal in two aspects: (1) The PTM selection is task-speciﬁc and one PTM cannot be optimal for all the tasks: diﬀerent tasks
1. https://pytorch.org/hub/ 2. https://www.tensorflow.org/hub 3. https://huggingface.co/models
2

Ranking and Tuning Pre-trained Models

favor diﬀerent PTMs, depending on the compatibility between the pre-trained model and the target task (You et al., 2021). (2) Only one PTM is exploited, and the rest PTMs are put aside. Correspondingly, there are two reasons why practitioners resort to the suboptimal na¨ıve practice: (1) maximally exploiting a PTM hub requires trying all combinations of PTMs and extensively ﬁne-tuning each PTM combination, which requires unaﬀordable exponential computation; (2) even if the humongous computational cost can be paid, it is unclear how to exploit multiple PTMs in transfer learning. As pointed out in Section 2.4, Shu et al. (2021) studied the problem in a limited case, but a general solution is still missing.
To fully exploit PTM hubs, we propose a new paradigm: ranking and tuning pre-trained models. Figure 2 provides an overview of the paradigm. It consists of two parts: (1) PTMs are ranked by a transferability metric; (2) top-ranked PTMs are tuned to meet downstream applications’ requirements. Our preliminary work (You et al., 2021) proposed LogME to estimate the compatibility between PTMs and downstream datasets, and demonstrated its eﬀectiveness on a variety of PTMs and tasks, well supporting the ranking of PTMs. With a provided transferability rank, the best ranked PTM can be ﬁne-tuned if there are no constraints on network architecture like inference time or hardware-friendly operators. If these constraints are present, the qualiﬁed PTM with desired architecture might not be the best-ranked one, but it can be tuned by top-K ranked PTMs via a novel B-Tuning algorithm proposed in Section 5.3.

PTM Hub

Transfer Rank
1

Fine-tune

Target Data 2

Single PTM Tuning

……

PTMs Ranking

K…

…
Top 1
Top K

B-Tuning
Multiple PTMs Tuning

Target PTM

PTM Params Tuned Params

Figure 2: The proposed paradigm of ranking and tuning pre-trained models. PTMs are ranked by their transferability w.r.t. the target data, then either the best PTM is ﬁne-tuned, or the target PTM is tuned by top-K PTMs via the proposed B-Tuning.

Compared with picking the most popular PTM, our proposed new paradigm features two signiﬁcant advantages: (1) it provides a task-adaptive ranking of all PTMs in a PTM hub to enable optimal selection of PTMs; (2) it opens the new possibility to exploit multiple
3

Kaichao You et al.

PTMs for tuning, breaking the stereotype that ﬁne-tuning must be tied up with a single PTM. The new paradigm can be useful in a broad variety of scenarios, as pre-trained models are increasingly important in deep learning.
Besides a new paradigm of exploiting PTM hubs, this paper brings novel theoretical analyses and a new algorithm for multiple PTMs tuning. (1) On the theoretical side, we derive the suﬃcient condition for the evidence maximization algorithm (MacKay, 1992) to converge and analyze the inﬂuence of dimensionality on LogME. The evidence maximization algorithm (MacKay, 1992) has been supposed to be purely heuristic for decades, and to the best of our knowledge, we are the ﬁrst to successfully derive its convergence condition. (2) On the algorithm design side, we devise B-Tuning for tuning multiple PTMs using Bayesian learning, which surpasses the dedicated method (Shu et al., 2021) for homogeneous PTMs (PTMs with the same architecture) and also works for the challenging scenario with heterogeneous PTMs (PTMs with diﬀerent architectures).
The contributions of this paper are summarized as follows:

1. We propose a new paradigm of exploiting PTM hubs, namely ranking and tuning pre-trained models. It has signiﬁcant advantages compared with the common practice of na¨ıvely ﬁne-tuning a popular pre-trained model.
2. Concerning ranking PTMs, we propose LogME for transferability assessment and develop a fast algorithm to accelerate the computation. LogME is easy to interpret and is extremely eﬃcient: it brings at most 3700× speedup in wall-clock time and requires just 1% memory footprint compared with brute-force ﬁne-tuning. Theoretical analyses conﬁrm the rationality of LogME, and lay a theoretical foundation for a decades-long heuristic algorithm in evidence maximization.
3. For tuning PTMs, two possible scenarios are studied. In the academic scenario without speciﬁc requirements for the PTM architecture, the best ranked pre-trained model according to the transferability rank can be selected for subsequent ﬁne-tuning; in the industrial scenario where a speciﬁc PTM architecture is required to meet the budget of computation and energy, we propose B-Tuning to tune the given pre-trained model with top-K ranked PTMs, even though these PTMs are heterogeneous.

Compared with our conference paper (You et al., 2021) that only proposed LogME for

transferability estimation, this paper extends LogME to a paradigm of ranking and tuning

pre-trained models. Additional theoretical analyses are available in the ranking part, and a

new algorithm is presented in the tuning part. Moreover, LogME is tested against additional

tasks like named entity recognition (Sang and De Meulder, 2003) in Section 6.2.5 and prompt

learning (Liu et al., 2021a) in Section 6.6.

Note that this paper contains heavy math and many notations. For the convenience

of readers, all the notations and their meanings are listed in Table 9 for reference. The

basic problem setup contains a PTM hub with M pre-trained models {φk}M k=1, and the

transfer

learning

task

is

given

by

a

labeled

dataset

D

=

{

(x

i

,

Yi

)}

n i=1

with

n

labeled

data

points. This paper focuses on classiﬁcation and regression tasks, so the label Yi ∈ RC is C

dimensional. Before this paper, it is common practice to select a popular pre-trained model

φ and to ﬁne-tune it on the target task. To suﬃciently exploit the PTM hub, we propose a

new paradigm of “ranking and tuning pre-trained models”.

The rest of the paper is organized as follows: Section 2 summarizes related work, Section 3

focuses on ranking and introduces a transferability metric named LogME, Section 4 holds

4

Ranking and Tuning Pre-trained Models
theoretical analyses about LogME, Section 5 focuses on tuning and introduces a novel B-Tuning method for multiple PTMs tuning, Section 6 holds all the experiments, and ﬁnally Section 7 concludes the paper.
2. Related work
2.1 Transfer learning
Transfer learning (Thrun and Pratt, 1998) consists of transductive transfer, inductive transfer, task transfer, and so on. A well-known transductive paradigm is domain adaptation (Quionero-Candela et al., 2009), which aims for reducing domain shifts by transferring samples, hidden features (Long et al., 2015; Ganin and Lempitsky, 2015), and categorical information (Cao et al., 2022). Inductive transfer, namely ﬁne-tuning in deep learning (Erhan et al., 2010; Yosinski et al., 2014), exploits prior knowledge (pre-trained models) to improve the performance of target tasks. Task transfer learning (Zamir et al., 2018) focuses on how to transfer tasks rather than pre-trained models. It aims to discover the relevance among tasks (Ben-David and Schuller, 2003) and to exploit the relationship for improvement on the target task. In the context of deep learning, transfer learning usually means inductive transfer with a pre-trained model, which is the focus of this paper.
Many previous works (Yosinski et al., 2014; Kornblith et al., 2019; Neyshabur et al., 2020) revealed the beneﬁt of initializing a deep neural network with a pre-trained model. Apart from the vanilla method (i.e., the pre-trained model is just used for initialization), researchers have recently proposed sophisticated ﬁne-tuning techniques like regularization (Li et al., 2018; Chen et al., 2019), additional supervision (You et al., 2020), and carefully designed architecture (Kou et al., 2020). They can further improve transfer learning performance, but empirically these ﬁne-tuning methods do not change the ranking of pre-trained models on downstream tasks. That is, if pre-trained model A is better than pre-trained model B after vanilla ﬁne-tuning, empirically A is better than B when those advanced techniques are integrated. For example, on three datasets and four sampling rates in Table 2 of You et al. (2020), better ﬁne-tuning performance primarily indicates better Co-Tuning (their proposed method) performance, implying that the transferability of a pre-trained model might be task-speciﬁc rather than method-speciﬁc. Therefore, our experiments stick to the vanilla ﬁne-tuning during PTM ranking.
2.2 PTMs and PTM hubs
Pre-trained models (PTMs) are generalizable deep networks trained on large-scale data. They can be transferred to a series of downstream tasks. They have become a cornerstone in deep learning and sometimes are known as foundation models (Bommasani et al., 2021). Typical categories of PTMs are summarized in the following.
Supervised PTMs. In the ImageNet classiﬁcation challenge, He et al. (2015) developed the ﬁrst deep neural network that surpassed human performance. By supervised pretraining on the ImageNet dataset, deep models marched towards higher accuracy, fewer parameters, and lower computation. InceptionNet (Szegedy et al., 2015) made use of parallel convolutional ﬁlters to extract diﬀerent levels of features. ResNet (He et al., 2016) introduced skip-connections to ease the vanishing gradient problem so that much deeper networks could
5

Kaichao You et al.
be trained. Inspired by ResNet, DenseNet (Huang et al., 2017) was equipped with dense skip-connections to reuse features in a parameter-eﬃcient manner. MobileNet (Sandler et al., 2018) was a low-parameter, mobile-friendly network structure which was further optimized with the help of network architecture search to become MNASNet (Tan et al., 2019).
Unsupervised PTMs. Although supervised pre-training is the most common practice, the labeling cost of large-scale data is too expensive to aﬀord. As a large amount of unlabeled data on the Internet are available but under-exploited, recently many researchers have sought to apply self-supervised learning (Jing and Tian, 2020) on unlabeled data (Mahajan et al., 2018) with contrastive loss (Gutmann and Hyv¨arinen, 2010). And then, a family of unsupervised deep models has emerged in recent years. He et al. (2020) proposed Momentum Contrast with a creative queue structure to fully exploit the manifold structure of unlabeled data. Chen et al. (2020a) greatly improved the performance by exploring data augmentation, multi-layer projection head, and empirical designs. Designing better strategies for contrastive pre-training is still under active research (Tian et al., 2020).
Language PTMs. In recent years, natural language processing, the crown jewel of artiﬁcial intelligence, has been revolutionized by language PTMs. Unsupervised pre-trained models have been well established by training masked language models (Devlin et al., 2019) or autoregressive language models (Yang et al., 2019) on large unlabeled corpora (Merity et al., 2017). Liu et al. (2019) explored many practical details on how to improve the training of language models. Sanh et al. (2019) proposed distillation to make PTMs smaller and faster. These pre-trained language models are very common in winning submissions on important benchmarks like GLUE (Wang et al., 2018) and SQuAD (Rajpurkar et al., 2016), and have established their profound inﬂuence in the industry.
PTMs are grouped together to be hosted in PTM Hubs like TorchVision and HuggingFace Models. Technical giants spent humongous resources on training so many PTMs, but unfortunately, PTM Hubs are rather under-exploited, as quantitatively measured in Figure 1 and described in the introduction section in detail. The goal of this paper is to develop a new paradigm of exploiting PTM hubs, so that pre-trained models can be suﬃciently utilized.
2.3 Assessing the transferability of pre-trained models
Assessing the transferability of PTMs has great signiﬁcance in guiding the practice of deep learning. It can be used to rank available PTMs and act as a criterion for pre-trained model selection. Yosinski et al. (2014) studied the performance of transferring diﬀerent layers of a pre-trained model, and Kornblith et al. (2019) studied a wide variety of ImageNet PTMs with modern network architectures. These papers aim for a deep understanding (Neyshabur et al., 2020) of transfer learning by expensive and exhaustive ﬁne-tuning with humongous computation cost (see Section 6.5), which is hard for practitioners to aﬀord. In most scenarios, practitioners care most about PTMs’ relative ranking on target tasks to guide PTM selection, requiring a practical assessment method that is eﬃcient, accurate, and general : a transferability assessment method should be eﬃcient enough compared with brute-force ﬁne-tuning (Zamir et al., 2018), should be accurate enough to identify potentially best models, and should be general enough to tackle a wide variety of common scenarios.
LEEP (Nguyen et al., 2020) and NCE (Tran et al., 2019) were the ﬁrst two methods to assess the transferability of pre-trained models. Nguyen et al. (2020) constructed an
6

Ranking and Tuning Pre-trained Models

empirical predictor from the joint distribution p(yt, ys) over pre-trained labels ys and target labels yt, and calculated the log expectation of the empirical predictor (LEEP) as the transferability measure. The empirical predictor predicts the probability of the target class yt as ys∈Ys p(yt|ys)p(ys), where p(ys) comes from the PTM’s prediction over pre-trained categories. Negative Conditional Entropy (NCE) proposed by Tran et al. (2019) depended on an information-theoretic quantity (Cover, 1999) to reveal the transferability and hardness between diﬀerent tasks. It estimated the joint distribution p(yt, ys) with one-hot labels and predictions, and deﬁned NCE as −H(yt|ys), i.e., the negative conditional entropy of target labels yt given PTM’s predictions ys.

Table 1: Applicability of existing methods and LogME proposed in this paper.

Modality

Pre-train

Target

Method LEEP NCE LogME

classiﬁcation classiﬁcation 





vision

classiﬁcation

regression







contrastive

classiﬁcation 





contrastive

regression







language language modeling classiﬁcation 





However, the above methods left plenty of room for further improvement. As shown in Table 1, they can only handle classiﬁcation tasks with supervised pre-trained models. Increasingly popular contrastive pre-trained models and language models are out of their scope. The LogME algorithm proposed in this paper greatly extends the applicability of transferability assessment. LogME is fast to compute, less prone to over-ﬁtting, and broadly applicable to various pre-trained models/downstream tasks/data modalities. Its performance is validated by extensive experiments. Prior to this paper, for most (4 out of 5) transfer learning settings, task adaptive transferability assessment did not have a decent solution. In addition, LogME’s statistical rigor makes it extensible to multiple PTMs tuning (see Section 5.3), which completes the new paradigm of ranking and tuning pre-trained models.
2.4 Multiple PTMs tuning
The straightforward approach in transfer learning is to ﬁne-tune models initialized from pre-trained parameters, which we call “single PTM tuning” because it can only exploit a speciﬁc pre-trained model during ﬁne-tuning.
It is widely acknowledged that the success of transfer learning comes from the knowledge in the pre-trained model. Considering that there are so many PTMs in a PTM hub, it is appealing to transfer multiple PTMs simultaneously, a problem we call “multiple PTMs tuning”. Naturally, we expect multiple PTMs tuning can outperform single PTM tuning.
Unfortunately, multiple PTMs tuning is under-explored due to its technical diﬃculty. If multiple PTMs are homogeneous, i.e., they share the same network architecture, the problem becomes easier. Researchers in this area focused on how to align and merge multiple PTMs. Singh and Jaggi (2020) deﬁned transportation cost between neural representations and minimized the induced Wasserstein distance to align neurons from each PTM. Shu

7

Kaichao You et al.
et al. (2021) developed a channel-wise alignment method dedicated to convolutional neural networks with a learnable gating function to merge multiple PTMs. Prior to this paper, Shu et al. (2021) held the state-of-the-art result in homogeneous PTMs tuning.
Heterogeneous PTMs tuning is much more diﬃcult than homogeneous PTMs tuning. It is still an unanswered question due to the architectural heterogeneity among PTMs. In practice, however, this challenging problem is more important as pre-trained models in PTM hubs generally have diﬀerent architectures and heterogeneity is common.
This paper tries to exploit PTM hubs as suﬃciently as possible. In the proposed paradigm, PTMs are ﬁrst ranked by LogME, then top-K ranked PTMs from the PTM hub are selected for multiple PTMs tuning. A Bayesian tuning method (B-Tuning, see Section 5.3) is further proposed to solve the multiple PTMs tuning problem. Excitingly, our method is capable of both homogeneous PTMs and heterogeneous PTMs. It ﬁlls the blank in heterogeneous PTMs tuning, and surpasses the state-of-the-art method (Shu et al., 2021) dedicated to homogeneous PTMs tuning.
3. Ranking pre-trained models
Ranking pre-trained models requires a transferability metric. But before introducing the transferability metric in Section 3.2, we need to know how to quantify its ﬁdelity to the reference transferability performance, which is elaborated in the following Section 3.1.
3.1 How to measure the performance of a transferability metric?
A transfer learning task (in the form of a dataset D = {(xi, Yi)}ni=1) should have an evaluation metric (accuracy, MAP, MSE, etc.) to measure the reference transfer performance Tk of ﬁne-tuning φk with suﬃcient hyper-parameter tuning. A practical assessment method should produce a score Sk for each pre-trained model φk (ideally without ﬁne-tuning φk on D), and the scores {Sk}M k=1 should well correlate with {Tk}M k=1 so that top-performing pre-trained models can be selected by simply evaluating the scores {Sk}M k=1.
A perfect pre-trained model assessing method would produce {Sk}M k=1 with precisely the same order as {Tk}M k=1. To measure the deviation from the perfect method, we can use simple metrics like top-1 accuracy or top-K accuracy (whether the fraction among top-K in {Sk}M k=1 are also top-K in {Tk}M k=1). Nevertheless, top-1 accuracy is too conservative and top-K accuracy is not comparable across diﬀerent values of M . Rank correlation (Fagin et al., 2003) is a good alternative to directly measure the correlation between {Sk}M k=1 and {Tk}M k=1. The prior work (Nguyen et al., 2020) adopted Pearson’s linear correlation coeﬃcient, but neither Pearson’s linear correlation nor its variant (Spearman’s rank correlation) has a simple interpretation (see the interpretation of τ below). Therefore, they are not used in this paper.
The rank correlation method we choose is Kendall’s τ coeﬃcient (Kendall, 1938), which counts concordant pairs to capture the possibility of Ti being better than Tj if Si is better than Sj in choosing a good pre-trained model.
Without loss of generality, we assume larger values of transfer performance T and score S are preferred (e.g., accuracy). If this is not the case (e.g., transfer performance is measured by mean square error and small values are favored), the negation −T can be considered. For a pair of measures (Ti, Si) and (Tj, Sj), the pair is concordant if Ti < Tj ∧ Si < Sj or Ti > Tj ∧ Si > Sj (concisely speaking, sgn(Ti − Tj)sgn(Si − Sj) = 1). The Kendall’s τ
8

Ranking and Tuning Pre-trained Models

coeﬃcient is deﬁned by the following equation, which enumerates all

M 2

pairs and counts

the number of concordant pairs minus the number of discordant pairs.

τ = 1≤i<j≤M sgn(TiM− Tj)sgn(Si − Sj)
2

How to interpret τ (Fagin et al., 2003): The range of τ is [−1, 1]. τ = 1 means T and

S are perfectly correlated (Si > Sj ⇐⇒ Ti > Tj), and τ = −1 means T and S are reversely

correlated (Si > Sj ⇐⇒ Ti < Tj). If T and S have a correlation value of τ , the probability

of

Ti

> Tj

is

τ +1 2

when

Si

> Sj.

Pay attention to top-performing models. Since a major application of transfer-

ability metric is to select top-performing pre-trained models, discordant/concordant pairs

should be weighted more if Ti, Tj, Si, Sj are larger. This can be taken care of by τw (Vigna,

2015), a weighted variant of Kendall’s τ . The details of calculating τw can be found in

the SciPy implementation. With the weighting scheme, correlation value τw corresponds

to a proportion interval of concordant pairs rather than a unique proportion value τw2+1 .

Nevertheless, the interval lies near the value τw2+1 . Therefore, we can roughly use the

probability

τw +1 2

of

concordant

pairs

to

interpret

correlation

value

τw .

In short, we measure the correlation between {Sk}M k=1 and {Tk}M k=1 by τw (Vigna, 2015).

Larger τw indicates a better correlation and better assessment.

3.2 The LogME approach
This section describes LogME in detail. Since a transferability metric measures the transferability of pre-trained models, it should produce a score Sk for each PTM φk independent of the rest PTMs. We thus drop the subscript k in this section.
An important goal of designing transferability metrics is to quickly assess many PTMs. With that in mind, we set minimizing assessment time as a priority. First, to avoid expensive optimization of the whole PTM, PTM φ is regarded as a ﬁxed feature extractor. Note that Nguyen et al. (2020) were limited to supervised pre-trained models because they used a pre-trained classiﬁcation head h. In contrast, we only use the pre-trained representation model φ so that the proposed method can be applied to any pre-trained model (whether supervised pre-trained or unsupervised pre-trained).
With φ ﬁxed, features {fi = φ(xi)}ni=1 and labels {Yi}ni=1 of the target task are what we can use to assess pre-trained models. The rest of this section discusses how to estimate the compatibility of features and labels as a transferability metric.

3.2.1 Evidence calculation
We ﬁrst consider a simple case with D-dimensional features fi ∈ RD and scalar labels yi ∈ R. Note that the actual label Yi can be non-scalar, and how to extend from scalar labels yi to vector labels Yi is explained in Section 3.2.2.
Let the feature matrix F ∈ Rn×D denote all the features and y ∈ Rn denote all the labels. A direct measurement of the compatibility between features F and labels y is the probability density p(y|F ), which is intractable without a parameterized model. Since the rule-of-thumb transfer learning practice is to add a linear layer on top of the pre-trained model, we use a linear model upon features parameterized by w.

9

Kaichao You et al.

A straightforward approach to deal with the linear model is to ﬁnd the best w∗ by logistic or linear regression under maximum likelihood estimation, and to assess pre-trained models by the likelihood p(y|F, w∗). However, it is well known that maximum likelihood estimation is prone to over-ﬁtting (Bishop, 2006). Regularization techniques like L2 regularization may alleviate over-ﬁtting at the cost of additional hyper-parameters, which requires manual intervention or grid search to tune those hyper-parameters. Even after extensive hyperparameter tuning, its performance is not satisfying as observed in Section 6.6, because ﬁnding an optimal hyper-parameter is very diﬃcult. Ideally, a transferability metric should
have no hyper-parameters so that it can be applied to downstream tasks without manual
intervention. Obviously, this approach does not satisfy the hyperparameter-free property. The disadvantage of the above approach can be overcome by the evidence approach
introduced below. Evidence (also known as marginalized likelihood) is deﬁned as p(y|F ) = p(w)p(y|F, w)dw, which integrates over all possible values of w rather than one w∗ value.
This evidence-based approach is an elegant model selection approach and has a rigorous theoretical foundation (Knuth et al., 2015). p(w) and p(y|F, w) are modeled by a graphical model (Figure 3) speciﬁed by two positive hyper-parameters α and β: the prior distribution of the weight is an isotropic multivariate Gaussian w ∼ N (0, α−1I), and the distribution of each observation is a one-dimensional normal distribution p(yi|fi, w, β) ∼ N (yi|wT fi, β−1). Fortunately, hyper-parameters α and β can be automatically set to their optimal values as described in Section 3.2.2.

< l a t e x i t s h a 1 _ b a s e 6 4 = " g y T 4 t 2 m R u z 3 c 6 w t s 8 u u 8 4 k 4 Y z + o = " > A A A C D H i c b V D L S g M x F M 3 U V 6 2 v q k s 3 w S J U 0 D K j o i 6 L b n Q j F e w D O m O 5 k 6 Z t a O Z B k l H K M B / g x l 9 x 4 0 I R t 3 6 A O / / G T D s L r R 4 I H M 4 5 l 9 x 7 3 J A z q U z z y 8 j N z M 7 N L + Q X C 0 v L K 6 t r x f W N h g w i Q W i d B D w Q L R c k 5 c y n d c U U p 6 1 Q U P B c T p v u 8 D z 1 m 3 d U S B b 4 N 2 o U U s e D v s 9 6 j I D S U q d Y u s e 2 Z B 6 2 P V A D A j y + S s r m H r a B h w O 4 j f e t 5 H J X p 8 y K O Q b + S 6 y M l F C G W q f 4 a X c D E n n U V 4 S D l G 3 L D J U T g 1 C M c J o U 7 E j S E M g Q + r S t q Q 8 e l U 4 8 P i b B O 1 r p 4 l 4 g 9 P M V H q s / J 2 L w p B x 5 r k 6 m O 8 t p L x X / 8 9 q R 6 p 0 6 M f P D S F G f T D 7 q R R y r A K f N 4 C 4 T l C g + 0 g S I Y H p X T A Y g g C j d X 0 G X Y E 2 f / J c 0 D i r W c e X w + q h U P c v q y K M t t I 3 K y E I n q I o u U A 3 V E U E P 6 A m 9 o F f j 0 X g 2 3 o z 3 S T R n Z D O b 6 B e M j 2 + i C Z o V < / l a t e x i t >
w

⇠ N (0, ↵

1I)

w< l a t e x i ts h a 1 _ b a s e 6 4 = " I K y P j L K y e 3 6 w 4 5 l J Y u o H V f X 9 m v I = " > A A A B 6 H i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 K o m I e i x 6 8 d i C / Y A 2 l M 1 2 0 q 7 d b M L u R i m h v 8 C L B 0 W 8 + p O 8 + W / c t j l o 6 4 O B x 3 s z z M w L E s G 1 c d 1 v Z 2 V 1 b X 1 j s 7 B V 3 N 7 Z 3 d s v H R w 2 d Z w q h g 0 W i 1 i 1 A 6 p R c I k N w 4 3 A d q K Q R o H A V j C 6 n f q t R 1 S a x / L e j B P 0 I z q Q P O S M G i v V n 3 q l s l t x Z y D L x M t J G X L U e q W v b j 9 m a Y T S M E G 1 7 n h u Y v y M K s O Z w E m x m 2 p M K B v R A X Y s l T R C 7 W e z Q y f k 1 C p 9 E s b K l j R k p v 6 e y G i k 9 T g K b G d E z V A v e l P x P 6 + T m v D a z 7 h M U o O S z R e F q S A m J t O v S Z 8 r Z E a M L a F M c X s r Y U O q K D M 2 m 6 I N w V t 8 e Z k 0 z y v e Z c W t X 5 S r N 3 k c B T i G E z g D D 6 6 g C n d Q g w Y w Q H i G V 3 h z H p w X 5 9 3 5 m L e u O P n M E f y B 8 / k D 5 l u N A A = = < / l a t e x i t >

y 1 < l a t e x i t s h a 1 _ b a s e 6 4 = " R D 1 x n 0 4 M C 7 W l x q B n + M 6 h 5 p M V 2 R 8 = " > A A A B 6 3 i c b V D L S g N B E O z 1 G e M r 6 l G R w S B 4 C r u i 6 D H o x W M C 5 g H J E m Y n s 8 m Q m d l l Z l Y I S 4 5 e v X h Q x K v / k O / w 5 j f 4 E 8 4 m O W h i Q U N R 1 U 1 3 V x B z p o 3 r f j l L y y u r a + u 5 j f z m 1 v b O b m F v v 6 6 j R B F a I x G P V D P A m n I m a c 0 w w 2 k z V h S L g N N G M L j N / M Y D V Z p F 8 t 4 M Y + o L 3 J M s Z A S b T B p 2 v H y n U H R L 7 g R o k X g z U i w f j a v f j 8 f j S q f w 2 e 5 G J B F U G s K x 1 i 3 P j Y 2 f Y m U Y 4 X S U b y e a x p g M c I + 2 L J V Y U O 2 n k 1 t H 6 N Q q X R R G y p Y 0 a K L + n k i x 0 H o o A t s p s O n r e S 8 T / / N a i Q m v / Z T J O D F U k u m i M O H I R C h 7 H H W Z o s T w o S W Y K G Z v R a S P F S b G x p O F 4 M 2 / v E j q 5 y X v o n R Z t W n c w B Q 5 O I Q T O A M P r q A M d 1 C B G h D o w x O 8 w K s j n G f n z X m f t i 4 5 s 5 k D + A P n 4 w c x 5 p F k < / l a t e x i t >

< l a t e x i t s h a 1 _ b a s e 6 4 = " b G D T O K y V 7 G D R Z r D o b 8 G d o O 8 h X f o = " > A A A C E n i c b V D L S s N A F J 3 U V 6 2 v q E s 3 g 0 V o Q U s i o i 6 L b l x J h b 6 g S c N k O m m H T h 7 M T J Q Q 8 g 1 u / B U 3 L h R x 6 8 q d f + O k 7 U J b D 1 w 4 n H M v 9 9 7 j R o w K a R j f W m F p e W V 1 r b h e 2 t j c 2 t 7 R d / f a I o w 5 J i 0 c s p B 3 X S Q I o w F p S S o Z 6 U a c I N 9 l p O O O r 3 O / c 0 + 4 o G H Q l E l E b B 8 N A + p R j K S S H L 2 a O B R a g v r Q 8 p E c Y c T S 2 6 z y 0 G 9 C z 6 H H 0 H K J R P 3 0 x M y q j l 4 2 a s Y E c J G Y M 1 I G M z Q c / c s a h D j 2 S S A x Q 0 L 0 T C O S d o q 4 p J i R r G T F g k Q I j 9 G Q 9 B Q N k E + E n U 5 e y u C R U g b Q C 7 m q Q M K J + n s i R b 4 Q i e + q z v x u M e / l 4 n 9 e L 5 b e p Z 3 S I I o l C f B 0 k R c z K E O Y 5 w M H l B M s W a I I w p y q W y E e I Y 6 w V C m W V A j m / M u L p H 1 a M 8 9 r x t 1 Z u X 4 1 i 6 M I D s A h q A A T X I A 6 u A E N 0 A I Y P I J n 8 A r e t C f t R X v X P q a t B W 0 2 s w / + Q P v 8 A W 4 m n K w = < / l a t e x i t >
y.i .⇠.< l a t e x i ts h a 1 _ b a s e 6 4 = " 7 8 L 7 Y W u J H Z K G a Y K T 1 U A o v U k S Y V U = " > A A A B 6 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 i k o s e i F 4 8 V 7 Q e 0 o W y 2 k 3 b p Z h N 2 N 0 I p / Q l e P C j i 1 V / k z X / j t s 1 B W x 8 M P N 6 b Y W Z e m A q u j e d 9 O 4 W 1 9 Y 3 N r e J 2 a W d 3 b / + g f H j U 1 E m m G D Z Y I h L V D q l G w S U 2 D D c C 2 6 l C G o c C W + H o d u a 3 n l B p n s h H M 0 4 x i O l A 8 o g z a q z 0 4 L p u r 1 z x X G 8 O s k r 8 n F Q g R 7 1 X / u r 2 E 5 b F K A 0 T V O u O 7 6 U m m F B l O B M 4 L X U z j S l l I z r A j q W S x q i D y f z U K T m z S p 9 E i b I l D Z m r v y c m N N Z 6 H I e 2 M 6 Z m q J e 9 m f i f 1 8 l M d B 1 M u E w z g 5 I t F k W Z I C Y h s 7 9 J n y t k R o w t o U x x e y t h Q 6 o o M z a d k g 3 B X 3 5 5 l T Q v X L / q X t 5 X K 7 W b P I 4 i n M A p n I M P V 1 C D O 6 h D A x g M 4 B l e 4 c 0 R z o v z 7 n w s W g t O P n M M f + B 8 / g B O f I 0 q < / l a t e x i t >

N (wT
y i < l a t e x i t s h a 1 _ b a s e 6 4 = " / q 3 y w K t y q r X 0 Y d j + R n x 0 X M H 2 G K c = " > A A A B 6 3 i c b V D L S g N B E O z 1 G e M r 6 l G R w S B 4 C r u i 6 D H o x W M C 5 g H J E m Y n s 8 m Q m d l l Z l Y I S 4 5 e v X h Q x K v / k O / w 5 j f 4 E 8 4 m O W h i Q U N R 1 U 1 3 V x B z p o 3 r f j l L y y u r a + u 5 j f z m 1 v b O b m F v v 6 6 j R B F a I x G P V D P A m n I m a c 0 w w 2 k z V h S L g N N G M L j N / M Y D V Z p F 8 t 4 M Y + o L 3 J M s Z A S b T B p 2 W L 5 T K L o l d w K 0 S L w Z K Z a P x t X v x + N x p V P 4 b H c j k g g q D e F Y 6 5 b n x s Z P s T K M c D r K t x N N Y 0 w G u E d b l k o s q P b T y a 0 j d G q V L g o j Z U s a N F F / T 6 R Y a D 0 U g e 0 U 2 P T 1 v J e J / 3 m t x I T X f s p k n B g q y X R R m H B k I p Q 9 j r p M U W L 4 0 B J M F L O 3 I t L H C h N j 4 8 l C 8 O Z f X i T 1 8 5 J 3 U b q s 2 j R u Y I o c H M I J n I E H V 1 C G O 6 h A D Q j 0 4 Q l e 4 N U R z r P z 5 r x P W 5 e c 2 c w B / I H z 8 Q O G / p G c < / l a t e x i t >

fi

,. . .1 < l a t e x i ts h a 1 _ b a s e 6 4 = " 7 8 L 7 Y W u J H Z K G a Y K T 1 U A o v U k S Y V U = " > A A A B 6 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 i k o s e i F 4 8 V 7 Q e 0 o W y 2 k 3 b p Z h N 2 N 0 I p / Q l e P C j i 1 V / k z X / j t s 1 B W x 8 M P N 6 b Y W Z e m A q u j e d 9 O 4 W 1 9 Y 3 N r e J 2 a W d 3 b / + g f H j U 1 E m m G D Z Y I h L V D q l G w S U 2 D D c C 2 6 l C G o c C W + H o d u a 3 n l B p n s h H M 0 4 x i O l A 8 o g z a q z 0 4 L p u r 1 z x X G 8 O s k r 8 n F Q g R 7 1 X / u r 2 E 5 b F K A 0 T V O u O 7 6 U m m F B l O B M 4 L X U z j S l l I z r A j q W S x q i D y f z U K T m z S p 9 E i b I l D Z m r v y c m N N Z 6 H I e 2 M 6 Z m q J e 9 m f i f 1 8 l M d B 1 M u E w z g 5 I t F k W Z I C Y h s 7 9 J n y t k R o w t o U x x e y t h Q 6 o o M z a d k g 3 B X 3 5 5 l T Q v X L / q X t 5 X K 7 W b P I 4 i n M A p n I M P V 1 C D O 6 h D A x g M 4 B l e 4 c 0 R z o v z 7 n w s W g t O P n M M f + B 8 / g B O f I 0 q < / l a t e x i t >

)

y n < l a t e x i t s h a 1 _ b a s e 6 4 = " f 5 W w f C o o o v R b r / S + 1 E h R e 7 / x 4 o k = " > A A A B 6 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m k o s e i F 4 8 V 7 Q e 0 o W y 2 m 3 b p Z h N 2 J 0 I o / Q l e P C j i 1 V / k z X / j t s 1 B W x 8 M P N 6 b Y W Z e k E h h 0 H W / n c L a + s b m V n G 7 t L O 7 t 3 9 Q P j x q m T j V j D d Z L G P d C a j h U i j e R I G S d x L N a R R I 3 g 7 G t z O / / c S 1 E b F 6 x C z h f k S H S o S C U b T S Q 9 Z X / X L F r b p z k F X i 5 a Q C O R r 9 8 l d v E L M 0 4 g q Z p M Z 0 P T d B f 0 I 1 C i b 5 t N R L D U 8 o G 9 M h 7 1 q q a M S N P 5 m f O i V n V h m Q M N a 2 F J K 5 + n t i Q i N j s i i w n R H F k V n 2 Z u J / X j f F 8 N q f C J W k y B V b L A p T S T A m s 7 / J Q G j O U G a W U K a F v Z W w E d W U o U 2 n Z E P w l l 9 e J a 2 L q l e r X t 7 X K v W b P I 4 i n M A p n I M H V 1 C H O 2 h A E x g M 4 R l e 4 c 2 R z o v z 7 n w s W g t O P n M M f + B 8 / g B s Q o 3 m < / l a t e x i t >

. . .< l a t e x i ts h a 1 _ b a s e 6 4 = " 7 8 L 7 Y W u J H Z K G a Y K T 1 U A o v U k S Y V U = " > A A A B 6 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 i k o s e i F 4 8 V 7 Q e 0 o W y 2 k 3 b p Z h N 2 N 0 I p / Q l e P C j i 1 V / k z X / j t s 1 B W x 8 M P N 6 b Y W Z e m A q u j e d 9 O 4 W 1 9 Y 3 N r e J 2 a W d 3 b / + g f H j U 1 E m m G D Z Y I h L V D q l G w S U 2 D D c C 2 6 l C G o c C W + H o d u a 3 n l B p n s h H M 0 4 x i O l A 8 o g z a q z 0 4 L p u r 1 z x X G 8 O s k r 8 n F Q g R 7 1 X / u r 2 E 5 b F K A 0 T V O u O 7 6 U m m F B l O B M 4 L X U z j S l l I z r A j q W S x q i D y f z U K T m z S p 9 E i b I l D Z m r v y c m N N Z 6 H I e 2 M 6 Z m q J e 9 m f i f 1 8 l M d B 1 M u E w z g 5 I t F k W Z I C Y h s 7 9 J n y t k R o w t o U x x e y t h Q 6 o o M z a d k g 3 B X 3 5 5 l T Q v X L / q X t 5 X K 7 W b P I 4 i n M A p n I M P V 1 C D O 6 h D A x g M 4 B l e 4 c 0 R z o v z 7 n w s W g t O P n M M f + B 8 / g B O f I 0 q < / l a t e x i t >

. . .< l a t e x i ts h a 1 _ b a s e 6 4 = " 7 8 L 7 Y W u J H Z K G a Y K T 1 U A o v U k S Y V U = " > A A A B 6 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 i k o s e i F 4 8 V 7 Q e 0 o W y 2 k 3 b p Z h N 2 N 0 I p / Q l e P C j i 1 V / k z X / j t s 1 B W x 8 M P N 6 b Y W Z e m A q u j e d 9 O 4 W 1 9 Y 3 N r e J 2 a W d 3 b / + g f H j U 1 E m m G D Z Y I h L V D q l G w S U 2 D D c C 2 6 l C G o c C W + H o d u a 3 n l B p n s h H M 0 4 x i O l A 8 o g z a q z 0 4 L p u r 1 z x X G 8 O s k r 8 n F Q g R 7 1 X / u r 2 E 5 b F K A 0 T V O u O 7 6 U m m F B l O B M 4 L X U z j S l l I z r A j q W S x q i D y f z U K T m z S p 9 E i b I l D Z m r v y c m N N Z 6 H I e 2 M 6 Z m q J e 9 m f i f 1 8 l M d B 1 M u E w z g 5 I t F k W Z I C Y h s 7 9 J n y t k R o w t o U x x e y t h Q 6 o o M z a d k g 3 B X 3 5 5 l T Q v X L / q X t 5 X K 7 W b P I 4 i n M A p n I M P V 1 C D O 6 h D A x g M 4 B l e 4 c 0 R z o v z 7 n w s W g t O P n M M f + B 8 / g B O f I 0 q < / l a t e x i t >

y< l a t e x i ts h a 1 _ b a s e 6 4 = " k q a W h W k G p N + G G G d 2 t v e N u 9 R 1 h T 0 = " > A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 1 G P R i 8 c W 7 A e 0 o W y 2 k 3 b t Z h N 2 N 0 I o / Q V e P C j i 1 Z / k z X / j t s 1 B W x 8 M P N 6 b Y W Z e k A i u j e t + O 4 W 1 9 Y 3 N r e J 2 a W d 3 b / + g f H j U 0 n G q G D Z Z L G L V C a h G w S U 2 D T c C O 4 l C G g U C 2 8 H 4 b u a 3 n 1 B p H s s H k y X o R 3 Q o e c g Z N V Z q Z P 1 y x a 2 6 c 5 B V 4 u W k A j n q / f J X b x C z N E J p m K B a d z 0 3 M f 6 E K s O Z w G m p l 2 p M K B v T I X Y t l T R C 7 U / m h 0 7 J m V U G J I y V L W n I X P 0 9 M a G R 1 l k U 2 M 6 I m p F e 9 m b i f 1 4 3 N e G N P + E y S Q 1 K t l g U p o K Y m M y + J g O u k B m R W U K Z 4 v Z W w k Z U U W Z s N i U b g r f 8 8 i p p X V S 9 q 6 r b u K z U b v M 4 i n A C p 3 A O H l x D D e 6 h D k 1 g g P A M r / D m P D o v z r v z s W g t O P n M M f y B 8 / k D 6 W O N A g = = < / l a t e x i t >
< l a t e x i t s h a 1 _ b a s e 6 4 = " B 7 f U q C A d l Q p 1 h / X 7 N S f 6 f q 9 t v I A = " > A A A B 7 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 1 G P R i 8 c K p i 2 0 o W y 2 k 3 b p Z h N 2 N 0 I J / Q 1 e P C j i 1 R / k z X / j t s 1 B W x 8 M P N 6 b Y W Z e m A q u j e t + O 6 W 1 9 Y 3 N r f J 2 Z W d 3 b / + g e n j U 0 k m m G P o s E Y n q h F S j 4 B J 9 w 4 3 A T q q Q x q H A d j i + m / n t J 1 S a J / L R T F I M Y j q U P O K M G i v 5 v R A N 7 V d r b t 2 d g 6 w S r y A 1 K N D s V 7 9 6 g 4 R l M U r D B N W 6 6 7 m p C X K q D G c C p 5 V e p j G l b E y H 2 L V U 0 h h 1 k M + P n Z I z q w x I l C h b 0 p C 5 + n s i p 7 H W k z i 0 n T E 1 I 7 3 s z c T / v G 5 m o p s g 5 z L N D E q 2 W B R l g p i E z D 4 n A 6 6 Q G T G x h D L F 7 a 2 E j a i i z N h 8 K j Y E b / n l V d K 6 q H t X d f f h s t a 4 L e I o w w m c w j l 4 c A 0 N u I c m + M C A w z O 8 w p s j n R f n 3 f l Y t J a c Y u Y Y / s D 5 / A H F f I 6 p < / l a t e x i t >

↵< l a t e x i t s h a 1 _ b a s e 6 4 = " W 4 N s Z 3 U d M d 3 J S q H J 0 n b S Z y X s A D w = " > A A A B 7 X i c b V D L S g N B E O z 1 G e M r 6 t H L Y B A 8 h V 0 R 9 R j 0 4 j G C e U C y h N 7 J b D J m d m a Z m R V C y D 9 4 8 a C I V / / H m 3 / j J N m D J h Y 0 F F X d d H d F q e D G + v 6 3 t 7 K 6 t r 6 x W d g q b u / s 7 u 2 X D g 4 b R m W a s j p V Q u l W h I Y J L l n d c i t Y K 9 U M k 0 i w Z j S 8 n f r N J 6 Y N V / L B j l I W J t i X P O Y U r Z M a H R T p A L u l s l / x Z y D L J M h J G X L U u q W v T k / R L G H S U o H G t A M / t e E Y t e V U s E m x k x m W I h 1 i n 7 U d l Z g w E 4 5 n 1 0 7 I q V N 6 J F b a l b R k p v 6 e G G N i z C i J X G e C d m A W v a n 4 n 9 f O b H w d j r l M M 8 s k n S + K M 0 G s I t P X S Y 9 r R q 0 Y O Y J U c 3 c r o Q P U S K 0 L q O h C C B Z f X i a N 8 0 p w W f H v L 8 r V m z y O A h z D C Z x B A F d Q h T u o Q R 0 o P M I z v M K b p 7 w X 7 9 3 7 m L e u e P n M E f y B 9 / k D j S e P H Q = = < / l a t e x i t >

f 1 < l a t e x i t s h a 1 _ b a s e 6 4 = " y X R D 1 n N z v O E X T 3 Q L / r 3 a n 1 h 8 l P Y = " > A A A B 6 n i c b V D L S g N B E O y N r x h f U Y + K D A b B U 9 g V R Y 9 B L x 4 T N A 9 I l j A 7 m U 2 G z M w u M 7 N C W H L 0 6 M W D I l 7 9 i H y H N 7 / B n 3 D y O G h i Q U N R 1 U 1 3 V x B z p o 3 r f j m Z p e W V 1 b X s e m 5 j c 2 t 7 J 7 + 7 V 9 N R o g i t k o h H q h F g T T m T t G q Y 4 b Q R K 4 p F w G k 9 6 N + M / f o D V Z p F 8 t 4 M Y u o L 3 J U s Z A Q b K 9 2 F b a + d L 7 h F d w K 0 S L w Z K Z Q O R 5 X v x 6 N R u Z 3 / b H U i k g g q D e F Y 6 6 b n x s Z P s T K M c D r M t R J N Y 0 z 6 u E u b l k o s q P b T y a l D d G K V D g o j Z U s a N F F / T 6 R Y a D 0 Q g e 0 U 2 P T 0 v D c W / / O a i Q m v / J T J O D F U k u m i M O H I R G j 8 N + o w R Y n h A 0 s w U c z e i k g P K 0 y M T S d n Q / D m X 1 4 k t b O i d 1 6 8 q N g 0 r m G K L B z A M Z y C B 5 d Q g l s o Q x U I d O E J X u D V 4 c 6 z 8 + a 8 T 1 s z z m x m H / 7 A + f g B 3 8 y R P Q = = < / l a t e x i t >

. . .< l a t e x i ts h a 1 _ b a s e 6 4 = " 7 8 L 7 Y W u J H Z K G a Y K T 1 U A o v U k S Y V U = " > A A A B 6 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 i k o s e i F 4 8 V 7 Q e 0 o W y 2 k 3 b p Z h N 2 N 0 I p / Q l e P C j i 1 V / k z X / j t s 1 B W x 8 M P N 6 b Y W Z e m A q u j e d 9 O 4 W 1 9 Y 3 N r e J 2 a W d 3 b / + g f H j U 1 E m m G D Z Y I h L V D q l G w S U 2 D D c C 2 6 l C G o c C W + H o d u a 3 n l B p n s h H M 0 4 x i O l A 8 o g z a q z 0 4 L p u r 1 z x X G 8 O s k r 8 n F Q g R 7 1 X / u r 2 E 5 b F K A 0 T V O u O 7 6 U m m F B l O B M 4 L X U z j S l l I z r A j q W S x q i D y f z U K T m z S p 9 E i b I l D Z m r v y c m N N Z 6 H I e 2 M 6 Z m q J e 9 m f i f 1 8 l M d B 1 M u E w z g 5 I t F k W Z I C Y h s 7 9 J n y t k R o w t o U x x e y t h Q 6 o o M z a d k g 3 B X 3 5 5 l T Q v X L / q X t 5 X K 7 W b P I 4 i n M A p n I M P V 1 C D O 6 h D A x g M 4 B l e 4 c 0 R z o v z 7 n w s W g t O P n M M f + B 8 / g B O f I 0 q < / l a t e x i t >

f i < l a t e x i t s h a 1 _ b a s e 6 4 = " k u 6 6 B b F S g T N H N i M 6 G n r H v 8 z P K F M = " > A A A B 6 3 i c b V D L S g N B E O z 1 G e M r 6 l G R w S B 4 C r u i 6 D H o x W M C 5 g H J E m Y n s 8 m Q m d l l Z l Y I S 4 5 e v X h Q x K v / k O / w 5 j f 4 E 8 4 m O W h i Q U N R 1 U 1 3 V x B z p o 3 r f j l L y y u r a + u 5 j f z m 1 v b O b m F v v 6 6 j R B F a I x G P V D P A m n I m a c 0 w w 2 k z V h S L g N N G M L j N / M Y D V Z p F 8 t 4 M Y + o L 3 J M s Z A S b T A o 7 L N 8 p F N 2 S O w F a J N 6 M F M t H 4 + r 3 4 / G 4 0 i l 8 t r s R S Q S V h n C s d c t z Y + O n W B l G O B 3 l 2 4 m m M S Y D 3 K M t S y U W V P v p 5 N Y R O r V K F 4 W R s i U N m q i / J 1 I s t B 6 K w H Y K b P p 6 3 s v E / 7 x W Y s J r P 2 U y T g y V Z L o o T D g y E c o e R 1 2 m K D F 8 a A k m i t l b E e l j h Y m x 8 W Q h e P M v L 5 L 6 e c m 7 K F 1 W b R o 3 M E U O D u E E z s C D K y j D H V S g B g T 6 8 A Q v 8 O o I 5 9 l 5 c 9 6 n r U v O b O Y A / s D 5 + A F p + Z G J < / l a t e x i t >

. . .< l a t e x i ts h a 1 _ b a s e 6 4 = " 7 8 L 7 Y W u J H Z K G a Y K T 1 U A o v U k S Y V U = " > A A A B 6 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 i k o s e i F 4 8 V 7 Q e 0 o W y 2 k 3 b p Z h N 2 N 0 I p / Q l e P C j i 1 V / k z X / j t s 1 B W x 8 M P N 6 b Y W Z e m A q u j e d 9 O 4 W 1 9 Y 3 N r e J 2 a W d 3 b / + g f H j U 1 E m m G D Z Y I h L V D q l G w S U 2 D D c C 2 6 l C G o c C W + H o d u a 3 n l B p n s h H M 0 4 x i O l A 8 o g z a q z 0 4 L p u r 1 z x X G 8 O s k r 8 n F Q g R 7 1 X / u r 2 E 5 b F K A 0 T V O u O 7 6 U m m F B l O B M 4 L X U z j S l l I z r A j q W S x q i D y f z U K T m z S p 9 E i b I l D Z m r v y c m N N Z 6 H I e 2 M 6 Z m q J e 9 m f i f 1 8 l M d B 1 M u E w z g 5 I t F k W Z I C Y h s 7 9 J n y t k R o w t o U x x e y t h Q 6 o o M z a d k g 3 B X 3 5 5 l T Q v X L / q X t 5 X K 7 W b P I 4 i n M A p n I M P V 1 C D O 6 h D A x g M 4 B l e 4 c 0 R z o v z 7 n w s W g t O P n M M f + B 8 / g B O f I 0 q < / l a t e x i t >

f n < l a t e x i t s h a 1 _ b a s e 6 4 = " b 5 2 U V N S P / m 4 x B y F N e r w I t v P L s M 0 = " > A A A B 6 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m k o s e i F 4 8 V 7 Q e 0 o W y 2 k 3 b p Z h N 2 N 0 I J / Q l e P C j i 1 V / k z X / j t s 1 B W x 8 M P N 6 b Y W Z e k A i u j e t + O 4 W 1 9 Y 3 N r e J 2 a W d 3 b / + g f H j U 0 n G q G D Z Z L G L V C a h G w S U 2 D T c C O 4 l C G g U C 2 8 H 4 d u a 3 n 1 B p H s t H M 0 n Q j + h Q 8 p A z a q z 0 E P Z l v 1 x x q + 4 c Z J V 4 O a l A j k a / / N U b x C y N U B o m q N Z d z 0 2 M n 1 F l O B M 4 L f V S j Q l l Y z r E r q W S R q j 9 b H 7 q l J x Z Z U D C W N m S h s z V 3 x M Z j b S e R I H t j K g Z 6 W V v J v 7 n d V M T X v s Z l 0 l q U L L F o j A V x M R k 9 j c Z c I X M i I k l l C l u b y V s R B V l x q Z T s i F 4 y y + v k t Z F 1 a t V L + 9 r l f p N H k c R T u A U z s G D K 6 j D H T S g C Q y G 8 A y v 8 O Y I 5 8 V 5 d z 4 W r Q U n n z m G P 3 A + f w B P U I 3 T < / l a t e x i t >

F< l a t e x i t s h a 1 _ b a s e 6 4 = " Z O A R U C S 6 W T M k f n w p Q 7 R 3 1 h f C N O k = " > A A A B 6 H i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 K o m I e i w K 4 r E F + w F t K J v t p F 2 7 2 Y T d j V B C f 4 E X D 4 p 4 9 S d 5 8 9 + 4 b X P Q 1 g c D j / d m m J k X J I J r 4 7 r f z s r q 2 v r G Z m G r u L 2 z u 7 d f O j h s 6 j h V D B s s F r F q B 1 S j 4 B I b h h u B 7 U Q h j Q K B r W B 0 O / V b T 6 g 0 j + W D G S f o R 3 Q g e c g Z N V a q 3 / V K Z b f i z k C W i Z e T M u S o 9 U p f 3 X 7 M 0 g i l Y Y J q 3 f H c x P g Z V Y Y z g Z N i N 9 W Y U D a i A + x Y K m m E 2 s 9 m h 0 7 I q V X 6 J I y V L W n I T P 0 9 k d F I 6 3 E U 2 M 6 I m q F e 9 K b i f 1 4 n N e G 1 n 3 G Z p A Y l m y 8 K U 0 F M T K Z f k z 5 X y I w Y W 0 K Z 4 v Z W w o Z U U W Z s N k U b g r f 4 8 j J p n l e 8 y 4 p b v y h X b / I 4 C n A M J 3 A G H l x B F e 6 h B g 1 g g P A M r / D m P D o v z r v z M W 9 d c f K Z I / g D 5 / M H n B e M z w = = < / l a t e x i t >

Figure 3: The directed graphical model for calculating evidence.

According to the causal structure in Figure 3 and the basic principles in graphical models (Koller and Friedman, 2009), the evidence can be calculated analytically as follows:

p(y|F, α, β) =

p(w|α)

n i=1

p(yi|fi,

w,

β)dw

=

( 2βπ

) n2

( 2απ

) D2

e

−

α 2

w

T

w

−

β 2

|

|

F

w

−

y

||

2

dw

.

(1)

Equation 1 can be simpliﬁed by the identity

e−

1 2

(w

T

Aw

+b

T

w

+

c)

dw

=

(2|πA)|D e− 12 c+ 18 bT A−1b.

Taking the logarithm to make the equation simple, Equation 2 shows the logarithm of the

evidence L as a function of α, β, where A = αI + βF T F, m = βA−1F T y.

L(α, β)

=

log p(y|F, α, β)

=

n 2

log β

+

D 2

log α

−

n 2

log 2π

−

β 2

||F

m

−

y||22

−

α 2

mT

m

−

1 2

log |A|

(2)

3.2.2 Evidence maximization and LogME
An unresolved issue in Equation 2 is how to choose α, β. Gull (1989) suggested choosing α, β to maximize the evidence, i.e., use (α∗, β∗) = arg maxα,β L(α, β). Because m and A are

10

Ranking and Tuning Pre-trained Models

coupled, it is a diﬃcult problem to directly maximize L(α, β). Fortunately, MacKay (1992)

proposed a heuristic algorithm to solve the maximization problem: (1) set initial value of

α, β; (2) evaluate A, m, γ with given α, β: A = αI + βF T F, m = βA−1F T y, γ = D βσi2 ,

i=1 α+βσi2

where

σi

are

singular

values

of

F;

(3)

maximize

α, β

by

solving

∂L ∂α

=

0,

∂L ∂β

=

0

with

m, γ

ﬁxed, which yields α ← mTγm , β ← ||Fnm−−γy||22 . The algorithm is called MacKay’s algorithm (Algorithm 2), and has been heuristic as of its invention by MacKay (1992). Section 4.1 gives

the ﬁrst theoretical analysis for the convergence guarantee. Interestingly, the ﬁxed point

iteration used for the convergence analysis brings a new and faster algorithm for evidence

maximization (Algorithm 3). Please refer to Section 4.1 for details.

After the convergence of evidence maximization, the logarithm maximum evidence
L(α∗, β∗) is used to evaluate the compatibility between features and labels. Because L(α∗, β∗) scales linearly with n, we normalize it as L(αn∗,β∗) and term it LogME (logarithm of maximum evidence). Discussion on the inﬂuence of dimensionality D is presented in

Section 4.2. LogME can be intuitively interpreted as the logarithm of maximum label

evidence given pre-trained features.

Extending LogME to complex cases. The LogME approach starts from a singletarget regression. If the target problem is a multivariate regression task, i.e., Y ∈ Rn×C,

we can calculate LogME for each dimension c (1 ≤ c ≤ C) and average them over the

C dimension. If the target problem is a classiﬁcation task with C classes, Equation 1

cannot be calculated analytically (Daunizeau, 2017) with a categorical prior distribution.

State-of-the-art approximation methods like Laplace approximation (Immer et al., 2021)

work well in toy data, but perform unsatisfyingly in realistic tasks, as mentioned later in

Section 6.1. Therefore, we turn to an alternative solution: convert the classiﬁcation labels to

one-hot labels and treat the problem as multivariate regression. This approach also works

for multi-label classiﬁcation as well. This way, LogME can be used in both (single-label and

multi-label) classiﬁcation and regression tasks.

The overall algorithm of LogME is described in Algorithm 1.

3.2.3 Computational speedup
Although the Bayesian approach of maximum evidence has a rigorous theoretical explanation (Knuth et al., 2015), it inherits the drawback of Bayesian methods with high computational complexity. A na¨ıve implementation with Algorithm 2 results in a total complexity of O(CD3 + nCD2). For typical usage with D ≈ 103, n ≈ 104, C ≈ 103, the computational cost is 1013, with the wall-clock time comparable to ﬁne-tuning PTM φ.
Our conference paper (You et al., 2021) accelerated the computation by avoiding matrix inversion and matrix-matrix multiplication, as shown in Line 8 of Algorithm 2. In this paper, we present a convergence analysis of the MacKay’s algorithm by ﬁxed point iteration. It turns out that the analysis implies a faster algorithm for evidence maximization. The algorithm is presented in Algorithm 3 and its rationale is explained in Section 4.1.
Table 2 compares the complexity of calculating LogME with three implementations of evidence maximization. The na¨ıve implementation is biquadratic, You et al. (2021) made it cubic, and this paper further reduces the number of cubic terms. The optimized algorithm makes a time-consuming Bayesian approach fast enough, reducing the wall-clock time by

11

Kaichao You et al.

Algorithm 1 LogME

1: Input: Pre-trained model φ and target dataset D = {(xi, Yi)}ni=1 2: Output: Logarithm of Maximum Evidence (LogME)

3: Extract features using pre-trained model φ: F ∈ Rn×D, fi = φ(xi), Y ∈ Rn×C

4: Compute SVD of F : F = U ΣV T . Then F T F = V diag{σ2}V T

5: for dimension c = 1 to C do

6: Let y = Y (c) ∈ Rn,

7: Calculate the LogME value Lc by evidence maximization (Algorithm 2 or Algorithm 3).

8: end for

9:

Return

LogME

1 C

C c=1

Lc

Algorithm 2 Evidence Maximization by MacKay’s Algorithm

1: Input: Extracted features F ∈ Rn×D and corresponding labels y ∈ Rn

2: Output: Logarithm of Maximum Evidence (LogME) 3: Note: F has been pre-decomposed into F = U ΣV T

4: Initialize α = 1, β = 1

5: while α, β not converge do

6: Compute γ = D βσi2 , Λ = diag{(α + βσ2)}
i=1 α+βσi2
7: Na¨ıve: A = αI + βF T F, m = βA−1F T y

8: Optimized by You et al. (2021): m = β(V (Λ−1(V T (F T y))))

9: Update α ← mTγ m , β ← ||F nm−−γy||22 10: end while

11:

Compute

and

return

L=

1 n

L(

α

,

β

)

using

Equation

2

Algorithm 3 Evidence Maximization by Optimized Fixed Point Iteration

1: Input: Extracted features F ∈ Rn×D and corresponding labels y ∈ Rn

2: Output: Logarithm of Maximum Evidence (LogME)

3: Require: Truncated SVD of F : F = UrΣrVrT , with Ur ∈ Rn×r, Σr ∈ Rr×r, Vr ∈ RD×r.

4: Compute the ﬁrst r entries of z = UrT y

5: Compute the sum of remaining entries ∆ =

n i=r+1

zi2

=

n i=1

yi2

−

r i=1

zi2

6:

Initialize

α = 1, β

= 1, t =

α β

=1

7: while t not converge do

8: Compute mT m = ri=1 (tσ+i2σzi2i2)2 , γ = ri=1 t+σσi2 i2 , ||F m − y||22 =

9:

Update

α

←

mTγ m , β

←

||F nm−−γy||2 , t

=

α β

2

10: end while

r

zi2 + ∆

i=1 (1+σi2/t)2

11: Compute m = VrΣ z, where Σii = t+σσi 2 (1 ≤ i ≤ r).

i

12:

Compute

and

return

L=

1 n

L(

α

,

β

)

using

Equation

2

12

Ranking and Tuning Pre-trained Models

Table 2: The complexity of Algorithm 1 with three implementations of evidence maximization. n, C are the number of samples and the number of classes in classiﬁcation (or the number of target variables in regression) in downstream tasks, and D is the dimension of features produced by a pre-trained model.

Evidence maximization method
na¨ıve implementation optimized by You et al. (2021) ﬁxed point iteration (this paper)

Complexity per while-loop
O(D3 + nD2) O(D2 + nD)
O(n)

Overall complexity
O(nCD2 + CD3) O(nD2 + nCD + CD2 + D3)
O(nD2 + nCD)

order of 102 (see Section 6.5 for a quantitative measurement). Note that three implementation methods are functionally equivalent and only diﬀer in computational complexity. Therefore, the ﬁxed point iteration proposed in this paper is used by default in our implementation.
4. Theoretical analyses of LogME
In this section, we analyze two theoretical aspects of the proposed LogME, which further explains the rationality behind the LogME algorithm and why LogME works.
4.1 Convergence analysis of evidence maximization
Historical remarks: The evidence maximization procedure in Section 3.2.2 was ﬁrstly proposed by MacKay (1992) as a heuristic method to maximize the evidence of given data, following the spirit of empirical Bayesian learning (Bishop, 1995). Its theoretical analysis is missing and it remains heuristic in modern machine learning textbooks like Murphy (2012). In 2016, researchers (Li et al., 2016) revealed that if the predictive uncertainty β is known, the maximization over model uncertainty α can be viewed as a special instantiation of the EM algorithm (Dempster et al., 1977). However, pre-determining β is suboptimal, and in practice α, β are simultaneously maximized. To the best of our knowledge, we are the ﬁrst to analyze the MacKay’s algorithm when α, β are simultaneously optimized.
For the convenience of readers, we collect necessary notations here: n is the number of data examples; D is the size of feature dimensionality; F ∈ Rn×D is the feature matrix, with r = rank(F ) being its rank; y ∈ Rn is the label vector of data examples. We can immediately get r ≤ min{n, D}.
The key in our analysis is to take full advantage of the singular value decomposition of the feature matrix F = U ΣV T , where U ∈ Rn×n, V ∈ RD×D, and Σ ∈ Rn×D. Note that Σ only has r non-zero entries: Σii = σi > 0 (1 ≤ i ≤ r) where σi2 is the i-th largest eigenvalue of F T F and σi = 0 (r + 1 ≤ i ≤ max(n, D)). To simplify the expression, let z = U T y be the transformed y under orthogonal bases U , i.e. y = U z.
The MacKay’s algorithm (Algorithm 2) consists of a while-loop. The key to analyzing the whole algorithm is to analyze each iteration of the while-loop, which is listed in Algorithm 4 for readers’ convenience. During each iteration, new values α , β are computed based on old values α, β, which can be regarded as evaluating a vector-valued function (α , β ) = g(α, β).
13

Kaichao You et al.

Algorithm 4 One iteration of evidence maximization in Algorithm 2.

1: Input: α, β; Output: α , β for the next iteration.

2: Compute A = αI + βF T F, m = βA−1F T y, γ = 3: Return α = mTγ m , β = ||F nm−−γy||22

D βσi2 i=1 α+βσi2

The MacKay’s algorithm converges if and only if (α , β ) = (α, β) in Algorithm 4. With F, y as constants, the convergence of Algorithm 2 is equivalent to the existence of the ﬁxed point of the vector-valued function g, i.e., the existence of α, β such that (α, β) = g(α, β).
In general, ﬁxed points of vector-valued functions are diﬃcult to analyze and visualize. Fortunately, we ﬁnd that the vector-valued function (α , β ) = g(α, β) is homogeneous: g(kα, kβ) = kg(α, β), ∀k > 0. Let t = α/β, and t = α /β , the vector-valued function (α , β ) = g(α, β) induces a scalar function t = f (t), whose explicit form can be derived in Theorem 1. Evaluating g(α, β) is equivalent to calculating f ( αβ ), which is easier to analyze.

Theorem

1

Algorithm 4 induces a scalar function (Equation 3)

with

t=

α β

and

t

=

α β

.

n

zi2

t = f (t) = ( n

− 1)t2 i=1 (t+σi2)2

(3)

n − D σi2

n

σi2 zi2

i=1 t+σi2

i=1 (t+σi2)2

The proof is in Appendix B. Although f (t) seems very complicated and completely understanding its behavior is diﬃcult, surprisingly, the existence of f (t)’s ﬁxed point can be guaranteed with an interpretable condition, which is presented in the following Theorem 2.
Theorem 2 If r < n and 1≤i,j≤n(zi2 − zj2)(σi2 − σj2) > 0, then f (t) has a ﬁxed point and thus the MacKay’s algorithm will converge.

The proof is in Appendix C. Theorem 2 requires two conditions to guarantee the ﬁxed

point: r < n and 1≤i,j≤n(zi2 − zj2)(σi2 − σj2) > 0. The ﬁrst condition is easy to interpret and can be easily satisﬁed: usually n > D, and n > D ≥ r naturally holds. The condition

1≤i,j≤n(zi2 − zj2)(σi2 − σj2) > 0 is new in this paper.

Note that z

= UTy

and zi

=

U

T i

y

,

where Ui (the i-th column of U ) is the left-singular vector of the singular value σi, which

means that zi is the projection of label vector y in the direction of the left-singular vector

for the singular value σi. Intuitively speaking, 1≤i,j≤n(zi2 − zj2)(σi2 − σj2) > 0 requires zi2

to share roughly the same descending order as σi2. For larger σi2 (i.e. smaller i), it means

the projection of y in the corresponding left-singular vector should be larger, which can

be interpreted as a rigorous way to say that labels y are meaningful with respect to the

features F . We would like to emphasize that the requirement on the order of zi2 is soft: strict order zi2 ≥ zj2 ⇐⇒ i ≤ j ⇐⇒ σi2 ≥ σj2 certainly assures the convergence condition

1≤i,j≤n(zi2 − zj2)(σi2 − σj2) > 0, but as long as most zi2 follow the order, the condition can

be satisﬁed. We ﬁnd that all experiments in this paper admit the convergence condition, i.e.,

the evidence maximization algorithm is guaranteed to converge if the data is meaningful.

For example, Figure 4 plots f (t) on the CIFAR10 dataset, which clearly shows cross points

of f (t) and t, so the convergence condition 1≤i,j≤n(zi2 − zj2)(σi2 − σj2) > 0 holds.

14

Ranking and Tuning Pre-trained Models

ResNet-50

Inception v1

300 250 200 150 100 50
0 0
300 250 200 150 100 50
0 0
400
300
200
100
0 0
300 250 200 150 100 50
0 0

Class 1

100

200

100

200

300

250

200

150

100

50

0

300

0

300

250

200

150

100

50

0

300

0

400

300

200

100

100 200 300

100

200

0

400

0

300

250

200

150

100

50

0

300

0

Class 2

100

200

100

200

300

250

200

150

100

50

0

300

0

300

250

200

150

100

50

0

300

0

400

Class 3

100

200

100

200

300

250

200

150

100

50

0

300

0

300

250

200

150

100

50

0

300

0

400

300

300

200

200

100

100

100 200 300

100

200

0

400

0

300

250

200

150

100

50

0

300

0

100 200 300

100

200

t'=f(t)

t'=t

0

400

0

300

250

200

150

100

50

0

300

0

Class 4

100

200

300

100

200

300

100 200 300 400

100

200

300

MobileNet v2

NASNet-A Mobile

Figure 4: Fixed points of f (t) in Equation 3 for the ﬁrst 4 classes in CIFAR10 with 4 pre-trained models. The full ﬁgure for all the 10 classes in CIFAR10 with 5 pre-trained models can be found in Figure 13, which is omitted here to prettify the layout. We plot t = f (t) (in blue) and t = t (in orange), whose intersections are ﬁxed points f (t) = t. The existence of ﬁxed points guarantees the convergence of the MacKay’s algorithm for evidence maximization.

15

Kaichao You et al.

Make the ﬁxed point iteration faster. Note that the ﬁxed point iteration Equation 3 requires explicitly computing z = U T y with O(n2) storage and computation, which would be undesired if n is very large. To make it a practical algorithm, we take advantage of the fact that σi = 0 for i > r, and optimize the ﬁxed point iteration as follows:

t = f (t) = ( n−
=( n−
=( n−

n

− 1)t2

D σi2

i=1 t+σi2

n

− 1)t2

r

σi2

i=1 t+σi2

n

− 1)t2

r

σi2

i=1 t+σi2

n

zi2

i=1 (t+σi2)2

n

σi2 zi2

i=1 (t+σi2)2

r

zi2 + 1

i=1 (t+σi2)2 t2

n i=r+1

zi2

r

σi2 zi2

i=1 (t+σi2)2

r

zi2 + 1 (

i=1 (t+σi2)2 t2

n i=1

yi2

−

r

σi2 zi2

i=1 (t+σi2)2

r i=1

zi2)

Therefore, we can derive a faster algorithm (Equation 4) for the ﬁxed point iteration, which only requires the ﬁrst r entries of z without computing the full U matrix or the full z vector. This is the exact algorithm we implement in Algorithm 3.

t = f (t) = ( n

r

zi2 + 1 ( n y2 − r z2)

− 1)t2 i=1 (t+σi2)2 t2 i=1 i

i=1 i

(4)

n − r σi2

r

σi2 zi2

i=1 t+σi2

i=1 (t+σi2)2

4.2 Inﬂuence of dimensionality
In Section 3.2.2, we normalize the LogME value by the number of examples because Equation 2 scales linearly with n. The inﬂuence of feature dimension D is, however, unclear. In this section, we ﬁnd two cases (feature duplicate and feature padding) where LogME value remains unchanged when the feature dimension goes up without introducing more information. The two cases show the existence of inﬁnitely many features with arbitrary feature dimensions that share the same LogME value, therefore canceling the necessity of dimensionality normalization.

Corollary 3 (feature duplicate) LogME value will remain the same if the feature consists of arbitrary replicas of the original feature. Formally speaking, if the LogME value for F ∈ Rn×D and y ∈ Rn is L, then the LogME value for F˜ = [F, ..., F ] ∈ Rn×qD and y ∈ Rn is also L. (q ∈ N is a natural number to represent the number of replicas.)

Corollary 4 (feature padding) LogME value will remain the same if the feature is padded with an arbitrary number of zeros. Formally speaking, if the LogME value for F ∈ Rn×D and y ∈ Rn is L, then the LogME value for F˜ = [F, 0] ∈ Rn×(D+d) and y ∈ Rn is also L. (d ∈ N is a natural number and 0 ∈ Rn×d is a matrix with all zero entries.)

The proofs of Corollary 3 and Corollary 4 are in Appendix D and Appendix E, respectively. The core idea is to ﬁnd the closed-form relationship between decompositions of F˜ and F .

16

Ranking and Tuning Pre-trained Models
Corollary 3 and Corollary 4 imply that duplicating features or padding features with zeros will not change the value of LogME. LogME is capable of ﬁltering out redundant information in features, explaining its excellent empirical performance in You et al. (2021).
5. Tuning pre-trained models
The new paradigm we propose consists of ranking and tuning pre-trained models. Sections 3 and 4 described technical details in ranking pre-trained models, including the transferability metric LogME and its theoretical analyses. This section starts to focus on tuning pre-trained models, completing the rest part of the new paradigm.
We identify two possible scenarios of tuning pre-trained models: single best PTM tuning and multiple PTMs tuning. (1) Single best PTM tuning is suitable if there are no constraints on the network architecture, parameter count, or FLOPs of computation. These constraints are common in industrial applications, but are less important in academic research. Therefore, single best PTM tuning is common in academic research. Intuitively, the rest PTMs are inferior to the best ranked PTM so they would not help much. We refer readers to dedicated papers (Chen et al., 2019; Kou et al., 2020; You et al., 2020) on how to ﬁne-tune a single PTM, which is not the focus of this paper. (2) When we deploy neural networks in industrial applications, typically there are strict constraints on the budget of memory footprint or power consumption. Therefore, the pre-trained model φt satisfying these constraints is probably not the best ranked. Before this paper, practitioners could only ﬁne-tune φt, and the knowledge of the PTM hub {φi}M i=1 cannot be exploited. In this paper, we show that it is possible to transfer knowledge from several teacher PTMs {φk}Kk=1 to the target pre-trained model φt during ﬁne-tuning, which we call “multiple PTMs tuning”.
A side issue of tuning multiple PTMs is how to select the teacher PTMs. Typically, K < M , i.e., not all PTMs are necessary, since some PTMs may not be suitable for the target task and would hinder the transfer learning procedure. However, for M pre-trained models, the possible number of teacher combinations is O(2M ), which is impractical to enumerate. To overcome the exponential complexity, the PTM ranking can come to the rescue. With the PTM ranking, we can greedily select teacher PTMs following the rank. For example, if we want to choose K teacher PTMs, then the top-K ranked PTMs {φk}Kk=1 are the teacher of knowledge transfer. As for how to choose the hyper-parameter K (1 ≤ K ≤ M ), we give empirical guidelines in Section 6.3.
Multiple PTMs tuning oﬀers a unique advantage over simply ﬁne-tuning the target pre-trained model φt: if the speciﬁed target pre-trained model φt is not the best-ranked, we can still improve it by transferring knowledge from top-performing PTMs {φk}Kk=1.
5.1 Problem setup of multiple PTMs tuning
Now suppose we have selected K pre-trained models {φk}Kk=1, with each pre-trained model φk transforming input x into a Dk dimensional feature vector. In general, pre-trained models {φk}Kk=1 have various network architectures, and dimensionality of features {Dk}Kk=1 can vary. Let φt be the target architecture, which transforms the input into Dt dimensional feature vector. Formally, the multiple PTMs tuning problem is to ﬁne-tune a pre-trained model φt by leveraging selected pre-trained models {φk}Kk=1, as shown in Figure 2.
17

Kaichao You et al.

To ﬁne-tune a model φt in a target task, a new output head would be attached after φt, where target-speciﬁc loss is calculated. The target-speciﬁc head and loss are necessary for every possible solution to multiple PTMs tuning, which is taken cared of by a loss function Ltask. We will not elaborate on Ltask as it varies with tasks. Next we summarize existing approaches to the problem in Section 5.2 and introduce our method in Section 5.3.

5.2 Existing approaches to the problem of multiple PTMs tuning

A baseline approach is to ﬁne-tune φt without considering teacher PTMs {φk}Kk=1. This can serve as a baseline to measure the improvement brought by multiple PTMs tuning.

The knowledge distillation approach to multiple PTMs tuning is knowledge

distillation (Hinton et al., 2015) in the feature space via mean-square error. Since the feature

dimensions between φt and {φk}Kk=1 may be diﬀerent, a transformation module is necessary.

The knowledge distillation (KD) method takes advantage of selected pre-trained models by

adding

a

regularization

term

LKD

=

1 n

n1 i=1 K

K k=1

||φk(xi)

−

Wk φt (xi )||22 ,

where

Wk

is

learnable parameter to transform Dt dimensional feature φt(xi) into Dk dimensional vector

compatible with φk(xi). Even if Dk = Dt, the semantic of each dimension in φt and φk may

vary, making it necessary to introduce the transformation parameter Wk. The ﬁnal loss

is Ltask + λLKD, with hyper-parameter λ trading two terms. The KD method is another

simple but general baseline in multiple PTM tuning. It can be applied to various PTMs but

the performance improvement is limited.

Zoo-tuning for homogeneous PTMs tuning. In the special case when φt and {φk}Kk=1 all share the same network architecture, Zoo-tuning proposed by Shu et al. (2021) adaptively aggregates parameters of {φk}Kk=1 into φt in a layer-wise fashion. It does not modify the loss Ltask, but changes the training process by model aggregation. Zoo-tuning is
the current state-of-the-art method for homogeneous PTMs tuning, but it fails to deal with the heterogeneous scenario when architectures of φt and {φk}Kk=1 are diﬀerent.

5.3 B-Tuning: A Bayesian approach to multiple PTMs tuning
We draw lessons from the shortcomings of the aforementioned knowledge distillation approach and the Zoo-tuning approach. Knowledge distillation operates at the level of output features, which works for heterogeneous PTMs but aligning features across PTMs is not easy. Zootuning operates at the level of parameters (layers), thereby limiting itself in the homogeneous case. Taking advantages from both sides, our approach should operate at the level of features to hide the heterogeneity among PTMs, and should go beyond features to avoid explicitly aligning features from various pre-trained models. Inspired by the ranking metric (LogME), we propose a B-Tuning approach with the help of posterior predictive distribution in Bayesian regression.
Posterior predictive distribution is p(y |f, F, y) = w p(y |w, f )p(w|F, y)dw, which predicts the label y of incoming feature f conditioned on all the available training features F and labels y rather than just using f . With pre-computed α∗, β∗, m (byproducts of the LogME algorithm), p(y |w, f ) ∼ N (wT f, β∗−1) by deﬁnition, and p(w|F, y) = p(w)p(F,y|w) by
w p(w )p(F,y|w )dw
the Bayesian rule. Rasmussen (2003) show that p(w|F, y) ∼ N (β∗A−1F T y, A−1) with A = α∗I + β∗F T F . Plugging in the distributions of p(y |w, f ) and p(w|F, y), Rasmussen

18

Ranking and Tuning Pre-trained Models

(2003) show that p(y |f, F, y) ∼ N (f T m, f T A−1f + β∗−1) with m = β∗A−1F T y. In short, for extracted features F ∈ Rn×D and labels y ∈ Rn, the LogME algorithm gives α∗, β∗, m, and the posterior predictive distribution is p(y |f, F, y) ∼ N (f T m, f T A−1f + β∗−1). The
derivation details of posterior predictive distribution can be found in Rasmussen (2003).

X Training data

Labels 𝑦

𝜙1

𝑓

𝐹1

1

Input
𝑥

𝜙𝐾
PTMs (fixed)

𝐹𝐾 𝑓𝐾 Training features
Input features

𝑝 𝑦′ ∣ 𝑓, 𝐹, 𝑦 ∼ 𝒩 𝑓𝑇𝑚, 𝑓𝑇𝐴−1𝑓 + 𝛽∗−1
𝑚1 𝑦1′

Weighted Avg
𝑦ത′

𝑚𝐾

𝑦′

𝐾

Expectation Alignment

𝐿𝐵𝑎𝑦𝑒𝑠𝑖𝑎𝑛

Posteorior Predictive Distribution Estimation

𝜙𝑡

𝐹𝑡 𝑓

𝑚𝑡

𝑡

Target model

𝑦𝑡′ Predicted distribution

ℎ𝑡

𝐿𝑡𝑎𝑠𝑘

Figure 5: Illustration of B-Tuning. Dashed lines are pre-calculated before tuning.

Posterior predictive distribution depends on the training data (F, y) and incoming feature

f . Let Fk be features extracted by the pre-trained model φk, fk = φk(x) be the output feature

of the current data point extracted by the pre-trained model φk, then each pre-trained model

can produce a posterior predictive distribution p(yk|fk, Fk, y) ∼ N (fkT mk, fkT A−k 1fk + βk∗−1).

How to combining these distributions? We propose to mix them according to their LogME

values {Lk}Kk=1 as mixture of Gaussians y¯ =

K k=1

πk

yk

where

πk

=

exp(Lk /t)
K

and t is

j=1 exp(Lj /t)

the temperature hyper-parameter (Hinton et al., 2015) that can be adjusted according to

the diﬀerence of LogME values. Although {yk}Kk=1 admit simple Gaussian distributions, the exact distribution of mixed y¯ is untractable because features Fk come from the same

dataset and {yk}Kk=1 are dependent. Nevertheless, according to the linearity property of

expectation, Ey¯ =

K k=1

πk Eyk

=

K k=1

πkfkT mk,

the

expectation

of

y¯

is known.

For the target model φt, the posterior predictive distribution is deﬁned as p(yt|ft, Ft, y) ∼

N (ftT mt, ftT A−t 1ft + βt∗−1). Since y¯ can be regarded as prior knowledge from pre-trained

models {φk}Kk=1, we can align the expectation of yt and y¯ as a regularization term LBayesian =

1 n

n i=1

||Ey¯

− Eyt||22.

Note

that

the

expectation

is

taken

over

the

predictive

distributions

and can be calculated analytically. Extending the formula to multiple classes, the ﬁnal

expression of the regularization term is

LBayesian = n1 n C1 C ( K πkfkT mk,c − ftT mt,c)2, (5)
i=1 c=1 k=1

where mk,c, mt,c are calculated by the LogME algorithm and are ﬁxed during training. The ﬁnal loss would be Ltask + λLBayesian, with λ to trade-oﬀ two terms. Because the method

19

Kaichao You et al.

depends on the Bayesian approach of calculating posterior predictive distribution, we call it Bayesian Tuning, or B-Tuning. Figure 5 describes the method and the computation graph. Only φt is updated during B-Tuning while the teacher PTMs {φk}Kk=1 are ﬁxed.
B-Tuning has two advantages over previous methods. (1) It hides the heterogeneity among PTMs by operating at the level of features, characterizing itself as a general solution to multiple PTMs tuning in both homogeneous and heterogeneous cases. (2) Despite its heavy mathematical derivation, B-Tuning has a simple interpretation: it aligns features adaptively with m serving as an attention-like mechanism, canceling the necessity of learning to transform features into a shared space like the knowledge distillation approach. The superiority of B-Tuning in multiple PTMs tuning is empirically demonstrated in Section 6.3.

6. Experiments
This paper presents comprehensive experiments. Section 6.1 illustrates the behavior of LogME on toy problems. Experiments on ranking PTMs and tuning PTMs are in Section 6.2 and Section 6.3 respectively, demonstrating the power of the proposed new paradigm. Section 6.5 quantitatively measures the eﬃciency of LogME and Section 6.6 compares LogME against a common approach of re-training head over a ﬁxed feature extractor, providing a comprehensive understanding of LogME. Original data for some ﬁgures are available in the appendix. Code for LogME is available at https://github.com/thuml/LogME.

6.1 Illustration with toy data
To give readers an intuitive sense of how LogME works, we generate features with increasing noise to mimic the features extracted by pre-trained models with decreasing transferability and to check if LogME can measure the quality of features.

LogME LogME

0.0 −0.2 −0.4 −0.6 −0.8 −1.0
0

2
LogME

Laplace Approximation

1

0

−1

−2

−3

5

10

15

20

25

−40

Standard deviation of noise

LogME Laplace Approximation

1

2

3

4

Standard deviation of noise

Figure 6: Toy data show that LogME value goes down with decreasing feature quality. Laplace Approximation means the LogME value calculated by Immer et al. (2021) using Laplace approximation.

For classiﬁcation (Figure 6 left), three clusters in a 2-D plane are generated, with colors representing the categories. Initially, the features are well separable so LogME has a large value. Then we add Gaussian noise with increasing variance to the data and the clustering structure in feature space disappears, leading to smaller LogME values as expected.
For regression (Figure 6 right), x is uniformly distributed (x ∼ U[0, 1]) and the output y = 2x + with observation error ∼ N (0, 0.12). By adding noise to the feature x =

20

Ranking and Tuning Pre-trained Models
x + N (0, t2), the quality of feature x becomes worse and it is harder to predict y from x . With larger t (the standard deviation of noise), LogME becomes smaller as expected.
These toy experiments on synthesized data show that LogME is a good measure of the feature quality, and therefore can provide a good ranking for PTMs in a pre-trained model hub. The solid theoretical foundation behind LogME also brings an eﬀective B-Tuning method for multiple PTMs tuning. Together they make LogME a central technique in the new paradigm of ranking and tuning pre-trained models.
Figure 6 also shows the LogME value calculated by Immer et al. (2021) using Laplace approximation. In this toy experiment, both LogME and Laplace approximation correctly measure the trend of feature quality. In regression, the Laplace Approximation is strictly lower than LogME; in classiﬁcation, Laplace approximation uses a categorical prior and approximates the marginal likelihood, while LogME converts classiﬁcation labels to one-hot labels (with a Gaussian prior) and calculates the exact value without approximation. The left plot in Figure 6 conﬁrms that both approaches can reﬂect the trend of feature quality. However, we notice that Laplace approximation has larger ﬂuctuations than LogME, and the Laplace approximation requires more computation than LogME. In addition, its performance in realistic data (Section 6.2.1) is not satisfying. Therefore, when dealing with classiﬁcation data, we convert classiﬁcation labels to one-hot labels and treat the problem as a multivariate regression problem in LogME. How to analytically calculate the value with a categorical prior is left as a future research question.
6.2 Ranking pre-trained models
This section focuses on the ﬁrst part of the proposed paradigm: ranking pre-trained models. The goal is to rank pre-trained models so that potentially best PTMs can be selected for the subsequent tuning process. This section attaches great importance to the diversity of pre-trained models and downstream tasks. Section 6.2.1 and Section 6.2.2 transfer supervised pre-trained models to classiﬁcation and regression tasks, respectively; Section 6.2.3 explores unsupervised pre-trained models on both classiﬁcation and regression; Section 6.2.4 and Section 6.2.5 study pre-trained language models on language understanding tasks and a sequential tagging task, respectively. These extensive experiments demonstrate the generality and eﬀectiveness of the proposed LogME in ranking pre-trained models.
6.2.1 Ranking supervised pre-trained models in classification tasks
We use 12 ImageNet pre-trained models available from PyTorch: Inception V1 (Szegedy et al., 2015), Inception V3 (Szegedy et al., 2016), ResNet 34 (He et al., 2016), ResNet 50 (He et al., 2016), ResNet 101 (He et al., 2016), ResNet 152 (He et al., 2016), Wide ResNet 50 (Zagoruyko and Komodakis, 2017), DenseNet 121 (Huang et al., 2017), DenseNet 169 (Huang et al., 2017), DenseNet 201 (Huang et al., 2017), MobileNet V2 (Sandler et al., 2018), and NASNet-A Mobile (Tan et al., 2019). These pre-trained models cover most of the supervised pre-trained models in transfer learning that practitioners frequently use.
For downstream classiﬁcation tasks, we take 9 commonly used datasets: Aircraft (Maji et al., 2013), Birdsnap (Berg et al., 2014), Caltech (Fei-Fei et al., 2004), Cars (Krause et al., 2013), CIFAR10 (Krizhevsky and Hinton, 2009), CIFAR100 (Krizhevsky and Hinton, 2009),
21

Kaichao You et al.

LEEP

Inception v1 Incetpion v3

ResNet 34 ResNet 50

ResNet 101 ResNet 152

Wide ResNet 50 DenseNet 121

DenseNet 169 DenseNet 201

MobileNet v2 NASNet A Mobile

Aircraft (0.13)
−0.3

−0.4

−0.5

−0.6

−0.7

−0.8

75

80

85

Birdsnap (0.19) Caltech (0.30)

−2.0

−1.6

−1.4

−1.8

−2.2

−1.6

−2.0 −2.4 −1.8

−2.2

−2.6

−2.0

−2.4

60

65

70

75

90

92

94

Cars (0.26)

CIFAR10 (0.72) CIFAR100 (0.66)
−3.0 −3.2

DTD (−0.06)

Pets (0.66)

−0.9

−2.5

−3.2

−3.7

−1.0

−2.6

−3.4

−3.4 −3.6 −3.8 −1.1 −2.7

−1.2

−2.8

−3.6 −3.8 −3.9 −2.9

−1.3

−3.8

−4.0

−4.0

−3.0

−1.4

−4.0

−4.2

−4.1

−1.5

−3.1

88

90

92

96.0 96.5 97.0 97.5 98.0

82

84

86

70

72

74

76

90

92

94

SUN (0.54)

62

64

66

Aircraft (0.39)

−0.25

−0.30

−0.35

−0.40

−0.45

75

80

85

Birdsnap (0.51) Caltech (0.69) −1.10

−1.4

−1.75

−1.15

−1.5

−1.80

−1.85

−1.20

−1.6

−1.90

−1.25

−1.7 −1.95 −1.30

−1.8

−2.00

60

65

70

75

90

92

94

Cars (0.36)

CIFAR10 (0.51) CIFAR100 (0.53) −3.075 DTD (−0.35) −0.85

−3.1

−3.0

−3.100

−0.90

−3.2

−3.1

−3.125

−0.95

−3.3

−3.150

−3.2 −3.175

−1.00

−3.4

−3.3

−3.200

−1.05

−3.5

−3.225

−1.10

−3.6

−3.4

−3.250

−1.15

Pets (0.83)

−2.450 −2.475 −2.500 −2.525 −2.550 −2.575

88

90

92

96.0 96.5 97.0 97.5 98.0

82

84

86

70

72

74

76

90

92

94

SUN (0.68)

62

64

66

Aircraft (0.59)

0.950

0.945

0.940

0.935

0.930

75

80

85

0.85 Birdsnap (0.66) Caltech (0.66)

1.60

1.2600

0.84

1.2575

1.55

1.2550

0.83

1.50

1.2525

0.82

1.45

1.2500

0.81

1.40

1.2475

1.2450

0.80 60

65

70

75

1.35

90

92

94

Cars (0.69)

88

90

92

CIFAR10 (0.82) CIFAR100 (0.77)
0.475

0.450

1.12

0.425 1.10
0.400

0.375

1.08

0.350

1.06

0.325 1.04
0.300

96.0 96.5 97.0 97.5 98.0

82

84

86

0.76 0.75 0.74 0.73 0.72 0.71
70

DTD (0.50)

72

74

76

Pets (0.61)
1.2 1.1

SUN (0.71)
1.75 1.74

1.0

1.73

1.72 0.9
1.71

90

92

94

62

64

66

NCE

LogME

Figure 7: Correlation values (τw) between ﬁne-tuned accuracy (X-axis) and scores produced by three methods (Y-axis) for ranking PTMs on 9 datasets with 12 pre-trained models. One row for each method, one column for each dataset (with τw in the parenthesis near the dataset name), and one marker for each pre-trained model. The best τw in each dataset is marked in bold.

DTD (Cimpoi et al., 2014), Pets (Parkhi et al., 2012), and SUN (Xiao et al., 2010). The description of each dataset and data statistics are listed in Appendix F.
For all the datasets we use, we respect the oﬃcial train/val/test splits if they exist, otherwise we use 60% data for training, 20% data for validation (searching hyper-parameters to measure the reference transfer learning performance) and 20% data for testing. Models are trained with a ﬁxed number of epochs, and the best model in the validation split is used as the ﬁnal model to be tested in the test split.
To compute the reference transfer learning performance {Tm}M m=1 (M = 12), we carefully ﬁne-tune pre-trained models with grid-search of hyper-parameters. Li et al. (2020) pointed out that learning rate and weight decay are the two most important hyper-parameters. Hence we grid search learning rate and weight decay (7 learning rates from 10−1 to 10−4, 7 weight decays from 10−6 to 10−3, all logarithmically spaced) to select the best hyper-parameter on the validation split and compute the accuracy on the test split as the reference transfer learning performance. It is noteworthy that LogME requires neither ﬁne-tuning nor grid search. Here we ﬁne-tune pre-trained models to see how well do LogME values correlate with the reference transfer performance, but practitioners can straightforwardly use LogME to evaluate pre-trained models without ﬁne-tuning.
We compare LogME against LEEP (Nguyen et al., 2020) and NCE (Tran et al., 2019). Results of calculating the evidence using Laplace approximation (Immer et al., 2021) are not shown but listed in the appendix, because its performance is unsatisfying. Before this paper, LEEP and NCE were the only two methods to rank PTMs without ﬁne-tuning, and they can only rank supervised pre-trained models in classiﬁcation tasks. We use LEEP, NCE, and
22

Ranking and Tuning Pre-trained Models

LogME to compute scores {Sm}M m=1 by applying 12 pre-trained models to the datasets. The

correlation values τw between scores and ﬁne-tuned accuracies are presented in Figure 7.

We can ﬁnd that LogME has consistently better correlation than LEEP, and outperforms

NCE on most datasets (7 datasets out of 9 datasets). Note that LEEP and NCE even

show negative correlation values in DTD (Cimpoi et al., 2014), because they rely on the

relationship between classes of the pre-trained task and the target task but DTD classes

(textures) are very diﬀerent from ImageNet categories (objects). In contrast, LogME still

performs reasonably well for DTD.

According to the interpretation of τw in Section 3.1, correlation value τw can be roughly

translated

into

τw +1 2

probability

of

correct

comparison

(concordant

pairs).

The

smallest

τw

of LogME in Figure 7 is around 0.5, so the probability of a pre-trained model φA transferring

better than φB is about 75% if φA has a larger LogME. For most tasks τw of LogME is 0.7

or 0.8, so the probability of correct selection is 85% or 90%, suﬃcient for practical usage.

6.2.2 Ranking supervised pre-trained models in a regression task
Besides extensive classiﬁcation tasks considered above, this section shows how LogME can assess pre-trained models for a regression task. The two prior methods (LEEP and NCE) depend on the category relationship between pre-trained categories and downstream categories, therefore they do not apply to regression tasks.

LogME

1.7

ResNet 34

DenseNet 169

ResNet 50

DenseNet 201

1.6

ResNet 101

Inception v1

ResNet 152

Incetpion v3

Wide ResNet 50

MobileNet v2

1.5

DenseNet 121

NASNet-A Mobile

1.4

1.3

1.2

1.1

0.025

0.030

0.035 MSE

0.040

0.045

Figure 8: Supervised pre-trained models transferred to dSprites.

The regression task we use is the dSprites (Matthey et al., 2017) dataset from the Visual Task Adaptation Benchmark (Zhai et al., 2020), a common benchmark for evaluating the quality of learned representations. The input is an image containing a sprite (heart, square, and ellipse) with varying scale/orientation/position. Pre-trained models are transferred to predict four scalars (scale, orientation, and (x, y) positions) together, and mean square error (MSE) on the test data is reported. The supervised pre-trained models and the hyper-parameter tuning scheme are the same as in Section 6.2.1.
Results are plotted in Figure 8. It is clear that LogME and MSE are well correlated and the correlation coeﬃcient τw = 0.79 is very large: if a pre-trained model φA has larger LogME than φB, with roughly 89.5% probability φA is better (has smaller MSE) than φB after actually ﬁne-tuning.

23

Kaichao You et al.

6.2.3 Ranking contrastive pre-trained models in downstream tasks

The recently emerged unsupervised pre-trained models (He et al., 2020) have attracted much attention due to their potential ability to exploit humongous unlabeled data on the Internet. They use the contrastive loss (Gutmann and Hyva¨rinen, 2010) to inject supervision signals into pre-training with unlabeled data, and they feature a projection head with continuous output. Ranking contrastive pre-trained models is an emerging demand, but LEEP and NCE cannot be extended to deal with the projection head of contrastive-based unsupervised pre-trained models because they rely on discrete category relationship.
Since LogME only requires features extracted from pre-trained models, it can be applied to contrastive pre-trained models. To demonstrate this, we use four popular models pretrained with various training schemes: MoCo V1 (He et al., 2020) with momentum contrast, MoCo V2 (Chen et al., 2020b) with an MLP projection head and strong data augmentation, MoCo 800 trained with 800 epochs as suggested by Chen et al. (2020a), and SimCLR (Chen et al., 2020a) trained by a carefully designed training scheme (Chen et al., 2020a).
For classiﬁcation, we use Aircraft (Maji et al., 2013), the ﬁrst dataset (alphabetically) in Section 6.2.1; for regression, we use dSprites (Matthey et al., 2017), the only regression task in this paper. Results are shown in Table 3. SimCLR on dSprites is not reported as it does not converge after several trials, possibly because it is heavily tailored to classiﬁcation tasks. LogME gives the perfect order of both accuracy and MSE. Note that the reference order on transfer learning performance in Aircraft (MoCo V1 < MoCo V2 < MoCo 800) is diﬀerent from the order in dSprites (MoCo V1 < MoCo 800 < MoCo V2), emphasizing that ranking pre-trained models is task adaptive. We also observe that LogME values of unsupervised pre-trained models are similar (the diﬀerence is smaller than their supervised counterparts in Section 6.2.1), mainly because unsupervised features are not very discriminative.

Table 3: Use LogME to rank unsupervised pre-trained models.

PTM
MoCo V1 MoCo V2 MoCo 800 SimCLR

Aircraft

Accuracy (%) LogME

81.68 84.16 86.99 88.10

0.934 0.941 0.946 0.950

τw: 1.0

dSprites

MSE LogME

0.069 1.52

0.047 1.64

0.050 1.58

-

-

τw: 1.0

6.2.4 Ranking pre-trained language models in the GLUE benchmark
To further demonstrate the generality of LogME, we show how LogME can work for pretrained language models. Again, existing methods (LEEP and NCE) cannot deal with these pre-trained language models.
Here we take another approach to evaluating the reference transfer performance {Tm}M m=1. We do not ﬁne-tune pre-trained models, but directly take ﬁne-tuned results from HuggingFace Models, and check if LogME values can correlate well with the results. Speciﬁcally, we take pre-trained models that have GLUE performance tuned by the HuggingFace organization, and select the top 8 downloaded models: RoBERTa (Liu et al., 2019), RoBERTa-D, uncased
24

Ranking and Tuning Pre-trained Models

LogME

MNLI (0.66)

−0.58

−0.60

−0.62

−0.64

−0.66

80

82

84

86

QQP (0.73)

−0.47

−0.48

−0.49

−0.50

−0.51

−0.52

88

89

90

91

−0.57

−0.58

−0.59

−0.60

−0.61

−0.62

92

88

QNLI (1.00)

90

92

SST-2 (0.68)
−0.35 −0.40 −0.45 −0.50

CoLA (1.00)
−0.50
−0.52
−0.54
−0.56

91

92

93

94

95

50

55

60

MRPC (0.53)

RTE (1.00)

−0.710

−0.58

−0.715

−0.59

−0.720

−0.60

−0.725

86

88

90

60

65

70

75

Popularity

MNLI (0.28)
6.000

5.000

4.000

3.000

2.000

1.000

0.000

80

82

84

86

QQP (0.00)
6.000

5.000

4.000

3.000

2.000

1.000

88

89

90

91

6.000 5.000 4.000 3.000 2.000 1.000 92 88

QNLI (0.00)

90

92

SST-2 (0.38)
6.000

6.000

5.000

5.000 4.000 3.000 2.000 1.000

4.000

3.000

2.000

1.000

91

92

93

94

95

RoBERTa

RoBERTa-D

uncased BERT-D

cased BERT-D

ALBERT-v1

CoLA (0.00)

50

55

60

ALBERT-v2

MRPC (0.33)

RTE (−0.29)

6.000

6.000

5.000

5.000

4.000

4.000

3.000

3.000

2.000

2.000

1.000

1.000

86

88

90

60

65

70

75

ELECTRA-base

ELECTRA-small

Figure 9: Correlation values (τw) between ﬁne-tuned accuracy (X-axis) and LogME value / Popularity value (Y-axis) in 7 GLUE tasks with 8 popular PTMs. One sub-ﬁgure for each task (with its τw in the parenthesis), and one marker for each PTM.

BERT-D, cased BERT-D, ALBERT-v1 (Lan et al., 2020), ALBERT-v2 (Lan et al., 2020), ELECTRA-base (Clark et al., 2020), and ELECTRA-small (Clark et al., 2020) (“D” means distilled version). The LogME values on 7 GLUE tasks together with ﬁne-tuned accuracies are plotted in Figure 9. Some models only have results for certain tasks and we keep them as they are. Even though these accuracy numbers are tuned by the HuggingFace organization, LogME perfectly estimates the ranking of transfer performance for 3 tasks (with τw = 1), showing the surprising eﬀectiveness of LogME in ranking pre-trained models.
One may wonder how well does a pre-trained model’s popularity indicate its transfer learning performance, because it is a common belief that PTMs with consistent improvements across many tasks may tend to become popular. To answer this question, a quantitative measurement of popularity is required. Two possible quantities are considered: the citation number of the paper proposing the pre-trained model, and the download count of the pre-trained model. The paper citation number is not a proper metric for assessing individual PTM’s transferability, because one paper can contain many PTMs. For example, the BERT paper (Devlin et al., 2019) contains BERT-base and BERT-large, they have the same citation number but are in diﬀerent transferability levels. Download count is a PTM-wise well-deﬁned metric, hence we can use it as a proxy for popularity.
Thanks to the public data from HuggingFace, each PTM’s download count (measured in millions) is available to approximate the popularity. The bottom ﬁgure in Figure 9 shows how well popularity performs when it is used as a transferability metric. It is clear that popularity does not correlate well with transfer learning performance: the τw values of popularity are signiﬁcantly lower than LogME’s τw values, and negative correlation value occurs in the RTE task. Note that BERT models are the most popular but RoBERTa is the best among these tasks, revealing a mismatch between popularity and transfer learning performance. The above experiments again echo with the motivation of this paper: practitioners usually select the most popular pre-trained model due to the lack of a satisfying selection strategy, and LogME can come to their rescue.
25

Kaichao You et al.

6.2.5 Ranking pre-trained language models in a sequential tagging task
So far, we have only considered simple classiﬁcation and regression tasks. It would be valuable to extend LogME to tasks with structured output such as object detection and semantic segmentation. Next we show how LogME can be used in a sequential tagging task where both the input and the output are structured. How to deal with a general task with structured output is left as future work.
The speciﬁc task we consider in this section is named entity recognition (Sang and De Meulder, 2003). It requires the model to predict the entity label (person, location, organization, etc.) of every token in a sentence, therefore the output is structured. Considering that the named entity recognition task is sometimes referred to as “token-level classiﬁcation”, we can ﬂatten the token dimension to apply LogME. The only change is that n represents the number of tokens rather than the number of sentences.
We use the same PTMs as in Section 6.2.4, and the dataset is CoNLL-2003 (Sang and De Meulder, 2003) whose performance is measured by F-1 score. Table 4 holds the results. The rank correlation value τw is 0.20, smaller than results in previous sections. The small τw is caused by an outlier PTM named RoBERTa (Liu et al., 2019), which has the largest F-1 score with a relatively small LogME value. We conjecture that RoBERTa has a small LogME value because it is trained much longer than BERT in the masked language modeling task, which might make its representation tailored to the task, lowering its LogME score in the dissimilar task of named entity recognition. On the other hand, RoBERTa is robustly optimized, so it can be easily ﬁne-tuned to downstream tasks with competitive results.
If we select the best PTM by the largest LogME value, ALBERT-v1 will be used and its performance is comparable to the best (97.0% vs. 97.4%). From this perspective, LogME is decently useful. In general, how to deal with structured tasks better would be a research problem requiring further eﬀorts.

Table 4: Ranking pre-trained models in named entity recognition (CoNLL-2003 task).

PTM RoBERTa RoBERTa-D uncased BERT-D cased BERT-D ALBERT-v1 ALBERT-v2 ELECTRA-base ELECTRA-small τw

F-1 score (%) 97.4

96.6

96.8

95.5

97.0

97.4

97.2

91.9

LogME

0.685

0.723

0.783

0.623

0.834

0.809

0.746

0.646

0.20

6.3 Tuning pre-trained models
This section turns to the second part of the proposed paradigm: tuning pre-trained models. As mentioned in Section 5, most academic researchers are not constrained by the inference cost of deployed models, and they can use the best-ranked (according to the LogME value) PTM straightforward. This paper is concerned about the practical usage scenario, where computational constraints require us to use a speciﬁc PTM but we still want to leverage the knowledge from other PTMs in the pre-trained model hub.
Experiments in this section are designed to compare three methods of tuning multiple PTMs: the knowledge distillation approach, the Zoo-tuning approach and the proposed B-Tuning. We ﬁrst conduct experiments with multiple homogeneous PTMs where all three methods are applicable, then we dive into the practical case of multiple heterogeneous PTMs. By default, the temperature scaling hyper-parameter t is set to 0.1 in Equation 5.
26

Ranking and Tuning Pre-trained Models

6.3.1 Tuning multiple homogeneous PTMs
We use ﬁve homogeneous pre-trained models following the experimental setup of Zootuning (Shu et al., 2021). They are ResNet-50 models trained by diﬀerent pre-training tasks: (1) Supervised pre-trained on ImageNet (He et al., 2016); (2) Unsupervised pre-trained by MoCo (He et al., 2020); (3) MaskRCNN model (He et al., 2017); (4) DeepLab V3 (Chen et al., 2017); (5) KeyPoint detection model pre-trained on COCO (Lin et al., 2014). The dataset we use is Aircraft (Maji et al., 2013), the ﬁrst dataset (alphabetically) in Section 6.2.1. The target model is ResNet-50 pre-trained in ImageNet, following the setting of Shu et al. (2021).
To demonstrate the eﬀectiveness of B-Tuning, we use all ﬁve PTMs as the teacher models, and report the performance of three methods (B-Tuning, knowledge distillation, and Zoo-tuning) on multiple PTM tuning in the ﬁrst row of Table 5. Zoo-tuning performs better than vanilla knowledge distillation, and the proposed B-Tuning even surpasses Zoo-tuning, setting a new state-of-the-art result for multiple PTMs tuning.

Table 5: Accuracy (%) of multiple PTMs tuning in Aircraft, with diﬀerent teacher models and tuning methods. As a baseline, single PTM ﬁne-tuning yields 82.99% accuracy.

method teacher models all PTMs from the PTM hub top-3 PTMs (ranked by LogME)

Knowledge Distillation
82.97 ± 0.27 84.29 ± 0.30

Zoo-tuning
83.32 ± 0.32 -

B-Tuning
83.49 ± 0.17 85.12 ± 0.15

To demonstrate the eﬀectiveness of LogME selection in multiple PTM tuning, we rank ﬁve PTMs by LogME, and select the top-K PTMs as the teacher models in the subsequent tuning. Shu et al. (2021) used all ﬁve PTMs to tune the target PTM, since they do not investigate how to select PTMs. To suﬃciently test the eﬀect of selection, we choose K = arg max3≤K≤5(5K) = 3, so that there are many possible selections and later we can explore how optimal LogME selection is. The results are in the second row of Table 5. Surprisingly, selecting top-3 PTMs brings a signiﬁcant performance improvement, demonstrating the eﬀectiveness of the “ranking and tuning pre-trained models” paradigm.
To evaluate the optimality of LogME selection, we try all the (53) = 10 combinations of selecting 3 PTMs from 5 PTMs. Vanilla knowledge distillation is used to avoid confounders. Results are shown in Figure 10, with the accuracy of ﬁne-tuning a single ResNet-50 as the baseline. We have two observations from Figure 10: (1) Transferring the knowledge from multiple PTMs consistently outperforms ﬁne-tuning a single pre-trained model (82.99%), which adheres to our intuition that utilizing the rich knowledge from various PTMs is better than ﬁne-tuning alone. (2) The best combination achieves 84.41% accuracy, but usually it is too expensive to try all the combinations (10 trials). Instead, we can use LogME to select the top-3 PTMs, which achieves 84.29% accuracy and is the second best. Moreover, we can select the top-3 PTMs by LogME and then perform B-Tuning, which even surpasses the best combination and has an accuracy of 85.12%.
We can draw three conclusions from experiments in this section: (1) multiple PTMs tuning is better than single PTM ﬁne-tuning; (2) it is better (near-optimal among all the possible selections) to select top-ranked PTMs according to LogME than to use all the PTMs; (3) B-Tuning is superior to knowledge distillation and Zoo-tuning.

27

Kaichao You et al.

ImageNet Sup. MaskRCNN PT. MoCo PT. KeyPoint PT. DeepLab PT.
PTM name

Accuracy / %

84.41
82.99 82.5

84.29

84.12

83.95 83.78 83.66 ResNet-50 Fine-tune

83.66

83.48

83.43

83.24

0.947 0.936 0.934 0.914 0.913

LogME

Figure 10: Accuracy of knowledge distillation with 3 PTM teachers. All (53) = 10 combinations of selecting 3 PTM teachers are reported. Selecting top-3 PTMs according
to LogME achieves the second best performance among 10 combinations.

In addition, we would like to point out that selection based on LogME value is a greedy procedure, which could fail to capture the complicated high-order interaction among PTMs. For example, in Figure 10, DeepLab pre-trained model has the lowest LogME value, but it appears in the best combination. How to analyze the high-order interaction among PTMs would be a worthwhile research question in the future. The goal of this section is to show the greedy selection based on LogME value has a reasonably good performance.
6.3.2 Tuning multiple heterogeneous PTMs
Section 6.3.1 studies multiple PTMs tuning with homogeneous models, which follows the setting of Shu et al. (2021) and demonstrates the superiority of B-Tuning. Nonetheless, compared with tuning multiple homogeneous PTMs, a more general and more attractive application of multiple PTMs tuning is to transfer knowledge from a large hub of heterogeneous PTMs. This section focuses on the latter setting, and provides some guidelines on how to select a proper number of PTMs (i.e., the hyper-parameter K) as teachers.
The alphabetically ﬁrst and second datasets (Aircraft and Birdsnap) are chosen and the PTM hub consists of the 12 PTMs used in Section 6.2.1. The 12 PTMs are ranked by their LogME values, and the target PTM φt is the most common ResNet-50. Top-K PTMs are used in B-Tuning to ﬁne-tune the target model, with K varying from 1 to 12. Results are plotted in Figure 11, where the X-axis is the value of K.
We conclude the following observations from Figure 11: (1) B-Tuning with multiple PTMs is consistently better than single PTM ﬁne-tuning. (2) B-Tuning with all the 12 PTMs does not yield the best accuracy, which emphasizes the importance of selecting proper PTMs. (3) The trend of accuracy with respect to K is complicated, and how to select an optimal K can be a valuable topic for future researchers.
28

Ranking and Tuning Pre-trained Models

Accuracy / %

86.0 85.5 85.0 84.5 84.0 83.5 83.0 82.5
1
74.00 73.00 72.00 71.00 70.00 69.00 68.00 67.00
1

Aircraft
85.9 85.5 85.2

Single Model Fine-tune

2

3

4

5

6

7

8

9

10

11

12

Number of Teachers (K)

Birdsnap

73.0

73.3

72.3

72.2

Single Model Fine-tune

2

3

4

5

6

7

8

9

10

11

12

Number of Teachers (K)

Accuracy / %

Figure 11: A study on the number of PTMs (K) to use in B-Tuning with two datasets (Top: Aircraft; Bottom: Birdsnap).

For practitioners, there are two concerns about the choice of K: (1) choosing the optimal K can yield the best accuracy; (2) but larger K incurs a much larger computational cost, since a forward pass of each PTM during tuning is required. Considering the results in Figure 11 and the trade-oﬀ between the computational cost and the performance improvement, we recommend choosing K from 2, 3, and 4 in practice.
6.4 Using ImageNet-1K as the downstream task
The above experiments focus on small-scale and medium-scale downstream tasks, which are common in transfer learning research. This section takes a step further to use the largescale ImageNet-1K (Deng et al., 2009) as the downstream dataset. In this case, a dataset larger than ImageNet-1K should be used for pre-training. JFT-300M (Sun et al., 2017), Instagram-1B (Mahajan et al., 2018), and ImageNet-21K (Deng et al., 2009) are commonlyused datasets that are larger than ImageNet-1K. Among them, ImageNet-21K is the only publicly available dataset, which serves as the pre-training dataset here. ImageNet-21K pre-trained models are provided by the timm project. It mainly contains models pre-trained on ImageNet-1K, but also has three models pre-trained on ImageNet-21K and ﬁne-tuned on ImageNet-1K, including MLP-Mixer (Tolstikhin et al., 2021), ViT (Dosovitskiy et al., 2021), and Swin-T (Liu et al., 2021b). With ImageNet-1K as the downstream dataset, their LogME score and ﬁne-tuned reference transfer learning performance are presented in Table 6. LogME is perfectly aligned with the reference performance, with a correlation value τw = 1. Then we use B-Tuning to tune the commonly used ResNet-50 trained in ImageNet-1K, with Vit and Swin as teacher models. The accuracy is increased from 76.15% to 76.50%.
Experiments in this section demonstrate that LogME and B-Tuning work for not only small-scale and medium-scale datasets but also large-scale datasets.
29

Kaichao You et al.

Table 6: Ranking models pre-trained on ImageNet-21K transferred to ImageNet-1K.

PTM pre-trained on ImageNet-21K MLP-Mixer ViT Swin-T τw

Fine-tuned Accuracy on ImageNet-1K (%) LogME value

76.61 2.075

84.53 85.25 1.00 2.085 2.134

6.5 Eﬃciency of LogME
A theoretically sound algorithm is usually complicated and computationally expensive, as is the case for LogME without optimization. Fortunately, we successfully reduced the computational complexity after analyzing its theoretical convergence property in Section 4.1 by the ﬁxed point iteration. The algorithmic complexity has been analyzed in Table 2, and Table 7 presents the wall-clock time speedup measured in Aircraft with ResNet-50. The na¨ıve implementation is very slow. Our conference paper (You et al., 2021) proposed an optimization scheme for matrix multiplication and matrix inversion, which brings 61.7× speedup. This paper further proposes the ﬁxed point iteration algorithm, which results in a much larger speedup (131.5×). Thanks to the optimized method, LogME is not only theoretically sound but also computationally eﬃcient.

Table 7: Quantitative measurement of computational speedup in evidence maximization.

Wall-clock time (second) Speedup

evidence maximization (na¨ıve implementation) evidence maximization (optimized by You et al. (2021)) evidence maximization (ﬁxed point iteration, proposed)

802.5 ± 5.6 13.1 ± 0.7 6.1 ± 0.7

61.7× 131.5×

Next, we quantitatively measure the wall-clock and memory footprint of LogME in both computer vision and natural language processing in Table 8. ResNet 50 on Aircraft is used for computer vision, and RoBERTa-D on MNLI task is used for NLP. The cost for the rest of the models and datasets varies, but the proportion is similar. The cost of computing reference transferability Tm (ﬁne-tuning with hyper-parameter search) serves as the upper bound of ranking pre-trained models. Note that, because carelessly tuned hyper-parameters cannot tell good models apart from bad models, it is necessary to attribute the cost of hyper-parameter search to ﬁne-tuning. We also list the cost of extracting features by pre-trained models, which is the lower bound of ranking pre-trained models.
According to Table 8, we have the following observations: (1) brute-force ﬁne-tuning is computationally expensive, requiring about a day for one dataset with one pre-trained model. Selecting the best pre-trained model out of 12 models would cost 12 GPU-days. (2) Extracting features is very cheap and costs much less than ﬁne-tuning. (3) The additional time-cost of LogME compared to feature extraction is rather small, which means that LogME’s cost is very close to the lower bound. In computer vision, LogME is 3700× faster than ﬁne-tuning, with 120× less memory footprint. In the NLP domain, feature extraction is much slower than that in computer vision, and therefore the wall-clock time speedup (73×) is not that striking.
30

Ranking and Tuning Pre-trained Models

Table 8: Computational cost and memory footprint of LogME.

Computer Vision Natural Language Processing

wall-clock time

ﬁne-tune (upper bound) 161000s

extract feature (lower bound) 37s

LogME

43s

beneﬁt

3700 ↑

ﬁne-tune (upper bound) 100200s

extract feature (lower bound) 1130s

LogME

1136s

beneﬁt

88 ↑

memory footprint ﬁne-tune (upper bound) extract feature (lower bound) LogME beneﬁt ﬁne-tune (upper bound) extract feature (lower bound) LogME beneﬁt

6.3 GB 43 MB 53 MB
120 ↑ 88 GB 1.2 GB 1.2 GB
73 ↑

In summary, LogME is eﬃcient in terms of both wall-clock time and memory footprint, thanks to the optimized algorithm (ﬁxed point iteration) inspired by the theoretical analysis.

6.6 Comparing LogME to re-training head
A straightforward way to measure the relationship between features and labels is to train a linear classiﬁcation/regression head for the downstream task, and to use the head’s performance as a metric, which is known as “linear probing” or “linear protocol evaluation”. Empirically, we ﬁnd that re-training head does not work well. In the following, we summarize why re-training head is inferior to LogME from three perspectives, which partially explains why the important problem of ranking and tuning PTMs was under-explored in the past.

0.75

0.70
LogME
0.65

τw

0.35

0.30

0.25

1

5

10

15

20

25

Number of hyper-parameter trials

Figure 12: The correlation of re-training head w.r.t. the number of hyper-parameter trials.

(1) LogME is more eﬃcient than re-training head. In linear protocol evaluation, parameters in the head are learned by maximum likelihood estimation, which is prone to overﬁtting. To alleviate over-ﬁtting, grid search for its hyper-parameters (such as the strength of L2 regularization) should be tuned extensively on a validation set, making head re-training ineﬃcient. For example, in the Caltech dataset, we extract features from 12 PTMs, train softmax regressors with tuned hyper-parameters (the L2 regularization strength), and plot the correlation τw between the best head accuracy and the reference transfer performance w.r.t. the number of hyper-parameter trials in Figure 12. The correlation of LogME is plotted as a reference. Computing LogME requires 3× less time than re-training a head with

31

Kaichao You et al.
one ﬁxed hyper-parameter, and re-training head with exhaustive hyper-parameter search is still much inferior to LogME.
(2) Re-training head does not work well with limited training data. Because re-training head follows a supervised learning paradigm, it suﬀers in low-shot learning scenarios. For example, prompt learning (Liu et al., 2021a) is an active research area in natural language processing, where researchers try to exploit the potential of frozen pretrained models by only a few training data. In the sentiment classiﬁcation task SST-2 (Socher et al., 2013), prompt learning (Liu et al., 2021a) extracts the sentence embedding ES for each sentence S, and compare ES with word embeddings EP , EN of a positive anchor word P (i.e. “good”, “fantastic”) and a negative anchor word N (i.e. “bad”, “awful”). The decision rule is: sentence S contains positive sentiment ⇐⇒ EST EP > EST EN . In this case, searching proper anchor words on validation data yields 61.4% accuracy (complete results are available in Table 13). Re-training head (i.e., training a simple classiﬁcation head with limited training data on frozen sentence embedding ES and tuning the weight-decay hyperparameter on validation set) only achieves 51.64% accuracy with 10 sentences for training. Meanwhile, we can apply LogME (or to be speciﬁc, the posterior predictive distribution introduced in Section 5.3) to this problem, which does not require any hyper-parameter tuning. This way, we can combine training data with validation data to compute the predictive weight m for each class, and use m as the embedding of a virtual “anchor word”, which results in 79.24% accuracy, a huge improvement over re-training head and manually selected anchor words! The superior performance of LogME is interpretable: we analyzed the predictive weight m for the negative sentiment class, and ﬁnd that it is closest to the embedding of “dump”, “:(”, “doomed”, “Worse”, “worse”. To our surprise, it can discover that “:(”, a cyber word to express unhappy emotion, contains negative sentiment.
(3) Re-training head does not have a clear metric. As a side issue, even if we re-train a head for a downstream task, it is unclear which quantity should be used as the ranking metric. When the performance of a downstream task is evaluated by accuracy or MSE, is it over-ﬁtting to use the accuracy or MSE of the re-trained head? Indeed, in Figure 12, when the number of hyper-parameter trials increases, the correlation can even go down, conﬁrming the concern of over-ﬁtting. In contrast, LogME has uniﬁed modeling of label evidence, which has a clear statistical support.
7. Conclusion
Pre-trained models are universally acknowledged as the foundation of deep learning. Researchers have explored a lot how to create and exploit PTMs. In this paper, we switch from individual PTMs to PTM hubs, and study how to suﬃciently exploit PTM hubs with a new paradigm of ranking and tuning pre-trained models. The ranking part introduces a theoretically sound and computationally eﬃcient transferability metric named LogME. The theoretical investigation of LogME solved the decades-long unanswered question about the convergence of the MacKay’s algorithm. LogME is then further extended to be a multiple PTMs tuning method named B-Tuning, which completes the tuning part of the paradigm.
The extensive experiments conﬁrm the eﬀectiveness of the proposed methods in ranking (LogME vs. brute-force ﬁne-tuning/LEEP/NCE), selection (top-K PTMs by LogME vs.
32

Ranking and Tuning Pre-trained Models exponentially many combinations), and tuning (B-Tuning vs. Zoo-tuning and Knowledge Distillation), making the new paradigm of exploiting PTM hubs attractive for practitioners.
Acknowledgments
We would like to dedicate our thanks to Ximei Wang, Xinyang Chen, Yang Shu at Tsinghua University, Yi Zeng at Peking University, and Yonglong Tian at MIT for helpful discussions. Kaichao You, Yong Liu, Jianmin Wang, and Mingsheng Long are supported by the National Megaproject for New Generation AI (2020AAA0109201), the National Natural Science Foundation of China (62022050 and 62021002), the Beijing Nova Program (Z201100006820041), the BNRist Scholar Fund (BNR2021RC01002), and the Tsinghua-Huawei Innovation Fund.
33

Kaichao You et al.

Appendix A. Notation Table
Notations used in this paper are listed in the following table. We tried our best to avoid notation conﬂicts, but the following notation conﬂicts are unavoidable due to the heavy math in this paper: (1) fi represents features of xi and f (t) represents the ﬁxed point iteration function. (2) w represents parameters in the linear head while the weighted Kendall’s rank correlation τw uses w as its subscript. (3) t is used in the convergence proof and also used for the temperature hyper-parameter in B-Tuning.

notation

i, j, k

M, n

K

C, D

φ

xi

fi = φ(xi)

F = [f1, . . . , fn]T

Yi

yi

y

y

πk =

exp(Lk /t)

K j=1

exp(Lj

/t)

y¯ =

K k=1

πk

yk

T

S

τ

τw

w

α/β

A = αID + βF T F

m = βA−1F T y

γ

L = L(α, β)
α∗, β∗ U, Σ, V
σ r

z = UTy

t

=

α β

t ,α ,β

f (t)

L˜, F˜

W

Table 9: Notations used in this paper.

dimensionality N N N N − −
RD Rn×D
RC
R Rn −
R
− R R R R
RD
R
RD×D RD
R R R − R N Rn R
R − − −

meaning running subscripts the number of PTMs and samples the number of selected PTMs for subsequent tuning the dimension of label and extracted feature a pre-trained model an input sample extracted feature of an input example stacked features of fi label of xi a component of Yi the label component for all n samples predictive distribution of an input sample
weighted coeﬃcient for each teacher model φk
weighted average of y reference transfer performance score produced by a transferability metric Kendall’s rank correlation weighted Kendall’s rank correlation parameter in linear head hyper-parameter of the Bayesian linear model a quantity in calculating LogME a quantity in calculating LogME a quantity in calculating LogME log evidence given α, β α, β to achieve maximum evidence matrices in SVD (F = UΣV T ) diagonal entries in Σ the rank of matrix F transformation of y under U a quantity in convergence proof
the value of t, α, β after an iteration the ﬁxed point iteration function L, F for duplicated or padded features transformation matrix in knowledge distillation

34

Ranking and Tuning Pre-trained Models

Appendix B. Proof of Theorem 1

Theorem 1: Algorithm 4 induces a scalar function t = f (t) = (
n−

n 2 −1)t2
D σi i=1 t+σ2
i

. n

zi2

i=1 (t+σ2)2

i

n

σ2z2 ii

i=1 (t+σ2)2

i

Proof Let’s express all symbols in a uniﬁed form with respect to α, β, Σ, z, U, V :

• A = αI + βF T F = V (αI + βΣT Σ)V T

• A−1 = V ΣinvV T where (Σinv)ii = α+1βσi2 (1 ≤ i ≤ D)

• m = βA−1F T y = βV ΣinvΣT z

• mT m = zT Σmz with Σm = β2ΣΣ2invΣT and (Σm)ii = (α+β2βσσi2i2)2 , so mT m =

n

β 2 σi2 zi2

i=1 (α+βσi2)2

• F m = βU ΣΣinvΣT z, F m − y = U Σresz with Σres = βΣΣinvΣT − I, (Σres)ii = − α+αβσi2

• ||F m − y||22 = (F m − y)T (F m − y) = zT (Σres)2z =

n

α2 zi2

i=1 (α+βσi2)2

• γ=

D βσi2 =
i=1 α+βσi2

D σi2 i=1 t+σi2

Putting them together, we have

t = α = γ ||F m − y||22 = (

β n − γ mT m

n−

n

− 1)t2

D σi2

i=1 t+σi2

n

zi2

i=1 (t+σi2)2

n

σi2 zi2

i=1 (t+σi2)2

= f (t)

Appendix C. Proof of Theorem 2
Theorem 2: If r < n and 1≤i,j≤n(zi2 − zj2)(σi2 − σj2) > 0, then f (t) has a ﬁxed point and thus the MacKay’s algorithm will converge.

Proof The theorem can be proved by studying the behavior of f (t) near 0 and ∞.

We have limt→0 f (t) = n−r r ni=ri=r+1 1zi2zi2 > 0, which is a constant and positive number.

When

t

approaches

inﬁnity,

we

ﬁnd

that

limt→∞

f (t) t

=

n i=1

σi2

n

n i=1

zi2

n

is constant,

i=1 σi2zi2

which means f (t) behaves linearly when t is large enough.

Noticing a trick used in proving the Chebyshev’s Sum Inequality (Hardy et al., 1952), we

can get

1≤i,j≤n(zi2 − zj2)(σi2 − σj2) = 2n

n i=1

σi2zi2

−

2(

n i=1

σi2)(

n i=1

zi2).

The

condition

(z2 − z2)(σ2 − σ2) > 0 thus translates into

n i=1

σi2

n i=1

zi2

n

< 1, which means f (t)

1≤i,j≤n i

ji

j

n

i=1 σi2zi2

increases

linearly

with

a

slope

smaller

than

1

(i.e.

limt→∞

f (t) t

=

n i=1

σi2

n

< 1). n
i=1

zi2

n i=1

σi2 zi2

In summary, When t approaches 0, it is assured that limt→0 f (t) > t = 0; when t is

large enough, it is assured that f (t) < t. Putting these two conditions together, they

imply the existence of a ﬁxed point t0 > 0 such that f (t0) = t0.

35

Kaichao You et al.

Appendix D. Proof of Corollary 3

Corollary 3: LogME value will remain the same if the feature consists of arbitrary replicas of the original feature. Formally speaking, if the LogME value for F ∈ Rn×D and y ∈ Rn is L, then the LogME value for F˜ = [F, ..., F ] ∈ Rn×qD and y ∈ Rn is also L. (q ∈ N is a
natural number to represent the number of replicas.)

Proof Since LogME is calculated via an iterative algorithm, we prove the corollary by an iterative invariant (a quantitative relation that holds after every while-loop iteration).

Preliminary: SVD of F˜. We have already known the SVD of F is F = U ΣV T , and σi is the i-th largest eigenvalue of F F T . Since F˜F˜T = qF F T , duplicated feature

F˜ has singular values σ˜i2 = q0σi2 1D≤+i1≤≤Di ≤ qD , and its left orthogonal matrix is the

same as F : U˜ = U . The right orthogonal matrix of F˜ is somewhat complicated. Let’s ﬁnd

an orthogonal matrix Qq×q, whose entries in the ﬁrst column are √1q . Entries in the other

columns do not matter, as long as Qq×q is a valid orthogonal matrix. For example, we can

√1 − √1

 √1 − √1 − √1 

3

6

2

use Q2×2 =

2 √1

2 √1

, and Q3×3 =  √13

√2 6

0 . Then the right orthogonal 

2

2

√1 − √1 √1

3

6

2

matrix of F˜ is V˜ = Qq×q ⊗V , where ⊗ is the Kronecker product of two matrices. Using the

 √1p V . . . . . . 

block

matrix

form

of

Kronecker

product,

we

can

write

down

V˜

as

V˜

=

 

...

...

...

 

∈





√1p V . . . . . .

RqD×qD, with the ﬁrst D columns of V˜ corresponding to singular values √qσi, 1 ≤ i ≤ D,

and the other (q − 1) × D columns of V˜ are orthogonal basis with respect to singular values σi = 0. In summary, if the SVD of F is F = U ΣV T , then the SVD of F˜ = [F, ..., F ] is

 √1q V F˜ = U˜ Σ˜ V˜ T , where U˜ = U, Σ˜ = [√qΣ, 0, ..., 0], V˜ =  ...
 √1q V

... ... 

...

...

 

=

Qq×q

⊗

V

.



... ...

Iterative invariant: if we apply Algorithm 2 to both F˜ and F , with a small change that we initialize α˜ = q, β˜ = 1, then α˜ = qα, β˜ = β holds before Line 5. Suppose α˜ = qα, β˜ = β holds before a while-loop, then we can have:

qD β˜σ˜i2

D qβσi2

D βσi2

γ˜ = i=1 α˜ + β˜σ˜i2 = i=1 qα + qβσi2 = i=1 α + βσi2 = γ

Λ˜ = diag α˜ + β˜σ˜i2 , α˜ + β˜σ˜i2 =

q(α + βσi2) qα

1≤i≤D D + 1 ≤ i ≤ qD

36

Ranking and Tuning Pre-trained Models

m˜ = β˜A˜−1F˜T y = βV˜ Λ˜ −1V˜ T V˜ Σ˜ T U˜ T y = βV˜ Λ˜ −1Σ˜ T U T y

 √1 V . . . . . . 

 √qΣT

q

=

β

 

...

...

...

 

1q Λ−1

0 


1



q1α I(q−1)×D  . . .

√q V . . . . . .

0



1 q

V

Λ−1U

T

y



 1q m 

=  ···  =  ... 

1 q

V

Λ−1U

T

y

1q m



 

U

T

y



 1q m  Therefore m˜ T m˜ = 1q mT m, F˜m˜ = [F, . . . , F ]  . . .  = F m.
1q m

After the while-loop iteration, α˜

=

γ˜ m˜ T m˜

=

γ 1 mT m

= qα , β˜

=

n−γ˜ ||F˜m˜ −y||2

=

n−γ ||F m−y||2

=β,

q

2

2

then the iterative invariant α˜ = qα, β˜ = β still holds. Therefore we know that when the algorithm converges, α˜∗ = qα∗, β˜∗ = β∗. The corresponding maximum evidence is

L˜ = n2 log β˜∗ + q2D log α˜∗ − n2 log 2π − β˜2∗ ||F˜m˜ − y||22 − α˜2∗ m˜ T m˜ − 12 log A˜∗ = n2 log β∗ + q2D log(qα∗) − n2 log 2π − β2∗ ||F m − y||22 − α2∗ mT m − 21 log Λ˜ ∗ = n2 log β∗ + q2D log (qα∗) − n2 log 2π − β2∗ ||F m − y||22 − α2∗ mT m

− 1 log |Λ∗| − 1 log qD (qα∗)(q−1)D

2

2

= L − D log α∗ + qD log(qα∗) − 1 log qD (qα∗)(q−1)D

2

2

2

=L

By the convergence analysis in Section 4.1, initialization of α, β only changes the initial value of t, which does not impact the convergence value of the ﬁxed point iteration. Therefore, we can conclude that duplicating features will not change the value of LogME.
Although the above proof targets at Algorithm 2, it is straightforward to adapt the proof to Algorithm 3.

37

Kaichao You et al.

Appendix E. Proof of Corollary 4

Corollary 4: LogME value will remain the same if the feature is padded with arbitrary number of zeros. Formally speaking, if the LogME value for F ∈ Rn×D and y ∈ Rn is L, then the LogME value for F˜ = [F, 0] ∈ Rn×(D+d) and y ∈ Rn is also L. d ∈ N is a natural number and 0 ∈ Rn×d is a matrix with all zero entries. Proof The proof follows the same idea as Corollary 3, but the SVD of F˜ is simpler than Corollary 3. If the SVD of F is F = U ΣV T , then the SVD of F˜ = [F, 0] is F˜ = U˜ Σ˜ V˜ T ,

where U˜ = U, Σ˜ = [Σ, 0], V˜ =

V W

, with W ∈ Rd×d an orthogonal matrix that

satisﬁes W T W = Id. Note that Σ˜ = [Σ, 0] translates into σ˜i2 = σ0i2 1D≤+i1≤≤Di ≤ D + d .

Iterative invariant: if we apply Algorithm 2 to both F˜ and F , with the same initialization α˜ = 1, β˜ = 1, then α˜ = α, β˜ = β holds before Line 5. Suppose α˜ = α, β˜ = β holds

before a while-loop, then we can have:

Λ˜ = diag

D+d
γ˜ =

β˜σ˜i2

D
=

βσi2 = γ

i=1 α˜ + β˜σ˜i2 i=1 α + βσi2

α˜ + β˜σ˜i2 , α˜ + β˜σ˜i2 =

α + βσi2 α

1≤i≤D D+1≤i≤D+d

m˜ = β˜A˜−1F˜T y = β V W

Λ−1 α1 Id

VT WT

FT y = m

0Tn×d

0d×1

m˜ T m˜ = mT m, F˜m˜ = [F, 0n×d]

m 0d×1

= Fm

After the while-loop iteration, α˜

=

γ˜ m˜ T m˜

=

γ mT m

= α , β˜

=

n−γ˜ ||F˜m˜ −y||2

=

n−γ ||F m−y||2

=β,

2

2

then the iterative invariant α˜ = α, β˜ = β still holds. Therefore, we know that when the

algorithm converges, α˜∗ = α∗, β˜∗ = β∗. The corresponding maximum evidence is

L˜ = n2 log β˜∗ + D 2+ d log α˜∗ − n2 log 2π − β˜2∗ ||F˜m˜ − y||22 − α˜2∗ m˜ T m˜ − 12 log A˜∗ = n2 log β∗ + D 2+ d log α∗ − n2 log 2π − β2∗ ||F m − y||22 − α2∗ mT m − 12 log Λ˜ ∗ = n2 log β∗ + D 2+ d log α∗ − n2 log 2π − β2∗ ||F m − y||22 − α2∗ mT m

− 1 log |Λ∗| − 1 log (α∗)d

2

2

= L + d log α∗ − d log α∗

2

2

=L

38

Ranking and Tuning Pre-trained Models

Appendix F. Detailed descriptions of the datasets
Aircraft: The dataset contains ﬁne-grained classiﬁcation of 10,000 aircraft pictures which belong to 100 classes, with 100 images per class.
Birdsnap: The dataset contains 49,829 images of 500 species of North American birds. Caltech: The dataset contains 9,144 pictures of objects belonging to 101 categories. There are about 40 to 800 images per category. Most categories have about 50 images. Cars: The dataset contains 16,185 images of 196 classes of cars. The data is split into 8,144 training images and 8,041 testing images. CIFAR 10: The dataset consists of 60,000 32×32 colorful images in 10 classes, with 6,000 images per class. There are 50,000 training images and 10,000 test images. CIFAR 100: The dataset is just like the CIFAR 10, except it has 100 classes containing 600 images each. DTD: The dataset contains a collection of 5,640 textural images in the wild, annotated with a series of human-centric attributes. It has 47 classes and 120 images per class. Pets: The dataset contains 7,049 images of cat and dog species which belong to 47 classes, with around 200 images per class. SUN: The dataset contains 39,700 scenery pictures with 397 classes and 100 samples per class.

Appendix G. Original Results in Figures
Original results in ﬁgures are shown in the Table 10, Table 11, and Table 12.

Table 10: Original results in Figure 4.

task

ResNet-34 ResNet-50 ResNet-101 ResNet-152 WideResNet-50 DenseNet-121 DenseNet-169 DenseNet-201 Inception v1 Inception v3 MobileNet v2 NASNet-A Mobile τw

Aircraft

Accuracy Laplace LEEP NCE LogME

79.9 -2.864 -0.497 -0.364 0.930

86.6 -3.127 -0.412 -0.297 0.946

85.6 -3.080 -0.349 -0.244 0.948

85.3 -3.158 -0.308 -0.214 0.950

83.2 -3.721 -0.337 -0.248 0.934

85.4 -2.235 -0.431 -0.296 0.938

84.5 -1.906 -0.340 -0.259 0.943

84.6 -1.754 -0.462 -0.322 0.942

82.7 -2.382 -0.795 -0.348 0.934

88.8 -2.822 -0.492 -0.250 0.953

82.8 -2.217 -0.515 -0.411 0.941

72.8 -1.481 -0.32 -0.506 0.13 -0.444 0.39 0.948 0.59

Birdsnap

Accuracy LEEP NCE LogME

59.5 -1.758 -1.640 0.802

74.7 -1.647 -1.538 0.829

73.8 -1.553 -1.479 0.836

74.3 -1.481 -1.417 0.839

63.1 -1.554 -1.399 0.825

73.2 -1.729 -1.566 0.810

71.4 -1.756 -1.644 0.815

72.6 -1.645 -1.493 0.822

73.0 -2.483 -1.807 0.806

77.2 -1.776 -1.354 0.848

69.3 -1.951 -1.815 0.808

68.3 -1.835 -1.778 0.824

0.19 0.51 0.66

Caltech

Accuracy LEEP NCE LogME

90.2 -2.249 -1.899 1.362

91.8 -2.195 -1.820 1.509

93.1 -2.067 -1.777 1.548

93.2 -1.984 -1.721 1.567

91.0 -2.179 -1.828 1.505

91.9 -2.159 -1.807 1.365

92.5 -2.039 -1.774 1.417

93.4 -2.122 -1.808 1.428

91.7 -2.718 -1.849 1.440

94.3 -2.286 -1.722 1.605

89.1 -2.373 -2.009 1.365

91.5 -2.263 -1.966 1.389

0.30 0.69 0.66

Cars

Accuracy LEEP NCE LogME

86.4 -1.534 -1.203 1.245

91.7 -1.570 -1.181 1.253

91.7 -1.370 -1.142 1.255

92.0 -1.334 -1.128 1.260

89.7 -1.406 -1.183 1.250

91.5 -1.562 -1.111 1.249

91.5 -1.505 -1.192 1.252

91.0 -1.687 -1.319 1.251

91.0 -2.149 -1.201 1.246

92.3 -1.637 -1.195 1.259

91.0 -1.695 -1.312 1.250

88.5 -1.588 -1.334 1.254

0.26 0.36 0.69

Accuracy CIFAR10 LEEP
NCE LogME

97.1 -3.418 -3.398 0.323

96.8 -3.407 -3.395 0.388

97.7 -3.184 -3.232 0.463

97.9 -3.020 -3.084 0.469

97.7 -3.335 -3.348 0.398

97.2 -3.651 -3.541 0.302

97.4 -3.345 -3.427 0.343

97.4 -3.458 -3.467 0.369

96.2 -4.074 -3.338 0.293

97.5 -3.976 -3.625 0.349

95.7 -3.624 -3.511 0.291

96.8 -3.467 -3.436 0.304

0.72 0.51 0.82

Accuracy CIFAR100 LEEP
NCE LogME

84.5 -3.531 -3.230 1.036

84.5 -3.520 -3.241 1.099

87.0 -3.330 -3.112 1.130

87.6 -3.167 -2.980 1.133

86.4, -3.391 -3.158 1.102

84.8 -3.715 -3.304 1.029

85.0 -3.525 -3.313 1.051

86.0 -3.643 -3.323 1.061

83.2 -4.279 -3.253 1.037

86.6 -4.100 -3.447 1.070

80.8 -3.733 -3.336 1.039

83.9 -3.560 -3.254 1.051

0.66 0.53 0.77

DTD

Accuracy LEEP NCE LogME

70.0 -3.670 -3.104 0.704

75.2 -3.663 -3.119 0.761

76.2 -3.718 -3.199 0.757

75.4 -3.653 -3.138 0.766

70.1 -3.764 -3.259 0.731

74.9 -3.847 -3.198 0.710

74.8 -3.646 -3.218 0.730

74.5 -3.757 -3.203 0.730

73.6 -4.124 -3.082 0.727

77.2 -4.096 -3.261 0.746

72.9 -3.805 -3.176 0.712

72.8 -3.691 -3.149 0.724

-0.06 -0.35 0.50

Accuracy Pets LEEP
NCE LogME

92.3 -1.174 -1.094 0.835

92.5 -1.031 -0.956 1.029

94.0 -0.915 -0.885 1.061

94.5 -0.892 -0.862 1.084

92.8 -0.945 -0.900 1.016

92.9 -1.100 -0.987 0.839

93.1 -1.111 -1.072 0.874

92.8 -1.108 -1.026 0.908

91.9 -1.520 -1.076 0.913

93.5 -1.129 -0.893 1.191

90.5 -1.228 -1.156 0.821

89.4 -1.150 -1.146 0.833

0.66 0.83 0.61

SUN

Accuracy LEEP NCE LogME

63.1 -2.727 -2.573 1.704

64.7 -2.611 -2.469 1.744

64.8 -2.531 -2.455 1.749

66.0 -2.513 -2.444 1.755

67.4 -2.569 -2.457 1.750

62.3 -2.713 -2.500 1.704

63.0 -2.570 -2.480 1.716

64.7 -2.618 -2.465 1.718

62.0 -3.153 -2.534 1.715

65.7 -2.943 -2.529 1.753

60.5 -2.764 -2.590 1.713

60.7 -2.687 -2.586 1.721

0.54 0.68 0.71

39

Kaichao You et al.

Table 11: Original results in Figure 5.

task

ResNet-34 ResNet-50 ResNet-101 ResNet-152 WideResNet-50 DenseNet-121 DenseNet-169 DenseNet-201 Inception v1 Inception v3 MobileNet v2 NASNet-A Mobile τw

dSprites MSE LogME

0.037 1.05

0.031 1.53

0.028 1.64

0.028 1.63

0.034 1.31

0.039 1.35

0.035 1.25

0.036 1.34

0.045 1.18

0.044 1.22

0.037 1.18

0.035 1.39 0.79

Table 12: Original results in Figure 9. (Popularity is measured by download count in millions.)

task

MNLI

Accuracy LogME Popularity

QQP

Accuracy LogME Popularity

QNLI

Accuracy LogME Popularity

SST-2

Accuracy LogME Popularity

CoLA

Accuracy LogME Popularity

MRPC

Accuracy LogME Popularity

RTE

Accuracy LogME Popularity

RoBERTa
87.6 -0.568
3.78
91.9 -0.465
3.78
92.8 -0.565
3.78
94.8 -0.312
3.78
63.6 -0.499
3.78
90.2 -0.573
3.78
78.7 -0.709
3.78

RoBERTa-D
84.0 -0.599
0.61
89.4 -0.492
0.61
90.8 -0.603
0.61
92.5 -0.330
0.61
59.3 -0.536
0.61
86.6 -0.586
0.61
67.9 -0.723
0.61

uncased BERT-D
82.2 -0.603
6.01
88.5 -0.488
6.01
89.2 -0.613
6.01
91.3 -0.331
6.01
51.3 -0.568
6.01
87.5 -0.605
6.01
59.9 -0.725
6.01

cased BERT-D
81.5 -0.612
1.09
87.8 -0.521
1.09
88.2 -0.618
1.09
90.4 -0.353
1.09
47.2 -0.572
1.09
85.6 -0.604
1.09
60.6 -0.725
1.09

ALBERT-v1
81.6 -0.614
0.11
-
-
90.3 -0.525
0.11
-
-
-

ALBERT-v2
84.6 -0.594
1.25
-
-
92.9 -0.447
1.25
-
-
-

ELECTRA-base
79.7 -0.666
0.13
-
-
-
-
-
-

ELECTRA-small
85.8 -0.621
0.23
-
-
-
-
-
-

τw
0.66 0.28
0.73 0.00
1.00 0.00
0.68 0.38
1.00 0.00
0.53 0.33
1.00 -0.29

Appendix H. Complete results in prompt learning

Table 13: Complete results in prompt learning with manually selected anchor words.

accuracy
anchor P positive good ﬁne great nice

anchor N negative bad

ill evil poor

49.8 52.8 49.1 49.1 60.9 51.0 50.9 49.0 52.4 50.9 51.7 51.0 49.1 54.5 50.9 55.4 53.1 49.1 61.4 51.0 51.6 50.6 49.1 51.0 50.8

40

Ranking and Tuning Pre-trained Models

Appendix I. Full Figure in Convergence Analysis

ResNet-50

Inception v1

DenseNet-121

300 250 200 150 100 50
0 0
300 250 200 150 100 50
0 0
1000
800
600
400
200
0 0
400

Class 1 100 200 100 200

300

250

200

150

100

50

0

300

0

300

250

200

150

100

50

0

300

0

1000

800

600

400

200

0

250 500 750 1000

0

400

Class 2 100 200 100 200

300

250

200

150

100

50

0

300

0

300

250

200

150

100

50

0

300

0

1000

800

600

400

200

0

250 500 750 1000

0

400

Class 3 100 200 100 200

300

250

200

150

100

50

0

300

0

300

250

200

150

100

50

0

300

0

1000

800

600

400

200

0

250 500 750 1000

0

400

Class 4 100 200 100 200

300

250

200

150

100

50

0

300

0

300

250

200

150

100

50

0

300

0

1000

800

600

400

200

0

250 500 750 1000

0

400

Class 5 100 200 100 200

300

250

200

150

100

50

0

300

0

300

250

200

150

100

50

0

300

0

1000

800

600

400

200

0

250 500 750 1000

0

400

Class 6 100 200 100 200

300

250

200

150

100

50

0

300

0

300

250

200

150

100

50

0

300

0

1000

800

600

400

200

0

250 500 750 1000

0

400

Class 7 100 200 100 200

300

250

200

150

100

50

0

300

0

300

250

200

150

100

50

0

300

0

1000

800

600

400

200

0

250 500 750 1000

0

400

Class 8 100 200 100 200

300

250

200

150

100

50

0

300

0

300

250

200

150

100

50

0

300

0

1000

800

600

400

200

0

250 500 750 1000

0

400

Class 9 100 200 100 200

300

250

200

150

100

50

0

300

0

300

250

200

150

100

50

0

300

0

1000

800

600

400

200

0

250 500 750 1000

0

400

Class 10 100 200 300 100 200 300 250 500 750 1000

300

300

300

300

300

300

300

300

300

300

200

200

200

200

200

200

200

200

200

200

100

100

100

100

100

100

100

100

100

100

0 0
300 250 200 150 100 50
0 0

100 200 300 100 200

0

400

0

300

250

200

150

100

50

0

300

0

100 200 300 100 200

0

400

0

300

250

200

150

100

50

0

300

0

100 200 300 100 200

0

400

0

300

250

200

150

100

50

0

300

0

100 200 300 100 200

0

400

0

300

250

200

150

100

50

0

300

0

100 200 300 100 200

0

400

0

300

250

200

150

100

50

0

300

0

100 200 300 100 200

0

400

0

300

250

200

150

100

50

0

300

0

100 200 300 100 200

0

400

0

300

250

200

150

100

50

0

300

0

100 200 300 100 200

0

400

0

300

250

200

150

100

50

0

300

0

100 200 300 100 200

0

400

0

300

250

200

150

100

50

0

300

0

100 200 300 400 100 200 300

t'=f(t)

t'=t

MobileNet v2

NASNet-A Mobile

Figure 13: Fixed points of f (t) in Equation 3 for all 10 classes in CIFAR10 with 5 pre-trained models. We plot t = f (t) (in blue) and t = t (in orange), whose intersections are ﬁxed points.The existence of ﬁxed points guarantees the convergence of the evidence maximization procedure in LogME.

41

Kaichao You et al.
References
S. Ben-David and R. Schuller. Exploiting task relatedness for multiple task learning. In COLT, 2003.
T. Berg, J. Liu, S. Woo Lee, M. L. Alexander, D. W. Jacobs, and P. N. Belhumeur. Birdsnap: Large-scale ﬁne-grained visual categorization of birds. In CVPR, 2014.
C. M. Bishop. Neural networks for pattern recognition. 1995.
C. M. Bishop. Pattern recognition and machine learning. 2006.
R. Bommasani, D. A. Hudson, E. Adeli, R. Altman, S. Arora, S. von Arx, M. S. Bernstein, J. Bohg, A. Bosselut, E. Brunskill, E. Brynjolfsson, S. Buch, D. Card, R. Castellon, N. Chatterji, A. Chen, K. Creel, J. Q. Davis, D. Demszky, C. Donahue, M. Doumbouya, E. Durmus, S. Ermon, J. Etchemendy, K. Ethayarajh, L. Fei-Fei, C. Finn, T. Gale, L. Gillespie, K. Goel, N. Goodman, S. Grossman, N. Guha, T. Hashimoto, P. Henderson, J. Hewitt, D. E. Ho, J. Hong, K. Hsu, J. Huang, T. Icard, S. Jain, D. Jurafsky, P. Kalluri, S. Karamcheti, G. Keeling, F. Khani, O. Khattab, P. W. Kohd, M. Krass, R. Krishna, R. Kuditipudi, A. Kumar, F. Ladhak, M. Lee, T. Lee, J. Leskovec, I. Levent, X. L. Li, X. Li, T. Ma, A. Malik, C. D. Manning, S. Mirchandani, E. Mitchell, Z. Munyikwa, S. Nair, A. Narayan, D. Narayanan, B. Newman, A. Nie, J. C. Niebles, H. Nilforoshan, J. Nyarko, G. Ogut, L. Orr, I. Papadimitriou, J. S. Park, C. Piech, E. Portelance, C. Potts, A. Raghunathan, R. Reich, H. Ren, F. Rong, Y. Roohani, C. Ruiz, J. Ryan, C. R´e, D. Sadigh, S. Sagawa, K. Santhanam, A. Shih, K. Srinivasan, A. Tamkin, R. Taori, A. W. Thomas, F. Tram`er, R. E. Wang, W. Wang, B. Wu, J. Wu, Y. Wu, S. M. Xie, M. Yasunaga, J. You, M. Zaharia, M. Zhang, T. Zhang, X. Zhang, Y. Zhang, L. Zheng, K. Zhou, and P. Liang. On the Opportunities and Risks of Foundation Models. arXiv:2108.07258 [cs], 2021.
T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei. Language Models are Few-Shot Learners. In NeurIPS, 2020.
Z. Cao, K. You, Z. Zhang, J. Wang, and M. Long. From Big to Small: Adaptive Learning to Partial-Set Domains. TPAMI, 2022.
L.-C. Chen, G. Papandreou, F. Schroﬀ, and H. Adam. Rethinking Atrous Convolution for Semantic Image Segmentation. arXiv:1706.05587 [cs], 2017.
T. Chen, S. Kornblith, M. Norouzi, and G. Hinton. A Simple Framework for Contrastive Learning of Visual Representations. In ICML, 2020a.
X. Chen, S. Wang, B. Fu, M. Long, and J. Wang. Catastrophic Forgetting Meets Negative Transfer: Batch Spectral Shrinkage for Safe Transfer Learning. In NeurIPS, 2019.
X. Chen, H. Fan, R. Girshick, and K. He. Improved Baselines with Momentum Contrastive Learning. arXiv:2003.04297 [cs], 2020b.
42

Ranking and Tuning Pre-trained Models
M. Cimpoi, S. Maji, I. Kokkinos, S. Mohamed, and A. Vedaldi. Describing textures in the wild. In CVPR, 2014.
K. Clark, M.-T. Luong, Q. V. Le, and C. D. Manning. ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators. In ICLR, 2020.
T. M. Cover. Elements of information theory. 1999.
J. Daunizeau. Semi-analytical approximations to statistical moments of sigmoid and softmax mappings of normal variables. arXiv preprint arXiv:1703.00091, 2017.
A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society: Series B (Methodological), 1977.
J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imagenet: A large-scale hierarchical image database. In CVPR, 2009.
J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In NAACL, 2019.
J. Donahue, Y. Jia, O. Vinyals, J. Hoﬀman, N. Zhang, E. Tzeng, and T. Darrell. Decaf: A deep convolutional activation feature for generic visual recognition. In ICML, 2014.
A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. In ICLR, 2021.
D. Erhan, A. Courville, Y. Bengio, and P. Vincent. Why does unsupervised pre-training help deep learning? In AISTATS, 2010.
R. Fagin, R. Kumar, and D. Sivakumar. Comparing top k lists. In SODA, 2003.
L. Fei-Fei, R. Fergus, and P. Perona. Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories. In CVPR Workshops, 2004.
Y. Ganin and V. Lempitsky. Unsupervised Domain Adaptation by Backpropagation. In ICML, 2015.
R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature hierarchies for accurate object detection and semantic segmentation. In CVPR, 2014.
S. F. Gull. Developments in maximum entropy data analysis. In Maximum entropy and Bayesian methods. 1989.
M. Gutmann and A. Hyv¨arinen. Noise-contrastive estimation: A new estimation principle for unnormalized statistical models. In AISTATS, 2010.
43

Kaichao You et al.
X. Han, Z. Zhang, N. Ding, Y. Gu, X. Liu, Y. Huo, J. Qiu, L. Zhang, W. Han, M. Huang, Q. Jin, Y. Lan, Y. Liu, Z. Liu, Z. Lu, X. Qiu, R. Song, J. Tang, J.-R. Wen, J. Yuan, W. X. Zhao, and J. Zhu. Pre-Trained Models: Past, Present and Future. arXiv:2106.07139 [cs], 2021.
G. H. Hardy, J. E. Littlewood, G. P´olya, and G. P´olya. Inequalities. 1952.
K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into rectiﬁers: Surpassing human-level performance on imagenet classiﬁcation. In ICCV, 2015.
K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In CVPR, 2016.
K. He, G. Gkioxari, P. Doll´ar, and R. Girshick. Mask R-CNN. In ICCV, 2017.
K. He, H. Fan, Y. Wu, S. Xie, and R. Girshick. Momentum contrast for unsupervised visual representation learning. In CVPR, 2020.
G. Hinton, O. Vinyals, and J. Dean. Distilling the knowledge in a neural network, 2015.
W. Hu, B. Liu, J. Gomes, M. Zitnik, P. Liang, V. Pande, and J. Leskovec. Strategies for Pre-training Graph Neural Networks. In ICLR, 2020.
G. Huang, Z. Liu, K. Q. Weinberger, and L. van der Maaten. Densely connected convolutional networks. In CVPR, 2017.
A. Immer, M. Bauer, V. Fortuin, G. Ra¨tsch, and K. M. Emtiyaz. Scalable marginal likelihood estimation for model selection in deep learning. In ICML, 2021.
L. Jing and Y. Tian. Self-supervised visual feature learning with deep neural networks: A survey. TPAMI, 2020.
N. P. Jouppi, C. Young, N. Patil, D. Patterson, G. Agrawal, R. Bajwa, S. Bates, S. Bhatia, N. Boden, and A. Borchers. In-datacenter performance analysis of a tensor processing unit. In ISCA, 2017.
M. G. Kendall. A new measure of rank correlation. Biometrika, 1938.
K. H. Knuth, M. Habeck, N. K. Malakar, A. M. Mubeen, and B. Placek. Bayesian Evidence and Model Selection. Digital Signal Processing, 2015.
D. Koller and N. Friedman. Probabilistic graphical models: principles and techniques. 2009.
S. Kornblith, J. Shlens, and Q. V. Le. Do better imagenet models transfer better? In CVPR, 2019.
Z. Kou, K. You, M. Long, and J. Wang. Stochastic Normalization. In NeurIPS, 2020.
J. Krause, J. Deng, M. Stark, and L. Fei-Fei. Collecting a large-scale dataset of ﬁne-grained cars. 2013.
44

Ranking and Tuning Pre-trained Models
A. Krizhevsky and G. Hinton. Learning multiple layers of features from tiny images. Technical report, 2009.
Z. Lan, M. Chen, S. Goodman, K. Gimpel, P. Sharma, and R. Soricut. ALBERT: A Lite BERT for Self-supervised Learning of Language Representations. In ICLR, 2020.
C. Li, Y. Mao, R. Zhang, and J. Huai. On hyper-parameter estimation in empirical Bayes: a revisit of the MacKay algorithm. In UAI, 2016.
H. Li, P. Chaudhari, H. Yang, M. Lam, A. Ravichandran, R. Bhotika, and S. Soatto. Rethinking the Hyperparameters for Fine-tuning. In ICLR, 2020.
X. Li, Y. Grandvalet, and F. Davoine. Explicit Inductive Bias for Transfer Learning with Convolutional Networks. In ICML, 2018.
T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Doll´ar, and C. L. Zitnick. Microsoft coco: Common objects in context. In ECCV, 2014.
P. Liu, W. Yuan, J. Fu, Z. Jiang, H. Hayashi, and G. Neubig. Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing. arXiv:2107.13586 [cs], 2021a.
Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V. Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019.
Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo. Swin Transformer: Hierarchical Vision Transformer Using Shifted Windows. In ICCV, 2021b.
M. Long, Y. Cao, J. Wang, and M. Jordan. Learning Transferable Features with Deep Adaptation Networks. In ICML, 2015.
D. J. MacKay. Bayesian interpolation. Neural computation, 1992.
D. Mahajan, R. Girshick, V. Ramanathan, K. He, M. Paluri, Y. Li, A. Bharambe, and L. van der Maaten. Exploring the limits of weakly supervised pretraining. In ECCV, 2018.
S. Maji, E. Rahtu, J. Kannala, M. Blaschko, and A. Vedaldi. Fine-Grained Visual Classiﬁcation of Aircraft. arXiv:1306.5151 [cs], 2013.
L. Matthey, I. Higgins, D. Hassabis, and A. Lerchner. dsprites: Disentanglement testing sprites dataset, 2017.
S. Merity, C. Xiong, J. Bradbury, and R. Socher. Pointer Sentinel Mixture Models. In ICLR, 2017.
K. P. Murphy. Machine learning: a probabilistic perspective. 2012.
B. Neyshabur, H. Sedghi, and C. Zhang. What is being transferred in transfer learning? In NeurIPS, 2020.
45

Kaichao You et al.
C. Nguyen, T. Hassner, M. Seeger, and C. Archambeau. LEEP: A New Measure to Evaluate Transferability of Learned Representations. In ICML, 2020.
O. M. Parkhi, A. Vedaldi, A. Zisserman, and C. V. Jawahar. Cats and dogs. In CVPR, 2012.
X. Qiu, T. Sun, Y. Xu, Y. Shao, N. Dai, and X. Huang. Pre-trained models for natural language processing: A survey. Science China Technological Sciences, 2020.
J. Quionero-Candela, M. Sugiyama, A. Schwaighofer, and N. D. Lawrence. Dataset shift in machine learning. 2009.
P. Rajpurkar, J. Zhang, K. Lopyrev, and P. Liang. SQuAD: 100,000+ Questions for Machine Comprehension of Text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, 2016.
C. E. Rasmussen. Gaussian processes in machine learning. In Summer school on machine learning, 2003.
O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, and M. Bernstein. Imagenet large scale visual recognition challenge. IJCV, 2015.
M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, and L.-C. Chen. MobileNetV2: Inverted Residuals and Linear Bottlenecks. In CVPR, 2018.
E. T. K. Sang and F. De Meulder. Introduction to the CoNLL-2003 Shared Task: LanguageIndependent Named Entity Recognition. In NAACL, 2003.
V. Sanh, L. Debut, J. Chaumond, and T. Wolf. DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108, 2019.
Y. Shu, Z. Kou, Z. Cao, J. Wang, and M. Long. Zoo-Tuning: Adaptive Transfer from A Zoo of Models. In ICML, 2021.
S. P. Singh and M. Jaggi. Model fusion via optimal transport. In NeurIPS, 2020.
R. Socher, A. Perelygin, J. Wu, J. Chuang, C. D. Manning, A. Y. Ng, and C. Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In EMNLP, 2013.
C. Sun, A. Shrivastava, S. Singh, and A. Gupta. Revisiting unreasonable eﬀectiveness of data in deep learning era. In ICCV, 2017.
C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich. Going deeper with convolutions. 2015.
C. Szegedy, V. Vanhoucke, S. Ioﬀe, J. Shlens, and Z. Wojna. Rethinking the Inception Architecture for Computer Vision. In CVPR, 2016.
M. Tan, B. Chen, R. Pang, V. Vasudevan, M. Sandler, A. Howard, and Q. V. Le. Mnasnet: Platform-aware neural architecture search for mobile. In CVPR, 2019.
46

Ranking and Tuning Pre-trained Models
S. Thrun and L. Pratt. Learning to Learn: Introduction and Overview. In Learning to Learn. 1998.
Y. Tian, C. Sun, B. Poole, D. Krishnan, C. Schmid, and P. Isola. What Makes for Good Views for Contrastive Learning? In NeurIPS, 2020.
I. O. Tolstikhin, N. Houlsby, A. Kolesnikov, L. Beyer, X. Zhai, T. Unterthiner, J. Yung, A. Steiner, D. Keysers, and J. Uszkoreit. Mlp-mixer: An all-mlp architecture for vision. In NeurIPS, 2021.
A. T. Tran, C. V. Nguyen, and T. Hassner. Transferability and hardness of supervised classiﬁcation tasks. In ICCV, 2019.
S. Vigna. A Weighted Correlation Index for Rankings with Ties. In WWW, 2015.
A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bowman. GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding. In EMNLP, 2018.
A. Wang, Y. Pruksachatkun, N. Nangia, A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bowman. SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems. In NeurIPS, 2019.
T. Wolf, J. Chaumond, L. Debut, V. Sanh, C. Delangue, A. Moi, P. Cistac, M. Funtowicz, J. Davison, and S. Shleifer. Transformers: State-of-the-art natural language processing. In EMNLP, 2020.
J. Xiao, J. Hays, K. A. Ehinger, A. Oliva, and A. Torralba. Sun database: Large-scale scene recognition from abbey to zoo. In CVPR, 2010.
Z. Yang, Z. Dai, Y. Yang, J. Carbonell, R. R. Salakhutdinov, and Q. V. Le. Xlnet: Generalized autoregressive pretraining for language understanding. In NeurIPS, 2019.
J. Yosinski, J. Clune, Y. Bengio, and H. Lipson. How transferable are features in deep neural networks? In NeurIPS, 2014.
K. You, Z. Kou, M. Long, and J. Wang. Co-Tuning for Transfer Learning. In NeurIPS, 2020.
K. You, Y. Liu, J. Wang, and M. Long. LogME: Practical Assessment of Pre-trained Models for Transfer Learning. In ICML, 2021.
S. Zagoruyko and N. Komodakis. Wide residual networks. 2017.
A. R. Zamir, A. Sax, W. Shen, L. J. Guibas, J. Malik, and S. Savarese. Taskonomy: Disentangling Task Transfer Learning. In CVPR, 2018.
X. Zhai, J. Puigcerver, A. Kolesnikov, P. Ruyssen, C. Riquelme, M. Lucic, J. Djolonga, A. S. Pinto, M. Neumann, A. Dosovitskiy, L. Beyer, O. Bachem, M. Tschannen, M. Michalski, O. Bousquet, S. Gelly, and N. Houlsby. A Large-scale Study of Representation Learning with the Visual Task Adaptation Benchmark. arXiv:1910.04867 [cs, stat], 2020.
47

