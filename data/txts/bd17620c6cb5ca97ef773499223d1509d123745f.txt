Dive into Deep Learning
Release 0.17.0 Aston Zhang, Zachary C. Lipton, Mu Li, and Alexander J. Smola
Jul 26, 2021

Contents

Preface

1

Installation

9

Notation

13

1 Introduction

17

1.1 A Motivating Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18

1.2 Key Components . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20

1.3 Kinds of Machine Learning Problems . . . . . . . . . . . . . . . . . . . . . . . . . 22

1.4 Roots . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34

1.5 The Road to Deep Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36

1.6 Success Stories . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38

1.7 Characteristics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40

2 Preliminaries

43

2.1 Data Manipulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43

2.1.1 Getting Started . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44

2.1.2 Operations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46

2.1.3 Broadcasting Mechanism . . . . . . . . . . . . . . . . . . . . . . . . . . . 48

2.1.4 Indexing and Slicing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49

2.1.5 Saving Memory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49

2.1.6 Conversion to Other Python Objects . . . . . . . . . . . . . . . . . . . . . 50

2.2 Data Preprocessing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51

2.2.1 Reading the Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51

2.2.2 Handling Missing Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52

2.2.3 Conversion to the Tensor Format . . . . . . . . . . . . . . . . . . . . . . . 53

2.3 Linear Algebra . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53

2.3.1 Scalars . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54

2.3.2 Vectors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54

2.3.3 Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56

2.3.4 Tensors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57

2.3.5 Basic Properties of Tensor Arithmetic . . . . . . . . . . . . . . . . . . . . 58

2.3.6 Reduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59

2.3.7 Dot Products . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61

2.3.8 Matrix-Vector Products . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61

2.3.9 Matrix-Matrix Multiplication . . . . . . . . . . . . . . . . . . . . . . . . . 62

2.3.10 Norms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63

2.3.11 More on Linear Algebra . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65

2.4 Calculus . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66

2.4.1 Derivatives and Differentiation . . . . . . . . . . . . . . . . . . . . . . . . 67

i

2.4.2 Partial Derivatives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70 2.4.3 Gradients . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70 2.4.4 Chain Rule . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71 2.5 Automatic Differentiation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72 2.5.1 A Simple Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72 2.5.2 Backward for Non-Scalar Variables . . . . . . . . . . . . . . . . . . . . . . 73 2.5.3 Detaching Computation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74 2.5.4 Computing the Gradient of Python Control Flow . . . . . . . . . . . . . . 74 2.6 Probability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76 2.6.1 Basic Probability Theory . . . . . . . . . . . . . . . . . . . . . . . . . . . 77 2.6.2 Dealing with Multiple Random Variables . . . . . . . . . . . . . . . . . . 80 2.6.3 Expectation and Variance . . . . . . . . . . . . . . . . . . . . . . . . . . . 83 2.7 Documentation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84 2.7.1 Finding All the Functions and Classes in a Module . . . . . . . . . . . . . 84 2.7.2 Finding the Usage of Specific Functions and Classes . . . . . . . . . . . . 85

3 Linear Neural Networks

87

3.1 Linear Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 87

3.1.1 Basic Elements of Linear Regression . . . . . . . . . . . . . . . . . . . . . 87

3.1.2 Vectorization for Speed . . . . . . . . . . . . . . . . . . . . . . . . . . . . 91

3.1.3 The Normal Distribution and Squared Loss . . . . . . . . . . . . . . . . . 93

3.1.4 From Linear Regression to Deep Networks . . . . . . . . . . . . . . . . . 94

3.2 Linear Regression Implementation from Scratch . . . . . . . . . . . . . . . . . . 97

3.2.1 Generating the Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97

3.2.2 Reading the Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 98

3.2.3 Initializing Model Parameters . . . . . . . . . . . . . . . . . . . . . . . . 99

3.2.4 Defining the Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100

3.2.5 Defining the Loss Function . . . . . . . . . . . . . . . . . . . . . . . . . . 100

3.2.6 Defining the Optimization Algorithm . . . . . . . . . . . . . . . . . . . . . 100

3.2.7 Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101

3.3 Concise Implementation of Linear Regression . . . . . . . . . . . . . . . . . . . . 103

3.3.1 Generating the Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103

3.3.2 Reading the Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103

3.3.3 Defining the Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 104

3.3.4 Initializing Model Parameters . . . . . . . . . . . . . . . . . . . . . . . . 105

3.3.5 Defining the Loss Function . . . . . . . . . . . . . . . . . . . . . . . . . . 105

3.3.6 Defining the Optimization Algorithm . . . . . . . . . . . . . . . . . . . . . 105

3.3.7 Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 106

3.4 Softmax Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107

3.4.1 Classification Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . 108

3.4.2 Network Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 108

3.4.3 Parameterization Cost of Fully-Connected Layers . . . . . . . . . . . . . . 109

3.4.4 Softmax Operation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 109

3.4.5 Vectorization for Minibatches . . . . . . . . . . . . . . . . . . . . . . . . 110

3.4.6 Loss Function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 110

3.4.7 Information Theory Basics . . . . . . . . . . . . . . . . . . . . . . . . . . 112

3.4.8 Model Prediction and Evaluation . . . . . . . . . . . . . . . . . . . . . . . 113

3.5 The Image Classification Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . 114

3.5.1 Reading the Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 114

3.5.2 Reading a Minibatch . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115

3.5.3 Putting All Things Together . . . . . . . . . . . . . . . . . . . . . . . . . . 116

ii

3.6 Implementation of Softmax Regression from Scratch . . . . . . . . . . . . . . . . 117 3.6.1 Initializing Model Parameters . . . . . . . . . . . . . . . . . . . . . . . . 117 3.6.2 Defining the Softmax Operation . . . . . . . . . . . . . . . . . . . . . . . 118 3.6.3 Defining the Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 119 3.6.4 Defining the Loss Function . . . . . . . . . . . . . . . . . . . . . . . . . . 119 3.6.5 Classification Accuracy . . . . . . . . . . . . . . . . . . . . . . . . . . . . 120 3.6.6 Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 121 3.6.7 Prediction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 123
3.7 Concise Implementation of Softmax Regression . . . . . . . . . . . . . . . . . . . 124 3.7.1 Initializing Model Parameters . . . . . . . . . . . . . . . . . . . . . . . . 125 3.7.2 Softmax Implementation Revisited . . . . . . . . . . . . . . . . . . . . . . 125 3.7.3 Optimization Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . 126 3.7.4 Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 126

4 Multilayer Perceptrons

129

4.1 Multilayer Perceptrons . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 129

4.1.1 Hidden Layers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 129

4.1.2 Activation Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 132

4.2 Implementation of Multilayer Perceptrons from Scratch . . . . . . . . . . . . . . 138

4.2.1 Initializing Model Parameters . . . . . . . . . . . . . . . . . . . . . . . . 138

4.2.2 Activation Function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 138

4.2.3 Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 139

4.2.4 Loss Function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 139

4.2.5 Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 139

4.3 Concise Implementation of Multilayer Perceptrons . . . . . . . . . . . . . . . . . 140

4.3.1 Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 141

4.4 Model Selection, Underfitting, and Overfitting . . . . . . . . . . . . . . . . . . . . 142

4.4.1 Training Error and Generalization Error . . . . . . . . . . . . . . . . . . . 143

4.4.2 Model Selection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 145

4.4.3 Underfitting or Overfitting? . . . . . . . . . . . . . . . . . . . . . . . . . . 146

4.4.4 Polynomial Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . 148

4.5 Weight Decay . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 152

4.5.1 Norms and Weight Decay . . . . . . . . . . . . . . . . . . . . . . . . . . . 153

4.5.2 High-Dimensional Linear Regression . . . . . . . . . . . . . . . . . . . . 154

4.5.3 Implementation from Scratch . . . . . . . . . . . . . . . . . . . . . . . . 155

4.5.4 Concise Implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . 157

4.6 Dropout . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 159

4.6.1 Overfitting Revisited . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 160

4.6.2 Robustness through Perturbations . . . . . . . . . . . . . . . . . . . . . . 160

4.6.3 Dropout in Practice . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 161

4.6.4 Implementation from Scratch . . . . . . . . . . . . . . . . . . . . . . . . 162

4.6.5 Concise Implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . 164

4.7 Forward Propagation, Backward Propagation, and Computational Graphs . . . . . 166

4.7.1 Forward Propagation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 166

4.7.2 Computational Graph of Forward Propagation . . . . . . . . . . . . . . . . 167

4.7.3 Backpropagation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 167

4.7.4 Training Neural Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . 168

4.8 Numerical Stability and Initialization . . . . . . . . . . . . . . . . . . . . . . . . . 170

4.8.1 Vanishing and Exploding Gradients . . . . . . . . . . . . . . . . . . . . . 170

4.8.2 Parameter Initialization . . . . . . . . . . . . . . . . . . . . . . . . . . . . 173

4.9 Environment and Distribution Shift . . . . . . . . . . . . . . . . . . . . . . . . . . 175

iii

4.9.1 Types of Distribution Shift . . . . . . . . . . . . . . . . . . . . . . . . . . 176 4.9.2 Examples of Distribution Shift . . . . . . . . . . . . . . . . . . . . . . . . 178 4.9.3 Correction of Distribution Shift . . . . . . . . . . . . . . . . . . . . . . . . 180 4.9.4 A Taxonomy of Learning Problems . . . . . . . . . . . . . . . . . . . . . . 183 4.9.5 Fairness, Accountability, and Transparency in Machine Learning . . . . . 185 4.10 Predicting House Prices on Kaggle . . . . . . . . . . . . . . . . . . . . . . . . . . 186 4.10.1 Downloading and Caching Datasets . . . . . . . . . . . . . . . . . . . . . 186 4.10.2 Kaggle . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 188 4.10.3 Accessing and Reading the Dataset . . . . . . . . . . . . . . . . . . . . . . 189 4.10.4 Data Preprocessing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 190 4.10.5 Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 191 4.10.6 K-Fold Cross-Validation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 192 4.10.7 Model Selection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 193 4.10.8 Submitting Predictions on Kaggle . . . . . . . . . . . . . . . . . . . . . . 194

5 Deep Learning Computation

197

5.1 Layers and Blocks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 197

5.1.1 A Custom Block . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 199

5.1.2 The Sequential Block . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 201

5.1.3 Executing Code in the Forward Propagation Function . . . . . . . . . . . . 202

5.1.4 Efficiency . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 203

5.2 Parameter Management . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 204

5.2.1 Parameter Access . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 205

5.2.2 Parameter Initialization . . . . . . . . . . . . . . . . . . . . . . . . . . . . 208

5.2.3 Tied Parameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 210

5.3 Deferred Initialization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 211

5.3.1 Instantiating a Network . . . . . . . . . . . . . . . . . . . . . . . . . . . . 212

5.4 Custom Layers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 214

5.4.1 Layers without Parameters . . . . . . . . . . . . . . . . . . . . . . . . . . 214

5.4.2 Layers with Parameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . 215

5.5 File I/O . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 216

5.5.1 Loading and Saving Tensors . . . . . . . . . . . . . . . . . . . . . . . . . 216

5.5.2 Loading and Saving Model Parameters . . . . . . . . . . . . . . . . . . . . 217

5.6 GPUs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 218

5.6.1 Computing Devices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 220

5.6.2 Tensors and GPUs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 221

5.6.3 Neural Networks and GPUs . . . . . . . . . . . . . . . . . . . . . . . . . . 223

6 Convolutional Neural Networks

225

6.1 From Fully-Connected Layers to Convolutions . . . . . . . . . . . . . . . . . . . . 226

6.1.1 Invariance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 226

6.1.2 Constraining the MLP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 227

6.1.3 Convolutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 229

6.1.4 “Whereʼs Waldo” Revisited . . . . . . . . . . . . . . . . . . . . . . . . . . 229

6.2 Convolutions for Images . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 231

6.2.1 The Cross-Correlation Operation . . . . . . . . . . . . . . . . . . . . . . . 231

6.2.2 Convolutional Layers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 233

6.2.3 Object Edge Detection in Images . . . . . . . . . . . . . . . . . . . . . . . 233

6.2.4 Learning a Kernel . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 234

6.2.5 Cross-Correlation and Convolution . . . . . . . . . . . . . . . . . . . . . . 235

6.2.6 Feature Map and Receptive Field . . . . . . . . . . . . . . . . . . . . . . . 236

iv

6.3 Padding and Stride . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 237 6.3.1 Padding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 237 6.3.2 Stride . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 239
6.4 Multiple Input and Multiple Output Channels . . . . . . . . . . . . . . . . . . . . 241 6.4.1 Multiple Input Channels . . . . . . . . . . . . . . . . . . . . . . . . . . . 241 6.4.2 Multiple Output Channels . . . . . . . . . . . . . . . . . . . . . . . . . . . 242 6.4.3 1 × 1 Convolutional Layer . . . . . . . . . . . . . . . . . . . . . . . . . . . 243
6.5 Pooling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 245 6.5.1 Maximum Pooling and Average Pooling . . . . . . . . . . . . . . . . . . . 246 6.5.2 Padding and Stride . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 247 6.5.3 Multiple Channels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 248
6.6 Convolutional Neural Networks (LeNet) . . . . . . . . . . . . . . . . . . . . . . . 250 6.6.1 LeNet . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 250 6.6.2 Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 253

7 Modern Convolutional Neural Networks

257

7.1 Deep Convolutional Neural Networks (AlexNet) . . . . . . . . . . . . . . . . . . . 257

7.1.1 Learning Representations . . . . . . . . . . . . . . . . . . . . . . . . . . . 258

7.1.2 AlexNet . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 261

7.1.3 Reading the Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 264

7.1.4 Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 264

7.2 Networks Using Blocks (VGG) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 265

7.2.1 VGG Blocks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 266

7.2.2 VGG Network . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 266

7.2.3 Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 268

7.3 Network in Network (NiN) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 270

7.3.1 NiN Blocks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 270

7.3.2 NiN Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 272

7.3.3 Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 273

7.4 Networks with Parallel Concatenations (GoogLeNet) . . . . . . . . . . . . . . . . 274

7.4.1 Inception Blocks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 274

7.4.2 GoogLeNet Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 275

7.4.3 Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 278

7.5 Batch Normalization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 279

7.5.1 Training Deep Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . 279

7.5.2 Batch Normalization Layers . . . . . . . . . . . . . . . . . . . . . . . . . . 281

7.5.3 Implementation from Scratch . . . . . . . . . . . . . . . . . . . . . . . . 282

7.5.4 Applying Batch Normalization in LeNet . . . . . . . . . . . . . . . . . . . 283

7.5.5 Concise Implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . 284

7.5.6 Controversy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 285

7.6 Residual Networks (ResNet) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 287

7.6.1 Function Classes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 287

7.6.2 Residual Blocks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 288

7.6.3 ResNet Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 291

7.6.4 Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 293

7.7 Densely Connected Networks (DenseNet) . . . . . . . . . . . . . . . . . . . . . . 294

7.7.1 From ResNet to DenseNet . . . . . . . . . . . . . . . . . . . . . . . . . . . 294

7.7.2 Dense Blocks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 295

7.7.3 Transition Layers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 296

7.7.4 DenseNet Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 297

7.7.5 Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 297

v

8 Recurrent Neural Networks

299

8.1 Sequence Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 299

8.1.1 Statistical Tools . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 301

8.1.2 Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 303

8.1.3 Prediction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 305

8.2 Text Preprocessing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 308

8.2.1 Reading the Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 309

8.2.2 Tokenization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 309

8.2.3 Vocabulary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 310

8.2.4 Putting All Things Together . . . . . . . . . . . . . . . . . . . . . . . . . . 312

8.3 Language Models and the Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . 313

8.3.1 Learning a Language Model . . . . . . . . . . . . . . . . . . . . . . . . . . 313

8.3.2 Markov Models and n-grams . . . . . . . . . . . . . . . . . . . . . . . . . 314

8.3.3 Natural Language Statistics . . . . . . . . . . . . . . . . . . . . . . . . . . 315

8.3.4 Reading Long Sequence Data . . . . . . . . . . . . . . . . . . . . . . . . . 317

8.4 Recurrent Neural Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 321

8.4.1 Neural Networks without Hidden States . . . . . . . . . . . . . . . . . . . 322

8.4.2 Recurrent Neural Networks with Hidden States . . . . . . . . . . . . . . . 322

8.4.3 RNN-based Character-Level Language Models . . . . . . . . . . . . . . . . 324

8.4.4 Perplexity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 325

8.5 Implementation of Recurrent Neural Networks from Scratch . . . . . . . . . . . . 327

8.5.1 One-Hot Encoding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 327

8.5.2 Initializing the Model Parameters . . . . . . . . . . . . . . . . . . . . . . 328

8.5.3 RNN Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 329

8.5.4 Prediction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 330

8.5.5 Gradient Clipping . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 330

8.5.6 Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 331

8.6 Concise Implementation of Recurrent Neural Networks . . . . . . . . . . . . . . . 335

8.6.1 Defining the Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 335

8.6.2 Training and Predicting . . . . . . . . . . . . . . . . . . . . . . . . . . . . 337

8.7 Backpropagation Through Time . . . . . . . . . . . . . . . . . . . . . . . . . . . . 338

8.7.1 Analysis of Gradients in RNNs . . . . . . . . . . . . . . . . . . . . . . . . 338

8.7.2 Backpropagation Through Time in Detail . . . . . . . . . . . . . . . . . . 341

9 Modern Recurrent Neural Networks

345

9.1 Gated Recurrent Units (GRU) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 345

9.1.1 Gated Hidden State . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 346

9.1.2 Implementation from Scratch . . . . . . . . . . . . . . . . . . . . . . . . 349

9.1.3 Concise Implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . 351

9.2 Long Short-Term Memory (LSTM) . . . . . . . . . . . . . . . . . . . . . . . . . . 352

9.2.1 Gated Memory Cell . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 353

9.2.2 Implementation from Scratch . . . . . . . . . . . . . . . . . . . . . . . . 356

9.2.3 Concise Implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . 358

9.3 Deep Recurrent Neural Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . 359

9.3.1 Functional Dependencies . . . . . . . . . . . . . . . . . . . . . . . . . . . 360

9.3.2 Concise Implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . 361

9.3.3 Training and Prediction . . . . . . . . . . . . . . . . . . . . . . . . . . . . 361

9.4 Bidirectional Recurrent Neural Networks . . . . . . . . . . . . . . . . . . . . . . . 363

9.4.1 Dynamic Programming in Hidden Markov Models . . . . . . . . . . . . . 363

9.4.2 Bidirectional Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 365

9.4.3 Training a Bidirectional RNN for a Wrong Application . . . . . . . . . . . 367

vi

9.5 Machine Translation and the Dataset . . . . . . . . . . . . . . . . . . . . . . . . . 368 9.5.1 Downloading and Preprocessing the Dataset . . . . . . . . . . . . . . . . 369 9.5.2 Tokenization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 370 9.5.3 Vocabulary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 371 9.5.4 Reading the Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 372 9.5.5 Putting All Things Together . . . . . . . . . . . . . . . . . . . . . . . . . . 373
9.6 Encoder-Decoder Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . 374 9.6.1 Encoder . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 374 9.6.2 Decoder . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 375 9.6.3 Putting the Encoder and Decoder Together . . . . . . . . . . . . . . . . . 375
9.7 Sequence to Sequence Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . 376 9.7.1 Encoder . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 377 9.7.2 Decoder . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 379 9.7.3 Loss Function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 380 9.7.4 Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 381 9.7.5 Prediction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 383 9.7.6 Evaluation of Predicted Sequences . . . . . . . . . . . . . . . . . . . . . . 384
9.8 Beam Search . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 386 9.8.1 Greedy Search . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 386 9.8.2 Exhaustive Search . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 387 9.8.3 Beam Search . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 388

10 Attention Mechanisms

391

10.1 Attention Cues . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 391

10.1.1 Attention Cues in Biology . . . . . . . . . . . . . . . . . . . . . . . . . . . 392

10.1.2 Queries, Keys, and Values . . . . . . . . . . . . . . . . . . . . . . . . . . . 393

10.1.3 Visualization of Attention . . . . . . . . . . . . . . . . . . . . . . . . . . . 394

10.2 Attention Pooling: Nadaraya-Watson Kernel Regression . . . . . . . . . . . . . . . 396

10.2.1 Generating the Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . 396

10.2.2 Average Pooling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 397

10.2.3 Nonparametric Attention Pooling . . . . . . . . . . . . . . . . . . . . . . 398

10.2.4 Parametric Attention Pooling . . . . . . . . . . . . . . . . . . . . . . . . 400

10.3 Attention Scoring Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 403

10.3.1 Masked Softmax Operation . . . . . . . . . . . . . . . . . . . . . . . . . . 405

10.3.2 Additive Attention . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 406

10.3.3 Scaled Dot-Product Attention . . . . . . . . . . . . . . . . . . . . . . . . . 407

10.4 Bahdanau Attention . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 409

10.4.1 Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 410

10.4.2 Defining the Decoder with Attention . . . . . . . . . . . . . . . . . . . . . 410

10.4.3 Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 412

10.5 Multi-Head Attention . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 414

10.5.1 Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 415

10.5.2 Implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 415

10.6 Self-Attention and Positional Encoding . . . . . . . . . . . . . . . . . . . . . . . . 418

10.6.1 Self-Attention . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 418

10.6.2 Comparing CNNs, RNNs, and Self-Attention . . . . . . . . . . . . . . . . . 418

10.6.3 Positional Encoding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 420

10.7 Transformer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 423

10.7.1 Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 423

10.7.2 Positionwise Feed-Forward Networks . . . . . . . . . . . . . . . . . . . . 425

10.7.3 Residual Connection and Layer Normalization . . . . . . . . . . . . . . . 426

vii

10.7.4 Encoder . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 427 10.7.5 Decoder . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 428 10.7.6 Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 430

11 Optimization Algorithms

435

11.1 Optimization and Deep Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . 435

11.1.1 Goal of Optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 436

11.1.2 Optimization Challenges in Deep Learning . . . . . . . . . . . . . . . . . 437

11.2 Convexity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 441

11.2.1 Definitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 441

11.2.2 Properties . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 444

11.2.3 Constraints . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 447

11.3 Gradient Descent . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 450

11.3.1 One-Dimensional Gradient Descent . . . . . . . . . . . . . . . . . . . . . 450

11.3.2 Multivariate Gradient Descent . . . . . . . . . . . . . . . . . . . . . . . . 453

11.3.3 Adaptive Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 455

11.4 Stochastic Gradient Descent . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 460

11.4.1 Stochastic Gradient Updates . . . . . . . . . . . . . . . . . . . . . . . . . 460

11.4.2 Dynamic Learning Rate . . . . . . . . . . . . . . . . . . . . . . . . . . . . 462

11.4.3 Convergence Analysis for Convex Objectives . . . . . . . . . . . . . . . . 464

11.4.4 Stochastic Gradients and Finite Samples . . . . . . . . . . . . . . . . . . . 466

11.5 Minibatch Stochastic Gradient Descent . . . . . . . . . . . . . . . . . . . . . . . . 467

11.5.1 Vectorization and Caches . . . . . . . . . . . . . . . . . . . . . . . . . . . 467

11.5.2 Minibatches . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 469

11.5.3 Reading the Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 470

11.5.4 Implementation from Scratch . . . . . . . . . . . . . . . . . . . . . . . . 471

11.5.5 Concise Implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . 475

11.6 Momentum . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 476

11.6.1 Basics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 477

11.6.2 Practical Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 481

11.6.3 Theoretical Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 484

11.7 Adagrad . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 487

11.7.1 Sparse Features and Learning Rates . . . . . . . . . . . . . . . . . . . . . 487

11.7.2 Preconditioning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 487

11.7.3 The Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 489

11.7.4 Implementation from Scratch . . . . . . . . . . . . . . . . . . . . . . . . 491

11.7.5 Concise Implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . 491

11.8 RMSProp . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 493

11.8.1 The Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 493

11.8.2 Implementation from Scratch . . . . . . . . . . . . . . . . . . . . . . . . 494

11.8.3 Concise Implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . 496

11.9 Adadelta . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 497

11.9.1 The Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 497

11.9.2 Implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 497

11.10 Adam . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 499

11.10.1 The Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 500

11.10.2 Implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 501

11.10.3 Yogi . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 502

11.11 Learning Rate Scheduling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 504

11.11.1 Toy Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 504

11.11.2 Schedulers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 506

viii

11.11.3 Policies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 508

12 Computational Performance

515

12.1 Compilers and Interpreters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 515

12.1.1 Symbolic Programming . . . . . . . . . . . . . . . . . . . . . . . . . . . . 516

12.1.2 Hybrid Programming . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 517

12.1.3 Hybridizing the Sequential Class . . . . . . . . . . . . . . . . . . . . . . . 518

12.2 Asynchronous Computation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 522

12.2.1 Asynchrony via Backend . . . . . . . . . . . . . . . . . . . . . . . . . . . 523

12.2.2 Barriers and Blockers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 525

12.2.3 Improving Computation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 526

12.3 Automatic Parallelism . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 527

12.3.1 Parallel Computation on GPUs . . . . . . . . . . . . . . . . . . . . . . . . 528

12.3.2 Parallel Computation and Communication . . . . . . . . . . . . . . . . . 529

12.4 Hardware . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 531

12.4.1 Computers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 532

12.4.2 Memory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 533

12.4.3 Storage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 534

12.4.4 CPUs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 535

12.4.5 GPUs and other Accelerators . . . . . . . . . . . . . . . . . . . . . . . . . 538

12.4.6 Networks and Buses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 541

12.4.7 More Latency Numbers . . . . . . . . . . . . . . . . . . . . . . . . . . . . 542

12.5 Training on Multiple GPUs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 544

12.5.1 Splitting the Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 544

12.5.2 Data Parallelism . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 546

12.5.3 A Toy Network . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 548

12.5.4 Data Synchronization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 548

12.5.5 Distributing Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 549

12.5.6 Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 550

12.6 Concise Implementation for Multiple GPUs . . . . . . . . . . . . . . . . . . . . . 553

12.6.1 A Toy Network . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 553

12.6.2 Network Initialization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 554

12.6.3 Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 555

12.7 Parameter Servers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 558

12.7.1 Data-Parallel Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 558

12.7.2 Ring Synchronization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 561

12.7.3 Multi-Machine Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . 563

12.7.4 Key–Value Stores . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 565

13 Computer Vision

567

13.1 Image Augmentation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 567

13.1.1 Common Image Augmentation Methods . . . . . . . . . . . . . . . . . . . 568

13.1.2 Training with Image Augmentation . . . . . . . . . . . . . . . . . . . . . 572

13.2 Fine-Tuning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 576

13.2.1 Steps . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 576

13.2.2 Hot Dog Recognition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 577

13.3 Object Detection and Bounding Boxes . . . . . . . . . . . . . . . . . . . . . . . . 582

13.3.1 Bounding Boxes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 583

13.4 Anchor Boxes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 585

13.4.1 Generating Multiple Anchor Boxes . . . . . . . . . . . . . . . . . . . . . . 585

13.4.2 Intersection over Union (IoU) . . . . . . . . . . . . . . . . . . . . . . . . . 588

ix

13.4.3 Labeling Anchor Boxes in Training Data . . . . . . . . . . . . . . . . . . . 589 13.4.4 Predicting Bounding Boxes with Non-Maximum Suppression . . . . . . . 594 13.5 Multiscale Object Detection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 598 13.5.1 Multiscale Anchor Boxes . . . . . . . . . . . . . . . . . . . . . . . . . . . 599 13.5.2 Multiscale Detection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 601 13.6 The Object Detection Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 602 13.6.1 Downloading the Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . 602 13.6.2 Reading the Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 603 13.6.3 Demonstration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 604 13.7 Single Shot Multibox Detection . . . . . . . . . . . . . . . . . . . . . . . . . . . . 605 13.7.1 Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 606 13.7.2 Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 611 13.7.3 Prediction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 614 13.8 Region-based CNNs (R-CNNs) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 617 13.8.1 R-CNNs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 617 13.8.2 Fast R-CNN . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 618 13.8.3 Faster R-CNN . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 621 13.8.4 Mask R-CNN . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 622 13.9 Semantic Segmentation and the Dataset . . . . . . . . . . . . . . . . . . . . . . . 623 13.9.1 Image Segmentation and Instance Segmentation . . . . . . . . . . . . . . 623 13.9.2 The Pascal VOC2012 Semantic Segmentation Dataset . . . . . . . . . . . . 624 13.10 Transposed Convolution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 629 13.10.1 Basic Operation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 630 13.10.2 Padding, Strides, and Multiple Channels . . . . . . . . . . . . . . . . . . . 631 13.10.3 Connection to Matrix Transposition . . . . . . . . . . . . . . . . . . . . . 633 13.11 Fully Convolutional Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 635 13.11.1 The Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 635 13.11.2 Initializing Transposed Convolutional Layers . . . . . . . . . . . . . . . . 637 13.11.3 Reading the Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 638 13.11.4 Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 639 13.11.5 Prediction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 639 13.12 Neural Style Transfer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 641 13.12.1 Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 642 13.12.2 Reading the Content and Style Images . . . . . . . . . . . . . . . . . . . . 643 13.12.3 Preprocessing and Postprocessing . . . . . . . . . . . . . . . . . . . . . . 644 13.12.4 Extracting Features . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 644 13.12.5 Defining the Loss Function . . . . . . . . . . . . . . . . . . . . . . . . . . 646 13.12.6 Initializing the Synthesized Image . . . . . . . . . . . . . . . . . . . . . . 647 13.12.7 Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 648 13.13 Image Classification (CIFAR-10) on Kaggle . . . . . . . . . . . . . . . . . . . . . . 650 13.13.1 Obtaining and Organizing the Dataset . . . . . . . . . . . . . . . . . . . . 651 13.13.2 Image Augmentation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 653 13.13.3 Reading the Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 654 13.13.4 Defining the Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 655 13.13.5 Defining the Training Function . . . . . . . . . . . . . . . . . . . . . . . . 656 13.13.6 Training and Validating the Model . . . . . . . . . . . . . . . . . . . . . . 657 13.13.7 Classifying the Testing Set and Submitting Results on Kaggle . . . . . . . . 657 13.14 Dog Breed Identification (ImageNet Dogs) on Kaggle . . . . . . . . . . . . . . . . 659 13.14.1 Obtaining and Organizing the Dataset . . . . . . . . . . . . . . . . . . . . 660 13.14.2 Image Augmentation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 661 13.14.3 Reading the Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 661
x

13.14.4 Fine-Tuning a Pretrained Model . . . . . . . . . . . . . . . . . . . . . . . 662 13.14.5 Defining the Training Function . . . . . . . . . . . . . . . . . . . . . . . . 663 13.14.6 Training and Validating the Model . . . . . . . . . . . . . . . . . . . . . . 664 13.14.7 Classifying the Testing Set and Submitting Results on Kaggle . . . . . . . . 664

14 Natural Language Processing: Pretraining

667

14.1 Word Embedding (word2vec) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 668

14.1.1 One-Hot Vectors Are a Bad Choice . . . . . . . . . . . . . . . . . . . . . . 668

14.1.2 Self-Supervised word2vec . . . . . . . . . . . . . . . . . . . . . . . . . . . 669

14.1.3 The Skip-Gram Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 669

14.1.4 The Continuous Bag of Words (CBOW) Model . . . . . . . . . . . . . . . . 671

14.2 Approximate Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 673

14.2.1 Negative Sampling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 673

14.2.2 Hierarchical Softmax . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 674

14.3 The Dataset for Pretraining Word Embeddings . . . . . . . . . . . . . . . . . . . . 676

14.3.1 Reading the Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 676

14.3.2 Subsampling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 677

14.3.3 Extracting Center Words and Context Words . . . . . . . . . . . . . . . . 678

14.3.4 Negative Sampling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 680

14.3.5 Loading Training Examples in Minibatches . . . . . . . . . . . . . . . . . 681

14.3.6 Putting All Things Together . . . . . . . . . . . . . . . . . . . . . . . . . . 682

14.4 Pretraining word2vec . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 683

14.4.1 The Skip-Gram Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 683

14.4.2 Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 685

14.4.3 Applying Word Embeddings . . . . . . . . . . . . . . . . . . . . . . . . . 687

14.5 Word Embedding with Global Vectors (GloVe) . . . . . . . . . . . . . . . . . . . . 688

14.5.1 Skip-Gram with Global Corpus Statistics . . . . . . . . . . . . . . . . . . . 688

14.5.2 The GloVe Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 689

14.5.3 Interpreting GloVe from the Ratio of Co-occurrence Probabilities . . . . . 689

14.6 Subword Embedding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 691

14.6.1 The fastText Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 691

14.6.2 Byte Pair Encoding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 692

14.7 Word Similarity and Analogy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 695

14.7.1 Loading Pretrained Word Vectors . . . . . . . . . . . . . . . . . . . . . . 695

14.7.2 Applying Pretrained Word Vectors . . . . . . . . . . . . . . . . . . . . . . 697

14.8 Bidirectional Encoder Representations from Transformers (BERT) . . . . . . . . . 699

14.8.1 From Context-Independent to Context-Sensitive . . . . . . . . . . . . . . 699

14.8.2 From Task-Specific to Task-Agnostic . . . . . . . . . . . . . . . . . . . . . 700

14.8.3 BERT: Combining the Best of Both Worlds . . . . . . . . . . . . . . . . . . 700

14.8.4 Input Representation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 701

14.8.5 Pretraining Tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 704

14.8.6 Putting All Things Together . . . . . . . . . . . . . . . . . . . . . . . . . . 706

14.9 The Dataset for Pretraining BERT . . . . . . . . . . . . . . . . . . . . . . . . . . . 708

14.9.1 Defining Helper Functions for Pretraining Tasks . . . . . . . . . . . . . . 709

14.9.2 Transforming Text into the Pretraining Dataset . . . . . . . . . . . . . . . 711

14.10 Pretraining BERT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 714

14.10.1 Pretraining BERT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 714

14.10.2 Representing Text with BERT . . . . . . . . . . . . . . . . . . . . . . . . . 716

15 Natural Language Processing: Applications

719

15.1 Sentiment Analysis and the Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . 720

xi

15.1.1 Reading the Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 720 15.1.2 Preprocessing the Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . 721 15.1.3 Creating Data Iterators . . . . . . . . . . . . . . . . . . . . . . . . . . . . 722 15.1.4 Putting All Things Together . . . . . . . . . . . . . . . . . . . . . . . . . . 723 15.2 Sentiment Analysis: Using Recurrent Neural Networks . . . . . . . . . . . . . . . 724 15.2.1 Representing Single Text with RNNs . . . . . . . . . . . . . . . . . . . . . 724 15.2.2 Loading Pretrained Word Vectors . . . . . . . . . . . . . . . . . . . . . . 725 15.2.3 Training and Evaluating the Model . . . . . . . . . . . . . . . . . . . . . . 726 15.3 Sentiment Analysis: Using Convolutional Neural Networks . . . . . . . . . . . . . 727 15.3.1 One-Dimensional Convolutions . . . . . . . . . . . . . . . . . . . . . . . . 728 15.3.2 Max-Over-Time Pooling . . . . . . . . . . . . . . . . . . . . . . . . . . . . 730 15.3.3 The textCNN Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 730 15.4 Natural Language Inference and the Dataset . . . . . . . . . . . . . . . . . . . . . 734 15.4.1 Natural Language Inference . . . . . . . . . . . . . . . . . . . . . . . . . 734 15.4.2 The Stanford Natural Language Inference (SNLI) Dataset . . . . . . . . . . 735 15.5 Natural Language Inference: Using Attention . . . . . . . . . . . . . . . . . . . . 738 15.5.1 The Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 739 15.5.2 Training and Evaluating the Model . . . . . . . . . . . . . . . . . . . . . . 743 15.6 Fine-Tuning BERT for Sequence-Level and Token-Level Applications . . . . . . . . 745 15.6.1 Single Text Classification . . . . . . . . . . . . . . . . . . . . . . . . . . . 746 15.6.2 Text Pair Classification or Regression . . . . . . . . . . . . . . . . . . . . 746 15.6.3 Text Tagging . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 747 15.6.4 Question Answering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 748 15.7 Natural Language Inference: Fine-Tuning BERT . . . . . . . . . . . . . . . . . . . 750 15.7.1 Loading Pretrained BERT . . . . . . . . . . . . . . . . . . . . . . . . . . . 751 15.7.2 The Dataset for Fine-Tuning BERT . . . . . . . . . . . . . . . . . . . . . . 752 15.7.3 Fine-Tuning BERT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 753

16 Appendix: Tools for Deep Learning

757

16.1 Using Jupyter . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 757

16.1.1 Editing and Running the Code Locally . . . . . . . . . . . . . . . . . . . . 757

16.1.2 Advanced Options . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 761

16.2 Using Amazon SageMaker . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 762

16.2.1 Registering and Logging In . . . . . . . . . . . . . . . . . . . . . . . . . . 762

16.2.2 Creating a SageMaker Instance . . . . . . . . . . . . . . . . . . . . . . . . 763

16.2.3 Running and Stopping an Instance . . . . . . . . . . . . . . . . . . . . . . 764

16.2.4 Updating Notebooks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 765

16.3 Using AWS EC2 Instances . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 766

16.3.1 Creating and Running an EC2 Instance . . . . . . . . . . . . . . . . . . . . 766

16.3.2 Installing CUDA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 771

16.3.3 Installing MXNet and Downloading the D2L Notebooks . . . . . . . . . . . 772

16.3.4 Running Jupyter . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 773

16.3.5 Closing Unused Instances . . . . . . . . . . . . . . . . . . . . . . . . . . . 774

16.4 Using Google Colab . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 774

16.5 Selecting Servers and GPUs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 775

16.5.1 Selecting Servers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 776

16.5.2 Selecting GPUs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 777

16.6 Contributing to This Book . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 780

16.6.1 Minor Text Changes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 780

16.6.2 Propose a Major Change . . . . . . . . . . . . . . . . . . . . . . . . . . . 780

16.6.3 Adding a New Section or a New Framework Implementation . . . . . . . . 781

xii

16.6.4 Submitting a Major Change . . . . . . . . . . . . . . . . . . . . . . . . . . 781 16.7 d2l API Document . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 785

Bibliography

807

Python Module Index

819

Index

821

xiii

xiv

Preface
Just a few years ago, there were no legions of deep learning scientists developing intelligent products and services at major companies and startups. When we entered the field, machine learning did not command headlines in daily newspapers. Our parents had no idea what machine learning was, let alone why we might prefer it to a career in medicine or law. Machine learning was a blue skies academic discipline whose industrial significance was limited to a narrow set of real-world applications, including speech recognition and computer vision. Moreover, many of these applications required so much domain knowledge that they were often regarded as entirely separate areas for which machine learning was one small component. At that time, neural networks—the predecessors of the deep learning methods that we focus on in this book—were generally regarded as outmoded. In just the past five years, deep learning has taken the world by surprise, driving rapid progress in such diverse fields as computer vision, natural language processing, automatic speech recognition, reinforcement learning, and biomedical informatics. Moreover, the success of deep learning on so many tasks of practical interest has even catalyzed developments in theoretical machine learning and statistics. With these advances in hand, we can now build cars that drive themselves with more autonomy than ever before (and less autonomy than some companies might have you believe), smart reply systems that automatically draft the most mundane emails, helping people dig out from oppressively large inboxes, and software agents that dominate the worldʼs best humans at board games like Go, a feat once thought to be decades away. Already, these tools exert ever-wider impacts on industry and society, changing the way movies are made, diseases are diagnosed, and playing a growing role in basic sciences—from astrophysics to biology.
About This Book
This book represents our attempt to make deep learning approachable, teaching you the concepts, the context, and the code.
One Medium Combining Code, Math, and HTML
For any computing technology to reach its full impact, it must be well-understood, welldocumented, and supported by mature, well-maintained tools. The key ideas should be clearly distilled, minimizing the onboarding time needing to bring new practitioners up to date. Mature libraries should automate common tasks, and exemplar code should make it easy for practitioners to modify, apply, and extend common applications to suit their needs. Take dynamic web applications as an example. Despite a large number of companies, like Amazon, developing successful database-driven web applications in the 1990s, the potential of this technology to aid creative en-
1

trepreneurs has been realized to a far greater degree in the past ten years, owing in part to the development of powerful, well-documented frameworks.
Testing the potential of deep learning presents unique challenges because any single application brings together various disciplines. Applying deep learning requires simultaneously understanding (i) the motivations for casting a problem in a particular way; (ii) the mathematical form of a given model; (iii) the optimization algorithms for fitting the models to data; (iv) the statistical principles that tell us when we should expect our models to generalize to unseen data and practical methods for certifying that they have, in fact, generalized; and (v) the engineering techniques required to train models efficiently, navigating the pitfalls of numerical computing and getting the most out of available hardware. Teaching both the critical thinking skills required to formulate problems, the mathematics to solve them, and the software tools to implement those solutions all in one place presents formidable challenges. Our goal in this book is to present a unified resource to bring would-be practitioners up to speed.
When we started this book project, there were no resources that simultaneously (i) were up to date; (ii) covered the full breadth of modern machine learning with substantial technical depth; and (iii) interleaved exposition of the quality one expects from an engaging textbook with the clean runnable code that one expects to find in hands-on tutorials. We found plenty of code examples for how to use a given deep learning framework (e.g., how to do basic numerical computing with matrices in TensorFlow) or for implementing particular techniques (e.g., code snippets for LeNet, AlexNet, ResNets, etc.) scattered across various blog posts and GitHub repositories. However, these examples typically focused on how to implement a given approach, but left out the discussion of why certain algorithmic decisions are made. While some interactive resources have popped up sporadically to address a particular topic, e.g., the engaging blog posts published on the website Distill3, or personal blogs, they only covered selected topics in deep learning, and often lacked associated code. On the other hand, while several deep learning textbooks have emerged—e.g., (Goodfellow et al., 2016), which offers a comprehensive survey on the basics of deep learning— these resources do not marry the descriptions to realizations of the concepts in code, sometimes leaving readers clueless as to how to implement them. Moreover, too many resources are hidden behind the paywalls of commercial course providers.
We set out to create a resource that could (i) be freely available for everyone; (ii) offer sufficient technical depth to provide a starting point on the path to actually becoming an applied machine learning scientist; (iii) include runnable code, showing readers how to solve problems in practice; (iv) allow for rapid updates, both by us and also by the community at large; and (v) be complemented by a forum4 for interactive discussion of technical details and to answer questions.
These goals were often in conflict. Equations, theorems, and citations are best managed and laid out in LaTeX. Code is best described in Python. And webpages are native in HTML and JavaScript. Furthermore, we want the content to be accessible both as executable code, as a physical book, as a downloadable PDF, and on the Internet as a website. At present there exist no tools and no workflow perfectly suited to these demands, so we had to assemble our own. We describe our approach in detail in Section 16.6. We settled on GitHub to share the source and to facilitate community contributions, Jupyter notebooks for mixing code, equations and text, Sphinx as a rendering engine to generate multiple outputs, and Discourse for the forum. While our system is not yet perfect, these choices provide a good compromise among the competing concerns. We believe that this might be the first book published using such an integrated workflow.
3 http://distill.pub 4 http://discuss.d2l.ai

2

Contents

Learning by Doing
Many textbooks present concepts in succession, covering each in exhaustive detail. For example, Chris Bishopʼs excellent textbook (Bishop, 2006), teaches each topic so thoroughly that getting to the chapter on linear regression requires a non-trivial amount of work. While experts love this book precisely for its thoroughness, for true beginners, this property limits its usefulness as an introductory text.
In this book, we will teach most concepts just in time. In other words, you will learn concepts at the very moment that they are needed to accomplish some practical end. While we take some time at the outset to teach fundamental preliminaries, like linear algebra and probability, we want you to taste the satisfaction of training your first model before worrying about more esoteric probability distributions.
Aside from a few preliminary notebooks that provide a crash course in the basic mathematical background, each subsequent chapter introduces both a reasonable number of new concepts and provides single self-contained working examples—using real datasets. This presents an organizational challenge. Some models might logically be grouped together in a single notebook. And some ideas might be best taught by executing several models in succession. On the other hand, there is a big advantage to adhering to a policy of one working example, one notebook: This makes it as easy as possible for you to start your own research projects by leveraging our code. Just copy a notebook and start modifying it.
We will interleave the runnable code with background material as needed. In general, we will often err on the side of making tools available before explaining them fully (and we will follow up by explaining the background later). For instance, we might use stochastic gradient descent before fully explaining why it is useful or why it works. This helps to give practitioners the necessary ammunition to solve problems quickly, at the expense of requiring the reader to trust us with some curatorial decisions.
This book will teach deep learning concepts from scratch. Sometimes, we want to delve into fine details about the models that would typically be hidden from the user by deep learning frameworksʼ advanced abstractions. This comes up especially in the basic tutorials, where we want you to understand everything that happens in a given layer or optimizer. In these cases, we will often present two versions of the example: one where we implement everything from scratch, relying only on NumPy-like functionality and automatic differentiation, and another, more practical example, where we write succinct code using the high-level APIs of deep learning frameworks. Once we have taught you how some component works, we can just use the high-level APIs in subsequent tutorials.

Contents

3

Content and Structure
The book can be roughly divided into three parts, focusing on preliminaries, deep learning techniques, and advanced topics focused on real systems and applications (Fig. 1).

Fig. 1: Book structure
• The first part covers basics and preliminaries. Chapter 1 offers an introduction to deep learning. Then, in Chapter 2, we quickly bring you up to speed on the prerequisites required for hands-on deep learning, such as how to store and manipulate data, and how to apply various numerical operations based on basic concepts from linear algebra, calculus, and probability. Chapter 3 and Chapter 4 cover the most basic concepts and techniques in deep learning, including regression and classification; linear models and multilayer perceptrons; and overfitting and regularization.
• The next five chapters focus on modern deep learning techniques. Chapter 5 describes the key computational components of deep learning systems and lays the groundwork for our subsequent implementations of more complex models. Next, Chapter 6 and Chapter 7, introduce convolutional neural networks (CNNs), powerful tools that form the backbone of most modern computer vision systems. Similarly, Chapter 8 and Chapter 9 introduce recurrent neural networks (RNNs), models that exploit sequential (e.g., temporal) structure in data and are commonly used for natural language processing and time series prediction. In Chapter 10, we introduce a relatively new class of models based on so-called attention mechanisms that has displaced RNNs as the dominant architecture for most natural language processing tasks. These sections will bring you up to speed on the most powerful and general tools that are widely used by deep learning practitioners.
• Part three discusses scalability, efficiency, and applications. First, in Chapter 11, we dis-

4

Contents

cuss several common optimization algorithms used to train deep learning models. The next chapter, Chapter 12, examines several key factors that influence the computational performance of your deep learning code. In Chapter 13, we illustrate major applications of deep learning in computer vision. In Chapter 14 and Chapter 15, we show how to pretrain language representation models and apply them to natural language processing tasks.
Code
Most sections of this book feature executable code. We believe that some intuitions are best developed via trial and error, tweaking the code in small ways and observing the results. Ideally, an elegant mathematical theory might tell us precisely how to tweak our code to achieve a desired result. However, today deep learning practitioners today must often tread where no cogent theory can provide firm guidance. Despite our best attempts, formal explanations for the efficacy of various techniques are still lacking, both because the mathematics to characterize these models can be so difficult and also because serious inquiry on these topics has only just recently kicked into high gear. We are hopeful that as the theory of deep learning progresses, future editions of this book can provide insights that eclipse those presently available.
To avoid unnecessary repetition, we encapsulate some of our most frequently imported and referred-to functions and classes in the d2l package. To indicate a block of code, such as a function, class, or collection of import statements, that will be subsequently accessed via the d2l package, we will mark it with #@save. We offer a detailed overview of these functions and classes in Section 16.7. The d2l package is lightweight and only requires the following dependencies:
#@save import collections import hashlib import math import os import random import re import shutil import sys import tarfile import time import zipfile from collections import defaultdict import pandas as pd import requests from IPython import display from matplotlib import pyplot as plt
d2l = sys.modules[__name__]
Most of the code in this book is based on Apache MXNet, an open-source framework for deep learning that is the preferred choice of AWS (Amazon Web Services), as well as many colleges and companies. All of the code in this book has passed tests under the newest MXNet version. However, due to the rapid development of deep learning, some code in the print edition may not work properly in future versions of MXNet. We plan to keep the online version up-to-date. In case you encounter any problems, please consult Installation (page 9) to update your code and runtime environment.
Here is how we import modules from MXNet.

Contents

5

#@save from mxnet import autograd, context, gluon, image, init, np, npx from mxnet.gluon import nn, rnn

Target Audience
This book is for students (undergraduate or graduate), engineers, and researchers, who seek a solid grasp of the practical techniques of deep learning. Because we explain every concept from scratch, no previous background in deep learning or machine learning is required. Fully explaining the methods of deep learning requires some mathematics and programming, but we will only assume that you come in with some basics, including modest amounts of linear algebra, calculus, probability, and Python programming. Just in case you forget the basics, the Appendix provides a refresher on most of the mathematics you will find in this book. Most of the time, we will prioritize intuition and ideas over mathematical rigor. If you would like to extend these foundations beyond the prerequisites to understand our book, we happily recommend some other terrific resources: Linear Analysis by Bela Bollobas (Bollobas, 1999) covers linear algebra and functional analysis in great depth. All of Statistics (Wasserman, 2013) provides a marvelous introduction to statistics. Joe Blitzsteinʼs books5 and courses6 on probability and inference are pedagogical gems. And if you have not used Python before, you may want to peruse this Python tutorial7.
Forum
Associated with this book, we have launched a discussion forum, located at discuss.d2l.ai8. When you have questions on any section of the book, you can find a link to the associated discussion page at the end of each notebook.

Acknowledgments
We are indebted to the hundreds of contributors for both the English and the Chinese drafts. They helped improve the content and offered valuable feedback. Specifically, we thank every contributor of this English draft for making it better for everyone. Their GitHub IDs or names are (in no particular order): alxnorden, avinashingit, bowen0701, brettkoonce, Chaitanya Prakash Bapat, cryptonaut, Davide Fiocco, edgarroman, gkutiel, John Mitro, Liang Pu, Rahul Agarwal, Mohamed Ali Jamaoui, Michael (Stu) Stewart, Mike Müller, NRauschmayr, Prakhar Srivastav, sad-, sfermigier, Sheng Zha, sundeepteki, topecongiro, tpdi, vermicelli, Vishaal Kapoor, Vishwesh Ravi Shrimali, YaYaB, Yuhong Chen, Evgeniy Smirnov, lgov, Simon Corston-Oliver, Igor Dzreyev, Ha Nguyen, pmuens, Andrei Lukovenko, senorcinco, vfdev-5, dsweet, Mohammad Mahdi Rahimi, Abhishek Gupta, uwsd, DomKM, Lisa Oakley, Bowen Li, Aarush Ahuja, Prasanth Buddareddygari, brianhendee, mani2106, mtn, lkevinzc, caojilin, Lakshya, Fiete Lüer, Surbhi Vijayvargeeya, Muhyun Kim, dennismalmgren, adursun, Anirudh Dagar, liqingnz, Pedro Larroy, lgov, ati-ozgur, Jun Wu, Matthias Blume, Lin Yuan, geogunow, Josh Gardner, Maximilian Böther, Rakib Islam, Leonard Lausen, Abhinav Upadhyay, rongruosong, Steve Sedlmeyer, Ruslan Baratov, Rafael Schlatter, liusy182, Giannis Pappas, ati-ozgur, qbaza, dchoi77, Adam Gerson, Phuc
5 https://www.amazon.com/Introduction-Probability-Chapman-Statistical-Science/dp/1138369918 6 https://projects.iq.harvard.edu/stat110/home 7 http://learnpython.org/ 8 https://discuss.d2l.ai/

6

Contents

Le, Mark Atwood, christabella, vn09, Haibin Lin, jjangga0214, RichyChen, noelo, hansent, Giel Dops, dvincent1337, WhiteD3vil, Peter Kulits, codypenta, joseppinilla, ahmaurya, karolszk, heytitle, Peter Goetz, rigtorp, Tiep Vu, sfilip, mlxd, Kale-ab Tessera, Sanjar Adilov, MatteoFerrara, hsneto, Katarzyna Biesialska, Gregory Bruss, Duy–Thanh Doan, paulaurel, graytowne, Duc Pham, sl7423, Jaedong Hwang, Yida Wang, cys4, clhm, Jean Kaddour, austinmw, trebeljahr, tbaums, Cuong V. Nguyen, pavelkomarov, vzlamal, NotAnotherSystem, J-Arun-Mani, jancio, eldarkurtic, the-great-shazbot, doctorcolossus, gducharme, cclauss, Daniel-Mietchen, hoonose, biagiom, abhinavsp0730, jonathanhrandall, ysraell, Nodar Okroshiashvili, UgurKap, Jiyang Kang, StevenJokes, Tomer Kaftan, liweiwp, netyster, ypandya, NishantTharani, heiligerl, SportsTHU, Hoa Nguyen, manuel-arno-korfmann-webentwicklung, aterzis-personal, nxby, Xiaoting He, Josiah Yoder, mathresearch, mzz2017, jroberayalas, iluu, ghejc, BSharmi, vkramdev, simonwardjones, LakshKD, TalNeoran, djliden, Nikhil95, Oren Barkan, guoweis, haozhu233, pratikhack, Yue Ying, tayfununal, steinsag, charleybeller, Andrew Lumsdaine, Jiekui Zhang, Deepak Pathak, Florian Donhauser, Tim Gates, Adriaan Tijsseling, Ron Medina, Gaurav Saha, Murat Semerci, Lei Mao, Levi McClenny, Joshua Broyde, jake221, jonbally, zyhazwraith, Brian Pulfer, Nick Tomasino, Lefan Zhang, Hongshen Yang, Vinney Cavallo, yuntai, Yuanxiang Zhu, amarazov, pasricha, Ben Greenawald, Shivam Upadhyay, Quanshangze Du, Biswajit Sahoo, Parthe Pandit, Ishan Kumar, HomunculusK, Lane Schwartz, varadgunjal, Jason Wiener, Armin Gholampoor, Shreshtha13, eigen-arnav, Hyeonggyu Kim, EmilyOng, Bálint Mucsányi, Chase DuBois.
We thank Amazon Web Services, especially Swami Sivasubramanian, Raju Gulabani, Charlie Bell, and Andrew Jassy for their generous support in writing this book. Without the available time, resources, discussions with colleagues, and continuous encouragement, this book would not have happened.
Summary
• Deep learning has revolutionized pattern recognition, introducing technology that now powers a wide range of technologies, including computer vision, natural language processing, automatic speech recognition.
• To successfully apply deep learning, you must understand how to cast a problem, the mathematics of modeling, the algorithms for fitting your models to data, and the engineering techniques to implement it all.
• This book presents a comprehensive resource, including prose, figures, mathematics, and code, all in one place.
• To answer questions related to this book, visit our forum at https://discuss.d2l.ai/.
• All notebooks are available for download on GitHub.
Exercises
1. Register an account on the discussion forum of this book discuss.d2l.ai9.
2. Install Python on your computer.
9 https://discuss.d2l.ai/

Contents

7

3. Follow the links at the bottom of the section to the forum, where you will be able to seek out help and discuss the book and find answers to your questions by engaging the authors and broader community.
Discussions10

10 https://discuss.d2l.ai/t/18
8

Contents

Installation
In order to get you up and running for hands-on learning experience, we need to set you up with an environment for running Python, Jupyter notebooks, the relevant libraries, and the code needed to run the book itself.
Installing Miniconda
The simplest way to get going will be to install Miniconda11. The Python 3.x version is required. You can skip the following steps if your machine already has conda installed. Visit the Miniconda website and determine the appropriate version for your system based on your Python 3.x version and machine architecture. For example, if you are using macOS and Python 3.x you would download the bash script whose name contains the strings “Miniconda3” and “MacOSX”, navigate to the download location, and execute the installation as follows: sh Miniconda3-latest-MacOSX-x86_64.sh -b A Linux user with Python 3.x would download the file whose name contains the strings “Miniconda3” and “Linux” and execute the following at the download location: sh Miniconda3-latest-Linux-x86_64.sh -b Next, initialize the shell so we can run conda directly. ~/miniconda3/bin/conda init Now close and reopen your current shell. You should be able to create a new environment as follows: conda create --name d2l python=3.8 -y
11 https://conda.io/en/latest/miniconda.html
9

Downloading the D2L Notebooks
Next, we need to download the code of this book. You can click the “All Notebooks” tab on the top of any HTML page to download and unzip the code. Alternatively, if you have unzip (otherwise run sudo apt install unzip) available:
mkdir d2l-en && cd d2l-en curl https://d2l.ai/d2l-en.zip -o d2l-en.zip unzip d2l-en.zip && rm d2l-en.zip
Now we can activate the d2l environment:
conda activate d2l
Installing the Framework and the d2l Package
Before installing any deep learning framework, please first check whether or not you have proper GPUs on your machine (the GPUs that power the display on a standard laptop are not relevant for our purposes). If you are working on a GPU server, proceed to GPU Support (page 11) for instructions on how to install GPU-friendly versions of the relevant libraries. If your machine does not house any GPUs, there is no need to worry just yet. Your CPU provides more than enough horsepower to get you through the first few chapters. Just remember that you will want to access GPUs before running larger models. To install the the CPU version, execute the following command.
pip install mxnet==1.7.0.post1
Our next step is to install the d2l package that we developed in order to encapsulate frequently used functions and classes found throughout this book.
# -U: Upgrade all packages to the newest available version pip install -U d2l
Once you have completed these installation steps, we can the Jupyter notebook server by running:
jupyter notebook
At this point, you can open http://localhost:8888 (it may have already opened automatically) in your Web browser. Then we can run the code for each section of the book. Please always execute conda activate d2l to activate the runtime environment before running the code of the book or updating the deep learning framework or the d2l package. To exit the environment, run conda deactivate.

10

Contents

GPU Support
By default, MXNet is installed without GPU support to ensure that it will run on any computer (including most laptops). Part of this book requires or recommends running with GPU. If your computer has NVIDIA graphics cards and has installed CUDA12, then you should install a GPUenabled version. If you have installed the CPU-only version, you may need to remove it first by running:
pip uninstall mxnet
We now need to find out what version of CUDA you have installed. You can check this by running nvcc --version or cat /usr/local/cuda/version.txt. Assume that you have installed CUDA 10.1, then you can install with the following command:
# For Windows users pip install mxnet-cu101==1.7.0 -f https://dist.mxnet.io/python
# For Linux and macOS users pip install mxnet-cu101==1.7.0
You may change the last digits according to your CUDA version, e.g., cu100 for CUDA 10.0 and cu90 for CUDA 9.0.
Exercises
1. Download the code for the book and install the runtime environment. Discussions13

12 https://developer.nvidia.com/cuda-downloads 13 https://discuss.d2l.ai/t/23

Contents

11

12

Contents

Notation
Throughout this book, we adhere to the following notational conventions. Note that some of these symbols are placeholders, while others refer to specific objects. As a general rule of thumb, the indefinite article “a” indicates that the symbol is a placeholder and that similarly formatted symbols can denote other objects of the same type. For example, “x: a scalar” means that lowercased letters generally represent scalar values.
Numerical Objects
• x: a scalar • x: a vector • X: a matrix • X: a general tensor • I: an identity matrix—square, with 1 on all diagonal entries and 0 on all off-diagonals • xi, [x]i: the ith element of vector x • xij, xi,j,[X]ij, [X]i,j: the element of matrix X at row i and column j.
Set Theory
• X : a set • Z: the set of integers • Z+: the set of positive integers • R: the set of real numbers • Rn: the set of n-dimensional vectors of real numbers • Ra×b: The set of matrices of real numbers with a rows and b columns • |X |: cardinality (number of elements) of set X • A ∪ B: union of sets A and B • A ∩ B: intersection of sets A and B • A \ B: set subtraction of B from A (contains only those elements of A that do not belong to
B)
13

Functions and Operators
• f (·): a function • log(·): the natural logarithm (base e) • log2(·): logarithm with base 2 • exp(·): the exponential function • 1(·): the indicator function, evaluates to 1 if the boolean argument is true and 0 otherwise • 1X (z): the set-membership indicator function, evaluates to 1 if the element z belongs to the
set X and 0 otherwise • (·)⊤: transpose of a vector or a matrix • X−1: inverse of matrix X • ⊙: Hadamard (elementwise) product • [·, ·]: concatenation • ∥ · ∥p: Lp norm • ∥ · ∥: L2 norm • ⟨x, y⟩: dot product of vectors x and y
∑ • : summation over a collection of elements
∏ • : product over a collection of elements • d=ef: an equality asserted as a definition of the symbol on the left-hand side

Calculus
• ddxy : derivative of y with respect to x • ∂∂xy : partial derivative of y with respect to x • ∇xy: gradient of y with respect to x • ∫ab f (x) dx: definite integral of f from a to b with respect to x
∫ • f (x) dx: indefinite integral of f with respect to x

Probability and Information Theory
• X: a random variable • P : a probability distribution • X ∼ P : the random variable X has distribution P • P (X = x): the probability assigned to the event where random variable X takes value x • P (X | Y ): the conditional probability distribution of X given Y

14

Contents

• p(·): a probability density function (PDF) associated with distribution P • E[X]: expectation of a random variable X • X ⊥ Y : random variables X and Y are independent • X ⊥ Y | Z: random variables X and Y are conditionally independent given Z • σX : standard deviation of random variable X • Var(X): variance of random variable X, equal to σX2 • Cov(X, Y ): covariance of random variables X and Y • ρ(X, Y ): the Pearson correlation coefficient between X and Y , equals CoσvX(XσY,Y ) • H(X): entropy of random variable X • DKL(P ∥Q): the KL-divergence (or relative entropy) from distribution Q to distribution P Discussions14

14 https://discuss.d2l.ai/t/25

Contents

15

16

Contents

1 | Introduction
Until recently, nearly every computer program that we interact with daily was coded by software developers from first principles. Say that we wanted to write an application to manage an ecommerce platform. After huddling around a whiteboard for a few hours to ponder the problem, we would come up with the broad strokes of a working solution that might probably look something like this: (i) users interact with the application through an interface running in a web browser or mobile application; (ii) our application interacts with a commercial-grade database engine to keep track of each userʼs state and maintain records of historical transactions; and (iii) at the heart of our application, the business logic (you might say, the brains) of our application spells out in methodical detail the appropriate action that our program should take in every conceivable circumstance.
To build the brains of our application, we would have to step through every possible corner case that we anticipate encountering, devising appropriate rules. Each time a customer clicks to add an item to their shopping cart, we add an entry to the shopping cart database table, associating that userʼs ID with the requested productʼs ID. While few developers ever get it completely right the first time (it might take some test runs to work out the kinks), for the most part, we could write such a program from first principles and confidently launch it before ever seeing a real customer. Our ability to design automated systems from first principles that drive functioning products and systems, often in novel situations, is a remarkable cognitive feat. And when you are able to devise solutions that work 100% of the time, you should not be using machine learning.
Fortunately for the growing community of machine learning scientists, many tasks that we would like to automate do not bend so easily to human ingenuity. Imagine huddling around the whiteboard with the smartest minds you know, but this time you are tackling one of the following problems:
• Write a program that predicts tomorrowʼs weather given geographic information, satellite images, and a trailing window of past weather.
• Write a program that takes in a question, expressed in free-form text, and answers it correctly.
• Write a program that given an image can identify all the people it contains, drawing outlines around each.
• Write a program that presents users with products that they are likely to enjoy but unlikely, in the natural course of browsing, to encounter.
In each of these cases, even elite programmers are incapable of coding up solutions from scratch. The reasons for this can vary. Sometimes the program that we are looking for follows a pattern that changes over time, and we need our programs to adapt. In other cases, the relationship (say between pixels, and abstract categories) may be too complicated, requiring thousands or millions of computations that are beyond our conscious understanding even if our eyes manage the task
17

effortlessly. Machine learning is the study of powerful techniques that can learn from experience. As a machine learning algorithm accumulates more experience, typically in the form of observational data or interactions with an environment, its performance improves. Contrast this with our deterministic e-commerce platform, which performs according to the same business logic, no matter how much experience accrues, until the developers themselves learn and decide that it is time to update the software. In this book, we will teach you the fundamentals of machine learning, and focus in particular on deep learning, a powerful set of techniques driving innovations in areas as diverse as computer vision, natural language processing, healthcare, and genomics.
1.1 A Motivating Example
Before beginning writing, the authors of this book, like much of the work force, had to become caffeinated. We hopped in the car and started driving. Using an iPhone, Alex called out “Hey Siri”, awakening the phoneʼs voice recognition system. Then Mu commanded “directions to Blue Bottle coffee shop”. The phone quickly displayed the transcription of his command. It also recognized that we were asking for directions and launched the Maps application (app) to fulfill our request. Once launched, the Maps app identified a number of routes. Next to each route, the phone displayed a predicted transit time. While we fabricated this story for pedagogical convenience, it demonstrates that in the span of just a few seconds, our everyday interactions with a smart phone can engage several machine learning models.
Imagine just writing a program to respond to a wake word such as “Alexa”, “OK Google”, and “Hey Siri”. Try coding it up in a room by yourself with nothing but a computer and a code editor, as illustrated in Fig. 1.1.1. How would you write such a program from first principles? Think about it… the problem is hard. Every second, the microphone will collect roughly 44000 samples. Each sample is a measurement of the amplitude of the sound wave. What rule could map reliably from a snippet of raw audio to confident predictions {yes, no} on whether the snippet contains the wake word? If you are stuck, do not worry. We do not know how to write such a program from scratch either. That is why we use machine learning.

Fig. 1.1.1: Identify a wake word.
Here is the trick. Often, even when we do not know how to tell a computer explicitly how to map from inputs to outputs, we are nonetheless capable of performing the cognitive feat ourselves. In other words, even if you do not know how to program a computer to recognize the word “Alexa”, you yourself are able to recognize it. Armed with this ability, we can collect a huge dataset containing examples of audio and label those that do and that do not contain the wake word. In the machine learning approach, we do not attempt to design a system explicitly to recognize wake words. Instead, we define a flexible program whose behavior is determined by a number of parameters. Then we use the dataset to determine the best possible set of parameters, those that improve the performance of our program with respect to some measure of performance on the task of interest.
You can think of the parameters as knobs that we can turn, manipulating the behavior of the program. Fixing the parameters, we call the program a model. The set of all distinct programs

18

Chapter 1. Introduction

(input-output mappings) that we can produce just by manipulating the parameters is called a family of models. And the meta-program that uses our dataset to choose the parameters is called a learning algorithm.
Before we can go ahead and engage the learning algorithm, we have to define the problem precisely, pinning down the exact nature of the inputs and outputs, and choosing an appropriate model family. In this case, our model receives a snippet of audio as input, and the model generates a selection among {yes, no} as output. If all goes according to plan the modelʼs guesses will typically be correct as to whether the snippet contains the wake word.
If we choose the right family of models, there should exist one setting of the knobs such that the model fires “yes” every time it hears the word “Alexa”. Because the exact choice of the wake word is arbitrary, we will probably need a model family sufficiently rich that, via another setting of the knobs, it could fire “yes” only upon hearing the word “Apricot”. We expect that the same model family should be suitable for “Alexa” recognition and “Apricot” recognition because they seem, intuitively, to be similar tasks. However, we might need a different family of models entirely if we want to deal with fundamentally different inputs or outputs, say if we wanted to map from images to captions, or from English sentences to Chinese sentences.
As you might guess, if we just set all of the knobs randomly, it is unlikely that our model will recognize “Alexa”, “Apricot”, or any other English word. In machine learning, the learning is the process by which we discover the right setting of the knobs coercing the desired behavior from our model. In other words, we train our model with data. As shown in Fig. 1.1.2, the training process usually looks like the following:
1. Start off with a randomly initialized model that cannot do anything useful.
2. Grab some of your data (e.g., audio snippets and corresponding {yes, no} labels).
3. Tweak the knobs so the model sucks less with respect to those examples.
4. Repeat Step 2 and 3 until the model is awesome.

Fig. 1.1.2: A typical training process.
To summarize, rather than code up a wake word recognizer, we code up a program that can learn to recognize wake words, if we present it with a large labeled dataset. You can think of this act of determining a programʼs behavior by presenting it with a dataset as programming with data. That is to say, we can “program” a cat detector by providing our machine learning system with many examples of cats and dogs. This way the detector will eventually learn to emit a very large positive number if it is a cat, a very large negative number if it is a dog, and something closer to zero if it is not sure, and this barely scratches the surface of what machine learning can do. Deep learning, which we will explain in greater detail later, is just one among many popular methods for solving machine learning problems.

1.1. A Motivating Example

19

1.2 Key Components
In our wake word example, we described a dataset consisting of audio snippets and binary labels, and we gave a hand-wavy sense of how we might train a model to approximate a mapping from snippets to classifications. This sort of problem, where we try to predict a designated unknown label based on known inputs given a dataset consisting of examples for which the labels are known, is called supervised learning. This is just one among many kinds of machine learning problems. Later we will take a deep dive into different machine learning problems. First, we would like to shed more light on some core components that will follow us around, no matter what kind of machine learning problem we take on:
1. The data that we can learn from.
2. A model of how to transform the data.
3. An objective function that quantifies how well (or badly) the model is doing.
4. An algorithm to adjust the modelʼs parameters to optimize the objective function.
1.2.1 Data
It might go without saying that you cannot do data science without data. We could lose hundreds of pages pondering what precisely constitutes data, but for now, we will err on the practical side and focus on the key properties to be concerned with. Generally, we are concerned with a collection of examples. In order to work with data usefully, we typically need to come up with a suitable numerical representation. Each example (or data point, data instance, sample) typically consists of a set of attributes called features (or covariates), from which the model must make its predictions. In the supervised learning problems above, the thing to predict is a special attribute that is designated as the label (or target).
If we were working with image data, each individual photograph might constitute an example, each represented by an ordered list of numerical values corresponding to the brightness of each pixel. A 200 × 200 color photograph would consist of 200 × 200 × 3 = 120000 numerical values, corresponding to the brightness of the red, green, and blue channels for each spatial location. In another traditional task, we might try to predict whether or not a patient will survive, given a standard set of features such as age, vital signs, and diagnoses.
When every example is characterized by the same number of numerical values, we say that the data consist of fixed-length vectors and we describe the constant length of the vectors as the dimensionality of the data. As you might imagine, fixed-length can be a convenient property. If we wanted to train a model to recognize cancer in microscopy images, fixed-length inputs mean we have one less thing to worry about.
However, not all data can easily be represented as fixed-length vectors. While we might expect microscope images to come from standard equipment, we cannot expect images mined from the Internet to all show up with the same resolution or shape. For images, we might consider cropping them all to a standard size, but that strategy only gets us so far. We risk losing information in the cropped out portions. Moreover, text data resist fixed-length representations even more stubbornly. Consider the customer reviews left on e-commerce sites such as Amazon, IMDB, and TripAdvisor. Some are short: “it stinks!”. Others ramble for pages. One major advantage of deep learning over traditional methods is the comparative grace with which modern models can handle varying-length data.

20

Chapter 1. Introduction

Generally, the more data we have, the easier our job becomes. When we have more data, we can train more powerful models and rely less heavily on pre-conceived assumptions. The regime change from (comparatively) small to big data is a major contributor to the success of modern deep learning. To drive the point home, many of the most exciting models in deep learning do not work without large datasets. Some others work in the small data regime, but are no better than traditional approaches.
Finally, it is not enough to have lots of data and to process it cleverly. We need the right data. If the data are full of mistakes, or if the chosen features are not predictive of the target quantity of interest, learning is going to fail. The situation is captured well by the cliché: garbage in, garbage out. Moreover, poor predictive performance is not the only potential consequence. In sensitive applications of machine learning, like predictive policing, resume screening, and risk models used for lending, we must be especially alert to the consequences of garbage data. One common failure mode occurs in datasets where some groups of people are unrepresented in the training data. Imagine applying a skin cancer recognition system in the wild that had never seen black skin before. Failure can also occur when the data do not merely under-represent some groups but reflect societal prejudices. For example, if past hiring decisions are used to train a predictive model that will be used to screen resumes, then machine learning models could inadvertently capture and automate historical injustices. Note that this can all happen without the data scientist actively conspiring, or even being aware.

1.2.2 Models
Most machine learning involves transforming the data in some sense. We might want to build a system that ingests photos and predicts smiley-ness. Alternatively, we might want to ingest a set of sensor readings and predict how normal vs. anomalous the readings are. By model, we denote the computational machinery for ingesting data of one type, and spitting out predictions of a possibly different type. In particular, we are interested in statistical models that can be estimated from data. While simple models are perfectly capable of addressing appropriately simple problems, the problems that we focus on in this book stretch the limits of classical methods. Deep learning is differentiated from classical approaches principally by the set of powerful models that it focuses on. These models consist of many successive transformations of the data that are chained together top to bottom, thus the name deep learning. On our way to discussing deep models, we will also discuss some more traditional methods.

1.2.3 Objective Functions
Earlier, we introduced machine learning as learning from experience. By learning here, we mean improving at some task over time. But who is to say what constitutes an improvement? You might imagine that we could propose to update our model, and some people might disagree on whether the proposed update constituted an improvement or a decline.
In order to develop a formal mathematical system of learning machines, we need to have formal measures of how good (or bad) our models are. In machine learning, and optimization more generally, we call these objective functions. By convention, we usually define objective functions so that lower is better. This is merely a convention. You can take any function for which higher is better, and turn it into a new function that is qualitatively identical but for which lower is better by flipping the sign. Because lower is better, these functions are sometimes called loss functions.
When trying to predict numerical values, the most common loss function is squared error, i.e., the square of the difference between the prediction and the ground-truth. For classification, the most

1.2. Key Components

21

common objective is to minimize error rate, i.e., the fraction of examples on which our predictions disagree with the ground truth. Some objectives (e.g., squared error) are easy to optimize. Others (e.g., error rate) are difficult to optimize directly, owing to non-differentiability or other complications. In these cases, it is common to optimize a surrogate objective.
Typically, the loss function is defined with respect to the modelʼs parameters and depends upon the dataset. We learn the best values of our modelʼs parameters by minimizing the loss incurred on a set consisting of some number of examples collected for training. However, doing well on the training data does not guarantee that we will do well on unseen data. So we will typically want to split the available data into two partitions: the training dataset (or training set, for fitting model parameters) and the test dataset (or test set, which is held out for evaluation), reporting how the model performs on both of them. You could think of training performance as being like a studentʼs scores on practice exams used to prepare for some real final exam. Even if the results are encouraging, that does not guarantee success on the final exam. In other words, the test performance can deviate significantly from the training performance. When a model performs well on the training set but fails to generalize to unseen data, we say that it is overfitting. In real-life terms, this is like flunking the real exam despite doing well on practice exams.
1.2.4 Optimization Algorithms
Once we have got some data source and representation, a model, and a well-defined objective function, we need an algorithm capable of searching for the best possible parameters for minimizing the loss function. Popular optimization algorithms for deep learning are based on an approach called gradient descent. In short, at each step, this method checks to see, for each parameter, which way the training set loss would move if you perturbed that parameter just a small amount. It then updates the parameter in the direction that may reduce the loss.
1.3 Kinds of Machine Learning Problems
The wake word problem in our motivating example is just one among many problems that machine learning can tackle. To motivate the reader further and provide us with some common language when we talk about more problems throughout the book, in the following we list a sampling of machine learning problems. We will constantly refer to our aforementioned concepts such as data, models, and training techniques.
1.3.1 Supervised Learning
Supervised learning addresses the task of predicting labels given input features. Each feature– label pair is called an example. Sometimes, when the context is clear, we may use the term examples to refer to a collection of inputs, even when the corresponding labels are unknown. Our goal is to produce a model that maps any input to a label prediction.
To ground this description in a concrete example, if we were working in healthcare, then we might want to predict whether or not a patient would have a heart attack. This observation, “heart attack” or “no heart attack”, would be our label. The input features might be vital signs such as heart rate, diastolic blood pressure, and systolic blood pressure.
The supervision comes into play because for choosing the parameters, we (the supervisors) provide the model with a dataset consisting of labeled examples, where each example is matched with

22

Chapter 1. Introduction

the ground-truth label. In probabilistic terms, we typically are interested in estimating the conditional probability of a label given input features. While it is just one among several paradigms within machine learning, supervised learning accounts for the majority of successful applications of machine learning in industry. Partly, that is because many important tasks can be described crisply as estimating the probability of something unknown given a particular set of available data:
• Predict cancer vs. not cancer, given a computer tomography image.
• Predict the correct translation in French, given a sentence in English.
• Predict the price of a stock next month based on this monthʼs financial reporting data.
Even with the simple description “predicting labels given input features” supervised learning can take a great many forms and require a great many modeling decisions, depending on (among other considerations) the type, size, and the number of inputs and outputs. For example, we use different models to process sequences of arbitrary lengths and for processing fixed-length vector representations. We will visit many of these problems in depth throughout this book.
Informally, the learning process looks something like the following. First, grab a big collection of examples for which the features are known and select from them a random subset, acquiring the ground-truth labels for each. Sometimes these labels might be available data that have already been collected (e.g., did a patient die within the following year?) and other times we might need to employ human annotators to label the data, (e.g., assigning images to categories). Together, these inputs and corresponding labels comprise the training set. We feed the training dataset into a supervised learning algorithm, a function that takes as input a dataset and outputs another function: the learned model. Finally, we can feed previously unseen inputs to the learned model, using its outputs as predictions of the corresponding label. The full process is drawn in Fig. 1.3.1.

Fig. 1.3.1: Supervised learning.

Regression
Perhaps the simplest supervised learning task to wrap your head around is regression. Consider, for example, a set of data harvested from a database of home sales. We might construct a table, where each row corresponds to a different house, and each column corresponds to some relevant attribute, such as the square footage of a house, the number of bedrooms, the number of bathrooms, and the number of minutes (walking) to the center of town. In this dataset, each example would be a specific house, and the corresponding feature vector would be one row in the table. If you live in New York or San Francisco, and you are not the CEO of Amazon, Google, Microsoft, or Facebook, the (sq. footage, no. of bedrooms, no. of bathrooms, walking distance) feature vector for your home might look something like: [600, 1, 1, 60]. However, if you live in Pittsburgh, it might look more like [3000, 4, 3, 10]. Feature vectors like this are essential for most classic machine learning algorithms.
What makes a problem a regression is actually the output. Say that you are in the market for a

1.3. Kinds of Machine Learning Problems

23

new home. You might want to estimate the fair market value of a house, given some features like above. The label, the price of sale, is a numerical value. When labels take on arbitrary numerical values, we call this a regression problem. Our goal is to produce a model whose predictions closely approximate the actual label values.
Lots of practical problems are well-described regression problems. Predicting the rating that a user will assign to a movie can be thought of as a regression problem and if you designed a great algorithm to accomplish this feat in 2009, you might have won the 1-million-dollar Netflix prize15. Predicting the length of stay for patients in the hospital is also a regression problem. A good rule of thumb is that any how much? or how many? problem should suggest regression, such as:
• How many hours will this surgery take?
• How much rainfall will this town have in the next six hours?
Even if you have never worked with machine learning before, you have probably worked through a regression problem informally. Imagine, for example, that you had your drains repaired and that your contractor spent 3 hours removing gunk from your sewage pipes. Then he sent you a bill of 350 dollars. Now imagine that your friend hired the same contractor for 2 hours and that he received a bill of 250 dollars. If someone then asked you how much to expect on their upcoming gunk-removal invoice you might make some reasonable assumptions, such as more hours worked costs more dollars. You might also assume that there is some base charge and that the contractor then charges per hour. If these assumptions held true, then given these two data examples, you could already identify the contractorʼs pricing structure: 100 dollars per hour plus 50 dollars to show up at your house. If you followed that much then you already understand the high-level idea behind linear regression.
In this case, we could produce the parameters that exactly matched the contractorʼs prices. Sometimes this is not possible, e.g., if some of the variance owes to a few factors besides your two features. In these cases, we will try to learn models that minimize the distance between our predictions and the observed values. In most of our chapters, we will focus on minimizing the squared error loss function. As we will see later, this loss corresponds to the assumption that our data were corrupted by Gaussian noise.

Classification
While regression models are great for addressing how many? questions, lots of problems do not bend comfortably to this template. For example, a bank wants to add check scanning to its mobile app. This would involve the customer snapping a photo of a check with their smart phoneʼs camera and the app would need to be able to automatically understand text seen in the image. Specifically, it would also need to understand handwritten text to be even more robust, such as mapping a handwritten character to one of the known characters. This kind of which one? problem is called classification. It is treated with a different set of algorithms than those used for regression although many techniques will carry over.
In classification, we want our model to look at features, e.g., the pixel values in an image, and then predict which category (formally called class), among some discrete set of options, an example belongs. For handwritten digits, we might have ten classes, corresponding to the digits 0 through 9. The simplest form of classification is when there are only two classes, a problem which we call binary classification. For example, our dataset could consist of images of animals and our labels might be the classes {cat, dog}. While in regression, we sought a regressor to output a numerical value, in classification, we seek a classifier, whose output is the predicted class assignment.
15 https://en.wikipedia.org/wiki/Netflix_Prize

24

Chapter 1. Introduction

For reasons that we will get into as the book gets more technical, it can be hard to optimize a model that can only output a hard categorical assignment, e.g., either “cat” or “dog”. In these cases, it is usually much easier to instead express our model in the language of probabilities. Given features of an example, our model assigns a probability to each possible class. Returning to our animal classification example where the classes are {cat, dog}, a classifier might see an image and output the probability that the image is a cat as 0.9. We can interpret this number by saying that the classifier is 90% sure that the image depicts a cat. The magnitude of the probability for the predicted class conveys one notion of uncertainty. It is not the only notion of uncertainty and we will discuss others in more advanced chapters.
When we have more than two possible classes, we call the problem multiclass classification. Common examples include hand-written character recognition {0, 1, 2, ...9, a, b, c, ...}. While we attacked regression problems by trying to minimize the squared error loss function, the common loss function for classification problems is called cross-entropy, whose name can be demystified via an introduction to information theory in subsequent chapters.
Note that the most likely class is not necessarily the one that you are going to use for your decision. Assume that you find a beautiful mushroom in your backyard as shown in Fig. 1.3.2.

Fig. 1.3.2: Death cap—do not eat!

Now, assume that you built a classifier and trained it to predict if a mushroom is poisonous based on a photograph. Say our poison-detection classifier outputs that the probability that Fig. 1.3.2 contains a death cap is 0.2. In other words, the classifier is 80% sure that our mushroom is not a death cap. Still, you would have to be a fool to eat it. That is because the certain benefit of a delicious dinner is not worth a 20% risk of dying from it. In other words, the effect of the uncertain risk outweighs the benefit by far. Thus, we need to compute the expected risk that we incur as the loss function, i.e., we need to multiply the probability of the outcome with the benefit (or harm) associated with it. In this case, the loss incurred by eating the mushroom can be 0.2×∞+0.8×0 = ∞, whereas the loss of discarding it is 0.2 × 0 + 0.8 × 1 = 0.8. Our caution was justified: as any mycologist would tell us, the mushroom in Fig. 1.3.2 actually is a death cap.
Classification can get much more complicated than just binary, multiclass, or even multi-label classification. For instance, there are some variants of classification for addressing hierarchies. Hierarchies assume that there exist some relationships among the many classes. So not all errors are equal—if we must err, we would prefer to misclassify to a related class rather than to a distant class. Usually, this is referred to as hierarchical classification. One early example is due to Linnaeus16, who organized the animals in a hierarchy.
16 https://en.wikipedia.org/wiki/Carl_Linnaeus

1.3. Kinds of Machine Learning Problems

25

In the case of animal classification, it might not be so bad to mistake a poodle (a dog breed) for a schnauzer (another dog breed), but our model would pay a huge penalty if it confused a poodle for a dinosaur. Which hierarchy is relevant might depend on how you plan to use the model. For example, rattle snakes and garter snakes might be close on the phylogenetic tree, but mistaking a rattler for a garter could be deadly.
Tagging
Some classification problems fit neatly into the binary or multiclass classification setups. For example, we could train a normal binary classifier to distinguish cats from dogs. Given the current state of computer vision, we can do this easily, with off-the-shelf tools. Nonetheless, no matter how accurate our model gets, we might find ourselves in trouble when the classifier encounters an image of the Town Musicians of Bremen, a popular German fairy tale featuring four animals in Fig. 1.3.3.

Fig. 1.3.3: A donkey, a dog, a cat, and a rooster.

As you can see, there is a cat in Fig. 1.3.3, and a rooster, a dog, and a donkey, with some trees in the background. Depending on what we want to do with our model ultimately, treating this as a binary classification problem might not make a lot of sense. Instead, we might want to give the model the option of saying the image depicts a cat, a dog, a donkey, and a rooster.
The problem of learning to predict classes that are not mutually exclusive is called multi-label classification. Auto-tagging problems are typically best described as multi-label classification problems. Think of the tags people might apply to posts on a technical blog, e.g., “machine learning”, “technology”, “gadgets”, “programming languages”, “Linux”, “cloud computing”, “AWS”. A typical article might have 5–10 tags applied because these concepts are correlated. Posts about “cloud

26

Chapter 1. Introduction

computing” are likely to mention “AWS” and posts about “machine learning” could also deal with “programming languages”.
We also have to deal with this kind of problem when dealing with the biomedical literature, where correctly tagging articles is important because it allows researchers to do exhaustive reviews of the literature. At the National Library of Medicine, a number of professional annotators go over each article that gets indexed in PubMed to associate it with the relevant terms from MeSH, a collection of roughly 28000 tags. This is a time-consuming process and the annotators typically have a one-year lag between archiving and tagging. Machine learning can be used here to provide provisional tags until each article can have a proper manual review. Indeed, for several years, the BioASQ organization has hosted competitions17 to do precisely this.
Search
Sometimes we do not just want to assign each example to a bucket or to a real value. In the field of information retrieval, we want to impose a ranking on a set of items. Take web search for an example. The goal is less to determine whether a particular page is relevant for a query, but rather, which one of the plethora of search results is most relevant for a particular user. We really care about the ordering of the relevant search results and our learning algorithm needs to produce ordered subsets of elements from a larger set. In other words, if we are asked to produce the first 5 letters from the alphabet, there is a difference between returning “A B C D E” and “C A B E D”. Even if the result set is the same, the ordering within the set matters.
One possible solution to this problem is to first assign to every element in the set a corresponding relevance score and then to retrieve the top-rated elements. PageRank18, the original secret sauce behind the Google search engine was an early example of such a scoring system but it was peculiar in that it did not depend on the actual query. Here they relied on a simple relevance filter to identify the set of relevant items and then on PageRank to order those results that contained the query term. Nowadays, search engines use machine learning and behavioral models to obtain query-dependent relevance scores. There are entire academic conferences devoted to this subject.
Recommender Systems
Recommender systems are another problem setting that is related to search and ranking. The problems are similar insofar as the goal is to display a set of relevant items to the user. The main difference is the emphasis on personalization to specific users in the context of recommender systems. For instance, for movie recommendations, the results page for a science fiction fan and the results page for a connoisseur of Peter Sellers comedies might differ significantly. Similar problems pop up in other recommendation settings, e.g., for retail products, music, and news recommendation.
In some cases, customers provide explicit feedback communicating how much they liked a particular product (e.g., the product ratings and reviews on Amazon, IMDb, and Goodreads). In some other cases, they provide implicit feedback, e.g., by skipping titles on a playlist, which might indicate dissatisfaction but might just indicate that the song was inappropriate in context. In the simplest formulations, these systems are trained to estimate some score, such as an estimated rating or the probability of purchase, given a user and an item.
17 http://bioasq.org/ 18 https://en.wikipedia.org/wiki/PageRank

1.3. Kinds of Machine Learning Problems

27

Given such a model, for any given user, we could retrieve the set of objects with the largest scores, which could then be recommended to the user. Production systems are considerably more advanced and take detailed user activity and item characteristics into account when computing such scores. Fig. 1.3.4 is an example of deep learning books recommended by Amazon based on personalization algorithms tuned to capture oneʼs preferences.

Fig. 1.3.4: Deep learning books recommended by Amazon.
Despite their tremendous economic value, recommendation systems naively built on top of predictive models suffer some serious conceptual flaws. To start, we only observe censored feedback: users preferentially rate movies that they feel strongly about. For example, on a five-point scale, you might notice that items receive many five and one star ratings but that there are conspicuously few three-star ratings. Moreover, current purchase habits are often a result of the recommendation algorithm currently in place, but learning algorithms do not always take this detail into account. Thus it is possible for feedback loops to form where a recommender system preferentially pushes an item that is then taken to be better (due to greater purchases) and in turn is recommended even more frequently. Many of these problems about how to deal with censoring, incentives, and feedback loops, are important open research questions.

28

Chapter 1. Introduction

Sequence Learning

So far, we have looked at problems where we have some fixed number of inputs and produce a fixed number of outputs. For example, we considered predicting house prices from a fixed set of features: square footage, number of bedrooms, number of bathrooms, walking time to downtown. We also discussed mapping from an image (of fixed dimension) to the predicted probabilities that it belongs to each of a fixed number of classes, or taking a user ID and a product ID, and predicting a star rating. In these cases, once we feed our fixed-length input into the model to generate an output, the model immediately forgets what it just saw.
This might be fine if our inputs truly all have the same dimensions and if successive inputs truly have nothing to do with each other. But how would we deal with video snippets? In this case, each snippet might consist of a different number of frames. And our guess of what is going on in each frame might be much stronger if we take into account the previous or succeeding frames. Same goes for language. One popular deep learning problem is machine translation: the task of ingesting sentences in some source language and predicting their translation in another language.
These problems also occur in medicine. We might want a model to monitor patients in the intensive care unit and to fire off alerts if their risk of death in the next 24 hours exceeds some threshold. We definitely would not want this model to throw away everything it knows about the patient history each hour and just make its predictions based on the most recent measurements.
These problems are among the most exciting applications of machine learning and they are instances of sequence learning. They require a model to either ingest sequences of inputs or to emit sequences of outputs (or both). Specifically, sequence to sequence learning considers problems where input and output are both variable-length sequences, such as machine translation and transcribing text from the spoken speech. While it is impossible to consider all types of sequence transformations, the following special cases are worth mentioning.
Tagging and Parsing. This involves annotating a text sequence with attributes. In other words, the number of inputs and outputs is essentially the same. For instance, we might want to know where the verbs and subjects are. Alternatively, we might want to know which words are the named entities. In general, the goal is to decompose and annotate text based on structural and grammatical assumptions to get some annotation. This sounds more complex than it actually is. Below is a very simple example of annotating a sentence with tags indicating which words refer to named entities (tagged as “Ent”).

Tom has dinner in Washington with Sally

Ent - - - Ent

- Ent

Automatic Speech Recognition. With speech recognition, the input sequence is an audio recording of a speaker (shown in Fig. 1.3.5), and the output is the textual transcript of what the speaker said. The challenge is that there are many more audio frames (sound is typically sampled at 8kHz or 16kHz) than text, i.e., there is no 1:1 correspondence between audio and text, since thousands of samples may correspond to a single spoken word. These are sequence to sequence learning problems where the output is much shorter than the input.

1.3. Kinds of Machine Learning Problems

29

Fig. 1.3.5: -D-e-e-p- L-ea-r-ni-ng- in an audio recording.

Text to Speech. This is the inverse of automatic speech recognition. In other words, the input is text and the output is an audio file. In this case, the output is much longer than the input. While it is easy for humans to recognize a bad audio file, this is not quite so trivial for computers.
Machine Translation. Unlike the case of speech recognition, where corresponding inputs and outputs occur in the same order (after alignment), in machine translation, order inversion can be vital. In other words, while we are still converting one sequence into another, neither the number of inputs and outputs nor the order of corresponding data examples are assumed to be the same. Consider the following illustrative example of the peculiar tendency of Germans to place the verbs at the end of sentences.

German: English: Wrong alignment:

Haben Sie sich schon dieses grossartige Lehrwerk angeschaut? Did you already check out this excellent tutorial? Did you yourself already this excellent tutorial looked-at?

Many related problems pop up in other learning tasks. For instance, determining the order in which a user reads a webpage is a two-dimensional layout analysis problem. Dialogue problems exhibit all kinds of additional complications, where determining what to say next requires taking into account real-world knowledge and the prior state of the conversation across long temporal distances. These are active areas of research.

1.3.2 Unsupervised and Self-Supervised Learning

All the examples so far were related to supervised learning, i.e., situations where we feed the model a giant dataset containing both the features and corresponding label values. You could think of the supervised learner as having an extremely specialized job and an extremely banal boss. The boss stands over your shoulder and tells you exactly what to do in every situation until you learn to map from situations to actions. Working for such a boss sounds pretty lame. On the other hand, it is easy to please this boss. You just recognize the pattern as quickly as possible and imitate their actions.
In a completely opposite way, it could be frustrating to work for a boss who has no idea what they want you to do. However, if you plan to be a data scientist, you had better get used to it. The boss might just hand you a giant dump of data and tell you to do some data science with it! This sounds vague because it is. We call this class of problems unsupervised learning, and the type and number of questions we could ask is limited only by our creativity. We will address unsupervised learning techniques in later chapters. To whet your appetite for now, we describe a few of the following questions you might ask.
• Can we find a small number of prototypes that accurately summarize the data? Given a set of photos, can we group them into landscape photos, pictures of dogs, babies, cats, and

30

Chapter 1. Introduction

mountain peaks? Likewise, given a collection of usersʼ browsing activities, can we group them into users with similar behavior? This problem is typically known as clustering.
• Can we find a small number of parameters that accurately capture the relevant properties of the data? The trajectories of a ball are quite well described by velocity, diameter, and mass of the ball. Tailors have developed a small number of parameters that describe human body shape fairly accurately for the purpose of fitting clothes. These problems are referred to as subspace estimation. If the dependence is linear, it is called principal component analysis.
• Is there a representation of (arbitrarily structured) objects in Euclidean space such that symbolic properties can be well matched? This can be used to describe entities and their relations, such as “Rome” − “Italy” + “France” = “Paris”.
• Is there a description of the root causes of much of the data that we observe? For instance, if we have demographic data about house prices, pollution, crime, location, education, and salaries, can we discover how they are related simply based on empirical data? The fields concerned with causality and probabilistic graphical models address this problem.
• Another important and exciting recent development in unsupervised learning is the advent of generative adversarial networks. These give us a procedural way to synthesize data, even complicated structured data like images and audio. The underlying statistical mechanisms are tests to check whether real and fake data are the same.
As a form of unsupervised learning, self-supervised learning leverages unlabeled data to provide supervision in training, such as by predicting some withheld part of the data using other parts. For text, we can train models to “fill in the blanks” by predicting randomly masked words using their surrounding words (contexts) in big corpora without any labeling effort (Devlin et al., 2018)! For images, we may train models to tell the relative position between two cropped regions of the same image (Doersch et al., 2015). In these two examples of self-supervised learning, training models to predict possible words and relative positions are both classification tasks (from supervised learning).
1.3.3 Interacting with an Environment
So far, we have not discussed where data actually come from, or what actually happens when a machine learning model generates an output. That is because supervised learning and unsupervised learning do not address these issues in a very sophisticated way. In either case, we grab a big pile of data upfront, then set our pattern recognition machines in motion without ever interacting with the environment again. Because all of the learning takes place after the algorithm is disconnected from the environment, this is sometimes called offline learning. For supervised learning, the process by considering data collection from an environment looks like Fig. 1.3.6.

1.3. Kinds of Machine Learning Problems

31

Fig. 1.3.6: Collecting data for supervised learning from an environment.
This simplicity of offline learning has its charms. The upside is that we can worry about pattern recognition in isolation, without any distraction from these other problems. But the downside is that the problem formulation is quite limiting. If you are more ambitious, or if you grew up reading Asimovʼs Robot series, then you might imagine artificially intelligent bots capable not only of making predictions, but also of taking actions in the world. We want to think about intelligent agents, not just predictive models. This means that we need to think about choosing actions, not just making predictions. Moreover, unlike predictions, actions actually impact the environment. If we want to train an intelligent agent, we must account for the way its actions might impact the future observations of the agent.
Considering the interaction with an environment opens a whole set of new modeling questions. The following are just a few examples.
• Does the environment remember what we did previously?
• Does the environment want to help us, e.g., a user reading text into a speech recognizer?
• Does the environment want to beat us, i.e., an adversarial setting like spam filtering (against spammers) or playing a game (vs. an opponent)?
• Does the environment not care?
• Does the environment have shifting dynamics? For example, does future data always resemble the past or do the patterns change over time, either naturally or in response to our automated tools?
This last question raises the problem of distribution shift, when training and test data are different. It is a problem that most of us have experienced when taking exams written by a lecturer, while the homework was composed by his teaching assistants. Next, we will briefly describe reinforcement learning, a setting that explicitly considers interactions with an environment.

32

Chapter 1. Introduction

1.3.4 Reinforcement Learning
If you are interested in using machine learning to develop an agent that interacts with an environment and takes actions, then you are probably going to wind up focusing on reinforcement learning. This might include applications to robotics, to dialogue systems, and even to developing artificial intelligence (AI) for video games. Deep reinforcement learning, which applies deep learning to reinforcement learning problems, has surged in popularity. The breakthrough deep Q-network that beat humans at Atari games using only the visual input, and the AlphaGo program that dethroned the world champion at the board game Go are two prominent examples.
Reinforcement learning gives a very general statement of a problem, in which an agent interacts with an environment over a series of time steps. At each time step, the agent receives some observation from the environment and must choose an action that is subsequently transmitted back to the environment via some mechanism (sometimes called an actuator). Finally, the agent receives a reward from the environment. This process is illustrated in Fig. 1.3.7. The agent then receives a subsequent observation, and chooses a subsequent action, and so on. The behavior of an reinforcement learning agent is governed by a policy. In short, a policy is just a function that maps from observations of the environment to actions. The goal of reinforcement learning is to produce a good policy.

Fig. 1.3.7: The interaction between reinforcement learning and an environment.
It is hard to overstate the generality of the reinforcement learning framework. For example, we can cast any supervised learning problem as a reinforcement learning problem. Say we had a classification problem. We could create a reinforcement learning agent with one action corresponding to each class. We could then create an environment which gave a reward that was exactly equal to the loss function from the original supervised learning problem.
That being said, reinforcement learning can also address many problems that supervised learning cannot. For example, in supervised learning we always expect that the training input comes associated with the correct label. But in reinforcement learning, we do not assume that for each observation the environment tells us the optimal action. In general, we just get some reward. Moreover, the environment may not even tell us which actions led to the reward.
Consider for example the game of chess. The only real reward signal comes at the end of the game when we either win, which we might assign a reward of 1, or when we lose, which we could assign a reward of -1. So reinforcement learners must deal with the credit assignment problem: determining which actions to credit or blame for an outcome. The same goes for an employee who gets a promotion on October 11. That promotion likely reflects a large number of well-chosen actions over the previous year. Getting more promotions in the future requires figuring out what actions along the way led to the promotion.

1.3. Kinds of Machine Learning Problems

33

Reinforcement learners may also have to deal with the problem of partial observability. That is, the current observation might not tell you everything about your current state. Say a cleaning robot found itself trapped in one of many identical closets in a house. Inferring the precise location (and thus state) of the robot might require considering its previous observations before entering the closet.
Finally, at any given point, reinforcement learners might know of one good policy, but there might be many other better policies that the agent has never tried. The reinforcement learner must constantly choose whether to exploit the best currently-known strategy as a policy, or to explore the space of strategies, potentially giving up some short-run reward in exchange for knowledge.
The general reinforcement learning problem is a very general setting. Actions affect subsequent observations. Rewards are only observed corresponding to the chosen actions. The environment may be either fully or partially observed. Accounting for all this complexity at once may ask too much of researchers. Moreover, not every practical problem exhibits all this complexity. As a result, researchers have studied a number of special cases of reinforcement learning problems.
When the environment is fully observed, we call the reinforcement learning problem a Markov decision process. When the state does not depend on the previous actions, we call the problem a contextual bandit problem. When there is no state, just a set of available actions with initially unknown rewards, this problem is the classic multi-armed bandit problem.
1.4 Roots
We have just reviewed a small subset of problems that machine learning can address. For a diverse set of machine learning problems, deep learning provides powerful tools for solving them. Although many deep learning methods are recent inventions, the core idea of programming with data and neural networks (names of many deep learning models) has been studied for centuries. In fact, humans have held the desire to analyze data and to predict future outcomes for long and much of natural science has its roots in this. For instance, the Bernoulli distribution is named after Jacob Bernoulli (1655–1705)19, and the Gaussian distribution was discovered by Carl Friedrich Gauss (1777–1855)20. He invented, for instance, the least mean squares algorithm, which is still used today for countless problems from insurance calculations to medical diagnostics. These tools gave rise to an experimental approach in the natural sciences—for instance, Ohmʼs law relating current and voltage in a resistor is perfectly described by a linear model.
Even in the middle ages, mathematicians had a keen intuition of estimates. For instance, the geometry book of Jacob Köbel (1460–1533)21 illustrates averaging the length of 16 adult menʼs feet to obtain the average foot length.
19 https://en.wikipedia.org/wiki/Jacob_Bernoulli 20 https://en.wikipedia.org/wiki/Carl_Friedrich_Gauss 21 https://www.maa.org/press/periodicals/convergence/mathematical-treasures-jacob-kobels-geometry

34

Chapter 1. Introduction

Fig. 1.4.1: Estimating the length of a foot.

Fig. 1.4.1 illustrates how this estimator works. The 16 adult men were asked to line up in a row, when leaving the church. Their aggregate length was then divided by 16 to obtain an estimate for what now amounts to 1 foot. This “algorithm” was later improved to deal with misshapen feet— the 2 men with the shortest and longest feet respectively were sent away, averaging only over the remainder. This is one of the earliest examples of the trimmed mean estimate.
Statistics really took off with the collection and availability of data. One of its titans, Ronald Fisher (1890–1962)22, contributed significantly to its theory and also its applications in genetics. Many of his algorithms (such as linear discriminant analysis) and formula (such as the Fisher information matrix) are still in frequent use today. In fact, even the Iris dataset that Fisher released in 1936 is still used sometimes to illustrate machine learning algorithms. He was also a proponent of eugenics, which should remind us that the morally dubious use of data science has as long and enduring a history as its productive use in industry and the natural sciences.
A second influence for machine learning came from information theory by Claude Shannon (1916–2001)23 and the theory of computation via Alan Turing (1912–1954)24. Turing posed the question “can machines think?” in his famous paper Computing Machinery and Intelligence (Turing, 1950). In what he described as the Turing test, a machine can be considered intelligent if it is difficult for a human evaluator to distinguish between the replies from a machine and a human based on textual interactions.
Another influence can be found in neuroscience and psychology. After all, humans clearly exhibit intelligent behavior. It is thus only reasonable to ask whether one could explain and possibly reverse engineer this capacity. One of the oldest algorithms inspired in this fashion was formulated
22 https://en.wikipedia.org/wiki/Ronald_Fisher 23 https://en.wikipedia.org/wiki/Claude_Shannon 24 https://en.wikipedia.org/wiki/Alan_Turing

1.4. Roots

35

by Donald Hebb (1904–1985)25. In his groundbreaking book The Organization of Behavior (Hebb & Hebb, 1949), he posited that neurons learn by positive reinforcement. This became known as the Hebbian learning rule. It is the prototype of Rosenblattʼs perceptron learning algorithm and it laid the foundations of many stochastic gradient descent algorithms that underpin deep learning today: reinforce desirable behavior and diminish undesirable behavior to obtain good settings of the parameters in a neural network.
Biological inspiration is what gave neural networks their name. For over a century (dating back to the models of Alexander Bain, 1873 and James Sherrington, 1890), researchers have tried to assemble computational circuits that resemble networks of interacting neurons. Over time, the interpretation of biology has become less literal but the name stuck. At its heart, lie a few key principles that can be found in most networks today:
• The alternation of linear and nonlinear processing units, often referred to as layers.
• The use of the chain rule (also known as backpropagation) for adjusting parameters in the entire network at once.
After initial rapid progress, research in neural networks languished from around 1995 until 2005. This was mainly due to two reasons. First, training a network is computationally very expensive. While random-access memory was plentiful at the end of the past century, computational power was scarce. Second, datasets were relatively small. In fact, Fisherʼs Iris dataset from 1932 was a popular tool for testing the efficacy of algorithms. The MNIST dataset with its 60000 handwritten digits was considered huge.
Given the scarcity of data and computation, strong statistical tools such as kernel methods, decision trees and graphical models proved empirically superior. Unlike neural networks, they did not require weeks to train and provided predictable results with strong theoretical guarantees.

1.5 The Road to Deep Learning

Much of this changed with the ready availability of large amounts of data, due to the World Wide Web, the advent of companies serving hundreds of millions of users online, a dissemination of cheap, high-quality sensors, cheap data storage (Kryderʼs law), and cheap computation (Mooreʼs law), in particular in the form of GPUs, originally engineered for computer gaming. Suddenly algorithms and models that seemed computationally infeasible became relevant (and vice versa). This is best illustrated in Table 1.5.1.

Decade 1970 1980 1990 2000 2010 2020

Table 1.5.1: Dataset vs. computer memory and computational power

Dataset

Memory Floating point calculations per second

100 (Iris)

1 KB

100 KF (Intel 8080)

1 K (House prices in Boston)

100 KB 1 MF (Intel 80186)

10 K (optical character recognition) 10 MB 10 MF (Intel 80486)

10 M (web pages)

100 MB 1 GF (Intel Core)

10 G (advertising)

1 GB

1 TF (Nvidia C2050)

1 T (social network)

100 GB 1 PF (Nvidia DGX-2)

It is evident that random-access memory has not kept pace with the growth in data. At the same time, the increase in computational power has outpaced that of the data available. This means that
25 https://en.wikipedia.org/wiki/Donald_O._Hebb

36

Chapter 1. Introduction

statistical models need to become more memory efficient (this is typically achieved by adding nonlinearities) while simultaneously being able to spend more time on optimizing these parameters, due to an increased computational budget. Consequently, the sweet spot in machine learning and statistics moved from (generalized) linear models and kernel methods to deep neural networks. This is also one of the reasons why many of the mainstays of deep learning, such as multilayer perceptrons (McCulloch & Pitts, 1943), convolutional neural networks (LeCun et al., 1998), long short-term memory (Hochreiter & Schmidhuber, 1997), and Q-Learning (Watkins & Dayan, 1992), were essentially “rediscovered” in the past decade, after laying comparatively dormant for considerable time.
The recent progress in statistical models, applications, and algorithms has sometimes been likened to the Cambrian explosion: a moment of rapid progress in the evolution of species. Indeed, the state of the art is not just a mere consequence of available resources, applied to decades old algorithms. Note that the list below barely scratches the surface of the ideas that have helped researchers achieve tremendous progress over the past decade.
• Novel methods for capacity control, such as dropout (Srivastava et al., 2014), have helped to mitigate the danger of overfitting. This was achieved by applying noise injection (Bishop, 1995) throughout the neural network, replacing weights by random variables for training purposes.
• Attention mechanisms solved a second problem that had plagued statistics for over a century: how to increase the memory and complexity of a system without increasing the number of learnable parameters. Researchers found an elegant solution by using what can only be viewed as a learnable pointer structure (Bahdanau et al., 2014). Rather than having to remember an entire text sequence, e.g., for machine translation in a fixed-dimensional representation, all that needed to be stored was a pointer to the intermediate state of the translation process. This allowed for significantly increased accuracy for long sequences, since the model no longer needed to remember the entire sequence before commencing the generation of a new sequence.
• Multi-stage designs, e.g., via the memory networks (Sukhbaatar et al., 2015) and the neural programmer-interpreter (Reed & DeFreitas, 2015) allowed statistical modelers to describe iterative approaches to reasoning. These tools allow for an internal state of the deep neural network to be modified repeatedly, thus carrying out subsequent steps in a chain of reasoning, similar to how a processor can modify memory for a computation.
• Another key development was the invention of generative adversarial networks (Goodfellow et al., 2014). Traditionally, statistical methods for density estimation and generative models focused on finding proper probability distributions and (often approximate) algorithms for sampling from them. As a result, these algorithms were largely limited by the lack of flexibility inherent in the statistical models. The crucial innovation in generative adversarial networks was to replace the sampler by an arbitrary algorithm with differentiable parameters. These are then adjusted in such a way that the discriminator (effectively a two-sample test) cannot distinguish fake from real data. Through the ability to use arbitrary algorithms to generate data, it opened up density estimation to a wide variety of techniques. Examples of galloping Zebras (Zhu et al., 2017) and of fake celebrity faces (Karras et al., 2017) are both testimony to this progress. Even amateur doodlers can produce photorealistic images based on just sketches that describe how the layout of a scene looks like (Park et al., 2019).
• In many cases, a single GPU is insufficient to process the large amounts of data available for training. Over the past decade the ability to build parallel and distributed training algorithms has improved significantly. One of the key challenges in designing scalable algorithms is that the workhorse of deep learning optimization, stochastic gradient descent, re-

1.5. The Road to Deep Learning

37

lies on relatively small minibatches of data to be processed. At the same time, small batches limit the efficiency of GPUs. Hence, training on 1024 GPUs with a minibatch size of, say 32 images per batch amounts to an aggregate minibatch of about 32000 images. Recent work, first by Li (Li, 2017), and subsequently by (You et al., 2017) and (Jia et al., 2018) pushed the size up to 64000 observations, reducing training time for the ResNet-50 model on the ImageNet dataset to less than 7 minutes. For comparison—initially training times were measured in the order of days.
• The ability to parallelize computation has also contributed quite crucially to progress in reinforcement learning, at least whenever simulation is an option. This has led to significant progress in computers achieving superhuman performance in Go, Atari games, Starcraft, and in physics simulations (e.g., using MuJoCo). See e.g., (Silver et al., 2016) for a description of how to achieve this in AlphaGo. In a nutshell, reinforcement learning works best if plenty of (state, action, reward) triples are available, i.e., whenever it is possible to try out lots of things to learn how they relate to each other. Simulation provides such an avenue.
• Deep learning frameworks have played a crucial role in disseminating ideas. The first generation of frameworks allowing for easy modeling encompassed Caffe26, Torch27, and Theano28. Many seminal papers were written using these tools. By now, they have been superseded by TensorFlow29 (often used via its high level API Keras30), CNTK31, Caffe 232, and Apache MXNet33. The third generation of tools, namely imperative tools for deep learning, was arguably spearheaded by Chainer34, which used a syntax similar to Python NumPy to describe models. This idea was adopted by both PyTorch35, the Gluon API36 of MXNet, and Jax37.
The division of labor between system researchers building better tools and statistical modelers building better neural networks has greatly simplified things. For instance, training a linear logistic regression model used to be a nontrivial homework problem, worthy to give to new machine learning Ph.D. students at Carnegie Mellon University in 2014. By now, this task can be accomplished with less than 10 lines of code, putting it firmly into the grasp of programmers.
1.6 Success Stories
AI has a long history of delivering results that would be difficult to accomplish otherwise. For instance, the mail sorting systems using optical character recognition have been deployed since the 1990s. This is, after all, the source of the famous MNIST dataset of handwritten digits. The same applies to reading checks for bank deposits and scoring creditworthiness of applicants. Financial transactions are checked for fraud automatically. This forms the backbone of many e-commerce payment systems, such as PayPal, Stripe, AliPay, WeChat, Apple, Visa, and MasterCard. Computer programs for chess have been competitive for decades. Machine learning feeds search, recom-
26 https://github.com/BVLC/caffe 27 https://github.com/torch 28 https://github.com/Theano/Theano 29 https://github.com/tensorflow/tensorflow 30 https://github.com/keras-team/keras 31 https://github.com/Microsoft/CNTK 32 https://github.com/caffe2/caffe2 33 https://github.com/apache/incubator-mxnet 34 https://github.com/chainer/chainer 35 https://github.com/pytorch/pytorch 36 https://github.com/apache/incubator-mxnet 37 https://github.com/google/jax

38

Chapter 1. Introduction

mendation, personalization, and ranking on the Internet. In other words, machine learning is pervasive, albeit often hidden from sight.
It is only recently that AI has been in the limelight, mostly due to solutions to problems that were considered intractable previously and that are directly related to consumers. Many of such advances are attributed to deep learning.
• Intelligent assistants, such as Appleʼs Siri, Amazonʼs Alexa, and Googleʼs assistant, are able to answer spoken questions with a reasonable degree of accuracy. This includes menial tasks such as turning on light switches (a boon to the disabled) up to making barberʼs appointments and offering phone support dialog. This is likely the most noticeable sign that AI is affecting our lives.
• A key ingredient in digital assistants is the ability to recognize speech accurately. Gradually the accuracy of such systems has increased to the point where they reach human parity for certain applications (Xiong et al., 2018).
• Object recognition likewise has come a long way. Estimating the object in a picture was a fairly challenging task in 2010. On the ImageNet benchmark researchers from NEC Labs and University of Illinois at Urbana-Champaign achieved a top-5 error rate of 28% (Lin et al., 2010). By 2017, this error rate was reduced to 2.25% (Hu et al., 2018). Similarly, stunning results have been achieved for identifying birds or diagnosing skin cancer.
• Games used to be a bastion of human intelligence. Starting from TD-Gammon, a program for playing backgammon using temporal difference reinforcement learning, algorithmic and computational progress has led to algorithms for a wide range of applications. Unlike backgammon, chess has a much more complex state space and set of actions. DeepBlue beat Garry Kasparov using massive parallelism, special-purpose hardware and efficient search through the game tree (Campbell et al., 2002). Go is more difficult still, due to its huge state space. AlphaGo reached human parity in 2015, using deep learning combined with Monte Carlo tree sampling (Silver et al., 2016). The challenge in Poker was that the state space is large and it is not fully observed (we do not know the opponentsʼ cards). Libratus exceeded human performance in Poker using efficiently structured strategies (Brown & Sandholm, 2017). This illustrates the impressive progress in games and the fact that advanced algorithms played a crucial part in them.
• Another indication of progress in AI is the advent of self-driving cars and trucks. While full autonomy is not quite within reach yet, excellent progress has been made in this direction, with companies such as Tesla, NVIDIA, and Waymo shipping products that enable at least partial autonomy. What makes full autonomy so challenging is that proper driving requires the ability to perceive, to reason and to incorporate rules into a system. At present, deep learning is used primarily in the computer vision aspect of these problems. The rest is heavily tuned by engineers.
Again, the above list barely scratches the surface of where machine learning has impacted practical applications. For instance, robotics, logistics, computational biology, particle physics, and astronomy owe some of their most impressive recent advances at least in parts to machine learning. Machine learning is thus becoming a ubiquitous tool for engineers and scientists.
Frequently, the question of the AI apocalypse, or the AI singularity has been raised in nontechnical articles on AI. The fear is that somehow machine learning systems will become sentient and decide independently from their programmers (and masters) about things that directly affect the livelihood of humans. To some extent, AI already affects the livelihood of humans in an immediate way: creditworthiness is assessed automatically, autopilots mostly navigate vehicles, decisions about whether to grant bail use statistical data as input. More frivolously, we can ask

1.6. Success Stories

39

Alexa to switch on the coffee machine.
Fortunately, we are far from a sentient AI system that is ready to manipulate its human creators (or burn their coffee). First, AI systems are engineered, trained and deployed in a specific, goaloriented manner. While their behavior might give the illusion of general intelligence, it is a combination of rules, heuristics and statistical models that underlie the design. Second, at present tools for artificial general intelligence simply do not exist that are able to improve themselves, reason about themselves, and that are able to modify, extend, and improve their own architecture while trying to solve general tasks.
A much more pressing concern is how AI is being used in our daily lives. It is likely that many menial tasks fulfilled by truck drivers and shop assistants can and will be automated. Farm robots will likely reduce the cost for organic farming but they will also automate harvesting operations. This phase of the industrial revolution may have profound consequences on large swaths of society, since truck drivers and shop assistants are some of the most common jobs in many countries. Furthermore, statistical models, when applied without care can lead to racial, gender, or age bias and raise reasonable concerns about procedural fairness if automated to drive consequential decisions. It is important to ensure that these algorithms are used with care. With what we know today, this strikes us a much more pressing concern than the potential of malevolent superintelligence to destroy humanity.
1.7 Characteristics
Thus far, we have talked about machine learning broadly, which is both a branch of AI and an approach to AI. Though deep learning is a subset of machine learning, the dizzying set of algorithms and applications makes it difficult to assess what specifically the ingredients for deep learning might be. This is as difficult as trying to pin down required ingredients for pizza since almost every component is substitutable.
As we have described, machine learning can use data to learn transformations between inputs and outputs, such as transforming audio into text in speech recognition. In doing so, it is often necessary to represent data in a way suitable for algorithms to transform such representations into the output. Deep learning is deep in precisely the sense that its models learn many layers of transformations, where each layer offers the representation at one level. For example, layers near the input may represent low-level details of the data, while layers closer to the classification output may represent more abstract concepts used for discrimination. Since representation learning aims at finding the representation itself, deep learning can be referred to as multi-level representation learning.
The problems that we have discussed so far, such as learning from the raw audio signal, the raw pixel values of images, or mapping between sentences of arbitrary lengths and their counterparts in foreign languages, are those where deep learning excels and where traditional machine learning methods falter. It turns out that these many-layered models are capable of addressing lowlevel perceptual data in a way that previous tools could not. Arguably the most significant commonality in deep learning methods is the use of end-to-end training. That is, rather than assembling a system based on components that are individually tuned, one builds the system and then tunes their performance jointly. For instance, in computer vision scientists used to separate the process of feature engineering from the process of building machine learning models. The Canny edge detector (Canny, 1987) and Loweʼs SIFT feature extractor (Lowe, 2004) reigned supreme for over a decade as algorithms for mapping images into feature vectors. In bygone days, the crucial part of applying machine learning to these problems consisted of coming up with manually-engineered

40

Chapter 1. Introduction

ways of transforming the data into some form amenable to shallow models. Unfortunately, there is only so little that humans can accomplish by ingenuity in comparison with a consistent evaluation over millions of choices carried out automatically by an algorithm. When deep learning took over, these feature extractors were replaced by automatically tuned filters, yielding superior accuracy.
Thus, one key advantage of deep learning is that it replaces not only the shallow models at the end of traditional learning pipelines, but also the labor-intensive process of feature engineering. Moreover, by replacing much of the domain-specific preprocessing, deep learning has eliminated many of the boundaries that previously separated computer vision, speech recognition, natural language processing, medical informatics, and other application areas, offering a unified set of tools for tackling diverse problems.
Beyond end-to-end training, we are experiencing a transition from parametric statistical descriptions to fully nonparametric models. When data are scarce, one needs to rely on simplifying assumptions about reality in order to obtain useful models. When data are abundant, this can be replaced by nonparametric models that fit reality more accurately. To some extent, this mirrors the progress that physics experienced in the middle of the previous century with the availability of computers. Rather than solving parametric approximations of how electrons behave by hand, one can now resort to numerical simulations of the associated partial differential equations. This has led to much more accurate models, albeit often at the expense of explainability.
Another difference to previous work is the acceptance of suboptimal solutions, dealing with nonconvex nonlinear optimization problems, and the willingness to try things before proving them. This newfound empiricism in dealing with statistical problems, combined with a rapid influx of talent has led to rapid progress of practical algorithms, albeit in many cases at the expense of modifying and re-inventing tools that existed for decades.
In the end, the deep learning community prides itself on sharing tools across academic and corporate boundaries, releasing many excellent libraries, statistical models, and trained networks as open source. It is in this spirit that the notebooks forming this book are freely available for distribution and use. We have worked hard to lower the barriers of access for everyone to learn about deep learning and we hope that our readers will benefit from this.
Summary
• Machine learning studies how computer systems can leverage experience (often data) to improve performance at specific tasks. It combines ideas from statistics, data mining, and optimization. Often, it is used as a means of implementing AI solutions.
• As a class of machine learning, representational learning focuses on how to automatically find the appropriate way to represent data. Deep learning is multi-level representation learning through learning many layers of transformations.
• Deep learning replaces not only the shallow models at the end of traditional machine learning pipelines, but also the labor-intensive process of feature engineering.
• Much of the recent progress in deep learning has been triggered by an abundance of data arising from cheap sensors and Internet-scale applications, and by significant progress in computation, mostly through GPUs.
• Whole system optimization is a key component in obtaining high performance. The availability of efficient deep learning frameworks has made design and implementation of this

1.7. Characteristics

41

significantly easier.
Exercises
1. Which parts of code that you are currently writing could be “learned”, i.e., improved by learning and automatically determining design choices that are made in your code? Does your code include heuristic design choices?
2. Which problems that you encounter have many examples for how to solve them, yet no specific way to automate them? These may be prime candidates for using deep learning.
3. Viewing the development of AI as a new industrial revolution, what is the relationship between algorithms and data? Is it similar to steam engines and coal? What is the fundamental difference?
4. Where else can you apply the end-to-end training approach, such as in Fig. 1.1.2, physics, engineering, and econometrics?
Discussions38

38 https://discuss.d2l.ai/t/22
42

Chapter 1. Introduction

2 | Preliminaries
To get started with deep learning, we will need to develop a few basic skills. All machine learning is concerned with extracting information from data. So we will begin by learning the practical skills for storing, manipulating, and preprocessing data.
Moreover, machine learning typically requires working with large datasets, which we can think of as tables, where the rows correspond to examples and the columns correspond to attributes. Linear algebra gives us a powerful set of techniques for working with tabular data. We will not go too far into the weeds but rather focus on the basic of matrix operations and their implementation.
Additionally, deep learning is all about optimization. We have a model with some parameters and we want to find those that fit our data the best. Determining which way to move each parameter at each step of an algorithm requires a little bit of calculus, which will be briefly introduced. Fortunately, the autograd package automatically computes differentiation for us, and we will cover it next.
Next, machine learning is concerned with making predictions: what is the likely value of some unknown attribute, given the information that we observe? To reason rigorously under uncertainty we will need to invoke the language of probability.
In the end, the official documentation provides plenty of descriptions and examples that are beyond this book. To conclude the chapter, we will show you how to look up documentation for the needed information.
This book has kept the mathematical content to the minimum necessary to get a proper understanding of deep learning. However, it does not mean that this book is mathematics free. Thus, this chapter provides a rapid introduction to basic and frequently-used mathematics to allow anyone to understand at least most of the mathematical content of the book. If you wish to understand all of the mathematical content, further reviewing the online appendix on mathematics39 should be sufficient.
2.1 Data Manipulation
In order to get anything done, we need some way to store and manipulate data. Generally, there are two important things we need to do with data: (i) acquire them; and (ii) process them once they are inside the computer. There is no point in acquiring data without some way to store it, so let us get our hands dirty first by playing with synthetic data. To start, we introduce the n-dimensional array, which is also called the tensor.
If you have worked with NumPy, the most widely-used scientific computing package in Python, then you will find this section familiar. No matter which framework you use, its tensor class
39 https://d2l.ai/chapter_appendix-mathematics-for-deep-learning/index.html
43

(ndarray in MXNet, Tensor in both PyTorch and TensorFlow) is similar to NumPyʼs ndarray with a few killer features. First, GPU is well-supported to accelerate the computation whereas NumPy only supports CPU computation. Second, the tensor class supports automatic differentiation. These properties make the tensor class suitable for deep learning. Throughout the book, when we say tensors, we are referring to instances of the tensor class unless otherwise stated.
2.1.1 Getting Started
In this section, we aim to get you up and running, equipping you with the basic math and numerical computing tools that you will build on as you progress through the book. Do not worry if you struggle to grok some of the mathematical concepts or library functions. The following sections will revisit this material in the context of practical examples and it will sink in. On the other hand, if you already have some background and want to go deeper into the mathematical content, just skip this section. To start, we import the np (numpy) and npx (numpy_extension) modules from MXNet. Here, the np module includes functions supported by NumPy, while the npx module contains a set of extensions developed to empower deep learning within a NumPy-like environment. When using tensors, we almost always invoke the set_np function: this is for compatibility of tensor processing by other components of MXNet.
from mxnet import np, npx
npx.set_np()
A tensor represents a (possibly multi-dimensional) array of numerical values. With one axis, a tensor corresponds (in math) to a vector. With two axes, a tensor corresponds to a matrix. Tensors with more than two axes do not have special mathematical names. To start, we can use arange to create a row vector x containing the first 12 integers starting with 0, though they are created as floats by default. Each of the values in a tensor is called an element of the tensor. For instance, there are 12 elements in the tensor x. Unless otherwise specified, a new tensor will be stored in main memory and designated for CPU-based computation.
x = np.arange(12) x
array([ 0., 1., 2., 3., 4., 5., 6., 7., 8., 9., 10., 11.])
We can access a tensorʼs shape (the length along each axis) by inspecting its shape property.
x.shape
(12,)
If we just want to know the total number of elements in a tensor, i.e., the product of all of the shape elements, we can inspect its size. Because we are dealing with a vector here, the single element of its shape is identical to its size.
x.size

44

Chapter 2. Preliminaries

12
To change the shape of a tensor without altering either the number of elements or their values, we can invoke the reshape function. For example, we can transform our tensor, x, from a row vector with shape (12,) to a matrix with shape (3, 4). This new tensor contains the exact same values, but views them as a matrix organized as 3 rows and 4 columns. To reiterate, although the shape has changed, the elements have not. Note that the size is unaltered by reshaping.
X = x.reshape(3, 4) X

array([[ 0., [ 4., [ 8.,

1., 2., 3.], 5., 6., 7.], 9., 10., 11.]])

Reshaping by manually specifying every dimension is unnecessary. If our target shape is a matrix with shape (height, width), then after we know the width, the height is given implicitly. Why should we have to perform the division ourselves? In the example above, to get a matrix with 3 rows, we specified both that it should have 3 rows and 4 columns. Fortunately, tensors can automatically work out one dimension given the rest. We invoke this capability by placing -1 for the dimension that we would like tensors to automatically infer. In our case, instead of calling x.reshape(3, 4), we could have equivalently called x.reshape(-1, 4) or x.reshape(3, -1).
Typically, we will want our matrices initialized either with zeros, ones, some other constants, or numbers randomly sampled from a specific distribution. We can create a tensor representing a tensor with all elements set to 0 and a shape of (2, 3, 4) as follows:
np.zeros((2, 3, 4))

array([[[0., 0., 0., 0.], [0., 0., 0., 0.], [0., 0., 0., 0.]],
[[0., 0., 0., 0.], [0., 0., 0., 0.], [0., 0., 0., 0.]]])
Similarly, we can create tensors with each element set to 1 as follows:
np.ones((2, 3, 4))

array([[[1., 1., 1., 1.], [1., 1., 1., 1.], [1., 1., 1., 1.]],
[[1., 1., 1., 1.], [1., 1., 1., 1.], [1., 1., 1., 1.]]])
Often, we want to randomly sample the values for each element in a tensor from some probability distribution. For example, when we construct arrays to serve as parameters in a neural network,

2.1. Data Manipulation

45

we will typically initialize their values randomly. The following snippet creates a tensor with shape (3, 4). Each of its elements is randomly sampled from a standard Gaussian (normal) distribution with a mean of 0 and a standard deviation of 1.
np.random.normal(0, 1, size=(3, 4))

array([[ 2.2122064 , 1.1630787 , 0.7740038 , [ 1.0434405 , 0.29956347, 1.1839255 , [ 1.8917114 , -1.1688148 , -1.2347414 ,

0.4838046 ], 0.15302546], 1.5580711 ]])

We can also specify the exact values for each element in the desired tensor by supplying a Python list (or list of lists) containing the numerical values. Here, the outermost list corresponds to axis 0, and the inner list to axis 1.
np.array([[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])

array([[2., 1., 4., 3.], [1., 2., 3., 4.], [4., 3., 2., 1.]])

2.1.2 Operations
This book is not about software engineering. Our interests are not limited to simply reading and writing data from/to arrays. We want to perform mathematical operations on those arrays. Some of the simplest and most useful operations are the elementwise operations. These apply a standard scalar operation to each element of an array. For functions that take two arrays as inputs, elementwise operations apply some standard binary operator on each pair of corresponding elements from the two arrays. We can create an elementwise function from any function that maps from a scalar to a scalar.
In mathematical notation, we would denote such a unary scalar operator (taking one input) by the signature f : R → R. This just means that the function is mapping from any real number (R) onto another. Likewise, we denote a binary scalar operator (taking two real inputs, and yielding one output) by the signature f : R, R → R. Given any two vectors u and v of the same shape, and a binary operator f , we can produce a vector c = F (u, v) by setting ci ← f (ui, vi) for all i, where ci, ui, and vi are the ith elements of vectors c, u, and v. Here, we produced the vector-valued F : Rd, Rd → Rd by lifting the scalar function to an elementwise vector operation.
The common standard arithmetic operators (+, -, *, /, and **) have all been lifted to elementwise operations for any identically-shaped tensors of arbitrary shape. We can call elementwise operations on any two tensors of the same shape. In the following example, we use commas to formulate a 5-element tuple, where each element is the result of an elementwise operation.

46

Chapter 2. Preliminaries

Operations

The common standard arithmetic operators (+, -, *, /, and **) have all been lifted to elementwise operations.

x = np.array([1, 2, 4, 8]) y = np.array([2, 2, 2, 2]) x + y, x - y, x * y, x / y, x**y

# The ** operator is exponentiation

(array([ 3., 4., 6., 10.]), array([-1., 0., 2., 6.]), array([ 2., 4., 8., 16.]), array([0.5, 1. , 2. , 4. ]), array([ 1., 4., 16., 64.]))
Many more operations can be applied elementwise, including unary operators like exponentiation.
np.exp(x)

array([2.7182817e+00, 7.3890562e+00, 5.4598148e+01, 2.9809580e+03])
In addition to elementwise computations, we can also perform linear algebra operations, including vector dot products and matrix multiplication. We will explain the crucial bits of linear algebra (with no assumed prior knowledge) in Section 2.3.
We can also concatenate multiple tensors together, stacking them end-to-end to form a larger tensor. We just need to provide a list of tensors and tell the system along which axis to concatenate. The example below shows what happens when we concatenate two matrices along rows (axis 0, the first element of the shape) vs. columns (axis 1, the second element of the shape). We can see that the first output tensorʼs axis-0 length (6) is the sum of the two input tensorsʼ axis-0 lengths (3 + 3); while the second output tensorʼs axis-1 length (8) is the sum of the two input tensorsʼ axis-1 lengths (4 + 4).
X = np.arange(12).reshape(3, 4) Y = np.array([[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]]) np.concatenate([X, Y], axis=0), np.concatenate([X, Y], axis=1)

(array([[ 0., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.], [ 2., 1., 4., 3.], [ 1., 2., 3., 4.], [ 4., 3., 2., 1.]]),
array([[ 0., 1., 2., 3., 2., 1., 4., 3.], [ 4., 5., 6., 7., 1., 2., 3., 4.], [ 8., 9., 10., 11., 4., 3., 2., 1.]]))
Sometimes, we want to construct a binary tensor via logical statements. Take X == Y as an example. For each position, if X and Y are equal at that position, the corresponding entry in the new tensor takes a value of 1, meaning that the logical statement X == Y is true at that position; otherwise that position takes 0.

2.1. Data Manipulation

47

X == Y
array([[False, True, False, True], [False, False, False, False], [False, False, False, False]])
Summing all the elements in the tensor yields a tensor with only one element.
X.sum()
array(66.)
2.1.3 Broadcasting Mechanism
In the above section, we saw how to perform elementwise operations on two tensors of the same shape. Under certain conditions, even when shapes differ, we can still perform elementwise operations by invoking the broadcasting mechanism. This mechanism works in the following way: First, expand one or both arrays by copying elements appropriately so that after this transformation, the two tensors have the same shape. Second, carry out the elementwise operations on the resulting arrays. In most cases, we broadcast along an axis where an array initially only has length 1, such as in the following example:
a = np.arange(3).reshape(3, 1) b = np.arange(2).reshape(1, 2) a, b
(array([[0.], [1.], [2.]]),
array([[0., 1.]]))
Since a and b are 3 × 1 and 1 × 2 matrices respectively, their shapes do not match up if we want to add them. We broadcast the entries of both matrices into a larger 3 × 2 matrix as follows: for matrix a it replicates the columns and for matrix b it replicates the rows before adding up both elementwise.
a+b
array([[0., 1.], [1., 2.], [2., 3.]])

48

Chapter 2. Preliminaries

2.1.4 Indexing and Slicing
Just as in any other Python array, elements in a tensor can be accessed by index. As in any Python array, the first element has index 0 and ranges are specified to include the first but before the last element. As in standard Python lists, we can access elements according to their relative position to the end of the list by using negative indices. Thus, [-1] selects the last element and [1:3] selects the second and the third elements as follows:
X[-1], X[1:3]

(array([ 8., 9., 10., 11.]), array([[ 4., 5., 6., 7.],
[ 8., 9., 10., 11.]]))
Beyond reading, we can also write elements of a matrix by specifying indices.
X[1, 2] = 9 X

array([[ 0., [ 4., [ 8.,

1., 2., 3.], 5., 9., 7.], 9., 10., 11.]])

If we want to assign multiple elements the same value, we simply index all of them and then assign them the value. For instance, [0:2, :] accesses the first and second rows, where : takes all the elements along axis 1 (column). While we discussed indexing for matrices, this obviously also works for vectors and for tensors of more than 2 dimensions.
X[0:2, :] = 12 X

array([[12., 12., 12., 12.], [12., 12., 12., 12.], [ 8., 9., 10., 11.]])

2.1.5 Saving Memory
Running operations can cause new memory to be allocated to host results. For example, if we write Y = X + Y, we will dereference the tensor that Y used to point to and instead point Y at the newly allocated memory. In the following example, we demonstrate this with Pythonʼs id() function, which gives us the exact address of the referenced object in memory. After running Y = Y + X, we will find that id(Y) points to a different location. That is because Python first evaluates Y + X, allocating new memory for the result and then makes Y point to this new location in memory.
before = id(Y) Y=Y+X id(Y) == before

2.1. Data Manipulation

49

False
This might be undesirable for two reasons. First, we do not want to run around allocating memory unnecessarily all the time. In machine learning, we might have hundreds of megabytes of parameters and update all of them multiple times per second. Typically, we will want to perform these updates in place. Second, we might point at the same parameters from multiple variables. If we do not update in place, other references will still point to the old memory location, making it possible for parts of our code to inadvertently reference stale parameters.
Fortunately, performing in-place operations is easy. We can assign the result of an operation to a previously allocated array with slice notation, e.g., Y[:] = <expression>. To illustrate this concept, we first create a new matrix Z with the same shape as another Y, using zeros_like to allocate a block of 0 entries.
Z = np.zeros_like(Y) print('id(Z):', id(Z)) Z[:] = X + Y print('id(Z):', id(Z))
id(Z): 140273297113152 id(Z): 140273297113152
If the value of X is not reused in subsequent computations, we can also use X[:] = X + Y or X += Y to reduce the memory overhead of the operation.
before = id(X) X += Y id(X) == before
True

2.1.6 Conversion to Other Python Objects
Converting to a NumPy tensor (ndarray), or vice versa, is easy. The converted result does not share memory. This minor inconvenience is actually quite important: when you perform operations on the CPU or on GPUs, you do not want to halt computation, waiting to see whether the NumPy package of Python might want to be doing something else with the same chunk of memory.
A = X.asnumpy() B = np.array(A) type(A), type(B)
(numpy.ndarray, mxnet.numpy.ndarray)
To convert a size-1 tensor to a Python scalar, we can invoke the item function or Pythonʼs built-in functions.
a = np.array([3.5]) a, a.item(), float(a), int(a)

50

Chapter 2. Preliminaries

(array([3.5]), 3.5, 3.5, 3)

Summary
• The main interface to store and manipulate data for deep learning is the tensor (ndimensional array). It provides a variety of functionalities including basic mathematics operations, broadcasting, indexing, slicing, memory saving, and conversion to other Python objects.
Exercises
1. Run the code in this section. Change the conditional statement X == Y in this section to X < Y or X > Y, and then see what kind of tensor you can get.
2. Replace the two tensors that operate by element in the broadcasting mechanism with other shapes, e.g., 3-dimensional tensors. Is the result the same as expected?
Discussions40

2.2 Data Preprocessing
So far we have introduced a variety of techniques for manipulating data that are already stored in tensors. To apply deep learning to solving real-world problems, we often begin with preprocessing raw data, rather than those nicely prepared data in the tensor format. Among popular data analytic tools in Python, the pandas package is commonly used. Like many other extension packages in the vast ecosystem of Python, pandas can work together with tensors. So, we will briefly walk through steps for preprocessing raw data with pandas and converting them into the tensor format. We will cover more data preprocessing techniques in later chapters.

2.2.1 Reading the Dataset
As an example, we begin by creating an artificial dataset that is stored in a csv (comma-separated values) file ../data/house_tiny.csv. Data stored in other formats may be processed in similar ways.
Below we write the dataset row by row into a csv file.
import os
os.makedirs(os.path.join('..', 'data'), exist_ok=True) data_file = os.path.join('..', 'data', 'house_tiny.csv') with open(data_file, 'w') as f:
f.write('NumRooms,Alley,Price\n') # Column names f.write('NA,Pave,127500\n') # Each row represents a data example f.write('2,NA,106000\n') f.write('4,NA,178100\n') f.write('NA,NA,140000\n')
40 https://discuss.d2l.ai/t/26

2.2. Data Preprocessing

51

To load the raw dataset from the created csv file, we import the pandas package and invoke the read_csv function. This dataset has four rows and three columns, where each row describes the number of rooms (“NumRooms”), the alley type (“Alley”), and the price (“Price”) of a house.
# If pandas is not installed, just uncomment the following line: # !pip install pandas import pandas as pd
data = pd.read_csv(data_file) print(data)

NumRooms Alley Price

0

NaN Pave 127500

1

2.0 NaN 106000

2

4.0 NaN 178100

3

NaN NaN 140000

2.2.2 Handling Missing Data
Note that “NaN” entries are missing values. To handle missing data, typical methods include imputation and deletion, where imputation replaces missing values with substituted ones, while deletion ignores missing values. Here we will consider imputation.
By integer-location based indexing (iloc), we split data into inputs and outputs, where the former takes the first two columns while the latter only keeps the last column. For numerical values in inputs that are missing, we replace the “NaN” entries with the mean value of the same column.
inputs, outputs = data.iloc[:, 0:2], data.iloc[:, 2] inputs = inputs.fillna(inputs.mean()) print(inputs)

NumRooms Alley

0

3.0 Pave

1

2.0 NaN

2

4.0 NaN

3

3.0 NaN

For categorical or discrete values in inputs, we consider “NaN” as a category. Since the “Alley” column only takes two types of categorical values “Pave” and “NaN”, pandas can automatically convert this column to two columns “Alley_Pave” and “Alley_nan”. A row whose alley type is “Pave” will set values of “Alley_Pave” and “Alley_nan” to 1 and 0. A row with a missing alley type will set their values to 0 and 1.
inputs = pd.get_dummies(inputs, dummy_na=True) print(inputs)

NumRooms Alley_Pave Alley_nan

0

3.0

1

0

1

2.0

0

1

2

4.0

0

1

3

3.0

0

1

52

Chapter 2. Preliminaries

2.2.3 Conversion to the Tensor Format
Now that all the entries in inputs and outputs are numerical, they can be converted to the tensor format. Once data are in this format, they can be further manipulated with those tensor functionalities that we have introduced in Section 2.1.
from mxnet import np
X, y = np.array(inputs.values), np.array(outputs.values) X, y
(array([[3., 1., 0.], [2., 0., 1.], [4., 0., 1.], [3., 0., 1.]], dtype=float64),
array([127500, 106000, 178100, 140000], dtype=int64))
Summary
• Like many other extension packages in the vast ecosystem of Python, pandas can work together with tensors.
• Imputation and deletion can be used to handle missing data.
Exercises
Create a raw dataset with more rows and columns. 1. Delete the column with the most missing values. 2. Convert the preprocessed dataset to the tensor format.
Discussions41
2.3 Linear Algebra
Now that you can store and manipulate data, let us briefly review the subset of basic linear algebra that you will need to understand and implement most of models covered in this book. Below, we introduce the basic mathematical objects, arithmetic, and operations in linear algebra, expressing each of them through mathematical notation and the corresponding implementation in code.
41 https://discuss.d2l.ai/t/28

2.3. Linear Algebra

53

2.3.1 Scalars
If you never studied linear algebra or machine learning, then your past experience with math probably consisted of thinking about one number at a time. And, if you ever balanced a checkbook or even paid for dinner at a restaurant then you already know how to do basic things like adding and multiplying pairs of numbers. For example, the temperature in Palo Alto is 52 degrees Fahrenheit. Formally, we call values consisting of just one numerical quantity scalars. If you wanted to convert this value to Celsius (the metric systemʼs more sensible temperature scale), you would evaluate the expression c = 59 (f − 32), setting f to 52. In this equation, each of the terms—5, 9, and 32—are scalar values. The placeholders c and f are called variables and they represent unknown scalar values.
In this book, we adopt the mathematical notation where scalar variables are denoted by ordinary lower-cased letters (e.g., x, y, and z). We denote the space of all (continuous) real-valued scalars by R. For expedience, we will punt on rigorous definitions of what precisely space is, but just remember for now that the expression x ∈ R is a formal way to say that x is a real-valued scalar. The symbol ∈ can be pronounced “in” and simply denotes membership in a set. Analogously, we could write x, y ∈ {0, 1} to state that x and y are numbers whose value can only be 0 or 1.
A scalar is represented by a tensor with just one element. In the next snippet, we instantiate two scalars and perform some familiar arithmetic operations with them, namely addition, multiplication, division, and exponentiation.
from mxnet import np, npx
npx.set_np()
x = np.array(3.0) y = np.array(2.0)
x + y, x * y, x / y, x**y
(array(5.), array(6.), array(1.5), array(9.))

2.3.2 Vectors
You can think of a vector as simply a list of scalar values. We call these values the elements (entries or components) of the vector. When our vectors represent examples from our dataset, their values hold some real-world significance. For example, if we were training a model to predict the risk that a loan defaults, we might associate each applicant with a vector whose components correspond to their income, length of employment, number of previous defaults, and other factors. If we were studying the risk of heart attacks hospital patients potentially face, we might represent each patient by a vector whose components capture their most recent vital signs, cholesterol levels, minutes of exercise per day, etc. In math notation, we will usually denote vectors as bold-faced, lower-cased letters (e.g., x, y, and z).
We work with vectors via one-dimensional tensors. In general tensors can have arbitrary lengths, subject to the memory limits of your machine.
x = np.arange(4) x

54

Chapter 2. Preliminaries

array([0., 1., 2., 3.])

We can refer to any element of a vector by using a subscript. For example, we can refer to the ith

element of x by xi. Note that the element xi is a scalar, so we do not bold-face the font when refer-

ring to it. Extensive literature considers column vectors to be the default orientation of vectors,

so does this book. In math, a vector x can be written as

 x1
x2  x =  ...  ,

(2.3.1)

xn

where x1, . . . , xn are elements of the vector. In code, we access any element by indexing into the tensor.

x[3]

array(3.)

Length, Dimensionality, and Shape
Let us revisit some concepts from Section 2.1. A vector is just an array of numbers. And just as every array has a length, so does every vector. In math notation, if we want to say that a vector x consists of n real-valued scalars, we can express this as x ∈ Rn. The length of a vector is commonly called the dimension of the vector. As with an ordinary Python array, we can access the length of a tensor by calling Pythonʼs built-in len() function.
len(x)
4
When a tensor represents a vector (with precisely one axis), we can also access its length via the .shape attribute. The shape is a tuple that lists the length (dimensionality) along each axis of the tensor. For tensors with just one axis, the shape has just one element.
x.shape
(4,)
Note that the word “dimension” tends to get overloaded in these contexts and this tends to confuse people. To clarify, we use the dimensionality of a vector or an axis to refer to its length, i.e., the number of elements of a vector or an axis. However, we use the dimensionality of a tensor to refer to the number of axes that a tensor has. In this sense, the dimensionality of some axis of a tensor will be the length of that axis.

2.3. Linear Algebra

55

2.3.3 Matrices

Just as vectors generalize scalars from order zero to order one, matrices generalize vectors from order one to order two. Matrices, which we will typically denote with bold-faced, capital letters (e.g., X, Y, and Z), are represented in code as tensors with two axes.

In math notation, we use A ∈ Rm×n to express that the matrix A consists of m rows and n columns

of real-valued scalars. Visually, we can illustrate any matrix A ∈ Rm×n as a table, where each element aij belongs to the ith row and jth column:





a11 a12 · · · a1n

 a21 a22 · · · a2n 

A =  ... ... . . . ...  .

(2.3.2)

am1 am2 · · · amn

For any A ∈ Rm×n, the shape of A is (m, n) or m × n. Specifically, when a matrix has the same number of rows and columns, its shape becomes a square; thus, it is called a square matrix.

We can create an m × n matrix by specifying a shape with two components m and n when calling any of our favorite functions for instantiating a tensor.

A = np.arange(20).reshape(5, 4) A

array([[ 0., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.], [12., 13., 14., 15.], [16., 17., 18., 19.]])

We can access the scalar element aij of a matrix A in (2.3.2) by specifying the indices for the row (i) and column (j), such as [A]ij. When the scalar elements of a matrix A, such as in (2.3.2), are not given, we may simply use the lower-case letter of the matrix A with the index subscript, aij, to refer to [A]ij. To keep notation simple, commas are inserted to separate indices only when necessary, such as a2,3j and [A]2i−1,3.

Sometimes, we want to flip the axes. When we exchange a matrixʼs rows and columns, the result is

called the transpose of the matrix. Formally, we signify a matrix Aʼs transpose by A⊤ and if B = A⊤,

then bij = aji for any i and j. Thus, the transpose of A in (2.3.2) is a n × m matrix:





a11 a21 . . . am1

⊤ a12 a22 . . . am2

A =  ... ... . . . ...  .

(2.3.3)

a1n a2n . . . amn

Now we access a matrixʼs transpose in code.

A.T

array([[ 0., [ 1., [ 2., [ 3.,

4., 8., 12., 16.], 5., 9., 13., 17.], 6., 10., 14., 18.], 7., 11., 15., 19.]])

56

Chapter 2. Preliminaries

As a special type of the square matrix, a symmetric matrix A is equal to its transpose: A = A⊤. Here we define a symmetric matrix B.
B = np.array([[1, 2, 3], [2, 0, 4], [3, 4, 5]]) B

array([[1., 2., 3.], [2., 0., 4.], [3., 4., 5.]])
Now we compare B with its transpose.
B == B.T

array([[ True, [ True, [ True,

True, True, True,

True], True], True]])

Matrices are useful data structures: they allow us to organize data that have different modalities of variation. For example, rows in our matrix might correspond to different houses (data examples), while columns might correspond to different attributes. This should sound familiar if you have ever used spreadsheet software or have read Section 2.2. Thus, although the default orientation of a single vector is a column vector, in a matrix that represents a tabular dataset, it is more conventional to treat each data example as a row vector in the matrix. And, as we will see in later chapters, this convention will enable common deep learning practices. For example, along the outermost axis of a tensor, we can access or enumerate minibatches of data examples, or just data examples if no minibatch exists.

2.3.4 Tensors
Just as vectors generalize scalars, and matrices generalize vectors, we can build data structures with even more axes. Tensors (“tensors” in this subsection refer to algebraic objects) give us a generic way of describing n-dimensional arrays with an arbitrary number of axes. Vectors, for example, are first-order tensors, and matrices are second-order tensors. Tensors are denoted with capital letters of a special font face (e.g., X, Y, and Z) and their indexing mechanism (e.g., xijk and [X]1,2i−1,3) is similar to that of matrices.
Tensors will become more important when we start working with images, which arrive as ndimensional arrays with 3 axes corresponding to the height, width, and a channel axis for stacking the color channels (red, green, and blue). For now, we will skip over higher order tensors and focus on the basics.
X = np.arange(24).reshape(2, 3, 4) X

array([[[ 0., [ 4., [ 8.,

1., 2., 3.], 5., 6., 7.], 9., 10., 11.]],

[[12., 13., 14., 15.],

2.3. Linear Algebra

(continues on next page)
57

[16., 17., 18., 19.], [20., 21., 22., 23.]]])

(continued from previous page)

2.3.5 Basic Properties of Tensor Arithmetic
Scalars, vectors, matrices, and tensors (“tensors” in this subsection refer to algebraic objects) of an arbitrary number of axes have some nice properties that often come in handy. For example, you might have noticed from the definition of an elementwise operation that any elementwise unary operation does not change the shape of its operand. Similarly, given any two tensors with the same shape, the result of any binary elementwise operation will be a tensor of that same shape. For example, adding two matrices of the same shape performs elementwise addition over these two matrices.
A = np.arange(20).reshape(5, 4) B = A.copy() # Assign a copy of `A` to `B` by allocating new memory A, A + B

(array([[ 0., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.], [12., 13., 14., 15.], [16., 17., 18., 19.]]),
array([[ 0., 2., 4., 6.], [ 8., 10., 12., 14.], [16., 18., 20., 22.], [24., 26., 28., 30.], [32., 34., 36., 38.]]))

Specifically, elementwise multiplication of two matrices is called their Hadamard product (math notation ⊙). Consider matrix B ∈ Rm×n whose element of row i and column j is bij. The Hadamard

product of matrices A (defined in (2.3.2)) and B

 a11b11
 a21b21 A ⊙ B =  ...



a12b12 . . . a1nb1n

a22b22 . . . a2nb2n 

...

...

...  .

(2.3.4)

am1bm1 am2bm2 . . . amnbmn

A*B

array([[ 0., 1., 4., 9.], [ 16., 25., 36., 49.], [ 64., 81., 100., 121.], [144., 169., 196., 225.], [256., 289., 324., 361.]])
Multiplying or adding a tensor by a scalar also does not change the shape of the tensor, where each element of the operand tensor will be added or multiplied by the scalar.

58

Chapter 2. Preliminaries

a=2 X = np.arange(24).reshape(2, 3, 4) a + X, (a * X).shape
(array([[[ 2., 3., 4., 5.], [ 6., 7., 8., 9.], [10., 11., 12., 13.]],
[[14., 15., 16., 17.], [18., 19., 20., 21.], [22., 23., 24., 25.]]]), (2, 3, 4))

2.3.6 Reduction

One useful operation that we can perform with arbitrary tensors∑is to calculate the sum of their

elements. In mathematical notation, we express s∑ ums using the symbol. To express the sum

of the elements in a vector x of length d, we write

d i=1

xi.

In

code,

we

can

just

call

the

function

for calculating the sum.

x = np.arange(4) x, x.sum()

(array([0., 1., 2., 3.]), array(6.))

We can express sums over the elements of tensor∑s of ar∑bitrary shape. For example, the sum of the

elements of an m × n matrix A could be written

m i=1

n j=1

aij

.

A.shape, A.sum()

((5, 4), array(190.))
By default, invoking the function for calculating the sum reduces a tensor along all its axes to a scalar. We can also specify the axes along which the tensor is reduced via summation. Take matrices as an example. To reduce the row dimension (axis 0) by summing up elements of all the rows, we specify axis=0 when invoking the function. Since the input matrix reduces along axis 0 to generate the output vector, the dimension of axis 0 of the input is lost in the output shape.
A_sum_axis0 = A.sum(axis=0) A_sum_axis0, A_sum_axis0.shape

(array([40., 45., 50., 55.]), (4,))
Specifying axis=1 will reduce the column dimension (axis 1) by summing up elements of all the columns. Thus, the dimension of axis 1 of the input is lost in the output shape.
A_sum_axis1 = A.sum(axis=1) A_sum_axis1, A_sum_axis1.shape

2.3. Linear Algebra

59

(array([ 6., 22., 38., 54., 70.]), (5,))
Reducing a matrix along both rows and columns via summation is equivalent to summing up all the elements of the matrix. A.sum(axis=[0, 1]) # Same as `A.sum()`
array(190.)
A related quantity is the mean, which is also called the average. We calculate the mean by dividing the sum by the total number of elements. In code, we could just call the function for calculating the mean on tensors of arbitrary shape. A.mean(), A.sum() / A.size
(array(9.5), array(9.5))
Likewise, the function for calculating the mean can also reduce a tensor along the specified axes. A.mean(axis=0), A.sum(axis=0) / A.shape[0]
(array([ 8., 9., 10., 11.]), array([ 8., 9., 10., 11.]))

Non-Reduction Sum
However, sometimes it can be useful to keep the number of axes unchanged when invoking the function for calculating the sum or mean.
sum_A = A.sum(axis=1, keepdims=True) sum_A

array([[ 6.], [22.], [38.], [54.], [70.]])
For instance, since sum_A still keeps its two axes after summing each row, we can divide A by sum_A with broadcasting.
A / sum_A

array([[0.

, 0.16666667, 0.33333334, 0.5

],

[0.18181819, 0.22727273, 0.27272728, 0.3181818 ],

[0.21052632, 0.23684211, 0.2631579 , 0.28947368],

[0.22222222, 0.24074075, 0.25925925, 0.2777778 ],

[0.22857143, 0.24285714, 0.25714287, 0.27142859]])

60

Chapter 2. Preliminaries

If we want to calculate the cumulative sum of elements of A along some axis, say axis=0 (row by row), we can call the cumsum function. This function will not reduce the input tensor along any axis.
A.cumsum(axis=0)
array([[ 0., 1., 2., 3.], [ 4., 6., 8., 10.], [12., 15., 18., 21.], [24., 28., 32., 36.], [40., 45., 50., 55.]])

2.3.7 Dot Products

So far, we have only performed elementwise operations, sums, and averages. And if this was all

we could do, linear algebra probably would not deserve its own section. However, one of the most

fundamental operations is the dot product. Given two vectors x, y ∈ Rd, their ∑ dot product x⊤y (or

⟨x, y⟩) is a sum over the products of the elements at the same position: x⊤y =

d i=1

xiyi.

y = np.ones(4) x, y, np.dot(x, y)

(array([0., 1., 2., 3.]), array([1., 1., 1., 1.]), array(6.))
Note that we can express the dot product of two vectors equivalently by performing an elementwise multiplication and then a sum: np.sum(x * y)

array(6.)

Dot products are useful in a wide range of contexts. For example, given some set of values, denoted

by a vector x ∈ Rd and a set of weights denoted by w ∈ Rd, the weighted sum of the values in x

according to the weights w could b(e∑expressed )as the dot product x⊤w. When the weights are

non-negative and sum to one (i.e.,

d i=1

wi

=

1

), the dot product expresses a weighted average.

After normalizing two vectors to have the unit length, the dot products express the cosine of the

angle between them. We will formally introduce this notion of length later in this section.

2.3.8 Matrix-Vector Products

Now that we know how to calculate dot products, we can begin to understand matrix-vector products. Recall the matrix A ∈ Rm×n and the vector x ∈ Rn defined and visualized in (2.3.2) and (2.3.1)

respectively. Let us start off by visualizing the matrix A in terms of its row vectors

a⊤ 
1
a⊤2  A =  ...  ,
a⊤m

(2.3.5)

2.3. Linear Algebra

61

where each a⊤i ∈ Rn is a row vector representing the ith row of the matrix A.

The matrix-vector product Ax is simply a column vector of length m, whose ith element is the dot

product a⊤i x:

a⊤ a⊤x

1

1

a⊤2  a⊤2 x

Ax =  ...  x =  ...  .

(2.3.6)

a⊤m

a⊤mx

We can think of multiplication by a matrix A ∈ Rm×n as a transformation that projects vectors from Rn to Rm. These transformations turn out to be remarkably useful. For example, we can represent rotations as multiplications by a square matrix. As we will see in subsequent chapters, we can also use matrix-vector products to describe the most intensive calculations required when computing each layer in a neural network given the values of the previous layer.

Expressing matrix-vector products in code with tensors, we use the same dot function as for dot products. When we call np.dot(A, x) with a matrix A and a vector x, the matrix-vector product is performed. Note that the column dimension of A (its length along axis 1) must be the same as the dimension of x (its length).

A.shape, x.shape, np.dot(A, x)

((5, 4), (4,), array([ 14., 38., 62., 86., 110.]))

2.3.9 Matrix-Matrix Multiplication

If you have gotten the hang of dot products and matrix-vector products, then matrix-matrix multiplication should be straightforward.

Say that we have two matrices A ∈ Rn×k and B ∈ Rk×m:









a11 a12 · · · a1k

b11 b12 · · · b1m

a21 a22 · · · a2k 

b21 b22 · · · b2m

A =  ... ... . . . ...  , B =  ... ... . . . ...  .

an1 an2 · · · ank

bk1 bk2 · · · bkm

(2.3.7)

Denote by a⊤i ∈ Rk the row vector representing the ith row of the matrix A, and let bj ∈ Rk be the column vector from the jth column of the matrix B. To produce the matrix product C = AB, it is

easiest to think of A in terms of its row vectors and B in terms of its column vectors:

a⊤

1

a⊤2 

[

]

A =  ...  , B = b1 b2 · · · bm .

(2.3.8)

a⊤n

Then the matrix product C ∈ Rn×m is produced as we simply compute each element cij as the dot

product a⊤i bj:

a⊤

a⊤b1 a⊤b2 · · · a⊤bm

1

1

1

1

a⊤2  [

] a⊤2 b1 a⊤2 b2 · · · a⊤2 bm

C = AB =  ...  b1 b2 · · · bm =  ...

... . . . ...  .

(2.3.9)

a⊤n

a⊤n b1 a⊤n b2 · · · a⊤n bm

62

Chapter 2. Preliminaries

We can think of the matrix-matrix multiplication AB as simply performing m matrix-vector products and stitching the results together to form an n × m matrix. In the following snippet, we perform matrix multiplication on A and B. Here, A is a matrix with 5 rows and 4 columns, and B is a matrix with 4 rows and 3 columns. After multiplication, we obtain a matrix with 5 rows and 3 columns.
B = np.ones(shape=(4, 3)) np.dot(A, B)

array([[ 6., 6., 6.], [22., 22., 22.], [38., 38., 38.], [54., 54., 54.], [70., 70., 70.]])
Matrix-matrix multiplication can be simply called matrix multiplication, and should not be confused with the Hadamard product.

2.3.10 Norms

Some of the most useful operators in linear algebra are norms. Informally, the norm of a vector tells us how big a vector is. The notion of size under consideration here concerns not dimensionality but rather the magnitude of the components.

In linear algebra, a vector norm is a function f that maps a vector to a scalar, satisfying a handful of properties. Given any vector x, the first property says that if we scale all the elements of a vector by a constant factor α, its norm also scales by the absolute value of the same constant factor:

f (αx) = |α|f (x).

(2.3.10)

The second property is the familiar triangle inequality:

f (x + y) ≤ f (x) + f (y).

(2.3.11)

The third property simply says that the norm must be non-negative:

f (x) ≥ 0.

(2.3.12)

That makes sense, as in most contexts the smallest size for anything is 0. The final property requires that the smallest norm is achieved and only achieved by a vector consisting of all zeros.

∀i, [x]i = 0 ⇔ f (x) = 0.

(2.3.13)

You might notice that norms sound a lot like measures of distance. And if you remember Euclidean distances (think Pythagorasʼ theorem) from grade school, then the concepts of non-negativity and the triangle inequality might ring a bell. In fact, the Euclidean distance is a norm: specifically it is the L2 norm. Suppose that the elements in the n-dimensional vector x are x1, . . . , xn.

The L2 norm of x is the square root of the sum of the squares of the vector elements:

∥x∥2 =

∑n x2i ,
i=1

(2.3.14)

where the subscript 2 is often omitted in L2 norms, i.e., ∥x∥ is equivalent to ∥x∥2. In code, we can calculate the L2 norm of a vector as follows.

2.3. Linear Algebra

63

u = np.array([3, -4]) np.linalg.norm(u)

array(5.)

In deep learning, we work more often with the squared L2 norm.

You will also frequently encounter the L1 norm, which is expressed as the sum of the absolute values of the vector elements:

∑n ∥x∥1 = |xi| .
i=1

(2.3.15)

As compared with the L2 norm, it is less influenced by outliers. To calculate the L1 norm, we compose the absolute value function with a sum over the elements.

np.abs(u).sum()

array(7.)

Both the L2 norm and the L1 norm are special cases of the more general Lp norm:

(∑n )1/p

∥x∥p =

|xi|p .

i=1

(2.3.16)

Analogous to L2 norms of vectors, the Frobenius norm of a matrix X ∈ Rm×n is the square root of the sum of the squares of the matrix elements:

∥X∥F =

∑ m ∑n x2ij .
i=1 j=1

(2.3.17)

The Frobenius norm satisfies all the properties of vector norms. It behaves as if it were an L2 norm of a matrix-shaped vector. Invoking the following function will calculate the Frobenius norm of a matrix.

np.linalg.norm(np.ones((4, 9)))

array(6.)

Norms and Objectives

While we do not want to get too far ahead of ourselves, we can plant some intuition already about why these concepts are useful. In deep learning, we are often trying to solve optimization problems: maximize the probability assigned to observed data; minimize the distance between predictions and the ground-truth observations. Assign vector representations to items (like words, products, or news articles) such that the distance between similar items is minimized, and the distance between dissimilar items is maximized. Oftentimes, the objectives, perhaps the most important components of deep learning algorithms (besides the data), are expressed as norms.

64

Chapter 2. Preliminaries

2.3.11 More on Linear Algebra
In just this section, we have taught you all the linear algebra that you will need to understand a remarkable chunk of modern deep learning. There is a lot more to linear algebra and a lot of that mathematics is useful for machine learning. For example, matrices can be decomposed into factors, and these decompositions can reveal low-dimensional structure in real-world datasets. There are entire subfields of machine learning that focus on using matrix decompositions and their generalizations to high-order tensors to discover structure in datasets and solve prediction problems. But this book focuses on deep learning. And we believe you will be much more inclined to learn more mathematics once you have gotten your hands dirty deploying useful machine learning models on real datasets. So while we reserve the right to introduce more mathematics much later on, we will wrap up this section here.
If you are eager to learn more about linear algebra, you may refer to either the online appendix on linear algebraic operations42 or other excellent resources (Strang, 1993; Kolter, 2008; Petersen et al., 2008).
Summary
• Scalars, vectors, matrices, and tensors are basic mathematical objects in linear algebra.
• Vectors generalize scalars, and matrices generalize vectors.
• Scalars, vectors, matrices, and tensors have zero, one, two, and an arbitrary number of axes, respectively.
• A tensor can be reduced along the specified axes by sum and mean.
• Elementwise multiplication of two matrices is called their Hadamard product. It is different from matrix multiplication.
• In deep learning, we often work with norms such as the L1 norm, the L2 norm, and the Frobenius norm.
• We can perform a variety of operations over scalars, vectors, matrices, and tensors.
Exercises
1. Prove that the transpose of a matrix Aʼs transpose is A: (A⊤)⊤ = A.
2. Given two matrices A and B, show that the sum of transposes is equal to the transpose of a sum: A⊤ + B⊤ = (A + B)⊤.
3. Given any square matrix A, is A + A⊤ always symmetric? Why? 4. We defined the tensor X of shape (2, 3, 4) in this section. What is the output of len(X)? 5. For a tensor X of arbitrary shape, does len(X) always correspond to the length of a certain
axis of X? What is that axis? 6. Run A / A.sum(axis=1) and see what happens. Can you analyze the reason?
7. When traveling between two points in Manhattan, what is the distance that you need to cover in terms of the coordinates, i.e., in terms of avenues and streets? Can you travel diagonally?
42 https://d2l.ai/chapter_appendix-mathematics-for-deep-learning/geometry-linear-algebraic-ops.html

2.3. Linear Algebra

65

8. Consider a tensor with shape (2, 3, 4). What are the shapes of the summation outputs along axis 0, 1, and 2?
9. Feed a tensor with 3 or more axes to the linalg.norm function and observe its output. What does this function compute for tensors of arbitrary shape?
Discussions43
2.4 Calculus
Finding the area of a polygon had remained mysterious until at least 2,500 years ago, when ancient Greeks divided a polygon into triangles and summed their areas. To find the area of curved shapes, such as a circle, ancient Greeks inscribed polygons in such shapes. As shown in Fig. 2.4.1, an inscribed polygon with more sides of equal length better approximates the circle. This process is also known as the method of exhaustion.

Fig. 2.4.1: Find the area of a circle with the method of exhaustion.
In fact, the method of exhaustion is where integral calculus (will be described in sec_integral_calculus) originates from. More than 2,000 years later, the other branch of calculus, differential calculus, was invented. Among the most critical applications of differential calculus, optimization problems consider how to do something the best. As discussed in Section 2.3.10, such problems are ubiquitous in deep learning.
In deep learning, we train models, updating them successively so that they get better and better as they see more and more data. Usually, getting better means minimizing a loss function, a score that answers the question “how bad is our model?” This question is more subtle than it appears. Ultimately, what we really care about is producing a model that performs well on data that we have never seen before. But we can only fit the model to data that we can actually see. Thus we can decompose the task of fitting models into two key concerns: (i) optimization: the process of fitting our models to observed data; (ii) generalization: the mathematical principles and practitionersʼ wisdom that guide as to how to produce models whose validity extends beyond the exact set of data examples used to train them.
To help you understand optimization problems and methods in later chapters, here we give a very brief primer on differential calculus that is commonly used in deep learning.
43 https://discuss.d2l.ai/t/30

66

Chapter 2. Preliminaries

2.4.1 Derivatives and Differentiation

We begin by addressing the calculation of derivatives, a crucial step in nearly all deep learning optimization algorithms. In deep learning, we typically choose loss functions that are differentiable with respect to our modelʼs parameters. Put simply, this means that for each parameter, we can determine how rapidly the loss would increase or decrease, were we to increase or decrease that parameter by an infinitesimally small amount.
Suppose that we have a function f : R → R, whose input and output are both scalars. The derivative of f is defined as

f ′(x)

=

lim

f (x

+

h)

−

f (x) ,

h→0

h

(2.4.1)

if this limit exists. If f ′(a) exists, f is said to be differentiable at a. If f is differentiable at every number of an interval, then this function is differentiable on this interval. We can interpret the derivative f ′(x) in (2.4.1) as the instantaneous rate of change of f (x) with respect to x. The so-called instantaneous rate of change is based on the variation h in x, which approaches 0.

To illustrate derivatives, let us experiment with an example. Define u = f (x) = 3x2 − 4x.

%matplotlib inline from IPython import display from mxnet import np, npx from d2l import mxnet as d2l

npx.set_np()

def f(x): return 3 * x ** 2 - 4 * x

By

setting

x

=

1

and

letting

h

approach

0,

the

numerical

result

of

f (x+h)−f (x) h

in

(2.4.1)

approaches

2. Though this experiment is not a mathematical proof, we will see later that the derivative u′ is 2

when x = 1.

def numerical_lim(f, x, h): return (f(x + h) - f(x)) / h

h = 0.1 for i in range(5):
print(f'h={h:.5f}, numerical limit={numerical_lim(f, 1, h):.5f}') h *= 0.1

h=0.10000, numerical limit=2.30000 h=0.01000, numerical limit=2.03000 h=0.00100, numerical limit=2.00300 h=0.00010, numerical limit=2.00030 h=0.00001, numerical limit=2.00003

Let us familiarize ourselves with a few equivalent notations for derivatives. Given y = f (x), where x and y are the independent variable and the dependent variable of the function f , respectively. The following expressions are equivalent:

f ′(x) = y′ = dy = df

=

d f (x) = Df (x) = Dxf (x),

dx dx dx

(2.4.2)

2.4. Calculus

67

where

symbols

d dx

and

D

are

differentiation

operators

that

indicate

operation

of

differentiation.

We

can use the following rules to differentiate common functions:

• DC = 0 (C is a constant),

• Dxn = nxn−1 (the power rule, n is any real number),

• Dex = ex,

• D ln(x) = 1/x.

To differentiate a function that is formed from a few simpler functions such as the above common functions, the following rules can be handy for us. Suppose that functions f and g are both differentiable and C is a constant, we have the constant multiple rule

the sum rule the product rule

d

d

[Cf (x)] = C f (x),

dx

dx

d

d

d

[f (x) + g(x)] = f (x) + g(x),

dx

dx

dx

(2.4.3) (2.4.4)

and the quotient rule

d

d

d

[f (x)g(x)] = f (x) [g(x)] + g(x) [f (x)],

dx

dx

dx

[] d f (x)

g(x) d [f (x)] − f (x) d [g(x)]

dx g(x) = dx [g(x)]2 dx .

(2.4.5) (2.4.6)

Now we can apply a few of the above rules to find u′ = f ′(x) = 3 ddx x2 − 4 ddx x = 6x − 4. Thus, by setting x = 1, we have u′ = 2: this is supported by our earlier experiment in this section where the numerical result approaches 2. This derivative is also the slope of the tangent line to the curve u = f (x) when x = 1.
To visualize such an interpretation of derivatives, we will use matplotlib, a popular plotting library in Python. To configure properties of the figures produced by matplotlib, we need to define a few functions. In the following, the use_svg_display function specifies the matplotlib package to output the svg figures for sharper images. Note that the comment #@save is a special mark where the following function, class, or statements are saved in the d2l package so later they can be directly invoked (e.g., d2l.use_svg_display()) without being redefined.

def use_svg_display(): #@save """Use the svg format to display a plot in Jupyter.""" display.set_matplotlib_formats('svg')

We define the set_figsize function to specify the figure sizes. Note that here we directly use d2l. plt since the import statement from matplotlib import pyplot as plt has been marked for being saved in the d2l package in the preface.
def set_figsize(figsize=(3.5, 2.5)): #@save """Set the figure size for matplotlib.""" use_svg_display() d2l.plt.rcParams['figure.figsize'] = figsize

68

Chapter 2. Preliminaries

The following set_axes function sets properties of axes of figures produced by matplotlib.

#@save def set_axes(axes, xlabel, ylabel, xlim,
"""Set the axes for matplotlib.""" axes.set_xlabel(xlabel) axes.set_ylabel(ylabel) axes.set_xscale(xscale) axes.set_yscale(yscale) axes.set_xlim(xlim) axes.set_ylim(ylim) if legend:
axes.legend(legend) axes.grid()

ylim,

xscale,

yscale,

legend):

With these three functions for figure configurations, we define the plot function to plot multiple curves succinctly since we will need to visualize many curves throughout the book.

#@save def plot(X, Y=None, xlabel=None, ylabel=None, legend=None, xlim=None,
ylim=None, xscale='linear', yscale='linear', fmts=('-', 'm--', 'g-.', 'r:'), figsize=(3.5, 2.5), axes=None): """Plot data points.""" if legend is None: legend = []

set_figsize(figsize) axes = axes if axes else d2l.plt.gca()

# Return True if `X` (tensor or list) has 1 axis def has_one_axis(X):
return (hasattr(X, "ndim") and X.ndim == 1 or isinstance(X, list) and not hasattr(X[0], "__len__"))

if has_one_axis(X): X = [X]
if Y is None: X, Y = [[]] * len(X), X
elif has_one_axis(Y): Y = [Y]
if len(X) != len(Y): X = X * len(Y)
axes.cla() for x, y, fmt in zip(X, Y, fmts):
if len(x): axes.plot(x, y, fmt)
else: axes.plot(y, fmt)
set_axes(axes, xlabel, ylabel, xlim,

ylim,

xscale,

yscale,

legend)

Now we can plot the function u = f (x) and its tangent line y = 2x−3 at x = 1, where the coefficient 2 is the slope of the tangent line.
x = np.arange(0, 3, 0.1) plot(x, [f(x), 2 * x - 3], 'x', 'f(x)', legend=['f(x)', 'Tangent line (x=1)'])

2.4. Calculus

69

2.4.2 Partial Derivatives

So far we have dealt with the differentiation of functions of just one variable. In deep learning, functions often depend on many variables. Thus, we need to extend the ideas of differentiation to these multivariate functions.

Let y = f (x1, x2, . . . , xn) be a function with n variables. The partial derivative of y with respect to its ith parameter xi is

∂y = lim f (x1, . . . , xi−1, xi + h, xi+1, . . . , xn) − f (x1, . . . , xi, . . . , xn) .

∂xi h→0

h

(2.4.7)

To calculate ∂∂xyi , we can simply treat x1, . . . , xi−1, xi+1, . . . , xn as constants and calculate the derivative of y with respect to xi. For notation of partial derivatives, the following are equivalent:

∂y ∂f ∂xi = ∂xi = fxi = fi = Dif = Dxi f.

(2.4.8)

2.4.3 Gradients

We can concatenate partial derivatives of a multivariate function with respect to all its variables to obtain the gradient vector of the function. Suppose that the input of function f : Rn → R is an n-dimensional vector x = [x1, x2, . . . , xn]⊤ and the output is a scalar. The gradient of the function
f (x) with respect to x is a vector of n partial derivatives:

[ ∂f (x) ∂f (x)

∂f (x) ]⊤

∇xf (x) = ∂x1 , ∂x2 , . . . , ∂xn ,

(2.4.9)

where ∇xf (x) is often replaced by ∇f (x) when there is no ambiguity.

Let x be an n-dimensional vector, the following rules are often used when differentiating multivariate functions:
• For all A ∈ Rm×n, ∇xAx = A⊤, • For all A ∈ Rn×m, ∇xx⊤A = A, • For all A ∈ Rn×n, ∇xx⊤Ax = (A + A⊤)x, • ∇x∥x∥2 = ∇xx⊤x = 2x.

70

Chapter 2. Preliminaries

Similarly, for any matrix X, we have ∇X∥X∥2F = 2X. As we will see later, gradients are useful for designing optimization algorithms in deep learning.

2.4.4 Chain Rule

However, such gradients can be hard to find. This is because multivariate functions in deep learning are often composite, so we may not apply any of the aforementioned rules to differentiate these functions. Fortunately, the chain rule enables us to differentiate composite functions.
Let us first consider functions of a single variable. Suppose that functions y = f (u) and u = g(x) are both differentiable, then the chain rule states that

dy dy du

=

.

dx du dx

(2.4.10)

Now let us turn our attention to a more general scenario where functions have an arbitrary number of variables. Suppose that the differentiable function y has variables u1, u2, . . . , um, where each differentiable function ui has variables x1, x2, . . . , xn. Note that y is a function of x1, x2, . . . , xn. Then the chain rule gives

dy = dy du1 + dy du2 + · · · + dy dum

dxi du1 dxi du2 dxi

dum dxi

(2.4.11)

for any i = 1, 2, . . . , n.

Summary
• Differential calculus and integral calculus are two branches of calculus, where the former can be applied to the ubiquitous optimization problems in deep learning.
• A derivative can be interpreted as the instantaneous rate of change of a function with respect to its variable. It is also the slope of the tangent line to the curve of the function.
• A gradient is a vector whose components are the partial derivatives of a multivariate function with respect to all its variables.
• The chain rule enables us to differentiate composite functions.

Exercises

1.

Plot the function y

= f (x) = x3 −

1 x

and its tangent line when x = 1.

2. Find the gradient of the function f (x) = 3x21 + 5ex2.

3. What is the gradient of the function f (x) = ∥x∥2?

4. Can you write out the chain rule for the case where u = f (x, y, z) and x = x(a, b), y = y(a, b), and z = z(a, b)?

Discussions44
44 https://discuss.d2l.ai/t/32

2.4. Calculus

71

2.5 Automatic Differentiation
As we have explained in Section 2.4, differentiation is a crucial step in nearly all deep learning optimization algorithms. While the calculations for taking these derivatives are straightforward, requiring only some basic calculus, for complex models, working out the updates by hand can be a pain (and often error-prone). Deep learning frameworks expedite this work by automatically calculating derivatives, i.e., automatic differentiation. In practice, based on our designed model the system builds a computational graph, tracking which data combined through which operations to produce the output. Automatic differentiation enables the system to subsequently backpropagate gradients. Here, backpropagate simply means to trace through the computational graph, filling in the partial derivatives with respect to each parameter.
2.5.1 A Simple Example
As a toy example, say that we are interested in differentiating the function y = 2x⊤x with respect to the column vector x. To start, let us create the variable x and assign it an initial value.
from mxnet import autograd, np, npx
npx.set_np()
x = np.arange(4.0) x
array([0., 1., 2., 3.])
Before we even calculate the gradient of y with respect to x, we will need a place to store it. It is important that we do not allocate new memory every time we take a derivative with respect to a parameter because we will often update the same parameters thousands or millions of times and could quickly run out of memory. Note that a gradient of a scalar-valued function with respect to a vector x is itself vector-valued and has the same shape as x.
# We allocate memory for a tensor's gradient by invoking `attach_grad` x.attach_grad() # After we calculate a gradient taken with respect to `x`, we will be able to # access it via the `grad` attribute, whose values are initialized with 0s x.grad
array([0., 0., 0., 0.])
Now let us calculate y.
# Place our code inside an `autograd.record` scope to build the computational # graph with autograd.record():
y = 2 * np.dot(x, x) y

72

Chapter 2. Preliminaries

array(28.)
Since x is a vector of length 4, an dot product of x and x is performed, yielding the scalar output that we assign to y. Next, we can automatically calculate the gradient of y with respect to each component of x by calling the function for backpropagation and printing the gradient. y.backward() x.grad
array([ 0., 4., 8., 12.])
The gradient of the function y = 2x⊤x with respect to x should be 4x. Let us quickly verify that our desired gradient was calculated correctly. x.grad == 4 * x
array([ True, True, True, True])
Now let us calculate another function of x. with autograd.record():
y = x.sum() y.backward() x.grad # Overwritten by the newly calculated gradient
array([1., 1., 1., 1.])

2.5.2 Backward for Non-Scalar Variables
Technically, when y is not a scalar, the most natural interpretation of the differentiation of a vector y with respect to a vector x is a matrix. For higher-order and higher-dimensional y and x, the differentiation result could be a high-order tensor.
However, while these more exotic objects do show up in advanced machine learning (including in deep learning), more often when we are calling backward on a vector, we are trying to calculate the derivatives of the loss functions for each constituent of a batch of training examples. Here, our intent is not to calculate the differentiation matrix but rather the sum of the partial derivatives computed individually for each example in the batch.
# When we invoke `backward` on a vector-valued variable `y` (function of `x`), # a new scalar variable is created by summing the elements in `y`. Then the # gradient of that scalar variable with respect to `x` is computed with autograd.record():
y = x * x # `y` is a vector y.backward() x.grad # Equals to y = sum(x * x)
array([0., 2., 4., 6.])

2.5. Automatic Differentiation

73

2.5.3 Detaching Computation
Sometimes, we wish to move some calculations outside of the recorded computational graph. For example, say that y was calculated as a function of x, and that subsequently z was calculated as a function of both y and x. Now, imagine that we wanted to calculate the gradient of z with respect to x, but wanted for some reason to treat y as a constant, and only take into account the role that x played after y was calculated.
Here, we can detach y to return a new variable u that has the same value as y but discards any information about how y was computed in the computational graph. In other words, the gradient will not flow backwards through u to x. Thus, the following backpropagation function computes the partial derivative of z = u * x with respect to x while treating u as a constant, instead of the partial derivative of z = x * x * x with respect to x.
with autograd.record(): y=x*x u = y.detach() z=u*x
z.backward() x.grad == u
array([ True, True, True, True])
Since the computation of y was recorded, we can subsequently invoke backpropagation on y to get the derivative of y = x * x with respect to x, which is 2 * x.
y.backward() x.grad == 2 * x
array([ True, True, True, True])

2.5.4 Computing the Gradient of Python Control Flow
One benefit of using automatic differentiation is that even if building the computational graph of a function required passing through a maze of Python control flow (e.g., conditionals, loops, and arbitrary function calls), we can still calculate the gradient of the resulting variable. In the following snippet, note that the number of iterations of the while loop and the evaluation of the if statement both depend on the value of the input a.
def f(a): b=a*2 while np.linalg.norm(b) < 1000: b=b*2 if b.sum() > 0: c=b else: c = 100 * b return c
Let us compute the gradient.

74

Chapter 2. Preliminaries

a = np.random.normal() a.attach_grad() with autograd.record():
d = f(a) d.backward()
We can now analyze the f function defined above. Note that it is piecewise linear in its input a. In other words, for any a there exists some constant scalar k such that f(a) = k * a, where the value of k depends on the input a. Consequently d / a allows us to verify that the gradient is correct.
a.grad == d / a
array(True)

Summary
• Deep learning frameworks can automate the calculation of derivatives. To use it, we first attach gradients to those variables with respect to which we desire partial derivatives. We then record the computation of our target value, execute its function for backpropagation, and access the resulting gradient.

Exercises

1. Why is the second derivative much more expensive to compute than the first derivative?

2. After running the function for backpropagation, immediately run it again and see what happens.

3. In the control flow example where we calculate the derivative of d with respect to a, what would happen if we changed the variable a to a random vector or matrix. At this point, the result of the calculation f(a) is no longer a scalar. What happens to the result? How do we
analyze this?

4. Redesign an example of finding the gradient of the control flow. Run and analyze the result.

5.

Let f (x)

=

sin(x).

Plot f (x) and

df (x) dx

,

where

the

latter

is

computed

without

exploiting

that

f ′(x) = cos(x).

Discussions45
45 https://discuss.d2l.ai/t/34

2.5. Automatic Differentiation

75

2.6 Probability
In some form or another, machine learning is all about making predictions. We might want to predict the probability of a patient suffering a heart attack in the next year, given their clinical history. In anomaly detection, we might want to assess how likely a set of readings from an airplaneʼs jet engine would be, were it operating normally. In reinforcement learning, we want an agent to act intelligently in an environment. This means we need to think about the probability of getting a high reward under each of the available actions. And when we build recommender systems we also need to think about probability. For example, say hypothetically that we worked for a large online bookseller. We might want to estimate the probability that a particular user would buy a particular book. For this we need to use the language of probability. Entire courses, majors, theses, careers, and even departments, are devoted to probability. So naturally, our goal in this section is not to teach the whole subject. Instead we hope to get you off the ground, to teach you just enough that you can start building your first deep learning models, and to give you enough of a flavor for the subject that you can begin to explore it on your own if you wish.
We have already invoked probabilities in previous sections without articulating what precisely they are or giving a concrete example. Let us get more serious now by considering the first case: distinguishing cats and dogs based on photographs. This might sound simple but it is actually a formidable challenge. To start with, the difficulty of the problem may depend on the resolution of the image.

Fig. 2.6.1: Images of varying resolutions (10 × 10, 20 × 20, 40 × 40, 80 × 80, and 160 × 160 pixels).

As shown in Fig. 2.6.1, while it is easy for humans to recognize cats and dogs at the resolution of 160 × 160 pixels, it becomes challenging at 40 × 40 pixels and next to impossible at 10 × 10 pixels. In other words, our ability to tell cats and dogs apart at a large distance (and thus low resolution) might approach uninformed guessing. Probability gives us a formal way of reasoning about our level of certainty. If we are completely sure that the image depicts a cat, we say that the probability that the corresponding label y is “cat”, denoted P (y = “cat”) equals 1. If we had no evidence to suggest that y = “cat” or that y = “dog”, then we might say that the two possibilities were equally likely expressing this as P (y = “cat”) = P (y = “dog”) = 0.5. If we were reasonably confident, but not sure that the image depicted a cat, we might assign a probability 0.5 < P (y = “cat”) < 1.

76

Chapter 2. Preliminaries

Now consider the second case: given some weather monitoring data, we want to predict the probability that it will rain in Taipei tomorrow. If it is summertime, the rain might come with probability 0.5.
In both cases, we have some value of interest. And in both cases we are uncertain about the outcome. But there is a key difference between the two cases. In this first case, the image is in fact either a dog or a cat, and we just do not know which. In the second case, the outcome may actually be a random event, if you believe in such things (and most physicists do). So probability is a flexible language for reasoning about our level of certainty, and it can be applied effectively in a broad set of contexts.

2.6.1 Basic Probability Theory
Say that we cast a die and want to know what the chance is of seeing a 1 rather than another digit. If the die is fair, all the six outcomes {1, . . . , 6} are equally likely to occur, and thus we would see a 1 in one out of six cases. Formally we state that 1 occurs with probability 16 . For a real die that we receive from a factory, we might not know those proportions and we would need to check whether it is tainted. The only way to investigate the die is by casting it many times and recording the outcomes. For each cast of the die, we will observe a value in {1, . . . , 6}. Given these outcomes, we want to investigate the probability of observing each outcome.
One natural approach for each value is to take the individual count for that value and to divide it by the total number of tosses. This gives us an estimate of the probability of a given event. The law of large numbers tell us that as the number of tosses grows this estimate will draw closer and closer to the true underlying probability. Before going into the details of what is going here, let us try it out.
To start, let us import the necessary packages.
%matplotlib inline import random from mxnet import np, npx from d2l import mxnet as d2l
npx.set_np()
Next, we will want to be able to cast the die. In statistics we call this process of drawing examples from probability distributions sampling. The distribution that assigns probabilities to a number of discrete choices is called the multinomial distribution. We will give a more formal definition of distribution later, but at a high level, think of it as just an assignment of probabilities to events.
To draw a single sample, we simply pass in a vector of probabilities. The output is another vector of the same length: its value at index i is the number of times the sampling outcome corresponds to i.
fair_probs = [1.0 / 6] * 6 np.random.multinomial(1, fair_probs)

array([0, 0, 0, 1, 0, 0], dtype=int64)
If you run the sampler a bunch of times, you will find that you get out random values each time. As with estimating the fairness of a die, we often want to generate many samples from the same

2.6. Probability

77

distribution. It would be unbearably slow to do this with a Python for loop, so the function we are using supports drawing multiple samples at once, returning an array of independent samples in any shape we might desire.
np.random.multinomial(10, fair_probs)
array([1, 1, 5, 1, 1, 1], dtype=int64)
Now that we know how to sample rolls of a die, we can simulate 1000 rolls. We can then go through and count, after each of the 1000 rolls, how many times each number was rolled. Specifically, we calculate the relative frequency as the estimate of the true probability.
counts = np.random.multinomial(1000, fair_probs).astype(np.float32) counts / 1000
array([0.162, 0.149, 0.178, 0.17 , 0.166, 0.175])
Because we generated the data from a fair die, we know that each outcome has true probability 16 , roughly 0.167, so the above output estimates look good.
We can also visualize how these probabilities converge over time towards the true probability. Let us conduct 500 groups of experiments where each group draws 10 samples.
counts = np.random.multinomial(10, fair_probs, size=500) cum_counts = counts.astype(np.float32).cumsum(axis=0) estimates = cum_counts / cum_counts.sum(axis=1, keepdims=True)
d2l.set_figsize((6, 4.5)) for i in range(6):
d2l.plt.plot(estimates[:, i].asnumpy(), label=("P(die=" + str(i + 1) + ")"))
d2l.plt.axhline(y=0.167, color='black', linestyle='dashed') d2l.plt.gca().set_xlabel('Groups of experiments') d2l.plt.gca().set_ylabel('Estimated probability') d2l.plt.legend();

78

Chapter 2. Preliminaries

Each solid curve corresponds to one of the six values of the die and gives our estimated probability that the die turns up that value as assessed after each group of experiments. The dashed black line gives the true underlying probability. As we get more data by conducting more experiments, the 6 solid curves converge towards the true probability.

Axioms of Probability Theory

When dealing with the rolls of a die, we call the set S = {1, 2, 3, 4, 5, 6} the sample space or outcome space, where each element is an outcome. An event is a set of outcomes from a given sample space. For instance, “seeing a 5” ({5}) and “seeing an odd number” ({1, 3, 5}) are both valid events of rolling a die. Note that if the outcome of a random experiment is in event A, then event A has occurred. That is to say, if 3 dots faced up after rolling a die, since 3 ∈ {1, 3, 5}, we can say that the event “seeing an odd number” has occurred.

Formally, probability can be thought of as a function that maps a set to a real value. The probability of an event A in the given sample space S, denoted as P (A), satisfies the following properties:

• For any event A, its probability is never negative, i.e., P (A) ≥ 0;

• Probability of the entire sample space is 1, i.e., P (S) = 1;

• For any countable sequence of events A1, A2, . . . that are mutually exclusive (Ai∩Aj = ∅ for all

i ̸= j), t∪he probabil∑ity that any happens is equal to the sum of their individual probabilities,

i.e., P (

∞ i=1

Ai)

=

∞ i=1

P

(Ai

).

These are also the axioms of probability theory, proposed by Kolmogorov in 1933. Thanks to this axiom system, we can avoid any philosophical dispute on randomness; instead, we can reason rigorously with a mathematical language. For instance, by letting event A1 be the entire sample space and Ai = ∅ for all i > 1, we can prove that P (∅) = 0, i.e., the probability of an impossible event is 0.

2.6. Probability

79

Random Variables
In our random experiment of casting a die, we introduced the notion of a random variable. A random variable can be pretty much any quantity and is not deterministic. It could take one value among a set of possibilities in a random experiment. Consider a random variable X whose value is in the sample space S = {1, 2, 3, 4, 5, 6} of rolling a die. We can denote the event “seeing a 5” as {X = 5} or X = 5, and its probability as P ({X = 5}) or P (X = 5). By P (X = a), we make a distinction between the random variable X and the values (e.g., a) that X can take. However, such pedantry results in a cumbersome notation. For a compact notation, on one hand, we can just denote P (X) as the distribution over the random variable X: the distribution tells us the probability that X takes any value. On the other hand, we can simply write P (a) to denote the probability that a random variable takes the value a. Since an event in probability theory is a set of outcomes from the sample space, we can specify a range of values for a random variable to take. For example, P (1 ≤ X ≤ 3) denotes the probability of the event {1 ≤ X ≤ 3}, which means {X = 1, 2, or, 3}. Equivalently, P (1 ≤ X ≤ 3) represents the probability that the random variable X can take a value from {1, 2, 3}.
Note that there is a subtle difference between discrete random variables, like the sides of a die, and continuous ones, like the weight and the height of a person. There is little point in asking whether two people have exactly the same height. If we take precise enough measurements you will find that no two people on the planet have the exact same height. In fact, if we take a fine enough measurement, you will not have the same height when you wake up and when you go to sleep. So there is no purpose in asking about the probability that someone is 1.80139278291028719210196740527486202 meters tall. Given the world population of humans the probability is virtually 0. It makes more sense in this case to ask whether someoneʼs height falls into a given interval, say between 1.79 and 1.81 meters. In these cases we quantify the likelihood that we see a value as a density. The height of exactly 1.80 meters has no probability, but nonzero density. In the interval between any two different heights we have nonzero probability. In the rest of this section, we consider probability in discrete space. For probability over continuous random variables, you may refer to sec_random_variables.
2.6.2 Dealing with Multiple Random Variables
Very often, we will want to consider more than one random variable at a time. For instance, we may want to model the relationship between diseases and symptoms. Given a disease and a symptom, say “flu” and “cough”, either may or may not occur in a patient with some probability. While we hope that the probability of both would be close to zero, we may want to estimate these probabilities and their relationships to each other so that we may apply our inferences to effect better medical care.
As a more complicated example, images contain millions of pixels, thus millions of random variables. And in many cases images will come with a label, identifying objects in the image. We can also think of the label as a random variable. We can even think of all the metadata as random variables such as location, time, aperture, focal length, ISO, focus distance, and camera type. All of these are random variables that occur jointly. When we deal with multiple random variables, there are several quantities of interest.

80

Chapter 2. Preliminaries

Joint Probability
The first is called the joint probability P (A = a, B = b). Given any values a and b, the joint probability lets us answer, what is the probability that A = a and B = b simultaneously? Note that for any values a and b, P (A = a, B = b) ≤ P (A = a). This has to be the case, since for A = a and B = b to happen, A = a has to happen and B = b also has to happen (and vice versa). Thus, A = a and B = b cannot be more likely than A = a or B = b individually.

Conditional Probability
This brings us to an interesting ratio: 0 ≤ P (PA=(Aa=,Ba=) b) ≤ 1. We call this ratio a conditional probability and denote it by P (B = b | A = a): it is the probability of B = b, provided that A = a has occurred.

Bayes’ theorem

Using the definition of conditional probabilities, we can derive one of the most useful and celebrated equations in statistics: Bayes’ theorem. It goes as follows. By construction, we have the multiplication rule that P (A, B) = P (B | A)P (A). By symmetry, this also holds for P (A, B) = P (A | B)P (B). Assume that P (B) > 0. Solving for one of the conditional variables we get

P (A | B) = P (B | A)P (A) . P (B)

(2.6.1)

Note that here we use the more compact notation where P (A, B) is a joint distribution and P (A | B) is a conditional distribution. Such distributions can be evaluated for particular values A = a, B = b.

Marginalization

Bayesʼ theorem is very useful if we want to infer one thing from the other, say cause and effect,

but we only know the properties in the reverse direction, as we will see later in this section. One

important operation that we need, to make this work, is marginalization. It is the operation of

determining P (B) from P (A, B). We can see that the probability of B amounts to accounting for

all possible choices of A and aggregating the joint probabilities over all of them:

∑ P (B) = P (A, B),
A

(2.6.2)

which is also known as the sum rule. The probability or distribution as a result of marginalization is called a marginal probability or a marginal distribution.

Independence
Another useful property to check for is dependence vs. independence. Two random variables A and B being independent means that the occurrence of one event of A does not reveal any information about the occurrence of an event of B. In this case P (B | A) = P (B). Statisticians typically express this as A ⊥ B. From Bayesʼ theorem, it follows immediately that also P (A | B) = P (A). In all the other cases we call A and B dependent. For instance, two successive rolls of a die are independent. In contrast, the position of a light switch and the brightness in the room are not

2.6. Probability

81

(they are not perfectly deterministic, though, since we could always have a broken light bulb, power failure, or a broken switch).
Since P (A | B) = PP(A(B,B) ) = P (A) is equivalent to P (A, B) = P (A)P (B), two random variables are independent if and only if their joint distribution is the product of their individual distributions. Likewise, two random variables A and B are conditionally independent given another random variable C if and only if P (A, B | C) = P (A | C)P (B | C). This is expressed as A ⊥ B | C.

Application

Let us put our skills to the test. Assume that a doctor administers an HIV test to a patient. This test is fairly accurate and it fails only with 1% probability if the patient is healthy but reporting him as diseased. Moreover, it never fails to detect HIV if the patient actually has it. We use D1 to indicate the diagnosis (1 if positive and 0 if negative) and H to denote the HIV status (1 if positive and 0 if negative). Table 2.6.1 lists such conditional probabilities.

Table 2.6.1: Conditional probability of P (D1 | H).

Conditional probability H = 1 H = 0

P (D1 = 1 | H)

1

0.01

P (D1 = 0 | H)

0

0.99

Note that the column sums are all 1 (but the row sums are not), since the conditional probability needs to sum up to 1, just like the probability. Let us work out the probability of the patient having HIV if the test comes back positive, i.e., P (H = 1 | D1 = 1). Obviously this is going to depend on how common the disease is, since it affects the number of false alarms. Assume that the population is quite healthy, e.g., P (H = 1) = 0.0015. To apply Bayesʼ theorem, we need to apply marginalization and the multiplication rule to determine

P (D1 = 1) =P (D1 = 1, H = 0) + P (D1 = 1, H = 1) =P (D1 = 1 | H = 0)P (H = 0) + P (D1 = 1 | H = 1)P (H = 1) =0.011485.

(2.6.3)

Thus, we get

P (H = 1 | D1 = 1) = P (D1 = 1 | H = 1)P (H = 1) .
P (D1 = 1) =0.1306

(2.6.4)

In other words, there is only a 13.06% chance that the patient actually has HIV, despite using a very accurate test. As we can see, probability can be counterintuitive.

What should a patient do upon receiving such terrifying news? Likely, the patient would ask the physician to administer another test to get clarity. The second test has different characteristics and it is not as good as the first one, as shown in Table 2.6.2.

Table 2.6.2: Conditional probability of P (D2 | H).

Conditional probability H = 1 H = 0

P (D2 = 1 | H)

0.98 0.03

P (D2 = 0 | H)

0.02 0.97

82

Chapter 2. Preliminaries

Unfortunately, the second test comes back positive, too. Let us work out the requisite probabilities to invoke Bayesʼ theorem by assuming the conditional independence:

P (D1 = 1, D2 = 1 | H = 0) =P (D1 = 1 | H = 0)P (D2 = 1 | H = 0) =0.0003,

(2.6.5)

P (D1 = 1, D2 = 1 | H = 1) =P (D1 = 1 | H = 1)P (D2 = 1 | H = 1) =0.98. Now we can apply marginalization and the multiplication rule:
P (D1 = 1, D2 = 1) =P (D1 = 1, D2 = 1, H = 0) + P (D1 = 1, D2 = 1, H = 1) =P (D1 = 1, D2 = 1 | H = 0)P (H = 0) + P (D1 = 1, D2 = 1 | H = 1)P (H = 1) =0.00176955.

(2.6.6) (2.6.7)

In the end, the probability of the patient having HIV given both positive tests is
P (H = 1 | D1 = 1, D2 = 1) = P (D1 = 1, D2 = 1 | H = 1)P (H = 1)
P (D1 = 1, D2 = 1) =0.8307.

(2.6.8)

That is, the second test allowed us to gain much higher confidence that not all is well. Despite the second test being considerably less accurate than the first one, it still significantly improved our estimate.

2.6.3 Expectation and Variance

To summarize key characteristics of probability distributions, we need some measures. The ex-

pectation (or average) of the random variable X is denoted as

∑ E[X] = xP (X = x).
x

(2.6.9)

When the input of a function f (x) is a random variable drawn from the distribution P with differ-

ent values x, the expectation of f (x) is computed as

∑ Ex∼P [f (x)] = f (x)P (x).
x

(2.6.10)

In many cases we want to measure by how much the random variable X deviates from its expec-

tation. This can be quantified by the variance

Var[X ]

=

E

[ (X

−

E [X ])2 ]

=

E [X 2 ]

−

E [X ]2 .

(2.6.11)

Its square root is called the standard deviation. The variance of a function of a random variable

measures by how much the function deviates from the expectation of the function, as different

values x of the random variable are sampled from its distribution:

[

]

Var[f (x)] = E (f (x) − E[f (x)])2 .

(2.6.12)

2.6. Probability

83

Summary
• We can sample from probability distributions. • We can analyze multiple random variables using joint distribution, conditional distribution,
Bayesʼ theorem, marginalization, and independence assumptions. • Expectation and variance offer useful measures to summarize key characteristics of proba-
bility distributions.
Exercises
1. We conducted m = 500 groups of experiments where each group draws n = 10 samples. Vary m and n. Observe and analyze the experimental results.
2. Given two events with probability P (A) and P (B), compute upper and lower bounds on P (A ∪ B) and P (A ∩ B). (Hint: display the situation using a Venn Diagram46.)
3. Assume that we have a sequence of random variables, say A, B, and C, where B only depends on A, and C only depends on B, can you simplify the joint probability P (A, B, C)? (Hint: this is a Markov Chain47.)
4. In Section 2.6.2, the first test is more accurate. Why not run the first test twice rather than run both the first and second tests?
Discussions48

2.7 Documentation
Due to constraints on the length of this book, we cannot possibly introduce every single MXNet function and class (and you probably would not want us to). The API documentation and additional tutorials and examples provide plenty of documentation beyond the book. In this section we provide you with some guidance to exploring the MXNet API.

2.7.1 Finding All the Functions and Classes in a Module
In order to know which functions and classes can be called in a module, we invoke the dir function. For instance, we can query all properties in the module for generating random numbers: from mxnet import np print(dir(np.random))

['__all__', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', → '__package__', '__spec__', '_mx_nd_np', 'beta', 'chisquare', 'choice', 'exponential', →'gamma', 'gumbel', 'logistic', 'lognormal', 'multinomial', 'multivariate_normal', 'normal', → 'pareto', 'power', 'rand', 'randint', 'randn', 'rayleigh', 'shuffle', 'uniform', 'weibull →']
46 https://en.wikipedia.org/wiki/Venn_diagram 47 https://en.wikipedia.org/wiki/Markov_chain 48 https://discuss.d2l.ai/t/36

84

Chapter 2. Preliminaries

Generally, we can ignore functions that start and end with __ (special objects in Python) or functions that start with a single _(usually internal functions). Based on the remaining function or attribute names, we might hazard a guess that this module offers various methods for generating random numbers, including sampling from the uniform distribution (uniform), normal distribution (normal), and multinomial distribution (multinomial).
2.7.2 Finding the Usage of Specific Functions and Classes
For more specific instructions on how to use a given function or class, we can invoke the help function. As an example, let us explore the usage instructions for tensorsʼ ones function.
help(np.ones)
Help on function ones in module mxnet.numpy:
ones(shape, dtype=<class 'numpy.float32'>, order='C', ctx=None) Return a new array of given shape and type, filled with ones. This function currently only supports storing multi-dimensional data in row-major (C-style).
Parameters ---------shape : int or tuple of int
The shape of the empty array. dtype : str or numpy.dtype, optional
An optional value type. Default is numpy.float32. Note that this behavior is different from NumPy's ones function where float64 is the default value, because float32 is considered as the default data type in deep learning. order : {'C'}, optional, default: 'C' How to store multi-dimensional data in memory, currently only row-major (C-style) is supported. ctx : Context, optional An optional device context (default is the current default context).
Returns ------out : ndarray
Array of ones with the given shape, dtype, and ctx.
Examples ------->>> np.ones(5) array([1., 1., 1., 1., 1.])
>>> np.ones((5,), dtype=int) array([1, 1, 1, 1, 1], dtype=int64)
>>> np.ones((2, 1)) array([[1.],

2.7. Documentation

85

[1.]])
>>> s = (2,2) >>> np.ones(s) array([[1., 1.],
[1., 1.]]) From the documentation, we can see that the ones function creates a new tensor with the specified shape and sets all the elements to the value of 1. Whenever possible, you should run a quick test to confirm your interpretation:
np.ones(4)
array([1., 1., 1., 1.])
In the Jupyter notebook, we can use ? to display the document in another window. For example, list? will create content that is almost identical to help(list), displaying it in a new browser window. In addition, if we use two question marks, such as list??, the Python code implementing the function will also be displayed.
Summary
• The official documentation provides plenty of descriptions and examples that are beyond this book.
• We can look up documentation for the usage of an API by calling the dir and help functions, or ? and ?? in Jupyter notebooks.
Exercises
1. Look up the documentation for any function or class in the deep learning framework. Can you also find the documentation on the official website of the framework?
Discussions49

49 https://discuss.d2l.ai/t/38
86

Chapter 2. Preliminaries

3 | Linear Neural Networks
Before we get into the details of deep neural networks, we need to cover the basics of neural network training. In this chapter, we will cover the entire training process, including defining simple neural network architectures, handling data, specifying a loss function, and training the model. In order to make things easier to grasp, we begin with the simplest concepts. Fortunately, classic statistical learning techniques such as linear and softmax regression can be cast as linear neural networks. Starting from these classic algorithms, we will introduce you to the basics, providing the basis for more complex techniques in the rest of the book.
3.1 Linear Regression
Regression refers to a set of methods for modeling the relationship between one or more independent variables and a dependent variable. In the natural sciences and social sciences, the purpose of regression is most often to characterize the relationship between the inputs and outputs. Machine learning, on the other hand, is most often concerned with prediction. Regression problems pop up whenever we want to predict a numerical value. Common examples include predicting prices (of homes, stocks, etc.), predicting length of stay (for patients in the hospital), demand forecasting (for retail sales), among countless others. Not every prediction problem is a classic regression problem. In subsequent sections, we will introduce classification problems, where the goal is to predict membership among a set of categories.
3.1.1 Basic Elements of Linear Regression
Linear regression may be both the simplest and most popular among the standard tools to regression. Dating back to the dawn of the 19th century, linear regression flows from a few simple assumptions. First, we assume that the relationship between the independent variables x and the dependent variable y is linear, i.e., that y can be expressed as a weighted sum of the elements in x, given some noise on the observations. Second, we assume that any noise is well-behaved (following a Gaussian distribution). To motivate the approach, let us start with a running example. Suppose that we wish to estimate the prices of houses (in dollars) based on their area (in square feet) and age (in years). To actually develop a model for predicting house prices, we would need to get our hands on a dataset consisting of sales for which we know the sale price, area, and age for each home. In the terminology of machine learning, the dataset is called a training dataset or training set, and each row (here the data corresponding to one sale) is called an example (or data point, data instance, sample). The thing we are trying to predict (price) is called a label (or target). The independent variables (age and area) upon which the predictions are based are called features (or covariates).
87

Typically, we will use n to denote the number of examples in our dataset. We index the data examples by i, denoting each input as x(i) = [x(1i), x(2i)]⊤ and the corresponding label as y(i).

Linear Model

The linearity assumption just says that the target (price) can be expressed as a weighted sum of the features (area and age):

price = warea · area + wage · age + b.

(3.1.1)

In (3.1.1), warea and wage are called weights, and b is called a bias (also called an offset or intercept). The weights determine the influence of each feature on our prediction and the bias just says what value the predicted price should take when all of the features take value 0. Even if we will never see any homes with zero area, or that are precisely zero years old, we still need the bias or else we will limit the expressivity of our model. Strictly speaking, (3.1.1) is an affine transformation of input features, which is characterized by a linear transformation of features via weighted sum, combined with a translation via the added bias.
Given a dataset, our goal is to choose the weights w and the bias b such that on average, the predictions made according to our model best fit the true prices observed in the data. Models whose output prediction is determined by the affine transformation of input features are linear models, where the affine transformation is specified by the chosen weights and bias.
In disciplines where it is common to focus on datasets with just a few features, explicitly expressing models long-form like this is common. In machine learning, we usually work with highdimensional datasets, so it is more convenient to employ linear algebra notation. When our inputs consist of d features, we express our prediction yˆ (in general the “hat” symbol denotes estimates) as

yˆ = w1x1 + ... + wdxd + b.

(3.1.2)

Collecting all features into a vector x ∈ Rd and all weights into a vector w ∈ Rd, we can express our model compactly using a dot product:

yˆ = w⊤x + b.

(3.1.3)

In (3.1.3), the vector x corresponds to features of a single data example. We will often find it convenient to refer to features of our entire dataset of n examples via the design matrix X ∈ Rn×d. Here, X contains one row for every example and one column for every feature.
For a collection of features X, the predictions yˆ ∈ Rn can be expressed via the matrix-vector product:

yˆ = Xw + b,

(3.1.4)

where broadcasting (see Section 2.1.3) is applied during the summation. Given features of a training dataset X and corresponding (known) labels y, the goal of linear regression is to find the weight vector w and the bias term b that given features of a new data example sampled from the same distribution as X, the new exampleʼs label will (in expectation) be predicted with the lowest error.
Even if we believe that the best model for predicting y given x is linear, we would not expect to find a real-world dataset of n examples where y(i) exactly equals w⊤x(i) + b for all 1 ≤ i ≤ n. For example, whatever instruments we use to observe the features X and labels y might suffer small

88

Chapter 3. Linear Neural Networks

amount of measurement error. Thus, even when we are confident that the underlying relationship is linear, we will incorporate a noise term to account for such errors.
Before we can go about searching for the best parameters (or model parameters) w and b, we will need two more things: (i) a quality measure for some given model; and (ii) a procedure for updating the model to improve its quality.

Loss Function

Before we start thinking about how to fit data with our model, we need to determine a measure of fitness. The loss function quantifies the distance between the real and predicted value of the target. The loss will usually be a non-negative number where smaller values are better and perfect predictions incur a loss of 0. The most popular loss function in regression problems is the squared error. When our prediction for an example i is yˆ(i) and the corresponding true label is y(i), the squared error is given by:

l(i)(w, b) = 1 (yˆ(i) − y(i))2 . 2

(3.1.5)

The

constant

1 2

makes

no

real

difference

but

will

prove

notationally

convenient,

canceling

out

when we take the derivative of the loss. Since the training dataset is given to us, and thus out of

our control, the empirical error is only a function of the model parameters. To make things more

concrete, consider the example below where we plot a regression problem for a one-dimensional

case as shown in Fig. 3.1.1.

Fig. 3.1.1: Fit data with a linear model.

Note that large differences between estimates yˆ(i) and observations y(i) lead to even larger contributions to the loss, due to the quadratic dependence. To measure the quality of a model on the entire dataset of n examples, we simply average (or equivalently, sum) the losses on the training set.

L(w, b) = 1 ∑n l(i)(w, b) = 1 ∑n 1 (w⊤x(i) + b − y(i))2 .

n

n2

i=1

i=1

(3.1.6)

When training the model, we want to find parameters (w∗, b∗) that minimize the total loss across all training examples:

w∗, b∗ = argmin L(w, b).
w,b

(3.1.7)

3.1. Linear Regression

89

Analytic Solution

Linear regression happens to be an unusually simple optimization problem. Unlike most other models that we will encounter in this book, linear regression can be solved analytically by applying a simple formula. To start, we can subsume the bias b into the parameter w by appending a column to the design matrix consisting of all ones. Then our prediction problem is to minimize ∥y−Xw∥2. There is just one critical point on the loss surface and it corresponds to the minimum of the loss over the entire domain. Taking the derivative of the loss with respect to w and setting it equal to zero yields the analytic (closed-form) solution:

w∗ = (X⊤X)−1X⊤y.

(3.1.8)

While simple problems like linear regression may admit analytic solutions, you should not get used to such good fortune. Although analytic solutions allow for nice mathematical analysis, the requirement of an analytic solution is so restrictive that it would exclude all of deep learning.

Minibatch Stochastic Gradient Descent

Even in cases where we cannot solve the models analytically, it turns out that we can still train models effectively in practice. Moreover, for many tasks, those difficult-to-optimize models turn out to be so much better that figuring out how to train them ends up being well worth the trouble.

The key technique for optimizing nearly any deep learning model, and which we will call upon throughout this book, consists of iteratively reducing the error by updating the parameters in the direction that incrementally lowers the loss function. This algorithm is called gradient descent.

The most naive application of gradient descent consists of taking the derivative of the loss function, which is an average of the losses computed on every single example in the dataset. In practice, this can be extremely slow: we must pass over the entire dataset before making a single update. Thus, we will often settle for sampling a random minibatch of examples every time we need to compute the update, a variant called minibatch stochastic gradient descent.

In each iteration, we first randomly sample a minibatch B consisting of a fixed number of training examples. We then compute the derivative (gradient) of the average loss on the minibatch with regard to the model parameters. Finally, we multiply the gradient by a predetermined positive value η and subtract the resulting term from the current parameter values.

We can express the update mathematically as follows (∂ denotes the partial derivative): (w, b) ← (w, b) − |Bη | ∑ ∂(w,b)l(i)(w, b).
i∈B

(3.1.9)

To summarize, steps of the algorithm are the following: (i) we initialize the values of the model

parameters, typically at random; (ii) we iteratively sample random minibatches from the data,

updating the parameters in the direction of the negative gradient. For quadratic losses and affine

transformations, we can write this out explicitly as follows:

w ← w − η ∑ ∂wl(i)(w, b) = w − η ∑ x(i) (w⊤x(i) + b − y(i)) ,

|B|

|B|

i∈B
η∑

η ∑i∈(B

)

b←b−

∂bl(i)(w, b) = b −

w⊤x(i) + b − y(i) .

|B|

|B|

i∈B

i∈B

(3.1.10)

Note that w and x are vectors in (3.1.10). Here, the more elegant vector notation makes the math much more readable than expressing things in terms of coefficients, say w1, w2, . . . , wd. The set

90

Chapter 3. Linear Neural Networks

cardinality |B| represents the number of examples in each minibatch (the batch size) and η denotes the learning rate. We emphasize that the values of the batch size and learning rate are manually pre-specified and not typically learned through model training. These parameters that are tunable but not updated in the training loop are called hyperparameters. Hyperparameter tuning is the process by which hyperparameters are chosen, and typically requires that we adjust them based on the results of the training loop as assessed on a separate validation dataset (or validation set).
After training for some predetermined number of iterations (or until some other stopping criteria are met), we record the estimated model parameters, denoted wˆ , ˆb. Note that even if our function is truly linear and noiseless, these parameters will not be the exact minimizers of the loss because, although the algorithm converges slowly towards the minimizers it cannot achieve it exactly in a finite number of steps.
Linear regression happens to be a learning problem where there is only one minimum over the entire domain. However, for more complicated models, like deep networks, the loss surfaces contain many minima. Fortunately, for reasons that are not yet fully understood, deep learning practitioners seldom struggle to find parameters that minimize the loss on training sets. The more formidable task is to find parameters that will achieve low loss on data that we have not seen before, a challenge called generalization. We return to these topics throughout the book.

Making Predictions with the Learned Model
Given the learned linear regression model wˆ ⊤x + ˆb, we can now estimate the price of a new house (not contained in the training data) given its area x1 and age x2. Estimating targets given features is commonly called prediction or inference.
We will try to stick with prediction because calling this step inference, despite emerging as standard jargon in deep learning, is somewhat of a misnomer. In statistics, inference more often denotes estimating parameters based on a dataset. This misuse of terminology is a common source of confusion when deep learning practitioners talk to statisticians.

3.1.2 Vectorization for Speed
When training our models, we typically want to process whole minibatches of examples simultaneously. Doing this efficiently requires that we vectorize the calculations and leverage fast linear algebra libraries rather than writing costly for-loops in Python.
%matplotlib inline import math import time from mxnet import np from d2l import mxnet as d2l

To illustrate why this matters so much, we can consider two methods for adding vectors. To start we instantiate two 10000-dimensional vectors containing all ones. In one method we will loop over the vectors with a Python for-loop. In the other method we will rely on a single call to +.
n = 10000 a = np.ones(n) b = np.ones(n)

Since we will benchmark the running time frequently in this book, let us define a timer.

3.1. Linear Regression

91

class Timer: #@save """Record multiple running times.""" def __init__(self): self.times = [] self.start()
def start(self): """Start the timer.""" self.tik = time.time()
def stop(self): """Stop the timer and record the time in a list.""" self.times.append(time.time() - self.tik) return self.times[-1]
def avg(self): """Return the average time.""" return sum(self.times) / len(self.times)
def sum(self): """Return the sum of time.""" return sum(self.times)
def cumsum(self): """Return the accumulated time.""" return np.array(self.times).cumsum().tolist()
Now we can benchmark the workloads. First, we add them, one coordinate at a time, using a for-loop.
c = np.zeros(n) timer = Timer() for i in range(n):
c[i] = a[i] + b[i] f'{timer.stop():.5f} sec'
'4.49220 sec'
Alternatively, we rely on the reloaded + operator to compute the elementwise sum.
timer.start() d=a+b f'{timer.stop():.5f} sec'
'0.00049 sec'
You probably noticed that the second method is dramatically faster than the first. Vectorizing code often yields order-of-magnitude speedups. Moreover, we push more of the mathematics to the library and need not write as many calculations ourselves, reducing the potential for errors.

92

Chapter 3. Linear Neural Networks

3.1.3 The Normal Distribution and Squared Loss

While you can already get your hands dirty using only the information above, in the following we can more formally motivate the squared loss objective via assumptions about the distribution of noise.

Linear regression was invented by Gauss in 1795, who also discovered the normal distribution

(also called the Gaussian). It turns out that the connection between the normal distribution and

linear regression runs deeper than common parentage. To refresh your memory, the probability

density of a normal distribution with mean µ and variance σ2 (standard deviation σ) is given as

(

)

p(x) = √ 1 exp − 1 (x − µ)2 .

2πσ2

2σ2

(3.1.11)

Below we define a Python function to compute the normal distribution.

def normal(x, mu, sigma): p = 1 / math.sqrt(2 * math.pi * sigma**2) return p * np.exp(-0.5 / sigma**2 * (x - mu)**2)

We can now visualize the normal distributions.
# Use numpy again for visualization x = np.arange(-7, 7, 0.01)
# Mean and standard deviation pairs params = [(0, 1), (0, 2), (3, 1)] d2l.plot(x, [normal(x, mu, sigma) for mu, sigma in params], xlabel='x',
ylabel='p(x)', figsize=(4.5, 2.5), legend=[f'mean {mu}, std {sigma}' for mu, sigma in params])

As we can see, changing the mean corresponds to a shift along the x-axis, and increasing the variance spreads the distribution out, lowering its peak.
One way to motivate linear regression with the mean squared error loss function (or simply squared loss) is to formally assume that observations arise from noisy observations, where the noise is normally distributed as follows:

y = w⊤x + b + ϵ where ϵ ∼ N (0, σ2).

(3.1.12)

3.1. Linear Regression

93

Thus, we can now write out the likelihood of seeing a particular y for a given x via

(

)

P (y | x) = √ 1 exp − 1 (y − w⊤x − b)2 .

2πσ2

2σ2

(3.1.13)

Now, according to the principle of maximum likelihood, the best values of parameters w and b are those that maximize the likelihood of the entire dataset:

∏n P (y | X) = p(y(i)|x(i)).

(3.1.14)

i=1

Estimators chosen according to the principle of maximum likelihood are called maximum likelihood estimators. While, maximizing the product of many exponential functions, might look difficult, we can simplify things significantly, without changing the objective, by maximizing the log of the likelihood instead. For historical reasons, optimizations are more often expressed as minimization rather than maximization. So, without changing anything we can minimize the negative log-likelihood − log P (y | X). Working out the mathematics gives us:

− log P (y | X) = ∑n 1 log(2πσ2) + 1 (y(i) − w⊤x(i) − b)2 .

2

2σ2

i=1

(3.1.15)

Now we just need one more assumption that σ is some fixed constant. Thus we can ignore the first

term because it does not depend on w or b. Now the second term is identical to the squared error

loss

introduced

earlier,

except

for

the

multiplicative

constant

1 σ2

.

Fortunately,

the

solution

does

not depend on σ. It follows that minimizing the mean squared error is equivalent to maximum

likelihood estimation of a linear model under the assumption of additive Gaussian noise.

3.1.4 From Linear Regression to Deep Networks
So far we only talked about linear models. While neural networks cover a much richer family of models, we can begin thinking of the linear model as a neural network by expressing it in the language of neural networks. To begin, let us start by rewriting things in a “layer” notation.

Neural Network Diagram
Deep learning practitioners like to draw diagrams to visualize what is happening in their models. In Fig. 3.1.2, we depict our linear regression model as a neural network. Note that these diagrams highlight the connectivity pattern such as how each input is connected to the output, but not the values taken by the weights or biases.

Fig. 3.1.2: Linear regression is a single-layer neural network.

For the neural network shown in Fig. 3.1.2, the inputs are x1, . . . , xd, so the number of inputs (or feature dimensionality) in the input layer is d. The output of the network in Fig. 3.1.2 is o1, so the

94

Chapter 3. Linear Neural Networks

number of outputs in the output layer is 1. Note that the input values are all given and there is just a single computed neuron. Focusing on where computation takes place, conventionally we do not consider the input layer when counting layers. That is to say, the number of layers for the neural network in Fig. 3.1.2 is 1. We can think of linear regression models as neural networks consisting of just a single artificial neuron, or as single-layer neural networks.
Since for linear regression, every input is connected to every output (in this case there is only one output), we can regard this transformation (the output layer in Fig. 3.1.2) as a fully-connected layer or dense layer. We will talk a lot more about networks composed of such layers in the next chapter.

Biology

Since linear regression (invented in 1795) predates computational neuroscience, it might seem anachronistic to describe linear regression as a neural network. To see why linear models were a natural place to begin when the cyberneticists/neurophysiologists Warren McCulloch and Walter Pitts began to develop models of artificial neurons, consider the cartoonish picture of a biological neuron in Fig. 3.1.3, consisting of dendrites (input terminals), the nucleus (CPU), the axon (output wire), and the axon terminals (output terminals), enabling connections to other neurons via synapses.

Dendrite

Cell body

Node of Ranvier

Axon Terminal

Nucleus

Axon

Schwann cell Myelin sheath

Fig. 3.1.3: The real neuron.

Information xi arriving from other neurons (or environmental sensors such as the retina) is received in the dendrites. In particular, that information is weighted by synaptic weights wi determining the effect of the inputs (e.g., activation or inhibition via the product xiwi). The weig∑hted inputs arriving from multiple sources are aggregated in the nucleus as a weighted sum y = i xiwi + b, and this information is then sent for further processing in the axon y, typically after some nonlinear processing via σ(y). From there it either reaches its destination (e.g., a muscle) or is fed into another neuron via its dendrites.
Certainly, the high-level idea that many such units could be cobbled together with the right connectivity and right learning algorithm, to produce far more interesting and complex behavior than any one neuron alone could express owes to our study of real biological neural systems.
At the same time, most research in deep learning today draws little direct inspiration in neuroscience. We invoke Stuart Russell and Peter Norvig who, in their classic AI text book Artificial Intelligence: A Modern Approach (Russell & Norvig, 2016), pointed out that although airplanes might have been inspired by birds, ornithology has not been the primary driver of aeronautics innovation

3.1. Linear Regression

95

for some centuries. Likewise, inspiration in deep learning these days comes in equal or greater measure from mathematics, statistics, and computer science.

Summary
• Key ingredients in a machine learning model are training data, a loss function, an optimization algorithm, and quite obviously, the model itself.
• Vectorizing makes everything better (mostly math) and faster (mostly code). • Minimizing an objective function and performing maximum likelihood estimation can
mean the same thing. • Linear regression models are neural networks, too.

Exercises

1. A∑ssume that we have some data x1, . . . , xn ∈ R. Our goal is to find a constant b such that i(xi − b)2 is minimized.

1. Find a analytic solution for the optimal value of b.

2. How does this problem and its solution relate to the normal distribution?

2. Derive the analytic solution to the optimization problem for linear regression with squared error. To keep things simple, you can omit the bias b from the problem (we can do this in principled fashion by adding one column to X consisting of all ones).

1. Write out the optimization problem in matrix and vector notation (treat all the data as a single matrix, and all the target values as a single vector).

2. Compute the gradient of the loss with respect to w.

3. Find the analytic solution by setting the gradient equal to zero and solving the matrix equation.

4. When might this be better than using stochastic gradient descent? When might this method break?

3. Assume that the noise model governing the additive noise ϵ is the exponential distribution.

That

is,

p(ϵ)

=

1 2

exp(−|ϵ|).

1. Write out the negative log-likelihood of the data under the model − log P (y | X).

2. Can you find a closed form solution?

3. Suggest a stochastic gradient descent algorithm to solve this problem. What could possibly go wrong (hint: what happens near the stationary point as we keep on updating the parameters)? Can you fix this?

Discussions50
50 https://discuss.d2l.ai/t/40

96

Chapter 3. Linear Neural Networks

3.2 Linear Regression Implementation from Scratch
Now that you understand the key ideas behind linear regression, we can begin to work through a hands-on implementation in code. In this section, we will implement the entire method from scratch, including the data pipeline, the model, the loss function, and the minibatch stochastic gradient descent optimizer. While modern deep learning frameworks can automate nearly all of this work, implementing things from scratch is the only way to make sure that you really know what you are doing. Moreover, when it comes time to customize models, defining our own layers or loss functions, understanding how things work under the hood will prove handy. In this section, we will rely only on tensors and auto differentiation. Afterwards, we will introduce a more concise implementation, taking advantage of bells and whistles of deep learning frameworks.
%matplotlib inline import random from mxnet import autograd, np, npx from d2l import mxnet as d2l
npx.set_np()

3.2.1 Generating the Dataset

To keep things simple, we will construct an artificial dataset according to a linear model with additive noise. Our task will be to recover this modelʼs parameters using the finite set of examples contained in our dataset. We will keep the data low-dimensional so we can visualize it easily. In the following code snippet, we generate a dataset containing 1000 examples, each consisting of 2 features sampled from a standard normal distribution. Thus our synthetic dataset will be a matrix X ∈ R1000×2.
The true parameters generating our dataset will be w = [2, −3.4]⊤ and b = 4.2, and our synthetic labels will be assigned according to the following linear model with the noise term ϵ:

y = Xw + b + ϵ.

(3.2.1)

You could think of ϵ as capturing potential measurement errors on the features and labels. We will assume that the standard assumptions hold and thus that ϵ obeys a normal distribution with mean of 0. To make our problem easy, we will set its standard deviation to 0.01. The following code generates our synthetic dataset.
def synthetic_data(w, b, num_examples): #@save """Generate y = Xw + b + noise.""" X = np.random.normal(0, 1, (num_examples, len(w))) y = np.dot(X, w) + b y += np.random.normal(0, 0.01, y.shape) return X, y.reshape((-1, 1))

true_w = np.array([2, -3.4]) true_b = 4.2 features, labels = synthetic_data(true_w, true_b, 1000)
Note that each row in features consists of a 2-dimensional data example and that each row in labels consists of a 1-dimensional label value (a scalar).

3.2. Linear Regression Implementation from Scratch

97

print('features:', features[0], '\nlabel:', labels[0])
features: [2.2122064 1.1630787] label: [4.662078]
By generating a scatter plot using the second feature features[:, 1] and labels, we can clearly observe the linear correlation between the two.
d2l.set_figsize() # The semicolon is for displaying the plot only d2l.plt.scatter(features[:, (1)].asnumpy(), labels.asnumpy(), 1);

3.2.2 Reading the Dataset
Recall that training models consists of making multiple passes over the dataset, grabbing one minibatch of examples at a time, and using them to update our model. Since this process is so fundamental to training machine learning algorithms, it is worth defining a utility function to shuffle the dataset and access it in minibatches.
In the following code, we define the data_iter function to demonstrate one possible implementation of this functionality. The function takes a batch size, a matrix of features, and a vector of labels, yielding minibatches of the size batch_size. Each minibatch consists of a tuple of features and labels.
def data_iter(batch_size, features, labels): num_examples = len(features) indices = list(range(num_examples)) # The examples are read at random, in no particular order random.shuffle(indices) for i in range(0, num_examples, batch_size): batch_indices = np.array(indices[i:min(i + batch_size, num_examples)]) yield features[batch_indices], labels[batch_indices]
In general, note that we want to use reasonably sized minibatches to take advantage of the GPU hardware, which excels at parallelizing operations. Because each example can be fed through our models in parallel and the gradient of the loss function for each example can also be taken in

98

Chapter 3. Linear Neural Networks

parallel, GPUs allow us to process hundreds of examples in scarcely more time than it might take to process just a single example.
To build some intuition, let us read and print the first small batch of data examples. The shape of the features in each minibatch tells us both the minibatch size and the number of input features. Likewise, our minibatch of labels will have a shape given by batch_size.
batch_size = 10
for X, y in data_iter(batch_size, features, labels): print(X, '\n', y) break
[[-1.4548141 -0.02985478] [ 0.5586312 -0.12628113] [-0.52308416 -1.1180092 ] [-1.4760977 -0.18857454] [ 0.0901759 0.27758422] [-0.34835348 0.929739 ] [-0.18156302 0.19848816] [-0.1894692 1.2885122 ] [-0.01811524 -0.2241009 ] [-0.96688974 0.6003209 ]] [[ 1.4094979 ] [ 5.7577643 ] [ 6.9662256 ] [ 1.8882912 ] [ 3.427059 ] [ 0.33858034] [ 3.1565459 ] [-0.5663205 ] [ 4.915421 ] [ 0.2236586 ]]
As we run the iteration, we obtain distinct minibatches successively until the entire dataset has been exhausted (try this). While the iteration implemented above is good for didactic purposes, it is inefficient in ways that might get us in trouble on real problems. For example, it requires that we load all the data in memory and that we perform lots of random memory access. The built-in iterators implemented in a deep learning framework are considerably more efficient and they can deal with both data stored in files and data fed via data streams.
3.2.3 Initializing Model Parameters
Before we can begin optimizing our modelʼs parameters by minibatch stochastic gradient descent, we need to have some parameters in the first place. In the following code, we initialize weights by sampling random numbers from a normal distribution with mean 0 and a standard deviation of 0.01, and setting the bias to 0.
w = np.random.normal(0, 0.01, (2, 1)) b = np.zeros(1) w.attach_grad() b.attach_grad()

3.2. Linear Regression Implementation from Scratch

99

After initializing our parameters, our next task is to update them until they fit our data sufficiently well. Each update requires taking the gradient of our loss function with respect to the parameters. Given this gradient, we can update each parameter in the direction that may reduce the loss.
Since nobody wants to compute gradients explicitly (this is tedious and error prone), we use automatic differentiation, as introduced in Section 2.5, to compute the gradient.
3.2.4 Defining the Model
Next, we must define our model, relating its inputs and parameters to its outputs. Recall that to calculate the output of the linear model, we simply take the matrix-vector dot product of the input features X and the model weights w, and add the offset b to each example. Note that below Xw is a vector and b is a scalar. Recall the broadcasting mechanism as described in Section 2.1.3. When we add a vector and a scalar, the scalar is added to each component of the vector.
def linreg(X, w, b): #@save """The linear regression model.""" return np.dot(X, w) + b

3.2.5 Defining the Loss Function
Since updating our model requires taking the gradient of our loss function, we ought to define the loss function first. Here we will use the squared loss function as described in Section 3.1. In the implementation, we need to transform the true value y into the predicted valueʼs shape y_hat. The result returned by the following function will also have the same shape as y_hat.
def squared_loss(y_hat, y): #@save """Squared loss.""" return (y_hat - y.reshape(y_hat.shape))**2 / 2

3.2.6 Defining the Optimization Algorithm
As we discussed in Section 3.1, linear regression has a closed-form solution. However, this is not a book about linear regression: it is a book about deep learning. Since none of the other models that this book introduces can be solved analytically, we will take this opportunity to introduce your first working example of minibatch stochastic gradient descent.
At each step, using one minibatch randomly drawn from our dataset, we will estimate the gradient of the loss with respect to our parameters. Next, we will update our parameters in the direction that may reduce the loss. The following code applies the minibatch stochastic gradient descent update, given a set of parameters, a learning rate, and a batch size. The size of the update step is determined by the learning rate lr. Because our loss is calculated as a sum over the minibatch of examples, we normalize our step size by the batch size (batch_size), so that the magnitude of a typical step size does not depend heavily on our choice of the batch size.
def sgd(params, lr, batch_size): #@save """Minibatch stochastic gradient descent.""" for param in params: param[:] = param - lr * param.grad / batch_size

100

Chapter 3. Linear Neural Networks

3.2.7 Training
Now that we have all of the parts in place, we are ready to implement the main training loop. It is crucial that you understand this code because you will see nearly identical training loops over and over again throughout your career in deep learning.
In each iteration, we will grab a minibatch of training examples, and pass them through our model to obtain a set of predictions. After calculating the loss, we initiate the backwards pass through the network, storing the gradients with respect to each parameter. Finally, we will call the optimization algorithm sgd to update the model parameters.
In summary, we will execute the following loop:
• Initialize parameters (w, b)
• Repeat until done – Compute gradient g ← ∂(w,b) |B1| ∑i∈B l(x(i), y(i), w, b)
– Update parameters (w, b) ← (w, b) − ηg
In each epoch, we will iterate through the entire dataset (using the data_iter function) once passing through every example in the training dataset (assuming that the number of examples is divisible by the batch size). The number of epochs num_epochs and the learning rate lr are both hyperparameters, which we set here to 3 and 0.03, respectively. Unfortunately, setting hyperparameters is tricky and requires some adjustment by trial and error. We elide these details for now but revise them later in Chapter 11.
lr = 0.03 num_epochs = 3 net = linreg loss = squared_loss
for epoch in range(num_epochs): for X, y in data_iter(batch_size, features, labels): with autograd.record(): l = loss(net(X, w, b), y) # Minibatch loss in `X` and `y` # Because `l` has a shape (`batch_size`, 1) and is not a scalar # variable, the elements in `l` are added together to obtain a new # variable, on which gradients with respect to [`w`, `b`] are computed l.backward() sgd([w, b], lr, batch_size) # Update parameters using their gradient train_l = loss(net(features, w, b), labels) print(f'epoch {epoch + 1}, loss {float(train_l.mean()):f}')
epoch 1, loss 0.024938 epoch 2, loss 0.000087 epoch 3, loss 0.000051
In this case, because we synthesized the dataset ourselves, we know precisely what the true parameters are. Thus, we can evaluate our success in training by comparing the true parameters with those that we learned through our training loop. Indeed they turn out to be very close to each other.

3.2. Linear Regression Implementation from Scratch

101

print(f'error in estimating w: {true_w - w.reshape(true_w.shape)}') print(f'error in estimating b: {true_b - b}')
error in estimating w: [0.00036132 0.00064158] error in estimating b: [0.00033903]
Note that we should not take it for granted that we are able to recover the parameters perfectly. However, in machine learning, we are typically less concerned with recovering true underlying parameters, and more concerned with parameters that lead to highly accurate prediction. Fortunately, even on difficult optimization problems, stochastic gradient descent can often find remarkably good solutions, owing partly to the fact that, for deep networks, there exist many configurations of the parameters that lead to highly accurate prediction.
Summary
• We saw how a deep network can be implemented and optimized from scratch, using just tensors and auto differentiation, without any need for defining layers or fancy optimizers.
• This section only scratches the surface of what is possible. In the following sections, we will describe additional models based on the concepts that we have just introduced and learn how to implement them more concisely.
Exercises
1. What would happen if we were to initialize the weights to zero. Would the algorithm still work?
2. Assume that you are Georg Simon Ohm51 trying to come up with a model between voltage and current. Can you use auto differentiation to learn the parameters of your model?
3. Can you use Planckʼs Law52 to determine the temperature of an object using spectral energy density?
4. What are the problems you might encounter if you wanted to compute the second derivatives? How would you fix them?
5. Why is the reshape function needed in the squared_loss function?
6. Experiment using different learning rates to find out how fast the loss function value drops.
7. If the number of examples cannot be divided by the batch size, what happens to the data_iter functionʼs behavior?
Discussions53
51 https://en.wikipedia.org/wiki/Georg_Ohm 52 https://en.wikipedia.org/wiki/Planck%27s_law 53 https://discuss.d2l.ai/t/42

102

Chapter 3. Linear Neural Networks

3.3 Concise Implementation of Linear Regression
Broad and intense interest in deep learning for the past several years has inspired companies, academics, and hobbyists to develop a variety of mature open source frameworks for automating the repetitive work of implementing gradient-based learning algorithms. In Section 3.2, we relied only on (i) tensors for data storage and linear algebra; and (ii) auto differentiation for calculating gradients. In practice, because data iterators, loss functions, optimizers, and neural network layers are so common, modern libraries implement these components for us as well. In this section, we will show you how to implement the linear regression model from Section 3.2 concisely by using high-level APIs of deep learning frameworks.
3.3.1 Generating the Dataset
To start, we will generate the same dataset as in Section 3.2.
from mxnet import autograd, gluon, np, npx from d2l import mxnet as d2l
npx.set_np()
true_w = np.array([2, -3.4]) true_b = 4.2 features, labels = d2l.synthetic_data(true_w, true_b, 1000)
3.3.2 Reading the Dataset
Rather than rolling our own iterator, we can call upon the existing API in a framework to read data. We pass in features and labels as arguments and specify batch_size when instantiating a data iterator object. Besides, the boolean value is_train indicates whether or not we want the data iterator object to shuffle the data on each epoch (pass through the dataset).
def load_array(data_arrays, batch_size, is_train=True): #@save """Construct a Gluon data iterator.""" dataset = gluon.data.ArrayDataset(*data_arrays) return gluon.data.DataLoader(dataset, batch_size, shuffle=is_train)
batch_size = 10 data_iter = load_array((features, labels), batch_size)
Now we can use data_iter in much the same way as we called the data_iter function in Section 3.2. To verify that it is working, we can read and print the first minibatch of examples. Comparing with Section 3.2, here we use iter to construct a Python iterator and use next to obtain the first item from the iterator.
next(iter(data_iter))

3.3. Concise Implementation of Linear Regression

103

[array([[ 1.0940796 , 0.3688622 ], [-0.49231794, -1.9087739 ], [ 0.390759 , 1.2583892 ], [ 0.30454847, -0.41055235], [-1.7383806 , -0.6379685 ], [ 0.58654 , -0.882999 ], [-0.80323476, 0.45648918], [-0.2931439 , -1.680917 ], [ 2.0085623 , 2.526586 ], [-1.4225755 , -1.1081946 ]]),
array([[ 5.1219654 ], [ 9.706129 ], [ 0.69431883], [ 6.2031784 ], [ 2.8726306 ], [ 8.370204 ], [ 1.0427636 ], [ 9.32764 ], [-0.38527727], [ 5.1158175 ]])]
3.3.3 Defining the Model
When we implemented linear regression from scratch in Section 3.2, we defined our model parameters explicitly and coded up the calculations to produce output using basic linear algebra operations. You should know how to do this. But once your models get more complex, and once you have to do this nearly every day, you will be glad for the assistance. The situation is similar to coding up your own blog from scratch. Doing it once or twice is rewarding and instructive, but you would be a lousy web developer if every time you needed a blog you spent a month reinventing the wheel.
For standard operations, we can use a frameworkʼs predefined layers, which allow us to focus especially on the layers used to construct the model rather than having to focus on the implementation. We will first define a model variable net, which will refer to an instance of the Sequential class. The Sequential class defines a container for several layers that will be chained together. Given input data, a Sequential instance passes it through the first layer, in turn passing the output as the second layerʼs input and so forth. In the following example, our model consists of only one layer, so we do not really need Sequential. But since nearly all of our future models will involve multiple layers, we will use it anyway just to familiarize you with the most standard workflow.
Recall the architecture of a single-layer network as shown in Fig. 3.1.2. The layer is said to be fullyconnected because each of its inputs is connected to each of its outputs by means of a matrix-vector multiplication.
In Gluon, the fully-connected layer is defined in the Dense class. Since we only want to generate a single scalar output, we set that number to 1.
It is worth noting that, for convenience, Gluon does not require us to specify the input shape for each layer. So here, we do not need to tell Gluon how many inputs go into this linear layer. When we first try to pass data through our model, e.g., when we execute net(X) later, Gluon will automatically infer the number of inputs to each layer. We will describe how this works in more detail later.

104

Chapter 3. Linear Neural Networks

# `nn` is an abbreviation for neural networks from mxnet.gluon import nn
net = nn.Sequential() net.add(nn.Dense(1))
3.3.4 Initializing Model Parameters
Before using net, we need to initialize the model parameters, such as the weights and bias in the linear regression model. Deep learning frameworks often have a predefined way to initialize the parameters. Here we specify that each weight parameter should be randomly sampled from a normal distribution with mean 0 and standard deviation 0.01. The bias parameter will be initialized to zero. We will import the initializer module from MXNet. This module provides various methods for model parameter initialization. Gluon makes init available as a shortcut (abbreviation) to access the initializer package. We only specify how to initialize the weight by calling init. Normal(sigma=0.01). Bias parameters are initialized to zero by default.
from mxnet import init
net.initialize(init.Normal(sigma=0.01))
The code above may look straightforward but you should note that something strange is happening here. We are initializing parameters for a network even though Gluon does not yet know how many dimensions the input will have! It might be 2 as in our example or it might be 2000. Gluon lets us get away with this because behind the scene, the initialization is actually deferred. The real initialization will take place only when we for the first time attempt to pass data through the network. Just be careful to remember that since the parameters have not been initialized yet, we cannot access or manipulate them.
3.3.5 Defining the Loss Function
In Gluon, the loss module defines various loss functions. In this example, we will use the Gluon implementation of squared loss (L2Loss).
loss = gluon.loss.L2Loss()
3.3.6 Defining the Optimization Algorithm
Minibatch stochastic gradient descent is a standard tool for optimizing neural networks and thus Gluon supports it alongside a number of variations on this algorithm through its Trainer class. When we instantiate Trainer, we will specify the parameters to optimize over (obtainable from our model net via net.collect_params()), the optimization algorithm we wish to use (sgd), and a dictionary of hyperparameters required by our optimization algorithm. Minibatch stochastic gradient descent just requires that we set the value learning_rate, which is set to 0.03 here.

3.3. Concise Implementation of Linear Regression

105

from mxnet import gluon trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.03})

3.3.7 Training

You might have noticed that expressing our model through high-level APIs of a deep learning framework requires comparatively few lines of code. We did not have to individually allocate parameters, define our loss function, or implement minibatch stochastic gradient descent. Once we start working with much more complex models, advantages of high-level APIs will grow considerably. However, once we have all the basic pieces in place, the training loop itself is strikingly similar to what we did when implementing everything from scratch.
To refresh your memory: for some number of epochs, we will make a complete pass over the dataset (train_data), iteratively grabbing one minibatch of inputs and the corresponding groundtruth labels. For each minibatch, we go through the following ritual:
• Generate predictions by calling net(X) and calculate the loss l (the forward propagation).
• Calculate gradients by running the backpropagation.
• Update the model parameters by invoking our optimizer.
For good measure, we compute the loss after each epoch and print it to monitor progress.

num_epochs = 3 for epoch in range(num_epochs):
for X, y in data_iter: with autograd.record(): l = loss(net(X), y) l.backward() trainer.step(batch_size)
l = loss(net(features), labels) print(f'epoch {epoch + 1}, loss

{l.mean().asnumpy():f}')

epoch 1, loss 0.025230 epoch 2, loss 0.000089 epoch 3, loss 0.000052
Below, we compare the model parameters learned by training on finite data and the actual parameters that generated our dataset. To access parameters, we first access the layer that we need from net and then access that layerʼs weights and bias. As in our from-scratch implementation, note that our estimated parameters are close to their ground-truth counterparts.
w = net[0].weight.data() print(f'error in estimating w: {true_w - w.reshape(true_w.shape)}') b = net[0].bias.data() print(f'error in estimating b: {true_b - b}')

error in estimating w: [ 0.00016081 -0.00085139] error in estimating b: [0.00038671]

106

Chapter 3. Linear Neural Networks

Summary
• Using Gluon, we can implement models much more concisely. • In Gluon, the data module provides tools for data processing, the nn module defines a large
number of neural network layers, and the loss module defines many common loss functions. • MXNetʼs module initializer provides various methods for model parameter initialization. • Dimensionality and storage are automatically inferred, but be careful not to attempt to access parameters before they have been initialized.
Exercises
1. If we replace l = loss(output, y) with l = loss(output, y).mean(), we need to change trainer.step(batch_size) to trainer.step(1) for the code to behave identically. Why?
2. Review the MXNet documentation to see what loss functions and initialization methods are provided in the modules gluon.loss and init. Replace the loss by Huberʼs loss.
3. How do you access the gradient of dense.weight? Discussions54
3.4 Softmax Regression
In Section 3.1, we introduced linear regression, working through implementations from scratch in Section 3.2 and again using high-level APIs of a deep learning framework in Section 3.3 to do the heavy lifting. Regression is the hammer we reach for when we want to answer how much? or how many? questions. If you want to predict the number of dollars (price) at which a house will be sold, or the number of wins a baseball team might have, or the number of days that a patient will remain hospitalized before being discharged, then you are probably looking for a regression model. In practice, we are more often interested in classification: asking not “how much” but “which one”:
• Does this email belong in the spam folder or the inbox? • Is this customer more likely to sign up or not to sign up for a subscription service? • Does this image depict a donkey, a dog, a cat, or a rooster? • Which movie is Aston most likely to watch next? Colloquially, machine learning practitioners overload the word classification to describe two subtly different problems: (i) those where we are interested only in hard assignments of examples to categories (classes); and (ii) those where we wish to make soft assignments, i.e., to assess the probability that each category applies. The distinction tends to get blurred, in part, because often, even when we only care about hard assignments, we still use models that make soft assignments.
54 https://discuss.d2l.ai/t/44

3.4. Softmax Regression

107

3.4.1 Classification Problem

To get our feet wet, let us start off with a simple image classification problem. Here, each input consists of a 2 × 2 grayscale image. We can represent each pixel value with a single scalar, giving us four features x1, x2, x3, x4. Further, let us assume that each image belongs to one among the categories “cat”, “chicken”, and “dog”.

Next, we have to choose how to represent the labels. We have two obvious choices. Perhaps the most natural impulse would be to choose y ∈ {1, 2, 3}, where the integers represent {dog, cat, chicken} respectively. This is a great way of storing such information on a computer. If the categories had some natural ordering among them, say if we were trying to predict {baby, toddler, adolescent, young adult, adult, geriatric}, then it might even make sense to cast this problem as regression and keep the labels in this format.

But general classification problems do not come with natural orderings among the classes. Fortunately, statisticians long ago invented a simple way to represent categorical data: the one-hot encoding. A one-hot encoding is a vector with as many components as we have categories. The component corresponding to particular instanceʼs category is set to 1 and all other components are set to 0. In our case, a label y would be a three-dimensional vector, with (1, 0, 0) corresponding to “cat”, (0, 1, 0) to “chicken”, and (0, 0, 1) to “dog”:

y ∈ {(1, 0, 0), (0, 1, 0), (0, 0, 1)}.

(3.4.1)

3.4.2 Network Architecture

In order to estimate the conditional probabilities associated with all the possible classes, we need a model with multiple outputs, one per class. To address classification with linear models, we will need as many affine functions as we have outputs. Each output will correspond to its own affine function. In our case, since we have 4 features and 3 possible output categories, we will need 12 scalars to represent the weights (w with subscripts), and 3 scalars to represent the biases (b with subscripts). We compute these three logits, o1, o2, and o3, for each input:

o1 = x1w11 + x2w12 + x3w13 + x4w14 + b1, o2 = x1w21 + x2w22 + x3w23 + x4w24 + b2, o3 = x1w31 + x2w32 + x3w33 + x4w34 + b3.

(3.4.2)

We can depict this calculation with the neural network diagram shown in Fig. 3.4.1. Just as in linear regression, softmax regression is also a single-layer neural network. And since the calculation of each output, o1, o2, and o3, depends on all inputs, x1, x2, x3, and x4, the output layer of softmax regression can also be described as fully-connected layer.

Fig. 3.4.1: Softmax regression is a single-layer neural network.

To express the model more compactly, we can use linear algebra notation. In vector form, we arrive at o = Wx + b, a form better suited both for mathematics, and for writing code. Note that

108

Chapter 3. Linear Neural Networks

we have gathered all of our weights into a 3×4 matrix and that for features of a given data example x, our outputs are given by a matrix-vector product of our weights by our input features plus our biases b.

3.4.3 Parameterization Cost of Fully-Connected Layers
As we will see in subsequent chapters, fully-connected layers are ubiquitous in deep learning. However, as the name suggests, fully-connected layers are fully connected with potentially many learnable parameters. Specifically, for any fully-connected layer with d inputs and q outputs, the parameterization cost is O(dq), which can be prohibitively high in practice. Fortunately, this cost of transforming d inputs into q outputs can be reduced to O( dnq ), where the hyperparameter n can be flexibly specified by us to balance between parameter saving and model effectiveness in real-world applications (Zhang et al., 2021).

3.4.4 Softmax Operation

The main approach that we are going to take here is to interpret the outputs of our model as probabilities. We will optimize our parameters to produce probabilities that maximize the likelihood of the observed data. Then, to generate predictions, we will set a threshold, for example, choosing the label with the maximum predicted probabilities.
Put formally, we would like any output yˆj to be interpreted as the probability that a given item belongs to class j. Then we can choose the class with the largest output value as our prediction argmaxj yj. For example, if yˆ1, yˆ2, and yˆ3 are 0.1, 0.8, and 0.1, respectively, then we predict category 2, which (in our example) represents “chicken”.
You might be tempted to suggest that we interpret the logits o directly as our outputs of interest. However, there are some problems with directly interpreting the output of the linear layer as a probability. On one hand, nothing constrains these numbers to sum to 1. On the other hand, depending on the inputs, they can take negative values. These violate basic axioms of probability presented in Section 2.6
To interpret our outputs as probabilities, we must guarantee that (even on new data), they will be nonnegative and sum up to 1. Moreover, we need a training objective that encourages the model to estimate faithfully probabilities. Of all instances when a classifier outputs 0.5, we hope that half of those examples will actually belong to the predicted class. This is a property called calibration.
The softmax function, invented in 1959 by the social scientist R. Duncan Luce in the context of choice models, does precisely this. To transform our logits such that they become nonnegative and sum to 1, while requiring that the model remains differentiable, we first exponentiate each logit (ensuring non-negativity) and then divide by their sum (ensuring that they sum to 1):

yˆ = softmax(o) where yˆj = ∑exp(oj) . k exp(ok)

(3.4.3)

It is easy to see yˆ1 + yˆ2 + yˆ3 = 1 with 0 ≤ yˆj ≤ 1 for all j. Thus, yˆ is a proper probability distribution whose element values can be interpreted accordingly. Note that the softmax operation does not change the ordering among the logits o, which are simply the pre-softmax values that determine the probabilities assigned to each class. Therefore, during prediction we can still pick out the most likely class by

argmax yˆj = argmax oj.

j

j

(3.4.4)

3.4. Softmax Regression

109

Although softmax is a nonlinear function, the outputs of softmax regression are still determined by an affine transformation of input features; thus, softmax regression is a linear model.

3.4.5 Vectorization for Minibatches

To improve computational efficiency and take advantage of GPUs, we typically carry out vector
calculations for minibatches of data. Assume that we are given a minibatch X of examples with
feature dimensionality (number of inputs) d and batch size n. Moreover, assume that we have q categories in the output. Then the minibatch features X are in Rn×d, weights W ∈ Rd×q, and the bias satisfies b ∈ R1×q.

O = XW + b, Yˆ = softmax(O).

(3.4.5)

This accelerates the dominant operation into a matrix-matrix product XW vs. the matrix-vector products we would be executing if we processed one example at a time. Since each row in X represents a data example, the softmax operation itself can be computed rowwise: for each row of O, exponentiate all entries and then normalize them by the sum. Triggering broadcasting during the summation XW + b in (3.4.5), both the minibatch logits O and output probabilities Yˆ are n × q matrices.

3.4.6 Loss Function
Next, we need a loss function to measure the quality of our predicted probabilities. We will rely on maximum likelihood estimation, the very same concept that we encountered when providing a probabilistic justification for the mean squared error objective in linear regression (Section 3.1.3).

Log-Likelihood

The softmax function gives us a vector yˆ, which we can interpret as estimated conditional prob-
abilities of each class given any input x, e.g., yˆ1 = P (y = cat | x). Suppose that the entire dataset {X, Y} has n examples, where the example indexed by i consists of a feature vector x(i) and a onehot label vector y(i). We can compare the estimates with reality by checking how probable the
actual classes are according to our model, given the features:

∏n P (Y | X) = P (y(i) | x(i)).
i=1

(3.4.6)

According to maximum likelihood estimation, we maximize P (Y | X), which is equivalent to minimizing the negative log-likelihood:

∑n

∑n

− log P (Y | X) = − log P (y(i) | x(i)) = l(y(i), yˆ(i)),

i=1

i=1

(3.4.7)

where for any pair of label y and model prediction yˆ over q classes, the loss function l is

∑q l(y, yˆ) = − yj log yˆj.
j=1

(3.4.8)

110

Chapter 3. Linear Neural Networks

For reasons explained later on, the loss function in (3.4.8) is commonly called the cross-entropy loss. Since y is a one-hot vector of length q, the sum over all its coordinates j vanishes for all but one term. Since all yˆj are predicted probabilities, their logarithm is never larger than 0. Consequently, the loss function cannot be minimized any further if we correctly predict the actual label with certainty, i.e., if the predicted probability P (y | x) = 1 for the actual label y. Note that this is often impossible. For example, there might be label noise in the dataset (some examples may be mislabeled). It may also not be possible when the input features are not sufficiently informative to classify every example perfectly.

Softmax and Derivatives

Since the softmax and the corresponding loss are so common, it is worth understanding a bit better how it is computed. Plugging (3.4.3) into the definition of the loss in (3.4.8) and using the definition of the softmax we obtain:

∑q

exp(oj )

l(y, yˆ) = − j=1 yj log ∑qk=1 exp(ok)

∑q

∑q

∑q

= yj log exp(ok) − yjoj

j=1

k=1

j=1

∑q

∑q

= log exp(ok) − yjoj.

k=1

j=1

(3.4.9)

To understand a bit better what is going on, consider the derivative with respect to any logit oj. We get

∂o l(y, yˆ) = ∑qexp(oj) − yj = softmax(o)j − yj.

j

k=1 exp(ok)

(3.4.10)

In other words, the derivative is the difference between the probability assigned by our model, as expressed by the softmax operation, and what actually happened, as expressed by elements in the one-hot label vector. In this sense, it is very similar to what we saw in regression, where the gradient was the difference between the observation y and estimate yˆ. This is not coincidence. In any exponential family (see the online appendix on distributions55) model, the gradients of the log-likelihood are given by precisely this term. This fact makes computing gradients easy in practice.

Cross-Entropy Loss
Now consider the case where we observe not just a single outcome but an entire distribution over outcomes. We can use the same representation as before for the label y. The only difference is that rather than a vector containing only binary entries, say (0, 0, 1), we now have a generic probability vector, say (0.1, 0.2, 0.7). The math that we used previously to define the loss l in (3.4.8) still works out fine, just that the interpretation is slightly more general. It is the expected value of the loss for a distribution over labels. This loss is called the cross-entropy loss and it is one of the most commonly used losses for classification problems. We can demystify the name by introducing just the basics of information theory. If you wish to understand more details of information theory, you may further refer to the online appendix on information theory56.
55 https://d2l.ai/chapter_appendix-mathematics-for-deep-learning/distributions.html 56 https://d2l.ai/chapter_appendix-mathematics-for-deep-learning/information-theory.html

3.4. Softmax Regression

111

3.4.7 Information Theory Basics
Information theory deals with the problem of encoding, decoding, transmitting, and manipulating information (also known as data) in as concise form as possible.

Entropy

The central idea in information theory is to quantify the information content in data. This quantity

places a hard limit on our ability to compress the data. In information theory, this quantity is

called the entropy of a distribution P , and it is captured by the following equation:

∑ H[P ] = −P (j) log P (j).
j

(3.4.11)

One of the fundamental theorems of information theory states that in order to encode data drawn

randomly from the distribution P , we need at least H[P ] “nats” to encode it. If you wonder what

a “nat” is, it is the equivalent of bit but when using a code with base e rather than one with base 2.

Thus,

one

nat

is

1 log(2)

≈

1.44

bit.

Surprisal

You might be wondering what compression has to do with prediction. Imagine that we have a stream of data that we want to compress. If it is always easy for us to predict the next token, then this data is easy to compress! Take the extreme example where every token in the stream always takes the same value. That is a very boring data stream! And not only it is boring, but it is also easy to predict. Because they are always the same, we do not have to transmit any information to communicate the contents of the stream. Easy to predict, easy to compress.

However if we cannot perfectly predict every event, then we might sometimes be surprised. Our

surprise is greater when we assigned an event lower probability. Claude Shannon settled on

log

1 P (j)

=

− log P (j)

to

quantify

oneʼs

surprisal

at

observing

an

event

j

having

assigned

it

a

(sub-

jective) probability P (j). The entropy defined in (3.4.11) is then the expected surprisal when one

assigned the correct probabilities that truly match the data-generating process.

Cross-Entropy Revisited
So if entropy is level of surprise experienced by someone who knows the true probability, then you might be wondering, what is cross-entropy? The cross-entropy from P to Q, denoted H(P, Q), is the expected surprisal of an observer with subjective probabilities Q upon seeing data that were actually generated according to probabilities P . The lowest possible cross-entropy is achieved when P = Q. In this case, the cross-entropy from P to Q is H(P, P ) = H(P ).
In short, we can think of the cross-entropy classification objective in two ways: (i) as maximizing the likelihood of the observed data; and (ii) as minimizing our surprisal (and thus the number of bits) required to communicate the labels.

112

Chapter 3. Linear Neural Networks

3.4.8 Model Prediction and Evaluation
After training the softmax regression model, given any example features, we can predict the probability of each output class. Normally, we use the class with the highest predicted probability as the output class. The prediction is correct if it is consistent with the actual class (label). In the next part of the experiment, we will use accuracy to evaluate the modelʼs performance. This is equal to the ratio between the number of correct predictions and the total number of predictions.

Summary
• The softmax operation takes a vector and maps it into probabilities. • Softmax regression applies to classification problems. It uses the probability distribution of
the output class in the softmax operation. • Cross-entropy is a good measure of the difference between two probability distributions. It
measures the number of bits needed to encode the data given our model.

Exercises

1. We can explore the connection between exponential families and the softmax in some more depth.

1. Compute the second derivative of the cross-entropy loss l(y, yˆ) for the softmax.

2. Compute the variance of the distribution given by softmax(o) and show that it matches the second derivative computed above.

2. Assume that we have three classes which occur with equal probability, i.e., the probability

vector

is

(

1 3

,

1 3

,

1 3

).

1. What is the problem if we try to design a binary code for it?

2. Can you design a better code? Hint: what happens if we try to encode two independent observations? What if we encode n observations jointly?

3. Softmax is a misnomer for the mapping introduced above (but everyone in deep learning uses it). The real softmax is defined as RealSoftMax(a, b) = log(exp(a) + exp(b)).

1. Prove that RealSoftMax(a, b) > max(a, b).

2. Prove that this holds for λ−1RealSoftMax(λa, λb), provided that λ > 0.

3. Show that for λ → ∞ we have λ−1RealSoftMax(λa, λb) → max(a, b).

4. What does the soft-min look like?

5. Extend this to more than two numbers.

Discussions57
57 https://discuss.d2l.ai/t/46

3.4. Softmax Regression

113

3.5 The Image Classification Dataset
One of the widely used dataset for image classification is the MNIST dataset (LeCun et al., 1998). While it had a good run as a benchmark dataset, even simple models by todayʼs standards achieve classification accuracy over 95%, making it unsuitable for distinguishing between stronger models and weaker ones. Today, MNIST serves as more of sanity checks than as a benchmark. To up the ante just a bit, we will focus our discussion in the coming sections on the qualitatively similar, but comparatively complex Fashion-MNIST dataset (Xiao et al., 2017), which was released in 2017.
%matplotlib inline import sys from mxnet import gluon from d2l import mxnet as d2l
d2l.use_svg_display()

3.5.1 Reading the Dataset
We can download and read the Fashion-MNIST dataset into memory via the build-in functions in the framework.
mnist_train = gluon.data.vision.FashionMNIST(train=True) mnist_test = gluon.data.vision.FashionMNIST(train=False)
Fashion-MNIST consists of images from 10 categories, each represented by 6000 images in the training dataset and by 1000 in the test dataset. A test dataset (or test set) is used for evaluating model performance and not for training. Consequently the training set and the test set contain 60000 and 10000 images, respectively.
len(mnist_train), len(mnist_test)

(60000, 10000)
The height and width of each input image are both 28 pixels. Note that the dataset consists of grayscale images, whose number of channels is 1. For brevity, throughout this book we store the shape of any image with height h width w pixels as h × w or (h, w).
mnist_train[0][0].shape

(28, 28, 1)

The images in Fashion-MNIST are associated with the following categories: t-shirt, trousers, pullover, dress, coat, sandal, shirt, sneaker, bag, and ankle boot. The following function converts between numeric label indices and their names in text.

def get_fashion_mnist_labels(labels): #@save """Return text labels for the Fashion-MNIST dataset.""" text_labels = [

(continues on next page)

114

Chapter 3. Linear Neural Networks

(continued from previous page) 't-shirt', 'trouser', 'pullover', 'dress', 'coat', 'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot'] return [text_labels[int(i)] for i in labels]
We can now create a function to visualize these examples.
def show_images(imgs, num_rows, num_cols, titles=None, scale=1.5): #@save """Plot a list of images.""" figsize = (num_cols * scale, num_rows * scale) _, axes = d2l.plt.subplots(num_rows, num_cols, figsize=figsize) axes = axes.flatten() for i, (ax, img) in enumerate(zip(axes, imgs)): ax.imshow(img.asnumpy()) ax.axes.get_xaxis().set_visible(False) ax.axes.get_yaxis().set_visible(False) if titles: ax.set_title(titles[i]) return axes
Here are the images and their corresponding labels (in text) for the first few examples in the training dataset.
X, y = mnist_train[:18]
print(X.shape) show_images(X.squeeze(axis=-1), 2, 9, titles=get_fashion_mnist_labels(y));
(18, 28, 28, 1)

3.5.2 Reading a Minibatch

To make our life easier when reading from the training and test sets, we use the built-in data iterator rather than creating one from scratch. Recall that at each iteration, a data iterator reads a minibatch of data with size batch_size each time. We also randomly shuffle the examples for the training data iterator.

batch_size = 256

def get_dataloader_workers(): #@save """Use 4 processes to read the data except for Windows."""

(continues on next page)

3.5. The Image Classification Dataset

115

return 0 if sys.platform.startswith('win') else 4

(continued from previous page)

# `ToTensor` converts the image data from uint8 to 32-bit floating point. It # divides all numbers by 255 so that all pixel values are between 0 and 1 transformer = gluon.data.vision.transforms.ToTensor() train_iter = gluon.data.DataLoader(mnist_train.transform_first(transformer),
batch_size, shuffle=True, num_workers=get_dataloader_workers())

Let us look at the time it takes to read the training data.
timer = d2l.Timer() for X, y in train_iter:
continue f'{timer.stop():.2f} sec'

'1.75 sec'

3.5.3 Putting All Things Together
Now we define the load_data_fashion_mnist function that obtains and reads the Fashion-MNIST dataset. It returns the data iterators for both the training set and validation set. In addition, it accepts an optional argument to resize images to another shape.
def load_data_fashion_mnist(batch_size, resize=None): #@save """Download the Fashion-MNIST dataset and then load it into memory.""" dataset = gluon.data.vision trans = [dataset.transforms.ToTensor()] if resize: trans.insert(0, dataset.transforms.Resize(resize)) trans = dataset.transforms.Compose(trans) mnist_train = dataset.FashionMNIST(train=True).transform_first(trans) mnist_test = dataset.FashionMNIST(train=False).transform_first(trans) return (gluon.data.DataLoader(mnist_train, batch_size, shuffle=True, num_workers=get_dataloader_workers()), gluon.data.DataLoader(mnist_test, batch_size, shuffle=False, num_workers=get_dataloader_workers()))
Below we test the image resizing feature of the load_data_fashion_mnist function by specifying the resize argument.
train_iter, test_iter = load_data_fashion_mnist(32, resize=64) for X, y in train_iter:
print(X.shape, X.dtype, y.shape, y.dtype) break
(32, 1, 64, 64) <class 'numpy.float32'> (32,) <class 'numpy.int32'>
We are now ready to work with the Fashion-MNIST dataset in the sections that follow.

116

Chapter 3. Linear Neural Networks

Summary
• Fashion-MNIST is an apparel classification dataset consisting of images representing 10 categories. We will use this dataset in subsequent sections and chapters to evaluate various classification algorithms.
• We store the shape of any image with height h width w pixels as h × w or (h, w). • Data iterators are a key component for efficient performance. Rely on well-implemented
data iterators that exploit high-performance computing to avoid slowing down your training loop.
Exercises
1. Does reducing the batch_size (for instance, to 1) affect the reading performance? 2. The data iterator performance is important. Do you think the current implementation is fast
enough? Explore various options to improve it. 3. Check out the frameworkʼs online API documentation. Which other datasets are available? Discussions58
3.6 Implementation of Softmax Regression from Scratch
Just as we implemented linear regression from scratch, we believe that softmax regression is similarly fundamental and you ought to know the gory details of how to implement it yourself. We will work with the Fashion-MNIST dataset, just introduced in Section 3.5, setting up a data iterator with batch size 256.
from IPython import display from mxnet import autograd, gluon, np, npx from d2l import mxnet as d2l
npx.set_np()
batch_size = 256 train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)

3.6.1 Initializing Model Parameters
As in our linear regression example, each example here will be represented by a fixed-length vector. Each example in the raw dataset is a 28 × 28 image. In this section, we will flatten each image, treating them as vectors of length 784. In the future, we will talk about more sophisticated strategies for exploiting the spatial structure in images, but for now we treat each pixel location as just another feature.
Recall that in softmax regression, we have as many outputs as there are classes. Because our dataset has 10 classes, our network will have an output dimension of 10. Consequently, our weights
58 https://discuss.d2l.ai/t/48

3.6. Implementation of Softmax Regression from Scratch

117

will constitute a 784 × 10 matrix and the biases will constitute a 1 × 10 row vector. As with linear regression, we will initialize our weights W with Gaussian noise and our biases to take the initial value 0.
num_inputs = 784 num_outputs = 10
W = np.random.normal(0, 0.01, (num_inputs, num_outputs)) b = np.zeros(num_outputs) W.attach_grad() b.attach_grad()

3.6.2 Defining the Softmax Operation
Before implementing the softmax regression model, let us briefly review how the sum operator works along specific dimensions in a tensor, as discussed in Section 2.3.6 and Section 2.3.6. Given a matrix X we can sum over all elements (by default) or only over elements in the same axis, i.e., the same column (axis 0) or the same row (axis 1). Note that if X is a tensor with shape (2, 3) and we sum over the columns, the result will be a vector with shape (3,). When invoking the sum operator, we can specify to keep the number of axes in the original tensor, rather than collapsing out the dimension that we summed over. This will result in a two-dimensional tensor with shape (1, 3).
X = np.array([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]]) X.sum(0, keepdims=True), X.sum(1, keepdims=True)

(array([[5., 7., 9.]]), array([[ 6.],
[15.]]))

We are now ready to implement the softmax operation. Recall that softmax consists of three steps: (i) we exponentiate each term (using exp); (ii) we sum over each row (we have one row per example in the batch) to get the normalization constant for each example; (iii) we divide each row by its normalization constant, ensuring that the result sums to 1. Before looking at the code, let us recall how this looks expressed as an equation:

softmax(X)ij = ∑exp(Xij) . k exp(Xik)

(3.6.1)

The denominator, or normalization constant, is also sometimes called the partition function (and
its logarithm is called the log-partition function). The origins of that name are in statistical physics59 where a related equation models the distribution over an ensemble of particles.

def softmax(X): X_exp = np.exp(X) partition = X_exp.sum(1, keepdims=True) return X_exp / partition # The broadcasting mechanism is applied here

As you can see, for any random input, we turn each element into a non-negative number. Moreover, each row sums up to 1, as is required for a probability.
59 https://en.wikipedia.org/wiki/Partition_function_(statistical_mechanics)

118

Chapter 3. Linear Neural Networks

X = np.random.normal(0, 1, (2, 5)) X_prob = softmax(X) X_prob, X_prob.sum(1)

(array([[0.22376052, 0.06659239, 0.06583703, 0.29964197, 0.3441681 ],

[0.63209665, 0.03179282, 0.194987 , 0.09209415, 0.04902935]]),

array([1.

, 0.99999994]))

Note that while this looks correct mathematically, we were a bit sloppy in our implementation because we failed to take precautions against numerical overflow or underflow due to large or very small elements of the matrix.

3.6.3 Defining the Model
Now that we have defined the softmax operation, we can implement the softmax regression model. The below code defines how the input is mapped to the output through the network. Note that we flatten each original image in the batch into a vector using the reshape function before passing the data through our model.
def net(X): return softmax(np.dot(X.reshape((-1, W.shape[0])), W) + b)

3.6.4 Defining the Loss Function
Next, we need to implement the cross-entropy loss function, as introduced in Section 3.4. This may be the most common loss function in all of deep learning because, at the moment, classification problems far outnumber regression problems.
Recall that cross-entropy takes the negative log-likelihood of the predicted probability assigned to the true label. Rather than iterating over the predictions with a Python for-loop (which tends to be inefficient), we can pick all elements by a single operator. Below, we create sample data y_hat with 2 examples of predicted probabilities over 3 classes and their corresponding labels y. With y we know that in the first example the first class is the correct prediction and in the second example the third class is the ground-truth. Using y as the indices of the probabilities in y_hat, we pick the probability of the first class in the first example and the probability of the third class in the second example.
y = np.array([0, 2]) y_hat = np.array([[0.1, 0.3, 0.6], [0.3, 0.2, 0.5]]) y_hat[[0, 1], y]
array([0.1, 0.5])
Now we can implement the cross-entropy loss function efficiently with just one line of code.
def cross_entropy(y_hat, y): return -np.log(y_hat[range(len(y_hat)), y])
cross_entropy(y_hat, y)

3.6. Implementation of Softmax Regression from Scratch

119

array([2.3025851, 0.6931472])
3.6.5 Classification Accuracy
Given the predicted probability distribution y_hat, we typically choose the class with the highest predicted probability whenever we must output a hard prediction. Indeed, many applications require that we make a choice. Gmail must categorize an email into “Primary”, “Social”, “Updates”, or “Forums”. It might estimate probabilities internally, but at the end of the day it has to choose one among the classes.
When predictions are consistent with the label class y, they are correct. The classification accuracy is the fraction of all predictions that are correct. Although it can be difficult to optimize accuracy directly (it is not differentiable), it is often the performance measure that we care most about, and we will nearly always report it when training classifiers.
To compute accuracy we do the following. First, if y_hat is a matrix, we assume that the second dimension stores prediction scores for each class. We use argmax to obtain the predicted class by the index for the largest entry in each row. Then we compare the predicted class with the groundtruth y elementwise. Since the equality operator == is sensitive to data types, we convert y_hatʼs data type to match that of y. The result is a tensor containing entries of 0 (false) and 1 (true). Taking the sum yields the number of correct predictions.
def accuracy(y_hat, y): #@save """Compute the number of correct predictions.""" if len(y_hat.shape) > 1 and y_hat.shape[1] > 1: y_hat = y_hat.argmax(axis=1) cmp = y_hat.astype(y.dtype) == y return float(cmp.astype(y.dtype).sum())
We will continue to use the variables y_hat and y defined before as the predicted probability distributions and labels, respectively. We can see that the first exampleʼs prediction class is 2 (the largest element of the row is 0.6 with the index 2), which is inconsistent with the actual label, 0. The second exampleʼs prediction class is 2 (the largest element of the row is 0.5 with the index of 2), which is consistent with the actual label, 2. Therefore, the classification accuracy rate for these two examples is 0.5.
accuracy(y_hat, y) / len(y)
0.5
Similarly, we can evaluate the accuracy for any model net on a dataset that is accessed via the data iterator data_iter.
def evaluate_accuracy(net, data_iter): #@save """Compute the accuracy for a model on a dataset.""" metric = Accumulator(2) # No. of correct predictions, no. of predictions for X, y in data_iter: metric.add(accuracy(net(X), y), y.size) return metric[0] / metric[1]

120

Chapter 3. Linear Neural Networks

Here Accumulator is a utility class to accumulate sums over multiple variables. In the above evaluate_accuracy function, we create 2 variables in the Accumulator instance for storing both the number of correct predictions and the number of predictions, respectively. Both will be accumulated over time as we iterate over the dataset.
class Accumulator: #@save """For accumulating sums over `n` variables.""" def __init__(self, n): self.data = [0.0] * n
def add(self, *args): self.data = [a + float(b) for a, b in zip(self.data, args)]
def reset(self): self.data = [0.0] * len(self.data)
def __getitem__(self, idx): return self.data[idx]
Because we initialized the net model with random weights, the accuracy of this model should be close to random guessing, i.e., 0.1 for 10 classes.
evaluate_accuracy(net, test_iter)
0.0811

3.6.6 Training
The training loop for softmax regression should look strikingly familiar if you read through our implementation of linear regression in Section 3.2. Here we refactor the implementation to make it reusable. First, we define a function to train for one epoch. Note that updater is a general function to update the model parameters, which accepts the batch size as an argument. It can be either a wrapper of the d2l.sgd function or a frameworkʼs built-in optimization function.
def train_epoch_ch3(net, train_iter, loss, updater): #@save """Train a model within one epoch (defined in Chapter 3).""" # Sum of training loss, sum of training accuracy, no. of examples metric = Accumulator(3) if isinstance(updater, gluon.Trainer): updater = updater.step for X, y in train_iter: # Compute gradients and update parameters with autograd.record(): y_hat = net(X) l = loss(y_hat, y) l.backward() updater(X.shape[0]) metric.add(float(l.sum()), accuracy(y_hat, y), y.size) # Return training loss and training accuracy return metric[0] / metric[2], metric[1] / metric[2]

Before showing the implementation of the training function, we define a utility class that plot data in animation. Again, it aims to simplify code in the rest of the book.

3.6. Implementation of Softmax Regression from Scratch

121

class Animator: #@save """For plotting data in animation.""" def __init__(self, xlabel=None, ylabel=None, legend=None, xlim=None, ylim=None, xscale='linear', yscale='linear', fmts=('-', 'm--', 'g-.', 'r:'), nrows=1, ncols=1, figsize=(3.5, 2.5)): # Incrementally plot multiple lines if legend is None: legend = [] d2l.use_svg_display() self.fig, self.axes = d2l.plt.subplots(nrows, ncols, figsize=figsize) if nrows * ncols == 1: self.axes = [self.axes,] # Use a lambda function to capture arguments self.config_axes = lambda: d2l.set_axes(self.axes[ 0], xlabel, ylabel, xlim, ylim, xscale, yscale, legend) self.X, self.Y, self.fmts = None, None, fmts
def add(self, x, y): # Add multiple data points into the figure if not hasattr(y, "__len__"): y = [y] n = len(y) if not hasattr(x, "__len__"): x = [x] * n if not self.X: self.X = [[] for _ in range(n)] if not self.Y: self.Y = [[] for _ in range(n)] for i, (a, b) in enumerate(zip(x, y)): if a is not None and b is not None: self.X[i].append(a) self.Y[i].append(b) self.axes[0].cla() for x, y, fmt in zip(self.X, self.Y, self.fmts): self.axes[0].plot(x, y, fmt) self.config_axes() display.display(self.fig) display.clear_output(wait=True)
The following training function then trains a model net on a training dataset accessed via train_iter for multiple epochs, which is specified by num_epochs. At the end of each epoch, the model is evaluated on a testing dataset accessed via test_iter. We will leverage the Animator class to visualize the training progress.
def train_ch3(net, train_iter, test_iter, loss, num_epochs, updater): #@save """Train a model (defined in Chapter 3).""" animator = Animator(xlabel='epoch', xlim=[1, num_epochs], ylim=[0.3, 0.9], legend=['train loss', 'train acc', 'test acc']) for epoch in range(num_epochs): train_metrics = train_epoch_ch3(net, train_iter, loss, updater) test_acc = evaluate_accuracy(net, test_iter) animator.add(epoch + 1, train_metrics + (test_acc,)) train_loss, train_acc = train_metrics assert train_loss < 0.5, train_loss
(continues on next page)

122

Chapter 3. Linear Neural Networks

assert train_acc <= 1 and train_acc > 0.7, train_acc assert test_acc <= 1 and test_acc > 0.7, test_acc

(continued from previous page)

As an implementation from scratch, we use the minibatch stochastic gradient descent defined in Section 3.2 to optimize the loss function of the model with a learning rate 0.1.
lr = 0.1
def updater(batch_size): return d2l.sgd([W, b], lr, batch_size)

Now we train the model with 10 epochs. Note that both the number of epochs (num_epochs), and learning rate (lr) are adjustable hyperparameters. By changing their values, we may be able to increase the classification accuracy of the model.
num_epochs = 10 train_ch3(net, train_iter, test_iter, cross_entropy, num_epochs, updater)

3.6.7 Prediction
Now that training is complete, our model is ready to classify some images. Given a series of images, we will compare their actual labels (first line of text output) and the predictions from the model (second line of text output).
def predict_ch3(net, test_iter, n=6): #@save """Predict labels (defined in Chapter 3).""" for X, y in test_iter: break trues = d2l.get_fashion_mnist_labels(y) preds = d2l.get_fashion_mnist_labels(net(X).argmax(axis=1)) titles = [true + '\n' + pred for true, pred in zip(trues, preds)] d2l.show_images(X[0:n].reshape((n, 28, 28)), 1, n, titles=titles[0:n])
predict_ch3(net, test_iter)

3.6. Implementation of Softmax Regression from Scratch

123

Summary
• With softmax regression, we can train models for multiclass classification. • The training loop of softmax regression is very similar to that in linear regression: retrieve
and read data, define models and loss functions, then train models using optimization algorithms. As you will soon find out, most common deep learning models have similar training procedures.
Exercises
1. In this section, we directly implemented the softmax function based on the mathematical definition of the softmax operation. What problems might this cause? Hint: try to calculate the size of exp(50).
2. The function cross_entropy in this section was implemented according to the definition of the cross-entropy loss function. What could be the problem with this implementation? Hint: consider the domain of the logarithm.
3. What solutions you can think of to fix the two problems above? 4. Is it always a good idea to return the most likely label? For example, would you do this for
medical diagnosis? 5. Assume that we want to use softmax regression to predict the next word based on some
features. What are some problems that might arise from a large vocabulary? Discussions60

3.7 Concise Implementation of Softmax Regression

Just as high-level APIs of deep learning frameworks made it much easier to implement linear regression in Section 3.3, we will find it similarly (or possibly more) convenient for implementing classification models. Let us stick with the Fashion-MNIST dataset and keep the batch size at 256 as in Section 3.6.
from mxnet import gluon, init, npx from mxnet.gluon import nn from d2l import mxnet as d2l
npx.set_np()
60 https://discuss.d2l.ai/t/50

124

Chapter 3. Linear Neural Networks

batch_size = 256 train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)

3.7.1 Initializing Model Parameters
As mentioned in Section 3.4, the output layer of softmax regression is a fully-connected layer. Therefore, to implement our model, we just need to add one fully-connected layer with 10 outputs to our Sequential. Again, here, the Sequential is not really necessary, but we might as well form the habit since it will be ubiquitous when implementing deep models. Again, we initialize the weights at random with zero mean and standard deviation 0.01.
net = nn.Sequential() net.add(nn.Dense(10)) net.initialize(init.Normal(sigma=0.01))

3.7.2 Softmax Implementation Revisited

In the previous example of Section 3.6, we calculated our modelʼs output and then ran this output through the cross-entropy loss. Mathematically, that is a perfectly reasonable thing to do. However, from a computational perspective, exponentiation can be a source of numerical stability issues.
Recall that the softmax function calculates yˆj = ∑ekxepx(po(jo)k) , where yˆj is the jth element of the predicted probability distribution yˆ and oj is the jth element of the logits o. If some of the ok are very large (i.e., very positive), then exp(ok) might be larger than the largest number we can have for certain data types (i.e., overflow). This would make the denominator (and/or numerator) inf (infinity) and we wind up encountering either 0, inf, or nan (not a number) for yˆj. In these situations we do not get a well-defined return value for cross-entropy.

One trick to get around this is to first subtract max(ok) from all ok before proceeding with the softmax calculation. You can see that this shifting of each ok by constant factor does not change the return value of softmax:

yˆj = ∑exp(oj − max(ok)) exp(max(ok)) k exp(ok − max(ok)) exp(max(ok))
= ∑exp(oj − max(ok)) . k exp(ok − max(ok))

(3.7.1)

After the subtraction and normalization step, it might be possible that some oj − max(ok) have large negative values and thus that the corresponding exp(oj − max(ok)) will take values close to zero. These might be rounded to zero due to finite precision (i.e., underflow), making yˆj zero and giving us -inf for log(yˆj). A few steps down the road in backpropagation, we might find ourselves faced with a screenful of the dreaded nan results.

Fortunately, we are saved by the fact that even though we are computing exponential functions, we ultimately intend to take their log (when calculating the cross-entropy loss). By combining these two operators softmax and cross-entropy together, we can escape the numerical stability issues that might otherwise plague us during backpropagation. As shown in the equation below, we avoid calculating exp(oj − max(ok)) and can use instead oj − max(ok) directly due to the canceling

3.7. Concise Implementation of Softmax Regression

125

in log(exp(·)):

(

)

log (yˆj) = log ∑exp(oj − max(ok))

k exp(ok − max(ok)) (

)

∑

= log (exp(oj − max(ok))) − log

exp(ok − max(ok))

(

k

)

∑

= oj − max(ok) − log

exp(ok − max(ok)) .

k

(3.7.2)

We will want to keep the conventional softmax function handy in case we ever want to evaluate
the output probabilities by our model. But instead of passing softmax probabilities into our new
loss function, we will just pass the logits and compute the softmax and its log all at once inside the cross-entropy loss function, which does smart things like the “LogSumExp trick”61.

loss = gluon.loss.SoftmaxCrossEntropyLoss()

3.7.3 Optimization Algorithm
Here, we use minibatch stochastic gradient descent with a learning rate of 0.1 as the optimization algorithm. Note that this is the same as we applied in the linear regression example and it illustrates the general applicability of the optimizers.
trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.1})

3.7.4 Training
Next we call the training function defined in Section 3.6 to train the model.
num_epochs = 10 d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)

61 https://en.wikipedia.org/wiki/LogSumExp
126

Chapter 3. Linear Neural Networks

As before, this algorithm converges to a solution that achieves a decent accuracy, albeit this time with fewer lines of code than before.
Summary
• Using high-level APIs, we can implement softmax regression much more concisely. • From a computational perspective, implementing softmax regression has intricacies. Note
that in many cases, a deep learning framework takes additional precautions beyond these most well-known tricks to ensure numerical stability, saving us from even more pitfalls that we would encounter if we tried to code all of our models from scratch in practice.
Exercises
1. Try adjusting the hyperparameters, such as the batch size, number of epochs, and learning rate, to see what the results are.
2. Increase the number of epochs for training. Why might the test accuracy decrease after a while? How could we fix this?
Discussions62

62 https://discuss.d2l.ai/t/52

3.7. Concise Implementation of Softmax Regression

127

128

Chapter 3. Linear Neural Networks

4 | Multilayer Perceptrons
In this chapter, we will introduce your first truly deep network. The simplest deep networks are called multilayer perceptrons, and they consist of multiple layers of neurons each fully connected to those in the layer below (from which they receive input) and those above (which they, in turn, influence). When we train high-capacity models we run the risk of overfitting. Thus, we will need to provide your first rigorous introduction to the notions of overfitting, underfitting, and model selection. To help you combat these problems, we will introduce regularization techniques such as weight decay and dropout. We will also discuss issues relating to numerical stability and parameter initialization that are key to successfully training deep networks. Throughout, we aim to give you a firm grasp not just of the concepts but also of the practice of using deep networks. At the end of this chapter, we apply what we have introduced so far to a real case: house price prediction. We punt matters relating to the computational performance, scalability, and efficiency of our models to subsequent chapters.
4.1 Multilayer Perceptrons
In Chapter 3, we introduced softmax regression (Section 3.4), implementing the algorithm from scratch (Section 3.6) and using high-level APIs (Section 3.7), and training classifiers to recognize 10 categories of clothing from low-resolution images. Along the way, we learned how to wrangle data, coerce our outputs into a valid probability distribution, apply an appropriate loss function, and minimize it with respect to our modelʼs parameters. Now that we have mastered these mechanics in the context of simple linear models, we can launch our exploration of deep neural networks, the comparatively rich class of models with which this book is primarily concerned.
4.1.1 Hidden Layers
We have described the affine transformation in Section 3.1.1, which is a linear transformation added by a bias. To begin, recall the model architecture corresponding to our softmax regression example, illustrated in Fig. 3.4.1. This model mapped our inputs directly to our outputs via a single affine transformation, followed by a softmax operation. If our labels truly were related to our input data by an affine transformation, then this approach would be sufficient. But linearity in affine transformations is a strong assumption.
129

Linear Models May Go Wrong
For example, linearity implies the weaker assumption of monotonicity: that any increase in our feature must either always cause an increase in our modelʼs output (if the corresponding weight is positive), or always cause a decrease in our modelʼs output (if the corresponding weight is negative). Sometimes that makes sense. For example, if we were trying to predict whether an individual will repay a loan, we might reasonably imagine that holding all else equal, an applicant with a higher income would always be more likely to repay than one with a lower income. While monotonic, this relationship likely is not linearly associated with the probability of repayment. An increase in income from 0 to 50 thousand likely corresponds to a bigger increase in likelihood of repayment than an increase from 1 million to 1.05 million. One way to handle this might be to preprocess our data such that linearity becomes more plausible, say, by using the logarithm of income as our feature.
Note that we can easily come up with examples that violate monotonicity. Say for example that we want to predict probability of death based on body temperature. For individuals with a body temperature above 37°C (98.6°F), higher temperatures indicate greater risk. However, for individuals with body temperatures below 37° C, higher temperatures indicate lower risk! In this case too, we might resolve the problem with some clever preprocessing. Namely, we might use the distance from 37°C as our feature.
But what about classifying images of cats and dogs? Should increasing the intensity of the pixel at location (13, 17) always increase (or always decrease) the likelihood that the image depicts a dog? Reliance on a linear model corresponds to the implicit assumption that the only requirement for differentiating cats vs. dogs is to assess the brightness of individual pixels. This approach is doomed to fail in a world where inverting an image preserves the category.
And yet despite the apparent absurdity of linearity here, as compared with our previous examples, it is less obvious that we could address the problem with a simple preprocessing fix. That is because the significance of any pixel depends in complex ways on its context (the values of the surrounding pixels). While there might exist a representation of our data that would take into account the relevant interactions among our features, on top of which a linear model would be suitable, we simply do not know how to calculate it by hand. With deep neural networks, we used observational data to jointly learn both a representation via hidden layers and a linear predictor that acts upon that representation.
Incorporating Hidden Layers
We can overcome these limitations of linear models and handle a more general class of functions by incorporating one or more hidden layers. The easiest way to do this is to stack many fullyconnected layers on top of each other. Each layer feeds into the layer above it, until we generate outputs. We can think of the first L−1 layers as our representation and the final layer as our linear predictor. This architecture is commonly called a multilayer perceptron, often abbreviated as MLP. Below, we depict an MLP diagrammatically (Fig. 4.1.1).

130

Chapter 4. Multilayer Perceptrons

Fig. 4.1.1: An MLP with a hidden layer of 5 hidden units.

This MLP has 4 inputs, 3 outputs, and its hidden layer contains 5 hidden units. Since the input layer does not involve any calculations, producing outputs with this network requires implementing the computations for both the hidden and output layers; thus, the number of layers in this MLP is 2. Note that these layers are both fully connected. Every input influences every neuron in the hidden layer, and each of these in turn influences every neuron in the output layer. However, as suggested by Section 3.4.3, the parameterization cost of MLPs with fully-connected layers can be prohibitively high, which may motivate tradeoff between parameter saving and model effectiveness even without changing the input or output size (Zhang et al., 2021).

From Linear to Nonlinear

As before, by the matrix X ∈ Rn×d, we denote a minibatch of n examples where each example has
d inputs (features). For a one-hidden-layer MLP whose hidden layer has h hidden units, denote by H ∈ Rn×h the outputs of the hidden layer, which are hidden representations. In mathematics or
code, H is also known as a hidden-layer variable or a hidden variable. Since the hidden and output layers are both fully connected, we have hidden-layer weights W(1) ∈ Rd×h and biases b(1) ∈ R1×h and output-layer weights W(2) ∈ Rh×q and biases b(2) ∈ R1×q. Formally, we calculate the outputs O ∈ Rn×q of the one-hidden-layer MLP as follows:

H = XW(1) + b(1), O = HW(2) + b(2).

(4.1.1)

Note that after adding the hidden layer, our model now requires us to track and update additional sets of parameters. So what have we gained in exchange? You might be surprised to find out that— in the model defined above—we gain nothing for our troubles! The reason is plain. The hidden units above are given by an affine function of the inputs, and the outputs (pre-softmax) are just an affine function of the hidden units. An affine function of an affine function is itself an affine function. Moreover, our linear model was already capable of representing any affine function.
We can view the equivalence formally by proving that for any values of the weights, we can just collapse out the hidden layer, yielding an equivalent single-layer model with parameters W = W(1)W(2) and b = b(1)W(2) + b(2):

O = (XW(1) + b(1))W(2) + b(2) = XW(1)W(2) + b(1)W(2) + b(2) = XW + b.

(4.1.2)

In order to realize the potential of multilayer architectures, we need one more key ingredient: a nonlinear activation function σ to be applied to each hidden unit following the affine transformation. The outputs of activation functions (e.g., σ(·)) are called activations. In general, with

4.1. Multilayer Perceptrons

131

activation functions in place, it is no longer possible to collapse our MLP into a linear model:

H = σ(XW(1) + b(1)), O = HW(2) + b(2).

(4.1.3)

Since each row in X corresponds to an example in the minibatch, with some abuse of notation, we define the nonlinearity σ to apply to its inputs in a rowwise fashion, i.e., one example at a time. Note that we used the notation for softmax in the same way to denote a rowwise operation in Section 3.4.5. Often, as in this section, the activation functions that we apply to hidden layers are not merely rowwise, but elementwise. That means that after computing the linear portion of the layer, we can calculate each activation without looking at the values taken by the other hidden units. This is true for most activation functions.
To build more general MLPs, we can continue stacking such hidden layers, e.g., H(1) = σ1(XW(1) + b(1)) and H(2) = σ2(H(1)W(2) + b(2)), one atop another, yielding ever more expressive models.

Universal Approximators
MLPs can capture complex interactions among our inputs via their hidden neurons, which depend on the values of each of the inputs. We can easily design hidden nodes to perform arbitrary computation, for instance, basic logic operations on a pair of inputs. Moreover, for certain choices of the activation function, it is widely known that MLPs are universal approximators. Even with a single-hidden-layer network, given enough nodes (possibly absurdly many), and the right set of weights, we can model any function, though actually learning that function is the hard part. You might think of your neural network as being a bit like the C programming language. The language, like any other modern language, is capable of expressing any computable program. But actually coming up with a program that meets your specifications is the hard part.
Moreover, just because a single-hidden-layer network can learn any function does not mean that you should try to solve all of your problems with single-hidden-layer networks. In fact, we can approximate many functions much more compactly by using deeper (vs. wider) networks. We will touch upon more rigorous arguments in subsequent chapters.

4.1.2 Activation Functions
Activation functions decide whether a neuron should be activated or not by calculating the weighted sum and further adding bias with it. They are differentiable operators to transform input signals to outputs, while most of them add non-linearity. Because activation functions are fundamental to deep learning, let us briefly survey some common activation functions.
%matplotlib inline from mxnet import autograd, np, npx from d2l import mxnet as d2l
npx.set_np()

132

Chapter 4. Multilayer Perceptrons

ReLU Function

The most popular choice, due to both simplicity of implementation and its good performance on a variety of predictive tasks, is the rectified linear unit (ReLU). ReLU provides a very simple nonlinear transformation. Given an element x, the function is defined as the maximum of that element and 0:

ReLU(x) = max(x, 0).

(4.1.4)

Informally, the ReLU function retains only positive elements and discards all negative elements by setting the corresponding activations to 0. To gain some intuition, we can plot the function. As you can see, the activation function is piecewise linear.
x = np.arange(-8.0, 8.0, 0.1) x.attach_grad() with autograd.record():
y = npx.relu(x) d2l.plot(x, y, 'x', 'relu(x)', figsize=(5, 2.5))

When the input is negative, the derivative of the ReLU function is 0, and when the input is positive, the derivative of the ReLU function is 1. Note that the ReLU function is not differentiable when the input takes value precisely equal to 0. In these cases, we default to the left-hand-side derivative and say that the derivative is 0 when the input is 0. We can get away with this because the input may never actually be zero. There is an old adage that if subtle boundary conditions matter, we are probably doing (real) mathematics, not engineering. That conventional wisdom may apply here. We plot the derivative of the ReLU function plotted below.
y.backward() d2l.plot(x, x.grad, 'x', 'grad of relu', figsize=(5, 2.5))

4.1. Multilayer Perceptrons

133

The reason for using ReLU is that its derivatives are particularly well behaved: either they vanish or they just let the argument through. This makes optimization better behaved and it mitigated the well-documented problem of vanishing gradients that plagued previous versions of neural networks (more on this later).
Note that there are many variants to the ReLU function, including the parameterized ReLU (pReLU) function (He et al., 2015). This variation adds a linear term to ReLU, so some information still gets through, even when the argument is negative:

pReLU(x) = max(0, x) + α min(0, x).

(4.1.5)

Sigmoid Function

The sigmoid function transforms its inputs, for which values lie in the domain R, to outputs that lie on the interval (0, 1). For that reason, the sigmoid is often called a squashing function: it squashes any input in the range (-inf, inf) to some value in the range (0, 1):

sigmoid(x) =

1 .

1 + exp(−x)

(4.1.6)

In the earliest neural networks, scientists were interested in modeling biological neurons which either fire or do not fire. Thus the pioneers of this field, going all the way back to McCulloch and Pitts, the inventors of the artificial neuron, focused on thresholding units. A thresholding activation takes value 0 when its input is below some threshold and value 1 when the input exceeds the threshold.
When attention shifted to gradient based learning, the sigmoid function was a natural choice because it is a smooth, differentiable approximation to a thresholding unit. Sigmoids are still widely used as activation functions on the output units, when we want to interpret the outputs as probabilities for binary classification problems (you can think of the sigmoid as a special case of the softmax). However, the sigmoid has mostly been replaced by the simpler and more easily trainable ReLU for most use in hidden layers. In later chapters on recurrent neural networks, we will describe architectures that leverage sigmoid units to control the flow of information across time.
Below, we plot the sigmoid function. Note that when the input is close to 0, the sigmoid function approaches a linear transformation.

134

Chapter 4. Multilayer Perceptrons

with autograd.record(): y = npx.sigmoid(x)
d2l.plot(x, y, 'x', 'sigmoid(x)', figsize=(5, 2.5))

The derivative of the sigmoid function is given by the following equation:

d

exp(−x)

dx sigmoid(x) = (1 + exp(−x))2 = sigmoid(x) (1 − sigmoid(x)) .

(4.1.7)

The derivative of the sigmoid function is plotted below. Note that when the input is 0, the derivative of the sigmoid function reaches a maximum of 0.25. As the input diverges from 0 in either direction, the derivative approaches 0.

y.backward() d2l.plot(x, x.grad, 'x', 'grad of sigmoid', figsize=(5, 2.5))

4.1. Multilayer Perceptrons

135

Tanh Function

Like the sigmoid function, the tanh (hyperbolic tangent) function also squashes its inputs, transforming them into elements on the interval between -1 and 1:

tanh(x) = 1 − exp(−2x) . 1 + exp(−2x)

(4.1.8)

We plot the tanh function below. Note that as the input nears 0, the tanh function approaches a linear transformation. Although the shape of the function is similar to that of the sigmoid function, the tanh function exhibits point symmetry about the origin of the coordinate system.

with autograd.record(): y = np.tanh(x)
d2l.plot(x, y, 'x', 'tanh(x)', figsize=(5, 2.5))

The derivative of the tanh function is:

d tanh(x) = 1 − tanh2(x). dx

(4.1.9)

The derivative of tanh function is plotted below. As the input nears 0, the derivative of the tanh function approaches a maximum of 1. And as we saw with the sigmoid function, as the input moves away from 0 in either direction, the derivative of the tanh function approaches 0.

y.backward() d2l.plot(x, x.grad, 'x', 'grad of tanh', figsize=(5, 2.5))

136

Chapter 4. Multilayer Perceptrons

In summary, we now know how to incorporate nonlinearities to build expressive multilayer neural network architectures. As a side note, your knowledge already puts you in command of a similar toolkit to a practitioner circa 1990. In some ways, you have an advantage over anyone working in the 1990s, because you can leverage powerful open-source deep learning frameworks to build models rapidly, using only a few lines of code. Previously, training these networks required researchers to code up thousands of lines of C and Fortran.
Summary
• MLP adds one or multiple fully-connected hidden layers between the output and input layers and transforms the output of the hidden layer via an activation function.
• Commonly-used activation functions include the ReLU function, the sigmoid function, and the tanh function.
Exercises
1. Compute the derivative of the pReLU activation function. 2. Show that an MLP using only ReLU (or pReLU) constructs a continuous piecewise linear
function. 3. Show that tanh(x) + 1 = 2 sigmoid(2x). 4. Assume that we have a nonlinearity that applies to one minibatch at a time. What kinds of
problems do you expect this to cause? Discussions63
63 https://discuss.d2l.ai/t/90

4.1. Multilayer Perceptrons

137

4.2 Implementation of Multilayer Perceptrons from Scratch
Now that we have characterized multilayer perceptrons (MLPs) mathematically, let us try to implement one ourselves. To compare against our previous results achieved with softmax regression (Section 3.6), we will continue to work with the Fashion-MNIST image classification dataset (Section 3.5).
from mxnet import gluon, np, npx from d2l import mxnet as d2l
npx.set_np()
batch_size = 256 train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)
4.2.1 Initializing Model Parameters
Recall that Fashion-MNIST contains 10 classes, and that each image consists of a 28×28 = 784 grid of grayscale pixel values. Again, we will disregard the spatial structure among the pixels for now, so we can think of this as simply a classification dataset with 784 input features and 10 classes. To begin, we will implement an MLP with one hidden layer and 256 hidden units. Note that we can regard both of these quantities as hyperparameters. Typically, we choose layer widths in powers of 2, which tend to be computationally efficient because of how memory is allocated and addressed in hardware. Again, we will represent our parameters with several tensors. Note that for every layer, we must keep track of one weight matrix and one bias vector. As always, we allocate memory for the gradients of the loss with respect to these parameters.
num_inputs, num_outputs, num_hiddens = 784, 10, 256
W1 = np.random.normal(scale=0.01, size=(num_inputs, num_hiddens)) b1 = np.zeros(num_hiddens) W2 = np.random.normal(scale=0.01, size=(num_hiddens, num_outputs)) b2 = np.zeros(num_outputs) params = [W1, b1, W2, b2]
for param in params: param.attach_grad()
4.2.2 Activation Function
To make sure we know how everything works, we will implement the ReLU activation ourselves using the maximum function rather than invoking the built-in relu function directly.
def relu(X): return np.maximum(X, 0)

138

Chapter 4. Multilayer Perceptrons

4.2.3 Model
Because we are disregarding spatial structure, we reshape each two-dimensional image into a flat vector of length num_inputs. Finally, we implement our model with just a few lines of code.
def net(X): X = X.reshape((-1, num_inputs)) H = relu(np.dot(X, W1) + b1) return np.dot(H, W2) + b2
4.2.4 Loss Function
To ensure numerical stability, and because we already implemented the softmax function from scratch (Section 3.6), we leverage the integrated function from high-level APIs for calculating the softmax and cross-entropy loss. Recall our earlier discussion of these intricacies in Section 3.7.2. We encourage the interested reader to examine the source code for the loss function to deepen their knowledge of implementation details.
loss = gluon.loss.SoftmaxCrossEntropyLoss()
4.2.5 Training
Fortunately, the training loop for MLPs is exactly the same as for softmax regression. Leveraging the d2l package again, we call the train_ch3 function (see Section 3.6), setting the number of epochs to 10 and the learning rate to 0.1.
num_epochs, lr = 10, 0.1 d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs,
lambda batch_size: d2l.sgd(params, lr, batch_size))

To evaluate the learned model, we apply it on some test data. d2l.predict_ch3(net, test_iter)

4.2. Implementation of Multilayer Perceptrons from Scratch

139

Summary
• We saw that implementing a simple MLP is easy, even when done manually. • However, with a large number of layers, implementing MLPs from scratch can still get messy
(e.g., naming and keeping track of our modelʼs parameters).
Exercises
1. Change the value of the hyperparameter num_hiddens and see how this hyperparameter influences your results. Determine the best value of this hyperparameter, keeping all others constant.
2. Try adding an additional hidden layer to see how it affects the results. 3. How does changing the learning rate alter your results? Fixing the model architecture and
other hyperparameters (including number of epochs), what learning rate gives you the best results? 4. What is the best result you can get by optimizing over all the hyperparameters (learning rate, number of epochs, number of hidden layers, number of hidden units per layer) jointly? 5. Describe why it is much more challenging to deal with multiple hyperparameters. 6. What is the smartest strategy you can think of for structuring a search over multiple hyperparameters? Discussions64
4.3 Concise Implementation of Multilayer Perceptrons
As you might expect, by relying on the high-level APIs, we can implement MLPs even more concisely.
from mxnet import gluon, init, npx from mxnet.gluon import nn from d2l import mxnet as d2l
npx.set_np()
64 https://discuss.d2l.ai/t/92

140

Chapter 4. Multilayer Perceptrons

4.3.1 Model
As compared with our concise implementation of softmax regression implementation (Section 3.7), the only difference is that we add two fully-connected layers (previously, we added one). The first is our hidden layer, which contains 256 hidden units and applies the ReLU activation function. The second is our output layer.
net = nn.Sequential() net.add(nn.Dense(256, activation='relu'), nn.Dense(10)) net.initialize(init.Normal(sigma=0.01))
The training loop is exactly the same as when we implemented softmax regression. This modularity enables us to separate matters concerning the model architecture from orthogonal considerations.
batch_size, lr, num_epochs = 256, 0.1, 10 loss = gluon.loss.SoftmaxCrossEntropyLoss() trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': lr})
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size) d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)

4.3. Concise Implementation of Multilayer Perceptrons

141

Summary
• Using high-level APIs, we can implement MLPs much more concisely.
• For the same classification problem, the implementation of an MLP is the same as that of softmax regression except for additional hidden layers with activation functions.
Exercises
1. Try adding different numbers of hidden layers (you may also modify the learning rate). What setting works best?
2. Try out different activation functions. Which one works best?
3. Try different schemes for initializing the weights. What method works best? Discussions65
4.4 Model Selection, Underfitting, and Overfitting
As machine learning scientists, our goal is to discover patterns. But how can we be sure that we have truly discovered a general pattern and not simply memorized our data? For example, imagine that we wanted to hunt for patterns among genetic markers linking patients to their dementia status, where the labels are drawn from the set {dementia, mild cognitive impairment, healthy}. Because each personʼs genes identify them uniquely (ignoring identical siblings), it is possible to memorize the entire dataset.
We do not want our model to say “That’s Bob! I remember him! He has dementia!” The reason why is simple. When we deploy the model in the future, we will encounter patients that the model has never seen before. Our predictions will only be useful if our model has truly discovered a general pattern.
To recapitulate more formally, our goal is to discover patterns that capture regularities in the underlying population from which our training set was drawn. If we are successful in this endeavor, then we could successfully assess risk even for individuals that we have never encountered before. This problem—how to discover patterns that generalize—is the fundamental problem of machine learning.
The danger is that when we train models, we access just a small sample of data. The largest public image datasets contain roughly one million images. More often, we must learn from only thousands or tens of thousands of data examples. In a large hospital system, we might access hundreds of thousands of medical records. When working with finite samples, we run the risk that we might discover apparent associations that turn out not to hold up when we collect more data.
The phenomenon of fitting our training data more closely than we fit the underlying distribution is called overfitting, and the techniques used to combat overfitting are called regularization. In the previous sections, you might have observed this effect while experimenting with the FashionMNIST dataset. If you altered the model structure or the hyperparameters during the experiment, you might have noticed that with enough neurons, layers, and training epochs, the model can eventually reach perfect accuracy on the training set, even as the accuracy on test data deteriorates.
65 https://discuss.d2l.ai/t/94

142

Chapter 4. Multilayer Perceptrons

4.4.1 Training Error and Generalization Error

In order to discuss this phenomenon more formally, we need to differentiate between training error and generalization error. The training error is the error of our model as calculated on the training dataset, while generalization error is the expectation of our modelʼs error were we to apply it to an infinite stream of additional data examples drawn from the same underlying data distribution as our original sample.

Problematically, we can never calculate the generalization error exactly. That is because the stream of infinite data is an imaginary object. In practice, we must estimate the generalization error by applying our model to an independent test set constituted of a random selection of data examples that were withheld from our training set.

The following three thought experiments will help illustrate this situation better. Consider a college student trying to prepare for his final exam. A diligent student will strive to practice well and test his abilities using exams from previous years. Nonetheless, doing well on past exams is no guarantee that he will excel when it matters. For instance, the student might try to prepare by rote learning the answers to the exam questions. This requires the student to memorize many things. She might even remember the answers for past exams perfectly. Another student might prepare by trying to understand the reasons for giving certain answers. In most cases, the latter student will do much better.

Likewise, consider a model that simply uses a lookup table to answer questions. If the set of allowable inputs is discrete and reasonably small, then perhaps after viewing many training examples, this approach would perform well. Still this model has no ability to do better than random guessing when faced with examples that it has never seen before. In reality the input spaces are far too large to memorize the answers corresponding to every conceivable input. For example, consider the black and white 28 × 28 images. If each pixel can take one among 256 grayscale values, then there are 256784 possible images. That means that there are far more low-resolution grayscale thumbnail-sized images than there are atoms in the universe. Even if we could encounter such data, we could never afford to store the lookup table.

Last, consider the problem of trying to classify the outcomes of coin tosses (class 0: heads, class

1: tails) based on some contextual features that might be available. Suppose that the coin is fair.

No matter what algorithm we come up with, the generalization error will always be 21 . However, for most algorithms, we should expect our training error to be considerably lower, depending on

the luck of the draw, even if we did not have any features! Consider the dataset {0, 1, 1, 1, 0, 1}.

Our feature-less algorithm would have to fall back on always predicting the majority class, which

appears from our limited sample to be 1. In this case, the model that always predicts class 1 will

incur an error of 31 , considerably better than our generalization error. As we increase the amount

of

data,

the

probability

that

the

fraction

of

heads

will

deviate

significantly

from

1 2

diminishes,

and

our training error would come to match the generalization error.

4.4. Model Selection, Underfitting, and Overfitting

143

Statistical Learning Theory
Since generalization is the fundamental problem in machine learning, you might not be surprised to learn that many mathematicians and theorists have dedicated their lives to developing formal theories to describe this phenomenon. In their eponymous theorem66, Glivenko and Cantelli derived the rate at which the training error converges to the generalization error. In a series of seminal papers, Vapnik and Chervonenkis67 extended this theory to more general classes of functions. This work laid the foundations of statistical learning theory.
In the standard supervised learning setting, which we have addressed up until now and will stick with throughout most of this book, we assume that both the training data and the test data are drawn independently from identical distributions. This is commonly called the i.i.d. assumption, which means that the process that samples our data has no memory. In other words, the second example drawn and the third drawn are no more correlated than the second and the two-millionth sample drawn.
Being a good machine learning scientist requires thinking critically, and already you should be poking holes in this assumption, coming up with common cases where the assumption fails. What if we train a mortality risk predictor on data collected from patients at UCSF Medical Center, and apply it on patients at Massachusetts General Hospital? These distributions are simply not identical. Moreover, draws might be correlated in time. What if we are classifying the topics of Tweets? The news cycle would create temporal dependencies in the topics being discussed, violating any assumptions of independence.
Sometimes we can get away with minor violations of the i.i.d. assumption and our models will continue to work remarkably well. After all, nearly every real-world application involves at least some minor violation of the i.i.d. assumption, and yet we have many useful tools for various applications such as face recognition, speech recognition, and language translation.
Other violations are sure to cause trouble. Imagine, for example, if we try to train a face recognition system by training it exclusively on university students and then want to deploy it as a tool for monitoring geriatrics in a nursing home population. This is unlikely to work well since college students tend to look considerably different from the elderly.
In subsequent chapters, we will discuss problems arising from violations of the i.i.d. assumption. For now, even taking the i.i.d. assumption for granted, understanding generalization is a formidable problem. Moreover, elucidating the precise theoretical foundations that might explain why deep neural networks generalize as well as they do continues to vex the greatest minds in learning theory.
When we train our models, we attempt to search for a function that fits the training data as well as possible. If the function is so flexible that it can catch on to spurious patterns just as easily as to true associations, then it might perform too well without producing a model that generalizes well to unseen data. This is precisely what we want to avoid or at least control. Many of the techniques in deep learning are heuristics and tricks aimed at guarding against overfitting.
66 https://en.wikipedia.org/wiki/Glivenko%E2%80%93Cantelli_theorem 67 https://en.wikipedia.org/wiki/Vapnik%E2%80%93Chervonenkis_theory

144

Chapter 4. Multilayer Perceptrons

Model Complexity
When we have simple models and abundant data, we expect the generalization error to resemble the training error. When we work with more complex models and fewer examples, we expect the training error to go down but the generalization gap to grow. What precisely constitutes model complexity is a complex matter. Many factors govern whether a model will generalize well. For example a model with more parameters might be considered more complex. A model whose parameters can take a wider range of values might be more complex. Often with neural networks, we think of a model that takes more training iterations as more complex, and one subject to early stopping (fewer training iterations) as less complex.
It can be difficult to compare the complexity among members of substantially different model classes (say, decision trees vs. neural networks). For now, a simple rule of thumb is quite useful: a model that can readily explain arbitrary facts is what statisticians view as complex, whereas one that has only a limited expressive power but still manages to explain the data well is probably closer to the truth. In philosophy, this is closely related to Popperʼs criterion of falsifiability of a scientific theory: a theory is good if it fits data and if there are specific tests that can be used to disprove it. This is important since all statistical estimation is post hoc, i.e., we estimate after we observe the facts, hence vulnerable to the associated fallacy. For now, we will put the philosophy aside and stick to more tangible issues.
In this section, to give you some intuition, we will focus on a few factors that tend to influence the generalizability of a model class:
1. The number of tunable parameters. When the number of tunable parameters, sometimes called the degrees of freedom, is large, models tend to be more susceptible to overfitting.
2. The values taken by the parameters. When weights can take a wider range of values, models can be more susceptible to overfitting.
3. The number of training examples. It is trivially easy to overfit a dataset containing only one or two examples even if your model is simple. But overfitting a dataset with millions of examples requires an extremely flexible model.
4.4.2 Model Selection
In machine learning, we usually select our final model after evaluating several candidate models. This process is called model selection. Sometimes the models subject to comparison are fundamentally different in nature (say, decision trees vs. linear models). At other times, we are comparing members of the same class of models that have been trained with different hyperparameter settings.
With MLPs, for example, we may wish to compare models with different numbers of hidden layers, different numbers of hidden units, and various choices of the activation functions applied to each hidden layer. In order to determine the best among our candidate models, we will typically employ a validation dataset.

4.4. Model Selection, Underfitting, and Overfitting

145

Validation Dataset
In principle we should not touch our test set until after we have chosen all our hyperparameters. Were we to use the test data in the model selection process, there is a risk that we might overfit the test data. Then we would be in serious trouble. If we overfit our training data, there is always the evaluation on test data to keep us honest. But if we overfit the test data, how would we ever know?
Thus, we should never rely on the test data for model selection. And yet we cannot rely solely on the training data for model selection either because we cannot estimate the generalization error on the very data that we use to train the model.
In practical applications, the picture gets muddier. While ideally we would only touch the test data once, to assess the very best model or to compare a small number of models to each other, real-world test data is seldom discarded after just one use. We can seldom afford a new test set for each round of experiments.
The common practice to address this problem is to split our data three ways, incorporating a validation dataset (or validation set) in addition to the training and test datasets. The result is a murky practice where the boundaries between validation and test data are worryingly ambiguous. Unless explicitly stated otherwise, in the experiments in this book we are really working with what should rightly be called training data and validation data, with no true test sets. Therefore, the accuracy reported in each experiment of the book is really the validation accuracy and not a true test set accuracy.
K-Fold Cross-Validation
When training data is scarce, we might not even be able to afford to hold out enough data to constitute a proper validation set. One popular solution to this problem is to employ K-fold crossvalidation. Here, the original training data is split into K non-overlapping subsets. Then model training and validation are executed K times, each time training on K − 1 subsets and validating on a different subset (the one not used for training in that round). Finally, the training and validation errors are estimated by averaging over the results from the K experiments.
4.4.3 Underfitting or Overfitting?
When we compare the training and validation errors, we want to be mindful of two common situations. First, we want to watch out for cases when our training error and validation error are both substantial but there is a little gap between them. If the model is unable to reduce the training error, that could mean that our model is too simple (i.e., insufficiently expressive) to capture the pattern that we are trying to model. Moreover, since the generalization gap between our training and validation errors is small, we have reason to believe that we could get away with a more complex model. This phenomenon is known as underfitting.
On the other hand, as we discussed above, we want to watch out for the cases when our training error is significantly lower than our validation error, indicating severe overfitting. Note that overfitting is not always a bad thing. With deep learning especially, it is well known that the best predictive models often perform far better on training data than on holdout data. Ultimately, we usually care more about the validation error than about the gap between the training and validation errors.

146

Chapter 4. Multilayer Perceptrons

Whether we overfit or underfit can depend both on the complexity of our model and the size of the available training datasets, two topics that we discuss below.

Model Complexity

To illustrate some classical intuition about overfitting and model complexity, we give an example using polynomials. Given training data consisting of a single feature x and a corresponding realvalued label y, we try to find the polynomial of degree d

∑d yˆ = xiwi
i=0

(4.4.1)

to estimate the labels y. This is just a linear regression problem where our features are given by the powers of x, the modelʼs weights are given by wi, and the bias is given by w0 since x0 = 1 for all x. Since this is just a linear regression problem, we can use the squared error as our loss function.
A higher-order polynomial function is more complex than a lower-order polynomial function, since the higher-order polynomial has more parameters and the model functionʼs selection range is wider. Fixing the training dataset, higher-order polynomial functions should always achieve lower (at worst, equal) training error relative to lower degree polynomials. In fact, whenever the data examples each have a distinct value of x, a polynomial function with degree equal to the number of data examples can fit the training set perfectly. We visualize the relationship between polynomial degree and underfitting vs. overfitting in Fig. 4.4.1.

Fig. 4.4.1: Influence of model complexity on underfitting and overfitting
Dataset Size
The other big consideration to bear in mind is the dataset size. Fixing our model, the fewer samples we have in the training dataset, the more likely (and more severely) we are to encounter overfitting. As we increase the amount of training data, the generalization error typically decreases. Moreover, in general, more data never hurt. For a fixed task and data distribution, there is typically a relationship between model complexity and dataset size. Given more data, we might profitably attempt to fit a more complex model. Absent sufficient data, simpler models may be more

4.4. Model Selection, Underfitting, and Overfitting

147

difficult to beat. For many tasks, deep learning only outperforms linear models when many thousands of training examples are available. In part, the current success of deep learning owes to the current abundance of massive datasets due to Internet companies, cheap storage, connected devices, and the broad digitization of the economy.
4.4.4 Polynomial Regression
We can now explore these concepts interactively by fitting polynomials to data.
import math from mxnet import gluon, np, npx from mxnet.gluon import nn from d2l import mxnet as d2l
npx.set_np()

Generating the Dataset

First we need data. Given x, we will use the following cubic polynomial to generate the labels on training and test data:

y

=

5

+

1.2x

−

x2 3.4

+

x3 5.6

+

ϵ

where

ϵ

∼

N (0, 0.12).

2!

3!

(4.4.2)

The noise term ϵ obeys a normal distribution with a mean of 0 and a standard deviation of 0.1. For
optimization, we typically want to avoid very large values of gradients or losses. This is why the features are rescaled from xi to xi!i . It allows us to avoid very large values for large exponents i. We will synthesize 100 samples each for the training set and test set.

max_degree = 20 # Maximum degree of the polynomial n_train, n_test = 100, 100 # Training and test dataset sizes true_w = np.zeros(max_degree) # Allocate lots of empty space true_w[0:4] = np.array([5, 1.2, -3.4, 5.6])

features = np.random.normal(size=(n_train + n_test, 1)) np.random.shuffle(features) poly_features = np.power(features, np.arange(max_degree).reshape(1, -1)) for i in range(max_degree):
poly_features[:, i] /= math.gamma(i + 1) # `gamma(n)` = (n-1)! # Shape of `labels`: (`n_train` + `n_test`,) labels = np.dot(poly_features, true_w) labels += np.random.normal(scale=0.1, size=labels.shape)

Again, monomials stored in poly_features are rescaled by the gamma function, where Γ(n) = (n − 1)!. Take a look at the first 2 samples from the generated dataset. The value 1 is technically a feature, namely the constant feature corresponding to the bias.
features[:2], poly_features[:2, :], labels[:2]

148

Chapter 4. Multilayer Perceptrons

(array([[-0.03716067], [-1.1468065 ]]),
array([[ 1.0000000e+00, -3.7160669e-02, 7.9455290e-08, -5.9052235e-10, 9.0187767e-17, -3.7238198e-19, 1.4476556e-26, -4.1381425e-29, 6.3199942e-37, -1.3815009e-39,
[ 1.0000000e+00, -1.1468065e+00, 7.2069131e-02, -1.6529869e-02, 7.4199430e-05, -9.4547095e-06, 1.0803007e-08, -9.5299690e-10, 4.2778208e-13, -2.8857840e-14,
array([ 5.1432443 , -0.06415121]))

6.9045764e-04, 3.6573678e-12, 1.3837962e-21, 1.0984010e-31, 2.8516424e-42, 6.5758252e-01, 3.1594271e-03, 1.0842722e-06, 7.8064499e-11, 1.8385756e-15,

-8.5526226e-06, -1.9415747e-14, -4.6747992e-24, -2.7211542e-34, -5.6051939e-45], -2.5137332e-01, -5.1760738e-04, -1.1304095e-07, -5.9683248e-12, -1.1097316e-16]]),

Training and Testing the Model
Let us first implement a function to evaluate the loss on a given dataset.
def evaluate_loss(net, data_iter, loss): #@save """Evaluate the loss of a model on the given dataset.""" metric = d2l.Accumulator(2) # Sum of losses, no. of examples for X, y in data_iter: l = loss(net(X), y) metric.add(l.sum(), l.size) return metric[0] / metric[1]
Now define the training function.
def train(train_features, test_features, train_labels, test_labels, num_epochs=400):
loss = gluon.loss.L2Loss() net = nn.Sequential() # Switch off the bias since we already catered for it in the polynomial # features net.add(nn.Dense(1, use_bias=False)) net.initialize() batch_size = min(10, train_labels.shape[0]) train_iter = d2l.load_array((train_features, train_labels), batch_size) test_iter = d2l.load_array((test_features, test_labels), batch_size,
is_train=False) trainer = gluon.Trainer(net.collect_params(), 'sgd',
{'learning_rate': 0.01}) animator = d2l.Animator(xlabel='epoch', ylabel='loss', yscale='log',
xlim=[1, num_epochs], ylim=[1e-3, 1e2], legend=['train', 'test']) for epoch in range(num_epochs): d2l.train_epoch_ch3(net, train_iter, loss, trainer) if epoch == 0 or (epoch + 1) % 20 == 0: animator.add(epoch + 1, (evaluate_loss( net, train_iter, loss), evaluate_loss(net, test_iter, loss))) print('weight:', net[0].weight.data().asnumpy())

4.4. Model Selection, Underfitting, and Overfitting

149

Third-Order Polynomial Function Fitting (Normal)
We will begin by first using a third-order polynomial function, which is the same order as that of the data generation function. The results show that this modelʼs training and test losses can be both effectively reduced. The learned model parameters are also close to the true values w = [5, 1.2, −3.4, 5.6].
# Pick the first four dimensions, i.e., 1, x, x^2/2!, x^3/3! from the # polynomial features train(poly_features[:n_train, :4], poly_features[n_train:, :4],
labels[:n_train], labels[n_train:])
weight: [[ 5.019119 1.2220801 -3.4236798 5.5717764]]

Linear Function Fitting (Underfitting)
Let us take another look at linear function fitting. After the decline in early epochs, it becomes difficult to further decrease this modelʼs training loss. After the last epoch iteration has been completed, the training loss is still high. When used to fit nonlinear patterns (like the third-order polynomial function here) linear models are liable to underfit.
# Pick the first two dimensions, i.e., 1, x, from the polynomial features train(poly_features[:n_train, :2], poly_features[n_train:, :2],
labels[:n_train], labels[n_train:])
weight: [[2.6995673 4.2320466]]

150

Chapter 4. Multilayer Perceptrons

Higher-Order Polynomial Function Fitting (Overfitting)
Now let us try to train the model using a polynomial of too high degree. Here, there are insufficient data to learn that the higher-degree coefficients should have values close to zero. As a result, our overly-complex model is so susceptible that it is being influenced by noise in the training data. Though the training loss can be effectively reduced, the test loss is still much higher. It shows that the complex model overfits the data.
# Pick all the dimensions from the polynomial features train(poly_features[:n_train, :], poly_features[n_train:, :],
labels[:n_train], labels[n_train:], num_epochs=1500)
weight: [[ 4.992266 1.3058255 -3.353147 5.1167803 -0.11118089 1.3027221 0.1269997 0.1663855 0.05134555 -0.02277429 0.00806731 -0.05168024 -0.02426265 -0.0150221 -0.04941353 0.0638986 -0.04761845 -0.04380166 -0.05188227 0.05655775]]

In the subsequent sections, we will continue to discuss overfitting problems and methods for dealing with them, such as weight decay and dropout.

4.4. Model Selection, Underfitting, and Overfitting

151

Summary
• Since the generalization error cannot be estimated based on the training error, simply minimizing the training error will not necessarily mean a reduction in the generalization error. Machine learning models need to be careful to safeguard against overfitting so as to minimize the generalization error.
• A validation set can be used for model selection, provided that it is not used too liberally.
• Underfitting means that a model is not able to reduce the training error. When training error is much lower than validation error, there is overfitting.
• We should choose an appropriately complex model and avoid using insufficient training samples.
Exercises
1. Can you solve the polynomial regression problem exactly? Hint: use linear algebra.
2. Consider model selection for polynomials:
1. Plot the training loss vs. model complexity (degree of the polynomial). What do you observe? What degree of polynomial do you need to reduce the training loss to 0?
2. Plot the test loss in this case.
3. Generate the same plot as a function of the amount of data. 3. What happens if you drop the normalization (1/i!) of the polynomial features xi? Can you
fix this in some other way?
4. Can you ever expect to see zero generalization error? Discussions68
4.5 Weight Decay
Now that we have characterized the problem of overfitting, we can introduce some standard techniques for regularizing models. Recall that we can always mitigate overfitting by going out and collecting more training data. That can be costly, time consuming, or entirely out of our control, making it impossible in the short run. For now, we can assume that we already have as much high-quality data as our resources permit and focus on regularization techniques.
Recall that in our polynomial regression example (Section 4.4) we could limit our modelʼs capacity simply by tweaking the degree of the fitted polynomial. Indeed, limiting the number of features is a popular technique to mitigate overfitting. However, simply tossing aside features can be too blunt an instrument for the job. Sticking with the polynomial regression example, consider what might happen with high-dimensional inputs. The natural extensions of polynomials to multivariate data are called monomials, which are simply products of powers of variables. The degree of a monomial is the sum of the powers. For example, x21x2, and x3x25 are both monomials of degree 3.
68 https://discuss.d2l.ai/t/96

152

Chapter 4. Multilayer Perceptrons

Note that the number of terms with degree d blows up rapidly as d(grows)larger. Given k variables,

the number of monomials of degree d (i.e., k multichoose d) is

k−1+d k−1

.

Even small changes in

degree, say from 2 to 3, dramatically increase the complexity of our model. Thus we often need a

more fine-grained tool for adjusting function complexity.

4.5.1 Norms and Weight Decay

We have described both the L2 norm and the L1 norm, which are special cases of the more general Lp norm in Section 2.3.10. Weight decay (commonly called L2 regularization), might be the most widely-used technique for regularizing parametric machine learning models. The technique is motivated by the basic intuition that among all functions f , the function f = 0 (assigning the value 0 to all inputs) is in some sense the simplest, and that we can measure the complexity of a function by its distance from zero. But how precisely should we measure the distance between a function and zero? There is no single right answer. In fact, entire branches of mathematics, including parts of functional analysis and the theory of Banach spaces, are devoted to answering this issue.

One simple interpretation might be to measure the complexity of a linear function f (x) = w⊤x by some norm of its weight vector, e.g., ∥w∥2. The most common method for ensuring a small weight vector is to add its norm as a penalty term to the problem of minimizing the loss. Thus we replace our original objective, minimizing the prediction loss on the training labels, with new objective, minimizing the sum of the prediction loss and the penalty term. Now, if our weight vector grows too large, our learning algorithm might focus on minimizing the weight norm ∥w∥2 vs. minimizing the training error. That is exactly what we want. To illustrate things in code, let us revive our previous example from Section 3.1 for linear regression. There, our loss was given by

L(w, b) = 1 ∑n 1 (w⊤x(i) + b − y(i))2 . n2
i=1

(4.5.1)

Recall that x(i) are the features, y(i) are labels for all data examples i, and (w, b) are the weight and bias parameters, respectively. To penalize the size of the weight vector, we must somehow add ∥w∥2 to the loss function, but how should the model trade off the standard loss for this new additive penalty? In practice, we characterize this tradeoff via the regularization constant λ, a nonnegative hyperparameter that we fit using validation data:

L(w, b) + λ ∥w∥2, 2

(4.5.2)

For λ = 0, we recover our original loss function. For λ > 0, we restrict the size of ∥w∥. We divide by 2 by convention: when we take the derivative of a quadratic function, the 2 and 1/2 cancel out, ensuring that the expression for the update looks nice and simple. The astute reader might wonder why we work with the squared norm and not the standard norm (i.e., the Euclidean distance). We do this for computational convenience. By squaring the L2 norm, we remove the square root, leaving the sum of squares of each component of the weight vector. This makes the derivative of the penalty easy to compute: the sum of derivatives equals the derivative of the sum.

Moreover, you might ask why we work with the L2 norm in the first place and not, say, the L1 norm. In fact, other choices are valid and popular throughout statistics. While L2-regularized linear models constitute the classic ridge regression algorithm, L1-regularized linear regression is a similarly fundamental model in statistics, which is popularly known as lasso regression.

One reason to work with the L2 norm is that it places an outsize penalty on large components of the weight vector. This biases our learning algorithm towards models that distribute weight evenly

4.5. Weight Decay

153

across a larger number of features. In practice, this might make them more robust to measurement error in a single variable. By contrast, L1 penalties lead to models that concentrate weights on a small set of features by clearing the other weights to zero. This is called feature selection, which may be desirable for other reasons.

Using the same notation in (3.1.10), the minibatch stochastic gradient descent updates for L2-

regularized regression follow:

w ← (1 − ηλ) w − η ∑ x(i) (w⊤x(i) + b − y(i)) . |B| i∈B

(4.5.3)

As before, we update w based on the amount by which our estimate differs from the observation. However, we also shrink the size of w towards zero. That is why the method is sometimes called “weight decay”: given the penalty term alone, our optimization algorithm decays the weight at each step of training. In contrast to feature selection, weight decay offers us a continuous mechanism for adjusting the complexity of a function. Smaller values of λ correspond to less constrained w, whereas larger values of λ constrain w more considerably.
Whether we include a corresponding bias penalty b2 can vary across implementations, and may vary across layers of a neural network. Often, we do not regularize the bias term of a networkʼs output layer.

4.5.2 High-Dimensional Linear Regression
We can illustrate the benefits of weight decay through a simple synthetic example.
%matplotlib inline from mxnet import autograd, gluon, init, np, npx from mxnet.gluon import nn from d2l import mxnet as d2l
npx.set_np()

First, we generate some data as before

∑d y = 0.05 + 0.01xi + ϵ where ϵ ∼ N (0, 0.012).
i=1

(4.5.4)

We choose our label to be a linear function of our inputs, corrupted by Gaussian noise with zero mean and standard deviation 0.01. To make the effects of overfitting pronounced, we can increase the dimensionality of our problem to d = 200 and work with a small training set containing only 20 examples.

n_train, n_test, num_inputs, batch_size = 20, 100, 200, 5 true_w, true_b = np.ones((num_inputs, 1)) * 0.01, 0.05 train_data = d2l.synthetic_data(true_w, true_b, n_train) train_iter = d2l.load_array(train_data, batch_size) test_data = d2l.synthetic_data(true_w, true_b, n_test) test_iter = d2l.load_array(test_data, batch_size, is_train=False)

154

Chapter 4. Multilayer Perceptrons

4.5.3 Implementation from Scratch
In the following, we will implement weight decay from scratch, simply by adding the squared L2 penalty to the original target function.
Initializing Model Parameters
First, we will define a function to randomly initialize our model parameters.
def init_params(): w = np.random.normal(scale=1, size=(num_inputs, 1)) b = np.zeros(1) w.attach_grad() b.attach_grad() return [w, b]
Defining L2 Norm Penalty
Perhaps the most convenient way to implement this penalty is to square all terms in place and sum them up.
def l2_penalty(w): return (w**2).sum() / 2
Defining the Training Loop
The following code fits a model on the training set and evaluates it on the test set. The linear network and the squared loss have not changed since Chapter 3, so we will just import them via d2l.linreg and d2l.squared_loss. The only change here is that our loss now includes the penalty term.
def train(lambd): w, b = init_params() net, loss = lambda X: d2l.linreg(X, w, b), d2l.squared_loss num_epochs, lr = 100, 0.003 animator = d2l.Animator(xlabel='epochs', ylabel='loss', yscale='log', xlim=[5, num_epochs], legend=['train', 'test']) for epoch in range(num_epochs): for X, y in train_iter: with autograd.record(): # The L2 norm penalty term has been added, and broadcasting # makes `l2_penalty(w)` a vector whose length is `batch_size` l = loss(net(X), y) + lambd * l2_penalty(w) l.backward() d2l.sgd([w, b], lr, batch_size) if (epoch + 1) % 5 == 0: animator.add(epoch + 1, (d2l.evaluate_loss(net, train_iter, loss), d2l.evaluate_loss(net, test_iter, loss))) print('L2 norm of w:', np.linalg.norm(w))

4.5. Weight Decay

155

Training without Regularization We now run this code with lambd = 0, disabling weight decay. Note that we overfit badly, decreasing the training error but not the test error—a textbook case of overfitting. train(lambd=0) L2 norm of w: 13.259392
Using Weight Decay Below, we run with substantial weight decay. Note that the training error increases but the test error decreases. This is precisely the effect we expect from regularization. train(lambd=3) L2 norm of w: 0.38250616

156

Chapter 4. Multilayer Perceptrons

4.5.4 Concise Implementation
Because weight decay is ubiquitous in neural network optimization, the deep learning framework makes it especially convenient, integrating weight decay into the optimization algorithm itself for easy use in combination with any loss function. Moreover, this integration serves a computational benefit, allowing implementation tricks to add weight decay to the algorithm, without any additional computational overhead. Since the weight decay portion of the update depends only on the current value of each parameter, the optimizer must touch each parameter once anyway.
In the following code, we specify the weight decay hyperparameter directly through wd when instantiating our Trainer. By default, Gluon decays both weights and biases simultaneously. Note that the hyperparameter wd will be multiplied by wd_mult when updating model parameters. Thus, if we set wd_mult to zero, the bias parameter b will not decay.
def train_concise(wd): net = nn.Sequential() net.add(nn.Dense(1)) net.initialize(init.Normal(sigma=1)) loss = gluon.loss.L2Loss() num_epochs, lr = 100, 0.003 trainer = gluon.Trainer(net.collect_params(), 'sgd', { 'learning_rate': lr, 'wd': wd}) # The bias parameter has not decayed. Bias names generally end with "bias" net.collect_params('.*bias').setattr('wd_mult', 0) animator = d2l.Animator(xlabel='epochs', ylabel='loss', yscale='log', xlim=[5, num_epochs], legend=['train', 'test']) for epoch in range(num_epochs): for X, y in train_iter: with autograd.record(): l = loss(net(X), y) l.backward() trainer.step(batch_size) if (epoch + 1) % 5 == 0: animator.add(epoch + 1, (d2l.evaluate_loss(net, train_iter, loss), d2l.evaluate_loss(net, test_iter, loss))) print('L2 norm of w:', np.linalg.norm(net[0].weight.data()))
The plots look identical to those when we implemented weight decay from scratch. However, they run appreciably faster and are easier to implement, a benefit that will become more pronounced for larger problems.
train_concise(0)
L2 norm of w: 15.014066

4.5. Weight Decay

157

train_concise(3) L2 norm of w: 0.33991596

So far, we only touched upon one notion of what constitutes a simple linear function. Moreover, what constitutes a simple nonlinear function can be an even more complex question. For instance, reproducing kernel Hilbert space (RKHS)69 allows one to apply tools introduced for linear functions in a nonlinear context. Unfortunately, RKHS-based algorithms tend to scale poorly to large, high-dimensional data. In this book we will default to the simple heuristic of applying weight decay on all layers of a deep network.
69 https://en.wikipedia.org/wiki/Reproducing_kernel_Hilbert_space

158

Chapter 4. Multilayer Perceptrons

Summary
• Regularization is a common method for dealing with overfitting. It adds a penalty term to the loss function on the training set to reduce the complexity of the learned model.
• One particular choice for keeping the model simple is weight decay using an L2 penalty. This leads to weight decay in the update steps of the learning algorithm.
• The weight decay functionality is provided in optimizers from deep learning frameworks.
• Different sets of parameters can have different update behaviors within the same training loop.
Exercises
1. Experiment with the value of λ in the estimation problem in this section. Plot training and test accuracy as a function of λ. What do you observe?
2. Use a validation set to find the optimal value of λ. Is it really the optimal value? Does this matter?
3. What would the update equations look like if instead of ∥w∥2 we used ∑i |wi| as our penalty of choice (L1 regularization)?
4. We know that ∥w∥2 = w⊤w. Can you find a similar equation for matrices (see the Frobenius norm in Section 2.3.10)?
5. Review the relationship between training error and generalization error. In addition to weight decay, increased training, and the use of a model of suitable complexity, what other ways can you think of to deal with overfitting?
6. In Bayesian statistics we use the product of prior and likelihood to arrive at a posterior via P (w | x) ∝ P (x | w)P (w). How can you identify P (w) with regularization?
Discussions70
4.6 Dropout
In Section 4.5, we introduced the classical approach to regularizing statistical models by penalizing the L2 norm of the weights. In probabilistic terms, we could justify this technique by arguing that we have assumed a prior belief that weights take values from a Gaussian distribution with mean zero. More intuitively, we might argue that we encouraged the model to spread out its weights among many features rather than depending too much on a small number of potentially spurious associations.
70 https://discuss.d2l.ai/t/98

4.6. Dropout

159

4.6.1 Overfitting Revisited
Faced with more features than examples, linear models tend to overfit. But given more examples than features, we can generally count on linear models not to overfit. Unfortunately, the reliability with which linear models generalize comes at a cost. Naively applied, linear models do not take into account interactions among features. For every feature, a linear model must assign either a positive or a negative weight, ignoring context.
In traditional texts, this fundamental tension between generalizability and flexibility is described as the bias-variance tradeoff. Linear models have high bias: they can only represent a small class of functions. However, these models have low variance: they give similar results across different random samples of the data.
Deep neural networks inhabit the opposite end of the bias-variance spectrum. Unlike linear models, neural networks are not confined to looking at each feature individually. They can learn interactions among groups of features. For example, they might infer that “Nigeria” and “Western Union” appearing together in an email indicates spam but that separately they do not.
Even when we have far more examples than features, deep neural networks are capable of overfitting. In 2017, a group of researchers demonstrated the extreme flexibility of neural networks by training deep nets on randomly-labeled images. Despite the absence of any true pattern linking the inputs to the outputs, they found that the neural network optimized by stochastic gradient descent could label every image in the training set perfectly. Consider what this means. If the labels are assigned uniformly at random and there are 10 classes, then no classifier can do better than 10% accuracy on holdout data. The generalization gap here is a whopping 90%. If our models are so expressive that they can overfit this badly, then when should we expect them not to overfit?
The mathematical foundations for the puzzling generalization properties of deep networks remain open research questions, and we encourage the theoretically-oriented reader to dig deeper into the topic. For now, we turn to the investigation of practical tools that tend to empirically improve the generalization of deep nets.
4.6.2 Robustness through Perturbations
Let us think briefly about what we expect from a good predictive model. We want it to peform well on unseen data. Classical generalization theory suggests that to close the gap between train and test performance, we should aim for a simple model. Simplicity can come in the form of a small number of dimensions. We explored this when discussing the monomial basis functions of linear models in Section 4.4. Additionally, as we saw when discussing weight decay (L2 regularization) in Section 4.5, the (inverse) norm of the parameters also represents a useful measure of simplicity. Another useful notion of simplicity is smoothness, i.e., that the function should not be sensitive to small changes to its inputs. For instance, when we classify images, we would expect that adding some random noise to the pixels should be mostly harmless.
In 1995, Christopher Bishop formalized this idea when he proved that training with input noise is equivalent to Tikhonov regularization (Bishop, 1995). This work drew a clear mathematical connection between the requirement that a function be smooth (and thus simple), and the requirement that it be resilient to perturbations in the input.
Then, in 2014, Srivastava et al. (Srivastava et al., 2014) developed a clever idea for how to apply Bishopʼs idea to the internal layers of a network, too. Namely, they proposed to inject noise into each layer of the network before calculating the subsequent layer during training. They realized

160

Chapter 4. Multilayer Perceptrons

that when training a deep network with many layers, injecting noise enforces smoothness just on the input-output mapping.

Their idea, called dropout, involves injecting noise while computing each internal layer during forward propagation, and it has become a standard technique for training neural networks. The method is called dropout because we literally drop out some neurons during training. Throughout training, on each iteration, standard dropout consists of zeroing out some fraction of the nodes in each layer before calculating the subsequent layer.

To be clear, we are imposing our own narrative with the link to Bishop. The original paper on dropout offers intuition through a surprising analogy to sexual reproduction. The authors argue that neural network overfitting is characterized by a state in which each layer relies on a specifc pattern of activations in the previous layer, calling this condition co-adaptation. Dropout, they claim, breaks up co-adaptation just as sexual reproduction is argued to break up co-adapted genes.

The key challenge then is how to inject this noise. One idea is to inject the noise in an unbiased manner so that the expected value of each layer—while fixing the others—equals to the value it would have taken absent noise.

In Bishopʼs work, he added Gaussian noise to the inputs to a linear model. At each training iteration, he added noise sampled from a distribution with mean zero ϵ ∼ N (0, σ2) to the input x, yielding a perturbed point x′ = x + ϵ. In expectation, E[x′] = x.

In standard dropout regularization, one debiases each layer by normalizing by the fraction of

nodes that were retained (not dropped out). In other words, with dropout probability p, each intermediate activation h is replaced by a random variable h′ as follows:

{ h′ = 0
h 1−p

with probability p otherwise

(4.6.1)

By design, the expectation remains unchanged, i.e., E[h′] = h.

4.6.3 Dropout in Practice
Recall the MLP with a hidden layer and 5 hidden units in Fig. 4.1.1. When we apply dropout to a hidden layer, zeroing out each hidden unit with probability p, the result can be viewed as a network containing only a subset of the original neurons. In Fig. 4.6.1, h2 and h5 are removed. Consequently, the calculation of the outputs no longer depends on h2 or h5 and their respective gradient also vanishes when performing backpropagation. In this way, the calculation of the output layer cannot be overly dependent on any one element of h1, . . . , h5.

4.6. Dropout

161

Fig. 4.6.1: MLP before and after dropout.

Typically, we disable dropout at test time. Given a trained model and a new example, we do not drop out any nodes and thus do not need to normalize. However, there are some exceptions: some researchers use dropout at test time as a heuristic for estimating the uncertainty of neural network predictions: if the predictions agree across many different dropout masks, then we might say that the network is more confident.

4.6.4 Implementation from Scratch
To implement the dropout function for a single layer, we must draw as many samples from a Bernoulli (binary) random variable as our layer has dimensions, where the random variable takes value 1 (keep) with probability 1 − p and 0 (drop) with probability p. One easy way to implement this is to first draw samples from the uniform distribution U [0, 1]. Then we can keep those nodes for which the corresponding sample is greater than p, dropping the rest.
In the following code, we implement a dropout_layer function that drops out the elements in the tensor input X with probability dropout, rescaling the remainder as described above: dividing the survivors by 1.0-dropout.
from mxnet import autograd, gluon, init, np, npx from mxnet.gluon import nn from d2l import mxnet as d2l
npx.set_np()
def dropout_layer(X, dropout): assert 0 <= dropout <= 1 # In this case, all elements are dropped out if dropout == 1: return np.zeros_like(X) # In this case, all elements are kept if dropout == 0: return X mask = np.random.uniform(0, 1, X.shape) > dropout return mask.astype(np.float32) * X / (1.0 - dropout)

We can test out the dropout_layer function on a few examples. In the following lines of code, we pass our input X through the dropout operation, with probabilities 0, 0.5, and 1, respectively.

162

Chapter 4. Multilayer Perceptrons

X = np.arange(16).reshape(2, 8) print(dropout_layer(X, 0)) print(dropout_layer(X, 0.5)) print(dropout_layer(X, 1))

[[ 0. 1. 2. [ 8. 9. 10. [[ 0. 2. 4. [ 0. 18. 20. [[0. 0. 0. 0. [0. 0. 0. 0.

3. 4. 5. 6. 11. 12. 13. 14. 6. 8. 10. 12. 0. 0. 0. 28. 0. 0. 0. 0.] 0. 0. 0. 0.]]

7.] 15.]] 14.] 0.]]

Defining Model Parameters
Again, we work with the Fashion-MNIST dataset introduced in Section 3.5. We define an MLP with two hidden layers containing 256 units each.
num_inputs, num_outputs, num_hiddens1, num_hiddens2 = 784, 10, 256, 256
W1 = np.random.normal(scale=0.01, size=(num_inputs, num_hiddens1)) b1 = np.zeros(num_hiddens1) W2 = np.random.normal(scale=0.01, size=(num_hiddens1, num_hiddens2)) b2 = np.zeros(num_hiddens2) W3 = np.random.normal(scale=0.01, size=(num_hiddens2, num_outputs)) b3 = np.zeros(num_outputs)
params = [W1, b1, W2, b2, W3, b3] for param in params:
param.attach_grad()

Defining the Model
The model below applies dropout to the output of each hidden layer (following the activation function). We can set dropout probabilities for each layer separately. A common trend is to set a lower dropout probability closer to the input layer. Below we set it to 0.2 and 0.5 for the first and second hidden layers, respectively. We ensure that dropout is only active during training.
dropout1, dropout2 = 0.2, 0.5
def net(X): X = X.reshape(-1, num_inputs) H1 = npx.relu(np.dot(X, W1) + b1) # Use dropout only when training the model if autograd.is_training(): # Add a dropout layer after the first fully connected layer H1 = dropout_layer(H1, dropout1) H2 = npx.relu(np.dot(H1, W2) + b2) if autograd.is_training(): # Add a dropout layer after the second fully connected layer H2 = dropout_layer(H2, dropout2) return np.dot(H2, W3) + b3

4.6. Dropout

163

Training and Testing
This is similar to the training and testing of MLPs described previously.
num_epochs, lr, batch_size = 10, 0.5, 256 loss = gluon.loss.SoftmaxCrossEntropyLoss() train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size) d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs,
lambda batch_size: d2l.sgd(params, lr, batch_size))

4.6.5 Concise Implementation
With high-level APIs, all we need to do is add a Dropout layer after each fully-connected layer, passing in the dropout probability as the only argument to its constructor. During training, the Dropout layer will randomly drop out outputs of the previous layer (or equivalently, the inputs to the subsequent layer) according to the specified dropout probability. When not in training mode, the Dropout layer simply passes the data through during testing.
net = nn.Sequential() net.add(
nn.Dense(256, activation="relu"), # Add a dropout layer after the first fully connected layer nn.Dropout(dropout1), nn.Dense(256, activation="relu"), # Add a dropout layer after the second fully connected layer nn.Dropout(dropout2), nn.Dense(10)) net.initialize(init.Normal(sigma=0.01))
Next, we train and test the model.
trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': lr}) d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)

164

Chapter 4. Multilayer Perceptrons

Summary
• Beyond controlling the number of dimensions and the size of the weight vector, dropout is yet another tool to avoid overfitting. Often they are used jointly.
• Dropout replaces an activation h with a random variable with expected value h.
• Dropout is only used during training.
Exercises
1. What happens if you change the dropout probabilities for the first and second layers? In particular, what happens if you switch the ones for both layers? Design an experiment to answer these questions, describe your results quantitatively, and summarize the qualitative takeaways.
2. Increase the number of epochs and compare the results obtained when using dropout with those when not using it.
3. What is the variance of the activations in each hidden layer when dropout is and is not applied? Draw a plot to show how this quantity evolves over time for both models.
4. Why is dropout not typically used at test time?
5. Using the model in this section as an example, compare the effects of using dropout and weight decay. What happens when dropout and weight decay are used at the same time? Are the results additive? Are there diminished returns (or worse)? Do they cancel each other out?
6. What happens if we apply dropout to the individual weights of the weight matrix rather than the activations?
7. Invent another technique for injecting random noise at each layer that is different from the standard dropout technique. Can you develop a method that outperforms dropout on the Fashion-MNIST dataset (for a fixed architecture)?
Discussions71
71 https://discuss.d2l.ai/t/100

4.6. Dropout

165

4.7 Forward Propagation, Backward Propagation, and Computational Graphs
So far, we have trained our models with minibatch stochastic gradient descent. However, when we implemented the algorithm, we only worried about the calculations involved in forward propagation through the model. When it came time to calculate the gradients, we just invoked the backpropagation function provided by the deep learning framework.
The automatic calculation of gradients (automatic differentiation) profoundly simplifies the implementation of deep learning algorithms. Before automatic differentiation, even small changes to complicated models required recalculating complicated derivatives by hand. Surprisingly often, academic papers had to allocate numerous pages to deriving update rules. While we must continue to rely on automatic differentiation so we can focus on the interesting parts, you ought to know how these gradients are calculated under the hood if you want to go beyond a shallow understanding of deep learning.
In this section, we take a deep dive into the details of backward propagation (more commonly called backpropagation). To convey some insight for both the techniques and their implementations, we rely on some basic mathematics and computational graphs. To start, we focus our exposition on a one-hidden-layer MLP with weight decay (L2 regularization).

4.7.1 Forward Propagation

Forward propagation (or forward pass) refers to the calculation and storage of intermediate variables (including outputs) for a neural network in order from the input layer to the output layer. We now work step-by-step through the mechanics of a neural network with one hidden layer. This may seem tedious but in the eternal words of funk virtuoso James Brown, you must “pay the cost to be the boss”.
For the sake of simplicity, let us assume that the input example is x ∈ Rd and that our hidden layer does not include a bias term. Here the intermediate variable is:

z = W(1)x,

(4.7.1)

where W(1) ∈ Rh×d is the weight parameter of the hidden layer. After running the intermediate variable z ∈ Rh through the activation function ϕ we obtain our hidden activation vector of length
h,

h = ϕ(z).

(4.7.2)

The hidden variable h is also an intermediate variable. Assuming that the parameters of the output layer only possess a weight of W(2) ∈ Rq×h, we can obtain an output layer variable with a vector
of length q:

o = W(2)h.

(4.7.3)

Assuming that the loss function is l and the example label is y, we can then calculate the loss term for a single data example,

L = l(o, y).

(4.7.4)

166

Chapter 4. Multilayer Perceptrons

According to the definition of L2 regularization, given the hyperparameter λ, the regularization

term is

s = λ (∥W(1)∥2F + ∥W(2)∥2F ) , 2

(4.7.5)

where the Frobenius norm of the matrix is simply the L2 norm applied after flattening the matrix into a vector. Finally, the modelʼs regularized loss on a given data example is:

J = L + s.

(4.7.6)

We refer to J as the objective function in the following discussion.

4.7.2 Computational Graph of Forward Propagation
Plotting computational graphs helps us visualize the dependencies of operators and variables within the calculation. Fig. 4.7.1 contains the graph associated with the simple network described above, where squares denote variables and circles denote operators. The lower-left corner signifies the input and the upper-right corner is the output. Notice that the directions of the arrows (which illustrate data flow) are primarily rightward and upward.

Fig. 4.7.1: Computational graph of forward propagation.

4.7.3 Backpropagation

Backpropagation refers to the method of calculating the gradient of neural network parameters.

In short, the method traverses the network in reverse order, from the output to the input layer,

according to the chain rule from calculus. The algorithm stores any intermediate variables (partial

derivatives) required while calculating the gradient with respect to some parameters. Assume that

we have functions Y = f (X) and Z = g(Y), in which the input and the output X, Y, Z are tensors of

arbitrary shapes. By using the chain rule, we can compute the derivative of Z with respect to X via

∂Z (∂Z ∂Y) ∂X = prod ∂Y , ∂X .

(4.7.7)

Here we use the prod operator to multiply its arguments after the necessary operations, such as transposition and swapping input positions, have been carried out. For vectors, this is straightforward: it is simply matrix-matrix multiplication. For higher dimensional tensors, we use the appropriate counterpart. The operator prod hides all the notation overhead.

Recall that the parameters of the simple network with one hidden layer, whose computational graph is in Fig. 4.7.1, are W(1) and W(2). The objective of backpropagation is to calculate the gradients ∂J/∂W(1) and ∂J/∂W(2). To accomplish this, we apply the chain rule and calculate, in turn,
the gradient of each intermediate variable and parameter. The order of calculations are reversed

4.7. Forward Propagation, Backward Propagation, and Computational Graphs

167

relative to those performed in forward propagation, since we need to start with the outcome of the computational graph and work our way towards the parameters. The first step is to calculate the gradients of the objective function J = L + s with respect to the loss term L and the regularization term s.

∂J = 1 and ∂J = 1.

∂L

∂s

(4.7.8)

Next, we compute the gradient of the objective function with respect to variable of the output layer

o according to the chain rule:

(

)

∂J = prod

∂J ∂L ,

= ∂L ∈ Rq.

∂o

∂L ∂o ∂o

(4.7.9)

Next, we calculate the gradients of the regularization term with respect to both parameters:

∂s = λW(1) and ∂s = λW(2).

∂W(1)

∂W(2)

(4.7.10)

Now we are able to calculate the gradient ∂J/∂W(2) ∈ Rq×h of the model parameters closest to

the output layer. Using the chain rule yields:

(

)

(

)

∂J = prod ∂J , ∂o + prod ∂J , ∂s = ∂J h⊤ + λW(2).

∂W(2)

∂o ∂W(2)

∂s ∂W(2)

∂o

(4.7.11)

To obtain the gradient with respect to W(1) we need to continue backpropagation along the output layer to the hidden layer. The gradient with respect to the hidden layerʼs outputs ∂J/∂h ∈ Rh is

given by

(

)

∂J = prod ∂J , ∂o = W(2)⊤ ∂J .

∂h

∂o ∂h

∂o

(4.7.12)

Since the activation function ϕ applies elementwise, calculating the gradient ∂J/∂z ∈ Rh of the

intermediate variable z requires that we use the elementwise multiplication operator, which we

denote by ⊙:

(

)

∂J = prod ∂J , ∂h = ∂J ⊙ ϕ′ (z) .

∂z

∂h ∂z ∂h

(4.7.13)

Finally, we can obtain the gradient ∂J/∂W(1) ∈ Rh×d of the model parameters closest to the input

layer. According to the chain rule, we get

(

)

(

)

∂J = prod ∂J , ∂z + prod ∂J , ∂s = ∂J x⊤ + λW(1).

∂W(1)

∂z ∂W(1)

∂s ∂W(1)

∂z

(4.7.14)

4.7.4 Training Neural Networks
When training neural networks, forward and backward propagation depend on each other. In particular, for forward propagation, we traverse the computational graph in the direction of dependencies and compute all the variables on its path. These are then used for backpropagation where the compute order on the graph is reversed.
Take the aforementioned simple network as an example to illustrate. On one hand, computing the regularization term (4.7.5) during forward propagation depends on the current values of model

168

Chapter 4. Multilayer Perceptrons

parameters W(1) and W(2). They are given by the optimization algorithm according to backpropagation in the latest iteration. On the other hand, the gradient calculation for the parameter (4.7.11) during backpropagation depends on the current value of the hidden variable h, which is given by forward propagation.
Therefore when training neural networks, after model parameters are initialized, we alternate forward propagation with backpropagation, updating model parameters using gradients given by backpropagation. Note that backpropagation reuses the stored intermediate values from forward propagation to avoid duplicate calculations. One of the consequences is that we need to retain the intermediate values until backpropagation is complete. This is also one of the reasons why training requires significantly more memory than plain prediction. Besides, the size of such intermediate values is roughly proportional to the number of network layers and the batch size. Thus, training deeper networks using larger batch sizes more easily leads to out of memory errors.
Summary
• Forward propagation sequentially calculates and stores intermediate variables within the computational graph defined by the neural network. It proceeds from the input to the output layer.
• Backpropagation sequentially calculates and stores the gradients of intermediate variables and parameters within the neural network in the reversed order.
• When training deep learning models, forward propagation and back propagation are interdependent.
• Training requires significantly more memory than prediction.
Exercises
1. Assume that the inputs X to some scalar function f are n × m matrices. What is the dimensionality of the gradient of f with respect to X?
2. Add a bias to the hidden layer of the model described in this section (you do not need to include bias in the regularization term).
1. Draw the corresponding computational graph.
2. Derive the forward and backward propagation equations.
3. Compute the memory footprint for training and prediction in the model described in this section.
4. Assume that you want to compute second derivatives. What happens to the computational graph? How long do you expect the calculation to take?
5. Assume that the computational graph is too large for your GPU.
1. Can you partition it over more than one GPU?
2. What are the advantages and disadvantages over training on a smaller minibatch? Discussions72
72 https://discuss.d2l.ai/t/102

4.7. Forward Propagation, Backward Propagation, and Computational Graphs

169

4.8 Numerical Stability and Initialization
Thus far, every model that we have implemented required that we initialize its parameters according to some pre-specified distribution. Until now, we took the initialization scheme for granted, glossing over the details of how these choices are made. You might have even gotten the impression that these choices are not especially important. To the contrary, the choice of initialization scheme plays a significant role in neural network learning, and it can be crucial for maintaining numerical stability. Moreover, these choices can be tied up in interesting ways with the choice of the nonlinear activation function. Which function we choose and how we initialize parameters can determine how quickly our optimization algorithm converges. Poor choices here can cause us to encounter exploding or vanishing gradients while training. In this section, we delve into these topics with greater detail and discuss some useful heuristics that you will find useful throughout your career in deep learning.

4.8.1 Vanishing and Exploding Gradients

Consider a deep network with L layers, input x and output o. With each layer l defined by a transformation fl parameterized by weights W(l), whose hidden variable is h(l) (let h(0) = x), our net-
work can be expressed as:

h(l) = fl(h(l−1)) and thus o = fL ◦ . . . ◦ f1(x).

(4.8.1)

If all the hidden variables and the input are vectors, we can write the gradient of o with respect to any set of parameters W(l) as follows:

∂W(l) o = ∂h(L−1) h(L) · . . . · ∂h(l) h(l+1) ∂W(l) h(l) .

M(L) d=ef

M(l+1) d=ef

v(l) d=ef

(4.8.2)

In other words, this gradient is the product of L − l matrices M(L) · . . . · M(l+1) and the gradient vector v(l). Thus we are susceptible to the same problems of numerical underflow that often crop up when multiplying together too many probabilities. When dealing with probabilities, a common trick is to switch into log-space, i.e., shifting pressure from the mantissa to the exponent of the numerical representation. Unfortunately, our problem above is more serious: initially the matrices M(l) may have a wide variety of eigenvalues. They might be small or large, and their product might be very large or very small.

The risks posed by unstable gradients go beyond numerical representation. Gradients of unpredictable magnitude also threaten the stability of our optimization algorithms. We may be facing parameter updates that are either (i) excessively large, destroying our model (the exploding gradient problem); or (ii) excessively small (the vanishing gradient problem), rendering learning impossible as parameters hardly move on each update.

170

Chapter 4. Multilayer Perceptrons

Vanishing Gradients
One frequent culprit causing the vanishing gradient problem is the choice of the activation function σ that is appended following each layerʼs linear operations. Historically, the sigmoid function 1/(1 + exp(−x)) (introduced in Section 4.1) was popular because it resembles a thresholding function. Since early artificial neural networks were inspired by biological neural networks, the idea of neurons that fire either fully or not at all (like biological neurons) seemed appealing. Let us take a closer look at the sigmoid to see why it can cause vanishing gradients.
%matplotlib inline from mxnet import autograd, np, npx from d2l import mxnet as d2l
npx.set_np()
x = np.arange(-8.0, 8.0, 0.1) x.attach_grad() with autograd.record():
y = npx.sigmoid(x) y.backward()
d2l.plot(x, [y, x.grad], legend=['sigmoid', 'gradient'], figsize=(4.5, 2.5))

As you can see, the sigmoidʼs gradient vanishes both when its inputs are large and when they are small. Moreover, when backpropagating through many layers, unless we are in the Goldilocks zone, where the inputs to many of the sigmoids are close to zero, the gradients of the overall product may vanish. When our network boasts many layers, unless we are careful, the gradient will likely be cut off at some layer. Indeed, this problem used to plague deep network training. Consequently, ReLUs, which are more stable (but less neurally plausible), have emerged as the default choice for practitioners.

4.8. Numerical Stability and Initialization

171

Exploding Gradients
The opposite problem, when gradients explode, can be similarly vexing. To illustrate this a bit better, we draw 100 Gaussian random matrices and multiply them with some initial matrix. For the scale that we picked (the choice of the variance σ2 = 1), the matrix product explodes. When this happens due to the initialization of a deep network, we have no chance of getting a gradient descent optimizer to converge.
M = np.random.normal(size=(4, 4)) print('a single matrix', M) for i in range(100):
M = np.dot(M, np.random.normal(size=(4, 4)))
print('after multiplying 100 matrices', M)

a single matrix [[ 2.2122064 1.1630787 0.7740038 0.4838046 ] [ 1.0434405 0.29956347 1.1839255 0.15302546] [ 1.8917114 -1.1688148 -1.2347414 1.5580711 ] [-1.771029 -0.5459446 -0.45138445 -2.3556297 ]] after multiplying 100 matrices [[ 3.4459714e+23 -7.8040680e+23 5.9973287e+23 [ 2.5275089e+23 -5.7240326e+23 4.3988473e+23 3.3174740e+23] [ 1.3731286e+24 -3.1097155e+24 2.3897773e+24 1.8022959e+24] [-4.4951040e+23 1.0180033e+24 -7.8232281e+23 -5.9000354e+23]]

4.5229990e+23]

Breaking the Symmetry
Another problem in neural network design is the symmetry inherent in their parametrization. Assume that we have a simple MLP with one hidden layer and two units. In this case, we could permute the weights W(1) of the first layer and likewise permute the weights of the output layer to obtain the same function. There is nothing special differentiating the first hidden unit vs. the second hidden unit. In other words, we have permutation symmetry among the hidden units of each layer.
This is more than just a theoretical nuisance. Consider the aforementioned one-hidden-layer MLP with two hidden units. For illustration, suppose that the output layer transforms the two hidden units into only one output unit. Imagine what would happen if we initialized all of the parameters of the hidden layer as W(1) = c for some constant c. In this case, during forward propagation either hidden unit takes the same inputs and parameters, producing the same activation, which is fed to the output unit. During backpropagation, differentiating the output unit with respect to parameters W(1) gives a gradient whose elements all take the same value. Thus, after gradientbased iteration (e.g., minibatch stochastic gradient descent), all the elements of W(1) still take the same value. Such iterations would never break the symmetry on its own and we might never be able to realize the networkʼs expressive power. The hidden layer would behave as if it had only a single unit. Note that while minibatch stochastic gradient descent would not break this symmetry, dropout regularization would!

172

Chapter 4. Multilayer Perceptrons

4.8.2 Parameter Initialization
One way of addressing—or at least mitigating—the issues raised above is through careful initialization. Additional care during optimization and suitable regularization can further enhance stability.

Default Initialization
In the previous sections, e.g., in Section 3.3, we used a normal distribution to initialize the values of our weights. If we do not specify the initialization method, the framework will use a default random initialization method, which often works well in practice for moderate problem sizes.

Xavier Initialization

Let us look at the scale distribution of an output (e.g., a hidden variable) oi for some fully-
connected layer without nonlinearities. With nin inputs xj and their associated weights wij for this layer, an output is given by

∑ nin oi = wij xj .
j=1

(4.8.3)

The weights wij are all drawn independently from the same distribution. Furthermore, let us assume that this distribution has zero mean and variance σ2. Note that this does not mean that
the distribution has to be Gaussian, just that the mean and variance need to exist. For now, let us assume that the inputs to the layer xj also have zero mean and variance γ2 and that they are independent of wij and independent of each other. In this case, we can compute the mean and
variance of oi as follows:

∑ nin E[oi] = E[wijxj]
j=1
∑ nin = E[wij]E[xj]
j=1
= 0,
Var[oi] = E[o2i ] − (E[oi])2 ∑ nin
= E[wi2jx2j ] − 0
j=1
∑ nin = E[wi2j]E[x2j ]
j=1
= ninσ2γ2.

(4.8.4)

One way to keep the variance fixed is to set ninσ2 = 1. Now consider backpropagation. There we face a similar problem, albeit with gradients being propagated from the layers closer to the
output. Using the same reasoning as for forward propagation, we see that the gradientsʼ variance can blow up unless noutσ2 = 1, where nout is the number of outputs of this layer. This leaves us in a dilemma: we cannot possibly satisfy both conditions simultaneously. Instead, we simply try to

4.8. Numerical Stability and Initialization

173

satisfy:

√

1 (nin

+

nout)σ2

=

1

or

equivalently

σ

=

2 .

2

nin + nout

(4.8.5)

This is the reasoning underlying the now-standard and practically beneficial Xavier initialization,

named after the first author of its creators (Glorot & Bengio, 2010). Typically, the Xavier initializa-
tion samples weights from a Gaussian distribution with zero mean and variance σ2 = nin+2nout . We can also adapt Xavierʼs intuition to choose the variance when sampling weights from a uniform distribution. Note that the uniform distribution U (−a, a) has variance a32 . Plugging a32 into our condition on σ2 yields the suggestion to initialize according to

(√

√

)

6

6

U−

,

.

nin + nout nin + nout

(4.8.6)

Though the assumption for nonexistence of nonlinearities in the above mathematical reasoning can be easily violated in neural networks, the Xavier initialization method turns out to work well in practice.

Beyond
The reasoning above barely scratches the surface of modern approaches to parameter initialization. A deep learning framework often implements over a dozen different heuristics. Moreover, parameter initialization continues to be a hot area of fundamental research in deep learning. Among these are heuristics specialized for tied (shared) parameters, super-resolution, sequence models, and other situations. For instance, Xiao et al. demonstrated the possibility of training 10000-layer neural networks without architectural tricks by using a carefully-designed initialization method (Xiao et al., 2018).
If the topic interests you we suggest a deep dive into this moduleʼs offerings, reading the papers that proposed and analyzed each heuristic, and then exploring the latest publications on the topic. Perhaps you will stumble across or even invent a clever idea and contribute an implementation to deep learning frameworks.

Summary
• Vanishing and exploding gradients are common issues in deep networks. Great care in parameter initialization is required to ensure that gradients and parameters remain well controlled.
• Initialization heuristics are needed to ensure that the initial gradients are neither too large nor too small.
• ReLU activation functions mitigate the vanishing gradient problem. This can accelerate convergence.
• Random initialization is key to ensure that symmetry is broken before optimization.
• Xavier initialization suggests that, for each layer, variance of any output is not affected by the number of inputs, and variance of any gradient is not affected by the number of outputs.

174

Chapter 4. Multilayer Perceptrons

Exercises
1. Can you design other cases where a neural network might exhibit symmetry requiring breaking besides the permutation symmetry in an MLPʼs layers?
2. Can we initialize all weight parameters in linear regression or in softmax regression to the same value?
3. Look up analytic bounds on the eigenvalues of the product of two matrices. What does this tell you about ensuring that gradients are well conditioned?
4. If we know that some terms diverge, can we fix this after the fact? Look at the paper on layerwise adaptive rate scaling for inspiration (You et al., 2017).
Discussions73
4.9 Environment and Distribution Shift
In the previous sections, we worked through a number of hands-on applications of machine learning, fitting models to a variety of datasets. And yet, we never stopped to contemplate either where data come from in the first place or what we plan to ultimately do with the outputs from our models. Too often, machine learning developers in possession of data rush to develop models without pausing to consider these fundamental issues.
Many failed machine learning deployments can be traced back to this pattern. Sometimes models appear to perform marvelously as measured by test set accuracy but fail catastrophically in deployment when the distribution of data suddenly shifts. More insidiously, sometimes the very deployment of a model can be the catalyst that perturbs the data distribution. Say, for example, that we trained a model to predict who will repay vs. default on a loan, finding that an applicantʼs choice of footwear was associated with the risk of default (Oxfords indicate repayment, sneakers indicate default). We might be inclined to thereafter grant loans to all applicants wearing Oxfords and to deny all applicants wearing sneakers.
In this case, our ill-considered leap from pattern recognition to decision-making and our failure to critically consider the environment might have disastrous consequences. For starters, as soon as we began making decisions based on footwear, customers would catch on and change their behavior. Before long, all applicants would be wearing Oxfords, without any coinciding improvement in credit-worthiness. Take a minute to digest this because similar issues abound in many applications of machine learning: by introducing our model-based decisions to the environment, we might break the model.
While we cannot possibly give these topics a complete treatment in one section, we aim here to expose some common concerns, and to stimulate the critical thinking required to detect these situations early, mitigate damage, and use machine learning responsibly. Some of the solutions are simple (ask for the “right” data), some are technically difficult (implement a reinforcement learning system), and others require that we step outside the realm of statistical prediction altogether and grapple with difficult philosophical questions concerning the ethical application of algorithms.
73 https://discuss.d2l.ai/t/103

4.9. Environment and Distribution Shift

175

4.9.1 Types of Distribution Shift
To begin, we stick with the passive prediction setting considering the various ways that data distributions might shift and what might be done to salvage model performance. In one classic setup, we assume that our training data were sampled from some distribution pS(x, y) but that our test data will consist of unlabeled examples drawn from some different distribution pT (x, y). Already, we must confront a sobering reality. Absent any assumptions on how pS and pT relate to each other, learning a robust classifier is impossible.
Consider a binary classification problem, where we wish to distinguish between dogs and cats. If the distribution can shift in arbitrary ways, then our setup permits the pathological case in which the distribution over inputs remains constant: pS(x) = pT (x), but the labels are all flipped: pS(y|x) = 1 − pT (y|x). In other words, if God can suddenly decide that in the future all “cats” are now dogs and what we previously called “dogs” are now cats—without any change in the distribution of inputs p(x), then we cannot possibly distinguish this setting from one in which the distribution did not change at all.
Fortunately, under some restricted assumptions on the ways our data might change in the future, principled algorithms can detect shift and sometimes even adapt on the fly, improving on the accuracy of the original classifier.
Covariate Shift
Among categories of distribution shift, covariate shift may be the most widely studied. Here, we assume that while the distribution of inputs may change over time, the labeling function, i.e., the conditional distribution P (y | x) does not change. Statisticians call this covariate shift because the problem arises due to a shift in the distribution of the covariates (features). While we can sometimes reason about distribution shift without invoking causality, we note that covariate shift is the natural assumption to invoke in settings where we believe that x causes y.
Consider the challenge of distinguishing cats and dogs. Our training data might consist of images of the kind in Fig. 4.9.1.

Fig. 4.9.1: Training data for distinguishing cats and dogs. At test time we are asked to classify the images in Fig. 4.9.2.

176

Chapter 4. Multilayer Perceptrons

Fig. 4.9.2: Test data for distinguishing cats and dogs.
The training set consists of photos, while the test set contains only cartoons. Training on a dataset with substantially different characteristics from the test set can spell trouble absent a coherent plan for how to adapt to the new domain.
Label Shift
Label shift describes the converse problem. Here, we assume that the label marginal P (y) can change but the class-conditional distribution P (x | y) remains fixed across domains. Label shift is a reasonable assumption to make when we believe that y causes x. For example, we may want to predict diagnoses given their symptoms (or other manifestations), even as the relative prevalence of diagnoses are changing over time. Label shift is the appropriate assumption here because diseases cause symptoms. In some degenerate cases the label shift and covariate shift assumptions can hold simultaneously. For example, when the label is deterministic, the covariate shift assumption will be satisfied, even when y causes x. Interestingly, in these cases, it is often advantageous to work with methods that flow from the label shift assumption. That is because these methods tend to involve manipulating objects that look like labels (often low-dimensional), as opposed to objects that look like inputs, which tend to be high-dimensional in deep learning.
Concept Shift
We may also encounter the related problem of concept shift, which arises when the very definitions of labels can change. This sounds weird—a cat is a cat, no? However, other categories are subject to changes in usage over time. Diagnostic criteria for mental illness, what passes for fashionable, and job titles, are all subject to considerable amounts of concept shift. It turns out that if we navigate around the United States, shifting the source of our data by geography, we will find considerable concept shift regarding the distribution of names for soft drinks as shown in Fig. 4.9.3.

4.9. Environment and Distribution Shift

177

Fig. 4.9.3: Concept shift on soft drink names in the United States.
If we were to build a machine translation system, the distribution P (y | x) might be different depending on our location. This problem can be tricky to spot. We might hope to exploit knowledge that shift only takes place gradually either in a temporal or geographic sense.
4.9.2 Examples of Distribution Shift
Before delving into formalism and algorithms, we can discuss some concrete situations where covariate or concept shift might not be obvious.
Medical Diagnostics
Imagine that you want to design an algorithm to detect cancer. You collect data from healthy and sick people and you train your algorithm. It works fine, giving you high accuracy and you conclude that you are ready for a successful career in medical diagnostics. Not so fast.
The distributions that gave rise to the training data and those you will encounter in the wild might differ considerably. This happened to an unfortunate startup that some of us (authors) worked with years ago. They were developing a blood test for a disease that predominantly affects older men and hoped to study it using blood samples that they had collected from patients. However, it is considerably more difficult to obtain blood samples from healthy men than sick patients already in the system. To compensate, the startup solicited blood donations from students on a university campus to serve as healthy controls in developing their test. Then they asked whether we could help them to build a classifier for detecting the disease.
As we explained to them, it would indeed be easy to distinguish between the healthy and sick cohorts with near-perfect accuracy. However, that is because the test subjects differed in age, hormone levels, physical activity, diet, alcohol consumption, and many more factors unrelated to the disease. This was unlikely to be the case with real patients. Due to their sampling procedure, we could expect to encounter extreme covariate shift. Moreover, this case was unlikely to be correctable via conventional methods. In short, they wasted a significant sum of money.

178

Chapter 4. Multilayer Perceptrons

Self-Driving Cars
Say a company wanted to leverage machine learning for developing self-driving cars. One key component here is a roadside detector. Since real annotated data are expensive to get, they had the (smart and questionable) idea to use synthetic data from a game rendering engine as additional training data. This worked really well on “test data” drawn from the rendering engine. Alas, inside a real car it was a disaster. As it turned out, the roadside had been rendered with a very simplistic texture. More importantly, all the roadside had been rendered with the same texture and the roadside detector learned about this “feature” very quickly.
A similar thing happened to the US Army when they first tried to detect tanks in the forest. They took aerial photographs of the forest without tanks, then drove the tanks into the forest and took another set of pictures. The classifier appeared to work perfectly. Unfortunately, it had merely learned how to distinguish trees with shadows from trees without shadows—the first set of pictures was taken in the early morning, the second set at noon.
Nonstationary Distributions
A much more subtle situation arises when the distribution changes slowly (also known as nonstationary distribution) and the model is not updated adequately. Below are some typical cases.
• We train a computational advertising model and then fail to update it frequently (e.g., we forget to incorporate that an obscure new device called an iPad was just launched).
• We build a spam filter. It works well at detecting all spam that we have seen so far. But then the spammers wisen up and craft new messages that look unlike anything we have seen before.
• We build a product recommendation system. It works throughout the winter but then continues to recommend Santa hats long after Christmas.
More Anecdotes
• We build a face detector. It works well on all benchmarks. Unfortunately it fails on test data—the offending examples are close-ups where the face fills the entire image (no such data were in the training set).
• We build a Web search engine for the US market and want to deploy it in the UK.
• We train an image classifier by compiling a large dataset where each among a large set of classes is equally represented in the dataset, say 1000 categories, represented by 1000 images each. Then we deploy the system in the real world, where the actual label distribution of photographs is decidedly non-uniform.

4.9. Environment and Distribution Shift

179

4.9.3 Correction of Distribution Shift
As we have discussed, there are many cases where training and test distributions P (x, y) are different. In some cases, we get lucky and the models work despite covariate, label, or concept shift. In other cases, we can do better by employing principled strategies to cope with the shift. The remainder of this section grows considerably more technical. The impatient reader could continue on to the next section as this material is not prerequisite to subsequent concepts.

Empirical Risk and Risk

Let us first reflect about what exactly is happening during model training: we iterate over features and associated labels of training data {(x1, y1), . . . , (xn, yn)} and update the parameters of a model f after every minibatch. For simplicity we do not consider regularization, so we largely minimize the loss on the training:

1 ∑n

minimize

f

n

l(f (xi), yi),

i=1

(4.9.1)

where l is the loss function measuring “how bad” the prediction f (xi) is given the associated label

yi. Statisticians call the term in (4.9.1) empirical risk. The empirical risk is an average loss over the

training data to approximate the risk, which is the expectation of the loss over the entire population

of data drawn from their true distribution p(x, y):

∫∫

Ep(x,y)[l(f (x), y)] =

l(f (x), y)p(x, y) dxdy.

(4.9.2)

However, in practice we typically cannot obtain the entire population of data. Thus, empirical risk minimization, which is minimizing the empirical risk in (4.9.1), is a practical strategy for machine learning, with the hope to approximate minimizing the risk.

Covariate Shift Correction

Assume that we want to estimate some dependency P (y | x) for which we have labeled data (xi, yi).

Unfortunately, the observations xi are drawn from some source distribution q(x) rather than the

target distribution p(x). Fortunately, the dependency assumption means that the conditional dis-

tribution does not change: p(y | x) = q(y | x). If the source distribution q(x) is “wrong”, we can

correct for that by using the following simple identity in the risk:

∫ ∫ ∫ ∫ p(x)

l(f (x), y)p(y | x)p(x) dxdy =

l(f (x), y)q(y | x)q(x) q(x) dxdy.

(4.9.3)

In other words, we need to reweigh each data example by the ratio of the probability that it would have been drawn from the correct distribution to that from the wrong one:

def p(xi)

βi

=

. q(xi)

(4.9.4)

Plugging in the weight βi for each data example (xi, yi) we can train our model using weighted empirical risk minimization:

1 ∑n

minimize

f

n

βil(f (xi), yi).

i=1

(4.9.5)

180

Chapter 4. Multilayer Perceptrons

Alas, we do not know that ratio, so before we can do anything useful we need to estimate it. Many methods are available, including some fancy operator-theoretic approaches that attempt to recalibrate the expectation operator directly using a minimum-norm or a maximum entropy principle. Note that for any such approach, we need samples drawn from both distributions—the “true” p, e.g., by access to test data, and the one used for generating the training set q (the latter is trivially available). Note however, that we only need features x ∼ p(x); we do not need to access labels y ∼ p(y).
In this case, there exists a very effective approach that will give almost as good results as the original: logistic regression, which is a special case of softmax regression (see Section 3.4) for binary classification. This is all that is needed to compute estimated probability ratios. We learn a classifier to distinguish between data drawn from p(x) and data drawn from q(x). If it is impossible to distinguish between the two distributions then it means that the associated instances are equally likely to come from either one of the two distributions. On the other hand, any instances that can be well discriminated should be significantly overweighted or underweighted accordingly.
For simplicityʼs sake assume that we have an equal number of instances from both distributions p(x) and q(x), respectively. Now denote by z labels that are 1 for data drawn from p and −1 for data drawn from q. Then the probability in a mixed dataset is given by

P (z = 1 | x) = p(x) and hence P (z = 1 | x) = p(x) .

p(x) + q(x)

P (z = −1 | x) q(x)

(4.9.6)

Thus,

if

we use

a

logistic

regression approach,

where

P (z

=

1

|

x)

=

1 1+exp(−h(x))

(h

is

a

parame-

terized function), it follows that

βi =

1/(1 + exp(−h(xi)))

= exp(h(xi)).

exp(−h(xi))/(1 + exp(−h(xi)))

(4.9.7)

As a result, we need to solve two problems: first one to distinguish between data drawn from both distributions, and then a weighted empirical risk minimization problem in (4.9.5) where we weigh terms by βi.
Now we are ready to describe a correction algorithm. Suppose that we have a training set {(x1, y1), . . . , (xn, yn)} and an unlabeled test set {u1, . . . , um}. For covariate shift, we assume that xi for all 1 ≤ i ≤ n are drawn from some source distribution and ui for all 1 ≤ i ≤ m are drawn from the target distribution. Here is a prototypical algorithm for correcting covariate shift:
1. Generate a binary-classification training set: {(x1, −1), . . . , (xn, −1), (u1, 1), . . . , (um, 1)}.
2. Train a binary classifier using logistic regression to get function h.
3. Weigh training data using βi = exp(h(xi)) or better βi = min(exp(h(xi)), c) for some constant c.
4. Use weights βi for training on {(x1, y1), . . . , (xn, yn)} in (4.9.5).
Note that the above algorithm relies on a crucial assumption. For this scheme to work, we need that each data example in the target (e.g., test time) distribution had nonzero probability of occurring at training time. If we find a point where p(x) > 0 but q(x) = 0, then the corresponding importance weight should be infinity.

4.9. Environment and Distribution Shift

181

Label Shift Correction

Assume that we are dealing with a classification task with k categories. Using the same notation in

Section 4.9.3, q and p are the source distribution (e.g., training time) and target distribution (e.g.,

test time), respectively. Assume that the distribution of labels shifts over time: q(y) ̸= p(y), but

the class-conditional distribution stays the same: q(x | y) = p(x | y). If the source distribution

q(y) is “wrong”, we can correct for that according to the following identity in the risk as defined

in (4.9.2):

∫ ∫ ∫ ∫ p(y)

l(f (x), y)p(x | y)p(y) dxdy =

l(f (x), y)q(x | y)q(y) q(y) dxdy.

(4.9.8)

Here, our importance weights will correspond to the label likelihood ratios

def p(yi)

βi =

.

q(yi)

(4.9.9)

One nice thing about label shift is that if we have a reasonably good model on the source distribution, then we can get consistent estimates of these weights without ever having to deal with the ambient dimension. In deep learning, the inputs tend to be high-dimensional objects like images, while the labels are often simpler objects like categories.
To estimate the target label distribution, we first take our reasonably good off-the-shelf classifier (typically trained on the training data) and compute its confusion matrix using the validation set (also from the training distribution). The confusion matrix, C, is simply a k × k matrix, where each column corresponds to the label category (ground truth) and each row corresponds to our modelʼs predicted category. Each cellʼs value cij is the fraction of total predictions on the validation set where the true label was j and our model predicted i.
Now, we cannot calculate the confusion matrix on the target data directly, because we do not get to see the labels for the examples that we see in the wild, unless we invest in a complex real-time annotation pipeline. What we can do, however, is average all of our models predictions at test time together, yielding the mean model outputs µ(yˆ) ∈ Rk, whose ith element µ(yˆi) is the fraction of total predictions on the test set where our model predicted i.
It turns out that under some mild conditions—if our classifier was reasonably accurate in the first place, and if the target data contain only categories that we have seen before, and if the label shift assumption holds in the first place (the strongest assumption here), then we can estimate the test set label distribution by solving a simple linear system

Cp(y) = µ(yˆ),

(4.9.10)

because as an estimate ∑kj=1 cijp(yj) = µ(yˆi) holds for all 1 ≤ i ≤ k, where p(yj) is the jth element of the k-dimensional label distribution vector p(y). If our classifier is sufficiently accurate to begin with, then the confusion matrix C will be invertible, and we get a solution p(y) = C−1µ(yˆ).

Because we observe the labels on the source data, it is easy to estimate the distribution q(y). Then for any training example i with label yi, we can take the ratio of our estimated p(yi)/q(yi) to calculate the weight βi, and plug this into weighted empirical risk minimization in (4.9.5).

182

Chapter 4. Multilayer Perceptrons

Concept Shift Correction
Concept shift is much harder to fix in a principled manner. For instance, in a situation where suddenly the problem changes from distinguishing cats from dogs to one of distinguishing white from black animals, it will be unreasonable to assume that we can do much better than just collecting new labels and training from scratch. Fortunately, in practice, such extreme shifts are rare. Instead, what usually happens is that the task keeps on changing slowly. To make things more concrete, here are some examples:
• In computational advertising, new products are launched, old products become less popular. This means that the distribution over ads and their popularity changes gradually and any click-through rate predictor needs to change gradually with it.
• Traffic camera lenses degrade gradually due to environmental wear, affecting image quality progressively.
• News content changes gradually (i.e., most of the news remains unchanged but new stories appear).
In such cases, we can use the same approach that we used for training networks to make them adapt to the change in the data. In other words, we use the existing network weights and simply perform a few update steps with the new data rather than training from scratch.
4.9.4 A Taxonomy of Learning Problems
Armed with knowledge about how to deal with changes in distributions, we can now consider some other aspects of machine learning problem formulation.
Batch Learning
In batch learning, we have access to training features and labels {(x1, y1), . . . , (xn, yn)}, which we use to train a model f (x). Later on, we deploy this model to score new data (x, y) drawn from the same distribution. This is the default assumption for any of the problems that we discuss here. For instance, we might train a cat detector based on lots of pictures of cats and dogs. Once we trained it, we ship it as part of a smart catdoor computer vision system that lets only cats in. This is then installed in a customerʼs home and is never updated again (barring extreme circumstances).
Online Learning
Now imagine that the data (xi, yi) arrives one sample at a time. More specifically, assume that we first observe xi, then we need to come up with an estimate f (xi) and only once we have done this, we observe yi and with it, we receive a reward or incur a loss, given our decision. Many real problems fall into this category. For example, we need to predict tomorrowʼs stock price, this allows us to trade based on that estimate and at the end of the day we find out whether our estimate allowed us to make a profit. In other words, in online learning, we have the following cycle where we are continuously improving our model given new observations.
model ft −→ data xt −→ estimate ft(xt) −→ observation yt −→ loss l(yt, ft(xt)) −→ model ft+1 (4.9.11)

4.9. Environment and Distribution Shift

183

Bandits
Bandits are a special case of the problem above. While in most learning problems we have a continuously parametrized function f where we want to learn its parameters (e.g., a deep network), in a bandit problem we only have a finite number of arms that we can pull, i.e., a finite number of actions that we can take. It is not very surprising that for this simpler problem stronger theoretical guarantees in terms of optimality can be obtained. We list it mainly since this problem is often (confusingly) treated as if it were a distinct learning setting.
Control
In many cases the environment remembers what we did. Not necessarily in an adversarial manner but it will just remember and the response will depend on what happened before. For instance, a coffee boiler controller will observe different temperatures depending on whether it was heating the boiler previously. PID (proportional-integral-derivative) controller algorithms are a popular choice there. Likewise, a userʼs behavior on a news site will depend on what we showed him previously (e.g., he will read most news only once). Many such algorithms form a model of the environment in which they act such as to make their decisions appear less random. Recently, control theory (e.g., PID variants) has also been used to automatically tune hyperparameters to achive better disentangling and reconstruction quality, and improve the diversity of generated text and the reconstruction quality of generated images (Shao et al., 2020).
Reinforcement Learning
In the more general case of an environment with memory, we may encounter situations where the environment is trying to cooperate with us (cooperative games, in particular for non-zero-sum games), or others where the environment will try to win. Chess, Go, Backgammon, or StarCraft are some of the cases in reinforcement learning. Likewise, we might want to build a good controller for autonomous cars. The other cars are likely to respond to the autonomous carʼs driving style in nontrivial ways, e.g., trying to avoid it, trying to cause an accident, and trying to cooperate with it.
Considering the Environment
One key distinction between the different situations above is that the same strategy that might have worked throughout in the case of a stationary environment, might not work throughout when the environment can adapt. For instance, an arbitrage opportunity discovered by a trader is likely to disappear once he starts exploiting it. The speed and manner at which the environment changes determines to a large extent the type of algorithms that we can bring to bear. For instance, if we know that things may only change slowly, we can force any estimate to change only slowly, too. If we know that the environment might change instantaneously, but only very infrequently, we can make allowances for that. These types of knowledge are crucial for the aspiring data scientist to deal with concept shift, i.e., when the problem that he is trying to solve changes over time.

184

Chapter 4. Multilayer Perceptrons

4.9.5 Fairness, Accountability, and Transparency in Machine Learning
Finally, it is important to remember that when you deploy machine learning systems you are not merely optimizing a predictive model—you are typically providing a tool that will be used to (partially or fully) automate decisions. These technical systems can impact the lives of individuals subject to the resulting decisions. The leap from considering predictions to decisions raises not only new technical questions, but also a slew of ethical questions that must be carefully considered. If we are deploying a medical diagnostic system, we need to know for which populations it may work and which it may not. Overlooking foreseeable risks to the welfare of a subpopulation could cause us to administer inferior care. Moreover, once we contemplate decision-making systems, we must step back and reconsider how we evaluate our technology. Among other consequences of this change of scope, we will find that accuracy is seldom the right measure. For instance, when translating predictions into actions, we will often want to take into account the potential cost sensitivity of erring in various ways. If one way of misclassifying an image could be perceived as a racial sleight of hand, while misclassification to a different category would be harmless, then we might want to adjust our thresholds accordingly, accounting for societal values in designing the decision-making protocol. We also want to be careful about how prediction systems can lead to feedback loops. For example, consider predictive policing systems, which allocate patrol officers to areas with high forecasted crime. It is easy to see how a worrying pattern can emerge:
1. Neighborhoods with more crime get more patrols.
2. Consequently, more crimes are discovered in these neighborhoods, entering the training data available for future iterations.
3. Exposed to more positives, the model predicts yet more crime in these neighborhoods.
4. In the next iteration, the updated model targets the same neighborhood even more heavily leading to yet more crimes discovered, etc.
Often, the various mechanisms by which a modelʼs predictions become coupled to its training data are unaccounted for in the modeling process. This can lead to what researchers call runaway feedback loops. Additionally, we want to be careful about whether we are addressing the right problem in the first place. Predictive algorithms now play an outsize role in mediating the dissemination of information. Should the news that an individual encounters be determined by the set of Facebook pages they have Liked? These are just a few among the many pressing ethical dilemmas that you might encounter in a career in machine learning.

Summary
• In many cases training and test sets do not come from the same distribution. This is called distribution shift.
• The risk is the expectation of the loss over the entire population of data drawn from their true distribution. However, this entire population is usually unavailable. Empirical risk is an average loss over the training data to approximate the risk. In practice, we perform empirical risk minimization.
• Under the corresponding assumptions, covariate and label shift can be detected and corrected for at test time. Failure to account for this bias can become problematic at test time.
• In some cases, the environment may remember automated actions and respond in surprising ways. We must account for this possibility when building models and continue to mon-

4.9. Environment and Distribution Shift

185

itor live systems, open to the possibility that our models and the environment will become entangled in unanticipated ways.
Exercises
1. What could happen when we change the behavior of a search engine? What might the users do? What about the advertisers?
2. Implement a covariate shift detector. Hint: build a classifier. 3. Implement a covariate shift corrector. 4. Besides distribution shift, what else could affect how the empirical risk approximates the
risk? Discussions74

4.10 Predicting House Prices on Kaggle
Now that we have introduced some basic tools for building and training deep networks and regularizing them with techniques including weight decay and dropout, we are ready to put all this knowledge into practice by participating in a Kaggle competition. The house price prediction competition is a great place to start. The data are fairly generic and do not exhibit exotic structure that might require specialized models (as audio or video might). This dataset, collected by Bart de Cock in 2011 (DeCock, 2011), covers house prices in Ames, IA from the period of 2006–2010. It is considerably larger than the famous Boston housing dataset75 of Harrison and Rubinfeld (1978), boasting both more examples and more features.
In this section, we will walk you through details of data preprocessing, model design, and hyperparameter selection. We hope that through a hands-on approach, you will gain some intuitions that will guide you in your career as a data scientist.

4.10.1 Downloading and Caching Datasets
Throughout the book, we will train and test models on various downloaded datasets. Here, we implement several utility functions to facilitate data downloading. First, we maintain a dictionary DATA_HUB that maps a string (the name of the dataset) to a tuple containing both the URL to locate the dataset and the SHA-1 key that verifies the integrity of the file. All such datasets are hosted at the site whose address is DATA_URL.
import hashlib import os import tarfile import zipfile import requests
#@save DATA_HUB = dict() DATA_URL = 'http://d2l-data.s3-accelerate.amazonaws.com/'
74 https://discuss.d2l.ai/t/105 75 https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.names

186

Chapter 4. Multilayer Perceptrons

The following download function downloads a dataset, caches it in a local directory (../data by default), and returns the name of the downloaded file. If a file corresponding to this dataset already exists in the cache directory and its SHA-1 matches the one stored in DATA_HUB, our code will use the cached file to avoid clogging up your internet with redundant downloads.
def download(name, cache_dir=os.path.join('..', 'data')): #@save """Download a file inserted into DATA_HUB, return the local filename.""" assert name in DATA_HUB, f"{name} does not exist in {DATA_HUB}." url, sha1_hash = DATA_HUB[name] os.makedirs(cache_dir, exist_ok=True) fname = os.path.join(cache_dir, url.split('/')[-1]) if os.path.exists(fname): sha1 = hashlib.sha1() with open(fname, 'rb') as f: while True: data = f.read(1048576) if not data: break sha1.update(data) if sha1.hexdigest() == sha1_hash: return fname # Hit cache print(f'Downloading {fname} from {url}...') r = requests.get(url, stream=True, verify=True) with open(fname, 'wb') as f: f.write(r.content) return fname
We also implement two additional utility functions: one is to download and extract a zip or tar file and the other to download all the datasets used in this book from DATA_HUB into the cache directory.
def download_extract(name, folder=None): #@save """Download and extract a zip/tar file.""" fname = download(name) base_dir = os.path.dirname(fname) data_dir, ext = os.path.splitext(fname) if ext == '.zip': fp = zipfile.ZipFile(fname, 'r') elif ext in ('.tar', '.gz'): fp = tarfile.open(fname, 'r') else: assert False, 'Only zip/tar files can be extracted.' fp.extractall(base_dir) return os.path.join(base_dir, folder) if folder else data_dir
def download_all(): #@save """Download all files in the DATA_HUB.""" for name in DATA_HUB: download(name)

4.10. Predicting House Prices on Kaggle

187

4.10.2 Kaggle
Kaggle76 is a popular platform that hosts machine learning competitions. Each competition centers on a dataset and many are sponsored by stakeholders who offer prizes to the winning solutions. The platform helps users to interact via forums and shared code, fostering both collaboration and competition. While leaderboard chasing often spirals out of control, with researchers focusing myopically on preprocessing steps rather than asking fundamental questions, there is also tremendous value in the objectivity of a platform that facilitates direct quantitative comparisons among competing approaches as well as code sharing so that everyone can learn what did and did not work. If you want to participate in a Kaggle competition, you will first need to register for an account (see Fig. 4.10.1).

Fig. 4.10.1: The Kaggle website.
On the house price prediction competition page, as illustrated in Fig. 4.10.2, you can find the dataset (under the “Data” tab), submit predictions, and see your ranking, The URL is right here:
https://www.kaggle.com/c/house-prices-advanced-regression-techniques

Fig. 4.10.2: The house price prediction competition page.
76 https://www.kaggle.com

188

Chapter 4. Multilayer Perceptrons

4.10.3 Accessing and Reading the Dataset
Note that the competition data is separated into training and test sets. Each record includes the property value of the house and attributes such as street type, year of construction, roof type, basement condition, etc. The features consist of various data types. For example, the year of construction is represented by an integer, the roof type by discrete categorical assignments, and other features by floating point numbers. And here is where reality complicates things: for some examples, some data are altogether missing with the missing value marked simply as “na”. The price of each house is included for the training set only (it is a competition after all). We will want to partition the training set to create a validation set, but we only get to evaluate our models on the official test set after uploading predictions to Kaggle. The “Data” tab on the competition tab in Fig. 4.10.2 has links to download the data.
To get started, we will read in and process the data using pandas, which we have introduced in Section 2.2. So, you will want to make sure that you have pandas installed before proceeding further. Fortunately, if you are reading in Jupyter, we can install pandas without even leaving the notebook.
# If pandas is not installed, please uncomment the following line: # !pip install pandas
%matplotlib inline import pandas as pd from mxnet import autograd, gluon, init, np, npx from mxnet.gluon import nn from d2l import mxnet as d2l
npx.set_np()
For convenience, we can download and cache the Kaggle housing dataset using the script we defined above.
DATA_HUB['kaggle_house_train'] = ( #@save DATA_URL + 'kaggle_house_pred_train.csv', '585e9cc93e70b39160e7921475f9bcd7d31219ce')
DATA_HUB['kaggle_house_test'] = ( #@save DATA_URL + 'kaggle_house_pred_test.csv', 'fa19780a7b011d9b009e8bff8e99922a8ee2eb90')
We use pandas to load the two csv files containing training and test data respectively.
train_data = pd.read_csv(download('kaggle_house_train')) test_data = pd.read_csv(download('kaggle_house_test'))
Downloading ../data/kaggle_house_pred_train.csv from http://d2l-data.s3-accelerate.amazonaws. →com/kaggle_house_pred_train.csv... Downloading ../data/kaggle_house_pred_test.csv from http://d2l-data.s3-accelerate.amazonaws. →com/kaggle_house_pred_test.csv...
The training dataset includes 1460 examples, 80 features, and 1 label, while the test data contains 1459 examples and 80 features.

4.10. Predicting House Prices on Kaggle

189

print(train_data.shape) print(test_data.shape)

(1460, 81) (1459, 80)
Let us take a look at the first four and last two features as well as the label (SalePrice) from the first four examples.
print(train_data.iloc[0:4, [0, 1, 2, 3, -3, -2, -1]])

Id MSSubClass MSZoning LotFrontage SaleType SaleCondition SalePrice

01

60

RL

65.0

WD

Normal

208500

12

20

RL

80.0

WD

Normal

181500

23

60

RL

68.0

WD

Normal

223500

34

70

RL

60.0

WD

Abnorml

140000

We can see that in each example, the first feature is the ID. This helps the model identify each training example. While this is convenient, it does not carry any information for prediction purposes. Hence, we remove it from the dataset before feeding the data into the model.
all_features = pd.concat((train_data.iloc[:, 1:-1], test_data.iloc[:, 1:]))

4.10.4 Data Preprocessing

As stated above, we have a wide variety of data types. We will need to preprocess the data before we can start modeling. Let us start with the numerical features. First, we apply a heuristic, replacing all missing values by the corresponding featureʼs mean. Then, to put all features on a common scale, we standardize the data by rescaling features to zero mean and unit variance:

x ← x − µ, σ

(4.10.1)

where µ and σ denote mean and standard deviation, respectively. To verify that this indeed trans-

forms our feature (variable) such that it has zero mean and unit variance, note that E[ x−σµ ] =

µ−µ σ

=

0 and that E[(x − µ)2]

=

(σ2

+ µ2) − 2µ2

+ µ2

=

σ2.

Intuitively, we standardize the data

for two reasons. First, it proves convenient for optimization. Second, because we do not know

a priori which features will be relevant, we do not want to penalize coefficients assigned to one

feature more than on any other.

# If test data were inaccessible, mean and standard deviation could be # calculated from training data numeric_features = all_features.dtypes[all_features.dtypes != 'object'].index all_features[numeric_features] = all_features[numeric_features].apply(
lambda x: (x - x.mean()) / (x.std())) # After standardizing the data all means vanish, hence we can set missing # values to 0 all_features[numeric_features] = all_features[numeric_features].fillna(0)

Next we deal with discrete values. This includes features such as “MSZoning”. We replace them by a one-hot encoding in the same way that we previously transformed multiclass labels into vectors

190

Chapter 4. Multilayer Perceptrons

(see Section 3.4.1). For instance, “MSZoning” assumes the values “RL” and “RM”. Dropping the “MSZoning” feature, two new indicator features “MSZoning_RL” and “MSZoning_RM” are created with values being either 0 or 1. According to one-hot encoding, if the original value of “MSZoning” is “RL”, then “MSZoning_RL” is 1 and “MSZoning_RM” is 0. The pandas package does this automatically for us.
# `Dummy_na=True` considers "na" (missing value) as a valid feature value, and # creates an indicator feature for it all_features = pd.get_dummies(all_features, dummy_na=True) all_features.shape
(2919, 331)
You can see that this conversion increases the number of features from 79 to 331. Finally, via the values attribute, we can extract the NumPy format from the pandas format and convert it into the tensor representation for training.
n_train = train_data.shape[0] train_features = np.array(all_features[:n_train].values, dtype=np.float32) test_features = np.array(all_features[n_train:].values, dtype=np.float32) train_labels = np.array(train_data.SalePrice.values.reshape(-1, 1),
dtype=np.float32)

4.10.5 Training
To get started we train a linear model with squared loss. Not surprisingly, our linear model will not lead to a competition-winning submission but it provides a sanity check to see whether there is meaningful information in the data. If we cannot do better than random guessing here, then there might be a good chance that we have a data processing bug. And if things work, the linear model will serve as a baseline giving us some intuition about how close the simple model gets to the best reported models, giving us a sense of how much gain we should expect from fancier models.
loss = gluon.loss.L2Loss()
def get_net(): net = nn.Sequential() net.add(nn.Dense(1)) net.initialize() return net

With house prices, as with stock prices, we care about relative quantities more than absolute quan-

tities.

Thus

we

tend

to

care

more

about

the

relative

error

y−yˆ y

than

about

the

absolute

error

y

−

yˆ.

For instance, if our prediction is off by USD 100,000 when estimating the price of a house in Rural

Ohio, where the value of a typical house is 125,000 USD, then we are probably doing a horrible job.

On the other hand, if we err by this amount in Los Altos Hills, California, this might represent a

stunningly accurate prediction (there, the median house price exceeds 4 million USD).

One way to address this problem is to measure the discrepancy in the logarithm of the price esti-

mates. In fact, this is also the official error measure used by the competition to evaluate the quality

of

submissions.

After

all,

a

small

value

δ

for

| log y

−

log yˆ|

≤

δ

translates

into

e−δ

≤

yˆ y

≤

eδ .

This

4.10. Predicting House Prices on Kaggle

191

leads to the following root-mean-squared-error between the logarithm of the predicted price and the logarithm of the label price:

1 ∑n (log yi − log yˆi)2. n
i=1

(4.10.2)

def log_rmse(net, features, labels): # To further stabilize the value when the logarithm is taken, set the # value less than 1 as 1 clipped_preds = np.clip(net(features), 1, float('inf')) return np.sqrt(2 * loss(np.log(clipped_preds), np.log(labels)).mean())

Unlike in previous sections, our training functions will rely on the Adam optimizer (we will describe it in greater detail later). The main appeal of this optimizer is that, despite doing no better (and sometimes worse) given unlimited resources for hyperparameter optimization, people tend to find that it is significantly less sensitive to the initial learning rate.
def train(net, train_features, train_labels, test_features, test_labels, num_epochs, learning_rate, weight_decay, batch_size):
train_ls, test_ls = [], [] train_iter = d2l.load_array((train_features, train_labels), batch_size) # The Adam optimization algorithm is used here trainer = gluon.Trainer(net.collect_params(), 'adam', {
'learning_rate': learning_rate, 'wd': weight_decay}) for epoch in range(num_epochs): for X, y in train_iter:
with autograd.record(): l = loss(net(X), y)
l.backward() trainer.step(batch_size) train_ls.append(log_rmse(net, train_features, train_labels)) if test_labels is not None: test_ls.append(log_rmse(net, test_features, test_labels)) return train_ls, test_ls

4.10.6 K-Fold Cross-Validation

You might recall that we introduced K-fold cross-validation in the section where we discussed how to deal with model selection (Section 4.4). We will put this to good use to select the model design and to adjust the hyperparameters. We first need a function that returns the ith fold of the data in a K-fold cross-validation procedure. It proceeds by slicing out the ith segment as validation data and returning the rest as training data. Note that this is not the most efficient way of handling data and we would definitely do something much smarter if our dataset was considerably larger. But this added complexity might obfuscate our code unnecessarily so we can safely omit it here owing to the simplicity of our problem.

def get_k_fold_data(k, i, X, y): assert k > 1 fold_size = X.shape[0] // k X_train, y_train = None, None

(continues on next page)

192

Chapter 4. Multilayer Perceptrons

for j in range(k): idx = slice(j * fold_size, (j + 1) * fold_size) X_part, y_part = X[idx, :], y[idx] if j == i: X_valid, y_valid = X_part, y_part elif X_train is None: X_train, y_train = X_part, y_part else: X_train = np.concatenate([X_train, X_part], 0) y_train = np.concatenate([y_train, y_part], 0)
return X_train, y_train, X_valid, y_valid

(continued from previous page)

The training and verification error averages are returned when we train K times in the K-fold cross-validation.
def k_fold(k, X_train, y_train, num_epochs, learning_rate, weight_decay, batch_size):
train_l_sum, valid_l_sum = 0, 0 for i in range(k):
data = get_k_fold_data(k, i, X_train, y_train) net = get_net() train_ls, valid_ls = train(net, *data, num_epochs, learning_rate,
weight_decay, batch_size) train_l_sum += train_ls[-1] valid_l_sum += valid_ls[-1] if i == 0:
d2l.plot(list(range(1, num_epochs + 1)), [train_ls, valid_ls], xlabel='epoch', ylabel='rmse', xlim=[1, num_epochs], legend=['train', 'valid'], yscale='log')
print(f'fold {i + 1}, train log rmse {float(train_ls[-1]):f}, ' f'valid log rmse {float(valid_ls[-1]):f}')
return train_l_sum / k, valid_l_sum / k

4.10.7 Model Selection
In this example, we pick an untuned set of hyperparameters and leave it up to the reader to improve the model. Finding a good choice can take time, depending on how many variables one optimizes over. With a large enough dataset, and the normal sorts of hyperparameters, K-fold cross-validation tends to be reasonably resilient against multiple testing. However, if we try an unreasonably large number of options we might just get lucky and find that our validation performance is no longer representative of the true error.
k, num_epochs, lr, weight_decay, batch_size = 5, 100, 5, 0, 64 train_l, valid_l = k_fold(k, train_features, train_labels, num_epochs, lr,
weight_decay, batch_size) print(f'{k}-fold validation: avg train log rmse: {float(train_l):f}, '
f'avg valid log rmse: {float(valid_l):f}')

fold 1, train log rmse 0.169920, valid log rmse 0.157133 fold 2, train log rmse 0.162058, valid log rmse 0.189706 fold 3, train log rmse 0.163755, valid log rmse 0.168006

(continues on next page)

4.10. Predicting House Prices on Kaggle

193

(continued from previous page)
fold 4, train log rmse 0.167755, valid log rmse 0.154697 fold 5, train log rmse 0.162671, valid log rmse 0.182936 5-fold validation: avg train log rmse: 0.165232, avg valid log rmse: 0.170495

Notice that sometimes the number of training errors for a set of hyperparameters can be very low, even as the number of errors on K-fold cross-validation is considerably higher. This indicates that we are overfitting. Throughout training you will want to monitor both numbers. Less overfitting might indicate that our data can support a more powerful model. Massive overfitting might suggest that we can gain by incorporating regularization techniques.
4.10.8 Submitting Predictions on Kaggle
Now that we know what a good choice of hyperparameters should be, we might as well use all the data to train on it (rather than just 1 − 1/K of the data that are used in the cross-validation slices). The model that we obtain in this way can then be applied to the test set. Saving the predictions in a csv file will simplify uploading the results to Kaggle.
def train_and_pred(train_features, test_feature, train_labels, test_data, num_epochs, lr, weight_decay, batch_size):
net = get_net() train_ls, _ = train(net, train_features, train_labels, None, None,
num_epochs, lr, weight_decay, batch_size) d2l.plot(np.arange(1, num_epochs + 1), [train_ls], xlabel='epoch',
ylabel='log rmse', xlim=[1, num_epochs], yscale='log') print(f'train log rmse {float(train_ls[-1]):f}') # Apply the network to the test set preds = net(test_features).asnumpy() # Reformat it to export to Kaggle test_data['SalePrice'] = pd.Series(preds.reshape(1, -1)[0]) submission = pd.concat([test_data['Id'], test_data['SalePrice']], axis=1) submission.to_csv('submission.csv', index=False)
One nice sanity check is to see whether the predictions on the test set resemble those of the K-fold cross-validation process. If they do, it is time to upload them to Kaggle. The following code will generate a file called submission.csv.

194

Chapter 4. Multilayer Perceptrons

train_and_pred(train_features, test_features, train_labels, test_data, num_epochs, lr, weight_decay, batch_size)
train log rmse 0.162752

Next, as demonstrated in Fig. 4.10.3, we can submit our predictions on Kaggle and see how they compare with the actual house prices (labels) on the test set. The steps are quite simple:
• Log in to the Kaggle website and visit the house price prediction competition page.
• Click the “Submit Predictions” or “Late Submission” button (as of this writing, the button is located on the right).
• Click the “Upload Submission File” button in the dashed box at the bottom of the page and select the prediction file you wish to upload.
• Click the “Make Submission” button at the bottom of the page to view your results.

Fig. 4.10.3: Submitting data to Kaggle

4.10. Predicting House Prices on Kaggle

195

Summary
• Real data often contain a mix of different data types and need to be preprocessed. • Rescaling real-valued data to zero mean and unit variance is a good default. So is replacing
missing values with their mean. • Transforming categorical features into indicator features allows us to treat them like one-hot
vectors. • We can use K-fold cross-validation to select the model and adjust the hyperparameters. • Logarithms are useful for relative errors.
Exercises
1. Submit your predictions for this section to Kaggle. How good are your predictions? 2. Can you improve your model by minimizing the logarithm of prices directly? What happens
if you try to predict the logarithm of the price rather than the price? 3. Is it always a good idea to replace missing values by their mean? Hint: can you construct a
situation where the values are not missing at random? 4. Improve the score on Kaggle by tuning the hyperparameters through K-fold cross-
validation. 5. Improve the score by improving the model (e.g., layers, weight decay, and dropout). 6. What happens if we do not standardize the continuous numerical features like what we have
done in this section? Discussions77

77 https://discuss.d2l.ai/t/106
196

Chapter 4. Multilayer Perceptrons

5 | Deep Learning Computation
Alongside giant datasets and powerful hardware, great software tools have played an indispensable role in the rapid progress of deep learning. Starting with the pathbreaking Theano library released in 2007, flexible open-source tools have enabled researchers to rapidly prototype models, avoiding repetitive work when recycling standard components while still maintaining the ability to make low-level modifications. Over time, deep learningʼs libraries have evolved to offer increasingly coarse abstractions. Just as semiconductor designers went from specifying transistors to logical circuits to writing code, neural networks researchers have moved from thinking about the behavior of individual artificial neurons to conceiving of networks in terms of whole layers, and now often design architectures with far coarser blocks in mind.
So far, we have introduced some basic machine learning concepts, ramping up to fully-functional deep learning models. In the last chapter, we implemented each component of an MLP from scratch and even showed how to leverage high-level APIs to roll out the same models effortlessly. To get you that far that fast, we called upon the libraries, but skipped over more advanced details about how they work. In this chapter, we will peel back the curtain, digging deeper into the key components of deep learning computation, namely model construction, parameter access and initialization, designing custom layers and blocks, reading and writing models to disk, and leveraging GPUs to achieve dramatic speedups. These insights will move you from end user to power user, giving you the tools needed to reap the benefits of a mature deep learning library while retaining the flexibility to implement more complex models, including those you invent yourself! While this chapter does not introduce any new models or datasets, the advanced modeling chapters that follow rely heavily on these techniques.
5.1 Layers and Blocks
When we first introduced neural networks, we focused on linear models with a single output. Here, the entire model consists of just a single neuron. Note that a single neuron (i) takes some set of inputs; (ii) generates a corresponding scalar output; and (iii) has a set of associated parameters that can be updated to optimize some objective function of interest. Then, once we started thinking about networks with multiple outputs, we leveraged vectorized arithmetic to characterize an entire layer of neurons. Just like individual neurons, layers (i) take a set of inputs, (ii) generate corresponding outputs, and (iii) are described by a set of tunable parameters. When we worked through softmax regression, a single layer was itself the model. However, even when we subsequently introduced MLPs, we could still think of the model as retaining this same basic structure.
Interestingly, for MLPs, both the entire model and its constituent layers share this structure. The entire model takes in raw inputs (the features), generates outputs (the predictions), and possesses parameters (the combined parameters from all constituent layers). Likewise, each individual layer ingests inputs (supplied by the previous layer) generates outputs (the inputs to the subsequent
197

layer), and possesses a set of tunable parameters that are updated according to the signal that flows backwards from the subsequent layer.
While you might think that neurons, layers, and models give us enough abstractions to go about our business, it turns out that we often find it convenient to speak about components that are larger than an individual layer but smaller than the entire model. For example, the ResNet-152 architecture, which is wildly popular in computer vision, possesses hundreds of layers. These layers consist of repeating patterns of groups of layers. Implementing such a network one layer at a time can grow tedious. This concern is not just hypothetical—such design patterns are common in practice. The ResNet architecture mentioned above won the 2015 ImageNet and COCO computer vision competitions for both recognition and detection (He et al., 2016a) and remains a go-to architecture for many vision tasks. Similar architectures in which layers are arranged in various repeating patterns are now ubiquitous in other domains, including natural language processing and speech.
To implement these complex networks, we introduce the concept of a neural network block. A block could describe a single layer, a component consisting of multiple layers, or the entire model itself! One benefit of working with the block abstraction is that they can be combined into larger artifacts, often recursively. This is illustrated in Fig. 5.1.1. By defining code to generate blocks of arbitrary complexity on demand, we can write surprisingly compact code and still implement complex neural networks.

Fig. 5.1.1: Multiple layers are combined into blocks, forming repeating patterns of larger models.
From a programing standpoint, a block is represented by a class. Any subclass of it must define a forward propagation function that transforms its input into output and must store any necessary parameters. Note that some blocks do not require any parameters at all. Finally a block must possess a backpropagation function, for purposes of calculating gradients. Fortunately, due to some behind-the-scenes magic supplied by the auto differentiation (introduced in Section 2.5) when defining our own block, we only need to worry about parameters and the forward propagation function.
To begin, we revisit the code that we used to implement MLPs (Section 4.3). The following code generates a network with one fully-connected hidden layer with 256 units and ReLU activation, followed by a fully-connected output layer with 10 units (no activation function).

198

Chapter 5. Deep Learning Computation

from mxnet import np, npx from mxnet.gluon import nn
npx.set_np()
net = nn.Sequential() net.add(nn.Dense(256, activation='relu')) net.add(nn.Dense(10)) net.initialize()
X = np.random.uniform(size=(2, 20)) net(X)
array([[ 0.06240274, -0.03268593, 0.02582653, 0.02254181, -0.03728798, -0.04253785, 0.00540612, -0.01364185, -0.09915454, -0.02272737], [ 0.02816679, -0.03341204, 0.03565665, 0.02506384, -0.04136416, -0.04941844, 0.01738529, 0.01081963, -0.09932579, -0.01176296]])
In this example, we constructed our model by instantiating an nn.Sequential, assigning the returned object to the net variable. Next, we repeatedly call its add function, appending layers in the order that they should be executed. In short, nn.Sequential defines a special kind of Block, the class that presents a block in Gluon. It maintains an ordered list of constituent Blocks. The add function simply facilitates the addition of each successive Block to the list. Note that each layer is an instance of the Dense class which is itself a subclass of Block. The forward propagation (forward) function is also remarkably simple: it chains each Block in the list together, passing the output of each as the input to the next. Note that until now, we have been invoking our models via the construction net(X) to obtain their outputs. This is actually just shorthand for net.forward(X), a slick Python trick achieved via the Block classʼs __call__ function.
5.1.1 A Custom Block
Perhaps the easiest way to develop intuition about how a block works is to implement one ourselves. Before we implement our own custom block, we briefly summarize the basic functionality that each block must provide:
1. Ingest input data as arguments to its forward propagation function.
2. Generate an output by having the forward propagation function return a value. Note that the output may have a different shape from the input. For example, the first fully-connected layer in our model above ingests an input of arbitrary dimension but returns an output of dimension 256.
3. Calculate the gradient of its output with respect to its input, which can be accessed via its backpropagation function. Typically this happens automatically.
4. Store and provide access to those parameters necessary to execute the forward propagation computation.
5. Initialize model parameters as needed.
In the following snippet, we code up a block from scratch corresponding to an MLP with one hidden layer with 256 hidden units, and a 10-dimensional output layer. Note that the MLP class below inherits the class that represents a block. We will heavily rely on the parent classʼs functions, sup-

5.1. Layers and Blocks

199

plying only our own constructor (the __init__ function in Python) and the forward propagation function.
class MLP(nn.Block): # Declare a layer with model parameters. Here, we declare two # fully-connected layers def __init__(self, **kwargs): # Call the constructor of the `MLP` parent class `Block` to perform # the necessary initialization. In this way, other function arguments # can also be specified during class instantiation, such as the model # parameters, `params` (to be described later) super().__init__(**kwargs) self.hidden = nn.Dense(256, activation='relu') # Hidden layer self.out = nn.Dense(10) # Output layer
# Define the forward propagation of the model, that is, how to return the # required model output based on the input `X` def forward(self, X):
return self.out(self.hidden(X))
Let us first focus on the forward propagation function. Note that it takes X as the input, calculates the hidden representation with the activation function applied, and outputs its logits. In this MLP implementation, both layers are instance variables. To see why this is reasonable, imagine instantiating two MLPs, net1 and net2, and training them on different data. Naturally, we would expect them to represent two different learned models.
We instantiate the MLPʼs layers in the constructor and subsequently invoke these layers on each call to the forward propagation function. Note a few key details. First, our customized __init__ function invokes the parent classʼs __init__ function via super().__init__() sparing us the pain of restating boilerplate code applicable to most blocks. We then instantiate our two fullyconnected layers, assigning them to self.hidden and self.out. Note that unless we implement a new operator, we need not worry about the backpropagation function or parameter initialization. The system will generate these functions automatically. Let us try this out.
net = MLP() net.initialize() net(X)
array([[-0.03989595, -0.10414709, 0.06799038, 0.05245074, 0.0252606 , -0.00640342, 0.04182098, -0.01665318, -0.02067345, -0.07863816], [-0.03612847, -0.07210435, 0.09159479, 0.07890773, 0.02494171, -0.01028665, 0.01732427, -0.02843244, 0.03772651, -0.06671703]])
A key virtue of the block abstraction is its versatility. We can subclass a block to create layers (such as the fully-connected layer class), entire models (such as the MLP class above), or various components of intermediate complexity. We exploit this versatility throughout the following chapters, such as when addressing convolutional neural networks.

200

Chapter 5. Deep Learning Computation

5.1.2 The Sequential Block
We can now take a closer look at how the Sequential class works. Recall that Sequential was designed to daisy-chain other blocks together. To build our own simplified MySequential, we just need to define two key function: 1. A function to append blocks one by one to a list. 2. A forward propagation function to pass an input through the chain of blocks, in the same order as they were appended.
The following MySequential class delivers the same functionality of the default Sequential class.
class MySequential(nn.Block): def add(self, block): # Here, `block` is an instance of a `Block` subclass, and we assume # that it has a unique name. We save it in the member variable # `_children` of the `Block` class, and its type is OrderedDict. When # the `MySequential` instance calls the `initialize` function, the # system automatically initializes all members of `_children` self._children[block.name] = block
def forward(self, X): # OrderedDict guarantees that members will be traversed in the order # they were added for block in self._children.values(): X = block(X) return X
The add function adds a single block to the ordered dictionary _children. You might wonder why every Gluon Block possesses a _children attribute and why we used it rather than just define a Python list ourselves. In short the chief advantage of _children is that during our blockʼs parameter initialization, Gluon knows to look inside the _children dictionary to find sub-blocks whose parameters also need to be initialized.
When our MySequentialʼs forward propagation function is invoked, each added block is executed in the order in which they were added. We can now reimplement an MLP using our MySequential class.
net = MySequential() net.add(nn.Dense(256, activation='relu')) net.add(nn.Dense(10)) net.initialize() net(X)
array([[-0.0764568 , -0.01130233, 0.04952145, -0.04651389, -0.04131571, -0.05884131, -0.06213811, 0.01311471, -0.01379425, -0.02514282], [-0.05124623, 0.00711232, -0.00155933, -0.07555379, -0.06675334, -0.01762914, 0.00589085, 0.0144719 , -0.04330775, 0.03317727]])
Note that this use of MySequential is identical to the code we previously wrote for the Sequential class (as described in Section 4.3).

5.1. Layers and Blocks

201

5.1.3 Executing Code in the Forward Propagation Function
The Sequential class makes model construction easy, allowing us to assemble new architectures without having to define our own class. However, not all architectures are simple daisy chains. When greater flexibility is required, we will want to define our own blocks. For example, we might want to execute Pythonʼs control flow within the forward propagation function. Moreover, we might want to perform arbitrary mathematical operations, not simply relying on predefined neural network layers.
You might have noticed that until now, all of the operations in our networks have acted upon our networkʼs activations and its parameters. Sometimes, however, we might want to incorporate terms that are neither the result of previous layers nor updatable parameters. We call these constant parameters. Say for example that we want a layer that calculates the function f (x, w) = c·w⊤x, where x is the input, w is our parameter, and c is some specified constant that is not updated during optimization. So we implement a FixedHiddenMLP class as follows.
class FixedHiddenMLP(nn.Block): def __init__(self, **kwargs): super().__init__(**kwargs) # Random weight parameters created with the `get_constant` function # are not updated during training (i.e., constant parameters) self.rand_weight = self.params.get_constant( 'rand_weight', np.random.uniform(size=(20, 20))) self.dense = nn.Dense(20, activation='relu')
def forward(self, X): X = self.dense(X) # Use the created constant parameters, as well as the `relu` and `dot` # functions X = npx.relu(np.dot(X, self.rand_weight.data()) + 1) # Reuse the fully-connected layer. This is equivalent to sharing # parameters with two fully-connected layers X = self.dense(X) # Control flow while np.abs(X).sum() > 1: X /= 2 return X.sum()
In this FixedHiddenMLP model, we implement a hidden layer whose weights (self.rand_weight) are initialized randomly at instantiation and are thereafter constant. This weight is not a model parameter and thus it is never updated by backpropagation. The network then passes the output of this “fixed” layer through a fully-connected layer.
Note that before returning the output, our model did something unusual. We ran a while-loop, testing on the condition its L1 norm is larger than 1, and dividing our output vector by 2 until it satisfied the condition. Finally, we returned the sum of the entries in X. To our knowledge, no standard neural network performs this operation. Note that this particular operation may not be useful in any real-world task. Our point is only to show you how to integrate arbitrary code into the flow of your neural network computations.
net = FixedHiddenMLP() net.initialize() net(X)

202

Chapter 5. Deep Learning Computation

array(0.52637565)
We can mix and match various ways of assembling blocks together. In the following example, we nest blocks in some creative ways.
class NestMLP(nn.Block): def __init__(self, **kwargs): super().__init__(**kwargs) self.net = nn.Sequential() self.net.add(nn.Dense(64, activation='relu'), nn.Dense(32, activation='relu')) self.dense = nn.Dense(16, activation='relu')
def forward(self, X): return self.dense(self.net(X))
chimera = nn.Sequential() chimera.add(NestMLP(), nn.Dense(20), FixedHiddenMLP()) chimera.initialize() chimera(X)
array(0.9772054)
5.1.4 Efficiency
The avid reader might start to worry about the efficiency of some of these operations. After all, we have lots of dictionary lookups, code execution, and lots of other Pythonic things taking place in what is supposed to be a high-performance deep learning library. The problems of Pythonʼs global interpreter lock78 are well known. In the context of deep learning, we may worry that our extremely fast GPU(s) might have to wait until a puny CPU runs Python code before it gets another job to run. The best way to speed up Python is by avoiding it altogether.
One way that Gluon does this is by allowing for hybridization, which will be described later. Here, the Python interpreter executes a block the first time it is invoked. The Gluon runtime records what is happening and the next time around it short-circuits calls to Python. This can accelerate things considerably in some cases but care needs to be taken when control flow (as above) leads down different branches on different passes through the net. We recommend that the interested reader checks out the hybridization section (Section 12.1) to learn about compilation after finishing the current chapter.
78 https://wiki.python.org/moin/GlobalInterpreterLock

5.1. Layers and Blocks

203

Summary
• Layers are blocks. • Many layers can comprise a block. • Many blocks can comprise a block. • A block can contain code. • Blocks take care of lots of housekeeping, including parameter initialization and backpropa-
gation. • Sequential concatenations of layers and blocks are handled by the Sequential block.
Exercises
1. What kinds of problems will occur if you change MySequential to store blocks in a Python list?
2. Implement a block that takes two blocks as an argument, say net1 and net2 and returns the concatenated output of both networks in the forward propagation. This is also called a parallel block.
3. Assume that you want to concatenate multiple instances of the same network. Implement a factory function that generates multiple instances of the same block and build a larger network from it.
Discussions79
5.2 Parameter Management
Once we have chosen an architecture and set our hyperparameters, we proceed to the training loop, where our goal is to find parameter values that minimize our loss function. After training, we will need these parameters in order to make future predictions. Additionally, we will sometimes wish to extract the parameters either to reuse them in some other context, to save our model to disk so that it may be executed in other software, or for examination in the hope of gaining scientific understanding. Most of the time, we will be able to ignore the nitty-gritty details of how parameters are declared and manipulated, relying on deep learning frameworks to do the heavy lifting. However, when we move away from stacked architectures with standard layers, we will sometimes need to get into the weeds of declaring and manipulating parameters. In this section, we cover the following:
• Accessing parameters for debugging, diagnostics, and visualizations. • Parameter initialization. • Sharing parameters across different model components. We start by focusing on an MLP with one hidden layer.
79 https://discuss.d2l.ai/t/54

204

Chapter 5. Deep Learning Computation

from mxnet import init, np, npx from mxnet.gluon import nn
npx.set_np()
net = nn.Sequential() net.add(nn.Dense(8, activation='relu')) net.add(nn.Dense(1)) net.initialize() # Use the default initialization method
X = np.random.uniform(size=(2, 4)) net(X) # Forward computation
array([[0.0054572 ], [0.00488594]])
5.2.1 Parameter Access
Let us start with how to access parameters from the models that you already know. When a model is defined via the Sequential class, we can first access any layer by indexing into the model as though it were a list. Each layerʼs parameters are conveniently located in its attribute. We can inspect the parameters of the second fully-connected layer as follows.
print(net[1].params)
dense1_ ( Parameter dense1_weight (shape=(1, 8), dtype=float32) Parameter dense1_bias (shape=(1,), dtype=float32)
)
The output tells us a few important things. First, this fully-connected layer contains two parameters, corresponding to that layerʼs weights and biases, respectively. Both are stored as single precision floats (float32). Note that the names of the parameters allow us to uniquely identify each layerʼs parameters, even in a network containing hundreds of layers.
Targeted Parameters
Note that each parameter is represented as an instance of the parameter class. To do anything useful with the parameters, we first need to access the underlying numerical values. There are several ways to do this. Some are simpler while others are more general. The following code extracts the bias from the second neural network layer, which returns a parameter class instance, and further accesses that parameterʼs value.
print(type(net[1].bias)) print(net[1].bias) print(net[1].bias.data())

5.2. Parameter Management

205

<class 'mxnet.gluon.parameter.Parameter'> Parameter dense1_bias (shape=(1,), dtype=float32) [0.]
Parameters are complex objects, containing values, gradients, and additional information. Thatʼs why we need to request the value explicitly. In addition to the value, each parameter also allows us to access the gradient. Because we have not invoked backpropagation for this network yet, it is in its initial state.
net[1].weight.grad()
array([[0., 0., 0., 0., 0., 0., 0., 0.]])

All Parameters at Once
When we need to perform operations on all parameters, accessing them one-by-one can grow tedious. The situation can grow especially unwieldy when we work with more complex blocks (e.g., nested blocks), since we would need to recurse through the entire tree to extract each subblockʼs parameters. Below we demonstrate accessing the parameters of the first fully-connected layer vs. accessing all layers.
print(net[0].collect_params()) print(net.collect_params())

dense0_ ( Parameter dense0_weight (shape=(8, Parameter dense0_bias (shape=(8,),
) sequential0_ (
Parameter dense0_weight (shape=(8, Parameter dense0_bias (shape=(8,), Parameter dense1_weight (shape=(1, Parameter dense1_bias (shape=(1,), )

4), dtype=float32) dtype=float32)
4), dtype=float32) dtype=float32) 8), dtype=float32) dtype=float32)

This provides us with another way of accessing the parameters of the network as follows. net.collect_params()['dense1_bias'].data()

array([0.])

206

Chapter 5. Deep Learning Computation

Collecting Parameters from Nested Blocks
Let us see how the parameter naming conventions work if we nest multiple blocks inside each other. For that we first define a function that produces blocks (a block factory, so to speak) and then combine these inside yet larger blocks.
def block1(): net = nn.Sequential() net.add(nn.Dense(32, activation='relu')) net.add(nn.Dense(16, activation='relu')) return net
def block2(): net = nn.Sequential() for _ in range(4): # Nested here net.add(block1()) return net
rgnet = nn.Sequential() rgnet.add(block2()) rgnet.add(nn.Dense(10)) rgnet.initialize() rgnet(X)

array([[-6.3465846e-09, -1.1265507e-09, 5.9429879e-09, [-8.6219423e-09, -1.6740003e-09, 8.0741742e-09,

-1.1096752e-09, 6.4161787e-09, 1.3284951e-10, 9.3619388e-09, 8.8181435e-09], -7.5150686e-10, 8.3133251e-09, 3.2405989e-10, 1.2115976e-08, 1.2075874e-08]])

6.6354140e-09, 3.2229084e-09,
8.9321128e-09, 4.4926449e-09,

Now that we have designed the network, let us see how it is organized.
print(rgnet.collect_params) print(rgnet.collect_params())

<bound method Block.collect_params of Sequential( (0): Sequential( (0): Sequential( (0): Dense(4 -> 32, Activation(relu)) (1): Dense(32 -> 16, Activation(relu)) ) (1): Sequential( (0): Dense(16 -> 32, Activation(relu)) (1): Dense(32 -> 16, Activation(relu)) ) (2): Sequential( (0): Dense(16 -> 32, Activation(relu)) (1): Dense(32 -> 16, Activation(relu)) ) (3): Sequential( (0): Dense(16 -> 32, Activation(relu)) (1): Dense(32 -> 16, Activation(relu))

(continues on next page)

5.2. Parameter Management

207

) ) (1): Dense(16 -> 10, linear) )> sequential1_ ( Parameter dense2_weight (shape=(32, 4), dtype=float32) Parameter dense2_bias (shape=(32,), dtype=float32) Parameter dense3_weight (shape=(16, 32), dtype=float32) Parameter dense3_bias (shape=(16,), dtype=float32) Parameter dense4_weight (shape=(32, 16), dtype=float32) Parameter dense4_bias (shape=(32,), dtype=float32) Parameter dense5_weight (shape=(16, 32), dtype=float32) Parameter dense5_bias (shape=(16,), dtype=float32) Parameter dense6_weight (shape=(32, 16), dtype=float32) Parameter dense6_bias (shape=(32,), dtype=float32) Parameter dense7_weight (shape=(16, 32), dtype=float32) Parameter dense7_bias (shape=(16,), dtype=float32) Parameter dense8_weight (shape=(32, 16), dtype=float32) Parameter dense8_bias (shape=(32,), dtype=float32) Parameter dense9_weight (shape=(16, 32), dtype=float32) Parameter dense9_bias (shape=(16,), dtype=float32) Parameter dense10_weight (shape=(10, 16), dtype=float32) Parameter dense10_bias (shape=(10,), dtype=float32) )

(continued from previous page)

Since the layers are hierarchically nested, we can also access them as though indexing through nested lists. For instance, we can access the first major block, within it the second sub-block, and within that the bias of the first layer, with as follows.
rgnet[0][1][0].bias.data()

array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])

5.2.2 Parameter Initialization
Now that we know how to access the parameters, let us look at how to initialize them properly. We discussed the need for proper initialization in Section 4.8. The deep learning framework provides default random initializations to its layers. However, we often want to initialize our weights according to various other protocols. The framework provides most commonly used protocols, and also allows to create a custom initializer.
By default, MXNet initializes weight parameters by randomly drawing from a uniform distribution U (−0.07, 0.07), clearing bias parameters to zero. MXNetʼs init module provides a variety of preset initialization methods.

208

Chapter 5. Deep Learning Computation

Built-in Initialization
Let us begin by calling on built-in initializers. The code below initializes all weight parameters as Gaussian random variables with standard deviation 0.01, while bias parameters cleared to zero.
# Here `force_reinit` ensures that parameters are freshly initialized even if # they were already initialized previously net.initialize(init=init.Normal(sigma=0.01), force_reinit=True) net[0].weight.data()[0]

array([-0.00324057, -0.00895028, -0.00698632, 0.01030831])
We can also initialize all the parameters to a given constant value (say, 1). net.initialize(init=init.Constant(1), force_reinit=True) net[0].weight.data()[0]

array([1., 1., 1., 1.])
We can also apply different initializers for certain blocks. For example, below we initialize the first layer with the Xavier initializer and initialize the second layer to a constant value of 42.
net[0].weight.initialize(init=init.Xavier(), force_reinit=True) net[1].initialize(init=init.Constant(42), force_reinit=True) print(net[0].weight.data()[0]) print(net[1].weight.data())

[-0.17594433 0.02314097 -0.1992535 [[42. 42. 42. 42. 42. 42. 42. 42.]]

0.09509248]

Custom Initialization

Sometimes, the initialization methods we need are not provided by the deep learning framework.

In the example below, we define an initializer for any weight parameter w using the following

strange distribution:

 U (5, 10) w ∼ 0
U (−10, −5)

with

probability

1 4

with

probability

1 2

with

probability

1 4

(5.2.1)

Here we define a subclass of the Initializer class. Usually, we only need to implement the _init_weight function which takes a tensor argument (data) and assigns to it the desired initialized values.

class MyInit(init.Initializer): def _init_weight(self, name, data): print('Init', name, data.shape) data[:] = np.random.uniform(-10, 10, data.shape) data *= np.abs(data) >= 5

(continues on next page)

5.2. Parameter Management

209

net.initialize(MyInit(), force_reinit=True) net[0].weight.data()[:2]

(continued from previous page)

Init dense0_weight (8, 4) Init dense1_weight (1, 8)

array([[ 0. [ 0.

, -0.

, -0.

, -8.828651 , -0.

, 8.522827 ], , -5.6012006]])

Note that we always have the option of setting parameters directly.
net[0].weight.data()[:] += 1 net[0].weight.data()[0, 0] = 42 net[0].weight.data()[0]

array([42.

, 1.

, 1.

, 9.522827])

A note for advanced users: if you want to adjust parameters within an autograd scope, you need to use set_data to avoid confusing the automatic differentiation mechanics.

5.2.3 Tied Parameters
Often, we want to share parameters across multiple layers. Let us see how to do this elegantly. In the following we allocate a dense layer and then use its parameters specifically to set those of another layer.
net = nn.Sequential() # We need to give the shared layer a name so that we can refer to its # parameters shared = nn.Dense(8, activation='relu') net.add(nn.Dense(8, activation='relu'), shared,
nn.Dense(8, activation='relu', params=shared.params), nn.Dense(10)) net.initialize()
X = np.random.uniform(size=(2, 20)) net(X)
# Check whether the parameters are the same print(net[1].weight.data()[0] == net[2].weight.data()[0]) net[1].weight.data()[0, 0] = 100 # Make sure that they are actually the same object rather than just having the # same value print(net[1].weight.data()[0] == net[2].weight.data()[0])

[ True True True True True True True True] [ True True True True True True True True]

This example shows that the parameters of the second and third layer are tied. They are not just equal, they are represented by the same exact tensor. Thus, if we change one of the parameters,

210

Chapter 5. Deep Learning Computation

the other one changes, too. You might wonder, when parameters are tied what happens to the gradients? Since the model parameters contain gradients, the gradients of the second hidden layer and the third hidden layer are added together during backpropagation.
Summary
• We have several ways to access, initialize, and tie model parameters. • We can use custom initialization.
Exercises
1. Use the FancyMLP model defined in Section 5.1 and access the parameters of the various layers.
2. Look at the initialization module document to explore different initializers. 3. Construct an MLP containing a shared parameter layer and train it. During the training
process, observe the model parameters and gradients of each layer. 4. Why is sharing parameters a good idea? Discussions80
5.3 Deferred Initialization
So far, it might seem that we got away with being sloppy in setting up our networks. Specifically, we did the following unintuitive things, which might not seem like they should work:
• We defined the network architectures without specifying the input dimensionality. • We added layers without specifying the output dimension of the previous layer. • We even “initialized” these parameters before providing enough information to determine
how many parameters our models should contain. You might be surprised that our code runs at all. After all, there is no way the deep learning framework could tell what the input dimensionality of a network would be. The trick here is that the framework defers initialization, waiting until the first time we pass data through the model, to infer the sizes of each layer on the fly. Later on, when working with convolutional neural networks, this technique will become even more convenient since the input dimensionality (i.e., the resolution of an image) will affect the dimensionality of each subsequent layer. Hence, the ability to set parameters without the need to know, at the time of writing the code, what the dimensionality is can greatly simplify the task of specifying and subsequently modifying our models. Next, we go deeper into the mechanics of initialization.
80 https://discuss.d2l.ai/t/56

5.3. Deferred Initialization

211

5.3.1 Instantiating a Network
To begin, let us instantiate an MLP.
from mxnet import np, npx from mxnet.gluon import nn
npx.set_np()
def get_net(): net = nn.Sequential() net.add(nn.Dense(256, activation='relu')) net.add(nn.Dense(10)) return net
net = get_net()
At this point, the network cannot possibly know the dimensions of the input layerʼs weights because the input dimension remains unknown. Consequently the framework has not yet initialized any parameters. We confirm by attempting to access the parameters below.
print(net.collect_params) print(net.collect_params())

<bound method Block.collect_params of Sequential( (0): Dense(-1 -> 256, Activation(relu)) (1): Dense(-1 -> 10, linear)
)> sequential0_ (
Parameter dense0_weight (shape=(256, -1), dtype=float32) Parameter dense0_bias (shape=(256,), dtype=float32) Parameter dense1_weight (shape=(10, -1), dtype=float32) Parameter dense1_bias (shape=(10,), dtype=float32) )
Note that while the parameter objects exist, the input dimension to each layer is listed as -1. MXNet uses the special value -1 to indicate that the parameter dimension remains unknown. At this point, attempts to access net[0].weight.data() would trigger a runtime error stating that the network must be initialized before the parameters can be accessed. Now let us see what happens when we attempt to initialize parameters via the initialize function.
net.initialize() net.collect_params()

sequential0_ ( Parameter dense0_weight (shape=(256, -1), dtype=float32) Parameter dense0_bias (shape=(256,), dtype=float32) Parameter dense1_weight (shape=(10, -1), dtype=float32) Parameter dense1_bias (shape=(10,), dtype=float32)
)
As we can see, nothing has changed. When input dimensions are unknown, calls to initialize do not truly initialize the parameters. Instead, this call registers to MXNet that we wish (and option-

212

Chapter 5. Deep Learning Computation

ally, according to which distribution) to initialize the parameters. Next let us pass data through the network to make the framework finally initialize parameters.
X = np.random.uniform(size=(2, 20)) net(X)
net.collect_params()
sequential0_ ( Parameter dense0_weight (shape=(256, 20), dtype=float32) Parameter dense0_bias (shape=(256,), dtype=float32) Parameter dense1_weight (shape=(10, 256), dtype=float32) Parameter dense1_bias (shape=(10,), dtype=float32)
)
As soon as we know the input dimensionality, 20, the framework can identify the shape of the first layerʼs weight matrix by plugging in the value of 20. Having recognized the first layerʼs shape, the framework proceeds to the second layer, and so on through the computational graph until all shapes are known. Note that in this case, only the first layer requires deferred initialization, but the framework initializes sequentially. Once all parameter shapes are known, the framework can finally initialize the parameters.
Summary
• Deferred initialization can be convenient, allowing the framework to infer parameter shapes automatically, making it easy to modify architectures and eliminating one common source of errors.
• We can pass data through the model to make the framework finally initialize parameters.
Exercises
1. What happens if you specify the input dimensions to the first layer but not to subsequent layers? Do you get immediate initialization?
2. What happens if you specify mismatching dimensions? 3. What would you need to do if you have input of varying dimensionality? Hint: look at the
parameter tying. Discussions81
81 https://discuss.d2l.ai/t/280

5.3. Deferred Initialization

213

5.4 Custom Layers
One factor behind deep learningʼs success is the availability of a wide range of layers that can be composed in creative ways to design architectures suitable for a wide variety of tasks. For instance, researchers have invented layers specifically for handling images, text, looping over sequential data, and performing dynamic programming. Sooner or later, you will encounter or invent a layer that does not exist yet in the deep learning framework. In these cases, you must build a custom layer. In this section, we show you how.
5.4.1 Layers without Parameters
To start, we construct a custom layer that does not have any parameters of its own. This should look familiar if you recall our introduction to block in Section 5.1. The following CenteredLayer class simply subtracts the mean from its input. To build it, we simply need to inherit from the base layer class and implement the forward propagation function.
from mxnet import np, npx from mxnet.gluon import nn
npx.set_np()
class CenteredLayer(nn.Block): def __init__(self, **kwargs): super().__init__(**kwargs)
def forward(self, X): return X - X.mean()
Let us verify that our layer works as intended by feeding some data through it.
layer = CenteredLayer() layer(np.array([1, 2, 3, 4, 5]))
array([-2., -1., 0., 1., 2.])
We can now incorporate our layer as a component in constructing more complex models.
net = nn.Sequential() net.add(nn.Dense(128), CenteredLayer()) net.initialize()
As an extra sanity check, we can send random data through the network and check that the mean is in fact 0. Because we are dealing with floating point numbers, we may still see a very small nonzero number due to quantization.
Y = net(np.random.uniform(size=(4, 8))) Y.mean()
array(3.783498e-10)

214

Chapter 5. Deep Learning Computation

5.4.2 Layers with Parameters
Now that we know how to define simple layers, let us move on to defining layers with parameters that can be adjusted through training. We can use built-in functions to create parameters, which provide some basic housekeeping functionality. In particular, they govern access, initialization, sharing, saving, and loading model parameters. This way, among other benefits, we will not need to write custom serialization routines for every custom layer.
Now let us implement our own version of the fully-connected layer. Recall that this layer requires two parameters, one to represent the weight and the other for the bias. In this implementation, we bake in the ReLU activation as a default. This layer requires to input arguments: in_units and units, which denote the number of inputs and outputs, respectively.
class MyDense(nn.Block): def __init__(self, units, in_units, **kwargs): super().__init__(**kwargs) self.weight = self.params.get('weight', shape=(in_units, units)) self.bias = self.params.get('bias', shape=(units,))
def forward(self, x): linear = np.dot( x, self.weight.data(ctx=x.ctx)) + self.bias.data(ctx=x.ctx) return npx.relu(linear)
Next, we instantiate the MyDense class and access its model parameters.
dense = MyDense(units=3, in_units=5) dense.params

mydense0_ ( Parameter mydense0_weight (shape=(5, 3), dtype=<class 'numpy.float32'>) Parameter mydense0_bias (shape=(3,), dtype=<class 'numpy.float32'>)
)
We can directly carry out forward propagation calculations using custom layers.
dense.initialize() dense(np.random.uniform(size=(2, 5)))

array([[0.

, 0.01633355, 0.

],

[0.

, 0.01581812, 0.

]])

We can also construct models using custom layers. Once we have that we can use it just like the built-in fully-connected layer.
net = nn.Sequential() net.add(MyDense(8, in_units=64), MyDense(1, in_units=8)) net.initialize() net(np.random.uniform(size=(2, 64)))

array([[0.06508517], [0.0615553 ]])

5.4. Custom Layers

215

Summary
• We can design custom layers via the basic layer class. This allows us to define flexible new layers that behave differently from any existing layers in the library.
• Once defined, custom layers can be invoked in arbitrary contexts and architectures. • Layers can have local parameters, which can be created through built-in functions.
Exercises
1. D∑esign a layer that takes an input and computes a tensor reduction, i.e., it returns yk = i,j Wijkxixj .
2. Design a layer that returns the leading half of the Fourier coefficients of the data. Discussions82
5.5 File I/O
So far we discussed how to process data and how to build, train, and test deep learning models. However, at some point, we will hopefully be happy enough with the learned models that we will want to save the results for later use in various contexts (perhaps even to make predictions in deployment). Additionally, when running a long training process, the best practice is to periodically save intermediate results (checkpointing) to ensure that we do not lose several days worth of computation if we trip over the power cord of our server. Thus it is time to learn how to load and store both individual weight vectors and entire models. This section addresses both issues.
5.5.1 Loading and Saving Tensors
For individual tensors, we can directly invoke the load and save functions to read and write them respectively. Both functions require that we supply a name, and save requires as input the variable to be saved. from mxnet import np, npx from mxnet.gluon import nn
npx.set_np()
x = np.arange(4) npx.save('x-file', x)
We can now read the data from the stored file back into memory. x2 = npx.load('x-file') x2
[array([0., 1., 2., 3.])]
82 https://discuss.d2l.ai/t/58

216

Chapter 5. Deep Learning Computation

We can store a list of tensors and read them back into memory.
y = np.zeros(4) npx.save('x-files', [x, y]) x2, y2 = npx.load('x-files') (x2, y2)
(array([0., 1., 2., 3.]), array([0., 0., 0., 0.]))
We can even write and read a dictionary that maps from strings to tensors. This is convenient when we want to read or write all the weights in a model.
mydict = {'x': x, 'y': y} npx.save('mydict', mydict) mydict2 = npx.load('mydict') mydict2
{'x': array([0., 1., 2., 3.]), 'y': array([0., 0., 0., 0.])}

5.5.2 Loading and Saving Model Parameters
Saving individual weight vectors (or other tensors) is useful, but it gets very tedious if we want to save (and later load) an entire model. After all, we might have hundreds of parameter groups sprinkled throughout. For this reason the deep learning framework provides built-in functionalities to load and save entire networks. An important detail to note is that this saves model parameters and not the entire model. For example, if we have a 3-layer MLP, we need to specify the architecture separately. The reason for this is that the models themselves can contain arbitrary code, hence they cannot be serialized as naturally. Thus, in order to reinstate a model, we need to generate the architecture in code and then load the parameters from disk. Let us start with our familiar MLP.
class MLP(nn.Block): def __init__(self, **kwargs): super(MLP, self).__init__(**kwargs) self.hidden = nn.Dense(256, activation='relu') self.output = nn.Dense(10)
def forward(self, x): return self.output(self.hidden(x))
net = MLP() net.initialize() X = np.random.uniform(size=(2, 20)) Y = net(X)
Next, we store the parameters of the model as a file with the name “mlp.params”.
net.save_parameters('mlp.params')
To recover the model, we instantiate a clone of the original MLP model. Instead of randomly initializing the model parameters, we read the parameters stored in the file directly.

5.5. File I/O

217

clone = MLP() clone.load_parameters('mlp.params')
Since both instances have the same model parameters, the computational result of the same input X should be the same. Let us verify this.
Y_clone = clone(X) Y_clone == Y

array([[ True, True, True],
[ True, True, True]])

True, True,

True, True,

True, True,

True, True,

True, True,

True, True,

True, True,

Summary
• The save and load functions can be used to perform file I/O for tensor objects. • We can save and load the entire sets of parameters for a network via a parameter dictionary. • Saving the architecture has to be done in code rather than in parameters.
Exercises
1. Even if there is no need to deploy trained models to a different device, what are the practical benefits of storing model parameters?
2. Assume that we want to reuse only parts of a network to be incorporated into a network of a different architecture. How would you go about using, say the first two layers from a previous network in a new network?
3. How would you go about saving the network architecture and parameters? What restrictions would you impose on the architecture?
Discussions83

5.6 GPUs
In Table 1.5.1, we discussed the rapid growth of computation over the past two decades. In a nutshell, GPU performance has increased by a factor of 1000 every decade since 2000. This offers great opportunities but it also suggests a significant need to provide such performance.
In this section, we begin to discuss how to harness this computational performance for your research. First by using single GPUs and at a later point, how to use multiple GPUs and multiple servers (with multiple GPUs).
Specifically, we will discuss how to use a single NVIDIA GPU for calculations. First, make sure you have at least one NVIDIA GPU installed. Then, download the NVIDIA driver and CUDA84 and
83 https://discuss.d2l.ai/t/60 84 https://developer.nvidia.com/cuda-downloads

218

Chapter 5. Deep Learning Computation

follow the prompts to set the appropriate path. Once these preparations are complete, the nvidiasmi command can be used to view the graphics card information.
!nvidia-smi

Tue Jun 22 22:43:20 2021

+-----------------------------------------------------------------------------+

| NVIDIA-SMI 418.67

Driver Version: 418.67

CUDA Version: 10.1 |

|-------------------------------+----------------------+----------------------+

| GPU Name

Persistence-M| Bus-Id

Disp.A | Volatile Uncorr. ECC |

| Fan Temp Perf Pwr:Usage/Cap|

Memory-Usage | GPU-Util Compute M. |

|===============================+======================+======================|

| 0 Tesla V100-SXM2... Off | 00000000:00:1B.0 Off |

0|

| N/A 50C P0 53W / 300W | 10375MiB / 16130MiB |

0%

Default |

+-------------------------------+----------------------+----------------------+

| 1 Tesla V100-SXM2... Off | 00000000:00:1C.0 Off |

0|

| N/A 50C P0 55W / 300W | 10645MiB / 16130MiB |

0%

Default |

+-------------------------------+----------------------+----------------------+

| 2 Tesla V100-SXM2... Off | 00000000:00:1D.0 Off |

0|

| N/A 52C P0 57W / 300W | 4863MiB / 16130MiB |

0%

Default |

+-------------------------------+----------------------+----------------------+

| 3 Tesla V100-SXM2... Off | 00000000:00:1E.0 Off |

0|

| N/A 47C P0 57W / 300W | 6434MiB / 16130MiB |

0%

Default |

+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+

| Processes:

GPU Memory |

| GPU

PID Type Process name

Usage

|

|=============================================================================|

| 0 47319

C ...buntu/miniconda3/envs/d2l-en/bin/python 2479MiB |

| 0 59570

C ...iconda3/envs/d2l-en-master-0/bin/python 1687MiB |

| 0 78495

C ...buntu/miniconda3/envs/d2l-en/bin/python 2763MiB |

| 1 47319

C ...buntu/miniconda3/envs/d2l-en/bin/python 2127MiB |

| 1 59570

C ...iconda3/envs/d2l-en-master-0/bin/python 1685MiB |

| 1 78495

C ...buntu/miniconda3/envs/d2l-en/bin/python 2581MiB |

| 2 47319

C ...buntu/miniconda3/envs/d2l-en/bin/python 2199MiB |

| 2 78495

C ...buntu/miniconda3/envs/d2l-en/bin/python 2653MiB |

| 3 47319

C ...buntu/miniconda3/envs/d2l-en/bin/python 2199MiB |

| 3 78495

C ...buntu/miniconda3/envs/d2l-en/bin/python 2653MiB |

+-----------------------------------------------------------------------------+

You might have noticed that a MXNet tensor looks almost identical to a NumPy ndarray. But there are a few crucial differences. One of the key features that distinguishes MXNet from NumPy is its support for diverse hardware devices.
In MXNet, every array has a context. So far, by default, all variables and associated computation have been assigned to the CPU. Typically, other contexts might be various GPUs. Things can get even hairier when we deploy jobs across multiple servers. By assigning arrays to contexts intelligently, we can minimize the time spent transferring data between devices. For example, when training neural networks on a server with a GPU, we typically prefer for the modelʼs parameters to live on the GPU.
Next, we need to confirm that the GPU version of MXNet is installed. If a CPU version of MXNet is already installed, we need to uninstall it first. For example, use the pip uninstall mxnet command, then install the corresponding MXNet version according to your CUDA version. Assuming you have CUDA 10.0 installed, you can install the MXNet version that supports CUDA 10.0 via pip

5.6. GPUs

219

install mxnet-cu100. To run the programs in this section, you need at least two GPUs. Note that this might be extravagant for most desktop computers but it is easily available in the cloud, e.g., by using the AWS EC2 multiGPU instances. Almost all other sections do not require multiple GPUs. Instead, this is simply to illustrate how data flow between different devices.
5.6.1 Computing Devices
We can specify devices, such as CPUs and GPUs, for storage and calculation. By default, tensors are created in the main memory and then use the CPU to calculate it. In MXNet, the CPU and GPU can be indicated by cpu() and gpu(). It should be noted that cpu() (or any integer in the parentheses) means all physical CPUs and memory. This means that MXNetʼs calculations will try to use all CPU cores. However, gpu() only represents one card and the corresponding memory. If there are multiple GPUs, we use gpu(i) to represent the ith GPU (i starts from 0). Also, gpu(0) and gpu() are equivalent.
from mxnet import np, npx from mxnet.gluon import nn
npx.set_np()
npx.cpu(), npx.gpu(), npx.gpu(1)
(cpu(0), gpu(0), gpu(1))
We can query the number of available GPUs.
npx.num_gpus()
2
Now we define two convenient functions that allow us to run code even if the requested GPUs do not exist.
def try_gpu(i=0): #@save """Return gpu(i) if exists, otherwise return cpu().""" return npx.gpu(i) if npx.num_gpus() >= i + 1 else npx.cpu()
def try_all_gpus(): #@save """Return all available GPUs, or [cpu()] if no GPU exists.""" devices = [npx.gpu(i) for i in range(npx.num_gpus())] return devices if devices else [npx.cpu()]
try_gpu(), try_gpu(10), try_all_gpus()
(gpu(0), cpu(0), [gpu(0), gpu(1)])

220

Chapter 5. Deep Learning Computation

5.6.2 Tensors and GPUs
By default, tensors are created on the CPU. We can query the device where the tensor is located.
x = np.array([1, 2, 3]) x.ctx
cpu(0)
It is important to note that whenever we want to operate on multiple terms, they need to be on the same device. For instance, if we sum two tensors, we need to make sure that both arguments live on the same device—otherwise the framework would not know where to store the result or even how to decide where to perform the computation.
Storage on the GPU
There are several ways to store a tensor on the GPU. For example, we can specify a storage device when creating a tensor. Next, we create the tensor variable X on the first gpu. The tensor created on a GPU only consumes the memory of this GPU. We can use the nvidia-smi command to view GPU memory usage. In general, we need to make sure that we do not create data that exceed the GPU memory limit.
X = np.ones((2, 3), ctx=try_gpu()) X
array([[1., 1., 1.], [1., 1., 1.]], ctx=gpu(0))
Assuming that you have at least two GPUs, the following code will create a random tensor on the second GPU.
Y = np.random.uniform(size=(2, 3), ctx=try_gpu(1)) Y
array([[0.67478997, 0.07540122, 0.9956977 ], [0.09488854, 0.415456 , 0.11231736]], ctx=gpu(1))
Copying
If we want to compute X + Y, we need to decide where to perform this operation. For instance, as shown in Fig. 5.6.1, we can transfer X to the second GPU and perform the operation there. Do not simply add X and Y, since this will result in an exception. The runtime engine would not know what to do: it cannot find data on the same device and it fails. Since Y lives on the second GPU, we need to move X there before we can add the two.

5.6. GPUs

221

Fig. 5.6.1: Copy data to perform an operation on the same device.
Z = X.copyto(try_gpu(1)) print(X) print(Z)
[[1. 1. 1.] [1. 1. 1.]] @gpu(0) [[1. 1. 1.] [1. 1. 1.]] @gpu(1)
Now that the data are on the same GPU (both Z and Y are), we can add them up.
Y+Z
array([[1.6747899, 1.0754012, 1.9956977], [1.0948886, 1.415456 , 1.1123173]], ctx=gpu(1))
Imagine that your variable Z already lives on your second GPU. What happens if we still call Z. copyto(gpu(1))? It will make a copy and allocate new memory, even though that variable already lives on the desired device. There are times where, depending on the environment our code is running in, two variables may already live on the same device. So we want to make a copy only if the variables currently live in different devices. In these cases, we can call as_in_ctx. If the variable already live in the specified device then this is a no-op. Unless you specifically want to make a copy, as_in_ctx is the method of choice.
Z.as_in_ctx(try_gpu(1)) is Z
True

Side Notes
People use GPUs to do machine learning because they expect them to be fast. But transferring variables between devices is slow. So we want you to be 100% certain that you want to do something slow before we let you do it. If the deep learning framework just did the copy automatically without crashing then you might not realize that you had written some slow code.
Also, transferring data between devices (CPU, GPUs, and other machines) is something that is much slower than computation. It also makes parallelization a lot more difficult, since we have to wait for data to be sent (or rather to be received) before we can proceed with more operations. This

222

Chapter 5. Deep Learning Computation

is why copy operations should be taken with great care. As a rule of thumb, many small operations are much worse than one big operation. Moreover, several operations at a time are much better than many single operations interspersed in the code unless you know what you are doing. This is the case since such operations can block if one device has to wait for the other before it can do something else. It is a bit like ordering your coffee in a queue rather than pre-ordering it by phone and finding out that it is ready when you are.
Last, when we print tensors or convert tensors to the NumPy format, if the data is not in the main memory, the framework will copy it to the main memory first, resulting in additional transmission overhead. Even worse, it is now subject to the dreaded global interpreter lock that makes everything wait for Python to complete.

5.6.3 Neural Networks and GPUs
Similarly, a neural network model can specify devices. The following code puts the model parameters on the GPU.
net = nn.Sequential() net.add(nn.Dense(1)) net.initialize(ctx=try_gpu())
We will see many more examples of how to run models on GPUs in the following chapters, simply since they will become somewhat more computationally intensive. When the input is a tensor on the GPU, the model will calculate the result on the same GPU.
net(X)

array([[0.04995865], [0.04995865]], ctx=gpu(0))
Let us confirm that the model parameters are stored on the same GPU. net[0].weight.data().ctx

gpu(0)
In short, as long as all data and parameters are on the same device, we can learn models efficiently. In the following chapters we will see several such examples.

Summary
• We can specify devices for storage and calculation, such as the CPU or GPU. By default, data are created in the main memory and then use the CPU for calculations.
• The deep learning framework requires all input data for calculation to be on the same device, be it CPU or the same GPU.
• You can lose significant performance by moving data without care. A typical mistake is as follows: computing the loss for every minibatch on the GPU and reporting it back to the user on the command line (or logging it in a NumPy ndarray) will trigger a global interpreter lock

5.6. GPUs

223

which stalls all GPUs. It is much better to allocate memory for logging inside the GPU and only move larger logs.
Exercises
1. Try a larger computation task, such as the multiplication of large matrices, and see the difference in speed between the CPU and GPU. What about a task with a small amount of calculations?
2. How should we read and write model parameters on the GPU? 3. Measure the time it takes to compute 1000 matrix-matrix multiplications of 100 × 100 matri-
ces and log the Frobenius norm of the output matrix one result at a time vs. keeping a log on the GPU and transferring only the final result. 4. Measure how much time it takes to perform two matrix-matrix multiplications on two GPUs at the same time vs. in sequence on one GPU. Hint: you should see almost linear scaling. Discussions85

85 https://discuss.d2l.ai/t/62
224

Chapter 5. Deep Learning Computation

6 | Convolutional Neural Networks
In earlier chapters, we came up against image data, for which each example consists of a twodimensional grid of pixels. Depending on whether we are handling black-and-white or color images, each pixel location might be associated with either one or multiple numerical values, respectively. Until now, our way of dealing with this rich structure was deeply unsatisfying. We simply discarded each imageʼs spatial structure by flattening them into one-dimensional vectors, feeding them through a fully-connected MLP. Because these networks are invariant to the order of the features, we could get similar results regardless of whether we preserve an order corresponding to the spatial structure of the pixels or if we permute the columns of our design matrix before fitting the MLPʼs parameters. Preferably, we would leverage our prior knowledge that nearby pixels are typically related to each other, to build efficient models for learning from image data.
This chapter introduces convolutional neural networks (CNNs), a powerful family of neural networks that are designed for precisely this purpose. CNN-based architectures are now ubiquitous in the field of computer vision, and have become so dominant that hardly anyone today would develop a commercial application or enter a competition related to image recognition, object detection, or semantic segmentation, without building off of this approach.
Modern CNNs, as they are called colloquially owe their design to inspirations from biology, group theory, and a healthy dose of experimental tinkering. In addition to their sample efficiency in achieving accurate models, CNNs tend to be computationally efficient, both because they require fewer parameters than fully-connected architectures and because convolutions are easy to parallelize across GPU cores. Consequently, practitioners often apply CNNs whenever possible, and increasingly they have emerged as credible competitors even on tasks with a one-dimensional sequence structure, such as audio, text, and time series analysis, where recurrent neural networks are conventionally used. Some clever adaptations of CNNs have also brought them to bear on graph-structured data and in recommender systems.
First, we will walk through the basic operations that comprise the backbone of all convolutional networks. These include the convolutional layers themselves, nitty-gritty details including padding and stride, the pooling layers used to aggregate information across adjacent spatial regions, the use of multiple channels at each layer, and a careful discussion of the structure of modern architectures. We will conclude the chapter with a full working example of LeNet, the first convolutional network successfully deployed, long before the rise of modern deep learning. In the next chapter, we will dive into full implementations of some popular and comparatively recent CNN architectures whose designs represent most of the techniques commonly used by modern practitioners.
225

6.1 From Fully-Connected Layers to Convolutions
To this day, the models that we have discussed so far remain appropriate options when we are dealing with tabular data. By tabular, we mean that the data consist of rows corresponding to examples and columns corresponding to features. With tabular data, we might anticipate that the patterns we seek could involve interactions among the features, but we do not assume any structure a priori concerning how the features interact.
Sometimes, we truly lack knowledge to guide the construction of craftier architectures. In these cases, an MLP may be the best that we can do. However, for high-dimensional perceptual data, such structure-less networks can grow unwieldy.
For instance, let us return to our running example of distinguishing cats from dogs. Say that we do a thorough job in data collection, collecting an annotated dataset of one-megapixel photographs. This means that each input to the network has one million dimensions. According to our discussions of parameterization cost of fully-connected layers in Section 3.4.3, even an aggressive reduction to one thousand hidden dimensions would require a fully-connected layer characterized by 106 × 103 = 109 parameters. Unless we have lots of GPUs, a talent for distributed optimization, and an extraordinary amount of patience, learning the parameters of this network may turn out to be infeasible.
A careful reader might object to this argument on the basis that one megapixel resolution may not be necessary. However, while we might be able to get away with one hundred thousand pixels, our hidden layer of size 1000 grossly underestimates the number of hidden units that it takes to learn good representations of images, so a practical system will still require billions of parameters. Moreover, learning a classifier by fitting so many parameters might require collecting an enormous dataset. And yet today both humans and computers are able to distinguish cats from dogs quite well, seemingly contradicting these intuitions. That is because images exhibit rich structure that can be exploited by humans and machine learning models alike. Convolutional neural networks (CNNs) are one creative way that machine learning has embraced for exploiting some of the known structure in natural images.
6.1.1 Invariance
Imagine that you want to detect an object in an image. It seems reasonable that whatever method we use to recognize objects should not be overly concerned with the precise location of the object in the image. Ideally, our system should exploit this knowledge. Pigs usually do not fly and planes usually do not swim. Nonetheless, we should still recognize a pig were one to appear at the top of the image. We can draw some inspiration here from the childrenʼs game “Whereʼs Waldo” (depicted in Fig. 6.1.1). The game consists of a number of chaotic scenes bursting with activities. Waldo shows up somewhere in each, typically lurking in some unlikely location. The readerʼs goal is to locate him. Despite his characteristic outfit, this can be surprisingly difficult, due to the large number of distractions. However, what Waldo looks like does not depend upon where Waldo is located. We could sweep the image with a Waldo detector that could assign a score to each patch, indicating the likelihood that the patch contains Waldo. CNNs systematize this idea of spatial invariance, exploiting it to learn useful representations with fewer parameters.

226

Chapter 6. Convolutional Neural Networks

Fig. 6.1.1: An image of the “Whereʼs Waldo” game.

We can now make these intuitions more concrete by enumerating a few desiderata to guide our design of a neural network architecture suitable for computer vision:
1. In the earliest layers, our network should respond similarly to the same patch, regardless of where it appears in the image. This principle is called translation invariance.
2. The earliest layers of the network should focus on local regions, without regard for the contents of the image in distant regions. This is the locality principle. Eventually, these local representations can be aggregated to make predictions at the whole image level.
Let us see how this translates into mathematics.

6.1.2 Constraining the MLP

To start off, we can consider an MLP with two-dimensional images X as inputs and their immediate hidden representations H similarly represented as matrices in mathematics and as twodimensional tensors in code, where both X and H have the same shape. Let that sink in. We now conceive of not only the inputs but also the hidden representations as possessing spatial structure.

Let [X]i,j and [H]i,j denote the pixel at location (i, j) in the input image and hidden representation,

respectively. Consequently, to have each of the hidden units receive input from each of the input

pixels, we would switch from using weight matrices (as we did previously in MLPs) to represent-

ing our parameters as fourth-order weight tensors W. Suppose that U contains biases, we could

formally express the fully-connected layer as

∑∑

[H]i,j = [U]i,j +

[W]i,j,k,l[X]k,l

∑k ∑l

,

= [U]i,j +

[V]i,j,a,b[X]i+a,j+b.

(6.1.1)

ab

where the switch from W to V is entirely cosmetic for now since there is a one-to-one correspondence between coefficients in both fourth-order tensors. We simply re-index the subscripts (k, l) such that k = i + a and l = j + b. In other words, we set [V]i,j,a,b = [W]i,j,i+a,j+b. The indices a and

6.1. From Fully-Connected Layers to Convolutions

227

b run over both positive and negative offsets, covering the entire image. For any given location (i, j) in the hidden representation [H]i,j, we compute its value by summing over pixels in x, centered around (i, j) and weighted by [V]i,j,a,b.

Translation Invariance

Now let us invoke the first principle established above: translation invariance. This implies that

a shift in the input X should simply lead to a shift in the hidden representation H. This is only possible if V and U do not actually depend on (i, j), i.e., we have [V]i,j,a,b = [V]a,b and U is a constant, say u. As a result, we can simplify the definition for H:

∑∑

[H]i,j = u +

[V]a,b[X]i+a,j+b.

ab

(6.1.2)

This is a convolution! We are effectively weighting pixels at (i + a, j + b) in the vicinity of location (i, j) with coefficients [V]a,b to obtain the value [H]i,j. Note that [V]a,b needs many fewer coefficients than [V]i,j,a,b since it no longer depends on the location within the image. We have made significant progress!

Locality

Now let us invoke the second principle: locality. As motivated above, we believe that we should not have to look very far away from location (i, j) in order to glean relevant information to assess what is going on at [H]i,j. This means that outside some range |a| > ∆ or |b| > ∆, we should set [V]a,b = 0. Equivalently, we can rewrite [H]i,j as

∑ ∆ ∑ ∆

[H]i,j = u +

[V]a,b[X]i+a,j+b.

a=−∆ b=−∆

(6.1.3)

Note that (6.1.3), in a nutshell, is a convolutional layer. Convolutional neural networks (CNNs) are a special family of neural networks that contain convolutional layers. In the deep learning research community, V is referred to as a convolution kernel, a filter, or simply the layerʼs weights that are often learnable parameters. When the local region is small, the difference as compared with a fully-connected network can be dramatic. While previously, we might have required billions of parameters to represent just a single layer in an image-processing network, we now typically need just a few hundred, without altering the dimensionality of either the inputs or the hidden representations. The price paid for this drastic reduction in parameters is that our features are now translation invariant and that our layer can only incorporate local information, when determining the value of each hidden activation. All learning depends on imposing inductive bias. When that bias agrees with reality, we get sample-efficient models that generalize well to unseen data. But of course, if those biases do not agree with reality, e.g., if images turned out not to be translation invariant, our models might struggle even to fit our training data.

228

Chapter 6. Convolutional Neural Networks

6.1.3 Convolutions

Before going further, we should briefly review why the above operation is called a convolution. In mathematics, the convolution between two functions, say f, g : Rd → R is defined as

∫

(f ∗ g)(x) = f (z)g(x − z)dz.

(6.1.4)

That is, we measure the overlap between f and g when one function is “flipped” and shifted by x.

Whenever we have discrete objects, the integral turns into a sum. For instance, for vectors from

the set of square summable infinite dimensional vectors with index running over Z we obtain the

following definition:

∑ (f ∗ g)(i) = f (a)g(i − a).
a

(6.1.5)

For two-dimensional tensors, we have a corresponding sum with indices (a, b) for f and (i−a, j−b)

for g, respectively:

∑∑

(f ∗ g)(i, j) =

f (a, b)g(i − a, j − b).

ab

(6.1.6)

This looks similar to (6.1.3), with one major difference. Rather than using (i+a, j +b), we are using the difference instead. Note, though, that this distinction is mostly cosmetic since we can always match the notation between (6.1.3) and (6.1.6). Our original definition in (6.1.3) more properly describes a cross-correlation. We will come back to this in the following section.

6.1.4 “Where’s Waldo” Revisited
Returning to our Waldo detector, let us see what this looks like. The convolutional layer picks windows of a given size and weighs intensities according to the filter V, as demonstrated in Fig. 6.1.2. We might aim to learn a model so that wherever the “waldoness” is highest, we should find a peak in the hidden layer representations.

Fig. 6.1.2: Detect Waldo.

6.1. From Fully-Connected Layers to Convolutions

229

Channels

There is just one problem with this approach. So far, we blissfully ignored that images consist of 3 channels: red, green, and blue. In reality, images are not two-dimensional objects but rather third-order tensors, characterized by a height, width, and channel, e.g., with shape 1024×1024×3 pixels. While the first two of these axes concern spatial relationships, the third can be regarded as assigning a multidimensional representation to each pixel location. We thus index X as [X]i,j,k. The convolutional filter has to adapt accordingly. Instead of [V]a,b, we now have [V]a,b,c.
Moreover, just as our input consists of a third-order tensor, it turns out to be a good idea to similarly formulate our hidden representations as third-order tensors H. In other words, rather than just having a single hidden representation corresponding to each spatial location, we want an entire vector of hidden representations corresponding to each spatial location. We could think of the hidden representations as comprising a number of two-dimensional grids stacked on top of each other. As in the inputs, these are sometimes called channels. They are also sometimes called feature maps, as each provides a spatialized set of learned features to the subsequent layer. Intuitively, you might imagine that at lower layers that are closer to inputs, some channels could become specialized to recognize edges while others could recognize textures.
To support multiple channels in both inputs (X) and hidden representations (H), we can add a fourth coordinate to V: [V]a,b,c,d. Putting everything together we have:

∑ ∆ ∑ ∆ ∑

[H]i,j,d =

[V]a,b,c,d[X]i+a,j+b,c,

a=−∆ b=−∆ c

(6.1.7)

where d indexes the output channels in the hidden representations H. The subsequent convolutional layer will go on to take a third-order tensor, H, as the input. Being more general, (6.1.7) is the definition of a convolutional layer for multiple channels, where V is a kernel or filter of the layer.
There are still many operations that we need to address. For instance, we need to figure out how to combine all the hidden representations to a single output, e.g., whether there is a Waldo anywhere in the image. We also need to decide how to compute things efficiently, how to combine multiple layers, appropriate activation functions, and how to make reasonable design choices to yield networks that are effective in practice. We turn to these issues in the remainder of the chapter.

Summary
• Translation invariance in images implies that all patches of an image will be treated in the same manner.
• Locality means that only a small neighborhood of pixels will be used to compute the corresponding hidden representations.
• In image processing, convolutional layers typically require many fewer parameters than fully-connected layers.
• CNNS are a special family of neural networks that contain convolutional layers.
• Channels on input and output allow our model to capture multiple aspects of an image at each spatial location.

230

Chapter 6. Convolutional Neural Networks

Exercises
1. Assume that the size of the convolution kernel is ∆ = 0. Show that in this case the convolution kernel implements an MLP independently for each set of channels.
2. Why might translation invariance not be a good idea after all? 3. What problems must we deal with when deciding how to treat hidden representations cor-
responding to pixel locations at the boundary of an image? 4. Describe an analogous convolutional layer for audio. 5. Do you think that convolutional layers might also be applicable for text data? Why or why
not? 6. Prove that f ∗ g = g ∗ f . Discussions86
6.2 Convolutions for Images
Now that we understand how convolutional layers work in theory, we are ready to see how they work in practice. Building on our motivation of convolutional neural networks as efficient architectures for exploring structure in image data, we stick with images as our running example.
6.2.1 The Cross-Correlation Operation
Recall that strictly speaking, convolutional layers are a misnomer, since the operations they express are more accurately described as cross-correlations. Based on our descriptions of convolutional layers in Section 6.1, in such a layer, an input tensor and a kernel tensor are combined to produce an output tensor through a cross-correlation operation. Let us ignore channels for now and see how this works with two-dimensional data and hidden representations. In Fig. 6.2.1, the input is a two-dimensional tensor with a height of 3 and width of 3. We mark the shape of the tensor as 3 × 3 or (3, 3). The height and width of the kernel are both 2. The shape of the kernel window (or convolution window) is given by the height and width of the kernel (here it is 2 × 2).

Fig. 6.2.1: Two-dimensional cross-correlation operation. The shaded portions are the first output element as well as the input and kernel tensor elements used for the output computation: 0 × 0 + 1 × 1 + 3 × 2 + 4 × 3 = 19.

In the two-dimensional cross-correlation operation, we begin with the convolution window positioned at the upper-left corner of the input tensor and slide it across the input tensor, both from
86 https://discuss.d2l.ai/t/64

6.2. Convolutions for Images

231

left to right and top to bottom. When the convolution window slides to a certain position, the input subtensor contained in that window and the kernel tensor are multiplied elementwise and the resulting tensor is summed up yielding a single scalar value. This result gives the value of the output tensor at the corresponding location. Here, the output tensor has a height of 2 and width of 2 and the four elements are derived from the two-dimensional cross-correlation operation:

0 × 0 + 1 × 1 + 3 × 2 + 4 × 3 = 19, 1 × 0 + 2 × 1 + 4 × 2 + 5 × 3 = 25, 3 × 0 + 4 × 1 + 6 × 2 + 7 × 3 = 37, 4 × 0 + 5 × 1 + 7 × 2 + 8 × 3 = 43.

(6.2.1)

Note that along each axis, the output size is slightly smaller than the input size. Because the kernel has width and height greater than one, we can only properly compute the cross-correlation for locations where the kernel fits wholly within the image, the output size is given by the input size nh × nw minus the size of the convolution kernel kh × kw via

(nh − kh + 1) × (nw − kw + 1).

(6.2.2)

This is the case since we need enough space to “shift” the convolution kernel across the image. Later we will see how to keep the size unchanged by padding the image with zeros around its boundary so that there is enough space to shift the kernel. Next, we implement this process in the corr2d function, which accepts an input tensor X and a kernel tensor K and returns an output tensor Y.

from mxnet import autograd, np, npx from mxnet.gluon import nn from d2l import mxnet as d2l

npx.set_np()

def corr2d(X, K): #@save """Compute 2D cross-correlation.""" h, w = K.shape Y = np.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1)) for i in range(Y.shape[0]): for j in range(Y.shape[1]): Y[i, j] = (X[i:i + h, j:j + w] * K).sum() return Y
We can construct the input tensor X and the kernel tensor K from Fig. 6.2.1 to validate the output of the above implementation of the two-dimensional cross-correlation operation.
X = np.array([[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]]) K = np.array([[0.0, 1.0], [2.0, 3.0]]) corr2d(X, K)

array([[19., 25.], [37., 43.]])

232

Chapter 6. Convolutional Neural Networks

6.2.2 Convolutional Layers
A convolutional layer cross-correlates the input and kernel and adds a scalar bias to produce an output. The two parameters of a convolutional layer are the kernel and the scalar bias. When training models based on convolutional layers, we typically initialize the kernels randomly, just as we would with a fully-connected layer. We are now ready to implement a two-dimensional convolutional layer based on the corr2d function defined above. In the __init__ constructor function, we declare weight and bias as the two model parameters. The forward propagation function calls the corr2d function and adds the bias.
class Conv2D(nn.Block): def __init__(self, kernel_size, **kwargs): super().__init__(**kwargs) self.weight = self.params.get('weight', shape=kernel_size) self.bias = self.params.get('bias', shape=(1,))
def forward(self, x): return corr2d(x, self.weight.data()) + self.bias.data()
In h × w convolution or a h × w convolution kernel, the height and width of the convolution kernel are h and w, respectively. We also refer to a convolutional layer with a h × w convolution kernel simply as a h × w convolutional layer.
6.2.3 Object Edge Detection in Images
Let us take a moment to parse a simple application of a convolutional layer: detecting the edge of an object in an image by finding the location of the pixel change. First, we construct an “image” of 6 × 8 pixels. The middle four columns are black (0) and the rest are white (1).
X = np.ones((6, 8)) X[:, 2:6] = 0 X
array([[1., 1., 0., 0., 0., 0., 1., 1.], [1., 1., 0., 0., 0., 0., 1., 1.], [1., 1., 0., 0., 0., 0., 1., 1.], [1., 1., 0., 0., 0., 0., 1., 1.], [1., 1., 0., 0., 0., 0., 1., 1.], [1., 1., 0., 0., 0., 0., 1., 1.]])
Next, we construct a kernel K with a height of 1 and a width of 2. When we perform the crosscorrelation operation with the input, if the horizontally adjacent elements are the same, the output is 0. Otherwise, the output is non-zero.
K = np.array([[1.0, -1.0]])
We are ready to perform the cross-correlation operation with arguments X (our input) and K (our kernel). As you can see, we detect 1 for the edge from white to black and -1 for the edge from black to white. All other outputs take value 0.

6.2. Convolutions for Images

233

Y = corr2d(X, K) Y
array([[ 0., 1., 0., 0., 0., -1., 0.], [ 0., 1., 0., 0., 0., -1., 0.], [ 0., 1., 0., 0., 0., -1., 0.], [ 0., 1., 0., 0., 0., -1., 0.], [ 0., 1., 0., 0., 0., -1., 0.], [ 0., 1., 0., 0., 0., -1., 0.]])
We can now apply the kernel to the transposed image. As expected, it vanishes. The kernel K only detects vertical edges.
corr2d(X.T, K)
array([[0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.]])

6.2.4 Learning a Kernel

Designing an edge detector by finite differences [1, -1] is neat if we know this is precisely what we are looking for. However, as we look at larger kernels, and consider successive layers of convolutions, it might be impossible to specify precisely what each filter should be doing manually.
Now let us see whether we can learn the kernel that generated Y from X by looking at the input– output pairs only. We first construct a convolutional layer and initialize its kernel as a random tensor. Next, in each iteration, we will use the squared error to compare Y with the output of the convolutional layer. We can then calculate the gradient to update the kernel. For the sake of simplicity, in the following we use the built-in class for two-dimensional convolutional layers and ignore the bias.
# Construct a two-dimensional convolutional layer with 1 output channel and a # kernel of shape (1, 2). For the sake of simplicity, we ignore the bias here conv2d = nn.Conv2D(1, kernel_size=(1, 2), use_bias=False) conv2d.initialize()

# The two-dimensional convolutional layer uses four-dimensional input and # output in the format of (example, channel, height, width), where the batch # size (number of examples in the batch) and the number of channels are both 1 X = X.reshape(1, 1, 6, 8) Y = Y.reshape(1, 1, 6, 7) lr = 3e-2 # Learning rate

for i in range(10): with autograd.record():

(continues on next page)

234

Chapter 6. Convolutional Neural Networks

Y_hat = conv2d(X) l = (Y_hat - Y)**2 l.backward() # Update the kernel conv2d.weight.data()[:] -= if (i + 1) % 2 == 0: print(f'batch {i + 1},

lr * loss

conv2d.weight.grad() {float(l.sum()):.3f}')

(continued from previous page)

batch 2, loss 4.949 batch 4, loss 0.831 batch 6, loss 0.140 batch 8, loss 0.024 batch 10, loss 0.004
Note that the error has dropped to a small value after 10 iterations. Now we will take a look at the kernel tensor we learned.
conv2d.weight.data().reshape((1, 2))

array([[ 0.9895 , -0.9873705]]) Indeed, the learned kernel tensor is remarkably close to the kernel tensor K we defined earlier.

6.2.5 Cross-Correlation and Convolution
Recall our observation from Section 6.1 of the correspondence between the cross-correlation and convolution operations. Here let us continue to consider two-dimensional convolutional layers. What if such layers perform strict convolution operations as defined in (6.1.6) instead of crosscorrelations? In order to obtain the output of the strict convolution operation, we only need to flip the two-dimensional kernel tensor both horizontally and vertically, and then perform the crosscorrelation operation with the input tensor.
It is noteworthy that since kernels are learned from data in deep learning, the outputs of convolutional layers remain unaffected no matter such layers perform either the strict convolution operations or the cross-correlation operations.
To illustrate this, suppose that a convolutional layer performs cross-correlation and learns the kernel in Fig. 6.2.1, which is denoted as the matrix K here. Assuming that other conditions remain unchanged, when this layer performs strict convolution instead, the learned kernel K′ will be the same as K after K′ is flipped both horizontally and vertically. That is to say, when the convolutional layer performs strict convolution for the input in Fig. 6.2.1 and K′, the same output in Fig. 6.2.1 (cross-correlation of the input and K) will be obtained.
In keeping with standard terminology with deep learning literature, we will continue to refer to the cross-correlation operation as a convolution even though, strictly-speaking, it is slightly different. Besides, we use the term element to refer to an entry (or component) of any tensor representing a layer representation or a convolution kernel.

6.2. Convolutions for Images

235

6.2.6 Feature Map and Receptive Field
As described in Section 6.1.4, the convolutional layer output in Fig. 6.2.1 is sometimes called a feature map, as it can be regarded as the learned representations (features) in the spatial dimensions (e.g., width and height) to the subsequent layer. In CNNs, for any element x of some layer, its receptive field refers to all the elements (from all the previous layers) that may affect the calculation of x during the forward propagation. Note that the receptive field may be larger than the actual size of the input.
Let us continue to use Fig. 6.2.1 to explain the receptive field. Given the 2 × 2 convolution kernel, the receptive field of the shaded output element (of value 19) is the four elements in the shaded portion of the input. Now let us denote the 2 × 2 output as Y and consider a deeper CNN with an additional 2 × 2 convolutional layer that takes Y as its input, outputting a single element z. In this case, the receptive field of z on Y includes all the four elements of Y, while the receptive field on the input includes all the nine input elements. Thus, when any element in a feature map needs a larger receptive field to detect input features over a broader area, we can build a deeper network.
Summary
• The core computation of a two-dimensional convolutional layer is a two-dimensional crosscorrelation operation. In its simplest form, this performs a cross-correlation operation on the two-dimensional input data and the kernel, and then adds a bias.
• We can design a kernel to detect edges in images.
• We can learn the kernelʼs parameters from data.
• With kernels learned from data, the outputs of convolutional layers remain unaffected regardless of such layersʼ performed operations (either strict convolution or crosscorrelation).
• When any element in a feature map needs a larger receptive field to detect broader features on the input, a deeper network can be considered.
Exercises
1. Construct an image X with diagonal edges. 1. What happens if you apply the kernel K in this section to it? 2. What happens if you transpose X? 3. What happens if you transpose K?
2. When you try to automatically find the gradient for the Conv2D class we created, what kind of error message do you see?
3. How do you represent a cross-correlation operation as a matrix multiplication by changing the input and kernel tensors?
4. Design some kernels manually.
1. What is the form of a kernel for the second derivative?
2. What is the kernel for an integral?

236

Chapter 6. Convolutional Neural Networks

3. What is the minimum size of a kernel to obtain a derivative of degree d? Discussions87
6.3 Padding and Stride
In the previous example of Fig. 6.2.1, our input had both a height and width of 3 and our convolution kernel had both a height and width of 2, yielding an output representation with dimension 2 × 2. As we generalized in Section 6.2, assuming that the input shape is nh × nw and the convolution kernel shape is kh × kw, then the output shape will be (nh − kh + 1) × (nw − kw + 1). Therefore, the output shape of the convolutional layer is determined by the shape of the input and the shape of the convolution kernel.
In several cases, we incorporate techniques, including padding and strided convolutions, that affect the size of the output. As motivation, note that since kernels generally have width and height greater than 1, after applying many successive convolutions, we tend to wind up with outputs that are considerably smaller than our input. If we start with a 240 × 240 pixel image, 10 layers of 5 × 5 convolutions reduce the image to 200 × 200 pixels, slicing off 30% of the image and with it obliterating any interesting information on the boundaries of the original image. Padding is the most popular tool for handling this issue.
In other cases, we may want to reduce the dimensionality drastically, e.g., if we find the original input resolution to be unwieldy. Strided convolutions are a popular technique that can help in these instances.
6.3.1 Padding
As described above, one tricky issue when applying convolutional layers is that we tend to lose pixels on the perimeter of our image. Since we typically use small kernels, for any given convolution, we might only lose a few pixels, but this can add up as we apply many successive convolutional layers. One straightforward solution to this problem is to add extra pixels of filler around the boundary of our input image, thus increasing the effective size of the image. Typically, we set the values of the extra pixels to zero. In Fig. 6.3.1, we pad a 3 × 3 input, increasing its size to 5 × 5. The corresponding output then increases to a 4 × 4 matrix. The shaded portions are the first output element as well as the input and kernel tensor elements used for the output computation: 0 × 0 + 0 × 1 + 0 × 2 + 0 × 3 = 0.

Fig. 6.3.1: Two-dimensional cross-correlation with padding.
87 https://discuss.d2l.ai/t/65

6.3. Padding and Stride

237

In general, if we add a total of ph rows of padding (roughly half on top and half on bottom) and a total of pw columns of padding (roughly half on the left and half on the right), the output shape will be

(nh − kh + ph + 1) × (nw − kw + pw + 1).

(6.3.1)

This means that the height and width of the output will increase by ph and pw, respectively.
In many cases, we will want to set ph = kh − 1 and pw = kw − 1 to give the input and output the same height and width. This will make it easier to predict the output shape of each layer when constructing the network. Assuming that kh is odd here, we will pad ph/2 rows on both sides of the height. If kh is even, one possibility is to pad ⌈ph/2⌉ rows on the top of the input and ⌊ph/2⌋ rows on the bottom. We will pad both sides of the width in the same way.
CNNs commonly use convolution kernels with odd height and width values, such as 1, 3, 5, or 7. Choosing odd kernel sizes has the benefit that we can preserve the spatial dimensionality while padding with the same number of rows on top and bottom, and the same number of columns on left and right.
Moreover, this practice of using odd kernels and padding to precisely preserve dimensionality offers a clerical benefit. For any two-dimensional tensor X, when the kernelʼs size is odd and the number of padding rows and columns on all sides are the same, producing an output with the same height and width as the input, we know that the output Y[i, j] is calculated by cross-correlation of the input and convolution kernel with the window centered on X[i, j].
In the following example, we create a two-dimensional convolutional layer with a height and width of 3 and apply 1 pixel of padding on all sides. Given an input with a height and width of 8, we find that the height and width of the output is also 8.

from mxnet import np, npx from mxnet.gluon import nn

npx.set_np()

# For convenience, we define a function to calculate the convolutional layer. # This function initializes the convolutional layer weights and performs # corresponding dimensionality elevations and reductions on the input and # output def comp_conv2d(conv2d, X):
conv2d.initialize() # Here (1, 1) indicates that the batch size and the number of channels # are both 1 X = X.reshape((1, 1) + X.shape) Y = conv2d(X) # Exclude the first two dimensions that do not interest us: examples and # channels return Y.reshape(Y.shape[2:])

# Note that here 1 row or column is padded on either side, so a total of 2 # rows or columns are added conv2d = nn.Conv2D(1, kernel_size=3, padding=1) X = np.random.uniform(size=(8, 8)) comp_conv2d(conv2d, X).shape

238

Chapter 6. Convolutional Neural Networks

(8, 8)
When the height and width of the convolution kernel are different, we can make the output and input have the same height and width by setting different padding numbers for height and width.
# Here, we use a convolution kernel with a height of 5 and a width of 3. The # padding numbers on either side of the height and width are 2 and 1, # respectively conv2d = nn.Conv2D(1, kernel_size=(5, 3), padding=(2, 1)) comp_conv2d(conv2d, X).shape
(8, 8)
6.3.2 Stride
When computing the cross-correlation, we start with the convolution window at the upper-left corner of the input tensor, and then slide it over all locations both down and to the right. In previous examples, we default to sliding one element at a time. However, sometimes, either for computational efficiency or because we wish to downsample, we move our window more than one element at a time, skipping the intermediate locations.
We refer to the number of rows and columns traversed per slide as the stride. So far, we have used strides of 1, both for height and width. Sometimes, we may want to use a larger stride. Fig. 6.3.2 shows a two-dimensional cross-correlation operation with a stride of 3 vertically and 2 horizontally. The shaded portions are the output elements as well as the input and kernel tensor elements used for the output computation: 0 × 0 + 0 × 1 + 1 × 2 + 2 × 3 = 8, 0 × 0 + 6 × 1 + 0 × 2 + 0 × 3 = 6. We can see that when the second element of the first column is outputted, the convolution window slides down three rows. The convolution window slides two columns to the right when the second element of the first row is outputted. When the convolution window continues to slide two columns to the right on the input, there is no output because the input element cannot fill the window (unless we add another column of padding).

Fig. 6.3.2: Cross-correlation with strides of 3 and 2 for height and width, respectively.

In general, when the stride for the height is sh and the stride for the width is sw, the output shape is

⌊(nh − kh + ph + sh)/sh⌋ × ⌊(nw − kw + pw + sw)/sw⌋.

(6.3.2)

6.3. Padding and Stride

239

If we set ph = kh − 1 and pw = kw − 1, then the output shape will be simplified to ⌊(nh + sh − 1)/sh⌋ × ⌊(nw + sw − 1)/sw⌋. Going a step further, if the input height and width are divisible by the strides on the height and width, then the output shape will be (nh/sh) × (nw/sw). Below, we set the strides on both the height and width to 2, thus halving the input height and width.
conv2d = nn.Conv2D(1, kernel_size=3, padding=1, strides=2) comp_conv2d(conv2d, X).shape
(4, 4)
Next, we will look at a slightly more complicated example.
conv2d = nn.Conv2D(1, kernel_size=(3, 5), padding=(0, 1), strides=(3, 4)) comp_conv2d(conv2d, X).shape
(2, 2)
For the sake of brevity, when the padding number on both sides of the input height and width are ph and pw respectively, we call the padding (ph, pw). Specifically, when ph = pw = p, the padding is p. When the strides on the height and width are sh and sw, respectively, we call the stride (sh, sw). Specifically, when sh = sw = s, the stride is s. By default, the padding is 0 and the stride is 1. In practice, we rarely use inhomogeneous strides or padding, i.e., we usually have ph = pw and sh = sw.
Summary
• Padding can increase the height and width of the output. This is often used to give the output the same height and width as the input.
• The stride can reduce the resolution of the output, for example reducing the height and width of the output to only 1/n of the height and width of the input (n is an integer greater than 1).
• Padding and stride can be used to adjust the dimensionality of the data effectively.
Exercises
1. For the last example in this section, use mathematics to calculate the output shape to see if it is consistent with the experimental result.
2. Try other padding and stride combinations on the experiments in this section. 3. For audio signals, what does a stride of 2 correspond to? 4. What are the computational benefits of a stride larger than 1? Discussions88
88 https://discuss.d2l.ai/t/67

240

Chapter 6. Convolutional Neural Networks

6.4 Multiple Input and Multiple Output Channels
While we have described the multiple channels that comprise each image (e.g., color images have the standard RGB channels to indicate the amount of red, green and blue) and convolutional layers for multiple channels in Section 6.1.4, until now, we simplified all of our numerical examples by working with just a single input and a single output channel. This has allowed us to think of our inputs, convolution kernels, and outputs each as two-dimensional tensors.
When we add channels into the mix, our inputs and hidden representations both become threedimensional tensors. For example, each RGB input image has shape 3×h×w. We refer to this axis, with a size of 3, as the channel dimension. In this section, we will take a deeper look at convolution kernels with multiple input and multiple output channels.
6.4.1 Multiple Input Channels
When the input data contain multiple channels, we need to construct a convolution kernel with the same number of input channels as the input data, so that it can perform cross-correlation with the input data. Assuming that the number of channels for the input data is ci, the number of input channels of the convolution kernel also needs to be ci. If our convolution kernelʼs window shape is kh × kw, then when ci = 1, we can think of our convolution kernel as just a two-dimensional tensor of shape kh × kw.
However, when ci > 1, we need a kernel that contains a tensor of shape kh × kw for every input channel. Concatenating these ci tensors together yields a convolution kernel of shape ci × kh × kw. Since the input and convolution kernel each have ci channels, we can perform a cross-correlation operation on the two-dimensional tensor of the input and the two-dimensional tensor of the convolution kernel for each channel, adding the ci results together (summing over the channels) to yield a two-dimensional tensor. This is the result of a two-dimensional cross-correlation between a multi-channel input and a multi-input-channel convolution kernel.
In Fig. 6.4.1, we demonstrate an example of a two-dimensional cross-correlation with two input channels. The shaded portions are the first output element as well as the input and kernel tensor elements used for the output computation: (1×1+2×2+4×3+5×4)+(0×0+1×1+3×2+4×3) = 56.

Fig. 6.4.1: Cross-correlation computation with 2 input channels.

To make sure we really understand what is going on here, we can implement cross-correlation operations with multiple input channels ourselves. Notice that all we are doing is performing one cross-correlation operation per channel and then adding up the results.

6.4. Multiple Input and Multiple Output Channels

241

from mxnet import np, npx from d2l import mxnet as d2l
npx.set_np()
def corr2d_multi_in(X, K): # First, iterate through the 0th dimension (channel dimension) of `X` and # `K`. Then, add them together return sum(d2l.corr2d(x, k) for x, k in zip(X, K))
We can construct the input tensor X and the kernel tensor K corresponding to the values in Fig. 6.4.1 to validate the output of the cross-correlation operation.
X = np.array([[[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]], [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]])
K = np.array([[[0.0, 1.0], [2.0, 3.0]], [[1.0, 2.0], [3.0, 4.0]]])
corr2d_multi_in(X, K)
array([[ 56., 72.], [104., 120.]])
6.4.2 Multiple Output Channels
Regardless of the number of input channels, so far we always ended up with one output channel. However, as we discussed in Section 6.1.4, it turns out to be essential to have multiple channels at each layer. In the most popular neural network architectures, we actually increase the channel dimension as we go higher up in the neural network, typically downsampling to trade off spatial resolution for greater channel depth. Intuitively, you could think of each channel as responding to some different set of features. Reality is a bit more complicated than the most naive interpretations of this intuition since representations are not learned independent but are rather optimized to be jointly useful. So it may not be that a single channel learns an edge detector but rather that some direction in channel space corresponds to detecting edges.
Denote by ci and co the number of input and output channels, respectively, and let kh and kw be the height and width of the kernel. To get an output with multiple channels, we can create a kernel tensor of shape ci × kh × kw for every output channel. We concatenate them on the output channel dimension, so that the shape of the convolution kernel is co × ci × kh × kw. In crosscorrelation operations, the result on each output channel is calculated from the convolution kernel corresponding to that output channel and takes input from all channels in the input tensor.
We implement a cross-correlation function to calculate the output of multiple channels as shown below.
def corr2d_multi_in_out(X, K): # Iterate through the 0th dimension of `K`, and each time, perform # cross-correlation operations with input `X`. All of the results are # stacked together return np.stack([corr2d_multi_in(X, k) for k in K], 0)

242

Chapter 6. Convolutional Neural Networks

We construct a convolution kernel with 3 output channels by concatenating the kernel tensor K with K+1 (plus one for each element in K) and K+2.
K = np.stack((K, K + 1, K + 2), 0) K.shape
(3, 2, 2, 2)
Below, we perform cross-correlation operations on the input tensor X with the kernel tensor K. Now the output contains 3 channels. The result of the first channel is consistent with the result of the previous input tensor X and the multi-input channel, single-output channel kernel.
corr2d_multi_in_out(X, K)
array([[[ 56., 72.], [104., 120.]],
[[ 76., 100.], [148., 172.]],
[[ 96., 128.], [192., 224.]]])
6.4.3 1 × 1 Convolutional Layer
At first, a 1 × 1 convolution, i.e., kh = kw = 1, does not seem to make much sense. After all, a convolution correlates adjacent pixels. A 1 × 1 convolution obviously does not. Nonetheless, they are popular operations that are sometimes included in the designs of complex deep networks. Let us see in some detail what it actually does. Because the minimum window is used, the 1 × 1 convolution loses the ability of larger convolutional layers to recognize patterns consisting of interactions among adjacent elements in the height and width dimensions. The only computation of the 1 × 1 convolution occurs on the channel dimension. Fig. 6.4.2 shows the cross-correlation computation using the 1 × 1 convolution kernel with 3 input channels and 2 output channels. Note that the inputs and outputs have the same height and width. Each element in the output is derived from a linear combination of elements at the same position in the input image. You could think of the 1 × 1 convolutional layer as constituting a fully-connected layer applied at every single pixel location to transform the ci corresponding input values into co output values. Because this is still a convolutional layer, the weights are tied across pixel location. Thus the 1 × 1 convolutional layer requires co × ci weights (plus the bias).

6.4. Multiple Input and Multiple Output Channels

243

Fig. 6.4.2: The cross-correlation computation uses the 1 × 1 convolution kernel with 3 input channels and 2 output channels. The input and output have the same height and width.
Let us check whether this works in practice: we implement a 1 × 1 convolution using a fullyconnected layer. The only thing is that we need to make some adjustments to the data shape before and after the matrix multiplication.
def corr2d_multi_in_out_1x1(X, K): c_i, h, w = X.shape c_o = K.shape[0] X = X.reshape((c_i, h * w)) K = K.reshape((c_o, c_i)) # Matrix multiplication in the fully-connected layer Y = np.dot(K, X) return Y.reshape((c_o, h, w))
When performing 1 × 1 convolution, the above function is equivalent to the previously implemented cross-correlation function corr2d_multi_in_out. Let us check this with some sample data.
X = np.random.normal(0, 1, (3, 3, 3)) K = np.random.normal(0, 1, (2, 3, 1, 1))
Y1 = corr2d_multi_in_out_1x1(X, K) Y2 = corr2d_multi_in_out(X, K) assert float(np.abs(Y1 - Y2).sum()) < 1e-6
Summary
• Multiple channels can be used to extend the model parameters of the convolutional layer. • The 1 × 1 convolutional layer is equivalent to the fully-connected layer, when applied on a
per pixel basis. • The 1 × 1 convolutional layer is typically used to adjust the number of channels between
network layers and to control model complexity.

244

Chapter 6. Convolutional Neural Networks

Exercises
1. Assume that we have two convolution kernels of size k1 and k2, respectively (with no nonlinearity in between).
1. Prove that the result of the operation can be expressed by a single convolution.
2. What is the dimensionality of the equivalent single convolution?
3. Is the converse true?
2. Assume an input of shape ci × h × w and a convolution kernel of shape co × ci × kh × kw, padding of (ph, pw), and stride of (sh, sw).
1. What is the computational cost (multiplications and additions) for the forward propagation?
2. What is the memory footprint?
3. What is the memory footprint for the backward computation?
4. What is the computational cost for the backpropagation?
3. By what factor does the number of calculations increase if we double the number of input channels ci and the number of output channels co? What happens if we double the padding?
4. If the height and width of a convolution kernel is kh = kw = 1, what is the computational complexity of the forward propagation?
5. Are the variables Y1 and Y2 in the last example of this section exactly the same? Why?
6. How would you implement convolutions using matrix multiplication when the convolution window is not 1 × 1?
Discussions89
6.5 Pooling
Often, as we process images, we want to gradually reduce the spatial resolution of our hidden representations, aggregating information so that the higher up we go in the network, the larger the receptive field (in the input) to which each hidden node is sensitive.
Often our ultimate task asks some global question about the image, e.g., does it contain a cat? So typically the units of our final layer should be sensitive to the entire input. By gradually aggregating information, yielding coarser and coarser maps, we accomplish this goal of ultimately learning a global representation, while keeping all of the advantages of convolutional layers at the intermediate layers of processing.
Moreover, when detecting lower-level features, such as edges (as discussed in Section 6.2), we often want our representations to be somewhat invariant to translation. For instance, if we take the image X with a sharp delineation between black and white and shift the whole image by one pixel to the right, i.e., Z[i, j] = X[i, j + 1], then the output for the new image Z might be vastly different. The edge will have shifted by one pixel. In reality, objects hardly ever occur exactly at the same place. In fact, even with a tripod and a stationary object, vibration of the camera due to
89 https://discuss.d2l.ai/t/69

6.5. Pooling

245

the movement of the shutter might shift everything by a pixel or so (high-end cameras are loaded with special features to address this problem).
This section introduces pooling layers, which serve the dual purposes of mitigating the sensitivity of convolutional layers to location and of spatially downsampling representations.
6.5.1 Maximum Pooling and Average Pooling
Like convolutional layers, pooling operators consist of a fixed-shape window that is slid over all regions in the input according to its stride, computing a single output for each location traversed by the fixed-shape window (sometimes known as the pooling window). However, unlike the crosscorrelation computation of the inputs and kernels in the convolutional layer, the pooling layer contains no parameters (there is no kernel). Instead, pooling operators are deterministic, typically calculating either the maximum or the average value of the elements in the pooling window. These operations are called maximum pooling (max pooling for short) and average pooling, respectively.
In both cases, as with the cross-correlation operator, we can think of the pooling window as starting from the upper-left of the input tensor and sliding across the input tensor from left to right and top to bottom. At each location that the pooling window hits, it computes the maximum or average value of the input subtensor in the window, depending on whether max or average pooling is employed.

Fig. 6.5.1: Maximum pooling with a pooling window shape of 2 × 2. The shaded portions are the first output element as well as the input tensor elements used for the output computation: max(0, 1, 3, 4) = 4.

The output tensor in Fig. 6.5.1 has a height of 2 and a width of 2. The four elements are derived from the maximum value in each pooling window:

max(0, 1, 3, 4) = 4, max(1, 2, 4, 5) = 5, max(3, 4, 6, 7) = 7, max(4, 5, 7, 8) = 8.

(6.5.1)

A pooling layer with a pooling window shape of p × q is called a p × q pooling layer. The pooling operation is called p × q pooling.
Let us return to the object edge detection example mentioned at the beginning of this section. Now we will use the output of the convolutional layer as the input for 2 × 2 maximum pooling. Set the convolutional layer input as X and the pooling layer output as Y. Whether or not the values of X[i, j] and X[i, j + 1] are different, or X[i, j + 1] and X[i, j + 2] are different, the pooling layer always outputs Y[i, j] = 1. That is to say, using the 2 × 2 maximum pooling layer, we can still detect if the pattern recognized by the convolutional layer moves no more than one element in height or width.

246

Chapter 6. Convolutional Neural Networks

In the code below, we implement the forward propagation of the pooling layer in the pool2d function. This function is similar to the corr2d function in Section 6.2. However, here we have no kernel, computing the output as either the maximum or the average of each region in the input.
from mxnet import np, npx from mxnet.gluon import nn from d2l import mxnet as d2l
npx.set_np()
def pool2d(X, pool_size, mode='max'): p_h, p_w = pool_size Y = np.zeros((X.shape[0] - p_h + 1, X.shape[1] - p_w + 1)) for i in range(Y.shape[0]): for j in range(Y.shape[1]): if mode == 'max': Y[i, j] = X[i:i + p_h, j:j + p_w].max() elif mode == 'avg': Y[i, j] = X[i:i + p_h, j:j + p_w].mean() return Y
We can construct the input tensor X in Fig. 6.5.1 to validate the output of the two-dimensional maximum pooling layer.
X = np.array([[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]]) pool2d(X, (2, 2))
array([[4., 5.], [7., 8.]])
Also, we experiment with the average pooling layer.
pool2d(X, (2, 2), 'avg')
array([[2., 3.], [5., 6.]])
6.5.2 Padding and Stride
As with convolutional layers, pooling layers can also change the output shape. And as before, we can alter the operation to achieve a desired output shape by padding the input and adjusting the stride. We can demonstrate the use of padding and strides in pooling layers via the built-in twodimensional maximum pooling layer from the deep learning framework. We first construct an input tensor X whose shape has four dimensions, where the number of examples (batch size) and number of channels are both 1.
X = np.arange(16, dtype=np.float32).reshape((1, 1, 4, 4)) X

6.5. Pooling

247

array([[[[ 0., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.], [12., 13., 14., 15.]]]])
By default, the stride and the pooling window in the instance from the frameworkʼs built-in class have the same shape. Below, we use a pooling window of shape (3, 3), so we get a stride shape of (3, 3) by default.
pool2d = nn.MaxPool2D(3) # Because there are no model parameters in the pooling layer, we do not need # to call the parameter initialization function pool2d(X)
array([[[[10.]]]])
The stride and padding can be manually specified.
pool2d = nn.MaxPool2D(3, padding=1, strides=2) pool2d(X)
array([[[[ 5., 7.], [13., 15.]]]])
Of course, we can specify an arbitrary rectangular pooling window and specify the padding and stride for height and width, respectively.
pool2d = nn.MaxPool2D((2, 3), padding=(0, 1), strides=(2, 3)) pool2d(X)
array([[[[ 5., 7.], [13., 15.]]]])

6.5.3 Multiple Channels
When processing multi-channel input data, the pooling layer pools each input channel separately, rather than summing the inputs up over channels as in a convolutional layer. This means that the number of output channels for the pooling layer is the same as the number of input channels. Below, we will concatenate tensors X and X + 1 on the channel dimension to construct an input with 2 channels.
X = np.concatenate((X, X + 1), 1) X

array([[[[ 0., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.], [12., 13., 14., 15.]],
248

(continues on next page)
Chapter 6. Convolutional Neural Networks

(continued from previous page)
[[ 1., 2., 3., 4.], [ 5., 6., 7., 8.], [ 9., 10., 11., 12.], [13., 14., 15., 16.]]]])
As we can see, the number of output channels is still 2 after pooling.
pool2d = nn.MaxPool2D(3, padding=1, strides=2) pool2d(X)
array([[[[ 5., 7.], [13., 15.]],
[[ 6., 8.], [14., 16.]]]])

Summary
• Taking the input elements in the pooling window, the maximum pooling operation assigns the maximum value as the output and the average pooling operation assigns the average value as the output.
• One of the major benefits of a pooling layer is to alleviate the excessive sensitivity of the convolutional layer to location.
• We can specify the padding and stride for the pooling layer. • Maximum pooling, combined with a stride larger than 1 can be used to reduce the spatial
dimensions (e.g., width and height). • The pooling layerʼs number of output channels is the same as the number of input channels.
Exercises
1. Can you implement average pooling as a special case of a convolution layer? If so, do it. 2. Can you implement maximum pooling as a special case of a convolution layer? If so, do it. 3. What is the computational cost of the pooling layer? Assume that the input to the pooling
layer is of size c × h × w, the pooling window has a shape of ph × pw with a padding of (ph, pw) and a stride of (sh, sw). 4. Why do you expect maximum pooling and average pooling to work differently? 5. Do we need a separate minimum pooling layer? Can you replace it with another operation? 6. Is there another operation between average and maximum pooling that you could consider (hint: recall the softmax)? Why might it not be so popular? Discussions90
90 https://discuss.d2l.ai/t/71

6.5. Pooling

249

6.6 Convolutional Neural Networks (LeNet)
We now have all the ingredients required to assemble a fully-functional CNN. In our earlier encounter with image data, we applied a softmax regression model (Section 3.6) and an MLP model (Section 4.2) to pictures of clothing in the Fashion-MNIST dataset. To make such data amenable to softmax regression and MLPs, we first flattened each image from a 28 × 28 matrix into a fixedlength 784-dimensional vector, and thereafter processed them with fully-connected layers. Now that we have a handle on convolutional layers, we can retain the spatial structure in our images. As an additional benefit of replacing fully-connected layers with convolutional layers, we will enjoy more parsimonious models that require far fewer parameters.
In this section, we will introduce LeNet, among the first published CNNs to capture wide attention for its performance on computer vision tasks. The model was introduced by (and named for) Yann LeCun, then a researcher at AT&T Bell Labs, for the purpose of recognizing handwritten digits in images (LeCun et al., 1998). This work represented the culmination of a decade of research developing the technology. In 1989, LeCun published the first study to successfully train CNNs via backpropagation.
At the time LeNet achieved outstanding results matching the performance of support vector machines, then a dominant approach in supervised learning. LeNet was eventually adapted to recognize digits for processing deposits in ATM machines. To this day, some ATMs still run the code that Yann and his colleague Leon Bottou wrote in the 1990s!
6.6.1 LeNet
At a high level, LeNet (LeNet-5) consists of two parts: (i) a convolutional encoder consisting of two convolutional layers; and (ii) a dense block consisting of three fully-connected layers; The architecture is summarized in Fig. 6.6.1.

Fig. 6.6.1: Data flow in LeNet. The input is a handwritten digit, the output a probability over 10 possible outcomes.
The basic units in each convolutional block are a convolutional layer, a sigmoid activation function, and a subsequent average pooling operation. Note that while ReLUs and max-pooling work better, these discoveries had not yet been made in the 1990s. Each convolutional layer uses a 5 × 5 kernel and a sigmoid activation function. These layers map spatially arranged inputs to a number

250

Chapter 6. Convolutional Neural Networks

of two-dimensional feature maps, typically increasing the number of channels. The first convolutional layer has 6 output channels, while the second has 16. Each 2 × 2 pooling operation (stride 2) reduces dimensionality by a factor of 4 via spatial downsampling. The convolutional block emits an output with shape given by (batch size, number of channel, height, width).
In order to pass output from the convolutional block to the dense block, we must flatten each example in the minibatch. In other words, we take this four-dimensional input and transform it into the two-dimensional input expected by fully-connected layers: as a reminder, the twodimensional representation that we desire uses the first dimension to index examples in the minibatch and the second to give the flat vector representation of each example. LeNetʼs dense block has three fully-connected layers, with 120, 84, and 10 outputs, respectively. Because we are still performing classification, the 10-dimensional output layer corresponds to the number of possible output classes.
While getting to the point where you truly understand what is going on inside LeNet may have taken a bit of work, hopefully the following code snippet will convince you that implementing such models with modern deep learning frameworks is remarkably simple. We need only to instantiate a Sequential block and chain together the appropriate layers.
from mxnet import autograd, gluon, init, np, npx from mxnet.gluon import nn from d2l import mxnet as d2l
npx.set_np()
net = nn.Sequential() net.add(
nn.Conv2D(channels=6, kernel_size=5, padding=2, activation='sigmoid'), nn.AvgPool2D(pool_size=2, strides=2), nn.Conv2D(channels=16, kernel_size=5, activation='sigmoid'), nn.AvgPool2D(pool_size=2, strides=2), # `Dense` will transform an input of the shape (batch size, number of # channels, height, width) into an input of the shape (batch size, # number of channels * height * width) automatically by default nn.Dense(120, activation='sigmoid'), nn.Dense(84, activation='sigmoid'), nn.Dense(10))
We took a small liberty with the original model, removing the Gaussian activation in the final layer. Other than that, this network matches the original LeNet-5 architecture.
By passing a single-channel (black and white) 28 × 28 image through the network and printing the output shape at each layer, we can inspect the model to make sure that its operations line up with what we expect from Fig. 6.6.2.

6.6. Convolutional Neural Networks (LeNet)

251

Fig. 6.6.2: Compressed notation for LeNet-5.

X = np.random.uniform(size=(1, 1, 28, 28)) net.initialize() for layer in net:
X = layer(X) print(layer.name, 'output shape:\t', X.shape)

conv0 output shape: (1, 6, 28, 28)

pool0 output shape: (1, 6, 14, 14)

conv1 output shape: (1, 16, 10, 10)

pool1 output shape: (1, 16, 5, 5)

dense0 output shape:

(1, 120)

dense1 output shape:

(1, 84)

dense2 output shape:

(1, 10)

Note that the height and width of the representation at each layer throughout the convolutional block is reduced (compared with the previous layer). The first convolutional layer uses 2 pixels of padding to compensate for the reduction in height and width that would otherwise result from using a 5×5 kernel. In contrast, the second convolutional layer forgoes padding, and thus the height and width are both reduced by 4 pixels. As we go up the stack of layers, the number of channels increases layer-over-layer from 1 in the input to 6 after the first convolutional layer and 16 after the second convolutional layer. However, each pooling layer halves the height and width. Finally, each fully-connected layer reduces dimensionality, finally emitting an output whose dimension matches the number of classes.

252

Chapter 6. Convolutional Neural Networks

6.6.2 Training
Now that we have implemented the model, let us run an experiment to see how LeNet fares on Fashion-MNIST.
batch_size = 256 train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size=batch_size)

While CNNs have fewer parameters, they can still be more expensive to compute than similarly deep MLPs because each parameter participates in many more multiplications. If you have access to a GPU, this might be a good time to put it into action to speed up training.
For evaluation, we need to make a slight modification to the evaluate_accuracy function that we described in Section 3.6. Since the full dataset is in the main memory, we need to copy it to the GPU memory before the model uses GPU to compute with the dataset.
def evaluate_accuracy_gpu(net, data_iter, device=None): #@save """Compute the accuracy for a model on a dataset using a GPU.""" if not device: # Query the first device where the first parameter is on device = list(net.collect_params().values())[0].list_ctx()[0] # No. of correct predictions, no. of predictions metric = d2l.Accumulator(2) for X, y in data_iter: X, y = X.as_in_ctx(device), y.as_in_ctx(device) metric.add(d2l.accuracy(net(X), y), y.size) return metric[0] / metric[1]
We also need to update our training function to deal with GPUs. Unlike the train_epoch_ch3 defined in Section 3.6, we now need to move each minibatch of data to our designated device (hopefully, the GPU) prior to making the forward and backward propagations.
The training function train_ch6 is also similar to train_ch3 defined in Section 3.6. Since we will be implementing networks with many layers going forward, we will rely primarily on high-level APIs. The following training function assumes a model created from high-level APIs as input and is optimized accordingly. We initialize the model parameters on the device indicated by the device argument, using Xavier initialization as introduced in Section 4.8.2. Just as with MLPs, our loss function is cross-entropy, and we minimize it via minibatch stochastic gradient descent. Since each epoch takes tens of seconds to run, we visualize the training loss more frequently.
#@save def train_ch6(net, train_iter, test_iter, num_epochs, lr, device):
"""Train a model with a GPU (defined in Chapter 6).""" net.initialize(force_reinit=True, ctx=device, init=init.Xavier()) loss = gluon.loss.SoftmaxCrossEntropyLoss() trainer = gluon.Trainer(net.collect_params(), 'sgd',
{'learning_rate': lr}) animator = d2l.Animator(xlabel='epoch', xlim=[1, num_epochs],
legend=['train loss', 'train acc', 'test acc']) timer, num_batches = d2l.Timer(), len(train_iter) for epoch in range(num_epochs):
# Sum of training loss, sum of training accuracy, no. of examples metric = d2l.Accumulator(3) for i, (X, y) in enumerate(train_iter):
timer.start()
(continues on next page)

6.6. Convolutional Neural Networks (LeNet)

253

(continued from previous page)
# Here is the major difference from `d2l.train_epoch_ch3` X, y = X.as_in_ctx(device), y.as_in_ctx(device) with autograd.record():
y_hat = net(X) l = loss(y_hat, y) l.backward() trainer.step(X.shape[0]) metric.add(l.sum(), d2l.accuracy(y_hat, y), X.shape[0]) timer.stop() train_l = metric[0] / metric[2] train_acc = metric[1] / metric[2] if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1: animator.add(epoch + (i + 1) / num_batches,
(train_l, train_acc, None)) test_acc = evaluate_accuracy_gpu(net, test_iter) animator.add(epoch + 1, (None, None, test_acc)) print(f'loss {train_l:.3f}, train acc {train_acc:.3f}, '
f'test acc {test_acc:.3f}') print(f'{metric[2] * num_epochs / timer.sum():.1f} examples/sec '
f'on {str(device)}')
Now let us train and evaluate the LeNet-5 model.
lr, num_epochs = 0.9, 10 train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())
loss 0.471, train acc 0.823, test acc 0.811 37405.6 examples/sec on gpu(0)

254

Chapter 6. Convolutional Neural Networks

Summary
• A CNN is a network that employs convolutional layers. • In a CNN, we interleave convolutions, nonlinearities, and (often) pooling operations. • In a CNN, convolutional layers are typically arranged so that they gradually decrease the
spatial resolution of the representations, while increasing the number of channels. • In traditional CNNs, the representations encoded by the convolutional blocks are processed
by one or more fully-connected layers prior to emitting output. • LeNet was arguably the first successful deployment of such a network.
Exercises
1. Replace the average pooling with maximum pooling. What happens? 2. Try to construct a more complex network based on LeNet to improve its accuracy.
1. Adjust the convolution window size. 2. Adjust the number of output channels. 3. Adjust the activation function (e.g., ReLU). 4. Adjust the number of convolution layers. 5. Adjust the number of fully connected layers. 6. Adjust the learning rates and other training details (e.g., initialization and number of
epochs.) 3. Try out the improved network on the original MNIST dataset. 4. Display the activations of the first and second layer of LeNet for different inputs (e.g.,
sweaters and coats). Discussions91

91 https://discuss.d2l.ai/t/73

6.6. Convolutional Neural Networks (LeNet)

255

256

Chapter 6. Convolutional Neural Networks

7 | Modern Convolutional Neural Networks
Now that we understand the basics of wiring together CNNs, we will take you through a tour of modern CNN architectures. In this chapter, each section corresponds to a significant CNN architecture that was at some point (or currently) the base model upon which many research projects and deployed systems were built. Each of these networks was briefly a dominant architecture and many were winners or runners-up in the ImageNet competition, which has served as a barometer of progress on supervised learning in computer vision since 2010.
These models include AlexNet, the first large-scale network deployed to beat conventional computer vision methods on a large-scale vision challenge; the VGG network, which makes use of a number of repeating blocks of elements; the network in network (NiN) which convolves whole neural networks patch-wise over inputs; GoogLeNet, which uses networks with parallel concatenations; residual networks (ResNet), which remain the most popular off-the-shelf architecture in computer vision; and densely connected networks (DenseNet), which are expensive to compute but have set some recent benchmarks.
While the idea of deep neural networks is quite simple (stack together a bunch of layers), performance can vary wildly across architectures and hyperparameter choices. The neural networks described in this chapter are the product of intuition, a few mathematical insights, and a whole lot of trial and error. We present these models in chronological order, partly to convey a sense of the history so that you can form your own intuitions about where the field is heading and perhaps develop your own architectures. For instance, batch normalization and residual connections described in this chapter have offered two popular ideas for training and designing deep models.
7.1 Deep Convolutional Neural Networks (AlexNet)
Although CNNs were well known in the computer vision and machine learning communities following the introduction of LeNet, they did not immediately dominate the field. Although LeNet achieved good results on early small datasets, the performance and feasibility of training CNNs on larger, more realistic datasets had yet to be established. In fact, for much of the intervening time between the early 1990s and the watershed results of 2012, neural networks were often surpassed by other machine learning methods, such as support vector machines.
For computer vision, this comparison is perhaps not fair. That is although the inputs to convolutional networks consist of raw or lightly-processed (e.g., by centering) pixel values, practitioners would never feed raw pixels into traditional models. Instead, typical computer vision pipelines consisted of manually engineering feature extraction pipelines. Rather than learn the features, the features were crafted. Most of the progress came from having more clever ideas for features, and the learning algorithm was often relegated to an afterthought.
257

Although some neural network accelerators were available in the 1990s, they were not yet sufficiently powerful to make deep multichannel, multilayer CNNs with a large number of parameters. Moreover, datasets were still relatively small. Added to these obstacles, key tricks for training neural networks including parameter initialization heuristics, clever variants of stochastic gradient descent, non-squashing activation functions, and effective regularization techniques were still missing.
Thus, rather than training end-to-end (pixel to classification) systems, classical pipelines looked more like this:
1. Obtain an interesting dataset. In early days, these datasets required expensive sensors (at the time, 1 megapixel images were state-of-the-art).
2. Preprocess the dataset with hand-crafted features based on some knowledge of optics, geometry, other analytic tools, and occasionally on the serendipitous discoveries of lucky graduate students.
3. Feed the data through a standard set of feature extractors such as the SIFT (scale-invariant feature transform) (Lowe, 2004), the SURF (speeded up robust features) (Bay et al., 2006), or any number of other hand-tuned pipelines.
4. Dump the resulting representations into your favorite classifier, likely a linear model or kernel method, to train a classifier.
If you spoke to machine learning researchers, they believed that machine learning was both important and beautiful. Elegant theories proved the properties of various classifiers. The field of machine learning was thriving, rigorous, and eminently useful. However, if you spoke to a computer vision researcher, you would hear a very different story. The dirty truth of image recognition, they would tell you, is that features, not learning algorithms, drove progress. Computer vision researchers justifiably believed that a slightly bigger or cleaner dataset or a slightly improved feature-extraction pipeline mattered far more to the final accuracy than any learning algorithm.

7.1.1 Learning Representations
Another way to cast the state of affairs is that the most important part of the pipeline was the representation. And up until 2012 the representation was calculated mechanically. In fact, engineering a new set of feature functions, improving results, and writing up the method was a prominent genre of paper. SIFT (Lowe, 2004), SURF (Bay et al., 2006), HOG (histograms of oriented gradient) (Dalal & Triggs, 2005), bags of visual words92 and similar feature extractors ruled the roost.
Another group of researchers, including Yann LeCun, Geoff Hinton, Yoshua Bengio, Andrew Ng, Shun-ichi Amari, and Juergen Schmidhuber, had different plans. They believed that features themselves ought to be learned. Moreover, they believed that to be reasonably complex, the features ought to be hierarchically composed with multiple jointly learned layers, each with learnable parameters. In the case of an image, the lowest layers might come to detect edges, colors, and textures. Indeed, Alex Krizhevsky, Ilya Sutskever, and Geoff Hinton proposed a new variant of a CNN, AlexNet, that achieved excellent performance in the 2012 ImageNet challenge. AlexNet was named after Alex Krizhevsky, the first author of the breakthrough ImageNet classification paper (Krizhevsky et al., 2012).
Interestingly in the lowest layers of the network, the model learned feature extractors that resembled some traditional filters. Fig. 7.1.1 is reproduced from the AlexNet paper (Krizhevsky et al., 2012) and describes lower-level image descriptors.
92 https://en.wikipedia.org/wiki/Bag-of-words_model_in_computer_vision

258

Chapter 7. Modern Convolutional Neural Networks

Fig. 7.1.1: Image filters learned by the first layer of AlexNet.
Higher layers in the network might build upon these representations to represent larger structures, like eyes, noses, blades of grass, and so on. Even higher layers might represent whole objects like people, airplanes, dogs, or frisbees. Ultimately, the final hidden state learns a compact representation of the image that summarizes its contents such that data belonging to different categories can be easily separated.
While the ultimate breakthrough for many-layered CNNs came in 2012, a core group of researchers had dedicated themselves to this idea, attempting to learn hierarchical representations of visual data for many years. The ultimate breakthrough in 2012 can be attributed to two key factors.
Missing Ingredient: Data
Deep models with many layers require large amounts of data in order to enter the regime where they significantly outperform traditional methods based on convex optimizations (e.g., linear and kernel methods). However, given the limited storage capacity of computers, the relative expense of sensors, and the comparatively tighter research budgets in the 1990s, most research relied on tiny datasets. Numerous papers addressed the UCI collection of datasets, many of which contained only hundreds or (a few) thousands of images captured in unnatural settings with low resolution.
In 2009, the ImageNet dataset was released, challenging researchers to learn models from 1 million examples, 1000 each from 1000 distinct categories of objects. The researchers, led by Fei-Fei Li, who introduced this dataset leveraged Google Image Search to prefilter large candidate sets for each category and employed the Amazon Mechanical Turk crowdsourcing pipeline to confirm for each image whether it belonged to the associated category. This scale was unprecedented. The associated competition, dubbed the ImageNet Challenge pushed computer vision and machine learning research forward, challenging researchers to identify which models performed best at a greater scale than academics had previously considered.

7.1. Deep Convolutional Neural Networks (AlexNet)

259

Missing Ingredient: Hardware
Deep learning models are voracious consumers of compute cycles. Training can take hundreds of epochs, and each iteration requires passing data through many layers of computationallyexpensive linear algebra operations. This is one of the main reasons why in the 1990s and early 2000s, simple algorithms based on the more-efficiently optimized convex objectives were preferred.
Graphical processing units (GPUs) proved to be a game changer in making deep learning feasible. These chips had long been developed for accelerating graphics processing to benefit computer games. In particular, they were optimized for high throughput 4×4 matrix-vector products, which are needed for many computer graphics tasks. Fortunately, this math is strikingly similar to that required to calculate convolutional layers. Around that time, NVIDIA and ATI had begun optimizing GPUs for general computing operations, going as far as to market them as general-purpose GPUs (GPGPU).
To provide some intuition, consider the cores of a modern microprocessor (CPU). Each of the cores is fairly powerful running at a high clock frequency and sporting large caches (up to several megabytes of L3). Each core is well-suited to executing a wide range of instructions, with branch predictors, a deep pipeline, and other bells and whistles that enable it to run a large variety of programs. This apparent strength, however, is also its Achilles heel: general-purpose cores are very expensive to build. They require lots of chip area, a sophisticated support structure (memory interfaces, caching logic between cores, high-speed interconnects, and so on), and they are comparatively bad at any single task. Modern laptops have up to 4 cores, and even high-end servers rarely exceed 64 cores, simply because it is not cost effective.
By comparison, GPUs consist of 100 ∼ 1000 small processing elements (the details differ somewhat between NVIDIA, ATI, ARM and other chip vendors), often grouped into larger groups (NVIDIA calls them warps). While each core is relatively weak, sometimes even running at sub1GHz clock frequency, it is the total number of such cores that makes GPUs orders of magnitude faster than CPUs. For instance, NVIDIAʼs recent Volta generation offers up to 120 TFlops per chip for specialized instructions (and up to 24 TFlops for more general-purpose ones), while floating point performance of CPUs has not exceeded 1 TFlop to date. The reason for why this is possible is actually quite simple: first, power consumption tends to grow quadratically with clock frequency. Hence, for the power budget of a CPU core that runs 4 times faster (a typical number), you can use 16 GPU cores at 1/4 the speed, which yields 16 × 1/4 = 4 times the performance. Furthermore, GPU cores are much simpler (in fact, for a long time they were not even able to execute generalpurpose code), which makes them more energy efficient. Last, many operations in deep learning require high memory bandwidth. Again, GPUs shine here with buses that are at least 10 times as wide as many CPUs.
Back to 2012. A major breakthrough came when Alex Krizhevsky and Ilya Sutskever implemented a deep CNN that could run on GPU hardware. They realized that the computational bottlenecks in CNNs, convolutions and matrix multiplications, are all operations that could be parallelized in hardware. Using two NVIDIA GTX 580s with 3GB of memory, they implemented fast convolutions. The code cuda-convnet93 was good enough that for several years it was the industry standard and powered the first couple years of the deep learning boom.
93 https://code.google.com/archive/p/cuda-convnet/

260

Chapter 7. Modern Convolutional Neural Networks

7.1.2 AlexNet
AlexNet, which employed an 8-layer CNN, won the ImageNet Large Scale Visual Recognition Challenge 2012 by a phenomenally large margin. This network showed, for the first time, that the features obtained by learning can transcend manually-designed features, breaking the previous paradigm in computer vision.
The architectures of AlexNet and LeNet are very similar, as Fig. 7.1.2 illustrates. Note that we provide a slightly streamlined version of AlexNet removing some of the design quirks that were needed in 2012 to make the model fit on two small GPUs.

Fig. 7.1.2: From LeNet (left) to AlexNet (right).
The design philosophies of AlexNet and LeNet are very similar, but there are also significant differences. First, AlexNet is much deeper than the comparatively small LeNet5. AlexNet consists of eight layers: five convolutional layers, two fully-connected hidden layers, and one fully-connected output layer. Second, AlexNet used the ReLU instead of the sigmoid as its activation function. Let us delve into the details below.

7.1. Deep Convolutional Neural Networks (AlexNet)

261

Architecture
In AlexNetʼs first layer, the convolution window shape is 11 × 11. Since most images in ImageNet are more than ten times higher and wider than the MNIST images, objects in ImageNet data tend to occupy more pixels. Consequently, a larger convolution window is needed to capture the object. The convolution window shape in the second layer is reduced to 5×5, followed by 3×3. In addition, after the first, second, and fifth convolutional layers, the network adds maximum pooling layers with a window shape of 3 × 3 and a stride of 2. Moreover, AlexNet has ten times more convolution channels than LeNet.
After the last convolutional layer there are two fully-connected layers with 4096 outputs. These two huge fully-connected layers produce model parameters of nearly 1 GB. Due to the limited memory in early GPUs, the original AlexNet used a dual data stream design, so that each of their two GPUs could be responsible for storing and computing only its half of the model. Fortunately, GPU memory is comparatively abundant now, so we rarely need to break up models across GPUs these days (our version of the AlexNet model deviates from the original paper in this aspect).
Activation Functions
Besides, AlexNet changed the sigmoid activation function to a simpler ReLU activation function. On one hand, the computation of the ReLU activation function is simpler. For example, it does not have the exponentiation operation found in the sigmoid activation function. On the other hand, the ReLU activation function makes model training easier when using different parameter initialization methods. This is because, when the output of the sigmoid activation function is very close to 0 or 1, the gradient of these regions is almost 0, so that backpropagation cannot continue to update some of the model parameters. In contrast, the gradient of the ReLU activation function in the positive interval is always 1. Therefore, if the model parameters are not properly initialized, the sigmoid function may obtain a gradient of almost 0 in the positive interval, so that the model cannot be effectively trained.
Capacity Control and Preprocessing
AlexNet controls the model complexity of the fully-connected layer by dropout (Section 4.6), while LeNet only uses weight decay. To augment the data even further, the training loop of AlexNet added a great deal of image augmentation, such as flipping, clipping, and color changes. This makes the model more robust and the larger sample size effectively reduces overfitting. We will discuss data augmentation in greater detail in Section 13.1.
from mxnet import np, npx from mxnet.gluon import nn from d2l import mxnet as d2l
npx.set_np()
net = nn.Sequential() # Here, we use a larger 11 x 11 window to capture objects. At the same time, # we use a stride of 4 to greatly reduce the height and width of the output. # Here, the number of output channels is much larger than that in LeNet net.add(
nn.Conv2D(96, kernel_size=11, strides=4, activation='relu'),
(continues on next page)

262

Chapter 7. Modern Convolutional Neural Networks

(continued from previous page)
nn.MaxPool2D(pool_size=3, strides=2), # Make the convolution window smaller, set padding to 2 for consistent # height and width across the input and output, and increase the # number of output channels nn.Conv2D(256, kernel_size=5, padding=2, activation='relu'), nn.MaxPool2D(pool_size=3, strides=2), # Use three successive convolutional layers and a smaller convolution # window. Except for the final convolutional layer, the number of # output channels is further increased. Pooling layers are not used to # reduce the height and width of input after the first two # convolutional layers nn.Conv2D(384, kernel_size=3, padding=1, activation='relu'), nn.Conv2D(384, kernel_size=3, padding=1, activation='relu'), nn.Conv2D(256, kernel_size=3, padding=1, activation='relu'), nn.MaxPool2D(pool_size=3, strides=2), # Here, the number of outputs of the fully-connected layer is several # times larger than that in LeNet. Use the dropout layer to mitigate # overfitting nn.Dense(4096, activation='relu'), nn.Dropout(0.5), nn.Dense(4096, activation='relu'), nn.Dropout(0.5), # Output layer. Since we are using Fashion-MNIST, the number of # classes is 10, instead of 1000 as in the paper nn.Dense(10))
We construct a single-channel data example with both height and width of 224 to observe the output shape of each layer. It matches the AlexNet architecture in Fig. 7.1.2.
X = np.random.uniform(size=(1, 1, 224, 224)) net.initialize() for layer in net:
X = layer(X) print(layer.name, 'output shape:\t', X.shape)

conv0 output shape: (1, pool0 output shape: (1, conv1 output shape: (1, pool1 output shape: (1, conv2 output shape: (1, conv3 output shape: (1, conv4 output shape: (1, pool2 output shape: (1, dense0 output shape: dropout0 output shape: dense1 output shape: dropout1 output shape: dense2 output shape:

96, 54, 54) 96, 26, 26) 256, 26, 26) 256, 12, 12) 384, 12, 12) 384, 12, 12) 256, 12, 12) 256, 5, 5)
(1, 4096) (1, 4096) (1, 4096) (1, 4096) (1, 10)

7.1. Deep Convolutional Neural Networks (AlexNet)

263

7.1.3 Reading the Dataset
Although AlexNet is trained on ImageNet in the paper, we use Fashion-MNIST here since training an ImageNet model to convergence could take hours or days even on a modern GPU. One of the problems with applying AlexNet directly on Fashion-MNIST is that its images have lower resolution (28 × 28 pixels) than ImageNet images. To make things work, we upsample them to 224 × 224 (generally not a smart practice, but we do it here to be faithful to the AlexNet architecture). We perform this resizing with the resize argument in the d2l.load_data_fashion_mnist function.
batch_size = 128 train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=224)
7.1.4 Training
Now, we can start training AlexNet. Compared with LeNet in Section 6.6, the main change here is the use of a smaller learning rate and much slower training due to the deeper and wider network, the higher image resolution, and the more costly convolutions.
lr, num_epochs = 0.01, 10 d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())
loss 0.333, train acc 0.877, test acc 0.884 3981.2 examples/sec on gpu(0)

Summary
• AlexNet has a similar structure to that of LeNet, but uses more convolutional layers and a larger parameter space to fit the large-scale ImageNet dataset.
• Today AlexNet has been surpassed by much more effective architectures but it is a key step from shallow to deep networks that are used nowadays.
• Although it seems that there are only a few more lines in AlexNetʼs implementation than in LeNet, it took the academic community many years to embrace this conceptual change and

264

Chapter 7. Modern Convolutional Neural Networks

take advantage of its excellent experimental results. This was also due to the lack of efficient computational tools. • Dropout, ReLU, and preprocessing were the other key steps in achieving excellent performance in computer vision tasks.
Exercises
1. Try increasing the number of epochs. Compared with LeNet, how are the results different? Why?
2. AlexNet may be too complex for the Fashion-MNIST dataset. 1. Try simplifying the model to make the training faster, while ensuring that the accuracy does not drop significantly. 2. Design a better model that works directly on 28 × 28 images.
3. Modify the batch size, and observe the changes in accuracy and GPU memory. 4. Analyze computational performance of AlexNet.
1. What is the dominant part for the memory footprint of AlexNet? 2. What is the dominant part for computation in AlexNet? 3. How about memory bandwidth when computing the results? 5. Apply dropout and ReLU to LeNet-5. Does it improve? How about preprocessing? Discussions94
7.2 Networks Using Blocks (VGG)
While AlexNet offered empirical evidence that deep CNNs can achieve good results, it did not provide a general template to guide subsequent researchers in designing new networks. In the following sections, we will introduce several heuristic concepts commonly used to design deep networks. Progress in this field mirrors that in chip design where engineers went from placing transistors to logical elements to logic blocks. Similarly, the design of neural network architectures had grown progressively more abstract, with researchers moving from thinking in terms of individual neurons to whole layers, and now to blocks, repeating patterns of layers. The idea of using blocks first emerged from the Visual Geometry Group95 (VGG) at Oxford University, in their eponymously-named VGG network. It is easy to implement these repeated structures in code with any modern deep learning framework by using loops and subroutines.
94 https://discuss.d2l.ai/t/75 95 http://www.robots.ox.ac.uk/~vgg/

7.2. Networks Using Blocks (VGG)

265

7.2.1 VGG Blocks
The basic building block of classic CNNs is a sequence of the following: (i) a convolutional layer with padding to maintain the resolution, (ii) a nonlinearity such as a ReLU, (iii) a pooling layer such as a maximum pooling layer. One VGG block consists of a sequence of convolutional layers, followed by a maximum pooling layer for spatial downsampling. In the original VGG paper (Simonyan & Zisserman, 2014), the authors employed convolutions with 3 × 3 kernels with padding of 1 (keeping height and width) and 2×2 maximum pooling with stride of 2 (halving the resolution after each block). In the code below, we define a function called vgg_block to implement one VGG block.
The function takes two arguments corresponding to the number of convolutional layers num_convs and the number of output channels num_channels.
from mxnet import np, npx from mxnet.gluon import nn from d2l import mxnet as d2l
npx.set_np()
def vgg_block(num_convs, num_channels): blk = nn.Sequential() for _ in range(num_convs): blk.add( nn.Conv2D(num_channels, kernel_size=3, padding=1, activation='relu')) blk.add(nn.MaxPool2D(pool_size=2, strides=2)) return blk
7.2.2 VGG Network
Like AlexNet and LeNet, the VGG Network can be partitioned into two parts: the first consisting mostly of convolutional and pooling layers and the second consisting of fully-connected layers. This is depicted in Fig. 7.2.1.

266

Chapter 7. Modern Convolutional Neural Networks

Fig. 7.2.1: From AlexNet to VGG that is designed from building blocks.

The convolutional part of the network connects several VGG blocks from Fig. 7.2.1 (also defined in the vgg_block function) in succession. The following variable conv_arch consists of a list of tuples (one per block), where each contains two values: the number of convolutional layers and the number of output channels, which are precisely the arguments required to call the vgg_block function. The fully-connected part of the VGG network is identical to that covered in AlexNet.
The original VGG network had 5 convolutional blocks, among which the first two have one convolutional layer each and the latter three contain two convolutional layers each. The first block has 64 output channels and each subsequent block doubles the number of output channels, until that number reaches 512. Since this network uses 8 convolutional layers and 3 fully-connected layers, it is often called VGG-11.
conv_arch = ((1, 64), (1, 128), (2, 256), (2, 512), (2, 512))
The following code implements VGG-11. This is a simple matter of executing a for-loop over conv_arch.
def vgg(conv_arch): net = nn.Sequential() # The convolutional part for (num_convs, num_channels) in conv_arch: net.add(vgg_block(num_convs, num_channels)) # The fully-connected part net.add(nn.Dense(4096, activation='relu'), nn.Dropout(0.5), nn.Dense(4096, activation='relu'), nn.Dropout(0.5), nn.Dense(10)) return net
net = vgg(conv_arch)
Next, we will construct a single-channel data example with a height and width of 224 to observe

7.2. Networks Using Blocks (VGG)

267

the output shape of each layer.
net.initialize() X = np.random.uniform(size=(1, 1, 224, 224)) for blk in net:
X = blk(X) print(blk.name, 'output shape:\t', X.shape)

sequential1 output shape: sequential2 output shape: sequential3 output shape: sequential4 output shape: sequential5 output shape: dense0 output shape: dropout0 output shape: dense1 output shape: dropout1 output shape: dense2 output shape:

(1, 64, 112, 112) (1, 128, 56, 56) (1, 256, 28, 28) (1, 512, 14, 14) (1, 512, 7, 7) (1, 4096) (1, 4096) (1, 4096) (1, 4096) (1, 10)

As you can see, we halve height and width at each block, finally reaching a height and width of 7 before flattening the representations for processing by the fully-connected part of the network.

7.2.3 Training
Since VGG-11 is more computationally-heavy than AlexNet we construct a network with a smaller number of channels. This is more than sufficient for training on Fashion-MNIST.
ratio = 4 small_conv_arch = [(pair[0], pair[1] // ratio) for pair in conv_arch] net = vgg(small_conv_arch)
Apart from using a slightly larger learning rate, the model training process is similar to that of AlexNet in Section 7.1.
lr, num_epochs, batch_size = 0.05, 10, 128 train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=224) d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())

loss 0.174, train acc 0.935, test acc 0.927 1771.4 examples/sec on gpu(0)

268

Chapter 7. Modern Convolutional Neural Networks

Summary
• VGG-11 constructs a network using reusable convolutional blocks. Different VGG models can be defined by the differences in the number of convolutional layers and output channels in each block.
• The use of blocks leads to very compact representations of the network definition. It allows for efficient design of complex networks.
• In their VGG paper, Simonyan and Ziserman experimented with various architectures. In particular, they found that several layers of deep and narrow convolutions (i.e., 3 × 3) were more effective than fewer layers of wider convolutions.
Exercises
1. When printing out the dimensions of the layers we only saw 8 results rather than 11. Where did the remaining 3 layer information go?
2. Compared with AlexNet, VGG is much slower in terms of computation, and it also needs more GPU memory. Analyze the reasons for this.
3. Try changing the height and width of the images in Fashion-MNIST from 224 to 96. What influence does this have on the experiments?
4. Refer to Table 1 in the VGG paper (Simonyan & Zisserman, 2014) to construct other common models, such as VGG-16 or VGG-19.
Discussions96
96 https://discuss.d2l.ai/t/77

7.2. Networks Using Blocks (VGG)

269

7.3 Network in Network (NiN)
LeNet, AlexNet, and VGG all share a common design pattern: extract features exploiting spatial structure via a sequence of convolution and pooling layers and then post-process the representations via fully-connected layers. The improvements upon LeNet by AlexNet and VGG mainly lie in how these later networks widen and deepen these two modules. Alternatively, one could imagine using fully-connected layers earlier in the process. However, a careless use of dense layers might give up the spatial structure of the representation entirely, network in network (NiN) blocks offer an alternative. They were proposed based on a very simple insight: to use an MLP on the channels for each pixel separately (Lin et al., 2013).
7.3.1 NiN Blocks
Recall that the inputs and outputs of convolutional layers consist of four-dimensional tensors with axes corresponding to the example, channel, height, and width. Also recall that the inputs and outputs of fully-connected layers are typically two-dimensional tensors corresponding to the example and feature. The idea behind NiN is to apply a fully-connected layer at each pixel location (for each height and width). If we tie the weights across each spatial location, we could think of this as a 1 × 1 convolutional layer (as described in Section 6.4) or as a fully-connected layer acting independently on each pixel location. Another way to view this is to think of each element in the spatial dimension (height and width) as equivalent to an example and a channel as equivalent to a feature.
Fig. 7.3.1 illustrates the main structural differences between VGG and NiN, and their blocks. The NiN block consists of one convolutional layer followed by two 1 × 1 convolutional layers that act as per-pixel fully-connected layers with ReLU activations. The convolution window shape of the first layer is typically set by the user. The subsequent window shapes are fixed to 1 × 1.

270

Chapter 7. Modern Convolutional Neural Networks

Fig. 7.3.1: Comparing architectures of VGG and NiN, and their blocks.
from mxnet import np, npx from mxnet.gluon import nn from d2l import mxnet as d2l
npx.set_np()
def nin_block(num_channels, kernel_size, strides, padding): blk = nn.Sequential() blk.add( nn.Conv2D(num_channels, kernel_size, strides, padding, activation='relu'), nn.Conv2D(num_channels, kernel_size=1, activation='relu'), nn.Conv2D(num_channels, kernel_size=1, activation='relu')) return blk

7.3. Network in Network (NiN)

271

7.3.2 NiN Model
The original NiN network was proposed shortly after AlexNet and clearly draws some inspiration. NiN uses convolutional layers with window shapes of 11×11, 5×5, and 3×3, and the corresponding numbers of output channels are the same as in AlexNet. Each NiN block is followed by a maximum pooling layer with a stride of 2 and a window shape of 3 × 3.
One significant difference between NiN and AlexNet is that NiN avoids fully-connected layers altogether. Instead, NiN uses an NiN block with a number of output channels equal to the number of label classes, followed by a global average pooling layer, yielding a vector of logits. One advantage of NiNʼs design is that it significantly reduces the number of required model parameters. However, in practice, this design sometimes requires increased model training time.
net = nn.Sequential() net.add(
nin_block(96, kernel_size=11, strides=4, padding=0), nn.MaxPool2D(pool_size=3, strides=2), nin_block(256, kernel_size=5, strides=1, padding=2), nn.MaxPool2D(pool_size=3, strides=2), nin_block(384, kernel_size=3, strides=1, padding=1), nn.MaxPool2D(pool_size=3, strides=2), nn.Dropout(0.5), # There are 10 label classes nin_block(10, kernel_size=3, strides=1, padding=1), # The global average pooling layer automatically sets the window shape # to the height and width of the input nn.GlobalAvgPool2D(), # Transform the four-dimensional output into two-dimensional output # with a shape of (batch size, 10) nn.Flatten())
We create a data example to see the output shape of each block.
X = np.random.uniform(size=(1, 1, 224, 224)) net.initialize() for layer in net:
X = layer(X) print(layer.name, 'output shape:\t', X.shape)

sequential1 output shape: (1, 96, 54, 54)

pool0 output shape: (1, 96, 26, 26)

sequential2 output shape: (1, 256, 26, 26)

pool1 output shape: (1, 256, 12, 12)

sequential3 output shape: (1, 384, 12, 12)

pool2 output shape: (1, 384, 5, 5)

dropout0 output shape:

(1, 384, 5, 5)

sequential4 output shape: (1, 10, 5, 5)

pool3 output shape: (1, 10, 1, 1)

flatten0 output shape:

(1, 10)

272

Chapter 7. Modern Convolutional Neural Networks

7.3.3 Training
As before we use Fashion-MNIST to train the model. NiNʼs training is similar to that for AlexNet and VGG.
lr, num_epochs, batch_size = 0.1, 10, 128 train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=224) d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())
loss 0.989, train acc 0.649, test acc 0.650 2926.8 examples/sec on gpu(0)

Summary
• NiN uses blocks consisting of a convolutional layer and multiple 1 × 1 convolutional layers. This can be used within the convolutional stack to allow for more per-pixel nonlinearity.
• NiN removes the fully-connected layers and replaces them with global average pooling (i.e., summing over all locations) after reducing the number of channels to the desired number of outputs (e.g., 10 for Fashion-MNIST).
• Removing the fully-connected layers reduces overfitting. NiN has dramatically fewer parameters.
• The NiN design influenced many subsequent CNN designs.

Exercises
1. Tune the hyperparameters to improve the classification accuracy. 2. Why are there two 1 × 1 convolutional layers in the NiN block? Remove one of them, and
then observe and analyze the experimental phenomena. 3. Calculate the resource usage for NiN.
1. What is the number of parameters? 2. What is the amount of computation?

7.3. Network in Network (NiN)

273

3. What is the amount of memory needed during training? 4. What is the amount of memory needed during prediction? 4. What are possible problems with reducing the 384 × 5 × 5 representation to a 10 × 5 × 5 representation in one step? Discussions97
7.4 Networks with Parallel Concatenations (GoogLeNet)
In 2014, GoogLeNet won the ImageNet Challenge, proposing a structure that combined the strengths of NiN and paradigms of repeated blocks (Szegedy et al., 2015). One focus of the paper was to address the question of which sized convolution kernels are best. After all, previous popular networks employed choices as small as 1 × 1 and as large as 11 × 11. One insight in this paper was that sometimes it can be advantageous to employ a combination of variously-sized kernels. In this section, we will introduce GoogLeNet, presenting a slightly simplified version of the original model: we omit a few ad-hoc features that were added to stabilize training but are unnecessary now with better training algorithms available.
7.4.1 Inception Blocks
The basic convolutional block in GoogLeNet is called an Inception block, likely named due to a quote from the movie Inception (“We need to go deeper”), which launched a viral meme.

Fig. 7.4.1: Structure of the Inception block.
As depicted in Fig. 7.4.1, the inception block consists of four parallel paths. The first three paths use convolutional layers with window sizes of 1 × 1, 3 × 3, and 5 × 5 to extract information from different spatial sizes. The middle two paths perform a 1 × 1 convolution on the input to reduce the number of channels, reducing the modelʼs complexity. The fourth path uses a 3 × 3 maximum pooling layer, followed by a 1 × 1 convolutional layer to change the number of channels. The four paths all use appropriate padding to give the input and output the same height and width. Finally, the outputs along each path are concatenated along the channel dimension and comprise the blockʼs output. The commonly-tuned hyperparameters of the Inception block are the number of output channels per layer.
97 https://discuss.d2l.ai/t/79

274

Chapter 7. Modern Convolutional Neural Networks

from mxnet import np, npx from mxnet.gluon import nn from d2l import mxnet as d2l
npx.set_np()
class Inception(nn.Block): # `c1`--`c4` are the number of output channels for each path def __init__(self, c1, c2, c3, c4, **kwargs): super(Inception, self).__init__(**kwargs) # Path 1 is a single 1 x 1 convolutional layer self.p1_1 = nn.Conv2D(c1, kernel_size=1, activation='relu') # Path 2 is a 1 x 1 convolutional layer followed by a 3 x 3 # convolutional layer self.p2_1 = nn.Conv2D(c2[0], kernel_size=1, activation='relu') self.p2_2 = nn.Conv2D(c2[1], kernel_size=3, padding=1, activation='relu') # Path 3 is a 1 x 1 convolutional layer followed by a 5 x 5 # convolutional layer self.p3_1 = nn.Conv2D(c3[0], kernel_size=1, activation='relu') self.p3_2 = nn.Conv2D(c3[1], kernel_size=5, padding=2, activation='relu') # Path 4 is a 3 x 3 maximum pooling layer followed by a 1 x 1 # convolutional layer self.p4_1 = nn.MaxPool2D(pool_size=3, strides=1, padding=1) self.p4_2 = nn.Conv2D(c4, kernel_size=1, activation='relu')
def forward(self, x): p1 = self.p1_1(x) p2 = self.p2_2(self.p2_1(x)) p3 = self.p3_2(self.p3_1(x)) p4 = self.p4_2(self.p4_1(x)) # Concatenate the outputs on the channel dimension return np.concatenate((p1, p2, p3, p4), axis=1)
To gain some intuition for why this network works so well, consider the combination of the filters. They explore the image in a variety of filter sizes. This means that details at different extents can be recognized efficiently by filters of different sizes. At the same time, we can allocate different amounts of parameters for different filters.
7.4.2 GoogLeNet Model
As shown in Fig. 7.4.2, GoogLeNet uses a stack of a total of 9 inception blocks and global average pooling to generate its estimates. Maximum pooling between inception blocks reduces the dimensionality. The first module is similar to AlexNet and LeNet. The stack of blocks is inherited from VGG and the global average pooling avoids a stack of fully-connected layers at the end.

7.4. Networks with Parallel Concatenations (GoogLeNet)

275

Fig. 7.4.2: The GoogLeNet architecture.

We can now implement GoogLeNet piece by piece. The first module uses a 64-channel 7 × 7 convolutional layer.
b1 = nn.Sequential() b1.add(nn.Conv2D(64, kernel_size=7, strides=2, padding=3, activation='relu'),
nn.MaxPool2D(pool_size=3, strides=2, padding=1))

The second module uses two convolutional layers: first, a 64-channel 1 × 1 convolutional layer, then a 3 × 3 convolutional layer that triples the number of channels. This corresponds to the second path in the Inception block.
b2 = nn.Sequential() b2.add(nn.Conv2D(64, kernel_size=1, activation='relu'),
nn.Conv2D(192, kernel_size=3, padding=1, activation='relu'), nn.MaxPool2D(pool_size=3, strides=2, padding=1))

The third module connects two complete Inception blocks in series. The number of output channels of the first Inception block is 64 + 128 + 32 + 32 = 256, and the number-of-output-channel ratio among the four paths is 64 : 128 : 32 : 32 = 2 : 4 : 1 : 1. The second and third paths first reduce the number of input channels to 96/192 = 1/2 and 16/192 = 1/12, respectively, and then connect the second convolutional layer. The number of output channels of the second Inception block is increased to 128 + 192 + 96 + 64 = 480, and the number-of-output-channel ratio among the four paths is 128 : 192 : 96 : 64 = 4 : 6 : 3 : 2. The second and third paths first reduce the number of input channels to 128/256 = 1/2 and 32/256 = 1/8, respectively.

276

Chapter 7. Modern Convolutional Neural Networks

b3 = nn.Sequential() b3.add(Inception(64, (96, 128), (16, 32), 32),
Inception(128, (128, 192), (32, 96), 64), nn.MaxPool2D(pool_size=3, strides=2, padding=1))
The fourth module is more complicated. It connects five Inception blocks in series, and they have 192+208+48+64 = 512, 160+224+64+64 = 512, 128+256+64+64 = 512, 112+288+64+64 = 528, and 256+320+128+128 = 832 output channels, respectively. The number of channels assigned to these paths is similar to that in the third module: the second path with the 3×3 convolutional layer outputs the largest number of channels, followed by the first path with only the 1×1 convolutional layer, the third path with the 5×5 convolutional layer, and the fourth path with the 3×3 maximum pooling layer. The second and third paths will first reduce the number of channels according to the ratio. These ratios are slightly different in different Inception blocks.
b4 = nn.Sequential() b4.add(Inception(192, (96, 208), (16, 48), 64),
Inception(160, (112, 224), (24, 64), 64), Inception(128, (128, 256), (24, 64), 64), Inception(112, (144, 288), (32, 64), 64), Inception(256, (160, 320), (32, 128), 128), nn.MaxPool2D(pool_size=3, strides=2, padding=1))
The fifth module has two Inception blocks with 256 + 320 + 128 + 128 = 832 and 384 + 384 + 128 + 128 = 1024 output channels. The number of channels assigned to each path is the same as that in the third and fourth modules, but differs in specific values. It should be noted that the fifth block is followed by the output layer. This block uses the global average pooling layer to change the height and width of each channel to 1, just as in NiN. Finally, we turn the output into a two-dimensional array followed by a fully-connected layer whose number of outputs is the number of label classes.
b5 = nn.Sequential() b5.add(Inception(256, (160, 320), (32, 128), 128),
Inception(384, (192, 384), (48, 128), 128), nn.GlobalAvgPool2D())
net = nn.Sequential() net.add(b1, b2, b3, b4, b5, nn.Dense(10))
The GoogLeNet model is computationally complex, so it is not as easy to modify the number of channels as in VGG. To have a reasonable training time on Fashion-MNIST, we reduce the input height and width from 224 to 96. This simplifies the computation. The changes in the shape of the output between the various modules are demonstrated below.
X = np.random.uniform(size=(1, 1, 96, 96)) net.initialize() for layer in net:
X = layer(X) print(layer.name, 'output shape:\t', X.shape)

sequential0 output shape: sequential1 output shape: sequential2 output shape: sequential3 output shape:

(1, 64, 24, 24) (1, 192, 12, 12) (1, 480, 6, 6) (1, 832, 3, 3)

(continues on next page)

7.4. Networks with Parallel Concatenations (GoogLeNet)

277

sequential4 output shape: dense0 output shape:

(1, 1024, 1, 1) (1, 10)

(continued from previous page)

7.4.3 Training
As before, we train our model using the Fashion-MNIST dataset. We transform it to 96 × 96 pixel resolution before invoking the training procedure.
lr, num_epochs, batch_size = 0.1, 10, 128 train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=96) d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())
loss 0.889, train acc 0.673, test acc 0.800 2046.3 examples/sec on gpu(0)

Summary
• The Inception block is equivalent to a subnetwork with four paths. It extracts information in parallel through convolutional layers of different window shapes and maximum pooling layers. 1 × 1 convolutions reduce channel dimensionality on a per-pixel level. Maximum pooling reduces the resolution.
• GoogLeNet connects multiple well-designed Inception blocks with other layers in series. The ratio of the number of channels assigned in the Inception block is obtained through a large number of experiments on the ImageNet dataset.
• GoogLeNet, as well as its succeeding versions, was one of the most efficient models on ImageNet, providing similar test accuracy with lower computational complexity.

278

Chapter 7. Modern Convolutional Neural Networks

Exercises
1. There are several iterations of GoogLeNet. Try to implement and run them. Some of them include the following:
• Add a batch normalization layer (Ioffe & Szegedy, 2015), as described later in Section 7.5.
• Make adjustments to the Inception block (Szegedy et al., 2016).
• Use label smoothing for model regularization (Szegedy et al., 2016).
• Include it in the residual connection (Szegedy et al., 2017), as described later in Section 7.6.
2. What is the minimum image size for GoogLeNet to work?
3. Compare the model parameter sizes of AlexNet, VGG, and NiN with GoogLeNet. How do the latter two network architectures significantly reduce the model parameter size?
Discussions98
7.5 Batch Normalization
Training deep neural networks is difficult. And getting them to converge in a reasonable amount of time can be tricky. In this section, we describe batch normalization, a popular and effective technique that consistently accelerates the convergence of deep networks (Ioffe & Szegedy, 2015). Together with residual blocks—covered later in Section 7.6—batch normalization has made it possible for practitioners to routinely train networks with over 100 layers.
7.5.1 Training Deep Networks
To motivate batch normalization, let us review a few practical challenges that arise when training machine learning models and neural networks in particular.
First, choices regarding data preprocessing often make an enormous difference in the final results. Recall our application of MLPs to predicting house prices (Section 4.10). Our first step when working with real data was to standardize our input features to each have a mean of zero and variance of one. Intuitively, this standardization plays nicely with our optimizers because it puts the parameters a priori at a similar scale.
Second, for a typical MLP or CNN, as we train, the variables (e.g., affine transformation outputs in MLP) in intermediate layers may take values with widely varying magnitudes: both along the layers from the input to the output, across units in the same layer, and over time due to our updates to the model parameters. The inventors of batch normalization postulated informally that this drift in the distribution of such variables could hamper the convergence of the network. Intuitively, we might conjecture that if one layer has variable values that are 100 times that of another layer, this might necessitate compensatory adjustments in the learning rates.
Third, deeper networks are complex and easily capable of overfitting. This means that regularization becomes more critical.
98 https://discuss.d2l.ai/t/81

7.5. Batch Normalization

279

Batch normalization is applied to individual layers (optionally, to all of them) and works as follows: In each training iteration, we first normalize the inputs (of batch normalization) by subtracting their mean and dividing by their standard deviation, where both are estimated based on the statistics of the current minibatch. Next, we apply a scale coefficient and a scale offset. It is precisely due to this normalization based on batch statistics that batch normalization derives its name.

Note that if we tried to apply batch normalization with minibatches of size 1, we would not be able to learn anything. That is because after subtracting the means, each hidden unit would take value 0! As you might guess, since we are devoting a whole section to batch normalization, with large enough minibatches, the approach proves effective and stable. One takeaway here is that when applying batch normalization, the choice of batch size may be even more significant than without batch normalization.

Formally, denoting by x ∈ B an input to batch normalization (BN) that is from a minibatch B, batch normalization transforms x according to the following expression:

BN(x) = γ ⊙ x − µˆ B + β. σˆ B

(7.5.1)

In (7.5.1), µˆ B is the sample mean and σˆ B is the sample standard deviation of the minibatch B. After applying standardization, the resulting minibatch has zero mean and unit variance. Because the choice of unit variance (vs. some other magic number) is an arbitrary choice, we commonly include elementwise scale parameter γ and shift parameter β that have the same shape as x. Note that γ and β are parameters that need to be learned jointly with the other model parameters.

Consequently, the variable magnitudes for intermediate layers cannot diverge during training be-
cause batch normalization actively centers and rescales them back to a given mean and size (via
µˆ B and σˆ B). One piece of practitionerʼs intuition or wisdom is that batch normalization seems to allow for more aggressive learning rates.

Formally, we calculate µˆ B and σˆ B in (7.5.1) as follows: 1∑
µˆ B = |B| x,
x∈B
σˆ 2B = |B1| ∑(x − µˆ B)2 + ϵ.
x∈B

(7.5.2)

Note that we add a small constant ϵ > 0 to the variance estimate to ensure that we never attempt
division by zero, even in cases where the empirical variance estimate might vanish. The estimates
µˆ B and σˆ B counteract the scaling issue by using noisy estimates of mean and variance. You might think that this noisiness should be a problem. As it turns out, this is actually beneficial.

This turns out to be a recurring theme in deep learning. For reasons that are not yet wellcharacterized theoretically, various sources of noise in optimization often lead to faster training and less overfitting: this variation appears to act as a form of regularization. In some preliminary research, (Teye et al., 2018) and (Luo et al., 2018) relate the properties of batch normalization to Bayesian priors and penalties respectively. In particular, this sheds some light on the puzzle of why batch normalization works best for moderate minibatches sizes in the 50 ∼ 100 range.

Fixing a trained model, you might think that we would prefer using the entire dataset to estimate the mean and variance. Once training is complete, why would we want the same image to be classified differently, depending on the batch in which it happens to reside? During training, such exact calculation is infeasible because the intermediate variables for all data examples change every time we update our model. However, once the model is trained, we can calculate the means and variances of each layerʼs variables based on the entire dataset. Indeed this is standard practice

280

Chapter 7. Modern Convolutional Neural Networks

for models employing batch normalization and thus batch normalization layers function differently in training mode (normalizing by minibatch statistics) and in prediction mode (normalizing by dataset statistics).
We are now ready to take a look at how batch normalization works in practice.

7.5.2 Batch Normalization Layers
Batch normalization implementations for fully-connected layers and convolutional layers are slightly different. We discuss both cases below. Recall that one key differences between batch normalization and other layers is that because batch normalization operates on a full minibatch at a time, we cannot just ignore the batch dimension as we did before when introducing other layers.

Fully-Connected Layers

When applying batch normalization to fully-connected layers, the original paper inserts batch normalization after the affine transformation and before the nonlinear activation function (later applications may insert batch normalization right after activation functions) (Ioffe & Szegedy, 2015). Denoting the input to the fully-connected layer by x, the affine transformation by Wx + b (with the weight parameter W and the bias parameter b), and the activation function by ϕ, we can express the computation of a batch-normalization-enabled, fully-connected layer output h as follows:

h = ϕ(BN(Wx + b)).

(7.5.3)

Recall that mean and variance are computed on the same minibatch on which the transformation is applied.

Convolutional Layers
Similarly, with convolutional layers, we can apply batch normalization after the convolution and before the nonlinear activation function. When the convolution has multiple output channels, we need to carry out batch normalization for each of the outputs of these channels, and each channel has its own scale and shift parameters, both of which are scalars. Assume that our minibatches contain m examples and that for each channel, the output of the convolution has height p and width q. For convolutional layers, we carry out each batch normalization over the m ·p ·q elements per output channel simultaneously. Thus, we collect the values over all spatial locations when computing the mean and variance and consequently apply the same mean and variance within a given channel to normalize the value at each spatial location.

7.5. Batch Normalization

281

Batch Normalization During Prediction
As we mentioned earlier, batch normalization typically behaves differently in training mode and prediction mode. First, the noise in the sample mean and the sample variance arising from estimating each on minibatches are no longer desirable once we have trained the model. Second, we might not have the luxury of computing per-batch normalization statistics. For example, we might need to apply our model to make one prediction at a time.
Typically, after training, we use the entire dataset to compute stable estimates of the variable statistics and then fix them at prediction time. Consequently, batch normalization behaves differently during training and at test time. Recall that dropout also exhibits this characteristic.
7.5.3 Implementation from Scratch
Below, we implement a batch normalization layer with tensors from scratch.
from mxnet import autograd, init, np, npx from mxnet.gluon import nn from d2l import mxnet as d2l
npx.set_np()
def batch_norm(X, gamma, beta, moving_mean, moving_var, eps, momentum): # Use `autograd` to determine whether the current mode is training mode or # prediction mode if not autograd.is_training(): # If it is prediction mode, directly use the mean and variance # obtained by moving average X_hat = (X - moving_mean) / np.sqrt(moving_var + eps) else: assert len(X.shape) in (2, 4) if len(X.shape) == 2: # When using a fully-connected layer, calculate the mean and # variance on the feature dimension mean = X.mean(axis=0) var = ((X - mean)**2).mean(axis=0) else: # When using a two-dimensional convolutional layer, calculate the # mean and variance on the channel dimension (axis=1). Here we # need to maintain the shape of `X`, so that the broadcasting # operation can be carried out later mean = X.mean(axis=(0, 2, 3), keepdims=True) var = ((X - mean)**2).mean(axis=(0, 2, 3), keepdims=True) # In training mode, the current mean and variance are used for the # standardization X_hat = (X - mean) / np.sqrt(var + eps) # Update the mean and variance using moving average moving_mean = momentum * moving_mean + (1.0 - momentum) * mean moving_var = momentum * moving_var + (1.0 - momentum) * var Y = gamma * X_hat + beta # Scale and shift return Y, moving_mean, moving_var
We can now create a proper BatchNorm layer. Our layer will maintain proper parameters for scale gamma and shift beta, both of which will be updated in the course of training. Additionally, our

282

Chapter 7. Modern Convolutional Neural Networks

layer will maintain moving averages of the means and variances for subsequent use during model prediction.
Putting aside the algorithmic details, note the design pattern underlying our implementation of the layer. Typically, we define the mathematics in a separate function, say batch_norm. We then integrate this functionality into a custom layer, whose code mostly addresses bookkeeping matters, such as moving data to the right device context, allocating and initializing any required variables, keeping track of moving averages (here for mean and variance), and so on. This pattern enables a clean separation of mathematics from boilerplate code. Also note that for the sake of convenience we did not worry about automatically inferring the input shape here, thus we need to specify the number of features throughout. Do not worry, the high-level batch normalization APIs in the deep learning framework will care of this for us and we will demonstrate that later.
class BatchNorm(nn.Block): # `num_features`: the number of outputs for a fully-connected layer # or the number of output channels for a convolutional layer. `num_dims`: # 2 for a fully-connected layer and 4 for a convolutional layer def __init__(self, num_features, num_dims, **kwargs): super().__init__(**kwargs) if num_dims == 2: shape = (1, num_features) else: shape = (1, num_features, 1, 1) # The scale parameter and the shift parameter (model parameters) are # initialized to 1 and 0, respectively self.gamma = self.params.get('gamma', shape=shape, init=init.One()) self.beta = self.params.get('beta', shape=shape, init=init.Zero()) # The variables that are not model parameters are initialized to 0 and 1 self.moving_mean = np.zeros(shape) self.moving_var = np.ones(shape)
def forward(self, X): # If `X` is not on the main memory, copy `moving_mean` and # `moving_var` to the device where `X` is located if self.moving_mean.ctx != X.ctx: self.moving_mean = self.moving_mean.copyto(X.ctx) self.moving_var = self.moving_var.copyto(X.ctx) # Save the updated `moving_mean` and `moving_var` Y, self.moving_mean, self.moving_var = batch_norm( X, self.gamma.data(), self.beta.data(), self.moving_mean, self.moving_var, eps=1e-12, momentum=0.9) return Y

7.5.4 Applying Batch Normalization in LeNet

To see how to apply BatchNorm in context, below we apply it to a traditional LeNet model (Section 6.6). Recall that batch normalization is applied after the convolutional layers or fully-connected layers but before the corresponding activation functions.

net = nn.Sequential() net.add(nn.Conv2D(6, kernel_size=5), BatchNorm(6, num_dims=4),
nn.Activation('sigmoid'), nn.AvgPool2D(pool_size=2, strides=2), nn.Conv2D(16, kernel_size=5), BatchNorm(16, num_dims=4),

(continues on next page)

7.5. Batch Normalization

283

(continued from previous page) nn.Activation('sigmoid'), nn.AvgPool2D(pool_size=2, strides=2), nn.Dense(120), BatchNorm(120, num_dims=2), nn.Activation('sigmoid'), nn.Dense(84), BatchNorm(84, num_dims=2), nn.Activation('sigmoid'), nn.Dense(10))
As before, we will train our network on the Fashion-MNIST dataset. This code is virtually identical to that when we first trained LeNet (Section 6.6). The main difference is the larger learning rate.
lr, num_epochs, batch_size = 1.0, 10, 256 train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size) d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())
loss 0.271, train acc 0.901, test acc 0.856 15785.6 examples/sec on gpu(0)

Let us have a look at the scale parameter gamma and the shift parameter beta learned from the first batch normalization layer.
net[1].gamma.data().reshape(-1,), net[1].beta.data().reshape(-1,)
(array([0.3476513, 2.3587034, 3.7174723, 2.4445746, 3.3097095, 2.2687821], ctx=gpu(0)), array([ 0.27111825, 2.7037778 , -2.6709592 , 0.972922 , -0.1964196 ,
2.543834 ], ctx=gpu(0)))

7.5.5 Concise Implementation
Compared with the BatchNorm class, which we just defined ourselves, we can use the BatchNorm class defined in high-level APIs from the deep learning framework directly. The code looks virtually identical to our implementation above.
net = nn.Sequential() net.add(nn.Conv2D(6, kernel_size=5), nn.BatchNorm(), nn.Activation('sigmoid'),
(continues on next page)

284

Chapter 7. Modern Convolutional Neural Networks

(continued from previous page) nn.AvgPool2D(pool_size=2, strides=2), nn.Conv2D(16, kernel_size=5), nn.BatchNorm(), nn.Activation('sigmoid'), nn.AvgPool2D(pool_size=2, strides=2), nn.Dense(120), nn.BatchNorm(), nn.Activation('sigmoid'), nn.Dense(84), nn.BatchNorm(), nn.Activation('sigmoid'), nn.Dense(10))
Below, we use the same hyperparameters to train our model. Note that as usual, the high-level API variant runs much faster because its code has been compiled to C++ or CUDA while our custom implementation must be interpreted by Python.
d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())
loss 0.274, train acc 0.898, test acc 0.865 32031.0 examples/sec on gpu(0)

7.5.6 Controversy
Intuitively, batch normalization is thought to make the optimization landscape smoother. However, we must be careful to distinguish between speculative intuitions and true explanations for the phenomena that we observe when training deep models. Recall that we do not even know why simpler deep neural networks (MLPs and conventional CNNs) generalize well in the first place. Even with dropout and weight decay, they remain so flexible that their ability to generalize to unseen data cannot be explained via conventional learning-theoretic generalization guarantees.
In the original paper proposing batch normalization, the authors, in addition to introducing a powerful and useful tool, offered an explanation for why it works: by reducing internal covariate shift. Presumably by internal covariate shift the authors meant something like the intuition expressed above—the notion that the distribution of variable values changes over the course of training. However, there were two problems with this explanation: i) This drift is very different from covariate shift, rendering the name a misnomer. ii) The explanation offers an under-specified intuition but leaves the question of why precisely this technique works an open question wanting for a rigorous explanation. Throughout this book, we aim to convey the intuitions that practitioners use to guide their development of deep neural networks. However, we believe that it is important to separate these guiding intuitions from established scientific fact. Eventually, when you master

7.5. Batch Normalization

285

this material and start writing your own research papers you will want to be clear to delineate between technical claims and hunches.
Following the success of batch normalization, its explanation in terms of internal covariate shift has repeatedly surfaced in debates in the technical literature and broader discourse about how to present machine learning research. In a memorable speech given while accepting a Test of Time Award at the 2017 NeurIPS conference, Ali Rahimi used internal covariate shift as a focal point in an argument likening the modern practice of deep learning to alchemy. Subsequently, the example was revisited in detail in a position paper outlining troubling trends in machine learning (Lipton & Steinhardt, 2018). Other authors have proposed alternative explanations for the success of batch normalization, some claiming that batch normalizationʼs success comes despite exhibiting behavior that is in some ways opposite to those claimed in the original paper (Santurkar et al., 2018).
We note that the internal covariate shift is no more worthy of criticism than any of thousands of similarly vague claims made every year in the technical machine learning literature. Likely, its resonance as a focal point of these debates owes to its broad recognizability to the target audience. Batch normalization has proven an indispensable method, applied in nearly all deployed image classifiers, earning the paper that introduced the technique tens of thousands of citations.
Summary
• During model training, batch normalization continuously adjusts the intermediate output of the neural network by utilizing the mean and standard deviation of the minibatch, so that the values of the intermediate output in each layer throughout the neural network are more stable.
• The batch normalization methods for fully-connected layers and convolutional layers are slightly different.
• Like a dropout layer, batch normalization layers have different computation results in training mode and prediction mode.
• Batch normalization has many beneficial side effects, primarily that of regularization. On the other hand, the original motivation of reducing internal covariate shift seems not to be a valid explanation.
Exercises
1. Can we remove the bias parameter from the fully-connected layer or the convolutional layer before the batch normalization? Why?
2. Compare the learning rates for LeNet with and without batch normalization.
1. Plot the increase in training and test accuracy.
2. How large can you make the learning rate?
3. Do we need batch normalization in every layer? Experiment with it?
4. Can you replace dropout by batch normalization? How does the behavior change?
5. Fix the parameters beta and gamma, and observe and analyze the results.

286

Chapter 7. Modern Convolutional Neural Networks

6. Review the online documentation for BatchNorm from the high-level APIs to see the other applications for batch normalization.
7. Research ideas: think of other normalization transforms that you can apply? Can you apply the probability integral transform? How about a full rank covariance estimate?
Discussions99

7.6 Residual Networks (ResNet)
As we design increasingly deeper networks it becomes imperative to understand how adding layers can increase the complexity and expressiveness of the network. Even more important is the ability to design networks where adding layers makes networks strictly more expressive rather than just different. To make some progress we need a bit of mathematics.

7.6.1 Function Classes

Consider F, the class of functions that a specific network architecture (together with learning
rates and other hyperparameter settings) can reach. That is, for all f ∈ F there exists some set of
parameters (e.g., weights and biases) that can be obtained through training on a suitable dataset. Let us assume that f ∗ is the “truth” function that we really would like to find. If it is in F, we are in good shape but typically we will not be quite so lucky. Instead, we will try to find some fF∗ which is our best bet within F. For instance, given a dataset with features X and labels y, we might try
finding it by solving the following optimization problem:

fF∗ d=ef argmin L(X, y, f ) subject to f ∈ F .
f

(7.6.1)

It is only reasonable to assume that if we design a different and more powerful architecture F ′ we should arrive at a better outcome. In other words, we would expect that fF∗ ′ is “better” than fF∗ . However, if F ̸⊆ F ′ there is no guarantee that this should even happen. In fact, fF∗ ′ might well be worse. As illustrated by Fig. 7.6.1, for non-nested function classes, a larger function class does not always move closer to the “truth” function f ∗. For instance, on the left of Fig. 7.6.1, though F3 is closer to f ∗ than F1, F6 moves away and there is no guarantee that further increasing the complexity can reduce the distance from f ∗. With nested function classes where F1 ⊆ . . . ⊆ F6 on the right of Fig. 7.6.1, we can avoid the aforementioned issue from the non-nested function
classes.
99 https://discuss.d2l.ai/t/83

7.6. Residual Networks (ResNet)

287

Fig. 7.6.1: For non-nested function classes, a larger (indicated by area) function class does not guarantee to get closer to the “truth” function (f ∗). This does not happen in nested function classes.
Thus, only if larger function classes contain the smaller ones are we guaranteed that increasing them strictly increases the expressive power of the network. For deep neural networks, if we can train the newly-added layer into an identity function f (x) = x, the new model will be as effective as the original model. As the new model may get a better solution to fit the training dataset, the added layer might make it easier to reduce training errors.
This is the question that He et al. considered when working on very deep computer vision models (He et al., 2016a). At the heart of their proposed residual network (ResNet) is the idea that every additional layer should more easily contain the identity function as one of its elements. These considerations are rather profound but they led to a surprisingly simple solution, a residual block. With it, ResNet won the ImageNet Large Scale Visual Recognition Challenge in 2015. The design had a profound influence on how to build deep neural networks.
7.6.2 Residual Blocks
Let us focus on a local part of a neural network, as depicted in Fig. 7.6.2. Denote the input by x. We assume that the desired underlying mapping we want to obtain by learning is f (x), to be used as the input to the activation function on the top. On the left of Fig. 7.6.2, the portion within the dotted-line box must directly learn the mapping f (x). On the right, the portion within the dottedline box needs to learn the residual mapping f (x) − x, which is how the residual block derives its name. If the identity mapping f (x) = x is the desired underlying mapping, the residual mapping is easier to learn: we only need to push the weights and biases of the upper weight layer (e.g., fully-connected layer and convolutional layer) within the dotted-line box to zero. The right figure in Fig. 7.6.2 illustrates the residual block of ResNet, where the solid line carrying the layer input x to the addition operator is called a residual connection (or shortcut connection). With residual blocks, inputs can forward propagate faster through the residual connections across layers.

288

Chapter 7. Modern Convolutional Neural Networks

Fig. 7.6.2: A regular block (left) and a residual block (right).

ResNet follows VGGʼs full 3 × 3 convolutional layer design. The residual block has two 3 × 3 convolutional layers with the same number of output channels. Each convolutional layer is followed by a batch normalization layer and a ReLU activation function. Then, we skip these two convolution operations and add the input directly before the final ReLU activation function. This kind of design requires that the output of the two convolutional layers has to be of the same shape as the input, so that they can be added together. If we want to change the number of channels, we need to introduce an additional 1 × 1 convolutional layer to transform the input into the desired shape for the addition operation. Let us have a look at the code below.

from mxnet import np, npx from mxnet.gluon import nn from d2l import mxnet as d2l

npx.set_np()

class Residual(nn.Block): #@save """The Residual block of ResNet.""" def __init__(self, num_channels, use_1x1conv=False, strides=1, **kwargs): super().__init__(**kwargs) self.conv1 = nn.Conv2D(num_channels, kernel_size=3, padding=1, strides=strides) self.conv2 = nn.Conv2D(num_channels, kernel_size=3, padding=1) if use_1x1conv: self.conv3 = nn.Conv2D(num_channels, kernel_size=1, strides=strides) else: self.conv3 = None self.bn1 = nn.BatchNorm() self.bn2 = nn.BatchNorm()

def forward(self, X): Y = npx.relu(self.bn1(self.conv1(X))) Y = self.bn2(self.conv2(Y))

(continues on next page)

7.6. Residual Networks (ResNet)

289

if self.conv3: X = self.conv3(X)
return npx.relu(Y + X)

(continued from previous page)

This code generates two types of networks: one where we add the input to the output before applying the ReLU nonlinearity whenever use_1x1conv=False, and one where we adjust channels and resolution by means of a 1 × 1 convolution before adding. Fig. 7.6.3 illustrates this:

Fig. 7.6.3: ResNet block with and without 1 × 1 convolution.
Now let us look at a situation where the input and output are of the same shape. blk = Residual(3) blk.initialize() X = np.random.uniform(size=(4, 3, 6, 6)) blk(X).shape
(4, 3, 6, 6)
We also have the option to halve the output height and width while increasing the number of output channels. blk = Residual(6, use_1x1conv=True, strides=2) blk.initialize() blk(X).shape
(4, 6, 3, 3)

290

Chapter 7. Modern Convolutional Neural Networks

7.6.3 ResNet Model
The first two layers of ResNet are the same as those of the GoogLeNet we described before: the 7×7 convolutional layer with 64 output channels and a stride of 2 is followed by the 3×3 maximum pooling layer with a stride of 2. The difference is the batch normalization layer added after each convolutional layer in ResNet.
net = nn.Sequential() net.add(nn.Conv2D(64, kernel_size=7, strides=2, padding=3), nn.BatchNorm(),
nn.Activation('relu'), nn.MaxPool2D(pool_size=3, strides=2, padding=1))
GoogLeNet uses four modules made up of Inception blocks. However, ResNet uses four modules made up of residual blocks, each of which uses several residual blocks with the same number of output channels. The number of channels in the first module is the same as the number of input channels. Since a maximum pooling layer with a stride of 2 has already been used, it is not necessary to reduce the height and width. In the first residual block for each of the subsequent modules, the number of channels is doubled compared with that of the previous module, and the height and width are halved.
Now, we implement this module. Note that special processing has been performed on the first module.
def resnet_block(num_channels, num_residuals, first_block=False): blk = nn.Sequential() for i in range(num_residuals): if i == 0 and not first_block: blk.add(Residual(num_channels, use_1x1conv=True, strides=2)) else: blk.add(Residual(num_channels)) return blk
Then, we add all the modules to ResNet. Here, two residual blocks are used for each module.
net.add(resnet_block(64, 2, first_block=True), resnet_block(128, 2), resnet_block(256, 2), resnet_block(512, 2))
Finally, just like GoogLeNet, we add a global average pooling layer, followed by the fully-connected layer output.
net.add(nn.GlobalAvgPool2D(), nn.Dense(10))
There are 4 convolutional layers in each module (excluding the 1×1 convolutional layer). Together with the first 7 × 7 convolutional layer and the final fully-connected layer, there are 18 layers in total. Therefore, this model is commonly known as ResNet-18. By configuring different numbers of channels and residual blocks in the module, we can create different ResNet models, such as the deeper 152-layer ResNet-152. Although the main architecture of ResNet is similar to that of GoogLeNet, ResNetʼs structure is simpler and easier to modify. All these factors have resulted in the rapid and widespread use of ResNet. Fig. 7.6.4 depicts the full ResNet-18.

7.6. Residual Networks (ResNet)

291

Fig. 7.6.4: The ResNet-18 architecture.
Before training ResNet, let us observe how the input shape changes across different modules in ResNet. As in all the previous architectures, the resolution decreases while the number of channels increases up until the point where a global average pooling layer aggregates all features.
X = np.random.uniform(size=(1, 1, 224, 224)) net.initialize() for layer in net:
X = layer(X) print(layer.name, 'output shape:\t', X.shape)

292

Chapter 7. Modern Convolutional Neural Networks

conv5 output shape: (1, 64, 112, 112)

batchnorm4 output shape: (1, 64, 112, 112)

relu0 output shape: (1, 64, 112, 112)

pool0 output shape: (1, 64, 56, 56)

sequential1 output shape: (1, 64, 56, 56)

sequential2 output shape: (1, 128, 28, 28)

sequential3 output shape: (1, 256, 14, 14)

sequential4 output shape: (1, 512, 7, 7)

pool1 output shape: (1, 512, 1, 1)

dense0 output shape:

(1, 10)

7.6.4 Training
We train ResNet on the Fashion-MNIST dataset, just like before.
lr, num_epochs, batch_size = 0.05, 10, 256 train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=96) d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())
loss 0.013, train acc 0.997, test acc 0.924 4877.6 examples/sec on gpu(0)

Summary
• Nested function classes are desirable. Learning an additional layer in deep neural networks as an identity function (though this is an extreme case) should be made easy.
• The residual mapping can learn the identity function more easily, such as pushing parameters in the weight layer to zero.
• We can train an effective deep neural network by having residual blocks. Inputs can forward propagate faster through the residual connections across layers.
• ResNet had a major influence on the design of subsequent deep neural networks, both for convolutional and sequential nature.

7.6. Residual Networks (ResNet)

293

Exercises
1. What are the major differences between the Inception block in Fig. 7.4.1 and the residual block? After removing some paths in the Inception block, how are they related to each other?
2. Refer to Table 1 in the ResNet paper (He et al., 2016a) to implement different variants.
3. For deeper networks, ResNet introduces a “bottleneck” architecture to reduce model complexity. Try to implement it.
4. In subsequent versions of ResNet, the authors changed the “convolution, batch normalization, and activation” structure to the “batch normalization, activation, and convolution” structure. Make this improvement yourself. See Figure 1 in (He et al., 2016b) for details.
5. Why canʼt we just increase the complexity of functions without bound, even if the function classes are nested?
Discussions100

7.7 Densely Connected Networks (DenseNet)
ResNet significantly changed the view of how to parametrize the functions in deep networks. DenseNet (dense convolutional network) is to some extent the logical extension of this (Huang et al., 2017). To understand how to arrive at it, let us take a small detour to mathematics.

7.7.1 From ResNet to DenseNet

Recall the Taylor expansion for functions. For the point x = 0 it can be written as

f (x) = f (0) + f ′(0)x + f ′′(0) x2 + f ′′′(0) x3 + . . . .

2!

3!

(7.7.1)

The key point is that it decomposes a function into increasingly higher order terms. In a similar vein, ResNet decomposes functions into

f (x) = x + g(x).

(7.7.2)

That is, ResNet decomposes f into a simple linear term and a more complex nonlinear one. What if we want to capture (not necessarily add) information beyond two terms? One solution was DenseNet (Huang et al., 2017).

Fig. 7.7.1: The main difference between ResNet (left) and DenseNet (right) in cross-layer connections: use of addition and use of concatenation.
100 https://discuss.d2l.ai/t/85

294

Chapter 7. Modern Convolutional Neural Networks

As shown in Fig. 7.7.1, the key difference between ResNet and DenseNet is that in the latter case outputs are concatenated (denoted by [, ]) rather than added. As a result, we perform a mapping from x to its values after applying an increasingly complex sequence of functions:

x → [x, f1(x), f2([x, f1(x)]), f3([x, f1(x), f2([x, f1(x)])]), . . .] .

(7.7.3)

In the end, all these functions are combined in MLP to reduce the number of features again. In terms of implementation this is quite simple: rather than adding terms, we concatenate them. The name DenseNet arises from the fact that the dependency graph between variables becomes quite dense. The last layer of such a chain is densely connected to all previous layers. The dense connections are shown in Fig. 7.7.2.

Fig. 7.7.2: Dense connections in DenseNet.

The main components that compose a DenseNet are dense blocks and transition layers. The former define how the inputs and outputs are concatenated, while the latter control the number of channels so that it is not too large.

7.7.2 Dense Blocks
DenseNet uses the modified “batch normalization, activation, and convolution” structure of ResNet (see the exercise in Section 7.6). First, we implement this convolution block structure.
from mxnet import np, npx from mxnet.gluon import nn from d2l import mxnet as d2l
npx.set_np()
def conv_block(num_channels): blk = nn.Sequential() blk.add(nn.BatchNorm(), nn.Activation('relu'), nn.Conv2D(num_channels, kernel_size=3, padding=1)) return blk

A dense block consists of multiple convolution blocks, each using the same number of output channels. In the forward propagation, however, we concatenate the input and output of each convolution block on the channel dimension.

class DenseBlock(nn.Block): def __init__(self, num_convs, num_channels, **kwargs): super().__init__(**kwargs) self.net = nn.Sequential()

(continues on next page)

7.7. Densely Connected Networks (DenseNet)

295

for _ in range(num_convs): self.net.add(conv_block(num_channels))

(continued from previous page)

def forward(self, X): for blk in self.net: Y = blk(X) # Concatenate the input and output of each block on the channel # dimension X = np.concatenate((X, Y), axis=1) return X

In the following example, we define a DenseBlock instance with 2 convolution blocks of 10 output channels. When using an input with 3 channels, we will get an output with 3+2×10 = 23 channels. The number of convolution block channels controls the growth in the number of output channels relative to the number of input channels. This is also referred to as the growth rate.
blk = DenseBlock(2, 10) blk.initialize() X = np.random.uniform(size=(4, 3, 8, 8)) Y = blk(X) Y.shape

(4, 23, 8, 8)

7.7.3 Transition Layers
Since each dense block will increase the number of channels, adding too many of them will lead to an excessively complex model. A transition layer is used to control the complexity of the model. It reduces the number of channels by using the 1 × 1 convolutional layer and halves the height and width of the average pooling layer with a stride of 2, further reducing the complexity of the model.
def transition_block(num_channels): blk = nn.Sequential() blk.add(nn.BatchNorm(), nn.Activation('relu'), nn.Conv2D(num_channels, kernel_size=1), nn.AvgPool2D(pool_size=2, strides=2)) return blk
Apply a transition layer with 10 channels to the output of the dense block in the previous example. This reduces the number of output channels to 10, and halves the height and width.
blk = transition_block(10) blk.initialize() blk(Y).shape
(4, 10, 4, 4)

296

Chapter 7. Modern Convolutional Neural Networks

7.7.4 DenseNet Model
Next, we will construct a DenseNet model. DenseNet first uses the same single convolutional layer and maximum pooling layer as in ResNet.
net = nn.Sequential() net.add(nn.Conv2D(64, kernel_size=7, strides=2, padding=3), nn.BatchNorm(),
nn.Activation('relu'), nn.MaxPool2D(pool_size=3, strides=2, padding=1))
Then, similar to the four modules made up of residual blocks that ResNet uses, DenseNet uses four dense blocks. Similar to ResNet, we can set the number of convolutional layers used in each dense block. Here, we set it to 4, consistent with the ResNet-18 model in Section 7.6. Furthermore, we set the number of channels (i.e., growth rate) for the convolutional layers in the dense block to 32, so 128 channels will be added to each dense block.
In ResNet, the height and width are reduced between each module by a residual block with a stride of 2. Here, we use the transition layer to halve the height and width and halve the number of channels.
# `num_channels`: the current number of channels num_channels, growth_rate = 64, 32 num_convs_in_dense_blocks = [4, 4, 4, 4]
for i, num_convs in enumerate(num_convs_in_dense_blocks): net.add(DenseBlock(num_convs, growth_rate)) # This is the number of output channels in the previous dense block num_channels += num_convs * growth_rate # A transition layer that halves the number of channels is added between # the dense blocks if i != len(num_convs_in_dense_blocks) - 1: num_channels //= 2 net.add(transition_block(num_channels))
Similar to ResNet, a global pooling layer and a fully-connected layer are connected at the end to produce the output.
net.add(nn.BatchNorm(), nn.Activation('relu'), nn.GlobalAvgPool2D(), nn.Dense(10))
7.7.5 Training
Since we are using a deeper network here, in this section, we will reduce the input height and width from 224 to 96 to simplify the computation.
lr, num_epochs, batch_size = 0.1, 10, 256 train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=96) d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())
loss 0.140, train acc 0.949, test acc 0.885 4875.7 examples/sec on gpu(0)

7.7. Densely Connected Networks (DenseNet)

297

Summary
• In terms of cross-layer connections, unlike ResNet, where inputs and outputs are added together, DenseNet concatenates inputs and outputs on the channel dimension.
• The main components that compose DenseNet are dense blocks and transition layers. • We need to keep the dimensionality under control when composing the network by adding
transition layers that shrink the number of channels again.
Exercises
1. Why do we use average pooling rather than maximum pooling in the transition layer? 2. One of the advantages mentioned in the DenseNet paper is that its model parameters are
smaller than those of ResNet. Why is this the case? 3. One problem for which DenseNet has been criticized is its high memory consumption.
1. Is this really the case? Try to change the input shape to 224 × 224 to see the actual GPU memory consumption.
2. Can you think of an alternative means of reducing the memory consumption? How would you need to change the framework?
4. Implement the various DenseNet versions presented in Table 1 of the DenseNet paper (Huang et al., 2017).
5. Design an MLP-based model by applying the DenseNet idea. Apply it to the housing price prediction task in Section 4.10.
Discussions101

101 https://discuss.d2l.ai/t/87
298

Chapter 7. Modern Convolutional Neural Networks

8 | Recurrent Neural Networks
So far we encountered two types of data: tabular data and image data. For the latter we designed specialized layers to take advantage of the regularity in them. In other words, if we were to permute the pixels in an image, it would be much more difficult to reason about its content of something that would look much like the background of a test pattern in the times of analog TV.
Most importantly, so far we tacitly assumed that our data are all drawn from some distribution, and all the examples are independently and identically distributed (i.i.d.). Unfortunately, this is not true for most data. For instance, the words in this paragraph are written in sequence, and it would be quite difficult to decipher its meaning if they were permuted randomly. Likewise, image frames in a video, the audio signal in a conversation, and the browsing behavior on a website, all follow sequential order. It is thus reasonable to assume that specialized models for such data will do better at describing them.
Another issue arises from the fact that we might not only receive a sequence as an input but rather might be expected to continue the sequence. For instance, the task could be to continue the series 2, 4, 6, 8, 10, . . . This is quite common in time series analysis, to predict the stock market, the fever curve of a patient, or the acceleration needed for a race car. Again we want to have models that can handle such data.
In short, while CNNs can efficiently process spatial information, recurrent neural networks (RNNs) are designed to better handle sequential information. RNNs introduce state variables to store past information, together with the current inputs, to determine the current outputs.
Many of the examples for using recurrent networks are based on text data. Hence, we will emphasize language models in this chapter. After a more formal review of sequence data we introduce practical techniques for preprocessing text data. Next, we discuss basic concepts of a language model and use this discussion as the inspiration for the design of RNNs. In the end, we describe the gradient calculation method for RNNs to explore problems that may be encountered when training such networks.
8.1 Sequence Models
Imagine that you are watching movies on Netflix. As a good Netflix user, you decide to rate each of the movies religiously. After all, a good movie is a good movie, and you want to watch more of them, right? As it turns out, things are not quite so simple. Peopleʼs opinions on movies can change quite significantly over time. In fact, psychologists even have names for some of the effects:
• There is anchoring, based on someone elseʼs opinion. For instance, after the Oscar awards, ratings for the corresponding movie go up, even though it is still the same movie. This effect persists for a few months until the award is forgotten. It has been shown that the effect lifts rating by over half a point (Wu et al., 2017).
299

• There is the hedonic adaptation, where humans quickly adapt to accept an improved or a worsened situation as the new normal. For instance, after watching many good movies, the expectations that the next movie is equally good or better are high. Hence, even an average movie might be considered as bad after many great ones are watched.
• There is seasonality. Very few viewers like to watch a Santa Claus movie in August.
• In some cases, movies become unpopular due to the misbehaviors of directors or actors in the production.
• Some movies become cult movies, because they were almost comically bad. Plan 9 from Outer Space and Troll 2 achieved a high degree of notoriety for this reason.
In short, movie ratings are anything but stationary. Thus, using temporal dynamics led to more accurate movie recommendations (Koren, 2009). Of course, sequence data are not just about movie ratings. The following gives more illustrations.
• Many users have highly particular behavior when it comes to the time when they open apps. For instance, social media apps are much more popular after school with students. Stock market trading apps are more commonly used when the markets are open.
• It is much harder to predict tomorrowʼs stock prices than to fill in the blanks for a stock price we missed yesterday, even though both are just a matter of estimating one number. After all, foresight is so much harder than hindsight. In statistics, the former (predicting beyond the known observations) is called extrapolation whereas the latter (estimating between the existing observations) is called interpolation.
• Music, speech, text, and videos are all sequential in nature. If we were to permute them they would make little sense. The headline dog bites man is much less surprising than man bites dog, even though the words are identical.
• Earthquakes are strongly correlated, i.e., after a massive earthquake there are very likely several smaller aftershocks, much more so than without the strong quake. In fact, earthquakes are spatiotemporally correlated, i.e., the aftershocks typically occur within a short time span and in close proximity.
• Humans interact with each other in a sequential nature, as can be seen in Twitter fights, dance patterns, and debates.

300

Chapter 8. Recurrent Neural Networks

8.1.1 Statistical Tools
We need statistical tools and new deep neural network architectures to deal with sequence data. To keep things simple, we use the stock price (FTSE 100 index) illustrated in Fig. 8.1.1 as an example.

Fig. 8.1.1: FTSE 100 index over about 30 years.

Let us denote the prices by xt, i.e., at time step t ∈ Z+ we observe price xt. Note that for sequences in this text, t will typically be discrete and vary over integers or its subset. Suppose that a trader who wants to do well in the stock market on day t predicts xt via

xt ∼ P (xt | xt−1, . . . , x1).

(8.1.1)

Autoregressive Models
In order to achieve this, our trader could use a regression model such as the one that we trained in Section 3.3. There is just one major problem: the number of inputs, xt−1, . . . , x1 varies, depending on t. That is to say, the number increases with the amount of data that we encounter, and we will need an approximation to make this computationally tractable. Much of what follows in this chapter will revolve around how to estimate P (xt | xt−1, . . . , x1) efficiently. In a nutshell it boils down to two strategies as follows.
First, assume that the potentially rather long sequence xt−1, . . . , x1 is not really necessary. In this case we might content ourselves with some timespan of length τ and only use xt−1, . . . , xt−τ observations. The immediate benefit is that now the number of arguments is always the same, at least for t > τ . This allows us to train a deep network as indicated above. Such models will be called autoregressive models, as they quite literally perform regression on themselves.
The second strategy, shown in Fig. 8.1.2, is to keep some summary ht of the past observations, and at the same time update ht in addition to the prediction xˆt. This leads to models that estimate xt with xˆt = P (xt | ht) and moreover updates of the form ht = g(ht−1, xt−1). Since ht is never observed, these models are also called latent autoregressive models.

8.1. Sequence Models

301

Fig. 8.1.2: A latent autoregressive model.

Both cases raise the obvious question of how to generate training data. One typically uses historical observations to predict the next observation given the ones up to right now. Obviously we do not expect time to stand still. However, a common assumption is that while the specific values of xt might change, at least the dynamics of the sequence itself will not. This is reasonable, since novel dynamics are just that, novel and thus not predictable using data that we have so far. Statisticians call dynamics that do not change stationary. Regardless of what we do, we will thus get an estimate of the entire sequence via

∏T P (x1, . . . , xT ) = P (xt | xt−1, . . . , x1).
t=1

(8.1.2)

Note that the above considerations still hold if we deal with discrete objects, such as words, rather than continuous numbers. The only difference is that in such a situation we need to use a classifier rather than a regression model to estimate P (xt | xt−1, . . . , x1).

Markov Models

Recall the approximation that in an autoregressive model we use only xt−1, . . . , xt−τ instead of xt−1, . . . , x1 to estimate xt. Whenever this approximation is accurate we say that the sequence satisfies a Markov condition. In particular, if τ = 1, we have a first-order Markov model and P (x) is given by

∏T P (x1, . . . , xT ) = P (xt | xt−1) where P (x1 | x0) = P (x1).
t=1

(8.1.3)

Such models are particularly nice whenever xt assumes only a discrete value, since in this case

dynamic programming can be used to compute values along the chain exactly. For instance, we

can compute P (xt+1 | xt−1) efficiently:

∑

P (xt+1 | xt−1) = xt P (xt+1, xt, xt−1)

∑ P (xt−1)

= xt P (xt+1 | xt, xt−1)P (xt, xt−1)

∑

P (xt−1)

= P (xt+1 | xt)P (xt | xt−1)

(8.1.4)

xt

by using the fact that we only need to take into account a very short history of past observations: P (xt+1 | xt, xt−1) = P (xt+1 | xt). Going into details of dynamic programming is beyond the scope of this section. Control and reinforcement learning algorithms use such tools extensively.

302

Chapter 8. Recurrent Neural Networks

Causality

In principle, there is nothing wrong with unfolding P (x1, . . . , xT ) in reverse order. After all, by conditioning we can always write it via

∏1 P (x1, . . . , xT ) = P (xt | xt+1, . . . , xT ).
t=T

(8.1.5)

In fact, if we have a Markov model, we can obtain a reverse conditional probability distribution, too. In many cases, however, there exists a natural direction for the data, namely going forward in time. It is clear that future events cannot influence the past. Hence, if we change xt, we may be able to influence what happens for xt+1 going forward but not the converse. That is, if we change xt, the distribution over past events will not change. Consequently, it ought to be easier to explain P (xt+1 | xt) rather than P (xt | xt+1). For instance, it has been shown that in some cases we can find xt+1 = f (xt)+ϵ for some additive noise ϵ, whereas the converse is not true (Hoyer et al., 2009). This is great news, since it is typically the forward direction that we are interested in estimating. The book by Peters et al. has explained more on this topic (Peters et al., 2017a). We are barely scratching the surface of it.

8.1.2 Training
After reviewing so many statistical tools, let us try this out in practice. We begin by generating some data. To keep things simple we generate our sequence data by using a sine function with some additive noise for time steps 1, 2, . . . , 1000.
%matplotlib inline from mxnet import autograd, gluon, init, np, npx from mxnet.gluon import nn from d2l import mxnet as d2l
npx.set_np()

T = 1000 # Generate a total of 1000 points time = np.arange(1, T + 1, dtype=np.float32) x = np.sin(0.01 * time) + np.random.normal(0, 0.2, (T,)) d2l.plot(time, [x], 'time', 'x', xlim=[1, 1000], figsize=(6, 3))

8.1. Sequence Models

303

Next, we need to turn such a sequence into features and labels that our model can train on. Based on the embedding dimension τ we map the data into pairs yt = xt and xt = [xt−τ , . . . , xt−1]. The astute reader might have noticed that this gives us τ fewer data examples, since we do not have sufficient history for the first τ of them. A simple fix, in particular if the sequence is long, is to discard those few terms. Alternatively we could pad the sequence with zeros. Here we only use the first 600 feature-label pairs for training.
tau = 4 features = np.zeros((T - tau, tau)) for i in range(tau):
features[:, i] = x[i:T - tau + i] labels = x[tau:].reshape((-1, 1))
batch_size, n_train = 16, 600 # Only the first `n_train` examples are used for training train_iter = d2l.load_array((features[:n_train], labels[:n_train]),
batch_size, is_train=True)
Here we keep the architecture fairly simple: just an MLP with two fully-connected layers, ReLU activation, and squared loss.
# A simple MLP def get_net():
net = nn.Sequential() net.add(nn.Dense(10, activation='relu'), nn.Dense(1)) net.initialize(init.Xavier()) return net
# Square loss loss = gluon.loss.L2Loss()
Now we are ready to train the model. The code below is essentially identical to the training loop in previous sections, such as Section 3.3. Thus, we will not delve into much detail.

304

Chapter 8. Recurrent Neural Networks

def train(net, train_iter, loss, epochs, lr): trainer = gluon.Trainer(net.collect_params(), 'adam', {'learning_rate': lr}) for epoch in range(epochs): for X, y in train_iter: with autograd.record(): l = loss(net(X), y) l.backward() trainer.step(batch_size) print(f'epoch {epoch + 1}, ' f'loss: {d2l.evaluate_loss(net, train_iter, loss):f}')
net = get_net() train(net, train_iter, loss, 5, 0.01)
epoch 1, loss: 0.037935 epoch 2, loss: 0.030307 epoch 3, loss: 0.028663 epoch 4, loss: 0.026948 epoch 5, loss: 0.026431
8.1.3 Prediction
Since the training loss is small, we would expect our model to work well. Let us see what this means in practice. The first thing to check is how well the model is able to predict what happens just in the next time step, namely the one-step-ahead prediction.
onestep_preds = net(features) d2l.plot([time, time[tau:]],
[x.asnumpy(), onestep_preds.asnumpy()], 'time', 'x', legend=['data', '1-step preds'], xlim=[1, 1000], figsize=(6, 3))

The one-step-ahead predictions look nice, just as we expected. Even beyond 604 (n_train + tau) observations the predictions still look trustworthy. However, there is just one little problem to

8.1. Sequence Models

305

this: if we observe sequence data only until time step 604, we cannot hope to receive the inputs for all the future one-step-ahead predictions. Instead, we need to work our way forward one step at a time:

xˆ605 = f (x601, x602, x603, x604),

xˆ606 = f (x602, x603, x604, xˆ605),

xˆ607 = f (x603, x604, xˆ605, xˆ606), xˆ608 = f (x604, xˆ605, xˆ606, xˆ607),

(8.1.6)

xˆ609 = f (xˆ605, xˆ606, xˆ607, xˆ608),

...

Generally, for an observed sequence up to xt, its predicted output xˆt+k at time step t + k is called the k-step-ahead prediction. Since we have observed up to x604, its k-step-ahead prediction is xˆ604+k. In other words, we will have to use our own predictions to make multistep-ahead predictions. Let us see how well this goes.

multistep_preds = np.zeros(T) multistep_preds[:n_train + tau] = x[:n_train + tau] for i in range(n_train + tau, T):
multistep_preds[i] = net(multistep_preds[i - tau:i].reshape((1, -1)))

d2l.plot([time, time[tau:], time[n_train + tau:]], [ x.asnumpy(), onestep_preds.asnumpy(), multistep_preds[n_train + tau:].asnumpy()], 'time', 'x', legend=['data', '1-step preds', 'multistep preds'], xlim=[1, 1000], figsize=(6, 3))

As the above example shows, this is a spectacular failure. The predictions decay to a constant pretty quickly after a few prediction steps. Why did the algorithm work so poorly? This is ultimately due to the fact that the errors build up. Let us say that after step 1 we have some error ϵ1 = ϵ¯. Now the input for step 2 is perturbed by ϵ1, hence we suffer some error in the order of ϵ2 = ϵ¯ + cϵ1 for some constant c, and so on. The error can diverge rather rapidly from the true observations. This is a common phenomenon. For instance, weather forecasts for the next 24 hours tend to be pretty accurate but beyond that the accuracy declines rapidly. We will discuss methods for improving this throughout this chapter and beyond.

306

Chapter 8. Recurrent Neural Networks

Let us take a closer look at the difficulties in k-step-ahead predictions by computing predictions on the entire sequence for k = 1, 4, 16, 64.
max_steps = 64
features = np.zeros((T - tau - max_steps + 1, tau + max_steps)) # Column `i` (`i` < `tau`) are observations from `x` for time steps from # `i + 1` to `i + T - tau - max_steps + 1` for i in range(tau):
features[:, i] = x[i:i + T - tau - max_steps + 1]
# Column `i` (`i` >= `tau`) are the (`i - tau + 1`)-step-ahead predictions for # time steps from `i + 1` to `i + T - tau - max_steps + 1` for i in range(tau, tau + max_steps):
features[:, i] = net(features[:, i - tau:i]).reshape(-1)
steps = (1, 4, 16, 64) d2l.plot([time[tau + i - 1:T - max_steps + i] for i in steps],
[features[:, (tau + i - 1)].asnumpy() for i in steps], 'time', 'x', legend=[f'{i}-step preds'
for i in steps], xlim=[5, 1000], figsize=(6, 3))

This clearly illustrates how the quality of the prediction changes as we try to predict further into the future. While the 4-step-ahead predictions still look good, anything beyond that is almost useless.

8.1. Sequence Models

307

Summary
• There is quite a difference in difficulty between interpolation and extrapolation. Consequently, if you have a sequence, always respect the temporal order of the data when training, i.e., never train on future data.
• Sequence models require specialized statistical tools for estimation. Two popular choices are autoregressive models and latent-variable autoregressive models.
• For causal models (e.g., time going forward), estimating the forward direction is typically a lot easier than the reverse direction.
• For an observed sequence up to time step t, its predicted output at time step t + k is the kstep-ahead prediction. As we predict further in time by increasing k, the errors accumulate and the quality of the prediction degrades, often dramatically.
Exercises
1. Improve the model in the experiment of this section. 1. Incorporate more than the past 4 observations? How many do you really need? 2. How many past observations would you need if there was no noise? Hint: you can write sin and cos as a differential equation. 3. Can you incorporate older observations while keeping the total number of features constant? Does this improve accuracy? Why? 4. Change the neural network architecture and evaluate the performance.
2. An investor wants to find a good security to buy. He looks at past returns to decide which one is likely to do well. What could possibly go wrong with this strategy?
3. Does causality also apply to text? To which extent? 4. Give an example for when a latent autoregressive model might be needed to capture the
dynamic of the data. Discussions102

8.2 Text Preprocessing
We have reviewed and evaluated statistical tools and prediction challenges for sequence data. Such data can take many forms. Specifically, as we will focus on in many chapters of the book, text is one of the most popular examples of sequence data. For example, an article can be simply viewed as a sequence of words, or even a sequence of characters. To facilitate our future experiments with sequence data, we will dedicate this section to explain common preprocessing steps for text. Usually, these steps are:
1. Load text as strings into memory.
2. Split strings into tokens (e.g., words and characters).
3. Build a table of vocabulary to map the split tokens to numerical indices.
102 https://discuss.d2l.ai/t/113

308

Chapter 8. Recurrent Neural Networks

4. Convert text into sequences of numerical indices so they can be manipulated by models easily.
import collections import re from d2l import mxnet as d2l

8.2.1 Reading the Dataset
To get started we load text from H. G. Wellsʼ The Time Machine103. This is a fairly small corpus of just over 30000 words, but for the purpose of what we want to illustrate this is just fine. More realistic document collections contain many billions of words. The following function reads the dataset into a list of text lines, where each line is a string. For simplicity, here we ignore punctuation and capitalization.
#@save d2l.DATA_HUB['time_machine'] = (d2l.DATA_URL + 'timemachine.txt',
'090b5e7e70c295757f55df93cb0a180b9691891a')
def read_time_machine(): #@save """Load the time machine dataset into a list of text lines.""" with open(d2l.download('time_machine'), 'r') as f: lines = f.readlines() return [re.sub('[^A-Za-z]+', ' ', line).strip().lower() for line in lines]
lines = read_time_machine() print(f'# text lines: {len(lines)}') print(lines[0]) print(lines[10])
# text lines: 3221 the time machine by h g wells twinkled and his usually pale face was flushed and animated the

8.2.2 Tokenization
The following tokenize function takes a list (lines) as the input, where each element is a text sequence (e.g., a text line). Each text sequence is split into a list of tokens. A token is the basic unit in text. In the end, a list of token lists are returned, where each token is a string.
def tokenize(lines, token='word'): #@save """Split text lines into word or character tokens.""" if token == 'word': return [line.split() for line in lines] elif token == 'char': return [list(line) for line in lines] else: print('ERROR: unknown token type: ' + token)

103 http://www.gutenberg.org/ebooks/35

(continues on next page)

8.2. Text Preprocessing

309

tokens = tokenize(lines) for i in range(11):
print(tokens[i])

(continued from previous page)

['the', 'time', 'machine', 'by', 'h', 'g', 'wells'] [] [] [] [] ['i'] [] [] ['the', 'time', 'traveller', 'for', 'so', 'it', 'will', 'be', 'convenient', 'to', 'speak', →'of', 'him'] ['was', 'expounding', 'a', 'recondite', 'matter', 'to', 'us', 'his', 'grey', 'eyes', 'shone', → 'and'] ['twinkled', 'and', 'his', 'usually', 'pale', 'face', 'was', 'flushed', 'and', 'animated', →'the']

8.2.3 Vocabulary

The string type of the token is inconvenient to be used by models, which take numerical inputs. Now let us build a dictionary, often called vocabulary as well, to map string tokens into numerical indices starting from 0. To do so, we first count the unique tokens in all the documents from the training set, namely a corpus, and then assign a numerical index to each unique token according to its frequency. Rarely appeared tokens are often removed to reduce the complexity. Any token that does not exist in the corpus or has been removed is mapped into a special unknown token “<unk>”. We optionally add a list of reserved tokens, such as “<pad>” for padding, “<bos>” to present the beginning for a sequence, and “<eos>” for the end of a sequence.

class Vocab: #@save """Vocabulary for text.""" def __init__(self, tokens=None, min_freq=0, reserved_tokens=None): if tokens is None: tokens = [] if reserved_tokens is None: reserved_tokens = [] # Sort according to frequencies counter = count_corpus(tokens) self._token_freqs = sorted(counter.items(), key=lambda x: x[1], reverse=True) # The index for the unknown token is 0 self.idx_to_token = ['<unk>'] + reserved_tokens self.token_to_idx = { token: idx for idx, token in enumerate(self.idx_to_token)} for token, freq in self._token_freqs: if freq < min_freq: break if token not in self.token_to_idx: self.idx_to_token.append(token) self.token_to_idx[token] = len(self.idx_to_token) - 1

(continues on next page)

310

Chapter 8. Recurrent Neural Networks

(continued from previous page)

def __len__(self): return len(self.idx_to_token)

def __getitem__(self, tokens): if not isinstance(tokens, (list, tuple)): return self.token_to_idx.get(tokens, self.unk) return [self.__getitem__(token) for token in tokens]

def to_tokens(self, indices): if not isinstance(indices, (list, tuple)): return self.idx_to_token[indices] return [self.idx_to_token[index] for index in indices]

@property def unk(self):
return 0

# Index for the unknown token

@property def token_freqs(self): # Index for the unknown token
return self._token_freqs

def count_corpus(tokens): #@save """Count token frequencies.""" # Here `tokens` is a 1D list or 2D list if len(tokens) == 0 or isinstance(tokens[0], list): # Flatten a list of token lists into a list of tokens tokens = [token for line in tokens for token in line] return collections.Counter(tokens)

We construct a vocabulary using the time machine dataset as the corpus. Then we print the first few frequent tokens with their indices.
vocab = Vocab(tokens) print(list(vocab.token_to_idx.items())[:10])

[('<unk>', 0), ('the', 1), ('i', 2), ('and', 3), ('of', 4), ('a', 5), ('to', 6), ('was', 7),␣ →('in', 8), ('that', 9)]
Now we can convert each text line into a list of numerical indices.
for i in [0, 10]: print('words:', tokens[i]) print('indices:', vocab[tokens[i]])

words: ['the', 'time', 'machine', 'by', 'h', 'g', 'wells'] indices: [1, 19, 50, 40, 2183, 2184, 400] words: ['twinkled', 'and', 'his', 'usually', 'pale', 'face', 'was', 'flushed', 'and', →'animated', 'the'] indices: [2186, 3, 25, 1044, 362, 113, 7, 1421, 3, 1045, 1]

8.2. Text Preprocessing

311

8.2.4 Putting All Things Together
Using the above functions, we package everything into the load_corpus_time_machine function, which returns corpus, a list of token indices, and vocab, the vocabulary of the time machine corpus. The modifications we did here are: (i) we tokenize text into characters, not words, to simplify the training in later sections; (ii) corpus is a single list, not a list of token lists, since each text line in the time machine dataset is not necessarily a sentence or a paragraph.
def load_corpus_time_machine(max_tokens=-1): #@save """Return token indices and the vocabulary of the time machine dataset.""" lines = read_time_machine() tokens = tokenize(lines, 'char') vocab = Vocab(tokens) # Since each text line in the time machine dataset is not necessarily a # sentence or a paragraph, flatten all the text lines into a single list corpus = [vocab[token] for line in tokens for token in line] if max_tokens > 0: corpus = corpus[:max_tokens] return corpus, vocab
corpus, vocab = load_corpus_time_machine() len(corpus), len(vocab)
(170580, 28)
Summary
• Text is an important form of sequence data. • To preprocess text, we usually split text into tokens, build a vocabulary to map token strings
into numerical indices, and convert text data into token indices for models to manipulate.
Exercises
1. Tokenization is a key preprocessing step. It varies for different languages. Try to find another three commonly used methods to tokenize text.
2. In the experiment of this section, tokenize text into words and vary the min_freq arguments of the Vocab instance. How does this affect the vocabulary size?
Discussions104
104 https://discuss.d2l.ai/t/115

312

Chapter 8. Recurrent Neural Networks

8.3 Language Models and the Dataset

In Section 8.2, we see how to map text data into tokens, where these tokens can be viewed as a sequence of discrete observations, such as words or characters. Assume that the tokens in a text sequence of length T are in turn x1, x2, . . . , xT . Then, in the text sequence, xt(1 ≤ t ≤ T ) can be considered as the observation or label at time step t. Given such a text sequence, the goal of a language model is to estimate the joint probability of the sequence

P (x1, x2, . . . , xT ).

(8.3.1)

Language models are incredibly useful. For instance, an ideal language model would be able to generate natural text just on its own, simply by drawing one token at a time xt ∼ P (xt | xt−1, . . . , x1). Quite unlike the monkey using a typewriter, all text emerging from such a model would pass as natural language, e.g., English text. Furthermore, it would be sufficient for generating a meaningful dialog, simply by conditioning the text on previous dialog fragments. Clearly we are still very far from designing such a system, since it would need to understand the text rather than just generate grammatically sensible content.
Nonetheless, language models are of great service even in their limited form. For instance, the phrases “to recognize speech” and “to wreck a nice beach” sound very similar. This can cause ambiguity in speech recognition, which is easily resolved through a language model that rejects the second translation as outlandish. Likewise, in a document summarization algorithm it is worthwhile knowing that “dog bites man” is much more frequent than “man bites dog”, or that “I want to eat grandma” is a rather disturbing statement, whereas “I want to eat, grandma” is much more benign.

8.3.1 Learning a Language Model

The obvious question is how we should model a document, or even a sequence of tokens. Suppose that we tokenize text data at the word level. We can take recourse to the analysis we applied to sequence models in Section 8.1. Let us start by applying basic probability rules:

∏T P (x1, x2, . . . , xT ) = P (xt | x1, . . . , xt−1).
t=1

(8.3.2)

For example, the probability of a text sequence containing four words would be given as:

P (deep, learning, is, fun) = P (deep)P (learning | deep)P (is | deep, learning)P (fun | deep, learning, is). (8.3.3)

In order to compute the language model, we need to calculate the probability of words and the conditional probability of a word given the previous few words. Such probabilities are essentially language model parameters.
Here, we assume that the training dataset is a large text corpus, such as all Wikipedia entries, Project Gutenberg105, and all text posted on the Web. The probability of words can be calculated from the relative word frequency of a given word in the training dataset. For example, the estimate Pˆ(deep) can be calculated as the probability of any sentence starting with the word “deep”. A slightly less accurate approach would be to count all occurrences of the word “deep” and divide it
105 https://en.wikipedia.org/wiki/Project_Gutenberg

8.3. Language Models and the Dataset

313

by the total number of words in the corpus. This works fairly well, particularly for frequent words. Moving on, we could attempt to estimate

Pˆ(learning | deep) = n(deep, learning) , n(deep)

(8.3.4)

where n(x) and n(x, x′) are the number of occurrences of singletons and consecutive word pairs, respectively. Unfortunately, estimating the probability of a word pair is somewhat more difficult, since the occurrences of “deep learning” are a lot less frequent. In particular, for some unusual word combinations it may be tricky to find enough occurrences to get accurate estimates. Things take a turn for the worse for three-word combinations and beyond. There will be many plausible three-word combinations that we likely will not see in our dataset. Unless we provide some solution to assign such word combinations nonzero count, we will not be able to use them in a language model. If the dataset is small or if the words are very rare, we might not find even a single one of them.

A common strategy is to perform some form of Laplace smoothing. The solution is to add a small constant to all counts. Denote by n the total number of words in the training set and m the number of unique words. This solution helps with singletons, e.g., via

Pˆ(x) = n(x) + ϵ1/m , n + ϵ1

Pˆ(x′

|

x)

=

n(x, x′)

+

ϵ2Pˆ(x′) ,

n(x) + ϵ2

Pˆ(x′′

|

x, x′)

=

n(x, x′, x′′)

+

ϵ3Pˆ(x′′) .

n(x, x′) + ϵ3

(8.3.5)

Here ϵ1, ϵ2, and ϵ3 are hyperparameters. Take ϵ1 as an example: when ϵ1 = 0, no smoothing is applied; when ϵ1 approaches positive infinity, Pˆ(x) approaches the uniform probability 1/m. The
above is a rather primitive variant of what other techniques can accomplish (Wood et al., 2011).

Unfortunately, models like this get unwieldy rather quickly for the following reasons. First, we need to store all counts. Second, this entirely ignores the meaning of the words. For instance, “cat” and “feline” should occur in related contexts. It is quite difficult to adjust such models to additional contexts, whereas, deep learning based language models are well suited to take this into account. Last, long word sequences are almost certain to be novel, hence a model that simply counts the frequency of previously seen word sequences is bound to perform poorly there.

8.3.2 Markov Models and n-grams

Before we discuss solutions involving deep learning, we need some more terminology and concepts. Recall our discussion of Markov Models in Section 8.1. Let us apply this to language modeling. A distribution over sequences satisfies the Markov property of first order if P (xt+1 | xt, . . . , x1) = P (xt+1 | xt). Higher orders correspond to longer dependencies. This leads to a number of approximations that we could apply to model a sequence:

P (x1, x2, x3, x4) = P (x1)P (x2)P (x3)P (x4), P (x1, x2, x3, x4) = P (x1)P (x2 | x1)P (x3 | x2)P (x4 | x3), P (x1, x2, x3, x4) = P (x1)P (x2 | x1)P (x3 | x1, x2)P (x4 | x2, x3).

(8.3.6)

The probability formulae that involve one, two, and three variables are typically referred to as unigram, bigram, and trigram models, respectively. In the following, we will learn how to design better models.

314

Chapter 8. Recurrent Neural Networks

8.3.3 Natural Language Statistics
Let us see how this works on real data. We construct a vocabulary based on the time machine dataset as introduced in Section 8.2 and print the top 10 most frequent words.
import random from mxnet import np, npx from d2l import mxnet as d2l
npx.set_np()
tokens = d2l.tokenize(d2l.read_time_machine()) # Since each text line is not necessarily a sentence or a paragraph, we # concatenate all text lines corpus = [token for line in tokens for token in line] vocab = d2l.Vocab(corpus) vocab.token_freqs[:10]
[('the', 2261), ('i', 1267), ('and', 1245), ('of', 1155), ('a', 816), ('to', 695), ('was', 552), ('in', 541), ('that', 443), ('my', 440)]
As we can see, the most popular words are actually quite boring to look at. They are often referred to as stop words and thus filtered out. Nonetheless, they still carry meaning and we will still use them. Besides, it is quite clear that the word frequency decays rather rapidly. The 10th most frequent word is less than 1/5 as common as the most popular one. To get a better idea, we plot the figure of the word frequency.
freqs = [freq for token, freq in vocab.token_freqs] d2l.plot(freqs, xlabel='token: x', ylabel='frequency: n(x)', xscale='log',
yscale='log')

8.3. Language Models and the Dataset

315

We are on to something quite fundamental here: the word frequency decays rapidly in a well-
defined way. After dealing with the first few words as exceptions, all the remaining words roughly
follow a straight line on a log-log plot. This means that words satisfy Zipf’s law, which states that the frequency ni of the ith most frequent word is:

1 ni ∝ iα ,

(8.3.7)

which is equivalent to

log ni = −α log i + c,

(8.3.8)

where α is the exponent that characterizes the distribution and c is a constant. This should already give us pause if we want to model words by counting statistics and smoothing. After all, we will significantly overestimate the frequency of the tail, also known as the infrequent words. But what about the other word combinations, such as bigrams, trigrams, and beyond? Let us see whether the bigram frequency behaves in the same manner as the unigram frequency.

bigram_tokens = [pair for pair in zip(corpus[:-1], corpus[1:])] bigram_vocab = d2l.Vocab(bigram_tokens) bigram_vocab.token_freqs[:10]

[(('of', 'the'), 309), (('in', 'the'), 169), (('i', 'had'), 130), (('i', 'was'), 112), (('and', 'the'), 109), (('the', 'time'), 102), (('it', 'was'), 99), (('to', 'the'), 85), (('as', 'i'), 78), (('of', 'a'), 73)]
One thing is notable here. Out of the ten most frequent word pairs, nine are composed of both stop words and only one is relevant to the actual book—“the time”. Furthermore, let us see whether the trigram frequency behaves in the same manner.
trigram_tokens = [ triple for triple in zip(corpus[:-2], corpus[1:-1], corpus[2:])]
trigram_vocab = d2l.Vocab(trigram_tokens) trigram_vocab.token_freqs[:10]

[(('the', 'time', 'traveller'), 59), (('the', 'time', 'machine'), 30), (('the', 'medical', 'man'), 24), (('it', 'seemed', 'to'), 16), (('it', 'was', 'a'), 15), (('here', 'and', 'there'), 15), (('seemed', 'to', 'me'), 14), (('i', 'did', 'not'), 14), (('i', 'saw', 'the'), 13), (('i', 'began', 'to'), 13)]

Last, let us visualize the token frequency among these three models: unigrams, bigrams, and trigrams.

316

Chapter 8. Recurrent Neural Networks

bigram_freqs = [freq for token, freq in bigram_vocab.token_freqs] trigram_freqs = [freq for token, freq in trigram_vocab.token_freqs] d2l.plot([freqs, bigram_freqs, trigram_freqs], xlabel='token: x',
ylabel='frequency: n(x)', xscale='log', yscale='log', legend=['unigram', 'bigram', 'trigram'])

This figure is quite exciting for a number of reasons. First, beyond unigram words, sequences of words also appear to be following Zipfʼs law, albeit with a smaller exponent α in (8.3.7), depending on the sequence length. Second, the number of distinct n-grams is not that large. This gives us hope that there is quite a lot of structure in language. Third, many n-grams occur very rarely, which makes Laplace smoothing rather unsuitable for language modeling. Instead, we will use deep learning based models.
8.3.4 Reading Long Sequence Data
Since sequence data are by their very nature sequential, we need to address the issue of processing it. We did so in a rather ad-hoc manner in Section 8.1. When sequences get too long to be processed by models all at once, we may wish to split such sequences for reading. Now let us describe general strategies. Before introducing the model, let us assume that we will use a neural network to train a language model, where the network processes a minibatch of sequences with predefined length, say n time steps, at a time. Now the question is how to read minibatches of features and labels at random.
To begin with, since a text sequence can be arbitrarily long, such as the entire The Time Machine book, we can partition such a long sequence into subsequences with the same number of time steps. When training our neural network, a minibatch of such subsequences will be fed into the model. Suppose that the network processes a subsequence of n time steps at a time. Fig. 8.3.1 shows all the different ways to obtain subsequences from an original text sequence, where n = 5 and a token at each time step corresponds to a character. Note that we have quite some freedom since we could pick an arbitrary offset that indicates the initial position.

8.3. Language Models and the Dataset

317

Fig. 8.3.1: Different offsets lead to different subsequences when splitting up text.
Hence, which one should we pick from Fig. 8.3.1? In fact, all of them are equally good. However, if we pick just one offset, there is limited coverage of all the possible subsequences for training our network. Therefore, we can start with a random offset to partition a sequence to get both coverage and randomness. In the following, we describe how to accomplish this for both random sampling and sequential partitioning strategies.

Random Sampling

In random sampling, each example is a subsequence arbitrarily captured on the original long sequence. The subsequences from two adjacent random minibatches during iteration are not necessarily adjacent on the original sequence. For language modeling, the target is to predict the next token based on what tokens we have seen so far, hence the labels are the original sequence, shifted by one token.
The following code randomly generates a minibatch from the data each time. Here, the argument batch_size specifies the number of subsequence examples in each minibatch and num_steps is the predefined number of time steps in each subsequence.

def seq_data_iter_random(corpus, batch_size, num_steps): #@save """Generate a minibatch of subsequences using random sampling.""" # Start with a random offset (inclusive of `num_steps - 1`) to partition a # sequence corpus = corpus[random.randint(0, num_steps - 1):] # Subtract 1 since we need to account for labels num_subseqs = (len(corpus) - 1) // num_steps # The starting indices for subsequences of length `num_steps` initial_indices = list(range(0, num_subseqs * num_steps, num_steps)) # In random sampling, the subsequences from two adjacent random # minibatches during iteration are not necessarily adjacent on the # original sequence random.shuffle(initial_indices)

def data(pos): # Return a sequence of length `num_steps` starting from `pos` return corpus[pos:pos + num_steps]

num_batches = num_subseqs // batch_size for i in range(0, batch_size * num_batches, batch_size):

(continues on next page)

318

Chapter 8. Recurrent Neural Networks

(continued from previous page)
# Here, `initial_indices` contains randomized starting indices for # subsequences initial_indices_per_batch = initial_indices[i:i + batch_size] X = [data(j) for j in initial_indices_per_batch] Y = [data(j + 1) for j in initial_indices_per_batch] yield np.array(X), np.array(Y)
Let us manually generate a sequence from 0 to 34. We assume that the batch size and numbers of time steps are 2 and 5, respectively. This means that we can generate ⌊(35−1)/5⌋ = 6 feature-label subsequence pairs. With a minibatch size of 2, we only get 3 minibatches.
my_seq = list(range(35)) for X, Y in seq_data_iter_random(my_seq, batch_size=2, num_steps=5):
print('X: ', X, '\nY:', Y)
X: [[ 8. 9. 10. 11. 12.] [13. 14. 15. 16. 17.]] Y: [[ 9. 10. 11. 12. 13.] [14. 15. 16. 17. 18.]] X: [[ 3. 4. 5. 6. 7.] [18. 19. 20. 21. 22.]] Y: [[ 4. 5. 6. 7. 8.] [19. 20. 21. 22. 23.]] X: [[23. 24. 25. 26. 27.] [28. 29. 30. 31. 32.]] Y: [[24. 25. 26. 27. 28.] [29. 30. 31. 32. 33.]]

Sequential Partitioning
In addition to random sampling of the original sequence, we can also ensure that the subsequences from two adjacent minibatches during iteration are adjacent on the original sequence. This strategy preserves the order of split subsequences when iterating over minibatches, hence is called sequential partitioning.
def seq_data_iter_sequential(corpus, batch_size, num_steps): #@save """Generate a minibatch of subsequences using sequential partitioning.""" # Start with a random offset to partition a sequence offset = random.randint(0, num_steps) num_tokens = ((len(corpus) - offset - 1) // batch_size) * batch_size Xs = np.array(corpus[offset:offset + num_tokens]) Ys = np.array(corpus[offset + 1:offset + 1 + num_tokens]) Xs, Ys = Xs.reshape(batch_size, -1), Ys.reshape(batch_size, -1) num_batches = Xs.shape[1] // num_steps for i in range(0, num_steps * num_batches, num_steps): X = Xs[:, i:i + num_steps] Y = Ys[:, i:i + num_steps] yield X, Y
Using the same settings, let us print features X and labels Y for each minibatch of subsequences read by sequential partitioning. Note that the subsequences from two adjacent minibatches dur-

8.3. Language Models and the Dataset

319

ing iteration are indeed adjacent on the original sequence.
for X, Y in seq_data_iter_sequential(my_seq, batch_size=2, num_steps=5): print('X: ', X, '\nY:', Y)
X: [[ 2. 3. 4. 5. 6.] [18. 19. 20. 21. 22.]] Y: [[ 3. 4. 5. 6. 7.] [19. 20. 21. 22. 23.]] X: [[ 7. 8. 9. 10. 11.] [23. 24. 25. 26. 27.]] Y: [[ 8. 9. 10. 11. 12.] [24. 25. 26. 27. 28.]] X: [[12. 13. 14. 15. 16.] [28. 29. 30. 31. 32.]] Y: [[13. 14. 15. 16. 17.] [29. 30. 31. 32. 33.]]
Now we wrap the above two sampling functions to a class so that we can use it as a data iterator later.
class SeqDataLoader: #@save """An iterator to load sequence data.""" def __init__(self, batch_size, num_steps, use_random_iter, max_tokens): if use_random_iter: self.data_iter_fn = d2l.seq_data_iter_random else: self.data_iter_fn = d2l.seq_data_iter_sequential self.corpus, self.vocab = d2l.load_corpus_time_machine(max_tokens) self.batch_size, self.num_steps = batch_size, num_steps
def __iter__(self): return self.data_iter_fn(self.corpus, self.batch_size, self.num_steps)
Last, we define a function load_data_time_machine that returns both the data iterator and the vocabulary, so we can use it similarly as other other functions with the load_data prefix, such as d2l.load_data_fashion_mnist defined in Section 3.5.
def load_data_time_machine(batch_size, num_steps, #@save use_random_iter=False, max_tokens=10000):
"""Return the iterator and the vocabulary of the time machine dataset.""" data_iter = SeqDataLoader(batch_size, num_steps, use_random_iter,
max_tokens) return data_iter, data_iter.vocab

320

Chapter 8. Recurrent Neural Networks

Summary
• Language models are key to natural language processing. • n-grams provide a convenient model for dealing with long sequences by truncating the de-
pendence. • Long sequences suffer from the problem that they occur very rarely or never. • Zipfʼs law governs the word distribution for not only unigrams but also the other n-grams. • There is a lot of structure but not enough frequency to deal with infrequent word combina-
tions efficiently via Laplace smoothing. • The main choices for reading long sequences are random sampling and sequential parti-
tioning. The latter can ensure that the subsequences from two adjacent minibatches during iteration are adjacent on the original sequence.
Exercises
1. Suppose there are 100, 000 words in the training dataset. How much word frequency and multi-word adjacent frequency does a four-gram need to store?
2. How would you model a dialogue? 3. Estimate the exponent of Zipfʼs law for unigrams, bigrams, and trigrams. 4. What other methods can you think of for reading long sequence data? 5. Consider the random offset that we use for reading long sequences.
1. Why is it a good idea to have a random offset? 2. Does it really lead to a perfectly uniform distribution over the sequences on the docu-
ment? 3. What would you have to do to make things even more uniform? 6. If we want a sequence example to be a complete sentence, what kind of problem does this introduce in minibatch sampling? How can we fix the problem? Discussions106

8.4 Recurrent Neural Networks

In Section 8.3 we introduced n-gram models, where the conditional probability of word xt at time step t only depends on the n − 1 previous words. If we want to incorporate the possible effect of words earlier than time step t − (n − 1) on xt, we need to increase n. However, the number of model parameters would also increase exponentially with it, as we need to store |V|n numbers for a vocabulary set V. Hence, rather than modeling P (xt | xt−1, . . . , xt−n+1) it is preferable to use a latent variable model:

106 https://discuss.d2l.ai/t/117

P (xt | xt−1, . . . , x1) ≈ P (xt | ht−1),

(8.4.1)

8.4. Recurrent Neural Networks

321

where ht−1 is a hidden state (also known as a hidden variable) that stores the sequence information up to time step t − 1. In general, the hidden state at any time step t could be computed based on both the current input xt and the previous hidden state ht−1:

ht = f (xt, ht−1).

(8.4.2)

For a sufficiently powerful function f in (8.4.2), the latent variable model is not an approximation. After all, ht may simply store all the data it has observed so far. However, it could potentially make both computation and storage expensive.
Recall that we have discussed hidden layers with hidden units in Chapter 4. It is noteworthy that hidden layers and hidden states refer to two very different concepts. Hidden layers are, as explained, layers that are hidden from view on the path from input to output. Hidden states are technically speaking inputs to whatever we do at a given step, and they can only be computed by looking at data at previous time steps.
Recurrent neural networks (RNNs) are neural networks with hidden states. Before introducing the RNN model, we first revisit the MLP model introduced in Section 4.1.

8.4.1 Neural Networks without Hidden States

Let us take a look at an MLP with a single hidden layer. Let the hidden layerʼs activation function be ϕ. Given a minibatch of examples X ∈ Rn×d with batch size n and d inputs, the hidden layerʼs output H ∈ Rn×h is calculated as

H = ϕ(XWxh + bh).

(8.4.3)

In (8.4.3), we have the weight parameter Wxh ∈ Rd×h, the bias parameter bh ∈ R1×h, and the number of hidden units h, for the hidden layer. Thus, broadcasting (see Section 2.1.3) is applied during the summation. Next, the hidden variable H is used as the input of the output layer. The output layer is given by

O = HWhq + bq,

(8.4.4)

where O ∈ Rn×q is the output variable, Whq ∈ Rh×q is the weight parameter, and bq ∈ R1×q is the bias parameter of the output layer. If it is a classification problem, we can use softmax(O) to compute the probability distribution of the output categories.
This is entirely analogous to the regression problem we solved previously in Section 8.1, hence we omit details. Suffice it to say that we can pick feature-label pairs at random and learn the parameters of our network via automatic differentiation and stochastic gradient descent.

8.4.2 Recurrent Neural Networks with Hidden States
Matters are entirely different when we have hidden states. Let us look at the structure in some more detail.
Assume that we have a minibatch of inputs Xt ∈ Rn×d at time step t. In other words, for a minibatch of n sequence examples, each row of Xt corresponds to one example at time step t from the sequence. Next, denote by Ht ∈ Rn×h the hidden variable of time step t. Unlike the MLP, here we save the hidden variable Ht−1 from the previous time step and introduce a new weight parameter Whh ∈ Rh×h to describe how to use the hidden variable of the previous time step in the

322

Chapter 8. Recurrent Neural Networks

current time step. Specifically, the calculation of the hidden variable of the current time step is determined by the input of the current time step together with the hidden variable of the previous time step:

Ht = ϕ(XtWxh + Ht−1Whh + bh).

(8.4.5)

Compared with (8.4.3), (8.4.5) adds one more term Ht−1Whh and thus instantiates (8.4.2). From the relationship between hidden variables Ht and Ht−1 of adjacent time steps, we know that these variables captured and retained the sequenceʼs historical information up to their current time step, just like the state or memory of the neural networkʼs current time step. Therefore, such a hidden variable is called a hidden state. Since the hidden state uses the same definition of the previous time step in the current time step, the computation of (8.4.5) is recurrent. Hence, neural networks with hidden states based on recurrent computation are named recurrent neural networks. Layers that perform the computation of (8.4.5) in RNNs are called recurrent layers.
There are many different ways for constructing RNNs. RNNs with a hidden state defined by (8.4.5) are very common. For time step t, the output of the output layer is similar to the computation in the MLP:

Ot = HtWhq + bq.

(8.4.6)

Parameters of the RNN include the weights Wxh ∈ Rd×h, Whh ∈ Rh×h, and the bias bh ∈ R1×h of the hidden layer, together with the weights Whq ∈ Rh×q and the bias bq ∈ R1×q of the output layer. It is worth mentioning that even at different time steps, RNNs always use these model parameters. Therefore, the parameterization cost of an RNN does not grow as the number of time steps increases.
Fig. 8.4.1 illustrates the computational logic of an RNN at three adjacent time steps. At any time step t, the computation of the hidden state can be treated as: (i) concatenating the input Xt at the current time step t and the hidden state Ht−1 at the previous time step t − 1; (ii) feeding the concatenation result into a fully-connected layer with the activation function ϕ. The output of such a fully-connected layer is the hidden state Ht of the current time step t. In this case, the model parameters are the concatenation of Wxh and Whh, and a bias of bh, all from (8.4.5). The hidden state of the current time step t, Ht, will participate in computing the hidden state Ht+1 of the next time step t + 1. What is more, Ht will also be fed into the fully-connected output layer to compute the output Ot of the current time step t.

Fig. 8.4.1: An RNN with a hidden state.

8.4. Recurrent Neural Networks

323

We just mentioned that the calculation of XtWxh + Ht−1Whh for the hidden state is equivalent to matrix multiplication of concatenation of Xt and Ht−1 and concatenation of Wxh and Whh. Though this can be proven in mathematics, in the following we just use a simple code snippet to show this. To begin with, we define matrices X, W_xh, H, and W_hh, whose shapes are (3, 1), (1, 4), (3, 4), and (4, 4), respectively. Multiplying X by W_xh, and H by W_hh, respectively, and then adding these two multiplications, we obtain a matrix of shape (3, 4).
from mxnet import np, npx from d2l import mxnet as d2l
npx.set_np()
X, W_xh = np.random.normal(0, 1, (3, 1)), np.random.normal(0, 1, (1, 4)) H, W_hh = np.random.normal(0, 1, (3, 4)), np.random.normal(0, 1, (4, 4)) np.dot(X, W_xh) + np.dot(H, W_hh)
array([[-0.21952868, 4.256434 , 4.5812645 , -5.344988 ], [ 3.4478583 , -3.0177274 , -1.6777471 , 7.535347 ], [ 2.239007 , 1.4199957 , 4.744728 , -8.421293 ]])
Now we concatenate the matrices X and H along columns (axis 1), and the matrices W_xh and W_hh along rows (axis 0). These two concatenations result in matrices of shape (3, 5) and of shape (5, 4), respectively. Multiplying these two concatenated matrices, we obtain the same output matrix of shape (3, 4) as above.
np.dot(np.concatenate((X, H), 1), np.concatenate((W_xh, W_hh), 0))
array([[-0.2195287, 4.256434 , 4.5812645, -5.344988 ], [ 3.4478583, -3.0177271, -1.677747 , 7.535347 ], [ 2.2390068, 1.4199957, 4.744728 , -8.421294 ]])
8.4.3 RNN-based Character-Level Language Models
Recall that for language modeling in Section 8.3, we aim to predict the next token based on the current and past tokens, thus we shift the original sequence by one token as the labels. Bengio et al. first proposed to use a neural network for language modeling (Bengio et al., 2003). In the following we illustrate how RNNs can be used to build a language model. Let the minibatch size be one, and the sequence of the text be “machine”. To simplify training in subsequent sections, we tokenize text into characters rather than words and consider a character-level language model. Fig. 8.4.2 demonstrates how to predict the next character based on the current and previous characters via an RNN for character-level language modeling.

324

Chapter 8. Recurrent Neural Networks

Fig. 8.4.2: A character-level language model based on the RNN. The input and label sequences are “machin” and “achine”, respectively.

During the training process, we run a softmax operation on the output from the output layer for each time step, and then use the cross-entropy loss to compute the error between the model output and the label. Due to the recurrent computation of the hidden state in the hidden layer, the output of time step 3 in Fig. 8.4.2, O3, is determined by the text sequence “m”, “a”, and “c”. Since the next character of the sequence in the training data is “h”, the loss of time step 3 will depend on the probability distribution of the next character generated based on the feature sequence “m”, “a”, “c” and the label “h” of this time step.
In practice, each token is represented by a d-dimensional vector, and we use a batch size n > 1. Therefore, the input Xt at time step t will be a n × d matrix, which is identical to what we discussed in Section 8.4.2.

8.4.4 Perplexity
Last, let us discuss about how to measure the language model quality, which will be used to evaluate our RNN-based models in the subsequent sections. One way is to check how surprising the text is. A good language model is able to predict with high-accuracy tokens that what we will see next. Consider the following continuations of the phrase “It is raining”, as proposed by different language models:
1. “It is raining outside”
2. “It is raining banana tree”
3. “It is raining piouw;kcj pwepoiut”
In terms of quality, example 1 is clearly the best. The words are sensible and logically coherent. While it might not quite accurately reflect which word follows semantically (“in San Francisco” and “in winter” would have been perfectly reasonable extensions), the model is able to capture which kind of word follows. Example 2 is considerably worse by producing a nonsensical extension. Nonetheless, at least the model has learned how to spell words and some degree of correlation between words. Last, example 3 indicates a poorly trained model that does not fit data properly.
We might measure the quality of the model by computing the likelihood of the sequence. Unfortunately this is a number that is hard to understand and difficult to compare. After all, shorter sequences are much more likely to occur than the longer ones, hence evaluating the model on Tolstoyʼs magnum opus War and Peace will inevitably produce a much smaller likelihood than, say, on Saint-Exuperyʼs novella The Little Prince. What is missing is the equivalent of an average.

8.4. Recurrent Neural Networks

325

Information theory comes handy here. We have defined entropy, surprisal, and cross-entropy when we introduced the softmax regression (Section 3.4.7) and more of information theory is discussed in the online appendix on information theory107. If we want to compress text, we can ask about predicting the next token given the current set of tokens. A better language model should allow us to predict the next token more accurately. Thus, it should allow us to spend fewer bits in compressing the sequence. So we can measure it by the cross-entropy loss averaged over all the n tokens of a sequence:

1 ∑n n − log P (xt | xt−1, . . . , x1),
t=1

(8.4.7)

where P is given by a language model and xt is the actual token observed at time step t from the

sequence. This makes the performance on documents of different lengths comparable. For his-

torical reasons, scientists in natural language processing prefer to use a quantity called perplexity.

In a nutshell, it is the exponential of (8.4.7):

(

)

1 ∑n

exp − n log P (xt | xt−1, . . . , x1) .

t=1

(8.4.8)

Perplexity can be best understood as the harmonic mean of the number of real choices that we have when deciding which token to pick next. Let us look at a number of cases:
• In the best case scenario, the model always perfectly estimates the probability of the label token as 1. In this case the perplexity of the model is 1.
• In the worst case scenario, the model always predicts the probability of the label token as 0. In this situation, the perplexity is positive infinity.
• At the baseline, the model predicts a uniform distribution over all the available tokens of the vocabulary. In this case, the perplexity equals the number of unique tokens of the vocabulary. In fact, if we were to store the sequence without any compression, this would be the best we could do to encode it. Hence, this provides a nontrivial upper bound that any useful model must beat.
In the following sections, we will implement RNNs for character-level language models and use perplexity to evaluate such models.

Summary
• A neural network that uses recurrent computation for hidden states is called a recurrent neural network (RNN).
• The hidden state of an RNN can capture historical information of the sequence up to the current time step.
• The number of RNN model parameters does not grow as the number of time steps increases. • We can create character-level language models using an RNN. • We can use perplexity to evaluate the quality of language models.
107 https://d2l.ai/chapter_appendix-mathematics-for-deep-learning/information-theory.html

326

Chapter 8. Recurrent Neural Networks

Exercises
1. If we use an RNN to predict the next character in a text sequence, what is the required dimension for any output?
2. Why can RNNs express the conditional probability of a token at some time step based on all the previous tokens in the text sequence?
3. What happens to the gradient if you backpropagate through a long sequence? 4. What are some of the problems associated with the language model described in this sec-
tion? Discussions108
8.5 Implementation of Recurrent Neural Networks from Scratch
In this section we will implement an RNN from scratch for a character-level language model, according to our descriptions in Section 8.4. Such a model will be trained on H. G. Wellsʼ The Time Machine. As before, we start by reading the dataset first, which is introduced in Section 8.3.
%matplotlib inline import math from mxnet import autograd, gluon, np, npx from d2l import mxnet as d2l
npx.set_np()
batch_size, num_steps = 32, 35 train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)
8.5.1 One-Hot Encoding
Recall that each token is represented as a numerical index in train_iter. Feeding these indices directly to a neural network might make it hard to learn. We often represent each token as a more expressive feature vector. The easiest representation is called one-hot encoding, which is introduced in Section 3.4.1. In a nutshell, we map each index to a different unit vector: assume that the number of different tokens in the vocabulary is N (len(vocab)) and the token indices range from 0 to N − 1. If the index of a token is the integer i, then we create a vector of all 0s with a length of N and set the element at position i to 1. This vector is the one-hot vector of the original token. The one-hot vectors with indices 0 and 2 are shown below.
npx.one_hot(np.array([0, 2]), len(vocab))
108 https://discuss.d2l.ai/t/337

8.5. Implementation of Recurrent Neural Networks from Scratch

327

array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])
The shape of the minibatch that we sample each time is (batch size, number of time steps). The one_hot function transforms such a minibatch into a three-dimensional tensor with the last dimension equals to the vocabulary size (len(vocab)). We often transpose the input so that we will obtain an output of shape (number of time steps, batch size, vocabulary size). This will allow us to more conveniently loop through the outermost dimension for updating hidden states of a minibatch, time step by time step.
X = np.arange(10).reshape((2, 5)) npx.one_hot(X.T, 28).shape
(5, 2, 28)
8.5.2 Initializing the Model Parameters
Next, we initialize the model parameters for the RNN model. The number of hidden units num_hiddens is a tunable hyperparameter. When training language models, the inputs and outputs are from the same vocabulary. Hence, they have the same dimension, which is equal to the vocabulary size.
def get_params(vocab_size, num_hiddens, device): num_inputs = num_outputs = vocab_size
def normal(shape): return np.random.normal(scale=0.01, size=shape, ctx=device)
# Hidden layer parameters W_xh = normal((num_inputs, num_hiddens)) W_hh = normal((num_hiddens, num_hiddens)) b_h = np.zeros(num_hiddens, ctx=device) # Output layer parameters W_hq = normal((num_hiddens, num_outputs)) b_q = np.zeros(num_outputs, ctx=device) # Attach gradients params = [W_xh, W_hh, b_h, W_hq, b_q] for param in params:
param.attach_grad() return params

328

Chapter 8. Recurrent Neural Networks

8.5.3 RNN Model
To define an RNN model, we first need an init_rnn_state function to return the hidden state at initialization. It returns a tensor filled with 0 and with a shape of (batch size, number of hidden units). Using tuples makes it easier to handle situations where the hidden state contains multiple variables, which we will encounter in later sections.
def init_rnn_state(batch_size, num_hiddens, device): return (np.zeros((batch_size, num_hiddens), ctx=device),)
The following rnn function defines how to compute the hidden state and output at a time step. Note that the RNN model loops through the outermost dimension of inputs so that it updates hidden states H of a minibatch, time step by time step. Besides, the activation function here uses the tanh function. As described in Section 4.1, the mean value of the tanh function is 0, when the elements are uniformly distributed over the real numbers.
def rnn(inputs, state, params): # Shape of `inputs`: (`num_steps`, `batch_size`, `vocab_size`) W_xh, W_hh, b_h, W_hq, b_q = params H, = state outputs = [] # Shape of `X`: (`batch_size`, `vocab_size`) for X in inputs: H = np.tanh(np.dot(X, W_xh) + np.dot(H, W_hh) + b_h) Y = np.dot(H, W_hq) + b_q outputs.append(Y) return np.concatenate(outputs, axis=0), (H,)
With all the needed functions being defined, next we create a class to wrap these functions and store parameters for an RNN model implemented from scratch.
class RNNModelScratch: #@save """An RNN Model implemented from scratch.""" def __init__(self, vocab_size, num_hiddens, device, get_params, init_state, forward_fn): self.vocab_size, self.num_hiddens = vocab_size, num_hiddens self.params = get_params(vocab_size, num_hiddens, device) self.init_state, self.forward_fn = init_state, forward_fn
def __call__(self, X, state): X = npx.one_hot(X.T, self.vocab_size) return self.forward_fn(X, state, self.params)
def begin_state(self, batch_size, ctx): return self.init_state(batch_size, self.num_hiddens, ctx)
Let us check whether the outputs have the correct shapes, e.g., to ensure that the dimensionality of the hidden state remains unchanged.
num_hiddens = 512 net = RNNModelScratch(len(vocab), num_hiddens, d2l.try_gpu(), get_params,
init_rnn_state, rnn) state = net.begin_state(X.shape[0], d2l.try_gpu())
(continues on next page)

8.5. Implementation of Recurrent Neural Networks from Scratch

329

Y, new_state = net(X.as_in_context(d2l.try_gpu()), state) Y.shape, len(new_state), new_state[0].shape

(continued from previous page)

((10, 28), 1, (2, 512))
We can see that the output shape is (number of time steps × batch size, vocabulary size), while the hidden state shape remains the same, i.e., (batch size, number of hidden units).

8.5.4 Prediction
Let us first define the prediction function to generate new characters following the user-provided prefix, which is a string containing several characters. When looping through these beginning characters in prefix, we keep passing the hidden state to the next time step without generating any output. This is called the warm-up period, during which the model updates itself (e.g., update the hidden state) but does not make predictions. After the warm-up period, the hidden state is generally better than its initialized value at the beginning. So we generate the predicted characters and emit them.
def predict_ch8(prefix, num_preds, net, vocab, device): #@save """Generate new characters following the `prefix`.""" state = net.begin_state(batch_size=1, ctx=device) outputs = [vocab[prefix[0]]] get_input = lambda: np.array([outputs[-1]], ctx=device).reshape((1, 1)) for y in prefix[1:]: # Warm-up period _, state = net(get_input(), state) outputs.append(vocab[y]) for _ in range(num_preds): # Predict `num_preds` steps y, state = net(get_input(), state) outputs.append(int(y.argmax(axis=1).reshape(1))) return ''.join([vocab.idx_to_token[i] for i in outputs])
Now we can test the predict_ch8 function. We specify the prefix as time traveller and have it generate 10 additional characters. Given that we have not trained the network, it will generate nonsensical predictions.
predict_ch8('time traveller ', 10, net, vocab, d2l.try_gpu())

'time traveller iiiiiiiiii'

8.5.5 Gradient Clipping
For a sequence of length T , we compute the gradients over these T time steps in an iteration, which results in a chain of matrix-products with length O(T ) during backpropagation. As mentioned in Section 4.8, it might result in numerical instability, e.g., the gradients may either explode or vanish, when T is large. Therefore, RNN models often need extra help to stabilize the training.
Generally speaking, when solving an optimization problem, we take update steps for the model parameter, say in the vector form x, in the direction of the negative gradient g on a minibatch. For

330

Chapter 8. Recurrent Neural Networks

example, with η > 0 as the learning rate, in one iteration we update x as x − ηg. Let us further assume that the objective function f is well behaved, say, Lipschitz continuous with constant L. That is to say, for any x and y we have

|f (x) − f (y)| ≤ L∥x − y∥.

(8.5.1)

In this case we can safely assume that if we update the parameter vector by ηg, then

|f (x) − f (x − ηg)| ≤ Lη∥g∥,

(8.5.2)

which means that we will not observe a change by more than Lη∥g∥. This is both a curse and a blessing. On the curse side, it limits the speed of making progress; whereas on the blessing side, it limits the extent to which things can go wrong if we move in the wrong direction.

Sometimes the gradients can be quite large and the optimization algorithm may fail to converge.

We could address this by reducing the learning rate η. But what if we only rarely get large gradients?

In this case such an approach may appear entirely unwarranted. One popular alternative is to clip

the gradient g by projecting them back to a ball of a given radius, say θ via

(

)

g ← min 1, θ g.

∥g∥

(8.5.3)

By doing so we know that the gradient norm never exceeds θ and that the updated gradient is entirely aligned with the original direction of g. It also has the desirable side-effect of limiting the influence any given minibatch (and within it any given sample) can exert on the parameter vector. This bestows a certain degree of robustness to the model. Gradient clipping provides a quick fix to the gradient exploding. While it does not entirely solve the problem, it is one of the many techniques to alleviate it.
Below we define a function to clip the gradients of a model that is implemented from scratch or a model constructed by the high-level APIs. Also note that we compute the gradient norm over all the model parameters.

def grad_clipping(net, theta): #@save """Clip the gradient.""" if isinstance(net, gluon.Block): params = [p.data() for p in net.collect_params().values()] else: params = net.params norm = math.sqrt(sum((p.grad**2).sum() for p in params)) if norm > theta: for param in params: param.grad[:] *= theta / norm

8.5.6 Training

Before training the model, let us define a function to train the model in one epoch. It differs from how we train the model of Section 3.6 in three places:
1. Different sampling methods for sequential data (random sampling and sequential partitioning) will result in differences in the initialization of hidden states.
2. We clip the gradients before updating the model parameters. This ensures that the model does not diverge even when gradients blow up at some point during the training process.

8.5. Implementation of Recurrent Neural Networks from Scratch

331

3. We use perplexity to evaluate the model. As discussed in Section 8.4.4, this ensures that sequences of different length are comparable.
Specifically, when sequential partitioning is used, we initialize the hidden state only at the beginning of each epoch. Since the ith subsequence example in the next minibatch is adjacent to the current ith subsequence example, the hidden state at the end of the current minibatch will be used to initialize the hidden state at the beginning of the next minibatch. In this way, historical information of the sequence stored in the hidden state might flow over adjacent subsequences within an epoch. However, the computation of the hidden state at any point depends on all the previous minibatches in the same epoch, which complicates the gradient computation. To reduce computational cost, we detach the gradient before processing any minibatch so that the gradient computation of the hidden state is always limited to the time steps in one minibatch.
When using the random sampling, we need to re-initialize the hidden state for each iteration since each example is sampled with a random position. Same as the train_epoch_ch3 function in Section 3.6, updater is a general function to update the model parameters. It can be either the d2l. sgd function implemented from scratch or the built-in optimization function in a deep learning framework.
#@save def train_epoch_ch8(net, train_iter, loss, updater, device, use_random_iter):
"""Train a model within one epoch (defined in Chapter 8).""" state, timer = None, d2l.Timer() metric = d2l.Accumulator(2) # Sum of training loss, no. of tokens for X, Y in train_iter:
if state is None or use_random_iter: # Initialize `state` when either it is the first iteration or # using random sampling state = net.begin_state(batch_size=X.shape[0], ctx=device)
else: for s in state: s.detach()
y = Y.T.reshape(-1) X, y = X.as_in_ctx(device), y.as_in_ctx(device) with autograd.record():
y_hat, state = net(X, state) l = loss(y_hat, y).mean() l.backward() grad_clipping(net, 1) updater(batch_size=1) # Since the `mean` function has been invoked metric.add(l * y.size, y.size) return math.exp(metric[0] / metric[1]), metric[1] / timer.stop()

The training function supports an RNN model implemented either from scratch or using highlevel APIs.
def train_ch8(net, train_iter, vocab, lr, num_epochs, device, #@save use_random_iter=False):
"""Train a model (defined in Chapter 8).""" loss = gluon.loss.SoftmaxCrossEntropyLoss() animator = d2l.Animator(xlabel='epoch', ylabel='perplexity',
legend=['train'], xlim=[10, num_epochs]) # Initialize if isinstance(net, gluon.Block):
net.initialize(ctx=device, force_reinit=True, init=init.Normal(0.01))
(continues on next page)

332

Chapter 8. Recurrent Neural Networks

(continued from previous page)
trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': lr})
updater = lambda batch_size: trainer.step(batch_size) else:
updater = lambda batch_size: d2l.sgd(net.params, lr, batch_size) predict = lambda prefix: predict_ch8(prefix, 50, net, vocab, device) # Train and predict for epoch in range(num_epochs):
ppl, speed = train_epoch_ch8(net, train_iter, loss, updater, device, use_random_iter)
if (epoch + 1) % 10 == 0: animator.add(epoch + 1, [ppl])
print(f'perplexity {ppl:.1f}, {speed:.1f} tokens/sec on {str(device)}') print(predict('time traveller')) print(predict('traveller'))
Now we can train the RNN model. Since we only use 10000 tokens in the dataset, the model needs more epochs to converge better.
num_epochs, lr = 500, 1 train_ch8(net, train_iter, vocab, lr, num_epochs, d2l.try_gpu())
perplexity 1.0, 32611.0 tokens/sec on gpu(0) time travelleryou can show black is white by argument said filby travelleryou can show black is white by argument said filby

Finally, let us check the results of using the random sampling method.
net = RNNModelScratch(len(vocab), num_hiddens, d2l.try_gpu(), get_params, init_rnn_state, rnn)
train_ch8(net, train_iter, vocab, lr, num_epochs, d2l.try_gpu(), use_random_iter=True)
perplexity 1.4, 32473.4 tokens/sec on gpu(0) time travellerit s against reason said filbywhat veri ccanother travellerit s against reason said filbywhat veri ccanother

8.5. Implementation of Recurrent Neural Networks from Scratch

333

While implementing the above RNN model from scratch is instructive, it is not convenient. In the next section we will see how to improve the RNN model, such as how to make it easier to implement and make it run faster.
Summary
• We can train an RNN-based character-level language model to generate text following the user-provided text prefix.
• A simple RNN language model consists of input encoding, RNN modeling, and output generation.
• RNN models need state initialization for training, though random sampling and sequential partitioning use different ways.
• When using sequential partitioning, we need to detach the gradient to reduce computational cost.
• A warm-up period allows a model to update itself (e.g., obtain a better hidden state than its initialized value) before making any prediction.
• Gradient clipping prevents gradient explosion, but it cannot fix vanishing gradients.
Exercises
1. Show that one-hot encoding is equivalent to picking a different embedding for each object. 2. Adjust the hyperparameters (e.g., number of epochs, number of hidden units, number of
time steps in a minibatch, and learning rate) to improve the perplexity. • How low can you go? • Replace one-hot encoding with learnable embeddings. Does this lead to better performance? • How well will it work on other books by H. G. Wells, e.g., The War of the Worlds109?
109 http://www.gutenberg.org/ebooks/36

334

Chapter 8. Recurrent Neural Networks

3. Modify the prediction function such as to use sampling rather than picking the most likely next character. • What happens? • Bias the model towards more likely outputs, e.g., by sampling from q(xt | xt−1, . . . , x1) ∝ P (xt | xt−1, . . . , x1)α for α > 1.
4. Run the code in this section without clipping the gradient. What happens? 5. Change sequential partitioning so that it does not separate hidden states from the computa-
tional graph. Does the running time change? How about the perplexity? 6. Replace the activation function used in this section with ReLU and repeat the experiments
in this section. Do we still need gradient clipping? Why? Discussions110
8.6 Concise Implementation of Recurrent Neural Networks
While Section 8.5 was instructive to see how RNNs are implemented, this is not convenient or fast. This section will show how to implement the same language model more efficiently using functions provided by high-level APIs of a deep learning framework. We begin as before by reading the time machine dataset.
from mxnet import np, npx from mxnet.gluon import nn, rnn from d2l import mxnet as d2l
npx.set_np()
batch_size, num_steps = 32, 35 train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)
8.6.1 Defining the Model
High-level APIs provide implementations of recurrent neural networks. We construct the recurrent neural network layer rnn_layer with a single hidden layer and 256 hidden units. In fact, we have not even discussed yet what it means to have multiple layers—this will happen in Section 9.3. For now, suffice it to say that multiple layers simply amount to the output of one layer of RNN being used as the input for the next layer of RNN.
num_hiddens = 256 rnn_layer = rnn.RNN(num_hiddens) rnn_layer.initialize()
Initializing the hidden state is straightforward. We invoke the member function begin_state. This returns a list (state) that contains an initial hidden state for each example in the minibatch, whose shape is (number of hidden layers, batch size, number of hidden units). For some models to be introduced later (e.g., long short-term memory), such a list also contains other information.
110 https://discuss.d2l.ai/t/336

8.6. Concise Implementation of Recurrent Neural Networks

335

state = rnn_layer.begin_state(batch_size=batch_size) len(state), state[0].shape
(1, (1, 32, 256))
With a hidden state and an input, we can compute the output with the updated hidden state. It should be emphasized that the “output” (Y) of rnn_layer does not involve computation of output layers: it refers to the hidden state at each time step, and they can be used as the input to the subsequent output layer.
Besides, the updated hidden state (state_new) returned by rnn_layer refers to the hidden state at the last time step of the minibatch. It can be used to initialize the hidden state for the next minibatch within an epoch in sequential partitioning. For multiple hidden layers, the hidden state of each layer will be stored in this variable (state_new). For some models to be introduced later (e.g., long short-term memory), this variable also contains other information.
X = np.random.uniform(size=(num_steps, batch_size, len(vocab))) Y, state_new = rnn_layer(X, state) Y.shape, len(state_new), state_new[0].shape
((35, 32, 256), 1, (1, 32, 256))
Similar to Section 8.5, we define an RNNModel class for a complete RNN model. Note that rnn_layer only contains the hidden recurrent layers, we need to create a separate output layer.
#@save class RNNModel(nn.Block):
"""The RNN model.""" def __init__(self, rnn_layer, vocab_size, **kwargs):
super(RNNModel, self).__init__(**kwargs) self.rnn = rnn_layer self.vocab_size = vocab_size self.dense = nn.Dense(vocab_size)
def forward(self, inputs, state): X = npx.one_hot(inputs.T, self.vocab_size) Y, state = self.rnn(X, state) # The fully-connected layer will first change the shape of `Y` to # (`num_steps` * `batch_size`, `num_hiddens`). Its output shape is # (`num_steps` * `batch_size`, `vocab_size`). output = self.dense(Y.reshape(-1, Y.shape[-1])) return output, state
def begin_state(self, *args, **kwargs): return self.rnn.begin_state(*args, **kwargs)

336

Chapter 8. Recurrent Neural Networks

8.6.2 Training and Predicting
Before training the model, let us make a prediction with the a model that has random weights.
device = d2l.try_gpu() net = RNNModel(rnn_layer, len(vocab)) net.initialize(force_reinit=True, ctx=device) d2l.predict_ch8('time traveller', 10, net, vocab, device)
'time travellervmoopwrrrr'
As is quite obvious, this model does not work at all. Next, we call train_ch8 with the same hyperparameters defined in Section 8.5 and train our model with high-level APIs.
num_epochs, lr = 500, 1 d2l.train_ch8(net, train_iter, vocab, lr, num_epochs, device)
perplexity 1.2, 153206.6 tokens/sec on gpu(0) time traveller after thereats ended and hime seet and of the thr traveller after thereats expect us the great difficulte tra

Compared with the last section, this model achieves comparable perplexity, albeit within a shorter period of time, due to the code being more optimized by high-level APIs of the deep learning framework.
Summary
• High-level APIs of the deep learning framework provides an implementation of the RNN layer.
• The RNN layer of high-level APIs returns an output and an updated hidden state, where the output does not involve output layer computation.
• Using high-level APIs leads to faster RNN training than using its implementation from scratch.

8.6. Concise Implementation of Recurrent Neural Networks

337

Exercises
1. Can you make the RNN model overfit using the high-level APIs?
2. What happens if you increase the number of hidden layers in the RNN model? Can you make the model work?
3. Implement the autoregressive model of Section 8.1 using an RNN. Discussions111
8.7 Backpropagation Through Time
So far we have repeatedly alluded to things like exploding gradients, vanishing gradients, and the need to detach the gradient for RNNs. For instance, in Section 8.5 we invoked the detach function on the sequence. None of this was really fully explained, in the interest of being able to build a model quickly and to see how it works. In this section, we will delve a bit more deeply into the details of backpropagation for sequence models and why (and how) the mathematics works.
We encountered some of the effects of gradient explosion when we first implemented RNNs (Section 8.5). In particular, if you solved the exercises, you would have seen that gradient clipping is vital to ensure proper convergence. To provide a better understanding of this issue, this section will review how gradients are computed for sequence models. Note that there is nothing conceptually new in how it works. After all, we are still merely applying the chain rule to compute gradients. Nonetheless, it is worth while reviewing backpropagation (Section 4.7) again.
We have described forward and backward propagations and computational graphs in MLPs in Section 4.7. Forward propagation in an RNN is relatively straightforward. Backpropagation through time is actually a specific application of backpropagation in RNNs (Werbos, 1990). It requires us to expand the computational graph of an RNN one time step at a time to obtain the dependencies among model variables and parameters. Then, based on the chain rule, we apply backpropagation to compute and store gradients. Since sequences can be rather long, the dependency can be rather lengthy. For instance, for a sequence of 1000 characters, the first token could potentially have significant influence on the token at the final position. This is not really computationally feasible (it takes too long and requires too much memory) and it requires over 1000 matrix products before we would arrive at that very elusive gradient. This is a process fraught with computational and statistical uncertainty. In the following we will elucidate what happens and how to address this in practice.
8.7.1 Analysis of Gradients in RNNs
We start with a simplified model of how an RNN works. This model ignores details about the specifics of the hidden state and how it is updated. The mathematical notation here does not explicitly distinguish scalars, vectors, and matrices as it used to do. These details are immaterial to the analysis and would only serve to clutter the notation in this subsection.
In this simplified model, we denote ht as the hidden state, xt as the input, and ot as the output at time step t. Recall our discussions in Section 8.4.2 that the input and the hidden state can be concatenated to be multiplied by one weight variable in the hidden layer. Thus, we use wh and
111 https://discuss.d2l.ai/t/335

338

Chapter 8. Recurrent Neural Networks

wo to indicate the weights of the hidden layer and the output layer, respectively. As a result, the hidden states and outputs at each time steps can be explained as

ht = f (xt, ht−1, wh), ot = g(ht, wo),

(8.7.1)

where f and g are transformations of the hidden layer and the output layer, respectively. Hence, we have a chain of values {. . . , (xt−1, ht−1, ot−1), (xt, ht, ot), . . .} that depend on each other via recurrent computation. The forward propagation is fairly straightforward. All we need is to loop through the (xt, ht, ot) triples one time step at a time. The discrepancy between output ot and the desired label yt is then evaluated by an objective function across all the T time steps as

1 ∑ T L(x1, . . . , xT , y1, . . . , yT , wh, wo) = T l(yt, ot).
t=1

(8.7.2)

For backpropagation, matters are a bit trickier, especially when we compute the gradients with regard to the parameters wh of the objective function L. To be specific, by the chain rule,

∂L 1 ∑ T ∂l(yt, ot) =
∂wh T t=1 ∂wh

1 ∑ T ∂l(yt, ot) ∂g(ht, wo) ∂ht

=

.

T t=1 ∂ot

∂ht ∂wh

(8.7.3)

The first and the second factors of the product in (8.7.3) are easy to compute. The third factor ∂ht/∂wh is where things get tricky, since we need to recurrently compute the effect of the parameter wh on ht. According to the recurrent computation in (8.7.1), ht depends on both ht−1 and wh, where computation of ht−1 also depends on wh. Thus, using the chain rule yields

∂ht = ∂f (xt, ht−1, wh) + ∂f (xt, ht−1, wh) ∂ht−1 .

∂wh

∂wh

∂ht−1

∂wh

(8.7.4)

To derive the above gradient, assume that we have three sequences {at}, {bt}, {ct} satisfying a0 = 0

and at = bt + ctat−1 for t = 1, 2, . . .. Then for t ≥ 1, it is easy to show





∑ t−1 ∏t

at = bt + 

cj bi.

(8.7.5)

i=1 j=i+1

By substituting at, bt, and ct according to

at = ∂ht , ∂wh
bt = ∂f (xt, ht−1, wh) , ∂wh
ct = ∂f (xt, ht−1, wh) , ∂ht−1

(8.7.6)

the gradient computation in (8.7.4) satisfies at = bt + ctat−1. Thus, per (8.7.5), we can remove the

recurrent computation in (8.7.4) with





∂ht

=

∂f (xt, ht−1, wh)

∑ t−1 +

∏t

∂f (xj, hj−1, wh)  ∂f (xi, hi−1, wh) .

∂wh

∂wh

i=1 j=i+1

∂hj−1

∂wh

(8.7.7)

While we can use the chain rule to compute ∂ht/∂wh recursively, this chain can get very long whenever t is large. Let us discuss a number of strategies for dealing with this problem.

8.7. Backpropagation Through Time

339

Full Computation
Obviously, we can just compute the full sum in (8.7.7). However, this is very slow and gradients can blow up, since subtle changes in the initial conditions can potentially affect the outcome a lot. That is, we could see things similar to the butterfly effect where minimal changes in the initial conditions lead to disproportionate changes in the outcome. This is actually quite undesirable in terms of the model that we want to estimate. After all, we are looking for robust estimators that generalize well. Hence this strategy is almost never used in practice.

Truncating Time Steps
Alternatively, we can truncate the sum in (8.7.7) after τ steps. This is what we have been discussing so far, such as when we detached the gradients in Section 8.5. This leads to an approximation of the true gradient, simply by terminating the sum at ∂ht−τ /∂wh. In practice this works quite well. It is what is commonly referred to as truncated backpropgation through time (Jaeger, 2002). One of the consequences of this is that the model focuses primarily on short-term influence rather than long-term consequences. This is actually desirable, since it biases the estimate towards simpler and more stable models.

Randomized Truncation

Last, we can replace ∂ht/∂wh by a random variable which is correct in expectation but truncates
the sequence. This is achieved by using a sequence of ξt with predefined 0 ≤ πt ≤ 1, where P (ξt = 0) = 1 − πt and P (ξt = πt−1) = πt, thus E[ξt] = 1. We use this to replace the gradient ∂ht/∂wh in (8.7.4) with

zt = ∂f (xt, ht−1, wh) + ξt ∂f (xt, ht−1, wh) ∂ht−1 .

∂wh

∂ht−1

∂wh

(8.7.8)

It follows from the definition of ξt that E[zt] = ∂ht/∂wh. Whenever ξt = 0 the recurrent computation terminates at that time step t. This leads to a weighted sum of sequences of varying lengths where long sequences are rare but appropriately overweighted. This idea was proposed by Tallec and Ollivier (Tallec & Ollivier, 2017).

Comparing Strategies

Fig. 8.7.1: Comparing strategies for computing gradients in RNNs. From top to bottom: randomized truncation, regular truncation, and full computation.
Fig. 8.7.1 illustrates the three strategies when analyzing the first few characters of The Time Machine book using backpropagation through time for RNNs:

340

Chapter 8. Recurrent Neural Networks

• The first row is the randomized truncation that partitions the text into segments of varying lengths.
• The second row is the regular truncation that breaks the text into subsequences of the same length. This is what we have been doing in RNN experiments.
• The third row is the full backpropagation through time that leads to a computationally infeasible expression.
Unfortunately, while appealing in theory, randomized truncation does not work much better than regular truncation, most likely due to a number of factors. First, the effect of an observation after a number of backpropagation steps into the past is quite sufficient to capture dependencies in practice. Second, the increased variance counteracts the fact that the gradient is more accurate with more steps. Third, we actually want models that have only a short range of interactions. Hence, regularly truncated backpropagation through time has a slight regularizing effect that can be desirable.

8.7.2 Backpropagation Through Time in Detail

After discussing the general principle, let us discuss backpropagation through time in detail. Different from the analysis in Section 8.7.1, in the following we will show how to compute the gradients of the objective function with respect to all the decomposed model parameters. To keep things simple, we consider an RNN without bias parameters, whose activation function in the hidden layer uses the identity mapping (ϕ(x) = x). For time step t, let the single example input and the label be xt ∈ Rd and yt, respectively. The hidden state ht ∈ Rh and the output ot ∈ Rq are computed as

ht = Whxxt + Whhht−1, ot = Wqhht,

(8.7.9)

where Whx ∈ Rh×d, Whh ∈ Rh×h, and Wqh ∈ Rq×h are the weight parameters. Denote by l(ot, yt) the loss at time step t. Our objective function, the loss over T time steps from the beginning of the sequence is thus

1 ∑ T

L= T

l(ot, yt).

t=1

(8.7.10)

In order to visualize the dependencies among model variables and parameters during computation of the RNN, we can draw a computational graph for the model, as shown in Fig. 8.7.2. For example, the computation of the hidden states of time step 3, h3, depends on the model parameters Whx and Whh, the hidden state of the last time step h2, and the input of the current time step x3.

8.7. Backpropagation Through Time

341

Fig. 8.7.2: Computational graph showing dependencies for an RNN model with three time steps. Boxes represent variables (not shaded) or parameters (shaded) and circles represent operators.

As just mentioned, the model parameters in Fig. 8.7.2 are Whx, Whh, and Wqh. Generally, training this model requires gradient computation with respect to these parameters ∂L/∂Whx, ∂L/∂Whh, and ∂L/∂Wqh. According to the dependencies in Fig. 8.7.2, we can traverse in the opposite direction of the arrows to calculate and store the gradients in turn. To flexibly express the multiplication of matrices, vectors, and scalars of different shapes in the chain rule, we continue to use the prod operator as described in Section 4.7.
First of all, differentiating the objective function with respect to the model output at any time step t is fairly straightforward:

∂L = ∂l(ot, yt) ∈ Rq. ∂ot T · ∂ot

(8.7.11)

Now, we can calculate the gradient of the objective function with respect to the parameter Wqh in the output layer: ∂L/∂Wqh ∈ Rq×h. Based on Fig. 8.7.2, the objective function L depends on Wqh
via o1, . . . , oT . Using the chain rule yields

∂L ∑ T

( ∂L

) ∂ot

∑ T ∂L

= ∂Wqh

prod ∂ot , ∂Wqh =

∂ot h⊤t ,

t=1

t=1

(8.7.12)

where ∂L/∂ot is given by (8.7.11).

Next, as shown in Fig. 8.7.2, at the final time step T the objective function L depends on the hidden state hT only via oT . Therefore, we can easily find the gradient ∂L/∂hT ∈ Rh using the chain rule:

(

)

∂L = prod

∂L , ∂oT

= W⊤

∂L .

∂hT

∂oT ∂hT

qh ∂oT

(8.7.13)

It gets trickier for any time step t < T , where the objective function L depends on ht via ht+1 and ot. According to the chain rule, the gradient of the hidden state ∂L/∂ht ∈ Rh at any time step

t < T can be recurrently computed as:

(

)

(

)

∂L = prod

∂L , ∂ht+1

+ prod

∂L , ∂ot

= W⊤

∂L

+ W⊤

∂L .

∂ht

∂ht+1 ∂ht

∂ot ∂ht

hh ∂ht+1

qh ∂ot

(8.7.14)

For analysis, expanding the recurrent computation for any time step 1 ≤ t ≤ T gives

∂L ∑ T ( )T −i

∂L

= ∂ht

W⊤hh

W⊤qh ∂oT +t−i .

i=t

(8.7.15)

342

Chapter 8. Recurrent Neural Networks

We can see from (8.7.15) that this simple linear example already exhibits some key problems of long sequence models: it involves potentially very large powers of W⊤hh. In it, eigenvalues smaller than 1 vanish and eigenvalues larger than 1 diverge. This is numerically unstable, which manifests itself in the form of vanishing and exploding gradients. One way to address this is to truncate the time steps at a computationally convenient size as discussed in Section 8.7.1. In practice, this truncation is effected by detaching the gradient after a given number of time steps. Later on we will see how more sophisticated sequence models such as long short-term memory can alleviate this further.

Finally, Fig. 8.7.2 shows that the objective function L depends on model parameters Whx and Whh
in the hidden layer via hidden states h1, . . . , hT . To compute gradients with respect to such parameters ∂L/∂Whx ∈ Rh×d and ∂L/∂Whh ∈ Rh×h, we apply the chain rule that gives

∂L ∑ T

( ∂L

) ∂ht

∑ T ∂L

= ∂Whx

prod ∂ht , ∂Whx =

∂ht x⊤t ,

t=1

t=1

∂L ∑ T

( ∂L

) ∂ht

∑ T ∂L

= ∂Whh

prod ∂ht , ∂Whh =

∂ht h⊤t−1,

t=1

t=1

(8.7.16)

where ∂L/∂ht that is recurrently computed by (8.7.13) and (8.7.14) is the key quantity that affects the numerical stability.

Since backpropagation through time is the application of backpropagation in RNNs, as we have explained in Section 4.7, training RNNs alternates forward propagation with backpropagation through time. Besides, backpropagation through time computes and stores the above gradients in turn. Specifically, stored intermediate values are reused to avoid duplicate calculations, such as storing ∂L/∂ht to be used in computation of both ∂L/∂Whx and ∂L/∂Whh.

Summary
• Backpropagation through time is merely an application of backpropagation to sequence models with a hidden state.
• Truncation is needed for computational convenience and numerical stability, such as regular truncation and randomized truncation.
• High powers of matrices can lead to divergent or vanishing eigenvalues. This manifests itself in the form of exploding or vanishing gradients.
• For efficient computation, intermediate values are cached during backpropagation through time.

Exercises
1. Assume that we have a symmetric matrix M ∈ Rn×n with eigenvalues λi whose corresponding eigenvectors are vi (i = 1, . . . , n). Without loss of generality, assume that they are ordered in the order |λi| ≥ |λi+1|. 1. Show that Mk has eigenvalues λki . 2. Prove that for a random vector x ∈ Rn, with high probability Mkx will be very much aligned with the eigenvector v1 of M. Formalize this statement.
3. What does the above result mean for gradients in RNNs?

8.7. Backpropagation Through Time

343

2. Besides gradient clipping, can you think of any other methods to cope with gradient explosion in recurrent neural networks?
Discussions112

112 https://discuss.d2l.ai/t/334
344

Chapter 8. Recurrent Neural Networks

9 | Modern Recurrent Neural Networks
We have introduced the basics of RNNs, which can better handle sequence data. For demonstration, we implemented RNN-based language models on text data. However, such techniques may not be sufficient for practitioners when they face a wide range of sequence learning problems nowadays.
For instance, a notable issue in practice is the numerical instability of RNNs. Although we have applied implementation tricks such as gradient clipping, this issue can be alleviated further with more sophisticated designs of sequence models. Specifically, gated RNNs are much more common in practice. We will begin by introducing two of such widely-used networks, namely gated recurrent units (GRUs) and long short-term memory (LSTM). Furthermore, we will expand the RNN architecture with a single unidirectional hidden layer that has been discussed so far. We will describe deep architectures with multiple hidden layers, and discuss the bidirectional design with both forward and backward recurrent computations. Such expansions are frequently adopted in modern recurrent networks. When explaining these RNN variants, we continue to consider the same language modeling problem introduced in Chapter 8.
In fact, language modeling reveals only a small fraction of what sequence learning is capable of. In a variety of sequence learning problems, such as automatic speech recognition, text to speech, and machine translation, both inputs and outputs are sequences of arbitrary length. To explain how to fit this type of data, we will take machine translation as an example, and introduce the encoder-decoder architecture based on RNNs and beam search for sequence generation.
9.1 Gated Recurrent Units (GRU)
In Section 8.7, we discussed how gradients are calculated in RNNs. In particular we found that long products of matrices can lead to vanishing or exploding gradients. Let us briefly think about what such gradient anomalies mean in practice:
• We might encounter a situation where an early observation is highly significant for predicting all future observations. Consider the somewhat contrived case where the first observation contains a checksum and the goal is to discern whether the checksum is correct at the end of the sequence. In this case, the influence of the first token is vital. We would like to have some mechanisms for storing vital early information in a memory cell. Without such a mechanism, we will have to assign a very large gradient to this observation, since it affects all the subsequent observations.
• We might encounter situations where some tokens carry no pertinent observation. For instance, when parsing a web page there might be auxiliary HTML code that is irrelevant for the purpose of assessing the sentiment conveyed on the page. We would like to have some mechanism for skipping such tokens in the latent state representation.
345

• We might encounter situations where there is a logical break between parts of a sequence. For instance, there might be a transition between chapters in a book, or a transition between a bear and a bull market for securities. In this case it would be nice to have a means of resetting our internal state representation.
A number of methods have been proposed to address this. One of the earliest is long short-term memory (Hochreiter & Schmidhuber, 1997) which we will discuss in Section 9.2. The gated recurrent unit (GRU) (Cho et al., 2014a) is a slightly more streamlined variant that often offers comparable performance and is significantly faster to compute (Chung et al., 2014). Due to its simplicity, let us start with the GRU.
9.1.1 Gated Hidden State
The key distinction between vanilla RNNs and GRUs is that the latter support gating of the hidden state. This means that we have dedicated mechanisms for when a hidden state should be updated and also when it should be reset. These mechanisms are learned and they address the concerns listed above. For instance, if the first token is of great importance we will learn not to update the hidden state after the first observation. Likewise, we will learn to skip irrelevant temporary observations. Last, we will learn to reset the latent state whenever needed. We discuss this in detail below.
Reset Gate and Update Gate
The first thing we need to introduce are the reset gate and the update gate. We engineer them to be vectors with entries in (0, 1) such that we can perform convex combinations. For instance, a reset gate would allow us to control how much of the previous state we might still want to remember. Likewise, an update gate would allow us to control how much of the new state is just a copy of the old state.
We begin by engineering these gates. Fig. 9.1.1 illustrates the inputs for both the reset and update gates in a GRU, given the input of the current time step and the hidden state of the previous time step. The outputs of two gates are given by two fully-connected layers with a sigmoid activation function.

346

Chapter 9. Modern Recurrent Neural Networks

Fig. 9.1.1: Computing the reset gate and the update gate in a GRU model.

Mathematically, for a given time step t, suppose that the input is a minibatch Xt ∈ Rn×d (number of examples: n, number of inputs: d) and the hidden state of the previous time step is Ht−1 ∈ Rn×h (number of hidden units: h). Then, the reset gate Rt ∈ Rn×h and update gate Zt ∈ Rn×h are
computed as follows:

Rt = σ(XtWxr + Ht−1Whr + br), Zt = σ(XtWxz + Ht−1Whz + bz),

(9.1.1)

where Wxr, Wxz ∈ Rd×h and Whr, Whz ∈ Rh×h are weight parameters and br, bz ∈ R1×h are biases. Note that broadcasting (see Section 2.1.3) is triggered during the summation. We use sigmoid functions (as introduced in Section 4.1) to transform input values to the interval (0, 1).

Candidate Hidden State

Next, let us integrate the reset gate Rt with the regular latent state updating mechanism in (8.4.5). It leads to the following candidate hidden state H˜ t ∈ Rn×h at time step t:

H˜ t = tanh(XtWxh + (Rt ⊙ Ht−1) Whh + bh),

(9.1.2)

where Wxh ∈ Rd×h and Whh ∈ Rh×h are weight parameters, bh ∈ R1×h is the bias, and the symbol ⊙ is the Hadamard (elementwise) product operator. Here we use a nonlinearity in the form of tanh to ensure that the values in the candidate hidden state remain in the interval (−1, 1).
The result is a candidate since we still need to incorporate the action of the update gate. Comparing with (8.4.5), now the influence of the previous states can be reduced with the elementwise multiplication of Rt and Ht−1 in (9.1.2). Whenever the entries in the reset gate Rt are close to 1, we recover a vanilla RNN such as in (8.4.5). For all entries of the reset gate Rt that are close to 0, the candidate hidden state is the result of an MLP with Xt as the input. Any pre-existing hidden state is thus reset to defaults.
Fig. 9.1.2 illustrates the computational flow after applying the reset gate.

9.1. Gated Recurrent Units (GRU)

347

Fig. 9.1.2: Computing the candidate hidden state in a GRU model.

Hidden State

Finally, we need to incorporate the effect of the update gate Zt. This determines the extent to which
the new hidden state Ht ∈ Rn×h is just the old state Ht−1 and by how much the new candidate state H˜ t is used. The update gate Zt can be used for this purpose, simply by taking elementwise convex combinations between both Ht−1 and H˜ t. This leads to the final update equation for the GRU:

Ht = Zt ⊙ Ht−1 + (1 − Zt) ⊙ H˜ t.

(9.1.3)

Whenever the update gate Zt is close to 1, we simply retain the old state. In this case the information from Xt is essentially ignored, effectively skipping time step t in the dependency chain. In contrast, whenever Zt is close to 0, the new latent state Ht approaches the candidate latent state H˜ t. These designs can help us cope with the vanishing gradient problem in RNNs and better capture dependencies for sequences with large time step distances. For instance, if the update gate has been close to 1 for all the time steps of an entire subsequence, the old hidden state at the time step of its beginning will be easily retained and passed to its end, regardless of the length of the subsequence.
Fig. 9.1.3 illustrates the computational flow after the update gate is in action.

348

Chapter 9. Modern Recurrent Neural Networks

Fig. 9.1.3: Computing the hidden state in a GRU model.
In summary, GRUs have the following two distinguishing features: • Reset gates help capture short-term dependencies in sequences. • Update gates help capture long-term dependencies in sequences.
9.1.2 Implementation from Scratch
To gain a better understanding of the GRU model, let us implement it from scratch. We begin by reading the time machine dataset that we used in Section 8.5. The code for reading the dataset is given below.
from mxnet import np, npx from mxnet.gluon import rnn from d2l import mxnet as d2l
npx.set_np()
batch_size, num_steps = 32, 35 train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)

Initializing Model Parameters
The next step is to initialize the model parameters. We draw the weights from a Gaussian distribution with standard deviation to be 0.01 and set the bias to 0. The hyperparameter num_hiddens defines the number of hidden units. We instantiate all weights and biases relating to the update gate, the reset gate, the candidate hidden state, and the output layer.
def get_params(vocab_size, num_hiddens, device): num_inputs = num_outputs = vocab_size
(continues on next page)

9.1. Gated Recurrent Units (GRU)

349

def normal(shape): return np.random.normal(scale=0.01, size=shape, ctx=device)

(continued from previous page)

def three(): return (normal( (num_inputs, num_hiddens)), normal( (num_hiddens, num_hiddens)), np.zeros(num_hiddens, ctx=device))

W_xz, W_hz, b_z = three() # Update gate parameters W_xr, W_hr, b_r = three() # Reset gate parameters W_xh, W_hh, b_h = three() # Candidate hidden state parameters # Output layer parameters W_hq = normal((num_hiddens, num_outputs)) b_q = np.zeros(num_outputs, ctx=device) # Attach gradients params = [W_xz, W_hz, b_z, W_xr, W_hr, b_r, W_xh, W_hh, b_h, W_hq, b_q] for param in params:
param.attach_grad() return params

Defining the Model
Now we will define the hidden state initialization function init_gru_state. Just like the init_rnn_state function defined in Section 8.5, this function returns a tensor with a shape (batch size, number of hidden units) whose values are all zeros.
def init_gru_state(batch_size, num_hiddens, device): return (np.zeros(shape=(batch_size, num_hiddens), ctx=device),)
Now we are ready to define the GRU model. Its structure is the same as that of the basic RNN cell, except that the update equations are more complex.
def gru(inputs, state, params): W_xz, W_hz, b_z, W_xr, W_hr, b_r, W_xh, W_hh, b_h, W_hq, b_q = params H, = state outputs = [] for X in inputs: Z = npx.sigmoid(np.dot(X, W_xz) + np.dot(H, W_hz) + b_z) R = npx.sigmoid(np.dot(X, W_xr) + np.dot(H, W_hr) + b_r) H_tilda = np.tanh(np.dot(X, W_xh) + np.dot(R * H, W_hh) + b_h) H = Z * H + (1 - Z) * H_tilda Y = np.dot(H, W_hq) + b_q outputs.append(Y) return np.concatenate(outputs, axis=0), (H,)

350

Chapter 9. Modern Recurrent Neural Networks

Training and Predicting
Training and prediction work in exactly the same manner as in Section 8.5. After training, we print out the perplexity on the training set and the predicted sequence following the provided prefixes “time traveller” and “traveller”, respectively.
vocab_size, num_hiddens, device = len(vocab), 256, d2l.try_gpu() num_epochs, lr = 500, 1 model = d2l.RNNModelScratch(len(vocab), num_hiddens, device, get_params,
init_gru_state, gru) d2l.train_ch8(model, train_iter, vocab, lr, num_epochs, device)
perplexity 1.2, 11939.7 tokens/sec on gpu(0) time traveller for so it will be convenient to speak of himwas e travelleryou conly ubout awouknet s all hud i jeen aimerede

9.1.3 Concise Implementation
In high-level APIs, we can directly instantiate a GPU model. This encapsulates all the configuration detail that we made explicit above. The code is significantly faster as it uses compiled operators rather than Python for many details that we spelled out before.
gru_layer = rnn.GRU(num_hiddens) model = d2l.RNNModel(gru_layer, len(vocab)) d2l.train_ch8(model, train_iter, vocab, lr, num_epochs, device)
perplexity 1.1, 160475.5 tokens/sec on gpu(0) time traveller for so it will be convenient to speak of himwas e traveller with a slight accession ofcheerfulness really thi

9.1. Gated Recurrent Units (GRU)

351

Summary
• Gated RNNs can better capture dependencies for sequences with large time step distances. • Reset gates help capture short-term dependencies in sequences. • Update gates help capture long-term dependencies in sequences. • GRUs contain basic RNNs as their extreme case whenever the reset gate is switched on. They
can also skip subsequences by turning on the update gate.
Exercises
1. Assume that we only want to use the input at time step t′ to predict the output at time step t > t′. What are the best values for the reset and update gates for each time step?
2. Adjust the hyperparameters and analyze the their influence on running time, perplexity, and the output sequence.
3. Compare runtime, perplexity, and the output strings for rnn.RNN and rnn.GRU implementations with each other.
4. What happens if you implement only parts of a GRU, e.g., with only a reset gate or only an update gate?
Discussions113
9.2 Long Short-Term Memory (LSTM)
The challenge to address long-term information preservation and short-term input skipping in latent variable models has existed for a long time. One of the earliest approaches to address this was the long short-term memory (LSTM) (Hochreiter & Schmidhuber, 1997). It shares many of the properties of the GRU. Interestingly, LSTMs have a slightly more complex design than GRUs but predates GRUs by almost two decades.
113 https://discuss.d2l.ai/t/342

352

Chapter 9. Modern Recurrent Neural Networks

9.2.1 Gated Memory Cell
Arguably LSTMʼs design is inspired by logic gates of a computer. LSTM introduces a memory cell (or cell for short) that has the same shape as the hidden state (some literatures consider the memory cell as a special type of the hidden state), engineered to record additional information. To control the memory cell we need a number of gates. One gate is needed to read out the entries from the cell. We will refer to this as the output gate. A second gate is needed to decide when to read data into the cell. We refer to this as the input gate. Last, we need a mechanism to reset the content of the cell, governed by a forget gate. The motivation for such a design is the same as that of GRUs, namely to be able to decide when to remember and when to ignore inputs in the hidden state via a dedicated mechanism. Let us see how this works in practice.
Input Gate, Forget Gate, and Output Gate
Just like in GRUs, the data feeding into the LSTM gates are the input at the current time step and the hidden state of the previous time step, as illustrated in Fig. 9.2.1. They are processed by three fullyconnected layers with a sigmoid activation function to compute the values of the input, forget. and output gates. As a result, values of the three gates are in the range of (0, 1).

Fig. 9.2.1: Computing the input gate, the forget gate, and the output gate in an LSTM model.

Mathematically, suppose that there are h hidden units, the batch size is n, and the number of
inputs is d. Thus, the input is Xt ∈ Rn×d and the hidden state of the previous time step is Ht−1 ∈ Rn×h. Correspondingly, the gates at time step t are defined as follows: the input gate is It ∈ Rn×h, the forget gate is Ft ∈ Rn×h, and the output gate is Ot ∈ Rn×h. They are calculated as follows:

It = σ(XtWxi + Ht−1Whi + bi), Ft = σ(XtWxf + Ht−1Whf + bf ), Ot = σ(XtWxo + Ht−1Who + bo),

(9.2.1)

where Wxi, Wxf , Wxo ∈ Rd×h and Whi, Whf , Who ∈ Rh×h are weight parameters and bi, bf , bo ∈ R1×h are bias parameters.

9.2. Long Short-Term Memory (LSTM)

353

Candidate Memory Cell

Next we design the memory cell. Since we have not specified the action of the various gates yet, we first introduce the candidate memory cell C˜t ∈ Rn×h. Its computation is similar to that of the three
gates described above, but using a tanh function with a value range for (−1, 1) as the activation
function. This leads to the following equation at time step t:

C˜t = tanh(XtWxc + Ht−1Whc + bc),

(9.2.2)

where Wxc ∈ Rd×h and Whc ∈ Rh×h are weight parameters and bc ∈ R1×h is a bias parameter. A quick illustration of the candidate memory cell is shown in Fig. 9.2.2.

Fig. 9.2.2: Computing the candidate memory cell in an LSTM model.

Memory Cell

In GRUs, we have a mechanism to govern input and forgetting (or skipping). Similarly, in LSTMs
we have two dedicated gates for such purposes: the input gate It governs how much we take new data into account via C˜t and the forget gate Ft addresses how much of the old memory cell content Ct−1 ∈ Rn×h we retain. Using the same pointwise multiplication trick as before, we arrive at the
following update equation:

Ct = Ft ⊙ Ct−1 + It ⊙ C˜t.

(9.2.3)

If the forget gate is always approximately 1 and the input gate is always approximately 0, the past memory cells Ct−1 will be saved over time and passed to the current time step. This design is introduced to alleviate the vanishing gradient problem and to better capture long range dependencies within sequences.
We thus arrive at the flow diagram in Fig. 9.2.3.

354

Chapter 9. Modern Recurrent Neural Networks

Fig. 9.2.3: Computing the memory cell in an LSTM model.

Hidden State

Last, we need to define how to compute the hidden state Ht ∈ Rn×h. This is where the output gate comes into play. In LSTM it is simply a gated version of the tanh of the memory cell. This ensures that the values of Ht are always in the interval (−1, 1).

Ht = Ot ⊙ tanh(Ct).

(9.2.4)

Whenever the output gate approximates 1 we effectively pass all memory information through to the predictor, whereas for the output gate close to 0 we retain all the information only within the memory cell and perform no further processing.
Fig. 9.2.4 has a graphical illustration of the data flow.

Fig. 9.2.4: Computing the hidden state in an LSTM model.

9.2. Long Short-Term Memory (LSTM)

355

9.2.2 Implementation from Scratch
Now let us implement an LSTM from scratch. As same as the experiments in Section 8.5, we first load the time machine dataset.
from mxnet import np, npx from mxnet.gluon import rnn from d2l import mxnet as d2l
npx.set_np()
batch_size, num_steps = 32, 35 train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)
Initializing Model Parameters
Next we need to define and initialize the model parameters. As previously, the hyperparameter num_hiddens defines the number of hidden units. We initialize weights following a Gaussian distribution with 0.01 standard deviation, and we set the biases to 0.
def get_lstm_params(vocab_size, num_hiddens, device): num_inputs = num_outputs = vocab_size
def normal(shape): return np.random.normal(scale=0.01, size=shape, ctx=device)
def three(): return (normal( (num_inputs, num_hiddens)), normal( (num_hiddens, num_hiddens)), np.zeros(num_hiddens, ctx=device))
W_xi, W_hi, b_i = three() # Input gate parameters W_xf, W_hf, b_f = three() # Forget gate parameters W_xo, W_ho, b_o = three() # Output gate parameters W_xc, W_hc, b_c = three() # Candidate memory cell parameters # Output layer parameters W_hq = normal((num_hiddens, num_outputs)) b_q = np.zeros(num_outputs, ctx=device) # Attach gradients params = [
W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o, W_xc, W_hc, b_c, W_hq, b_q] for param in params: param.attach_grad() return params

356

Chapter 9. Modern Recurrent Neural Networks

Defining the Model
In the initialization function, the hidden state of the LSTM needs to return an additional memory cell with a value of 0 and a shape of (batch size, number of hidden units). Hence we get the following state initialization.
def init_lstm_state(batch_size, num_hiddens, device): return (np.zeros((batch_size, num_hiddens), ctx=device), np.zeros((batch_size, num_hiddens), ctx=device))
The actual model is defined just like what we discussed before: providing three gates and an auxiliary memory cell. Note that only the hidden state is passed to the output layer. The memory cell Ct does not directly participate in the output computation.
def lstm(inputs, state, params): [ W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o, W_xc, W_hc, b_c, W_hq, b_q] = params (H, C) = state outputs = [] for X in inputs: I = npx.sigmoid(np.dot(X, W_xi) + np.dot(H, W_hi) + b_i) F = npx.sigmoid(np.dot(X, W_xf) + np.dot(H, W_hf) + b_f) O = npx.sigmoid(np.dot(X, W_xo) + np.dot(H, W_ho) + b_o) C_tilda = np.tanh(np.dot(X, W_xc) + np.dot(H, W_hc) + b_c) C = F * C + I * C_tilda H = O * np.tanh(C) Y = np.dot(H, W_hq) + b_q outputs.append(Y) return np.concatenate(outputs, axis=0), (H, C)
Training and Prediction
Let us train an LSTM as same as what we did in Section 9.1, by instantiating the RNNModelScratch class as introduced in Section 8.5.
vocab_size, num_hiddens, device = len(vocab), 256, d2l.try_gpu() num_epochs, lr = 500, 1 model = d2l.RNNModelScratch(len(vocab), num_hiddens, device, get_lstm_params,
init_lstm_state, lstm) d2l.train_ch8(model, train_iter, vocab, lr, num_epochs, device)
perplexity 1.1, 10626.9 tokens/sec on gpu(0) time travelleryou can show black is white by argument said filby travelleryou can show black is white by argument said filby

9.2. Long Short-Term Memory (LSTM)

357

9.2.3 Concise Implementation
Using high-level APIs, we can directly instantiate an LSTM model. This encapsulates all the configuration details that we made explicit above. The code is significantly faster as it uses compiled operators rather than Python for many details that we spelled out in detail before.
lstm_layer = rnn.LSTM(num_hiddens) model = d2l.RNNModel(lstm_layer, len(vocab)) d2l.train_ch8(model, train_iter, vocab, lr, num_epochs, device)
perplexity 1.2, 167284.5 tokens/sec on gpu(0) time traveller for so it will be convenient to speak of himwas e traveller abdec that us instonbut buccoune weched and and a

LSTMs are the prototypical latent variable autoregressive model with nontrivial state control. Many variants thereof have been proposed over the years, e.g., multiple layers, residual connections, different types of regularization. However, training LSTMs and other sequence models (such as GRUs) are quite costly due to the long range dependency of the sequence. Later we will encounter alternative models such as transformers that can be used in some cases.

358

Chapter 9. Modern Recurrent Neural Networks

Summary
• LSTMs have three types of gates: input gates, forget gates, and output gates that control the flow of information.
• The hidden layer output of LSTM includes the hidden state and the memory cell. Only the hidden state is passed into the output layer. The memory cell is entirely internal.
• LSTMs can alleviate vanishing and exploding gradients.
Exercises
1. Adjust the hyperparameters and analyze the their influence on running time, perplexity, and the output sequence.
2. How would you need to change the model to generate proper words as opposed to sequences of characters?
3. Compare the computational cost for GRUs, LSTMs, and regular RNNs for a given hidden dimension. Pay special attention to the training and inference cost.
4. Since the candidate memory cell ensures that the value range is between −1 and 1 by using the tanh function, why does the hidden state need to use the tanh function again to ensure that the output value range is between −1 and 1?
5. Implement an LSTM model for time series prediction rather than character sequence prediction.
Discussions114
9.3 Deep Recurrent Neural Networks
Up to now, we only discussed RNNs with a single unidirectional hidden layer. In it the specific functional form of how latent variables and observations interact is rather arbitrary. This is not a big problem as long as we have enough flexibility to model different types of interactions. With a single layer, however, this can be quite challenging. In the case of the linear models, we fixed this problem by adding more layers. Within RNNs this is a bit trickier, since we first need to decide how and where to add extra nonlinearity.
In fact, we could stack multiple layers of RNNs on top of each other. This results in a flexible mechanism, due to the combination of several simple layers. In particular, data might be relevant at different levels of the stack. For instance, we might want to keep high-level data about financial market conditions (bear or bull market) available, whereas at a lower level we only record shorterterm temporal dynamics.
Beyond all the above abstract discussion it is probably easiest to understand the family of models we are interested in by reviewing Fig. 9.3.1. It describes a deep RNN with L hidden layers. Each hidden state is continuously passed to both the next time step of the current layer and the current time step of the next layer.
114 https://discuss.d2l.ai/t/343

9.3. Deep Recurrent Neural Networks

359

Fig. 9.3.1: Architecture of a deep RNN.

9.3.1 Functional Dependencies

We can formalize the functional dependencies within the deep architecture of L hidden layers depicted in Fig. 9.3.1. Our following discussion focuses primarily on the vanilla RNN model, but it applies to other sequence models, too.
Suppose that we have a minibatch input Xt ∈ Rn×d (number of examples: n, number of inputs in each example: d) at time step t. At the same time step, let the hidden state of the lth hidden layer (l = 1, . . . , L) be H(tl) ∈ Rn×h (number of hidden units: h) and the output layer variable be Ot ∈ Rn×q (number of outputs: q). Setting H(t0) = Xt, the hidden state of the lth hidden layer that uses the activation function ϕl is expressed as follows:

H(tl) = ϕl(H(tl−1)W(xlh) + H(t−l)1W(hlh) + b(hl)),

(9.3.1)

where the weights W(xlh) ∈ Rh×h and W(hlh) ∈ Rh×h, together with the bias b(hl) ∈ R1×h, are the model parameters of the lth hidden layer.
In the end, the calculation of the output layer is only based on the hidden state of the final Lth hidden layer:

Ot = H(tL)Whq + bq,

(9.3.2)

where the weight Whq ∈ Rh×q and the bias bq ∈ R1×q are the model parameters of the output layer.
Just as with MLPs, the number of hidden layers L and the number of hidden units h are hyperparameters. In other words, they can be tuned or specified by us. In addition, we can easily get a deep gated RNN by replacing the hidden state computation in (9.3.1) with that from a GRU or an LSTM.

360

Chapter 9. Modern Recurrent Neural Networks

9.3.2 Concise Implementation
Fortunately many of the logistical details required to implement multiple layers of an RNN are readily available in high-level APIs. To keep things simple we only illustrate the implementation using such built-in functionalities. Let us take an LSTM model as an example. The code is very similar to the one we used previously in Section 9.2. In fact, the only difference is that we specify the number of layers explicitly rather than picking the default of a single layer. As usual, we begin by loading the dataset.
from mxnet import npx from mxnet.gluon import rnn from d2l import mxnet as d2l
npx.set_np()
batch_size, num_steps = 32, 35 train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)
The architectural decisions such as choosing hyperparameters are very similar to those of Section 9.2. We pick the same number of inputs and outputs as we have distinct tokens, i.e., vocab_size. The number of hidden units is still 256. The only difference is that we now select a nontrivial number of hidden layers by specifying the value of num_layers.
vocab_size, num_hiddens, num_layers = len(vocab), 256, 2 device = d2l.try_gpu() lstm_layer = rnn.LSTM(num_hiddens, num_layers) model = d2l.RNNModel(lstm_layer, len(vocab))
9.3.3 Training and Prediction
Since now we instantiate two layers with the LSTM model, this rather more complex architecture slows down training considerably.
num_epochs, lr = 500, 2 d2l.train_ch8(model, train_iter, vocab, lr, num_epochs, device)
perplexity 1.1, 107323.6 tokens/sec on gpu(0) time travelleryou can show black is white by argument said filby travelleryou can show black is white by argument said filby

9.3. Deep Recurrent Neural Networks

361

Summary
• In deep RNNs, the hidden state information is passed to the next time step of the current layer and the current time step of the next layer.
• There exist many different flavors of deep RNNs, such as LSTMs, GRUs, or vanilla RNNs. Conveniently these models are all available as parts of the high-level APIs of deep learning frameworks.
• Initialization of models requires care. Overall, deep RNNs require considerable amount of work (such as learning rate and clipping) to ensure proper convergence.
Exercises
1. Try to implement a two-layer RNN from scratch using the single layer implementation we discussed in Section 8.5.
2. Replace the LSTM by a GRU and compare the accuracy and training speed. 3. Increase the training data to include multiple books. How low can you go on the perplexity
scale? 4. Would you want to combine sources of different authors when modeling text? Why is this a
good idea? What could go wrong? Discussions115
115 https://discuss.d2l.ai/t/340

362

Chapter 9. Modern Recurrent Neural Networks

9.4 Bidirectional Recurrent Neural Networks
In sequence learning, so far we assumed that our goal is to model the next output given what we have seen so far, e.g., in the context of a time series or in the context of a language model. While this is a typical scenario, it is not the only one we might encounter. To illustrate the issue, consider the following three tasks of filling in the blank in a text sequence:
• I am ___.
• I am ___ hungry.
• I am ___ hungry, and I can eat half a pig.
Depending on the amount of information available, we might fill in the blanks with very different words such as “happy”, “not”, and “very”. Clearly the end of the phrase (if available) conveys significant information about which word to pick. A sequence model that is incapable of taking advantage of this will perform poorly on related tasks. For instance, to do well in named entity recognition (e.g., to recognize whether “Green” refers to “Mr. Green” or to the color) longer-range context is equally vital. To get some inspiration for addressing the problem let us take a detour to probabilistic graphical models.
9.4.1 Dynamic Programming in Hidden Markov Models
This subsection serves to illustrate the dynamic programming problem. The specific technical details do not matter for understanding the deep learning models but they help in motivating why one might use deep learning and why one might pick specific architectures.
If we want to solve the problem using probabilistic graphical models we could for instance design a latent variable model as follows. At any time step t, we assume that there exists some latent variable ht that governs our observed emission xt via P (xt | ht). Moreover, any transition ht → ht+1 is given by some state transition probability P (ht+1 | ht). This probabilistic graphical model is then a hidden Markov model as in Fig. 9.4.1.

Fig. 9.4.1: A hidden Markov model.

Thus, for a sequence of T observations we have the following joint probability distribution over the observed and hidden states:

∏T P (x1, . . . , xT , h1, . . . , hT ) = P (ht | ht−1)P (xt | ht), where P (h1 | h0) = P (h1).
t=1

(9.4.1)

Now assume that we observe all xi with the exception of some xj and it is our goal to compute P (xj | x−j), where x−j = (x1, . . . , xj−1, xj+1, . . . , xT ). Since there is no latent variable in P (xj | x−j), we consider summing over all the possible combinations of choices for h1, . . . , hT . In case any hi can take on k distinct values (a finite number of states), this means that we need to sum over kT terms—usually mission impossible! Fortunately there is an elegant solution for this: dynamic
programming.

9.4. Bidirectional Recurrent Neural Networks

363

To see how it works, consider summing over latent variables h1, . . . , hT in turn. According to (9.4.1), this yields:

P (x1, . . . , xT ) ∑

=

P (x1, . . . , xT , h1, . . . , hT )

h1,...,hT

∑ ∏T

=

P (ht | ht−1)P (xt | ht)

h1,...,hT

t=1




∑∑

∏T

=

 P (h1)P (x1 | h1)P (h2 | h1) P (x2 | h2) P (ht | ht−1)P (xt | ht)

h2,...,hT h1

t=3

 π2(h2)d=ef 

∑∑

∏T

=

 π2(h2)P (x2 | h2)P (h3 | h2) P (x3 | h3) P (ht | ht−1)P (xt | ht)

h3,...,hT h2

t=4

(9.4.2)

π3 (h3 )d=ef

=... ∑
= πT (hT )P (xT | hT ).
hT

In general we have the forward recursion as ∑
πt+1(ht+1) = πt(ht)P (xt | ht)P (ht+1 | ht).
ht

(9.4.3)

The recursion is initialized as π1(h1) = P (h1). In abstract terms this can be written as πt+1 = f (πt, xt), where f is some learnable function. This looks very much like the update equation in the latent variable models we discussed so far in the context of RNNs!

Entirely analogously to the forward recursion, we can also sum over the same set of latent variables with a backward recursion. This yields:

P (x1, . . . , xT ) ∑

=

P (x1, . . . , xT , h1, . . . , hT )

h1,...,hT

∑ T∏−1

=

P (ht | ht−1)P (xt | ht) · P (hT | hT −1)P (xT | hT )

h1,...,hT t=1





∑ T∏−1

∑

=

P (ht | ht−1)P (xt | ht) ·  P (hT | hT −1)P (xT | hT )

h1,...,hT −1 t=1

hT

  ρT −1(hT −1)d=ef

∑ T∏−2

∑

=

P (ht | ht−1)P (xt | ht) ·  P (hT −1 | hT −2)P (xT −1 | hT −1)ρT −1(hT −1)

h1,...,hT −2 t=1

hT −1

(9.4.4)

=... ∑
= P (h1)P (x1 | h1)ρ1(h1).
h1

ρT −2(hT −2)d=ef

364

Chapter 9. Modern Recurrent Neural Networks

We can thus write the backward recursion as ∑
ρt−1(ht−1) = P (ht | ht−1)P (xt | ht)ρt(ht),
ht

(9.4.5)

with initialization ρT (hT ) = 1. Both the forward and backward recursions allow us to sum over

T latent variables in O(kT ) (linear) time over all values of (h1, . . . , hT ) rather than in exponential

time. This is one of the great benefits of the probabilistic inference with graphical models. It is also

a very special instance of a general message passing algorithm (Aji & McEliece, 2000). Combining

both forward and backward recursions, we are able to compute

∑ P (xj | x−j) ∝ πj(hj)ρj(hj)P (xj | hj).
hj

(9.4.6)

Note that in abstract terms the backward recursion can be written as ρt−1 = g(ρt, xt), where g is a learnable function. Again, this looks very much like an update equation, just running backwards unlike what we have seen so far in RNNs. Indeed, hidden Markov models benefit from knowing future data when it is available. Signal processing scientists distinguish between the two cases of knowing and not knowing future observations as interpolation v.s. extrapolation. See the introductory chapter of the book on sequential Monte Carlo algorithms for more details (Doucet et al., 2001).

9.4.2 Bidirectional Model
If we want to have a mechanism in RNNs that offers comparable look-ahead ability as in hidden Markov models, we need to modify the RNN design that we have seen so far. Fortunately, this is easy conceptually. Instead of running an RNN only in the forward mode starting from the first token, we start another one from the last token running from back to front. Bidirectional RNNs add a hidden layer that passes information in a backward direction to more flexibly process such information. Fig. 9.4.2 illustrates the architecture of a bidirectional RNN with a single hidden layer.

Fig. 9.4.2: Architecture of a bidirectional RNN.

In fact, this is not too dissimilar to the forward and backward recursions in the dynamic programing of hidden Markov models. The main distinction is that in the previous case these equations had a specific statistical meaning. Now they are devoid of such easily accessible interpretations and we can just treat them as generic and learnable functions. This transition epitomizes many of the principles guiding the design of modern deep networks: first, use the type of functional dependencies of classical statistical models, and then parameterize them in a generic form.

9.4. Bidirectional Recurrent Neural Networks

365

Definition

Bidirectional RNNs were introduced by (Schuster & Paliwal, 1997). For a detailed discussion of the various architectures see also the paper (Graves & Schmidhuber, 2005). Let us look at the specifics of such a network.

For any time step t, given a minibatch input Xt ∈ Rn×d (number of examples: n, number of inputs in each example: d) and let the hidden layer activation function be ϕ. In the bidirectional architecture, we assume that the forward and backward hidden states for this time step are −→H t ∈ Rn×h and ←H−t ∈ Rn×h, respectively, where h is the number of hidden units. The forward and backward hidden state updates are as follows:

−→H t = ϕ(XtW(xfh) + −→H t−1W(hfh) + b(hf)), ←H−t = ϕ(XtW(xbh) + ←H−t+1W(hbh) + b(hb)),

(9.4.7)

where the weights W(xfh) ∈ Rd×h, Wh(fh) ∈ Rh×h, W(xbh) ∈ Rd×h, and W(hbh) ∈ Rh×h, and biases b(hf) ∈ R1×h and b(hb) ∈ R1×h are all the model parameters.
Next, we concatenate the forward and backward hidden states −→H t and ←H−t to obtain the hidden state Ht ∈ Rn×2h to be fed into the output layer. In deep bidirectional RNNs with multiple hidden
layers, such information is passed on as input to the next bidirectional layer. Last, the output layer computes the output Ot ∈ Rn×q (number of outputs: q):

Ot = HtWhq + bq.

(9.4.8)

Here, the weight matrix Whq ∈ R2h×q and the bias bq ∈ R1×q are the model parameters of the output layer. In fact, the two directions can have different numbers of hidden units.

Computational Cost and Applications
One of the key features of a bidirectional RNN is that information from both ends of the sequence is used to estimate the output. That is, we use information from both future and past observations to predict the current one. In the case of next token prediction this is not quite what we want. After all, we do not have the luxury of knowing the next to next token when predicting the next one. Hence, if we were to use a bidirectional RNN naively we would not get a very good accuracy: during training we have past and future data to estimate the present. During test time we only have past data and thus poor accuracy. We will illustrate this in an experiment below.
To add insult to injury, bidirectional RNNs are also exceedingly slow. The main reasons for this are that the forward propagation requires both forward and backward recursions in bidirectional layers and that the backpropagation is dependent on the outcomes of the forward propagation. Hence, gradients will have a very long dependency chain.
In practice bidirectional layers are used very sparingly and only for a narrow set of applications, such as filling in missing words, annotating tokens (e.g., for named entity recognition), and encoding sequences wholesale as a step in a sequence processing pipeline (e.g., for machine translation). In Section 14.8 and Section 15.2, we will introduce how to use bidirectional RNNs to encode text sequences.

366

Chapter 9. Modern Recurrent Neural Networks

9.4.3 Training a Bidirectional RNN for a Wrong Application
If we were to ignore all advice regarding the fact that bidirectional RNNs use past and future data and simply apply it to language models, we will get estimates with acceptable perplexity. Nonetheless, the ability of the model to predict future tokens is severely compromised as the experiment below illustrates. Despite reasonable perplexity, it only generates gibberish even after many iterations. We include the code below as a cautionary example against using them in the wrong context.
from mxnet import npx from mxnet.gluon import rnn from d2l import mxnet as d2l
npx.set_np()
# Load data batch_size, num_steps, device = 32, 35, d2l.try_gpu() train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps) # Define the bidirectional LSTM model by setting `bidirectional=True` vocab_size, num_hiddens, num_layers = len(vocab), 256, 2 lstm_layer = rnn.LSTM(num_hiddens, num_layers, bidirectional=True) model = d2l.RNNModel(lstm_layer, len(vocab)) # Train the model num_epochs, lr = 500, 1 d2l.train_ch8(model, train_iter, vocab, lr, num_epochs, device)
perplexity 1.2, 62341.8 tokens/sec on gpu(0) time travellerererererererererererererererererererererererererer travellerererererererererererererererererererererererererer

The output is clearly unsatisfactory for the reasons described above. For a discussion of more effective uses of bidirectional RNNs, please see the sentiment analysis application in Section 15.2.

9.4. Bidirectional Recurrent Neural Networks

367

Summary
• In bidirectional RNNs, the hidden state for each time step is simultaneously determined by the data prior to and after the current time step.
• Bidirectional RNNs bear a striking resemblance with the forward-backward algorithm in probabilistic graphical models.
• Bidirectional RNNs are mostly useful for sequence encoding and the estimation of observations given bidirectional context.
• Bidirectional RNNs are very costly to train due to long gradient chains.
Exercises
1. If the different directions use a different number of hidden units, how will the shape of Ht change?
2. Design a bidirectional RNN with multiple hidden layers.
3. Polysemy is common in natural languages. For example, the word “bank” has different meanings in contexts “i went to the bank to deposit cash” and “i went to the bank to sit down”. How can we design a neural network model such that given a context sequence and a word, a vector representation of the word in the context will be returned? What type of neural architectures is preferred for handling polysemy?
Discussions116
9.5 Machine Translation and the Dataset
We have used RNNs to design language models, which are key to natural language processing. Another flagship benchmark is machine translation, a central problem domain for sequence transduction models that transform input sequences into output sequences. Playing a crucial role in various modern AI applications, sequence transduction models will form the focus of the remainder of this chapter and Chapter 10. To this end, this section introduces the machine translation problem and its dataset that will be used later.
Machine translation refers to the automatic translation of a sequence from one language to another. In fact, this field may date back to 1940s soon after digital computers were invented, especially by considering the use of computers for cracking language codes in World War II. For decades, statistical approaches had been dominant in this field (Brown et al., 1988, 1990) before the rise of end-to-end learning using neural networks. The latter is often called neural machine translation to distinguish itself from statistical machine translation that involves statistical analysis in components such as the translation model and the language model.
Emphasizing end-to-end learning, this book will focus on neural machine translation methods. Different from our language model problem in Section 8.3 whose corpus is in one single language, machine translation datasets are composed of pairs of text sequences that are in the source language and the target language, respectively. Thus, instead of reusing the preprocessing routine for language modeling, we need a different way to preprocess machine translation datasets. In the following, we show how to load the preprocessed data into minibatches for training.
116 https://discuss.d2l.ai/t/339

368

Chapter 9. Modern Recurrent Neural Networks

import os from mxnet import np, npx from d2l import mxnet as d2l
npx.set_np()

9.5.1 Downloading and Preprocessing the Dataset
To begin with, we download an English-French dataset that consists of bilingual sentence pairs from the Tatoeba Project117. Each line in the dataset is a tab-delimited pair of an English text sequence and the translated French text sequence. Note that each text sequence can be just one sentence or a paragraph of multiple sentences. In this machine translation problem where English is translated into French, English is the source language and French is the target language.
#@save d2l.DATA_HUB['fra-eng'] = (d2l.DATA_URL + 'fra-eng.zip',
'94646ad1522d915e7b0f9296181140edcf86a4f5')
#@save def read_data_nmt():
"""Load the English-French dataset.""" data_dir = d2l.download_extract('fra-eng') with open(os.path.join(data_dir, 'fra.txt'), 'r') as f:
return f.read()
raw_text = read_data_nmt() print(raw_text[:75])

Go. Va !

Hi. Salut !

Run!

Cours￿!

Run!

Courez￿!

Who?

Qui ?

Wow!

Ça alors￿!

After downloading the dataset, we proceed with several preprocessing steps for the raw text data. For instance, we replace non-breaking space with space, convert uppercase letters to lowercase ones, and insert space between words and punctuation marks.
#@save def preprocess_nmt(text):
"""Preprocess the English-French dataset.""" def no_space(char, prev_char):
return char in set(',.!?') and prev_char != ' '
# Replace non-breaking space with space, and convert uppercase letters to # lowercase ones text = text.replace('\u202f', ' ').replace('\xa0', ' ').lower() # Insert space between words and punctuation marks out = [
(continues on next page)
117 http://www.manythings.org/anki/

9.5. Machine Translation and the Dataset

369

(continued from previous page) ' ' + char if i > 0 and no_space(char, text[i - 1]) else char for i, char in enumerate(text)] return ''.join(out)
text = preprocess_nmt(raw_text) print(text[:80])

go . hi . run ! run ! who ? wow !

va ! salut ! cours ! courez ! qui ? ça alors !

9.5.2 Tokenization
Different from character-level tokenization in Section 8.3, for machine translation we prefer wordlevel tokenization here (state-of-the-art models may use more advanced tokenization techniques). The following tokenize_nmt function tokenizes the the first num_examples text sequence pairs, where each token is either a word or a punctuation mark. This function returns two lists of token lists: source and target. Specifically, source[i] is a list of tokens from the ith text sequence in the source language (English here) and target[i] is that in the target language (French here).
#@save def tokenize_nmt(text, num_examples=None):
"""Tokenize the English-French dataset.""" source, target = [], [] for i, line in enumerate(text.split('\n')):
if num_examples and i > num_examples: break
parts = line.split('\t') if len(parts) == 2:
source.append(parts[0].split(' ')) target.append(parts[1].split(' ')) return source, target
source, target = tokenize_nmt(text) source[:6], target[:6]
([['go', '.'], ['hi', '.'], ['run', '!'], ['run', '!'], ['who', '?'], ['wow', '!']], [['va', '!'], ['salut', '!'], ['cours', '!'], ['courez', '!'], ['qui', '?'], ['ça', 'alors', '!']])

370

Chapter 9. Modern Recurrent Neural Networks

Let us plot the histogram of the number of tokens per text sequence. In this simple English-French dataset, most of the text sequences have fewer than 20 tokens.
#@save def show_list_len_pair_hist(legend, xlabel, ylabel, xlist, ylist):
"""Plot the histogram for list length pairs.""" d2l.set_figsize() _, _, patches = d2l.plt.hist([[len(l) for l in xlist],
[len(l) for l in ylist]]) d2l.plt.xlabel(xlabel) d2l.plt.ylabel(ylabel) for patch in patches[1].patches:
patch.set_hatch('/') d2l.plt.legend(legend)
show_list_len_pair_hist(['source', 'target'], '# tokens per sequence', 'count', source, target);

9.5.3 Vocabulary
Since the machine translation dataset consists of pairs of languages, we can build two vocabularies for both the source language and the target language separately. With word-level tokenization, the vocabulary size will be significantly larger than that using character-level tokenization. To alleviate this, here we treat infrequent tokens that appear less than 2 times as the same unknown (“<unk>”) token. Besides that, we specify additional special tokens such as for padding (“<pad>”) sequences to the same length in minibatches, and for marking the beginning (“<bos>”) or end (“<eos>”) of sequences. Such special tokens are commonly used in natural language processing tasks.
src_vocab = d2l.Vocab(source, min_freq=2, reserved_tokens=['<pad>', '<bos>', '<eos>'])
len(src_vocab)
10012

9.5. Machine Translation and the Dataset

371

9.5.4 Reading the Dataset

Recall that in language modeling each sequence example, either a segment of one sentence or a span over multiple sentences, has a fixed length. This was specified by the num_steps (number of time steps or tokens) argument in Section 8.3. In machine translation, each example is a pair of source and target text sequences, where each text sequence may have different lengths.
For computational efficiency, we can still process a minibatch of text sequences at one time by truncation and padding. Suppose that every sequence in the same minibatch should have the same length num_steps. If a text sequence has fewer than num_steps tokens, we will keep appending the special “<pad>” token to its end until its length reaches num_steps. Otherwise, we will truncate the text sequence by only taking its first num_steps tokens and discarding the remaining. In this way, every text sequence will have the same length to be loaded in minibatches of the same shape.
The following truncate_pad function truncates or pads text sequences as described before.

#@save def truncate_pad(line, num_steps, padding_token):
"""Truncate or pad sequences.""" if len(line) > num_steps:
return line[:num_steps] # Truncate return line + [padding_token] * (num_steps - len(line))

# Pad

truncate_pad(src_vocab[source[0]], 10, src_vocab['<pad>'])

[47, 4, 1, 1, 1, 1, 1, 1, 1, 1]
Now we define a function to transform text sequences into minibatches for training. We append the special “<eos>” token to the end of every sequence to indicate the end of the sequence. When a model is predicting by generating a sequence token after token, the generation of the “<eos>” token can suggest that the output sequence is complete. Besides, we also record the length of each text sequence excluding the padding tokens. This information will be needed by some models that we will cover later.
#@save def build_array_nmt(lines, vocab, num_steps):
"""Transform text sequences of machine translation into minibatches.""" lines = [vocab[l] for l in lines] lines = [l + [vocab['<eos>']] for l in lines] array = np.array([
truncate_pad(l, num_steps, vocab['<pad>']) for l in lines]) valid_len = (array != vocab['<pad>']).astype(np.int32).sum(1) return array, valid_len

372

Chapter 9. Modern Recurrent Neural Networks

9.5.5 Putting All Things Together
Finally, we define the load_data_nmt function to return the data iterator, together with the vocabularies for both the source language and the target language.
#@save def load_data_nmt(batch_size, num_steps, num_examples=600):
"""Return the iterator and the vocabularies of the translation dataset.""" text = preprocess_nmt(read_data_nmt()) source, target = tokenize_nmt(text, num_examples) src_vocab = d2l.Vocab(source, min_freq=2,
reserved_tokens=['<pad>', '<bos>', '<eos>']) tgt_vocab = d2l.Vocab(target, min_freq=2,
reserved_tokens=['<pad>', '<bos>', '<eos>']) src_array, src_valid_len = build_array_nmt(source, src_vocab, num_steps) tgt_array, tgt_valid_len = build_array_nmt(target, tgt_vocab, num_steps) data_arrays = (src_array, src_valid_len, tgt_array, tgt_valid_len) data_iter = d2l.load_array(data_arrays, batch_size) return data_iter, src_vocab, tgt_vocab

Let us read the first minibatch from the English-French dataset.

train_iter, src_vocab, tgt_vocab = load_data_nmt(batch_size=2, for X, X_valid_len, Y, Y_valid_len in train_iter:
print('X:', X.astype(np.int32)) print('valid lengths for X:', X_valid_len) print('Y:', Y.astype(np.int32)) print('valid lengths for Y:', Y_valid_len) break

num_steps=8)

X: [[105 6 9 11 3 1 [ 6 97 4 3 1 1 1 valid lengths for X: [5 4] Y: [[87 0 0 9 3 1 1 1] [10 0 4 3 1 1 1 1]] valid lengths for Y: [5 4]

1 1] 1]]

Summary
• Machine translation refers to the automatic translation of a sequence from one language to another.
• Using word-level tokenization, the vocabulary size will be significantly larger than that using character-level tokenization. To alleviate this, we can treat infrequent tokens as the same unknown token.
• We can truncate and pad text sequences so that all of them will have the same length to be loaded in minibatches.

9.5. Machine Translation and the Dataset

373

Exercises
1. Try different values of the num_examples argument in the load_data_nmt function. How does this affect the vocabulary sizes of the source language and the target language?
2. Text in some languages such as Chinese and Japanese does not have word boundary indicators (e.g., space). Is word-level tokenization still a good idea for such cases? Why or why not?
Discussions118
9.6 Encoder-Decoder Architecture
As we have discussed in Section 9.5, machine translation is a major problem domain for sequence transduction models, whose input and output are both variable-length sequences. To handle this type of inputs and outputs, we can design an architecture with two major components. The first component is an encoder: it takes a variable-length sequence as the input and transforms it into a state with a fixed shape. The second component is a decoder: it maps the encoded state of a fixed shape to a variable-length sequence. This is called an encoder-decoder architecture, which is depicted in Fig. 9.6.1.

Fig. 9.6.1: The encoder-decoder architecture.

Let us take machine translation from English to French as an example. Given an input sequence in English: “They”, “are”, “watching”, “.”, this encoder-decoder architecture first encodes the variable-length input into a state, then decodes the state to generate the translated sequence token by token as the output: “Ils”, “regardent”, “.”. Since the encoder-decoder architecture forms the basis of different sequence transduction models in subsequent sections, this section will convert this architecture into an interface that will be implemented later.

9.6.1 Encoder
In the encoder interface, we just specify that the encoder takes variable-length sequences as the input X. The implementation will be provided by any model that inherits this base Encoder class.
from mxnet.gluon import nn
#@save class Encoder(nn.Block):
"""The base encoder interface for the encoder-decoder architecture.""" def __init__(self, **kwargs):
super(Encoder, self).__init__(**kwargs)
def forward(self, X, *args): raise NotImplementedError
118 https://discuss.d2l.ai/t/344

374

Chapter 9. Modern Recurrent Neural Networks

9.6.2 Decoder
In the following decoder interface, we add an additional init_state function to convert the encoder output (enc_outputs) into the encoded state. Note that this step may need extra inputs such as the valid length of the input, which was explained in Section 9.5.4. To generate a variable-length sequence token by token, every time the decoder may map an input (e.g., the generated token at the previous time step) and the encoded state into an output token at the current time step.
#@save class Decoder(nn.Block):
"""The base decoder interface for the encoder-decoder architecture.""" def __init__(self, **kwargs):
super(Decoder, self).__init__(**kwargs)
def init_state(self, enc_outputs, *args): raise NotImplementedError
def forward(self, X, state): raise NotImplementedError
9.6.3 Putting the Encoder and Decoder Together
In the end, the encoder-decoder architecture contains both an encoder and a decoder, with optionally extra arguments. In the forward propagation, the output of the encoder is used to produce the encoded state, and this state will be further used by the decoder as one of its input.
#@save class EncoderDecoder(nn.Block):
"""The base class for the encoder-decoder architecture.""" def __init__(self, encoder, decoder, **kwargs):
super(EncoderDecoder, self).__init__(**kwargs) self.encoder = encoder self.decoder = decoder
def forward(self, enc_X, dec_X, *args): enc_outputs = self.encoder(enc_X, *args) dec_state = self.decoder.init_state(enc_outputs, *args) return self.decoder(dec_X, dec_state)
The term “state” in the encoder-decoder architecture has probably inspired you to implement this architecture using neural networks with states. In the next section, we will see how to apply RNNs to design sequence transduction models based on this encoder-decoder architecture.

9.6. Encoder-Decoder Architecture

375

Summary
• The encoder-decoder architecture can handle inputs and outputs that are both variablelength sequences, thus is suitable for sequence transduction problems such as machine translation.
• The encoder takes a variable-length sequence as the input and transforms it into a state with a fixed shape.
• The decoder maps the encoded state of a fixed shape to a variable-length sequence.
Exercises
1. Suppose that we use neural networks to implement the encoder-decoder architecture. Do the encoder and the decoder have to be the same type of neural network?
2. Besides machine translation, can you think of another application where the encoderdecoder architecture can be applied?
Discussions119
9.7 Sequence to Sequence Learning
As we have seen in Section 9.5, in machine translation both the input and output are a variablelength sequence. To address this type of problem, we have designed a general encoder-decoder architecture in Section 9.6. In this section, we will use two RNNs to design the encoder and the decoder of this architecture and apply it to sequence to sequence learning for machine translation (Sutskever et al., 2014; Cho et al., 2014b). Following the design principle of the encoder-decoder architecture, the RNN encoder can take a variable-length sequence as the input and transforms it into a fixed-shape hidden state. In other words, information of the input (source) sequence is encoded in the hidden state of the RNN encoder. To generate the output sequence token by token, a separate RNN decoder can predict the next token based on what tokens have been seen (such as in language modeling) or generated, together with the encoded information of the input sequence. Fig. 9.7.1 illustrates how to use two RNNs for sequence to sequence learning in machine translation.

Fig. 9.7.1: Sequence to sequence learning with an RNN encoder and an RNN decoder.
119 https://discuss.d2l.ai/t/341

376

Chapter 9. Modern Recurrent Neural Networks

In Fig. 9.7.1, the special “<eos>” token marks the end of the sequence. The model can stop making predictions once this token is generated. At the initial time step of the RNN decoder, there are two special design decisions. First, the special beginning-of-sequence “<bos>” token is an input. Second, the final hidden state of the RNN encoder is used to initiate the hidden state of the decoder. In designs such as (Sutskever et al., 2014), this is exactly how the encoded input sequence information is fed into the decoder for generating the output (target) sequence. In some other designs such as (Cho et al., 2014b), the final hidden state of the encoder is also fed into the decoder as part of the inputs at every time step as shown in Fig. 9.7.1. Similar to the training of language models in Section 8.3, we can allow the labels to be the original output sequence, shifted by one token: “<bos>”, “Ils”, “regardent”, “.” → “Ils”, “regardent”, “.”, “<eos>”.
In the following, we will explain the design of Fig. 9.7.1 in greater detail. We will train this model for machine translation on the English-French dataset as introduced in Section 9.5.
import collections import math from mxnet import autograd, gluon, init, np, npx from mxnet.gluon import nn, rnn from d2l import mxnet as d2l
npx.set_np()

9.7.1 Encoder

Technically speaking, the encoder transforms an input sequence of variable length into a fixedshape context variable c, and encodes the input sequence information in this context variable. As depicted in Fig. 9.7.1, we can use an RNN to design the encoder.
Let us consider a sequence example (batch size: 1). Suppose that the input sequence is x1, . . . , xT , such that xt is the tth token in the input text sequence. At time step t, the RNN transforms the input feature vector xt for xt and the hidden state ht−1 from the previous time step into the current hidden state ht. We can use a function f to express the transformation of the RNNʼs recurrent layer:

ht = f (xt, ht−1).

(9.7.1)

In general, the encoder transforms the hidden states at all the time steps into the context variable through a customized function q:

c = q(h1, . . . , hT ).

(9.7.2)

For example, when choosing q(h1, . . . , hT ) = hT such as in Fig. 9.7.1, the context variable is just the hidden state hT of the input sequence at the final time step.
So far we have used a unidirectional RNN to design the encoder, where a hidden state only depends on the input subsequence at and before the time step of the hidden state. We can also construct encoders using bidirectional RNNs. In this case, a hidden state depends on the subsequence before and after the time step (including the input at the current time step), which encodes the information of the entire sequence.
Now let us implement the RNN encoder. Note that we use an embedding layer to obtain the feature vector for each token in the input sequence. The weight of an embedding layer is a matrix whose number of rows equals to the size of the input vocabulary (vocab_size) and number of columns

9.7. Sequence to Sequence Learning

377

equals to the feature vectorʼs dimension (embed_size). For any input token index i, the embedding layer fetches the ith row (starting from 0) of the weight matrix to return its feature vector. Besides, here we choose a multilayer GRU to implement the encoder.
#@save class Seq2SeqEncoder(d2l.Encoder):
"""The RNN encoder for sequence to sequence learning.""" def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,
dropout=0, **kwargs): super(Seq2SeqEncoder, self).__init__(**kwargs) # Embedding layer self.embedding = nn.Embedding(vocab_size, embed_size) self.rnn = rnn.GRU(num_hiddens, num_layers, dropout=dropout)
def forward(self, X, *args): # The output `X` shape: (`batch_size`, `num_steps`, `embed_size`) X = self.embedding(X) # In RNN models, the first axis corresponds to time steps X = X.swapaxes(0, 1) state = self.rnn.begin_state(batch_size=X.shape[1], ctx=X.ctx) output, state = self.rnn(X, state) # `output` shape: (`num_steps`, `batch_size`, `num_hiddens`) # `state[0]` shape: (`num_layers`, `batch_size`, `num_hiddens`) return output, state

The returned variables of recurrent layers have been explained in Section 8.6. Let us still use a concrete example to illustrate the above encoder implementation. Below we instantiate a twolayer GRU encoder whose number of hidden units is 16. Given a minibatch of sequence inputs X (batch size: 4, number of time steps: 7), the hidden states of the last layer at all the time steps (output return by the encoderʼs recurrent layers) are a tensor of shape (number of time steps, batch size, number of hidden units).

encoder = Seq2SeqEncoder(vocab_size=10, num_layers=2)
encoder.initialize() X = np.zeros((4, 7)) output, state = encoder(X) output.shape

embed_size=8,

num_hiddens=16,

(7, 4, 16)
Since a GRU is employed here, the shape of the multilayer hidden states at the final time step is (number of hidden layers, batch size, number of hidden units). If an LSTM is used, memory cell information will also be contained in state.
len(state), state[0].shape

(1, (2, 4, 16))

378

Chapter 9. Modern Recurrent Neural Networks

9.7.2 Decoder

As we just mentioned, the context variable c of the encoderʼs output encodes the entire input sequence x1, . . . , xT . Given the output sequence y1, y2, . . . , yT ′ from the training dataset, for each time step t′ (the symbol differs from the time step t of input sequences or encoders), the probability of the decoder output yt′ is conditional on the previous output subsequence y1, . . . , yt′−1 and the context variable c, i.e., P (yt′ | y1, . . . , yt′−1, c).
To model this conditional probability on sequences, we can use another RNN as the decoder. At any time step t′ on the output sequence, the RNN takes the output yt′−1 from the previous time step and the context variable c as its input, then transforms them and the previous hidden state st′−1 into the hidden state st′ at the current time step. As a result, we can use a function g to express the transformation of the decoderʼs hidden layer:

st′ = g(yt′−1, c, st′−1).

(9.7.3)

After obtaining the hidden state of the decoder, we can use an output layer and the softmax operation to compute the conditional probability distribution P (yt′ | y1, . . . , yt′−1, c) for the output at time step t′.
Following Fig. 9.7.1, when implementing the decoder as follows, we directly use the hidden state at the final time step of the encoder to initialize the hidden state of the decoder. This requires that the RNN encoder and the RNN decoder have the same number of layers and hidden units. To further incorporate the encoded input sequence information, the context variable is concatenated with the decoder input at all the time steps. To predict the probability distribution of the output token, a fully-connected layer is used to transform the hidden state at the final layer of the RNN decoder.

class Seq2SeqDecoder(d2l.Decoder): """The RNN decoder for sequence to sequence learning.""" def __init__(self, vocab_size, embed_size, num_hiddens, num_layers, dropout=0, **kwargs): super(Seq2SeqDecoder, self).__init__(**kwargs) self.embedding = nn.Embedding(vocab_size, embed_size) self.rnn = rnn.GRU(num_hiddens, num_layers, dropout=dropout) self.dense = nn.Dense(vocab_size, flatten=False)

def init_state(self, enc_outputs, *args): return enc_outputs[1]

def forward(self, X, state): # The output `X` shape: (`num_steps`, `batch_size`, `embed_size`) X = self.embedding(X).swapaxes(0, 1) # `context` shape: (`batch_size`, `num_hiddens`) context = state[0][-1] # Broadcast `context` so it has the same `num_steps` as `X` context = np.broadcast_to( context, (X.shape[0], context.shape[0], context.shape[1])) X_and_context = np.concatenate((X, context), 2) output, state = self.rnn(X_and_context, state) output = self.dense(output).swapaxes(0, 1) # `output` shape: (`batch_size`, `num_steps`, `vocab_size`) # `state[0]` shape: (`num_layers`, `batch_size`, `num_hiddens`) return output, state

9.7. Sequence to Sequence Learning

379

To illustrate the implemented decoder, below we instantiate it with the same hyperparameters from the aforementioned encoder. As we can see, the output shape of the decoder becomes (batch size, number of time steps, vocabulary size), where the last dimension of the tensor stores the predicted token distribution.

decoder = Seq2SeqDecoder(vocab_size=10, embed_size=8, num_layers=2)
decoder.initialize() state = decoder.init_state(encoder(X)) output, state = decoder(X, state) output.shape, len(state), state[0].shape

num_hiddens=16,

((4, 7, 10), 1, (2, 4, 16)) To summarize, the layers in the above RNN encoder-decoder model are illustrated in Fig. 9.7.2.

Fig. 9.7.2: Layers in an RNN encoder-decoder model.
9.7.3 Loss Function
At each time step, the decoder predicts a probability distribution for the output tokens. Similar to language modeling, we can apply softmax to obtain the distribution and calculate the crossentropy loss for optimization. Recall Section 9.5 that the special padding tokens are appended to the end of sequences so sequences of varying lengths can be efficiently loaded in minibatches of the same shape. However, prediction of padding tokens should be excluded from loss calculations. To this end, we can use the following sequence_mask function to mask irrelevant entries with zero values so later multiplication of any irrelevant prediction with zero equals to zero. For example, if the valid length of two sequences excluding padding tokens are one and two, respectively, the remaining entries after the first one and the first two entries are cleared to zeros.
X = np.array([[1, 2, 3], [4, 5, 6]]) npx.sequence_mask(X, np.array([1, 2]), True, axis=1)
array([[1., 0., 0.], [4., 5., 0.]])
We can also mask all the entries across the last few axes. If you like, you may even specify to replace such entries with a non-zero value.

380

Chapter 9. Modern Recurrent Neural Networks

X = np.ones((2, 3, 4)) npx.sequence_mask(X, np.array([1, 2]), True, value=-1, axis=1)

array([[[ 1., 1., 1., 1.], [-1., -1., -1., -1.], [-1., -1., -1., -1.]],
[[ 1., 1., 1., 1.], [ 1., 1., 1., 1.], [-1., -1., -1., -1.]]])
Now we can extend the softmax cross-entropy loss to allow the masking of irrelevant predictions. Initially, masks for all the predicted tokens are set to one. Once the valid length is given, the mask corresponding to any padding token will be cleared to zero. In the end, the loss for all the tokens will be multipled by the mask to filter out irrelevant predictions of padding tokens in the loss.
#@save class MaskedSoftmaxCELoss(gluon.loss.SoftmaxCELoss):
"""The softmax cross-entropy loss with masks."""
# `pred` shape: (`batch_size`, `num_steps`, `vocab_size`) # `label` shape: (`batch_size`, `num_steps`) # `valid_len` shape: (`batch_size`,) def forward(self, pred, label, valid_len):
# `weights` shape: (`batch_size`, `num_steps`, 1) weights = np.expand_dims(np.ones_like(label), axis=-1) weights = npx.sequence_mask(weights, valid_len, True, axis=1) return super(MaskedSoftmaxCELoss, self).forward(pred, label, weights)
For a sanity check, we can create three identical sequences. Then we can specify that the valid lengths of these sequences are 4, 2, and 0, respectively. As a result, the loss of the first sequence should be twice as large as that of the second sequence, while the third sequence should have a zero loss.
loss = MaskedSoftmaxCELoss() loss(np.ones((3, 4, 10)), np.ones((3, 4)), np.array([4, 2, 0]))

array([2.3025851, 1.1512926, 0.

])

9.7.4 Training

In the following training loop, we concatenate the special beginning-of-sequence token and the original output sequence excluding the final token as the input to the decoder, as shown in Fig. 9.7.1. This is called teacher forcing because the original output sequence (token labels) is fed into the decoder. Alternatively, we could also feed the predicted token from the previous time step as the current input to the decoder.

#@save def train_seq2seq(net, data_iter, lr, num_epochs, tgt_vocab, device):
"""Train a model for sequence to sequence."""

(continues on next page)

9.7. Sequence to Sequence Learning

381

(continued from previous page)
net.initialize(init.Xavier(), force_reinit=True, ctx=device) trainer = gluon.Trainer(net.collect_params(), 'adam',
{'learning_rate': lr}) loss = MaskedSoftmaxCELoss() animator = d2l.Animator(xlabel='epoch', ylabel='loss',
xlim=[10, num_epochs]) for epoch in range(num_epochs):
timer = d2l.Timer() metric = d2l.Accumulator(2) # Sum of training loss, no. of tokens for batch in data_iter:
X, X_valid_len, Y, Y_valid_len = [ x.as_in_ctx(device) for x in batch]
bos = np.array([tgt_vocab['<bos>']] * Y.shape[0], ctx=device).reshape(-1, 1)
dec_input = np.concatenate([bos, Y[:, :-1]], 1) # Teacher forcing with autograd.record():
Y_hat, _ = net(X, dec_input, X_valid_len) l = loss(Y_hat, Y, Y_valid_len) l.backward() d2l.grad_clipping(net, 1) num_tokens = Y_valid_len.sum() trainer.step(num_tokens) metric.add(l.sum(), num_tokens) if (epoch + 1) % 10 == 0: animator.add(epoch + 1, (metric[0] / metric[1],)) print(f'loss {metric[0] / metric[1]:.3f}, {metric[1] / timer.stop():.1f} ' f'tokens/sec on {str(device)}')
Now we can create and train an RNN encoder-decoder model for sequence to sequence learning on the machine translation dataset.
embed_size, num_hiddens, num_layers, dropout = 32, 32, 2, 0.1 batch_size, num_steps = 64, 10 lr, num_epochs, device = 0.005, 300, d2l.try_gpu()
train_iter, src_vocab, tgt_vocab = d2l.load_data_nmt(batch_size, num_steps) encoder = Seq2SeqEncoder(len(src_vocab), embed_size, num_hiddens, num_layers,
dropout) decoder = Seq2SeqDecoder(len(tgt_vocab), embed_size, num_hiddens, num_layers,
dropout) net = d2l.EncoderDecoder(encoder, decoder) train_seq2seq(net, train_iter, lr, num_epochs, tgt_vocab, device)
loss 0.021, 5350.6 tokens/sec on gpu(0)

382

Chapter 9. Modern Recurrent Neural Networks

9.7.5 Prediction
To predict the output sequence token by token, at each decoder time step the predicted token from the previous time step is fed into the decoder as an input. Similar to training, at the initial time step the beginning-of-sequence (“<bos>”) token is fed into the decoder. This prediction process is illustrated in Fig. 9.7.3. When the end-of-sequence (“<eos>”) token is predicted, the prediction of the output sequence is complete.

Fig. 9.7.3: Predicting the output sequence token by token using an RNN encoder-decoder.

We will introduce different strategies for sequence generation in Section 9.8.
#@save def predict_seq2seq(net, src_sentence, src_vocab, tgt_vocab, num_steps,
device, save_attention_weights=False): """Predict for sequence to sequence.""" src_tokens = src_vocab[src_sentence.lower().split(' ')] + [
src_vocab['<eos>']] enc_valid_len = np.array([len(src_tokens)], ctx=device) src_tokens = d2l.truncate_pad(src_tokens, num_steps, src_vocab['<pad>']) # Add the batch axis enc_X = np.expand_dims(np.array(src_tokens, ctx=device), axis=0) enc_outputs = net.encoder(enc_X, enc_valid_len) dec_state = net.decoder.init_state(enc_outputs, enc_valid_len) # Add the batch axis dec_X = np.expand_dims(np.array([tgt_vocab['<bos>']], ctx=device), axis=0) output_seq, attention_weight_seq = [], []
(continues on next page)

9.7. Sequence to Sequence Learning

383

(continued from previous page)
for _ in range(num_steps): Y, dec_state = net.decoder(dec_X, dec_state) # We use the token with the highest prediction likelihood as the input # of the decoder at the next time step dec_X = Y.argmax(axis=2) pred = dec_X.squeeze(axis=0).astype('int32').item() # Save attention weights (to be covered later) if save_attention_weights: attention_weight_seq.append(net.decoder.attention_weights) # Once the end-of-sequence token is predicted, the generation of the # output sequence is complete if pred == tgt_vocab['<eos>']: break output_seq.append(pred)
return ' '.join(tgt_vocab.to_tokens(output_seq)), attention_weight_seq

9.7.6 Evaluation of Predicted Sequences

We can evaluate a predicted sequence by comparing it with the label sequence (the groundtruth). BLEU (Bilingual Evaluation Understudy), though originally proposed for evaluating machine translation results (Papineni et al., 2002), has been extensively used in measuring the quality of output sequences for different applications. In principle, for any n-grams in the predicted sequence, BLEU evaluates whether this n-grams appears in the label sequence.

Denote by pn the precision of n-grams, which is the ratio of the number of matched n-grams in the predicted and label sequences to the number of n-grams in the predicted sequence. To explain, given a label sequence A, B, C, D, E, F , and a predicted sequence A, B, B, C, D, we have p1 = 4/5, p2 = 3/4, p3 = 1/3, and p4 = 0. Besides, let lenlabel and lenpred be the numbers of tokens in the label sequence and the predicted sequence, respectively. Then, BLEU is defined as

((

lenlabel )) ∏k 1/2n

exp min 0, 1 − lenpred

pn ,

n=1

(9.7.4)

where k is the longest n-grams for matching.

Based on the definition of BLEU in (9.7.4), whenever the predicted sequence is the same as the label sequence, BLEU is 1. Moreover, since matching longer n-grams is more difficult, BLEU assigns a greater weight to a longer n-gram precision. Specifically, when pn is fixed, p1n/2n increases as n grows (the original paper uses p1n/n). Furthermore, since predicting shorter sequences tends to obtain a higher pn value, the coefficient before the multiplication term in (9.7.4) penalizes shorter predicted sequences. For example, when k = 2, given the label sequence A, B, C, D, E, F and the predicted sequence A, B, although p1 = p2 = 1, the penalty factor exp(1 − 6/2) ≈ 0.14 lowers the BLEU.

We implement the BLEU measure as follows.

def bleu(pred_seq, label_seq, k): #@save """Compute the BLEU.""" pred_tokens, label_tokens = pred_seq.split(' '), label_seq.split(' ') len_pred, len_label = len(pred_tokens), len(label_tokens) score = math.exp(min(0, 1 - len_label / len_pred))
(continues on next page)

384

Chapter 9. Modern Recurrent Neural Networks

(continued from previous page)
for n in range(1, k + 1): num_matches, label_subs = 0, collections.defaultdict(int) for i in range(len_label - n + 1): label_subs[''.join(label_tokens[i:i + n])] += 1 for i in range(len_pred - n + 1): if label_subs[''.join(pred_tokens[i:i + n])] > 0: num_matches += 1 label_subs[''.join(pred_tokens[i:i + n])] -= 1 score *= math.pow(num_matches / (len_pred - n + 1), math.pow(0.5, n))
return score

In the end, we use the trained RNN encoder-decoder to translate a few English sentences into French and compute the BLEU of the results.

engs = ['go .', "i lost .", 'he\'s calm .', 'i\'m home .'] fras = ['va !', 'j\'ai perdu .', 'il est calme .', 'je suis for eng, fra in zip(engs, fras):
translation, attention_weight_seq = predict_seq2seq( net, eng, src_vocab, tgt_vocab, num_steps, device)
print(f'{eng} => {translation}, bleu {bleu(translation,

chez fra,

moi .'] k=2):.3f}')

go . => va !, bleu 1.000 i lost . => j'ai perdu ., bleu 1.000 he's calm . => il est riche essaye le <unk> tomber plus tomber !, bleu 0.258 i'm home . => je suis chez chez vu ?, bleu 0.562

Summary
• Following the design of the encoder-decoder architecture, we can use two RNNs to design a model for sequence to sequence learning.
• When implementing the encoder and the decoder, we can use multilayer RNNs. • We can use masks to filter out irrelevant computations, such as when calculating the loss. • In encoder-decoder training, the teacher forcing approach feeds original output sequences
(in contrast to predictions) into the decoder. • BLEU is a popular measure for evaluating output sequences by matching n-grams between
the predicted sequence and the label sequence.

Exercises

1. Can you adjust the hyperparameters to improve the translation results?
2. Rerun the experiment without using masks in the loss calculation. What results do you observe? Why?
3. If the encoder and the decoder differ in the number of layers or the number of hidden units, how can we initialize the hidden state of the decoder?
4. In training, replace teacher forcing with feeding the prediction at the previous time step into the decoder. How does this influence the performance?

9.7. Sequence to Sequence Learning

385

5. Rerun the experiment by replacing GRU with LSTM. 6. Are there any other ways to design the output layer of the decoder? Discussions120

9.8 Beam Search

In Section 9.7, we predicted the output sequence token by token until the special end-of-sequence “<eos>” token is predicted. In this section, we will begin with formalizing this greedy search strategy and exploring issues with it, then compare this strategy with other alternatives: exhaustive search and beam search.

Before a formal introduction to greedy search, let us formalize the search problem using the same

mathematical notation from Section 9.7. At any time step t′, the probability of the decoder output

yt′ is conditional on the output subsequence y1, . . . , yt′−1 before t′ and the context variable c that

encodes the information of the input sequence. To quantify computational cost, denote by Y (it

contains “<eos>”) the output vocabulary. So the cardinality |Y| of this vocabulary set is the vocab-

ulary size. Let us also specify the maximum number of tokens of an output sequence as T ′. As a

result,

our

goal

is

to

search

for

an

ideal

output

from

all

the

O(|Y |T

′
)

possible

output

sequences.

Of course, for all these output sequences, portions including and after “<eos>” will be discarded

in the actual output.

9.8.1 Greedy Search

First, let us take a look at a simple strategy: greedy search. This strategy has been used to predict sequences in Section 9.7. In greedy search, at any time step t′ of the output sequence, we search
for the token with the highest conditional probability from Y, i.e.,

yt′ = argmax P (y | y1, . . . , yt′−1, c),
y∈Y

(9.8.1)

as the output. Once “<eos>” is outputted or the output sequence has reached its maximum length T ′, the output sequence is completed.

So what can go wrong with g∏ree′ dy search? In fact, the optimal sequence should be the output se-

quence with the maximum

T t′=1

P

(yt′

|

y1, . . . , yt′−1, c), which is the conditional probability of

generating an output sequence based on the input sequence. Unfortunately, there is no guaran-

tee that the optimal sequence will be obtained by greedy search.

Fig. 9.8.1: At each time step, greedy search selects the token with the highest conditional probability.
120 https://discuss.d2l.ai/t/345

386

Chapter 9. Modern Recurrent Neural Networks

Let us illustrate it with an example. Suppose that there are four tokens “A”, “B”, “C”, and “<eos>” in the output dictionary. In Fig. 9.8.1, the four numbers under each time step represent the conditional probabilities of generating “A”, “B”, “C”, and “<eos>” at that time step, respectively.
At each time step, greedy search selects the token with the highest conditional probability. Therefore, the output sequence “A”, “B”, “C”, and “<eos>” will be predicted in Fig. 9.8.1. The conditional probability of this output sequence is 0.5 × 0.4 × 0.4 × 0.6 = 0.048.

Fig. 9.8.2: The four numbers under each time step represent the conditional probabilities of generating “A”, “B”, “C”, and “<eos>” at that time step. At time step 2, the token “C”, which has the second highest conditional probability, is selected.
Next, let us look at another example in Fig. 9.8.2. Unlike in Fig. 9.8.1, at time step 2 we select the token “C” in Fig. 9.8.2, which has the second highest conditional probability. Since the output subsequences at time steps 1 and 2, on which time step 3 is based, have changed from “A” and “B” in Fig. 9.8.1 to “A” and “C” in Fig. 9.8.2, the conditional probability of each token at time step 3 has also changed in Fig. 9.8.2. Suppose that we choose the token “B” at time step 3. Now time step 4 is conditional on the output subsequence at the first three time steps “A”, “C”, and “B”, which is different from “A”, “B”, and “C” in Fig. 9.8.1. Therefore, the conditional probability of generating each token at time step 4 in Fig. 9.8.2 is also different from that in Fig. 9.8.1. As a result, the conditional probability of the output sequence “A”, “C”, “B”, and “<eos>” in Fig. 9.8.2 is 0.5 × 0.3 × 0.6 × 0.6 = 0.054, which is greater than that of greedy search in Fig. 9.8.1. In this example, the output sequence “A”, “B”, “C”, and “<eos>” obtained by the greedy search is not an optimal sequence.
9.8.2 Exhaustive Search
If the goal is to obtain the optimal sequence, we may consider using exhaustive search: exhaustively enumerate all the possible output sequences with their conditional probabilities, then output the one with the highest conditional probability.
Although we can use exhaustive search to obtain the optimal sequence, its computational cost O(|Y|T ′) is likely to be excessively high. For example, when |Y| = 10000 and T ′ = 10, we will need to evaluate 1000010 = 1040 sequences. This is next to impossible! On the other hand, the computational cost of greedy search is O(|Y| T ′): it is usually significantly smaller than that of exhaustive search. For example, when |Y| = 10000 and T ′ = 10, we only need to evaluate 10000 × 10 = 105 sequences.

9.8. Beam Search

387

9.8.3 Beam Search
Decisions about sequence searching strategies lie on a spectrum, with easy questions at either extreme. What if only accuracy matters? Obviously, exhaustive search. What if only computational cost matters? Clearly, greedy search. A real-world application usually asks a complicated question, somewhere in between those two extremes.
Beam search is an improved version of greedy search. It has a hyperparameter named beam size, k. At time step 1, we select k tokens with the highest conditional probabilities. Each of them will be the first token of k candidate output sequences, respectively. At each subsequent time step, based on the k candidate output sequences at the previous time step, we continue to select k candidate output sequences with the highest conditional probabilities from k |Y| possible choices.

Fig. 9.8.3: The process of beam search (beam size: 2, maximum length of an output sequence: 3). The candidate output sequences are A, C, AB, CE, ABD, and CED.

Fig. 9.8.3 demonstrates the process of beam search with an example. Suppose that the output vocabulary contains only five elements: Y = {A, B, C, D, E}, where one of them is “<eos>”. Let the beam size be 2 and the maximum length of an output sequence be 3. At time step 1, suppose that the tokens with the highest conditional probabilities P (y1 | c) are A and C. At time step 2, for all y2 ∈ Y, we compute

P (A, y2 | c) = P (A | c)P (y2 | A, c), P (C, y2 | c) = P (C | c)P (y2 | C, c),

(9.8.2)

and pick the largest two among these ten values, say P (A, B | c) and P (C, E | c). Then at time step 3, for all y3 ∈ Y, we compute

P (A, B, y3 | c) = P (A, B | c)P (y3 | A, B, c), P (C, E, y3 | c) = P (C, E | c)P (y3 | C, E, c),

(9.8.3)

and pick the largest two among these ten values, say P (A, B, D | c) and P (C, E, D | c). As a result, we get six candidates output sequences: (i) A; (ii) C; (iii) A, B; (iv) C, E; (v) A, B, D; and (vi) C, E, D.

388

Chapter 9. Modern Recurrent Neural Networks

In the end, we obtain the set of final candidate output sequences based on these six sequences (e.g., discard portions including and after “<eos>”). Then we choose the sequence with the highest of the following score as the output sequence:

1

1 ∑L

Lα log P (y1, . . . , yL | c) = Lα log P (yt′ | y1, . . . , yt′−1, c),

t′=1

(9.8.4)

where L is the length of the final candidate sequence and α is usually set to 0.75. Since a longer sequence has more logarithmic terms in the summation of (9.8.4), the term Lα in the denominator penalizes long sequences.
The computational cost of beam search is O(k |Y| T ′). This result is in between that of greedy search and that of exhaustive search. In fact, greedy search can be treated as a special type of beam search with a beam size of 1. With a flexible choice of the beam size, beam search provides a tradeoff between accuracy versus computational cost.

Summary
• Sequence searching strategies include greedy search, exhaustive search, and beam search. • Beam search provides a tradeoff between accuracy versus computational cost via its flexible
choice of the beam size.

Exercises
1. Can we treat exhaustive search as a special type of beam search? Why or why not? 2. Apply beam search in the machine translation problem in Section 9.7. How does the beam
size affect the translation results and the prediction speed? 3. We used language modeling for generating text following user-provided prefixes in Section
8.5. Which kind of search strategy does it use? Can you improve it? Discussions121

121 https://discuss.d2l.ai/t/338

9.8. Beam Search

389

390

Chapter 9. Modern Recurrent Neural Networks

10 | Attention Mechanisms
The optic nerve of a primateʼs visual system receives massive sensory input, far exceeding what the brain can fully process. Fortunately, not all stimuli are created equal. Focalization and concentration of consciousness have enabled primates to direct attention to objects of interest, such as preys and predators, in the complex visual environment. The ability of paying attention to only a small fraction of the information has evolutionary significance, allowing human beings to live and succeed.
Scientists have been studying attention in the cognitive neuroscience field since the 19th century. In this chapter, we will begin by reviewing a popular framework explaining how attention is deployed in a visual scene. Inspired by the attention cues in this framework, we will design models that leverage such attention cues. Notably, the Nadaraya-Waston kernel regression in 1964 is a simple demonstration of machine learning with attention mechanisms.
Next, we will go on to introduce attention functions that have been extensively used in the design of attention models in deep learning. Specifically, we will show how to use these functions to design the Bahdanau attention, a groundbreaking attention model in deep learning that can align bidirectionally and is differentiable.
In the end, equipped with the more recent multi-head attention and self-attention designs, we will describe the transformer architecture based solely on attention mechanisms. Since their proposal in 2017, transformers have been pervasive in modern deep learning applications, such as in areas of language, vision, speech, and reinforcement learning.
10.1 Attention Cues
Thank you for your attention to this book. Attention is a scarce resource: at the moment you are reading this book and ignoring the rest. Thus, similar to money, your attention is being paid with an opportunity cost. To ensure that your investment of attention right now is worthwhile, we have been highly motivated to pay our attention carefully to produce a nice book. Attention is the keystone in the arch of life and holds the key to any workʼs exceptionalism.
Since economics studies the allocation of scarce resources, we are in the era of the attention economy, where human attention is treated as a limited, valuable, and scarce commodity that can be exchanged. Numerous business models have been developed to capitalize on it. On music or video streaming services, we either pay attention to their ads or pay money to hide them. For growth in the world of online games, we either pay attention to participate in battles, which attract new gamers, or pay money to instantly become powerful. Nothing comes for free.
All in all, information in our environment is not scarce, attention is. When inspecting a visual scene, our optic nerve receives information at the order of 108 bits per second, far exceeding what our brain can fully process. Fortunately, our ancestors had learned from experience (also known
391

as data) that not all sensory inputs are created equal. Throughout human history, the capability of directing attention to only a fraction of information of interest has enabled our brain to allocate resources more smartly to survive, to grow, and to socialize, such as detecting predators, preys, and mates.
10.1.1 Attention Cues in Biology
To explain how our attention is deployed in the visual world, a two-component framework has emerged and been pervasive. This idea dates back to William James in the 1890s, who is considered the “father of American psychology” (James, 2007). In this framework, subjects selectively direct the spotlight of attention using both the nonvolitional cue and volitional cue.
The nonvolitional cue is based on the saliency and conspicuity of objects in the environment. Imagine there are five objects in front of you: a newspaper, a research paper, a cup of coffee, a notebook, and a book such as in Fig. 10.1.1. While all the paper products are printed in black and white, the coffee cup is red. In other words, this coffee is intrinsically salient and conspicuous in this visual environment, automatically and involuntarily drawing attention. So you bring the fovea (the center of the macula where visual acuity is highest) onto the coffee as shown in Fig. 10.1.1.

Fig. 10.1.1: Using the nonvolitional cue based on saliency (red cup, non-paper), attention is involuntarily directed to the coffee.
After drinking coffee, you become caffeinated and want to read a book. So you turn your head, refocus your eyes, and look at the book as depicted in Fig. 10.1.2. Different from the case in Fig. 10.1.1 where the coffee biases you towards selecting based on saliency, in this task-dependent case you select the book under cognitive and volitional control. Using the volitional cue based on variable selection criteria, this form of attention is more deliberate. It is also more powerful with the subjectʼs voluntary effort.

392

Chapter 10. Attention Mechanisms

Fig. 10.1.2: Using the volitional cue (want to read a book) that is task-dependent, attention is directed to the book under volitional control.
10.1.2 Queries, Keys, and Values
Inspired by the nonvolitional and volitional attention cues that explain the attentional deployment, in the following we will describe a framework for designing attention mechanisms by incorporating these two attention cues.
To begin with, consider the simpler case where only nonvolitional cues are available. To bias selection over sensory inputs, we can simply use a parameterized fully-connected layer or even non-parameterized max or average pooling.
Therefore, what sets attention mechanisms apart from those fully-connected layers or pooling layers is the inclusion of the volitional cues. In the context of attention mechanisms, we refer to volitional cues as queries. Given any query, attention mechanisms bias selection over sensory inputs (e.g., intermediate feature representations) via attention pooling. These sensory inputs are called values in the context of attention mechanisms. More generally, every value is paired with a key, which can be thought of the nonvolitional cue of that sensory input. As shown in Fig. 10.1.3, we can design attention pooling so that the given query (volitional cue) can interact with keys (nonvolitional cues), which guides bias selection over values (sensory inputs).

10.1. Attention Cues

393

Fig. 10.1.3: Attention mechanisms bias selection over values (sensory inputs) via attention pooling, which incorporates queries (volitional cues) and keys (nonvolitional cues).

Note that there are many alternatives for the design of attention mechanisms. For instance, we can design a non-differentiable attention model that can be trained using reinforcement learning methods (Mnih et al., 2014). Given the dominance of the framework in Fig. 10.1.3, models under this framework will be the center of our attention in this chapter.

10.1.3 Visualization of Attention
Average pooling can be treated as a weighted average of inputs, where weights are uniform. In practice, attention pooling aggregates values using weighted average, where weights are computed between the given query and different keys.
from mxnet import np, npx from d2l import mxnet as d2l
npx.set_np()
To visualize attention weights, we define the show_heatmaps function. Its input matrices has the shape (number of rows for display, number of columns for display, number of queries, number of keys).
#@save def show_heatmaps(matrices, xlabel, ylabel, titles=None, figsize=(2.5, 2.5),
cmap='Reds'): """Show heatmaps of matrices.""" d2l.use_svg_display() num_rows, num_cols = matrices.shape[0], matrices.shape[1] fig, axes = d2l.plt.subplots(num_rows, num_cols, figsize=figsize,
sharex=True, sharey=True, squeeze=False) for i, (row_axes, row_matrices) in enumerate(zip(axes, matrices)):
for j, (ax, matrix) in enumerate(zip(row_axes, row_matrices)): pcm = ax.imshow(matrix.asnumpy(), cmap=cmap) if i == num_rows - 1: ax.set_xlabel(xlabel)
(continues on next page)

394

Chapter 10. Attention Mechanisms

if j == 0: ax.set_ylabel(ylabel)
if titles: ax.set_title(titles[j])
fig.colorbar(pcm, ax=axes, shrink=0.6);

(continued from previous page)

For demonstration, we consider a simple case where the attention weight is one only when the query and the key are the same; otherwise it is zero.
attention_weights = np.eye(10).reshape((1, 1, 10, 10)) show_heatmaps(attention_weights, xlabel='Keys', ylabel='Queries')

In the subsequent sections, we will often invoke this function to visualize attention weights.
Summary
• Human attention is a limited, valuable, and scarce resource. • Subjects selectively direct attention using both the nonvolitional and volitional cues. The
former is based on saliency and the latter is task-dependent. • Attention mechanisms are different from fully-connected layers or pooling layers due to in-
clusion of the volitional cues. • Attention mechanisms bias selection over values (sensory inputs) via attention pooling,
which incorporates queries (volitional cues) and keys (nonvolitional cues). Keys and values are paired. • We can visualize attention weights between queries and keys.

10.1. Attention Cues

395

Exercises
1. What can be the volitional cue when decoding a sequence token by token in machine translation? What are the nonvolitional cues and the sensory inputs?
2. Randomly generate a 10 × 10 matrix and use the softmax operation to ensure each row is a valid probability distribution. Visualize the output attention weights.
Discussions122
10.2 Attention Pooling: Nadaraya-Watson Kernel Regression
Now you know the major components of attention mechanisms under the framework in Fig. 10.1.3. To recapitulate, the interactions between queries (volitional cues) and keys (nonvolitional cues) result in attention pooling. The attention pooling selectively aggregates values (sensory inputs) to produce the output. In this section, we will describe attention pooling in greater detail to give you a high-level view of how attention mechanisms work in practice. Specifically, the Nadaraya-Watson kernel regression model proposed in 1964 is a simple yet complete example for demonstrating machine learning with attention mechanisms.
from mxnet import autograd, gluon, np, npx from mxnet.gluon import nn from d2l import mxnet as d2l
npx.set_np()

10.2.1 Generating the Dataset

To keep things simple, let us consider the following regression problem: given a dataset of inputoutput pairs {(x1, y1), . . . , (xn, yn)}, how to learn f to predict the output yˆ = f (x) for any new input x?
Here we generate an artificial dataset according to the following nonlinear function with the noise term ϵ:

yi = 2 sin(xi) + x0i .8 + ϵ,

(10.2.1)

where ϵ obeys a normal distribution with zero mean and standard deviation 0.5. Both 50 training examples and 50 testing examples are generated. To better visualize the pattern of attention later, the training inputs are sorted.

n_train = 50 # No. of training examples x_train = np.sort(np.random.rand(n_train) * 5) # Training inputs

def f(x): return 2 * np.sin(x) + x**0.8
y_train = f(x_train) + np.random.normal(0.0, 0.5,
122 https://discuss.d2l.ai/t/1596

(continues on next page)

396

Chapter 10. Attention Mechanisms

(continued from previous page) (n_train,)) # Training outputs x_test = np.arange(0, 5, 0.1) # Testing examples y_truth = f(x_test) # Ground-truth outputs for the testing examples n_test = len(x_test) # No. of testing examples n_test
50
The following function plots all the training examples (represented by circles), the ground-truth data generation function f without the noise term (labeled by “Truth”), and the learned prediction function (labeled by “Pred”).
def plot_kernel_reg(y_hat): d2l.plot(x_test, [y_truth, y_hat], 'x', 'y', legend=['Truth', 'Pred'], xlim=[0, 5], ylim=[-1, 5]) d2l.plt.plot(x_train, y_train, 'o', alpha=0.5);

10.2.2 Average Pooling

We begin with perhaps the worldʼs “dumbest” estimator for this regression problem: using average pooling to average over all the training outputs:

1 ∑n f (x) = n yi,
i=1

(10.2.2)

which is plotted below. As we can see, this estimator is indeed not so smart.

y_hat = y_train.mean().repeat(n_test) plot_kernel_reg(y_hat)

10.2. Attention Pooling: Nadaraya-Watson Kernel Regression

397

10.2.3 Nonparametric Attention Pooling

Obviously, average pooling omits the inputs xi. A better idea was proposed by Nadaraya (Nadaraya, 1964) and Watson (Watson, 1964) to weigh the outputs yi according to their input locations:

∑n K(x − xi)

f (x) = ∑n

yi,

i=1 j=1 K(x − xj )

(10.2.3)

where K is a kernel. The estimator in (10.2.3) is called Nadaraya-Watson kernel regression. Here we will not dive into details of kernels. Recall the framework of attention mechanisms in Fig. 10.1.3. From the perspective of attention, we can rewrite (10.2.3) in a more generalized form of attention pooling:

∑n f (x) = α(x, xi)yi,
i=1

(10.2.4)

where x is the query and (xi, yi) is the key-value pair. Comparing (10.2.4) and (10.2.2), the attention pooling here is a weighted average of values yi. The attention weight α(x, xi) in (10.2.4) is assigned to the corresponding value yi based on the interaction between the query x and the key xi modeled by α. For any query, its attention weights over all the key-value pairs are a valid probability distribution: they are non-negative and sum up to one.

To gain intuitions of attention pooling, just consider a Gaussian kernel defined as

1

u2

K(u) = √2π exp(− 2 ).

(10.2.5)

Plugging the Gaussian kernel into (10.2.4) and (10.2.3) gives

∑n f (x) = α(x, xi)yi

i=1
∑n

exp (− 1 (x − x )2)

= i=1 ∑nj=1 exp (2− 12 (x −i xj )2) yi

∑n

( 1

)

= softmax − (x − xi)2 yi.

2

i=1

(10.2.6)

In (10.2.6), a key xi that is closer to the given query x will get more attention via a larger attention weight assigned to the keyʼs corresponding value yi.

Notably, Nadaraya-Watson kernel regression is a nonparametric model; thus (10.2.6) is an example of nonparametric attention pooling. In the following, we plot the prediction based on this nonparametric attention model. The predicted line is smooth and closer to the ground-truth than that produced by average pooling.

# Shape of `X_repeat`: (`n_test`, `n_train`), where each row contains the # same testing inputs (i.e., same queries) X_repeat = x_test.repeat(n_train).reshape((-1, n_train)) # Note that `x_train` contains the keys. Shape of `attention_weights`: # (`n_test`, `n_train`), where each row contains attention weights to be # assigned among the values (`y_train`) given each query attention_weights = npx.softmax(-(X_repeat - x_train)**2 / 2)
(continues on next page)

398

Chapter 10. Attention Mechanisms

(continued from previous page) # Each element of `y_hat` is weighted average of values, where weights are # attention weights y_hat = np.dot(attention_weights, y_train) plot_kernel_reg(y_hat)
Now let us take a look at the attention weights. Here testing inputs are queries while training inputs are keys. Since both inputs are sorted, we can see that the closer the query-key pair is, the higher attention weight is in the attention pooling. d2l.show_heatmaps(np.expand_dims(np.expand_dims(attention_weights, 0),
0), xlabel='Sorted training inputs', ylabel='Sorted testing inputs')

10.2. Attention Pooling: Nadaraya-Watson Kernel Regression

399

10.2.4 Parametric Attention Pooling

Nonparametric Nadaraya-Watson kernel regression enjoys the consistency benefit: given enough data this model converges to the optimal solution. Nonetheless, we can easily integrate learnable parameters into attention pooling.
As an example, slightly different from (10.2.6), in the following the distance between the query x and the key xi is multiplied by a learnable parameter w:

∑n f (x) = α(x, xi)yi

i=1
∑n

exp (− 1 ((x − x )w)2)

= i=1 ∑nj=1 exp (2− 12 ((x −i xj)w)2) yi

∑n

( 1

)

= softmax − ((x − xi)w)2 yi.

2

i=1

(10.2.7)

In the rest of the section, we will train this model by learning the parameter of the attention pooling in (10.2.7).

Batch Matrix Multiplication
To more efficiently compute attention for minibatches, we can leverage batch matrix multiplication utilities provided by deep learning frameworks.
Suppose that the first minibatch contains n matrices X1, . . . , Xn of shape a × b, and the second minibatch contains n matrices Y1, . . . , Yn of shape b × c. Their batch matrix multiplication results in n matrices X1Y1, . . . , XnYn of shape a × c. Therefore, given two tensors of shape (n, a, b) and (n, b, c), the shape of their batch matrix multiplication output is (n, a, c).
X = np.ones((2, 1, 4)) Y = np.ones((2, 4, 6)) npx.batch_dot(X, Y).shape

(2, 1, 6)
In the context of attention mechanisms, we can use minibatch matrix multiplication to compute weighted averages of values in a minibatch.
weights = np.ones((2, 10)) * 0.1 values = np.arange(20).reshape((2, 10)) npx.batch_dot(np.expand_dims(weights, 1), np.expand_dims(values, -1))

array([[[ 4.5]], [[14.5]]])

400

Chapter 10. Attention Mechanisms

Defining the Model
Using minibatch matrix multiplication, below we define the parametric version of NadarayaWatson kernel regression based on the parametric attention pooling in (10.2.7).
class NWKernelRegression(nn.Block): def __init__(self, **kwargs): super().__init__(**kwargs) self.w = self.params.get('w', shape=(1,))
def forward(self, queries, keys, values): # Shape of the output `queries` and `attention_weights`: # (no. of queries, no. of key-value pairs) queries = queries.repeat(keys.shape[1]).reshape((-1, keys.shape[1])) self.attention_weights = npx.softmax( -((queries - keys) * self.w.data())**2 / 2) # Shape of `values`: (no. of queries, no. of key-value pairs) return npx.batch_dot(np.expand_dims(self.attention_weights, 1), np.expand_dims(values, -1)).reshape(-1)
Training
In the following, we transform the training dataset to keys and values to train the attention model. In the parametric attention pooling, any training input takes key-value pairs from all the training examples except for itself to predict its output.
# Shape of `X_tile`: (`n_train`, `n_train`), where each column contains the # same training inputs X_tile = np.tile(x_train, (n_train, 1)) # Shape of `Y_tile`: (`n_train`, `n_train`), where each column contains the # same training outputs Y_tile = np.tile(y_train, (n_train, 1)) # Shape of `keys`: ('n_train', 'n_train' - 1) keys = X_tile[(1 - np.eye(n_train)).astype('bool')].reshape((n_train, -1)) # Shape of `values`: ('n_train', 'n_train' - 1) values = Y_tile[(1 - np.eye(n_train)).astype('bool')].reshape((n_train, -1))
Using the squared loss and stochastic gradient descent, we train the parametric attention model.
net = NWKernelRegression() net.initialize() loss = gluon.loss.L2Loss() trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.5}) animator = d2l.Animator(xlabel='epoch', ylabel='loss', xlim=[1, 5])
for epoch in range(5): with autograd.record(): l = loss(net(x_train, keys, values), y_train) l.backward() trainer.step(1) print(f'epoch {epoch + 1}, loss {float(l.sum()):.6f}') animator.add(epoch + 1, float(l.sum()))

10.2. Attention Pooling: Nadaraya-Watson Kernel Regression

401

After training the parametric attention model, we can plot its prediction. Trying to fit the training dataset with noise, the predicted line is less smooth than its nonparametric counterpart that was plotted earlier.
# Shape of `keys`: (`n_test`, `n_train`), where each column contains the same # training inputs (i.e., same keys) keys = np.tile(x_train, (n_test, 1)) # Shape of `value`: (`n_test`, `n_train`) values = np.tile(y_train, (n_test, 1)) y_hat = net(x_test, keys, values) plot_kernel_reg(y_hat)

Comparing with nonparametric attention pooling, the region with large attention weights becomes sharper in the learnable and parametric setting.
d2l.show_heatmaps(np.expand_dims(np.expand_dims(net.attention_weights, 0), 0), xlabel='Sorted training inputs',
ylabel='Sorted testing inputs')

402

Chapter 10. Attention Mechanisms

Summary
• Nadaraya-Watson kernel regression is an example of machine learning with attention mechanisms.
• The attention pooling of Nadaraya-Watson kernel regression is a weighted average of the training outputs. From the attention perspective, the attention weight is assigned to a value based on a function of a query and the key that is paired with the value.
• Attention pooling can be either nonparametric or parametric.
Exercises
1. Increase the number of training examples. Can you learn nonparametric Nadaraya-Watson kernel regression better?
2. What is the value of our learned w in the parametric attention pooling experiment? Why does it make the weighted region sharper when visualizing the attention weights?
3. How can we add hyperparameters to nonparametric Nadaraya-Watson kernel regression to predict better?
4. Design another parametric attention pooling for the kernel regression of this section. Train this new model and visualize its attention weights.
Discussions123
10.3 Attention Scoring Functions
In Section 10.2, we used a Gaussian kernel to model interactions between queries and keys. Treating the exponent of the Gaussian kernel in (10.2.6) as an attention scoring function (or scoring function for short), the results of this function were essentially fed into a softmax operation. As a result, we obtained a probability distribution (attention weights) over values that are paired with keys. In the end, the output of the attention pooling is simply a weighted sum of the values based on these attention weights.
123 https://discuss.d2l.ai/t/1598

10.3. Attention Scoring Functions

403

At a high level, we can use the above algorithm to instantiate the framework of attention mechanisms in Fig. 10.1.3. Denoting an attention scoring function by a, Fig. 10.3.1 illustrates how the output of attention pooling can be computed as a weighted sum of values. Since attention weights are a probability distribution, the weighted sum is essentially a weighted average.

Fig. 10.3.1: Computing the output of attention pooling as a weighted average of values.

Mathematically, suppose that we have a query q ∈ Rq and m key-value pairs (k1, v1), . . . , (km, vm), where any ki ∈ Rk and any vi ∈ Rv. The attention pooling f is instantiated as a weighted sum of
the values:

∑ m f (q, (k1, v1), . . . , (km, vm)) = α(q, ki)vi ∈ Rv,
i=1

(10.3.1)

where the attention weight (scalar) for the query q and key ki is computed by the softmax operation of an attention scoring function a that maps two vectors to a scalar:

α(q, ki) = softmax(a(q, ki)) = ∑mexp(a(q, ki)) ∈ R. j=1 exp(a(q, kj))

(10.3.2)

As we can see, different choices of the attention scoring function a lead to different behaviors of attention pooling. In this section, we introduce two popular scoring functions that we will use to develop more sophisticated attention mechanisms later.

import math from mxnet import np, npx from mxnet.gluon import nn from d2l import mxnet as d2l

npx.set_np()

404

Chapter 10. Attention Mechanisms

10.3.1 Masked Softmax Operation
As we just mentioned, a softmax operation is used to output a probability distribution as attention weights. In some cases, not all the values should be fed into attention pooling. For instance, for efficient minibatch processing in Section 9.5, some text sequences are padded with special tokens that do not carry meaning. To get an attention pooling over only meaningful tokens as values, we can specify a valid sequence length (in number of tokens) to filter out those beyond this specified range when computing softmax. In this way, we can implement such a masked softmax operation in the following masked_softmax function, where any value beyond the valid length is masked as zero.
#@save def masked_softmax(X, valid_lens):
"""Perform softmax operation by masking elements on the last axis.""" # `X`: 3D tensor, `valid_lens`: 1D or 2D tensor if valid_lens is None:
return npx.softmax(X) else:
shape = X.shape if valid_lens.ndim == 1:
valid_lens = valid_lens.repeat(shape[1]) else:
valid_lens = valid_lens.reshape(-1) # On the last axis, replace masked elements with a very large negative # value, whose exponentiation outputs 0 X = npx.sequence_mask(X.reshape(-1, shape[-1]), valid_lens, True,
value=-1e6, axis=1) return npx.softmax(X).reshape(shape)
To demonstrate how this function works, consider a minibatch of two 2 × 4 matrix examples, where the valid lengths for these two examples are two and three, respectively. As a result of the masked softmax operation, values beyond the valid lengths are all masked as zero.
masked_softmax(np.random.uniform(size=(2, 2, 4)), np.array([2, 3]))

array([[[0.488994 , 0.511006 , 0.

, 0.

],

[0.4365484 , 0.56345165, 0.

, 0.

]],

[[0.288171 , 0.3519408 , 0.3598882 , 0.

],

[0.29034296, 0.25239873, 0.45725837, 0.

]]])

Similarly, we can also use a two-dimensional tensor to specify valid lengths for every row in each matrix example.
masked_softmax(np.random.uniform(size=(2, 2, 4)), np.array([[1, 3], [2, 4]]))

array([[[1.

, 0.

, 0.

, 0.

],

[0.35848376, 0.3658879 , 0.27562833, 0.

]],

[[0.54370314, 0.45629686, 0.

, 0.

],

[0.19598778, 0.25580427, 0.19916739, 0.3490406 ]]])

10.3. Attention Scoring Functions

405

10.3.2 Additive Attention

In general, when queries and keys are vectors of different lengths, we can use additive attention as the scoring function. Given a query q ∈ Rq and a key k ∈ Rk, the additive attention scoring
function

a(q, k) = w⊤v tanh(Wqq + Wkk) ∈ R,

(10.3.3)

where learnable parameters Wq ∈ Rh×q, Wk ∈ Rh×k, and wv ∈ Rh. Equivalent to (10.3.3), the query and the key are concatenated and fed into an MLP with a single hidden layer whose number of hidden units is h, a hyperparameter. By using tanh as the activation function and disabling bias terms, we implement additive attention in the following.

#@save class AdditiveAttention(nn.Block):
"""Additive attention.""" def __init__(self, num_hiddens, dropout, **kwargs):
super(AdditiveAttention, self).__init__(**kwargs) # Use `flatten=False` to only transform the last axis so that the # shapes for the other axes are kept the same self.W_k = nn.Dense(num_hiddens, use_bias=False, flatten=False) self.W_q = nn.Dense(num_hiddens, use_bias=False, flatten=False) self.w_v = nn.Dense(1, use_bias=False, flatten=False) self.dropout = nn.Dropout(dropout)

def forward(self, queries, keys, values, valid_lens): queries, keys = self.W_q(queries), self.W_k(keys) # After dimension expansion, shape of `queries`: (`batch_size`, no. of # queries, 1, `num_hiddens`) and shape of `keys`: (`batch_size`, 1, # no. of key-value pairs, `num_hiddens`). Sum them up with # broadcasting features = np.expand_dims(queries, axis=2) + np.expand_dims( keys, axis=1) features = np.tanh(features) # There is only one output of `self.w_v`, so we remove the last # one-dimensional entry from the shape. Shape of `scores`: # (`batch_size`, no. of queries, no. of key-value pairs) scores = np.squeeze(self.w_v(features), axis=-1) self.attention_weights = masked_softmax(scores, valid_lens) # Shape of `values`: (`batch_size`, no. of key-value pairs, value # dimension) return npx.batch_dot(self.dropout(self.attention_weights), values)

Let us demonstrate the above AdditiveAttention class with a toy example, where shapes (batch size, number of steps or sequence length in tokens, feature size) of queries, keys, and values are (2, 1, 20), (2, 10, 2), and (2, 10, 4), respectively. The attention pooling output has a shape of (batch size, number of steps for queries, feature size for values).

queries, keys = np.random.normal(0, 1, (2, 1, 20)), np.ones((2, 10, 2)) # The two value matrices in the `values` minibatch are identical values = np.arange(40).reshape(1, 10, 4).repeat(2, axis=0) valid_lens = np.array([2, 6])

attention = AdditiveAttention(num_hiddens=8, dropout=0.1)

(continues on next page)

406

Chapter 10. Attention Mechanisms

attention.initialize() attention(queries, keys, values, valid_lens)

(continued from previous page)

array([[[ 2.

, 3.

, 4.

, 5.

]],

[[10.

, 11.

, 12.000001, 13.

]]])

Although additive attention contains learnable parameters, since every key is the same in this example, the attention weights are uniform, determined by the specified valid lengths.
d2l.show_heatmaps(attention.attention_weights.reshape((1, 1, 2, 10)), xlabel='Keys', ylabel='Queries')

10.3.3 Scaled Dot-Product Attention

A more computationally efficient design for the scoring function can be simply dot product. However, the dot product operation requires that both the query and the key have the same vector length, say d. Assume that all the elements of the query and the key are independent random variables with zero mean and unit variance. The dot product of both vectors has zero mean and a variance of d. To ensure that the variance of the dot product still remains one regardless of vector length, the scaled dot-product attention scoring function

a(q, k) = q⊤k/√d

(10.3.4)

√ divides the dot product by d. In practice, we often think in minibatches for efficiency, such as

computing attention for n queries and m key-value pairs, where queries and keys are of length d

and values are of length v. The scaled dot-product attention of queries Q ∈ Rn×d, keys K ∈ Rm×d,

and values V ∈ Rm×v is

( QK⊤ ) softmax √ V ∈ Rn×v.
d

(10.3.5)

In the following implementation of the scaled dot product attention, we use dropout for model regularization.

#@save class DotProductAttention(nn.Block):
"""Scaled dot product attention.""" def __init__(self, dropout, **kwargs):
super(DotProductAttention, self).__init__(**kwargs)

(continues on next page)

10.3. Attention Scoring Functions

407

self.dropout = nn.Dropout(dropout)

(continued from previous page)

# Shape of `queries`: (`batch_size`, no. of queries, `d`) # Shape of `keys`: (`batch_size`, no. of key-value pairs, `d`) # Shape of `values`: (`batch_size`, no. of key-value pairs, value # dimension) # Shape of `valid_lens`: (`batch_size`,) or (`batch_size`, no. of queries) def forward(self, queries, keys, values, valid_lens=None):
d = queries.shape[-1] # Set `transpose_b=True` to swap the last two dimensions of `keys` scores = npx.batch_dot(queries, keys, transpose_b=True) / math.sqrt(d) self.attention_weights = masked_softmax(scores, valid_lens) return npx.batch_dot(self.dropout(self.attention_weights), values)

To demonstrate the above DotProductAttention class, we use the same keys, values, and valid lengths from the earlier toy example for additive attention. For the dot product operation, we make the feature size of queries the same as that of keys.
queries = np.random.normal(0, 1, (2, 1, 2)) attention = DotProductAttention(dropout=0.5) attention.initialize() attention(queries, keys, values, valid_lens)

array([[[ 2.

, 3.

, 4.

, 5.

]],

[[10.

, 11.

, 12.000001, 13.

]]])

Same as in the additive attention demonstration, since keys contains the same element that cannot be differentiated by any query, uniform attention weights are obtained.
d2l.show_heatmaps(attention.attention_weights.reshape((1, 1, 2, 10)), xlabel='Keys', ylabel='Queries')

408

Chapter 10. Attention Mechanisms

Summary
• We can compute the output of attention pooling as a weighted average of values, where different choices of the attention scoring function lead to different behaviors of attention pooling.
• When queries and keys are vectors of different lengths, we can use the additive attention scoring function. When they are the same, the scaled dot-product attention scoring function is more computationally efficient.
Exercises
1. Modify keys in the toy example and visualize attention weights. Do additive attention and scaled dot-product attention still output the same attention weights? Why or why not?
2. Using matrix multiplications only, can you design a new scoring function for queries and keys with different vector lengths?
3. When queries and keys have the same vector length, is vector summation a better design than dot product for the scoring function? Why or why not?
Discussions124
10.4 Bahdanau Attention
We studied the machine translation problem in Section 9.7, where we designed an encoderdecoder architecture based on two RNNs for sequence to sequence learning. Specifically, the RNN encoder transforms a variable-length sequence into a fixed-shape context variable, then the RNN decoder generates the output (target) sequence token by token based on the generated tokens and the context variable. However, even though not all the input (source) tokens are useful for decoding a certain token, the same context variable that encodes the entire input sequence is still used at each decoding step.
In a separate but related challenge of handwriting generation for a given text sequence, Graves designed a differentiable attention model to align text characters with the much longer pen trace, where the alignment moves only in one direction (Graves, 2013). Inspired by the idea of learning to align, Bahdanau et al. proposed a differentiable attention model without the severe unidirectional alignment limitation (Bahdanau et al., 2014). When predicting a token, if not all the input tokens are relevant, the model aligns (or attends) only to parts of the input sequence that are relevant to the current prediction. This is achieved by treating the context variable as an output of attention pooling.
124 https://discuss.d2l.ai/t/346

10.4. Bahdanau Attention

409

10.4.1 Model

When describing Bahdanau attention for the RNN encoder-decoder below, we will follow the same
notation in Section 9.7. The new attention-based model is the same as that in Section 9.7 except that the context variable c in (9.7.3) is replaced by ct′ at any decoding time step t′. Suppose that there are T tokens in the input sequence, the context variable at the decoding time step t′ is the
output of attention pooling:

∑ T ct′ = α(st′−1, ht)ht,
t=1

(10.4.1)

where the decoder hidden state st′−1 at time step t′ − 1 is the query, and the encoder hidden states ht are both the keys and values, and the attention weight α is computed as in (10.3.2) using the additive attention scoring function defined by (10.3.3).

Slightly different from the vanilla RNN encoder-decoder architecture in Fig. 9.7.2, the same architecture with Bahdanau attention is depicted in Fig. 10.4.1.

Fig. 10.4.1: Layers in an RNN encoder-decoder model with Bahdanau attention.
from mxnet import np, npx from mxnet.gluon import nn, rnn from d2l import mxnet as d2l npx.set_np()

10.4.2 Defining the Decoder with Attention
To implement the RNN encoder-decoder with Bahdanau attention, we only need to redefine the decoder. To visualize the learned attention weights more conveniently, the following AttentionDecoder class defines the base interface for decoders with attention mechanisms.
#@save class AttentionDecoder(d2l.Decoder):
"""The base attention-based decoder interface.""" def __init__(self, **kwargs):
super(AttentionDecoder, self).__init__(**kwargs)

(continues on next page)

410

Chapter 10. Attention Mechanisms

@property def attention_weights(self):
raise NotImplementedError

(continued from previous page)

Now let us implement the RNN decoder with Bahdanau attention in the following Seq2SeqAttentionDecoder class. The state of the decoder is initialized with (i) the encoder final-layer hidden states at all the time steps (as keys and values of the attention); (ii) the encoder all-layer hidden state at the final time step (to initialize the hidden state of the decoder); and (iii) the encoder valid length (to exclude the padding tokens in attention pooling). At each decoding time step, the decoder final-layer hidden state at the previous time step is used as the query of the attention. As a result, both the attention output and the input embedding are concatenated as the input of the RNN decoder.
class Seq2SeqAttentionDecoder(AttentionDecoder): def __init__(self, vocab_size, embed_size, num_hiddens, num_layers, dropout=0, **kwargs): super(Seq2SeqAttentionDecoder, self).__init__(**kwargs) self.attention = d2l.AdditiveAttention(num_hiddens, dropout) self.embedding = nn.Embedding(vocab_size, embed_size) self.rnn = rnn.GRU(num_hiddens, num_layers, dropout=dropout) self.dense = nn.Dense(vocab_size, flatten=False)
def init_state(self, enc_outputs, enc_valid_lens, *args): # Shape of `outputs`: (`num_steps`, `batch_size`, `num_hiddens`). # Shape of `hidden_state[0]`: (`num_layers`, `batch_size`, # `num_hiddens`) outputs, hidden_state = enc_outputs return (outputs.swapaxes(0, 1), hidden_state, enc_valid_lens)
def forward(self, X, state): # Shape of `enc_outputs`: (`batch_size`, `num_steps`, `num_hiddens`). # Shape of `hidden_state[0]`: (`num_layers`, `batch_size`, # `num_hiddens`) enc_outputs, hidden_state, enc_valid_lens = state # Shape of the output `X`: (`num_steps`, `batch_size`, `embed_size`) X = self.embedding(X).swapaxes(0, 1) outputs, self._attention_weights = [], [] for x in X: # Shape of `query`: (`batch_size`, 1, `num_hiddens`) query = np.expand_dims(hidden_state[0][-1], axis=1) # Shape of `context`: (`batch_size`, 1, `num_hiddens`) context = self.attention(query, enc_outputs, enc_outputs, enc_valid_lens) # Concatenate on the feature dimension x = np.concatenate((context, np.expand_dims(x, axis=1)), axis=-1) # Reshape `x` as (1, `batch_size`, `embed_size` + `num_hiddens`) out, hidden_state = self.rnn(x.swapaxes(0, 1), hidden_state) outputs.append(out) self._attention_weights.append(self.attention.attention_weights) # After fully-connected layer transformation, shape of `outputs`: # (`num_steps`, `batch_size`, `vocab_size`) outputs = self.dense(np.concatenate(outputs, axis=0)) return outputs.swapaxes(0, 1), [
(continues on next page)

10.4. Bahdanau Attention

411

enc_outputs, hidden_state, enc_valid_lens]
@property def attention_weights(self):
return self._attention_weights

(continued from previous page)

In the following, we test the implemented decoder with Bahdanau attention using a minibatch of 4 sequence inputs of 7 time steps.
encoder = d2l.Seq2SeqEncoder(vocab_size=10, embed_size=8, num_hiddens=16, num_layers=2)
encoder.initialize() decoder = Seq2SeqAttentionDecoder(vocab_size=10, embed_size=8, num_hiddens=16,
num_layers=2) decoder.initialize() X = np.zeros((4, 7)) # (`batch_size`, `num_steps`) state = decoder.init_state(encoder(X), None) output, state = decoder(X, state) output.shape, len(state), state[0].shape, len(state[1]), state[1][0].shape

((4, 7, 10), 3, (4, 7, 16), 1, (2, 4, 16))

10.4.3 Training
Similar to Section 9.7.4, here we specify hyperparemeters, instantiate an encoder and a decoder with Bahdanau attention, and train this model for machine translation. Due to the newly added attention mechanism, this training is much slower than that in Section 9.7.4 without attention mechanisms.
embed_size, num_hiddens, num_layers, dropout = 32, 32, 2, 0.1 batch_size, num_steps = 64, 10 lr, num_epochs, device = 0.005, 250, d2l.try_gpu()
train_iter, src_vocab, tgt_vocab = d2l.load_data_nmt(batch_size, num_steps) encoder = d2l.Seq2SeqEncoder(len(src_vocab), embed_size, num_hiddens,
num_layers, dropout) decoder = Seq2SeqAttentionDecoder(len(tgt_vocab), embed_size, num_hiddens,
num_layers, dropout) net = d2l.EncoderDecoder(encoder, decoder) d2l.train_seq2seq(net, train_iter, lr, num_epochs, tgt_vocab, device)
loss 0.026, 3144.5 tokens/sec on gpu(0)

412

Chapter 10. Attention Mechanisms

After the model is trained, we use it to translate a few English sentences into French and compute their BLEU scores.
engs = ['go .', "i lost .", 'he\'s calm .', 'i\'m home .'] fras = ['va !', 'j\'ai perdu .', 'il est calme .', 'je suis chez moi .'] for eng, fra in zip(engs, fras):
translation, dec_attention_weight_seq = d2l.predict_seq2seq( net, eng, src_vocab, tgt_vocab, num_steps, device, True)
print(f'{eng} => {translation}, ', f'bleu {d2l.bleu(translation, fra, k=2):.3f}')
go . => va !, bleu 1.000 i lost . => je suis calme ., bleu 0.000 he's calm . => il est malade ., bleu 0.658 i'm home . => je suis chez moi ., bleu 1.000
attention_weights = np.concatenate( [step[0][0][0] for step in dec_attention_weight_seq], 0).reshape( (1, 1, -1, num_steps))
By visualizing the attention weights when translating the last English sentence, we can see that each query assigns non-uniform weights over key-value pairs. It shows that at each decoding step, different parts of the input sequences are selectively aggregated in the attention pooling.
# Plus one to include the end-of-sequence token d2l.show_heatmaps(attention_weights[:, :, :, :len(engs[-1].split()) + 1],
xlabel='Key positions', ylabel='Query positions')

10.4. Bahdanau Attention

413

Summary
• When predicting a token, if not all the input tokens are relevant, the RNN encoder-decoder with Bahdanau attention selectively aggregates different parts of the input sequence. This is achieved by treating the context variable as an output of additive attention pooling.
• In the RNN encoder-decoder, Bahdanau attention treats the decoder hidden state at the previous time step as the query, and the encoder hidden states at all the time steps as both the keys and values.
Exercises
1. Replace GRU with LSTM in the experiment. 2. Modify the experiment to replace the additive attention scoring function with the scaled
dot-product. How does it influence the training efficiency? Discussions125

10.5 Multi-Head Attention
In practice, given the same set of queries, keys, and values we may want our model to combine knowledge from different behaviors of the same attention mechanism, such as capturing dependencies of various ranges (e.g., shorter-range vs. longer-range) within a sequence. Thus, it may be beneficial to allow our attention mechanism to jointly use different representation subspaces of queries, keys, and values.
To this end, instead of performing a single attention pooling, queries, keys, and values can be transformed with h independently learned linear projections. Then these h projected queries, keys, and values are fed into attention pooling in parallel. In the end, h attention pooling outputs are concatenated and transformed with another learned linear projection to produce the final output. This design is called multi-head attention, where each of the h attention pooling outputs is a head (Vaswani et al., 2017). Using fully-connected layers to perform learnable linear transformations, Fig. 10.5.1 describes multi-head attention.
125 https://discuss.d2l.ai/t/347

414

Chapter 10. Attention Mechanisms

Fig. 10.5.1: Multi-head attention, where multiple heads are concatenated then linearly transformed.

10.5.1 Model

Before providing the implementation of multi-head attention, let us formalize this model mathematically. Given a query q ∈ Rdq , a key k ∈ Rdk , and a value v ∈ Rdv , each attention head hi
(i = 1, . . . , h) is computed as

hi = f (W(iq)q, W(ik)k, W(iv)v) ∈ Rpv ,

(10.5.1)

where learnable parameters Wi(q) ∈ Rpq×dq , W(ik) ∈ Rpk×dk and W(iv) ∈ Rpv×dv , and f is attention

pooling, such as additive attention and scaled dot-product attention in Section 10.3. The multi-

head attention output is another linear transformation via learnable parameters Wo ∈ Rpo×hpv of

the concatenation of h heads:

 h1
Wo  ...  ∈ Rpo.

(10.5.2)

hh

Based on this design, each head may attend to different parts of the input. More sophisticated functions than the simple weighted average can be expressed.

import math from mxnet import autograd, np, npx from mxnet.gluon import nn from d2l import mxnet as d2l

npx.set_np()

10.5.2 Implementation
In our implementation, we choose the scaled dot-product attention for each head of the multihead attention. To avoid significant growth of computational cost and parameterization cost, we set pq = pk = pv = po/h. Note that h heads can be computed in parallel if we set the number of outputs of linear transformations for the query, key, and value to pqh = pkh = pvh = po. In the following implementation, po is specified via the argument num_hiddens.

10.5. Multi-Head Attention

415

#@save class MultiHeadAttention(nn.Block):
"""Multi-head attention.""" def __init__(self, num_hiddens, num_heads, dropout, use_bias=False,
**kwargs): super(MultiHeadAttention, self).__init__(**kwargs) self.num_heads = num_heads self.attention = d2l.DotProductAttention(dropout) self.W_q = nn.Dense(num_hiddens, use_bias=use_bias, flatten=False) self.W_k = nn.Dense(num_hiddens, use_bias=use_bias, flatten=False) self.W_v = nn.Dense(num_hiddens, use_bias=use_bias, flatten=False) self.W_o = nn.Dense(num_hiddens, use_bias=use_bias, flatten=False)
def forward(self, queries, keys, values, valid_lens): # Shape of `queries`, `keys`, or `values`: # (`batch_size`, no. of queries or key-value pairs, `num_hiddens`) # Shape of `valid_lens`: # (`batch_size`,) or (`batch_size`, no. of queries) # After transposing, shape of output `queries`, `keys`, or `values`: # (`batch_size` * `num_heads`, no. of queries or key-value pairs, # `num_hiddens` / `num_heads`) queries = transpose_qkv(self.W_q(queries), self.num_heads) keys = transpose_qkv(self.W_k(keys), self.num_heads) values = transpose_qkv(self.W_v(values), self.num_heads)
if valid_lens is not None: # On axis 0, copy the first item (scalar or vector) for # `num_heads` times, then copy the next item, and so on valid_lens = valid_lens.repeat(self.num_heads, axis=0)
# Shape of `output`: (`batch_size` * `num_heads`, no. of queries, # `num_hiddens` / `num_heads`) output = self.attention(queries, keys, values, valid_lens)
# Shape of `output_concat`: # (`batch_size`, no. of queries, `num_hiddens`) output_concat = transpose_output(output, self.num_heads) return self.W_o(output_concat)

To allow for parallel computation of multiple heads, the above MultiHeadAttention class uses two transposition functions as defined below. Specifically, the transpose_output function reverses the operation of the transpose_qkv function.

#@save def transpose_qkv(X, num_heads):
"""Transposition for parallel computation of multiple attention heads.""" # Shape of input `X`: # (`batch_size`, no. of queries or key-value pairs, `num_hiddens`). # Shape of output `X`: # (`batch_size`, no. of queries or key-value pairs, `num_heads`, # `num_hiddens` / `num_heads`) X = X.reshape(X.shape[0], X.shape[1], num_heads, -1)

# Shape of output `X`: # (`batch_size`, `num_heads`, no. of queries or key-value pairs,

(continues on next page)

416

Chapter 10. Attention Mechanisms

# `num_hiddens` / `num_heads`) X = X.transpose(0, 2, 1, 3)

(continued from previous page)

# Shape of `output`: # (`batch_size` * `num_heads`, no. of queries or key-value pairs, # `num_hiddens` / `num_heads`) return X.reshape(-1, X.shape[2], X.shape[3])

#@save def transpose_output(X, num_heads):
"""Reverse the operation of `transpose_qkv`.""" X = X.reshape(-1, num_heads, X.shape[1], X.shape[2]) X = X.transpose(0, 2, 1, 3) return X.reshape(X.shape[0], X.shape[1], -1)

Let us test our implemented MultiHeadAttention class using a toy example where keys and values are the same. As a result, the shape of the multi-head attention output is (batch_size, num_queries, num_hiddens).
num_hiddens, num_heads = 100, 5 attention = MultiHeadAttention(num_hiddens, num_heads, 0.5) attention.initialize()

batch_size, num_queries, num_kvpairs, valid_lens = 2, 4, 6, np.array([3, 2]) X = np.ones((batch_size, num_queries, num_hiddens)) Y = np.ones((batch_size, num_kvpairs, num_hiddens)) attention(X, Y, Y, valid_lens).shape

(2, 4, 100)

Summary
• Multi-head attention combines knowledge of the same attention pooling via different representation subspaces of queries, keys, and values.
• To compute multiple heads of multi-head attention in parallel, proper tensor manipulation is needed.
Exercises
1. Visualize attention weights of multiple heads in this experiment. 2. Suppose that we have a trained model based on multi-head attention and we want to prune
least important attention heads to increase the prediction speed. How can we design experiments to measure the importance of an attention head? Discussions126
126 https://discuss.d2l.ai/t/1634

10.5. Multi-Head Attention

417

10.6 Self-Attention and Positional Encoding
In deep learning, we often use CNNs or RNNs to encode a sequence. Now with attention mechanisms, imagine that we feed a sequence of tokens into attention pooling so that the same set of tokens act as queries, keys, and values. Specifically, each query attends to all the key-value pairs and generates one attention output. Since the queries, keys, and values come from the same place, this performs self-attention (Lin et al., 2017b; Vaswani et al., 2017), which is also called intraattention (Cheng et al., 2016; Parikh et al., 2016; Paulus et al., 2017). In this section, we will discuss sequence encoding using self-attention, including using additional information for the sequence order.
import math from mxnet import autograd, np, npx from mxnet.gluon import nn from d2l import mxnet as d2l
npx.set_np()

10.6.1 Self-Attention

Given a sequence of input tokens x1, . . . , xn where any xi ∈ Rd (1 ≤ i ≤ n), its self-attention outputs a sequence of the same length y1, . . . , yn, where

yi = f (xi, (x1, x1), . . . , (xn, xn)) ∈ Rd

(10.6.1)

according to the definition of attention pooling f in (10.2.4). Using multi-head attention, the following code snippet computes the self-attention of a tensor with shape (batch size, number of time steps or sequence length in tokens, d). The output tensor has the same shape.
num_hiddens, num_heads = 100, 5 attention = d2l.MultiHeadAttention(num_hiddens, num_heads, 0.5) attention.initialize()

batch_size, num_queries, valid_lens = 2, 4, np.array([3, 2]) X = np.ones((batch_size, num_queries, num_hiddens)) attention(X, X, X, valid_lens).shape

(2, 4, 100)

10.6.2 Comparing CNNs, RNNs, and Self-Attention
Let us compare architectures for mapping a sequence of n tokens to another sequence of equal length, where each input or output token is represented by a d-dimensional vector. Specifically, we will consider CNNs, RNNs, and self-attention. We will compare their computational complexity, sequential operations, and maximum path lengths. Note that sequential operations prevent parallel computation, while a shorter path between any combination of sequence positions makes it easier to learn long-range dependencies within the sequence (Hochreiter et al., 2001).

418

Chapter 10. Attention Mechanisms

Fig. 10.6.1: Comparing CNN (padding tokens are omitted), RNN, and self-attention architectures.
Consider a convolutional layer whose kernel size is k. We will provide more details about sequence processing using CNNs in later chapters. For now, we only need to know that since the sequence length is n, the numbers of input and output channels are both d, the computational complexity of the convolutional layer is O(knd2). As Fig. 10.6.1 shows, CNNs are hierarchical so there are O(1) sequential operations and the maximum path length is O(n/k). For example, x1 and x5 are within the receptive field of a two-layer CNN with kernel size 3 in Fig. 10.6.1.
When updating the hidden state of RNNs, multiplication of the d × d weight matrix and the ddimensional hidden state has a computational complexity of O(d2). Since the sequence length is n, the computational complexity of the recurrent layer is O(nd2). According to Fig. 10.6.1, there are O(n) sequential operations that cannot be parallelized and the maximum path length is also O(n).
In self-attention, the queries, keys, and values are all n × d matrices. Consider the scaled dotproduct attention in (10.3.5), where a n×d matrix is multiplied by a d×n matrix, then the output n× n matrix is multiplied by a n × d matrix. As a result, the self-attention has a O(n2d) computational complexity. As we can see in Fig. 10.6.1, each token is directly connected to any other token via self-attention. Therefore, computation can be parallel with O(1) sequential operations and the maximum path length is also O(1).
All in all, both CNNs and self-attention enjoy parallel computation and self-attention has the shortest maximum path length. However, the quadratic computational complexity with respect to the sequence length makes self-attention prohibitively slow for very long sequences.

10.6. Self-Attention and Positional Encoding

419

10.6.3 Positional Encoding

Unlike RNNs that recurrently process tokens of a sequence one by one, self-attention ditches sequential operations in favor of parallel computation. To use the sequence order information, we can inject absolute or relative positional information by adding positional encoding to the input representations. Positional encodings can be either learned or fixed. In the following, we describe a fixed positional encoding based on sine and cosine functions (Vaswani et al., 2017).

Suppose that the input representation X ∈ Rn×d contains the d-dimensional embeddings for n

tokens of a sequence. The positional encoding outputs X + P using a positional embedding matrix

P ∈ Rn×d of the same shape, whose element on the ith row and the (2j)th or the (2j + 1)th column

is

(

)

i

pi,2j = sin 100002j/d ,

(

)

i

pi,2j+1 = cos 100002j/d .

(10.6.2)

At first glance, this trigonometric-function design looks weird. Before explanations of this design, let us first implement it in the following PositionalEncoding class.

#@save class PositionalEncoding(nn.Block):
"""Positional encoding.""" def __init__(self, num_hiddens, dropout, max_len=1000):
super(PositionalEncoding, self).__init__() self.dropout = nn.Dropout(dropout) # Create a long enough `P` self.P = np.zeros((1, max_len, num_hiddens)) X = np.arange(max_len).reshape(-1, 1) / np.power(
10000, np.arange(0, num_hiddens, 2) / num_hiddens) self.P[:, :, 0::2] = np.sin(X) self.P[:, :, 1::2] = np.cos(X)

def forward(self, X): X = X + self.P[:, :X.shape[1], :].as_in_ctx(X.ctx) return self.dropout(X)

In the positional embedding matrix P, rows correspond to positions within a sequence and columns represent different positional encoding dimensions. In the example below, we can see that the 6th and the 7th columns of the positional embedding matrix have a higher frequency than the 8th and the 9th columns. The offset between the 6th and the 7th (same for the 8th and the 9th) columns is due to the alternation of sine and cosine functions.
encoding_dim, num_steps = 32, 60 pos_encoding = PositionalEncoding(encoding_dim, 0) pos_encoding.initialize() X = pos_encoding(np.zeros((1, num_steps, encoding_dim))) P = pos_encoding.P[:, :X.shape[1], :] d2l.plot(np.arange(num_steps), P[0, :, 6:10].T, xlabel='Row (position)',
figsize=(6, 2.5), legend=["Col %d" % d for d in np.arange(6, 10)])

420

Chapter 10. Attention Mechanisms

Absolute Positional Information
To see how the monotonically decreased frequency along the encoding dimension relates to absolute positional information, let us print out the binary representations of 0, 1, . . . , 7. As we can see, the lowest bit, the second-lowest bit, and the third-lowest bit alternate on every number, every two numbers, and every four numbers, respectively.
for i in range(8): print(f'{i} in binary is {i:>03b}')
0 in binary is 000 1 in binary is 001 2 in binary is 010 3 in binary is 011 4 in binary is 100 5 in binary is 101 6 in binary is 110 7 in binary is 111
In binary representations, a higher bit has a lower frequency than a lower bit. Similarly, as demonstrated in the heat map below, the positional encoding decreases frequencies along the encoding dimension by using trigonometric functions. Since the outputs are float numbers, such continuous representations are more space-efficient than binary representations.
P = np.expand_dims(np.expand_dims(P[0, :, :], 0), 0) d2l.show_heatmaps(P, xlabel='Column (encoding dimension)',
ylabel='Row (position)', figsize=(3.5, 4), cmap='Blues')

10.6. Self-Attention and Positional Encoding

421

Relative Positional Information

Besides capturing absolute positional information, the above positional encoding also allows a model to easily learn to attend by relative positions. This is because for any fixed position offset δ, the positional encoding at position i + δ can be represented by a linear projection of that at position i.

This projection can be explained mathematically. Denoting ωj = 1/100002j/d, any pair of (pi,2j, pi,2j+1) in (10.6.2) can be linearly projected to (pi+δ,2j, pi+δ,2j+1) for any fixed offset δ:

[

][

]

cos(δωj) sin(δωj) pi,2j

− sin(δωj) cos(δωj) pi,2j+1

[

]

= cos(δωj) sin(iωj) + sin(δωj) cos(iωj)

− sin(δωj) sin(iωj) + cos(δωj) cos(iωj)

[

]

= sin ((i + δ)ωj)

cos ((i + δ)ωj)

[

]

= pi+δ,2j ,

pi+δ,2j+1

(10.6.3)

where the 2 × 2 projection matrix does not depend on any position index i.

422

Chapter 10. Attention Mechanisms

Summary
• In self-attention, the queries, keys, and values all come from the same place. • Both CNNs and self-attention enjoy parallel computation and self-attention has the shortest
maximum path length. However, the quadratic computational complexity with respect to the sequence length makes self-attention prohibitively slow for very long sequences. • To use the sequence order information, we can inject absolute or relative positional information by adding positional encoding to the input representations.
Exercises
1. Suppose that we design a deep architecture to represent a sequence by stacking selfattention layers with positional encoding. What could be issues?
2. Can you design a learnable positional encoding method? Discussions127
10.7 Transformer
We have compared CNNs, RNNs, and self-attention in Section 10.6.2. Notably, self-attention enjoys both parallel computation and the shortest maximum path length. Therefore natually, it is appealing to design deep architectures by using self-attention. Unlike earlier self-attention models that still rely on RNNs for input representations (Cheng et al., 2016; Lin et al., 2017b; Paulus et al., 2017), the transformer model is solely based on attention mechanisms without any convolutional or recurrent layer (Vaswani et al., 2017). Though originally proposed for sequence to sequence learning on text data, transformers have been pervasive in a wide range of modern deep learning applications, such as in areas of language, vision, speech, and reinforcement learning.
10.7.1 Model
As an instance of the encoder-decoder architecture, the overall architecture of the transformer is presented in Fig. 10.7.1. As we can see, the transformer is composed of an encoder and a decoder. Different from Bahdanau attention for sequence to sequence learning in Fig. 10.4.1, the input (source) and output (target) sequence embeddings are added with positional encoding before being fed into the encoder and the decoder that stack modules based on self-attention.
127 https://discuss.d2l.ai/t/1651

10.7. Transformer

423

Fig. 10.7.1: The transformer architecture.
Now we provide an overview of the transformer architecture in Fig. 10.7.1. On a high level, the transformer encoder is a stack of multiple identical layers, where each layer has two sublayers (either is denoted as sublayer). The first is a multi-head self-attention pooling and the second is a positionwise feed-forward network. Specifically, in the encoder self-attention, queries, keys, and values are all from the the outputs of the previous encoder layer. Inspired by the ResNet design in Section 7.6, a residual connection is employed around both sublayers. In the transformer, for any input x ∈ Rd at any position of the sequence, we require that sublayer(x) ∈ Rd so that the residual connection x+sublayer(x) ∈ Rd is feasible. This addition from the residual connection is immediately followed by layer normalization (Ba et al., 2016). As a result, the transformer encoder outputs a d-dimensional vector representation for each position of the input sequence.
The transformer decoder is also a stack of multiple identical layers with residual connections and

424

Chapter 10. Attention Mechanisms

layer normalizations. Besides the two sublayers described in the encoder, the decoder inserts a third sublayer, known as the encoder-decoder attention, between these two. In the encoderdecoder attention, queries are from the outputs of the previous decoder layer, and the keys and values are from the transformer encoder outputs. In the decoder self-attention, queries, keys, and values are all from the the outputs of the previous decoder layer. However, each position in the decoder is allowed to only attend to all positions in the decoder up to that position. This masked attention preserves the auto-regressive property, ensuring that the prediction only depends on those output tokens that have been generated.
We have already described and implemented multi-head attention based on scaled dot-products in Section 10.5 and positional encoding in Section 10.6.3. In the following, we will implement the rest of the transformer model.
import math import pandas as pd from mxnet import autograd, np, npx from mxnet.gluon import nn from d2l import mxnet as d2l
npx.set_np()

10.7.2 Positionwise Feed-Forward Networks
The positionwise feed-forward network transforms the representation at all the sequence positions using the same MLP. This is why we call it positionwise. In the implementation below, the input X with shape (batch size, number of time steps or sequence length in tokens, number of hidden units or feature dimension) will be transformed by a two-layer MLP into an output tensor of shape (batch size, number of time steps, ffn_num_outputs).
#@save class PositionWiseFFN(nn.Block):
"""Positionwise feed-forward network.""" def __init__(self, ffn_num_hiddens, ffn_num_outputs, **kwargs):
super(PositionWiseFFN, self).__init__(**kwargs) self.dense1 = nn.Dense(ffn_num_hiddens, flatten=False,
activation='relu') self.dense2 = nn.Dense(ffn_num_outputs, flatten=False)
def forward(self, X): return self.dense2(self.dense1(X))
The following example shows that the innermost dimension of a tensor changes to the number of outputs in the positionwise feed-forward network. Since the same MLP transforms at all the positions, when the inputs at all these positions are the same, their outputs are also identical.
ffn = PositionWiseFFN(4, 8) ffn.initialize() ffn(np.ones((2, 3, 4)))[0]

array([[ 0.00239431, -0.00162741,

0.00927085, -0.00021069, -0.00923989, -0.0082903 , 0.00659031, 0.00023905],

(continues on next page)

10.7. Transformer

425

[ 0.00239431, -0.00162741, [ 0.00239431, -0.00162741,

(continued from previous page)
0.00927085, -0.00021069, -0.00923989, -0.0082903 , 0.00659031, 0.00023905], 0.00927085, -0.00021069, -0.00923989, -0.0082903 , 0.00659031, 0.00023905]])

10.7.3 Residual Connection and Layer Normalization
Now let us focus on the “add & norm” component in Fig. 10.7.1. As we described at the beginning of this section, this is a residual connection immediately followed by layer normalization. Both are key to effective deep architectures.
In Section 7.5, we explained how batch normalization recenters and rescales across the examples within a minibatch. Layer normalization is the same as batch normalization except that the former normalizes across the feature dimension. Despite its pervasive applications in computer vision, batch normalization is usually empirically less effective than layer normalization in natural language processing tasks, whose inputs are often variable-length sequences.
The following code snippet compares the normalization across different dimensions by layer normalization and batch normalization.
ln = nn.LayerNorm() ln.initialize() bn = nn.BatchNorm() bn.initialize() X = np.array([[1, 2], [2, 3]]) # Compute mean and variance from `X` in the training mode with autograd.record():
print('layer norm:', ln(X), '\nbatch norm:', bn(X))
layer norm: [[-0.99998 0.99998] [-0.99998 0.99998]] batch norm: [[-0.99998 -0.99998] [ 0.99998 0.99998]]
Now we can implement the AddNorm class using a residual connection followed by layer normalization. Dropout is also applied for regularization.
#@save class AddNorm(nn.Block):
"""Residual connection followed by layer normalization.""" def __init__(self, dropout, **kwargs):
super(AddNorm, self).__init__(**kwargs) self.dropout = nn.Dropout(dropout) self.ln = nn.LayerNorm()
def forward(self, X, Y): return self.ln(self.dropout(Y) + X)
The residual connection requires that the two inputs are of the same shape so that the output tensor also has the same shape after the addition operation.

426

Chapter 10. Attention Mechanisms

add_norm = AddNorm(0.5) add_norm.initialize() add_norm(np.ones((2, 3, 4)), np.ones((2, 3, 4))).shape
(2, 3, 4)

10.7.4 Encoder
With all the essential components to assemble the transformer encoder, let us start by implementing a single layer within the encoder. The following EncoderBlock class contains two sublayers: multi-head self-attention and positionwise feed-forward networks, where a residual connection followed by layer normalization is employed around both sublayers.
#@save class EncoderBlock(nn.Block):
"""Transformer encoder block.""" def __init__(self, num_hiddens, ffn_num_hiddens, num_heads, dropout,
use_bias=False, **kwargs): super(EncoderBlock, self).__init__(**kwargs) self.attention = d2l.MultiHeadAttention(num_hiddens, num_heads,
dropout, use_bias) self.addnorm1 = AddNorm(dropout) self.ffn = PositionWiseFFN(ffn_num_hiddens, num_hiddens) self.addnorm2 = AddNorm(dropout)
def forward(self, X, valid_lens): Y = self.addnorm1(X, self.attention(X, X, X, valid_lens)) return self.addnorm2(Y, self.ffn(Y))
As we can see, any layer in the transformer encoder does not change the shape of its input.
X = np.ones((2, 100, 24)) valid_lens = np.array([3, 2]) encoder_blk = EncoderBlock(24, 48, 8, 0.5) encoder_blk.initialize() encoder_blk(X, valid_lens).shape

(2, 100, 24)

In the following transformer encoder implementation, we stack num_layers instances of the above EncoderBlock classes. Since we use the fixed positional encoding whose values are always between -1 and 1, we multiply values of the learnable input embeddings by the square root of the embedding dimension to rescale before summing up the input embedding and the positional encoding.

#@save class TransformerEncoder(d2l.Encoder):
"""Transformer encoder.""" def __init__(self, vocab_size, num_hiddens, ffn_num_hiddens,
num_layers, dropout, use_bias=False, **kwargs): super(TransformerEncoder, self).__init__(**kwargs)

num_heads,

(continues on next page)

10.7. Transformer

427

(continued from previous page)
self.num_hiddens = num_hiddens self.embedding = nn.Embedding(vocab_size, num_hiddens) self.pos_encoding = d2l.PositionalEncoding(num_hiddens, dropout) self.blks = nn.Sequential() for _ in range(num_layers):
self.blks.add( EncoderBlock(num_hiddens, ffn_num_hiddens, num_heads, dropout, use_bias))
def forward(self, X, valid_lens, *args): # Since positional encoding values are between -1 and 1, the embedding # values are multiplied by the square root of the embedding dimension # to rescale before they are summed up X = self.pos_encoding(self.embedding(X) * math.sqrt(self.num_hiddens)) self.attention_weights = [None] * len(self.blks) for i, blk in enumerate(self.blks): X = blk(X, valid_lens) self.attention_weights[ i] = blk.attention.attention.attention_weights return X
Below we specify hyperparameters to create a two-layer transformer encoder. The shape of the transformer encoder output is (batch size, number of time steps, num_hiddens).
encoder = TransformerEncoder(200, 24, 48, 8, 2, 0.5) encoder.initialize() encoder(np.ones((2, 100)), valid_lens).shape
(2, 100, 24)

10.7.5 Decoder
As shown in Fig. 10.7.1, the transformer decoder is composed of multiple identical layers. Each layer is implemented in the following DecoderBlock class, which contains three sublayers: decoder self-attention, encoder-decoder attention, and positionwise feed-forward networks. These sublayers employ a residual connection around them followed by layer normalization.
As we described earlier in this section, in the masked multi-head decoder self-attention (the first sublayer), queries, keys, and values all come from the outputs of the previous decoder layer. When training sequence-to-sequence models, tokens at all the positions (time steps) of the output sequence are known. However, during prediction the output sequence is generated token by token; thus, at any decoder time step only the generated tokens can be used in the decoder self-attention. To preserve auto-regression in the decoder, its masked self-attention specifies dec_valid_lens so that any query only attends to all positions in the decoder up to the query position.
class DecoderBlock(nn.Block): # The `i`-th block in the decoder def __init__(self, num_hiddens, ffn_num_hiddens, num_heads, dropout, i, **kwargs): super(DecoderBlock, self).__init__(**kwargs)
(continues on next page)

428

Chapter 10. Attention Mechanisms

(continued from previous page)
self.i = i self.attention1 = d2l.MultiHeadAttention(num_hiddens, num_heads,
dropout) self.addnorm1 = AddNorm(dropout) self.attention2 = d2l.MultiHeadAttention(num_hiddens, num_heads,
dropout) self.addnorm2 = AddNorm(dropout) self.ffn = PositionWiseFFN(ffn_num_hiddens, num_hiddens) self.addnorm3 = AddNorm(dropout)
def forward(self, X, state): enc_outputs, enc_valid_lens = state[0], state[1] # During training, all the tokens of any output sequence are processed # at the same time, so `state[2][self.i]` is `None` as initialized. # When decoding any output sequence token by token during prediction, # `state[2][self.i]` contains representations of the decoded output at # the `i`-th block up to the current time step if state[2][self.i] is None: key_values = X else: key_values = np.concatenate((state[2][self.i], X), axis=1) state[2][self.i] = key_values
if autograd.is_training(): batch_size, num_steps, _ = X.shape # Shape of `dec_valid_lens`: (`batch_size`, `num_steps`), where # every row is [1, 2, ..., `num_steps`] dec_valid_lens = np.tile(np.arange(1, num_steps + 1, ctx=X.ctx), (batch_size, 1))
else: dec_valid_lens = None
# Self-attention X2 = self.attention1(X, key_values, key_values, dec_valid_lens) Y = self.addnorm1(X, X2) # Encoder-decoder attention. Shape of `enc_outputs`: # (`batch_size`, `num_steps`, `num_hiddens`) Y2 = self.attention2(Y, enc_outputs, enc_outputs, enc_valid_lens) Z = self.addnorm2(Y, Y2) return self.addnorm3(Z, self.ffn(Z)), state
To facilitate scaled dot-product operations in the encoder-decoder attention and addition operations in the residual connections, the feature dimension (num_hiddens) of the decoder is the same as that of the encoder.
decoder_blk = DecoderBlock(24, 48, 8, 0.5, 0) decoder_blk.initialize() X = np.ones((2, 100, 24)) state = [encoder_blk(X, valid_lens), valid_lens, [None]] decoder_blk(X, state)[0].shape

(2, 100, 24) Now we construct the entire transformer decoder composed of num_layers instances of De-

10.7. Transformer

429

coderBlock. In the end, a fully-connected layer computes the prediction for all the vocab_size possible output tokens. Both of the decoder self-attention weights and the encoder-decoder attention weights are stored for later visualization.
class TransformerDecoder(d2l.AttentionDecoder): def __init__(self, vocab_size, num_hiddens, ffn_num_hiddens, num_heads, num_layers, dropout, **kwargs): super(TransformerDecoder, self).__init__(**kwargs) self.num_hiddens = num_hiddens self.num_layers = num_layers self.embedding = nn.Embedding(vocab_size, num_hiddens) self.pos_encoding = d2l.PositionalEncoding(num_hiddens, dropout) self.blks = nn.Sequential() for i in range(num_layers): self.blks.add( DecoderBlock(num_hiddens, ffn_num_hiddens, num_heads, dropout, i)) self.dense = nn.Dense(vocab_size, flatten=False)
def init_state(self, enc_outputs, enc_valid_lens, *args): return [enc_outputs, enc_valid_lens, [None] * self.num_layers]
def forward(self, X, state): X = self.pos_encoding(self.embedding(X) * math.sqrt(self.num_hiddens)) self._attention_weights = [[None] * len(self.blks) for _ in range(2)] for i, blk in enumerate(self.blks): X, state = blk(X, state) # Decoder self-attention weights self._attention_weights[0][ i] = blk.attention1.attention.attention_weights # Encoder-decoder attention weights self._attention_weights[1][ i] = blk.attention2.attention.attention_weights return self.dense(X), state
@property def attention_weights(self):
return self._attention_weights

10.7.6 Training
Let us instantiate an encoder-decoder model by following the transformer architecture. Here we specify that both the transformer encoder and the transformer decoder have 2 layers using 4head attention. Similar to Section 9.7.4, we train the transformer model for sequence to sequence learning on the English-French machine translation dataset.
num_hiddens, num_layers, dropout, batch_size, num_steps = 32, 2, 0.1, 64, 10 lr, num_epochs, device = 0.005, 200, d2l.try_gpu() ffn_num_hiddens, num_heads = 64, 4
train_iter, src_vocab, tgt_vocab = d2l.load_data_nmt(batch_size, num_steps)
encoder = TransformerEncoder(len(src_vocab), num_hiddens, ffn_num_hiddens, num_heads, num_layers, dropout) (continues on next page)

430

Chapter 10. Attention Mechanisms

(continued from previous page) decoder = TransformerDecoder(len(tgt_vocab), num_hiddens, ffn_num_hiddens,
num_heads, num_layers, dropout) net = d2l.EncoderDecoder(encoder, decoder) d2l.train_seq2seq(net, train_iter, lr, num_epochs, tgt_vocab, device)
loss 0.033, 2663.5 tokens/sec on gpu(0)

After training, we use the transformer model to translate a few English sentences into French and compute their BLEU scores.
engs = ['go .', "i lost .", 'he\'s calm .', 'i\'m home .'] fras = ['va !', 'j\'ai perdu .', 'il est calme .', 'je suis chez moi .'] for eng, fra in zip(engs, fras):
translation, dec_attention_weight_seq = d2l.predict_seq2seq( net, eng, src_vocab, tgt_vocab, num_steps, device, True)
print(f'{eng} => {translation}, ', f'bleu {d2l.bleu(translation, fra, k=2):.3f}')

go . => va !, bleu 1.000 i lost . => j'ai perdu ., bleu 1.000 he's calm . => il est calme ., bleu 1.000 i'm home . => je suis chez moi ., bleu 1.000
Let us visualize the transformer attention weights when translating the last English sentence into French. The shape of the encoder self-attention weights is (number of encoder layers, number of attention heads, num_steps or number of queries, num_steps or number of key-value pairs).
enc_attention_weights = np.concatenate(net.encoder.attention_weights, 0).reshape((num_layers, num_heads, -1, num_steps))
enc_attention_weights.shape

(2, 4, 10, 10)

In the encoder self-attention, both queries and keys come from the same input sequence. Since

10.7. Transformer

431

padding tokens do not carry meaning, with specified valid length of the input sequence, no query attends to positions of padding tokens. In the following, two layers of multi-head attention weights are presented row by row. Each head independently attends based on a separate representation subspaces of queries, keys, and values.
d2l.show_heatmaps(enc_attention_weights, xlabel='Key positions', ylabel='Query positions', titles=['Head %d' % i for i in range(1, 5)], figsize=(7, 3.5))

To visualize both the decoder self-attention weights and the encoder-decoder attention weights, we need more data manipulations. For example, we fill the masked attention weights with zero. Note that the decoder self-attention weights and the encoder-decoder attention weights both have the same queries: the beginning-of-sequence token followed by the output tokens.
dec_attention_weights_2d = [ np.array(head[0]).tolist() for step in dec_attention_weight_seq for attn in step for blk in attn for head in blk]
dec_attention_weights_filled = np.array( pd.DataFrame(dec_attention_weights_2d).fillna(0.0).values)
dec_attention_weights = dec_attention_weights_filled.reshape( (-1, 2, num_layers, num_heads, num_steps))
dec_self_attention_weights, dec_inter_attention_weights = \ dec_attention_weights.transpose(1, 2, 3, 0, 4)
dec_self_attention_weights.shape, dec_inter_attention_weights.shape

((2, 4, 6, 10), (2, 4, 6, 10))

Due to the auto-regressive property of the decoder self-attention, no query attends to key-value pairs after the query position.

# Plus one to include the beginning-of-sequence token d2l.show_heatmaps(

(continues on next page)

432

Chapter 10. Attention Mechanisms

(continued from previous page)
dec_self_attention_weights[:, :, :, :len(translation.split()) + 1], xlabel='Key positions', ylabel='Query positions', titles=['Head %d' % i for i in range(1, 5)], figsize=(7, 3.5))

Similar to the case in the encoder self-attention, via the specified valid length of the input sequence, no query from the output sequence attends to those padding tokens from the input sequence.
d2l.show_heatmaps(dec_inter_attention_weights, xlabel='Key positions', ylabel='Query positions', titles=['Head %d' % i for i in range(1, 5)], figsize=(7, 3.5))

Although the transformer architecture was originally proposed for sequence-to-sequence learn-

10.7. Transformer

433

ing, as we will discover later in the book, either the transformer encoder or the transformer decoder is often individually used for different deep learning tasks.
Summary
• The transformer is an instance of the encoder-decoder architecture, though either the encoder or the decoder can be used individually in practice.
• In the transformer, multi-head self-attention is used for representing the input sequence and the output sequence, though the decoder has to preserve the auto-regressive property via a masked version.
• Both the residual connections and the layer normalization in the transformer are important for training a very deep model.
• The positionwise feed-forward network in the transformer model transforms the representation at all the sequence positions using the same MLP.
Exercises
1. Train a deeper transformer in the experiments. How does it affect the training speed and the translation performance?
2. Is it a good idea to replace scaled dot-product attention with additive attention in the transformer? Why?
3. For language modeling, should we use the transformer encoder, decoder, or both? How to design this method?
4. What can be challenges to transformers if input sequences are very long? Why? 5. How to improve computational and memory efficiency of transformers? Hint: you may refer
to the survey paper by Tay et al. (Tay et al., 2020). 6. How can we design transformer-based models for image classification tasks without using
CNNs? Hint: you may refer to the vision transformer (Dosovitskiy et al., 2021). Discussions128

128 https://discuss.d2l.ai/t/348
434

Chapter 10. Attention Mechanisms

11 | Optimization Algorithms
If you read the book in sequence up to this point you already used a number of optimization algorithms to train deep learning models. They were the tools that allowed us to continue updating model parameters and to minimize the value of the loss function, as evaluated on the training set. Indeed, anyone content with treating optimization as a black box device to minimize objective functions in a simple setting might well content oneself with the knowledge that there exists an array of incantations of such a procedure (with names such as “SGD” and “Adam”). To do well, however, some deeper knowledge is required. Optimization algorithms are important for deep learning. On one hand, training a complex deep learning model can take hours, days, or even weeks. The performance of the optimization algorithm directly affects the modelʼs training efficiency. On the other hand, understanding the principles of different optimization algorithms and the role of their hyperparameters will enable us to tune the hyperparameters in a targeted manner to improve the performance of deep learning models. In this chapter, we explore common deep learning optimization algorithms in depth. Almost all optimization problems arising in deep learning are nonconvex. Nonetheless, the design and analysis of algorithms in the context of convex problems have proven to be very instructive. It is for that reason that this chapter includes a primer on convex optimization and the proof for a very simple stochastic gradient descent algorithm on a convex objective function.
11.1 Optimization and Deep Learning
In this section, we will discuss the relationship between optimization and deep learning as well as the challenges of using optimization in deep learning. For a deep learning problem, we will usually define a loss function first. Once we have the loss function, we can use an optimization algorithm in attempt to minimize the loss. In optimization, a loss function is often referred to as the objective function of the optimization problem. By tradition and convention most optimization algorithms are concerned with minimization. If we ever need to maximize an objective there is a simple solution: just flip the sign on the objective.
435

11.1.1 Goal of Optimization
Although optimization provides a way to minimize the loss function for deep learning, in essence, the goals of optimization and deep learning are fundamentally different. The former is primarily concerned with minimizing an objective whereas the latter is concerned with finding a suitable model, given a finite amount of data. In Section 4.4, we discussed the difference between these two goals in detail. For instance, training error and generalization error generally differ: since the objective function of the optimization algorithm is usually a loss function based on the training dataset, the goal of optimization is to reduce the training error. However, the goal of deep learning (or more broadly, statistical inference) is to reduce the generalization error. To accomplish the latter we need to pay attention to overfitting in addition to using the optimization algorithm to reduce the training error.
%matplotlib inline from mpl_toolkits import mplot3d from mxnet import np, npx from d2l import mxnet as d2l
npx.set_np()
To illustrate the aforementioned different goals, let us consider the empirical risk and the risk. As described in Section 4.9.3, the empirical risk is an average loss on the training dataset while the risk is the expected loss on the entire population of data. Below we define two functions: the risk function f and the empirical risk function g. Suppose that we have only a finite amount of training data. As a result, here g is less smooth than f.
def f(x): return x * np.cos(np.pi * x)
def g(x): return f(x) + 0.2 * np.cos(5 * np.pi * x)
The graph below illustrates that the minimum of the empirical risk on a training dataset may be at a different location from the minimum of the risk (generalization error).
def annotate(text, xy, xytext): #@save d2l.plt.gca().annotate(text, xy=xy, xytext=xytext, arrowprops=dict(arrowstyle='->'))
x = np.arange(0.5, 1.5, 0.01) d2l.set_figsize((4.5, 2.5)) d2l.plot(x, [f(x), g(x)], 'x', 'risk') annotate('min of\nempirical risk', (1.0, -1.2), (0.5, -1.1)) annotate('min of risk', (1.1, -1.05), (0.95, -0.5))

436

Chapter 11. Optimization Algorithms

11.1.2 Optimization Challenges in Deep Learning
In this chapter, we are going to focus specifically on the performance of optimization algorithms in minimizing the objective function, rather than a modelʼs generalization error. In Section 3.1 we distinguished between analytical solutions and numerical solutions in optimization problems. In deep learning, most objective functions are complicated and do not have analytical solutions. Instead, we must use numerical optimization algorithms. The optimization algorithms in this chapter all fall into this category.
There are many challenges in deep learning optimization. Some of the most vexing ones are local minima, saddle points, and vanishing gradients. Let us have a look at them.

Local Minima

For any objective function f (x), if the value of f (x) at x is smaller than the values of f (x) at any other points in the vicinity of x, then f (x) could be a local minimum. If the value of f (x) at x is the minimum of the objective function over the entire domain, then f (x) is the global minimum.
For example, given the function

f (x) = x · cos(πx) for − 1.0 ≤ x ≤ 2.0,

(11.1.1)

we can approximate the local minimum and global minimum of this function.
x = np.arange(-1.0, 2.0, 0.01) d2l.plot(x, [f(x),], 'x', 'f(x)') annotate('local minimum', (-0.3, -0.25), (-0.77, -1.0)) annotate('global minimum', (1.1, -0.95), (0.6, 0.8))

11.1. Optimization and Deep Learning

437

The objective function of deep learning models usually has many local optima. When the numerical solution of an optimization problem is near the local optimum, the numerical solution obtained by the final iteration may only minimize the objective function locally, rather than globally, as the gradient of the objective functionʼs solutions approaches or becomes zero. Only some degree of noise might knock the parameter out of the local minimum. In fact, this is one of the beneficial properties of minibatch stochastic gradient descent where the natural variation of gradients over minibatches is able to dislodge the parameters from local minima.
Saddle Points
Besides local minima, saddle points are another reason for gradients to vanish. A saddle point is any location where all gradients of a function vanish but which is neither a global nor a local minimum. Consider the function f (x) = x3. Its first and second derivative vanish for x = 0. Optimization might stall at this point, even though it is not a minimum.
x = np.arange(-2.0, 2.0, 0.01) d2l.plot(x, [x**3], 'x', 'f(x)') annotate('saddle point', (0, -0.2), (-0.52, -5.0))

Saddle points in higher dimensions are even more insidious, as the example below shows. Consider the function f (x, y) = x2 − y2. It has its saddle point at (0, 0). This is a maximum with

438

Chapter 11. Optimization Algorithms

respect to y and a minimum with respect to x. Moreover, it looks like a saddle, which is where this mathematical property got its name.

x, y = np.meshgrid(np.linspace(-1.0, 1.0, 101), np.linspace(-1.0, 1.0, 101)) z = x**2 - y**2

ax = d2l.plt.figure().add_subplot(111, projection='3d') ax.plot_wireframe(x, y, z, **{'rstride': 10, 'cstride': ax.plot([0], [0], [0], 'rx') ticks = [-1, 0, 1] d2l.plt.xticks(ticks) d2l.plt.yticks(ticks) ax.set_zticks(ticks) d2l.plt.xlabel('x') d2l.plt.ylabel('y');

10})

We assume that the input of a function is a k-dimensional vector and its output is a scalar, so its Hessian matrix will have k eigenvalues (refer to the online appendix on eigendecompositions129). The solution of the function could be a local minimum, a local maximum, or a saddle point at a position where the function gradient is zero:
• When the eigenvalues of the functionʼs Hessian matrix at the zero-gradient position are all positive, we have a local minimum for the function.
• When the eigenvalues of the functionʼs Hessian matrix at the zero-gradient position are all negative, we have a local maximum for the function.
• When the eigenvalues of the functionʼs Hessian matrix at the zero-gradient position are negative and positive, we have a saddle point for the function.
For high-dimensional problems the likelihood that at least some of the eigenvalues are negative is quite high. This makes saddle points more likely than local minima. We will discuss some exceptions to this situation in the next section when introducing convexity. In short, convex functions are those where the eigenvalues of the Hessian are never negative. Sadly, though, most deep learning problems do not fall into this category. Nonetheless it is a great tool to study optimization algorithms.
129 https://d2l.ai/chapter_appendix-mathematics-for-deep-learning/eigendecomposition.html

11.1. Optimization and Deep Learning

439

Vanishing Gradients
Probably the most insidious problem to encounter is the vanishing gradient. Recall our commonly-used activation functions and their derivatives in Section 4.1.2. For instance, assume that we want to minimize the function f (x) = tanh(x) and we happen to get started at x = 4. As we can see, the gradient of f is close to nil. More specifically, f ′(x) = 1 − tanh2(x) and thus f ′(4) = 0.0013. Consequently, optimization will get stuck for a long time before we make progress. This turns out to be one of the reasons that training deep learning models was quite tricky prior to the introduction of the ReLU activation function.
x = np.arange(-2.0, 5.0, 0.01) d2l.plot(x, [np.tanh(x)], 'x', 'f(x)') annotate('vanishing gradient', (4, 1), (2, 0.0))

As we saw, optimization for deep learning is full of challenges. Fortunately there exists a robust range of algorithms that perform well and that are easy to use even for beginners. Furthermore, it is not really necessary to find the best solution. Local optima or even approximate solutions thereof are still very useful.
Summary
• Minimizing the training error does not guarantee that we find the best set of parameters to minimize the generalization error.
• The optimization problems may have many local minima. • The problem may have even more saddle points, as generally the problems are not convex. • Vanishing gradients can cause optimization to stall. Often a reparameterization of the prob-
lem helps. Good initialization of the parameters can be beneficial, too.

440

Chapter 11. Optimization Algorithms

Exercises
1. Consider a simple MLP with a single hidden layer of, say, d dimensions in the hidden layer and a single output. Show that for any local minimum there are at least d! equivalent solutions that behave identically.
2. Assume that we have a symmetric random matrix M where the entries Mij = Mji are each drawn from some probability distribution pij. Furthermore assume that pij(x) = pij(−x), i.e., that the distribution is symmetric (see e.g., (Wigner, 1958) for details). 1. Prove that the distribution over eigenvalues is also symmetric. That is, for any eigenvector v the probability that the associated eigenvalue λ satisfies P (λ > 0) = P (λ < 0). 2. Why does the above not imply P (λ > 0) = 0.5?
3. What other challenges involved in deep learning optimization can you think of? 4. Assume that you want to balance a (real) ball on a (real) saddle.
1. Why is this hard? 2. Can you exploit this effect also for optimization algorithms? Discussions130
11.2 Convexity
Convexity plays a vital role in the design of optimization algorithms. This is largely due to the fact that it is much easier to analyze and test algorithms in such a context. In other words, if the algorithm performs poorly even in the convex setting, typically we should not hope to see great results otherwise. Furthermore, even though the optimization problems in deep learning are generally nonconvex, they often exhibit some properties of convex ones near local minima. This can lead to exciting new optimization variants such as (Izmailov et al., 2018).
%matplotlib inline from mpl_toolkits import mplot3d from mxnet import np, npx from d2l import mxnet as d2l
npx.set_np()
11.2.1 Definitions
Before convex analysis, we need to define convex sets and convex functions. They lead to mathematical tools that are commonly applied to machine learning.
130 https://discuss.d2l.ai/t/349

11.2. Convexity

441

Convex Sets

Sets are the basis of convexity. Simply put, a set X in a vector space is convex if for any a, b ∈ X the line segment connecting a and b is also in X . In mathematical terms this means that for all λ ∈ [0, 1] we have

λa + (1 − λ)b ∈ X whenever a, b ∈ X .

(11.2.1)

This sounds a bit abstract. Consider Fig. 11.2.1. The first set is not convex since there exist line segments that are not contained in it. The other two sets suffer no such problem.

Fig. 11.2.1: The first set is nonconvex and the other two are convex.
Definitions on their own are not particularly useful unless you can do something with them. In this case we can look at intersections as shown in Fig. 11.2.2. Assume that X and Y are convex sets. Then X ∩ Y is also convex. To see this, consider any a, b ∈ X ∩ Y. Since X and Y are convex, the line segments connecting a and b are contained in both X and Y. Given that, they also need to be contained in X ∩ Y, thus proving our theorem.

Fig. 11.2.2: The intersection between two convex sets is convex.
We can strengthen this result with little effort: given convex sets Xi, their intersection ∩iXi is convex. To see that the converse is not true, consider two disjoint sets X ∩ Y = ∅. Now pick a ∈ X and b ∈ Y. The line segment in Fig. 11.2.3 connecting a and b needs to contain some part that is neither in X nor in Y, since we assumed that X ∩ Y = ∅. Hence the line segment is not in X ∪ Y either, thus proving that in general unions of convex sets need not be convex.

442

Chapter 11. Optimization Algorithms

Fig. 11.2.3: The union of two convex sets need not be convex.
Typically the problems in deep learning are defined on convex sets. For instance, Rd, the set of d-dimensional vectors of real numbers, is a convex set (after all, the line between any two points in Rd remains in Rd). In some cases we work with variables of bounded length, such as balls of radius r as defined by {x|x ∈ Rd and ∥x∥ ≤ r}.

Convex Functions

Now that we have convex sets we can introduce convex functions f . Given a convex set X , a function f : X → R is convex if for all x, x′ ∈ X and for all λ ∈ [0, 1] we have

λf (x) + (1 − λ)f (x′) ≥ f (λx + (1 − λ)x′).

(11.2.2)

To illustrate this let us plot a few functions and check which ones satisfy the requirement. Below we define a few functions, both convex and nonconvex.
f = lambda x: 0.5 * x**2 # Convex g = lambda x: np.cos(np.pi * x) # Nonconvex h = lambda x: np.exp(0.5 * x) # Convex
x, segment = np.arange(-2, 2, 0.01), np.array([-1.5, 1]) d2l.use_svg_display() _, axes = d2l.plt.subplots(1, 3, figsize=(9, 3)) for ax, func in zip(axes, [f, g, h]):
d2l.plot([x, segment], [func(x), func(segment)], axes=ax)

11.2. Convexity

443

As expected, the cosine function is nonconvex, whereas the parabola and the exponential function
are. Note that the requirement that X is a convex set is necessary for the condition to make sense. Otherwise the outcome of f (λx + (1 − λ)x′) might not be well defined.

Jensen’s Inequality

Given a convex function f , one of the most useful mathematical tools is Jensen’s inequality. It

amounts to a generalization of the definition of convexity:

(

)

∑

∑

αif (xi) ≥ f

αixi and EX [f (X)] ≥ f (EX [X]) ,

(11.2.3)

i

i

∑ where αi are nonnegative real numbers such that i αi = 1 and X is a random variable. In other

words, the expectation of a convex function is no less than the convex function of an expectation,

where the latter is usually a simpler expression. To prove the first inequality we repeatedly apply

the definition of convexity to one term in the sum at a time.

One of the common applications of Jensenʼs inequality is to bound a more complicated expression by a simpler one. For example, its application can be with regard to the log-likelihood of partially observed random variables. That is, we use

EY ∼P (Y )[− log P (X | Y )] ≥ − log P (X),

(11.2.4)

∫ since P (Y )P (X | Y )dY = P (X). This can be used in variational methods. Here Y is typically

the unobserved random variable, P (Y ) is the best guess of how it might be distributed, and P (X)

is the distribution with Y integrated out. For instance, in clustering Y might be the cluster labels

and P (X | Y ) is the generative model when applying cluster labels.

11.2.2 Properties
Convex functions have many useful properties. We describe a few commonly-used ones below.

Local Minima Are Global Minima

First and foremost, the local minima of convex functions are also the global minima. We can prove it by contradiction as follows.

Consider a convex function f defined on a convex set X . Suppose that x∗ ∈ X is a local minimum: there exists a small positive value p so that for x ∈ X that satisfies 0 < |x − x∗| ≤ p we have f (x∗) < f (x).

Assume that the local minimum x∗ is not the global minumum of f : there exists x′ ∈ X for which

f

(x′)

<

f

(x∗).

There

also

exists

λ

∈

[0,

1)

such

as

λ

=

1−

p |x∗−x′|

so

that

0

<

|λx∗ +(1−λ)x′ −x∗ |

≤

p.

However, according to the definition of convex functions, we have

f (λx∗ + (1 − λ)x′) ≤ λf (x∗) + (1 − λ)f (x′) < λf (x∗) + (1 − λ)f (x∗) = f (x∗),

(11.2.5)

which contradicts with our statement that x∗ is a local minimum. Therefore, there does not exist x′ ∈ X for which f (x′) < f (x∗). The local minimum x∗ is also the global minimum.

444

Chapter 11. Optimization Algorithms

For instance, the convex function f (x) = (x − 1)2 has a local minimum at x = 1, which is also the global minimum.
f = lambda x: (x - 1)**2 d2l.set_figsize() d2l.plot([x, segment], [f(x), f(segment)], 'x', 'f(x)')

The fact that the local minima for convex functions are also the global minima is very convenient. It means that if we minimize functions we cannot “get stuck”. Note, though, that this does not mean that there cannot be more than one global minimum or that there might even exist one. For instance, the function f (x) = max(|x| − 1, 0) attains its minimum value over the interval [−1, 1]. Conversely, the function f (x) = exp(x) does not attain a minimum value on R: for x → −∞ it asymptotes to 0, but there is no x for which f (x) = 0.

Below Sets of Convex Functions Are Convex

We can conveniently define convex sets via below sets of convex functions. Concretely, given a convex function f defined on a convex set X , any below set

Sb := {x|x ∈ X and f (x) ≤ b}

(11.2.6)

is convex.
Let us prove this quickly. Recall that for any x, x′ ∈ Sb we need to show that λx + (1 − λ)x′ ∈ Sb as long as λ ∈ [0, 1]. Since f (x) ≤ b and f (x′) ≤ b, by the definition of convexity we have

f (λx + (1 − λ)x′) ≤ λf (x) + (1 − λ)f (x′) ≤ b.

(11.2.7)

11.2. Convexity

445

Convexity and Second Derivatives

Whenever the second derivative of a function f : Rn → R exists it is very easy to check whether f is convex. All we need to do is check whether the Hessian of f is positive semidefinite: ∇2f ⪰ 0, i.e., denoting the Hessian matrix ∇2f by H, x⊤Hx ≥ 0 for all x ∈ Rn. For instance, the function f (x) = 12 ∥x∥2 is convex since ∇2f = 1, i.e., its Hessian is an identity matrix.

Formally, a twice-differentiable one-dimensional function f : R → R is convex if and only if its second derivative f ′′ ≥ 0. For any twice-differentiable multi-dimensional function f : Rn → R, it is convex if and only if its Hessian ∇2f ⪰ 0.

First, we need to prove the one-dimensional case. To see that convexity of f implies f ′′ ≥ 0 we

use the fact that

(

)

1 f (x + ϵ) + 1 f (x − ϵ) ≥ f x + ϵ + x − ϵ = f (x).

2

2

2

2

(11.2.8)

Since the second derivative is given by the limit over finite differences it follows that

f ′′(x) = lim f (x + ϵ) + f (x − ϵ) − 2f (x) ≥ 0.

ϵ→0

ϵ2

(11.2.9)

To see that f ′′ ≥ 0 implies that f is convex we use the fact that f ′′ ≥ 0 implies that f ′ is a monotonically nondecreasing function. Let a < x < b be three points in R, where x = (1 − λ)a + λb and λ ∈ (0, 1). According to the mean value theorem, there exist α ∈ [a, x] and β ∈ [x, b] such that

f ′(α)

=

f (x)

−

f (a)

and

f ′(β)

=

f (b)

−

f (x) .

x−a

b−x

By monotonicity f ′(β) ≥ f ′(α), hence

(11.2.10)

x − a f (b) + b − x f (a) ≥ f (x).

b−a

b−a

Since x = (1 − λ)a + λb, we have

(11.2.11)

λf (b) + (1 − λ)f (a) ≥ f ((1 − λ)a + λb),

(11.2.12)

thus proving convexity.
Second, we need a lemma before proving the multi-dimensional case: f : Rn → R is convex if and only if for all x, y ∈ Rn

g(z) d=ef f (zx + (1 − z)y) where z ∈ [0, 1]

(11.2.13)

is convex.
To prove that convexity of f implies that g is convex, we can show that for all a, b, λ ∈ [0, 1] (thus 0 ≤ λa + (1 − λ)b ≤ 1)

g(λa + (1 − λ)b) =f ((λa + (1 − λ)b) x + (1 − λa − (1 − λ)b) y) =f (λ (ax + (1 − a)y) + (1 − λ) (bx + (1 − b)y)) ≤λf (ax + (1 − a)y) + (1 − λ)f (bx + (1 − b)y) =λg(a) + (1 − λ)g(b).

(11.2.14)

446

Chapter 11. Optimization Algorithms

To prove the converse, we can show that for all λ ∈ [0, 1]

f (λx + (1 − λ)y) =g(λ · 1 + (1 − λ) · 0) ≤λg(1) + (1 − λ)g(0) =λf (x) + (1 − λ)g(y).

(11.2.15)

Finally, using the lemma above and the result of the one-dimensional case, the multi-dimensional case can be proven as follows. A multi-dimensional function f : Rn → R is convex if and only if for all x, y ∈ Rn g(z) d=ef f (zx+(1−z)y), where z ∈ [0, 1], is convex. According to the one-dimensional case, this holds if and only if g′′ = (x − y)⊤H(x − y) ≥ 0 (H d=ef ∇2f ) for all x, y ∈ Rn, which is
equivalent to H ⪰ 0 per the definition of positive semidefinite matrices.

11.2.3 Constraints

One of the nice properties of convex optimization is that it allows us to handle constraints efficiently. That is, it allows us to solve constrained optimization problems of the form:

minimize f (x)
x
subject to ci(x) ≤ 0 for all i ∈ {1, . . . , n},

(11.2.16)

where f is the objective and the functions ci are constraint functions. To see what this does consider the case where c1(x) = ∥x∥2 − 1. In this case the parameters x are constrained to the unit ball. If a second constraint is c2(x) = v⊤x + b, then this corresponds to all x lying on a half-space. Satisfying both constraints simultaneously amounts to selecting a slice of a ball.

Lagrangian

In general, solving a constrained optimization problem is difficult. One way of addressing it stems from physics with a rather simple intuition. Imagine a ball inside a box. The ball will roll to the place that is lowest and the forces of gravity will be balanced out with the forces that the sides of the box can impose on the ball. In short, the gradient of the objective function (i.e., gravity) will be offset by the gradient of the constraint function (the ball need to remain inside the box by virtue of the walls “pushing back”). Note that some constraints may not be active: the walls that are not touched by the ball will not be able to exert any force on the ball.
Skipping over the derivation of the Lagrangian L, the above reasoning can be expressed via the following saddle point optimization problem:

∑n L(x, α1, . . . , αn) = f (x) + αici(x) where αi ≥ 0.
i=1

(11.2.17)

Here the variables αi (i = 1, . . . , n) are the so-called Lagrange multipliers that ensure that constraints are properly enforced. They are chosen just large enough to ensure that ci(x) ≤ 0 for all i. For instance, for any x where ci(x) < 0 naturally, weʼd end up picking αi = 0. Moreover, this is a saddle point optimization problem where one wants to maximize L with respect to all αi and simultaneously minimize it with respect to x. There is a rich body of literature explaining how to arrive at the function L(x, α1, . . . , αn). For our purposes it is sufficient to know that the saddle point of L is where the original constrained optimization problem is solved optimally.

11.2. Convexity

447

Penalties
One way of satisfying constrained optimization problems at least approximately is to adapt the Lagrangian L. Rather than satisfying ci(x) ≤ 0 we simply add αici(x) to the objective function f (x). This ensures that the constraints will not be violated too badly.
In fact, we have been using this trick all along. Consider weight decay in Section 4.5. In it we add λ2 ∥w∥2 to the objective function to ensure that w does not grow too large. From the constrained optimization point of view we can see that this will ensure that ∥w∥2 − r2 ≤ 0 for some radius r. Adjusting the value of λ allows us to vary the size of w.
In general, adding penalties is a good way of ensuring approximate constraint satisfaction. In practice this turns out to be much more robust than exact satisfaction. Furthermore, for nonconvex problems many of the properties that make the exact approach so appealing in the convex case (e.g., optimality) no longer hold.

Projections

An alternative strategy for satisfying constraints is projections. Again, we encountered them before, e.g., when dealing with gradient clipping in Section 8.5. There we ensured that a gradient has length bounded by θ via

g ← g · min(1, θ/∥g∥).

(11.2.18)

This turns out to be a projection of g onto the ball of radius θ. More generally, a projection on a convex set X is defined as

ProjX (x) = argmin ∥x − x′∥,
x′∈X

(11.2.19)

which is the closest point in X to x.

Fig. 11.2.4: Convex Projections.
The mathematical definition of projections may sound a bit abstract. Fig. 11.2.4 explains it somewhat more clearly. In it we have two convex sets, a circle and a diamond. Points inside both sets (yellow) remain unchanged during projections. Points outside both sets (black) are projected to the points inside the sets (red) that are closet to the original points (black). While for L2 balls this leaves the direction unchanged, this need not be the case in general, as can be seen in the case of the diamond.
One of the uses for convex projections is to compute sparse weight vectors. In this case we project weight vectors onto an L1 ball, which is a generalized version of the diamond case in Fig. 11.2.4.

448

Chapter 11. Optimization Algorithms

Summary
In the context of deep learning the main purpose of convex functions is to motivate optimization algorithms and help us understand them in detail. In the following we will see how gradient descent and stochastic gradient descent can be derived accordingly.
• Intersections of convex sets are convex. Unions are not.
• The expectation of a convex function is no less than the convex function of an expectation (Jensenʼs inequality).
• A twice-differentiable function is convex if and only if its Hessian (a matrix of second derivatives) is positive semidefinite.
• Convex constraints can be added via the Lagrangian. In practice we may simply add them with a penalty to the objective function.
• Projections map to points in the convex set closest to the original points.
Exercises
1. Assume that we want to verify convexity of a set by drawing all lines between points within the set and checking whether the lines are contained.
1. Prove that it is sufficient to check only the points on the boundary.
2. Prove that it is sufficient to check only the vertices of the set. 2. Denote by Bp[r] d=ef {x|x ∈ Rd and ∥x∥p ≤ r} the ball of radius r using the p-norm. Prove that
Bp[r] is convex for all p ≥ 1.
3. Given convex functions f and g, show that max(f, g) is convex, too. Prove that min(f, g) is not convex.
4. Prove that the normaliz∑ ation of the softmax function is convex. More specifically prove the convexity of f (x) = log i exp(xi).
5. Prove that linear subspaces, i.e., X = {x|Wx = b}, are convex sets.
6. Prove that in the case of linear subspaces with b = 0 the projection ProjX can be written as Mx for some matrix M.
7. Show that for twice-differentiable convex functions f we can write f (x + ϵ) = f (x) + ϵf ′(x) + 21 ϵ2f ′′(x + ξ) for some ξ ∈ [0, ϵ].
8. Given a vector w ∈ Rd with ∥w∥1 > 1 compute the projection on the L1 unit ball. 1. As an intermediate step write out the penalized objective ∥w − w′∥2 + λ∥w′∥1 and compute the solution for a given λ > 0.
2. Can you find the “right” value of λ without a lot of trial and error?
9. Given a convex set X and two vectors x and y, prove that projections never increase distances, i.e., ∥x − y∥ ≥ ∥ProjX (x) − ProjX (y)∥.
Discussions131
131 https://discuss.d2l.ai/t/350

11.2. Convexity

449

11.3 Gradient Descent
In this section we are going to introduce the basic concepts underlying gradient descent. Although it is rarely used directly in deep learning, an understanding of gradient descent is key to understanding stochastic gradient descent algorithms. For instance, the optimization problem might diverge due to an overly large learning rate. This phenomenon can already be seen in gradient descent. Likewise, preconditioning is a common technique in gradient descent and carries over to more advanced algorithms. Let us start with a simple special case.

11.3.1 One-Dimensional Gradient Descent

Gradient descent in one dimension is an excellent example to explain why the gradient descent algorithm may reduce the value of the objective function. Consider some continuously differentiable real-valued function f : R → R. Using a Taylor expansion we obtain

f (x + ϵ) = f (x) + ϵf ′(x) + O(ϵ2).

(11.3.1)

That is, in first-order approximation f (x+ϵ) is given by the function value f (x) and the first derivative f ′(x) at x. It is not unreasonable to assume that for small ϵ moving in the direction of the neg-
ative gradient will decrease f . To keep things simple we pick a fixed step size η > 0 and choose ϵ = −ηf ′(x). Plugging this into the Taylor expansion above we get

f (x − ηf ′(x)) = f (x) − ηf ′2(x) + O(η2f ′2(x)).

(11.3.2)

If the derivative f ′(x) ̸= 0 does not vanish we make progress since ηf ′2(x) > 0. Moreover, we can always choose η small enough for the higher-order terms to become irrelevant. Hence we arrive at

f (x − ηf ′(x)) ⪅ f (x).

(11.3.3)

This means that, if we use

x ← x − ηf ′(x)

(11.3.4)

to iterate x, the value of function f (x) might decline. Therefore, in gradient descent we first choose an initial value x and a constant η > 0 and then use them to continuously iterate x until the stop condition is reached, for example, when the magnitude of the gradient |f ′(x)| is small enough or the number of iterations has reached a certain value.
For simplicity we choose the objective function f (x) = x2 to illustrate how to implement gradient descent. Although we know that x = 0 is the solution to minimize f (x), we still use this simple function to observe how x changes.

%matplotlib inline from mxnet import np, npx from d2l import mxnet as d2l

npx.set_np()

def f(x): # Objective function return x**2
def f_grad(x): # Gradient (derivative) of the objective function return 2 * x

450

Chapter 11. Optimization Algorithms

Next, we use x = 10 as the initial value and assume η = 0.2. Using gradient descent to iterate x for 10 times we can see that, eventually, the value of x approaches the optimal solution.
def gd(eta, f_grad): x = 10.0 results = [x] for i in range(10): x -= eta * f_grad(x) results.append(float(x)) print(f'epoch 10, x: {x:f}') return results
results = gd(0.2, f_grad)
epoch 10, x: 0.060466
The progress of optimizing over x can be plotted as follows.
def show_trace(results, f): n = max(abs(min(results)), abs(max(results))) f_line = np.arange(-n, n, 0.01) d2l.set_figsize() d2l.plot([f_line, results], [[f(x) for x in f_line], [f(x) for x in results]], 'x', 'f(x)', fmts=['-', '-o'])
show_trace(results, f)

Learning Rate
The learning rate η can be set by the algorithm designer. If we use a learning rate that is too small, it will cause x to update very slowly, requiring more iterations to get a better solution. To show what happens in such a case, consider the progress in the same optimization problem for η = 0.05. As we can see, even after 10 steps we are still very far from the optimal solution.
show_trace(gd(0.05, f_grad), f)

11.3. Gradient Descent

451

epoch 10, x: 3.486784
Conversely, if we use an excessively high learning rate, |ηf ′(x)| might be too large for the first-order Taylor expansion formula. That is, the term O(η2f ′2(x)) in (11.3.2) might become significant. In this case, we cannot guarantee that the iteration of x will be able to lower the value of f (x). For example, when we set the learning rate to η = 1.1, x overshoots the optimal solution x = 0 and gradually diverges. show_trace(gd(1.1, f_grad), f) epoch 10, x: 61.917364

452

Chapter 11. Optimization Algorithms

Local Minima
To illustrate what happens for nonconvex functions consider the case of f (x) = x · cos(cx) for some constant c. This function has infinitely many local minima. Depending on our choice of the learning rate and depending on how well conditioned the problem is, we may end up with one of many solutions. The example below illustrates how an (unrealistically) high learning rate will lead to a poor local minimum.
c = np.array(0.15 * np.pi)
def f(x): # Objective function return x * np.cos(c * x)
def f_grad(x): # Gradient of the objective function return np.cos(c * x) - c * x * np.sin(c * x)
show_trace(gd(2, f_grad), f)
epoch 10, x: -1.528165

11.3.2 Multivariate Gradient Descent

Now that we have a better intuition of the univariate case, let us consider the situation where x = [x1, x2, . . . , xd]⊤. That is, the objective function f : Rd → R maps vectors into scalars. Corre-
spondingly its gradient is multivariate, too. It is a vector consisting of d partial derivatives:

[ ∂f (x) ∂f (x)

∂f (x) ]⊤

∇f (x) = ∂x1 , ∂x2 , . . . , ∂xd .

(11.3.5)

Each partial derivative element ∂f (x)/∂xi in the gradient indicates the rate of change of f at x with respect to the input xi. As before in the univariate case we can use the corresponding Taylor approximation for multivariate functions to get some idea of what we should do. In particular, we have that

f (x + ϵ) = f (x) + ϵ⊤∇f (x) + O(∥ϵ∥2).

(11.3.6)

11.3. Gradient Descent

453

In other words, up to second-order terms in ϵ the direction of steepest descent is given by the negative gradient −∇f (x). Choosing a suitable learning rate η > 0 yields the prototypical gradient descent algorithm:

x ← x − η∇f (x).

(11.3.7)

To see how the algorithm behaves in practice let us construct an objective function f (x) = x21 +2x22 with a two-dimensional vector x = [x1, x2]⊤ as input and a scalar as output. The gradient is given by ∇f (x) = [2x1, 4x2]⊤. We will observe the trajectory of x by gradient descent from the initial position [−5, −2].
To begin with, we need two more helper functions. The first uses an update function and applies it 20 times to the initial value. The second helper visualizes the trajectory of x.

def train_2d(trainer, steps=20, f_grad=None): #@save """Optimize a 2D objective function with a customized trainer.""" # `s1` and `s2` are internal state variables that will be used later x1, x2, s1, s2 = -5, -2, 0, 0 results = [(x1, x2)] for i in range(steps): if f_grad: x1, x2, s1, s2 = trainer(x1, x2, s1, s2, f_grad) else: x1, x2, s1, s2 = trainer(x1, x2, s1, s2) results.append((x1, x2)) print(f'epoch {i + 1}, x1: {float(x1):f}, x2: {float(x2):f}') return results

def show_trace_2d(f, results): #@save """Show the trace of 2D variables during optimization.""" d2l.set_figsize() d2l.plt.plot(*zip(*results), '-o', color='#ff7f0e') x1, x2 = np.meshgrid(np.arange(-5.5, 1.0, 0.1), np.arange(-3.0, 1.0, 0.1)) d2l.plt.contour(x1, x2, f(x1, x2), colors='#1f77b4') d2l.plt.xlabel('x1') d2l.plt.ylabel('x2')

Next, we observe the trajectory of the optimization variable x for learning rate η = 0.1. We can see that after 20 steps the value of x approaches its minimum at [0, 0]. Progress is fairly well-behaved albeit rather slow.
def f_2d(x1, x2): # Objective function return x1**2 + 2 * x2**2
def f_2d_grad(x1, x2): # Gradient of the objective function return (2 * x1, 4 * x2)
def gd_2d(x1, x2, s1, s2, f_grad): g1, g2 = f_grad(x1, x2) return (x1 - eta * g1, x2 - eta * g2, 0, 0)
eta = 0.1 show_trace_2d(f_2d, train_2d(gd_2d, f_grad=f_2d_grad))

454

Chapter 11. Optimization Algorithms

epoch 20, x1: -0.057646, x2: -0.000073

11.3.3 Adaptive Methods
As we could see in Section 11.3.1, getting the learning rate η “just right” is tricky. If we pick it too small, we make little progress. If we pick it too large, the solution oscillates and in the worst case it might even diverge. What if we could determine η automatically or get rid of having to select a learning rate at all? Second-order methods that look not only at the value and gradient of the objective function but also at its curvature can help in this case. While these methods cannot be applied to deep learning directly due to the computational cost, they provide useful intuition into how to design advanced optimization algorithms that mimic many of the desirable properties of the algorithms outlined below.

Newton’s Method

Reviewing the Taylor expansion of some function f : Rd → R there is no need to stop after the first term. In fact, we can write it as

f (x + ϵ) = f (x) + ϵ⊤∇f (x) + 1 ϵ⊤∇2f (x)ϵ + O(∥ϵ∥3). 2

(11.3.8)

To avoid cumbersome notation we define H d=ef ∇2f (x) to be the Hessian of f , which is a d × d matrix. For small d and simple problems H is easy to compute. For deep neural networks, on the other hand, H may be prohibitively large, due to the cost of storing O(d2) entries. Furthermore it may be too expensive to compute via backpropagation. For now let us ignore such considerations and look at what algorithm we would get.
After all, the minimum of f satisfies ∇f = 0. Following calculus rules in Section 2.4.3, by taking derivatives of (11.3.8) with regard to ϵ and ignoring higher-order terms we arrive at

∇f (x) + Hϵ = 0 and hence ϵ = −H−1∇f (x).

(11.3.9)

That is, we need to invert the Hessian H as part of the optimization problem.
As a simple example, for f (x) = 12 x2 we have ∇f (x) = x and H = 1. Hence for any x we obtain ϵ = −x. In other words, a single step is sufficient to converge perfectly without the need for any

11.3. Gradient Descent

455

adjustment! Alas, we got a bit lucky here: the Taylor expansion was exact since f (x + ϵ) = 12 x2 + ϵx + 21 ϵ2. Let us see what happens in other problems. Given a convex hyperbolic cosine function f (x) = cosh(cx) for some constant c, we can see that the global minimum at x = 0 is reached after a few iterations.
c = np.array(0.5)
def f(x): # Objective function return np.cosh(c * x)
def f_grad(x): # Gradient of the objective function return c * np.sinh(c * x)
def f_hess(x): # Hessian of the objective function return c**2 * np.cosh(c * x)
def newton(eta=1): x = 10.0 results = [x] for i in range(10): x -= eta * f_grad(x) / f_hess(x) results.append(float(x)) print('epoch 10, x:', x) return results
show_trace(newton(), f)
epoch 10, x: 0.0

Now let us consider a nonconvex function, such as f (x) = x cos(cx) for some constant c. After all, note that in Newtonʼs method we end up dividing by the Hessian. This means that if the second derivative is negative we may walk into the direction of increasing the value of f . That is a fatal flaw of the algorithm. Let us see what happens in practice.
c = np.array(0.15 * np.pi)
(continues on next page)

456

Chapter 11. Optimization Algorithms

def f(x): # Objective function return x * np.cos(c * x)
def f_grad(x): # Gradient of the objective function return np.cos(c * x) - c * x * np.sin(c * x)
def f_hess(x): # Hessian of the objective function return -2 * c * np.sin(c * x) - x * c**2 * np.cos(c * x)
show_trace(newton(), f)
epoch 10, x: 26.834133

(continued from previous page)

This went spectacularly wrong. How can we fix it? One way would be to “fix” the Hessian by taking its absolute value instead. Another strategy is to bring back the learning rate. This seems to defeat the purpose, but not quite. Having second-order information allows us to be cautious whenever the curvature is large and to take longer steps whenever the objective function is flatter. Let us see how this works with a slightly smaller learning rate, say η = 0.5. As we can see, we have quite an efficient algorithm.
show_trace(newton(0.5), f)
epoch 10, x: 7.26986

11.3. Gradient Descent

457

Convergence Analysis

We only analyze the convergence rate of Newtonʼs method for some convex and three times differentiable objective function f , where the second derivative is nonzero, i.e., f ′′ > 0. The multivariate proof is a straightforward extension of the one-dimensional argument below and omitted since it does not help us much in terms of intuition.

Denote by x(k) the value of x at the kth iteration and let e(k) d=ef x(k) − x∗ be the distance from optimality at the kth iteration. By Taylor expansion we have that the condition f ′(x∗) = 0 can be
written as

0 = f ′(x(k) − e(k)) = f ′(x(k)) − e(k)f ′′(x(k)) + 1 (e(k))2f ′′′(ξ(k)), 2

(11.3.10)

which holds for some ξ(k) ∈ [x(k) − e(k), x(k)]. Dividing the above expansion by f ′′(x(k)) yields

(k) f ′(x(k)) 1 (k) 2 f ′′′(ξ(k))

e

−

f ′′(x(k))

=

(e 2

) f ′′(x(k)) .

(11.3.11)

Recall that we have the update x(k+1) = x(k) − f ′(x(k))/f ′′(x(k)). Plugging in this update equation and taking the absolute value of both sides, we have

e(k+1) = 1 (e(k))2 f ′′′(ξ(k)) .

2

f ′′(x(k))

(11.3.12)

Consequently, whenever we are in a region of bounded f ′′′(ξ(k)) /(2f ′′(x(k))) ≤ c, we have a quadratically decreasing error

e(k+1) ≤ c(e(k))2.

(11.3.13)

As an aside, optimization researchers call this linear convergence, whereas a condition such as e(k+1) ≤ α e(k) would be called a constant rate of convergence. Note that this analysis comes with a number of caveats. First, we do not really have much of a guarantee when we will reach the region of rapid convergence. Instead, we only know that once we reach it, convergence will be very quick. Second, this analysis requires that f is well-behaved up to higher-order derivatives. It comes down to ensuring that f does not have any “surprising” properties in terms of how it might change its values.

458

Chapter 11. Optimization Algorithms

Preconditioning

Quite unsurprisingly computing and storing the full Hessian is very expensive. It is thus desirable to find alternatives. One way to improve matters is preconditioning. It avoids computing the Hessian in its entirety but only computes the diagonal entries. This leads to update algorithms of the form

x ← x − ηdiag(H)−1∇f (x).

(11.3.14)

While this is not quite as good as the full Newtonʼs method, it is still much better than not using it. To see why this might be a good idea consider a situation where one variable denotes height in millimeters and the other one denotes height in kilometers. Assuming that for both the natural scale is in meters, we have a terrible mismatch in parameterizations. Fortunately, using preconditioning removes this. Effectively preconditioning with gradient descent amounts to selecting a different learning rate for each variable (coordinate of vector x). As we will see later, preconditioning drives some of the innovation in stochastic gradient descent optimization algorithms.

Gradient Descent with Line Search
One of the key problems in gradient descent is that we might overshoot the goal or make insufficient progress. A simple fix for the problem is to use line search in conjunction with gradient descent. That is, we use the direction given by ∇f (x) and then perform binary search as to which learning rate η minimizes f (x − η∇f (x)).
This algorithm converges rapidly (for an analysis and proof see e.g., (Boyd & Vandenberghe, 2004)). However, for the purpose of deep learning this is not quite so feasible, since each step of the line search would require us to evaluate the objective function on the entire dataset. This is way too costly to accomplish.

Summary
• Learning rates matter. Too large and we diverge, too small and we do not make progress. • Gradient descent can get stuck in local minima. • In high dimensions adjusting the learning rate is complicated. • Preconditioning can help with scale adjustment. • Newtonʼs method is a lot faster once it has started working properly in convex problems. • Beware of using Newtonʼs method without any adjustments for nonconvex problems.

Exercises
1. Experiment with different learning rates and objective functions for gradient descent. 2. Implement line search to minimize a convex function in the interval [a, b].
1. Do you need derivatives for binary search, i.e., to decide whether to pick [a, (a + b)/2] or [(a + b)/2, b].
2. How rapid is the rate of convergence for the algorithm?

11.3. Gradient Descent

459

3. Implement the algorithm and apply it to minimizing log(exp(x) + exp(−2x − 3)). 3. Design an objective function defined on R2 where gradient descent is exceedingly slow. Hint:
scale different coordinates differently. 4. Implement the lightweight version of Newtonʼs method using preconditioning:
1. Use diagonal Hessian as preconditioner. 2. Use the absolute values of that rather than the actual (possibly signed) values. 3. Apply this to the problem above. 5. Apply the algorithm above to a number of objective functions (convex or not). What happens if you rotate coordinates by 45 degrees? Discussions132
11.4 Stochastic Gradient Descent
In earlier chapters we kept using stochastic gradient descent in our training procedure, however, without explaining why it works. To shed some light on it, we just described the basic principles of gradient descent in Section 11.3. In this section, we go on to discuss stochastic gradient descent in greater detail.
%matplotlib inline import math from mxnet import np, npx from d2l import mxnet as d2l
npx.set_np()

11.4.1 Stochastic Gradient Updates

In deep learning, the objective function is usually the average of the loss functions for each example in the training dataset. Given a training dataset of n examples, we assume that fi(x) is the loss function with respect to the training example of index i, where x is the parameter vector. Then we arrive at the objective function

1 ∑n f (x) = n fi(x).
i=1

(11.4.1)

The gradient of the objective function at x is computed as

1 ∑n ∇f (x) = n ∇fi(x).
i=1

(11.4.2)

If gradient descent is used, the computational cost for each independent variable iteration is O(n), which grows linearly with n. Therefore, when the training dataset is larger, the cost of gradient descent for each iteration will be higher.
132 https://discuss.d2l.ai/t/351

460

Chapter 11. Optimization Algorithms

Stochastic gradient descent (SGD) reduces computational cost at each iteration. At each iteration of stochastic gradient descent, we uniformly sample an index i ∈ {1, . . . , n} for data examples at random, and compute the gradient ∇fi(x) to update x:

x ← x − η∇fi(x),

(11.4.3)

where η is the learning rate. We can see that the computational cost for each iteration drops from O(n) of the gradient descent to the constant O(1). Moreover, we want to emphasize that the stochastic gradient ∇fi(x) is an unbiased estimate of the full gradient ∇f (x) because

1 ∑n Ei∇fi(x) = n ∇fi(x) = ∇f (x).
i=1

(11.4.4)

This means that, on average, the stochastic gradient is a good estimate of the gradient.
Now, we will compare it with gradient descent by adding random noise with a mean of 0 and a variance of 1 to the gradient to simulate a stochastic gradient descent.

def f(x1, x2): # Objective function return x1**2 + 2 * x2**2

def f_grad(x1, x2): # Gradient of the objective function return 2 * x1, 4 * x2

def sgd(x1, x2, s1, s2, f_grad): g1, g2 = f_grad(x1, x2) # Simulate noisy gradient g1 += np.random.normal(0.0, 1, (1,)) g2 += np.random.normal(0.0, 1, (1,)) eta_t = eta * lr() return (x1 - eta_t * g1, x2 - eta_t * g2, 0, 0)

def constant_lr(): return 1
eta = 0.1 lr = constant_lr # Constant learning rate d2l.show_trace_2d(f, d2l.train_2d(sgd, steps=50, f_grad=f_grad))

epoch 50, x1: -0.472513, x2: 0.110780

11.4. Stochastic Gradient Descent

461

As we can see, the trajectory of the variables in the stochastic gradient descent is much more noisy than the one we observed in gradient descent in Section 11.3. This is due to the stochastic nature of the gradient. That is, even when we arrive near the minimum, we are still subject to the uncertainty injected by the instantaneous gradient via η∇fi(x). Even after 50 steps the quality is still not so good. Even worse, it will not improve after additional steps (we encourage you to experiment with a larger number of steps to confirm this). This leaves us with the only alternative: change the learning rate η. However, if we pick this too small, we will not make any meaningful progress initially. On the other hand, if we pick it too large, we will not get a good solution, as seen above. The only way to resolve these conflicting goals is to reduce the learning rate dynamically as optimization progresses.
This is also the reason for adding a learning rate function lr into the sgd step function. In the example above any functionality for learning rate scheduling lies dormant as we set the associated lr function to be constant.

11.4.2 Dynamic Learning Rate

Replacing η with a time-dependent learning rate η(t) adds to the complexity of controlling convergence of an optimization algorithm. In particular, we need to figure out how rapidly η should decay. If it is too quick, we will stop optimizing prematurely. If we decrease it too slowly, we waste too much time on optimization. The following are a few basic strategies that are used in adjusting η over time (we will discuss more advanced strategies later):

η(t) = ηi if ti ≤ t ≤ ti+1 η(t) = η0 · e−λt η(t) = η0 · (βt + 1)−α

piecewise constant exponential decay polynomial decay

(11.4.5)

In the first piecewise constant scenario we decrease the learning rate, e.g., whenever progress in optimization stalls. This is a common strategy for training deep networks. Alternatively we could decrease it much more aggressively by an exponential decay. Unfortunately this often leads to premature stopping before the algorithm has converged. A popular choice is polynomial decay with α = 0.5. In the case of convex optimization there are a number of proofs that show that this rate is well behaved.
Let us see what the exponential decay looks like in practice.

462

Chapter 11. Optimization Algorithms

def exponential_lr(): # Global variable that is defined outside this function and updated inside global t t += 1 return math.exp(-0.1 * t)
t=1 lr = exponential_lr d2l.show_trace_2d(f, d2l.train_2d(sgd, steps=1000, f_grad=f_grad))
epoch 1000, x1: -0.820458, x2: 0.004701

As expected, the variance in the parameters is significantly reduced. However, this comes at the expense of failing to converge to the optimal solution x = (0, 0). Even after 1000 iteration steps are we are still very far away from the optimal solution. Indeed, the algorithm fails to converge at all. On the other hand, if we use a polynomial decay where the learning rate decays with the inverse square root of the number of steps, convergence gets better after only 50 steps.
def polynomial_lr(): # Global variable that is defined outside this function and updated inside global t t += 1 return (1 + 0.1 * t)**(-0.5)
t=1 lr = polynomial_lr d2l.show_trace_2d(f, d2l.train_2d(sgd, steps=50, f_grad=f_grad))
epoch 50, x1: 0.025029, x2: 0.115820

11.4. Stochastic Gradient Descent

463

There exist many more choices for how to set the learning rate. For instance, we could start with a small rate, then rapidly ramp up and then decrease it again, albeit more slowly. We could even alternate between smaller and larger learning rates. There exists a large variety of such schedules. For now let us focus on learning rate schedules for which a comprehensive theoretical analysis is possible, i.e., on learning rates in a convex setting. For general nonconvex problems it is very difficult to obtain meaningful convergence guarantees, since in general minimizing nonlinear nonconvex problems is NP hard. For a survey see e.g., the excellent lecture notes133 of Tibshirani 2015.

11.4.3 Convergence Analysis for Convex Objectives

The following convergence analysis of stochastic gradient descent for convex objective functions is optional and primarily serves to convey more intuition about the problem. We limit ourselves to one of the simplest proofs (Nesterov & Vial, 2000). Significantly more advanced proof techniques exist, e.g., whenever the objective function is particularly well behaved.
Suppose that the objective function f (ξ, x) is convex in x for all ξ. More concretely, we consider the stochastic gradient descent update:

xt+1 = xt − ηt∂xf (ξt, x),

(11.4.6)

where f (ξt, x) is the objective function with respect to the training example ξt drawn from some distribution at step t and x is the model parameter. Denote by

R(x) = Eξ[f (ξ, x)]

(11.4.7)

the expected risk and by R∗ its minimum with regard to x. Last let x∗ be the minimizer (we assume
that it exists within the domain where x is defined). In this case we can track the distance between the current parameter xt at time t and the risk minimizer x∗ and see whether it improves over time:

∥xt+1 − x∗∥2 =∥xt − ηt∂xf (ξt, x) − x∗∥2 =∥xt − x∗∥2 + ηt2∥∂xf (ξt, x)∥2 − 2ηt ⟨xt − x∗, ∂xf (ξt, x)⟩ .

(11.4.8)

133 https://www.stat.cmu.edu/~ryantibs/convexopt-F15/lectures/26-nonconvex.pdf

464

Chapter 11. Optimization Algorithms

We assume that the L2 norm of stochastic gradient ∂xf (ξt, x) is bounded by some constant L, hence we have that

ηt2∥∂xf (ξt, x)∥2 ≤ ηt2L2.

(11.4.9)

We are mostly interested in how the distance between xt and x∗ changes in expectation. In fact,
for any specific sequence of steps the distance might well increase, depending on whichever ξt we encounter. Hence we need to bound the dot product. Since for any convex function f it holds that f (y) ≥ f (x) + ⟨f ′(x), y − x⟩ for all x and y, by convexity we have

f (ξt, x∗) ≥ f (ξt, xt) + ⟨x∗ − xt, ∂xf (ξt, xt)⟩ .

(11.4.10)

Plugging both inequalities (11.4.9) and (11.4.10) into (11.4.8) we obtain a bound on the distance between parameters at time t + 1 as follows:

∥xt − x∗∥2 − ∥xt+1 − x∗∥2 ≥ 2ηt(f (ξt, xt) − f (ξt, x∗)) − ηt2L2.

(11.4.11)

This means that we make progress as long as the difference between current loss and the optimal loss outweighs ηtL2/2. Since this difference is bound to converge to zero it follows that the learning
rate ηt also needs to vanish.

Next we take expectations over (11.4.11). This yields E [∥xt − x∗∥2] − E [∥xt+1 − x∗∥2] ≥ 2ηt[E[R(xt)] − R∗] − ηt2L2.

(11.4.12)

The last step involves summing over the inequalities for t ∈ {1, . . . , T }. Since the sum telescopes

and by dropping the lower term we obtain

(∑ T ) ∑ T

∥x1 − x∗∥2 ≥ 2

ηt [E[R(xt)] − R∗] − L2 ηt2.

(11.4.13)

t=1

t=1

Note that we exploited that x1 is given and thus the expectation can be dropped. Last define

∑T
def

ηtxt

x¯ = ∑t=T1 .

t=1 ηt

(11.4.14)

Since

( ∑T

) ∑T

E t∑=1 ηtR(xt) = t=∑ 1 ηtE[R(xt)] = E[R(xt)],

T t=1

ηt

T t=1

ηt

(11.4.15)

by Jensenʼs inequality (setting i = t, αi = ηt/ ∑Tt=1 ηt in (11.2.3)) and convexity of R it follows that

E[R(xt)] ≥ E[R(x¯)], thus

∑ T

∑ T

ηtE[R(xt)] ≥ ηtE [R(x¯)] .

t=1

t=1

(11.4.16)

Plugging this into the inequality (11.4.13) yields the bound

r2 + L2 ∑T η2

[E[x¯]] − R∗ ≤

∑T t=1 t ,

2 t=1 ηt

(11.4.17)

where r2 d=ef ∥x1 − x∗∥2 is a bound on the distance between the initial choice of parameters and the final outcome. In short, the speed of convergence depends on how the norm of stochastic gradient is bounded (L) and how far away from optimality the initial parameter value is (r). Note that the bound is in terms of x¯ rather than xT . This is the case since x¯ is a smoothed version of th√e optimization path. Whenever r, L, a√nd T are known we can pick the learnin√g rate η = r/(L T ). This yields as upper bound rL/ T . That is, we converge with rate O(1/ T ) to the optimal solution.

11.4. Stochastic Gradient Descent

465

11.4.4 Stochastic Gradients and Finite Samples

So far we have played a bit fast and loose when it comes to talking about stochastic gradient de-

scent. We posited that we draw instances xi, typically with labels yi from some distribution p(x, y)

and that we use this to update the model parameters in some manner∑ . In particular, for a finite

sample

size

we

simply

argued

that

the

discrete

distribution

p(x, y)

=

1 n

n i=1

δxi

(x)δyi

(y)

for

some

functions δxi and δyi allows us to perform stochastic gradient descent over it.

However, this is not really what we did. In the toy examples in the current section we simply added noise to an otherwise non-stochastic gradient, i.e., we pretended to have pairs (xi, yi). It turns out that this is justified here (see the exercises for a detailed discussion). More troubling is that in all previous discussions we clearly did not do this. Instead we iterated over all instances exactly once. To see why this is preferable consider the converse, namely that we are sampling n observations from the discrete distribution with replacement. The probability of choosing an element i at random is 1/n. Thus to choose it at least once is

P (choose i) = 1 − P (omit i) = 1 − (1 − 1/n)n ≈ 1 − e−1 ≈ 0.63.

(11.4.18)

A similar reasoning shows that the probability of picking some sample (i.e., training example) exactly once is given by

() (

)n−1

(

)n

n1

1

1−

n =

1 1−

≈ e−1 ≈ 0.37.

1n

n

n−1

n

(11.4.19)

This leads to an increased variance and decreased data efficiency relative to sampling without replacement. Hence, in practice we perform the latter (and this is the default choice throughout this book). Last note that repeated passes through the training dataset traverse it in a different random order.

Summary
• For convex problems we can prove that for a wide choice of learning rates stochastic gradient descent will converge to the optimal solution.
• For deep learning this is generally not the case. However, the analysis of convex problems gives us useful insight into how to approach optimization, namely to reduce the learning rate progressively, albeit not too quickly.
• Problems occur when the learning rate is too small or too large. In practice a suitable learning rate is often found only after multiple experiments.
• When there are more examples in the training dataset, it costs more to compute each iteration for gradient descent, so stochastic gradient descent is preferred in these cases.
• Optimality guarantees for stochastic gradient descent are in general not available in nonconvex cases since the number of local minima that require checking might well be exponential.

466

Chapter 11. Optimization Algorithms

Exercises
1. Experiment with different learning rate schedules for stochastic gradient descent and with different numbers of iterations. In particular, plot the distance from the optimal solution (0, 0) as a function of the number of iterations.
2. Prove that for the function f (x1, x2) = x21 + 2x22 adding normal noise to the gradient is equivalent to minimizing a loss function f (x, w) = (x1 − w1)2 + 2(x2 − w2)2 where x is drawn from a normal distribution.
3. Compare convergence of stochastic gradient descent when you sample from {(x1, y1), . . . , (xn, yn)} with replacement and when you sample without replacement.
4. How would you change the stochastic gradient descent solver if some gradient (or rather some coordinate associated with it) was consistently larger than all the other gradients?
5. Assume that f (x) = x2(1 + sin x). How many local minima does f have? Can you change f in such a way that to minimize it one needs to evaluate all the local minima?
Discussions134

11.5 Minibatch Stochastic Gradient Descent
So far we encountered two extremes in the approach to gradient based learning: Section 11.3 uses the full dataset to compute gradients and to update parameters, one pass at a time. Conversely Section 11.4 processes one observation at a time to make progress. Each of them has its own drawbacks. Gradient Descent is not particularly data efficient whenever data is very similar. Stochastic Gradient Descent is not particularly computationally efficient since CPUs and GPUs cannot exploit the full power of vectorization. This suggests that there might be a happy medium, and in fact, thatʼs what we have been using so far in the examples we discussed.

11.5.1 Vectorization and Caches
At the heart of the decision to use minibatches is computational efficiency. This is most easily understood when considering parallelization to multiple GPUs and multiple servers. In this case we need to send at least one image to each GPU. With 8 GPUs per server and 16 servers we already arrive at a minibatch size of 128.
Things are a bit more subtle when it comes to single GPUs or even CPUs. These devices have multiple types of memory, often multiple type of compute units and different bandwidth constraints between them. For instance, a CPU has a small number of registers and then L1, L2 and in some cases even L3 cache (which is shared between the different processor cores). These caches are of increasing size and latency (and at the same time they are of decreasing bandwidth). Suffice it to say, the processor is capable of performing many more operations than what the main memory interface is able to provide.
• A 2GHz CPU with 16 cores and AVX-512 vectorization can process up to 2 · 109 · 16 · 32 = 1012 bytes per second. The capability of GPUs easily exceeds this number by a factor of 100. On the other hand, a midrange server processor might not have much more than 100 GB/s bandwidth, i.e., less than one tenth of what would be required to keep the processor fed. To make matters worse, not all memory access is created equal: first, memory interfaces are
134 https://discuss.d2l.ai/t/352

11.5. Minibatch Stochastic Gradient Descent

467

typically 64 bit wide or wider (e.g., on GPUs up to 384 bit), hence reading a single byte incurs the cost of a much wider access.
• There is significant overhead for the first access whereas sequential access is relatively cheap (this is often called a burst read). There are many more things to keep in mind, such as caching when we have multiple sockets, chiplets and other structures. A detailed discussion of this is beyond the scope of this section. See e.g., this Wikipedia article135 for a more indepth discussion.
The way to alleviate these constraints is to use a hierarchy of CPU caches which are actually fast enough to supply the processor with data. This is the driving force behind batching in deep learning. To keep matters simple, consider matrix-matrix multiplication, say A = BC. We have a number of options for calculating A. For instance we could try the following:
1. We could compute Aij = Bi,:C⊤:,j, i.e., we could compute it elementwise by means of dot products.
2. We could compute A:,j = BC⊤:,j, i.e., we could compute it one column at a time. Likewise we could compute A one row Ai,: at a time.
3. We could simply compute A = BC.
4. We could break B and C into smaller block matrices and compute A one block at a time.
If we follow the first option, we will need to copy one row and one column vector into the CPU each time we want to compute an element Aij. Even worse, due to the fact that matrix elements are aligned sequentially we are thus required to access many disjoint locations for one of the two vectors as we read them from memory. The second option is much more favorable. In it, we are able to keep the column vector C:,j in the CPU cache while we keep on traversing through B. This halves the memory bandwidth requirement with correspondingly faster access. Of course, option 3 is most desirable. Unfortunately, most matrices might not entirely fit into cache (this is what we are discussing after all). However, option 4 offers a practically useful alternative: we can move blocks of the matrix into cache and multiply them locally. Optimized libraries take care of this for us. Let us have a look at how efficient these operations are in practice.
Beyond computational efficiency, the overhead introduced by Python and by the deep learning framework itself is considerable. Recall that each time we execute a command the Python interpreter sends a command to the MXNet engine which needs to insert it into the computational graph and deal with it during scheduling. Such overhead can be quite detrimental. In short, it is highly advisable to use vectorization (and matrices) whenever possible.
%matplotlib inline from mxnet import autograd, gluon, init, np, npx from mxnet.gluon import nn from d2l import mxnet as d2l
npx.set_np()
timer = d2l.Timer() A = np.zeros((256, 256)) B = np.random.normal(0, 1, (256, 256)) C = np.random.normal(0, 1, (256, 256))
Element-wise assignment simply iterates over all rows and columns of B and C respectively to assign the value to A.
135 https://en.wikipedia.org/wiki/Cache_hierarchy

468

Chapter 11. Optimization Algorithms

# Compute A = BC one element at a time timer.start() for i in range(256):
for j in range(256): A[i, j] = np.dot(B[i, :], C[:, j])
A.wait_to_read() timer.stop()

68.20669913291931

A faster strategy is to perform column-wise assignment.

# Compute A = BC one column at a timer.start() for j in range(256):
A[:, j] = np.dot(B, C[:, j]) A.wait_to_read() timer.stop()

time

0.22968602180480957
Last, the most effective manner is to perform the entire operation in one block. Let us see what the respective speed of the operations is.
# Compute A = BC in one go timer.start() A = np.dot(B, C) A.wait_to_read() timer.stop()
# Multiply and add count as separate operations (fused in practice) gigaflops = [2 / i for i in timer.times] print(f'performance in Gigaflops: element {gigaflops[0]:.3f}, '
f'column {gigaflops[1]:.3f}, full {gigaflops[2]:.3f}')

performance in Gigaflops: element 0.029, column 8.708, full 436.316

11.5.2 Minibatches

In the past we took it for granted that we would read minibatches of data rather than single observations to update parameters. We now give a brief justification for it. Processing single observations requires us to perform many single matrix-vector (or even vector-vector) multiplications, which is quite expensive and which incurs a significant overhead on behalf of the underlying deep learning framework. This applies both to evaluating a network when applied to data (often referred to as inference) and when computing gradients to update parameters. That is, this applies whenever we perform w ← w − ηtgt where

gt = ∂wf (xt, w)

(11.5.1)

We can increase the computational efficiency of this operation by applying it to a minibatch of observations at a time. That is, we replace the gradient gt over a single observation by one over a

11.5. Minibatch Stochastic Gradient Descent

469

small batch

1∑ gt = ∂w |Bt| f (xi, w)
i∈Bt

(11.5.2)

Let us see what this does to the statistical properties of gt: since both xt and also all elements

of the minibatch Bt are drawn uniformly at random from the training set, the expectation of the

gradient remains unchanged. The variance, on the other hand, is reduced significantly. Since the

minibatch gradient is composed of b := |Bt| independent gradients which are being averaged, its

standard

deviation

is

reduced

by

a

factor

of

b−

1 2

.

This,

by

itself,

is

a

good

thing,

since

it

means

that

the updates are more reliably aligned with the full gradient.

Naively this would indicate that choosing a large minibatch Bt would be universally desirable. Alas, after some point, the additional reduction in standard deviation is minimal when compared to the linear increase in computational cost. In practice we pick a minibatch that is large enough to offer good computational efficiency while still fitting into the memory of a GPU. To illustrate the savings let us have a look at some code. In it we perform the same matrix-matrix multiplication, but this time broken up into “minibatches” of 64 columns at a time.

timer.start() for j in range(0, 256, 64):
A[:, j:j + 64] = np.dot(B, C[:, j:j + 64]) timer.stop() print(f'performance in Gigaflops: block {2 / timer.times[3]:.3f}')

performance in Gigaflops: block 501.471
As we can see, the computation on the minibatch is essentially as efficient as on the full matrix. A word of caution is in order. In Section 7.5 we used a type of regularization that was heavily dependent on the amount of variance in a minibatch. As we increase the latter, the variance decreases and with it the benefit of the noise-injection due to batch normalization. See e.g., (Ioffe, 2017) for details on how to rescale and compute the appropriate terms.

11.5.3 Reading the Dataset
Let us have a look at how minibatches are efficiently generated from data. In the following we use a dataset developed by NASA to test the wing noise from different aircraft136 to compare these optimization algorithms. For convenience we only use the first 1, 500 examples. The data is whitened for preprocessing, i.e., we remove the mean and rescale the variance to 1 per coordinate.
#@save d2l.DATA_HUB['airfoil'] = (d2l.DATA_URL + 'airfoil_self_noise.dat',
'76e5be1548fd8222e5074cf0faae75edff8cf93f')
#@save def get_data_ch11(batch_size=10, n=1500):
data = np.genfromtxt(d2l.download('airfoil'), dtype=np.float32, delimiter='\t')
data = (data - data.mean(axis=0)) / data.std(axis=0) data_iter = d2l.load_array((data[:n, :-1], data[:n, -1]), batch_size,
(continues on next page)
136 https://archive.ics.uci.edu/ml/datasets/Airfoil+Self-Noise

470

Chapter 11. Optimization Algorithms

is_train=True) return data_iter, data.shape[1] - 1

(continued from previous page)

11.5.4 Implementation from Scratch
Recall the minibatch stochastic gradient descent implementation from Section 3.2. In the following we provide a slightly more general implementation. For convenience it has the same call signature as the other optimization algorithms introduced later in this chapter. Specifically, we add the status input states and place the hyperparameter in dictionary hyperparams. In addition, we will average the loss of each minibatch example in the training function, so the gradient in the optimization algorithm does not need to be divided by the batch size.
def sgd(params, states, hyperparams): for p in params: p[:] -= hyperparams['lr'] * p.grad

Next, we implement a generic training function to facilitate the use of the other optimization algorithms introduced later in this chapter. It initializes a linear regression model and can be used to train the model with minibatch stochastic gradient descent and other algorithms introduced subsequently.
#@save def train_ch11(trainer_fn, states, hyperparams, data_iter, feature_dim,
num_epochs=2): # Initialization w = np.random.normal(scale=0.01, size=(feature_dim, 1)) b = np.zeros(1) w.attach_grad() b.attach_grad() net, loss = lambda X: d2l.linreg(X, w, b), d2l.squared_loss # Train animator = d2l.Animator(xlabel='epoch', ylabel='loss',
xlim=[0, num_epochs], ylim=[0.22, 0.35]) n, timer = 0, d2l.Timer() for _ in range(num_epochs):
for X, y in data_iter: with autograd.record(): l = loss(net(X), y).mean() l.backward() trainer_fn([w, b], states, hyperparams) n += X.shape[0] if n % 200 == 0: timer.stop() animator.add(n / X.shape[0] / len(data_iter), (d2l.evaluate_loss(net, data_iter, loss),)) timer.start()
print(f'loss: {animator.Y[0][-1]:.3f}, {timer.avg():.3f} sec/epoch') return timer.cumsum(), animator.Y[0]

Let us see how optimization proceeds for batch gradient descent. This can be achieved by setting the minibatch size to 1500 (i.e., to the total number of examples). As a result the model parameters are updated only once per epoch. There is little progress. In fact, after 6 steps progress stalls.

11.5. Minibatch Stochastic Gradient Descent

471

def train_sgd(lr, batch_size, num_epochs=2): data_iter, feature_dim = get_data_ch11(batch_size) return train_ch11(sgd, None, {'lr': lr}, data_iter, feature_dim, num_epochs)
gd_res = train_sgd(1, 1500, 10)
loss: 0.254, 0.244 sec/epoch

When the batch size equals 1, we use stochastic gradient descent for optimization. For simplicity of implementation we picked a constant (albeit small) learning rate. In stochastic gradient descent, the model parameters are updated whenever an example is processed. In our case this amounts to 1500 updates per epoch. As we can see, the decline in the value of the objective function slows down after one epoch. Although both the procedures processed 1500 examples within one epoch, stochastic gradient descent consumes more time than gradient descent in our experiment. This is because stochastic gradient descent updated the parameters more frequently and since it is less efficient to process single observations one at a time.
sgd_res = train_sgd(0.005, 1)
loss: 0.244, 0.985 sec/epoch

472

Chapter 11. Optimization Algorithms

Finally, when the batch size equals 100, we use minibatch stochastic gradient descent for optimization. The time required per epoch is shorter than the time needed for stochastic gradient descent and the time for batch gradient descent.
mini1_res = train_sgd(.4, 100)
loss: 0.243, 0.020 sec/epoch

Reducing the batch size to 10, the time for each epoch increases because the workload for each batch is less efficient to execute. mini2_res = train_sgd(.05, 10)
loss: 0.245, 0.180 sec/epoch

11.5. Minibatch Stochastic Gradient Descent

473

Now we can compare the time vs. loss for the previous four experiments. As can be seen, although stochastic gradient descent converges faster than GD in terms of number of examples processed, it uses more time to reach the same loss than GD because computing the gradient example by example is not as efficient. Minibatch stochastic gradient descent is able to trade-off convergence speed and computation efficiency. A minibatch size of 10 is more efficient than stochastic gradient descent; a minibatch size of 100 even outperforms GD in terms of runtime.
d2l.set_figsize([6, 3]) d2l.plot(*list(map(list, zip(gd_res, sgd_res, mini1_res, mini2_res))),
'time (sec)', 'loss', xlim=[1e-2, 10], legend=['gd', 'sgd', 'batch size=100', 'batch size=10']) d2l.plt.gca().set_xscale('log')

474

Chapter 11. Optimization Algorithms

11.5.5 Concise Implementation
In Gluon, we can use the Trainer class to call optimization algorithms. This is used to implement a generic training function. We will use this throughout the current chapter.
#@save def train_concise_ch11(tr_name, hyperparams, data_iter, num_epochs=2):
# Initialization net = nn.Sequential() net.add(nn.Dense(1)) net.initialize(init.Normal(sigma=0.01)) trainer = gluon.Trainer(net.collect_params(), tr_name, hyperparams) loss = gluon.loss.L2Loss() animator = d2l.Animator(xlabel='epoch', ylabel='loss',
xlim=[0, num_epochs], ylim=[0.22, 0.35]) n, timer = 0, d2l.Timer() for _ in range(num_epochs):
for X, y in data_iter: with autograd.record(): l = loss(net(X), y) l.backward() trainer.step(X.shape[0]) n += X.shape[0] if n % 200 == 0: timer.stop() animator.add(n / X.shape[0] / len(data_iter), (d2l.evaluate_loss(net, data_iter, loss),)) timer.start()
print(f'loss: {animator.Y[0][-1]:.3f}, {timer.avg():.3f} sec/epoch')
Using Gluon to repeat the last experiment shows identical behavior.
data_iter, _ = get_data_ch11(10) train_concise_ch11('sgd', {'learning_rate': 0.05}, data_iter)
loss: 0.246, 0.133 sec/epoch

11.5. Minibatch Stochastic Gradient Descent

475

Summary
• Vectorization makes code more efficient due to reduced overhead arising from the deep learning framework and due to better memory locality and caching on CPUs and GPUs.
• There is a trade-off between statistical efficiency arising from stochastic gradient descent and computational efficiency arising from processing large batches of data at a time.
• Minibatch stochastic gradient descent offers the best of both worlds: computational and statistical efficiency.
• In minibatch stochastic gradient descent we process batches of data obtained by a random permutation of the training data (i.e., each observation is processed only once per epoch, albeit in random order).
• It is advisable to decay the learning rates during training.
• In general, minibatch stochastic gradient descent is faster than stochastic gradient descent and gradient descent for convergence to a smaller risk, when measured in terms of clock time.
Exercises
1. Modify the batch size and learning rate and observe the rate of decline for the value of the objective function and the time consumed in each epoch.
2. Read the MXNet documentation and use the Trainer class set_learning_rate function to reduce the learning rate of the minibatch stochastic gradient descent to 1/10 of its previous value after each epoch.
3. Compare minibatch stochastic gradient descent with a variant that actually samples with replacement from the training set. What happens?
4. An evil genie replicates your dataset without telling you (i.e., each observation occurs twice and your dataset grows to twice its original size, but nobody told you). How does the behavior of stochastic gradient descent, minibatch stochastic gradient descent and that of gradient descent change?
Discussions137
11.6 Momentum
In Section 11.4 we reviewed what happens when performing stochastic gradient descent, i.e., when performing optimization where only a noisy variant of the gradient is available. In particular, we noticed that for noisy gradients we need to be extra cautious when it comes to choosing the learning rate in the face of noise. If we decrease it too rapidly, convergence stalls. If we are too lenient, we fail to converge to a good enough solution since noise keeps on driving us away from optimality.
137 https://discuss.d2l.ai/t/353

476

Chapter 11. Optimization Algorithms

11.6.1 Basics
In this section, we will explore more effective optimization algorithms, especially for certain types of optimization problems that are common in practice.

Leaky Averages

The previous section saw us discussing minibatch SGD as a means for accelerating computation. It also had the nice side-effect that averaging gradients reduced the amount of variance. The minibatch stochastic gradient descent can be calculated by:

1∑

1∑

gt,t−1 = ∂w |Bt| f (xi, wt−1) = |Bt| hi,t−1.

i∈Bt

i∈Bt

(11.6.1)

To keep the notation simple, here we used hi,t−1 = ∂wf (xi, wt−1) as the stochastic gradient descent for sample i using the weights updated at time t − 1. It would be nice if we could benefit from the effect of variance reduction even beyond averaging gradients on a minibatch. One option to accomplish this task is to replace the gradient computation by a “leaky average”:

vt = βvt−1 + gt,t−1

(11.6.2)

for some β ∈ (0, 1). This effectively replaces the instantaneous gradient by one thatʼs been averaged over multiple past gradients. v is called momentum. It accumulates past gradients similar to how a heavy ball rolling down the objective function landscape integrates over past forces. To see what is happening in more detail let us expand vt recursively into

∑ t−1 vt = β2vt−2 + βgt−1,t−2 + gt,t−1 = . . . , = βτ gt−τ,t−τ −1.
τ =0

(11.6.3)

Large β amounts to a long-range average, whereas small β amounts to only a slight correction relative to a gradient method. The new gradient replacement no longer points into the direction of steepest descent on a particular instance any longer but rather in the direction of a weighted average of past gradients. This allows us to realize most of the benefits of averaging over a batch without the cost of actually computing the gradients on it. We will revisit this averaging procedure in more detail later.
The above reasoning formed the basis for what is now known as accelerated gradient methods, such as gradients with momentum. They enjoy the additional benefit of being much more effective in cases where the optimization problem is ill-conditioned (i.e., where there are some directions where progress is much slower than in others, resembling a narrow canyon). Furthermore, they allow us to average over subsequent gradients to obtain more stable directions of descent. Indeed, the aspect of acceleration even for noise-free convex problems is one of the key reasons why momentum works and why it works so well.
As one would expect, due to its efficacy momentum is a well-studied subject in optimization for deep learning and beyond. See e.g., the beautiful expository article138 by (Goh, 2017) for an indepth analysis and interactive animation. It was proposed by (Polyak, 1964). (Nesterov, 2018) has a detailed theoretical discussion in the context of convex optimization. Momentum in deep learning has been known to be beneficial for a long time. See e.g., the discussion by (Sutskever et al., 2013) for details.
138 https://distill.pub/2017/momentum/

11.6. Momentum

477

An Ill-conditioned Problem

To get a better understanding of the geometric properties of the momentum method we revisit
gradient descent, albeit with a significantly less pleasant objective function. Recall that in Section 11.3 we used f (x) = x21 + 2x22, i.e., a moderately distorted ellipsoid objective. We distort this function further by stretching it out in the x1 direction via

f (x) = 0.1x21 + 2x22.

(11.6.4)

As before f has its minimum at (0, 0). This function is very flat in the direction of x1. Let us see what happens when we perform gradient descent as before on this new function. We pick a learning rate of 0.4.

%matplotlib inline from mxnet import np, npx from d2l import mxnet as d2l

npx.set_np()

eta = 0.4 def f_2d(x1, x2):
return 0.1 * x1 ** 2 + 2 * x2 ** 2 def gd_2d(x1, x2, s1, s2):
return (x1 - eta * 0.2 * x1, x2 - eta * 4 * x2, 0, 0)

d2l.show_trace_2d(f_2d, d2l.train_2d(gd_2d))

epoch 20, x1: -0.943467, x2: -0.000073

By construction, the gradient in the x2 direction is much higher and changes much more rapidly than in the horizontal x1 direction. Thus we are stuck between two undesirable choices: if we pick a small learning rate we ensure that the solution does not diverge in the x2 direction but we are saddled with slow convergence in the x1 direction. Conversely, with a large learning rate we progress rapidly in the x1 direction but diverge in x2. The example below illustrates what happens even after a slight increase in learning rate from 0.4 to 0.6. Convergence in the x1 direction improves but the overall solution quality is much worse.

478

Chapter 11. Optimization Algorithms

eta = 0.6 d2l.show_trace_2d(f_2d, d2l.train_2d(gd_2d))
epoch 20, x1: -0.387814, x2: -1673.365109

The Momentum Method

The momentum method allows us to solve the gradient descent problem described above. Looking at the optimization trace above we might intuit that averaging gradients over the past would work well. After all, in the x1 direction this will aggregate well-aligned gradients, thus increasing the distance we cover with every step. Conversely, in the x2 direction where gradients oscillate, an aggregate gradient will reduce step size due to oscillations that cancel each other out. Using vt instead of the gradient gt yields the following update equations:

vt ← βvt−1 + gt,t−1, xt ← xt−1 − ηtvt.

(11.6.5)

Note that for β = 0 we recover regular gradient descent. Before delving deeper into the mathematical properties let us have a quick look at how the algorithm behaves in practice.

def momentum_2d(x1, x2, v1, v2): v1 = beta * v1 + 0.2 * x1 v2 = beta * v2 + 4 * x2 return x1 - eta * v1, x2 - eta * v2, v1, v2

eta, beta = 0.6, 0.5 d2l.show_trace_2d(f_2d, d2l.train_2d(momentum_2d))

epoch 20, x1: 0.007188, x2: 0.002553

11.6. Momentum

479

As we can see, even with the same learning rate that we used before, momentum still converges well. Let us see what happens when we decrease the momentum parameter. Halving it to β = 0.25 leads to a trajectory that barely converges at all. Nonetheless, it is a lot better than without momentum (when the solution diverges).
eta, beta = 0.6, 0.25 d2l.show_trace_2d(f_2d, d2l.train_2d(momentum_2d))
epoch 20, x1: -0.126340, x2: -0.186632

Note that we can combine momentum with stochastic gradient descent and in particular, minibatch stochastic gradient descent. The only change is that in that case we replace the gradients gt,t−1 with gt. Last, for convenience we initialize v0 = 0 at time t = 0. Let us look at what leaky averaging actually does to the updates.

480

Chapter 11. Optimization Algorithms

Effective Sample Weight

Recall that vt = ∑tτ−=10 βτ gt−τ,t−τ−1. In the limit the terms add up to ∑∞ τ=0 βτ = 1−1β . In other words, rather than taking a step of size η in gradient descent or stochastic gradient descent we take

a

step

of

size

η 1−β

while

at

the

same

time,

dealing

with

a

potentially

much

better

behaved

descent

direction. These are two benefits in one. To illustrate how weighting behaves for different choices

of β consider the diagram below.

d2l.set_figsize() betas = [0.95, 0.9, 0.6, 0] for beta in betas:
x = np.arange(40).asnumpy() d2l.plt.plot(x, beta**x, label=f'beta = {beta:.2f}') d2l.plt.xlabel('time') d2l.plt.legend();

11.6.2 Practical Experiments
Let us see how momentum works in practice, i.e., when used within the context of a proper optimizer. For this we need a somewhat more scalable implementation.

Implementation from Scratch
Compared with (minibatch) stochastic gradient descent the momentum method needs to maintain a set of auxiliary variables, i.e., velocity. It has the same shape as the gradients (and variables of the optimization problem). In the implementation below we call these variables states.
def init_momentum_states(feature_dim): v_w = np.zeros((feature_dim, 1)) v_b = np.zeros(1) return (v_w, v_b)

def sgd_momentum(params, states, hyperparams): for p, v in zip(params, states):

(continues on next page)

11.6. Momentum

481

v[:] = hyperparams['momentum'] * v + p.grad p[:] -= hyperparams['lr'] * v

(continued from previous page)

Let us see how this works in practice.
def train_momentum(lr, momentum, num_epochs=2): d2l.train_ch11(sgd_momentum, init_momentum_states(feature_dim), { 'lr': lr, 'momentum': momentum}, data_iter, feature_dim, num_epochs)
data_iter, feature_dim = d2l.get_data_ch11(batch_size=10) train_momentum(0.02, 0.5)

loss: 0.245, 0.132 sec/epoch

When we increase the momentum hyperparameter momentum to 0.9, it amounts to a significantly

larger effective sample size of

1 1−0.9

=

10.

We reduce the learning rate slightly to 0.01 to keep

matters under control.

train_momentum(0.01, 0.9)

loss: 0.251, 0.130 sec/epoch

482

Chapter 11. Optimization Algorithms

Reducing the learning rate further addresses any issue of non-smooth optimization problems. Setting it to 0.005 yields good convergence properties. train_momentum(0.005, 0.9)
loss: 0.243, 0.368 sec/epoch

Concise Implementation
There is very little to do in Gluon since the standard sgd solver already had momentum built in. Setting matching parameters yields a very similar trajectory.
d2l.train_concise_ch11('sgd', { 'learning_rate': 0.005, 'momentum': 0.9}, data_iter)
loss: 0.244, 0.445 sec/epoch

11.6. Momentum

483

11.6.3 Theoretical Analysis
So far the 2D example of f (x) = 0.1x21 + 2x22 seemed rather contrived. We will now see that this is actually quite representative of the types of problem one might encounter, at least in the case of minimizing convex quadratic objective functions.

Quadratic Convex Functions

Consider the function

h(x) = 1 x⊤Qx + x⊤c + b. 2

(11.6.6)

This is a general quadratic function. For positive definite matrices Q ≻ 0, i.e., for matrices with
positive eigenvalues this has a minimizer at x∗ = −Q−1c with minimum value b− 12 c⊤Q−1c. Hence we can rewrite h as

h(x) = 1 (x − Q−1c)⊤Q(x − Q−1c) + b − 1 c⊤Q−1c.

2

2

(11.6.7)

The gradient is given by ∂xf (x) = Q(x − Q−1c). That is, it is given by the distance between x and the minimizer, multiplied by Q. Consequently also the momentum is a linear combination of terms Q(xt − Q−1c).
Since Q is positive definite it can be decomposed into its eigensystem via Q = O⊤ΛO for an orthogonal (rotation) matrix O and a diagonal matrix Λ of positive eigenvalues. This allows us to perform a change of variables from x to z := O(x − Q−1c) to obtain a much simplified expression:

h(z) = 1 z⊤Λz + b′. 2

(11.6.8)

Here b′ = b − 12 c⊤Q−1c. Since O is only an orthogonal matrix this does not perturb the gradients in a meaningful way. Expressed in terms of z gradient descent becomes

zt = zt−1 − Λzt−1 = (I − Λ)zt−1.

(11.6.9)

484

Chapter 11. Optimization Algorithms

The important fact in this expression is that gradient descent does not mix between different eigenspaces. That is, when expressed in terms of the eigensystem of Q the optimization problem proceeds in a coordinate-wise manner. This also holds for momentum.

vt = βvt−1 + Λzt−1 zt = zt−1 − η (βvt−1 + Λzt−1)
= (I − ηΛ)zt−1 − ηβvt−1.

(11.6.10)

In doing this we just proved the following theorem: Gradient Descent with and without momentum for a convex quadratic function decomposes into coordinate-wise optimization in the direction of the eigenvectors of the quadratic matrix.

Scalar Functions

Given the above result let us see what happens when we minimize the function f (x) = λ2 x2. For gradient descent we have

xt+1 = xt − ηλxt = (1 − ηλ)xt.

(11.6.11)

Whenever |1 − ηλ| < 1 this optimization converges at an exponential rate since after t steps we have xt = (1 − ηλ)tx0. This shows how the rate of convergence improves initially as we increase the learning rate η until ηλ = 1. Beyond that things diverge and for ηλ > 2 the optimization
problem diverges.

lambdas = [0.1, 1, 10, 19] eta = 0.1 d2l.set_figsize((6, 4)) for lam in lambdas:
t = np.arange(20).asnumpy() d2l.plt.plot(t, (1 - eta * lam)**t, label=f'lambda = {lam:.2f}') d2l.plt.xlabel('time') d2l.plt.legend();

11.6. Momentum

485

To analyze convergence in the case of momentum we begin by rewriting the update equations in

terms of two scalars: one for x and one for the momentum v. This yields:

[ ][

vt+1 = β

xt+1

−ηβ

][ ]

[]

λ

vt = R(β, η, λ) vt .

(1 − ηλ) xt

xt

(11.6.12)

We used R to denote the 2 × 2 governing convergence behavior. After t steps the initial choice [v0, x0] becomes R(β, η, λ)t[v0, x0]. Hence, it is up to the eigenvalues of R to determine the speed of convergence. See the Distill post139 of (Goh, 2017) for a great animation and (Flammarion &
Bach, 2015) for a detailed analysis. One can show that 0 < ηλ < 2 + 2β momentum converges.
This is a larger range of feasible parameters when compared to 0 < ηλ < 2 for gradient descent. It
also suggests that in general large values of β are desirable. Further details require a fair amount
of technical detail and we suggest that the interested reader consult the original publications.

Summary

• Momentum replaces gradients with a leaky average over past gradients. This accelerates convergence significantly.

• It is desirable for both noise-free gradient descent and (noisy) stochastic gradient descent.

• Momentum prevents stalling of the optimization process that is much more likely to occur for stochastic gradient descent.

•

The effective number of gradients is given by

1 1−β

due to exponentiated downweighting of

past data.

• In the case of convex quadratic problems this can be analyzed explicitly in detail.

• Implementation is quite straightforward but it requires us to store an additional state vector (momentum v).

Exercises

1. Use other combinations of momentum hyperparameters and learning rates and observe and analyze the different experimental results.

2. Try out GD an∑d momentum for a quadratic problem where you have multiple eigenvalues,

i.e.,

f (x)

=

1 2

i λix2i , e.g., λi = 2−i. Plot how the values of x decrease for the initialization

xi = 1.

3.

Derive minimum value and minimizer for h(x) =

1 2

x⊤

Qx

+

x⊤

c

+

b.

4. What changes when we perform stochastic gradient descent with momentum? What happens when we use minibatch stochastic gradient descent with momentum? Experiment with the parameters?

Discussions140
139 https://distill.pub/2017/momentum/ 140 https://discuss.d2l.ai/t/354

486

Chapter 11. Optimization Algorithms

11.7 Adagrad
Let us begin by considering learning problems with features that occur infrequently.

11.7.1 Sparse Features and Learning Rates
Imagine that we are training a language model. To get good accuracy we typically want to decrease the learning rate as we keep on training, usually at a rate of O(t− 12 ) or slower. Now consider a model training on sparse features, i.e., features that occur only infrequently. This is common for natural language, e.g., it is a lot less likely that we will see the word preconditioning than learning. However, it is also common in other areas such as computational advertising and personalized collaborative filtering. After all, there are many things that are of interest only for a small number of people.
Parameters associated with infrequent features only receive meaningful updates whenever these features occur. Given a decreasing learning rate we might end up in a situation where the parameters for common features converge rather quickly to their optimal values, whereas for infrequent features we are still short of observing them sufficiently frequently before their optimal values can be determined. In other words, the learning rate either decreases too slowly for frequent features or too quickly for infrequent ones.
A possible hack to redress this issue would be to count the number of times we see a particular feature and to use this as a clock for adjusting learning rates. That is, rather than choosing a learning rate of the form η = √ηt+0 c we could use ηi = √s(ηi0,t)+c . Here s(i, t) counts the number of nonzeros for feature i that we have observed up to time t. This is actually quite easy to implement at no meaningful overhead. However, it fails whenever we do not quite have sparsity but rather just data where the gradients are often very small and only rarely large. After all, it is unclear where one would draw the line between something that qualifies as an observed feature or not.
Adagrad by (Duchi et al., 2011) addresses this by replacing the rather crude counter s(i, t) by an aggregate of the squares of previously observed gradients. In particular, it uses s(i, t + 1) = s(i, t) + (∂if (x))2 as a means to adjust the learning rate. This has two benefits: first, we no longer need to decide just when a gradient is large enough. Second, it scales automatically with the magnitude of the gradients. Coordinates that routinely correspond to large gradients are scaled down significantly, whereas others with small gradients receive a much more gentle treatment. In practice this leads to a very effective optimization procedure for computational advertising and related problems. But this hides some of the additional benefits inherent in Adagrad that are best understood in the context of preconditioning.

11.7.2 Preconditioning

Convex optimization problems are good for analyzing the characteristics of algorithms. After all, for most nonconvex problems it is difficult to derive meaningful theoretical guarantees, but intuition and insight often carry over. Let us look at the problem of minimizing f (x) = 12 x⊤Qx+c⊤x+b.
As we saw in Section 11.6, it is possible to rewrite this problem in terms of its eigendecomposition Q = U⊤ΛU to arrive at a much simplified problem where each coordinate can be solved individually:

f (x) = f¯(x¯) = 1 x¯⊤Λx¯ + c¯⊤x¯ + b. 2

(11.7.1)

11.7. Adagrad

487

Here we used x = Ux and consequently c = Uc. The modified problem has as its minimizer x¯ = −Λ−1c¯ and minimum value − 12 c¯⊤Λ−1c¯ + b. This is much easier to compute since Λ is a diagonal matrix containing the eigenvalues of Q.
If we perturb c slightly we would hope to find only slight changes in the minimizer of f . Unfortunately this is not the case. While slight changes in c lead to equally slight changes in c¯, this is not the case for the minimizer of f (and of f¯ respectively). Whenever the eigenvalues Λi are large we will see only small changes in x¯i and in the minimum of f¯. Conversely, for small Λi changes in x¯i can be dramatic. The ratio between the largest and the smallest eigenvalue is called the condition number of an optimization problem.

κ = Λ1 . Λd

(11.7.2)

If the condition number κ is large, it is difficult to solve the optimization problem accurately. We need to ensure that we are careful in getting a large dynamic range of values right. Our analysis leads to an obvious, albeit somewhat naive question: couldnʼt we simply “fix” the problem by distorting the space such that all eigenvalues are 1. In theory this is quite easy: we only need the
1
eigenvalues and eigenvectors of Q to rescale the problem from x to one in z := Λ 2 Ux. In the new coordinate system x⊤Qx could be simplified to ∥z∥2. Alas, this is a rather impractical suggestion. Computing eigenvalues and eigenvectors is in general much more expensive than solving the actual problem.
While computing eigenvalues exactly might be expensive, guessing them and computing them even somewhat approximately may already be a lot better than not doing anything at all. In particular, we could use the diagonal entries of Q and rescale it accordingly. This is much cheaper than computing eigenvalues.

Q˜

=

diag−

1 2

(Q)Qdiag−

1 2

(Q).

(11.7.3)

In this case we have Q˜ ij

=

√ Qij / QiiQjj

and

specifically

Q˜ ii

=

1 for all i.

In most cases this

simplifies the condition number considerably. For instance, the cases we discussed previously,

this would entirely eliminate the problem at hand since the problem is axis aligned.

Unfortunately we face yet another problem: in deep learning we typically do not even have access to the second derivative of the objective function: for x ∈ Rd the second derivative even on a minibatch may require O(d2) space and work to compute, thus making it practically infeasible. The ingenious idea of Adagrad is to use a proxy for that elusive diagonal of the Hessian that is both relatively cheap to compute and effective—the magnitude of the gradient itself.
In order to see why this works, let us look at f¯(x¯). We have that

∂x¯f¯(x¯) = Λx¯ + c¯ = Λ (x¯ − x¯0) ,

(11.7.4)

where x¯0 is the minimizer of f¯. Hence the magnitude of the gradient depends both on Λ and the distance from optimality. If x¯ − x¯0 didnʼt change, this would be all thatʼs needed. After all, in this case the magnitude of the gradient ∂x¯f¯(x¯) suffices. Since AdaGrad is a stochastic gradient descent algorithm, we will see gradients with nonzero variance even at optimality. As a result we can safely
use the variance of the gradients as a cheap proxy for the scale of the Hessian. A thorough analysis
is beyond the scope of this section (it would be several pages). We refer the reader to (Duchi et al.,
2011) for details.

488

Chapter 11. Optimization Algorithms

11.7.3 The Algorithm

Let us formalize the discussion from above. We use the variable st to accumulate past gradient variance as follows.

gt = ∂wl(yt, f (xt, w)),
st = st−1 + g2t , η
wt = wt−1 − √st + ϵ · gt.

(11.7.5)

Here

the

operation

are

applied

coordinate

wise.

That

is,

v2

has

entries

vi2.

Likewise

√1 v

has

entries

√1vi and u · v has entries uivi. As before η is the learning rate and ϵ is an additive constant that

ensures that we do not divide by 0. Last, we initialize s0 = 0.

Just like in the case of momentum we need to keep track of an auxiliary variable, in this case to allow for an individual learning rate per coordinate. This does not increase the cost of Adagrad significantly relative to SGD, simply since the main cost is typically to compute l(yt, f (xt, w)) and its derivative.

Note that accumulating squared gradients in st means that st grows essentially at linear rate (some-

what slower than linearly in practice, since the gradients initially diminish). This leads to an

O

(t

−

1 2

)

learning

rate,

albeit

adjusted

on

a

per

coordinate

basis.

For

convex

problems

this

is

per-

fectly adequate. In deep learning, though, we might want to decrease the learning rate rather

more slowly. This led to a number of Adagrad variants that we will discuss in the subsequent

chapters. For now let us see how it behaves in a quadratic convex problem. We use the same

problem as before:

f (x) = 0.1x21 + 2x22.

(11.7.6)

We are going to implement Adagrad using the same learning rate previously, i.e., η = 0.4. As we can see, the iterative trajectory of the independent variable is smoother. However, due to the cumulative effect of st, the learning rate continuously decays, so the independent variable does not move as much during later stages of iteration.

%matplotlib inline import math from mxnet import np, npx from d2l import mxnet as d2l

npx.set_np()

def adagrad_2d(x1, x2, s1, s2): eps = 1e-6 g1, g2 = 0.2 * x1, 4 * x2 s1 += g1**2 s2 += g2**2 x1 -= eta / math.sqrt(s1 + eps) * g1 x2 -= eta / math.sqrt(s2 + eps) * g2 return x1, x2, s1, s2
def f_2d(x1, x2): return 0.1 * x1**2 + 2 * x2**2
11.7. Adagrad

(continues on next page)
489

eta = 0.4 d2l.show_trace_2d(f_2d, d2l.train_2d(adagrad_2d))
epoch 20, x1: -2.382563, x2: -0.158591

(continued from previous page)

As we increase the learning rate to 2 we see much better behavior. This already indicates that the decrease in learning rate might be rather aggressive, even in the noise-free case and we need to ensure that parameters converge appropriately.
eta = 2 d2l.show_trace_2d(f_2d, d2l.train_2d(adagrad_2d))
epoch 20, x1: -0.002295, x2: -0.000000

490

Chapter 11. Optimization Algorithms

11.7.4 Implementation from Scratch
Just like the momentum method, Adagrad needs to maintain a state variable of the same shape as the parameters.
def init_adagrad_states(feature_dim): s_w = np.zeros((feature_dim, 1)) s_b = np.zeros(1) return (s_w, s_b)
def adagrad(params, states, hyperparams): eps = 1e-6 for p, s in zip(params, states): s[:] += np.square(p.grad) p[:] -= hyperparams['lr'] * p.grad / np.sqrt(s + eps)
Compared to the experiment in Section 11.5 we use a larger learning rate to train the model.
data_iter, feature_dim = d2l.get_data_ch11(batch_size=10) d2l.train_ch11(adagrad, init_adagrad_states(feature_dim), {'lr': 0.1},
data_iter, feature_dim);
loss: 0.242, 0.602 sec/epoch

11.7.5 Concise Implementation
Using the Trainer instance of the algorithm adagrad, we can invoke the Adagrad algorithm in Gluon. d2l.train_concise_ch11('adagrad', {'learning_rate': 0.1}, data_iter)
loss: 0.244, 0.569 sec/epoch

11.7. Adagrad

491

Summary
• Adagrad decreases the learning rate dynamically on a per-coordinate basis.
• It uses the magnitude of the gradient as a means of adjusting how quickly progress is achieved - coordinates with large gradients are compensated with a smaller learning rate.
• Computing the exact second derivative is typically infeasible in deep learning problems due to memory and computational constraints. The gradient can be a useful proxy.
• If the optimization problem has a rather uneven structure Adagrad can help mitigate the distortion.
• Adagrad is particularly effective for sparse features where the learning rate needs to decrease more slowly for infrequently occurring terms.
• On deep learning problems Adagrad can sometimes be too aggressive in reducing learning rates. We will discuss strategies for mitigating this in the context of Section 11.10.

Exercises

1. Prove that for an orthogonal matrix U and a vector c the following holds: ∥c − δ∥2 = ∥Uc − Uδ∥2. Why does this mean that the magnitude of perturbations does not change after an orthogonal change of variables?

2. Try out Adagrad for f (x) = 0.1x21 + 2x22 and also for the objective function was rotated by 45 degrees, i.e., f (x) = 0.1(x1 + x2)2 + 2(x1 − x2)2. Does it behave differently?

3. Prove Gersch∑gorinʼs circle theorem141 which states that eigenvalues λi of a matrix M satisfy |λi − Mjj| ≤ k̸=j |Mjk| for at least one choice of j.

4. What does Gerschgorinʼs theorem tell us about the eigenvalues of the diagonally precondi-

tioned

matrix

diag−

1 2

(M)Mdiag−

1 2

(M)?

5. Try out Adagrad for a proper deep network, such as Section 6.6 when applied to Fashion MNIST.

6. How would you need to modify Adagrad to achieve a less aggressive decay in learning rate?

141 https://en.wikipedia.org/wiki/Gershgorin_circle_theorem

492

Chapter 11. Optimization Algorithms

Discussions142

11.8 RMSProp
One of the key issues in Section 11.7 is that the learning rate decreases at a predefined schedule of effectively O(t− 12 ). While this is generally appropriate for convex problems, it might not be ideal for nonconvex ones, such as those encountered in deep learning. Yet, the coordinate-wise adaptivity of Adagrad is highly desirable as a preconditioner.
(Tieleman & Hinton, 2012) proposed the RMSProp algorithm as a simple fix to decouple rate scheduling from coordinate-adaptive learning rates. The issue is that Adagrad accumulates the squares of the gradient gt into a state vector st = st−1 + g2t . As a result st keeps on growing without bound due to the lack of normalization, essentially linearly as the algorithm converges.
One way of fixing this problem would be to use st/t. For reasonable distributions of gt this will converge. Unfortunately it might take a very long time until the limit behavior starts to matter since the procedure remembers the full trajectory of values. An alternative is to use a leaky average in the same way we used in the momentum method, i.e., st ← γst−1 + (1 − γ)g2t for some parameter γ > 0. Keeping all other parts unchanged yields RMSProp.

11.8.1 The Algorithm

Let us write out the equations in detail.

st ← γst−1 + (1 − γ)g2t , η
xt ← xt−1 − √st + ϵ ⊙ gt.

(11.8.1)

The constant ϵ > 0 is typically set to 10−6 to ensure that we do not suffer from division by zero or overly large step sizes. Given this expansion we are now free to control the learning rate η independently of the scaling that is applied on a per-coordinate basis. In terms of leaky averages we can apply the same reasoning as previously applied in the case of the momentum method. Expanding the definition of st yields

st = (1 − γ)g2t + γst−1 = (1 − γ) (g2t + γg2t−1 + γ2gt−2 + . . . , ) .

(11.8.2)

As

before

in

Section

11.6

we

use

1

+

γ

+

γ2

+

...,=

1 1−

γ

.

Hence

the

sum

of

weights

is

normalized

to 1 with a half-life time of an observation of γ−1. Let us visualize the weights for the past 40 time

steps for various choices of γ.

%matplotlib inline import math from mxnet import np, npx from d2l import mxnet as d2l

npx.set_np()

142 https://discuss.d2l.ai/t/355

11.8. RMSProp

493

d2l.set_figsize() gammas = [0.95, 0.9, 0.8, 0.7] for gamma in gammas:
x = np.arange(40).asnumpy() d2l.plt.plot(x, (1 - gamma) * gamma**x, label=f'gamma = {gamma:.2f}') d2l.plt.xlabel('time');

11.8.2 Implementation from Scratch
As before we use the quadratic function f (x) = 0.1x21 + 2x22 to observe the trajectory of RMSProp. Recall that in Section 11.7, when we used Adagrad with a learning rate of 0.4, the variables moved only very slowly in the later stages of the algorithm since the learning rate decreased too quickly. Since η is controlled separately this does not happen with RMSProp.
def rmsprop_2d(x1, x2, s1, s2): g1, g2, eps = 0.2 * x1, 4 * x2, 1e-6 s1 = gamma * s1 + (1 - gamma) * g1**2 s2 = gamma * s2 + (1 - gamma) * g2**2 x1 -= eta / math.sqrt(s1 + eps) * g1 x2 -= eta / math.sqrt(s2 + eps) * g2 return x1, x2, s1, s2
def f_2d(x1, x2): return 0.1 * x1**2 + 2 * x2**2
eta, gamma = 0.4, 0.9 d2l.show_trace_2d(f_2d, d2l.train_2d(rmsprop_2d))
epoch 20, x1: -0.010599, x2: 0.000000

494

Chapter 11. Optimization Algorithms

Next, we implement RMSProp to be used in a deep network. This is equally straightforward.
def init_rmsprop_states(feature_dim): s_w = np.zeros((feature_dim, 1)) s_b = np.zeros(1) return (s_w, s_b)
def rmsprop(params, states, hyperparams): gamma, eps = hyperparams['gamma'], 1e-6 for p, s in zip(params, states): s[:] = gamma * s + (1 - gamma) * np.square(p.grad) p[:] -= hyperparams['lr'] * p.grad / np.sqrt(s + eps)
We set the initial learning rate to 0.01 and the weighting term γ to 0.9. That is, s aggregates on average over the past 1/(1 − γ) = 10 observations of the square gradient.
data_iter, feature_dim = d2l.get_data_ch11(batch_size=10) d2l.train_ch11(rmsprop, init_rmsprop_states(feature_dim), {
'lr': 0.01, 'gamma': 0.9}, data_iter, feature_dim);
loss: 0.243, 0.246 sec/epoch

11.8. RMSProp

495

11.8.3 Concise Implementation
Since RMSProp is a rather popular algorithm it is also available in the Trainer instance. All we need to do is instantiate it using an algorithm named rmsprop, assigning γ to the parameter gamma1.
d2l.train_concise_ch11('rmsprop', { 'learning_rate': 0.01, 'gamma1': 0.9}, data_iter)
loss: 0.245, 0.204 sec/epoch

Summary
• RMSProp is very similar to Adagrad insofar as both use the square of the gradient to scale coefficients.
• RMSProp shares with momentum the leaky averaging. However, RMSProp uses the technique to adjust the coefficient-wise preconditioner.
• The learning rate needs to be scheduled by the experimenter in practice. • The coefficient γ determines how long the history is when adjusting the per-coordinate
scale.
Exercises
1. What happens experimentally if we set γ = 1? Why? 2. Rotate the optimization problem to minimize f (x) = 0.1(x1 + x2)2 + 2(x1 − x2)2. What
happens to the convergence? 3. Try out what happens to RMSProp on a real machine learning problem, such as training on
Fashion-MNIST. Experiment with different choices for adjusting the learning rate. 4. Would you want to adjust γ as optimization progresses? How sensitive is RMSProp to this?

496

Chapter 11. Optimization Algorithms

Discussions143

11.9 Adadelta
Adadelta is yet another variant of AdaGrad (Section 11.7). The main difference lies in the fact that it decreases the amount by which the learning rate is adaptive to coordinates. Moreover, traditionally it referred to as not having a learning rate since it uses the amount of change itself as calibration for future change. The algorithm was proposed in (Zeiler, 2012). It is fairly straightforward, given the discussion of previous algorithms so far.

11.9.1 The Algorithm

In a nutshell, Adadelta uses two state variables, st to store a leaky average of the second moment of the gradient and ∆xt to store a leaky average of the second moment of the change of parameters in the model itself. Note that we use the original notation and naming of the authors for compatibility with other publications and implementations (there is no other real reason why one should use different Greek variables to indicate a parameter serving the same purpose in momentum, Adagrad, RMSProp, and Adadelta).
Here are the technical details of Adadelta. Given the parameter du jour is ρ, we obtain the following leaky updates similarly to Section 11.8:

st = ρst−1 + (1 − ρ)g2t .

(11.9.1)

The difference to Section 11.8 is that we perform updates with the rescaled gradient g′t, i.e.,

xt = xt−1 − g′t.

(11.9.2)

So what is the rescaled gradient g′t? We can calculate it as follows:

√

g′ = ∆√xt−1 + ϵ ⊙ gt,

t

st + ϵ

(11.9.3)

where ∆xt−1 is the leaky average of the squared rescaled gradients g′t. We initialize ∆x0 to be 0 and update it at each step with g′t, i.e.,

∆xt = ρ∆xt−1 + (1 − ρ)g′t2,

(11.9.4)

and ϵ (a small value such as 10−5) is added to maintain numerical stability.

11.9.2 Implementation
Adadelta needs to maintain two state variables for each variable, st and ∆xt. This yields the following implementation.
143 https://discuss.d2l.ai/t/356

11.9. Adadelta

497

%matplotlib inline from mxnet import np, npx from d2l import mxnet as d2l
npx.set_np()
def init_adadelta_states(feature_dim): s_w, s_b = np.zeros((feature_dim, 1)), np.zeros(1) delta_w, delta_b = np.zeros((feature_dim, 1)), np.zeros(1) return ((s_w, delta_w), (s_b, delta_b))
def adadelta(params, states, hyperparams): rho, eps = hyperparams['rho'], 1e-5 for p, (s, delta) in zip(params, states): # In-place updates via [:] s[:] = rho * s + (1 - rho) * np.square(p.grad) g = (np.sqrt(delta + eps) / np.sqrt(s + eps)) * p.grad p[:] -= g delta[:] = rho * delta + (1 - rho) * g * g
Choosing ρ = 0.9 amounts to a half-life time of 10 for each parameter update. This tends to work quite well. We get the following behavior.
data_iter, feature_dim = d2l.get_data_ch11(batch_size=10) d2l.train_ch11(adadelta, init_adadelta_states(feature_dim), {'rho': 0.9},
data_iter, feature_dim);
loss: 0.247, 0.201 sec/epoch

For a concise implementation we simply use the adadelta algorithm from the Trainer class. This yields the following one-liner for a much more compact invocation. d2l.train_concise_ch11('adadelta', {'rho': 0.9}, data_iter)
loss: 0.243, 0.219 sec/epoch

498

Chapter 11. Optimization Algorithms

Summary
• Adadelta has no learning rate parameter. Instead, it uses the rate of change in the parameters itself to adapt the learning rate.
• Adadelta requires two state variables to store the second moments of gradient and the change in parameters.
• Adadelta uses leaky averages to keep a running estimate of the appropriate statistics.
Exercises
1. Adjust the value of ρ. What happens? 2. Show how to implement the algorithm without the use of g′t. Why might this be a good idea? 3. Is Adadelta really learning rate free? Could you find optimization problems that break
Adadelta? 4. Compare Adadelta to Adagrad and RMS prop to discuss their convergence behavior. Discussions144
11.10 Adam
In the discussions leading up to this section we encountered a number of techniques for efficient optimization. Let us recap them in detail here:
• We saw that Section 11.4 is more effective than Gradient Descent when solving optimization problems, e.g., due to its inherent resilience to redundant data.
• We saw that Section 11.5 affords significant additional efficiency arising from vectorization, using larger sets of observations in one minibatch. This is the key to efficient multi-machine, multi-GPU and overall parallel processing.
144 https://discuss.d2l.ai/t/357

11.10. Adam

499

• Section 11.6 added a mechanism for aggregating a history of past gradients to accelerate convergence.
• Section 11.7 used per-coordinate scaling to allow for a computationally efficient preconditioner.
• Section 11.8 decoupled per-coordinate scaling from a learning rate adjustment.
Adam (Kingma & Ba, 2014) combines all these techniques into one efficient learning algorithm. As expected, this is an algorithm that has become rather popular as one of the more robust and effective optimization algorithms to use in deep learning. It is not without issues, though. In particular, (Reddi et al., 2019) show that there are situations where Adam can diverge due to poor variance control. In a follow-up work (Zaheer et al., 2018) proposed a hotfix to Adam, called Yogi which addresses these issues. More on this later. For now let us review the Adam algorithm.

11.10.1 The Algorithm

One of the key components of Adam is that it uses exponential weighted moving averages (also known as leaky averaging) to obtain an estimate of both the momentum and also the second moment of the gradient. That is, it uses the state variables

vt ← β1vt−1 + (1 − β1)gt, st ← β2st−1 + (1 − β2)g2t .

(11.10.1)

Here β1 and β2 are nonnegative weighting parameters. Common choices for them are β1 = 0.9

and β2 = 0.999. That is, the variance estimate moves much more slowly than the momentum term.

Note

that

if

we

initialize

v0

=

s0

=

0

we

have

a

signific∑ant

amount

of bias
t

initially

towards

smaller

values. This can be addressed by using the fact that

t i=0

βi

=

1−β 1−β

to re-normalize terms.

Cor-

respondingly the normalized state variables are given by

vˆt = vt and ˆst = st .

1 − β1t

1 − β2t

(11.10.2)

Armed with the proper estimates we can now write out the update equations. First, we rescale the gradient in a manner very much akin to that of RMSProp to obtain

g′ = √ηvˆt .

t

ˆst + ϵ

(11.10.3)

Unlike RMSProp our update uses the momentum vˆt rather than the gradient itself. Moreover, there is a slight cosmetic difference as the rescaling happens using √ˆs1t+ϵ instead of √ˆs1t+ϵ . The former works arguably slightly better in practice, hence the deviation from RMSProp. Typically
we pick ϵ = 10−6 for a good trade-off between numerical stability and fidelity.

Now we have all the pieces in place to compute updates. This is slightly anticlimactic and we have a simple update of the form

xt ← xt−1 − g′t.

(11.10.4)

Reviewing the design of Adam its inspiration is clear. Momentum and scale are clearly visible in the state variables. Their rather peculiar definition forces us to debias terms (this could be fixed by a slightly different initialization and update condition). Second, the combination of both terms is pretty straightforward, given RMSProp. Last, the explicit learning rate η allows us to control the step length to address issues of convergence.

500

Chapter 11. Optimization Algorithms

11.10.2 Implementation
Implementing Adam from scratch is not very daunting. For convenience we store the time step counter t in the hyperparams dictionary. Beyond that all is straightforward.
%matplotlib inline from mxnet import np, npx from d2l import mxnet as d2l
npx.set_np()
def init_adam_states(feature_dim): v_w, v_b = np.zeros((feature_dim, 1)), np.zeros(1) s_w, s_b = np.zeros((feature_dim, 1)), np.zeros(1) return ((v_w, s_w), (v_b, s_b))
def adam(params, states, hyperparams): beta1, beta2, eps = 0.9, 0.999, 1e-6 for p, (v, s) in zip(params, states): v[:] = beta1 * v + (1 - beta1) * p.grad s[:] = beta2 * s + (1 - beta2) * np.square(p.grad) v_bias_corr = v / (1 - beta1 ** hyperparams['t']) s_bias_corr = s / (1 - beta2 ** hyperparams['t']) p[:] -= hyperparams['lr'] * v_bias_corr / (np.sqrt(s_bias_corr) + eps) hyperparams['t'] += 1
We are ready to use Adam to train the model. We use a learning rate of η = 0.01.
data_iter, feature_dim = d2l.get_data_ch11(batch_size=10) d2l.train_ch11(adam, init_adam_states(feature_dim), {
'lr': 0.01, 't': 1}, data_iter, feature_dim);
loss: 0.244, 0.200 sec/epoch

A more concise implementation is straightforward since adam is one of the algorithms provided as part of the Gluon trainer optimization library. Hence we only need to pass configuration parameters for an implementation in Gluon.

11.10. Adam

501

d2l.train_concise_ch11('adam', {'learning_rate': 0.01}, data_iter) loss: 0.243, 0.090 sec/epoch

11.10.3 Yogi

One of the problems of Adam is that it can fail to converge even in convex settings when the second

moment estimate in st blows up. As a fix (Zaheer et al., 2018) proposed a refined update (and

initialization) for st. To understand whatʼs going on, let us rewrite the Adam update as follows:

st ← st−1 + (1 − β2) (g2t − st−1) .

(11.10.5)

Whenever g2t has high variance or updates are sparse, st might forget past values too quickly. A possible fix for this is to replace g2t − st−1 by g2t ⊙ sgn(g2t − st−1). Now the magnitude of the update no longer depends on the amount of deviation. This yields the Yogi updates

st ← st−1 + (1 − β2)g2t ⊙ sgn(g2t − st−1).

(11.10.6)

The authors furthermore advise to initialize the momentum on a larger initial batch rather than just initial pointwise estimate. We omit the details since they are not material to the discussion and since even without this convergence remains pretty good.

def yogi(params, states, hyperparams): beta1, beta2, eps = 0.9, 0.999, 1e-3 for p, (v, s) in zip(params, states): v[:] = beta1 * v + (1 - beta1) * p.grad s[:] = s + (1 - beta2) * np.sign(np.square(p.grad) - s) * np.square( p.grad) v_bias_corr = v / (1 - beta1**hyperparams['t']) s_bias_corr = s / (1 - beta2**hyperparams['t']) p[:] -= hyperparams['lr'] * v_bias_corr / (np.sqrt(s_bias_corr) + eps) hyperparams['t'] += 1

data_iter, feature_dim = d2l.get_data_ch11(batch_size=10) d2l.train_ch11(yogi, init_adam_states(feature_dim), {
'lr': 0.01, 't': 1}, data_iter, feature_dim);

502

Chapter 11. Optimization Algorithms

loss: 0.243, 0.162 sec/epoch

Summary
• Adam combines features of many optimization algorithms into a fairly robust update rule. • Created on the basis of RMSProp, Adam also uses EWMA on the minibatch stochastic gradi-
ent. • Adam uses bias correction to adjust for a slow startup when estimating momentum and a
second moment. • For gradients with significant variance we may encounter issues with convergence. They
can be amended by using larger minibatches or by switching to an improved estimate for st. Yogi offers such an alternative.
Exercises
1. Adjust the learning rate and observe and analyze the experimental results. 2. Can you rewrite momentum and second moment updates such that it does not require bias
correction? 3. Why do you need to reduce the learning rate η as we converge? 4. Try to construct a case for which Adam diverges and Yogi converges? Discussions145
145 https://discuss.d2l.ai/t/358

11.10. Adam

503

11.11 Learning Rate Scheduling

So far we primarily focused on optimization algorithms for how to update the weight vectors rather than on the rate at which they are being updated. Nonetheless, adjusting the learning rate is often just as important as the actual algorithm. There are a number of aspects to consider:

• Most obviously the magnitude of the learning rate matters. If it is too large, optimization diverges, if it is too small, it takes too long to train or we end up with a suboptimal result. We saw previously that the condition number of the problem matters (see e.g., Section 11.6 for details). Intuitively it is the ratio of the amount of change in the least sensitive direction vs. the most sensitive one.

• Secondly, the rate of decay is just as important. If the learning rate remains large we may

simply end up bouncing around the minimum and thus not reach optimality. Section 11.5

discussed this in some detail and we analyzed performance guarantees in Section 11.4. In

short,

we

want

the

rate

to

decay,

but

probably

more

slowly

than

O(t−

1 2

)

which

would

be

a

good choice for convex problems.

• Another aspect that is equally important is initialization. This pertains both to how the parameters are set initially (review Section 4.8 for details) and also how they evolve initially. This goes under the moniker of warmup, i.e., how rapidly we start moving towards the solution initially. Large steps in the beginning might not be beneficial, in particular since the initial set of parameters is random. The initial update directions might be quite meaningless, too.

• Lastly, there are a number of optimization variants that perform cyclical learning rate adjustment. This is beyond the scope of the current chapter. We recommend the reader to review details in (Izmailov et al., 2018), e.g., how to obtain better solutions by averaging over an entire path of parameters.

Given the fact that there is a lot of detail needed to manage learning rates, most deep learning frameworks have tools to deal with this automatically. In the current chapter we will review the effects that different schedules have on accuracy and also show how this can be managed efficiently via a learning rate scheduler.

11.11.1 Toy Problem

We begin with a toy problem that is cheap enough to compute easily, yet sufficiently nontrivial to illustrate some of the key aspects. For that we pick a slightly modernized version of LeNet (relu instead of sigmoid activation, MaxPooling rather than AveragePooling), as applied to FashionMNIST. Moreover, we hybridize the network for performance. Since most of the code is standard we just introduce the basics without further detailed discussion. See Chapter 6 for a refresher as needed.

%matplotlib inline from mxnet import autograd, gluon, init, lr_scheduler, np, npx from mxnet.gluon import nn from d2l import mxnet as d2l

npx.set_np()

net = nn.HybridSequential()

(continues on next page)

504

Chapter 11. Optimization Algorithms

(continued from previous page)
net.add(nn.Conv2D(channels=6, kernel_size=5, padding=2, activation='relu'), nn.MaxPool2D(pool_size=2, strides=2), nn.Conv2D(channels=16, kernel_size=5, activation='relu'), nn.MaxPool2D(pool_size=2, strides=2), nn.Dense(120, activation='relu'), nn.Dense(84, activation='relu'), nn.Dense(10))
net.hybridize() loss = gluon.loss.SoftmaxCrossEntropyLoss() device = d2l.try_gpu()
batch_size = 256 train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size=batch_size)
# The code is almost identical to `d2l.train_ch6` defined in the # lenet section of chapter convolutional neural networks def train(net, train_iter, test_iter, num_epochs, loss, trainer, device):
net.initialize(force_reinit=True, ctx=device, init=init.Xavier()) animator = d2l.Animator(xlabel='epoch', xlim=[0, num_epochs],
legend=['train loss', 'train acc', 'test acc']) for epoch in range(num_epochs):
metric = d2l.Accumulator(3) # train_loss, train_acc, num_examples for i, (X, y) in enumerate(train_iter):
X, y = X.as_in_ctx(device), y.as_in_ctx(device) with autograd.record():
y_hat = net(X) l = loss(y_hat, y) l.backward() trainer.step(X.shape[0]) metric.add(l.sum(), d2l.accuracy(y_hat, y), X.shape[0]) train_loss = metric[0] / metric[2] train_acc = metric[1] / metric[2] if (i + 1) % 50 == 0: animator.add(epoch + i / len(train_iter),
(train_loss, train_acc, None)) test_acc = d2l.evaluate_accuracy_gpu(net, test_iter) animator.add(epoch + 1, (None, None, test_acc)) print(f'train loss {train_loss:.3f}, train acc {train_acc:.3f}, '
f'test acc {test_acc:.3f}')
Let us have a look at what happens if we invoke this algorithm with default settings, such as a learning rate of 0.3 and train for 30 iterations. Note how the training accuracy keeps on increasing while progress in terms of test accuracy stalls beyond a point. The gap between both curves indicates overfitting.
lr, num_epochs = 0.3, 30 net.initialize(force_reinit=True, ctx=device, init=init.Xavier()) trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': lr}) train(net, train_iter, test_iter, num_epochs, loss, trainer, device)
train loss 0.165, train acc 0.937, test acc 0.888

11.11. Learning Rate Scheduling

505

11.11.2 Schedulers
One way of adjusting the learning rate is to set it explicitly at each step. This is conveniently achieved by the set_learning_rate method. We could adjust it downward after every epoch (or even after every minibatch), e.g., in a dynamic manner in response to how optimization is progressing.
trainer.set_learning_rate(0.1) print(f'learning rate is now {trainer.learning_rate:.2f}')

learning rate is now 0.10

More generally we want to define a scheduler. When invoked with the number of updates it returns

the appropriate value of the learning rate. Let us define a simple one that sets the learning rate to

η

=

η0(t

+

1)−

1 2

.

class SquareRootScheduler: def __init__(self, lr=0.1): self.lr = lr

def __call__(self, num_update): return self.lr * pow(num_update + 1.0, -0.5)

Let us plot its behavior over a range of values.
scheduler = SquareRootScheduler(lr=0.1) d2l.plot(np.arange(num_epochs), [scheduler(t) for t in range(num_epochs)])

506

Chapter 11. Optimization Algorithms

Now let us see how this plays out for training on Fashion-MNIST. We simply provide the scheduler as an additional argument to the training algorithm.
trainer = gluon.Trainer(net.collect_params(), 'sgd', {'lr_scheduler': scheduler})
train(net, train_iter, test_iter, num_epochs, loss, trainer, device)
train loss 0.522, train acc 0.810, test acc 0.811

This worked quite a bit better than previously. Two things stand out: the curve was rather more smooth than previously. Secondly, there was less overfitting. Unfortunately it is not a wellresolved question as to why certain strategies lead to less overfitting in theory. There is some argument that a smaller stepsize will lead to parameters that are closer to zero and thus simpler. However, this does not explain the phenomenon entirely since we do not really stop early but simply reduce the learning rate gently.

11.11. Learning Rate Scheduling

507

11.11.3 Policies
While we cannot possibly cover the entire variety of learning rate schedulers, we attempt to give a brief overview of popular policies below. Common choices are polynomial decay and piecewise constant schedules. Beyond that, cosine learning rate schedules have been found to work well empirically on some problems. Lastly, on some problems it is beneficial to warm up the optimizer prior to using large learning rates.
Factor Scheduler
One alternative to a polynomial decay would be a multiplicative one, that is ηt+1 ← ηt · α for α ∈ (0, 1). To prevent the learning rate from decaying beyond a reasonable lower bound the update equation is often modified to ηt+1 ← max(ηmin, ηt · α).
class FactorScheduler: def __init__(self, factor=1, stop_factor_lr=1e-7, base_lr=0.1): self.factor = factor self.stop_factor_lr = stop_factor_lr self.base_lr = base_lr
def __call__(self, num_update): self.base_lr = max(self.stop_factor_lr, self.base_lr * self.factor) return self.base_lr
scheduler = FactorScheduler(factor=0.9, stop_factor_lr=1e-2, base_lr=2.0) d2l.plot(np.arange(50), [scheduler(t) for t in range(50)])

This can also be accomplished by a built-in scheduler in MXNet via the lr_scheduler. FactorScheduler object. It takes a few more parameters, such as warmup period, warmup mode (linear or constant), the maximum number of desired updates, etc.; Going forward we will use the built-in schedulers as appropriate and only explain their functionality here. As illustrated, it is fairly straightforward to build your own scheduler if needed.

508

Chapter 11. Optimization Algorithms

Multi Factor Scheduler A common strategy for training deep networks is to keep the learning rate piecewise constant and to decrease it by a given amount every so often. That is, given a set of times when to decrease the rate, such as s = {5, 10, 20} decrease ηt+1 ← ηt · α whenever t ∈ s. Assuming that the values are halved at each step we can implement this as follows. scheduler = lr_scheduler.MultiFactorScheduler(step=[15, 30], factor=0.5,
base_lr=0.5) d2l.plot(np.arange(num_epochs), [scheduler(t) for t in range(num_epochs)])
The intuition behind this piecewise constant learning rate schedule is that one lets optimization proceed until a stationary point has been reached in terms of the distribution of weight vectors. Then (and only then) do we decrease the rate such as to obtain a higher quality proxy to a good local minimum. The example below shows how this can produce ever slightly better solutions. trainer = gluon.Trainer(net.collect_params(), 'sgd',
{'lr_scheduler': scheduler}) train(net, train_iter, test_iter, num_epochs, loss, trainer, device) train loss 0.205, train acc 0.923, test acc 0.891

11.11. Learning Rate Scheduling

509

Cosine Scheduler

A rather perplexing heuristic was proposed by (Loshchilov & Hutter, 2016). It relies on the observation that we might not want to decrease the learning rate too drastically in the beginning and moreover, that we might want to “refine” the solution in the end using a very small learning rate. This results in a cosine-like schedule with the following functional form for learning rates in the range t ∈ [0, T ].

ηt = ηT + η0 − ηT (1 + cos(πt/T )) 2

(11.11.1)

Here η0 is the initial learning rate, ηT is the target rate at time T . Furthermore, for t > T we simply pin the value to ηT without increasing it again. In the following example, we set the max update step T = 20.

scheduler = lr_scheduler.CosineScheduler(max_update=20, base_lr=0.3, final_lr=0.01)
d2l.plot(np.arange(num_epochs), [scheduler(t) for t in range(num_epochs)])

In the context of computer vision this schedule can lead to improved results. Note, though, that such improvements are not guaranteed (as can be seen below).
trainer = gluon.Trainer(net.collect_params(), 'sgd', {'lr_scheduler': scheduler})
train(net, train_iter, test_iter, num_epochs, loss, trainer, device)
train loss 0.343, train acc 0.878, test acc 0.869

510

Chapter 11. Optimization Algorithms

Warmup
In some cases initializing the parameters is not sufficient to guarantee a good solution. This particularly a problem for some advanced network designs that may lead to unstable optimization problems. We could address this by choosing a sufficiently small learning rate to prevent divergence in the beginning. Unfortunately this means that progress is slow. Conversely, a large learning rate initially leads to divergence.
A rather simple fix for this dilemma is to use a warmup period during which the learning rate increases to its initial maximum and to cool down the rate until the end of the optimization process. For simplicity one typically uses a linear increase for this purpose. This leads to a schedule of the form indicated below.
scheduler = lr_scheduler.CosineScheduler(20, warmup_steps=5, base_lr=0.3, final_lr=0.01)
d2l.plot(np.arange(num_epochs), [scheduler(t) for t in range(num_epochs)])

Note that the network converges better initially (in particular observe the performance during the first 5 epochs).

11.11. Learning Rate Scheduling

511

trainer = gluon.Trainer(net.collect_params(), 'sgd', {'lr_scheduler': scheduler})
train(net, train_iter, test_iter, num_epochs, loss, trainer, device)
train loss 0.351, train acc 0.873, test acc 0.874

Warmup can be applied to any scheduler (not just cosine). For a more detailed discussion of learning rate schedules and many more experiments see also (Gotmare et al., 2018). In particular they find that a warmup phase limits the amount of divergence of parameters in very deep networks. This makes intuitively sense since we would expect significant divergence due to random initialization in those parts of the network that take the most time to make progress in the beginning.
Summary
• Decreasing the learning rate during training can lead to improved accuracy and (most perplexingly) reduced overfitting of the model.
• A piecewise decrease of the learning rate whenever progress has plateaued is effective in practice. Essentially this ensures that we converge efficiently to a suitable solution and only then reduce the inherent variance of the parameters by reducing the learning rate.
• Cosine schedulers are popular for some computer vision problems. See e.g., GluonCV146 for details of such a scheduler.
• A warmup period before optimization can prevent divergence.
• Optimization serves multiple purposes in deep learning. Besides minimizing the training objective, different choices of optimization algorithms and learning rate scheduling can lead to rather different amounts of generalization and overfitting on the test set (for the same amount of training error).
146 http://gluon-cv.mxnet.io

512

Chapter 11. Optimization Algorithms

Exercises
1. Experiment with the optimization behavior for a given fixed learning rate. What is the best model you can obtain this way?
2. How does convergence change if you change the exponent of the decrease in the learning rate? Use PolyScheduler for your convenience in the experiments.
3. Apply the cosine scheduler to large computer vision problems, e.g., training ImageNet. How does it affect performance relative to other schedulers?
4. How long should warmup last?
5. Can you connect optimization and sampling? Start by using results from (Welling & Teh, 2011) on Stochastic Gradient Langevin Dynamics.
Discussions147

147 https://discuss.d2l.ai/t/359

11.11. Learning Rate Scheduling

513

514

Chapter 11. Optimization Algorithms

12 | Computational Performance
In deep learning, datasets and models are usually large, which involves heavy computation. Therefore, computational performance matters a lot. This chapter will focus on the major factors that affect computational performance: imperative programming, symbolic programming, asynchronous computing, automatic parallelism, and multi-GPU computation. By studying this chapter, you may further improve computational performance of those models implemented in the previous chapters, for example, by reducing training time without affecting accuracy.
12.1 Compilers and Interpreters
So far, this book has focused on imperative programming, which makes use of statements such as print, +, and if to change a programʼs state. Consider the following example of a simple imperative program. def add(a, b):
return a + b def fancy_func(a, b, c, d):
e = add(a, b) f = add(c, d) g = add(e, f) return g print(fancy_func(1, 2, 3, 4))
10
Python is an interpreted language. When evaluating the above fancy_func function it performs the operations making up the functionʼs body in sequence. That is, it will evaluate e = add(a, b) and store the results as variable e, thereby changing the programʼs state. The next two statements f = add(c, d) and g = add(e, f) will be executed similarly, performing additions and storing the results as variables. Fig. 12.1.1 illustrates the flow of data.
515

Fig. 12.1.1: Data flow in an imperative program.

Although imperative programming is convenient, it may be inefficient. On one hand, even if the add function is repeatedly called throughout fancy_func, Python will execute the three function
calls individually. If these are executed, say, on a GPU (or even on multiple GPUs), the overhead
arising from the Python interpreter can become overwhelming. Moreover, it will need to save the variable values of e and f until all the statements in fancy_func have been executed. This is because we do not know whether the variables e and f will be used by other parts of the program after the statements e = add(a, b) and f = add(c, d) are executed.

12.1.1 Symbolic Programming

Consider the alternative, symbolic programming, where computation is usually performed only once the process has been fully defined. This strategy is used by multiple deep learning frameworks, including Theano and TensorFlow (the latter has acquired imperative extensions). It usually involves the following steps:
1. Define the operations to be executed.
2. Compile the operations into an executable program.
3. Provide the required inputs and call the compiled program for execution.
This allows for a significant amount of optimization. First, we can skip the Python interpreter in many cases, thus removing a performance bottleneck that can become significant on multiple fast GPUs paired with a single Python thread on a CPU. Second, a compiler might optimize and rewrite the above code into print((1 + 2) + (3 + 4)) or even print(10). This is possible since a compiler gets to see the full code before turning it into machine instructions. For instance, it can release memory (or never allocate it) whenever a variable is no longer needed. Or it can transform the code entirely into an equivalent piece. To get a better idea, consider the following simulation of imperative programming (it is Python after all) below.

def add_(): return '''
def add(a, b): return a + b
'''

def fancy_func_(): return '''
def fancy_func(a, b, c, d): e = add(a, b) f = add(c, d)

(continues on next page)

516

Chapter 12. Computational Performance

g = add(e, f) return g '''

(continued from previous page)

def evoke_(): return add_() + fancy_func_() + 'print(fancy_func(1, 2, 3, 4))'

prog = evoke_() print(prog) y = compile(prog, '', 'exec') exec(y)

def add(a, b): return a + b
def fancy_func(a, b, c, d): e = add(a, b) f = add(c, d) g = add(e, f) return g
print(fancy_func(1, 2, 3, 4)) 10
The differences between imperative (interpreted) programming and symbolic programming are as follows:
• Imperative programming is easier. When imperative programming is used in Python, the majority of the code is straightforward and easy to write. It is also easier to debug imperative programming code. This is because it is easier to obtain and print all relevant intermediate variable values, or use Pythonʼs built-in debugging tools.
• Symbolic programming is more efficient and easier to port. Symbolic programming makes it easier to optimize the code during compilation, while also having the ability to port the program into a format independent of Python. This allows the program to be run in a nonPython environment, thus avoiding any potential performance issues related to the Python interpreter.

12.1.2 Hybrid Programming
Historically most deep learning frameworks choose between an imperative or a symbolic approach. For example, Theano, TensorFlow (inspired by the former), Keras, and CNTK formulate models symbolically. Conversely, Chainer and PyTorch take an imperative approach. An imperative mode was added to TensorFlow 2.0 and Keras in later revisions.
When designing Gluon, developers considered whether it would be possible to combine the benefits of both programming paradigms. This led to a hybrid model that lets users develop and debug with pure imperative programming, while having the ability to convert most programs into symbolic programs to be run when product-level computing performance and deployment are required.
In practice this means that we build models using the HybridBlock or HybridSequential class. By default, either of them is executed in the same way the Block or Sequential class is executed in

12.1. Compilers and Interpreters

517

imperative programming. The HybridSequential class is a subclass of HybridBlock ( just like Sequential subclasses Block). When the hybridize function is called, Gluon compiles the model into the form used in symbolic programming. This allows one to optimize the computation-intensive components without sacrifices in the way a model is implemented. We will illustrate the benefits below, focusing on sequential models and blocks.

12.1.3 Hybridizing the Sequential Class

The easiest way to get a feel for how hybridization works is to consider deep networks with multiple layers. Conventionally the Python interpreter will need to execute the code for all layers to generate an instruction that can then be forwarded to a CPU or a GPU. For a single (fast) computing device this does not cause any major issues. On the other hand, if we use an advanced 8-GPU server such as an AWS P3dn.24xlarge instance Python will struggle to keep all GPUs busy. The single-threaded Python interpreter becomes the bottleneck here. Let us see how we can address this for significant parts of the code by replacing Sequential with HybridSequential. We begin by defining a simple MLP.

from mxnet import np, npx from mxnet.gluon import nn from d2l import mxnet as d2l

npx.set_np()

# Factory for networks def get_net():
net = nn.HybridSequential() net.add(nn.Dense(256, activation='relu'),
nn.Dense(128, activation='relu'), net.initialize() return net

nn.Dense(2))

x = np.random.normal(size=(1, 512)) net = get_net() net(x)

array([[ 0.16526186, -0.14005628]])
By calling the hybridize function, we are able to compile and optimize the computation in the MLP. The modelʼs computation result remains unchanged.
net.hybridize() net(x)

array([[ 0.16526186, -0.14005628]])
This seems almost too good to be true: simply designate a block to be HybridSequential, write the same code as before and invoke hybridize. Once this happens the network is optimized (we will benchmark the performance below). Unfortunately this does not work magically for every layer. That said, a layer will not be optimized if it inherits from the Block class instead of the HybridBlock class.

518

Chapter 12. Computational Performance

Acceleration by Hybridization
To demonstrate the performance improvement gained by compilation we compare the time needed to evaluate net(x) before and after hybridization. Let us define a class to measure this time first. It will come handy throughout the chapter as we set out to measure (and improve) performance.
#@save class Benchmark:
"""For measuring running time.""" def __init__(self, description='Done'):
self.description = description
def __enter__(self): self.timer = d2l.Timer() return self
def __exit__(self, *args): print(f'{self.description}: {self.timer.stop():.4f} sec')
Now we can invoke the network twice, once with and once without hybridization.
net = get_net() with Benchmark('Without hybridization'):
for i in range(1000): net(x)
npx.waitall()
net.hybridize() with Benchmark('With hybridization'):
for i in range(1000): net(x)
npx.waitall()
Without hybridization: 0.6680 sec With hybridization: 0.3646 sec
As is observed in the above results, after a HybridSequential instance calls the hybridize function, computing performance is improved through the use of symbolic programming.
Serialization
One of the benefits of compiling the models is that we can serialize (save) the model and its parameters to disk. This allows us to store a model in a manner that is independent of the front-end language of choice. This allows us to deploy trained models to other devices and easily use other front-end programming languages. At the same time the code is often faster than what can be achieved in imperative programming. Let us see the export function in action.
net.export('my_mlp') !ls -lh my_mlp*

12.1. Compilers and Interpreters

519

-rw-r--r-- 1 jenkins jenkins 643K Jul 21 08:59 my_mlp-0000.params -rw-r--r-- 1 jenkins jenkins 3.0K Jul 21 08:59 my_mlp-symbol.json
The model is decomposed into a (large binary) parameter file and a JSON description of the program required to execute the model computation. The files can be read by other front-end languages supported by Python or MXNet, such as C++, R, Scala, and Perl. Let us have a look at the first few lines in the model description.
!head my_mlp-symbol.json
{ "nodes": [ { "op": "null", "name": "data", "inputs": [] }, { "op": "null", "name": "dense3_weight",
Earlier, we demonstrated that, after calling the hybridize function, the model is able to achieve superior computing performance and portability. Note, though that hybridization can affect model flexibility, in particular in terms of control flow.
Besides, contrary to the Block instance, which needs to use the forward function, for a HybridBlock instance we need to use the hybrid_forward function.
class HybridNet(nn.HybridBlock): def __init__(self, **kwargs): super(HybridNet, self).__init__(**kwargs) self.hidden = nn.Dense(4) self.output = nn.Dense(2)
def hybrid_forward(self, F, x): print('module F: ', F) print('value x: ', x) x = F.npx.relu(self.hidden(x)) print('result : ', x) return self.output(x)
The code above implements a simple network with 4 hidden units and 2 outputs. The hybrid_forward function takes an additional argument F. This is needed since, depending on whether the code has been hybridized or not, it will use a slightly different library (ndarray or symbol) for processing. Both classes perform very similar functions and MXNet automatically determines the argument. To understand what is going on we print the arguments as part of the function invocation.
net = HybridNet() net.initialize() x = np.random.normal(size=(1, 3)) net(x)

520

Chapter 12. Computational Performance

module F: <module 'mxnet.ndarray' from '/var/lib/jenkins/miniconda3/envs/d2l-en-master-1/

→lib/python3.8/site-packages/mxnet/ndarray/__init__.py'>

value x: [[-0.6338663 0.40156594 0.46456942]]

result : [[0.01641375 0.

0.

0.

]]

array([[0.00097611, 0.00019453]])
Repeating the forward computation will lead to the same output (we omit details). Now let us see what happens if we invoke the hybridize function.
net.hybridize() net(x)

module F: <module 'mxnet.symbol' from '/var/lib/jenkins/miniconda3/envs/d2l-en-master-1/lib/ →python3.8/site-packages/mxnet/symbol/__init__.py'> value x: <_Symbol data> result : <_Symbol hybridnet0_relu0>

array([[0.00097611, 0.00019453]])
Instead of using ndarray we now use the symbol module for F. Moreover, even though the input is of ndarray type, the data flowing through the network is now converted to symbol type as part of the compilation process. Repeating the function call leads to a surprising outcome:
net(x)

array([[0.00097611, 0.00019453]])
This is quite different from what we saw previously. All print statements, as defined in hybrid_forward, are omitted. Indeed, after hybridization the execution of net(x) does not involve the Python interpreter any longer. This means that any spurious Python code is omitted (such as print statements) in favor of a much more streamlined execution and better performance. Instead, MXNet directly calls the C++ backend. Also note that some functions are not supported in the symbol module (e.g., asnumpy) and operations in-place such as a += b and a[:] = a + b must be rewritten as a = a + b. Nonetheless, compilation of models is worth the effort whenever speed matters. The benefit can range from small percentage points to more than twice the speed, depending on the complexity of the model, the speed of the CPU, and the speed and number of GPUs.

Summary
• Imperative programming makes it easy to design new models since it is possible to write code with control flow and the ability to use a large amount of the Python software ecosystem.
• Symbolic programming requires that we specify the program and compile it before executing it. The benefit is improved performance.
• MXNet is able to combine the advantages of both approaches as needed.

12.1. Compilers and Interpreters

521

• Models constructed by the HybridSequential and HybridBlock classes are able to convert imperative programs into symbolic programs by calling the hybridize function.
Exercises
1. Add x.asnumpy() to the first line of the hybrid_forward function of the HybridNet class in this section. Execute the code and observe the errors you encounter. Why do they happen?
2. What happens if we add control flow, i.e., the Python statements if and for in the hybrid_forward function?
3. Review the models that interest you in the previous chapters. Can you improve their computational performance by reimplementing them?
Discussions148
12.2 Asynchronous Computation
Todayʼs computers are highly parallel systems, consisting of multiple CPU cores (often multiple threads per core), multiple processing elements per GPU, and often multiple GPUs per device. In short, we can process many different things at the same time, often on different devices. Unfortunately Python is not a great way of writing parallel and asynchronous code, at least not without some extra help. After all, Python is single-threaded and this is unlikely to change in the future. Deep learning frameworks such as MXNet and TensorFlow adopt an asynchronous programming model to improve performance, while PyTorch uses Pythonʼs own scheduler leading to a different performance trade-off. For PyTorch, by default, GPU operations are asynchronous. When you call a function that uses the GPU, the operations are enqueued to the particular device, but not necessarily executed until later. This allows us to execute more computations in parallel, including operations on the CPU or other GPUs.
Hence, understanding how asynchronous programming works helps us to develop more efficient programs, by proactively reducing computational requirements and mutual dependencies. This allows us to reduce memory overhead and increase processor utilization.
import os import subprocess import numpy from mxnet import autograd, gluon, np, npx from mxnet.gluon import nn from d2l import mxnet as d2l
npx.set_np()
148 https://discuss.d2l.ai/t/360

522

Chapter 12. Computational Performance

12.2.1 Asynchrony via Backend
For a warmup consider the following toy problem: we want to generate a random matrix and multiply it. Let us do that both in NumPy and in mxnet.np to see the difference.
with d2l.Benchmark('numpy'): for _ in range(10): a = numpy.random.normal(size=(1000, 1000)) b = numpy.dot(a, a)
with d2l.Benchmark('mxnet.np'): for _ in range(10): a = np.random.normal(size=(1000, 1000)) b = np.dot(a, a)
numpy: 1.6333 sec mxnet.np: 0.0214 sec
The benchmark output via MXNet is orders of magnitude faster. Since both are executed on the same processor something else must be going on. Forcing MXNet to finish all the backend computation prior to returning shows what happened previously: computation is executed by the backend while the frontend returns control to Python.
with d2l.Benchmark(): for _ in range(10): a = np.random.normal(size=(1000, 1000)) b = np.dot(a, a) npx.waitall()
Done: 0.8500 sec
Broadly speaking, MXNet has a frontend for direct interactions with users, e.g., via Python, as well as a backend used by the system to perform the computation. As shown in Fig. 12.2.1, users can write MXNet programs in various frontend languages, such as Python, R, Scala, and C++. Regardless of the frontend programming language used, the execution of MXNet programs occurs primarily in the backend of C++ implementations. Operations issued by the frontend language are passed on to the backend for execution. The backend manages its own threads that continuously collect and execute queued tasks. Note that for this to work the backend must be able to keep track of the dependencies between various steps in the computational graph. Hence, it is not possible to parallelize operations that depend on each other.

12.2. Asynchronous Computation

523

Fig. 12.2.1: Programming language frontends and deep learning framework backends.
Let us look at another toy example to understand the dependency graph a bit better. x = np.ones((1, 2)) y = np.ones((1, 2)) z=x*y+2 z
array([[3., 3.]])

Fig. 12.2.2: The backend tracks dependencies between various steps in the computational graph.
The code snippet above is also illustrated in Fig. 12.2.2. Whenever the Python frontend thread executes one of the first three statements, it simply returns the task to the backend queue. When the last statementʼs results need to be printed, the Python frontend thread will wait for the C++ backend thread to finish computing the result of the variable z. One benefit of this design is that the Python frontend thread does not need to perform actual computations. Thus, there is little impact on the programʼs overall performance, regardless of Pythonʼs performance. Fig. 12.2.3 illustrates how frontend and backend interact.

524

Chapter 12. Computational Performance

Fig. 12.2.3: Interactions of the frontend and backend.

12.2.2 Barriers and Blockers
There are a number of operations that will force Python to wait for completion:
• Most obviously npx.waitall() waits until all computation has completed, regardless of when the compute instructions were issued. In practice it is a bad idea to use this operator unless absolutely necessary since it can lead to poor performance.
• If we just want to wait until a specific variable is available we can call z.wait_to_read(). In this case MXNet blocks return to Python until the variable z has been computed. Other computation may well continue afterwards.
Let us see how this works in practice.
with d2l.Benchmark('waitall'): b = np.dot(a, a) npx.waitall()
with d2l.Benchmark('wait_to_read'): b = np.dot(a, a) b.wait_to_read()

waitall: 0.0116 sec wait_to_read: 0.0047 sec
Both operations take approximately the same time to complete. Besides the obvious blocking operations we recommend that you are aware of implicit blockers. Printing a variable clearly requires the variable to be available and is thus a blocker. Last, conversions to NumPy via z.asnumpy() and conversions to scalars via z.item() are blocking, since NumPy has no notion of asynchrony. It needs access to the values just like the print function.
Copying small amounts of data frequently from MXNetʼs scope to NumPy and back can destroy performance of an otherwise efficient code, since each such operation requires the computational graph to evaluate all intermediate results needed to get the relevant term before anything else can be done.
with d2l.Benchmark('numpy conversion'): b = np.dot(a, a) b.asnumpy()
(continues on next page)

12.2. Asynchronous Computation

525

with d2l.Benchmark('scalar conversion'): b = np.dot(a, a) b.sum().item()
numpy conversion: 0.0051 sec scalar conversion: 0.0153 sec

(continued from previous page)

12.2.3 Improving Computation
On a heavily multithreaded system (even regular laptops have 4 threads or more and on multisocket servers this number can exceed 256) the overhead of scheduling operations can become significant. This is why it is highly desirable to have computation and scheduling occur asynchronously and in parallel. To illustrate the benefit of doing so let us see what happens if we increment a variable by 1 multiple times, both in sequence or asynchronously. We simulate synchronous execution by inserting a wait_to_read barrier in between each addition.
with d2l.Benchmark('synchronous'): for _ in range(10000): y=x+1 y.wait_to_read()
with d2l.Benchmark('asynchronous'): for _ in range(10000): y=x+1 npx.waitall()
synchronous: 0.9692 sec asynchronous: 0.8461 sec
A slightly simplified interaction between the Python frontend thread and the C++ backend thread can be summarized as follows: 1. The frontend orders the backend to insert the computation task y = x + 1 into the queue. 1. The backend then receives the computation tasks from the queue and performs the actual computations. 1. The backend then returns the computation results to the frontend. Assume that the durations of these three stages are t1, t2 and t3, respectively. If we do not use asynchronous programming, the total time taken to perform 10000 computations is approximately 10000(t1 + t2 + t3). If asynchronous programming is used, the total time taken to perform 10000 computations can be reduced to t1 + 10000t2 + t3 (assuming 10000t2 > 9999t1), since the frontend does not have to wait for the backend to return computation results for each loop.

526

Chapter 12. Computational Performance

Summary
• Deep learning frameworks may decouple the Python frontend from an execution backend. This allows for fast asynchronous insertion of commands into the backend and associated parallelism.
• Asynchrony leads to a rather responsive frontend. However, use caution not to overfill the task queue since it may lead to excessive memory consumption. It is recommended to synchronize for each minibatch to keep frontend and backend approximately synchronized.
• Chip vendors offer sophisticated performance analysis tools to obtain a much more finegrained insight into the efficiency of deep learning.
• Be aware of the fact that conversions from MXNetʼs memory management to Python will force the backend to wait until the specific variable is ready. Functions such as print, asnumpy and item all have this effect. This can be desirable but a careless use of synchronization can ruin performance.
Exercises
1. We mentioned above that using asynchronous computation can reduce the total amount of time needed to perform 10000 computations to t1 + 10000t2 + t3. Why do we have to assume 10000t2 > 9999t1 here?
2. Measure the difference between waitall and wait_to_read. Hint: perform a number of instructions and synchronize for an intermediate result.
Discussions149
12.3 Automatic Parallelism
Deep learning frameworks (e.g., MXNet and PyTorch) automatically construct computational graphs at the backend. Using a computational graph, the system is aware of all the dependencies, and can selectively execute multiple non-interdependent tasks in parallel to improve speed. For instance, Fig. 12.2.2 in Section 12.2 initializes two variables independently. Consequently the system can choose to execute them in parallel.
Typically, a single operator will use all the computational resources on all CPUs or on a single GPU. For example, the dot operator will use all cores (and threads) on all CPUs, even if there are multiple CPU processors on a single machine. The same applies to a single GPU. Hence parallelization is not quite so useful for single-device computers. With multiple devices things matter more. While parallelization is typically most relevant between multiple GPUs, adding the local CPU will increase performance slightly. For example, see (Hadjis et al., 2016) that focuses on training computer vision models combining a GPU and a CPU. With the convenience of an automatically parallelizing framework we can accomplish the same goal in a few lines of Python code. More broadly, our discussion of automatic parallel computation focuses on parallel computation using both CPUs and GPUs, as well as the parallelization of computation and communication.
Note that we need at least two GPUs to run the experiments in this section.
149 https://discuss.d2l.ai/t/361

12.3. Automatic Parallelism

527

from mxnet import np, npx from d2l import mxnet as d2l
npx.set_np()

12.3.1 Parallel Computation on GPUs
Let us start by defining a reference workload to test: the run function below performs 10 matrixmatrix multiplications on the device of our choice using data allocated into two variables: x_gpu1 and x_gpu2.
devices = d2l.try_all_gpus()
def run(x): return [x.dot(x) for _ in range(50)]
x_gpu1 = np.random.uniform(size=(4000, 4000), ctx=devices[0]) x_gpu2 = np.random.uniform(size=(4000, 4000), ctx=devices[1])
Now we apply the function to the data. To ensure that caching does not play a role in the results we warm up the devices by performing a single pass on either of them prior to measuring.
run(x_gpu1) # Warm-up both devices run(x_gpu2) npx.waitall()
with d2l.Benchmark('GPU1 time'): run(x_gpu1) npx.waitall()
with d2l.Benchmark('GPU2 time'): run(x_gpu2) npx.waitall()
GPU1 time: 0.4847 sec GPU2 time: 0.4863 sec
If we remove the waitall statement between both tasks the system is free to parallelize computation on both devices automatically.
with d2l.Benchmark('GPU1 & GPU2'): run(x_gpu1) run(x_gpu2) npx.waitall()
GPU1 & GPU2: 0.4923 sec
In the above case the total execution time is less than the sum of its parts, since the deep learning framework automatically schedules computation on both GPU devices without the need for sophisticated code on behalf of the user.

528

Chapter 12. Computational Performance

12.3.2 Parallel Computation and Communication
In many cases we need to move data between different devices, say between the CPU and GPU, or between different GPUs. For instance, this occurs when we want to perform distributed optimization where we need to aggregate the gradients over multiple accelerator cards. Let us simulate this by computing on the GPU and then copying the results back to the CPU.
def copy_to_cpu(x): return [y.copyto(npx.cpu()) for y in x]
with d2l.Benchmark('Run on GPU1'): y = run(x_gpu1) npx.waitall()
with d2l.Benchmark('Copy to CPU'): y_cpu = copy_to_cpu(y) npx.waitall()
Run on GPU1: 0.5228 sec Copy to CPU: 2.6559 sec
This is somewhat inefficient. Note that we could already start copying parts of y to the CPU while the remainder of the list is still being computed. This situation occurs, e.g., when we compute the gradient on a minibatch. The gradients of some of the parameters will be available earlier than that of others. Hence it works to our advantage to start using PCI-Express bus bandwidth while the GPU is still running. Removing waitall between both parts allows us to simulate this scenario.
with d2l.Benchmark('Run on GPU1 and copy to CPU'): y = run(x_gpu1) y_cpu = copy_to_cpu(y) npx.waitall()
Run on GPU1 and copy to CPU: 3.1102 sec
The total time required for both operations is (as expected) less than the sum of their parts. Note that this task is different from parallel computation as it uses a different resource: the bus between the CPU and GPUs. In fact, we could compute on both devices and communicate, all at the same time. As noted above, there is a dependency between computation and communication: y[i] must be computed before it can be copied to the CPU. Fortunately, the system can copy y[i-1] while computing y[i] to reduce the total running time.
We conclude with an illustration of the computational graph and its dependencies for a simple two-layer MLP when training on a CPU and two GPUs, as depicted in Fig. 12.3.1. It would be quite painful to schedule the parallel program resulting from this manually. This is where it is advantageous to have a graph-based computing backend for optimization.

12.3. Automatic Parallelism

529

Fig. 12.3.1: The computational graph and its dependencies of a two-layer MLP on a CPU and two GPUs.
Summary
• Modern systems have a variety of devices, such as multiple GPUs and CPUs. They can be used in parallel, asynchronously.
• Modern systems also have a variety of resources for communication, such as PCI Express, storage (typically solid-state drives or via networks), and network bandwidth. They can be used in parallel for peak efficiency.
• The backend can improve performance through automatic parallel computation and communication.

530

Chapter 12. Computational Performance

Exercises
1. Eight operations were performed in the run function defined in this section. There are no dependencies between them. Design an experiment to see if the deep learning framework will automatically execute them in parallel.
2. When the workload of an individual operator is sufficiently small, parallelization can help even on a single CPU or GPU. Design an experiment to verify this.
3. Design an experiment that uses parallel computation on CPUs, GPUs, and communication between both devices.
4. Use a debugger such as NVIDIAʼs Nsight150 to verify that your code is efficient.
5. Designing computation tasks that include more complex data dependencies, and run experiments to see if you can obtain the correct results while improving performance.
Discussions151
12.4 Hardware
Building systems with great performance requires a good understanding of the algorithms and models to capture the statistical aspects of the problem. At the same time it is also indispensable to have at least a modicum of knowledge of the underlying hardware. The current section is no substitute for a proper course on hardware and system design. Instead, it might serve as a starting point for understanding why some algorithms are more efficient than others and how to achieve good throughput. A good design can easily make a difference of an order of magnitude and, in turn, this can make the difference between being able to train a network (e.g., in a week) and not at all (in 3 months, thus missing the deadline). We will start by looking at computers. Then we will zoom in to look more carefully at CPUs and GPUs. Lastly we zoom out to review how multiple computers are connected in a server center or in the cloud.

Fig. 12.4.1: Latency Numbers that every programmer should know.

Impatient readers may be able to get by with Fig. 12.4.1. It is taken from Colin Scottʼs interactive post152 that gives a good overview of the progress over the past decade. The original numbers are
150 https://developer.nvidia.com/nsight-compute-2019_5 151 https://discuss.d2l.ai/t/362 152 https://people.eecs.berkeley.edu/~rcs/research/interactive_latency.html

12.4. Hardware

531

due to Jeff Deanʼs Stanford talk from 2010153. The discussion below explains some of the rationale for these numbers and how they can guide us in designing algorithms. The discussion below is very high level and cursory. It is clearly no substitute for a proper course but rather just meant to provide enough information for a statistical modeler to make suitable design decisions. For an in-depth overview of computer architecture we refer the reader to (Hennessy & Patterson, 2011) or a recent course on the subject, such as the one by Arste Asanovic154.
12.4.1 Computers
Most deep learning researchers and practitioners have access to a computer with a fair amount of memory, computation, some form of an accelerator such as a GPU, or multiples thereof. A computer consists of the following key components:
• A processor (also referred to as a CPU) that is able to execute the programs we give it (in addition to running an operating system and many other things), typically consisting of 8 or more cores.
• Memory (RAM) to store and retrieve the results from computation, such as weight vectors and activations, and training data.
• An Ethernet network connection (sometimes multiple) with speeds ranging from 1 GB/s to 100 GB/s. On high end servers more advanced interconnects can be found.
• A high speed expansion bus (PCIe) to connect the system to one or more GPUs. Servers have up to 8 accelerators, often connected in an advanced topology, while desktop systems have 1 or 2, depending on the budget of the user and the size of the power supply.
• Durable storage, such as a magnetic hard disk drive, a solid state drive, in many cases connected using the PCIe bus. It provides efficient transfer of training data to the system and storage of intermediate checkpoints as needed.

Fig. 12.4.2: Connectivity of components of a computer.
As Fig. 12.4.2 indicates, most components (network, GPU, and storage) are connected to the CPU across the PCIe bus. It consists of multiple lanes that are directly attached to the CPU. For instance AMDʼs Threadripper 3 has 64 PCIe 4.0 lanes, each of which is capable 16 Gbit/s data transfer in both directions. The memory is directly attached to the CPU with a total bandwidth of up to 100 GB/s.
When we run code on a computer we need to shuffle data to the processors (CPUs or GPUs), perform computation, and then move the results off the processor back to RAM and durable storage. Hence, in order to get good performance we need to make sure that this works seamlessly without any one of the systems becoming a major bottleneck. For instance, if we cannot load images quickly enough the processor will not have any work to do. Likewise, if we cannot move matrices
153 https://static.googleusercontent.com/media/research.google.com/en//people/jeff/Stanford-DL-Nov-2010.pdf 154 http://inst.eecs.berkeley.edu/~cs152/sp19/

532

Chapter 12. Computational Performance

quickly enough to the CPU (or GPU), its processing elements will starve. Finally, if we want to synchronize multiple computers across the network, the latter should not slow down computation. One option is to interleave communication and computation. Let us have a look at the various components in more detail.
12.4.2 Memory
At its most basic memory is used to store data that needs to be readily accessible. At present CPU RAM is typically of the DDR4155 variety, offering 20–25 GB/s bandwidth per module. Each module has a 64-bit-wide bus. Typically pairs of memory modules are used to allow for multiple channels. CPUs have between 2 and 4 memory channels, i.e., they have between 4 0GB/s and 100 GB/s peak memory bandwidth. Often there are two banks per channel. For instance AMDʼs Zen 3 Threadripper has 8 slots.
While these numbers are impressive, indeed, they only tell part of the story. When we want to read a portion from memory we first need to tell the memory module where the information can be found. That is, we first need to send the address to RAM. Once this is accomplished we can choose to read just a single 64 bit record or a long sequence of records. The latter is called burst read. In a nutshell, sending an address to memory and setting up the transfer takes approximately 100 ns (details depend on the specific timing coefficients of the memory chips used), every subsequent transfer takes only 0.2 ns. In short, the first read is 500 times as expensive as subsequent ones! Note that we could perform up to 10,000,000 random reads per second. This suggests that we avoid random memory access as far as possible and use burst reads (and writes) instead.
Matters are a bit more complex when we take into account that we have multiple banks. Each bank can read memory largely independently. This means two things. On one hand, the effective number of random reads is up to 4 times higher, provided that they are spread evenly across memory. It also means that it is still a bad idea to perform random reads since burst reads are 4 times faster, too. On the other hand, due to memory alignment to 64 bit boundaries it is a good idea to align any data structures with the same boundaries. Compilers do this pretty much automatically156 when the appropriate flags are set. Curious readers are encouraged to review a lecture on DRAMs such as the one by Zeshan Chishti157.
GPU memory is subject to even higher bandwidth requirements since they have many more processing elements than CPUs. By and large there are two options to address them. The first is to make the memory bus significantly wider. For instance, NVIDIAʼs RTX 2080 Ti has a 352-bit-wide bus. This allows for much more information to be transferred at the same time. Second, GPUs use specific high-performance memory. Consumer-grade devices, such as NVIDIAʼs RTX and Titan series typically use GDDR6158 chips with over 500 GB/s aggregate bandwidth. An alternative is to use HBM (high bandwidth memory) modules. They use a very different interface and connect directly with GPUs on a dedicated silicon wafer. This makes them very expensive and their use is typically limited to high-end server chips, such as the NVIDIA Volta V100 series of accelerators. Quite unsurprisingly, GPU memory is generally much smaller than CPU memory due to the higher cost of the former. For our purposes, by and large their performance characteristics are similar, just a lot faster. We can safely ignore the details for the purpose of this book. They only matter when tuning GPU kernels for high throughput.
155 https://en.wikipedia.org/wiki/DDR4_SDRAM 156 https://en.wikipedia.org/wiki/Data_structure_alignment 157 http://web.cecs.pdx.edu/~zeshan/ece585_lec5.pdf 158 https://en.wikipedia.org/wiki/GDDR6_SDRAM

12.4. Hardware

533

12.4.3 Storage
We saw that some of the key characteristics of RAM are bandwidth and latency. The same is true for storage devices, just that the differences can be even more extreme.
Hard Disk Drives
Hard disk drives (HDDs) have been in use for over half a century. In a nutshell they contain a number of spinning platters with heads that can be positioned to read or write at any given track. High-end disks hold up to 16 TB on 9 platters. One of the key benefits of HDDs is that they are relatively inexpensive. One of their many downsides are their typically catastrophic failure modes and their relatively high read latency.
To understand the latter, consider the fact that HDDs spin at around 7,200 RPM (revolutions per minute). If they were much faster they would shatter due to the centrifugal force exerted on the platters. This has a major downside when it comes to accessing a specific sector on the disk: we need to wait until the platter has rotated in position (we can move the heads but not accelerate the actual disks). Hence it can take over 8 ms until the requested data are available. A common way this is expressed is to say that HDDs can operate at approximately 100 IOPs (input/output operations per second). This number has essentially remained unchanged for the past two decades. Worse still, it is equally difficult to increase bandwidth (it is in the order of 100–200 MB/s). After all, each head reads a track of bits, hence the bit rate only scales with the square root of the information density. As a result, HDDs are quickly becoming relegated to archival storage and low-grade storage for very large datasets.
Solid State Drives
Solid state drives (SSDs) use flash memory to store information persistently. This allows for much faster access to stored records. Modern SSDs can operate at 100,000 to 500,000 IOPs, i.e., up to 3 orders of magnitude faster than HDDs. Furthermore, their bandwidth can reach 1–3GB/s, i.e., one order of magnitude faster than HDDs. These improvements sound almost too good to be true. Indeed, they come with the following caveats, due to the way SSDs are designed.
• SSDs store information in blocks (256 KB or larger). They can only be written as a whole, which takes significant time. Consequently bit-wise random writes on SSD have very poor performance. Likewise, writing data in general takes significant time since the block has to be read, erased and then rewritten with new information. By now SSD controllers and firmware have developed algorithms to mitigate this. Nonetheless, writes can be much slower, in particular for QLC (quad level cell) SSDs. The key for improved performance is to maintain a queue of operations, to prefer reads and to write in large blocks if possible.
• The memory cells in SSDs wear out relatively quickly (often already after a few thousand writes). Wear-level protection algorithms are able to spread the degradation over many cells. That said, it is not recommended to use SSDs for swapping files or for large aggregations of log-files.
• Lastly, the massive increase in bandwidth has forced computer designers to attach SSDs directly to the PCIe bus. The drives capable of handling this, referred to as NVMe (Non Volatile Memory enhanced), can use up to 4 PCIe lanes. This amounts to up to 8GB/s on PCIe 4.0.

534

Chapter 12. Computational Performance

Cloud Storage
Cloud storage provides a configurable range of performance. That is, the assignment of storage to virtual machines is dynamic, both in terms of quantity and in terms of speed, as chosen by users. We recommend that users increase the provisioned number of IOPs whenever latency is too high, e.g., during training with many small records.
12.4.4 CPUs
Central processing units (CPUs) are the centerpiece of any computer. They consist of a number of key components: processor cores that are able to execute machine code, a bus connecting them (the specific topology differs significantly between processor models, generations, and vendors), and caches to allow for higher bandwidth and lower latency memory access than what is possible by reads from main memory. Lastly, almost all modern CPUs contain vector processing units to aid with high performance linear algebra and convolutions, as they are common in media processing and machine learning.

Fig. 12.4.3: Intel Skylake consumer quad-core CPU.
Fig. 12.4.3 depicts an Intel Skylake consumer-grade quad-core CPU. It has an integrated GPU, caches, and a ringbus connecting the four cores. Peripherals, such as Ethernet, WiFi, Bluetooth, SSD controller, and USB, are either part of the chipset or directly attached (PCIe) to the CPU.

12.4. Hardware

535

Microarchitecture
Each of the processor cores consists of a rather sophisticated set of components. While details differ between generations and vendors, the basic functionality is pretty much standard. The front-end loads instructions and tries to predict which path will be taken (e.g., for control flow). Instructions are then decoded from assembly code to microinstructions. Assembly code is often not the lowest level code that a processor executes. Instead, complex instructions may be decoded into a set of more lower level operations. These are then processed by the actual execution core. Often the latter is capable of performing many operations simultaneously. For instance, the ARM Cortex A77 core of Fig. 12.4.4 is able to perform up to 8 operations simultaneously.

Fig. 12.4.4: ARM Cortex A77 Microarchitecture.
This means that efficient programs might be able to perform more than one instruction per clock cycle, provided that they can be carried out independently. Not all units are created equal. Some specialize in integer instructions whereas others are optimized for floating point performance. To increase throughput, the processor might also follow multiple code paths simultaneously in a branching instruction and then discard the results of the branches not taken. This is why branch prediction units matter (on the front-end) such that only the most promising paths are pursued.
Vectorization
Deep learning is extremely compute-hungry. Hence, to make CPUs suitable for machine learning, one needs to perform many operations in one clock cycle. This is achieved via vector units. They have different names: on ARM they are called NEON, on x86 they (a recent generation) are referred to as AVX2159 units. A common aspect is that they are able to perform SIMD (single instruction multiple data) operations. Fig. 12.4.5 shows how 8 short integers can be added in one clock cycle on ARM.
159 https://en.wikipedia.org/wiki/Advanced_Vector_Extensions

536

Chapter 12. Computational Performance

Fig. 12.4.5: 128 bit NEON vectorization.

Depending on architecture choices, such registers are up to 512 bits long, allowing for the combination of up to 64 pairs of numbers. For instance, we might be multiplying two numbers and adding them to a third, which is also known as a fused multiply-add. Intelʼs OpenVino160 uses these to achieve respectable throughput for deep learning on server-grade CPUs. Note, though, that this number is entirely dwarfed by what GPUs are capable of achieving. For instance, NVIDIAʼs RTX 2080 Ti has 4,352 CUDA cores, each of which is capable of processing such an operation at any time.

Cache
Consider the following situation: we have a modest CPU core with 4 cores as depicted in Fig. 12.4.3 above, running at 2 GHz frequency. Moreover, let us assume that we have an IPC (instructions per clock) count of 1 and that the units have AVX2 with 256-bit width enabled. Let us furthermore assume that at least one of the registers used for AVX2 operations needs to be retrieved from memory. This means that the CPU consumes 4 × 256 bit = 128 bytes of data per clock cycle. Unless we are able to transfer 2 × 109 × 128 = 256 × 109 bytes to the processor per second the processing elements are going to starve. Unfortunately the memory interface of such a chip only supports 20–40 GB/s data transfer, i.e., one order of magnitude less. The fix is to avoid loading new data from memory as far as possible and rather to cache it locally on the CPU. This is where caches come in handy. Commonly the following names or concepts are used:
• Registers are strictly speaking not part of the cache. They help stage instructions. That said, CPU registers are memory locations that a CPU can access at clock speed without any delay penalty. CPUs have tens of registers. It is up to the compiler (or programmer) to use registers efficiently. For instance the C programming language has a register keyword.
• L1 caches are the first line of defense against high memory bandwidth requirements. L1 caches are tiny (typical sizes might be 32–64 KB) and often split into data and instructions caches. When data are found in the L1 cache, access is very fast. If they cannot be found there, the search progresses down the cache hierarchy.
• L2 caches are the next stop. Depending on architecture design and processor size they might be exclusive. They might be accessible only by a given core or shared among multiple cores. L2 caches are larger (typically 256–512 KB per core) and slower than L1. Furthermore, to access something in L2 we first need to check to realize that the data are not in L1, which adds a small amount of extra latency.
160 https://01.org/openvinotoolkit

12.4. Hardware

537

• L3 caches are shared among multiple cores and can be quite large. AMDʼs Epyc 3 server CPUs have a whopping 256 MB of cache spread across multiple chiplets. More typical numbers are in the 4–8 MB range.
Predicting which memory elements will be needed next is one of the key optimization parameters in chip design. For instance, it is advisable to traverse memory in a forward direction since most caching algorithms will try to read ahead rather than backwards. Likewise, keeping memory access patterns local is a good way of improving performance.
Adding caches is a double-edge sword. On one hand they ensure that the processor cores do not starve of data. At the same time they increase chip size, using up area that otherwise could have been spent on increasing processing power. Moreover, cache misses can be expensive. Consider the worst case scenario, false sharing, as depicted in Fig. 12.4.6. A memory location is cached on processor 0 when a thread on processor 1 requests the data. To obtain it, processor 0 needs to stop what it is doing, write the information back to main memory and then let processor 1 read it from memory. During this operation both processors wait. Quite potentially such code runs more slowly on multiple processors when compared with an efficient single-processor implementation. This is one more reason for why there is a practical limit to cache sizes (besides their physical size).

Fig. 12.4.6: False sharing (image courtesy of Intel).
12.4.5 GPUs and other Accelerators
It is not an exaggeration to claim that deep learning would not have been successful without GPUs. By the same token, it is quite reasonable to argue that GPU manufacturersʼ fortunes have increased significantly due to deep learning. This co-evolution of hardware and algorithms has led to a situation where for better or worse deep learning is the preferable statistical modeling paradigm. Hence it pays to understand the specific benefits that GPUs and related accelerators such as the TPU (Jouppi et al., 2017).
Of note is a distinction that is often made in practice: accelerators are optimized either for training or inference. For the latter we only need to compute the forward propagation in a network. No storage of intermediate data is needed for backpropagation. Moreover, we may not need very precise computation (FP16 or INT8 typically suffice). On the other hand, during training all intermediate results need storage to compute gradients. Moreover, accumulating gradients requires higher precision to avoid numerical underflow (or overflow). This means that FP16 (or mixed precision with FP32) is the minimum requirement. All of this necessitates faster and larger memory

538

Chapter 12. Computational Performance

(HBM2 vs. GDDR6) and more processing power. For instance, NVIDIAʼs Turing161 T4 GPUs are optimized for inference whereas the V100 GPUs are preferable for training. Recall vectorization as illustrated in Fig. 12.4.5. Adding vector units to a processor core allowed us to increase throughput significantly. For example, in the example in Fig. 12.4.5 we were able to perform 16 operations simultaneously. First, what if we added operations that optimized not just operations between vectors but also between matrices? This strategy led to tensor cores (to be covered shortly). Second, what if we added many more cores? In a nutshell, these two strategies summarize the design decisions in GPUs. Fig. 12.4.7 gives an overview of a basic processing block. It contains 16 integer and 16 floating point units. In addition to that, two tensor cores accelerate a narrow subset of additional operations relevant for deep learning. Each streaming multiprocessor consists of four such blocks.
Fig. 12.4.7: NVIDIA Turing processing block (image courtesy of NVIDIA).
Next, 12 streaming multiprocessors are grouped into graphics processing clusters which make up the high-end TU102 processors. Ample memory channels and an L2 cache complement the setup. Fig. 12.4.8 has the relevant details. One of the reasons for designing such a device is that individual blocks can be added or removed as needed to allow for more compact chips and to deal with yield issues (faulty modules might not be activated). Fortunately programming such devices is well hidden from the casual deep learning researcher beneath layers of CUDA and framework code. In particular, more than one of the programs might well be executed simultaneously on the GPU, provided that there are available resources. Nonetheless it pays to be aware of the limitations of the devices to avoid picking models that do not fit into device memory.
161 https://devblogs.nvidia.com/nvidia-turing-architecture-in-depth/

12.4. Hardware

539

Fig. 12.4.8: NVIDIA Turing architecture (image courtesy of NVIDIA)
A last aspect that is worth mentioning in more detail are tensor cores. They are an example of a recent trend of adding more optimized circuits that are specifically effective for deep learning. For instance, the TPU added a systolic array (Kung, 1988) for fast matrix multiplication. There the design was to support a very small number (one for the first generation of TPUs) of large operations. Tensor cores are at the other end. They are optimized for small operations involving between 4 × 4 and 16 × 16 matrices, depending on their numerical precision. Fig. 12.4.9 gives an overview of the optimizations.

Fig. 12.4.9: NVIDIA tensor cores in Turing (image courtesy of NVIDIA).

Obviously when optimizing for computation we end up making certain compromises. One of them is that GPUs are not very good at handling interrupts and sparse data. While there are notable exceptions, such as Gunrock162 (Wang et al., 2016), the access pattern of sparse matrices and vectors do not go well with the high bandwidth burst read operations where GPUs excel. Matching both goals is an area of active research. See e.g., DGL163, a library tuned for deep learning on
162 https://github.com/gunrock/gunrock 163 http://dgl.ai

540

Chapter 12. Computational Performance

graphs.
12.4.6 Networks and Buses
Whenever a single device is insufficient for optimization we need to transfer data to and from it to synchronize processing. This is where networks and buses come in handy. We have a number of design parameters: bandwidth, cost, distance, and flexibility. On one end we have WiFi that has a pretty good range, is very easy to use (no wires, after all), cheap but it offers comparatively mediocre bandwidth and latency. No machine learning researcher within their right mind would use it to build a cluster of servers. In what follows we focus on interconnects that are suitable for deep learning.
• PCIe is a dedicated bus for very high bandwidth point-to-point connections (up to 32 GB/s on PCIe 4.0 in a 16-lane slot) per lane. Latency is in the order of single-digit microseconds (5 µs). PCIe links are precious. Processors only have a limited number of them: AMDʼs EPYC 3 has 128 lanes, Intelʼs Xeon has up to 48 lanes per chip; on desktop-grade CPUs the numbers are 20 (Ryzen 9) and 16 (Core i9) respectively. Since GPUs have typically 16 lanes, this limits the number of GPUs that can connect to the CPU at full bandwidth. After all, they need to share the links with other high bandwidth peripherals such as storage and Ethernet. Just like with RAM access, large bulk transfers are preferable due to reduced packet overhead.
• Ethernet is the most commonly used way of connecting computers. While it is significantly slower than PCIe, it is very cheap and resilient to install and covers much longer distances. Typical bandwidth for low-grade servers is 1 GBit/s. Higher-end devices (e.g., C5 instances164 in the cloud) offer between 10 and 100 GBit/s bandwidth. As in all previous cases data transmission has significant overheads. Note that we almost never use raw Ethernet directly but rather a protocol that is executed on top of the physical interconnect (such as UDP or TCP/IP). This adds further overhead. Like PCIe, Ethernet is designed to connect two devices, e.g., a computer and a switch.
• Switches allow us to connect multiple devices in a manner where any pair of them can carry out a (typically full bandwidth) point-to-point connection simultaneously. For instance, Ethernet switches might connect 40 servers at high cross-sectional bandwidth. Note that switches are not unique to traditional computer networks. Even PCIe lanes can be switched165. This occurs, e.g., to connect a large number of GPUs to a host processor, as is the case for the P2 instances166.
• NVLink is an alternative to PCIe when it comes to very high bandwidth interconnects. It offers up to 300 Gbit/s data transfer rate per link. Server GPUs (Volta V100) have six links whereas consumer-grade GPUs (RTX 2080 Ti) have only one link, operating at a reduced 100 Gbit/s rate. We recommend to use NCCL167 to achieve high data transfer between GPUs.
164 https://aws.amazon.com/ec2/instance-types/c5/ 165 https://www.broadcom.com/products/pcie-switches-bridges/pcie-switches 166 https://aws.amazon.com/ec2/instance-types/p2/ 167 https://github.com/NVIDIA/nccl

12.4. Hardware

541

12.4.7 More Latency Numbers
The summary in Table 12.4.1 and Table 12.4.2 are from Eliot Eshelman168 who maintains an updated version of the numbers as a GitHub gist169.

Table 12.4.1: Common Latency Numbers.

Action L1 cache reference/hit Floating-point add/mult/FMA L2 cache reference/hit Branch mispredict L3 cache hit (unshared cache) L3 cache hit (shared in another core) Mutex lock/unlock L3 cache hit (modified in another core) L3 cache hit (on a remote CPU socket) QPI hop to a another CPU (per hop) 64MB memory ref. (local CPU) 64MB memory ref. (remote CPU) 256MB memory ref. (local CPU) Intel Optane random write 256MB memory ref. (remote CPU) Intel Optane random read Send 4KB over 100 Gbps HPC fabric Compress 1KB with Google Snappy Send 4KB over 10 Gbps ethernet Write 4KB randomly to NVMe SSD Transfer 1MB to/from NVLink GPU Transfer 1MB to/from PCI-E GPU Read 4KB randomly from NVMe SSD Read 1MB sequentially from NVMe SSD Write 4KB randomly to SATA SSD Read 4KB randomly from SATA SSD Round trip within same datacenter Read 1MB sequentially from SATA SSD Read 1MB sequentially from disk Random Disk Access (seek+rotation) Send packet CA->Netherlands->CA

Time 1.5 ns 1.5 ns 5 ns 6 ns 16 ns 25 ns 25 ns 29 ns 40 ns 40 ns 46 ns 70 ns 75 ns 94 ns 120 ns 305 ns 1 µs 3 µs 10 µs 30 µs 30 µs 80 µs 120 µs 208 µs 500 µs 500 µs 500 µs 2 ms 5 ms 10 ms 150 ms

Notes 4 cycles 4 cycles 12 ~ 17 cycles 15 ~ 20 cycles 42 cycles 65 cycles
75 cycles 100 ~ 300 cycles (40 ~ 116 ns)
TinyMemBench on Broadwell E5-2690v4 TinyMemBench on Broadwell E5-2690v4 TinyMemBench on Broadwell E5-2690v4 UCSD Non-Volatile Systems Lab TinyMemBench on Broadwell E5-2690v4 UCSD Non-Volatile Systems Lab MVAPICH2 over Intel Omni-Path
DC P3608 NVMe SSD (QOS 99% is 500µs) ~33GB/s on NVIDIA 40GB NVLink ~12GB/s on PCIe 3.0 x16 link DC P3608 NVMe SSD (QOS 99%) ~4.8GB/s DC P3608 NVMe SSD DC S3510 SATA SSD (QOS 99.9%) DC S3510 SATA SSD (QOS 99.9%) One-way ping is ~250µs ~550MB/s DC S3510 SATA SSD ~200MB/s server HDD

Table 12.4.2: Latency Numbers for NVIDIA Tesla GPUs.

Action

Time Notes

GPU Shared Memory access

30 ns 30~90 cycles (bank conflicts add latency)

GPU Global Memory access

200 ns 200~800 cycles

Launch CUDA kernel on GPU

10 µs Host CPU instructs GPU to start kernel

Transfer 1MB to/from NVLink GPU 30 µs ~33GB/s on NVIDIA 40GB NVLink

Transfer 1MB to/from PCI-E GPU 80 µs ~12GB/s on PCI-Express x16 link

168 https://gist.github.com/eshelman 169 https://gist.github.com/eshelman/343a1c46cb3fba142c1afdcdeec17646

542

Chapter 12. Computational Performance

Summary
• Devices have overheads for operations. Hence it is important to aim for a small number of large transfers rather than many small ones. This applies to RAM, SSDs, networks and GPUs.
• Vectorization is key for performance. Make sure you are aware of the specific abilities of your accelerator. E.g., some Intel Xeon CPUs are particularly good for INT8 operations, NVIDIA Volta GPUs excel at FP16 matrix-matrix operations and NVIDIA Turing shines at FP16, INT8, and INT4 operations.
• Numerical overflow due to small data types can be a problem during training (and to a lesser extent during inference).
• Aliasing can significantly degrade performance. For instance, memory alignment on 64 bit CPUs should be done with respect to 64 bit boundaries. On GPUs it is a good idea to keep convolution sizes aligned, e.g., to tensor cores.
• Match your algorithms to the hardware (e.g., memory footprint, and bandwidth). Great speedup (orders of magnitude) can be achieved when fitting the parameters into caches.
• We recommend that you sketch out the performance of a novel algorithm on paper before verifying the experimental results. Discrepancies of an order-of-magnitude or more are reasons for concern.
• Use profilers to debug performance bottlenecks.
• Training and inference hardware have different sweet spots in terms of price and performance.

Exercises
1. Write C code to test whether there is any difference in speed between accessing memory aligned or misaligned relative to the external memory interface. Hint: be careful of caching effects.
2. Test the difference in speed between accessing memory in sequence or with a given stride.
3. How could you measure the cache sizes on a CPU?
4. How would you lay out data across multiple memory channels for maximum bandwidth? How would you lay it out if you had many small threads?
5. An enterprise-class HDD is spinning at 10,000 rpm. What is the absolutely minimum time an HDD needs to spend worst case before it can read data (you can assume that heads move almost instantaneously)? Why are 2.5” HDDs becoming popular for commercial servers (relative to 3.5” and 5.25” drives)?
6. Assume that an HDD manufacturer increases the storage density from 1 Tbit per square inch to 5 Tbit per square inch. How much information can you store on a ring on a 2.5” HDD? Is there a difference between the inner and outer tracks?
7. Going from 8 bit to 16 bit data types increases the amount of silicon approximately by four times. Why? Why might NVIDIA have added INT4 operations to their Turing GPUs?
8. How much faster is it to read forward through memory vs. reading backwards? Does this number differ between different computers and CPU vendors? Why? Write C code and experiment with it.

12.4. Hardware

543

9. Can you measure the cache size of your disk? What is it for a typical HDD? Do SSDs need a cache?
10. Measure the packet overhead when sending messages across the Ethernet. Look up the difference between UDP and TCP/IP connections.
11. Direct memory access allows devices other than the CPU to write (and read) directly to (from) memory. Why is this a good idea?
12. Look at the performance numbers for the Turing T4 GPU. Why does the performance “only” double as you go from FP16 to INT8 and INT4?
13. What is the shortest time it should take for a packet on a round trip between San Francisco and Amsterdam? Hint: you can assume that the distance is 10,000 km.
Discussions170

12.5 Training on Multiple GPUs
So far we discussed how to train models efficiently on CPUs and GPUs. We even showed how deep learning frameworks allow one to parallelize computation and communication automatically between them in Section 12.3. We also showed in Section 5.6 how to list all the available GPUs on a computer using the nvidia-smi command. What we did not discuss is how to actually parallelize deep learning training. Instead, we implied in passing that one would somehow split the data across multiple devices and make it work. The present section fills in the details and shows how to train a network in parallel when starting from scratch. Details on how to take advantage of functionality in high-level APIs is relegated to Section 12.6. We assume that you are familiar with minibatch stochastic gradient descent algorithms such as the ones described in Section 11.5.

12.5.1 Splitting the Problem
Let us start with a simple computer vision problem and a slightly archaic network, e.g., with multiple layers of convolutions, pooling, and possibly a few fully-connected layers in the end. That is, let us start with a network that looks quite similar to LeNet (LeCun et al., 1998) or AlexNet (Krizhevsky et al., 2012). Given multiple GPUs (2 if it is a desktop server, 4 on an AWS g4dn.12xlarge instance, 8 on a p3.16xlarge, or 16 on a p2.16xlarge), we want to partition training in a manner as to achieve good speedup while simultaneously benefitting from simple and reproducible design choices. Multiple GPUs, after all, increase both memory and computation ability. In a nutshell, we have the following choices, given a minibatch of training data that we want to classify.
First, we could partition the network across multiple GPUs. That is, each GPU takes as input the data flowing into a particular layer, processes data across a number of subsequent layers and then sends the data to the next GPU. This allows us to process data with larger networks when compared with what a single GPU could handle. Besides, memory footprint per GPU can be well controlled (it is a fraction of the total network footprint).
However, the interface between layers (and thus GPUs) requires tight synchronization. This can be tricky, in particular if the computational workloads are not properly matched between layers. The problem is exacerbated for large numbers of GPUs. The interface between layers also requires large amounts of data transfer, such as activations and gradients. This may overwhelm the bandwidth of the GPU buses. Moreover, compute-intensive, yet sequential operations are nontrivial to
170 https://discuss.d2l.ai/t/363

544

Chapter 12. Computational Performance

partition. See e.g., (Mirhoseini et al., 2017) for a best effort in this regard. It remains a difficult problem and it is unclear whether it is possible to achieve good (linear) scaling on nontrivial problems. We do not recommend it unless there is excellent framework or operating system support for chaining together multiple GPUs. Second, we could split the work layerwise. For instance, rather than computing 64 channels on a single GPU we could split up the problem across 4 GPUs, each of which generates data for 16 channels. Likewise, for a fully-connected layer we could split the number of output units. Fig. 12.5.1 (taken from (Krizhevsky et al., 2012)) illustrates this design, where this strategy was used to deal with GPUs that had a very small memory footprint (2 GB at the time). This allows for good scaling in terms of computation, provided that the number of channels (or units) is not too small. Besides, multiple GPUs can process increasingly larger networks since the available memory scales linearly.
Fig. 12.5.1: Model parallelism in the original AlexNet design due to limited GPU memory.
However, we need a very large number of synchronization or barrier operations since each layer depends on the results from all the other layers. Moreover, the amount of data that needs to be transferred is potentially even larger than when distributing layers across GPUs. Thus, we do not recommend this approach due to its bandwidth cost and complexity. Last, we could partition data across multiple GPUs. This way all GPUs perform the same type of work, albeit on different observations. Gradients are aggregated across GPUs after each minibatch of training data. This is the simplest approach and it can be applied in any situation. We only need to synchronize after each minibatch. That said, it is highly desirable to start exchanging gradients parameters already while others are still being computed. Moreover, larger numbers of GPUs lead to larger minibatch sizes, thus increasing training efficiency. However, adding more GPUs does not allow us to train larger models.

12.5. Training on Multiple GPUs

545

Fig. 12.5.2: Parallelization on multiple GPUs. From left to right: original problem, network partitioning, layerwise partitioning, data parallelism.
A comparison of different ways of parallelization on multiple GPUs is depicted in Fig. 12.5.2. By and large, data parallelism is the most convenient way to proceed, provided that we have access to GPUs with sufficiently large memory. See also (Li et al., 2014) for a detailed description of partitioning for distributed training. GPU memory used to be a problem in the early days of deep learning. By now this issue has been resolved for all but the most unusual cases. We focus on data parallelism in what follows.
12.5.2 Data Parallelism
Assume that there are k GPUs on a machine. Given the model to be trained, each GPU will maintain a complete set of model parameters independently though parameter values across the GPUs are identical and synchronized. As an example, Fig. 12.5.3 illustrates training with data parallelism when k = 2.

546

Chapter 12. Computational Performance

Fig. 12.5.3: Calculation of minibatch stochastic gradient descent using data parallelism on two GPUs.
In general, the training proceeds as follows:
• In any iteration of training, given a random minibatch, we split the examples in the batch into k portions and distribute them evenly across the GPUs.
• Each GPU calculates loss and gradient of the model parameters based on the minibatch subset it was assigned.
• The local gradients of each of the k GPUs are aggregated to obtain the current minibatch stochastic gradient.
• The aggregate gradient is re-distributed to each GPU.
• Each GPU uses this minibatch stochastic gradient to update the complete set of model parameters that it maintains.
Note that in practice we increase the minibatch size k-fold when training on k GPUs such that each GPU has the same amount of work to do as if we were training on a single GPU only. On a 16GPU server this can increase the minibatch size considerably and we may have to increase the learning rate accordingly. Also note that batch normalization in Section 7.5 needs to be adjusted, e.g., by keeping a separate batch normalization coefficient per GPU. In what follows we will use a toy network to illustrate multi-GPU training.
%matplotlib inline from mxnet import autograd, gluon, np, npx from d2l import mxnet as d2l
npx.set_np()

12.5. Training on Multiple GPUs

547

12.5.3 A Toy Network
We use LeNet as introduced in Section 6.6 (with slight modifications). We define it from scratch to illustrate parameter exchange and synchronization in detail.
# Initialize model parameters scale = 0.01 W1 = np.random.normal(scale=scale, size=(20, 1, 3, 3)) b1 = np.zeros(20) W2 = np.random.normal(scale=scale, size=(50, 20, 5, 5)) b2 = np.zeros(50) W3 = np.random.normal(scale=scale, size=(800, 128)) b3 = np.zeros(128) W4 = np.random.normal(scale=scale, size=(128, 10)) b4 = np.zeros(10) params = [W1, b1, W2, b2, W3, b3, W4, b4]
# Define the model def lenet(X, params):
h1_conv = npx.convolution(data=X, weight=params[0], bias=params[1], kernel=(3, 3), num_filter=20)
h1_activation = npx.relu(h1_conv) h1 = npx.pooling(data=h1_activation, pool_type='avg', kernel=(2, 2),
stride=(2, 2)) h2_conv = npx.convolution(data=h1, weight=params[2], bias=params[3],
kernel=(5, 5), num_filter=50) h2_activation = npx.relu(h2_conv) h2 = npx.pooling(data=h2_activation, pool_type='avg', kernel=(2, 2),
stride=(2, 2)) h2 = h2.reshape(h2.shape[0], -1) h3_linear = np.dot(h2, params[4]) + params[5] h3 = npx.relu(h3_linear) y_hat = np.dot(h3, params[6]) + params[7] return y_hat
# Cross-entropy loss function loss = gluon.loss.SoftmaxCrossEntropyLoss()
12.5.4 Data Synchronization
For efficient multi-GPU training we need two basic operations. First we need to have the ability to distribute a list of parameters to multiple devices and to attach gradients (get_params). Without parameters it is impossible to evaluate the network on a GPU. Second, we need the ability to sum parameters across multiple devices, i.e., we need an allreduce function.
def get_params(params, device): new_params = [p.copyto(device) for p in params] for p in new_params: p.attach_grad() return new_params
Let us try it out by copying the model parameters to one GPU.

548

Chapter 12. Computational Performance

new_params = get_params(params, d2l.try_gpu(0)) print('b1 weight:', new_params[1]) print('b1 grad:', new_params[1].grad)
b1 weight: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] @gpu(0) b1 grad: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] @gpu(0)
Since we did not perform any computation yet, the gradient with regard to the bias parameter is still zero. Now let us assume that we have a vector distributed across multiple GPUs. The following allreduce function adds up all vectors and broadcasts the result back to all GPUs. Note that for this to work we need to copy the data to the device accumulating the results.
def allreduce(data): for i in range(1, len(data)): data[0][:] += data[i].copyto(data[0].ctx) for i in range(1, len(data)): data[0].copyto(data[i])
Let us test this by creating vectors with different values on different devices and aggregate them.
data = [np.ones((1, 2), ctx=d2l.try_gpu(i)) * (i + 1) for i in range(2)] print('before allreduce:\n', data[0], '\n', data[1]) allreduce(data) print('after allreduce:\n', data[0], '\n', data[1])
before allreduce: [[1. 1.]] @gpu(0) [[2. 2.]] @gpu(1) after allreduce: [[3. 3.]] @gpu(0) [[3. 3.]] @gpu(1)

12.5.5 Distributing Data

We need a simple utility function to distribute a minibatch evenly across multiple GPUs. For instance, on two GPUs we would like to have half of the data to be copied to either of the GPUs. Since it is more convenient and more concise, we use the built-in function from the deep learning framework to try it out on a 4 × 5 matrix.

data = np.arange(20).reshape(4, 5) devices = [npx.gpu(0), npx.gpu(1)] split = gluon.utils.split_and_load(data, print('input :', data) print('load into', devices) print('output:', split)

devices)

input : [[ 0. 1. 2. 3. 4.] [ 5. 6. 7. 8. 9.] [10. 11. 12. 13. 14.]

(continues on next page)

12.5. Training on Multiple GPUs

549

(continued from previous page)
[15. 16. 17. 18. 19.]] load into [gpu(0), gpu(1)] output: [array([[0., 1., 2., 3., 4.],
[5., 6., 7., 8., 9.]], ctx=gpu(0)), array([[10., 11., 12., 13., 14.], [15., 16., 17., 18., 19.]], ctx=gpu(1))]
For later reuse we define a split_batch function that splits both data and labels.
#@save def split_batch(X, y, devices):
"""Split `X` and `y` into multiple devices.""" assert X.shape[0] == y.shape[0] return (gluon.utils.split_and_load(X, devices),
gluon.utils.split_and_load(y, devices))

12.5.6 Training
Now we can implement multi-GPU training on a single minibatch. Its implementation is primarily based on the data parallelism approach described in this section. We will use the auxiliary functions we just discussed, allreduce and split_and_load, to synchronize the data among multiple GPUs. Note that we do not need to write any specific code to achieve parallelism. Since the computational graph does not have any dependencies across devices within a minibatch, it is executed in parallel automatically.
def train_batch(X, y, device_params, devices, lr): X_shards, y_shards = split_batch(X, y, devices) with autograd.record(): # Loss is calculated separately on each GPU ls = [ loss(lenet(X_shard, device_W), y_shard) for X_shard, y_shard, device_W in zip( X_shards, y_shards, device_params)] for l in ls: # Backpropagation is performed separately on each GPU l.backward() # Sum all gradients from each GPU and broadcast them to all GPUs for i in range(len(device_params[0])): allreduce([device_params[c][i].grad for c in range(len(devices))]) # The model parameters are updated separately on each GPU for param in device_params: d2l.sgd(param, lr, X.shape[0]) # Here, we use a full-size batch

Now, we can define the training function. It is slightly different from the ones used in the previous chapters: we need to allocate the GPUs and copy all the model parameters to all the devices. Obviously each batch is processed using the train_batch function to deal with multiple GPUs. For convenience (and conciseness of code) we compute the accuracy on a single GPU, though this is inefficient since the other GPUs are idle.

def train(num_gpus, batch_size, lr): train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size) devices = [d2l.try_gpu(i) for i in range(num_gpus)] # Copy model parameters to `num_gpus` GPUs device_params = [get_params(params, d) for d in devices]

(continues on next page)

550

Chapter 12. Computational Performance

(continued from previous page)
num_epochs = 10 animator = d2l.Animator('epoch', 'test acc', xlim=[1, num_epochs]) timer = d2l.Timer() for epoch in range(num_epochs):
timer.start() for X, y in train_iter:
# Perform multi-GPU training for a single minibatch train_batch(X, y, device_params, devices, lr) npx.waitall() timer.stop() # Evaluate the model on GPU 0 animator.add(epoch + 1, (d2l.evaluate_accuracy_gpu( lambda x: lenet(x, device_params[0]), test_iter, devices[0]),)) print(f'test acc: {animator.Y[0][-1]:.2f}, {timer.avg():.1f} sec/epoch ' f'on {str(devices)}')
Let us see how well this works on a single GPU. We first use a batch size of 256 and a learning rate of 0.2.
train(num_gpus=1, batch_size=256, lr=0.2)
test acc: 0.82, 2.6 sec/epoch on [gpu(0)]

By keeping the batch size and learning rate unchanged and increasing the number of GPUs to 2, we can see that the test accuracy roughly stays the same compared with the previous experiment. In terms of the optimization algorithms, they are identical. Unfortunately there is no meaningful speedup to be gained here: the model is simply too small; moreover we only have a small dataset, where our slightly unsophisticated approach to implementing multi-GPU training suffered from significant Python overhead. We will encounter more complex models and more sophisticated ways of parallelization going forward. Let us see what happens nonetheless for Fashion-MNIST.
train(num_gpus=2, batch_size=256, lr=0.2)
test acc: 0.85, 5.2 sec/epoch on [gpu(0), gpu(1)]

12.5. Training on Multiple GPUs

551

Summary
• There are multiple ways to split deep network training over multiple GPUs. We could split them between layers, across layers, or across data. The former two require tightly choreographed data transfers. Data parallelism is the simplest strategy.
• Data parallel training is straightforward. However, it increases the effective minibatch size to be efficient.
• In data parallelism, data are split across multiple GPUs, where each GPU executes its own forward and backward operation and subsequently gradients are aggregated and results are broadcast back to the GPUs.
• We may use slightly increased learning rates for larger minibatches.
Exercises
1. When training on k GPUs, change the minibatch size from b to k · b, i.e., scale it up by the number of GPUs.
2. Compare accuracy for different learning rates. How does it scale with the number of GPUs? 3. Implement a more efficient allreduce function that aggregates different parameters on dif-
ferent GPUs? Why is it more efficient? 4. Implement multi-GPU test accuracy computation. Discussions171
171 https://discuss.d2l.ai/t/364

552

Chapter 12. Computational Performance

12.6 Concise Implementation for Multiple GPUs
Implementing parallelism from scratch for every new model is no fun. Moreover, there is significant benefit in optimizing synchronization tools for high performance. In the following we will show how to do this using high-level APIs of deep learning frameworks. The math and the algorithms are the same as in Section 12.5. Quite unsurprisingly you will need at least two GPUs to run code of this section.
from mxnet import autograd, gluon, init, np, npx from mxnet.gluon import nn from d2l import mxnet as d2l
npx.set_np()
12.6.1 A Toy Network
Let us use a slightly more meaningful network than LeNet from Section 12.5 that is still sufficiently easy and quick to train. We pick a ResNet-18 variant (He et al., 2016a). Since the input images are tiny we modify it slightly. In particular, the difference from Section 7.6 is that we use a smaller convolution kernel, stride, and padding at the beginning. Moreover, we remove the maximum pooling layer.
#@save def resnet18(num_classes):
"""A slightly modified ResNet-18 model.""" def resnet_block(num_channels, num_residuals, first_block=False):
blk = nn.Sequential() for i in range(num_residuals):
if i == 0 and not first_block: blk.add( d2l.Residual(num_channels, use_1x1conv=True, strides=2))
else: blk.add(d2l.Residual(num_channels))
return blk
net = nn.Sequential() # This model uses a smaller convolution kernel, stride, and padding and # removes the maximum pooling layer net.add(nn.Conv2D(64, kernel_size=3, strides=1, padding=1),
nn.BatchNorm(), nn.Activation('relu')) net.add(resnet_block(64, 2, first_block=True), resnet_block(128, 2),
resnet_block(256, 2), resnet_block(512, 2)) net.add(nn.GlobalAvgPool2D(), nn.Dense(num_classes)) return net

12.6. Concise Implementation for Multiple GPUs

553

12.6.2 Network Initialization
The initialize function allows us to initialize parameters on a device of our choice. For a refresher on initialization methods see Section 4.8. What is particularly convenient is that it also allows us to initialize the network on multiple devices simultaneously. Let us try how this works in practice.
net = resnet18(10) # Get a list of GPUs devices = d2l.try_all_gpus() # Initialize all the parameters of the network net.initialize(init=init.Normal(sigma=0.01), ctx=devices)
Using the split_and_load function introduced in Section 12.5 we can divide a minibatch of data and copy portions to the list of devices provided by the devices variable. The network instance automatically uses the appropriate GPU to compute the value of the forward propagation. Here we generate 4 observations and split them over the GPUs.
x = np.random.uniform(size=(4, 1, 28, 28)) x_shards = gluon.utils.split_and_load(x, devices) net(x_shards[0]), net(x_shards[1])

(array([[ 2.2610243e-06, 5.1373199e-06, -2.8279187e-06,
[ 2.0698681e-06, 5.5506434e-06, -3.7365003e-06,
array([[ 2.4629790e-06, 5.6387898e-06, -3.1957322e-06,
[ 1.9431698e-06, 5.4830934e-06, -3.2530236e-06,

2.2045997e-06, -5.4046786e-06, -3.8297999e-06, 1.4339082e-07, -3.9651113e-06], 2.0084669e-06, -5.6382505e-06, -4.1065464e-06, 6.0830314e-07, -4.1891635e-06]], ctx=gpu(0)), 2.6015518e-06, -5.4362627e-06, -4.1360122e-06, 3.5758876e-07, -4.2976312e-06], 2.2600404e-06, -5.2698206e-06, -3.9678885e-06, 7.5751814e-08, -4.0943942e-06]], ctx=gpu(1)))

1.2869945e-06, 5.4683437e-06,
1.0498486e-06, 5.4521765e-06,
1.2938217e-06, 5.5125238e-06,
1.4807433e-06, 5.6764379e-06,

Once data pass through the network, the corresponding parameters are initialized on the device the data passed through. This means that initialization happens on a per-device basis. Since we picked GPU 0 and GPU 1 for initialization, the network is initialized only there, and not on the CPU. In fact, the parameters do not even exist on the CPU. We can verify this by printing out the parameters and observing any errors that might arise.
weight = net[0].params.get('weight')
try: weight.data()
except RuntimeError: print('not initialized on cpu')
weight.data(devices[0])[0], weight.data(devices[1])[0]

not initialized on cpu

554

Chapter 12. Computational Performance

(array([[[ 0.01382882, [-0.00319718, [-0.00835081,
array([[[ 0.01382882, [-0.00319718, [-0.00835081,

-0.01183044, 0.00439528, 0.01387452, -0.01183044, 0.00439528, 0.01387452,

0.01417866], 0.02562625], -0.01035946]]], 0.01417866], 0.02562625], -0.01035946]]],

ctx=gpu(0)), ctx=gpu(1)))

Next, let us replace the code to evaluate the accuracy by one that works in parallel across multiple devices. This serves as a replacement of the evaluate_accuracy_gpu function from Section 6.6. The main difference is that we split a minibatch before invoking the network. All else is essentially identical.
#@save def evaluate_accuracy_gpus(net, data_iter, split_f=d2l.split_batch):
"""Compute the accuracy for a model on a dataset using multiple GPUs.""" # Query the list of devices devices = list(net.collect_params().values())[0].list_ctx() # No. of correct predictions, no. of predictions metric = d2l.Accumulator(2) for features, labels in data_iter:
X_shards, y_shards = split_f(features, labels, devices) # Run in parallel pred_shards = [net(X_shard) for X_shard in X_shards] metric.add(
sum( float(d2l.accuracy(pred_shard, y_shard)) for pred_shard, y_shard in zip(pred_shards, y_shards)),
labels.size) return metric[0] / metric[1]

12.6.3 Training
As before, the training code needs to perform several basic functions for efficient parallelism:
• Network parameters need to be initialized across all devices.
• While iterating over the dataset minibatches are to be divided across all devices.
• We compute the loss and its gradient in parallel across devices.
• Gradients are aggregated and parameters are updated accordingly.
In the end we compute the accuracy (again in parallel) to report the final performance of the network. The training routine is quite similar to implementations in previous chapters, except that we need to split and aggregate data.
def train(num_gpus, batch_size, lr): train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size) ctx = [d2l.try_gpu(i) for i in range(num_gpus)] net.initialize(init=init.Normal(sigma=0.01), ctx=ctx, force_reinit=True) trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': lr}) loss = gluon.loss.SoftmaxCrossEntropyLoss() timer, num_epochs = d2l.Timer(), 10 (continues on next page)

12.6. Concise Implementation for Multiple GPUs

555

(continued from previous page)
animator = d2l.Animator('epoch', 'test acc', xlim=[1, num_epochs]) for epoch in range(num_epochs):
timer.start() for features, labels in train_iter:
X_shards, y_shards = d2l.split_batch(features, labels, ctx) with autograd.record():
ls = [ loss(net(X_shard), y_shard) for X_shard, y_shard in zip(X_shards, y_shards)]
for l in ls: l.backward()
trainer.step(batch_size) npx.waitall() timer.stop() animator.add(epoch + 1, (evaluate_accuracy_gpus(net, test_iter),)) print(f'test acc: {animator.Y[0][-1]:.2f}, {timer.avg():.1f} sec/epoch '
f'on {str(ctx)}')
Let us see how this works in practice. As a warm-up we train the network on a single GPU.
train(num_gpus=1, batch_size=256, lr=0.1)
test acc: 0.93, 13.4 sec/epoch on [gpu(0)]

Next we use 2 GPUs for training. Compared with LeNet evaluated in Section 12.5, the model for ResNet-18 is considerably more complex. This is where parallelization shows its advantage. The time for computation is meaningfully larger than the time for synchronizing parameters. This improves scalability since the overhead for parallelization is less relevant.
train(num_gpus=2, batch_size=512, lr=0.2)
test acc: 0.91, 8.8 sec/epoch on [gpu(0), gpu(1)]

556

Chapter 12. Computational Performance

Summary
• Gluon provides primitives for model initialization across multiple devices by providing a context list.
• Data are automatically evaluated on the devices where the data can be found. • Take care to initialize the networks on each device before trying to access the parameters on
that device. Otherwise you will encounter an error. • The optimization algorithms automatically aggregate over multiple GPUs.
Exercises
1. This section uses ResNet-18. Try different epochs, batch sizes, and learning rates. Use more GPUs for computation. What happens if you try this with 16 GPUs (e.g., on an AWS p2.16xlarge instance)?
2. Sometimes, different devices provide different computing power. We could use the GPUs and the CPU at the same time. How should we divide the work? Is it worth the effort? Why? Why not?
3. What happens if we drop npx.waitall()? How would you modify training such that you have an overlap of up to two steps for parallelism?
Discussions172
172 https://discuss.d2l.ai/t/365

12.6. Concise Implementation for Multiple GPUs

557

12.7 Parameter Servers
As we move from a single GPU to multiple GPUs and then to multiple servers containing multiple GPUs, possibly all spread out across multiple racks and network switches, our algorithms for distributed and parallel training need to become much more sophisticated. Details matter since different interconnects have very different bandwidth (e.g., NVLink can offer up to 100 GB/s across 6 links in an appropriate setting, PCIe 4.0 (16-lane) offers 32 GB/s, while even high speed 100GbE Ethernet only amounts to 10 GB/s). At the same time it is unreasonable to expect that a statistical modeler be an expert in networking and systems.
The core idea of the parameter server was introduced in (Smola & Narayanamurthy, 2010) in the context of distributed latent variable models. A description of the push and pull semantics then followed in (Ahmed et al., 2012) and a description of the system and an open source library followed in (Li et al., 2014). In the following we will motivate the components needed for efficiency.
12.7.1 Data-Parallel Training
Let us review the data parallel training approach to distributed training. We will use this to the exclusion of all others in this section since it is significantly simpler to implement in practice. There are virtually no use cases (besides deep learning on graphs) where any other strategy for parallelism is preferred since GPUs have plenty of memory nowadays. Fig. 12.7.1 describes the variant of data parallelism that we implemented in Section 12.5. The key aspect in it is that the aggregation of gradients occurs on GPU 0 before the updated parameters are rebroadcast to all GPUs.

558

Chapter 12. Computational Performance

Fig. 12.7.1: Left: single GPU training. Right: a variant of multi-GPU training: (1) we compute loss and gradient, (2) all gradients are aggregated on one GPU, (3) parameter update happens and the parameters are re-distributed to all GPUs.
In retrospect, the decision to aggregate on GPU 0 seems rather ad-hoc. After all, we might just as well aggregate on the CPU. In fact, we could even decide to aggregate some of the parameters on one GPU and some others on another. Provided that the optimization algorithm supports this, there is no real reason for why we could not. For instance, if we have four parameter vectors with associated gradients g1, . . . , g4 we could aggregate the gradients on one GPU for each gi (i = 1, . . . , 4).
This reasoning seems arbitrary and frivolous. After all, the mathematics is the same throughout. However, we are dealing with real physical hardware where different buses have different bandwidth as discussed in Section 12.4. Consider a real 4-way GPU server as described in Fig. 12.7.2. If it is particularly well connected, it might have a 100 GbE network card. More typical numbers are in the 1–10 GbE range with an effective bandwidth of 100 MB/s to 1 GB/s. Since the CPUs have too few PCIe lanes to connect to all GPUs directly (e.g., consumer-grade Intel CPUs have 24 lanes) we need a multiplexer173. The bandwidth from the CPU on a 16x Gen3 link is 16 GB/s. This is also the speed at which each of the GPUs is connected to the switch. This means that it is more effective to communicate between the devices.
173 https://www.broadcom.com/products/pcie-switches-bridges/pcie-switches

12.7. Parameter Servers

559

Fig. 12.7.2: A 4-way GPU server.
For the sake of the argument let us assume that the gradients are of 160 MB. In this case it takes 30 ms to send the gradients from all 3 remaining GPUs to the fourth one (each transfer takes 10 ms = 160 MB / 16 GB/s). Adding another 30 ms to transmit the weight vectors back we arrive at a total of 60 ms. If we send all data to the CPU we incur a penalty of 40 ms since each of the four GPUs needs to send the data to the CPU, yielding a total of 80 ms. Lastly assume that we are able to split the gradients into 4 parts of 40 MB each. Now we can aggregate each of the parts on a different GPU simultaneously since the PCIe switch offers a full-bandwidth operation between all links. Instead of 30 ms this takes 7.5 ms, yielding a total of 15 ms for a synchronization operation. In short, depending on how we synchronize parameters the same operation can take anywhere from 15 ms to 80 ms. Fig. 12.7.3 depicts the different strategies for exchanging parameters.

Fig. 12.7.3: Parameter synchronization strategies.
Note that we have yet another tool at our disposal when it comes to improving performance: in a deep network it takes some time to compute all gradients from the top to the bottom. We can begin synchronizing gradients for some parameter groups even while we are still busy computing them for others. See e.g., (Sergeev & DelBalso, 2018) for details on how to do this in Horovod174.
174 https://github.com/horovod/horovod

560

Chapter 12. Computational Performance

12.7.2 Ring Synchronization
When it comes to synchronization on modern deep learning hardware we often encounter significantly bespoke network connectivity. For instance, the AWS p3.16xlarge and NVIDIA DGX-2 instances share the connectivity structure of Fig. 12.7.4. Each GPU connects to a host CPU via a PCIe link which operates at best at 16 GB/s. Additionally each GPU also has 6 NVLink connections, each of which is capable of transferring 300 Gbit/s bidirectionally. This amounts to around 18 GB/s per link per direction. In short, the aggregate NVLink bandwidth is significantly higher than the PCIe bandwidth. The question is how to use it most efficiently.

Fig. 12.7.4: NVLink connectivity on 8 V100 GPU servers (image courtesy of NVIDIA).
It turns out that the optimal synchronization strategy is to decompose the network into two rings and to use them to synchronize data directly (Wang et al., 2018). Fig. 12.7.5 illustrates that the network can be decomposed into one ring (1-2-3-4-5-6-7-8-1) with double NVLink bandwidth and into one (1-4-6-3-5-8-2-7-1) with regular bandwidth. Designing an efficient synchronization protocol in this case is nontrivial.

12.7. Parameter Servers

561

Fig. 12.7.5: Decomposition of the NVLink network into two rings.
Consider the following thought experiment: given a ring of n computing nodes (or GPUs) we can send gradients from the first to the second node. There it is added to the local gradient and sent on to the third node, and so on. After n − 1 steps the aggregate gradient can be found in the lastvisited node. That is, the time to aggregate gradients grows linearly with the number of nodes. But if we do this the algorithm is quite inefficient. After all, at any time there is only one of the nodes communicating. What if we broke the gradients into n chunks and started synchronizing chunk i starting at node i? Since each chunk is of size 1/n the total time is now (n − 1)/n ≈ 1. In other words, the time spent to aggregate gradients does not grow as we increase the size of the ring. This is quite an astonishing result. Fig. 12.7.6 illustrates the sequence of steps on n = 4 nodes.

562

Chapter 12. Computational Performance

Fig. 12.7.6: Ring synchronization across 4 nodes. Each node starts transmitting parts of gradients to its left neighbor until the assembled gradient can be found in its right neighbor.
If we use the same example of synchronizing 160 MB across 8 V100 GPUs we arrive at approximately 2 · 160MB/(3 · 18GB/s) ≈ 6ms. This is better than using the PCIe bus, even though we are now using 8 GPUs. Note that in practice these numbers are a bit worse, since deep learning frameworks often fail to assemble communication into large burst transfers.
Note that there is a common misconception that ring synchronization is fundamentally different from other synchronization algorithms. The only difference is that the synchronization path is somewhat more elaborate when compared with a simple tree.
12.7.3 Multi-Machine Training
Distributed training on multiple machines adds a further challenge: we need to communicate with servers that are only connected across a comparatively lower bandwidth fabric that can be over an order of magnitude slower in some cases. Synchronization across devices is tricky. After all, different machines running training code will have subtly different speed. Hence we need to synchronize them if we want to use synchronous distributed optimization. Fig. 12.7.7 illustrates how distributed parallel training occurs.
1. A (different) batch of data are read on each machine, split across multiple GPUs and transferred to GPU memory. There predictions and gradients are computed on each GPU batch separately.
2. The gradients from all local GPUs are aggregated on one GPU (or parts of it are aggregated over different GPUs).
3. The gradients are sent to the CPUs.
4. The CPUs send the gradients to a central parameter server which aggregates all the gradients.

12.7. Parameter Servers

563

5. The aggregate gradients are then used to update the parameters and the updated parameters are broadcast back to the individual CPUs.
6. The information is sent to one (or multiple) GPUs.
7. The updated parameters are spread across all GPUs.

Fig. 12.7.7: Multi-machine multi-GPU distributed parallel training.
Each of these operations seems rather straightforward. And, indeed, they can be carried out efficiently within a single machine. Once we look at multiple machines, though, we can see that the central parameter server becomes the bottleneck. After all, the bandwidth per server is limited, hence for m workers the time it takes to send all gradients to the server is O(m). We can break through this barrier by increasing the number of servers to n. At this point each server only needs to store O(1/n) of the parameters, hence the total time for updates and optimization becomes O(m/n). Matching both numbers yields constant scaling regardless of how many workers we are dealing with. In practice we use the same machines both as workers and as servers. Fig. 12.7.8 illustrates the design (see also (Li et al., 2014) for details). In particular, ensuring that multiple machines work without unreasonable delays is nontrivial. We omit details on barriers and will only briefly touch on synchronous and asynchronous updates below.

564

Chapter 12. Computational Performance

Fig. 12.7.8: Top: a single parameter server is a bottleneck since its bandwidth is finite. Bottom: multiple parameter servers store parts of the parameters with aggregate bandwidth.

12.7.4 Key–Value Stores

Implementing the steps required for distributed multi-GPU training in practice is nontrivial. This is why it pays to use a common abstraction, namely that of a key–value store with redefined update semantics.

Across many workers and many GPUs the computation for gradient i can be defined as

∑ gi =

∑ gij k ,

k∈workers j∈GPUs

(12.7.1)

where gijk is part of gradient i split on GPU j of worker k. The key aspect in this operation is that it is a commutative reduction, that is, it turns many vectors into one and the order in which the operation is applied does not matter. This is great for our purposes since we do not (need to) have fine grained control over when which gradient is received. Besides, note that this operation is independent among different i.
This allows us to define the following two operations: push, which accumulates gradients, and pull, which retrieves aggregate gradients. Since we have many different sets of gradients (after all, we have many layers), we need to index the gradients with a key i. This similarity to key–value stores,

12.7. Parameter Servers

565

such as the one introduced in Dynamo (DeCandia et al., 2007) is not by coincidence. They, too, satisfy many similar characteristics, in particular when it comes to distributing the parameters across multiple servers.
The push and pull operations for key-value stores are described as follows:
• push(key, value) sends a particular gradient (the value) from a worker to a common storage. There the value is aggregated, e.g., by summing it up.
• pull(key, value) retrieves an aggregate value from common storage, e.g., after combining the gradients from all workers.
By hiding all the complexity about synchronization behind a simple push and pull operation we can decouple the concerns of statistical modelers who want to be able to express optimization in simple terms and the system engineers who need to deal with the complexity inherent in distributed synchronization.
Summary
• Synchronization needs to be highly adaptive to specific network infrastructure and connectivity within a server. This can make a significant difference to the time it takes to synchronize.
• Ring-synchronization can be optimal for p3 and DGX-2 servers. For others possibly not so much.
• A hierarchical synchronization strategy works well when adding multiple parameter servers for increased bandwidth.
Exercises
1. Can you increase the ring synchronization even further? Hint: you can send messages in both directions.
2. Is it possible to allow asynchronous communication (while computation is still ongoing)? How does it affect performance?
3. What if we lost a server during a long-running computation? How can we design a fault tolerance mechanism to avoid restarting the computation fully?
Discussions175

175 https://discuss.d2l.ai/t/366
566

Chapter 12. Computational Performance

13 | Computer Vision

Whether it is medical diagnosis, self-driving vehicles, camera monitoring, or smart filters, many applications in the field of computer vision are closely related to our current and future lives. In recent years, deep learning has been the transformative power for advancing the performance of computer vision systems. It can be said that the most advanced computer vision applications are almost inseparable from deep learning. In view of this, this chapter will focus on the field of computer vision, and investigate methods and applications that have recently been influential in academia and industry.
In Chapter 6 and Chapter 7, we studied various convolutional neural networks that are commonly used in computer vision, and applied them to simple image classification tasks. At the beginning of this chapter, we will describe two methods that may improve model generalization, namely image augmentation and fine-tuning, and apply them to image classification. Since deep neural networks can effectively represent images in multiple levels, such layerwise representations have been successfully used in various computer vision tasks such as object detection, semantic segmentation, and style transfer. Following the key idea of leveraging layerwise representations in computer vision, we will begin with major components and techniques for object detection. Next, we will show how to use fully convolutional networks for semantic segmentation of images. Then we will explain how to use style transfer techniques to generate images like the cover of this book. In the end, we conclude this chapter by applying the materials of this chapter and several previous chapters on two popular computer vision benchmark datasets.

13.1 Image Augmentation

In Section 7.1, we mentioned that large datasets are a prerequisite for the success of deep neural networks in various applications. Image augmentation generates similar but distinct training examples after a series of random changes to the training images, thereby expanding the size of the training set. Alternatively, image augmentation can be motivated by the fact that random tweaks of training examples allow models to less rely on certain attributes, thereby improving their generalization ability. For example, we can crop an image in different ways to make the object of interest appear in different positions, thereby reducing the dependence of a model on the position of the object. We can also adjust factors such as brightness and color to reduce a modelʼs sensitivity to color. It is probably true that image augmentation was indispensable for the success of AlexNet at that time. In this section we will discuss this widely used technique in computer vision.

%matplotlib inline from mxnet import autograd, gluon, image, init, np, npx from mxnet.gluon import nn

(continues on next page)

567

from d2l import mxnet as d2l npx.set_np()

(continued from previous page)

13.1.1 Common Image Augmentation Methods
In our investigation of common image augmentation methods, we will use the following 400 × 500 image an example.
d2l.set_figsize() img = image.imread('../img/cat1.jpg') d2l.plt.imshow(img.asnumpy());

Most image augmentation methods have a certain degree of randomness. To make it easier for us to observe the effect of image augmentation, next we define an auxiliary function apply. This function runs the image augmentation method aug multiple times on the input image img and shows all the results.
def apply(img, aug, num_rows=2, num_cols=4, scale=1.5): Y = [aug(img) for _ in range(num_rows * num_cols)] d2l.show_images(Y, num_rows, num_cols, scale=scale)
Flipping and Cropping
Flipping the image left and right usually does not change the category of the object. This is one of the earliest and most widely used methods of image augmentation. Next, we use the transforms module to create the RandomFlipLeftRight instance, which flips an image left and right with a 50% chance.
apply(img, gluon.data.vision.transforms.RandomFlipLeftRight())

568

Chapter 13. Computer Vision

Flipping up and down is not as common as flipping left and right. But at least for this example image, flipping up and down does not hinder recognition. Next, we create a RandomFlipTopBottom instance to flip an image up and down with a 50% chance.
apply(img, gluon.data.vision.transforms.RandomFlipTopBottom())

In the example image we used, the cat is in the middle of the image, but this may not be the case in general. In Section 6.5, we explained that the pooling layer can reduce the sensitivity of a convolutional layer to the target position. In addition, we can also randomly crop the image to make objects appear in different positions in the image at different scales, which can also reduce the sensitivity of a model to the target position.
In the code below, we randomly crop an area with an area of 10% ∼ 100% of the original area each time, and the ratio of width to height of this area is randomly selected from 0.5 ∼ 2. Then, the width and height of the region are both scaled to 200 pixels. Unless otherwise specified, the random number between a and b in this section refers to a continuous value obtained by random and uniform sampling from the interval [a, b].
shape_aug = gluon.data.vision.transforms.RandomResizedCrop( (200, 200), scale=(0.1, 1), ratio=(0.5, 2))
apply(img, shape_aug)

13.1. Image Augmentation

569

Changing Colors
Another augmentation method is changing colors. We can change four aspects of the image color: brightness, contrast, saturation, and hue. In the example below, we randomly change the brightness of the image to a value between 50% (1 − 0.5) and 150% (1 + 0.5) of the original image.
apply(img, gluon.data.vision.transforms.RandomBrightness(0.5))

Similarly, we can randomly change the hue of the image. apply(img, gluon.data.vision.transforms.RandomHue(0.5))

570

Chapter 13. Computer Vision

We can also create a RandomColorJitter instance and set how to randomly change the brightness, contrast, saturation, and hue of the image at the same time.
color_aug = gluon.data.vision.transforms.RandomColorJitter( brightness=0.5, contrast=0.5, saturation=0.5, hue=0.5)
apply(img, color_aug)

Combining Multiple Image Augmentation Methods
In practice, we will combine multiple image augmentation methods. For example, we can combine the different image augmentation methods defined above and apply them to each image via a Compose instance.
augs = gluon.data.vision.transforms.Compose([ gluon.data.vision.transforms.RandomFlipLeftRight(), color_aug, shape_aug])
apply(img, augs)

13.1. Image Augmentation

571

13.1.2 Training with Image Augmentation
Let us train a model with image augmentation. Here we use the CIFAR-10 dataset instead of the Fashion-MNIST dataset that we used before. This is because the position and size of the objects in the Fashion-MNIST dataset have been normalized, while the color and size of the objects in the CIFAR-10 dataset have more significant differences. The first 32 training images in the CIFAR-10 dataset are shown below.
d2l.show_images( gluon.data.vision.CIFAR10(train=True)[0:32][0], 4, 8, scale=0.8);

In order to obtain definitive results during prediction, we usually only apply image augmentation to training examples, and do not use image augmentation with random operations during prediction. Here we only use the simplest random left-right flipping method. In addition, we use a ToTensor instance to convert a minibatch of images into the format required by the deep learning framework, i.e., 32-bit floating point numbers between 0 and 1 with the shape of (batch size, number of channels, height, width).

572

Chapter 13. Computer Vision

train_augs = gluon.data.vision.transforms.Compose([ gluon.data.vision.transforms.RandomFlipLeftRight(), gluon.data.vision.transforms.ToTensor()])
test_augs = gluon.data.vision.transforms.Compose([ gluon.data.vision.transforms.ToTensor()])
Next, we define an auxiliary function to facilitate reading the image and applying image augmentation. The transform_first function provided by Gluonʼs datasets applies image augmentation to the first element of each training example (image and label), i.e., the image. For a detailed introduction to DataLoader, please refer to Section 3.5.
def load_cifar10(is_train, augs, batch_size): return gluon.data.DataLoader( gluon.data.vision.CIFAR10(train=is_train).transform_first(augs), batch_size=batch_size, shuffle=is_train, num_workers=d2l.get_dataloader_workers())

Multi-GPU Training
We train the ResNet-18 model from Section 7.6 on the CIFAR-10 dataset. Recall the introduction to multi-GPU training in Section 12.6. In the following, we define a function to train and evaluate the model using multiple GPUs.
#@save def train_batch_ch13(net, features, labels, loss, trainer, devices,
split_f=d2l.split_batch): """Train for a minibatch with mutiple GPUs (defined in Chapter 13).""" X_shards, y_shards = split_f(features, labels, devices) with autograd.record():
pred_shards = [net(X_shard) for X_shard in X_shards] ls = [
loss(pred_shard, y_shard) for pred_shard, y_shard in zip(pred_shards, y_shards)] for l in ls: l.backward() # The `True` flag allows parameters with stale gradients, which is useful # later (e.g., in fine-tuning BERT) trainer.step(labels.shape[0], ignore_stale_grad=True) train_loss_sum = sum([float(l.sum()) for l in ls]) train_acc_sum = sum( d2l.accuracy(pred_shard, y_shard) for pred_shard, y_shard in zip(pred_shards, y_shards)) return train_loss_sum, train_acc_sum

#@save def train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs,
devices=d2l.try_all_gpus(), split_f=d2l.split_batch): """Train a model with mutiple GPUs (defined in Chapter 13).""" timer, num_batches = d2l.Timer(), len(train_iter) animator = d2l.Animator(xlabel='epoch', xlim=[1, num_epochs], ylim=[0, 1],
legend=['train loss', 'train acc', 'test acc'])
(continues on next page)

13.1. Image Augmentation

573

(continued from previous page)
for epoch in range(num_epochs): # Sum of training loss, sum of training accuracy, no. of examples, # no. of predictions metric = d2l.Accumulator(4) for i, (features, labels) in enumerate(train_iter): timer.start() l, acc = train_batch_ch13(net, features, labels, loss, trainer, devices, split_f) metric.add(l, acc, labels.shape[0], labels.size) timer.stop() if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1: animator.add( epoch + (i + 1) / num_batches, (metric[0] / metric[2], metric[1] / metric[3], None)) test_acc = d2l.evaluate_accuracy_gpus(net, test_iter, split_f) animator.add(epoch + 1, (None, None, test_acc))
print(f'loss {metric[0] / metric[2]:.3f}, train acc ' f'{metric[1] / metric[3]:.3f}, test acc {test_acc:.3f}')
print(f'{metric[2] * num_epochs / timer.sum():.1f} examples/sec on ' f'{str(devices)}')
Now we can define the train_with_data_aug function to train the model with image augmentation. This function gets all available GPUs, uses Adam as the optimization algorithm, applies image augmentation to the training dataset, and finally calls the train_ch13 function just defined to train and evaluate the model.
batch_size, devices, net = 256, d2l.try_all_gpus(), d2l.resnet18(10) net.initialize(init=init.Xavier(), ctx=devices)
def train_with_data_aug(train_augs, test_augs, net, lr=0.001): train_iter = load_cifar10(True, train_augs, batch_size) test_iter = load_cifar10(False, test_augs, batch_size) loss = gluon.loss.SoftmaxCrossEntropyLoss() trainer = gluon.Trainer(net.collect_params(), 'adam', {'learning_rate': lr}) train_ch13(net, train_iter, test_iter, loss, trainer, 10, devices)
Let us train the model using image augmentation based on random left-right flipping.
train_with_data_aug(train_augs, test_augs, net)
loss 0.170, train acc 0.941, test acc 0.820 4724.0 examples/sec on [gpu(0), gpu(1)]

574

Chapter 13. Computer Vision

Summary
• Image augmentation generates random images based on existing training data to improve the generalization ability of models.
• In order to obtain definitive results during prediction, we usually only apply image augmentation to training examples, and do not use image augmentation with random operations during prediction.
• Deep learning frameworks provide many different image augmentation methods, which can be applied simultaneously.
Exercises
1. Train the model without using image augmentation: train_with_data_aug(test_augs, test_augs). Compare training and testing accuracy when using and not using image augmentation. Can this comparative experiment support the argument that image augmentation can mitigate overfitting? Why?
2. Combine multiple different image augmentation methods in model training on the CIFAR-10 dataset. Does it improve test accuracy?
3. Refer to the online documentation of the deep learning framework. What other image augmentation methods does it also provide?
Discussions176
176 https://discuss.d2l.ai/t/367

13.1. Image Augmentation

575

13.2 Fine-Tuning
In earlier chapters, we discussed how to train models on the Fashion-MNIST training dataset with only 60000 images. We also described ImageNet, the most widely used large-scale image dataset in academia, which has more than 10 million images and 1000 objects. However, the size of the dataset that we usually encounter is between those of the two datasets.
Suppose that we want to recognize different types of chairs from images, and then recommend purchase links to users. One possible method is to first identify 100 common chairs, take 1000 images of different angles for each chair, and then train a classification model on the collected image dataset. Although this chair dataset may be larger than the Fashion-MNIST dataset, the number of examples is still less than one-tenth of that in ImageNet. This may lead to overfitting of complicated models that are suitable for ImageNet on this chair dataset. Besides, due to the limited amount of training examples, the accuracy of the trained model may not meet practical requirements.
In order to address the above problems, an obvious solution is to collect more data. However, collecting and labeling data can take a lot of time and money. For example, in order to collect the ImageNet dataset, researchers have spent millions of dollars from research funding. Although the current data collection cost has been significantly reduced, this cost still cannot be ignored.
Another solution is to apply transfer learning to transfer the knowledge learned from the source dataset to the target dataset. For example, although most of the images in the ImageNet dataset have nothing to do with chairs, the model trained on this dataset may extract more general image features, which can help identify edges, textures, shapes, and object composition. These similar features may also be effective for recognizing chairs.
13.2.1 Steps
In this section, we will introduce a common technique in transfer learning: fine-tuning. As shown in Fig. 13.2.1, fine-tuning consists of the following four steps:
1. Pretrain a neural network model, i.e., the source model, on a source dataset (e.g., the ImageNet dataset).
2. Create a new neural network model, i.e., the target model. This copies all model designs and their parameters on the source model except the output layer. We assume that these model parameters contain the knowledge learned from the source dataset and this knowledge will also be applicable to the target dataset. We also assume that the output layer of the source model is closely related to the labels of the source dataset; thus it is not used in the target model.
3. Add an output layer to the target model, whose number of outputs is the number of categories in the target dataset. Then randomly initialize the model parameters of this layer.
4. Train the target model on the target dataset, such as a chair dataset. The output layer will be trained from scratch, while the parameters of all the other layers are fine-tuned based on the parameters of the source model.

576

Chapter 13. Computer Vision

Fig. 13.2.1: Fine tuning.
When target datasets are much smaller than source datasets, fine-tuning helps to improve modelsʼ generalization ability.
13.2.2 Hot Dog Recognition
Let us demonstrate fine-tuning via a concrete case: hot dog recognition. We will fine-tune a ResNet model on a small dataset, which was pretrained on the ImageNet dataset. This small dataset consists of thousands of images with and without hot dogs. We will use the fine-tuned model to recognize hot dogs from images.
%matplotlib inline import os from mxnet import gluon, init, np, npx from mxnet.gluon import nn from d2l import mxnet as d2l
npx.set_np()

Reading the Dataset

The hot dog dataset we use was taken from online images. This dataset consists of 1400 positiveclass images containing hot dogs, and as many negative-class images containing other foods. 1000 images of both classes are used for training and the rest are for testing.
After unzipping the downloaded dataset, we obtain two folders hotdog/train and hotdog/test. Both folders have hotdog and not-hotdog subfolders, either of which contains images of the corresponding class.

#@save d2l.DATA_HUB['hotdog'] = (d2l.DATA_URL + 'hotdog.zip',

(continues on next page)

13.2. Fine-Tuning

577

(continued from previous page) 'fba480ffa8aa7e0febbb511d181409f899b9baa5')
data_dir = d2l.download_extract('hotdog')
Downloading ../data/hotdog.zip from http://d2l-data.s3-accelerate.amazonaws.com/hotdog.zip...
We create two instances to read all the image files in the training and testing datasets, respectively.
train_imgs = gluon.data.vision.ImageFolderDataset( os.path.join(data_dir, 'train'))
test_imgs = gluon.data.vision.ImageFolderDataset( os.path.join(data_dir, 'test'))
The first 8 positive examples and the last 8 negative images are shown below. As you can see, the images vary in size and aspect ratio.
hotdogs = [train_imgs[i][0] for i in range(8)] not_hotdogs = [train_imgs[-i - 1][0] for i in range(8)] d2l.show_images(hotdogs + not_hotdogs, 2, 8, scale=1.4);

During training, we first crop a random area of random size and random aspect ratio from the image, and then scale this area to a 224 × 224 input image. During testing, we scale both the height and width of an image to 256 pixels, and then crop a central 224 × 224 area as input. In addition, for the three RGB (red, green, and blue) color channels we standardize their values channel by channel. Concretely, the mean value of a channel is subtracted from each value of that channel and then the result is divided by the standard deviation of that channel.

# Specify the means and standard deviations of the three RGB channels to # standardize each channel normalize = gluon.data.vision.transforms.Normalize([0.485, 0.456, 0.406],
[0.229, 0.224, 0.225])

train_augs = gluon.data.vision.transforms.Compose([ gluon.data.vision.transforms.RandomResizedCrop(224), gluon.data.vision.transforms.RandomFlipLeftRight(), gluon.data.vision.transforms.ToTensor(), normalize])

test_augs = gluon.data.vision.transforms.Compose([ gluon.data.vision.transforms.Resize(256),

(continues on next page)

578

Chapter 13. Computer Vision

gluon.data.vision.transforms.CenterCrop(224), gluon.data.vision.transforms.ToTensor(), normalize])

(continued from previous page)

Defining and Initializing the Model
We use ResNet-18, which was pretrained on the ImageNet dataset, as the source model. Here, we specify pretrained=True to automatically download the pretrained model parameters. If this model is used for the first time, Internet connection is required for download.
pretrained_net = gluon.model_zoo.vision.resnet18_v2(pretrained=True)
The pretrained source model instance contains two member variables: features and output. The former contains all layers of the model except the output layer, and the latter is the output layer of the model. The main purpose of this division is to facilitate the fine-tuning of model parameters of all layers but the output layer. The member variable output of source model is shown below.
pretrained_net.output
Dense(512 -> 1000, linear)
As a fully-connected layer, it transforms ResNetʼs final global average pooling outputs into 1000 class outputs of the ImageNet dataset. We then construct a new neural network as the target model. It is defined in the same way as the pretrained source model except that its number of outputs in the final layer is set to the number of classes in the target dataset (rather than 1000).
In the following code, the model parameters in the member variable features of the target model instance finetune_net are initialized to the model parameters of the corresponding layer of the source model. Since the model parameters in the features are pre-trained on the ImageNet data set and are good enough, generally only a small learning rate is needed to fine-tune these parameters.
The model parameters in the member variable output are initialized randomly, and generally require a larger learning rate to train from scratch. Assuming that the learning rate in the Trainer instance is η, we set the learning rate of the model parameters in the member variable output to be 10η in the iteration.
In the code below, the model parameters before the output layer of the target model instance finetune_net are initialized to model parameters of the corresponding layers from the source model. Since these model parameters were obtained via pretraining on ImageNet, they are effective. Therefore, we can only use a small learning rate to fine-tune such pretrained parameters. In contrast, model parameters in the output layer are randomly initialized and generally require a larger learning rate to be learned from scratch. Let the base learning rate be η, a learning rate of 10η will be used to iterate the model parameters in the output layer.
finetune_net = gluon.model_zoo.vision.resnet18_v2(classes=2) finetune_net.features = pretrained_net.features finetune_net.output.initialize(init.Xavier()) # The model parameters in the output layer will be iterated using a learning # rate ten times greater finetune_net.output.collect_params().setattr('lr_mult', 10)

13.2. Fine-Tuning

579

Fine-Tuning the Model
First, we define a training function train_fine_tuning that uses fine-tuning so it can be called multiple times.
def train_fine_tuning(net, learning_rate, batch_size=128, num_epochs=5): train_iter = gluon.data.DataLoader(train_imgs.transform_first(train_augs), batch_size, shuffle=True) test_iter = gluon.data.DataLoader(test_imgs.transform_first(test_augs), batch_size) devices = d2l.try_all_gpus() net.collect_params().reset_ctx(devices) net.hybridize() loss = gluon.loss.SoftmaxCrossEntropyLoss() trainer = gluon.Trainer(net.collect_params(), 'sgd', { 'learning_rate': learning_rate, 'wd': 0.001}) d2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs, devices)
We set the base learning rate to a small value in order to fine-tune the model parameters obtained via pretraining. Based on the previous settings, we will train the output layer parameters of the target model from scratch using a learning rate ten times greater.
train_fine_tuning(finetune_net, 0.01)
loss 0.336, train acc 0.914, test acc 0.850 298.3 examples/sec on [gpu(0), gpu(1)]

For comparison, we define an identical model, but initialize all of its model parameters to random values. Since the entire model needs to be trained from scratch, we can use a larger learning rate.
scratch_net = gluon.model_zoo.vision.resnet18_v2(classes=2) scratch_net.initialize(init=init.Xavier()) train_fine_tuning(scratch_net, 0.1)

580

Chapter 13. Computer Vision

loss 0.394, train acc 0.832, test acc 0.873 153.8 examples/sec on [gpu(0), gpu(1)]

As we can see, the fine-tuned model tends to perform better for the same epoch because its initial parameter values are more effective.
Summary
• Transfer learning transfers knowledge learned from the source dataset to the target dataset. Fine-tuning is a common technique for transfer learning.
• The target model copies all model designs with their parameters from the source model except the output layer, and fine-tunes these parameters based on the target dataset. In contrast, the output layer of the target model needs to be trained from scratch.
• Generally, fine-tuning parameters uses a smaller learning rate, while training the output layer from scratch can use a larger learning rate.
Exercises
1. Keep increasing the learning rate of finetune_net. How does the accuracy of the model change?
2. Further adjust hyperparameters of finetune_net and scratch_net in the comparative experiment. Do they still differ in accuracy?
3. Set the parameters before the output layer of finetune_net to those of the source model and do not update them during training. How does the accuracy of the model change? You can use the following code.
finetune_net.features.collect_params().setattr('grad_req', 'null')
4. In fact, there is a “hotdog” class in the ImageNet dataset. Its corresponding weight parameter in the output layer can be obtained via the following code. How can we leverage this weight parameter?

13.2. Fine-Tuning

581

weight = pretrained_net.output.weight hotdog_w = np.split(weight.data(), 1000, axis=0)[713] hotdog_w.shape
(1, 512)
Discussions177
13.3 Object Detection and Bounding Boxes
In earlier sections (e.g., Section 7.1–Section 7.4), we introduced various models for image classification. In image classification tasks, we assume that there is only one major object in the image and we only focus on how to recognize its category. However, there are often multiple objects in the image of interest. We not only want to know their categories, but also their specific positions in the image. In computer vision, we refer to such tasks as object detection (or object recognition). Object detection has been widely applied in many fields. For example, self-driving needs to plan traveling routes by detecting the positions of vehicles, pedestrians, roads, and obstacles in the captured video images. Besides, robots may use this technique to detect and localize objects of interest throughout its navigation of an environment. Moreover, security systems may need to detect abnormal objects, such as intruders or bombs. In the next few sections, we will introduce several deep learning methods for object detection. We will begin with an introduction to positions (or locations) of objects.
%matplotlib inline from mxnet import image, np, npx from d2l import mxnet as d2l
npx.set_np()
We will load the sample image to be used in this section. We can see that there is a dog on the left side of the image and a cat on the right. They are the two major objects in this image.
d2l.set_figsize() img = image.imread('../img/catdog.jpg').asnumpy() d2l.plt.imshow(img);
177 https://discuss.d2l.ai/t/368

582

Chapter 13. Computer Vision

13.3.1 Bounding Boxes
In object detection, we usually use a bounding box to describe the spatial location of an object. The bounding box is rectangular, which is determined by the x and y coordinates of the upper-left corner of the rectangle and the such coordinates of the lower-right corner. Another commonly used bounding box representation is the (x, y)-axis coordinates of the bounding box center, and the width and height of the box.
Here we define functions to convert between these two representations: box_corner_to_center converts from the two-corner representation to the center-width-height presentation, and box_center_to_corner vice versa. The input argument boxes should be a two-dimensional tensor of shape (n, 4), where n is the number of bounding boxes.
#@save def box_corner_to_center(boxes):
"""Convert from (upper-left, lower-right) to (center, width, height).""" x1, y1, x2, y2 = boxes[:, 0], boxes[:, 1], boxes[:, 2], boxes[:, 3] cx = (x1 + x2) / 2 cy = (y1 + y2) / 2 w = x2 - x1 h = y2 - y1 boxes = np.stack((cx, cy, w, h), axis=-1) return boxes
#@save def box_center_to_corner(boxes):
"""Convert from (center, width, height) to (upper-left, lower-right).""" cx, cy, w, h = boxes[:, 0], boxes[:, 1], boxes[:, 2], boxes[:, 3] x1 = cx - 0.5 * w y1 = cy - 0.5 * h x2 = cx + 0.5 * w y2 = cy + 0.5 * h boxes = np.stack((x1, y1, x2, y2), axis=-1) return boxes
We will define the bounding boxes of the dog and the cat in the image based on the coordinate information. The origin of the coordinates in the image is the upper-left corner of the image, and to the right and down are the positive directions of the x and y axes, respectively.

13.3. Object Detection and Bounding Boxes

583

# Here `bbox` is the abbreviation for bounding box dog_bbox, cat_bbox = [60.0, 45.0, 378.0, 516.0], [400.0, 112.0, 655.0, 493.0]
We can verify the correctness of the two bounding box conversion functions by converting twice.
boxes = np.array((dog_bbox, cat_bbox)) box_center_to_corner(box_corner_to_center(boxes)) == boxes
array([[ True, True, True, True], [ True, True, True, True]])
Let us draw the bounding boxes in the image to check if they are accurate. Before drawing, we will define a helper function bbox_to_rect. It represents the bounding box in the bounding box format of the matplotlib package.
#@save def bbox_to_rect(bbox, color):
"""Convert bounding box to matplotlib format.""" # Convert the bounding box (upper-left x, upper-left y, lower-right x, # lower-right y) format to the matplotlib format: ((upper-left x, # upper-left y), width, height) return d2l.plt.Rectangle(xy=(bbox[0], bbox[1]), width=bbox[2] - bbox[0],
height=bbox[3] - bbox[1], fill=False, edgecolor=color, linewidth=2)
After adding the bounding boxes on the image, we can see that the main outline of the two objects are basically inside the two boxes.
fig = d2l.plt.imshow(img) fig.axes.add_patch(bbox_to_rect(dog_bbox, 'blue')) fig.axes.add_patch(bbox_to_rect(cat_bbox, 'red'));

584

Chapter 13. Computer Vision

Summary
• Object detection not only recognizes all the objects of interest in the image, but also their positions. The position is generally represented by a rectangular bounding box.
• We can convert between two commonly used bounding box representations.
Exercises
1. Find another image and try to label a bounding box that contains the object. Compare labeling bounding boxes and categories: which usually takes longer?
2. Why is the innermost dimension of the input argument boxes of box_corner_to_center and box_center_to_corner always 4?
Discussions178
13.4 Anchor Boxes
Object detection algorithms usually sample a large number of regions in the input image, determine whether these regions contain objects of interest, and adjust the boundaries of the regions so as to predict the ground-truth bounding boxes of the objects more accurately. Different models may adopt different region sampling schemes. Here we introduce one of such methods: it generates multiple bounding boxes with varying scales and aspect ratios centered on each pixel. These bounding boxes are called anchor boxes. We will design an object detection model based on anchor boxes in Section 13.7. First, let us modify the printing accuracy just for more concise outputs.
%matplotlib inline from mxnet import gluon, image, np, npx from d2l import mxnet as d2l
np.set_printoptions(2) # Simplify printing accuracy npx.set_np()
13.4.1 Generating Multiple Anchor Boxes
Suppose that the input image has a height of h and width of w. We generate anchor boxes with different shapes centered on each pixel of the image. Let the scale be s ∈ (0, 1] and the asp√ect ratio (ra√tio of width to height) is r > 0. Then the width and height of the anchor box are ws r and hs/ r, respectively. Note that when the center position is given, an anchor box with known width and height is determined. To generate multiple anchor boxes with different shapes, let us set a series of scales s1, . . . , sn and a series of aspect ratios r1, . . . , rm. When using all the combinations of these scales and aspect ratios with each pixel as the center, the input image will have a total of whnm anchor boxes. Although
178 https://discuss.d2l.ai/t/369

13.4. Anchor Boxes

585

these anchor boxes may cover all the ground-truth bounding boxes, the computational complexity is easily too high. In practice, we can only consider those combinations containing s1 or r1:

(s1, r1), (s1, r2), . . . , (s1, rm), (s2, r1), (s3, r1), . . . , (sn, r1).

(13.4.1)

That is to say, the number of anchor boxes centered on the same pixel is n + m − 1. For the entire input image, we will generate a total of wh(n + m − 1) anchor boxes.
The above method of generating anchor boxes is implemented in the following multibox_prior function. We specify the input image, a list of scales, and a list of aspect ratios, then this function will return all the anchor boxes.

#@save def multibox_prior(data, sizes, ratios):
"""Generate anchor boxes with different shapes centered on each pixel.""" in_height, in_width = data.shape[-2:] device, num_sizes, num_ratios = data.ctx, len(sizes), len(ratios) boxes_per_pixel = (num_sizes + num_ratios - 1) size_tensor = np.array(sizes, ctx=device) ratio_tensor = np.array(ratios, ctx=device) # Offsets are required to move the anchor to the center of a pixel. Since # a pixel has height=1 and width=1, we choose to offset our centers by 0.5 offset_h, offset_w = 0.5, 0.5 steps_h = 1.0 / in_height # Scaled steps in y-axis steps_w = 1.0 / in_width # Scaled steps in x-axis

# Generate all center points for the anchor boxes center_h = (np.arange(in_height, ctx=device) + offset_h) * steps_h center_w = (np.arange(in_width, ctx=device) + offset_w) * steps_w shift_x, shift_y = np.meshgrid(center_w, center_h) shift_x, shift_y = shift_x.reshape(-1), shift_y.reshape(-1)

# Generate `boxes_per_pixel` number of heights and widths that are later # used to create anchor box corner coordinates (xmin, xmax, ymin, ymax) w = np.concatenate((size_tensor * np.sqrt(ratio_tensor[0]),
sizes[0] * np.sqrt(ratio_tensor[1:]))) \ * in_height / in_width # Handle rectangular inputs h = np.concatenate((size_tensor / np.sqrt(ratio_tensor[0]), sizes[0] / np.sqrt(ratio_tensor[1:]))) # Divide by 2 to get half height and half width anchor_manipulations = np.tile( np.stack((-w, -h, w, h)).T, (in_height * in_width, 1)) / 2

# Each center point will have `boxes_per_pixel` number of anchor boxes, so # generate a grid of all anchor box centers with `boxes_per_pixel` repeats out_grid = np.stack([shift_x, shift_y, shift_x, shift_y],
axis=1).repeat(boxes_per_pixel, axis=0) output = out_grid + anchor_manipulations return np.expand_dims(output, axis=0)

We can see that the shape of the returned anchor box variable Y is (batch size, number of anchor boxes, 4).
img = image.imread('../img/catdog.jpg').asnumpy() h, w = img.shape[:2]

(continues on next page)

586

Chapter 13. Computer Vision

print(h, w) X = np.random.uniform(size=(1, 3, h, w)) # Construct input data Y = multibox_prior(X, sizes=[0.75, 0.5, 0.25], ratios=[1, 2, 0.5]) Y.shape

(continued from previous page)

561 728

(1, 2042040, 4)
After changing the shape of the anchor box variable Y to (image height, image width, number of anchor boxes centered on the same pixel, 4), we can obtain all the anchor boxes centered on a specified pixel position. In the following, we access the first anchor box centered on (250, 250). It has four elements: the (x, y)-axis coordinates at the upper-left corner and the (x, y)-axis coordinates at the lower-right corner of the anchor box. The coordinate values of both axes are divided by the width and height of the image, respectively; thus, the range is between 0 and 1.
boxes = Y.reshape(h, w, 5, 4) boxes[250, 250, 0, :]

array([0.06, 0.07, 0.63, 0.82])

In order to show all the anchor boxes centered on one pixel in the image, we define the following show_bboxes function to draw multiple bounding boxes on the image.
#@save def show_bboxes(axes, bboxes, labels=None, colors=None):
"""Show bounding boxes.""" def make_list(obj, default_values=None):
if obj is None: obj = default_values
elif not isinstance(obj, (list, tuple)): obj = [obj]
return obj
labels = make_list(labels) colors = make_list(colors, ['b', 'g', 'r', 'm', 'c']) for i, bbox in enumerate(bboxes):
color = colors[i % len(colors)] rect = d2l.bbox_to_rect(bbox.asnumpy(), color) axes.add_patch(rect) if labels and len(labels) > i:
text_color = 'k' if color == 'w' else 'w' axes.text(rect.xy[0], rect.xy[1], labels[i], va='center',
ha='center', fontsize=9, color=text_color, bbox=dict(facecolor=color, lw=0))
As we just saw, the coordinate values of the x and y axes in the variable boxes have been divided by the width and height of the image, respectively. When drawing anchor boxes, we need to restore their original coordinate values; thus, we define variable bbox_scale below. Now, we can draw all the anchor boxes centered on (250, 250) in the image. As you can see, the blue anchor box with a scale of 0.75 and an aspect ratio of 1 well surrounds the dog in the image.

13.4. Anchor Boxes

587

d2l.set_figsize() bbox_scale = np.array((w, h, w, h)) fig = d2l.plt.imshow(img) show_bboxes(fig.axes, boxes[250, 250, :, :] * bbox_scale, [
's=0.75, r=1', 's=0.5, r=1', 's=0.25, r=1', 's=0.75, r=2', ])

's=0.75,

r=0.5'

13.4.2 Intersection over Union (IoU)

We just mentioned that an anchor box “well” surrounds the dog in the image. If the ground-truth bounding box of the object is known, how can “well” here be quantified? Intuitively, we can measure the similarity between the anchor box and the ground-truth bounding box. We know that the Jaccard index can measure the similarity between two sets. Given sets A and B, their Jaccard index is the size of their intersection divided by the size of their union:

J(A, B) = |A ∩ B| . |A ∪ B|

(13.4.2)

In fact, we can consider the pixel area of any bounding box as a set of pixels. In this way, we can measure the similarity of the two bounding boxes by the Jaccard index of their pixel sets. For two bounding boxes, we usually refer their Jaccard index as intersection over union (IoU), which is the ratio of their intersection area to their union area, as shown in Fig. 13.4.1. The range of an IoU is between 0 and 1: 0 means that two bounding boxes do not overlap at all, while 1 indicates that the two bounding boxes are equal.

Fig. 13.4.1: IoU is the ratio of the intersection area to the union area of two bounding boxes.

588

Chapter 13. Computer Vision

For the remainder of this section, we will use IoU to measure the similarity between anchor boxes and ground-truth bounding boxes, and between different anchor boxes. Given two lists of anchor or bounding boxes, the following box_iou computes their pairwise IoU across these two lists.
#@save def box_iou(boxes1, boxes2):
"""Compute pairwise IoU across two lists of anchor or bounding boxes.""" box_area = lambda boxes: ((boxes[:, 2] - boxes[:, 0]) *
(boxes[:, 3] - boxes[:, 1])) # Shape of `boxes1`, `boxes2`, `areas1`, `areas2`: (no. of boxes1, 4), # (no. of boxes2, 4), (no. of boxes1,), (no. of boxes2,) areas1 = box_area(boxes1) areas2 = box_area(boxes2) # Shape of `inter_upperlefts`, `inter_lowerrights`, `inters`: (no. of # boxes1, no. of boxes2, 2) inter_upperlefts = np.maximum(boxes1[:, None, :2], boxes2[:, :2]) inter_lowerrights = np.minimum(boxes1[:, None, 2:], boxes2[:, 2:]) inters = (inter_lowerrights - inter_upperlefts).clip(min=0) # Shape of `inter_areas` and `union_areas`: (no. of boxes1, no. of boxes2) inter_areas = inters[:, :, 0] * inters[:, :, 1] union_areas = areas1[:, None] + areas2 - inter_areas return inter_areas / union_areas
13.4.3 Labeling Anchor Boxes in Training Data
In a training dataset, we consider each anchor box as a training example. In order to train an object detection model, we need class and offset labels for each anchor box, where the former is the class of the object relevant to the anchor box and the latter is the offset of the ground-truth bounding box relative to the anchor box. During the prediction, for each image we generate multiple anchor boxes, predict classes and offsets for all the anchor boxes, adjust their positions according to the predicted offsets to obtain the predicted bounding boxes, and finally only output those predicted bounding boxes that satisfy certain criteria.
As we know, an object detection training set comes with labels for locations of ground-truth bounding boxes and classes of their surrounded objects. To label any generated anchor box, we refer to the labeled location and class of its assigned ground-truth bounding box that is closest to the anchor box. In the following, we describe an algorithm for assigning closest ground-truth bounding boxes to anchor boxes.
Assigning Ground-Truth Bounding Boxes to Anchor Boxes
Given an image, suppose that the anchor boxes are A1, A2, . . . , Ana and the ground-truth bounding boxes are B1, B2, . . . , Bnb, where na ≥ nb. Let us define a matrix X ∈ Rna×nb, whose element xij in the ith row and jth column is the IoU of the anchor box Ai and the ground-truth bounding box Bj. The algorithm consists of the following steps:
1. Find the largest element in matrix X and denote its row and column indices as i1 and j1, respectively. Then the ground-truth bounding box Bj1 is assigned to the anchor box Ai1. This is quite intuitive because Ai1 and Bj1 are the closest among all the pairs of anchor boxes and ground-truth bounding boxes. After the first assignment, discard all the elements in the i1th row and the j1th column in matrix X.

13.4. Anchor Boxes

589

2. Find the largest of the remaining elements in matrix X and denote its row and column indices as i2 and j2, respectively. We assign ground-truth bounding box Bj2 to anchor box Ai2 and discard all the elements in the i2th row and the j2th column in matrix X.
3. At this point, elements in two rows and two columns in matrix X have been discarded. We proceed until all elements in nb columns in matrix X are discarded. At this time, we have assigned a ground-truth bounding box to each of nb anchor boxes.
4. Only traverse through the remaining na − nb anchor boxes. For example, given any anchor box Ai, find the ground-truth bounding box Bj with the largest IoU with Ai throughout the ith row of matrix X, and assign Bj to Ai only if this IoU is greater than a predefined threshold.
Let us illustrate the above algorithm using a concrete example. As shown in Fig. 13.4.2 (left), assuming that the maximum value in matrix X is x23, we assign the ground-truth bounding box B3 to the anchor box A2. Then, we discard all the elements in row 2 and column 3 of the matrix, find the largest x71 in the remaining elements (shaded area), and assign the ground-truth bounding box B1 to the anchor box A7. Next, as shown in Fig. 13.4.2 (middle), discard all the elements in row 7 and column 1 of the matrix, find the largest x54 in the remaining elements (shaded area), and assign the ground-truth bounding box B4 to the anchor box A5. Finally, as shown in Fig. 13.4.2 (right), discard all the elements in row 5 and column 4 of the matrix, find the largest x92 in the remaining elements (shaded area), and assign the ground-truth bounding box B2 to the anchor box A9. After that, we only need to traverse through the remaining anchor boxes A1, A3, A4, A6, A8 and determine whether to assign them ground-truth bounding boxes according to the threshold.

Fig. 13.4.2: Assigning ground-truth bounding boxes to anchor boxes.

This algorithm is implemented in the following assign_anchor_to_bbox function.
#@save def assign_anchor_to_bbox(ground_truth, anchors, device, iou_threshold=0.5):
"""Assign closest ground-truth bounding boxes to anchor boxes.""" num_anchors, num_gt_boxes = anchors.shape[0], ground_truth.shape[0] # Element x_ij in the i-th row and j-th column is the IoU of the anchor
(continues on next page)

590

Chapter 13. Computer Vision

(continued from previous page)
# box i and the ground-truth bounding box j jaccard = box_iou(anchors, ground_truth) # Initialize the tensor to hold the assigned ground-truth bounding box for # each anchor anchors_bbox_map = np.full((num_anchors,), -1, dtype=np.int32, ctx=device) # Assign ground-truth bounding boxes according to the threshold max_ious, indices = np.max(jaccard, axis=1), np.argmax(jaccard, axis=1) anc_i = np.nonzero(max_ious >= 0.5)[0] box_j = indices[max_ious >= 0.5] anchors_bbox_map[anc_i] = box_j col_discard = np.full((num_anchors,), -1) row_discard = np.full((num_gt_boxes,), -1) for _ in range(num_gt_boxes):
max_idx = np.argmax(jaccard) # Find the largest IoU box_idx = (max_idx % num_gt_boxes).astype('int32') anc_idx = (max_idx / num_gt_boxes).astype('int32') anchors_bbox_map[anc_idx] = box_idx jaccard[:, box_idx] = col_discard jaccard[anc_idx, :] = row_discard return anchors_bbox_map

Labeling Classes and Offsets

Now we can label the class and offset for each anchor box. Suppose that an anchor box A is as-

signed a ground-truth bounding box B. On one hand, the class of the anchor box A will be labeled

as that of B. On the other hand, the offset of the anchor box A will be labeled according to the rel-

ative position between the central coordinates of B and A together with the relative size between

these two boxes. Given varying positions and sizes of different boxes in the dataset, we can apply

transformations to those relative positions and sizes that may lead to more uniformly distributed

offsets that are easier to fit. Here we describe a common transformation. Given the central coor-

dinates of A and B as (xa, ya) and (xb, yb), their widths as wa and wb, and their heights as ha and

hb, respectively. We may label the offset of A as

(

)

xb−xa − µx

yb−ya h

− µy

log

wb w

− µw

log

hb h

− µh

wa

,a

,

a

,

a

,

σx

σy

σw

σh

(13.4.3)

where default values of the constants are µx = µy = µw = µh = 0, σx = σy = 0.1, and σw = σh = 0.2. This transformation is implemented below in the offset_boxes function.
#@save def offset_boxes(anchors, assigned_bb, eps=1e-6):
"""Transform for anchor box offsets.""" c_anc = d2l.box_corner_to_center(anchors) c_assigned_bb = d2l.box_corner_to_center(assigned_bb) offset_xy = 10 * (c_assigned_bb[:, :2] - c_anc[:, :2]) / c_anc[:, 2:] offset_wh = 5 * np.log(eps + c_assigned_bb[:, 2:] / c_anc[:, 2:]) offset = np.concatenate([offset_xy, offset_wh], axis=1) return offset

If an anchor box is not assigned a ground-truth bounding box, we just label the class of the anchor box as “background”. Anchor boxes whose classes are background are often referred to as negative

13.4. Anchor Boxes

591

anchor boxes, and the rest are called positive anchor boxes. We implement the following multibox_target function to label classes and offsets for anchor boxes (the anchors argument) using ground-truth bounding boxes (the labels argument). This function sets the background class to zero and increments the integer index of a new class by one.
#@save def multibox_target(anchors, labels):
"""Label anchor boxes using ground-truth bounding boxes.""" batch_size, anchors = labels.shape[0], anchors.squeeze(0) batch_offset, batch_mask, batch_class_labels = [], [], [] device, num_anchors = anchors.ctx, anchors.shape[0] for i in range(batch_size):
label = labels[i, :, :] anchors_bbox_map = assign_anchor_to_bbox(label[:, 1:], anchors,
device) bbox_mask = np.tile((np.expand_dims(
(anchors_bbox_map >= 0), axis=-1)), (1, 4)).astype('int32') # Initialize class labels and assigned bounding box coordinates with # zeros class_labels = np.zeros(num_anchors, dtype=np.int32, ctx=device) assigned_bb = np.zeros((num_anchors, 4), dtype=np.float32, ctx=device) # Label classes of anchor boxes using their assigned ground-truth # bounding boxes. If an anchor box is not assigned any, we label its # class as background (the value remains zero) indices_true = np.nonzero(anchors_bbox_map >= 0)[0] bb_idx = anchors_bbox_map[indices_true] class_labels[indices_true] = label[bb_idx, 0].astype('int32') + 1 assigned_bb[indices_true] = label[bb_idx, 1:] # Offset transformation offset = offset_boxes(anchors, assigned_bb) * bbox_mask batch_offset.append(offset.reshape(-1)) batch_mask.append(bbox_mask.reshape(-1)) batch_class_labels.append(class_labels) bbox_offset = np.stack(batch_offset) bbox_mask = np.stack(batch_mask) class_labels = np.stack(batch_class_labels) return (bbox_offset, bbox_mask, class_labels)

An Example

Let us illustrate anchor box labeling via a concrete example. We define ground-truth bounding boxes for the dog and cat in the loaded image, where the first element is the class (0 for dog and 1 for cat) and the remaining four elements are the (x, y)-axis coordinates at the upper-left corner and the lower-right corner (range is between 0 and 1). We also construct five anchor boxes to be labeled using the coordinates of the upper-left corner and the lower-right corner: A0, . . . , A4 (the index starts from 0). Then we plot these ground-truth bounding boxes and anchor boxes in the image.

ground_truth = np.array([[0, 0.1, 0.08, 0.52, 0.92], [1, 0.55, 0.2, 0.9, 0.88]])
anchors = np.array([[0, 0.1, 0.2, 0.3], [0.15, 0.2, 0.4, 0.4], [0.63, 0.05, 0.88, 0.98], [0.66, 0.45, 0.8, 0.8], [0.57, 0.3, 0.92, 0.9]])

(continues on next page)

592

Chapter 13. Computer Vision

(continued from previous page)
fig = d2l.plt.imshow(img) show_bboxes(fig.axes, ground_truth[:, 1:] * bbox_scale, ['dog', 'cat'], 'k') show_bboxes(fig.axes, anchors * bbox_scale, ['0', '1', '2', '3', '4']);

Using the multibox_target function defined above, we can label classes and offsets of these anchor boxes based on the ground-truth bounding boxes for the dog and cat. In this example, indices of the background, dog, and cat classes are 0, 1, and 2, respectively. Below we add an dimension for examples of anchor boxes and ground-truth bounding boxes.
labels = multibox_target(np.expand_dims(anchors, axis=0), np.expand_dims(ground_truth, axis=0))
There are three items in the returned result, all of which are in the tensor format. The third item contains the labeled classes of the input anchor boxes.
Let us analyze the returned class labels below based on anchor box and ground-truth bounding box positions in the image. First, among all the pairs of anchor boxes and ground-truth bounding boxes, the IoU of the anchor box A4 and the ground-truth bounding box of the cat is the largest. Thus, the class of A4 is labeled as the cat. Taking out pairs containing A4 or the ground-truth bounding box of the cat, among the rest the pair of the anchor box A1 and the ground-truth bounding box of the dog has the largest IoU. So the class of A1 is labeled as the dog. Next, we need to traverse through the remaining three unlabeled anchor boxes: A0, A2, and A3. For A0, the class of the ground-truth bounding box with the largest IoU is the dog, but the IoU is below the predefined threshold (0.5), so the class is labeled as background; for A2, the class of the ground-truth bounding box with the largest IoU is the cat and the IoU exceeds the threshold, so the class is labeled as the cat; for A3, the class of the ground-truth bounding box with the largest IoU is the cat, but the value is below the threshold, so the class is labeled as background.
labels[2]

array([[0, 1, 2, 0, 2]], dtype=int32)

The second returned item is a mask variable of the shape (batch size, four times the number of anchor boxes). Every four elements in the mask variable correspond to the four offset values of each anchor box. Since we do not care about background detection, offsets of this negative class

13.4. Anchor Boxes

593

should not affect the objective function. Through elementwise multiplications, zeros in the mask variable will filter out negative class offsets before calculating the objective function.
labels[1]
array([[0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1]], dtype=int32)
The first returned item contains the four offset values labeled for each anchor box. Note that the offsets of negative-class anchor boxes are labeled as zeros.
labels[0]
array([[-0.00e+00, -0.00e+00, -0.00e+00, -0.00e+00, 1.40e+00, 1.00e+01, 2.59e+00, 7.18e+00, -1.20e+00, 2.69e-01, 1.68e+00, -1.57e+00, -0.00e+00, -0.00e+00, -0.00e+00, -0.00e+00, -5.71e-01, -1.00e+00, 4.17e-06, 6.26e-01]])
13.4.4 Predicting Bounding Boxes with Non-Maximum Suppression
During prediction, we generate multiple anchor boxes for the image and predict classes and offsets for each of them. A predicted bounding box is thus obtained according to an anchor box with its predicted offset. Below we implement the offset_inverse function that takes in anchors and offset predictions as inputs and applies inverse offset transformations to return the predicted bounding box coordinates.
#@save def offset_inverse(anchors, offset_preds):
"""Predict bounding boxes based on anchor boxes with predicted offsets.""" anc = d2l.box_corner_to_center(anchors) pred_bbox_xy = (offset_preds[:, :2] * anc[:, 2:] / 10) + anc[:, :2] pred_bbox_wh = np.exp(offset_preds[:, 2:] / 5) * anc[:, 2:] pred_bbox = np.concatenate((pred_bbox_xy, pred_bbox_wh), axis=1) predicted_bbox = d2l.box_center_to_corner(pred_bbox) return predicted_bbox
When there are many anchor boxes, many similar (with significant overlap) predicted bounding boxes can be potentially output for surrounding the same object. To simplify the output, we can merge similar predicted bounding boxes that belong to the same object by using non-maximum suppression (NMS).
Here is how non-maximum suppression works. For a predicted bounding box B, the object detection model calculates the predicted likelihood for each class. Denoting by p the largest predicted likelihood, the class corresponding to this probability is the predicted class for B. Specifically, we refer to p as the confidence (score) of the predicted bounding box B. On the same image, all the predicted non-background bounding boxes are sorted by confidence in descending order to generate a list L. Then we manipulate the sorted list L in the following steps:
1. Select the predicted bounding box B1 with the highest confidence from L as a basis and remove all non-basis predicted bounding boxes whose IoU with B1 exceeds a predefined

594

Chapter 13. Computer Vision

threshold ϵ from L. At this point, L keeps the predicted bounding box with the highest confidence but drops others that are too similar to it. In a nutshell, those with non-maximum confidence scores are suppressed.
2. Select the predicted bounding box B2 with the second highest confidence from L as another basis and remove all non-basis predicted bounding boxes whose IoU with B2 exceeds ϵ from L.
3. Repeat the above process until all the predicted bounding boxes in L have been used as a basis. At this time, the IoU of any pair of predicted bounding boxes in L is below the threshold ϵ; thus, no pair is too similar with each other.
4. Output all the predicted bounding boxes in the list L.
The following nms function sorts confidence scores in descending order and returns their indices.
#@save def nms(boxes, scores, iou_threshold):
"""Sort confidence scores of predicted bounding boxes.""" B = scores.argsort()[::-1] keep = [] # Indices of predicted bounding boxes that will be kept while B.size > 0:
i = B[0] keep.append(i) if B.size == 1: break iou = box_iou(boxes[i, :].reshape(-1, 4),
boxes[B[1:], :].reshape(-1, 4)).reshape(-1) inds = np.nonzero(iou <= iou_threshold)[0] B = B[inds + 1] return np.array(keep, dtype=np.int32, ctx=boxes.ctx)
We define the following multibox_detection to apply non-maximum suppression to predicting bounding boxes. Do not worry if you find the implementation a bit complicated: we will show how it works with a concrete example right after the implementation.
#@save def multibox_detection(cls_probs, offset_preds, anchors, nms_threshold=0.5,
pos_threshold=0.009999999): """Predict bounding boxes using non-maximum suppression.""" device, batch_size = cls_probs.ctx, cls_probs.shape[0] anchors = np.squeeze(anchors, axis=0) num_classes, num_anchors = cls_probs.shape[1], cls_probs.shape[2] out = [] for i in range(batch_size):
cls_prob, offset_pred = cls_probs[i], offset_preds[i].reshape(-1, 4) conf, class_id = np.max(cls_prob[1:], 0), np.argmax(cls_prob[1:], 0) predicted_bb = offset_inverse(anchors, offset_pred) keep = nms(predicted_bb, conf, nms_threshold) # Find all non-`keep` indices and set the class to background all_idx = np.arange(num_anchors, dtype=np.int32, ctx=device) combined = np.concatenate((keep, all_idx)) unique, counts = np.unique(combined, return_counts=True) non_keep = unique[counts == 1] all_id_sorted = np.concatenate((keep, non_keep)) class_id[non_keep] = -1 class_id = class_id[all_id_sorted].astype('float32')
(continues on next page)

13.4. Anchor Boxes

595

(continued from previous page)
conf, predicted_bb = conf[all_id_sorted], predicted_bb[all_id_sorted] # Here `pos_threshold` is a threshold for positive (non-background) # predictions below_min_idx = (conf < pos_threshold) class_id[below_min_idx] = -1 conf[below_min_idx] = 1 - conf[below_min_idx] pred_info = np.concatenate((np.expand_dims(
class_id, axis=1), np.expand_dims(conf, axis=1), predicted_bb), axis=1)
out.append(pred_info) return np.stack(out)
Now let us apply the above implementations to a concrete example with four anchor boxes. For simplicity, we assume that the predicted offsets are all zeros. This means that the predicted bounding boxes are anchor boxes. For each class among the background, dog, and cat, we also define its predicted likelihood.
anchors = np.array([[0.1, 0.08, 0.52, 0.92], [0.08, 0.2, 0.56, 0.95], [0.15, 0.3, 0.62, 0.91], [0.55, 0.2, 0.9, 0.88]])
offset_preds = np.array([0] * anchors.size) cls_probs = np.array([[0] * 4, # Predicted background likelihood
[0.9, 0.8, 0.7, 0.1], # Predicted dog likelihood [0.1, 0.2, 0.3, 0.9]]) # Predicted cat likelihood
We can plot these predicted bounding boxes with their confidence on the image.
fig = d2l.plt.imshow(img) show_bboxes(fig.axes, anchors * bbox_scale,
['dog=0.9', 'dog=0.8', 'dog=0.7', 'cat=0.9'])

Now we can invoke the multibox_detection function to perform non-maximum suppression, where the threshold is set to 0.5. Note that we add a dimension for examples in the tensor input.
We can see that the shape of the returned result is (batch size, number of anchor boxes, 6). The six elements in the innermost dimension gives the output information for the same predicted bounding box. The first element is the predicted class index, which starts from 0 (0 is dog and 1 is cat). The value -1 indicates background or removal in non-maximum suppression. The second element

596

Chapter 13. Computer Vision

is the confidence of the predicted bounding box. The remaining four elements are the (x, y)-axis coordinates of the upper-left corner and the lower-right corner of the predicted bounding box, respectively (range is between 0 and 1).
output = multibox_detection(np.expand_dims(cls_probs, axis=0), np.expand_dims(offset_preds, axis=0), np.expand_dims(anchors, axis=0), nms_threshold=0.5)
output
array([[[ 1. , 0.9 , 0.55, 0.2 , 0.9 , 0.88], [ 0. , 0.9 , 0.1 , 0.08, 0.52, 0.92], [-1. , 0.8 , 0.08, 0.2 , 0.56, 0.95], [-1. , 0.7 , 0.15, 0.3 , 0.62, 0.91]]])
After removing those predicted bounding boxes of class -1, we can output the final predicted bounding box kept by non-maximum suppression.
fig = d2l.plt.imshow(img) for i in output[0].asnumpy():
if i[0] == -1: continue
label = ('dog=', 'cat=')[int(i[0])] + str(i[1]) show_bboxes(fig.axes, [np.array(i[2:]) * bbox_scale], label)

In practice, we can remove predicted bounding boxes with lower confidence even before performing non-maximum suppression, thereby reducing computation in this algorithm. We may also post-process the output of non-maximum suppression, for example, by only keeping results with higher confidence in the final output.

13.4. Anchor Boxes

597

Summary
• We generate anchor boxes with different shapes centered on each pixel of the image. • Intersection over union (IoU), also known as Jaccard index, measures the similarity of two
bounding boxes. It is the ratio of their intersection area to their union area. • In a training set, we need two types of labels for each anchor box. One is the class of the
object relevant to the anchor box and the other is the offset of the ground-truth bounding box relative to the anchor box. • During prediction, we can use non-maximum suppression (NMS) to remove similar predicted bounding boxes, thereby simplifying the output.
Exercises
1. Change values of sizes and ratios in the multibox_prior function. What are the changes to the generated anchor boxes?
2. Construct and visualize two bounding boxes with an IoU of 0.5. How do they overlap with each other?
3. Modify the variable anchors in Section 13.4.3 and Section 13.4.4. How do the results change? 4. Non-maximum suppression is a greedy algorithm that suppresses predicted bounding boxes
by removing them. Is it possible that some of these removed ones are actually useful? How can this algorithm be modified to suppress softly? You may refer to Soft-NMS (Bodla et al., 2017). 5. Rather than being hand-crafted, can non-maximum suppression be learned? Discussions179
13.5 Multiscale Object Detection
In Section 13.4, we generated multiple anchor boxes centered on each pixel of an input image. Essentially these anchor boxes represent samples of different regions of the image. However, we may end up with too many anchor boxes to compute if they are generated for every pixel. Think of a 561 × 728 input image. If five anchor boxes with varying shapes are generated for each pixel as their center, over two million anchor boxes (561 × 728 × 5) need to be labeled and predicted on the image.
179 https://discuss.d2l.ai/t/370

598

Chapter 13. Computer Vision

13.5.1 Multiscale Anchor Boxes
You may realize that it is not difficult to reduce anchor boxes on an image. For instance, we can just uniformly sample a small portion of pixels from the input image to generate anchor boxes centered on them. In addition, at different scales we can generate different numbers of anchor boxes of different sizes. Intuitively, smaller objects are more likely to appear on an image than larger ones. As an example, 1 × 1, 1 × 2, and 2 × 2 objects can appear on a 2 × 2 image in 4, 2, and 1 possible ways, respectively. Therefore, when using smaller anchor boxes to detect smaller objects, we can sample more regions, while for larger objects we can sample fewer regions.
To demonstrate how to generate anchor boxes at multiple scales, let us read an image. Its height and width are 561 and 728 pixels, respectively.
%matplotlib inline from mxnet import image, np, npx from d2l import mxnet as d2l
npx.set_np()
img = image.imread('../img/catdog.jpg') h, w = img.shape[:2] h, w
(561, 728)
Recall that in Section 6.2 we call a two-dimensional array output of a convolutional layer a feature map. By defining the feature map shape, we can determine centers of uniformly sampled anchor boxes on any image.
The display_anchors function is defined below. We generate anchor boxes (anchors) on the feature map (fmap) with each unit (pixel) as the anchor box center. Since the (x, y)-axis coordinate values in the anchor boxes (anchors) have been divided by the width and height of the feature map (fmap), these values are between 0 and 1, which indicate the relative positions of anchor boxes in the feature map.
Since centers of the anchor boxes (anchors) are spread over all units on the feature map (fmap), these centers must be uniformly distributed on any input image in terms of their relative spatial positions. More concretely, given the width and height of the feature map fmap_w and fmap_h, respectively, the following function will uniformly sample pixels in fmap_h rows and fmap_w columns on any input image. Centered on these uniformly sampled pixels, anchor boxes of scale s (assuming the length of the list s is 1) and different aspect ratios (ratios) will be generated.
def display_anchors(fmap_w, fmap_h, s): d2l.set_figsize() # Values on the first two dimensions do not affect the output fmap = np.zeros((1, 10, fmap_h, fmap_w)) anchors = npx.multibox_prior(fmap, sizes=s, ratios=[1, 2, 0.5]) bbox_scale = np.array((w, h, w, h)) d2l.show_bboxes( d2l.plt.imshow(img.asnumpy()).axes, anchors[0] * bbox_scale)
First, let us consider detection of small objects. In order to make it easier to distinguish when displayed, the anchor boxes with different centers here do not overlap: the anchor box scale is set

13.5. Multiscale Object Detection

599

to 0.15 and the height and width of the feature map are set to 4. We can see that the centers of the anchor boxes in 4 rows and 4 columns on the image are uniformly distributed.
display_anchors(fmap_w=4, fmap_h=4, s=[0.15])

We move on to reduce the height and width of the feature map by half and use larger anchor boxes to detect larger objects. When the scale is set to 0.4, some anchor boxes will overlap with each other.
display_anchors(fmap_w=2, fmap_h=2, s=[0.4])

Finally, we further reduce the height and width of the feature map by half and increase the anchor box scale to 0.8. Now the center of the anchor box is the center of the image.
display_anchors(fmap_w=1, fmap_h=1, s=[0.8])

600

Chapter 13. Computer Vision

13.5.2 Multiscale Detection
Since we have generated multiscale anchor boxes, we will use them to detect objects of various sizes at different scales. In the following we introduce a CNN-based multiscale object detection method that we will implement in Section 13.7.
At some scale, say that we have c feature maps of shape h × w. Using the method in Section 13.5.1, we generate hw sets of anchor boxes, where each set has a anchor boxes with the same center. For example, at the first scale in the experiments in Section 13.5.1, given ten (number of channels) 4×4 feature maps, we generated 16 sets of anchor boxes, where each set contains 3 anchor boxes with the same center. Next, each anchor box is labeled with the class and offset based on ground-truth bounding boxes. At the current scale, the object detection model needs to predict the classes and offsets of hw sets of anchor boxes on the input image, where different sets have different centers.
Assume that the c feature maps here are the intermediate outputs obtained by the CNN forward propagation based on the input image. Since there are hw different spatial positions on each feature map, the same spatial position can be thought of as having c units. According to the definition of receptive field in Section 6.2, these c units at the same spatial position of the feature maps have the same receptive field on the input image: they represent the input image information in the same receptive field. Therefore, we can transform the c units of the feature maps at the same spatial position into the classes and offsets of the a anchor boxes generated using this spatial position. In essence, we use the information of the input image in a certain receptive field to predict the classes and offsets of the anchor boxes that are close to that receptive field on the input image.
When the feature maps at different layers have varying-size receptive fields on the input image, they can be used to detect objects of different sizes. For example, we can design a neural network where units of feature maps that are closer to the output layer have wider receptive fields, so they can detect larger objects from the input image.
In a nutshell, we can leverage layerwise representations of images at multiple levels by deep neural networks for multiscale object detection. We will show how this works through a concrete example in Section 13.7.

13.5. Multiscale Object Detection

601

Summary
• At multiple scales, we can generate anchor boxes with different sizes to detect objects with different sizes.
• By defining the shape of feature maps, we can determine centers of uniformly sampled anchor boxes on any image.
• We use the information of the input image in a certain receptive field to predict the classes and offsets of the anchor boxes that are close to that receptive field on the input image.
• Through deep learning, we can leverage its layerwise representations of images at multiple levels for multiscale object detection.
Exercises
1. According to our discussions in Section 7.1, deep neural networks learn hierarchical features with increasing levels of abstraction for images. In multiscale object detection, do feature maps at different scales correspond to different levels of abstraction? Why or why not?
2. At the first scale (fmap_w=4, fmap_h=4) in the experiments in Section 13.5.1, generate uniformly distributed anchor boxes that may overlap.
3. Given a feature map variable with shape 1 × c × h × w, where c, h, and w are the number of channels, height, and width of the feature maps, respectively. How can you transform this variable into the classes and offsets of anchor boxes? What is the shape of the output?
Discussions180
13.6 The Object Detection Dataset
There is no small dataset such as MNIST and Fashion-MNIST in the field of object detection. In order to quickly demonstrate object detection models, we collected and labeled a small dataset. First, we took photos of free bananas from our office and generated 1000 banana images with different rotations and sizes. Then we placed each banana image at a random position on some background image. In the end, we labeled bounding boxes for those bananas on the images.
13.6.1 Downloading the Dataset
The banana detection dataset with all the image and csv label files can be downloaded directly from the Internet.
%matplotlib inline import os import pandas as pd from mxnet import gluon, image, np, npx from d2l import mxnet as d2l
npx.set_np()
180 https://discuss.d2l.ai/t/371

602

Chapter 13. Computer Vision

#@save d2l.DATA_HUB['banana-detection'] = (
d2l.DATA_URL + 'banana-detection.zip', '5de26c8fce5ccdea9f91267273464dc968d20d72')

13.6.2 Reading the Dataset
We are going to read the banana detection dataset in the read_data_bananas function below. The dataset includes a csv file for object class labels and ground-truth bounding box coordinates at the upper-left and lower-right corners.
#@save def read_data_bananas(is_train=True):
"""Read the banana detection dataset images and labels.""" data_dir = d2l.download_extract('banana-detection') csv_fname = os.path.join(data_dir,
'bananas_train' if is_train else 'bananas_val', 'label.csv') csv_data = pd.read_csv(csv_fname) csv_data = csv_data.set_index('img_name') images, targets = [], [] for img_name, target in csv_data.iterrows(): images.append( image.imread( os.path.join(data_dir, 'bananas_train' if is_train else 'bananas_val', 'images', f'{img_name}'))) # Here `target` contains (class, upper-left x, upper-left y, # lower-right x, lower-right y), where all the images have the same # banana class (index 0) targets.append(list(target)) return images, np.expand_dims(np.array(targets), 1) / 256
By using the read_data_bananas function to read images and labels, the following BananasDataset class will allow us to create a customized Dataset instance for loading the banana detection dataset.
#@save class BananasDataset(gluon.data.Dataset):
"""A customized dataset to load the banana detection dataset.""" def __init__(self, is_train):
self.features, self.labels = read_data_bananas(is_train) print('read ' + str(len(self.features)) + (
f' training examples' if is_train else f' validation examples'))
def __getitem__(self, idx): return (self.features[idx].astype('float32').transpose(2, 0, 1), self.labels[idx])
def __len__(self): return len(self.features)
Finally, we define the load_data_bananas function to return two data iterator instances for both

13.6. The Object Detection Dataset

603

the training and test sets. For the test dataset, there is no need to read it in random order.
#@save def load_data_bananas(batch_size):
"""Load the banana detection dataset.""" train_iter = gluon.data.DataLoader(BananasDataset(is_train=True),
batch_size, shuffle=True) val_iter = gluon.data.DataLoader(BananasDataset(is_train=False),
batch_size) return train_iter, val_iter
Let us read a minibatch and print the shapes of both images and labels in this minibatch. The shape of the image minibatch, (batch size, number of channels, height, width), looks familiar: it is the same as in our earlier image classification tasks. The shape of the label minibatch is (batch size, m, 5), where m is the largest possible number of bounding boxes that any image has in the dataset.
Although computation in minibatches is more efficient, it requires that all the image examples contain the same number of bounding boxes to form a minibatch via concatenation. In general, images may have a varying number of bounding boxes; thus, images with fewer than m bounding boxes will be padded with illegal bounding boxes until m is reached. Then the label of each bounding box is represented by an array of length 5. The first element in the array is the class of the object in the bounding box, where -1 indicates an illegal bounding box for padding. The remaining four elements of the array are the (x, y)-coordinate values of the upper-left corner and the lower-right corner of the bounding box (the range is between 0 and 1). For the banana dataset, since there is only one bounding box on each image, we have m = 1.
batch_size, edge_size = 32, 256 train_iter, _ = load_data_bananas(batch_size) batch = next(iter(train_iter)) batch[0].shape, batch[1].shape
read 1000 training examples read 100 validation examples
((32, 3, 256, 256), (32, 1, 5))
13.6.3 Demonstration
Let us demonstrate ten images with their labeled ground-truth bounding boxes. We can see that the rotations, sizes, and positions of bananas vary across all these images. Of course, this is just a simple artificial dataset. In practice, real-world datasets are usually much more complicated.
imgs = (batch[0][0:10].transpose(0, 2, 3, 1)) / 255 axes = d2l.show_images(imgs, 2, 5, scale=2) for ax, label in zip(axes, batch[1][0:10]):
d2l.show_bboxes(ax, [label[0][1:5] * edge_size], colors=['w'])

604

Chapter 13. Computer Vision

Summary
• The banana detection dataset we collected can be used to demonstrate object detection models.
• The data loading for object detection is similar to that for image classification. However, in object detection the labels also contain information of ground-truth bounding boxes, which is missing in image classification.
Exercises
1. Demonstrate other images with ground-truth bounding boxes in the banana detection dataset. How do they differ with respect to bounding boxes and objects?
2. Say that we want to apply data augmentation, such as random cropping, to object detection. How can it be different from that in image classification? Hint: what if a cropped image only contains a small portion of an object?
Discussions181
13.7 Single Shot Multibox Detection
In Section 13.3–Section 13.6, we introduced bounding boxes, anchor boxes, multiscale object detection, and the dataset for object detection. Now we are ready to use such background knowledge to design an object detection model: single shot multibox detection (SSD) (Liu et al., 2016). This model is simple, fast, and widely used. Although this is just one of vast amounts of object detection models, some of the design principles and implementation details in this section are also applicable to other models.
181 https://discuss.d2l.ai/t/372

13.7. Single Shot Multibox Detection

605

13.7.1 Model
Fig. 13.7.1 provides an overview of the design of single-shot multibox detection. This model mainly consists of a base network followed by several multiscale feature map blocks. The base network is for extracting features from the input image, so it can use a deep CNN. For example, the original single-shot multibox detection paper adopts a VGG network truncated before the classification layer (Liu et al., 2016), while ResNet has also been commonly used. Through our design we can make the base network output larger feature maps so as to generate more anchor boxes for detecting smaller objects. Subsequently, each multiscale feature map block reduces (e.g., by half) the height and width of the feature maps from the previous block, and enables each unit of the feature maps to increase its receptive field on the input image.
Recall the design of multiscale object detection through layerwise representations of images by deep neural networks in Section 13.5. Since multiscale feature maps closer to the top of Fig. 13.7.1 are smaller but have larger receptive fields, they are suitable for detecting fewer but larger objects.
In a nutshell, via its base network and several multiscale feature map blocks, single-shot multibox detection generates a varying number of anchor boxes with different sizes, and detects varyingsize objects by predicting classes and offsets of these anchor boxes (thus the bounding boxes); thus, this is a multiscale object detection model.

Fig. 13.7.1: As a multiscale object detection model, single-shot multibox detection mainly consists of a base network followed by several multiscale feature map blocks.
In the following, we will describe the implementation details of different blocks in Fig. 13.7.1. To begin with, we discuss how to implement the class and bounding box prediction.

606

Chapter 13. Computer Vision

Class Prediction Layer
Let the number of object classes be q. Then anchor boxes have q + 1 classes, where class 0 is background. At some scale, suppose that the height and width of feature maps are h and w, respectively. When a anchor boxes are generated with each spatial position of these feature maps as their center, a total of hwa anchor boxes need to be classified. This often makes classification with fully-connected layers infeasible due to likely heavy parameterization costs. Recall how we used channels of convolutional layers to predict classes in Section 7.3. Single-shot multibox detection uses the same technique to reduce model complexity.
Specifically, the class prediction layer uses a convolutional layer without altering width or height of feature maps. In this way, there can be a one-to-one correspondence between outputs and inputs at the same spatial dimensions (width and height) of feature maps. More concretely, channels of the output feature maps at any spatial position (x, y) represent class predictions for all the anchor boxes centered on (x, y) of the input feature maps. To produce valid predictions, there must be a(q + 1) output channels, where for the same spatial position the output channel with index i(q+1)+j represents the prediction of the class j (0 ≤ j ≤ q) for the anchor box i (0 ≤ i < a).
Below we define such a class prediction layer, specifying a and q via arguments num_anchors and num_classes, respectively. This layer uses a 3 × 3 convolutional layer with a padding of 1. The width and height of the input and output of this convolutional layer remain unchanged.
%matplotlib inline from mxnet import autograd, gluon, image, init, np, npx from mxnet.gluon import nn from d2l import mxnet as d2l
npx.set_np()
def cls_predictor(num_anchors, num_classes): return nn.Conv2D(num_anchors * (num_classes + 1), kernel_size=3, padding=1)
Bounding Box Prediction Layer
The design of the bounding box prediction layer is similar to that of the class prediction layer. The only difference lies in the number of outputs for each anchor box: here we need to predict four offsets rather than q + 1 classes.
def bbox_predictor(num_anchors): return nn.Conv2D(num_anchors * 4, kernel_size=3, padding=1)

13.7. Single Shot Multibox Detection

607

Concatenating Predictions for Multiple Scales
As we mentioned, single-shot multibox detection uses multiscale feature maps to generate anchor boxes and predict their classes and offsets. At different scales, the shapes of feature maps or the numbers of anchor boxes centered on the same unit may vary. Therefore, shapes of the prediction outputs at different scales may vary. In the following example, we construct feature maps at two different scales, Y1 and Y2, for the same minibatch, where the height and width of Y2 are half of those of Y1. Let us take class prediction as an example. Suppose that 5 and 3 anchor boxes are generated for every unit in Y1 and Y2, respectively. Suppose further that the number of object classes is 10. For feature maps Y1 and Y2 the numbers of channels in the class prediction outputs are 5 × (10 + 1) = 55 and 3 × (10 + 1) = 33, respectively, where either output shape is (batch size, number of channels, height, width).
def forward(x, block): block.initialize() return block(x)
Y1 = forward(np.zeros((2, 8, 20, 20)), cls_predictor(5, 10)) Y2 = forward(np.zeros((2, 16, 10, 10)), cls_predictor(3, 10)) Y1.shape, Y2.shape
((2, 55, 20, 20), (2, 33, 10, 10))
As we can see, except for the batch size dimension, the other three dimensions all have different sizes. To concatenate these two prediction outputs for more efficient computation, we will transform these tensors into a more consistent format.
Note that the channel dimension holds the predictions for anchor boxes with the same center. We first move this dimension to the innermost. Since the batch size remains the same for different scales, we can transform the prediction output into a two-dimensional tensor with shape (batch size, height × width × number of channels). Then we can concatenate such outputs at different scales along dimension 1.
def flatten_pred(pred): return npx.batch_flatten(pred.transpose(0, 2, 3, 1))
def concat_preds(preds): return np.concatenate([flatten_pred(p) for p in preds], axis=1)
In this way, even though Y1 and Y2 have different sizes in channels, heights, and widths, we can still concatenate these two prediction outputs at two different scales for the same minibatch.
concat_preds([Y1, Y2]).shape
(2, 25300)

608

Chapter 13. Computer Vision

Downsampling Block
In order to detect objects at multiple scales, we define the following downsampling block down_sample_blk that halves the height and width of input feature maps. In fact, this block applies the design of VGG blocks in Section 7.2.1. More concretely, each downsampling block consists of two 3 × 3 convolutional layers with padding of 1 followed by a 2 × 2 maximum pooling layer with stride of 2. As we know, 3 × 3 convolutional layers with padding of 1 do not change the shape of feature maps. However, the subsequent 2 × 2 maximum pooling reduces the height and width of input feature maps by half. For both input and output feature maps of this downsampling block, because 1 × 2 + (3 − 1) + (3 − 1) = 6, each unit in the output has a 6 × 6 receptive field on the input. Therefore, the downsampling block enlarges the receptive field of each unit in its output feature maps.
def down_sample_blk(num_channels): blk = nn.Sequential() for _ in range(2): blk.add(nn.Conv2D(num_channels, kernel_size=3, padding=1), nn.BatchNorm(in_channels=num_channels), nn.Activation('relu')) blk.add(nn.MaxPool2D(2)) return blk
In the following example, our constructed downsampling block changes the number of input channels and halves the height and width of the input feature maps.
forward(np.zeros((2, 3, 20, 20)), down_sample_blk(10)).shape
(2, 10, 10, 10)
Base Network Block
The base network block is used to extract features from input images. For simplicity, we construct a small base network consisting of three downsampling blocks that double the number of channels at each block. Given a 256 × 256 input image, this base network block outputs 32 × 32 feature maps (256/23 = 32).
def base_net(): blk = nn.Sequential() for num_filters in [16, 32, 64]: blk.add(down_sample_blk(num_filters)) return blk
forward(np.zeros((2, 3, 256, 256)), base_net()).shape
(2, 64, 32, 32)

13.7. Single Shot Multibox Detection

609

The Complete Model
The complete single shot multibox detection model consists of five blocks. The feature maps produced by each block are used for both (i) generating anchor boxes and (ii) predicting classes and offsets of these anchor boxes. Among these five blocks, the first one is the base network block, the second to the fourth are downsampling blocks, and the last block uses global maximum pooling to reduce both the height and width to 1. Technically, the second to the fifth blocks are all those multiscale feature map blocks in Fig. 13.7.1.
def get_blk(i): if i == 0: blk = base_net() elif i == 4: blk = nn.GlobalMaxPool2D() else: blk = down_sample_blk(128) return blk

Now we define the forward propagation for each block. Different from in image classification tasks, outputs here include (i) CNN feature maps Y, (ii) anchor boxes generated using Y at the current scale, and (iii) classes and offsets predicted (based on Y) for these anchor boxes.
def blk_forward(X, blk, size, ratio, cls_predictor, bbox_predictor): Y = blk(X) anchors = d2l.multibox_prior(Y, sizes=size, ratios=ratio) cls_preds = cls_predictor(Y) bbox_preds = bbox_predictor(Y) return (Y, anchors, cls_preds, bbox_preds)

Recall that in Fig. 13.7.1 a multiscale feature map block that is closer to the top is for detecting larger objects; thus, it needs to generate larger anchor boxes. In the above forward propagation, at each multiscale feature map block we pass in a list of two scale values via the sizes argument of the invoked multibox_prior function (described in Section 13.4). In the following, the interval between 0.2 and 1.05 is split evenly into five sections to determine the smaller scale√values at the five blo√cks: 0.2, 0.37, 0.54, 0.71, and 0.88. Then their larger scale values are given by 0.2 × 0.37 = 0.272, 0.37 × 0.54 = 0.447, and so on.
sizes = [[0.2, 0.272], [0.37, 0.447], [0.54, 0.619], [0.71, 0.79], [0.88, 0.961]]
ratios = [[1, 2, 0.5]] * 5 num_anchors = len(sizes[0]) + len(ratios[0]) - 1
Now we can define the complete model TinySSD as follows.
class TinySSD(nn.Block): def __init__(self, num_classes, **kwargs): super(TinySSD, self).__init__(**kwargs) self.num_classes = num_classes for i in range(5): # Equivalent to the assignment statement `self.blk_i = get_blk(i)` setattr(self, f'blk_{i}', get_blk(i)) setattr(self, f'cls_{i}', cls_predictor(num_anchors, num_classes)) setattr(self, f'bbox_{i}', bbox_predictor(num_anchors))
(continues on next page)

610

Chapter 13. Computer Vision

(continued from previous page)
def forward(self, X): anchors, cls_preds, bbox_preds = [None] * 5, [None] * 5, [None] * 5 for i in range(5): # Here `getattr(self, 'blk_%d' % i)` accesses `self.blk_i` X, anchors[i], cls_preds[i], bbox_preds[i] = blk_forward( X, getattr(self, f'blk_{i}'), sizes[i], ratios[i], getattr(self, f'cls_{i}'), getattr(self, f'bbox_{i}')) anchors = np.concatenate(anchors, axis=1) cls_preds = concat_preds(cls_preds) cls_preds = cls_preds.reshape(cls_preds.shape[0], -1, self.num_classes + 1) bbox_preds = concat_preds(bbox_preds) return anchors, cls_preds, bbox_preds
We create a model instance and use it to perform forward propagation on a minibatch of 256 × 256 images X.
As shown earlier in this section, the first block outputs 32×32 feature maps. Recall that the second to fourth downsampling blocks halve the height and width and the fifth block uses global pooling. Since 4 anchor boxes are generated for each unit along spatial dimensions of feature maps, at all the five scales a total of (322 + 162 + 82 + 42 + 1) × 4 = 5444 anchor boxes are generated for each image.
net = TinySSD(num_classes=1) net.initialize() X = np.zeros((32, 3, 256, 256)) anchors, cls_preds, bbox_preds = net(X)
print('output anchors:', anchors.shape) print('output class preds:', cls_preds.shape) print('output bbox preds:', bbox_preds.shape)
output anchors: (1, 5444, 4) output class preds: (32, 5444, 2) output bbox preds: (32, 21776)
13.7.2 Training
Now we will explain how to train the single shot multibox detection model for object detection.

13.7. Single Shot Multibox Detection

611

Reading the Dataset and Initializing the Model
To begin with, let us read the banana detection dataset described in Section 13.6.
batch_size = 32 train_iter, _ = d2l.load_data_bananas(batch_size)
read 1000 training examples read 100 validation examples
There is only one class in the banana detection dataset. After defining the model, we need to initialize its parameters and define the optimization algorithm.
device, net = d2l.try_gpu(), TinySSD(num_classes=1) net.initialize(init=init.Xavier(), ctx=device) trainer = gluon.Trainer(net.collect_params(), 'sgd', {
'learning_rate': 0.2, 'wd': 5e-4})
Defining Loss and Evaluation Functions
Object detection has two types of losses. The first loss concerns classes of anchor boxes: its computation can simply reuse the cross-entropy loss function that we used for image classification. The second loss concerns offsets of positive (non-background) anchor boxes: this is a regression problem. For this regression problem, however, here we do not use the squared loss described in Section 3.1.3. Instead, we use the L1 norm loss, the absolute value of the difference between the prediction and the ground-truth. The mask variable bbox_masks filters out negative anchor boxes and illegal (padded) anchor boxes in the loss calculation. In the end, we sum up the anchor box class loss and the anchor box offset loss to obtain the loss function for the model.
cls_loss = gluon.loss.SoftmaxCrossEntropyLoss() bbox_loss = gluon.loss.L1Loss()
def calc_loss(cls_preds, cls_labels, bbox_preds, bbox_labels, bbox_masks): cls = cls_loss(cls_preds, cls_labels) bbox = bbox_loss(bbox_preds * bbox_masks, bbox_labels * bbox_masks) return cls + bbox
We can use accuracy to evaluate the classification results. Due to the used L1 norm loss for the offsets, we use the mean absolute error to evaluate the predicted bounding boxes. These prediction results are obtained from the generated anchor boxes and the predicted offsets for them.
def cls_eval(cls_preds, cls_labels): # Because the class prediction results are on the final dimension, # `argmax` needs to specify this dimension return float((cls_preds.argmax(axis=-1).astype( cls_labels.dtype) == cls_labels).sum())
def bbox_eval(bbox_preds, bbox_labels, bbox_masks): return float((np.abs((bbox_labels - bbox_preds) * bbox_masks)).sum())

612

Chapter 13. Computer Vision

Training the Model
When training the model, we need to generate multiscale anchor boxes (anchors) and predict their classes (cls_preds) and offsets (bbox_preds) in the forward propagation. Then we label the classes (cls_labels) and offsets (bbox_labels) of such generated anchor boxes based on the label information Y. Finally, we calculate the loss function using the predicted and labeled values of the classes and offsets. For concise implementations, evaluation of the test dataset is omitted here.
num_epochs, timer = 20, d2l.Timer() animator = d2l.Animator(xlabel='epoch', xlim=[1, num_epochs],
legend=['class error', 'bbox mae']) for epoch in range(num_epochs):
# Sum of training accuracy, no. of examples in sum of training accuracy, # Sum of absolute error, no. of examples in sum of absolute error metric = d2l.Accumulator(4) for features, target in train_iter:
timer.start() X = features.as_in_ctx(device) Y = target.as_in_ctx(device) with autograd.record():
# Generate multiscale anchor boxes and predict their classes and # offsets anchors, cls_preds, bbox_preds = net(X) # Label the classes and offsets of these anchor boxes bbox_labels, bbox_masks, cls_labels = d2l.multibox_target(
anchors, Y) # Calculate the loss function using the predicted and labeled # values of the classes and offsets l = calc_loss(cls_preds, cls_labels, bbox_preds, bbox_labels,
bbox_masks) l.backward() trainer.step(batch_size) metric.add(cls_eval(cls_preds, cls_labels), cls_labels.size,
bbox_eval(bbox_preds, bbox_labels, bbox_masks), bbox_labels.size) cls_err, bbox_mae = 1 - metric[0] / metric[1], metric[2] / metric[3] animator.add(epoch + 1, (cls_err, bbox_mae)) print(f'class err {cls_err:.2e}, bbox mae {bbox_mae:.2e}') print(f'{len(train_iter._dataset) / timer.stop():.1f} examples/sec on ' f'{str(device)}')
class err 3.50e-03, bbox mae 3.73e-03 2697.2 examples/sec on gpu(0)

13.7. Single Shot Multibox Detection

613

13.7.3 Prediction
During prediction, the goal is to detect all the objects of interest on the image. Below we read and resize a test image, converting it to a four-dimensional tensor that is required by convolutional layers.
img = image.imread('../img/banana.jpg') feature = image.imresize(img, 256, 256).astype('float32') X = np.expand_dims(feature.transpose(2, 0, 1), axis=0)
Using the multibox_detection function below, the predicted bounding boxes are obtained from the anchor boxes and their predicted offsets. Then non-maximum suppression is used to remove similar predicted bounding boxes.
def predict(X): anchors, cls_preds, bbox_preds = net(X.as_in_ctx(device)) cls_probs = npx.softmax(cls_preds).transpose(0, 2, 1) output = d2l.multibox_detection(cls_probs, bbox_preds, anchors) idx = [i for i, row in enumerate(output[0]) if row[0] != -1] return output[0, idx]
output = predict(X)
Finally, we display all the predicted bounding boxes with confidence 0.9 or above as the output.
def display(img, output, threshold): d2l.set_figsize((5, 5)) fig = d2l.plt.imshow(img.asnumpy()) for row in output: score = float(row[1]) if score < threshold: continue h, w = img.shape[0:2] bbox = [row[2:6] * np.array((w, h, w, h), ctx=row.ctx)] d2l.show_bboxes(fig.axes, bbox, '%.2f' % score, 'w')
display(img, output, threshold=0.9)

614

Chapter 13. Computer Vision

Summary
• Single shot multibox detection is a multiscale object detection model. Via its base network and several multiscale feature map blocks, single-shot multibox detection generates a varying number of anchor boxes with different sizes, and detects varying-size objects by predicting classes and offsets of these anchor boxes (thus the bounding boxes).
• When training the single-shot multibox detection model, the loss function is calculated based on the predicted and labeled values of the anchor box classes and offsets.

Exercises

1. Can you improve the single-shot multibox detection by improving the loss function? For

example, replace L1 norm loss with smooth L1 norm loss for the predicted offsets. This

loss function uses a square function around zero for smoothness, which is controlled by the

hyperparameter σ:

{ (σx)2/2,
f (x) = |x| − 0.5/σ2,

if |x| < 1/σ2 otherwise

(13.7.1)

When σ is very large, this loss is similar to the L1 norm loss. When its value is smaller, the loss function is smoother.

sigmas = [10, 1, 0.5] lines = ['-', '--', '-.'] x = np.arange(-2, 2, 0.1)

(continues on next page)

13.7. Single Shot Multibox Detection

615

d2l.set_figsize()

(continued from previous page)

for l, s in zip(lines, sigmas): y = npx.smooth_l1(x, scalar=s) d2l.plt.plot(x.asnumpy(), y.asnumpy(), l, label='sigma=%.1f' % s)
d2l.plt.legend();

Besides, in the experiment we used cross-entropy loss for class prediction: denoting by pj the predicted probability for the ground-truth class j, the cross-entropy loss is − log pj. We can also use the focal loss (Lin et al., 2017a): given hyperparameters γ > 0 and α > 0, this loss is defined as:

−α(1 − pj)γ log pj.

(13.7.2)

As we can see, increasing γ can effectively reduce the relative loss for well-classified examples (e.g., pj > 0.5) so the training can focus more on those difficult examples that are misclassified.

def focal_loss(gamma, x): return -(1 - x)**gamma * np.log(x)

x = np.arange(0.01, 1, 0.01) for l, gamma in zip(lines, [0, 1, 5]):
y = d2l.plt.plot(x.asnumpy(), focal_loss(gamma, x).asnumpy(), l, label='gamma=%.1f' % gamma)
d2l.plt.legend();

616

Chapter 13. Computer Vision

2. Due to space limitations, we have omitted some implementation details of the single shot multibox detection model in this section. Can you further improve the model in the following aspects:
1. When an object is much smaller compared with the image, the model could resize the input image bigger.
2. There are typically a vast number of negative anchor boxes. To make the class distribution more balanced, we could downsample negative anchor boxes.
3. In the loss function, assign different weight hyperparameters to the class loss and the offset loss.
4. Use other methods to evaluate the object detection model, such as those in the single shot multibox detection paper (Liu et al., 2016).
Discussions182

13.8 Region-based CNNs (R-CNNs)
Besides single shot multibox detection described in Section 13.7, region-based CNNs or regions with CNN features (R-CNNs) are also among many pioneering approaches of applying deep learning to object detection (Girshick et al., 2014). In this section, we will introduce the R-CNN and its series of improvements: the fast R-CNN (Girshick, 2015), the faster R-CNN (Ren et al., 2015), and the mask R-CNN (He et al., 2017a). Due to limited space, we will only focus on the design of these models.

13.8.1 R-CNNs
The R-CNN first extracts many (e.g., 2000) region proposals from the input image (e.g., anchor boxes can also be considered as region proposals), labeling their classes and bounding boxes (e.g., offsets).
(Girshick et al., 2014)
Then a CNN is used to perform forward propagation on each region proposal to extract its features. Next, features of each region proposal are used for predicting the class and bounding box of this region proposal.
182 https://discuss.d2l.ai/t/373

13.8. Region-based CNNs (R-CNNs)

617

Fig. 13.8.1: The R-CNN model.
Fig. 13.8.1 shows the R-CNN model. More concretely, the R-CNN consists of the following four steps:
1. Perform selective search to extract multiple high-quality region proposals on the input image (Uijlings et al., 2013). These proposed regions are usually selected at multiple scales with different shapes and sizes. Each region proposal will be labeled with a class and a groundtruth bounding box.
2. Choose a pretrained CNN and truncate it before the output layer. Resize each region proposal to the input size required by the network, and output the extracted features for the region proposal through forward propagation.
3. Take the extracted features and labeled class of each region proposal as an example. Train multiple support vector machines to classify objects, where each support vector machine individually determines whether the example contains a specific class.
4. Take the extracted features and labeled bounding box of each region proposal as an example. Train a linear regression model to predict the ground-truth bounding box.
Although the R-CNN model uses pretrained CNNs to effectively extract image features, it is slow. Imagine that we select thousands of region proposals from a single input image: this requires thousands of CNN forward propagations to perform object detection. This massive computing load makes it infeasible to widely use R-CNNs in real-world applications.
13.8.2 Fast R-CNN
The main performance bottleneck of an R-CNN lies in the independent CNN forward propagation for each region proposal, without sharing computation. Since these regions usually have overlaps, independent feature extractions lead to much repeated computation. One of the major improvements of the fast R-CNN from the R-CNN is that the CNN forward propagation is only performed on the entire image (Girshick, 2015).

618

Chapter 13. Computer Vision

Fig. 13.8.2: The fast R-CNN model.

Fig. 13.8.2 describes the fast R-CNN model. Its major computations are as follows:
1. Compared with the R-CNN, in the fast R-CNN the input of the CNN for feature extraction is the entire image, rather than individual region proposals. Moreover, this CNN is trainable. Given an input image, let the shape of the CNN output be 1 × c × h1 × w1.
2. Suppose that selective search generates n region proposals. These region proposals (of different shapes) mark regions of interest (of different shapes) on the CNN output. Then these regions of interest further extract features of the same shape (say height h2 and width w2 are specified) in order to be easily concatenated. To achieve this, the fast R-CNN introduces the region of interest (RoI) pooling layer: the CNN output and region proposals are input into this layer, outputting concatenated features of shape n × c × h2 × w2 that are further extracted for all the region proposals.
3. Using a fully-connected layer, transform the concatenated features into an output of shape n × d, where d depends on the model design.
4. Predict the class and bounding box for each of the n region proposals. More concretely, in class and bounding box prediction, transform the fully-connected layer output into an output of shape n × q (q is the number of classes) and an output of shape n × 4, respectively. The class prediction uses softmax regression.
The region of interest pooling layer proposed in the fast R-CNN is different from the pooling layer introduced in Section 6.5. In the pooling layer, we indirectly control the output shape by specifying sizes of the pooling window, padding, and stride. In contrast, we can directly specify the output shape in the region of interest pooling layer.
For example, let us specify the output height and width for each region as h2 and w2, respectively. For any region of interest window of shape h × w, this window is divided into a h2 × w2 grid of subwindows, where the shape of each subwindow is approximately (h/h2) × (w/w2). In practice, the height and width of any subwindow shall be rounded up, and the largest element shall be used as the output of the subwindow. Therefore, the region of interest pooling layer can extract features of the same shape even when regions of interest have different shapes.
As an illustrative example, in Fig. 13.8.3, the upper-left 3 × 3 region of interest is selected on a 4 × 4 input. For this region of interest, we use a 2 × 2 region of interest pooling layer to obtain a 2 × 2

13.8. Region-based CNNs (R-CNNs)

619

output. Note that each of the four divided subwindows contains elements 0, 1, 4, and 5 (5 is the maximum); 2 and 6 (6 is the maximum); 8 and 9 (9 is the maximum); and 10.

Fig. 13.8.3: A 2 × 2 region of interest pooling layer.
Below we demonstrate the computation of the region of interest pooling layer. Suppose that the height and width of the CNN-extracted features X are both 4, and there is only a single channel.
from mxnet import np, npx
npx.set_np()
X = np.arange(16).reshape(1, 1, 4, 4) X
array([[[[ 0., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.], [12., 13., 14., 15.]]]])
Let us further suppose that the height and width of the input image are both 40 pixels and that selective search generates two region proposals on this image. Each region proposal is expressed as five elements: its object class followed by the (x, y)-coordinates of its upper-left and lower-right corners.
rois = np.array([[0, 0, 0, 20, 20], [0, 0, 10, 30, 30]])
Because the height and width of X are 1/10 of the height and width of the input image, the coordinates of the two region proposals are multiplied by 0.1 according to the specified spatial_scale argument. Then the two regions of interest are marked on X as X[:, :, 0:3, 0:3] and X[:, :, 1:4, 0:4], respectively. Finally in the 2 × 2 region of interest pooling, each region of interest is divided into a grid of sub-windows to further extract features of the same shape 2 × 2.
npx.roi_pooling(X, rois, pooled_size=(2, 2), spatial_scale=0.1)
array([[[[ 5., 6.], [ 9., 10.]]],
[[[ 9., 11.], [13., 15.]]]])

620

Chapter 13. Computer Vision

13.8.3 Faster R-CNN
To be more accurate in object detection, the fast R-CNN model usually has to generate a lot of region proposals in selective search. To reduce region proposals without loss of accuracy, the faster R-CNN proposes to replace selective search with a region proposal network (Ren et al., 2015).

Fig. 13.8.4: The faster R-CNN model.
Fig. 13.8.4 shows the faster R-CNN model. Compared with the fast R-CNN, the faster R-CNN only changes the region proposal method from selective search to a region proposal network. The rest of the model remain unchanged. The region proposal network works in the following steps:
1. Use a 3×3 convolutional layer with padding of 1 to transform the CNN output to a new output with c channels. In this way, each unit along the spatial dimensions of the CNN-extracted feature maps gets a new feature vector of length c.
2. Centered on each pixel of the feature maps, generate multiple anchor boxes of different scales and aspect ratios and label them.
3. Using the length-c feature vector at the center of each anchor box, predict the binary class (background or objects) and bounding box for this anchor box.
4. Consider those predicted bounding boxes whose predicted classes are objects. Remove overlapped results using non-maximum suppression. The remaining predicted bounding boxes for objects are the region proposals required by the region of interest pooling layer.
It is worth noting that, as part of the faster R-CNN model, the region proposal network is jointly trained with the rest of the model. In other words, the objective function of the faster R-CNN includes not only the class and bounding box prediction in object detection, but also the binary class and bounding box prediction of anchor boxes in the region proposal network. As a result of the end-to-end training, the region proposal network learns how to generate high-quality region proposals, so as to stay accurate in object detection with a reduced number of region proposals that are learned from data.

13.8. Region-based CNNs (R-CNNs)

621

13.8.4 Mask R-CNN
In the training dataset, if pixel-level positions of object are also labeled on images, the mask R-CNN can effectively leverage such detailed labels to further improve the accuracy of object detection (He et al., 2017a).

Fig. 13.8.5: The mask R-CNN model.
As shown in Fig. 13.8.5, the mask R-CNN is modified based on the faster R-CNN. Specifically, the mask R-CNN replaces the region of interest pooling layer with the region of interest (RoI) alignment layer. This region of interest alignment layer uses bilinear interpolation to preserve the spatial information on the feature maps, which is more suitable for pixel-level prediction. The output of this layer contains feature maps of the same shape for all the regions of interest. They are used to predict not only the class and bounding box for each region of interest, but also the pixellevel position of the object through an additional fully convolutional network. More details on using a fully convolutional network to predict pixel-level semantics of an image will be provided in subsequent sections of this chapter.
Summary
• The R-CNN extracts many region proposals from the input image, uses a CNN to perform forward propagation on each region proposal to extract its features, then uses these features to predict the class and bounding box of this region proposal.
• One of the major improvements of the fast R-CNN from the R-CNN is that the CNN forward propagation is only performed on the entire image. It also introduces the region of interest pooling layer, so that features of the same shape can be further extracted for regions of interest that have different shapes.
• The faster R-CNN replaces the selective search used in the fast R-CNN with a jointly trained region proposal network, so that the former can stay accurate in object detection with a reduced number of region proposals.
• Based on the faster R-CNN, the mask R-CNN additionally introduces a fully convolutional

622

Chapter 13. Computer Vision

network, so as to leverage pixel-level labels to further improve the accuracy of object detection.
Exercises
1. Can we frame object detection as a single regression problem, such as predicting bounding boxes and class probabilities? You may refer to the design of the YOLO model (Redmon et al., 2016).
2. Compare single shot multibox detection with the methods introduced in this section. What are their major differences? You may refer to Figure 2 of (Zhao et al., 2019).
Discussions183
13.9 Semantic Segmentation and the Dataset
When discussing object detection tasks in Section 13.3–Section 13.8, rectangular bounding boxes are used to label and predict objects in images. This section will discuss the problem of semantic segmentation, which focuses on how to divide an image into regions belonging to different semantic classes. Different from object detection, semantic segmentation recognizes and understands what are in images in pixel level: its labeling and prediction of semantic regions are in pixel level. Fig. 13.9.1 shows the labels of the dog, cat, and background of the image in semantic segmentation. Compared with in object detection, the pixel-level borders labeled in semantic segmentation are obviously more fine-grained.

Fig. 13.9.1: Labels of the dog, cat, and background of the image in semantic segmentation.
13.9.1 Image Segmentation and Instance Segmentation
There are also two important tasks in the field of computer vision that are similar to semantic segmentation, namely image segmentation and instance segmentation. We will briefly distinguish them from semantic segmentation as follows.
• Image segmentation divides an image into several constituent regions. The methods for this type of problem usually make use of the correlation between pixels in the image. It does not need label information about image pixels during training, and it cannot guarantee that the segmented regions will have the semantics that we hope to obtain during prediction. Taking the image in Fig. 13.9.1 as input, image segmentation may divide the dog into two regions:
183 https://discuss.d2l.ai/t/374

13.9. Semantic Segmentation and the Dataset

623

one covers the mouth and eyes which are mainly black, and the other covers the rest of the body which is mainly yellow.
• Instance segmentation is also called simultaneous detection and segmentation. It studies how to recognize the pixel-level regions of each object instance in an image. Different from semantic segmentation, instance segmentation needs to distinguish not only semantics, but also different object instances. For example, if there are two dogs in the image, instance segmentation needs to distinguish which of the two dogs a pixel belongs to.

13.9.2 The Pascal VOC2012 Semantic Segmentation Dataset
On of the most important semantic segmentation dataset is Pascal VOC2012184. In the following, we will take a look at this dataset.
%matplotlib inline import os from mxnet import gluon, image, np, npx from d2l import mxnet as d2l
npx.set_np()

The tar file of the dataset is about 2 GB, so it may take a while to download the file. The extracted dataset is located at ../data/VOCdevkit/VOC2012.
#@save d2l.DATA_HUB['voc2012'] = (d2l.DATA_URL + 'VOCtrainval_11-May-2012.tar',
'4e443f8a2eca6b1dac8a6c57641b67dd40621a49')
voc_dir = d2l.download_extract('voc2012', 'VOCdevkit/VOC2012')
After entering the path ../data/VOCdevkit/VOC2012, we can see the different components of the dataset. The ImageSets/Segmentation path contains text files that specify training and test samples, while the JPEGImages and SegmentationClass paths store the input image and label for each example, respectively. The label here is also in the image format, with the same size as its labeled input image. Besides, pixels with the same color in any label image belong to the same semantic class. The following defines the read_voc_images function to read all the input images and labels into the memory.
#@save def read_voc_images(voc_dir, is_train=True):
"""Read all VOC feature and label images.""" txt_fname = os.path.join(voc_dir, 'ImageSets', 'Segmentation',
'train.txt' if is_train else 'val.txt') with open(txt_fname, 'r') as f:
images = f.read().split() features, labels = [], [] for i, fname in enumerate(images):
features.append( image.imread(os.path.join(voc_dir, 'JPEGImages', f'{fname}.jpg')))
labels.append( image.imread(
(continues on next page)
184 http://host.robots.ox.ac.uk/pascal/VOC/voc2012/

624

Chapter 13. Computer Vision

(continued from previous page) os.path.join(voc_dir, 'SegmentationClass', f'{fname}.png'))) return features, labels
train_features, train_labels = read_voc_images(voc_dir, True)
We draw the first five input images and their labels. In the label images, white and black represent borders and background, respectively, while the other colors correspond to different classes.
n=5 imgs = train_features[0:n] + train_labels[0:n] d2l.show_images(imgs, 2, n);

Next, we enumerate the RGB color values and class names for all the labels in this dataset.
#@save VOC_COLORMAP = [[0, 0, 0], [128, 0, 0], [0, 128, 0], [128, 128, 0],
[0, 0, 128], [128, 0, 128], [0, 128, 128], [128, 128, 128], [64, 0, 0], [192, 0, 0], [64, 128, 0], [192, 128, 0], [64, 0, 128], [192, 0, 128], [64, 128, 128], [192, 128, 128], [0, 64, 0], [128, 64, 0], [0, 192, 0], [128, 192, 0], [0, 64, 128]]
#@save VOC_CLASSES = [
'background', 'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person', 'potted plant', 'sheep', 'sofa', 'train', 'tv/monitor']

With the two constants defined above, we can conveniently find the class index for each pixel in a label. We define the voc_colormap2label function to build the mapping from the above RGB color values to class indices, and the voc_label_indices function to map any RGB values to their class
indices in this Pascal VOC2012 dataset.

#@save def voc_colormap2label():
"""Build the mapping from RGB to class indices for VOC labels.""" colormap2label = np.zeros(256**3) for i, colormap in enumerate(VOC_COLORMAP):

(continues on next page)

13.9. Semantic Segmentation and the Dataset

625

colormap2label[(colormap[0] * 256 + colormap[1]) * 256 + colormap[2]] = i
return colormap2label

(continued from previous page)

#@save def voc_label_indices(colormap, colormap2label):
"""Map any RGB values in VOC labels to their class indices.""" colormap = colormap.astype(np.int32) idx = ((colormap[:, :, 0] * 256 + colormap[:, :, 1]) * 256 +
colormap[:, :, 2]) return colormap2label[idx]

For example, in the first example image, the class index for the front part of the airplane is 1, while the background index is 0.
y = voc_label_indices(train_labels[0], voc_colormap2label()) y[105:115, 130:140], VOC_CLASSES[1]

(array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1.], [0., 0., 0., 0., 0., 0., 0., 1., 1., 1.], [0., 0., 0., 0., 0., 0., 1., 1., 1., 1.], [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.], [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.], [0., 0., 0., 0., 1., 1., 1., 1., 1., 1.], [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.], [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.], [0., 0., 0., 0., 0., 0., 1., 1., 1., 1.], [0., 0., 0., 0., 0., 0., 0., 0., 1., 1.]]),
'aeroplane')

Data Preprocessing

In previous experiments such as in Section 7.1–Section 7.4, images are rescaled to fit the modelʼs required input shape. However, in semantic segmentation, doing so requires rescaling the predicted pixel classes back to the original shape of the input image. Such rescaling may be inaccurate, especially for segmented regions with different classes. To avoid this issue, we crop the image to a fixed shape instead of rescaling. Specifically, using random cropping from image augmentation, we crop the same area of the input image and the label.

#@save def voc_rand_crop(feature, label, height, width):
"""Randomly crop both feature and label images.""" feature, rect = image.random_crop(feature, (width, label = image.fixed_crop(label, *rect) return feature, label

height))

imgs = [] for _ in range(n):
imgs += voc_rand_crop(train_features[0], train_labels[0], 200, 300) d2l.show_images(imgs[::2] + imgs[1::2], 2, n);

626

Chapter 13. Computer Vision

Custom Semantic Segmentation Dataset Class
We define a custom semantic segmentation dataset class VOCSegDataset by inheriting the Dataset class provided by high-level APIs. By implementing the __getitem__ function, we can arbitrarily access the input image indexed as idx in the dataset and the class index of each pixel in this image. Since some images in the dataset have a smaller size than the output size of random cropping, these examples are filtered out by a custom filter function. In addition, we also define the normalize_image function to standardize the values of the three RGB channels of input images.
#@save class VOCSegDataset(gluon.data.Dataset):
"""A customized dataset to load the VOC dataset.""" def __init__(self, is_train, crop_size, voc_dir):
self.rgb_mean = np.array([0.485, 0.456, 0.406]) self.rgb_std = np.array([0.229, 0.224, 0.225]) self.crop_size = crop_size features, labels = read_voc_images(voc_dir, is_train=is_train) self.features = [
self.normalize_image(feature) for feature in self.filter(features)] self.labels = self.filter(labels) self.colormap2label = voc_colormap2label() print('read ' + str(len(self.features)) + ' examples')
def normalize_image(self, img): return (img.astype('float32') / 255 - self.rgb_mean) / self.rgb_std
def filter(self, imgs): return [ img for img in imgs if (img.shape[0] >= self.crop_size[0] and img.shape[1] >= self.crop_size[1])]
def __getitem__(self, idx): feature, label = voc_rand_crop(self.features[idx], self.labels[idx], *self.crop_size) return (feature.transpose(2, 0, 1), voc_label_indices(label, self.colormap2label))
def __len__(self): return len(self.features)

13.9. Semantic Segmentation and the Dataset

627

Reading the Dataset
We use the custom VOCSegDataset class to create instances of the training set and test set, respectively. Suppose that we specify that the output shape of randomly cropped images is 320 × 480. Below we can view the number of examples that are retained in the training set and test set.
crop_size = (320, 480) voc_train = VOCSegDataset(True, crop_size, voc_dir) voc_test = VOCSegDataset(False, crop_size, voc_dir)
read 1114 examples read 1078 examples
Setting the batch size to 64, we define the data iterator for the training set. Let us print the shape of the first minibatch. Different from in image classification or object detection, labels here are three-dimensional tensors.
batch_size = 64 train_iter = gluon.data.DataLoader(voc_train, batch_size, shuffle=True,
last_batch='discard', num_workers=d2l.get_dataloader_workers()) for X, Y in train_iter: print(X.shape) print(Y.shape) break
(64, 3, 320, 480) (64, 320, 480)
Putting All Things Together
Finally, we define the following load_data_voc function to download and read the Pascal VOC2012 semantic segmentation dataset. It returns data iterators for both the training and test datasets.
#@save def load_data_voc(batch_size, crop_size):
"""Load the VOC semantic segmentation dataset.""" voc_dir = d2l.download_extract('voc2012',
os.path.join('VOCdevkit', 'VOC2012')) num_workers = d2l.get_dataloader_workers() train_iter = gluon.data.DataLoader(
VOCSegDataset(True, crop_size, voc_dir), batch_size, shuffle=True, last_batch='discard', num_workers=num_workers) test_iter = gluon.data.DataLoader( VOCSegDataset(False, crop_size, voc_dir), batch_size, last_batch='discard', num_workers=num_workers) return train_iter, test_iter

628

Chapter 13. Computer Vision

Summary
• Semantic segmentation recognizes and understands what are in an image in pixel level by dividing the image into regions belonging to different semantic classes.
• On of the most important semantic segmentation dataset is Pascal VOC2012. • In semantic segmentation, since the input image and label correspond one-to-one on the
pixel, the input image is randomly cropped to a fixed shape rather than rescaled.
Exercises
1. How can semantic segmentation be applied in autonomous vehicles and medical image diagnostics? Can you think of other applications?
2. Recall the descriptions of data augmentation in Section 13.1. Which of the image augmentation methods used in image classification would be infeasible to be applied in semantic segmentation?
Discussions185
13.10 Transposed Convolution
The CNN layers we have seen so far, such as convolutional layers (Section 6.2) and pooling layers (Section 6.5), typically reduce (downsample) the spatial dimensions (height and width) of the input, or keep them unchanged. In semantic segmentation that classifies at pixel-level, it will be convenient if the spatial dimensions of the input and output are the same. For example, the channel dimension at one output pixel can hold the classification results for the input pixel at the same spatial position. To achieve this, especially after the spatial dimensions are reduced by CNN layers, we can use another type of CNN layers that can increase (upsample) the spatial dimensions of intermediate feature maps. In this section, we will introduce transposed convolution, which is also called fractionally-strided convolution (Dumoulin & Visin, 2016), for reversing downsampling operations by the convolution.
from mxnet import init, np, npx from mxnet.gluon import nn from d2l import mxnet as d2l
npx.set_np()
185 https://discuss.d2l.ai/t/375

13.10. Transposed Convolution

629

13.10.1 Basic Operation
Ignoring channels for now, let us begin with the basic transposed convolution operation with stride of 1 and no padding. Suppose that we are given a nh × nw input tensor and a kh × kw kernel. Sliding the kernel window with stride of 1 for nw times in each row and nh times in each column yields a total of nhnw intermediate results. Each intermediate result is a (nh +kh −1)×(nw +kw −1) tensor that are initialized as zeros. To compute each intermediate tensor, each element in the input tensor is multiplied by the kernel so that the resulting kh ×kw tensor replaces a portion in each intermediate tensor. Note that the position of the replaced portion in each intermediate tensor corresponds to the position of the element in the input tensor used for the computation. In the end, all the intermediate results are summed over to produce the output.
As an example, Fig. 13.10.1 illustrates how transposed convolution with a 2 × 2 kernel is computed for a 2 × 2 input tensor.

Fig. 13.10.1: Transposed convolution with a 2 × 2 kernel. The shaded portions are a portion of an intermediate tensor as well as the input and kernel tensor elements used for the computation.
We can implement this basic transposed convolution operation trans_conv for a input matrix X and a kernel matrix K.
def trans_conv(X, K): h, w = K.shape Y = np.zeros((X.shape[0] + h - 1, X.shape[1] + w - 1)) for i in range(X.shape[0]): for j in range(X.shape[1]): Y[i:i + h, j:j + w] += X[i, j] * K return Y
In contrast to the regular convolution (in Section 6.2) that reduces input elements via the kernel, the transposed convolution broadcasts input elements via the kernel, thereby producing an output that is larger than the input. We can construct the input tensor X and the kernel tensor K from Fig. 13.10.1 to validate the output of the above implementation of the basic two-dimensional transposed convolution operation.
X = np.array([[0.0, 1.0], [2.0, 3.0]]) K = np.array([[0.0, 1.0], [2.0, 3.0]]) trans_conv(X, K)

630

Chapter 13. Computer Vision

array([[ 0., 0., [ 0., 4., [ 4., 12.,

1.], 6.], 9.]])

Alternatively, when the input X and kernel K are both four-dimensional tensors, we can use highlevel APIs to obtain the same results.
X, K = X.reshape(1, 1, 2, 2), K.reshape(1, 1, 2, 2) tconv = nn.Conv2DTranspose(1, kernel_size=2) tconv.initialize(init.Constant(K)) tconv(X)

array([[[[ 0., 0., [ 0., 4., [ 4., 12.,

1.], 6.], 9.]]]])

13.10.2 Padding, Strides, and Multiple Channels
Different from in the regular convolution where padding is applied to input, it is applied to output in the transposed convolution. For example, when specifying the padding number on either side of the height and width as 1, the first and last rows and columns will be removed from the transposed convolution output.
tconv = nn.Conv2DTranspose(1, kernel_size=2, padding=1) tconv.initialize(init.Constant(K)) tconv(X)
array([[[[4.]]]])
In the transposed convolution, strides are specified for intermediate results (thus output), not for input. Using the same input and kernel tensors from Fig. 13.10.1, changing the stride from 1 to 2 increases both the height and weight of intermediate tensors, hence the output tensor in Fig. 13.10.2.

13.10. Transposed Convolution

631

Fig. 13.10.2: Transposed convolution with a 2 × 2 kernel with stride of 2. The shaded portions are a portion of an intermediate tensor as well as the input and kernel tensor elements used for the computation.

The following code snippet can validate the transposed convolution output for stride of 2 in Fig. 13.10.2.
tconv = nn.Conv2DTranspose(1, kernel_size=2, strides=2) tconv.initialize(init.Constant(K)) tconv(X)

array([[[[0., 0., 0., 1.], [0., 0., 2., 3.], [0., 2., 0., 3.], [4., 6., 6., 9.]]]])

For multiple input and output channels, the transposed convolution works in the same way as the regular convolution. Suppose that the input has ci channels, and that the transposed convolution assigns a kh × kw kernel tensor to each input channel. When multiple output channels are specified, we will have a ci × kh × kw kernel for each output channel.
As in all, if we feed X into a convolutional layer f to output Y = f (X) and create a transposed convolutional layer g with the same hyperparameters as f except for the number of output channels being the number of channels in X, then g(Y ) will have the same shape as X. This can be illustrated in the following example.

X = np.random.uniform(size=(1, 10, 16, 16)) conv = nn.Conv2D(20, kernel_size=5, padding=2, strides=3) tconv = nn.Conv2DTranspose(10, kernel_size=5, padding=2, strides=3) conv.initialize()

(continues on next page)

632

Chapter 13. Computer Vision

tconv.initialize() tconv(conv(X)).shape == X.shape
True

(continued from previous page)

13.10.3 Connection to Matrix Transposition
The transposed convolution is named after the matrix transposition. To explain, let us first see how to implement convolutions using matrix multiplications. In the example below, we define a 3 × 3 input X and a 2 × 2 convolution kernel K, and then use the corr2d function to compute the convolution output Y.
X = np.arange(9.0).reshape(3, 3) K = np.array([[1.0, 2.0], [3.0, 4.0]]) Y = d2l.corr2d(X, K) Y
array([[27., 37.], [57., 67.]])
Next, we rewrite the convolution kernel K as a sparse weight matrix W containing a lot of zeros. The shape of the weight matrix is (4, 9), where the non-zero elements come from the convolution kernel K.
def kernel2matrix(K): k, W = np.zeros(5), np.zeros((4, 9)) k[:2], k[3:5] = K[0, :], K[1, :] W[0, :5], W[1, 1:6], W[2, 3:8], W[3, 4:] = k, k, k, k return W
W = kernel2matrix(K) W
array([[1., 2., 0., 3., 4., 0., 0., 0., 0.], [0., 1., 2., 0., 3., 4., 0., 0., 0.], [0., 0., 0., 1., 2., 0., 3., 4., 0.], [0., 0., 0., 0., 1., 2., 0., 3., 4.]])
Concatenate the input X row by row to get a vector of length 9. Then the matrix multiplication of W and the vectorized X gives a vector of length 4. After reshaping it, we can obtain the same result Y from the original convolution operation above: we just implemented convolutions using matrix multiplications.
Y == np.dot(W, X.reshape(-1)).reshape(2, 2)
array([[ True, True], [ True, True]])

13.10. Transposed Convolution

633

Likewise, we can implement transposed convolutions using matrix multiplications. In the following example, we take the 2 × 2 output Y from the above regular convolution as the input to the transposed convolution. To implement this operation by multiplying matrices, we only need to transpose the weight matrix W with the new shape (9, 4).
Z = trans_conv(Y, K) Z == np.dot(W.T, Y.reshape(-1)).reshape(3, 3)

array([[ True, [ True, [ True,

True, True, True,

True], True], True]])

Consider implementing the convolution by multiplying matrices. Given an input vector x and a
weight matrix W, the forward propagation function of the convolution can be implemented by
multiplying its input with the weight matrix and outputting a vector y = Wx. Since backpropagation follows the chain rule and ∇xy = W⊤, the backpropagation function of the convolution can be implemented by multiplying its input with the transposed weight matrix W⊤. Therefore, the
transposed convolutional layer can just exchange the forward propagation function and the back-
propagation function of the convolutional layer: its forward propagation and backpropagation functions multiply their input vector with W⊤ and W, respectively.

Summary
• In contrast to the regular convolution that reduces input elements via the kernel, the transposed convolution broadcasts input elements via the kernel, thereby producing an output that is larger than the input.
• If we feed X into a convolutional layer f to output Y = f (X) and create a transposed convolutional layer g with the same hyperparameters as f except for the number of output channels being the number of channels in X, then g(Y ) will have the same shape as X.
• We can implement convolutions using matrix multiplications. The transposed convolutional layer can just exchange the forward propagation function and the backpropagation function of the convolutional layer.

Exercises
1. In Section 13.10.3, the convolution input X and the transposed convolution output Z have the same shape. Do they have the same value? Why?
2. Is it efficient to use matrix multiplications to implement convolutions? Why? Discussions186
186 https://discuss.d2l.ai/t/376

634

Chapter 13. Computer Vision

13.11 Fully Convolutional Networks
As discussed in Section 13.9, semantic segmentation classifies images in pixel level. A fully convolutional network (FCN) uses a convolutional neural network to transform image pixels to pixel classes (Long et al., 2015). Unlike the CNNs that we encountered earlier for image classification or object detection, a fully convolutional network transforms the height and width of intermediate feature maps back to those of the input image: this is achieved by the transposed convolutional layer introduced in Section 13.10. As a result, the classification output and the input image have a one-to-one correspondence in pixel level: the channel dimension at any output pixel holds the classification results for the input pixel at the same spatial position.
%matplotlib inline from mxnet import gluon, image, init, np, npx from mxnet.gluon import nn from d2l import mxnet as d2l
npx.set_np()
13.11.1 The Model
Here we describe the basic design of the fully convolutional network model. As shown in Fig. 13.11.1, this model first uses a CNN to extract image features, then transforms the number of channels into the number of classes via a 1 × 1 convolutional layer, and finally transforms the height and width of the feature maps to those of the input image via the transposed convolution introduced in Section 13.10. As a result, the model output has the same height and width as the input image, where the output channel contains the predicted classes for the input pixel at the same spatial position.

Fig. 13.11.1: Fully convolutional network.

13.11. Fully Convolutional Networks

635

Below, we use a ResNet-18 model pretrained on the ImageNet dataset to extract image features and denote the model instance as pretrained_net. The last few layers of this model include a global average pooling layer and a fully-connected layer: they are not needed in the fully convolutional network.
pretrained_net = gluon.model_zoo.vision.resnet18_v2(pretrained=True) pretrained_net.features[-3:], pretrained_net.output
(HybridSequential( (0): Activation(relu) (1): GlobalAvgPool2D(size=(1, 1), stride=(1, 1), padding=(0, 0), ceil_mode=True, global_
→pool=True, pool_type=avg, layout=NCHW) (2): Flatten
), Dense(512 -> 1000, linear))
Next, we create the fully convolutional network instance net. It copies all the pretrained layers in the ResNet-18 except for the final global average pooling layer and the fully-connected layer that are closest to the output.
net = nn.HybridSequential() for layer in pretrained_net.features[:-2]:
net.add(layer)
Given an input with height and width of 320 and 480 respectively, the forward propagation of net reduces the input height and width to 1/32 of the original, namely 10 and 15.
X = np.random.uniform(size=(1, 3, 320, 480)) net(X).shape
(1, 512, 10, 15)
Next, we use a 1 × 1 convolutional layer to transform the number of output channels into the number of classes (21) of the Pascal VOC2012 dataset. Finally, we need to increase the height and width of the feature maps by 32 times to change them back to the height and width of the input image. Recall how to calculate the output shape of a convolutional layer in Section 6.3. Since (320 − 64 + 16 × 2 + 32)/32 = 10 and (480 − 64 + 16 × 2 + 32)/32 = 15, we construct a transposed convolutional layer with stride of 32, setting the height and width of the kernel to 64, the padding to 16. In general, we can see that for stride s, padding s/2 (assuming s/2 is an integer), and the height and width of the kernel 2s, the transposed convolution will increase the height and width of the input by s times.
num_classes = 21 net.add(
nn.Conv2D(num_classes, kernel_size=1), nn.Conv2DTranspose(num_classes, kernel_size=64, padding=16, strides=32))

636

Chapter 13. Computer Vision

13.11.2 Initializing Transposed Convolutional Layers
We already know that transposed convolutional layers can increase the height and width of feature maps. In image processing, we may need to scale up an image, i.e., upsampling. Bilinear interpolation is one of the commonly used upsampling techniques. It is also often used for initializing transposed convolutional layers.
To explain bilinear interpolation, say that given an input image we want to calculate each pixel of the upsampled output image. In order to calculate the pixel of the output image at coordinate (x, y), first map (x, y) to coordinate (x′, y′) on the input image, for example, according to the ratio of the input size to the output size. Note that the mapped x′ and y′ are real numbers. Then, find the four pixels closest to coordinate (x′, y′) on the input image. Finally, the pixel of the output image at coordinate (x, y) is calculated based on these four closest pixels on the input image and their relative distance from (x′, y′).
Upsampling of bilinear interpolation can be implemented by the transposed convolutional layer with the kernel constructed by the following bilinear_kernel function. Due to space limitations, we only provide the implementation of the bilinear_kernel function below without discussions on its algorithm design.
def bilinear_kernel(in_channels, out_channels, kernel_size): factor = (kernel_size + 1) // 2 if kernel_size % 2 == 1: center = factor - 1 else: center = factor - 0.5 og = (np.arange(kernel_size).reshape(-1, 1), np.arange(kernel_size).reshape(1, -1)) filt = (1 - np.abs(og[0] - center) / factor) * \ (1 - np.abs(og[1] - center) / factor) weight = np.zeros((in_channels, out_channels, kernel_size, kernel_size)) weight[range(in_channels), range(out_channels), :, :] = filt return np.array(weight)
Let us experiment with upsampling of bilinear interpolation that is implemented by a transposed convolutional layer. We construct a transposed convolutional layer that doubles the height and weight, and initialize its kernel with the bilinear_kernel function.
conv_trans = nn.Conv2DTranspose(3, kernel_size=4, padding=1, strides=2) conv_trans.initialize(init.Constant(bilinear_kernel(3, 3, 4)))
Read the image X and assign the upsampling output to Y. In order to print the image, we need to adjust the position of the channel dimension.
img = image.imread('../img/catdog.jpg') X = np.expand_dims(img.astype('float32').transpose(2, 0, 1), axis=0) / 255 Y = conv_trans(X) out_img = Y[0].transpose(1, 2, 0)
As we can see, the transposed convolutional layer increases both the height and width of the image by a factor of two. Except for the different scales in coordinates, the image scaled up by bilinear interpolation and the original image printed in Section 13.3 look the same.

13.11. Fully Convolutional Networks

637

d2l.set_figsize() print('input image shape:', img.shape) d2l.plt.imshow(img.asnumpy()) print('output image shape:', out_img.shape) d2l.plt.imshow(out_img.asnumpy());
input image shape: (561, 728, 3) output image shape: (1122, 1456, 3)

In a fully convolutional network, we initialize the transposed convolutional layer with upsampling of bilinear interpolation. For the 1 × 1 convolutional layer, we use Xavier initialization.
W = bilinear_kernel(num_classes, num_classes, 64) net[-1].initialize(init.Constant(W)) net[-2].initialize(init=init.Xavier())
13.11.3 Reading the Dataset
We read the semantic segmentation dataset as introduced in Section 13.9. The output image shape of random cropping is specified as 320 × 480: both the height and width are divisible by 32.
batch_size, crop_size = 32, (320, 480) train_iter, test_iter = d2l.load_data_voc(batch_size, crop_size)
read 1114 examples read 1078 examples

638

Chapter 13. Computer Vision

13.11.4 Training

Now we can train our constructed fully convolutional network. The loss function and accuracy calculation here are not essentially different from those in image classification of earlier chapters. Because we use the output channel of the transposed convolutional layer to predict the class for each pixel, the channel dimension is specified in the loss calculation. In addition, the accuracy is calculated based on correctness of the predicted class for all the pixels.

num_epochs, lr, wd, devices = 5, 0.1, 1e-3, d2l.try_all_gpus() loss = gluon.loss.SoftmaxCrossEntropyLoss(axis=1) net.collect_params().reset_ctx(devices) trainer = gluon.Trainer(net.collect_params(), 'sgd', {
'learning_rate': lr, 'wd': wd}) d2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs,

devices)

loss 0.323, train acc 0.893, test acc 0.848 189.5 examples/sec on [gpu(0), gpu(1)]

13.11.5 Prediction
When predicting, we need to standardize the input image in each channel and transform the image into the four-dimensional input format required by the CNN.
def predict(img): X = test_iter._dataset.normalize_image(img) X = np.expand_dims(X.transpose(2, 0, 1), axis=0) pred = net(X.as_in_ctx(devices[0])).argmax(axis=1) return pred.reshape(pred.shape[1], pred.shape[2])

To visualize the predicted class of each pixel, we map the predicted class back to its label color in the dataset.

def label2image(pred): colormap = np.array(d2l.VOC_COLORMAP, ctx=devices[0], dtype='uint8')

(continues on next page)

13.11. Fully Convolutional Networks

639

X = pred.astype('int32') return colormap[X, :]

(continued from previous page)

Images in the test dataset vary in size and shape. Since the model uses a transposed convolutional layer with stride of 32, when the height or width of an input image is indivisible by 32, the output height or width of the transposed convolutional layer will deviate from the shape of the input image. In order to address this issue, we can crop multiple rectangular areas with height and width that are integer multiples of 32 in the image, and perform forward propagation on the pixels in these areas separately. Note that the union of these rectangular areas needs to completely cover the input image. When a pixel is covered by multiple rectangular areas, the average of the transposed convolution outputs in separate areas for this same pixel can be input to the softmax operation to predict the class.
For simplicity, we only read a few larger test images, and crop a 320 × 480 area for prediction starting from the upper-left corner of an image. For these test images, we print their cropped areas, prediction results, and ground-truth row by row.
voc_dir = d2l.download_extract('voc2012', 'VOCdevkit/VOC2012') test_images, test_labels = d2l.read_voc_images(voc_dir, False) n, imgs = 4, [] for i in range(n):
crop_rect = (0, 0, 480, 320) X = image.fixed_crop(test_images[i], *crop_rect) pred = label2image(predict(X)) imgs += [X, pred, image.fixed_crop(test_labels[i], *crop_rect)] d2l.show_images(imgs[::3] + imgs[1::3] + imgs[2::3], 3, n, scale=2);

640

Chapter 13. Computer Vision

Summary
• The fully convolutional network first uses a CNN to extract image features, then transforms the number of channels into the number of classes via a 1×1 convolutional layer, and finally transforms the height and width of the feature maps to those of the input image via the transposed convolution.
• In a fully convolutional network, we can use upsampling of bilinear interpolation to initialize the transposed convolutional layer.
Exercises
1. If we use Xavier initialization for the transposed convolutional layer in the experiment, how does the result change?
2. Can you further improve the accuracy of the model by tuning the hyperparameters?
3. Predict the classes of all pixels in test images.
4. The original fully convolutional network paper also uses outputs of some intermediate CNN layers (Long et al., 2015). Try to implement this idea.
Discussions187
13.12 Neural Style Transfer
If you are a photography enthusiast, you may be familiar with the filter. It can change the color style of photos so that landscape photos become sharper or portrait photos have whitened skins. However, one filter usually only changes one aspect of the photo. To apply an ideal style to a photo, you probably need to try many different filter combinations. This process is as complex as tuning the hyperparameters of a model.
In this section, we will leverage layerwise representations of a CNN to automatically apply the style of one image to another image, i.e., style transfer (Gatys et al., 2016). This task needs two input images: one is the content image and the other is the style image. We will use neural networks to modify the content image to make it close to the style image in style. For example, the content image in Fig. 13.12.1 is a landscape photo taken by us in Mount Rainier National Park in the suburbs of Seattle, while the style image is an oil painting with the theme of autumn oak trees. In the output synthesized image, the oil brush strokes of the style image are applied, leading to more vivid colors, while preserving the main shape of the objects in the content image.
187 https://discuss.d2l.ai/t/377

13.12. Neural Style Transfer

641

Fig. 13.12.1: Given content and style images, style transfer outputs a synthesized image.
13.12.1 Method
Fig. 13.12.2 illustrates the CNN-based style transfer method with a simplified example. First, we initialize the synthesized image, for example, into the content image. This synthesized image is the only variable that needs to be updated during the style transfer process, i.e., the model parameters to be updated during training. Then we choose a pretrained CNN to extract image features and freeze its model parameters during training. This deep CNN uses multiple layers to extract hierarchical features for images. We can choose the output of some of these layers as content features or style features. Take Fig. 13.12.2 as an example. The pretrained neural network here has 3 convolutional layers, where the second layer outputs the content features, and the first and third layers output the style features.

Fig. 13.12.2: CNN-based style transfer process. Solid lines show the direction of forward propagation and dotted lines show backward propagation.

642

Chapter 13. Computer Vision

Next, we calculate the loss function of style transfer through forward propagation (direction of solid arrows), and update the model parameters (the synthesized image for output) through backpropagation (direction of dashed arrows). The loss function commonly used in style transfer consists of three parts: (i) content loss makes the synthesized image and the content image close in content features; (ii) style loss makes the synthesized image and style image close in style features; and (iii) total variation loss helps to reduce the noise in the synthesized image. Finally, when the model training is over, we output the model parameters of the style transfer to generate the final synthesized image.
In the following, we will explain the technical details of style transfer via a concrete experiment.
13.12.2 Reading the Content and Style Images
First, we read the content and style images. From their printed coordinate axes, we can tell that these images have different sizes.
%matplotlib inline from mxnet import autograd, gluon, image, init, np, npx from mxnet.gluon import nn from d2l import mxnet as d2l
npx.set_np()
d2l.set_figsize() content_img = image.imread('../img/rainier.jpg') d2l.plt.imshow(content_img.asnumpy());

style_img = image.imread('../img/autumn-oak.jpg') d2l.plt.imshow(style_img.asnumpy());

13.12. Neural Style Transfer

643

13.12.3 Preprocessing and Postprocessing
Below, we define two functions for preprocessing and postprocessing images. The preprocess function standardizes each of the three RGB channels of the input image and transforms the results into the CNN input format. The postprocess function restores the pixel values in the output image to their original values before standardization. Since the image printing function requires that each pixel has a floating point value from 0 to 1, we replace any value smaller than 0 or greater than 1 with 0 or 1, respectively.
rgb_mean = np.array([0.485, 0.456, 0.406]) rgb_std = np.array([0.229, 0.224, 0.225])
def preprocess(img, image_shape): img = image.imresize(img, *image_shape) img = (img.astype('float32') / 255 - rgb_mean) / rgb_std return np.expand_dims(img.transpose(2, 0, 1), axis=0)
def postprocess(img): img = img[0].as_in_ctx(rgb_std.ctx) return (img.transpose(1, 2, 0) * rgb_std + rgb_mean).clip(0, 1)

13.12.4 Extracting Features
We use the VGG-19 model pretrained on the ImageNet dataset to extract image features (Gatys et al., 2016).
pretrained_net = gluon.model_zoo.vision.vgg19(pretrained=True)
In order to extract the content features and style features of the image, we can select the output of certain layers in the VGG network. Generally speaking, the closer to the input layer, the easier to extract details of the image, and vice versa, the easier to extract the global information of the image. In order to avoid excessively retaining the details of the content image in the synthesized image, we choose a VGG layer that is closer to the output as the content layer to output the content features of the image. We also select the output of different VGG layers for extracting local and global style features. These layers are also called style layers. As mentioned in Section 7.2, the VGG network uses 5 convolutional blocks. In the experiment, we choose the last convolutional layer

644

Chapter 13. Computer Vision

of the fourth convolutional block as the content layer, and the first convolutional layer of each convolutional block as the style layer. The indices of these layers can be obtained by printing the pretrained_net instance.
style_layers, content_layers = [0, 5, 10, 19, 28], [25]
When extracting features using VGG layers, we only need to use all those from the input layer to the content layer or style layer that is closest to the output layer. Let us construct a new network instance net, which only retains all the VGG layers to be used for feature extraction.
net = nn.Sequential() for i in range(max(content_layers + style_layers) + 1):
net.add(pretrained_net.features[i])
Given the input X, if we simply invoke the forward propagation net(X), we can only get the output of the last layer. Since we also need the outputs of intermediate layers, we need to perform layerby-layer computation and keep the content and style layer outputs.
def extract_features(X, content_layers, style_layers): contents = [] styles = [] for i in range(len(net)): X = net[i](X) if i in style_layers: styles.append(X) if i in content_layers: contents.append(X) return contents, styles
Two functions are defined below: the get_contents function extracts content features from the content image, and the get_styles function extracts style features from the style image. Since there is no need to update the model parameters of the pretrained VGG during training, we can extract the content and the style features even before the training starts. Since the synthesized image is a set of model parameters to be updated for style transfer, we can only extract the content and style features of the synthesized image by calling the extract_features function during training.
def get_contents(image_shape, device): content_X = preprocess(content_img, image_shape).copyto(device) contents_Y, _ = extract_features(content_X, content_layers, style_layers) return content_X, contents_Y
def get_styles(image_shape, device): style_X = preprocess(style_img, image_shape).copyto(device) _, styles_Y = extract_features(style_X, content_layers, style_layers) return style_X, styles_Y

13.12. Neural Style Transfer

645

13.12.5 Defining the Loss Function
Now we will describe the loss function for style transfer. The loss function consists of the content loss, style loss, and total variation loss.
Content Loss
Similar to the loss function in linear regression, the content loss measures the difference in content features between the synthesized image and the content image via the squared loss function. The two inputs of the squared loss function are both outputs of the content layer computed by the extract_features function.
def content_loss(Y_hat, Y): return np.square(Y_hat - Y).mean()
Style Loss
Style loss, similar to content loss, also uses the squared loss function to measure the difference in style between the synthesized image and the style image. To express the style output of any style layer, we first use the extract_features function to compute the style layer output. Suppose that the output has 1 example, c channels, height h, and width w, we can transform this output into matrix X with c rows and hw columns. This matrix can be thought of as the concatenation of c vectors x1, . . . , xc, each of which has a length of hw. Here, vector xi represents the style feature of channel i. In the Gram matrix of these vectors XX⊤ ∈ Rc×c, element xij in row i and column j is the dot product of vectors xi and xj. It represents the correlation of the style features of channels i and j. We use this Gram matrix to represent the style output of any style layer. Note that when the value of hw is larger, it likely leads to larger values in the Gram matrix. Note also that the height and width of the Gram matrix are both the number of channels c. To allow style loss not to be affected by these values, the gram function below divides the Gram matrix by the number of its elements, i.e., chw.
def gram(X): num_channels, n = X.shape[1], X.size // X.shape[1] X = X.reshape((num_channels, n)) return np.dot(X, X.T) / (num_channels * n)
Obviously, the two Gram matrix inputs of the squared loss function for style loss are based on the style layer outputs for the synthesized image and the style image. It is assumed here that the Gram matrix gram_Y based on the style image has been precomputed.
def style_loss(Y_hat, gram_Y): return np.square(gram(Y_hat) - gram_Y).mean()

646

Chapter 13. Computer Vision

Total Variation Loss

Sometimes, the learned synthesized image has a lot of high-frequency noise, i.e., particularly bright or dark pixels. One common noise reduction method is total variation denoising. Denote by xi,j the pixel value at coordinate (i, j). Reducing total variation loss

∑ |xi,j − xi+1,j | + |xi,j − xi,j+1|
i,j

(13.12.1)

makes values of neighboring pixels on the synthesized image closer.

def tv_loss(Y_hat): return 0.5 * (np.abs(Y_hat[:, :, 1:, :] - Y_hat[:, :, :-1, :]).mean() + np.abs(Y_hat[:, :, :, 1:] - Y_hat[:, :, :, :-1]).mean())

Loss Function
The loss function of style transfer is the weighted sum of content loss, style loss, and total variation loss. By adjusting these weight hyperparameters, we can balance among content retention, style transfer, and noise reduction on the synthesized image.
content_weight, style_weight, tv_weight = 1, 1e3, 10
def compute_loss(X, contents_Y_hat, styles_Y_hat, contents_Y, styles_Y_gram): # Calculate the content, style, and total variance losses respectively contents_l = [ content_loss(Y_hat, Y) * content_weight for Y_hat, Y in zip(contents_Y_hat, contents_Y)] styles_l = [ style_loss(Y_hat, Y) * style_weight for Y_hat, Y in zip(styles_Y_hat, styles_Y_gram)] tv_l = tv_loss(X) * tv_weight # Add up all the losses l = sum(10 * styles_l + contents_l + [tv_l]) return contents_l, styles_l, tv_l, l

13.12.6 Initializing the Synthesized Image
In style transfer, the synthesized image is the only variable that needs to be updated during training. Thus, we can define a simple model, SynthesizedImage, and treat the synthesized image as the model parameters. In this model, forward propagation just returns the model parameters.
class SynthesizedImage(nn.Block): def __init__(self, img_shape, **kwargs): super(SynthesizedImage, self).__init__(**kwargs) self.weight = self.params.get('weight', shape=img_shape)
def forward(self): return self.weight.data()

13.12. Neural Style Transfer

647

Next, we define the get_inits function. This function creates a synthesized image model instance and initializes it to the image X. Gram matrices for the style image at various style layers, styles_Y_gram, are computed prior to training.
def get_inits(X, device, lr, styles_Y): gen_img = SynthesizedImage(X.shape) gen_img.initialize(init.Constant(X), ctx=device, force_reinit=True) trainer = gluon.Trainer(gen_img.collect_params(), 'adam', {'learning_rate': lr}) styles_Y_gram = [gram(Y) for Y in styles_Y] return gen_img(), styles_Y_gram, trainer
13.12.7 Training
When training the model for style transfer, we continuously extract content features and style features of the synthesized image, and calculate the loss function. Below defines the training loop.
def train(X, contents_Y, styles_Y, device, lr, num_epochs, lr_decay_epoch): X, styles_Y_gram, trainer = get_inits(X, device, lr, styles_Y) animator = d2l.Animator(xlabel='epoch', ylabel='loss', xlim=[10, num_epochs], ylim=[0, 20], legend=['content', 'style', 'TV'], ncols=2, figsize=(7, 2.5)) for epoch in range(num_epochs): with autograd.record(): contents_Y_hat, styles_Y_hat = extract_features( X, content_layers, style_layers) contents_l, styles_l, tv_l, l = compute_loss( X, contents_Y_hat, styles_Y_hat, contents_Y, styles_Y_gram) l.backward() trainer.step(1) if (epoch + 1) % lr_decay_epoch == 0: trainer.set_learning_rate(trainer.learning_rate * 0.8) if (epoch + 1) % 10 == 0: animator.axes[1].imshow(postprocess(X).asnumpy()) animator.add( epoch + 1, [float(sum(contents_l)), float(sum(styles_l)), float(tv_l)]) return X
Now we start to train the model. We rescale the height and width of the content and style images to 300 by 450 pixels. We use the content image to initialize the synthesized image.
device, image_shape = d2l.try_gpu(), (450, 300) net.collect_params().reset_ctx(device) content_X, contents_Y = get_contents(image_shape, device) _, styles_Y = get_styles(image_shape, device) output = train(content_X, contents_Y, styles_Y, device, 0.9, 500, 50)

648

Chapter 13. Computer Vision

We can see that the synthesized image retains the scenery and objects of the content image, and transfers the color of the style image at the same time. For example, the synthesized image has blocks of color like those in the style image. Some of these blocks even have the subtle texture of brush strokes.
Summary
• The loss function commonly used in style transfer consists of three parts: (i) content loss makes the synthesized image and the content image close in content features; (ii) style loss makes the synthesized image and style image close in style features; and (iii) total variation loss helps to reduce the noise in the synthesized image.
• We can use a pretrained CNN to extract image features and minimize the loss function to continuously update the synthesized image as model parameters during training.
• We use Gram matrices to represent the style outputs from the style layers.
Exercises
1. How does the output change when you select different content and style layers? 2. Adjust the weight hyperparameters in the loss function. Does the output retain more content
or have less noise? 3. Use different content and style images. Can you create more interesting synthesized images? 4. Can we apply style transfer for text? Hint: you may refer to the survey paper by Hu et al. (Hu
et al., 2020). Discussions188
188 https://discuss.d2l.ai/t/378

13.12. Neural Style Transfer

649

13.13 Image Classification (CIFAR-10) on Kaggle
So far, we have been using high-level APIs of deep learning frameworks to directly obtain image datasets in tensor format. However, custom image datasets often come in the form of image files. In this section, we will start from raw image files, and organize, read, then transform them into tensor format step by step.
We experimented with the CIFAR-10 dataset in Section 13.1, which is an important dataset in computer vision. In this section, we will apply the knowledge we learned in previous sections to practice the Kaggle competition of CIFAR-10 image classification. The web address of the competition is https://www.kaggle.com/c/cifar-10
Fig. 13.13.1 shows the information on the competitionʼs webpage. In order to submit the results, you need to register a Kaggle account.

Fig. 13.13.1: CIFAR-10 image classification competition webpage information. The competition dataset can be obtained by clicking the “Data” tab.
import collections import math import os import shutil import pandas as pd from mxnet import gluon, init, npx from mxnet.gluon import nn from d2l import mxnet as d2l
npx.set_np()

650

Chapter 13. Computer Vision

13.13.1 Obtaining and Organizing the Dataset
The competition dataset is divided into a training set and a test set, which contain 50000 and 300000 images, respectively. In the test set, 10000 images will be used for evaluation, while the remaining 290000 images will not be evaluated: they are included just to make it hard to cheat with manually labeled results of the test set. The images in this dataset are all png color (RGB channels) image files, whose height and width are both 32 pixels. The images cover a total of 10 categories, namely airplanes, cars, birds, cats, deer, dogs, frogs, horses, boats, and trucks. The upper-left corner of Fig. 13.13.1 shows some images of airplanes, cars, and birds in the dataset.
Downloading the Dataset
After logging in to Kaggle, we can click the “Data” tab on the CIFAR-10 image classification competition webpage shown in Fig. 13.13.1 and download the dataset by clicking the “Download All” button. After unzipping the downloaded file in ../data, and unzipping train.7z and test.7z inside it, you will find the entire dataset in the following paths:
• ../data/cifar-10/train/[1-50000].png
• ../data/cifar-10/test/[1-300000].png
• ../data/cifar-10/trainLabels.csv
• ../data/cifar-10/sampleSubmission.csv
where the train and test directories contain the training and testing images, respectively, trainLabels.csv provides labels for the training images, and sample_submission.csv is a sample submission file.
To make it easier to get started, we provide a small-scale sample of the dataset that contains the first 1000 training images and 5 random testing images. To use the full dataset of the Kaggle competition, you need to set the following demo variable to False.
#@save d2l.DATA_HUB['cifar10_tiny'] = (d2l.DATA_URL + 'kaggle_cifar10_tiny.zip',
'2068874e4b9a9f0fb07ebe0ad2b29754449ccacd')
# If you use the full dataset downloaded for the Kaggle competition, set # `demo` to False demo = True
if demo: data_dir = d2l.download_extract('cifar10_tiny')
else: data_dir = '../data/cifar-10/'
Downloading ../data/kaggle_cifar10_tiny.zip from http://d2l-data.s3-accelerate.amazonaws.com/ →kaggle_cifar10_tiny.zip...

13.13. Image Classification (CIFAR-10) on Kaggle

651

Organizing the Dataset
We need to organize datasets to facilitate model training and testing. Let us first read the labels from the csv file. The following function returns a dictionary that maps the non-extension part of the filename to its label.
#@save def read_csv_labels(fname):
"""Read `fname` to return a filename to label dictionary.""" with open(fname, 'r') as f:
# Skip the file header line (column name) lines = f.readlines()[1:] tokens = [l.rstrip().split(',') for l in lines] return dict(((name, label) for name, label in tokens))
labels = read_csv_labels(os.path.join(data_dir, 'trainLabels.csv')) print('# training examples:', len(labels)) print('# classes:', len(set(labels.values())))
# training examples: 1000 # classes: 10
Next, we define the reorg_train_valid function to split the validation set out of the original training set. The argument valid_ratio in this function is the ratio of the number of examples in the validation set to the number of examples in the original training set. More concretely, let n be the number of images of the class with the least examples, and r be the ratio. The validation set will split out max(⌊nr⌋, 1) images for each class. Let us use valid_ratio=0.1 as an example. Since the original training set has 50000 images, there will be 45000 images used for training in the path train_valid_test/train, while the other 5000 images will be split out as validation set in the path train_valid_test/valid. After organizing the dataset, images of the same class will be placed under the same folder.
#@save def copyfile(filename, target_dir):
"""Copy a file into a target directory.""" os.makedirs(target_dir, exist_ok=True) shutil.copy(filename, target_dir)
#@save def reorg_train_valid(data_dir, labels, valid_ratio):
"""Split the validation set out of the original training set.""" # The number of examples of the class that has the fewest examples in the # training dataset n = collections.Counter(labels.values()).most_common()[-1][1] # The number of examples per class for the validation set n_valid_per_label = max(1, math.floor(n * valid_ratio)) label_count = {} for train_file in os.listdir(os.path.join(data_dir, 'train')):
label = labels[train_file.split('.')[0]] fname = os.path.join(data_dir, 'train', train_file) copyfile(
fname, os.path.join(data_dir, 'train_valid_test', 'train_valid', label))
(continues on next page)

652

Chapter 13. Computer Vision

(continued from previous page)
if label not in label_count or label_count[label] < n_valid_per_label: copyfile( fname, os.path.join(data_dir, 'train_valid_test', 'valid', label)) label_count[label] = label_count.get(label, 0) + 1
else: copyfile( fname, os.path.join(data_dir, 'train_valid_test', 'train', label))
return n_valid_per_label
The reorg_test function below organizes the testing set for data loading during prediction.
#@save def reorg_test(data_dir):
"""Organize the testing set for data loading during prediction.""" for test_file in os.listdir(os.path.join(data_dir, 'test')):
copyfile( os.path.join(data_dir, 'test', test_file), os.path.join(data_dir, 'train_valid_test', 'test', 'unknown'))
Finally, we use a function to invoke the read_csv_labels, reorg_train_valid, and reorg_test functions defined above.
def reorg_cifar10_data(data_dir, valid_ratio): labels = read_csv_labels(os.path.join(data_dir, 'trainLabels.csv')) reorg_train_valid(data_dir, labels, valid_ratio) reorg_test(data_dir)
Here we only set the batch size to 32 for the small-scale sample of the dataset. When training and testing the complete dataset of the Kaggle competition, batch_size should be set to a larger integer, such as 128. We split out 10% of the training examples as the validation set for tuning hyperparameters.
batch_size = 32 if demo else 128 valid_ratio = 0.1 reorg_cifar10_data(data_dir, valid_ratio)

13.13.2 Image Augmentation
We use image augmentation to address overfitting. For example, images can be flipped horizontally at random during training. We can also perform standardization for the three RGB channels of color images. Below lists some of these operations that you can tweak.
transform_train = gluon.data.vision.transforms.Compose([ # Scale the image up to a square of 40 pixels in both height and width gluon.data.vision.transforms.Resize(40), # Randomly crop a square image of 40 pixels in both height and width to # produce a small square of 0.64 to 1 times the area of the original # image, and then scale it to a square of 32 pixels in both height and
(continues on next page)

13.13. Image Classification (CIFAR-10) on Kaggle

653

(continued from previous page) # width gluon.data.vision.transforms.RandomResizedCrop(32, scale=(0.64, 1.0),
ratio=(1.0, 1.0)), gluon.data.vision.transforms.RandomFlipLeftRight(), gluon.data.vision.transforms.ToTensor(), # Standardize each channel of the image gluon.data.vision.transforms.Normalize([0.4914, 0.4822, 0.4465],
[0.2023, 0.1994, 0.2010])])
During testing, we only perform standardization on images so as to remove randomness in the evaluation results.
transform_test = gluon.data.vision.transforms.Compose([ gluon.data.vision.transforms.ToTensor(), gluon.data.vision.transforms.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010])])
13.13.3 Reading the Dataset
Next, we read the organized dataset consisting of raw image files. Each example includes an image and a label.
train_ds, valid_ds, train_valid_ds, test_ds = [ gluon.data.vision.ImageFolderDataset( os.path.join(data_dir, 'train_valid_test', folder)) for folder in ['train', 'valid', 'train_valid', 'test']]
During training, we need to specify all the image augmentation operations defined above. When the validation set is used for model evaluation during hyperparameter tuning, no randomness from image augmentation should be introduced. Before final prediction, we train the model on the combined training set and validation set to make full use of all the labeled data.
train_iter, train_valid_iter = [ gluon.data.DataLoader(dataset.transform_first(transform_train), batch_size, shuffle=True, last_batch='discard') for dataset in (train_ds, train_valid_ds)]
valid_iter = gluon.data.DataLoader(valid_ds.transform_first(transform_test), batch_size, shuffle=False, last_batch='discard')
test_iter = gluon.data.DataLoader(test_ds.transform_first(transform_test), batch_size, shuffle=False, last_batch='keep')

654

Chapter 13. Computer Vision

13.13.4 Defining the Model
Here, we build the residual blocks based on the HybridBlock class, which is slightly different from the implementation described in Section 7.6. This is for improving computational efficiency.
class Residual(nn.HybridBlock): def __init__(self, num_channels, use_1x1conv=False, strides=1, **kwargs): super(Residual, self).__init__(**kwargs) self.conv1 = nn.Conv2D(num_channels, kernel_size=3, padding=1, strides=strides) self.conv2 = nn.Conv2D(num_channels, kernel_size=3, padding=1) if use_1x1conv: self.conv3 = nn.Conv2D(num_channels, kernel_size=1, strides=strides) else: self.conv3 = None self.bn1 = nn.BatchNorm() self.bn2 = nn.BatchNorm()
def hybrid_forward(self, F, X): Y = F.npx.relu(self.bn1(self.conv1(X))) Y = self.bn2(self.conv2(Y)) if self.conv3: X = self.conv3(X) return F.npx.relu(Y + X)

Next, we define the ResNet-18 model.
def resnet18(num_classes): net = nn.HybridSequential() net.add(nn.Conv2D(64, kernel_size=3, strides=1, padding=1), nn.BatchNorm(), nn.Activation('relu'))
def resnet_block(num_channels, num_residuals, first_block=False): blk = nn.HybridSequential() for i in range(num_residuals): if i == 0 and not first_block: blk.add(Residual(num_channels, use_1x1conv=True, strides=2)) else: blk.add(Residual(num_channels)) return blk
net.add(resnet_block(64, 2, first_block=True), resnet_block(128, 2), resnet_block(256, 2), resnet_block(512, 2))
net.add(nn.GlobalAvgPool2D(), nn.Dense(num_classes)) return net

We use Xavier initialization described in Section 4.8.2 before training begins.
def get_net(devices): num_classes = 10 net = resnet18(num_classes) net.initialize(ctx=devices, init=init.Xavier()) return net

(continues on next page)

13.13. Image Classification (CIFAR-10) on Kaggle

655

loss = gluon.loss.SoftmaxCrossEntropyLoss()

(continued from previous page)

13.13.5 Defining the Training Function
We will select models and tune hyperparameters according to the modelʼs performance on the validation set. In the following, we define the model training function train.
def train(net, train_iter, valid_iter, num_epochs, lr, wd, devices, lr_period, lr_decay):
trainer = gluon.Trainer(net.collect_params(), 'sgd', { 'learning_rate': lr, 'momentum': 0.9, 'wd': wd})
num_batches, timer = len(train_iter), d2l.Timer() legend = ['train loss', 'train acc'] if valid_iter is not None:
legend.append('valid acc') animator = d2l.Animator(xlabel='epoch', xlim=[1, num_epochs],
legend=legend) for epoch in range(num_epochs):
metric = d2l.Accumulator(3) if epoch > 0 and epoch % lr_period == 0:
trainer.set_learning_rate(trainer.learning_rate * lr_decay) for i, (features, labels) in enumerate(train_iter):
timer.start() l, acc = d2l.train_batch_ch13(net, features,
labels.astype('float32'), loss, trainer, devices, d2l.split_batch) metric.add(l, acc, labels.shape[0]) timer.stop() if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1: animator.add( epoch + (i + 1) / num_batches, (metric[0] / metric[2], metric[1] / metric[2], None)) if valid_iter is not None: valid_acc = d2l.evaluate_accuracy_gpus(net, valid_iter,
d2l.split_batch) animator.add(epoch + 1, (None, None, valid_acc)) measures = (f'train loss {metric[0] / metric[2]:.3f}, '
f'train acc {metric[1] / metric[2]:.3f}') if valid_iter is not None:
measures += f', valid acc {valid_acc:.3f}' print(measures + f'\n{metric[2] * num_epochs / timer.sum():.1f}'
f' examples/sec on {str(devices)}')

656

Chapter 13. Computer Vision

13.13.6 Training and Validating the Model
Now, we can train and validate the model. All the following hyperparameters can be tuned. For example, we can increase the number of epochs. When lr_period and lr_decay are set to 4 and 0.9, respectively, the learning rate of the optimization algorithm will be multiplied by 0.9 after every 4 epochs. Just for ease of demonstration, we only train 20 epochs here.
devices, num_epochs, lr, wd = d2l.try_all_gpus(), 20, 0.02, 5e-4 lr_period, lr_decay, net = 4, 0.9, get_net(devices) net.hybridize() train(net, train_iter, valid_iter, num_epochs, lr, wd, devices, lr_period,
lr_decay)
train loss 0.849, train acc 0.704, valid acc 0.469 558.1 examples/sec on [gpu(0), gpu(1)]

13.13.7 Classifying the Testing Set and Submitting Results on Kaggle
After obtaining a promising model with hyperparameters, we use all the labeled data (including the validation set) to retrain the model and classify the testing set.
net, preds = get_net(devices), [] net.hybridize() train(net, train_valid_iter, None, num_epochs, lr, wd, devices, lr_period,
lr_decay)
for X, _ in test_iter: y_hat = net(X.as_in_ctx(devices[0])) preds.extend(y_hat.argmax(axis=1).astype(int).asnumpy())
sorted_ids = list(range(1, len(test_ds) + 1)) sorted_ids.sort(key=lambda x: str(x)) df = pd.DataFrame({'id': sorted_ids, 'label': preds}) df['label'] = df['label'].apply(lambda x: train_valid_ds.synsets[x]) df.to_csv('submission.csv', index=False)

13.13. Image Classification (CIFAR-10) on Kaggle

657

train loss 0.745, train acc 0.723 514.6 examples/sec on [gpu(0), gpu(1)]

The above code will generate a submission.csv file, whose format meets the requirement of the Kaggle competition. The method for submitting results to Kaggle is similar to that in Section 4.10.
Summary
• We can read datasets containing raw image files after organizing them into the required format.
• We can use convolutional neural networks, image augmentation, and hybrid programing in an image classification competition.
Exercises
1. Use the complete CIFAR-10 dataset for this Kaggle competition. Set hyperparameters as batch_size = 128, num_epochs = 100, lr = 0.1, lr_period = 50, and lr_decay = 0.1. See what accuracy and ranking you can achieve in this competition. Can you further improve them?
2. What accuracy can you get when not using image augmentation? Discussions189
189 https://discuss.d2l.ai/t/379

658

Chapter 13. Computer Vision

13.14 Dog Breed Identification (ImageNet Dogs) on Kaggle
In this section, we will practice the dog breed identification problem on Kaggle. The web address of this competition is https://www.kaggle.com/c/dog-breed-identification
In this competition, 120 different breeds of dogs will be recognized. In fact, the dataset for this competition is a subset of the ImageNet dataset. Unlike the images in the CIFAR-10 dataset in Section 13.13, the images in the ImageNet dataset are both higher and wider in varying dimensions. Fig. 13.14.1 shows the information on the competitionʼs webpage. You need a Kaggle account to submit your results.

Fig. 13.14.1: The dog breed identification competition website. The competition dataset can be obtained by clicking the “Data” tab.
import os from mxnet import autograd, gluon, init, npx from mxnet.gluon import nn from d2l import mxnet as d2l
npx.set_np()

13.14. Dog Breed Identification (ImageNet Dogs) on Kaggle

659

13.14.1 Obtaining and Organizing the Dataset
The competition dataset is divided into a training set and a test set, which contain 10222 and 10357 JPEG images of three RGB (color) channels, respectively. Among the training dataset, there are 120 breeds of dogs such as Labradors, Poodles, Dachshunds, Samoyeds, Huskies, Chihuahuas, and Yorkshire Terriers.
Downloading the Dataset
After logging into Kaggle, you can click on the “Data” tab on the competition webpage shown in Fig. 13.14.1 and download the dataset by clicking the “Download All” button. After unzipping the downloaded file in ../data, you will find the entire dataset in the following paths:
• ../data/dog-breed-identification/labels.csv
• ../data/dog-breed-identification/sample_submission.csv
• ../data/dog-breed-identification/train
• ../data/dog-breed-identification/test
You may have noticed that the above structure is similar to that of the CIFAR-10 competition in Section 13.13, where folders train/ and test/ contain training and testing dog images, respectively, and labels.csv contains the labels for the training images. Similarly, to make it easier to get started, we provide a small sample of the dataset mentioned above: train_valid_test_tiny. zip. If you are going to use the full dataset for the Kaggle competition, you need to change the demo variable below to False.
#@save d2l.DATA_HUB['dog_tiny'] = (d2l.DATA_URL + 'kaggle_dog_tiny.zip',
'0cb91d09b814ecdc07b50f31f8dcad3e81d6a86d')
# If you use the full dataset downloaded for the Kaggle competition, change # the variable below to `False` demo = True if demo:
data_dir = d2l.download_extract('dog_tiny') else:
data_dir = os.path.join('..', 'data', 'dog-breed-identification')

Organizing the Dataset
We can organize the dataset similarly to what we did in Section 13.13, namely splitting out a validation set from the original training set, and moving images into subfolders grouped by labels.
The reorg_dog_data function below reads the training data labels, splits out the validation set, and organizes the training set.
def reorg_dog_data(data_dir, valid_ratio): labels = d2l.read_csv_labels(os.path.join(data_dir, 'labels.csv')) d2l.reorg_train_valid(data_dir, labels, valid_ratio) d2l.reorg_test(data_dir)

(continues on next page)

660

Chapter 13. Computer Vision

batch_size = 32 if demo else 128 valid_ratio = 0.1 reorg_dog_data(data_dir, valid_ratio)

(continued from previous page)

13.14.2 Image Augmentation
Recall that this dog breed dataset is a subset of the ImageNet dataset, whose images are larger than those of the CIFAR-10 dataset in Section 13.13. The following lists a few image augmentation operations that might be useful for relatively larger images.
transform_train = gluon.data.vision.transforms.Compose([ # Randomly crop the image to obtain an image with an area of 0.08 to 1 of # the original area and height-to-width ratio between 3/4 and 4/3. Then, # scale the image to create a new 224 x 224 image gluon.data.vision.transforms.RandomResizedCrop( 224, scale=(0.08, 1.0), ratio=(3.0 / 4.0, 4.0 / 3.0)), gluon.data.vision.transforms.RandomFlipLeftRight(), # Randomly change the brightness, contrast, and saturation gluon.data.vision.transforms.RandomColorJitter(brightness=0.4, contrast=0.4, saturation=0.4), # Add random noise gluon.data.vision.transforms.RandomLighting(0.1), gluon.data.vision.transforms.ToTensor(), # Standardize each channel of the image gluon.data.vision.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

During prediction, we only use image preprocessing operations without randomness.

transform_test = gluon.data.vision.transforms.Compose([ gluon.data.vision.transforms.Resize(256), # Crop a 224 x 224 square area from the center of the gluon.data.vision.transforms.CenterCrop(224), gluon.data.vision.transforms.ToTensor(), gluon.data.vision.transforms.Normalize([0.485, 0.456, [0.229, 0.224,

image
0.406], 0.225])])

13.14.3 Reading the Dataset
As in Section 13.13, we can read the organized dataset consisting of raw image files.
train_ds, valid_ds, train_valid_ds, test_ds = [ gluon.data.vision.ImageFolderDataset( os.path.join(data_dir, 'train_valid_test', folder)) for folder in ('train', 'valid', 'train_valid', 'test')]
Below we create data iterator instances the same way as in Section 13.13.

13.14. Dog Breed Identification (ImageNet Dogs) on Kaggle

661

train_iter, train_valid_iter = [ gluon.data.DataLoader(dataset.transform_first(transform_train), batch_size, shuffle=True, last_batch='discard') for dataset in (train_ds, train_valid_ds)]
valid_iter = gluon.data.DataLoader(valid_ds.transform_first(transform_test), batch_size, shuffle=False, last_batch='discard')
test_iter = gluon.data.DataLoader(test_ds.transform_first(transform_test), batch_size, shuffle=False, last_batch='keep')

13.14.4 Fine-Tuning a Pretrained Model
Again, the dataset for this competition is a subset of the ImageNet dataset. Therefore, we can use the approach discussed in Section 13.2 to select a model pretrained on the full ImageNet dataset and use it to extract image features to be fed into a custom small-scale output network. High-level APIs of deep learning frameworks provide a wide range of models pretrained on the ImageNet dataset. Here, we choose a pretrained ResNet-34 model, where we simply reuse the input of this modelʼs output layer (i.e., the extracted features). Then we can replace the original output layer with a small custom output network that can be trained, such as stacking two fully-connected layers. Different from the experiment in Section 13.2, the following does not retrain the pretrained model used for feature extraction. This reduces training time and memory for storing gradients.
Recall that we standardized images using the means and standard deviations of the three RGB channels for the full ImageNet dataset. In fact, this is also consistent with the standardization operation by the pretrained model on ImageNet.
def get_net(devices): finetune_net = gluon.model_zoo.vision.resnet34_v2(pretrained=True) # Define a new output network finetune_net.output_new = nn.HybridSequential(prefix='') finetune_net.output_new.add(nn.Dense(256, activation='relu')) # There are 120 output categories finetune_net.output_new.add(nn.Dense(120)) # Initialize the output network finetune_net.output_new.initialize(init.Xavier(), ctx=devices) # Distribute the model parameters to the CPUs or GPUs used for computation finetune_net.collect_params().reset_ctx(devices) return finetune_net

Before calculating the loss, we first obtain the input of the pretrained modelʼs output layer, i.e., the extracted feature. Then we use this feature as the input for our small custom output network to calculate the loss.

loss = gluon.loss.SoftmaxCrossEntropyLoss()

def evaluate_loss(data_iter, net, devices): l_sum, n = 0.0, 0 for features, labels in data_iter: X_shards, y_shards = d2l.split_batch(features, labels, devices)

(continues on next page)

662

Chapter 13. Computer Vision

(continued from previous page)
output_features = [net.features(X_shard) for X_shard in X_shards] outputs = [net.output_new(feature) for feature in output_features] ls = [
loss(output, y_shard).sum() for output, y_shard in zip(outputs, y_shards)] l_sum += sum([float(l.sum()) for l in ls]) n += labels.size return l_sum / n

13.14.5 Defining the Training Function
We will select the model and tune hyperparameters according to the modelʼs performance on the validation set. The model training function train only iterates parameters of the small custom output network.
def train(net, train_iter, valid_iter, num_epochs, lr, wd, devices, lr_period, lr_decay):
# Only train the small custom output network trainer = gluon.Trainer(net.output_new.collect_params(), 'sgd', {
'learning_rate': lr, 'momentum': 0.9, 'wd': wd}) num_batches, timer = len(train_iter), d2l.Timer() legend = ['train loss'] if valid_iter is not None: legend.append('valid loss') animator = d2l.Animator(xlabel='epoch', xlim=[1, num_epochs],
legend=legend) for epoch in range(num_epochs):
metric = d2l.Accumulator(2) if epoch > 0 and epoch % lr_period == 0:
trainer.set_learning_rate(trainer.learning_rate * lr_decay) for i, (features, labels) in enumerate(train_iter):
timer.start() X_shards, y_shards = d2l.split_batch(features, labels, devices) output_features = [net.features(X_shard) for X_shard in X_shards] with autograd.record():
outputs = [ net.output_new(feature) for feature in output_features]
ls = [ loss(output, y_shard).sum() for output, y_shard in zip(outputs, y_shards)]
for l in ls: l.backward()
trainer.step(batch_size) metric.add(sum([float(l.sum()) for l in ls]), labels.shape[0]) timer.stop() if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:
animator.add(epoch + (i + 1) / num_batches, (metric[0] / metric[1], None))
if valid_iter is not None: valid_loss = evaluate_loss(valid_iter, net, devices) animator.add(epoch + 1, (None, valid_loss))
(continues on next page)

13.14. Dog Breed Identification (ImageNet Dogs) on Kaggle

663

(continued from previous page) measures = f'train loss {metric[0] / metric[1]:.3f}' if valid_iter is not None:
measures += f', valid loss {valid_loss:.3f}' print(measures + f'\n{metric[1] * num_epochs / timer.sum():.1f}'
f' examples/sec on {str(devices)}')
13.14.6 Training and Validating the Model
Now we can train and validate the model. The following hyperparameters are all tunable. For example, the number of epochs can be increased. Because lr_period and lr_decay are set to 2 and 0.9, respectively, the learning rate of the optimization algorithm will be multiplied by 0.9 after every 2 epochs.
devices, num_epochs, lr, wd = d2l.try_all_gpus(), 10, 5e-3, 1e-4 lr_period, lr_decay, net = 2, 0.9, get_net(devices) net.hybridize() train(net, train_iter, valid_iter, num_epochs, lr, wd, devices, lr_period,
lr_decay)
train loss 0.964, valid loss 0.817 218.0 examples/sec on [gpu(0), gpu(1)]

13.14.7 Classifying the Testing Set and Submitting Results on Kaggle
Similar to the final step in Section 13.13, in the end all the labeled data (including the validation set) are used for training the model and classifying the testing set. We will use the trained custom output network for classification.
net = get_net(devices) net.hybridize() train(net, train_valid_iter, None, num_epochs, lr, wd, devices, lr_period,
lr_decay)

(continues on next page)

664

Chapter 13. Computer Vision

(continued from previous page)

preds = [] for data, label in test_iter:
output_features = net.features(data.as_in_ctx(devices[0])) output = npx.softmax(net.output_new(output_features)) preds.extend(output.asnumpy()) ids = sorted( os.listdir(os.path.join(data_dir, 'train_valid_test', 'test', with open('submission.csv', 'w') as f: f.write('id,' + ','.join(train_valid_ds.synsets) + '\n') for i, output in zip(ids, preds):
f.write( i.split('.')[0] + ',' + ','.join([str(num) for num in output])

'unknown'))) + '\n')

train loss 0.878 281.8 examples/sec on [gpu(0), gpu(1)]

The above code will generate a submission.csv file to be submitted to Kaggle in the same way described in Section 4.10.
Summary
• Images in the ImageNet dataset are larger (with varying dimensions) than CIFAR-10 images. We may modify image augmentation operations for tasks on a different dataset.
• To classify a subset of the ImageNet dataset, we can leverage pre-trained models on the full ImageNet dataset to extract features and only train a custom small-scale output network. This will lead to less computational time and memory cost.

13.14. Dog Breed Identification (ImageNet Dogs) on Kaggle

665

Exercises
1. When using the full Kaggle competition dataset, what results can you achieve when you increase batch_size (batch size) and num_epochs (number of epochs) while setting some other hyperparameters as lr = 0.01, lr_period = 10, and lr_decay = 0.1?
2. Do you get better results if you use a deeper pretrained model? How do you tune hyperparameters? Can you further improve the results?
Discussions190

190 https://discuss.d2l.ai/t/380
666

Chapter 13. Computer Vision

14 | Natural Language Processing: Pretraining
Humans need to communicate. Out of this basic need of the human condition, a vast amount of written text has been generated on an everyday basis. Given rich text in social media, chat apps, emails, product reviews, news articles, research papers, and books, it becomes vital to enable computers to understand them to offer assistance or make decisions based on human languages. Natural language processing studies interactions between computers and humans using natural languages. In practice, it is very common to use natural language processing techniques to process and analyze text (human natural language) data, such as language models in Section 8.3 and machine translation models in Section 9.5. To understand text, we can begin by learning its representations. Leveraging the existing text sequences from large corpora, self-supervised learning has been extensively used to pretrain text representations, such as by predicting some hidden part of the text using some other part of their surrounding text. In this way, models learn through supervision from massive text data without expensive labeling efforts! As we will see in this chapter, when treating each word or subword as an individual token, the representation of each token can be pretrained using word2vec, GloVe, or subword embedding models on large corpora. After pretraining, representation of each token can be a vector, however, it remains the same no matter what the context is. For instance, the vector representation of “bank” is the same in both “go to the bank to deposit some money” and “go to the bank to sit down”. Thus, many more recent pretraining models adapt representation of the same token to different contexts. Among them is BERT, a much deeper self-supervised model based on the transformer encoder. In this chapter, we will focus on how to pretrain such representations for text, as highlighted in Fig. 14.1.
667

Fig. 14.1: Pretrained text representations can be fed to various deep learning architectures for different downstream natural language processing applications. This chapter focuses on the upstream text representation pretraining.
For sight of the big picture, Fig. 14.1 shows that the pretrained text representations can be fed to a variety of deep learning architectures for different downstream natural language processing applications. We will cover them in Chapter 15.

14.1 Word Embedding (word2vec)
Natural language is a complex system used to express meanings. In this system, words are the basic unit of the meaning. As the name implies, word vectors are vectors used to represent words, and can also be considered as feature vectors or representations of words. The technique of mapping words to real vectors is called word embedding. In recent years, word embedding has gradually become the basic knowledge of natural language processing.

14.1.1 One-Hot Vectors Are a Bad Choice

We used one-hot vectors to represent words (characters are words) in Section 8.5. Suppose that the number of different words in the dictionary (the dictionary size) is N , and each word corresponds to a different integer (index) from 0 to N −1. To obtain the one-hot vector representation for any word with index i, we create a length-N vector with all 0s and set the element at position i to 1. In this way, each word is represented as a vector of length N , and it can be used directly by neural networks.

Although one-hot word vectors are easy to construct, they are usually not a good choice. A main reason is that one-hot word vectors cannot accurately express the similarity between different words, such as the cosine similarity that we often use. For vectors x, y ∈ Rd, their cosine similarity is the cosine of the angle between them:

x⊤y ∥x∥∥y∥ ∈ [−1, 1].

(14.1.1)

Since the cosine similarity between one-hot vectors of any two different words is 0, one-hot vectors cannot encode similarities among words.

668

Chapter 14. Natural Language Processing: Pretraining

14.1.2 Self-Supervised word2vec
The word2vec191 tool was proposed to address the above issue. It maps each word to a fixed-length vector, and these vectors can better express the similarity and analogy relationship among different words. The word2vec tool contains two models, namely skip-gram (Mikolov et al., 2013b) and continuous bag of words (CBOW) (Mikolov et al., 2013a). For semantically meaningful representations, their training relies on conditional probabilities that can be viewed as predicting some words using some of their surrounding words in corpora. Since supervision comes from the data without labels, both skip-gram and continuous bag of words are self-supervised models.
In the following, we will introduce these two models and their training methods.

14.1.3 The Skip-Gram Model

The skip-gram model assumes that a word can be used to generate its surrounding words in a text sequence. Take the text sequence “the”, “man”, “loves”, “his”, “son” as an example. Let us choose “loves” as the center word and set the context window size to 2. As shown in Fig. 14.1.1, given the center word “loves”, the skip-gram model considers the conditional probability for generating the context words: “the”, “man”, “his”, and “son”, which are no more than 2 words away from the center word:

P ("the", "man", "his", "son" | "loves").

(14.1.2)

Assume that the context words are independently generated given the center word (i.e., conditional independence). In this case, the above conditional probability can be rewritten as

P ("the" | "loves") · P ("man" | "loves") · P ("his" | "loves") · P ("son" | "loves").

(14.1.3)

Fig. 14.1.1: The skip-gram model considers the conditional probability of generating the surrounding context words given a center word.

In the skip-gram model, each word has two d-dimensional-vector representations for calculating conditional probabilities. More concretely, for any word with index i in the dictionary, denote by vi ∈ Rd and ui ∈ Rd its two vectors when used as a center word and a context word, respectively. The conditional probability of generating any context word wo (with index o in the dictionary) given the center word wc (with index c in the dictionary) can be modeled by a softmax operation on vector dot products:

P (wo | wc) = ∑ exp(u⊤o vc) , i∈V exp(u⊤i vc)

(14.1.4)

191 https://code.google.com/archive/p/word2vec/

14.1. Word Embedding (word2vec)

669

where the vocabulary index set V = {0, 1, . . . , |V| − 1}. Given a text sequence of length T , where the word at time step t is denoted as w(t). Assume that context words are independently generated given any center word. For context window size m, the likelihood function of the skip-gram model is the probability of generating all context words given any center word:

∏T ∏ P (w(t+j) | w(t)),
t=1 −m≤j≤m, j̸=0

(14.1.5)

where any time step that is less than 1 or greater than T can be omitted.

Training

The skip-gram model parameters are the center word vector and context word vector for each word in the vocabulary. In training, we learn the model parameters by maximizing the likelihood function (i.e., maximum likelihood estimation). This is equivalent to minimizing the following loss function:

∑ T ∑

−

log P (w(t+j) | w(t)).

t=1 −m≤j≤m, j̸=0

(14.1.6)

When using stochastic gradient descent to minimize the loss, in each iteration we can randomly

sample a shorter subsequence to calculate the (stochastic) gradient for this subsequence to update

the model parameters. To calculate this (stochastic) gradient, we need to obtain the gradients of

the log conditional probability with respect to the center word vector and the context word vector.

In general, according to (14.1.4) the log conditional probability involving any pair of the center

word wc and the context word wo is

(

)

∑

log P (wo | wc) = u⊤o vc − log

exp(u⊤i vc) .

(14.1.7)

i∈V

Through differentiation, we can obtain its gradient with respect to the center word vector vc as

∂log P (w | w )

∑ exp(u⊤vc)uj
j

o c = uo − ∑j∈V

∂vc

(i∈V exp(u⊤i vc) )

∑

exp(u⊤j vc)

= uo −

∑ exp(u⊤vc) uj

j∈V

i∈V

i

∑

= uo − P (wj | wc)uj.

(14.1.8)

j∈V

Note that the calculation in (14.1.8) requires the conditional probabilities of all words in the dictionary with wc as the center word. The gradients for the other word vectors can be obtained in the same way.
After training, for any word with index i in the dictionary, we obtain both word vectors vi (as the center word) and ui (as the context word). In natural language processing applications, the center word vectors of the skip-gram model are typically used as the word representations.

670

Chapter 14. Natural Language Processing: Pretraining

14.1.4 The Continuous Bag of Words (CBOW) Model

The continuous bag of words (CBOW) model is similar to the skip-gram model. The major difference from the skip-gram model is that the continuous bag of words model assumes that a center word is generated based on its surrounding context words in the text sequence. For example, in the same text sequence “the”, “man”, “loves”, “his”, and “son”, with “loves” as the center word and the context window size being 2, the continuous bag of words model considers the conditional probability of generating the center word “loves” based on the context words “the”, “man”, “his” and “son” (as shown in fig_cbow), which is

P ("loves" | "the", "man", "his", "son").

(14.1.9)

Fig. 14.1.2: The continuous bag of words model considers the conditional probability of generating the center word given its surrounding context words. :label: fig_cbow

Since there are multiple context words in the continuous bag of words model, these context word

vectors are averaged in the calculation of the conditional probability. Specifically, for any word with index i in the dictionary, denote by vi ∈ Rd and ui ∈ Rd its two vectors when used as a

context word and a center word (meanings are switched in the skip-gram model), respectively. The

conditional probability of generating any center word wc (with index c in the dictionary) given its

surrounding context words wo1, . . . , wo2m (with index o1, . . . , o2m in the dictionary) can be modeled by

( exp

1

u⊤(vo

+ . . . , +vo

) )

P (wc | wo1 , . . . , wo2m ) = ∑

exp2m( 1c u⊤1(v + . . . , +2mv

). )

i∈V

2m i o1

o2m

(14.1.10)

For brevity, let Wo = {wo1, . . . , wo2m} and v¯o = (vo1 + . . . , +vo2m) /(2m). Then (14.1.10) can be simplified as

()

P (w | W ) = ∑ exp u⊤c(v¯o ) .

c

o

i∈V exp u⊤i v¯o

(14.1.11)

Given a text sequence of length T , where the word at time step t is denoted as w(t). For context window size m, the likelihood function of the continuous bag of words model is the probability of generating all center words given their context words:

∏T P (w(t) | w(t−m), . . . , w(t−1), w(t+1), . . . , w(t+m)).
t=1

(14.1.12)

14.1. Word Embedding (word2vec)

671

Training

Training continuous bag of words models is almost the same as training skip-gram models. The maximum likelihood estimation of the continuous bag of words model is equivalent to minimizing the following loss function:

∑ T − log P (w(t) | w(t−m), . . . , w(t−1), w(t+1), . . . , w(t+m)).
t=1

(14.1.13)

Notice that

( ∑

(

))

log P (wc | Wo) = u⊤c v¯o − log

exp u⊤i v¯o .

i∈V

(14.1.14)

Through differentiation, we can obtain its gradient with respect to any context word vector voi(i = 1, . . . , 2m) as









∂ log P (wc | Wo) =

1

u

∑ exp(u⊤j v¯o)uj −∑

=

1

u

∑ − P (w

| W )u  .

∂voi

2m c

i∈V exp(u⊤i v¯o)

2m c

j

oj

j∈V

j∈V

(14.1.15)

The gradients for the other word vectors can be obtained in the same way. Unlike the skip-gram model, the continuous bag of words model typically uses context word vectors as the word representations.

Summary
• Word vectors are vectors used to represent words, and can also be considered as feature vectors or representations of words. The technique of mapping words to real vectors is called word embedding.
• The word2vec tool contains both the skip-gram and continuous bag of words models.
• The skip-gram model assumes that a word can be used to generate its surrounding words in a text sequence; while the continuous bag of words model assumes that a center word is generated based on its surrounding context words.

Exercises
1. What is the computational complexity for calculating each gradient? What could be the issue if the dictionary size is huge?
2. Some fixed phrases in English consist of multiple words, such as “new york”. How to train their word vectors? Hint: see Section 4 in the word2vec paper (Mikolov et al., 2013b).
3. Let us reflect on the word2vec design by taking the skip-gram model as an example. What is the relationship between the dot product of two word vectors in the skip-gram model and the cosine similarity? For a pair of words with similar semantics, why may the cosine similarity of their word vectors (trained by the skip-gram model) be high?
Discussions192
192 https://discuss.d2l.ai/t/381

672

Chapter 14. Natural Language Processing: Pretraining

14.2 Approximate Training
Recall our discussions in Section 14.1. The main idea of the skip-gram model is using softmax operations to calculate the conditional probability of generating a context word wo based on the given center word wc in (14.1.4), whose corresponding logarithmic loss is given by the opposite of (14.1.7).
Due to the nature of the softmax operation, since a context word may be anyone in the dictionary V, the opposite of (14.1.7) contains the summation of items as many as the entire size of the vocabulary. Consequently, the gradient calculation for the skip-gram model in (14.1.8) and that for the continuous bag-of-words model in (14.1.15) both contain the summation. Unfortunately, the computational cost for such gradients that sum over a large dictionary (often with hundreds of thousands or millions of words) is huge!
In order to reduce the aforementioned computational complexity, this section will introduce two approximate training methods: negative sampling and hierarchical softmax. Due to the similarity between the skip-gram model and the continuous bag of words model, we will just take the skipgram model as an example to describe these two approximate training methods.

14.2.1 Negative Sampling

Negative sampling modifies the original objective function. Given the context window of a center word wc, the fact that any (context) word wo comes from this context window is considered as an event with the probability modeled by

P (D = 1 | wc, wo) = σ(u⊤o vc),

(14.2.1)

where σ uses the definition of the sigmoid activation function: 1
σ(x) = 1 + exp(−x) .

(14.2.2)

Let us begin by maximizing the joint probability of all such events in text sequences to train word embeddings. Specifically, given a text sequence of length T , denote by w(t) the word at time step
t and let the context window size be m, consider maximizing the joint probability

∏T ∏ P (D = 1 | w(t), w(t+j)).
t=1 −m≤j≤m, j̸=0

(14.2.3)

However, (14.2.3) only considers those events that involve positive examples. As a result, the joint probability in (14.2.3) is maximized to 1 only if all the word vectors are equal to infinity. Of course, such results are meaningless. To make the objective function more meaningful, negative sampling adds negative examples sampled from a predefined distribution.

Denote by S the event that a context word wo comes from the context window of a center word wc. For this event involving wo, from a predefined distribution P (w) sample K noise words that are not from this context window. Denote by Nk the event that a noise word wk (k = 1, . . . , K) does not come from the context window of wc. Assume that these events involving both the positive example and negative examples S, N1, . . . , NK are mutually independent. Negative sampling rewrites the joint probability (involving only positive examples) in (14.2.3) as

∏T ∏ P (w(t+j) | w(t)),
t=1 −m≤j≤m, j̸=0

(14.2.4)

14.2. Approximate Training

673

where the conditional probability is approximated through events S, N1, . . . , NK:

∏K

P (w(t+j) | w(t)) = P (D = 1 | w(t), w(t+j))

P (D = 0 | w(t), wk).

k=1, wk∼P (w)

(14.2.5)

Denote by it and hk the indices of a word w(t) at time step t of a text sequence and a noise word wk, respectively. The logarithmic loss with respect to the conditional probabilities in (14.2.5) is

∑ K

− log P (w(t+j) | w(t)) = − log P (D = 1 | w(t), w(t+j)) −

log P (D = 0 | w(t), wk)

k=1, wk∼P (w)

(

)

∑ K

(

(

))

= − log σ u⊤it+j vit −

log 1 − σ u⊤hk vit

k=1, wk∼P (w)

(

)

∑ K

(

)

= − log σ u⊤it+j vit −

log σ −u⊤hk vit .

k=1, wk∼P (w)

(14.2.6)

We can see that now the computational cost for gradients at each training step has nothing to do with the dictionary size, but linearly depends on K. When setting the hyperparameter K to a smaller value, the computational cost for gradients at each training step with negative sampling is smaller.

14.2.2 Hierarchical Softmax
As an alternative approximate training method, hierarchical softmax uses the binary tree, a data structure illustrated in Fig. 14.2.1, where each leaf node of the tree represents a word in dictionary V.

Fig. 14.2.1: Hierarchical softmax for approximate training, where each leaf node of the tree represents a word in the dictionary.

Denote by L(w) the number of nodes (including both ends) on the path from the root node to the leaf node representing word w in the binary tree. Let n(w, j) be the jth node on this path, with its

674

Chapter 14. Natural Language Processing: Pretraining

context word vector being un(w,j). For example, L(w3) = 4 in Fig. 14.2.1. Hierarchical softmax approximates the conditional probability in (14.1.4) as

L(w∏o)−1 (

)

P (wo | wc) =

σ [[n(wo, j + 1) = leftChild(n(wo, j))]] · u⊤n(wo,j)vc ,

j=1

(14.2.7)

where function σ is defined in (14.2.2), and leftChild(n) is the left child node of node n: if x is true, [[x]] = 1; otherwise [[x]] = −1.
To illustrate, let us calculate the conditional probability of generating word w3 given word wc in Fig. 14.2.1. This requires dot products between the word vector vc of wc and non-leaf node vectors on the path (the path in bold in Fig. 14.2.1) from the root to w3, which is traversed left, right, then left:

P (w3 | wc) = σ(u⊤n(w3,1)vc) · σ(−u⊤n(w3,2)vc) · σ(u⊤n(w3,3)vc).

(14.2.8)

Since σ(x) + σ(−x) = 1, it holds that the conditional probabilities of generating all the words in

dictionary V based on any word wc sum up to one:

∑ P (w | wc) = 1.
w∈V

(14.2.9)

Fortunately, since L(wo)−1 is on the order of O(log2|V|) due to the binary tree structure, when the dictionary size V is huge, the computational cost for each training step using hierarchical softmax
is significantly reduced compared with that without approximate training.

Summary
• Negative sampling constructs the loss function by considering mutually independent events that involve both positive and negative examples. The computational cost for training is linearly dependent on the number of noise words at each step.
• Hierarchical softmax constructs the loss function using the path from the root node to the leaf node in the binary tree. The computational cost for training is dependent on the logarithm of the dictionary size at each step.

Exercises
1. How can we sample noise words in negative sampling? 2. Verify that (14.2.9) holds. 3. How to train the continuous bag of words model using negative sampling and hierarchical
softmax, respectively? Discussions193
193 https://discuss.d2l.ai/t/382

14.2. Approximate Training

675

14.3 The Dataset for Pretraining Word Embeddings
Now that we know the technical details of the word2vec models and approximate training methods, let us walk through their implementations. Specifically, we will take the skip-gram model in Section 14.1 and negative sampling in Section 14.2 as an example. In this section, we begin with the dataset for pretraining the word embedding model: the original format of the data will be transformed into minibatches that can be iterated over during training.
import math import os import random from mxnet import gluon, np from d2l import mxnet as d2l
14.3.1 Reading the Dataset
The dataset that we use here is Penn Tree Bank (PTB)194. This corpus is sampled from Wall Street Journal articles, split into training, validation, and test sets. In the original format, each line of the text file represents a sentence of words that are separated by spaces. Here we treat each word as a token.
#@save d2l.DATA_HUB['ptb'] = (d2l.DATA_URL + 'ptb.zip',
'319d85e578af0cdc590547f26231e4e31cdf1e42')
#@save def read_ptb():
"""Load the PTB dataset into a list of text lines.""" data_dir = d2l.download_extract('ptb') # Read the training set. with open(os.path.join(data_dir, 'ptb.train.txt')) as f:
raw_text = f.read() return [line.split() for line in raw_text.split('\n')]
sentences = read_ptb() f'# sentences: {len(sentences)}'
'# sentences: 42069'
After reading the training set, we build a vocabulary for the corpus, where any word that appears less than 10 times is replaced by the “<unk>” token. Note that the original dataset also contains “<unk>” tokens that represent rare (unknown) words.
vocab = d2l.Vocab(sentences, min_freq=10) f'vocab size: {len(vocab)}'
'vocab size: 6719'
194 https://catalog.ldc.upenn.edu/LDC99T42

676

Chapter 14. Natural Language Processing: Pretraining

14.3.2 Subsampling

Text data typically have high-frequency words such as “the”, “a”, and “in”: they may even occur

billions of times in very large corpora. However, these words often co-occur with many different

words in context windows, providing little useful signals. For instance, consider the word “chip”

in a context window: intuitively its co-occurrence with a low-frequency word “intel” is more useful

in training than the co-occurrence with a high-frequency word “a”. Moreover, training with vast

amounts of (high-frequency) words is slow. Thus, when training word embedding models, high-

frequency words can be subsampled (Mikolov et al., 2013b). Specifically, each indexed word wi in

the dataset will be discarded with probability

(√

)

P (wi) = max 1 −

t ,0 ,
f (wi)

(14.3.1)

where f (wi) is the ratio of the number of words wi to the total number of words in the dataset, and the constant t is a hyperparameter (10−4 in the experiment). We can see that only when the relative frequency f (wi) > t can the (high-frequency) word wi be discarded, and the higher the relative frequency of the word, the greater the probability of being discarded.
#@save def subsample(sentences, vocab):
"""Subsample high-frequency words.""" # Exclude unknown tokens '<unk>' sentences = [[token for token in line if vocab[token] != vocab.unk]
for line in sentences] counter = d2l.count_corpus(sentences) num_tokens = sum(counter.values())
# Return True if `token` is kept during subsampling def keep(token):
return (random.uniform(0, 1) < math.sqrt( 1e-4 / counter[token] * num_tokens))
return ([[token for token in line if keep(token)] for line in sentences], counter)
subsampled, counter = subsample(sentences, vocab)

The following code snippet plots the histogram of the number of tokens per sentence before and after subsampling. As expected, subsampling significantly shortens sentences by dropping highfrequency words, which will lead to training speedup.
d2l.show_list_len_pair_hist(['origin', 'subsampled'], '# tokens per sentence', 'count', sentences, subsampled);

14.3. The Dataset for Pretraining Word Embeddings

677

For individual tokens, the sampling rate of the high-frequency word “the” is less than 1/20. def compare_counts(token):
return (f'# of "{token}": ' f'before={sum([l.count(token) for l in sentences])}, ' f'after={sum([l.count(token) for l in subsampled])}')
compare_counts('the')
'# of "the": before=50770, after=2051'
In contrast, low-frequency words “join” are completely kept. compare_counts('join')
'# of "join": before=45, after=45'
After subsampling, we map tokens to their indices for the corpus. corpus = [vocab[line] for line in subsampled] corpus[:3]
[[], [392, 2115, 145, 274, 406], [22, 5277, 3054, 1580]]

14.3.3 Extracting Center Words and Context Words

The following get_centers_and_contexts function extracts all the center words and their context words from corpus. It uniformly samples an integer between 1 and max_window_size at random as the context window size. For any center word, those words whose distance from it does not exceed the sampled context window size are its context words.

#@save def get_centers_and_contexts(corpus, max_window_size):

(continues on next page)

678

Chapter 14. Natural Language Processing: Pretraining

(continued from previous page)
"""Return center words and context words in skip-gram.""" centers, contexts = [], [] for line in corpus:
# To form a "center word--context word" pair, each sentence needs to # have at least 2 words if len(line) < 2:
continue centers += line for i in range(len(line)): # Context window centered at `i`
window_size = random.randint(1, max_window_size) indices = list(
range(max(0, i - window_size), min(len(line), i + 1 + window_size)))
# Exclude the center word from the context words indices.remove(i) contexts.append([line[idx] for idx in indices]) return centers, contexts
Next, we create an artificial dataset containing two sentences of 7 and 3 words, respectively. Let the maximum context window size be 2 and print all the center words and their context words.
tiny_dataset = [list(range(7)), list(range(7, 10))] print('dataset', tiny_dataset) for center, context in zip(*get_centers_and_contexts(tiny_dataset, 2)):
print('center', center, 'has contexts', context)
dataset [[0, 1, 2, 3, 4, 5, 6], [7, 8, 9]] center 0 has contexts [1, 2] center 1 has contexts [0, 2, 3] center 2 has contexts [1, 3] center 3 has contexts [1, 2, 4, 5] center 4 has contexts [3, 5] center 5 has contexts [3, 4, 6] center 6 has contexts [5] center 7 has contexts [8] center 8 has contexts [7, 9] center 9 has contexts [8]
When training on the PTB dataset, we set the maximum context window size to 5. The following extracts all the center words and their context words in the dataset.
all_centers, all_contexts = get_centers_and_contexts(corpus, 5) f'# center-context pairs: {sum([len(contexts) for contexts in all_contexts])}'
'# center-context pairs: 1499844'

14.3. The Dataset for Pretraining Word Embeddings

679

14.3.4 Negative Sampling
We use negative sampling for approximate training. To sample noise words according to a predefined distribution, we define the following RandomGenerator class, where the (possibly unnormalized) sampling distribution is passed via the argument sampling_weights.
#@save class RandomGenerator:
"""Randomly draw among {1, ..., n} according to n sampling weights.""" def __init__(self, sampling_weights):
# Exclude self.population = list(range(1, len(sampling_weights) + 1)) self.sampling_weights = sampling_weights self.candidates = [] self.i = 0
def draw(self): if self.i == len(self.candidates): # Cache `k` random sampling results self.candidates = random.choices(self.population, self.sampling_weights, k=10000) self.i = 0 self.i += 1 return self.candidates[self.i - 1]
For example, we can draw 10 random variables X among indices 1, 2, and 3 with sampling probabilities P (X = 1) = 2/9, P (X = 2) = 3/9, and P (X = 3) = 4/9 as follows.
generator = RandomGenerator([2, 3, 4]) [generator.draw() for _ in range(10)]

[3, 2, 3, 1, 3, 3, 2, 2, 2, 3]

For a pair of center word and context word, we randomly sample K (5 in the experiment) noise words. According to the suggestions in the word2vec paper, the sampling probability P (w) of a noise word w is set to its relative frequency in the dictionary raised to the power of 0.75 (Mikolov et al., 2013b).

#@save def get_negatives(all_contexts, vocab, counter, K):
"""Return noise words in negative sampling.""" # Sampling weights for words with indices 1, 2, ... (index 0 is the # excluded unknown token) in the vocabulary sampling_weights = [
counter[vocab.to_tokens(i)]**0.75 for i in range(1, len(vocab))] all_negatives, generator = [], RandomGenerator(sampling_weights) for contexts in all_contexts:
negatives = [] while len(negatives) < len(contexts) * K:
neg = generator.draw() # Noise words cannot be context words if neg not in contexts:
negatives.append(neg) all_negatives.append(negatives)

(continues on next page)

680

Chapter 14. Natural Language Processing: Pretraining

return all_negatives all_negatives = get_negatives(all_contexts, vocab, counter, 5)

(continued from previous page)

14.3.5 Loading Training Examples in Minibatches
After all the center words together with their context words and sampled noise words are extracted, they will be transformed into minibatches of examples that can be iteratively loaded during training.
In a minibatch, the ith example includes a center word and its ni context words and mi noise words. Due to varying context window sizes, ni + mi varies for different i. Thus, for each example we concatenate its context words and noise words in the contexts_negatives variable, and pad zeros until the concatenation length reaches maxi ni + mi (max_len). To exclude paddings in the calculation of the loss, we define a mask variable masks. There is a one-to-one correspondence between elements in masks and elements in contexts_negatives, where zeros (otherwise ones) in masks correspond to paddings in contexts_negatives.
To distinguish between positive and negative examples, we separate context words from noise words in contexts_negatives via a labels variable. Similar to masks, there is also a one-to-one correspondence between elements in labels and elements in contexts_negatives, where ones (otherwise zeros) in labels correspond to context words (positive examples) in contexts_negatives.
The above idea is implemented in the following batchify function. Its input data is a list with length equal to the batch size, where each element is an example consisting of the center word center, its context words context, and its noise words negative. This function returns a minibatch that can be loaded for calculations during training, such as including the mask variable.
#@save def batchify(data):
"""Return a minibatch of examples for skip-gram with negative sampling.""" max_len = max(len(c) + len(n) for _, c, n in data) centers, contexts_negatives, masks, labels = [], [], [], [] for center, context, negative in data:
cur_len = len(context) + len(negative) centers += [center] contexts_negatives += [context + negative + [0] * (max_len - cur_len)] masks += [[1] * cur_len + [0] * (max_len - cur_len)] labels += [[1] * len(context) + [0] * (max_len - len(context))] return (np.array(centers).reshape((-1, 1)), np.array(contexts_negatives),
np.array(masks), np.array(labels))
Let us test this function using a minibatch of two examples.
x_1 = (1, [2, 2], [3, 3, 3, 3]) x_2 = (1, [2, 2, 2], [3, 3]) batch = batchify((x_1, x_2))
names = ['centers', 'contexts_negatives', 'masks', 'labels'] for name, data in zip(names, batch):
print(name, '=', data)

14.3. The Dataset for Pretraining Word Embeddings

681

centers = [[1.] [1.]] contexts_negatives = [[2. 2. 3. 3. 3. 3.] [2. 2. 2. 3. 3. 0.]] masks = [[1. 1. 1. 1. 1. 1.] [1. 1. 1. 1. 1. 0.]] labels = [[1. 1. 0. 0. 0. 0.] [1. 1. 1. 0. 0. 0.]]
14.3.6 Putting All Things Together
Last, we define the load_data_ptb function that reads the PTB dataset and returns the data iterator and the vocabulary.
#@save def load_data_ptb(batch_size, max_window_size, num_noise_words):
"""Download the PTB dataset and then load it into memory.""" sentences = read_ptb() vocab = d2l.Vocab(sentences, min_freq=10) subsampled, counter = subsample(sentences, vocab) corpus = [vocab[line] for line in subsampled] all_centers, all_contexts = get_centers_and_contexts(
corpus, max_window_size) all_negatives = get_negatives(all_contexts, vocab, counter,
num_noise_words) dataset = gluon.data.ArrayDataset(all_centers, all_contexts,
all_negatives) data_iter = gluon.data.DataLoader(
dataset, batch_size, shuffle=True, batchify_fn=batchify, num_workers=d2l.get_dataloader_workers()) return data_iter, vocab
Let us print the first minibatch of the data iterator.
data_iter, vocab = load_data_ptb(512, 5, 5) for batch in data_iter:
for name, data in zip(names, batch): print(name, 'shape:', data.shape)
break
centers shape: (512, 1) contexts_negatives shape: (512, 60) masks shape: (512, 60) labels shape: (512, 60)

682

Chapter 14. Natural Language Processing: Pretraining

Summary
• High-frequency words may not be so useful in training. We can subsample them for speedup in training.
• For computational efficiency, we load examples in minibatches. We can define other variables to distinguish paddings from non-paddings, and positive examples from negative ones.
Exercises
1. How does the running time of code in this section changes if not using subsampling? 2. The RandomGenerator class caches k random sampling results. Set k to other values and see
how it affects the data loading speed. 3. What other hyperparameters in the code of this section may affect the data loading speed? Discussions195
14.4 Pretraining word2vec
We go on to implement the skip-gram model defined in Section 14.1. Then we will pretrain word2vec using negative sampling on the PTB dataset. First of all, let us obtain the data iterator and the vocabulary for this dataset by calling the d2l.load_data_ptb function, which was described in Section 14.3
import math from mxnet import autograd, gluon, np, npx from mxnet.gluon import nn from d2l import mxnet as d2l
npx.set_np()
batch_size, max_window_size, num_noise_words = 512, 5, 5 data_iter, vocab = d2l.load_data_ptb(batch_size, max_window_size,
num_noise_words)
14.4.1 The Skip-Gram Model
We implement the skip-gram model by using embedding layers and batch matrix multiplications. First, let us review how embedding layers work.
195 https://discuss.d2l.ai/t/383

14.4. Pretraining word2vec

683

Embedding Layer
As described in Section 9.7, an embedding layer maps a tokenʼs index to its feature vector. The weight of this layer is a matrix whose number of rows equals to the dictionary size (input_dim) and number of columns equals to the vector dimension for each token (output_dim). After a word embedding model is trained, this weight is what we need.
embed = nn.Embedding(input_dim=20, output_dim=4) embed.initialize() embed.weight

Parameter embedding0_weight (shape=(20, 4), dtype=float32)
The input of an embedding layer is the index of a token (word). For any token index i, its vector representation can be obtained from the ith row of the weight matrix in the embedding layer. Since the vector dimension (output_dim) was set to 4, the embedding layer returns vectors with shape (2, 3, 4) for a minibatch of token indices with shape (2, 3).
x = np.array([[1, 2, 3], [4, 5, 6]]) embed(x)

array([[[ 0.01438687, 0.05011239, [-0.01068833, 0.01729892, [-0.00873779, -0.02834515,

0.00628365, 0.04861524], 0.02042518, -0.01618656], 0.05484822, -0.06206018]],

[[ 0.06491279, -0.03182812, -0.01631819, -0.00312688], [ 0.0408415 , 0.04370362, 0.00404529, -0.0028032 ], [ 0.00952624, -0.01501013, 0.05958354, 0.04705103]]])

Defining the Forward Propagation
In the forward propagation, the input of the skip-gram model includes the center word indices center of shape (batch size, 1) and the concatenated context and noise word indices contexts_and_negatives of shape (batch size, max_len), where max_len is defined in Section 14.3.5. These two variables are first transformed from the token indices into vectors via the embedding layer, then their batch matrix multiplication (described in Section 10.2.4) returns an output of shape (batch size, 1, max_len). Each element in the output is the dot product of a center word vector and a context or noise word vector.
def skip_gram(center, contexts_and_negatives, embed_v, embed_u): v = embed_v(center) u = embed_u(contexts_and_negatives) pred = npx.batch_dot(v, u.swapaxes(1, 2)) return pred
Let us print the output shape of this skip_gram function for some example inputs.
skip_gram(np.ones((2, 1)), np.ones((2, 4)), embed, embed).shape

684

Chapter 14. Natural Language Processing: Pretraining

(2, 1, 4)
14.4.2 Training
Before training the skip-gram model with negative sampling, let us first define its loss function.
Binary Cross-Entropy Loss
According to the definition of the loss function for negative sampling in Section 14.2.1, we will use the binary cross-entropy loss. loss = gluon.loss.SigmoidBCELoss()
Recall our descriptions of the mask variable and the label variable in Section 14.3.5. The following calculates the binary cross-entropy loss for the given variables. pred = np.array([[1.1, -2.2, 3.3, -4.4]] * 2) label = np.array([[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0]]) mask = np.array([[1, 1, 1, 1], [1, 1, 0, 0]]) loss(pred, label, mask) * mask.shape[1] / mask.sum(axis=1)
array([0.93521017, 1.8462094 ])
Below shows how the above results are calculated (in a less efficient way) using the sigmoid activation function in the binary cross-entropy loss. We can consider the two outputs as two normalized losses that are averaged over non-masked predictions. def sigmd(x):
return -math.log(1 / (1 + math.exp(-x))) print(f'{(sigmd(1.1) + sigmd(2.2) + sigmd(-3.3) + sigmd(4.4)) / 4:.4f}') print(f'{(sigmd(-1.1) + sigmd(-2.2)) / 2:.4f}')
0.9352 1.8462
Initializing Model Parameters
We define two embedding layers for all the words in the vocabulary when they are used as center words and context words, respectively. The word vector dimension embed_size is set to 100. embed_size = 100 net = nn.Sequential() net.add(nn.Embedding(input_dim=len(vocab), output_dim=embed_size),
nn.Embedding(input_dim=len(vocab), output_dim=embed_size))

14.4. Pretraining word2vec

685

Defining the Training Loop
The training loop is defined below. Because of the existence of padding, the calculation of the loss function is slightly different compared to the previous training functions.
def train(net, data_iter, lr, num_epochs, device=d2l.try_gpu()): net.initialize(ctx=device, force_reinit=True) trainer = gluon.Trainer(net.collect_params(), 'adam', {'learning_rate': lr}) animator = d2l.Animator(xlabel='epoch', ylabel='loss', xlim=[1, num_epochs]) # Sum of normalized losses, no. of normalized losses metric = d2l.Accumulator(2) for epoch in range(num_epochs): timer, num_batches = d2l.Timer(), len(data_iter) for i, batch in enumerate(data_iter): center, context_negative, mask, label = [ data.as_in_ctx(device) for data in batch] with autograd.record(): pred = skip_gram(center, context_negative, net[0], net[1]) l = (loss(pred.reshape(label.shape), label, mask) * mask.shape[1] / mask.sum(axis=1)) l.backward() trainer.step(batch_size) metric.add(l.sum(), l.size) if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1: animator.add(epoch + (i + 1) / num_batches, (metric[0] / metric[1],)) print(f'loss {metric[0] / metric[1]:.3f}, ' f'{metric[1] / timer.stop():.1f} tokens/sec on {str(device)}')
Now we can train a skip-gram model using negative sampling.
lr, num_epochs = 0.002, 5 train(net, data_iter, lr, num_epochs)
loss 0.408, 108160.1 tokens/sec on gpu(0)

686

Chapter 14. Natural Language Processing: Pretraining

14.4.3 Applying Word Embeddings
After training the word2vec model, we can use the cosine similarity of word vectors from the trained model to find words from the dictionary that are most semantically similar to an input word.
def get_similar_tokens(query_token, k, embed): W = embed.weight.data() x = W[vocab[query_token]] # Compute the cosine similarity. Add 1e-9 for numerical stability cos = np.dot(W, x) / np.sqrt(np.sum(W * W, axis=1) * np.sum(x * x) + 1e-9) topk = npx.topk(cos, k=k + 1, ret_typ='indices').asnumpy().astype('int32') for i in topk[1:]: # Remove the input words print(f'cosine sim={float(cos[i]):.3f}: {vocab.to_tokens(i)}')
get_similar_tokens('chip', 3, net[0])
cosine sim=0.634: intel cosine sim=0.563: microprocessor cosine sim=0.524: bugs
Summary
• We can train a skip-gram model with negative sampling using embedding layers and the binary cross-entropy loss.
• Applications of word embeddings include finding semantically similar words for a given word based on the cosine similarity of word vectors.
Exercises
1. Using the trained model, find semantically similar words for other input words. Can you improve the results by tuning hyperparameters?
2. When a training corpus is huge, we often sample context words and noise words for the center words in the current minibatch when updating model parameters. In other words, the same center word may have different context words or noise words in different training epochs. What are the benefits of this method? Try to implement this training method.
Discussions196
196 https://discuss.d2l.ai/t/384

14.4. Pretraining word2vec

687

14.5 Word Embedding with Global Vectors (GloVe)
Word-word co-occurrences within context windows may carry rich semantic information. For example, in a large corpus word “solid” is more likely to co-occur with “ice” than “steam”, but word “gas” probably co-occurs with “steam” more frequently than “ice”. Besides, global corpus statistics of such co-occurrences can be precomputed: this can lead to more efficient training. To leverage statistical information in the entire corpus for word embedding, let us first revisit the skip-gram model in Section 14.1.3, but interpreting it using global corpus statistics such as co-occurrence counts.

14.5.1 Skip-Gram with Global Corpus Statistics

Denoting by qij the conditional probability P (wj | wi) of word wj given word wi in the skip-gram model, we have

exp(u⊤j vi)

qij = ∑ exp(u⊤vi) ,

k∈V

k

(14.5.1)

where for any index i vectors vi and ui represent word wi as the center word and context word, respectively, and V = {0, 1, . . . , |V| − 1} is the index set of the vocabulary.

Consider word wi that may occur multiple times in the corpus. In the entire corpus, all the context words wherever wi is taken as their center word form a multiset Ci of word indices that allows for multiple instances of the same element. For any element, its number of instances is called its multiplicity. To illustrate with an example, suppose that word wi occurs twice in the corpus and indices of the context words that take wi as their center word in the two context windows are k, j, m, k and k, l, k, j. Thus, multiset Ci = {j, j, k, k, k, k, l, m}, where multiplicities of elements j, k, l, m are 2, 4, 1, 1, respectively.

Now let us denote the multiplicity of element j in multiset Ci as xij. This is the global co-

occurrence count of word wj (as the context word) and word wi (as the center word) in the same

context window in the entire corpus. Using such global corpus statistics, the loss function of the

skip-gram model is equivalent to

∑∑

−

xij log qij.

i∈V j∈V

(14.5.2)

We further denote by xi the number of all the context words in the context windows where wi

occurs as their center word, which is equivalent to |Ci|. Letting pij be the conditional probability

xij/xi for generating context word wj given center word wi, (14.5.2) can be rewritten as

∑∑

− xi pij log qij.

(14.5.3)

i∈V j∈V

∑

In (14.5.3), − j∈V pij log qij calculates the cross-entropy of the conditional distribution pij of

global corpus statistics and the conditional distribution qij of model predictions. This loss is also

weighted by xi as explained above. Minimizing the loss function in (14.5.3) will allow the pre-

dicted conditional distribution to get close to the conditional distribution from the global corpus

statistics.

Though being commonly used for measuring the distance between probability distributions, the cross-entropy loss function may not be a good choice here. On one hand, as we mentioned in Section 14.2, the cost of properly normalizing qij results in the sum over the entire vocabulary, which can be computationally expensive. On the other hand, a large number of rare events from a large corpus are often modeled by the cross-entropy loss to be assigned with too much weight.

688

Chapter 14. Natural Language Processing: Pretraining

14.5.2 The GloVe Model

In view of this, the GloVe model makes three changes to the skip-gram model based on squared loss (Pennington et al., 2014):
1. Use variables p′ij = xij and qi′j = exp(u⊤j vi) that a(re not probabilit)y2 dis(tributions and t)a2ke the logarithm of both, so the squared loss term is log p′ij − log qi′j = u⊤j vi − log xij .

2. Add two scalar model parameters for each word wi: the center word bias bi and the context word bias ci.
3. Replace the weight of each loss term with the weight function h(xij), where h(x) is increasing in the interval of [0, 1].

Putting all things together, training GloVe is to minimize the following loss function:

∑∑

(

)2

h(xij) u⊤j vi + bi + cj − log xij .

i∈V j∈V

(14.5.4)

For the weight function, a suggested choice is: h(x) = (x/c)α (e.g α = 0.75) if x < c (e.g., c = 100); otherwise h(x) = 1. In this case, because h(0) = 0, the squared loss term for any xij = 0 can be omitted for computational efficiency. For example, when using minibatch stochastic gradient descent for training, at each iteration we randomly sample a minibatch of non-zero xij to calculate gradients and update the model parameters. Note that these non-zero xij are precomputed global corpus statistics; thus, the model is called GloVe for Global Vectors.

It should be emphasized that if word wi appears in the context window of word wj, then vice versa. Therefore, xij = xji. Unlike word2vec that fits the asymmetric conditional probability pij, GloVe fits the symmetric log xij. Therefore, the center word vector and the context word vector of any word are mathematically equivalent in the GloVe model. However in practice, owing to different initialization values, the same word may still get different values in these two vectors after training: GloVe sums them up as the output vector.

14.5.3 Interpreting GloVe from the Ratio of Co-occurrence Probabilities
We can also interpret the GloVe model from another perspective. Using the same notation in Section 14.5.1, let pij d=ef P (wj | wi) be the conditional probability of generating the context word wj given wi as the center word in the corpus. Table 14.5.1 lists several co-occurrence probabilities given words “ice” and “steam” and their ratios based on statistics from a large corpus.

Table 14.5.1: Word-word co-occurrence probabilities and their ratios from a large corpus (adapted from Table 1 in [Pennington.Socher.Manning.2014]:)

wk = p1 = P (wk | ice) p2 = P (wk | steam) p1/p2

solid 0.00019 0.000022 8.9

gas 0.000066 0.00078 0.085

water 0.003 0.0022 1.36

fashion 0.000017 0.000018 0.96

We can observe the following from Table 14.5.1:
• For a word wk that is related to “ice” but unrelated to “steam”, such as wk = solid, we expect a larger ratio of co-occurence probabilities, such as 8.9.

14.5. Word Embedding with Global Vectors (GloVe)

689

• For a word wk that is related to “steam” but unrelated to “ice”, such as wk = gas, we expect a smaller ratio of co-occurence probabilities, such as 0.085.

• For a word wk that is related to both “ice” and “steam”, such as wk = water, we expect a ratio of co-occurence probabilities that is close to 1, such as 1.36.

• For a word wk that is unrelated to both “ice” and “steam”, such as wk = fashion, we expect a ratio of co-occurence probabilities that is close to 1, such as 0.96.

It can be seen that the ratio of co-occurrence probabilities can intuitively express the relationship between words. Thus, we can design a function of three word vectors to fit this ratio. For the ratio of co-occurrence probabilities pij/pik with wi being the center word and wj and wk being the context words, we want to fit this ratio using some function f :

f (uj, uk, vi) ≈ pij . pik

(14.5.5)

Among many possible designs for f , we only pick a reasonable choice in the following. Since

the ratio of co-occu(rrence probab)ilities is a scalar, we require that f be a scalar function, such as f (uj, uk, vi) = f (uj − uk)⊤vi . Switching word indices j and k in (14.5.5), it must hold that

f (x)f (−x) = 1, so one possibility is f (x) = exp(x), i.e.,

()

exp u⊤j vi

pij

f (uj, uk, vi) =

() exp u⊤vi

≈

. pik

k

(14.5.6)

() Now let us pick exp u⊤j vi ≈ αpij, where α is a constant. Since pij = xij/xi, after taking the

logarithm on both sides we get u⊤j vi ≈ log α + log xij − log xi. We may use additional bias terms to fit − log α + log xi, such as the center word bias bi and the context word bias cj:

u⊤j vi + bi + cj ≈ log xij.

(14.5.7)

Measuring the squared error of (14.5.7) with weights, the GloVe loss function in (14.5.4) is obtained.

Summary
• The skip-gram model can be interpreted using global corpus statistics such as word-word co-occurrence counts.
• The cross-entropy loss may not be a good choice for measuring the difference of two probability distributions, especially for a large corpus. GloVe uses squared loss to fit precomputed global corpus statistics.
• The center word vector and the context word vector are mathematically equivalent for any word in GloVe.
• GloVe can be interpreted from the ratio of word-word co-occurrence probabilities.

690

Chapter 14. Natural Language Processing: Pretraining

Exercises
1. If words wi and wj co-occur in the same context window, how can we use their distance in the text sequence to redesign the method for calculating the conditional probability pij? Hint: see Section 4.2 of the GloVe paper (Pennington et al., 2014).
2. For any word, are its center word bias and context word bias mathematically equivalent in GloVe? Why?
Discussions197

14.6 Subword Embedding
In English, words such as “helps”, “helped”, and “helping” are inflected forms of the same word “help”. The relationship between “dog” and “dogs” is the same as that between “cat” and “cats”, and the relationship between “boy” and “boyfriend” is the same as that between “girl” and “girlfriend”. In other languages such as French and Spanish, many verbs have over 40 inflected forms, while in Finnish, a noun may have up to 15 cases. In linguistics, morphology studies word formation and word relationships. However, the internal structure of words was neither explored in word2vec nor in GloVe.

14.6.1 The fastText Model

Recall how words are represented in word2vec. In both the skip-gram model and the continuous bag-of-words model, different inflected forms of the same word are directly represented by different vectors without shared parameters. To use morphological information, the fastText model proposed a subword embedding approach, where a subword is a character n-gram (Bojanowski et al., 2017). Instead of learning word-level vector representations, fastText can be considered as the subword-level skip-gram, where each center word is represented by the sum of its subword vectors.

Let us illustrate how to obtain subwords for each center word in fastText using the word “where”. First, add special characters “<” and “>” at the beginning and end of the word to distinguish prefixes and suffixes from other subwords. Then, extract character n-grams from the word. For example, when n = 3, we obtain all subwords of length 3: “<wh”, “whe”, “her”, “ere”, “re>”, and the special subword “<where>”.

In fastText, for any word w, denote by Gw the union of all its subwords of length between 3 and

6 and its special subword. The vocabulary is the union of the subwords of all words. Letting zg

be the vector of subword g in the dictionary, the vector vw for word w as a center word in the

skip-gram model is the sum of its subword vectors:

∑

vw =

zg .

g∈Gw

(14.6.1)

The rest of fastText is the same as the skip-gram model. Compared with the skip-gram model, the vocabulary in fastText is larger, resulting in more model parameters. Besides, to calculate the representation of a word, all its subword vectors have to be summed, leading to higher computational complexity. However, thanks to shared parameters from subwords among words with similar structures, rare words and even out-of-vocabulary words may obtain better vector representations in fastText.
197 https://discuss.d2l.ai/t/385

14.6. Subword Embedding

691

14.6.2 Byte Pair Encoding
In fastText, all the extracted subwords have to be of the specified lengths, such as 3 to 6, thus the vocabulary size cannot be predefined. To allow for variable-length subwords in a fixed-size vocabulary, we can apply a compression algorithm called byte pair encoding (BPE) to extract subwords (Sennrich et al., 2015).
Byte pair encoding performs a statistical analysis of the training dataset to discover common symbols within a word, such as consecutive characters of arbitrary length. Starting from symbols of length 1, byte pair encoding iteratively merges the most frequent pair of consecutive symbols to produce new longer symbols. Note that for efficiency, pairs crossing word boundaries are not considered. In the end, we can use such symbols as subwords to segment words. Byte pair encoding and its variants has been used for input representations in popular natural language processing pretraining models such as GPT-2 (Radford et al., 2019) and RoBERTa (Liu et al., 2019). In the following, we will illustrate how byte pair encoding works.
First, we initialize the vocabulary of symbols as all the English lowercase characters, a special end-of-word symbol '_', and a special unknown symbol '[UNK]'.
import collections
symbols = [ 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '_', '[UNK]']
Since we do not consider symbol pairs that cross boundaries of words, we only need a dictionary raw_token_freqs that maps words to their frequencies (number of occurrences) in a dataset. Note that the special symbol '_' is appended to each word so that we can easily recover a word sequence (e.g., “a taller man”) from a sequence of output symbols ( e.g., “a_ tall er_ man”). Since we start the merging process from a vocabulary of only single characters and special symbols, space is inserted between every pair of consecutive characters within each word (keys of the dictionary token_freqs). In other words, space is the delimiter between symbols within a word.
raw_token_freqs = {'fast_': 4, 'faster_': 3, 'tall_': 5, 'taller_': 4} token_freqs = {} for token, freq in raw_token_freqs.items():
token_freqs[' '.join(list(token))] = raw_token_freqs[token] token_freqs
{'f a s t _': 4, 'f a s t e r _': 3, 't a l l _': 5, 't a l l e r _': 4}
We define the following get_max_freq_pair function that returns the most frequent pair of consecutive symbols within a word, where words come from keys of the input dictionary token_freqs.
def get_max_freq_pair(token_freqs): pairs = collections.defaultdict(int) for token, freq in token_freqs.items(): symbols = token.split() for i in range(len(symbols) - 1): # Key of `pairs` is a tuple of two consecutive symbols pairs[symbols[i], symbols[i + 1]] += freq return max(pairs, key=pairs.get) # Key of `pairs` with the max value

692

Chapter 14. Natural Language Processing: Pretraining

As a greedy approach based on frequency of consecutive symbols, byte pair encoding will use the following merge_symbols function to merge the most frequent pair of consecutive symbols to produce new symbols.
def merge_symbols(max_freq_pair, token_freqs, symbols): symbols.append(''.join(max_freq_pair)) new_token_freqs = dict() for token, freq in token_freqs.items(): new_token = token.replace(' '.join(max_freq_pair), ''.join(max_freq_pair)) new_token_freqs[new_token] = token_freqs[token] return new_token_freqs
Now we iteratively perform the byte pair encoding algorithm over the keys of the dictionary token_freqs. In the first iteration, the most frequent pair of consecutive symbols are 't' and 'a', thus byte pair encoding merges them to produce a new symbol 'ta'. In the second iteration, byte pair encoding continues to merge 'ta' and 'l' to result in another new symbol 'tal'.
num_merges = 10 for i in range(num_merges):
max_freq_pair = get_max_freq_pair(token_freqs) token_freqs = merge_symbols(max_freq_pair, token_freqs, symbols) print(f'merge #{i + 1}:', max_freq_pair)

merge merge merge merge merge merge merge merge merge merge

#1: ('t', 'a') #2: ('ta', 'l') #3: ('tal', 'l') #4: ('f', 'a') #5: ('fa', 's') #6: ('fas', 't') #7: ('e', 'r') #8: ('er', '_') #9: ('tall', '_') #10: ('fast', '_')

After 10 iterations of byte pair encoding, we can see that list symbols now contains 10 more symbols that are iteratively merged from other symbols.
print(symbols)

['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's →', 't', 'u', 'v', 'w', 'x', 'y', 'z', '_', '[UNK]', 'ta', 'tal', 'tall', 'fa', 'fas', 'fast →', 'er', 'er_', 'tall_', 'fast_']
For the same dataset specified in the keys of the dictionary raw_token_freqs, each word in the dataset is now segmented by subwords “fast_”, “fast”, “er_”, “tall_”, and “tall” as a result of the byte pair encoding algorithm. For instance, words “faster_” and “taller_” are segmented as “fast er_” and “tall er_”, respectively.
print(list(token_freqs.keys()))

14.6. Subword Embedding

693

['fast_', 'fast er_', 'tall_', 'tall er_']
Note that the result of byte pair encoding depends on the dataset being used. We can also use the subwords learned from one dataset to segment words of another dataset. As a greedy approach, the following segment_BPE function tries to break words into the longest possible subwords from the input argument symbols.
def segment_BPE(tokens, symbols): outputs = [] for token in tokens: start, end = 0, len(token) cur_output = [] # Segment token with the longest possible subwords from symbols while start < len(token) and start < end: if token[start:end] in symbols: cur_output.append(token[start:end]) start = end end = len(token) else: end -= 1 if start < len(token): cur_output.append('[UNK]') outputs.append(' '.join(cur_output)) return outputs
In the following, we use the subwords in list symbols, which is learned from the aforementioned dataset, to segment tokens that represent another dataset.
tokens = ['tallest_', 'fatter_'] print(segment_BPE(tokens, symbols))
['tall e s t _', 'fa t t er_']
Summary
• The fastText model proposes a subword embedding approach. Based on the skip-gram model in word2vec, it represents a center word as the sum of its subword vectors.
• Byte pair encoding performs a statistical analysis of the training dataset to discover common symbols within a word. As a greedy approach, byte pair encoding iteratively merges the most frequent pair of consecutive symbols.
• Subword embedding may improve the quality of representations of rare words and out-ofdictionary words.

694

Chapter 14. Natural Language Processing: Pretraining

Exercises
1. As an example, there are about 3 × 108 possible 6-grams in English. What is the issue when there are too many subwords? How to address the issue? Hint: refer to the end of Section 3.2 of the fastText paper (Bojanowski et al., 2017).
2. How to design a subword embedding model based on the continuous bag-of-words model? 3. To get a vocabulary of size m, how many merging operations are needed when the initial
symbol vocabulary size is n? 4. How to extend the idea of byte pair encoding to extract phrases? Discussions198
14.7 Word Similarity and Analogy
In Section 14.4, we trained a word2vec model on a small dataset, and applied it to find semantically similar words for an input word. In practice, word vectors that are pretrained on large corpora can be applied to downstream natural language processing tasks, which will be covered later in Chapter 15. To demonstrate semantics of pretrained word vectors from large corpora in a straightforward way, let us apply them in the word similarity and analogy tasks.
import os from mxnet import np, npx from d2l import mxnet as d2l
npx.set_np()

14.7.1 Loading Pretrained Word Vectors
Below lists pretrained GloVe embeddings of dimension 50, 100, and 300, which can be downloaded from the GloVe website199. The pretrained fastText embeddings are available in multiple languages. Here we consider one English version (300-dimensional “wiki.en”) that can be downloaded from the fastText website200.
#@save d2l.DATA_HUB['glove.6b.50d'] = (d2l.DATA_URL + 'glove.6B.50d.zip',
'0b8703943ccdb6eb788e6f091b8946e82231bc4d')
#@save d2l.DATA_HUB['glove.6b.100d'] = (d2l.DATA_URL + 'glove.6B.100d.zip',
'cd43bfb07e44e6f27cbcc7bc9ae3d80284fdaf5a')
#@save d2l.DATA_HUB['glove.42b.300d'] = (d2l.DATA_URL + 'glove.42B.300d.zip',
'b5116e234e9eb9076672cfeabf5469f3eec904fa')

198 https://discuss.d2l.ai/t/386 199 https://nlp.stanford.edu/projects/glove/ 200 https://fasttext.cc/

(continues on next page)

14.7. Word Similarity and Analogy

695

(continued from previous page)
#@save d2l.DATA_HUB['wiki.en'] = (d2l.DATA_URL + 'wiki.en.zip',
'c1816da3821ae9f43899be655002f6c723e91b88')
To load these pretrained GloVe and fastText embeddings, we define the following TokenEmbedding class.
#@save class TokenEmbedding:
"""Token Embedding.""" def __init__(self, embedding_name):
self.idx_to_token, self.idx_to_vec = self._load_embedding( embedding_name)
self.unknown_idx = 0 self.token_to_idx = {
token: idx for idx, token in enumerate(self.idx_to_token)}
def _load_embedding(self, embedding_name): idx_to_token, idx_to_vec = ['<unk>'], [] data_dir = d2l.download_extract(embedding_name) # GloVe website: https://nlp.stanford.edu/projects/glove/ # fastText website: https://fasttext.cc/ with open(os.path.join(data_dir, 'vec.txt'), 'r') as f: for line in f: elems = line.rstrip().split(' ') token, elems = elems[0], [float(elem) for elem in elems[1:]] # Skip header information, such as the top row in fastText if len(elems) > 1: idx_to_token.append(token) idx_to_vec.append(elems) idx_to_vec = [[0] * len(idx_to_vec[0])] + idx_to_vec return idx_to_token, np.array(idx_to_vec)
def __getitem__(self, tokens): indices = [ self.token_to_idx.get(token, self.unknown_idx) for token in tokens] vecs = self.idx_to_vec[np.array(indices)] return vecs
def __len__(self): return len(self.idx_to_token)
Below we load the 50-dimensional GloVe embeddings (pretrained on a Wikipedia subset). When creating the TokenEmbedding instance, the specified embedding file has to be downloaded if it was not yet.
glove_6b50d = TokenEmbedding('glove.6b.50d')
Output the vocabulary size. The vocabulary contains 400000 words (tokens) and a special unknown token.
len(glove_6b50d)

696

Chapter 14. Natural Language Processing: Pretraining

400001 We can get the index of a word in the vocabulary, and vice versa. glove_6b50d.token_to_idx['beautiful'], glove_6b50d.idx_to_token[3367]
(3367, 'beautiful')

14.7.2 Applying Pretrained Word Vectors
Using the loaded GloVe vectors, we will demonstrate their semantics by applying them in the following word similarity and analogy tasks.
Word Similarity
Similar to Section 14.4.3, in order to find semantically similar words for an input word based on cosine similarities between word vectors, we implement the following knn (k-nearest neighbors) function.
def knn(W, x, k): # Add 1e-9 for numerical stability cos = np.dot(W, x.reshape( -1,)) / (np.sqrt(np.sum(W * W, axis=1) + 1e-9) * np.sqrt( (x * x).sum())) topk = npx.topk(cos, k=k, ret_typ='indices') return topk, [cos[int(i)] for i in topk]
Then, we search for similar words using the pretrained word vectors from the TokenEmbedding instance embed.
def get_similar_tokens(query_token, k, embed): topk, cos = knn(embed.idx_to_vec, embed[[query_token]], k + 1) for i, c in zip(topk[1:], cos[1:]): # Exclude the input word print(f'cosine sim={float(c):.3f}: {embed.idx_to_token[int(i)]}')
The vocabulary of the pretrained word vectors in glove_6b50d contains 400000 words and a special unknown token. Excluding the input word and unknown token, among this vocabulary let us find three most semantically similar words to word “chip”.
get_similar_tokens('chip', 3, glove_6b50d)
cosine sim=0.856: chips cosine sim=0.749: intel cosine sim=0.749: electronics
Below outputs similar words to “baby” and “beautiful”.
get_similar_tokens('baby', 3, glove_6b50d)

14.7. Word Similarity and Analogy

697

cosine sim=0.839: babies cosine sim=0.800: boy cosine sim=0.792: girl
get_similar_tokens('beautiful', 3, glove_6b50d)
cosine sim=0.921: lovely cosine sim=0.893: gorgeous cosine sim=0.830: wonderful

Word Analogy
Besides finding similar words, we can also apply word vectors to word analogy tasks. For example, “man”:“woman”::“son”:“daughter” is the form of a word analogy: “man” is to “woman” as “son” is to “daughter”. Specifically, the word analogy completion task can be defined as: for a word analogy a : b :: c : d, given the first three words a, b and c, find d. Denote the vector of word w by vec(w). To complete the analogy, we will find the word whose vector is most similar to the result of vec(c) + vec(b) − vec(a).
def get_analogy(token_a, token_b, token_c, embed): vecs = embed[[token_a, token_b, token_c]] x = vecs[1] - vecs[0] + vecs[2] topk, cos = knn(embed.idx_to_vec, x, 1) return embed.idx_to_token[int(topk[0])] # Remove unknown words
Let us verify the “male-female” analogy using the loaded word vectors.
get_analogy('man', 'woman', 'son', glove_6b50d)
'daughter'
Below completes a “capital-country” analogy: “beijing”:“china”::“tokyo”:“japan”. This demonstrates semantics in the pretrained word vectors.
get_analogy('beijing', 'china', 'tokyo', glove_6b50d)
'japan'
For the “adjective-superlative adjective” analogy such as “bad”:“worst”::“big”:“biggest”, we can see that the pretrained word vectors may capture the syntactic information.
get_analogy('bad', 'worst', 'big', glove_6b50d)
'biggest'
To show the captured notion of past tense in the pretrained word vectors, we can test the syntax using the “present tense-past tense” analogy: “do”:“did”::“go”:“went”.

698

Chapter 14. Natural Language Processing: Pretraining

get_analogy('do', 'did', 'go', glove_6b50d) 'went'

Summary
• In practice, word vectors that are pretrained on large corpora can be applied to downstream natural language processing tasks.
• Pretrained word vectors can be applied to the word similarity and analogy tasks.
Exercises
1. Test the fastText results using TokenEmbedding('wiki.en'). 2. When the vocabulary is extremely large, how can we find similar words or complete a word
analogy faster? Discussions201

14.8 Bidirectional Encoder Representations from Transformers (BERT)
We have introduced several word embedding models for natural language understanding. After pretraining, the output can be thought of as a matrix where each row is a vector that represents a word of a predefined vocabulary. In fact, these word embedding models are all contextindependent. Let us begin by illustrating this property.

14.8.1 From Context-Independent to Context-Sensitive
Recall the experiments in Section 14.4 and Section 14.7. For instance, word2vec and GloVe both assign the same pretrained vector to the same word regardless of the context of the word (if any). Formally, a context-independent representation of any token x is a function f (x) that only takes x as its input. Given the abundance of polysemy and complex semantics in natural languages, context-independent representations have obvious limitations. For instance, the word “crane” in contexts “a crane is flying” and “a crane driver came” has completely different meanings; thus, the same word may be assigned different representations depending on contexts.
This motivates the development of context-sensitive word representations, where representations of words depend on their contexts. Hence, a context-sensitive representation of token x is a function f (x, c(x)) depending on both x and its context c(x). Popular context-sensitive representations include TagLM (language-model-augmented sequence tagger) (Peters et al., 2017b), CoVe (Context Vectors) (McCann et al., 2017), and ELMo (Embeddings from Language Models) (Peters et al., 2018).
For example, by taking the entire sequence as the input, ELMo is a function that assigns a representation to each word from the input sequence. Specifically, ELMo combines all the intermediate layer representations from pretrained bidirectional LSTM as the output representation.
201 https://discuss.d2l.ai/t/387

14.8. Bidirectional Encoder Representations from Transformers (BERT)

699

Then the ELMo representation will be added to a downstream taskʼs existing supervised model as additional features, such as by concatenating ELMo representation and the original representation (e.g., GloVe) of tokens in the existing model. On one hand, all the weights in the pretrained bidirectional LSTM model are frozen after ELMo representations are added. On the other hand, the existing supervised model is specifically customized for a given task. Leveraging different best models for different tasks at that time, adding ELMo improved the state of the art across six natural language processing tasks: sentiment analysis, natural language inference, semantic role labeling, coreference resolution, named entity recognition, and question answering.
14.8.2 From Task-Specific to Task-Agnostic
Although ELMo has significantly improved solutions to a diverse set of natural language processing tasks, each solution still hinges on a task-specific architecture. However, it is practically nontrivial to craft a specific architecture for every natural language processing task. The GPT (Generative Pre-Training) model represents an effort in designing a general task-agnostic model for context-sensitive representations (Radford et al., 2018). Built on a transformer decoder, GPT pretrains a language model that will be used to represent text sequences. When applying GPT to a downstream task, the output of the language model will be fed into an added linear output layer to predict the label of the task. In sharp contrast to ELMo that freezes parameters of the pretrained model, GPT fine-tunes all the parameters in the pretrained transformer decoder during supervised learning of the downstream task. GPT was evaluated on twelve tasks of natural language inference, question answering, sentence similarity, and classification, and improved the state of the art in nine of them with minimal changes to the model architecture.
However, due to the autoregressive nature of language models, GPT only looks forward (left-toright). In contexts “i went to the bank to deposit cash” and “i went to the bank to sit down”, as “bank” is sensitive to the context to its left, GPT will return the same representation for “bank”, though it has different meanings.
14.8.3 BERT: Combining the Best of Both Worlds
As we have seen, ELMo encodes context bidirectionally but uses task-specific architectures; while GPT is task-agnostic but encodes context left-to-right. Combining the best of both worlds, BERT (Bidirectional Encoder Representations from Transformers) encodes context bidirectionally and requires minimal architecture changes for a wide range of natural language processing tasks (Devlin et al., 2018). Using a pretrained transformer encoder, BERT is able to represent any token based on its bidirectional context. During supervised learning of downstream tasks, BERT is similar to GPT in two aspects. First, BERT representations will be fed into an added output layer, with minimal changes to the model architecture depending on nature of tasks, such as predicting for every token vs. predicting for the entire sequence. Second, all the parameters of the pretrained transformer encoder are fine-tuned, while the additional output layer will be trained from scratch. Fig. 14.8.1 depicts the differences among ELMo, GPT, and BERT.

700

Chapter 14. Natural Language Processing: Pretraining

Fig. 14.8.1: A comparison of ELMo, GPT, and BERT.
BERT further improved the state of the art on eleven natural language processing tasks under broad categories of (i) single text classification (e.g., sentiment analysis), (ii) text pair classification (e.g., natural language inference), (iii) question answering, (iv) text tagging (e.g., named entity recognition). All proposed in 2018, from context-sensitive ELMo to task-agnostic GPT and BERT, conceptually simple yet empirically powerful pretraining of deep representations for natural languages have revolutionized solutions to various natural language processing tasks.
In the rest of this chapter, we will dive into the pretraining of BERT. When natural language processing applications are explained in Chapter 15, we will illustrate fine-tuning of BERT for downstream applications.
from mxnet import gluon, np, npx from mxnet.gluon import nn from d2l import mxnet as d2l
npx.set_np()

14.8.4 Input Representation

In natural language processing, some tasks (e.g., sentiment analysis) take single text as the input, while in some other tasks (e.g., natural language inference), the input is a pair of text sequences. The BERT input sequence unambiguously represents both single text and text pairs. In the former, the BERT input sequence is the concatenation of the special classification token “<cls>”, tokens of a text sequence, and the special separation token “<sep>”. In the latter, the BERT input sequence is the concatenation of “<cls>”, tokens of the first text sequence, “<sep>”, tokens of the second text sequence, and “<sep>”. We will consistently distinguish the terminology “BERT input sequence” from other types of “sequences”. For instance, one BERT input sequence may include either one text sequence or two text sequences.
To distinguish text pairs, the learned segment embeddings eA and eB are added to the token embeddings of the first sequence and the second sequence, respectively. For single text inputs, only

14.8. Bidirectional Encoder Representations from Transformers (BERT)

701

eA is used.
The following get_tokens_and_segments takes either one sentence or two sentences as the input, then returns tokens of the BERT input sequence and their corresponding segment IDs.

#@save def get_tokens_and_segments(tokens_a, tokens_b=None):
"""Get tokens of the BERT input sequence and their segment tokens = ['<cls>'] + tokens_a + ['<sep>'] # 0 and 1 are marking segment A and B, respectively segments = [0] * (len(tokens_a) + 2) if tokens_b is not None:
tokens += tokens_b + ['<sep>'] segments += [1] * (len(tokens_b) + 1) return tokens, segments

IDs."""

BERT chooses the transformer encoder as its bidirectional architecture. Common in the transformer encoder, positional embeddings are added at every position of the BERT input sequence. However, different from the original transformer encoder, BERT uses learnable positional embeddings. To sum up, Fig. 14.8.2 shows that the embeddings of the BERT input sequence are the sum of the token embeddings, segment embeddings, and positional embeddings.

Fig. 14.8.2: The embeddings of the BERT input sequence are the sum of the token embeddings, segment embeddings, and positional embeddings.

The following BERTEncoder class is similar to the TransformerEncoder class as implemented in Section 10.7. Different from TransformerEncoder, BERTEncoder uses segment embeddings and learnable positional embeddings.
#@save class BERTEncoder(nn.Block):
"""BERT encoder.""" def __init__(self, vocab_size, num_hiddens, ffn_num_hiddens, num_heads,
num_layers, dropout, max_len=1000, **kwargs): super(BERTEncoder, self).__init__(**kwargs) self.token_embedding = nn.Embedding(vocab_size, num_hiddens) self.segment_embedding = nn.Embedding(2, num_hiddens) self.blks = nn.Sequential() for _ in range(num_layers):
self.blks.add( d2l.EncoderBlock(num_hiddens, ffn_num_hiddens, num_heads,
(continues on next page)

702

Chapter 14. Natural Language Processing: Pretraining

(continued from previous page)
dropout, True)) # In BERT, positional embeddings are learnable, thus we create a # parameter of positional embeddings that are long enough self.pos_embedding = self.params.get('pos_embedding',
shape=(1, max_len, num_hiddens))
def forward(self, tokens, segments, valid_lens): # Shape of `X` remains unchanged in the following code snippet: # (batch size, max sequence length, `num_hiddens`) X = self.token_embedding(tokens) + self.segment_embedding(segments) X = X + self.pos_embedding.data(ctx=X.ctx)[:, :X.shape[1], :] for blk in self.blks: X = blk(X, valid_lens) return X
Suppose that the vocabulary size is 10000. To demonstrate forward inference of BERTEncoder, let us create an instance of it and initialize its parameters.
vocab_size, num_hiddens, ffn_num_hiddens, num_heads = 10000, 768, 1024, 4 num_layers, dropout = 2, 0.2 encoder = BERTEncoder(vocab_size, num_hiddens, ffn_num_hiddens, num_heads,
num_layers, dropout) encoder.initialize()
We define tokens to be 2 BERT input sequences of length 8, where each token is an index of the vocabulary. The forward inference of BERTEncoder with the input tokens returns the encoded result where each token is represented by a vector whose length is predefined by the hyperparameter num_hiddens. This hyperparameter is usually referred to as the hidden size (number of hidden units) of the transformer encoder.
tokens = np.random.randint(0, vocab_size, (2, 8)) segments = np.array([[0, 0, 0, 0, 1, 1, 1, 1], [0, 0, 0, 1, 1, 1, 1, 1]]) encoded_X = encoder(tokens, segments, None) encoded_X.shape
(2, 8, 768)

14.8. Bidirectional Encoder Representations from Transformers (BERT)

703

14.8.5 Pretraining Tasks
The forward inference of BERTEncoder gives the BERT representation of each token of the input text and the inserted special tokens “<cls>” and “<seq>”. Next, we will use these representations to compute the loss function for pretraining BERT. The pretraining is composed of the following two tasks: masked language modeling and next sentence prediction.

Masked Language Modeling

As illustrated in Section 8.3, a language model predicts a token using the context on its left. To encode context bidirectionally for representing each token, BERT randomly masks tokens and uses tokens from the bidirectional context to predict the masked tokens in a self-supervised fashion. This task is referred to as a masked language model.
In this pretraining task, 15% of tokens will be selected at random as the masked tokens for prediction. To predict a masked token without cheating by using the label, one straightforward approach is to always replace it with a special “<mask>” token in the BERT input sequence. However, the artificial special token “<mask>” will never appear in fine-tuning. To avoid such a mismatch between pretraining and fine-tuning, if a token is masked for prediction (e.g., “great” is selected to be masked and predicted in “this movie is great”), in the input it will be replaced with:
• a special “<mask>” token for 80% of the time (e.g., “this movie is great” becomes “this movie is <mask>”);
• a random token for 10% of the time (e.g., “this movie is great” becomes “this movie is drink”);
• the unchanged label token for 10% of the time (e.g., “this movie is great” becomes “this movie is great”).
Note that for 10% of 15% time a random token is inserted. This occasional noise encourages BERT to be less biased towards the masked token (especially when the label token remains unchanged) in its bidirectional context encoding.
We implement the following MaskLM class to predict masked tokens in the masked language model task of BERT pretraining. The prediction uses a one-hidden-layer MLP (self.mlp). In forward inference, it takes two inputs: the encoded result of BERTEncoder and the token positions for prediction. The output is the prediction results at these positions.

#@save class MaskLM(nn.Block):
"""The masked language model task of BERT.""" def __init__(self, vocab_size, num_hiddens, **kwargs):
super(MaskLM, self).__init__(**kwargs) self.mlp = nn.Sequential() self.mlp.add(nn.Dense(num_hiddens, flatten=False, activation='relu')) self.mlp.add(nn.LayerNorm()) self.mlp.add(nn.Dense(vocab_size, flatten=False))

def forward(self, X, pred_positions): num_pred_positions = pred_positions.shape[1] pred_positions = pred_positions.reshape(-1) batch_size = X.shape[0] batch_idx = np.arange(0, batch_size) # Suppose that `batch_size` = 2, `num_pred_positions` = 3, then

(continues on next page)

704

Chapter 14. Natural Language Processing: Pretraining

(continued from previous page) # `batch_idx` is `np.array([0, 0, 0, 1, 1, 1])` batch_idx = np.repeat(batch_idx, num_pred_positions) masked_X = X[batch_idx, pred_positions] masked_X = masked_X.reshape((batch_size, num_pred_positions, -1)) mlm_Y_hat = self.mlp(masked_X) return mlm_Y_hat
To demonstrate the forward inference of MaskLM, we create its instance mlm and initialize it. Recall that encoded_X from the forward inference of BERTEncoder represents 2 BERT input sequences. We define mlm_positions as the 3 indices to predict in either BERT input sequence of encoded_X. The forward inference of mlm returns prediction results mlm_Y_hat at all the masked positions mlm_positions of encoded_X. For each prediction, the size of the result is equal to the vocabulary size.
mlm = MaskLM(vocab_size, num_hiddens) mlm.initialize() mlm_positions = np.array([[1, 5, 2], [6, 1, 5]]) mlm_Y_hat = mlm(encoded_X, mlm_positions) mlm_Y_hat.shape
(2, 3, 10000)
With the ground truth labels mlm_Y of the predicted tokens mlm_Y_hat under masks, we can calculate the cross-entropy loss of the masked language model task in BERT pretraining.
mlm_Y = np.array([[7, 8, 9], [10, 20, 30]]) loss = gluon.loss.SoftmaxCrossEntropyLoss() mlm_l = loss(mlm_Y_hat.reshape((-1, vocab_size)), mlm_Y.reshape(-1)) mlm_l.shape
(6,)

Next Sentence Prediction

Although masked language modeling is able to encode bidirectional context for representing words, it does not explicitly model the logical relationship between text pairs. To help understand the relationship between two text sequences, BERT considers a binary classification task, next sentence prediction, in its pretraining. When generating sentence pairs for pretraining, for half of the time they are indeed consecutive sentences with the label “True”; while for the other half of the time the second sentence is randomly sampled from the corpus with the label “False”.
The following NextSentencePred class uses a one-hidden-layer MLP to predict whether the second sentence is the next sentence of the first in the BERT input sequence. Due to self-attention in the transformer encoder, the BERT representation of the special token “<cls>” encodes both the two sentences from the input. Hence, the output layer (self.output) of the MLP classifier takes X as the input, where X is the output of the MLP hidden layer whose input is the encoded “<cls>” token.

#@save class NextSentencePred(nn.Block):

(continues on next page)

14.8. Bidirectional Encoder Representations from Transformers (BERT)

705

"""The next sentence prediction task of BERT.""" def __init__(self, **kwargs):
super(NextSentencePred, self).__init__(**kwargs) self.output = nn.Dense(2)
def forward(self, X): # `X` shape: (batch size, `num_hiddens`) return self.output(X)

(continued from previous page)

We can see that the forward inference of an NextSentencePred instance returns binary predictions for each BERT input sequence.
nsp = NextSentencePred() nsp.initialize() nsp_Y_hat = nsp(encoded_X) nsp_Y_hat.shape

(2, 2)
The cross-entropy loss of the 2 binary classifications can also be computed.
nsp_y = np.array([0, 1]) nsp_l = loss(nsp_Y_hat, nsp_y) nsp_l.shape

(2,)
It is noteworthy that all the labels in both the aforementioned pretraining tasks can be trivially obtained from the pretraining corpus without manual labeling effort. The original BERT has been pretrained on the concatenation of BookCorpus (Zhu et al., 2015) and English Wikipedia. These two text corpora are huge: they have 800 million words and 2.5 billion words, respectively.

14.8.6 Putting All Things Together
When pretraining BERT, the final loss function is a linear combination of both the loss functions for masked language modeling and next sentence prediction. Now we can define the BERTModel class by instantiating the three classes BERTEncoder, MaskLM, and NextSentencePred. The forward inference returns the encoded BERT representations encoded_X, predictions of masked language modeling mlm_Y_hat, and next sentence predictions nsp_Y_hat.
#@save class BERTModel(nn.Block):
"""The BERT model.""" def __init__(self, vocab_size, num_hiddens, ffn_num_hiddens, num_heads,
num_layers, dropout, max_len=1000): super(BERTModel, self).__init__() self.encoder = BERTEncoder(vocab_size, num_hiddens, ffn_num_hiddens,
num_heads, num_layers, dropout, max_len) self.hidden = nn.Dense(num_hiddens, activation='tanh')
(continues on next page)

706

Chapter 14. Natural Language Processing: Pretraining

self.mlm = MaskLM(vocab_size, num_hiddens) self.nsp = NextSentencePred()

(continued from previous page)

def forward(self, tokens, segments, valid_lens=None, pred_positions=None): encoded_X = self.encoder(tokens, segments, valid_lens) if pred_positions is not None: mlm_Y_hat = self.mlm(encoded_X, pred_positions) else: mlm_Y_hat = None # The hidden layer of the MLP classifier for next sentence prediction. # 0 is the index of the '<cls>' token nsp_Y_hat = self.nsp(self.hidden(encoded_X[:, 0, :])) return encoded_X, mlm_Y_hat, nsp_Y_hat

Summary
• Word embedding models such as word2vec and GloVe are context-independent. They assign the same pretrained vector to the same word regardless of the context of the word (if any). It is hard for them to handle well polysemy or complex semantics in natural languages.
• For context-sensitive word representations such as ELMo and GPT, representations of words depend on their contexts.
• ELMo encodes context bidirectionally but uses task-specific architectures (however, it is practically non-trivial to craft a specific architecture for every natural language processing task); while GPT is task-agnostic but encodes context left-to-right.
• BERT combines the best of both worlds: it encodes context bidirectionally and requires minimal architecture changes for a wide range of natural language processing tasks.
• The embeddings of the BERT input sequence are the sum of the token embeddings, segment embeddings, and positional embeddings.
• Pretraining BERT is composed of two tasks: masked language modeling and next sentence prediction. The former is able to encode bidirectional context for representing words, while the latter explicitly models the logical relationship between text pairs.
Exercises
1. Why does BERT succeed?
2. All other things being equal, will a masked language model require more or fewer pretraining steps to converge than a left-to-right language model? Why?
3. In the original implementation of BERT, the positionwise feed-forward network in BERTEncoder (via d2l.EncoderBlock) and the fully-connected layer in MaskLM both use the Gaussian error linear unit (GELU) (Hendrycks & Gimpel, 2016) as the activation function. Research into the difference between GELU and ReLU.
Discussions202
202 https://discuss.d2l.ai/t/388

14.8. Bidirectional Encoder Representations from Transformers (BERT)

707

14.9 The Dataset for Pretraining BERT
To pretrain the BERT model as implemented in Section 14.8, we need to generate the dataset in the ideal format to facilitate the two pretraining tasks: masked language modeling and next sentence prediction. On one hand, the original BERT model is pretrained on the concatenation of two huge corpora BookCorpus and English Wikipedia (see Section 14.8.5), making it hard to run for most readers of this book. On the other hand, the off-the-shelf pretrained BERT model may not fit for applications from specific domains like medicine. Thus, it is getting popular to pretrain BERT on a customized dataset. To facilitate the demonstration of BERT pretraining, we use a smaller corpus WikiText-2 (Merity et al., 2016).
Comparing with the PTB dataset used for pretraining word2vec in Section 14.3, WikiText-2 (i) retains the original punctuation, making it suitable for next sentence prediction; (ii) retains the original case and numbers; (iii) is over twice larger.
import os import random from mxnet import gluon, np, npx from d2l import mxnet as d2l
npx.set_np()
In the WikiText-2 dataset, each line represents a paragraph where space is inserted between any punctuation and its preceding token. Paragraphs with at least two sentences are retained. To split sentences, we only use the period as the delimiter for simplicity. We leave discussions of more complex sentence splitting techniques in the exercises at the end of this section.
#@save d2l.DATA_HUB['wikitext-2'] = (
'https://s3.amazonaws.com/research.metamind.io/wikitext/' 'wikitext-2-v1.zip', '3c914d17d80b1459be871a5039ac23e752a53cbe')
#@save def _read_wiki(data_dir):
file_name = os.path.join(data_dir, 'wiki.train.tokens') with open(file_name, 'r') as f:
lines = f.readlines() # Uppercase letters are converted to lowercase ones paragraphs = [
line.strip().lower().split(' . ') for line in lines if len(line.split(' . ')) >= 2] random.shuffle(paragraphs) return paragraphs

708

Chapter 14. Natural Language Processing: Pretraining

14.9.1 Defining Helper Functions for Pretraining Tasks
In the following, we begin by implementing helper functions for the two BERT pretraining tasks: next sentence prediction and masked language modeling. These helper functions will be invoked later when transforming the raw text corpus into the dataset of the ideal format to pretrain BERT.
Generating the Next Sentence Prediction Task
According to descriptions of Section 14.8.5, the _get_next_sentence function generates a training example for the binary classification task.
#@save def _get_next_sentence(sentence, next_sentence, paragraphs):
if random.random() < 0.5: is_next = True
else: # `paragraphs` is a list of lists of lists next_sentence = random.choice(random.choice(paragraphs)) is_next = False
return sentence, next_sentence, is_next
The following function generates training examples for next sentence prediction from the input paragraph by invoking the _get_next_sentence function. Here paragraph is a list of sentences, where each sentence is a list of tokens. The argument max_len specifies the maximum length of a BERT input sequence during pretraining.
#@save def _get_nsp_data_from_paragraph(paragraph, paragraphs, vocab, max_len):
nsp_data_from_paragraph = [] for i in range(len(paragraph) - 1):
tokens_a, tokens_b, is_next = _get_next_sentence( paragraph[i], paragraph[i + 1], paragraphs)
# Consider 1 '<cls>' token and 2 '<sep>' tokens if len(tokens_a) + len(tokens_b) + 3 > max_len:
continue tokens, segments = d2l.get_tokens_and_segments(tokens_a, tokens_b) nsp_data_from_paragraph.append((tokens, segments, is_next)) return nsp_data_from_paragraph

Generating the Masked Language Modeling Task
In order to generate training examples for the masked language modeling task from a BERT input sequence, we define the following _replace_mlm_tokens function. In its inputs, tokens is a list of tokens representing a BERT input sequence, candidate_pred_positions is a list of token indices of the BERT input sequence excluding those of special tokens (special tokens are not predicted in the masked language modeling task), and num_mlm_preds indicates the number of predictions (recall 15% random tokens to predict). Following the definition of the masked language modeling task in Section 14.8.5, at each prediction position, the input may be replaced by a special “<mask>” token or a random token, or remain unchanged. In the end, the function returns the input tokens after possible replacement, the token indices where predictions take place and labels for these predictions.

14.9. The Dataset for Pretraining BERT

709

#@save def _replace_mlm_tokens(tokens, candidate_pred_positions, num_mlm_preds,
vocab): # Make a new copy of tokens for the input of a masked language model, # where the input may contain replaced '<mask>' or random tokens mlm_input_tokens = [token for token in tokens] pred_positions_and_labels = [] # Shuffle for getting 15% random tokens for prediction in the masked # language modeling task random.shuffle(candidate_pred_positions) for mlm_pred_position in candidate_pred_positions:
if len(pred_positions_and_labels) >= num_mlm_preds: break
masked_token = None # 80% of the time: replace the word with the '<mask>' token if random.random() < 0.8:
masked_token = '<mask>' else:
# 10% of the time: keep the word unchanged if random.random() < 0.5:
masked_token = tokens[mlm_pred_position] # 10% of the time: replace the word with a random word else:
masked_token = random.randint(0, len(vocab) - 1) mlm_input_tokens[mlm_pred_position] = masked_token pred_positions_and_labels.append(
(mlm_pred_position, tokens[mlm_pred_position])) return mlm_input_tokens, pred_positions_and_labels
By invoking the aforementioned _replace_mlm_tokens function, the following function takes a BERT input sequence (tokens) as an input and returns indices of the input tokens (after possible token replacement as described in Section 14.8.5), the token indices where predictions take place, and label indices for these predictions.
#@save def _get_mlm_data_from_tokens(tokens, vocab):
candidate_pred_positions = [] # `tokens` is a list of strings for i, token in enumerate(tokens):
# Special tokens are not predicted in the masked language modeling # task if token in ['<cls>', '<sep>']:
continue candidate_pred_positions.append(i) # 15% of random tokens are predicted in the masked language modeling task num_mlm_preds = max(1, round(len(tokens) * 0.15)) mlm_input_tokens, pred_positions_and_labels = _replace_mlm_tokens( tokens, candidate_pred_positions, num_mlm_preds, vocab) pred_positions_and_labels = sorted(pred_positions_and_labels,
key=lambda x: x[0]) pred_positions = [v[0] for v in pred_positions_and_labels] mlm_pred_labels = [v[1] for v in pred_positions_and_labels] return vocab[mlm_input_tokens], pred_positions, vocab[mlm_pred_labels]

710

Chapter 14. Natural Language Processing: Pretraining

14.9.2 Transforming Text into the Pretraining Dataset
Now we are almost ready to customize a Dataset class for pretraining BERT. Before that, we still need to define a helper function _pad_bert_inputs to append the special “<mask>” tokens to the inputs. Its argument examples contain the outputs from the helper functions _get_nsp_data_from_paragraph and _get_mlm_data_from_tokens for the two pretraining tasks.
#@save def _pad_bert_inputs(examples, max_len, vocab):
max_num_mlm_preds = round(max_len * 0.15) all_token_ids, all_segments, valid_lens, = [], [], [] all_pred_positions, all_mlm_weights, all_mlm_labels = [], [], [] nsp_labels = [] for (token_ids, pred_positions, mlm_pred_label_ids, segments,
is_next) in examples: all_token_ids.append(
np.array( token_ids + [vocab['<pad>']] * (max_len - len(token_ids)), dtype='int32'))
all_segments.append( np.array(segments + [0] * (max_len - len(segments)), dtype='int32'))
# `valid_lens` excludes count of '<pad>' tokens valid_lens.append(np.array(len(token_ids), dtype='float32')) all_pred_positions.append(
np.array( pred_positions + [0] * (max_num_mlm_preds - len(pred_positions)), dtype='int32'))
# Predictions of padded tokens will be filtered out in the loss via # multiplication of 0 weights all_mlm_weights.append(
np.array([1.0] * len(mlm_pred_label_ids) + [0.0] * (max_num_mlm_preds - len(pred_positions)), dtype='float32'))
all_mlm_labels.append( np.array( mlm_pred_label_ids + [0] * (max_num_mlm_preds - len(mlm_pred_label_ids)), dtype='int32'))
nsp_labels.append(np.array(is_next)) return (all_token_ids, all_segments, valid_lens, all_pred_positions,
all_mlm_weights, all_mlm_labels, nsp_labels)

Putting the helper functions for generating training examples of the two pretraining tasks, and the helper function for padding inputs together, we customize the following _WikiTextDataset class as the WikiText-2 dataset for pretraining BERT. By implementing the __getitem__function, we can arbitrarily access the pretraining (masked language modeling and next sentence prediction) examples generated from a pair of sentences from the WikiText-2 corpus.
The original BERT model uses WordPiece embeddings whose vocabulary size is 30000 (Wu et al., 2016). The tokenization method of WordPiece is a slight modification of the original byte pair encoding algorithm in Section 14.6.2. For simplicity, we use the d2l.tokenize function for tokenization. Infrequent tokens that appear less than five times are filtered out.

#@save class _WikiTextDataset(gluon.data.Dataset):

(continues on next page)

14.9. The Dataset for Pretraining BERT

711

(continued from previous page)
def __init__(self, paragraphs, max_len): # Input `paragraphs[i]` is a list of sentence strings representing a # paragraph; while output `paragraphs[i]` is a list of sentences # representing a paragraph, where each sentence is a list of tokens paragraphs = [ d2l.tokenize(paragraph, token='word') for paragraph in paragraphs] sentences = [ sentence for paragraph in paragraphs for sentence in paragraph] self.vocab = d2l.Vocab( sentences, min_freq=5, reserved_tokens=['<pad>', '<mask>', '<cls>', '<sep>']) # Get data for the next sentence prediction task examples = [] for paragraph in paragraphs: examples.extend( _get_nsp_data_from_paragraph(paragraph, paragraphs, self.vocab, max_len)) # Get data for the masked language model task examples = [(_get_mlm_data_from_tokens(tokens, self.vocab) + (segments, is_next)) for tokens, segments, is_next in examples] # Pad inputs (self.all_token_ids, self.all_segments, self.valid_lens, self.all_pred_positions, self.all_mlm_weights, self.all_mlm_labels, self.nsp_labels) = _pad_bert_inputs(examples, max_len, self.vocab)
def __getitem__(self, idx): return (self.all_token_ids[idx], self.all_segments[idx], self.valid_lens[idx], self.all_pred_positions[idx], self.all_mlm_weights[idx], self.all_mlm_labels[idx], self.nsp_labels[idx])
def __len__(self): return len(self.all_token_ids)
By using the _read_wiki function and the _WikiTextDataset class, we define the following load_data_wiki to download and WikiText-2 dataset and generate pretraining examples from it.
#@save def load_data_wiki(batch_size, max_len):
"""Load the WikiText-2 dataset.""" num_workers = d2l.get_dataloader_workers() data_dir = d2l.download_extract('wikitext-2', 'wikitext-2') paragraphs = _read_wiki(data_dir) train_set = _WikiTextDataset(paragraphs, max_len) train_iter = gluon.data.DataLoader(train_set, batch_size, shuffle=True,
num_workers=num_workers) return train_iter, train_set.vocab
Setting the batch size to 512 and the maximum length of a BERT input sequence to be 64, we print out the shapes of a minibatch of BERT pretraining examples. Note that in each BERT input sequence, 10 (64 × 0.15) positions are predicted for the masked language modeling task.

712

Chapter 14. Natural Language Processing: Pretraining

batch_size, max_len = 512, 64 train_iter, vocab = load_data_wiki(batch_size, max_len)
for (tokens_X, segments_X, valid_lens_x, pred_positions_X, mlm_weights_X, mlm_Y, nsp_y) in train_iter: print(tokens_X.shape, segments_X.shape, valid_lens_x.shape, pred_positions_X.shape, mlm_weights_X.shape, mlm_Y.shape, nsp_y.shape) break
(512, 64) (512, 64) (512,) (512, 10) (512, 10) (512, 10) (512,)
In the end, let us take a look at the vocabulary size. Even after filtering out infrequent tokens, it is still over twice larger than that of the PTB dataset.
len(vocab)
20256
Summary
• Comparing with the PTB dataset, the WikiText-2 dateset retains the original punctuation, case and numbers, and is over twice larger.
• We can arbitrarily access the pretraining (masked language modeling and next sentence prediction) examples generated from a pair of sentences from the WikiText-2 corpus.
Exercises
1. For simplicity, the period is used as the only delimiter for splitting sentences. Try other sentence splitting techniques, such as the spaCy and NLTK. Take NLTK as an example. You need to install NLTK first: pip install nltk. In the code, first import nltk. Then, download the Punkt sentence tokenizer: nltk.download('punkt'). To split sentences such as sentences = 'This is great ! Why not ?', invoking nltk.tokenize.sent_tokenize(sentences) will return a list of two sentence strings: ['This is great !', 'Why not ?'].
2. What is the vocabulary size if we do not filter out any infrequent token? Discussions203
203 https://discuss.d2l.ai/t/389

14.9. The Dataset for Pretraining BERT

713

14.10 Pretraining BERT
With the BERT model implemented in Section 14.8 and the pretraining examples generated from the WikiText-2 dataset in Section 14.9, we will pretrain BERT on the WikiText-2 dataset in this section.
from mxnet import autograd, gluon, init, np, npx from d2l import mxnet as d2l
npx.set_np()
To start, we load the WikiText-2 dataset as minibatches of pretraining examples for masked language modeling and next sentence prediction. The batch size is 512 and the maximum length of a BERT input sequence is 64. Note that in the original BERT model, the maximum length is 512.
batch_size, max_len = 512, 64 train_iter, vocab = d2l.load_data_wiki(batch_size, max_len)

14.10.1 Pretraining BERT
The original BERT has two versions of different model sizes (Devlin et al., 2018). The base model (BERTBASE) uses 12 layers (transformer encoder blocks) with 768 hidden units (hidden size) and 12 self-attention heads. The large model (BERTLARGE) uses 24 layers with 1024 hidden units and 16 self-attention heads. Notably, the former has 110 million parameters while the latter has 340 million parameters. For demonstration with ease, we define a small BERT, using 2 layers, 128 hidden units, and 2 self-attention heads.
net = d2l.BERTModel(len(vocab), num_hiddens=128, ffn_num_hiddens=256, num_heads=2, num_layers=2, dropout=0.2)
devices = d2l.try_all_gpus() net.initialize(init.Xavier(), ctx=devices) loss = gluon.loss.SoftmaxCELoss()
Before defining the training loop, we define a helper function _get_batch_loss_bert. Given the shard of training examples, this function computes the loss for both the masked language modeling and next sentence prediction tasks. Note that the final loss of BERT pretraining is just the sum of both the masked language modeling loss and the next sentence prediction loss.
#@save def _get_batch_loss_bert(net, loss, vocab_size, tokens_X_shards,
segments_X_shards, valid_lens_x_shards, pred_positions_X_shards, mlm_weights_X_shards, mlm_Y_shards, nsp_y_shards): mlm_ls, nsp_ls, ls = [], [], [] for (tokens_X_shard, segments_X_shard, valid_lens_x_shard, pred_positions_X_shard, mlm_weights_X_shard, mlm_Y_shard, nsp_y_shard) in zip(tokens_X_shards, segments_X_shards,
valid_lens_x_shards, pred_positions_X_shards, mlm_weights_X_shards, mlm_Y_shards, nsp_y_shards): # Forward pass
(continues on next page)

714

Chapter 14. Natural Language Processing: Pretraining

(continued from previous page)
_, mlm_Y_hat, nsp_Y_hat = net(tokens_X_shard, segments_X_shard, valid_lens_x_shard.reshape(-1), pred_positions_X_shard)
# Compute masked language model loss mlm_l = loss(mlm_Y_hat.reshape((-1, vocab_size)),
mlm_Y_shard.reshape(-1), mlm_weights_X_shard.reshape((-1, 1))) mlm_l = mlm_l.sum() / (mlm_weights_X_shard.sum() + 1e-8) # Compute next sentence prediction loss nsp_l = loss(nsp_Y_hat, nsp_y_shard) nsp_l = nsp_l.mean() mlm_ls.append(mlm_l) nsp_ls.append(nsp_l) ls.append(mlm_l + nsp_l) npx.waitall() return mlm_ls, nsp_ls, ls
Invoking the two aforementioned helper functions, the following train_bert function defines the procedure to pretrain BERT (net) on the WikiText-2 (train_iter) dataset. Training BERT can take very long. Instead of specifying the number of epochs for training as in the train_ch13 function (see Section 13.1), the input num_steps of the following function specifies the number of iteration steps for training.
def train_bert(train_iter, net, loss, vocab_size, devices, num_steps): trainer = gluon.Trainer(net.collect_params(), 'adam', {'learning_rate': 1e-3}) step, timer = 0, d2l.Timer() animator = d2l.Animator(xlabel='step', ylabel='loss', xlim=[1, num_steps], legend=['mlm', 'nsp']) # Sum of masked language modeling losses, sum of next sentence prediction # losses, no. of sentence pairs, count metric = d2l.Accumulator(4) num_steps_reached = False while step < num_steps and not num_steps_reached: for batch in train_iter: (tokens_X_shards, segments_X_shards, valid_lens_x_shards, pred_positions_X_shards, mlm_weights_X_shards, mlm_Y_shards, nsp_y_shards) = [ gluon.utils.split_and_load(elem, devices, even_split=False) for elem in batch] timer.start() with autograd.record(): mlm_ls, nsp_ls, ls = _get_batch_loss_bert( net, loss, vocab_size, tokens_X_shards, segments_X_shards, valid_lens_x_shards, pred_positions_X_shards, mlm_weights_X_shards, mlm_Y_shards, nsp_y_shards) for l in ls: l.backward() trainer.step(1) mlm_l_mean = sum([float(l) for l in mlm_ls]) / len(mlm_ls) nsp_l_mean = sum([float(l) for l in nsp_ls]) / len(nsp_ls) metric.add(mlm_l_mean, nsp_l_mean, batch[0].shape[0], 1) timer.stop() animator.add(step + 1,
(continues on next page)

14.10. Pretraining BERT

715

(continued from previous page) (metric[0] / metric[3], metric[1] / metric[3])) step += 1 if step == num_steps: num_steps_reached = True break
print(f'MLM loss {metric[0] / metric[3]:.3f}, ' f'NSP loss {metric[1] / metric[3]:.3f}')
print(f'{metric[2] / timer.sum():.1f} sentence pairs/sec on ' f'{str(devices)}')
We can plot both the masked language modeling loss and the next sentence prediction loss during BERT pretraining.
train_bert(train_iter, net, loss, len(vocab), devices, 50)
MLM loss 7.897, NSP loss 0.730 3985.5 sentence pairs/sec on [gpu(0), gpu(1)]

14.10.2 Representing Text with BERT
After pretraining BERT, we can use it to represent single text, text pairs, or any token in them. The following function returns the BERT (net) representations for all tokens in tokens_a and tokens_b.
def get_bert_encoding(net, tokens_a, tokens_b=None): tokens, segments = d2l.get_tokens_and_segments(tokens_a, tokens_b) token_ids = np.expand_dims(np.array(vocab[tokens], ctx=devices[0]), axis=0) segments = np.expand_dims(np.array(segments, ctx=devices[0]), axis=0) valid_len = np.expand_dims(np.array(len(tokens), ctx=devices[0]), axis=0) encoded_X, _, _ = net(token_ids, segments, valid_len) return encoded_X
Consider the sentence “a crane is flying”. Recall the input representation of BERT as discussed in Section 14.8.4. After inserting special tokens “<cls>” (used for classification) and “<sep>” (used for

716

Chapter 14. Natural Language Processing: Pretraining

separation), the BERT input sequence has a length of six. Since zero is the index of the “<cls>” token, encoded_text[:, 0, :] is the BERT representation of the entire input sentence. To evaluate the polysemy token “crane”, we also print out the first three elements of the BERT representation of the token.
tokens_a = ['a', 'crane', 'is', 'flying'] encoded_text = get_bert_encoding(net, tokens_a) # Tokens: '<cls>', 'a', 'crane', 'is', 'flying', '<sep>' encoded_text_cls = encoded_text[:, 0, :] encoded_text_crane = encoded_text[:, 2, :] encoded_text.shape, encoded_text_cls.shape, encoded_text_crane[0][:3]

((1, 6, 128), (1, 128), array([ 0.76616675,

1.0122514 , -0.3863761 ], ctx=gpu(0)))

Now consider a sentence pair “a crane driver came” and “he just left”. Similarly, encoded_pair[:, 0, :] is the encoded result of the entire sentence pair from the pretrained BERT. Note that the first three elements of the polysemy token “crane” are different from those when the context is different. This supports that BERT representations are context-sensitive.
tokens_a, tokens_b = ['a', 'crane', 'driver', 'came'], ['he', 'just', 'left'] encoded_pair = get_bert_encoding(net, tokens_a, tokens_b) # Tokens: '<cls>', 'a', 'crane', 'driver', 'came', '<sep>', 'he', 'just', # 'left', '<sep>' encoded_pair_cls = encoded_pair[:, 0, :] encoded_pair_crane = encoded_pair[:, 2, :] encoded_pair.shape, encoded_pair_cls.shape, encoded_pair_crane[0][:3]

((1, 10, 128), (1, 128), array([ 0.71957666,

0.9928638 , -0.45282146], ctx=gpu(0)))

In Chapter 15, we will fine-tune a pretrained BERT model for downstream natural language processing applications.

Summary
• The original BERT has two versions, where the base model has 110 million parameters and the large model has 340 million parameters.
• After pretraining BERT, we can use it to represent single text, text pairs, or any token in them. • In the experiment, the same token has different BERT representation when their contexts
are different. This supports that BERT representations are context-sensitive.

14.10. Pretraining BERT

717

Exercises
1. In the experiment, we can see that the masked language modeling loss is significantly higher than the next sentence prediction loss. Why?
2. Set the maximum length of a BERT input sequence to be 512 (same as the original BERT model). Use the configurations of the original BERT model such as BERTLARGE. Do you encounter any error when running this section? Why?
Discussions204

204 https://discuss.d2l.ai/t/390
718

Chapter 14. Natural Language Processing: Pretraining

15 | Natural Language Processing: Applications
We have seen how to represent tokens in text sequences and train their representations in Chapter 14. Such pretrained text representations can be fed to various models for different downstream natural language processing tasks. In fact, earlier chapters have already discussed some natural language processing applications without pretraining, just for explaining deep learning architectures. For instance, in Chapter 8, we have relied on RNNs to design language models to generate novella-like text. In Chapter 9 and Chapter 10, we have also designed models based on RNNs and attention mechanisms for machine translation. However, this book does not intend to cover all such applications in a comprehensive manner. Instead, our focus is on how to apply (deep) representation learning of languages to addressing natural language processing problems. Given pretrained text representations, this chapter will explore two popular and representative downstream natural language processing tasks: sentiment analysis and natural language inference, which analyze single text and relationships of text pairs, respectively.
Fig. 15.1: Pretrained text representations can be fed to various deep learning architectures for different downstream natural language processing applications. This chapter focuses on how to design models for different downstream natural language processing applications.
As depicted in Fig. 15.1, this chapter focuses on describing the basic ideas of designing natural language processing models using different types of deep learning architectures, such as MLPs, CNNs, RNNs, and attention. Though it is possible to combine any pretrained text representations
719

with any architecture for either application in Fig. 15.1, we select a few representative combinations. Specifically, we will explore popular architectures based on RNNs and CNNs for sentiment analysis. For natural language inference, we choose attention and MLPs to demonstrate how to analyze text pairs. In the end, we introduce how to fine-tune a pretrained BERT model for a wide range of natural language processing applications, such as on a sequence level (single text classification and text pair classification) and a token level (text tagging and question answering). As a concrete empirical case, we will fine-tune BERT for natural language inference.
As we have introduced in Section 14.8, BERT requires minimal architecture changes for a wide range of natural language processing applications. However, this benefit comes at the cost of finetuning a huge number of BERT parameters for the downstream applications. When space or time is limited, those crafted models based on MLPs, CNNs, RNNs, and attention are more feasible. In the following, we start by the sentiment analysis application and illustrate the model design based on RNNs and CNNs, respectively.
15.1 Sentiment Analysis and the Dataset
With the proliferation of online social media and review platforms, a plethora of opinionated data have been logged, bearing great potential for supporting decision making processes. Sentiment analysis studies peopleʼs sentiments in their produced text, such as product reviews, blog comments, and forum discussions. It enjoys wide applications to fields as diverse as politics (e.g., analysis of public sentiments towards policies), finance (e.g., analysis of sentiments of the market), and marketing (e.g., product research and brand management).
Since sentiments can be categorized as discrete polarities or scales (e.g., positive and negative), we can consider sentiment analysis as a text classification task, which transforms a varying-length text sequence into a fixed-length text category. In this chapter, we will use Stanfordʼs large movie review dataset205 for sentiment analysis. It consists of a training set and a testing set, either containing 25000 movie reviews downloaded from IMDb. In both datasets, there are equal number of “positive” and “negative” labels, indicating different sentiment polarities.
import os from mxnet import np, npx from d2l import mxnet as d2l
npx.set_np()
15.1.1 Reading the Dataset
First, download and extract this IMDb review dataset in the path ../data/aclImdb.
#@save d2l.DATA_HUB['aclImdb'] = (
'http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz', '01ada507287d82875905620988597833ad4e0903')
data_dir = d2l.download_extract('aclImdb', 'aclImdb')
205 https://ai.stanford.edu/~amaas/data/sentiment/

720

Chapter 15. Natural Language Processing: Applications

Next, read the training and test datasets. Each example is a review and its label: 1 for “positive” and 0 for “negative”.

#@save def read_imdb(data_dir, is_train):
"""Read the IMDb review dataset text sequences and labels.""" data, labels = [], [] for label in ('pos', 'neg'):
folder_name = os.path.join(data_dir, 'train' if is_train else label)
for file in os.listdir(folder_name): with open(os.path.join(folder_name, file), 'rb') as f: review = f.read().decode('utf-8').replace('\n', '') data.append(review) labels.append(1 if label == 'pos' else 0)
return data, labels

'test',

train_data = read_imdb(data_dir, is_train=True) print('# trainings:', len(train_data[0])) for x, y in zip(train_data[0][:3], train_data[1][:3]):
print('label:', y, 'review:', x[0:60])

# trainings: 25000 label: 1 review: Normally the best way to annoy me in a film is to include so label: 1 review: The Bible teaches us that the love of money is the root of a label: 1 review: Being someone who lists Night of the Living Dead at number t

15.1.2 Preprocessing the Dataset
Treating each word as a token and filtering out words that appear less than 5 times, we create a vocabulary out of the training dataset.
train_tokens = d2l.tokenize(train_data[0], token='word') vocab = d2l.Vocab(train_tokens, min_freq=5, reserved_tokens=['<pad>'])
After tokenization, let us plot the histogram of review lengths in tokens.
d2l.set_figsize() d2l.plt.xlabel('# tokens per review') d2l.plt.ylabel('count') d2l.plt.hist([len(line) for line in train_tokens], bins=range(0, 1000, 50));

15.1. Sentiment Analysis and the Dataset

721

As we expected, the reviews have varying lengths. To process a minibatch of such reviews at each time, we set the length of each review to 500 with truncation and padding, which is similar to the preprocessing step for the machine translation dataset in Section 9.5.
num_steps = 500 # sequence length train_features = np.array([
d2l.truncate_pad(vocab[line], num_steps, vocab['<pad>']) for line in train_tokens]) print(train_features.shape)
(25000, 500)
15.1.3 Creating Data Iterators
Now we can create data iterators. At each iteration, a minibatch of examples are returned.
train_iter = d2l.load_array((train_features, train_data[1]), 64)
for X, y in train_iter: print('X:', X.shape, ', y:', y.shape) break
print('# batches:', len(train_iter))
X: (64, 500) , y: (64,) # batches: 391

722

Chapter 15. Natural Language Processing: Applications

15.1.4 Putting All Things Together
Last, we wrap up the above steps into the load_data_imdb function. It returns training and test data iterators and the vocabulary of the IMDb review dataset.
#@save def load_data_imdb(batch_size, num_steps=500):
"""Return data iterators and the vocabulary of the IMDb review dataset.""" data_dir = d2l.download_extract('aclImdb', 'aclImdb') train_data = read_imdb(data_dir, True) test_data = read_imdb(data_dir, False) train_tokens = d2l.tokenize(train_data[0], token='word') test_tokens = d2l.tokenize(test_data[0], token='word') vocab = d2l.Vocab(train_tokens, min_freq=5) train_features = np.array([
d2l.truncate_pad(vocab[line], num_steps, vocab['<pad>']) for line in train_tokens]) test_features = np.array([ d2l.truncate_pad(vocab[line], num_steps, vocab['<pad>']) for line in test_tokens]) train_iter = d2l.load_array((train_features, train_data[1]), batch_size) test_iter = d2l.load_array((test_features, test_data[1]), batch_size,
is_train=False) return train_iter, test_iter, vocab
Summary
• Sentiment analysis studies peopleʼs sentiments in their produced text, which is considered as a text classification problem that transforms a varying-length text sequence into a fixedlength text category.
• After preprocessing, we can load Stanfordʼs large movie review dataset (IMDb review dataset) into data iterators with a vocabulary.
Exercises
1. What hyperparameters in this section can we modify to accelerate training sentiment analysis models?
2. Can you implement a function to load the dataset of Amazon reviews206 into data iterators and labels for sentiment analysis?
Discussions207
206 https://snap.stanford.edu/data/web-Amazon.html 207 https://discuss.d2l.ai/t/391

15.1. Sentiment Analysis and the Dataset

723

15.2 Sentiment Analysis: Using Recurrent Neural Networks
Like word similarity and analogy tasks, we can also apply pretrained word vectors to sentiment analysis. Since the IMDb review dataset in Section 15.1 is not very big, using text representations that were pretrained on large-scale corpora may reduce overfitting of the model. As a specific example illustrated in Fig. 15.2.1, we will represent each token using the pretrained GloVe model, and feed these token representations into a multilayer bidirectional RNN to obtain the text sequence representation, which will be transformed into sentiment analysis outputs (Maas et al., 2011). For the same downstream application, we will consider a different architectural choice later.

Fig. 15.2.1: This section feeds pretrained GloVe to an RNN-based architecture for sentiment analysis.
from mxnet import gluon, init, np, npx from mxnet.gluon import nn, rnn from d2l import mxnet as d2l
npx.set_np()
batch_size = 64 train_iter, test_iter, vocab = d2l.load_data_imdb(batch_size)
15.2.1 Representing Single Text with RNNs
In text classifications tasks, such as sentiment analysis, a varying-length text sequence will be transformed into fixed-length categories. In the following BiRNN class, while each token of a text sequence gets its individual pretrained GloVe representation via the embedding layer (self. embedding), the entire sequence is encoded by a bidirectional RNN (self.encoder). More concretely, the hidden states (at the last layer) of the bidirectional LSTM at both the initial and final time steps are concatenated as the representation of the text sequence. This single text representation is then transformed into output categories by a fully-connected layer (self.decoder) with two outputs (“positive” and “negative”).

724

Chapter 15. Natural Language Processing: Applications

class BiRNN(nn.Block): def __init__(self, vocab_size, embed_size, num_hiddens, num_layers, **kwargs): super(BiRNN, self).__init__(**kwargs) self.embedding = nn.Embedding(vocab_size, embed_size) # Set `bidirectional` to True to get a bidirectional RNN self.encoder = rnn.LSTM(num_hiddens, num_layers=num_layers, bidirectional=True, input_size=embed_size) self.decoder = nn.Dense(2)
def forward(self, inputs): # The shape of `inputs` is (batch size, no. of time steps). Because # LSTM requires its input's first dimension to be the temporal # dimension, the input is transposed before obtaining token # representations. The output shape is (no. of time steps, batch size, # word vector dimension) embeddings = self.embedding(inputs.T) # Returns hidden states of the last hidden layer at different time # steps. The shape of `outputs` is (no. of time steps, batch size, # 2 * no. of hidden units) outputs = self.encoder(embeddings) # Concatenate the hidden states at the initial and final time steps as # the input of the fully-connected layer. Its shape is (batch size, # 4 * no. of hidden units) encoding = np.concatenate((outputs[0], outputs[-1]), axis=1) outs = self.decoder(encoding) return outs
Let us construct a bidirectional RNN with two hidden layers to represent single text for sentiment analysis.
embed_size, num_hiddens, num_layers, devices = 100, 100, 2, d2l.try_all_gpus() net = BiRNN(len(vocab), embed_size, num_hiddens, num_layers)
net.initialize(init.Xavier(), ctx=devices)

15.2.2 Loading Pretrained Word Vectors
Below we load the pretrained 100-dimensional (needs to be consistent with embed_size) GloVe embeddings for tokens in the vocabulary. glove_embedding = d2l.TokenEmbedding('glove.6b.100d')
Print the shape of the vectors for all the tokens in the vocabulary. embeds = glove_embedding[vocab.idx_to_token] embeds.shape

(49346, 100)

We use these pretrained word vectors to represent tokens in the reviews and will not update these vectors during training.

15.2. Sentiment Analysis: Using Recurrent Neural Networks

725

net.embedding.weight.set_data(embeds) net.embedding.collect_params().setattr('grad_req', 'null')
15.2.3 Training and Evaluating the Model
Now we can train the bidirectional RNN for sentiment analysis. lr, num_epochs = 0.01, 5 trainer = gluon.Trainer(net.collect_params(), 'adam', {'learning_rate': lr}) loss = gluon.loss.SoftmaxCrossEntropyLoss() d2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs, devices)
loss 0.352, train acc 0.848, test acc 0.852 479.0 examples/sec on [gpu(0), gpu(1)]

We define the following function to predict the sentiment of a text sequence using the trained model net.
#@save def predict_sentiment(net, vocab, sequence):
"""Predict the sentiment of a text sequence.""" sequence = np.array(vocab[sequence.split()], ctx=d2l.try_gpu()) label = np.argmax(net(sequence.reshape(1, -1)), axis=1) return 'positive' if label == 1 else 'negative'
Finally, let us use the trained model to predict the sentiment for two simple sentences.
predict_sentiment(net, vocab, 'this movie is so great')
'positive'
predict_sentiment(net, vocab, 'this movie is so bad')

726

Chapter 15. Natural Language Processing: Applications

'negative'
Summary
• Pretrained word vectors can represent individual tokens in a text sequence. • Bidirectional RNNs can represent a text sequence, such as via the concatenation of its hidden
states at the initial and final time steps. This single text representation can be transformed into categories using a fully-connected layer.
Exercises
1. Increase the number of epochs. Can you improve the training and testing accuracies? How about tuning other hyperparameters?
2. Use larger pretrained word vectors, such as 300-dimensional GloVe embeddings. Does it improve classification accuracy?
3. Can we improve the classification accuracy by using the spaCy tokenization? You need to install spaCy (pip install spacy) and install the English package (python -m spacy download en). In the code, first, import spaCy (import spacy). Then, load the spaCy English package (spacy_en = spacy.load('en')). Finally, define the function def tokenizer(text): return [tok.text for tok in spacy_en.tokenizer(text)] and replace the original tokenizer function. Note the different forms of phrase tokens in GloVe and spaCy. For example, the phrase token “new york” takes the form of “new-york” in GloVe and the form of “new york” after the spaCy tokenization.
Discussions208
15.3 Sentiment Analysis: Using Convolutional Neural Networks
In Chapter 6, we investigated mechanisms for processing two-dimensional image data with twodimensional CNNs, which were applied to local features such as adjacent pixels. Though originally designed for computer vision, CNNs are also widely used for natural language processing. Simply put, just think of any text sequence as a one-dimensional image. In this way, one-dimensional CNNs can process local features such as n-grams in text. In this section, we will use the textCNN model to demonstrate how to design a CNN architecture for representing single text (Kim, 2014). Compared with Fig. 15.2.1 that uses an RNN architecture with GloVe pretraining for sentiment analysis, the only difference in Fig. 15.3.1 lies in the choice of the architecture.
208 https://discuss.d2l.ai/t/392

15.3. Sentiment Analysis: Using Convolutional Neural Networks

727

Fig. 15.3.1: This section feeds pretrained GloVe to a CNN-based architecture for sentiment analysis.
from mxnet import gluon, init, np, npx from mxnet.gluon import nn from d2l import mxnet as d2l
npx.set_np()
batch_size = 64 train_iter, test_iter, vocab = d2l.load_data_imdb(batch_size)
15.3.1 One-Dimensional Convolutions
Before introducing the model, let us see how a one-dimensional convolution works. Bear in mind that it is just a special case of a two-dimensional convolution based on the cross-correlation operation.

Fig. 15.3.2: One-dimensional cross-correlation operation. The shaded portions are the first output element as well as the input and kernel tensor elements used for the output computation: 0 × 1 + 1 × 2 = 2.

As shown in Fig. 15.3.2, in the one-dimensional case, the convolution window slides from left to right across the input tensor. During sliding, the input subtensor (e.g., 0 and 1 in Fig. 15.3.2) contained in the convolution window at a certain position and the kernel tensor (e.g., 1 and 2 in Fig. 15.3.2) are multiplied elementwise. The sum of these multiplications gives the single scalar value (e.g., 0 × 1 + 1 × 2 = 2 in Fig. 15.3.2) at the corresponding position of the output tensor.
We implement one-dimensional cross-correlation in the following corr1d function. Given an input tensor X and a kernel tensor K, it returns the output tensor Y.

728

Chapter 15. Natural Language Processing: Applications

def corr1d(X, K): w = K.shape[0] Y = np.zeros((X.shape[0] - w + 1)) for i in range(Y.shape[0]): Y[i] = (X[i:i + w] * K).sum() return Y
We can construct the input tensor X and the kernel tensor K from Fig. 15.3.2 to validate the output of the above one-dimensional cross-correlation implementation.
X, K = np.array([0, 1, 2, 3, 4, 5, 6]), np.array([1, 2]) corr1d(X, K)
array([ 2., 5., 8., 11., 14., 17.])
For any one-dimensional input with multiple channels, the convolution kernel needs to have the same number of input channels. Then for each channel, perform a cross-correlation operation on the one-dimensional tensor of the input and the one-dimensional tensor of the convolution kernel, summing the results over all the channels to produce the one-dimensional output tensor. Fig. 15.3.3 shows a one-dimensional cross-correlation operation with 3 input channels.

Fig. 15.3.3: One-dimensional cross-correlation operation with 3 input channels. The shaded portions are the first output element as well as the input and kernel tensor elements used for the output computation: 0 × 1 + 1 × 2 + 1 × 3 + 2 × 4 + 2 × (−1) + 3 × (−3) = 2.
We can implement the one-dimensional cross-correlation operation for multiple input channels and validate the results in Fig. 15.3.3.
def corr1d_multi_in(X, K): # First, iterate through the 0th dimension (channel dimension) of `X` and # `K`. Then, add them together return sum(corr1d(x, k) for x, k in zip(X, K))
X = np.array([[0, 1, 2, 3, 4, 5, 6], [1, 2, 3, 4, 5, 6, 7], [2, 3, 4, 5, 6, 7, 8]])
K = np.array([[1, 2], [3, 4], [-1, -3]]) corr1d_multi_in(X, K)
array([ 2., 8., 14., 20., 26., 32.])
Note that multi-input-channel one-dimensional cross-correlations are equivalent to single-inputchannel two-dimensional cross-correlations. To illustrate, an equivalent form of the multiinput-channel one-dimensional cross-correlation in Fig. 15.3.3 is the single-input-channel two-

15.3. Sentiment Analysis: Using Convolutional Neural Networks

729

dimensional cross-correlation in Fig. 15.3.4, where the height of the convolution kernel has to be the same as that of the input tensor.

Fig. 15.3.4: Two-dimensional cross-correlation operation with a single input channel. The shaded portions are the first output element as well as the input and kernel tensor elements used for the output computation: 2 × (−1) + 3 × (−3) + 1 × 3 + 2 × 4 + 0 × 1 + 1 × 2 = 2.
Both the outputs in Fig. 15.3.2 and Fig. 15.3.3 have only one channel. Same as two-dimensional convolutions with multiple output channels described in Section 6.4.2, we can also specify multiple output channels for one-dimensional convolutions.
15.3.2 Max-Over-Time Pooling
Similarly, we can use pooling to extract the highest value from sequence representations as the most important feature across time steps. The max-over-time pooling used in textCNN works like the one-dimensional global maximum pooling (Collobert et al., 2011). For a multi-channel input where each channel stores values at different time steps, the output at each channel is the maximum value for that channel. Note that the max-over-time pooling allows different numbers of time steps at different channels.
15.3.3 The textCNN Model
Using the one-dimensional convolution and max-over-time pooling, the textCNN model takes individual pretrained token representations as the input, then obtains and transforms sequence representations for the downstream application.
For a single text sequence with n tokens represented by d-dimensional vectors, the width, height, and number of channels of the input tensor are n, 1, and d, respectively. The textCNN model transforms the input into the output as follows:
1. Define multiple one-dimensional convolution kernels and perform convolution operations separately on the inputs. Convolution kernels with different widths may capture local features among different numbers of adjacent tokens.
2. Perform max-over-time pooling on all the output channels, and then concatenate all the scalar pooling outputs as a vector.
3. Transform the concatenated vector into the output categories using the fully-connected layer. Dropout can be used for reducing overfitting.

730

Chapter 15. Natural Language Processing: Applications

Fig. 15.3.5: The model architecture of textCNN.

Fig. 15.3.5 illustrates the model architecture of textCNN with a concrete example. The input is a sentence with 11 tokens, where each token is represented by a 6-dimensional vectors. So we have a 6-channel input with width 11. Define two one-dimensional convolution kernels of widths 2 and 4, with 4 and 5 output channels, respectively. They produce 4 output channels with width 11 − 2 + 1 = 10 and 5 output channels with width 11 − 4 + 1 = 8. Despite different widths of these 9 channels, the max-over-time pooling gives a concatenated 9-dimensional vector, which is finally transformed into a 2-dimensional output vector for binary sentiment predictions.

Defining the Model
We implement the textCNN model in the following class. Compared with the bidirectional RNN model in Section 15.2, besides replacing recurrent layers with convolutional layers, we also use two embedding layers: one with trainable weights and the other with fixed weights.
class TextCNN(nn.Block): def __init__(self, vocab_size, embed_size, kernel_sizes, num_channels, **kwargs): super(TextCNN, self).__init__(**kwargs) self.embedding = nn.Embedding(vocab_size, embed_size) # The embedding layer not to be trained self.constant_embedding = nn.Embedding(vocab_size, embed_size) self.dropout = nn.Dropout(0.5) self.decoder = nn.Dense(2) # The max-over-time pooling layer has no parameters, so this instance
(continues on next page)

15.3. Sentiment Analysis: Using Convolutional Neural Networks

731

# can be shared self.pool = nn.GlobalMaxPool1D() # Create multiple one-dimensional convolutional layers self.convs = nn.Sequential() for c, k in zip(num_channels, kernel_sizes):
self.convs.add(nn.Conv1D(c, k, activation='relu'))

(continued from previous page)

def forward(self, inputs): # Concatenate two embedding layer outputs with shape (batch size, no. # of tokens, token vector dimension) along vectors embeddings = np.concatenate( (self.embedding(inputs), self.constant_embedding(inputs)), axis=2) # Per the input format of one-dimensional convolutional layers, # rearrange the tensor so that the second dimension stores channels embeddings = embeddings.transpose(0, 2, 1) # For each one-dimensional convolutional layer, after max-over-time # pooling, a tensor of shape (batch size, no. of channels, 1) is # obtained. Remove the last dimension and concatenate along channels encoding = np.concatenate([ np.squeeze(self.pool(conv(embeddings)), axis=-1) for conv in self.convs], axis=1) outputs = self.decoder(self.dropout(encoding)) return outputs

Let us create a textCNN instance. It has 3 convolutional layers with kernel widths of 3, 4, and 5, all with 100 output channels.
embed_size, kernel_sizes, nums_channels = 100, [3, 4, 5], [100, 100, 100] devices = d2l.try_all_gpus() net = TextCNN(len(vocab), embed_size, kernel_sizes, nums_channels) net.initialize(init.Xavier(), ctx=devices)

Loading Pretrained Word Vectors
Same as Section 15.2, we load pretrained 100-dimensional GloVe embeddings as the initialized token representations. These token representations (embedding weights) will be trained in embedding and fixed in constant_embedding.
glove_embedding = d2l.TokenEmbedding('glove.6b.100d') embeds = glove_embedding[vocab.idx_to_token] net.embedding.weight.set_data(embeds) net.constant_embedding.weight.set_data(embeds) net.constant_embedding.collect_params().setattr('grad_req', 'null')

732

Chapter 15. Natural Language Processing: Applications

Training and Evaluating the Model
Now we can train the textCNN model for sentiment analysis.
lr, num_epochs = 0.001, 5 trainer = gluon.Trainer(net.collect_params(), 'adam', {'learning_rate': lr}) loss = gluon.loss.SoftmaxCrossEntropyLoss() d2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs, devices)
loss 0.090, train acc 0.970, test acc 0.866 3629.8 examples/sec on [gpu(0), gpu(1)]

Below we use the trained model to predict the sentiment for two simple sentences. d2l.predict_sentiment(net, vocab, 'this movie is so great')
'positive'
d2l.predict_sentiment(net, vocab, 'this movie is so bad')
'negative'
Summary
• One-dimensional CNNs can process local features such as n-grams in text. • Multi-input-channel one-dimensional cross-correlations are equivalent to single-input-
channel two-dimensional cross-correlations. • The max-over-time pooling allows different numbers of time steps at different channels. • The textCNN model transforms individual token representations into downstream applica-
tion outputs using one-dimensional convolutional layers and max-over-time pooling layers.

15.3. Sentiment Analysis: Using Convolutional Neural Networks

733

Exercises
1. Tune hyperparameters and compare the two architectures for sentiment analysis in Section 15.2 and in this section, such as in classification accuracy and computational efficiency.
2. Can you further improve the classification accuracy of the model by using the methods introduced in the exercises of Section 15.2?
3. Add positional encoding in the input representations. Does it improve the classification accuracy?
Discussions209
15.4 Natural Language Inference and the Dataset
In Section 15.1, we discussed the problem of sentiment analysis. This task aims to classify a single text sequence into predefined categories, such as a set of sentiment polarities. However, when there is a need to decide whether one sentence can be inferred form another, or eliminate redundancy by identifying sentences that are semantically equivalent, knowing how to classify one text sequence is insufficient. Instead, we need to be able to reason over pairs of text sequences.
15.4.1 Natural Language Inference
Natural language inference studies whether a hypothesis can be inferred from a premise, where both are a text sequence. In other words, natural language inference determines the logical relationship between a pair of text sequences. Such relationships usually fall into three types:
• Entailment: the hypothesis can be inferred from the premise. • Contradiction: the negation of the hypothesis can be inferred from the premise. • Neutral: all the other cases. Natural language inference is also known as the recognizing textual entailment task. For example, the following pair will be labeled as entailment because “showing affection” in the hypothesis can be inferred from “hugging one another” in the premise.
Premise: Two women are hugging each other. Hypothesis: Two women are showing affection. The following is an example of contradiction as “running the coding example” indicates “not sleeping” rather than “sleeping”. Premise: A man is running the coding example from Dive into Deep Learning. Hypothesis: The man is sleeping. The third example shows a neutrality relationship because neither “famous” nor “not famous” can be inferred from the fact that “are performing for us”. Premise: The musicians are performing for us. Hypothesis: The musicians are famous.
209 https://discuss.d2l.ai/t/393

734

Chapter 15. Natural Language Processing: Applications

Natural language inference has been a central topic for understanding natural language. It enjoys wide applications ranging from information retrieval to open-domain question answering. To study this problem, we will begin by investigating a popular natural language inference benchmark dataset.
15.4.2 The Stanford Natural Language Inference (SNLI) Dataset
Stanford Natural Language Inference (SNLI) Corpus is a collection of over 500000 labeled English sentence pairs (Bowman et al., 2015). We download and store the extracted SNLI dataset in the path ../data/snli_1.0.
import os import re from mxnet import gluon, np, npx from d2l import mxnet as d2l
npx.set_np()
#@save d2l.DATA_HUB['SNLI'] = ('https://nlp.stanford.edu/projects/snli/snli_1.0.zip',
'9fcde07509c7e87ec61c640c1b2753d9041758e4')
data_dir = d2l.download_extract('SNLI')

Reading the Dataset
The original SNLI dataset contains much richer information than what we really need in our experiments. Thus, we define a function read_snli to only extract part of the dataset, then return lists of premises, hypotheses, and their labels.
#@save def read_snli(data_dir, is_train):
"""Read the SNLI dataset into premises, hypotheses, and labels.""" def extract_text(s):
# Remove information that will not be used by us s = re.sub('\\(', '', s) s = re.sub('\\)', '', s) # Substitute two or more consecutive whitespace with space s = re.sub('\\s{2,}', ' ', s) return s.strip()
label_set = {'entailment': 0, 'contradiction': 1, 'neutral': 2} file_name = os.path.join(
data_dir, 'snli_1.0_train.txt' if is_train else 'snli_1.0_test.txt') with open(file_name, 'r') as f:
rows = [row.split('\t') for row in f.readlines()[1:]] premises = [extract_text(row[1]) for row in rows if row[0] in label_set] hypotheses = [extract_text(row[2]) for row in rows if row[0] in label_set] labels = [label_set[row[0]] for row in rows if row[0] in label_set] return premises, hypotheses, labels

Now let us print the first 3 pairs of premise and hypothesis, as well as their labels (“0”, “1”, and “2” correspond to “entailment”, “contradiction”, and “neutral”, respectively ).

15.4. Natural Language Inference and the Dataset

735

train_data = read_snli(data_dir, is_train=True) for x0, x1, y in zip(train_data[0][:3], train_data[1][:3], train_data[2][:3]):
print('premise:', x0) print('hypothesis:', x1) print('label:', y)
premise: A person on a horse jumps over a broken down airplane . hypothesis: A person is training his horse for a competition . label: 2 premise: A person on a horse jumps over a broken down airplane . hypothesis: A person is at a diner , ordering an omelette . label: 1 premise: A person on a horse jumps over a broken down airplane . hypothesis: A person is outdoors , on a horse . label: 0
The training set has about 550000 pairs, and the testing set has about 10000 pairs. The following shows that the three labels “entailment”, “contradiction”, and “neutral” are balanced in both the training set and the testing set.
test_data = read_snli(data_dir, is_train=False) for data in [train_data, test_data]:
print([[row for row in data[2]].count(i) for i in range(3)])
[183416, 183187, 182764] [3368, 3237, 3219]

Defining a Class for Loading the Dataset
Below we define a class for loading the SNLI dataset by inheriting from the Dataset class in Gluon. The argument num_steps in the class constructor specifies the length of a text sequence so that each minibatch of sequences will have the same shape. In other words, tokens after the first num_steps ones in longer sequence are trimmed, while special tokens “<pad>” will be appended to shorter sequences until their length becomes num_steps. By implementing the __getitem__ function, we can arbitrarily access the premise, hypothesis, and label with the index idx.
#@save class SNLIDataset(gluon.data.Dataset):
"""A customized dataset to load the SNLI dataset.""" def __init__(self, dataset, num_steps, vocab=None):
self.num_steps = num_steps all_premise_tokens = d2l.tokenize(dataset[0]) all_hypothesis_tokens = d2l.tokenize(dataset[1]) if vocab is None:
self.vocab = d2l.Vocab(all_premise_tokens + all_hypothesis_tokens, min_freq=5, reserved_tokens=['<pad>'])
else: self.vocab = vocab
self.premises = self._pad(all_premise_tokens) self.hypotheses = self._pad(all_hypothesis_tokens) self.labels = np.array(dataset[2])
(continues on next page)

736

Chapter 15. Natural Language Processing: Applications

print('read ' + str(len(self.premises)) + ' examples')

(continued from previous page)

def _pad(self, lines): return np.array([ d2l.truncate_pad(self.vocab[line], self.num_steps, self.vocab['<pad>']) for line in lines])

def __getitem__(self, idx): return (self.premises[idx], self.hypotheses[idx]), self.labels[idx]

def __len__(self): return len(self.premises)

Putting All Things Together
Now we can invoke the read_snli function and the SNLIDataset class to download the SNLI dataset and return DataLoader instances for both training and testing sets, together with the vocabulary of the training set. It is noteworthy that we must use the vocabulary constructed from the training set as that of the testing set. As a result, any new token from the testing set will be unknown to the model trained on the training set.
#@save def load_data_snli(batch_size, num_steps=50):
"""Download the SNLI dataset and return data iterators and vocabulary.""" num_workers = d2l.get_dataloader_workers() data_dir = d2l.download_extract('SNLI') train_data = read_snli(data_dir, True) test_data = read_snli(data_dir, False) train_set = SNLIDataset(train_data, num_steps) test_set = SNLIDataset(test_data, num_steps, train_set.vocab) train_iter = gluon.data.DataLoader(train_set, batch_size, shuffle=True,
num_workers=num_workers) test_iter = gluon.data.DataLoader(test_set, batch_size, shuffle=False,
num_workers=num_workers) return train_iter, test_iter, train_set.vocab
Here we set the batch size to 128 and sequence length to 50, and invoke the load_data_snli function to get the data iterators and vocabulary. Then we print the vocabulary size.
train_iter, test_iter, vocab = load_data_snli(128, 50) len(vocab)
read 549367 examples read 9824 examples
18678
Now we print the shape of the first minibatch. Contrary to sentiment analysis, we have two inputs X[0] and X[1] representing pairs of premises and hypotheses.

15.4. Natural Language Inference and the Dataset

737

for X, Y in train_iter: print(X[0].shape) print(X[1].shape) print(Y.shape) break
(128, 50) (128, 50) (128,)
Summary
• Natural language inference studies whether a hypothesis can be inferred from a premise, where both are a text sequence.
• In natural language inference, relationships between premises and hypotheses include entailment, contradiction, and neutral.
• Stanford Natural Language Inference (SNLI) Corpus is a popular benchmark dataset of natural language inference.
Exercises
1. Machine translation has long been evaluated based on superficial n-gram matching between an output translation and a ground-truth translation. Can you design a measure for evaluating machine translation results by using natural language inference?
2. How can we change hyperparameters to reduce the vocabulary size? Discussions210
15.5 Natural Language Inference: Using Attention
We introduced the natural language inference task and the SNLI dataset in Section 15.4. In view of many models that are based on complex and deep architectures, Parikh et al. proposed to address natural language inference with attention mechanisms and called it a “decomposable attention model” (Parikh et al., 2016). This results in a model without recurrent or convolutional layers, achieving the best result at the time on the SNLI dataset with much fewer parameters. In this section, we will describe and implement this attention-based method (with MLPs) for natural language inference, as depicted in Fig. 15.5.1.
210 https://discuss.d2l.ai/t/394

738

Chapter 15. Natural Language Processing: Applications

Fig. 15.5.1: This section feeds pretrained GloVe to an architecture based on attention and MLPs for natural language inference.
15.5.1 The Model
Simpler than preserving the order of tokens in premises and hypotheses, we can just align tokens in one text sequence to every token in the other, and vice versa, then compare and aggregate such information to predict the logical relationships between premises and hypotheses. Similar to alignment of tokens between source and target sentences in machine translation, the alignment of tokens between premises and hypotheses can be neatly accomplished by attention mechanisms.

Fig. 15.5.2: Natural language inference using attention mechanisms.
Fig. 15.5.2 depicts the natural language inference method using attention mechanisms. At a high level, it consists of three jointly trained steps: attending, comparing, and aggregating. We will illustrate them step by step in the following.

15.5. Natural Language Inference: Using Attention

739

from mxnet import gluon, init, np, npx from mxnet.gluon import nn from d2l import mxnet as d2l
npx.set_np()

Attending

The first step is to align tokens in one text sequence to each token in the other sequence. Suppose that the premise is “i do need sleep” and the hypothesis is “i am tired”. Due to semantical similarity, we may wish to align “i” in the hypothesis with “i” in the premise, and align “tired” in the hypothesis with “sleep” in the premise. Likewise, we may wish to align “i” in the premise with “i” in the hypothesis, and align “need” and “sleep” in the premise with “tired” in the hypothesis. Note that such alignment is soft using weighted average, where ideally large weights are associated with the tokens to be aligned. For ease of demonstration, Fig. 15.5.2 shows such alignment in a hard way.

Now we describe the soft alignment using attention mechanisms in more detail. Denote by A =
(a1, . . . , am) and B = (b1, . . . , bn) the premise and hypothesis, whose number of tokens are m and n, respectively, where ai, bj ∈ Rd (i = 1, . . . , m, j = 1, . . . , n) is a d-dimensional word vector. For soft alignment, we compute the attention weights eij ∈ R as

eij = f (ai)⊤f (bj),

(15.5.1)

where the function f is an MLP defined in the following mlp function. The output dimension of f is specified by the num_hiddens argument of mlp.

def mlp(num_hiddens, flatten): net = nn.Sequential() net.add(nn.Dropout(0.2)) net.add(nn.Dense(num_hiddens, activation='relu', flatten=flatten)) net.add(nn.Dropout(0.2)) net.add(nn.Dense(num_hiddens, activation='relu', flatten=flatten)) return net

It should be highlighted that, in (15.5.1) f takes inputs ai and bj separately rather than takes a pair of them together as the input. This decomposition trick leads to only m + n applications (linear complexity) of f rather than mn applications (quadratic complexity).

Normalizing the attention weights in (15.5.1), we compute the weighted average of all the token vectors in the hypothesis to obtain representation of the hypothesis that is softly aligned with the token indexed by i in the premise:

∑n exp(eij) βi = j=1 ∑nk=1 exp(eik) bj .

(15.5.2)

Likewise, we compute soft alignment of premise tokens for each token indexed by j in the hypoth-

esis:

∑ m exp(eij) αj = i=1 ∑mk=1 exp(ekj ) ai.

(15.5.3)

Below we define the Attend class to compute the soft alignment of hypotheses (beta) with input premises A and soft alignment of premises (alpha) with input hypotheses B.

740

Chapter 15. Natural Language Processing: Applications

class Attend(nn.Block): def __init__(self, num_hiddens, **kwargs): super(Attend, self).__init__(**kwargs) self.f = mlp(num_hiddens=num_hiddens, flatten=False)
def forward(self, A, B): # Shape of `A`/`B`: (b`atch_size`, no. of tokens in sequence A/B, # `embed_size`) # Shape of `f_A`/`f_B`: (`batch_size`, no. of tokens in sequence A/B, # `num_hiddens`) f_A = self.f(A) f_B = self.f(B) # Shape of `e`: (`batch_size`, no. of tokens in sequence A, # no. of tokens in sequence B) e = npx.batch_dot(f_A, f_B, transpose_b=True) # Shape of `beta`: (`batch_size`, no. of tokens in sequence A, # `embed_size`), where sequence B is softly aligned with each token # (axis 1 of `beta`) in sequence A beta = npx.batch_dot(npx.softmax(e), B) # Shape of `alpha`: (`batch_size`, no. of tokens in sequence B, # `embed_size`), where sequence A is softly aligned with each token # (axis 1 of `alpha`) in sequence B alpha = npx.batch_dot(npx.softmax(e.transpose(0, 2, 1)), A) return beta, alpha

Comparing

In the next step, we compare a token in one sequence with the other sequence that is softly aligned with that token. Note that in soft alignment, all the tokens from one sequence, though with probably different attention weights, will be compared with a token in the other sequence. For easy of demonstration, Fig. 15.5.2 pairs tokens with aligned tokens in a hard way. For example, suppose that the attending step determines that “need” and “sleep” in the premise are both aligned with “tired” in the hypothesis, the pair “tired–need sleep” will be compared.

In the comparing step, we feed the concatenation (operator [·, ·]) of tokens from one sequence and aligned tokens from the other sequence into a function g (an MLP):

vA,i = g([ai, βi]), i = 1, . . . , m vB,j = g([bj, αj]), j = 1, . . . , n.

(15.5.4)

In (15.5.4), vA,i is the comparison between token i in the premise and all the hypothesis tokens that are softly aligned with token i; while vB,j is the comparison between token j in the hypothesis and all the premise tokens that are softly aligned with token j. The following Compare class defines such as comparing step.

class Compare(nn.Block): def __init__(self, num_hiddens, **kwargs): super(Compare, self).__init__(**kwargs) self.g = mlp(num_hiddens=num_hiddens, flatten=False)

def forward(self, A, B, beta, alpha): V_A = self.g(np.concatenate([A, beta], axis=2)) V_B = self.g(np.concatenate([B, alpha], axis=2)) return V_A, V_B

15.5. Natural Language Inference: Using Attention

741

Aggregating

With two sets of comparison vectors vA,i (i = 1, . . . , m) and vB,j (j = 1, . . . , n) on hand, in the last step we will aggregate such information to infer the logical relationship. We begin by summing up both sets:

∑ m vA = vA,i,
i=1

∑n vB = vB,j .
j=1

(15.5.5)

Next we feed the concatenation of both summarization results into function h (an MLP) to obtain the classification result of the logical relationship:

yˆ = h([vA, vB]).
The aggregation step is defined in the following Aggregate class.
class Aggregate(nn.Block): def __init__(self, num_hiddens, num_outputs, **kwargs): super(Aggregate, self).__init__(**kwargs) self.h = mlp(num_hiddens=num_hiddens, flatten=True) self.h.add(nn.Dense(num_outputs))
def forward(self, V_A, V_B): # Sum up both sets of comparison vectors V_A = V_A.sum(axis=1) V_B = V_B.sum(axis=1) # Feed the concatenation of both summarization results into an MLP Y_hat = self.h(np.concatenate([V_A, V_B], axis=1)) return Y_hat

(15.5.6)

Putting All Things Together
By putting the attending, comparing, and aggregating steps together, we define the decomposable attention model to jointly train these three steps.
class DecomposableAttention(nn.Block): def __init__(self, vocab, embed_size, num_hiddens, **kwargs): super(DecomposableAttention, self).__init__(**kwargs) self.embedding = nn.Embedding(len(vocab), embed_size) self.attend = Attend(num_hiddens) self.compare = Compare(num_hiddens) # There are 3 possible outputs: entailment, contradiction, and neutral self.aggregate = Aggregate(num_hiddens, 3)
def forward(self, X): premises, hypotheses = X A = self.embedding(premises) B = self.embedding(hypotheses) beta, alpha = self.attend(A, B) V_A, V_B = self.compare(A, B, beta, alpha) Y_hat = self.aggregate(V_A, V_B) return Y_hat

742

Chapter 15. Natural Language Processing: Applications

15.5.2 Training and Evaluating the Model
Now we will train and evaluate the defined decomposable attention model on the SNLI dataset. We begin by reading the dataset.
Reading the dataset
We download and read the SNLI dataset using the function defined in Section 15.4. The batch size and sequence length are set to 256 and 50, respectively.
batch_size, num_steps = 256, 50 train_iter, test_iter, vocab = d2l.load_data_snli(batch_size, num_steps)
read 549367 examples read 9824 examples
Creating the Model
We use the pretrained 100-dimensional GloVe embedding to represent the input tokens. Thus, we predefine the dimension of vectors ai and bj in (15.5.1) as 100. The output dimension of functions f in (15.5.1) and g in (15.5.4) is set to 200. Then we create a model instance, initialize its parameters, and load the GloVe embedding to initialize vectors of input tokens.
embed_size, num_hiddens, devices = 100, 200, d2l.try_all_gpus() net = DecomposableAttention(vocab, embed_size, num_hiddens) net.initialize(init.Xavier(), ctx=devices) glove_embedding = d2l.TokenEmbedding('glove.6b.100d') embeds = glove_embedding[vocab.idx_to_token] net.embedding.weight.set_data(embeds)
Training and Evaluating the Model
In contrast to the split_batch function in Section 12.5 that takes single inputs such as text sequences (or images), we define a split_batch_multi_inputs function to take multiple inputs such as premises and hypotheses in minibatches.
#@save def split_batch_multi_inputs(X, y, devices):
"""Split multi-input `X` and `y` into multiple devices.""" X = list(
zip(*[ gluon.utils.split_and_load(feature, devices, even_split=False) for feature in X]))
return (X, gluon.utils.split_and_load(y, devices, even_split=False))
Now we can train and evaluate the model on the SNLI dataset.

15.5. Natural Language Inference: Using Attention

743

lr, num_epochs = 0.001, 4 trainer = gluon.Trainer(net.collect_params(), 'adam', {'learning_rate': lr}) loss = gluon.loss.SoftmaxCrossEntropyLoss() d2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs, devices,
split_batch_multi_inputs)
loss 0.518, train acc 0.795, test acc 0.819 9992.1 examples/sec on [gpu(0), gpu(1)]

Using the Model
Finally, define the prediction function to output the logical relationship between a pair of premise and hypothesis.
#@save def predict_snli(net, vocab, premise, hypothesis):
"""Predict the logical relationship between the premise and hypothesis.""" premise = np.array(vocab[premise], ctx=d2l.try_gpu()) hypothesis = np.array(vocab[hypothesis], ctx=d2l.try_gpu()) label = np.argmax(
net([premise.reshape((1, -1)), hypothesis.reshape((1, -1))]), axis=1)
return 'entailment' if label == 0 else 'contradiction' if label == 1 \ else 'neutral'
We can use the trained model to obtain the natural language inference result for a sample pair of sentences.
predict_snli(net, vocab, ['he', 'is', 'good', '.'], ['he', 'is', 'bad', '.'])
'contradiction'

744

Chapter 15. Natural Language Processing: Applications

Summary
• The decomposable attention model consists of three steps for predicting the logical relationships between premises and hypotheses: attending, comparing, and aggregating.
• With attention mechanisms, we can align tokens in one text sequence to every token in the other, and vice versa. Such alignment is soft using weighted average, where ideally large weights are associated with the tokens to be aligned.
• The decomposition trick leads to a more desirable linear complexity than quadratic complexity when computing attention weights.
• We can use pretrained word vectors as the input representation for downstream natural language processing task such as natural language inference.
Exercises
1. Train the model with other combinations of hyperparameters. Can you get better accuracy on the test set?
2. What are major drawbacks of the decomposable attention model for natural language inference?
3. Suppose that we want to get the level of semantical similarity (e.g., a continuous value between 0 and 1) for any pair of sentences. How shall we collect and label the dataset? Can you design a model with attention mechanisms?
Discussions211

15.6 Fine-Tuning BERT for Sequence-Level and Token-Level Applications

In the previous sections of this chapter, we have designed different models for natural language processing applications, such as based on RNNs, CNNs, attention, and MLPs. These models are helpful when there is space or time constraint, however, crafting a specific model for every natural language processing task is practically infeasible. In Section 14.8, we introduced a pretraining model, BERT, that requires minimal architecture changes for a wide range of natural language processing tasks. One one hand, at the time of its proposal, BERT improved the state of the art on various natural language processing tasks. On the other hand, as noted in Section 14.10, the two versions of the original BERT model come with 110 million and 340 million parameters. Thus, when there are sufficient computational resources, we may consider fine-tuning BERT for downstream natural language processing applications.
In the following, we generalize a subset of natural language processing applications as sequencelevel and token-level. On the sequence level, we introduce how to transform the BERT representation of the text input to the output label in single text classification and text pair classification or regression. On the token level, we will briefly introduce new applications such as text tagging and question answering and shed light on how BERT can represent their inputs and get transformed into output labels. During fine-tuning, the “minimal architecture changes” required by BERT across different applications are the extra fully-connected layers. During supervised learning of a downstream application, parameters of the extra layers are learned from scratch while all the parameters in the pretrained BERT model are fine-tuned.
211 https://discuss.d2l.ai/t/395

15.6. Fine-Tuning BERT for Sequence-Level and Token-Level Applications

745

15.6.1 Single Text Classification
Single text classification takes a single text sequence as the input and outputs its classification result. Besides sentiment analysis that we have studied in this chapter, the Corpus of Linguistic Acceptability (CoLA) is also a dataset for single text classification, judging whether a given sentence is grammatically acceptable or not (Warstadt et al., 2019). For instance, “I should study.” is acceptable but “I should studying.” is not.

Fig. 15.6.1: Fine-tuning BERT for single text classification applications, such as sentiment analysis and testing linguistic acceptability. Suppose that the input single text has six tokens.
Section 14.8 describes the input representation of BERT. The BERT input sequence unambiguously represents both single text and text pairs, where the special classification token “<cls>” is used for sequence classification and the special classification token “<sep>” marks the end of single text or separates a pair of text. As shown in Fig. 15.6.1, in single text classification applications, the BERT representation of the special classification token “<cls>” encodes the information of the entire input text sequence. As the representation of the input single text, it will be fed into a small MLP consisting of fully-connected (dense) layers to output the distribution of all the discrete label values.
15.6.2 Text Pair Classification or Regression
We have also examined natural language inference in this chapter. It belongs to text pair classification, a type of application classifying a pair of text.
Taking a pair of text as the input but outputting a continuous value, semantic textual similarity is a popular text pair regression task. This task measures semantic similarity of sentences. For instance, in the Semantic Textual Similarity Benchmark dataset, the similarity score of a pair of sentences is an ordinal scale ranging from 0 (no meaning overlap) to 5 (meaning equivalence) (Cer et al., 2017). The goal is to predict these scores. Examples from the Semantic Textual Similarity Benchmark dataset include (sentence 1, sentence 2, similarity score):
• “A plane is taking off.”, “An air plane is taking off.”, 5.000;
• “A woman is eating something.”, “A woman is eating meat.”, 3.000;

746

Chapter 15. Natural Language Processing: Applications

• “A woman is dancing.”, “A man is talking.”, 0.000.

Fig. 15.6.2: Fine-tuning BERT for text pair classification or regression applications, such as natural language inference and semantic textual similarity. Suppose that the input text pair has two and three tokens.
Comparing with single text classification in Fig. 15.6.1, fine-tuning BERT for text pair classification in Fig. 15.6.2 is different in the input representation. For text pair regression tasks such as semantic textual similarity, trivial changes can be applied such as outputting a continuous label value and using the mean squared loss: they are common for regression.
15.6.3 Text Tagging
Now let us consider token-level tasks, such as text tagging, where each token is assigned a label. Among text tagging tasks, part-of-speech tagging assigns each word a part-of-speech tag (e.g., adjective and determiner) according to the role of the word in the sentence. For example, according to the Penn Treebank II tag set, the sentence “John Smith ʼs car is new” should be tagged as “NNP (noun, proper singular) NNP POS (possessive ending) NN (noun, singular or mass) VB (verb, base form) JJ (adjective)”.

15.6. Fine-Tuning BERT for Sequence-Level and Token-Level Applications

747

Fig. 15.6.3: Fine-tuning BERT for text tagging applications, such as part-of-speech tagging. Suppose that the input single text has six tokens.
Fine-tuning BERT for text tagging applications is illustrated in Fig. 15.6.3. Comparing with Fig. 15.6.1, the only distinction lies in that in text tagging, the BERT representation of every token of the input text is fed into the same extra fully-connected layers to output the label of the token, such as a part-of-speech tag.
15.6.4 Question Answering
As another token-level application, question answering reflects capabilities of reading comprehension. For example, the Stanford Question Answering Dataset (SQuAD v1.1) consists of reading passages and questions, where the answer to every question is just a segment of text (text span) from the passage that the question is about (Rajpurkar et al., 2016). To explain, consider a passage “Some experts report that a maskʼs efficacy is inconclusive. However, mask makers insist that their products, such as N95 respirator masks, can guard against the virus.” and a question “Who say that N95 respirator masks can guard against the virus?”. The answer should be the text span “mask makers” in the passage. Thus, the goal in SQuAD v1.1 is to predict the start and end of the text span in the passage given a pair of question and passage.

748

Chapter 15. Natural Language Processing: Applications

Fig. 15.6.4: Fine-tuning BERT for question answering. Suppose that the input text pair has two and three tokens.
To fine-tune BERT for question answering, the question and passage are packed as the first and second text sequence, respectively, in the input of BERT. To predict the position of the start of the text span, the same additional fully-connected layer will transform the BERT representation of any token from the passage of position i into a scalar score si. Such scores of all the passage tokens are further transformed by the softmax operation into a probability distribution, so that each token position i in the passage is assigned a probability pi of being the start of the text span. Predicting the end of the text span is the same as above, except that parameters in its additional fully-connected layer are independent from those for predicting the start. When predicting the end, any passage token of position i is transformed by the same fully-connected layer into a scalar score ei. Fig. 15.6.4 depicts fine-tuning BERT for question answering.
For question answering, the supervised learningʼs training objective is as straightforward as maximizing the log-likelihoods of the ground-truth start and end positions. When predicting the span, we can compute the score si + ej for a valid span from position i to position j (i ≤ j), and output the span with the highest score.
Summary
• BERT requires minimal architecture changes (extra fully-connected layers) for sequencelevel and token-level natural language processing applications, such as single text classification (e.g., sentiment analysis and testing linguistic acceptability), text pair classification or regression (e.g., natural language inference and semantic textual similarity), text tagging (e.g., part-of-speech tagging), and question answering.
• During supervised learning of a downstream application, parameters of the extra layers are learned from scratch while all the parameters in the pretrained BERT model are fine-tuned.

15.6. Fine-Tuning BERT for Sequence-Level and Token-Level Applications

749

Exercises
1. Let us design a search engine algorithm for news articles. When the system receives an query (e.g., “oil industry during the coronavirus outbreak”), it should return a ranked list of news articles that are most relevant to the query. Suppose that we have a huge pool of news articles and a large number of queries. To simplify the problem, suppose that the most relevant article has been labeled for each query. How can we apply negative sampling (see Section 14.2.1) and BERT in the algorithm design?
2. How can we leverage BERT in training language models? 3. Can we leverage BERT in machine translation? Discussions212
15.7 Natural Language Inference: Fine-Tuning BERT
In earlier sections of this chapter, we have designed an attention-based architecture (in Section 15.5) for the natural language inference task on the SNLI dataset (as described in Section 15.4). Now we revisit this task by fine-tuning BERT. As discussed in Section 15.6, natural language inference is a sequence-level text pair classification problem, and fine-tuning BERT only requires an additional MLP-based architecture, as illustrated in Fig. 15.7.1.

Fig. 15.7.1: This section feeds pretrained BERT to an MLP-based architecture for natural language inference.

In this section, we will download a pretrained small version of BERT, then fine-tune it for natural language inference on the SNLI dataset.

import json import multiprocessing import os from mxnet import gluon, np, npx from mxnet.gluon import nn
212 https://discuss.d2l.ai/t/396

(continues on next page)

750

Chapter 15. Natural Language Processing: Applications

from d2l import mxnet as d2l npx.set_np()

(continued from previous page)

15.7.1 Loading Pretrained BERT
We have explained how to pretrain BERT on the WikiText-2 dataset in Section 14.9 and Section 14.10 (note that the original BERT model is pretrained on much bigger corpora). As discussed in Section 14.10, the original BERT model has hundreds of millions of parameters. In the following, we provide two versions of pretrained BERT: “bert.base” is about as big as the original BERT base model that requires a lot of computational resources to fine-tune, while “bert.small” is a small version to facilitate demonstration.
d2l.DATA_HUB['bert.base'] = (d2l.DATA_URL + 'bert.base.zip', '7b3820b35da691042e5d34c0971ac3edbd80d3f4')
d2l.DATA_HUB['bert.small'] = (d2l.DATA_URL + 'bert.small.zip', 'a4e718a47137ccd1809c9107ab4f5edd317bae2c')
Either pretrained BERT model contains a “vocab.json” file that defines the vocabulary set and a “pretrained.params” file of the pretrained parameters. We implement the following load_pretrained_model function to load pretrained BERT parameters.
def load_pretrained_model(pretrained_model, num_hiddens, ffn_num_hiddens, num_heads, num_layers, dropout, max_len, devices):
data_dir = d2l.download_extract(pretrained_model) # Define an empty vocabulary to load the predefined vocabulary vocab = d2l.Vocab() vocab.idx_to_token = json.load(open(os.path.join(data_dir, 'vocab.json'))) vocab.token_to_idx = {
token: idx for idx, token in enumerate(vocab.idx_to_token)} bert = d2l.BERTModel(len(vocab), num_hiddens, ffn_num_hiddens, num_heads,
num_layers, dropout, max_len) # Load pretrained BERT parameters bert.load_parameters(os.path.join(data_dir, 'pretrained.params'),
ctx=devices) return bert, vocab
To facilitate demonstration on most of machines, we will load and fine-tune the small version (“bert.small”) of the pretrained BERT in this section. In the exercise, we will show how to finetune the much larger “bert.base” to significantly improve the testing accuracy.
devices = d2l.try_all_gpus() bert, vocab = load_pretrained_model('bert.small', num_hiddens=256,
ffn_num_hiddens=512, num_heads=4, num_layers=2, dropout=0.1, max_len=512, devices=devices)

15.7. Natural Language Inference: Fine-Tuning BERT

751

15.7.2 The Dataset for Fine-Tuning BERT

For the downstream task natural language inference on the SNLI dataset, we define a customized dataset class SNLIBERTDataset. In each example, the premise and hypothesis form a pair of text sequence and is packed into one BERT input sequence as depicted in Fig. 15.6.2. Recall Section 14.8.4 that segment IDs are used to distinguish the premise and the hypothesis in a BERT input sequence. With the predefined maximum length of a BERT input sequence (max_len), the last token of the longer of the input text pair keeps getting removed until max_len is met. To accelerate generation of the SNLI dataset for fine-tuning BERT, we use 4 worker processes to generate training or testing examples in parallel.

class SNLIBERTDataset(gluon.data.Dataset): def __init__(self, dataset, max_len, vocab=None): all_premise_hypothesis_tokens = [[ p_tokens, h_tokens] for p_tokens, h_tokens in zip(*[ d2l.tokenize([s.lower() for s in sentences]) for sentences in dataset[:2]])]

self.labels = np.array(dataset[2]) self.vocab = vocab self.max_len = max_len (self.all_token_ids, self.all_segments, self.valid_lens) = self._preprocess(all_premise_hypothesis_tokens) print('read ' + str(len(self.all_token_ids)) + ' examples')

def _preprocess(self, all_premise_hypothesis_tokens): pool = multiprocessing.Pool(4) # Use 4 worker processes out = pool.map(self._mp_worker, all_premise_hypothesis_tokens) all_token_ids = [token_ids for token_ids, segments, valid_len in out] all_segments = [segments for token_ids, segments, valid_len in out] valid_lens = [valid_len for token_ids, segments, valid_len in out] return (np.array(all_token_ids, dtype='int32'), np.array(all_segments, dtype='int32'), np.array(valid_lens))

def _mp_worker(self, premise_hypothesis_tokens): p_tokens, h_tokens = premise_hypothesis_tokens self._truncate_pair_of_tokens(p_tokens, h_tokens) tokens, segments = d2l.get_tokens_and_segments(p_tokens, h_tokens) token_ids = self.vocab[tokens] + [self.vocab['<pad>']] \ * (self.max_len - len(tokens)) segments = segments + [0] * (self.max_len - len(segments)) valid_len = len(tokens) return token_ids, segments, valid_len

def _truncate_pair_of_tokens(self, p_tokens, h_tokens): # Reserve slots for '<CLS>', '<SEP>', and '<SEP>' tokens for the BERT # input while len(p_tokens) + len(h_tokens) > self.max_len - 3: if len(p_tokens) > len(h_tokens): p_tokens.pop() else: h_tokens.pop()

def __getitem__(self, idx): return (self.all_token_ids[idx], self.all_segments[idx],

(continues on next page)

752

Chapter 15. Natural Language Processing: Applications

self.valid_lens[idx]), self.labels[idx]
def __len__(self): return len(self.all_token_ids)

(continued from previous page)

After downloading the SNLI dataset, we generate training and testing examples by instantiating the SNLIBERTDataset class. Such examples will be read in minibatches during training and testing of natural language inference.
# Reduce `batch_size` if there is an out of memory error. In the original BERT # model, `max_len` = 512 batch_size, max_len, num_workers = 512, 128, d2l.get_dataloader_workers() data_dir = d2l.download_extract('SNLI') train_set = SNLIBERTDataset(d2l.read_snli(data_dir, True), max_len, vocab) test_set = SNLIBERTDataset(d2l.read_snli(data_dir, False), max_len, vocab) train_iter = gluon.data.DataLoader(train_set, batch_size, shuffle=True,
num_workers=num_workers) test_iter = gluon.data.DataLoader(test_set, batch_size,
num_workers=num_workers)

read 549367 examples read 9824 examples

15.7.3 Fine-Tuning BERT
As Fig. 15.6.2 indicates, fine-tuning BERT for natural language inference requires only an extra MLP consisting of two fully-connected layers (see self.hidden and self.output in the following BERTClassifier class). This MLP transforms the BERT representation of the special “<cls>” token, which encodes the information of both the premise and the hypothesis, into three outputs of natural language inference: entailment, contradiction, and neutral.
class BERTClassifier(nn.Block): def __init__(self, bert): super(BERTClassifier, self).__init__() self.encoder = bert.encoder self.hidden = bert.hidden self.output = nn.Dense(3)
def forward(self, inputs): tokens_X, segments_X, valid_lens_x = inputs encoded_X = self.encoder(tokens_X, segments_X, valid_lens_x) return self.output(self.hidden(encoded_X[:, 0, :]))
In the following, the pretrained BERT model bert is fed into the BERTClassifier instance net for the downstream application. In common implementations of BERT fine-tuning, only the parameters of the output layer of the additional MLP (net.output) will be learned from scratch. All the parameters of the pretrained BERT encoder (net.encoder) and the hidden layer of the additional MLP (net.hidden) will be fine-tuned.

15.7. Natural Language Inference: Fine-Tuning BERT

753

net = BERTClassifier(bert) net.output.initialize(ctx=devices)
Recall that in Section 14.8 both the MaskLM class and the NextSentencePred class have parameters in their employed MLPs. These parameters are part of those in the pretrained BERT model bert, and thus part of parameters in net. However, such parameters are only for computing the masked language modeling loss and the next sentence prediction loss during pretraining. These two loss functions are irrelevant to fine-tuning downstream applications, thus the parameters of the employed MLPs in MaskLM and NextSentencePred are not updated (staled) when BERT is fine-tuned.
To allow parameters with stale gradients, the flag ignore_stale_grad=True is set in the step function of d2l.train_batch_ch13. We use this function to train and evaluate the model net using the training set (train_iter) and the testing set (test_iter) of SNLI. Due to the limited computational resources, the training and testing accuracy can be further improved: we leave its discussions in the exercises.
lr, num_epochs = 1e-4, 5 trainer = gluon.Trainer(net.collect_params(), 'adam', {'learning_rate': lr}) loss = gluon.loss.SoftmaxCrossEntropyLoss() d2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs, devices,
d2l.split_batch_multi_inputs)
loss 0.478, train acc 0.810, test acc 0.784 8149.9 examples/sec on [gpu(0), gpu(1)]

Summary
• We can fine-tune the pretrained BERT model for downstream applications, such as natural language inference on the SNLI dataset.
• During fine-tuning, the BERT model becomes part of the model for the downstream application. Parameters that are only related to pretraining loss will not be updated during finetuning.

754

Chapter 15. Natural Language Processing: Applications

Exercises
1. Fine-tune a much larger pretrained BERT model that is about as big as the original BERT base model if your computational resource allows. Set arguments in the load_pretrained_model function as: replacing ʻbert.smallʼ with ʻbert.baseʼ, increasing values of num_hiddens=256, ffn_num_hiddens=512, num_heads=4, and num_layers=2 to 768, 3072, 12, and 12, respectively. By increasing fine-tuning epochs (and possibly tuning other hyperparameters), can you get a testing accuracy higher than 0.86?
2. How to truncate a pair of sequences according to their ratio of length? Compare this pair truncation method and the one used in the SNLIBERTDataset class. What are their pros and cons?
Discussions213

213 https://discuss.d2l.ai/t/397

15.7. Natural Language Inference: Fine-Tuning BERT

755

756

Chapter 15. Natural Language Processing: Applications

16 | Appendix: Tools for Deep Learning
In this chapter, we will walk you through major tools for deep learning, from introducing Jupyter notebook in Section 16.1 to empowering you training models on Cloud such as Amazon SageMaker in Section 16.2, Amazon EC2 in Section 16.3 and Google Colab in Section 16.4. Besides, if you would like to purchase your own GPUs, we also note down some practical suggestions in Section 16.5. If you are interested in being a contributor of this book, you may follow the instructions in Section 16.6.
16.1 Using Jupyter
This section describes how to edit and run the code in the chapters of this book using Jupyter Notebooks. Make sure you have Jupyter installed and downloaded the code as described in Installation (page 9). If you want to know more about Jupyter see the excellent tutorial in their Documentation214.
16.1.1 Editing and Running the Code Locally
Suppose that the local path of code of the book is “xx/yy/d2l-en/”. Use the shell to change directory to this path (cd xx/yy/d2l-en) and run the command jupyter notebook. If your browser does not do this automatically, open http://localhost:8888 and you will see the interface of Jupyter and all the folders containing the code of the book, as shown in Fig. 16.1.1.
Fig. 16.1.1: The folders containing the code in this book.
214 https://jupyter.readthedocs.io/en/latest/
757

You can access the notebook files by clicking on the folder displayed on the webpage. They usually have the suffix “.ipynb”. For the sake of brevity, we create a temporary “test.ipynb” file. The content displayed after you click it is as shown in Fig. 16.1.2. This notebook includes a markdown cell and a code cell. The content in the markdown cell includes “This is A Title” and “This is text”. The code cell contains two lines of Python code.

Fig. 16.1.2: Markdown and code cells in the “text.ipynb” file.
Double click on the markdown cell to enter edit mode. Add a new text string “Hello world.” at the end of the cell, as shown in Fig. 16.1.3.

Fig. 16.1.3: Edit the markdown cell. As shown in Fig. 16.1.4, click “Cell” → “Run Cells” in the menu bar to run the edited cell.

758

Chapter 16. Appendix: Tools for Deep Learning

Fig. 16.1.4: Run the cell. After running, the markdown cell is as shown in Fig. 16.1.5.

Fig. 16.1.5: The markdown cell after editing.
Next, click on the code cell. Multiply the elements by 2 after the last line of code, as shown in Fig. 16.1.6.

16.1. Using Jupyter

759

Fig. 16.1.6: Edit the code cell.
You can also run the cell with a shortcut (“Ctrl + Enter” by default) and obtain the output result from Fig. 16.1.7.

Fig. 16.1.7: Run the code cell to obtain the output.
When a notebook contains more cells, we can click “Kernel” → “Restart & Run All” in the menu bar to run all the cells in the entire notebook. By clicking “Help” → “Edit Keyboard Shortcuts” in the menu bar, you can edit the shortcuts according to your preferences.

760

Chapter 16. Appendix: Tools for Deep Learning

16.1.2 Advanced Options
Beyond local editing there are two things that are quite important: editing the notebooks in markdown format and running Jupyter remotely. The latter matters when we want to run the code on a faster server. The former matters since Jupyterʼs native .ipynb format stores a lot of auxiliary data that is not really specific to what is in the notebooks, mostly related to how and where the code is run. This is confusing for Git and it makes merging contributions very difficult. Fortunately there is an alternative—native editing in Markdown.
Markdown Files in Jupyter
If you wish to contribute to the content of this book, you need to modify the source file (md file, not ipynb file) on GitHub. Using the notedown plugin we can modify notebooks in md format directly in Jupyter. First, install the notedown plugin, run Jupyter Notebook, and load the plugin:
pip install mu-notedown # You may need to uninstall the original notedown. jupyter notebook --NotebookApp.contents_manager_class='notedown.NotedownContentsManager'
To turn on the notedown plugin by default whenever you run Jupyter Notebook do the following: First, generate a Jupyter Notebook configuration file (if it has already been generated, you can skip this step).
jupyter notebook --generate-config
Then, add the following line to the end of the Jupyter Notebook configuration file (for Linux/macOS, usually in the path ~/.jupyter/jupyter_notebook_config.py):
c.NotebookApp.contents_manager_class = 'notedown.NotedownContentsManager'
After that, you only need to run the jupyter notebook command to turn on the notedown plugin by default.
Running Jupyter Notebook on a Remote Server
Sometimes, you may want to run Jupyter Notebook on a remote server and access it through a browser on your local computer. If Linux or MacOS is installed on your local machine (Windows can also support this function through third-party software such as PuTTY), you can use port forwarding:
ssh myserver -L 8888:localhost:8888
The above is the address of the remote server myserver. Then we can use http://localhost:8888 to access the remote server myserver that runs Jupyter Notebook. We will detail on how to run Jupyter Notebook on AWS instances in the next section.

16.1. Using Jupyter

761

Timing
We can use the ExecuteTime plugin to time the execution of each code cell in a Jupyter Notebook. Use the following commands to install the plugin:
pip install jupyter_contrib_nbextensions jupyter contrib nbextension install --user jupyter nbextension enable execute_time/ExecuteTime
Summary
• To edit the book chapters you need to activate markdown format in Jupyter. • You can run servers remotely using port forwarding.
Exercises
1. Try to edit and run the code in this book locally. 2. Try to edit and run the code in this book remotely via port forwarding. 3. Measure A⊤B vs. AB for two square matrices in R1024×1024. Which one is faster? Discussions215
16.2 Using Amazon SageMaker
Many deep learning applications require a significant amount of computation. Your local machine might be too slow to solve these problems in a reasonable amount of time. Cloud computing services give you access to more powerful computers to run the GPU-intensive portions of this book. This tutorial will guide you through Amazon SageMaker: a service that allows you to run this book easily.
16.2.1 Registering and Logging In
First, we need to register an account at https://aws.amazon.com/. We encourage you to use twofactor authentication for additional security. It is also a good idea to set up detailed billing and spending alerts to avoid any unexpected surprises in case you forget to stop any running instance. Note that you will need a credit card. After logging into your AWS account, go to your console216 and search for “SageMaker” (see Fig. 16.2.1) then click to open the SageMaker panel.
215 https://discuss.d2l.ai/t/421 216 http://console.aws.amazon.com/

762

Chapter 16. Appendix: Tools for Deep Learning

Fig. 16.2.1: Open the SageMaker panel.
16.2.2 Creating a SageMaker Instance
Next, let us create a notebook instance as described in Fig. 16.2.2.

Fig. 16.2.2: Create a SageMaker instance.
SageMaker provides multiple instance types217 of different computational power and prices. When creating an instance, we can specify the instance name and choose its type. In Fig. 16.2.3, we choose ml.p3.2xlarge. With one Tesla V100 GPU and an 8-core CPU, this instance is powerful enough for most chapters.

Fig. 16.2.3: Choose the instance type.
A Jupyter notebook version of this book for fitting SageMaker is available at https://github.com/ d2l-ai/d2l-en-sagemaker. We can specify this GitHub repository URL to let SageMaker clone this repository during instance creation, as shown in Fig. 16.2.4.
217 https://aws.amazon.com/sagemaker/pricing/instance-types/

16.2. Using Amazon SageMaker

763

Fig. 16.2.4: Specify the GitHub repository.
16.2.3 Running and Stopping an Instance
It may take a few minutes before the instance is ready. When it is ready, you can click on the “Open Jupyter” link as shown in Fig. 16.2.5.
Fig. 16.2.5: Open Jupyter on the created SageMaker instance. Then, as shown in Fig. 16.2.6, you may navigate through the Jupyter server running on this instance.

Fig. 16.2.6: The Jupyter server running on the SageMaker instance.
Running and editing Jupyter notebooks on the SageMaker instance is similar to what we have discussed in Section 16.1. After finishing your work, do not forget to stop the instance to avoid further charging, as shown in Fig. 16.2.7.

764

Chapter 16. Appendix: Tools for Deep Learning

Fig. 16.2.7: Stop a SageMaker instance.
16.2.4 Updating Notebooks
We will regularly update the notebooks in the d2l-ai/d2l-en-sagemaker218 GitHub repository. You can simply use the git pull command to update to the latest version. First, you need to open a terminal as shown in Fig. 16.2.8.

Fig. 16.2.8: Open a terminal on the SageMaker instance.
You may want to commit your local changes before pulling the updates. Alternatively, you can simply ignore all your local changes with the following commands in the terminal. cd SageMaker/d2l-en-sagemaker/ git reset --hard git pull
Summary
• We can launch and stop a Jupyter server through Amazon SageMaker to run this book. • We can update notebooks via the terminal on the Amazon SageMaker instance.
218 https://github.com/d2l-ai/d2l-en-sagemaker

16.2. Using Amazon SageMaker

765

Exercises
1. Try to edit and run the code in this book using Amazon SageMaker. 2. Access the source code directory via the terminal. Discussions219
16.3 Using AWS EC2 Instances
In this section, we will show you how to install all libraries on a raw Linux machine. Remember that in Section 16.2 we discussed how to use Amazon SageMaker, while building an instance by yourself costs less on AWS. The walkthrough includes a number of steps:
1. Request for a GPU Linux instance from AWS EC2. 2. Optionally: install CUDA or use an AMI with CUDA preinstalled. 3. Set up the corresponding MXNet GPU version. This process applies to other instances (and other clouds), too, albeit with some minor modifications. Before going forward, you need to create an AWS account, see Section 16.2 for more details.
16.3.1 Creating and Running an EC2 Instance
After logging into your AWS account, click “EC2” (marked by the red box in Fig. 16.3.1) to go to the EC2 panel.

Fig. 16.3.1: Open the EC2 console.
Fig. 16.3.2 shows the EC2 panel with sensitive account information greyed out.
219 https://discuss.d2l.ai/t/422

766

Chapter 16. Appendix: Tools for Deep Learning

Fig. 16.3.2: EC2 panel.
Presetting Location
Select a nearby data center to reduce latency, e.g., “Oregon” (marked by the red box in the topright of Fig. 16.3.2). If you are located in China, you can select a nearby Asia Pacific region, such as Seoul or Tokyo. Please note that some data centers may not have GPU instances.
Increasing Limits
Before choosing an instance, check if there are quantity restrictions by clicking the “Limits” label in the bar on the left as shown in Fig. 16.3.2. Fig. 16.3.3 shows an example of such a limitation. The account currently cannot open “p2.xlarge” instance per region. If you need to open one or more instances, click on the “Request limit increase” link to apply for a higher instance quota. Generally, it takes one business day to process an application.

Fig. 16.3.3: Instance quantity restrictions.

16.3. Using AWS EC2 Instances

767

Launching Instance
Next, click the “Launch Instance” button marked by the red box in Fig. 16.3.2 to launch your instance. We begin by selecting a suitable AMI (AWS Machine Image). Enter “Ubuntu” in the search box (marked by the red box in Fig. 16.3.4).

Fig. 16.3.4: Choose an operating system.

EC2 provides many different instance configurations to choose from. This can sometimes feel overwhelming to a beginner. Hereʼs a table of suitable machines:

Name g2 p2 g3 p3 g4

GPU Grid K520 Kepler K80 Maxwell M60 Volta V100 Turing T4

Notes ancient old but often cheap as spot good trade-off high performance for FP16 inference optimized FP16/INT8

All the above servers come in multiple flavors indicating the number of GPUs used. For example, a p2.xlarge has 1 GPU and a p2.16xlarge has 16 GPUs and more memory. For more details, see the AWS EC2 documentation220 or a summary page221. For the purpose of illustration, a p2.xlarge will suffice (marked in red box of Fig. 16.3.5).
Note: you must use a GPU enabled instance with suitable drivers and a version of MXNet that is GPU enabled. Otherwise you will not see any benefit from using GPUs.
220 https://aws.amazon.com/ec2/instance-types/ 221 https://www.ec2instances.info

768

Chapter 16. Appendix: Tools for Deep Learning

Fig. 16.3.5: Choose an instance. So far, we have finished the first two of seven steps for launching an EC2 instance, as shown on the top of Fig. 16.3.6. In this example, we keep the default configurations for the steps “3. Configure Instance”, “5. Add Tags”, and “6. Configure Security Group”. Tap on “4. Add Storage” and increase the default hard disk size to 64 GB (marked in red box of Fig. 16.3.6). Note that CUDA by itself already takes up 4 GB.
Fig. 16.3.6: Modify instance hard disk size. Finally, go to “7. Review” and click “Launch” to launch the configured instance. The system will now prompt you to select the key pair used to access the instance. If you do not have a key pair, select “Create a new key pair” in the first drop-down menu in Fig. 16.3.7 to generate a key pair. Subsequently, you can select “Choose an existing key pair” for this menu and then select the previously generated key pair. Click “Launch Instances” to launch the created instance.

Fig. 16.3.7: Select a key pair.

16.3. Using AWS EC2 Instances

769

Make sure that you download the key pair and store it in a safe location if you generated a new one. This is your only way to SSH into the server. Click the instance ID shown in Fig. 16.3.8 to view the status of this instance.
Fig. 16.3.8: Click the instance ID. Connecting to the Instance As shown in Fig. 16.3.9, after the instance state turns green, right-click the instance and select Connect to view the instance access method.
Fig. 16.3.9: View instance access and startup method. If this is a new key, it must not be publicly viewable for SSH to work. Go to the folder where you store D2L_key.pem (e.g., the Downloads folder) and make sure that the key is not publicly viewable. cd /Downloads ## if D2L_key.pem is stored in Downloads folder chmod 400 D2L_key.pem

Fig. 16.3.10: View instance access and startup method.

770

Chapter 16. Appendix: Tools for Deep Learning

Now, copy the ssh command in the lower red box of Fig. 16.3.10 and paste onto the command line: ssh -i "D2L_key.pem" ubuntu@ec2-xx-xxx-xxx-xxx.y.compute.amazonaws.com
When the command line prompts “Are you sure you want to continue connecting (yes/no)”, enter “yes” and press Enter to log into the instance. Your server is ready now.
16.3.2 Installing CUDA
Before installing CUDA, be sure to update the instance with the latest drivers. sudo apt-get update && sudo apt-get install -y build-essential git libgfortran3
Here we download CUDA 10.1. Visit NVIDIAʼs official repository222 to find the download link of CUDA 10.1 as shown in Fig. 16.3.11.

Fig. 16.3.11: Find the CUDA 10.1 download address.
Copy the instructions and paste them into the terminal to install CUDA 10.1.
## Paste the copied link from CUDA website wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/cuda→ubuntu1804.pin sudo mv cuda-ubuntu1804.pin /etc/apt/preferences.d/cuda-repository-pin-600 wget http://developer.download.nvidia.com/compute/cuda/10.1/Prod/local_installers/cuda-repo→ubuntu1804-10-1-local-10.1.243-418.87.00_1.0-1_amd64.deb sudo dpkg -i cuda-repo-ubuntu1804-10-1-local-10.1.243-418.87.00_1.0-1_amd64.deb sudo apt-key add /var/cuda-repo-10-1-local-10.1.243-418.87.00/7fa2af80.pub sudo apt-get update sudo apt-get -y install cuda
After installing the program, run the following command to view the GPUs.
222 https://developer.nvidia.com/cuda-downloads

16.3. Using AWS EC2 Instances

771

nvidia-smi
Finally, add CUDA to the library path to help other libraries find it. echo "export LD_LIBRARY_PATH=\${LD_LIBRARY_PATH}:/usr/local/cuda/lib64" >> ~/.bashrc
16.3.3 Installing MXNet and Downloading the D2L Notebooks
First, to simplify the installation, you need to install Miniconda223 for Linux. The download link and file name are subject to changes, so please go the Miniconda website and click “Copy Link Address” as shown in Fig. 16.3.12.

Fig. 16.3.12: Download Miniconda.
# The link and file name are subject to changes wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh sh Miniconda3-latest-Linux-x86_64.sh -b
After the Miniconda installation, run the following command to activate CUDA and conda. ~/miniconda3/bin/conda init source ~/.bashrc
Next, download the code for this book. sudo apt-get install unzip mkdir d2l-en && cd d2l-en curl https://d2l.ai/d2l-en.zip -o d2l-en.zip unzip d2l-en.zip && rm d2l-en.zip
Then create the conda d2l environment and enter y to proceed with the installation. conda create --name d2l -y
After creating the d2l environment, activate it and install pip. conda activate d2l conda install python=3.7 pip -y
223 https://conda.io/en/latest/miniconda.html

772

Chapter 16. Appendix: Tools for Deep Learning

Finally, install MXNet and the d2l package. The postfix cu101 means that this is the CUDA 10.1 variant. For different versions, say only CUDA 10.0, you would want to choose cu100 instead. pip install mxnet-cu101==1.7.0 pip install git+https://github.com/d2l-ai/d2l-en
You can quickly test whether everything went well as follows: $ python >>> from mxnet import np, npx >>> np.zeros((1024, 1024), ctx=npx.gpu())
16.3.4 Running Jupyter
To run Jupyter remotely you need to use SSH port forwarding. After all, the server in the cloud does not have a monitor or keyboard. For this, log into your server from your desktop (or laptop) as follows. # This command must be run in the local command line ssh -i "/path/to/key.pem" ubuntu@ec2-xx-xxx-xxx-xxx.y.compute.amazonaws.com -L␣ →8889:localhost:8888 conda activate d2l jupyter notebook
Fig. 16.3.13 shows the possible output after you run Jupyter Notebook. The last row is the URL for port 8888.
Fig. 16.3.13: Output after running Jupyter Notebook. The last row is the URL for port 8888.
Since you used port forwarding to port 8889 you will need to replace the port number and use the secret as given by Jupyter when opening the URL in your local browser.

16.3. Using AWS EC2 Instances

773

16.3.5 Closing Unused Instances
As cloud services are billed by the time of use, you should close instances that are not being used. Note that there are alternatives: “stopping” an instance means that you will be able to start it again. This is akin to switching off the power for your regular server. However, stopped instances will still be billed a small amount for the hard disk space retained. “Terminate” deletes all data associated with it. This includes the disk, hence you cannot start it again. Only do this if you know that you will not need it in the future. If you want to use the instance as a template for many more instances, right-click on the example in Fig. 16.3.9 and select “Image” → “Create” to create an image of the instance. Once this is complete, select “Instance State” → “Terminate” to terminate the instance. The next time you want to use this instance, you can follow the steps for creating and running an EC2 instance described in this section to create an instance based on the saved image. The only difference is that, in “1. Choose AMI” shown in Fig. 16.3.4, you must use the “My AMIs” option on the left to select your saved image. The created instance will retain the information stored on the image hard disk. For example, you will not have to reinstall CUDA and other runtime environments.
Summary
• You can launch and stop instances on demand without having to buy and build your own computer.
• You need to install suitable GPU drivers before you can use them.
Exercises
1. The cloud offers convenience, but it does not come cheap. Find out how to launch spot instances224 to see how to reduce prices.
2. Experiment with different GPU servers. How fast are they? 3. Experiment with multi-GPU servers. How well can you scale things up? Discussions225
16.4 Using Google Colab
We introduced how to run this book on AWS in Section 16.2 and Section 16.3. Another option is running this book on Google Colab226, which provides free GPU if you have a Google account. To run a section on Colab, you can simply click the Colab button to the right of the title of that section, such as in Fig. 16.4.1.
224 https://aws.amazon.com/ec2/spot/ 225 https://discuss.d2l.ai/t/423 226 https://colab.research.google.com/

774

Chapter 16. Appendix: Tools for Deep Learning

Fig. 16.4.1: Open a section on Colab
When it is the first time you execute a code cell, you will receive a warning message as shown in Fig. 16.4.2. You may click “RUN ANYWAY” to ignore it.

Fig. 16.4.2: The warning message for running a section on Colab
Next, Colab will connect you to an instance to run this notebook. Specifically, if GPU is needed, such as when invoking the d2l.try_gpu() function, we will request Colab to connect to a GPU instance automatically.
Summary
• You can use Google Colab to run each section of this book with GPUs.
Exercises
1. Try to edit and run the code in this book using Google Colab. Discussions227

16.5 Selecting Servers and GPUs
Deep learning training generally requires large amounts of computation. At present GPUs are the most cost-effective hardware accelerators for deep learning. In particular, compared with CPUs, GPUs are cheaper and offer higher performance, often by over an order of magnitude. Furthermore, a single server can support multiple GPUs, up to 8 for high end servers. More typical numbers are up to 4 GPUs for an engineering workstation, since heat, cooling and power requirements escalate quickly beyond what an office building can support. For larger deployments cloud computing, such as Amazonʼs P3228 and G4229 instances are a much more practical solution.
227 https://discuss.d2l.ai/t/424 228 https://aws.amazon.com/ec2/instance-types/p3/ 229 https://aws.amazon.com/blogs/aws/in-the-works-ec2-instances-g4-with-nvidia-t4-gpus/

16.5. Selecting Servers and GPUs

775

16.5.1 Selecting Servers
There is typically no need to purchase high-end CPUs with many threads since much of the computation occurs on the GPUs. That said, due to the Global Interpreter Lock (GIL) in Python singlethread performance of a CPU can matter in situations where we have 4-8 GPUs. All things equal this suggests that CPUs with a smaller number of cores but a higher clock frequency might be a more economical choice. E.g., when choosing between a 6-core 4 GHz and an 8-core 3.5 GHz CPU, the former is much preferable, even though its aggregate speed is less. An important consideration is that GPUs use lots of power and thus dissipate lots of heat. This requires very good cooling and a large enough chassis to use the GPUs. Follow the guidelines below if possible:
1. Power Supply. GPUs use significant amounts of power. Budget with up to 350W per device (check for the peak demand of the graphics card rather than typical demand, since efficient code can use lots of energy). If your power supply is not up to the demand you will find that your system becomes unstable.
2. Chassis Size. GPUs are large and the auxiliary power connectors often need extra space. Also, large chassis are easier to cool.
3. GPU Cooling. If you have large numbers of GPUs you might want to invest in water cooling. Also, aim for reference designs even if they have fewer fans, since they are thin enough to allow for air intake between the devices. If you buy a multi-fan GPU it might be too thick to get enough air when installing multiple GPUs and you will run into thermal throttling.
4. PCIe Slots. Moving data to and from the GPU (and exchanging it between GPUs) requires lots of bandwidth. We recommend PCIe 3.0 slots with 16 lanes. If you mount multiple GPUs, be sure to carefully read the motherboard description to ensure that 16x bandwidth is still available when multiple GPUs are used at the same time and that you are getting PCIe 3.0 as opposed to PCIe 2.0 for the additional slots. Some motherboards downgrade to 8x or even 4x bandwidth with multiple GPUs installed. This is partly due to the number of PCIe lanes that the CPU offers.
In short, here are some recommendations for building a deep learning server:
• Beginner. Buy a low end GPU with low power consumption (cheap gaming GPUs suitable for deep learning use 150-200W). If you are lucky your current computer will support it.
• 1 GPU. A low-end CPU with 4 cores will be plenty sufficient and most motherboards suffice. Aim for at least 32 GB DRAM and invest into an SSD for local data access. A power supply with 600W should be sufficient. Buy a GPU with lots of fans.
• 2 GPUs. A low-end CPU with 4-6 cores will suffice. Aim for 64 GB DRAM and invest into an SSD. You will need in the order of 1000W for two high-end GPUs. In terms of mainboards, make sure that they have two PCIe 3.0 x16 slots. If you can, get a mainboard that has two free spaces (60mm spacing) between the PCIe 3.0 x16 slots for extra air. In this case, buy two GPUs with lots of fans.
• 4 GPUs. Make sure that you buy a CPU with relatively fast single-thread speed (i.e., high clock frequency). You will probably need a CPU with a larger number of PCIe lanes, such as an AMD Threadripper. You will likely need relatively expensive mainboards to get 4 PCIe 3.0 x16 slots since they probably need a PLX to multiplex the PCIe lanes. Buy GPUs with reference design that are narrow and let air in between the GPUs. You need a 1600-2000W power supply and the outlet in your office might not support that. This server will probably run loud and hot. You do not want it under your desk. 128 GB of DRAM is recommended. Get

776

Chapter 16. Appendix: Tools for Deep Learning

an SSD (1-2 TB NVMe) for local storage and a bunch of hard disks in RAID configuration to store your data.
• 8 GPUs. You need to buy a dedicated multi-GPU server chassis with multiple redundant power supplies (e.g., 2+1 for 1600W per power supply). This will require dual socket server CPUs, 256 GB ECC DRAM, a fast network card (10 GBE recommended), and you will need to check whether the servers support the physical form factor of the GPUs. Airflow and wiring placement differ significantly between consumer and server GPUs (e.g., RTX 2080 vs. Tesla V100). This means that you might not be able to install the consumer GPU in a server due to insufficient clearance for the power cable or lack of a suitable wiring harness (as one of the coauthors painfully discovered).

16.5.2 Selecting GPUs
At present, AMD and NVIDIA are the two main manufacturers of dedicated GPUs. NVIDIA was the first to enter the deep learning field and provides better support for deep learning frameworks via CUDA. Therefore, most buyers choose NVIDIA GPUs.
NVIDIA provides two types of GPUs, targeting individual users (e.g., via the GTX and RTX series) and enterprise users (via its Tesla series). The two types of GPUs provide comparable compute power. However, the enterprise user GPUs generally use (passive) forced cooling, more memory, and ECC (error correcting) memory. These GPUs are more suitable for data centers and usually cost ten times more than consumer GPUs.
If you are a large company with 100+ servers you should consider the NVIDIA Tesla series or alternatively use GPU servers in the cloud. For a lab or a small to medium company with 10+ servers the NVIDIA RTX series is likely most cost effective. You can buy preconfigured servers with Supermicro or Asus chassis that hold 4-8 GPUs efficiently.
GPU vendors typically release a new generation every 1-2 years, such as the GTX 1000 (Pascal) series released in 2017 and the RTX 2000 (Turing) series released in 2019. Each series offers several different models that provide different performance levels. GPU performance is primarily a combination of the following three parameters:
1. Compute power. Generally we look for 32-bit floating-point compute power. 16-bit floating point training (FP16) is also entering the mainstream. If you are only interested in prediction, you can also use 8-bit integer. The latest generation of Turing GPUs offers 4-bit acceleration. Unfortunately at present the algorithms to train low-precision networks are not widespread yet.
2. Memory size. As your models become larger or the batches used during training grow bigger, you will need more GPU memory. Check for HBM2 (High Bandwidth Memory) vs. GDDR6 (Graphics DDR) memory. HBM2 is faster but much more expensive.
3. Memory bandwidth. You can only get the most out of your compute power when you have sufficient memory bandwidth. Look for wide memory buses if using GDDR6.
For most users, it is enough to look at compute power. Note that many GPUs offer different types of acceleration. E.g., NVIDIAʼs TensorCores accelerate a subset of operators by 5x. Ensure that your libraries support this. The GPU memory should be no less than 4 GB (8 GB is much better). Try to avoid using the GPU also for displaying a GUI (use the built-in graphics instead). If you cannot avoid it, add an extra 2 GB of RAM for safety.
Fig. 16.5.1 compares the 32-bit floating-point compute power and price of the various GTX 900, GTX 1000 and RTX 2000 series models. The prices are the suggested prices found on Wikipedia.

16.5. Selecting Servers and GPUs

777

Fig. 16.5.1: Floating-point compute power and price comparison.
We can see a number of things:
1. Within each series, price and performance are roughly proportional. Titan models command a significant premium for the benefit of larger amounts of GPU memory. However, the newer models offer better cost effectiveness, as can be seen by comparing the 980 Ti and 1080 Ti. The price does not appear to improve much for the RTX 2000 series. However, this is due to the fact that they offer far superior low precision performance (FP16, INT8 and INT4).
2. The performance-to-cost ratio of the GTX 1000 series is about two times greater than the 900 series.
3. For the RTX 2000 series the price is an affine function of the price.

778

Chapter 16. Appendix: Tools for Deep Learning

Fig. 16.5.2: Floating-point compute power and energy consumption.
Fig. 16.5.2 shows how energy consumption scales mostly linearly with the amount of computation. Second, later generations are more efficient. This seems to be contradicted by the graph corresponding to the RTX 2000 series. However, this is a consequence of the TensorCores which draw disproportionately much energy.
Summary
• Watch out for power, PCIe bus lanes, CPU single thread speed and cooling when building a server.
• You should purchase the latest GPU generation if possible. • Use the cloud for large deployments. • High density servers may not be compatible with all GPUs. Check the mechanical and cool-
ing specifications before you buy. • Use FP16 or lower precision for high efficiency. Discussions230
230 https://discuss.d2l.ai/t/425

16.5. Selecting Servers and GPUs

779

16.6 Contributing to This Book
Contributions by readers231 help us improve this book. If you find a typo, an outdated link, something where you think we missed a citation, where the code does not look elegant or where an explanation is unclear, please contribute back and help us help our readers. While in regular books the delay between print runs (and thus between typo corrections) can be measured in years, it typically takes hours to days to incorporate an improvement in this book. This is all possible due to version control and continuous integration testing. To do so you need to submit a pull request232 to the GitHub repository. When your pull request is merged into the code repository by the author, you will become a contributor.
16.6.1 Minor Text Changes
The most common contributions are editing one sentence or fixing typos. We recommend you to find the source file in the github repo233 and edit the file directly. For example, you can search the file through the Find file234 button (Fig. 16.6.1) to locate the source file, which is a markdown file. Then you click the “Edit this file” button on the upper-right corner to make your changes in the markdown file.

Fig. 16.6.1: Edit the file on Github.
After you are done, fill in your change descriptions in the “Propose file change” panel on the page bottom and then click the “Propose file change” button. It will redirect you to a new page to review your changes (Fig. 16.6.7). If everything is good, you can submit a pull request by clicking the “Create pull request” button.
16.6.2 Propose a Major Change
If you plan to update a large portion of text or code, then you need to know a little bit more about the format this book is using. The source file is based on the markdown format235 with a set of extensions through the d2lbook236 package such as referring to equations, images, chapters, and citations. You can use any Markdown editors to open these files and make your changes.
231 https://github.com/d2l-ai/d2l-en/graphs/contributors 232 https://github.com/d2l-ai/d2l-en/pulls 233 https://github.com/d2l-ai/d2l-en 234 https://github.com/d2l-ai/d2l-en/find/master 235 https://daringfireball.net/projects/markdown/syntax 236 http://book.d2l.ai/user/markdown.html

780

Chapter 16. Appendix: Tools for Deep Learning

If you would like to change the code, we recommend you to use Jupyter to open these Markdown files as described in Section 16.1. So that you can run and test your changes. Please remember to clear all outputs before submitting your changes, our CI system will execute the sections you updated to generate outputs. Some sections may support multiple framework implementations, you can use d2lbook to activate a particular framework, so other framework implementations become Markdown code blocks and will not be executed when you “Run All” in Jupyter. In other words, first install d2lbook by running
pip install git+https://github.com/d2l-ai/d2l-book
Then in the root directory of d2l-en, you can activate a particular implementation by running one of the following commands:
d2lbook activate mxnet chapter_multilayer-perceptrons/mlp-scratch.md d2lbook activate pytorch chapter_multilayer-perceptrons/mlp-scratch.md d2lbook activate tensorflow chapter_multilayer-perceptrons/mlp-scratch.md
Before submitting your changes, please clear all code block outputs and activate all by
d2lbook activate all chapter_multilayer-perceptrons/mlp-scratch.md
If you add a new code block not for the default implementation, which is MXNet, please use #@tab to mark this block on the beginning line. For example, #@tab pytorch for a PyTorch code block, #@tab tensorflow for a TensorFlow code block, or #@tab all a shared code block for all implementations. You may refer to d2lbook237 for more information.
16.6.3 Adding a New Section or a New Framework Implementation
If you want to create a new chapter, e.g. reinforcement learning, or add implementations of new frameworks, such as TensorFlow, please contact the authors first, either by emailing or using github issues238.
16.6.4 Submitting a Major Change
We suggest you to use the standard git process to submit a major change. In a nutshell the process works as described in Fig. 16.6.2.

Fig. 16.6.2: Contributing to the book.
237 http://book.d2l.ai/user/code_tabs.html 238 https://github.com/d2l-ai/d2l-en/issues

16.6. Contributing to This Book

781

We will walk you through the steps in detail. If you are already familiar with Git you can skip this section. For concreteness we assume that the contributorʼs user name is “astonzhang”.
Installing Git The Git open source book describes how to install Git239. This typically works via apt install git on Ubuntu Linux, by installing the Xcode developer tools on macOS, or by using GitHubʼs desktop client240. If you do not have a GitHub account, you need to sign up for one.
Logging in to GitHub Enter the address241 of the bookʼs code repository in your browser. Click on the Fork button in the red box at the upper-right of Fig. 16.6.3, to make a copy of the repository of this book. This is now your copy and you can change it any way you want.
Fig. 16.6.3: The code repository page.
Now, the code repository of this book will be forked (i.e., copied) to your username, such as astonzhang/d2l-en shown at the upper-left of the screenshot Fig. 16.6.4.
Fig. 16.6.4: Fork the code repository.
Cloning the Repository To clone the repository (i.e., to make a local copy) we need to get its repository address. The green button in Fig. 16.6.5 displays this. Make sure that your local copy is up to date with the main repository if you decide to keep this fork around for longer. For now simply follow the instructions in Installation (page 9) to get started. The main difference is that you are now downloading your own fork of the repository.
239 https://git-scm.com/book/en/v2 240 https://desktop.github.com 241 https://github.com/d2l-ai/d2l-en/

782

Chapter 16. Appendix: Tools for Deep Learning

Fig. 16.6.5: Git clone.
# Replace your_github_username with your GitHub username git clone https://github.com/your_github_username/d2l-en.git
Editing the Book and Push
Now it is time to edit the book. It is best to edit the notebooks in Jupyter following instructions in Section 16.1. Make the changes and check that they are OK. Assume we have modified a typo in the file ~/d2l-en/chapter_appendix_tools/how-to-contribute.md. You can then check which files you have changed: At this point Git will prompt that the chapter_appendix_tools/how-to-contribute.md file has been modified.
mylaptop:d2l-en me$ git status On branch master Your branch is up-to-date with 'origin/master'.
Changes not staged for commit: (use "git add <file>..." to update what will be committed) (use "git checkout -- <file>..." to discard changes in working directory)
modified: chapter_appendix_tools/how-to-contribute.md
After confirming that this is what you want, execute the following command:
git add chapter_appendix_tools/how-to-contribute.md git commit -m 'fix typo in git documentation' git push
The changed code will then be in your personal fork of the repository. To request the addition of your change, you have to create a pull request for the official repository of the book.

16.6. Contributing to This Book

783

Pull Request As shown in Fig. 16.6.6, go to your fork of the repository on GitHub and select “New pull request”. This will open up a screen that shows you the changes between your edits and what is current in the main repository of the book.
Fig. 16.6.6: Pull Request.
Submitting Pull Request Finally, submit a pull request by clicking the button as shown in Fig. 16.6.7. Make sure to describe the changes you have made in the pull request. This will make it easier for the authors to review it and to merge it with the book. Depending on the changes, this might get accepted right away, rejected, or more likely, you will get some feedback on the changes. Once you have incorporated them, you are good to go.

Fig. 16.6.7: Create Pull Request.

Your pull request will appear among the list of requests in the main repository. We will make every effort to process it quickly.

Summary

• You can use GitHub to contribute to this book.
• You can edit the file on GitHub directly for minor changes.
• For a major change, please fork the repository, edit things locally and only contribute back once you are ready.
• Pull requests are how contributions are being bundled up. Try not to submit huge pull requests since this makes them hard to understand and incorporate. Better send several smaller ones.

784

Chapter 16. Appendix: Tools for Deep Learning

Exercises
1. Star and fork the d2l-en repository. 2. Find some code that needs improvement and submit a pull request. 3. Find a reference that we missed and submit a pull request. 4. It is usually a better practice to create a pull request using a new branch. Learn how to do it
with Git branching242. Discussions243

16.7 d2l API Document

The implementations of the following members of the d2l package and sections where they are defined and explained can be found in the source file244.

class d2l.mxnet.Accumulator(n) For accumulating sums over n variables.

class d2l.mxnet.AddNorm(dropout, **kwargs) Residual connection followed by layer normalization.

forward(X, Y ) Overrides to implement forward computation using NDArray. Only accepts positional arguments.

*args [list of NDArray] Input tensors.

class d2l.mxnet.AdditiveAttention(num_hiddens, dropout, **kwargs) Additive attention.

forward(queries, keys, values, valid_lens) Overrides to implement forward computation using NDArray. Only accepts positional arguments.

*args [list of NDArray] Input tensors.

class d2l.mxnet.Animator(xlabel=None, ylabel=None, legend=None, xlim=None, ylim=None,
xscale='linear', yscale='linear', fmts=('-', 'm--', 'g-.', 'r:'), nrows=1, ncols=1, figsize=(3.5, 2.5)) For plotting data in animation.

class d2l.mxnet.AttentionDecoder(**kwargs) The base attention-based decoder interface.

class d2l.mxnet.BERTEncoder(vocab_size, num_hiddens, ffn_num_hiddens,
num_layers, dropout, max_len=1000, **kwargs) BERT encoder.

num_heads,

forward(tokens, segments, valid_lens) Overrides to implement forward computation using NDArray. Only accepts positional arguments.

242 https://git-scm.com/book/en/v2/Git-Branching-Branches-in-a-Nutshell 243 https://discuss.d2l.ai/t/426 244 https://github.com/d2l-ai/d2l-en/tree/master/d2l

16.7. d2l API Document

785

*args [list of NDArray] Input tensors.

class d2l.mxnet.BERTModel(vocab_size, num_hiddens, ffn_num_hiddens,
num_layers, dropout, max_len=1000) The BERT model.

num_heads,

forward(tokens, segments, valid_lens=None, pred_positions=None) Overrides to implement forward computation using NDArray. Only accepts positional arguments.

*args [list of NDArray] Input tensors.

class d2l.mxnet.BPRLoss(weight=None, batch_axis=0, **kwargs)

forward(positive, negative) Defines the forward computation. Arguments can be either NDArray or Symbol.

class d2l.mxnet.BananasDataset(is_train) A customized dataset to load the banana detection dataset.

class d2l.mxnet.Benchmark(description='Done') For measuring running time.

class d2l.mxnet.CTRDataset(data_path,

feat_mapper=None,

min_threshold=4, num_feat=34)

class d2l.mxnet.Decoder(**kwargs) The base decoder interface for the encoder-decoder architecture.

defaults=None,

forward(X, state) Overrides to implement forward computation using NDArray. Only accepts positional arguments.

*args [list of NDArray] Input tensors.

class d2l.mxnet.DotProductAttention(dropout, **kwargs) Scaled dot product attention.

forward(queries, keys, values, valid_lens=None) Overrides to implement forward computation using NDArray. Only accepts positional arguments.

*args [list of NDArray] Input tensors.

class d2l.mxnet.Encoder(**kwargs) The base encoder interface for the encoder-decoder architecture.

forward(X, *args) Overrides to implement forward computation using NDArray. Only accepts positional arguments.

*args [list of NDArray] Input tensors.

class d2l.mxnet.EncoderBlock(num_hiddens, ffn_num_hiddens,
use_bias=False, **kwargs) Transformer encoder block.

num_heads,

dropout,

forward(X, valid_lens) Overrides to implement forward computation using NDArray. Only accepts positional arguments.

786

Chapter 16. Appendix: Tools for Deep Learning

*args [list of NDArray] Input tensors.
class d2l.mxnet.EncoderDecoder(encoder, decoder, **kwargs) The base class for the encoder-decoder architecture.
forward(enc_X, dec_X, *args) Overrides to implement forward computation using NDArray. Only accepts positional arguments.
*args [list of NDArray] Input tensors.
class d2l.mxnet.HingeLossbRec(weight=None, batch_axis=0, **kwargs)
forward(positive, negative, margin=1) Defines the forward computation. Arguments can be either NDArray or Symbol.
class d2l.mxnet.MaskLM(vocab_size, num_hiddens, **kwargs) The masked language model task of BERT.
forward(X, pred_positions) Overrides to implement forward computation using NDArray. Only accepts positional arguments.
*args [list of NDArray] Input tensors.
class d2l.mxnet.MaskedSoftmaxCELoss(axis=- 1, sparse_label=True, from_logits=False, weight=None, batch_axis=0, **kwargs)
The softmax cross-entropy loss with masks.
forward(pred, label, valid_len) Defines the forward computation. Arguments can be either NDArray or Symbol.
class d2l.mxnet.MultiHeadAttention(num_hiddens, num_heads, dropout, use_bias=False, **kwargs)
Multi-head attention.
forward(queries, keys, values, valid_lens) Overrides to implement forward computation using NDArray. Only accepts positional arguments.
*args [list of NDArray] Input tensors.
class d2l.mxnet.NextSentencePred(**kwargs) The next sentence prediction task of BERT.
forward(X) Overrides to implement forward computation using NDArray. Only accepts positional arguments.
*args [list of NDArray] Input tensors.
class d2l.mxnet.PositionWiseFFN(ffn_num_hiddens, ffn_num_outputs, **kwargs) Positionwise feed-forward network.
forward(X) Overrides to implement forward computation using NDArray. Only accepts positional arguments.
*args [list of NDArray] Input tensors.

16.7. d2l API Document

787

class d2l.mxnet.PositionalEncoding(num_hiddens, dropout, max_len=1000) Positional encoding.

forward(X) Overrides to implement forward computation using NDArray. Only accepts positional arguments.

*args [list of NDArray] Input tensors.

class d2l.mxnet.RNNModel(rnn_layer, vocab_size, **kwargs) The RNN model.

forward(inputs, state) Overrides to implement forward computation using NDArray. Only accepts positional arguments.

*args [list of NDArray] Input tensors.

class d2l.mxnet.RNNModelScratch(vocab_size, num_hiddens, device, get_params, init_state,
forward_fn) An RNN Model implemented from scratch.

class d2l.mxnet.RandomGenerator(sampling_weights) Randomly draw among {1, …, n} according to n sampling weights.

class d2l.mxnet.Residual(num_channels, use_1x1conv=False, strides=1, **kwargs) The Residual block of ResNet.

forward(X) Overrides to implement forward computation using NDArray. Only accepts positional arguments.

*args [list of NDArray] Input tensors.

class d2l.mxnet.SNLIDataset(dataset, num_steps, vocab=None) A customized dataset to load the SNLI dataset.

class d2l.mxnet.Seq2SeqEncoder(vocab_size, embed_size,
dropout=0, **kwargs) The RNN encoder for sequence to sequence learning.

num_hiddens,

num_layers,

forward(X, *args) Overrides to implement forward computation using NDArray. Only accepts positional arguments.

*args [list of NDArray] Input tensors.

class d2l.mxnet.SeqDataLoader(batch_size, num_steps, use_random_iter, max_tokens) An iterator to load sequence data.

class d2l.mxnet.Timer Record multiple running times.

avg() Return the average time.

cumsum() Return the accumulated time.

start() Start the timer.

788

Chapter 16. Appendix: Tools for Deep Learning

stop() Stop the timer and record the time in a list.

sum() Return the sum of time.

class d2l.mxnet.TokenEmbedding(embedding_name) Token Embedding.

class d2l.mxnet.TransformerEncoder(vocab_size,
num_heads, **kwargs) Transformer encoder.

num_hiddens, ffn_num_hiddens, num_layers, dropout, use_bias=False,

forward(X, valid_lens, *args) Overrides to implement forward computation using NDArray. Only accepts positional arguments.

*args [list of NDArray] Input tensors.

class d2l.mxnet.VOCSegDataset(is_train, crop_size, voc_dir) A customized dataset to load the VOC dataset.

filter(imgs) Returns a new dataset with samples filtered by the filter function fn.

Note that if the Dataset is the result of a lazily transformed one with transform(lazy=False), the filter is eagerly applied to the transformed samples without materializing the transformed result. That is, the transformation will be applied again whenever a sample is retrieved after filter().

fn [callable] A filter function that takes a sample as input and returns a boolean. Samples that return False are discarded.

Dataset The filtered dataset.

class d2l.mxnet.Vocab(tokens=None, min_freq=0, reserved_tokens=None) Vocabulary for text.
d2l.mxnet.abs(x, out=None, **kwargs) Calculate the absolute value element-wise.
x [ndarray or scalar] Input array.
out [ndarray or None, optional] A location into which the result is stored. If provided, it must have a shape that the inputs broadcast to. If not provided or None, a freshlyallocated array is returned.

absolute [ndarray] An ndarray containing the absolute value of each element in x. This is a scalar if x is a scalar.

>>> x = np.array([-1.2, 1.2]) >>> np.abs(x) array([1.2, 1.2])
d2l.mxnet.accuracy(y_hat, y) Compute the number of correct predictions.

16.7. d2l API Document

789

d2l.mxnet.arange(start, stop=None, step=1, dtype=None, ctx=None) Return evenly spaced values within a given interval. Values are generated within the half-open interval [start, stop) (in other words, the interval including start but excluding stop). For integer arguments the function is equivalent to the Python built-in range function, but returns an ndarray rather than a list.
start [number, optional] Start of interval. The interval includes this value. The default start value is 0.
stop [number] End of interval. The interval does not include this value, except in some cases where step is not an integer and floating point round-off affects the length of out.
step [number, optional] Spacing between values. For any output out, this is the distance between two adjacent values, out[i+1] - out[i]. The default step size is 1. If step is specified as a position argument, start must also be given.
dtype [dtype] The type of the output array. The default is float32.
arange [ndarray] Array of evenly spaced values. For floating point arguments, the length of the result is ceil((stop - start)/step). Because of floating point overflow, this rule may result in the last element of out being greater than stop.
>>> np.arange(3) array([0., 1., 2.])
>>> np.arange(3.0) array([0., 1., 2.])
>>> np.arange(3,7) array([3., 4., 5., 6.])
>>> np.arange(3,7,2) array([3., 5.])
d2l.mxnet.assign_anchor_to_bbox(ground_truth, anchors, device, iou_threshold=0.5) Assign closest ground-truth bounding boxes to anchor boxes.
d2l.mxnet.batchify(data) Return a minibatch of examples for skip-gram with negative sampling.
d2l.mxnet.bbox_to_rect(bbox, color) Convert bounding box to matplotlib format.
d2l.mxnet.bleu(pred_seq, label_seq, k) Compute the BLEU.
d2l.mxnet.box_center_to_corner(boxes) Convert from (center, width, height) to (upper-left, lower-right).
d2l.mxnet.box_corner_to_center(boxes) Convert from (upper-left, lower-right) to (center, width, height).

790

Chapter 16. Appendix: Tools for Deep Learning

d2l.mxnet.box_iou(boxes1, boxes2) Compute pairwise IoU across two lists of anchor or bounding boxes.
d2l.mxnet.build_array_nmt(lines, vocab, num_steps) Transform text sequences of machine translation into minibatches.
d2l.mxnet.concat(seq, axis=0, out=None) Join a sequence of arrays along an existing axis.
a1, a2, … [sequence of array_like] The arrays must have the same shape, except in the dimension corresponding to axis (the first, by default).
axis [int, optional] The axis along which the arrays will be joined. If axis is None, arrays are flattened before use. Default is 0.
out [ndarray, optional] If provided, the destination to place the result. The shape must be correct, matching that of what concatenate would have returned if no out argument were specified.
res [ndarray] The concatenated array.
split : Split array into a list of multiple sub-arrays of equal size. hsplit : Split array into multiple sub-arrays horizontally (column wise) vsplit : Split array into multiple sub-arrays vertically (row wise) dsplit : Split array into multiple sub-arrays along the 3rd axis (depth). stack : Stack a sequence of arrays along a new axis. hstack : Stack arrays in sequence horizontally (column wise) vstack : Stack arrays in sequence vertically (row wise) dstack : Stack arrays in sequence depth wise (along third dimension)
>>> a = np.array([[1, 2], [3, 4]]) >>> b = np.array([[5, 6]]) >>> np.concatenate((a, b), axis=0) array([[1., 2.],
[3., 4.], [5., 6.]])
>>> np.concatenate((a, b.T), axis=1) array([[1., 2., 5.],
[3., 4., 6.]])
>>> np.concatenate((a, b), axis=None) array([1., 2., 3., 4., 5., 6.])
d2l.mxnet.copyfile(filename, target_dir) Copy a file into a target directory.
d2l.mxnet.corr2d(X, K) Compute 2D cross-correlation.
d2l.mxnet.cos(x, out=None, **kwargs) Cosine, element-wise.
x [ndarray or scalar] Angle, in radians (2π rad equals 360 degrees).
out [ndarray or None] A location into which the result is stored. If provided, it must have a shape that the inputs broadcast to. If not provided or None, a freshly-allocated array

16.7. d2l API Document

791

is returned. The dtype of the output is the same as that of the input if the input is an ndarray.
y [ndarray or scalar] The corresponding cosine values. This is a scalar if x is a scalar.
This function only supports input type of float.
>>> np.cos(np.array([0, np.pi/2, np.pi])) array([ 1.000000e+00, -4.371139e-08, -1.000000e+00]) >>> # Example of providing the optional output parameter >>> out1 = np.array([0], dtype='f') >>> out2 = np.cos(np.array([0.1]), out1) >>> out2 is out1 True
d2l.mxnet.cosh(x, out=None, **kwargs) Hyperbolic cosine, element-wise. Equivalent to 1/2 * (np.exp(x) + np.exp(-x)) and np. cos(1j*x).
x [ndarray or scalar] Input array or scalar.
out [ndarray or None] A location into which the result is stored. If provided, it must have a shape that the inputs broadcast to. If not provided or None, a freshly-allocated array is returned. The dtype of the output is the same as that of the input if the input is an ndarray.
y [ndarray or scalar] The corresponding hyperbolic cosine values. This is a scalar if x is a scalar.
This function only supports input type of float.
>>> np.cosh(0) 1.0
d2l.mxnet.count_corpus(tokens) Count token frequencies.
class d2l.mxnet.defaultdict defaultdict(default_factory[, …]) –> dict with default factory
The default factory is called without arguments to produce a new value when a key is not present, in __getitem__ only. A defaultdict compares equal to a dict with the same items. All remaining arguments are treated the same as if they were passed to the dict constructor, including keyword arguments.
copy() → a shallow copy of D.
default_factory Factory for default value called by __missing__().
d2l.mxnet.download(name, cache_dir='../data') Download a file inserted into DATA_HUB, return the local filename.
d2l.mxnet.download_all() Download all files in the DATA_HUB.

792

Chapter 16. Appendix: Tools for Deep Learning

d2l.mxnet.download_extract(name, folder=None) Download and extract a zip/tar file.
d2l.mxnet.evaluate_accuracy(net, data_iter) Compute the accuracy for a model on a dataset.
d2l.mxnet.evaluate_accuracy_gpu(net, data_iter, device=None) Compute the accuracy for a model on a dataset using a GPU.
d2l.mxnet.evaluate_accuracy_gpus(net, data_iter, split_f=<function split_batch>) Compute the accuracy for a model on a dataset using multiple GPUs.
d2l.mxnet.evaluate_loss(net, data_iter, loss) Evaluate the loss of a model on the given dataset.
d2l.mxnet.exp(x, out=None, **kwargs) Calculate the exponential of all elements in the input array.
x [ndarray or scalar] Input values.
out [ndarray or None, optional] A location into which the result is stored. If provided, it must have a shape that the inputs broadcast to. If not provided or None, a freshlyallocated array is returned.
out [ndarray or scalar] Output array, element-wise exponential of x. This is a scalar if x is a scalar.
>>> np.exp(1) 2.718281828459045 >>> x = np.array([-1, 1, -2, 2]) >>> np.exp(x) array([0.36787945, 2.7182817 , 0.13533528, 7.389056 ])
d2l.mxnet.eye(N, M=None, k=0, dtype=<class 'numpy.float32'>, **kwargs) Return a 2-D array with ones on the diagonal and zeros elsewhere.
N [int] Number of rows in the output.
M [int, optional] Number of columns in the output. If None, defaults to N.
k [int, optional] Index of the diagonal: 0 (the default) refers to the main diagonal, a positive value refers to an upper diagonal, and a negative value to a lower diagonal.
dtype [data-type, optional] Data-type of the returned array.
I [ndarray of shape (N,M)] An array where all elements are equal to zero, except for the k-th diagonal, whose values are equal to one.
>>> np.eye(2, dtype=int) array([[1, 0],
[0, 1]], dtype=int64) >>> np.eye(3, k=1) array([[0., 1., 0.],
[0., 0., 1.], [0., 0., 0.]])

16.7. d2l API Document

793

class d2l.mxnet.float32 Single-precision floating-point number type, compatible with C float. Character code: 'f'. Canonical name: np.single. Alias on this platform: np.float32: 32-bit-precision floatingpoint number type: sign bit, 8 bits exponent, 23 bits mantissa.
as_integer_ratio() Return a pair of integers, whose ratio is exactly equal to the original floating point number, and with a positive denominator. Raise OverflowError on infinities and a ValueError on NaNs.
>>> np.single(10.0).as_integer_ratio() (10, 1) >>> np.single(0.0).as_integer_ratio() (0, 1) >>> np.single(-.25).as_integer_ratio() (-1, 4)

d2l.mxnet.get_centers_and_contexts(corpus, max_window_size) Return center words and context words in skip-gram.
d2l.mxnet.get_dataloader_workers() Use 4 processes to read the data except for Windows.
d2l.mxnet.get_fashion_mnist_labels(labels) Return text labels for the Fashion-MNIST dataset.
d2l.mxnet.get_negatives(all_contexts, vocab, counter, K) Return noise words in negative sampling.
d2l.mxnet.get_tokens_and_segments(tokens_a, tokens_b=None) Get tokens of the BERT input sequence and their segment IDs.
d2l.mxnet.grad_clipping(net, theta) Clip the gradient.
class d2l.mxnet.int32 Signed integer type, compatible with C int. Character code: 'i'. Canonical name: np.intc. Alias on this platform: np.int32: 32-bit signed integer (-2147483648 to 2147483647).
d2l.mxnet.linreg(X, w, b) The linear regression model.
d2l.mxnet.linspace(start, stop, num=50, endpoint=True, retstep=False, dtype=None, axis=0, ctx=None)
Return evenly spaced numbers over a specified interval.
Returns num evenly spaced samples, calculated over the interval [start, stop]. The endpoint of the interval can optionally be excluded.
start [real number] The starting value of the sequence.
stop [real number] The end value of the sequence, unless endpoint is set to False. In that case, the sequence consists of all but the last of num + 1 evenly spaced samples, so that stop is excluded. Note that the step size changes when endpoint is False.
num [int, optional] Number of samples to generate. Default is 50. Must be non-negative.
endpoint [bool, optional] If True, stop is the last sample. Otherwise, it is not included. Default is True.

794

Chapter 16. Appendix: Tools for Deep Learning

retstep [bool, optional] If True, return (samples, step), where step is the spacing between samples.
dtype [dtype, optional] The type of the output array. If dtype is not given, infer the data type from the other input arguments.
axis [int, optional] The axis in the result to store the samples. Relevant only if start or stop are array-like. By default (0), the samples will be along a new axis inserted at the beginning. Use -1 to get an axis at the end.
samples [ndarray] There are num equally spaced samples in the closed interval [start, stop] or the half-open interval [start, stop) (depending on whether endpoint is True or False).
step [float, optional] Only returned if retstep is True Size of spacing between samples.
arange [Similar to linspace, but uses a step size (instead of the] number of samples).
>>> np.linspace(2.0, 3.0, num=5) array([2. , 2.25, 2.5 , 2.75, 3. ]) >>> np.linspace(2.0, 3.0, num=5, endpoint=False) array([2. , 2.2, 2.4, 2.6, 2.8]) >>> np.linspace(2.0, 3.0, num=5, retstep=True) (array([2. , 2.25, 2.5 , 2.75, 3. ]), 0.25)
Graphical illustration:
>>> import matplotlib.pyplot as plt >>> N = 8 >>> y = np.zeros(N) >>> x1 = np.linspace(0, 10, N, endpoint=True) >>> x2 = np.linspace(0, 10, N, endpoint=False) >>> plt.plot(x1.asnumpy(), y.asnumpy(), 'o') [<matplotlib.lines.Line2D object at 0x...>] >>> plt.plot(x2.asnumpy(), (y + 0.5).asnumpy(), 'o') [<matplotlib.lines.Line2D object at 0x...>] >>> plt.ylim([-0.5, 1]) (-0.5, 1) >>> plt.show()
This function differs from the original numpy.linspace245 in the following aspects:
• start and stop do not support list, numpy ndarray and mxnet ndarray
• axis could only be 0
• There could be an additional ctx argument to specify the device, e.g. the i-th GPU.
d2l.mxnet.load_array(data_arrays, batch_size, is_train=True) Construct a Gluon data iterator.
d2l.mxnet.load_corpus_time_machine(max_tokens=- 1) Return token indices and the vocabulary of the time machine dataset.
d2l.mxnet.load_data_bananas(batch_size) Load the banana detection dataset.
245 https://docs.scipy.org/doc/numpy/reference/generated/numpy.linspace.html

16.7. d2l API Document

795

d2l.mxnet.load_data_fashion_mnist(batch_size, resize=None) Download the Fashion-MNIST dataset and then load it into memory.
d2l.mxnet.load_data_imdb(batch_size, num_steps=500) Return data iterators and the vocabulary of the IMDb review dataset.
d2l.mxnet.load_data_nmt(batch_size, num_steps, num_examples=600) Return the iterator and the vocabularies of the translation dataset.
d2l.mxnet.load_data_ptb(batch_size, max_window_size, num_noise_words) Download the PTB dataset and then load it into memory.
d2l.mxnet.load_data_snli(batch_size, num_steps=50) Download the SNLI dataset and return data iterators and vocabulary.
d2l.mxnet.load_data_time_machine(batch_size, num_steps, use_random_iter=False, max_tokens=10000)
Return the iterator and the vocabulary of the time machine dataset.
d2l.mxnet.load_data_voc(batch_size, crop_size) Load the VOC semantic segmentation dataset.
d2l.mxnet.load_data_wiki(batch_size, max_len) Load the WikiText-2 dataset.
d2l.mxnet.log(x, out=None, **kwargs) Natural logarithm, element-wise. The natural logarithm log is the inverse of the exponential function, so that log(exp(x)) = x. The natural logarithm is logarithm in base e.
x [ndarray] Input value. Elements must be of real value.
out [ndarray or None, optional] A location into which the result is stored. If provided, it must have the same shape and dtype as input ndarray. If not provided or None, a freshlyallocated array is returned.

y [ndarray] The natural logarithm of x, element-wise. This is a scalar if x is a scalar.

Currently only supports data of real values and inf as input. Returns data of real value, inf, -inf and nan according to the input. This function differs from the original numpy.log246 in
the following aspects: - Does not support complex number for now - Input type does not support Python native iterables(list, tuple, …). - out param: cannot perform auto broadcasting. out ndarrayʼs shape must be the same as the expected output. - out param: cannot perform auto type cast. out ndarrayʼs dtype must be the same as the expected output. - out param
does not support scalar input case.

>>> a = np.array([1, np.exp(1), >>> np.log(a) array([ 0., 1., 2., -inf], >>> # Using the default float32 >>> a = np.array([1, np.exp(1), >>> np.log(a) array([ 0., 0.99999994, 2., >>> np.log(1) 0.0

np.exp(2), 0],
dtype=float64) dtype leads to np.exp(2), 0])
-inf])

dtype=np.float64) slightly different

behavior

246 https://docs.scipy.org/doc/numpy/reference/generated/numpy.log.html

796

Chapter 16. Appendix: Tools for Deep Learning

d2l.mxnet.masked_softmax(X, valid_lens) Perform softmax operation by masking elements on the last axis.
d2l.mxnet.matmul(a, b, out=None) Dot product of two arrays. Specifically,
• If both a and b are 1-D arrays, it is inner product of vectors
• If both a and b are 2-D arrays, it is matrix multiplication, • If either a or b is 0-D (scalar), it is equivalent to multiply() and using np.multiply(a,
b) or a * b is preferred.
• If a is an N-D array and b is a 1-D array, it is a sum product over the last axis of a and b.
• If a is an N-D array and b is a 2-D array, it is a sum product over the last axis of a and the second-to-last axis of b:
dot(a, b)[i,j,k] = sum(a[i,j,:] * b[:,k])

a [ndarray] First argument.
b [ndarray] Second argument.
out [ndarray, optional] Output argument. It must have the same shape and type as the expected output.
output [ndarray] Returns the dot product of a and b. If a and b are both scalars or both 1-D arrays then a scalar is returned; otherwise an array is returned. If out is given, then it is returned
>>> a = np.array(3) >>> b = np.array(4) >>> np.dot(a, b) array(12.)
For 2-D arrays it is the matrix product:
>>> a = np.array([[1, 0], [0, 1]]) >>> b = np.array([[4, 1], [2, 2]]) >>> np.dot(a, b) array([[4., 1.],
[2., 2.]])

>>> a = np.arange(3*4*5*6).reshape((3,4,5,6)) >>> b = np.arange(5*6)[::-1].reshape((6,5)) >>> np.dot(a, b)[2,3,2,2] array(29884.) >>> np.sum(a[2,3,2,:] * b[:,2]) array(29884.)

d2l.mxnet.meshgrid(*xi, **kwargs) Return coordinate matrices from coordinate vectors.
Make N-D coordinate arrays for vectorized evaluations of N-D scalar/vector fields over N-D grids, given one-dimensional coordinate arrays x1, x2,…, xn.

16.7. d2l API Document

797

x1, x2,…, xn [ndarrays] 1-D arrays representing the coordinates of a grid.
indexing [{ʻxyʼ, ʻijʼ}, optional] Cartesian (ʻxyʼ, default) or matrix (ʻijʼ) indexing of output. See Notes for more details.
sparse [bool, optional] If True a sparse grid is returned in order to conserve memory. Default is False. Please note that sparse=True is currently not supported.
copy [bool, optional] If False, a view into the original arrays are returned in order to conserve memory. Default is True. Please note that copy=False is currently not supported.
X1, X2,…, XN [ndarray] For vectors x1, x2,…, ʻxnʼ with lengths Ni=len(xi) , return (N1, N2, N3,...Nn) shaped arrays if indexing=ʼijʼ or (N2, N1, N3,...Nn) shaped arrays if indexing=ʼxyʼ with the elements of xi repeated to fill the matrix along the first dimension for x1, the second for x2 and so on.
This function supports both indexing conventions through the indexing keyword argument. Giving the string ʻijʼ returns a meshgrid with matrix indexing, while ʻxyʼ returns a meshgrid with Cartesian indexing. In the 2-D case with inputs of length M and N, the outputs are of shape (N, M) for ʻxyʼ indexing and (M, N) for ʻijʼ indexing. In the 3-D case with inputs of length M, N and P, outputs are of shape (N, M, P) for ʻxyʼ indexing and (M, N, P) for ʻijʼ indexing. The difference is illustrated by the following code snippet:
xv, yv = np.meshgrid(x, y, sparse=False, indexing='ij') for i in range(nx):
for j in range(ny): # treat xv[i,j], yv[i,j]
xv, yv = np.meshgrid(x, y, sparse=False, indexing='xy') for i in range(nx):
for j in range(ny): # treat xv[j,i], yv[j,i]

In the 1-D and 0-D case, the indexing and sparse keywords have no effect.

d2l.mxnet.multibox_detection(cls_probs, offset_preds, anchors,
pos_threshold=0.009999999) Predict bounding boxes using non-maximum suppression.

nms_threshold=0.5,

d2l.mxnet.multibox_prior(data, sizes, ratios) Generate anchor boxes with different shapes centered on each pixel.

d2l.mxnet.multibox_target(anchors, labels) Label anchor boxes using ground-truth bounding boxes.

d2l.mxnet.nms(boxes, scores, iou_threshold) Sort confidence scores of predicted bounding boxes.

d2l.mxnet.normal(loc=0.0, scale=1.0, size=None, dtype=None, ctx=None, out=None) Draw random samples from a normal (Gaussian) distribution.

Samples are distributed according to a normal distribution parametrized by loc (mean) and scale (standard deviation).

loc [float, optional] Mean (centre) of the distribution.

scale [float, optional] Standard deviation (spread or “width”) of the distribution.

798

Chapter 16. Appendix: Tools for Deep Learning

size [int or tuple of ints, optional] Output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. If size is None (default), a scalar tensor containing a single value is returned if loc and scale are both scalars. Otherwise, np.broadcast(low, high).size samples are drawn.
dtype [{ʻfloat16ʼ, ʻfloat32ʼ, ʻfloat64ʼ}, optional] Data type of output samples. Default is ʻfloat32ʼ
ctx [Context, optional] Device context of output, default is current context.
out [ndarray, optional] Store output to an existing ndarray.

out [ndarray] Drawn samples from the parameterized normal distribution.

The probability density for the Gaussian distribution is

p(x) = √ 1 e , − (x2−σµ2)2 2πσ2

(16.7.1)

where µ is the mean and σ the standard deviation. The square of the standard deviation, σ2, is called the variance.

The function has its peak at the mean, and its “spread” increases with the standard deviation (the function reaches 0.607 times its maximum at x + σ and x − σ2). This implies that
numpy.random.normal is more likely to return samples lying close to the mean, rather than
those far away.

>>> mu, sigma = 0, 0.1 # mean and standard deviation >>> s = np.random.normal(mu, sigma, 1000)

Verify the mean and the variance:
>>> np.abs(mu - np.mean(s)) < 0.01 array(True)

d2l.mxnet.offset_boxes(anchors, assigned_bb, eps=1e-06) Transform for anchor box offsets.
d2l.mxnet.offset_inverse(anchors, offset_preds) Predict bounding boxes based on anchor boxes with predicted offsets.
d2l.mxnet.ones(shape, dtype=<class 'numpy.float32'>, order='C', ctx=None) Return a new array of given shape and type, filled with ones. This function currently only supports storing multi-dimensional data in row-major (C-style).
shape [int or tuple of int] The shape of the empty array.
dtype [str or numpy.dtype, optional] An optional value type. Default is numpy.float32. Note that this behavior is different from NumPyʼs ones function where float64 is the default value, because float32 is considered as the default data type in deep learning.
order [{ʻCʼ}, optional, default: ʻCʼ] How to store multi-dimensional data in memory, currently only row-major (C-style) is supported.
ctx [Context, optional] An optional device context (default is the current default context).
2 P. R. Peebles Jr., “Central Limit Theorem” in “Probability, Random Variables and Random Signal Principles”, 4th ed., 2001, pp. 51, 51, 125.

16.7. d2l API Document

799

out [ndarray] Array of ones with the given shape, dtype, and ctx.
>>> np.ones(5) array([1., 1., 1., 1., 1.])

>>> np.ones((5,), dtype=int) array([1, 1, 1, 1, 1], dtype=int64)

>>> np.ones((2, 1)) array([[1.],
[1.]])

>>> s = (2,2) >>> np.ones(s) array([[1., 1.],
[1., 1.]])

d2l.mxnet.plot(X, Y=None, xlabel=None, ylabel=None, legend=None, xlim=None, ylim=None,
xscale='linear', yscale='linear', fmts=('-', 'm--', 'g-.', 'r:'), figsize=(3.5, 2.5), axes=None) Plot data points.

d2l.mxnet.predict_ch3(net, test_iter, n=6) Predict labels (defined in Chapter 3).

d2l.mxnet.predict_ch8(prefix, num_preds, net, vocab, device) Generate new characters following the prefix.

d2l.mxnet.predict_sentiment(net, vocab, sequence) Predict the sentiment of a text sequence.

d2l.mxnet.predict_seq2seq(net, src_sentence, src_vocab,
save_attention_weights=False) Predict for sequence to sequence.

tgt_vocab,

num_steps,

device,

d2l.mxnet.predict_snli(net, vocab, premise, hypothesis) Predict the logical relationship between the premise and hypothesis.

d2l.mxnet.preprocess_nmt(text) Preprocess the English-French dataset.

d2l.mxnet.rand(*size, **kwargs) Random values in a given shape.

Create an array of the given shape and populate it with random samples from a uniform distribution over [0, 1). Parameters ———- d0, d1, …, dn : int, optional

The dimensions of the returned array, should be all positive. If no argument is given a single Python float is returned.

out [ndarray] Random values.

800

Chapter 16. Appendix: Tools for Deep Learning

>>> np.random.rand(3,2) array([[ 0.14022471, 0.96360618], #random
[ 0.37601032, 0.25528411], #random [ 0.49313049, 0.94909878]]) #random
d2l.mxnet.read_csv_labels(fname) Read fname to return a filename to label dictionary.
d2l.mxnet.read_data_bananas(is_train=True) Read the banana detection dataset images and labels.
d2l.mxnet.read_data_nmt() Load the English-French dataset.
d2l.mxnet.read_imdb(data_dir, is_train) Read the IMDb review dataset text sequences and labels.
d2l.mxnet.read_ptb() Load the PTB dataset into a list of text lines.
d2l.mxnet.read_snli(data_dir, is_train) Read the SNLI dataset into premises, hypotheses, and labels.
d2l.mxnet.read_time_machine() Load the time machine dataset into a list of text lines.
d2l.mxnet.read_voc_images(voc_dir, is_train=True) Read all VOC feature and label images.
d2l.mxnet.reorg_test(data_dir) Organize the testing set for data loading during prediction.
d2l.mxnet.reorg_train_valid(data_dir, labels, valid_ratio) Split the validation set out of the original training set.
d2l.mxnet.resnet18(num_classes) A slightly modified ResNet-18 model.
d2l.mxnet.seq_data_iter_random(corpus, batch_size, num_steps) Generate a minibatch of subsequences using random sampling.
d2l.mxnet.seq_data_iter_sequential(corpus, batch_size, num_steps) Generate a minibatch of subsequences using sequential partitioning.
d2l.mxnet.set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend) Set the axes for matplotlib.
d2l.mxnet.set_figsize(figsize=(3.5, 2.5)) Set the figure size for matplotlib.
d2l.mxnet.sgd(params, lr, batch_size) Minibatch stochastic gradient descent.
d2l.mxnet.show_bboxes(axes, bboxes, labels=None, colors=None) Show bounding boxes.
d2l.mxnet.show_heatmaps(matrices, xlabel, ylabel, titles=None, figsize=(2.5, 2.5), cmap='Reds') Show heatmaps of matrices.

16.7. d2l API Document

801

d2l.mxnet.show_images(imgs, num_rows, num_cols, titles=None, scale=1.5) Plot a list of images.
d2l.mxnet.show_list_len_pair_hist(legend, xlabel, ylabel, xlist, ylist) Plot the histogram for list length pairs.
d2l.mxnet.show_trace_2d(f, results) Show the trace of 2D variables during optimization.
d2l.mxnet.sin(x, out=None, **kwargs) Trigonometric sine, element-wise.
x [ndarray or scalar] Angle, in radians (2π rad equals 360 degrees).
out [ndarray or None] A location into which the result is stored. If provided, it must have a shape that the inputs broadcast to. If not provided or None, a freshly-allocated array is returned. The dtype of the output is the same as that of the input if the input is an ndarray.

y [ndarray or scalar] The sine of each element of x. This is a scalar if x is a scalar.

This function only supports input type of float.

>>> np.sin(np.pi/2.)

1.0

>>> np.sin(np.array((0., 30., 45., 60., 90.)) * np.pi / 180.)

array([0.

, 0.5

, 0.70710677, 0.86602545, 1.

])

d2l.mxnet.sinh(x, out=None, **kwargs) Hyperbolic sine, element-wise. Equivalent to 1/2 * (np.exp(x) - np.exp(-x)) or -1j * np.sin(1j*x).
x [ndarray or scalar] Input array or scalar.
out [ndarray or None] A location into which the result is stored. If provided, it must have a shape that the inputs broadcast to. If not provided or None, a freshly-allocated array is returned. The dtype of the output is the same as that of the input if the input is an ndarray.

y [ndarray or scalar] The corresponding hyperbolic sine values. This is a scalar if x is a scalar.

This function only supports input type of float.

>>> np.sinh(0) 0.0 >>> # Example of providing the optional output >>> out1 = np.array([0], dtype='f') >>> out2 = np.sinh(np.array([0.1]), out1) >>> out2 is out1 True

parameter

d2l.mxnet.split_batch(X, y, devices) Split X and y into multiple devices.

802

Chapter 16. Appendix: Tools for Deep Learning

d2l.mxnet.split_batch_multi_inputs(X, y, devices) Split multi-input X and y into multiple devices.

d2l.mxnet.split_data_ml100k(data, num_users, num_items,
test_ratio=0.1) Split the dataset in random mode or seq-aware mode.

split_mode='random',

d2l.mxnet.squared_loss(y_hat, y) Squared loss.

d2l.mxnet.stack(arrays, axis=0, out=None)

Join a sequence of arrays along a new axis. The axis parameter specifies the index of the new axis in the dimensions of the result. For example, if axis=0 it will be the first dimension and if axis=-1 it will be the last dimension.

arrays [sequence of array_like] Each array must have the same shape.
axis [int, optional] The axis in the result array along which the input arrays are stacked.
out [ndarray, optional] If provided, the destination to place the result. The shape must be correct, matching that of what stack would have returned if no out argument were specified.

stacked [ndarray] The stacked array has one more dimension than the input arrays.

concatenate : Join a sequence of arrays along an existing axis. split : Split array into a list of multiple sub-arrays of equal size.
>>> arrays = [np.random.rand(3, 4) for _ in range(10)] >>> np.stack(arrays, axis=0).shape (10, 3, 4)

>>> np.stack(arrays, axis=1).shape (3, 10, 4)

>>> np.stack(arrays, axis=2).shape (3, 4, 10)

>>> a = np.array([1, 2, 3]) >>> b = np.array([2, 3, 4]) >>> np.stack((a, b)) array([[1., 2., 3.],
[2., 3., 4.]])

>>> np.stack((a, b), axis=-1) array([[1., 2.],
[2., 3.], [3., 4.]])
d2l.mxnet.subsample(sentences, vocab) Subsample high-frequency words.

16.7. d2l API Document

803

d2l.mxnet.synthetic_data(w, b, num_examples) Generate y = Xw + b + noise.
d2l.mxnet.tanh(x, out=None, **kwargs) Compute hyperbolic tangent element-wise. Equivalent to np.sinh(x)/np.cosh(x).
x [ndarray or scalar.] Input array.
out [ndarray or None] A location into which the result is stored. If provided, it must have a shape that the inputs fill into. If not provided or None, a freshly-allocated array is returned. The dtype of the output and input must be the same.

y [ndarray or scalar] The corresponding hyperbolic tangent values.

If out is provided, the function writes the result into it, and returns a reference to out. (See Examples) - input x does not support complex computation (like imaginary number) >>> np.tanh(np.pi*1j) TypeError: type <type ʻcomplexʼ> not supported

>>> np.tanh(np.array[0, np.pi]))

array([0.

, 0.9962721])

>>> np.tanh(np.pi)

0.99627207622075

>>> # Example of providing the optional output parameter illustrating

>>> # that what is returned is a reference to said parameter

>>> out1 = np.array(1)

>>> out2 = np.tanh(np.array(0.1), out1)

>>> out2 is out1

True

d2l.mxnet.tensor(object, dtype=None, ctx=None) Create an array.
object [array_like or numpy.ndarray or mxnet.numpy.ndarray] An array, any object exposing the array interface, an object whose __array__ method returns an array, or any (nested) sequence.
dtype [data-type, optional] The desired data-type for the array. Default is float32.
ctx [device context, optional] Device context on which the memory is allocated. Default is mxnet.context.current_context().
out [ndarray] An array object satisfying the specified requirements.

>>> np.array([1, 2, 3]) array([1., 2., 3.])

>>> np.array([[1, 2], [3, 4]]) array([[1., 2.],
[3., 4.]])

>>> np.array([[1, 0], [0, 1]], dtype=bool) array([[ True, False],
[False, True]])

804

Chapter 16. Appendix: Tools for Deep Learning

d2l.mxnet.tokenize(lines, token='word') Split text lines into word or character tokens.
d2l.mxnet.tokenize_nmt(text, num_examples=None) Tokenize the English-French dataset.
d2l.mxnet.train_2d(trainer, steps=20, f_grad=None) Optimize a 2D objective function with a customized trainer.
d2l.mxnet.train_batch_ch13(net, features, labels, loss, trainer, devices, split_f=<function split_batch>)
Train for a minibatch with mutiple GPUs (defined in Chapter 13).
d2l.mxnet.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs, devices=[gpu(0), gpu(1), gpu(2), gpu(3)], split_f=<function split_batch>)
Train a model with mutiple GPUs (defined in Chapter 13).
d2l.mxnet.train_ch3(net, train_iter, test_iter, loss, num_epochs, updater) Train a model (defined in Chapter 3).
d2l.mxnet.train_ch6(net, train_iter, test_iter, num_epochs, lr, device) Train a model with a GPU (defined in Chapter 6).
d2l.mxnet.train_ch8(net, train_iter, vocab, lr, num_epochs, device, use_random_iter=False) Train a model (defined in Chapter 8).
d2l.mxnet.train_epoch_ch3(net, train_iter, loss, updater) Train a model within one epoch (defined in Chapter 3).
d2l.mxnet.train_epoch_ch8(net, train_iter, loss, updater, device, use_random_iter) Train a model within one epoch (defined in Chapter 8).
d2l.mxnet.train_seq2seq(net, data_iter, lr, num_epochs, tgt_vocab, device) Train a model for sequence to sequence.
d2l.mxnet.transpose_output(X, num_heads) Reverse the operation of transpose_qkv.
d2l.mxnet.transpose_qkv(X, num_heads) Transposition for parallel computation of multiple attention heads.
d2l.mxnet.truncate_pad(line, num_steps, padding_token) Truncate or pad sequences.
d2l.mxnet.try_all_gpus() Return all available GPUs, or [cpu()] if no GPU exists.
d2l.mxnet.try_gpu(i=0) Return gpu(i) if exists, otherwise return cpu().
d2l.mxnet.update_D(X, Z, net_D, net_G, loss, trainer_D) Update discriminator.
d2l.mxnet.update_G(Z, net_D, net_G, loss, trainer_G) Update generator.
d2l.mxnet.use_svg_display() Use the svg format to display a plot in Jupyter.
d2l.mxnet.voc_colormap2label() Build the mapping from RGB to class indices for VOC labels.

16.7. d2l API Document

805

d2l.mxnet.voc_label_indices(colormap, colormap2label) Map any RGB values in VOC labels to their class indices.
d2l.mxnet.voc_rand_crop(feature, label, height, width) Randomly crop both feature and label images.
d2l.mxnet.zeros(shape, dtype=None, order='C', ctx=None) Return a new array of given shape and type, filled with zeros. This function currently only supports storing multi-dimensional data in row-major (C-style). shape [int or tuple of int] The shape of the empty array. dtype [str or numpy.dtype, optional] An optional value type (default is numpy.float32). Note that this behavior is different from NumPyʼs zeros function where float64 is the default value, because float32 is considered as the default data type in deep learning. order [{ʻCʼ}, optional, default: ʻCʼ] How to store multi-dimensional data in memory, currently only row-major (C-style) is supported. ctx [Context, optional] An optional device context (default is the current default context).
out [ndarray] Array of zeros with the given shape, dtype, and ctx.
>>> np.zeros(5) array([0., 0., 0., 0., 0.])
>>> np.zeros((5,), dtype=int) array([0, 0, 0, 0, 0], dtype=int64)
>>> np.zeros((2, 1)) array([[0.],
[0.]])

806

Chapter 16. Appendix: Tools for Deep Learning

Bibliography
Ahmed, A., Aly, M., Gonzalez, J., Narayanamurthy, S., & Smola, A. J. (2012). Scalable inference in latent variable models. Proceedings of the fifth ACM international conference on Web search and data mining (pp. 123–132).
Aji, S. M., & McEliece, R. J. (2000). The generalized distributive law. IEEE transactions on Information Theory, 46(2), 325–343.
Ba, J. L., Kiros, J. R., & Hinton, G. E. (2016). Layer normalization. arXiv preprint arXiv:1607.06450. Bahdanau, D., Cho, K., & Bengio, Y. (2014). Neural machine translation by jointly learning to align
and translate. arXiv preprint arXiv:1409.0473. Bay, H., Tuytelaars, T., & Van Gool, L. (2006). Surf: speeded up robust features. European conference
on computer vision (pp. 404–417). Bengio, Y., Ducharme, R., Vincent, P., & Jauvin, C. (2003). A neural probabilistic language model.
Journal of machine learning research, 3(Feb), 1137–1155. Bishop, C. M. (1995). Training with noise is equivalent to tikhonov regularization. Neural compu-
tation, 7(1), 108–116. Bishop, C. M. (2006). Pattern recognition and machine learning. springer. Bodla, N., Singh, B., Chellappa, R., & Davis, L. S. (2017). Soft-nms–improving object detection with
one line of code. Proceedings of the IEEE international conference on computer vision (pp. 5561– 5569). Bojanowski, P., Grave, E., Joulin, A., & Mikolov, T. (2017). Enriching word vectors with subword information. Transactions of the Association for Computational Linguistics, 5, 135–146. Bollobás, B. (1999). Linear analysis. Cambridge University Press, Cambridge. Bowman, S. R., Angeli, G., Potts, C., & Manning, C. D. (2015). A large annotated corpus for learning natural language inference. arXiv preprint arXiv:1508.05326. Boyd, S., & Vandenberghe, L. (2004). Convex Optimization. Cambridge, England: Cambridge University Press. Brown, N., & Sandholm, T. (2017). Libratus: the superhuman ai for no-limit poker. IJCAI (pp. 5226– 5228). Brown, P. F., Cocke, J., Della Pietra, S. A., Della Pietra, V. J., Jelinek, F., Lafferty, J., … Roossin, P. S. (1990). A statistical approach to machine translation. Computational linguistics, 16(2), 79–85. Brown, P. F., Cocke, J., Della Pietra, S. A., Della Pietra, V. J., Jelinek, F., Mercer, R. L., & Roossin, P. (1988). A statistical approach to language translation. Coling Budapest 1988 Volume 1: International Conference on Computational Linguistics.
807

Campbell, M., Hoane Jr, A. J., & Hsu, F.-h. (2002). Deep blue. Artificial intelligence, 134(1-2), 57–83.
Canny, J. (1987). A computational approach to edge detection. Readings in computer vision (pp. 184– 203). Elsevier.
Cer, D., Diab, M., Agirre, E., Lopez-Gazpio, I., & Specia, L. (2017). Semeval-2017 task 1: semantic textual similarity multilingual and crosslingual focused evaluation. Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017) (pp. 1–14).
Cheng, J., Dong, L., & Lapata, M. (2016). Long short-term memory-networks for machine reading. Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (pp. 551– 561).
Cho, K., Van Merriënboer, B., Bahdanau, D., & Bengio, Y. (2014). On the properties of neural machine translation: encoder-decoder approaches. arXiv preprint arXiv:1409.1259.
Cho, K., Van Merriënboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H., & Bengio, Y. (2014). Learning phrase representations using rnn encoder-decoder for statistical machine translation. arXiv preprint arXiv:1406.1078.
Chung, J., Gulcehre, C., Cho, K., & Bengio, Y. (2014). Empirical evaluation of gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555.
Collobert, R., Weston, J., Bottou, L., Karlen, M., Kavukcuoglu, K., & Kuksa, P. (2011). Natural language processing (almost) from scratch. Journal of machine learning research, 12(ARTICLE), 2493–2537.
Csiszár, I. (2008). Axiomatic characterizations of information measures. Entropy, 10(3), 261–273.
Dalal, N., & Triggs, B. (2005). Histograms of oriented gradients for human detection. 2005 IEEE computer society conference on computer vision and pattern recognition (CVPR’05) (pp. 886–893).
De Cock, D. (2011). Ames, iowa: alternative to the boston housing data as an end of semester regression project. Journal of Statistics Education, 19(3).
DeCandia, G., Hastorun, D., Jampani, M., Kakulapati, G., Lakshman, A., Pilchin, A., … Vogels, W. (2007). Dynamo: amazonʼs highly available key-value store. ACM SIGOPS operating systems review (pp. 205–220).
Devlin, J., Chang, M.-W., Lee, K., & Toutanova, K. (2018). Bert: pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.
Doersch, C., Gupta, A., & Efros, A. A. (2015). Unsupervised visual representation learning by context prediction. Proceedings of the IEEE international conference on computer vision (pp. 1422–1430).
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., … others. (2021). An image is worth 16x16 words: transformers for image recognition at scale. International Conference on Learning Representations.
Doucet, A., De Freitas, N., & Gordon, N. (2001). An introduction to sequential monte carlo methods. Sequential Monte Carlo methods in practice (pp. 3–14). Springer.
Duchi, J., Hazan, E., & Singer, Y. (2011). Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12(Jul), 2121–2159.
Dumoulin, V., & Visin, F. (2016). A guide to convolution arithmetic for deep learning. arXiv preprint arXiv:1603.07285.
Flammarion, N., & Bach, F. (2015). From averaging to acceleration, there is only a step-size. Conference on Learning Theory (pp. 658–695).

808

Bibliography

Gatys, L. A., Ecker, A. S., & Bethge, M. (2016). Image style transfer using convolutional neural networks. Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 2414– 2423).
Ginibre, J. (1965). Statistical ensembles of complex, quaternion, and real matrices. Journal of Mathematical Physics, 6(3), 440–449.
Girshick, R. (2015). Fast r-cnn. Proceedings of the IEEE international conference on computer vision (pp. 1440–1448).
Girshick, R., Donahue, J., Darrell, T., & Malik, J. (2014). Rich feature hierarchies for accurate object detection and semantic segmentation. Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 580–587).
Glorot, X., & Bengio, Y. (2010). Understanding the difficulty of training deep feedforward neural networks. Proceedings of the thirteenth international conference on artificial intelligence and statistics (pp. 249–256).
Goh, G. (2017). Why momentum really works. Distill. URL: http://distill.pub/2017/momentum, doi:10.23915/distill.00006247
Goldberg, D., Nichols, D., Oki, B. M., & Terry, D. (1992). Using collaborative filtering to weave an information tapestry. Communications of the ACM, 35(12), 61–71.
Goodfellow, I., Bengio, Y., & Courville, A. (2016). Deep Learning. MIT Press. http://www.deeplearningbook.org.
Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., … Bengio, Y. (2014). Generative adversarial nets. Advances in neural information processing systems (pp. 2672–2680).
Gotmare, A., Keskar, N. S., Xiong, C., & Socher, R. (2018). A closer look at deep learning heuristics: learning rate restarts, warmup and distillation. arXiv preprint arXiv:1810.13243.
Graves, A. (2013). Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850.
Graves, A., & Schmidhuber, J. (2005). Framewise phoneme classification with bidirectional lstm and other neural network architectures. Neural networks, 18(5-6), 602–610.
Gunawardana, A., & Shani, G. (2015). Evaluating recommender systems. Recommender systems handbook (pp. 265–308). Springer.
Guo, H., Tang, R., Ye, Y., Li, Z., & He, X. (2017). Deepfm: a factorization-machine based neural network for ctr prediction. Proceedings of the 26th International Joint Conference on Artificial Intelligence (pp. 1725–1731).
Hadjis, S., Zhang, C., Mitliagkas, I., Iter, D., & Ré, C. (2016). Omnivore: an optimizer for multidevice deep learning on cpus and gpus. arXiv preprint arXiv:1606.04487.
He, K., Gkioxari, G., Dollár, P., & Girshick, R. (2017). Mask r-cnn. Proceedings of the IEEE international conference on computer vision (pp. 2961–2969).
He, K., Zhang, X., Ren, S., & Sun, J. (2015). Delving deep into rectifiers: surpassing human-level performance on imagenet classification. Proceedings of the IEEE international conference on computer vision (pp. 1026–1034).
He, K., Zhang, X., Ren, S., & Sun, J. (2016). Deep residual learning for image recognition. Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 770–778).
247 https://doi.org/10.23915/distill.00006

Bibliography

809

He, K., Zhang, X., Ren, S., & Sun, J. (2016). Identity mappings in deep residual networks. European conference on computer vision (pp. 630–645).
He, X., & Chua, T.-S. (2017). Neural factorization machines for sparse predictive analytics. Proceedings of the 40th International ACM SIGIR conference on Research and Development in Information Retrieval (pp. 355–364).
He, X., Liao, L., Zhang, H., Nie, L., Hu, X., & Chua, T.-S. (2017). Neural collaborative filtering. Proceedings of the 26th international conference on world wide web (pp. 173–182).
Hebb, D. O., & Hebb, D. (1949). The organization of behavior. Vol. 65. Wiley New York.
Hendrycks, D., & Gimpel, K. (2016). Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415.
Hennessy, J. L., & Patterson, D. A. (2011). Computer architecture: a quantitative approach. Elsevier.
Herlocker, J. L., Konstan, J. A., Borchers, A., & Riedl, J. (1999). An algorithmic framework for performing collaborative filtering. 22nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 1999 (pp. 230–237).
Hidasi, B., Karatzoglou, A., Baltrunas, L., & Tikk, D. (2015). Session-based recommendations with recurrent neural networks. arXiv preprint arXiv:1511.06939.
Hochreiter, S., Bengio, Y., Frasconi, P., Schmidhuber, J., & others (2001). Gradient flow in recurrent nets: the difficulty of learning long-term dependencies.
Hochreiter, S., & Schmidhuber, J. (1997). Long short-term memory. Neural computation, 9(8), 1735– 1780.
Hoyer, P. O., Janzing, D., Mooij, J. M., Peters, J., & Schölkopf, B. (2009). Nonlinear causal discovery with additive noise models. Advances in neural information processing systems (pp. 689–696).
Hu, J., Shen, L., & Sun, G. (2018). Squeeze-and-excitation networks. Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 7132–7141).
Hu, Y., Koren, Y., & Volinsky, C. (2008). Collaborative filtering for implicit feedback datasets. 2008 Eighth IEEE International Conference on Data Mining (pp. 263–272).
Hu, Z., Lee, R. K.-W., Aggarwal, C. C., & Zhang, A. (2020). Text style transfer: a review and experimental evaluation. arXiv preprint arXiv:2010.12742.
Huang, G., Liu, Z., Van Der Maaten, L., & Weinberger, K. Q. (2017). Densely connected convolutional networks. Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 4700–4708).
Ioffe, S. (2017). Batch renormalization: towards reducing minibatch dependence in batchnormalized models. Advances in neural information processing systems (pp. 1945–1953).
Ioffe, S., & Szegedy, C. (2015). Batch normalization: accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167.
Izmailov, P., Podoprikhin, D., Garipov, T., Vetrov, D., & Wilson, A. G. (2018). Averaging weights leads to wider optima and better generalization. arXiv preprint arXiv:1803.05407.
Jaeger, H. (2002). Tutorial on training recurrent neural networks, covering BPPT, RTRL, EKF and the” echo state network” approach. Vol. 5. GMD-Forschungszentrum Informationstechnik Bonn.
James, W. (2007). The principles of psychology. Vol. 1. Cosimo, Inc.

810

Bibliography

Jia, X., Song, S., He, W., Wang, Y., Rong, H., Zhou, F., … others. (2018). Highly scalable deep learning training system with mixed-precision: training imagenet in four minutes. arXiv preprint arXiv:1807.11205.
Jouppi, N. P., Young, C., Patil, N., Patterson, D., Agrawal, G., Bajwa, R., … others. (2017). Indatacenter performance analysis of a tensor processing unit. 2017 ACM/IEEE 44th Annual International Symposium on Computer Architecture (ISCA) (pp. 1–12).
Karras, T., Aila, T., Laine, S., & Lehtinen, J. (2017). Progressive growing of gans for improved quality, stability, and variation. arXiv preprint arXiv:1710.10196.
Kim, Y. (2014). Convolutional neural networks for sentence classification. arXiv preprint arXiv:1408.5882.
Kingma, D. P., & Ba, J. (2014). Adam: a method for stochastic optimization. arXiv preprint arXiv:1412.6980.
Koller, D., & Friedman, N. (2009). Probabilistic graphical models: principles and techniques. MIT press.
Kolter, Z. (2008). Linear algebra review and reference. Available online: http.
Koren, Y. (2009). Collaborative filtering with temporal dynamics. Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining (pp. 447–456).
Koren, Y., Bell, R., & Volinsky, C. (2009). Matrix factorization techniques for recommender systems. Computer, pp. 30–37.
Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2012). Imagenet classification with deep convolutional neural networks. Advances in neural information processing systems (pp. 1097–1105).
Kung, S. Y. (1988). Vlsi array processors. Englewood Cliffs, NJ, Prentice Hall, 1988, 685 p. Research supported by the Semiconductor Research Corp., SDIO, NSF, and US Navy.
LeCun, Y., Bottou, L., Bengio, Y., Haffner, P., & others. (1998). Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11), 2278–2324.
Li, M. (2017). Scaling Distributed Machine Learning with System and Algorithm Co-design (Doctoral dissertation). PhD Thesis, CMU.
Li, M., Andersen, D. G., Park, J. W., Smola, A. J., Ahmed, A., Josifovski, V., … Su, B.-Y. (2014). Scaling distributed machine learning with the parameter server. 11th $\$USENIX$\$ Symposium on Operating Systems Design and Implementation ($\$OSDI$\$ 14) (pp. 583–598).
Lin, M., Chen, Q., & Yan, S. (2013). Network in network. arXiv preprint arXiv:1312.4400.
Lin, T.-Y., Goyal, P., Girshick, R., He, K., & Dollár, P. (2017). Focal loss for dense object detection. Proceedings of the IEEE international conference on computer vision (pp. 2980–2988).
Lin, Y., Lv, F., Zhu, S., Yang, M., Cour, T., Yu, K., … others. (2010). Imagenet classification: fast descriptor coding and large-scale svm training. Large scale visual recognition challenge.
Lin, Z., Feng, M., Santos, C. N. d., Yu, M., Xiang, B., Zhou, B., & Bengio, Y. (2017). A structured self-attentive sentence embedding. arXiv preprint arXiv:1703.03130.
Lipton, Z. C., & Steinhardt, J. (2018). Troubling trends in machine learning scholarship. arXiv preprint arXiv:1807.03341.
Liu, W., Anguelov, D., Erhan, D., Szegedy, C., Reed, S., Fu, C.-Y., & Berg, A. C. (2016). Ssd: single shot multibox detector. European conference on computer vision (pp. 21–37).

Bibliography

811

Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., … Stoyanov, V. (2019). Roberta: a robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692.
Long, J., Shelhamer, E., & Darrell, T. (2015). Fully convolutional networks for semantic segmentation. Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 3431–3440).
Loshchilov, I., & Hutter, F. (2016). Sgdr: stochastic gradient descent with warm restarts. arXiv preprint arXiv:1608.03983.
Lowe, D. G. (2004). Distinctive image features from scale-invariant keypoints. International journal of computer vision, 60(2), 91–110.
Luo, P., Wang, X., Shao, W., & Peng, Z. (2018). Towards understanding regularization in batch normalization. arXiv preprint.
Maas, A. L., Daly, R. E., Pham, P. T., Huang, D., Ng, A. Y., & Potts, C. (2011). Learning word vectors for sentiment analysis. Proceedings of the 49th annual meeting of the association for computational linguistics: Human language technologies-volume 1 (pp. 142–150).
McCann, B., Bradbury, J., Xiong, C., & Socher, R. (2017). Learned in translation: contextualized word vectors. Advances in Neural Information Processing Systems (pp. 6294–6305).
McCulloch, W. S., & Pitts, W. (1943). A logical calculus of the ideas immanent in nervous activity. The bulletin of mathematical biophysics, 5(4), 115–133.
McMahan, H. B., Holt, G., Sculley, D., Young, M., Ebner, D., Grady, J., … others. (2013). Ad click prediction: a view from the trenches. Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining (pp. 1222–1230).
Merity, S., Xiong, C., Bradbury, J., & Socher, R. (2016). Pointer sentinel mixture models. arXiv preprint arXiv:1609.07843.
Mikolov, T., Chen, K., Corrado, G., & Dean, J. (2013). Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.
Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., & Dean, J. (2013). Distributed representations of words and phrases and their compositionality. Advances in neural information processing systems (pp. 3111–3119).
Mirhoseini, A., Pham, H., Le, Q. V., Steiner, B., Larsen, R., Zhou, Y., … Dean, J. (2017). Device placement optimization with reinforcement learning. Proceedings of the 34th International Conference on Machine Learning-Volume 70 (pp. 2430–2439).
Mnih, V., Heess, N., Graves, A., & others. (2014). Recurrent models of visual attention. Advances in neural information processing systems (pp. 2204–2212).
Morey, R. D., Hoekstra, R., Rouder, J. N., Lee, M. D., & Wagenmakers, E.-J. (2016). The fallacy of placing confidence in confidence intervals. Psychonomic bulletin & review, 23(1), 103–123.
Nadaraya, E. A. (1964). On estimating regression. Theory of Probability & Its Applications, 9(1), 141– 142.
Nesterov, Y., & Vial, J.-P. (2000). Confidence level solutions for stochastic programming, Stochastic Programming E-Print Series.
Nesterov, Y. (2018). Lectures on convex optimization. Vol. 137. Springer.
Neyman, J. (1937). Outline of a theory of statistical estimation based on the classical theory of probability. Philosophical Transactions of the Royal Society of London. Series A, Mathematical and Physical Sciences, 236(767), 333–380.

812

Bibliography

Papineni, K., Roukos, S., Ward, T., & Zhu, W.-J. (2002). Bleu: a method for automatic evaluation of machine translation. Proceedings of the 40th annual meeting of the Association for Computational Linguistics (pp. 311–318).
Parikh, A. P., Täckström, O., Das, D., & Uszkoreit, J. (2016). A decomposable attention model for natural language inference. arXiv preprint arXiv:1606.01933.
Park, T., Liu, M.-Y., Wang, T.-C., & Zhu, J.-Y. (2019). Semantic image synthesis with spatiallyadaptive normalization. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 2337–2346).
Paulus, R., Xiong, C., & Socher, R. (2017). A deep reinforced model for abstractive summarization. arXiv preprint arXiv:1705.04304.
Pennington, J., Schoenholz, S., & Ganguli, S. (2017). Resurrecting the sigmoid in deep learning through dynamical isometry: theory and practice. Advances in neural information processing systems (pp. 4785–4795).
Pennington, J., Socher, R., & Manning, C. (2014). Glove: global vectors for word representation. Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP) (pp. 1532–1543).
Peters, J., Janzing, D., & Schölkopf, B. (2017). Elements of causal inference: foundations and learning algorithms. MIT press.
Peters, M., Ammar, W., Bhagavatula, C., & Power, R. (2017). Semi-supervised sequence tagging with bidirectional language models. Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) (pp. 1756–1765).
Peters, M., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., & Zettlemoyer, L. (2018). Deep contextualized word representations. Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers) (pp. 2227–2237).
Petersen, K. B., Pedersen, M. S., & others. (2008). The matrix cookbook. Technical University of Denmark, 7(15), 510.
Polyak, B. T. (1964). Some methods of speeding up the convergence of iteration methods. USSR Computational Mathematics and Mathematical Physics, 4(5), 1–17.
Quadrana, M., Cremonesi, P., & Jannach, D. (2018). Sequence-aware recommender systems. ACM Computing Surveys (CSUR), 51(4), 66.
Radford, A., Metz, L., & Chintala, S. (2015). Unsupervised representation learning with deep convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434.
Radford, A., Narasimhan, K., Salimans, T., & Sutskever, I. (2018). Improving language understanding by generative pre-training. OpenAI.
Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., & Sutskever, I. (2019). Language models are unsupervised multitask learners. OpenAI Blog, 1(8), 9.
Rajpurkar, P., Zhang, J., Lopyrev, K., & Liang, P. (2016). Squad: 100,000+ questions for machine comprehension of text. arXiv preprint arXiv:1606.05250.
Reddi, S. J., Kale, S., & Kumar, S. (2019). On the convergence of adam and beyond. arXiv preprint arXiv:1904.09237.

Bibliography

813

Redmon, J., Divvala, S., Girshick, R., & Farhadi, A. (2016). You only look once: unified, realtime object detection. Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 779–788).
Reed, S., & De Freitas, N. (2015). Neural programmer-interpreters. arXiv preprint arXiv:1511.06279.
Ren, S., He, K., Girshick, R., & Sun, J. (2015). Faster r-cnn: towards real-time object detection with region proposal networks. Advances in neural information processing systems (pp. 91–99).
Rendle, S. (2010). Factorization machines. 2010 IEEE International Conference on Data Mining (pp. 995–1000).
Rendle, S., Freudenthaler, C., Gantner, Z., & Schmidt-Thieme, L. (2009). Bpr: bayesian personalized ranking from implicit feedback. Proceedings of the twenty-fifth conference on uncertainty in artificial intelligence (pp. 452–461).
Rumelhart, D. E., Hinton, G. E., Williams, R. J., & others. (1988). Learning representations by back-propagating errors. Cognitive modeling, 5(3), 1.
Russell, S. J., & Norvig, P. (2016). Artificial intelligence: a modern approach. Malaysia; Pearson Education Limited,.
Santurkar, S., Tsipras, D., Ilyas, A., & Madry, A. (2018). How does batch normalization help optimization? Advances in Neural Information Processing Systems (pp. 2483–2493).
Sarwar, B. M., Karypis, G., Konstan, J. A., Riedl, J., & others. (2001). Item-based collaborative filtering recommendation algorithms. Www, 1, 285–295.
Schein, A. I., Popescul, A., Ungar, L. H., & Pennock, D. M. (2002). Methods and metrics for coldstart recommendations. Proceedings of the 25th annual international ACM SIGIR conference on Research and development in information retrieval (pp. 253–260).
Schuster, M., & Paliwal, K. K. (1997). Bidirectional recurrent neural networks. IEEE Transactions on Signal Processing, 45(11), 2673–2681.
Sedhain, S., Menon, A. K., Sanner, S., & Xie, L. (2015). Autorec: autoencoders meet collaborative filtering. Proceedings of the 24th International Conference on World Wide Web (pp. 111–112).
Sennrich, R., Haddow, B., & Birch, A. (2015). Neural machine translation of rare words with subword units. arXiv preprint arXiv:1508.07909.
Sergeev, A., & Del Balso, M. (2018). Horovod: fast and easy distributed deep learning in tensorflow. arXiv preprint arXiv:1802.05799.
Shannon, C. E. (1948 , 7). A mathematical theory of communication. The Bell System Technical Journal, 27(3), 379–423.
Shao, H., Yao, S., Sun, D., Zhang, A., Liu, S., Liu, D., … Abdelzaher, T. (2020). Controlvae: controllable variational autoencoder. Proceedings of the 37th International Conference on Machine Learning.
Silver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., Van Den Driessche, G., … others. (2016). Mastering the game of go with deep neural networks and tree search. nature, 529(7587), 484.
Simonyan, K., & Zisserman, A. (2014). Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556.
Smola, A., & Narayanamurthy, S. (2010). An architecture for parallel topic models. Proceedings of the VLDB Endowment, 3(1-2), 703–710.

814

Bibliography

Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., & Salakhutdinov, R. (2014). Dropout: a simple way to prevent neural networks from overfitting. The Journal of Machine Learning Research, 15(1), 1929–1958.
Strang, G. (1993). Introduction to linear algebra. Vol. 3. Wellesley-Cambridge Press Wellesley, MA.
Su, X., & Khoshgoftaar, T. M. (2009). A survey of collaborative filtering techniques. Advances in artificial intelligence, 2009.
Sukhbaatar, S., Weston, J., Fergus, R., & others. (2015). End-to-end memory networks. Advances in neural information processing systems (pp. 2440–2448).
Sutskever, I., Martens, J., Dahl, G., & Hinton, G. (2013). On the importance of initialization and momentum in deep learning. International conference on machine learning (pp. 1139–1147).
Sutskever, I., Vinyals, O., & Le, Q. V. (2014). Sequence to sequence learning with neural networks. Advances in neural information processing systems (pp. 3104–3112).
Szegedy, C., Ioffe, S., Vanhoucke, V., & Alemi, A. A. (2017). Inception-v4, inception-resnet and the impact of residual connections on learning. Thirty-First AAAI Conference on Artificial Intelligence.
Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., … Rabinovich, A. (2015). Going deeper with convolutions. Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 1–9).
Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., & Wojna, Z. (2016). Rethinking the inception architecture for computer vision. Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 2818–2826).
Tallec, C., & Ollivier, Y. (2017). Unbiasing truncated backpropagation through time. arXiv preprint arXiv:1705.08209.
Tang, J., & Wang, K. (2018). Personalized top-n sequential recommendation via convolutional sequence embedding. Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining (pp. 565–573).
Tay, Y., Dehghani, M., Bahri, D., & Metzler, D. (2020). Efficient transformers: a survey. arXiv preprint arXiv:2009.06732.
Teye, M., Azizpour, H., & Smith, K. (2018). Bayesian uncertainty estimation for batch normalized deep networks. arXiv preprint arXiv:1802.06455.
Tieleman, T., & Hinton, G. (2012). Lecture 6.5-rmsprop: divide the gradient by a running average of its recent magnitude. COURSERA: Neural networks for machine learning, 4(2), 26–31.
Turing, A. (1950). Computing machinery and intelligence. Mind, 59(236), 433.
Töscher, A., Jahrer, M., & Bell, R. M. (2009). The bigchaos solution to the netflix grand prize. Netflix prize documentation, pp. 1–52.
Uijlings, J. R., Van De Sande, K. E., Gevers, T., & Smeulders, A. W. (2013). Selective search for object recognition. International journal of computer vision, 104(2), 154–171.
Van Loan, C. F., & Golub, G. H. (1983). Matrix computations. Johns Hopkins University Press.
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., … Polosukhin, I. (2017). Attention is all you need. Advances in neural information processing systems (pp. 5998–6008).
Wang, L., Li, M., Liberty, E., & Smola, A. J. (2018). Optimal message scheduling for aggregation. NETWORKS, 2(3), 2–3.

Bibliography

815

Wang, Y., Davidson, A., Pan, Y., Wu, Y., Riffel, A., & Owens, J. D. (2016). Gunrock: a highperformance graph processing library on the gpu. ACM SIGPLAN Notices (p. 11).
Warstadt, A., Singh, A., & Bowman, S. R. (2019). Neural network acceptability judgments. Transactions of the Association for Computational Linguistics, 7, 625–641.
Wasserman, L. (2013). All of statistics: a concise course in statistical inference. Springer Science & Business Media.
Watkins, C. J., & Dayan, P. (1992). Q-learning. Machine learning, 8(3-4), 279–292.
Watson, G. S. (1964). Smooth regression analysis. Sankhyā: The Indian Journal of Statistics, Series A, pp. 359–372.
Welling, M., & Teh, Y. W. (2011). Bayesian learning via stochastic gradient langevin dynamics. Proceedings of the 28th international conference on machine learning (ICML-11) (pp. 681–688).
Werbos, P. J. (1990). Backpropagation through time: what it does and how to do it. Proceedings of the IEEE, 78(10), 1550–1560.
Wigner, E. P. (1958). On the distribution of the roots of certain symmetric matrices. Ann. Math (pp. 325–327).
Wood, F., Gasthaus, J., Archambeau, C., James, L., & Teh, Y. W. (2011). The sequence memoizer. Communications of the ACM, 54(2), 91–98.
Wu, C.-Y., Ahmed, A., Beutel, A., Smola, A. J., & Jing, H. (2017). Recurrent recommender networks. Proceedings of the tenth ACM international conference on web search and data mining (pp. 495–503).
Wu, Y., Schuster, M., Chen, Z., Le, Q. V., Norouzi, M., Macherey, W., … others. (2016). Googleʼs neural machine translation system: bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144.
Xiao, H., Rasul, K., & Vollgraf, R. (2017). Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms. arXiv preprint arXiv:1708.07747.
Xiao, L., Bahri, Y., Sohl-Dickstein, J., Schoenholz, S., & Pennington, J. (2018). Dynamical isometry and a mean field theory of cnns: how to train 10,000-layer vanilla convolutional neural networks. International Conference on Machine Learning (pp. 5393–5402).
Xiong, W., Wu, L., Alleva, F., Droppo, J., Huang, X., & Stolcke, A. (2018). The microsoft 2017 conversational speech recognition system. 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) (pp. 5934–5938).
Ye, M., Yin, P., Lee, W.-C., & Lee, D.-L. (2011). Exploiting geographical influence for collaborative point-of-interest recommendation. Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval (pp. 325–334).
You, Y., Gitman, I., & Ginsburg, B. (2017). Large batch training of convolutional networks. arXiv preprint arXiv:1708.03888.
Zaheer, M., Reddi, S., Sachan, D., Kale, S., & Kumar, S. (2018). Adaptive methods for nonconvex optimization. Advances in Neural Information Processing Systems (pp. 9793–9803).
Zeiler, M. D. (2012). Adadelta: an adaptive learning rate method. arXiv preprint arXiv:1212.5701.
Zhang, A., Tay, Y., Zhang, S., Chan, A., Luu, A. T., Hui, S. C., & Fu, J. (2021). Beyond fullyconnected layers with quaternions: parameterization of hypercomplex multiplications with 1/n parameters. International Conference on Learning Representations.

816

Bibliography

Zhang, S., Yao, L., Sun, A., & Tay, Y. (2019). Deep learning based recommender system: a survey and new perspectives. ACM Computing Surveys (CSUR), 52(1), 5.
Zhao, Z.-Q., Zheng, P., Xu, S.-t., & Wu, X. (2019). Object detection with deep learning: a review. IEEE transactions on neural networks and learning systems, 30(11), 3212–3232.
Zhu, J.-Y., Park, T., Isola, P., & Efros, A. A. (2017). Unpaired image-to-image translation using cycle-consistent adversarial networks. Proceedings of the IEEE international conference on computer vision (pp. 2223–2232).
Zhu, Y., Kiros, R., Zemel, R., Salakhutdinov, R., Urtasun, R., Torralba, A., & Fidler, S. (2015). Aligning books and movies: towards story-like visual explanations by watching movies and reading books. Proceedings of the IEEE international conference on computer vision (pp. 19–27).

Bibliography

817

818

Bibliography

Python Module Index
d
d2l.mxnet, 785
819

820

Python Module Index

Index

A

abs() (in module d2l.mxnet), 789 Accumulator (class in d2l.mxnet), 785 accuracy() (in module d2l.mxnet), 789 AdditiveAttention (class in d2l.mxnet), 785 AddNorm (class in d2l.mxnet), 785 Animator (class in d2l.mxnet), 785 arange() (in module d2l.mxnet), 789 as_integer_ratio() (d2l.mxnet.float32 method),

794

assign_anchor_to_bbox()

(in

module

d2l.mxnet), 790 AttentionDecoder (class in d2l.mxnet), 785 avg() (d2l.mxnet.Timer method), 788

B
BananasDataset (class in d2l.mxnet), 786 batchify() (in module d2l.mxnet), 790 bbox_to_rect() (in module d2l.mxnet), 790 Benchmark (class in d2l.mxnet), 786 BERTEncoder (class in d2l.mxnet), 785 BERTModel (class in d2l.mxnet), 786 bleu() (in module d2l.mxnet), 790 box_center_to_corner() (in module d2l.mxnet),
790 box_corner_to_center() (in module d2l.mxnet),
790 box_iou() (in module d2l.mxnet), 790 BPRLoss (class in d2l.mxnet), 786 build_array_nmt() (in module d2l.mxnet), 791

C
concat() (in module d2l.mxnet), 791 copy() (d2l.mxnet.defaultdict method), 792 copyfile() (in module d2l.mxnet), 791 corr2d() (in module d2l.mxnet), 791 cos() (in module d2l.mxnet), 791 cosh() (in module d2l.mxnet), 792 count_corpus() (in module d2l.mxnet), 792 CTRDataset (class in d2l.mxnet), 786 cumsum() (d2l.mxnet.Timer method), 788

D
d2l.mxnet
module, 785 Decoder (class in d2l.mxnet), 786 default_factory (d2l.mxnet.defaultdict at-
tribute), 792 defaultdict (class in d2l.mxnet), 792 DotProductAttention (class in d2l.mxnet), 786 download() (in module d2l.mxnet), 792 download_all() (in module d2l.mxnet), 792 download_extract() (in module d2l.mxnet), 792

E

Encoder (class in d2l.mxnet), 786

EncoderBlock (class in d2l.mxnet), 786

EncoderDecoder (class in d2l.mxnet), 787

evaluate_accuracy() (in module d2l.mxnet), 793

evaluate_accuracy_gpu()

(in

module

d2l.mxnet), 793 evaluate_accuracy_gpus() (in module

d2l.mxnet), 793 evaluate_loss() (in module d2l.mxnet), 793 exp() (in module d2l.mxnet), 793 eye() (in module d2l.mxnet), 793

F

filter() (d2l.mxnet.VOCSegDataset method), 789 float32 (class in d2l.mxnet), 793 forward() (d2l.mxnet.AdditiveAttention method),

785

forward() (d2l.mxnet.AddNorm method), 785

forward() (d2l.mxnet.BERTEncoder method), 785

forward() (d2l.mxnet.BERTModel method), 786

forward() (d2l.mxnet.BPRLoss method), 786

forward() (d2l.mxnet.Decoder method), 786

forward()

(d2l.mxnet.DotProductAttention

method), 786 forward() (d2l.mxnet.Encoder method), 786 forward() (d2l.mxnet.EncoderBlock method), 786 forward() (d2l.mxnet.EncoderDecoder method),

787

821

forward() (d2l.mxnet.HingeLossbRec method),

787

forward() (d2l.mxnet.MaskedSoftmaxCELoss

method), 787

forward() (d2l.mxnet.MaskLM method), 787

forward()

(d2l.mxnet.MultiHeadAttention

method), 787

forward() (d2l.mxnet.NextSentencePred method),

787

forward()

(d2l.mxnet.PositionalEncoding

method), 788

forward() (d2l.mxnet.PositionWiseFFN method),

787

forward() (d2l.mxnet.Residual method), 788

forward() (d2l.mxnet.RNNModel method), 788

forward() (d2l.mxnet.Seq2SeqEncoder method),

788

forward()

(d2l.mxnet.TransformerEncoder

method), 789

G
get_centers_and_contexts() (in module d2l.mxnet), 794
get_dataloader_workers() (in module d2l.mxnet), 794
get_fashion_mnist_labels() (in module d2l.mxnet), 794
get_negatives() (in module d2l.mxnet), 794 get_tokens_and_segments() (in module
d2l.mxnet), 794 grad_clipping() (in module d2l.mxnet), 794

H
HingeLossbRec (class in d2l.mxnet), 787

I
int32 (class in d2l.mxnet), 794

L
linreg() (in module d2l.mxnet), 794 linspace() (in module d2l.mxnet), 794 load_array() (in module d2l.mxnet), 795 load_corpus_time_machine() (in module
d2l.mxnet), 795 load_data_bananas() (in module d2l.mxnet), 795 load_data_fashion_mnist() (in module
d2l.mxnet), 795 load_data_imdb() (in module d2l.mxnet), 796 load_data_nmt() (in module d2l.mxnet), 796 load_data_ptb() (in module d2l.mxnet), 796 load_data_snli() (in module d2l.mxnet), 796

load_data_time_machine() (in module d2l.mxnet), 796
load_data_voc() (in module d2l.mxnet), 796 load_data_wiki() (in module d2l.mxnet), 796 log() (in module d2l.mxnet), 796
M
masked_softmax() (in module d2l.mxnet), 796 MaskedSoftmaxCELoss (class in d2l.mxnet), 787 MaskLM (class in d2l.mxnet), 787 matmul() (in module d2l.mxnet), 797 meshgrid() (in module d2l.mxnet), 797 module
d2l.mxnet, 785 multibox_detection() (in module d2l.mxnet),
798 multibox_prior() (in module d2l.mxnet), 798 multibox_target() (in module d2l.mxnet), 798 MultiHeadAttention (class in d2l.mxnet), 787
N
NextSentencePred (class in d2l.mxnet), 787 nms() (in module d2l.mxnet), 798 normal() (in module d2l.mxnet), 798
O
offset_boxes() (in module d2l.mxnet), 799 offset_inverse() (in module d2l.mxnet), 799 ones() (in module d2l.mxnet), 799
P
plot() (in module d2l.mxnet), 800 PositionalEncoding (class in d2l.mxnet), 787 PositionWiseFFN (class in d2l.mxnet), 787 predict_ch3() (in module d2l.mxnet), 800 predict_ch8() (in module d2l.mxnet), 800 predict_sentiment() (in module d2l.mxnet), 800 predict_seq2seq() (in module d2l.mxnet), 800 predict_snli() (in module d2l.mxnet), 800 preprocess_nmt() (in module d2l.mxnet), 800
R
rand() (in module d2l.mxnet), 800 RandomGenerator (class in d2l.mxnet), 788 read_csv_labels() (in module d2l.mxnet), 801 read_data_bananas() (in module d2l.mxnet), 801 read_data_nmt() (in module d2l.mxnet), 801 read_imdb() (in module d2l.mxnet), 801 read_ptb() (in module d2l.mxnet), 801 read_snli() (in module d2l.mxnet), 801 read_time_machine() (in module d2l.mxnet), 801

822

Index

read_voc_images() (in module d2l.mxnet), 801 reorg_test() (in module d2l.mxnet), 801 reorg_train_valid() (in module d2l.mxnet), 801 Residual (class in d2l.mxnet), 788 resnet18() (in module d2l.mxnet), 801 RNNModel (class in d2l.mxnet), 788 RNNModelScratch (class in d2l.mxnet), 788
S
Seq2SeqEncoder (class in d2l.mxnet), 788 seq_data_iter_random() (in module d2l.mxnet),
801 seq_data_iter_sequential() (in module
d2l.mxnet), 801 SeqDataLoader (class in d2l.mxnet), 788 set_axes() (in module d2l.mxnet), 801 set_figsize() (in module d2l.mxnet), 801 sgd() (in module d2l.mxnet), 801 show_bboxes() (in module d2l.mxnet), 801 show_heatmaps() (in module d2l.mxnet), 801 show_images() (in module d2l.mxnet), 801 show_list_len_pair_hist() (in module
d2l.mxnet), 802 show_trace_2d() (in module d2l.mxnet), 802 sin() (in module d2l.mxnet), 802 sinh() (in module d2l.mxnet), 802 SNLIDataset (class in d2l.mxnet), 788 split_batch() (in module d2l.mxnet), 802 split_batch_multi_inputs() (in module
d2l.mxnet), 802 split_data_ml100k() (in module d2l.mxnet), 803 squared_loss() (in module d2l.mxnet), 803 stack() (in module d2l.mxnet), 803 start() (d2l.mxnet.Timer method), 788 stop() (d2l.mxnet.Timer method), 788 subsample() (in module d2l.mxnet), 803 sum() (d2l.mxnet.Timer method), 789 synthetic_data() (in module d2l.mxnet), 803
T
tanh() (in module d2l.mxnet), 804 tensor() (in module d2l.mxnet), 804 Timer (class in d2l.mxnet), 788 TokenEmbedding (class in d2l.mxnet), 789 tokenize() (in module d2l.mxnet), 804 tokenize_nmt() (in module d2l.mxnet), 805 train_2d() (in module d2l.mxnet), 805 train_batch_ch13() (in module d2l.mxnet), 805 train_ch3() (in module d2l.mxnet), 805 train_ch6() (in module d2l.mxnet), 805 train_ch8() (in module d2l.mxnet), 805

train_ch13() (in module d2l.mxnet), 805 train_epoch_ch3() (in module d2l.mxnet), 805 train_epoch_ch8() (in module d2l.mxnet), 805 train_seq2seq() (in module d2l.mxnet), 805 TransformerEncoder (class in d2l.mxnet), 789 transpose_output() (in module d2l.mxnet), 805 transpose_qkv() (in module d2l.mxnet), 805 truncate_pad() (in module d2l.mxnet), 805 try_all_gpus() (in module d2l.mxnet), 805 try_gpu() (in module d2l.mxnet), 805
U
update_D() (in module d2l.mxnet), 805 update_G() (in module d2l.mxnet), 805 use_svg_display() (in module d2l.mxnet), 805
V
voc_colormap2label() (in module d2l.mxnet), 805
voc_label_indices() (in module d2l.mxnet), 806 voc_rand_crop() (in module d2l.mxnet), 806 Vocab (class in d2l.mxnet), 789 VOCSegDataset (class in d2l.mxnet), 789
Z
zeros() (in module d2l.mxnet), 806

Index

823

