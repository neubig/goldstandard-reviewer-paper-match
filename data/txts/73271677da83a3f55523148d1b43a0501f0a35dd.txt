ONLINE LEARNING IN PERIODIC ZERO-SUM GAMES

arXiv:2111.03377v1 [cs.GT] 5 Nov 2021

Tanner Fiez∗ University of Washington
Seattle, Washington fiezt@uw.edu

Ryann Sim∗ SUTD
Singapore ryann_sim@mymail.sutd.edu.sg

Stratis Skoulakis∗ SUTD
Singapore efstratios@sutd.edu.sg

Georgios Piliouras† SUTD
Singapore georgios@sutd.edu.sg

Lillian Ratliff† University of Washington
Seattle, Washington ratliffl@uw.edu

ABSTRACT
A seminal result in game theory is von Neumann’s minmax theorem, which states that zero-sum games admit an essentially unique equilibrium solution. Classical learning results build on this theorem to show that online no-regret dynamics converge to an equilibrium in a time-average sense in zero-sum games. In the past several years, a key research direction has focused on characterizing the day-to-day behavior of such dynamics. General results in this direction show that broad classes of online learning dynamics are cyclic, and formally Poincaré recurrent, in zero-sum games. We analyze the robustness of these online learning behaviors in the case of periodic zero-sum games with a time-invariant equilibrium. This model generalizes the usual repeated game formulation while also being a realistic and natural model of a repeated competition between players that depends on exogenous environmental variations such as time-of-day effects, week-to-week trends, and seasonality. Interestingly, time-average convergence may fail even in the simplest such settings, in spite of the equilibrium being ﬁxed. In contrast, using novel analysis methods, we show that Poincaré recurrence provably generalizes despite the complex, non-autonomous nature of these dynamical systems.
1 Introduction
The study of learning dynamics in zero-sum games is arguably as old of a ﬁeld as game theory itself, dating back to the seminal work of Brown and Robinson [7, 27], which followed shortly after the foundational minmax theorem of von Neumann [33]. The dynamics of online no-regret learning algorithms [10, 29] are of particular interest in zero-sum games as they are designed with an adversarial environment in mind. Moreover, well known results imply that such dynamics converge in a time-average sense to a minmax equilibrium in zero-sum games [10, 15].
Despite the classical nature of the study of online no-regret learning dynamics in zero-sum games, the actual transient behavior of such dynamics was historically not as understood. However, in the past several years this topic has gained attention with a number of works studying such dynamics in zero-sum games (and variants thereof) with a particular focus on continuous-time analysis [24, 25, 20, 6, 32, 23, 22]. The unifying emergent picture is that the dynamics are “approximately cyclic" in a formal sense known as Poincaré recurrence. Moreover, these results have acted as fundamental building blocks for understanding the limiting behavior of their discrete-time variants [2, 12, 21, 11, 3].
Despite the plethora of emerging results regarding online learning dynamics in zero-sum games, an important and well motivated aspect of this problem has only begun to receive attention.
How do online learning dynamics behave if the zero-sum game evolves over time?
Clearly, the answer to this question depends critically on how the game is allowed to evolve.
∗Joint ﬁrst authors †Joint last authors

Online Learning in Periodic Zero-Sum Games
Problem Setting and Model. We study periodic zero-sum games with a time-invariant equilibrium, which is a class of games we formally deﬁne in Section 2. In a periodic zero-sum game, the payoffs that dictate the game are both T -periodic and zero-sum at all times. We consider both periodic zero-sum bilinear games on inﬁnite, unconstrained strategy spaces and periodic zero-sum matrix games (along with network generalizations thereof) on ﬁnite strategy spaces. The goal of this work is to evaluate the robustness of the archetypal online learning behaviors in zero-sum games, Poincaré recurrence and time-average equilibrium convergence, to this natural model of game evolution.
Connections to Repeated Game Models. The time-evolving game model we study can be seen as a generalization of usual repeated game formulations. A time-invariant game is a trivial version of a periodic game, in which case we recover the repeated static game setting. For a general periodic zero-sum game with period T , each stage game now is chosen according to a ﬁxed length T sequence of games, capturing interactions between the players with time-dependent payoffs.
Periodic zero-sum games can also ﬁt into the frameworks of multi-agent contextual games [28] and dynamic games (see, e.g., [5]). In a multi-agent contextual game [28], the environment selects a context from a set before each round of play and this choice deﬁnes the game that is played. Periodic zero-sum games can be seen as a multi-agent contextual game where the environment draws contexts from the available set in a T -periodic fashion with each context deﬁning a zero-sum game with a common equilibrium. In the class of dynamic games, there is a game state on which the payoffs may depend that evolves with dynamics. Periodic zero-sum games can be interpreted as a dynamic game where the state transitions do not depend on the strategies of the players, the state is T -periodic, and the payoffs are completely deﬁned by the state. We remark that the focus of existing work on contextual games and dynamic games is distinct from the questions investigated in this paper.
The periodic zero-sum game model allows us to capture competitive settings where exogenous environmental variations manifest in an effectively periodic/epochal fashion. This naturally occurs in market competitions where time-of-day effects, week-to-week trends, and seasonality can dictate the game between players. To illustrate this point, consider a competition between service providers that wish to maximize their users, while the total market size evolves seasonally over time. This evolution affects the utility functions, even if the fundamentals of the market, and consequently the equilibrium, remain invariant.
Contributions and Approach. In this paper, for the classes of periodic zero-sum bilinear games and periodic zerosum polymatrix games with time-invariant equilibrium, we investigate the day-to-day and time-average behaviors of continuous-time gradient descent-ascent (GDA) and follow-the-regularized-leader (FTRL) learning dynamics, respectively. This study highlights the careful attention that must be given to the dynamical systems in periodic zero-sum games which preclude standard proof techniques for Poincaré recurrence, while also revealing that intuition from existing results on static zero-sum games can be totally invalidated even by simple restricted examples in periodic zero-sum games.
Contribution 1: Poincaré Recurrence. A key technical challenge in this work is that the dynamical systems which emerge from learning dynamics in periodic zero-sum games correspond to non-autonomous ordinary differential equations, whereas learning dynamics in static zero-sum games correspond to autonomous ordinary differential equations. Consequently, the usual proof methods from static zero-sum games for showing Poincaré recurrence are insufﬁcient on their own in periodic zero-sum games. We overcome this challenge by delicately piecing together properties of periodic systems to construct a discrete-time autonomous system that we are able to show is Poincaré recurrent. This approach allows to prove both the GDA and FTRL learning dynamics are Poincaré recurrent in the respective classes of periodic zero-sum games (Theorems 1 & 2). Finally, we show both periodicity and a time-invariant equilibrium are necessary for such results in evolving games (Proposition 1).
Contribution 2: Time-Average Strategy Equilibration Fails. Given that Poincaré recurrence provably generalizes from static zero-sum games to periodic zero-sum games, it may be expected that the time-average strategies in periodic zero-sum games converge to the time-invariant equilibrium as in static zero-sum games. Surprisingly, we show that counterexamples can be constructed to this intuition even in the simplest of periodic zero-sum games. In particular, we prove the negative result that the time-average GDA and FTRL strategies do not necessarily converge to the time-invariant equilibrium in the respective classes of zero-sum games (Propositions 2 & 3).
Contribution 3: Time-Average Equilibrium Utility Convergence. Despite the negative result for time-average strategy convergence, in the special case of periodic zero-sum bimatrix games we are able to show a complimentary positive result on the time-average utility convergence. Speciﬁcally, we show that the time-average utilities of the FTRL learning dynamics converge to the average of the equilibrium utility values of all the zero-sum games included in a single period of our time-evolving games. (Theorem 3).
Organization. In Section 2, we formalize the classes of games that we study. We present characteristics of dynamical systems as they pertain to this work in Section 3. Section 4 and 5 contain our results analyzing GDA and FTRL learning
2

Online Learning in Periodic Zero-Sum Games

dynamics in continuous and ﬁnite strategy periodic zero-sum bilinear and polymatrix games, respectively. We present numerical experiments in Section 6 and ﬁnish with a discussion in Section 7. Proofs of our theoretical results are deferred to the appendix.

2 Game-Theoretic Preliminaries

2.1 Continuous Strategy Periodic Zero-Sum Games

For continuous strategy periodic zero-sum games, we study periodic zero-sum bilinear games. We begin by formalizing zero-sum bilinear games and then deﬁne the periodic variant.

Zero-Sum Bilinear Games. Given a matrix A ∈ Rn1×n2 , a zero-sum bilinear game on continuous strategy spaces can

be deﬁned by the max-min problem maxx1∈Rn1 minx2∈Rn2 x1 Ax2. Formally, the game is deﬁned by the pair of payoff matrices {A, −A } and the action space of agents 1 and 2 are given by Rn1 and Rn2 , respectively. Player 1 seeks to

maximize the utility function u1(x1, x2) = x1 Ax2 while player 2 optimizes the utility u2(x1, x2) = −x2 A x1. The

game is zero-sum since for any x1 ∈ Rn1 and x2 ∈ Rn2 , the sum of utility over each player is zero. For zero-sum

bilinear

games,

a

Nash

equilibrium

corresponds

to

a

joint

strategy

(x

∗ 1

,

x∗2

)

such

that

for

each

player

i

and

j

=

i,

ui

(x

∗ i

,

x∗j

)

≥

ui(xi, x∗j ),

∀xi

∈

Rni .

Note

that

(x

∗ 1

,

x∗2

)

=

(0, 0)

is

always

a

Nash

equilibrium

of

a

zero-sum

bilinear

game.

Periodic Zero-Sum Bilinear Games. We study the continuous-time GDA learning dynamics in a class of games we refer to as periodic zero-sum bilinear games. The key distinction from a typical static zero-sum bilinear game is that the payoff matrix is no longer ﬁxed in this class of games. Instead, the payoff matrix may change at each time instant as long as game remains zero-sum and the continuous-time sequence of payoffs is periodic. The next deﬁnition formalizes this class of games.

Deﬁnition 1 (Periodic Zero-Sum Bilinear Game). A periodic zero-sum bilinear game is an inﬁnite sequence of zero-sum bilinear games {A(t), −A(t) }∞ t=0 in which the player set and strategy spaces are ﬁxed and the payoff matrix is such that A(t) = A(t + T ) for a ﬁnite period T and all t ≥ 0. Note that in such a game, (0, 0) is always a time-invariant
Nash equilibrium. Furthermore, we assume that the dependence of the payoff entries on time is smooth everywhere
except for a ﬁnite set of points.

2.2 Finite Strategy Periodic Zero-Sum Games

For ﬁnite strategy periodic zero-sum games, we analyze periodic zero-sum polymatrix games. In what follows we deﬁne a zero-sum polymatrix game, which is a network generalization of a bimatrix game, and then detail the periodic variant considered in this paper.

Zero-Sum Polymatrix Games. An N -player polymatrix game is deﬁned by an undirected graph G = (V, E) where
V is the player set and E is the edge set where a bimatrix game is played between the endpoints of each edge [8].
Each player i ∈ V has a set of actions Ai = {1, . . . , ni} that can be selected at random from a distribution xi called a mixed strategy. The mixed strategy set of player i ∈ V is the simplex in Rni denoted by Xi = ∆ni−1 = {xi ∈ Rn≥i0 : α∈Ai xiα = 1} where xiα denotes the probability of action α ∈ Ai. The joint strategy space is denoted by by X = Πi∈V Xi.

The bimatrix game on edge (i, j) is described using a pair of matrices Aij ∈ Rni×nj and Aji ∈ Rnj×ni . The utility or payoff of agent i ∈ V under the strategy proﬁle x ∈ X is given by ui(x) = j:(i,j)∈E xi Aijxj and corresponds to the sum of payoffs from the bimatrix games the player participates in. We further denote by uiα(x) = j:(i,j)∈E(Aijxj)α
the utility of player i ∈ V under the strategy proﬁle x = (α, x−i) ∈ X for α ∈ Ai. The game is called zero-sum if i∈V ui(x) = 0 for all x ∈ X . Each bimatrix edge game is not necessarily zero-sum in a zero-sum polymatrix game.

A Nash equilibrium in a polymatrix game is a mixed strategy proﬁle x∗ ∈ X such that for each player i ∈ V ,

ui

(x

∗ i

,

x∗−

i

)

≥

ui(xi, x∗−i),

∀xi

∈

Xi.

A Nash equilibrium is said to be an interior if supp(x∗i )

=

Ai

∀i

∈

V

where

supp(x∗i ) = {α ∈ Ai : xiα > 0} is the support of x∗i ∈ Xi.

Periodic Zero-Sum Polymatrix Games. We analyze the continuous-time FTRL learning dynamics in a class of games we call periodic zero-sum polymatrix games. This class of games is such that the payoffs deﬁned by the edge games evolve periodically. We consider that this periodic evolution is such that there is a common interior Nash equilibrium that arises in each zero-sum polymatrix game that arrives. The following deﬁnition formalizes the games we study on ﬁnite strategy spaces.

3

Online Learning in Periodic Zero-Sum Games

Deﬁnition 2 (Periodic Zero-Sum Polymatrix Game). A periodic zero-sum polymatrix game is an inﬁnite sequence of zero-sum polymatrix games {G(t) = (V (t), E(t))}∞ t=0 in which the set of players, strategy spaces, and edges are ﬁxed and each bimatrix game on an edge (i, j) is such that Aij(t) = Aij(t + T ) and Aji(t) = Aji(t + T ) for some ﬁnite period T and all t ≥ 0. We assume there is a common interior Nash equilibrium x∗ ∈ X of the polymatrix game G(t)
for all t ≥ 0. Furthermore, we assume that the dependence of the payoff entries on time is smooth everywhere except
for a ﬁnite set of points.

3 Preliminaries on Dynamical Systems

We now cover concepts from dynamical systems theory that will help us analyze learning dynamics in periodic zero-sum games and prove Poincaré recurrence. Careful attention must be given to these preliminaries in this work since the dynamical systems we study are non-autonomous whereas typical recurrence analysis in the study of learning in games deals with autonomous dynamical systems.

3.1 Background on Dynamical Systems

We begin this section by providing dynamical systems background that is necessary both for deﬁning Poincaré recurrence and sketching typical proof methods along with our approach.

Flows. Consider an ordinary differential equation x˙ = f (t, x) on a topological space X. We can deﬁne the ﬂow

φ : R × X → X of a dynamical system x˙ , for which the following holds: (i) φ(t, ·) : X → X, often denoted

φt : X → X, is a homeomorphism for each t ∈ R, (ii) φ(t + s, x) = φ(t, φ(s, x)) for all t, s ∈ R and all x ∈ X, (iii)

for

each

x

∈

X,

d dt

|

t=0

φ

(t,

x)

=

f (t, x),

and

(iv)

φ(t, x0)

=

x(t)

is

the

solution.

Existence and Uniqueness. We utilize Carathéodory’s existence theorem to guarantee the existence of a ﬂow for x˙ , even for some discontinuous functions f .

Theorem (Carathéodory’s existence theorem [13, 16]). Consider a differential equation x˙ = f (t, x) on a rectangular domain R = {(t, y)| |t − t0| ≤ a, |x − x0| ≤ b}. If f satisﬁes the following conditions:

1. f (t, x) is continuous in y for each ﬁxed t,

2. f (t, x) is measurable in t for each ﬁxed y,

3. there is a Lebesgue-integrable function m : [t0 − a, t0 + a] → [0, ∞) such that |f (t, x)| ≤ m(t) for all (t, x) ∈ R,

then the differential equation has a solution. Moreover, if f is also Lipschitz continuous, meaning |f (t, x1)−f (t, x2)| ≤ k(t)|x1 − x2| with some Lebesgue-integrable function k : [t0 − a, t0 + a] → [0, ∞), then there exists a unique solution of the differential equation.

In the settings we study, the above three conditions hold: Condition 1 holds because for every ﬁxed t, the dynamics we study (speciﬁcally GDA and FTRL) are continuous functions of their state space. Condition 2 holds because the systems we study are ﬁnite and continuous almost-everywhere, and so by Lusin’s theorem [18] are measurable for each ﬁxed y. Finally, Condition 3 is always satisﬁed because the games we study always admit bounded orbits. Hence, it follows that a unique ﬂow exists for all the dynamical systems studied in this paper.
Conservation of Volume. The ﬂow φ of an ordinary differential equations is called volume preserving if the volume of the image of any set U ⊆ Rd under φt is preserved, meaning that vol(φt(U )) = vol(U ). Liouville’s theorem states that a ﬂow is volume preserving if the divergence of f at any point x ∈ Rd equals zero: that is, divf (t, x) = tr(Df (t, x)) =
di=1 ∂f∂(xt,ix) = 0.
We now transition to give general Poincaré recurrence statements along with discussion of how the results are usually applied in game theory before we outline our proof methods.

3.2 Poincaré Recurrence in Autonomous Dynamical Systems
A number of works in the past several years show that online no-regret learning dynamics are Poincaré recurrent in repeated static zero-sum games (see, e.g., [24, 20, 6]). The proof methods for deriving such results crucially rely on the static nature of the game for the reason that the learning dynamics amount to an autonomous dynamical system. Informally, the standard Poincaré recurrence theorem states that if an autonomous dynamical system preserves volume and every orbit remains bounded, almost all trajectories return arbitrarily close to their initial position, and do so

4

Online Learning in Periodic Zero-Sum Games
inﬁnitely often [26]; this property of a dynamical system is known as Poincaré recurrence. Thus, proving the Poincaré recurrence of dynamics in repeated static zero-sum games is tantamount to verifying the volume preservation and bounded orbit properties. We now formalize the Poincaré recurrence theorem. Given a ﬂow φt on a topological space X, a point x ∈ X is nonwandering for φt if for each open neighborhood U containing x, there exists T > 1 such that U ∩ φT (U ) = ∅. The set of all nonwandering points for φt, called the nonwandering set, is denoted Ω(φt).
Theorem (Poincaré Recurrence for Continuous-Time Systems [26]). If a ﬂow preserves volume and has only bounded orbits, then for each open set almost all orbits intersecting the set intersect it inﬁnitely often: if φt is a volume preserving ﬂow on a bounded set Z ⊂ Rd, then Ω(φt) = Z.
In order to describe our proof methods in the following subsection for showing Poincaré recurrence in periodic zero-sum games, it will be useful for us to state an alternative formulation of the Poincaré recurrence theorem that is applicable to autonomous discrete-time systems.
Theorem (Poincaré Recurrence for Discrete-Time Maps [4]). Let (X, Σ, µ) be a ﬁnite measure space and let φ : X → X be a measure-preserving map. For any E ∈ Σ, the set of those points x of E for which there exists N ∈ N such that φn(x) ∈/ E for all n > N has zero measure. In other words, almost every point of E returns to E. In fact, almost every point returns inﬁnitely often. That is, µ ({x ∈ E : ∃ N s.t. φn(x) ∈/ E for all n > N }) = 0.
3.3 Poincaré Recurrence in Periodic Dynamical Systems
The proof methods described in the last section for showing the Poincaré recurrence of dynamics in static zero-sum games cannot directly be applied to time-evolving zero-sum games as a result of the non-autonomous nature of the systems. In fact, we can construct time-evolving zero-sum games without both periodic payoffs and a time-invariant equilibrium, where online learning dynamics are not Poincaré recurrent.3 Despite this hurdle, we show that in the natural subclass of time-evolving games covered by periodic zero-sum games (periodic payoffs and a time-invariant equilibrium), we can develop proof methods to show the Poincaré recurrence of online learning dynamics. Given the previous claim regarding the possible non-recurrent nature of learning dynamics when there is not both periodic payoffs and a time-invariant equilibrium, this is perhaps the most general class of time-evolving zero-sum games with obtainable positive results in this direction.
We now give an overview of our approach, beginning by recalling properties of periodic systems.
Periodic Systems and Poincaré Maps. A system x˙ = f (t, x) is T -periodic if f (t + T, x) = f (t, x) for all (x, t). Let φt : Rn → Rn denote the mapping taking x ∈ Rn to the value at time t. For a T -periodic system, φT +s = φs ◦ φT so that φkT = (φT )k for any integer k. The mapping φT : Rn → Rn is called the Poincaré map or the mapping at a period.
If the differential equation is well-deﬁned for all x and has a solution for all t ∈ [0, T ], then for each initial condition (where we have suppressed the dependence on x0), the Poincaré map φT deﬁnes a discrete-time autonomous dynamical system x+ = φT (x). The learning dynamics we study in periodic zero-sum games form T -periodic dynamical systems. Thus, the discrete-time autonomous dynamical system x+ = φT (x) formed by the Poincaré map is key to the analysis methods we pursue. In particular, our approach is to show that this system is Poincaré recurrent, which we then use to conclude that the original continuous-time non-autonomous system is Poincaré recurrent.
Given the previously presented Poincaré recurrence theorem for discrete-time maps, proving the Poincaré recurrence of the system x+ = φT (x) requires verifying the volume preservation and bounded orbit properties, which then implies the measure preserving property. The following result states that if the divergence of a T -periodic vector ﬁeld f (x, t) is divergence free so that the ﬂow φt is volume preserving, then the Poincaré map φT and the resulting discrete-time dynamical system x+ = φT (x) is also volume preserving.
Theorem (Volume preservation for T -Periodic Systems [1, 3.16.B, Thm 2]). If the T -periodic system x˙ = f (t, x) is divergence-free, then φT preserves volume.
Similarly, if orbits of x˙ = f (t, x) are bounded, then clearly the orbits of x+ = φT (x) are bounded. Hence, to show the system x+ = φT (x) is Poincaré recurrent, we prove x˙ = f (t, x) has a divergence-free vector-ﬁeld (equivalently, that the ﬂow is volume preserving) and only bounded orbits. This will then be sufﬁcient to conclude x˙ = f (t, x) is Poincaré recurrent since the discrete-time system forms a subsequence of the continuous-time system.
3We formalize this statement in Proposition 1 of the following section.
5

Online Learning in Periodic Zero-Sum Games
4 Gradient Descent-Ascent in Periodic Zero-Sum Bilinear Games
This section focuses on the continuous-time GDA learning dynamics in periodic zero-sum bilinear games. The dynamics are such that each player seeks to maximize their utility by following the gradient with respect to their choice variable and are given by
x˙ 1 = A(t)x2(t) x˙ 2 = −A (t)x1(t).
4.1 Poincaré Recurrence
The focus of this section is on characterizing the transient behavior of the continuous-time GDA learning dynamics in periodic zero-sum games. Speciﬁcally, we show the following result. Theorem 1. The continuous-time GDA learning dynamics are Poincaré recurrent in any periodic zero-sum bilinear game as given in Deﬁnition 1.
Theorem 1 establishes that the recurrent nature of continuous-time GDA dynamics in static zero-sum bilinear games is robust to the dynamic evolution of the payoffs in periodic zero-sum bilinear games. Prior to outlining the proof steps for Theorem 1, we elaborate on the claim from the previous section that without the periodicity property and a time-invariant equilibrium, such a result is unobtainable. In particular, we show the Poincaré recurrence of the GDA dynamics is not guaranteed without both properties by constructing counterexamples when only one of the properties holds. Proposition 1. There exists time-evolving zero-sum games such that there is a time-invariant equilibrium or the payoffs are periodic (but not both simultaneously) in which the GDA dynamics are not Poincaré recurrent.
This proposition highlights the strength of our results regarding GDA, given that the assumptions needed to obtain them are more or less tight. We now outline the key intermediate results we prove to obtain Theorem 1, following the techniques described in Section 3.3. For the GDA dynamics, we utilize the observation that the corresponding vector ﬁelds are divergence free to show that the learning dynamics are volume-preserving. We now state this result formally. Lemma 1. The GDA learning dynamics are volume preserving in any periodic zero-sum bilinear game as given in Deﬁnition 1.
We then proceed by showing that the GDA orbits are bounded by deriving a time-invariant function. This step relies on the fact that we have a time-invariant equilibrium. Lemma 2. The function Φ(t) = 12 x1 (t)x1(t) + x2 (t)x2(t) is time-invariant. Hence, the GDA orbits are bounded in any periodic zero-sum bilinear game as given in Deﬁnition 1.
Given the volume preservation and bounded orbit characteristics of the continuous-time GDA learning dynamics in periodic zero-sum games, the proof of recurrence follows by applying the arguments described in Section 3.3.
4.2 Time-Average Convergence
The Poincaré recurrence continuous-time GDA learning dynamics in periodic zero-sum bilinear games indicates that the system has regularities which couple the evolving players and evolving game despite the failure to converge to a ﬁxed point. A natural follow-up question to the cyclic transient behavior of the dynamics is whether the long-run converges to a game-theoretically meaningful outcome. We show that in periodic zero-sum bilinear games, the time-average of GDA learning dynamics may not converge to the time-invariant Nash equilibrium. To prove this, we consider a periodic zero-sum bilinear game with the action space of each player on R so that the evolving payoff simply rescales the vector ﬁeld. We construct the payoff sequence so that the dynamics return back to the initial condition after a period of the game, while the time-average of the dynamics are not equal to the time-invariant equilibrium (x∗1, x∗2) = (0, 0). Given the simplicity of this example, it effectively rules out hope to provide a meaningful time-average convergence guarantee in this class of games. Proposition 2. There exists periodic zero-sum bilinear games satisfying Deﬁnition 1 where the time-average strategies of the GDA dynamics fail to converge to the time-invariant equilibrium (0, 0).
6

Online Learning in Periodic Zero-Sum Games

5 Follow-the-Regularized-Leader in Periodic Zero-Sum Polymatrix Games

We now analyze continuous-time FTRL learning dynamics in periodic zero-sum polymatrix games. Players that follow FTRL learning dynamics in this class of games select a mixed strategy at each time that maximizes the difference between the cumulative payoff evaluated over the history of games and a regularization penalty. This adaptive strategy balances exploitation based on the past with exploration.

Formally, the continuous-time FTRL learning dynamics for any player i ∈ V in a periodic zero-sum polymatrix game with an initial payoff vector yi(0) ∈ Rni are given by
yi(t) = yi(0) + 0t j:(i,j)∈E Aij (τ )xj (τ )dτ (1) xi(t) = argmaxxi∈Xi { xi, yi(t) − hi(xi)}
where hi : Xi → R is a penalty term which encourages exploration away from the strategy which maximizes the cumulative payoffs in hindsight. We assume that the regularization function hi(·) for each player i ∈ V is continuous, strictly convex on Xi, and smooth on the relative interior of every face of Xi. These assumptions ensure the update xi(t) is well-deﬁned since a unique solution exists.

Common FTRL learning dynamics include the multiplicative weights update and the projected gradient dynamics. The

multiplicative weights dynamics for a player i ∈ V arise from the regularization function hi(xi) = α∈Ai xiα log xiα and correspond to the replicator dynamics. The projected gradient dynamics for a player i ∈ V derive from the

Euclidean

regularization

hi(xi)

=

1 2

xi

22.

To simplify notation, the FTRL dynamics can equivalently be formulated as the following update

yi(t) = y(0) + 0t vi(x(τ ), τ )dτ (2) xi(t) = Qi(yi(t)).

Observe that we denote by vi(x, τ ) = (uiα(x, τ ))α∈Ai the vector of each pure strategy α ∈ Ai utility for agent i ∈ V under the joint proﬁle x = (α, x−i) ∈ X at time τ ≥ 0. Moreover, Qi : Rni → Xi is known as the choice map and deﬁned as
Qi(yi(t)) = argmaxxi∈Xi { yi(t), xi − hi(xi)}. In this notation, the utility of the player i ∈ V under the joint strategy x = (xi, x−i) ∈ X at time t ≥ 0 is given by ui(x, τ ) = vi(x, τ ), xi . Observe that in our notation of utility we are now including the time index to make the dependence on the evolving game and payoffs explicit.

For any player i ∈ V we denote by h∗i : Rni → R the convex conjugate of the regularization function hi : Xi → R which is given by the quantity h∗i (yi(t)) = maxxi∈Xi { xi, yi(t) − hi(xi)}.

5.1 Poincaré Recurrence
We now focus on characterizing the transient behavior of the continuous-time FTRL learning dynamics in periodic zero-sum polymatrix games. It is known that the continuous-time FTRL learning dynamics are Poincaré recurrent in static zero-sum polymatrix games [20]. The following result demonstrates that this characteristic holds even in games that are evolving in a periodic fashion with a time-invariant equilibrium, providing a broad generalization. Theorem 2. The FTRL learning dynamics are Poincaré recurrent in any periodic zero-sum polymatrix game as given in Deﬁnition 2.
For the remainder of this subsection, we describe our proof methods. The general approach is that we prove the Poincaré recurrence of a transformed system using the techniques described in Section 3.3. This conclusion then allows us to infer the equivalent property for the original FTRL system.
The utility differences for each player i ∈ V and pure strategy αi ∈ Ai \ βi evolve following the differential equation z˙iαi = viαi (x(t), t) − viβi (x(t), t).
Toward proving that this system is Poincaré recurrent, we show that the vector ﬁeld z˙ is divergence free and hence volume preserving. Lemma 3. The dynamics deﬁned by the system z˙ are volume preserving in any periodic zero-sum polymatrix game as given in Deﬁnition 2.
We then construct a time-invariant function along the evolution of the system that is sufﬁcient to guarantee that the orbits generated by the z˙ dynamics are bounded. Recall that x∗ denotes the common, time-invariant interior Nash equilibrium of the periodic zero-sum polymatrix game.

7

Online Learning in Periodic Zero-Sum Games

Lemma 4. The function Φ(x∗, y(t)) = i∈V h∗i (yi(t)) − x∗i , yi(t) + hi(x∗i ) is time-invariant. Hence, the orbits generated by the z˙ dynamics are bounded in any periodic zero-sum polymatrix game as given in Deﬁnition 2.
From this point, we follow the arguments from Section 3.3 to conclude the z˙ dynamics are Poincaré recurrent. Finally, we show that Poincaré recurrence of the z˙ system is sufﬁcient to guarantee the Poincaré recurrence of the FTRL learning dynamics.
The proofs of the results in this section can be found in Appendix D.
5.2 Time-Average Convergence
A number of well-known properties of zero-sum bimatrix games fail to generalize to zero-sum polymatrix games. Indeed, fundamental characteristics of zero-sum bimatrix games include that each agent has a unique utility value in any Nash equilibrium and that equilibrium strategies are exchangeable. However, Cai and Daskalakis [8] show that neither of these properties are guaranteed in zero-sum polymatrix games. Consequently, in general, time-average convergence to the set of equilibrium values in the utility and strategy spaces does not equate to the stronger notion of pointwise convergence.
For the reasons just outlined, we pursue a different notion of time-average convergence in periodic zero-sum polymatrix games. That is, we consider the subclass of periodic zero-sum bimatrix games (2-player periodic zero-sum polymatrix games) and show that the time-average utility of each agent converges to the time-average of the game values (that is, the unique utility the player obtains at any Nash equilibrium) over a period of the periodic game. Theorem 3. In periodic zero-sum bimatrix games satisfying Deﬁnition 2, if each player follows FTRL dynamics, then the time-average utility of each player converges to the time-average over a period of the game equilibrium utility values.
Theorem 3 paints a positive view of the time-average behavior of FTRL learning dynamics in periodic zero-sum games. However, the following result demonstrates that much like in the case of GDA in periodic zero-sum bilinear games, the time-average strategies are not guaranteed to converge to the time-invariant Nash equilibrium. Proposition 3. There exist periodic zero-sum bimatrix games satisfying Deﬁnition 1 in which the time-average strategies of FTRL dynamics fail to converge to the time-invariant Nash equilibrium.
We prove the negative result in this proposition by constructing a simple counterexample that corresponds to a time-varying rescaling of Matching Pennies.
The proofs of the results in this section can be found in Appendix E.

6 Experiments

In this section, we present several experimental simulations that illustrate our theoretical results. To begin, for

continuous-time GDA dynamics we show that Poincaré recurrence holds in a periodic zero-sum bilinear game. We

consider the ubiquitous Matching Pennies game with payoff matrix

1 −1 −1 1

.

We then use the following periodic

rescaling with period 2π:

α(t) =

sin(t)

0 ≤ t ≤ 32π

π2 (t mod(2π) − 2π) 32π ≤ t ≤ 2π

When players follow the GDA learning dynamics, we see from Figure 1 that the trajectories when plotted alongside the value of the periodic rescaling are bounded. A similar experimental result holds in the case of FTRL dynamics. In the supplementary material, we simulate replicator dynamics with the same periodic rescaling as in Figure 1. The trajectories in the dual/payoff space also remain bounded due to the invariance of the Kullback-Leibler divergences (KL-divergence).

Lemmas 2 and 4 describe functions Φ which remain time-invariant. In the case of replicator dynamics, Φ(t) is the sum of Kullback-Leibler divergences measured between the strategy of each player and their mixed Nash strategy [1/2, 1/2]. We simulated a 64-player polymatrix extension to the Matching Pennies game, where each agent plays against the opponent immediately adjacent to them, forming a ’toroid’-like chain of games. Furthermore, we randomly rescale each game with a different periodic function. Figure 2 depicts the claim presented in the lemmas: although each agent’s speciﬁc divergence term KL(x∗i xi(t)) ﬂuctuates, the sum i∈V KL(x∗i xi(t)) remains constant.

To generate Figure 3, we show the data from a simpliﬁed 64-player polymatrix game simulation, where the graph that represents player interactions is sparse. Here, the strategy of each player informs the RGB value of a corresponding

8

Online Learning in Periodic Zero-Sum Games

Figure 1: Bounded trajectories for a periodically rescaled Matching Pennies game updated using GDA.

Figure 2: Weighted sum of KL-divergences for a 64-player periodically rescaled Matching Pennies game. Note that despite the complicated trajectories of each player, the weighted sum of their divergences remains constant.

(a) T = 0

(b) T = 700

(c) T = 1500 (d) T = 3000 (e) T = 5000

(f) T = 6226

Figure 3: Sequence of images showing Poincaré recurrence in an 8 × 8 zero-sum polymatrix game, where the changing color of each pixel on the grid represents the mixed strategy of the player over time. After time T = 6226, we see that an approximation of the original image is recovered, showing that the recurrence property holds.

pixel on a grid. If the system exhibits Poincaré recurrence, we should eventually see similar patterns emerge as the pixels change color over time (i.e., as their corresponding mixed strategies evolve). As observed in Figure 3, the system returns near the initial image at time T = 6226. Further details about the experiments can be found in Appendix F.
7 Discussion
We study both GDA and FTRL learning dynamics in periodically varying zero-sum games. We prove that the recurrent nature of such dynamics carries over from static games to the classes of evolving games we study. Yet, in the settings we analyze, the time-average convergence behavior from static zero-sum games can fail to generalize. This work takes a step toward understanding the behavior of classical learning algorithms for games in the more realistic setting where the game itself is not ﬁxed.
We conclude by discussing related works on learning dynamics in evolving games. The existing literature considers a number of models that admit distinct results [19, 30, 9, 14]. In a class of time-evolving games where the evolution can be arbitrary [9], algorithms are designed that provide a novel type of regret guarantee called Nash equilibrium regret. In a sense these algorithms are competitive against the Nash equilibrium of the long-term-averaged payoff matrix. In an analysis of discrete-time FTRL dynamics in evolving games that are strictly/strongly monotone [14], sufﬁcient conditions (e.g., when the evolving game stabilizes) under which the dynamics track/converge to the evolving equilibrium are derived. Unfortunately, zero-sum games do not satisfy these strong properties. Finally, in a model of endogenously evolving zero-sum games where a parametric game evolves itself adversarially towards the participating agents, a transformation that treats the game as an additional “hidden" agents allows for a reduction to a more standard static network zero-sum game has been developed [19, 30] under which both time-average convergence to equilibrium as well as Poincaré recurrence holds.
This growing literature indicates that time-varying games can exhibit distinct and often times more complex behavior than their classic static counterparts. As such, there is much potential for future work towards a better understanding of their learning dynamics.
9

Online Learning in Periodic Zero-Sum Games
Acknowledgements
This research/project is supported in part by the National Research Foundation, Singapore under its AI Singapore Program (AISG Award No: AISG2-RP-2020-016), NRF 2018 Fellowship NRF-NRFF2018-07, NRF2019-NRFANR095 ALIAS grant, grant PIE-SGP-AI-2018-01, AME Programmatic Fund (Grant No. A20H6b0151) from the Agency for Science, Technology and Research (A*STAR). Tanner Fiez was supported by a National Defense Science and Engineering Graduate Fellowship.
References
[1] V. I. Arnol’d. Mathematical methods of classical mechanics, volume 60. Springer Science & Business Media, 2013.
[2] J. P. Bailey and G. Piliouras. Multiplicative weights update in zero-sum games. In ACM Conference on Economics and Computation, pages 321–338, 2018.
[3] J. P. Bailey, G. Gidel, and G. Piliouras. Finite regret and cycles with ﬁxed step-size via alternating gradient descent-ascent. In Conference on Learning Theory, pages 391–407, 2020.
[4] L. Barreira. Poincaré recurrence: old and new. In XIVth International Congress on Mathematical Physics, pages 415–422. World Scientiﬁc, 2006.
[5] T. Bas¸ar and G. J. Olsder. Dynamic noncooperative game theory. SIAM, 1998. [6] V. Boone and G. Piliouras. From Darwin to Poincaré and von Neumann: Recurrence and Cycles in Evolutionary
and Algorithmic Game Theory. In International Conference on Web and Internet Economics, pages 85–99, 2019. [7] G. Brown. Iterative solutions of games by ﬁctitious play. In Activity Analysis of Production and Allocation, T.C.
Koopmans (Ed.), New York: Wiley., 1951. [8] Y. Cai and C. Daskalakis. On minmax theorems for multiplayer games. In Symposium of Discrete Algorithms,
pages 217–234, 2011. [9] A. R. Cardoso, J. Abernethy, H. Wang, and H. Xu. Competing against nash equilibria in adversarially changing
zero-sum games. In International Conference on Machine Learning, pages 921–930, 2019. [10] N. Cesa-Bianchi and G. Lugoisi. Prediction, Learning, and Games. Cambridge University Press, 2006. [11] Y. K. Cheung. Multiplicative weights updates with constant step-size in graphical constant-sum games. In
Advances in Neural Information Processing Systems, pages 3528–3538, 2018. [12] Y. K. Cheung and G. Piliouras. Vortices instead of equilibria in minmax optimization: Chaos and butterﬂy effects
of online learning in zero-sum games. In Conference on Learning Theory, pages 807–834, 2019. [13] E. A. Coddington and N. Levinson. Theory of ordinary differential equations. Tata McGraw-Hill Education, 1955. [14] B. Duvocelle, P. Mertikopoulos, M. Staudigl, and D. Vermeulen. Learning in time-varying games. arXiv preprint
arXiv:1809.03066, 2018. [15] Y. Freund and R. E. Schapire. Adaptive game playing using multiplicative weights. Games and Economic
Behavior, 29(1-2):79–103, 1999. [16] J. K. Hale. Ordinary differential equations. Robert E. Krieger Publishing Company, 1980. [17] M. Heino, J. A. Metz, and V. Kaitala. The enigma of frequency-dependent selection. Trends in Ecology &
Evolution, 13(9):367–370, 1998. [18] N. Lusin. Sur les propriétés des fonctions mesurables. CR Acad. Sci. Paris, 154(25):1688–1690, 1912. [19] T. Mai, M. Mihail, I. Panageas, W. Ratcliff, V. Vazirani, and P. Yunker. Cycles in Zero-Sum Differential Games
and Biological Diversity. In ACM Conference on Economics and Computation, page 339–350, 2018. [20] P. Mertikopoulos, C. Papadimitriou, and G. Piliouras. Cycles in adversarial regularized learning. In Symposium of
Discrete Algorithms, pages 2703–2717, 2018. [21] P. Mertikopoulos, B. Lecouat, H. Zenati, C.-S. Foo, V. Chandrasekhar, and G. Piliouras. Optimistic mirror
descent in saddle-point problems: Going the extra (gradient) mile. In International Conference on Learning Representations, 2019. [22] S. G. Nagarajan, D. Balduzzi, and G. Piliouras. From chaos to order: Symmetry and conservation laws in game dynamics. In International Conference on Machine Learning, pages 7186–7196, 2020.
10

Online Learning in Periodic Zero-Sum Games
[23] J. Perolat, R. Munos, J.-B. Lespiau, S. Omidshaﬁei, M. Rowland, P. Ortega, N. Burch, T. Anthony, D. Balduzzi, B. De Vylder, G. Piliouras, M. Lanctot, and K. Tuyls. From Poincaré Recurrence to Convergence in Imperfect Information Games: Finding Equilibrium via Regularization. arXiv preprint arXiv:2002.08456, 2020.
[24] G. Piliouras and J. S. Shamma. Optimization despite chaos: Convex relaxations to complex limit sets via Poincaré recurrence. In Symposium of Discrete Algorithms, pages 861–873, 2014.
[25] G. Piliouras, C. Nieto-Granda, H. I. Christensen, and J. S. Shamma. Persistent patterns: Multi-agent learning beyond equilibrium and utility. In International Conference on Autonomous Agents and Multi-Agent Systems, pages 181–188, 2014.
[26] H. Poincaré. Sur le problème des trois corps et les équations de la dynamique. Acta mathematica, 13(1), 1890. [27] J. Robinson. An iterative method of solving a game. Annals of Mathematics, 54:296–301, 1951. [28] P. G. Sessa, I. Bogunovic, A. Krause, and M. Kamgarpour. Contextual games: Multi-agent learning with side
information. In Advances in Neural Information Processing Systems, 2020. [29] S. Shalev-Shwartz et al. Online learning and online convex optimization. Foundations and trends in Machine
Learning, 4(2):107–194, 2011. [30] S. Skoulakis, T. Fiez, R. Sim, G. Piliouras, and L. Ratliff. Evolutionary game theory squared: Evolving agents in
endogenously evolving zero sum games. In AAAI Conference on Artiﬁcial Intelligence, 2021. [31] A. R. Tilman, J. B. Plotkin, and E. Akçay. Evolutionary games with environmental feedbacks. Nature communica-
tions, 11(1):1–11, 2020. [32] E.-V. Vlatakis-Gkaragkounis, L. Flokas, and G. Piliouras. Poincaré Recurrence, Cycles and Spurious Equilibria in
Gradient-Descent-Ascent for Non-Convex Non-Concave Zero-Sum Games. In Advances in Neural Information Processing Systems, pages 10450–10461, 2019. [33] J. von Neumann. Zur theorie der gesellschaftsspiele. Mathematische Annalen, 100:295–300, 1928. [34] J. S. Weitz, C. Eksin, K. Paarporn, S. P. Brown, and W. C. Ratcliff. An oscillating tragedy of the commons in replicator dynamics with game-environment feedback. National Academy of Sciences, 113(47), 2016.
11

Online Learning in Periodic Zero-Sum Games

Appendices

Appendix Organization and Contents
The organization and contents of this appendix is as follows. Appendix A covers additional related work, which we provide a separate bibliography for at the end of the document. Following Appendix A, proofs for the theoretical results in the paper are presented in the order that they appeared. Speciﬁcally, Appendix B contains the proofs for the results presented in Section 4.1 on GDA dynamics and recurrence. This includes the proofs of Proposition 1, Lemma 1, Lemma 2, and Theorem 1. In Appendix C, we provide the proof of Proposition 2 from Section 4.2 on the time-average behavior of GDA. Appendix D contains the analysis for Section 5.1 on FTRL dynamics and recurrence, including the proofs of Lemma 3, Lemma 4, and Theorem 2. Appendix E covers the time-average behavior of FTRL dynamics as presented in Section 5.2. Speciﬁcally, the proofs of Theorem 3 and Proposition 3 are given in Appendix E. Finally, Appendix F contains additional experimental results.

A Additional Related Work
The discussions of related work presented in Section 1 and Section 7 focused on comparable theoretical results in static zero-sum games and studies on classes of certain evolving zero-sum games. We remark that there is also a rich literature studying evolutionary dynamics in action-dependent (endogenous) evolving games in problems strongly motivated by applications in science, economics, and sociology. Often this line of work falls under the hood of what is known as negative frequency dependent selection [17]. Negative frequency dependent selection is an evolutionary process in which the ﬁtness of a strategy dissipates as it becomes more common. Indeed, a number of works (see, [34, 31] and the references therein) analyze evolutionary dynamics in speciﬁc formulations, typically in low-dimensional strategy spaces, and characterize the outcomes. In contrast, we study a broad class of learning dynamics in a general class of exogenously evolving games.

B GDA Recurrence Results: Proofs for Section 4.1
This appendix includes the proofs of Proposition 1, Lemma 1, Lemma 2, and Theorem 1.

B.1 Proof of Proposition 1

To prove this result, we construct time-evolving zero-sum games without both periodic payoffs and a time-invariant equilibrium in which the GDA dynamics are not Poincaré recurrent.

Example 1. Consider a time-evolving zero-sum game on scalar action spaces so that x1, x2 ∈ R with the time-evolving payoff matrix A(t) = t−2. Without loss of generality, we can consider t > 0 so that the GDA dynamics are well-deﬁned. This time-evolving zero-sum does not have periodic payoffs, but (x∗1, x∗2) = (0, 0) is a time-invariant Nash equilibrium. We now show that the GDA dynamics are not Poincaré recurrent in this game. Before formally proving this, we remark
that the intuition for why this statement holds is that since the payoff matrix goes to zero, the distance the dynamics can
travel is bounded so it is impossible that the trajectory could return arbitrarily close to an initial condition inﬁnitely
often.

The GDA dynamics in this time-evolving zero-sum game are described by the system

x˙ 1 = 0

1 t2

x1(t) .

x˙ 2

−

1 t2

0

x2(t)

The solution of a time-varying linear system of this form is given by

x1(t) = exp x2(t)

t0 t0 − τ12

1
τ02 dτ

x1(t0) . x2(t0)

We consider t0 > 1 without loss of generality. To derive the explicit solution, we begin by computing the integral of the evolving payoff matrix and get that

t0 t0 − τ12

1

0

τ2
0

dτ =

1− 1

t t0

t10 − 1t . 0

12

Online Learning in Periodic Zero-Sum Games

Recalling the following identity

exp θ 0 −1 10

= cos(θ) sin(θ) , − sin(θ) cos(θ)

we can determine that the matrix exponential is then given by

0

1 −1

cos( 1 − 1 ) − sin( 1 − 1 )

exp

1− 1

t0 t
0

=

t t0
sin( 1 − 1 )

t t0
cos( 1 − 1 )

.

t t0

t t0

t t0

Therefore, the solution of the system simpliﬁes to be given by

x1(t) = cos( 1t − t10 )

x2(t)

sin( 1t − t10 )

− sin( 1t − t10 ) cos( 1t − t10 )

x1(t0) , x2(t0)

and equivalently,

x1(t) = cos( 1t − t10 )x1(t0) − sin( 1t − t10 )x2(t0) .

x2(t)

sin( 1t − t10 )x1(t0) + cos( 1t − t10 )x2(t0)

Given the explicit form of the solution, we now show that we can construct an initial condition that the strategies do not return back to inﬁnitely often. This will immediately allow us to conclude the system is not Poincaré recurrent by deﬁnition. We remark that the following choice of initial condition is only for the simplicity of the proof and identical conclusions would hold for almost all initial conditions.

Let x1(t0) = 1 and x2(t0) = 0. Given this initial condition, the solution of the system simpliﬁes to be given by

x1(t) = cos( 1t − t10 ) .

x2(t)

sin( 1t − t10 )

Taking the limit of the solution as t → ∞, we have that

11

1

lim x1(t) = lim cos − = cos

t→∞

t→∞

t t0

t0

and

11

1

lim x2(t) = lim sin − = − sin .

t→∞

t→∞

t t0

t0

This shows that the dynamics converge to a ﬁxed point (x¯1, x¯2) = (cos t10 , − sin t10 ). Since (x¯1, x¯2) = (cos t10 , sin t10 ) = (1, 0) = (x1(t0), x2(t0)) for t0 > 1 unless t0 → ∞, this immediately implies that the GDA dynamics are not Poincaré recurrent in this time-evolving zero-sum game since they do not return inﬁnitely often

back to an arbitrarily small neighborhood around the initial condition.

Example 2. In the previous example, we showed that the GDA dynamics were not Poincaré recurrent in a time-evolving zero-sum game that had a time-invariant Nash equilibrium but not periodic payoffs. In this example, we provide theoretical evidence that the GDA dynamics are not Poincaré recurrent in a time-evolving zero-sum game with periodic payoffs but without a time-invariant Nash equilibrium.

Consider a time-evolving zero-sum game on scalar action spaces so that x1, x2 ∈ R with a periodic payoff matrix A(t) = A(t + T ) for any t ≥ 0 and T = 3 that evolves over a period such that A(t) = 1 for 0 ≤ t ≤ 1 and A(t) = −1
for 1 ≤ t ≤ 3. We treat player 2 as a dummy player that just plays the ﬁxed strategy of x2 = 1 for all t. Thus, this time-evolving zero-sum game can be viewed as a trivial game that is equivalent to an optimization problem for player 1.
Since player 1 is a utility maximizer, the Nash equilibrium of the game at each time simply corresponds to the strategy of player 1 that maximizes its utility. Thus, the Nash equilibrium when A(t) = 1 is x∗1 = ∞ and given A(t) = −1 it is x21∗ = −∞. Therefore, this corresponds to a time-evolving zero-sum game that is periodic but there is not a time-invariant Nash equilibrium.

We now show that the GDA dynamics are not Poincaré recurrent in this game. The dynamics can be described by the system x˙ 1 = A(t). Consequently, the solution is x1(t) = x1(t0) + t on the interval with A(t) = 1 and x1(t) = x1(t0) − 1 on the interval with A(t) = −1. This means that after 1 period of the game, x1(t) = x1(t0) − 1, which implies that x1(t) → −∞ as t → ∞. Thus the dynamics are not Poincaré recurrent in this time-evolving zero-sum game since they do not return inﬁnitely often back to an arbitrarily small neighborhood around the initial
condition.

Example 3. Additionally, we provide an experimental example to show the non-existence of Poincaré recurrence in the setting of FTRL dynamics. In particular, we simulate a time-evolving zero-sum game which has periodic payoffs but

13

Online Learning in Periodic Zero-Sum Games

Figure 4: (Left, Center) Replicator trajectories for periodically evolving game without time-invariant equilibrium. (Right) L1-norm plot showing that recurrence does not hold in this example.

does not have a time-invariant Nash equilibrium. Consider a time-evolving zero-sum game where the payoff matrix for

the ﬁrst quarter of a period is standard Matching Pennies A =

1 −1 −1 1

. For the remaining three quarters of the period

it is instead A =

0.05 −0.5 −0.5 5

. Note that here the payoff matrix for the second player is just −A. The former game

has a mixed Nash equilibrium where both players play [0.5, 0.5] and the latter game has a mixed Nash equilibrium

where both players play [0.9091, 0.0909]. We simulate this example with replicator dynamics, which is an instantiation

of FTRL dynamics. First, we plot the trajectories of the player’s strategies against each other. Moreover, we plot the

L1-norm between the joint trajectory of the players and the initial condition. The simulation results show that the

trajectory does not return back arbitrarily close to the initial condition, thus the dynamics are not Poincaré recurrent in

this periodic evolving game without a time-invariant equilibrium (Figure 4).

B.2 Proof of Lemma 1

We can show that the GDA dynamics are volume preserving by showing that the vector ﬁeld is divergence free and then applying Liouville’s theorem. Indeed,

2 ni ∂x˙ ij

div(x˙ ) =

= 0,

i=1 j=1 ∂xij

which follows from the fact that x˙ ij is independent of xij for each i, j. The divergence free property of the vector ﬁeld then ensures that the ﬂow φt of the differential equation is volume preserving by Liouville’s theorem.

B.3 Proof of Lemma 2
To prove this statement, we claim the following function is time-invariant:
1 Φ(t) = 2 x1 (t)x1(t) + x2 (t)x2(t) .
By taking the time-derivative of the Φ(t) we can verify the function is a constant of motion. Indeed, this holds based on the following analysis:
dΦ 1 dt = 2 x1 (t)x˙ 1 + x˙ 1 x1(t) + x2 (t)x˙ 2 + x˙ 2 x2(t)
= x1 (t)x˙ 1 + x2 (t)x˙ 2 = x1 (t)A(t)x2(t) − x2(t) A (t)x1(t) = 0.
Finally, observe that given a bounded initial condition, the time-invariance of Φ(t) directly implies that no strategy of any player can become unbounded so the ﬂow φt of the differential equation has bounded orbits.

14

Online Learning in Periodic Zero-Sum Games

B.4 Proof of Theorem 1
Given the previous intermediate results, Theorem 1 follows from the arguments presented in Section 3.3. In particular, observe that by deﬁnition of the periodic zero-sum bilinear game, the GDA dynamics are T -periodic. Now, consider the discrete-time dynamical system deﬁned by the Poincaré map φT that arises. This system retains the volume preservation property of the continuous-time system from Lemma 1 since as presented in Section 3.3, if a T -periodic system is divergence-free then the discrete-time system deﬁned by φT is also volume preserving [1, 3.16.B, Thm 2]. Similarly, the discrete-time system deﬁned by the φT retains the bounded orbits guarantee of the continuous-time system from Lemma 2 since it holds at any set of times. Thus, we are able to apply the Poincaré recurrence theorem for discrete-time systems from Section 3.3 to the discrete-time system deﬁned by φT to conclude the discrete-time system is Poincaré recurrent. This immediately implies that the GDA dynamics are Poincaré recurrent since the discrete-time system deﬁned by φT forms a subsequence of the continuous-time system.

C GDA Time-Average Result: Proof of Proposition 2

Consider a periodic zero-sum bilinear game with x1, x2 ∈ R and a periodic payoff matrix A(t) such that A(t) = A(t+T ) with T = 3π for any t ≥ 0. Moreover, let the payoff matrix evolve over a period as follows:

 −1  A(t) = 1  −1

0≤t≤π π ≤ t ≤ 32π 32π ≤ t ≤ 3π.

The joint strategy (x∗1, x∗2) = (0, 0) is the time-invariant Nash equilibrium. We now show that the time-average of the strategies produced by the GDA dynamics do not converge to the time-invariant Nash equilibrium.

The GDA dynamics in this periodic zero-sum bilinear game are given by

x˙ 1 = A(t)x2(t) x˙ 2 = −A(t)x1(t).

The solution to the differential equation that describes the GDA dynamics can be constructed in a piecewise manner. On each of the three intervals we have a linear system deﬁned by

x˙ 1 = 0 A(t) x1 .

x˙ 2

−A(t) 0 x2

Now recall the following identity

exp θ 0 1 −1 0

= cos(θ) sin(θ) . − sin(θ) cos(θ)

Hence for initial condition (x1(0), x2(0)) and interval [0, π) we know that A(t) = −1 for all t in the interval which implies that the solution on this interval is given by

x1(t) = cos(−t) sin(−t) x1(0) . x2(t) − sin(−t) cos(−t) x2(0)

On the interval [π, 3π/2), A(t) = 1 so that

x1(t) = cos(t − π) sin(t − π) x1(π) . x2(t) − sin(t − π) cos(t − π) x2(π)

Finally, on [3π/2, 3π), A(t) = −1 so that

x1(t) = cos(−(t − 3π/2)) sin(−(t − 3π/2)) x1(3π/2) . x2(t) − sin(−(t − 3π/2)) cos(−(t − 3π/2)) x2(3π/2)

Now, let us consider the initial condition (x1(0), x2(0)) = (1, 0). Then,

(x1(t), x2(t)) =

(cos(t), sin(t)) = (cos(−t), − sin(−t)) (cos(t), − sin(t)) = (− cos(t − π), sin(t − π)) (− cos(t), − sin(t)) = (sin(−(t − 3π/2)), cos(−(t − 3π/2))))

t ∈ [0, π) t ∈ [π, 3π/2) t ∈ [3π/2, 3π)

15

Online Learning in Periodic Zero-Sum Games

Observe that from this solution we can determine that the GDA dynamics return to the initial condition at the end of a period. Thus, to assess convergence of the time-average it is sufﬁcient to evaluate the time-average of the dynamics over a period of the evolving game. Integrating the solution over a period, we have that

3π

3π/2

3π

x1(t)dt =

cos(t)dt +

− cos(t)dt

0

0

3π/2

= [sin(3π/2) − sin(0)] − [sin(3π) − sin(3π/2)]

= −2

and

3π

π

3π

x2(t)dt = sin(t)dt + − sin(t)dt

0

0

π

= [− cos(π) + cos(0)] + [cos(3π) − cos(π)]

=2

This implies that the time-average strategies of the players do not equal to zero, so the time-average of the GDA dynamics do not converge to the time-invariant Nash equilibrium. The fact that it is non-zero holds generally even changing the initial condition.

This completes the proof and shows that there exists periodic zero-sum bilinear games where the time-average GDA strategies do not converge to the time-invariant Nash equilibrium.

D FTRL Poincaré Recurrence Results: Proofs for Section 5.1

This appendix includes the proofs of Lemma 3, Lemma 4, and Theorem 2.

D.1 Proof of Lemma 3

Recall that this result states that the dynamics deﬁned by the system z˙ (given again in (3)) are volume preserving in any periodic zero-sum polymatrix game. Here, we also explain how the z˙ dynamics were formulated. Then, we show that the divergence of this vector ﬁeld is zero, from which we conclude the dynamics are volume preserving by Liouville’s theorem. This proof follows closely arguments in [20].

For each player i ∈ V , given a ﬁxed strategy β ∈ Ai, for all α ∈ Ai \ β the cumulative utility differences are deﬁned by

ziα(t) = yiα(t) − yiβ(t).

This transformation from the cumulative utilities to the cumulative utility differences yields a linear map Πi : Rni → Rni−1 from yi(t) to zi(t) for each player i ∈ V . Moreover, deﬁne by Π = (Π1, . . . , Π|V |) the product map of the linear maps Πi of each player i ∈ V . This map is surjective, but not injective.

Observe that the cumulative utility differences for each player i ∈ V and all α ∈ Ai \ β evolve following the differential

equation

z˙iα(t) = viα(x(t), t) − viβ(x(t), t).

(3)

The above differential equation is obtained directly by the form of yi and the fundamental theorem of calculus. Moreover, recall that for any player i ∈ V and pure strategy γ ∈ Ai, the quantity viγ(x(t), t) gives utility of player i ∈ V at any time t ≥ 0 for selecting the pure strategy γ ∈ Ai.

To analyze the dynamics from the system in (3) we need it to be well-deﬁned, which is not immediate since it depends
on x(t) = Q(y(t)) and the mapping from y(t) to z(t) via Π is not invertible so that y(t) cannot be expressed as
a fuction of z(t). However, despite this, the system is in fact well-deﬁned. To see this, for each player i ∈ V , consider the reduced choice map Qˆi : Rni−1 → Xi deﬁned as Qˆi(zi(t)) = Qi(yi(t)) for some yi(t) ∈ Rni such that Πi(yi(t)) = zi(t) which is guaranteed to exist since Πi is surjective. Then, the fact that Qˆi(zi(t)) is well-deﬁned for each player i ∈ V holds since by the construction Πi(yi(t)) = Πi(yi(t)) if and only if yiα(t) = yiα(t) + c for c ∈ R and every αi ∈ Ai which immediately implies Qi(yi(t)) = Qi(yi(t)) if and only if Πi(yi(t)) = Πi(yi(t)). Finally, let Qˆ = (Qˆ1, . . . , Qˆ|V |) be the combined reduced choice map and note that Q(y(t)) = Qˆ(Π(y(t)) = Qˆ(z(t)) by the construction. As a result, the dynamics from the system in (3) are equivalently given by the following system

z˙iα = viαi (Qˆi(z(t), t)) − viβi (Qˆi(z(t)), t).

16

Online Learning in Periodic Zero-Sum Games

This system is well-deﬁned by the arguments above which ensures that the system in (3) is well-deﬁned.

Now that we have shown the system is well-deﬁned, we prove that is is volume preserving. To see this, observe that the vector ﬁeld is divergence free. Indeed,

div(z˙) =

∂z˙iα =

∂z˙iα ∂xiγ = 0.

i∈V α∈Ai ∂ziα i∈V α∈Ai γi∈Ai ∂xiγ ∂ziαi

Note that the equation above holds since for each player i ∈ V , the pure strategy utilities at any time t ≥ 0 given by vi(x(t), t) where viα(x(t), t) = ui((α, x−i(t)), t) do not depend on xi(t). Finally, the divergence free property of the vector ﬁeld ensures that the ﬂow φt of the differential equation is volume preserving by Liouville’s theorem.

D.2 Proof of Lemma 4

Recall that Lemma 4 states that the orbits of the z˙ dynamics are bounded. To prove this statement, we show that the

function

Φ(x∗, y(t)) =

h∗i (yi(t)) −

x

∗ i

,

yi

(t

)

+ hi(x∗i )

i∈V

is time-invariant where x∗ denotes the time-invariant fully mixed Nash equilibrium and then argue that this is sufﬁcient

to ensure that orbits of the z˙ dynamics are bounded.

To prove that the function Φ(x∗, y(t)) is time-invariant, we show that the time-derivative of the function is equal to zero. The time-derivative of Φ(x∗, y(t)) simpliﬁes using the fact that hi(x∗i ) is time-independent to the following:

dΦ(x∗, y(t)) d

∗

d

∗

dt

= dt

hi (yi(t)) + dt

xi , yi(t) .

i∈V

i∈V

We begin by showing that the time-derivative of i∈V h∗i (yi(t)) = 0. This holds by the following computation that is explained below:

ddt h∗i (yi(t)) = ∇h∗i (yi(t)), y˙i(t) (4)

i∈V

i∈V

= xi(t), y˙i(t)

(5)

i∈V

= xi(t), vi(x(t), t)

(6)

i∈V

= ui(x(t), t)

(7)

i∈V

= 0.

(8)

We obtain (4) by the chain rule, (5) by the maximizing argument of convex conjugates (see e.g., Shalev-Shwartz et al. 29, Chapter 2) that implies xi(t) = Qi(yi(t)) = ∇h∗i (yi(t)), (6) by the deﬁnition of yi(t) and the fundamental theorem of calculus, (7) by deﬁnition of the pure strategy utilities vi(x(t), t) and the utility ui(x(t), t), and (8) by the fact that
the polymatrix game is zero-sum.

We now ﬁnish by showing that the time-derivative of

i∈V

x

∗ i

,

yi

(

t)

= 0. To begin, observe that the time-derivative

can be described by

ddt x∗i , yi(t) = x∗i , y˙i(t)

i∈V

i∈V

=

(x∗i ) Aij(t)xj(t)

(9)

i∈V j:(i,j)∈E

=

(x∗i ) Aij(t)(xj(t) − x∗j ).

(10)

i∈V j:(i,j)∈E

Observe that (9) follows from the deﬁnition of yi(t) and the fundamental theorem of calculus and (10) comes about from subtracting i∈V ui(x∗, t) which is zero by the fact that the polymatrix game is zero-sum for any t ≥ 0.
To continue, we remark that any zero-sum polymatrix game can be transformed to a payoff equivalent, pairwise constant-sum game [8]. This means that for each edge (i, j) ∈ E there exists a matrix Bij(t) such that the following properties hold (see Lemma 3.1, 3.2, and 3.4, respectively Cai and Daskalakis 8):

17

Online Learning in Periodic Zero-Sum Games

Property 1. Aiαjβ(t) − Aiαjγ(t) = Bαijβ(t) − Bαijγ(t) for any pure strategies α ∈ Ai and β, γ ∈ Aj.
Property 2. Bij(t) + (Bji(t)) = cij(t) · 1ni×nj , where cij(t) is a constant and 1ni×nj is an ni × nj matrix of ones.
Property 3. In every joint pure strategy proﬁle, every player i ∈ V has the same utility in the game deﬁned by the individual payoff matrices {Aij(t)}(i,j)∈E as in the game deﬁned by the individual payoff matrices {Bij (t)}(i,j)∈E .

Fixing a strategy γ ∈ Aj, we can equivalently express any summand of (10) in the following manner that is justiﬁed below:

(x∗i ) Aij (xj (t) − x∗j ) =

x∗iαAiαjβ (xjβ (t) − x∗jβ )

α∈Ai β∈Aj

= x∗iα Bαijβ(t) − Bαijγ (t) + Aiαjγ (t) (xjβ (t) − x∗jβ ) (11)
α∈Ai β∈Aj

= (x∗i ) Bij(t)(xj(t) − x∗j ) +

x∗iα Aiαjγ (t) − Bαijγ (t)

(xjβ(t) − x∗jβ).

α∈Ai

β∈Aj

= (x∗i ) Bij(t)(xj(t) − x∗j ).

(12)

Observe that (11) results from applying Property 1 and (12) holds since both xj(t) and x∗j are on the simplex so that β∈Aj xjβ = 1 and β∈Aj x∗jβ = 1 which implies β∈Aj (xjβ − x∗jβ) = 0.
Thus, continuing from (10) and using that (x∗i ) Aij(xj(t) − x∗j ) = (x∗i ) Bij(t)(xj(t) − x∗j ) from above and swapping the sum indexing and taking the transpose of the quadratic form (x∗i ) Bij(t)(xj(t) − x∗j ), we get that

ddt x∗i , yi(t) =

(x∗i ) Aij (t)(xj (t) − x∗j )

i∈V

i∈V j:(i,j)∈E

=

(x∗i ) Bij (t)(xj (t) − x∗j )

(13)

i∈V j:(i,j)∈E

=

(xj(t) − x∗j ) (Bij(t)) x∗i .

j∈V i:(j,i)∈E

Moreover, we obtain the following expression that is justiﬁed below:

ddt x∗i , yi(t) =

(xj (t) − x∗j ) (Bij(t)) x∗i

i∈V

j∈V i:(j,i)∈E

=

(xj (t) − x∗j ) (cji(t)1nj×ni − Bji(t))x∗i

(14)

j∈V i:(j,i)∈E

=

cji(t)(xj (t) − x∗j ) 1nj×ni x∗i −

(xj (t) − x∗j ) Bji(t)x∗i

j∈V i:(j,i)∈E

j∈V i:(j,i)∈E

=−

(xj (t) − x∗j ) Bji(t)x∗i .

(15)

j∈V i:(j,i)∈E

Note that (14) results from applying Property 2 and we obtain (15) using that j∈V i:(j,i)∈E cji(t)(xj(t)−x∗j ) = 0 since each summand is zero as can be seen by noting that α∈Aj xjα = α∈Aj x∗jα = α∈Ai x∗iα = 1 which gives
cji(t)(xj (t) − x∗j ) 1nj×ni x∗i = cji(xj (t) − x∗j ) 1nj = cji(t) − cji(t) = 0.

18

Online Learning in Periodic Zero-Sum Games

We now analyze the summand in (15) for some j ∈ V . Fixing any pure strategy γi ∈ Ai for each i ∈ V \ {j}, obtain the following simpliﬁcation that is explained below:

(xj (t) − x∗j ) Bji(t)x∗i =

(xjα(t) − x∗jα)Bαjiβ (t)x∗iβ

i:(j,i)∈E

i:(j,i)∈E α∈Aj β∈Ai

= (xjα(t) − x∗jα) Ajαiβ (t) − Ajαiγi (t) + Bαjiγi (t) x∗iβ
i:(j,i)∈E α∈Aj β∈Ai

(16)

= (xj (t) − x∗j ) Aji(t)x∗i + (xjα(t) − x∗jα) (Bαjiγi (t) − Ajαiγi (t)) x∗iβ

i:(j,i)∈E

α∈Aj

i:(j,i)∈E

β∈Ai

= (xj (t) − x∗j ) Aji(t)x∗i + (xjα(t) − x∗jα) (Bαjiγi (t) − Ajαiγi (t))

i:(j,i)∈E

α∈Aj

i:(j,i)∈E

(17)

=

(xj (t) − x∗j ) Aji(t)x∗i .

(18)

i:(j,i)∈E

The equation in (16) follows from applying Property 1 and the equation in (17) holds since β∈Ai x∗iβ = 1 as a result of the strategy spaces being on the simplex. Finally, to see how (18) is obtained, observe that for each α ∈ Aj the terms
i:(j,i)∈E Ajαiγi (t) and i:(j,i)∈E Bαjiγi (t) give the utility of player j ∈ V in the games with payoffs {Aji(t)}(j,i)∈E
and {Bji(t)}(j,i)∈E respectively under a joint pure strategy. Hence, by Property 3, the respective utilities are equal so
that the difference is zero.

Finally, relating (18) back to (15), we conclude that the time-derivative is zero:

ddt x∗i , yi(t) = −

(xj (t) − x∗j ) Bji(t)x∗i

i∈V

j∈V i:(j,i)∈E

=−

(xj(t) − x∗j ) Aji(t)x∗i = 0.

j∈V i:(j,i)∈E

The ﬁnal equality holds since x∗ is an interior Nash equilibrium, which implies ujα(x∗, t) = uj(x∗, t) for all strategies α ∈ Aj and any linear combination thereof.

Hence,

dΦ(x∗, y(t)) d

∗

d

∗

dt

= dt

hi (yi(t)) + dt

xi , yi(t) = 0,

i∈V

i∈V

which implies that Φ(x∗, y(t)) is time-invariant.

Finally, by Lemma D.2 of Mertikopoulos et al. [20], the time-invariance of Φ is sufﬁcient to ensure that the ﬂow φt of the differential equation z˙ has bounded orbits. This ﬁnishes the proof.

D.3 Proof of Theorem 2
Theorem 2 states that the FTRL dynamics are Poincaré recurrent in periodic zero-sum polymatrix games. The proof of this claim follows from Lemma 3, Lemma 4 and the methods described in Section 3.3. Indeed, to begin, observe that the z˙ dynamics given in (3) are T -periodic. This follows immediately from the deﬁnition of a T -periodic system as described in Section 3.3 and the fact that the payoff matrices are T -periodic. Consider the discrete-time dynamical system deﬁned by the Poincaré map φT that arises. This system retains the volume preservation property of the continuous-time system from Lemma 3 since as presented in Section 3.3, if a T -periodic system is divergence-free then the discrete-time system deﬁned by φT is also volume preserving [1, 3.16.B, Thm 2]. Furthermore, the bounded orbits guarantee of the continuous-time system from Lemma 4 imply the discrete-time system deﬁned by φT has bounded orbits since it is a subsequence of the continuous-time system.
Thus, we are able to apply the Poincaré recurrence theorem to the system deﬁned by φT to conclude that the discretetime system is Poincaré recurrent. This implies that the z˙ dynamics are Poincaré recurrent since the discrete-time system deﬁned by φT forms a subsequence of the continuous-time system. Finally, the Poincaré recurrence of the z˙ dynamics directly imply the Poincaré recurrence of the FTRL strategies. Indeed, since there is an increasing sequence of times tn such that z(tn) → z(0) by Poincaré recurrence, so using continuity there is also an increasing sequence of times tn such that x(tn) = Q(y(tn)) = Qˆ(z(tn)) → Qˆ(z(t0)) = x(0) which means the FTRL dynamics are Poincaré recurrent.

19

Online Learning in Periodic Zero-Sum Games

E FTRL Time-Average Convergence Results: Proofs for Section 5.2

This appendix includes the proofs of Theorem 3 and Proposition 3.

E.1 Proof of Theorem 3

The outline of this proof is as follows. We begin by restating the relevant notation specialized to periodic zero-sum bimatrix games and then provide a more formal mathematical statement of the claim being proven. Following that we introduce a technical result regarding the bounded regret property of FTRL dynamics and state the implications that can be drawn from it. Finally, using the implications of bounded regret and properties of zero-sum bimatrix games we reach the conclusion.

Notation. Recall that we consider a periodic zero-sum bimatrix game for this result, which is a game that consists
of a pair of players i and j and the bimatrix game between them at time t ≥ 0 is described by the pair of payoffs {Aij(t), Aji(t)} = {A(t), −A (t)} and the sequence is periodic so that {Aij(t + T ), Aji(t + T )} = {Aij(t), Aji(t)}
or equivalently {A(t + T ), −A (t + T )} = {A(t), −A (t)} for some ﬁnite period T and all time t ≥ 0. Moreover,
the bimatrix game at each time t ≥ 0 is zero-sum which means that ui(xi, xj, t) + uj(xi, xj, t) = 0 for any strategy pair xi ∈ Xi and xj ∈ Xj. Observe that we include the time-dependence t in the notation of player’s utility to make it explicit the utility is time-dependent as a result of the time-varying payoff matrix. Finally, let the strategy pair (x∗i , x∗j ) ∈ Xi × Xj denote the time-invariant Nash equilibrium.

Formal Statement of Result. Our goal is to prove that the time-average utility of each player converges to the time-average of the values of the games over a period. That is, we seek to show

1 lim

t

1

u (x(τ ), τ )dτ =

T
u (x∗, x∗, τ )dτ = V¯

(19)

t→∞ t 0 i

T0 ii j

and

1 lim

t

1

u (x(τ ), τ )dτ =

T
u (x∗, x∗, τ )dτ = −V¯ ,

(20)

t→∞ t 0 j

T0 jj i

where V¯ := T1 0T V (τ )dτ denotes the time-average of the values of the games over a period and V (τ ) denotes the value of the game at time τ for any τ ≥ 0.

Bounded Regret Property. The proof of the above statement requires an intermediate technical result. The following result of Mertikopoulos et al. [20] states that regardless of what other players do in a polymatrix game (not necessarily zero-sum), if a player follows FTRL learning dynamics then the regret of the player is bounded. It is important to remark that this result directly applies to periodic zero-sum polymatrix games. This follows from the fact that there is no assumptions on the behavior of other players, so the dynamics from the game can be viewed as arising from the behavior of the other players.

Proposition 4 (Theorem 3.1, Mertikopoulos et al. 20). Let hmax,i = maxxi∈Xi hi(xi) and hmin,i = minxi∈Xi hi(xi). If player i ∈ V in a polymatrix game follows FTRL dynamics, then for every continuous trajectory of play x−i(t) of the
opponents of player i the following regret bound holds:

1t

hmax,i − hmin,i

max

ui(xi, x−i(τ ), τ ) − ui(x(τ ), τ ) dτ ≤

.

xi∈Xi t 0

t

Implications of Bounded Regret. Proposition 4 ensures that the following bounds hold for the regret of player i:

1t

∗

1t

t 0 ui(xi , xj(τ ), τ ) − ui(x(τ ), τ ) dτ ≤ xmi∈aXxi t 0 ui(xi, xj(τ ), τ ) − ui(x(τ ), τ ) dτ

≤ hmax,i − hmin,i . (21) t

Similarly, Proposition 4 guarantees the following bounds hold for the regret of player j:

1t

∗

1t

t 0 uj(xj , xi(τ ), τ ) − uj(x(τ ), τ ) dτ ≤ xmj∈aXxj t 0 uj(xj, xi(τ ), τ ) − uj(x(τ ), τ ) dτ

≤ hmax,j − hmin,j . (22) t

20

Online Learning in Periodic Zero-Sum Games

Observe that the lower bounds in (21) and (22) hold by replacing the maximizing argument over the strategy space of a player with a ﬁxed strategy. In particular, the ﬁxed strategy is taken to be the invariant Nash equilibrium strategy for player i or j.
Now, taking the limit as t → ∞ of each side of (21) and using the zero-sum property of the bimatrix game at each time τ ≥ 0, we obtain the following:

1t

∗

1t

∗

tl→im∞ t 0 ui(xi , xj(τ ), τ ) − ui(x(τ ), τ ) dτ = tl→im∞ t 0 ui(xi , xj(τ ), τ ) + uj(x(τ ), τ ) dτ

≤ 0.

(23)

Similarly, taking the limit as t → ∞ of each side of (22) and using the zero-sum property of the bimatrix game at each time τ ≥ 0, we get that:

1t

∗

1t

∗

tl→im∞ t 0 uj(xj , xi(τ ), τ ) − uj(x(τ ), τ ) dτ = tl→im∞ t 0 uj(xj , xi(τ ), τ ) + ui(x(τ ), τ ) dτ

≤ 0.

(24)

Time-Average Utility Convergence. We now proceed to show that

1t

∗∗

1t

1t

∗∗

tl→im∞ t 0 ui(xi , xj , τ )dτ ≤ tl→im∞ t 0 ui(x(τ ), τ )dτ ≤ tl→im∞ t 0 ui(xi , xj , τ )dτ. (25)

The lower bound on the time-average utility of player i holds by the following analysis that is explained below:

1t

1t

∗

tl→im∞ t 0 ui(x(τ ), τ )dτ ≥ tl→im∞ t 0 ui(xi , xj(τ ), τ )dτ (26)

1t

∗

≥ tl→im∞ t 0 xmj∈iXnj ui(xi , xj , τ )dτ (27)

1t

∗∗

= tl→im∞ t 0 ui(xi , xj , τ )dτ. (28)

The inequality in (26) is a direct implication of (23). Moreover, the inequality in (27) is immediate by the fact that any ﬁxed strategy of player j must give at least as much utility to player i as the strategy which minimizes the utility of player i. Finally, the last conclusion in (28) holds by the deﬁnition of a Nash equilibrium in a zero-sum bimatrix game.

The upper bound on the time-average utility of player i holds by the following similar analysis that is detailed below:

1t

1t

∗

tl→im∞ t 0 ui(x(τ ), τ )dτ ≤ − tl→im∞ t 0 uj(xj , xi(τ ), τ )dτ (29)

1t

∗

= tl→im∞ t 0 ui(xi(τ ), xj , τ )dτ (30)

1t

∗

≤ tl→im∞ t 0 xmi∈aXxi ui(xi, xj , τ )dτ (31)

1t

∗∗

= tl→im∞ t 0 ui(xi , xj , τ )dτ. (32)

The inequality in (29) follows directly from (24) and the equality in (30) is a result of the zero-sum property of the game at each time τ ≥ 0. Furthermore, the inequality in (31) holds by the fact that the the strategy of player i that maximizes the utility must give at least as much utility as any ﬁxed strategy. The last conclusion in (32) again holds by the deﬁnition of a Nash equilibrium in a zero-sum bimatrix game.

The preceding arguments prove that the claimed inequalities in (25) hold. Observe that the time-average of the utility values of player i at the invariant Nash equilibrium converge to the time-average of the values of the games over a period as a result of the periodic nature of the game and the fact that the utility value at any Nash equilibrium in a zero-sum bimatrix game is unique. That is,

1 lim t→∞ t

t ∗∗

1

0 ui(xi , xj , τ )dτ = T

T

ui

(x

∗ i

,

x∗j

,

τ

)dτ

=

V¯ .

0

21

Online Learning in Periodic Zero-Sum Games

Thus, the squeeze theorem applied to (25) allows us to conclude the statement given in (19):

1 lim t→∞ t

t

1

0 ui(x(τ ), τ )dτ = T

T

u

i

(x

∗ i

,

x∗j

,

τ

)

dτ

=

V¯

0

Finally, by the zero-sum property of the bimatrix game at each time τ ≥ 0, the statement given in (20) immediately follows from the equation above. That is,

1 lim

t

1

u (x(τ ), τ )dτ =

T
u (x∗, x∗, τ )dτ = −V¯ .

t→∞ t 0 j

T0 ij i

This ﬁnishes the proof.

E.2 Proof of Proposition 3

We now provide the proof of Proposition 3 stating that there exists periodic zero-sum bimatrix games satisfying Deﬁnition 2 in which the time-average strategies of FTRL dynamics fail to converge to the time-invariant Nash equilibrium.
To prove this result we construct a speciﬁc periodic zero-sum bimatrix game that is the basis of the counterexample. In order to demonstrate that the FTRL strategies may not converge to the time-invariant Nash equilibrium, we consider the regularization function that leads to the replicator dynamics. Then, for replicator dynamics in the constructed game, we prove that the strategies are symmetric about the half period of the game and consequently return to the initial condition in a period of the game so that the time-average of the strategies in the limit corresponds to the time-average of the strategies over a half-period of the game. Finally, we use this property to show that the choice of the period of the game can ensure that the time-average strategies cannot converge to the time-invariant Nash equilibrium.
Counterexample Construction. A periodic zero-sum bimatrix game between players i and j is described by a periodic sequence of payoffs where the game at time t ≥ 0 is described by the pair of payoffs {Aij(t), Aji(t)} = {A(t), −A (t)}. To obtain a counterexample, we consider the periodic zero-sum bimatrix game deﬁned by

A(t) = γ(t)A where A = −11 −11

2πt

and γ(t) = sin

.

T

This game corresponds to a periodic version of matching pennies and the period of the game is T . The joint strategy (x∗i , x∗j ) where x∗i = (1/2, 1/2) and x∗j = (1/2, 1/2) is the unique time-invariant Nash equilibrium of the game.
Recall that in periodic zero-sum bimatrix games between players i and j, we denote the utility of each player at time t ≥ 0 under the joint strategy x(t) by ui(x(t), t) and uj(x(t), t) to emphasize the dependence on the time-dependent payoffs which are given by {γ(t)A, −γ(t)A } in this construction. Furthermore, in this problem construction we denote the utility of each player with payoffs {A, −A } under the joint strategy x(t) by ui(x(t)) and uj(x(t)) where A is deﬁned at the matching pennies payoff matrix deﬁned above.

The regularization function hi(xi) = α∈Ai xiα log xiα in FTRL dynamics gives rise to the replicator dynamics commonly studied in evolutionary game theory. For the periodic zero-sum bimatrix game under consideration, the replicator dynamics for any strategy α ∈ Ai of player i are given by

x˙ iα(t) = xiα(t) uiα(x(t), t) − ui(x(t), t) = γ(t)xiα(t) uiα(x(t)) − ui(x(t)) := γ(t)x˙ iα(t).

Similarly, the replicator dynamics for any strategy α ∈ Aj of player j are given by

x˙ jα(t) = xjα(t) ujα(x(t), t) − uj(x(t), t) = γ(t)xjα(t) ujα(x(t)) − uj(x(t)) := γ(t)x˙ jα(t).

We now analyze the time-average of the dynamics of any strategy for each player in this game and show that they do not correspond to the time-invariant Nash equilibrium.

Time-Average Strategies. We begin by showing that for each player k ∈ {i, j} and strategy of the player α ∈ Ak,

xkα T2 + t = xkα T2 − t .

(33)

To see this, observe that for each player k ∈ {i, j} and strategy of the player α ∈ Ak and some initial condition t0,

t

t

xkα(t) = xkα(t0) + x˙ jα(τ )dτ = xkα(t) + sin 2Tπ τ x˙ kα(τ )dτ.

(34)

t0

t0

22

Online Learning in Periodic Zero-Sum Games

To prove the claim in (33), we show that both xkα T2 + t and xkα T2 − t satisfy the same ordinary differential equation and initial condition. That is, we invoke the fundamental theorem of ordinary differential equations which says the solutions exist and are unique so that the claim holds.

Indeed, from (34) and the fundamental theorem of calculus

Similarly,

d dt xkα

T2 + t

= x˙ kα

T2 + t

sin

2π T

T2 + t

= x˙ kα T2 + t sin π + 2Tπ t = −x˙ kα T2 + t sin 2Tπ t .

ddt xkα T2 − t = −x˙ kα T2 − t sin 2Tπ T2 − t = −x˙ kα T2 − t sin π − 2Tπ t = −x˙ kα T2 − t sin 2Tπ t .

We conclude that the functions xkα T2 + t and xkα T2 − t satisfy the same ordinary differential equation. That is,

the functional form of the ordinary differential equation is the same in both expressions. Furthermore, xkα T2 + 0 =

xkα T2 − 0

= xkα

T 2

so they satisfy the same initial condition. Hence, invoking the uniqueness property of the

fundamental theorem of ordinary differential equations, the claim given in (33) holds.

The property in (33) implies for each player k ∈ {i, j} and strategy of the player α ∈ Ak that

1 lim t→∞ t

t

1

0 xkα(τ )dτ = T

T

2

0 xkα(τ )dτ = T

T /2
xkα(τ )dτ.
0

That is, the limiting time-average strategy is equal to the time-average strategy over half a period of the periodic game.

Now recall that the time-invariant Nash equilibrium strategy is given by the joint strategy (x∗i , x∗j ) where x∗i = (1/2, 1/2) and x∗j = (1/2, 1/2). Thus, to ﬁnish the proof we need to show for some player k ∈ {i, j} and strategy of α ∈ Ak that

2 T /2

1

T 0 xkα(τ )dτ = 2 .

To see that the claim above holds, recall from (34) that

t
xkα(t) = xkα(0) + sin 2Tπ τ x˙ kα(τ )dτ.
0

Observe that for any τ ≥ 0, 2π
−1 ≤ sin τ ≤ 1 and T
The previous expressions combine to imply

− 2 ≤ x˙ kα(τ ) ≤ 2.

xkα(0) − 2t ≤ xkα(t) ≤ xkα(0) + 2t

and consequently

T2 xkα(0) − 2 ≤ T

T /2

T

0 xkα(τ )dτ ≤ xkα(0) + 2 .

Finally, suppose that xkα(0) > 1/2. Then, when T < 2(xkα(0) − 1/2), the time-average of the strategy

2 T /2

1

T 0 xkα(τ )dτ > 2 .

Analogously, suppose that xkα(0) < 1/2. Then, when T < 2(1/2 − xkα(0)), the time-average of the strategy

2 T /2

1

T 0 xkα(τ )dτ < 2 .

Thus, unless the players initialize at the time-invariant Nash equilibrium strategy, there is a choice of the period T of the periodic zero-sum matrix such that the time-average of the strategies do not converge to the time-invariant Nash equilibrium. This completes the proof.

23

Online Learning in Periodic Zero-Sum Games

F Experiments

In this section, we present additional simulations and details that serve to strengthen the results in the main paper.

F.1 GDA Results

First, for continuous-time GDA dynamics we show that Poincaré recurrence holds in a periodic zero-sum bilinear game.

We consider the ubiquitous Matching Pennies game with payoff matrix A =

1 −1 −1 1

.

We then use the following

periodic rescaling with period 2π:

sin(t)

0 ≤ t ≤ 3π

α(t) =

2
2 (t mod(2π) − 2π) 3π ≤ t ≤ 2π

(35)

π

2

Hence, the bilinear zero-sum game at time t ≥ 0 is then described by the payoffs {α(t)A(t), −α(t)A(t) }. When agents use GDA learning dynamics, we see from Figure 1 that the agents’ trajectories when plotted alongside the value of the periodic rescaling are bounded.

Another key result in the space of GDA learning dynamics is that the time-average behavior fails to converge in general to the time invariant equilibrium (0, 0). A simple counterexample can be constructed by considering a periodic zero-sum bilinear game with x1, x2 ∈ R and a periodic rescaling β(t) such that β(t) = β(t + T ) with T = 3π for any t ≥ 0. Moreover, let the rescaling evolve over a period as follows:

 −1 0 ≤ t ≤ π



β(t) =

1 π ≤ t ≤ 32π

(36)

 −1 32π ≤ t ≤ 3π.

For the simulation, we consider the payoff matrices described by {β(t)A, −β(t)A } where A =

1 −1 −1 1

. We show

for this example that when both players use GDA, the time average strategy of each player remains bounded away from

the Nash [1/2, 1/2], as shown in Figure 5.

Figure 5: Time-average convergence away from the mixed Nash
F.2 FTRL Results
For the case of FTRL dynamics, we perform simulations on Matching Pennies updated with replicator dynamics. The reader is reminded of the deﬁnition of replicator dynamics as a continuous analogue of multiplicative weights update, as described in Section 5. In polymatrix games, replicator dynamics for each i ∈ V uses regularization function hi(xi) = α∈Ai xiα log xiα in the FTRL dynamics. Like the GDA case, we also use the periodic rescaling described in Equation 35 and obtain recurrent dynamics, as seen in Figure 6. Theorem 3 states that the time-average utility of each player converges to the time-average value of periodic zero-sum games when each player follows FTRL dynamics. However, Proposition 3 states that there exist periodic zero-sum bimatrix games where the time-average strategies of FTRL dynamics fail to converge to the time-invariant Nash equilibrium. Here, we show a simple example that exhibits both results. Consider a Matching Pennies game that is
24

Online Learning in Periodic Zero-Sum Games
Figure 6: Periodically Rescaled Matching Pennies (Replicator) rescaled with a sin function. Speciﬁcally, the periodic bimatrix game is given by
A(t) = sin(t) −11 −11 (37) Even with this simple example, the time-average utilities for both players go to zero, while the time-average strategies do not converge to the [1/2, 1/2] time-invariant Nash, as seen in Figure 7.
Figure 7: Time average results for MP rescaled with sin function. Notice that although the average utility goes to 0, the time average actions/strategy does not go to the time invariant Nash [1/2, 1/2]. F.3 Large Scale Simulation We now perform simulations on larger-scale systems that present our ﬁndings in a more visually striking manner. Firstly, the time-invariant function presented in Lemma 3 and its proof can be demonstrated in any two-player periodic zero-sum polymatrix game. For replicator dynamics, the invariant function is the KL-divergence between each player’s strategy and the unique mixed Nash. Using the same simulated data that was used to generate Figure 6, we show that the sum of divergences is indeed constant when both agents are using replicator dynamics. Figure 8 shows this phenomenon in the case where there are just two players playing a periodically rescaled Matching Pennies game. Speciﬁcally, the blue area represents the KL-divergence of the ﬁrst player from the mixed Nash over time, and the green area represents the divergence of the second player. To extend this formulation to the multiplayer setting, we implement a graphical polymatrix game where a number of agents are arranged in a line. Each agent then plays a bimatrix game against the agent directly adjacent to them, and the ﬁnal agent also plays against the ﬁrst agent. This results in a ‘toroid’-like chain of games, where each agent plays
25

Online Learning in Periodic Zero-Sum Games
Figure 8: Time invariant function for two player periodically rescaled Matching Pennies game against two other agents. In our simulation, each pair of agents plays the Matching Pennies game rescaled with sin against each other (Equation 37). With this system, we simulated a chain of games with 64 nodes (agents), resulting in much more complex dynamics than the two player case. Indeed, in Figure 10 we show zoomed-in plots of Figure 2. Here, similar to previous plots, each player’s KL-divergence is represented by a different-colored area. The ﬁgure showcases that on a more granular level, the individual KL-divergences of each agent can become extremely erratic, and look nowhere near periodic. Nevertheless, we see from Figure 2 that the sum of KL-divergences remains constant.
Figure 9: 8 × 8 grid of colors generated by sigmoid function We also represent the trajectories of each agent by equating the strategy values of each player to RGB values in an 8x8 grid. In particular, the color of each pixel on the grid represents the probability of the respective player playing the ﬁrst strategy, tuned with a sigmoid function. We then select initial conditions the correspond to RGB values such that they form the image shown in Figure 9. With the sigmoid function, any changes from the initial condition are reﬂected by changes in the color of the individual pixels. Thus, as agents play pairwise bimatrix games using replicator dynamics, the colors of the grid evolve. If recurrence holds, we expect to see the same image after some time. For the case of the Matching Pennies games rescaled with sin, we obtain the various images found in Figure 3, which exhibit recurrence. We also provide an animation showing how the ﬁgure evolves over time in the supplementary Jupyter notebook. F.4 Reproducibility Details All experiments performed for this work were done using Python 3.7 and have been compiled into a Jupyter notebook for ease of viewing. Running the code requires only basic scientiﬁc computing packages such as NumPy and SciPy, as well as data visualization packages such as Matplotlib and Plotly. Most of the code in our submission has been edited such that it can be easily executed on a standard computer in a matter of minutes.
26

Online Learning in Periodic Zero-Sum Games
Figure 10: Zoomed-in time invariant functions for 64-player game.
Appendix References
[1] Alexander, J.A. & Mozer, M.C. (1995) Template-based algorithms for connectionist rule extraction. In G. Tesauro, D.S. Touretzky and T.K. Leen (eds.), Advances in Neural Information Processing Systems 7, pp. 609–616. Cambridge, MA: MIT Press. [2] Bower, J.M. & Beeman, D. (1995) The Book of GENESIS: Exploring Realistic Neural Models with the GEneral NEural SImulation System. New York: TELOS/Springer–Verlag. [3] Hasselmo, M.E., Schnell, E. & Barkai, E. (1995) Dynamics of learning and recall at excitatory recurrent synapses and cholinergic modulation in rat hippocampal region CA3. Journal of Neuroscience 15(7):5249-5262.
27

