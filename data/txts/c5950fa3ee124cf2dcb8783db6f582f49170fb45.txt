RATT: Leveraging Unlabeled Data to Guarantee Generalization

arXiv:2105.00303v2 [cs.LG] 6 Nov 2021

Saurabh Garg 1 Sivaraman Balakrishnan 1 J. Zico Kolter 1 Zachary C. Lipton 1

Abstract
To assess generalization, machine learning scientists typically either (i) bound the generalization gap and then (after training) plug in the empirical risk to obtain a bound on the true risk; or (ii) validate empirically on holdout data. However, (i) typically yields vacuous guarantees for overparameterized models; and (ii) shrinks the training set and its guarantee erodes with each reuse of the holdout set. In this paper, we leverage unlabeled data to produce generalization bounds. After augmenting our (labeled) training set with randomly labeled data, we train in the standard fashion. Whenever classiﬁers achieve low error on the clean data but high error on the random data, our bound ensures that the true risk is low. We prove that our bound is valid for 0-1 empirical risk minimization and with linear classiﬁers trained by gradient descent. Our approach is especially useful in conjunction with deep learning due to the early learning phenomenon whereby networks ﬁt true labels before noisy labels but requires one intuitive assumption. Empirically, on canonical computer vision and NLP tasks, our bound provides non-vacuous generalization guarantees that track actual performance closely. This work enables practitioners to certify generalization even when (labeled) holdout data is unavailable and provides insights into the relationship between random label noise and generalization. Code is available at https://github.com/acmilab/RATT generalization bound.
1. Introduction
Typically, machine learning scientists establish generalization in one of two ways. One approach, favored by learning theorists, places an a priori bound on the gap between the empirical and true risks, usually in terms of the complex-
1Carnegie Mellon University. Correspondence to: Saurabh Garg <sgarg2@andrew.cmu.edu>.
Proceedings of the 38 th International Conference on Machine Learning, PMLR 139, 2021. Copyright 2021 by the author(s).

ity of the hypothesis class. After ﬁtting the model on the available data, one can plug in the empirical risk to obtain a guarantee on the true risk. The second approach, favored by practitioners, involves splitting the available data into training and holdout partitions, ﬁtting the models on the former and estimating the population risk with the latter.
Surely, both approaches are useful, with the former providing theoretical insights and the latter guiding the development of a vast array of practical technology. Nevertheless, both methods have drawbacks. Most a priori generalization bounds rely on uniform convergence and thus fail to explain the ability of overparameterized networks to generalize (Zhang et al., 2016; Nagarajan & Kolter, 2019b). On the other hand, provisioning a holdout dataset restricts the amount of labeled data available for training. Moreover, risk estimates based on holdout sets lose their validity with successive re-use of the holdout data due to adaptive overﬁtting (Murphy, 2012; Dwork et al., 2015; Blum & Hardt, 2015). However, recent empirical studies suggest that on large benchmark datasets, adaptive overﬁtting is surprisingly absent (Recht et al., 2019).
In this paper, we propose Randomly Assign, Train and Track (RATT), a new method that leverages unlabeled data to provide a post-training bound on the true risk (i.e., the population error). Here, we assign random labels to a fresh batch of unlabeled data, augmenting the clean training dataset with these randomly labeled points. Next, we train on this data, following standard risk minimization practices. Finally, we track the error on the randomly labeled portion of training data, estimating the error on the mislabeled portion and using this quantity to upper bound the population error.
Counterintuitively, we guarantee generalization by guaranteeing overﬁtting. Speciﬁcally, we prove that Empirical Risk Minimization (ERM) with 0-1 loss leads to lower error on the mislabeled training data than on the mislabeled population. Thus, if despite minimizing the loss on the combined training data, we nevertheless have high error on the mislabeled portion, then the (mislabeled) population error will be even higher. Then, by complementarity, the (clean) population error must be low. Finally, we show how to obtain this guarantee using randomly labeled (vs mislabeled data), thus enabling us to incorporate unlabeled data.
To expand the applicability of our idea beyond ERM on 0-1

Leveraging Unlabeled Data to Guarantee Generalization

100

MLP

90

ResNet

Test Predicted bound

Accuracy

80

70

60

50 0

20

40

60

80 100

Epoch

Figure 1. Predicted lower bound on the clean population error with ResNet and MLP on binary CIFAR. Results aggregated over 5 seeds. ‘*’ denotes the best test performance achieved when training with only clean data and the same hyperparameters (except for the stopping point). The bound predicted by RATT (RHS in (2)) closely tracks the population accuracy on clean data.

error, we prove corresponding results for a linear classiﬁer trained by gradient descent to minimize squared loss. Furthermore, leveraging the connection between early stopping and 2-regularization in linear models (Ali et al., 2018; 2020; Suggala et al., 2018), our results extend to early-stopped gradient descent. Because we make no assumptions on the data distribution, our results on linear models hold for more complex models such as kernel regression and neural networks in the Neural Tangent Kernel (NTK) regime (Jacot et al., 2018; Du et al., 2018; 2019; Allen-Zhu et al., 2019b; Chizat et al., 2019).
Addressing practical deep learning models, our guarantee requires an additional (reasonable) assumption. Our experiments show that the bound yields non-vacuous guarantees that track test error across several major architectures on a range of benchmark datasets for computer vision and Natural Language Processing (NLP). Because, in practice, overparameterized deep networks exhibit an early learning phenomenon, ﬁtting clean data before mislabeled data (Liu et al., 2020; Arora et al., 2019; Li et al., 2019), our procedure yields tight bounds in the early phases of learning. Experimentally, we conﬁrm the early learning phenomenon in standard Stochastic Gradient Descent (SGD) training and illustrate the effectiveness of weight decay combined with large initial learning rates in avoiding interpolation to mislabeled data while maintaining ﬁt on the training data, strengthening the guarantee provided by our method.
To be clear, we do not advocate RATT as a blanket replacement for the holdout approach. Our main contribution is to introduce a new theoretical perspective on generalization and to provide a method that may be applicable even when the holdout approach is unavailable. Of interest, unlike generalization bounds based on uniform-convergence that restrict the complexity of the hypothesis class (Neyshabur

et al., 2018; 2015; 2017b; Bartlett et al., 2017; Nagarajan & Kolter, 2019a), our post hoc bounds depend only on the ﬁt to mislabeled data. We emphasize that our theory does not guarantee a priori that early learning should take place but only a posteriori that when it does, we can provide nonvacuous bounds on the population error. Conceptually, this ﬁnding underscores the signiﬁcance of the early learning phenomenon in the presence of noisy labels and motivates further work to explain why it occurs.

2. Preliminaries

By ||¨||, and x¨, ¨y we denote the Euclidean norm and inner product, respectively. For a vector v P Rd, we use vj to denote its jth entry, and for an event E we let I rEs denote
the binary indicator of the event.

Suppose we have a multiclass classiﬁcation problem with

the input domain X Ď Rd and label space Y “

t1, 2, . . . , ku1. By D, we denote the distribution over X ˆY.

A dataset S :“ tpxi, yiquni“1 „ Dn contains n points sam-

pled i.i.d. from D. By S, T , and Sr, we denote the (uni-

form) empirical distribution over points in datasets S, T ,

and Sr, respectively. Let F be a class of hypotheses map-

ping X to Rk. A training algorithm A: takes a dataset S

and returns a classiﬁer f pA, Sq P F. When the context

is clear, we drop the parentheses for convenience. Given

a classiﬁer f and datum px, yq, we denote the 0-1 error

(i.e., classiﬁcation error) on that point by Epf pxq, yq :“ I “y R arg maxjPY fjpxq‰, We express the population error on D as EDpf q :“ Epx,yq„D rEpf pxq, yqs and the em-

pirical error on S as ES pf q :“ Epx,yq„S rEpf pxq, yqs “

1 n

řn
i“1

Epf pxiq,

yiq.

Throughout, we consider a random label assignment proce-
dure: draw x „ DX (the underlying distribution over X ), and then assign a label sampled uniformly at random. We denote a randomly labeled dataset by Sr :“ tpxi, yiqum i“1 „ Drm, where Dr is the distribution of randomly labeled data. By D1, we denote the mislabeled distribution that corre-
sponds to selecting examples px, yq according to D and
then re-assigning the label by sampling among the incorrect labels y1 ‰ y (renormalizing the label marginal).

3. Generalization Bound for RATT with ERM
We now present our generalization bound and proof sketches for ERM on the 0-1 loss (full proofs in App. A). For any dataset T , ERM returns the classiﬁer fp that minimizes the empirical error:

fp :“ arg min ET pf q .

(1)

f PF

1For binary classiﬁcation, we use Y “ t´1, 1u.

Leveraging Unlabeled Data to Guarantee Generalization

We focus ﬁrst on binary classiﬁcation. Assume we have a clean dataset S „ Dn of n points and a randomly labeled dataset Sr „ Drm of m pă nq points with labels in Sr are assigned uniformly at random. We show that with 0-1 loss minimization on the union of S and Sr, we obtain a classiﬁer whose error on D is upper bounded by a function of the empirical errors on clean data ES (lower is better) and on randomly labeled data ES (higher is better):
r
Theorem 1. For any classiﬁer fp obtained by ERM (1) on dataset S Y Sr, for any δ ą 0, with probability at least 1 ´ δ, we have

EDpfpq ď ES pfpq ` 1 ´ 2ES pfpq
r

c

´?

m ¯ logp4{δq

`

2ES pfpq
r

`

2

`

2n

. (2) m

In short, this theorem tells us that if after training on both clean and randomly labeled data, we achieve low error on the clean data but high error (close to 1{2) on the randomly labeled data, then low population error is guaranteed. Note that because the labels in Sr are assigned randomly, the error ES pf q for any ﬁxed predictor f (not dependent on Sr) will be
r
approximately 1/2. Thus, if ERM produces a classiﬁer that
has not ﬁt to the randomly labeled data, then p1 ´ 2ES pfpqq
r
will be approximately 0, and our error will be determined by the ﬁt to clean data. The ﬁnal term accounts for ﬁnite sample error—notably, it (i) does not depend on the complexi?ty of the hypothesis class; and (ii) approaches 0 at a Op1{ mq rate (for m ă n).

Our proof strategy unfolds in three steps. First, in Lemma 1 we bound EDpfpq in terms of the error on the mislabeled subset of Sr. Next, in Lemmas 2 and 3, we show that the error on the mislabeled subset can be accurately estimated using only clean and randomly labeled data.

To begin, assume that we actually knew the original labels

for the randomly labeled data. By SrC and SrM , we denote

the clean and mislabeled portions of the randomly labeled

data, respectively (with Sr “ SrM Y SrC ). Note that for

binary classiﬁcation, a lower bound on mislabeled popu-

lation error ED1 pfpq directly upper bounds the error on the

original population EDpfpq. Thus we only need to prove

that the empirical error on the mislabeled portion of our

data is lower than the error on unseen mislabe?led data, i.e.,

ES pfpq ď ED1 pfpq “ 1 ´ ES pfpq (upto Op1{ mq).

rM

rM

Lemma 1. Assume the same setup as in Theorem 1. Then

for any δ ą 0, with probability at least 1 ´ δ over the

random draws of mislabeled data SrM , we have

c

logp1{δq

EDpfpq ď 1 ´ ES pfpq `
rM

. m

(3)

Proof Sketch. The main idea of our proof is to regard the clean portion of the data (SYSrC) as ﬁxed. Then, there exists

a classiﬁer f ˚ that is optimal over draws of the mislabeled data SrM . Formally,
f ˚ :“ arg min EDpf q,
q f PF

where Dq is a combination of the empirical distribution

over correctly labeled data S Y SrC and the (population)

distribution over mislabeled data D1. Recall that fp :“

arg minfPF ESYS pf q. Since, fp minimizes 0-1 error on
r

S Y Sr, we have ESYS pfpq ď ESYS pf ˚q. Moreover, since

r

r

f ˚ is independent of SrM , we have with probability at least

1 ´ δ that

c

ES pf ˚q ď ED1 pf ˚q `
rM

logp1{δq . m

Finally, since f ˚ is the optimal classiﬁer on Dq, we have

EDpf ˚q ď EDpfpq. Combining the above steps and using

q

q

the fact that ED “ 1 ´ ED1 , we obtain the desired result.

While the LHS in (3) depends on the unknown portion SrM ,

our goal is to use unlabeled data (with randomly assigned

labels) for which the mislabeled portion cannot be read-

ily identiﬁed. Fortunately, we do not need to identify the

mislabeled points to estimate the error on these points in

aggregate ES pfpq. Note that because the label marginal is
rM
uniform, approximately half of the data will be correctly

labeled and the remaining half will be mislabeled. Conse-

quently, we can utilize the value of ES pfpq and an estimate
r

of ES pfpq to lower bound ES pfpq. We formalize this as

rC

rM

follows:

Lemma 2. Assume the same setup as Theorem 1. Then for any δ ą 0, with probability at least 1 ´ δ over the ran-

dom draws of Sr, we have 2ES pfpq ´ ES pfpq ´ ES pfpq ď

r

rC

rM

b 2ESrpfpq log2pm4{δq .

To complete the argument, we show that due to the exchangeability of the clean data S and the clean portion of the randomly labeled data SC, we can estimate the error on the latter ES pfpq by the error on the former ES pfpq.
rC
Lemma 3. Assume the same setup as Theorem 1. Then for any δ ą 0, with probability at least 1 ´ δ over the
random draws of SrC and S, we have ES pfpq ´ ES pfpq ď
rC
b `1 ` 2mn ˘ logm p2{δq .

Lemma 3 establishes a tight bound on the difference of the error of classiﬁer fp on SrC and on S. The proof uses Hoeffding’s inequality for randomly sampled points from a ﬁxed population (Hoeffding, 1994; Bardenet et al., 2015).

Having established these core components, we can now summarize the proof strategy for Theorem 1. We bound the

Leveraging Unlabeled Data to Guarantee Generalization

population error on clean data (the term on the LHS of (2))

in three steps: (i) use Lemma 1 to upper bound the error on

clean distribution EDpfpq, by the error on mislabeled training

data ES pfpq; (ii) approximate ES pfpq by ES pfpq and the

rM

rM

rC

error on randomly labeled training data (i.e., ES pfpq) using
r

Lemma 2; and (iii) use Lemma 3 to estimate ES pfpq using
rC

the error on clean training data (ES pfpq).

Comparison with Rademacher bound Our bound in Theorem 1 shows that we can upper bound the clean population error of a classiﬁer by estimating its accuracy on the clean and randomly labeled portions of the training data. Next, we show that our bound’s dominating term is upper bounded by the Rademacher complexity (Shalev-Shwartz & Ben-David, 2014), a standard distribution-dependent complexity measure.
Proposition 1. Fix a randomly labeled dataset Sr „ Drm. Then for any classiﬁer f P F (possibly dependent on Sr)2 and for any δ ą 0, with probability at least 1 ´ δ over random draws of Sr, we have

d

«

ﬀ

1 ´ 2E pf q ď E ,x

ˆř sup i

if pxiq ˙

`

Sr

f PF

m

2 logp 2δ q , m

where is drawn from a uniform distribution over t´1, 1um and x is drawn from DXm.

In other words, the proposition above highlights that the accuracy on the randomly labeled data is never larger than the Rademacher complexity of F w.r.t. the underlying distribution over X , implying that our bound is never looser than a bound based on Rademacher complexity. The proof follows by application of the bounded difference condition and McDiarmid’s inequality (McDiarmid, 1989). We now discuss extensions of Theorem 1 to regularized ERM and multiclass classiﬁcation.

Extension to regularized ERM Consider any function R : F Ñ R, e.g., a regularizer that penalizes some measure of complexity for functions in class F. Consider the following regularized ERM:

fp :“ arg min ES pf q ` λRpf q ,

(4)

f PF

where λ is a regularization constant. If the regularization coefﬁcient is independent of the training data S Y Sr, then our guarantee (Theorem 1) holds. Formally,
Theorem 2. For any regularization function R, assume we perform regularized ERM as in (4) on S Y Sr and obtain a classiﬁer fp. Then, for any δ ą 0, with probability at
2We restrict F to functions which output a label in Y “ t´1, 1u.

least 1 ´ δ, we have EDpfpq ď ES pfpq ` 1 ´ 2ES pfpq `

r

´? 2E

pf q ` 2 `

¯b

m

logp1{δq .

Sr p

2n

m

A key insight here is that the proof of Theorem 1 treats the clean data S as ﬁxed and considers random draws of the mislabeled portion. Thus a data-independent regularization function does not alter our chain of arguments and hence, has no impact on the resulting inequality. We prove this result formally in App. A.
We note one immediate corollary from Theorem 2: when learning any function f parameterized by w with L2-norm penalty on the parameters w, the population error with fp is determined by the error on the clean training data as long as the error on randomly labeled data is high (close to 1{2).

Extension to multiclass classiﬁcation Thus far, we have addressed binary classiﬁcation. We now extend these results to the multiclass setting. As before, we obtain datasets S and Sr. Here, random labels are assigned uniformly among all classes.

Theorem 3. For any regularization function R, assume we

perform regularized ERM as in (4) on S Y Sr and obtain a

classiﬁer fp. For a multiclass classiﬁcation problem with k

classes, for any δ ą 0, with probability at least 1 ´ δ, we

have

´

¯

EDpfpq ď ES pfpq ` pk ´ 1q 1 ´ k´k 1 ES pfpq

r

d ` c logp 4δ q , (5)
2m

? for some constant c ď p2k ` k ` m? q.
nk

We ﬁrst discuss the implications of Theorem 3. Besides

empirical error on clean data, the dominating term in the

´

¯

above expression is given by pk ´ 1q 1 ´ k´k 1 ES pfpq . For

r

any predictor f (not dependent on Sr), the term ES pfpq would
r
be approximately pk ´ 1q{k and for fp, the difference now evaluates to the accuracy of the randomly labeled data. Note that for binary classiﬁcation, (5) simpliﬁes to Theorem 1.

The core of our proof involves obtaining an inequality similar to (3). While for binary classiﬁcation, we could upper bound ES with 1 ´ ED (in the proof of Lemma 1), for
rM
multiclass classiﬁcation, error on the mislabeled data and accuracy on the clean data in the population are not so directly related. To establish an inequality analogous to (3), we break the error on the (unknown) mislabeled data into two parts: one term corresponds to predicting the true label on mislabeled data, and the other corresponds to predicting neither the true label nor the assigned (mis-)label. Finally, we relate these errors to their population counterparts to establish an inequality similar to (3).

Leveraging Unlabeled Data to Guarantee Generalization

4. Generalization Bound for RATT with Gradient Descent

In the previous section, we presented results with ERM on 0-1 loss. While minimizing the 0-1 loss is hard in general, these results provide important theoretical insights. In this section, we show parallel results for linear models trained with Gradient Descent (GD).

To begin, we introduce the setup and some additional no-
tation. For simplicity, we begin discussion with binary classiﬁcation with X “ Rd. Deﬁne a linear function f px; wq :“ wT x for some w P Rd and x P X . Given training set S, we suppose that the parameters of the linear
function are obtained via gradient descent on the following L2 regularized problem:

n

LSpw; λq

:“

ÿ

pwT xi

´

yiq2

`

λ

||

w

||

2 2

,

(6)

i“1

where λ ě 0 is a regularization parameter. Our choice to analyze squared loss minimization for linear networks is motivated in part by its analytical convenience, and follows recent theoretical work which analyze neural networks trained via squared loss minimization in the Neural Tangent Kernel (NTK) regime when they are well approximated by linear networks (Jacot et al., 2018; Arora et al., 2019; Du et al., 2019; Hu et al., 2019). Moreover, recent research suggests that for classiﬁcation tasks, squared loss minimization performs comparably to cross-entropy loss minimization (Muthukumar et al., 2020; Hui & Belkin, 2020).

For a given training set S, we use Spiq to denote the training set S with the ith point removed. We now introduce one stability condition:
Condition 1 (Hypothesis Stability). We have β hypothesis stability if our training algorithm A satisﬁes the following for all i P t1, 2, . . . , nu:

“

`

˘‰ β

ES,px,yqPD E pf pxq, yq ´ E fpiqpxq, y ď n ,

where fpiq :“ f pA, Spiqq and f :“ f pA, Sq.

This condition is similar to a notion of stability called hypothesis stability (Bousquet & Elisseeff, 2002; Kearns & Ron, 1999; Elisseeff et al., 2003). Intuitively, Condition 1 states that empirical leave-one-out error and average population error of leave-one-out classiﬁers are close. This condition is mild and does not guarantee generalization. We discuss the implications in more detail in App. B.3.

Now we present the main result of this section. As

before, we assume access to a clean dataset S “

tp

x

i

,

yi

qu

n i“

1

„

Dn

and randomly labeled dataset Sr

“

tpxi, yiquni“`nm`1 „ Drm. Let X “ rx1, x2, ¨ ¨ ¨ , xm`ns and

y “ ry1, y2, ¨ ¨ ¨ , ym`ns. Fix a positive learning rate η such

´

¯

that η ď 1{ ˇˇˇˇXT Xˇˇˇˇop ` λ2 and an initialization w0 “ 0.

Consider the following gradient descent iterates to minimize

objective (6) on S Y Sr:

wt “ wt´1 ´ η∇wLSYSpwt´1; λq @t “ 1, 2, . . . . (7)
r

Then we have twtu converge to the limiting solution w “ p
`XT X ` λI˘´1 XT y. Deﬁne fppxq :“ f px; wq. p
Theorem 4. Assume that this gradient descent algorithm satisﬁes Condition 1 with β “ Op1q. Then for any δ ą 0, with probability at least 1 ´ δ over the random draws of datasets Sr and S, we have:

d

EDpfpq ď ES pfpq ` 1 ´ 2ES pfpq `
r

4 ˆ 1 3β ˙ δ m ` m`n

c

´?

m ¯ logp4{δq

`

2ES pfpq
r

`

1

`

2n

. m

(8)

With a mild regularity condition, we establish the same bound on GD training with squared loss, notably the same dominating term on the population error, as in Theorem 1. In App. B.2, we present the extension to multiclass classiﬁcation, where we again obtain a result parallel to Theorem 3.

Proof Sketch. Because squared loss minimization does not imply 0-1 error minimization, we cannot use arguments from Lemma 1. This is the main technical difﬁculty. To compare the 0-1 error at a train point with an unseen point, we use the closed-form expression for w. We show that
p the train error on mislabeled points is less than the population error on the distribution of mislabeled data (parallel to Lemma 1).

For a mislabeled training point pxi, yiq in Sr, we show that

I “yixTi w ď 0‰ ď I “yixTi wpiq ď 0‰ ,

(9)

p

p

where wppiq is the classiﬁer obtained by leaving out the ith point from the training set. Intuitively, this condition states that the train error at a training point is less than the leaveone-out error at that point, i.e. the error obtained by removing that point and re-training. Using Condition 1, we then relate the average leave-one-out error (over the index i of the RHS in (9)) to the population error on the mislabeled distribution to obtain an inequality similar to (3).

Extensions to kernel regression Since the result in Theorem 4 does not impose any regularity conditions on the underlying distribution over X ˆ Y, our guarantees extend straightforwardly to kernel regression by using the transformation x Ñ φpxq for some feature transform function φ. Furthermore, recent literature has pointed out a concrete connection between neural networks and kernel regression with

Leveraging Unlabeled Data to Guarantee Generalization

the so-called Neural Tangent Kernel (NTK) which holds in a certain regime where weights do not change much during training (Jacot et al., 2018; Du et al., 2019; 2018; Chizat et al., 2019). Using this concrete correspondence, our bounds on the clean population error (Theorem 4) extend to wide neural networks operating in the NTK regime.

Extensions to early stopped GD Often in practice, gradi-

ent descent is stopped early. We now provide theoretical ev-

idence that our guarantees may continue to hold for an early

stopped GD iterate. Concretely, we show that in expectation,

the outputs of the GD iterates are close to that of a problem

with data-independent regularization (as considered in The-

orem 2). First, we introduce some notation. By LSpwq,

we denote the objective in (6) with λ “ 0. Consider the

GD

iterates

deﬁned

in

(7).

Let

wλ r

“

arg minw

LS pw; λq.

Deﬁne ftpxq :“ f px; wtq as the solution at the tth iterate

and frλpxq :“ f px; wλq as the regularized solution. Let κ be r

the condition number of the population covariance matrix

and let smin be the minimum positive singular value of the

empirical covariance matrix.

Proposition 2 (informal). For λ “ t1η , we have

”

ı

Ex„DX pftpxq ´ frλpxqq2 ď cpt, ηq ¨ Ex„DX “ftpxq2‰ ,

where cpt, ηq « κ ¨ minp0.25, s2min1t2η2 q. An equivalent guarantee holds for a point x sampled from the training data.

The proposition above states that for large enough t, GD iterates stay close to a regularized solution with dataindependent regularization constant. Together with our guarantees in Theorem 4 for regularization solution with λ “ t1η , Proposition 2 shows that our guarantees with RATT may hold on early stopped GD. See the formal result in App. B.4.
Remark Proposition 2 only bounds the expected squared difference between the tth gradient descent iterate and a corresponding regularized solution. The expected squared difference and the expected difference of classiﬁcation errors (what we wish to bound) are not related, in general. However, they can be related under standard low-noise (margin) assumptions. For instance, under the Tsybakov noise condition (Tsybakov et al., 1997; Yao et al., 2007), we can lower-bound the expression on the LHS of Proposition 2 with the difference of expected classiﬁcation error.

Extensions to deep learning Note that the main lemma underlying our bound on (clean) population error states that when training on a mixture of clean and randomly labeled data, we obtain a classiﬁer whose empirical error on the mislabeled training data is lower than its population error on the distribution of mislabeled data. We prove this for ERM on 0-1 loss (Lemma 1). For linear models (and networks in NTK regime), we obtained this result by assuming hypothesis stability and relating training error at a datum with the

leave-one-out error (Theorem 4). However, to extend our bound to deep models we must assume that training on the mixture of random and clean data leads to overﬁtting on the random mixture. Formally:
Assumption 1. Let fp be a model obtained by training with an algorithm A on a mixture of clean data S and randomly labeled data Sr. Then with probability 1 ´ δ over the random draws of mislabeled data SrM , we assume that the following condition holds:

c

logp1{δq

ES pfpq ď ED1 pfpq ` c
rM

, 2m

for a ﬁxed constant c ą 0.

Under Assumption 1, our results in Theorem 1, 2 and 3 extend beyond ERM with the 0-1 loss to general learning algorithms. We include the formal result in App. B.5. Note that given the ability of neural networks to interpolate the data, this assumption seems uncontroversial in the later stages of training. Moreover, concerning the early phases of training, recent research has shown that learning dynamics for complex deep networks resemble those for linear models (Nakkiran et al., 2019; Hu et al., 2020), much like the wide neural networks that we do analyze. Together, these arguments help to justify Assumption 1 and hence, the applicability of our bound in deep learning. Motivated by our analysis on linear models trained with gradient descent, we discuss conditions in App. B.6 which imply Assumption 1 for constant values δ ą 0. In the next section, we empirically demonstrate applicability of our bounds for deep models.

5. Empirical Study and Implications
Having established our framework theoretically, we now demonstrate its utility experimentally. First, for linear models and wide networks in the NTK regime where our guarantee holds, we conﬁrm that our bound is not only valid, but closely tracks the generalization error. Next, we show that in practical deep learning settings, optimizing crossentropy loss by SGD, the expression for our (0-1) ERM bound nevertheless tracks test performance closely and in numerous experiments on diverse models and datasets is never violated empirically.
Datasets To verify our results on linear models, we consider a toy dataset, where the class conditional distribution ppx|yq for each label is Gaussian. For binary tasks, we use binarized CIFAR-10 (ﬁrst 5 classes vs rest) (Krizhevsky & Hinton, 2009), binary MNIST (0-4 vs 5-9) (LeCun et al., 1998) and IMDb sentiment analysis dataset (Maas et al., 2011). For multiclass setup, we use MNIST and CIFAR-10.

Leveraging Unlabeled Data to Guarantee Generalization

Accuracy Accuracy Accuracy

100

Underparameterized model

MSE

Test

90

CE

Predicted bound

80

70

60

50 0.0 0.1 0.2 0.3 0.4 Fraction of unlabeled data

(a)

100 90 80 70 60 50 0.0

MNIST

SGD Early stop Weight decay

Test Predicted bound

0.1 0.2 0.3 0.4 Fraction of unlabeled data

(b)

100 90 80 70 60 50 0.0

ELMo-LSTM BERT

Test Predicted bound

0.2 0.4 0.6 0.8 1.0 Fraction of steps

(c)

Figure 2. We plot the accuracy and corresponding bound (RHS in (1)) at δ “ 0.1. for binary classiﬁcation tasks. Results aggregated over 3 seeds. (a) Accuracy vs fraction of unlabeled data (w.r.t clean data) in the toy setup with a linear model trained with GD. (b) Accuracy vs fraction of unlabeled data for a 2-layer wide network trained with SGD on binary MNIST. With SGD and no regularization (red curve in (b)), we interpolate the training data and hence the predicted lower bound is 0. However, with early stopping (or weight decay) we obtain tight guarantees. (c) Accuracy vs gradient iteration on IMDb dataset with unlabeled fraction ﬁxed at 0.2. In plot (c), ‘*’ denotes the best test accuracy with the same hyperparameters and training only on clean data. See App. C for exact hyperparameter values.

Architectures To simulate the NTK regime, we experiment with 2-layered wide networks both (i) with the second layer ﬁxed at random initialization; (ii) and updating both layers’ weights. For vision datasets (e.g., MNIST and CIFAR10), we consider (fully connected) multilayer perceptrons (MLPs) with ReLU activations and ResNet18 (He et al., 2016). For the IMDb dataset, we train Long ShortTerm Memory Networks (LSTMs; Hochreiter & Schmidhuber (1997)) with ELMo embeddings (Peters et al., 2018) and ﬁne-tune an off-the-shelf uncased BERT model (Devlin et al., 2018; Wolf et al., 2020).
Methodology To bound the population error, we require access to both clean and unlabeled data. For toy datasets, we obtain unlabeled data by sampling from the underlying distribution over X . For image and text datasets, we hold out a small fraction of the clean training data and discard their labels to simulate unlabeled data. We use the random labeling procedure described in Sec. 2. After augmenting clean training data with randomly labeled data, we train in the standard fashion. See App. C for experimental details.

in the fraction of unlabele?d data, we observe a relatively sharper decrease in Op p1{ mq term of the bound, leading to an overall increase in the predicted accuracy bound. In this toy setup, we also evaluated a kernel regression bound from Bartlett & Mendelson (2002) (Theorem 21), however, the predicted kernel regression bound remains vacuous.
Wide Nets Next, we consider MNIST binary classiﬁcation with a wide 2-layer fully-connected network. In experiments with SGD training on MSE loss without early stopping or weight decay regularization, we ﬁnd that adding extra randomly label data hurts the unseen clean performance (Fig. 2(b)). Additionally, due to the perfect ﬁt on the training data, our bound is rendered vacuous. However, with early stopping (or weight decay), we observe close to zero performance difference with additional randomly labeled data. Alongside, we obtain tight bounds on the accuracy on unseen clean data paying only a small price to negligible for incorporating randomly labeled data. Similar results hold for SGD and GD and when cross-entropy loss is substituted for MSE (ref. App. C).

Underparameterized linear models On toy Gaussian data, we train linear models with GD to minimize crossentropy loss and mean squared error. Varying the fraction of randomly labeled data we observe that the accuracy on clean unseen data is barely impacted (Fig. 2(a)). This highlights that in low dimensional models adding randomly labeled data with the clean dataset (in toy setup) has minimal effect on the performance on unseen clean data. Moreover, we ﬁnd that RATT offers a tight lower bound on the unseen clean data accuracy. We observe the same behavior with Stochastic Gradient Descent (SGD) training (ref. App. C). Observe that the predicted bound goes up as the fraction of unlabeled data increases. While the accuracy as dictated by the dominating term in the RHS of (2) decreases with an increase

Deep Nets We verify our ﬁndings on (i) ResNet-18 and 5-layer MLPs trained with binary CIFAR (Fig. 1); and (ii) ELMo-LSTM and BERT-Base models ﬁne-tuned on the IMDb dataset (Fig. 2(c)). See App. C for additional results with deep models on binary MNIST. We ﬁx the amount of unlabeled data at 20% of the clean dataset size and train all models with standard hyperparameters. Consistently, we ﬁnd that our predicted bounds are never violated in practice. And as training proceeds, the ﬁt on the mislabeled data increases with perfect overﬁtting in the interpolation regime rendering our bounds vacuous. However, with early stopping, our bound predicts test performance closely. For example, on IMDb dataset with BERT ﬁne-tuning we predict 79.8 as the accuracy of the classiﬁer, when the true

Leveraging Unlabeled Data to Guarantee Generalization

Dataset Model Pred. Acc Test Acc. Best Acc.

MNIST MLP 93.1

97.4

97.9

ResNet 96.8

98.8

98.9

CIFAR10 MLP 48.4

54.2

60.0

ResNet 76.4

88.9

92.3

Table 1. Results on multiclass classiﬁcation tasks. With pred. acc. we refer to the dominating term in RHS of (5). At the given sample size and δ “ 0.1, the remaining term evaluates to 30.7, decreasing our predicted accuracy by the same. We note that test acc. denotes the corresponding accuracy on unseen clean data. Best acc. is the best achievable accuracy with just training on just the clean data (and same hyperparamters except the stopping point). Note that across all tasks our predicted bound is tight and the gap between the best accuracy and test accuracy is small. Exact hyperparameters are included in App. C.

performance is 88.04 (and the best achievable performance on unseen data is 92.45). Additionally, we observe that our method tracks the performance from the beginning of the training and not just towards the end.
Finally, we verify our multiclass bound on MNIST and CIFAR10 with deep MLPs and ResNets (see results in Table 1 and per-epoch curves in App. C). As before, we ﬁx the amount of unlabeled data at 20% of the clean dataset to minimize cross-entropy loss via SGD. In all four settings, our bound predicts non-vacuous performance on unseen data. In App. C, we investigate our approach on CIFAR100 showing that even though our bound grows pessimistic with greater numbers of classes, the error on the mislabeled data nevertheless tracks population accuracy.

6. Discussion and Connections to Prior Work
Implicit bias in deep learning Several recent lines of research attempt to explain the generalization of neural networks despite massive overparameterization via the implicit bias of gradient descent (Soudry et al., 2018; Gunasekar et al., 2018a;b; Ji & Telgarsky, 2019; Chizat & Bach, 2020). Noting that even for overparameterized linear models, there exist multiple parameters capable of overﬁtting the training data (with arbitrarily low loss), of which some generalize well and others do not, they seek to characterize the favored solution. Notably, Soudry et al. (2018) ﬁnd that for linear networks, gradient descent converges (slowly) to the max margin solution. A complementary line of work focuses on the early phases of training, ﬁnding both empirically (Rolnick et al., 2017; Arpit et al., 2017) and theoretically (Arora et al., 2019; Li et al., 2020; Liu et al., 2020) that even in the presence of a small amount of mislabeled data, gradient descent is biased to ﬁt the clean data ﬁrst during initial phases of training. However, to best our knowledge, no prior work leverages this phenomenon to obtain generalization guarantees on the clean data, which is the primary

focus of our work. Our method exploits this phenomenon to produce non-vacuous generalization bounds. Even when we cannot prove a priori that models will ﬁt the clean data well while performing badly on the mislabeled data, we can observe that it indeed happens (often in practice), and thus, a posteriori, provide tight bounds on the population error. Moreover, by using regularizers like early stopping or weight decay, we can accentuate this phenomenon, enabling our framework to provide even tighter guarantees.
Generalization bounds Conventionally, generalization in machine learning has been studied through the lens of uniform convergence bounds (Blumer et al., 1989; Vapnik, 1999). Representative works on understanding generalization in overparameterized networks within this framework include Neyshabur et al. (2015; 2017b;a; 2018); Dziugaite & Roy (2017); Bartlett et al. (2017); Arora et al. (2018); Li & Liang (2018); Zhou et al. (2018); Allen-Zhu et al. (2019a); Nagarajan & Kolter (2019a). However, uniform convergence based bounds typically remain numerically loose relative to the true generalization error. Several works have also questioned the ability of uniform convergence based approaches to explain generalization in overparameterized models (Zhang et al., 2016; Nagarajan & Kolter, 2019b). Subsequently, recent works have proposed unconventional ways to derive generalization bounds (Negrea et al., 2020; Zhou et al., 2020). In a similar spirit, we take departure from complexity-based approaches to generalization bounds in our work. In particular, we leverage unlabeled data to derive a post-hoc generalization bound. Our work provides guarantees on overparameterized networks by using early stopping or weight decay regularization, preventing a perfect ﬁt on the training data. Notably, in our framework, the model can perfectly ﬁt the clean portion of the data, so long as they nevertheless ﬁt the mislabeled data poorly.
Leveraging noisy data to provide generalization guarantees In parallel work, Bansal et al. (2020) presented an upper bound on the generalization gap of linear classiﬁers trained on representations learned via self-supervision. Under certain noise-robustness and rationality assumptions on the training procedure, the authors obtained bounds dependent on the complexity of the linear classiﬁer and independent of the complexity of representations. By contrast, we present generalization bounds for supervised learning that are non-vacuous by virtue of the early learning phenomenon. While both frameworks highlight how robustness to random label corruptions can be leveraged to obtain bounds that do not depend directly on the complexity of the underlying hypothesis class, our framework, methodology, claims, and generalization results are very different from theirs.
Other related work. A long line of work relates early stopped GD to a corresponding regularized solution (Fried-

Leveraging Unlabeled Data to Guarantee Generalization

man & Popescu, 2003; Yao et al., 2007; Suggala et al., 2018; Ali et al., 2018; Neu & Rosasco, 2018; Ali et al., 2020). In the most relevant work, Ali et al. (2018) and Suggala et al. (2018) address a regression task, theoretically relating the solutions of early-stopped GD and a regularized problem, obtained with a data-independent regularization coefﬁcient. Towards understanding generalization numerous stability conditions have been discussed (Kearns & Ron, 1999; Bousquet & Elisseeff, 2002; Mukherjee et al., 2006; Shalev-Shwartz et al., 2010). Hardt et al. (2016) studies the uniform stability property to obtain generalization guarantees with early-stopped SGD. While we assume a benign stability condition to relate leave-one-out performance with population error, we do not rely on any stability condition that implies generalization.
7. Conclusion and Future work
Our work introduces a new approach for obtaining generalization bounds that do not directly depend on the underlying complexity of the model class. For linear models, we provably obtain a bound in terms of the ﬁt on randomly labeled data added during training. Our ﬁndings raise a number of questions to be explored next. While our empirical ﬁndings and theoretical results with 0-1 loss hold absent further assumptions and shed light on why the bound may apply for more general models, we hope to extend our proof that overﬁtting (in terms classiﬁcation error) to the ﬁnite sample of mislabeled data occurs with SGD training on broader classes of models and loss functions. We hope to build on some early results (Nakkiran et al., 2019; Hu et al., 2020) which provide evidence that deep models behave like linear models in the early phases of training. We also wish to extend our framework to the interpolation regime. Since many important aspects of neural network learning take place within early epochs (Achille et al., 2017; Frankle et al., 2020), including gradient dynamics converging to very small subspace (Gur-Ari et al., 2018), we might imagine operationalizing our bounds in the interpolation regime by discarding the randomly labeled data after initial stages of training.
Acknowledgements
SG thanks Divyansh Kaushik for help with NLP code. This material is based on research sponsored by Air Force Research Laboratory (AFRL) under agreement number FA8750-19-1-1000. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation therein. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the ofﬁcial policies or endorsements, either expressed or implied, of Air Force Laboratory, DARPA or the U.S. Gov-

ernment. SB acknowledges funding from the NSF grants DMS-1713003 and CIF-1763734, as well as Amazon AI and a Google Research Scholar Award. ZL acknowledges Amazon AI, Salesforce Research, Facebook, UPMC, Abridge, the PwC Center, the Block Center, the Center for Machine Learning and Health, and the CMU Software Engineering Institute (SEI), for their generous support of ACMI Lab’s research on machine learning under distribution shift.
References
Abadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean, J., Devin, M., Ghemawat, S., Irving, G., Isard, M., et al. Tensorﬂow: A system for large-scale machine learning. In 12th USENIX Symposium on Operating Systems Design and Implementation, 2016.
Abou-Moustafa, K. and Szepesva´ri, C. An exponential efron-stein inequality for lq stable learning rules. arXiv preprint arXiv:1903.05457, 2019.
Achille, A., Rovere, M., and Soatto, S. Critical learning periods in deep neural networks. arXiv preprint arXiv:1711.08856, 2017.
Ali, A., Kolter, J. Z., and Tibshirani, R. J. A continuous-time view of early stopping for least squares. arXiv preprint arXiv:1810.10082, 2018.
Ali, A., Dobriban, E., and Tibshirani, R. J. The implicit regularization of stochastic gradient ﬂow for least squares. arXiv preprint arXiv:2003.07802, 2020.
Allen-Zhu, Z., Li, Y., and Liang, Y. Learning and generalization in overparameterized neural networks, going beyond two layers. In Advances in neural information processing systems, pp. 6158–6169, 2019a.
Allen-Zhu, Z., Li, Y., and Song, Z. A convergence theory for deep learning via over-parameterization. In International Conference on Machine Learning, pp. 242–252. PMLR, 2019b.
Arora, S., Ge, R., Neyshabur, B., and Zhang, Y. Stronger generalization bounds for deep nets via a compression approach. arXiv preprint arXiv:1802.05296, 2018.
Arora, S., Du, S. S., Hu, W., Li, Z., and Wang, R. Finegrained analysis of optimization and generalization for overparameterized two-layer neural networks. arXiv preprint arXiv:1901.08584, 2019.
Arpit, D., Jastrzebski, S., Ballas, N., Krueger, D., Bengio, E., Kanwal, M. S., Maharaj, T., Fischer, A., Courville, A., Bengio, Y., et al. A closer look at memorization in deep networks. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pp. 233– 242. JMLR. org, 2017.

Leveraging Unlabeled Data to Guarantee Generalization

Bansal, Y., Kaplun, G., and Barak, B. For self-supervised learning, rationality implies generalization, provably. arXiv preprint arXiv:2010.08508, 2020.
Bardenet, R., Maillard, O.-A., et al. Concentration inequalities for sampling without replacement. Bernoulli, 21(3): 1361–1385, 2015.
Bartlett, P. L. and Mendelson, S. Rademacher and gaussian complexities: Risk bounds and structural results. Journal of Machine Learning Research, 3(Nov):463–482, 2002.
Bartlett, P. L., Foster, D. J., and Telgarsky, M. J. Spectrallynormalized margin bounds for neural networks. In Advances in neural information processing systems, pp. 6240–6249, 2017.
Blum, A. and Hardt, M. The ladder: A reliable leaderboard for machine learning competitions. In International Conference on Machine Learning, pp. 1006–1014. PMLR, 2015.
Blumer, A., Ehrenfeucht, A., Haussler, D., and Warmuth, M. K. Learnability and the vapnik-chervonenkis dimension. Journal of the ACM (JACM), 36(4):929–965, 1989.
Bousquet, O. and Elisseeff, A. Stability and generalization. Journal of machine learning research, 2(Mar):499–526, 2002.
Chizat, L. and Bach, F. Implicit bias of gradient descent for wide two-layer neural networks trained with the logistic loss. In Conference on Learning Theory, pp. 1305–1338. PMLR, 2020.
Chizat, L., Oyallon, E., and Bach, F. On lazy training in differentiable programming. In Advances in Neural Information Processing Systems, pp. 2937–2947, 2019.
Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.
Du, S., Lee, J., Li, H., Wang, L., and Zhai, X. Gradient descent ﬁnds global minima of deep neural networks. In International Conference on Machine Learning, pp. 1675–1685. PMLR, 2019.
Du, S. S., Zhai, X., Poczos, B., and Singh, A. Gradient descent provably optimizes over-parameterized neural networks. arXiv preprint arXiv:1810.02054, 2018.
Dwork, C., Feldman, V., Hardt, M., Pitassi, T., Reingold, O., and Roth, A. L. Preserving statistical validity in adaptive data analysis. In Proceedings of the forty-seventh annual ACM symposium on Theory of computing, pp. 117–126, 2015.

Dziugaite, G. K. and Roy, D. M. Computing nonvacuous generalization bounds for deep (stochastic) neural networks with many more parameters than training data. arXiv preprint arXiv:1703.11008, 2017.
Elisseeff, A., Pontil, M., et al. Leave-one-out error and stability of learning algorithms with applications. NATO science series sub series iii computer and systems sciences, 190:111–130, 2003.
Frankle, J., Schwab, D. J., and Morcos, A. S. The early phase of neural network training. arXiv preprint arXiv:2002.10365, 2020.
Friedman, J. and Popescu, B. E. Gradient directed regularization for linear regression and classiﬁcation. Technical report, Technical Report, Statistics Department, Stanford University, 2003.
Gunasekar, S., Lee, J., Soudry, D., and Srebro, N. Implicit bias of gradient descent on linear convolutional networks. arXiv preprint arXiv:1806.00468, 2018a.
Gunasekar, S., Woodworth, B., Bhojanapalli, S., Neyshabur, B., and Srebro, N. Implicit regularization in matrix factorization. In 2018 Information Theory and Applications Workshop (ITA), pp. 1–10. IEEE, 2018b.
Gur-Ari, G., Roberts, D. A., and Dyer, E. Gradient descent happens in a tiny subspace. arXiv preprint arXiv:1812.04754, 2018.
Hardt, M., Recht, B., and Singer, Y. Train faster, generalize better: Stability of stochastic gradient descent. In International Conference on Machine Learning, pp. 1225–1234. PMLR, 2016.
He, K., Zhang, X., Ren, S., and Sun, J. Deep Residual Learning for Image Recognition. In Computer Vision and Pattern Recognition (CVPR), 2016.
Hochreiter, S. and Schmidhuber, J. Long short-term memory. Neural computation, 9(8):1735–1780, 1997.
Hoeffding, W. Probability inequalities for sums of bounded random variables. In The Collected Works of Wassily Hoeffding, pp. 409–426. Springer, 1994.
Hu, W., Li, Z., and Yu, D. Simple and effective regularization methods for training on noisily labeled data with generalization guarantee. arXiv preprint arXiv:1905.11368, 2019.
Hu, W., Xiao, L., Adlam, B., and Pennington, J. The surprising simplicity of the early-time learning dynamics of neural networks. arXiv preprint arXiv:2006.14599, 2020.

Leveraging Unlabeled Data to Guarantee Generalization

Hui, L. and Belkin, M. Evaluation of neural architectures trained with square loss vs cross-entropy in classiﬁcation tasks. arXiv preprint arXiv:2006.07322, 2020.
Jacot, A., Gabriel, F., and Hongler, C. Neural tangent kernel: Convergence and generalization in neural networks. In Advances in neural information processing systems, pp. 8571–8580, 2018.
Ji, Z. and Telgarsky, M. The implicit bias of gradient descent on nonseparable data. In Conference on Learning Theory, pp. 1772–1798. PMLR, 2019.
Kearns, M. and Ron, D. Algorithmic stability and sanitycheck bounds for leave-one-out cross-validation. Neural computation, 11(6):1427–1453, 1999.
Krizhevsky, A. and Hinton, G. Learning Multiple Layers of Features from Tiny Images. Technical report, Citeseer, 2009.
LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. GradientBased Learning Applied to Document Recognition. Proceedings of the IEEE, 86, 1998.
Li, M., Soltanolkotabi, M., and Oymak, S. Gradient descent with early stopping is provably robust to label noise for overparameterized neural networks. arXiv preprint arXiv:1903.11680, 2019.
Li, M., Soltanolkotabi, M., and Oymak, S. Gradient descent with early stopping is provably robust to label noise for overparameterized neural networks. In International Conference on Artiﬁcial Intelligence and Statistics, pp. 4313–4324. PMLR, 2020.
Li, Y. and Liang, Y. Learning overparameterized neural networks via stochastic gradient descent on structured data. In Advances in Neural Information Processing Systems, pp. 8157–8166, 2018.
Liu, S., Niles-Weed, J., Razavian, N., and FernandezGranda, C. Early-learning regularization prevents memorization of noisy labels. arXiv preprint arXiv:2007.00151, 2020.
Maas, A., Daly, R. E., Pham, P. T., Huang, D., Ng, A. Y., and Potts, C. Learning word vectors for sentiment analysis. In Proceedings of the 49th annual meeting of the association for computational linguistics: Human language technologies, pp. 142–150, 2011.
McDiarmid, C. On the method of bounded differences, pp. 148–188. London Mathematical Society Lecture Note Series. Cambridge University Press, 1989.
Mukherjee, S., Niyogi, P., Poggio, T., and Rifkin, R. Learning theory: stability is sufﬁcient for generalization and

necessary and sufﬁcient for consistency of empirical risk minimization. Advances in Computational Mathematics, 25(1):161–193, 2006.
Murphy, K. P. Machine Learning: A Probabilistic Perspective. MIT Press, 2012.
Muthukumar, V., Narang, A., Subramanian, V., Belkin, M., Hsu, D., and Sahai, A. Classiﬁcation vs regression in overparameterized regimes: Does the loss function matter? arXiv preprint arXiv:2005.08054, 2020.
Nagarajan, V. and Kolter, J. Z. Deterministic pac-bayesian generalization bounds for deep networks via generalizing noise-resilience. arXiv preprint arXiv:1905.13344, 2019a.
Nagarajan, V. and Kolter, J. Z. Uniform convergence may be unable to explain generalization in deep learning. In Advances in Neural Information Processing Systems, pp. 11615–11626, 2019b.
Nakkiran, P., Kaplun, G., Kalimeris, D., Yang, T., Edelman, B. L., Zhang, F., and Barak, B. Sgd on neural networks learns functions of increasing complexity. arXiv preprint arXiv:1905.11604, 2019.
Negrea, J., Dziugaite, G. K., and Roy, D. In defense of uniform convergence: Generalization via derandomization with an application to interpolating predictors. In International Conference on Machine Learning, pp. 7263–7272. PMLR, 2020.
Neu, G. and Rosasco, L. Iterate averaging as regularization for stochastic gradient descent. In Conference On Learning Theory, pp. 3222–3242. PMLR, 2018.
Neyshabur, B., Tomioka, R., and Srebro, N. Norm-based capacity control in neural networks. In Conference on Learning Theory, pp. 1376–1401, 2015.
Neyshabur, B., Bhojanapalli, S., McAllester, D., and Srebro, N. Exploring generalization in deep learning. arXiv preprint arXiv:1706.08947, 2017a.
Neyshabur, B., Bhojanapalli, S., and Srebro, N. A pac-bayesian approach to spectrally-normalized margin bounds for neural networks. arXiv preprint arXiv:1707.09564, 2017b.
Neyshabur, B., Li, Z., Bhojanapalli, S., LeCun, Y., and Srebro, N. The role of over-parametrization in generalization of neural networks. In International Conference on Learning Representations, 2018.
Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Kopf, A., Yang, E., DeVito, Z., Raison,

Leveraging Unlabeled Data to Guarantee Generalization

M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L., Bai, J., and Chintala, S. Pytorch: An imperative style, high-performance deep learning library. In Advances in Neural Information Processing Systems 32, 2019.
Peters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., and Zettlemoyer, L. Deep contextualized word representations. In Proc. of NAACL, 2018.
Recht, B., Roelofs, R., Schmidt, L., and Shankar, V. Do imagenet classiﬁers generalize to imagenet? In International Conference on Machine Learning, pp. 5389–5400. PMLR, 2019.
Rolnick, D., Veit, A., Belongie, S., and Shavit, N. Deep learning is robust to massive label noise. arXiv preprint arXiv:1705.10694, 2017.
Shalev-Shwartz, S. and Ben-David, S. Understanding machine learning: From theory to algorithms. Cambridge university press, 2014.
Shalev-Shwartz, S., Shamir, O., Srebro, N., and Sridharan, K. Learnability, stability and uniform convergence. The Journal of Machine Learning Research, 11:2635–2670, 2010.
Sherman, J. and Morrison, W. J. Adjustment of an inverse matrix corresponding to a change in one element of a given matrix. The Annals of Mathematical Statistics, 21 (1):124–127, 1950.
Soudry, D., Hoffer, E., Nacson, M. S., Gunasekar, S., and Srebro, N. The implicit bias of gradient descent on separable data. The Journal of Machine Learning Research, 19(1):2822–2878, 2018.
Suggala, A., Prasad, A., and Ravikumar, P. K. Connecting optimization and regularization paths. In Advances in Neural Information Processing Systems, pp. 10608– 10619, 2018.
Tsybakov, A. B. et al. On nonparametric estimation of density level sets. The Annals of Statistics, 25(3):948– 969, 1997.
Vapnik, V. N. An overview of statistical learning theory. IEEE transactions on neural networks, 10(5):988–999, 1999.
Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P., Rault, T., Louf, R., Funtowicz, M., Davison, J., Shleifer, S., von Platen, P., Ma, C., Jernite, Y., Plu, J., Xu, C., Scao, T. L., Gugger, S., Drame, M., Lhoest, Q., and Rush, A. M. Transformers: State-ofthe-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pp. 38– 45. Association for Computational Linguistics, 2020.

Yao, Y., Rosasco, L., and Caponnetto, A. On early stopping in gradient descent learning. Constructive Approximation, 26(2):289–315, 2007.
Zhang, C., Bengio, S., Hardt, M., Recht, B., and Vinyals, O. Understanding deep learning requires rethinking generalization. arXiv preprint arXiv:1611.03530, 2016.
Zhou, L., Sutherland, D. J., and Srebro, N. On uniform convergence and low-norm interpolation learning. arXiv preprint arXiv:2006.05942, 2020.
Zhou, W., Veitch, V., Austern, M., Adams, R. P., and Orbanz, P. Non-vacuous generalization bounds at the imagenet scale: a pac-bayesian compression approach. arXiv preprint arXiv:1804.05862, 2018.

Leveraging Unlabeled Data to Guarantee Generalization

Supplementary Material
Throughout this discussion, we will make frequently use of the following standard results concerning the exponential concentration of random variables:
Lemma 4 (Hoeffding’s inequality for independent RVs (Hoeffding, 1994)). Let Z1, Z2, . . . , Zn be independent bounded random variables with Zi P ra, bs for all i, then

˜ 1

n
ÿ

¸ ˆ 2nt2 ˙

P n pZi ´ E rZisq ě t ď exp ´ pb ´ aq2

i“1

and

˜ 1

n
ÿ

¸ ˆ 2nt2 ˙

P n pZi ´ E rZisq ď ´t ď exp ´ pb ´ aq2

i“1

for all t ě 0.
Lemma 5 (Hoeffding’s inequality for sampling with replacement (Hoeffding, 1994)). Let Z “ pZ1, Z2, . . . , ZN q be a ﬁnite population of N points with Zi P ra.bs for all i. Let X1, X2, . . . Xn be a random sample drawn without replacement from Z. Then for all t ě 0, we have

˜ 1

n
ÿ

¸ ˆ 2nt2 ˙

P n pXi ´ µq ě t ď exp ´ pb ´ aq2

i“1

and

˜ 1

n
ÿ

¸ ˆ 2nt2 ˙

P n pXi ´ µq ď ´t ď exp ´ pb ´ aq2 ,

i“1

where

µ

“

1 N

řN
i“1

Zi.

We now discuss one condition that generalizes the exponential concentration to dependent random variables.
Condition 2 (Bounded difference inequality). Let Z be some set and φ : Zn Ñ R. We say that φ satisﬁes the bounded difference assumption if there exists c1, c2, . . . cn ě 0 s.t. for all i, we have

sup

φpZ1, . . . , Zi, . . . , Znq ´ φpZ1, . . . , Zi1, . . . , Znq ď ci .

Z1,Z2,...,Zn,Zi1 PZn`1

Lemma 6 (McDiarmid’s inequality (McDiarmid, 1989)). Let Z1, Z2, . . . , Zn be independent random variables on set Z and φ : Zn Ñ R satisfy bounded difference inequality (Condition 2). Then for all t ą 0, we have

ˆ 2t2 ˙ P pφpZ1, Z2, . . . , Znq ´ E rφpZ1, Z2, . . . , Znqs ě tq ď exp ´ řni“1 c2i

and ˆ 2t2 ˙
P pφpZ1, Z2, . . . , Znq ´ E rφpZ1, Z2, . . . , Znqs ď ´tq ď exp ´ řni“1 c2i .

A. Proofs from Sec. 3
Additional notation Let m1 be the number of mislabeled points (SrM ) and m2 be the number of correctly labeled points (SrC ). Note m1 ` m2 “ m.

Leveraging Unlabeled Data to Guarantee Generalization

A.1. Proof of Theorem 1

Proof of Lemma 1. The main idea of our proof is to regard the clean portion of the data (S Y SrC) as ﬁxed. Then, there exists an (unknown) classiﬁer f ˚ that minimizes the expected risk calculated on the (ﬁxed) clean data and (random draws
of) the mislabeled data SrM . Formally,

f ˚ :“ arg min EDpf q ,

(10)

q

f PF

where

Dq “ n S ` m2 SrC ` m1 D1 .

m`n m`n

m`n

Note here that Dq is a combination of the empirical distribution over correctly labeled data S Y SrC and the (population) distribution over mislabeled data D1. Recall that

fp :“ arg min ESYSpf q .

(11)

r

f PF

Since, fp minimizes 0-1 error on S Y Sr, using ERM optimality on (11), we have

ESYS pfpq ď ESYS pf ˚q .

(12)

r

r

Moreover, since f ˚ is independent of SrM , using Hoeffding’s bound, we have with probability at least 1 ´ δ that

d

E pf ˚q ď ED1 pf ˚q ` logp1{δq .

(13)

SrM

2m1

Finally, since f ˚ is the optimal classiﬁer on Dq, we have

EDpf ˚q ď EDpfpq .

(14)

q

q

Now

to

relate

(12)

and

(14),

we

multiply

(13)

by

m1 m`n

and

add

mn`n ES pf q

`

mm`2n ES

pf q both the sides. Hence, we can

rC

rewrite (13) as follows:

d

E pf ˚q ď E pf ˚q ` m1 logp1{δq .

(15)

S YSr

Dq

m ` n 2m1

Now we combine equations (12), (15), and (14), to get

d

m1 logp1{δq

ESYS pfpq
r

ď

ED pfpq
q

`

m

`

n

, 2m1

(16)

which implies

d

logp1{δq

ES pfpq ď ED1 pfpq `
rM

. 2m1

(17)

Since Sr is obtained by randomly labeling an unlabeled dataset, we assume 2m1 « m 3. Moreover, using ED1 “ 1 ´ ED we obtain the desired result.

Proof of Lemma 2. Recall ESpf q “ mm1 ES pf q ` mm2 ES pf q. Hence, we have

r

rM

rC

ˆ 2m1

˙ ˆ 2m2

˙

2ESpf q ´ ES pf q ´ ES pf q “

r

rM

rC

m

ES pf q ´ ES pf q

rM

rM

`

m

ES pf q ´ ES pf q

rC

rC

(18)

ˆ 2m1 ˙

ˆ 2m2 ˙

“

m

´1

ES pf q `
rM

m

´1

ES pf q .
rC

(19)

3Formally, with probability at least 1 ´ δ, we have pm ´ 2m1q ď am logp1{δq{2.

Leveraging Unlabeled Data to Guarantee Generalization

b Since the dataset is labeled uniformly at random, with probability at least 1 ´ δ, we have ` 2m m1 ´ 1˘ ď log2pm1{δq . Similarly,
b we have with probability at least 1 ´ δ, ` 2m m2 ´ 1˘ ď log2pm1{δq . Using union bound, with probability at least 1 ´ δ, we have

c

logp2{δq ´

¯

2ES ´ ES pf q ´ ES pf q ď

r

rM

rC

2m

ES pf q ` ES pf q .

rM

rC

(20)

With re-arranging ES pf q ` ES pf q and using the inequality 1 ´ a ď 1`1a , we have

rM

rC

c

logp2{δq

2ES ´ ES pf q ´ ES pf q ď 2ES

r

rM

rC

r

. 2m

(21)

Proof of Lemma 3. In the set of correctly labeled points S Y SrC, we have S as a random subset of S Y SrC. Hence, using Hoeffding’s inequality for sampling without replacement (Lemma 5), we have with probability at least 1 ´ δ

d

logp1{δq

ESC pfpq ´ ESYSC pfpq ď

r

r

. 2m2

(22)

Re-writing ESYS

pfpq as mm`2 n ES

pfpq

`

m

n `

n

ES

pfpq

,

we

have

with

probability

at

least

1

´

δ

rC

2

rC

2

d

ˆ

n

˙ ´

¯ logp1{δq

n ` m2

ES pfpq ´ ES pfpq ď
rC

. 2m2

(23)

As before, assuming 2m2 « m, we have with probability at least 1 ´ δ

c

c

´ m2 ¯ logp1{δq ´ m ¯ logp1{δq

ES pfpq ´ ES pfpq ď
rC

1`

n

m ď 1 ` 2n

. m

(24)

Proof of Theorem 1. Having established these core intermediate results, we can now combine above three lemmas to prove the main result. In particular, we bound the population error on clean data (EDpfpq) as follows:

(i) First, use (17), to obtain an upper bound on the population error on clean data, i.e., with probability at least 1 ´ δ{4, we have

c

logp4{δq

EDpfpq ď 1 ´ ES pfpq `
rM

. m

(25)

(ii) Second, use (21), to relate the error on the mislabeled fraction with error on clean portion of randomly labeled data and error on whole randomly labeled dataset, i.e., with probability at least 1 ´ δ{2, we have

c

logp4{δq

´ES pf q ď ES pf q ´ 2ES ` 2ES

rM

rC

r

r

. 2m

(26)

(iii) Finally, use (24) to relate the error on the clean portion of randomly labeled data and error on clean training data, i.e., with probability 1 ´ δ{4, we have

c

´ m ¯ logp4{δq

ES pfpq ď ´ES pfpq `
rC

1 ` 2n

. m

(27)

Using union bound on the above three steps, we have with probability at least 1 ´ δ:

c

´?

m ¯ logp4{δq

EDpfpq ď ES pfpq ` 1 ´ 2ES pfpq `
r

2ES
r

`

2

`

2n

. m

(28)

Leveraging Unlabeled Data to Guarantee Generalization

A.2. Proof of Proposition 1

Proof of Proposition 1. For a classiﬁer f : X Ñ t´1, 1u, we have 1 ´ 2 I rf pxq ‰ ys “ y ¨ f pxq. Hence, by deﬁnition of E, we have

1

m
ÿ

1

m
ÿ

1 ´ 2ESrpf q “ m i“1 yi ¨ f pxiq ď fsuPFp m i“1 yi ¨ f pxiq . (29)

Note that for ﬁxed inputs px1, x2, . . . , xmq in Sr, py1, y2, . . . ymq are random labels. Deﬁne φ1py1, y2, . . . , ymq :“

supf PF

1 m

řm
i“1

yi

¨

f pxiq.

We

have

the

following

bounded

difference

condition

on

φ1.

For

all

i,

sup

φ1py1, . . . , yi, . . . , ymq ´ φ1py1, . . . , yi1, . . . , ymq ď 1{m .

(30)

y1,...ym,yi1 Pt´1,1um`1

Similarly, we deﬁne φ2px1, x2, . . . , xmq :“ Eyi„U t´1,1u “supfPF

1 m

řm
i“1

yi

¨

f pxiq‰.

We have the following bounded

difference condition on φ2. For all i,

sup

φ2px1, . . . , xi, . . . , xmq ´ φ1px1, . . . , x1i, . . . , xmq ď 1{m .

(31)

x1

,...xm

,x

1 i

P

X

m`1

Using McDiarmid’s inequality (Lemma 6) twice with Condition (30) and (31), with probability at least 1 ´ δ, we have

1

m
ÿ

«

1

m
ÿ

ﬀc 2 logp2{δq

sup

yi ¨ f pxiq ´ Ex,y sup

yi ¨ f pxiq ď

.

(32)

f PF m i“1

f PF m i“1

m

Combining (29) and (32), we obtain the desired result.

A.3. Proof of Theorem 2

Proof of Theorem 2 follows similar to the proof of Theorem 1. Note that the same results in Lemma 1, Lemma 2, and Lemma 3 hold in the regularized ERM case. However, the arguments in the proof of Lemma 1 change slightly. Hence, we state the lemma for regularized ERM and prove it here for completeness.
Lemma 7. Assume the same setup as Theorem 2. Then for any δ ą 0, with probability at least 1 ´ δ over the random draws of mislabeled data SrM , we have

c

logp1{δq

EDpfpq ď 1 ´ ES pfpq `
rM

. m

(33)

Proof. The main idea of the proof remains the same, i.e. regard the clean portion of the data (S Y SrC) as ﬁxed. Then, there exists a classiﬁer f ˚ that is optimal over draws of the mislabeled data SrM .
Formally,

f ˚ :“ arg min EDpf q ` λRpf q ,

(34)

q

f PF

where

Dq “ n S ` m1 SrC ` m2 D1 .

m`n m`n

m`n

That is, Dq a combination of the empirical distribution over correctly labeled data S Y SrC and the (population) distribution over mislabeled data D1. Recall that

fp :“ arg min ESYSpf q ` λRpf q .

(35)

r

f PF

Since, fp minimizes 0-1 error on S Y Sr, using ERM optimality on (11), we have

ESYS pfpq ` λRpfpq ď ESYS pf ˚q ` λRpf ˚q .

(36)

r

r

Leveraging Unlabeled Data to Guarantee Generalization

Moreover, since f ˚ is independent of SrM , using Hoeffding’s bound, we have with probability at least 1 ´ δ that

d

E pf ˚q ď ED1 pf ˚q ` logp1{δq .

(37)

SrM

2m1

Finally, since f ˚ is the optimal classiﬁer on Dq, we have

EDpf ˚q ` λRpf ˚q ď EDpfpq ` λRpfpq .

(38)

q

q

Now to relate (36) and (38), we can re-write the (37) as follows:

d

E pf ˚q ď E pf ˚q ` m1 logp1{δq .

(39)

S YSr

Dq

m ` n 2m1

After adding λRpf ˚q on both sides in (39), we combine equations (36), (39), and (38), to get

d

m1 logp1{δq

ESYS pfpq
r

ď

ED pfpq
q

`

m

`

n

, 2m1

(40)

which implies

d

logp1{δq

ES pfpq ď ED1 pfpq `
rM

. 2m1

(41)

Similar as before, since Sr is obtained by randomly labeling an unlabeled dataset, we assume 2m1 « m. Moreover, using ED1 “ 1 ´ ED we obtain the desired result.

A.4. Proof of Theorem 3

To prove our results in the multiclass case, we ﬁrst state and prove lemmas parallel to those used in the proof of balanced binary case. We then combine these results to obtain the result in Theorem 3.
Before stating the result, we deﬁne mislabeled distribution D1 for any D. While D1 and D share the same marginal distribution over inputs X , the conditional distribution over labels y given an input x „ DX is changed as follows: For any x, the Probability Mass Function (PMF) over y is deﬁned as: pD1 p¨|xq :“ 1´pkD´p1¨|xq , where pDp¨|xq is the PMF over y for the distribution D.
Lemma 8. Assume the same setup as Theorem 3. Then for any δ ą 0, with probability at least 1 ´ δ over the random draws of mislabeled data SrM , we have

c

´

¯

logp1{δq

EDpfpq ď pk ´ 1q 1 ´ ES pfpq ` pk ´ 1q
rM

. m

(42)

Proof. The main idea of the proof remains the same. We begin by regarding the clean portion of the data (S Y SrC) as ﬁxed. Then, there exists a classiﬁer f ˚ that is optimal over draws of the mislabeled data SrM .

However, in the multiclass case, we cannot as easily relate the population error on mislabeled data to the population accuracy on clean data. While for binary classiﬁcation, we could lower bound the population accuracy 1 ´ ED with the empirical error on mislabeled data ES (in the proof of Lemma 1), for multiclass classiﬁcation, error on the mislabeled data and accuracy on
rM
the clean data in the population are not so directly related. To establish (42), we break the error on the (unknown) mislabeled
data into two parts: one term corresponds to predicting the true label on mislabeled data, and the other corresponds to
predicting neither the true label nor the assigned (mis-)label. Finally, we relate these errors to their population counterparts
to establish (42).

Formally,

f ˚ :“ arg min EDpf q ` λRpf q ,

(43)

q

f PF

Leveraging Unlabeled Data to Guarantee Generalization

where

Dq “ n S ` m1 SrC ` m2 D1 .

m`n m`n

m`n

That is, Dq is a combination of the empirical distribution over correctly labeled data S Y SrC and the (population) distribution over mislabeled data D1. Recall that

fp :“ arg min ESYSpf q ` λRpf q .

(44)

r

f PF

Following the exact steps from the proof of Lemma 7, with probability at least 1 ´ δ, we have

d

logp1{δq

ES pfpq ď ED1 pfpq `
rM

. 2m1

(45)

Similar to before, since Sr is obtained by randomly labeling an unlabeled dataset, we assume k´k 1 m1 « m.
Now we will relate ED1 pfpq with EDpfpq. Let yT denote the (unknown) true label for a mislabeled point px, yq (i.e., label before replacing it with a mislabel).

””

ıı

””

ıı

Epx,yqP„D1 I fppxq ‰ y “ Epx,yqP„D1 I fppxq ‰ y ^ fppxq ‰ yT

l

jh

n

I

””

ıı

` Epx,yqP„D1 I fppxq ‰ y ^ fppxq “ yT .

(46)

l

jh

n

II

Clearly, term 2 is one minus the accuracy on the clean unseen data, i.e.,

””

ıı

II “ 1 ´ Ex,y„D I fppxq ‰ y “ 1 ´ EDpfpq .

(47)

Next, we relate term 1 with the error on the unseen clean data. We show that term 1 is equal to the error on the unseen clean data scaled by kk´´21 , where k is the number of labels. Using the deﬁnition of mislabeled distribution D1, we have

˜

«

ﬀ¸

1

ÿ”

ı

k´2

I “ k ´ 1 Epx,yqP„D

I fppxq ‰ i ^ fppxq ‰ y “ k ´ 1 EDpfpq .

(48)

iPY ^i‰y

Combining the result in (47), (48) and (46), we have

1 ED1 pfpq “ 1 ´ k ´ 1 EDpfpq . (49)

Finally, combining the result in (49) with equation (45), we have with probability 1 ´ δ,

d

´

¯

k logp1{δq

EDpfpq ď pk ´ 1q 1 ´ ES pfpq ` pk ´ 1q
rM

. 2pk ´ 1qm

(50)

Lemma 9. Assume the same setup as Theorem 3. Then for any δ ą 0, with probability at least 1 ´ δ over the random draws

of Sr, we have

c

logp4{δq

kES pfpq ´ ES pfpq ´ pk ´ 1qES pfpq ď 2k

r

rC

rM

. 2m

Leveraging Unlabeled Data to Guarantee Generalization

Proof. Recall ESpf q “ mm1 ES pf q ` mm2 ES pf q. Hence, we have

r

rM

rC

ˆ km1

˙

kESpf q ´ pk ´ 1qES pf q ´ ES pf q “ pk ´ 1q

r

rM

rC

pk

´

1qm

ES
rM

pf

q

´

ES
rM

pf

q

ˆ km2

˙

`

m

ES pf q ´ ES pf q

rC

rC

„ˆ m1 k ´ 1 ˙

ˆ m2 1 ˙



“k

m´ k

ES pf q `
rM

m ´k

ES pf q
rC

.

b Since the dataset is randomly labeled, we have with probability at least 1 ´ δ, ` mm1 ´ k´k 1 ˘ ď log2pm1{δq . Similarly, we
b have with probability at least 1 ´ δ, ` mm2 ´ k1 ˘ ď log2pm1{δq . Using union bound, we have with probability at least 1 ´ δ

c

logp2{δq ´

¯

kESpf q ´ pk ´ 1qES pf q ´ ES pf q ď k

r

rM

rC

2m

ES pf q ` ES pf q .

rM

rC

(51)

Lemma 10. Assume the same setup as Theorem 3. Then for any δ ą 0, with probability at least 1 ´ δ over the random

draws of SrC and S, we have

c

k logp2{δq

ES pfpq ´ ES pfpq ď 1.5
rC

. 2m

Proof. In the set of correctly labeled points S Y SrC, we have S as a random subset of S Y SrC. Hence, using Hoeffding’s inequality for sampling without replacement (Lemma 5), we have with probability at least 1 ´ δ

d

logp1{δq

ESc pfpq ´ ESYSC pfpq ď

r

r

. 2m2

(52)

Re-writing ESYS

pfpq as mm`2 n ES

pfpq

`

m

n `

n

ES

pfpq

,

we

have

with

probability

at

least

1

´

δ

rC

2

rC

2

d

ˆ

n

˙ ´

¯ logp1{δq

n ` m2

ES pfpq ´ ES pfpq ď
rc

. 2m2

(53)

As before, assuming km2 « m, we have with probability at least 1 ´ δ

c

c

´ m2 ¯ k logp1{δq ˆ 1 ˙ k logp1{δq

ES pfpq ´ ES pfpq ď
rc

1`

n

2m ď 1 ` k

. 2m

(54)

Proof of Theorem 3. Having established these core intermediate results, we can now combine above three lemmas. In particular, we bound the population error on clean data (EDpfpq) as follows:

(i) First, use (50), to obtain an upper bound on the population error on clean data, i.e., with probability at least 1 ´ δ{4, we have

d

´

¯

k logp4{δq

EDpfpq ď pk ´ 1q 1 ´ ES pfpq ` pk ´ 1q
rM

. 2pk ´ 1qm

(55)

(ii) Second, use (51) to relate the error on the mislabeled fraction with error on clean portion of randomly labeled data and error on whole randomly labeled dataset, i.e., with probability at least 1 ´ δ{2, we have

c

logp4{δq

´pk ´ 1qES pf q ď ES pf q ´ kES ` k

rM

rC

r

. 2m

(56)

Leveraging Unlabeled Data to Guarantee Generalization

(iii) Finally, use (54) to relate the error on the clean portion of randomly labeled data and error on clean training data, i.e., with probability 1 ´ δ{4, we have

c

´ m ¯ k logp4{δq

ES pfpq ď ´ES pfpq `
rC

1 ` kn

. 2m

(57)

Using union bound on the above three steps, we have with probability at least 1 ´ δ:

a

?

c m logp4{δq

EDpfpq ď ES pfpq ` pk ´ 1q ´ kES pfpq ` p kpk ´ 1q ` k ` k ` ? q

r

nk

. 2m

(58)

Simplifying the term in RHS of (58), we get the desired result. in the ﬁnal bound.

Leveraging Unlabeled Data to Guarantee Generalization

B. Proofs from Sec. 4

We suppose that the parameters of the linear function are obtained via gradient descent on the following L2 regularized problem:

n

LSpw; λq

:“

ÿ

pwT xi

´

yiq2

`

λ

||

w

||

2 2

,

(59)

i“1

where

λ

ě

0

is

a

regularization

parameter.

We

assume

access

to

a

clean

dataset

S

“

tpx

i

,

yi

qu

n i“1

„

Dn

and

randomly

labeled dataset Sr “ tpxi, yiquni“`nm`1 „ Drm. Let X “ rx1, x2, ¨ ¨ ¨ , xm`ns and y “ ry1, y2, ¨ ¨ ¨ , ym`ns. Fix a positive

´

¯

learning rate η such that η ď 1{ ˇˇˇˇXT Xˇˇˇˇop ` λ2 and an initialization w0 “ 0. Consider the following gradient descent

iterates to minimize objective (59) on S Y Sr:

wt “ wt´1 ´ η∇wLSYSpwt´1; λq @t “ 1, 2, . . .

(60)

r

Then we have twtu converge to the limiting solution w “ `XT X ` λI˘´1 XT y. Deﬁne fppxq :“ f px; wq.

p

p

B.1. Proof of Theorem 4

We use a standard result from linear algebra, namely the Shermann-Morrison formula (Sherman & Morrison, 1950) for matrix inversion:
Lemma 11 (Sherman & Morrison (1950)). Suppose A P Rnˆn is an invertible square matrix and u, v P Rn are column vectors. Then A ` uvT is invertible iff 1 ` vT Au ‰ 0 and in particular

pA ` uvT q´1 “ A´1 ´ A´1uvT A´1 .

(61)

1 ` vT A´1u

For a given training set S Y SrC, deﬁne leave-one-out error on mislabeled points in the training data as

ř
px ,y qPS

E pfpiqpxiq, yiq

ELOOpS q “

i i rM

,

rM

SrM

where fpiq :“ f pA, pS Y Srqpiqq. To relate empirical leave-one-out error and population error with hypothesis stability condition, we use the following lemma:
Lemma 12 (Bousquet & Elisseeff (2002)). For the leave-one-out error, we have

„ ´

¯2 1

3β

E ED1 pfpq ´ ELOOpSM q
r

ď 2m1 ` n ` m .

(62)

Proof of the above lemma is similar to the proof of Lemma 9 in Bousquet & Elisseeff (2002) and can be found in App. D.

Before presenting the proof of Theorem 4, we introduce some more notation. Let Xpiq denote the matrix of covariates with

the ith point removed. Similarly, let ypiq be the array of responses with the ith point removed. Deﬁne the corresponding

´

¯´1

regularized GD solution as wppiq “ XpTiqXpiq ` λI XpTiqypiq. Deﬁne fppiqpxq :“ f px; wppiqq.

Proof of Theorem 4. Because squared loss minimization does not imply 0-1 error minimization, we cannot use arguments from Lemma 1. This is the main technical difﬁculty. To compare the 0-1 error at a train point with an unseen point, we use the closed-form expression for w and Shermann-Morrison formula to upper bound training error with leave-one-out cross
p validation error.
The proof is divided into three parts: In part one, we show that 0-1 error on mislabeled points in the training set is lower than the error obtained by leave-one-out error at those points. In part two, we relate this leave-one-out error with the population error on mislabeled distribution using Condition 1. While the empirical leave-one-out error is an unbiased estimator of the average population error of leave-one-out classiﬁers, we need hypothesis stability to control the variance of empirical

Leveraging Unlabeled Data to Guarantee Generalization

leave-one-out error. Finally, in part three, we show that the error on the mislabeled training points can be estimated with just the randomly labeled and clean training data (as in proof of Theorem 1).
Part 1 First we relate training error with leave-one-out error. For any training point pxi, yiq in Sr Y S, we have

E pfppxi q,

yiq

“

I

“yi

¨

xTi

w

ă

0‰

“

I

” yi

¨

xTi

`X T

X

`

λI ˘´1

XT

y

ă

ı 0

(63)

p

»

ﬁ

—

´

¯´1

ﬃ

“ I ——yi ¨ xTi XpTiqXpiq ` xTi xi ` λI pXpTiqypiq ` yi ¨ xiq ă 0ﬃﬃ . (64)

–

ﬂ

l

jh

n

I

´

¯

Letting A “ XpTiqXpiq ` λI and using Lemma 11 on term 1, we have

„ Epf px q, y q “ I y

¨

xT

„ A´1

´

A´1xixTi A´1  pXT

y

 `y ¨x qă0

(65)

pi i

ii

1 ` xTi A´1xi

piq piq

ii

„ „ xTi A´1p1 ` xTi A´1xiq ´ xTi A´1xixTi A´1  T



“ I yi ¨

1 ` xTi A´1xi

pXpiqypiq ` yi ¨ xiq ă 0

(66)

„

„

x

T i

A

´1



T



“ I yi ¨ 1 ` xTi A´1xi pXpiqypiq ` yi ¨ xiq ă 0 . (67)

Since 1 ` xTi A´1xi ą 0, we have

”

ı

E pfppxiq, yiq “ I yi ¨ xTi A´1pXpTiqypiq ` yi ¨ xiq ă 0

(68)

”

ı

“ I xTi A´1xi ` yi ¨ xTi A´1pXpTiqypiqq ă 0

(69)

”

ı

ď I yi ¨ xTi A´1pXpTiqypiqq ă 0 “ E pfppiqpxiq, yiq .

(70)

Using (70), we have

ř
px ,y qPS

Epfppiqpxiq, yiq

ES pfpq ď ELOOpS q :“

i i rM

.

rM

rM

SrM

(71)

Part 2 We now relate RHS in (71) with the population error on mislabeled distribution. To do this, we leverage Condition 1 and Lemma 12. In particular, we have

„ ´

¯2 1

3β

ESYSM ED1 pfpq ´ ELOOpSM q

r

r

ď 2m1 ` m ` n .

(72)

Using Chebyshev’s inequality, with probability at least 1 ´ δ, we have

d

1ˆ 1

3β ˙

ELOOpSM q ď ED1 pfpq `
r

δ

2m1 ` m ` n .

(73)

Part 3 Combining (73) and (71), we have

d

1ˆ 1

3β ˙

ES pfpq ď ED1 pfpq `
rM

δ

2m1 ` m ` n .

(74)

Leveraging Unlabeled Data to Guarantee Generalization

Compare (74) with (17) in the proof of Lemma 1. We obtain a similar relationship between ES and ED1 but with a
rM
polynomial concentration instead of exponential concentration. In addition, since we just use concentration arguments to
relate mislabeled error to the errors on the clean and unlabeled portions of the randomly labeled data, we can directly use the
results in Lemma 2 and Lemma 3. Therefore, combining results in Lemma 2, Lemma 3, and (74) with union bound, we have with probability at least 1 ´ δ

c

d

´?

m ¯ logp4{δq 4 ˆ 1 3β ˙

EDpfpq ď ES pfpq ` 1 ´ 2ES pfpq `
r

2ES pfpq
r

`

1

`

2n

m ` δ m ` m`n .

(75)

B.2. Extension to multiclass classiﬁcation

For multiclass problems with squared loss minimization, as standard practice, we consider one-hot encoding for the underlying label, i.e., a class label c P rks is treated as p0, ¨, 0, 1, 0, ¨, 0q P Rk (with c-th coordinate being 1). As before, we
suppose that the parameters of the linear function are obtained via gradient descent on the following L2 regularized problem:

n

k

LSpw; λq :“ ÿ ˇˇˇˇwT xi ´ yiˇˇˇˇ22 ` λ ÿ ||wj||22 , (76)

i“1

j“1

where

λ

ě

0

is

a

regularization

parameter.

We

assume

access

to

a

clean

dataset

S

“

tpx

i

,

yi

qu

n i“1

„

Dn

and

randomly

labeled dataset Sr “ tpxi, yiquni“`nm`1 „ Drm. Let X “ rx1, x2, ¨ ¨ ¨ , xm`ns and y “ rey1 , ey2 , ¨ ¨ ¨ , eym`n s. Fix a positive

´

¯

learning rate η such that η ď 1{ ˇˇˇˇXT Xˇˇˇˇop ` λ2 and an initialization w0 “ 0. Consider the following gradient descent

iterates to minimize objective (59) on S Y Sr:

wjt “ wjt´1 ´ η∇wj LSYSpwt´1; λq @t “ 1, 2, . . . and j “ 1, 2, . . . , k .

(77)

r

Then we have twjtu for all j “ 1, 2, ¨ ¨ ¨ , k converge to the limiting solution wj “ `XT X ` λI˘´1 XT yj. Deﬁne p
fppxq :“ f px; wq. p
Theorem 5. Assume that this gradient descent algorithm satisﬁes Condition 1 with β “ Op1q. Then for a multiclass classiﬁcation problem wth k classes, for any δ ą 0, with probability at least 1 ´ δ, we have:

ˆ

k

˙

EDpfpq ď ES pfpq ` pk ´ 1q 1 ´ k ´ 1 ESrpfpq

c

d

ˆ ? m ˙ logp4{δq a

4 ˆ 1 3β ˙

` k ` k ` n?k 2m ` kpk ´ 1q δ m ` m ` n . (78)

Proof. The proof of this theorem is divided into two parts. In the ﬁrst part, we relate the error on the mislabeled samples with the population error on the mislabeled data. Similar to the proof of Theorem 4, we use Shermann-Morrison formula to upper bound training error with leave-one-out error on each wj. Second part of the proof follows entirely from the proof of
p Theorem 3. In essence, the ﬁrst part derives an equivalent of (45) for GD training with squared loss and then the second part follows from the proof of Theorem 3.

Part-1: Consider a training point pxi, yiq in Sr Y S. For simplicity, we use ci to denote the class of i-th point and use yi as

the

corresponding

one-hot

embedding.

Recall

error

in

multiclass

point

is

given

by

E pfppxi q,

yiq

“

I

“ci

R

arg

max

xTi

w‰. p

Thus, there exists a j ‰ ci P rks, such that we have

Epfppxiq, yiq

“

I “ci

R

arg

m

ax

x

T i

w

‰

“

I “xTi wci

ă

x

T i

wj

‰

(79)

p

p

p

“ I ”xTi `XT X ` λI˘´1 XT yci ă xTi `XT X ` λI˘´1 XT yj ı (80)

»

ﬁ

—´

¯´1 ´

¯ﬃ

“ I ——xTi XpTiqXpiq ` xTi xi ` λI

XpTiqyci piq ` xi ´ XpTiqyj piq

ă 0ﬃ . ﬃ

(81)

–

ﬂ

l

jh

n

I

Leveraging Unlabeled Data to Guarantee Generalization

´

¯

Letting A “ XpTiqXpiq ` λI and using Lemma 11 on term 1, we have

Epf px

q, y

q

“

„ I xT

„ A´1

´

A´1xixTi A´1  ´XT

y

 ¯ ` x ´ XT y ă 0

(82)

pi i

i

1 ` xTi A´1xi

piq ci piq

i

piq j piq

„„ xTi A´1p1 ` xTi A´1xiq ´ xTi A´1xixTi A´1  ´ T

 ¯
T

“ I 1 ` xTi A´1xi Xpiqyci piq ` xi ´ Xpiqyj piq ă 0 (83)

„„

xTi A´1

 ´
T

 ¯
T

“ I 1 ` xTi A´1xi Xpiqyci piq ` xi ´ Xpiqyj piq ă 0 . (84)

Since 1 ` xTi A´1xi ą 0, we have

”

´

¯ı

Epfppxiq, yiq “ I

x

T i

A

´1

XpTiqyci piq ` xi ´ XpTiqyj piq

ă0

(85)

”

ı

“I

x

T i

A

´1

x

i

`

xTi A´1XpTiqyci piq

´

xTi A´1XpTiqyj piq

ă

0

(86)

”

ı

ď I xTi A´1XpTiqyci piq ´ xTi A´1XpTiqyj piq ă 0 “ E pfppiqpxiq, yiq .

(87)

Using (87), we have

ř
px ,y qPS

Epfppiqpxiq, yiq

ES pfpq ď ELOOpS q :“

i i rM

.

rM

rM

SrM

(88)

We now relate RHS in (71) with the population error on mislabeled distribution. Similar as before, to do this, we leverage Condition 1 and Lemma 12. Using (73) and (88), we have

d

1ˆ 1

3β ˙

ES pfpq ď ED1 pfpq `
rM

δ

2m1 ` m ` n .

(89)

We have now derived a parallel to (45). Using the same arguments in the proof of Lemma 8, we have

d

´

¯

k ˆ1

3β ˙

EDpfpq ď pk ´ 1q 1 ´ ES pfpq ` pk ´ 1q
rM

δpk ´ 1q

2m1 ` m ` n

.

(90)

Part-2: We now combine the results in Lemma 9 and Lemma 10 to obtain the ﬁnal inequality in terms of quantities that can be computed from just the randomly labeled and clean data. Similar to the binary case, we obtained a polynomial concentration instead of exponential concentration. Combining (90) with Lemma 9 and Lemma 10, we have with probability at least 1 ´ δ

ˆ

k

˙

EDpfpq ď ES pfpq ` pk ´ 1q 1 ´ k ´ 1 ESrpfpq

c

d

ˆ ? m ˙ logp4{δq a

4 ˆ 1 3β ˙

` k ` k ` n?k 2m ` kpk ´ 1q δ m ` m ` n . (91)

B.3. Discussion on Condition 1
The quantity in LHS of Condition 1 measures how much the function learned by the algorithm (in terms of error on unseen point) will change when one point in the training set is removed. We need hypothesis stability condition to control the variance of the empirical leave-one-out error to show concentration of average leave-one-error with the population error.
Additionally, we note that while the dominating term in the RHS of Theorem 4 matches with the dominating term in ERM a
bound in Theorem 1, there is a polynomial concentration term (dependence on 1{δ instead of logp 1{δq) in Theorem 4.

Leveraging Unlabeled Data to Guarantee Generalization

Since with hypothesis stability, we just bound the variance, the polynomial concentration is due to the use of Chebyshev’s inequality instead of an exponential tail inequality (as in Lemma 1). Recent works have highlighted that a slightly stronger condition than hypothesis stability can be used to obtain an exponential concentration for leave-one-out error (Abou-Moustafa & Szepesva´ri, 2019), but we leave this for future work for now.

B.4. Formal statement and proof of Proposition 2
Bλe“fo0re. AfosrsmumalelySpinregsuelnartiVngaltuheeDreescuolmt,pwoesitwioinll (iSnVtroDd)uocfeXsomase?nontUatiSo1n{.2BVyTL. HSpewncqe, wXeTdXeno“teVthSe VobTje.cCtiovnesiinde(r59th)ewGitDh iterates deﬁned in (60). We now derive closed form expression for the tth iterate of gradient descent:

wt “ wt´1 ` η ¨ XT py ´ Xwt´1q “ pI ´ ηV SV T qwk´1 ` ηXT y .

(92)

Rotating by V T , we get

wt r

“

pI

´

ηSqwrk´1

`

ηy, r

(93)

where wt “ V T wt and y “ V T XT y. Assuming the initial point w0 “ 0 and applying the recursion in (93), we get

r

r

wt “ S´1pI ´ pI ´ ηSqkqy ,

(94)

r

r

Projecting solution back to the original space, we have

wt “ V S´1pI ´ pI ´ ηSqkqV T XT y .

(95)

Deﬁne

ftpxq

:“

f px; wtq

as

the

solution

at

the

tth

iterate.

Let

wλ r

“

arg minw LS pw; λq

“

pXT X

` λIq´1XT y

“

V pS ` λIq´1V T XT y. and deﬁne frλpxq :“ f px; wλq as the regularized solution. Assume κ be the condition number of r

the population covariance matrix and let smin be the minimum positive singular value of the empirical covariance matrix. Our

proof idea is inspired from recent work on relating gradient ﬂow solution and regularized solution for regression problems

(Ali et al., 2018). We will use the following lemma in the proof:

Lemma 13.

For all x P r0, 1s and for all k P N, we have (a)

kx 1`kx

ď 1 ´ p1 ´ xqk

and (b) 1 ´ p1 ´ xqk

ď2¨

kxk`x 1 .

Proof. Using p1 ´ xqk ď 1`1kx , we have part (a). For part (b), we numerically maximize p1`kxqp1k´xp1´xqkq for all k ě 1 and for all x P r0, 1s.

Proposition 3 (Formal statement of Proposition 2). Let λ “ t1η . For a training point x, we have

”

ı

Ex„S pftpxq ´ frλpxqq2 ď cpt, ηq ¨ Ex„S “ftpxq2‰ ,

where cpt, ηq :“ minp0.25, s2min1t2η2 q. Similarly for a test point, we have

”

ı

Ex„DX pftpxq ´ frλpxqq2 ď κ ¨ cpt, ηq ¨ Ex„DX “ftpxq2‰ .

Proof. We want to analyze the expected squared difference output of regularized linear regression with regularization

constant

λ

“

1 ηt

and

the

gradient

descent

solution

at

the

tth

iterate.

We

separately

expand

the

algebraic

expression

for

squared

difference at a training point and a test point. Then the main step is to show that “S´1pI ´ pI ´ ηSqkq ´ pS ` λIq´1‰ ĺ

cpη, tq ¨ S´1pI ´ pI ´ ηSqkq.

Part 1 First, we will analyze the squared difference of the output at a training point (for simplicity, we refer to S Y Sr as S),

Leveraging Unlabeled Data to Guarantee Generalization

i.e.,

„ ´

¯2

Ex„S ftpxq ´ frλpxq “ ||Xwt ´ Xwλ||22

r

“

ˇˇ ˇˇX V

S´1pI

´

pI

´

ηSqtqV

T

XT

y

´

XV

pS

`

λI q´1 V

T

XT

ˇˇ2 yˇˇ

2

“ ˇˇˇˇXV `S´1pI ´ pI ´ ηSqtq ´ pS ` λIq´1˘ V T XT yˇˇˇˇ2

¨

˛2

“ yT V X ˚˚S´1pI ´ pI ´ ηSqtq ´ pS ` λIq´1‹‹ SV T XT y .

˝

‚

l

jh

n

I

We now separately consider term 1. Substituting λ “ t1η , we get

S´1pI ´ pI ´ ηSqtq ´ pS ` λIq´1 “ S´1 `pI ´ pI ´ ηSqtq ´ pI ` S´1λq´1˘

“ S´1 `pI ´ pI ´ ηSqtq ´ pI ` pStηq´1q´1˘ .

l

jh

n

A

(96) (97) (98) (99)
(100) (101)

We now separately bound the diagonal entries in matrix A. With si, we denote ith diagonal entry of S. Note that since η ď 1{ ||S||op, for all i, ηsi ď 1. Consider ith diagonal term (which is non-zero) of the diagonal matrix A, we have

¨

˛

1ˆ

t

tηsi ˙

1

´

p1

´

siηqt

˚ ˚

tηsi

‹ ‹

Aii “ si 1 ´ p1 ´ siηq ´ 1 ` tηsi “

si

˚1 ˚

´

p1

`

tηsiqp1

´

p1

´

‹ siηqtq ‹

˝

‚

l

jh

n

(102)

ď 1 „ 1 ´ p1 ´ siηqt  .

2

si

II
(Using Lemma 13 (b))

Additionally, we can also show the following upper bound on term 2:

tηsi

p1 ` tηsiqp1 ´ p1 ´ siηqtq ´ tηsi

1 ´ p1 ` tηsiqp1 ´ p1 ´ siηqtq “ p1 ` tηsiqp1 ´ p1 ´ siηqtq

1 ´ p1 ´ siηqt ´ tηsip1 ´ siηqt ď p1 ` tηsiqp1 ´ p1 ´ siηqtq

1 ď tηsi .

(103) (104) (Using Lemma 13 (a))

Combining both the upper bounds on each diagonal entry Aii, we have A ĺ c1pη, tq ¨ S´1pI ´ pI ´ ηSqtq ,
where c1pη, tq “ minp0.5, ts1iη q. Plugging this into (99), we have Ex„S „´ftpxq ´ frλpxq¯2 ď cpη, tq ¨ yT V X `S´1pI ´ pI ´ ηSqtq˘2 SV T XT y “ cpη, tq ¨ yT V X `S´1pI ´ pI ´ ηSqtq˘ S `S´1pI ´ pI ´ ηSqtq˘ V T XT y “ cpη, tq ¨ ||Xwt||22 “ cpη, tq ¨ Ex„S ”pftpxqq2ı ,
where cpη, tq “ minp0.25, t2s12i η2 q.

(105)
(106) (107) (108) (109)

Leveraging Unlabeled Data to Guarantee Generalization

Part 2 With Σ, we denote the underlying true covariance matrix. We now consider the squared difference of output at an unseen point:

„ ´

¯2

Ex„DX ftpxq ´ frλpxq

“

Ex„DX

“ˇˇˇˇxT wt

´

x

T

w

λ

ˇˇ ˇˇ

‰

r2

“ ˇˇˇˇxT V S´1pI ´ pI ´ ηSqtqV T XT y ´ xT V pS ` λIq´1V T XT yˇˇˇˇ2 “ ˇˇˇˇxT V `S´1pI ´ pI ´ ηSqtq ´ pS ` λIq´1˘ V T XT yˇˇˇˇ2 “ yT V X `S´1pI ´ pI ´ ηSqtq ´ pS ` λIq´1˘ V T ΣV

`pI ´ pI ´ ηSqtq ´ pS ` λIq´1˘ V T XT y

¨

˛2

(110)
(111) (112) (113) (114)

ď σmax ¨ yT V X ˚˚S´1pI ´ pI ´ ηSqtq ´ pS ` λIq´1‹‹ V T XT y ,

˝

‚

l

jh

n

I

(115)

where σmax is the maximum eigenvalue of the underlying covariance matrix Σ. Using the upper bound on term 1 in (105), we have

Ex„D

„ ´
ftpxq

´

¯2 frλpxq

ď

σmax

¨

cpη,

tq

¨

yT

V

X

`S´1pI

´

pI

´

ηSqtq˘2

V

T

XT

y

X

(116)

“ κ ¨ cpη, tq ¨ σmin ¨ ˇˇˇˇV `S´1pI ´ pI ´ ηSqtq˘ V T XT yˇˇˇˇ22

ď

κ ¨ cpη, tq ¨ “V

`S´1pI

´ pI

´

η

S

qt

˘ q

V

T XT ‰T

Σ

“V

`S´1pI

´ pI

´

η

S

qt

˘ q

V

TXT‰ y

“ κ ¨ cpη, tq ¨ Ex„DX “ˇˇˇˇxT wtˇˇˇˇ2‰ .

(117) (118) (119) (120)

B.5. Extension to deep learning

Under Assumption B.6, we present the formal result parallel to Theorem 3.

Theorem 6. Consider a multiclass classiﬁcation problem with k classes. Under Assumption 1, for any δ ą 0, with

probability at least 1 ´ δ, we have d

´

¯

EDpfpq ď ES pfpq ` pk ´ 1q 1 ´ k´k 1 ES pfpq ` c

r

logp 4δ q , 2m

(121)

? for some constant c ď ppc ` 1qk ` k ` m? q.
nk

The proof follows exactly as in step (i) to (iii) in Theorem 3.

B.6. Justifying Assumption 1
Motivated by the analysis on linear models, we now discuss alternate (and weaker) conditions that imply Assumption 1. We need hypothesis stability (Condition 1) and the following assumption relating training error and leave-one-error:
Assumption 2. Let fpbe a model obtained by training with algorithm A on a mixture of clean S and randomly labeled data Sr. Then we assume we have

ESM pfpq ď ELOOpSM q ,

r

r

for all pxi, yiq P SrM where fppiq :“ f pA, S Y SrM piqq and ELOOpSrM q :“ řpxi,yiqPSrM|SrM Ep|fppiqpxiq,yiq .

Intuitively, this assumption states that the error on a (mislabeled) datum px, yq included in the training set is less than the error on that datum px, yq obtained by a model trained on the training set S ´ tpx, yqu. We proved this for linear models

Leveraging Unlabeled Data to Guarantee Generalization

trained with GD in the proof of Theorem 5. Condition 1 with β “ Op1q and Assumption 2 together with Lemma 12 implies Assumption 1 with a polynomial residual term (instead of logarithmic in 1{δ):

d

ESM pfpq ď ED1 pfpq `

1 ˆ 1 3β ˙ δ m ` m`n .

(122)

Leveraging Unlabeled Data to Guarantee Generalization
C. Additional experiments and details
C.1. Datasets
Toy Dataset Assume ﬁxed constants µ and σ. For a given label y, we simulate features x in our toy classiﬁcation setup as follows:
x :“ concat rx1, x2s where x1 „ N py ¨ µ, σ2Idˆdq and x1 „ N p0, σ2Idˆdq . ?
In experiements throughout the paper, we ﬁx dimention d “ 100, µ “ 1.0, and σ “ d. Intuitively, x1 carries the information about the underlying label and x2 is additional noise independent of the underlying label. CV datasets We use MNIST (LeCun et al., 1998) and CIFAR10 (Krizhevsky & Hinton, 2009). We produce a binary variant from the multiclass classiﬁcation problem by mapping classes t0, 1, 2, 3, 4u to label 1 and t5, 6, 7, 8, 9u to label ´1. For CIFAR dataset, we also use the standard data augementation of random crop and horizontal ﬂip. PyTorch code is as follows:
(transforms.RandomCrop(32, padding=4), transforms.RandomHorizontalFlip())
NLP dataset We use IMDb Sentiment analysis (Maas et al., 2011) corpus.
C.2. Architecture Details
All experiments were run on NVIDIA GeForce RTX 2080 Ti GPUs. We used PyTorch (Paszke et al., 2019) and Keras with Tensorﬂow (Abadi et al., 2016) backend for experiments.
Linear model For the toy dataset, we simulate a linear model with scalar output and the same number of parameters as the number of dimensions.
Wide nets To simulate the NTK regime, we experiment with 2´layered wide nets. The PyTorch code for 2-layer wide MLP is as follows:
nn.Sequential( nn.Flatten(), nn.Linear(input dims, 200000, bias=True), nn.ReLU(), nn.Linear(200000, 1, bias=True) )
We experiment both (i) with the second layer ﬁxed at random initialization; (ii) and updating both layers’ weights.
Deep nets for CV tasks We consider a 4-layered MLP. The PyTorch code for 4-layer MLP is as follows:
nn.Sequential(nn.Flatten(), nn.Linear(input dim, 5000, bias=True), nn.ReLU(), nn.Linear(5000, 5000, bias=True), nn.ReLU(), nn.Linear(5000, 5000, bias=True), nn.ReLU(), nn.Linear(1024, num label, bias=True) )
For MNIST, we use 1000 nodes instead of 5000 nodes in the hidden layer. We also experiment with convolutional nets. In particular, we use ResNet18 (He et al., 2016). Implementation adapted from: https://github.com/kuangliu/ pytorch-cifar.git.
Deep nets for NLP We use a simple LSTM model with embeddings intialized with ELMo embeddings (Peters et al., 2018). Code adapted from: https://github.com/kamujun/elmo_experiments/blob/master/elmo_ experiment/notebooks/elmo_text_classification_on_imdb.ipynb

Leveraging Unlabeled Data to Guarantee Generalization

We also evaluate our bounds with a BERT model. In particular, we ﬁne-tune an off-the-shelf uncased BERT model (Devlin et al., 2018). Code adapted from Hugging Face Transformers (Wolf et al., 2020): https://huggingface.co/ transformers/v3.1.0/custom_datasets.html.

C.3. Additonal experiments

Results with SGD on underparameterized linear models

Accuracy

100 Underparameterized model

90

MSE

Test

CE

Predicted bound

80

70

60

50 0.0 0.1 0.2 0.3 0.4 Fraction of unlabeled data

Figure 3. We plot the accuracy and corresponding bound (RHS in (1)) at δ “ 0.1 for toy binary classiﬁcation task. Results aggregated over 3 seeds. Accuracy vs fraction of unlabeled data (w.r.t clean data) in the toy setup with a linear model trained with SGD. Results parallel to Fig. 2(a) with SGD.

Results with wide nets on binary MNIST

Accuracy Accuracy Accuracy

100

MNIST

90

80

70

60

GD Early stop

Test Predicted bound

50 0.0

Weight decay
0.1 0.2

0.3

0.4

Fraction of unlabeled data

(a) GD with MSE loss

100

MNIST

90

80

70

60

SGD Early stop

Test Predicted bound

50 0.0

Weight decay
0.1 0.2

0.3

0.4

Fraction of unlabeled data

(b) SGD with CE loss

100

MNIST

90

80

70

60

SGD Early stop

Test Predicted bound

50 0.0

Weight decay
0.1 0.2

0.3

0.4

Fraction of unlabeled data

(c) SGD with MSE loss

Figure 4. We plot the accuracy and corresponding bound (RHS in (1)) at δ “ 0.1 for binary MNIST classiﬁcation. Results aggregated over 3 seeds. Accuracy vs fraction of unlabeled data for a 2-layer wide network on binary MNIST with both the layers training in (a,b) and only ﬁrst layer training in (c). Results parallel to Fig. 2(b) .

Results on CIFAR 10 and MNIST We plot epoch wise error curve for results in Table 1(Fig. 5 and Fig. 6). We observe the same trend as in Fig. 1. Additionally, we plot an oracle bound obtained by tracking the error on mislabeled data which nevertheless were predicted as true label. To obtain an exact emprical value of the oracle bound, we need underlying true labels for the randomly labeled data. While with just access to extra unlabeled data we cannot calculate oracle bound, we note that the oracle bound is very tight and never violated in practice underscoring an importamt aspect of generalization in multiclass problems. This highlight that even a stronger conjecture may hold in multiclass classiﬁcation, i.e., error on mislabeled data (where nevertheless true label was predicted) lower bounds the population error on the distribution of mislabeled data and hence, the error on (a speciﬁc) mislabeled portion predicts the population accuracy on clean data. On the other hand, the dominating term of in Theorem 3 is loose when compared with the oracle bound. The main reason, we believe is the pessimistic upper bound in (45) in the proof of Lemma 8. We leave an investigation on this gap for future.
Results on CIFAR 100 On CIFAR100, our bound in (5) yields vacous bounds. However, the oracle bound as explained above yields tight guarantees in the initial phase of the learning (i.e., when learning rate is less than 0.1) (Fig. 7).

C.4. Hyperparameter Details
Fig. 1 We use clean training dataset of size 40, 000. We ﬁx the amount of unlabeled data at 20% of the clean size, i.e. we include additional 8, 000 points with randomly assigned labels. We use test set of 10, 000 points. For both MLP and ResNet,

Accuracy

Leveraging Unlabeled Data to Guarantee Generalization

100

90 80

Test accuracy Predicted bound

70 Oracle bound

60

50

40

30

20

10 0 10 20 30 40 50 60 70

Epoch

(a) MLP

Accuracy

100

90

80

70

60

50

40

Test accuracy

30

Predicted bound

20

Oracle bound

10 0 10 20 30 40 50

Epoch

(b) ResNet

Figure 5. Per epoch curves for CIFAR10 corresponding results in Table 1. As before, we just plot the dominating term in the RHS of (5) as predicted bound. Additionally, we also plot the predicted lower bound by the error on mislabeled data which nevertheless were predicted as true label. We refer to this as “Oracle bound”. See text for more details.

Accuracy

100

90

80

70

60

50

40

Test accuracy

30

Predicted bound

20

Oracle bound

100.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0

Epoch

(a) MLP

Accuracy

100

90

80

70

60

50

40

Test accuracy

30

Predicted bound

20

Oracle bound

100.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0

Epoch

(b) ResNet

Figure 6. Per epoch curves for MNIST corresponding results in Table 1. As before, we just plot the dominating term in the RHS of (5) as predicted bound. Additionally, we also plot the predicted lower bound by the error on mislabeled data which nevertheless were predicted as true label. We refer to this as “Oracle bound”. See text for more details.

we use SGD with an initial learning rate of 0.1 and momentum 0.9. We ﬁx the weight decay parameter at 5 ˆ 10´4. After 100 epochs, we decay the learning rate to 0.01. We use SGD batch size of 100.
Fig. 2 (a) We obtain a toy dataset according to the process described in Sec. C.1. We ﬁx d “ 100 and create a dataset of 50, 000 points with balanced classes. Moreover, we sample additional covariates with the same procedure to create randomly labeled dataset. For both SGD and GD training, we use a ﬁxed learning rate 0.1.
Fig. 2 (b) Similar to binary CIFAR, we use clean training dataset of size 40, 000 and ﬁx the amount of unlabeled data at 20% of the clean dataset size. To train wide nets, we use a ﬁxed learning of 0.001 with GD and SGD. We decide the weight decay parameter and the early stopping point that maximizes our generalization bound (i.e. without peeking at unseen data ). We use SGD batch size of 100.
Fig. 2 (c) With IMDb dataset, we use a clean dataset of size 20, 000 and as before, ﬁx the amount of unlabeled data at 20% of the clean data. To train ELMo model, we use Adam optimizer with a ﬁxed learning rate 0.01 and weight decay 10´6 to minimize cross entropy loss. We train with batch size 32 for 3 epochs. To ﬁne-tune BERT model, we use Adam optimizer with learning rate 5 ˆ 10´5 to minimize cross entropy loss. We train with a batch size of 16 for 1 epoch.
Table 1 For multiclass datasets, we train both MLP and ResNet with the same hyperparameters as described before. We sample a clean training dataset of size 40, 000 and ﬁx the amount of unlabeled data at 20% of the clean size. We use SGD with an initial learning rate of 0.1 and momentum 0.9. We ﬁx the weight decay parameter at 5 ˆ 10´4. After 30 epochs for ResNet and after 50 epochs for MLP, we decay the learning rate to 0.01. We use SGD with batch size 100. For Fig. 7, we use the same hyperparameters as CIFAR10 training, except we now decay learning rate after 100 epochs.
In all experiments, to identify the best possible accuracy on just the clean data, we use the exact same set of hyperparamters except the stopping point. We choose a stopping point that maximizes test performance.

Leveraging Unlabeled Data to Guarantee Generalization

Accuracy

100 80 60 40 20 00

Test accuracy Oracle bound 20 40 Epo6c0h

80 100

Figure 7. Predicted lower bound by the error on mislabeled data which nevertheless were predicted as true label with ResNet18 on CIFAR100. We refer to this as “Oracle bound”. See text for more details. The bound predicted by RATT (RHS in (5)) is vacuous.

C.5. Summary of experiments

Classiﬁcation type Binary
Multiclass

Model category Low dimensional Overparameterized
linear nets
Deep nets
Deep nets

Model Linear model 2-layer wide net
MLP
ResNet ELMo-LSTM model BERT pre-trained model
MLP
ResNet

Dataset
Toy Gaussain dataset
Binary MNIST
Binary MNIST Binary CIFAR Binary MNIST Binary CIFAR IMDb Sentiment Analysis IMDb Sentiment Analysis
MNIST CIFAR10 MNIST CIFAR10 CIFAR100

Leveraging Unlabeled Data to Guarantee Generalization

D. Proof of Lemma 12

Proof of Lemma 12. Recall, we have a training set S Y SrC. We deﬁned leave-one-out error on mislabeled points as

ř
px ,y qPS

E pfpiqpxiq, yiq

ELOOpS q “

i i rM

,

rM

SrM

where fpiq :“ f pA, pS Y Srqpiqq. Deﬁne S1 :“ S Y Sr. Assume px, yq and px1, y1q as i.i.d. samples from D1. Using Lemma 25 in Bousquet & Elisseeff (2002), we have

„ ´

¯2

”

ı

”

ı

E ED1 pfpq ´ ELOOpS q ďES1,px,yq,px1,y1q E pfppxq, yqE pfppx1q, y1q ´ 2ES1,px,yq E pfppxq, yqE pfpiqpxiq, yiq

rM

m1 ´ 1 “

‰1 “

‰

` m1 ES1 E pfpiqpxiq, yiqE pfpjqpxj q, yj q ` m1 ES1 E pfpiqpxiq, yiq .

(123)

We can rewrite the equation above as :

„ ´

¯2

”

ı

E ED1 pfpq ´ ELOOpS q ď ES1,px,yq,px1,y1q E pfppxq, yqE pfppx1q, y1q ´ E pfppxq, yqE pfpiqpxiq, yiq

rM

l

jh

n

I

”

ı

` ES1 Epfpiqpxiq, yiqEpfpjqpxjq, yjq ´ Epfppxq, yqEpfpiqpxiq, yiq

l

jh

n

II

1“

‰

` m1 ES1 E pfpiqpxiq, yiq ´ E pfpiqpxiq, yiqE pfpjqpxj q, yj q .

l

jh

n

III

(124)

We will now bound term III. Using Cauchy-Schwarz’s inequality, we have

ES1 “E pfpiqpxiq, yiq ´ E pfpiqpxiq, yiqE pfpjqpxj q, yj q‰2 ď ES1 “E pfpiqpxiq, yiq‰2 ES1 “1 ´ E pfpjqpxj q, yj q‰2 1
ď 4.

(125) (126)

Note that since pxi, yiq, pxj, yjq, px, yq, and px1, y1q are all from same distribution D1, we directly incorporate the bounds on term I and II from the proof of Lemma 9 in Bousquet & Elisseeff (2002). Combining that with (126) and our deﬁnition of hypothesis stability in Condition 1, we have the required claim.

