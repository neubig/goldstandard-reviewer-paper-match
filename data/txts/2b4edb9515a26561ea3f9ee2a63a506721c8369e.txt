Aspect-based Sentiment Analysis of Scientific Reviews

arXiv:2006.03257v1 [cs.CL] 5 Jun 2020

Souvic Chakraborty
Indian Institute of Technology, Kharagpur
West bengal, India chakra.souvic@gmail.com

Pawan Goyal
Indian Institute of Technology, Kharagpur
West bengal, India pawang@cse.iitkgp.ac.in

Animesh Mukherjee
Indian Institute of Technology, Kharagpur
West bengal, India animeshm@cse.iitkgp.ac.in

ABSTRACT
Scientific papers are complex and understanding the usefulness of these papers requires prior knowledge. Peer reviews are comments on a paper provided by designated experts on that field and hold a substantial amount of information, not only for the editors and chairs to make the final decision, but also to judge the potential impact of the paper. In this paper, we propose to use aspect-based sentiment analysis of scientific reviews to be able to extract useful information, which correlates well with the accept/reject decision.
While working on a dataset of close to 8k reviews from ICLR, one of the top conferences in the field of machine learning, we use an active learning framework to build a training dataset for aspect prediction, which is further used to obtain the aspects and sentiments for the entire dataset. We show that the distribution of aspect-based sentiments obtained from a review is significantly different for accepted and rejected papers. We use the aspect sentiments from these reviews to make an intriguing observation, certain aspects present in a paper and discussed in the review strongly determine the final recommendation. As a second objective, we quantify the extent of disagreement among the reviewers refereeing a paper. We also investigate the extent of disagreement between the reviewers and the chair and find that the inter-reviewer disagreement may have a link to the disagreement with the chair. One of the most interesting observations from this study is that reviews, where the reviewer score and the aspect sentiments extracted from the review text written by the reviewer are consistent, are also more likely to be concurrent with the chair’s decision.
1 INTRODUCTION
Peer reviewing is an integral part in identifying important and relevant research in any specific area of science. This has been adopted by major journals and conferences in many areas of science including computer science. However, peer reviewing is manual and it takes a huge amount of time even for an expert in that particular field to review a scientific document. Researchers have spent approximately 63.4 million hours on peer reviews in 2015 alone [13]. Nevertheless, the process is not foolproof and there have been many recent studies on the inefficacy of the peer-reviewing system and many alternate systems have been proposed. A major conference in the field of computer science did a study in 2014, assigning 10% of their papers to two different groups of reviewers to analyze the reproducibility and subjectivity in reviews. Disagreement over the accept/reject decision was found for a

quarter of those papers even when that conference adopts a three-reviewer majority-vote based algorithm for taking the final decision for any paper [14].
Ragone et al. [18] found that there is very little correlation between peer reviewers’ comments and citation counts of these papers. They also report that the peer reviews are not very effective in finding the major flaws in a paper.
These criticisms make studies on peer review dataset very important as it helps to improve science directly by identifying flaws in the scientific process of highlighting best research thus inspiring better feedback mechanisms to draw attention of serious researchers. However, till now, there have been very few studies on this front mainly due to unavailability of peer-review dataset in the public domain.
To address this need, we perform a large scale analysis on the public dataset of peer reviews made available by one of the largest conferences in the area of machine learning – ICLR1. ICLR has peer reviews for both the accepted and rejected papers available with rebuttal comments.
We postulate that the reviewers largely follow a welldefined structure while writing reviews identifying the pros and cons of the paper and a peer review corpus is therefore sentiment rich as it is supposed to reflect the qualities of the paper to help the chair/editor understand whether it should be accepted. In particular the reviews would be organized around various aspects and the reviewers would tend to express varying sentiments across these aspects.
In this paper we adopt the reviewing aspects used in the ACL conferences. We manually inspected the reviewing guidelines of various top CS conferences and finally selected the ACL format. This choice is motivated by the level of details that the ACL format brings on board. As per the ACL reviewing guidelines, the eight different aspects that a paper is rated on are – appropriateness, clarity, originality, empirical/theoretical soundness, meaningful comparison, substance, impact of dataset/software/ideas and recommendation.
However, note that our peer review corpora –ICLR 2017, 2018 and 2019 – are not annotated based on these aspects. Therefore we design an annotation framework to annotate each review with the relevant aspects and the corresponding aspect sentiments present in the review. Following this we perform rigorous data analysis to discover many interesting results illustrated with figures and tables in the later sections. We present the most intuitive explanations of the results in their individual sections as well as in the conclusion. Our specific contributions are enumerated below.
1 https://openreview.net/group?id=ICLR.cc

∙ We collect peer review data for one of the top machine learning conferences – ICLR. In addition, for every paper we also separately collect various metadata like the authors, the abstracts, rebuttal comments, confidence score of the reviews etc.
∙ We design an annotation framework and manually annotate ∼ 2, 500 review sentences at two levels – (i) identify and mark the aspects present (if any) in the sentence and (ii) the sentiment associated with each of the identified aspect. We use an active learning framework to choose the sentences to be annotated.
∙ We design a supervised algorithm to label the aspects and the sentiments for the rest of the corpus.
∙ We analyze the machine-labelled data to obtain various interesting insights. The aspect sentiment polarities for the accepted papers are substantially different from those of the rejected papers. For accepted papers, the reviewers express higher positive sentiments in appropriateness, clarity, originality, empirical/theoretical soundness, substance, impact of ideas and recommendation. On the other hand, the rejected papers have higher negative sentiments in aspects like appropriateness, clarity, empirical/theoretical soundness, substance, impact of ideas and recommendation.
∙ We further identify why the chair might differ from the mean recommendation of the reviewers and establish the importance of review texts through that. In particular, we define inter-reviewer disagreement and establish that aspect level disagreements have clear correlation with aggregate level disagreement. Evidently, the chair disagrees more in cases where the correlation between the recommendation score and the corresponding review aspect sentiments are low, i.e., where scores exhibit individual bias on part of the reviewers (which is different from the normative behavior of the reviewers.
∙ Finally, one of the most interesting findings is that sentiments associated with aspects like clarity, empirical/theoretical soundness and impact of ideas strongly determine the intended recommendation in a review. In both the methodologies (linear and non-linear), used to determine the importance of the aspects, we come up with similar results. This might be effective in telling the authors which aspects they should focus on in order to get their papers accepted.
Organisation of the paper: The rest of the paper is structured as follows. In Section 2 we review some of the relevant works in this area. In Section 3 we describe the ICLR review dataset that we work on and how we label the data with aspect sentiments. In the next section we perform an exploratory analysis of the aspect sentiment annotated reviews. We study the disagreement of the reviewers in Section 6. We find how the different aspect sentiments in the review text are correlated to the overall recommendation 7. Finally we conclude in Section 8.

2 RELATED WORK
We divide the related work into two broad segments. The first discusses the techniques and developments in sentiment analysis. The second details the studies so far done on peerreview text.
2.1 Sentiment prediction
Detection of sentiment and its classification has been the focus of research in sentiment analysis. Researchers have used unsupervised methods [24], lexicon based methods [12, 16], transfer learning [9, 27], deep learning architectures [15, 26] etc. to predict the sentiments.
Variants of this main task have also been extensively proposed. Apart from the sentiment prediction of the sentence as a whole, other tasks like aspect-level sentiment prediction [10] and target-dependent sentiment prediction [21] are also in place.
For instance, aspect based sentiment analysis has previously been carried out on product reviews [10, 21], movie reviews [22], tweets [3], hotel reviews [1] and so on. It has also been observed that the classifiers trained in sentiment prediction task in one domain do not generalize well when applied on a different dataset from a different domain [17].
The specific task that we solve here does not have an aspect-sentiment annotated peer review dataset and hence we make a crucial contribution by building such a dataset. In addition we use different classifier architectures and perform rigorous experiments on the validation dataset to choose the best among these architectures.
2.2 Analysis of peer review dataset
In absence of a public domain peer review dataset with sufficient datapoints, most works on peer review before 2017 have been on private datasets limiting the number of works. In a case study on peer review data of Journal of High Energy Physics, Sikdar et al. [19] has identified key features to cluster anomalous editors and reviewers whose works are often not aligned with the goal of the peer review process in general. Sikdar et al. [20] show that specific characteristics of the assigned reviewers influence the long-term citation profile of a paper more than the linguistic features of the review or the author characteristics. Gender bias in peer-review data has been studied in [8]. Tomkins et al. [23] established that a single blind reviewing system extends disproportionate advantage to the papers authored by famous scientists and scientists from highly reputed institutions.
Kang et al. [11] collects and analyzes openly available peerreview data for the first time and provides several baselines defining major tasks. Wang and Wan [25] uses state-of-the-art techniques like a Multiple Instance Learning Framework [2, 6] with modifications using attention mechanism using the abstract text for sentiment analysis and recommendation score prediction. Gao et al. [7] shows that author rebuttals play little role in changing the recommendation scores of a paper while a larger peer pressure plays a greater role for the non-conforming reviewer in the rebuttal phase.

Oral Poster Workshop Rejected (All tracks)

2017 15 183

48

245

2018 23 313

195

634

2019 24 478

108

1077

Table 1: Number of papers in the ICLR dataset from 2017 to

2019. Note that some of the papers may have been withdrawn

and we have removed those entries from the calculations here-

after.

3 DATASET
Machine learning conferences like ICLR have adopted an ‘open review’ model making all the comments, author rebuttals and reviews public. Kang et al. [11] has open sourced the framework to collect data from ICLR. However, on close observation of the data quality, we decided to collect the data again using our own framework to avoid mixing of some parts of the data like author rebuttals with the reviews which might have introduced specific biases in the dataset.
3.1 Description of the raw dataset
ICLR reviews are completely public and all parts of the reviews are readily accessible online. All the comments and author rebuttals are also available. Final score of recommendation is available in a scale of 1-10 for each of the individual reviews. We mined a total of 5,289 ICLR papers (main conference + workshops); however before 2017 this data is very noisy and incomplete. In fact, prior to 2017, the volume of the data is very less and the quality of the data is also poor as the author rebuttals cannot be separated from the reviews automatically owing to the design of the openreview website. Therefore we use the data for the years 2017, 2018 and 2019 only which makes a total of 3,343 papers. The statistics of the data is noted in Table 1.
3.2 Data annotation process
Peer reviews consist of several technical words and most of the sentences in peer reviews are devoid of any sentiment. Thus, it is difficult to come up with a framework to get enough training data labelled to train the classifier where the resource is limited. Therefore, we use a special type of batch-mode active learning framework to annotate the data.
3.2.1 Avoiding cold start. If we start annotating using a random pool of sentences, we may find that most of the sentences are non-informative and devoid of any sentiment. Thus, if we had a mechanism to filter out sentences which are quite likely to have the presence of the aspect sentiments, we may get a more balanced corpus.
We tackle the problem as follows. We choose a set of 50 sentiment-bearing sentences equally distributed across various aspects (mentioned below) from randomly chosen reviews, and generate sentence embeddings using Google’s universal sentence encoder [5]. Then we choose 30 closest sentences in the embedding space in terms of cosine similarity for each of these seed sentences. We repeat the same process once

again taking these newly found sentences as seed. Finally, we use the 𝑛-gram based Jaccard similarity metric to put these sentences into 300 clusters using 𝐾-means clustering and randomly select 1,500 sentences from these clusters with equal representation from each cluster. We finally get a reduced set of 1,321 sentences after the final label selection using the aggregation methodology discussed in the Annotation guidelines section.
3.2.2 Collecting more data for annotation. In order to generate more annotated data we use the standard batch selection method used for active learning. Using the already collected batch of sentences, we train a simple random forest classifier. Now we run this classifier on the rest of the sentences to label them. From this set of automatically labelled sentences, we choose 15,000 sentences, 70% of which are taken from the high entropy zone, i.e., where the classifier is most confused (we take the sentences with entropy larger than 0.5) and the 30% from the rest of the sentences. One can imagine that for the 70% high entropy sentences, the confusion of the classifier is so high that these can be literally considered ‘unannotated’ and hence the need for manual annotation. We again use the 𝑛-gram based similarity as earlier and decompose the sentences into 300 clusters and choose another 1,500 sentences randomly representing all the clusters equally.
3.2.3 Annotation guidelines. Each of the above 3,000 sentences are annotated by three experts who are ML/NLP researchers and are quite familiar with the peer-review process in these areas. In our annotation framework, each annotator has to label every sentence with the aspects present in the sentence and the corresponding sentiment that binds to that aspect. For our work, we use the aspect labels used for the ACL 20162 reviews. There are eight aspects, namely, appropriateness, clarity, originality, empirical/theoretical soundness, meaningful comparison, substance, impact of dataset/ software/ ideas and recommendation in this guideline. The sentiment binding to an aspect could be one of the four labels – ‘positive’, ‘negative’, ‘neutral’ and ‘absent’. We ask the annotators to annotate the aspect and the sentiment labels present in each sentence. Note that a particular sentence might not have any aspect (and therefore no sentiments) or multiple aspects with a sentiment binding to each of these aspects. Two annotators work on the annotation at first. In case, two of the annotators agree on a specific label, it is taken for training, else, the third annotator is asked to resolve the disagreement. The third annotator may choose one of the two labels provided by the other two annotators or mark the sentence as ambiguous. We annotated a total of 3,091 sentences and found that the first two annotators agree for 63.1% of the sentences (1,951). Out of the remaining 1,140 sentences, 502 sentences were discarded as ambiguous by the third annotator, leaving us with a total of 2,589 annotated sentences.
After the annotation, we present the statistics for various aspects and sentiments in Table 2.
2 http://mirror.aclweb.org/acl2016/

Aspects

Positive Negative Neutral

Appropriateness

12

83

11

Clarity

198

359

20

Originality

166

65

24

Empirical/Theoretical Soundness

71

278

64

Meaningful Comparison

53

182

25

Substance

20

54

2

Impact Of Ideas/Results/Dataset 171

152

38

Recommendation

32

43

9

Table 2: Distribution of sentiments across various aspects in

the annotated 2,589 sentences. Note that, we only present the

distributions for the sentences where an aspect is present.

4 ASPECT-LEVEL SENTIMENT CLASSIFICATION
We employ several established methods for the aspect-level sentiment classification task. To begin with we use various traditional approaches like Multinomial Naïve Bayes (MNB), Random Forest (RF) and SVM with linear (SVM-lin) and rbf (SVM-rbf) kernels.
Next we use a bunch of modern deep learning techniques like BiLSTM with CNN, FFNN fed with (i) Google’s universal sentence encoder embeddings (FFNN-uni) [5] and (ii) SciBERT embeddings (FFNN-sci) [4]. Micro-F1 scores (to handle the class imbalance problem) using 10-fold cross-validation for the aspect detection task are reported in Table 3 for each aspect.

Classifiers

F1-score

MNB

0.56

RF

0.62

SVM-rbf

0.62

SVM-lin

0.60

BiLSTM-CNN 0.63

FFNN-uni

0.71

FFNN-sci

0.70

Table 3: Performance of the classifiers in the aspect detection

task.

From the results in the table we observe that FFNN-uni outperforms all the other techniques. FFNN-sci is a close competitor; however it takes far more time for training. We further note the aspect-level sentiment detection performance in Table 4 using the best classifier (FFNN-uni). Since our classes are unbalanced, we have reported the class wise macro averaged F1-score which is most appropriate in such a situation. Note that the scores for Substance and Recommendation are lower than the scores of the other aspects. Lower number of training instances (specifically, for the “neutral” sentiment category) may be a possible reason here. Albeit, the classifier is able to pick up useful information in all the cases having better macro-averaged F1-scores than a majority classifier dealing with 4 classes. Some representative examples of aspect sentiments obtained using this model are noted in Table 5.

Aspects

Precision Recall Macro F1

Appropriateness

0.83

0.94

0.86

Clarity

0.78

0.91

0.83

Originality

0.69

0.79

0.72

Emp./Theo. Soundness

0.62

0.70

0.65

Meaningful Comparison 0.78

0.88

0.81

Substance

0.50

0.56

0.53

Impact of Ideas

0.70

0.69

0.69

Recommendation

0.64

0.62

0.62

Table 4: Class wise sentiment prediction performance for the

best model (FFNN-uni). Since the classes are unbalanced, all

values reported here are macro average of the class wise re-

sults.

The results show that the classifier fidelity is reasonably well and can be reliably used for annotating the full dataset. We therefore run the best classifier on the full dataset and perform a rigorous exploratory analysis in the next section.
5 EXPLORATORY DATA ANALYSIS
Using the automatically annotated full dataset, we investigate the connection between the aspect sentiments of the reviews and the final accept/reject decision of a paper. Aggregation of the results: For the subsequent analysis, we aggregate the results as follows. For a given paper we collect all the review sentences written by all the reviewers. For each aspect, we aggregate the positive and negative sentiment scores separately and normalize each of these scores by the total number of sentences in the reviews for that paper.
5.1 Aspect sentiment distribution
In the 2017 – 2019 block, the total number of accepted ICLR papers is 1,035 and rejected papers is 1,646. The year wise distribution is shown in Table 6. The number of sentences, the average sentence length and the number of unique words are noted in Table 7. Sentiment distribution: In Figure 1 we plot the distribution of the positive sentiments binding to different aspects across the accepted and the rejected papers. Note that the aggregation has been done as discussed earlier. After obtaining the mean positive (or negative) scores for all the reviews, we normalize these for each contrasting facet. Thus, while for each aspect, the figure shows the distribution between accept and reject papers, the relative heights of the bars across aspects are not comparable. We use the same strategy across all the figures for easy visualization. The results show some very interesting trends. The accepted papers have a substantially higher positive sentiment for all the aspects – appropriateness, originality, clarity, empirical/theoretical soundness, meaningful comparison, substance, impact of ideas and recommendation.
In Figure 2 we plot the distribution of the negative sentiments binding to different aspects across the accepted and the rejected papers. The rejected papers have significantly higher negative sentiment for the aspects appropriateness,

Aspects

Polarity Sentences

Positive in summary, i think the paper can be accepted for iclr.

Appropriateness

Neutral in the end, it’s a useful paper to read, but it’s not going to be the highlight of the conference.

Negative i find this paper not suitable for iclr.

Positive the paper is clear, well organized, well written and easy to follow.

Clarity

Neutral the writing is fairly clear, though many of the charts and tables are hard to decipher without labels

Negative the paper needs major improvements in terms of clarity .

Positive the proposed idea is interesting and novel.

Originality

Neutral this is another simple idea that appears to be effective.

Negative since both approaches have been published before , the novelty seems to be limited.

Positive the experimental results support the theoretical predictions.

Empirical/Theoretical Soundness Neutral lastly, this paper also provides some simulation studies.

Negative i don’t think any of the experiments reported actually refute any of the original paper’s claim.

Positive the related work section is complete and well documented.

Meaningful Comparison

Neutral they are the best in terms of precision .

Negative however, no comparison is clearly made.

Positive pros: - the proposed method leads to state of the art results .

Substance

Neutral pros: - real hardware results are provided.

Negative the experiments do not present any baselines,

so it is unclear how well the method performs compared to the alternatives.

Positive evaluation on a very large real dataset demonstrates the usefulness of the model for real world tasks.

Impact of ideas/results/dataset Neutral the model is tested on a proprietary dataset of real manufacturing product.

Negative the study is restricted to 2-dimensional synthetic datasets.

Recommendation

Positive Neutral

i strongly recommend to accept this paper. after reading their feedback i still believe that novelty is incremental and would like to keep my score.

Negative overall, i’m afraid i must recommend that this paper be rejected.

Table 5: Examples of detected sentiments for different aspects. The sentences chosen are those where the best classifier (FFNN-

uni) predicts with a high confidence.

Conf.

#Accepted papers #Rejected papers

ICLR 2017

198

244

ICLR 2018

336

486

ICLR 2019

501

916

Table 6: Distribution of the accepted and rejected conference

papers for the ICLR dataset (not showing withdrawn papers).

Parameters

Accepted

Rejected

#Sentences

20.24±13.34 20.77±12.97

Avg. sent. length 18.59±12.08 18.03±11.77

#Unique words 197.14±93.53 195.73±88.82

Table 7: Various statistics for the ICLR review dataset. We see

that the reviews for the accepted and rejected papers follow

very similar distributions in terms of the number of sentences,

average sentence length and number of unique words.

clarity, empirical/theoretical soundness, meaningful comparison, substance, impact of ideas and recommendation. So, apart from the aspect originality, where the scores are very close, in all cases, negative sentiments are expressed more for rejected papers on average.

5.2 Individual recommendation scores vs the final recommendation decision
We investigate here how the final decision about the paper taken by the chair tallies with the individual reviewer decisions. In order to quantify individual reviewer decision, we consider their recommendation scores of 1-5 to a paper as reject and 6-10 as accept. Next we estimate what percentage of the reviewers disagree with the final recommendation of the chair. If the review scores were completely random this would be 50%. In the ICLR dataset we find this to be 24.9%. If the disagreement is calculated based on majority voting then this comes down to 14.8% which indicates that the chair’s decision may significantly vary from the recommendation of the majority of the reviewers. Also, it indicates that the reviewers tend to be more judicious in deciding which half their review scores will fall in between (1-5) and (6-10) bins than assigning the relative scores within each bin if the chair’s decision is assumed to be correct. We also explicitly compute the correlation of the recommendation scores with the final recommendation decision. The correlation statistics using three different measures is shown in Table 8. Once again we observe that the correlation values never go beyond 0.78.
5.3 Why review texts are important when scores are already present?
As we explored in the previous section, the weighted mean of the scores correlate well with the final recommendation. However, we also find that the final decision matches with

Figure 1: Distribution of positive sentiments binding to different aspects across accepted and rejected papers in the ICLR dataset. Note that the positive sentiment scores for the accepted and rejected papers are normalized across each aspect to add up to 1. Thus, the relative heights across aspects are not comparable.

greater than 5 and as “reject” otherwise. We run our model to find the aspect sentiment distribution for these reviews. Next we estimate the correlation between each of the aspect sentiment scores with the decision given by that reviewer. We find that the correlation scores are significantly higher in most of the aspects (for other cases the correlation scores are negligible for both cases) for the reviewers whose decisions agree with the chair’s decision. The results of this analysis are shown in Fig. 3 and Fig. 4. We show both the results of rank correlation and correlation by value to strengthen the validity of our results.
Further, we check the average confidence of the reviewers who agree with the chair’s decision vs who do not. The mean confidence of the reviewers who agree is 3.88 which is higher than the mean confidence of the reviewers who do not, which is 3.67. We test the statistical significance of this result for these two distributions and find the 𝑡-statistic to be 6.83 with a 𝑝-value of 1.07 x 10−11, thus rejecting the null hypothesis considerably. Hence, in conclusion, the reviewers who do not agree with the chair’s overall decision generally have less confidence and have low correlation between the sentiments expressed in their review text and their review decision. This can be used in future to model the reviewers and to identify the best reviewers as some reviewers clearly are more consistent in text and the scores and their reviews are more likely to help the decision of the chair.

Figure 2: Distribution of negative sentiments binding to different aspects across accepted and rejected papers in the ICLR dataset. Normalization is done similar to Figure 1.

the mean score for almost 90% of the cases. Thus, for the rest of the cases, the chair has to look at the text more thoroughly and make the final decision rather than depending upon the scores given. Also, it will be difficult to identify which papers will fall in that “90%” due to high rate of disagreement among the reviewers and absence of any other good indicator. A possible explanation for this scenario is that there exists disparity between the review written by the reviewers and the scores given by them. We can check the aspect sentiment scores to verify our assumption.
We perform a thorough analysis on the text of the reviews whose score based decision agrees with the chair’s decision vs whose score based decision does not, where we define the decision of the reviewer as “accept” if the review score is

Figure 3: Pearson’s correlation coefficient between difference of positive and negative sentiments binding to different aspects across reviews which disagree and agree with the final recommendation, respectively. Note that the sentiment scores for the reviews are normalized across each aspect to add up to 1 so that the relative difference in correlation values can be visualized better. Thus, the relative heights across aspects are not comparable. The 𝑝-values across these correlation scores are less than 0.001.

Figure 4: Spearman’s rank correlation coefficient between difference of positive and negative sentiments binding to different aspects across reviews which disagree and agree with the final recommendation, respectively. Note that the sentiment scores for the reviews are normalized across each aspect to add up to 1 so that the relative difference in correlation values can be visualized better. Thus, the relative heights across aspects are not comparable. The 𝑝-values across these correlation scores are less than 0.001.

Correlation

Pearson’s Spearman’s Rank Kendall’s 𝜏

Mean Score

0.71

0.74

0.62

Median Score

0.69

0.71

0.63

Majority Voting

0.67

0.67

0.67

Weighted Mean Score 0.75

0.78

0.64

Table 8: Correlation of recommendation scores with the final recommendation decision of the chair (𝑝-value < 10−150 in

all these cases.)

6 DISAGREEMENT AMONG THE REVIEWERS
In the previous section we already observed that in around 10% cases the mean score of the reviewers do not match with the final recommendation. This observation indicates that there are cases of disagreement among the reviewers of a paper on the scores they awarded. In this section, therefore, we study the reviews for the differences of opinion among the reviewers. Intuitively, the papers which are unanimously accepted or rejected must be different from those in which the reviewers disagree upon. However, for the purpose of this investigation, we first need to have a quantitative definition of disagreement.
6.1 Definition
We define aggregate disagreement as the standard deviation of the recommendation scores normalized by the maximum standard deviation possible in case of n reviewers assigned to every paper. In our case, 𝑛 = 3 and the obvious case of highest disagreement arises when the recommendation scores are 1, 1, 10 or 10, 10, 1. In both the cases, the standard

Figure 5: Distribution of average aggregate disagreement values across different bins of average review scores.
deviation is 4.26 which is used as a constant to normalize the scores.
6.2 Disagreement distribution
In the Figure 5 we plot the average aggregate disagreement values (𝑦-axis) for different bins of average review score that a paper receives (𝑥-axis). For instance, 𝑥 = 2 represents the set of all papers receiving an average reviewer score in the range [1,2) and the average of the aggregate disagreement values of this set of papers is the corresponding 𝑦 value. Similarly, 𝑥 = 3 denotes [2,3) and so on. We observe that the 𝑦 values are centered, i.e., the values are non-decreasing till score of 5 and strictly decreasing thereafter. This is quite obvious though as the cases where reviewers are not confident, they are likely to mark a paper in the boundary (score = [5-6)) making the middle part of the curve high in average aggregate disagreement from other reviewers. An alternative possible explanation may be that there is more space to disagree in case when the mean value is near 5 than when the mean value is at the one of the extremes.
6.3 Disagreement: Aspect level vs aggregate
Here we wish to observe if the disagreement at the aspect level correlates with the aggregate disagreement defined in the previous section. We compute the aspect level disagreement score by taking the standard deviation of the difference of positive and negative sentiment scores for a review. For every paper therefore we obtain an aspect level disagreement score. Next we divide the papers in three bins based on their aspect level disagreement score (low (33rd percentile), high (66th percentile) and mid (between 33rd and 66th percentile)). For the papers in each bin, we compute the average aggregate disagreement score. We plot the results in Fig. 6. The blank columns signify that there is no paper in that bin. We see that higher disagreement at aspect level on average corresponds to higher disagreement at aggregate level for all the aspects.

Aspects

Pearson Correlation Co-eff

Appropriateness

0.066

Clarity

0.212

Originality

0.028

Empirical/theoretical soundness

0.267

Meaningful Comparison

0.101

Substance

0.010

Impact of Ideas

0.273

Recommendation

0.074

Table 9: Correlation of the aspect sentiments with the final

recommendation.

Figure 6: Aggregate vs aspect level disagreements.
6.4 When will the chair have to intervene?
In a previous section, we saw that the chair does not go always with the mean decision of the reviewers. We wish to explore if disagreement has some role to play in those cases. We divide the papers in two bins – (i) bin containing papers where the decision of the chair matches with mean score awarded by the reviewer and (ii) bin containing papers where the decision of the chair does not match. The average aggregate disagreement score for bin (i) is 0.175 which is far lower than bin (ii) where the value is 0.229 (~30% higher). Indeed therefore the disagreement among the reviewers is much larger in the latter case needing the chair’s intervention.
7 DETERMINING THE IMPORTANCE OF THE ASPECTS IN RECOMMENDATION
We conduct two different studies here to understand which features contribute more to the final recommendation. First we observe the correlation of the aspect sentiments of the different aspects and the final recommendation. Second we develop a full-fledged deep neural model that uses the aspect sentiments to predict the final recommendation. The aspect sentiment features used in the model are then ranked to identify the most predictive aspects.
7.1 The correlation study
We take the difference of average positive sentiments and average negative sentiments for all sentences in the review as the final sentiment scores at aspect level for each of the aspect. We then compute the Pearson correlation coefficients of the aspect level sentiment values and the final recommendation. The result is shown in the Table 9. We can see that the aspects clarity, empirical/ theoretical soundness and impact of ideas have higher correlation values than the other features.
7.2 Deep neural model
For every individual review for a paper, we attempt to predict the intended decision as to whether that review indicates

acceptance or rejection of the paper directly from the review text. Since the data is already small, we integrate the reviews of 2017, 2018 and 2019 together for the prediction task. This makes a total of 8151 reviews. Details of the review text: Each review in the dataset consists of 20.57 sentences on average and each sentence contains 18.37 words on average. The recommendation scores range from 1 – 10, 10 being the best. The distribution of the recommendation scores is shown in Table 10.

Scores 1 2 3 4

5

6

7

8 9 10

Count 16 146 651 1528 1645 1874 1619 513 146 13

Table 10: Distribution of recommendation scores in the ICLR

dataset.

Prediction classes: The review scores 1 – 10 are translated to accept/reject indicators for the 2-class classification task. For a particular review, we translate the scores 1 – 5 to a reject decision and the scores 6 – 10 as accept decision. Thus all the 8151 reviews get tagged with one of the accept/ reject labels in this way. The distribution of the number of reviews in each class is noted in Table 11.

Conference #Accepted (> 5) #Rejected (≤ 5)

ICLR 2017

776

573

ICLR 2018

1242

1234

ICLR 2019

2147

2179

Table 11: Distribution of the reviews for the 2-class classifica-

tion task.

Model description: We devise our model to show the usefulness of the aspect sentiments in predicting the recommendation intended in a review and to compute the effect of dropout of individual features in the model to understand the hierarchy of importance of the aspects. For this we build aspect sentiment features as follows. Aspect sentiment features: Recall that we have eight aspects. For each aspect we have three sentiment categories – ‘positive’, ‘negative’ and ‘neutral’. Therefore we construct a 8 × 3 = 24 dimensional vector. For each of the eight aspects we keep three cells. We aggregate the sentiments in each cell using the same aggregation method discussed earlier (see section 5)

Aspects

% difference in accuracy

Accuracy (all aspects)

0.0

Appropriateness

1.11

Clarity

1.71

Originality

0.88

Empirical/theoretical soundness

3.84

Meaningful Comparison

1.22

Substance

0.32

Impact of Ideas

3.15

Recommendation

0.11

Table 12: Feature importance study: % drop in classifier per-

formance while dropping each aspect sentiment feature.

except that we perform 𝑍-score normalization on each of the 24 features for faster training. Prediction: We pass the 24 dimensional aspect sentiment vector through one dense layer with ReLU activation followed by another dense layer and sigmoid activation function to obtain the final classification result. Hyperparameters: We use ADAM for optimization and cross entropy as the loss function with default TensorFlow hyperparameters. As an exercise, we try using different number of nodes in the dense layer, different dropout rates and number of hidden layers and choose the architecture performing the best based on 10-fold cross validation accuracy. We used 64 nodes with a dropout rate of 0.8 in all dense layers. Results: We obtain 65.3% accuracy after 10-fold cross validation in this task making it apparent that the aspect sentiment features hold useful information about the final recommendation. We further investigate the importance of the sentiment features in our task. We drop one aspect (3 sentiment features, positive, negative and neutral) each time and observe the % drop in the accuracy in Table 12. While the method used in the last section (i.e., section 7.1) studies linear correlation, this one is a non-linear method. We note that the sentiments binding to the aspects empirical/theoretical soundness, impact of ideas and clarity are the most effective ones in predicting the intended recommendation in a review. This again confirms the results of the previous section. We believe that this result is particularly important because it acts as a guideline for the authors as to what particular aspects they should focus on while submitting their papers.
8 CONCLUSION
In this paper we presented a detailed analysis of the latent aspect sentiments in the peer review text of the papers submitted to/published in one of the top-tier machine learning conferences – ICLR. In particular we made the following contributions.
∙ We collected the whole ICLR review corpus for the years 2017-19 with all metadata like author names, rebuttal comments, accept/reject/withdrawn information, etc.
∙ We annotated around 2,500 review sentences with the aspects present in them and the associated sentiments

binding to those aspects. We used an active learning framework to annotate the data in steps. We do not know of any such data that already exists and stress that this is an important contribution of our work. We plan to release this dataset upon acceptance. ∙ We built aspect sentiment detection models that are trained using the above data and obtain a good performance. This allows us to confidently label the rest of the data with appropriate aspects and sentiments. ∙ We perform an exploratory analysis on this full dataset and investigate how the aspect sentiment distributions vary between accepted and rejected papers. We also establish that the reviewers whose review scores/ recommendations are contradicted by the chair’s decision, generally have low correlation between the raw scores and the aspect sentiments coming from text of their review. Thus, the text contains important cues in these cases to make the final decision which cannot be made from their review scores. ∙ We define inter-reviewer disagreement and perform multiple analysis to establish the connection between aspect level disagreements with aggregate level disagreement empirically. ∙ We observe very clearly through multiple experiments that certain aspects are more relevant in the prediction of the final recommendation. These aspects can act as guidelines for authors telling them what to focus more on in order to get their papers accepted.
In future we would like to extend this work in multiple directions. One immediate task would be to integrate aspect sentiments in the modern future citation count prediction algorithms and observe if we get additional benefits. Another direction would be to study author specific aspect sentiments, i.e., for a given author can we characterise him/her based on the aspect sentiments present in the reviews of the papers authored by him/her? For instance, are certain aspect sentiments more pronounced for successful/high impact authors? A third line of investigation would be the temporal change in aspect sentiments in the reviews received by an author. Does this change provide early indication of the growth/decline of an author in productivity/citation impact?
REFERENCES
[1] Mohammad Al-Smadi, Omar Qawasmeh, Mahmoud Al-Ayyoub, Yaser Jararweh, and Brij Gupta. 2018. Deep Recurrent neural network vs. support vector machine for aspect-based sentiment analysis of Arabic hotels'reviews. Journal of computational science 27 (2018), 386–393.
[2] Stefanos Angelidis and Mirella Lapata. 2018. Multiple instance learning networks for fine-grained sentiment analysis. Transactions of the Association for Computational Linguistics 6 (2018), 17–31.
[3] Seyed-Ali Bahrainian and Andreas Dengel. 2013. Sentiment analysis and summarization of twitter data. In 2013 IEEE 16th International Conference on Computational Science and Engineering. IEEE, 227–234.
[4] Iz Beltagy, Kyle Lo, and Arman Cohan. 2019. SciBERT: A pretrained language model for scientific text. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 3606–3611.

[5] Daniel Cer, Yinfei Yang, Sheng-yi Kong, Nan Hua, Nicole Limtiaco, Rhomni St John, Noah Constant, Mario Guajardo-Cespedes, Steve Yuan, Chris Tar, et al. 2018. Universal sentence encoder for English. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. 169–174.
[6] Daniel R Dooly, Qi Zhang, Sally A Goldman, and Robert A Amar. 2002. Multiple-instance learning of real-valued data. Journal of Machine Learning Research 3, Dec (2002), 651–678.
[7] Yang Gao, Steffen Eger, Ilia Kuznetsov, Iryna Gurevych, and Yusuke Miyao. 2019. Does My Rebuttal Matter? Insights from a Major NLP Conference. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). 1274–1290.
[8] Markus Helmer, Manuel Schottdorf, Andreas Neef, and Demian Battaglia. 2017. Gender bias in scholarly peer review. eLife 6 (2017), e21718.
[9] Jeremy Howard and Sebastian Ruder. 2018. Universal Language Model Fine-tuning for Text Classification. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 328–339.
[10] Binxuan Huang, Yanglan Ou, and Kathleen M Carley. 2018. Aspect level sentiment classification with attention-over-attention neural networks. In International Conference on Social Computing, Behavioral-Cultural Modeling and Prediction and Behavior Representation in Modeling and Simulation. Springer, 197–206.
[11] Dongyeop Kang, Waleed Ammar, Bhavana Dalvi, Madeleine van Zuylen, Sebastian Kohlmeier, Eduard Hovy, and Roy Schwartz. 2018. A Dataset of Peer Reviews (PeerRead): Collection, Insights and NLP Applications. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers). 1647–1661.
[12] Chetan Kaushik and Atul Mishra. 2014. A Scalable, Lexicon Based Technique for Sentiment Analysis. International Journal in Foundations of Computer Science & Technology 4, 5 (Sep
2014), 35âĂŞ56. https://doi.org/10.5121/ijfcst.2014.4504 [13] Michail Kovanis, Raphaël Porcher, Philippe Ravaud, and Ludovic
Trinquart. 2016. The global burden of journal peer review in the biomedical literature: Strong imbalance in the collective enterprise. PloS one 11, 11 (2016). [14] Langford and Guzdial. 2015. The Arbitrariness of Reviews, and Advice for School Administrators. Commun. ACM 58 (03 2015), 12–13. https://doi.org/10.1145/2732417 [15] Zeyang Lei, Yujiu Yang, Min Yang, and Yi Liu. 2018. A Multisentiment-resource Enhanced Attention Network for Sentiment Classification. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers). 758–763. [16] Prem Melville, Wojciech Gryc, and Richard D. Lawrence. 2009. Sentiment Analysis of Blogs by Combining Lexical Knowledge with Text Classification. In Proceedings of the 15th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (Paris, France) (KDD ’09). ACM, New York, NY, USA, 1275–1284. https://doi.org/10.1145/1557019.1557156 [17] Bo Pang, Lillian Lee, et al. 2008. Opinion mining and sentiment analysis. Foundations and Trends® in Information Retrieval 2, 1–2 (2008), 1–135. [18] Azzurra Ragone, Katsiaryna Mirylenka, Fabio Casati, and Maurizio Marchese. 2013. On peer review in computer science: analysis of its effectiveness and suggestions for improvement. Scientometrics 97, 2 (01 Nov 2013), 317–356. https://doi.org/10.1007/ s11192- 013- 1002- z [19] Sandipan Sikdar, Matteo Marsili, Niloy Ganguly, and Animesh Mukherjee. 2016. Anomalies in the peer-review system: A case study of the journal of High Energy Physics. In Proceedings of the 25th ACM International on Conference on Information and Knowledge Management. 2245–2250. [20] Sandipan Sikdar, Matteo Marsili, Niloy Ganguly, and Animesh Mukherjee. 2017. Influence of reviewer interaction network on long-term citations: a case study of the scientific peer-review system of the journal of high energy physics. In 2017 ACM/IEEE Joint Conference on Digital Libraries (JCDL). IEEE, 1–10. [21] Youwei Song, Jiahai Wang, Tao Jiang, Zhiyue Liu, and Yanghui Rao. 2019. Targeted Sentiment Classification with Attentional Encoder Network. Lecture Notes in Computer Science (2019),

93âĂŞ103. https://doi.org/10.1007/978-3-030-30490-4_9 [22] Tun Thura Thet, Jin-Cheon Na, and Christopher S. G. Khoo. 2010.
Aspect-based sentiment analysis of movie reviews on discussion boards. J. Information Science 36 (2010), 823–848. [23] Andrew Tomkins, Min Zhang, and William D. Heavlin. 2017. Reviewer bias in single- versus double-blind peer review. PNAS 114 (2017), 12708–12713. Issue 48. [24] Peter D Turney. 2002. Thumbs up or thumbs down?: semantic orientation applied to unsupervised classification of reviews. In Proceedings of the 40th annual meeting on association for computational linguistics. Association for Computational Linguistics, 417–424. [25] Ke Wang and Xiaojun Wan. 2018. Sentiment analysis of peer review texts for scholarly papers. In The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval. 175–184. [26] Min Yang, Wei Zhao, Jianbo Ye, Zeyang Lei, Zhou Zhao, and Soufei Zhang. 2018. Investigating Capsule Networks with Dynamic Routing for Text Classification. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. 3110–3119. [27] Yasuhisa Yoshida, Tsutomu Hirao, Tomoharu Iwata, Masaaki Nagata, and Yuji Matsumoto. 2011. Transfer learning for
multiple-domain sentiment analysisâĂŤidentifying domain dependent/independent word polarity. In Proceedings of the TwentyFifth AAAI Conference on Artificial Intelligence. AAAI Press, 1286–1291.

