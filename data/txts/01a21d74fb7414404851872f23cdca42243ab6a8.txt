1
Progressive Transfer Learning
Zhengxu Yu, Dong Shen, Zhongming Jin, Jianqiang Huang, Deng Cai, Member, IEEE and Xian-Sheng Hua, Fellow, IEEE

arXiv:1908.02492v3 [cs.CV] 6 Nov 2020

Abstract—Model ﬁne-tuning is a widely used transfer learning approach in person Re-identiﬁcation (ReID) applications, which ﬁne-tuning a pre-trained feature extraction model into the target scenario instead of training a model from scratch. It is challenging due to the signiﬁcant variations inside the target scenario, e.g., different camera viewpoint, illumination changes, and occlusion. These variations result in a gap between the distribution of each mini-batch and the whole dataset’s distribution when using mini-batch training. In this paper, we study model ﬁne-tuning from the perspective of the aggregation and utilization of the global information of the dataset when using mini-batch training. Speciﬁcally, we introduce a novel network structure called Batchrelated Convolutional Cell (BConv-Cell), which progressively collects the global information of the dataset into a latent state and uses it to rectify the extracted feature. Based on BConv-Cells, we further proposed the Progressive Transfer Learning (PTL) method to facilitate the model ﬁne-tuning process by jointly optimizing the BConv-Cells and the pre-trained ReID model. Empirical experiments show that our proposal can improve the performance of the ReID model greatly on MSMT17, Market1501, CUHK03 and DukeMTMC-reID datasets. Moreover, we extend our proposal to the general image classiﬁcation task. The experiments in several image classiﬁcation benchmark datasets demonstrate that our proposal can signiﬁcantly improve the performance of baseline models. The code has been released at https://github.com/ZJULearning/PTL
Index Terms—Person Re-identiﬁcation, Transfer Learning, Image Classiﬁcation
I. INTRODUCTION
Person re-identiﬁcation (ReID) is to re-identify the same person in different images captured by different cameras or at different time. Due to its wide applications in surveillance and security, person ReID has attracted much interest from both academia and industry in recent years.
With the development of deep learning methods and the newly emerged person ReID datasets, the performance of person ReID has been signiﬁcantly boosted recently. However, several open problems remain. First, training a feature extraction model from scratch needs a large volume of annotation data. However, the annotated data is hard to acquire in person ReID tasks due to the poor quality of the image and pedestrians’ privacy concerns. Hence, using existing datasets to help optimize the feature extractor has attracted great attention in the community. Second, the signiﬁcant variations between different scenarios and within the same scenario make the person ReID task challenging. A noticeable performance
Z. Yu, D. Shen and D. Cai are with the State Key Laboratory of CAD&CG, College of Computer Science, Zhejiang University, Hangzhou, Zhejiang 310058, China (emails: yuzxfred@gmail.com; dongshen@zju.edu.cn; dengcai@gmail.com).
Z. Jin, J. Huang and X.-S. Hua are with Alibaba Group, Hangzhou, Zhejiang, China (emails: zhongming.jinzm@alibaba-inc.com; jianqiang.hjq@alibaba-inc.com; xiansheng.hxs@alibaba-inc.com).

degradation often occurs if we directly apply a pre-trained model on the target dataset without ﬁne-tuning it into the target scenario.
Most of the recently proposed works [1], [2] have focused on mitigating the impact of variations between different datasets. Most of these works focus on transferring the image style of the target domain and the source domain to the same by using Generative Adversarial Networks (GANs) based models. However, the imperfect style transferring models can bring in noises and potentially change the data distribution of the whole dataset. Meanwhile, the person ID in the generated images is not guaranteed to be the same as in the real images.
As for variations inside the dataset, which we focused on in this work, it is less mentioned in recently proposed works. The distribution difference between each mini-batch and the entire dataset caused by internal variations signiﬁcantly inﬂuences the model ﬁne-tuning process. This difference leads to a deviation of gradient estimation and thus affect the effect of model ﬁne-tuning. The most straightforward approach to mitigate this problem is increasing the batch size. However, Keskar et al. [3] and our experiments revealed that using a large-batch setting tends to converge to sharp minimizers, and further leads to poorer performance.
Moreover, most state-of-the-art deep learning methods in person ReID task have used an off-the-shelf network, like DenseNet [4] and ResNet [5], as backbone network. However, Deep CNNs are difﬁcult to initialize and optimize with limited training data. Therefore, model ﬁne-tuning is widely used to mitigate shortages of annotated training data in person ReID tasks, making the study of how to mitigate the impact of the internal variation more critically. For instance, most of the off-the-shelf models used in ReID tasks are pre-trained on a relatively larger dataset like ImageNet [6] and then ﬁne-tuning into the target dataset.
We study how to mitigate the impact of internal variations from the viewpoint of aggregation and utilization of the global information of the dataset. First, we propose a novel CNN building block, which we call the Batch-related Convolutional Cell (BConv-Cell). The BConv-Cell progressively aggregates the global information of the dataset into a latent state in a batch-wise manner. The latent state aggregated in previous batches will be used to mitigate the impact of the subsequent batches’ internal variations. Based on the BConv-Cells, we further propose the Progressive Transfer Learning (PTL) method to ﬁne-tune the pre-trained model by integrating it with the BConv-Cells. We conduct extensive experiments on MSMT17 [7], Market-1501 [8], CUHK03 [9] and DukeMTMC-reID [10] datasets to show that our proposal can effectively promote the ReID performance.
Moreover, we propose a variation of the BConv-Cell

2

(BConv-Cell-v2) in this work and extend the application scenario of our proposal from the person ReID task to general image classiﬁcation tasks. Differ from the BConv-Cell, the BConv-Cell-v2 not only maintain the latent states of the BConv-Cells of previous mini-batches but also the output hidden states. We further use the BConv-Cell-v2 to replace the BConv-Cells we used in the PTL network and named it PTL-v2. The experimental results in several image classiﬁcation benchmark datasets demonstrate that PTL-v2 outperforms baselines in all experiments, and outperforms the PTL in the dataset with fewer categories.
We summarize the contributions of this work as follows:
1) We propose a novel network structure called the Batch-related Convolutional Cell (BConv-Cell) and its variation (BConv-Cell-v2). In mini-batch training, the BConv-Cells can progressively aggregate the global information of the dataset and then use it to optimize the model in the next batches.
2) Based on the BConv-Cells, we propose the Progressive Transfer Learning (PTL) method to ﬁne-tune a pretrained model into the target scenario by integrating the BConv-Cells.
3) The experimental results show that the model ﬁnetuned using our proposal can achieve state-of-the-art performance on four persuasive person ReID datasets.
4) We extend the application scenario of our proposal from the person ReID task to general image classiﬁcation tasks in this work. Experimental results in several image classiﬁcation benchmark datasets demonstrate that our proposal can signiﬁcantly improve the performance of the backbone models.
II. THE BATCH-RELATED CONVOLUTIONAL CELL AND ITS
VARIATION
The Batch-related Convolutional Cell (BConv-Cell) is based on a straightforward thought that making use of the global information of the dataset to mitigate the adverse inﬂuence caused by internal variation. In this section, we introduce the formula and architecture of the BConv-Cell and its variation BConv-Cell-v2.
A. The Batch-related Convolutional Cell
The BConv-Cell is inspired by the Conv-LSTMs [11], as shown in the left of Figure 1. However, there are several fundamental differences between the BConv-Cells and the ConvLSTMs. First, there is no time concept and explicit temporal connections between inputs in the BConv-Cells. Meanwhile, the BConv-Cells is not designed to handle sequential inputs but single segmented images. Second, the BConv-Cells have a different architecture from the Conv-LSTMs. The BConv-Cells only maintained a latent state that contained the aggerated global information, but the Conv-LSTMs reserved both the hidden state and the cell state. Moreover, the BConv-Cells is not designed to conduct prediction. Using the memory mechanism, the BConv-Cells can progressively collect global information and use it to facilitate the parameter optimization process during ﬁne-tuning. Unlike other LSTM based methods

like meta-learners, the output of the BConv-Cells can be directly used as the extracted feature. Meanwhile, the nature of the BConv-Cells is a stack of Conv-layers so that it can be used as a building block of a multi-layer feature extraction network. The key equations of the BConv-Cell have shown as follow:

ib = σ(Wxi ∗ xb + bi)

fb = σ(Wxf ∗ xb + bf )

ob = σ(Wxo ∗ xb + bo)

(1)

Cb = fb ◦ Cb−1 + ib ◦ tanh(Wxc ∗ xb + bc)

yb = ob ◦ tanh(Cb),

where ∗ denotes the convolution operator, ◦ denotes the Hadamard product, σ denotes a sigmoid function, xb is the input of the BConv-Cell in b-th batch. ib, fb and ob is the output of input gate i, forget gate f and output gate o respectively, Cb is the latent state reserved after b-th batch, W is the weight of the corresponding convolutional layer in the BConv-Cell and yb is the output of the BConv-Cell. All the input xb, latent state Cb and gate output ib, fb, ob are 3dimensional tensors.
As shown in Eq. 1, the output yb is determined by the latent state Cb and the input xb. The latent state Cb is determined by the input xb and Cb−1. From the fourth formula of Eq. 1, we can notice that the Cb maintains part of the information of all the historical input batches. The iteration formula of latent state Cb as:

Cb = g(x1, x2, ..., xb),

(2)

where g is the simpliﬁed notation of the composition of functions {gi|1 ≤ i ≤ b}.

B. The Batch-related Convolutional Cell-v2
In this work, we also propose a variation of the BConv-Cell and name it BConv-Cell-v2. The comparison of the BConvCell and BConv-Cell-v2 have shown in Figure 1. As we can notice, BConv-Cell-v2 maintains the latent states of previous mini-batches and the output hidden states at the same time. In each mini-batch, the output hidden states are aggregated with the hidden states of previous mini-batches. Meanwhile, the features of historical inputs are partially recovered by using a transpose convolution layer before concatenated with the input features of the current mini-batch. We argue that the concatenated features of historical inputs can mitigate the impact of the data generation bias of mini-batch, and thus improve the stability of the model. More explanation can be found in Section III-E.

III. PROGRESSIVE TRANSFER LEARNING NETWORK
A. Progressive Transfer Learning
Given an off-the-shelf CNN as the backbone network, we pair up the BConv-Cells with its Conv-blocks to form a new network, and we name it as the progressive transfer learning (PTL) network. A sketch of the PTL network has shown in Figure 2. The red dotted box denotes a building block of the

3

,$ ./

⋅

,$

-$

+

⋅

⋅

𝑐" ,-

⋅

𝑐"

𝑦"

+

⋅

⋅

%$&

%$(

%$)

%$'

𝑧$
"

𝑧"&

𝑧"'

𝑧"%

Slice.
3×3 Conv-layer
#$

𝑦" ,-

Slice.
3×3 Conv-layer Transpose Cat. Conv-layer
𝑥"

Fig. 1. The network architecture comparison of two kind of BConv-Cells we used in PTL and PTL-v2. Left: BConv-Cell, which we used in PTL. Right: BConv-Cell-v2 used in PTL-v2, which we proposed in this work. The circle and ellipse in this ﬁgure denote operators, e.g., ’Cat.’ denotes tensor concatenation operator. The hollow rectangle and square denote tensors, and the gray rectangles denote CNN layers.

PTL network, which formed by a BConv-Cell, a 1x1 Convlayer and the Conv-block of the backbone network. Formally, we deﬁne this building block as:

xib = Fconv(xib−1)

ybi = Fbconv(F1×1(xib, ybi−1), Cbi−1)

(3)

Cbi = g(xi1, xi2, ..., xib),

where x0b indicate the input image of b-th batch, xib is the output of the i-th (i >= 1) Conv-block in b-th batch, ybi is the output of the i-th BConv-Cell. Eq. 3 only contains the second and the third equation when i = 0. The function Fconv and Fbconv represent the mapping function learned by Conv-block and BConv-Cell respectively. F1×1 is the 1x1 Conv-layer as shown in Figure 2. Cbi is the latent state of the i-th BConvCell after the b-th batch. The structure of the Conv-block is
ﬂexible, which can be replaced by Conv-block of many Deep
CNNs like DenseNet or ResNet.
As shown in Eq. 3, the BConv-Cell learn the mapping
function from input to feature space while collecting global
information and updating the latent state. We can notice from
Eq. 3 that the discriminative knowledge of the past batches is
progressively aggregated into the latent state.

B. Progressive Transfer Learning Network Architecture
We have tested the PTL method with several different structures of backbone networks, including DenseNets and ResNets. We use the DenseNet-161 as the backbone network to describe the construction of the PTL network.
The DenseNet-161 consists of ﬁve Conv-blocks. We use four BConv-Cells to pair up with the top four Conv-blocks, as

shown in Figure 2. At the top of the network, we use a BConvCell to capture the low-level feature of the input image, as shown in the left of Figure 2. At the bottom of the network, the output of the last BConv-Cell is concatenated with the output of the last Conv-block and then feed into a 1x1 Convlayer to get the feature vector. During training, the feature vector is then fed into a classiﬁer which contains three Fully connection layers. For simplicity, the classiﬁer is not shown in Figure 2. During evaluating, we directly use the feature vector conduct image retrieve.
As we can see in Figure 2, feature maps transmit along two dimensions in the PTL network. The ﬁrst is batch iteration, BConv-Cells evolve the latent states with each input batch and transmit it to the next batch. The second is the depth of the network, in which feature maps transmit from the ﬁrst layer to the last layer.
During testing, we set all the latent states as zeros. We set all the latent states to zeros at the beginning of each epoch during training to simulate the test condition. This setting ensures that historical knowledge is progressively collected and aggregated only once in each epoch.
As mentioned above, the backbone in Figure 2 can be replaced by most of the commonly used feature extraction networks. In this work, we use ResNet-50, DenseNet-161 and MGN [12] as backbone network.
C. Progressive Transfer Learning Network-v2
In this work, we also propose the PTL-v2 network, a variation of PTL network in which the BConv-Cells are replaced with BConv-Cell-v2 blocks, while all other components

4

Input 𝒙𝒃𝟎

𝐶&)'( Block pair

BConv- 𝑦&) 1x1

Cell

Conv

𝐶&('(

BConvCell

𝑦&( 1x1 Conv

Convblock 𝑥&(
Backbone

Convblock

𝐶&*'(

BConvCell

𝑦&* 1x1 Conv

Conv𝑥&* block

𝐶&+'(

BConvCell

𝑦&+ 1x1 Conv

Conv𝑥&+ block

𝐶&,'(

BConv- 𝑦&, Cell

Conv-

𝑥&,

block 𝑥&/

Feature vector
1x1 Conv

Fig. 2. Sketch of the PTL network. The black dotted box indicates the backbone network. xib and ybi are the outputs of the i-th Conv-block of the backbone and the related BConv-Cell respectively, x0b denotes the input image, b indicates the b-th input batch, Cbi−1 is the latent state of the i-th BConv-Cell after the last batch. The red dotted box denotes the block pair of the Conv-block and the BConv-Cell. In each block pair, xib and ybi−1 are concatenated and feeding into a 1x1 Conv layer before feed into the BConv-Cell. The output of the last BConv-Cell and that of backbone network are concatenated before feeding into the 1x1 Conv-layer. The latent state of the BConv-Cell is stored after every batch and feedback to the same BConv-Cell when next batch coming.

remain unchanged. The difference between BConv-Cell and BConv-Cell-v2 has shown in Figure 1.
In each BConv-Cell-v2 block of PTL-v2, we ﬁrst recover the features of historical inputs of previous mini-batches before concatenating it with the current mini-batch input features. We use the aggregated information of previous mini-batches to disturb the newly input features, thus mitigate the impact of the data generation bias. The experimental results show that the PTL-v2 can outperform all baselines in several image classiﬁcation benchmarks while outperforming the PTL network in image classiﬁcation benchmarks with fewer categories. [13]
D. Parameter Optimization Procedure
Our proposal facilitates parameter optimization using the BConv-Cells to cooperate with the backbone network, which has no limit on the model optimization method. Hence, the combined model still can be optimized by using commonly used optimizers like SGD and SGD-M.
We argue that the PTL method can make up for two shortcomings of the SGD-M optimizer. First, in SGD-M, the historical gradient is aggregated in a linear sum roughly by using humanly pre-deﬁned weights, which make it inﬂexible and not optimized. Second, the loss after each batch only determined by the current input batch, which has a strong bias and leading to performance oscillation during training.
Using the PTL method, the historical gradient aggregation is replaced by calculating the gradient of a composition function recursively with learnable weights. More than that, the sample bias of the current batch can be mitigated by using the historical knowledge carried by the learned latent states Cb.
E. Another Causality based Explanation
We also offer a causality-based explanation to help understand why our approach works. Many deep-learning based methods typically follow a consistent assumption that the training and testing datasets are generated from the same data distribution (i.e., i.i.d hypothesis). Under i.i.d hypothesis, a trained model can be directly applied to the testing dataset, and its performance should be equivalent to the training dataset. Although this approach is empirically proven to be highly successful in many public datasets, it is considered ﬂawed in real-world applications.

In real-world applications, we rarely know the true underlying model, and we cannot guarantee the unknown test data will have the same distribution as the training data. Moreover, in real-world applications, signiﬁcant data generation bias can result in substantial variations between different training and testing datasets. Consequently, models trained in such a training dataset will fall into the statistical correlation of features and result in unstable performance to yielding lower training loss. To mitigate this problem, our proposal progressively aggregates the global information of the training dataset and then applying it to mitigate the impacts of data generation bias during training. As shown in experiments, our proposal can signiﬁcantly improve the performance of backbone models in both person ReID and general image classiﬁcation tasks.
To verify our idea, we further visualized the loss surfaces of DenseNet-100 with/without PTL-v2 on the CIFAR-100 dataset using the visualization tools proposed by Li et al. [14]. As shown in Figure 3, we can notice that the loss surface of DenseNet-100+PTL-v2 is wider than DenseNet-100. As Li et al. [14] illustrated and proved in their work, the large the loss surface, the better the model’s generalization ability. Therefore, we can draw a plain conclusion that PTL helps improve the model’s performance in the ﬁne-tuning process using the global information of the dataset.

IV. STUDENT-TEACHER DISTILLATION METHOD
Compared with the backbone network, the parameter number of the PTL network grew up inevitably. To fairly compare with baselines, we introduce an improved model distillation method called Student-Teacher Distillation (STD) method to ﬁne-tune a backbone model (like ResNet-50) by using the ﬁnetuned PTL model. The STD method is not essential for the PTL method in practical applications.
As the prerequisite, we assume we have obtained a ﬁnetuned model by using our proposed PTL method. We then introduce a new objective function for model distillation. The objective function consists of two parts. First is a cross-entropy loss between the output predictions of the student model and the ground truth. The second is an L1 loss between the student model’s output feature vector and that of the teacher model. The new objective function is given by:

Ldistill = (1 − λ)LCE + λLl1,

(4)

5

Fig. 3. The loss surfaces of DenseNet-100 with/without PTL-v2 on CIFAR-100 dataset generated by using visualization tools proposed by Li et al [14]. Left: DenseNet-100+PTL-v2, Right: DenseNet-100. The large the loss surface, the better the model’s generalization ability.

Input 𝑥"

As Target

Teacher model
Student model

L1 Loss
𝜆
+ Final loss
1−𝜆
Cross Entropy Loss

Label
Fig. 4. The implementation of STD method. The teacher model is set to evaluation mode during the whole process.

where λ is a hyper-parameter to adjust the ratio of the cross-entropy loss and the L1 loss. This new object function combines both supervision information and merit of the PTL network to extract discriminative features.
The implementation of the STD method has shown in Figure 4. We set the teacher model to evaluation mode during the whole process. The input image feeds into teacher and student model at the same time. After this, the parameter of the student network will be updated according to the proposed objective function in Eq. 4. After training, the teacher model can be abandoned.
V. TOWARDS GENERAL IMAGE CLASSIFICATION
We further applied our proposal to general image classiﬁcation tasks. The key idea of PTLs of progressively aggregating the global information of the dataset is adaptable in general image classiﬁcation tasks.
In a typical person ReID task, we ﬁrst obtained the embeddings of all gallery and query images using the feature extraction model. After that, metric learning methods are used to learn a distance metric. Then, the nearest neighbor of the query image is retrieved from the gallery set, and its label is output as prediction label for the query image. In a general

image classiﬁcation task, we followed the standard image classiﬁcation procedure. The output of the feature extraction model (PTL or PTL-v2) is fed into a classiﬁer that consists of two linear layers. The output of the classiﬁer is then fed into a Softmax function to generate a predicted class label.
VI. EXPERIMENTS
We ﬁrst carried out model ﬁne-tuning experiments with our proposal on four convincing ReID datasets and compared it with both the state-of-the-art ReID methods and several transfer-learning methods. We then conduct model transferring experiments among multiple datasets to evaluate the PTL method’s performance when handling multiple-step transferring.
A. Dataset
We selected four persuasive ReID datasets to evaluate our proposal, including Market-1501, DukeMTMC-reID, MSMT17, and CUHK03. We further applied our proposal to general image classiﬁcation tasks, and tested it in several image classiﬁcation datasets: CIFAR-10, CIFAR-100.
a) Market-1501.: The Market-1501 dataset contains 32,668 annotated bounding boxes of 1,501 identities.
b) DukeMTMC-reID.: The DukeMTMC-reID dataset contains 1,404 identities. 702 IDs are selected as the training set and the remaining 702 IDs as the testing set.
c) MSMT17.: The raw video on the MSMT17 dataset is recorded in 4 days with different weather conditions in a month using 12 outdoor cameras and three indoor cameras. The MSMT17 dataset contains 126,441 bounding boxes of 4,101 identities. We followed the same dataset split by Wei et al. [7], and we also used the evaluation code provided by them (https://github.com/JoinWei-PKU/MSMT17 Evaluation).
d) CUHK03.: The CUHK03 dataset consists of 14,097 images of 1,467 persons from 6 cameras. Two types of annotations are provided in this dataset: manually labeled pedestrian bounding boxes and DPM-detected bounding boxes. We followed the same dataset split as used in the [12]. For all experiments on Market-1501, DukeMTMC-reID and

6

TABLE I RESULTS ON THE MSMT17 DATASET.

TABLE II RESULTS ON THE MARKET-1501 DATASET.

Method

#Param.1 mAP CMC-1

GoogLeNet [7] PDC [7] GLAD [7]

-

23.00 47.60

-

29.70 58.00

-

34.00 61.40

ResNet-50

28m

28.63 59.77

ResNet-50+PTL

35m

32.58 62.76

DenseNet-161

32m

38.60 70.80

DenseNet-161+PTL

42m

42.25 72.65

DenseNet-161+PTL+STD

32m

41.38 73.12

1. #Param. indicates parameter number, while m indicates million.

CUHK03, we used the evaluation code provided in Open-ReID (https://github.com/Cysu/open-reid).
e) Market-Duke.: We use the training sets of the two datasets Market-1501 and DukeMTMC-reID to form a new dataset called the Market-Duke dataset. We further use this dataset to train the models to compare the difference between the one-step model ﬁne-tuning and multi-step model ﬁnetuning. All validation, query and gallery set of these two datasets are abandoned.
f) CIFAR-10.: The CIFAR-10 dataset consists of 60000 32x32 color images in 10 classes, with 6000 images per category. There are 50000 training images and 10000 test images.
g) CIFAR-100.: This dataset is just like the CIFAR10, except it has 100 classes containing 600 images each. There are 500 training images and 100 testing images per category. The 100 classes in the CIFAR-100 are grouped into 20 superclasses.

Method

mAP CMC-1

DML [15] HA-CNN [16] PCB+RPP [17] MGN [12]
DenseNet-161*1 DenseNet-161 DenseNet-161+PTL DenseNet-161+PTL+STD MGN (reproduced)2 MGN+PTL

70.51 75.70 81.60 86.90
69.90 76.40 77.50 77.50 85.80 87.34

89.34 91.20 93.80 95.70
88.30 91.70 92.50 92.20 94.60 94.83

1. DenseNet-161* used a batch size of 90, while DenseNet-161 used batch size 32. 2. MGN (reproduced) is our reproduction of the MGN [12].

and MGN+PTL, we followed the experiment setting in [12]. Meanwhile, we use the single-query setting in all experiments.
To the best of our knowledge, the ofﬁcial implementation of the MGN has not been released. Hence, we use the reproduction version published in https://github.com/GNAYUOHZ/ ReID-MGN. The results obtained by this reproduction code are noted as MGN (reproduced) in all the tables. Although the MGN model uses the ResNet-50 as the backbone network, the parameter number of the MGN model (66m) is much more than the ResNet-50 model (28m). Due to the GPU usage limitation, we have not conducted experiments about the MGN+PTL+STD.
We use the cross-entropy loss in all of the ﬁne-tuning processes in our experiments, except for the MGN+PTL. The MGN uses a combined loss function insists of cross-entropy loss and triplet loss, we use the same loss function in all the experiments of the MGN+PTL.

B. Experiment Setting
We select the DenseNet-161 model and ResNet-50 model both pre-trained on the ImageNet dataset as the backbone model. As for the state-of-the-art model in ReID tasks, we select the MGN [12] model, which also uses a ResNet-50 as the backbone network. We modiﬁed the backbone network by using our proposed PTL method, and name these models as DenseNet-161+PTL, ResNet-50+PTL and MGN+PTL, respectively. We then use the STD method to train the DenseNet161 model (DenseNet-161+PTL+STD) using the DenseNet161+PTL as the teacher model.
In person ReID experiments, images have been reshaped into 256x128 (height x width) before feeding into the network except for the experiments of MGN and MGN+PTL, which use image size 384x128. We take out the output of the 1x1 Conv-layer as the discriminative feature. The initial learning rate is set to 0.01 and decay the learning rate ten times every ten epochs. Models are ﬁne-tuned for 50 epochs. Unless otherwise stated, in all of our experiments, we use SGD-M as the optimizer. The hyper-parameter λ is set to 0.8 by practicing in the following experiments. For experiments involved MGN

C. MSMT17
We ﬁrst evaluate the PTL method on the MSMT17 dataset. The detailed evaluation statistics has shown in Table I. We can see a signiﬁcant performance promotion by using the PTL method. The performance of the DenseNet-161+PTL+STD outperforms not only the backbone model but also all of the baseline methods. We also can notice that the DenseNet161+PTL+STD model can achieve a higher CMC-1 score than the DenseNet-161+PTL model. We attribute the success to the combined loss function of the STD method. By combining the cross-entropy Loss with the L1 loss, the student model can learn the discriminative knowledge from the teacher model while imposing restrictions on the learned knowledge.
D. Market-1501
We use DenseNet-161 and MGN as backbone model to evaluate the performance of the PTL method on Market-1501 dataset. We select several state-of-the-art person ReID methods as baselines. Among these methods, the DML [15] is also pretrained on ImageNet and transferred to Market-1501.

7

TABLE III RESULTS ON THE CUHK03 DATASET.

TABLE V RESULTS OF TRANSFER AMONG MULTIPLE DATASETS.

Methods
HA-CNN [16] PCB [17] PCB+RPP [17] MGN [12] MGN (reproduced) MGN+PTL

Detected mAP CMC-1

38.60 54.20 57.50 66.00

41.70 61.30 63.70 66.80

69.41 74.22

71.64 76.14

Labelled mAP CMC-1

41.00 -
67.40

44.40 -
68.00

72.96 77.31

74.07 79.79

TABLE IV RESULTS ON THE DUKEMTMC-REID DATASET.

Method
HA-CNN [16] PCB [17] MGN [12]
MGN (reproduced) MGN+PTL

mAP
63.80 69.20 78.40
77.07 79.16

CMC-1
80.50 83.30 88.70
87.70 89.36

The results has summarized in Table II. We can notice that a simple DenseNet-161 model can outperform the stateof-the-art transfer learning based person ReID methods on the Market-1501 dataset using the PTL method and the STD method. Meanwhile, the MGN+PTL outperforms all the stateof-the-art methods.
Moreover, we can notice that using a large batch size (DenseNet-161*) is not an effective way to narrow the gap between the distribution of each mini-batch and the distribution of the dataset. In contrast, large batch size can result in poor performance.
E. CUHK03
We then conduct model ﬁne-tuning experiments on the CUHK03 dataset. We compare the performance of MGN+PTL with several state-of-the-art methods. The results have summarized in Table III. We can notice that by using the PTL method, the ReID performance of the MGN model has promoted tremendously, and outperforms all the state-of-the-art methods.
F. DukeMTMC-reID
We then conduct experiments on the DukeMTMC-reID dataset. As for baselines, we compare the performance with several state-of-the-art methods, including HA-CNN, PCB and MGN. The results have shown in Table IV, we can notice that by using our method, the MGN+PTL model can outperform all state-of-the-art methods.
G. Transfer among Multiple Datasets
In real-world applications, the ReID model needs to transfer among a sort of datasets to take advantage of all available

Method
DenseNet-161 DenseNet-161+PTL+STD
DenseNet-161 DenseNet-161+PTL+STD
DenseNet-161 DenseNet-161+PTL+STD
DenseNet-161+PTL DenseNet-161+PTL+STD
DenseNet-161+PTL DenseNet-161+PTL+STD

Fine-tuning list*
Market-Duke to MSMT17 Market-Duke to MSMT17
Market to Duke to MSMT17 Market to Duke to MSMT17
Duke to Market to MSMT17 Duke to Market to MSMT17
Duke to Market Duke to Market
Duke to MSMT17 to Market Duke to MSMT17 to Market

mAP
41.12 42.53
41.22 42.34
41.80 42.73
76.00 75.50
77.90 77.40

CMC-1
72.49 74.11
72.78 73.60
73.00 74.31
91.30 91.10
91.60 91.60

*Fine-tuning list indicates the ﬁne-tuning order. E.g., ’Duke to Market’ means the model is ﬁne-tuned on DukeMTMC-reID before ﬁne-tune it on Market-1501, while ’Market’ and ’Duke’ denote Market-1501 dataset and DukeMTMC-reID dataset respectively.

data. Therefore, we conduct multiple datasets transferring experiments to evaluate the performance of our proposal when dealing with model ﬁne-tuning among multiple datasets. Similar to the experiment on the MSMT17 dataset, we also use the STD method to train a DenseNet-161 model to compare with the baselines fairly.
The detailed results have shown in Table VI-G, from which we can see that the PTL method achieves better performance compared with baseline models. We also notice that the performance of two-step transferring achieves better performance compare with one step transferring. For instance, ﬁne-tuning follow the order ’Duke to Market to MSMT17’ outperforms ’Market-Duke to MSMT17’. We argue that the substantial style variation in the Market-Duke dataset is caused by the substantial style variation, which is richer than either Market1501 or the DukeMTMC-reID dataset.
More than that, we can see that the order of ﬁne-tuning can inﬂuence the performance of the ﬁnal model. The model ﬁnetuned by ’Duke to Market to MSMT17’ can achieve highest score in both mAP and CMC-1.
H. Evaluate STD Method on MSMT17
In this subsection, we evaluate the STD method on the MSMT17 dataset. We conduct a series of comparative experiments by adjusting the ratio of the cross-entropy loss and L1 loss.
We use the DenseNet-161+PTL model transferring from Market-1501 to MSMT17 in Table VI-G as the teacher model. The student model is a DenseNet-161 model which has been transferred from ImageNet to Market-1501 using a SGD-M optimizer.
The detailed results are shown in Table VI. From this table, we can see that by using the STD method, the performance of the DenseNet-161 model is promoted signiﬁcantly. Meanwhile, we can see that the score of the DenseNet161+PTL+STD grows up when λ grows up. However, when

8

TABLE VI RESULTS OF THE STD METHOD ON THE MSMT17 DATASET.

TABLE VII ERROR RATES ON CIFAR-10, CIFAR-100 AND CIFAR-100+ DATASETS.

Method

#Param. λ* mAP CMC-1

DenseNet-161+PTL

42m

- 42.45 72.48

DenseNet-161+PTL+STD 32m 0.00 38.60 70.80

DenseNet-161+PTL+STD 32m 0.30 41.26 72.52

DenseNet-161+PTL+STD 32m 0.50 42.27 73.49

DenseNet-161+PTL+STD 32m 0.80 42.51 73.37

DenseNet-161+PTL+STD 32m 1.00 41.66 72.32

*λ is the hyper-parameter in Eq. 4, which denotes the proportion of L1 loss in the combined loss function. λ = 0 means use a SGDM optimizer to ﬁne-tune a DenseNet-161 model on MSMT17 without using the STD method.

λ bigger than 0.8, the score no longer increases anymore. We argue that it is because the cross-entropy loss in the combined loss function is essential.
I. General Image Classiﬁcation Benchmarks
In this work, we extend the application scenario of our proposal from the person Re-ID task to the general image classiﬁcation tasks. Thus, we conduct experiments on three image classiﬁcation benchmarks, including CIFAR-10, CIFAR-100, and CIFAR-100+ datasets. We compare the performance of the proposed PTL and PTL-v2 with the baseline model to evaluate our proposal’s performance in general image classiﬁcation tasks. The ”+” indicates standard data augmentation (translation and/or mirroring). The detailed results are shown in Table VII. From this table, we can notice that the PTL and PTL-v2 models’ performance outperform the baseline model. Moreover, we can notice that the PTL-v2 model can outperform the PTL model in the CIFAR-10 dataset, which has fewer categories than the CIFAR-100 dataset.
VII. RELATED WORKS
A. Transfer Learning Methods
Many transfer learning methods have been proposed recently. Zhong et al. [18] proposed a domain adaption approach that transfers images from one camera to the style of another camera. Fan et al. [19] proposed an unsupervised ﬁne-tuning approach that used an IDE model trained on DukeMTMC-reID as the start point and ﬁne-tuned it on target dataset. Unlike these approaches, our method is based on model ﬁne-tuning, which is more ﬂexible and easy to conduct.
As for optimization methods used in transfer learning, methods in which a meta-learner is applied to learn how to update the parameters of the backbone model has attracted lots of attention recently [20], [21]. In these approaches, parameters are updated using a learned update algorithm. For instance, Finn et al. [21] proposed a meta-learning method MAML by using an LSTM network to update parameters. Our proposal is distinct from these approaches in several aspects. First, these meta-learning works aim to ﬁnd a better parameter optimization route that can efﬁciently optimize the model parameter. Differently, the PTL network is designed

Method
DenseNet-100 DenseNet-100+PTL DenseNet-100+PTL-v2

CIFAR-10
5.92% 5.62% 5.38%

CIFAR-100
24.15% 23.17% 23.2%

CIFAR-100+
22.27% 21.52% 21.83%

to mitigate the distribution difference between mini-batch and the whole dataset. Meanwhile, the BConv-Cells can be directly participating in the feature extraction.
B. Person Re-identiﬁcation Networks
With the prosperity of deep learning, using deep learning networks as feature extractor has become common in person ReID tasks. Many deep learning based person ReID methods [9], [22], [23] have been proposed. As for the transfer learning based deep person ReID method, Geng et al. [24] proposed a deep transfer learning model to address the data sparsity problem.
C. Knowledge Distillation Methods
Our proposed STD method is a special case of knowledge distillation [25]. More generally, it can be seen as a special case of learning with privileged information. Using distillation for model compression is mentioned by Hinton et al. [25]. Wu et al. [26] used the distillation method to improve the accuracy of binary neural networks on ImageNet.
VIII. CONCLUSION
In this paper, we propose a Batch-related Convolutional Cell (BConv-Cell) and its variation BConv-Cell-v2 to mitigate the impact of the bias of each mini-batch caused by internal variations. The BConv-Cells can progressively collect the global information of the dataset during training while participating in the feature extraction. This global information will be used to mitigate the bias of each mini-batch in the next iterations. Based on the BConv-Cells, we propose the Progressive Transfer Learning (PTL) method to ﬁne-tune the pre-trained model into the target dataset. Extensive experiments show that our method can signiﬁcantly improve the backbone network’s performance and achieve state-of-the-art performance on four datasets, including Market-1501, MSMT17, CUHK03, and DukeMTMC-reID datasets. Moreover, we extend our proposal’s application scenario from the person Re-ID task to the general image classiﬁcation tasks. We also evaluate the performance of our proposal in several image classiﬁcation benchmarks. The experimental results show that our proposal can improve the performance of backbone models in image classiﬁcation tasks.
REFERENCES
[1] W. Deng, L. Zheng, Q. Ye, G. Kang, Y. Yang, and J. Jiao, “Imageimage domain adaptation with preserved self-similarity and domaindissimilarity for person re-identiﬁcation,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2018, pp. 994– 1003.

9

[2] L. Ma, Q. Sun, S. Georgoulis, L. Van Gool, B. Schiele, and M. Fritz, “Disentangled person image generation,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2018, pp. 99– 108.
[3] N. S. Keskar, D. Mudigere, J. Nocedal, M. Smelyanskiy, and P. T. P. Tang, “On large-batch training for deep learning: Generalization gap and sharp minima,” CoRR, vol. abs/1609.04836, 2016. [Online]. Available: http://arxiv.org/abs/1609.04836
[4] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger, “Densely connected convolutional networks.” in CVPR, vol. 1, 2017, p. 3.
[5] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image recognition,” in Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp. 770–778.
[6] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and L. Fei-Fei, “ImageNet Large Scale Visual Recognition Challenge,” International Journal of Computer Vision (IJCV), vol. 115, no. 3, pp. 211–252, 2015.
[7] L. Wei, S. Zhang, W. Gao, and Q. Tian, “Person trasfer gan to bridge domain gap for person re-identiﬁcation,” in Computer Vision and Pattern Recognition, IEEE Conference on, 2018.
[8] L. Zheng, L. Shen, L. Tian, S. Wang, J. Wang, and Q. Tian, “Scalable person re-identiﬁcation: A benchmark,” in Proceedings of the IEEE International Conference on Computer Vision, 2015, pp. 1116–1124.
[9] W. Li, R. Zhao, T. Xiao, and X. Wang, “Deepreid: Deep ﬁlter pairing neural network for person re-identiﬁcation,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2014, pp. 152– 159.
[10] Z. Zheng, L. Zheng, and Y. Yang, “Unlabeled samples generated by gan improve the person re-identiﬁcation baseline in vitro,” in Proceedings of the IEEE International Conference on Computer Vision, 2017.
[11] S. Xingjian, Z. Chen, H. Wang, D.-Y. Yeung, W.-K. Wong, and W.c. Woo, “Convolutional lstm network: A machine learning approach for precipitation nowcasting,” in Advances in neural information processing systems, 2015, pp. 802–810.
[12] G. Wang, Y. Yuan, X. Chen, J. Li, and X. Zhou, “Learning discriminative features with multiple granularities for person re-identiﬁcation,” in 2018 ACM Multimedia Conference on Multimedia Conference. ACM, 2018, pp. 274–282.
[13] Z. Yu, S. Liang, L. Wei, Z. Jin, J. Huang, D. Cai, X. He, and X.-S. Hua, “Macar: Urban trafﬁc light control via active multiagent communication and action rectiﬁcation,” in Proceedings of the Twenty-Ninth International Joint Conference on Artiﬁcial Intelligence, IJCAI-20, C. Bessiere, Ed. International Joint Conferences on Artiﬁcial Intelligence Organization, 7 2020, pp. 2491–2497, main track. [Online]. Available: https://doi.org/10.24963/ijcai.2020/345
[14] H. Li, Z. Xu, G. Taylor, C. Studer, and T. Goldstein, “Visualizing the loss landscape of neural nets,” in Neural Information Processing Systems, 2018.
[15] Y. Zhang, T. Xiang, T. M. Hospedales, and H. Lu, “Deep mutual learning,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2018, pp. 4320–4328.
[16] W. Li, X. Zhu, and S. Gong, “Harmonious attention network for person re-identiﬁcation,” in CVPR, vol. 1, 2018, p. 2.
[17] Y. Sun, L. Zheng, Y. Yang, Q. Tian, and S. Wang, “Beyond part models: Person retrieval with reﬁned part pooling (and a strong convolutional baseline),” in Proceedings of the European Conference on Computer Vision (ECCV), 2018, pp. 480–496.
[18] Z. Zhong, L. Zheng, Z. Zheng, S. Li, and Y. Yang, “Camera style adaptation for person re-identiﬁcation,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2018, pp. 5157–5166.
[19] H. Fan, L. Zheng, C. Yan, and Y. Yang, “Unsupervised person reidentiﬁcation: Clustering and ﬁne-tuning,” ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM), vol. 14, no. 4, p. 83, 2018.
[20] D. Ha, A. M. Dai, and Q. V. Le, “Hypernetworks,” CoRR, vol. abs/1609.09106, 2016. [Online]. Available: http://arxiv.org/abs/1609. 09106
[21] C. Finn, P. Abbeel, and S. Levine, “Model-agnostic meta-learning for fast adaptation of deep networks,” in Proceedings of the 34th International Conference on Machine Learning-Volume 70. JMLR. org, 2017, pp. 1126–1135.
[22] R. R. Varior, M. Haloi, and G. Wang, “Gated siamese convolutional neural network architecture for human re-identiﬁcation,” in European Conference on Computer Vision. Springer, 2016, pp. 791–808.

[23] X. Zhang, H. Luo, X. Fan, W. Xiang, Y. Sun, Q. Xiao, W. Jiang, C. Zhang, and J. Sun, “Alignedreid: Surpassing human-level performance in person re-identiﬁcation,” arXiv preprint arXiv:1711.08184, 2017.
[24] M. Geng, Y. Wang, T. Xiang, and Y. Tian, “Deep transfer learning for person re-identiﬁcation,” arXiv preprint arXiv:1611.05244, 2016.
[25] G. Hinton, O. Vinyals, and J. Dean, “Distilling the knowledge in a neural network,” arXiv preprint arXiv:1503.02531, 2015.
[26] X. Wu, “High performance binarized neural networks trained on the imagenet classiﬁcation task,” CoRR, vol. abs/1604.03058, 2016. [Online]. Available: http://arxiv.org/abs/1604.03058
Zhengxu Yu is now a doctoral candidate at Zhejiang University, and he is currently a research intern at Alibaba DAMO Academy. His research interests include large scale machine learning and computer vision.
Dong Shen received a B.S. degree in computer science from Zhejiang University, China, in 2018. He is currently a master student in computer science at Zhejiang University. His research interests include machine learning and computer vision.
Zhongming Jin is now a staff algorithm engineer at Alibaba DAMO Academy. Previously, he was a researcher at Baidu Research. He received his Ph.D. degree from Zhejiang University in Mar. 2015. His research interests include large scale machine learning and computer vision.
Jianqiang Huang is a director of Alibaba DAMO Academy. He received the second prize of National Science and Technology Progress Award in 2010. His research interests focus on visual intelligence in the city brain project of Alibaba.

10
Deng Cai is a Professor in the State Key Lab of CAD&CG, College of Computer Science at Zhejiang University, China. He received a Ph.D. degree in computer science from the University of Illinois at Urbana Champaign in 2009. His research interests include machine learning, data mining and information retrieval.
Xian-Sheng Hua (F’16) received the B.S. and Ph.D. degrees in applied mathematics from Peking University, Beijing, in 1996 and 2001, respectively. In 2001, he joined Microsoft Research Asia as a Researcher and has been a Senior Researcher of Microsoft Research Redmond since 2013. He became a Researcher and the Senior Director of the Alibaba Group in 2015. He has authored or co-authored over 250 research papers and has ﬁled over 90 patents. His research interests have been in the areas of multimedia search, advertising, understanding, and mining, and pattern recognition and machine learning. He was honored as one of the recipients of MIT35. He served as a Program Co-Chair for the IEEE ICME 2013, the ACM Multimedia 2012, and the IEEE ICME 2012, and on the Technical Directions Board of the IEEE Signal Processing Society. He is an ACM Distinguished Scientist and IEEE Fellow.

