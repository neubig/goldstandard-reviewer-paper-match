Multi-Head Decoder for End-to-End Speech Recognition
Tomoki Hayashi1, Shinji Watanabe2, Tomoki Toda1, Kazuya Takeda1
1Nagoya University, 2Johns Hopkins University
hayashi.tomoki@g.sp.m.is.nagoya-u.ac.jp, shinjiw@jhu.edu, tomoki@icts.nagoya-u.ac.jp, kazuya.takeda@nagoya-u.ac.jp

arXiv:1804.08050v2 [cs.CL] 28 Jul 2018

Abstract
This paper presents a new network architecture called multihead decoder for end-to-end speech recognition as an extension of a multi-head attention model. In the multi-head attention model, multiple attentions are calculated, and then, they are integrated into a single attention. On the other hand, instead of the integration in the attention level, our proposed method uses multiple decoders for each attention and integrates their outputs to generate a ﬁnal output. Furthermore, in order to make each head to capture the different modalities, different attention functions are used for each head, leading to the improvement of the recognition performance with an ensemble effect. To evaluate the effectiveness of our proposed method, we conduct an experimental evaluation using Corpus of Spontaneous Japanese. Experimental results demonstrate that our proposed method outperforms the conventional methods such as locationbased and multi-head attention models, and that it can capture different speech/linguistic contexts within the attention-based encoder-decoder framework. Index Terms: speech recognition, end-to-end, attention, dynamical neural network
1. Introduction
Automatic speech recognition (ASR) is the task to convert a continuous speech signal into a sequence of discrete characters, and it is a key technology to realize the interaction between human and machine. ASR has a great potential for various applications such as voice search and voice input, making our lives more rich. Typical ASR systems [1] consist of many modules such as an acoustic model, a lexicon model, and a language model. Factorizing the ASR system into these modules makes it possible to deal with each module as a separate problem. Over the past decades, this factorization has been the basis of the ASR system, however, it makes the system much more complex.
With the improvement of deep learning techniques, end-toend approaches have been proposed [2]. In the end-to-end approach, a continuous acoustic signal or a sequence of acoustic features is directly converted into a sequence of characters with a single neural network. Therefore, the end-to-end approach does not require the factorization into several modules, as described above, making it easy to optimize the whole system. Furthermore, it does not require lexicon information, which is handcrafted by human experts in general.
The end-to-end approach is classiﬁed into two types. One approach is based on connectionist temporal classiﬁcation (CTC) [2–4], which makes it possible to handle the difference in the length of input and output sequences with dynamic programming. The CTC-based approach can efﬁciently solve the sequential problem, however, CTC uses Markov assumptions to perform dynamic programming and predicts output symbols such as characters or phonemes for each frame independently.

Consequently, except in the case of huge training data [5, 6], it requires the language model and graph-based decoding [7].
The other approach utilizes attention-based method [8]. In this approach, encoder-decoder architecture [9, 10] is used to perform a direct mapping from a sequence of input features into text. The encoder network converts the sequence of input features to that of discriminative hidden states, and the decoder network uses attention mechanism to get an alignment between each element of the output sequence and the encoder hidden states. And then it estimates the output symbol using weighted averaged hidden states, which is based on the alignment, as the inputs of the decoder network. Compared with the CTC-based approach, the attention-based method does not require any conditional independence assumptions including the Markov assumption, language models, and complex decoding. However, non-causal alignment problem is caused by a too ﬂexible alignment of the attention mechanism [11]. To address this issue, the study [11] combines the objective function of the attentionbased model with that of CTC to constrain ﬂexible alignments of the attention. Another study [12] uses a multi-head attention (MHA) to get more suitable alignments. In MHA, multiple attentions are calculated, and then, they are integrated into a single attention. Using MHA enables the model to jointly focus on information from different representation subspaces at different positions [13], leading to the improvement of the recognition performance.
Inspired by the idea of MHA, in this study we present a new network architecture called multi-head decoder for end-toend speech recognition as an extension of a multi-head attention model. Instead of the integration in the attention level, our proposed method uses multiple decoders for each attention and integrates their outputs to generate a ﬁnal output. Furthermore, in order to make each head to capture the different modalities, different attention functions are used for each head, leading to the improvement of the recognition performance with an ensemble effect. To evaluate the effectiveness of our proposed method, we conduct an experimental evaluation using Corpus of Spontaneous Japanese. Experimental results demonstrate that our proposed method outperforms the conventional methods such as location-based and multi-head attention models, and that it can capture different speech/linguistic contexts within the attentionbased encoder-decoder framework.
2. Attention-Based End-to-End ASR
The overview of attention-based network architecture is shown in Fig. 1. The attention-based method directly estimates a posterior p(C|X), where X = {x1, x2, . . . , xT } represents a sequence of input features, C = {c1, c2, . . . , cL} represents a sequence of output characters. The posterior p(C|X) is factor-

Figure 1: Overview of attention-based network architecture.

ized with a probabilistic chain rule as follows:

L

p(C|X) = p(cl|c1:l−1, X),

(1)

l=1

where c1:l−1 represents a subsequence {c1, c2, . . . cl−1}, and p(cl|c1:l−1, X) is calculated as follows:

ht = Encoder(X),

(2)

 DotProductAttention(ql−1, ht),



 

AdditiveAttention(ql−1, ht),

alt =

(3)

 LocationAttention(ql−1, ht, al−1),



 

CoverageAttention(ql−1, ht, a1:l−1),

T

rl = altht,

(4)

t=1

p(cl|c1:l−1, X) = Decoder(rl, ql−1, cl−1),

(5)

where Eq. (2) and Eq. (5) represent encoder and decoder networks, respectively, alt represents an attention weight, al represents an attention weight vector, which is a sequence of attention weights {al0, al1, . . . , alT }, a1:l−1 represents a subsequence of attention vectors {a1, a2, . . . , al−1}, ht and ql represent hidden states of encoder and decoder networks, respectively, and rl represents the letter-wise hidden vector, which is a weighted summarization of hidden vectors with the attention weight vector al.
The encoder network in Eq. (2) converts a sequence of input features X into frame-wise discriminative hidden states ht, and it is typically modeled by a bidirectional long short-term memory recurrent neural network (BLSTM):

Encoder(X) = BLSTM(X).

(6)

In the case of ASR, the length of the input sequence is signiﬁcantly different from the length of the output sequence. Hence, basically outputs of BLSTM are often subsampled to reduce the computational cost [8, 14].
The attention weight alt in Eq. (3) represents a soft alignment between each element of the output sequence cl and the encoder hidden states ht.

• DotProductAttention(·) in Eq. (3), which is the most simplest attention [15], is calculated as follows:

elt = qTl−1Waht,

(7)

al = Softmax(el),

(8)

where Wa represents trainable matrix parameters, and el represent a sequence of energies {el1, el2, . . . , elT }
• AdditiveAttention(·) in Eq. (3) is additive attention [16], and the calculation of the energy in Eq. (7) is replaced with following equation:
elt = gT tanh(Wqql−1 + Whht + b), (9)

where W∗ represents trainable matrix parameters, g and b represent trainable vector parameters.
• LocationAttention(·) in Eq. (3) is location-based attention [8], and the calculation of the energy in Eq. (7) is replaced with following equations:

Fl = K ∗ al−1,

(10)

elt = gT tanh(Wqql−1 +Whht +Wf flt +b), (11)

where Fl consists of the vectors {fl1, f l2, . . . , flT }, and K represents trainable one-dimensional convolution ﬁlters.
• CoverageAttention(·) in Eq. (3) is coverage mechanism [17], and the calculation of the energy in Eq. (7) is replaced with following equations:

l−1

vl = al′ ,

(12)

l′ =1
elt = gT tanh(Wqql−1 + Whht + wvvlt + b), (13)

where w represents trainable vector parameters.
The decoder network in Eq. (5) estimates the next character cl from the previous character cl−1, hidden state vector of itself ql−1 and the letter-wise hidden state vector rl, similar to RNN language model (RNNLM) [18]. It is typically modeled using LSTM as follows:

ql = LSTM(cl−1, ql−1, rl),

(14)

Decoder(·) = Softmax(Wql + b),

(15)

where W and b represent trainable matrix and vector parameters, respectively.
Finally, the whole of above networks are optimized using back-propagation through time (BPTT) [19] to minimize the following objective function:

L = − log p(C|X)

L

(16)

= − log

p(cl|c∗1:l−1, X) ,

l=1

where c∗1:l−1 = {c∗1, c∗2, . . . , c∗l−1} represents the ground truth of the previous characters.

3. Multi-Head Decoder
The overview of our proposed multi-head decoder (MHD) architecture is shown in Fig. 2. In MHD architecture, multiple attentions are calculated with the same manner in the conventional multi-head attention (MHA) [13]. We ﬁrst describe the conventional MHA, and extend it to our proposed multi-head decoder (MHD).

Figure 2: Overview of multi-head decoder architecture.

3.1. Multi-head attention (MHA)
The layer-wise hidden vector at the head n is calculated as follows:

a(ltn) = Attention(WQ(n)ql−1, WK (n)ht, . . . ), (17)

T

r(ln) =

a(ltn) WV(n) ht ,

t=1

(18)

where WQ(n), WK (n), and WV(n) represent trainable matrix parameters, and any types of attention in Eq. (3) can be used for Attention(·) in Eq. (17).
In the case of MHA, the layer-wise hidden vectors of each head are integrated into a single vector with a trainable linear transformation:

rl = WO r(l1)⊤, r(l2)⊤, . . . , r(lN)⊤ ⊤ ,

(19)

where WO is a trainable matrix parameter, N represents the number of heads.

3.2. Multi-head decoder (MHD)
On the other hand, in the case of MHD, instead of the integration at attention level, we assign multiple decoders for each head and then integrate their outputs to get a ﬁnal output. Since each attention decoder captures different modalities, it is expected to improve the recognition performance with an ensemble effect. The calculation of the attention weight at the head n in Eq. (17) is replaced with following equation:

a(ltn) = Attention(WQ(n)q(l−n)1, WK (n)ht, . . . ).

(20)

Instead of the integration of the letter-wise hidden vectors
{r(l1), r(l2), . . . , r(lN)} with linear transformation, each letterwise hidden vector r(ln) is fed to n-th decoder LSTM:

q(ln) = LSTM(n)(cl−1, q(l−n)1, r(ln)).

(21)

Note that each LSTM has its own hidden state q(ln) which is used for the calculation of the attention weight a(ltn), while the input character cl is the same among all of the LSTMs. Finally,
all of the outputs are integrated as follows:

p(cl|c1:l−1, X) = Softmax

N
W(n)q(ln) + b
n=1

, (22)

where W(n) represents a trainable matrix parameter, and b represents a trainable vector parameter.

3.3. Heterogeneous multi-head decoder (HMHD)
As a further extension, we propose heterogeneous multi-head decoder (HMHD). Original MHA methods [12,13] use the same attention function such as dot-product or additive attention for each head. On the other hand, HMHD uses different attention functions for each head. We expect that this extension enables to capture the further different context in speech within the attention-based encoder-decoder framework.

4. Experimental Evaluation
To evaluate the performance of our proposed method, we conducted experimental evaluation using Corpus of Spontaneous Japanese (CSJ) [21], including 581 hours of training data, and three types of evaluation data. To compare the performance, we used following dot, additive, location, and three variants of multi-head attention methods:
• Dot-product attention-based model (Dot),
• Additive attention-based model (Add),
• Location-aware attention-based model (Loc),
• Multi-head dot-product attention model (MHA-Dot),
• Multi-head additive attention model (MHA-Add),
• Multi-head location attention model (MHA-Loc).
We used the input feature vector consisting of 80 dimensional log Mel ﬁlter bank and three dimensional pitch feature, which is extracted using open-source speech recognition toolkit Kaldi [22]. Encoder and decoder networks were six-layered BLSTM with projection layer [23] (BLSTMP) and one-layered LSTM, respectively. In the second and third bottom layers in the encoder, subsampling was performed to reduce the length of utterance, yielding the length T /4. For MHA/MHD, we set the number of heads to four. For HMHD, we used two kind of settings: (1) dot-product attention + additive attention + location-based attention + coverage mechanism attention (Dot+Add+Loc+Cov), and (2) two location-based attentions + two coverage mechanism attentions (2×Loc+2×Cov). The number of distinct output characters was 3,315 including Kanji, Hiragana, Katakana, alphabets, Arabic number and sos/eos symbols. In decoding, we used beam search algorithm [10] with beam size 20. We manually set maximum and minimum lengths of the output sequence to 0.1 and 0.5 times the length of the subsampled input sequence, respectively, and the length penalty to 0.1 times the length of the output sequence.

Table 1: Experimental conditions.

# training # evaluation (task 1) # evaluation (task 2) # evaluation (task 3)
Sampling rate Window size Shift size
Encoder type # encoder layers # encoder units # projection units Decoder type # decoder layers # decoder units # heads in MHA # ﬁlter in location att. Filter size in location att.
Learning rate Initialization Gradient clipping norm Batch size Maximum epoch Optimization method AdaDelta ρ AdaDelta ǫ AdaDelta ǫ decay rate
Beam size Maximum length Minimum length

445,068 utterances (581 hours) 1,288 utterances (1.9 hours) 1,305 utterances (2.0 hours) 1,389 utterances (1.3 hours)
16,000 Hz 25 ms 10 ms
BLSTMP 6 320 320 LSTM 1 320 4 10 100
1.0 Uniform [−0.1, 0.1] 5 30 15 AdaDelta [20] 0.95 10−8 10−2
20 0.5 0.1

All of the networks were trained using end-to-end speech processing toolkit ESPnet [24] with a single GPU (Titan X pascal). Character error rate (CER) was used as a metric. The detail of experimental condition is shown in Table 1.
Experimental results are shown in Table 2. First, we focus on the results of the conventional methods. Basically, it is known that location-based attention yields better performance than additive attention [11]. However, in the case of Japanese sentence, its length is much shorter than that of English sentence, which makes the use of location-based attention less effective. In most of the cases, the use of MHA brings the improvement of the recognition performance. Next, we focus on the effectiveness of our proposed MHD architecture. By comparing with the MHA-Loc, MHD-Loc (proposed method) improved the performance in Tasks 1 and 2, while we observed the degradation in Task 3. However, the heterogeneous extension (HMHD), as introduced in Section 3.3, brings the further improvement for the performance of MHD, achieving the best performance among all of the methods for all test sets.
Finally, Figure 3 shows the alignment information of each head of HMHD (2×Loc+2×Cov), which was obtained by visualizing the attention weights. Interestingly, the alignments of the right and left ends seem to capture more abstracted dynamics of speech, while the rest of two alignments behave like normal alignments obtained by a standard attention mechanism. Thus, we can see that the attention weights of each head have a

Table 2: Experimental results.

Dot Add Loc MHA-Dot MHA-Add MHA-Loc
MHD-Loc HMHD (Dot+Add+Loc+Cov) HMHD (2×Loc+2×Cov)

CER [%]

Task 1 Task 2 Task 3

12.7 9.8 10.7

11.1 8.4

9.0

11.7 8.8 10.2

11.6 8.5

9.3

10.7 8.2

9.1

11.5 8.6

9.0

11.0 8.4

9.5

11.0 8.3

9.0

10.4 7.7

8.9

Figure 3: Attention weights of each head. Two left ﬁgures represent the attention weights of the location-based attention, and the remaining ﬁgures represent that of the coverage mechanism attention.
different tendency, and it supports our hypothesis that HMHD can capture different speech/linguistic contexts within its framework.
5. Conclusions
In this paper, we proposed a new network architecture called multi-head decoder for end-to-end speech recognition as an extension of a multi-head attention model. Instead of the integration in the attention level, our proposed method utilized multiple decoders for each attention and integrated their outputs to generate a ﬁnal output. Furthermore, in order to make each head to capture the different modalities, we used different attention functions for each head. To evaluate the effectiveness of our proposed method, we conducted an experimental evaluation using Corpus of Spontaneous Japanese. Experimental results demonstrated that our proposed methods outperformed the conventional methods such as location-based and multi-head attention models, and that it could capture different speech/linguistic contexts within the attention-based encoderdecoder framework.
In the future work, we will combine the multi-head decoder architecture with Joint CTC/Attention architecture [11], and evaluate the performance using other databases.

6. References
[1] F. Jelinek, “Continuous speech recognition by statistical methods,” Proceedings of the IEEE, vol. 64, no. 4, pp. 532–556, 1976.
[2] J. Chorowski, D. Bahdanau, K. Cho, and Y. Bengio, “End-to-end continuous speech recognition using attention-based recurrent nn: First results,” arXiv preprint arXiv:1412.1602, 2014.
[3] A. Graves, S. Ferna´ndez, F. Gomez, and J. Schmidhuber, “Connectionist temporal classiﬁcation: labelling unsegmented sequence data with recurrent neural networks,” in Proceedings of the 23rd international conference on Machine learning. ACM, 2006, pp. 369–376.
[4] A. Graves and N. Jaitly, “Towards end-to-end speech recognition with recurrent neural networks,” in International Conference on Machine Learning, 2014, pp. 1764–1772.
[5] D. Amodei, S. Ananthanarayanan, R. Anubhai, J. Bai, E. Battenberg, C. Case, J. Casper, B. Catanzaro, Q. Cheng, G. Chen et al., “Deep speech 2: End-to-end speech recognition in english and mandarin,” in International Conference on Machine Learning, 2016, pp. 173–182.
[6] H. Soltau, H. Liao, and H. Sak, “Neural speech recognizer: Acoustic-to-word lstm model for large vocabulary speech recognition,” arXiv preprint arXiv:1610.09975, 2016.
[7] Y. Miao, M. Gowayyed, and F. Metze, “Eesen: End-to-end speech recognition using deep rnn models and wfst-based decoding,” in IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU). IEEE, 2015, pp. 167–174.
[8] J. K. Chorowski, D. Bahdanau, D. Serdyuk, K. Cho, and Y. Bengio, “Attention-based models for speech recognition,” in Advances in neural information processing systems, 2015, pp. 577– 585.
[9] K. Cho, B. Van Merrie¨nboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk, and Y. Bengio, “Learning phrase representations using rnn encoder-decoder for statistical machine translation,” arXiv preprint arXiv:1406.1078, 2014.
[10] I. Sutskever, O. Vinyals, and Q. V. Le, “Sequence to sequence learning with neural networks,” in Advances in neural information processing systems, 2014, pp. 3104–3112.
[11] S. Watanabe, T. Hori, S. Kim, J. R. Hershey, and T. Hayashi, “Hybrid ctc/attention architecture for end-to-end speech recognition,” IEEE Journal of Selected Topics in Signal Processing, vol. 11, no. 8, pp. 1240–1253, 2017.
[12] C.-C. Chiu, T. N. Sainath, Y. Wu, R. Prabhavalkar, P. Nguyen, Z. Chen, A. Kannan, R. J. Weiss, K. Rao, K. Gonina et al., “Stateof-the-art speech recognition with sequence-to-sequence models,” arXiv preprint arXiv:1712.01769, 2017.
[13] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” in Advances in Neural Information Processing Systems, 2017, pp. 6000–6010.
[14] W. Chan, N. Jaitly, Q. Le, and O. Vinyals, “Listen, attend and spell: A neural network for large vocabulary conversational speech recognition,” in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2016, pp. 4960–4964.
[15] M.-T. Luong, H. Pham, and C. D. Manning, “Effective approaches to attention-based neural machine translation,” arXiv preprint arXiv:1508.04025, 2015.
[16] D. Bahdanau, K. Cho, and Y. Bengio, “Neural machine translation by jointly learning to align and translate,” arXiv preprint arXiv:1409.0473, 2014.
[17] A. See, P. J. Liu, and C. D. Manning, “Get to the point: Summarization with pointer-generator networks,” arXiv preprint arXiv:1704.04368, 2017.
[18] T. Mikolov, M. Karaﬁa´t, L. Burget, J. Cˇ ernocky`, and S. Khudanpur, “Recurrent neural network based language model,” in Eleventh Annual Conference of the International Speech Communication Association, 2010.

[19] P. J. Werbos, “Backpropagation through time: what it does and how to do it,” Proceedings of the IEEE, vol. 78, no. 10, pp. 1550– 1560, 1990.
[20] M. D. Zeiler, “Adadelta: an adaptive learning rate method,” arXiv preprint arXiv:1212.5701, 2012.
[21] K. Maekawa, H. Koiso, S. Furui, and H. Isahara, “Spontaneous speech corpus of japanese.” in LREC. Citeseer, 2000.
[22] D. Povey, A. Ghoshal, G. Boulianne, L. Burget, O. Glembek, N. Goel, M. Hannemann, P. Motlicek, Y. Qian, P. Schwarz et al., “The kaldi speech recognition toolkit,” in IEEE 2011 workshop on automatic speech recognition and understanding, no. EPFLCONF-192584. IEEE Signal Processing Society, 2011.
[23] H. Sak, A. Senior, and F. Beaufays, “Long short-term memory recurrent neural network architectures for large scale acoustic modeling,” in Fifteenth annual conference of the international speech communication association, 2014.
[24] “ESPnet,” https://github.com/espnet/espnet.

