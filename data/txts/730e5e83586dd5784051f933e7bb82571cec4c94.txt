EEND-SS: Joint End-to-End Neural Speaker Diarization and Speech Separation for Flexible Number of Speakers
Yushi Ueda1, Soumi Maiti1, Shinji Watanabe1, Chunlei Zhang2, Meng Yu2, Shi-Xiong Zhang2, Yong Xu2
1Carnegie Mellon University, Pittsburgh, PA, USA, 2Tencent AI Lab, Bellevue, WA, USA
{yueda,smaiti,swatanab}@andrew.cmu.edu {cleizhang,raymondmyu,auszhang,lucayongxu}@tencent.com

arXiv:2203.17068v1 [eess.AS] 31 Mar 2022

Abstract
In this paper, we present a novel framework that jointly performs speaker diarization, speech separation, and speaker counting. Our proposed method combines end-to-end speaker diarization and speech separation methods, namely, End-to-End Neural Speaker Diarization with Encoder-Decoder-based Attractor calculation (EEND-EDA) and the Convolutional Timedomain Audio Separation Network (ConvTasNet) as multitasking joint model. We also propose the multiple 1×1 convolutional layer architecture for estimating the separation masks corresponding to the number of speakers, and a post-processing technique for reﬁning the separated speech signal with speech activity. Experiments using LibriMix dataset show that our proposed method outperforms the baselines in terms of diarization and separation performance for both ﬁxed and ﬂexible numbers of speakers, as well as speaker counting performance for ﬂexible numbers of speakers. All materials will be open-sourced and reproducible in ESPnet toolkit1. Index Terms: speaker diarization, speech separation, end-toend, multitask learning
1. Introduction
Speaker diarization is the task of estimating multiple speakers’ speech activities (“who spoke when”) from input audio [1]. On the other hand, speech separation is the task of separating each speaker from input mixture audio. Both speech separation and speaker diarization are used as key technologies for Automatic Speech Recognition where multiple speakers are present, such as meetings [2, 3] or parties [4].
Suppose that the information of “who spoke when” is known beforehand, it is expected that we could separate the overlapped speech more easily, and vice versa. Therefore, we can assume that the tasks are mutually related, and the result of one task beneﬁts the performance of the other. However, in most cases, it is not possible to obtain either of the information in advance, which makes it difﬁcult for both tasks to make use of each other. There are also cases where the number of speakers is unknown, which makes the tasks even more challenging.
Traditional clustering-based diarization systems [5, 6] assume that one speaker is active at a time. Hence, they have a problem with realistic data with speaker overlaps. Alternatively, fully end-to-end neural diarization (EEND) [7–9] systems can handle speaker overlap by training with the speaker overlap data. One drawback of EEND is the number of speakers has to be known and ﬁxed beforehand. Several techniques are proposed for EEND with a variable number of speakers,
1https://github.com/espnet/espnet

such as using the maximum number of speakers in the mixture [10] or iteratively extracting one speaker activity at a time using a conditional speaker chain rule [11]. The most notable work, EEND with Encoder-Decoder-based Attractor calculation (EEND-EDA) [12] uses LSTM encoder-decoder based attractors to estimate the speaker counting within diarization.
There are also several works in speech separation that handle a variable number of speakers. The approaches include: recursively separating the speakers one by one [13, 14]; inferring the number of speakers before the separation, then selecting the model corresponding to the number of speakers [15]; ﬁrst conducting separation using the model corresponding to the largest possible number of speakers, then applying speech detection to each of the separated signals to select the model for the detected number of speakers [16].
Even though speaker diarization and speech separation are often used together, their optimal order is not ﬁxed and ordering varies with the scenario and dataset [4, 17, 18]. This different ordering issue suggests that these two tasks should be solved jointly. Our solution is to unify these models as a single neural network and jointly train it with multi-task learning so that both tasks can beneﬁt from each other. Some previous work shows that joint modeling with voice activity detection (VAD) improves speaker diarization [19], target speech separation [20], and speech enhancement tasks [21]. Online Recurrent Selective Attention Network (RSAN) [22, 23] proposes to jointly model speaker counting, diarization, and separation. RSAN focuses on one speaker’s separation iteratively, by doing so they inherently learn each speaker activity information. Though the idea is similar, our proposed model does not require an iterative process and optimizes speaker counting, diarization and separation directly with multi-tasking loss.
This paper proposes a novel framework, Joint End-to-End Neural Speaker Diarization and Separation (EEND-SS), that combines end-to-end speaker diarization and speech separation methods, EEND-EDA [12] and Convolutional Time-domain Audio Separation Network (Conv-TasNet) [24]. Ideally, we can use any speech separation technique, we choose Conv-TasNet as it is one of the most well-known separation models. EENDSS can be trained to directly and jointly minimize speech separation, speaker diarization, and speaker counting errors. We also propose the multiple 1×1 convolutional layer architecture for estimating the separation masks corresponding to the number of speakers, and a post-processing technique for reﬁning the separated speech signals with speech activity. Experimental results show that EEND-SS can improve separation and diarization performances using 2-speaker and 3-speaker datasets, and can also improve speaker counting performance using a variable number of speakers with a mix of 2 and 3 speakers datasets.

2. Proposed method

In this section, we introduce speaker diarization and speech sep-
aration methods behind our study, followed by our proposed
method. Let x ∈ R1×T be a single-channel T -length input speech
mixture of C speakers and noise in anechoic condition, the input speech mixture x can be formulated as follows:

C

x = sc + n,

(1)

c=1

where sc, n ∈ R1×T are the speech signal of speaker c, and the
noise signal in the input speech mixture, respectively.
Speaker diarization, speech separation, and speaker counting tasks aim to estimate the speaker label sequence Yˆ = [yˆ1, · · · , yˆT ] ∈ {0, 1}C×T , the separated speech signals ˆs1, · · · , ˆsC ∈ R1×T , and the number of speakers Cˆ, given x.

2.1. Background Methods

End-to-end neural diarization (EEND) [7–9] is a method for

estimating the speech activities of each speaker from a multi-

ple speaker input mixture using an end-to-end neural network.
Given a T -length sequence of F -dimensional acoustic features O = (ot ∈ RF |t = 1, · · · , T ) derived from input mixture x in Eq. (1), the network converts the acoustic features O into D-dimensional embeddings E = (et ∈ RD|t = 1, · · · , T ). Then it estimates the corresponding speaker label sequence Yˆ . Here, each element of Yˆ , yˆc,t, represents the activity (yˆc,t = 1)
or in-activity (yˆc,t = 0) of the speaker c at the time frame t.
Furthermore, in the case of speaker overlap, where speakers c
and c are active at the same time t, both yˆc,t = 1 and yˆc ,t = 1. Thus, EEND is capable of handling overlapped speech.

EEND minimizes the permutation invariant training (PIT)
error Ldiar between the posterior speech activity probabilities P = (pt ∈ (0, 1)C |t = 1, · · · , T ) and the ground-truth labels Y = (yt ∈ {0, 1}C |t = 1, · · · , T ). Posterior speech activ-
ity probabilities are calculated by applying a fully connected

layer and an element-wise sigmoid function to the embedding et. Ldiar is deﬁned as:

1

T φ

Ldiar =

min

H(yt , pt). (2)

T C (φ1,··· ,φC )∈Φ(C)

t=1

Here, Φ(C) : set of all possible permutations of the sequence (1, · · · , C), ytφ := [yφc,t ∈ {0, 1}|c = 1, · · · , C] is the permuted groundtruth labels, and H(·, ·) is the binary cross entropy deﬁned as
C
H(yt, pt) := {−yc,tlogpc,t − (1 − yc,t)log(1 − pc,t)} .
c=1
(3) We obtain yˆc,t for each speaker c and time frame t, as introduced in the preliminary part of Section 2, by comparing the posterior probability pc,t with the given threshold θ ∈ (0, 1). One drawback of EEND is that the number of speakers C has to be ﬁxed in advance. To overcome this difﬁculty, EEND with Encoder-Decoder Attractor calculation (EENDEDA) [12] was proposed, which handles a ﬂexible number of speakers by predicting speaker existence. In EEND-EDA, an LSTM-based encoder-decoder generates speaker-wise attractors ac ∈ RD until a stopping criterion is satisﬁed. Attractor existence probabilities qc ∈ (0, 1) are calculated by applying a fully connected layer with a sigmoid function to the attractor ac.

The number of speakers Cˆ, as introduced in the preliminary part of Section 2, is estimated by using qc. Posterior speech activity probabilities pt are then calculated with matrix multiplication of embeddings et and attractors A = [a1, · · · , aCˆ ].
Since the oracle number of speakers C is known during
training, the training objective of the attractor existence probabilities is based on the ﬁrst (C +1)-th attractors using the binary
cross entropy deﬁned in Eq. (3):

1

Lexist =

H(l, q),

(4)

C +1

C

where l := [1, · · · , 1, 0] and q := [q1, · · · , qC+1] . During inference, Cˆ is estimated by counting the ﬁrst Cˆ attractor existence probabilities qc that satisfy q1 ≥ τ, · · · , qCˆ ≥ τ, qCˆ+1 < τ , where τ ∈ (0, 1) is a given threshold. Convolutional Time-domain Audio Separation Network (Conv-TasNet) [24] is one of the most well-known methods to separate the audio signal in the time domain. Conv-TasNet consists of three fully convolutional modules: encoder, decoder, and separator. It uses a convolution encoder to encode the input audio signal x into T segments of N -dimensional representations W = (wk ∈ R1×N |k = 1, · · · , T ) and perform speech separation on them. We use the same time resolution as the diarization network. The representations are then reconstructed back to separated audio signals ˆs1, · · · , ˆsC by a deconvolution decoder, as introduced in the preliminary part of Section 2.
In the separator, output from the encoder is processed by a global layer normalization and a 1×1 convolutional layer. A repeated temporal convolutional network (TCN) modules each composed of stacked 1-D dilated convolutional blocks follow. Two 1×1 convolutional layers in the 1-D convolutional blocks each serve as the residual path and the skip-connection path. The output of the residual path is the input of the next block. The skip-connection paths of all blocks are summed up and used as the input (hereinafter referred to as “TCN bottleneck features”) to the last 1×1 convolutional layer and a nonlinear activation layer to estimate C masks m1, · · · , mC ∈ R1×N . Conv-TasNet is trained with the SI-SDR loss deﬁned as:

ˆs,s s 2

s2

LSI-SDR = −10log10

2.

(5)

ˆs − ˆss,s 2s

2.2. Proposed Method: Joint End-to-End Neural Speaker Diarization and Speech Separation (EEND-SS)
Overall structure: Our proposed model Joint End-to-End Neural Speaker Diarization and Separation (EEND-SS) performs three tasks: speaker diarization, speech separation, and speaker counting. Figure 1 shows the overall structure of EEND-SS, which includes the encoder, separator, decoder, and diarization modules. The encoder, the ﬁrst 1×1 convolutional layer and the TCNs in the separator module (indicated as yellow blocks in Figure 1) are shared between the separation and diarization branches. The encoder, separator, and decoder modules in EEND-SS are equivalent to those of Conv-TasNet, except for the last 1×1 convolution layer in the separator, which is described in detail below. The diarization module architecture is the same as EEND-EDA except for the input features. EEND-SS uses learned TCN bottleneck features from the separator while EEND-EDA uses log-mel ﬁlterbank features (LMF). Optionally, the EEND-SS diarization module may also include the LMF features concatenated with TCN bottleneck features (shown with dotted lines in Figure 1).

Separation Branch Diarization Branch Shared Network

Separated Sources
Decoder (1-D conv)

Diarization Attractor Results Existence Prob.
Diarization Module (EEND-EDA)

Separator Module

Masks

1x1 conv Non Linear

concat

TCN bottleneck features

TCNs
LayerNorm 1x1 conv

Log-mel filterbank features

Encoder (1-D conv)

Log-mel filterbank

Input Mixture

Figure 1: Overall structure of the proposed model (EEND-SS).

C Masks

1 Mask
1x1 conv Non Linear

2 Masks ...
1x1 conv Non Linear

Multiple 1x1 conv layers

C Masks
1x1 conv Non Linear

... Cmax Masks 1x1 conv Non Linear

Figure 2: Multiple 1×1 convolutional layer architecture.

Multiple 1×1 convolutional layers: In Conv-TasNet, the num-

ber of speakers is predetermined, and the last 1×1 convolutional

layer only creates masks corresponding to the predetermined

number. In EEND-SS, to allow masks for a variable number

of speakers, multiple 1×1 convolutional layers are used. Each

layer corresponds to a different number of speakers (surrounded

by red lines in Figure 1). One of them is selected according to

the number of speakers C in the input, as shown in Figure 2. For

C, the oracle number is used during training, and the number

estimated by the diarization module Cˆ is used during inference.

This architecture is similar to multi-decoder DPRNN [15] in

terms of selecting the network corresponding to the estimated

number of speakers. However, while multi-decoder DPRNN

switches the whole decoder, EEND-SS only switches a single

layer and shares the decoder structure. Thus, the decoder in

EEND-SS is trained using the input mixture with a various num-

ber of speakers. This architecture is thought to be efﬁcient, es-

pecially when the training samples including a speciﬁc number

of speakers are not sufﬁcient. In multiple 1×1 convolutional

layer architecture, the maximum number of speakers that the

model can handle is bound to the number of multiple 1×1 con-

volutional layers Cmax. However, in practice, we can handle an

arbitrary number of speakers by setting Cmax to a sufﬁciently

large number. Note that since the unused 1×1 layers will not

interfere with the rest of the network, we can safely set Cmax to

a large number without hurting the performance.

Training: The network is trained with a multi-task cost func-

tion

L = λ1LSI-SDR + λ2Ldiar + λ3Lexist,

(6)

which is a weighted sum of LSI-SDR in Eq. (5), Ldiar in Eq. (2) and Lexist in Eq. (4). λ1, λ2, λ3 ∈ R+ are the weighting parameters that are chosen empirically. Inference: To handle a variable number of speakers, we utilize
the following 2-pass inference procedure: (1) Obtain diarization result P & the number of speakers Cˆ from input speech mixture. (2) Select 1×1 convolutional layer corresponding to Cˆ masks, then obtain separated speech signals ˆs1, · · · , ˆsCˆ . Post-processing: We also propose to utilize post-processing for
reﬁning the separated speech signals with speech activity sim-
ilar to [19, 20, 25], e.g., reducing the background noise while
the speaker is not present. The post-processing is conducted

by multiplying the separated speech signals ˆs from the decoder module and the posterior speech activity probabilities pt from the diarization module. Unlike VAD which only outputs the result for a single speaker, in the case of a multi-speaker situation, we need to ﬁnd the corresponding speakers between the separated speech signals and the diarization results since the output ordering of the speakers may differ. In this work, the corresponding speakers are determined by the combination that maximizes the sum of correlations between the amplitude of the separated speech signals and the posterior probabilities. Let ˆs be the separated speech signals after the post-processing, the post-processing can be formulated as follows:

ˆs = ˆs pφmax ,

(7)

C

pφmax := argmax

r(abs(ˆs), pφ). (8)

(φ1,··· ,φC )∈Φ(C) c=1

Here, denotes element-wise multiplication, r(·, ·) denotes the
correlation function, Φ(C) is as introduced in Eq. (2), and pφ := [pφc,t ∈ (0, 1)|c = 1, · · · , C] is the permuted posterior probabilities.

3. Experiments

3.1. Experimental settings

Dataset: For the training and evaluation, we used the LibriMix2 [26] dataset. LibriMix uses speech samples from LibriSpeech [27] train-clean100/dev-clean/test-clean and the noise samples from WHAM! [28] to generate mixtures for training/validation/testing. The dataset includes 58h/11h/11h of training/validation/testing sets for a two-speaker mixture (Libri2Mix), and 40h/11h/11h for a three-speaker mixture (Libri3Mix). We used an 8kHz sampling rate and the min mode. Conﬁgurations: The model parameters used for the experiments are as follows: For the encoder and decoder, we set the kernel size to 16 and dimension of the representation to 512. We set number of channels for TCN bottleneck features to 128, the repeat of TCN modules to 3, and the stack of 1-D convolutional layers to 8 for the separator. For the EEND-EDA, we use 2-D convolutional layer with 1/8 sub-sampling as an input layer and 4-stacked Transformer encoders with 4 attention heads without positional encodings and dimensionality of the representations as 256. We use 80-dimensional LMF converted from power spectra calculated with frame length of 512 samples and frame shift of 64 samples. We set the thresholds θ and τ for obtaining the diarization results and speaker counting to 0.5. We empirically set the values of λ1, λ2 and λ3 in Eq. 6 as 1.0, 0.2, 0.2 respectively, unless otherwise noted. We use the same model parameters for Conv-TasNet, EEND-EDA and EEND-SS. We employed the Adam optimizer for the training with mini-batch as 16 and learning rate as 10−3. Learning rate was halved and training was stopped if there was no improvement for 3 or 5 consecutive epochs, respectively. Evaluation Metrics: We report separation performance with multiple objective metrics, including source-to-distortion ratio improvement (SDRi(dB)) [29], scale-invariant source-todistortion ratio improvement (SI-SDRi(dB)) [30], and shorttime objective intelligibility (STOI) [31], and diarization performance with the diarization error rate (DER(%)) [32]. When calculating the DER, collar tolerance of 0.0 sec and median ﬁltering of 11 frames were used. We also report the Speaker Counting Accuracy (SCA(%)) for speaker counting performance.

2We used the groundtruth diarization labels available at https: //github.com/s3prl/LibriMix

Table 1: Experimental results on Libri2Mix. “PP” indicates Post-Processing.

Method
Conv-TasNet 3 EEND-EDA EEND-SS (λ1 = 0)
EEND-SS + PP + LMF + LMF + PP

STOI
0.824 – –
0.826 0.826 0.831 0.831

SI-SDRi
9.54 – –
9.76 9.83 10.62 10.70

SDRi
10.40 – –
10.57 10.67 11.13 11.23

DER –
5.93 5.26
5.08
5.17

Table 2: Experimental results on Libri3Mix.

Method
Conv-TasNet EEND-EDA EEND-SS (λ1 = 0)
EEND-SS + PP + LMF + LMF + PP

STOI
0.721 – –
0.722 0.722 0.723 0.723

SI-SDRi
7.94 – –
7.66 7.71 8.39 8.40

SDRi
8.73 – –
8.60 8.66 8.96 9.00

DER –
8.81 6.50
6.26
6.00

3.2. Results
3.2.1. Fixed number of speakers
First, we evaluated our method on the 2-speaker and 3-speaker ﬁxed conditions using Libri2Mix and Libri3Mix datasets, respectively. As shown in Table 1 and 2, in both speaker diarization and speech separation performances, EEND-SS outperformed the baseline Conv-TasNet and EEND-EDA, as well as EEND-SS trained only on speaker diarization task (setting λ1 = 0 in Eq. 6), for Libri2Mix and Libri3Mix datasets. Furthermore, a constant performance gain for the speech separation is achieved by concatenating LMF described in section 2.2 and applying the post-processing (PP) in Eq. (7). Thus, we show the effectiveness of joint speech separation and speaker diarization based on the proposed method for ﬁxed numbers of speakers.
Additionally, we tested our proposed method on Libri2Mix max mode4 to compare the diarization performance with EEND-based models reported in [33]. EEND-SS outperformed the DERs of the model using LMF as well as 10 other models using self-supervised pretraining for feature extraction, as shown in Table 3. However, we were not able to reach their performance using HuBERT [34] and wav2vec 2.0 [35], which are reported to achieve high performances for many other speech processing tasks as well [33]. The results indicate room for further improvement using self-supervised features instead of LMF, which is left for future work.
3.2.2. Flexible number of speakers
Next, we evaluated our method on the 2 & 3-speaker mixed condition created by combining both Libri2 & 3 Mix datasets. We followed the training procedure of a ﬂexible number of speakers in [12], and ﬁnetuned the models from the weights trained on Libri2Mix. In this experiment, the number of reference speech signals C and the separated speech signals Cˆ may differ due to speaker counting error. To evaluate the separation performance
3Our SI-SDRi & SDRi for Libri2Mix did not reach the numbers reported in https://github.com/asteroid-team/ asteroid/tree/master/egs/librimix/ConvTasNet, possibly due to the difference in batch size, where we use 16 while asteroid team uses 24.
4We used the models trained on min mode.

Table 3: Comparison of DERs on Libri2Mix max mode. “SS Pretrained” indicates Self-Supervised Pretrained models.

Method EEND [33] EEND-SS

Features
SS Pretrained wav2vec 2.0/HuBERT Others
LMF
TCN Bottleneck TCN Bottleneck+LMF

DER
5.62–6.08 6.59–10.54
10.05 7.49 6.54

Table 4: Experimental results on Libri2Mix & Libri3Mix mixture dataset.

Method
Conv-TasNet (oracle) EEND-EDA EEND-SS (λ1 = 0)
EEND-SS + PP + LMF + LMF + PP

STOI
0.756 – –
0.760 0.760 0.767 0.767

SI-SDRi
7.66 – –
9.31 9.38 8.83 8.87

SDRi
8.71 – –
7.50 7.59 9.72 9.77

DER –
10.16 8.79
6.27
6.04

SCA –
86.2 90.4
97.9
98.2

in such cases, we append |C − Cˆ| silent audio signals to the reference or the separated speech signals so that the number of signals will match. To avoid the objective metrics from diverging, signals with amplitude of 10−6 is used in our implementation. Since Conv-TasNet cannot perform speaker counting, the oracle numbers were used during inference. SCA was also measured in this experiment.
The results for ﬂexible numbers of speakers are shown in Table 4. Likewise the results for ﬁxed numbers of speakers, EEND-SS outperformed the baseline methods in all the metrics, including speaker counting. Interestingly, despite EEND-SS estimates the number of speakers, it even outperformed ConvTasNet using the oracle number in the separation performance metrics. We can assume that in EEND-SS, TCN bottleneck features that are suitable for speech separation, as well as speaker diarization were learned thanks to the joint training framework. Thus, we show the effectiveness of joint speech separation, speaker diarization, and speaker counting based on the proposed method for ﬂexible numbers of speakers.

4. Conclusion
In this paper, we proposed a framework for jointly and directly optimizing speaker diarization, speech separation, and speaker counting. In addition, we proposed the multiple 1×1 convolutional layer architecture for estimating separation masks for variable number of speakers and the post-processing for reﬁning separated speech with speech activity. The experimental results show that the proposed method outperforms baseline systems evaluated with both ﬁxed and ﬂexible numbers of speakers. Future work includes using other separation techniques, as well as using the features from self-supervised pretrained models.

5. Acknowledgements
We thank Shota Horiguchi (Hitachi, Ltd.) for his helpful advice on the implementation of EEND-EDA. This work used the Extreme Science and Engineering Discovery Environment (XSEDE) [36], which is supported by NSF grant number ACI1548562. Speciﬁcally, it used the Bridges system [37], which is supported by NSF award number ACI-1445606, at the Pittsburgh Supercomputing Center (PSC).

6. References
[1] T. J. Park, N. Kanda, D. Dimitriadis, K. J. Han, S. Watanabe, and S. Narayanan, “A review of speaker diarization: Recent advances with deep learning,” Computer Speech & Language, vol. 72, 2022.
[2] J. Carletta, S. Ashby, S. Bourban, M. Flynn, M. Guillemot, T. Hain, J. Kadlec, V. Karaiskos, W. Kraaij, M. Kronenthal et al., “The AMI meeting corpus: A pre-announcement,” in Proc. MLMI, 2005, pp. 28–39.
[3] A. Janin, D. Baron, J. Edwards, D. Ellis, D. Gelbart, N. Morgan, B. Peskin, T. Pfau, E. Shriberg, A. Stolcke, and C. Wooters, “The ICSI meeting corpus,” in Proc. ICASSP, vol. 1, 2003.
[4] S. Watanabe, M. Mandel, J. Barker, E. Vincent, A. Arora, X. Chang, S. Khudanpur, V. Manohar, D. Povey, D. Raj, D. Snyder, A. S. Subramanian, J. Trmal, B. B. Yair, C. Boeddeker, Z. Ni, Y. Fujita, S. Horiguchi, N. Kanda, T. Yoshioka, and N. Ryant, “CHiME-6 challenge: Tackling multispeaker speech recognition for unsegmented recordings,” in CHiME-6, 2020.
[5] G. Sell and D. Garcia-Romero, “Speaker diarization with PLDA ivector scoring and unsupervised calibration,” in Proc. SLT, 2014, pp. 413–417.
[6] S. H. Shum, N. Dehak, R. Dehak, and J. R. Glass, “Unsupervised methods for speaker diarization: An integrated and iterative approach,” TASLP, vol. 21, no. 10, pp. 2015–2028, 2013.
[7] Y. Fujita, N. Kanda, S. Horiguchi, K. Nagamatsu, and S. Watanabe, “End-to-end neural speaker diarization with permutation-free objectives,” in Proc. Interspeech, 2019, pp. 4300–4304.
[8] Y. Fujita, N. Kanda, S. Horiguchi, Y. Xue, K. Nagamatsu, and S. Watanabe, “End-to-end neural speaker diarization with selfattention,” in Proc. ASRU, 2019, pp. 296–303.
[9] Y. C. Liu, E. Han, C. Lee, and A. Stolcke, “End-to-end neural diarization: From transformer to conformer,” in Proc. Interspeech, 2021, pp. 3081–3085.
[10] S. Maiti, H. Erdogan, K. Wilson, S. Wisdom, S. Watanabe, and J. R. Hershey, “End-to-end diarization for variable number of speakers with local-global networks and discriminative speaker embeddings,” in Proc. ICASSP, 2021, pp. 7183–7187.
[11] Y. Fujita, S. Watanabe, S. Horiguchi, Y. Xue, J. Shi, and K. Nagamatsu, “Neural speaker diarization with speaker-wise chain rule,” arXiv preprint arXiv:2006.01796, 2020.
[12] S. Horiguchi, Y. Fujita, S. Watanabe, Y. Xue, and K. Nagamatsu, “End-to-end speaker diarization for an unknown number of speakers with encoder-decoder based attractors,” in Proc. Interspeech, 2020, pp. 269–273.
[13] N. Takahashi, S. Parthasaarathy, N. Goswami, and Y. Mitsufuji, “Recursive speech separation for unknown number of speakers,” in Proc. Interspeech, 2019, pp. 1348–1352.
[14] K. Kinoshita, L. Drude, M. Delcroix, and T. Nakatani, “Listening to each speaker one by one with recurrent selective hearing networks,” in Proc. ICASSP, 2018, pp. 5064–5068.
[15] J. Zhu, R. A. Yeh, and M. Hasegawa-Johnson, “Multi-decoder DPRNN: Source separation for variable number of speakers,” in Proc. ICASSP, 2021, pp. 3420–3424.
[16] E. Nachmani, Y. Adi, and L. Wolf, “Voice separation with an unknown number of multiple speakers,” in Proc. ICML, 2020, pp. 7164–7175.
[17] Z. Chen, T. Yoshioka, L. Lu, T. Zhou, Z. Meng, Y. Luo, J. Wu, X. Xiao, and J. Li, “Continuous speech separation: Dataset and analysis,” in Proc. ICASSP, 2020, pp. 7284–7288.
[18] D. Raj, P. Denisov, Z. Chen, H. Erdogan, Z. Huang, M. He, S. Watanabe, J. Du, T. Yoshioka, Y. Luo, N. Kanda, J. Li, S. Wisdom, and J. R. Hershey, “Integration of speech separation, diarization, and recognition for multi-speaker meetings: System description, comparison, and analysis,” in Proc. SLT, 2021, pp. 897–904.
[19] Y. Takashima, Y. Fujita, S. Watanabe, S. Horiguchi, P. Garc´ıa, and K. Nagamatsu, “End-to-end speaker diarization conditioned on speech activity and overlap detection,” in Proc. SLT, 2021, pp. 849–856.

[20] Q. Lin, L. Yang, X. Wang, L. Xie, C. Jia, and J. Wang, “Sparsely overlapped speech training in the time domain: Joint learning of target speech separation and personal vad beneﬁts,” arXiv preprint arXiv:2106.14371, 2021.
[21] X. Tan and X.-L. Zhang, “Speech enhancement aided end-to-end multi-task learning for voice activity detection,” in Proc. ICASSP, 2021, pp. 6823–6827.
[22] T. v. Neumann, K. Kinoshita, M. Delcroix, S. Araki, T. Nakatani, and R. Haeb-Umbach, “All-neural online source separation, counting, and diarization for meeting analysis,” in Proc. ICASSP, 2019, pp. 91–95.
[23] K. Kinoshita, M. Delcroix, S. Araki, and T. Nakatani, “Tackling real noisy reverberant meetings with all-neural source separation, counting, and diarization system,” in Proc. ICASSP, 2020, pp. 381–385.
[24] Y. Luo and N. Mesgarani, “Conv-TasNet: Surpassing ideal time–frequency magnitude masking for speech separation,” TASLP, vol. 27, no. 8, pp. 1256–1266, 2019.
[25] T. Ochiai, M. Delcroix, R. Ikeshita, K. Kinoshita, T. Nakatani, and S. Araki, “Beam-TasNet: Time-domain audio separation network meets frequency-domain beamformer,” in Proc. ICASSP, 2020, pp. 6384–6388.
[26] J. Cosentino, M. Pariente, S. Cornell, A. Deleforge, and E. Vincent, “LibriMix: An open-source dataset for generalizable speech separation,” arXiv preprint arXiv:2005.11262, 2020.
[27] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, “LibriSpeech: an ASR corpus based on public domain audio books,” in Proc. ICASSP, 2015, pp. 5206–5210.
[28] G. Wichern, J. Antognini, M. Flynn, L. R. Zhu, E. McQuinn, D. Crow, E. Manilow, and J. Le Roux, “WHAM!: Extending speech separation to noisy environments,” in Proc. Interspeech, 2019, pp. 1368–1372.
[29] C. Fe´votte, R. Gribonval, and E. Vincent, “BSS-EVAL toolbox user guide : Revision 2.0,” IRISA, Tech. Rep. 1706, 2005. [Online]. Available: https://www.irit.fr/Cedric.Fevotte/ publications/techreps/BSSEVAL2userguide.pdf
[30] J. Le Roux, S. Wisdom, H. Erdogan, and J. R. Hershey, “SDR – half-baked or well done?” in Proc. ICASSP, 2019, pp. 626–630.
[31] C. H. Taal, R. C. Hendriks, R. Heusdens, and J. Jensen, “A shorttime objective intelligibility measure for time-frequency weighted noisy speech,” in Proc. ICASSP, 2010, pp. 4214–4217.
[32] J. Fiscus, J. Ajot, M. Michel, and J. Garofolo, “The rich transcription 2006 spring meeting recognition evaluation,” in Proc. MLMI, 2006, pp. 309–322.
[33] S. wen Yang, P.-H. Chi, Y.-S. Chuang, C.-I. J. Lai, K. Lakhotia, Y. Y. Lin, A. T. Liu, J. Shi, X. Chang, G.-T. Lin, T.-H. Huang, W.-C. Tseng, K. tik Lee, D.-R. Liu, Z. Huang, S. Dong, S.-W. Li, S. Watanabe, A. Mohamed, and H. yi Lee, “SUPERB: Speech Processing Universal PERformance Benchmark,” in Proc. Interspeech, 2021, pp. 1194–1198.
[34] W.-N. Hsu, B. Bolte, Y.-H. Tsai, K. Lakhotia, R. Salakhutdinov, and A. Mohamed, “HuBERT: Self-supervised speech representation learning by masked prediction of hidden units,” TASLP, 2021.
[35] A. Baevski, Y. Zhou, A. Mohamed, and M. Auli, “wav2vec 2.0: A framework for self-supervised learning of speech representations,” in Proc. NeurIPS, vol. 33, 2020, pp. 12 449–12 460.
[36] J. Towns, T. Cockerill, M. Dahan, I. Foster, K. Gaither, A. Grimshaw, V. Hazlewood, S. Lathrop, D. Lifka, G. D. Peterson, R. Roskies, J. R. Scott, and N. Wilkins-Diehr, “XSEDE: Accelerating scientiﬁc discovery,” Computing in Science & Engineering, vol. 16, no. 5, pp. 62–74, 2014.
[37] N. A. Nystrom, M. J. Levine, R. Z. Roskies, and J. R. Scott, “Bridges: a uniquely ﬂexible hpc resource for new communities and data analytics,” in Proc. 2015 XSEDE Conference: Scientiﬁc Advancements Enabled by Enhanced Cyberinfrastructure, 2015, pp. 1–8.

