Efﬁcient detection of adversarial images
Darpan Kumar Yadav, Kartik Mundra, Rahul Modpur, Arpan Chattopadhyay, Indra Narayan Kar

arXiv:2007.04564v1 [eess.IV] 9 Jul 2020

Abstract—In this paper, detection of deception attack on deep neural network (DNN) based image classiﬁcation in autonomous and cyber-physical systems is considered. Several studies have shown the vulnerability of DNN to malicious deception attacks. In such attacks, some or all pixel values of an image are modiﬁed by an external attacker, so that the change is almost invisible to human eye but signiﬁcant enough for a DNN-based classiﬁer to misclassify it. This paper ﬁrst proposes a novel pre-processing technique that facilitates detection of such modiﬁed images under any DNN-based image classiﬁer as well as attacker model. The proposed pre-processing algorithm involves a certain combination of principal component analysis (PCA)-based decomposition of the image, and random perturbation based detection to reduce computational complexity. Next, an adaptive version of this algorithm is proposed where a random number of perturbations are chosen adaptively using a doubly-threshold policy, and the threshold values are learnt via stochastic approximation in order to minimize the expected number of perturbations subject to constraints on the false alarm and missed detection probabilities. Numerical experiments show that the proposed detection scheme outperform a competing algorithm while achieving reasonably low computational complexity.
Index Terms—Image classiﬁcation, Adversarial image, Deep neural network, Deception attack, Attack detection, Cyberphysical systems, Autonomous vehicles, stochastic approximation.

Over the last few years, several studies have suggested that the DNN-based image classiﬁer is highly vulnerable to deception attack [2], [3]. In fact, with the emergence of internet-of-things (IoT) providing an IP address to all gadgets including cameras, the autonomous vehicles will become more vulnerable to such attacks [4]. Hackers can easily tamper with the pixel values (see Figure 1) or the image data sent by the camera to the classiﬁer. In a similar way, networked surveillance cameras will also become vulnerable to such malicious attacks.
In order to address the above challenge, we propose a new class of algorithms for adversarial image detection. Our ﬁrst perturbation-based algorithm PERT performs PCA (Prinicipal Component Analysis) on clean image data set, and detects an adversary by perturbing a test image in the spectral domain along certain carefully chosen coordinates obtained from PCA. Next, its adaptive version APERT chooses the number of perturbations adaptively in order to minimize the expected number of perturbations subject to constraints on the false alarm and missed detection probabilities. Numerical results demonstrate the efﬁcacy of these two algorithms.

I. INTRODUCTION
Recently there have been signiﬁcant research interest in cyber-physical systems (CPS) that connect the cyber world and the physical world, via integration of sensing, control, communication, computation and learning. Popular CPS applications include networked monitoring of industry, disaster management, smart grids, intelligent transportation systems, networked surveillance, etc. One important component of future intelligent transportation systems is autonomous vehicle. It is envisioned that future autonomous vehicles will be equipped with high-quality cameras, whose images will be classiﬁed by a DNN-based classiﬁer for object detection and recognition, in order to facilitate an informed maneuvering decision by the controller or autopilot. Clearly, vehicular safety in such cases is highly sensitive to image classiﬁcation; any mistake in object detection or classiﬁcation can lead to accidents. In the context of surveillance or security systems, adversarial images can greatly endanger human and system security.
The authors are with the Department of Electrical Engineering, Indian Institute of Technology (IIT), Delhi. Arpan Chattopadhyay is also associated with the Bharti School of Telecom Technology and Management, IIT Delhi. Email: {ee3180534, ee3160495, ee3160503, arpanc, ink}@ee.iitd.ac.in
This work was supported by the faculty seed grant, professional development allowance (PDA) and professional development fund (PDF) of Arpan Chattopadhyay at IIT Delhi.
This manuscript is an extended version of our conference paper [1]

A. Related work
The existing research on adversarial images can be divided into two categories: attack design and attack mitigation.
1) Attack design: While there have been numerous attempts to tackle deception attacks in sensor-based remote estimation systems [5]–[7], the problem of design and mitigation of adversarial attack on images to cause misclassiﬁcation is relatively new. The ﬁrst paper on adversarial image generation was reported in [8]. Since then, there have been signiﬁcant research on attack design in this setting. All these attack schemes can be divided into two categories:
1) White box attack: Here the attacker knows the architecture, parameters, cost functions, etc of the classiﬁer. Hence, it is easier to design such attacks. Examples of such attacks are given in [9], [8], [10], [11], [12], [13].
2) Black box attack: Here the adversary has access only to the output (e.g., logits or probabilities) of the classiﬁer against a test input. Hence, the attacker has to probe the classiﬁer with many test input images in order to estimate the sensitivity of the output with respect to the input. One black box attack is reported in [14].
On the other hand, depending on attack goals, the attack schemes can be divided into two categories:
1) Targeted attack: Such attacks seek to misclassify a particular class to another pre-deﬁned class. For example, a fruit classiﬁer is made to classify all the apple images as banana. Such attacks are reported in [15] and [14].

Figure 1. Example of an adversarial image. The original image is classiﬁed as a cat. Addition of a carefully designed noise changes the same classiﬁer’s output to ostrich, while the visual change in the image is not signiﬁcant.

2) Reliability Attack: Such attacks only seek to increase the classiﬁcation error. Such attacks have been reported in [16], [14], [9], [11], [8].
Some popular adversarial attacks are summarized below:
• L-BFGS Attack [8]: This white box attack tries to ﬁnd a perturbation η to an image x such that the perturbed image x = x + η minimizes a cost function Jθ(x, l) (where θ is cost parameter and l is the label) of the classiﬁer, while η remains within some small set around the origin to ensure small perturbation. A Lagrange multiplier is used to relax the constraint on η, which is found via line search.
• Fast Gradient Sign Method (FGSM) [9]: Here the perturbation is computed as η = (∇xJθ(x, l)) where the magnitude of perturbation. This perturbation can be computed via backpropagation. Basic Iterative Method (BIM) [13]: This is an iterative variant of FGSM.
• CarliniWagner(L2)(CW) Attack [10]: This is similar to [8] except that: (i) [10] uses a cost function that is different from the classiﬁer’s cost function Jθ(·, ·), and (ii) the optimal Lagrange multiplier is found via binary search.
• Projected Gradient Descent (PGD) [11]: This involves applying FGSM iteratively and clipping the iterate images to ensure that they remain close to the original image.
• Jacobian Saliency Map Attack (JSMA) [12]: It is a greedy attack algorithm which selects the most important pixels by calculating Jacobian based saliency map, and modiﬁes those pixels iteratively.
• Boundary Attack [14]: This is a black box attack which starts from an adversarial point and then performs a random walk along the decision boundary between the adversarial and the non-adversarial regions, such that the iterate image stays in the adversarial region but the distance between the iterate image and the target image is progressively minimized. This is done via rejection sampling using a suitable proposal distribution, in order to ﬁnd progressively smaller adversarial perturbations.

2) Attack mitigation: There are two possible approaches for defence against adversarial attack:
1) Robustness based defense: These methods seek to classify adversarial images correctly, e.g., [17], [18].
2) Detection based defense: These methods seek to just distinguish between adversarial and clean images; eg., [19], [20].
Here we describe some popular attack mitigation schemes. The authors of [17] proposed feature denoising to improve robustness of CNNs against adversarial images. They found that certain architectures were good for robustness even though they are not sufﬁcient for accuracy improvements. However, when combined with adversarial training, these designs could be more robust. The authors of [19] put forth a Bayesian view of detecting adversarial samples, claiming that the uncertainty associated with adversarial examples is more compared to clean ones. They used a Bayesian neural network to distinguish between adversarial and clean images on the basis of uncertainty estimation.
The authors of [20] trained a PixelCNN network [21] to differentiate between clean and adversarial examples. They rejected adversarial samples using p-value based ranking of PixelCNN. This scheme was able to detect several attacks like CW (L2), Deepfool, BIM. The paper [22] observed that there is a signiﬁcant difference between the percentage of label change due to perturbation in adversarial samples as compared to clean ones. They designed a statistical adversary detection algorithm called nMutant; inspired by mutation testing from software engineering community.
The authors of [18] designed a method called network distillation to defend DNNs against adversarial examples. The original purpose of network distillation was the reduction of size of DNNs by transferring knowledge from a bigger network to a smaller one [23], [24]. The authors discovered that using high-temperature softmax reduces the model’s sensitivity towards small perturbations. This defense was tested on the MNIST and CIFAR-10 data sets. It was observed that network distillation reduces the success rate of JSMA attack [12] by 0.5% and 5% respectively. However, a lot of new attacks have

been proposed since then, which defeat defensive distillation (e.g., [25]). The paper [9] tried training an MNIST classiﬁer with adversarial examples (adversarial retraining approach). A comprehensive analysis of this method on ImageNet data set found it to be effective against one-step attacks (eg., FGSM), but ineffective against iterative attacks (e.g., BIM [13]). After evaluating network distillation with adversarially trained networks on MNIST and ImageNet, [26] found it to be robust against white box attacks but not against black box ones.

Figure 2. Cumulative explained variance versus components of PCA.

B. Our Contributions
In this paper, we make the following contributions:
1) We propose a novel detection algorithm PERT for adversarial attack detection. The algorithm performs PCA on clean image data set to obtain a set of orthonormal bases. Projection of a test image along some least signiﬁcant principal components are randomly perturbed for detecting proximity to a decision boundary, which is used for detection. This combination of PCA and image perturbation in spectral domain, which is motivated by the empirical ﬁndings in [27], is new to the literature.1
2) PERT has low computational complexity; PCA is performed only once off-line.
3) We also propose an adaptive version of PERT called APERT. The APERT algorithm declares an image to be adversarial by checking whether a speciﬁc sequential probability ratio exceeds an upper or a lower threshold. The problem of minimizing the expected number of perturbations per test image, subject to constraints on false alarm and missed detection probabilities, is relaxed via a pair of Lagrange multipliers. The relaxed problem is solved via simultaneous perturbation stochastic approximation (SPSA; see [30]) to obtain the optimal threshold values, and the optimal Lagrange multipliers are learnt via two-timescale stochastic approximation [31] in order to meet the constraints. The use of stochastic approximation and SPSA to optimize the threshold values are new to the signal processing literature to the best of our knowledge. Also, the APERT algorithm has a sound theoretical motivation which is rare in most papers on adversarial image detection.
4) PERT and APERT are agnostic to attacker and classiﬁer models, which makes them attractive to many practical applications.
5) Numerical results demonstrate high probability of attack detection and small value for false alarm probability under PERT and APERT against a competing algorithm, and reasonably low computational complexity in APERT.
1The paper [28] uses PCA but throws away least signiﬁcant components, thereby removing useful information along those components, possibly leading to high false alarm rate. The paper [29] showed that their attack can break simple PCA-based defence, while our algorithm performs well against the attack of [29] as seen later in the numerical results.

C. Organization
The rest of the paper is organized as follows. The PERT algorithm is described in Section II. The APERT algorithm is described in Section III. Numerical exploration of the proposed algorithm is summarized in Section IV, followed by the conclusion in Section V.
II. STATIC PERTURBATION BASED ALGORITHM
In this section, we propose an adversarial image detection algorithm based on random perturbation of an image in the spectral domain; the algorithm is called PERT. This algorithm is motivated by the two key observations:
1) The authors of [27] found that the injected adversarial noise mainly resides in least signiﬁcant principal components. Intuitively, this makes sense since injecting noise to the most signiﬁcant principal components would lead to detection by human eye. We have applied PCA on CIFAR-10 training data set to learn its principal components sorted by decreasing eigenvalues; the ones with higher eigenvalues are the most signiﬁcant principal components. CIFAR-10 data set consists of 3072 dimensional images, applying PCA on the entire data set yields 3072 principal components. The cumulative explained variance ratio as a function of the number of components (in decreasing order of the eigenvalues) is shown in Figure 2; this ﬁgure shows that most of the variance is concentrated along the ﬁrst few principal components. Hence, least signiﬁcant components do not provide much additional information, and adversarial perturbation of these components should not change the image signiﬁcantly.
2) Several attackers intend push the image close to the decision boundary to fool a classiﬁer [14]. Thus it is possible to detect an adversarial image if we can check whether it is close to a decision boundary or not. Hence, we propose a new scheme for exploring the neighborhood of a given image in spectral domain.
Hence, our algorithm performs PCA on a training data set, and ﬁnds the principal components. When a new test image (potentially adversarial) comes, it projects that image along these principal components, randomly perturbs the projection along a given number of least signiﬁcant components, and then obtains another image from this perturbed spectrum. If the classiﬁer yields same label for this new image and the

original test image, then it is concluded that the original image is most likely not near a decision boundary and hence not an adversarial; else, an alarm is raised for adversarial attack. In fact, multiple perturbed images can be generated by this process, and if the label of the original test image differs with that of at least one perturbed image, an alarm is raised. The intuition behind this is that if an image is adversarial it will lie close to a decision boundary, and perturbation should push it to another region, thus changing the label generated by the classiﬁer.

Algorithm 1: The PERT algorithm

1 Training Phase (PCA): 2 Input: Training image set 3 Output: Principal components of the data set
1) Vectorize the pixel values of all images. 2) Find the sample covariance matrix of these vectors. 3) Perform singular value decomposition (SVD) of the
sample covariance matrix. 4) Obtain the eigenvectors {Φ1, Φ2, · · · , ΦM } arranged
from most signiﬁcant components to least signiﬁcant components.

Test Phase (Perturbation based attack detection):

Initialization: Boolean result = False

Input: Input image x (vectorized), no. of purturbed

image samples to generate T , no. of coefﬁcients

to perturb C

Output: True, if input is adversarial

False, if input is not adversarial

4 Get prediction y for input image x through classiﬁer.

5 Compute the projections (dot products)

< x, Φ1 >, < x, Φ2 >, · · · , < x, ΦM > and vectorize

these M values as xˆ = [xˆ1, xˆ2, · · · , xˆM ].

6 for i = 1 to T do

7 Add realizations of C i.i.d. zero-mean Gaussian

random variables to xˆM−C+1, xˆM−C+2, · · · , xˆM .

This will convert xˆ to xˆ .

8 Get inverse transform of xˆ to get a new image xi.

9 Get prediction yi for image xi through classiﬁer.

10 if y not equal yi then

11

result = True;

12

break;

13 else

14

continue;

15 end

16 end

Discussion: PERT has several advantages over most algorithms in the literature:
1) PERT is basically a pre-processing algorithm for the test image, and hence it is agnostic to the attacker and classiﬁer models.
2) The on-line part of PERT involves computing simple dot products and perturbations, which have very low complexity. PCA can be performed once off-line and used for ever.

However, one should remember that PERT perturbs the least signiﬁcant components randomly, and hence there is no guarantee that a perturbation will be in the right direction to ensure a crossover of the decision boundary. This issue can be resolved by developing more sophisticated perturbation methods using direction search, speciﬁcally in case some knowledge of the decision boundaries is available to the detector. Another option is to create many perturbations of a test image, at the expense of more computational complexity. However, in the next section, we will formulate the sequential version of PERT, which will minimize the mean number of image perturbations per image, under a budget on the missed detection probability and false alarm probability.
III. ADAPTIVE PERTURBATION BASED ALGORITHM
In Section II, the PERT algorithm used up to a constant T number of perturbations of the test image in the spectral domain. However, the major drawback of PERT is that it might be wasteful in terms of computations. If an adversarial image is very close to the decision boundary, then small number of perturbations might be sufﬁcient for detection. On the other hand, if the adversarial image is far away from a decision boundary, then more perturbations will be required to cross the decision boundary with high probability. Also, the PERT algorithm only checks for a decision boundary crossover (hard decision), while many DNNs yield a belief probability vector for the class of a test image (soft output); this soft output of DNNs can be used to improve detector performance and reduce its complexity.
In this section, we propose an adaptive version of PERT called APERT. The APERT algorithm sequentially perturbs the test image in spectral domain. A stopping rule is used by the pre-processing unit to decide when to stop perturbing a test image and declare a decision (adversarial or non-adversarial); this stopping rule is a two-threshold rule motivated by the sequential probability ratio test (SPRT [32]), on top of the decision boundary crossover checking. The threshold values are optimized using the theory of stochastic approximation [31] and SPSA [30].

A. Mathematical formulation
Let us denote the random number of perturbations used in any adaptive technique µ based on random perturbation by N , and let the probabilities of false alarm and missed detection of any randomly chosen test image under this technique be denoted by PF (µ) and PM (µ) respectively. We seek to solve the following constrained problem:
min Eµ(N ) such that PF (µ) ≤ α, PM (µ) ≤ β (CP)
µ
where α ∈ (0, 1) and β ∈ (0, 1) are two constraint values. However, (CP) can be relaxed by using two Lagrange multipliers λ1 ≥ 0, λ2 ≥ 0 to obtain the following unconstrained problem:

min Eµ(N ) + λ1PF (µ) + λ2PM (µ)
µ

(UP)

Let the optimal decision rule for (UP) under (λ1, λ2) be

denoted by µ∗(λ1, λ2). It is well known that, if there ex-

ists λ∗1

≥

0 and λ∗2

≥

0

such

that

P

F

(

µ∗

(λ

∗ 1

,

λ∗2

))

=

α, PM (µ∗(λ∗1, λ∗2)) = β, then µ∗(λ∗1, λ∗2) is an optimal so-

lution for (CP) as well.

Finding out µ∗(λ1, λ2) for a pair (λ1, λ2) is very challenging. Hence, we focus on the class of SPRT-type algorithms

instead. Let us assume that the DNN based classiﬁer generates

a probability value against an input image; this probability is

the belief of the classiﬁer that the image under consideration

is adversarial. Now, suppose that we sequentially perturb an

image in the spectral domain as in PERT, and feed these

perturbed images one by one to the DNN, which acts as

our classiﬁer. Let the DNN return category wise probabilistic

distribution of the image in the form of a vector. We use these vectors to determine qi which indicates the likelihood (not necessarily a probability) of the i-th perturbed image being

adversarial. Motivated by SPRT, the proposed APERT algorithm checks if the ratio |(1−q1)(q11−q2q·2··)q··k·(1−qk)| crosses an upper threshold B or a lower threshold A after the k-th perturbation; an adversarial image is declared if |(1−q1)(q11−q2q·2··)q··k·(1−qk)| > B, a non-adversarial image is declared if |(1−q1)(q11−q2q·2··)q··k·(1−qk)| < A, and the algorithm continues perturbing the image if |(1−q1)(q11−q2q·2··)q··k·(1−qk)| ∈ (A, B). In case k exceeds a predetermined maximum number of perturbations T without any

threshold crossing, the image is declared to be non-adversarial.

Clearly, for given (λ1, λ2), the algorithm needs to compute the optimal threshold values A∗(λ1, λ2) and B∗(λ1, λ2) to minimize the cost in (UP). Also, λ∗1 and λ∗2 need to be com-
puted to meet the constraints in (CP) with equality. APERT

uses two-timescale stochastic approximation and SPSA for

updating the Lagrange multipliers and the threshold values

in the training phase, learns the optimal parameter values, and

uses these parameter values in the test phase.

B. The SRT algorithm for image classiﬁcation
Here we describe an SPRT based algorithm called sequential ration test or SRT for classifying an image x. The algorithm takes A, B, C, T, x, the PCA eigenvectors {Φi}1≤i≤M , and a binary variable Q ∈ {0, 1} as input, and classiﬁes x as adversarial or non-adversarial. This algorithm is used as one important component of the APERT algorithm described later. SRT blends ideas from PERT and the standard SPRT algorithm. However, as seen in the pseudocode of SRT, we use a quantity qj in the threshold testing where qj ∈ (0, 1) cannot be interpreted as a probability. Instead, qj is the normalized value of the p-norm of the difference between outputs y and yj of the DNN against inputs x and its j-th perturbation xj. The binary variable Q is used as a switch; if Q = 1 and if the belief probability vectors y and yj lead to two different predicted categories, then SRT directly declares x to be adversarial. It has been observed numerically that this results in a better adversarial image detection probability, and hence any test image in the proposed APERT scheme later is classiﬁed via SRT with Q = 1.

Algorithm 2: SRT(A, B, C, T, x, {Φi}1≤i≤M , Q) algorithm

Initialization: j = 1, Boolean result = False

Input: Threshold pair (A, B), number of coefﬁcients to

perturb C, maximum number of perturbations T ,

input image x (vectorized), orthonormal basis

vectors {Φi}1≤i≤M (typically obtained from PCA), Switch for Category change detection Q

where Q ∈ {0, 1}

Output: True, if input image is adversarial

False, if input image is not adversarial

1 Get category wise probability classiﬁcation vector y for

input image x through classiﬁer.

2 Compute the projections (dot products)

< x, Φ1 >, < x, Φ2 >, · · · , < x, ΦM > and vectorize these M values as xˆ = [xˆ1, xˆ2, · · · , xˆM ]. 3 while j ≤ T and result = F alse do

4 Add realizations of C i.i.d. zero-mean Gaussian

random variables to xˆM−C+1, xˆM−C+2, · · · , xˆM . This will convert xˆ to xˆ . Get inverse transform of

xˆ to get a new image xj.
5 Get category wise probability classiﬁcation vector yj for input image xj through classiﬁer.
6 Get qj by taking y−Kyj p , where K is the dimension of row vector y and 1 ≤ p

7 if Predicted category changed in perturbed image

and Q = 1 then

8

result = True;

9

break;

10 else if |(1−qq11)q.2.....(q1j−qj )| < A then

11

result = False;

12

break;

13 else if |(1−qq11)q.2.....(q1j−qj )| > B then

14

result = True;

15

break;

16 else

17

j = j + 1 continue;

18 end

19 end

20 return result

C. The APERT algorithm

1) The training phases: The APERT algorithm, designed

for (CP), consists of two training phases and a testing phase.

The ﬁrst training phase simply runs the PCA algorithm. The

second training phase basically runs stochastic approximation iterations to ﬁnd A∗, B∗, λ∗1, λ∗2 so that the false alarm and

missed detection probability constraints are satisﬁed with

equality.

The second training phase of APERT requires three non-

negative sequences a(t), δ(t) and d(t) are chosen such that:

(i) ∞ t=0 a(t) = ∞ t=0 d(t) = ∞, (ii) ∞ t=0 a2(t) < ∞, ∞ t=0 d2(t) < ∞, (iii) limt→∞ δ(t) = 0, (iv) ∞ t=0 aδ22((tt)) <

∞,

(v)

limt→∞

d(t) a(t)

=

0.

The

ﬁrst

two

conditions

are

stan-

dard requirements for stochastic approximation. The third and

fourth conditions are required for convergence of SPSA, and

the ﬁfth condition maintains the necessary timescale separation

explained later.

The APERT algorithm also requires θ which is the percent-

age of adversarial images among all image samples used in the

training phase II. It also maintains two iterates nclean(t) and

nadv(t) to represent the number of clean and images encoun-

tered up to the t-th training image; i.e., nclean(t)+nadv(t) = t.

Steps 5 − 13 of APERT correspond to SPSA which is

basically a stochastic gradient descent scheme with noisy

estimate of gradient, used for minimizing the objective of

(UP) over A and B for current (λ1(t), λ2(t)) iterates. SPSA

allows us to compute a noisy gradient of the objective of

(CP) by randomly and simultaneously perturbing (A(t), B(t))

in two opposite directions and obtaining the noisy estimate

of gradient from the difference in the objective function

evaluated at these two perturbed values; this allows us to avoid

coordinate-wise perturbation in gradient estimation. In has to

be noted that, the cost to be optimized by SPSA has to be

obtained from SRT. The A(t) and B(t) iterates are projected

onto non-overlapping compact intervals [Amin, Amax] and

[Bmin, Bmax] (with Amax < Bmin) to ensure boundedness. Steps 14 − 15 are used to ﬁnd λ∗1 and λ∗2 via stochastic

approximation in a slower timescale. In has to be noted that,

since

limt→∞

d(t) a(t)

=

0,

we

have

a

two-timescale

stochastic

ap-

proximation [31] where the Lagrange multipliers are updated

in a slower timescale and the threshold values are updated via

SPSA in a faster timescale. The faster timescale iterates view

the slower timescale iterates as quasi-static, while the slower-

timescale iterates view the faster timescale iterates as almost

equilibriated; as if, the slower timescale iterates vary in an

outer loop and the faster timescale iterates vary in an inner

loop. It has to be noted that, though standard two-timescale

stochastic approximation theory guarantees some convergence

under suitable conditions [31], here we cannot provide any

convergence guarantee of the iterates due to the lack of

established statistical properties of the images. It is also noted

that, λ1(t) and λ2(t) are updated at different time instants; this

corresponds to asynchronous stochastic approximation [31].

The λ1(t) and λ2(t) iterates are projected onto [0, ∞) to

ensure non-negativity. Intuitively, if a false alarm is observed,

the cost of false alarm, λ1(t) is increased. Similarly, if a

missed detection is observed, then the cost of missed detection,

λ2(t), is increased, else it is decreased. Ideally, the goal is to reach to a pair (λ∗1, λ∗2) so that the constraints in (CP) are met

with equality, through we do not have any formal convergence

proof.

2) Testing phase: The testing phase just uses SRT with

Q = 1 for any test image. Since Q = 1, a test image

bypasses the threshold testing and is declared adversarial, in

case the random perturbation results in predicted category

change of the test image. It has been numerically observed

(see Section IV) that this results in a small increase in false

alarm rate but a high increase in adversarial image detection

rate compared to Q = 0. However, one has the liberty to

avoid this and only use the threshold test in SRT by setting

Q = 0. Alternatively, one can set a slightly smaller value of α

in APERT with Q = 1 in order to compensate for the increase

in false alarm.
IV. EXPERIMENTS
A. Performance of PERT
We evaluated our proposed algorithm on CIFAR-10 data set and the classiﬁer of [11] implemented in a challenge to explore adversarial robustness of neural networks (see [33]).2 Each image in CIFAR-10 has 32 × 32 pixels, where each pixel has three channels: red, green, blue. Hence, PCA provides M = 32 × 32 × 3 = 3072 orthonormal basis vectors. CIFAR10 has 60000 images, out of which 50000 images were used for PCA based training and rest of the images were used for evaluating the performance of the algorithm.
Table I shows the variation of detection probability (percentage of detected adversarial images) for adversarial images generated using various attacks, for number of components C = 1000 and various values for maximum possible number of samples T (number of perturbations for a given image). Due to huge computational requirement in generating adversarial images via black box attack, we have considered only four white box attacks. It is evident that the attack detection probability (percentage) increases with T ; this is intuitive since larger T results in a higher probability of decision boundary crossover if an adversarial image is perturbed. The second column of Table I denotes the percentage of clean images that were declared to be adversarial by our algorithm, i.e., it contains the false alarm probabilities which also increase with T . However, we observe that our pre-processing algorithm achieves very low false alarm probability and high attack detection probability under these four popular white box attacks. This conclusion is further reinforced in Table II, which shows the variation in detection performance with varying C, for T = 10 and T = 20. It is to be noted that the detection probability under the detection algorithm of [22] are 56.7% and 72.9% for CW (L2) and FGSM attacks; clearly our detection algorithm outperforms [22] while having low computation. The last column of Table II suggests that there is an optimal value of C, since perturbation along more principal components may increase the decision boundary crossover probability but at the same time can modify the information along some most signiﬁcant components as well.

B. Performance of APERT

For APERT, we initialize A(0) = 10−10, B(0) =

0.5, λ1(0) = λ2(0) = 10, and choose step sizes a(t) =

O

(

1 t0.7

),

d(t)

=

O( 1t ), δ(t)

=

O

(

1 t0.1

).

The

Foolbox

library

was used to craft adversarial examples. The classiﬁcation

neural network is taken from [33]

2-norm is used to obtain the qi values since it was observed

that 2-norm outperforms 1-norm. In the training process, 50%

of the training images were clean and 50% images were

adversarial.

2Codes for our numerical experiments are available in [34] and [35]. We used Foolbox library [36] for generating adversarial images. PCA was performed using Scikit-learn [37] library in Python; this library allows us to customize the computational complexity and accuracy in PCA.

Algorithm 3: The APERT algorithm

1 Training Phase I (PCA): 2 Input: Training image set

3 Output: Principal components of the data set

1) Vectorize the pixel values of all images.

2) Find the sample covariance matrix of these vectors.

3) Perform singular value decomposition (SVD) of the sample

covariance matrix. 4) Obtain the eigenvectors {Φ1, Φ2, · · · , ΦM } arranged from
most signiﬁcant components to least signiﬁcant components.

Training Phase II (Determining A and B) Initialization: nclean(0) = nadv(0) = 0, non-negative
0 < A(0) < B(0), λ1(0), λ2(0) Input: θ, training image set with each image in vectorized
form, number of training images nmax, no. of coefﬁcients to perturb C, α, β, sequences {a(t), δ(t), d(t)}t≥0, maximum number of perturbations Nmax, range of accepted values of the thresholds [Amin, Amax], [Bmin, Bmax] such that 0 < Amin < Amax < Bmin < Bmax Output: Final Values of (A∗, B∗) (and also λ∗1 and λ∗2 which
are not used in the test phase) 4 for t ← 0 to nmax − 1 do 5 Randomly generate b1(t) ∈ {−1, 1} and b2(t) ∈ {−1, 1}
with probability 0.5 6 Compute A (t) = A(t) + δ(t)b1(t) and
B (t) = B(t) + δ(t)b2(t), similarly A (t) = A(t) − δ(t)b1(t) and B (t) = B(t) − δ(t)b2(t). 7 Randomly pick the training image xt (can be adversarial with probability θ/100)

8 If image is actually adversarial then nadv(t) = nadv(t − 1) + 1, If image is clean then nclean(t) = nclean(t − 1) + 1
9 Deﬁne I{miss} and I{f alsealarm} as an indicator of missed detection and false alarm respectively by SRT algorithm and I{cleanimage} as indicator of a non adversarial image

10 Compute cost

c (t) = N (t) + λ1(t)I{f alsealarm} + λ2(t)I{miss}

by using SRT (A (t), B (t), C, T, xt, {Φi}1≤i≤M , 0)

11 Compute cost

c (t) = N (t) + λ1(t)I{f alsealarm} + λ2(t)I{miss}

by using SRT (A (t), B (t), C, T, xt, {Φi}1≤i≤M , 0)

12 Update A(t + 1) = [A(t) − a(t) × c2(bt1)(−t)cδ((tt)) ] and then project A(t + 1) on [Amin, Amax]

13 Update B(t + 1) = [B(t) − a(t) × c2(bt2)(−t)cδ((tt)) ] and then project B(t + 1) on [Bmin, Bmax]

14 Again Determine I{f alsealarm} and I{miss} from

SRT(A(t + 1), B(t + 1), C, T, xt, {Φi}1≤i≤M , 0)

15 Update λ1(t + 1) = [λ1(t) + I{cleanimage}d(nclean(t)) × (I{f alsealarm} − α)]∞ 0 ,

λ2(t + 1) = [λ2(t) + I{adversarialimage}d(nadv(t)) × (I{miss} − β)]∞ 0

16 end

∗.

∗.

17 return (A = A(nmax), B = B(nmax))

18 Testing Phase:

Initialization: Boolean result = False

Input: Input image x (vectorized), maximum number of

perturbed image samples to generate T , no. of

coefﬁcients to perturb C, lower threshold A, upper

threshold B

Output: True, if input image is adversarial

False, if input image is not adversarial

19 return SRT(A, B, C, T, x, {Φi}1≤i≤M , 1)

Table I DETECTION AND FALSE ALARM PERFORMANCE OF PERT ALGORITHM
FOR VARIOUS VALUES OF T .

No. of Samples T
05 10 15 20 25 aClean images

Cleana 1.2 1.5 1.7 1.9 1.9
that are

Percentage Detection (%)

FGSM L-BFGS PGD

50.02

89.16 55.03

63.53

92.50 65.08

69.41

93.33 67.45

73.53

95.03 71.01

75.29

95.03 75.14

detected as adversarial

CW(L2) 96.47 98.23 99.41 99.41 100.00

Table II DETECTION AND FALSE ALARM PERFORMANCE OF PERT ALGORITHM
FOR VARIOUS VALUES OF C .

No. of Coefﬁcients C

Cleana

Percentage Detection (%) FGSM L-BFGS PGD

No. of Samples(T ): 10

0500

1.20

58.23

90.83 57.40

1000

1.50

69.41

93.33 60.95

1500

2.10

64.11

91.67 61.53

No. of Samples(T ): 20

0500

1.20

68.23

93.33 68.05

1000

1.90

74.11

94.16 70.41

1500

2.50

71.18

95.00

aClean images that are detected as adversarial

71.00

CW(L2)
95.90 95.45 95.00
95.90 95.90 95.00

Though there is no theoretical convergence guarantee for APERT, we have numerically observed convergence of A(t), B(t), λ1(t) and λ2(t)
1) Computational complexity of PERT and APERT: We note that, a major source of computational complexity in PERT and APERT is perturbing an image and passing it through a classiﬁer. In Table III and Table IV, we numerically compare the mean number of perturbations required for PERT and APERT under Q = 1 and Q = 0 respectively. The classiﬁcation neural network was taken from [33].
Table III and Table IV show that the APERT algorithm requires much less perturbations compared to PERT for almost similar detection performance, for various attack algorithms and various test images that result in false alarm, adversarial image detection, missed detection and (correctly) clean image detection. It is also noted that, for the images resulting in missed detection and clean image detection, PERT has to exhaust all T = 25 perturbation options before stopping. As a result, the mean number of perturbations in APERT becomes signiﬁcantly smaller than PERT; see Table V. The key reason behind smaller number of perturbations in APERT is the fact that APERT uses a doubly-threshold stopping rule motivated by the popular SPRT algorithm in detection theory. It is also observed that APERT with Q = 1 in the testing phase has slightly lower computaional complexity than APERT with Q = 0, since APERT with Q = 1 has an additional ﬂexibility of stopping the perturbation if there is a change in predicted category.
We also implemented a Gaussian process regression based detector (GPRBD) from [38] (not sequential in nature) which uses the neural network classiﬁer of [33], tested it against our adversarial examples, and compared its runtime against that of PERT and APERT equipped with the neural network

Figure 3. ROC plot comparison of PERT, APERT with Q = 0, APERT with Q = 1 and GPRBD detection algorithms for various attack schemes. Top left: CW attack. Top right: LBFGS attack. Bottom left: FGSM attack. Bottom right: PGD attack.

Table III MEAN NUMBER OF SAMPLES GENERATED FOR PERT AND APERT ALGORITHM.HERE PERT AND APERT’S PARAMETER WERE SET IN A
WAY THAT THEY BRING THEIR FALSE ALARM PERFORMANCE CLOSEST TO
EACH OTHER FOR Q = 1 IN TESTING PHASE FOR APERT

Attack Type
CW (L2) LBFGS FGSM
PGD Attack Type
CW (L2) LBFGS FGSM
PGD

False

Alarm

PERT APERT

9.76

1.17

11.86

1.42

1.68

1.08

14.12

1.15

False

Alarm Probability

PERT APERT

4.56

5.12

4.85

5.24

5.41

5.88

4.01

4.51

Mean Number of Samples Generated

Detected

Missed

Adversarial

Detection

PERT

APERT

PERT APERT

1.19

1.02

25

4.09

1.87

1.07

25

4.97

4.97

1.07

25

5.08

4.87

1.41

25

5.47

Corresponding Detection performance %

Detected

Missed

Adversarial Probability Detection Probability

PERT

APERT

PERT APERT

97.10

98.10

2.90

1.90

96.3

94.35

3.7

5.65

79.31

87.64

20.69

12.36

83.99

84.45

16.01

15.55

Detected

Clean

PERT APERT

25

2.37

25

3.41

25

2.97

25

3.03

Detected Clean Probability PERT APERT 95.44 94.88 95.15 94.76 94.59 94.12 95.99 95.49

Table IV MEAN NUMBER OF SAMPLES GENERATED FOR PERT AND APERT ALGORITHM.HERE APERT’S PARAMETERS WERE SET IN A WAY THAT THEY BRING THE FALSE ALARM PERFORMANCE OF APERT CLOSEST TO CORRESPONDING PERT’S FALSE ALARM PERFORMANCE IN TABLE III
FOR Q = 0 IN TESTING PHASE FOR APERT

Attack Type
CW (L2) LBFGS FGSM
PGD Attack Type
CW (L2) LBFGS FGSM
PGD

False

Alarm

PERT APERT

9.76

1.0

11.86

1.02

1.68

1.05

14.12

1.07

False

Alarm Probability

PERT APERT

4.56

5.45

4.85

5.97

5.41

6.47

4.01

5.12

Mean Number of Samples Generated

Detected

Missed

Adversarial

Detection

PERT

APERT

PERT APERT

1.19

1.0

25

6.11

1.87

1.0

25

6.07

4.97

1.02

25

6.11

4.87

1.21

25

6.037

Corresponding Detection performance %

Detected

Missed

Adversarial Probability Detection Probability

PERT

APERT

PERT APERT

97.10

84.09

2.90

15.91

96.3

80.57

3.7

19.43

79.31

65.29

20.69

34.71

83.99

65.49

16.01

34.51

Detected

Clean

PERT APERT

25

2.96

25

3.28

25

3.10

25

3.05

Detected Clean Probability PERT APERT 95.44 94.55 95.15 94.03 94.59 93.53 95.99 94.88

classiﬁer of [33]. These experiments were run under the same colab runtime environment, in a single session. The runtine speciﬁcations are- CPU Model name: Intel(R) Xeon(R) CPU @ 2.30GHz, Socket(s): 1, Core(s) per socket: 1, Thread(s) per core: 2, L3 cache: 46080K, CPU MHz: 2300.000, RAM available: 12.4 GB, Disk Space Available: 71 GB. Table VI shows that, APERT has signiﬁcantly smaller runtime than PERT as expected, and slightly larger runtime than GPRBD. Also, APERT with Q = 1 has smaller runtime than Q = 0.

2) Performance of PERT and APERT: In Figure 3, we compare the ROC (receiver operating characteristic) plots of PERT, APERT and GPRBD algorithms, all implemented with the same neural network classiﬁer of [33]. The Gaussian model used for GPRBD was implemented using [39] with the Kernel parameters set as follows: input dimensions = 10, variance = 1 and length scale = 0.01 as in [38]. The Gaussian model parameter optimization was done using LBFGS with max

Table V MEAN NUMBER OF SAMPLES GENERATED FOR T = 25 AND C = 1000 FOR
APERT AND PERT ALGORITHM OVER A DATASET WITH 50% AVERSARIAL IMAGES AND 50% CLEAN IMAGES

Attack
CW (L2) LBFGS FGSM
PGD

Mean Number of Samples Generated

PERT

APERT

Q=1

Q=0

13.3 1.85

1.92

13.5 2.19

2.19

15.5 2.14

2.56

14.48 2.20

2.57

Table VI PERFORMANCE OF OUR IMPLEMENTATION OF GAUSSIAN PROCESS REGRESSION BASED DETECTOR(GPRBD) VS OUR APERT ALGORITHM
FOR T = 25, C = 1000

Attack
CW (L2) LBFGS FGSM
PGD

Average Time Taken per Image (seconds)

GPRBD

APERT

PERT

Q=1 Q=0

0.2829 0.6074 0.6398 4.1257

0.2560 0.6982 0.7059 4.7895

0.2728 0.6372 0.7801 4.6421

0.2694 0.6475 0.7789 4.4216

iterations = 1000. It is obvious from Figure 3 that, for the same false alarm probability, APERT has higher or almost same attack detection rate compared to PERT. Also, APERT and PERT signiﬁcantly outperform GPRBD. Hence, APERT yields a good compromise between ROC performance and computational complexity. It is also observed that APERT with Q = 1 always has a better ROC curve than APERT with Q = 0 in the testing phase.
Table VII and Table VIII show that the false alarm probability and attack detection probability of APERT increases with C for a ﬁxed T , for both Q = 1 and Q = 0. As C increases, more least signiﬁcant components are perturbed in the spectral domain, resulting in a higher probability of decision boundary crossover.
Table VII VARIATION IN PERFORMANCE OF APERT WITH VALUES OF C USING SECOND NORM FOR T = 10 AND T = 20 FOR Q = 1 IN TESTING PHASE

Attack Type
CW (L2) LBFGS FGSM
PGD
CW (L2) LBFGS FGSM
PGD

C False Alarm
5.0 4.16 2.94 2.36
1.01 2.5 2.94 1.77

= 500 Detection Probability No. 96.81 94.16 62.35 63.9 No. 95.46 90.83 57.64 60.9

Percentage Detection(%)

C = 1000

False Detection

Alarm Probability

of Samples(T ): 10

8.18

98.18

10.0

93.33

4.70

80.0

5.91

77.54

of Samples(T ): 20

5.45

96.81

9.16

94.16

4.70

79.41

4.14

79.88

C = 1500 False Detection Alarm Probability

11.36 12.5 5.88 7.10

99.09 97.5 94.70 88.16

6.36

98.18

15.0

96.66

7.64

90.58

9.46

88.757

V. CONCLUSION
In this paper, we have proposed two novel pre-processing schemes for detection of adversarial images, via a combination of PCA-based spectral decomposition, random perturbation,

Table VIII VARIATION IN PERFORMANCE OF APERT WITH VALUES OF C USING SECOND NORM FOR T = 10 AND T = 20, FOR Q = 0 IN TESTING PHASE

Attack Type
CW (L2) LBFGS FGSM
PGD
CW (L2) LBFGS FGSM
PGD

C False Alarm
3.18 6.66 2.35 2.36
5.45 7.5 3.529 2.95

= 500 Detection Probability No. 89.09 81.66 50.0 39.05 No. 84.09 85.0 44.70 40.82

Percentage Detection(%)

C = 1000

False Detection

Alarm Probability

of Samples(T ): 10

8.18

91.81

15.0

85.83

7.05

67.65

6.5

68.63

of Samples(T ): 20

8.18

91.81

12.5

93.33

6.47

65.29

7.10

67.45

C = 1500 False Detection Alarm Probability

12.72 19.16 8.23 9.46

93.63 96.66 85.88 82.84

11.81 17.5 7.05 10.0

93.18 94.166 83.53 85.21

SPSA and two-timescale stochastic approximation. The pro-
posed schemes have reasonably low computational complexity
and are agnostic to attacker and classiﬁer models. Numerical
results on detection and false alarm probabilities demonstrate
the efﬁcacy of the proposed algorithms, despite having low
computational complexity. We will extend this work for de-
tection of black box attacks in our future research endeavour.
REFERENCES
[1] Kartik Mundra, Rahul Modpur, Arpan Chattopadhyay, and Indra Narayan Kar. Adversarial image detection in cyber-physical systems. In Proceedings of the 1st ACM Workshop on Autonomous and Intelligent Mobile Systems, pages 1–5, 2020.
[2] Naveed Akhtar and Ajmal Mian. Threat of adversarial attacks on deep learning in computer vision: A survey. IEEE Access, 6:14410–14430, 2018.
[3] Kevin Eykholt, Ivan Evtimov, Earlence Fernandes, Bo Li, Amir Rahmati, Chaowei Xiao, Atul Prakash, Tadayoshi Kohno, and Dawn Song. Robust physical-world attacks on deep learning models. arXiv preprint arXiv:1707.08945, 2017.
[4] Alesia Chernikova, Alina Oprea, Cristina Nita-Rotaru, and BaekGyu Kim. Are self-driving cars secure? evasion attacks against deep neural networks for steering angle prediction. arXiv preprint arXiv:1904.07370, 2019.
[5] Arpan Chattopadhyay and Urbashi Mitra. Security against false data injection attack in cyber-physical systems. IEEE Transactions on Control of Network Systems, 2019.
[6] Arpan Chattopadhyay, Urbashi Mitra, and Erik G Ström. Secure estimation in v2x networks with injection and packet drop attacks. In 2018 15th International Symposium on Wireless Communication Systems (ISWCS), pages 1–6. IEEE, 2018.
[7] Arpan Chattopadhyay and Urbashi Mitra. Attack detection and secure estimation under false data injection attack in cyber-physical systems. In 2018 52nd Annual Conference on Information Sciences and Systems (CISS), pages 1–6. IEEE, 2018.
[8] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.
[9] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572, 2014.
[10] Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In 2017 IEEE Symposium on Security and Privacy (SP), pages 39–57. IEEE, 2017.
[11] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083, 2017.
[12] Nicolas Papernot, Patrick McDaniel, Somesh Jha, Matt Fredrikson, Z Berkay Celik, and Ananthram Swami. The limitations of deep learning in adversarial settings. In 2016 IEEE European Symposium on Security and Privacy (EuroS&P), pages 372–387. IEEE, 2016.
[13] Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial machine learning at scale. arXiv preprint arXiv:1611.01236, 2016.

[14] Wieland Brendel, Jonas Rauber, and Matthias Bethge. Decision-based

adversarial attacks: Reliable attacks against black-box machine learning

models. arXiv preprint arXiv:1712.04248, 2017.

[15] Nicholas Carlini and David Wagner. Audio adversarial examples:

Targeted attacks on speech-to-text. In 2018 IEEE Security and Privacy

Workshops (SPW), pages 1–7. IEEE, 2018.

[16] Xiaoyong Yuan, Pan He, Qile Zhu, and Xiaolin Li. Adversarial

examples: Attacks and defenses for deep learning. IEEE transactions

on neural networks and learning systems, 2019.

[17] Cihang Xie, Yuxin Wu, Laurens van der Maaten, Alan L Yuille, and

Kaiming He. Feature denoising for improving adversarial robustness. In

Proceedings of the IEEE Conference on Computer Vision and Pattern

Recognition, pages 501–509, 2019.

[18] Nicolas Papernot, Patrick McDaniel, Xi Wu, Somesh Jha, and Anan-

thram Swami. Distillation as a defense to adversarial perturbations

against deep neural networks. In 2016 IEEE Symposium on Security

and Privacy (SP), pages 582–597. IEEE, 2016.

[19] Reuben Feinman, Ryan R Curtin, Saurabh Shintre, and Andrew B

Gardner. Detecting adversarial samples from artifacts. arXiv preprint

arXiv:1703.00410, 2017.

[20] Yang Song, Taesup Kim, Sebastian Nowozin, Stefano Ermon, and Nate

Kushman. Pixeldefend: Leveraging generative models to understand and

defend against adversarial examples. arXiv preprint arXiv:1710.10766,

2017.

[21] Tim Salimans, Andrej Karpathy, Xi Chen, and Diederik P Kingma.

Pixelcnn++: Improving the pixelcnn with discretized logistic mixture

likelihood and other modiﬁcations. arXiv preprint arXiv:1701.05517,

2017.

[22] Jingyi Wang, Jun Sun, Peixin Zhang, and Xinyu Wang. Detecting

adversarial samples for deep neural networks through mutation testing.

arXiv preprint arXiv:1805.05010, 2018.

[23] Jimmy Ba and Rich Caruana. Do deep nets really need to be deep? In

Advances in neural information processing systems, pages 2654–2662,

2014.

[24] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge

in a neural network. arXiv preprint arXiv:1503.02531, 2015.

[25] Nicholas Carlini and David Wagner. Defensive distillation is not robust

to adversarial examples. arXiv preprint arXiv:1607.04311, 2016.

[26] Florian Tramèr, Alexey Kurakin, Nicolas Papernot, Ian Goodfellow, Dan

Boneh, and Patrick McDaniel. Ensemble adversarial training: Attacks

and defenses. arXiv preprint arXiv:1705.07204, 2017.

[27] Dan Hendrycks and Kevin Gimpel. Early methods for detecting

adversarial images. arXiv preprint arXiv:1608.00530, 2016.

[28] Davis Liang, Patrick Hayes, and Alric Althoff. Deep adversarial

robustness. 2017.

[29] Nicholas Carlini and David Wagner. Adversarial examples are not

easily detected: Bypassing ten detection methods. In Proceedings of

the 10th ACM Workshop on Artiﬁcial Intelligence and Security, pages

3–14. ACM, 2017.

[30] James C Spall et al. Multivariate stochastic approximation using a

simultaneous perturbation gradient approximation. IEEE transactions

on automatic control, 37(3):332–341, 1992.

[31] Vivek S Borkar. Stochastic approximation: a dynamical systems view-

point, volume 48. Springer, 2009.

[32] H Vincent Poor. An introduction to signal detection and estimation.

Springer Science & Business Media, 2013.

[33] https:// github.com/ bethgelab/ cifar10_challenge.

[34] Rahul Modpur and Kartik Mundra. https:// github.com/ rmodpur/

PCA-based-Adversarial-Image-Detector.

[35] Darpan Kumar Yadav.

https:// github.com/ darpan17/

Efﬁcient-Adversarial-Image-Detection.

[36] Jonas Rauber, Wieland Brendel, and Matthias Bethge. Foolbox: A

python toolbox to benchmark the robustness of machine learning models.

arXiv preprint arXiv:1707.04131, 2017.

[37] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion,

O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vander-

plas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duch-

esnay. Scikit-learn: Machine learning in Python. Journal of Machine

Learning Research, 12:2825–2830, 2011.

[38] Sangheon Lee, Noo-ri Kim, Youngwha Cho, Jae-Young Choi, Suntae

Kim, Jeong-Ah Kim, and Jee-Hyong Lee. Adversarial detection with

gaussian process regression-based detector. TIIS, 13(8):4285–4299,

2019.

[39] GPy. GPy: A gaussian process framework in python. http://github.com/

ShefﬁeldML/GPy, since 2012.

