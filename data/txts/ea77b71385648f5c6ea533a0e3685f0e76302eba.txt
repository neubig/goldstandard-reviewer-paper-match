A Little Annotation does a Lot of Good: A Study in Bootstrapping Low-resource Named Entity Recognizers
Aditi Chaudhary, Jiateng Xie, Zaid Sheikh, Graham Neubig, Jaime G. Carbonell
{aschaudh, jiatengx, zsheikh, gneubig, jgc}@cs.cmu.edu Language Technologies Institute Carnegie Mellon University

arXiv:1908.08983v1 [cs.CL] 23 Aug 2019

Abstract
Most state-of-the-art models for named entity recognition (NER) rely on the availability of large amounts of labeled data, making them challenging to extend to new, lowerresourced languages. However, there are now several proposed approaches involving either cross-lingual transfer learning, which learns from other highly resourced languages, or active learning, which efﬁciently selects effective training data based on model predictions. This paper poses the question: given this recent progress, and limited human annotation, what is the most effective method for efﬁciently creating high-quality entity recognizers in under-resourced languages? Based on extensive experimentation using both simulated and real human annotation, we ﬁnd a dualstrategy approach best, starting with a crosslingual transferred model, then performing targeted annotation of only uncertain entity spans in the target language, minimizing annotator effort. Results demonstrate that cross-lingual transfer is a powerful tool when very little data can be annotated, but an entity-targeted annotation strategy can achieve competitive accuracy quickly, with just one-tenth of training data. The code is publicly available here.1
1 Introduction
Named entity recognition (NER) is the task of detecting and classifying named entities in text into a ﬁxed set of pre-deﬁned categories (person, location, etc.) with several downstream applications including machine reading (Chen et al., 2017), entity and event co-reference (Yang and Mitchell, 2016), and text mining (Han and Sun, 2012). Recent advances in deep learning have yielded stateof-the-art performance on many sequence labeling tasks, including NER (Collobert et al., 2011;
1https://github.com/Aditi138/ EntityTargetedActiveLearning

Ma and Hovy, 2016; Lample et al., 2016; Peters et al., 2018). However, the performance of these models is highly dependent on the availability of large amounts of annotated data, and as a result their accuracy is signiﬁcantly lower on languages that have fewer resources than English. In this work, we ask the question “how can we efﬁciently bootstrap a high-quality named entity recognizer for a low-resource language with only a small amount of human effort?” Speciﬁcally, we leverage recent advances in data-efﬁcient learning for low-resource languages, proposing the following “recipe” for bootstrapping low-resource entity recognizers: First, we use cross-lingual transfer learning (Yarowsky et al., 2001; Ammar et al., 2016), which applies a model trained on another language to low-resource languages, to provide a good preliminary model to start the bootstrapping process. Speciﬁcally, we use the model of Xie et al. (2018), which reports strong results on a number of language pairs. Next, on top of this transferred model we further employ active learning (Settles and Craven, 2008; Marcheggiani and Artieres, 2014), which helps improve annotation efﬁciency by using model predictions to select informative, rather than random, data for human annotators. Finally, the model is ﬁne-tuned on data obtained using active learning to improve accuracy in the target language.
Within this recipe, the choice of speciﬁc method for choosing and annotating data within active learning is highly important to minimize human effort. One relatively standard method used in previous work on NER is to select full sequences based on a criterion for the uncertainty of the entities recognized therein (Culotta and McCallum, 2005). However, as it is often the case that only a single entity within the sentence may be of interest, it can still be tedious and wasteful to annotate full sequences when only a small portion of the

train model

NER Model

ﬁtted model

ू ल और श क क कमी पर सु ीम कोट नेमांगा जवाब
BORGIORG

Cross-Lingual Transfer Learning
English labeled dataset

label
Target Language labeled dataset

query spans

Target language unlabeled dataset
ू ल और श क क कमी पर सु ीम कोट नेमांगा जवाब

Active Learning

Figure 1: Our proposed recipe: cross-lingual transfer is used for projecting annotations from an English labeled dataset to the target language. Entity-targeted active learning is then used to select informative sub-spans which are likely entities for humans to annotate. Finally, the NER model is ﬁne-tuned on this partially-labeled dataset.

sentence is of interest (Neubig et al., 2011; Sperber et al., 2014). Inspired by this ﬁnding and considering the fact that named entities are both important and sparse, we propose an entity-targeted strategy to save annotator effort. Speciﬁcally, we select uncertain subspans of tokens within a sequence that are most likely named entities. This way, the annotators only need to assign types to the chosen subspans without having to read and annotate the full sequence. To cope with the resulting partial annotation of sequences, we apply a constrained version of conditional random ﬁelds (CRFs), partial CRFs, during training that only learn from the annotated subspans (Tsuboi et al., 2008; Wanvarie et al., 2011).
To evaluate our proposed methods, we conducted simulated active learning experiments on 5 languages: Spanish, Dutch, German, Hindi and Indonesian. Additionally, to study our method in a more practical setting, we conduct human annotation experiments on two low-resource languages, Indonesian and Hindi, and one simulated low-resource language, Spanish. In sum, this paper makes the following contributions:
1. We present a bootstrapping recipe for improving low-resource NER. With just onetenth of tokens annotated, our proposed entity-targeted active learning method provides the best results among all active learning baselines, with an average improvement of 9.9 F1.
2. Through simulated experiments, we show that cross-lingual transfer is a powerful tool, outperforming the un-transferred systems by an average of 8.6 F1 with only one-tenth of tokens annotated.

3. Human annotation experiments show that annotators are more accurate in annotating entities when using the entity-targeted strategy as opposed to full sequence annotation. Moreover, this strategy minimizes annotator effort by requiring them to label fewer tokens than the full-sequence annotation.
2 Approach
As noted in the introduction, our bootstrapping recipe consists of three components (1) crosslingual transfer learning, (2) active learning to select relevant parts of the data to annotate, and (3) ﬁne-tuning of the model on these annotated segments. Steps (2) and (3) are continued until the model has achieved an acceptable level of accuracy, or until we have exhausted our annotation budget. The system overview can be seen in Figure 1. In the following sections, we describe each of these three steps in detail.
2.1 Cross-lingual Transfer Learning
The goal of cross-lingual learning is to take a recognizer trained in a source language, and transfer it to a target language. Our approach to doing so for NER follows that of Xie et al. (2018), and we provide a brief review in this section.
To begin with, we assume access to two sets of pre-trained monolingual word embeddings in the source and target languages, X and Y , one small bilingual lexicon, either provided or obtained in an unsupervised manner (Artetxe et al., 2017; Conneau et al., 2017a), and labeled training data in the source language. Using these resources, we train bilingual word embeddings (BWE) to create a word-to-word translation dictionary, and ﬁnally

use this dictionary to translate the source training data into the target language, which we use to train an NER model.
To learn BWE, we ﬁrst obtain a linear mapping W by solving the following objective:

W ∗ = arg min W XD − YD F s.t. W W = I,
W

where XD and YD correspond to the aligned word

embeddings from the bilingual lexicon. F denotes

the Frobenius norm. We can ﬁrst compute the sin-

gular

value

decomposition

Y

T D

XD

=

U

V,

and solve the objective by taking W ∗ = U V . We

obtain BWE by linearly transforming the source

and target monolingual word embeddings with U

and V , namely XU and Y V .

After obtaining the BWE, we ﬁnd the nearest

neighbor target word for every source word in

the BWE space using the cross-domain similar-

ity local scaling (CSLS) metric (Conneau et al.,

2017b), which produces a word-to-word transla-

tion dictionary. We use this dictionary to translate

the source training data into the target language,

and simply copy the label for each word, which

yields transferred training data in the target lan-

guage. We train an NER model on this transferred

data as our preliminary model. Going forward, we

refer to the use of cross-lingual transferred data as

CT.

2.2 Entity-Targeted Active Learning

After training a model using cross-lingual transfer
learning, we start the active learning process based
on this model’s outputs. We begin by training a NER model Θ using the above model’s outputs as
training data. Using this trained model, our pro-
posed entity-targeted active learning strategy, re-
ferred as ETAL, then selects the most informative spans from a corpus D of unlabeled sequences. Given an unlabeled sequence s, ETAL ﬁrst selects a span of tokens sji = si · · · sj such that sji is a likely named entity, where i, j ∈ [0, |s|]. Then, in order to obtain highly informative spans across D, ETAL computes the entropy H for each occurrence of the span sji in D and then aggregates them over the entire corpus D, given by:

Haggregate(sji ) =

H(xji )1(xji = sji )

xji ∈D

where x is an unlabeled sequence in D. Finally, the spans sji with the highest aggregate uncertainty
Haggregate are selected for manual annotation.

We now describe the procedure for calculating H(xji ), which is the entropy of a span xji being a likely entity. Given an unlabeled sequence x,
the trained NER model Θ is used for computing
the marginal probabilities pθ(yi|x) for each token xi across all possible labels yi ∈ Y using the forward-backward algorithm (Rabiner, 1989),
where Y is the set of all labels. Using these
marginals we calculate the entropy of a given span xji being an entity as shown in Algorithm 1.

Algorithm 1 Entity-Targeted Active Learning

1: B ← label-set denoting beginning of an entity

2: I ← label-set denoting inside of an entity

3: O ← outside of an entity span

4: pθ(yi|x) ← marginal probability of label yi

5: for token xi

6:

7: for i ← 1...len(x), j = 1 do

8: pisjpan = y∈B pθ(yi|x)

9: for j ← i + 1...len(x) do

10:

pentity = pisjpan ∗ pθ(Oj |x)

11:

H = entropy (pentity)

12:

if H > Hthreshold then

13:

Haggregate(xji )+ = H

14:

pisjpan = pisjpan ∗ y∈I pθ(yj |x)

Let B denote the set of labels indicating beginning of an entity, I the set of labels indicating inside of an entity and O denoting outside of an en-
tity. First, we compute the probability of a span xji being an entity, starting with the token i, by marginalizing pθ(yi|x) over all labels in B, denoted as pisjpan. Since an entity can span multiple tokens, for each subsequent token j being part of that entity, we marginalize pθ(yj|x) over all labels in I and combine it with pisjpan. Finally, we compute pentity = pisjpan ∗ pθ(Oj|x), which denotes end of a likely entity. Since we use the marginal probability for computing pentity, it already factors in the transition probability between tags.
Thus, any invalid sequences such as BPERIORG
have low scores. Since contiguous spans have
overlapping tokens, using dynamic programming (DP) to compute pisjpan avoids an exponential computation when considering all possible spans in a sequence. Using pentity, we compute the entropy H and only consider the spans having H higher than a pre-deﬁned threshold Hthreshold. The reason for this thresholding is purely for computa-
tional purposes as it allows us to discard all spans

that have a very low probability of being an entity, keeping the number of spans actually stored in memory low. As mentioned above, we aggregate the entropy of spans Haggregate over the entire unlabeled set, thus combining uncertainty sampling with a bias towards high frequency entities.
Using this strategy, we select subspans in each sequence for annotation. The annotator only needs to assign named entity types to the chosen subspans, adjust the span boundary if needed, and ignore the rest of the sequence, saving much effort.

2.3 Training the NER model
With the newly obtained training data from active learning, we attempt to improve the original transferred model. In this section, we ﬁrst describe our model architecture, and try to address: 1) how to train the NER model effectively with partially annotated sequences? 2) what training scheme is best suited to improve the transferred model?

2.3.1 Model Architecture
Our NER model is a BiLSTM-CNN-CRF model based on Ma and Hovy (2016) consisting of: a character-level CNN, that allows the model to capture subword information; a word-level BiLSTM, that consumes word embeddings and produces context sensitive hidden representations; and a linear-chain CRF layer that models the dependency between labels for inference. We use the above model for training the initial NER model on the transferred data as well as for re-training the model on the data acquired from active learning.

2.3.2 PARTIAL-CRF
Active learning with span-based strategies such as ETAL, produces a training dataset of partially labeled sequences. To train the NER model on these partially labeled sequences, we take inspiration from Bellare and McCallum (2007); Tsuboi et al. (2008) and use a constrained CRF decoder. Normally, CRF computes the likelihood of a label sequence y given a sequence x as follows:

pθ(y|x) =

T t=1

ψi

(yt−1

,

yt

,

x,

t)

Z (x)

T

Z(x) =

ψi(yt−1, yt, x, t)

y∈Y(T ) t=1

where T is the length of the sequence, Y(T ) denotes the set of all possible label sequences with

length T , and ψi(yt−1, yt, x) = exp(WyTt−1,yt xi+ byt−1,yt) is the energy function. To compute the likelihood of a sequence where some labels are unknown, we use a constrained CRF which marginalizes out the un-annotated tokens. Specifically, let YL denote the set of all possible sequences that include the partial annotations (for un-annotated tokens, all labels are possible), and we compute the likelihood as: pθ(YL|x) =
y∈YL pθ(y|x), referred as PARTIAL-CRF.
2.3.3 Training Scheme
To improve our model with the newly labeled data, we directly ﬁne-tune the initial model, trained on the transferred data, on the data acquired through active learning, referred as FINETUNE. Each active learning run produces more labeled data, for which this training procedure is repeated again. We also compare the NER performance using two other training schemes: CORPUSAUG, where we train the model on the concatenated corpus of transferred data and the newly acquired data, and CORPUSAUG+FINETUNE, where we additionally ﬁne-tune the model trained using CORPUSAUG on just the newly acquired data.
3 Experiments
In this section, we evaluate the effectiveness of our proposed strategy in both simulated (§3.2) and human-annotation experiments (§3.3).
3.1 Experimental Settings
Datasets: The ﬁrst evaluation set includes the benchmark CoNLL 2002 and 2003 NER datasets (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003) for Spanish (from the Romance family), Dutch and German (like English, from the Germanic family). We use the standard corpus splits for train/dev/test. The second evaluation set is for the low-resource setting where we use the Indonesian (from the Austronesian family), Hindi (from the Indo-Aryan family) and Spanish datasets released by the Linguistic Data Consortium (LDC).2 We generate the train/dev/test split by random sampling. Details of the corpus statistics are in the Appendix §A.1.
English-transferred Data: We use the same experimental settings and resources as described in Xie et al. (2018) to get the translations of the English training data for each target language.
2LDC2017E62,LDC2016E97,LDC2017E66

Active Learning Setup: As described in Section §2.2, a DP-based algorithm is employed to select the uncertain entity spans which runs for all n-grams having length <= 5. This length was approximated by computing the 90th percentile on the length of entities in the English training data. Hthreshold is a hyper-parameter set to 1e-8. The details of the NER model hyper-parameters can be found in the Appendix §A.2 .
3.2 Simulation Experiments
Setup: We use cross-lingual transfer (§2.1) to train our initial NER model and test on the target language. This is the same setting as Xie et al. (2018) and serves as our baseline. Then we use several active learning strategies to select data for manual annotation using this trained NER model. We compare our proposed ETAL strategy with the following baseline strategies:
SAL: Select whole sequences for which the model has least conﬁdence in the most likely labeling (Culotta and McCallum, 2005).
CFEAL: Select least conﬁdent spans within a sequence using the conﬁdence ﬁeld estimation method (Culotta and McCallum, 2004).
RAND: Select spans randomly from the unlabeled set for annotation.
In this experimental setting, we simulate manual annotation by using gold labels for the data selected by active learning. At each subsequent run, we annotate 200 tokens and ﬁne-tune the NER model on all the data acquired so far, which is then used to select data for the next run of annotation.
3.2.1 Results
Figure 2 summarizes the results for all datasets across the different experimental settings. Each data-point on the x-axis corresponds to the NER performance after annotating 200 additional tokens. CT denotes using cross-lingual transferred data to train the initial NER model for both kickstarting the active learning process and also for ﬁne-tuning the NER model on the newly-acquired data. PARTIAL-CRF/FULL-CRF denote the type of CRF decoder used in the NER model. Throughout this paper, we report results averaged across all active learning runs unless otherwise noted. Individual scores are reported in the Appendix §A.5.
As can be seen in the ﬁgure, our proposed recipe ETAL+PARTIAL-CRF+CT outperforms the previous

Dataset Hindi LDC
Indonesian LDC
Spanish LDC

Tokens
200 600 1200
200 600 1200
200 600 1200

ETAL
54.8 ± 2.6 64.7 ± 2.6 69.9 ± 2.5
47.4± 2.6 54.5 ± 2.4 60.5 ± 2.3
66.3 ± 3.8 65.8 ± 4.1 78.9 ± 3.5

SAL
49.6 ± 2.8 51.5 ± 2.9 56.6 ± 2.7
47.9 ± 2.4 44.5 ± 2.2 44.7 ± 2.3
62.0 ± 3.6 62.5 ± 3.7 62.3 ± 3.6

RAND
50.2 ± 0.4 56.1 ± 2.6 56.8 ± 2.6
46.8 ± 2.3 47.2 ± 2.2 51.9 ± 2.3
61.2 ± 1.2 61.9 ± 2.0 64.6 ± 4.0

CFEAL
50.4 ± 2.8 54.3 ± 2.5 64.4 ± 2.7
48.5 ± 2.3 46.0 ± 2.3 49.5 ± 2.3
62.5 ± 3.7 63.8 ± 3.7 68.6 ± 3.9

Table 1: Variance analysis for signiﬁcance testing of different active learning systems using paired bootstrap resampling. ± denotes the 95% conﬁdence intervals. Systems which are not statistically signiﬁcant than the best system ETAL are in bold. The CoNLL datasets reﬂect the same observation, as can be seen in Appendix §A.4.

active learning baselines for all the datasets. Holding the other two components of CT and PARTIALCRF constant, we conduct experiments to compare the different active learning strategies, which are denoted by the solid lines in Figure 2. We see that ETAL outperforms the other strategies by a significant margin for both the CoNLL datasets: German (+6.1 F1), Spanish (+5.3 F1), Dutch (+6.3 F1) and the LDC datasets: Hindi (+9.3 F1), Indonesian (+9.0 F1), Spanish (+7.5 F1), at the end of all runs. Furthermore, even with just one-tenth annotated tokens, the proposed recipe is only (avg.) -5.2 F1 behind the model trained using all labeled data, denoted by SUPERVISED ALL. Although CFEAL also selects informative spans, ETAL outperforms it because ETAL is optimized to select likely entities, causing more entities to be annotated for Hindi (+43), Indonesian (+207), Spanish-CoNLL (+1579), German (+906), Dutch (+836), except for Spanish-LDC (-184). Despite fully labeled data being adding in SAL, ETAL outperforms it because SAL selects longer sentences with fewer entities: Hindi (-934), Indonesian (-1290), SpanishLDC (-527), Spanish-CoNLL (-2395), German (2086), Dutch (-2213).
From Figure 2 we see that ETAL performs better than the baselines across multiple runs. To verify that this is not an artifact of randomness in the test data, we use a paired bootstrap resampling method, as illustrated in Koehn (2004), to compare SAL, CFEAL, RAND with ETAL. For each system, we compute the F1 score on randomly sampled 50% of the data and perform 10k bootstrapping steps at three active learning runs. From Table 1 we see that the baselines are signiﬁcantly worse than ETAL at 600 and 1200 annotated tokens.

Baseline 200 400 600 800
1000 1200 1400 1600 1800 2000 2200 2400 2600 2800 3000 3200 3400 3600 3800 4000 Baseline
200 400 600 800 1000 1200 1400 1600 1800 2000 2200 2400 2600 2800 3000 3200 3400 3600 3800 4000

100 90 80 Supervised All 70 60 50 40 30 20 10 0
100 90 Supervised All 80 70 60 50 40 30 20 10 0

Hindi LDC

Supervised All

Indonesian LDC

Supervised All

Spanish LDC

Spanish CoNLL

Supervised All Supervised All

Spanish LDC
German CoNLL

eline 200 400 600 800 1000 1200 1400 1600 1800 2000 2200 2400 Bas

Supervised All

Dutch CoNLL

0 200 400 600 800 1000 1200 1400 1600 1800 2000 2200 2400

ETAL + PARTIAL-CRF + CT RAND + PARTIA L-CRF + CT

CFEAL + PARTIAL-CRF + CT SAL + FULL-CRF + CT

ETAL + PARTIAL-CRF ETAL + FULL-CRF + CT

Baseline 200 400 600 800
1000 1200 1400 1600 1800 2000 2200 2400 2600 2800 3000 3200 3400 3600 3800 4000 Baseline
200 400 600 800 1000 1200 1400 1600 1800 2000 2200 2400 2600 2800 3000 3200 3400 3600 3800 4000 Baseline 200 400 600 800 1000 1200 1400 1600 1800 2000 2200 2400 2600 2800 3000 3200 3400 3600 3800 4000

Figure 2: Comparison of the NER performance trained with the FineTune scheme, across six datasets. Solid lines compare the different active learning strategies. Dashed lines show the ablation experiments. The x-axis denotes the total number of tokens annotated and the y-axis denotes the F1 score.

3.2.2 Ablation Study
In order to study the contribution of CT and PARTIAL-CRF in improving the NER performance, we conduct the following ablation, denoted by dashed lines in Figure 2.
CT: We observe that the transferred data from English provides a good start to the NER model: 69.4 (Dutch), 63.0 (Spanish-LDC), 65.7 (SpanishCoNLL), 54.7 (German), 45.4 (Indonesian), 45.0 (Hindi) F1. As expected, cross-lingual transfer helps more for the languages closely related to English which are Dutch, German, Spanish. For this ablation, we train a ETAL+PARTIAL-CRF where no transferred data is used. Therefore, to create the seed data, we randomly annotate 200 tokens in the target language and thereafter use ETAL. We observe that as more in-domain data is acquired, the un-transferred setting soon approaches the transferred setting ETAL+PARTIAL-CRF+CT suggesting that an efﬁcient annotation strategy can help close the gap between these two systems with as few as ∼1000 tokens (avg.).
PARTIAL-CRF: We study the effect of using the original CRF (FULL-CRF) instead of the PARTIALCRF for training with partially labeled data. Since the former requires fully labeled sequences, the

un-annotated tokens in a sequence are labeled with the model predictions. We see from Figure 2 that the ETAL+FULL-CRF+CT performs worse (avg. 4.1 F1) than ETAL+PARTIAL-CRF+CT. This is because the FULL-CRF signiﬁcantly hurts the recall, as much as by an average of -11.0 points for Hindi, 1.4 for Indonesian, -7.4 for Spanish-LDC, -3.3 for German, -3.7 for Dutch, -4.8 for Spanish CoNLL.
3.2.3 Comparison of Training Schemes
We experiment with different NER training regimes (described in §2.3.3) for ETAL. We observe that generally ﬁne-tuning not only speeds up the training but also gives better performance than CORPUSAUG. For brevity of space, we compare results for two languages in Figure 3:3 Dutch, a relative of English, and Hindi, a distant language. We see that FINETUNE performs better for Hindi whereas CORPUSAUG+FINETUNE performs better for Dutch. This is because Dutch is closely related to English and beneﬁts the most from the transferred data being explicitly augmented. Whereas for Hindi, which is typologically distant from English, the transferred data is noisy and thus the model doesn’t gain much from the transferred data. Xie
3The results for other datasets can be found in Appendix §A.3

Hindi
100 80 60 40 20 0

Dutch

Baseline 200 400 600 800
1000 1200 1400 1600 1800 2000 2200 2400 2600 2800 3000 3200 3400 3600 3800 4000 Baseline 200 400 600 800 1000 1200 1400 1600 1800 2000 2200 2400 2600 2800 3000 3200 3400 3600 3800 4000

Figure 3: Comparison of the NER performance trained with different schemes for the ETAL strategy. The x-axis denotes the total number of tokens annotated and the y-axis denotes the F1 score.

et al. (2018) make a similar observation in their experiments with German.
3.3 Human Annotation Experiments
Setup: We conduct human annotation experiments for Hindi, Indonesian and Spanish to understand whether ETAL helps reduce the annotation effort and improve annotation quality in practical settings. We compare ETAL with the full sequence strategy (SAL). We use six native speakers, two for each language, with different levels of familiarity with the NER task. Each annotator was provided with practice sessions to gain familiarity with the annotation guidelines and the user interface. The annotators annotated for 20 mins time for each strategy. For ETAL, the annotator was required to annotate single spans i.e each sequence contained one span of tokens. This involved assigning the correct label and adjusting the span boundary if required. For SAL, the annotator was required to annotate all possible entities in the sequence. We randomized the order in which the annotators had to annotate using ETAL and SAL strategy. Figure 5 illustrates the human annotation process for the ETAL strategy in the annotation interface. 4
3.3.1 Results
Table 2 records the results of the human annotation experiments. We ﬁrst compare each annotator’s annotation quality with respect to the oracle under both ETAL and SAL, denoted by Annotator Performance. We see that both Hindi and Spanish annotators have higher annotation quality using ETAL. This is because by selecting possible entity spans, ETAL not only saves effort on searching the entities in a sequence but also allows the an-
4The code for the annotation interface can be found at https://gitlab.com/cmu_ariel/ ariel-annotation-interface.

Annotator Test Performance (# annotated tokens) Performance

ETAL SAL ETAL

SAL

SAL-Full

HI-1 78.8 63.7 50.4 (326) 44.2 (326) 53.3 (1894) HI-2 82.7 72.2 49.1 (234) 45.9 (234) 55.6 (2242)

ID-1 66.1 77.8 50.4 (425) 45.8 (425) 51.3 (3232) ID-2 73.0 79.5 51.2 (251) 46.5 (251) 54.0 (2874)

ES-1 79.7 75.0 63.7 (204) 62.2 (204) 64.6 (2134) ES-2 83.1 70.4 63.8 (199) 62.2 (199) 62.6 (2134)

Table 2: Annotator performance measures F1 of each annotator with respect to the oracle. Test Performance measures the NER F1 scores using the annotations as training data. The number in the brackets denote the number of annotated tokens used for training the NER model. ES:Spanish, HI:Hindi, ID: Indonesian.

notators to read less overall and concentrate more on the things that they do read, as seen in Figure 4(a). However, for SAL we see that the annotator missed a likely entity because they focused on the other more salient entities in the sequence. For Indonesian, we see an opposite trend due to several inconsistencies in the gold labels. The most common inconsistency being when a common noun is part of an entity. For instance, the gold standard annotates the span Kabupaten Bogor as an entity where Kabupaten means “district”. Whereas for Kabupaten Aceh tengah, the gold standard does not include Kabupaten. Similarly, the same span gunung krakatau is annotated inconsistently across different mentions where sometimes they exclude the gunung (mountain) token.Since the annotators referred to these examples during their practice session, their annotations had similar inconsistencies. This issue affects ETAL more than SAL because ETAL selects more entities for annotation.
The Test Performance compares the performance of the NER models trained with these annotations. The number in the brackets denotes the

NUMBER OF ENTITIES 127
63 126
63 240
81 186
100 312
147 188
137

ETA L

Sentence: !कू ल और (श*क+ School and teachers
Gold: Human:

क, कमी पर ‘s lack of

[स2ु ीम Supreme
BORG BORG

कोट5] ने Court
IORG IORG

मांगा जवाब asks answer

Sentence: ?वराट [कोहलA] को आईसीसी Virat Kohli has ICC
Gold: BPER IPER Human: BPER IPER

क, टे!ट ‘s Test

टAम मD Team in

जगह नहAं place no

Sentence: [(मE के 21 ईसाइय+ बंधक+ का IS ने

Egypt ‘s 21 Christian brothers ‘s IS

Gold: BGPE O O BORG

O

O BORG O

Human: BGPE O O O

O

O BORG O

Iकया (सर कलम] made head lines OO O OO O

(a)

ETAL SAL
ES-1 ES-2 HI-1 HI-2 ID-1 ID-2 ORACLE ANNOTATORS
(b)

SA L

Figure 4: (a) Examples from Hindi human annotation experiments for both ETAL and SAL. Square brackets denote the spans (for ETAL) or the entire sequence (for SAL) selected by the respective active learning strategy. (b) Comparing the number of entities in the data selected by ETAL and SAL, as annotated by oracle.

total number of annotated tokens used for training the NER model. We observe that SAL has a larger number of annotated tokens than ETAL. This is because most sequences selected by SAL did not have any entities. Since “not-an-entity” is the default label in the annotation interface, no operation is required for annotating these, allowing for more tokens to be annotated per unit times. When we count the number of entities present in the data selected by the two strategies, we see in Figure 4(b) that data selected by ETAL has a signiﬁcantly larger number of entities than SAL, across all the 6 annotation experiments. Therefore, we ﬁrst compare the NER performance on the same number of annotated tokens. From Table 2 we see that under this setting ETAL outperforms SAL, similar to the simulation results. We note that when we consider all the annotated tokens, SAL (denoted by SALFULL) has slightly better results. However, despite having 6 times fewer annotated tokens, the difference between ETAL and SAL-FULL is (avg.) 2.1 F1. This suggests that ETAL can achieve competitive performance with fewer annotations.
From both the simulation and human experiments, we can conclude that a targeted annotation strategy such as ETAL achieves competitive performance with less manual effort while maintaining high annotation quality. Given that ETAL can help ﬁnd twice as many entities as SAL, a potential application of ETAL can also be for creating a highquality entity gazetteer under a short time budget. Since a naive strategy of SAL allows for more labelled data to be acquired in the same amount of time, in the future we plan to explore mixed-mode

annotation where we choose either full sequences or spans for annotation.
4 Related Work
Cross-Lingual Transfer: Transferring knowledge from high-resource languages has been extensively used for improving low-resource NER. More common approaches rely on annotation projection methods where annotations in source language are projected to the target language using parallel corpora (Zitouni and Florian, 2008; Ehrmann et al., 2011) or bilingual dictionaries (Xie et al., 2018; Mayhew et al., 2017). Crosslingual word embeddings (Bharadwaj et al., 2016; Chaudhary et al., 2018) also provide a way to leverage annotations from related languages.
Active Learning (AL): AL has been widely explored for many NLP tasks- NER: Shen et al. (2017) explore token-level annotation strategies, Chen et al. (2015) present a study on AL for clinical NER; Baldridge and Palmer (2009) evaluate how well AL works with annotator expertise and label suggestions, Garrette and Baldridge (2013) study type and token based strategies for lowresource languages. Settles and Craven (2008) present a nice survey on the different AL strategies for sequence labeling tasks, whereas Marcheggiani and Artieres (2014) discuss the strategies for acquiring partially labeled data. Wanvarie et al. (2011); Neubig et al. (2011); Sperber et al. (2014) show the advantages of training a model on this partially labeled data. All above methods focus on either token or full sequence annotation.

(a) Selected spans using ETAL strategy are highlighted for the human annotator to annotate.

(b) Human annotator correcting the span boundary and assigning the correct entity type.

(c) Human annotator assigning the correct entity type only since selected span boundary is correct.

(d) Partially-annotated sequences after being annotated by the human annotator. Figure 5: Example of the human annotation process for Hindi.

The most similar work to ours perhaps is that of Fang and Cohn (2017), which selects informative word types for low-resource POS tagging. However, their method requires the annotator to annotate single tokens, which is not trivially applicable for multi-word entities in practical settings.
5 Conclusion
In this paper, we presented a study on how to efﬁciently bootstrap NER systems for low-resource languages using a combination of cross-lingual transfer learning and active learning. We conducted both simulated and human annotation experiments across different languages and found that: 1) cross-lingual transfer is a powerful tool, constantly beating systems without using transfer; 2) our proposed recipe works the best among known active learning baselines; 3) our proposed active learning strategy saves annotator much effort while ensuring high quality. In future, to account for different levels of annotator expertise, we plan to combine proactive learning (Li et al., 2017) with our proposed method.

Acknowledgement
The authors would like to thank Sachin Kumar, Kundan Krishna, Aldrian Obaja Muis, Shirley Anugrah Hayati, Rodolfo Vega and Ramon Sanabria for participating in the human annotation experiments. This work is sponsored by Defense Advanced Research Projects Agency Information Innovation Ofﬁce (I2O). Program: Low Resource Languages for Emergent Incidents (LORELEI). Issued by DARPA/I2O under Contract No. HR0011-15-C0114. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the ofﬁcial policies, either expressed or implied, of the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation here on.
References
Waleed Ammar, George Mulcaire, Yulia Tsvetkov, Guillaume Lample, Chris Dyer, and Noah A Smith.

2016. Massively multilingual word embeddings. arXiv preprint arXiv:1602.01925.
Mikel Artetxe, Gorka Labaka, and Eneko Agirre. 2017. Learning bilingual word embeddings with (almost) no bilingual data. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 451–462, Vancouver, Canada. Association for Computational Linguistics.
Jason Baldridge and Alexis Palmer. 2009. How well does active learning actually work? Time-based evaluation of cost-reduction strategies for language documentation. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 296–305, Singapore. Association for Computational Linguistics.
Kedar Bellare and Andrew McCallum. 2007. Learning extractors from unlabeled text using relevant databases. In Sixth international workshop on information integration on the web.
Akash Bharadwaj, David Mortensen, Chris Dyer, and Jaime Carbonell. 2016. Phonologically aware neural model for named entity recognition in low resource transfer settings. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1462–1472.
Aditi Chaudhary, Chunting Zhou, Lori Levin, Graham Neubig, David R. Mortensen, and Jaime Carbonell. 2018. Adapting word embeddings to new languages with morphological and phonological subword representations. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3285–3295. Association for Computational Linguistics.
Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. 2017. Reading Wikipedia to answer opendomain questions. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1870– 1879, Vancouver, Canada. Association for Computational Linguistics.
Yukun Chen, Thomas A Lasko, Qiaozhu Mei, Joshua C Denny, and Hua Xu. 2015. A study of active learning methods for named entity recognition in clinical text. Journal of biomedical informatics, 58:11–18.
Ronan Collobert, Jason Weston, Le´on Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. Natural language processing (almost) from scratch. Journal of Machine Learning Research, 12(Aug):2493–2537.
Alexis Conneau, Guillaume Lample, Marc’Aurelio Ranzato, Ludovic Denoyer, and Herve´ Je´gou. 2017a. Word translation without parallel data. arXiv preprint arXiv:1710.04087.

Alexis Conneau, Guillaume Lample, Marc’Aurelio Ranzato, Ludovic Denoyer, and Herve´ Je´gou. 2017b. Word translation without parallel data. arXiv preprint arXiv:1710.04087.
Aron Culotta and Andrew McCallum. 2004. Conﬁdence estimation for information extraction. In Proceedings of HLT-NAACL 2004: Short Papers, pages 109–112. Association for Computational Linguistics.
Aron Culotta and Andrew McCallum. 2005. Reducing labeling effort for structured prediction tasks. In Association for the Advancement of Artiﬁcial Intelligence (AAAI), volume 5, pages 746–751.
Maud Ehrmann, Marco Turchi, and Ralf Steinberger. 2011. Building a multilingual named entityannotated corpus using annotation projection. In Proceedings of the International Conference Recent Advances in Natural Language Processing 2011, pages 118–124.
Meng Fang and Trevor Cohn. 2017. Model transfer for tagging low-resource languages using a bilingual dictionary. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 587–593, Vancouver, Canada. Association for Computational Linguistics.
Dan Garrette and Jason Baldridge. 2013. Learning a part-of-speech tagger from two hours of annotation. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 138–147, Atlanta, Georgia. Association for Computational Linguistics.
Xianpei Han and Le Sun. 2012. An entity-topic model for entity linking. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 105–115. Association for Computational Linguistics.
Philipp Koehn. 2004. Statistical signiﬁcance tests for machine translation evaluation. In Proceedings of the 2004 conference on empirical methods in natural language processing.
Guillaume Lample, Miguel Ballesteros, Sandeep Subramanian, Kazuya Kawakami, and Chris Dyer. 2016. Neural architectures for named entity recognition. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 260–270, San Diego, California. Association for Computational Linguistics.
Maolin Li, Nhung Nguyen, and Sophia Ananiadou. 2017. Proactive learning for named entity recognition. In BioNLP 2017, pages 117–125, Vancouver, Canada,. Association for Computational Linguistics.

Xuezhe Ma and Eduard Hovy. 2016. End-to-end sequence labeling via bi-directional LSTM-CNNsCRF. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1064–1074. Association for Computational Linguistics.
Diego Marcheggiani and Thierry Artieres. 2014. An experimental comparison of active learning strategies for partially labeled sequences. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 898–906.
Stephen Mayhew, Chen-Tse Tsai, and Dan Roth. 2017. Cheap translation for cross-lingual named entity recognition. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2536–2545.
Graham Neubig, Yosuke Nakata, and Shinsuke Mori. 2011. Pointwise prediction for robust, adaptable japanese morphological analysis. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers-Volume 2, pages 529–533. Association for Computational Linguistics.
Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. Glove: Global vectors for word representation. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 1532–1543.
Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word representations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 2227–2237, New Orleans, Louisiana. Association for Computational Linguistics.
Lawrence R Rabiner. 1989. A tutorial on hidden markov models and selected applications in speech recognition. Proceedings of the IEEE, 77(2):257– 286.
Burr Settles and Mark Craven. 2008. An analysis of active learning strategies for sequence labeling tasks. In Proceedings of the conference on empirical methods in natural language processing, pages 1070– 1079. Association for Computational Linguistics.
Yanyao Shen, Hyokun Yun, Zachary Lipton, Yakov Kronrod, and Animashree Anandkumar. 2017. Deep active learning for named entity recognition. In Proceedings of the 2nd Workshop on Representation Learning for NLP, pages 252–256, Vancouver, Canada. Association for Computational Linguistics.
Matthias Sperber, Mirjam Simantzik, Graham Neubig, Satoshi Nakamura, and Alex Waibel. 2014. Segmentation for efﬁcient supervised language annota-

tion with an explicit cost-utility tradeoff. Transactions of the Association for Computational Linguistics, 2:169–180.
Erik F. Tjong Kim Sang. 2002. Introduction to the conll-2002 shared task: Language-independent named entity recognition. In Proceedings of the 6th Conference on Natural Language Learning - Volume 20, COLING-02, pages 1–4, Stroudsburg, PA, USA. Association for Computational Linguistics.
Erik F. Tjong Kim Sang and Fien De Meulder. 2003. Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003, pages 142–147.
Yuta Tsuboi, Hisashi Kashima, Hiroki Oda, Shinsuke Mori, and Yuji Matsumoto. 2008. Training conditional random ﬁelds using incomplete annotations. In Proceedings of the 22nd International Conference on Computational Linguistics-Volume 1, pages 897– 904. Association for Computational Linguistics.
Dittaya Wanvarie, Hiroya Takamura, and Manabu Okumura. 2011. Active learning with subsequence sampling strategy for sequence labeling tasks. Information and Media Technologies, 6(3):680–700.
Jiateng Xie, Zhilin Yang, Graham Neubig, Noah A. Smith, and Jaime Carbonell. 2018. Neural crosslingual named entity recognition with minimal resources. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 369–379, Brussels, Belgium. Association for Computational Linguistics.
Bishan Yang and Tom M. Mitchell. 2016. Joint extraction of events and entities within a document context. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 289–299, San Diego, California. Association for Computational Linguistics.
David Yarowsky, Grace Ngai, and Richard Wicentowski. 2001. Inducing multilingual text analysis tools via robust projection across aligned corpora. In Proceedings of the ﬁrst international conference on Human language technology research, pages 1– 8. Association for Computational Linguistics.
Imed Zitouni and Radu Florian. 2008. Mention detection crossing the language barrier. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 600–609. Association for Computational Linguistics.

A Appendix
A.1 Corpus Statistics Table 3 presents the train/dev/test splits used for the NER model training, along with the total number of tokens present in the training data.

Source LDC
CoNLL

Dataset
Hindi Indonesian Spanish Dutch German Spanish

Train / Dev / Test # Sentences
2570 / 809 / 1592 3181 / 1001 / 1991 1398 / 465 / 928
13274 / 2307 / 4227 12067 / 2849 / 2984 8357 / 1915 / 1517

Total Tokens in Train
48604 55270 31799
200059 206846 264715

Table 3: Corpus Statistics.

A.2 NER Model Hyperparameters
For each language, we train the model with 100d pre-trained GloVe (Pennington et al., 2014) word embeddings trained on Wikipedia and the monolingual text extracted from the train set. We use hidden size of 200 for each direction of the LSTM and a dropout of 0.5. SGD is used as the optimizer with a learning rate of 0.015. During ﬁne-tuning, the NER model is ﬁrst trained on the transferred data with the above settings. For the ﬁrst active learning run, the model is ﬁne-tuned on the target language with a lower learning rate of 1e-5 and for each subsequent run, this rate is increased to 0.015.
A.3 Training Schemes
The results for comparing the different training schemes for Spanish CoNLL, German CoNLL and Indonesian can be seen in Figure 6.
A.4 Variance Analysis
Figure 4 shows the 95% conﬁdence intervals of the NER models comparing the different active learning strategies for the CoNLL datasets using the bootstrap re-sampling method.
A.5 Comprehensive Results
Table 5, 6, 7, 7, 9, 10 compares the number of entities present in the data selected by ETAL, CFEAL and SAL across all the datasets.
Tables 11, 12, 13, 14, 15, 16 show the tabulated results for the NER models trained with different active learning strategies for Hindi, Indonesian, German, Spanish and Dutch datasets.

Dataset Dutch CoNLL
German CoNLL
Spanish CoNLL

Tokens
200 600 1200
200 600 1200
200 600 1200

ETAL
69.4 ± 1.6 74.8 ± 1.6 77.0 ± 1.5
59.3 ± 1.7 62.9 ± 1.7 64.7 ± 1.7
69.7 ± 1.7 75.3 ± 1.8 77.1 ± 1.7

SAL
69.6 ± 1.6 69.4 ± 1.6 69.6 ± 1.7
57.4 ± 1.9 58.7 ± 1.8 58.7 ± 1.8
65.8 ± 1.8 66.3 ± 1.8 65.7 ± 1.8

RAND
69.4 ± 1.6 67.2 ± 2.1 74.0 ± 0.0
55.2 ± 2.1 58.1 ± 2.0 60.7 ± 1.8
69.5 ± 1.6 73.3 ± 1.8 73.2 ± 1.8

CFEAL
69.4 ± 1.6 66.3 ± 1.8 68.7 ± 1.8
54.7 ± 2.1 57.2 ± 1.8 60.1 ± 1.7
65.3 ± 1.7 67.8 ± 1.7 70.2 ± 1.7

Table 4: Variance analysis for signiﬁcance testing of different active learning systems using paired bootstrap resampling. ± denotes the 95% conﬁdence intervals. Systems which are not statistically signiﬁcant than the best system ETAL are highlighted in bold.

As mentioned in the ablation study which evaluates the effectiveness of PARTIAL-CRF over FULLCRF, we ﬁnd that FULL-CRF signiﬁcantly hurts the recall. Table 17, 18, 19, 20, 21 documents the results of the recall scores across the two settings for Hindi, Indonesian, Spanish-LDC, SpanishCoNLL, German and Dutch respectively.

Spanish CoNLL
100 90 80 70 60 50 40 30 20 10 0

German CoNLL

Indonesian

Baseline 200 400 600 800 1000 1200 1400 1600 1800 2000 2200 2400 2600 2800 3000 3200 3400 3600 3800 4000
Baseline 200 400 600 800 1000 1200 1400 1600 1800 2000 2200 2400 2600 2800 3000 3200 3400 3600 3800 4000
Baseline 200 400 600 800 1000 1200 1400 1600 1800 2000 2200 2400 2600 2800 3000 3200 3400 3600 3800 4000

Figure 6: Comparison of the NER performance trained with different schemes for the ETAL strategy.

Method

1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20

ETAL + PARTIAL-CRF + CT 115 192 281 379 482 580 675 769 854 934 994 1083 1135 1158 1171 1178 1178 1179 1180 1180

CFEAL+ PARTIAL-CRF + CT 88 207 298 397 506 608 698 793 877 978 1047 1078 1104 1111 1113 1119 1123 1131 1132 1137

SAL+ FULL-CRF + CT

21 42 45 52 60 70 88 95 111 126 133 150 158 174 184 195 210 227 235 246

Table 5: Comparing number of entities across ETAL, SAL and CFEAL for the Hindi LDC dataset.

Method

1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20

ETAL + PARTIAL-CRF + CT 87 186 303 413 525 647 741 849 949 1056 1138 1221 1303 1360 1450 1484 1511 1525 1535 1536

CFEAL+ PARTIAL-CRF + CT 86 192 280 371 449 517 601 666 726 793 847 911 973 1021 1069 1125 1186 1244 1269 1329

SAL+ FULL-CRF + CT

7 16 28 39 46 50 63 79 90 106 132 143 158 161 168 187 209 225 231 246

Table 6: Comparing number of entities across ETAL, SAL and CFEAL for the Indonesian LDC dataset.

Method

1 2 3 4 5 6 7 8 9 10 11 12

ETAL + PARTIAL-CRF + CT 84 187 280 391 492 534 585 610 617 619 620 621

CFEAL+ PARTIAL-CRF + CT 79 238 408 530 628 709 777 794 800 801 804 805

SAL+ FULL-CRF + CT

5 10 15 18 20 25 30 46 55 66 80 94

Table 7: Comparing number of entities across ETAL, SAL and CFEAL for the Spanish LDC dataset.

Method

12345678

9

10 11 12 13 14 15 16 17 18 19 20

ETAL + PARTIAL-CRF + CT 152 298 427 562 693 823 950 1094 1234 1381 1503 1636 1753 1882 2010 2130 2257 2384 2522 2674

CFEAL+ PARTIAL-CRF + CT 64 128 184 236 293 343 389 440 492 543 593 642 682 729 767 803 873 945 1021 1095

SAL+ FULL-CRF + CT

27 44 66 79 88 102 117 129 132 142 154 172 180 196 223 232 240 252 263 279

Table 8: Comparing number of entities across ETAL, SAL and CFEAL for the Spanish CoNLL dataset.

Method

12345678

9

10 11 12 13 14 15 16 17 18 19 20

ETAL + PARTIAL-CRF + CT 154 264 386 513 664 775 883 1016 1153 1275 1365 1490 1588 1730 1827 1954 2064 2121 2211 2329

CFEAL+ PARTIAL-CRF + CT 80 158 217 285 365 424 490 566 640 704 772 847 941 1008 1084 1146 1220 1285 1358 1423

SAL+ FULL-CRF + CT

22 68 74 81 93 101 112 123 135 148 166 175 188 198 205 213 224 230 239 243

Table 9: Comparing number of entities across ETAL, SAL and CFEAL for the German CoNLL dataset.

Method

1234567

8

9

10 11 12 13 14 15 16 17 18 19 20

ETAL + PARTIAL-CRF + CT 166 311 448 584 730 862 1008 1119 1227 1356 1466 1592 1708 1810 1931 2041 2152 2256 2376 2496

CFEAL+ PARTIAL-CRF + CT 89 172 253 342 420 494 581 672 767 855 942 1020 1102 1181 1259 1341 1416 1505 1583 1660

SAL+ FULL-CRF + CT

27 48 69 83 96 107 141 151 160 163 171 188 204 226 237 252 262 275 282 283

Table 10: Comparing number of entities across ETAL, SAL and CFEAL for the Dutch CoNLL dataset.

Type Span

Method ETAL + PARTIAL-CRF + CT ETAL + PARTIAL-CRF ETAL + FULL-CRF + CT

0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 45.0 54.8 60.0 64.7 68.6 69.7 70.0 71.6 72.3 73.1 74.0 73.2 73.7 74.2 75.1 74.4 74.2 73.8 74.1 73.1 74.3 0.0 17.5 30.3 51.3 59.0 61.7 64.8 65.2 66.8 67.7 68.5 68.0 70.1 72.0 72.5 73.0 71.4 72.1 72.2 72.0 72.8 45.0 54.2 55.8 57.8 60.0 59.5 61.7 62.0 62.5 63.5 63.7 64.1 64.2 64.3 64.4 65.2 65.1 64.0 64.9 64.9 64.2

Span

CFEAL + PARTIAL-CRF + CT 45.0 47.8 46.7 47.4 47.5 60.0 65.5 66.0 67.3 68.0 68.8 69.2 69.6 69.9 70.6 68.8 70.7 71.4 71.1 71.2 72.1 RAND + PARTIAL-CRF + CT 45.0 50.2 53.2 56.1 57.4 56.1 56.9 58.2 59.5 59.5 58.9 60.3 61.7 60.7 61.9 62.4 62.2 62.8 63.3 64.5 65.2

Sequence SAL + FULL-CRF + CT

45.0 49.6 51.2 51.6 52.6 54.4 56.6 58.6 58.8 59.1 61.2 62.2 60.2 60.1 60.4 60.6 62.7 62.9 62.9 63.1 64.2

Table 11: Comparison of NER performance of different active learning strategies for the Hindi LDC dataset. F1 scores are reported. Each column corresponds to NER performance on 200 additional annotated tokens.

Type Span

Method ETAL + PARTIAL-CRF + CT ETAL + PARTIAL-CRF ETAL + FULL-CRF + CT

0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 45.4 47.4 50.8 54.5 58.0 60.1 60.5 62.3 65.8 63.0 64.0 65.4 65.2 65.7 65.1 67.6 66.7 67.6 66.4 66.7 67.2 0.0 22.8 36.8 42.6 47.0 49.1 51.5 53.8 56.3 55.9 57.6 56.6 59.1 59.6 60.8 60.4 61.2 62.7 61.7 61.9 60.9 45.4 48.4 52.3 52.4 54.2 54.6 55.2 57.0 57.0 58.4 59.1 59.1 59.5 60.7 60.7 61.3 60.3 60.3 61.2 60.9 60.4

Span

CFEAL + PARTIAL-CRF + CT 45.4 48.5 47.1 46.0 47.5 49.8 49.5 53.9 55.7 54.1 54.9 57.1 55.5 54.7 57.9 56.2 57.9 59.3 58.2 58.6 60.2 RAND + PARTIAL-CRF + CT 45.4 46.8 48.1 47.2 47.2 51.5 51.9 52.5 52.8 52.4 53.2 53.5 54.6 54.1 56.2 55.2 55.8 56.6 58.4 58.6 56.8

Sequence SAL + FULL-CRF + CT

45.4 47.9 45.7 44.5 45.1 45.4 44.7 45.4 48.8 47.8 49.2 50.6 50.3 51.8 51.0 49.9 52.0 51.8 52.4 50.4 52.7

Table 12: Comparison of NER performance of different active learning strategies for the Indonesian LDC dataset. F1 scores are reported. Each column corresponds to NER performance on 200 additional annotated tokens.

Type Span

Method ETAL + PARTIAL-CRF + CT ETAL + PARTIAL-CRF ETAL + FULL-CRF + CT

0 1 2 3 4 5 6 7 8 9 10 11 12 63.0 66.3 67.9 65.7 69.4 74.1 78.9 77.6 78.2 77.6 78.2 76.1 77.2 0.0 10.9 39.8 58.2 63.3 66.8 70.1 70.3 74.5 72.5 72.3 72.5 71.1 63.0 62.9 66.6 67.2 68.3 68.0 70.6 70.1 68.5 69.4 69.4 70.6 69.7

Span

CFEAL + PARTIAL-CRF + CT 63.0 62.5 63.9 63.8 64.1 68.2 68.7 67.2 69.3 68.7 71.9 70.2 70.3 RAND + PARTIAL-CRF + CT 63.0 61.2 61.5 61.9 65.7 65.2 64.6 69.3 67.0 67.3 69.3 69.7 68.7

Sequence SAL + CT

63.0 62.0 61.8 62.5 61.9 62.3 62.3 62.1 62.3 62.3 62.5 62.7 62.2

Table 13: Comparison of NER performance (F1 scores) of different active learning strategies for the Spanish LDC dataset. Each column, except Run 0, corresponds to NER performance on 200 additional annotated tokens.

Type Span

Method ETAL + PARTIAL-CRF + CT ETAL + PARTIAL-CRF ETAL + FULL-CRF + CT

0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 54.7 59.3 64.1 63.0 66.5 65.0 64.7 65.4 66.0 66.8 67.4 67.9 67.7 69.5 69.0 69.6 70.8 67.7 67.1 71.3 70.5 0.0 9.0 39.8 45.1 51.9 53.9 56.5 60.3 61.0 64.0 61.2 61.1 64.3 66.0 64.9 65.0 64.4 66.8 67.4 67.6 66.4 54.7 60.7 63.6 63.9 65.4 66.5 66.6 66.4 67.5 66.9 67.3 66.9 67.7 67.7 68.5 69.3 69.3 69.8 70.7 71.0 70.2

Span

CFEAL + PARTIAL-CRF + CT 54.7 54.7 55.4 57.2 59.0 61.3 60.2 62.3 62.1 61.4 64.5 63.9 63.5 63.9 65.4 65.0 66.2 65.1 65.8 65.4 66.9 RAND + PARTIAL-CRF + CT 54.7 55.2 57.0 58.1 59.8 57.7 60.7 59.5 57.4 57.7 59.5 60.5 58.1 59.5 61.0 58.5 58.8 60.2 61.6 61.8 58.7

Sequence SAL + FULL-CRF + CT

54.7 57.4 57.9 58.8 58.5 59.1 58.7 58.8 58.8 59.5 57.9 57.0 56.6 60.4 60.2 60.5 61.2 60.2 61.8 60.9 60.8

Table 14: Comparison of NER performance of different active learning strategies for the German CoNLL dataset. F1 scores are reported. Each column corresponds to NER performance on 200 annotated tokens.

Type Span

Method ETAL + PARTIAL-CRF + CT ETAL + PARTIAL-CRF ETAL + FULL-CRF + CT

0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 65.7 69.8 74.4 75.3 77.0 76.5 77.1 77.4 77.7 77.2 78.4 78.0 77.9 79.0 79.3 78.7 79.5 79.1 78.3 79.7 79.0 0.0 36.4 54.0 64.5 70.5 72.9 72.8 73.7 74.3 75.8 75.2 74.1 76.0 76.2 75.7 76.0 76.5 76.8 76.9 77.2 77.8 65.7 72.0 68.8 71.2 71.7 72.2 72.8 73.3 73.4 72.7 73.3 74.7 74.2 73.9 73.6 74.0 73.9 74.1 74.9 74.5 73.7

Span

CFEAL + PARTIAL-CRF + CT 65.7 65.3 66.9 67.8 70.9 71.0 70.2 71.6 71.2 73.2 73.2 73.2 72.5 72.7 72.6 72.9 72.0 73.6 73.6 73.4 73.8 RAND + PARTIAL-CRF + CT 65.7 69.5 69.5 70.6 72.1 73.2 70.0 72.0 73.9 73.9 73.6 73.0 71.3 75.7 73.5 74.3 75.1 73.7 74.4 76.2 74.9

Sequence SAL + FULL-CRF + CT

65.7 65.8 67.4 68.2 68.4 68.2 67.3 67.6 69.4 69.6 69.2 68.9 69.0 69.8 70.0 70.6 71.5 70.7 73.0 70.7 72.7

Table 15: Comparison of NER performance of different active learning strategies for the Spanish CoNLL dataset. F1 scores are reported. Each column corresponds to NER performance on 200 annotated tokens.

Type Span

Method ETAL + PARTIAL-CRF + CT ETAL + PARTIAL-CRF ETAL + FULL-CRF + CT

0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 69.4 69.4 70.0 74.8 75.2 75.6 77.0 79.4 78.7 78.7 79.2 79.2 80.1 79.5 80.8 81.2 80.4 81.3 81.7 79.8 82.1 0.0 18.1 31.4 47.0 62.9 64.9 67.1 69.3 71.7 72.0 74.7 75.0 73.8 76.3 76.5 75.5 76.5 76.7 77.3 76.5 77.5 69.4 69.6 69.3 70.4 72.6 72.1 75.7 75.1 75.7 74.8 76.3 76.9 75.4 76.8 75.8 77.0 77.3 76.1 77.2 75.7 76.3

Span

CFEAL + PARTIAL-CRF + CT 69.4 69.5 69.6 69.8 69.6 69.9 69.8 69.7 69.8 69.8 69.8 69.8 69.9 69.6 69.6 69.8 69.6 69.6 69.7 69.7 69.7 RAND + PARTIAL-CRF + CT 69.4 69.5 69.8 67.2 71.3 72.7 74.0 72.5 72.6 72.7 72.5 73.1 73.9 73.8 73.4 72.8 74.4 74.3 73.1 74.6 74.6

Sequence SAL + FULL-CRF + CT

69.4 69.6 69.7 69.4 69.9 69.8 69.6 69.8 69.9 70.1 69.1 70.3 69.7 69.1 69.9 71.0 68.6 71.9 71.0 71.8 71.4

Table 16: Comparison of NER performance of different active learning strategies for the Dutch CoNLL dataset. F1 scores are reported. Each column corresponds to NER performance on 200 annotated tokens.

Method

0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20

ETAL + PARTIAL-CRF + CT 38.6 51.4 56.8 59.7 60.4 61.8 63.2 64.1 65.3 66.8 68.7 65.5 66.9 67.6 69.8 69.9 71.1 68.1 68.5 68.4 70.7

ETAL + FULL-CRF + CT

38.6 45.8 46.0 48.3 50.6 51.1 52.6 53.0 54.2 55.6 55.9 56.4 56.4 54.6 54.9 56.6 56.2 55.1 57.3 57.5 55.7

Table 17: Comparing recall scores for evaluating the effectiveness of PARTIAL-CRF over FULLCRF for the Hindi LDC dataset.

Method

0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20

ETAL + PARTIAL-CRF + CT 51.0 47.1 48.3 52.1 55.0 57.6 61.3 59.8 64.3 61.1 63.2 64.0 64.2 64.3 62.8 66.6 64.5 64.6 63.0 65.3 64.2

ETAL + FULL-CRF + CT

51.0 51.3 55.3 54.6 56.6 55.4 56.2 58.6 58.9 60.5 60.8 61.1 61.2 60.7 63.0 62.5 60.1 58.9 62.5 62.7 62.3

Table 18: Comparing recall scores for evaluating the effectiveness of PARTIAL-CRF over FULLCRF for the Indonesian LDC dataset.

Method

0 1 2 3 4 5 6 7 8 9 10 11 12

ETAL + PARTIAL-CRF + CT 57.4 59.5 58.5 57.5 63.9 72.2 75.1 76.0 76.0 75.5 75.6 73.7 74.6

ETAL + FULL-CRF + CT

57.4 59.4 61.7 60.6 61.5 61.9 63.1 62.1 62.1 61.9 63.3 63.0 61.9

Table 19: Comparing recall scores for evaluating the effectiveness of PARTIAL-CRF over FULLCRF for the Spanish LDC dataset.

Method

0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20

ETAL + PARTIAL-CRF + CT 45.7 58.3 61.6 63.9 63.2 66.0 64.1 64.2 62.8 65.3 67.4 67.8 68.9 68.1 69.4 67.3 68.4 63.5 63.1 69.5 65.5

ETAL + FULL-CRF + CT

45.7 52.2 56.6 60.2 61.3 61.3 61.1 62.6 61.1 61.0 61.2 62.8 63.1 63.4 63.8 64.4 65.6 64.2 64.8 67.2 65.5

Table 20: Comparing recall scores for evaluating the effectiveness of PARTIAL-CRF over FULLCRF for the German CoNLL dataset.

Method

0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20

ETAL + PARTIAL-CRF + CT 65.8 66.4 70.6 73.9 75.1 75.6 76.2 79.4 78.7 78.6 79.1 78.7 79.7 79.0 80.3 80.5 79.7 81.1 81.2 78.9 81.7

ETAL + FULL-CRF + CT

65.8 66.9 66.1 68.8 70.9 70.8 75.5 74.1 75.4 73.6 75.6 76.5 74.9 76.1 75.3 76.5 77.0 75.1 76.9 75.1 75.5

Table 21: Comparing recall scores for evaluating the effectiveness of PARTIAL-CRF over FULLCRF for the Dutch CoNLL dataset.

