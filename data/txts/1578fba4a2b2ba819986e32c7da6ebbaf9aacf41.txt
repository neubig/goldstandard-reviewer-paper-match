CMU-01 at the SIGMORPHON 2019 Shared Task on Crosslinguality and Context in Morphology
Aditi Chaudhary Elizabeth Salesky Gayatri Bhat David R. Mortensen Jaime G. Carbonell Yulia Tsvetkov {aschaudh, esalesky, gbhat, dmortens, jgc, ytsvetko}@cs.cmu.edu
Language Technologies Institute Carnegie Mellon University

arXiv:1907.10129v1 [cs.CL] 23 Jul 2019

Abstract
This paper presents the submission by the CMU-01 team to the SIGMORPHON 2019 task 2 of Morphological Analysis and Lemmatization in Context. This task requires us to produce the lemma and morpho-syntactic description of each token in a sequence, for 107 treebanks. We approach this task with a hierarchical neural conditional random ﬁeld (CRF) model which predicts each coarse-grained feature (eg. POS, Case, etc.) independently. However, most treebanks are under-resourced, thus making it challenging to train deep neural models for them. Hence, we propose a multi-lingual transfer training regime where we transfer from multiple related languages that share similar typology.1
1 Introduction
Morphological analysis (Hajic and Hladka´, 1998; Oﬂazer and Kuruo¨z, 1994) is the task of predicting morpho-syntactic properties along with the lemma of each token in a sequence, with several downstream applications including machine translation (Vylomova et al., 2017), named entity recognition (Gu¨ngo¨r et al., 2018) and semantic role labeling (Strubell et al., 2018). Advances in deep learning have enabled signiﬁcant progress for the task of morphological tagging (Mu¨ller and Schuetze, 2015; Heigold et al., 2017) and lemmatization (Malaviya et al., 2019) under large amounts of annotated data. However, most languages are underresourced and often exhibit diverse linguistic phenomena, thus making it challenging to generalize existing state-of-the-art models for all languages.
In order to tackle the issue of data scarcity, recent approaches have coupled deep learning with cross-lingual transfer learning (Malaviya et al., 2018; Cotterell and Heigold, 2017; Kondratyuk,
1The code is available at https://github.com/ Aditi138/MorphologicalAnalysis/.

2019) and have shown promising results. Previous works (e.g., Cotterell and Heigold, 2017) combine the set of morphological properties into a single monolithic tag and employ multi-sequence classiﬁcation. This runs the risk of data sparsity and exploding output space for morphologically rich languages. Malaviya et al. (2018) instead predict each coarse-grained feature, such as part-ofspeech (POS) or Case, separately by modeling dependencies between these features and also between the labels across the sequence using a factorial conditional random ﬁeld (CRF). However, this results in a large number of factors leading to a slower training time (over 24h).
To address the issues of both data sparsity and having a tractable computation time, we propose a hierarchical neural model which predicts each coarse-grained feature independently, but without modeling the pairwise interactions within them. This results in a time-efﬁcient computation (5–6h) and substantially outperforms the baselines. To more explicitly incorporate syntactic knowledge, we embed POS information in an encoder which is shared with all feature decoders. To address the issue of data scarcity, we present two multilingual transfer approaches where we train on a group of typologically related languages and ﬁnd that language-groups with shallower time-depths (i.e., period of time during which languages diverged to become independent) tend to beneﬁt the most from transfer. We focus on the task of contextual morphological analysis and use the provided baseline model for the task of lemmatization (Malaviya et al., 2019).
This paper makes the following contributions: 1. We present a hierarchical neural model for contextual morphological analysis with a shared encoder and independent decoders for each coarse-grained feature. This provides us with the ﬂexibility to produce any combination of features.

2. We analyze the dependencies among different morphological features to inform model choices, and ﬁnd that adding POS information to the encoder signiﬁcantly improves prediction accuracy by reducing errors across features, particularly Gender errors.
3. We evaluate our proposed approach on 107 treebanks and achieve +14.76 (accuracy) average improvement over the shared task baseline (McCarthy et al., 2019) for morphological analysis.
2 Contextual Morphological Analysis
In this section, we formally deﬁne the task (§2.1) and describe our proposed approach (§2.2).
2.1 Task Formulation
Formally, we deﬁne the task of contextual morphological analysis as a sequence tagging problem. Given a sequence of tokens x = x1, x2, · · · , xn, the task is to predict the morphological tagset y = y1, y2, · · · , yn where the target label yi for a token xi constitutes the ﬁne-grained morpho-syntactic traits {N;PL;NOM;FEM}.
2.2 Our Method
In line with Malaviya et al. (2018), we formulate morphological analysis as a feature-wise sequence prediction task, where we predict the ﬁne-grained labels (e.g N, NOM, ...) for the corresponding coarse-grained features F ={POS,Case,...} as shown in Figure 1. However, we only model the transition dependencies between the labels of a feature. This is done for two reasons: 1) As per Malaviya et al. (2018)’s analysis, the removal of pairwise dependencies led to only a -0.93 (avg.) decrease in the F1 score. We further observe in our experiments that our formulation performs better even without explicitly modeling pairwise dependencies; 2) The factorial CRF model gets computationally expensive to train with pairwise dependencies since loopy belief propagation is used for inference.
Therefore, we propose a feature-wise hierarchical neural CRF tagger (Lample et al., 2016; Ma and Hovy, 2016; Yang et al., 2016) with independent predictions for each coarse-grained feature for a given time-step, without explicitly modeling the pairwise dependencies.
2.2.1 Hierarchical Neural CRF model
The hierarchical neural CRF model comprises of two major components, an encoder which com-

bines character and word-level features into a continuous representation and a multi-class multilabel decoder. Given an unlabeled sequence x, the encoder computes the context-senstive hidden representations for each token xi. These representations are shared across |F | independent linearchain CRFs for inference. We refer to this model as MDCRF.

Decoder: Our decoder comprises of |F | independent feature-wise CRFs whose objective function is given as follows:

F
p(y|x) = pf (yf |x)
j=1

pf (yf |x) =

n t=1

ψi

(yf,t−1

,

yf,t

,

x,

t)

Z (x)

where F = {POS, Case, Gender,...} is the set
of coarse-grained features observed in the train-
ing dataset. pf (yf |x) is a feature-wise CRF tagger with ψi(yt−1, yt, x) = exp(Wf Tyf,t−1,yf,t xi + bf yf,t−1,yf,t ) being the energy function for each feature f . During inference the predictions from
each feature-wise decoder is concatenated to-
gether to output the complete morphological anal-
ysis of the sequence x.

Encoder: We adopt a standard hierarchical sequence encoder which is shared among all the |F | feature-wise decoders. It consists of a characterlevel bi-LSTM that computes hidden representations for each token in the sequence. These subword representations help in capturing information about morphological inﬂections. To further enforce this signal, we add a layer of self-attention (Vaswani et al., 2017) on top of the characterlevel bi-LSTM. Self-attention provides each character with a context from all the characters in the token. A bi-LSTM modeling layer is added on top of the self-attention layer which produces a token-level representation. These representations are then concatenated with a word embedding vector and fed to another bi-LSTM to produce context sensitive token representations which are then fed to all the |F | CRFs for inference.

2.2.2 Adding Linguistic Knowledge
Part-of-speech (POS) is perhaps the most important coarse-grained feature. Not only is every token annotated for POS, but most other features depend on it. For instance, verbs do not have Case,

Encoder

Bi-LSTM

POS Word Char
Cumartesi günü

Singapur'dan

Bi-LSTM self-attention
Bi-LSTM d e

Decoder

3

3

_

3 Person CRF

SG

SG

_

SG Number CRF

NOM NOM

_

AT + Case CRF

ABL

Cumartesi günü de Singapur'dan

Figure 1: Hierarchical neural model for contextual morphological analysis with independent CRF decoders for each coarse-grained feature F . For the model MDCRF+POS, POS embeddings are concatenated to the word and char-level representations as depicted above. This model has |F |-1 decoders since POS tagger is run separately as a prior step. MDCRF refers to the above model without POS embeddings having all |F | decoders.

Language: <tr> vector
Token: de

Encoder

Decoder

Wl tl:Typological Feature vector
Figure 2: Polyglot model being used for the token “de” in Turkish, denoted by language vector <tr>.

nouns do not have Tense. In order to leverage these linguistic constraints, we incorporate POS information for each token into our shared encoder. We refer to this variant of the model as MDCRF+POS, as shown in Figure 1.
Since POS tags are not available as input, we ﬁrst run a separate hierarchical neural CRF tagger for POS alone and use the model predictions as input to the MDCRF+POS. For each token, we encode its predicted POS tag into a continuous representation and concatenate it with the character and word-level token representations. Finally, these concatenated representations are fed to the wordlevel bi-LSTM and inference is performed using |F |-1 decoders, excluding the POS decoder. Going forward, we use this model architecture for all our experiments unless otherwise noted.

2.2.3 Multi-lingual Transfer
So far, we have described our model architecture for a monolingual setting. However, the performance of neural models is highly dependent on the availability of large amounts of annotated data, making it challenging to generalize to lowresource languages. Cross-lingual transfer learning attempts to alleviate this challenge by transferring knowledge from high-resource languages. Prior work (Cotterell and Heigold, 2017; Malaviya et al., 2018; Buys and Botha, 2016) has shown the beneﬁts of cross-lingual transfer for morphological tagging. Malaviya et al. (2018) restrict to transferring from one language, whereas Cotterell and Heigold (2017) show that multi-source transfer performs better than single-source. Inspired by this, we experiment with two approaches for multi-lingual transfer learning.
MULTI-SOURCE: In this method, we augment the training data from related languages with the target language data. Similar to Cotterell and Heigold (2017), we perform a hard clustering of languages based on the typological and orthographic similarity of the source languages with the target language. For instance, we construct a language cluster Indo-Aryan, which comprises of all the languages in the dataset that belong to the Indo-Aryan language family which are Hindi, Marathi and Sanskrit. For some larger language

families such as Germanic and Slavic, we construct language clusters from a subset of languages. For instance, the North-Germanic language cluster comprises of treebanks from German, Norwegian, Swedish and Danish. Some languages such as Urdu, Tamil are the only representative languages of their respective language families in the dataset. For these languages, we create a cluster with the next closest language with respect to typology or orthography. For Urdu, we add Hindi because of typological similarity. For other such isolates, we add Turkish because of its extensive agglutination. A total of 24 language clusters were deﬁned based on the literature and with help from a linguist, the details of which can be seen in the Appendix Section §B.
Given a language cluster, all the training data from each language within it is ﬁrst concatenated together. Then, for each language we concatenate the language embedding vector with the token representation in the encoder by adding the language id <LANG ID> at the beginning and end of each sequence. Given a sequence x, the encoder produces contextualized hidden representation hi for each token xi:
hi = Wencoder(ei, ci, pi, li)
where ei is the word embedding vector, ci is the character-level representation, pi is the POS embedding and li is the language embedding vector. This is done to help the model disambiguate languages as often same tokens have different morpho-syntactic description across languages. For example, the token “ ” is a part of both Hindi and Marathi vocabulary. In Hindi it denotes a CONJ whereas in Marathi it is a pronoun with the following description: 3;MASC;PRO;NOM;SG.
POLYGLOT: Languages are often related to multiple languages along different dimensions. For instance, Swedish is lexically similar to German, but it is morpho-syntactically closer to English. To enable a model to utilize these relationships, we feed explicit typological information to the encoder, drawing inspiration from the polyglot model proposed by Tsvetkov et al. (2016). In this multilingual model, we ﬁrst concatenate all the training data from the source languages, similar to the MULTI-SOURCE setting and compute hi for each token. Then context vector hi is factored by the typology feature vector tl to integrate these

manually deﬁned features as follows:

fl = tanh(Wltl + bl)

gil = hi ⊗ flT

where Wl, bl are language-speciﬁc parameters which project the typology vector into a lowdimentional space. Finally, gil computes the global-context language matrix which is vectorized into a column vector and fed to the decoder, as shown in Figure 2.
Tsvetkov et al. (2016) derive their typology vectors from the URIEL database (Littell et al., 2017). We consider a subset of these typology features which are most relevant to the task of morphosyntactic analysis and obtain 18 Syntax-WALS features.2 However, we observed that for most language clusters, these typology feature values within a cluster were not discriminating, which defeats the purpose of using POLYGLOT for disambiguating languages across typological dimensions. Therefore, we construct custom typological vector per each language cluster based on the training data global statistics.
For every coarse-grained feature, this constructed vector contains the proportion of words in the training data that are annotated with that feature. We also experiment with calculating these proportions separately for words for each POS label (N, V, ...). Given the importance of POS, we also include the number of ﬁne-grained POS labels that the most frequent coarse-grained features (Gender, Number, Person, Case) can take. This results in bi-gram features such as N-FEM, NNOM, N-SG. We remove features which do not occur within a given cluster to avoid sparse features. Table 1 shows a portion of the example vector constructed for the Indo-Aryan cluster. From the table we can see that, some features such as ADJ-Gender-FEM and V-Person-1 are present in all the three languages within the cluster. Whereas some features such as ADJ-Gender-NEUT is absent from Hindi because Hindi only has two genders which are MASC and FEM.

2S-SVO, S-SOV, S-VSO, S-VOS, S-OVS, S-OSV,

S-SUBJECT-BEFORE-VERB,

S-SUBJECT-AFTER-

VERB, S-OBJECT-AFTER-VERB, S-OBJECT-BEFORE-

VERB, S-SUBJECT-BEFORE-OBJECT, S-SUBJECT-

AFTER-OBJECT,S-ADPOSITION-BEFORE-NOUN, S-

ADPOSITION-AFTER-NOUN, S-POSSESSOR-BEFORE-

NOUN, S-POSSESSOR-AFTER-NOUN, S-ADJECTIVE-

BEFORE-NOUN, S-ADJECTIVE-AFTER-NOUN

Feature
ADJ-Gender-FEM V-Person-1
ADJ-Gender-NEUT ADJ-Case-DAT/GEN

Hindi
0.054 0.004 0.0 0.0002

Marathi
0.144 0.037 0.144 0.0

Sanskrit
0.080 0.0736 0.159
0.0

Table 1: Example of manually constructed typology features for the Indo-Aryan cluster.

Training Regime: For both the multi-lingual transfer methods, we train one model per language cluster and ﬁne-tune this model for each individual language. which saves time and compute for training 107 individual models from scratch. Furthermore, since a language cluster can have multiple high-resource languages, we take min (5000, #training data-points) for each language to have a tractable training time. We up-sample the lowresource languages to match the number of training data-points of the high-resource languages.
3 Contextual Lemmatization
We use the neural model from Malaviya et al. (2019) for contextual lemmatization. This is a neural sequence-to-sequence model with hard attention, which takes both the inﬂected form and morphological tag set for a token as input and produces a lemma, both at the character level. The decoder uses the concatenation of the previous character and the tag set to produce the next character in the lemma. The lemmatization model is jointly trained with an LSTM-based tagger using jackkniﬁng to reduce exposure bias in training: Malaviya et al. (2019) report signiﬁcantly lower lemmatization results training with gold tags and using predicted tags only at test time. We use their tagger for training and our contextual morphological analysis models’ predicted tags at evaluation time. This model served as the baseline lemmatizer for Task 2; we refer readers to the shared task paper for model details (McCarthy et al., 2019).
4 Experiments
We conduct the following experiments: We compare our multi-lingual transfer approach with the baselines Malaviya et al. (2018) and Cotterell and Heigold (2017) under the same experimental settings. Next, we compare our approach with the shared task baseline (McCarthy et al., 2019). Finally, we analyze the contributions of different components of our proposed method.

Baselines: Cotterell and Heigold (2017) formulate this task as a sequence prediction problem with the output space being the set of all possible tagsets seen in the training data. Speciﬁcally, they construct a neural network based multi-class classiﬁer where each tagset {N;PL;NOM;FEM} forms a class. Since the output space is only restricted to the tagsets seen in the training data, this method cannot generalize to unseen tagsets. Furthermore, for morphologically rich languages such as Russian or Turkish, the output space of the tagset is huge leading to sparse training data. (McCarthy et al., 2019) follow a similar approach.
To overcome these drawbacks Malaviya et al. (2018) consider a feature-wise model which predicts ﬁne-grained labels for corresponding coarse categories {POS,Case,...}. Since morphosyntactic properties are often correlated, they model these inter-dependencies using a factorial CRF and deﬁne two inter-dependencies: 1) a pairwise dependency, which models correlations between the morpho-syntactic properties within a token, and 2) a transition dependency, which models label correlations across all tokens in a sequence. Although this formulation provides the ﬂexibility to produce any combination of tagsets, this model is computationally expensive to train since the factors model dependencies between all labels of all coarse-grained features, leading to >20k factors.
Data processing: We use the train/dev/test split provided in the shared task (McCarthy et al., 2018).3 Since we model feature-wise prediction for each coarse-grained feature, our model requires the provided data to be annotated for coarse-grained features. Therefore, we construct a feature-label dictionary based on the UM documentation4 to map the individual ﬁne-grained traits, which are in the UM schema, to their respective coarse-grained categories. This transforms the tagset {N;PL;NOM;FEM} as {POS=N;Number=PL;Case=NOM;Gender=FEM}. We note that usually a token has a subset of the coarse-grained categories, therefore we extend the morphological tagset for each token by adding the remaining features observed in the training set and assigning them a special value “ ” which denotes null.
3https://github.com/sigmorphon/2019/ tree/master/task2
4https://unimorph.github.io/doc/ unimorph-schema.pdf

Language RU/BG
FI/HU

Model
MDCRF + POS + MULTI-SOURCE (Malaviya et al., 2018)
(Cotterell and Heigold, 2017)
MDCRF + POS + MULTI-SOURCE (Malaviya et al., 2018)
(Cotterell and Heigold, 2017)

tgt-size=100

Accuracy F1-Macro F1-Micro

69.13 46.89 52.76

85.78 64.75 58.23

85.86 64.46 58.41

57.32 45.41 51.74

80.11 68.63 68.15

78.86 68.07 66.82

tgt-size=1,000

Accuracy F1-Macro F1-Micro

82.72 67.56 71.90

92.15 82.06 77.89

92.17 82.11 77.97

70.24 63.93 61.8

85.44 85.06 75.96

84.86 84.12 76.16

Table 2: Comparing our model for bilingual transfer with previous baselines.

Hyper-parameters: We use a hidden size of 200 for each direction of the LSTM with a dropout of 0.5. For the character-level bi-LSTM we use a hidden size of 25. We use 100 dimentional size for word and language embeddings with 64 dimensional POS embeddings, all randomly initialized. SGD was used as the optimizer with learning rate of 0.015. The models were trained until convergence. For POLYGLOT, we project the constructed typology vector into 20 dimension hidden size.
5 Results and Discussion
Table 2 shows the comparison results of our proposed approach with the baselines (Malaviya et al., 2018; Cotterell and Heigold, 2017) using cross-lingual transfer. Here MDCRF+POS refers to our model architecture and MULTI-SOURCE refers to our multi-lingual transfer approach. Malaviya et al. (2018) and Cotterell and Heigold (2017) test their approach on UD v2.1 (Nivre et al., 2017) under two settings: tgt size = 100 and tgt size = 1000, where tgt size denotes the number of target language data-points used during training. Malaviya et al. (2018) transfer from one related high-resource language. We use the same experimental resources for comparison and for a fair comparison we do not ﬁne-tune on the target language. Of the four language pairs tested by Malaviya et al. (2018), we choose RU/BG and FI/HU for comparison, where BG and HU are the target languages and RU and FI are the respective transfer languages, since these languages are morphologically challenging. We see that under both settings our approach outperforms the baselines by a signiﬁcant margin for both the language pairs.
Next, we compare our multi-lingual transfer approaches MULTI-SOURCE and MULTI-SOURCE + POLYGLOT in order to decide the model for our ﬁnal submission. We conduct experiments on three low-resource languages: Marathi (mr-ufal), Sanskrit (sa-ufal) and Belarusian (be-hse), all of

which have < 400 training data-points. The italicized text denotes the treebank used in the experiments. For mr-ufal and sa-ufal, we transfer from a related high-resource language of Hindi (hi-hdtb). For be-hse, we transfer from two related languages, Russian (ru-gsd) and Ukrainian (ukiu). However, from Table 3, we see that the performance of the two models is comparable. Therefore, for our ﬁnal submission we use only MULTISOURCE which is much faster to train than the MULTI-SOURCE + POLYGLOT. We discuss their comparative performance in greater detail in Section §5.1.

Model
MULTI-SOURCE +POLYGLOT

mr-ufal
63.52 / 78.22 61.18 / 77.42

sa-ufal
42.78 / 67.64 43.81 / 65.94

be-hse
77.07 / 82.89 76.51 / 83.27

Table 3: Multi-lingual comparison results for Marathi (mr-ufal), Sanskrit (sa-ufal) and Belarusian (be-hse) on the validation set.

Finally, we compare our approach with the shared task baseline. Table 5, 6 in the Appendix shows our results for all 107 treebanks. We observe that out system achieves an average improvement of +14.70 (accuracy) and +4.63 (F1) over the provided baseline (McCarthy et al., 2019). We note that for the shared task submission, we did not use self-attention over the character-level representations. Therefore, we additionally show the results after adding selfattention. We observe that the addition gives an average improvement of +0.60 (accuracy) and +0.30 (F1) over our previous best submission.
5.1 Analysis
Here we analyze the different components of our model in an effort to understand what it is learning.
Why does adding POS help? As discussed earlier (§2), we explicitly add the POS feature in the form of embeddings into the shared encoder. To

evaluate the contribution of POS alone, we conduct monolingual experiments without concatenating the POS embeddings with the token-level representations. Table 4 outlines the ablation results for three treebanks with varying training size. We observe that our monolingual model MDCRF signiﬁcantly outperforms the baseline (McCarthy et al., 2019) by +13.72 accuracy and +3.82 F1 (avg). On adding POS, we further gain +3.56 accuracy and +0.71 F1 over MDCRF across the three treebanks. We note that this improvement is more pronounced for the low-medium resource languages of Marathi (+6.12 accuracy) and Ukrainian (+3.57 accuracy).

Model
MDCRF+POS MDCRF
McCarthy et al. (2019)

mr-ufal
64.71 / 79.40 58.59 / 77.91 43.76 / 73.38

uk-iu
84.79 / 92.03 81.22 / 91.35 63.36 / 87.01

hi-hdtb
90.46 / 96.69 89.45 / 96.73 80.96 / 94.14

Table 4: Ablation results for Marathi (mr-ufal), Ukrainian (uk-iu) and Hindi (hi-hdtb) with training size of 373, 5441, 13381 respectively on the validation set.

MDCRF MDCRF+POS

# ERRORS 3 2
8 8
72 67 9 8 3 3 54 62
94 77 35 42 67 66 18 14 19 14

POLARITY

MDCRF

4

MDCRF+POS 3

MOOD
4 6

POS VERBFORM TENSE

348

16

14

348

17

16

CAS E
121 121

GENDER
198 197

PERSON
91 106

NUMBER FINITENESS ASPECT

243

65

49

248

78

63

Figure 3: Number of errors per coarse-grained feature for Marathi comparing the addition of POS to the encoder. The rows at the bottom denote the total number of predictions per each feature for both the models.

To understand where the addition of POS helps, we analyse the number of errors made per each coarse-grained feature. For the example of Marathi, POS helped the most in reducing Gender errors (Figure 3). For some word forms, the gender may be inferred from inﬂectional form alone, but for others, this information may be in-
sufﬁcient, e.g. “ कमत ” (price.N.FEM.SG.ACC) in Marathi which does not have the traditional female sufﬁx “ ”. We observe that this behavior corresponds to POS: verbs and adjectives are more predictable from surface forms alone than nouns. The addition of POS information in the encoder helps the model learn to weigh different encoded information more heavily when assigning gender to different parts of speech. For Ukrainian and Sanskrit, POS information also helped reduce errors in Case

and Number. More details can be found in Appendix Section §C.
Tkachenko and Sirts (2018) also model dependence on POS with a POS-dependent context vector in the decoder. However, they observe no signiﬁcant improvement; we hypothesize that incorporating POS information into the shared encoder instead provides the model with a stronger signal.
What is the model learning? One of the major advantages of our model’s use of self-attention is that it enables us to provide insights into what the model has learned. As seen in Figure 4, we found evidence of the model learning languagespeciﬁc inﬂectional properties. Both Marathi and Belarusian display morphological inﬂections predominantly in the form of sufﬁx and the attention maps for both these languages demonstrate the same. For the Marathi example, the last three characters denote the ergative case and we can see that the attention weights are concentrated on these three characters. Similarly for the Belarusian example, the last two characters denote the genitive case with plural number and is the focus of the attention. For Indonesian, inﬂections can be also found as circumﬁxes where the afﬁx is attached at both the beginning and end of the token. For instance, both ke- and -an afﬁxes are appended to form nouns and we can see from Figure 4 that the attention is focused both on the preﬁx and the sufﬁx. Interestingly for Indonesian, the model seems to have also discovered the stem camat, as evidenced from the attention pattern.
Does time-depth matter for transfer learning? As discussed earlier, we train one model per language cluster for multi-lingual transfer learning. We compare different clusters to see if time-depth of the languages within a cluster affects the extent of transfer. Time depth is the period of time that has elapsed since all languages in the group were a single language (in other words, the time since divergence). We consider the following three clusters: Hindi-Marathi-Sanskrit (Indo-Aryan), Russian-Ukrainian-Belarusian (Slavic) and Arabic-Hebrew-Amharic-Akkadian (Semitic). These three clusters were chosen because the languages in them became separate languages at varying time-depths. For instance, in the Semitic cluster the languages diverged

नोकराने = नोकर
‘servant’ servant.N.MASC.SG
न

+ ◌ा न ◌े
by.ERG

◌ो क र ◌ा

न
◌े न

◌ो क र ◌ा
(a) Marathi

न ◌े

kecamatan = ke + ‘district office’ NOM

camat district.head.N

+ an NOM

выпадкаў = выпадак

+ аў

‘of occassions’ occassion.MASC.N.INAN GEN.PL

(b) Indonesian

(c) Belarusian

Figure 4: Character-level attention maps for three typologically different languages. Marathi and Belarusian display morphological inﬂections pre-dominantly as sufﬁx. Indonesian displays inﬂections in the form of preﬁx, sufﬁx and circumﬁx where the afﬁx is found both at the beginning and end of a token.

roughly 5000 years ago, whereas for the Slavic cluster the time-depth is <1000 years. Therefore, we expect transfer to help more for languages where the time-depth is more recent. In Figure 5, we compare the MULTI-SOURCE model with our best mono-lingual model MDCRF+POS and we see that transfer helps most for the Slavic cluster by +2.9 accuracy. For the Indo-Aryan cluster it helps by +0.32 accuracy and for the Semitic cluster we observe a slight negative effect with transfer (-0.0176 accuracy). This supports our hypothesis that time-depth does affect the extent of transfer learning with language clusters having lower time-depths beneﬁting the most.
One particular advantage that the Slavic cluster has over both the Indo-Aryan and Semitic clusters is the similarity of script. Russian, Belarusian, and Ukrainian use variants of the same script; Hindi, Sanskrit, and Marathi do, as well, but the Semitic languages all use different scripts. This is also attributed to the shallower time-depths of the Slavic and Indo-Aryan clusters. Therefore, as suggested by the anonymous reviewers, we add Czech and Polish to the Slavic cluster and see to what extent the scripts are confusing the model. Czech and Polish use different script as compared to Russian, Belarusian, and Ukrainian. We observe that MULTI-SOURCE model like before, achieves similar improvements over the monolingual models for Belarusian (+8.17 accuracy) and Ukrainian (+1.2 accuracy). However, a slight decrease is observed for Russian ( -0.45 accuracy). This suggests that

the MULTI-SOURCE model is robust to scriptal changes and beneﬁts the low-resource languages by learning from typologically similar languages, more so for language clusters with shallow timedepths.

8

6.89

6

Absolute gain

4 1.85

2 0.23 0 0.75

0.82 0.99 0.36

0.2

0

-2

-0.17

-4 mr_ufal sa_ufal hi_hdtb be_hse ru_gsd

k_iu

padt

_pud

_htb

-3.12 dub

_att

u ar_ ar he isan amh

ak_p

Figure 5: Absolute gain of multi-lingual transfer over monolingual models. Blue denotes the Indo-Aryan cluster, pink the Slavic, and yellow the Semitic.

Why did POLYGLOT not help further? We hypothesize that one reason why POLYGLOT did not help over MULTI-SOURCE is because the language embedding vector probably learns the same typological information which the typology vector encodes. Hence, the typological vector doesn’t seem to add any new information. As evidence, we look at the transition weights learned in both the models; as shown in Figure 8, we see that the transition weights learned for the Case feature are very similar for both MULTI-SOURCE and MULTI-SOURCE

AT+A_ BL

DAT GEN VOC ERG {ACC CO /INS M
}

INS

AT}

{ACC/GE RG} C/D N}

{AC CC/E SS

S} E

+ES M

C/IN NO

{AC ACC

AT+A_ BL

DAT GEN VOC ERG {ACC CO /INS M
}

INS

AT}

{ACC/GE RG} C/D N}

{AC CC/E SS

S} E

+ES M

C/IN NO

{AC ACC

{A

{A

Figure 6: Transition weights for the Case feature for Hindi across MULTI-SOURCE (left) and MULTI-SOURCE + POLYGLOT (right) models trained with Hindi (hi-hdtb), Marathi (mr-ufal) and Sanskrit (sa-ufal).

+ POLYGLOT. In the future, we plan to explore the contextual parameter generation method (Platanios et al., 2018) for leveraging the typology vectors to inform the decoders during inference.
5.2 Error Analysis
In this section, we analyze the major error categories for the MULTI-SOURCE model for the IndoAryan cluster. We observe that Gender, Case, Number, Person features account for the most number of errors (65% for Marathi, 49% for Sanskrit). One reason for this is the non-overlapping output label space across the languages within a cluster. For instance, in the Indo-Aryan cluster, Hindi is a high-resource language (> 13k training sentences) with Marathi (373) and Sanskrit (184) being the low-resource languages. We observe that the label space for Case, Gender, Number overlap the least among the three languages. Marathi and Sanskrit have three genders: NEUT, FEM, MASC whereas Hindi only has FEM, MASC. Furthermore, only two Hindi Case labels (ACC, NOM) overlap with Marathi and Sanskrit because in Hindi the labels often have alternatives such as ACC/ERG, ACC/DAT. These differences in the output space negatively affect the transfer. For the Slavic cluster, we observe that almost all the feature labels overlap nicely for the languages therein, which is probably another reason why we see a gain of +6.89 for Belarusian in Figure 5 and only +0.32 increase for Marathi.
We also note that for some languages such as Belarusian and Russian, the POS errors in-

creased by 25.3% and 4.4% respectively for the MDCRF+POS model. This suggests that decoupling POS feature from the other feature decoders harmed the model. In future, we plan to improve the MDCRF+POS model by jointly training POS decoder with the other feature decoders which use the latent representation of POS in an end-to-end fashion.
6 Conclusion and Future Work
We implement a hierarchical neural model with independent decoders for each coarse-grained morphological feature and show that incorporating POS information in the shared encoder helps improve prediction for other features. Furthermore, our multi-lingual transfer methods not only help improve results for related languages but also eliminate the need of training individual models for each dataset from scratch. In future, we plan to explore the use of pre-trained multi-lingual word embeddings such as BERT (Devlin et al., 2019), in our encoder.
Acknowledgement
We are thankful to the anonymous reviewers for their valuable suggestions. This material is based upon work supported by the National Science Foundation under Grant No. IIS1812327.
References
Jan Buys and Jan A. Botha. 2016. Cross-lingual morphological tagging for low-resource languages. In

Proc. of ACL, pages 1954–1964.
Ryan Cotterell and Georg Heigold. 2017. Crosslingual character-level neural morphological tagging. In Proc. of EMNLP, pages 748–759.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proc. of NAACL, pages 4171–4186.
Onur Gu¨ngo¨r, Suzan U¨ sku¨darlı, and Tunga Gu¨ngo¨r. 2018. Improving named entity recognition by jointly learning to disambiguate morphological tags. arXiv preprint arXiv:1807.06683.
Jan Hajic and Barbora Hladka´. 1998. Tagging inlective languages: Prediction of morphological categories for a rich structured tagset. In Proc. of ACL, volume 1.
Georg Heigold, Guenter Neumann, and Josef van Genabith. 2017. An extensive empirical evaluation of character-based morphological tagging for 14 languages. In Proc. of EACL, pages 505–513.
Daniel Kondratyuk. 2019. 75 languages, 1 model: Parsing universal dependencies universally. arXiv preprint arXiv:1904.02099.
Guillaume Lample, Miguel Ballesteros, Sandeep Subramanian, Kazuya Kawakami, and Chris Dyer. 2016. Neural architectures for named entity recognition. In Proc. of NAACL, pages 260–270.
Patrick Littell, David R Mortensen, Ke Lin, Katherine Kairis, Carlisle Turner, and Lori Levin. 2017. Uriel and lang2vec: Representing languages as typological, geographical, and phylogenetic vectors. In Proc. of EACL, volume 2, pages 8–14.
Xuezhe Ma and Eduard Hovy. 2016. End-to-end sequence labeling via bi-directional LSTM-CNNsCRF. In Proc. of ACL, pages 1064–1074.
Chaitanya Malaviya, Matthew R. Gormley, and Graham Neubig. 2018. Neural factor graph models for cross-lingual morphological tagging. In Proc. of ACL, pages 2653–2663.
Chaitanya Malaviya, Shijie Wu, and Ryan Cotterell. 2019. A simple joint model for improved contextual neural lemmatization. arXiv preprint arXiv:1904.02306v2.
Arya D. McCarthy, Miikka Silfverberg, Ryan Cotterell, Mans Hulden, and David Yarowsky. 2018. Marrying Universal Dependencies and Universal Morphology. In Proceedings of the Second Workshop on Universal Dependencies (UDW 2018), pages 91–101.
Arya D. McCarthy, Ekaterina Vylomova, Shijie Wu, Chaitanya Malaviya, Lawrence Wolf-Sonkin, Garrett Nicolai, Christo Kirov, Miikka Silfverberg, Sebastian Mielke, Jeffrey Heinz, Ryan Cotterell, and Mans Hulden. 2019. The SIGMORPHON 2019

shared task: Crosslinguality and context in morphology. In Proceedings of the 16th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology.
Thomas Mu¨ller and Hinrich Schuetze. 2015. Robust morphological tagging with word representations. In Proc. of NAACL, pages 526–536.
Joakim Nivre, Zˇ eljko Agic´, Lars Ahrenberg, Lene Antonsen, Maria Jesus Aranzabe, Masayuki Asahara, Luma Ateyah, Mohammed Attia, Aitziber Atutxa, Liesbeth Augustinus, et al. 2017. Universal dependencies 2.1.
Kemal Oﬂazer and Ilker Kuruo¨z. 1994. Tagging and morphological disambiguation of turkish text. In Proc. of ANLP, pages 144–149.
Emmanouil Antonios Platanios, Mrinmaya Sachan, Graham Neubig, and Tom Mitchell. 2018. Contextual parameter generation for universal neural machine translation. In Proc. of EMNLP, pages 425– 435.
Emma Strubell, Patrick Verga, Daniel Andor, David Weiss, and Andrew McCallum. 2018. Linguistically-informed self-attention for semantic role labeling. In Proc. of EMNLP, pages 5027–5038.
Alexander Tkachenko and Kairit Sirts. 2018. Modeling composite labels for neural morphological tagging. arXiv preprint arXiv:1810.08815.
Yulia Tsvetkov, Sunayana Sitaram, Manaal Faruqui, Guillaume Lample, Patrick Littell, David Mortensen, Alan W. Black, Lori Levin, and Chris Dyer. 2016. Polyglot neural language models: A case study in cross-lingual phonetic representation learning. In Proc. of NAACL, pages 1357–1366.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Proc. of NIPS, pages 5998–6008.
Ekaterina Vylomova, Trevor Cohn, Xuanli He, and Gholamreza Haffari. 2017. Word representation models for morphologically rich languages in neural machine translation. In Proceedings of the First Workshop on Subword and Character Level Models in NLP, pages 103–108.
Zhilin Yang, Ruslan Salakhutdinov, and William Cohen. 2016. Multi-task cross-lingual sequence tagging from scratch. arXiv preprint arXiv:1603.06270.

Appendix
A Comprehensive Results
Table 5 and 6 document the comprehensive results of our submissions. MULTI-SOURCE was our previous submission to the shared task. We conducted additional experimentas with the addition of selfattention and also report the results for MULTISOURCE+SELF-ATTENTION. We report both the accuracy and F1 metric.
B Language Clusters
We train one model per language cluster for the multi-lingual transfer learning. Each language cluster was constructed based on the typological and orthographic similarity of the languages therein. Table 5, 6 show details of the language clusters. Figure 7 shows clusters graphically by relative size per dataset.
C Analysis
In order to understand where the addition of POS helps, we plot the number of errors per each coarse-grained feature for three languages in Figure 8. For Sanskrit and Ukrainian we see that POS generally helps reduce the errors predominantly for the features: Case, Gender, Number. For Belarusian, we did not observe a clear trend since the POS accuracy actually decreased for MDCRF+POS.

Language Cluster armenian
austronesian baltic celtic
dravidian egyptian germanic
northgermanic
hellenic indo-iranian indoaryan
isolate italic jako
niger-congo persian phillipine
sinotibetan
semitic

Target
UD-Armenian-ArmTDP
UD-Indonesian-GSD
UD-Latvian-LVTB UD-Lithuanian-HSE
UD-Breton-KEB UD-Irish-IDT
UD-Tamil-TTB
UD-Coptic-Scriptorium
UD-Afrikaans-AfriBooms UD-Dutch-Alpino UD-Dutch-LassySmall UD-English-EWT UD-English-GUM UD-English-LinES UD-English-ParTUT UD-English-PUD UD-Faroese-OFT UD-Gothic-PROIEL
UD-German-GSD UD-Danish-DDT UD-Norwegian-Nynorsk UD-Norwegian-NynorskLIA UD-Swedish-LinES UD-Swedish-PUD
UD-Ancient-Greek-Perseus UD-Ancient-Greek-PROIEL UD-Greek-GDT
UD-Urdu-UDTB
UD-Hindi-HDTB UD-Marathi-UFAL UD-Sanskrit-UFAL
UD-Basque-BDT
UD-Latin-ITTB UD-Latin-Perseus UD-Latin-PROIEL
UD-Japanese-GSD UD-Japanese-Modern UD-Japanese-PUD UD-Komi-Zyrian-IKDP UD-Komi-Zyrian-Lattice UD-Korean-GSD UD-Korean-Kaist UD-Korean-PUD UD-Kurmanji-MG
UD-Bambara-CRB UD-Naija-NSC UD-Yoruba-YTB
UD-Persian-Seraji
UD-Tagalog-TRG
UD-Cantonese-HK UD-Chinese-CFL UD-Chinese-GSD UD-Vietnamese-VTB
UD-Akkadian-PISANDUB UD-Amharic-ATT UD-Arabic-PADT UD-Arabic-PUD UD-Hebrew-HTB

MULTI-SOURCE + SELF-ATTENTION
Accuracy / F1
83.74 / 88.54
90.05 / 93.13
89.0 / 93.04 70.29 / 76.38
85.97 / 88.78 76.75 / 84.1
82.92 / 89.91
92.02 / 95.28
96.92 / 97.37 94.85 / 95.69 93.48 / 94.08 94.08 / 95.46 93.44 / 94.38 94.37 / 95.19 92.01 / 92.69 89.41 / 91.42 80.6 / 89.27 84.53 / 92.93
83.72 / 92.73 91.78 / 93.72 94.39 / 96.35 93.03 / 94.55 89.92 / 93.61 87.72 / 90.01
84.79 / 92.1 88.1 / 95.55 91.15 / 96.23
77.77 / 92.12
90.76 / 96.77 57.99 / 73.54 43.72 / 64.9
75.2 / 88.07
94.57 / 97.26 76.17 / 86.32 86.78 / 94.39
96.8 / 96.4 95.27 / 95.32 95.94 / 95.44 51.56 / 61.03 53.85 / 64.85 92.56 / 91.68 95.54 / 94.99 84.27 / 89.02 80.82 / 87.79
91.65 / 94.76 94.56 / 92.71 93.41 / 93.88
96.15 / 96.85
83.78 / 92.09
89.64 / 86.82 88.65 / 86.96 90.83 / 90.54 90.1 / 88.84
79.21 / 78.65 87.24 / 91.13 91.77 / 95.44 77.63 / 89.06 94.33 / 95.81

MULTI-SOURCE
Accuracy / F1
83.83 / 88.17
90.01 / 93.11
89.0 / 93.08 68.08 / 74.56
85.07 / 88.07 76.5 / 84.11
82.48 / 89.77
92.17 / 95.33
96.94 / 97.35 94.35 / 95.4 93.53 / 94.2 93.9 / 95.4 93.56 / 94.47 93.75 / 94.93 91.95 / 92.61 89.8 / 91.6 77.52 / 87.87 83.0 / 92.47
82.82 / 92.5 91.34 / 93.61 94.29 / 96.33 93.75 / 94.89 89.62 / 93.59 87.13 / 89.8
84.27 / 91.88 86.01 / 94.67 90.73 / 96.0
78.05 / 92.16
91.05 / 96.85 57.72 / 73.04 46.73 / 68.08
75.14 / 87.91
94.25 / 97.11 75.76 / 85.92 86.18 / 94.19
96.8 / 96.4 95.27 / 95.32 95.94 / 95.44 51.56 / 62.27 54.4 / 65.23 92.56 / 91.68 95.54 / 94.99 84.46 / 89.28 80.82 / 87.81
92.41 / 94.86 94.56 / 92.71 93.8 / 94.19
95.95 / 96.69
83.78 / 92.75
89.64 / 86.82 88.65 / 86.96 90.9 / 90.56 90.1 / 88.84
79.21 / 78.65 86.58 / 90.91 91.52 / 95.36 77.89 / 89.0 94.03 / 95.65

(McCarthy et al., 2019)
Accuracy / F1
-/-
71.49 / 86.02
70.21 / 89.53 43.13 / 67.41
77.41 / 88.58 67.45 / 81.72
75.64 / 90.23
87.99 / 93.78
84.05 / 92.32 82.15 / 91.26 76.24 / 88.13 79.19 / 90.46 79.63 / 90.04 81.03 / 90.99 79.57 / 89.04 78.85 / 88.8 67.11 / 87.27 83.01 / 91.3
-/77.89 / 90.89 71.8 / 88.16
-/77.97 / 91.02 77.78 / 89.32
-/-/78.14 / 93.49
67.99 / 88.42
80.96 / 94.14 43.76 / 73.38 44.33 / 68.34
67.61 / 87.63
77.62 / 93.19 53.23 / 77.5 82.27 / 91.38
85.25 / 90.31 94.29 / 95.2 84.73 / 89.63 33.73 / 62.59 45.6 / 70.61 80.18 / 86.08 84.32 / 89.4 81.6 / 91.15 70.2 / 85.85
78.86 / 89.41 68.66 / 78.96 71.2 / 81.83
-/-
44.0 / 69.31
70.15 / 77.76 74.65 / 79.91 76.81 / 84.35 70.71 / 79.01
84.0 / 84.19 76.0 / 88.16 77.03 / 92.03 63.81 / 86.29 81.59 / 91.84

# Training Sentences
825
4475
7937 211
711 817
481
673
1548 10867 5873 13298 3520 3652 1673 801 967 4321
12473 4410 14061 1117 3652 801
11136 13665 2017
4105
13318 373 185
7195
16809 1819 14721
6557 658 801 70 153 5072 21891 801 604
821 759 81
4798
45
521 361 3998 2401
81 860 6132 801 4973

Table 5: Comprehensive results

Cluster turkic romance
slavic
ugric westslavic

Target
UD-Turkish-IMST UD-Turkish-PUD
UD-Catalan-AnCora UD-French-GSD UD-French-ParTUT UD-French-Sequoia UD-French-Spoken UD-Galician-CTG UD-Galician-TreeGal UD-Italian-ISDT UD-Italian-ParTUT UD-Italian-PoSTWITA UD-Italian-PUD UD-Portuguese-Bosque UD-Portuguese-GSD UD-Romanian-Nonstandard UD-Romanian-RRT UD-Spanish-AnCora UD-Spanish-GSD
UD-Belarusian-HSE UD-Bulgarian-BTB UD-Buryat-BDT UD-Old-Church-Slavonic-PROIEL UD-Russian-GSD UD-Russian-PUD UD-Russian-SynTagRus UD-Russian-Taiga UD-Ukrainian-IU UD-Upper-Sorbian-UFAL
UD-Estonian-EDT UD-Finnish-FTB UD-Finnish-PUD UD-Hungarian-Szeged UD-North-Sami-Giella UD-Norwegian-Bokmaal UD-Swedish-Talbanken UD-Finnish-TDT
UD-Croatian-SET UD-Czech-CAC UD-Czech-CLTT UD-Czech-FicTree UD-Czech-PDT UD-Czech-PUD UD-Polish-LFG UD-Polish-SZ UD-Serbian-SET UD-Slovak-SNK UD-Slovenian-SSJ UD-Slovenian-SST

MULTI-SOURCE + SELF-ATTENTION
Accuracy / F1
85.68 / 90.64 79.78 / 90.88
textbf96.68 / 98.26 96.19 / 97.51 93.04 / 96.05 95.08 / 96.95 96.05 / 96.08 96.65 / 96.31 89.69 / 93.2 95.91 / 97.24 95.0 / 96.39 92.13 / 93.13 87.55 / 92.4 92.28 / 95.57 97.33 / 97.54 91.13 / 95.33 94.67 / 96.58 96.97 / 98.25 94.05 / 97.08
79.63 / 85.37 94.22 / 96.44 78.85 / 81.24 87.22 / 94.13 84.26 / 91.91 76.77 / 87.55 91.65 / 95.96 74.14 / 80.23 86.02 / 92.41 74.04 / 82.45
87.71 / 94.58 83.24 / 90.38 77.05 / 86.33 80.57 / 90.88 84.35 / 88.8 94.97 / 96.68 93.94 / 96.01 86.51 / 92.63
87.23 / 94.04 90.66 / 96.72 91.29 / 96.15 90.05 / 95.42 89.78 / 96.37 75.65 / 88.19 87.76 / 93.7 82.27 / 91.38 91.89 / 95.46 85.59 / 93.12 89.05 / 94.03 85.13 / 90.16

MULTI-SOURCE
Accuracy / F1
85.02 / 90.43 79.33 / 90.54
96.63 / 98.24 95.76 / 97.32 93.04 / 96.12 94.96 / 96.96 96.05 / 96.08 96.66 / 96.32 89.3 / 93.25 95.96 / 97.27 94.87 / 96.39 92.03 / 93.02 87.38 / 92.46 92.06 / 95.5 97.33 / 97.54 91.07 / 95.29 94.82 / 96.63 96.86 / 98.22 94.07 / 97.1
77.28 / 84.11 93.99 / 96.37 75.96 / 78.66 86.94 / 94.03 83.25 / 91.55 77.25 / 87.49 92.74 / 96.5 75.24 / 81.25 85.33 / 92.2 70.12 / 81.21
88.47 / 94.93 83.63 / 90.7 77.49 / 86.77 79.16 / 90.13 83.78 / 88.65 94.58 / 96.51 93.64 / 95.9 85.55 / 92.2
86.88 / 93.91 91.38 / 96.99 91.07 / 96.22 90.0 / 95.49 54.13 / 73.56 77.72 / 89.37 87.81 / 93.65 81.01 / 90.88 91.35 / 95.29 84.99 / 92.83 87.92 / 93.55 85.51 / 90.02

(McCarthy et al., 2019)
Accuracy / F1
62.04 / 85.33 66.92 / 88.05
85.77 / 95.7 84.44 / 94.81 81.32 / 92.08 82.64 / 93.42 94.57 / 94.85 87.23 / 91.81 76.85 / 90.05 83.62 / 94.34 84.03 / 93.42 70.23 / 88.18 80.89 / 92.66 63.14 / 86.12
-/74.31 / 91.5 81.45 / 93.96 84.27 / 95.3
-/-
54.99 / 79.07 79.75 / 93.91 63.26 / 78.53 82.86 / 90.34 64.42 / 88.77 63.15 / 85.52 73.9 / 92.84 52.99 / 78.71 63.36 / 87.01 55.66 / 78.3
74.56 / 91.71 73.16 / 89.51 71.65 / 88.87 63.72 / 87.29 67.04 / 85.6 81.44 / 93.19
-/75.13 / 90.92
72.71 / 90.99 77.15 / 93.92 73.92 / 92.37 68.28 / 90.37 76.69 / 94.28 59.54 / 85.5
-/65.58 / 88.29 75.73 / 91.19 64.24 / 88.16 73.73 / 89.95 73.4 / 84.74

# Training Sentences
4509 801
13343 13074 817 2480 2229 3195 801 11334 1673 5371 801 7493 9663 8056 7620 14145 12811
315 8911 742 5070 4025 801 49512 1412 5441 517
24579 14979 801 1441 2498 16037 4821 12109
7112 19768 901 10209 70331 801 13797 6582 3113 8484 6401 2551

Table 6: Comprehensive results

Figure 7: Language family clusters, by number of sentences per dataset.

MDCRF MRCRF+POS

# ERRORS 0 0
17 29 71 89
7 6
30 39 98 117
0 3
81 85
18 15
21 35 75 75 67 52
21 35 38 48

POLA IRT Y

MDCRF

9

MDCRF+POS 9

MOOD
49 69

POS
590 590

V ERBFORM
1 6

TENSE
44 74

CA S E
366 346

COM PA RIS ON
3 3

GENDER
340 330

PE RS ON
27 45

V OICE
73 95

A NIM A CY
247 244

NUMBER
423 410

FINITENESS
66 87

A S PE CT
73 95

(a) Belarusian (be-hse)
MDCRF MDCRF+POS

# ERRORS 3
2 90 104 585 615
27 23
94 101
1018 856 24 20 0 2 640 568 64 67 30 25 622 603 416 330 95 111 290 288

POLA RIT Y

MDCRF

128

MDCRF+POS 129

MOOD
995 1053

POS
9182 9185

V ERBFORM
140 148

TENSE
987 1044

CASE COMPARISON V ALENCY

6467 6435

58

46

70

46

GENDER
5004 4957

PE RS ON
805 855

V OICE
103 113

A NIM A CY
3619 3602

NUMBER
6202 6177

FINITENESS
1216 1280

A S PE CT
1333 1427

(b) Ukrainian (uk-iu)

MDCRF MDCRF+POS

# ERRORS 1 1
8 9
68 60 1 1 19 19 19 23 61 51
66 62 8 9 22 26 40 37 6 8 2 2

POLA RIT Y

MDCRF

6

MDCRF+POS 6

MOOD
12 21

POS

POLITENESS V ERBFORM TENSE

178

0

178

0

1

9

9

18

CA S E
102 106

GENDER
99 105

PE RS ON
15 22

V OICE
14 31

NUMBER
141 134

FINITENESS
12 22

A S PE CT
7 7

(c) Sanskrit (sa-ufal)

Figure 8: Number of errors per coarse-grained feature for models comparing the addition of POS to the encoder. The rows at the bottom denote the total number of predictions per each feature for both the models.

