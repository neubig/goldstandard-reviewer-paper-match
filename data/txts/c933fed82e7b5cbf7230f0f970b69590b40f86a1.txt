A Uniﬁed Theory of Decentralized SGD with Changing Topology and Local Updates

arXiv:2003.10422v3 [cs.LG] 2 Mar 2021

Anastasia Koloskova * 1 Nicolas Loizou 2 Sadra Boreiri 1 Martin Jaggi 1 Sebastian U. Stich * 1

Abstract
Decentralized stochastic optimization methods have gained a lot of attention recently, mainly because of their cheap per iteration cost, data locality, and their communication-efﬁciency. In this paper we introduce a uniﬁed convergence analysis that covers a large variety of decentralized SGD methods which so far have required different intuitions, have different applications, and which have been developed separately in various communities. Our algorithmic framework covers local SGD updates and synchronous and pairwise gossip updates on adaptive network topology. We derive universal convergence rates for smooth (convex and non-convex) problems and the rates interpolate between the heterogeneous (non-identically distributed data) and iid-data settings, recovering linear convergence rates in many special cases, for instance for over-parametrized models. Our proofs rely on weak assumptions (typically improving over prior work in several aspects) and recover (and improve) the best known complexity results for a host of important scenarios, such as for instance coorperative SGD and federated averaging (local SGD).
1. Introduction
Training machine learning models in a non-centralized fashion can offer many advantages over traditional centralized approaches in core aspects such as data ownership, privacy, fault tolerance and scalability. In efforts to depart from the traditional parameter server paradigm (Dean et al., 2012), federated learning (Konecˇny` et al., 2016; McMahan
*Equal contribution 1EPFL, Lausanne, Switzerland 2Mila and DIRO, Universite´ de Montre´al, Canada. Correspondence to: Anastasia Koloskova <anastasia.koloskova@epﬂ.ch>, Sebastian U. Stich <sebastian.stich@epﬂ.ch>.
First version appeared in proceedings of the 37 th International Conference on Machine Learning, Online, PMLR 119, 2020. Copyright 2020 by the author(s).

et al., 2016; 2017; Kairouz et al., 2019) has emerged, but also fully decentralized approaches have been suggested recently—though yet still at a smaller scale than federated learning (Lian et al., 2017; Assran et al., 2019; Koloskova et al., 2020). However, the community has identiﬁed a host of challenges that come along with decentralized training: notably, high communication cost (Tang et al., 2018a; Wang et al., 2019; Koloskova et al., 2019), a need for time-varying topologies (Nedic´ & Olshevsky, 2014; Assran et al., 2019) and data-heterogeneity (Li et al., 2018; Karimireddy et al., 2019; Li et al., 2020a;b). It is imperative to have a good theoretical understanding of decentralized stochastic gradient descent (SGD) to predict the training performance of SGD in these scenarios and to assist the design of optimal decentralized training schemes for machine learning tasks.
In contrast to the centralized setting, where the convergence of SGD is well understood (Bach & Moulines, 2011; Rakhlin et al., 2012; Dekel et al., 2012), the analyses of SGD in non-centralized settings are often application speciﬁc and have been historically developed separately in different communities, besides some recent efforts towards a uniﬁed theory. Notably, Wang & Joshi (2018) propose a framework for decentralized optimization with non-heterogeneous data and Li et al. (2019) study decentralized SGD for non-convex heterogeneous settings. We here propose a signiﬁcantly extended framework that covers these previously proposed ones as special cases.
We provide tight convergence rates for a large family of decentralized SGD variants. Proving convergence rates in a uniﬁed framework is much more powerful than studying individual special cases on their own: We are not only able to recover many existing analyses and results, we can also often show improved rates under more general setting. Remarkably, for instance for local SGD (Zinkevich et al., 2010; Stich, 2019b; Patel & Dieuleveut, 2019) we show improved rates for the convex and strongly-convex case and recover the best known rates for the non-convex case under weaker assumptions than assumed in prior work (highlighted in Table 1).

A Uniﬁed Theory of Decentralized SGD

1.1. Contributions
• We present a uniﬁed framework for gossip based decentralized SGD methods that captures local updates and time-varying, randomly sampled, mixing distributions. Our framework covers a rich class of methods that previously needed individual convergence analyses.
• Our theoretical results rely on weak assumptions that measure the strength of the noise and the dissimilarity of the functions between workers and a novel assumption on the expected mixing rate of the gossip algorithm. This provides us with great ﬂexibility on how to select the topology of the network and the mixing weights.
• We demonstrate the effectiveness and tightness of our results by exemplary showing that our framework gives the best convergence rates for local SGD for both, heterogeneous and iid. data settings, improving over all previous analyses on convex functions.
• We provide a lower bound that conﬁrms that our convergence rates are tight on strongly convex functions.
• We empirically verify the tightness of our theoretical results on strongly convex functions and explain the impact of noise and data diversity on the convergence.
2. Related Work
The study of decentralized optimization algorithms can be tracked back at least to (Tsitsiklis, 1984). For the problem of computing aggregates (ﬁnding consensus) among clients, various gossip-based protocols have been proposed. For instance the push-sum algorithm (Kempe et al., 2003), based on the intuition of mixing in Markov chains and allowing for asymmetric communication, or the symmetric randomized gossip protocol for averaging over arbirary graphs (Xiao & Boyd, 2004; Boyd et al., 2006) that we follow closely in this work. For general optimization problems, the most common algorithms are either combinations of standard gradient based methods with gossip-type averaging step (Nedic´ & Ozdaglar, 2009; Johansson et al., 2010), or speciﬁcally designed methods relying on problem structure, such as alternating direction method of multipliers (ADMM) (Wei & Ozdaglar, 2012; Iutzeler et al., 2013), dual averaging (Duchi et al., 2012; Nedic´ et al., 2015; Rabbat, 2015), primal-dual methods (Alghunaim & Sayed, 2019), or block-coordinate methods for generalized linear models (He et al., 2018). There is a rich literature in the control community that discusses various special cases—motivated by particular applications—such as for instance asynchronity (Boyd et al., 2006) or time-varying graphs (Nedic´ & Olshevsky, 2014; Nedic´ & Olshevsky, 2016), see also (Nedic´ et al., 2018) for an overview.

For the deterministic (non-stochastic) descentralized optimization a recent line of work developed optimal algorithms based on acceleration (Jakovetic´ et al., 2014; Scaman et al., 2017; 2018; Uribe et al., 2018; Fallah et al., 2019). In the machine learning context, decentralized implementations of stochastic gradient descent have gained a lot of attention recently (Lian et al., 2017; Tang et al., 2018b; Assran et al., 2019; Koloskova et al., 2020), especially for the particular (but not fully decentralized) case of a star-shaped network topology, the federated learning setting (Konecˇny` et al., 2016; McMahan et al., 2016; 2017; Kairouz et al., 2019). Rates for the stochastic optimization are derived in (Shamir & Srebro, 2014; Rabbat, 2015), under the assumption that the distributions on all nodes are equal. However, this is a very strong assumption for practical problems.
It has been noted quite early that decentralized gradient based methods in heterogenous data setting suffer from a ‘client-drift’, i.e. the diversity in the functions on each node leads to a drift on each client towards the minima of fi—potentially far away from the global minima of f . This phenomena has been discussed (and sometimes been adressed by modiﬁng the SGD updates) for example in (Shi et al., 2015; Lee et al., 2015; Nedic´ et al., 2016) and been rediscovered frequently in the context of stochastic optimization (Zhao et al., 2018; Karimireddy et al., 2019). It is important to note that in analyses based on the bounded gradient assumption—which was traditionally assumend for analyzing SGD (Lacoste-Julien et al., 2012; Rakhlin et al., 2012)—the diversity in the data distribution on each worker sometimes can be hidden in this generous upper bound and the analyses cannot distinguish between iid. and non-iid. data cases, such as e.g. in (Koloskova et al., 2019; Nadiradze et al., 2019; Li et al., 2020b). In this work, we use much weaker assumptions and we show how the convergence rate depends on the similarity between the functions (by providing matching lower and upper bounds). Our results show that in overparametrized settings no drift effects occur and linear convergence can be achieved similar as to the centralized setting (Schmidt & Roux, 2013; Needell et al., 2016; Ma et al., 2018).
For reducing communication cost, various techniques have been proposed. In this work we do not consider gradient compression techniques (Alistarh et al., 2017; Stich et al., 2018; Tang et al., 2018a; 2019; Stich & Karimireddy, 2019)—but such orthogonal techniques could be added on top of our scheme—and instead only focus on local updates steps which are often efﬁcient in practice but challenging to handle in the theoretical analysis (McMahan et al., 2017; Stich, 2019b; Yu et al., 2019; Lin et al., 2020).

A Uniﬁed Theory of Decentralized SGD

3. Setup

We study the distributed stochastic optimization problem

1n

f := min f (x) :=

fi(x)

(1)

x∈Rd

n

i=1

where the components fi : Rd → R are distributed among n nodes and are given in stochastic form:

fi(x) := Eξi∼Di Fi(x, ξi),

(2)

where Di denotes the distribution of ξi over parameter

space Ωi on node i. Standard empirical risk minimization

is an important special case of this problem, when each Di

presents

a

ﬁnite

number

mi

of

elements

{

ξi1

,

.

.

.

,

ξ

m i

i

}.

Then fi can be rewritten as fi(x) = m1i m j=i1 Fi(x, ξij).

In the special case of mi = 1, for each i ∈ [n], we further

recover the deterministic distributed optimization problem.

It is important to note that we do not make any assumptions on the distributions Di. This means that we especially cover hard heterogeneous machine learning problems where data is only available locally to each worker i ∈ [n] := {1, . . . , n} and the local minima xi := arg minx∈Rd fi(x), can be far away from the global minimizer of (1). This covers a host of practically relevant problems over decentralized training data, as in federated learning (motivated by privacy), or large datasets stored across datacenters or devices (motivated by scalability). We will discuss several important examples in Section 3.2 below.

3.1. Assumptions on the objective function f
For all our theoretical results we assume that f is smooth. Assumption 1a (L-smoothness). Each function Fi(x, ξ) : Rd × Ωi → R, i ∈ [n] is differentiable for each ξ ∈ supp(Di) and there exists a constant L ≥ 0 such that for each x, y ∈ Rd, ξ ∈ supp(Di):
∇Fi(y, ξ) − ∇Fi(x, ξ) ≤ L x − y . (3)

Sometimes it will be enough to just assume smoothness of fi instead.
Assumption 1b (L-smoothness). Each function fi(x) : Rd → R, i ∈ [n] is differentiable and there exists a constant L ≥ 0 such that for each x, y ∈ Rd:

∇fi(y) − ∇fi(x) ≤ L x − y .

(4)

Remark 1. Clearly, Assumption 1b is more general than Assumption 1a. Moreover, for convex F (y, ξ) Assumption 1a implies Assumption 1b (Nesterov, 2004).

Assumption 1b is quite common in the literature (e.g. Lian et al., 2017; Wang & Joshi, 2018) but sometimes also the

stronger Assumption 1a is assumed (Nguyen et al., 2018). We here use this version in the convex case only, to allow for a more general assumption on the noise instead (see Section 3.2 below).

For some of the derived results we need in addition convexity. Speciﬁcally, µ-convexity for a parameter µ ≥ 0.
Assumption 2 (µ-convexity). Each function fi : Rd → R, i ∈ [n] is µ-(strongly) convex for constant µ ≥ 0. That is, for all x, y ∈ Rd:

µ

2

fi(x) − fi(y) + 2 x − y 2 ≤ ∇fi(x), x − y . (5)

3.2. Assumptions on the noise

We now formulate our conditions on the noise. For the convergence analysis of SGD on smooth convex functions it is typically enough to assume a bound on the noise at the optimum only (Needell et al., 2016; Bottou et al., 2018; Gower et al., 2019; Stich, 2019a). Similarly, to express the diversity of the functions fi in the convex case it is sufﬁcient to measure it only at the optimal point x (such a point always exists for strongly convex functions).
Assumption 3a (Bounded noise at the optimum). Let x = arg min f (x) and deﬁne

ζi2 :=

∇fi(x

)

2 2

,

ζ¯2 := n1

n i=1

ζi2

.

(6)

Further, deﬁne

σi2 := Eξi ∇Fi(x , ξi) − ∇fi(x ) 22 ,

(7)

and similarly as above, σ¯2 := 1

σ¯2 and ζ¯2 are bounded.

n

n i=1

σi2.

We

assume

that

Here, σ¯2 measures the noise level, and ζ¯2 the diversity of the functions fi. If all functions are identical, fi = fj, for all i, j, then ζ¯2 = 0. Many prior work in the context of stochastic decentralized optimization often assumed bounded diversity and bounded noise everywhere (such as e.g. Lian et al., 2017; Tang et al., 2018b), whereas we here only need to assume this bound locally at x .
For the non-convex case—where a unique x does not necessarily exist—we generalize Assumption 3a to:
Assumption 3b (Bounded noise). We assume that there exists constants P , ζˆ such that ∀x ∈ Rd,

1 n

n i=1

∇fi(x)

2 2

≤ ζˆ2 + P

∇f (x) 22 ,

(8)

and constants M , σˆ such that ∀x1, . . . xn ∈ Rd

Ψ ≤ σˆ2 + Mn

n i=1

∇fi(xi)

2 2

,

(9)

where Ψ := n1

n
i=1 Eξi

∇Fi(xi, ξi) − ∇fi(xi) 22.

A Uniﬁed Theory of Decentralized SGD

We see that Assumption 3a is weaker than Assumption 3b as it only needs ho hold for xi = x . Further, it is important to note that we do not assume a uniform bound on the variance (as many prior work, such as Li et al., 2019; Tang et al., 2018b; Lian et al., 2017; Assran et al., 2019) but instead allow the bound on the noise and the diversity to grow with the gradient norm (similar assumptions are common in the convex setting (Bottou et al., 2018)).

Discussion. We now show that the Assumption 3b is weaker than assuming a uniform upper bound on the noise. The uniform variance bound is given as

E

∇Fi(x, ξi) − ∇fi(x)

2 2

≤

σu2nif

,

∀x ∈ Rd ,

similarly for the similarity of functions between nodes

1 n

n i=1

∇fi(x) − ∇f (x)

2 2

≤ ζ¯u2nif ,

∀x ∈ Rd .

By recalling the equality n1

n i=1

ai − a¯

2 2

=

1 n

n i=1

ai

2 2

−

a¯

2 2

for

ai

∈ Rd, a¯ =

1 n

n i=1

ai,

it is easy to check that these two bounds imply Assump-

tion 3b with P = 1, M = 0, σˆ2 = σu2nif and ζˆ2 = ζ¯u2nif . Thus, our assumptions are weaker and ζˆ2 and σˆ2 can be

much smaller than ζ¯u2nif , σu2nif in general.

A second common assumption is to assume that the (stochastic) gradients are uniformly bounded (e.g. Koloskova et al., 2019; Li et al., 2020b), that is

E ∇Fi(x, ξi) 22 ≤ G2 ,

for a constant G. Under the bounded gradient assumption,
Assumption 3b is clearly satisﬁed, as all terms on the left hand side of (8) and (9) can be upper bounded by 2G2.

3.3. Notation
We use the notation x(it) to denote the iterates on node i at time step t. We further deﬁne the average

x¯(t) := n1

n i=1

x(it)

.

(10)

We use both vector and matrix notation whenever it is more convenient, and deﬁne

X(t) :=

x(1t)

,

.

.

.

,

x

(t) n

∈ Rd×n

(11)

and likewise deﬁne X¯ (t) := x¯(t), . . . , x¯(t) ≡ X(t) n1 11 .

of information (through gossip averaging) can only occur between connected nodes (neighbors). The algorithm (outlined in Algorithm 1) consists of two phases: (i) stochastic gradient updates, performed locally on each worker (lines 4–5), followed by a (ii) consensus operation, where nodes average their values with their neighbors (line 6).
The gossip averaging protocol can be compactly written in matrix notation, with Ni(t) := {j : wi(jt) > 0} denoting the neighbors of node i at iteration t:

X(t+1) = X(t)W (t) ⇔

x(it+1) =

j∈N (t) wi(jt)x(jt) , i

where the mixing matrix W (t) ∈ [0, 1]n×n encodes the network structure at time t and the averaging weights (nodes i and j are connected if wi(jt) > 0).
Our scheme shows great ﬂexibility as the mixing matrices can change over iterations and moreover can be selected from (changing) distributions.
Deﬁnition 1 (Mixing matrix). A symmetric (W = W ) doubly stochastic (W 1 = 1, 1 W = 1 ) matrix W ∈ [0, 1]n×n.

4.1. Algorithm

Algorithm 1 DECENTRALIZED SGD

input for each node i ∈ [n] initialize x(i0) ∈ Rd, stepsizes {ηt}Tt=−01, number of iterations T , mixing matrix distributions W(t) for t ∈ [0, T ]

1: for t in 0 . . . T do

2: Sample W (t) ∼ W(t)

3: In parallel (task for worker i, i ∈ [n])

4: Sample ξi(t), compute gi(t) := ∇Fi(x(it), ξi(t))

5: xi(t+ 21 ) = x(it) − ηtgi(t) stochastic gradient updates

6: x(it+1) := j∈Nit wi(jt)xj(t+ 21 )

gossip averaging

7: end for

In each iteration in Algorithm 1 a new mixing matrix W (t) is sampled from a possibly time-varying distribution W(t), t ∈ {0, . . . , T } (we will show below that also degenerate mixing matrices, for instance W (t) = In which implies no communication in round t, are possible choices). We will discuss several important instances below, but ﬁrst we now state our assumption on the quality of the mixing matrices. This assumption is novel in the literature to the best of our knowledge and a natural generalization of earlier versions.

4. Decentralized (Gossip) SGD
We now present the generalized decentralized SGD framework. Similar to existing works (Lian et al., 2017; Wang & Joshi, 2018; Li et al., 2019) our proposed method allows only decentralized communications. That is, the exchange

4.2. New assumption on mixing matrices We recall that for randomized gossip averaging with a randomly sampled mixing matrix W ∼ W it holds
EW XW − X¯ 2F ≤ (1 − p) X − X¯ 2F , (12)

A Uniﬁed Theory of Decentralized SGD

for a value p ≥ 0 (related to the spectrum of E W W ), that is, the averaging step brings the values in the columns of X ∈ Rd×n closer to their row-wise average X¯ := X · n1 11 in expectation (see e.g. Boyd et al., 2006).
In our analysis it will be enough to assume that a property similar to (12) holds for the composition of mixing matrixes, and does not necessarily hold for every single step.
Assumption 4 (Expected Consensus Rate). We assume that there exists two constants p ∈ (0, 1] and integer τ ≥ 1 such that for all matrices X ∈ Rd×n and all integers ∈ {0, . . . , T /τ },
EW XW ,τ − X¯ 2F ≤ (1 − p) X − X¯ 2F , (13)
where W ,τ = W (( +1)τ−1) · · · W ( τ) and X¯ := X 11n and E is taken over the distributions W (t) ∼ W(t) and indices t ∈ { τ, . . . , ( + 1)τ − 1}.

It is crucial to observe that this assumption does not require every realization W to satisfy a decrease property as for the standard analysis, it is enough if it holds over the concatenation of τ mixing steps. This assumption differs from the connectivity assumptions sometimes used in the control community. For example Nedic´ & Olshevsky (2014) require strong connectivity of the graph after every τ steps, whereas we here do not require this (for example, even sampling one single random edge leads to a positive decrease in expectation, whereas to ensure connectivity one would need to perform Ω(n) pairwise communications). This means that our bounds are typically much tighter that bounds derived on the strong connectivity assumption. However, as we require W to be symmetric, our setting is less general than the one considered in (Nedic´ et al., 2017; Xi & Khan, 2017; Saadatniaki et al., 2018; Assran & Rabbat, 2018; Scutari & Sun, 2019; Assran et al., 2019).

Commonly used weights are for instance the MetropolisHastings weights wij = wji = min deg(1i)+1 , deg(1j)+1

for (i, j) ∈ E, see also (Xiao & Boyd, 2004; Boyd et al.,

2006) for further guidelines. With these weights, the values

of p for commonly used graphs are p = 1 for the complete

graph, p = Θ n1

for 2-d torus on n nodes, and p = Θ

1 n2

for a cycle on n nodes. Intuitively, p−1/2 correlates with the

diameter of the graph and is related to the mixing time of

Markov chains. A commonly studied randomized scheme

is the pairwise random gossip algorithm (Boyd et al., 2006;

Loizou & Richta´rik, 2019), where one edge at a time is sam-

pled from an underlying graph G = ([n], E), i.e. the a random mixing matrix Zi,j := In − 12 (ei − ej)(ei − ej) , for all edges in the graph (i, j) ∈ E, where ei ∈ Rn is the ith

coordinate vector. In this case p = ρ(G)/|E|, where ρ(G)

denotes the algebraic connectivity of the network (Fiedler,

1973; Boyd et al., 2006; Loizou & Richta´rik, 2016). For

example, with the complete graph as base graph, pairwise

gossip attains p = Θ

1 n2

, i.e. enjoys equally fast mixing

as averaging over a (ﬁxed) cycle (which requires n pairwise

communications per round).

5. Examples Covered in the Framework
Our framework is very general and covers many special cases previously introduced in the literature.
5.1. Fixed Sampling Distribution (τ = 1, W(t) ≡ W)
The simplest instances of Algorithm 1 arise when the mixing matrix W is kept constant over the iterations. By choosing the fully connected matrix W = n1 11 we recover • centralized mini-batch SGD (Dekel et al., 2012) and by choosing an arbitrary connected W , we recover • decentralized SGD (Lian et al., 2017).
To reduce communication overheads, it has been proposed to choose sparse (not necessarily connected) subgraphs of the network topology. For instance in • MATCHA (Wang et al., 2019) it is proposed to sample edges from a matching decomposition of the underlying network topology, therefore allowing for pairwise communications between nodes. Whilst no explicit values of p were given for this approach, for the simpler instance of • pairwise randomized gossip (Boyd et al., 2006; Ram et al., 2010; Lee & Nedic´, 2015; Loizou & Richta´rik, 2019) we have p = Θ n1 , thus by sampling a linear number of (independent) edges—not necessarily a matching—we approximately have p = Θ˜ 1 for this • repeated pairwise randomized gossip variant. This approach can be generalized to • randomized subgraph gossip, where a subgraph of the base topology is selected for averaging. A special case of this is • clique gossip (Liu et al., 2019), or an alternative variant is to • sample from a ﬁxed set of communication topologies (known to all decentralized) workers.
One noteably instance of this type is • loopless local decentralized SGD where the mixing matrix is (a ﬁxed) W with probability τ1 , and In with probability 1 − τ1 , for a parameter τ ≥ 1. This algorithm mimicks the behavior of the local SGD (see subsection below), commonly analyzed for W = n1 11 only, but the loopless variant is much easier to analyze (with p decreased by a factor of τ , but no need to consider local steps explicitly in the analysis.).
5.2. Periodic Sampling (τ > 1, W(t) ≡ W(t+τ))
Our analysis covers the empirical (ﬁnite-sample) versions of the aforementioned algorithms, for instance • alternating decentralized SGD that sweeps through τ ﬁxed mixing matrices. A special algorithm of this type is • local SGD (Coppola, 2015; Zhou & Cong, 2018; Stich, 2019b) where averaging on the complete graph is performed every τ iterations and only local steps are performed otherwise

A Uniﬁed Theory of Decentralized SGD

(mixing matrix In for τ − 1 steps).
Our analysis covers also natural extensions such as • decentralized local SGD where mixing is performed with an arbitrary matrix W , and • random decentralized local SGD where the mixing matrix is sampled from a distribution. More generally, our framework also allows to combine local steps with all of the examples described in the previous section.
5.3. Non-Periodic Sampling
It is not necessary to have a periodic structure, it is sufﬁcient that the composition of every τ consecutive mixing matrixes satisﬁes Assumption 4. For instance as in • distriributed SGD over time-varying graphs (Nedic´ & Olshevsky, 2014).
5.4. Other Frameworks
In contrast to many prior works, we here allow the topology and the averaging weights to change between iterations. Our framework covers • Cooperative SGD (Wang & Joshi, 2018) which considers only the IID data case (fi = fj) with local updates and a ﬁxed mixing matrix W , and the recently proposed • periodic decentralized SGD (Li et al., 2019) that allows for multiple local update and multiple mixing steps (for ﬁxed W ) in a periodic manner. None of these work considered sampling of the mixing matrix and do only provide rates for non-convex functions.

error (T +1 1)

T t=0

(E

f

(x¯(t)

)

−

f

)≤

after

σ¯2 √L(ζ¯τ + σ¯√pτ ) Lτ

O n2+

p 3/2

+ p · R02

iterations, and if µ > 0, Strongly-Convex: then Tt=0 WwTt (E f (x¯(t)) − f ) +
µ E x¯(T +1) − x 2 ≤ for1

σ¯2 √L(ζ¯τ + σ¯√pτ ) Lτ 1

O˜

+

√

+ log

µn

µp

µp

iterations, for positive weights wt and F0 := f (x0) − f and R0 = x0 − x denote the initial errors.

6.2. Lower Bound
We now show that the terms depending on ζ¯ are necessary for the strongly convex setting and cannot be removed by an improved analysis.
Theorem 3. For n > 1 there exists strongly convex and smooth functions fi : Rd → R, i ∈ [n] with L = µ = 1 and without stochastic noise (σ¯2 = 0), such that Algorithm 1 for every constant mixing matrix W (t) ≡ W with p < 1 (see Assumption 4) for τ = 1, requires

T = Ω˜

ζ¯(1 − p) √

p

6. Convergence Result

iterations to converge to accuracy .

In this section we present the convergence results for decentralized SGD variants that ﬁt the template of Algorithm 1.

6.1. Complexity Estimates (Upper Bounds)

Theorem 2. For schemes as in Algorithm 1 with mixing matrices such as in Assumption 4 and any target accuracy
> 0 there exists a (constant) stepsize (potentially depending on ) such that the accuracy can be reached after at most the following number of iterations T : Non-Convex: Under Assumption 1b and 3b, it holds T +1 1 Tt=0 E ∇f (x¯(t)) 22 ≤ after

σˆ2 ζˆτ √M + 1 + σˆ√pτ τ (P + 1)(M + 1)

O n 2 + p 3/2 + p

· LF0

iterations. If we in addition assume convexity, Convex: Under Assumption 1a, 3a and 2 for µ ≥ 0, the

6.3. Discussion
Exemplary, we focus in our discussion on the strongly convex case only. For strongly convex functions we prove that the expected function value suboptimality decreases as

O˜ σ¯2 + L(τ 2ζ¯2 + τ pσ¯2) + Lτ R02 exp − µT p

nµT

µ2p2T 2

p

τL

where T denotes the iteration counter. We now argue that this rate is optimal up to acceleration.
Stochastic Terms. If σ¯2 > 0 the convergence rate is asymptotically dominated by the ﬁrst term, which cannot be further improved for stochastic methods (Nemirovsky & Yudin, 1983). We observe that the dominating ﬁrst term indicates a linear speedup in the number of workers n, and no dependence on the number of local steps τ , the mixing parameter p or the dissimilarity parameter ζ¯2. This means that decentralized SGD methods are ideal for the optimization in the high-noise regime even when network connectivity is low and number of local steps is large (see also (Chaturapruek
1O˜/Ω˜ -notation hides constants and polylogarithmic factors.

A Uniﬁed Theory of Decentralized SGD

Table 1. Comparison of convergence rates for Local SGD in non-iid settings, most recent results. We improve over the convex results, and recover the non-convex rate of Li et al. (2019).

Reference

convergence to -accuracy

strongly convex

convex

non-convex

Li et al. (2020b) Khaled et al. (2020) Li et al. (2019) this work

O nσ¯µ22 + τµ22ζ¯2 a -

-

O˜

σ¯ 2

√L(τ ζ¯+√τ σ¯)

+

√

+ κτ

nµ

µ

-

σ¯2+ζ¯2 √Lτ (ζ¯+σ¯) Lτ

O n2 +

3/2

+

-

σ¯2 √L(τ ζ¯+√τ σ¯) Lτ

O n2+

3/2

+

-

-

O Lσ¯2 + L(τ ζ¯+√τ σ¯) + Lτ

n2

3/2

O Lσˆ2 + L(τ ζˆ+√τ σˆ) + Lτ

n2

3/2

aThe paper relies on slightly different assumptions (bounded gradients and different measure of dissimilarity). For better comparison of the rates we write here ζ¯2 instead (which is strictly smaller than their parameter).

et al., 2015) and recent work (Pu et al., 2019)). In our rates the variance σ¯2 parameter also appears in the second term, but affects the convergence only mildly (for T = Ω(τ n/p) this second term gets dominated by the ﬁrst one).
Optimization Terms. Even when σ¯2 = 0, the convergence of decentralized SGD only sublinear when ζ¯2 > 0:2

O˜ Lτ 2ζ¯2 + Lτ R02 exp − µT p .

µ2p2T 2

p

τL

The dependence on the dissimilarity ζ¯2 cannot be removed in general as we show in Theorem 3. These results show that decentralized SGD methods without additional modiﬁcations (see also Shi et al., 2015; Karimireddy et al., 2019) cannot converge linearly.

We can further observe see that the rates only depend on the

ratio p/τ , but not on p or τ individually. This also means

that the rates for local variants of decentralized SGD are

the same as for their loopless variants (when the mixing is performed with probability τ1 only). The error term depending on R02 vanishes exponentially fast, as expected for

SGD methods (Bach & Moulines, 2011). The linear dependence on µLp (the therm in the exponent) is expected here, as we use non-accelerated ﬁrst order schemes and

standard gossip. This term could potentially be improved to µLp 1/2 with acceleration techniques, such as in (Scaman
et al., 2017). The linear dependence on τ cannot further be

improved in general. This follows from the lower bound

for the communication complexity of distributed convex

optimization (Arjevani & Shamir, 2015), as the number of

communication rounds is at most T (no communication

happens during the local steps).

τ
However,

when ζ¯2

=

0

(as for instance the case for identical functions fi on each

worker), this lower bound becomes vacuous and improve-

ment of the dependence on τ might be possible (which we

2Except for the special case when p = 1 (fully connected graph, such as for mini-batch SGD). In this case the rate does not depend on ζ¯2. We detail this (known result) in the appendix.

cannot not exploit here).
Linear Convergence for Overparametrized Settings. In overparametrized problems, there exists always x s.t. ∇fi(x ) 2 = 0, that is σ¯2 = 0 and ζ¯2 = 0. We prove here that decentralized SGD converges linearly in this case, similarly to mini-batch SGD (Bach & Moulines, 2011; Schmidt & Roux, 2013; Needell et al., 2016; Ma et al., 2018; Gower et al., 2019; Loizou et al., 2020).
7. Special Cases: Highlights
Our rates apply to all the examples discussed in Section 5 and of course we could design even more variants and combinations of these schemes. This gives great ﬂexibility in designing new schemes and algorithms for future applications. We leave the exploration of the trade-offs in these approaches for future work, and highlight here only a few special cases that could be of particular interest.
7.1. Best Rates for Local SGD
Local SGD is a simpliﬁed version of the federated averaging algorithm (McMahan et al., 2016; 2017) and has recently attracted the attention of the theoretical community in the seek of the best convergence rates (Stich, 2019b; Wang & Joshi, 2018; Yu et al., 2019; Basu et al., 2019; Patel & Dieuleveut, 2019; Stich & Karimireddy, 2019; Li et al., 2019; Khaled et al., 2020). Our work extends this chain and improves previous best results for convex settings and recovers the results of Li et al. (2019) in the non-convex case as we highlight in Table 1. We point out that all these rates are still dominated by large-batch SGD and do not match the lower bounds established in (Woodworth et al., 2018) for the iid. case ζ¯2 = 0. See also recent parallel work in (Woodworth et al., 2020). Whilst these previous analysis were often speciﬁcally tailored and only applicable to the mixing structure in local SGD, our analysis is much more general and tighter at the same time.

A Uniﬁed Theory of Decentralized SGD

Error

Error

100 10−3 10−6
0
100 10−3 10−6 100
100 10−3 10−6 100

σ 2 = 0, ζ 2 = 0

σ 2 = 0, ζ 2 = 1

100

100

10−3

10−3

200

400

600

σ 2 = 1, ζ 2 = 0

10−6 0
100

10−6

2000 4000 6000 8000 10000

0

σ 2 = 1, ζ 2 = 1

100

10−3

10−3

101

102

103

σ 2 = 100, ζ 2 = 0

104 10−6 100 100

101

102

103

σ 2 = 100, ζ 2 = 1

104 10−6 100 100

10−3

10−3

101

102

103

Iteration

104 10−6 100 ring

101

102

103

Iteration

104 10−6 100

torus

centralized

σ 2 = 0, ζ 2 = 10

5000 10000 15000
σ 2 = 1, ζ 2 = 10

101

102

103

104

σ 2 = 100, ζ 2 = 10

101

102

103

104

Iteration

Error

Figure 1. Convergence of n1 ni=1 xi(t) − x 22 to target accuracy = 10−5 for different problem difﬁculty (σ¯2 increasing to the bottom, ζ¯2 increasing to the right), and different topologies on n = 25 nodes, d = 50. Stepsizes were tuned for each experiment individually to reach target accuracy in as few iterations as possible.

In their recently updated parallel version, Karimireddy et al. (2019) improve upon these rates by removing σ from the second term. However, they do analyze a different version of local SGD (with different stepsizes for inner and outer loops) than we consider here. This change does not ﬁt in our framework and it is not clear if similar trick is possible the decentralized setting.
7.2. Comparison to Recent Frameworks
We mentioned major differences to other frameworks in Section 5.4 above already. Our results for the non-convex case recover the best results from (Wang & Joshi, 2018) for

error

101 100 10−1 10−2 10−3 10−4 10−5

ζ2=1 ζ2=0

σ 2 = 100 ζ2=1

σ2=1 σ2=0

σ 2 = 0, ζ 2 = 0

σ 2 = 1, ζ 2 = 0

0

100 200 300 400 500 600

iterations

Figure 2. Problem setup. Parameters σ¯2 and ζ¯2 change the
noise level and the difﬁculty of the problem. (Here we depict n1 ni=1 x(it) − x 22 on the ring with n = 25 nodes, d = 10, using ﬁxed stepsize η = 10−2 for illustration.

the iid. case3 (ζˆ2 = 0) and the non-iid. case from (Li et al., 2019) for their speciﬁc settings. We point out that our results also cover the convex setting and deterministic setting.
7.3. Best Rates for Decentralized SGD
We improve best known rates of Decentralized SGD (Olshevsky et al., 2019; Koloskova et al., 2019) for strongly convex objectives and recover the best rates in the nonconvex case (Lian et al., 2017).

8. Experiments

Complementing prior work that established the effectiveness of decentralized training methods (Lian et al., 2017; Assran et al., 2019) we here focus on verifying whether the numerical performance of decentralized stochastic optimization algorithms coincides with the rates predicted by theory, focusing on the strongly convex case for now.

We consider a distributed least squares objective with

fi(x)

:=

1 2

Aix − bi

22, for ﬁxed Hessian A2i

=

i2 n

· Id

and sample each bi ∼ N (0, ζ¯2/i2Id) for a parameter ζ¯2,

which controls the similarity of the functions (and coin-

cides with the parameter in Assumption 3a). We control

the stochastic noise σ¯2 by adding Gaussian noise to every

stochastic gradient. We depict the effect of these parameters

3These results can be recovered by optimizing the stepsize in (Wang & Joshi, 2018, Theorem 1) directly, instead of resorting to the worse rate stated in (Wang & Joshi, 2018, Corollary 1).

A Uniﬁed Theory of Decentralized SGD

in Figure 2.
Setup. We consider three common network topologies, ring, 2-d torus and fully-connected graph and use the Metropolis-Hasting mixing matrix W , i.e. wij = wji = deg(1i)+1 = deg(1j)+1 for {i, j} ∈ E. For all algorithms we tune the stepsize to reach a desired target accuracy with the fewest number of iterations.

We acknowledge funding from SNSF grant 200021 175796, as well as a Google Focused Research Award. Nicolas Loizou acknowledges support by the IVADO Postdoctoral Funding Program.

Discussion of Results. In Figure 1 we depict the results.

We observe that in the high noise regime (bottom row) the graph topology and the functions similarity ζ¯2 do not impact

the number of iterations needed to reach the target accuracy

(the σ¯2 term is dominating in this regime. We also see linear

T
rates when σ¯2

=

ζ¯2

=

0 as predicted.

When increasing

ζ¯2 (in the case of σ¯2 = 0) we see that on the ring and

torus topology the linear rate changes to a sublinear rate:

even thought the curves look like straight lines, they stop

converging when reaching the target accuracy (the stepsize

must be further decreased to achieve higher accuracy). By

comparing two top right plots, we see that for ﬁxed topology

tohfe√n1u0mwbehreonfiintecrraetaiosinnsginζ¯c2rebayseasfaacptporrooxfim1a0t,ealys obnyeawfaocutoldr expect from the term p2ζ¯T2 2 in the convergence rate (see also Figure 3 in the appendix). The difference in number of

iterations on the torus vs. ring scales approximately linear

in the ratio of their mixing parameters p, (that is, Θ(n) as

mentioned in Section 4.2).

9. Extensions
We presented a unifying framework for the analysis of decentralized SGD methods and provide the best known convergence guarantees. Our results show that when the noise is high, decentralized SGD methods can achieve linear speedup in the number of workers n and the convergence rate does only weakly depend on the graph topology, the number of local steps or the data heterogeneity. This shows that such methods are perfectly suited to solve stochastic optimization problems in a decentralized way. However, our results also reveal that when the noise is small (for e.g. when using large mini-batches), the effect of those parameters become more pronounced and especially function diversity can hamper the convergence of decentralized SGD methods.
Our framework can be further extended by considering gradient compression techniques (Koloskova et al., 2019) or overlapping communication steps (Assran et al., 2019; Wang et al., 2020) to additionally speedup the distributed training.

Acknowledgements
We would like to thank Edouard Oyallon for indicating an inaccuracy in the proof in the ﬁrst version of this manuscript.

A Uniﬁed Theory of Decentralized SGD

References
Alghunaim, S. A. and Sayed, A. H. Linear convergence of primal-dual gradient methods and their performance in distributed optimization. arXiv preprint arXiv:1904.01196, 2019.
Alistarh, D., Grubic, D., Li, J., Tomioka, R., and Vojnovic, M. QSGD: Communication-efﬁcient SGD via gradient quantization and encoding. In NIPS - Advances in Neural Information Processing Systems 30, pp. 1709–1720. Curran Associates, Inc., 2017.
Arjevani, Y. and Shamir, O. Communication complexity of distributed convex learning and optimization. In NIPS Advances in Neural Information Processing Systems 28, pp. 1756–1764. Curran Associates, Inc., 2015.
Assran, M. and Rabbat, M. Asynchronous subgradient-push. arXiv preprint arXiv:1803.08950, 2018.
Assran, M., Loizou, N., Ballas, N., and Rabbat, M. Stochastic gradient push for distributed deep learning. 2019.

Dekel, O., Gilad-Bachrach, R., Shamir, O., and Xiao, L. Optimal distributed online prediction using mini-batches. J. Mach. Learn. Res., 13(1):165–202, January 2012.
Duchi, J. C., Agarwal, A., and Wainwright, M. J. Dual averaging for distributed optimization: Convergence analysis and network scaling. IEEE Transactions on Automatic Control, 57(3):592–606, 2012. doi: 10.1109/ TAC.2011.2161027.
Fallah, A., Gu¨rbu¨zbalaban, M., Ozdaglar, A., S¸ ims¸ekli, U., and Zhu, L. Robust distributed accelerated stochastic gradient methods for multi-agent networks. arXiv preprint arXiv:1910.08701, 2019.
Fiedler, M. Algebraic connectivity of graphs. Czechoslovak mathematical journal, 23(2):298–305, 1973.
Gower, R. M., Loizou, N., Qian, X., Sailanbayev, A., Shulgin, E., and Richta´rik, P. SGD: General analysis and improved rates. In ICML - International Conference on Machine Learning, pp. 5200–5209, 2019.

Bach, F. R. and Moulines, E. Non-asymptotic analysis of stochastic approximation algorithms for machine learning. In NIPS - Advances in Neural Information Processing Systems 24, pp. 451–459. Curran Associates, Inc., 2011.
Basu, D., Data, D., Karakus, C., and Diggavi, S. Qsparselocal-SGD: Distributed SGD with quantization, sparsiﬁcation, and local computations. arXiv preprint arXiv:1906.02367, 2019.
Bottou, L., Curtis, F., and Nocedal, J. Optimization methods for large-scale machine learning. SIAM Review, 60(2): 223–311, 2018.
Boyd, S., Ghosh, A., Prabhakar, B., and Shah, D. Randomized gossip algorithms. IEEE/ACM Trans. Netw., 14(SI): 2508–2530, 2006.
Chaturapruek, S., Duchi, J. C., and Re´, C. Asynchronous stochastic convex optimization: the noise is in the noise and SGD don’t care. In NIPS - Advances in Neural Information Processing Systems 28, pp. 1531–1539. Curran Associates, Inc., 2015.
Coppola, G. Iterative parameter mixing for distributed large-margin training of structured predictors for natural language processing. PhD thesis, The University of Edinburgh, 2015.
Dean, J., Corrado, G., Monga, R., Chen, K., Devin, M., Mao, M., Ranzato, M., Senior, A., Tucker, P., Yang, K., Le, Q. V., and Ng, A. Y. Large scale distributed deep networks. In NIPS - Advances in Neural Information Processing Systems, pp. 1223–1231, 2012.

He, L., Bian, A., and Jaggi, M. Cola: Decentralized linear learning. In NeurIPS - Advances in Neural Information Processing Systems 31, pp. 4541–4551. 2018.
Iutzeler, F., Bianchi, P., Ciblat, P., and Hachem, W. Asynchronous distributed optimization using a randomized alternating direction method of multipliers. In Proceedings of the 52nd IEEE Conference on Decision and Control, CDC, pp. 3671–3676. IEEE, 2013.
Jakovetic´, D., Xavier, J., and Moura, J. M. F. Fast distributed gradient methods. IEEE Transactions on Automatic Control, 59(5):1131–1146, May 2014.
Johansson, B., Rabi, M., and Johansson, M. A randomized incremental subgradient method for distributed optimization in networked systems. SIAM Journal on Optimization, 20(3):1157–1170, 2010. doi: 10.1137/08073038X.
Kairouz, P., McMahan, H. B., Avent, B., Bellet, A., Bennis, M., Bhagoji, A. N., Bonawitz, K., Charles, Z., Cormode, G., Cummings, R., D’Oliveira, R. G. L., Rouayheb, S. E., Evans, D., Gardner, J., Garrett, Z., Gasco´n, A., Ghazi, B., Gibbons, P. B., Gruteser, M., Harchaoui, Z., He, C., He, L., Huo, Z., Hutchinson, B., Hsu, J., Jaggi, M., Javidi, T., Joshi, G., Khodak, M., Konecˇny´, J., Korolova, A., Koushanfar, F., Koyejo, S., Lepoint, T., Liu, Y., Mittal, P., Mohri, M., Nock, R., O¨ zgu¨r, A., Pagh, R., Raykova, M., Qi, H., Ramage, D., Raskar, R., Song, D., Song, W., Stich, S. U., Sun, Z., Suresh, A. T., Trame`r, F., Vepakomma, P., Wang, J., Xiong, L., Xu, Z., Yang, Q., Yu, F. X., Yu, H., and Zhao, S. Advances and open problems in federated learning. arXiv preprint arXiv:1912.04977, 2019.

A Uniﬁed Theory of Decentralized SGD

Karimireddy, S. P., Kale, S., Mohri, M., Reddi, S. J., Stich, S. U., and Suresh, A. T. SCAFFOLD: Stochastic controlled averaging for on-device federated learning. arXiv preprint arXiv:1910.06378, 2019.
Kempe, D., Dobra, A., and Gehrke, J. Gossip-based computation of aggregate information. In Proceedings of the 44th Annual IEEE Symposium on Foundations of Computer Science, FOCS ’03. IEEE Computer Society, 2003.
Khaled, A., Mishchenko, K., and Richta´rik, P. Tighter theory for local SGD on identical and heterogeneous data. arXiv preprint arXiv:1909.04746v2, 2020.
Koloskova, A., Stich, S., and Jaggi, M. Decentralized stochastic optimization and gossip algorithms with compressed communication. In ICML - Proceedings of the 36th International Conference on Machine Learning, volume 97, pp. 3478–3487. PMLR, 2019.
Koloskova, A., Lin, T., Stich, S. U., and Jaggi, M. Decentralized deep learning with arbitrary communication compression. ICLR - International Conference on Learning Representations, 2020.
Konecˇny`, J., McMahan, H. B., Ramage, D., and Richta´rik, P. Federated optimization: Distributed machine learning for on-device intelligence. arXiv preprint arXiv:1610.02527, 2016.
Lacoste-Julien, S., Schmidt, M. W., and Bach, F. R. A simpler approach to obtaining an O(1/t) convergence rate for the projected stochastic subgradient method. arXiv preprint arXiv:1212.2002, 2012.
Lee, J. D., Lin, Q., Ma, T., and Yang, T. Distributed stochastic variance reduced gradient methods and a lower bound for communication complexity. arXiv preprint arXiv:1507.07595, 2015.
Lee, S. and Nedic´, A. Asynchronous gossip-based random projection algorithms over networks. IEEE Transactions on Automatic Control, 61(4):953–968, 2015.
Li, T., Sahu, A. K., Sanjabi, M., Zaheer, M., Talwalkar, A., and Smith, V. On the convergence of federated optimization in heterogeneous networks. arXiv preprint arXiv:1812.06127, 2018.
Li, T., Sahu, A. K., Zaheer, M., Sanjabi, M., Talwalkar, A., and Smith, V. Feddane: A federated newton-type method. arXiv preprint arXiv:2001.01920, 2020a.
Li, X., Yang, W., Wang, S., and Zhang, Z. Communication efﬁcient decentralized training with multiple local updates. arXiv preprint arXiv:1910.09126, 2019.

Li, X., Huang, K., Yang, W., Wang, S., and Zhang, Z. On the convergence of FedAvg on non-IID data. ICLR International Conference on Learning Representations, openreview, 2020b.
Lian, X., Zhang, C., Zhang, H., Hsieh, C.-J., Zhang, W., and Liu, J. Can decentralized algorithms outperform centralized algorithms? a case study for decentralized parallel stochastic gradient descent. In NIPS - Advances in Neural Information Processing Systems 30, pp. 5330– 5340. Curran Associates, Inc., 2017.
Lin, T., Stich, S. U., Patel, K. K., and Jaggi, M. Don’t use large mini-batches, use local SGD. ICLR - International Conference on Learning Representations, 2020.
Liu, Y., Li, B., Anderson, B. D., and Shi, G. Clique gossiping. IEEE/ACM Transactions on Networking, 27(6): 2418–2431, 2019.
Loizou, N. and Richta´rik, P. A new perspective on randomized gossip algorithms. In 2016 IEEE Global Conference on Signal and Information Processing (GlobalSIP), pp. 440–444. IEEE, 2016.
Loizou, N. and Richta´rik, P. Revisiting randomized gossip algorithms: General framework, convergence rates and novel block and accelerated protocols. arXiv preprint arXiv:1905.08645, 2019.
Loizou, N., Vaswani, S., Laradji, I., and Lacoste-Julien, S. Stochastic polyak step-size for SGD: An adaptive learning rate for fast convergence. arXiv preprint arXiv:2002.10542, 2020.
Ma, S., Bassily, R., and Belkin, M. The power of interpolation: Understanding the effectiveness of SGD in modern over-parametrized learning. In ICML - Proceedings of the 35th International Conference on Machine Learning, volume 80, pp. 3325–3334. PMLR, 2018.
McMahan, B., Moore, E., Ramage, D., Hampson, S., and Arcas, B. A. y. Communication-Efﬁcient Learning of Deep Networks from Decentralized Data. In AISTATS Proceedings of the 20th International Conference on Artiﬁcial Intelligence and Statistics, pp. 1273–1282, 2017.
McMahan, H. B., Moore, E., Ramage, D., and y Arcas, B. A. Federated learning of deep networks using model averaging. arXiv preprint arXiv:1602.05629, 2016.
Nadiradze, G., Sabour, A., Sharma, A., Markov, I., Aksenov, V., and Alistarh, D. PopSGD: Decentralized Stochastic Gradient Descent in the Population Model. arXiv e-prints, art. arXiv:1910.12308, 2019.
Nedic´, A. and Olshevsky, A. Distributed optimization over time-varying directed graphs. IEEE Transactions on Automatic Control, 60(3):601–615, 2014.

A Uniﬁed Theory of Decentralized SGD

Nedic´, A. and Olshevsky, A. Stochastic gradient-push for strongly convex functions on time-varying directed graphs. IEEE Transactions on Automatic Control, 61(12): 3936–3947, 2016.
Nedic´, A. and Ozdaglar, A. Distributed subgradient methods for multi-agent optimization. IEEE Transactions on Automatic Control, 54(1):48–61, 2009.
Nedic´, A., Lee, S., and Raginsky, M. Decentralized online optimization with global objectives and local communication. In 2015 American Control Conference (ACC), pp. 4497–4503, 2015.
Nedic´, A., Olshevsky, A., and Shi, W. A geometrically convergent method for distributed optimization over timevarying graphs. In 2016 IEEE 55th Conference on Decision and Control (CDC), pp. 1023–1029, 2016.
Nedic´, A., Olshevsky, A., and Shi, W. Achieving geometric convergence for distributed optimization over timevarying graphs. SIAM Journal on Optimization, 27(4): 2597–2633, 2017.
Nedic´, A., Olshevsky, A., and Rabbat, M. G. Network topology and communication-computation tradeoffs in decentralized optimization. Proceedings of the IEEE, 106 (5):953–976, 2018.
Needell, D., Srebro, N., and Ward, R. Stochastic gradient descent, weighted sampling, and the randomized Kaczmarz algorithm. Mathematical Programming, 155(1): 549–573, 2016.
Nemirovsky, A. S. and Yudin, D. B. Problem complexity and method efﬁciency in optimization. Wiley, 1983.
Nesterov, Y. Introductory Lectures on Convex Optimization, volume 87 of Springer Science & Business Media. Springer US, Boston, MA, 2004.
Nguyen, L. M., Nguyen, P. H., Richta´rik, P., Scheinberg, K., Taka´cˇ, M., and van Dijk, M. New convergence aspects of stochastic gradient algorithms. arXiv preprint arXiv:1811.12403, 2018.
Olshevsky, A., Paschalidis, I. C., and Pu, S. A nonasymptotic analysis of network independence for distributed stochastic gradient descent. arXiv preprint arXiv:1906.02702, art. arXiv:1906.02702, 2019.
Patel, K. K. and Dieuleveut, A. Communication trade-offs for synchronized distributed SGD with large step size. arXiv preprint arXiv:1904.11325, 2019.
Pu, S., Olshevsky, A., and Paschalidis, I. C. A sharp estimate on the transient time of distributed stochastic gradient descent. arXiv preprint arXiv:1906.02702, 2019.

Rabbat, M. Multi-agent mirror descent for decentralized stochastic optimization. In 2015 IEEE 6th International Workshop on Computational Advances in Multi-Sensor Adaptive Processing (CAMSAP), pp. 517–520, 2015.
Rakhlin, A., Shamir, O., and Sridharan, K. Making gradient descent optimal for strongly convex stochastic optimization. In ICML - Proceedings of the 29th International Conference on Machine Learning, pp. 1571–1578. Omnipress, 2012.
Ram, S. S., Nedic´, A., and Veeravalli, V. V. Asynchronous gossip algorithm for stochastic optimization: Constant stepsize analysis. In Recent Advances in Optimization and its Applications in Engineering, pp. 51–60. Springer, 2010.
Saadatniaki, F., Xin, R., and Khan, U. A. Optimization over time-varying directed graphs with row and columnstochastic matrices. arXiv preprint arXiv:1810.07393, 2018.
Scaman, K., Bach, F., Bubeck, S., Lee, Y. T., and Massoulie´, L. Optimal algorithms for smooth and strongly convex distributed optimization in networks. In ICML - Proceedings of the 34th International Conference on Machine Learning, volume 70, pp. 3027–3036. PMLR, 2017.
Scaman, K., Bach, F., Bubeck, S., Massoulie´, L., and Lee, Y. T. Optimal algorithms for non-smooth distributed optimization in networks. In NeurIPS - Advances in Neural Information Processing Systems 31, pp. 2745– 2754. Curran Associates, Inc., 2018.
Schmidt, M. and Roux, N. L. Fast convergence of stochastic gradient descent under a strong growth condition. arXiv preprint arXiv:1308.6370, 2013.
Scutari, G. and Sun, Y. Distributed nonconvex constrained optimization over time-varying digraphs. Mathematical Programming, 176(1-2):497–544, 2019.
Shamir, O. and Srebro, N. Distributed stochastic optimization and learning. 2014 52nd Annual Allerton Conference on Communication, Control, and Computing (Allerton), pp. 850–857, 2014.
Shi, W., Ling, Q., Wu, G., and Yin, W. EXTRA: An exact ﬁrst-order algorithm for decentralized consensus optimization. SIAM Journal on Optimization, 25(2):944–966, 2015.
Stich, S. U. Uniﬁed optimal analysis of the (stochastic) gradient method. arXiv preprint arXiv:1907.04232, 2019a.
Stich, S. U. Local SGD converges fast and communicates little. ICLR - International Conference on Learning Representations, art. arXiv:1805.09767, 2019b.

A Uniﬁed Theory of Decentralized SGD

Stich, S. U. and Karimireddy, S. P. The error-feedback framework: Better rates for SGD with delayed gradients and compressed communication. arXiv preprint arXiv:1909.05350, 2019.
Stich, S. U., Cordonnier, J.-B., and Jaggi, M. Sparsiﬁed SGD with memory. In NeurIPS - Advances in Neural Information Processing Systems 31, pp. 4452–4463. 2018.
Tang, H., Gan, S., Zhang, C., Zhang, T., and Liu, J. Communication compression for decentralized training. In NeurIPS - Advances in Neural Information Processing Systems 31, pp. 7663–7673. Curran Associates, Inc., 2018a.
Tang, H., Lian, X., Yan, M., Zhang, C., and Liu, J. D2: Decentralized training over decentralized data. In ICML - Proceedings of the 35th International Conference on Machine Learning, volume 80, pp. 4848–4856. PMLR, 2018b.
Tang, H., Lian, X., Qiu, S., Yuan, L., Zhang, C., Zhang, T., and Liu, J. Deepsqueeze: Decentralization meets error-compensated compression. arXiv preprint arXiv:1907.07346, 2019.
Tsitsiklis, J. N. Problems in decentralized decision making and computation. PhD thesis, Massachusetts Institute of Technology, 1984.
Uribe, C. A., Lee, S., and Gasnikov, A. A dual approach for optimal algorithms in distributed optimization over networks. arXiv preprint arXiv:1809.00710, 2018.
Wang, J. and Joshi, G. Cooperative SGD: A uniﬁed framework for the design and analysis of communication-efﬁcient SGD algorithms. arXiv preprint arXiv:1808.07576, 2018.
Wang, J., Sahu, A. K., Yang, Z., Joshi, G., and Kar, S. MATCHA: speeding up decentralized SGD via matching decomposition sampling. arXiv preprint arXiv:1905.09435, 2019.
Wang, J., Liang, H., and Joshi, G. Overlap local-SGD: An algorithmic approach to hide communication delays in distributed SGD. manuscript, 2020.
Wei, E. and Ozdaglar, A. Distributed alternating direction method of multipliers. In 2012 IEEE 51st IEEE Conference on Decision and Control (CDC), pp. 5445–5450, 2012.
Woodworth, B., Patel, K. K., Stich, S. U., Dai, Z., Bullins, B., McMahan, H. B., Shamir, O., and Srebro, N. Is local SGD better than minibatch SGD? arXiv preprint arXiv:2002.07839, 2020.

Woodworth, B. E., Wang, J., Smith, A., McMahan, H. B., and Srebro, N. Graph oracle models, lower bounds, and gaps for parallel stochastic optimization. In NeurIPS Advances in Neural Information Processing Systems, pp. 8496–8506, 2018.
Xi, C. and Khan, U. A. Dextra: A fast algorithm for optimization over directed graphs. IEEE Transactions on Automatic Control, 62(10):4980–4993, 2017.
Xiao, L. and Boyd, S. Fast linear iterations for distributed averaging. Systems & Control Letters, 53(1):65–78, 2004.
Yu, H., Yang, S., and Zhu, S. Parallel restarted SGD with faster convergence and less communication: Demystifying why model averaging works for deep learning. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 33, pp. 5693–5700, 2019.
Zhao, Y., Li, M., Lai, L., Suda, N., Civin, D., and Chandra, V. Federated learning with non-iid data. arXiv preprint arXiv:1806.00582, 2018.
Zhou, F. and Cong, G. On the convergence properties of a k-step averaging stochastic gradient descent algorithm for nonconvex optimization. In Proceedings of the TwentySeventh International Joint Conference on Artiﬁcial Intelligence, IJCAI-18, pp. 3219–3227. International Joint Conferences on Artiﬁcial Intelligence Organization, 7 2018.
Zinkevich, M., Weimer, M., Li, L., and Smola, A. J. Parallelized stochastic gradient descent. In NIPS - Advances in Neural Information Processing Systems, pp. 2595–2603, 2010.

APPENDIX A Uniﬁed Theory of Decentralized SGD with Changing Topology and Local Updates

The appendix is organized as follows: In Section A, we rewrite Algorithm 1 equivalently in matrix notation as Algorithm 2 and give a sketch of the proof using this new notation. In Section B we state a few auxiliary technical lemmas, before giving all details for the proof of the theorem in Sections C and D. We conclude the appendix in Section F by presenting additional numerical results that conﬁrm the tightness of our theoretical analysis in the strongly convex case.

A. Proof of Theorem 2

A.1. Decentralized SGD in Matrix Notation

We can rewrite Algorithm 1 using the following matrix notation, extending the deﬁnition used in the main text:

X(t) := x(1t), . . . , x(nt) ∈ Rd×n,

X¯ (t) := x¯(t), . . . , x¯(t) ∈ Rd×n,

(14)

∂F (X(t), ξ(t)) :=

∇

F1

(x

(t 1

)

,

ξ1(t

)

)

,

.

.

.

,

∇

Fn

(

x

(t) n

,

ξn(t

)

)

∈ Rd×n.

Algorithm 2 DECENTRALIZED SGD (MATRIX NOTATION)

input : X(0), stepsizes {ηt}Tt=−01, number of iterations T , mixing matrix distributions W(t) for t ∈ [0, T ]

1: for t in 0 . . . T do

2: Sample W (t) ∼ W(t)

3:

X

(t

+

1 2

)

=

X (t)

−

ηt ∂ Fi (X (t) ,

ξi(t))

4:

X (t+1)

=

X

(t+

1 2

)

W

(t

)

stochastic gradient updates gossip averaging

5: end for

A.2. Proof Sketch—Combining Consensus Progress (Gossip) and Optimization Progress (SGD)

In this section we sketch of the proof for Theorem 2. As a ﬁrst step in the proof, we will derive an upper bound on

the expected progress, measured as distance to the optimum, rt = E x¯(t) − x

2
for the convex cases, and function

suboptimality rt = E f (x¯(t)) − f in the non-convex case. These bounds will have the following form:

rt+1 ≤ (1 − aηt)rt − bηtet + cηt2 + ηtBΞt ,

(15)

with

Ξt

=

1 n

Et

ni=1 x(it) − x¯(t) 2 and

• for both convex cases rt = E x¯(t) − x 2, et = f (x¯(t)) − f (x ), a = µ2 , b = 1, c = σ¯n2 , B = 3L (Lemma 8); • for the non-convex case rt = E f (x¯(t)) − f , et = ∇f (x¯(t)) 22, a = 0, b = 41 , c = Lnσˆ2 , B = L2 (Lemma 11).

We will then bound the consensus distance Ξt as detailed in Section C; Lemmas 9 and 12 by a recursion of the form

p

p t−1

t−1

t−1

Ξt ≤ 1 − Ξmτ +

Ξj + D

ηj2ej + A

ηj2,

(16)

2

64τ

j=mτ

j=mτ

j=mτ

A Uniﬁed Theory of Decentralized SGD

where m = t/τ − 1; for convex cases A = 8σ¯2 + 18pτ ζ¯2 , D = 72L τp (Lemma 9) and for non-convex case A = 2σˆ2 + 2 6pτ + M ζˆ2, D = 2P 6pτ + M (Lemma 12).
Note that (16) holds only for t ≥ (m + 1)τ . To be able to simplify (16) we additionally consider mτ ≤ t < (m + 1)τ and prove (Lemmas 10, 13) that with the same parameters as above, it holds

p

p t−1

t−1

t−1

Ξt ≤ 1 + Ξmτ +

Ξj + D

ηj2ej + A

ηj2,

(17)

2

64τ

j=mτ

j=mτ

j=mτ

Next, we simplify this recursive equation (16) using Lemma 14 and some positive weights {wt}t≥0 (see Lemma 14 for the deﬁnition of the weights wt) to

T

bT

τT

B · wtΞt ≤ · wtet + 64AB · wtηt2 ,

(18)

2

p

t=0

t=0

t=0

where

again

Ξt

=

1 n

Et

ni=1 x(it) − x¯(t) 2.

Then we combine (15) and (18). Firstly rearranging (15), multiplying by wt and dividing by ηt, we get

bwtet ≤ (1 − aηt) wtrt − wt rt+1 + cwtηt + BwtΞt ,

ηt

ηt

Now summing up and dividing by WT =

T t=0

wt,

1T

1T

WT bwtet ≤ WT

t=0

t=0

(18) 1 T ≤
WT t=0

(1 − aηt)wt rt − wt rt+1

ηt

ηt

(1 − aηt)wt rt − wt rt+1

ηt

ηt

cT

1T

+ WT

wtηt + WT B wtΞt

t=0

t=0

cT

1

+

wtηt +

T

64BA τ T

wtet +

wtηt2 ,

WT t=0

2WT t=0

WT p t=0

Therefore,

1T

1 T (1 − aηt)wt

wt

cT

64BA τ T 2

2WT bwtet ≤ WT

ηt

rt − ηt rt+1 + WT

wtηt + WT p wtηt

(19)

t=0

t=0

t=0

t=0

Finally, to solve this main recursion (19) and obtain the ﬁnal convergence rates of Theorem 2, we will use the following Lemmas, which will be presented in Section D:

• Lemma 15 for strongly convex case when a > 0. • Lemmas 16 and 17 for both weakly convex and non-convex cases as their common feature is that a = 0.

A.3. How the Proof of Theorem 2 Follows
In this section we summarize how the proof of Theorem 2 follows from the results that we establish in Sections C and D below. Note that for convex cases we require both fi and Fi to be convex as in Lemma 9.

Proof of Theorem 2, strongly convex case. The proof follows by applying the result of Lemma 15 to the equation (19)

(obtained with Lemmas 8, 9, 14) with rt = E x¯(t) − x 2, et = f (x¯(t)) − f (x ), a = µ , b = 1, c = σ¯2 , d = 96√3τL ,

2

n

p

A

=

σ¯2

+

18τ p

ζ¯2,

B

=

3L,

D

=

72L τp .

It

is

only

left

to

show

that

chosen

weights

wt

stepsizes

ηt

in

Lemma

15

satisfy

conditions

of

Lemmas

8,

9,

14.

It

is

shown

in

Proposition

4

that

{ηt}

is

8τ p

-slow

decreasing

and

{wt}

is

16τ p

-slow

increasing

(condition

in

Lemma

14).

Moreover

the

stepsize

ηt

:=

η

<

1 d

is

smaller

than

conditions

on

ηt

in

Lemmas

8,

9,

14.

A Uniﬁed Theory of Decentralized SGD

Proof of Theorem 2, weakly convex case. The proof follows by applying the result of Lemma 16 to the equation (19)

(obtained with Lemmas 8, 9, 14) with rt = E x¯(t) − x 2, et = f (x¯(t)) − f (x ), a = 0, b = 1, c = σ¯2 , d = 96√6τL ,

n

p

A = 8σ¯2 + 18pτ ζ¯2, B = 3L, D = 72L τp . It is shown in Proposition 4 that {ηt} and {wt} chosen in Lemma 16 satisfy

condition

in

Lemma

14:

{ηt}

is

8pτ -slow

decreasing

and

{wt}

is

16pτ -slow

increasing.

Moreover

the

stepsize

ηt

:=

η

<

1 d

is

smaller than conditions on ηt in Lemmas 8, 9, 14.

Proof of Theorem 2, non-convex case. applying the result of Lemma 16 to the equation (19) (obtained with Lemmas 11, 12, 14) with rt = E f (x¯(t)) − f , et = ∇f (x¯(t)) 22, a = 0, b = 14 , c = Lnσˆ2 , d = 64L 2 max{P, 1} 6pτ + M τp ,
A = 2σˆ2 + 2 6pτ + M ζˆ2, B = L2, D = 2P 6pτ + M . Weights wt stepsizes ηt chosen in Lemma 16 satisfy conditions of Lemmas 11, 12, 14, as shown in Proposition 4.

A.4. Improved rate when τ = 1 (recovering mini-batch SGD convergence results)

In the special case when τ = 1 the proof can be simpliﬁed and the rate can be improved: there will be an additional (1 − p) factor appearing in the middle term, e.g in strongly convex case the improved rate reads as

O˜ σ¯2 + L(ζ¯2 + pσ¯2)(1 − p) + LR02 exp − µT p .

nµT

µ2p2T 2

p

L

The main difference to the general result stated in Theorem 2 (for τ ≥ 1) is that the second term is multiplied with (1 − p), allowing to recover the rate of mini-batch SGD in the case of fully-connected graph when p = 1. This improvement also holds for the weakly-convex and non-convex case.

In order to do so, one has to observe that the consensus distance Lemmas 9 and 12 can be improved when τ = 1. In the ﬁrst

lines of both these proofs we multiply with (1 − p) not only the ﬁrst term

X(t) − X¯ (t)

2
but also the second term with

2

the gradient as during the 1-step averaging both x(t) and ηt∂Fi(X(t), ξi(t)) are averaged with mixing matrix W (t) (line 4

of Algorithm 2). We omit the full derivations for this special case, as they can easily be obtained by following the current

proofs.

B. Technical Preliminaries

B.1. Implications of the assumptions

Proposition 1. One step of gossip averaging with the mixing matrix W (def. 1) preserves the average of the iterates, i.e.

11

11

XW = X .

n

n

Proposition 2 (Implications of the smoothness Assumption 1a). If for functions Fi(x, ξ) Assumption 1a holds, then it also holds that

Fi(x, ξ) ≤ Fi(y, ξ) + ∇Fi(y, ξ), x − y + L2 x − y 22 , ∀x, y ∈ Rd, ξ ∈ Ωi (20)

If functions fi(x) = Eξ Fi(x, ξ), then

fi(x) ≤ fi(y) + ∇fi(y), x − y + L2 x − y 22 , ∀x, y ∈ Rd (21)

Moreover, if in addition Fi are convex functions, then

∇fi(x) − ∇fi(y) 2 ≤ L x − y 2 , ∇g(x) − ∇g(y) 22 ≤ 2L (g(x) − g(y) − x − y, ∇g(y) ) ,

∀x, y ∈ Rd,

(22)

∀x, y ∈ Rd,

(23)

where g(x) is either Fi or fi. Proposition 3 (Implications of the smoothness Assumption 1b). From Assumption 1b it follows that

fi(x) ≤ fi(y) + ∇fi(y), x − y + L2 x − y 22 , ∀x, y ∈ Rd . (24)

A Uniﬁed Theory of Decentralized SGD

B.2. Useful Inequalities Lemma 4. For arbitrary set of n vectors {ai}ni=1, ai ∈ Rd

n
ai
i=1

2

n

≤n

i=1

ai 2 .

(25)

Lemma 5. For given two vectors a, b ∈ Rd

2 a, b ≤ γ a 2 + γ−1 b 2 ,

∀γ > 0 .

(26)

Lemma 6. For given two vectors a, b ∈ Rd a + b 2 ≤ (1 + α) a 2 + (1 + α−1) b 2 ,

∀α > 0 .

(27)

This inequality also holds for the sum of two matrices A, B ∈ Rn×d in Frobenius norm. Remark 7. For A ∈ Rd×n, B ∈ Rn×n

AB F ≤ A F B 2 .

(28)

B.3. τ -slow Sequences Deﬁnition 2 (τ -slow sequences (Stich & Karimireddy, 2019)). The sequence {at}t≥0 of positive values is τ -slow decreasing for parameter τ > 0 if
1 at+1 ≤ at, ∀t ≥ 0 and, at+1 1 + 2τ ≥ at, ∀t ≥ 0 .
The sequence {at}t≥0 is τ -slow increasing if {a−t 1}t≥0 is τ -slow decreasing. Proposition 4 (Examples).

1.

The sequence {ηt2}t≥0 with ηt

=

b+a t , b ≥

32 p

is

p4 -slow decreasing.

2. The sequence of constant stepsizes {ηt2}t≥0 with ηt = η is τ -slow decreasing for any τ . 3. The sequence {wt}t≥0 with wt = (1 − 64pτc )−t, c ≥ 1 is 16pτ -slow increasing.

4.

The sequence {wt}t≥0 with wt

= (b + t)2, b ≥

128 p

is

1p6 -slow increasing.

5. The sequence of constant weights {wt}t≥0 with wt = 1 is τ -slow increasing for any τ .

C. Descent Lemmas and Consensus Recursions
In this section, according to our proof sketch we derive descent (15) and consensus recursions (18) for both convex and also non-convex cases.

C.1. Convex Cases

We require both fi and Fi to be convex. We do not need Assumption 2 to hold for all x, y ∈ Rd and we could weaken it to hold only for x = x and for all y ∈ Rd.

Proposition 5 (Mini-batch variance). Let functions Fi(x, ξ) , i ∈ [n] be L-smooth (Assumption 1a) with bounded noise at

the

optimum

(Assumption

3a).

Then

for

any

xi

∈

Rd, i

∈

[n]

and

x¯

:=

1 n

n i=1

xi

it

holds

Eξ1 ,...,ξn

1n

2 3L2 n

2 6L

3σ¯2

n (∇fi(xi) − ∇Fi(xi, ξi)) ≤ n2

xi − x¯ + n (f (x¯) − f (x )) + n .

i=1

i=1

A Uniﬁed Theory of Decentralized SGD

Proof.

Eξ1 ,...,ξn

1n

2 1n

n (∇fi(xi) − ∇Fi(xi, ξi)) ≤ n2 Eξi

i=1

i=1

∇Fi(xi, ξi) − ∇fi(xi) 2

3n ≤ n2 Eξi
i=1

∇Fi(xi, ξi) − ∇Fi(x¯, ξi) − ∇fi(xi) + ∇fi(x¯) 2

+

2
∇Fi(x¯, ξi) − ∇Fi(x , ξi) − ∇fi(x¯(t)) + ∇fi(x ) +

∇Fi(x , ξi) − ∇fi(x ) 2

3n ≤ n2 Eξi
i=1
≤ 3 n L2 n2
i=1

(t) (t)

2

2

∇Fi(xi , ξi ) − ∇Fi(x¯, ξi) + ∇Fi(x¯, ξi) − ∇Fi(x , ξi) +

x(it) − x¯ 2 + 2L fi(x¯(t)) − fi(x ) + σi2 ,

∇Fi(x , ξi) − ∇fi(x ) 2

where we used that E Y − a 2 = E Y 2 − a 2 ≤ E Y 2 if a = E Y .

Lemma 8 (Descent lemma for convex cases). Under Assumptions 1a, 2, 3a and 4, the averages x¯(t) := n1

iterates

of

Algorithm

1

with

the

stepsize

ηt

≤

1 12L

satisfy

n i=1

x(it)

of

the

E (t) (t) x¯(t+1) − x
ξ1 ,...,ξn

2 ≤ 1 − ηtµ 2

x¯(t) − x 2 + ηt2nσ¯2 − ηt f (x¯(t)) − f

3L n + ηt n
i=1

x¯(t) − x(t)

2
,

i

(29)

where σ¯2 = n1

n i=1

σi2.

Proof. Because all mixing matrixes preserve the average (Proposition 1), we have

x¯(t+1) − x

n

2

2
=

x¯(t) − ηt

n

∇Fi(x(it), ξi(t)) − x

i=1

n

n

n

2

= x¯(t) − x − ηt n

(t) ηt

∇fi(xi

)+ n

(t) ηt

∇fi(xi

)− n

∇Fi(x(it), ξi(t))

i=1

i=1

i=1

n

2

n

n

2

= x¯(t) − x − ηt n

∇fi(x(it)) + ηt2 n1

(t) 1

∇fi(xi

)− n

∇Fi(x(it), ξi(t)) +

i=1

i=1

i=1

2ηt (t)

ηt n

n (t)

n (t)

(t) (t)

+

x¯ − x −

n

n

∇fi(xi ), ∇fi(xi ) − ∇Fi(xi , ξi ) .

i=1

i=1

i=1

The last term is zero in expectation, as Eξ(t) ∇Fi(x(it), ξi(t)) = ∇fi(x(it)). The second term is estimated using Proposition 5. i
The ﬁrst term can be written as:

n

2

n

2

n

x¯(t) − x − ηt n

∇fi(x(it)) = x¯(t) − x 2 + ηt2 n1

∇fi(x(it))

− 2ηt

x¯(t) − x , 1 n

∇fi(x(it)) .

i=1

i=1

i=1

=:T1

=:T2

A Uniﬁed Theory of Decentralized SGD

We can estimate

T1 =

n

2

n1 (∇fi(x(it)) − ∇fi(x¯(t)) + ∇fi(x¯(t)) − ∇fi(x ))

i=1

(25) 2 n ≤
n i=1

∇fi(x(it)) − ∇fi(x¯(t)) 2 + 2

n

n

2

1 ∇fi(x¯(t)) − 1 ∇fi(x )

n i=1

n i=1

(22),(23) 2L2 n ≤ n i=1

(t)

2 (t)

4L n

xi − x¯

+ n

i=1

fi(x¯(t)) − fi(x )

2L2 n

(t)

2 (t)

(t)

= n

xi − x¯ + 4L f (x¯ ) − f .

i=1

And for the remaining T2 term:

1

2n

− ηt T2 = − n

i=1

x¯(t) − x(it), ∇fi(x(it)) + x(it) − x , ∇fi(x(it))

(21),(5) 2 n (t)

(t)

L (t)

(t) 2

(t)

µ (t)

2

≤− n

fi(x¯

) − fi(xi

)− 2

x¯

− xi

+ fi(xi ) − fi(x ) + 2 xi − x

i=1

(25) (t)

L + µ n (t) (t) 2 µ (t)

2

≤ −2 f (x¯ ) − f (x ) + n

x¯ − xi

− x¯ − x 2

,

i=1

Where at the last step (25) was applied to x¯(t) − x 2 ≤ 2 x¯(t) − x(it) 2 + 2 x(it) − x

and

using

that

ηt

≤

1 12L

we

are

getting

statement

of

the

lemma.

2
. Putting everything together

Lemma 9 (Recursion for consensus distance). Under Assumptions 1a, 2, 3a and 4, if in addition functions Fi are convex and if stepsizes ηt ≤ 96√p6τL , then

Ξt ≤ 1 − p2 Ξmτ + 64pτ t−1 Ξj + 72 τp L t−1 ηj2 f (x¯(j)) − f (x ) + 8σ¯2 + 18pτ ζ¯2

t−1
ηj2,

j=mτ

j=mτ

j=mτ

where Ξt = n1 E ni=1 x(it) − x¯(t) 2 is a consensus distance, m = t/τ − 1.

Proof. Using matrix notation (14), for t ≥ τ

2
nΞt = E X(t) − X¯ (t) = E X(t) − X¯ (mτ) − X¯ (t) − X¯ (mτ)
F

2

2

≤ E X(t) − X¯ (mτ) ,

F

F

where we used that A − A¯ 2 =
F
of the Algorithm 2,

n i=1

ai − a¯

2 2

≤

n i=1

ai 22 =

A 2F . Unrolling X(t) up to X(mτ) using lines 3–4

nΞt ≤ E =E

mτ

t−1

2 j

X (mτ )

W (i) − X¯ (mτ) −

ηj ∂F (X(j), ξ(j))

W (i)

i=t−1

j=mτ

i=t−1 F

mτ

t−1

j

t−2

X (mτ )

W (i) − X¯ (mτ) −

ηj ∂f (X(j))

W (i) −

ηj

i=t−1

j=mτ

i=t−1

j=mτ

+E

ηt−1

∂F (X(t−1), ξ(t−1)) − ∂f (X(t−1))

j

2

W (i)

i=t−1

F

∂F (X(j), ξ(j)) − ∂f (X(j))

2 j
W (i)
i=t−1 F

A Uniﬁed Theory of Decentralized SGD

where we used that E ∂F (X(t−1), ξ(t−1)) = ∂f (X(t−1)) and that ξ(t−1) is independent of the rest. To separate the rest of

the stochastic terms similar way (terms with ∂F (X(j), ξ(j)) − ∂f (X(j))), since X(t−1) depends on ξ(t−2), we ﬁrst need to

separate

the

term

with

∂f (X(t−1)).

Let

β1

=

1 C −1

for

some

constant

C

which

we

will

deﬁne

later,





2

(27),(28)

mτ

t−2

t−2

j

nΞt ≤ (1 + β1) E X(mτ)

W (i) − X¯ (mτ) − 

ηj ∂f (X(j)) −

ηj ∂F (X(j), ξ(j)) − ∂f (X(j)) 

W (i)

= CC−1

i=t−1

j=mτ

j=mτ

i=t−1 F

2

2

+ (1 + β1−1) E ηt−1∂f (X(t−1)) + ηt2−1 E ∂F (X(t−1), ξ(t−1)) − ∂f (X(t−1)) ,

F

F

=C

Now, similarly, we split terms that depend on X(t−2) with β2 = C−1 2 . Note that (1 + β1)(1 + β2−1) = C and (1 + β1)(1 + β2) = CC−2 :

C nΞt ≤ C − 2 E



mτ

t−2

t−2

X (mτ )

W (i) − X¯ (mτ) − 

ηj ∂f (X(j)) −

ηj

i=t−1

j=mτ

j=mτ



2

j

∂F (X(j), ξ(j)) − ∂f (X(j)) 

W (i)

i=t−1 F

t−1

2

t−1

+C

E ηj ∂f (X(j)) +

C ηj2 E ∂F (X(j), ξ(j)) − ∂f (X(j)) 2 ,

F

C + j − (t − 1)

F

j=t−2

j=t−2

Splitting the same way the rest of the terms and using that C+j−C(t−1) ≤ 2 for C ≥ 2τ ,

C nΞt ≤ C − 2τ E

mτ

X (mτ )

W (i) − X¯ (mτ )

i=t−1

2

t−1

+C

E

η

2 j

F

j=mτ

2

t−1

∂f (X(j)) +

2ηj2 E

F

j=mτ

2
∂F (X(j), ξ(j)) − ∂f (X(j)) ,
F

Taking C = 2τ (1 + p2 ) and using (13) to bound the ﬁrst term we get that

nΞt ≤ 1 − p2 E X(mτ) − X¯ (mτ) 2F + 6pτ t−1 ηj2 E ∂f (X(j)) 2F + t−1 2ηj2 E ∂F (X(j), ξ(j)) − ∂f (X(j)) 2F ,

j=mτ

j=mτ

:=T1

:=T2

Estimating separately the last two terms, and using the notation ±a = a − a = 0 ∀a,

2 (25)
T1 = E ∂f (X(j)) ± ∂f (X¯ (j)) ± ∂f (X ) ≤ 3 E ∂f (X(j)) − ∂f (X¯ (j))
F

(3),(23),(6)
≤3

2
L2 E X(j) − X¯ (j) + 2Ln E

f (x¯(j)) − f (x )

+ nζ¯2

F

2
+3E
F

∂f (X¯ (j)) − ∂f (X )

2
+3
F

∂f (X ) 2F

2
T2 = E ∂F (X(j), ξ(j)) ± ∂F (X¯ (j), ξ(j)) ± ∂F (X , ξ(j)) − ∂f (X(j)) ± ∂f (X¯ (j)) ± ∂f (X )
F

(25),(3),(23)

2

2

≤ 4 E 4L2 X(j) − X¯ (j) + 4Ln E f (x¯(j)) − f (x ) + ∂F (X , ξ(j)) − ∂f (X ) ,

F

F

where the last term is bounded by nσ¯2 by deﬁnition (7). Putting back estimates for T1 and T2 and using that ηt ≤ 96√p6τL we arrive to the statement of the lemma.

This recursion in Lemma 9 holds only when t ≥ (m + 1)τ . For these steps we are guaranteed to get (1 − p) decrease by Assumption 4. To simplify this recursion we would need similar relation also for smaller t that is mτ ≤ t < (m + 1)τ , that we derive in Lemma 10.

A Uniﬁed Theory of Decentralized SGD

Lemma 10 (Second recursion for consensus distance). Under Assumptions 1a, 2, 3a and 4, if in addition functions Fi are convex and if stepsizes ηt ≤ 96√p6τL , then

Ξt ≤ 1 + p2 Ξmτ + 64pτ t−1 Ξj + 72 τp L t−1 ηj2 f (x¯(j)) − f (x ) + 8σ¯2 + 18pτ ζ¯2

t−1
ηj2,

j=mτ

j=mτ

j=mτ

where Ξt = n1 Et ni=1 x(it) − x¯(t) 2 is a consensus distance, and t is such that mτ ≤ t < (m + 1)τ .

Proof. The proof follows exactly the same lines as in Lemma 9, with the change that we don’t use (13) to decrease the consensus distance by (1 − p), but instead we use the Deﬁnition 1 that each W (i) is doubly stochastic

mτ 2 2

E X(mτ )

W (i) − X¯ (mτ) ≤ E X(mτ) − X¯ (mτ) .

F

i=t−1

F

C.2. Non-convex Case

Here we derive descent recursive equation (15) and recursion for consensus distance (16) for the non-convex case.

Proposition 6 (Mini-batch variance). Let functions Fi(x, ξ) , i ∈ [n] be L-smooth (Assumption 1a) with bounded noise as

in

Assumption

3b.

Then

for

any

xi

∈

Rd, i

∈

[n]

and

x¯

:=

1 n

n i=1

xi

it

holds

1n

2 σˆ2 M n

2

Eξ1,...,ξn n (∇fi(xi) − ∇Fi(xi, ξi)) ≤ n + n2

∇f (xi)

(30)

i=1

i=1

Lemma 11 (Descent lemma for non-convex case). Under Assumptions 1b, 3b and 4, the averages x¯(t) := n1 the iterates of Algorithm 1 with the constant stepsize η < 4L(M1 +1) satisfy

n i=1

x(it)

of

(t+1)

(t) η

2 (t)

ηL2 n

(t)

(t) 2 L 2 2

Et+1 f (x¯

) ≤ f (x¯ ) − ∇f (x¯ ) +

4

2n

x¯

− xi

+ η σˆ . 2n

(31)

i=1

Proof. Because all mixing matrixes preserve the average (Proposition 1) and function f is L-smooth, we have

Et+1 f (x¯(t+1)) = Et+1 f

(t) η n

(t) (t)

x¯ − n

∇Fi(xi , ξi )

i=1

≤ f (x¯(t)) − Et+1

(t) η n

(t) (t)

∇f (x¯ ), n

∇Fi(xi , ξi )

i=1

:=T1

+Et+1 L η2 2

2

1n

(t) (t)

n ∇Fi(xi , ξi )

j=1 2

:=T2

To estimate the second term, we add and subtract ∇f (x¯(t))

T1 = −η

∇f (x¯(t)) 2 + η n n
i=1

∇f (x¯(t)), ∇fi(x¯(t)) − ∇fi(x(it))

(26),γ=1;(25) η ≤− 2

∇f (x¯(t)) 2 + η n 2n
i=1

∇fi(x¯(t)) − ∇fi(x(it)) 2

A Uniﬁed Theory of Decentralized SGD

For the last term, using the notation ± a = a − a = 0 ∀a,

T2 = Et+1

1n n j=1

∇Fi(x(it), ξi(t)) − ∇fi(x(it))

2 n

2

1

(t)

+ n

∇fi(xi )

2 i=1

2

(30) σˆ2 M n ≤ n + n2
i=1

∇f (x(t)) ± ∇f (x¯(t))

2
+

i

n

2

n1 ∇fi(x(it)) ± ∇f (x¯(t))

i=1

2

(27) σˆ2 2M n ≤ n + n2
i=1

∇f (x(t)) − ∇f (x¯(t))

2
+ (2M/n + 2)

i

∇f (x¯(t)) 2 + 2 n 2n
i=1

∇fi(x(it)) − ∇fi(x¯(t)) 2
2

Combining this together and using L-smoothness to estimate ∇fi(x¯(t)) − ∇fi(x(it)) 2 and ∇f (x¯(t)) − ∇f (x(it)) 2,

2

2

Et+1 f (x¯(t+1)) ≤ f (x¯(t)) − η

1 − Lη(M + 1)

2

2
∇f (x¯(t)) +
2

ηL2 L3η2(M + 1)

+

2n

n

n

(t)

(t) 2 L 2 2

x¯

− xi

+ η σˆ . 2n

i=1

Applying η < 4L(M1 +1) we get statement of the lemma.

Lemma 12 (Recursion for consensus distance). Under Assumptions 1b, 3b and 4, if the stepsize ηt ≤ √ p

, then

8L 2τ (6τ +pM )

Ξt ≤

p 1−
2

p t−1

Ξmτ + 16τ

Ξj + 2P

j=mτ

6τ +M
p

t−1
ηj2
j=mτ

2
∇f (x¯(j)) +
2

2σˆ2 + 2

6τ +M
p

t−1

ζˆ2

ηj2

j=mτ

where Ξt = n1 Et ni=1 x(it) − x¯(t) 2 is a consensus distance, m = t/τ − 1.

Proof.

We start exactly the same way as in the convex proof in Lemma 9 Deﬁning Ξt

=

1 n

Et

m = t/τ − 1 and using matrix notation (11), for t ≥ τ (and therefore m ≥ 0)

ni=1 x(it) − x¯(t) 2,

2
nΞt = E X(t) − X¯ (t) = E X(t) − X¯ (mτ) − X¯ (t) − X¯ (mτ)
F

2

2

≤ E X(t) − X¯ (mτ) ,

F

F

where we used that A − A¯ 2 =
F

n i=1

ai − a¯

≤

n i=1

ai

2 F

=

A 2F . Unrolling X(t) up to X(mτ) using lines 3-4 of

the Algorithm 2 and splitting stochastic terms similar way as for the convex cases in Lemma 9,

mτ

t−1

2 j

nΞt ≤ E X(mτ)

W (i) − X¯ (mτ) −

ηj ∂F (X(j), ξ(j))

W (i)

i=t−1

j=mτ

i=t−1 F

≤

p 1−

E

X(mτ ) − X¯ (mτ )

2

6τ +

t−1
η2 E

2

t−1

∂f (X(j)) +

2η2 E

2
∂F (X(j), ξ(j)) − ∂f (X(j))

2

Fp

j

Fj

F

j=mτ

j=mτ

(9)
≤

p 1−

E

2
X(mτ) − X¯ (mτ) +

2

F

6τ +M
p

t−1
ηj2
j=mτ

∂f (X(j))
:=T

2

t−1

+

2ηj2nσˆ2

F

j=mτ

Estimating T ,

(27)
T ≤2

2
∂f (X(j)) − ∂f (X¯ (j)) + 2

2 (4),(8)
∂f (X¯ (j)) ≤ 2L2

X(j) − X¯ (j)

2
+ 2nζˆ2 + 2P n

2
∇f (x¯(j))

F

F

F

2

Putting back estimate for T and using that ηt ≤ √ p

we arrive to the statement of this lemma.

8L 2τ (6τ +pM )

A Uniﬁed Theory of Decentralized SGD

Similarly to the convex cases, we additionally need a recursion for values t that are in between mτ ≤ t < (m + 1)τ

Lemma 13 (Second recursion for consensus distance). Under Assumptions 1b, 3b and 4, if the stepsize ηt ≤ √ p

,

8L 2τ (6τ +pM )

and t such that mτ ≤ t < (m + 1)τ then

Ξt ≤

p 1+
2

p t−1

Ξmτ + 64τ

Ξj + 2P

j=mτ

6τ +M
p

t−1
ηj2
j=mτ

2
∇f (x¯(j)) +
2

σˆ2 + 2

6τ +M
p

t−1

ζˆ2

ηj2

j=mτ

where Ξt = n1 Et ni=1 x(it) − x¯(t) 2 is a consensus distance.

Proof. As in the convex case, we need to change the proof of Lemma 12 just slightly, by applying Def. 1 instead of (13) as follows

mτ 2 2

E X(mτ )

W (i) − X¯ (mτ) ≤ E X(mτ) − X¯ (mτ)

F

i=t−1

F

C.3. Simplifying Consensus Recursion
In Lemmas 9, 12 we obtained the consensus recursive equation (16) for both convex and non-convex cases. In this section we simplify it to be able to easily combine it later with (15).
Lemma 14. If non-negative sequences {Ξt}t≥0, {et}t≥0 and {ηt}t≥0 satisfy (16) and (17) for some constants 0 < p ≤ 1, τ ≥ 1, A, D ≥ 0, moreover if the stepsizes {ηt2}t≥0 is 8pτ -slow decreasing sequence (Deﬁnition 2), and if {wt}t≥0 is 16pτ -slow increasing non-negative sequence of weights, then it holds that

T

bT

τT

B wtΞt ≤

wtet + 64BA

wtηt2,

2

p

t=0

t=0

t=0

for

some

constant

B

>

0

with

the

constraint

that

stepsizes

ηt

≤

1 16

DpBbτ .

Proof. Recursively substituting every Ξj for j ≥ (m + 1)τ in the second term of (16) we get

Ξt ≤

p

pτ

p τ p (m+1)τ −1

t−1

1 − Ξmτ 1 +

+ 1+

Ξj + D

p 1+

t−1−j
ηj2ej

2

64τ

64τ 64τ

64τ

j=mτ

j=(m+1)τ

(m+1)τ −1
+D
j=mτ

p 1+
64τ

t−(m+1)τ

t−1

ηj2ej + A

j=(m+1)τ

p 1+
64τ

t−1−j

(m+1)τ −1

ηj2 + A

j=mτ

p 1+
64τ

t−(m+1)τ
ηj2

We substitute the rest of Ξj for mτ ≤ j < (m + 1)τ with (17). Lets start with substituting Ξ(m+1)τ−1

Ξt ≤



pτ

p

p

1 + 64τ  1 − 2 Ξmτ + 64τ

p

p

1 + 2 Ξmτ + 1 + 64τ

 p (m+1)τ −2 64τ Ξj +
j=mτ

t−1
+D
j=(m+1)τ −1

p 1+
64τ

t−1−j

(m+1)τ −2

ηj2ej + D

j=mτ

p 1+
64τ

t−(m+1)τ +1
ηj2ej

t−1
+A
j=(m+1)τ −1

p 1+
64τ

t−1−j

(m+1)τ −2

ηj2 + A

j=mτ

p 1+
64τ

t−(m+1)τ +1
ηj2

A Uniﬁed Theory of Decentralized SGD

Since 0 < p ≤ 1, it holds that 64pτ 1 + p2 ≤ 1 − p2 16pτ and therefore



pτ

p

p

Ξt ≤ 1 + 64τ  1 − 2 Ξmτ 1 + 16τ

p + 1+
64τ

 p (m+1)τ −2 64τ Ξj +
j=mτ

t−1
+D
j=(m+1)τ −1

p 1+
64τ

t−1−j

(m+1)τ −2

ηj2ej + D

j=mτ

p 1+
64τ

t−(m+1)τ +1
ηj2ej

t−1
+A
j=(m+1)τ −1

p 1+
64τ

t−1−j

(m+1)τ −2

ηj2 + A

j=mτ

p 1+
64τ

t−(m+1)τ +1
ηj2

Applying the same way (17) to the rest of Ξj

and using that

p 64τ

≤

p 16τ

we get that

Ξt ≤

p 1−
2

Ξmτ

p 1+
16τ

2τ

t−1

+D

j=mτ

p 1+
16τ

t−1−j

t−1

ηj2ej + A

j=mτ

p 1+
16τ

t−1−j
ηj2

Using that

1 + 16pτ

2τ ≤ exp

p8 ≤ 1 + p4 for p ≤ 1 and also that (1 + 16pτ )t−1−j ≤ 1 + 16pτ

Ξt ≤

p 1−
4

t−1

t−1

Ξmτ + 2D

ηj2ej + 2A

ηj2,

j=mτ

j=mτ

2τ ≤ 1 + p4 ≤ 2

Unrolling Ξmτ recursively up to 0 we get,

t−1
Ξt ≤ 2D
j=0

p 1−
4

(t−j)/τ

t−1
ηj2ej + 2A
j=0

p 1−
4

(t−j)/τ
ηj2,

For the ﬁrst term estimating 1 − p 1/τ ≤ exp(− p ) ≤ 1 − p and that 1 − p τ (t−j)/τ ≤ 1 − p t−j 1 − p −τ .

4

τ

4τ

8τ

8τ

8τ

8τ

p −τ ≤

1
p

≤ (1 + p )τ because p ≤ 1 and ﬁnally 1 + p τ ≤ exp( p ) < 2,

For the last term, 1 − 8τ

1−

4τ

8τ 2

8τ

4τ

4

t−1
Ξt ≤ 4D
j=0

p 1−
8τ

t−j

t−1

ηj2ej + 4A

j=0

p 1−
8τ

t−j
ηj2,

Now

using

that

ηt2

is

8τ p

-slow

decreasing,

i.e.

ηj2

≤

ηt2

1 + 16pτ

t−j and using that (1 − 8pτ )(1 + 16pτ ) ≤ (1 − 16pτ )

t−1
Ξt ≤ 4Dηt2
j=0

p 1−
16τ

t−j

t−1

ej + 4Aηt2

j=0

p 1−
16τ

t−j

t−1

≤ 4Dηt2

j=0

p 1−
16τ

t−j ej + 64A τp ηt2

Now averaging Ξt with weights wt and using that wt is 16pτ -slow increasing sequence, i.e. wt ≤ wj 1 + 32pτ t−j, and also

using

that

ηt

≤

1 16

pb DBτ

T

T

t−1

B wtΞt ≤ 4DB ηt2 wj

t=0

t=0 j=0

p 1−
32τ

t−j

τT

ej + 64AB

wtηt2

p

t=0

pb T t−1 ≤ 64τ wj
t=0 j=0

p 1−
32τ

t−j

τT

ej +64AB

wtηt2

p

t=0

:=T1

And ﬁnally,

pb T

T

T1 = 64τ wjej

j=0

t=j+1

p 1−
32τ

t−j pb T

∞

≤ 64τ

wj ej

j=0

t=0

p 1−
32τ

t−j b T ≤ 2 wtet.
t=0

A Uniﬁed Theory of Decentralized SGD

D. Solving the Main Recursion (19)

D.1. a > 0 (strongly convex case)

Lemma 15. If non-negative sequences {rt}t≥0, {et}t≥0 satisfy (19) for some constants a, b, p > 0, c, A, B, τ ≥ 0, then

there

exists

a

constant

stepsize

ηt

=

η

<

1 d

such

that

for

weights

wt

=

(1 − aη)−(t+1)

and

WT

:=

T t=0

wt

it

holds:

1T be w + ar

≤ O˜

a(T + 1) r d exp −

+

c

BA τ +

,

2WT

tt

T +1

0

d

aT a2T 2 p

t=0

where O˜ hides polylogarithmic factors.

Proof.

Starting from (19) and using that ηt

= η and that

wt (1−aη ) η

=

wt−1 η

we obtain a telescoping sum,

1T

1

τ

bwtet ≤

((1 − aη)w0r0 − wT rT +1) + cη + 64BA η2 ,

2WT t=0 WT η p

And hence,

1 T bwtet + wT rT +1 ≤ r0 + cη + 64BA τ η2 , 2WT t=0 WT η WT η p

Using that WT

≤

wT aη

and WT

≥ wT

= (1 − aγ)−(T +1) we can simplify

1 T bwtet + arT +1 ≤ (1 − aη)T +1 r0 + cη + 64BA τ η2 ≤ r0 exp [−aη(T + 1)] + cη + 64BA τ η2 , 2WT t=0 η p η p

Now lemma follows by tuning η the same way as in (Stich, 2019a).

• If d1 ≥ ln(max{2,aaT2r0T 2/c}) then we choose η = ln(max{2,aaT2r0T 2/c}) and get that

O˜ ar0T exp − ln(max{2, a2r0T 2/c}) + O˜ c + O˜ BA τ = O˜ c + O˜ BA τ ,

aT

a2T 2 p

aT

a2T 2 p

• Otherwise d1 ≤ ln(max{2,aaT2r0T 2/c}) we pick η = d1 and get that

O˜

a(T + 1) r0d exp −

c BA τ ++

≤ O˜

a(T + 1) r0d exp −

+

c

BA τ +

.

d

d d2 p

d

aT a2T 2 p

D.2. a = 0 (weakly convex and non-convex cases)

Now we assume that in Assumption 2 µ = 0, which means that a = 0 in (19).

Lemma 16. If non-negative sequences {rt}t≥0, {et}t≥0 satisfy (19) with a = 0, b > 0, c, A, B ≥ 0, then there exists a

constant

stepsize

ηt

=

η

<

1 d

such

that

for

weights

{wt

=

1}t≥0

it

holds

that:

1T

1
cr0 2

BAτ 1/3

r0

2 3

dr0

(T + 1) et ≤ O 2 T + 1 + 2 p

+

.

T +1

T +1

t=0

Proof. With a = 0, constant stepsizes ηt = η and weights {wt = 1}t≥0 (19) is equivalent to

1 T et ≤ 1 T (rt − rt+1) + cη + 64 BAτ η2 ≤ r0 + cη + 64 BAτ η2.

2(T + 1)

(T + 1)η

p

(T + 1)η

p

t=0

t=0

To conclude the proof we tune the stepsize using Lemma 17.

A Uniﬁed Theory of Decentralized SGD

Lemma 17 (Tuning the stepsize).

For any parameters r0

≥

0, b

≥

0, e

≥

0, d

≥

0 there exists constant stepsize η

≤

1 d

such that

ΨT := r0 + bη + eη2 ≤ 2 η(T + 1)

br0 T +1

1 2
+ 2e1/3

r0 T +1

2
3 + dr0 T +1

Proof. Choosing η = min

1

1

b(Tr+0 1) 2 , e(Tr+0 1) 3 , d1

≤ d1 we have three cases

1

1

• η = d1 and is smaller than both b(Tr+0 1) 2 and e(Tr+0 1) 3 , then

1

2

ΨT ≤ dr0 + b + e ≤

br0

2
+

dr0

+ e1/3

r0

3

T + 1 d d2 T + 1

T +1

T +1

• η=

r0 b(T +1)

1
2<

1
e(Tr+0 1) 3 , then

1

1

2

r0b 2

r0

r0b 2

1

r0

3

ΨT ≤ 2

+e

≤2

+ e3

,

T +1

b(T + 1)

T +1

(T + 1)

1

1

• The last case, η = e(Tr+0 1) 3 < b(Tr+0 1) 2

1
ΨT ≤ 2e 3

r0 (T + 1)

2 3
+b

r0 e(T + 1)

1

3

1

≤ 2e 3

2
r0 3 + (T + 1)

1
br0 2 . T +1

E. Lower Bound

Proof of Theorem 3.

We consider minimization problem of the form (1) with fi(x) =

1 2

(x

−

yi)2,

x,

yi

∈ R which has the

solution x = n1

n i=1

yi,

L

=

µ

=

1.

We

denote

x

=

(x1,

.

.

.

,

xn)

and ∇f (x) = (∇f1(x1), . . . , ∇fn(xn)) .

We assume that the starting point x(0) is an eigenvector of W , corresponding to the second largest eigenvalue, i.e. W x(0) = λ2x(0) and we set yi such that y = 1 + x(0). With this choice of y, ζ¯2 = x(0) 22. It will be also useful to note that the average x¯(0) = 0 since it is orthogonal to 1, the eigenvector of W corresponding to the largest eigenvalue. We use
the notation z¯ := n1 11 z.

We start the proof by decomposing the error n1 x(t) − y¯ 22 on consensus and optimization terms

1

x(t) − y¯

2
=

1

x(t) − x¯(t) + x¯(t) − y¯

2
=

1

x(t) − x¯(t)

21 +

2
x¯(t) − y¯ .

n

2n

2n

2n

2

Using that for our chosen functions ∇f (x) = x − y, we can estimate the optimization term as

2

2

2

2

x¯(t) − y¯ = (1 − η)x¯(t−1) + ηy¯ − y¯ = (1 − η)2 x¯(t−1) − y¯ = (1 − η)2t x¯(0) − y¯ = (1 − η)2tn.

2

2

2

2

A Uniﬁed Theory of Decentralized SGD

For the consensus term,

2
x(t) − x¯(t) =
2

11 W−
n

x

(t

+

1 2

)

−

x¯(

t+

1 2

)

2
=
2

11 W−
n

2
(1 − η) x(t) − x¯(t) + η(y − y¯) =
2

t

t−1

τ +1

2

11 = W−

(1 − η)tx(0) + η

(1 − η)τ

11 W−

(y − y¯) =

n

n

τ =0

2

t−1

2

= λt2(1 − η)tx(0) + η (1 − η)τ λτ2+1x(0)

τ =0

2

t−1 2 2
= λt2(1 − η)t + η (1 − η)τ λτ2+1 x(0)
2 τ =0

 ≥ λ22t(1 − η)2t + η2

t−1

2

(1 − η)τ λτ2+1  nζ¯2

τ =0

In order to guarantee error less than , it is necessary to have simultaneously both optimization and consensus terms less than , therefore it is required that

(1 − η)2t ≤

(32)

(1 − η)2tλ22t ≤ ζ¯2

(33)

t−1

ττ

1 − (1 − η)tλt2

η

(1 − η) λ2

=η

≤

1 − (1 − η)λ2

ζ¯2λ2

(34)

τ =0

2

Equations (33), (34) imply

1 − (1 − η)λ2

1 − λ2 + η

η≤

ζ¯2λ2 1 −

≤ /ζ¯2

ζ¯2λ2 1 −

/ζ¯2

2

2

√

√

Note that λ2 = 1 − p, where p is from Assumption 4. Using that 1 − p ≥ 1 − p for p ∈ [0, 1],

√

1− 1−p+η

p+η

η ≤ ζ¯2(1 − p) 1 − /ζ¯2 ≤ ζ¯2(1 − p) 1 − /ζ¯2

√

ζ¯2 (1−p)

And therefore using that 1 − p ≤ 1 and for ≤ 16 ,

/[ζ¯2(1−p)]p

/[ζ¯2(1−p)]p

η≤

√

≤

≤2

1 − ( √1−p+1 ) /ζ¯2 1 − 2 /[ζ¯2(1−p)]

1−p

/[ζ¯2(1−p)]p

With this upper bound on η, the inequality (32) gives a lower bound on t:

log n

log 1 ζ¯√1 − p log 1

t≥

≥

≥

√

,

(35)

−2 log(1 − η) 2η

4p

here we used that log(1 − η) ≥ −η for η ≤ 45 .

F. Additional Experiments to Verify the O T12 Term
In Theorem 2 we proved an upper bound and in Theorem 3 we proved a lower bound, that indicates that in the noiseless (σ¯2 = 0) strongly convex case the convergence is not linear when ζ¯2 > 0. In this section we verify numerically that this rate indeed reﬂects tightly the convergence behavior of decentralized SGD.

A Uniﬁed Theory of Decentralized SGD

Ring
900

Torus
70

800

700

57.2

600

y = 30.2 x

500

60 50 y = 2.3 x 3.2
40

400

30

300 20

10

15

20

25

30

1/

10

15

20

25

30

1/

T T

Figure 3. Verifying the O p2ζ¯T2 2 convergence for the strongly convex noiseless (σˆ2 = 0) case. Number of iterations to converge to target accuracy on ring (left) and 2-d torus (right).

We consider the same setting as in Section 8 before, with σ¯2 = 0, ζ¯2 = 10, n = 25, and d = 10.
For both ring and 2-d torus (grid), we vary the target accuracy ( ) and tune the stepsize to ﬁnd the smallest number of iterations required (T ) to achieve this target accuracy. In Figure 3 we depict the results, where x-axis is √1 and y-axis is T . Based on the Theorem 2 for strongly convex case, ideally each of them should be a line, as we observe in the plots. Moreover, the ratio of the slopes of these lines is 30.2/2.3 = 13.13 which matches the ratio of the spectral gap of these graphs (pgrid/pring = 0.276/0.021 = 13.142), as it is shown in Theorems 2 and 3.

