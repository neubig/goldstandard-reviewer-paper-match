ADVANCING RNN TRANSDUCER TECHNOLOGY FOR SPEECH RECOGNITION George Saon, Zolta´n Tu¨ske, Daniel Bolanos and Brian Kingsbury IBM Research AI, Yorktown Heights, USA

arXiv:2103.09935v1 [cs.CL] 17 Mar 2021

ABSTRACT
We investigate a set of techniques for RNN Transducers (RNN-Ts) that were instrumental in lowering the word error rate on three different tasks (Switchboard 300 hours, conversational Spanish 780 hours and conversational Italian 900 hours). The techniques pertain to architectural changes, speaker adaptation, language model fusion, model combination and general training recipe. First, we introduce a novel multiplicative integration of the encoder and prediction network vectors in the joint network (as opposed to additive). Second, we discuss the applicability of i-vector speaker adaptation to RNNTs in conjunction with data perturbation. Third, we explore the effectiveness of the recently proposed density ratio language model fusion for these tasks. Last but not least, we describe the other components of our training recipe and their effect on recognition performance. We report a 5.9% and 12.5% word error rate on the Switchboard and CallHome test sets of the NIST Hub5 2000 evaluation and a 12.7% WER on the Mozilla CommonVoice Italian test set.
Index Terms— End-to-end ASR, recurrent neural network transducer, multiplicative integration
1. INTRODUCTION
End-to-end approaches directly map an acoustic feature sequence to a sequence of characters or even words without any conditional independence assumptions. Compared to traditional approaches which integrate various knowledge sources in a complex search algorithm, end-to-end methods resulted in a dramatic simpliﬁcation of both training and decoding pipelines. This led to a rapidly evolving research landscape in end-to-end modeling for ASR with Recurrent Neural Network Transducers (RNN-T) [1] and attention-based models [2, 3] being the most prominent examples. Attention based models are excellent at handling non-monotonic alignment problems such as translation [4], whereas RNN-Ts are an ideal match for the left-to-right nature of speech [5–17].
Nowadays, end-to-end models can reach unprecedented levels of speech recognition performance in spite of, or maybe because of, the signiﬁcantly simpler implementations. It has been shown that, given enough training data, end-to-end models are clearly able to outperform traditional approaches [18]. Nevertheless, data sparsity and overﬁtting are inherent problems for any direct sequenceto-sequence model and various approaches have been proposed to introduce useful variances and mitigate these issues [19–23].
After a short overview of RNN-T sequence modeling (section 2), section 3 investigates an architectural change and section 4 presents efﬁcient training and decoding recipes for such models. The proposed techniques are then evaluated on three different languages (section 5), before conclusions are drawn (section 6). As will be shown, the consistent application of these methods will result in remarkable end-to-end model performance, even if only a few hundred hours of training data are available.

2. RNN-T MODEL DESCRIPTION

Borrowing some notations from [1], RNN-Ts model the conditional distribution p(y|x) of an output sequence y = (y1, . . . , yU ) ∈ Y∗ of length U given an input sequence x = (x1, . . . , xT ) ∈ X ∗ of length T . The elements of x are typically continuous multidimen-
sional vectors whereas the elements of y belong to an output space
which is typically discrete. p(y|x) is expressed as a sum over all possible alignments a = (a1, . . . , aT +U ) that are consistent with y:

p(y|x) =

p(a|x)

(1)

a∈B−1(y)

The elements of a belong to the augmented vocabulary Y = Y ∪{φ} where φ (called BLANK) denotes the null output. The mapping B : Y∗ → Y∗ is deﬁned by B(a) = y. For example, if y = (C, A, T ) and x = (x1, x2, x3, x4) valid alignments include (φ, C, φ, A, φ, T, φ), (φ, φ, φ, φ, C, A, T ), (C, A, T, φ, φ, φ, φ), etc. Furthermore, p(a|x) can be factorized as follows:

T +U
p(a|x) = p(a|h) =∆ p(ai|hti , B(a1, . . . , ai−1)) =

i=1

T +U

T +U

p(ai|hti , y0, . . . , yui−1 ) =

p(ai|hti , gui ) (2)

i=1

i=1

where: h = (h1, . . . , hT ) = Encoder(x) is an embedding of the input sequence computed by an encoder network, g = (g1, . . . , gU ) is an embedding of the output sequence computed by a prediction network via the recursion gu = P rediction(gu−1, yu−1) (with the convention g0 = 0, y0 = φ), and p(ai|hti , gui ) is the predictive output distribution over Y computed by a joint network which is
commonly implemented as:

p(·|ht, gu) = softmax[Wouttanh(Wencht +Wpredgu +b)] (3)

Wenc ∈ IRJ×E , Wpred ∈ IRJ×P are linear projections that map ht ∈ IRE and gu ∈ IRP to a joint subspace which, after addition and

tanh, is mapped to the output space via Wout ∈ IR|Y|×J . RNN-Ts

are typically trained to a minimize the negative log-likelihood (NLL)

loss − log p(y|x). From (1), calculating the sum by direct enumer-

ation is intractable because the number of all possible alignments

of length T + U is B−1(y) = T +UU = (TT+!UU!)! ≥ 1 + UT U

where

n k

denotes the binomial coefﬁcient n choose k. Luckily, the

factorization in (2) allows for an efﬁcient forward-backward algo-

rithm with T × U complexity for both loss and gradient computa-

tion [1, 24].

3. MULTIPLICATIVE INTEGRATION OF ENCODER AND PREDICTION NETWORK OUTPUTS
Here, we discuss a simple change to the joint network equation (3) and its implications:

p(·|ht, gu) = softmax[Wouttanh(Wencht ⊙Wpredgu +b)] (4)

where ⊙ denotes elementwise multiplication (or Hadamard product). This modiﬁcation was inspired by the work of [25] where the authors use multiplicative integration (MI) in the context of a recurrent neural network for fusing the information from the hidden state vector and the input vector. The advantages of MI over additive integration for fusing different information sources have also been mentioned in [26] and are summarized below. Higher-order interactions MI allows for second-order interactions between the elements of ht and gu which facilitates modeling of more complex dependencies between the acoustic and LM embeddings. Importantly, the shift to second order is achieved with no increase in the number of parameters or computational complexity. In theory, feedforward neural nets with sufﬁcient capacity and training data are universal function approximators; therefore, a joint network with (ht, gu) inputs and multiple layers should be optimal. However the memory requirements are prohibitive because the input tensor size for such a network is N × T × U × (E + P ) and the memory for the output tensors is N × T × U × H × L where N is the batchsize, H is the number of hidden units and L is the number of layers. Therefore, for pure training efﬁciency reasons, we are constrained to use shallow (single layer) joint networks, in which case the functional form of the layer becomes important. Generalizes additive integration Indeed, when used in conjunction with biases, the additive terms appear in the expansion
(Wencht + benc) ⊙ (Wpredgu + bpred) = Wencht ⊙ Wpredgu
+benc ⊙ Wpredgu + bpred ⊙ Wenc ht + benc ⊙ bpred (5)
Scaling and gating effect By multiplying ˜ht := Wencht with g˜u := Wpredgu, one embedding has a scaling effect on the other embedding. In particular, unlike for additive interactions, in MI the gradient with respect to one component is gated by the other component and vice versa. Concretely, let us denote by L(x, y; θ) = − log p(y|x; θ) the NLL loss for one utterance for an RNN-T with parameters θ. The partial derivatives of the loss with respect to h˜t and g˜u are:

∂L = g˜u ⊙ ∂L

∂h˜t

∂(h˜t ⊙ g˜u)

∂L = h˜t ⊙ ∂L

(6)

∂g˜u

∂(h˜t ⊙ g˜u)

These arguments suggest that multiplicative integration is a good candidate for fusing the two different information streams coming from the encoder and the prediction network. We expect multiplicative RNN-Ts to be, if not better, then at least complementary to additive RNN-Ts which should be beneﬁcial for model combination.

4. TRAINING AND DECODING RECIPE
In this section, we discuss some of the techniques that are part of our RNN-T training and decoding recipe. While none of the techniques presented here are novel, the goal is to show their effectiveness for RNN-Ts in particular. The techniques can be roughly grouped into: (i) data augmentation/perturbation and model regularization, (ii) speaker adaptation and (iii) external language model fusion. Speed and tempo perturbation [19] changes the rate of speech in the interval [0.9, 1.1] with or without altering the pitch or timbre of the speaker. This technique generates additional replicas of the training data depending on the number of speed and tempo perturbation values. Sequence noise injection [20] adds, with a given probability, the downscaled spectra of randomly selected training utterances to the spectrum of the current training utterance. This technique does not increase the amount of training data per epoch. SpecAugment [21] masks the spectrum of a training utterance with a random number of blocks of random size in both time and frequency. DropConnect [22] zeros out entries randomly in the LSTM hiddento-hidden transition matrices. Switchout [23] randomly replaces labels in the output sequence with labels drawn uniformly from the output vocabulary. I-vector speaker adaptation [27] appends a speaker identity vector to the input features coming from a given speaker. When used in conjunction with speed and tempo perturbation, the perturbed audio recordings of a speaker are considered to be different speakers for the purpose of universal background model (UBM) training, total variability matrix training, and i-vector extraction. Alignment-length synchronous decoding [17] is a beam search technique with the property that all competing hypotheses within the beam have the same alignment length. It has been shown to be faster than time-synchronous search for the same accuracy. Density ratio (DR) LM fusion [13] is a shallow fusion technique that combines two language models: an external LM trained on a target domain corpus and a language model trained on the acoustic transcripts (source domain) only. The latter is used to subtract the effect of the intrinsic LM given by the prediction network (idea further developed in [14]). Decoding using DR fusion is done according to:
y∗ = argmax{log p(y|x) − µ log psrc(y) + λ log pext(y) + ρ|y|}
y∈Y ∗
(7)
where µ, λ, ρ are the weights corresponding to the source LM psrc, external LM pext, and label length reward |y|.
5. EXPERIMENTS AND RESULTS
We investigate the effectiveness of the proposed techniques on one public corpus (English conversational telephone speech 300 hours) and two internal datasets (Spanish and Italian conversational speech 780 hours and 900 hours, respectively).
5.1. Experiments on Switchboard 300 hours
The ﬁrst set of experiments was conducted on the Switchboard speech corpus which contains 300 hours of English telephone conversations between two strangers on a preassigned topic. The acoustic data segmentation and transcript preparation are done according

to the Kaldi s5c recipe [28]. We report results on the commonly used Hub5 2000 Switchboard and CallHome test sets as well as Hub5 2001 and RT’03, which are processed according to the LDC segmentation and scored using Kaldi scoring for measuring WER.
We extract 40-dimensional speaker independent log-Mel ﬁlterbank features every 10 ms which are mean and variance normalized per conversation side. The features are augmented with ∆ and ∆∆ coefﬁcients and every two consecutive frames are stacked and every second frame is skipped resulting in 240-dimensional vectors extracted every 20 ms. These features are augmented with 100dimensional i-vectors that are extracted using a 2048 40-dimensional diagonal covariance Gaussian mixture UBM trained on speaker independent PLP features transformed with LDA and a semi-tied covariance transform.
Speed and tempo perturbation is applied to each conversation side with values in {0.9, 1.1} for both speed and tempo separately resulting in 4 additional training data replicas which, together with the original data, amounts to 1500 hours of training data per epoch. For sequence noise injection, we add, with probability 0.8, to the spectrum of each training utterance the spectrum of one random utterance of similar length scaled by a factor of 0.4. For SpecAugment we used the settings published in [21]. Lastly, in label switchout, for a sequence of length U , we ﬁrst sample nˆ ∈ {0, . . . , U } with p(nˆ) ∝ e−nˆ/τ and then we replace, with probability nˆ/U , the true characters with random characters for each position in the sequence. We set the temperature to τ = 10 in our experiments.
The architecture of the trained models is as follows. The encoder contains 6 bidirectional LSTM layers with 640 cells per layer per direction and is initialized with a network trained with CTC based on [29] similar to [5, 6, 8, 15]. The prediction network is a single unidirectional LSTM layer with only 768 cells (this size has been found to be optimal after external LM fusion). The joint network projects the 1280-dimensional stacked encoder vectors from the last layer and the 768-dimensional prediction net embedding to 256 dimensions and combines the projected vectors using either + or ⊙. After the application of hyperbolic tangent, the output is projected to 46 logits followed by a softmax layer corresponding to 45 characters plus BLANK. All models have 57M parameters.

Optimizer

LR policy

Batch size SWB CH

Momentum SGD const+decay

256

9.6 17.5

Momentum SGD const+decay

64

9.4 17.5

AdamW

const+decay

256

9.4 17.8

AdamW

const+decay

64

8.5 16.7

AdamW

OneCycleLR

64

8.1 16.5

Table 1. Effect of optimizer, batch size and learning rate schedule on recognition performance for Switchboard 300 hours (Hub5’00 test set).
The models were trained in Pytorch on V100 GPUs for 20 epochs using SGD variants that differ in optimizer, learning rate schedule and batch size. In Table 1 we look at the effect on performance of changing the learning strategy. For momentum SGD, the learning rate was set to 0.01 and decays geometrically by a factor of 0.7 every epoch after epoch 10. For AdamW, the maximum learning rate is set to 5e-4 and the OneCycleLR policy [30] consists in a linear warmup phase from 5e-5 to 5e-4 over the ﬁrst 6 epochs followed by a linear annealing phase to 0 for the next 14 epochs. As can be seen, AdamW with OneCycleLR scheduling and a batch size of 64 appears to be optimal in this experiment.

Model
Baseline No Switchout No Seq. noise No i-vectors No CTC init. No Density ratio No DropConnect No SpecAugment No Speed/tempo

No ext LM SWB CH Avg 7.9 15.7 11.8 8.1 15.5 11.8 8.4 16.1 12.2 8.1 16.0 12.0 8.3 16.3 12.3 7.9 15.7 11.8 8.2 16.7 12.5 8.8 18.1 13.5 10.0 18.3 14.2

With ext LM SWB CH Avg 6.4 13.4 9.9 6.3 13.1 9.7 6.6 13.8 10.2 6.6 13.9 10.3 6.6 14.1 10.4 6.9 14.4 10.7 7.0 14.8 10.9 6.9 15.5 11.2 7.6 15.2 11.4

Table 2. Ablation study on Switchboard 300 hours (Hub5’00 test set).

Next, we perform an ablation study on the ﬁnal recipe to tease out the importance of the individual components. We show results in Table 2 for models with multiplicative integration with and without external language model fusion. The source LM is a one-layer LSTM with 768 cells (5M parameters) trained on the Switchboard 300 hours character-level transcripts (15.4M tokens) whereas the target LM is a two-layer LSTM with 2048 cells per layer (84M parameters) trained on the Switchboard+Fisher character-level transcripts (126M tokens).
Surprisingly, deactivating switchout from the ﬁnal recipe actually improves recognition performance after external LM fusion, which was not the case in prior experiments where this technique was marginally helpful. Also, another unexpected ﬁnding was the large gain due to density ratio LM fusion. We attribute this to optimizing the other elements of the training recipe around this technique (e.g. reducing the size of the prediction network).

ext Model Hub5’00

LM

swb ch

+ 8.0 15.6

no

⊙

8.1 15.5

Comb. 7.5 14.3

+ 6.4 13.4

yes ⊙ 6.3 13.1

Comb. 5.9 12.5

Hub5’01 swb s2p3 s2p4 8.7 11.4 16.3 8.5 11.7 15.9 7.9 10.8 15.2 7.0 9.2 13.4 7.1 9.4 13.6 6.6 8.6 12.6

RT’03 swb fsh 18.0 11.4 18.5 11.8 17.1 10.7 15.0 9.2 15.4 9.5 14.1 8.6

Table 3. Recognition results for additive, multiplicative and combined RNN-Ts on Switchboard 300 hours (Hub5’00, Hub5’01, RT’03 test sets).
In the next experiment, we compare RNN-Ts with additive versus multiplicative integration in the joint network. Based on the previous ﬁndings, we train these models without switchout. We also perform log-linear model combination with density ratio LM fusion according to:
y∗ = argmax {α log p(y|x; θ+) + β log p(y|x; θ⊙)
y∈H+ (x)∪H⊙ (x)
− µ log psrc(y) + λ log pext(y) + ρ|y|} (8)
where H+(x), H⊙(x) are the n-best hypotheses generated by the additive and multiplicative RNN-Ts. Concretely, we rescore the union of the top 32 hypotheses from each model with α = β = µ =

0.5, λ = 0.7 and ρ = 0.2. The results from Table 3 show that multiplicative RNN-Ts are comparable on Hub5’00 and Hub5’01 but slightly worse on RT’03 and that model combination signiﬁcantly improves the recognition performance across all test sets.
For comparison, we include in Table 4 the top performing single model systems from the literature on the Switchboard 300 hours corpus. The numbers should be compared with 6.3/13.1 (row 5 from Table 3). As can be seen, the proposed modeling techniques exhibit excellent performance on this task.

System

Type

ext. LM SWB CH

Park et al.’19 [21] Att. enc-dec LSTM 6.8 14.1

Irie et al.’19 [31]

Hybrid

Transf. 6.7 12.9

Hadian et al.’18 [32] LF-MMI

RNN 7.5 14.6

Tu¨ske et al.’20 [33] Att. enc-dec LSTM 6.4 12.5

Table 4. Single model performance for existing systems on Switchboard 300 hours (Hub5’00 test set).

5.2. Experiments on conversational Spanish 780 hours
We also investigated the effectiveness of the proposed techniques on an internal dataset which consists of 780 hours of Spanish call center data collected using a balanced distribution of Castilian and other Central and South American dialects from roughly 4000 speakers. The test set on which we report results comes from the call center domain and contains 4 hours of speech, 113 speakers and 31.6k words.
The model architecture, training and decoding recipes are identical to the ones from the previous subsection (except for a 40-character output layer) with the difference that we do not use i-vector speaker adaptation because of the small amount of data per speaker. The external language model is a two layer LSTM with 2048 cells per layer (84M parameters) trained on the character-level acoustic transcripts (36M characters) as well as additional text data from the customer care domain (173M characters). The source LM is a single-layer LSTM with 1024 cells (8.7M parameters).

Model/technique Initial experiment + DropConnect+seq. noise + Speed/tempo + CTC encoder pretraining + Multiplicative integration + SpecAugment + AdamW+OneCycleLR + Shallow LM fusion + Density ratio LM fusion

Training data 250 h 250 h 780 h 780 h 780 h 780 h 780 h 780 h 780 h

WER 34.8 27.6 25.0 23.6 22.7 21.9 20.8 20.3 20.0

Table 5. Cumulative effect of proposed techniques on conversational Spanish 780 hours (internal test set).
In Table 5, we look at the impact of the various techniques on word error rate for bidirectional RNN-Ts. The ﬁrst 4 models have additive integration and the ﬁrst 6 models are trained with momentum SGD and a constant+decay learning rate schedule for 20 epochs. Unlike in the previous experiments, here we observe a signiﬁcant gain from multiplicative integration (0.9% absolute) and a smaller gain from density ratio fusion (0.3% absolute).

5.3. Experiments on conversational Italian 900 hours
The last set of experiments was conducted on an internal Italian corpus of conversational speech that was collected using scripted dialogs from several domains such as banking, insurance, telco, retail to name a few. The data has a balanced demographic and dialectal distribution with a large fraction of the speakers speaking standard dialect (as spoken in the capital or news broadcasts). We report results on the Mozilla CommonVoice1 Italian test set which has 1.2 hours of audio, 764 utterances and 6.6K reference words.
Similar to the Spanish setup, we trained character-level RNNTs (53 outputs) without i-vector speaker adaptation using the recipe described in section 4 with speciﬁcs from subsection 5.1. In addition, we also train RNN-Ts with unidirectional LSTM encoders with 6 layers and 1024 cells per layer on stacked log-Mel features augmented with ∆, ∆∆ with 5 frames lookahead as proposed in [10]. The language model conﬁguration for density ratio fusion is as follows: the source LM has one LSTM layer with 1024 units (8.7M parameters) and is trained on the character-level acoustic transcripts (41.7M characters) whereas the external LM is a two layer 1024 cells/layer LSTM (21.3M parameters) trained on the acoustic transcripts and 10% of Italian Wikipedia data (306M characters).
In Table 6, we compare additive versus multiplicative models with bidirectional and unidirectional encoders with and without external LM fusion (regular and density ratio). Based on these results, three observations can be made. First, multiplicative integration signiﬁcantly outperforms the additive counterpart for unidirectional models and after shallow LM fusion (for both). Second, there is a severe degradation in recognition performance from using unidirectional encoders which can be mitigated with techniques from [12, 34]. Third, density ratio LM fusion signiﬁcantly outperforms regular shallow fusion which we attribute to the mismatched training and testing conditions.

No external LM Shallow fusion Density ratio fusion

Bidirectional +⊙ 17.8 17.6 15.2 13.9 13.6 12.7

Unidirectional

+

⊙

28.0 26.2

22.9 21.4

20.9 18.6

Table 6. Various comparisons on conversational Italian 900 hours (Mozilla CommonVoice test set).
6. CONCLUSION
The contribution of this paper is twofold. First, we have introduced a simple yet effective modiﬁcation of the joint network whereby we combine the encoder and prediction network embeddings using multiplicative integration instead of additive. MI outperforms additive integration on two out of three tasks and shows good model complementarity on the Switchboard 300 hours corpus. Second, we have shown that careful choice of optimizer and learning rate scheduling, data augmentation/perturbation and model regularization, speaker adaptation, external language model fusion, and model combination can lead to excellent recognition performance when applied to models with a very simple architecture and character outputs. Future work will look at alternative neural transducer architectures (e.g. [35]) and training criteria (e.g. [36]) and attempt to simplify the training recipe without sacriﬁcing recognition performance.
1https://commonvoice.mozilla.org

References
[1] A. Graves, “Sequence transduction with recurrent neural networks,” arXiv preprint arXiv:1211.3711, 2012.
[2] D. Bahdanau, J. Chorowski, D. Serdyuk, et al., “End-toend attention-based large vocabulary speech recognition,” in ICASSP. IEEE, 2016, pp. 4945–4949.
[3] W. Chan, N. Jaitly, Q. Le, and O. Vinyals, “Listen, attend and spell: A neural network for large vocabulary conversational speech recognition,” in ICASSP. IEEE, 2016, pp. 4960–4964.
[4] Y. Wu, M. Schuster, Z. Chen, et al., “Google’s neural machine translation system: Bridging the gap between human and machine translation,” arXiv preprint arXiv:1609.08144, 2016.
[5] A. Graves, A.-r. Mohamed, and G. Hinton, “Speech recognition with deep recurrent neural networks,” in ICASSP. IEEE, 2013, pp. 6645–6649.
[6] K. Rao, H. Sak, and R. Prabhavalkar, “Exploring architectures, data and units for streaming end-to-end speech recognition with RNN-transducer,” in Automatic Speech Recognition and Understanding Workshop (ASRU). IEEE, 2017, pp. 193– 199.
[7] E. Battenberg, J. Chen, R. Child, et al., “Exploring neural transducers for end-to-end speech recognition,” in Automatic Speech Recognition and Understanding Workshop (ASRU). IEEE, 2017, pp. 206–213.
[8] S. Wang, P. Zhou, W. Chen, et al., “Exploring RNNTransducer for Chinese speech recognition,” arXiv preprint arXiv:1811.05097, 2018.
[9] Y. He, T. N. Sainath, R. Prabhavalkar, et al., “Streaming endto-end speech recognition for mobile devices,” in ICASSP. IEEE, 2019, pp. 6381–6385.
[10] J. Li, R. Zhao, H. Hu, and Y. Gong, “Improving RNN transducer modeling for end-to-end speech recognition,” arXiv preprint arXiv:1909.12415, 2019.
[11] A. Tripathi, H. Lu, H. Sak, and H. Soltau, “Monotonic recurrent neural network transducer and decoding strategies,” in Automatic Speech Recognition and Understanding Workshop (ASRU). IEEE, 2019, pp. 944–948.
[12] M. Jain, K. Schubert, J. Mahadeokar, et al., “RNN-T for latency controlled ASR with improved beam search,” arXiv preprint arXiv:1911.01629, 2019.
[13] E. McDermott, H. Sak, and E. Variani, “A density ratio approach to language model fusion in end-to-end automatic speech recognition,” in Automatic Speech Recognition and Understanding Workshop (ASRU). IEEE, 2019, pp. 434–441.
[14] E. Variani, D. Rybach, C. Allauzen, and M. Riley, “Hybrid autoregressive transducer (HAT),” in ICASSP. IEEE, 2020, pp. 6139–6143.
[15] A. Zeyer, A. Merboldt, R. Schlu¨ter, and H. Ney, “A new training pipeline for an improved neural transducer,” arXiv preprint arXiv:2005.09319, 2020.
[16] C.-C. Chiu, A. Narayanan, W. Han, et al., “RNN-T models fail to generalize to out-of-domain audio: Causes and solutions,” arXiv preprint arXiv:2005.03271, 2020.
[17] G. Saon, Z. Tu¨ske, and K. Audhkhasi, “Alignment-length synchronous decoding for RNN transducer,” in ICASSP. IEEE, 2020, pp. 7804–7808.
[18] C.-C. Chiu, T. N. Sainath, Y. Wu, et al., “State-of-the-art speech recognition with sequence-to-sequence models,” in ICASSP. IEEE, 2018, pp. 4774–4778.

[19] T. Ko, V. Peddinti, D. Povey, and S. Khudanpur, “Audio augmentation for speech recognition,” in Sixteenth Annual Conference of the International Speech Communication Association, 2015.
[20] G. Saon, Z. Tu¨ske, K. Audhkhasi, and B. Kingsbury, “Sequence noise injected training for end-to-end speech recognition,” in ICASSP. IEEE, 2019.
[21] D. S. Park, W. Chan, Y. Zhang, et al., “SpecAugment: A simple data augmentation method for automatic speech recognition,” Proc. Interspeech 2019, pp. 2613–2617, 2019.
[22] L. Wan, M. Zeiler, S. Zhang, et al., “Regularization of neural networks using DropConnect,” in International conference on machine learning, 2013, pp. 1058–1066.
[23] X. Wang, H. Pham, Z. Dai, and G. Neubig, “Switchout: an efﬁcient data augmentation algorithm for neural machine translation,” arXiv preprint arXiv:1808.07512, 2018.
[24] T. Bagby, K. Rao, and K. C. Sim, “Efﬁcient implementation of recurrent neural network transducer in TensorFlow,” in Spoken Language Technology Workshop (SLT). IEEE, 2018, pp. 506– 512.
[25] Y. Wu, S. Zhang, Y. Zhang, et al., “On multiplicative integration with recurrent neural networks,” in Advances in neural information processing systems, 2016, pp. 2856–2864.
[26] S. M. Jayakumar, W. M. Czarnecki, J. Menick, et al., “Multiplicative interactions and where to ﬁnd them,” in International Conference on Learning Representations, 2019.
[27] G. Saon, H. Soltau, D. Nahamoo, and M. Picheny, “Speaker adaptation of neural network acoustic models using i-vectors,” in Proc. ASRU, 2013.
[28] D. Povey, A. Ghoshal, G. Boulianne, et al., “The Kaldi speech recognition toolkit,” in IEEE workshop on automatic speech recognition and understanding. IEEE Signal Processing Society, 2011.
[29] K. Audhkhasi, G. Saon, Z. Tu¨ske, et al., “Forget a bit to learn better: Soft forgetting for CTC-based automatic speech recognition.,” in INTERSPEECH, 2019, pp. 2618–2622.
[30] L. N. Smith and N. Topin, “Super-convergence: Very fast training of neural networks using large learning rates,” in Artiﬁcial Intelligence and Machine Learning for Multi-Domain Operations Applications. International Society for Optics and Photonics, 2019, vol. 11006, p. 1100612.
[31] K. Irie, A. Zeyer, R. Schlu¨ter, and H. Ney, “Training language models for long-span cross-sentence evaluation,” in Automatic Speech Recognition and Understanding Workshop (ASRU). IEEE, 2019, pp. 419–426.
[32] H. Hadian, H. Sameti, D. Povey, and S. Khudanpur, “End-toend speech recognition using lattice-free MMI,” in Proceedings of INTER-SPEECH, 2018.
[33] Z. Tu¨ske, G. Saon, K. Audhkhasi, and B. Kingsbury, “Single headed attention based sequence-to-sequence model for stateof-the-art results on Switchboard-300,” in INTERSPEECH, 2020.
[34] G. Kurata and G. Saon, “Knowledge distillation from ofﬂine to streaming RNN transducer for end-to-end speech recognition,” in INTERSPEECH, 2020.
[35] C.-F. Yeh, J. Mahadeokar, K. Kalgaonkar, et al., “Transformertransducer: End-to-end speech recognition with self-attention,” arXiv preprint arXiv:1910.12977, 2019.
[36] C. Weng, C. Yu, J. Cui, et al., “Minimum Bayes risk training of RNN-Transducer for end-to-end speech recognition,” arXiv preprint arXiv:1911.12487, 2019.

