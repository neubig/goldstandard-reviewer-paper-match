arXiv:1512.02673v3 [cs.DC] 29 Jan 2018

1
Speeding Up Distributed Machine Learning
Using Codes
Kangwook Lee, Maximilian Lam, Ramtin Pedarsani, Dimitris Papailiopoulos, and Kannan Ramchandran, Fellow, IEEE
Abstract
Codes are widely used in many engineering applications to offer robustness against noise. In large-scale systems there are several types of noise that can affect the performance of distributed machine learning algorithms – straggler nodes, system failures, or communication bottlenecks – but there has been little interaction cutting across codes, machine learning, and distributed systems. In this work, we provide theoretical insights on how coded solutions can achieve signiﬁcant gains compared to uncoded ones. We focus on two of the most basic building blocks of distributed learning algorithms: matrix multiplication and data shufﬂing. For matrix multiplication, we use codes to alleviate the effect of stragglers, and show that if the number of homogeneous workers is n, and the runtime of each subtask has an exponential tail, coded computation can speed up distributed matrix multiplication by a factor of log n. For data shufﬂing, we use codes to reduce communication bottlenecks, exploiting the excess in storage. We show that when a constant fraction α of the data matrix can be cached at each worker, and n is the number of workers, coded shufﬂing reduces the communication cost by a factor of (α + n1 )γ(n) compared to uncoded shufﬂing, where γ(n) is the ratio of the cost of unicasting n messages to n users to multicasting a common message (of the same size) to n users. For instance, γ(n) n if multicasting a message to n users is as cheap as unicasting a message to one user. We also provide experiment results, corroborating our theoretical gains of the coded algorithms.
I. INTRODUCTION
In recent years, the computational paradigm for large-scale machine learning and data analytics has shifted towards massively large distributed systems, comprising individually small and unreliable computational nodes (low-end, commodity hardware). Speciﬁcally, modern distributed systems like Apache Spark [4] and computational primitives like MapReduce [5] have gained signiﬁcant traction, as they enable the execution of production-scale tasks on data sizes of the order of petabytes. However, it is observed that the performance of a modern distributed system is signiﬁcantly affected by anomalous system behavior and bottlenecks [6], i.e., a form of “system noise”. Given the individually unpredictable nature of the nodes in these systems, we are faced with the challenge of securing fast and high-quality algorithmic results in the face of uncertainty.
In this work, we tackle this challenge using coding theoretic techniques. The role of codes in providing resiliency against noise has been studied for decades in several other engineering contexts, and is part of our everyday infrastructure (smartphones, laptops, WiFi and cellular systems, etc.). The goal of our work is to apply coding techniques to blueprint robust distributed systems, especially for distributed machine learning algorithms. The workﬂow of distributed machine learning algorithms in a large-scale system can be decomposed into three functional phases: a storage, a communication, and a computation phase, as shown in Fig. 1. In order to develop and deploy sophisticated solutions and tackle large-scale problems in machine learning, science, engineering, and commerce, it is important to understand and optimize novel and complex trade-offs across the multiple dimensions of computation, communication, storage, and the accuracy of results. Recently, codes have begun to transform the storage layer of distributed systems in modern data centers under the umbrella of regenerating and locally repairable codes for distributed storage [7]–[22] which are also having a major impact on industry [23]–[26].
In this paper, we explore the use of coding theory to remove bottlenecks caused during the other phases: the communication and computation phases of distributed algorithms. More speciﬁcally, we identify two core blocks relevant to the communication and computation phases that we believe are key primitives in a plethora of distributed data processing and machine learning algorithms: matrix multiplication and data shufﬂing.
For matrix multiplication, we use codes to leverage the plethora of nodes and alleviate the effect of stragglers, i.e., nodes that are signiﬁcantly slower than average. We show analytically that if there are n workers having identically distributed computing time statistics that are exponentially distributed, the optimal coded matrix multiplication is Θ(log n)1 times faster than the uncoded matrix multiplication on average.
Data shufﬂing is a core element of many machine learning applications, and is well-known to improve the statistical performance of learning algorithms. We show that codes can be used in a novel way to trade off excess in available storage for
Kangwook Lee is with the School of Electrical Engineering, KAIST. Maximilian Lam and Kannan Ramchandran are with the Department of Electrical Engineering and Computer Sciences, University of California, Berkeley. Ramtin Pedarsani is with the Department of Electrical and Computer Engineering, University of California, Santa Barbara. Dimitris Papailiopoulos is with the Department of Electrical and Computer Engineering, University of WisconsinMadison. Emails: kw1jjang@kaist.ac.kr, agnusmaximus@berkeley.edu, ramtin@ece.ucsb.edu, dimitris@papail.io, kannanr@eecs.berkeley.edu.
This work is published in IEEE Transactions on Information Theory [1], and was presented in part at the 2015 Neural Information Processing Systems (NIPS) Workshop on Machine Learning Systems [2], and the 2016 IEEE International Symposium on Information Theory (ISIT) [3].
1For any two sequences f (n) and g(n): f (n) = Ω(g(n)) if there exists a positive constant c such that f (n) ≥ cg(n); f (n) = o(g(n)) if limn→∞ fg((nn)) = 0.

2

input data

storage phase

dist. store

1 2
3

communication phase

computation phase

1

2

Communication graph

3

aggregate function


Fig. 1: Conceptual diagram of the phases of distributed computation. The algorithmic workﬂow of distributed (potentially iterative) tasks, can be seen
as receiving input data, storing them in distributed nodes, communicating data around the distributed network, and then computing locally a function at each distributed node. The main bottlenecks in this execution (communication, stragglers, system failures) can all be abstracted away by incorporating a notion of delays between these phases, denoted by ∆ boxes.

reduced communication cost for data shufﬂing done in parallel machine learning algorithms. We show that when a constant

fraction of the data matrix can be cached at each worker, and n is the number of workers, coded shufﬂing reduces the

communication cost by a factor Θ(γ(n)) compared to uncoded shufﬂing, where γ(n) is the ratio of the cost of unicasting n

messages to n users to multicasting a common message (of the same size) to n users. For instance, γ(n) n if multicasting

a message to n users is as cheap as unicasting a message to one user.

We would like to remark that a major innovation of our coding solutions is that they are woven into the fabric of the

algorithmic design, and coding/decoding is performed over the representation ﬁeld of the input data (e.g., ﬂoats or doubles).

In

sharp

contrast

to

most

coding

applications,

we

do

not
M

needCotdoed“%Mrea-tfraixc%tMorultcipolidcaet”ionand

mModifyAXthe

distributed

system

to

accommodate for our more impactful as it

issollouctiaotnesdA; “1ithiisghaellr

done seamlessly in up” in the system

the algorithmic layer hierarchy

design layer, compared to

an abstraction that we believe is much traditional applications of coding that

X

need to interact with the stored and transmitted “bits” (e.g., as is the case for coding solutions for the(Ap1h+Ay2s)iXcal or storage

layer).
A. Overview of The Main ResAul2ts

W1

W2

W3

W1

W2

W3

We now provide a brief overview of the maiAn1 results oAf2 this paAp1+eAr.2 The followinAg1 toy exaAm2ple illuAs1t+rAa2tes the main idea of
A

W1

A1

A1X

W1

A1

M

X

W2

A2

W3 A1+A2

M

W2

A2

AX (A1+A2)X W3 A1+A2

Fig. 2: Illustration of Coded Matrix Multiplication. Data matrix A is partitioned into 2 submatrices: A1 and A2. Node W1 stores A1, node W2 stores
A2, and node W3 stores A1 + A2. Upon receiving X, each node multiplies X with the stored matrix, and sends the product to the master node. Observe that the master node can always recover AX upon receiving any 2 products, without needing to wait for the slowest response. For instance, consider a case
where the master node has received A1X and (A1 + A2)X. By subtracting A1X from (A1 + A2)X, it can recover A2X and hence AX.

Coded Computation. Consider a system with three worker nodes and one master node, as depicted in Fig. 2. The goal is to compute a matrix multiplication AX for data matrix A ∈ Rq×r and input matrix X ∈ Rr×s. The data matrix A is divided into two submatrices A1 ∈ Rq/2×r and A2 ∈ Rq/2×r and stored in node 1 and node 2, as shown in Fig. 2. The sum of the two submatrices is stored in node 3. After the master node transmits X to the worker nodes, each node computes the matrix multiplication of the stored matrix and the received matrix X, and sends the computation result back to the master node. The master node can compute AX as soon as it receives any two computation results.
Coded Computation designs parallel tasks for a linear operation using erasure codes such that its runtime is not affected by up to a certain number of stragglers. Matrix multiplication is one of the most basic linear operations and is the workhorse of a host of machine learning and data analytics algorithms, e.g., gradient descent based algorithm for regression problems, poweriteration like algorithms for spectral analysis and graph ranking applications, etc. Hence, we focus on the example of matrix multiplication in this paper. With coded computation, we will show that the runtime of the algorithm can be signiﬁcantly reduced compared to that of other uncoded algorithms. The main result on Coded Computation is stated in the following (informal) theorem.
Theorem 1 (Coded computation). If the number of workers is n, and the runtime of each subtask has an exponential tail, the optimal coded matrix multiplication is Θ(log n) times faster than the uncoded matrix multiplication.
For the formal version of the theorem and its proof, see Sec. III-D.

Coded Shuffling M
3

We now overview the main results on coded shufﬂing. Consider a master-wWo1rker setuWp2where a master node holds the entire data set. The generic machine learning task that we wish to optimize is the following: 1) the data set is randomly permuted

and partitioned in batches at the master; 2) the master sends the batches tAo1 theA3 workA2ers;A43) each worker uses its batch and locally trains a model; 4) the local models are averaged at the master and the process is repeated. To reduce communication

overheads between master and workers, Coded Shufﬂing exploits i) the locally cached data points of previous passes and ii)

the “transmission strategy” of the master node.

A1

A W1

A1

A2

W1

A1

A3

2

M

A2+A3

M

A3

W2

A3

A4

W2

A2

A4

Fig. 3: IllustratAion4 of Coded Shufﬂing. Data matrix A is partitioned into 4 submatrices: A1 to A4. Before shufﬂing, worker W1 has A1 and A2 and
worker W2 has A3 and A4. The master node can send A2 + A3 in order to shufﬂe the data stored at the two workers.
A

We illustrate the basics of Coded Shufﬂing with a toy example. Consider a system with two worker nodes and one master
node. Assume that the data set consists of 4 batches A1, . . . , A4, which are stored across two workers as shown in Fig. 3. The sole objective of the master is to transmit A3 to the ﬁrst worker and A4 to the second. For this purpose, the master node can simply multicast a coded message A2 + A3 to the worker nodes since the workers can decode the desired batches using the stored batches. Compared to the na¨ıve (or uncoded) shufﬂing scheme in which the master node transmits A2 and A3 separately, this new shufﬂing scheme can save 50% of the communication cost, speeding up the overall machine learning
algorithm. The Coded Shufﬂing algorithm is a generalization of the above toy example, which we explain in detail in Sec. IV.

Note that the above example assumes that multicasting a message to all workers costs exactly the same as unicasting a message to one of the workers. In general, we capture the advantage of using multicasting over unicasting by deﬁning γ(n)

as follows:

γ(n)

def cost of unicasting n separate msgs to n workers

=

.

(1)

cost of multicasting a common msg to n workers

Clearly, 1 ≤ γ(n) ≤ n: if γ(n) = n, the cost of multicasting is equal to that of unicasting a single message (as in the above example); if γ(n) = 1, there is essentially no advantage of using multicast over unicast.
We now state the main result on Coded Shufﬂing in the following (informal) theorem.

Theorem 2 (Coded shufﬂing). Let α be the fraction of the data matrix that can be cached at each worker, and n be the

number of workers. Assume that the advantage of multicasting over unicasting is γ(n). Then, coded shufﬂing reduces the

communication cost by a factor of

α

+

1 n

γ(n) compared to uncoded shufﬂing.

For the formal version of the theorem and its proofs, see Sec. IV-D. The remainder of this paper is organized as follows. In Sec. II, we provide an extensive review of the related works in the literature. Sec. III introduces the coded matrix multiplication, and Sec. IV introduces the coded shufﬂing algorithm. Finally, Sec. V presents conclusions and discusses open problems.

II. RELATED WORK

A. Coded Computation and Straggler Mitigation

The straggler problem has been widely observed in distributed computing clusters. The authors of [6] show that running a computational task at a computing node often involves unpredictable latency due to several factors such as network latency, shared resources, maintenance activities, and power limits. Further, they argue that stragglers cannot be completely removed from a distributed computing cluster. The authors of [27] characterize the impact and causes of stragglers that arise due to resource contention, disk failures, varying network conditions, and imbalanced workload.
One approach to mitigate the adverse effect of stragglers is based on efﬁcient straggler detection algorithms. For instance, the default scheduler of Hadoop constantly detects stragglers while running computational tasks. Whenever it detects a straggler, it relaunches the task that was running on the detected straggler at some other available node. In [28], Zaharia et al. propose a modiﬁcation to the existing straggler detection algorithm and show that the proposed solution can effectively reduce the completion time of MapReduce tasks. In [27], Ananthanarayanan et al. propose a system that efﬁciently detects stragglers using real-time progress and cancels those stragglers, and show that the proposed system can further reduce the runtime of MapReduce tasks.
Another line of work is based on breaking the synchronization barriers in distributed algorithms [29], [30]. An asynchronous parallel execution can continuously make progress without having to wait for all the responses from the workers, and hence the overall runtime is less affected by stragglers. However, these asynchronous approaches break the serial consistency of the

4
algorithm to be parallelized, and do not guarantee “correctness” of the end result, i.e., the output of the asynchronous algorithm can differ from that of a serial execution with an identical number of iterations.
Recently, replication-based approaches have been explored to tackle the straggler problem: by replicating tasks and scheduling the replicas, the runtime of distributed algorithms can be signiﬁcantly improved [31]–[37]. By collecting outputs of the fastresponding nodes (and potentially canceling all the other slow-responding replicas), such replication-based scheduling algorithms can reduce latency. In [36], the authors show that even without replica cancellation, one can still reduce the average task latency by properly scheduling redundant requests. We view these policies as special instances of coded computation: such task replication schemes can be seen as repetition-coded computation. In Sec. III, we describe this connection in detail, and indicate that coded computation can signiﬁcantly outperform replication (as is usually the case for coding vs. replication in other engineering applications).
Another line of work that is closely related to coded computation is about the latency analysis of coded distributed storage systems. In [38], [39], the authors show that the ﬂexibility of erasure-coded distributed storage systems allows for faster data retrieval performance than replication-based distributed storage systems. Joshi et al. [40] show that scheduling redundant requests to an increased number of storage nodes can improve the latency performance, and characterize the resulting storagelatency tradeoff. Sun et al. [41] study the problem of adaptive redundant requests scheduling, and characterize the optimal strategies for various scenarios. In [42], [43], Kadhe and Soljanin analyze the latency performance of availability codes, a class of storage codes designed for enhanced availability. In [37], the authors study the cost associated with scheduling of redundant requests, and propose a general scheduling policy that achieves a delicate balance between the latency performance and the cost.
We now review some recent works on coded computation, which have been published after our conference publications [2], [3]. In [44], an anytime coding scheme for approximate matrix multiplication is proposed, and it is shown that the proposed scheme can improve the quality of approximation compared with the other existing coded schemes for exact computation. In [45], the authors propose a coded computation scheme called ‘Short-Dot’. Short-Dot induces additional sparsity to the encoded matrices at the cost of reduced decoding ﬂexibility, and hence potentially speeds up the computation. The authors of [46] consider the problem of computing gradients in a distributed system, and propose a novel coded computation scheme tailored for computing a sum of functions. In many machine learning problems, the objective function is a sum of per-data loss functions, and hence the gradient of the objective function is the sum of gradients of per-data loss functions. Based on this observation, they propose Gradient Coding, which can reliably compute the exact gradient of any function in the presence of stragglers. While Gradient coding can be applied to computing gradients of any functions, it usually incurs signiﬁcant storage and computation overheads. In [47], the authors consider a secure coded computation problem where the input data matrices need to be secured from the workers. They propose a secure computation scheme based on Staircase codes, which can speed up the distributed computation while securing the input data from the workers. In [48], the authors consider the problem of large matrix-matrix multiplication, and propose a new coded computation scheme based on product codes. In [49], the authors consider the coded computation problem on heterogenous computing clusters while our work assumes a homogeneous computing cluster. The authors show that by delicately distributing jobs across heterogenous workers, one can improve the performance of coded computation compared with the symmetric job allocation scheme, which is designed for homogeneous workers in our work. While most of the works focus on the application of coded computation to linear operations, a recent work shows that coding can be used also in distributed computing frameworks involving nonlinear operations [50]. The authors of [50] show that by leveraging the multi-core architecture in the worker computing units and “coding across” the multi-core computed outputs, signiﬁcant (and in some settings unbounded) gains in speed-up in computational time can be achieved between the coded and uncoded schemes.
B. Data Shufﬂing and Communication Overheads
Distributed learning algorithms on large-scale networked systems have been extensively studied in the literature [51]–[61]. Many of the distributed algorithms that are implemented in practice share a similar algorithmic “anatomy”: the data set is split among several cores or nodes, each node trains a model locally, then the local models are averaged, and the process is repeated. While training a model with parallel or distributed learning algorithms, it is common to randomly re-shufﬂe the data a number of times [30], [62]–[66]. This essentially means that after each shufﬂing the learning algorithm will go over the data in a different order than before. Although the effects of random shufﬂing are far from understood theoretically, the large statistical gains have turned it into a common practice. Intuitively, data shufﬂing before a new pass over the data, implies that nodes get a nearly “fresh” sample from the data set, which experimentally leads to better statistical performance. Moreover, bad orderings of the data—known to lead to slow convergence in the worst case [62], [65], [66]—are “averaged out”. However, the statistical beneﬁts of data shufﬂing do not come for free: each time a new shufﬂe is performed, the entire dataset is communicated over the network of nodes. This inevitably leads to performance bottlenecks due to heavy communication.
In this work, we propose to use coding opportunities to signiﬁcantly reduce the communication cost of some distributed learning algorithms that require data shufﬂing. Our coded shufﬂing algorithm is built upon the coded caching scheme by Maddah-Ali and Niesen [67]. Coded caching is a technique to reduce the communication rate in content delivery networks. Mainly motivated by video sharing applications, coded caching exploits the multicasting opportunities between users that

5
request different video ﬁles to signiﬁcantly reduce the communication burden of the server node that has access to the ﬁles. Coded caching has been studied in many scenarios such as decentralized coded caching [68], online coded caching [69], hierarchical coded caching for wireless communication [70], and device-to-device coded caching [71]. Recently, the authors in [72] proposed coded MapReduce that reduces the communication cost in the process of transferring the results of mappers to reducers.
Our proposed approach is signiﬁcantly different from all related studies on coded caching in two ways: (i) we shufﬂe the data points among the computing nodes to increase the statistical efﬁciency of distributed computation and machine learning algorithms; and (ii) we code the data over their actual representation (i.e., over the doubles or ﬂoats) unlike the traditional coding schemes over bits. In Sec. IV, we describe how coded shufﬂing can remarkably speed up the communication phase of large-scale parallel machine learning algorithms, and provide extensive numerical experiments to validate our results.
The coded shufﬂing problem that we study is related to the index coding problem [73], [74]. Indeed, given a ﬁxed “side information” reﬂecting the memory content of the nodes, the data delivery strategy for a particular permutation of the data rows induces an index coding problem. However, our coded shufﬂing framework is different from index coding in at least two signiﬁcant ways. First, the coded shufﬂing framework involves multiple iterations of data being stored across all the nodes. Secondly, when the caches of the nodes are updated in coded shufﬂing, the system is unaware of the upcoming permutations. Thus, the cache update rules need to be designed to target any possible unknown permutation of data in succeeding iterations of the algorithm.
We now review some recent works on coded shufﬂing, which have been published after our ﬁrst presentation [2], [3]. In [75], the authors study the information-theoretic limits of the coded shufﬂing problem. More speciﬁcally, the authors completely characterize the fundamental limits for the case of 2 workers and the case of 3 workers. In [76], the authors consider the worsecase formulation of the coded shufﬂing problem, and propose a two-stage shufﬂing algorithm. The authors of [77] propose a new coded shufﬂing scheme based on pliable index coding. While most of the existing works focus on either coded computation or coded shufﬂing, one notable exception is [78]. In this work, the authors generalize the original coded MapReduce framework by introducing stragglers to the computation phases. Observing that highly ﬂexible codes are not favorable to coded shufﬂing while replication codes allow for efﬁcient shufﬂing, the authors propose an efﬁcient way of coding to mitigate straggler effects as well as reduce the shufﬂing overheads.
III. CODED COMPUTATION
In this section, we propose a novel paradigm to mitigate the straggler problem. The core idea is simple: we introduce redundancy into subtasks of a distributed algorithm such that the original task’s result can be decoded from a subset of the subtask results, treating uncompleted subtasks as erasures. For this speciﬁc purpose, we use erasure codes to design coded subtasks.
An erasure code is a method of introducing redundancy to information for robustness to noise [79]. It encodes a message of k symbols into a longer message of n coded symbols such that the original k message symbols can be recovered by decoding a subset of coded symbols [79], [80]. We now show how erasure codes can be applied to distributed computation to mitigate the straggler problem.
A. Coded Computation
A coded distributed algorithm is speciﬁed by local functions, local data blocks, decodable sets of indices, and a decoding function: The local functions and data blocks specify the way the original computational task and the input data are distributed across n workers; and the decodable sets of indices and the decoding function are such that the desired computation result can be correctly recovered using the decoding function as long as the local computation results from any of the decodable sets are collected.
The formal deﬁnition of coded distributed algorithms is as follows.
Deﬁnition 1 (Coded computation). Consider a computational task fA(·). A coded distributed algorithm for computing fA(·) is speciﬁed by
• local functions fAi i (·) ni=1 and local data blocks Ai ni=1; • (minimal) decodable sets of indices I ⊂ P([n]) and a decoding function dec(·, ·), where [n] =def {1, 2, . . . , n}, and P(·) is the power set of a set. The decodable sets of indices I is minimal: no element of I is a subset of other elements. The decoding function takes a sequence of indices and a sequence of subtask results, and it must correctly output fA(x) if any decodable set of indices and its corresponding results are given.
A coded distributed algorithm can be run in a distributed computing cluster as follows. Assume that the ith (encoded) data block Ai is stored at the ith worker for all i. Upon receiving the input argument x, the master node multicasts x to all the workers, and then waits until it receives the responses from any of the decodable sets. Each worker node starts computing its local function when it receives its local input argument, and sends the task result to the master node. Once the master node receives the results from some decodable set, it decodes the received task results and obtains fA(x).

6

The algorithm described in Sec. I-A is an example of coded distributed algorithms: it is a coded distributed algorithm for matrix multiplication that uses an (n, n − 1) MDS code. One can generalize the described algorithm using an (n, k) MDS code as follows. For any 1 ≤ k ≤ n, the data matrix A is ﬁrst divided into k equal-sized submatrices2. Then, by applying an (n, k) MDS code to each element of the submatrices, n encoded submatrices are obtained. We denote these n encoded submatrices by A1, A2, . . . , An. Note that the Ai = Ai for 1 ≤ i ≤ k if a systematic MDS code is used for the encoding procedure. Upon receiving any k task results, the master node can use the decoding algorithm to decode k task results. Then,
one can ﬁnd AX simply by concatenating them.

B. Runtime of Uncoded/Coded Distributed Algorithms

In this section, we analyze the runtime of uncoded and coded distributed algorithms. We ﬁrst consider the overall runtime
of an uncoded distributed algorithm, Touvnecroadlled. Assume that the runtime of each task is identically distributed and independent of others. We denote the runtime of the ith worker under a computation scheme, say s, by Tis. Note that the distributions of Ti’s can differ across different computation schemes.

Touvnecroadlled

=

T(unn)coded

=def

max{T1uncoded

,

.

.

.

,

T

uncoded n

},

(2)

where T(i) is the ith smallest one in {Ti}ni=1. From (2), it is clear that a single straggler can slow down the overall algorithm. A coded distributed algorithm is terminated whenever the master node receives results from any decodable set of workers.
Thus, the overall runtime of a coded algorithm is not determined by the slowest worker, but by the ﬁrst time to collect results from some decodable set in I, i.e.,

Tocvoedreadll = T(cIod)ed =def min max Tjcoded

(3)

i∈I j∈i

We remark that the runtime of uncoded distributed algorithms (2) is a special case of (3) with I = {[n]}. In the following examples, we consider the runtime of the repetition-coded algorithms and the MDS-coded algorithms.

Example

1

(Repetition

codes).

Consider

an

nk -repetition-code

where

each

local

task

is

replicated

n k

times.

We

assume

that

each

group

of

n k

consecutive

workers

work

on

the

replicas

of

one

local

task.

Thus,

the

decodable

sets

of

indices

I

are

all

the

minimal

sets

that

have

k

distinct

task

results,

i.e.,

I

=

{1,

2,

.

.

.

,

n k

}

×

{

n k

+ 1,

n k

+ 2,

.

.

.

,

n k

+ k} × .

.

. × {n −

n k

+ 1,

n−

n k

+ 2,

.

.

.

,

n},

where A × B denotes the Cartesian product of matrix A and B. Thus,

ToRveepraeltlition-coded = max min {T(Ri−ep1et)itnio+n-jcoded}.

(4)

i∈[k]

j∈[

n k

]

k

Example 2 (MDS codes). If one uses an (n, k) MDS code, the decodable sets of indices are the sets of any k indices, i.e., I = {i|i ⊂ [n], |i| = k}. Thus,

ToMveDraSl-lcoded = T(Mk)DS-coded

(5)

That is, the algorithm’s runtime will be determined by the kth response, not by the nth response.

C. Probabilistic Model of Runtime

In this section, we analyze the runtime of uncoded/coded distributed algorithms assuming that task runtimes, including times to communicate inputs and outputs, are randomly distributed according to a certain distribution. For analytical purposes, we make a few assumptions as follows. We ﬁrst assume the existence of the mother runtime distribution F (t): we assume that running an algorithm using a single machine takes a random amount of time T0, that is a positive-valued, continuous random variable parallelized according to F , i.e. Pr(T0 ≤ t) = F (t). We also assume that T0 has a probability density function f (t). Then, when the algorithm is distributed into a certain number of subtasks, say , the runtime distribution of each of the subtasks is assumed to be a scaled distribution of the mother distribution, i.e., Pr(Ti ≤ t) = F ( t) for 1 ≤ i ≤ . Note that we are implicitly assuming a symmetric job allocation scheme, which is the optimal job allocation scheme if the underlying workers have the identical computing capabilities, i.e., homogeneous computing nodes are assumed. Finally, the computing times of the k tasks are assumed to be independent of one another.
Remark 1 (Homogeneous Clusters and Heterogenous Clusters). In this work, we assume homogeneous clusters: that is, all the workers have independent and identically distributed computing time statistics. While our symmetric job allocation is optimal for homogeneous cases, it can be strictly suboptimal for heterogenous cases. While our work focuses on homogeneous clusters, we refer the interested reader to a recent work [49] for a generalization of our problem setting to that of heterogeneous clusters, for which symmetric allocation strategies are no longer optimal.
We ﬁrst consider an uncoded distributed algorithm with n (uncoded) subtasks. Due to the assumptions mentioned above, the runtime of each subtask is F (nt). Thus, the runtime distribution of an uncoded distributed algorithm, denoted by Fouvnecroadlled(t), is simply [F (nt)]n.

2If the number of rows of A is not a multiple of k, one can append zero rows to A to make the number of rows a multiple of k.

7

100

100

10-1

10-1

P(T > t) P(T > t)

10-2

10-2

Uncoded Rep-coded MDS-coded

10-3

0

0.1

0.2

0.3

0.4

0.5

Compute latency (sec)

Uncoded Rep-coded MDS-coded

10-3

0

0.1

0.2

0.3

0.4

0.5

Compute latency (sec)

(a) Shifted-exponential distribution

(b) Empirical distribution

Fig. 4: Runtime distributions of uncoded/coded distributed algorithms. We plot the runtime distributions of uncoded/coded distributed algorithms. For
the uncoded algorithms, we use n = 10, and for the coded algorithms, we use n = 10 and k = 5. In (a), we plot the runtime distribution when the runtime of tasks are distributed according to the shifted-exponential distribution. Indeed, the curves in (a) are analytically obtainable: See Sec. III-D for more details. In (b), we use the empirical task runtime distribution measured on an Amazon EC2 cluster.

When repetition codes or MDS codes are used, an algorithm is ﬁrst divided into k (< n) systematic subtasks, and then n − k

coded tasks are designed to provide an appropriate level of redundancy. Thus, the runtime of each task is distributed according to F (kt). Using (4) and (5), one can easily ﬁnd the runtime distribution of an nk -repetition-coded distributed algorithm, FoRveepraeltlition, and the runtime distribution of an (n, k)-MDS-coded distributed algorithm, FoMveDraSl-lcoded. For an nk -repetition-coded distributed algorithm, one can ﬁrst ﬁnd the distribution of

min {T(Ri−ep1et)itnio+n-jcoded},

j

∈

[

n k

]

k

and then ﬁnd the distribution of the maximum of k such terms:

F Repetition(t) =

n
1 − [1 − F (kt)] k

k
.

(6)

overall

The runtime distribution of an (n, k)-MDS-coded distributed algorithm is simply the kth order statistic:

FoMveDraSl-lcoded(t)

= t nkf (kτ ) n − 1 F (kτ )k−1 [1 − F (kτ )]n−k dτ .

(7)

τ =0

k−1

Remark 2. For the same values of n and k, the runtime distribution of a repetition-coded distributed algorithm strictly dominates that of an MDS-coded distributed algorithm. This can be shown by observing that the decodable sets of the MDS-coded algorithm contain those of the repetition-coded algorithm.

In Fig. 4, we compare the runtime distributions of uncoded and coded distributed algorithms. We compare the runtime
distributions of uncoded algorithm, repetition-coded algorithm, and MDS-coded algorithm with n = 10 and k = 5. In Fig. 4a, we use a shifted-exponential distribution as the mother runtime distribution. That is, F (t) = 1 − et−1 for t ≥ 1. In Fig. 4b, we use the empirical task runtime distribution that is measured on an Amazon EC2 cluster3. Observe that for both cases, the
runtime distribution of the MDS-coded distribution has the lightest tail.

D. Optimal Code Design for Coded Distributed Algorithms: The Shifted-exponential Case

When a coded distributed algorithm is used, the original task is divided into a fewer number of tasks compared to the case of uncoded algorithms. Thus, the runtime of each task of a coded algorithm, which is F (kt), is stochastically larger than that of an uncoded algorithm, which is F (nt). If the value that we choose for k is too small, then the runtime of each task becomes so large that the overall runtime of the distributed coded algorithm will eventually increase. If k is too large, the level of redundancy may not be sufﬁcient to prevent the algorithm from being delayed by the stragglers.
Given the mother runtime distribution and the code parameters, one can compute the overall runtime distribution of the coded distributed algorithm using (6) and (7). Then, one can optimize the design based on various target metrics, e.g., the expected overall runtime, the 99th percentile runtime, etc.

3The detailed description of the experiments is provided in Sec. III-F.

8

In this section, we show how one can design an optimal coded algorithm that minimizes the expected overall runtime for a shifted-exponential mother distribution. The shifted-exponential distribution strikes a good balance between accuracy and analytical tractability. This model is motivated by the model proposed in [81]: the authors used this distribution to model latency of ﬁle queries from cloud storage systems. The shifted-exponential distribution is the sum of a constant and an exponential random variable, i.e.,

Pr(T0 ≤ t) = 1 − e−µ(t−1), ∀t ≥ 1,

(8)

where the exponential rate µ is called the straggling parameter. With this shifted-exponential model, we ﬁrst characterize a lower bound on the fundamental limit of the average runtime.

Proposition 3. The average runtime of any distributed algorithm, in a distributed computing cluster with n workers, is lower bounded by n1 .

Proof: One can show that the average runtime of any distributed algorithm strictly decreases if the mother runtime

distribution is replaced with a deterministic constant 1. Thus, the optimal average runtime with this deterministic mother

distribution serves as a strict lower bound on the optimal average runtime with the shifted-exponential mother distribution.

The constant mother distribution implies that stragglers do not exist, and hence the uncoded distributed algorithm achieves the

optimal runtime, which is n1 . We now analyze the average runtime of uncoded/coded distributed algorithms. We assume that n is large, and k is linear

in n. Accordingly, we approximate Hn =def

n1 i=1 i

log n and Hn−k

log (n − k). We ﬁrst note that the expected value of

the

maximum

of

n

independent

exponential

random

variables

with

rate

µ

is

Hn µ

.

Thus,

the

average

runtime

of

an

uncoded

distributed algorithm is

E[Touvnecroadlled] = n1 1 + µ1 log n = Θ long n .

For

the

average

runtime

of

an

nk -Repetition-coded

distributed

algorithm,

we

ﬁrst

note

that

the

minimum

of

n k

exponential random variables with rate µ is distributed as an exponential random variable with rate nk µ. Thus,

E[T Repetition-coded] = 1 1 + k log k = Θ log n .

overall

k

nµ

n

(9) independent
(10)

Finally, we note that the expected value of the kth statistic of n independent exponential random variables of rate µ is Hn−µHn−k . Therefore,

E[ToMveDraSl-lcoded] = k1 1 + µ1 log n −n k = Θ n1 . (11)

Using these closed-form expressions of the average runtime, one can easily ﬁnd the optimal value of k that achieves the optimal average runtime. The following lemma characterizes the optimal repetition code for the repetition-coded algorithms and their runtime performances.

Lemma 4 (Optimal repetition-coded distributed algorithms). If µ ≥ 1, the average runtime of an nk -Repetition-coded distributed algorithm, in a distributed computing cluster with n workers, is minimized by setting k = n, i.e., not replicating

tasks.

If

µ

=

1 v

for

some

integer

v

>

1,

the

average

runtime

is

minimized

by

setting

k

=

µn,

and

the

corresponding

minimum

average

runtime

is

1 nµ

(1 + log(nµ)).

Proof: It is easy to see that (10) as a function of k has a unique extreme point. By differentiating (10) with respect to k

and

equating

it

to

zero,

we

have

k

=

µn.

Thus,

if

µ

≥

1,

one

should

set

k

=

n;

if

µ

=

1 v

<

1

for

some

integer

v,

one

should

set k = µn.

The above lemma reveals that the optimal repetition-coded distributed algorithm can achieve a lower average runtime than

the uncoded distributed algorithm if µ < 1; however, the optimal repetition-coded distributed algorithm still suffers from the

factor of Θ(log n), and cannot achieve the order-optimal performance. The following lemma, on the other hand, shows that

the optimal MDS-coded distributed algorithm can achieve the order-optimal average runtime performance.

Lemma 5 (Optimal MDS-coded distributed algorithms). The average runtime of an (n, k)-MDS-coded distributed algorithm, in a distributed computing cluster with n workers, can be minimized by setting k = k where

1

k = 1 + W−1(−e−µ−1) n,

(12)

and W−1(·) is the lower branch of Lambert W function4 Thus,

T =def min E[T MDS-coded] = −W−1(−e−µ−1) =def γ (µ) .

k

overall

µn

n

(13)

4W−1(x), the lower branch of Lambert W function evaluated at x, is the unique solution of tet = x and t ≤ −1.

9

16

1

14

0.9

12 0.8 0.7
10 0.6

n × T* k* /n

8

0.5

6

0.4

0.3 4
0.2

2

0.1

0

0

10-1

100

101

µ, Straggling parameter

10-4 10-3 10-2 10-1 100 101 102 103 104 µ, Straggling parameter

(a) nT as a function of µ.

(b) kn as a function of µ.

Fig. 5: nT and kn as functions of µ. As a function of the straggling parameter, we plot the normalized optimal computing time and the optimal value
of k.

Proof: It is easy to see that (11) as a function of k has a unique extreme point. By differentiating (11) with respect to k

and

equating

it

to

zero,

we

have

1 k

1

+

1 µ

log

n n−k

=

11 µ n−k

. By setting k = α

n, we have

1 α

1

+

1 µ

log

1 1−α

=

11 µ 1−α

,

which

implies

µ+1

=

1 1−α

− log

1 1−α

.

By

deﬁning

β

=

1 1−α

and exponentiating both the sides, we have

eµ+1

=

eββ .

Note

that

the

solution

of

ex x

= t,

t≥e

and

x≥1

is

x

=

−W−1(−

1 t

).

Thus,

β

= −W−1(−e−µ−1).

By

plugging

the above equation into the deﬁnition of β, the claim is proved.

We plot nT

and

k µ

as functions of µ in Fig. 5. In addition to the order-optimality of MDS-coded distributed algorithms,

the above lemma precisely characterizes the gap between the achievable runtime and the optimistic lower bound of n1 . For

instance, when µ > 1, the optimal average runtime is only 3.15 away from the lower bound.

Remark 3 (Storage overhead). So far, we have considered only the runtime performance of distributed algorithms. Another

important metric to be considered is the storage cost. When coded computation is being used, the storage overhead may

increase.

For

instance,

the

MDS-coded

distributed

algorithm

for

matrix

multiplication,

described

in

Sec.

III-A,

requires

1 k

of

the whole data to be stored at each worker, while the uncoded distributed algorithm requires n1 . Thus, the storage overhead

factor is k1 −1 n1 = nk − 1. If one uses the runtime-optimal MDS-coded distributed algorithm for matrix multiplication, the storage

overhead isn kn − 1 = α1 − 1.

E. Coded Gradient Descent: An MDS-coded Distributed Algorithm for Linear Regression

In this section, as a concrete application of coded matrix multiplication, we propose the coded gradient descent for solving large-scale linear regression problems.
We ﬁrst describe the (uncoded) gradient-based distributed algorithm. Consider the following linear regression,

def

1

min f (x) = min

Ax − y

2,

(14)

x

x2

2

where y ∈ Rq is the label vector, A = [a1, a2, . . . , aq]T ∈ Rq×r is the data matrix, and x ∈ Rr is the unknown weight vector
to be found. We seek a distributed algorithm to solve this regression problem. Since f (x) is convex in x, the gradient-based distributed algorithm works as follows. We ﬁrst compute the objective function’s gradient: ∇f (x) = AT (Ax − y). Denoting by x(t) the estimate of x after the tth iteration, we iteratively update x(t) according to the following equation.

x(t+1) = x(t) − η∇f (x(t)) = x(t) − ηAT (Ax(t) − y)

(15)

The above algorithm is guaranteed to converge to the optimal solution if we use a small enough step size η [82], and can be easily

distributed. We describe one simple way of parallelizing the algorithm, which is implemented in many open-source machine

learning libraries including Spark mllib [83]. As AT (Ax(t) − y) =

q i=1

ai(aTi

x(t)

−

yi),

gradients

can

be

computed

in

a

distributed way by computing partial sums at different worker nodes and then adding all the partial sums at the master node.

This distributed algorithm is an uncoded distributed algorithm: in each round, the master node needs to wait for all the task

10

-+

-+

x(t)

A01 A02

A0n 1 A0n

A01x(t)
A01

A02x(t)
A02

A0n 1x(t)
A0n 1

A0nx(t)
A0n

(a) In the beginning of the tth iteration, the master node multicasts x(t) to the worker nodes.
-+

(b) The master node waits for the earliest responding k worker nodes, and computes Ax(t).
-+

z(t) = Ax(t) y

Ae 01 Ae 02

Ae 0n 1 Ae 0n

Ae 01z(t)
Ae 01

Ae 02z(t)
Ae 02

Ae 0n 1z(t)
Ae 0n 1

Ae 0nz(t)
Ae 0n

(c) The master node computes z(t) and multicasts it to the worker nodes.

(d) The master node waits for the k earliest responding worker nodes, and computes AT z(t) or ∇f (x(t)).

Fig. 6: Illustration of a coded gradient descent approach for linear regression. The coded gradient descent computes a gradient of the objective function
using coded matrix multiplication twice: in each iteration, it ﬁrst computes Ax(t) as depicted in (a) and (b), and then computes AT (Ax(t) − y) as depicted
in (c) and (d).

results in order to compute the gradient.5 Thus, the runtime of each update iteration is determined by the slowest response among all the worker nodes.
We now propose the coded gradient descent, a coded distributed algorithm for linear regression problems. Note that in each iteration, the following two matrix-vector multiplications are computed.

Ax(t), AT (Ax(t) − y) =def AT z(t)

(16)

In Sec. III-A, we proposed the MDS-coded distributed algorithm for matrix multiplication. Here, we apply the algorithm

twice to compute these two multiplications in each iteration. More speciﬁcally, for the ﬁrst matrix multiplication, we choose

1 ≤ k1 < n and use an (n, k1)-MDS-coded distributed algorithm for matrix multiplication to encode the data matrix A. Similarly for the second matrix multiplication, we choose 1 ≤ k2 < n and use a (n, k2)-MDS-coded distributed algorithm to

encode the transpose of the data matrix. Denoting the ith row-split (column-split) of A as Ai (Ai), the ith worker stores both Ai and Ai. In the beginning of each iteration, the master node multicasts x(t) to the worker nodes, each of which computes the local matrix multiplication for Ax(t) and sends the result to the master node. Upon receiving any k1 task results, the master node can start decoding the result and obtain z(t) = Ax(t). The master node now multicasts z(t) to the workers, and the workers compute local matrix multiplication for AT z(t). Finally, the master node can decode AT z(t) as soon as it receives

any k2 task results, and can proceed to the next iteration. Fig. 6 illustrates the protocol with k1 = k2 = n − 1.

Remark 4 (Storage overhead of the coded gradient descent). The coded gradient descent requires each node to store a ( k1 +

1−

1

)-fraction of the data matrix. As the minimum storage overhead per node is a

1 -fraction

of

the

data

1
matrix, the

k2

k1 k2

n

relative storage overhead of the coded gradient descent algorithm is at least about factor of 2, if k1 n and k2 n.

F. Experimental Results

In order to see the efﬁcacy of coded computation, we implement the proposed algorithms and test them on an Amazon EC2 cluster. We ﬁrst obtain the empirical distribution of task runtime in order to observe how frequently stragglers appear in our testbed by measuring round-trip times between the master node and each of 10 worker instances on an Amazon EC2 cluster. Each worker computes a matrix-vector multiplication and passes the computation result to the master node, and the master node measures round trip times that include both computation time and communication time. Each worker repeats this procedure 500 times, and we obtain the empirical distribution of round trip times across all the worker nodes.

5Indeed, one may apply another coded computation scheme called Gradient Coding [46], which was proposed after our conference publications. By applying Gradient Coding to this algorithm, one can achieve straggler tolerance but at the cost of signiﬁcant computation and storage overheads. More precisely, it incurs Θ(n) larger computation and storage overheads in order to protect the algorithm from Θ(n) stragglers. Later in this section, we will show that our coded computation scheme, which is tailor-designed for linear regression, incurs Θ(1) overheads to protect the algorithm from Θ(n) stragglers.

11

P(T > t)

1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1
0 0

0.1

0.2

0.3

0.4

Compute latency (sec)

Fig. 7: Empirical CCDF of the measured round trip times. We measure round trip times between the master node and each of 10 worker nodes on an
Amazon EC2 cluster. A round trip time consists of transmission time of the input vector from the master to a worker, computation time, and transmission time of the output vector from a worker to the master.

Average runtime (ms) 95th percentile runtime (ms)
Average runtime (ms) 95th percentile runtime (ms)

Block Col Row

350

300

250

200

150

100

50

0

Square

Fat

Matrix type

Coded
Tall

(a) m1-small, average runtime

Block Col Row Coded

400

300

200

100

0

Square

Fat

Tall

Matrix type

(b) m1-small, tail runtime

Block Col Row Coded
40

30

20

10

0

Square

Fat

Tall

Matrix type

(c) c1-med, average runtime

Block Col Row

60

50

40

30

20

10

0

Square

Fat

Matrix type

Coded
Tall

(d) c1-med, tail runtime

Fig. 8: Comparison of parallel matrix multiplication algorithms. We compare various parallel matrix multiplication algorithms: block, column-partition,
row-partition, and coded (row-partition) matrix multiplication. We implement the four algorithms using OpenMPI and test them on Amazon EC2 cluster of 25 instances. We measure the average and the 95th percentile runtime of the algorithms. Plotted in (a) and (b) are the results with m1-small instances, and
in (c) and (d) are the results with c1-medium instances.

In Fig. 7, we plot the histogram and complementary CDF (CCDF) of measured computing times; the average round trip time is 0.11 second, and the 95th percentile latency is 0.20 second, i.e., roughly ﬁve out of hundred tasks are going to be roughly two times slower than the average tasks. Assuming the probability of a worker being a straggler is 5%, if one runs an uncoded distributed algorithm with 10 workers, the probability of not seeing such a straggler is only about 60%, so the algorithm is slowed down by a factor of more than 2 with probability 40%. Thus, this observation strongly emphasizes the necessity of an efﬁcient straggler mitigation algorithm. In Fig. 4a, we plot the runtime distributions of uncoded/coded distributed algorithms
using this empirical distribution as the mother runtime distribution. When an uncoded distributed algorithm is used, the overall
runtime distribution entails a heavy tail, while the runtime distribution of the MDS-coded algorithm has almost no tail. We then implement the coded matrix multiplication in C++ using OpenMPI [84] and benchmark on a cluster of 26 EC2
instances (25 workers and a master)6. Also, three uncoded matrix multiplication algorithms – block, column-partition, and row-partition – are implemented and benchmarked.
We randomly draw a square matrix of size 5750 × 5750, a fat matrix of size 5750 × 11500, and a tall matrix of size 11500 × 5750, and multiply them with a column vector. For the coded matrix multiplication, we choose an (25, 23) MDS code so that the runtime of the algorithm is not affected by any 2 stragglers. Fig. 8 shows that the coded matrix multiplication outperforms all the other parallel matrix multiplication algorithms in most cases. On a cluster of m1-small, the most unreliable instances, the coded matrix multiplication achieves about 40% average runtime reduction and about 60% tail reduction compared to the best of the 3 uncoded matrix multiplication algorithmss. On a cluster of c1-medium instances, the coded algorithm achieves the best performance in most of the tested cases: the average runtime is reduced by at most 39.5%, and the 95th
6For the benchmark, we manage the cluster using the StarCluster toolkit [85]. Input data is generated using a Python script, and the input matrix is row-partitioned for each of the workers (with the required encoding as described in the previous sections) in a preprocessing step. The procedure begins by having all of the worker nodes read in their respective row-partitioned matrices. Then, the master node reads the input vector and distributes it to all worker nodes in the cluster through an asynchronous send (MPI_Isend). Upon receiving the input vector, each worker node begins matrix multiplication through a BLAS [86] routine call and once completed sends the result back to the master using MPI_Send. The master node waits for a sufﬁcient number of results to be received by continuously polling (MPI_Test) to see if any results are obtained. The procedure ends when the master node decodes the overall result after receiving enough partial results.

12

percentile runtime is reduced by at most 58.3%. Among the tested cases, we observe one case in which both the uncoded row-partition and the coded row-partition algorithms are outperformed by the uncoded column-partition algorithm. This is the case of a fat matrix multiplication with c1-medium instances. Note that when a row-partition algorithm is used, the size of messages from the master node to the workers is n times larger compared with the case of column-partition algorithms. Thus, when the variability of computational times becomes low compared with that of communication time, the larger communication overhead of row-partition algorithms seems to arise, nullifying the beneﬁts of coding.

Average runtime (ms) 95th percentile runtime (s)

Uncoded Coded
10

8

6

4

2

0

Square

Fat

Tall

Matrix type

(a) Average runtime

Uncoded Coded

12

10

8

6

4

2

0

Square

Fat

Tall

Matrix type

(b) Tail runtime

Fig. 9: Comparison of parallel gradient algorithms. We compare parallel gradient algorithms for linear regression problems. We implement both the
uncoded gradient descent algorithm and the coded gradient descent algorithm using Open MPI, and test them on an Amazon EC2 cluster of 10 worker instances. Plotted are the average and the 95th percentile runtimes of the algorithms.

We also evaluate the performance of the coded gradient descent algorithm for linear regression. The coded linear regression procedure is also implemented in C++ using OpenMPI, and benchmarked on a cluster of 11 EC2 machines (10 workers and a master). Similar to the previous benchmarks, we randomly draw a square matrix of size 2000 × 2000, a fat matrix of size 400 × 10000, and a tall matrix of size 10000 × 400, and use them as a data matrix. We use a (10, 8)-MDS code for the coded linear regression so that each multiplication of the gradient descent algorithm is not slowed down by up to 2 stragglers. Fig. 9 shows that the gradient algorithm with the coded matrix multiplication signiﬁcantly outperforms the one with the uncoded matrix multiplication; the average runtime is reduced by 31.3% to 35.7%, and the tail runtime is reduced by 27.9% to 35.6%.
IV. CODED SHUFFLING
We shift our focus from solving the straggler problem to solving the communication bottleneck problem. In this section, we explain the problem of data-shufﬂing, propose the Coded Shufﬂing algorithm, and analyze its performance.
A. Setup and Notations
We consider a master-worker distributed setup, where the master node has access to the entire data-set. Before every iteration of the distributed algorithm, the master node randomly partition the entire data set into n subsets, say A1, A2, . . . , An. The goal of the shufﬂing phase is to distribute each of these partitioned data sets to the corresponding worker so that each worker can perform its distributed task with its own exclusive data set after the shufﬂing phase.
We let A(J ) ∈ R|J |×r, J ⊂ [q] be the concatenation of |J | rows of matrix A with indices in J . Assume that each worker node has a cache of size s data rows (or s × r real numbers). In order to be able to fully store the data matrix across the worker nodes, we impose the inequality condition q/n ≤ s. Further, clearly if s > q, the data matrix can be fully stored at each worker node, eliminating the need for any shufﬂing. Thus, without loss of generality we assume that s ≤ q. As explained earlier working on the same data points at each worker node in all the iterations of the iterative optimization algorithm leads to slow convergence. Thus, to enhance the statistical efﬁciency of the algorithm, the data matrix is shufﬂed after each iteration. More precisely, at each iteration t, the set of data rows [q] is partitioned uniformly at random into n subsets Sit, 1 ≤ i ≤ n so that ∪ni=1Sit = [q] and Sit ∩ Sjt = ∅ when i = j; thus, each worker node computes a fresh local function of the data. Clearly, the data set that worker i works on has cardinality q/n, i.e., |Sit| = q/n. Note that the sampling we consider here is without replacement, and hence these data sets are non-overlapping.
B. Shufﬂing Schemes
We now present our coded shufﬂing algorithm, consisting of a transmission strategy for the master node, and caching and decoding strategies for the worker nodes. Let Cit be the cache content of node i (set of row indices stored in cache i) at the end of iteration t. We design a transmission algorithm (by the master node) and a cache update algorithm to ensure that (i) Sit ⊂ Cit; and (ii) Cit \ Sit is distributed uniformly at random without replacement in the set [q] \ Sit. The ﬁrst condition ensures that at each iteration, the workers have access to the data set that they are supposed to work on. The second condition provides the opportunity of effective coded transmissions for shufﬂing in the next iteration as will be explained later.
1) Cache Update Rule
We consider the following cache update rule: the new cache will contain the subset of the data points used in the current iteration (this is needed for the local computations), plus a random subset of the previous cached contents. More speciﬁcally,

13

q/n rows of the new cache are precisely the rows in Sit+1, and s − q/n rows of the cache are sampled points from the set Cit \ Sit+1, uniformly at random without replacement. Since the permutation πt is picked uniformly at random, the marginal distribution of the cache contents at iteration t+1 given Sit+1, 1 ≤ i ≤ n is described as follows: Sit+1 ⊂ Cit+1 and Cit+1 \Sit+1 is distributed uniformly at random in [q] \ Sit+1 without replacement.
2) Encoding and Transmission Schemes

We now formally describe two transmission schemes of the master node: (1) uncoded transmission and (2) coded transmission.

In the following descriptions, we drop the iteration index t (and t + 1) for the ease of notation.

The uncoded transmission ﬁrst ﬁnds how many data rows in Si are already cached in Ci, i.e. |Ci ∩ Si|. Since, the new

permutation (partitioning) is picked uniformly at random, s/q fraction of the data row indices in Si are cached in Ci, so as q

gets

large,

we

have

|Ci ∩ Si| + o(q)

=

q n

(1

−

s/q

).

Thus,

without

coding,

the

master

node

needs

to

transmit

q n

(1

−

s/q

)

data

points to each of the n worker nodes. The total communication rate (in data points transmitted per iteration) of the uncoded

scheme is then

Ru = n × q (1 − s/q) = q(1 − s/q).

(17)

n

We now describe the coded transmission scheme. Deﬁne the set of “exclusive” cache content as CI = (∩i∈ICi) ∩ ∩i ∈[n]\ICi that denotes the set of rows that are stored at the caches of I, and are not stored at the caches of [n] \ I. For
each subset I with |I| ≥ 2, the master node will multicast i∈I A(Si ∩ CI\{i}) to the worker nodes. Note that in general, the matrices A’s differ in their sizes, so one has to zero-pad the shorter matrices and sum the zero-padded matrices. Algorithm 1 provides the pseudocode of the coded encoding and transmission scheme.7

Algorithm 1 Coded Encoding and Transmission Scheme

procedure ENCODING([Ci]ni=1) for each I ∈ [n]n, |I| > 2 do

CI = (∩i∈I Ci) ∩ ∩i ∈[n]\I Ci ← max|iI=|1 |Si ∩ CI\{i}|
for each i ∈ I do Bi[1 : |Si ∩ CI\{i}|, :] ← A(Si ∩ CI\{i})

Bi[|Si ∩ CI\{i}| + 1 : , :] ← 0 end for

broadcast end for

i∈I Bi

end procedure

3) Decoding Algorithm
The decoding algorithm for the uncoded transmission scheme is straightforward: each worker simply takes the additional data rows that are required for the new iteration, and ignores the other data rows. We now describe the decoding algorithm for the coded transmission scheme. Each worker, say worker i, decodes each encoded data row as follows. Consider an encoded data row for some I that contains i. (All other data rows are discarded.) Such an encoded data row must be the sum of some data row in Si and |I| − 1 data rows in CI\{i}, which are available in worker i by the deﬁnition of C. Hence, the worker can always subtract the data rows corresponding to CI\{i} and decode the data row in Si.
C. Example
The following example illustrates the coded shufﬂing scheme.
Example 3. Let n = 3. Recall that worker node i needs to obtain A(Si ∩ Ci ) for the next iteration of the algorithm. Consider i = 1. The data rows in S1 ∩ C1 are stored either exclusively in C2 or C3 (i.e. C2 or C3), or stored in both C2 and C3 (i.e. C2,3). The transmitted message consists of 4 parts:
• (Part 1) M{1,2} = A(S1 ∩ C2) + A(S2 ∩ C1), • (Part 2) M{1,3} = A(S1 ∩ C3) + A(S3 ∩ C1), • (Part 3) M{2,3} = A(S2 ∩ C3) + A(S3 ∩ C2), and • (Part 4) M{1,2,3} = A(S1 ∩ C2,3) + A(S2 ∩ C1,3) + A(S3 ∩ C1,2).
7Note that for each encoded data row, the master node also needs to transmit tiny metadata describing which data rows are included in the summation. We omit this detail in the description of the algorithm.

14

1000 800

Uncoded shuffling Coded shuffling

Shuffling rate

600

400

200

0 0 0.2 0.4 0.6 0.8 1 s/q
Fig. 10: The achievable rates of coded and uncoded shufﬂing schemes. This ﬁgure shows the achievable rates of coded and uncoded schemes versus
the cache size for parallel stochastic gradient descent algorithm.

We show that worker node 1 can recover the data rows that it does not store or A(S1 ∩ C1 ). First, observe that node 1 stores S2 ∩ C1. Thus, it can recover A(S1 ∩ C2) using part 1 of the message since A(S1 ∩ C2) = M1 − A(S2 ∩ C1). Similarly, node 1 recovers A(S1 ∩ C3) = M2 − A(S3 ∩ C1). Finally, from part 4 of the message, node 1 recovers A(S1 ∩ C2,3) = M4 − A(S2 ∩ C1,3) − A(S3 ∩ C1,2).
D. Main Results
We now present the main result of this section, which characterizes the communication rate of the coded scheme. Let p = qs−−qq//nn .
Theorem 6 (Coded Shufﬂing Rate). Coded shufﬂing achieves communication rate
Rc = (nqp)2 (1 − p)n+1 + (n − 1)p(1 − p) − (1 − p)2 (18)
(in number of data rows transmitted per iteration from the master node), which is signiﬁcantly smaller than Ru in (17).
The reduction in communication rate is illustrated in Fig. 10 for n = 50 and q = 1000 as a function of s/q, where 1/n ≤ s/q ≤ 1. For instance, when s/q = 0.1, the communication overhead for data-shufﬂing is reduced by more than 81%. Thus, at a very low storage overhead for caching, the algorithm can be signiﬁcantly accelerated.
Before we present the proof of the theorem, we brieﬂy compare our main result with similar results shown in [67], [87]. Our coded shufﬂing algorithm is related to the coded caching problem [67], since one can design the right cache update rule to reduce the communication rate for an unknown demand or permutation of the data rows. A key difference though is that the coded shufﬂing algorithm is run over many iterations of the machine learning algorithm. Thus, the right cache update rule is required to guarantee the opportunity of coded transmission at every iteration. Furthermore, the coded shufﬂing problem has some connections to coded MapReduce [87] as both algorithms mitigate the communication bottlenecks in distributed computation and machine learning. However, coded shufﬂing enables coded transmission of raw data by leveraging the extra memory space available at each node, while coded MapReduce enables coded transmission of processed data in the shufﬂing phase of the MapReduce algorithm by cleverly introducing redundancy in the computation of the mappers.
We now prove Theorem 6.

Proof. To ﬁnd the transmission rate of the coded scheme we ﬁrst need to ﬁnd the cardinality of sets Sit+1 ∩ CIt for I ⊂ [n] and i ∈/ I. To this end, we ﬁrst ﬁnd the probability that a random data row, r, belongs to CIt . Denote this probability by Pr(r ∈ CIt ). Recall that the cache content distribution at iteration t: q/n rows of cache j are stored with Sjt and the other

15

s − q/n rows are stored uniformly at random. Thus, we can compute Pr(r ∈ CIt ) as follows.

Pr(r ∈ CIt )

n

= Pr(r ∈ CIt |r ∈ Sit) Pr(r ∈ Sit)

(19)

i=1

= n 1 Pr(r ∈ Ct |r ∈ St)

(20)

n

I

i

i=1

= 1 Pr(r ∈ Ct |r ∈ St)

(21)

n

I

i

i∈I

1 s − q/n |I|−1

s − q/n n−|I|

= n q − q/n

1 − q − q/n

(22)

i∈I

= |I| p|I|−1(1 − p)n−|I|.

(23)

n

(19) is by the law of total probability. (20) is by the fact that r is chosen randomly. To see (21), note that Pr(r ∈ CIt |r ∈

Sit, i ∈/ I) = 0. Thus, the summation can be written only on the indices of I. We now explain (22). Given that r belongs to

Sit, and i ∈ I, then r ∈ Ci with probability 1. The other |I| − 1 caches with indices in I \ {i} contain r with probability

s−q/n q−q/n

independently.

Further,

the

caches

with

indices

in

[n] \ I

do

not

contain

r

with

probability

1−

qs−−qq//nn .

By

deﬁning

p =def qs−−qq//nn , we have (23).

We now ﬁnd the cardinality of Sit+1 ∩ CIt for I ⊂ [n] and i ∈/ I. Note that |Sit+1| = q/n. Thus, as q gets large (and n

remains sub-linear in q), by the law of large numbers,

|St+1 ∩ Ct | = q × |I| p|I|−1(1 − p)n−|I| + o(q).

(24)

i

I nn

Recall that for each subset I with |I| ≥ 2, the master node will send i∈I A(Si ∩ CI\{i}) . Thus, the total rate of coded transmission is
Rc = n ni nq i −n 1 pi−2(1 − p)n−(i−1). (25)
i=2

To complete the proof, we simplify the above expression. Let x = 1−p p . Taking derivative with respect to x from both sides

of the equality

n i=1

n i

xi−1

=

1 x

[(1 + x)n − 1],

we

have

n n (i − 1)xi−2 = 1 + (1 + x)n−1(nx − x − 1) .

(26)

i

x2

i=2

Using (26) in (25) completes the proof.

Corollary 7. Consider the case that the cache sizes are just enough to store the data required for processing; that is s = q/n. Then, Rc = 12 Ru. Thus, one gets a factor 2 reduction gain in communication rate by exploiting coded caching.
Note that when s = q/n, p = 0. Finding the limit limp→0 Rc in (18), after some manipulations, one calculates

Rc = q 1 − s

1 = Ru/2,

(27)

q 1 + ns/q

which shows Corollary 7.

Corollary 8. Consider the regime of interest where n, s, and q get large, and s/q → c > 0 and n/q → 0. Then,

Rc → q 1 − s 1 = Ru

(28)

q ns/q ns/q

Thus, using coding, the communication rate is reduced by Θ(n).

Remark 5 (The advantage of using multicasting over unicasting). It is reasonable to assume that γ(n) n for wireless architecture that is of great interest with the emergence of wireless data centers, e.g. [88], [89], and mobile computing platforms [90]. However, still in many applications, the network topology is based on point-to-point communication, and the multicasting opportunity is not fully available, i.e., γ(n) < n. For these general cases, we have to renormalize the communication cost of

16

8000 7000

MPI Broadcast MPI Scatter

Average transmission time (ms)

6000

5000

4000

3000

2000

1000

0

24

8

16

32

Number of receivers

Fig. 11: Gains of multicasting over unicasting in distributed systems. We measure the time taken for a data block of size of 4.15 MB to be transmitted
to a targeted number of workers on an Amazon EC2 cluster, and compare the average transmission time taken with Message Passing Interface (MPI) scatter (unicast) and that with MPI broadcast. Observe that the average transmission time increases linearly as the number of receivers increases, but with MPI broadcast, the average transmission time increases logarithmically.

coded shufﬂing since we have assumed that γ(n) = n in our results. For instance, in the regime considered in Corollary 8, the renormalized communication cost of coded shufﬂing Rcγ given γ(n) is

Rcγ = γ(nn) Rc → γ(nR)us/q . (29)

Thus, the communication cost of coded shufﬂing is smaller than uncoded shufﬂing if γ(n) > q/s. Note that s/q is the fraction

of the data matrix that can be stored at each worker’s cache. Thus, in the regime of interest where s/q is a constant independent

of n, and γ(n) scales with n, the reduction gain of coded shufﬂing in communication cost is still unbounded and increasing

in n.

We emphasize that even in point-to-point communication networks, multicasting the same message to multiple nodes is

signiﬁcantly faster than unicasting different message (of the same size) to multiple nodes, i.e., γ(n) 1, justifying the

advantage of using coded shufﬂing. For instance, the MPI broadcast API (MPI_Bcast) utilizes a tree multicast algorithm,

which achieves γ(n) = Θ

n log n

. Shown in Fig. 11 is the time taken for a data block to be transmitted to an increasing

number of workers on an Amazon EC2 cluster, which consists of a point-to-point communication network. We compare the

average transmission time taken with MPI scatter (unicast) and that with MPI broadcast. Observe that the average transmission

time increases linearly as the number of receivers increases, but with MPI broadcast, the average transmission time increases

logarithmically.

V. CONCLUSION

In this paper, we have explored the power of coding in order to make distributed algorithms robust to a variety of sources of “system noise” such as stragglers and communication bottlenecks. We propose a novel Coded Computation framework that can signiﬁcantly speed up existing distributed algorithms, by introducing redundancy through codes into the computation. Further, we propose Coded Shufﬂing that can signiﬁcantly reduce the heavy price of data-shufﬂing, which is required for achieving high statistical efﬁciency in distributed machine learning algorithms. Our preliminary experimental results validate the power of our proposed schemes in effectively curtailing the negative effects of system bottlenecks, and attaining signiﬁcant speedups of up to 40%, compared to the current state-of-the-art methods.
There exists a whole host of theoretical and practical open problems related to the results of this paper. For coded computation, instead of the MDS codes, one could achieve different tradeoffs by employing another class of codes. Then, although matrix multiplication is one of the most basic computational blocks in many analytics, it would be interesting to leverage coding for a broader class of distributed algorithms.
For coded shufﬂing, convergence analysis of distributed machine learning algorithms under shufﬂing is not well understood. As we observed in the experiments, shufﬂing signiﬁcantly reduces the number of iterations required to achieve a target reliability, but missing is a rigorous analysis that compares the convergence performances of algorithms with shufﬂing or without shufﬂing. Further, the trade-offs between bandwidth, storage, and the statistical efﬁciency of the distributed algorithms are not well understood. Moreover, it is not clear how far our achievable scheme, which achieves a bandwidth reduction gain

17
of Θ( n1 ), is from the fundamental limit of communication rate for coded shufﬂing. Therefore, ﬁnding an information-theoretic lower bound on the rate of coded shufﬂing is another interesting open problem.
REFERENCES
[1] K. Lee, M. Lam, R. Pedarsani, D. Papailiopoulos, and K. Ramchandran, “Speeding up distributed machine learning using codes,” IEEE Transactions on Information Theory, vol. PP, no. 99, pp. 1–1, 2017.
[2] ——, “Speeding up distributed machine learning using codes,” Presented at 2015 Neural Information Processing Systems (NIPS): Workshop on Machine Learning Systems, December 2015.
[3] ——, “Speeding up distributed machine learning using codes,” in Proc. of IEEE International Symposium on Information Theory (ISIT), July 2016, pp. 1143–1147.
[4] M. Zaharia, M. Chowdhury, M. J. Franklin, S. Shenker, and I. Stoica, “Spark: Cluster computing with working sets,” in Proc. 2nd USENIX Workshop on Hot Topics in Cloud Computing (HotCloud), 2010. [Online]. Available: https://www.usenix.org/conference/hotcloud-10/ spark-cluster-computing-working-sets
[5] J. Dean and S. Ghemawat, “MapReduce: Simpliﬁed data processing on large clusters,” in Proc. 6th Symposium on Operating System Design and Implementation (OSDI), 2004, pp. 137–150. [Online]. Available: http://www.usenix.org/events/osdi04/tech/dean.html
[6] J. Dean and L. A. Barroso, “The tail at scale,” Commun. ACM, vol. 56, no. 2, pp. 74–80, 2013. [Online]. Available: http: //doi.acm.org/10.1145/2408776.2408794
[7] A. G. Dimakis, P. B. Godfrey, Y. Wu, M. J. Wainwright, and K. Ramchandran, “Network coding for distributed storage systems,” IEEE Transactions on Information Theory, vol. 56, no. 9, pp. 4539–4551, 2010.
[8] K. V. Rashmi, N. B. Shah, and P. V. Kumar, “Optimal exact-regenerating codes for distributed storage at the MSR and MBR points via a product-matrix construction,” IEEE Transactions on Information Theory, vol. 57, no. 8, pp. 5227–5239, 2011.
[9] C. Suh and K. Ramchandran, “Exact-repair MDS code construction using interference alignment,” IEEE Transactions on Information Theory, vol. 57, no. 3, pp. 1425–1442, 2011.
[10] I. Tamo, Z. Wang, and J. Bruck, “MDS array codes with optimal rebuilding,” in Proc. of IEEE International Symposium on Information Theory (ISIT), 2011, pp. 1240–1244.
[11] V. R. Cadambe, C. Huang, S. A. Jafar, and J. Li, “Optimal repair of MDS codes in distributed storage via subspace interference alignment,” arXiv preprint arXiv:1106.1250, 2011.
[12] D. S. Papailiopoulos, A. G. Dimakis, and V. R. Cadambe, “Repair optimal erasure codes through Hadamard designs,” in Proc. 49th Annual Allerton Conference on Communication, Control, and Computing (Allerton). IEEE, 2011, pp. 1382–1389.
[13] P. Gopalan, C. Huang, H. Simitci, and S. Yekhanin, “On the locality of codeword symbols,” IEEE Transactions on Information Theory, vol. 58, no. 11, pp. 6925–6934, 2011.
[14] F. Oggier and A. Datta, “Self-repairing homomorphic codes for distributed storage systems,” in Proc. of IEEE INFOCOM. IEEE, 2011, pp. 1215–1223. [15] D. S. Papailiopoulos, J. Luo, A. G. Dimakis, C. Huang, and J. Li, “Simple regenerating codes: Network coding for cloud storage,” in Proc. of IEEE
INFOCOM. IEEE, 2012, pp. 2801–2805. [16] J. Han and L. A. Lastras-Montano, “Reliable memories with subline accesses,” in Proc. of IEEE International Symposium on Information Theory (ISIT).
IEEE, 2007, pp. 2531–2535. [17] C. Huang, M. Chen, and J. Li, “Pyramid codes: Flexible schemes to trade space for access efﬁciency in reliable data storage systems,” in Proc. 6th
IEEE International Symposium on Network Computing and Applications (NCA). IEEE, 2007, pp. 79–86. [18] D. S. Papailiopoulos and A. G. Dimakis, “Locally repairable codes,” in Proc. of IEEE International Symposium on Information Theory (ISIT). IEEE,
2012, pp. 2771–2775. [19] G. M. Kamath, N. Prakash, V. Lalitha, and P. V. Kumar, “Codes with local regeneration,” arXiv preprint arXiv:1211.1932, 2012. [20] A. S. Rawat, O. O. Koyluoglu, N. Silberstein, and S. Vishwanath, “Optimal locally repairable and secure codes for distributed storage systems,” IEEE
Transactions on Information Theory, vol. 60, no. 1, pp. 212–236, Jan. 2014. [21] N. Prakash, G. M. Kamath, V. Lalitha, and P. V. Kumar, “Optimal linear codes with a local-error-correction property,” in Proc. of IEEE International
Symposium on Information Theory (ISIT). IEEE, 2012, pp. 2776–2780. [22] N. Silberstein, A. Singh Rawat, and S. Vishwanath, “Error resilience in distributed storage via rank-metric codes,” CoRR, vol. abs/1202.0800, 2012. [23] C. Huang, H. Simitci, Y. Xu, A. Ogus, B. Calder, P. Gopalan, J. Li, and S. Yekhanin, “Erasure coding in Windows Azure Storage,” in Proc. of USENIX
Annual Technical Conference (ATC), Jun. 2012. [24] M. Sathiamoorthy, M. Asteris, D. Papailiopoulos, A. G. Dimakis, R. Vadali, S. Chen, and D. Borthakur, “XORing elephants: Novel erasure codes for
big data,” in Proc. VLDB Endowment, vol. 6, no. 5. VLDB Endowment, 2013, pp. 325–336. [25] K. V. Rashmi, N. B. Shah, D. Gu, H. Kuang, D. Borthakur, and K. Ramchandran, “A solution to the network challenges of data recovery in erasure-coded
distributed storage systems: A study on the Facebook warehouse cluster,” in Proc. of USENIX HotStorage, Jun. 2013. [26] K. Rashmi, N. B. Shah, D. Gu, H. Kuang, D. Borthakur, and K. Ramchandran, “A hitchhiker’s guide to fast and efﬁcient data reconstruction in
erasure-coded data centers,” in Proc. ACM conference on SIGCOMM. ACM, 2014, pp. 331–342. [27] G. Ananthanarayanan, S. Kandula, A. G. Greenberg, I. Stoica, Y. Lu, B. Saha, and E. Harris, “Reining in the outliers in Map-Reduce clusters
using Mantri,” in Proc. 9th USENIX Symposium on Operating Systems Design and Implementation (OSDI), 2010, pp. 265–278. [Online]. Available: http://www.usenix.org/events/osdi10/tech/full papers/Ananthanarayanan.pdf [28] M. Zaharia, A. Konwinski, A. D. Joseph, R. H. Katz, and I. Stoica, “Improving MapReduce performance in heterogeneous environments,” in Proc. 8th USENIX Symposium on Operating Systems Design and Implementation (OSDI), 2008, pp. 29–42. [Online]. Available: http://www.usenix.org/events/osdi08/tech/full papers/zaharia/zaharia.pdf [29] A. Agarwal and J. C. Duchi, “Distributed delayed stochastic optimization,” in Proc. 25th Annual Conference on Neural Information Processing Systems (NIPS), 2011, pp. 873–881. [Online]. Available: http://papers.nips.cc/paper/4247-distributed-delayed-stochastic-optimization [30] B. Recht, C. Re, S. Wright, and F. Niu, “Hogwild: A lock-free approach to parallelizing stochastic gradient descent,” in Proc. 25th Annual Conference on Neural Information Processing (NIPS), 2011, pp. 693–701. [31] G. Ananthanarayanan, A. Ghodsi, S. Shenker, and I. Stoica, “Effective straggler mitigation: Attack of the clones,” in Proc. 10th USENIX Symposium on Networked Systems Design and Implementation (NSDI), 2013, pp. 185–198. [Online]. Available: https://www.usenix.org/conference/ nsdi13/technical-sessions/presentation/ananthanarayanan [32] N. B. Shah, K. Lee, and K. Ramchandran, “When do redundant requests reduce latency?” in Proc. 51st Annual Allerton Conference on Communication, Control, and Computing, 2013, pp. 731–738. [Online]. Available: http://dx.doi.org/10.1109/Allerton.2013.6736597 [33] D. Wang, G. Joshi, and G. W. Wornell, “Efﬁcient task replication for fast response times in parallel computation,” in Proc. of ACM SIGMETRICS, 2014, pp. 599–600. [34] K. Gardner, S. Zbarsky, S. Doroudi, M. Harchol-Balter, and E. Hyytia¨, “Reducing latency via redundant requests: Exact analysis,” in Proc. of ACM SIGMETRICS, 2015, pp. 347–360. [35] M. Chaubey and E. Saule, “Replicated data placement for uncertain scheduling,” in Proc. of IEEE International Parallel and Distributed Processing Symposium Workshop (IPDPS), 2015, pp. 464–472. [Online]. Available: http://dx.doi.org/10.1109/IPDPSW.2015.50

18
[36] K. Lee, R. Pedarsani, and K. Ramchandran, “On scheduling redundant requests with cancellation overheads,” in Proc. 53rd Annual Allerton conference on Communication, Control, and Computing, Oct. 2015.
[37] G. Joshi, E. Soljanin, and G. Wornell, “Efﬁcient redundancy techniques for latency reduction in cloud systems,” ACM Trans. Model. Perform. Eval. Comput. Syst., vol. 2, no. 2, pp. 12:1–12:30, Apr. 2017. [Online]. Available: http://doi.acm.org/10.1145/3055281
[38] L. Huang, S. Pawar, H. Zhang, and K. Ramchandran, “Codes can reduce queueing delay in data centers,” in Information Theory Proceedings (ISIT), 2012 IEEE International Symposium on, July 2012, pp. 2766–2770.
[39] K. Lee, N. B. Shah, L. Huang, and K. Ramchandran, “The MDS queue: Analysing the latency performance of erasure codes,” IEEE Transactions on Information Theory, vol. 63, no. 5, pp. 2822–2842, May 2017.
[40] G. Joshi, Y. Liu, and E. Soljanin, “On the delay-storage trade-off in content download from coded distributed storage systems,” IEEE Journal on Selected Areas in Communications, vol. 32, no. 5, pp. 989–997, 2014.
[41] Y. Sun, Z. Zheng, C. E. Koksal, K.-H. Kim, and N. B. Shroff, “Provably delay efﬁcient data retrieving in storage clouds,” arXiv:1501.01661, 2015. [42] S. Kadhe, E. Soljanin, and A. Sprintson, “When do the availability codes make the stored data more available?” in Proc. of 53rd Annual Allerton
Conference on Communication, Control, and Computing (Allerton), Sept 2015, pp. 956–963. [43] ——, “Analyzing the download time of availability codes,” in Proc. of IEEE International Symposium on Information Theory (ISIT), June 2015, pp.
1467–1471. [44] N. Ferdinand and S. Draper, “Anytime coding for distributed computation,” Presented at the 54th Annual Allerton conference on Communication, Control,
and Computing, Monticello, IL, 2016. [45] S. Dutta, V. Cadambe, and P. Grover, “Short-Dot: Computing large linear transforms distributedly using coded short dot products,” in Advances In Neural
Information Processing Systems, 2016, pp. 2092–2100. [46] R. Tandon, Q. Lei, A. G. Dimakis, and N. Karampatziakis, “Gradient coding,” arXiv preprint arXiv:1612.03301, 2016. [47] R. Bitar, P. Parag, and S. E. Rouayheb, “Minimizing latency for secure distributed computing,” arXiv preprint arXiv:1703.017504, 2017. [48] K. Lee, C. Suh, and K. Ramchandran, “High-dimensional coded matrix multiplication,” in Proc. of IEEE International Symposium on Information Theory
(ISIT), June 2017. [49] A. Reisizadehmobarakeh, S. Prakash, R. Pedarsani, and S. Avestimehr, “Coded computation over heterogeneous clusters,” arXiv preprint
arXiv:1701.05973, 2017. [50] K. Lee, R. Pedarsani, D. Papailiopoulos, and K. Ramchandran, “Coded computation for multicore setups,” in Proc. of IEEE International Symposium
on Information Theory (ISIT), June 2017. [51] D. P. Bertsekas, Nonlinear programming. Athena scientiﬁc, 1999. [52] A. Nedic and A. E. Ozdaglar, “Distributed subgradient methods for multi-agent optimization,” IEEE Transactions on Automatic Control, vol. 54, no. 1,
pp. 48–61, 2009. [Online]. Available: http://dx.doi.org/10.1109/TAC.2008.2009515 [53] S. P. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein, “Distributed optimization and statistical learning via the alternating direction method of
multipliers,” Foundations and Trends in Machine Learning, vol. 3, no. 1, pp. 1–122, 2011. [Online]. Available: http://dx.doi.org/10.1561/2200000016 [54] R. Bekkerman, M. Bilenko, and J. Langford, Scaling up machine learning: Parallel and distributed approaches. Cambridge University Press, 2011. [55] J. C. Duchi, A. Agarwal, and M. J. Wainwright, “Dual averaging for distributed optimization: Convergence analysis and network scaling,” IEEE
Transactions on Automatic Control, vol. 57, no. 3, pp. 592–606, 2012. [Online]. Available: http://dx.doi.org/10.1109/TAC.2011.2161027 [56] J. Chen and A. H. Sayed, “Diffusion adaptation strategies for distributed optimization and learning over networks,” IEEE Transactions on Signal
Processing, vol. 60, no. 8, pp. 4289–4305, 2012. [Online]. Available: http://dx.doi.org/10.1109/TSP.2012.2198470 [57] J. Dean, G. Corrado, R. Monga, K. Chen, M. Devin, Q. V. Le, M. Z. Mao, M. Ranzato, A. W. Senior, P. A. Tucker, K. Yang, and A. Y. Ng, “Large
scale distributed deep networks,” in Proc. 26th Annual Conference on Neural Information Processing Systems (NIPS), 2012, pp. 1232–1240. [Online]. Available: http://papers.nips.cc/paper/4687-large-scale-distributed-deep-networks [58] Y. Low, D. Bickson, J. Gonzalez, C. Guestrin, A. Kyrola, and J. M. Hellerstein, “Distributed graphlab: a framework for machine learning and data mining in the cloud,” Proc. VLDB Endowment, vol. 5, no. 8, pp. 716–727, 2012. [59] T. Kraska, A. Talwalkar, J. C. Duchi, R. Grifﬁth, M. J. Franklin, and M. I. Jordan, “MLbase: A distributed machine-learning system,” in Proc. 6th Biennial Conference on Innovative Data Systems Research (CIDR), 2013. [Online]. Available: http://www.cidrdb.org/cidr2013/Papers/CIDR13 Paper118.pdf [60] E. R. Sparks, A. Talwalkar, V. Smith, J. Kottalam, X. Pan, J. E. Gonzalez, M. J. Franklin, M. I. Jordan, and T. Kraska, “MLI: an API for distributed machine learning,” in Proc. IEEE 13th International Conference on Data Mining (ICDM), 2013, pp. 1187–1192. [Online]. Available: http://dx.doi.org/10.1109/ICDM.2013.158 [61] M. Li, D. G. Andersen, J. W. Park, A. J. Smola, A. Ahmed, V. Josifovski, J. Long, E. J. Shekita, and B. Su, “Scaling distributed machine learning with the parameter server,” in Proc. 11th USENIX Symposium on Operating Systems Design and Implementation (OSDI), 2014, pp. 583–598. [Online]. Available: https://www.usenix.org/conference/osdi14/technical-sessions/presentation/li mu [62] B. Recht and C. Re´, “Parallel stochastic gradient algorithms for large-scale matrix completion,” Mathematical Programming Computation, vol. 5, no. 2, pp. 201–226, 2013. [63] L. Bottou, “Stochastic gradient descent tricks,” in Neural Networks: Tricks of the Trade - Second Edition, 2012, pp. 421–436. [Online]. Available: http://dx.doi.org/10.1007/978-3-642-35289-8 25 [64] C. Zhang and C. Re´, “Dimmwitted: A study of main-memory statistical analytics,” Proc. VLDB Endowment, vol. 7, no. 12, pp. 1283–1294, 2014. [65] M. Gu¨rbu¨zbalaban, A. Ozdaglar, and P. Parrilo, “Why random reshufﬂing beats stochastic gradient descent,” arXiv preprint arXiv:1510.08560, 2015. [66] S. Ioffe and C. Szegedy, “Batch normalization: Accelerating deep network training by reducing internal covariate shift,” arXiv preprint arXiv:1502.03167, 2015. [67] M. A. Maddah-Ali and U. Niesen, “Fundamental limits of caching,” IEEE Transactions on Information Theory, vol. 60, no. 5, pp. 2856–2867, 2014. [68] M. A. Maddah-Ali and U. Niesen, “Decentralized coded caching attains order-optimal memory-rate tradeoff,” IEEE/ACM Transactions on Networking, vol. 23, no. 4, pp. 1029–1040, 2015. [Online]. Available: http://dx.doi.org/10.1109/TNET.2014.2317316 [69] R. Pedarsani, M. A. Maddah-Ali, and U. Niesen, “Online coded caching,” in Proc. of IEEE International Conference on Communications (ICC), 2014, pp. 1878–1883. [Online]. Available: http://dx.doi.org/10.1109/ICC.2014.6883597 [70] N. Karamchandani, U. Niesen, M. A. Maddah-Ali, and S. Diggavi, “Hierarchical coded caching,” in Proc. of IEEE International Symposium on Information Theory (ISIT), 2014, pp. 2142–2146. [71] M. Ji, G. Caire, and A. F. Molisch, “Fundamental limits of distributed caching in D2D wireless networks,” in Proc. of IEEE Information Theory Workshop (ITW), 2013, pp. 1–5. [Online]. Available: http://dx.doi.org/10.1109/ITW.2013.6691247 [72] S. Li, M. A. Maddah-ali, and S. Avestimehr, “Coded MapReduce,” Presented at the 53rd Annual Allerton conference on Communication, Control, and Computing, Monticello, IL, 2015. [73] Y. Birk and T. Kol, “Coding on demand by an informed source (iscod) for efﬁcient broadcast of different supplemental data to caching clients,” IEEE Transactions on Information Theory, vol. 52, no. 6, pp. 2825–2830, June 2006. [74] Z. Bar-Yossef, Y. Birk, T. S. Jayram, and T. Kol, “Index coding with side information,” IEEE Transactions on Information Theory, vol. 57, no. 3, pp. 1479–1494, March 2011. [75] M. A. Attia and R. Tandon, “Information theoretic limits of data shufﬂing for distributed learning,” in Proc. of IEEE Global Communications Conference (GLOBECOM), Dec 2016, pp. 1–6. [76] ——, “On the worst-case communication overhead for distributed data shufﬂing,” in Proc. of 54th Annual Allerton Conference on Communication, Control, and Computing (Allerton), Sept 2016, pp. 961–968.

19
[77] L. Song and C. Fragouli, “A pliable index coding approach to data shufﬂing,” arXiv preprint arXiv:1701.05540, 2017. [78] S. Li, M. A. Maddah-Ali, and A. S. Avestimehr, “A uniﬁed coding framework for distributed computing with straggling servers,” CoRR, vol.
abs/1609.01690, 2016. [Online]. Available: http://arxiv.org/abs/1609.01690 [79] T. M. Cover and J. A. Thomas, Elements of information theory. John Wiley & Sons, 2012. [80] D. Costello and S. Lin, Error control coding. New Jersey, 2004. [81] G. Liang and U. C. Kozat, “TOFEC: achieving optimal throughput-delay trade-off of cloud storage using erasure codes,” in Proc. of IEEE Conference
on Computer Communications (INFOCOM), 2014, pp. 826–834. [Online]. Available: http://dx.doi.org/10.1109/INFOCOM.2014.6848010 [82] S. Boyd and L. Vandenberghe, Convex optimization. Cambridge university press, 2004. [83] X. Meng, J. K. Bradley, B. Yavuz, E. R. Sparks, S. Venkataraman, D. Liu, J. Freeman, D. B. Tsai, M. Amde, S. Owen, D. Xin, R. Xin, M. J.
Franklin, R. Zadeh, M. Zaharia, and A. Talwalkar, “MLlib: Machine learning in apache spark,” CoRR, vol. abs/1505.06807, 2015. [Online]. Available: http://arxiv.org/abs/1505.06807 [84] “Open MPI: Open source high performance computing,” http://www.open-mpi.org, accessed: 2015-11-25. [85] “StarCluster,” http://star.mit.edu/cluster/, accessed: 2015-11-25. [86] “BLAS (Basic Linear Algebra Subprograms),” http://www.netlib.org/blas/, accessed: 2015-11-25. [87] S. Li, M. A. Maddah-Ali, and A. S. Avestimehr, “Fundamental tradeoff between computation and communication in distributed computing,” in Proc. of IEEE International Symposium on Information Theory (ISIT), July 2016, pp. 1814–1818. [88] D. Halperin, S. Kandula, J. Padhye, P. Bahl, and D. Wetherall, “Augmenting data center networks with multi-gigabit wireless links,” vol. 41, no. 4, pp. 38–49, 2011. [89] Y. Zhu, X. Zhou, Z. Zhang, L. Zhou, A. Vahdat, B. Y. Zhao, and H. Zheng, “Cutting the cord: a robust wireless facilities network for data centers,” in Proceedings of the 20th annual International Conference on Mobile Computing and networking (KCML). ACM, 2014, pp. 581–592. [90] M. Y. Arslan, I. Singh, S. Singh, H. V. Madhyastha, K. Sundaresan, and S. V. Krishnamurthy, “Computing while charging: building a distributed computing infrastructure using smartphones,” in Proc. 8th international conference on Emerging networking experiments and technologies. ACM, 2012, pp. 193–204.

